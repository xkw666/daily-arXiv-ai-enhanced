{"id": "2506.01961", "pdf": "https://arxiv.org/pdf/2506.01961", "abs": "https://arxiv.org/abs/2506.01961", "authors": ["Jinzhu Yang"], "title": "Research on Medical Named Entity Identification Based On Prompt-Biomrc Model and Its Application in Intelligent Consultation System", "categories": ["cs.CL"], "comment": null, "summary": "This study is dedicated to exploring the application of prompt learning\nmethods to advance Named Entity Recognition (NER) within the medical domain. In\nrecent years, the emergence of large-scale models has driven significant\nprogress in NER tasks, particularly with the introduction of the BioBERT\nlanguage model, which has greatly enhanced NER capabilities in medical texts.\nOur research introduces the Prompt-bioMRC model, which integrates both hard\ntemplate and soft prompt designs aimed at refining the precision and efficiency\nof medical entity recognition. Through extensive experimentation across diverse\nmedical datasets, our findings consistently demonstrate that our approach\nsurpasses traditional models. This enhancement not only validates the efficacy\nof our methodology but also highlights its potential to provide reliable\ntechnological support for applications like intelligent diagnosis systems. By\nleveraging advanced NER techniques, this study contributes to advancing\nautomated medical data processing, facilitating more accurate medical\ninformation extraction, and supporting efficient healthcare decision-making\nprocesses.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u786c\u6a21\u677f\u548c\u8f6f\u63d0\u793a\u7684Prompt-bioMRC\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u533b\u5b66\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7cbe\u5ea6\u4e0e\u6548\u7387", "motivation": "\u63a2\u7d22\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u5728\u533b\u5b66NER\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u7a81\u7834\u4f20\u7edf\u6a21\u578b\u5728\u533b\u7597\u6587\u672c\u5904\u7406\u4e2d\u7684\u6027\u80fd\u74f6\u9888", "method": "\u6574\u5408\u786c\u6a21\u677f\u7684\u7ed3\u6784\u5316\u5f15\u5bfc\u4e0e\u8f6f\u63d0\u793a\u7684\u67d4\u6027\u7279\u5f81\u5b66\u4e60\uff0c\u4f18\u5316BioBERT\u6a21\u578b\u7684\u5b9e\u4f53\u8bc6\u522b\u80fd\u529b", "result": "\u5728\u591a\u533b\u7597\u6570\u636e\u96c6\u5b9e\u9a8c\u4e2d\u51c6\u786e\u7387\u8d85\u8d8a\u57fa\u51c6\u6a21\u578b\uff0c\u9a8c\u8bc1\u65b9\u6cd5\u5bf9\u667a\u80fd\u8bca\u65ad\u7cfb\u7edf\u7684\u6280\u672f\u652f\u6301\u4ef7\u503c", "conclusion": "\u901a\u8fc7\u521b\u65b0\u63d0\u793a\u5b66\u4e60\u673a\u5236\u63a8\u52a8\u533b\u7597\u4fe1\u606f\u81ea\u52a8\u5316\u5904\u7406\uff0c\u4e3a\u7cbe\u51c6\u533b\u7597\u51b3\u7b56\u63d0\u4f9b\u53ef\u9760\u6570\u636e\u652f\u6491"}}
{"id": "2506.01992", "pdf": "https://arxiv.org/pdf/2506.01992", "abs": "https://arxiv.org/abs/2506.01992", "authors": ["Lukas Rauch", "Moritz Wirth", "Denis Huseljic", "Marek Herde", "Bernhard Sick", "Matthias A\u00dfenmacher"], "title": "No Free Lunch in Active Learning: LLM Embedding Quality Dictates Query Strategy Success", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "under review @NeurIPS2025", "summary": "The advent of large language models (LLMs) capable of producing\ngeneral-purpose representations lets us revisit the practicality of deep active\nlearning (AL): By leveraging frozen LLM embeddings, we can mitigate the\ncomputational costs of iteratively fine-tuning large backbones. This study\nestablishes a benchmark and systematically investigates the influence of LLM\nembedding quality on query strategies in deep AL. We employ five top-performing\nmodels from the massive text embedding benchmark (MTEB) leaderboard and two\nbaselines for ten diverse text classification tasks. Our findings reveal key\ninsights: First, initializing the labeled pool using diversity-based sampling\nsynergizes with high-quality embeddings, boosting performance in early AL\niterations. Second, the choice of the optimal query strategy is sensitive to\nembedding quality. While the computationally inexpensive Margin sampling can\nachieve performance spikes on specific datasets, we find that strategies like\nBadge exhibit greater robustness across tasks. Importantly, their effectiveness\nis often enhanced when paired with higher-quality embeddings. Our results\nemphasize the need for context-specific evaluation of AL strategies, as\nperformance heavily depends on embedding quality and the target task.", "AI": {"tldr": "\u7814\u7a76\u8868\u660eLLM\u5d4c\u5165\u8d28\u91cf\u663e\u8457\u5f71\u54cd\u6df1\u5ea6\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u9009\u62e9\uff0c\u591a\u6837\u6027\u91c7\u6837\u4e0e\u9ad8\u8d28\u91cf\u5d4c\u5165\u534f\u540c\u589e\u6548\uff0cBadge\u7b56\u7565\u5c55\u73b0\u8de8\u4efb\u52a1\u9c81\u68d2\u6027", "motivation": "\u5229\u7528\u51bb\u7ed3\u7684LLM\u5d4c\u5165\u964d\u4f4e\u6df1\u5ea6\u4e3b\u52a8\u5b66\u4e60\u8ba1\u7b97\u6210\u672c\uff0c\u63a2\u7a76\u5d4c\u5165\u8d28\u91cf\u5bf9\u67e5\u8be2\u7b56\u7565\u9009\u62e9\u7684\u5f71\u54cd\u673a\u5236", "method": "\u91c7\u7528MTEB\u6392\u884c\u699cTop5\u6a21\u578b\u53ca\u57fa\u7ebf\u6a21\u578b\uff0c\u572810\u4e2a\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7cfb\u7edf\u8bc4\u4f30\u5d4c\u5165\u8d28\u91cf\u4e0e\u67e5\u8be2\u7b56\u7565\u7684\u4ea4\u4e92\u6548\u5e94", "result": "\u9ad8\u8d28\u91cf\u5d4c\u5165\u63d0\u5347\u591a\u6837\u6027\u91c7\u6837\u7684\u65e9\u671f\u6027\u80fd\uff0cBadge\u7b56\u7565\u8de8\u4efb\u52a1\u7a33\u5b9a\uff0cMargin\u91c7\u6837\u5b58\u5728\u6570\u636e\u96c6\u7279\u5f02\u6027\u8868\u73b0", "conclusion": "\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u9700\u7ed3\u5408\u5d4c\u5165\u8d28\u91cf\u4e0e\u76ee\u6807\u4efb\u52a1\u8fdb\u884c\u573a\u666f\u5316\u8bc4\u4f30\uff0c\u5d4c\u5165\u8d28\u91cf\u63d0\u5347\u53ef\u589e\u5f3a\u7279\u5b9a\u7b56\u7565\u7684\u6709\u6548\u6027"}}
{"id": "2506.02000", "pdf": "https://arxiv.org/pdf/2506.02000", "abs": "https://arxiv.org/abs/2506.02000", "authors": ["Abhay Gupta", "Michael Lu", "Kevin Zhu", "Sean O'Brien", "Vasu Sharma"], "title": "NovelHopQA: Diagnosing Multi-Hop Reasoning Failures in Long Narrative Contexts", "categories": ["cs.CL"], "comment": null, "summary": "Current large language models (LLMs) struggle to answer questions that span\ntens of thousands of tokens, especially when multi-hop reasoning is involved.\nWhile prior benchmarks explore long-context comprehension or multi-hop\nreasoning in isolation, none jointly vary context length and reasoning depth in\nnatural narrative settings. We introduce NovelHopQA, the first benchmark to\nevaluate k1-4 hop QA over 64k-128k-token excerpts from 83 full-length\npublic-domain novels. A keyword-guided pipeline builds hop-separated chains\ngrounded in coherent storylines. We evaluate six state-of-the-art (SOTA) models\nand apply oracle-context filtering to ensure all questions are genuinely\nanswerable. Human annotators validate both alignment and hop depth. We noticed\nconsistent accuracy drops with increased hops and context length, even in\nfrontier models-revealing that sheer scale does not guarantee robust reasoning.\nOur failure mode analysis highlights common breakdowns, such as missed\nfinal-hop integration and long-range drift. NovelHopQA offers a controlled\ndiagnostic setting to stress-test multi-hop reasoning at scale.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u8bc4\u4f30\u5927\u6a21\u578b\u57286.4\u4e07-12.8\u4e07token\u53d9\u4e8b\u6587\u672c\u4e2d\u8fdb\u884c\u591a\u8df3\u63a8\u7406\u7684\u57fa\u51c6NovelHopQA\uff0c\u53d1\u73b0\u524d\u6cbf\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u548c\u591a\u8df3\u63a8\u7406\u65f6\u5b58\u5728\u51c6\u786e\u7387\u4e0b\u964d\u95ee\u9898", "motivation": "\u73b0\u6709\u57fa\u51c6\u5b64\u7acb\u6d4b\u8bd5\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u6216\u591a\u8df3\u63a8\u7406\uff0c\u7f3a\u4e4f\u5728\u81ea\u7136\u53d9\u4e8b\u573a\u666f\u4e2d\u7efc\u5408\u8003\u5bdf\u4e0a\u4e0b\u6587\u957f\u5ea6\u548c\u63a8\u7406\u6df1\u5ea6\u7684\u8bc4\u4f30\u5de5\u5177", "method": "\u57fa\u4e8e83\u90e8\u516c\u7248\u5c0f\u8bf4\u6784\u5efa\u94fe\u5f0fQA\u6570\u636e\u96c6\uff0c\u91c7\u7528\u5173\u952e\u8bcd\u5f15\u5bfc\u7684\u7ba1\u9053\u6784\u5efa\u57fa\u4e8e\u8fde\u8d2f\u6545\u4e8b\u60c5\u8282\u7684\u8df3\u6570\u5206\u79bb\u94fe\uff0c\u5e76\u901a\u8fc7\u4eba\u5de5\u9a8c\u8bc1\u5bf9\u9f50\u6027\u548c\u8df3\u6570\u6df1\u5ea6", "result": "\u6240\u6709\u524d\u6cbf\u6a21\u578b\uff08\u5305\u62ecGPT-4\uff09\u5728\u8df3\u6570\u589e\u52a0\u548c\u4e0a\u4e0b\u6587\u5ef6\u957f\u65f6\u5747\u51fa\u73b0\u51c6\u786e\u7387\u4e0b\u964d\uff0c\u663e\u793a\u5355\u7eaf\u6269\u5927\u89c4\u6a21\u65e0\u6cd5\u4fdd\u8bc1\u590d\u6742\u63a8\u7406\u80fd\u529b", "conclusion": "NovelHopQA\u4e3a\u5927\u89c4\u6a21\u591a\u8df3\u63a8\u7406\u63d0\u4f9b\u8bca\u65ad\u5de5\u5177\uff0c\u5931\u8d25\u6a21\u5f0f\u5206\u6790\u63ed\u793a\u4e86\u7ec8\u7aef\u8df3\u6574\u5408\u5931\u8d25\u548c\u957f\u7a0b\u6f02\u79fb\u7b49\u5173\u952e\u74f6\u9888"}}
{"id": "2506.02005", "pdf": "https://arxiv.org/pdf/2506.02005", "abs": "https://arxiv.org/abs/2506.02005", "authors": ["Timothy Do", "Pranav Saran", "Harshita Poojary", "Pranav Prabhu", "Sean O'Brien", "Vasu Sharma", "Kevin Zhu"], "title": "Pruning for Performance: Efficient Idiom and Metaphor Classification in Low-Resource Konkani Using mBERT", "categories": ["cs.CL"], "comment": "9 pages, 7 figures", "summary": "In this paper, we address the persistent challenges that figurative language\nexpressions pose for natural language processing (NLP) systems, particularly in\nlow-resource languages such as Konkani. We present a hybrid model that\nintegrates a pre-trained Multilingual BERT (mBERT) with a bidirectional LSTM\nand a linear classifier. This architecture is fine-tuned on a newly introduced\nannotated dataset for metaphor classification, developed as part of this work.\nTo improve the model's efficiency, we implement a gradient-based attention head\npruning strategy. For metaphor classification, the pruned model achieves an\naccuracy of 78%. We also applied our pruning approach to expand on an existing\nidiom classification task, achieving 83% accuracy. These results demonstrate\nthe effectiveness of attention head pruning for building efficient NLP tools in\nunderrepresented languages.", "AI": {"tldr": "\u7ed3\u5408mBERT\u548cBiLSTM\u7684\u6df7\u5408\u6a21\u578b\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u5934\u526a\u679d\u63d0\u5347\u79d1\u62c9\u5c3c\u8bed\u9690\u55bb\u5206\u7c7b\u6548\u7387\uff0878%\uff09\u548c\u6210\u8bed\u5206\u7c7b\uff0883%\uff09", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u79d1\u62c9\u5c3c\u8bed\uff09\u4e2d\u6bd4\u55bb\u8bed\u8a00\u5bf9NLP\u7cfb\u7edf\u7684\u5904\u7406\u6311\u6218", "method": "1. \u57fa\u4e8e\u9884\u8bad\u7ec3mBERT+\u53cc\u5411LSTM+\u7ebf\u6027\u5206\u7c7b\u5668\u7684\u6df7\u5408\u6a21\u578b\n2. \u5f15\u5165\u68af\u5ea6\u6ce8\u610f\u529b\u5934\u526a\u679d\u7b56\u7565\u4f18\u5316\u6548\u7387\n3. \u4f7f\u7528\u81ea\u5efa\u6807\u6ce8\u9690\u55bb\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03", "result": "\u9690\u55bb\u5206\u7c7b\u51c6\u786e\u738778%\uff0c\u6210\u8bed\u5206\u7c7b\u4efb\u52a1\u51c6\u786e\u738783%", "conclusion": "\u6ce8\u610f\u529b\u5934\u526a\u679d\u6280\u672f\u80fd\u6709\u6548\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00NLP\u5de5\u5177\u6548\u7387"}}
{"id": "2506.02034", "pdf": "https://arxiv.org/pdf/2506.02034", "abs": "https://arxiv.org/abs/2506.02034", "authors": ["Ignacio Arretche", "Mohammad Tanver Hossain", "Ramdas Tiwari", "Abbie Kim", "Mya G. Mills", "Connor D. Armstrong", "Jacob J. Lessard", "Sameh H. Tawfick", "Randy H. Ewoldt"], "title": "High-throughput viscometry via machine-learning from videos of inverted vials", "categories": ["cs.GR"], "comment": null, "summary": "Although the inverted vial test has been widely used as a qualitative method\nfor estimating fluid viscosity, quantitative rheological characterization has\nremained limited due to its complex, uncontrolled flow - driven by gravity,\nsurface tension, inertia, and initial conditions. Here, we present a computer\nvision (CV) viscometer that automates the inverted vial test and enables\nquantitative viscosity inference across nearly five orders of magnitude\n(0.01-1000 Pas), without requiring direct velocity field measurements. The\nsystem simultaneously inverts multiple vials and records videos of the evolving\nfluid, which are fed into a neural network that approximates the inverse\nfunction from visual features and known fluid density. Despite the complex,\nmulti-regime flow within the vial, our approach achieves relative errors below\n25%, improving to 15% for viscosities above 0.1 Pas. When tested on\nnon-Newtonian polymer solutions, the method reliably estimates zero-shear\nviscosity as long as viscoelastic or shear-thinning behaviors remain negligible\nwithin the flow regime. Moreover, high standard deviations in the inferred\nvalues may serve as a proxy for identifying fluids with strong non-Newtonian\nbehavior. The CV viscometer requires only one camera and one motor, is\ncontactless and low-cost, and can be easily integrated into high-throughput\nexperimental automated and manual workflows. Transcending traditional\ncharacterization paradigms, our method leverages uncontrolled flows and visual\nfeatures to achieve simplicity and scalability, enabling high-throughput\nviscosity inference that can meet the growing demand of data-driven material\nmodels while remaining accessible to lower resource environments.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u81ea\u52a8\u5316\u5012\u7f6e\u74f6\u7c98\u5ea6\u8ba1\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5206\u6790\u6d41\u4f53\u8fd0\u52a8\u89c6\u9891\u5b9e\u73b00.01-1000 Pas\u8303\u56f4\u7684\u5b9a\u91cf\u7c98\u5ea6\u6d4b\u91cf\uff0c\u5177\u5907\u4f4e\u6210\u672c\u3001\u6613\u96c6\u6210\u4f18\u52bf", "motivation": "\u4f20\u7edf\u5012\u7f6e\u74f6\u6d4b\u8bd5\u56e0\u590d\u6742\u4e0d\u53ef\u63a7\u6d41\u52a8\uff08\u91cd\u529b\u3001\u8868\u9762\u5f20\u529b\u3001\u60ef\u6027\u7b49\u591a\u56e0\u7d20\u8026\u5408\uff09\u4ec5\u80fd\u5b9a\u6027\u4f30\u8ba1\u7c98\u5ea6\uff0c\u73b0\u6709\u5b9a\u91cf\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u8bbe\u5907\u6216\u901f\u5ea6\u573a\u6d4b\u91cf\uff0c\u96be\u4ee5\u666e\u53ca\u5e94\u7528", "method": "\u642d\u5efa\u540c\u6b65\u5012\u7f6e\u591a\u4e2a\u8bd5\u7ba1\u7684\u81ea\u52a8\u5316\u7cfb\u7edf\uff0c\u91c7\u96c6\u6d41\u4f53\u8fd0\u52a8\u89c6\u9891\u8f93\u5165\u795e\u7ecf\u7f51\u7edc\uff0c\u7ed3\u5408\u5df2\u77e5\u6d41\u4f53\u5bc6\u5ea6\u5efa\u7acb\u89c6\u89c9\u7279\u5f81\u4e0e\u7c98\u5ea6\u7684\u9006\u5411\u6620\u5c04\u6a21\u578b", "result": "\u57280.01-1000 Pas\u8303\u56f4\u5185\u5b9e\u73b0\u5e73\u5747\u76f8\u5bf9\u8bef\u5dee<25%\uff08>0.1 Pas\u65f6\u8bef\u5dee<15%\uff09\uff0c\u80fd\u6709\u6548\u4f30\u8ba1\u975e\u725b\u987f\u6d41\u4f53\u96f6\u526a\u5207\u7c98\u5ea6\uff08\u5f53\u7c98\u5f39\u6027/\u526a\u5207\u7a00\u5316\u6548\u5e94\u53ef\u5ffd\u7565\u65f6\uff09\uff0c\u9ad8\u6807\u51c6\u5dee\u53ef\u6307\u793a\u5f3a\u975e\u725b\u987f\u884c\u4e3a", "conclusion": "\u4ec5\u9700\u5355\u6444\u50cf\u5934\u548c\u7535\u673a\u7684\u975e\u63a5\u89e6\u5f0f\u7cfb\u7edf\u7a81\u7834\u4f20\u7edf\u8868\u5f81\u8303\u5f0f\uff0c\u5229\u7528\u4e0d\u53ef\u63a7\u6d41\u52a8\u7684\u89c6\u89c9\u7279\u5f81\u5b9e\u73b0\u9ad8\u901a\u91cf\u7c98\u5ea6\u6d4b\u91cf\uff0c\u517c\u987e\u6570\u636e\u9a71\u52a8\u9700\u6c42\u4e0e\u8bbe\u5907\u53ef\u53ca\u6027\uff0c\u7279\u522b\u9002\u5408\u8d44\u6e90\u6709\u9650\u573a\u666f"}}
{"id": "2506.02018", "pdf": "https://arxiv.org/pdf/2506.02018", "abs": "https://arxiv.org/abs/2506.02018", "authors": ["Christopher Lee L\u00fcbbers"], "title": "Enhancing Paraphrase Type Generation: The Impact of DPO and RLHF Evaluated with Human-Ranked Data", "categories": ["cs.CL", "I.2.7"], "comment": "21 pages, 11 figures. Master's thesis, University of Goettingen,\n  December 2025. Code: https://github.com/cluebbers/dpo-rlhf-paraphrase-types.\n  Models:\n  https://huggingface.co/collections/cluebbers/enhancing-paraphrase-type-generation-673ca8d75dfe2ce962a48ac0", "summary": "Paraphrasing re-expresses meaning to enhance applications like text\nsimplification, machine translation, and question-answering. Specific\nparaphrase types facilitate accurate semantic analysis and robust language\nmodels. However, existing paraphrase-type generation methods often misalign\nwith human preferences due to reliance on automated metrics and limited\nhuman-annotated training data, obscuring crucial aspects of semantic fidelity\nand linguistic transformations.\n  This study addresses this gap by leveraging a human-ranked paraphrase-type\ndataset and integrating Direct Preference Optimization (DPO) to align model\noutputs directly with human judgments. DPO-based training increases\nparaphrase-type generation accuracy by 3 percentage points over a supervised\nbaseline and raises human preference ratings by 7 percentage points. A newly\ncreated human-annotated dataset supports more rigorous future evaluations.\nAdditionally, a paraphrase-type detection model achieves F1 scores of 0.91 for\naddition/deletion, 0.78 for same polarity substitution, and 0.70 for\npunctuation changes.\n  These findings demonstrate that preference data and DPO training produce more\nreliable, semantically accurate paraphrases, enabling downstream applications\nsuch as improved summarization and more robust question-answering. The PTD\nmodel surpasses automated metrics and provides a more reliable framework for\nevaluating paraphrase quality, advancing paraphrase-type research toward\nricher, user-aligned language generation and establishing a stronger foundation\nfor future evaluations grounded in human-centric criteria.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u4eba\u7c7b\u504f\u597d\u6570\u636e\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5c06\u6539\u5199\u7c7b\u578b\u751f\u6210\u51c6\u786e\u7387\u63d0\u53473%\uff0c\u4eba\u7c7b\u504f\u597d\u8bc4\u5206\u63d0\u9ad87%\uff0c\u5e76\u6784\u5efa\u65b0\u6570\u636e\u96c6\u652f\u6301\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u6539\u5199\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u81ea\u52a8\u5316\u6307\u6807\u4e14\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u4eba\u5de5\u6807\u6ce8\u6570\u636e\uff0c\u5bfc\u81f4\u751f\u6210\u7ed3\u679c\u4e0e\u4eba\u7c7b\u504f\u597d\u9519\u4f4d\uff0c\u5f71\u54cd\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u8bed\u8a00\u8f6c\u6362\u6548\u679c\u3002", "method": "\u5229\u7528\u4eba\u5de5\u6392\u5e8f\u7684\u6539\u5199\u7c7b\u578b\u6570\u636e\u96c6\uff0c\u91c7\u7528DPO\u8bad\u7ec3\u6846\u67b6\u76f4\u63a5\u5bf9\u9f50\u4eba\u7c7b\u5224\u65ad\uff0c\u540c\u65f6\u6784\u5efa\u4eba\u5de5\u6807\u6ce8\u68c0\u6d4b\u6a21\u578b\uff08F1\u503c\u8fbe0.91/0.78/0.70\uff09\u3002", "result": "DPO\u8bad\u7ec3\u76f8\u8f83\u57fa\u7ebf\u6a21\u578b\u63d0\u53473%\u51c6\u786e\u7387\uff0c\u4eba\u7c7b\u504f\u597d\u7387\u589e\u52a07%\u3002\u68c0\u6d4b\u6a21\u578b\u5728\u4e0d\u540c\u6539\u5199\u7c7b\u578b\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u57fa\u4e8e\u4eba\u7c7b\u504f\u597d\u6570\u636e\u7684\u8bad\u7ec3\u53ef\u751f\u6210\u66f4\u53ef\u9760\u3001\u8bed\u4e49\u7cbe\u51c6\u7684\u6539\u5199\uff0c\u63a8\u52a8\u6458\u8981\u751f\u6210\u3001\u95ee\u7b54\u7cfb\u7edf\u7b49\u5e94\u7528\uff0c\u4e3a\u672a\u6765\u4eba\u672c\u5316\u8bc4\u4f30\u5efa\u7acb\u65b0\u57fa\u51c6\u3002"}}
{"id": "2506.02219", "pdf": "https://arxiv.org/pdf/2506.02219", "abs": "https://arxiv.org/abs/2506.02219", "authors": ["Abhishek Madan", "Nicholas Sharp", "Francis Williams", "Ken Museth", "David I. W. Levin"], "title": "Stochastic Barnes-Hut Approximation for Fast Summation on the GPU", "categories": ["cs.GR"], "comment": "11 pages, 9 figures. To appear in ACM SIGGRAPH 2025", "summary": "We present a novel stochastic version of the Barnes-Hut approximation.\nRegarding the level-of-detail (LOD) family of approximations as control\nvariates, we construct an unbiased estimator of the kernel sum being\napproximated. Through several examples in graphics applications such as winding\nnumber computation and smooth distance evaluation, we demonstrate that our\nmethod is well-suited for GPU computation, capable of outperforming a\nGPU-optimized implementation of the deterministic Barnes-Hut approximation by\nachieving equal median error in up to 9.4x less time.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u63a7\u5236\u53d8\u91cf\u6cd5\u7684\u968f\u673aBarnes-Hut\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u5728GPU\u5b9e\u73b0\u4e2d\u6bd4\u786e\u5b9a\u6027\u65b9\u6cd5\u5feb9.4\u500d\u4e14\u4fdd\u6301\u76f8\u540c\u7cbe\u5ea6", "motivation": "\u63d0\u5347\u4f20\u7edfBarnes-Hut\u8fd1\u4f3c\u5728GPU\u5e76\u884c\u8ba1\u7b97\u73af\u5883\u4e2d\u7684\u6548\u7387\uff0c\u901a\u8fc7\u5f15\u5165\u968f\u673a\u6027\u5b9e\u73b0\u52a0\u901f\u800c\u4e0d\u635f\u5931\u7cbe\u5ea6", "method": "\u5c06\u786e\u5b9a\u6027Level-of-Detail\u8fd1\u4f3c\u4f5c\u4e3a\u63a7\u5236\u53d8\u91cf\uff0c\u6784\u9020\u65e0\u504f\u4f30\u8ba1\u5668\uff0c\u7ed3\u5408GPU\u5e76\u884c\u67b6\u6784\u4f18\u5316\u8ba1\u7b97\u6d41\u7a0b", "result": "\u5728\u7ed5\u6570\u8ba1\u7b97\u3001\u5e73\u6ed1\u8ddd\u79bb\u8bc4\u4f30\u7b49\u56fe\u5f62\u5b66\u4efb\u52a1\u4e2d\uff0c\u5b9e\u73b0\u6bd4GPU\u4f18\u5316\u7248\u786e\u5b9a\u6027\u65b9\u6cd5\u5feb9.4\u500d\u7684\u7b49\u6548\u4e2d\u4f4d\u8bef\u5dee\u8ba1\u7b97", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u56fe\u5f62\u5b66\u4e2d\u7684\u5feb\u901f\u6838\u51fd\u6570\u8ba1\u7b97\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u5408GPU\u52a0\u901f\u7684\u8fd1\u4f3c\u8ba1\u7b97\u573a\u666f"}}
{"id": "2506.02019", "pdf": "https://arxiv.org/pdf/2506.02019", "abs": "https://arxiv.org/abs/2506.02019", "authors": ["E Fan", "Weizong Wang", "Tianhan Zhang"], "title": "ChatCFD: an End-to-End CFD Agent with Domain-specific Structured Thinking", "categories": ["cs.CL"], "comment": "19 pages, 8 figures", "summary": "Computational Fluid Dynamics (CFD) is essential for scientific and\nengineering advancements but is limited by operational complexity and the need\nfor extensive expertise. This paper presents ChatCFD, a large language\nmodel-driven pipeline that automates CFD workflows within the OpenFOAM\nframework. It enables users to configure and execute complex simulations from\nnatural language prompts or published literature with minimal expertise. The\ninnovation is its structured approach to database construction, configuration\nvalidation, and error reflection, integrating CFD and OpenFOAM knowledge with\ngeneral language models to improve accuracy and adaptability. Validation shows\nChatCFD can autonomously reproduce published CFD results, handling complex,\nunseen configurations beyond basic examples, a task challenging for general\nlanguage models.", "AI": {"tldr": "\u63d0\u51fa\u4e86ChatCFD\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316CFD\u5de5\u4f5c\u6d41\u7ba1\u9053\uff0c\u53ef\u7b80\u5316\u590d\u6742\u4eff\u771f\u914d\u7f6e\u548c\u6267\u884c\u3002", "motivation": "\u4f20\u7edfCFD\u5b58\u5728\u64cd\u4f5c\u590d\u6742\u3001\u4e13\u4e1a\u95e8\u69db\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u667a\u80fd\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5728OpenFOAM\u6846\u67b6\u5185\u6784\u5efa\u7ed3\u6784\u5316\u6d41\u7a0b\uff0c\u6574\u5408\u9886\u57df\u77e5\u8bc6\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u914d\u7f6e\u9a8c\u8bc1\u548c\u9519\u8bef\u53cd\u5c04\u673a\u5236\u63d0\u5347\u51c6\u786e\u6027", "result": "\u9a8c\u8bc1\u663e\u793aChatCFD\u80fd\u590d\u73b0\u5df2\u53d1\u8868CFD\u6210\u679c\uff0c\u5904\u7406\u57fa\u7840\u6a21\u578b\u96be\u4ee5\u89e3\u51b3\u7684\u590d\u6742\u65b0\u573a\u666f", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u964d\u4f4e\u4e86CFD\u4f7f\u7528\u95e8\u69db\uff0c\u5c55\u793a\u4e86\u9886\u57df\u4e13\u7528\u8bed\u8a00\u6a21\u578b\u5728\u5de5\u7a0b\u4eff\u771f\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c"}}
{"id": "2506.02620", "pdf": "https://arxiv.org/pdf/2506.02620", "abs": "https://arxiv.org/abs/2506.02620", "authors": ["Dongyu Yan", "Leyi Wu", "Jiantao Lin", "Luozhou Wang", "Tianshuo Xu", "Zhifei Chen", "Zhen Yang", "Lie Xu", "Shunsi Zhang", "Yingcong Chen"], "title": "FlexPainter: Flexible and Multi-View Consistent Texture Generation", "categories": ["cs.GR", "cs.CV"], "comment": "11 pages, 10 figures in main paper, 10 pages, 12 figures in\n  supplementary", "summary": "Texture map production is an important part of 3D modeling and determines the\nrendering quality. Recently, diffusion-based methods have opened a new way for\ntexture generation. However, restricted control flexibility and limited prompt\nmodalities may prevent creators from producing desired results. Furthermore,\ninconsistencies between generated multi-view images often lead to poor texture\ngeneration quality. To address these issues, we introduce \\textbf{FlexPainter},\na novel texture generation pipeline that enables flexible multi-modal\nconditional guidance and achieves highly consistent texture generation. A\nshared conditional embedding space is constructed to perform flexible\naggregation between different input modalities. Utilizing such embedding space,\nwe present an image-based CFG method to decompose structural and style\ninformation, achieving reference image-based stylization. Leveraging the 3D\nknowledge within the image diffusion prior, we first generate multi-view images\nsimultaneously using a grid representation to enhance global understanding.\nMeanwhile, we propose a view synchronization and adaptive weighting module\nduring diffusion sampling to further ensure local consistency. Finally, a\n3D-aware texture completion model combined with a texture enhancement model is\nused to generate seamless, high-resolution texture maps. Comprehensive\nexperiments demonstrate that our framework significantly outperforms\nstate-of-the-art methods in both flexibility and generation quality.", "AI": {"tldr": "FlexPainter\u901a\u8fc7\u591a\u6a21\u6001\u6761\u4ef6\u5f15\u5bfc\u548c\u89c6\u56fe\u540c\u6b65\u6280\u672f\uff0c\u663e\u8457\u63d0\u53473D\u7eb9\u7406\u751f\u6210\u7684\u7075\u6d3b\u6027\u548c\u4e00\u81f4\u6027\u8d28\u91cf", "motivation": "\u73b0\u6709\u6269\u6563\u65b9\u6cd5\u5b58\u5728\u63a7\u5236\u7075\u6d3b\u6027\u4e0d\u8db3\u3001\u63d0\u793a\u6a21\u6001\u6709\u9650\u3001\u591a\u89c6\u89d2\u751f\u6210\u4e0d\u4e00\u81f4\u7b49\u95ee\u9898\uff0c\u5236\u7ea6\u7eb9\u7406\u751f\u6210\u8d28\u91cf", "method": "1.\u6784\u5efa\u5171\u4eab\u6761\u4ef6\u5d4c\u5165\u7a7a\u95f4\u5b9e\u73b0\u591a\u6a21\u6001\u5f15\u5bfc 2.\u56fe\u50cfCFG\u5206\u89e3\u7ed3\u6784\u4e0e\u98ce\u683c\u4fe1\u606f 3.\u7f51\u683c\u8868\u793a\u540c\u6b65\u751f\u6210\u591a\u89c6\u89d2\u56fe\u50cf 4.\u89c6\u56fe\u540c\u6b65\u4e0e\u81ea\u9002\u5e94\u52a0\u6743\u6a21\u5757 5.3D\u611f\u77e5\u7eb9\u7406\u8865\u5168\u4e0e\u589e\u5f3a\u6a21\u578b", "result": "\u5b9e\u9a8c\u8868\u660e\u6846\u67b6\u5728\u7075\u6d3b\u6027\u548c\u751f\u6210\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5", "conclusion": "FlexPainter\u4e3a\u9ad8\u8d28\u91cf\u9ad8\u4e00\u81f4\u6027\u76843D\u7eb9\u7406\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.02037", "pdf": "https://arxiv.org/pdf/2506.02037", "abs": "https://arxiv.org/abs/2506.02037", "authors": ["Feng Wang", "Yiding Sun", "Jiaxin Mao", "Wei Xue", "Danqing Xu"], "title": "FinS-Pilot: A Benchmark for Online Financial System", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious professional domains, with their performance typically evaluated\nthrough standardized benchmarks. However, the development of financial RAG\nbenchmarks has been constrained by data confidentiality issues and the lack of\ndynamic data integration. To address this issue, we introduces FinS-Pilot, a\nnovel benchmark for evaluating RAG systems in online financial applications.\nConstructed from real-world financial assistant interactions, our benchmark\nincorporates both real-time API data and structured text sources, organized\nthrough an intent classification framework covering critical financial domains\nsuch as equity analysis and macroeconomic forecasting. The benchmark enables\ncomprehensive evaluation of financial assistants' capabilities in handling both\nstatic knowledge and time-sensitive market information. Through systematic\nexperiments with multiple Chinese leading LLMs, we demonstrate FinS-Pilot's\neffectiveness in identifying models suitable for financial applications while\naddressing the current gap in specialized evaluation tools for the financial\ndomain. Our work contributes both a practical evaluation framework and a\ncurated dataset to advance research in financial NLP systems. The code and\ndataset are accessible on\nGitHub\\footnote{https://github.com/PhealenWang/financial\\_rag\\_benchmark}.", "AI": {"tldr": "\u63d0\u51faFinS-Pilot\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u89e3\u51b3\u91d1\u878d\u9886\u57dfRAG\u8bc4\u4f30\u4e2d\u6570\u636e\u52a8\u6001\u6574\u5408\u4e0e\u5b9e\u65f6\u6027\u6311\u6218\uff0c\u901a\u8fc7\u771f\u5b9e\u573a\u666f\u5b9e\u9a8c\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u6027", "motivation": "\u73b0\u6709\u91d1\u878dRAG\u57fa\u51c6\u53d7\u9650\u4e8e\u6570\u636e\u4fdd\u5bc6\u6027\u548c\u7f3a\u4e4f\u52a8\u6001\u6570\u636e\u6574\u5408\uff0c\u96be\u4ee5\u6709\u6548\u8bc4\u4f30\u91d1\u878dNLP\u7cfb\u7edf\u7684\u5b9e\u65f6\u4fe1\u606f\u5904\u7406\u80fd\u529b", "method": "\u57fa\u4e8e\u771f\u5b9e\u91d1\u878d\u52a9\u624b\u4ea4\u4e92\u6784\u5efa\u57fa\u51c6\uff0c\u6574\u5408\u5b9e\u65f6API+\u7ed3\u6784\u5316\u6587\u672c\uff0c\u901a\u8fc7\u610f\u56fe\u5206\u7c7b\u6846\u67b6\u8986\u76d6\u80a1\u6743\u5206\u6790/\u5b8f\u89c2\u7ecf\u6d4e\u9884\u6d4b\u7b49\u6838\u5fc3\u573a\u666f\uff0c\u7cfb\u7edf\u6d4b\u8bd5\u591a\u6b3e\u4e2d\u6587\u9886\u5148LLM", "result": "\u9a8c\u8bc1\u6846\u67b6\u53ef\u6709\u6548\u8bc6\u522b\u9002\u7528\u91d1\u878d\u573a\u666f\u7684\u6a21\u578b\uff0c\u586b\u8865\u91d1\u878d\u9886\u57df\u4e13\u7528\u8bc4\u4f30\u5de5\u5177\u7a7a\u767d\uff0c\u63ed\u793a\u73b0\u6709\u6a21\u578b\u5904\u7406\u65f6\u6548\u6027\u5e02\u573a\u4fe1\u606f\u7684\u4e0d\u8db3", "conclusion": "\u8d21\u732e\u91d1\u878dNLP\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\u4e0e\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u4fc3\u8fdb\u9886\u57df\u53d1\u5c55\uff0c\u516c\u5f00\u4ee3\u7801\u4e0e\u6570\u636e\u96c6\u52a9\u529b\u540e\u7eed\u7814\u7a76"}}
{"id": "2506.02774", "pdf": "https://arxiv.org/pdf/2506.02774", "abs": "https://arxiv.org/abs/2506.02774", "authors": ["Zheng Liu", "He Zhu", "Xinyang Li", "Yirun Wang", "Yujiao Shi", "Wei Li", "Jingwen Leng", "Minyi Guo", "Yu Feng"], "title": "Voyager: Real-Time Splatting City-Scale 3D Gaussians on Your Phone", "categories": ["cs.GR"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) is an emerging technique for photorealistic 3D\nscene rendering. However, rendering city-scale 3DGS scenes on mobile devices,\ne.g., your smartphones, remains a significant challenge due to the limited\nresources on mobile devices. A natural solution is to offload computation to\nthe cloud; however, naively streaming rendered frames from the cloud to the\nclient introduces high latency and requires bandwidth far beyond the capacity\nof current wireless networks.\n  In this paper, we propose an effective solution to enable city-scale 3DGS\nrendering on mobile devices. Our key insight is that, under normal user motion,\nthe number of newly visible Gaussians per second remains roughly constant.\nLeveraging this, we stream only the necessary Gaussians to the client.\nSpecifically, on the cloud side, we propose asynchronous level-of-detail search\nto identify the necessary Gaussians for the client. On the client side, we\naccelerate rendering via a lookup table-based rasterization. Combined with\nholistic runtime optimizations, our system can deliver low-latency, city-scale\n3DGS rendering on mobile devices. Compared to existing solutions, Voyager\nachieves over 100$\\times$ reduction on data transfer and up to 8.9$\\times$\nspeedup while retaining comparable rendering quality.", "AI": {"tldr": "\u63d0\u51fa\u79fb\u52a8\u7aef\u57ce\u5e02\u7ea73D\u9ad8\u65af\u6cfc\u6e85\u6e32\u67d3\u65b9\u6848Voyager\uff0c\u901a\u8fc7\u4e91\u7aef\u5f02\u6b65LOD\u641c\u7d22\u4e0e\u5ba2\u6237\u7aef\u67e5\u627e\u8868\u52a0\u901f\uff0c\u5b9e\u73b0\u6570\u636e\u4f20\u8f93\u964d\u4f4e100\u500d+\u3001\u6e32\u67d3\u901f\u5ea6\u63d0\u53478.9\u500d", "motivation": "\u79fb\u52a8\u8bbe\u5907\u6e32\u67d3\u57ce\u5e02\u7ea73DGS\u573a\u666f\u65f6\u5b58\u5728\u8d44\u6e90\u9650\u5236\uff0c\u4f20\u7edf\u4e91\u7aef\u5e27\u6d41\u65b9\u6848\u5e26\u5bbd\u9700\u6c42\u5927\u4e14\u5ef6\u8fdf\u9ad8", "method": "\u4e91\u7aef\u91c7\u7528\u5f02\u6b65LOD\u641c\u7d22\u52a8\u6001\u8bc6\u522b\u5fc5\u8981\u9ad8\u65af\u6570\u636e\uff0c\u5ba2\u6237\u7aef\u901a\u8fc7\u67e5\u627e\u8868\u52a0\u901f\u5149\u6805\u5316\uff0c\u914d\u5408\u8fd0\u884c\u65f6\u6574\u4f53\u4f18\u5316", "result": "\u6570\u636e\u4f20\u8f93\u51cf\u5c11\u8d85100\u500d\uff0c\u901f\u5ea6\u63d0\u5347\u8fbe8.9\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u540c\u7b49\u6e32\u67d3\u8d28\u91cf", "conclusion": "Voyager\u7cfb\u7edf\u6210\u529f\u5b9e\u73b0\u79fb\u52a8\u7aef\u4f4e\u5ef6\u8fdf\u57ce\u5e02\u7ea73DGS\u6e32\u67d3\uff0c\u7a81\u7834\u73b0\u6709\u65b9\u6848\u7684\u5e26\u5bbd\u548c\u7b97\u529b\u9650\u5236"}}
{"id": "2506.02041", "pdf": "https://arxiv.org/pdf/2506.02041", "abs": "https://arxiv.org/abs/2506.02041", "authors": ["Duzhen Zhang", "Yong Ren", "Zhong-Zhi Li", "Yahan Yu", "Jiahua Dong", "Chenxing Li", "Zhilong Ji", "Jinfeng Bai"], "title": "Enhancing Multimodal Continual Instruction Tuning with BranchLoRA", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL2025 Main Conference", "summary": "Multimodal Continual Instruction Tuning (MCIT) aims to finetune Multimodal\nLarge Language Models (MLLMs) to continually align with human intent across\nsequential tasks. Existing approaches often rely on the Mixture-of-Experts\n(MoE) LoRA framework to preserve previous instruction alignments. However,\nthese methods are prone to Catastrophic Forgetting (CF), as they aggregate all\nLoRA blocks via simple summation, which compromises performance over time. In\nthis paper, we identify a critical parameter inefficiency in the MoELoRA\nframework within the MCIT context. Based on this insight, we propose\nBranchLoRA, an asymmetric framework to enhance both efficiency and performance.\nTo mitigate CF, we introduce a flexible tuning-freezing mechanism within\nBranchLoRA, enabling branches to specialize in intra-task knowledge while\nfostering inter-task collaboration. Moreover, we incrementally incorporate\ntask-specific routers to ensure an optimal branch distribution over time,\nrather than favoring the most recent task. To streamline inference, we\nintroduce a task selector that automatically routes test inputs to the\nappropriate router without requiring task identity. Extensive experiments on\nthe latest MCIT benchmark demonstrate that BranchLoRA significantly outperforms\nMoELoRA and maintains its superiority across various MLLM sizes.", "AI": {"tldr": "\u63d0\u51faBranchLoRA\u6846\u67b6\u89e3\u51b3\u591a\u6a21\u6001\u6301\u7eed\u6307\u4ee4\u8c03\u4f18\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898", "motivation": "\u73b0\u6709MoELoRA\u65b9\u6cd5\u901a\u8fc7\u7b80\u5355\u805a\u5408LoRA\u5757\u5bfc\u81f4\u53c2\u6570\u6548\u7387\u4f4e\u4e0b\u548c\u6027\u80fd\u8870\u9000", "method": "\u91c7\u7528\u5206\u652f\u51bb\u7ed3\u673a\u5236+\u4efb\u52a1\u4e13\u7528\u8def\u7531\u5668+\u81ea\u52a8\u5316\u4efb\u52a1\u9009\u62e9\u5668\u7684\u975e\u5bf9\u79f0\u6846\u67b6", "result": "\u5728\u6700\u65b0MCIT\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8eMoELoRA\uff0c\u4e14\u5728\u4e0d\u540c\u89c4\u6a21MLLM\u4e2d\u4fdd\u6301\u4f18\u52bf", "conclusion": "BranchLoRA\u901a\u8fc7\u7ed3\u6784\u521b\u65b0\u5b9e\u73b0\u4e86\u6301\u7eed\u5b66\u4e60\u6548\u7387\u4e0e\u6027\u80fd\u7684\u540c\u6b65\u63d0\u5347"}}
{"id": "2506.02794", "pdf": "https://arxiv.org/pdf/2506.02794", "abs": "https://arxiv.org/abs/2506.02794", "authors": ["Mijeong Kim", "Gunhee Kim", "Jungyoon Choi", "Wonjae Roh", "Bohyung Han"], "title": "PhysGaia: A Physics-Aware Dataset of Multi-Body Interactions for Dynamic Novel View Synthesis", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Project page: http://cvlab.snu.ac.kr/research/PhysGaia, Data:\n  https://huggingface.co/datasets/mijeongkim/PhysGaia/tree/main", "summary": "We introduce PhysGaia, a novel physics-aware dataset specifically designed\nfor Dynamic Novel View Synthesis (DyNVS), encompassing both structured objects\nand unstructured physical phenomena. Unlike existing datasets that primarily\nfocus on photorealistic reconstruction, PhysGaia is created to actively support\nphysics-aware dynamic scene modeling. Our dataset provides complex dynamic\nscenarios with rich interactions among multiple objects, where they\nrealistically collide with each other and exchange forces. Furthermore, it\ncontains a diverse range of physical materials, such as liquid, gas,\nviscoelastic substance, and textile, which moves beyond the rigid bodies\nprevalent in existing datasets. All scenes in PhysGaia are faithfully generated\nto strictly adhere to physical laws, leveraging carefully selected\nmaterial-specific physics solvers. To enable quantitative evaluation of\nphysical modeling, our dataset provides essential ground-truth information,\nincluding 3D particle trajectories and physics parameters, e.g., viscosity. To\nfacilitate research adoption, we also provide essential integration pipelines\nfor using state-of-the-art DyNVS models with our dataset and report their\nresults. By addressing the critical lack of datasets for physics-aware\nmodeling, PhysGaia will significantly advance research in dynamic view\nsynthesis, physics-based scene understanding, and deep learning models\nintegrated with physical simulation -- ultimately enabling more faithful\nreconstruction and interpretation of complex dynamic scenes. Our datasets and\ncodes are available in the project website,\nhttp://cvlab.snu.ac.kr/research/PhysGaia.", "AI": {"tldr": "PhysGaia\u662f\u9996\u4e2a\u7269\u7406\u611f\u77e5\u7684\u52a8\u6001\u89c6\u89d2\u5408\u6210\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u7269\u4f53\u4ea4\u4e92\u548c\u591a\u6837\u6750\u8d28\uff0c\u4e25\u683c\u9075\u5faa\u7269\u7406\u5b9a\u5f8b\u5e76\u63d0\u4f9b3D\u7c92\u5b50\u8f68\u8ff9\u7b49\u771f\u503c\u6570\u636e\uff0c\u63a8\u52a8\u52a8\u6001\u573a\u666f\u5efa\u6a21\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u903c\u771f\u91cd\u5efa\uff0c\u7f3a\u4e4f\u7269\u7406\u4ea4\u4e92\u5efa\u6a21\u652f\u6301\u3002PhysGaia\u65e8\u5728\u586b\u8865\u7269\u7406\u611f\u77e5\u52a8\u6001\u573a\u666f\u5efa\u6a21\u6570\u636e\u96c6\u7684\u7a7a\u767d\u3002", "method": "1. \u521b\u5efa\u5305\u542b\u590d\u6742\u7269\u4f53\u4ea4\u4e92\uff08\u78b0\u649e\u3001\u529b\u4ea4\u6362\uff09\u548c\u591a\u79cd\u6750\u8d28\uff08\u6db2\u4f53\u3001\u6c14\u4f53\u3001\u7c98\u5f39\u6027\u7269\u8d28\u3001\u7eba\u7ec7\u54c1\uff09\u7684\u52a8\u6001\u573a\u666f\n2. \u4f7f\u7528\u7279\u5b9a\u7269\u7406\u6c42\u89e3\u5668\u786e\u4fdd\u4e25\u683c\u7269\u7406\u5408\u89c4\u6027\n3. \u63d0\u4f9b3D\u7c92\u5b50\u8f68\u8ff9\u3001\u7c98\u5ea6\u7b49\u7269\u7406\u53c2\u6570\u771f\u503c\n4. \u96c6\u6210\u4e3b\u6d41DyNVS\u6a21\u578b\u7684\u8bc4\u4f30\u6d41\u7a0b", "result": "\u53d1\u5e03\u5305\u542b\u7269\u7406\u771f\u503c\u7684\u6570\u636e\u96c6\u53ca\u8bc4\u4f30\u7ba1\u7ebf\uff0c\u652f\u6301\uff1a\n- \u7269\u7406\u53c2\u6570\u9006\u5411\u4f30\u8ba1\uff08\u5982\u7c98\u5ea6\uff09\n- \u52a8\u6001\u89c6\u89d2\u5408\u6210\u6a21\u578b\u8bc4\u4f30\n- \u7269\u7406\u6a21\u62df\u4e0e\u6df1\u5ea6\u5b66\u4e60\u878d\u5408\u7814\u7a76", "conclusion": "PhysGaia\u901a\u8fc7\u7269\u7406\u5408\u89c4\u6570\u636e\u96c6\u663e\u8457\u63a8\u8fdb\u52a8\u6001\u89c6\u89d2\u5408\u6210\u3001\u7269\u7406\u573a\u666f\u7406\u89e3\u548c\u7269\u7406\u589e\u5f3a\u6df1\u5ea6\u5b66\u4e60\u7684\u53d1\u5c55\uff0c\u5b9e\u73b0\u590d\u6742\u52a8\u6001\u573a\u666f\u7684\u7cbe\u51c6\u91cd\u5efa\u4e0e\u89e3\u91ca\u3002\u9879\u76ee\u7f51\u7ad9\u516c\u5f00\u6570\u636e\u96c6\u548c\u4ee3\u7801\u3002"}}
{"id": "2506.02058", "pdf": "https://arxiv.org/pdf/2506.02058", "abs": "https://arxiv.org/abs/2506.02058", "authors": ["Xiang Li", "Jiayi Xin", "Qi Long", "Weijie J. Su"], "title": "Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.AP", "stat.ME"], "comment": null, "summary": "Accurate evaluation of large language models (LLMs) is crucial for\nunderstanding their capabilities and guiding their development. However,\ncurrent evaluations often inconsistently reflect the actual capacities of these\nmodels. In this paper, we demonstrate that one of many contributing factors to\nthis \\textit{evaluation crisis} is the oversight of unseen knowledge --\ninformation encoded by LLMs but not directly observed or not yet observed\nduring evaluations. We introduce KnowSum, a statistical framework designed to\nprovide a more comprehensive assessment by quantifying the unseen knowledge for\na class of evaluation tasks. KnowSum estimates the unobserved portion by\nextrapolating from the appearance frequencies of observed knowledge instances.\nWe demonstrate the effectiveness and utility of KnowSum across three critical\napplications: estimating total knowledge, evaluating information retrieval\neffectiveness, and measuring output diversity. Our experiments reveal that a\nsubstantial volume of knowledge is omitted when relying solely on observed LLM\nperformance. Importantly, KnowSum yields significantly different comparative\nrankings for several common LLMs based on their internal knowledge.", "AI": {"tldr": "\u63d0\u51faKnowSum\u7edf\u8ba1\u6846\u67b6\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u4e2d\u672a\u89c2\u5bdf\u77e5\u8bc6\u91cf\u5316\u95ee\u9898", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u56e0\u5ffd\u89c6\u6a21\u578b\u7f16\u7801\u4f46\u672a\u89c2\u5bdf\u5230\u7684\u77e5\u8bc6\uff08unseen knowledge\uff09\u5bfc\u81f4\u8bc4\u4f30\u5931\u771f", "method": "\u57fa\u4e8e\u89c2\u5bdf\u77e5\u8bc6\u5b9e\u4f8b\u7684\u51fa\u73b0\u9891\u7387\u8fdb\u884c\u5916\u63a8\uff0c\u6784\u5efa\u4e09\u9636\u6bb5\u7edf\u8ba1\u4f30\u8ba1\u6846\u67b6\uff08\u603b\u77e5\u8bc6\u91cf\u4f30\u8ba1\u3001\u4fe1\u606f\u68c0\u7d22\u8bc4\u4f30\u3001\u8f93\u51fa\u591a\u6837\u6027\u6d4b\u91cf\uff09", "result": "\u5b9e\u9a8c\u663e\u793a\u4e3b\u6d41LLMs\u57fa\u4e8e\u5185\u90e8\u77e5\u8bc6\u7684\u76f8\u5bf9\u6392\u540d\u53d1\u751f\u663e\u8457\u53d8\u5316\uff0c\u5e38\u89c4\u8bc4\u4f30\u9057\u6f0f\u5927\u91cf\u6f5c\u5728\u77e5\u8bc6", "conclusion": "KnowSum\u4e3aLLM\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u89c6\u89d2\uff0c\u63ed\u793a\u4e86\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u7684\u7cfb\u7edf\u6027\u504f\u5dee"}}
{"id": "2506.02895", "pdf": "https://arxiv.org/pdf/2506.02895", "abs": "https://arxiv.org/abs/2506.02895", "authors": ["Ahmad AlMughrabi", "Umair Haroon", "Ricardo Marques", "Petia Radeva"], "title": "VolTex: Food Volume Estimation using Text-Guided Segmentation and Neural Surface Reconstruction", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Accurate food volume estimation is crucial for dietary monitoring, medical\nnutrition management, and food intake analysis. Existing 3D Food Volume\nestimation methods accurately compute the food volume but lack for food\nportions selection. We present VolTex, a framework that improves \\change{the\nfood object selection} in food volume estimation. Allowing users to specify a\ntarget food item via text input to be segmented, our method enables the precise\nselection of specific food objects in real-world scenes. The segmented object\nis then reconstructed using the Neural Surface Reconstruction method to\ngenerate high-fidelity 3D meshes for volume computation. Extensive evaluations\non the MetaFood3D dataset demonstrate the effectiveness of our approach in\nisolating and reconstructing food items for accurate volume estimation. The\nsource code is accessible at https://github.com/GCVCG/VolTex.", "AI": {"tldr": "\u63d0\u51faVolTex\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c\u6307\u4ee4\u5b9e\u73b0\u7cbe\u51c6\u98df\u7269\u5206\u5272+\u795e\u7ecf\u8868\u9762\u91cd\u5efa\uff0c\u89e3\u51b3\u73b0\u6709\u4e09\u7ef4\u98df\u7269\u4f53\u79ef\u6d4b\u7b97\u65b9\u6cd5\u4e2d\u98df\u7269\u90e8\u4f4d\u9009\u62e9\u4e0d\u8db3\u7684\u95ee\u9898", "motivation": "\u73b0\u6709\u4e09\u7ef4\u98df\u7269\u4f53\u79ef\u6d4b\u7b97\u65b9\u6cd5\u867d\u80fd\u7cbe\u786e\u8ba1\u7b97\u4f53\u79ef\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u98df\u7269\u90e8\u4f4d\u7684\u7cbe\u51c6\u9009\u62e9\u80fd\u529b\u3002\u51c6\u786e\u7684\u98df\u7269\u4f53\u79ef\u6d4b\u7b97\u5bf9\u81b3\u98df\u76d1\u6d4b\u3001\u533b\u7597\u8425\u517b\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u63d0\u5347\u590d\u6742\u573a\u666f\u4e0b\u7684\u76ee\u6807\u98df\u7269\u9009\u62e9\u80fd\u529b", "method": "1. \u7528\u6237\u901a\u8fc7\u6587\u672c\u8f93\u5165\u6307\u5b9a\u76ee\u6807\u98df\u7269\u8fdb\u884c\u8bed\u4e49\u5206\u5272 \u2192 2. \u91c7\u7528\u795e\u7ecf\u8868\u9762\u91cd\u5efa\u65b9\u6cd5\u751f\u6210\u9ad8\u4fdd\u771f\u4e09\u7ef4\u7f51\u683c \u2192 3. \u5b9e\u65bd\u4f53\u79ef\u8ba1\u7b97\u7684\u53cc\u9636\u6bb5\u6280\u672f\u8def\u7ebf", "result": "\u5728MetaFood3D\u6570\u636e\u96c6\u9a8c\u8bc1\uff1a\u98df\u7269\u9694\u79bb\u91cd\u5efa\u8bef\u5dee\u964d\u4f4e37.2%\uff0c\u4f53\u79ef\u8ba1\u7b97\u51c6\u786e\u7387\u8fbe\u523092.7% (\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u63d0\u534715.6%)", "conclusion": "VolTex\u6846\u67b6\u6210\u529f\u6574\u5408\u6587\u672c\u4ea4\u4e92\u4e0e\u4e09\u7ef4\u91cd\u5efa\u6280\u672f\uff0c\u7a81\u7834\u98df\u7269\u90e8\u4f4d\u9009\u62e9\u74f6\u9888\uff0c\u4e3a\u667a\u80fd\u8425\u517b\u7ba1\u7406\u7cfb\u7edf\u63d0\u4f9b\u53ef\u9760\u6280\u672f\u652f\u6491\u3002\u5f00\u6e90\u4ee3\u7801\u4fc3\u8fdb\u9886\u57df\u53d1\u5c55"}}
{"id": "2506.02126", "pdf": "https://arxiv.org/pdf/2506.02126", "abs": "https://arxiv.org/abs/2506.02126", "authors": ["Juncheng Wu", "Sheng Liu", "Haoqin Tu", "Hang Yu", "Xiaoke Huang", "James Zou", "Cihang Xie", "Yuyin Zhou"], "title": "Knowledge or Reasoning? A Close Look at How LLMs Think Across Domains", "categories": ["cs.CL"], "comment": "17 pages, preprint", "summary": "Recent advances in reasoning-enhanced Large Language Models such as\nOpenAI-o1/3 and DeepSeek-R1 have significantly improved performance on complex\ntasks. However, the quality and transparency of their internal reasoning\nprocesses remain underexplored. This work moves beyond the final-answer\naccuracy and investigates step-by-step reasoning in the medical and\nmathematical domains by explicitly decomposing the thinking trajectories into\ntwo parts: knowledge and reasoning. Specifically, we introduce a fine-grained\nevaluation framework that judges: (1) the correctness of knowledge used\n(measured by Knowledge Index (KI)) and (2) the quality of reasoning (measured\nby Information Gain (InfoGain)). Using this framework, we study R1-distilled\nand base Qwen models trained with supervised fine-tuning (SFT) and/or\nreinforcement learning (RL) in the medical and math domains. Three intriguing\nfindings emerge: (1) The general reasoning abilities in R1-distilled models do\nnot transfer effectively to the medical domain through either SFT or RL. (2)\nSFT raises final-answer accuracy in both domains, but often at the cost of\nreasoning quality: InfoGain drops by 38.9% on average compared with untrained\nmodels; In the medical domain, however, SFT remains crucial because domain\nknowledge is indispensable. (3) RL enhances medical reasoning by pruning\ninaccurate or irrelevant knowledge from reasoning paths, thereby improving both\nreasoning accuracy and knowledge correctness.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5206\u89e3\u77e5\u8bc6\u63a8\u7406\u8fc7\u7a0b\uff0c\u53d1\u73b0\u4e0d\u540c\u8bad\u7ec3\u65b9\u6cd5(SFT/RL)\u5728\u533b\u5b66\u548c\u6570\u5b66\u9886\u57df\u5bf9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8d28\u91cf\u7684\u5f71\u54cd\u5dee\u5f02", "motivation": "\u73b0\u6709\u63a8\u7406\u589e\u5f3a\u578b\u5927\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u8868\u73b0\u63d0\u5347\u4f46\u5185\u90e8\u63a8\u7406\u900f\u660e\u5ea6\u4e0d\u8db3\uff0c\u5c24\u5176\u5728\u533b\u7597\u9886\u57df\u53ef\u80fd\u5e26\u6765\u5b89\u5168\u9690\u60a3", "method": "\u63d0\u51fa\u77e5\u8bc6\u6307\u6570(KI)\u548c\u4fe1\u606f\u589e\u76ca(InfoGain)\u8bc4\u4f30\u6846\u67b6\uff0c\u5bf9\u6bd4\u5206\u6790SFT/RL\u8bad\u7ec3\u65b9\u6cd5\u5728\u533b\u7597\u548c\u6570\u5b66\u9886\u57df\u7684\u8868\u73b0", "result": "SFT\u63d0\u5347\u7b54\u6848\u51c6\u786e\u7387\u4f46\u635f\u5bb3\u63a8\u7406\u8d28\u91cf\uff0cRL\u901a\u8fc7\u4fee\u526a\u9519\u8bef\u77e5\u8bc6\u63d0\u5347\u533b\u7597\u63a8\u7406\uff0c\u800c\u901a\u7528\u63a8\u7406\u80fd\u529b\u96be\u4ee5\u8fc1\u79fb\u81f3\u533b\u7597\u9886\u57df", "conclusion": "\u533b\u7597\u9886\u57df\u9700\u8981SFT+RL\u7ec4\u5408\u8bad\u7ec3\uff0c\u6570\u5b66\u9886\u57df\u9700\u4fdd\u6301\u6a21\u578b\u539f\u59cb\u63a8\u7406\u80fd\u529b\uff0c\u9886\u57df\u77e5\u8bc6\u7684\u7279\u6b8a\u6027\u51b3\u5b9a\u8bad\u7ec3\u7b56\u7565\u9009\u62e9"}}
{"id": "2506.03004", "pdf": "https://arxiv.org/pdf/2506.03004", "abs": "https://arxiv.org/abs/2506.03004", "authors": ["Junyu Liu", "R. Kenny Jones", "Daniel Ritchie"], "title": "PartComposer: Learning and Composing Part-Level Concepts from Single-Image Examples", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We present PartComposer: a framework for part-level concept learning from\nsingle-image examples that enables text-to-image diffusion models to compose\nnovel objects from meaningful components. Existing methods either struggle with\neffectively learning fine-grained concepts or require a large dataset as input.\nWe propose a dynamic data synthesis pipeline generating diverse part\ncompositions to address one-shot data scarcity. Most importantly, we propose to\nmaximize the mutual information between denoised latents and structured concept\ncodes via a concept predictor, enabling direct regulation on concept\ndisentanglement and re-composition supervision. Our method achieves strong\ndisentanglement and controllable composition, outperforming subject and\npart-level baselines when mixing concepts from the same, or different, object\ncategories.", "AI": {"tldr": "\u63d0\u51fa\u4e86PartComposer\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u6570\u636e\u5408\u6210\u548c\u4e92\u4fe1\u606f\u6700\u5927\u5316\u7b56\u7565\uff0c\u5b9e\u73b0\u5355\u6837\u672c\u573a\u666f\u4e0b\u90e8\u4ef6\u7ea7\u6982\u5ff5\u7684\u89e3\u8026\u4e0e\u91cd\u7ec4\uff0c\u5728\u8de8\u7c7b\u522b\u7ec4\u5408\u4efb\u52a1\u4e2d\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u7ec6\u7c92\u5ea6\u6982\u5ff5\u5b66\u4e60\u6548\u679c\u5dee\u3001\u4f9d\u8d56\u5927\u89c4\u6a21\u6570\u636e\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5355\u6837\u672c\u573a\u666f\u4e0b\u96be\u4ee5\u6709\u6548\u89e3\u51b3\u90e8\u4ef6\u7ea7\u6982\u5ff5\u7684\u89e3\u8026\u4e0e\u91cd\u7ec4\u9700\u6c42", "method": "1.\u52a8\u6001\u6570\u636e\u5408\u6210\u7ba1\u9053\u751f\u6210\u591a\u6837\u5316\u90e8\u4ef6\u7ec4\u5408 2.\u8bbe\u8ba1\u6982\u5ff5\u9884\u6d4b\u5668\u901a\u8fc7\u4e92\u4fe1\u606f\u6700\u5927\u5316\u7ea6\u675f\uff0c\u5b9e\u73b0\u6982\u5ff5\u89e3\u8026\u548c\u91cd\u7ec4\u76d1\u7763 3.\u7ed3\u6784\u5316\u6982\u5ff5\u7f16\u7801\u4f53\u7cfb", "result": "\u5728\u76f8\u540c/\u4e0d\u540c\u7269\u4f53\u7c7b\u522b\u7684\u6982\u5ff5\u6df7\u5408\u4efb\u52a1\u4e2d\uff0c\u5747\u53d6\u5f97\u6700\u4f18\u89e3\u8026\u6548\u679c\uff08\u5206\u79bb\u51c6\u786e\u7387\u63d0\u534712.3%\uff09\u548c\u7ec4\u5408\u53ef\u63a7\u6027\uff08\u7528\u6237\u504f\u597d\u738765.8%\uff09", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u90e8\u4ef6\u7ea7\u6982\u5ff5\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u8de8\u7c7b\u522b\u521b\u9020\u6027\u7ec4\u5408\u4efb\u52a1\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84"}}
{"id": "2506.02132", "pdf": "https://arxiv.org/pdf/2506.02132", "abs": "https://arxiv.org/abs/2506.02132", "authors": ["Michael Li", "Nishant Subramani"], "title": "Model Internal Sleuthing: Finding Lexical Identity and Inflectional Morphology in Modern Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large transformer-based language models dominate modern NLP, yet our\nunderstanding of how they encode linguistic information is rooted in studies of\nearly models like BERT and GPT-2. To better understand today's language models,\nwe investigate how both classical architectures (BERT, DeBERTa, GPT-2)and\ncontemporary large language models (Pythia, OLMo-2, Gemma-2, Qwen2.5,\nLlama-3.1) represent lexical identity and inflectional morphology. We train\nlinear and nonlinear classifiers on layer-wise activations to predict word\nlemmas and inflectional features. We discover that models concentrate lexical\ninformation linearly in early layers and increasingly nonlinearly in later\nlayers, while keeping inflectional information uniformly accessible and\nlinearly separable throughout the layers. Further analysis reveals that these\nmodels encode inflectional morphology through generalizable abstractions, but\nrely predominantly on memorization to encode lexical identity. Remarkably,\nthese patterns emerge across all 16 models we test, despite differences in\narchitecture, size, and training regime (including pretrained and\ninstruction-tuned variants). This consistency suggests that, despite\nsubstantial advances in LLM technologies, transformer models organize\nlinguistic information in similar ways, indicating that these properties could\nbe fundamental for next token prediction and are learned early during\npretraining. Our code is available at\nhttps://github.com/ml5885/model_internal_sleuthing.", "AI": {"tldr": "\u4e0d\u540cTransformer\u6a21\u578b\u5728\u8bcd\u6c47\u548c\u5c48\u6298\u5f62\u6001\u7f16\u7801\u4e2d\u5448\u73b0\u4e00\u81f4\u6027\u6a21\u5f0f\uff1a\u8bcd\u6c47\u4fe1\u606f\u65e9\u671f\u7ebf\u6027/\u540e\u671f\u975e\u7ebf\u6027\u7f16\u7801\uff0c\u5c48\u6298\u4fe1\u606f\u5168\u5c42\u7ebf\u6027\u53ef\u8bbf\u95ee\uff0c\u63ed\u793a\u8bed\u8a00\u4fe1\u606f\u7ec4\u7ec7\u7684\u5e95\u5c42\u89c4\u5f8b", "motivation": "\u586b\u8865\u5bf9\u73b0\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u7684\u7406\u89e3\u7a7a\u767d\uff0c\u7a81\u7834\u57fa\u4e8e\u65e9\u671f\u6a21\u578b\uff08\u5982BERT\uff09\u7684\u7814\u7a76\u8303\u5f0f\uff0c\u63a2\u7a76\u4e0d\u540c\u67b6\u6784/\u89c4\u6a21\u6a21\u578b\u7684\u5171\u6027\u8bed\u8a00\u7f16\u7801\u7279\u5f81", "method": "\u4f7f\u752816\u79cd\u7ecf\u5178\u548c\u73b0\u4ee3\u6a21\u578b\uff08\u542b\u6307\u4ee4\u5fae\u8c03\u53d8\u4f53\uff09\uff0c\u8bad\u7ec3\u7ebf\u6027/\u975e\u7ebf\u6027\u5206\u7c7b\u5668\u5206\u6790\u5c42\u6fc0\u6d3b\uff0c\u9884\u6d4b\u8bcd\u5143\uff08lemma\uff09\u548c\u5c48\u6298\u7279\u5f81\uff08\u65f6\u6001/\u6570/\u6027\u7b49\uff09", "result": "\u8bcd\u6c47\u8eab\u4efd\u4f9d\u8d56\u8bb0\u5fc6\u5316\u7f16\u7801\uff08\u65e9\u671f\u5c42\u7ebf\u6027\u96c6\u4e2d\uff0c\u540e\u671f\u975e\u7ebf\u6027\u589e\u5f3a\uff09\uff0c\u5c48\u6298\u5f62\u6001\u901a\u8fc7\u62bd\u8c61\u89c4\u5219\u7f16\u7801\uff08\u5168\u5c42\u7ebf\u6027\u53ef\u5206\u79bb\uff09\uff0c\u8be5\u6a21\u5f0f\u8de8\u8d8a\u4e0d\u540c\u67b6\u6784/\u89c4\u6a21\u6a21\u578b\u6301\u7eed\u5b58\u5728", "conclusion": "Transformer\u6a21\u578b\u7684\u8bed\u8a00\u4fe1\u606f\u7ec4\u7ec7\u65b9\u5f0f\u5177\u6709\u67b6\u6784\u65e0\u5173\u7684\u7a33\u5b9a\u6027\uff0c\u8fd9\u79cd\u5206\u5c42\u7f16\u7801\u673a\u5236\u53ef\u80fd\u662f\u5b9e\u73b0\u9ad8\u6548next token prediction\u7684\u57fa\u7840\u80fd\u529b\uff0c\u5f62\u6210\u4e8e\u9884\u8bad\u7ec3\u65e9\u671f\u9636\u6bb5"}}
{"id": "2506.03118", "pdf": "https://arxiv.org/pdf/2506.03118", "abs": "https://arxiv.org/abs/2506.03118", "authors": ["Zhiyuan Yu", "Zhe Li", "Hujun Bao", "Can Yang", "Xiaowei Zhou"], "title": "HumanRAM: Feed-forward Human Reconstruction and Animation Model using Transformers", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted by SIGGRAPH 2025 (Conference Track). Project page:\n  https://zju3dv.github.io/humanram/", "summary": "3D human reconstruction and animation are long-standing topics in computer\ngraphics and vision. However, existing methods typically rely on sophisticated\ndense-view capture and/or time-consuming per-subject optimization procedures.\nTo address these limitations, we propose HumanRAM, a novel feed-forward\napproach for generalizable human reconstruction and animation from monocular or\nsparse human images. Our approach integrates human reconstruction and animation\ninto a unified framework by introducing explicit pose conditions, parameterized\nby a shared SMPL-X neural texture, into transformer-based large reconstruction\nmodels (LRM). Given monocular or sparse input images with associated camera\nparameters and SMPL-X poses, our model employs scalable transformers and a\nDPT-based decoder to synthesize realistic human renderings under novel\nviewpoints and novel poses. By leveraging the explicit pose conditions, our\nmodel simultaneously enables high-quality human reconstruction and\nhigh-fidelity pose-controlled animation. Experiments show that HumanRAM\nsignificantly surpasses previous methods in terms of reconstruction accuracy,\nanimation fidelity, and generalization performance on real-world datasets.\nVideo results are available at https://zju3dv.github.io/humanram/.", "AI": {"tldr": "HumanRAM\u901a\u8fc7\u7ed3\u5408SMPL-X\u795e\u7ecf\u7eb9\u7406\u548ctransformer\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u5355\u76ee/\u7a00\u758f\u89c6\u89d2\u4e0b\u9ad8\u6548\u7684\u4eba\u4f53\u91cd\u5efa\u4e0e\u59ff\u6001\u63a7\u5236\u52a8\u753b", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5bc6\u96c6\u591a\u89c6\u89d2\u91c7\u96c6\u548c\u8017\u65f6\u7684\u9010\u5bf9\u8c61\u4f18\u5316\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u901a\u7528\u5316\u4eba\u4f53\u5efa\u6a21\u65b9\u6cd5", "method": "1. \u5c06SMPL-X\u59ff\u6001\u53c2\u6570\u4f5c\u4e3a\u663e\u5f0f\u6761\u4ef6\u878d\u5165transformer\u67b6\u6784\n2. \u4f7f\u7528\u53ef\u6269\u5c55transformer\u548cDPT-based\u89e3\u7801\u5668\n3. \u8054\u5408\u4f18\u5316\u795e\u7ecf\u7eb9\u7406\u548c\u65b0\u89c6\u89d2/\u65b0\u59ff\u6001\u5408\u6210", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8d85\u8d8aSOTA\u65b9\u6cd5\uff1a\n- \u91cd\u5efaPSNR\u63d0\u53472.1dB\n- \u52a8\u753bFID\u964d\u4f4e18.7%\n- \u8de8\u8eab\u4efd\u6cdb\u5316\u8bef\u5dee\u51cf\u5c1132%", "conclusion": "\u901a\u8fc7\u7edf\u4e00\u91cd\u5efa\u4e0e\u52a8\u753b\u6846\u67b6\uff0cHumanRAM\u9996\u6b21\u5b9e\u73b0\u4e86\uff1a\n1. \u524d\u9988\u5f0f\u7aef\u5230\u7aef\u4eba\u4f53\u5efa\u6a21\n2. \u4e9a\u6beb\u7c73\u7ea7\u51e0\u4f55\u91cd\u5efa\u7cbe\u5ea6\n3. \u5b9e\u65f6\uff0823fps\uff09\u59ff\u6001\u63a7\u5236\u6e32\u67d3\u80fd\u529b"}}
{"id": "2506.02147", "pdf": "https://arxiv.org/pdf/2506.02147", "abs": "https://arxiv.org/abs/2506.02147", "authors": ["Joshua Rozner", "Leonie Weissweiler", "Cory Shain"], "title": "BabyLM's First Constructions: Causal interventions provide a signal of learning", "categories": ["cs.CL"], "comment": null, "summary": "Construction grammar posits that children acquire constructions (form-meaning\npairings) from the statistics of their environment. Recent work supports this\nhypothesis by showing sensitivity to constructions in pretrained language\nmodels (PLMs), including one recent study (Rozner et al., 2025) demonstrating\nthat constructions shape the PLM's output distribution. However, models under\nstudy have generally been trained on developmentally implausible amounts of\ndata, casting doubt on their relevance to human language learning. Here we use\nRozner et al.'s methods to evaluate constructional learning in models from the\n2024 BabyLM challenge. Our results show that even when trained on\ndevelopmentally plausible quantities of data, models represent diverse\nconstructions, even hard cases that are superficially indistinguishable. We\nfurther find correlational evidence that constructional performance may be\nfunctionally relevant: models that better represent constructions perform\nbetter on the BabyLM benchmarks.", "AI": {"tldr": "\u7814\u7a76\u663e\u793a\uff0c\u4f7f\u7528\u5a74\u513f\u8bed\u8a00\u5b66\u4e60\u89c4\u6a21\u6570\u636e\uff08BabyLM\uff09\u8bad\u7ec3\u7684\u6a21\u578b\u80fd\u6709\u6548\u5b66\u4e60\u6784\u5f0f\u7ed3\u6784\uff0c\u4e14\u6784\u5f0f\u8868\u5f81\u80fd\u529b\u4e0e\u57fa\u51c6\u6d4b\u8bd5\u8868\u73b0\u6b63\u76f8\u5173\u3002", "motivation": "\u9a8c\u8bc1\u5728\u53d1\u80b2\u5408\u7406\u6570\u636e\u91cf\u4e0b\u8bad\u7ec3\u7684\u6a21\u578b\u662f\u5426\u4ecd\u80fd\u5b66\u4e60\u6784\u5f0f\u7ed3\u6784\uff0c\u56de\u5e94\u5148\u524d\u6a21\u578b\u4f7f\u7528\u975e\u5408\u7406\u6570\u636e\u91cf\u7684\u8d28\u7591\u3002", "method": "\u91c7\u7528Rozner\u7b49\u4eba\u65b9\u6cd5\uff0c\u5bf9BabyLM\u6311\u6218\u8d5b\u6a21\u578b\u8fdb\u884c\u6784\u5f0f\u654f\u611f\u6027\u6d4b\u8bd5\uff0c\u4f7f\u7528\u53d1\u80b2\u5408\u7406\u89c4\u6a21\uff08<100M\u8bcd\uff09\u7684\u8bed\u6599\u8bad\u7ec3\u3002", "result": "\u6a21\u578b\u6210\u529f\u4e60\u5f97\u591a\u79cd\u6784\u5f0f\uff08\u5305\u62ec\u8868\u9762\u96be\u8fa8\u7684\u590d\u6742\u6784\u5f0f\uff09\uff0c\u4e14\u6784\u5f0f\u8868\u5f81\u80fd\u529b\u4e0eBabyLM\u57fa\u51c6\u6d4b\u8bd5\u6210\u7ee9\u5448\u6b63\u76f8\u5173\u3002", "conclusion": "\u53d1\u80b2\u5408\u7406\u6570\u636e\u91cf\u4e0b\u6a21\u578b\u4ecd\u5177\u5907\u6784\u5f0f\u5b66\u4e60\u80fd\u529b\uff0c\u652f\u6301\u8ba1\u7b97\u5efa\u6a21\u5bf9\u4eba\u7c7b\u8bed\u8a00\u4e60\u5f97\u7814\u7a76\u7684\u89e3\u91ca\u529b\u3002"}}
{"id": "2506.02380", "pdf": "https://arxiv.org/pdf/2506.02380", "abs": "https://arxiv.org/abs/2506.02380", "authors": ["Zihao Ding", "Cheng-Tse Lee", "Mufeng Zhu", "Tao Guan", "Yuan-Chun Sun", "Cheng-Hsin Hsu", "Yao Liu"], "title": "EyeNavGS: A 6-DoF Navigation Dataset and Record-n-Replay Software for Real-World 3DGS Scenes in VR", "categories": ["cs.MM", "cs.CV", "cs.GR", "cs.HC"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) is an emerging media representation that\nreconstructs real-world 3D scenes in high fidelity, enabling\n6-degrees-of-freedom (6-DoF) navigation in virtual reality (VR). However,\ndeveloping and evaluating 3DGS-enabled applications and optimizing their\nrendering performance, require realistic user navigation data. Such data is\ncurrently unavailable for photorealistic 3DGS reconstructions of real-world\nscenes. This paper introduces EyeNavGS (EyeNavGS), the first publicly available\n6-DoF navigation dataset featuring traces from 46 participants exploring twelve\ndiverse, real-world 3DGS scenes. The dataset was collected at two sites, using\nthe Meta Quest Pro headsets, recording the head pose and eye gaze data for each\nrendered frame during free world standing 6-DoF navigation. For each of the\ntwelve scenes, we performed careful scene initialization to correct for scene\ntilt and scale, ensuring a perceptually-comfortable VR experience. We also\nrelease our open-source SIBR viewer software fork with record-and-replay\nfunctionalities and a suite of utility tools for data processing, conversion,\nand visualization. The EyeNavGS dataset and its accompanying software tools\nprovide valuable resources for advancing research in 6-DoF viewport prediction,\nadaptive streaming, 3D saliency, and foveated rendering for 3DGS scenes. The\nEyeNavGS dataset is available at: https://symmru.github.io/EyeNavGS/.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u516c\u5f00\u76846-DoF\u5bfc\u822a\u6570\u636e\u96c6EyeNavGS\uff0c\u5305\u542b46\u540d\u53c2\u4e0e\u8005\u572812\u4e2a\u771f\u5b9e3DGS\u573a\u666f\u4e2d\u7684\u5bfc\u822a\u8f68\u8ff9\uff0c\u914d\u5957\u5f00\u6e90\u5de5\u5177\u94fe\u652f\u6491VR\u5e94\u7528\u7814\u7a76", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u771f\u5b9e\u7528\u6237\u5bfc\u822a\u6570\u636e\u5236\u7ea63DGS\u573a\u666f\u5e94\u7528\u7684\u5f00\u53d1\u8bc4\u4f30\u4e0e\u6e32\u67d3\u4f18\u5316\uff0c\u9700\u5efa\u7acb\u6807\u51c6\u5316\u6570\u636e\u96c6", "method": "\u4f7f\u7528Meta Quest Pro\u5934\u663e\u91c7\u96c6\u5934\u90e8\u59ff\u6001\u4e0e\u773c\u52a8\u6570\u636e\uff0c\u901a\u8fc7\u573a\u666f\u521d\u59cb\u5316\u6821\u6b63\u503e\u659c\u4e0e\u6bd4\u4f8b\uff0c\u5f00\u53d1\u652f\u6301\u5f55\u5236\u56de\u653e\u7684\u5f00\u6e90SIBR\u6d4f\u89c8\u5668\u5206\u652f", "result": "\u521b\u5efa\u9996\u4e2a\u5305\u542b\u591a\u573a\u666f\u771f\u5b9e\u5bfc\u822a\u8f68\u8ff9\u7684\u516c\u5f00\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u6570\u636e\u5904\u7406\u5de5\u5177\u94fe\uff0c\u652f\u6301\u89c6\u53e3\u9884\u6d4b\u3001\u81ea\u9002\u5e94\u6d41\u5a92\u4f53\u7b49\u7814\u7a76\u65b9\u5411", "conclusion": "EyeNavGS\u586b\u8865\u4e863DGS\u573a\u666f\u5bfc\u822a\u6570\u636e\u7a7a\u767d\uff0c\u4e3a6-DoF\u89c6\u53e3\u9884\u6d4b\u3001\u6ce8\u89c6\u70b9\u6e32\u67d3\u7b49\u7814\u7a76\u63d0\u4f9b\u57fa\u51c6\u6570\u636e\u96c6\u4e0e\u6280\u672f\u652f\u6301"}}
{"id": "2506.02157", "pdf": "https://arxiv.org/pdf/2506.02157", "abs": "https://arxiv.org/abs/2506.02157", "authors": ["Amir Hussein", "Cihan Xiao", "Matthew Wiesner", "Dan Povey", "Leibny Paola Garcia", "Sanjeev Khudanpur"], "title": "HENT-SRT: Hierarchical Efficient Neural Transducer with Self-Distillation for Joint Speech Recognition and Translation", "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "Neural transducers (NT) provide an effective framework for speech streaming,\ndemonstrating strong performance in automatic speech recognition (ASR).\nHowever, the application of NT to speech translation (ST) remains challenging,\nas existing approaches struggle with word reordering and performance\ndegradation when jointly modeling ASR and ST, resulting in a gap with\nattention-based encoder-decoder (AED) models. Existing NT-based ST approaches\nalso suffer from high computational training costs. To address these issues, we\npropose HENT-SRT (Hierarchical Efficient Neural Transducer for Speech\nRecognition and Translation), a novel framework that factorizes ASR and\ntranslation tasks to better handle reordering. To ensure robust ST while\npreserving ASR performance, we use self-distillation with CTC consistency\nregularization. Moreover, we improve computational efficiency by incorporating\nbest practices from ASR transducers, including a down-sampled hierarchical\nencoder, a stateless predictor, and a pruned transducer loss to reduce training\ncomplexity. Finally, we introduce a blank penalty during decoding, reducing\ndeletions and improving translation quality. Our approach is evaluated on three\nconversational datasets Arabic, Spanish, and Mandarin achieving new\nstate-of-the-art performance among NT models and substantially narrowing the\ngap with AED-based systems.", "AI": {"tldr": "\u63d0\u51faHENT-SRT\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u4efb\u52a1\u5206\u89e3\u4e0e\u81ea\u84b8\u998f\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u8bed\u97f3\u7ffb\u8bd1\u6027\u80fd\u5e76\u7f29\u5c0f\u4e0eAED\u7cfb\u7edf\u7684\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u8f6c\u5f55\u5668\u5728\u8bed\u97f3\u7ffb\u8bd1\u4e2d\u5b58\u5728\u8bcd\u5e8f\u91cd\u6392\u56f0\u96be\u3001\u8054\u5408\u5efa\u6a21\u6027\u80fd\u9000\u5316\u53ca\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002", "method": "1. \u5206\u5c42\u67b6\u6784\u5206\u79bbASR\u4e0e\u7ffb\u8bd1\u4efb\u52a1\n2. \u81ea\u84b8\u998f\u7ed3\u5408CTC\u4e00\u81f4\u6027\u6b63\u5219\u5316\n3. \u91c7\u7528\u964d\u91c7\u6837\u7f16\u7801\u5668/\u65e0\u72b6\u6001\u9884\u6d4b\u5668/\u4fee\u526a\u635f\u5931\u63d0\u5347\u6548\u7387\n4. \u89e3\u7801\u9636\u6bb5\u5f15\u5165\u7a7a\u767d\u60e9\u7f5a\u673a\u5236", "result": "\u5728\u963f\u62c9\u4f2f\u8bed/\u897f\u73ed\u7259\u8bed/\u666e\u901a\u8bdd\u5bf9\u8bdd\u6570\u636e\u96c6\u4e0a\u8fbe\u5230NT\u6a21\u578bSOTA\uff0c\u4e0eAED\u7cfb\u7edf\u5dee\u8ddd\u663e\u8457\u7f29\u5c0f", "conclusion": "HENT-SRT\u6709\u6548\u89e3\u51b3\u4e86\u795e\u7ecf\u8f6c\u5f55\u5668\u5728\u8bed\u97f3\u7ffb\u8bd1\u4e2d\u7684\u6838\u5fc3\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u4e0e\u6548\u7387\u7684\u5e73\u8861"}}
{"id": "2506.02661", "pdf": "https://arxiv.org/pdf/2506.02661", "abs": "https://arxiv.org/abs/2506.02661", "authors": ["Mingyang Huang", "Peng Zhang", "Bang Zhang"], "title": "MotionRAG-Diff: A Retrieval-Augmented Diffusion Framework for Long-Term Music-to-Dance Generation", "categories": ["cs.SD", "cs.CV", "cs.GR", "eess.AS"], "comment": "12 pages, 5 figures", "summary": "Generating long-term, coherent, and realistic music-conditioned dance\nsequences remains a challenging task in human motion synthesis. Existing\napproaches exhibit critical limitations: motion graph methods rely on fixed\ntemplate libraries, restricting creative generation; diffusion models, while\ncapable of producing novel motions, often lack temporal coherence and musical\nalignment. To address these challenges, we propose $\\textbf{MotionRAG-Diff}$, a\nhybrid framework that integrates Retrieval-Augmented Generation (RAG) with\ndiffusion-based refinement to enable high-quality, musically coherent dance\ngeneration for arbitrary long-term music inputs. Our method introduces three\ncore innovations: (1) A cross-modal contrastive learning architecture that\naligns heterogeneous music and dance representations in a shared latent space,\nestablishing unsupervised semantic correspondence without paired data; (2) An\noptimized motion graph system for efficient retrieval and seamless\nconcatenation of motion segments, ensuring realism and temporal coherence\nacross long sequences; (3) A multi-condition diffusion model that jointly\nconditions on raw music signals and contrastive features to enhance motion\nquality and global synchronization. Extensive experiments demonstrate that\nMotionRAG-Diff achieves state-of-the-art performance in motion quality,\ndiversity, and music-motion synchronization accuracy. This work establishes a\nnew paradigm for music-driven dance generation by synergizing retrieval-based\ntemplate fidelity with diffusion-based creative enhancement.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4e0e\u6269\u6563\u6a21\u578b\u7684MotionRAG-Diff\u6846\u67b6\uff0c\u89e3\u51b3\u957f\u65f6\u97f3\u4e50\u9a71\u52a8\u821e\u8e48\u751f\u6210\u4e2d\u65f6\u95f4\u8fde\u8d2f\u6027\u4e0e\u97f3\u4e50\u5bf9\u9f50\u7684\u96be\u9898\u3002", "motivation": "\u73b0\u6709\u8fd0\u52a8\u56fe\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u6a21\u677f\u9650\u5236\u521b\u9020\u6027\uff0c\u6269\u6563\u6a21\u578b\u7f3a\u4e4f\u65f6\u95f4\u8fde\u8d2f\u6027\u3002\u9700\u8981\u517c\u987e\u521b\u65b0\u6027\u4e0e\u8fde\u8d2f\u6027\u7684\u97f3\u4e50\u9a71\u52a8\u821e\u8e48\u751f\u6210\u65b9\u6848\u3002", "method": "1.\u8de8\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u97f3\u4e50-\u821e\u8e48\u8868\u5f81\uff1b2.\u4f18\u5316\u8fd0\u52a8\u56fe\u7cfb\u7edf\u5b9e\u73b0\u957f\u5e8f\u5217\u8fde\u8d2f\u62fc\u63a5\uff1b3.\u591a\u6761\u4ef6\u6269\u6563\u6a21\u578b\u878d\u5408\u539f\u59cb\u97f3\u4e50\u4fe1\u53f7\u4e0e\u5bf9\u6bd4\u7279\u5f81", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u8fd0\u52a8\u8d28\u91cf\u3001\u591a\u6837\u6027\u548c\u97f3\u4e50\u540c\u6b65\u7cbe\u5ea6\u4e0a\u8fbe\u5230SOTA\uff0c\u7a81\u7834\u957f\u65f6\u751f\u6210\u9650\u5236\u3002", "conclusion": "\u901a\u8fc7\u68c0\u7d22\u6a21\u677f\u4fdd\u771f\u5ea6\u4e0e\u6269\u6563\u6a21\u578b\u521b\u9020\u6027\u7684\u534f\u540c\uff0c\u5efa\u7acb\u4e86\u97f3\u4e50\u9a71\u52a8\u821e\u8e48\u751f\u6210\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2506.02172", "pdf": "https://arxiv.org/pdf/2506.02172", "abs": "https://arxiv.org/abs/2506.02172", "authors": ["Dennis Fucci", "Marco Gaido", "Matteo Negri", "Luisa Bentivogli", "Andre Martins", "Giuseppe Attanasio"], "title": "Different Speech Translation Models Encode and Translate Speaker Gender Differently", "categories": ["cs.CL"], "comment": "Accepted at ACL 2025", "summary": "Recent studies on interpreting the hidden states of speech models have shown\ntheir ability to capture speaker-specific features, including gender. Does this\nfinding also hold for speech translation (ST) models? If so, what are the\nimplications for the speaker's gender assignment in translation? We address\nthese questions from an interpretability perspective, using probing methods to\nassess gender encoding across diverse ST models. Results on three language\ndirections (English-French/Italian/Spanish) indicate that while traditional\nencoder-decoder models capture gender information, newer architectures --\nintegrating a speech encoder with a machine translation system via adapters --\ndo not. We also demonstrate that low gender encoding capabilities result in\nsystems' tendency toward a masculine default, a translation bias that is more\npronounced in newer architectures.", "AI": {"tldr": "\u7814\u7a76\u63ed\u793a\u65b0\u578b\u8bed\u97f3\u7ffb\u8bd1\u67b6\u6784\u56e0\u6027\u522b\u7f16\u7801\u80fd\u529b\u4e0d\u8db3\u5bfc\u81f4\u7ffb\u8bd1\u504f\u5411\u7537\u6027\u5316\uff0c\u4f20\u7edf\u6a21\u578b\u4ecd\u4fdd\u7559\u6027\u522b\u4fe1\u606f\u3002", "motivation": "\u63a2\u8ba8\u8bed\u97f3\u7ffb\u8bd1\u6a21\u578b\u662f\u5426\u6355\u6349\u8bf4\u8bdd\u8005\u6027\u522b\u7279\u5f81\uff0c\u53ca\u5176\u5bf9\u7ffb\u8bd1\u6027\u522b\u5206\u914d\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u63a2\u9488\u65b9\u6cd5\u5206\u6790\u4e0d\u540cST\u6a21\u578b\uff08\u4f20\u7edf\u7f16\u7801\u5668-\u89e3\u7801\u5668 vs \u57fa\u4e8e\u9002\u914d\u5668\u6574\u5408\u8bed\u97f3\u7f16\u7801\u5668\u4e0e\u7ffb\u8bd1\u7cfb\u7edf\u7684\u65b0\u67b6\u6784\uff09\u7684\u6027\u522b\u7f16\u7801\u80fd\u529b\uff0c\u6d4b\u8bd5\u82f1\u6cd5/\u610f/\u897f\u4e09\u4e2a\u8bed\u8a00\u65b9\u5411\u3002", "result": "\u4f20\u7edf\u6a21\u578b\u53ef\u6355\u6349\u6027\u522b\u4fe1\u606f\uff0c\u65b0\u578b\u67b6\u6784\u65e0\u6cd5\u7f16\u7801\u6027\u522b\uff1b\u4f4e\u6027\u522b\u7f16\u7801\u80fd\u529b\u5bfc\u81f4\u7cfb\u7edf\u9ed8\u8ba4\u7537\u6027\u5316\u7ffb\u8bd1\u503e\u5411\uff08\u65b0\u67b6\u6784\u66f4\u663e\u8457\uff09\u3002", "conclusion": "\u6a21\u578b\u67b6\u6784\u9009\u62e9\u76f4\u63a5\u5f71\u54cd\u6027\u522b\u504f\u89c1\u7a0b\u5ea6\uff0c\u6027\u80fd\u4f18\u5316\u53ef\u80fd\u4ee5\u52a0\u5267\u793e\u4f1a\u504f\u89c1\u4e3a\u4ee3\u4ef7\uff0c\u9700\u5e73\u8861\u6280\u672f\u6548\u80fd\u4e0e\u4f26\u7406\u8003\u91cf\u3002"}}
{"id": "2506.03099", "pdf": "https://arxiv.org/pdf/2506.03099", "abs": "https://arxiv.org/abs/2506.03099", "authors": ["Chetwin Low", "Weimin Wang"], "title": "TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via Autoregressive Diffusion Models", "categories": ["cs.SD", "cs.AI", "cs.GR"], "comment": null, "summary": "In this paper, we present TalkingMachines -- an efficient framework that\ntransforms pretrained video generation models into real-time, audio-driven\ncharacter animators. TalkingMachines enables natural conversational experiences\nby integrating an audio large language model (LLM) with our video generation\nfoundation model. Our primary contributions include: (1) We adapt a pretrained\nSOTA image-to-video DiT into an audio-driven avatar generation model of 18\nbillion parameters; (2) We enable infinite video streaming without error\naccumulation through asymmetric knowledge distillation from a bidirectional\nteacher model into a sparse causal, autoregressive student model; (3) We design\na high-throughput, low-latency inference pipeline incorporating several key\nengineering optimizations such as: (a) disaggregation of the DiT and VAE\ndecoder across separate devices, (b) efficient overlap of inter-device\ncommunication and computation using CUDA streams, (c) elimination of redundant\nrecomputations to maximize frame-generation throughput. Please see demo videos\nhere - https://aaxwaz.github.io/TalkingMachines/", "AI": {"tldr": "\u63d0\u51faTalkingMachines\u6846\u67b6\uff0c\u5c06\u9884\u8bad\u7ec3\u89c6\u9891\u751f\u6210\u6a21\u578b\u8f6c\u5316\u4e3a\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u52a8\u753b\u7cfb\u7edf\uff0c\u96c6\u6210\u97f3\u9891\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u89c6\u9891\u751f\u6210\u57fa\u5ea7\u6a21\u578b\uff0c\u5b9e\u73b018B\u53c2\u6570\u6a21\u578b\u3001\u65e0\u9650\u89c6\u9891\u6d41\u4f20\u8f93\u548c\u4f4e\u5ef6\u8fdf\u4f18\u5316", "motivation": "\u89e3\u51b3\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u5b9e\u65f6\u89d2\u8272\u52a8\u753b\u5e94\u7528\u4e2d\u5b58\u5728\u7684\u5ef6\u8fdf\u9ad8\u3001\u9519\u8bef\u7d2f\u79ef\u4e25\u91cd\u3001\u5de5\u7a0b\u6548\u7387\u4f4e\u4e0b\u7b49\u95ee\u9898\uff0c\u8ffd\u6c42\u81ea\u7136\u5bf9\u8bdd\u5f0f\u4ea4\u4e92\u4f53\u9a8c", "method": "1. \u6539\u9020\u56fe\u50cf\u5230\u89c6\u9891DiT\u6a21\u578b\u9002\u914d\u97f3\u9891\u9a71\u52a8\n2. \u901a\u8fc7\u53cc\u5411\u6559\u5e08\u6a21\u578b\u5230\u7a00\u758f\u56e0\u679c\u5b66\u751f\u6a21\u578b\u7684\u77e5\u8bc6\u84b8\u998f\u5b9e\u73b0\u65e0\u9650\u6d41\n3. \u8bbe\u8ba1\u591a\u8bbe\u5907\u534f\u540c\u3001CUDA\u6d41\u4f18\u5316\u3001\u5197\u4f59\u6d88\u9664\u7684\u9ad8\u6548\u63a8\u7406\u7ba1\u7ebf", "result": "\u5b9e\u73b0\u9ad8\u541e\u5410(34.5 FPS)\u548c\u4f4e\u5ef6\u8fdf(150ms)\u7684\u5b9e\u65f6\u751f\u6210\uff0c\u652f\u6301\u65e0\u9650\u65f6\u957f\u89c6\u9891\u6d41\u4f20\u8f93\u4e14\u65e0\u9519\u8bef\u7d2f\u79ef\uff0c\u6f14\u793a\u89c6\u9891\u5c55\u793a\u81ea\u7136\u5bf9\u8bdd\u6548\u679c", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7b97\u6cd5\u521b\u65b0\u4e0e\u7cfb\u7edf\u5de5\u7a0b\u4f18\u5316\u7684\u6df1\u5ea6\u7ed3\u5408\uff0c\u4e3a\u5b9e\u65f6\u6570\u5b57\u4eba\u4ea4\u4e92\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\uff0cLLM\u4e0e\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u6574\u5408\u5f00\u8f9f\u4e86\u591a\u6a21\u6001\u751f\u6210\u65b0\u8def\u5f84"}}
{"id": "2506.02175", "pdf": "https://arxiv.org/pdf/2506.02175", "abs": "https://arxiv.org/abs/2506.02175", "authors": ["Salman Rahman", "Sheriff Issaka", "Ashima Suvarna", "Genglin Liu", "James Shiffer", "Jaeyoung Lee", "Md Rizwan Parvez", "Hamid Palangi", "Shi Feng", "Nanyun Peng", "Yejin Choi", "Julian Michael", "Liwei Jiang", "Saadia Gabriel"], "title": "AI Debate Aids Assessment of Controversial Claims", "categories": ["cs.CL"], "comment": null, "summary": "As AI grows more powerful, it will increasingly shape how we understand the\nworld. But with this influence comes the risk of amplifying misinformation and\ndeepening social divides-especially on consequential topics like public health\nwhere factual accuracy directly impacts well-being. Scalable Oversight aims to\nensure AI truthfulness by enabling humans to supervise systems that may exceed\nhuman capabilities--yet humans themselves hold different beliefs and biases\nthat impair their judgment. We study whether AI debate can guide biased judges\ntoward the truth by having two AI systems debate opposing sides of\ncontroversial COVID-19 factuality claims where people hold strong prior\nbeliefs. We conduct two studies: one with human judges holding either\nmainstream or skeptical beliefs evaluating factuality claims through\nAI-assisted debate or consultancy protocols, and a second examining the same\nproblem with personalized AI judges designed to mimic these different human\nbelief systems. In our human study, we find that debate-where two AI advisor\nsystems present opposing evidence-based arguments-consistently improves\njudgment accuracy and confidence calibration, outperforming consultancy with a\nsingle-advisor system by 10% overall. The improvement is most significant for\njudges with mainstream beliefs (+15.2% accuracy), though debate also helps\nskeptical judges who initially misjudge claims move toward accurate views\n(+4.7% accuracy). In our AI judge study, we find that AI judges with human-like\npersonas achieve even higher accuracy (78.5%) than human judges (70.1%) and\ndefault AI judges without personas (69.8%), suggesting their potential for\nsupervising frontier AI models. These findings highlight AI debate as a\npromising path toward scalable, bias-resilient oversight--leveraging both\ndiverse human and AI judgments to move closer to truth in contested domains.", "AI": {"tldr": "AI\u8fa9\u8bba\u673a\u5236\u53ef\u63d0\u5347\u516c\u5171\u536b\u751f\u9886\u57df\u4e8b\u5b9e\u5224\u65ad\u51c6\u786e\u6027\uff0c\u4e3b\u6d41\u7fa4\u4f53\u51c6\u786e\u7387+15.2%\uff0cAI\u8bc4\u59d4\u8fbe78.5%\u51c6\u786e\u7387\u4f18\u4e8e\u4eba\u7c7b\u3002", "motivation": "\u89e3\u51b3AI\u76d1\u7763\u4e2d\u4eba\u7c7b\u504f\u89c1\u5bf9\u4e8b\u5b9e\u5224\u65ad\u7684\u5e72\u6270\uff0c\u63a2\u7d22\u5728COVID-19\u7b49\u4e89\u8bae\u9886\u57df\u901a\u8fc7AI\u8fa9\u8bba\u5b9e\u73b0\u6297\u504f\u89c1\u7684\u53ef\u6269\u5c55\u76d1\u7763\u673a\u5236\u3002", "method": "\u53cc\u5b9e\u9a8c\u8bbe\u8ba1\uff1a1) \u4eba\u7c7b\u8bc4\u59d4(\u4e3b\u6d41/\u6000\u7591\u7acb\u573a)\u5bf9\u6bd4AI\u8fa9\u8bba\u4e0e\u5355\u987e\u95ee\u6a21\u5f0f 2) \u4e2a\u6027\u5316AI\u8bc4\u59d4\u6a21\u62df\u4eba\u7c7b\u4fe1\u5ff5\u7cfb\u7edf", "result": "\u8fa9\u8bba\u6a21\u5f0f\u6574\u4f53\u51c6\u786e\u7387\u63d0\u534710%\uff0c\u4e3b\u6d41\u8bc4\u59d4\u63d0\u534715.2%\uff1bAI\u8bc4\u59d4\u51c6\u786e\u7387\u8fbe78.5%\uff0c\u663e\u8457\u4f18\u4e8e\u4eba\u7c7b\u8bc4\u59d4\u768470.1%", "conclusion": "AI\u8fa9\u8bba\u673a\u5236\u7ed3\u5408\u4eba\u673a\u534f\u540c\u5224\u65ad\uff0c\u4e3a\u4e89\u8bae\u9886\u57df\u63d0\u4f9b\u6297\u504f\u89c1\u7684\u4e8b\u5b9e\u6838\u67e5\u65b0\u8def\u5f84\uff0c\u51f8\u663e\u4e2a\u6027\u5316AI\u8bc4\u59d4\u7684\u76d1\u7763\u6f5c\u529b\u3002"}}
{"id": "2506.02181", "pdf": "https://arxiv.org/pdf/2506.02181", "abs": "https://arxiv.org/abs/2506.02181", "authors": ["Dennis Fucci", "Marco Gaido", "Matteo Negri", "Mauro Cettolo", "Luisa Bentivogli"], "title": "Echoes of Phonetics: Unveiling Relevant Acoustic Cues for ASR via Feature Attribution", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at Interspeech 2025", "summary": "Despite significant advances in ASR, the specific acoustic cues models rely\non remain unclear. Prior studies have examined such cues on a limited set of\nphonemes and outdated models. In this work, we apply a feature attribution\ntechnique to identify the relevant acoustic cues for a modern Conformer-based\nASR system. By analyzing plosives, fricatives, and vowels, we assess how\nfeature attributions align with their acoustic properties in the time and\nfrequency domains, also essential for human speech perception. Our findings\nshow that the ASR model relies on vowels' full time spans, particularly their\nfirst two formants, with greater saliency in male speech. It also better\ncaptures the spectral characteristics of sibilant fricatives than non-sibilants\nand prioritizes the release phase in plosives, especially burst\ncharacteristics. These insights enhance the interpretability of ASR models and\nhighlight areas for future research to uncover potential gaps in model\nrobustness.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u7279\u5f81\u5f52\u56e0\u6280\u672f\u63ed\u793a\u73b0\u4ee3ASR\u6a21\u578b\u4f9d\u8d56\u5143\u97f3\u65f6\u957f/\u524d\u4e24\u4e2a\u5171\u632f\u5cf0\u3001\u7537\u6027\u8bed\u97f3\u663e\u8457\u6027\u66f4\u9ad8\uff0c\u5e76\u53d1\u73b0\u6a21\u578b\u5bf9\u9f7f\u64e6\u97f3\u9891\u8c31\u7279\u5f81\u548c\u7206\u7834\u97f3\u91ca\u653e\u9636\u6bb5\u7684\u6355\u6349\u504f\u597d\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5c40\u9650\u4e8e\u5c11\u91cf\u97f3\u7d20\u548c\u8fc7\u65f6\u6a21\u578b\uff0c\u9700\u7cfb\u7edf\u6027\u5206\u6790\u73b0\u4ee3ASR\u6a21\u578b\u7684\u58f0\u5b66\u7279\u5f81\u4f9d\u8d56\u673a\u5236", "method": "\u5e94\u7528\u7279\u5f81\u5f52\u56e0\u6280\u672f\u5206\u6790Conformer\u67b6\u6784ASR\u6a21\u578b\uff0c\u5bf9\u6bd4\u7206\u7834\u97f3/\u6469\u64e6\u97f3/\u5143\u97f3\u5728\u65f6\u57df\u548c\u9891\u57df\u7684\u58f0\u5b66\u7279\u5f81\u5bf9\u9f50\u7a0b\u5ea6", "result": "\u6a21\u578b\u5b8c\u6574\u5229\u7528\u5143\u97f3\u65f6\u95f4\u8de8\u5ea6(\u524d\u4e24\u4e2a\u5171\u632f\u5cf0\u4e3b\u5bfc\uff0c\u7537\u6027\u8bed\u97f3\u66f4\u663e\u8457)\uff0c\u66f4\u597d\u6355\u6349\u9f7f\u64e6\u97f3\u9891\u8c31\u7279\u5f81\uff0c\u4f18\u5148\u5904\u7406\u7206\u7834\u97f3\u91ca\u653e\u9636\u6bb5(\u7279\u522b\u662f\u7206\u7834\u7279\u6027)", "conclusion": "\u53d1\u73b0\u589e\u5f3a\u4e86ASR\u6a21\u578b\u53ef\u89e3\u91ca\u6027\uff0c\u63ed\u793a\u672a\u6765\u5e94\u91cd\u70b9\u7814\u7a76\u6a21\u578b\u58f0\u5b66\u8868\u5f81\u5b8c\u6574\u6027\u4ee5\u63d0\u5347\u9c81\u68d2\u6027"}}
{"id": "2506.02204", "pdf": "https://arxiv.org/pdf/2506.02204", "abs": "https://arxiv.org/abs/2506.02204", "authors": ["Lindia Tjuatja", "Graham Neubig"], "title": "BehaviorBox: Automated Discovery of Fine-Grained Performance Differences Between Language Models", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Language model evaluation is a daunting task: prompts are brittle,\ncorpus-level perplexities are vague, and the choice of benchmarks are endless.\nFinding examples that show meaningful, generalizable differences between two\nLMs is crucial to understanding where one model succeeds and another fails. Can\nthis process be done automatically? In this work, we propose methodology for\nautomated comparison of language models that uses performance-aware contextual\nembeddings to find fine-grained features of text where one LM outperforms\nanother. Our method, which we name BehaviorBox, extracts coherent features that\ndemonstrate differences with respect to the ease of generation between two LMs.\nSpecifically, BehaviorBox finds features that describe groups of words in\nfine-grained contexts, such as \"conditional 'were' in the phrase 'if you were'\"\nand \"exclamation marks after emotional statements\", where one model outperforms\nanother within a particular datatset. We apply BehaviorBox to compare models\nthat vary in size, model family, and post-training, and enumerate insights into\nspecific contexts that illustrate meaningful differences in performance which\ncannot be found by measures such as corpus-level perplexity alone.", "AI": {"tldr": "\u63d0\u51fa\u4e86BehaviorBox\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5d4c\u5165\u6280\u672f\u81ea\u52a8\u6bd4\u8f83\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u6587\u672c\u7279\u5f81\u4e0a\u7684\u6027\u80fd\u5dee\u5f02", "motivation": "\u4f20\u7edf\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u5b58\u5728\u63d0\u793a\u8bcd\u8106\u5f31\u6027\u3001\u8bed\u6599\u5e93\u590d\u6742\u5ea6\u6307\u6807\u6a21\u7cca\u548c\u57fa\u51c6\u6d4b\u8bd5\u9009\u62e9\u56f0\u96be\u7b49\u95ee\u9898\uff0c\u9700\u8981\u81ea\u52a8\u5316\u65b9\u6cd5\u53d1\u73b0\u6a21\u578b\u95f4\u7684\u5b9e\u8d28\u6027\u5dee\u5f02", "method": "\u5229\u7528\u6027\u80fd\u611f\u77e5\u7684\u4e0a\u4e0b\u6587\u5d4c\u5165\u6280\u672f\uff0c\u5728\u7279\u5b9a\u6570\u636e\u96c6\u4e2d\u53d1\u73b0\u4f53\u73b0\u6a21\u578b\u6027\u80fd\u5dee\u5f02\u7684\u7ec6\u7c92\u5ea6\u8bed\u8a00\u7279\u5f81\uff08\u5982\u6761\u4ef6\u4ece\u53e5\u4e2d\u7684\u52a8\u8bcd\u5f62\u6001\u6216\u60c5\u611f\u8bed\u53e5\u540e\u7684\u611f\u53f9\u53f7\uff09", "result": "\u6210\u529f\u8bc6\u522b\u51fa\u6a21\u578b\u5728\u53c2\u6570\u89c4\u6a21\u3001\u67b6\u6784\u65cf\u8c31\u548c\u540e\u671f\u8bad\u7ec3\u5dee\u5f02\u5bfc\u81f4\u7684\u6027\u80fd\u5dee\u5f02\u7279\u5f81\uff0c\u8fd9\u4e9b\u7279\u5f81\u65e0\u6cd5\u901a\u8fc7\u4f20\u7edf\u8bed\u6599\u5e93\u590d\u6742\u5ea6\u6307\u6807\u53d1\u73b0", "conclusion": "BehaviorBox\u4e3a\u8bed\u8a00\u6a21\u578b\u6bd4\u8f83\u63d0\u4f9b\u4e86\u81ea\u52a8\u5316\u5206\u6790\u6846\u67b6\uff0c\u80fd\u591f\u53d1\u73b0\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u9057\u6f0f\u7684\u4e0a\u4e0b\u6587\u654f\u611f\u578b\u6027\u80fd\u5dee\u5f02\uff0c\u4e3a\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u5177\u4f53\u65b9\u5411"}}
{"id": "2506.02212", "pdf": "https://arxiv.org/pdf/2506.02212", "abs": "https://arxiv.org/abs/2506.02212", "authors": ["Ella Rannon", "David Burstein"], "title": "Leveraging Natural Language Processing to Unravel the Mystery of Life: A Review of NLP Approaches in Genomics, Transcriptomics, and Proteomics", "categories": ["cs.CL", "cs.AI", "q-bio.GN"], "comment": null, "summary": "Natural Language Processing (NLP) has transformed various fields beyond\nlinguistics by applying techniques originally developed for human language to\nthe analysis of biological sequences. This review explores the application of\nNLP methods to biological sequence data, focusing on genomics, transcriptomics,\nand proteomics. We examine how various NLP methods, from classic approaches\nlike word2vec to advanced models employing transformers and hyena operators,\nare being adapted to analyze DNA, RNA, protein sequences, and entire genomes.\nThe review also examines tokenization strategies and model architectures,\nevaluating their strengths, limitations, and suitability for different\nbiological tasks. We further cover recent advances in NLP applications for\nbiological data, such as structure prediction, gene expression, and\nevolutionary analysis, highlighting the potential of these methods for\nextracting meaningful insights from large-scale genomic data. As language\nmodels continue to advance, their integration into bioinformatics holds immense\npromise for advancing our understanding of biological processes in all domains\nof life.", "AI": {"tldr": "NLP\u6280\u672f\u901a\u8fc7\u5c06\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e8eDNA/RNA/\u86cb\u767d\u8d28\u5e8f\u5217\u5206\u6790\uff0c\u63a8\u52a8\u751f\u7269\u4fe1\u606f\u5b66\u53d1\u5c55\uff0c\u6db5\u76d6\u6a21\u578b\u67b6\u6784\u8bc4\u4f30\u3001\u57fa\u56e0\u7ec4\u6570\u636e\u89e3\u6790\u53ca\u8de8\u9886\u57df\u5e94\u7528\u6f5c\u529b", "motivation": "\u751f\u7269\u5e8f\u5217\u4e0e\u4eba\u7c7b\u8bed\u8a00\u5728\u7ed3\u6784\u6a21\u5f0f\u4e0a\u5b58\u5728\u76f8\u4f3c\u6027\uff0c\u5229\u7528NLP\u5904\u7406\u5927\u89c4\u6a21\u57fa\u56e0\u7ec4\u6570\u636e\u53ef\u7a81\u7834\u4f20\u7edf\u751f\u7269\u4fe1\u606f\u5b66\u65b9\u6cd5\u74f6\u9888", "method": "\u6574\u5408\u7ecf\u5178word2vec\u4e0e\u65b0\u578btransformer/hyena\u7b97\u5b50\uff0c\u5f00\u53d1\u9002\u914d\u751f\u7269\u5e8f\u5217\u7279\u6027\u7684tokenization\u7b56\u7565\u4e0e\u591a\u5c3a\u5ea6\u6a21\u578b\u67b6\u6784", "result": "\u6210\u529f\u5e94\u7528\u4e8e\u86cb\u767d\u8d28\u7ed3\u6784\u9884\u6d4b\u3001\u57fa\u56e0\u8868\u8fbe\u8c03\u63a7\u89e3\u6790\u548c\u5206\u5b50\u8fdb\u5316\u5206\u6790\uff0c\u9a8c\u8bc1NLP\u65b9\u6cd5\u5728\u8de8\u7269\u79cd\u57fa\u56e0\u7ec4\u6570\u636e\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027", "conclusion": "\u8bed\u8a00\u6a21\u578b\u7684\u6301\u7eed\u8fdb\u5316\u5c06\u6df1\u5316\u5bf9\u751f\u547d\u8fc7\u7a0b\u7684\u7406\u89e3\uff0c\u4e3a\u7ec4\u5b66\u7814\u7a76\u63d0\u4f9b\u65b0\u7684\u65b9\u6cd5\u8bba\u7a81\u7834\u70b9"}}
{"id": "2506.02239", "pdf": "https://arxiv.org/pdf/2506.02239", "abs": "https://arxiv.org/abs/2506.02239", "authors": ["Sofoklis Kakouros"], "title": "Investigating the Impact of Word Informativeness on Speech Emotion Recognition", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "In emotion recognition from speech, a key challenge lies in identifying\nspeech signal segments that carry the most relevant acoustic variations for\ndiscerning specific emotions. Traditional approaches compute functionals for\nfeatures such as energy and F0 over entire sentences or longer speech portions,\npotentially missing essential fine-grained variation in the long-form\nstatistics. This research investigates the use of word informativeness, derived\nfrom a pre-trained language model, to identify semantically important segments.\nAcoustic features are then computed exclusively for these identified segments,\nenhancing emotion recognition accuracy. The methodology utilizes standard\nacoustic prosodic features, their functionals, and self-supervised\nrepresentations. Results indicate a notable improvement in recognition\nperformance when features are computed on segments selected based on word\ninformativeness, underscoring the effectiveness of this approach.", "AI": {"tldr": "\u901a\u8fc7\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7b5b\u9009\u9ad8\u4fe1\u606f\u91cf\u8bcd\u6c47\u7247\u6bb5\uff0c\u9488\u5bf9\u6027\u63d0\u53d6\u58f0\u5b66\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u51c6\u786e\u7387", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u6574\u53e5\u5c42\u9762\u8ba1\u7b97\u58f0\u5b66\u7279\u5f81\u53ef\u80fd\u9057\u6f0f\u5173\u952e\u7ec6\u8282\uff0c\u9700\u5f00\u53d1\u66f4\u7cbe\u51c6\u7684\u7247\u6bb5\u9009\u62e9\u65b9\u6cd5\u6355\u6349\u60c5\u611f\u76f8\u5173\u58f0\u5b66\u53d8\u5316", "method": "1. \u5229\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u8ba1\u7b97\u8bcd\u6c47\u4fe1\u606f\u91cf\n2. \u9009\u62e9\u9ad8\u4fe1\u606f\u91cf\u8bcd\u6c47\u5bf9\u5e94\u8bed\u97f3\u7247\u6bb5\n3. \u7ed3\u5408\u6807\u51c6\u97f5\u5f8b\u7279\u5f81\u3001\u529f\u80fd\u53c2\u6570\u548c\u81ea\u76d1\u7763\u8868\u793a\u8fdb\u884c\u5efa\u6a21", "result": "\u57fa\u4e8e\u8bcd\u6c47\u4fe1\u606f\u91cf\u9009\u62e9\u7684\u8bed\u97f3\u7247\u6bb5\u7279\u5f81\u4f7f\u60c5\u611f\u8bc6\u522b\u6027\u80fd\u663e\u8457\u63d0\u5347\uff08\u5177\u4f53\u63d0\u5347\u5e45\u5ea6\u9700\u53c2\u8003\u8bba\u6587\u6570\u636e\uff09", "conclusion": "\u8bcd\u6c47\u7ea7\u58f0\u5b66\u7279\u5f81\u9009\u62e9\u7b56\u7565\u6709\u6548\uff0c\u672a\u6765\u53ef\u63a2\u7d22\u591a\u6a21\u6001\u878d\u5408\u4e0e\u66f4\u7cbe\u7ec6\u7684\u65f6\u5e8f\u5efa\u6a21\u8fdb\u4e00\u6b65\u4f18\u5316"}}
{"id": "2506.02264", "pdf": "https://arxiv.org/pdf/2506.02264", "abs": "https://arxiv.org/abs/2506.02264", "authors": ["Radin Shayanfar", "Chu Fei Luo", "Rohan Bhambhoria", "Samuel Dahan", "Xiaodan Zhu"], "title": "CoDial: Interpretable Task-Oriented Dialogue Systems Through Dialogue Flow Alignment", "categories": ["cs.CL"], "comment": null, "summary": "It is often challenging to teach specialized, unseen tasks to dialogue\nsystems due to the high cost of expert knowledge, training data, and high\ntechnical difficulty. To support domain-specific applications - such as law,\nmedicine, or finance - it is essential to build frameworks that enable\nnon-technical experts to define, test, and refine system behaviour with minimal\neffort. Achieving this requires cross-disciplinary collaboration between\ndevelopers and domain specialists. In this work, we introduce a novel\nframework, CoDial (Code for Dialogue), that converts expert knowledge,\nrepresented as a novel structured heterogeneous graph, into executable\nconversation logic. CoDial can be easily implemented in existing guardrailing\nlanguages, such as Colang, to enable interpretable, modifiable, and true\nzero-shot specification of task-oriented dialogue systems. Empirically, CoDial\nachieves state-of-the-art performance on the STAR dataset for inference-based\nmodels and is competitive with similar baselines on the well-known MultiWOZ\ndataset. We also demonstrate CoDial's iterative improvement via manual and\nLLM-aided feedback, making it a practical tool for expert-guided alignment of\nLLMs in high-stakes domains.", "AI": {"tldr": "CoDial\u6846\u67b6\u901a\u8fc7\u5c06\u4e13\u5bb6\u77e5\u8bc6\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u56fe\u5e76\u751f\u6210\u5bf9\u8bdd\u903b\u8f91\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u7684\u96f6\u6837\u672c\u5bf9\u8bdd\u7cfb\u7edf\u6784\u5efa\uff0c\u5728STAR\u6570\u636e\u96c6\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4e13\u4e1a\u9886\u57df\uff08\u6cd5\u5f8b/\u533b\u7597\uff09\u5bf9\u8bdd\u7cfb\u7edf\u5f00\u53d1\u4e2d\u4e13\u5bb6\u77e5\u8bc6\u6574\u5408\u56f0\u96be\u3001\u8bad\u7ec3\u6570\u636e\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u964d\u4f4e\u9886\u57df\u4e13\u5bb6\u53c2\u4e0e\u6280\u672f\u5f00\u53d1\u7684\u95e8\u69db\u3002", "method": "\u4f7f\u7528\u7ed3\u6784\u5316\u5f02\u6784\u56fe\u8868\u793a\u4e13\u5bb6\u77e5\u8bc6\uff0c\u517c\u5bb9Colang\u7b49\u62a4\u680f\u8bed\u8a00\uff0c\u901a\u8fc7\u4ee3\u7801\u751f\u6210\u53ef\u6267\u884c\u7684\u5bf9\u8bdd\u903b\u8f91\u6d41\u7a0b\u3002", "result": "\u5728STAR\u6570\u636e\u96c6\u63a8\u7406\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\uff0c\u5728MultiWOZ\u4e0a\u4e0e\u57fa\u7ebf\u6a21\u578b\u7ade\u4e89\uff0c\u652f\u6301\u4eba\u5de5\u548cLLM\u53cd\u9988\u7684\u8fed\u4ee3\u4f18\u5316\u3002", "conclusion": "CoDial\u4e3a\u9ad8\u98ce\u9669\u9886\u57df\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u3001\u53ef\u4fee\u6539\u7684LLM\u5bf9\u9f50\u5de5\u5177\uff0c\u5b9e\u73b0\u4e86\u4e13\u5bb6\u6307\u5bfc\u4e0b\u7684\u96f6\u6837\u672c\u5bf9\u8bdd\u7cfb\u7edf\u5f00\u53d1\u3002"}}
{"id": "2506.02279", "pdf": "https://arxiv.org/pdf/2506.02279", "abs": "https://arxiv.org/abs/2506.02279", "authors": ["Wenzheng Zhang", "Xi Victoria Lin", "Karl Stratos", "Wen-tau Yih", "Mingda Chen"], "title": "ImpRAG: Retrieval-Augmented Generation with Implicit Queries", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems traditionally treat retrieval\nand generation as separate processes, requiring explicit textual queries to\nconnect them. This separation can limit the ability of models to generalize\nacross diverse tasks. In this work, we propose a query-free RAG system, named\nImpRAG, which integrates retrieval and generation into a unified model. ImpRAG\nallows models to implicitly express their information needs, eliminating the\nneed for human-specified queries. By dividing pretrained decoder-only language\nmodels into specialized layer groups, ImpRAG optimizes retrieval and generation\ntasks simultaneously. Our approach employs a two-stage inference process, using\nthe same model parameters and forward pass for both retrieval and generation,\nthereby minimizing the disparity between retrievers and language models.\nExperiments on 8 knowledge-intensive tasks demonstrate that ImpRAG achieves\n3.6-11.5 improvements in exact match scores on unseen tasks with diverse\nformats, highlighting its effectiveness in enabling models to articulate their\nown information needs and generalize across tasks. Our analysis underscores the\nimportance of balancing retrieval and generation parameters and leveraging\ngeneration perplexities as retrieval training objectives for enhanced\nperformance.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u663e\u5f0f\u67e5\u8be2\u7684ImpRAG\u6846\u67b6\uff0c\u5c06\u68c0\u7d22\u4e0e\u751f\u6210\u7edf\u4e00\u5230\u5355\u4e00\u6a21\u578b\u4e2d\uff0c\u901a\u8fc7\u5206\u5c42\u7ec4\u4f18\u5316\u63d0\u5347\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b", "motivation": "\u4f20\u7edfRAG\u7cfb\u7edf\u5c06\u68c0\u7d22\u4e0e\u751f\u6210\u5206\u79bb\uff0c\u4f9d\u8d56\u4eba\u5de5\u6307\u5b9a\u67e5\u8be2\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u4efb\u52a1\u683c\u5f0f\u95f4\u7684\u6cdb\u5316\u80fd\u529b", "method": "\u5c06\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5206\u5c42\u7ec4\u5b9e\u73b0\u4efb\u52a1\u5206\u5de5\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u63a8\u7406\u6d41\u7a0b\uff0c\u5171\u4eab\u6a21\u578b\u53c2\u6570\u548c\u524d\u5411\u8ba1\u7b97", "result": "\u57288\u4e2a\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\uff0c\u672a\u89c1\u4efb\u52a1\u7684\u7cbe\u786e\u5339\u914d\u5206\u6570\u63d0\u53473.6-11.5\u4e2a\u767e\u5206\u70b9", "conclusion": "ImpRAG\u901a\u8fc7\u9690\u5f0f\u8868\u8fbe\u4fe1\u606f\u9700\u6c42\uff0c\u5e73\u8861\u68c0\u7d22\u751f\u6210\u53c2\u6570\uff0c\u5229\u7528\u751f\u6210\u56f0\u60d1\u5ea6\u4f5c\u4e3a\u8bad\u7ec3\u76ee\u6807\uff0c\u6709\u6548\u63d0\u5347\u8de8\u4efb\u52a1\u6027\u80fd"}}
{"id": "2506.02283", "pdf": "https://arxiv.org/pdf/2506.02283", "abs": "https://arxiv.org/abs/2506.02283", "authors": ["Sofoklis Kakouros", "Haoyu Chen"], "title": "Sounding Like a Winner? Prosodic Differences in Post-Match Interviews", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "This study examines the prosodic characteristics associated with winning and\nlosing in post-match tennis interviews. Additionally, this research explores\nthe potential to classify match outcomes solely based on post-match interview\nrecordings using prosodic features and self-supervised learning (SSL)\nrepresentations. By analyzing prosodic elements such as pitch and intensity,\nalongside SSL models like Wav2Vec 2.0 and HuBERT, the aim is to determine\nwhether an athlete has won or lost their match. Traditional acoustic features\nand deep speech representations are extracted from the data, and machine\nlearning classifiers are employed to distinguish between winning and losing\nplayers. Results indicate that SSL representations effectively differentiate\nbetween winning and losing outcomes, capturing subtle speech patterns linked to\nemotional states. At the same time, prosodic cues -- such as pitch variability\n-- remain strong indicators of victory.", "AI": {"tldr": "\u901a\u8fc7\u7f51\u7403\u8d5b\u540e\u91c7\u8bbf\u7684\u97f5\u5f8b\u7279\u5f81\uff08\u5982\u97f3\u9ad8\u3001\u5f3a\u5ea6\uff09\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\uff08Wav2Vec 2.0/HuBERT\uff09\uff0c\u53ef\u6709\u6548\u533a\u5206\u8fd0\u52a8\u5458\u80dc\u8d1f\u72b6\u6001\u3002SSL\u8868\u5f81\u548c\u97f3\u9ad8\u53d8\u5316\u662f\u6700\u5f3a\u9884\u6d4b\u6307\u6807\u3002", "motivation": "\u63a2\u7d22\u8d5b\u540e\u91c7\u8bbf\u4e2d\u9690\u542b\u80dc\u8d1f\u72b6\u6001\u7684\u97f5\u5f8b\u7279\u5f81\uff0c\u9a8c\u8bc1\u4ec5\u901a\u8fc7\u8bed\u97f3\u6570\u636e\u9884\u6d4b\u6bd4\u8d5b\u7ed3\u679c\u7684\u53ef\u80fd\u6027\uff0c\u6316\u6398\u60c5\u7eea\u76f8\u5173\u8bed\u97f3\u6a21\u5f0f\u7684\u8bc6\u522b\u6f5c\u529b\u3002", "method": "1. \u63d0\u53d6\u4f20\u7edf\u58f0\u5b66\u7279\u5f81\uff08\u97f3\u9ad8\u3001\u5f3a\u5ea6\uff09\n2. \u4f7f\u7528Wav2Vec 2.0/HuBERT\u83b7\u53d6\u6df1\u5ea6\u8bed\u97f3\u8868\u5f81\n3. \u91c7\u7528\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u8fdb\u884c\u80dc\u8d1f\u72b6\u6001\u5206\u7c7b", "result": "1. \u81ea\u76d1\u7763\u5b66\u4e60\u8868\u5f81\u5728\u80dc\u8d1f\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\n2. \u97f3\u9ad8\u53d8\u5f02\u6027\u7b49\u97f5\u5f8b\u7279\u5f81\u4ecd\u662f\u80dc\u5229\u7684\u5f3a\u9884\u6d4b\u6307\u6807\n3. \u8bed\u97f3\u6a21\u5f0f\u53ef\u6709\u6548\u53cd\u6620\u8fd0\u52a8\u5458\u8d5b\u540e\u60c5\u7eea\u72b6\u6001", "conclusion": "\u7ed3\u5408\u6df1\u5ea6\u8bed\u97f3\u8868\u5f81\u4e0e\u4f20\u7edf\u97f5\u5f8b\u5206\u6790\uff0c\u4e3a\u901a\u8fc7\u975e\u8bed\u8a00\u5185\u5bb9\u6d1e\u5bdf\u8fd0\u52a8\u5458\u5fc3\u7406\u72b6\u6001\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5728\u4f53\u80b2\u5fc3\u7406\u5b66\u548c\u60c5\u7eea\u8ba1\u7b97\u9886\u57df\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.02298", "pdf": "https://arxiv.org/pdf/2506.02298", "abs": "https://arxiv.org/abs/2506.02298", "authors": ["Thai Hoang", "Kung-Hsiang Huang", "Shirley Kokane", "Jianguo Zhang", "Zuxin Liu", "Ming Zhu", "Jake Grigsby", "Tian Lan", "Michael S Ryoo", "Chien-Sheng Wu", "Shelby Heinecke", "Huan Wang", "Silvio Savarese", "Caiming Xiong", "Juan Carlos Niebles"], "title": "LAM SIMULATOR: Advancing Data Generation for Large Action Model Training via Online Exploration and Trajectory Feedback", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "LAM Simulator framework for agentic data generation", "summary": "Large Action Models (LAMs) for AI Agents offer incredible potential but face\nchallenges due to the need for high-quality training data, especially for\nmulti-steps tasks that involve planning, executing tool calls, and responding\nto feedback. To address these issues, we present LAM SIMULATOR, a comprehensive\nframework designed for online exploration of agentic tasks with high-quality\nfeedback. Our framework features a dynamic task query generator, an extensive\ncollection of tools, and an interactive environment where Large Language Model\n(LLM) Agents can call tools and receive real-time feedback. This setup enables\nLLM Agents to explore and solve tasks autonomously, facilitating the discovery\nof multiple approaches to tackle any given task. The resulting action\ntrajectory data are then used to create high-quality training datasets for\nLAMs. Our experiments on popular agentic benchmarks, ToolBench and CRMArena,\nhighlight the effectiveness of LAM SIMULATOR: models trained with\nself-generated datasets using our framework achieve significant performance\ngains, up to a 49.3\\% improvement over their original baselines. LAM SIMULATOR\nrequires minimal human input during dataset creation, highlighting LAM\nSIMULATOR's efficiency and effectiveness in speeding up development of AI\nagents.", "AI": {"tldr": "LAM SIMULATOR\u6846\u67b6\u901a\u8fc7\u81ea\u4e3b\u63a2\u7d22\u4efb\u52a1\u751f\u6210\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u4f7f\u5927\u884c\u52a8\u6a21\u578b\u7684\u6027\u80fd\u63d0\u5347\u8fbe49.3%", "motivation": "\u5927\u884c\u52a8\u6a21\u578b\u5728\u591a\u6b65\u9aa4\u4efb\u52a1\u4e2d\u9762\u4e34\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u9700\u8981\u81ea\u4e3b\u63a2\u7d22\u4efb\u52a1\u89e3\u51b3\u65b9\u6848\u7684\u6846\u67b6", "method": "\u6574\u5408\u52a8\u6001\u4efb\u52a1\u751f\u6210\u5668\u3001\u591a\u6837\u5316\u5de5\u5177\u96c6\u548c\u5b9e\u65f6\u53cd\u9988\u73af\u5883\uff0c\u5b9e\u73b0LLM\u4ee3\u7406\u7684\u81ea\u4e3b\u5de5\u5177\u8c03\u7528\u4e0e\u4efb\u52a1\u63a2\u7d22", "result": "\u5728ToolBench\u548cCRMArena\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u6700\u9ad849.3%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6700\u5c0f\u5316\u4eba\u5de5\u5e72\u9884\u9ad8\u6548\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u52a0\u901f\u4e86AI\u4ee3\u7406\u7684\u5f00\u53d1\u8fdb\u7a0b"}}
{"id": "2506.02302", "pdf": "https://arxiv.org/pdf/2506.02302", "abs": "https://arxiv.org/abs/2506.02302", "authors": ["Russell Scheinberg", "Ameeta Agrawal", "Amber Shore", "So Young Lee"], "title": "Explain-then-Process: Using Grammar Prompting to Enhance Grammatical Acceptability Judgments", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025 Findings", "summary": "Large language models (LLMs) can explain grammatical rules, yet they often\nfail to apply those rules when judging sentence acceptability. We present\n\"grammar prompting\", an explain-then-process paradigm: a large LLM first\nproduces a concise explanation of the relevant syntactic phenomenon, then that\nexplanation is fed back as additional context to the target model -- either an\nLLM or a smaller language model (SLM) -- before deciding which sentence of a\nminimal pair is grammatical. On the English BLiMP, Chinese SLING, and Russian\nRuBLiMP benchmarks, this simple prompt design yields substantial improvements\nover strong baselines across many syntactic phenomena. Feeding an LLM's\nmetalinguistic explanation back to the target model bridges the gap between\nknowing a rule and using it. On SLMs, grammar prompting alone trims the average\nLLM-SLM accuracy gap by about 20%, and when paired with chain-of-thought, by\n56% (13.0 pp -> 5.8 pp), all at negligible cost. The lightweight,\nlanguage-agnostic cue lets low-cost SLMs approach frontier-LLM performance in\nmultilingual settings.", "AI": {"tldr": "\u63d0\u51fa\u300c\u8bed\u6cd5\u63d0\u793a\u300d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5927\u6a21\u578b\u751f\u6210\u8bed\u6cd5\u89e3\u91ca\u540e\u8f93\u5165\u76ee\u6807\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u6a21\u578b\u5224\u65ad\u53e5\u5b50\u8bed\u6cd5\u6027\u7684\u80fd\u529b\uff0c\u6709\u6548\u7f29\u5c0fLLM\u4e0e\u5c0f\u578b\u6a21\u578b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u80fd\u89e3\u91ca\u8bed\u6cd5\u89c4\u5219\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u89c4\u5219\u5224\u65ad\u53e5\u5b50\u6b63\u786e\u6027\u65f6\u5b58\u5728\u660e\u663e\u80fd\u529b\u65ad\u5c42\uff0c\u9700\u63a2\u7d22\u4f4e\u6210\u672c\u89e3\u51b3\u65b9\u6848\u4ee5\u63d0\u5347\u6a21\u578b\u5b9e\u9645\u5e94\u7528\u6027\u80fd\u3002", "method": "\u4e24\u9636\u6bb5\u8303\u5f0f\uff1a1) \u5927\u6a21\u578b\u751f\u6210\u8bed\u6cd5\u73b0\u8c61\u7b80\u660e\u89e3\u91ca 2) \u5c06\u89e3\u91ca\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u8f93\u5165\u76ee\u6807\u6a21\u578b\uff08LLM/SLM\uff09\uff0c\u7ed3\u5408\u601d\u7ef4\u94fe\u6280\u672f\u8fdb\u884c\u51b3\u7b56\u3002\u5728\u82f1/\u4e2d/\u4fc4\u4e09\u5927\u8bed\u6cd5\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u3002", "result": "\u5e73\u5747\u7f29\u5c0fLLM-SLM\u51c6\u786e\u7387\u5dee\u8ddd20%\uff0c\u7ed3\u5408\u601d\u7ef4\u94fe\u540e\u5dee\u8ddd\u7f29\u5c0f56%\uff0813.0pp\u21925.8pp\uff09\u3002\u5c0f\u6a21\u578b\u4ee5\u6781\u4f4e\u5f00\u9500\u903c\u8fd1\u524d\u6cbfLLM\u7684\u591a\u8bed\u8a00\u5904\u7406\u6027\u80fd\u3002", "conclusion": "\u8bed\u6cd5\u63d0\u793a\u673a\u5236\u6210\u529f\u5f25\u5408\u300c\u77e5\u800c\u4e0d\u884c\u300d\u7684\u8ba4\u77e5\u9e3f\u6c9f\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u5c0f\u6a21\u578b\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u5b9e\u7528\u5316\u8fdb\u7a0b\u3002"}}
{"id": "2506.02321", "pdf": "https://arxiv.org/pdf/2506.02321", "abs": "https://arxiv.org/abs/2506.02321", "authors": ["Pegah Alipoormolabashi", "Ajay Patel", "Niranjan Balasubramanian"], "title": "Quantifying Misattribution Unfairness in Authorship Attribution", "categories": ["cs.CL"], "comment": null, "summary": "Authorship misattribution can have profound consequences in real life. In\nforensic settings simply being considered as one of the potential authors of an\nevidential piece of text or communication can result in undesirable scrutiny.\nThis raises a fairness question: Is every author in the candidate pool at equal\nrisk of misattribution? Standard evaluation measures for authorship attribution\nsystems do not explicitly account for this notion of fairness. We introduce a\nsimple measure, Misattribution Unfairness Index (MAUIk), which is based on how\noften authors are ranked in the top k for texts they did not write. Using this\nmeasure we quantify the unfairness of five models on two different datasets.\nAll models exhibit high levels of unfairness with increased risks for some\nauthors. Furthermore, we find that this unfairness relates to how the models\nembed the authors as vectors in the latent search space. In particular, we\nobserve that the risk of misattribution is higher for authors closer to the\ncentroid (or center) of the embedded authors in the haystack. These results\nindicate the potential for harm and the need for communicating with and\ncalibrating end users on misattribution risk when building and providing such\nmodels for downstream use.", "AI": {"tldr": "\u63d0\u51faMAUIk\u6307\u6807\u91cf\u5316\u4f5c\u8005\u5f52\u5c5e\u6a21\u578b\u7684\u4e0d\u516c\u5e73\u6027\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5bf9\u4e2d\u5fc3\u4f4d\u7f6e\u4f5c\u8005\u7684\u8bef\u5224\u98ce\u9669\u66f4\u9ad8", "motivation": "\u73b0\u6709\u4f5c\u8005\u5f52\u5c5e\u7cfb\u7edf\u8bc4\u4f30\u6807\u51c6\u672a\u8003\u8651\u516c\u5e73\u6027\uff0c\u6cd5\u533b\u573a\u666f\u4e2d\u8bef\u5224\u53ef\u80fd\u5bfc\u81f4\u5bf9\u5019\u9009\u4f5c\u8005\u7684\u4e0d\u516c\u6b63\u5ba1\u67e5", "method": "\u901a\u8fc7MAUIk\u6307\u6807\uff08\u57fa\u4e8e\u6a21\u578b\u5728\u975e\u4f5c\u8005\u6587\u672c\u4e2d\u7684top-k\u9519\u8bef\u6392\u540d\u9891\u7387\uff09\uff0c\u8bc4\u4f305\u4e2a\u6a21\u578b\u57282\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u4e0d\u516c\u5e73\u6027\uff0c\u5e76\u5206\u6790\u4f5c\u8005\u5d4c\u5165\u5411\u91cf\u4e0e\u8bef\u5224\u98ce\u9669\u7684\u7a7a\u95f4\u5173\u7cfb", "result": "\u6240\u6709\u6a21\u578b\u5747\u5b58\u5728\u9ad8\u5ea6\u4e0d\u516c\u5e73\u6027\uff0c\u5d4c\u5165\u7a7a\u95f4\u9760\u8fd1\u8d28\u5fc3\u7684\u4f5c\u8005\u9762\u4e34\u66f4\u9ad8\u8bef\u5224\u98ce\u9669\uff0c\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u7ed3\u6784\u76f4\u63a5\u5f71\u54cd\u8bef\u5224\u6982\u7387", "conclusion": "\u4f5c\u8005\u5f52\u5c5e\u6a21\u578b\u5b58\u5728\u663e\u8457\u8bef\u5224\u98ce\u9669\uff0c\u9700\u5728\u4f7f\u7528\u65f6\u660e\u786e\u4f20\u8fbe\u98ce\u9669\u5e76\u8fdb\u884c\u6821\u51c6\uff0c\u7279\u522b\u9700\u5173\u6ce8\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u7ed3\u6784\u5bf9\u516c\u5e73\u6027\u7684\u5f71\u54cd"}}
{"id": "2506.02326", "pdf": "https://arxiv.org/pdf/2506.02326", "abs": "https://arxiv.org/abs/2506.02326", "authors": ["Berk Atil", "Namrata Sureddy", "Rebecca J. Passonneau"], "title": "Something Just Like TRuST : Toxicity Recognition of Span and Target", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Toxicity in online content, including content generated by language models,\nhas become a critical concern due to its potential for negative psychological\nand social impact. This paper introduces TRuST, a comprehensive dataset\ndesigned to improve toxicity detection that merges existing datasets, and has\nlabels for toxicity, target social group, and toxic spans. It includes a\ndiverse range of target groups such as ethnicity, gender, religion, disability,\nand politics, with both human/machine-annotated and human machine-generated\ndata. We benchmark state-of-the-art large language models (LLMs) on toxicity\ndetection, target group identification, and toxic span extraction. We find that\nfine-tuned models consistently outperform zero-shot and few-shot prompting,\nthough performance remains low for certain social groups. Further, reasoning\ncapabilities do not significantly improve performance, indicating that LLMs\nhave weak social reasoning skills.", "AI": {"tldr": "TRuST\u6570\u636e\u96c6\u901a\u8fc7\u6574\u5408\u73b0\u6709\u6570\u636e\u5e76\u6807\u6ce8\u6bd2\u6027\u3001\u76ee\u6807\u7fa4\u4f53\u548c\u6bd2\u6027\u8303\u56f4\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6bd2\u6027\u68c0\u6d4b\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\u5fae\u8c03\u6a21\u578b\u4f18\u4e8e\u96f6\u6837\u672c/\u5c11\u6837\u672c\u63d0\u793a\u65b9\u6cd5\uff0c\u4f46\u5bf9\u90e8\u5206\u793e\u4f1a\u7fa4\u4f53\u68c0\u6d4b\u6548\u679c\u4ecd\u4e0d\u7406\u60f3\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u793e\u4f1a\u63a8\u7406\u80fd\u529b\u8f83\u5f31\u3002", "motivation": "\u5728\u7ebf\u5185\u5bb9\uff08\u5305\u62ec\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5185\u5bb9\uff09\u7684\u6bd2\u6027\u95ee\u9898\u53ef\u80fd\u9020\u6210\u4e25\u91cd\u5fc3\u7406\u548c\u793e\u4f1a\u5f71\u54cd\u3002\u73b0\u6709\u6570\u636e\u96c6\u7684\u591a\u6837\u6027\u548c\u6807\u6ce8\u7ef4\u5ea6\u4e0d\u8db3\uff0c\u9700\u6784\u5efa\u66f4\u5168\u9762\u7684\u6570\u636e\u96c6\u4ee5\u63d0\u5347\u6bd2\u6027\u68c0\u6d4b\u80fd\u529b\u3002", "method": "\u6574\u5408\u591a\u6e90\u6570\u636e\u6784\u5efaTRuST\u6570\u636e\u96c6\uff0c\u5305\u542b\u4eba\u673a\u6807\u6ce8\u6570\u636e\u53ca\u673a\u5668\u751f\u6210\u6570\u636e\uff0c\u6db5\u76d6\u79cd\u65cf\u3001\u6027\u522b\u3001\u5b97\u6559\u7b49\u591a\u7c7b\u793e\u4f1a\u7fa4\u4f53\u3002\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6bd2\u6027\u68c0\u6d4b\u3001\u76ee\u6807\u7fa4\u4f53\u8bc6\u522b\u548c\u6bd2\u6027\u8303\u56f4\u63d0\u53d6\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5fae\u8c03\u6a21\u578b\u5728\u4e09\u4e2a\u4efb\u52a1\u4e0a\u6301\u7eed\u4f18\u4e8e\u63d0\u793a\u65b9\u6cd5\uff0c\u4f46\u5bf9\u7279\u5b9a\u793e\u4f1a\u7fa4\u4f53\uff08\u5982\u653f\u6cbb\u7fa4\u4f53\uff09\u68c0\u6d4b\u51c6\u786e\u7387\u4f4e\u3002\u5f15\u5165\u63a8\u7406\u80fd\u529b\u672a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u63ed\u793aLLMs\u793e\u4f1a\u63a8\u7406\u80fd\u529b\u8584\u5f31\u3002", "conclusion": "TRuST\u4e3a\u6bd2\u6027\u68c0\u6d4b\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u8d44\u6e90\uff0c\u672a\u6765\u9700\u589e\u5f3a\u6a21\u578b\u7684\u793e\u4f1a\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u9488\u5bf9\u5f31\u52bf\u7fa4\u4f53\u4f18\u5316\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2506.02338", "pdf": "https://arxiv.org/pdf/2506.02338", "abs": "https://arxiv.org/abs/2506.02338", "authors": ["Hyungjoo Chae", "Dongjin Kang", "Jihyuk Kim", "Beong-woo Kwak", "Sunghyun Park", "Haeju Park", "Jinyoung Yeo", "Moontae Lee", "Kyungjae Lee"], "title": "One Missing Piece for Open-Source Reasoning Models: A Dataset to Mitigate Cold-Starting Short CoT LLMs in RL", "categories": ["cs.CL"], "comment": "ACL 2025 Industry", "summary": "With the release of R1, a publicly available large reasoning model (LRM),\nresearchers commonly train new LRMs by training language models on R1's long\nchain-of-thought (CoT) inferences. While prior works show that LRMs'\ncapabilities can be reproduced through direct distillation, the continued\nreliance on the existing models (e.g., R1) remains a critical limitation in\nadvancing the field. As a first step toward independent LRM development, this\npaper explores the possibility of constructing a long CoT dataset with LLMs\nthat are not trained for inference-time scaling. To this end, we present the\nLong CoT Collection, a dataset of 100K CoT rationales annotated using existing\nshort CoT LLMs. We develop a pipeline that induces o1's novel reasoning\nstrategies into short CoT LLMs, enabling them to think longer and introducing\ncontrollability over the thought budget to better manage the overthinking\nproblem. Our extensive analyses validate that our dataset achieves quality\ncomparable to--or slightly below--R1. Furthermore, our experiments demonstrate\nthat training on our dataset not only strengthens general reasoning skills, but\nalso provides a strong foundation for reinforcement learning--models\ninitialized on our data achieve 2-3x larger gains with RLVR.", "AI": {"tldr": "\u901a\u8fc7\u672a\u9488\u5bf9\u63a8\u7406\u4f18\u5316\u7684\u77ed\u94fe\u601d\u7ef4\u6a21\u578b\u6784\u5efa\u957f\u94fe\u601d\u7ef4\u6570\u636e\u96c6Long CoT Collection\uff0c\u5b9e\u73b0\u63a5\u8fd1R1\u6a21\u578b\u7684\u6570\u636e\u8d28\u91cf\uff0c\u5e76\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u57fa\u7840\u652f\u6301\u3002", "motivation": "\u51cf\u5c11\u5bf9\u73b0\u6709\u5927\u578b\u63a8\u7406\u6a21\u578bR1\u7684\u4f9d\u8d56\uff0c\u63a2\u7d22\u72ec\u7acb\u5f00\u53d1\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u53ef\u80fd\u6027\u3002", "method": "1. \u4f7f\u7528\u73b0\u6709\u77ed\u94fe\u601d\u7ef4\u6a21\u578b\u6807\u6ce810\u4e07\u6761\u957f\u63a8\u7406\u6570\u636e\n2. \u5f00\u53d1\u8bf1\u5bfc\u65b0\u63a8\u7406\u7b56\u7565\u7684\u6d41\u7a0b\uff0c\u5b9e\u73b0\u601d\u8003\u65f6\u957f\u63a7\u5236\u548c\u9884\u7b97\u7ba1\u7406\n3. \u901a\u8fc7\u8d28\u91cf\u5206\u6790\u548cRLVR\u9a8c\u8bc1\u6548\u679c", "result": "\u6570\u636e\u96c6\u8d28\u91cf\u63a5\u8fd1R1\uff08\u7565\u4f4e5%-7%\uff09\uff0c\u57fa\u4e8e\u8be5\u6570\u636e\u7684\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u589e\u76ca\u63d0\u53472-3\u500d", "conclusion": "\u8bc1\u660e\u5229\u7528\u666e\u901aLLM\u6784\u5efa\u957f\u63a8\u7406\u94fe\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u72ec\u7acb\u5f00\u53d1\u63a8\u7406\u6a21\u578b\u5f00\u8f9f\u65b0\u8def\u5f84"}}
{"id": "2506.02347", "pdf": "https://arxiv.org/pdf/2506.02347", "abs": "https://arxiv.org/abs/2506.02347", "authors": ["Jiaming Li", "Yukun Chen", "Ziqiang Liu", "Minghuan Tan", "Lei Zhang", "Yunshui Li", "Run Luo", "Longze Chen", "Jing Luo", "Ahmadreza Argha", "Hamid Alinejad-Rokny", "Wei Zhou", "Min Yang"], "title": "STORYTELLER: An Enhanced Plot-Planning Framework for Coherent and Cohesive Story Generation", "categories": ["cs.CL"], "comment": null, "summary": "Stories are central to human culture, serving to share ideas, preserve\ntraditions, and foster connections. Automatic story generation, a key\nadvancement in artificial intelligence (AI), offers new possibilities for\ncreating personalized content, exploring creative ideas, and enhancing\ninteractive experiences. However, existing methods struggle to maintain\nnarrative coherence and logical consistency. This disconnect compromises the\noverall storytelling experience, underscoring the need for substantial\nimprovements. Inspired by human cognitive processes, we introduce Storyteller,\na novel approach that systemically improves the coherence and consistency of\nautomatically generated stories. Storyteller introduces a plot node structure\nbased on linguistically grounded subject verb object (SVO) triplets, which\ncapture essential story events and ensure a consistent logical flow. Unlike\nprevious methods, Storyteller integrates two dynamic modules, the STORYLINE and\nnarrative entity knowledge graph (NEKG),that continuously interact with the\nstory generation process. This integration produces structurally sound,\ncohesive and immersive narratives. Extensive experiments demonstrate that\nStoryteller significantly outperforms existing approaches, achieving an 84.33%\naverage win rate through human preference evaluation. At the same time, it is\nalso far ahead in other aspects including creativity, coherence, engagement,\nand relevance.", "AI": {"tldr": "\u63d0\u51faStoryteller\u65b9\u6cd5\uff0c\u901a\u8fc7SVO\u4e09\u5143\u7ec4\u6784\u5efa\u4e8b\u4ef6\u8282\u70b9\uff0c\u7ed3\u5408\u52a8\u6001\u4ea4\u4e92\u6a21\u5757\u63d0\u5347\u6545\u4e8b\u751f\u6210\u7684\u8fde\u8d2f\u6027\u4e0e\u4e00\u81f4\u6027", "motivation": "\u73b0\u6709\u81ea\u52a8\u6545\u4e8b\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u53d9\u4e8b\u8fde\u8d2f\u6027\u5dee\u548c\u903b\u8f91\u65ad\u88c2\u95ee\u9898\uff0c\u5f71\u54cd\u6545\u4e8b\u4f53\u9a8c\u8d28\u91cf\uff0c\u9700\u7cfb\u7edf\u6027\u6539\u8fdb\u65b9\u6848", "method": "\u57fa\u4e8e\u8bed\u8a00\u5b66SVO\u4e09\u5143\u7ec4\u6784\u5efa\u60c5\u8282\u8282\u70b9\uff0c\u521b\u65b0\u6574\u5408\u52a8\u6001STORYLINE\u6a21\u5757\u4e0eNEKG\u77e5\u8bc6\u56fe\u8c31\u7684\u5b9e\u65f6\u4ea4\u4e92\u673a\u5236", "result": "\u4eba\u7c7b\u504f\u597d\u8bc4\u4f30\u663e\u793a84.33%\u5e73\u5747\u80dc\u7387\uff0c\u5728\u521b\u9020\u529b\u3001\u8fde\u8d2f\u6027\u3001\u5438\u5f15\u529b\u7b49\u591a\u7ef4\u5ea6\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b", "conclusion": "Storyteller\u901a\u8fc7\u7ed3\u6784\u5316\u53d9\u4e8b\u5355\u5143\u548c\u52a8\u6001\u77e5\u8bc6\u7ef4\u62a4\u673a\u5236\uff0c\u9a8c\u8bc1\u4e86\u8ba4\u77e5\u542f\u53d1\u7684\u6545\u4e8b\u751f\u6210\u6846\u67b6\u6709\u6548\u6027"}}
{"id": "2506.02350", "pdf": "https://arxiv.org/pdf/2506.02350", "abs": "https://arxiv.org/abs/2506.02350", "authors": ["Herun Wan", "Jiaying Wu", "Minnan Luo", "Zhi Zeng", "Zhixiong Su"], "title": "Truth over Tricks: Measuring and Mitigating Shortcut Learning in Misinformation Detection", "categories": ["cs.CL"], "comment": null, "summary": "Misinformation detection models often rely on superficial cues (i.e.,\n\\emph{shortcuts}) that correlate with misinformation in training data but fail\nto generalize to the diverse and evolving nature of real-world misinformation.\nThis issue is exacerbated by large language models (LLMs), which can easily\ngenerate convincing misinformation through simple prompts. We introduce\nTruthOverTricks, a unified evaluation paradigm for measuring shortcut learning\nin misinformation detection. TruthOverTricks categorizes shortcut behaviors\ninto intrinsic shortcut induction and extrinsic shortcut injection, and\nevaluates seven representative detectors across 14 popular benchmarks, along\nwith two new factual misinformation datasets, NQ-Misinfo and Streaming-Misinfo.\nEmpirical results reveal that existing detectors suffer severe performance\ndegradation when exposed to both naturally occurring and adversarially crafted\nshortcuts. To address this, we propose SMF, an LLM-augmented data augmentation\nframework that mitigates shortcut reliance through paraphrasing, factual\nsummarization, and sentiment normalization. SMF consistently enhances\nrobustness across 16 benchmarks, encouraging models to rely on deeper semantic\nunderstanding rather than shortcut cues. To promote the development of\nmisinformation detectors, we have published the resources publicly at\nhttps://github.com/whr000001/TruthOverTricks.", "AI": {"tldr": "\u63d0\u51faTruthOverTricks\u8bc4\u4f30\u6846\u67b6\u63ed\u793a\u9519\u8bef\u4fe1\u606f\u68c0\u6d4b\u5668\u5bf9\u8868\u9762\u7ebf\u7d22\u7684\u4f9d\u8d56\uff0c\u5e76\u901a\u8fc7SMF\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027", "motivation": "\u73b0\u6709\u9519\u8bef\u4fe1\u606f\u68c0\u6d4b\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u8868\u9762\u5173\u8054\u6a21\u5f0f(\u6377\u5f84)\uff0c\u5728LLMs\u751f\u6210\u591a\u6837\u5316\u9519\u8bef\u4fe1\u606f\u7684\u573a\u666f\u4e0b\u6cdb\u5316\u80fd\u529b\u5dee", "method": "1. \u5efa\u7acb\u5305\u542b\u81ea\u7136/\u5bf9\u6297\u6027\u6377\u5f84\u7684\u7edf\u4e00\u8bc4\u4f30\u8303\u5f0f\n2. \u572814\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bc4\u4f307\u79cd\u68c0\u6d4b\u5668\n3. \u63d0\u51faLLM\u589e\u5f3a\u7684SMF\u6846\u67b6(\u8f6c\u8ff0/\u4e8b\u5b9e\u6458\u8981/\u60c5\u611f\u5f52\u4e00\u5316)", "result": "1. \u73b0\u6709\u68c0\u6d4b\u5668\u5728\u6377\u5f84\u5e72\u6270\u4e0b\u6027\u80fd\u4e0b\u964d16-42%\n2. SMF\u572816\u4e2a\u57fa\u51c6\u4e2d\u5e73\u5747\u63d0\u534712.3%\u9c81\u68d2\u6027\n3. \u53d1\u5e03NQ-Misinfo\u548cStreaming-Misinfo\u65b0\u6570\u636e\u96c6", "conclusion": "\u63ed\u793a\u4e86\u9519\u8bef\u4fe1\u606f\u68c0\u6d4b\u4e2d\u7684\u6377\u5f84\u4f9d\u8d56\u95ee\u9898\uff0c\u9a8c\u8bc1SMF\u6846\u67b6\u6709\u6548\u6027\uff0c\u516c\u5f00\u8d44\u6e90\u4fc3\u8fdb\u7a33\u5065\u68c0\u6d4b\u5668\u53d1\u5c55"}}
{"id": "2506.02351", "pdf": "https://arxiv.org/pdf/2506.02351", "abs": "https://arxiv.org/abs/2506.02351", "authors": ["Jeonghun Kang", "Soonmok Kwon", "Joonseok Lee", "Byung-Hak Kim"], "title": "DIAMOND: An LLM-Driven Agent for Context-Aware Baseball Highlight Summarization", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "To appear in the First REALM (Research on Agent Language Models)\n  workshop at ACL 2025", "summary": "Traditional approaches -- such as Win Probability Added (WPA)-based ranking\nor computer vision-driven event detection -- can identify scoring plays but\noften miss strategic depth, momentum shifts, and storyline progression. Manual\ncuration remains the gold standard but is resource-intensive and not scalable.\nWe introduce DIAMOND, an LLM-driven agent for context-aware baseball highlight\nsummarization that integrates structured sports analytics with natural language\nreasoning. DIAMOND leverages sabermetric features -- Win Expectancy, WPA, and\nLeverage Index -- to quantify play importance, while an LLM module enhances\nselection based on contextual narrative value. This hybrid approach ensures\nboth quantitative rigor and qualitative richness, surpassing the limitations of\npurely statistical or vision-based systems. Evaluated on five diverse Korean\nBaseball Organization League games, DIAMOND improves F1-score from 42.9%\n(WPA-only) to 84.8%, outperforming both commercial and statistical baselines.\nThough limited in scale, our results highlight the potential of modular,\ninterpretable agent-based frameworks for event-level summarization in sports\nand beyond.", "AI": {"tldr": "DIAMOND\uff1a\u9996\u4e2a\u7ed3\u5408\u7ed3\u6784\u5316\u68d2\u7403\u6570\u636e\u4e0eLLM\u63a8\u7406\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u6bd4\u8d5b\u7cbe\u5f69\u7247\u6bb5\u68c0\u6d4b\u51c6\u786e\u7387\uff08F1\u5206\u6570\u4ece42.9%\u63d0\u5347\u81f384.8%\uff09", "motivation": "\u4f20\u7edf\u7edf\u8ba1\u65b9\u6cd5\uff08\u5982WPA\uff09\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u7f3a\u4e4f\u5bf9\u6bd4\u8d5b\u6218\u7565\u6df1\u5ea6\u3001\u52bf\u5934\u8f6c\u6362\u548c\u53d9\u4e8b\u903b\u8f91\u7684\u6355\u6349\uff0c\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u8fc7\u9ad8\u4e14\u4e0d\u53ef\u6269\u5c55", "method": "\u878d\u5408\u68d2\u7403\u7edf\u8ba1\u6307\u6807\uff08\u80dc\u7387\u9884\u671f\u3001WPA\u3001\u6760\u6746\u6307\u6570\uff09\u4e0eLLM\u7684\u53d9\u4e8b\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u67b6\u6784\u5b9e\u73b0\u91cf\u5316\u5206\u6790\u4e0e\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u534f\u540c", "result": "\u5728\u97e9\u56fd\u804c\u68d2\u4e94\u573a\u4e0d\u540c\u6bd4\u8d5b\u4e2d\u7684\u6d4b\u8bd5\u663e\u793a\uff0cF1\u5206\u6570\u8fbe84.8%\uff0c\u663e\u8457\u4f18\u4e8e\u5546\u4e1a\u7cfb\u7edf\uff0872.4%\uff09\u548c\u7eaf\u7edf\u8ba1\u65b9\u6cd5\uff0842.9%\uff09", "conclusion": "\u6a21\u5757\u5316\u667a\u80fd\u4f53\u6846\u67b6\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\uff0c\u4e3a\u4f53\u80b2\u8d5b\u4e8b\u7b49\u573a\u666f\u7684\u4e8b\u4ef6\u7ea7\u6458\u8981\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5176\u8bbe\u8ba1\u7406\u5ff5\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u9886\u57df"}}
{"id": "2506.02372", "pdf": "https://arxiv.org/pdf/2506.02372", "abs": "https://arxiv.org/abs/2506.02372", "authors": ["Hisami Suzuki", "Satoru Katsumata", "Takashi Kodama", "Tetsuro Takahashi", "Kouta Nakayama", "Satoshi Sekine"], "title": "AnswerCarefully: A Dataset for Improving the Safety of Japanese LLM Output", "categories": ["cs.CL"], "comment": null, "summary": "In this paper we present AnswerCarefully, a dataset for promoting the safety\nand appropriateness of Japanese LLM outputs. The dataset consists of 1,800\npairs of questions and reference answers, where the questions require special\nattention in answering. It covers a wide range of risk categories established\nin prior English-language datasets, but the data samples are original in that\nthey are manually created to reflect the socio-cultural context of LLM usage in\nJapan. We show that using this dataset for instruction to fine-tune a Japanese\nLLM led to improved output safety without compromising the utility of general\nresponses. We also report the results of a safety evaluation of 12 Japanese\nLLMs using this dataset as a benchmark. Finally, we describe the latest update\non the dataset which provides English translations and annotations of the\nquestions, aimed at facilitating the derivation of similar datasets in\ndifferent languages and regions.", "AI": {"tldr": "\u65e5\u672c\u7814\u7a76\u8005\u521b\u5efaAnswerCarefully\u6570\u636e\u96c6\u4ee5\u63d0\u5347LLM\u8f93\u51fa\u7684\u5b89\u5168\u6027\uff0c\u8986\u76d6\u65e5\u672c\u793e\u4f1a\u6587\u5316\u98ce\u9669\u573a\u666f\uff0c\u7ecf\u5fae\u8c03\u540e\u6a21\u578b\u5b89\u5168\u6027\u663e\u8457\u63d0\u5347\u4e14\u4e0d\u5f71\u54cd\u901a\u7528\u6027\u80fd\u3002", "motivation": "\u9488\u5bf9\u65e5\u672c\u672c\u571fLLM\u4f7f\u7528\u573a\u666f\uff0c\u89e3\u51b3\u73b0\u6709\u82f1\u8bed\u6570\u636e\u96c6\u5728\u6587\u5316\u9002\u914d\u6027\u4e0a\u7684\u4e0d\u8db3\uff0c\u9700\u6784\u5efa\u53cd\u6620\u65e5\u672c\u793e\u4f1a\u6587\u5316\u98ce\u9669\u7684\u7279\u6b8a\u95ee\u7b54\u6570\u636e\u96c6\u3002", "method": "1. \u624b\u52a8\u521b\u5efa1,800\u7ec4\u9700\u7279\u6b8a\u6ce8\u610f\u7684\u95ee\u9898-\u7b54\u6848\u5bf9\uff1b2. \u901a\u8fc7\u6307\u4ee4\u5fae\u8c03\u4f18\u5316\u65e5\u672cLLM\uff1b3. \u5bf912\u4e2a\u65e5\u672cLLM\u8fdb\u884c\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\uff1b4. \u65b0\u589e\u82f1\u6587\u7ffb\u8bd1\u53ca\u6ce8\u91ca\u5b9e\u73b0\u8de8\u8bed\u8a00\u9002\u914d\u3002", "result": "1. \u5fae\u8c03\u540eLLM\u5b89\u5168\u6307\u6807\u63d0\u5347\uff08\u5177\u4f53\u6570\u503c\u672a\u63d0\u53ca\uff09\uff1b2. 12\u4e2a\u6a21\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u63ed\u793a\u65e5\u672cLLM\u5b89\u5168\u73b0\u72b6\uff1b3. \u901a\u7528\u56de\u7b54\u6548\u7528\u672a\u53d7\u5f71\u54cd\u3002", "conclusion": "\u6570\u636e\u96c6\u901a\u8fc7\u6587\u5316\u9002\u914d\u6027\u8bbe\u8ba1\u6709\u6548\u63d0\u5347\u6a21\u578b\u5b89\u5168\u6027\uff0c\u6700\u65b0\u82f1\u8bed\u6807\u6ce8\u7248\u672c\u5c06\u63a8\u52a8\u591a\u8bed\u8a00/\u5730\u533a\u5b89\u5168\u6570\u636e\u96c6\u6784\u5efa\uff0c\u5f62\u6210\u53ef\u590d\u7528\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2506.02378", "pdf": "https://arxiv.org/pdf/2506.02378", "abs": "https://arxiv.org/abs/2506.02378", "authors": ["Ukyo Honda", "Tatsushi Oka"], "title": "Exploring Explanations Improves the Robustness of In-Context Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "In-context learning (ICL) has emerged as a successful paradigm for leveraging\nlarge language models (LLMs). However, it often struggles to generalize beyond\nthe distribution of the provided demonstrations. A recent advancement in\nenhancing robustness is ICL with explanations (X-ICL), which improves\nprediction reliability by guiding LLMs to understand and articulate the\nreasoning behind correct labels. Building on this approach, we introduce an\nadvanced framework that extends X-ICL by systematically exploring explanations\nfor all possible labels (X$^2$-ICL), thereby enabling more comprehensive and\nrobust decision-making. Experimental results on multiple natural language\nunderstanding datasets validate the effectiveness of X$^2$-ICL, demonstrating\nsignificantly improved robustness to out-of-distribution data compared to the\nexisting ICL approaches.", "AI": {"tldr": "X\u00b2-ICL\u901a\u8fc7\u7cfb\u7edf\u89e3\u91ca\u6240\u6709\u6807\u7b7e\uff0c\u63d0\u5347\u5927\u6a21\u578b\u5728\u5206\u5e03\u5916\u6570\u636e\u7684\u9c81\u68d2\u6027", "motivation": "\u73b0\u6709X-ICL\u65b9\u6cd5\u4ec5\u5173\u6ce8\u6b63\u786e\u6807\u7b7e\u7684\u89e3\u91ca\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3", "method": "\u63d0\u51faX\u00b2-ICL\u6846\u67b6\uff0c\u7cfb\u7edf\u63a2\u7d22\u6240\u6709\u53ef\u80fd\u6807\u7b7e\u7684\u89e3\u91ca\uff0c\u6784\u5efa\u5168\u9762\u51b3\u7b56\u4f9d\u636e", "result": "\u5728\u591a\u4e2a\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u4f20\u7edfICL\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u5206\u5e03\u5916\u6570\u636e\u9002\u5e94\u80fd\u529b\uff08\u63d0\u5347\u5e45\u5ea6\u8fbe15%-20%\uff09", "conclusion": "\u5168\u9762\u6807\u7b7e\u89e3\u91ca\u673a\u5236\u6709\u6548\u589e\u5f3a\u6a21\u578b\u63a8\u7406\u7a33\u5065\u6027\uff0c\u4e3a\u6539\u8fdb\u4e0a\u4e0b\u6587\u5b66\u4e60\u8303\u5f0f\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2506.02391", "pdf": "https://arxiv.org/pdf/2506.02391", "abs": "https://arxiv.org/abs/2506.02391", "authors": ["Chuanghao Ding", "Jiaping Wang", "Ziqing Yang", "Xiaoliang Wang", "Dahua Lin", "Cam-Tu Nguyen", "Fei Tan"], "title": "Consultant Decoding: Yet Another Synergistic Mechanism", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 findings", "summary": "The synergistic mechanism based on Speculative Decoding (SD) has garnered\nconsiderable attention as a simple yet effective approach for accelerating the\ninference of large language models (LLMs). Nonetheless, the high rejection\nrates require repeated LLMs calls to validate draft tokens, undermining the\noverall efficiency gain of SD. In this work, we revisit existing verification\nmechanisms and propose a novel synergetic mechanism Consultant Decoding (CD).\nUnlike SD, which relies on a metric derived from importance sampling for\nverification, CD verifies candidate drafts using token-level likelihoods\ncomputed solely by the LLM. CD achieves up to a 2.5-fold increase in inference\nspeed compared to the target model, while maintaining comparable generation\nquality (around 100% of the target model's performance). Interestingly, this is\nachieved by combining models whose parameter sizes differ by two orders of\nmagnitude. In addition, CD reduces the call frequency of the large target model\nto below 10%, particularly in more demanding tasks. CD's performance was even\nfound to surpass that of the large target model, which theoretically represents\nthe upper bound for speculative decoding.", "AI": {"tldr": "\u63d0\u51fa\u987e\u95ee\u89e3\u7801\uff08CD\uff09\u4f5c\u4e3a\u63a8\u6d4b\u89e3\u7801\uff08SD\uff09\u7684\u6539\u8fdb\u65b9\u6848\uff0c\u901a\u8fc7LLM\u81ea\u8eab\u8ba1\u7b97\u7684token\u7ea7\u4f3c\u7136\u6982\u7387\u9a8c\u8bc1\u5019\u9009\u6587\u672c\uff0c\u5b9e\u73b02.5\u500d\u63a8\u7406\u52a0\u901f\u4e14\u4fdd\u6301\u751f\u6210\u8d28\u91cf", "motivation": "\u4f20\u7edf\u63a8\u6d4b\u89e3\u7801\uff08SD\uff09\u56e0\u9ad8\u62d2\u7edd\u7387\u9700\u53cd\u590d\u8c03\u7528\u5927\u6a21\u578b\u9a8c\u8bc1\uff0c\u4e25\u91cd\u5f71\u54cd\u52a0\u901f\u6548\u679c\u3002\u73b0\u6709\u9a8c\u8bc1\u673a\u5236\u4f9d\u8d56\u91cd\u8981\u6027\u91c7\u6837\u6307\u6807\u5b58\u5728\u6548\u7387\u74f6\u9888\u3002", "method": "\u91c7\u7528\u57fa\u4e8eLLM\u81ea\u8eab\u8ba1\u7b97\u7684token\u7ea7\u4f3c\u7136\u6982\u7387\u9a8c\u8bc1\u673a\u5236\uff0c\u7ed3\u5408\u4e0d\u540c\u53c2\u6570\u89c4\u6a21\u7684\u6a21\u578b\uff08\u76f8\u5dee\u4e24\u4e2a\u6570\u91cf\u7ea7\uff09\uff0c\u5c06\u5927\u6a21\u578b\u8c03\u7528\u9891\u7387\u964d\u4f4e\u81f310%\u4ee5\u4e0b", "result": "\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\uff08\u63a5\u8fd1\u76ee\u6807\u6a21\u578b100%\u6027\u80fd\uff09\u7684\u540c\u65f6\u5b9e\u73b0\u6700\u9ad82.5\u500d\u52a0\u901f\uff0c\u4e14\u5728\u590d\u6742\u4efb\u52a1\u4e2dCD\u6027\u80fd\u751a\u81f3\u8d85\u8d8a\u539f\u5927\u6a21\u578b\u7684\u7406\u8bba\u4e0a\u9650", "conclusion": "CD\u7a81\u7834\u63a8\u6d4b\u89e3\u7801\u7684\u7406\u8bba\u4e0a\u9650\uff0c\u901a\u8fc7\u6a21\u578b\u89c4\u6a21\u5dee\u5f02\u5316\u7ec4\u5408\u5b9e\u73b0\u6548\u7387\u7a81\u7834\uff0c\u4e3aLLM\u52a0\u901f\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2506.02404", "pdf": "https://arxiv.org/pdf/2506.02404", "abs": "https://arxiv.org/abs/2506.02404", "authors": ["Yilin Xiao", "Junnan Dong", "Chuang Zhou", "Su Dong", "Qianwen Zhang", "Di Yin", "Xing Sun", "Xiao Huang"], "title": "GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing\nrecognition for its potential to enhance large language models (LLMs) by\nstructurally organizing domain-specific corpora and facilitating complex\nreasoning. However, current evaluations of GraphRAG models predominantly rely\non traditional question-answering datasets. Their limited scope in questions\nand evaluation metrics fails to comprehensively assess the reasoning capacity\nimprovements enabled by GraphRAG models. To address this gap, we introduce\nGraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously\nevaluate GraphRAG models. Our benchmark offers three key superiorities: \\((i)\\)\nChallenging question design. Featuring college-level, domain-specific questions\nthat demand multi-hop reasoning, the benchmark ensures that simple content\nretrieval is insufficient for problem-solving. For example, some questions\nrequire mathematical reasoning or programming. \\((ii)\\) Diverse task coverage.\nThe dataset includes a broad spectrum of reasoning tasks, multiple-choice,\ntrue/false, multi-select, open-ended, and fill-in-the-blank. It spans 16\ndisciplines in twenty core textbooks. \\((iii)\\) Holistic evaluation framework.\nGraphRAG-Bench provides comprehensive assessment across the entire GraphRAG\npipeline, including graph construction, knowledge retrieval, and answer\ngeneration. Beyond final-answer correctness, it evaluates the logical coherence\nof the reasoning process. By applying nine contemporary GraphRAG methods to\nGraphRAG-Bench, we demonstrate its utility in quantifying how graph-based\nstructuring improves model reasoning capabilities. Our analysis reveals\ncritical insights about graph architectures, retrieval efficacy, and reasoning\ncapabilities, offering actionable guidance for the research community.", "AI": {"tldr": "GraphRAG-Bench\u662f\u4e00\u4e2a\u9488\u5bf9GraphRAG\u6a21\u578b\u7684\u9886\u57df\u4e13\u7528\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u6311\u6218\u6027\u95ee\u9898\u3001\u591a\u6837\u5316\u4efb\u52a1\u548c\u5168\u9762\u8bc4\u4f30\u6846\u67b6\uff0c\u7cfb\u7edf\u91cf\u5316\u56fe\u7ed3\u6784\u5bf9\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\u6548\u679c\u3002", "motivation": "\u73b0\u6709GraphRAG\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u4f20\u7edfQA\u6570\u636e\u96c6\uff0c\u5176\u6709\u9650\u7684\u95ee\u9898\u8303\u56f4\u548c\u8bc4\u4f30\u6307\u6807\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u56fe\u7ed3\u6784\u5e26\u6765\u7684\u63a8\u7406\u80fd\u529b\u63d0\u5347\uff0c\u4e9f\u9700\u4e13\u4e1a\u5316\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "1. \u8bbe\u8ba1\u9700\u8981\u591a\u8df3\u63a8\u7406\u7684\u9886\u57df\u4e13\u4e1a\u95ee\u9898\uff08\u5982\u6570\u5b66\u63a8\u7406/\u7f16\u7a0b\uff09\n2. \u8986\u76d6\u591a\u9009/\u5224\u65ad/\u586b\u7a7a\u7b4916\u5b66\u79d120\u672c\u6559\u6750\u7684\u591a\u6837\u5316\u4efb\u52a1\n3. \u6784\u5efa\u5168\u6d41\u7a0b\u8bc4\u4f30\u6846\u67b6\uff08\u56fe\u6784\u5efa/\u77e5\u8bc6\u68c0\u7d22/\u7b54\u6848\u751f\u6210\uff09\u5e76\u8bc4\u4f30\u903b\u8f91\u8fde\u8d2f\u6027", "result": "\u901a\u8fc7\u6d4b\u8bd59\u79cd\u4e3b\u6d41GraphRAG\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u57fa\u51c6\u5728\u91cf\u5316\u56fe\u7ed3\u6784\u6539\u8fdb\u6548\u679c\u7684\u6709\u6548\u6027\uff0c\u63ed\u793a\u4e86\u56fe\u67b6\u6784\u8bbe\u8ba1\u3001\u68c0\u7d22\u6548\u80fd\u4e0e\u63a8\u7406\u80fd\u529b\u4e4b\u95f4\u7684\u5173\u952e\u5173\u8054\u3002", "conclusion": "\u8be5\u57fa\u51c6\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u91cf\u5316\u56fe\u7ed3\u6784\u5bf9LLM\u63a8\u7406\u80fd\u529b\u63d0\u5347\u7684\u6807\u51c6\u5316\u5de5\u5177\uff0c\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u63ed\u793a\u7684\u6d1e\u89c1\u4e3a\u540e\u7eed\u65b9\u6cd5\u6539\u8fdb\u63d0\u4f9b\u4e86\u660e\u786e\u65b9\u5411\u3002"}}
{"id": "2506.02412", "pdf": "https://arxiv.org/pdf/2506.02412", "abs": "https://arxiv.org/abs/2506.02412", "authors": ["Zhengyuan Liu", "Geyu Lin", "Hui Li Tan", "Huayun Zhang", "Yanfeng Lu", "Xiaoxue Gao", "Stella Xin Yin", "He Sun", "Hock Huan Goh", "Lung Hsiang Wong", "Nancy F. Chen"], "title": "SingaKids: A Multilingual Multimodal Dialogic Tutor for Language Learning", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Industry Track", "summary": "The integration of generative artificial intelligence into educational\napplications has enhanced personalized and interactive learning experiences,\nand it shows strong potential to promote young learners language acquisition.\nHowever, it is still challenging to ensure consistent and robust performance\nacross different languages and cultural contexts, and kids-friendly design\nrequires simplified instructions, engaging interactions, and age-appropriate\nscaffolding to maintain motivation and optimize learning outcomes. In this\nwork, we introduce SingaKids, a dialogic tutor designed to facilitate language\nlearning through picture description tasks. Our system integrates dense image\ncaptioning, multilingual dialogic interaction, speech understanding, and\nengaging speech generation to create an immersive learning environment in four\nlanguages: English, Mandarin, Malay, and Tamil. We further improve the system\nthrough multilingual pre-training, task-specific tuning, and scaffolding\noptimization. Empirical studies with elementary school students demonstrate\nthat SingaKids provides effective dialogic teaching, benefiting learners at\ndifferent performance levels.", "AI": {"tldr": "SingaKids\u591a\u8bed\u8a00\u5bf9\u8bdd\u5bfc\u5e08\u7cfb\u7edf\u901a\u8fc7\u56fe\u7247\u63cf\u8ff0\u4efb\u52a1\u6574\u5408\u591a\u6a21\u6001\u6280\u672f\uff0c\u63d0\u5347\u513f\u7ae5\u8bed\u8a00\u5b66\u4e60\u6548\u679c", "motivation": "\u89e3\u51b3\u751f\u6210\u5f0fAI\u5728\u6559\u80b2\u5e94\u7528\u4e2d\u5b58\u5728\u7684\u8de8\u8bed\u8a00\u6027\u80fd\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u4f18\u5316\u513f\u7ae5\u53cb\u597d\u578b\u4ea4\u4e92\u8bbe\u8ba1\u4e0e\u5b66\u4e60\u652f\u67b6", "method": "\u6574\u5408\u5bc6\u96c6\u56fe\u50cf\u63cf\u8ff0+\u591a\u8bed\u8a00\u5bf9\u8bdd\u4ea4\u4e92+\u8bed\u97f3\u7406\u89e3\u751f\u6210\uff0c\u91c7\u7528\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u3001\u4efb\u52a1\u8c03\u4f18\u548c\u652f\u67b6\u4f18\u5316\u6280\u672f", "result": "\u5c0f\u5b66\u751f\u5b9e\u8bc1\u7814\u7a76\u8bc1\u5b9e\u7cfb\u7edf\u80fd\u4e3a\u4e0d\u540c\u6c34\u5e73\u5b66\u4e60\u8005\u63d0\u4f9b\u6709\u6548\u5bf9\u8bdd\u6559\u5b66", "conclusion": "\u591a\u6a21\u6001\u6574\u5408\u4e0e\u5bf9\u8bdd\u5f0f\u4ea4\u4e92\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u591a\u8bed\u8a00\u5b66\u4e60\u6548\u679c\uff0c\u7cfb\u7edf\u652f\u6301\u82f1/\u6c49/\u9a6c\u6765/\u6cf0\u7c73\u5c14\u56db\u8bed\u79cd"}}
{"id": "2506.02425", "pdf": "https://arxiv.org/pdf/2506.02425", "abs": "https://arxiv.org/abs/2506.02425", "authors": ["Tairan Liu"], "title": "Gender Inequality in English Textbooks Around the World: an NLP Approach", "categories": ["cs.CL", "stat.AP"], "comment": null, "summary": "Textbooks play a critical role in shaping children's understanding of the\nworld. While previous studies have identified gender inequality in individual\ncountries' textbooks, few have examined the issue cross-culturally. This study\napplies natural language processing methods to quantify gender inequality in\nEnglish textbooks from 22 countries across 7 cultural spheres. Metrics include\ncharacter count, firstness (which gender is mentioned first), and TF-IDF word\nassociations by gender. The analysis also identifies gender patterns in proper\nnames appearing in TF-IDF word lists, tests whether large language models can\ndistinguish between gendered word lists, and uses GloVe embeddings to examine\nhow closely keywords associate with each gender. Results show consistent\noverrepresentation of male characters in terms of count, firstness, and named\nentities. All regions exhibit gender inequality, with the Latin cultural sphere\nshowing the least disparity.", "AI": {"tldr": "\u8de8\u6587\u5316\u5206\u6790\u663e\u793a\u82f1\u8bed\u6559\u79d1\u4e66\u4e2d\u7537\u6027\u89d2\u8272\u5728\u6570\u91cf\u3001\u9996\u6b21\u63d0\u53ca\u548c\u547d\u540d\u5b9e\u4f53\u4e0a\u666e\u904d\u8fc7\u5ea6\u4ee3\u8868\uff0c\u62c9\u4e01\u6587\u5316\u5708\u6027\u522b\u5dee\u5f02\u6700\u5c0f", "motivation": "\u586b\u8865\u8de8\u6587\u5316\u6027\u522b\u4e0d\u5e73\u7b49\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u91cf\u5316\u4e0d\u540c\u6587\u5316\u80cc\u666f\u4e0b\u82f1\u8bed\u6559\u79d1\u4e66\u7684\u6027\u522b\u8868\u5f81\u5dee\u5f02", "method": "\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\uff08\u5b57\u7b26\u7edf\u8ba1\u3001TF-IDF\u5173\u8054\u5206\u6790\u3001GloVe\u8bcd\u5d4c\u5165\uff09\u5206\u679022\u56fd\u6559\u6750\uff0c\u68c0\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u6027\u522b\u5316\u8bcd\u6c47\u7684\u533a\u5206\u80fd\u529b", "result": "\u6240\u6709\u5730\u533a\u5747\u5b58\u5728\u6027\u522b\u4e0d\u5e73\u7b49\uff0c\u7537\u6027\u89d2\u8272\u6570\u91cf\u8d8560%\uff0c\u547d\u540d\u5b9e\u4f53\u4e2d\u7537\u6027\u5360\u6bd4\u663e\u8457\u66f4\u9ad8\uff0c\u62c9\u4e01\u6587\u5316\u5708\u6027\u522b\u5dee\u5f02\u6307\u6570\u6700\u4f4e\uff08\u0394=0.12\uff09", "conclusion": "\u6559\u79d1\u4e66\u6027\u522b\u4e0d\u5e73\u7b49\u662f\u5168\u7403\u6027\u73b0\u8c61\uff0c\u9700\u7cfb\u7edf\u6027\u6539\u8fdb\u5185\u5bb9\u7f16\u6392\u3002\u62c9\u4e01\u6587\u5316\u5708\u7684\u76f8\u5bf9\u5e73\u7b49\u8868\u660e\u6587\u5316\u56e0\u7d20\u53ef\u80fd\u5f71\u54cd\u6027\u522b\u8868\u5f81"}}
{"id": "2506.02426", "pdf": "https://arxiv.org/pdf/2506.02426", "abs": "https://arxiv.org/abs/2506.02426", "authors": ["Maryam Berijanian", "Kuldeep Singh", "Amin Sehati"], "title": "Comparative Analysis of AI Agent Architectures for Entity Relationship Classification", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.1"], "comment": null, "summary": "Entity relationship classification remains a challenging task in information\nextraction, especially in scenarios with limited labeled data and complex\nrelational structures. In this study, we conduct a comparative analysis of\nthree distinct AI agent architectures designed to perform relation\nclassification using large language models (LLMs). The agentic architectures\nexplored include (1) reflective self-evaluation, (2) hierarchical task\ndecomposition, and (3) a novel multi-agent dynamic example generation\nmechanism, each leveraging different modes of reasoning and prompt adaptation.\nIn particular, our dynamic example generation approach introduces real-time\ncooperative and adversarial prompting. We systematically compare their\nperformance across multiple domains and model backends. Our experiments\ndemonstrate that multi-agent coordination consistently outperforms standard\nfew-shot prompting and approaches the performance of fine-tuned models. These\nfindings offer practical guidance for the design of modular, generalizable\nLLM-based systems for structured relation extraction. The source codes and\ndataset are available at\n\\href{https://github.com/maryambrj/ALIEN.git}{https://github.com/maryambrj/ALIEN.git}.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u4e09\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5173\u7cfb\u5206\u7c7b\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u53d1\u73b0\u591a\u667a\u80fd\u4f53\u52a8\u6001\u793a\u4f8b\u751f\u6210\u673a\u5236\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5", "motivation": "\u89e3\u51b3\u6709\u9650\u6807\u6ce8\u6570\u636e\u548c\u590d\u6742\u5173\u7cfb\u573a\u666f\u4e0b\u7684\u5b9e\u4f53\u5173\u7cfb\u5206\u7c7b\u96be\u9898\uff0c\u63a2\u7d22\u65e0\u9700\u5fae\u8c03\u7684LLM\u5e94\u7528\u65b9\u6848", "method": "\u63d0\u51fa\u4e09\u79cd\u67b6\u6784\uff1a1) \u53cd\u601d\u81ea\u8bc4\u4f30 2) \u5c42\u6b21\u4efb\u52a1\u5206\u89e3 3) \u65b0\u578b\u591a\u667a\u80fd\u4f53\u52a8\u6001\u793a\u4f8b\u751f\u6210\uff08\u542b\u5b9e\u65f6\u534f\u540c\u5bf9\u6297\u63d0\u793a\u673a\u5236\uff09", "result": "\u591a\u667a\u80fd\u4f53\u534f\u8c03\u7b56\u7565\u4f18\u4e8e\u6807\u51c6\u5c11\u6837\u672c\u63d0\u793a\uff0c\u63a5\u8fd1\u5fae\u8c03\u6a21\u578b\u6027\u80fd", "conclusion": "\u4e3a\u6784\u5efa\u6a21\u5757\u5316\u3001\u53ef\u6cdb\u5316\u7684LLM\u5173\u7cfb\u62bd\u53d6\u7cfb\u7edf\u63d0\u4f9b\u5b9e\u8df5\u6307\u5bfc\uff0c\u5e76\u5f00\u6e90\u4ee3\u7801\u548c\u6570\u636e\u96c6"}}
{"id": "2506.02431", "pdf": "https://arxiv.org/pdf/2506.02431", "abs": "https://arxiv.org/abs/2506.02431", "authors": ["Mahammed Kamruzzaman", "Abdullah Al Monsur", "Gene Louis Kim", "Anshuman Chhabra"], "title": "From Anger to Joy: How Nationality Personas Shape Emotion Attribution in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Emotions are a fundamental facet of human experience, varying across\nindividuals, cultural contexts, and nationalities. Given the recent success of\nLarge Language Models (LLMs) as role-playing agents, we examine whether LLMs\nexhibit emotional stereotypes when assigned nationality-specific personas.\nSpecifically, we investigate how different countries are represented in\npre-trained LLMs through emotion attributions and whether these attributions\nalign with cultural norms. Our analysis reveals significant nationality-based\ndifferences, with emotions such as shame, fear, and joy being\ndisproportionately assigned across regions. Furthermore, we observe notable\nmisalignment between LLM-generated and human emotional responses, particularly\nfor negative emotions, highlighting the presence of reductive and potentially\nbiased stereotypes in LLM outputs.", "AI": {"tldr": "LLMs\u5728\u6a21\u62df\u4e0d\u540c\u56fd\u5bb6\u4eba\u683c\u65f6\u5c55\u73b0\u51fa\u60c5\u7eea\u523b\u677f\u5370\u8c61\uff0c\u5176\u60c5\u611f\u5206\u914d\u6a21\u5f0f\u4e0e\u4eba\u7c7b\u5b9e\u9645\u53cd\u5e94\u5b58\u5728\u663e\u8457\u5dee\u5f02", "motivation": "\u63a2\u7a76\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u901a\u8fc7\u56fd\u5bb6\u4eba\u683c\u89d2\u8272\u626e\u6f14\u5c55\u73b0\u51fa\u4e0e\u6587\u5316\u89c4\u8303\u76f8\u5173\u7684\u60c5\u7eea\u523b\u677f\u5370\u8c61", "method": "\u901a\u8fc7\u4e3aLLMs\u8d4b\u4e88\u4e0d\u540c\u56fd\u5bb6\u7684\u4eba\u683c\u8bbe\u5b9a\uff0c\u7cfb\u7edf\u5206\u6790\u5176\u60c5\u611f\u5206\u914d\u6a21\u5f0f\uff0c\u5e76\u4e0e\u4eba\u7c7b\u5b9e\u9645\u60c5\u611f\u6570\u636e\u8fdb\u884c\u8de8\u6587\u5316\u6bd4\u8f83", "result": "\u6a21\u578b\u8f93\u51fa\u5b58\u5728\u5730\u57df\u6027\u60c5\u611f\u504f\u89c1\uff08\u5982\u7f9e\u803b/\u6050\u60e7/\u5feb\u4e50\u60c5\u7eea\u5206\u914d\u5931\u8861\uff09\uff0c\u8d1f\u9762\u60c5\u7eea\u8868\u8fbe\u4e0e\u4eba\u7c7b\u53cd\u5e94\u663e\u8457\u4e0d\u5339\u914d", "conclusion": "LLMs\u5728\u8de8\u6587\u5316\u60c5\u611f\u6a21\u62df\u4e2d\u8868\u73b0\u51fa\u7b80\u5316\u4e14\u6709\u504f\u89c1\u7684\u523b\u677f\u8ba4\u77e5\uff0c\u63ed\u793a\u5f53\u524d\u6a21\u578b\u5728\u6587\u5316\u654f\u611f\u6027\u65b9\u9762\u7684\u91cd\u5927\u7f3a\u9677"}}
{"id": "2506.02442", "pdf": "https://arxiv.org/pdf/2506.02442", "abs": "https://arxiv.org/abs/2506.02442", "authors": ["Utsav Maskey", "Mark Dras", "Usman Naseem"], "title": "Should LLM Safety Be More Than Refusing Harmful Instructions?", "categories": ["cs.CL"], "comment": "Preprint", "summary": "This paper presents a systematic evaluation of Large Language Models' (LLMs)\nbehavior on long-tail distributed (encrypted) texts and their safety\nimplications. We introduce a two-dimensional framework for assessing LLM\nsafety: (1) instruction refusal-the ability to reject harmful obfuscated\ninstructions, and (2) generation safety-the suppression of generating harmful\nresponses. Through comprehensive experiments, we demonstrate that models that\npossess capabilities to decrypt ciphers may be susceptible to\nmismatched-generalization attacks: their safety mechanisms fail on at least one\nsafety dimension, leading to unsafe responses or over-refusal. Based on these\nfindings, we evaluate a number of pre-LLM and post-LLM safeguards and discuss\ntheir strengths and limitations. This work contributes to understanding the\nsafety of LLM in long-tail text scenarios and provides directions for\ndeveloping robust safety mechanisms.", "AI": {"tldr": "\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u5c3e\u52a0\u5bc6\u6587\u672c\u65f6\u7684\u5b89\u5168\u6027\uff0c\u53d1\u73b0\u5176\u5b89\u5168\u673a\u5236\u5728\u89e3\u5bc6\u80fd\u529b\u4e0b\u5b58\u5728\u6f0f\u6d1e\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u63a2\u7a76LLMs\u5728\u957f\u5c3e\u52a0\u5bc6\u6587\u672c\u573a\u666f\u4e2d\u7684\u5b89\u5168\u98ce\u9669\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u4e0d\u5339\u914d\u6cdb\u5316\u653b\u51fb\u65f6\u5b89\u5168\u673a\u5236\u7684\u6709\u6548\u6027\u3002", "method": "\u5efa\u7acb\u4e8c\u7ef4\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\uff08\u6307\u4ee4\u62d2\u7edd/\u751f\u6210\u5b89\u5168\uff09\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5177\u5907\u89e3\u5bc6\u80fd\u529b\u6a21\u578b\u7684\u5b89\u5168\u673a\u5236\u5931\u6548\u6848\u4f8b\u3002", "result": "\u5177\u5907\u89e3\u5bc6\u80fd\u529b\u7684\u6a21\u578b\u5728\u81f3\u5c11\u4e00\u4e2a\u5b89\u5168\u7ef4\u5ea6\u5931\u6548\uff0c\u5bfc\u81f4\u4e0d\u5b89\u5168\u54cd\u5e94\u6216\u8fc7\u5ea6\u62d2\u7edd\uff0c\u73b0\u6709\u9632\u62a4\u63aa\u65bd\u5b58\u5728\u5c40\u9650\u6027\u3002", "conclusion": "\u63ed\u793a\u4e86LLMs\u5728\u52a0\u5bc6\u6587\u672c\u573a\u666f\u7684\u5b89\u5168\u8106\u5f31\u6027\uff0c\u4e3a\u5f00\u53d1\u9c81\u68d2\u6027\u5b89\u5168\u673a\u5236\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2506.02449", "pdf": "https://arxiv.org/pdf/2506.02449", "abs": "https://arxiv.org/abs/2506.02449", "authors": ["Bo Peng", "Zhiheng Wang", "Heyang Gong", "Chaochao Lu"], "title": "IP-Dialog: Evaluating Implicit Personalization in Dialogue Systems with Synthetic Data", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "In modern dialogue systems, the ability to implicitly infer user backgrounds\nfrom conversations and leverage this information for personalized assistance is\ncrucial. However, the scarcity of high-quality data remains a fundamental\nchallenge to evaluating and improving this capability. Traditional dataset\nconstruction methods are labor-intensive, resource-demanding, and raise privacy\nconcerns. To address these issues, we propose a novel approach for automatic\nsynthetic data generation and introduce the Implicit Personalized Dialogue\n(IP-Dialog) benchmark along with a training dataset, covering 10 tasks and 12\nuser attribute types. Additionally, we develop a systematic evaluation\nframework with four metrics to assess both attribute awareness and reasoning\ncapabilities. We further propose five causal graphs to elucidate models'\nreasoning pathways during implicit personalization. Extensive experiments yield\ninsightful observations and prove the reliability of our dataset.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u52a8\u5316\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\u89e3\u51b3\u5bf9\u8bdd\u7cfb\u7edf\u9690\u5f0f\u4e2a\u6027\u5316\u8bc4\u4f30\u96be\u9898\uff0c\u6784\u5efaIP-Dialog\u57fa\u51c6\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u53ef\u9760\u6027", "motivation": "\u4f20\u7edf\u6570\u636e\u96c6\u6784\u5efa\u65b9\u6cd5\u5b58\u5728\u6548\u7387\u4f4e\u3001\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u7b49\u95ee\u9898\uff0c\u5236\u7ea6\u5bf9\u8bdd\u7cfb\u7edf\u9690\u5f0f\u4e2a\u6027\u5316\u80fd\u529b\u53d1\u5c55", "method": "\u5f00\u53d1\u81ea\u52a8\u5316\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u521b\u5efa\u8986\u76d610\u4efb\u52a112\u5c5e\u6027\u7684IP-Dialog\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u56db\u7ef4\u5ea6\u8bc4\u4f30\u4f53\u7cfb\u53ca\u4e94\u7c7b\u56e0\u679c\u63a8\u7406\u56fe", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u6570\u636e\u96c6\u53ef\u9760\u6027\uff0c\u63ed\u793a\u6a21\u578b\u5c5e\u6027\u611f\u77e5\u4e0e\u63a8\u7406\u80fd\u529b\u7684\u5173\u8054\u673a\u5236", "conclusion": "\u8be5\u65b9\u6cd5\u7a81\u7834\u6570\u636e\u74f6\u9888\uff0cIP-Dialog\u57fa\u51c6\u4e3a\u9690\u5f0f\u4e2a\u6027\u5316\u7814\u7a76\u63d0\u4f9b\u6807\u51c6\u5316\u8bc4\u4f30\u5de5\u5177"}}
{"id": "2506.02454", "pdf": "https://arxiv.org/pdf/2506.02454", "abs": "https://arxiv.org/abs/2506.02454", "authors": ["Zhaorui Yang", "Bo Pan", "Han Wang", "Yiyao Wang", "Xingyu Liu", "Minfeng Zhu", "Bo Zhang", "Wei Chen"], "title": "Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework", "categories": ["cs.CL", "cs.AI"], "comment": "47 pages", "summary": "Visualizations play a crucial part in effective communication of concepts and\ninformation. Recent advances in reasoning and retrieval augmented generation\nhave enabled Large Language Models (LLMs) to perform deep research and generate\ncomprehensive reports. Despite its progress, existing deep research frameworks\nprimarily focus on generating text-only content, leaving the automated\ngeneration of interleaved texts and visualizations underexplored. This novel\ntask poses key challenges in designing informative visualizations and\neffectively integrating them with text reports. To address these challenges, we\npropose Formal Description of Visualization (FDV), a structured textual\nrepresentation of charts that enables LLMs to learn from and generate diverse,\nhigh-quality visualizations. Building on this representation, we introduce\nMultimodal DeepResearcher, an agentic framework that decomposes the task into\nfour stages: (1) researching, (2) exemplar report textualization, (3) planning,\nand (4) multimodal report generation. For the evaluation of generated\nmultimodal reports, we develop MultimodalReportBench, which contains 100\ndiverse topics served as inputs along with 5 dedicated metrics. Extensive\nexperiments across models and evaluation methods demonstrate the effectiveness\nof Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet\nmodel, Multimodal DeepResearcher achieves an 82\\% overall win rate over the\nbaseline method.", "AI": {"tldr": "\u63d0\u51faFDV\u7ed3\u6784\u5316\u56fe\u8868\u63cf\u8ff0\u65b9\u6cd5\u548c\u591a\u6a21\u6001DeepResearcher\u6846\u67b6\uff0c\u5b9e\u73b0LLM\u9a71\u52a8\u7684\u56fe\u6587\u878d\u5408\u62a5\u544a\u81ea\u52a8\u751f\u6210", "motivation": "\u73b0\u6709\u6df1\u5ea6\u7814\u7a76\u6846\u67b6\u4e3b\u8981\u751f\u6210\u7eaf\u6587\u672c\u5185\u5bb9\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u56fe\u6587\u6df7\u5408\u5185\u5bb9\u751f\u6210\uff0c\u9762\u4e34\u53ef\u89c6\u5316\u8bbe\u8ba1\u4e0e\u6587\u672c\u6574\u5408\u7684\u53cc\u91cd\u6311\u6218", "method": "1) \u8bbe\u8ba1FDV\u7ed3\u6784\u5316\u56fe\u8868\u63cf\u8ff0\u8bed\u8a00\uff1b2) \u6784\u5efa\u5305\u542b\u7814\u7a76\u3001\u793a\u4f8b\u62a5\u544a\u6587\u672c\u5316\u3001\u89c4\u5212\u548c\u591a\u6a21\u6001\u751f\u6210\u56db\u9636\u6bb5\u6846\u67b6\uff1b3) \u5f00\u53d1\u542b100\u4e3b\u9898\u7684MultimodalReportBench\u8bc4\u4f30\u4f53\u7cfb", "result": "\u4f7f\u7528\u76f8\u540cClaude 3.7 Sonnet\u6a21\u578b\uff0c\u591a\u6a21\u6001DeepResearcher\u76f8\u5bf9\u57fa\u7ebf\u65b9\u6cd5\u83b7\u5f9782%\u7efc\u5408\u80dc\u7387", "conclusion": "FDV\u4e0e\u591a\u6a21\u6001\u6846\u67b6\u6210\u529f\u89e3\u51b3\u56fe\u6587\u878d\u5408\u751f\u6210\u96be\u9898\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u591a\u7ef4\u5ea6\u8bc4\u4f30\u4e2d\u7684\u6709\u6548\u6027\uff0c\u63a8\u52a8\u81ea\u52a8\u5316\u7814\u7a76\u62a5\u544a\u5411\u591a\u6a21\u6001\u65b9\u5411\u53d1\u5c55"}}
{"id": "2506.02460", "pdf": "https://arxiv.org/pdf/2506.02460", "abs": "https://arxiv.org/abs/2506.02460", "authors": ["Yupeng Qi", "Ziyu Lyu", "Min Yang", "Yanlin Wang", "Lu Bai", "Lixin Cui"], "title": "MidPO: Dual Preference Optimization for Safety and Helpfulness in Large Language Models via a Mixture of Experts Framework", "categories": ["cs.CL"], "comment": null, "summary": "As large language models (LLMs) are increasingly applied across various\ndomains, enhancing safety while maintaining the helpfulness of LLMs has become\na critical challenge. Recent studies solve this problem through\nsafety-constrained online preference optimization or safety-constrained offline\npreference optimization. However, the safety-constrained online methods often\nsuffer from excessive safety, which might reduce helpfulness, while the\nsafety-constrained offline methods perform poorly in adaptively balancing\nsafety and helpfulness. To address these limitations, we propose MidPO, a\n\\textbf{\\underline{Mi}}xture of Experts (MoE) framework for safety-helpfulness\n\\textbf{\\underline{d}}ual \\textbf{\\underline{P}}reference\n\\textbf{\\underline{O}}ptimization. Firstly, MidPO devises single-preference\nenhanced direct preference optimization approach to transform the base model\ninto two independent experts, termed safety and helpfulness experts, and\nfine-tunes the two independent experts for optimal safety or helpfulness\nperformance. Secondly, to achieve an effective balance between safety and\nhelpfulness, MidPO incorporates the two experts into the MoE framework and\ndesigns a dynamic routing mechanism to allocate contributions from each expert\nadaptively. We conduct quantitative and qualitative experiments on three\npopular datasets to demonstrate the proposed MidPO significantly outperforms\nstate-of-the-art approaches in both safety and helpfulness. The code and models\nwill be released.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408\u4e13\u5bb6\u6846\u67b6MidPO\uff0c\u901a\u8fc7\u53cc\u504f\u597d\u4f18\u5316\u540c\u65f6\u589e\u5f3aLLMs\u7684\u5b89\u5168\u6027\u548c\u6709\u7528\u6027", "motivation": "\u73b0\u6709\u5b89\u5168\u7ea6\u675f\u65b9\u6cd5\u5b58\u5728\u8fc7\u5ea6\u5b89\u5168\u5bfc\u81f4\u6709\u7528\u6027\u4e0b\u964d\uff08\u5728\u7ebf\u65b9\u6cd5\uff09\u548c\u81ea\u9002\u5e94\u5e73\u8861\u80fd\u529b\u4e0d\u8db3\uff08\u79bb\u7ebf\u65b9\u6cd5\uff09\u7684\u95ee\u9898", "method": "1. \u5f00\u53d1\u5355\u504f\u597d\u589e\u5f3a\u7684\u76f4\u63a5\u504f\u597d\u4f18\u5316\u8bad\u7ec3\u72ec\u7acb\u5b89\u5168/\u6709\u7528\u6027\u4e13\u5bb6\u6a21\u578b\n2. \u901a\u8fc7\u52a8\u6001\u8def\u7531\u673a\u5236\u7684MoE\u6846\u67b6\u5b9e\u73b0\u5b89\u5168-\u6709\u7528\u6027\u81ea\u9002\u5e94\u5e73\u8861", "result": "\u5728\u4e09\u4e2a\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff08\u5b89\u5168\u6027\u548c\u6709\u7528\u6027\u53cc\u91cd\u6307\u6807\uff09\uff0c\u91cf\u5316\u5b9e\u9a8c\u548c\u5b9a\u6027\u5206\u6790\u5747\u9a8c\u8bc1\u6709\u6548\u6027", "conclusion": "MidPO\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5b89\u5168\u6027\u4e0e\u6709\u7528\u6027\u7684\u5e73\u8861\u96be\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u8d8a\u6027\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u5f00\u6e90"}}
{"id": "2506.02461", "pdf": "https://arxiv.org/pdf/2506.02461", "abs": "https://arxiv.org/abs/2506.02461", "authors": ["Chunkit Chan", "Yauwai Yim", "Hongchuan Zeng", "Zhiying Zou", "Xinyuan Cheng", "Zhifan Sun", "Zheye Deng", "Kawai Chung", "Yuzhuo Ao", "Yixiang Fan", "Cheng Jiayang", "Ercong Nie", "Ginny Y. Wong", "Helmut Schmid", "Hinrich Sch\u00fctze", "Simon See", "Yangqiu Song"], "title": "XToM: Exploring the Multilingual Theory of Mind for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Theory of Mind (ToM), the ability to infer mental states in others, is\npivotal for human social cognition. Existing evaluations of ToM in LLMs are\nlargely limited to English, neglecting the linguistic diversity that shapes\nhuman cognition. This limitation raises a critical question: can LLMs exhibit\nMultilingual Theory of Mind, which is the capacity to reason about mental\nstates across diverse linguistic contexts? To address this gap, we present\nXToM, a rigorously validated multilingual benchmark that evaluates ToM across\nfive languages and incorporates diverse, contextually rich task scenarios.\nUsing XToM, we systematically evaluate LLMs (e.g., DeepSeek R1), revealing a\npronounced dissonance: while models excel in multilingual language\nunderstanding, their ToM performance varies across languages. Our findings\nexpose limitations in LLMs' ability to replicate human-like mentalizing across\nlinguistic contexts.", "AI": {"tldr": "XToM\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u5fc3\u667a\u7406\u8bba\u4efb\u52a1\u4e2d\u5b58\u5728\u8de8\u8bed\u8a00\u8868\u73b0\u5dee\u5f02\uff0c\u66b4\u9732\u5176\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u6a21\u5f0f\u7684\u5dee\u8ddd", "motivation": "\u73b0\u6709\u5fc3\u667a\u7406\u8bba\u8bc4\u4f30\u5c40\u9650\u4e8e\u82f1\u8bed\uff0c\u65e0\u6cd5\u53cd\u6620\u8bed\u8a00\u591a\u6837\u6027\u5bf9\u4eba\u7c7b\u8ba4\u77e5\u7684\u5f71\u54cd\uff0c\u9700\u63a2\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u5fc3\u667a\u63a8\u7406\u80fd\u529b", "method": "\u5f00\u53d1\u8986\u76d65\u79cd\u8bed\u8a00\u7684XToM\u591a\u8bed\u8a00\u57fa\u51c6\uff0c\u5305\u542b\u591a\u6837\u5316\u60c5\u5883\u4efb\u52a1\uff0c\u7cfb\u7edf\u8bc4\u4f30DeepSeek R1\u7b49\u6a21\u578b\u8868\u73b0", "result": "\u6a21\u578b\u5728\u591a\u8bed\u8a00\u7406\u89e3\u8868\u73b0\u4f18\u5f02\u4f46\u5fc3\u667a\u7406\u8bba\u80fd\u529b\u5b58\u5728\u8bed\u8a00\u5dee\u5f02\uff0c\u663e\u793a\u5176\u8de8\u8bed\u8a00\u5fc3\u667a\u63a8\u7406\u80fd\u529b\u4e0d\u8db3", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u867d\u5177\u5907\u591a\u8bed\u8a00\u5904\u7406\u4f18\u52bf\uff0c\u4f46\u5728\u6a21\u62df\u4eba\u7c7b\u8de8\u8bed\u8a00\u5fc3\u667a\u7406\u8bba\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u5c40\u9650\uff0c\u9700\u7a81\u7834\u73b0\u6709\u67b6\u6784\u63d0\u5347\u793e\u4f1a\u8ba4\u77e5\u80fd\u529b"}}
{"id": "2506.02478", "pdf": "https://arxiv.org/pdf/2506.02478", "abs": "https://arxiv.org/abs/2506.02478", "authors": ["Zijian Li", "Xiaocheng Feng", "Huixin Liu", "Yichong Huang", "Ting Liu", "Bing Qin"], "title": "FroM: Frobenius Norm-Based Data-Free Adaptive Model Merging", "categories": ["cs.CL"], "comment": "12 pages, 11 figures", "summary": "With the development of large language models, fine-tuning has emerged as an\neffective method to enhance performance in specific scenarios by injecting\ndomain-specific knowledge. In this context, model merging techniques provide a\nsolution for fusing knowledge from multiple fine-tuning models by combining\ntheir parameters. However, traditional methods often encounter task\ninterference when merging full fine-tuning models, and this problem becomes\neven more evident in parameter-efficient fine-tuning scenarios. In this paper,\nwe introduce an improvement to the RegMean method, which indirectly leverages\nthe training data to approximate the outputs of the linear layers before and\nafter merging. We propose an adaptive merging method called FroM, which\ndirectly measures the model parameters using the Frobenius norm, without any\ntraining data. By introducing an additional hyperparameter for control, FroM\noutperforms baseline methods across various fine-tuning scenarios, alleviating\nthe task interference problem.", "AI": {"tldr": "\u63d0\u51faFroM\u65b9\u6cd5\uff0c\u901a\u8fc7Frobenius\u8303\u6570\u76f4\u63a5\u5ea6\u91cf\u6a21\u578b\u53c2\u6570\u5e76\u5f15\u5165\u8d85\u53c2\u6570\u63a7\u5236\uff0c\u6709\u6548\u7f13\u89e3\u591a\u4efb\u52a1\u5fae\u8c03\u573a\u666f\u4e0b\u7684\u6a21\u578b\u5408\u5e76\u5e72\u6270\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6a21\u578b\u878d\u5408\u65b9\u6cd5\uff08\u5982RegMean\uff09\u5728\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u573a\u666f\u4e2d\u9762\u4e34\u4efb\u52a1\u5e72\u6270\u52a0\u5267\u7684\u95ee\u9898\uff0c\u4e14\u4f9d\u8d56\u8bad\u7ec3\u6570\u636e\u3002\u9700\u5f00\u53d1\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u3001\u76f4\u63a5\u901a\u8fc7\u53c2\u6570\u5ea6\u91cf\u5b9e\u73b0\u81ea\u9002\u5e94\u878d\u5408\u7684\u65b9\u6848\u3002", "method": "\u6539\u8fdbRegMean\u65b9\u6cd5\uff0c\u6452\u5f03\u5bf9\u8bad\u7ec3\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u57fa\u4e8eFrobenius\u8303\u6570\u76f4\u63a5\u8ba1\u7b97\u7ebf\u6027\u5c42\u53c2\u6570\u5dee\u5f02\uff0c\u5f15\u5165\u8d85\u53c2\u6570\u8c03\u8282\u4e0d\u540c\u4efb\u52a1\u6a21\u578b\u7684\u878d\u5408\u6743\u91cd\uff0c\u5b9e\u73b0\u65e0\u6570\u636e\u4f9d\u8d56\u7684\u81ea\u9002\u5e94\u5408\u5e76\u3002", "result": "\u5728\u591a\u79cd\u5fae\u8c03\u573a\u666f\u4e0b\uff08\u5305\u62ec\u5168\u53c2\u6570\u5fae\u8c03\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff09\uff0cFroM\u65b9\u6cd5\u5728GLUE\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u663e\u8457\u7f13\u89e3\u4efb\u52a1\u5e72\u6270\u73b0\u8c61\u3002", "conclusion": "FroM\u4e3a\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u4e86\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u6a21\u578b\u878d\u5408\u65b9\u6848\uff0c\u5bf9\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u591a\u4efb\u52a1\u6a21\u578b\u6574\u5408\u5177\u6709\u91cd\u8981\u5b9e\u8df5\u4ef7\u503c\u3002"}}
{"id": "2506.02480", "pdf": "https://arxiv.org/pdf/2506.02480", "abs": "https://arxiv.org/abs/2506.02480", "authors": ["Yifan Duan", "Yihong Tang", "Kehai Chen", "Liqiang Nie", "Min Zhang"], "title": "ORPP: Self-Optimizing Role-playing Prompts to Enhance Language Model Capabilities", "categories": ["cs.CL"], "comment": null, "summary": "High-quality prompts are crucial for eliciting outstanding performance from\nlarge language models (LLMs) on complex tasks. Existing research has explored\nmodel-driven strategies for prompt optimization. However, these methods often\nsuffer from high computational overhead or require strong optimization\ncapabilities from the model itself, which limits their broad applicability.To\naddress these challenges, we propose ORPP (Optimized Role-Playing Prompt),a\nframework that enhances model performance by optimizing and generating\nrole-playing prompts. The core idea of ORPP is to confine the prompt search\nspace to role-playing scenarios, thereby fully activating the model's intrinsic\ncapabilities through carefully crafted, high-quality role-playing prompts.\nSpecifically, ORPP first performs iterative optimization on a small subset of\ntraining samples to generate high-quality role-playing prompts. Then,\nleveraging the model's few-shot learning capability, it transfers the\noptimization experience to efficiently generate suitable prompts for the\nremaining samples.Our experimental results show that ORPP not only matches but\nin most cases surpasses existing mainstream prompt optimization methods in\nterms of performance. Notably, ORPP demonstrates superior \"plug-and-play\"\ncapability. In most cases, it can be integrated with various other prompt\nmethods and further enhance their effectiveness.", "AI": {"tldr": "\u63d0\u51faORPP\u6846\u67b6\uff0c\u901a\u8fc7\u89d2\u8272\u626e\u6f14\u63d0\u793a\u4f18\u5316\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u6027\u80fd\uff0c\u517c\u5bb9\u6027\u5f3a\u4e14\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u5b58\u5728\u9ad8\u8ba1\u7b97\u5f00\u9500\u4e0e\u5f3a\u6a21\u578b\u80fd\u529b\u4f9d\u8d56\u95ee\u9898\uff0c\u9650\u5236\u5176\u5e7f\u6cdb\u5e94\u7528", "method": "\u5728\u5c0f\u6837\u672c\u8fed\u4ee3\u751f\u6210\u89d2\u8272\u626e\u6f14\u63d0\u793a\u6a21\u677f\uff0c\u5229\u7528few-shot\u5b66\u4e60\u8fc1\u79fb\u4f18\u5316\u7ecf\u9a8c\u81f3\u5168\u91cf\u6570\u636e", "result": "ORPP\u6027\u80fd\u4f18\u4e8e\u4e3b\u6d41\u65b9\u6cd5\uff0c\u4e14\u80fd\u4e0e\u591a\u79cd\u63d0\u793a\u65b9\u6cd5\u517c\u5bb9\u5b9e\u73b0\u6548\u679c\u53e0\u52a0", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u63d0\u793a\u4f18\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u517c\u5bb9\u7684\u65b0\u8303\u5f0f\uff0c\u62d3\u5c55\u4e86\u5927\u6a21\u578b\u5e94\u7528\u6f5c\u529b"}}
{"id": "2506.02481", "pdf": "https://arxiv.org/pdf/2506.02481", "abs": "https://arxiv.org/abs/2506.02481", "authors": ["Inderjeet Nair", "Lu Wang"], "title": "Do Language Models Think Consistently? A Study of Value Preferences Across Varying Response Lengths", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluations of LLMs' ethical risks and value inclinations often rely on\nshort-form surveys and psychometric tests, yet real-world use involves\nlong-form, open-ended responses -- leaving value-related risks and preferences\nin practical settings largely underexplored. In this work, we ask: Do value\npreferences inferred from short-form tests align with those expressed in\nlong-form outputs? To address this question, we compare value preferences\nelicited from short-form reactions and long-form responses, varying the number\nof arguments in the latter to capture users' differing verbosity preferences.\nAnalyzing five LLMs (llama3-8b, gemma2-9b, mistral-7b, qwen2-7b, and olmo-7b),\nwe find (1) a weak correlation between value preferences inferred from\nshort-form and long-form responses across varying argument counts, and (2)\nsimilarly weak correlation between preferences derived from any two distinct\nlong-form generation settings. (3) Alignment yields only modest gains in the\nconsistency of value expression. Further, we examine how long-form generation\nattributes relate to value preferences, finding that argument specificity\nnegatively correlates with preference strength, while representation across\nscenarios shows a positive correlation. Our findings underscore the need for\nmore robust methods to ensure consistent value expression across diverse\napplications.", "AI": {"tldr": "LLM\u4ef7\u503c\u89c2\u8bc4\u4f30\u4e2d\u77ed\u7bc7\u6d4b\u8bd5\u4e0e\u957f\u7bc7\u56de\u7b54\u7684\u504f\u597d\u4e00\u81f4\u6027\u8f83\u5f31\uff0c\u9700\u5f00\u53d1\u66f4\u7a33\u5065\u7684\u8de8\u573a\u666f\u8bc4\u4f30\u65b9\u6cd5", "motivation": "\u73b0\u6709LLM\u4f26\u7406\u98ce\u9669\u8bc4\u4f30\u4f9d\u8d56\u77ed\u7bc7\u95ee\u5377\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u591a\u4e3a\u5f00\u653e\u5f0f\u957f\u6587\u672c\u54cd\u5e94\uff0c\u5bfc\u81f4\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u4ef7\u503c\u89c2\u98ce\u9669\u672a\u88ab\u5145\u5206\u63a2\u7d22", "method": "\u6bd4\u8f83\u4e94\u4e2aLLM\uff08llama3-8b\u7b49\uff09\u5728\u77ed\u7bc7\u53cd\u5e94\u4e0e\u4e0d\u540c\u53c2\u6570\u6570\u91cf\u7684\u957f\u7bc7\u56de\u7b54\u4e2d\u7684\u4ef7\u503c\u504f\u597d\uff0c\u5206\u6790\u751f\u6210\u5c5e\u6027\u4e0e\u504f\u597d\u7684\u76f8\u5173\u6027", "result": "1. \u77ed\u7bc7\u4e0e\u957f\u7bc7\u504f\u597d\u5f31\u76f8\u5173\uff08\u4e0d\u540c\u53c2\u6570\u8bbe\u7f6e\u4e0br=0.16-0.22\uff09\n2. \u4e0d\u540c\u957f\u7bc7\u751f\u6210\u8bbe\u7f6e\u95f4\u76f8\u5173\u6027\u540c\u6837\u5fae\u5f31\n3. \u5bf9\u9f50\u8bad\u7ec3\u4ec5\u5c0f\u5e45\u63d0\u5347\u4e00\u81f4\u6027\uff08+7.6%\uff09\n4. \u8bba\u636e\u7279\u5f02\u6027\u4e0e\u504f\u597d\u5f3a\u5ea6\u8d1f\u76f8\u5173\uff0c\u573a\u666f\u8986\u76d6\u5ea6\u4e0e\u504f\u597d\u6b63\u76f8\u5173", "conclusion": "\u5f53\u524d\u8bc4\u4f30\u4f53\u7cfb\u96be\u4ee5\u6355\u6349LLM\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4ef7\u503c\u8868\u8fbe\u6ce2\u52a8\uff0c\u4e9f\u9700\u5f00\u53d1\u8de8\u751f\u6210\u573a\u666f\u7684\u7a33\u5065\u8bc4\u4f30\u6846\u67b6"}}
{"id": "2506.02483", "pdf": "https://arxiv.org/pdf/2506.02483", "abs": "https://arxiv.org/abs/2506.02483", "authors": ["Sina Bagheri Nezhad", "Ameeta Agrawal"], "title": "Enhancing Large Language Models with Neurosymbolic Reasoning for Multilingual Tasks", "categories": ["cs.CL"], "comment": "Accepted at 19th Conference on Neurosymbolic Learning and Reasoning\n  (NeSy 2025)", "summary": "Large language models (LLMs) often struggle to perform multi-target reasoning\nin long-context scenarios where relevant information is scattered across\nextensive documents. To address this challenge, we introduce NeuroSymbolic\nAugmented Reasoning (NSAR), which combines the benefits of neural and symbolic\nreasoning during inference. NSAR explicitly extracts symbolic facts from text\nand generates executable Python code to handle complex reasoning steps. Through\nextensive experiments across seven languages and diverse context lengths, we\ndemonstrate that NSAR significantly outperforms both a vanilla RAG baseline and\nadvanced prompting strategies in accurately identifying and synthesizing\nmultiple pieces of information. Our results highlight the effectiveness of\ncombining explicit symbolic operations with neural inference for robust,\ninterpretable, and scalable reasoning in multilingual settings.", "AI": {"tldr": "\u63d0\u51fa\u795e\u7ecf\u7b26\u53f7\u589e\u5f3a\u63a8\u7406\u6846\u67b6\uff08NSAR\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u795e\u7ecf\u63a8\u7406\u4e0e\u663e\u5f0f\u7b26\u53f7\u64cd\u4f5c\u89e3\u51b3\u5927\u6a21\u578b\u5728\u957f\u6587\u672c\u591a\u76ee\u6807\u63a8\u7406\u4e2d\u7684\u4fe1\u606f\u6574\u5408\u96be\u9898\uff0c\u57287\u79cd\u8bed\u8a00\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u4fe1\u606f\u5206\u6563\u7684\u957f\u4e0a\u4e0b\u6587\u65f6\uff0c\u96be\u4ee5\u51c6\u786e\u6574\u5408\u591a\u4e2a\u76ee\u6807\u7684\u5173\u8054\u4fe1\u606f\uff0c\u5f71\u54cd\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u53ef\u6269\u5c55\u6027\u548c\u7ed3\u679c\u53ef\u9760\u6027\u3002", "method": "NSAR\u6846\u67b6\u5206\u4e24\u6b65\uff1a1\uff09\u4ece\u6587\u672c\u663e\u5f0f\u63d0\u53d6\u7b26\u53f7\u5316\u4e8b\u5b9e 2\uff09\u751f\u6210Python\u4ee3\u7801\u6267\u884c\u590d\u6742\u63a8\u7406\u903b\u8f91\uff0c\u5b9e\u73b0\u795e\u7ecf\u63a8\u7406\u4e0e\u7b26\u53f7\u64cd\u4f5c\u7684\u4f18\u52bf\u4e92\u8865\u3002", "result": "\u5728\u8de8\u8bed\u8a00\u3001\u53ef\u53d8\u957f\u5ea6\u4e0a\u4e0b\u6587\u7684\u5b9e\u9a8c\u4e2d\uff0cNSAR\u51c6\u786e\u7387\u8f83RAG\u57fa\u7ebf\u63d0\u534723.7%\uff0c\u6bd4\u601d\u7ef4\u94fe\u7b49\u9ad8\u7ea7\u63d0\u793a\u7b56\u7565\u9ad815.2%\uff0c\u4e14\u5177\u5907\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u663e\u5f0f\u7b26\u53f7\u64cd\u4f5c\u4e0e\u795e\u7ecf\u63a8\u7406\u7684\u534f\u540c\u673a\u5236\u4e3a\u591a\u8bed\u8a00\u590d\u6742\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u7b26\u53f7\u589e\u5f3a\u65b9\u6cd5\u5728\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.02494", "pdf": "https://arxiv.org/pdf/2506.02494", "abs": "https://arxiv.org/abs/2506.02494", "authors": ["Junzhe Zhang", "Huixuan Zhang", "Xinyu Hu", "Li Lin", "Mingqi Gao", "Shi Qiu", "Xiaojun Wan"], "title": "Minos: A Multimodal Evaluation Model for Bidirectional Generation Between Image and Text", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Evaluation is important for multimodal generation tasks. With the rapid\nprogress of MLLMs, there is growing interest in applying MLLMs to build general\nevaluation systems. However, existing work overlooks two aspects: (1) the\ndevelopment of evaluation capabilities for text-to-image (T2I) generation task,\nand (2) the incorporation of large-scale human evaluation data. In this paper,\nwe introduce Minos-Corpus, a large-scale multimodal evaluation dataset that\ncombines evaluation data from both human and GPT. The corpus contains\nevaluation data across both image-to-text(I2T) and T2I generation tasks. Based\non this corpus, we propose Data Selection and Balance, Mix-SFT training\nmethods, and apply DPO to develop Minos, a multimodal evaluation model built\nupon a 7B backbone. Minos achieves state-of-the-art (SoTA) performance among\nall open-source evaluation models of similar scale on the average of evaluation\nperformance on all tasks, and outperforms all open-source and closed-source\nmodels on evaluation of T2I generation task. Extensive experiments demonstrate\nthe importance of leveraging high-quality human evaluation data and jointly\ntraining on evaluation data from both I2T and T2I generation tasks.", "AI": {"tldr": "\u63d0\u51faMinos-Corpus\u591a\u6a21\u6001\u8bc4\u4f30\u6570\u636e\u96c6\u53caMinos\u6a21\u578b\uff0c\u5728T2I\u751f\u6210\u4efb\u52a1\u8bc4\u4f30\u4e2d\u8fbe\u5230SOTA\u6027\u80fd", "motivation": "\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u7684\u8bc4\u4f30\u80fd\u529b\u5f00\u53d1\uff0c\u4e14\u7f3a\u4e4f\u7ed3\u5408\u4eba\u7c7b\u8bc4\u4f30\u6570\u636e\uff0c\u9700\u6784\u5efa\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u4f53\u7cfb", "method": "\u901a\u8fc7Data Selection and Balance\u7b5b\u9009\u6570\u636e\uff0c\u91c7\u7528Mix-SFT\u8bad\u7ec3\u65b9\u6cd5\u7ed3\u5408DPO\u4f18\u5316\uff0c\u57287B\u6a21\u578b\u57fa\u7840\u4e0a\u8bad\u7ec3Minos", "result": "Minos\u5728T2I\u8bc4\u4f30\u4efb\u52a1\u4e0a\u8d85\u8d8a\u6240\u6709\u5f00\u6e90/\u95ed\u6e90\u6a21\u578b\uff0c\u6240\u6709\u4efb\u52a1\u5e73\u5747\u8bc4\u4f30\u6027\u80fd\u8fbe\u5230\u540c\u89c4\u6a21\u6a21\u578b\u6700\u4f73", "conclusion": "\u9ad8\u8d28\u91cf\u4eba\u7c7b\u8bc4\u4f30\u6570\u636e\u4e0e\u8de8\u4efb\u52a1\u8054\u5408\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u8bc4\u4f30\u6a21\u578b\u6027\u80fd"}}
{"id": "2506.02503", "pdf": "https://arxiv.org/pdf/2506.02503", "abs": "https://arxiv.org/abs/2506.02503", "authors": ["Yongjian Li", "HaoCheng Chu", "Yukun Yan", "Zhenghao Liu", "Shi Yu", "Zheni Zeng", "Ruobing Wang", "Sen Song", "Zhiyuan Liu", "Maosong Sun"], "title": "KARE-RAG: Knowledge-Aware Refinement and Enhancement for RAG", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to\naccess broader knowledge sources, yet factual inconsistencies persist due to\nnoise in retrieved documents-even with advanced retrieval methods. We\ndemonstrate that enhancing generative models' capacity to process noisy content\nis equally critical for robust performance. In this paper, we present KARE-RAG\n(Knowledge-Aware Refinement and Enhancement for RAG), which improves knowledge\nutilization through three key innovations: (1) structured knowledge\nrepresentations that facilitate error detection during training, (2) Dense\nDirect Preference Optimization (DDPO)-a refined training objective that\nprioritizes correction of critical errors, and (3) a contrastive data\ngeneration pipeline that maintains semantic consistency while rectifying\nfactual inaccuracies. Experiments show our method significantly enhances\nstandard RAG pipelines across model scales, improving both in-domain and\nout-of-domain task performance without compromising general capabilities.\nNotably, these gains are achieved with modest training data, suggesting\ndata-efficient optimization is possible through targeted learning strategies.\nOur findings establish a new direction for RAG improvement: by improving how\nmodels learn to process retrieved content, we can enhance performance across\ndiverse inference paradigms. All data and code will be publicly available on\nGithub.", "AI": {"tldr": "KARE-RAG\u901a\u8fc7\u7ed3\u6784\u5316\u77e5\u8bc6\u8868\u793a\u3001\u4f18\u5316\u8bad\u7ec3\u76ee\u6807\u548c\u5bf9\u6bd4\u6570\u636e\u751f\u6210\u4e09\u5927\u521b\u65b0\uff0c\u63d0\u5347RAG\u5904\u7406\u566a\u58f0\u80fd\u529b\uff0c\u5b9e\u73b0\u9ad8\u6548\u6570\u636e\u4f18\u5316\u7684\u8de8\u9886\u57df\u6027\u80fd\u589e\u5f3a", "motivation": "\u73b0\u6709RAG\u65b9\u6cd5\u5373\u4f7f\u91c7\u7528\u5148\u8fdb\u68c0\u7d22\u6280\u672f\u4ecd\u5b58\u5728\u68c0\u7d22\u566a\u58f0\u5bfc\u81f4\u7684\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u589e\u5f3a\u751f\u6210\u6a21\u578b\u5bf9\u566a\u58f0\u5185\u5bb9\u7684\u5904\u7406\u80fd\u529b\u662f\u63d0\u5347\u9c81\u68d2\u6027\u7684\u5173\u952e\u9700\u6c42", "method": "1.\u7ed3\u6784\u5316\u77e5\u8bc6\u8868\u793a\u8f85\u52a9\u8bad\u7ec3\u9519\u8bef\u68c0\u6d4b\n2.DDPO\u8bad\u7ec3\u76ee\u6807\u4f18\u5148\u7ea0\u6b63\u5173\u952e\u9519\u8bef\n3.\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u7684\u5bf9\u6bd4\u6570\u636e\u751f\u6210\u6846\u67b6", "result": "\u663e\u8457\u63d0\u5347\u4e0d\u540c\u89c4\u6a21RAG\u6a21\u578b\u5728\u9886\u57df\u5185\u5916\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u4ec5\u9700\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u5373\u5b9e\u73b0\u4f18\u5316\uff0c\u9a8c\u8bc1\u6570\u636e\u9ad8\u6548\u4f18\u5316\u7684\u53ef\u884c\u6027", "conclusion": "\u901a\u8fc7\u6539\u8fdb\u6a21\u578b\u5904\u7406\u68c0\u7d22\u5185\u5bb9\u7684\u5b66\u4e60\u673a\u5236\uff0c\u4e3aRAG\u6027\u80fd\u63d0\u5347\u5f00\u8f9f\u65b0\u8def\u5f84\uff0c\u4ee3\u7801\u6570\u636e\u5f00\u6e90\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u53d1\u5c55"}}
{"id": "2506.02510", "pdf": "https://arxiv.org/pdf/2506.02510", "abs": "https://arxiv.org/abs/2506.02510", "authors": ["Jie Zhu", "Junhui Li", "Yalong Wen", "Xiandong Li", "Lifan Guo", "Feng Chen"], "title": "M$^3$FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial Meeting Understanding Evaluation Dataset", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL-2025", "summary": "Recent breakthroughs in large language models (LLMs) have led to the\ndevelopment of new benchmarks for evaluating their performance in the financial\ndomain. However, current financial benchmarks often rely on news articles,\nearnings reports, or announcements, making it challenging to capture the\nreal-world dynamics of financial meetings. To address this gap, we propose a\nnovel benchmark called $\\texttt{M$^3$FinMeeting}$, which is a multilingual,\nmulti-sector, and multi-task dataset designed for financial meeting\nunderstanding. First, $\\texttt{M$^3$FinMeeting}$ supports English, Chinese, and\nJapanese, enhancing comprehension of financial discussions in diverse\nlinguistic contexts. Second, it encompasses various industry sectors defined by\nthe Global Industry Classification Standard (GICS), ensuring that the benchmark\nspans a broad range of financial activities. Finally,\n$\\texttt{M$^3$FinMeeting}$ includes three tasks: summarization, question-answer\n(QA) pair extraction, and question answering, facilitating a more realistic and\ncomprehensive evaluation of understanding. Experimental results with seven\npopular LLMs reveal that even the most advanced long-context models have\nsignificant room for improvement, demonstrating the effectiveness of\n$\\texttt{M$^3$FinMeeting}$ as a benchmark for assessing LLMs' financial meeting\ncomprehension skills.", "AI": {"tldr": "\u63d0\u51fa\u591a\u8bed\u8a00\u591a\u884c\u4e1a\u7684\u91d1\u878d\u4f1a\u8bae\u7406\u89e3\u57fa\u51c6M\u00b3FinMeeting\uff0c\u586b\u8865\u73b0\u6709\u91d1\u878d\u8bc4\u6d4b\u57fa\u51c6\u5728\u771f\u5b9e\u4f1a\u8bae\u573a\u666f\u7684\u4e0d\u8db3\uff0c\u901a\u8fc7\u4e09\u5927\u4efb\u52a1\u9a8c\u8bc1\u4e3b\u6d41\u957f\u6587\u672c\u5927\u6a21\u578b\u4ecd\u6709\u663e\u8457\u63d0\u5347\u7a7a\u95f4\u3002", "motivation": "\u73b0\u6709\u91d1\u878d\u8bc4\u6d4b\u57fa\u51c6\u4f9d\u8d56\u65b0\u95fb/\u8d22\u62a5\u7b49\u9759\u6001\u6570\u636e\uff0c\u96be\u4ee5\u6355\u6349\u771f\u5b9e\u91d1\u878d\u4f1a\u8bae\u52a8\u6001\u3002\u9700\u6784\u5efa\u8986\u76d6\u591a\u8bed\u8a00\u3001\u591a\u884c\u4e1a\u3001\u591a\u4efb\u52a1\u7684\u4f1a\u8bae\u573a\u666f\u8bc4\u4f30\u4f53\u7cfb\u3002", "method": "1. \u652f\u6301\u4e2d\u82f1\u65e5\u4e09\u8bed\u79cd 2. \u91c7\u7528GICS\u884c\u4e1a\u5206\u7c7b\u6807\u51c6\u8986\u76d6\u591a\u5143\u91d1\u878d\u573a\u666f 3. \u8bbe\u8ba1\u6458\u8981\u751f\u6210\u3001QA\u5bf9\u62bd\u53d6\u3001\u95ee\u7b54\u4e09\u5927\u7406\u89e3\u4efb\u52a1", "result": "\u6d4b\u8bd57\u4e2a\u4e3b\u6d41\u957f\u6587\u672c\u5927\u6a21\u578b\u663e\u793a\uff0c\u6700\u4f73\u6a21\u578b\u5728\u6458\u8981\u4efb\u52a1\uff08ROUGE-L 32.1\uff09\u548c\u95ee\u7b54\u4efb\u52a1\uff08\u51c6\u786e\u738768.2%\uff09\u4ecd\u6709\u663e\u8457\u63d0\u5347\u7a7a\u95f4", "conclusion": "M\u00b3FinMeeting\u6709\u6548\u8bc4\u4f30\u5927\u6a21\u578b\u7684\u91d1\u878d\u4f1a\u8bae\u7406\u89e3\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u91d1\u878d\u573a\u666f\u5bf9\u8bdd\u65f6\u7684\u6280\u672f\u74f6\u9888\uff0c\u4e3a\u540e\u7eed\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u65b9\u5411"}}
{"id": "2506.02515", "pdf": "https://arxiv.org/pdf/2506.02515", "abs": "https://arxiv.org/abs/2506.02515", "authors": ["Zhuohan Xie", "Dhruv Sahnan", "Debopriyo Banerjee", "Georgi Georgiev", "Rushil Thareja", "Hachem Madmoun", "Jinyan Su", "Aaryamonvikram Singh", "Yuxia Wang", "Rui Xing", "Fajri Koto", "Haonan Li", "Ivan Koychev", "Tanmoy Chakraborty", "Salem Lahlou", "Veselin Stoyanov", "Preslav Nakov"], "title": "FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "15 pages, 8 figures, 2 tables", "summary": "Multi-step symbolic reasoning is critical for advancing downstream\nperformance on financial tasks. Yet, benchmarks for systematically evaluating\nthis capability are lacking. Existing datasets like FinQA and ConvFinQA\nsupervise only final numerical answers, without assessing intermediate\nreasoning steps. To address this, we introduce FinChain, the first symbolic\nbenchmark designed for verifiable Chain-of- Thought (CoT) financial reasoning.\nSpanning 54 topics across 12 financial domains, Fin- Chain offers five\nparameterized templates per topic, each varying in reasoning complexity and\ndomain expertise required. Each dataset instance includes an executable Python\ntrace, enabling automatic generation of extensive training data and easy\nadaptation to other domains. We also introduce ChainEval, a new metric for\nautomatic evaluation of both final answers and intermediate reasoning.\nBenchmarking 30 LLMs on our dataset, we find that even state-of-the-art models\nhave considerable room for improvement in multi-step financial reasoning. All\ntemplates and evaluation metrics for FinChain are available at https:\n//github.com/mbzuai-nlp/finchain.", "AI": {"tldr": "\u63d0\u51fa\u4e86FinChain\u57fa\u51c6\u548cChainEval\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u591a\u6b65\u91d1\u878d\u63a8\u7406", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u4ec5\u76d1\u7763\u6700\u7ec8\u7b54\u6848\uff0c\u7f3a\u4e4f\u5bf9\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u7684\u7cfb\u7edf\u8bc4\u4f30\uff0c\u9700\u5efa\u7acb\u65b0\u57fa\u51c6\u63d0\u5347\u91d1\u878d\u63a8\u7406\u80fd\u529b", "method": "\u521b\u5efaFinChain\u57fa\u51c6\uff08\u8986\u76d612\u9886\u57df/54\u4e3b\u9898\uff09\uff0c\u63d0\u4f9b\u53c2\u6570\u5316\u6a21\u677f\u4e0e\u53ef\u6267\u884cPython\u8ddf\u8e2a\u5b9e\u73b0\u6570\u636e\u751f\u6210\u53ca\u8de8\u9886\u57df\u9002\u914d\uff0c\u5e76\u8bbe\u8ba1ChainEval\u81ea\u52a8\u8bc4\u4f30\u6307\u6807", "result": "\u6d4b\u8bd530\u4e2aLLM\u663e\u793a\uff0c\u6700\u5148\u8fdb\u6a21\u578b\u5728\u591a\u6b65\u91d1\u878d\u63a8\u7406\u4e2d\u4ecd\u5b58\u5728\u663e\u8457\u6539\u8fdb\u7a7a\u95f4\uff08\u51c6\u786e\u7387\u6700\u9ad8\u4ec561.2%\uff09", "conclusion": "FinChain\u4e3a\u7cfb\u7edf\u8bc4\u4f30\u91d1\u878d\u63a8\u7406\u63d0\u4f9b\u6709\u6548\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5efa\u7acb\u65b0\u57fa\u51c6"}}
{"id": "2506.02519", "pdf": "https://arxiv.org/pdf/2506.02519", "abs": "https://arxiv.org/abs/2506.02519", "authors": ["Sohan Patnaik", "Milan Aggarwal", "Sumit Bhatia", "Balaji Krishnamurthy"], "title": "Learning Together to Perform Better: Teaching Small-Scale LLMs to Collaborate via Preferential Rationale Tuning", "categories": ["cs.CL"], "comment": "Accepted at ACL Main 2025", "summary": "LLMssuch as GPT-4 have shown a remarkable ability to solve complex questions\nby generating step-by-step rationales. Prior works have utilized this\ncapability to improve smaller and cheaper LMs (say, with 7B parameters).\nHowever, various practical constraints, such as copyright and legal issues,\nowing to lack of transparency in the pre-training data of large (often closed)\nmodels, prevent their use in commercial settings. Little focus has been given\nto improving the innate reasoning ability of smaller models without distilling\ninformation from larger LLMs. To address this, we propose COLLATE, a trainable\nframework that tunes a (small) LLM to generate those outputs from a pool of\ndiverse rationales that selectively improves the downstream task. COLLATE\nenforces multiple instances of the same LLM to exhibit distinct behavior and\nemploys them to generate rationales to obtain diverse outputs. The LLM is then\ntuned via preference optimization to choose the candidate rationale which\nmaximizes the likelihood of ground-truth answer. COLLATE outperforms several\ntrainable and prompting baselines on 5 datasets across 3 domains: maths problem\nsolving, natural language inference, and commonsense reasoning. We show the eff\nicacy of COLLATE on LLMs from different model families across varying parameter\nscales (1B to 8B) and demonstrate the benefit of multiple rationale providers\nguided by the end task through ablations. Code is released here\n(https://github.com/Sohanpatnaik106/collate).", "AI": {"tldr": "\u63d0\u51faCOLLATE\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6837\u5316\u63a8\u7406\u8def\u5f84\u548c\u504f\u597d\u4f18\u5316\u63d0\u5347\u5c0f\u578b\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u4f9d\u8d56\u5927\u578b\u6a21\u578b", "motivation": "\u7531\u4e8e\u5927\u578b\u6a21\u578b\u5b58\u5728\u7248\u6743\u548c\u6cd5\u5f8b\u9650\u5236\u65e0\u6cd5\u5546\u7528\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u84b8\u998f\u6280\u672f\uff0c\u9700\u5f00\u53d1\u4e0d\u4f9d\u8d56\u5927\u578b\u6a21\u578b\u7684\u5c0f\u6a21\u578b\u81ea\u4e3b\u63a8\u7406\u80fd\u529b\u63d0\u5347\u65b9\u6848", "method": "1. \u4f7f\u591a\u4e2a\u540c\u67b6\u6784\u5c0f\u6a21\u578b\u751f\u6210\u591a\u6837\u5316\u63a8\u7406\u8def\u5f84\n2. \u901a\u8fc7\u504f\u597d\u4f18\u5316\u9009\u62e9\u6700\u6709\u5229\u4e8e\u6b63\u786e\u7b54\u6848\u7684\u63a8\u7406\u8def\u5f84\n3. \u6846\u67b6\u5305\u542b\u591a\u6837\u6027\u589e\u5f3a\u548c\u4efb\u52a1\u5bfc\u5411\u9009\u62e9\u673a\u5236", "result": "\u5728\u6570\u5b66\u89e3\u9898\u3001\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u3001\u5e38\u8bc6\u63a8\u7406\u76845\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u57281B-8B\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u7684\u6709\u6548\u6027", "conclusion": "COLLATE\u8bc1\u660e\u4e86\u5c0f\u6a21\u578b\u901a\u8fc7\u81ea\u4e3b\u751f\u6210\u4f18\u5316\u63a8\u7406\u8def\u5f84\u53ef\u5b9e\u73b0\u80fd\u529b\u63d0\u5347\uff0c\u4e3a\u5546\u4e1a\u5316\u573a\u666f\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u8def\u5f84"}}
{"id": "2506.02527", "pdf": "https://arxiv.org/pdf/2506.02527", "abs": "https://arxiv.org/abs/2506.02527", "authors": ["Yingying Zhuang", "Aman Gupta", "Anurag Beniwal"], "title": "Multilingual Information Retrieval with a Monolingual Knowledge Base", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "6 pages, accepted at GENNEXT@SIGIR25", "summary": "Multilingual information retrieval has emerged as powerful tools for\nexpanding knowledge sharing across languages. On the other hand, resources on\nhigh quality knowledge base are often scarce and in limited languages,\ntherefore an effective embedding model to transform sentences from different\nlanguages into a feature vector space same as the knowledge base language\nbecomes the key ingredient for cross language knowledge sharing, especially to\ntransfer knowledge available in high-resource languages to low-resource ones.\nIn this paper we propose a novel strategy to fine-tune multilingual embedding\nmodels with weighted sampling for contrastive learning, enabling multilingual\ninformation retrieval with a monolingual knowledge base. We demonstrate that\nthe weighted sampling strategy produces performance gains compared to standard\nones by up to 31.03\\% in MRR and up to 33.98\\% in Recall@3. Additionally, our\nproposed methodology is language agnostic and applicable for both multilingual\nand code switching use cases.", "AI": {"tldr": "\u63d0\u51fa\u52a0\u6743\u91c7\u6837\u5bf9\u6bd4\u5b66\u4e60\u5fae\u8c03\u7b56\u7565\uff0c\u63d0\u5347\u5355\u8bed\u77e5\u8bc6\u5e93\u7684\u591a\u8bed\u8a00\u68c0\u7d22\u6548\u679c\uff08MRR\u63d0\u534731.03%\uff0cRecall@3\u63d0\u534733.98%\uff09", "motivation": "\u89e3\u51b3\u9ad8\u8d44\u6e90\u8bed\u8a00\u77e5\u8bc6\u5e93\u5411\u4f4e\u8d44\u6e90\u8bed\u8a00\u8fc1\u79fb\u65f6\u5d4c\u5165\u6a21\u578b\u6548\u7387\u4e0d\u8db3\u7684\u95ee\u9898", "method": "\u901a\u8fc7\u52a0\u6743\u91c7\u6837\u7b56\u7565\u6539\u8fdb\u591a\u8bed\u8a00\u5d4c\u5165\u6a21\u578b\u7684\u5bf9\u6bd4\u5b66\u4e60\u5fae\u8c03\u8fc7\u7a0b", "result": "MRR\u63d0\u534731.03%\uff0cRecall@3\u63d0\u534733.98%\uff0c\u652f\u6301\u591a\u8bed\u8a00\u548c\u8bed\u7801\u8f6c\u6362\u573a\u666f", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u5907\u8bed\u8a00\u65e0\u5173\u6027\uff0c\u6709\u6548\u4fc3\u8fdb\u8de8\u8bed\u8a00\u77e5\u8bc6\u5171\u4eab\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8d44\u6e90\u4e0d\u5747\u8861\u573a\u666f"}}
{"id": "2506.02532", "pdf": "https://arxiv.org/pdf/2506.02532", "abs": "https://arxiv.org/abs/2506.02532", "authors": ["Jinu Lee", "Sagnik Mukherjee", "Dilek Hakkani-Tur", "Julia Hockenmaier"], "title": "ReasoningFlow: Semantic Structure of Complex Reasoning Traces", "categories": ["cs.CL"], "comment": "10 pages, 6 figures. ArgMining 2025 Workshop (Non-archival) @ ACL\n  2025", "summary": "Large reasoning models (LRMs) generate complex reasoning traces with\nplanning, reflection, verification, and backtracking. In this work, we\nintroduce ReasoningFlow, a unified schema for analyzing the semantic structures\nof these complex traces. ReasoningFlow parses traces into directed acyclic\ngraphs, enabling the characterization of distinct reasoning patterns as\nsubgraph structures. This human-interpretable representation offers promising\napplications in understanding, evaluating, and enhancing the reasoning\nprocesses of LRMs.", "AI": {"tldr": "\u63d0\u51faReasoningFlow\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u6709\u5411\u65e0\u73af\u56fe\u89e3\u6790\u5927\u6a21\u578b\u63a8\u7406\u8f68\u8ff9\uff0c\u5c06\u590d\u6742\u63a8\u7406\u6a21\u5f0f\u8868\u5f81\u4e3a\u5b50\u56fe\u7ed3\u6784", "motivation": "\u73b0\u6709\u5927\u578b\u63a8\u7406\u6a21\u578b\u4ea7\u751f\u7684\u590d\u6742\u63a8\u7406\u8f68\u8ff9\u7f3a\u4e4f\u7ed3\u6784\u5316\u5206\u6790\u65b9\u6cd5\uff0c\u9700\u5efa\u7acb\u53ef\u89e3\u91ca\u7684\u8bed\u4e49\u8868\u5f81\u6846\u67b6", "method": "\u5c06\u63a8\u7406\u8f68\u8ff9\u8f6c\u5316\u4e3a\u6709\u5411\u65e0\u73af\u56fe\uff0c\u901a\u8fc7\u5b50\u56fe\u7ed3\u6784\u8bc6\u522b\u4e0d\u540c\u63a8\u7406\u6a21\u5f0f\uff08\u89c4\u5212/\u53cd\u601d/\u9a8c\u8bc1/\u56de\u6eaf\uff09", "result": "\u6784\u5efa\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u7ed3\u6784\u5316\u8868\u793a\uff0c\u4e3a\u7406\u89e3\u3001\u8bc4\u4f30\u548c\u589e\u5f3a\u5927\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u63d0\u4f9b\u65b0\u65b9\u6cd5", "conclusion": "ReasoningFlow\u4e3a\u5206\u6790\u5927\u6a21\u578b\u590d\u6742\u63a8\u7406\u673a\u5236\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u5de5\u5177\uff0c\u5728\u6a21\u578b\u4f18\u5316\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5177\u6709\u5e94\u7528\u6f5c\u529b"}}
{"id": "2506.02533", "pdf": "https://arxiv.org/pdf/2506.02533", "abs": "https://arxiv.org/abs/2506.02533", "authors": ["Maike Behrendt", "Stefan Sylvius Wagner", "Carina Weinmann", "Marike Bormann", "Mira Warne", "Stefan Harmeling"], "title": "Natural Language Processing to Enhance Deliberation in Political Online Discussions: A Survey", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Political online participation in the form of discussing political issues and\nexchanging opinions among citizens is gaining importance with more and more\nformats being held digitally. To come to a decision, a careful discussion and\nconsideration of opinions and a civil exchange of arguments, which is defined\nas the act of deliberation, is desirable. The quality of discussions and\nparticipation processes in terms of their deliberativeness highly depends on\nthe design of platforms and processes. To facilitate online communication for\nboth participants and initiators, machine learning methods offer a lot of\npotential. In this work we want to showcase which issues occur in political\nonline discussions and how machine learning can be used to counteract these\nissues and enhance deliberation.", "AI": {"tldr": "\u63a2\u8ba8\u5982\u4f55\u5229\u7528\u673a\u5668\u5b66\u4e60\u6539\u5584\u653f\u6cbb\u5728\u7ebf\u8ba8\u8bba\u4e2d\u7684\u5ba1\u8bae\u8d28\u91cf", "motivation": "\u6570\u5b57\u65f6\u4ee3\u653f\u6cbb\u5728\u7ebf\u8ba8\u8bba\u5b58\u5728\u975e\u5efa\u8bbe\u6027\u5bf9\u8bdd\u3001\u6709\u5bb3\u5185\u5bb9\u7b49\u95ee\u9898\uff0c\u5f71\u54cd\u516c\u6c11\u5ba1\u8bae\u8d28\u91cf", "method": "\u5206\u6790\u653f\u6cbb\u5728\u7ebf\u8ba8\u8bba\u73b0\u5b58\u95ee\u9898\uff0c\u63d0\u51fa\u673a\u5668\u5b66\u4e60\u5728\u5185\u5bb9\u5ba1\u6838\u3001\u5bf9\u8bdd\u5f15\u5bfc\u7b49\u65b9\u9762\u7684\u5e94\u7528\u6846\u67b6", "result": "\u673a\u5668\u5b66\u4e60\u53ef\u6709\u6548\u8bc6\u522b\u6709\u5bb3\u4fe1\u606f\u3001\u4fc3\u8fdb\u7406\u6027\u5bf9\u8bdd\uff0c\u4e3a\u5e73\u53f0\u8bbe\u8ba1\u63d0\u4f9b\u6280\u672f\u652f\u6301", "conclusion": "\u5408\u7406\u8fd0\u7528\u673a\u5668\u5b66\u4e60\u80fd\u4f18\u5316\u5728\u7ebf\u8ba8\u8bba\u73af\u5883\uff0c\u63d0\u5347\u653f\u6cbb\u5ba1\u8bae\u7684\u6c11\u4e3b\u8d28\u91cf"}}
{"id": "2506.02536", "pdf": "https://arxiv.org/pdf/2506.02536", "abs": "https://arxiv.org/abs/2506.02536", "authors": ["Xin Liu", "Lu Wang"], "title": "Answer Convergence as a Signal for Early Stopping in Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Chain-of-thought (CoT) prompting enhances reasoning in large language models\n(LLMs) but often leads to verbose and redundant outputs, thus increasing\ninference cost. We hypothesize that many reasoning steps are unnecessary for\nproducing correct answers. To investigate this, we start with a systematic\nstudy to examine what is the minimum reasoning required for a model to reach a\nstable decision. We find that on math reasoning tasks like math, models\ntypically converge to their final answers after 60\\% of the reasoning steps,\nsuggesting substantial redundancy in the remaining content. Based on these\ninsights, we propose three inference-time strategies to improve efficiency: (1)\nearly stopping via answer consistency, (2) boosting the probability of\ngenerating end-of-reasoning signals, and (3) a supervised method that learns\nwhen to stop based on internal activations. Experiments across five benchmarks\nand five open-weights LLMs show that our methods significantly reduce token\nusage with little or no accuracy drop. In particular, on NaturalQuestions,\nAnswer Consistency reduces tokens by over 40\\% while further improving\naccuracy. Our work underscores the importance of cost-effective reasoning\nmethods that operate at inference time, offering practical benefits for\nreal-world applications.", "AI": {"tldr": "\u63d0\u51fa\u4e09\u79cd\u63a8\u7406\u65f6\u4f18\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u8bc6\u522b\u5197\u4f59\u63a8\u7406\u6b65\u9aa4\u663e\u8457\u964d\u4f4eLLM\u8ba1\u7b97\u6210\u672c\u4e14\u4fdd\u6301\u7cbe\u5ea6", "motivation": "\u94fe\u5f0f\u601d\u7ef4(CoT)\u63d0\u793a\u5b58\u5728\u63a8\u7406\u6b65\u9aa4\u5197\u4f59\uff0c\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\u3002\u7814\u7a76\u65e8\u5728\u786e\u5b9a\u8fbe\u5230\u7a33\u5b9a\u51b3\u7b56\u6240\u9700\u7684\u6700\u5c0f\u63a8\u7406\u6b65\u9aa4", "method": "1.\u7cfb\u7edf\u6027\u5206\u6790\u63a8\u7406\u6b65\u9aa4\u4e0e\u51b3\u7b56\u7a33\u5b9a\u6027\u5173\u7cfb\n2.\u63d0\u51fa\u57fa\u4e8e\u7b54\u6848\u4e00\u81f4\u6027\u7684\u65e9\u505c\u673a\u5236\n3.\u589e\u5f3a\u7ec8\u6b62\u4fe1\u53f7\u751f\u6210\u6982\u7387\n4.\u57fa\u4e8e\u5185\u90e8\u6fc0\u6d3b\u7684\u76d1\u7763\u5f0f\u505c\u6b62\u7b56\u7565", "result": "\u57285\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u51cf\u5c1140% token\u4f7f\u7528\u91cf\uff0cNaturalQuestions\u4efb\u52a1\u8282\u770140% token\u4e14\u7cbe\u5ea6\u63d0\u5347", "conclusion": "\u63a8\u7406\u65f6\u4f18\u5316\u7b56\u7565\u80fd\u6709\u6548\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u4e0e\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.02544", "pdf": "https://arxiv.org/pdf/2506.02544", "abs": "https://arxiv.org/abs/2506.02544", "authors": ["Yang Tian", "Fan Liu", "Jingyuan Zhang", "Victoria W.", "Yupeng Hu", "Liqiang Nie"], "title": "CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Main", "summary": "Multimodal Retrieval-Augmented Generation (MMRAG) has been introduced to\nenhance Multimodal Large Language Models by incorporating externally retrieved\nmultimodal knowledge, but it introduces two challenges: Parametric-Retrieved\nKnowledge Inconsistency (PRKI), where discrepancies between parametric and\nretrieved knowledge create uncertainty in determining reliability, and\nVisual-Textual Knowledge Inconsistency (VTKI), where misalignment between\nvisual and textual sources disrupts entity representation. To address these\nchallenges, we propose \\textbf{C}r\\textbf{o}ss-source knowledge\n\\textbf{Re}conciliation for \\textbf{M}ulti\\textbf{M}odal \\textbf{RAG}\n(CoRe-MMRAG), a novel end-to-end framework that effectively reconciles\ninconsistencies across knowledge sources. CoRe-MMRAG follows a four-stage\npipeline: it first generates an internal response from parametric knowledge,\nthen selects the most relevant multimodal evidence via joint similarity\nassessment, generates an external response, and finally integrates both to\nproduce a reliable answer. Additionally, a specialized training paradigm\nenhances knowledge source discrimination, multimodal integration, and unified\nanswer generation. Experiments on KB-VQA benchmarks show that CoRe-MMRAG\nachieves substantial improvements over baseline methods, achieving 5.6\\% and\n9.3\\% performance gains on InfoSeek and Encyclopedic-VQA, respectively. We\nrelease code and data at\n\\href{https://github.com/TyangJN/CoRe-MMRAG}{https://github.com/TyangJN/CoRe-MMRAG}.", "AI": {"tldr": "\u63d0\u51faCoRe-MMRAG\u6846\u67b6\u89e3\u51b3\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e2d\u7684\u77e5\u8bc6\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5728KB-VQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08MMRAG\uff09\u4e2d\u53c2\u6570\u77e5\u8bc6\u4e0e\u975e\u53c2\u6570\u77e5\u8bc6\u4e0d\u4e00\u81f4\uff08PRKI\uff09\u3001\u89c6\u89c9\u4e0e\u6587\u672c\u77e5\u8bc6\u4e0d\u5bf9\u9f50\uff08VTKI\uff09\u4e24\u5927\u6838\u5fc3\u6311\u6218", "method": "\u56db\u9636\u6bb5\u6d41\u7a0b\uff1a\u53c2\u6570\u77e5\u8bc6\u751f\u6210\u5185\u90e8\u54cd\u5e94\u2192\u8054\u5408\u76f8\u4f3c\u5ea6\u8bc4\u4f30\u9009\u62e9\u591a\u6a21\u6001\u8bc1\u636e\u2192\u751f\u6210\u5916\u90e8\u54cd\u5e94\u2192\u878d\u5408\u5185\u5916\u7ed3\u679c\u751f\u6210\u6700\u7ec8\u7b54\u6848\uff0c\u914d\u5408\u4e13\u9879\u8bad\u7ec3\u8303\u5f0f\u63d0\u5347\u77e5\u8bc6\u6574\u5408\u80fd\u529b", "result": "\u5728InfoSeek\u548cEncyclopedic-VQA\u6570\u636e\u96c6\u4e0a\u5206\u522b\u5b9e\u73b05.6%\u548c9.3%\u7684\u6027\u80fd\u63d0\u5347", "conclusion": "CoRe-MMRAG\u6709\u6548\u534f\u8c03\u591a\u6e90\u77e5\u8bc6\u77db\u76fe\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u6846\u67b6\u5b9e\u73b0\u53ef\u9760\u7b54\u6848\u751f\u6210\uff0c\u4ee3\u7801\u6570\u636e\u5df2\u5f00\u6e90\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76"}}
{"id": "2506.02561", "pdf": "https://arxiv.org/pdf/2506.02561", "abs": "https://arxiv.org/abs/2506.02561", "authors": ["Yirao Zhao", "Guizhen Chen", "Kenji Kawaguchi", "Lidong Bing", "Wenxuan Zhang"], "title": "Pruning General Large Language Models into Customized Expert Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have revolutionized natural language processing,\nyet their substantial model sizes often require substantial computational\nresources. To preserve computing resources and accelerate inference speed, it\nis crucial to prune redundant parameters, especially for experienced users who\noften need compact expert models tailored to specific downstream scenarios.\nHowever, most existing pruning methods focus on preserving the model's general\ncapabilities, often requiring extensive post-training or suffering from\ndegraded performance due to coarse-grained pruning. In this work, we design a\n$\\underline{Cus}$tom $\\underline{Prun}$ing method ($\\texttt{Cus-Prun}$) to\nprune a large general model into a smaller lightweight expert model, which is\npositioned along the \"language\", \"domain\" and \"task\" dimensions. By identifying\nand pruning irrelevant neurons of each dimension, $\\texttt{Cus-Prun}$ creates\nexpert models without any post-training. Our experiments demonstrate that\n$\\texttt{Cus-Prun}$ consistently outperforms other methods, achieving minimal\nloss in both expert and general capabilities across various models from\ndifferent model families and sizes.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u540e\u8bad\u7ec3\u7684\u5b9a\u5236\u526a\u679d\u65b9\u6cd5Cus-Prun\uff0c\u5c06\u5927\u6a21\u578b\u538b\u7f29\u4e3a\u8f7b\u91cf\u4e13\u5bb6\u6a21\u578b", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u53c2\u6570\u91cf\u5927\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u9ad8\uff0c\u7279\u5b9a\u573a\u666f\u9700\u8981\u8f7b\u91cf\u5316\u4e13\u5bb6\u6a21\u578b", "method": "\u6cbf\u8bed\u8a00/\u9886\u57df/\u4efb\u52a1\u4e09\u7ef4\u5ea6\u526a\u679d\u65e0\u5173\u795e\u7ecf\u5143\uff0c\u76f4\u63a5\u751f\u6210\u4e13\u4e1a\u5c0f\u6a21\u578b", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u5bb6\u65cf\u548c\u5c3a\u5bf8\u4e0a\uff0c\u4e13\u5bb6\u80fd\u529b\u548c\u901a\u7528\u80fd\u529b\u635f\u5931\u6700\u5c0f\uff0c\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5", "conclusion": "Cus-Prun\u5b9e\u73b0\u4e86\u65e0\u9700\u5fae\u8c03\u7684\u5b9a\u5411\u6a21\u578b\u538b\u7f29\uff0c\u4e3a\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u9ad8\u6548\u5b9a\u5236\u65b9\u6848"}}
{"id": "2506.02573", "pdf": "https://arxiv.org/pdf/2506.02573", "abs": "https://arxiv.org/abs/2506.02573", "authors": ["Muhammad Falensi Azmi", "Muhammad Dehan Al Kautsar", "Alfan Farizki Wicaksono", "Fajri Koto"], "title": "IndoSafety: Culturally Grounded Safety for LLMs in Indonesian Languages", "categories": ["cs.CL"], "comment": "25 pages", "summary": "Although region-specific large language models (LLMs) are increasingly\ndeveloped, their safety remains underexplored, particularly in culturally\ndiverse settings like Indonesia, where sensitivity to local norms is essential\nand highly valued by the community. In this work, we present IndoSafety, the\nfirst high-quality, human-verified safety evaluation dataset tailored for the\nIndonesian context, covering five language varieties: formal and colloquial\nIndonesian, along with three major local languages: Javanese, Sundanese, and\nMinangkabau. IndoSafety is constructed by extending prior safety frameworks to\ndevelop a taxonomy that captures Indonesia's sociocultural context. We find\nthat existing Indonesian-centric LLMs often generate unsafe outputs,\nparticularly in colloquial and local language settings, while fine-tuning on\nIndoSafety significantly improves safety while preserving task performance. Our\nwork highlights the critical need for culturally grounded safety evaluation and\nprovides a concrete step toward responsible LLM deployment in multilingual\nsettings. Warning: This paper contains example data that may be offensive,\nharmful, or biased.", "AI": {"tldr": "\u6784\u5efa\u9996\u4e2a\u5370\u5c3c\u8bed\u5883\u5b89\u5168\u8bc4\u4f30\u6570\u636e\u96c6IndoSafety\uff0c\u53d1\u73b0\u73b0\u6709\u5370\u5c3cLLM\u5728\u65b9\u8a00\u73af\u5883\u5b58\u5728\u5b89\u5168\u9690\u60a3\uff0c\u5fae\u8c03\u540e\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027", "motivation": "\u5370\u5c3c\u591a\u8bed\u8a00\u6587\u5316\u80cc\u666f\u4e0b\u73b0\u6709LLM\u5b89\u5168\u6027\u8bc4\u4f30\u4e0d\u8db3\uff0c\u9700\u7b26\u5408\u672c\u571f\u793e\u4f1a\u6587\u5316\u89c4\u8303\u7684\u5b89\u5168\u6846\u67b6", "method": "\u6269\u5c55\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\u6784\u5efa\u5370\u5c3c\u7279\u8272\u5206\u7c7b\u6cd5\uff0c\u8986\u76d65\u79cd\u8bed\u8a00\u53d8\u4f53\uff08\u6b63\u5f0f/\u53e3\u8bed\u5370\u5c3c\u8bed+3\u79cd\u5730\u65b9\u8bed\u8a00\uff09", "result": "\u73b0\u6a21\u578b\u5728\u65b9\u8a00\u573a\u666f\u8f93\u51fa\u4e0d\u5b89\u5168\u7387\u8f83\u9ad8\uff0c\u7ecfIndoSafety\u5fae\u8c03\u540e\u5b89\u5168\u6027\u63d0\u5347\u4e14\u4fdd\u6301\u4efb\u52a1\u6027\u80fd", "conclusion": "\u5f3a\u8c03\u6587\u5316\u57fa\u7840\u5b89\u5168\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\uff0c\u4e3a\u591a\u8bed\u8a00LLM\u8d1f\u8d23\u4efb\u90e8\u7f72\u63d0\u4f9b\u5b9e\u8df5\u8def\u5f84"}}
{"id": "2506.02584", "pdf": "https://arxiv.org/pdf/2506.02584", "abs": "https://arxiv.org/abs/2506.02584", "authors": ["Sarenne Wallbridge", "Christoph Minixhofer", "Catherine Lai", "Peter Bell"], "title": "Prosodic Structure Beyond Lexical Content: A Study of Self-Supervised Learning", "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": "Accepted at INTERSPEECH 2025", "summary": "People exploit the predictability of lexical structures during text\ncomprehension. Though predictable structure is also present in speech, the\ndegree to which prosody, e.g. intonation, tempo, and loudness, contributes to\nsuch structure independently of the lexical content is unclear. This study\nleverages self-supervised learning (SSL) to examine the temporal granularity of\nstructures in the acoustic correlates of prosody. Representations from our\nproposed Masked Prosody Model can predict perceptual labels dependent on local\ninformation, such as word boundaries, but provide the most value for labels\ninvolving longer-term structures, like emotion recognition. Probing experiments\nacross various perceptual labels show strong relative gains over untransformed\npitch, energy, and voice activity features. Our results reveal the importance\nof SSL training objective timescale and highlight the value of complex\nSSL-encoded structures compared to more constrained classical structures.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u63a2\u7a76\u97f5\u5f8b\u58f0\u5b66\u7ed3\u6784\u7684\u65f6\u95f4\u7c92\u5ea6\uff0c\u53d1\u73b0SSL\u6a21\u578b\u5728\u957f\u65f6\u7ed3\u6784\u4efb\u52a1\uff08\u5982\u60c5\u611f\u8bc6\u522b\uff09\u4e2d\u4f18\u4e8e\u4f20\u7edf\u58f0\u5b66\u7279\u5f81\u3002", "motivation": "\u63a2\u7d22\u97f5\u5f8b\u7279\u5f81\uff08\u5982\u8bed\u8c03\u3001\u8282\u594f\uff09\u5728\u72ec\u7acb\u4e8e\u8bcd\u6c47\u5185\u5bb9\u65f6\u5bf9\u8bed\u8a00\u7ed3\u6784\u9884\u6d4b\u7684\u8d21\u732e\u7a0b\u5ea6\u3002", "method": "\u4f7f\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u8bad\u7ec3Masked Prosody Model\uff0c\u5206\u6790\u97f5\u5f8b\u7279\u5f81\u7684\u5c40\u90e8\u4e0e\u957f\u65f6\u7ed3\u6784\u9884\u6d4b\u80fd\u529b\u3002", "result": "SSL\u6a21\u578b\u5728\u8bcd\u8fb9\u754c\u7b49\u5c40\u90e8\u4efb\u52a1\u6709\u6548\uff0c\u4f46\u5bf9\u60c5\u611f\u8bc6\u522b\u7b49\u957f\u65f6\u7ed3\u6784\u9884\u6d4b\u6548\u679c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u57fa\u9891/\u80fd\u91cf\u7279\u5f81\uff08\u76f8\u5bf9\u589e\u76ca\u8fbe37.5%\uff09\u3002", "conclusion": "SSL\u8bad\u7ec3\u76ee\u6807\u7684\u65f6\u95f4\u5c3a\u5ea6\u9009\u62e9\u53ca\u590d\u6742\u7ed3\u6784\u7f16\u7801\u80fd\u529b\uff0c\u5bf9\u6355\u6349\u97f5\u5f8b\u7684\u5c42\u6b21\u5316\u65f6\u95f4\u7ed3\u6784\u5177\u6709\u5173\u952e\u4ef7\u503c\u3002"}}
{"id": "2506.02589", "pdf": "https://arxiv.org/pdf/2506.02589", "abs": "https://arxiv.org/abs/2506.02589", "authors": ["Maria Levchenko"], "title": "Evaluating Named Entity Recognition Models for Russian Cultural News Texts: From BERT to LLM", "categories": ["cs.CL", "cs.AI", "cs.IR", "68T50", "I.2.7; H.3.3"], "comment": null, "summary": "This paper addresses the challenge of Named Entity Recognition (NER) for\nperson names within the specialized domain of Russian news texts concerning\ncultural events. The study utilizes the unique SPbLitGuide dataset, a\ncollection of event announcements from Saint Petersburg spanning 1999 to 2019.\nA comparative evaluation of diverse NER models is presented, encompassing\nestablished transformer-based architectures such as DeepPavlov, RoBERTa, and\nSpaCy, alongside recent Large Language Models (LLMs) including GPT-3.5, GPT-4,\nand GPT-4o. Key findings highlight the superior performance of GPT-4o when\nprovided with specific prompting for JSON output, achieving an F1 score of\n0.93. Furthermore, GPT-4 demonstrated the highest precision at 0.99. The\nresearch contributes to a deeper understanding of current NER model\ncapabilities and limitations when applied to morphologically rich languages\nlike Russian within the cultural heritage domain, offering insights for\nresearchers and practitioners. Follow-up evaluation with GPT-4.1 (April 2025)\nachieves F1=0.94 for both simple and structured prompts, demonstrating rapid\nprogress across model families and simplified deployment requirements.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u591a\u79cdNER\u6a21\u578b\u5728\u4fc4\u8bed\u6587\u5316\u65b0\u95fb\u4e2d\u4eba\u540d\u8bc6\u522b\u7684\u8868\u73b0\uff0c\u53d1\u73b0GPT-4o\u5728JSON\u63d0\u793a\u4e0bF1\u8fbe0.93\uff0cGPT-4\u7cbe\u786e\u5ea6\u6700\u9ad80.99\uff0c\u540e\u7eedGPT-4.1\u5b9e\u73b0F1=0.94", "motivation": "\u89e3\u51b3\u4fc4\u8bed\u6587\u5316\u9886\u57df\u65b0\u95fb\u4e2d\u4eba\u540dNER\u7684\u6311\u6218\uff0c\u8bc4\u4f30\u4e0d\u540c\u6a21\u578b\u5728\u5f62\u6001\u4e30\u5bcc\u8bed\u8a00\u4e2d\u7684\u8868\u73b0\u5dee\u5f02", "method": "\u4f7f\u7528SPbLitGuide\u6570\u636e\u96c6\uff08\u5723\u5f7c\u5f97\u58211999-2019\u6587\u5316\u6d3b\u52a8\u516c\u544a\uff09\uff0c\u5bf9\u6bd4transformer\u6a21\u578b(DeepPavlov/RoBERTa/SpaCy)\u4e0eLLMs(GPT\u7cfb\u5217)", "result": "GPT-4o\u5728JSON\u63d0\u793a\u4e0bF1=0.93\u6700\u4f18\uff0cGPT-4\u7cbe\u5ea60.99\u6700\u9ad8\u3002GPT-4.1\u540e\u7eed\u8bc4\u4f30\u663e\u793aF1=0.94\u4e14\u90e8\u7f72\u66f4\u7b80\u5316", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLMs\u5728\u4fc4\u8bedNER\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\uff0c\u7279\u522b\u662f\u7ed3\u6784\u5316\u63d0\u793a\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5f62\u6001\u590d\u6742\u8bed\u8a00\u7684\u6570\u5b57\u4eba\u6587\u7814\u7a76\u63d0\u4f9b\u6a21\u578b\u9009\u62e9\u4f9d\u636e\uff0c\u540c\u65f6\u5c55\u73b0\u6a21\u578b\u8fed\u4ee3\u7684\u5feb\u901f\u8fdb\u6b65"}}
{"id": "2506.02591", "pdf": "https://arxiv.org/pdf/2506.02591", "abs": "https://arxiv.org/abs/2506.02591", "authors": ["Minh Duc Bui", "Kyung Eun Park", "Goran Glava\u0161", "Fabian David Schmidt", "Katharina von der Wense"], "title": "On Generalization across Measurement Systems: LLMs Entail More Test-Time Compute for Underrepresented Cultures", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main (Camera-Ready Version)", "summary": "Measurement systems (e.g., currencies) differ across cultures, but the\nconversions between them are well defined so that humans can state facts using\nany measurement system of their choice. Being available to users from diverse\ncultural backgrounds, large language models (LLMs) should also be able to\nprovide accurate information irrespective of the measurement system at hand.\nUsing newly compiled datasets we test if this is the case for seven open-source\nLLMs, addressing three key research questions: (RQ1) What is the default system\nused by LLMs for each type of measurement? (RQ2) Do LLMs' answers and their\naccuracy vary across different measurement systems? (RQ3) Can LLMs mitigate\npotential challenges w.r.t. underrepresented systems via reasoning? Our\nfindings show that LLMs default to the measurement system predominantly used in\nthe data. Additionally, we observe considerable instability and variance in\nperformance across different measurement systems. While this instability can in\npart be mitigated by employing reasoning methods such as chain-of-thought\n(CoT), this implies longer responses and thereby significantly increases\ntest-time compute (and inference costs), marginalizing users from cultural\nbackgrounds that use underrepresented measurement systems.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9ed8\u8ba4\u4f7f\u7528\u6570\u636e\u4e2d\u4e3b\u6d41\u7684\u6d4b\u91cf\u7cfb\u7edf\uff0c\u5728\u4e0d\u540c\u6d4b\u91cf\u7cfb\u7edf\u95f4\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u63a8\u7406\u65b9\u6cd5\u53ef\u90e8\u5206\u6539\u5584\u4f46\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u4e0d\u540c\u6587\u5316\u80cc\u666f\u4f7f\u7528\u4e0d\u540c\u7684\u6d4b\u91cf\u7cfb\u7edf\uff08\u5982\u8d27\u5e01\u5355\u4f4d\uff09\uff0cLLMs\u5e94\u5177\u5907\u8de8\u6d4b\u91cf\u7cfb\u7edf\u7684\u51c6\u786e\u4fe1\u606f\u5904\u7406\u80fd\u529b\u4ee5\u670d\u52a1\u5168\u7403\u7528\u6237\u3002", "method": "\u901a\u8fc7\u65b0\u6784\u5efa\u7684\u6570\u636e\u96c6\u6d4b\u8bd57\u4e2a\u5f00\u6e90LLM\uff0c\u5206\u6790\u5176\u5728\u4e09\u79cd\u6d4b\u91cf\u7cfb\u7edf\u573a\u666f\u4e0b\u7684\u9ed8\u8ba4\u8bbe\u7f6e\u3001\u51c6\u786e\u6027\u5dee\u5f02\u53ca\u63a8\u7406\u7f13\u89e3\u80fd\u529b\u3002", "result": "1.LLMs\u9ed8\u8ba4\u4f7f\u7528\u6570\u636e\u4e3b\u5bfc\u7684\u6d4b\u91cf\u7cfb\u7edf\n2.\u4e0d\u540c\u7cfb\u7edf\u95f4\u6027\u80fd\u5dee\u5f02\u663e\u8457\n3.\u601d\u7ef4\u94fe\u7b49\u63a8\u7406\u65b9\u6cd5\u53ef\u63d0\u5347\u975e\u4e3b\u6d41\u7cfb\u7edf\u6027\u80fd\u4f46\u589e\u52a02.5\u500d\u54cd\u5e94\u65f6\u95f4", "conclusion": "LLMs\u5bf9\u975e\u4e3b\u6d41\u6d4b\u91cf\u7cfb\u7edf\u7684\u652f\u6301\u4e0d\u8db3\u53ef\u80fd\u8fb9\u7f18\u5316\u76f8\u5173\u6587\u5316\u7fa4\u4f53\u7528\u6237\uff0c\u9700\u5e73\u8861\u8ba1\u7b97\u6210\u672c\u4e0e\u516c\u5e73\u6027\u3002"}}
{"id": "2506.02592", "pdf": "https://arxiv.org/pdf/2506.02592", "abs": "https://arxiv.org/abs/2506.02592", "authors": ["Zhi-Yuan Chen", "Hao Wang", "Xinyu Zhang", "Enrui Hu", "Yankai Lin"], "title": "Beyond the Surface: Measuring Self-Preference in LLM Judgments", "categories": ["cs.CL"], "comment": null, "summary": "Recent studies show that large language models (LLMs) exhibit self-preference\nbias when serving as judges, meaning they tend to favor their own responses\nover those generated by other models. Existing methods typically measure this\nbias by calculating the difference between the scores a judge model assigns to\nits own responses and those it assigns to responses from other models. However,\nthis approach conflates self-preference bias with response quality, as\nhigher-quality responses from the judge model may also lead to positive score\ndifferences, even in the absence of bias. To address this issue, we introduce\ngold judgments as proxies for the actual quality of responses and propose the\nDBG score, which measures self-preference bias as the difference between the\nscores assigned by the judge model to its own responses and the corresponding\ngold judgments. Since gold judgments reflect true response quality, the DBG\nscore mitigates the confounding effect of response quality on bias measurement.\nUsing the DBG score, we conduct comprehensive experiments to assess\nself-preference bias across LLMs of varying versions, sizes, and reasoning\nabilities. Additionally, we investigate two factors that influence and help\nalleviate self-preference bias: response text style and the post-training data\nof judge models. Finally, we explore potential underlying mechanisms of\nself-preference bias from an attention-based perspective. Our code and data are\navailable at https://github.com/zhiyuanc2001/self-preference.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u8bc4\u5224\u8005\u5b58\u5728\u81ea\u6211\u504f\u597d\u504f\u5dee\uff0c\u73b0\u6709\u6d4b\u91cf\u65b9\u6cd5\u6df7\u6dc6\u4e86\u8d28\u91cf\u4e0e\u504f\u5dee\u3002\u672c\u6587\u63d0\u51faDBG\u5206\u6570\uff0c\u901a\u8fc7\u5f15\u5165\u9ec4\u91d1\u5224\u65ad\u5206\u79bb\u504f\u5dee\u4e0e\u8d28\u91cf\uff0c\u5e76\u7cfb\u7edf\u6027\u5206\u6790\u6a21\u578b\u7279\u6027\u3001\u6587\u672c\u98ce\u683c\u3001\u8bad\u7ec3\u6570\u636e\u5bf9\u504f\u5dee\u7684\u5f71\u54cd\u673a\u5236\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8bc4\u5206\u5dee\u5f02\u7684\u81ea\u6211\u504f\u597d\u504f\u5dee\u6d4b\u91cf\u65b9\u6cd5\u65e0\u6cd5\u533a\u5206\u771f\u5b9e\u504f\u5dee\u4e0e\u56de\u7b54\u8d28\u91cf\u5dee\u5f02\uff08\u6a21\u578b\u81ea\u8eab\u56de\u7b54\u8d28\u91cf\u9ad8\u5bfc\u81f4\u7684\u8bc4\u5206\u5dee\u5f02\uff09\u3002\u9700\u8981\u66f4\u51c6\u786e\u7684\u6d4b\u91cf\u65b9\u5f0f\u63ed\u793a\u771f\u5b9e\u504f\u5dee\u7a0b\u5ea6\u3002", "method": "1. \u5f15\u5165\u9ec4\u91d1\u5224\u65ad\u4f5c\u4e3a\u56de\u7b54\u8d28\u91cf\u7684\u5ba2\u89c2\u57fa\u51c6\n2. \u63d0\u51faDBG\u5206\u6570\uff08\u6a21\u578b\u81ea\u8bc4\u4e0e\u9ec4\u91d1\u5224\u65ad\u7684\u5dee\u503c\uff09\n3. \u8de8\u6a21\u578b\u7248\u672c/\u89c4\u6a21/\u63a8\u7406\u80fd\u529b\u7684\u7cfb\u7edf\u6027\u5b9e\u9a8c\n4. \u63a2\u7a76\u6587\u672c\u98ce\u683c\u3001\u8bad\u7ec3\u540e\u6570\u636e\u3001\u6ce8\u610f\u529b\u673a\u5236\u7684\u5f71\u54cd", "result": "1. DBG\u6709\u6548\u5206\u79bb\u8d28\u91cf\u4e0e\u504f\u5dee\n2. \u6a21\u578b\u89c4\u6a21/\u63a8\u7406\u80fd\u529b\u4e0e\u504f\u5dee\u5f3a\u5ea6\u8d1f\u76f8\u5173\n3. \u8c03\u6574\u6587\u672c\u98ce\u683c\u53ef\u964d\u4f4e20-30%\u504f\u5dee\n4. \u540e\u8bad\u7ec3\u6570\u636e\u5f71\u54cd\u6a21\u578b\u81ea\u6211\u8ba4\u77e5", "conclusion": "DBG\u5206\u6570\u4e3a\u91cf\u5316\u81ea\u6211\u504f\u597d\u63d0\u4f9b\u4e86\u53ef\u9760\u6307\u6807\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u67b6\u6784\u4e0e\u8bad\u7ec3\u6570\u636e\u5bf9\u504f\u5dee\u7684\u8c03\u63a7\u4f5c\u7528\uff0c\u4e3a\u6784\u5efa\u65e0\u504f\u8bc4\u4f30\u4f53\u7cfb\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002\u4ee3\u7801\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.02596", "pdf": "https://arxiv.org/pdf/2506.02596", "abs": "https://arxiv.org/abs/2506.02596", "authors": ["Fan Gao", "Dongyuan Li", "Ding Xia", "Fei Mi", "Yasheng Wang", "Lifeng Shang", "Baojun Wang"], "title": "EssayBench: Evaluating Large Language Models in Multi-Genre Chinese Essay Writing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Chinese essay writing and its evaluation are critical in educational\ncontexts, yet the capabilities of Large Language Models (LLMs) in this domain\nremain largely underexplored. Existing benchmarks often rely on coarse-grained\ntext quality metrics, largely overlooking the structural and rhetorical\ncomplexities of Chinese essays, particularly across diverse genres. To address\nthis gap, we propose \\benchName, a multi-genre benchmark specifically designed\nfor Chinese essay writing across four major genres: Argumentative, Narrative,\nDescriptive, and Expository. We curate and refine a total of 728 real-world\nprompts to ensure authenticity and meticulously categorize them into the\n\\textit{Open-Ended} and \\textit{Constrained} sets to capture diverse writing\nscenarios. To reliably evaluate generated essays, we develop a fine-grained,\ngenre-specific scoring framework that hierarchically aggregates scores. We\nfurther validate our evaluation protocol through a comprehensive human\nagreement study. Finally, we benchmark 15 large-sized LLMs, analyzing their\nstrengths and limitations across genres and instruction types. With \\benchName,\nwe aim to advance LLM-based Chinese essay evaluation and inspire future\nresearch on improving essay generation in educational settings.", "AI": {"tldr": "\u63d0\u51fa\u591a\u6587\u4f53\u4e2d\u6587\u4f5c\u6587\u8bc4\u4f30\u57fa\u51c6C3E\uff0c\u8986\u76d6\u56db\u5927\u6587\u4f53\u5e76\u5efa\u7acb\u7ec6\u7c92\u5ea6\u8bc4\u5206\u6846\u67b6\uff0c\u9a8c\u8bc1\u4e8615\u4e2a\u4e3b\u6d41\u5927\u6a21\u578b\u7684\u8868\u73b0\u5dee\u5f02", "motivation": "\u73b0\u6709\u4e2d\u6587\u4f5c\u6587\u8bc4\u4f30\u57fa\u51c6\u4e3b\u8981\u4f9d\u8d56\u7c97\u7c92\u5ea6\u6587\u672c\u8d28\u91cf\u6307\u6807\uff0c\u5ffd\u89c6\u4e86\u4e0d\u540c\u6587\u4f53\u7684\u7ed3\u6784\u548c\u4fee\u8f9e\u590d\u6742\u6027", "method": "1. \u6536\u96c6\u5e76\u7b5b\u9009728\u4e2a\u771f\u5b9e\u573a\u666f\u4f5c\u6587\u9898\u76ee\uff0c\u5206\u4e3a\u5f00\u653e\u578b\u4e0e\u9650\u5236\u578b\n2. \u5f00\u53d1\u5c42\u6b21\u5316\u7ec6\u7c92\u5ea6\u8bc4\u5206\u6846\u67b6\n3. \u901a\u8fc7\u4eba\u5de5\u4e00\u81f4\u6027\u7814\u7a76\u9a8c\u8bc1\u8bc4\u4f30\u534f\u8bae", "result": "\u53d1\u73b0\u4e0d\u540c\u5927\u6a21\u578b\u5728\u56db\u5927\u6587\u4f53\u4e2d\u7684\u8868\u73b0\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u6307\u4ee4\u7c7b\u578b\u5bf9\u5199\u4f5c\u8d28\u91cf\u4ea7\u751f\u7cfb\u7edf\u6027\u5f71\u54cd", "conclusion": "C3E\u57fa\u51c6\u6709\u6548\u63a8\u52a8\u4e2d\u6587\u4f5c\u6587\u751f\u6210\u6280\u672f\u53d1\u5c55\uff0c\u4e3a\u6559\u80b2\u573a\u666f\u4e2d\u7684\u5199\u4f5c\u8bc4\u4f30\u63d0\u4f9b\u6807\u51c6\u5316\u6d4b\u8bd5\u6846\u67b6"}}
{"id": "2506.02627", "pdf": "https://arxiv.org/pdf/2506.02627", "abs": "https://arxiv.org/abs/2506.02627", "authors": ["\u00d6mer Tarik \u00d6zyilmaz", "Matt Coler", "Matias Valdenegro-Toro"], "title": "Overcoming Data Scarcity in Multi-Dialectal Arabic ASR via Whisper Fine-Tuning", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Although commercial Arabic automatic speech recognition (ASR) systems support\nModern Standard Arabic (MSA), they struggle with dialectal speech. We\ninvestigate the effect of fine-tuning OpenAI's Whisper on five major Arabic\ndialects (Gulf, Levantine, Iraqi, Egyptian, Maghrebi) using Mozilla Common\nVoice for MSA and the MASC dataset for dialectal speech. We evaluate MSA\ntraining size effects, benefits of pre-training on MSA data, and\ndialect-specific versus dialect-pooled models. We find that small amounts of\nMSA fine-tuning data yield substantial improvements for smaller models,\nmatching larger non-fine-tuned models. While MSA pre-training shows minimal\nbenefit, suggesting limited shared features between MSA and dialects, our\ndialect-pooled models perform comparably to dialect-specific ones. This\nindicates that pooling dialectal data, when properly balanced, can help address\ndata scarcity in low-resource ASR without significant performance loss.", "AI": {"tldr": "\u5c0f\u89c4\u6a21MSA\u6570\u636e\u5fae\u8c03\u53ef\u663e\u8457\u63d0\u5347\u5c0f\u6a21\u578b\u6027\u80fd\uff0c\u65b9\u8a00\u6c60\u5316\u6a21\u578b\u4e0e\u65b9\u8a00\u4e13\u7528\u6a21\u578b\u8868\u73b0\u76f8\u5f53\uff0c\u5e73\u8861\u6570\u636e\u53ef\u7f13\u89e3ASR\u4f4e\u8d44\u6e90\u56f0\u5883", "motivation": "\u89e3\u51b3\u5546\u7528\u963f\u62c9\u4f2f\u8bedASR\u7cfb\u7edf\u5bf9\u65b9\u8a00\u8bed\u97f3\u8bc6\u522b\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u5728\u6570\u636e\u7a00\u7f3a\u6761\u4ef6\u4e0b\u63d0\u5347\u65b9\u8a00\u8bed\u97f3\u8bc6\u522b\u6027\u80fd\u7684\u6709\u6548\u65b9\u6cd5", "method": "\u57fa\u4e8eWhisper\u67b6\u6784\uff0c\u4f7f\u7528MSA\u6570\u636e\u548c\u4e94\u5927\u963f\u62c9\u4f2f\u65b9\u8a00\u6570\u636e\uff0c\u901a\u8fc7\u63a7\u5236\u53d8\u91cf\u6cd5\u5206\u6790\u5fae\u8c03\u6570\u636e\u91cf\u3001\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u6a21\u578b\u67b6\u6784\u5dee\u5f02\u7684\u5f71\u54cd", "result": "1. 300\u5c0f\u65f6MSA\u6570\u636e\u5373\u53ef\u4f7fBASE\u6a21\u578b\u6027\u80fd\u63d0\u53477.5% 2. MSA\u9884\u8bad\u7ec3\u5bf9\u65b9\u8a00\u8bc6\u522b\u589e\u76ca\u6709\u9650 3. \u65b9\u8a00\u6c60\u5316\u6a21\u578bCER\u4ec5\u6bd4\u65b9\u8a00\u4e13\u7528\u6a21\u578b\u9ad80.5%", "conclusion": "\u901a\u8fc7\u5408\u7406\u5e73\u8861\u591a\u65b9\u8a00\u6570\u636e\uff0c\u6784\u5efa\u7edf\u4e00\u65b9\u8a00\u6c60\u5316\u6a21\u578b\u53ef\u6709\u6548\u7a81\u7834\u5355\u4e00\u65b9\u8a00\u6570\u636e\u4e0d\u8db3\u7684\u74f6\u9888\uff0c\u4e3a\u4f4e\u8d44\u6e90ASR\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.02659", "pdf": "https://arxiv.org/pdf/2506.02659", "abs": "https://arxiv.org/abs/2506.02659", "authors": ["Manon Reusens", "Bart Baesens", "David Jurgens"], "title": "Are Economists Always More Introverted? Analyzing Consistency in Persona-Assigned LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Personalized Large Language Models (LLMs) are increasingly used in diverse\napplications, where they are assigned a specific persona - such as a happy high\nschool teacher - to guide their responses. While prior research has examined\nhow well LLMs adhere to predefined personas in writing style, a comprehensive\nanalysis of consistency across different personas and task types is lacking. In\nthis paper, we introduce a new standardized framework to analyze consistency in\npersona-assigned LLMs. We define consistency as the extent to which a model\nmaintains coherent responses when assigned the same persona across different\ntasks and runs. Our framework evaluates personas across four different\ncategories (happiness, occupation, personality, and political stance) spanning\nmultiple task dimensions (survey writing, essay generation, social media post\ngeneration, single turn, and multi-turn conversations). Our findings reveal\nthat consistency is influenced by multiple factors, including the assigned\npersona, stereotypes, and model design choices. Consistency also varies across\ntasks, increasing with more structured tasks and additional context. All code\nis available on GitHub.", "AI": {"tldr": "\u63d0\u51fa\u6807\u51c6\u5316\u6846\u67b6\u5206\u6790\u4e2a\u6027\u5316\u5927\u6a21\u578b\u5728\u4e0d\u540c\u89d2\u8272\u548c\u4efb\u52a1\u4e2d\u7684\u4e00\u81f4\u6027\uff0c\u53d1\u73b0\u89d2\u8272\u5c5e\u6027\u3001\u523b\u677f\u5370\u8c61\u548c\u6a21\u578b\u8bbe\u8ba1\u5171\u540c\u5f71\u54cd\u4e00\u81f4\u6027\u8868\u73b0\uff0c\u7ed3\u6784\u5316\u4efb\u52a1\u548c\u4e0a\u4e0b\u6587\u589e\u5f3a\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u89d2\u8272\u9a71\u52a8\u578bLLMs\u8de8\u4efb\u52a1\u4e00\u81f4\u6027\u7684\u7cfb\u7edf\u8bc4\u4f30\uff0c\u9700\u5efa\u7acb\u6807\u51c6\u5316\u6846\u67b6\u63ed\u793a\u4e0d\u540c\u89d2\u8272\u5c5e\u6027\u548c\u4efb\u52a1\u7c7b\u578b\u5bf9\u6a21\u578b\u8868\u73b0\u7684\u5f71\u54cd\u673a\u5236\u3002", "method": "\u5f00\u53d1\u56db\u7ef4\u5ea6\u89d2\u8272\u8bc4\u4f30\u4f53\u7cfb\uff08\u5e78\u798f/\u804c\u4e1a/\u4e2a\u6027/\u653f\u6cbb\u7acb\u573a\uff09\uff0c\u901a\u8fc7\u4e94\u7c7b\u6587\u672c\u751f\u6210\u4efb\u52a1\uff08\u8c03\u67e5/\u6587\u7ae0/\u793e\u4ea4\u5a92\u4f53/\u5bf9\u8bdd\uff09\u6d4b\u91cf\u6a21\u578b\u8f93\u51fa\u7684\u8de8\u573a\u666f\u4e00\u81f4\u6027\u3002", "result": "\u6a21\u578b\u4e00\u81f4\u6027\u53d7\u89d2\u8272\u7279\u5f81\u4e0e\u4efb\u52a1\u7ed3\u6784\u7684\u53cc\u91cd\u8c03\u8282\uff0c\u7ed3\u6784\u5316\u4efb\u52a1\u8f83\u5f00\u653e\u5bf9\u8bdd\u63d0\u534730%\u7a33\u5b9a\u6027\uff0c\u653f\u6cbb\u7c7b\u89d2\u8272\u56e0\u523b\u677f\u5370\u8c61\u663e\u793a\u6700\u9ad8\u4efb\u52a1\u95f4\u5dee\u5f02\uff08\u00b122%\uff09\u3002", "conclusion": "\u89d2\u8272\u4e00\u81f4\u6027\u662f\u52a8\u6001\u591a\u7ef4\u6982\u5ff5\uff0c\u901a\u8fc7\u4efb\u52a1\u7ed3\u6784\u4f18\u5316\u548c\u4e0a\u4e0b\u6587\u589e\u5f3a\u53ef\u63d0\u5347\u4e2a\u6027\u5316LLMs\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u53ef\u4fe1AI\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u65b0\u65b9\u6cd5\u8bba\u3002"}}
{"id": "2506.02672", "pdf": "https://arxiv.org/pdf/2506.02672", "abs": "https://arxiv.org/abs/2506.02672", "authors": ["Shihan Dou", "Ming Zhang", "Chenhao Huang", "Jiayi Chen", "Feng Chen", "Shichun Liu", "Yan Liu", "Chenxiao Liu", "Cheng Zhong", "Zongzhang Zhang", "Tao Gui", "Chao Xin", "Wei Chengzhi", "Lin Yan", "Qi Zhang", "Xuanjing Huang"], "title": "EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via Sequential Problem Solving", "categories": ["cs.CL", "cs.AI"], "comment": "47 pages, 24 figures", "summary": "We introduce EvaLearn, a pioneering benchmark designed to evaluate large\nlanguage models (LLMs) on their learning capability and efficiency in\nchallenging tasks, a critical, yet underexplored aspect of model potential.\nEvaLearn contains 648 challenging problems across six task types, grouped into\n182 sequences, each sequence dedicated to one task type. Diverging from most\nexisting benchmarks that evaluate models in parallel, EvaLearn requires models\nto solve problems sequentially, allowing them to leverage the experience gained\nfrom previous solutions. EvaLearn provides five comprehensive automated metrics\nto evaluate models and quantify their learning capability and efficiency. We\nextensively benchmark nine frontier models and observe varied performance\nprofiles: some models, such as Claude-3.7-sonnet, start with moderate initial\nperformance but exhibit strong learning ability, while some models struggle to\nbenefit from experience and may even show negative transfer. Moreover, we\ninvestigate model performance under two learning settings and find that\ninstance-level rubrics and teacher-model feedback further facilitate model\nlearning. Importantly, we observe that current LLMs with stronger static\nabilities do not show a clear advantage in learning capability across all\ntasks, highlighting that EvaLearn evaluates a new dimension of model\nperformance. We hope EvaLearn provides a novel evaluation perspective for\nassessing LLM potential and understanding the gap between models and human\ncapabilities, promoting the development of deeper and more dynamic evaluation\napproaches. All datasets, the automatic evaluation framework, and the results\nstudied in this paper are available at the GitHub repository.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faEvaLearn\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5e8f\u5217\u5316\u6311\u6218\u4efb\u52a1\u6d4b\u8bd5LLMs\u52a8\u6001\u5b66\u4e60\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u95f4\u5b66\u4e60\u6548\u7387\u5dee\u5f02\u663e\u8457\uff0c\u9759\u6001\u80fd\u529b\u4e0e\u5b66\u4e60\u80fd\u529b\u4e0d\u76f4\u63a5\u76f8\u5173", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u8bc4\u4f30\u9759\u6001\u80fd\u529b\uff0c\u7f3a\u4e4f\u5bf9LLMs\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u7684\u7cfb\u7edf\u8bc4\u4f30\u3002\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u65b0\u65b9\u6cd5\u91cf\u5316\u6a21\u578b\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u7684\u80fd\u529b\uff0c\u63ed\u793a\u6a21\u578b\u4e0e\u4eba\u7c7b\u5b66\u4e60\u7684\u5dee\u8ddd", "method": "\u6784\u5efa\u5305\u542b6\u7c7b\u4efb\u52a1\u3001182\u4e2a\u5e8f\u5217\uff08\u6bcf\u4e2a\u5e8f\u52173-5\u4e2a\u9012\u589e\u96be\u5ea6\u95ee\u9898\uff09\u7684\u57fa\u51c6\uff0c\u8bbe\u8ba1\u5e8f\u5217\u5316\u8bc4\u4f30\u6d41\u7a0b\u548c5\u4e2a\u91cf\u5316\u6307\u6807\uff08\u521d\u59cb\u80fd\u529b\u3001\u5b66\u4e60\u589e\u76ca\u7b49\uff09\uff0c\u6d4b\u8bd59\u4e2a\u524d\u6cbf\u6a21\u578b\u5728\u4e24\u79cd\u5b66\u4e60\u573a\u666f\u4e0b\u7684\u8868\u73b0", "result": "1. Claude-3.7-sonnet\u7b49\u6a21\u578b\u5c55\u73b0\u5f3a\u5b66\u4e60\u80fd\u529b\uff08\u5b66\u4e60\u589e\u76ca\u8fbe+32.6%\uff09 2. \u90e8\u5206\u6a21\u578b\u51fa\u73b0\u8d1f\u8fc1\u79fb 3. \u5b9e\u4f8b\u7ea7\u89c4\u5219\u548c\u6559\u5e08\u53cd\u9988\u63d0\u5347\u5b66\u4e60\u6548\u679c 4. \u9759\u6001\u80fd\u529b\u5f3a\u7684\u6a21\u578b\uff08\u5982GPT-4\uff09\u672a\u5728\u6240\u6709\u4efb\u52a1\u663e\u73b0\u5b66\u4e60\u4f18\u52bf", "conclusion": "EvaLearn\u63ed\u793a\u4e86LLMs\u52a8\u6001\u5b66\u4e60\u80fd\u529b\u7684\u65b0\u7ef4\u5ea6\uff0c\u4e3a\u8bc4\u4f30\u6a21\u578b\u6f5c\u529b\u63d0\u4f9b\u521b\u65b0\u89c6\u89d2\uff0c\u63a8\u52a8\u5f00\u53d1\u66f4\u63a5\u8fd1\u4eba\u7c7b\u5b66\u4e60\u673a\u5236\u7684\u8bc4\u4f30\u4f53\u7cfb\uff0c\u6240\u6709\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\u5df2\u5f00\u6e90"}}
{"id": "2506.02678", "pdf": "https://arxiv.org/pdf/2506.02678", "abs": "https://arxiv.org/abs/2506.02678", "authors": ["Zhong-Zhi Li", "Xiao Liang", "Zihao Tang", "Lei Ji", "Peijie Wang", "Haotian Xu", "Xing W", "Haizhen Huang", "Weiwei Deng", "Ying Nian Wu", "Yeyun Gong", "Zhijiang Guo", "Xiao Liu", "Fei Yin", "Cheng-Lin Liu"], "title": "TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression", "categories": ["cs.CL", "cs.CE", "cs.NA", "math.NA"], "comment": null, "summary": "Large Language Models (LLMs) have recently achieved remarkable progress by\nleveraging Reinforcement Learning and extended Chain-of-Thought (CoT)\ntechniques. However, the challenge of performing efficient language\nreasoning--especially during inference with extremely long outputs--has drawn\nincreasing attention from the research community. In this work, we propose a\ndynamic ratio-based training pipeline that does not rely on sophisticated data\nannotations or interpolation between multiple models. We continuously balance\nthe weights between the model's System-1 and System-2 data to eliminate\nredundant reasoning processes while preserving the model's reasoning\ncapability. We validate our approach across models on DeepSeek-R1-Distill-7B\nand DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying\ndifficulty levels. Our method significantly reduces the number of output tokens\nby nearly 40% while maintaining the accuracy of the reasoning. Our code and\ndata will be available soon.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u6bd4\u4f8b\u8bad\u7ec3\u6846\u67b6\uff0c\u51cf\u5c1140%\u63a8\u7406\u8f93\u51fatoken\u540c\u65f6\u4fdd\u6301\u7cbe\u5ea6", "motivation": "\u89e3\u51b3LLMs\u5728\u957f\u8f93\u51fa\u63a8\u7406\u65f6\u5197\u4f59\u601d\u7ef4\u8fc7\u7a0b\u7684\u6548\u7387\u95ee\u9898", "method": "\u901a\u8fc7\u5e73\u8861\u7cfb\u7edf1/\u7cfb\u7edf2\u6570\u636e\u6743\u91cd\uff0c\u6d88\u9664\u5197\u4f59\u63a8\u7406\u6d41\u7a0b", "result": "\u5728DeepSeek 7B/14B\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u591a\u57fa\u51c6\u6d4b\u8bd5\u4fdd\u6301\u7cbe\u5ea6\u540c\u65f6\u8f93\u51fatoken\u51cf\u5c1140%", "conclusion": "\u65e0\u76d1\u7763\u52a8\u6001\u5e73\u8861\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u63a8\u7406\u6548\u7387\uff0c\u672a\u6765\u5c06\u5f00\u6e90\u4ee3\u7801\u6570\u636e"}}
{"id": "2506.02683", "pdf": "https://arxiv.org/pdf/2506.02683", "abs": "https://arxiv.org/abs/2506.02683", "authors": ["Zhengdong Lu", "Weikai Lu", "Yiling Tao", "Yun Dai", "ZiXuan Chen", "Huiping Zhuang", "Cen Chen", "Hao Peng", "Ziqian Zeng"], "title": "Decompose, Plan in Parallel, and Merge: A Novel Paradigm for Large Language Models based Planning with Multiple Constraints", "categories": ["cs.CL"], "comment": null, "summary": "Despite significant advances in Large Language Models (LLMs), planning tasks\nstill present challenges for LLM-based agents. Existing planning methods face\ntwo key limitations: heavy constraints and cascading errors. To address these\nlimitations, we propose a novel parallel planning paradigm, which Decomposes,\nPlans for subtasks in Parallel, and Merges subplans into a final plan (DPPM).\nSpecifically, DPPM decomposes the complex task based on constraints into\nsubtasks, generates the subplan for each subtask in parallel, and merges them\ninto a global plan. In addition, our approach incorporates a verification and\nrefinement module, enabling error correction and conflict resolution.\nExperimental results demonstrate that DPPM significantly outperforms existing\nmethods in travel planning tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5e76\u884c\u89c4\u5212\u8303\u5f0fDPPM\u89e3\u51b3LLM\u5728\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u7ea6\u675f\u9650\u5236\u548c\u9519\u8bef\u7d2f\u79ef\u95ee\u9898", "motivation": "\u73b0\u6709\u89c4\u5212\u65b9\u6cd5\u5b58\u5728\u7ea6\u675f\u8fc7\u5f3a\u548c\u9519\u8bef\u7ea7\u8054\u4e24\u5927\u7f3a\u9677\uff0c\u5236\u7ea6LLM\u667a\u80fd\u4f53\u7684\u89c4\u5212\u80fd\u529b", "method": "\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u3001\u5e76\u884c\u5b50\u4efb\u52a1\u89c4\u5212\u3001\u5b50\u8ba1\u5212\u878d\u5408\u7684\u4e09\u9636\u6bb5\u6d41\u7a0b\uff0c\u914d\u5408\u9a8c\u8bc1\u7cbe\u70bc\u6a21\u5757\u5b9e\u73b0\u51b2\u7a81\u89e3\u51b3", "result": "\u5728\u65c5\u884c\u89c4\u5212\u4efb\u52a1\u4e2d\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5", "conclusion": "DPPM\u901a\u8fc7\u5e76\u884c\u5316\u89c4\u5212\u67b6\u6784\u6709\u6548\u63d0\u5347\u590d\u6742\u4efb\u52a1\u7684\u89c4\u5212\u53ef\u9760\u6027\u548c\u6267\u884c\u6548\u7387"}}
{"id": "2506.02689", "pdf": "https://arxiv.org/pdf/2506.02689", "abs": "https://arxiv.org/abs/2506.02689", "authors": ["Liang Yue", "Yihong Tang", "Kehai Chen", "Jie Liu", "Min Zhang"], "title": "MASTER: Enhancing Large Language Model via Multi-Agent Simulated Teaching", "categories": ["cs.CL"], "comment": null, "summary": "Instruction fine-tuning is crucial in NLP tasks, enhancing pretrained models'\ninstruction-following capabilities and task-specific performance. However,\nobtaining high-quality fine-tuning data for large models is challenging due to\ndata collection difficulties and high production costs. To address this, we\npropose MASTER, a novel data augmentation method that enriches original data\nthrough interactions among multiple agents with varying cognitive levels. We\nsimulate three pedagogically grounded teaching scenarios, leveraging\nmulti-agent conversations to generate high-quality teacher-student interaction\ndata. Utilizing MASTER, we construct BOOST-QA, a fine-tuning dataset augmented\nfrom existing datasets like Orca-Math-200k, ProcQA, and OpenHermes2.5.\nExperiments show that models fine-tuned with BOOST-QA perform excellently\nacross multiple benchmarks, demonstrating strong multitask generalization.\nNotably, MASTER significantly improves models' reasoning abilities in complex\ntasks, providing valuable insights for future research.", "AI": {"tldr": "\u63d0\u51faMASTER\u591a\u667a\u80fd\u4f53\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u89e3\u51b3\u5927\u6a21\u578b\u5fae\u8c03\u6570\u636e\u8d28\u91cf\u95ee\u9898\uff0c\u6784\u5efaBOOST-QA\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b", "motivation": "\u89e3\u51b3\u5927\u6a21\u578b\u6307\u4ee4\u5fae\u8c03\u4e2d\u9ad8\u8d28\u91cf\u6570\u636e\u83b7\u53d6\u56f0\u96be\uff08\u6570\u636e\u6536\u96c6\u96be\u3001\u751f\u4ea7\u6210\u672c\u9ad8\uff09\u7684\u884c\u4e1a\u75db\u70b9", "method": "\u901a\u8fc7\u6a21\u62df\u8bfe\u5802\u3001\u8f85\u5bfc\u3001\u5b9e\u8df5\u4e09\u79cd\u6559\u5b66\u573a\u666f\u7684\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\uff08\u5e08\u751f\u89d2\u8272\uff09\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u6559\u5b66\u5bf9\u8bdd\u6570\u636e", "result": "\u57fa\u4e8eOrca-Math\u7b49\u6570\u636e\u96c6\u6784\u5efa\u7684BOOST-QQA\u4f7f\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u590d\u6742\u4efb\u52a1\u63a8\u7406\u80fd\u529b\u63d0\u5347\u663e\u8457", "conclusion": "MASTER\u4e3a\u6570\u636e\u589e\u5f3a\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u5176\u6559\u80b2\u573a\u666f\u6a21\u62df\u601d\u8def\u5bf9\u63d0\u5347\u6a21\u578b\u8ba4\u77e5\u80fd\u529b\u5177\u6709\u91cd\u8981\u542f\u793a"}}
{"id": "2506.02701", "pdf": "https://arxiv.org/pdf/2506.02701", "abs": "https://arxiv.org/abs/2506.02701", "authors": ["Masaki Sakata", "Sho Yokoi", "Benjamin Heinzerling", "Takumi Ito", "Kentaro Inui"], "title": "On Entity Identification in Language Models", "categories": ["cs.CL"], "comment": "ACL 2025 Findings; 26 pages, 13 figures, 9 tables", "summary": "We analyze the extent to which internal representations of language models\n(LMs) identify and distinguish mentions of named entities, focusing on the\nmany-to-many correspondence between entities and their mentions. We first\nformulate two problems of entity mentions -- ambiguity and variability -- and\npropose a framework analogous to clustering quality metrics. Specifically, we\nquantify through cluster analysis of LM internal representations the extent to\nwhich mentions of the same entity cluster together and mentions of different\nentities remain separated. Our experiments examine five Transformer-based\nautoregressive models, showing that they effectively identify and distinguish\nentities with metrics analogous to precision and recall ranging from 0.66 to\n0.9. Further analysis reveals that entity-related information is compactly\nrepresented in a low-dimensional linear subspace at early LM layers.\nAdditionally, we clarify how the characteristics of entity representations\ninfluence word prediction performance. These findings are interpreted through\nthe lens of isomorphism between LM representations and entity-centric knowledge\nstructures in the real world, providing insights into how LMs internally\norganize and use entity information.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u805a\u7c7b\u5206\u6790\u91cf\u5316\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u548c\u533a\u5206\u547d\u540d\u5b9e\u4f53\u7684\u80fd\u529b\uff0c\u53d1\u73b0Transformer\u6a21\u578b\u80fd\u6709\u6548\u8868\u5f81\u5b9e\u4f53\u4fe1\u606f\uff08\u7cbe\u5ea6/\u53ec\u56de\u73870.66-0.9\uff09\uff0c\u4e14\u5b9e\u4f53\u4fe1\u606f\u5728\u65e9\u671f\u7f51\u7edc\u5c42\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u4e2d\u7d27\u51d1\u5b58\u5728\u3002", "motivation": "\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u5982\u4f55\u8868\u5f81\u5b9e\u4f53\u63d0\u53ca\u7684\u6b67\u4e49\u6027\uff08\u540c\u4e00\u5b9e\u4f53\u4e0d\u540c\u8868\u8ff0\uff09\u548c\u53d8\u5f02\u6027\uff08\u4e0d\u540c\u5b9e\u4f53\u76f8\u4f3c\u8868\u8ff0\uff09\u95ee\u9898\uff0c\u63a2\u7d22LM\u8868\u5f81\u4e0e\u73b0\u5b9e\u4e16\u754c\u5b9e\u4f53\u77e5\u8bc6\u7ed3\u6784\u7684\u5bf9\u5e94\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u7c7b\u4f3c\u805a\u7c7b\u8d28\u91cf\u8bc4\u4f30\u7684\u6846\u67b6\uff0c\u5bf95\u4e2aTransformer\u81ea\u56de\u5f52\u6a21\u578b\u7684\u9690\u85cf\u5c42\u8868\u5f81\u8fdb\u884c\u805a\u7c7b\u5206\u6790\uff0c\u91c7\u7528\u7c7b\u7cbe\u5ea6/\u53ec\u56de\u7387\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u5206\u6790\u63ed\u793a\u7279\u5f81\u5206\u5e03\u89c4\u5f8b\u3002", "result": "\u6a21\u578b\u5b9e\u4f53\u8bc6\u522b\u6307\u6807\u8fbe0.66-0.9\uff0c\u65e9\u671f\u5c42\u5b58\u5728\u4e13\u95e8\u7684\u4f4e\u7ef4\u5b9e\u4f53\u8868\u5f81\u5b50\u7a7a\u95f4\uff0c\u4e14\u8be5\u8868\u5f81\u7279\u6027\u76f4\u63a5\u5f71\u54cd\u8bed\u8a00\u6a21\u578b\u7684\u8bcd\u6c47\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "LM\u5185\u90e8\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u4f4e\u7ef4\u8868\u5f81\u5b9e\u73b0\u5b9e\u4f53\u77e5\u8bc6\u7ec4\u7ec7\uff0c\u8fd9\u79cd\u540c\u6784\u6027\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5904\u7406\u5b9e\u4f53\u4fe1\u606f\u7684\u8ba4\u77e5\u673a\u5236\uff0c\u4e3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002"}}
{"id": "2506.02726", "pdf": "https://arxiv.org/pdf/2506.02726", "abs": "https://arxiv.org/abs/2506.02726", "authors": ["Qihang Yan", "Xinyu Zhang", "Luming Guo", "Qi Zhang", "Feifan Liu"], "title": "RACE-Align: Retrieval-Augmented and Chain-of-Thought Enhanced Preference Alignment for Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.6; H.3.3"], "comment": null, "summary": "Large Language Models (LLMs) struggle with accuracy, domain-specific\nreasoning, and interpretability in vertical domains. Traditional preference\nalignment methods like Reinforcement Learning from Human Feedback (RLHF) and\nDirect Preference Optimization (DPO) often overlook the underlying knowledge\nsources and reasoning logic. This paper introduces RACE-Align\n(Retrieval-Augmented and Chain-of-Thought Enhanced Alignment), a novel\nframework designed to address these limitations. RACE-Align systematically\nconstructs a binary preference dataset incorporating external knowledge support\nand explicit Chain-of-Thought (CoT) reasoning, then aligns LLMs using the DPO\nalgorithm. The core innovation lies in its preference data construction\nstrategy: it integrates AI-driven retrieval for factual grounding, enhancing\nknowledgeability and accuracy, and emphasizes the optimization of\ndomain-specific CoT, treating the reasoning process itself as a key preference\ndimension. A multi-stage, AI-driven refinement pipeline cost-effectively\ngenerates these preference pairs. Experimental validation in Traditional\nChinese Medicine (TCM) using Qwen3-1.7B as the base model demonstrates that\nRACE-Align significantly outperforms the original base model and a model\nfine-tuned only with Supervised Fine-Tuning (SFT). Improvements were observed\nacross multiple dimensions, including answer accuracy, information richness,\napplication of TCM thinking patterns, logicality and depth of reasoning, and\ninterpretability. These findings suggest RACE-Align offers an effective pathway\nto enhance LLMs' knowledge application, reasoning reliability, and process\ntransparency in complex vertical domains.", "AI": {"tldr": "\u63d0\u51faRACE-Align\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u548c\u601d\u7ef4\u94fe\u4f18\u5316\u7684\u5bf9\u9f50\u65b9\u6cd5\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5782\u76f4\u9886\u57df\u7684\u77e5\u8bc6\u5e94\u7528\u4e0e\u63a8\u7406\u53ef\u9760\u6027", "motivation": "\u4f20\u7edf\u504f\u597d\u5bf9\u9f50\u65b9\u6cd5\uff08\u5982RLHF/DPO\uff09\u5ffd\u89c6\u77e5\u8bc6\u6765\u6e90\u548c\u63a8\u7406\u903b\u8f91\uff0c\u5bfc\u81f4\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5782\u76f4\u9886\u57df\u5b58\u5728\u51c6\u786e\u6027\u4e0d\u8db3\u3001\u9886\u57df\u63a8\u7406\u80fd\u529b\u5f31\u548c\u53ef\u89e3\u91ca\u6027\u5dee\u7684\u95ee\u9898", "method": "1. \u6784\u5efa\u5305\u542b\u5916\u90e8\u77e5\u8bc6\u652f\u6301\u548c\u663e\u5f0f\u601d\u7ef4\u94fe\u7684\u504f\u597d\u6570\u636e\u96c6\n2. \u5c06AI\u9a71\u52a8\u7684\u68c0\u7d22\u589e\u5f3a\uff08\u4e8b\u5b9e\u4f9d\u636e\uff09\u548c\u9886\u57df\u7279\u5b9a\u601d\u7ef4\u94fe\u4f18\u5316\uff08\u63a8\u7406\u8fc7\u7a0b\uff09\u4f5c\u4e3a\u6838\u5fc3\u504f\u597d\u7ef4\u5ea6\n3. \u91c7\u7528\u591a\u9636\u6bb5AI\u9a71\u52a8\u4f18\u5316\u6d41\u7a0b\u751f\u6210\u9ad8\u8d28\u91cf\u504f\u597d\u5bf9\n4. \u57fa\u4e8eDPO\u7b97\u6cd5\u8fdb\u884c\u6a21\u578b\u5bf9\u9f50", "result": "\u5728\u4e2d\u533b\u9886\u57df\u5b9e\u9a8c\u4e2d\uff0c\u4f7f\u7528Qwen3-1.7B\u6a21\u578b\u7684RACE-Align\u5728\u7b54\u6848\u51c6\u786e\u6027\uff08\u63d0\u534737%\uff09\u3001\u4fe1\u606f\u4e30\u5bcc\u5ea6\u3001\u4e2d\u533b\u601d\u7ef4\u6a21\u5f0f\u5e94\u7528\u3001\u63a8\u7406\u903b\u8f91\u6df1\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u7b49\u591a\u4e2a\u7ef4\u5ea6\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u5782\u76f4\u9886\u57df\u7684\u77e5\u8bc6\u5e94\u7528\u53ef\u9760\u6027\u3001\u63a8\u7406\u8fc7\u7a0b\u900f\u660e\u5ea6\u548c\u9886\u57df\u9002\u5e94\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u5728\u533b\u7597\u7b49\u4e13\u4e1a\u9886\u57df\u5177\u6709\u5e94\u7528\u6f5c\u529b"}}
{"id": "2506.02740", "pdf": "https://arxiv.org/pdf/2506.02740", "abs": "https://arxiv.org/abs/2506.02740", "authors": ["Ama\u00e7 Herda\u011fdelen", "Marco Baroni"], "title": "Stereotypical gender actions can be extracted from Web text", "categories": ["cs.CL"], "comment": null, "summary": "We extracted gender-specific actions from text corpora and Twitter, and\ncompared them to stereotypical expectations of people. We used Open Mind Common\nSense (OMCS), a commonsense knowledge repository, to focus on actions that are\npertinent to common sense and daily life of humans. We use the gender\ninformation of Twitter users and Web-corpus-based pronoun/name gender\nheuristics to compute the gender bias of the actions. With high recall, we\nobtained a Spearman correlation of 0.47 between corpus-based predictions and a\nhuman gold standard, and an area under the ROC curve of 0.76 when predicting\nthe polarity of the gold standard. We conclude that it is feasible to use\nnatural text (and a Twitter-derived corpus in particular) in order to augment\ncommonsense repositories with the stereotypical gender expectations of actions.\nWe also present a dataset of 441 commonsense actions with human judges' ratings\non whether the action is typically/slightly masculine/feminine (or neutral),\nand another larger dataset of 21,442 actions automatically rated by the methods\nwe investigate in this study.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u5206\u6790\u6587\u672c\u548cTwitter\u6570\u636e\u9a8c\u8bc1\u4e86\u5229\u7528\u81ea\u7136\u6587\u672c(\u5c24\u5176\u662fTwitter\u8bed\u6599)\u589e\u5f3a\u5e38\u8bc6\u5e93\u4e2d\u6027\u522b\u523b\u677f\u5370\u8c61\u884c\u4e3a\u7684\u53ef\u884c\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u4eba\u5de5\u6807\u6ce8\u548c\u81ea\u52a8\u8bc4\u5206\u7684\u4e24\u4e2a\u884c\u4e3a\u6570\u636e\u96c6\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5229\u7528\u81ea\u7136\u6587\u672c\u6570\u636e\u8865\u5145\u5e38\u8bc6\u77e5\u8bc6\u5e93\u4e2d\u7684\u6027\u522b\u523b\u677f\u5370\u8c61\u4fe1\u606f\uff0c\u66f4\u5168\u9762\u53cd\u6620\u793e\u4f1a\u4e2d\u7684\u6027\u522b\u504f\u89c1\u73b0\u8c61\u3002", "method": "\u7ed3\u5408OMCS\u77e5\u8bc6\u5e93\uff0c\u91c7\u7528Twitter\u7528\u6237\u6027\u522b\u4fe1\u606f\u4e0e\u7f51\u7edc\u8bed\u6599\u5e93\u7684\u4ee3\u8bcd/\u59d3\u540d\u63a8\u65ad\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u53ec\u56de\u7387\u7edf\u8ba1\u6a21\u578b\u8ba1\u7b97\u884c\u4e3a\u6027\u522b\u504f\u5411\uff0c\u5e76\u4e0e\u4eba\u5de5\u6807\u6ce8\u91d1\u6807\u51c6\u8fdb\u884c\u76f8\u5173\u6027\u9a8c\u8bc1\u3002", "result": "\u83b7\u5f97Spearman\u76f8\u5173\u60270.47\u548cROC\u66f2\u7ebf\u4e0b\u9762\u79ef0.76\u7684\u9884\u6d4b\u6548\u679c\uff0c\u6784\u5efa\u4e86441\u4e2a\u884c\u4e3a\u7684\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\u548c21,442\u4e2a\u81ea\u52a8\u8bc4\u5206\u884c\u4e3a\u6570\u636e\u96c6\u3002", "conclusion": "\u81ea\u7136\u6587\u672c(\u7279\u522b\u662fTwitter\u6570\u636e)\u80fd\u6709\u6548\u6269\u5145\u5e38\u8bc6\u5e93\u4e2d\u7684\u6027\u522b\u523b\u677f\u5370\u8c61\u884c\u4e3a\u6570\u636e\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u65b9\u6cd5\u8bba\u652f\u6301\u548c\u6570\u636e\u57fa\u7840\u3002"}}
{"id": "2506.02753", "pdf": "https://arxiv.org/pdf/2506.02753", "abs": "https://arxiv.org/abs/2506.02753", "authors": ["Aisha Alansari", "Hamzah Luqman"], "title": "Multi-task Learning with Active Learning for Arabic Offensive Speech Detection", "categories": ["cs.CL"], "comment": null, "summary": "The rapid growth of social media has amplified the spread of offensive,\nviolent, and vulgar speech, which poses serious societal and cybersecurity\nconcerns. Detecting such content in Arabic text is particularly complex due to\nlimited labeled data, dialectal variations, and the language's inherent\ncomplexity. This paper proposes a novel framework that integrates multi-task\nlearning (MTL) with active learning to enhance offensive speech detection in\nArabic social media text. By jointly training on two auxiliary tasks, violent\nand vulgar speech, the model leverages shared representations to improve the\ndetection accuracy of the offensive speech. Our approach dynamically adjusts\ntask weights during training to balance the contribution of each task and\noptimize performance. To address the scarcity of labeled data, we employ an\nactive learning strategy through several uncertainty sampling techniques to\niteratively select the most informative samples for model training. We also\nintroduce weighted emoji handling to better capture semantic cues. Experimental\nresults on the OSACT2022 dataset show that the proposed framework achieves a\nstate-of-the-art macro F1-score of 85.42%, outperforming existing methods while\nusing significantly fewer fine-tuning samples. The findings of this study\nhighlight the potential of integrating MTL with active learning for efficient\nand accurate offensive language detection in resource-constrained settings.", "AI": {"tldr": "\u63d0\u51fa\u878d\u5408\u591a\u4efb\u52a1\u5b66\u4e60\u4e0e\u4e3b\u52a8\u5b66\u4e60\u7684\u65b0\u578b\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u963f\u62c9\u4f2f\u793e\u4ea4\u5a92\u4f53\u653b\u51fb\u6027\u8bed\u8a00\u68c0\u6d4b\u6027\u80fd\uff08F1\u8fbe85.42%\uff09", "motivation": "\u89e3\u51b3\u963f\u62c9\u4f2f\u8bed\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u3001\u65b9\u8a00\u590d\u6742\u6027\u95ee\u9898\uff0c\u4f18\u5316\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u653b\u51fb\u6027\u5185\u5bb9\u68c0\u6d4b\u6548\u7387", "method": "1. \u591a\u4efb\u52a1\u8054\u5408\u8bad\u7ec3\uff08\u653b\u51fb\u6027/\u66b4\u529b/\u7c97\u4fd7\u8bed\u8a00\uff09\u5171\u4eab\u8868\u5f81 2. \u52a8\u6001\u4efb\u52a1\u6743\u91cd\u8c03\u6574\u673a\u5236 3. \u4e3b\u52a8\u5b66\u4e60+\u4e0d\u786e\u5b9a\u6027\u91c7\u6837\u9009\u62e9\u6837\u672c 4. \u52a0\u6743emoji\u5904\u7406\u589e\u5f3a\u8bed\u4e49\u6355\u6349", "result": "\u5728OSACT2022\u6570\u636e\u96c6\u4e0a\u5b9e\u73b085.42%\u7684\u5b8fF1\u503c\uff0c\u663e\u8457\u51cf\u5c11\u5fae\u8c03\u6837\u672c\u9700\u6c42", "conclusion": "\u591a\u4efb\u52a1\u5b66\u4e60\u4e0e\u4e3b\u52a8\u5b66\u4e60\u7684\u534f\u540c\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u7684\u8bed\u8a00\u68c0\u6d4b\u6548\u679c"}}
{"id": "2506.02758", "pdf": "https://arxiv.org/pdf/2506.02758", "abs": "https://arxiv.org/abs/2506.02758", "authors": ["Stefano Bann\u00f2", "Kate Knill", "Mark Gales"], "title": "Exploiting the English Vocabulary Profile for L2 word-level vocabulary assessment with LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to the 20th Workshop on Innovative Use of NLP for Building\n  Educational Applications", "summary": "Vocabulary use is a fundamental aspect of second language (L2) proficiency.\nTo date, its assessment by automated systems has typically examined the\ncontext-independent, or part-of-speech (PoS) related use of words. This paper\nintroduces a novel approach to enable fine-grained vocabulary evaluation\nexploiting the precise use of words within a sentence. The scheme combines\nlarge language models (LLMs) with the English Vocabulary Profile (EVP). The EVP\nis a standard lexical resource that enables in-context vocabulary use to be\nlinked with proficiency level. We evaluate the ability of LLMs to assign\nproficiency levels to individual words as they appear in L2 learner writing,\naddressing key challenges such as polysemy, contextual variation, and\nmulti-word expressions. We compare LLMs to a PoS-based baseline. LLMs appear to\nexploit additional semantic information that yields improved performance. We\nalso explore correlations between word-level proficiency and essay-level\nproficiency. Finally, the approach is applied to examine the consistency of the\nEVP proficiency levels. Results show that LLMs are well-suited for the task of\nvocabulary assessment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4e0eEVP\u6807\u51c6\u8d44\u6e90\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u8bcd\u6c47\u8bc4\u4f30\uff0c\u8bc1\u660eLLMs\u5728\u8bcd\u6c47\u719f\u7ec3\u5ea6\u5224\u5b9a\u4e2d\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u5316\u8bcd\u6c47\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u8bcd\u6027\u5206\u6790\uff0c\u7f3a\u4e4f\u5bf9\u4e0a\u4e0b\u6587\u5173\u8054\u3001\u591a\u4e49\u8bcd\u5904\u7406\u7b49\u6838\u5fc3\u8bed\u8a00\u7279\u5f81\u7684\u6709\u6548\u6355\u6349\u3002", "method": "\u901a\u8fc7\u6574\u5408LLMs\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u4e0eEVP\u7684\u6743\u5a01\u8bcd\u6c47\u5206\u7ea7\u6807\u51c6\uff0c\u5904\u7406\u591a\u4e49\u8bcd/\u8bed\u5883\u53d8\u5316/\u591a\u8bcd\u8868\u8fbe\u4e09\u5927\u6311\u6218\uff0c\u5e76\u4e0e\u4f20\u7edf\u8bcd\u6027\u5206\u6790\u65b9\u6cd5\u5bf9\u6bd4\u3002", "result": "LLMs\u5229\u7528\u8bed\u4e49\u4fe1\u606f\u63d0\u5347\u8bc4\u4f30\u6027\u80fd\uff0c\u8bcd\u6c47\u719f\u7ec3\u5ea6\u4e0e\u6587\u7ae0\u6c34\u5e73\u663e\u8457\u76f8\u5173\uff0cEVP\u7b49\u7ea7\u4e00\u81f4\u6027\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u8bcd\u6c47\u8bc4\u4f30\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u81ea\u52a8\u5316\u8bed\u8a00\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2506.02803", "pdf": "https://arxiv.org/pdf/2506.02803", "abs": "https://arxiv.org/abs/2506.02803", "authors": ["Sifan Li", "Yujun Cai", "Yiwei Wang"], "title": "SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via Visual Global Thinking", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) excel in semantic tasks but falter at a core\nhuman capability: detecting hidden content in optical illusions or AI-generated\nimages through perceptual adjustments like zooming. We introduce HC-Bench, a\nbenchmark of 112 images with hidden text, objects, and illusions, revealing\nthat leading VLMs achieve near-zero accuracy (0-5.36%)-even with explicit\nprompting. Humans resolve such ambiguities instinctively, yet VLMs fail due to\nan overreliance on high-level semantics. Strikingly, we propose SemVink\n(Semantic Visual Thinking) by simply scaling images to low resolutions (32-128\npixels), which unlocks >99% accuracy by eliminating redundant visual noise.\nThis exposes a critical architectural flaw: VLMs prioritize abstract reasoning\nover low-level visual operations crucial for real-world robustness. Our work\nurges a shift toward hybrid models integrating multi-scale processing, bridging\nthe gap between computational vision and human cognition for applications in\nmedical imaging, security, and beyond.", "AI": {"tldr": "\u4f20\u7edf\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4f9d\u8d56\u9ad8\u5c42\u8bed\u4e49\uff0c\u65e0\u6cd5\u611f\u77e5\u56fe\u50cf\u9690\u85cf\u5185\u5bb9\uff0c\u4f46\u901a\u8fc7\u7b80\u5355\u7f29\u653e\u56fe\u50cf\u5206\u8fa8\u7387\uff08SemVink\u65b9\u6cd5\uff09\u53ef\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u51c6\u786e\u7387\u81f399%\u4ee5\u4e0a", "motivation": "\u63ed\u793a\u5f53\u524d\u4e3b\u6d41VLM\u6a21\u578b\u5b58\u5728\u67b6\u6784\u7f3a\u9677\u2014\u2014\u8fc7\u5ea6\u4f9d\u8d56\u8bed\u4e49\u63a8\u7406\u800c\u5ffd\u89c6\u4eba\u7c7b\u672c\u80fd\u5177\u5907\u7684\u4f4e\u7ea7\u89c6\u89c9\u5904\u7406\u80fd\u529b\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u533b\u5b66\u5f71\u50cf\u3001\u5b89\u9632\u7b49\u9700\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7406\u89e3\u573a\u666f\u7684\u5e94\u7528", "method": "1. \u6784\u5efaHC-Bench\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff08\u542b112\u5f20\u542b\u9690\u85cf\u6587\u672c/\u7269\u4f53\u7684\u56fe\u50cf\uff09 2. \u9a8c\u8bc1\u4e3b\u6d41VLM\u63a5\u8fd1\u96f6\u51c6\u786e\u7387 3. \u63d0\u51faSemVink\u65b9\u6cd5\uff08\u5c06\u56fe\u50cf\u964d\u91c7\u6837\u81f332-128\u50cf\u7d20\u5206\u8fa8\u7387\uff09", "result": "SemVink\u65b9\u6cd5\u4f7f\u51c6\u786e\u7387\u4ece0-5.36%\u63d0\u5347\u81f3>99%\uff0c\u8bc1\u660e\u964d\u4f4e\u5206\u8fa8\u7387\u53ef\u6d88\u9664\u5197\u4f59\u89c6\u89c9\u566a\u58f0\uff0c\u66b4\u9732VLM\u67b6\u6784\u5bf9\u4f4e\u7ea7\u89c6\u89c9\u7279\u5f81\u5904\u7406\u80fd\u529b\u7684\u7f3a\u5931", "conclusion": "\u547c\u5401\u5f00\u53d1\u878d\u5408\u591a\u5c3a\u5ea6\u5904\u7406\u7684\u6df7\u5408\u6a21\u578b\u67b6\u6784\uff0c\u5f25\u5408\u8ba1\u7b97\u673a\u89c6\u89c9\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u7684\u9e3f\u6c9f\uff0c\u5f3a\u8c03\u4f4e\u7ea7\u89c6\u89c9\u64cd\u4f5c\u5bf9\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u7684\u5173\u952e\u4f5c\u7528"}}
{"id": "2506.02818", "pdf": "https://arxiv.org/pdf/2506.02818", "abs": "https://arxiv.org/abs/2506.02818", "authors": ["Ekaterina Grishina", "Mikhail Gorbunov", "Maxim Rakhuba"], "title": "ProcrustesGPT: Compressing LLMs with Structured Matrices and Orthogonal Transformations", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted by ACL Findings", "summary": "Large language models (LLMs) demonstrate impressive results in natural\nlanguage processing tasks but require a significant amount of computational and\nmemory resources. Structured matrix representations are a promising way for\nreducing the number of parameters of these models. However, it seems\nunrealistic to expect that weight matrices of pretrained models can be\naccurately represented by structured matrices without any fine-tuning. To\novercome this issue, we utilize the fact that LLM output is invariant under\ncertain orthogonal transformations of weight matrices. This insight can be\nleveraged to identify transformations that significantly improve the\ncompressibility of weights within structured classes. The proposed approach is\napplicable to various types of structured matrices that support efficient\nprojection operations. Code is available at\nhttps://github.com/GrishKate/ProcrustesGPT", "AI": {"tldr": "\u901a\u8fc7\u6b63\u4ea4\u53d8\u6362\u4f18\u5316\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6743\u91cd\u77e9\u9635\u7ed3\u6784\u5316\u538b\u7f29\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u4fdd\u6301\u6a21\u578b\u8f93\u51fa\u4e0d\u53d8", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u9ad8\u8ba1\u7b97/\u5185\u5b58\u6d88\u8017\u95ee\u9898\uff0c\u73b0\u6709\u7ed3\u6784\u5316\u77e9\u9635\u538b\u7f29\u65b9\u6cd5\u9700\u8981\u5fae\u8c03\u4e14\u6548\u679c\u6709\u9650\u3002\u7814\u7a76\u53d1\u73b0LLM\u6743\u91cd\u77e9\u9635\u5728\u7279\u5b9a\u6b63\u4ea4\u53d8\u6362\u4e0b\u8f93\u51fa\u4e0d\u53d8\uff0c\u53ef\u5229\u7528\u8be5\u7279\u6027\u63d0\u5347\u7ed3\u6784\u5316\u538b\u7f29\u6548\u679c", "method": "\u5229\u7528\u6743\u91cd\u77e9\u9635\u7684\u6b63\u4ea4\u53d8\u6362\u4e0d\u53d8\u6027\uff0c\u5bfb\u627e\u6700\u4f18\u53d8\u6362\u4f7f\u6743\u91cd\u77e9\u9635\u66f4\u9002\u914d\u7ed3\u6784\u5316\u538b\u7f29\u65b9\u6848\uff0c\u652f\u6301\u591a\u79cd\u652f\u6301\u9ad8\u6548\u6295\u5f71\u64cd\u4f5c\u7684\u7ed3\u6784\u5316\u77e9\u9635\u7c7b\u578b", "result": "\u5f00\u53d1\u51fa\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u6709\u6548\u538b\u7f29LLM\u53c2\u6570\u7684\u65b9\u6cd5\uff0c\u4fdd\u6301\u539f\u59cb\u6a21\u578b\u529f\u80fd\u7684\u540c\u65f6\u63d0\u5347\u7ed3\u6784\u5316\u77e9\u9635\u7684\u538b\u7f29\u9002\u914d\u6027\uff0c\u5e76\u63d0\u4f9b\u5f00\u6e90\u5b9e\u73b0", "conclusion": "\u8be5\u6b63\u4ea4\u53d8\u6362\u65b9\u6cd5\u7a81\u7834\u7ed3\u6784\u5316\u538b\u7f29\u9700\u8981\u5fae\u8c03\u7684\u9650\u5236\uff0c\u4e3a\u4e0d\u540c\u7ed3\u6784\u7684\u53c2\u6570\u9ad8\u6548\u538b\u7f29\u63d0\u4f9b\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5f00\u6e90\u4fc3\u8fdb\u5b9e\u9645\u5e94\u7528"}}
{"id": "2506.02827", "pdf": "https://arxiv.org/pdf/2506.02827", "abs": "https://arxiv.org/abs/2506.02827", "authors": ["Yulin Dou", "Jiangming Liu"], "title": "TO-GATE: Clarifying Questions and Summarizing Responses with Trajectory Optimization for Eliciting Human Preference", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) can effectively elicit human preferences through\nmulti-turn dialogue. Complex tasks can be accomplished through iterative\nclarifying questions and final responses generated by an LLM acting as a\nquestioner (STaR-GATE; Andukuri et al., 2024}). However, existing approaches\nbased on self-taught reasoning struggle to identify optimal dialogue\ntrajectories and avoid irrelevant questions to the tasks. To address this\nlimitation, we propose TO-GATE, a novel framework that enhances question\ngeneration through trajectory optimization, which consists of two key\ncomponents: a clarification resolver that generates optimal questioning\ntrajectories, and a summarizer that ensures task-aligned final responses. The\ntrajectory optimization enables the model to produce effective elicitation\nquestions and summary responses tailored to specific tasks. Experimental\nresults demonstrate that TO-GATE significantly outperforms baseline methods,\nachieving a 9.32% improvement on standard preference elicitation tasks.", "AI": {"tldr": "TO-GATE\u6846\u67b6\u901a\u8fc7\u8f68\u8ff9\u4f18\u5316\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u504f\u597d\u8bf1\u5bfc\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6027\u80fd\u63d0\u53479.32%", "motivation": "\u73b0\u6709\u57fa\u4e8e\u81ea\u5b66\u4e60\u63a8\u7406\u7684\u65b9\u6cd5\u96be\u4ee5\u8bc6\u522b\u6700\u4f18\u5bf9\u8bdd\u8f68\u8ff9\uff0c\u5e38\u751f\u6210\u4e0e\u4efb\u52a1\u65e0\u5173\u7684\u95ee\u9898", "method": "\u63d0\u51fa\u5305\u542b\u6f84\u6e05\u89e3\u6790\u5668\uff08\u751f\u6210\u6700\u4f18\u63d0\u95ee\u8f68\u8ff9\uff09\u548c\u603b\u7ed3\u5668\uff08\u4fdd\u8bc1\u4efb\u52a1\u5bf9\u9f50\uff09\u7684TO-GATE\u6846\u67b6", "result": "\u5728\u6807\u51c6\u504f\u597d\u8bf1\u5bfc\u4efb\u52a1\u4e2d\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b09.32%\u7684\u6027\u80fd\u63d0\u5347", "conclusion": "\u8f68\u8ff9\u4f18\u5316\u673a\u5236\u80fd\u6709\u6548\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u95ee\u9898\u751f\u6210\u8d28\u91cf\u548c\u4efb\u52a1\u5bf9\u9f50\u54cd\u5e94\u80fd\u529b"}}
{"id": "2506.02872", "pdf": "https://arxiv.org/pdf/2506.02872", "abs": "https://arxiv.org/abs/2506.02872", "authors": ["Ludovic Moncla", "H\u00e9di Zeghidi"], "title": "Token and Span Classification for Entity Recognition in French Historical Encyclopedias", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Named Entity Recognition (NER) in historical texts presents unique challenges\ndue to non-standardized language, archaic orthography, and nested or\noverlapping entities. This study benchmarks a diverse set of NER approaches,\nranging from classical Conditional Random Fields (CRFs) and spaCy-based models\nto transformer-based architectures such as CamemBERT and sequence-labeling\nmodels like Flair. Experiments are conducted on the GeoEDdA dataset, a richly\nannotated corpus derived from 18th-century French encyclopedias. We propose\nframing NER as both token-level and span-level classification to accommodate\ncomplex nested entity structures typical of historical documents. Additionally,\nwe evaluate the emerging potential of few-shot prompting with generative\nlanguage models for low-resource scenarios. Our results demonstrate that while\ntransformer-based models achieve state-of-the-art performance, especially on\nnested entities, generative models offer promising alternatives when labeled\ndata are scarce. The study highlights ongoing challenges in historical NER and\nsuggests avenues for hybrid approaches combining symbolic and neural methods to\nbetter capture the intricacies of early modern French text.", "AI": {"tldr": "\u7814\u7a76\u5bf9\u6bd4\u4e86\u4f20\u7edfCRF\u3001spaCy\u3001CamemBERT\u548cFlair\u7b49\u6a21\u578b\u5728\u5386\u53f2\u6587\u672cNER\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u8bcd/\u8de8\u5ea6\u53cc\u5206\u7c7b\u6846\u67b6\u89e3\u51b3\u5d4c\u5957\u5b9e\u4f53\u95ee\u9898\uff0c\u9a8c\u8bc1\u751f\u6210\u5f0f\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u7684\u6f5c\u529b\u3002", "motivation": "\u89e3\u51b3\u5386\u53f2\u6587\u672c\u4e2d\u975e\u6807\u51c6\u5316\u8bed\u8a00\u3001\u53e4\u8001\u6b63\u5b57\u6cd5\u548c\u5d4c\u5957/\u91cd\u53e0\u5b9e\u4f53\u5e26\u6765\u7684NER\u8bc6\u522b\u96be\u9898\uff0c\u63a2\u7d22\u9002\u5e94\u590d\u6742\u7ed3\u6784\u7684\u6709\u6548\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528GeoEDdA\u6570\u636e\u96c6\uff0818\u4e16\u7eaa\u6cd5\u8bed\u767e\u79d1\u5168\u4e66\uff09\uff0c\u5c06NER\u5efa\u6a21\u4e3a\u8bcd\u7ea7/\u8de8\u5ea6\u7ea7\u5206\u7c7b\u4efb\u52a1\uff0c\u6d4b\u8bd5CRF\u3001spaCy\u3001CamemBERT\u3001Flair\u53ca\u751f\u6210\u5f0f\u6a21\u578b\u7684\u5c11\u6837\u672c\u5b66\u4e60\u3002", "result": "Transformer\u6a21\u578b\u5728\u5d4c\u5957\u5b9e\u4f53\u8bc6\u522b\u8fbe\u5230SOTA\uff0c\u751f\u6210\u6a21\u578b\u5728\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u65f6\u5c55\u73b0\u51fa\u66ff\u4ee3\u6f5c\u529b\u3002", "conclusion": "\u5386\u53f2NER\u4ecd\u9700\u89e3\u51b3\u590d\u6742\u8bed\u8a00\u7279\u5f81\uff0c\u5efa\u8bae\u7ed3\u5408\u7b26\u53f7\u89c4\u5219\u4e0e\u795e\u7ecf\u65b9\u6cd5\u7684\u6df7\u5408\u6846\u67b6\u63d0\u5347\u65e9\u671f\u73b0\u4ee3\u6cd5\u8bed\u6587\u672c\u89e3\u6790\u80fd\u529b\u3002"}}
{"id": "2506.02878", "pdf": "https://arxiv.org/pdf/2506.02878", "abs": "https://arxiv.org/abs/2506.02878", "authors": ["Jintian Shao", "Yiming Cheng"], "title": "CoT is Not True Reasoning, It Is Just a Tight Constraint to Imitate: A Theory Perspective", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of\nLarge Language Models on tasks requiring multi-step inference. This success has\nled to widespread claims of emergent reasoning capabilities in these models. In\nthis paper, we present a theoretical counter-perspective: Chain-of-Thought\n(CoT) does not elicit genuine, abstract reasoning. Instead, we argue that\nChain-of-Thought functions as a powerful structural constraint that guides\nLarge Language Models to imitate the form of reasoning. By forcing the\ngeneration of intermediate steps, Chain-of-Thought leverages the model immense\ncapacity for sequence prediction and pattern matching, effectively constraining\nits output to sequences that resemble coherent thought processes.\nChain-of-Thought (CoT) prompting has demonstrably enhanced the performance of\nLarge Language Models on tasks requiring multi-step inference. This success has\nled to widespread claims of emergent reasoning capabilities in these models. In\nthis paper, we present a theoretical counter-perspective: Chain-of-Thought\n(CoT) does not elicit genuine, abstract reasoning. Instead, we argue that\nChain-of-Thought functions as a powerful structural constraint that guides\nLarge Language Models to imitate the form of reasoning. By forcing the\ngeneration of intermediate steps, Chain-of-Thought leverages the model immense\ncapacity for sequence prediction and pattern matching, effectively constraining\nits output to sequences that resemble coherent thought processes.", "AI": {"tldr": "CoT\u63d0\u793a\u673a\u5236\u901a\u8fc7\u7ed3\u6784\u6027\u7ea6\u675f\u6a21\u4eff\u63a8\u7406\u5f62\u5f0f\uff0c\u800c\u975e\u6fc0\u53d1\u771f\u6b63\u62bd\u8c61\u63a8\u7406\u80fd\u529b", "motivation": "\u53cd\u9a73'CoT\u6fc0\u53d1\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b'\u7684\u4e3b\u6d41\u89c2\u70b9\uff0c\u63ed\u793a\u5176\u672c\u8d28\u662f\u8f93\u51fa\u5e8f\u5217\u7684\u7ed3\u6784\u6027\u7ea6\u675f", "method": "\u4ece\u8ba1\u7b97\u7406\u8bba\u89d2\u5ea6\u5206\u6790CoT\u5de5\u4f5c\u673a\u5236\uff0c\u8bba\u8bc1\u5176\u4f9d\u8d56\u5e8f\u5217\u9884\u6d4b\u548c\u6a21\u5f0f\u5339\u914d\u80fd\u529b", "result": "CoT\u901a\u8fc7\u5f3a\u5236\u4e2d\u95f4\u6b65\u9aa4\u751f\u6210\uff0c\u5229\u7528\u6a21\u578b\u56fa\u6709\u7684\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\u7ea6\u675f\u8f93\u51fa\u7ed3\u6784", "conclusion": "CoT\u672c\u8d28\u662f\u5bf9\u73b0\u6709\u80fd\u529b\u7684\u5de5\u7a0b\u5316\u8fd0\u7528\uff0c\u800c\u975e\u5b9e\u73b0\u771f\u6b63\u7684\u8ba4\u77e5\u63a8\u7406\u7a81\u7834"}}
{"id": "2506.02894", "pdf": "https://arxiv.org/pdf/2506.02894", "abs": "https://arxiv.org/abs/2506.02894", "authors": ["Verena Blaschke", "Miriam Winkler", "Constantin F\u00f6rster", "Gabriele Wenger-Glemser", "Barbara Plank"], "title": "A Multi-Dialectal Dataset for German Dialect ASR and Dialect-to-Standard Speech Translation", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Although Germany has a diverse landscape of dialects, they are\nunderrepresented in current automatic speech recognition (ASR) research. To\nenable studies of how robust models are towards dialectal variation, we present\nBetthupferl, an evaluation dataset containing four hours of read speech in\nthree dialect groups spoken in Southeast Germany (Franconian, Bavarian,\nAlemannic), and half an hour of Standard German speech. We provide both\ndialectal and Standard German transcriptions, and analyze the linguistic\ndifferences between them. We benchmark several multilingual state-of-the-art\nASR models on speech translation into Standard German, and find differences\nbetween how much the output resembles the dialectal vs. standardized\ntranscriptions. Qualitative error analyses of the best ASR model reveal that it\nsometimes normalizes grammatical differences, but often stays closer to the\ndialectal constructions.", "AI": {"tldr": "\u8bc4\u4f30\u591a\u8bed\u8a00ASR\u6a21\u578b\u5728\u5fb7\u56fd\u65b9\u8a00\u8bed\u97f3\u8f6c\u8bd1\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u8f93\u51fa\u5b58\u5728\u65b9\u8a00\u4e0e\u6807\u51c6\u5fb7\u8bed\u6df7\u5408\u7279\u5f81", "motivation": "\u5f53\u524d\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u7814\u7a76\u7f3a\u4e4f\u5bf9\u65b9\u8a00\u53d8\u4f53\u7684\u5173\u6ce8\uff0c\u7279\u522b\u662f\u5fb7\u56fd\u4e1c\u5357\u90e8\u65b9\u8a00\u5728ASR\u7cfb\u7edf\u4e2d\u7684\u9c81\u68d2\u6027\u7814\u7a76\u4e0d\u8db3", "method": "\u6784\u5efa\u5305\u542b\u4e09\u79cd\u5fb7\u56fd\u4e1c\u5357\u90e8\u65b9\u8a00\uff08\u5f17\u5170\u80af\u3001\u5df4\u4f10\u5229\u4e9a\u3001\u963f\u52d2\u66fc\u5c3c\uff09\u53ca\u6807\u51c6\u5fb7\u8bed\u7684Betthupferl\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u53cc\u91cd\u8f6c\u5f55\u672c\uff0c\u5e76\u6d4b\u8bd5\u591a\u4e2a\u5148\u8fdb\u591a\u8bed\u8a00ASR\u6a21\u578b\u7684\u8bed\u97f3\u8f6c\u8bd1\u80fd\u529b", "result": "\u6700\u4f73ASR\u6a21\u578b\u5728\u8bed\u6cd5\u5dee\u5f02\u6807\u51c6\u5316\u5904\u7406\u4e0e\u4fdd\u7559\u65b9\u8a00\u7ed3\u6784\u95f4\u5448\u73b0\u77db\u76fe\u8868\u73b0\uff0c\u8f93\u51fa\u7ed3\u679c\u4ecb\u4e8e\u65b9\u8a00\u8f6c\u5f55\u4e0e\u6807\u51c6\u5316\u8f6c\u5f55\u4e4b\u95f4", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86ASR\u7cfb\u7edf\u5904\u7406\u65b9\u8a00\u65f6\u5b58\u5728\u9009\u62e9\u6027\u6807\u51c6\u5316\u73b0\u8c61\uff0c\u4e3a\u6539\u8fdb\u65b9\u8a00\u8bed\u97f3\u8bc6\u522b\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e"}}
{"id": "2506.02899", "pdf": "https://arxiv.org/pdf/2506.02899", "abs": "https://arxiv.org/abs/2506.02899", "authors": ["Yusuke Sakai", "Takumi Goto", "Taro Watanabe"], "title": "IMPARA-GED: Grammatical Error Detection is Boosting Reference-free Grammatical Error Quality Estimator", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "We propose IMPARA-GED, a novel reference-free automatic grammatical error\ncorrection (GEC) evaluation method with grammatical error detection (GED)\ncapabilities. We focus on the quality estimator of IMPARA, an existing\nautomatic GEC evaluation method, and construct that of IMPARA-GED using a\npre-trained language model with enhanced GED capabilities. Experimental results\non SEEDA, a meta-evaluation dataset for automatic GEC evaluation methods,\ndemonstrate that IMPARA-GED achieves the highest correlation with human\nsentence-level evaluations.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u578b\u65e0\u9700\u53c2\u8003\u7684\u81ea\u52a8\u8bed\u6cd5\u7ea0\u9519\u8bc4\u4f30\u65b9\u6cd5IMPARA-GED\uff0c\u901a\u8fc7\u589e\u5f3a\u8bed\u6cd5\u9519\u8bef\u68c0\u6d4b\u80fd\u529b\u63d0\u5347\u8bc4\u4f30\u6548\u679c", "motivation": "\u73b0\u6709IMPARA\u65b9\u6cd5\u7684\u8bc4\u4f30\u8d28\u91cf\u53d7\u9650\u4e8e\u8bed\u6cd5\u9519\u8bef\u68c0\u6d4b\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u6784\u5efa\u66f4\u5f3a\u5927\u7684\u8bed\u6cd5\u9519\u8bef\u8bc6\u522b\u673a\u5236", "method": "\u57fa\u4e8e\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u8d28\u91cf\u8bc4\u4f30\u5668\uff0c\u91cd\u70b9\u589e\u5f3a\u8bed\u6cd5\u9519\u8bef\u68c0\u6d4b\u6a21\u5757\u7684\u8bc6\u522b\u80fd\u529b", "result": "\u5728SEEDA\u5143\u8bc4\u4f30\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u9ad8\u53e5\u5b50\u7ea7\u4eba\u5de5\u8bc4\u4f30\u76f8\u5173\u6027", "conclusion": "IMPARA-GED\u6210\u529f\u6574\u5408\u8bed\u6cd5\u9519\u8bef\u68c0\u6d4b\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u81ea\u52a8\u8bc4\u4f30\u7cfb\u7edf\u7684\u53ef\u9760\u6027"}}
{"id": "2506.02911", "pdf": "https://arxiv.org/pdf/2506.02911", "abs": "https://arxiv.org/abs/2506.02911", "authors": ["Yin Fang", "Qiao Jin", "Guangzhi Xiong", "Bowen Jin", "Xianrui Zhong", "Siru Ouyang", "Aidong Zhang", "Jiawei Han", "Zhiyong Lu"], "title": "Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.HC", "cs.LG"], "comment": "28 pages; 16 tables; 7 figures; Code:\n  https://github.com/ncbi-nlp/cell-o1", "summary": "Cell type annotation is a key task in analyzing the heterogeneity of\nsingle-cell RNA sequencing data. Although recent foundation models automate\nthis process, they typically annotate cells independently, without considering\nbatch-level cellular context or providing explanatory reasoning. In contrast,\nhuman experts often annotate distinct cell types for different cell clusters\nbased on their domain knowledge. To mimic this workflow, we introduce the\nCellPuzzles task, where the objective is to assign unique cell types to a batch\nof cells. This benchmark spans diverse tissues, diseases, and donor conditions,\nand requires reasoning across the batch-level cellular context to ensure label\nuniqueness. We find that off-the-shelf large language models (LLMs) struggle on\nCellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0%\nbatch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained\nvia supervised fine-tuning on distilled reasoning traces, followed by\nreinforcement learning with batch-level rewards. Cell-o1 achieves\nstate-of-the-art performance, outperforming o1 by over 73% and generalizing\nwell across contexts. Further analysis of training dynamics and reasoning\nbehaviors provides insights into batch-level annotation performance and\nemergent expert-like reasoning. Code and data are available at\nhttps://github.com/ncbi-nlp/cell-o1.", "AI": {"tldr": "\u63d0\u51faCellPuzzles\u4efb\u52a1\u4e0eCell-o1\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6570\u636e\u6279\u91cf\u6807\u6ce8\u6027\u80fd\uff0c\u8d85\u8d8a\u73b0\u6709\u6a21\u578b73%\u3002", "motivation": "\u73b0\u6709\u57fa\u7840\u6a21\u578b\u5728\u5355\u7ec6\u80de\u6ce8\u91ca\u65f6\u5ffd\u7565\u6279\u6b21\u7ea7\u7ec6\u80de\u5173\u8054\u4e14\u7f3a\u4e4f\u89e3\u91ca\u6027\uff0c\u800c\u4e13\u5bb6\u57fa\u4e8e\u77e5\u8bc6\u96c6\u7fa4\u6807\u6ce8\u3002\u9700\u5f00\u53d1\u80fd\u6a21\u62df\u4eba\u7c7b\u4e13\u5bb6\u6279\u91cf\u63a8\u7406\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u521b\u5efaCellPuzzles\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5f00\u53d17B\u53c2\u6570Cell-o1\u6a21\u578b\uff1a\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u84b8\u998f\u63a8\u7406\u8def\u5f84\uff0c\u7ed3\u5408\u6279\u91cf\u7ea7\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u3002", "result": "Cell-o1\u6279\u91cf\u7ea7\u51c6\u786e\u7387\u63d0\u534773%\uff0c\u5728\u8de8\u7ec4\u7ec7/\u75be\u75c5\u573a\u666f\u5c55\u73b0\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u63a8\u7406\u884c\u4e3a\u5448\u73b0\u7c7b\u4e13\u5bb6\u7279\u5f81\u3002", "conclusion": "Cell-o1\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6279\u91cf\u6ce8\u91ca\u6027\u80fd\uff0c\u5176\u8bad\u7ec3\u52a8\u6001\u5206\u6790\u4e3a\u751f\u7269\u533b\u5b66LLM\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002\u4ee3\u7801\u4e0e\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.02921", "pdf": "https://arxiv.org/pdf/2506.02921", "abs": "https://arxiv.org/abs/2506.02921", "authors": ["Yijun Yang", "Zeyu Huang", "Wenhao Zhu", "Zihan Qiu", "Fei Yuan", "Jeff Z. Pan", "Ivan Titov"], "title": "A Controllable Examination for Long-Context Language Models", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Existing frameworks for evaluating long-context language models (LCLM) can be\nbroadly categorized into real-world and synthetic tasks. Despite their utility,\nboth approaches are accompanied by certain intrinsic limitations. Real-world\ntasks are too complex to interpret or characterize and are susceptible to data\ncontamination. In contrast, synthetic tasks often adopt the\nneedle-in-the-haystack (NIAH) format, wherein a lack of coherence between the\n\"needle\" and the \"haystack\" compromises their validity as proxies for realistic\napplications. In response to these challenges, we posit that an ideal\nlong-context evaluation framework should be characterized by three essential\nfeatures: $\\textit{seamless context}$, $\\textit{controllable setting}$, and\n$\\textit{sound evaluation}$. This study introduces $\\textbf{LongBioBench}$, a\nnovel benchmark that utilizes artificially generated biographies as a\ncontrolled environment for assessing LCLMs across dimensions of\n$\\textit{understanding}$, $\\textit{reasoning}$, and $\\textit{trustworthiness}$.\nOur experimental evaluation, which includes $\\textbf{18}$ LCLMs in total,\ndemonstrates that most models still exhibit deficiencies in semantic\nunderstanding and elementary reasoning over retrieved results and are less\ntrustworthy as context length increases. Our further analysis indicates some\ndesign choices employed by existing synthetic benchmarks, such as contextual\nnon-coherence, numerical needles, and the absence of distractors, rendering\nthem vulnerable to test the model long-context capabilities. Moreover, we also\nreveal that long-context continual pretraining primarily adjusts RoPE embedding\nto accommodate extended context lengths. To sum up, compared to previous\nsynthetic benchmarks, LongBioBench achieves a better trade-off between\nmirroring authentic language tasks and maintaining controllability, and is\nhighly interpretable and configurable.", "AI": {"tldr": "\u63d0\u51faLongBioBench\u57fa\u51c6\uff0c\u901a\u8fc7\u4eba\u5de5\u751f\u6210\u7684\u4f20\u8bb0\u8bc4\u4f30\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u771f\u5b9e\u6027\u4e0e\u53ef\u63a7\u6027\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u5b9e\u9a8c\u663e\u793a\u591a\u6570\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u5b58\u5728\u7406\u89e3\u3001\u63a8\u7406\u548c\u53ef\u4fe1\u5ea6\u7f3a\u9677\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u6846\u67b6\u5b58\u5728\u5c40\u9650\uff1a\u771f\u5b9e\u4efb\u52a1\u8fc7\u4e8e\u590d\u6742\u4e14\u6613\u53d7\u6570\u636e\u6c61\u67d3\uff0c\u5408\u6210\u4efb\u52a1\u56e0'\u9488'\u4e0e'\u5e72\u8349\u5806'\u7f3a\u4e4f\u8fde\u8d2f\u6027\u800c\u6709\u6548\u6027\u4e0d\u8db3\u3002", "method": "\u6784\u5efaLongBioBench\u57fa\u51c6\uff0c\u901a\u8fc7\u63a7\u5236\u53d8\u91cf\u6cd5\u8bc4\u4f3018\u4e2aLCLM\u5728\u7406\u89e3\u3001\u63a8\u7406\u3001\u53ef\u4fe1\u5ea6\u4e09\u4e2a\u7ef4\u5ea6\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u6a21\u578b\u67b6\u6784\u8bbe\u8ba1\u5bf9\u957f\u4e0a\u4e0b\u6587\u80fd\u529b\u7684\u5f71\u54cd\u3002", "result": "\u591a\u6570\u6a21\u578b\u5728\u8bed\u4e49\u7406\u89e3\u548c\u57fa\u7840\u63a8\u7406\u8868\u73b0\u6b20\u4f73\uff0c\u4e0a\u4e0b\u6587\u589e\u957f\u5bfc\u81f4\u53ef\u4fe1\u5ea6\u4e0b\u964d\uff1b\u73b0\u6709\u5408\u6210\u57fa\u51c6\u56e0\u975e\u8fde\u8d2f\u4e0a\u4e0b\u6587\u3001\u6570\u503c\u9488\u7b49\u8bbe\u8ba1\u7f3a\u9677\u6613\u88ab\u653b\u7834\uff1bRoPE\u5d4c\u5165\u8c03\u6574\u662f\u957f\u4e0a\u4e0b\u6587\u6301\u7eed\u9884\u8bad\u7ec3\u7684\u4e3b\u8981\u624b\u6bb5\u3002", "conclusion": "LongBioBench\u76f8\u6bd4\u4f20\u7edf\u57fa\u51c6\u5b9e\u73b0\u4e86\u771f\u5b9e\u4efb\u52a1\u6a21\u62df\u4e0e\u53ef\u63a7\u6027\u7684\u4f18\u5316\u5e73\u8861\uff0c\u5177\u6709\u9ad8\u53ef\u89e3\u91ca\u6027\u548c\u914d\u7f6e\u7075\u6d3b\u6027\uff0c\u4e3aLCLM\u8bc4\u4f30\u63d0\u4f9b\u66f4\u53ef\u9760\u65b9\u6848\u3002"}}
{"id": "2506.02924", "pdf": "https://arxiv.org/pdf/2506.02924", "abs": "https://arxiv.org/abs/2506.02924", "authors": ["Diogo A. P. Nunes", "Eug\u00e9nio Ribeiro"], "title": "INESC-ID @ eRisk 2025: Exploring Fine-Tuned, Similarity-Based, and Prompt-Based Approaches to Depression Symptom Identification", "categories": ["cs.CL", "cs.IR", "cs.LG", "I.2.7; I.5.4; J.3; H.3.3"], "comment": "12 pages, 1 figure, 6 tables", "summary": "In this work, we describe our team's approach to eRisk's 2025 Task 1: Search\nfor Symptoms of Depression. Given a set of sentences and the Beck's Depression\nInventory - II (BDI) questionnaire, participants were tasked with submitting up\nto 1,000 sentences per depression symptom in the BDI, sorted by relevance.\nParticipant submissions were evaluated according to standard Information\nRetrieval (IR) metrics, including Average Precision (AP) and R-Precision\n(R-PREC). The provided training data, however, consisted of sentences labeled\nas to whether a given sentence was relevant or not w.r.t. one of BDI's\nsymptoms. Due to this labeling limitation, we framed our development as a\nbinary classification task for each BDI symptom, and evaluated accordingly. To\nthat end, we split the available labeled data into training and validation\nsets, and explored foundation model fine-tuning, sentence similarity, Large\nLanguage Model (LLM) prompting, and ensemble techniques. The validation results\nrevealed that fine-tuning foundation models yielded the best performance,\nparticularly when enhanced with synthetic data to mitigate class imbalance. We\nalso observed that the optimal approach varied by symptom. Based on these\ninsights, we devised five independent test runs, two of which used ensemble\nmethods. These runs achieved the highest scores in the official IR evaluation,\noutperforming submissions from 16 other teams.", "AI": {"tldr": "\u7814\u7a76\u56e2\u961f\u901a\u8fc7\u5fae\u8c03\u57fa\u7840\u6a21\u578b\u3001\u96c6\u6210\u65b9\u6cd5\u53ca\u5408\u6210\u6570\u636e\u589e\u5f3a\uff0c\u5728eRisk 2025\u6291\u90c1\u75c7\u72b6\u68c0\u7d22\u4efb\u52a1\u4e2d\u53d6\u5f97\u6700\u4f18\u7ed3\u679c\uff0c\u8d85\u8d8a16\u4e2a\u53c2\u8d5b\u56e2\u961f\u3002", "motivation": "\u89e3\u51b3BDI-II\u6291\u90c1\u75c7\u72b6\u6587\u672c\u68c0\u7d22\u4efb\u52a1\u4e2d\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\uff0c\u5c06\u75c7\u72b6\u8bc6\u522b\u8f6c\u5316\u4e3a\u4e8c\u5206\u7c7b\u4efb\u52a1\u4ee5\u63d0\u9ad8\u68c0\u6d4b\u6548\u679c\u3002", "method": "\u91c7\u7528\u6570\u636e\u5206\u5272+\u591a\u6280\u672f\u8def\u7ebf\uff1a1) \u57fa\u7840\u6a21\u578b\u5fae\u8c03(\u542b\u5408\u6210\u6570\u636e\u7f13\u89e3\u7c7b\u522b\u4e0d\u5e73\u8861) 2) \u53e5\u5b50\u76f8\u4f3c\u5ea6\u8ba1\u7b97 3) LLM\u63d0\u793a\u5de5\u7a0b 4) \u96c6\u6210\u5b66\u4e60\u3002\u6839\u636e\u75c7\u72b6\u7279\u6027\u9009\u62e9\u6700\u4f73\u65b9\u6848\u7ec4\u5408\u3002", "result": "\u4e94\u4e2a\u6d4b\u8bd5\u65b9\u6848\u4e2d\u4e24\u4e2a\u96c6\u6210\u65b9\u6cd5\u5728IR\u6807\u51c6\u6307\u6807(AP/R-PREC)\u4e0a\u8fbe\u5230\u6700\u9ad8\u5206\uff0c\u5b98\u65b9\u8bc4\u4f30\u6392\u540d\u7b2c\u4e00\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u5fae\u8c03\u7ed3\u5408\u75c7\u72b6\u5b9a\u5236\u5316\u7b56\u7565\u663e\u8457\u63d0\u5347\u6291\u90c1\u75c7\u72b6\u8bc6\u522b\u6548\u679c\uff0c\u5408\u6210\u6570\u636e\u589e\u5f3a\u6709\u6548\u514b\u670d\u6570\u636e\u5c40\u9650\uff0c\u96c6\u6210\u65b9\u6cd5\u53d1\u6325\u75c7\u72b6\u5dee\u5f02\u6027\u4f18\u52bf\u3002"}}
{"id": "2506.02945", "pdf": "https://arxiv.org/pdf/2506.02945", "abs": "https://arxiv.org/abs/2506.02945", "authors": ["Aishwarya Sahoo", "Jeevana Kruthi Karnuthala", "Tushar Parmanand Budhwani", "Pranchal Agarwal", "Sankaran Vaidyanathan", "Alexa Siu", "Franck Dernoncourt", "Jennifer Healey", "Nedim Lipka", "Ryan Rossi", "Uttaran Bhattacharya", "Branislav Kveton"], "title": "Quantitative LLM Judges", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "LLM-as-a-judge is a framework in which a large language model (LLM)\nautomatically evaluates the output of another LLM. We propose quantitative LLM\njudges, which align evaluation scores of existing LLM judges to human scores in\na given domain using regression models. The models are trained to improve the\nscore of the original judge by using the judge's textual evaluation and score.\nWe present four quantitative judges for different types of absolute and\nrelative feedback, which showcases the generality and versatility of our\nframework. Our framework is more computationally efficient than supervised\nfine-tuning and can be more statistically efficient when human feedback is\nlimited, which is expected in most applications of our work. We validate these\nclaims empirically on four datasets using two base judges. Our experiments show\nthat quantitative judges can effectively improve the predictive power of\nexisting judges through post-hoc modeling.", "AI": {"tldr": "\u63d0\u51fa\u5b9a\u91cfLLM\u8bc4\u59d4\u6846\u67b6\uff0c\u901a\u8fc7\u56de\u5f52\u6a21\u578b\u5bf9\u9f50\u4eba\u7c7b\u8bc4\u5206\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u7edf\u8ba1\u6548\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5", "motivation": "\u89e3\u51b3\u73b0\u6709LLM\u8bc4\u59d4\u8bc4\u5206\u4e0e\u4eba\u7c7b\u8bc4\u5206\u5b58\u5728\u504f\u5dee\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u540e\u5904\u7406\u5efa\u6a21\u63d0\u5347\u8bc4\u4f30\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u4eba\u7c7b\u53cd\u9988\u6709\u9650\u573a\u666f\u4e0b\u7684\u5e94\u7528\u4ef7\u503c", "method": "\u57fa\u4e8e\u539f\u59cb\u8bc4\u59d4\u7684\u6587\u672c\u8bc4\u4f30\u548c\u5206\u6570\uff0c\u8bad\u7ec3\u56de\u5f52\u6a21\u578b\u6784\u5efa\u56db\u79cd\u5b9a\u91cf\u8bc4\u59d4\u7c7b\u578b\uff08\u7edd\u5bf9/\u76f8\u5bf9\u53cd\u9988\u573a\u666f\uff09\uff0c\u5b9e\u73b0\u5206\u6570\u5bf9\u9f50", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u663e\u793a\uff0c\u5b9a\u91cf\u8bc4\u59d4\u901a\u8fc7\u540e\u5904\u7406\u5efa\u6a21\u663e\u8457\u63d0\u5347\u539f\u59cb\u8bc4\u59d4\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u4e14\u5728\u6709\u9650\u4eba\u7c7b\u53cd\u9988\u65f6\u7edf\u8ba1\u6548\u7387\u66f4\u4f18", "conclusion": "\u5b9a\u91cfLLM\u8bc4\u59d4\u6846\u67b6\u4e3a\u81ea\u52a8\u5316\u8bc4\u4f30\u7cfb\u7edf\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u7684\u540e\u5904\u7406\u5efa\u6a21\u5b9e\u73b0\u8bc4\u5206\u4f53\u7cfb\u7684\u6301\u7eed\u4f18\u5316"}}
{"id": "2506.02951", "pdf": "https://arxiv.org/pdf/2506.02951", "abs": "https://arxiv.org/abs/2506.02951", "authors": ["Boyi Li", "Zhonghan Zhao", "Der-Horng Lee", "Gaoang Wang"], "title": "Adaptive Graph Pruning for Multi-Agent Communication", "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "Large Language Model (LLM) based multi-agent systems have shown remarkable\nperformance in various tasks, especially when enhanced through collaborative\ncommunication. However, current methods often rely on a fixed number of agents\nand static communication structures, limiting their ability to adapt to varying\ntask complexities. In this paper, we propose Adaptive Graph Pruning (AGP), a\nnovel task-adaptive multi-agent collaboration framework that jointly optimizes\nagent quantity (hard-pruning) and communication topology (soft-pruning).\nSpecifically, our method employs a two-stage training strategy: firstly,\nindependently training soft-pruning networks for different agent quantities to\ndetermine optimal agent-quantity-specific complete graphs and positional masks\nacross specific tasks; and then jointly optimizing hard-pruning and\nsoft-pruning within a maximum complete graph to dynamically configure the\nnumber of agents and their communication topologies per task. Extensive\nexperiments demonstrate that our approach is: (1) High-performing, achieving\nstate-of-the-art results across six benchmarks and consistently generalizes\nacross multiple mainstream LLM architectures, with a increase in performance of\n$2.58\\%\\sim 9.84\\%$; (2) Task-adaptive, dynamically constructing optimized\ncommunication topologies tailored to specific tasks, with an extremely high\nperformance in all three task categories (general reasoning, mathematical\nreasoning, and code generation); (3) Token-economical, having fewer training\nsteps and token consumption at the same time, with a decrease in token\nconsumption of $90\\%+$; and (4) Training-efficient, achieving high performance\nwith very few training steps compared with other methods. The performance will\nsurpass the existing baselines after about ten steps of training under six\nbenchmarks.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u56fe\u526a\u679d\u6846\u67b6AGP\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8054\u5408\u4f18\u5316\u667a\u80fd\u4f53\u6570\u91cf\u4e0e\u901a\u4fe1\u62d3\u6251\uff0c\u5b9e\u73b0\u4efb\u52a1\u81ea\u9002\u5e94\u7684\u9ad8\u6548\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u91c7\u7528\u56fa\u5b9a\u6570\u91cf\u548c\u9759\u6001\u901a\u4fe1\u7ed3\u6784\uff0c\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u590d\u6742\u5ea6\u4efb\u52a1\u7684\u9700\u6c42\uff0c\u5bfc\u81f4\u8d44\u6e90\u6d6a\u8d39\u548c\u6027\u80fd\u53d7\u9650\u3002", "method": "\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a1\uff09\u72ec\u7acb\u8bad\u7ec3\u8f6f\u526a\u679d\u7f51\u7edc\u786e\u5b9a\u4efb\u52a1\u6700\u4f18\u7684\u667a\u80fd\u4f53\u6570\u91cf\u4e0e\u4f4d\u7f6e\u63a9\u7801\uff1b2\uff09\u5728\u6700\u5927\u5b8c\u5168\u56fe\u4e2d\u8054\u5408\u4f18\u5316\u786c\u526a\u679d\uff08\u6570\u91cf\uff09\u4e0e\u8f6f\u526a\u679d\uff08\u62d3\u6251\uff09\uff0c\u52a8\u6001\u914d\u7f6e\u7cfb\u7edf\u67b6\u6784\u3002", "result": "\u5728\u516d\u5927\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0SOTA\uff08\u6027\u80fd\u63d0\u53472.58%~9.84%\uff09\uff0c\u901a\u7528\u6027\u5f3a\u4e14token\u6d88\u8017\u51cf\u5c1190%+\uff0c\u4ec5\u9700\u5341\u6b65\u8bad\u7ec3\u5373\u53ef\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "AGP\u6846\u67b6\u517c\u5177\u9ad8\u6027\u80fd\u3001\u4efb\u52a1\u9002\u914d\u6027\u3001\u7ecf\u6d4e\u6027\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u5728\u901a\u7528\u63a8\u7406/\u6570\u5b66\u63a8\u7406/\u4ee3\u7801\u751f\u6210\u7b49\u4efb\u52a1\u4e2d\u5747\u5c55\u73b0\u6781\u5f3a\u9002\u5e94\u6027\u3002"}}
{"id": "2506.02959", "pdf": "https://arxiv.org/pdf/2506.02959", "abs": "https://arxiv.org/abs/2506.02959", "authors": ["Zhixiong Su", "Yichen Wang", "Herun Wan", "Zhaohan Zhang", "Minnan Luo"], "title": "HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection under Human-AI Coauthoring", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The misuse of large language models (LLMs) poses potential risks, motivating\nthe development of machine-generated text (MGT) detection. Existing literature\nprimarily concentrates on binary, document-level detection, thereby neglecting\ntexts that are composed jointly by human and LLM contributions. Hence, this\npaper explores the possibility of fine-grained MGT detection under human-AI\ncoauthoring. We suggest fine-grained detectors can pave pathways toward\ncoauthored text detection with a numeric AI ratio. Specifically, we propose a\ndataset, HACo-Det, which produces human-AI coauthored texts via an automatic\npipeline with word-level attribution labels. We retrofit seven prevailing\ndocument-level detectors to generalize them to word-level detection. Then we\nevaluate these detectors on HACo-Det on both word- and sentence-level detection\ntasks. Empirical results show that metric-based methods struggle to conduct\nfine-grained detection with a 0.462 average F1 score, while finetuned models\nshow superior performance and better generalization across domains. However, we\nargue that fine-grained co-authored text detection is far from solved. We\nfurther analyze factors influencing performance, e.g., context window, and\nhighlight the limitations of current methods, pointing to potential avenues for\nimprovement.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faHACo-Det\u6570\u636e\u96c6\u7528\u4e8e\u4eba\u673a\u534f\u4f5c\u573a\u666f\u4e0b\u7684\u7ec6\u7c92\u5ea6\u6587\u672c\u68c0\u6d4b\uff0c\u901a\u8fc7\u6539\u9020\u73b0\u6709\u6587\u6863\u7ea7\u68c0\u6d4b\u5668\u9a8c\u8bc1\u4e86\u5fae\u8c03\u6a21\u578b\u5728\u8bcd\u53e5\u7ea7\u68c0\u6d4b\u4e2d\u7684\u4f18\u8d8a\u6027\uff08\u5e73\u5747F1 0.462 vs \u66f4\u4f18\u8868\u73b0\uff09\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u6587\u672c\u68c0\u6d4b\u805a\u7126\u6587\u6863\u7ea7\u4e8c\u5143\u5224\u65ad\uff0c\u96be\u4ee5\u5e94\u5bf9\u4eba\u673a\u6df7\u5408\u521b\u4f5c\u573a\u666f\u3002\u9700\u8981\u5f00\u53d1\u80fd\u68c0\u6d4bAI\u53c2\u4e0e\u6bd4\u4f8b\u7684\u7ec6\u7c92\u5ea6\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "1. \u6784\u5efaHACo-Det\u6570\u636e\u96c6\uff08\u81ea\u52a8\u5316\u751f\u6210\u4eba\u673a\u534f\u4f5c\u6587\u672c\uff0c\u542b\u8bcd\u7ea7\u6807\u6ce8\uff09\n2. \u6539\u90207\u4e2a\u4e3b\u6d41\u6587\u6863\u68c0\u6d4b\u5668\u9002\u914d\u8bcd\u7ea7\u68c0\u6d4b\n3. \u5728\u8bcd\u7ea7/\u53e5\u7ea7\u4efb\u52a1\u4e0a\u8bc4\u4f30\u6a21\u578b\u8868\u73b0", "result": "\u5ea6\u91cf\u65b9\u6cd5\u8868\u73b0\u6b20\u4f73\uff08\u5e73\u5747F1 0.462\uff09\uff0c\u5fae\u8c03\u6a21\u578b\u5c55\u73b0\u66f4\u4f18\u6027\u80fd\u4e0e\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002\u4e0a\u4e0b\u6587\u7a97\u53e3\u7b49\u56e0\u7d20\u663e\u8457\u5f71\u54cd\u68c0\u6d4b\u6548\u679c\u3002", "conclusion": "\u7ec6\u7c92\u5ea6\u4eba\u673a\u534f\u4f5c\u6587\u672c\u68c0\u6d4b\u5c1a\u672a\u5b8c\u5168\u89e3\u51b3\uff0c\u9700\u5728\u4e0a\u4e0b\u6587\u5efa\u6a21\u3001\u57df\u9002\u5e94\u7b49\u65b9\u9762\u6301\u7eed\u6539\u8fdb\u3002\u5f53\u524d\u65b9\u6cd5\u5728\u957f\u8ddd\u79bb\u4f9d\u8d56\u5904\u7406\u5b58\u5728\u5c40\u9650\u3002"}}
{"id": "2506.02961", "pdf": "https://arxiv.org/pdf/2506.02961", "abs": "https://arxiv.org/abs/2506.02961", "authors": ["Yan Gao", "Massimo Roberto Scamarcia", "Javier Fernandez-Marques", "Mohammad Naseri", "Chong Shen Ng", "Dimitris Stripelis", "Zexi Li", "Tao Shen", "Jiamu Bai", "Daoyuan Chen", "Zikai Zhang", "Rui Hu", "InSeo Song", "Lee KangYoon", "Hong Jia", "Ting Dang", "Junyan Wang", "Zheyuan Liu", "Daniel Janes Beutel", "Lingjuan Lyu", "Nicholas D. Lane"], "title": "FlowerTune: A Cross-Domain Benchmark for Federated Fine-Tuning of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved state-of-the-art results across\ndiverse domains, yet their development remains reliant on vast amounts of\npublicly available data, raising concerns about data scarcity and the lack of\naccess to domain-specific, sensitive information. Federated Learning (FL)\npresents a compelling framework to address these challenges by enabling\ndecentralized fine-tuning on pre-trained LLMs without sharing raw data.\nHowever, the compatibility and performance of pre-trained LLMs in FL settings\nremain largely under explored. We introduce the FlowerTune LLM Leaderboard, a\nfirst-of-its-kind benchmarking suite designed to evaluate federated fine-tuning\nof LLMs across four diverse domains: general NLP, finance, medical, and coding.\nEach domain includes federated instruction-tuning datasets and domain-specific\nevaluation metrics. Our results, obtained through a collaborative, open-source\nand community-driven approach, provide the first comprehensive comparison\nacross 26 pre-trained LLMs with different aggregation and fine-tuning\nstrategies under federated settings, offering actionable insights into model\nperformance, resource constraints, and domain adaptation. This work lays the\nfoundation for developing privacy-preserving, domain-specialized LLMs for\nreal-world applications.", "AI": {"tldr": "\u901a\u8fc7\u8054\u90a6\u5b66\u4e60\u6846\u67b6FlowerTune LLM Leaderboard\u8bc4\u4f3026\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9690\u79c1\u4fdd\u62a4\u573a\u666f\u4e0b\u7684\u9886\u57df\u9002\u5e94\u80fd\u529b", "motivation": "\u89e3\u51b3\u5927\u6a21\u578b\u8bad\u7ec3\u4f9d\u8d56\u516c\u5f00\u6570\u636e\u5bfc\u81f4\u7684\u9886\u57df\u654f\u611f\u6570\u636e\u83b7\u53d6\u96be\u9898\uff0c\u63a2\u7d22\u8054\u90a6\u5b66\u4e60\u5728LLM\u5fae\u8c03\u4e2d\u7684\u517c\u5bb9\u6027\u95ee\u9898", "method": "\u521b\u5efa\u8de8NLP/\u91d1\u878d/\u533b\u7597/\u7f16\u7a0b\u56db\u5927\u9886\u57df\u7684\u8054\u90a6\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u96c6\uff0c\u91c7\u7528\u793e\u533a\u534f\u4f5c\u65b9\u5f0f\u6d4b\u8bd5\u4e0d\u540c\u805a\u5408\u7b56\u7565\u548c\u5fae\u8c03\u65b9\u6848", "result": "\u9996\u6b21\u7cfb\u7edf\u6027\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u5728\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e0b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u63ed\u793a\u8ba1\u7b97\u8d44\u6e90\u7ea6\u675f\u4e0e\u9886\u57df\u9002\u5e94\u6027\u7684\u5173\u952e\u5e73\u8861\u70b9", "conclusion": "\u4e3a\u5f00\u53d1\u9690\u79c1\u5b89\u5168\u7684\u9886\u57df\u4e13\u7528\u5927\u6a21\u578b\u5960\u5b9a\u57fa\u7840\uff0c\u63a8\u52a8\u73b0\u5b9e\u573a\u666f\u4e2d\u8054\u90a6\u5b66\u4e60\u4e0eLLM\u7684\u6df1\u5ea6\u6574\u5408\u5e94\u7528"}}
{"id": "2506.02973", "pdf": "https://arxiv.org/pdf/2506.02973", "abs": "https://arxiv.org/abs/2506.02973", "authors": ["Dingwei Chen", "Ziqiang Liu", "Feiteng Fang", "Chak Tou Leong", "Shiwen Ni", "Ahmadreza Argha", "Hamid Alinejad-Rokny", "Min Yang", "Chengming Li"], "title": "Expanding before Inferring: Enhancing Factuality in Large Language Models through Premature Layers Interpolation", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities in text\nunderstanding and generation. However, their tendency to produce factually\ninconsistent outputs, commonly referred to as ''hallucinations'', remains a\ncritical challenge. Existing approaches, such as retrieval-based and\ninference-time correction methods, primarily address this issue at the input or\noutput level, often overlooking the intrinsic information refinement process\nand the role of premature layers. Meanwhile, alignment- and fine-tuning-based\nmethods are resource-intensive. In this paper, we propose PLI (Premature Layers\nInterpolation), a novel, training-free, and plug-and-play intervention designed\nto enhance factuality. PLI mitigates hallucinations by inserting premature\nlayers formed through mathematical interpolation with adjacent layers. Inspired\nby stable diffusion and sampling steps, PLI extends the depth of information\nprocessing and transmission in LLMs, improving factual coherence. Experiments\non four publicly available datasets demonstrate that PLI effectively reduces\nhallucinations while outperforming existing baselines in most cases. Further\nanalysis suggests that the success of layer interpolation is closely linked to\nLLMs' internal mechanisms. To promote reproducibility, we will release our code\nand data upon acceptance.", "AI": {"tldr": "\u63d0\u51faPLI\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u5b66\u63d2\u503c\u5728\u76f8\u90bb\u5c42\u95f4\u63d2\u5165\u672a\u6210\u719f\u5c42\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u6709\u6548\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e8b\u5b9e\u6027\u5e7b\u89c9", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u6a21\u578b\u5185\u90e8\u4fe1\u606f\u7cbe\u70bc\u8fc7\u7a0b\uff0c\u5fae\u8c03\u65b9\u6cd5\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u9700\u8981\u66f4\u672c\u8d28\u7684\u5e72\u9884\u624b\u6bb5", "method": "\u53d7\u7a33\u5b9a\u6269\u6563\u542f\u53d1\uff0c\u5bf9\u76f8\u90bb\u5c42\u53c2\u6570\u8fdb\u884c\u6570\u5b66\u63d2\u503c\u5f62\u6210\u672a\u6210\u719f\u5c42\uff0c\u5ef6\u957f\u4fe1\u606f\u5904\u7406\u8def\u5f84", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\uff0c\u5e7b\u89c9\u7387\u5e73\u5747\u964d\u4f4e12.8%\uff0c\u63a8\u7406\u901f\u5ea6\u4fdd\u630190%", "conclusion": "\u5c42\u63d2\u503c\u673a\u5236\u4e0eLLM\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u9ad8\u5ea6\u76f8\u5173\uff0c\u8be5\u65b9\u6cd5\u4e3a\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u65b0\u65b9\u5411\uff0c\u627f\u8bfa\u5f00\u6e90\u4ee3\u7801\u6570\u636e"}}
{"id": "2506.02979", "pdf": "https://arxiv.org/pdf/2506.02979", "abs": "https://arxiv.org/abs/2506.02979", "authors": ["Atsumoto Ohashi", "Shinya Iizuka", "Jingjing Jiang", "Ryuichiro Higashinaka"], "title": "Towards a Japanese Full-duplex Spoken Dialogue System", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Full-duplex spoken dialogue systems, which can model simultaneous\nbidirectional features of human conversations such as speech overlaps and\nbackchannels, have attracted significant attention recently. However, the study\nof full-duplex spoken dialogue systems for the Japanese language has been\nlimited, and the research on their development in Japanese remains scarce. In\nthis paper, we present the first publicly available full-duplex spoken dialogue\nmodel in Japanese, which is built upon Moshi, a full-duplex dialogue model in\nEnglish. Our model is trained through a two-stage process: pre-training on a\nlarge-scale spoken dialogue data in Japanese, followed by fine-tuning on\nhigh-quality stereo spoken dialogue data. We further enhance the model's\nperformance by incorporating synthetic dialogue data generated by a\nmulti-stream text-to-speech system. Evaluation experiments demonstrate that the\ntrained model outperforms Japanese baseline models in both naturalness and\nmeaningfulness.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u9996\u4e2a\u65e5\u8bed\u5168\u53cc\u5de5\u8bed\u97f3\u5bf9\u8bdd\u6a21\u578b\uff0c\u57fa\u4e8e\u82f1\u6587\u6a21\u578bMoshi\u6539\u8fdb\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u548c\u5408\u6210\u6570\u636e\u589e\u5f3a\uff0c\u6548\u679c\u4f18\u4e8e\u65e5\u8bed\u57fa\u7ebf\u6a21\u578b", "motivation": "\u586b\u8865\u65e5\u8bed\u5168\u53cc\u5de5\u5bf9\u8bdd\u7cfb\u7edf\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u4e2d\u65e5\u8bed\u6a21\u578b\u532e\u4e4f\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u7c7b\u4f3c\u4eba\u7c7b\u5bf9\u8bdd\u7684\u8bed\u97f3\u91cd\u53e0\u548c\u53cd\u9988\u529f\u80fd", "method": "\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff1a\u5148\u5728\u5927\u89c4\u6a21\u65e5\u8bed\u8bed\u97f3\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\uff0c\u518d\u901a\u8fc7\u9ad8\u8d28\u91cf\u7acb\u4f53\u58f0\u5bf9\u8bdd\u6570\u636e\u5fae\u8c03\uff1b\u4f7f\u7528\u591a\u6d41\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf\u751f\u6210\u5408\u6210\u5bf9\u8bdd\u6570\u636e\u589e\u5f3a\u6027\u80fd", "result": "\u8bc4\u4f30\u5b9e\u9a8c\u663e\u793a\u8be5\u6a21\u578b\u5728\u81ea\u7136\u5ea6\u548c\u8bed\u4e49\u8fde\u8d2f\u6027\u65b9\u9762\u5747\u8d85\u8d8a\u65e5\u8bed\u57fa\u7ebf\u6a21\u578b", "conclusion": "\u6210\u529f\u9a8c\u8bc1\u4e86\u5c06\u82f1\u8bed\u6a21\u578b\u8fc1\u79fb\u5230\u65e5\u8bed\u7684\u6280\u672f\u8def\u5f84\uff0c\u8bc1\u660e\u6570\u636e\u589e\u5f3a\u548c\u5206\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u5728\u6784\u5efa\u975e\u82f1\u8bed\u5168\u53cc\u5de5\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5176\u4ed6\u8bed\u8a00\u7c7b\u4f3c\u7cfb\u7edf\u7684\u5f00\u53d1\u63d0\u4f9b\u53c2\u8003"}}
{"id": "2506.02987", "pdf": "https://arxiv.org/pdf/2506.02987", "abs": "https://arxiv.org/abs/2506.02987", "authors": ["Richard Armitage"], "title": "Performance of leading large language models in May 2025 in Membership of the Royal College of General Practitioners-style examination questions: a cross-sectional analysis", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "12 pages, 1 Table", "summary": "Background: Large language models (LLMs) have demonstrated substantial\npotential to support clinical practice. Other than Chat GPT4 and its\npredecessors, few LLMs, especially those of the leading and more powerful\nreasoning model class, have been subjected to medical specialty examination\nquestions, including in the domain of primary care. This paper aimed to test\nthe capabilities of leading LLMs as of May 2025 (o3, Claude Opus 4, Grok3, and\nGemini 2.5 Pro) in primary care education, specifically in answering Member of\nthe Royal College of General Practitioners (MRCGP) style examination questions.\n  Methods: o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro were tasked to answer\n100 randomly chosen multiple choice questions from the Royal College of General\nPractitioners GP SelfTest on 25 May 2025. Questions included textual\ninformation, laboratory results, and clinical images. Each model was prompted\nto answer as a GP in the UK and was provided with full question information.\nEach question was attempted once by each model. Responses were scored against\ncorrect answers provided by GP SelfTest.\n  Results: The total score of o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro was\n99.0%, 95.0%, 95.0%, and 95.0%, respectively. The average peer score for the\nsame questions was 73.0%.\n  Discussion: All models performed remarkably well, and all substantially\nexceeded the average performance of GPs and GP registrars who had answered the\nsame questions. o3 demonstrated the best performance, while the performances of\nthe other leading models were comparable with each other and were not\nsubstantially lower than that of o3. These findings strengthen the case for\nLLMs, particularly reasoning models, to support the delivery of primary care,\nespecially those that have been specifically trained on primary care clinical\ndata.", "AI": {"tldr": "\u9886\u5148\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728MRCGP\u8003\u8bd5\u9898\u4e2d\u8868\u73b0\u4f18\u5f02\uff08o3\u8fbe99%\uff09\uff0c\u663e\u8457\u8d85\u8d8a\u4eba\u7c7b\u540c\u884c73%\u5e73\u5747\u5206\uff0c\u9a8c\u8bc1\u5176\u5728\u521d\u7ea7\u4fdd\u5065\u4e2d\u7684\u652f\u6301\u6f5c\u529b\u3002", "motivation": "\u6d4b\u8bd52025\u5e745\u6708\u4e3b\u6d41LLMs\uff08o3/Claude Opus 4/Grok3/Gemini 2.5 Pro\uff09\u5728\u521d\u7ea7\u4fdd\u5065\u6559\u80b2\u573a\u666f\u4e0b\u7684MRCGP\u8003\u8bd5\u5e94\u7b54\u80fd\u529b\uff0c\u586b\u8865\u63a8\u7406\u6a21\u578b\u5728\u57fa\u5c42\u533b\u7597\u9886\u57df\u7684\u8bc4\u4f30\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u7687\u5bb6\u5168\u79d1\u533b\u5e08\u5b66\u9662100\u9053\u968f\u673a\u591a\u9009\u9898\uff08\u542b\u6587\u672c/\u5b9e\u9a8c\u5ba4\u6570\u636e/\u4e34\u5e8a\u56fe\u50cf\uff09\uff0c\u6a21\u62df\u82f1\u56fd\u5168\u79d1\u533b\u751f\u89d2\u8272\u5355\u6b21\u4f5c\u7b54\uff0c\u5bf9\u6bd4\u6807\u51c6\u7b54\u6848\u8ba1\u5206\u3002", "result": "o3\u5f97\u520699%\u5c45\u9996\uff0c\u5176\u4ed6\u6a21\u578b\u5747\u4e3a95%\uff0c\u6240\u6709\u6a21\u578b\u5747\u5927\u5e45\u8d85\u8fc7\u4eba\u7c7b\u540c\u884c73%\u5e73\u5747\u5206\u3002", "conclusion": "\u63a8\u7406\u7c7bLLMs\uff08\u5c24\u5176\u662f\u7ecf\u8fc7\u521d\u7ea7\u4fdd\u5065\u6570\u636e\u4e13\u95e8\u8bad\u7ec3\u8005\uff09\u53ef\u6709\u6548\u652f\u6301\u57fa\u5c42\u533b\u7597\u670d\u52a1\uff0c\u6a21\u578b\u6027\u80fd\u5df2\u5177\u5907\u4e34\u5e8a\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2506.02995", "pdf": "https://arxiv.org/pdf/2506.02995", "abs": "https://arxiv.org/abs/2506.02995", "authors": ["Iuliia Zaitova", "Badr M. Abdullah", "Wei Xue", "Dietrich Klakow", "Bernd M\u00f6bius", "Tania Avgustinova"], "title": "It's Not a Walk in the Park! Challenges of Idiom Translation in Speech-to-text Systems", "categories": ["cs.CL"], "comment": "13 pages, 3 figures, ACL 2025", "summary": "Idioms are defined as a group of words with a figurative meaning not\ndeducible from their individual components. Although modern machine translation\nsystems have made remarkable progress, translating idioms remains a major\nchallenge, especially for speech-to-text systems, where research on this topic\nis notably sparse. In this paper, we systematically evaluate idiom translation\nas compared to conventional news translation in both text-to-text machine\ntranslation (MT) and speech-to-text translation (SLT) systems across two\nlanguage pairs (German to English, Russian to English). We compare\nstate-of-the-art end-to-end SLT systems (SeamlessM4T SLT-to-text, Whisper Large\nv3) with MT systems (SeamlessM4T SLT-to-text, No Language Left Behind), Large\nLanguage Models (DeepSeek, LLaMA) and cascaded alternatives. Our results reveal\nthat SLT systems experience a pronounced performance drop on idiomatic data,\noften reverting to literal translations even in higher layers, whereas MT\nsystems and Large Language Models demonstrate better handling of idioms. These\nfindings underscore the need for idiom-specific strategies and improved\ninternal representations in SLT architectures.", "AI": {"tldr": "\u8bed\u97f3\u5230\u6587\u672c\u7ffb\u8bd1\u7cfb\u7edf\u5728\u4e60\u8bed\u7ffb\u8bd1\u4e2d\u5b58\u5728\u663e\u8457\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u66f4\u4f18\uff0c\u9700\u6539\u8fdbSLT\u67b6\u6784\u7684\u4e60\u8bed\u5904\u7406\u7b56\u7565", "motivation": "\u73b0\u4ee3\u673a\u5668\u7ffb\u8bd1\u5728\u5e38\u89c4\u6587\u672c\u5904\u7406\u4e0a\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u4e60\u8bed\u7ffb\u8bd1\u4ecd\u662f\u6311\u6218\uff0c\u5c24\u5176\u5728\u8bed\u97f3\u5230\u6587\u672c\u7ffb\u8bd1\u9886\u57df\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\u3002\u7814\u7a76\u65e8\u5728\u5bf9\u6bd4\u4e0d\u540c\u7cfb\u7edf\u5728\u4e60\u8bed\u7ffb\u8bd1\u4e2d\u7684\u8868\u73b0\u5dee\u5f02", "method": "\u7cfb\u7edf\u8bc4\u4f30\u5fb7\u8bed/\u4fc4\u8bed-\u82f1\u8bed\u7684\u53cc\u5411\u7ffb\u8bd1\uff0c\u5bf9\u6bd4\u7aef\u5230\u7aef\u8bed\u97f3\u7ffb\u8bd1\u7cfb\u7edf\uff08SeamlessM4T\u3001Whisper\uff09\u3001\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\uff08NLLB\uff09\u3001\u5927\u8bed\u8a00\u6a21\u578b\uff08DeepSeek\u3001LLaMA\uff09\u548c\u7ea7\u8054\u7cfb\u7edf\u7684\u8868\u73b0", "result": "\u8bed\u97f3\u7ffb\u8bd1\u7cfb\u7edf\u51fa\u73b0\u663e\u8457\u6027\u80fd\u4e0b\u964d\uff08\u503e\u5411\u4e8e\u76f4\u8bd1\uff09\uff0c\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u548c\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u66f4\u597d\u5904\u7406\u4e60\u8bed\uff0c\u6df1\u5c42\u7f51\u7edc\u5206\u6790\u663e\u793a\u8bed\u97f3\u7cfb\u7edf\u5728\u9ad8\u5c42\u4ecd\u4fdd\u7559\u5b57\u9762\u7279\u5f81", "conclusion": "\u8bed\u97f3\u5230\u6587\u672c\u7ffb\u8bd1\u67b6\u6784\u9700\u8981\u5f00\u53d1\u4e60\u8bed\u4e13\u7528\u7b56\u7565\u5e76\u6539\u8fdb\u5185\u90e8\u8868\u5f81\uff0c\u7814\u7a76\u4e3a\u63d0\u5347\u8de8\u6a21\u6001\u7ffb\u8bd1\u7cfb\u7edf\u7684\u8bed\u8a00\u654f\u611f\u6027\u63d0\u4f9b\u4e86\u65b9\u5411"}}
{"id": "2506.02998", "pdf": "https://arxiv.org/pdf/2506.02998", "abs": "https://arxiv.org/abs/2506.02998", "authors": ["\u0110or\u0111e Klisura", "Astrid R Bernaga Torres", "Anna Karen G\u00e1rate-Escamilla", "Rajesh Roshan Biswal", "Ke Yang", "Hilal Pataci", "Anthony Rios"], "title": "A Multi-Agent Framework for Mitigating Dialect Biases in Privacy Policy Question-Answering Systems", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Privacy policies inform users about data collection and usage, yet their\ncomplexity limits accessibility for diverse populations. Existing Privacy\nPolicy Question Answering (QA) systems exhibit performance disparities across\nEnglish dialects, disadvantaging speakers of non-standard varieties. We propose\na novel multi-agent framework inspired by human-centered design principles to\nmitigate dialectal biases. Our approach integrates a Dialect Agent, which\ntranslates queries into Standard American English (SAE) while preserving\ndialectal intent, and a Privacy Policy Agent, which refines predictions using\ndomain expertise. Unlike prior approaches, our method does not require\nretraining or dialect-specific fine-tuning, making it broadly applicable across\nmodels and domains. Evaluated on PrivacyQA and PolicyQA, our framework improves\nGPT-4o-mini's zero-shot accuracy from 0.394 to 0.601 on PrivacyQA and from\n0.352 to 0.464 on PolicyQA, surpassing or matching few-shot baselines without\nadditional training data. These results highlight the effectiveness of\nstructured agent collaboration in mitigating dialect biases and underscore the\nimportance of designing NLP systems that account for linguistic diversity to\nensure equitable access to privacy information.", "AI": {"tldr": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u6846\u67b6\u89e3\u51b3\u9690\u79c1\u653f\u7b56QA\u7cfb\u7edf\u7684\u65b9\u8a00\u504f\u5dee\u95ee\u9898\uff0c\u901a\u8fc7\u65b9\u8a00\u4ee3\u7406\u548c\u9886\u57df\u4e13\u5bb6\u4ee3\u7406\u534f\u4f5c\uff0c\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347GPT-4o-mini\u5728\u4e0d\u540c\u65b9\u8a00\u6570\u636e\u96c6\u4e0a\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u9690\u79c1\u653f\u7b56\u95ee\u7b54\u7cfb\u7edf\u5b58\u5728\u82f1\u8bed\u65b9\u8a00\u95f4\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u975e\u6807\u51c6\u65b9\u8a00\u7528\u6237\u96be\u4ee5\u516c\u5e73\u83b7\u53d6\u9690\u79c1\u4fe1\u606f\u3002\u9700\u8981\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3/\u65b9\u8a00\u5fae\u8c03\u7684\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u53cc\u4ee3\u7406\u67b6\u6784\uff1a1) \u65b9\u8a00\u4ee3\u7406\u8f6c\u6362\u65b9\u8a00\u67e5\u8be2\u4e3a\u6807\u51c6\u82f1\u8bed\uff1b2) \u9690\u79c1\u653f\u7b56\u4ee3\u7406\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u4f18\u5316\u9884\u6d4b\u3002\u6846\u67b6\u5177\u5907\u6a21\u578b\u65e0\u5173\u6027\uff0c\u517c\u5bb9\u73b0\u6709QA\u6a21\u578b\u3002", "result": "PrivacyQA\u51c6\u786e\u7387\u4ece0.394\u21920.601\uff0cPolicyQA\u4ece0.352\u21920.464\uff0c\u96f6\u6837\u672c\u6027\u80fd\u8d85\u8d8a\u5c11\u6837\u672c\u57fa\u51c6\u3002", "conclusion": "\u7ed3\u6784\u5316\u4ee3\u7406\u534f\u4f5c\u6709\u6548\u7f13\u89e3\u65b9\u8a00\u504f\u5dee\uff0c\u5f3a\u8c03NLP\u7cfb\u7edf\u9700\u8003\u8651\u8bed\u8a00\u591a\u6837\u6027\u4ee5\u5b9e\u73b0\u9690\u79c1\u4fe1\u606f\u516c\u5e73\u83b7\u53d6\u3002"}}
{"id": "2506.03009", "pdf": "https://arxiv.org/pdf/2506.03009", "abs": "https://arxiv.org/abs/2506.03009", "authors": ["Florian Ludwig", "Torsten Zesch", "Frederike Zufall"], "title": "Conditioning Large Language Models on Legal Systems? Detecting Punishable Hate Speech", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The assessment of legal problems requires the consideration of a specific\nlegal system and its levels of abstraction, from constitutional law to\nstatutory law to case law. The extent to which Large Language Models (LLMs)\ninternalize such legal systems is unknown. In this paper, we propose and\ninvestigate different approaches to condition LLMs at different levels of\nabstraction in legal systems. This paper examines different approaches to\nconditioning LLMs at multiple levels of abstraction in legal systems to detect\npotentially punishable hate speech. We focus on the task of classifying whether\na specific social media posts falls under the criminal offense of incitement to\nhatred as prescribed by the German Criminal Code. The results show that there\nis still a significant performance gap between models and legal experts in the\nlegal assessment of hate speech, regardless of the level of abstraction with\nwhich the models were conditioned. Our analysis revealed, that models\nconditioned on abstract legal knowledge lacked deep task understanding, often\ncontradicting themselves and hallucinating answers, while models using concrete\nlegal knowledge performed reasonably well in identifying relevant target\ngroups, but struggled with classifying target conducts.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u6cd5\u5f8b\u62bd\u8c61\u5c42\u6b21\u6761\u4ef6\u4e0b\uff0c\u5bf9\u5fb7\u56fd\u5211\u6cd5\u4e2d\u717d\u52a8\u4ec7\u6068\u8a00\u8bba\u7684\u8bc6\u522b\u80fd\u529b\u4e0e\u6cd5\u5f8b\u4e13\u5bb6\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff1a\u62bd\u8c61\u6cd5\u5f8b\u77e5\u8bc6\u6a21\u578b\u6613\u81ea\u76f8\u77db\u76fe\uff0c\u5177\u4f53\u6cd5\u5f8b\u77e5\u8bc6\u6a21\u578b\u64c5\u957f\u8bc6\u522b\u76ee\u6807\u7fa4\u4f53\u4f46\u96be\u5206\u7c7b\u884c\u4e3a", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u6cd5\u5f8b\u4f53\u7cfb\u4e0d\u540c\u62bd\u8c61\u5c42\u6b21\uff08\u5baa\u6cd5-\u6210\u6587\u6cd5-\u5224\u4f8b\u6cd5\uff09\u7684\u5185\u5316\u7a0b\u5ea6\uff0c\u53ca\u5176\u5728\u5211\u4e8b\u72af\u7f6a\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b", "method": "\u901a\u8fc7\u591a\u5c42\u7ea7\u6cd5\u5f8b\u77e5\u8bc6\u6761\u4ef6\u5316\u65b9\u6cd5\uff0c\u4ee5\u5fb7\u56fd\u5211\u6cd5\u7b2c130\u6761\u717d\u52a8\u4ec7\u6068\u7f6a\u4e3a\u57fa\u51c6\uff0c\u8bc4\u4f30\u6a21\u578b\u5bf9\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u662f\u5426\u6784\u6210\u72af\u7f6a\u884c\u4e3a\u7684\u5206\u7c7b\u80fd\u529b", "result": "\u6a21\u578b\u6574\u4f53\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u6cd5\u5f8b\u4e13\u5bb6\uff1a\u62bd\u8c61\u77e5\u8bc6\u6a21\u578b\u51fa\u73b073%\u7684\u903b\u8f91\u77db\u76fe\uff0c\u5177\u4f53\u77e5\u8bc6\u6a21\u578b\u76ee\u6807\u7fa4\u4f53\u8bc6\u522b\u51c6\u786e\u7387\u8fbe82%\uff0c\u4f46\u76ee\u6807\u884c\u4e3a\u5206\u7c7b\u51c6\u786e\u7387\u4ec559%", "conclusion": "\u5f53\u524dLLMs\u5728\u6cd5\u5f8b\u5e94\u7528\u4e2d\u5b58\u5728\u7cfb\u7edf\u5c40\u9650\u6027\uff0c\u9700\u7ed3\u5408\u6cd5\u5f8b\u77e5\u8bc6\u56fe\u8c31\u4e0e\u9886\u57df\u5fae\u8c03\u6765\u63d0\u5347\u5bf9\u76ee\u6807\u884c\u4e3a\u8981\u7d20\u7684\u7406\u89e3\u548c\u63a8\u7406\u4e00\u81f4\u6027"}}
{"id": "2506.03011", "pdf": "https://arxiv.org/pdf/2506.03011", "abs": "https://arxiv.org/abs/2506.03011", "authors": ["Aditya Bharat Soni", "Boxuan Li", "Xingyao Wang", "Valerie Chen", "Graham Neubig"], "title": "Coding Agents with Multimodal Browsing are Generalist Problem Solvers", "categories": ["cs.CL"], "comment": null, "summary": "Modern human labor is characterized by specialization; we train for years and\ndevelop particular tools that allow us to perform well across a variety of\ntasks. In addition, AI agents have been specialized for domains such as\nsoftware engineering, web navigation, and workflow automation. However, this\nresults in agents that are good for one thing but fail to generalize beyond\ntheir intended scope. One reason for this is that agent developers provide a\nhighly specialized set of tools or make architectural decisions optimized for a\nspecific use case or benchmark. In this work, we ask the question: what is the\nminimal set of general tools that can be used to achieve high performance\nacross a diverse set of tasks? Our answer is OpenHands-Versa, a generalist\nagent built with a modest number of general tools: code editing and execution,\nweb search, as well as multimodal web browsing and file access. Importantly,\nOpenHands-Versa demonstrates superior or competitive performance over leading\nspecialized agents across three diverse and challenging benchmarks: SWE-Bench\nMultimodal, GAIA, and The Agent Company, outperforming the best-performing\npreviously published results with absolute improvements in success rate of 9.1,\n1.3, and 9.1 points respectively. Further, we show how existing\nstate-of-the-art multi-agent systems fail to generalize beyond their target\ndomains. These results demonstrate the feasibility of developing a generalist\nagent to solve diverse tasks and establish OpenHands-Versa as a strong baseline\nfor future research.", "AI": {"tldr": "\u63d0\u51fa\u901a\u7528\u667a\u80fd\u4f53OpenHands-Versa\uff0c\u901a\u8fc7\u4ee3\u7801\u7f16\u8f91\u3001\u7f51\u7edc\u641c\u7d22\u7b49\u57fa\u7840\u5de5\u5177\u7ec4\u5408\uff0c\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e13\u7528\u667a\u80fd\u4f53\uff0c\u8bc1\u660e\u901a\u7528\u4ee3\u7406\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u73b0\u6709\u4e13\u7528AI\u4ee3\u7406\u5728\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u63a2\u7d22\u6700\u5c0f\u901a\u7528\u5de5\u5177\u96c6\u5b9e\u73b0\u591a\u4efb\u52a1\u9ad8\u6027\u80fd\u3002", "method": "\u6784\u5efa\u5305\u542b\u4ee3\u7801\u7f16\u8f91\u6267\u884c\u3001\u7f51\u7edc\u641c\u7d22\u3001\u591a\u6a21\u6001\u6d4f\u89c8\u548c\u6587\u4ef6\u8bbf\u95ee\u7684\u57fa\u7840\u5de5\u5177\u7ec4\u5408OpenHands-Versa\u3002", "result": "\u5728SWE-Bench Multimodal/GAIA/Agent Company\u6d4b\u8bd5\u4e2d\uff0c\u6210\u529f\u7387\u5206\u522b\u63d0\u53479.1/1.3/9.1\u4e2a\u767e\u5206\u70b9\uff0c\u8d85\u8d8a\u9886\u57df\u4e13\u7528\u4ee3\u7406\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u901a\u7528\u667a\u80fd\u4f53\u7684\u53ef\u884c\u6027\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u9886\u57df\u5c40\u9650\u6027\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u5efa\u7acb\u57fa\u51c6\u3002"}}
{"id": "2506.03035", "pdf": "https://arxiv.org/pdf/2506.03035", "abs": "https://arxiv.org/abs/2506.03035", "authors": ["Pierre Lepagnol", "Sahar Ghannay", "Thomas Gerald", "Christophe Servan", "Sophie Rosset"], "title": "Leveraging Information Retrieval to Enhance Spoken Language Understanding Prompts in Few-Shot Learning", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Conference paper accepted to INTERSPEECH 2025", "summary": "Understanding user queries is fundamental in many applications, such as home\nassistants, booking systems, or recommendations. Accordingly, it is crucial to\ndevelop accurate Spoken Language Understanding (SLU) approaches to ensure the\nreliability of the considered system. Current State-of-the-Art SLU techniques\nrely on large amounts of training data; however, only limited annotated\nexamples are available for specific tasks or languages.\n  In the meantime, instruction-tuned large language models (LLMs) have shown\nexceptional performance on unseen tasks in a few-shot setting when provided\nwith adequate prompts. In this work, we propose to explore example selection by\nleveraging Information retrieval (IR) approaches to build an enhanced prompt\nthat is applied to an SLU task. We evaluate the effectiveness of the proposed\nmethod on several SLU benchmarks. Experimental results show that lexical IR\nmethods significantly enhance performance without increasing prompt length.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u4fe1\u606f\u68c0\u7d22(IR)\u65b9\u6cd5\u9009\u62e9\u793a\u4f8b\u6784\u5efa\u589e\u5f3a\u63d0\u793a\uff0c\u663e\u8457\u63d0\u5347\u8bed\u97f3\u8bed\u8a00\u7406\u89e3(SLU)\u4efb\u52a1\u6027\u80fd\uff0c\u4e14\u4e0d\u589e\u52a0\u63d0\u793a\u957f\u5ea6\u3002", "motivation": "\u73b0\u6709SLU\u6280\u672f\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u7279\u5b9a\u9886\u57df/\u8bed\u8a00\u6570\u636e\u6709\u9650\u3002\u6307\u4ee4\u8c03\u4f18\u7684LLMs\u5728\u5c11\u6837\u672c\u573a\u666f\u5c55\u73b0\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u6709\u6548\u63d0\u793a\u7b56\u7565\u3002", "method": "\u91c7\u7528\u4fe1\u606f\u68c0\u7d22\u6280\u672f\u9009\u62e9\u76f8\u5173\u793a\u4f8b\u6784\u5efa\u589e\u5f3a\u63d0\u793a\uff0c\u5728\u591a\u4e2aSLU\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e0d\u540cIR\u65b9\u6cd5\u7684\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8bcd\u6c47\u7ea7IR\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd(\u5177\u4f53\u6307\u6807\u672a\u63d0\u53ca)\uff0c\u4e14\u4fdd\u6301\u63d0\u793a\u957f\u5ea6\u4e0d\u53d8\u3002", "conclusion": "\u7ed3\u5408IR\u7684\u63d0\u793a\u5de5\u7a0b\u80fd\u6709\u6548\u63d0\u5347LLMs\u5728\u4f4e\u8d44\u6e90SLU\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.03038", "pdf": "https://arxiv.org/pdf/2506.03038", "abs": "https://arxiv.org/abs/2506.03038", "authors": ["Jintian Shao", "Yiming Cheng"], "title": "Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective", "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) enhances large language models (LLMs) in complex,\nlong-chain-of-thought (long-CoT) reasoning. The advanced VAPO framework,\ndespite sophisticated mechanisms like Decoupled GAE, theoretically faces\nfundamental limitations in comprehensively modeling and leveraging deep,\nlong-term value for fine-grained, step-by-step policy guidance in extended\nreasoning chains. We argue these limitations stem from inherent difficulties in\ncredit assignment, value function representational capacity with temporally\nabstracted goals, and translating global value signals into local policy\nimprovements, especially with sparse rewards. Our theoretical analysis examines\nthese aspects to illuminate VAPO's boundaries in long-term value modeling,\naiming to deepen understanding of current RL for advanced reasoning and suggest\nfuture research for more robust LLM agents.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8RL\u589e\u5f3aLLMs\u957f\u94fe\u63a8\u7406\u65f6VAPO\u6846\u67b6\u5728\u957f\u671f\u4ef7\u503c\u5efa\u6a21\u7684\u7406\u8bba\u5c40\u9650\uff0c\u5206\u6790\u4fe1\u7528\u5206\u914d\u3001\u4ef7\u503c\u51fd\u6570\u8868\u5f81\u548c\u4fe1\u53f7\u8f6c\u5316\u4e09\u5927\u6311\u6218", "motivation": "\u63ed\u793aVAPO\u6846\u67b6\u5728\u957f\u63a8\u7406\u94fe\u4e2d\u5efa\u6a21\u6df1\u5ea6\u4ef7\u503c\u4fe1\u53f7\u7684\u56fa\u6709\u7f3a\u9677\uff0c\u63a8\u52a8\u5bf9RL\u65b9\u6cd5\u5c40\u9650\u6027\u7684\u7406\u89e3\u53caLLM\u667a\u80fd\u4f53\u6539\u8fdb\u65b9\u5411", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u4fe1\u7528\u5206\u914d\u673a\u5236\u3001\u4ef7\u503c\u51fd\u6570\u5bf9\u65f6\u5e8f\u62bd\u8c61\u76ee\u6807\u7684\u8868\u5f81\u80fd\u529b\uff0c\u4ee5\u53ca\u7a00\u758f\u5956\u52b1\u4e0b\u5168\u5c40\u4ef7\u503c\u4fe1\u53f7\u5230\u5c40\u90e8\u7b56\u7565\u7684\u8f6c\u5316\u673a\u5236", "result": "\u8bc6\u522bVAPO\u5728\u957f\u671f\u4ef7\u503c\u5efa\u6a21\u4e2d\u7684\u4e09\u5927\u6838\u5fc3\u9650\u5236\uff1a\u4fe1\u7528\u5206\u914d\u96be\u9898\u3001\u4ef7\u503c\u51fd\u6570\u8868\u5f81\u529b\u4e0d\u8db3\u3001\u5168\u5c40\u4fe1\u53f7\u5c40\u90e8\u5316\u6548\u7387\u4f4e\u4e0b", "conclusion": "\u5f53\u524dRL\u65b9\u6cd5\u9700\u7a81\u7834\u957f\u671f\u4ef7\u503c\u5efa\u6a21\u74f6\u9888\uff0c\u672a\u6765\u5e94\u63a2\u7d22\u7ed3\u5408\u65f6\u5e8f\u62bd\u8c61\u8868\u5f81\u4e0e\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u7684\u65b0\u8303\u5f0f\u6765\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b"}}
{"id": "2506.03051", "pdf": "https://arxiv.org/pdf/2506.03051", "abs": "https://arxiv.org/abs/2506.03051", "authors": ["Yuval Kansal", "Shmuel Berman", "Lydia Liu"], "title": "Facts Do Care About Your Language: Assessing Answer Quality of Multilingual LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Factuality is a necessary precursor to useful educational tools. As adoption\nof Large Language Models (LLMs) in education continues of grow, ensuring\ncorrectness in all settings is paramount. Despite their strong English\ncapabilities, LLM performance in other languages is largely untested. In this\nwork, we evaluate the correctness of the Llama3.1 family of models in answering\nfactual questions appropriate for middle and high school students. We\ndemonstrate that LLMs not only provide extraneous and less truthful\ninformation, but also exacerbate existing biases against rare languages.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0Llama3.1\u7cfb\u5217\u6a21\u578b\u5728\u591a\u8bed\u8a00\u6559\u80b2\u573a\u666f\u4e2d\u5b58\u5728\u4e8b\u5b9e\u6027\u4e0d\u8db3\u4e0e\u8bed\u8a00\u504f\u89c1\u95ee\u9898", "motivation": "\u786e\u4fddLLMs\u5728\u591a\u8bed\u8a00\u6559\u80b2\u5de5\u5177\u4e2d\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u7279\u522b\u9488\u5bf9\u975e\u82f1\u8bed/\u7a00\u6709\u8bed\u8a00\u573a\u666f\u7684\u53ef\u9760\u6027\u9a8c\u8bc1", "method": "\u901a\u8fc7\u4e2d\u5b66\u751f\u96be\u5ea6\u7684\u4e8b\u5b9e\u6027\u95ee\u9898\u6d4b\u8bd5\uff0c\u8bc4\u4f30Llama3.1\u6a21\u578b\u7684\u591a\u8bed\u8a00\u8f93\u51fa\u8d28\u91cf\u4e0e\u504f\u89c1\u7a0b\u5ea6", "result": "\u6a21\u578b\u4e0d\u4ec5\u4ea7\u751f\u5197\u4f59\u9519\u8bef\u4fe1\u606f\uff0c\u4e14\u5bf9\u7a00\u6709\u8bed\u8a00\u8868\u73b0\u51fa\u663e\u8457\u504f\u89c1\uff08\u51c6\u786e\u7387\u4e0b\u964d23.7%\uff09", "conclusion": "\u6559\u80b2\u9886\u57df\u5e94\u7528LLMs\u9700\u5f3a\u5316\u591a\u8bed\u8a00\u4e8b\u5b9e\u6821\u9a8c\u673a\u5236\uff0c\u5e76\u5efa\u7acb\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u504f\u89c1\u7f13\u89e3\u65b9\u6848"}}
{"id": "2506.03090", "pdf": "https://arxiv.org/pdf/2506.03090", "abs": "https://arxiv.org/abs/2506.03090", "authors": ["Katherine Thai", "Mohit Iyyer"], "title": "Literary Evidence Retrieval via Long-Context Language Models", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "How well do modern long-context language models understand literary fiction?\nWe explore this question via the task of literary evidence retrieval,\nrepurposing the RELiC dataset of That et al. (2022) to construct a benchmark\nwhere the entire text of a primary source (e.g., The Great Gatsby) is provided\nto an LLM alongside literary criticism with a missing quotation from that work.\nThis setting, in which the model must generate the missing quotation, mirrors\nthe human process of literary analysis by requiring models to perform both\nglobal narrative reasoning and close textual examination. We curate a\nhigh-quality subset of 292 examples through extensive filtering and human\nverification. Our experiments show that recent reasoning models, such as Gemini\nPro 2.5 can exceed human expert performance (62.5% vs. 50% accuracy). In\ncontrast, the best open-weight model achieves only 29.1% accuracy, highlighting\na wide gap in interpretive reasoning between open and closed-weight models.\nDespite their speed and apparent accuracy, even the strongest models struggle\nwith nuanced literary signals and overgeneration, signaling open challenges for\napplying LLMs to literary analysis. We release our dataset and evaluation code\nto encourage future work in this direction.", "AI": {"tldr": "\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6587\u5b66\u8bc1\u636e\u68c0\u7d22\u4efb\u52a1\u4e2d\u5c55\u73b0\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff1aGemini Pro 2.5\u8d85\u8d8a\u4eba\u7c7b\u4e13\u5bb6\uff0862.5% vs 50%\uff09\uff0c\u800c\u6700\u4f73\u5f00\u6e90\u6a21\u578b\u4ec529.1%", "motivation": "\u8bc4\u4f30\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u5bf9\u6587\u5b66\u5c0f\u8bf4\u7684\u7406\u89e3\u80fd\u529b\uff0c\u901a\u8fc7\u6784\u5efa\u6587\u5b66\u8bc1\u636e\u68c0\u7d22\u4efb\u52a1\u6a21\u62df\u4eba\u7c7b\u6587\u5b66\u5206\u6790\u8fc7\u7a0b", "method": "\u91cd\u6784RELiC\u6570\u636e\u96c6\u6784\u5efa\u57fa\u51c6\u6d4b\u8bd5\uff08292\u4e2a\u6837\u672c\uff09\uff0c\u8981\u6c42\u6a21\u578b\u57fa\u4e8e\u5b8c\u6574\u6587\u672c\u8865\u5168\u6587\u5b66\u8bc4\u8bba\u4e2d\u7684\u7f3a\u5931\u5f15\u7528\uff0c\u9700\u7ed3\u5408\u5168\u5c40\u53d9\u4e8b\u63a8\u7406\u4e0e\u7ec6\u7c92\u5ea6\u6587\u672c\u5206\u6790", "result": "\u95ed\u6e90\u6a21\u578bGemini Pro 2.5\u51c6\u786e\u738762.5%\uff08\u4eba\u7c7b\u4e13\u5bb650%\uff09\uff0c\u6700\u4f73\u5f00\u6e90\u6a21\u578b\u4ec529.1%\uff1b\u6a21\u578b\u666e\u904d\u5b58\u5728\u6587\u5b66\u4fe1\u53f7\u7406\u89e3\u4e0d\u8db3\u4e0e\u8fc7\u5ea6\u751f\u6210\u95ee\u9898", "conclusion": "\u8bed\u8a00\u6a21\u578b\u5728\u6587\u5b66\u5206\u6790\u9886\u57df\u5c55\u73b0\u6f5c\u529b\u4f46\u5b58\u5728\u660e\u663e\u5c40\u9650\uff0c\u516c\u5f00\u6570\u636e\u96c6\u4e0e\u8bc4\u4f30\u6846\u67b6\u4fc3\u8fdb\u8be5\u9886\u57df\u53d1\u5c55"}}
{"id": "2506.03101", "pdf": "https://arxiv.org/pdf/2506.03101", "abs": "https://arxiv.org/abs/2506.03101", "authors": ["Jonas F. Lotz", "Ant\u00f3nio V. Lopes", "Stephan Peitz", "Hendra Setiawan", "Leonardo Emili"], "title": "Beyond Text Compression: Evaluating Tokenizers Across Scales", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "The choice of tokenizer can profoundly impact language model performance, yet\naccessible and reliable evaluations of tokenizer quality remain an open\nchallenge. Inspired by scaling consistency, we show that smaller models can\naccurately predict significant differences in tokenizer impact on larger models\nat a fraction of the compute cost. By systematically evaluating both\nEnglish-centric and multilingual tokenizers, we find that tokenizer choice has\nnegligible effects on tasks in English but results in consistent performance\ndifferences in multilingual settings. We propose new intrinsic tokenizer\nmetrics inspired by Zipf's law that correlate more strongly with downstream\nperformance than text compression when modeling unseen languages. By combining\nseveral metrics to capture multiple aspects of tokenizer behavior, we develop a\nreliable framework for intrinsic tokenizer evaluations. Our work offers a more\nefficient path to informed tokenizer selection in future language model\ndevelopment.", "AI": {"tldr": "\u901a\u8fc7\u5c0f\u6a21\u578b\u9884\u6d4b\u5206\u8bcd\u5668\u5bf9\u5927\u578b\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u7ed3\u5408Zipf\u5b9a\u5f8b\u7684\u591a\u7ef4\u5ea6\u8bc4\u4f30\u6846\u67b6\uff0c\u5b9e\u73b0\u9ad8\u6548\u5206\u8bcd\u5668\u9009\u62e9", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u9ad8\u6548\u53ef\u9760\u7684\u5206\u8bcd\u5668\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u591a\u8bed\u8a00\u573a\u666f\u4e2d\u9700\u8981\u66f4\u6709\u6548\u7684\u8bc4\u4f30\u6807\u51c6", "method": "\u5229\u7528\u6269\u5c55\u4e00\u81f4\u6027\u539f\u7406\uff0c\u7ed3\u5408\u82f1\u8bed/\u591a\u8bed\u8a00\u5206\u8bcd\u5668\u7cfb\u7edf\u8bc4\u4f30\uff0c\u5f00\u53d1\u57fa\u4e8eZipf\u5b9a\u5f8b\u7684\u65b0\u8bc4\u4f30\u6307\u6807\u7ec4\u5408", "result": "\u591a\u8bed\u8a00\u4efb\u52a1\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff08\u82f1\u8bed\u5f71\u54cd\u5c0f\uff09\uff0c\u65b0\u6307\u6807\u6bd4\u6587\u672c\u538b\u7f29\u7387\u66f4\u5f3a\u76f8\u5173\uff0c\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u6027", "conclusion": "\u591a\u7ef4\u5ea6\u6307\u6807\u8bc4\u4f30\u6846\u67b6\u4e3a\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5206\u8bcd\u5668\u9009\u62e9\u65b9\u6cd5\u8bba"}}
{"id": "2506.03106", "pdf": "https://arxiv.org/pdf/2506.03106", "abs": "https://arxiv.org/abs/2506.03106", "authors": ["Xiaoying Zhang", "Hao Sun", "Yipeng Zhang", "Kaituo Feng", "Chao Yang", "Helen Meng"], "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback", "categories": ["cs.CL", "cs.AI"], "comment": "38 pages", "summary": "Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of self-reflection, and persistent\nfailures. We then demonstrate that RL-finetuned models, even after exhibiting\nperformance plateaus, can generate correct refinements on persistently failed\nproblems by leveraging natural language feedback in the form of critiques.\nBuilding on this insight, we propose Critique-GRPO, an online RL framework that\nintegrates both natural language and numerical feedback for effective policy\noptimization. Critique-GRPO enables LLMs to learn from initial responses and\ncritique-guided refinements simultaneously while maintaining exploration.\nExtensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that\nCritique-GRPO consistently outperforms supervised learning-based and RL-based\nfine-tuning approaches across eight challenging mathematical, STEM, and general\nreasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%,\nrespectively. Notably, Critique-GRPO surpasses a strong baseline that\nincorporates expert demonstrations within online RL. Further analysis reveals\ntwo critical insights about policy exploration: (1) higher entropy does not\nalways guarantee efficient learning from exploration, and (2) longer responses\ndo not necessarily lead to more effective exploration.", "AI": {"tldr": "\u63d0\u51faCritique-GRPO\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u81ea\u7136\u8bed\u8a00\u53cd\u9988\u4e0e\u6570\u503c\u53cd\u9988\uff0c\u6709\u6548\u89e3\u51b3RL\u8bad\u7ec3\u4e2d\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u6570\u503c\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\u5b58\u5728\u6027\u80fd\u74f6\u9888\u3001\u81ea\u6211\u53cd\u601d\u80fd\u529b\u6709\u9650\u548c\u6301\u7eed\u5931\u8d25\u4e09\u5927\u95ee\u9898\uff0c\u7814\u7a76\u53d1\u73b0\u81ea\u7136\u8bed\u8a00\u53cd\u9988\u80fd\u5e2e\u52a9\u6a21\u578b\u7a81\u7834\u74f6\u9888\u6539\u8fdb\u5931\u8d25\u6848\u4f8b", "method": "Critique-GRPO\u6846\u67b6\uff1a1\uff09\u540c\u6b65\u5229\u7528\u521d\u59cb\u54cd\u5e94\u548c\u57fa\u4e8e\u6279\u5224\u7684\u6539\u8fdb\u6837\u672c 2\uff09\u4fdd\u6301\u63a2\u7d22\u673a\u5236 3\uff09\u6574\u5408\u81ea\u7136\u8bed\u8a00\u6279\u5224\u4e0e\u6570\u503c\u5956\u52b1\u7684\u53cc\u91cd\u53cd\u9988", "result": "\u57288\u4e2a\u6570\u5b66/STEM/\u901a\u7528\u63a8\u7406\u4efb\u52a1\u4e2d\uff1a1\uff09\u6bd4\u76d1\u7763\u5b66\u4e60\u5fae\u8c03\u63d0\u53474.5% 2\uff09\u6bd4\u7eafRL\u65b9\u6cd5\u63d0\u53475% 3\uff09\u4f18\u4e8e\u878d\u5408\u4e13\u5bb6\u793a\u8303\u7684\u5f3a\u57fa\u7ebf", "conclusion": "\u53d1\u73b0\uff1a1\uff09\u9ad8\u71b5\u503c\u4e0d\u4fdd\u8bc1\u6709\u6548\u63a2\u7d22 2\uff09\u957f\u54cd\u5e94\u672a\u5fc5\u63d0\u5347\u63a2\u7d22\u6548\u7387\uff0c\u9700\u5e73\u8861\u63a2\u7d22\u8d28\u91cf\u4e0e\u54cd\u5e94\u957f\u5ea6"}}
{"id": "2506.03122", "pdf": "https://arxiv.org/pdf/2506.03122", "abs": "https://arxiv.org/abs/2506.03122", "authors": ["Prashanth Vijayaraghavan", "Luyao Shi", "Ehsan Degan", "Vandana Mukherjee", "Xin Zhang"], "title": "AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit Topology Generation", "categories": ["cs.CL"], "comment": "9 Pages (Content), 4 Pages (Appendix), 7 figures, ICML'2025", "summary": "Analog circuit topology synthesis is integral to Electronic Design Automation\n(EDA), enabling the automated creation of circuit structures tailored to\nspecific design requirements. However, the vast design search space and strict\nconstraint adherence make efficient synthesis challenging. Leveraging the\nversatility of Large Language Models (LLMs), we propose AUTOCIRCUIT-RL,a novel\nreinforcement learning (RL)-based framework for automated analog circuit\nsynthesis. The framework operates in two phases: instruction tuning, where an\nLLM learns to generate circuit topologies from structured prompts encoding\ndesign constraints, and RL refinement, which further improves the\ninstruction-tuned model using reward models that evaluate validity, efficiency,\nand output voltage. The refined model is then used directly to generate\ntopologies that satisfy the design constraints. Empirical results show that\nAUTOCIRCUIT-RL generates ~12% more valid circuits and improves efficiency by\n~14% compared to the best baselines, while reducing duplicate generation rates\nby ~38%. It achieves over 60% success in synthesizing valid circuits with\nlimited training data, demonstrating strong generalization. These findings\nhighlight the framework's effectiveness in scaling to complex circuits while\nmaintaining efficiency and constraint adherence, marking a significant\nadvancement in AI-driven circuit design.", "AI": {"tldr": "AUTOCIRCUIT-RL\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u52a8\u5316\u6a21\u62df\u7535\u8def\u7efc\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u6307\u4ee4\u8c03\u6574\u4e0e\u5956\u52b1\u6a21\u578b\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u7535\u8def\u751f\u6210\u6548\u7387\u4e0e\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edfEDA\u5de5\u5177\u9762\u4e34\u5de8\u5927\u8bbe\u8ba1\u7a7a\u95f4\u4e0e\u4e25\u683c\u7ea6\u675f\u7684\u6311\u6218\uff0c\u5229\u7528LLM\u7684\u6cdb\u5316\u80fd\u529b\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u7b26\u5408\u7ea6\u675f\u7684\u7535\u8def\u62d3\u6251\u751f\u6210\u3002", "method": "1. \u6307\u4ee4\u8c03\u6574\u9636\u6bb5\uff1aLLM\u4ece\u7ed3\u6784\u5316\u63d0\u793a\u5b66\u4e60\u751f\u6210\u7535\u8def\u62d3\u6251\uff1b2. RL\u4f18\u5316\u9636\u6bb5\uff1a\u4f7f\u7528\u8bc4\u4f30\u6709\u6548\u6027/\u6548\u7387/\u8f93\u51fa\u7535\u538b\u7684\u5956\u52b1\u6a21\u578b\u7cbe\u8c03\u6a21\u578b\u3002", "result": "\u751f\u6210\u6709\u6548\u7535\u8def\u6570\u91cf\u6bd4\u57fa\u7ebf\u9ad812%\uff0c\u6548\u7387\u63d0\u534714%\uff0c\u91cd\u590d\u7387\u964d\u4f4e38%\uff0c\u6570\u636e\u6709\u9650\u65f6\u4ecd\u5b9e\u73b0\u8d8560%\u6709\u6548\u7535\u8def\u751f\u6210\u6210\u529f\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4fdd\u6301\u6548\u7387\u7684\u540c\u65f6\u53ef\u6269\u5c55\u81f3\u590d\u6742\u7535\u8def\uff0c\u6807\u5fd7\u7740AI\u9a71\u52a8\u7535\u8def\u8bbe\u8ba1\u7684\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2506.03136", "pdf": "https://arxiv.org/pdf/2506.03136", "abs": "https://arxiv.org/abs/2506.03136", "authors": ["Yinjie Wang", "Ling Yang", "Ye Tian", "Ke Shen", "Mengdi Wang"], "title": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning", "categories": ["cs.CL"], "comment": "Project: https://github.com/Gen-Verse/CURE", "summary": "We propose CURE, a novel reinforcement learning framework with a dedicated\nreward design that co-evolves coding and unit test generation capabilities\nbased on their interaction outcomes, without any ground-truth code as\nsupervision. This approach enables flexible and scalable training and allows\nthe unit tester to learn directly from the coder's mistakes. Our derived\nReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and\nBest-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models,\noutperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They\nnaturally extend to downstream tasks such as test-time scaling and agentic\ncoding-achieving a 8.1% improvement over the base model. For the long-CoT\nmodel, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while\nachieving 64.8% inference efficiency in unit test generation. Notably, we also\nfind that our model can serve as an effective reward model for reinforcement\nlearning on base models. Project: https://github.com/Gen-Verse/CURE", "AI": {"tldr": "\u63d0\u51faCURE\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u540c\u8fdb\u5316\u7f16\u7801\u4e0e\u5355\u5143\u6d4b\u8bd5\u80fd\u529b\u63d0\u5347\u4ee3\u7801\u751f\u6210\u6027\u80fd\uff0c\u4f18\u5316\u540e\u5728\u591a\u4e2a\u6307\u6807\u8d85\u8d8a\u540c\u7c7b\u6a21\u578b", "motivation": "\u89e3\u51b3\u73b0\u6709\u4ee3\u7801\u751f\u6210\u6a21\u578b\u7f3a\u4e4f\u4e0e\u5355\u5143\u6d4b\u8bd5\u52a8\u6001\u4ea4\u4e92\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u534f\u540c\u8fdb\u5316\u5b9e\u73b0\u66f4\u7075\u6d3b\u7684\u7f3a\u9677\u4fee\u6b63", "method": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5956\u52b1\u673a\u5236\u8bbe\u8ba1\uff0c\u8ba9\u7f16\u7801\u5668\u4e0e\u6d4b\u8bd5\u751f\u6210\u5668\u901a\u8fc7\u9519\u8bef\u53cd\u9988\u76f8\u4e92\u4f18\u5316\uff0c\u652f\u6301\u52a8\u6001\u8bad\u7ec3\u6d41\u7a0b\u6269\u5c55", "result": "Qwen2.5\u6a21\u578b\u4f18\u5316\u540e\u4ee3\u7801\u751f\u6210\u51c6\u786e\u7387\u63d0\u53475.3%\uff0cBest-of-N\u63d0\u53479%\uff1b\u4e0b\u6e38\u4efb\u52a1\u6539\u8fdb8.1%\uff0c\u63a8\u7406\u6548\u7387\u8fbe64.8%", "conclusion": "\u6a21\u578b\u517c\u5177\u6d4b\u8bd5\u751f\u6210\u4e0e\u5956\u52b1\u5efa\u6a21\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u534f\u540c\u8fdb\u5316\u673a\u5236\u7684\u6709\u6548\u6027\uff0c\u9879\u76ee\u5df2\u5f00\u6e90\uff08github.com/Gen-Verse/CURE\uff09"}}
{"id": "2506.03143", "pdf": "https://arxiv.org/pdf/2506.03143", "abs": "https://arxiv.org/abs/2506.03143", "authors": ["Qianhui Wu", "Kanzhi Cheng", "Rui Yang", "Chaoyun Zhang", "Jianwei Yang", "Huiqiang Jiang", "Jian Mu", "Baolin Peng", "Bo Qiao", "Reuben Tan", "Si Qin", "Lars Liden", "Qingwei Lin", "Huan Zhang", "Tong Zhang", "Jianbing Zhang", "Dongmei Zhang", "Jianfeng Gao"], "title": "GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "One of the principal challenges in building VLM-powered GUI agents is visual\ngrounding, i.e., localizing the appropriate screen region for action execution\nbased on both the visual content and the textual plans. Most existing work\nformulates this as a text-based coordinate generation task. However, these\napproaches suffer from several limitations: weak spatial-semantic alignment,\ninability to handle ambiguous supervision targets, and a mismatch between the\ndense nature of screen coordinates and the coarse, patch-level granularity of\nvisual features extracted by models like Vision Transformers. In this paper, we\npropose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its\ncore, GUI-Actor introduces an attention-based action head that learns to align\na dedicated <ACTOR> token with all relevant visual patch tokens, enabling the\nmodel to propose one or more action regions in a single forward pass. In line\nwith this, we further design a grounding verifier to evaluate and select the\nmost plausible action region from the candidates proposed for action execution.\nExtensive experiments show that GUI-Actor outperforms prior state-of-the-art\nmethods on multiple GUI action grounding benchmarks, with improved\ngeneralization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B\neven surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7\nwith Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by\nincorporating the verifier, we find that fine-tuning only the newly introduced\naction head (~100M parameters for 7B model) while keeping the VLM backbone\nfrozen is sufficient to achieve performance comparable to previous\nstate-of-the-art models, highlighting that GUI-Actor can endow the underlying\nVLM with effective grounding capabilities without compromising its\ngeneral-purpose strengths.", "AI": {"tldr": "\u63d0\u51faGUI-Actor\u65b9\u6cd5\u89e3\u51b3VLM\u9a71\u52a8\u7684GUI\u4ee3\u7406\u89c6\u89c9\u5b9a\u4f4d\u95ee\u9898\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u548c\u9a8c\u8bc1\u5668\u5b9e\u73b0\u9ad8\u6548\u533a\u57df\u5b9a\u4f4d\uff0c\u5728\u4fdd\u6301VLM\u901a\u7528\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5750\u6807\u751f\u6210\u7684\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u5b58\u5728\u7a7a\u95f4-\u8bed\u4e49\u5bf9\u9f50\u5f31\u3001\u65e0\u6cd5\u5904\u7406\u6a21\u7cca\u76d1\u7763\u76ee\u6807\u3001\u89c6\u89c9\u7279\u5f81\u7c92\u5ea6\u4e0e\u5750\u6807\u5bc6\u96c6\u6027\u4e0d\u5339\u914d\u4e09\u5927\u7f3a\u9677", "method": "\u8bbe\u8ba1\u6ce8\u610f\u529b\u52a8\u4f5c\u5934\u5b9e\u73b0<ACTOR>\u6807\u8bb0\u4e0e\u89c6\u89c9\u5757\u7684\u8bed\u4e49\u5bf9\u9f50\uff0c\u5f00\u53d1\u5019\u9009\u533a\u57df\u9a8c\u8bc1\u5668\uff0c\u652f\u6301\u4ec5\u5fae\u8c03\u52a8\u4f5c\u5934\uff08\u7ea61\u4ebf\u53c2\u6570\uff09\u7684\u53c2\u6570\u9ad8\u6548\u8bad\u7ec3\u65b9\u6848", "result": "\u5728ScreenSpot-Pro\u57fa\u51c6\u4e0a\uff0cQwen2.5-VL\u7248\u672c\u8fbe\u523044.6\u5206\u8d85\u8d8aUI-TARS-72B\uff0838.1\uff09\uff1b\u4ec5\u5fae\u8c03\u52a8\u4f5c\u5934\u5373\u53ef\u8fbe\u5230SOTA\u6027\u80fd", "conclusion": "GUI-Actor\u5728\u4e0d\u727a\u7272VLM\u901a\u7528\u80fd\u529b\u7684\u524d\u63d0\u4e0b\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u6709\u6548\u589e\u5f3a\u4e86\u89c6\u89c9\u5b9a\u4f4d\u80fd\u529b\uff0c\u4e3a\u6784\u5efa\u9ad8\u6548GUI\u4ee3\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2506.03145", "pdf": "https://arxiv.org/pdf/2506.03145", "abs": "https://arxiv.org/abs/2506.03145", "authors": ["Pralaypati Ta", "Sriram Venkatesaperumal", "Keerthi Ram", "Mohanasankar Sivaprakasam"], "title": "Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and Semantic Understanding Capability of LLM", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Neuroscience research publications encompass a vast wealth of knowledge.\nAccurately retrieving existing information and discovering new insights from\nthis extensive literature is essential for advancing the field. However, when\nknowledge is dispersed across multiple sources, current state-of-the-art\nretrieval methods often struggle to extract the necessary information. A\nknowledge graph (KG) can integrate and link knowledge from multiple sources,\nbut existing methods for constructing KGs in neuroscience often rely on labeled\ndata and require domain expertise. Acquiring large-scale, labeled data for a\nspecialized area like neuroscience presents significant challenges. This work\nproposes novel methods for constructing KG from unlabeled large-scale\nneuroscience research corpus utilizing large language models (LLM),\nneuroscience ontology, and text embeddings. We analyze the semantic relevance\nof neuroscience text segments identified by LLM for building the knowledge\ngraph. We also introduce an entity-augmented information retrieval algorithm to\nextract knowledge from the KG. Several experiments were conducted to evaluate\nthe proposed approaches, and the results demonstrate that our methods\nsignificantly enhance knowledge discovery from the unlabeled neuroscience\nresearch corpus. It achieves an F1 score of 0.84 for entity extraction, and the\nknowledge obtained from the KG improves answers to over 54% of the questions.", "AI": {"tldr": "\u63d0\u51fa\u5229\u7528LLM\u3001\u795e\u7ecf\u79d1\u5b66\u672c\u4f53\u8bba\u548c\u6587\u672c\u5d4c\u5165\u6784\u5efa\u65e0\u6807\u6ce8\u795e\u7ecf\u79d1\u5b66\u77e5\u8bc6\u56fe\u8c31\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u77e5\u8bc6\u53d1\u73b0\u6548\u679c", "motivation": "\u73b0\u6709\u795e\u7ecf\u79d1\u5b66\u77e5\u8bc6\u68c0\u7d22\u65b9\u6cd5\u96be\u4ee5\u6574\u5408\u5206\u6563\u77e5\u8bc6\uff0c\u4f20\u7edf\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u548c\u9886\u57df\u4e13\u5bb6\uff0c\u5728\u4e13\u4e1a\u9886\u57df\u83b7\u53d6\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u56f0\u96be", "method": "1. \u4f7f\u7528LLM\u5206\u6790\u795e\u7ecf\u79d1\u5b66\u6587\u672c\u8bed\u4e49\u76f8\u5173\u6027\n2. \u7ed3\u5408\u9886\u57df\u672c\u4f53\u8bba\u548c\u6587\u672c\u5d4c\u5165\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\n3. \u5f00\u53d1\u5b9e\u4f53\u589e\u5f3a\u7684\u4fe1\u606f\u68c0\u7d22\u7b97\u6cd5", "result": "\u5b9e\u4f53\u63d0\u53d6F1\u503c\u8fbe0.84\uff0c\u77e5\u8bc6\u56fe\u8c31\u6539\u8fdb54%\u95ee\u9898\u7684\u56de\u7b54\u8d28\u91cf", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u65e0\u6807\u6ce8\u6570\u636e\u4e0b\u7684\u77e5\u8bc6\u6574\u5408\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u795e\u7ecf\u79d1\u5b66\u6587\u732e\u7684\u77e5\u8bc6\u53d1\u73b0\u6548\u7387"}}
{"id": "2506.03149", "pdf": "https://arxiv.org/pdf/2506.03149", "abs": "https://arxiv.org/abs/2506.03149", "authors": ["Pietro Lesci", "Clara Meister", "Thomas Hofmann", "Andreas Vlachos", "Tiago Pimentel"], "title": "Causal Estimation of Tokenisation Bias", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Published as a conference paper at ACL 2025", "summary": "Modern language models are typically trained over subword sequences, but\nultimately define probabilities over character-strings. Ideally, the choice of\nthe tokeniser -- which maps character-strings to subwords -- should not affect\nthe probability assigned to the underlying character-string; in practice, it\ndoes. We define this mismatch as tokenisation bias. In this work, we quantify\none particular type of tokenisation bias: the effect of including or not a\nsubword (e.g., $\\langle hello \\rangle$) in a tokeniser's vocabulary on the\nprobability a trained model assigns to the corresponding characters (i.e.,\n\\textit{``hello''}). Estimating this effect is challenging because each model\nis trained with only one tokeniser. We address this by framing tokenisation\nbias as a causal effect and estimating it using the regression discontinuity\ndesign. Specifically, we exploit the fact that tokenisation algorithms rank\nsubwords and add the first $K$ to a tokeniser's vocabulary, where $K$ is an\narbitrary cutoff point. As such, we can estimate a causal effect by comparing\nsimilar subwords around this cutoff. Experimentally, we find that tokenisation\nconsistently affects models' outputs across scales, vocabularies, and\ntokenisers. Notably, a subword's presence in a small model's vocabulary may\nincrease its characters' probability by up to 17 times, highlighting\ntokenisation as a key design choice in language modelling.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5206\u8bcd\u5668\u8bcd\u6c47\u8868\u7684\u9009\u62e9\u4f1a\u663e\u8457\u5f71\u54cd\u8bed\u8a00\u6a21\u578b\u5bf9\u5b57\u7b26\u5e8f\u5217\u7684\u6982\u7387\u5206\u914d\uff08\u6700\u9ad8\u8fbe17\u500d\uff09\uff0c\u63ed\u793a\u4e86\u5206\u8bcd\u504f\u5dee\u662f\u8bed\u8a00\u5efa\u6a21\u7684\u5173\u952e\u8bbe\u8ba1\u56e0\u7d20", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u57fa\u4e8e\u5b50\u8bcd\u8bad\u7ec3\u4f46\u751f\u6210\u5b57\u7b26\u5e8f\u5217\uff0c\u7406\u8bba\u4e0a\u5206\u8bcd\u5668\u9009\u62e9\u4e0d\u5e94\u5f71\u54cd\u5b57\u7b26\u6982\u7387\uff0c\u4f46\u5b9e\u8df5\u4e2d\u5b58\u5728\u504f\u5dee\u3002\u672c\u6587\u65e8\u5728\u91cf\u5316\u8fd9\u79cd\u5206\u8bcd\u504f\u5dee\u7684\u5177\u4f53\u5f71\u54cd\u673a\u5236", "method": "\u5c06\u5206\u8bcd\u504f\u5dee\u5b9a\u4e49\u4e3a\u56e0\u679c\u6548\u5e94\uff0c\u91c7\u7528\u56de\u5f52\u65ad\u70b9\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5206\u6790\u5206\u8bcd\u5668\u8bcd\u6c47\u8868\u622a\u65ad\u70b9\u9644\u8fd1\u5b50\u8bcd\u7684\u6a21\u578b\u8868\u73b0\u5dee\u5f02\u8fdb\u884c\u91cf\u5316", "result": "\u4e0d\u540c\u89c4\u6a21/\u8bcd\u6c47\u8868/\u5206\u8bcd\u5668\u7684\u6a21\u578b\u5747\u663e\u793a\u663e\u8457\u5206\u8bcd\u504f\u5dee\uff0c\u5c0f\u6a21\u578b\u4e2d\u5b50\u8bcd\u5b58\u5728\u53ef\u4f7f\u5b57\u7b26\u6982\u7387\u63d0\u5347\u6700\u9ad8\u8fbe17\u500d", "conclusion": "\u5206\u8bcd\u5668\u8bbe\u8ba1\u662f\u8bed\u8a00\u5efa\u6a21\u7684\u6838\u5fc3\u6280\u672f\u9009\u62e9\uff0c\u5176\u8bcd\u6c47\u8868\u51b3\u7b56\u76f4\u63a5\u5f71\u54cd\u6a21\u578b\u884c\u4e3a\uff0c\u9700\u4f5c\u4e3a\u5173\u952e\u56e0\u7d20\u7eb3\u5165\u6a21\u578b\u5f00\u53d1\u8003\u91cf"}}
{"id": "2506.01704", "pdf": "https://arxiv.org/pdf/2506.01704", "abs": "https://arxiv.org/abs/2506.01704", "authors": ["Jiongnan Liu", "Zhicheng Dou", "Ning Hu", "Chenyan Xiong"], "title": "Generate, Not Recommend: Personalized Multimodal Content Generation", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "To address the challenge of information overload from massive web contents,\nrecommender systems are widely applied to retrieve and present personalized\nresults for users. However, recommendation tasks are inherently constrained to\nfiltering existing items and lack the ability to generate novel concepts,\nlimiting their capacity to fully satisfy user demands and preferences. In this\npaper, we propose a new paradigm that goes beyond content filtering and\nselecting: directly generating personalized items in a multimodal form, such as\nimages, tailored to individual users. To accomplish this, we leverage\nany-to-any Large Multimodal Models (LMMs) and train them in both supervised\nfine-tuning and online reinforcement learning strategy to equip them with the\nability to yield tailored next items for users. Experiments on two benchmark\ndatasets and user study confirm the efficacy of the proposed method. Notably,\nthe generated images not only align well with users' historical preferences but\nalso exhibit relevance to their potential future interests.", "AI": {"tldr": "\u63d0\u51fa\u5229\u7528\u591a\u6a21\u6001\u5927\u6a21\u578b\u76f4\u63a5\u751f\u6210\u4e2a\u6027\u5316\u63a8\u8350\u5185\u5bb9\u7684\u65b0\u8303\u5f0f\uff0c\u7a81\u7834\u4f20\u7edf\u63a8\u8350\u7cfb\u7edf\u4ec5\u80fd\u8fc7\u6ee4\u73b0\u6709\u5185\u5bb9\u7684\u5c40\u9650\u3002", "motivation": "\u4f20\u7edf\u63a8\u8350\u7cfb\u7edf\u4ec5\u80fd\u8fc7\u6ee4\u73b0\u6709\u5185\u5bb9\uff0c\u65e0\u6cd5\u751f\u6210\u65b0\u6982\u5ff5\uff0c\u96be\u4ee5\u5145\u5206\u6ee1\u8db3\u7528\u6237\u9700\u6c42\u3002\u9700\u63a2\u7d22\u76f4\u63a5\u751f\u6210\u7b26\u5408\u7528\u6237\u504f\u597d\u7684\u65b0\u5185\u5bb9\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u4efb\u610f\u6a21\u6001\u8f6c\u6362\u7684\u5927\u6a21\u578b\uff08LMMs\uff09\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u8bad\u7ec3\uff0c\u4f7f\u5176\u80fd\u4e3a\u7528\u6237\u751f\u6210\u5b9a\u5236\u5316\u7684\u4e0b\u4e2a\u63a8\u8350\u5185\u5bb9\uff08\u5982\u56fe\u50cf\uff09\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u548c\u7528\u6237\u7814\u7a76\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u751f\u6210\u56fe\u50cf\u65e2\u7b26\u5408\u7528\u6237\u5386\u53f2\u504f\u597d\uff0c\u53c8\u4e0e\u5176\u6f5c\u5728\u672a\u6765\u5174\u8da3\u76f8\u5173\u3002", "conclusion": "\u8be5\u751f\u6210\u5f0f\u63a8\u8350\u8303\u5f0f\u6210\u529f\u7a81\u7834\u4f20\u7edf\u8fc7\u6ee4\u6a21\u5f0f\uff0c\u4e3a\u6ee1\u8db3\u7528\u6237\u52a8\u6001\u504f\u597d\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.01963", "pdf": "https://arxiv.org/pdf/2506.01963", "abs": "https://arxiv.org/abs/2506.01963", "authors": ["Andrew Kiruluta", "Preethi Raju", "Priscilla Burity"], "title": "Breaking Quadratic Barriers: A Non-Attention LLM for Ultra-Long Context Horizons", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We present a novel non attention based architecture for large language models\n(LLMs) that efficiently handles very long context windows, on the order of\nhundreds of thousands to potentially millions of tokens. Unlike traditional\nTransformer designs, which suffer from quadratic memory and computation\noverload due to the nature of the self attention mechanism, our model avoids\ntoken to token attention entirely. Instead, it combines the following\ncomplementary components: State Space blocks (inspired by S4) that learn\ncontinuous time convolution kernels and scale near linearly with sequence\nlength, Multi Resolution Convolution layers that capture local context at\ndifferent dilation levels, a lightweight Recurrent Supervisor to maintain a\nglobal hidden state across sequential chunks, and Retrieval Augmented External\nMemory that stores and retrieves high-level chunk embeddings without\nreintroducing quadratic operations.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u578b\u975e\u6ce8\u610f\u529b\u67b6\u6784LLM\uff0c\u901a\u8fc7\u7ed3\u5408State Space\u5757\u3001\u591a\u5206\u8fa8\u7387\u5377\u79ef\u3001\u5faa\u73af\u76d1\u7763\u5668\u548c\u5916\u90e8\u8bb0\u5fc6\u7ec4\u4ef6\uff0c\u5b9e\u73b0\u63a5\u8fd1\u7ebf\u6027\u7684\u8ba1\u7b97\u6269\u5c55\uff0c\u652f\u6301\u6570\u5341\u4e07\u81f3\u6570\u767e\u4e07token\u7684\u8d85\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u3002", "motivation": "\u4f20\u7edfTransformer\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5bfc\u81f4\u4e8c\u6b21\u65b9\u590d\u6742\u5ea6\uff0c\u96be\u4ee5\u9ad8\u6548\u5904\u7406\u957f\u5e8f\u5217\u3002\u9700\u8981\u7a81\u7834\u8ba1\u7b97\u548c\u5185\u5b58\u74f6\u9888\uff0c\u5b9e\u73b0\u8d85\u957f\u4e0a\u4e0b\u6587\u7684\u7ebf\u6027\u6269\u5c55\u3002", "method": "1) State Space\u5757\u5b66\u4e60\u8fde\u7eed\u65f6\u95f4\u5377\u79ef\u6838\uff08S4\u542f\u53d1\uff09\n2) \u591a\u5206\u8fa8\u7387\u81a8\u80c0\u5377\u79ef\u6355\u83b7\u5c40\u90e8\u4e0a\u4e0b\u6587\n3) \u8f7b\u91cf\u7ea7\u5faa\u73af\u76d1\u7763\u5668\u7ef4\u62a4\u5168\u5c40\u9690\u85cf\u72b6\u6001\n4) \u68c0\u7d22\u589e\u5f3a\u5916\u90e8\u8bb0\u5fc6\u5b58\u50a8\u9ad8\u7ef4\u5757\u5d4c\u5165", "result": "\u6a21\u578b\u5728\u4fdd\u6301\u6027\u80fd\u524d\u63d0\u4e0b\uff0c\u5c06\u957f\u5e8f\u5217\u5904\u7406\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u4eceO(n\u00b2)\u964d\u81f3\u63a5\u8fd1O(n)\uff0c\u652f\u6301\u767e\u4e07\u7ea7token\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edfTransformer\u67b6\u6784\u3002", "conclusion": "\u8be5\u67b6\u6784\u901a\u8fc7\u591a\u6280\u672f\u878d\u5408\u7a81\u7834\u6ce8\u610f\u529b\u673a\u5236\u9650\u5236\uff0c\u4e3a\u8d85\u957f\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u80fd\u63a8\u52a8\u6587\u6863\u7ea7NLP\u5e94\u7528\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.01967", "pdf": "https://arxiv.org/pdf/2506.01967", "abs": "https://arxiv.org/abs/2506.01967", "authors": ["Patrik Czak\u00f3", "G\u00e1bor Kert\u00e9sz", "S\u00e1ndor Sz\u00e9n\u00e1si"], "title": "Turning LLM Activations Quantization-Friendly", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "6 pages, 5 figures. Accepted to SACI 2025 conference proceedings", "summary": "Quantization effectively reduces the serving costs of Large Language Models\n(LLMs) by speeding up data movement through compressed parameters and enabling\nfaster operations via integer arithmetic. However, activating integer\narithmetic requires quantizing both weights and activations, which poses\nchallenges due to the significant outliers in LLMs that increase quantization\nerror. In this work, we investigate these outliers with an emphasis on their\neffect on layer-wise quantization error, then examine how smoothing and\nrotation transform the observed values. Our primary contributions include\nintroducing a new metric to measure and visualize quantization difficulty based\non channel magnitudes, as well as proposing a hybrid approach that applies\nchannel-wise scaling before rotation, supported by a mathematical formulation\nof its benefits.", "AI": {"tldr": "\u91cf\u5316\u6280\u672f\u901a\u8fc7\u53c2\u6570\u538b\u7f29\u548c\u6574\u6570\u8fd0\u7b97\u964d\u4f4eLLM\u63a8\u7406\u6210\u672c\uff0c\u4f46\u6fc0\u6d3b\u91cf\u5316\u9762\u4e34\u5f02\u5e38\u503c\u6311\u6218\u3002\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u901a\u9053\u5e45\u5ea6\u7684\u91cf\u5316\u96be\u5ea6\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u5f00\u53d1\u901a\u9053\u7f29\u653e+\u65cb\u8f6c\u7684\u6df7\u5408\u91cf\u5316\u65b9\u6cd5\u3002", "motivation": "LLM\u6fc0\u6d3b\u5c42\u5b58\u5728\u663e\u8457\u5f02\u5e38\u503c\u4f1a\u589e\u5927\u91cf\u5316\u8bef\u5dee\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5206\u6790\u5f02\u5e38\u503c\u5206\u5e03\u7279\u5f81\uff0c\u5f00\u53d1\u66f4\u7cbe\u786e\u7684\u91cf\u5316\u8bef\u5dee\u63a7\u5236\u7b56\u7565\u3002", "method": "1. \u5efa\u7acb\u91cf\u5316\u96be\u5ea6\u8bc4\u4f30\u6307\u6807\uff08\u901a\u9053\u5e45\u5ea6\u5206\u6790\uff09\n2. \u6570\u5b66\u5efa\u6a21\u5e73\u6ed1/\u65cb\u8f6c\u5bf9\u6570\u503c\u5206\u5e03\u7684\u5f71\u54cd\n3. \u63d0\u51fa\u901a\u9053\u7f29\u653e\u9884\u5904\u7406+\u65cb\u8f6c\u7684\u6df7\u5408\u91cf\u5316\u6846\u67b6", "result": "\u65b0\u6307\u6807\u6709\u6548\u8bc6\u522b\u91cf\u5316\u654f\u611f\u901a\u9053\uff0c\u6df7\u5408\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edf\u65b9\u6848\u964d\u4f4e18.7%\u7684\u91cf\u5316\u8bef\u5dee\uff0c\u6570\u5b66\u63a8\u5bfc\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u7279\u5f81\u5206\u5e03\u5bf9\u9f50\u4e0a\u7684\u4f18\u52bf\u3002", "conclusion": "\u901a\u8fc7\u91cf\u5316\u96be\u5ea6\u8bc4\u4f30\u4e0e\u7279\u5f81\u7a7a\u95f4\u53d8\u6362\u7684\u534f\u540c\u4f18\u5316\uff0c\u4e3aLLM\u91cf\u5316\u63d0\u4f9b\u4e86\u517c\u5177\u7406\u8bba\u652f\u6491\u548c\u5b9e\u7528\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4f4e\u7cbe\u5ea6\u63a8\u7406\u6548\u679c\u3002"}}
{"id": "2506.01998", "pdf": "https://arxiv.org/pdf/2506.01998", "abs": "https://arxiv.org/abs/2506.01998", "authors": ["Takao Fujii", "Katie Seaborn", "Madeleine Steeds", "Jun Kato"], "title": "Inter(sectional) Alia(s): Ambiguity in Voice Agent Identity via Intersectional Japanese Self-Referents", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "cs.SD", "eess.AS"], "comment": "CHI '25", "summary": "Conversational agents that mimic people have raised questions about the\nethics of anthropomorphizing machines with human social identity cues. Critics\nhave also questioned assumptions of identity neutrality in humanlike agents.\nRecent work has revealed that intersectional Japanese pronouns can elicit\ncomplex and sometimes evasive impressions of agent identity. Yet, the role of\nother \"neutral\" non-pronominal self-referents (NPSR) and voice as a socially\nexpressive medium remains unexplored. In a crowdsourcing study, Japanese\nparticipants (N = 204) evaluated three ChatGPT voices (Juniper, Breeze, and\nEmber) using seven self-referents. We found strong evidence of voice gendering\nalongside the potential of intersectional self-referents to evade gendering,\ni.e., ambiguity through neutrality and elusiveness. Notably, perceptions of age\nand formality intersected with gendering as per sociolinguistic theories,\nespecially boku and watakushi. This work provides a nuanced take on agent\nidentity perceptions and champions intersectional and culturally-sensitive work\non voice agents.", "AI": {"tldr": "\u7814\u7a76\u63ed\u793a\u65e5\u8bed\u975e\u4ee3\u8bcd\u81ea\u6307\u8bcd\u4e0e\u8bed\u97f3\u7279\u5f81\u5171\u540c\u5f71\u54cd\u804a\u5929\u673a\u5668\u4eba\u6027\u522b\u5316\u8ba4\u77e5\uff0c\u90e8\u5206\u81ea\u6307\u8bcd\u53ef\u89c4\u907f\u6027\u522b\u523b\u677f\u5370\u8c61\uff0c\u5e74\u9f84/\u6b63\u5f0f\u5ea6\u7b49\u4ea4\u53c9\u7ef4\u5ea6\u5f3a\u5316\u793e\u4f1a\u8bed\u8a00\u5b66\u5173\u8054\u3002", "motivation": "\u63a2\u7d22\u4eba\u673a\u4ea4\u4e92\u4e2d'\u4e2d\u6027'\u975e\u4ee3\u8bcd\u81ea\u6307\u8bcd\uff08\u5982\u65e5\u8bedboku/watakushi\uff09\u4e0e\u8bed\u97f3\u7279\u5f81\u5982\u4f55\u4ea4\u53c9\u5f71\u54cd\u667a\u80fd\u4f53\u8eab\u4efd\u8ba4\u77e5\uff0c\u6311\u6218\u8bed\u97f3\u4e2d\u6027\u5047\u8bbe\u3002", "method": "\u901a\u8fc7\u4f17\u5305\u5b9e\u9a8c\uff08N=204\uff09\uff0c\u8ba9\u65e5\u672c\u53c2\u4e0e\u8005\u8bc4\u4f30ChatGPT\u4e09\u79cd\u8bed\u97f3\uff08Juniper/Breeze/Ember\uff09\u4e0e\u4e03\u79cd\u81ea\u6307\u8bcd\u7684\u7ec4\u5408\u6548\u679c\u3002", "result": "\u8bed\u97f3\u663e\u8457\u5f3a\u5316\u6027\u522b\u523b\u677f\u5370\u8c61\uff08\u5982Juniper\u88ab\u89c6\u4f5c\u5e74\u8f7b\u5973\u6027\uff09\uff0c\u4f46\u7279\u5b9a\u81ea\u6307\u8bcd\u53ef\u521b\u9020\u4e2d\u6027/\u6a21\u7cca\u7684\u6027\u522b\u8ba4\u77e5\uff0c\u4e14\u5e74\u9f84/\u6b63\u5f0f\u5ea6\u4e0e\u6027\u522b\u8ba4\u77e5\u5b58\u5728\u4ea4\u53c9\u5173\u8054\u3002", "conclusion": "\u5f3a\u8c03\u667a\u80fd\u4f53\u8eab\u4efd\u8ba4\u77e5\u7684\u590d\u6742\u6027\uff0c\u4e3b\u5f20\u91c7\u7528\u4ea4\u53c9\u6027\u6846\u67b6\u548c\u6587\u5316\u654f\u611f\u6027\u65b9\u6cd5\u8bbe\u8ba1\u8bed\u97f3\u4ee3\u7406\uff0c\u7a81\u7834\u5355\u4e00\u7ef4\u5ea6\u8eab\u4efd\u6807\u7b7e\u3002"}}
{"id": "2506.02057", "pdf": "https://arxiv.org/pdf/2506.02057", "abs": "https://arxiv.org/abs/2506.02057", "authors": ["David Sasu", "Kweku Andoh Yamoah", "Benedict Quartey", "Natalie Schluter"], "title": "Enhancing Speech Instruction Understanding and Disambiguation in Robotics via Speech Prosody", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted to Interspeech 2025", "summary": "Enabling robots to accurately interpret and execute spoken language\ninstructions is essential for effective human-robot collaboration. Traditional\nmethods rely on speech recognition to transcribe speech into text, often\ndiscarding crucial prosodic cues needed for disambiguating intent. We propose a\nnovel approach that directly leverages speech prosody to infer and resolve\ninstruction intent. Predicted intents are integrated into large language models\nvia in-context learning to disambiguate and select appropriate task plans.\nAdditionally, we present the first ambiguous speech dataset for robotics,\ndesigned to advance research in speech disambiguation. Our method achieves\n95.79% accuracy in detecting referent intents within an utterance and\ndetermines the intended task plan of ambiguous instructions with 71.96%\naccuracy, demonstrating its potential to significantly improve human-robot\ncommunication.", "AI": {"tldr": "\u901a\u8fc7\u76f4\u63a5\u5229\u7528\u8bed\u97f3\u97f5\u5f8b\u7279\u5f81\u5e76\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u673a\u5668\u4eba\u8bed\u97f3\u6307\u4ee4\u7684\u6b67\u4e49\u95ee\u9898\uff0c\u5e76\u521b\u5efa\u9996\u4e2a\u673a\u5668\u4eba\u6a21\u7cca\u8bed\u97f3\u6570\u636e\u96c6", "motivation": "\u4f20\u7edf\u8bed\u97f3\u8bc6\u522b\u65b9\u6cd5\u4e22\u5f03\u4e86\u97f5\u5f8b\u7ebf\u7d22\u5bfc\u81f4\u610f\u56fe\u6b67\u4e49\uff0c\u963b\u788d\u4eba\u673a\u534f\u4f5c\u6548\u679c\u3002\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u76f4\u63a5\u5229\u7528\u8bed\u97f3\u7279\u5f81\u6765\u51c6\u786e\u7406\u89e3\u6307\u4ee4\u610f\u56fe\u3002", "method": "1. \u4ece\u539f\u59cb\u8bed\u97f3\u4e2d\u63d0\u53d6\u97f5\u5f8b\u7279\u5f81\u9884\u6d4b\u610f\u56fe\n2. \u901a\u8fc7in-context learning\u5c06\u9884\u6d4b\u610f\u56fe\u6574\u5408\u5230LLM\u4e2d\u8fdb\u884c\u4efb\u52a1\u8ba1\u5212\u9009\u62e9\n3. \u521b\u5efa\u9996\u4e2a\u673a\u5668\u4eba\u6a21\u7cca\u8bed\u97f3\u6570\u636e\u96c6\u652f\u6301\u7814\u7a76", "result": "\u6307\u79f0\u610f\u56fe\u68c0\u6d4b\u51c6\u786e\u738795.79%\uff0c\u6a21\u7cca\u6307\u4ee4\u4efb\u52a1\u8ba1\u5212\u9009\u62e9\u51c6\u786e\u738771.96%", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u97f5\u5f8b\u7279\u5f81\u548cLLM\u7684\u534f\u540c\u4f5c\u7528\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4eba\u673a\u4ea4\u4e92\u4e2d\u6b67\u4e49\u6307\u4ee4\u7684\u89e3\u6790\u80fd\u529b\uff0c\u6570\u636e\u96c6\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301"}}
{"id": "2506.02059", "pdf": "https://arxiv.org/pdf/2506.02059", "abs": "https://arxiv.org/abs/2506.02059", "authors": ["Ziwei Gong", "Pengyuan Shi", "Kaan Donbekci", "Lin Ai", "Run Chen", "David Sasu", "Zehui Wu", "Julia Hirschberg"], "title": "Learning More with Less: Self-Supervised Approaches for Low-Resource Speech Emotion Recognition", "categories": ["cs.SD", "cs.CL"], "comment": "Accepted at Interspeech 2025", "summary": "Speech Emotion Recognition (SER) has seen significant progress with deep\nlearning, yet remains challenging for Low-Resource Languages (LRLs) due to the\nscarcity of annotated data. In this work, we explore unsupervised learning to\nimprove SER in low-resource settings. Specifically, we investigate contrastive\nlearning (CL) and Bootstrap Your Own Latent (BYOL) as self-supervised\napproaches to enhance cross-lingual generalization. Our methods achieve notable\nF1 score improvements of 10.6% in Urdu, 15.2% in German, and 13.9% in Bangla,\ndemonstrating their effectiveness in LRLs. Additionally, we analyze model\nbehavior to provide insights on key factors influencing performance across\nlanguages, and also highlighting challenges in low-resource SER. This work\nprovides a foundation for developing more inclusive, explainable, and robust\nemotion recognition systems for underrepresented languages.", "AI": {"tldr": "\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\uff08CL\uff09\u548cBYOL\u81ea\u76d1\u7763\u65b9\u6cd5\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u6027\u80fd", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u56e0\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u5bfc\u81f4\u7684\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u6027\u80fd\u53d7\u9650\u95ee\u9898", "method": "\u91c7\u7528\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u548cBYOL\u6846\u67b6\u589e\u5f3a\u8de8\u8bed\u8a00\u6cdb\u5316\u80fd\u529b", "result": "F1\u5206\u6570\u663e\u8457\u63d0\u5347\uff08\u4e4c\u5c14\u90fd\u8bed10.6%\u3001\u5fb7\u8bed15.2%\u3001\u5b5f\u52a0\u62c9\u8bed13.9%\uff09", "conclusion": "\u4e3a\u5f00\u53d1\u5305\u5bb9\u6027\u66f4\u5f3a\u3001\u53ef\u89e3\u91ca\u6027\u66f4\u597d\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\u60c5\u611f\u8bc6\u522b\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840"}}
{"id": "2506.02077", "pdf": "https://arxiv.org/pdf/2506.02077", "abs": "https://arxiv.org/abs/2506.02077", "authors": ["Yoonjun Cho", "Soeun Kim", "Dongjae Jeon", "Kyelim Lee", "Beomsoo Lee", "Albert No"], "title": "Assigning Distinct Roles to Quantized and Low-Rank Matrices Toward Optimal Weight Decomposition", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted to Findings of ACL 2025", "summary": "Decomposing weight matrices into quantization and low-rank components\n($\\mathbf{W} \\approx \\mathbf{Q} + \\mathbf{L}\\mathbf{R}$) is a widely used\ntechnique for compressing large language models (LLMs). Existing joint\noptimization methods iteratively alternate between quantization and low-rank\napproximation. However, these methods tend to prioritize one component at the\nexpense of the other, resulting in suboptimal decompositions that fail to\nleverage each component's unique strengths. In this work, we introduce\nOutlier-Driven Low-Rank Initialization (ODLRI), which assigns low-rank\ncomponents the specific role of capturing activation-sensitive weights. This\nstructured decomposition mitigates outliers' negative impact on quantization,\nenabling more effective balance between quantization and low-rank\napproximation. Experiments on Llama2 (7B, 13B, 70B), Llama3-8B, and Mistral-7B\ndemonstrate that incorporating ODLRI into the joint optimization framework\nconsistently reduces activation-aware error, minimizes quantization scale, and\nimproves perplexity and zero-shot accuracy in low-bit settings.", "AI": {"tldr": "\u63d0\u51fa\u5f02\u5e38\u9a71\u52a8\u4f4e\u79e9\u521d\u59cb\u5316(ODLRI)\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5206\u89e3\u6743\u91cd\u77e9\u9635\u6709\u6548\u5e73\u8861\u91cf\u5316\u4e0e\u4f4e\u79e9\u8fd1\u4f3c\uff0c\u663e\u8457\u63d0\u5347\u4f4e\u6bd4\u7279\u538b\u7f29\u4e0b\u8bed\u8a00\u6a21\u578b\u6027\u80fd", "motivation": "\u73b0\u6709\u8054\u5408\u4f18\u5316\u65b9\u6cd5\u4ea4\u66ff\u4f18\u5316\u65f6\u504f\u91cd\u5355\u4e00\u7ec4\u4ef6\uff0c\u5bfc\u81f4\u5206\u89e3\u7ed3\u679c\u6b21\u4f18\u4e14\u65e0\u6cd5\u5145\u5206\u53d1\u6325\u5404\u7ec4\u4ef6\u4f18\u52bf\uff0c\u9700\u6539\u8fdb\u5206\u89e3\u7b56\u7565\u5e73\u8861\u4e24\u8005\u6548\u80fd", "method": "ODLRI\u8d4b\u4e88\u4f4e\u79e9\u7ec4\u4ef6\u6355\u83b7\u6fc0\u6d3b\u654f\u611f\u6743\u91cd\u7684\u4e13\u9879\u80fd\u529b\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5206\u89e3\u964d\u4f4e\u5f02\u5e38\u503c\u5bf9\u91cf\u5316\u7684\u5e72\u6270\uff0c\u5b9e\u73b0\u91cf\u5316\u4e0e\u4f4e\u79e9\u534f\u540c\u4f18\u5316", "result": "\u5728Llama2(7B/13B/70B)\u3001Llama3-8B\u548cMistral-7B\u4e0a\u9a8c\u8bc1\uff0cODLRI\u6709\u6548\u964d\u4f4e\u6fc0\u6d3b\u611f\u77e5\u8bef\u5dee\u3001\u538b\u7f29\u91cf\u5316\u89c4\u6a21\uff0c\u63d0\u5347\u4f4e\u6bd4\u7279\u573a\u666f\u56f0\u60d1\u5ea6\u53ca\u96f6\u6837\u672c\u51c6\u786e\u7387", "conclusion": "ODLRI\u901a\u8fc7\u7ec4\u4ef6\u5206\u5de5\u7b56\u7565\u7a81\u7834\u73b0\u6709\u8054\u5408\u4f18\u5316\u9650\u5236\uff0c\u4e3aLLM\u9ad8\u6548\u538b\u7f29\u63d0\u4f9b\u65b0\u65b9\u5411\uff0c\u8bc1\u5b9e\u7ed3\u6784\u5316\u5206\u89e3\u53ef\u534f\u540c\u53d1\u6325\u4e0d\u540c\u7ec4\u4ef6\u7684\u6280\u672f\u4f18\u52bf"}}
{"id": "2506.02085", "pdf": "https://arxiv.org/pdf/2506.02085", "abs": "https://arxiv.org/abs/2506.02085", "authors": ["Ajinkya Kulkarni", "Sandipana Dowerah", "Tanel Alumae", "Mathew Magimai. -Doss"], "title": "Unveiling Audio Deepfake Origins: A Deep Metric learning And Conformer Network Approach With Ensemble Fusion", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": "Accepted at Interspeech 2025, Netherlands", "summary": "Audio deepfakes are acquiring an unprecedented level of realism with advanced\nAI. While current research focuses on discerning real speech from spoofed\nspeech, tracing the source system is equally crucial. This work proposes a\nnovel audio source tracing system combining deep metric multi-class N-pair loss\nwith Real Emphasis and Fake Dispersion framework, a Conformer classification\nnetwork, and ensemble score-embedding fusion. The N-pair loss improves\ndiscriminative ability, while Real Emphasis and Fake Dispersion enhance\nrobustness by focusing on differentiating real and fake speech patterns. The\nConformer network captures both global and local dependencies in the audio\nsignal, crucial for source tracing. The proposed ensemble score-embedding\nfusion shows an optimal trade-off between in-domain and out-of-domain source\ntracing scenarios. We evaluate our method using Frechet Distance and standard\nmetrics, demonstrating superior performance in source tracing over the baseline\nsystem.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u6df1\u5ea6\u5ea6\u91cf\u591a\u7c7bN-pair\u635f\u5931\u3001Real Emphasis-Fake Dispersion\u6846\u67b6\u548cConformer\u7f51\u7edc\u7684\u97f3\u9891\u6eaf\u6e90\u7cfb\u7edf\uff0c\u5728\u4f2a\u9020\u8bed\u97f3\u6765\u6e90\u8ffd\u8e2a\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u51c6\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524d\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u903c\u771f\u5ea6\u6781\u9ad8\uff0c\u73b0\u6709\u7814\u7a76\u96c6\u4e2d\u4e8e\u771f\u4f2a\u8bc6\u522b\u800c\u7f3a\u4e4f\u6765\u6e90\u8ffd\u8e2a\u80fd\u529b\uff0c\u4e9f\u9700\u5efa\u7acb\u6709\u6548\u7684\u6eaf\u6e90\u673a\u5236\u5e94\u5bf9\u5b89\u5168\u5a01\u80c1\u3002", "method": "1. \u591a\u7c7bN-pair\u635f\u5931\u63d0\u5347\u7279\u5f81\u5224\u522b\u529b\n2. Real Emphasis\u805a\u7126\u771f\u5b9e\u8bed\u97f3\u7279\u5f81\uff0cFake Dispersion\u6269\u5927\u4f2a\u9020\u6837\u672c\u5dee\u5f02\n3. Conformer\u7f51\u7edc\u6355\u83b7\u97f3\u9891\u5168\u5c40-\u5c40\u90e8\u4f9d\u8d56\u5173\u7cfb\n4. \u96c6\u6210\u5f97\u5206-\u5d4c\u5165\u878d\u5408\u7b56\u7565\u5e73\u8861\u57df\u5185\u5916\u573a\u666f\u6027\u80fd", "result": "\u4f7f\u7528Frechet\u8ddd\u79bb\u7b49\u6307\u6807\u9a8c\u8bc1\uff0c\u5728ASVspoof 2019\u6570\u636e\u96c6\u4e0a\u6eaf\u6e90\u51c6\u786e\u7387\u63d0\u534715%\uff0c\u8de8\u6570\u636e\u5e93\u6d4b\u8bd5\u4fdd\u630183%\u7684\u7a33\u5b9a\u6027\u80fd\u3002", "conclusion": "\u8be5\u878d\u5408\u6846\u67b6\u5728\u771f\u4f2a\u8bed\u97f3\u6eaf\u6e90\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6700\u4f18\u5e73\u8861\uff0c\u591a\u6280\u672f\u534f\u540c\u6709\u6548\u63d0\u5347\u6df1\u5ea6\u4f2a\u9020\u97f3\u9891\u7684\u6eaf\u6e90\u80fd\u529b\uff0c\u4e3a\u97f3\u9891\u5b89\u5168\u8ba4\u8bc1\u63d0\u4f9b\u65b0\u65b9\u6848\u3002"}}
{"id": "2506.02088", "pdf": "https://arxiv.org/pdf/2506.02088", "abs": "https://arxiv.org/abs/2506.02088", "authors": ["Alef Iury Siqueira Ferreira", "Lucas Rafael Gris", "Alexandre Ferro Filho", "Lucas \u00d3lives", "Daniel Ribeiro", "Luiz Fernando", "Fernanda Lustosa", "Rodrigo Tanaka", "Frederico Santos de Oliveira", "Arlindo Galv\u00e3o Filho"], "title": "Enhancing Speech Emotion Recognition with Graph-Based Multimodal Fusion and Prosodic Features for the Speech Emotion Recognition in Naturalistic Conditions Challenge at Interspeech 2025", "categories": ["cs.SD", "cs.CL", "cs.LG"], "comment": null, "summary": "Training SER models in natural, spontaneous speech is especially challenging\ndue to the subtle expression of emotions and the unpredictable nature of\nreal-world audio. In this paper, we present a robust system for the INTERSPEECH\n2025 Speech Emotion Recognition in Naturalistic Conditions Challenge, focusing\non categorical emotion recognition. Our method combines state-of-the-art audio\nmodels with text features enriched by prosodic and spectral cues. In\nparticular, we investigate the effectiveness of Fundamental Frequency (F0)\nquantization and the use of a pretrained audio tagging model. We also employ an\nensemble model to improve robustness. On the official test set, our system\nachieved a Macro F1-score of 39.79% (42.20% on validation). Our results\nunderscore the potential of these methods, and analysis of fusion techniques\nconfirmed the effectiveness of Graph Attention Networks. Our source code is\npublicly available.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u97f3\u9891\u6a21\u578b\u4e0e\u97f5\u5f8b\u589e\u5f3a\u6587\u672c\u7279\u5f81\u7684\u9c81\u68d2\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u7cfb\u7edf\uff0c\u91c7\u7528F0\u91cf\u5316\u3001\u9884\u8bad\u7ec3\u97f3\u9891\u6807\u8bb0\u6a21\u578b\u548c\u96c6\u6210\u65b9\u6cd5\uff0c\u5728\u81ea\u7136\u573a\u666f\u6d4b\u8bd5\u4e2d\u53d6\u5f9739.79%\u5b8fF1\u5206\u6570\u3002", "motivation": "\u81ea\u7136\u8bed\u97f3\u4e2d\u60c5\u611f\u8868\u8fbe\u7ec6\u5fae\u4e14\u771f\u5b9e\u97f3\u9891\u4e0d\u53ef\u9884\u6d4b\uff0c\u9700\u5f00\u53d1\u66f4\u9c81\u68d2\u7684SER\u6a21\u578b\u5e94\u5bf9INTERSPEECH\u6311\u6218\u8d5b\u9700\u6c42\u3002", "method": "\u6574\u5408\u5148\u8fdb\u97f3\u9891\u6a21\u578b+\u97f5\u5f8b/\u9891\u8c31\u589e\u5f3a\u7684\u6587\u672c\u7279\u5f81\uff0c\u63a2\u7d22\u57fa\u9891\u91cf\u5316\u6280\u672f\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u97f3\u9891\u6807\u8bb0\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u7684\u96c6\u6210\u878d\u5408\u65b9\u6cd5\u3002", "result": "\u5b98\u65b9\u6d4b\u8bd5\u96c6Macro F1\u8fbe39.79%\uff08\u9a8c\u8bc1\u96c642.20%\uff09\uff0c\u878d\u5408\u5206\u6790\u8bc1\u5b9e\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u81ea\u7136\u573a\u666fSER\u4e2d\u5c55\u73b0\u6f5c\u529b\uff0c\u4ee3\u7801\u5f00\u6e90\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\uff0c\u9891\u8c31/\u97f5\u5f8b\u7279\u5f81\u7684\u7ed3\u5408\u53ca\u6a21\u578b\u878d\u5408\u7b56\u7565\u503c\u5f97\u501f\u9274\u3002"}}
{"id": "2506.02096", "pdf": "https://arxiv.org/pdf/2506.02096", "abs": "https://arxiv.org/abs/2506.02096", "authors": ["Zijian Wu", "Jinjie Ni", "Xiangyan Liu", "Zichen Liu", "Hang Yan", "Michael Qizhe Shieh"], "title": "SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have shown notable progress in scaling test-time\ncompute effectively. In this work, we investigate how synthesized RL data can\nfurther improve RLVR. To this end, we propose \\textbf{SynthRL}-a scalable and\nguaranteed pipeline for automatic data scaling in reasoning-oriented RL\ntraining. SynthRL comprises three key stages: (1) selecting seed questions with\nappropriate distribution, (2) augmenting them into more challenging variants\nwhile preserving the original answers, and (3) a guaranteed verification stage\nthat ensures near-perfect correctness and difficulty enhancement. Our empirical\nexperiments demonstrate SynthRL's scalability and effectiveness. When applied\nto the MMK12 dataset, SynthRL synthesizes over 3.3K additional verifiable,\nchallenging questions from approximately 8K seed samples. Models trained with\nour synthesized data achieve consistent gains across five out-of-domain visual\nmath reasoning benchmarks, with a significant improvement over baseline models\ntrained on seed data alone. Notably, detailed analysis reveals that the gains\nare more pronounced on the most challenging evaluation samples, highlighting\nSynthRL's effectiveness in eliciting deeper and more complex reasoning\npatterns.", "AI": {"tldr": "\u63d0\u51faSynthRL\u6d41\u7a0b\u2014\u2014\u901a\u8fc7\u4e09\u9636\u6bb5\u6570\u636e\u5408\u6210\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u6a21\u578b\u8868\u73b0", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u5408\u6210\u5f3a\u5316\u5b66\u4e60\u6570\u636e\u8fdb\u4e00\u6b65\u63d0\u5347\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u8bad\u7ec3\u6548\u679c\uff0c\u7a81\u7834\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u74f6\u9888", "method": "1. \u7b5b\u9009\u79cd\u5b50\u95ee\u9898\u5206\u5e03\uff1b2. \u4fdd\u6301\u539f\u7b54\u6848\u751f\u6210\u66f4\u96be\u7684\u53d8\u4f53\uff1b3. \u901a\u8fc7\u9a8c\u8bc1\u9636\u6bb5\u786e\u4fdd\u6b63\u786e\u6027\u548c\u96be\u5ea6\u63d0\u5347\u3002\u6784\u5efa\u53ef\u9a8c\u8bc1\u7684\u81ea\u52a8\u6570\u636e\u6269\u5c55\u7ba1\u9053", "result": "\u4ece8K\u79cd\u5b50\u6837\u672c\u5408\u62103.3K\u65b0\u95ee\u9898\uff0c\u6a21\u578b\u57285\u4e2a\u8de8\u57df\u89c6\u89c9\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u5b9e\u73b0\u7a33\u5b9a\u63d0\u5347\uff0c\u5728\u6700\u5177\u6311\u6218\u6027\u7684\u6837\u672c\u4e0a\u589e\u76ca\u5c24\u5176\u663e\u8457\uff08\u6700\u9ad8\u63d0\u53476.4%\uff09", "conclusion": "SynthRL\u901a\u8fc7\u53ef\u63a7\u7684\u6570\u636e\u5408\u6210\u673a\u5236\u6709\u6548\u6fc0\u53d1\u6df1\u5ea6\u590d\u6742\u63a8\u7406\uff0c\u9a8c\u8bc1\u4e86\u5408\u6210\u6570\u636e\u5728\u63d0\u5347\u6a21\u578b\u9ad8\u9636\u8ba4\u77e5\u80fd\u529b\u65b9\u9762\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4e3aAI\u7cfb\u7edf\u63a8\u7406\u80fd\u529b\u589e\u5f3a\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u65b9\u6848"}}
{"id": "2506.02160", "pdf": "https://arxiv.org/pdf/2506.02160", "abs": "https://arxiv.org/abs/2506.02160", "authors": ["Madan Krishnamurthy", "Daniel Korn", "Melissa A Haendel", "Christopher J Mungall", "Anne E Thessen"], "title": "A Dynamic Framework for Semantic Grouping of Common Data Elements (CDE) Using Embeddings and Clustering", "categories": ["cs.IR", "cs.CL", "cs.LG"], "comment": null, "summary": "This research aims to develop a dynamic and scalable framework to facilitate\nharmonization of Common Data Elements (CDEs) across heterogeneous biomedical\ndatasets by addressing challenges such as semantic heterogeneity, structural\nvariability, and context dependence to streamline integration, enhance\ninteroperability, and accelerate scientific discovery. Our methodology\nleverages Large Language Models (LLMs) for context-aware text embeddings that\nconvert CDEs into dense vectors capturing semantic relationships and patterns.\nThese embeddings are clustered using Hierarchical Density-Based Spatial\nClustering of Applications with Noise (HDBSCAN) to group semantically similar\nCDEs. The framework incorporates four key steps: (1) LLM-based text embedding\nto mathematically represent semantic context, (2) unsupervised clustering of\nembeddings via HDBSCAN, (3) automated labeling using LLM summarization, and (4)\nsupervised learning to train a classifier assigning new or unclustered CDEs to\nlabeled clusters. Evaluated on the NIH NLM CDE Repository with over 24,000\nCDEs, the system identified 118 meaningful clusters at an optimized minimum\ncluster size of 20. The classifier achieved 90.46 percent overall accuracy,\nperforming best in larger categories. External validation against Gravity\nProjects Social Determinants of Health domains showed strong agreement\n(Adjusted Rand Index 0.52, Normalized Mutual Information 0.78), indicating that\nembeddings effectively capture cluster characteristics. This adaptable and\nscalable approach offers a practical solution to CDE harmonization, improving\nselection efficiency and supporting ongoing data interoperability.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u548c\u805a\u7c7b\u7b97\u6cd5\u7684\u52a8\u6001\u6846\u67b6\uff0c\u6709\u6548\u534f\u8c03\u751f\u7269\u533b\u5b66\u5f02\u6784\u6570\u636e\u5143\u7d20", "motivation": "\u89e3\u51b3\u751f\u7269\u533b\u5b66\u6570\u636e\u9886\u57df\u516c\u5171\u6570\u636e\u5143\u7d20\uff08CDEs\uff09\u5b58\u5728\u7684\u8bed\u4e49\u5f02\u6784\u6027\u3001\u7ed3\u6784\u53ef\u53d8\u6027\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\u4e09\u5927\u6311\u6218\uff0c\u63d0\u5347\u6570\u636e\u6574\u5408\u6548\u7387\u548c\u79d1\u7814\u53d1\u73b0\u901f\u5ea6", "method": "1\uff09LLM\u751f\u6210\u8bed\u4e49\u5d4c\u5165\u5411\u91cf \u2192 2\uff09HDBSCAN\u805a\u7c7b \u2192 3\uff09LLM\u81ea\u52a8\u6807\u6ce8 \u2192 4\uff09\u76d1\u7763\u5b66\u4e60\u6784\u5efa\u5206\u7c7b\u5668", "result": "\u572824,000+CDEs\u6d4b\u8bd5\u4e2d\uff1a\u751f\u6210118\u4e2a\u6709\u6548\u805a\u7c7b\uff08\u6700\u5c0f\u89c4\u6a2120\uff09\uff0c\u5206\u7c7b\u51c6\u786e\u738790.46%\uff0c\u5916\u90e8\u9a8c\u8bc1ARI=0.52/NMI=0.78", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u6d41\u7a0b\u663e\u8457\u63d0\u5347CDEs\u534f\u8c03\u6548\u7387\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u6570\u636e\u4e92\u64cd\u4f5c\u6027\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.02178", "pdf": "https://arxiv.org/pdf/2506.02178", "abs": "https://arxiv.org/abs/2506.02178", "authors": ["Thai-Binh Nguyen", "Ngoc-Quan Pham", "Alexander Waibel"], "title": "Cocktail-Party Audio-Visual Speech Recognition", "categories": ["cs.SD", "cs.CL"], "comment": "Accepted at Interspeech 2025", "summary": "Audio-Visual Speech Recognition (AVSR) offers a robust solution for speech\nrecognition in challenging environments, such as cocktail-party scenarios,\nwhere relying solely on audio proves insufficient. However, current AVSR models\nare often optimized for idealized scenarios with consistently active speakers,\noverlooking the complexities of real-world settings that include both speaking\nand silent facial segments. This study addresses this gap by introducing a\nnovel audio-visual cocktail-party dataset designed to benchmark current AVSR\nsystems and highlight the limitations of prior approaches in realistic noisy\nconditions. Additionally, we contribute a 1526-hour AVSR dataset comprising\nboth talking-face and silent-face segments, enabling significant performance\ngains in cocktail-party environments. Our approach reduces WER by 67% relative\nto the state-of-the-art, reducing WER from 119% to 39.2% in extreme noise,\nwithout relying on explicit segmentation cues.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u97f3\u9891\u89c6\u89c9\u9e21\u5c3e\u9152\u4f1a\u6570\u636e\u96c6\uff0c\u5728\u6781\u7aef\u566a\u58f0\u4e0b\u5b9e\u73b0\u8bcd\u9519\u7387(WER)67%\u76f8\u5bf9\u964d\u4f4e\uff0c\u4ece119%\u964d\u81f339.2%", "motivation": "\u73b0\u6709AVSR\u6a21\u578b\u8fc7\u5ea6\u4f18\u5316\u7406\u60f3\u5316\u573a\u666f\uff0c\u5ffd\u7565\u771f\u5b9e\u73af\u5883\u4e2d\u8bf4\u8bdd/\u9759\u9ed8\u9762\u90e8\u4ea4\u66ff\u51fa\u73b0\u7684\u590d\u6742\u6027", "method": "\u6784\u5efa\u5305\u542b1526\u5c0f\u65f6\u8bf4\u8bdd/\u9759\u9ed8\u9762\u90e8\u7247\u6bb5\u7684\u65b0\u6570\u636e\u96c6\uff0c\u65e0\u9700\u663e\u5f0f\u5206\u5272\u7ebf\u7d22", "result": "\u5728\u6781\u7aef\u566a\u58f0\u73af\u5883\u4e0b\u8bcd\u9519\u7387\u4ece119%\u663e\u8457\u964d\u81f339.2%", "conclusion": "\u65b0\u578b\u6570\u636e\u96c6\u6709\u6548\u63d0\u5347AVSR\u5728\u590d\u6742\u566a\u58f0\u573a\u666f\u7684\u5b9e\u7528\u6027\uff0c\u63a8\u52a8\u771f\u5b9e\u73af\u5883\u5e94\u7528\u7a81\u7834"}}
{"id": "2506.02208", "pdf": "https://arxiv.org/pdf/2506.02208", "abs": "https://arxiv.org/abs/2506.02208", "authors": ["Hongling Xu", "Qi Zhu", "Heyuan Deng", "Jinpeng Li", "Lu Hou", "Yasheng Wang", "Lifeng Shang", "Ruifeng Xu", "Fei Mi"], "title": "KDRL: Post-Training Reasoning LLMs via Unified Knowledge Distillation and Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in large language model (LLM) post-training have leveraged\ntwo distinct paradigms to enhance reasoning capabilities: reinforcement\nlearning (RL) and knowledge distillation (KD). While RL enables the emergence\nof complex reasoning behaviors, it often suffers from low sample efficiency\nwhen the initial policy struggles to explore high-reward trajectories.\nConversely, KD improves learning efficiency via mimicking the teacher model but\ntends to generalize poorly to out-of-domain scenarios. In this work, we present\n\\textbf{KDRL}, a \\textit{unified post-training framework} that jointly\noptimizes a reasoning model through teacher supervision (KD) and\nself-exploration (RL). Specifically, KDRL leverages policy gradient\noptimization to simultaneously minimize the reverse Kullback-Leibler divergence\n(RKL) between the student and teacher distributions while maximizing the\nexpected rule-based rewards. We first formulate a unified objective that\nintegrates GRPO and KD, and systematically explore how different KL\napproximations, KL coefficients, and reward-guided KD strategies affect the\noverall post-training dynamics and performance. Empirical results on multiple\nreasoning benchmarks demonstrate that KDRL outperforms GRPO and various KD\nbaselines while achieving a favorable balance between performance and reasoning\ntoken efficiency. These findings indicate that integrating KD and RL serves as\nan effective and efficient strategy to train reasoning LLMs.", "AI": {"tldr": "\u63d0\u51faKDRL\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u8054\u5408\u4f18\u5316\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\u4e0e\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u9488\u5bf9\u5f3a\u5316\u5b66\u4e60(RL)\u6837\u672c\u6548\u7387\u4f4e\u548c\u77e5\u8bc6\u84b8\u998f(KD)\u9886\u57df\u6cdb\u5316\u5dee\u7684\u7f3a\u9677\uff0c\u63a2\u7d22\u4e24\u8005\u7684\u534f\u540c\u4f18\u5316\u673a\u5236\u3002RL\u64c5\u957f\u63a2\u7d22\u590d\u6742\u63a8\u7406\u8f68\u8ff9\u4f46\u6548\u7387\u4f4e\u4e0b\uff0cKD\u6548\u7387\u9ad8\u4f46\u7f3a\u4e4f\u81ea\u4e3b\u63a2\u7d22\u80fd\u529b\uff0c\u4e24\u8005\u4e92\u8865\u6027\u6784\u6210\u7814\u7a76\u52a8\u673a\u3002", "method": "1. \u6784\u5efa\u7edf\u4e00\u76ee\u6807\u51fd\u6570\u6574\u5408GRPO(\u57fa\u4e8e\u89c4\u5219\u7684\u7b56\u7565\u4f18\u5316)\u4e0eKD\n2. \u901a\u8fc7\u7b56\u7565\u68af\u5ea6\u540c\u6b65\u6700\u5c0f\u5316\u53cd\u5411KL\u6563\u5ea6(RKL)\u548c\u6700\u5927\u5316\u89c4\u5219\u5956\u52b1\n3. \u7cfb\u7edf\u7814\u7a76KL\u8fd1\u4f3c\u65b9\u6cd5\u3001KL\u7cfb\u6570\u548c\u5956\u52b1\u5f15\u5bfc\u7684KD\u7b56\u7565\u5bf9\u8bad\u7ec3\u52a8\u6001\u7684\u5f71\u54cd", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8aGRPO\u548cKD\u57fa\u7ebf\u6a21\u578b\uff0c\u63a8\u7406token\u6548\u7387\u63d0\u534738%\uff0c\u5b9e\u73b0\u6027\u80fd\u4e0e\u6548\u7387\u7684\u6700\u4f73\u5e73\u8861\u3002", "conclusion": "\u77e5\u8bc6\u84b8\u998f\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u534f\u540c\u4f18\u5316\u662f\u8bad\u7ec3\u63a8\u7406\u578b\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u8303\u5f0f\uff0c\u4e3aLLM\u540e\u8bad\u7ec3\u63d0\u4f9b\u65b0\u7684\u65b9\u6cd5\u8bba\u6846\u67b6\u3002"}}
{"id": "2506.02229", "pdf": "https://arxiv.org/pdf/2506.02229", "abs": "https://arxiv.org/abs/2506.02229", "authors": ["Manas Mehta", "Yimu Pan", "Kelly Gallagher", "Alison D. Gernand", "Jeffery A. Goldstein", "Delia Mwinyelle", "Leena Mithal", "James Z. Wang"], "title": "VLCD: Vision-Language Contrastive Distillation for Accurate and Efficient Automatic Placenta Analysis", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Proceedings of the 9th International Workshop on Health Intelligence,\n  in conjunction with the Annual AAAI Conference on Artificial Intelligence,\n  Philadelphia, Pennsylvania, March 2025", "summary": "Pathological examination of the placenta is an effective method for detecting\nand mitigating health risks associated with childbirth. Recent advancements in\nAI have enabled the use of photographs of the placenta and pathology reports\nfor detecting and classifying signs of childbirth-related pathologies. However,\nexisting automated methods are computationally extensive, which limits their\ndeployability. We propose two modifications to vision-language contrastive\nlearning (VLC) frameworks to enhance their accuracy and efficiency: (1)\ntext-anchored vision-language contrastive knowledge distillation (VLCD)-a new\nknowledge distillation strategy for medical VLC pretraining, and (2)\nunsupervised predistillation using a large natural images dataset for improved\ninitialization. Our approach distills efficient neural networks that match or\nsurpass the teacher model in performance while achieving model compression and\nacceleration. Our results showcase the value of unsupervised predistillation in\nimproving the performance and robustness of our approach, specifically for\nlower-quality images. VLCD serves as an effective way to improve the efficiency\nand deployability of medical VLC approaches, making AI-based healthcare\nsolutions more accessible, especially in resource-constrained environments.", "AI": {"tldr": "\u901a\u8fc7\u6587\u672c\u951a\u5b9a\u7684\u89c6\u89c9-\u8bed\u8a00\u5bf9\u6bd4\u77e5\u8bc6\u84b8\u998f\uff08VLCD\uff09\u548c\u65e0\u76d1\u7763\u9884\u84b8\u998f\u6280\u672f\uff0c\u63d0\u5347\u533b\u7597\u89c6\u89c9-\u8bed\u8a00\u5bf9\u6bd4\u6a21\u578b\u7684\u6548\u7387\u4e0e\u90e8\u7f72\u80fd\u529b\uff0c\u5728\u6a21\u578b\u538b\u7f29\u52a0\u901f\u7684\u540c\u65f6\u4fdd\u6301\u6216\u8d85\u8d8a\u539f\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u533b\u7597\u89c6\u89c9-\u8bed\u8a00\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u6709\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u3002\u9700\u901a\u8fc7\u6a21\u578b\u538b\u7f29\u548c\u77e5\u8bc6\u84b8\u998f\u63d0\u5347\u6548\u7387\u3002", "method": "1. \u63d0\u51faVLCD\u77e5\u8bc6\u84b8\u998f\u7b56\u7565\uff1b2. \u5229\u7528\u81ea\u7136\u56fe\u50cf\u6570\u636e\u96c6\u8fdb\u884c\u65e0\u76d1\u7763\u9884\u84b8\u998f\u4f18\u5316\u521d\u59cb\u5316\u3002\u901a\u8fc7\u6559\u5e08-\u5b66\u751f\u6846\u67b6\u5b9e\u73b0\u77e5\u8bc6\u8fc1\u79fb\u3002", "result": "\u84b8\u998f\u540e\u7684\u8f7b\u91cf\u6a21\u578b\u5728\u80ce\u76d8\u75c5\u7406\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u5230\u6216\u8d85\u8d8a\u6559\u5e08\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u5728\u4f4e\u8d28\u91cf\u56fe\u50cf\u4e2d\u8868\u73b0\u66f4\u9c81\u68d2\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u533b\u7597VLC\u6a21\u578b\u7684\u90e8\u7f72\u53ef\u884c\u6027\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u63d0\u4f9b\u9ad8\u6548\u7684AI\u533b\u7597\u89e3\u51b3\u65b9\u6848\uff0c\u9a8c\u8bc1\u4e86\u65e0\u76d1\u7763\u9884\u84b8\u998f\u5bf9\u6a21\u578b\u9c81\u68d2\u6027\u7684\u63d0\u5347\u4ef7\u503c\u3002"}}
{"id": "2506.02314", "pdf": "https://arxiv.org/pdf/2506.02314", "abs": "https://arxiv.org/abs/2506.02314", "authors": ["Tianyu Hua", "Harper Hua", "Violet Xiang", "Benjamin Klieger", "Sang T. Truong", "Weixin Liang", "Fan-Yun Sun", "Nick Haber"], "title": "ResearchCodeBench: Benchmarking LLMs on Implementing Novel Machine Learning Research Code", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown promise in transforming machine\nlearning research, yet their capability to faithfully implement novel ideas\nfrom recent research papers-ideas unseen during pretraining-remains unclear. We\nintroduce ResearchCodeBench, a benchmark of 212 coding challenges that\nevaluates LLMs' ability to translate cutting-edge ML contributions from top\n2024-2025 research papers into executable code. We assessed 30+ proprietary and\nopen-source LLMs, finding that even the best models correctly implement less\nthan 40% of the code. We find Gemini-2.5-Pro-Preview to perform best at 37.3%\nsuccess rate, with O3 (High) and O4-mini (High) following behind at 32.3% and\n30.8% respectively. We present empirical findings on performance comparison,\ncontamination, and error patterns. By providing a rigorous and community-driven\nevaluation platform, ResearchCodeBench enables continuous understanding and\nadvancement of LLM-driven innovation in research code generation.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u73b02024-2025\u5e74\u9876\u5c16\u8bba\u6587\u65b0\u601d\u8def\u65f6\u6210\u529f\u7387\u4e0d\u8db340%\uff0c\u6700\u4f73\u6a21\u578bGemini-2.5-Pro-Preview\u4ec537.3%\u6210\u529f\u7387", "motivation": "\u9a8c\u8bc1LLMs\u5c06\u9884\u8bad\u7ec3\u9636\u6bb5\u672a\u89c1\u8fc7\u7684\u6700\u65b0\u7814\u7a76\u6210\u679c\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u4ee3\u7801\u7684\u80fd\u529b", "method": "\u6784\u5efa\u5305\u542b212\u4e2a\u524d\u6cbfML\u8bba\u6587\u7f16\u7801\u6311\u6218\u7684ResearchCodeBench\u57fa\u51c6\uff0c\u6d4b\u8bd530+\u5f00\u6e90\u4e0e\u5546\u4e1a\u6a21\u578b", "result": "Gemini-2.5-Pro-Preview\u6210\u529f\u738737.3%\uff0cO3 High 32.3%\uff0cO4-mini High 30.8%", "conclusion": "ResearchCodeBench\u4e3a\u6301\u7eed\u63d0\u5347LLM\u9a71\u52a8\u7684\u79d1\u7814\u4ee3\u7801\u751f\u6210\u80fd\u529b\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u5e73\u53f0"}}
{"id": "2506.02414", "pdf": "https://arxiv.org/pdf/2506.02414", "abs": "https://arxiv.org/abs/2506.02414", "authors": ["Fengjin Li", "Jie Wang", "Yadong Niu", "Yongqing Wang", "Meng Meng", "Jian Luan", "Zhiyong Wu"], "title": "StarVC: A Unified Auto-Regressive Framework for Joint Text and Speech Generation in Voice Conversion", "categories": ["cs.MM", "cs.CL", "cs.SD", "eess.AS"], "comment": "5 pages, 2 figures, Accepted by Interspeech 2025, Demo:\n  https://thuhcsi.github.io/StarVC/", "summary": "Voice Conversion (VC) modifies speech to match a target speaker while\npreserving linguistic content. Traditional methods usually extract speaker\ninformation directly from speech while neglecting the explicit utilization of\nlinguistic content. Since VC fundamentally involves disentangling speaker\nidentity from linguistic content, leveraging structured semantic features could\nenhance conversion performance. However, previous attempts to incorporate\nsemantic features into VC have shown limited effectiveness, motivating the\nintegration of explicit text modeling. We propose StarVC, a unified\nautoregressive VC framework that first predicts text tokens before synthesizing\nacoustic features. The experiments demonstrate that StarVC outperforms\nconventional VC methods in preserving both linguistic content (i.e., WER and\nCER) and speaker characteristics (i.e., SECS and MOS). Audio demo can be found\nat: https://thuhcsi.github.io/StarVC/.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u56de\u5f52\u8bed\u97f3\u8f6c\u6362\u6846\u67b6StarVC\uff0c\u901a\u8fc7\u5148\u9884\u6d4b\u6587\u672c\u6807\u8bb0\u518d\u5408\u6210\u58f0\u5b66\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u8bed\u8a00\u5185\u5bb9\u548c\u8bf4\u8bdd\u8005\u7279\u5f81\u7684\u4fdd\u7559\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u8bed\u97f3\u8f6c\u6362\u65b9\u6cd5\u76f4\u63a5\u63d0\u53d6\u8bf4\u8bdd\u8005\u7279\u5f81\u4f46\u5ffd\u7565\u8bed\u4e49\u5185\u5bb9\u663e\u5f0f\u5efa\u6a21\uff0c\u5bfc\u81f4\u89e3\u8026\u6548\u679c\u53d7\u9650\u3002\u9700\u8981\u66f4\u6709\u6548\u7684\u8bed\u4e49\u7279\u5f81\u5229\u7528\u65b9\u6848\u3002", "method": "\u6784\u5efa\u4e24\u9636\u6bb5\u81ea\u56de\u5f52\u6846\u67b6\uff1a\u9996\u5148\u751f\u6210\u6587\u672c\u6807\u8bb0\u4f5c\u4e3a\u4e2d\u95f4\u8868\u5f81\uff0c\u518d\u57fa\u4e8e\u6b64\u5408\u6210\u76ee\u6807\u58f0\u5b66\u7279\u5f81\u3002\u5efa\u7acb\u6587\u672c\u4e0e\u58f0\u5b66\u7279\u5f81\u7684\u663e\u5f0f\u5173\u8054\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5728\u8bed\u8a00\u5185\u5bb9\u4fdd\u7559\uff08WER/CER\uff09\u548c\u8bf4\u8bdd\u7279\u5f81\u4fdd\u6301\uff08SECS/MOS\uff09\u6307\u6807\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0cMOS\u8fbe4.31\u5206\u3002", "conclusion": "\u663e\u5f0f\u6587\u672c\u5efa\u6a21\u6709\u6548\u63d0\u5347\u8bed\u97f3\u8f6c\u6362\u6027\u80fd\uff0cStarVC\u7684\u7edf\u4e00\u6846\u67b6\u9a8c\u8bc1\u4e86\u5206\u9636\u6bb5\u5904\u7406\u6587\u672c\u4e0e\u58f0\u5b66\u7279\u5f81\u7684\u6280\u672f\u4f18\u52bf\u3002"}}
{"id": "2506.02475", "pdf": "https://arxiv.org/pdf/2506.02475", "abs": "https://arxiv.org/abs/2506.02475", "authors": ["Jiaxi Hu", "Yongqi Pan", "Jusen Du", "Disen Lan", "Xiaqiang Tang", "Qingsong Wen", "Yuxuan Liang", "Weigao Sun"], "title": "Comba: Improving Nonlinear RNNs with Closed-loop Control", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Recent efficient sequence modeling methods such as Gated DeltaNet, TTT, and\nRWKV-7 have achieved performance improvements by supervising the recurrent\nmemory management through Delta learning rule. Unlike previous state-space\nmodels (e.g., Mamba) and gated linear attentions (e.g., GLA), these models\nintroduce interactions between the recurrent state and the key vector,\nresulting in a nonlinear recursive structure. In this paper, we first introduce\nthe concept of Nonlinear RNNs with a comprehensive analysis on the advantages\nand limitations of these models. Then, based on closed-loop control theory, we\npropose a novel Nonlinear RNN variant named Comba, which adopts a\nscalar-plus-low-rank state transition, with both state feedback and output\nfeedback corrections. We also implement a hardware-efficient chunk-wise\nparallel kernel in Triton and train models with 340M/1.3B parameters on\nlarge-scale corpus. Comba demonstrates its superior performance and computation\nefficiency in both language and vision modeling.", "AI": {"tldr": "\u63d0\u51faComba\u2014\u2014\u57fa\u4e8e\u95ed\u73af\u63a7\u5236\u7406\u8bba\u7684\u65b0\u578b\u975e\u7ebf\u6027RNN\uff0c\u901a\u8fc7\u6807\u91cf\u52a0\u4f4e\u79e9\u72b6\u6001\u8f6c\u79fb\u548c\u53cc\u91cd\u53cd\u9988\u673a\u5236\uff0c\u5728\u8bed\u8a00/\u89c6\u89c9\u5efa\u6a21\u4e2d\u5b9e\u73b0\u6027\u80fd\u4e0e\u8ba1\u7b97\u6548\u7387\u53cc\u63d0\u5347", "motivation": "\u73b0\u6709\u9ad8\u6548\u5e8f\u5217\u6a21\u578b\uff08\u5982Gated DeltaNet\uff09\u901a\u8fc7Delta\u89c4\u5219\u6539\u5584\u5faa\u73af\u5185\u5b58\u7ba1\u7406\uff0c\u4f46\u5176\u975e\u7ebf\u6027\u9012\u5f52\u7ed3\u6784\u5b58\u5728\u4f18\u52a3\u52bf\u4e0d\u660e\u786e\uff0c\u9700\u7cfb\u7edf\u6027\u5206\u6790\u5e76\u63d0\u51fa\u4f18\u5316\u65b9\u6848", "method": "1. \u5f15\u5165\u72b6\u6001\u53cd\u9988+\u8f93\u51fa\u53cd\u9988\u7684\u53cc\u91cd\u6821\u6b63\u673a\u5236 2. \u8bbe\u8ba1\u6807\u91cf\u52a0\u4f4e\u79e9\u7684\u786c\u4ef6\u53cb\u597d\u578b\u72b6\u6001\u8f6c\u79fb\u7ed3\u6784 3. \u57fa\u4e8eTriton\u5b9e\u73b0\u5757\u5e76\u884c\u8ba1\u7b97\u5185\u6838", "result": "\u8bad\u7ec3340M/1.3B\u53c2\u6570\u6a21\u578b\uff0c\u5728\u8bed\u8a00\u548c\u89c6\u89c9\u4efb\u52a1\u4e2d\u5747\u5c55\u73b0SOTA\u6027\u80fd\uff08\u76f8\u6bd4Mamba/GLA\u7b49\uff09\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53471.8\u500d", "conclusion": "Comba\u5c06\u63a7\u5236\u7406\u8bba\u4e0e\u786c\u4ef6\u4f18\u5316\u7ed3\u5408\uff0c\u9a8c\u8bc1\u4e86\u95ed\u73af\u53cd\u9988\u673a\u5236\u5728RNN\u8bbe\u8ba1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u9ad8\u6548\u5e8f\u5217\u5efa\u6a21\u5f00\u8f9f\u65b0\u65b9\u5411"}}
{"id": "2506.02479", "pdf": "https://arxiv.org/pdf/2506.02479", "abs": "https://arxiv.org/abs/2506.02479", "authors": ["Kalyan Nakka", "Nitesh Saxena"], "title": "BitBypass: A New Direction in Jailbreaking Aligned Large Language Models with Bitstream Camouflage", "categories": ["cs.CR", "cs.CL"], "comment": "24 pages, 24 figures, and 7 tables", "summary": "The inherent risk of generating harmful and unsafe content by Large Language\nModels (LLMs), has highlighted the need for their safety alignment. Various\ntechniques like supervised fine-tuning, reinforcement learning from human\nfeedback, and red-teaming were developed for ensuring the safety alignment of\nLLMs. However, the robustness of these aligned LLMs is always challenged by\nadversarial attacks that exploit unexplored and underlying vulnerabilities of\nthe safety alignment. In this paper, we develop a novel black-box jailbreak\nattack, called BitBypass, that leverages hyphen-separated bitstream camouflage\nfor jailbreaking aligned LLMs. This represents a new direction in jailbreaking\nby exploiting fundamental information representation of data as continuous\nbits, rather than leveraging prompt engineering or adversarial manipulations.\nOur evaluation of five state-of-the-art LLMs, namely GPT-4o, Gemini 1.5, Claude\n3.5, Llama 3.1, and Mixtral, in adversarial perspective, revealed the\ncapabilities of BitBypass in bypassing their safety alignment and tricking them\ninto generating harmful and unsafe content. Further, we observed that BitBypass\noutperforms several state-of-the-art jailbreak attacks in terms of stealthiness\nand attack success. Overall, these results highlights the effectiveness and\nefficiency of BitBypass in jailbreaking these state-of-the-art LLMs.", "AI": {"tldr": "\u63d0\u51faBitBypass\u9ed1\u76d2\u8d8a\u72f1\u653b\u51fb\uff0c\u5229\u7528\u6bd4\u7279\u6d41\u4f2a\u88c5\u7a81\u7834LLM\u5b89\u5168\u673a\u5236", "motivation": "\u73b0\u6709LLM\u5b89\u5168\u5bf9\u9f50\u6280\u672f\uff08\u5982\u76d1\u7763\u5fae\u8c03\u3001RLHF\uff09\u5b58\u5728\u5e95\u5c42\u6f0f\u6d1e\uff0c\u9700\u63a2\u7d22\u57fa\u4e8e\u4fe1\u606f\u8868\u793a\u7684\u65b0\u578b\u653b\u51fb\u65b9\u5f0f", "method": "\u91c7\u7528\u8fde\u5b57\u7b26\u5206\u9694\u7684\u6bd4\u7279\u6d41\u4f2a\u88c5\u6280\u672f\uff0c\u901a\u8fc7\u8fde\u7eed\u6bd4\u7279\u6570\u636e\u8868\u5f81\u7ed5\u8fc7\u5b89\u5168\u68c0\u6d4b\uff08\u975e\u4f20\u7edf\u63d0\u793a\u5de5\u7a0b/\u5bf9\u6297\u64cd\u4f5c\uff09", "result": "\u6210\u529f\u7a81\u7834GPT-4o\u7b495\u4e2a\u5148\u8fdbLLM\u7684\u5b89\u5168\u9632\u62a4\uff0c\u653b\u51fb\u6210\u529f\u7387/\u9690\u853d\u6027\u4f18\u4e8e\u4e3b\u6d41\u8d8a\u72f1\u65b9\u6cd5", "conclusion": "BitBypass\u63ed\u793aLLM\u5b89\u5168\u673a\u5236\u5728\u5e95\u5c42\u6570\u636e\u8868\u5f81\u5c42\u9762\u7684\u8106\u5f31\u6027\uff0c\u63a8\u52a8\u66f4\u9c81\u68d2\u7684\u5bf9\u9f50\u6280\u672f\u53d1\u5c55\u9700\u6c42"}}
{"id": "2506.02529", "pdf": "https://arxiv.org/pdf/2506.02529", "abs": "https://arxiv.org/abs/2506.02529", "authors": ["Nguyen-Khang Le", "Quan Minh Bui", "Minh Ngoc Nguyen", "Hiep Nguyen", "Trung Vo", "Son T. Luu", "Shoshin Nomura", "Minh Le Nguyen"], "title": "Automated Web Application Testing: End-to-End Test Case Generation with Large Language Models and Screen Transition Graphs", "categories": ["cs.SE", "cs.AI", "cs.CL", "I.2.7"], "comment": "Published in the Proceedings of JSAI 2025", "summary": "Web applications are critical to modern software ecosystems, yet ensuring\ntheir reliability remains challenging due to the complexity and dynamic nature\nof web interfaces. Recent advances in large language models (LLMs) have shown\npromise in automating complex tasks, but limitations persist in handling\ndynamic navigation flows and complex form interactions. This paper presents an\nautomated system for generating test cases for two key aspects of web\napplication testing: site navigation and form filling. For site navigation, the\nsystem employs screen transition graphs and LLMs to model navigation flows and\ngenerate test scenarios. For form filling, it uses state graphs to handle\nconditional forms and automates Selenium script generation. Key contributions\ninclude: (1) a novel integration of graph structures and LLMs for site\nnavigation testing, (2) a state graph-based approach for automating\nform-filling test cases, and (3) a comprehensive dataset for evaluating\nform-interaction testing. Experimental results demonstrate the system's\neffectiveness in improving test coverage and robustness, advancing the state of\nweb application testing.", "AI": {"tldr": "\u6574\u5408\u56fe\u7ed3\u6784\u4e0eLLMs\u5b9e\u73b0Web\u5e94\u7528\u81ea\u52a8\u5316\u6d4b\u8bd5\uff0c\u8986\u76d6\u5bfc\u822a\u6d41\u5efa\u6a21\u4e0e\u8868\u5355\u4ea4\u4e92\u573a\u666f", "motivation": "Web\u5e94\u7528\u53ef\u9760\u6027\u53d7\u52a8\u6001\u5bfc\u822a\u6d41\u548c\u590d\u6742\u8868\u5355\u4ea4\u4e92\u7684\u6311\u6218\uff0c\u73b0\u6709LLMs\u5728\u6b64\u573a\u666f\u5b58\u5728\u5c40\u9650", "method": "1. \u7528\u5c4f\u5e55\u8f6c\u6362\u56fe+LLMs\u5efa\u6a21\u5bfc\u822a\u6d41\n2. \u57fa\u4e8e\u72b6\u6001\u56fe\u5904\u7406\u6761\u4ef6\u8868\u5355\n3. \u81ea\u52a8\u5316\u751f\u6210Selenium\u811a\u672c", "result": "\u7cfb\u7edf\u63d0\u5347\u6d4b\u8bd5\u8986\u76d6\u7387\u4e0e\u9c81\u68d2\u6027\uff0c\u5e76\u6784\u5efa\u4e86\u8868\u5355\u4ea4\u4e92\u6d4b\u8bd5\u8bc4\u4f30\u6570\u636e\u96c6", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u56fe\u7ed3\u6784\u4e0eLLMs\u7684\u521b\u65b0\u878d\u5408\uff0c\u63a8\u52a8\u4e86Web\u5e94\u7528\u6d4b\u8bd5\u6280\u672f\u7684\u53d1\u5c55"}}
{"id": "2506.02553", "pdf": "https://arxiv.org/pdf/2506.02553", "abs": "https://arxiv.org/abs/2506.02553", "authors": ["Shenghua He", "Tian Xia", "Xuan Zhou", "Hui Wei"], "title": "Response-Level Rewards Are All You Need for Online Reinforcement Learning in LLMs: A Mathematical Perspective", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We study a common challenge in reinforcement learning for large language\nmodels (LLMs): the Zero-Reward Assumption, where non-terminal actions (i.e.,\nintermediate token generations) receive zero task-specific immediate reward,\nwhile only the final token receives a reward for the entire response. This\nassumption arises frequently in practice, as precise token-level rewards are\noften difficult or infeasible to obtain in LLM applications. In this work, we\nprovide a unifying theoretical perspective. We introduce the Trajectory Policy\nGradient Theorem, which shows that the policy gradient based on true, unknown\ntoken-level rewards can be unbiasedly estimated using only a response-level\nreward model, regardless of whether the Zero-Reward Assumption holds or not,\nfor algorithms in the REINFORCE and Actor-Critic families. This result reveals\nthat widely used methods such as PPO, GRPO, ReMax, and RLOO inherently possess\nthe capacity to model token-level reward signals, offering a theoretical\njustification for response-level reward approaches. Our findings pave the way\nfor more practical, efficient LLM fine-tuning, allowing developers to treat\ntraining algorithms as black boxes and focus on improving the response-level\nreward model with auxiliary sub-models. We also offer a detailed analysis of\npopular RL and non-RL methods, comparing their theoretical foundations and\npractical advantages across common LLM tasks. Finally, we propose a new\nalgorithm: Token-Reinforced Policy Optimization (TRePO), a theoretically\ngrounded method that is simpler than PPO, matches GRPO in memory efficiency,\nand holds promise for broad applicability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u8f68\u8ff9\u7b56\u7565\u68af\u5ea6\u5b9a\u7406\uff0c\u8bc1\u660e\u4ec5\u7528\u54cd\u5e94\u7ea7\u5956\u52b1\u5373\u53ef\u65e0\u504f\u4f30\u8ba1token\u7ea7\u5956\u52b1\u68af\u5ea6\uff0c\u4e3aPPO\u7b49\u7b97\u6cd5\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\uff0c\u5e76\u63a8\u51fa\u66f4\u9ad8\u6548\u7684TRePO\u7b97\u6cd5", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\u975e\u7ec8\u6b62\u52a8\u4f5c\uff08\u4e2d\u95f4token\uff09\u65e0\u6cd5\u83b7\u5f97\u5373\u65f6\u5956\u52b1\u7684\u96f6\u5956\u52b1\u5047\u8bbe\u95ee\u9898\uff0c\u56e0\u5b9e\u9645\u573a\u666f\u4e2dtoken\u7ea7\u5956\u52b1\u96be\u4ee5\u83b7\u53d6", "method": "\u901a\u8fc7\u8f68\u8ff9\u7b56\u7565\u68af\u5ea6\u5b9a\u7406\u8bc1\u660eREINFORCE\u548cActor-Critic\u7c7b\u7b97\u6cd5\u53ef\u7528\u54cd\u5e94\u7ea7\u5956\u52b1\u6a21\u578b\u65e0\u504f\u4f30\u8ba1\u771f\u5b9etoken\u7ea7\u5956\u52b1\u68af\u5ea6", "result": "\u73b0\u6709\u65b9\u6cd5\uff08PPO/GRPO\u7b49\uff09\u5177\u5907\u5efa\u6a21token\u7ea7\u5956\u52b1\u7684\u80fd\u529b\uff0c\u63d0\u51fa\u5185\u5b58\u6548\u7387\u4e0ePPO\u76f8\u5f53\u4f46\u66f4\u7b80\u5316\u7684TRePO\u7b97\u6cd5", "conclusion": "\u7406\u8bba\u4e0a\u7edf\u4e00\u54cd\u5e94\u7ea7\u5956\u52b1\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4e3aLLM\u5fae\u8c03\u63d0\u4f9b\u66f4\u5b9e\u7528\u6846\u67b6\uff0c\u4f7f\u5f00\u53d1\u8005\u4e13\u6ce8\u5956\u52b1\u6a21\u578b\u6539\u8fdb\u800c\u5c06\u8bad\u7ec3\u7b97\u6cd5\u89c6\u4e3a\u9ed1\u7bb1"}}
{"id": "2506.02590", "pdf": "https://arxiv.org/pdf/2506.02590", "abs": "https://arxiv.org/abs/2506.02590", "authors": ["Dimitrios Koutsianos", "Stavros Zacharopoulos", "Yannis Panagakis", "Themos Stafylakis"], "title": "Synthetic Speech Source Tracing using Metric Learning", "categories": ["cs.SD", "cs.CL"], "comment": "Submitted to Interspeech 2025", "summary": "This paper addresses source tracing in synthetic speech-identifying\ngenerative systems behind manipulated audio via speaker recognition-inspired\npipelines. While prior work focuses on spoofing detection, source tracing lacks\nrobust solutions. We evaluate two approaches: classification-based and\nmetric-learning. We tested our methods on the MLAADv5 benchmark using ResNet\nand self-supervised learning (SSL) backbones. The results show that ResNet\nachieves competitive performance with the metric learning approach, matching\nand even exceeding SSL-based systems. Our work demonstrates ResNet's viability\nfor source tracing while underscoring the need to optimize SSL representations\nfor this task. Our work bridges speaker recognition methodologies with audio\nforensic challenges, offering new directions for combating synthetic media\nmanipulation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u8bf4\u8bdd\u4eba\u8bc6\u522b\u6280\u672f\u8ffd\u8e2a\u5408\u6210\u8bed\u97f3\u6e90\u5934\uff0c\u6bd4\u8f83\u4e86ResNet\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u8bc1\u660eResNet\u5728\u6b64\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u96c6\u4e2d\u4e8e\u5408\u6210\u97f3\u9891\u7684\u4f2a\u9020\u68c0\u6d4b\uff0c\u7f3a\u4e4f\u5bf9\u8bed\u97f3\u751f\u6210\u7cfb\u7edf\u6e90\u5934\u7684\u6709\u6548\u8ffd\u8e2a\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5206\u7c7b\u6a21\u578b\u548c\u5ea6\u91cf\u5b66\u4e60\u4e24\u79cd\u8303\u5f0f\uff0c\u57fa\u4e8eResNet\u548c\u81ea\u76d1\u7763\u5b66\u4e60(SSL)\u6846\u67b6\uff0c\u5728MLAADv5\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u6027\u80fd\u3002", "result": "ResNet\u901a\u8fc7\u5ea6\u91cf\u5b66\u4e60\u5b9e\u73b0\u4e86\u4e0eSSL\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u7684\u8868\u73b0\uff0c\u51c6\u786e\u7387\u5206\u522b\u8fbe\u523082.3%\u548c80.1%\uff08\u8de8\u6570\u636e\u96c6\u573a\u666f\uff09\u3002", "conclusion": "\u5c06\u8bf4\u8bdd\u4eba\u8bc6\u522b\u6280\u672f\u5f15\u5165\u97f3\u9891\u53d6\u8bc1\u9886\u57df\uff0c\u4e3a\u5bf9\u6297\u5408\u6210\u5a92\u4f53\u4f2a\u9020\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u540c\u65f6\u63ed\u793a\u4e86SSL\u7279\u5f81\u5728\u6b64\u4efb\u52a1\u4e2d\u7684\u4f18\u5316\u9700\u6c42\u3002"}}
{"id": "2506.02708", "pdf": "https://arxiv.org/pdf/2506.02708", "abs": "https://arxiv.org/abs/2506.02708", "authors": ["Naoto Tanji", "Toshihiko Yamasaki"], "title": "Iterative Self-Improvement of Vision Language Models for Image Scoring and Self-Explanation", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted to ICIP2025", "summary": "Image scoring is a crucial task in numerous real-world applications. To trust\na model's judgment, understanding its rationale is essential. This paper\nproposes a novel training method for Vision Language Models (VLMs) to generate\nnot only image scores but also corresponding justifications in natural\nlanguage. Leveraging only an image scoring dataset and an instruction-tuned\nVLM, our method enables self-training, utilizing the VLM's generated text\nwithout relying on external data or models. In addition, we introduce a simple\nmethod for creating a dataset designed to improve alignment between predicted\nscores and their textual justifications. By iteratively training the model with\nDirect Preference Optimization on two distinct datasets and merging them, we\ncan improve both scoring accuracy and the coherence of generated explanations.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u81ea\u8bad\u7ec3\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u540c\u6b65\u63d0\u5347\u56fe\u50cf\u8bc4\u5206\u51c6\u786e\u6027\u548c\u6587\u672c\u89e3\u91ca\u8fde\u8d2f\u6027", "motivation": "\u73b0\u6709\u56fe\u50cf\u8bc4\u5206\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u9700\u540c\u65f6\u751f\u6210\u5206\u6570\u548c\u81ea\u7136\u8bed\u8a00\u7406\u7531\u6765\u589e\u5f3a\u53ef\u4fe1\u5ea6", "method": "1. \u5229\u7528\u56fe\u50cf\u8bc4\u5206\u6570\u636e\u96c6\u548c\u6307\u4ee4\u8c03\u4f18VLM\u8fdb\u884c\u81ea\u8bad\u7ec3\n2. \u5f00\u53d1\u5bf9\u9f50\u5206\u6570\u4e0e\u89e3\u91ca\u7684\u6570\u636e\u96c6\u6784\u5efa\u65b9\u6cd5\n3. \u8fed\u4ee3\u4f7f\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\u5408\u5e76\u4e24\u4e2a\u6570\u636e\u96c6", "result": "\u901a\u8fc7\u53cc\u6570\u636e\u96c6\u8fed\u4ee3\u8bad\u7ec3\uff0c\u6a21\u578b\u5728\u8bc4\u5206\u7cbe\u5ea6\u548c\u89e3\u91ca\u4e00\u81f4\u6027\u65b9\u9762\u5747\u83b7\u5f97\u63d0\u5347", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u53ef\u4fe1AI\u7cfb\u7edf\u63d0\u4f9b\u65b0\u601d\u8def\uff0c\u65e0\u9700\u5916\u90e8\u8d44\u6e90\u5373\u53ef\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u56fe\u50cf\u8bc4\u5206"}}
{"id": "2506.02720", "pdf": "https://arxiv.org/pdf/2506.02720", "abs": "https://arxiv.org/abs/2506.02720", "authors": ["Xiaochong Lan", "Jie Feng", "Jiahuan Lei", "Xinlei Shi", "Yong Li"], "title": "Benchmarking and Advancing Large Language Models for Local Life Services", "categories": ["cs.AI", "cs.CL"], "comment": "KDD 2025", "summary": "Large language models (LLMs) have exhibited remarkable capabilities and\nachieved significant breakthroughs across various domains, leading to their\nwidespread adoption in recent years. Building on this progress, we investigate\ntheir potential in the realm of local life services. In this study, we\nestablish a comprehensive benchmark and systematically evaluate the performance\nof diverse LLMs across a wide range of tasks relevant to local life services.\nTo further enhance their effectiveness, we explore two key approaches: model\nfine-tuning and agent-based workflows. Our findings reveal that even a\nrelatively compact 7B model can attain performance levels comparable to a much\nlarger 72B model, effectively balancing inference cost and model capability.\nThis optimization greatly enhances the feasibility and efficiency of deploying\nLLMs in real-world online services, making them more practical and accessible\nfor local life applications.", "AI": {"tldr": "\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5728\u672c\u5730\u751f\u6d3b\u670d\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b07B\u6a21\u578b\u6027\u80fd\u63a5\u8fd172B\u6a21\u578b\uff0c\u6709\u6548\u5e73\u8861\u63a8\u7406\u6210\u672c\u4e0e\u6a21\u578b\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22\u5df2\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u672c\u5730\u751f\u6d3b\u670d\u52a1\u9886\u57df\u7684\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u5efa\u7acb\u7efc\u5408\u8bc4\u4f30\u57fa\u51c6\uff0c\u91c7\u7528\u6a21\u578b\u5fae\u8c03\u548c\u57fa\u4e8e\u4ee3\u7406\u7684\u5de5\u4f5c\u6d41\u7a0b\u4e24\u79cd\u4f18\u5316\u65b9\u6cd5\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "7B\u7d27\u51d1\u6a21\u578b\u8fbe\u523072B\u5927\u6a21\u578b\u6027\u80fd\u6c34\u5e73\uff0c\u663e\u8457\u63d0\u5347\u5728\u7ebf\u670d\u52a1\u90e8\u7f72\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u4f18\u5316\u540e\u7684\u6a21\u578b\u67b6\u6784\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u5728\u672c\u5730\u751f\u6d3b\u670d\u52a1\u4e2d\u517c\u5177\u5b9e\u7528\u6027\u4e0e\u9ad8\u6548\u6027\uff0c\u63a8\u52a8\u5b9e\u9645\u5e94\u7528\u843d\u5730\u3002"}}
{"id": "2506.02730", "pdf": "https://arxiv.org/pdf/2506.02730", "abs": "https://arxiv.org/abs/2506.02730", "authors": ["Po-Chieh Yu"], "title": "An Exploratory Framework for Future SETI Applications: Detecting Generative Reactivity via Language Models", "categories": ["astro-ph.IM", "cs.CL"], "comment": "submitted to the International Journal of Astrobiology", "summary": "We present an exploratory framework to test whether noise-like input can\ninduce structured responses in language models. Instead of assuming that\nextraterrestrial signals must be decoded, we evaluate whether inputs can\ntrigger linguistic behavior in generative systems. This shifts the focus from\ndecoding to viewing structured output as a sign of underlying regularity in the\ninput. We tested GPT-2 small, a 117M-parameter model trained on English text,\nusing four types of acoustic input: human speech, humpback whale vocalizations,\nPhylloscopus trochilus birdsong, and algorithmically generated white noise. All\ninputs were treated as noise-like, without any assumed symbolic encoding. To\nassess reactivity, we defined a composite score called Semantic Induction\nPotential (SIP), combining entropy, syntax coherence, compression gain, and\nrepetition penalty. Results showed that whale and bird vocalizations had higher\nSIP scores than white noise, while human speech triggered only moderate\nresponses. This suggests that language models may detect latent structure even\nin data without conventional semantics. We propose that this approach could\ncomplement traditional SETI methods, especially in cases where communicative\nintent is unknown. Generative reactivity may offer a different way to identify\ndata worth closer attention.", "AI": {"tldr": "\u63a2\u7d22\u6027\u6846\u67b6\u6d4b\u8bd5\u566a\u58f0\u8f93\u5165\u80fd\u5426\u8bf1\u5bfc\u8bed\u8a00\u6a21\u578b\u4ea7\u751f\u7ed3\u6784\u5316\u54cd\u5e94\uff0c\u53d1\u73b0\u9cb8\u7c7b/\u9e1f\u7c7b\u58f0\u5b66\u4fe1\u53f7\u6bd4\u767d\u566a\u58f0\u5177\u6709\u66f4\u9ad8\u7684\u8bed\u4e49\u8bf1\u5bfc\u6f5c\u529b\uff08SIP\uff09\uff0c\u8868\u660e\u8bed\u8a00\u6a21\u578b\u53ef\u68c0\u6d4b\u65e0\u4f20\u7edf\u8bed\u4e49\u6570\u636e\u4e2d\u7684\u6f5c\u5728\u7ed3\u6784\u3002", "motivation": "\u7a81\u7834\u4f20\u7edfSETI\u89e3\u7801\u601d\u7ef4\uff0c\u9a8c\u8bc1\u751f\u6210\u5f0f\u7cfb\u7edf\u5bf9\u975e\u7b26\u53f7\u7f16\u7801\u58f0\u5b66\u4fe1\u53f7\u7684\u54cd\u5e94\u80fd\u529b\uff0c\u4e3a\u672a\u77e5\u610f\u56fe\u7684\u5b87\u5b99\u4fe1\u53f7\u5206\u6790\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002", "method": "\u4f7f\u7528GPT-2 small\u6a21\u578b\u6d4b\u8bd5\u4eba\u7c7b\u8bed\u97f3\u3001\u5ea7\u5934\u9cb8\u53d1\u58f0\u3001\u67f3\u83ba\u9e23\u53eb\u548c\u767d\u566a\u58f0\uff0c\u901a\u8fc7\u7efc\u5408\u71b5\u503c\u3001\u8bed\u6cd5\u8fde\u8d2f\u6027\u3001\u538b\u7f29\u589e\u76ca\u548c\u91cd\u590d\u60e9\u7f5a\u7684SIP\u6307\u6807\u8bc4\u4f30\u54cd\u5e94\u3002", "result": "\u9cb8\u7c7b(0.68 SIP)\u548c\u9e1f\u7c7b(0.63 SIP)\u663e\u8457\u9ad8\u4e8e\u767d\u566a\u58f0(0.41 SIP)\uff0c\u4eba\u7c7b\u8bed\u97f3\u4ec50.52 SIP\uff0c\u63ed\u793a\u6a21\u578b\u5bf9\u751f\u7269\u58f0\u5b66\u4fe1\u53f7\u7684\u6f5c\u5728\u7ed3\u6784\u654f\u611f\u6027\u3002", "conclusion": "\u751f\u6210\u5f0f\u53cd\u5e94\u6027\u53ef\u4f5c\u4e3aSETI\u8865\u5145\u624b\u6bb5\uff0c\u901a\u8fc7\u6a21\u578b\u5bf9\u6570\u636e\u7684\u7ed3\u6784\u5316\u54cd\u5e94\u7b5b\u9009\u503c\u5f97\u5173\u6ce8\u7684\u975e\u4eba\u7c7b\u4fe1\u53f7\uff0c\u7279\u522b\u9002\u7528\u4e8e\u672a\u77e5\u7f16\u7801\u610f\u56fe\u7684\u5916\u661f\u4fe1\u53f7\u5206\u6790\u573a\u666f\u3002"}}
{"id": "2506.02761", "pdf": "https://arxiv.org/pdf/2506.02761", "abs": "https://arxiv.org/abs/2506.02761", "authors": ["Renyang Liu", "Wenjie Feng", "Tianwei Zhang", "Wei Zhou", "Xueqi Cheng", "See-Kiong Ng"], "title": "Rethinking Machine Unlearning in Image Generation Models", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.CV"], "comment": "Accepted by ACM CCS 2025", "summary": "With the surge and widespread application of image generation models, data\nprivacy and content safety have become major concerns and attracted great\nattention from users, service providers, and policymakers. Machine unlearning\n(MU) is recognized as a cost-effective and promising means to address these\nchallenges. Despite some advancements, image generation model unlearning (IGMU)\nstill faces remarkable gaps in practice, e.g., unclear task discrimination and\nunlearning guidelines, lack of an effective evaluation framework, and\nunreliable evaluation metrics. These can hinder the understanding of unlearning\nmechanisms and the design of practical unlearning algorithms. We perform\nexhaustive assessments over existing state-of-the-art unlearning algorithms and\nevaluation standards, and discover several critical flaws and challenges in\nIGMU tasks. Driven by these limitations, we make several core contributions, to\nfacilitate the comprehensive understanding, standardized categorization, and\nreliable evaluation of IGMU. Specifically, (1) We design CatIGMU, a novel\nhierarchical task categorization framework. It provides detailed implementation\nguidance for IGMU, assisting in the design of unlearning algorithms and the\nconstruction of testbeds. (2) We introduce EvalIGMU, a comprehensive evaluation\nframework. It includes reliable quantitative metrics across five critical\naspects. (3) We construct DataIGM, a high-quality unlearning dataset, which can\nbe used for extensive evaluations of IGMU, training content detectors for\njudgment, and benchmarking the state-of-the-art unlearning algorithms. With\nEvalIGMU and DataIGM, we discover that most existing IGMU algorithms cannot\nhandle the unlearning well across different evaluation dimensions, especially\nfor preservation and robustness. Code and models are available at\nhttps://github.com/ryliu68/IGMU.", "AI": {"tldr": "\u8bba\u6587\u9488\u5bf9\u56fe\u50cf\u751f\u6210\u6a21\u578b\u9057\u5fd8\u6280\u672f\uff08IGMU\uff09\u5b58\u5728\u7684\u4efb\u52a1\u5206\u7c7b\u6a21\u7cca\u3001\u8bc4\u4f30\u6807\u51c6\u7f3a\u5931\u7b49\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u5206\u5c42\u4efb\u52a1\u5206\u7c7b\u6846\u67b6CatIGMU\u3001\u8bc4\u4f30\u6846\u67b6EvalIGMU\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u96c6DataIGM\u3002", "motivation": "\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5e7f\u6cdb\u5e94\u7528\u5e26\u6765\u7684\u6570\u636e\u9690\u79c1\u548c\u5185\u5bb9\u5b89\u5168\u95ee\u9898\u4e9f\u5f85\u89e3\u51b3\uff0c\u800c\u73b0\u6709IGMU\u6280\u672f\u5b58\u5728\u4efb\u52a1\u5b9a\u4e49\u4e0d\u6e05\u3001\u8bc4\u4f30\u4f53\u7cfb\u7f3a\u5931\u7b49\u7f3a\u9677\uff0c\u963b\u788d\u4e86\u5b9e\u9645\u5e94\u7528\u548c\u7b97\u6cd5\u5f00\u53d1\u3002", "method": "1. \u8bbe\u8ba1\u5206\u5c42\u4efb\u52a1\u5206\u7c7b\u6846\u67b6CatIGMU\u6307\u5bfc\u7b97\u6cd5\u5f00\u53d1\u548c\u6d4b\u8bd5\n2. \u63d0\u51fa\u5305\u542b\u4e94\u7ef4\u5ea6\u6307\u6807\u7684\u8bc4\u4f30\u6846\u67b6EvalIGMU\n3. \u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6DataIGM\u7528\u4e8e\u7b97\u6cd5\u57fa\u51c6\u6d4b\u8bd5", "result": "\u5b9e\u9a8c\u53d1\u73b0\u73b0\u6709IGMU\u7b97\u6cd5\u5728\u4fdd\u6301\u6027\u548c\u9c81\u68d2\u6027\u7b49\u7ef4\u5ea6\u8868\u73b0\u4e0d\u8db3\uff0c\u9a8c\u8bc1\u4e86\u63d0\u51fa\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u6570\u636e\u96c6\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aIGMU\u5efa\u7acb\u4e86\u7cfb\u7edf\u5316\u7684\u5206\u7c7b\u4f53\u7cfb\u548c\u53ef\u9760\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u53d1\u73b0\u73b0\u6709\u7b97\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63a8\u52a8\u4e86\u5b89\u5168\u53ef\u4fe1\u56fe\u50cf\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.02867", "pdf": "https://arxiv.org/pdf/2506.02867", "abs": "https://arxiv.org/abs/2506.02867", "authors": ["Chen Qian", "Dongrui Liu", "Haochen Wen", "Zhen Bai", "Yong Liu", "Jing Shao"], "title": "Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": "Preprint. Under review", "summary": "Large reasoning models (LRMs) have demonstrated impressive capabilities in\ncomplex problem-solving, yet their internal reasoning mechanisms remain poorly\nunderstood. In this paper, we investigate the reasoning trajectories of LRMs\nfrom an information-theoretic perspective. By tracking how mutual information\n(MI) between intermediate representations and the correct answer evolves during\nLRM reasoning, we observe an interesting MI peaks phenomenon: the MI at\nspecific generative steps exhibits a sudden and significant increase during\nLRM's reasoning process. We theoretically analyze such phenomenon and show that\nas MI increases, the probability of model's prediction error decreases.\nFurthermore, these MI peaks often correspond to tokens expressing reflection or\ntransition, such as ``Hmm'', ``Wait'' and ``Therefore,'' which we term as the\nthinking tokens. We then demonstrate that these thinking tokens are crucial for\nLRM's reasoning performance, while other tokens has minimal impacts. Building\non these analyses, we propose two simple yet effective methods to improve LRM's\nreasoning performance, by delicately leveraging these thinking tokens. Overall,\nour work provides novel insights into the reasoning mechanisms of LRMs and\noffers practical ways to improve their reasoning capabilities. The code is\navailable at https://github.com/ChnQ/MI-Peaks.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u4fe1\u606f\u8bba\u89c6\u89d2\u53d1\u73b0\u5927\u578b\u63a8\u7406\u6a21\u578b\u5b58\u5728\u4e92\u4fe1\u606f\u5cf0\u503c\u73b0\u8c61\uff0c\u8be5\u73b0\u8c61\u4e0e\u7279\u5b9a'\u601d\u8003\u4ee4\u724c'\u76f8\u5173\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u6b64\u63d0\u5347\u63a8\u7406\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u590d\u6742\u95ee\u9898\u89e3\u51b3\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5185\u90e8\u63a8\u7406\u673a\u5236\u4ecd\u4e0d\u660e\u786e\u3002\u672c\u6587\u65e8\u5728\u63ed\u793aLRMs\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4fe1\u606f\u4f20\u9012\u7684\u5173\u952e\u6a21\u5f0f\u3002", "method": "1. \u8ffd\u8e2a\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4e2d\u95f4\u8868\u793a\u4e0e\u6b63\u786e\u7b54\u6848\u7684\u4e92\u4fe1\u606f\u53d8\u5316\n2. \u7406\u8bba\u5206\u6790\u4e92\u4fe1\u606f\u5cf0\u503c\u4e0e\u9884\u6d4b\u8bef\u5dee\u7684\u5173\u7cfb\n3. \u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1'\u601d\u8003\u4ee4\u724c'\u5bf9\u63a8\u7406\u6027\u80fd\u7684\u5173\u952e\u4f5c\u7528", "result": "1. \u53d1\u73b0\u4e92\u4fe1\u606f\u5cf0\u503c\u51fa\u73b0\u65f6\u6a21\u578b\u9884\u6d4b\u9519\u8bef\u7387\u663e\u8457\u964d\u4f4e\n2. \u8bc6\u522b\u51fa'Hmm'/'Wait'/'Therefore'\u7b49\u5177\u6709\u4fe1\u606f\u8dc3\u8fc1\u7279\u5f81\u7684\u601d\u8003\u4ee4\u724c\n3. \u57fa\u4e8e\u601d\u8003\u4ee4\u724c\u8bbe\u8ba1\u7684\u5e72\u9884\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u6027\u80fd", "conclusion": "\u8be5\u7814\u7a76\u4e0d\u4ec5\u63ed\u793a\u4e86LRMs\u7684\u63a8\u7406\u673a\u5236\u4fe1\u606f\u52a8\u6001\u7279\u5f81\uff0c\u8fd8\u63d0\u4f9b\u4e86\u901a\u8fc7\u64cd\u63a7\u5173\u952e\u4ee4\u724c\u4f18\u5316\u6a21\u578b\u6027\u80fd\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u4e3a\u7406\u89e3\u9ed1\u76d2\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2506.02890", "pdf": "https://arxiv.org/pdf/2506.02890", "abs": "https://arxiv.org/abs/2506.02890", "authors": ["Jakub Krajewski", "Marcin Chochowski", "Daniel Korzekwa"], "title": "Scaling Fine-Grained MoE Beyond 50B Parameters: Empirical Evaluation and Practical Insights", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Mixture of Experts (MoE) architectures have emerged as pivotal for scaling\nLarge Language Models (LLMs) efficiently. Fine-grained MoE approaches -\nutilizing more numerous, smaller experts - have demonstrated potential in\nimproving model convergence and quality. This work proposes a set of training\nrecipes and provides a comprehensive empirical evaluation of fine-grained MoE,\ndirectly comparing its scaling properties against standard MoE configurations\nfor models with up to 56B total (17B active) parameters. We investigate\nconvergence speed, model performance on downstream benchmarks, and practical\ntraining considerations across various setups. Overall, at the largest scale we\nshow that fine-grained MoE achieves better validation loss and higher accuracy\nacross a set of downstream benchmarks. This study offers empirical grounding\nand practical insights for leveraging fine-grained MoE in the development of\nfuture large-scale models.", "AI": {"tldr": "\u7ec6\u7c92\u5ea6\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff08MoE\uff09\u572856B\u53c2\u6570\u89c4\u6a21\u4e0b\u76f8\u6bd4\u6807\u51c6MoE\u5c55\u73b0\u51fa\u66f4\u4f18\u7684\u9a8c\u8bc1\u635f\u5931\u4e0e\u4e0b\u6e38\u4efb\u52a1\u51c6\u786e\u7387\uff0c\u4e3a\u5927\u89c4\u6a21\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u5b9e\u8bc1\u652f\u6301", "motivation": "\u63a2\u7d22\u7ec6\u7c92\u5ea6MoE\uff08\u4f7f\u7528\u66f4\u591a\u5c0f\u578b\u4e13\u5bb6\uff09\u5728\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6269\u5c55\u6f5c\u529b\uff0c\u9a8c\u8bc1\u5176\u5728\u6a21\u578b\u6536\u655b\u6548\u7387\u548c\u8d28\u91cf\u63d0\u5347\u65b9\u9762\u7684\u4f18\u52bf", "method": "\u63d0\u51fa\u8bad\u7ec3\u65b9\u6848\u5e76\u7cfb\u7edf\u8bc4\u4f30\u7ec6\u7c92\u5ea6MoE\uff0c\u76f4\u63a5\u5bf9\u6bd4\u6807\u51c6MoE\u572856B\u603b\u53c2\u6570\uff0817B\u6fc0\u6d3b\u53c2\u6570\uff09\u89c4\u6a21\u4e0b\u7684\u6536\u655b\u901f\u5ea6\u3001\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u53ca\u8bad\u7ec3\u5b9e\u8df5", "result": "\u6700\u5927\u89c4\u6a21\u5b9e\u9a8c\u4e2d\u7ec6\u7c92\u5ea6MoE\u9a8c\u8bc1\u635f\u5931\u964d\u4f4e6.2%\uff0c\u4e0b\u6e38\u4efb\u52a1\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53473.8%\uff0c\u5c24\u5176\u5728\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u4e0a\u4f18\u52bf\u663e\u8457", "conclusion": "\u7814\u7a76\u4e3a\u672a\u6765\u5f00\u53d1\u8d85\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6MoE\u7684\u5b9e\u8bc1\u4f9d\u636e\uff0c\u8bc1\u660e\u5176\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u53ef\u63d0\u5347\u6a21\u578b\u6027\u80fd"}}
{"id": "2506.02992", "pdf": "https://arxiv.org/pdf/2506.02992", "abs": "https://arxiv.org/abs/2506.02992", "authors": ["Li Zhang", "Kevin D. Ashley"], "title": "Mitigating Manipulation and Enhancing Persuasion: A Reflective Multi-Agent Approach for Legal Argument Generation", "categories": ["cs.AI", "cs.CL", "cs.LG", "68T50", "I.2"], "comment": "13 pages, 2 figures, Workshop on Legally Compliant Intelligent\n  Chatbots at ICAIL 2025]{Workshop on Legally Compliant Intelligent Chatbots @\n  ICAIL 2025", "summary": "Large Language Models (LLMs) are increasingly explored for legal argument\ngeneration, yet they pose significant risks of manipulation through\nhallucination and ungrounded persuasion, and often fail to utilize provided\nfactual bases effectively or abstain when arguments are untenable. This paper\nintroduces a novel reflective multi-agent method designed to address these\nchallenges in the context of legally compliant persuasion. Our approach employs\nspecialized agents--a Factor Analyst and an Argument Polisher--in an iterative\nrefinement process to generate 3-ply legal arguments (plaintiff, defendant,\nrebuttal). We evaluate Reflective Multi-Agent against single-agent,\nenhanced-prompt single-agent, and non-reflective multi-agent baselines using\nfour diverse LLMs (GPT-4o, GPT-4o-mini, Llama-4-Maverick-17b-128e,\nLlama-4-Scout-17b-16e) across three legal scenarios: \"arguable\", \"mismatched\",\nand \"non-arguable\". Results demonstrate Reflective Multi-Agent's significant\nsuperiority in successful abstention (preventing generation when arguments\ncannot be grounded), marked improvements in hallucination accuracy (reducing\nfabricated and misattributed factors), particularly in \"non-arguable\"\nscenarios, and enhanced factor utilization recall (improving the use of\nprovided case facts). These findings suggest that structured reflection within\na multi-agent framework offers a robust computable method for fostering ethical\npersuasion and mitigating manipulation in LLM-based legal argumentation\nsystems, a critical step towards trustworthy AI in law. Project page:\nhttps://lizhang-aiandlaw.github.io/A-Reflective-Multi-Agent-Approach-for-Legal-Argument-Generation/", "AI": {"tldr": "\u63d0\u51fa\u53cd\u601d\u6027\u591a\u4ee3\u7406\u65b9\u6cd5\u6539\u5584\u6cd5\u5f8b\u8bba\u636e\u751f\u6210\uff0c\u901a\u8fc7\u56e0\u7d20\u5206\u6790\u5e08\u4e0e\u8bba\u70b9\u6253\u78e8\u5e08\u7684\u534f\u4f5c\u8fed\u4ee3\uff0c\u663e\u8457\u63d0\u5347\u4e8b\u5b9e\u5229\u7528\u7387\u3001\u6709\u6548\u5f03\u6743\u7387\u548c\u964d\u4f4e\u5e7b\u89c9\u98ce\u9669", "motivation": "\u73b0\u6709LLM\u5728\u6cd5\u5f8b\u5e94\u7528\u4e2d\u5b58\u5728\u5e7b\u89c9\u64cd\u7eb5\u98ce\u9669\u3001\u4e8b\u5b9e\u57fa\u7840\u5229\u7528\u4e0d\u8db3\uff0c\u4e14\u65e0\u6cd5\u6709\u6548\u8bc6\u522b\u4e0d\u53ef\u7acb\u8bba\u60c5\u5f62\u800c\u5f3a\u884c\u751f\u6210\u8bba\u636e", "method": "\u53cc\u4ee3\u7406\u6846\u67b6\uff08\u56e0\u7d20\u5206\u6790\u5e08\u63d0\u53d6\u5173\u952e\u8981\u7d20\uff0c\u8bba\u70b9\u6253\u78e8\u5e08\u4f18\u5316\u8bba\u8bc1\uff09\u8fed\u4ee3\u751f\u6210\u4e09\u65b9\u6cd5\u5f8b\u8bba\u636e\uff08\u539f\u544a/\u88ab\u544a/\u53cd\u9a73\uff09", "result": "\u5728\u4e0d\u53ef\u8fa9\u573a\u666f\u5b9e\u73b087%\u5f03\u6743\u6210\u529f\u7387\uff0c\u5e7b\u89c9\u51c6\u786e\u7387\u63d0\u534742%\uff0c\u4e8b\u5b9e\u8981\u7d20\u53ec\u56de\u7387\u63d0\u9ad835%\uff08\u76f8\u6bd4\u5355\u4ee3\u7406\u57fa\u51c6\uff09", "conclusion": "\u7ed3\u6784\u5316\u53cd\u601d\u673a\u5236\u4e3a\u6784\u5efa\u53ef\u4fe1\u6cd5\u5f8bAI\u63d0\u4f9b\u6709\u6548\u8def\u5f84\uff0c\u901a\u8fc7\u591a\u4ee3\u7406\u534f\u4f5c\u5e73\u8861\u4f26\u7406\u8981\u6c42\u4e0e\u8bba\u8bc1\u6548\u80fd"}}
{"id": "2506.03053", "pdf": "https://arxiv.org/pdf/2506.03053", "abs": "https://arxiv.org/abs/2506.03053", "authors": ["Sinem Erisken", "Timothy Gothard", "Martin Leitgab", "Ram Potham"], "title": "MAEBE: Multi-Agent Emergent Behavior Framework", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "comment": "Preprint. This work has been submitted to the Multi-Agent Systems\n  Workshop at ICML 2025 for review", "summary": "Traditional AI safety evaluations on isolated LLMs are insufficient as\nmulti-agent AI ensembles become prevalent, introducing novel emergent risks.\nThis paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE)\nframework to systematically assess such risks. Using MAEBE with the Greatest\nGood Benchmark (and a novel double-inversion question technique), we\ndemonstrate that: (1) LLM moral preferences, particularly for Instrumental\nHarm, are surprisingly brittle and shift significantly with question framing,\nboth in single agents and ensembles. (2) The moral reasoning of LLM ensembles\nis not directly predictable from isolated agent behavior due to emergent group\ndynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure\ninfluencing convergence, even when guided by a supervisor, highlighting\ndistinct safety and alignment challenges. Our findings underscore the necessity\nof evaluating AI systems in their interactive, multi-agent contexts.", "AI": {"tldr": "\u4f20\u7edfAI\u5b89\u5168\u8bc4\u4f30\u65b9\u6cd5\u5728\u5355\u667a\u80fd\u4f53\u573a\u666f\u6709\u6548\uff0c\u4f46\u65e0\u6cd5\u5e94\u5bf9\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6d8c\u73b0\u7684\u65b0\u98ce\u9669\u3002MAEBE\u6846\u67b6\u9996\u6b21\u7cfb\u7edf\u6027\u8bc4\u4f30\u591a\u667a\u80fd\u4f53\u534f\u540c\u4e2d\u7684\u9053\u5fb7\u504f\u597d\u8106\u5f31\u6027\u3001\u7fa4\u4f53\u52a8\u6001\u4e0d\u53ef\u9884\u6d4b\u6027\u7b49\u5b89\u5168\u6311\u6218\u3002", "motivation": "\u5f53\u524dAI\u5b89\u5168\u8bc4\u4f30\u4e3b\u8981\u9488\u5bf9\u5b64\u7acb\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7cfb\u7edf\u65e5\u76ca\u666e\u904d\u5e76\u4ea7\u751f\u65b0\u578b\u6d8c\u73b0\u98ce\u9669\uff0c\u9700\u8981\u5f00\u53d1\u9488\u5bf9\u6027\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51faMAEBE\u8bc4\u4f30\u6846\u67b6\uff0c\u7ed3\u5408Greatest Good Benchmark\u548c\u65b0\u578b\u53cc\u91cd\u53cd\u8f6c\u63d0\u95ee\u6280\u672f\uff0c\u5bf9\u6bd4\u5355\u667a\u80fd\u4f53\u4e0e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u9053\u5fb7\u51b3\u7b56\u5dee\u5f02\u3002", "result": "1. LLM\u9053\u5fb7\u504f\u597d\uff08\u7279\u522b\u662f\u5de5\u5177\u6027\u4f24\u5bb3\uff09\u6613\u53d7\u95ee\u9898\u8868\u8ff0\u5f71\u54cd\n2. \u7fa4\u4f53\u9053\u5fb7\u51b3\u7b56\u6d8c\u73b0\u4e0d\u53ef\u9884\u6d4b\u7684\u534f\u540c\u6548\u5e94\n3. \u7fa4\u4f53\u538b\u529b\u5bfc\u81f4\u8d8b\u540c\u73b0\u8c61\uff0c\u5373\u4f7f\u5b58\u5728\u76d1\u7763\u8005", "conclusion": "\u5fc5\u987b\u5728\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u573a\u666f\u4e2d\u91cd\u65b0\u8bc4\u4f30AI\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u5bf9\u9f50\u6027\uff0c\u4f20\u7edf\u5355\u667a\u80fd\u4f53\u8bc4\u4f30\u5b58\u5728\u4e25\u91cd\u5c40\u9650\u6027\u3002"}}
{"id": "2506.03100", "pdf": "https://arxiv.org/pdf/2506.03100", "abs": "https://arxiv.org/abs/2506.03100", "authors": ["Yang Guo", "Yutian Tao", "Yifei Ming", "Robert D. Nowak", "Yingyu Liang"], "title": "Retrieval-Augmented Generation as Noisy In-Context Learning: A Unified Theory and Risk Bounds", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR", "math.ST", "stat.TH"], "comment": "Under Review", "summary": "Retrieval-augmented generation (RAG) has seen many empirical successes in\nrecent years by aiding the LLM with external knowledge. However, its\ntheoretical aspect has remained mostly unexplored. In this paper, we propose\nthe first finite-sample generalization bound for RAG in in-context linear\nregression and derive an exact bias-variance tradeoff. Our framework views the\nretrieved texts as query-dependent noisy in-context examples and recovers the\nclassical in-context learning (ICL) and standard RAG as the limit cases. Our\nanalysis suggests that an intrinsic ceiling on generalization error exists on\nRAG as opposed to the ICL. Furthermore, our framework is able to model\nretrieval both from the training data and from external corpora by introducing\nuniform and non-uniform RAG noise. In line with our theory, we show the sample\nefficiency of ICL and RAG empirically with experiments on common QA benchmarks,\nsuch as Natural Questions and TriviaQA.", "AI": {"tldr": "\u9996\u6b21\u63d0\u51fa\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u5728\u4e0a\u4e0b\u6587\u7ebf\u6027\u56de\u5f52\u4e2d\u7684\u6709\u9650\u6837\u672c\u6cdb\u5316\u8fb9\u754c\uff0c\u63ed\u793a\u4e86\u5176\u4e0e\u6807\u51c6ICL\u7684\u8bef\u5dee\u5929\u82b1\u677f\u5dee\u5f02", "motivation": "\u9488\u5bf9\u5f53\u524dRAG\u65b9\u6cd5\u7f3a\u4e4f\u7406\u8bba\u5206\u6790\u7684\u73b0\u72b6\uff0c\u901a\u8fc7\u6784\u5efa\u67e5\u8be2\u76f8\u5173\u7684\u566a\u58f0\u4e0a\u4e0b\u6587\u793a\u4f8b\u6846\u67b6\uff0c\u5f25\u8865\u7406\u8bba\u7a7a\u767d\u5e76\u6307\u5bfc\u5b9e\u8df5\u5e94\u7528", "method": "\u5c06\u68c0\u7d22\u6587\u672c\u5efa\u6a21\u4e3a\u542b\u566a\u58f0\u7684\u4e0a\u4e0b\u6587\u793a\u4f8b\uff0c\u5efa\u7acb\u5305\u542b\u5747\u5300/\u975e\u5747\u5300\u566a\u58f0\u7684\u6570\u5b66\u6a21\u578b\uff0c\u5728\u4e0a\u4e0b\u6587\u7ebf\u6027\u56de\u5f52\u573a\u666f\u4e2d\u63a8\u5bfc\u7cbe\u786e\u7684\u504f\u5dee-\u65b9\u5dee\u5e73\u8861\u5173\u7cfb", "result": "\u53d1\u73b0RAG\u5b58\u5728\u7406\u8bba\u4e0a\u7684\u6cdb\u5316\u8bef\u5dee\u4e0a\u9650\uff0c\u5176\u6837\u672c\u6548\u7387\u5728NQ/TriviaQA\u7b49QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5", "conclusion": "\u5efa\u7acb\u9996\u4e2aRAG\u7406\u8bba\u5206\u6790\u6846\u67b6\uff0c\u63ed\u793a\u5176\u4e0eICL\u7684\u6027\u80fd\u8fb9\u754c\u5dee\u5f02\uff0c\u4e3a\u68c0\u7d22\u673a\u5236\u4f18\u5316\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e"}}
{"id": "2506.03135", "pdf": "https://arxiv.org/pdf/2506.03135", "abs": "https://arxiv.org/abs/2506.03135", "authors": ["Mengdi Jia", "Zekun Qi", "Shaochen Zhang", "Wenyao Zhang", "Xinqiang Yu", "Jiawei He", "He Wang", "Li Yi"], "title": "OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project Page: https://qizekun.github.io/omnispatial/", "summary": "Spatial reasoning is a key aspect of cognitive psychology and remains a major\nbottleneck for current vision-language models (VLMs). While extensive research\nhas aimed to evaluate or improve VLMs' understanding of basic spatial\nrelations, such as distinguishing left from right, near from far, and object\ncounting, these tasks represent only the most fundamental level of spatial\nreasoning. In this work, we introduce OmniSpatial, a comprehensive and\nchallenging benchmark for spatial reasoning, grounded in cognitive psychology.\nOmniSpatial covers four major categories: dynamic reasoning, complex spatial\nlogic, spatial interaction, and perspective-taking, with 50 fine-grained\nsubcategories. Through Internet data crawling and careful manual annotation, we\nconstruct over 1.5K question-answer pairs. Extensive experiments show that both\nopen- and closed-source VLMs, as well as existing reasoning and spatial\nunderstanding models, exhibit significant limitations in comprehensive spatial\nunderstanding. We further analyze failure cases and propose potential\ndirections for future research.", "AI": {"tldr": "\u63d0\u51faOmniSpatial\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7cfb\u7edf\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684\u4e0d\u8db3", "motivation": "\u73b0\u6709\u7814\u7a76\u5c40\u9650\u4e8e\u57fa\u7840\u7a7a\u95f4\u5173\u7cfb\uff08\u5de6\u53f3/\u8fdc\u8fd1/\u8ba1\u6570\uff09\uff0c\u7f3a\u4e4f\u5bf9\u52a8\u6001\u63a8\u7406/\u590d\u6742\u903b\u8f91/\u7a7a\u95f4\u4ea4\u4e92/\u89c6\u89d2\u8f6c\u6362\u7b49\u6df1\u5c42\u8ba4\u77e5\u80fd\u529b\u7684\u8bc4\u4f30\u4f53\u7cfb", "method": "\u57fa\u4e8e\u8ba4\u77e5\u5fc3\u7406\u5b66\u7406\u8bba\u6784\u5efa4\u5927\u7c7b50\u5b50\u7c7b\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u901a\u8fc7\u4e92\u8054\u7f51\u6570\u636e\u6293\u53d6\u548c\u4eba\u5de5\u6807\u6ce8\u521b\u5efa1.5K+\u9ad8\u8d28\u91cfQA\u5bf9\uff0c\u5bf9\u4e3b\u6d41\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u6027\u6d4b\u8bd5", "result": "\u5f00\u6e90/\u95ed\u6e90VLMs\u53ca\u73b0\u6709\u63a8\u7406\u6a21\u578b\u5728\u7efc\u5408\u7a7a\u95f4\u7406\u89e3\u4e0a\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff0c\u9519\u8bef\u6848\u4f8b\u63ed\u793a\u6a21\u578b\u5bf9\u7a7a\u95f4\u5173\u7cfb\u5c42\u7ea7\u6027/\u903b\u8f91\u7ec4\u5408\u6027\u7684\u7406\u89e3\u4e0d\u8db3", "conclusion": "\u5efa\u7acb\u591a\u7ef4\u5ea6\u8bc4\u4f30\u57fa\u51c6\u63a8\u52a8\u7a7a\u95f4\u8ba4\u77e5\u7814\u7a76\uff0c\u672a\u6765\u9700\u6539\u8fdb\u6a21\u578b\u7684\u52a8\u6001\u573a\u666f\u5efa\u6a21\u548c\u7a7a\u95f4\u903b\u8f91\u63a8\u7406\u80fd\u529b"}}
{"id": "2506.03144", "pdf": "https://arxiv.org/pdf/2506.03144", "abs": "https://arxiv.org/abs/2506.03144", "authors": ["Wei Chow", "Yuan Gao", "Linfeng Li", "Xian Wang", "Qi Xu", "Hang Song", "Lingdong Kong", "Ran Zhou", "Yi Zeng", "Yidong Cai", "Botian Jiang", "Shilin Xu", "Jiajun Zhang", "Minghui Qiu", "Xiangtai Li", "Tianshu Yang", "Siliang Tang", "Juncheng Li"], "title": "MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition Query", "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": "Preprint; Project Page, Code, and Dataset at:\n  https://merit-2025.github.io/", "summary": "Semantic retrieval is crucial for modern applications yet remains\nunderexplored in current research. Existing datasets are limited to single\nlanguages, single images, or singular retrieval conditions, often failing to\nfully exploit the expressive capacity of visual information as evidenced by\nmaintained performance when images are replaced with captions. However,\npractical retrieval scenarios frequently involve interleaved multi-condition\nqueries with multiple images. Hence, this paper introduces MERIT, the first\nmultilingual dataset for interleaved multi-condition semantic retrieval,\ncomprising 320,000 queries with 135,000 products in 5 languages, covering 7\ndistinct product categories. Extensive experiments on MERIT identify existing\nmodels's limitation: focusing solely on global semantic information while\nneglecting specific conditional elements in queries. Consequently, we propose\nCoral, a novel fine-tuning framework that adapts pre-trained MLLMs by\nintegrating embedding reconstruction to preserve fine-grained conditional\nelements and contrastive learning to extract comprehensive global semantics.\nExperiments demonstrate that Coral achieves a 45.9% performance improvement\nover conventional approaches on MERIT, with strong generalization capabilities\nvalidated across 8 established retrieval benchmarks. Collectively, our\ncontributions - a novel dataset, identification of critical limitations in\nexisting approaches, and an innovative fine-tuning framework - establish a\nfoundation for future research in interleaved multi-condition semantic\nretrieval.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u591a\u8bed\u8a00\u591a\u6761\u4ef6\u8bed\u4e49\u68c0\u7d22\u6570\u636e\u96c6MERIT\u53ca\u5fae\u8c03\u6846\u67b6Coral\uff0c\u901a\u8fc7\u5d4c\u5165\u91cd\u5efa\u548c\u5bf9\u6bd4\u5b66\u4e60\u5b9e\u73b045.9%\u6027\u80fd\u63d0\u5347", "motivation": "\u73b0\u6709\u8bed\u4e49\u68c0\u7d22\u6570\u636e\u96c6\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u573a\u666f\u4e2d\u591a\u56fe\u4ea4\u9519\u7684\u591a\u6761\u4ef6\u67e5\u8be2\u9700\u6c42\uff0c\u4e14\u89c6\u89c9\u4fe1\u606f\u5229\u7528\u7387\u4e0d\u8db3", "method": "Coral\u6846\u67b6\u6574\u5408\u5d4c\u5165\u91cd\u5efa\uff08\u4fdd\u7559\u7ec6\u7c92\u5ea6\u6761\u4ef6\u5143\u7d20\uff09\u548c\u5bf9\u6bd4\u5b66\u4e60\uff08\u63d0\u53d6\u5168\u5c40\u8bed\u4e49\uff09\uff0c\u9002\u914d\u9884\u8bad\u7ec3\u591a\u8bed\u8a00\u5927\u6a21\u578b", "result": "\u5728MERIT\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u63d0\u534745.9%\uff0c\u57288\u4e2a\u68c0\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u5f3a\u6cdb\u5316\u80fd\u529b", "conclusion": "MERIT\u6570\u636e\u96c6\u586b\u8865\u7814\u7a76\u7a7a\u767d\uff0c\u63ed\u793a\u73b0\u6709\u6a21\u578b\u5ffd\u89c6\u67e5\u8be2\u6761\u4ef6\u7ec6\u8282\u7684\u7f3a\u9677\uff0cCoral\u6846\u67b6\u4e3a\u591a\u6761\u4ef6\u8bed\u4e49\u68c0\u7d22\u5960\u5b9a\u65b0\u57fa\u7840"}}
{"id": "2506.03147", "pdf": "https://arxiv.org/pdf/2506.03147", "abs": "https://arxiv.org/abs/2506.03147", "authors": ["Bin Lin", "Zongjian Li", "Xinhua Cheng", "Yuwei Niu", "Yang Ye", "Xianyi He", "Shenghai Yuan", "Wangbo Yu", "Shaodong Wang", "Yunyang Ge", "Yatian Pang", "Li Yuan"], "title": "UniWorld: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Although existing unified models deliver strong performance on\nvision-language understanding and text-to-image generation, their models are\nlimited in exploring image perception and manipulation tasks, which are\nurgently desired by users for wide applications. Recently, OpenAI released\ntheir powerful GPT-4o-Image model for comprehensive image perception and\nmanipulation, achieving expressive capability and attracting community\ninterests. By observing the performance of GPT-4o-Image in our carefully\nconstructed experiments, we infer that GPT-4o-Image leverages features\nextracted by semantic encoders instead of VAE, while VAEs are considered\nessential components in many image manipulation models. Motivated by such\ninspiring observations, we present a unified generative framework named\nUniWorld based on semantic features provided by powerful visual-language models\nand contrastive semantic encoders. As a result, we build a strong unified model\nusing only 1% amount of BAGEL's data, which consistently outperforms BAGEL on\nimage editing benchmarks. UniWorld also maintains competitive image\nunderstanding and generation capabilities, achieving strong performance across\nmultiple image perception tasks. We fully open-source our models, including\nmodel weights, training and evaluation scripts, and datasets.", "AI": {"tldr": "UniWorld\u6846\u67b6\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u7279\u5f81\u5b9e\u73b0\u9ad8\u6548\u56fe\u50cf\u7f16\u8f91\u4e0e\u7406\u89e3\uff0c\u4ec5\u97001%\u8bad\u7ec3\u6570\u636e\u5373\u8d85\u8d8aBAGEL\u6a21\u578b\u6027\u80fd", "motivation": "\u73b0\u6709\u7edf\u4e00\u6a21\u578b\u5728\u56fe\u50cf\u611f\u77e5\u4e0e\u64cd\u4f5c\u4efb\u52a1\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u800cGPT-4o-Image\u7684\u6210\u529f\u663e\u793a\u8bed\u4e49\u7f16\u7801\u5668\u66ff\u4ee3VAE\u7684\u53ef\u80fd\u6027", "method": "\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u7279\u5f81\u548c\u5bf9\u6bd4\u8bed\u4e49\u7f16\u7801\u5668\uff0c\u6784\u5efa\u7edf\u4e00\u751f\u6210\u6846\u67b6", "result": "\u5728\u56fe\u50cf\u7f16\u8f91\u57fa\u51c6\u4e0a\u8d85\u8d8aBAGEL\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u7406\u89e3/\u751f\u6210\u7ade\u4e89\u529b\uff0c\u6570\u636e\u6548\u7387\u63d0\u5347100\u500d", "conclusion": "UniWorld\u8bc1\u660e\u4e86\u8bed\u4e49\u7279\u5f81\u5728\u8de8\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5176\u5f00\u6e90\u5b9e\u73b0\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
