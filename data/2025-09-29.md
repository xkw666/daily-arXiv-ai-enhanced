<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 89]
- [cs.GR](#cs.GR) [Total: 5]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Novel Differential Feature Learning for Effective Hallucination Detection and Classification](https://arxiv.org/abs/2509.21357)
*Wenkai Wang,Vincent Lee,Yizhen Zheng*

Main category: cs.CL

TL;DR: 提出双模型架构（PF块+DFL机制）定位大语言模型幻觉信号，发现特征高度稀疏且呈现漏斗分布模式


<details>
  <summary>Details</summary>
Motivation: 现有研究未明确幻觉信号在神经网络层的具体分布，制约高效检测方法开发

Method: 基于投影融合块（PF）的自适应层间特征加权机制，配合差分特征学习（DFL）从并行编码器提取互补特征

Result: 在HaluEval数据集上实现QA和对话任务准确率提升，仅需1%特征维度即可保持检测性能

Conclusion: 幻觉信号集中度高于预期，为开发计算高效的检测系统提供新路径

Abstract: Large language model hallucination represents a critical challenge where
outputs deviate from factual accuracy due to distributional biases in training
data. While recent investigations establish that specific hidden layers exhibit
differences between hallucinatory and factual content, the precise localization
of hallucination signals within layers remains unclear, limiting the
development of efficient detection methods. We propose a dual-model
architecture integrating a Projected Fusion (PF) block for adaptive inter-layer
feature weighting and a Differential Feature Learning (DFL) mechanism that
identifies discriminative features by computing differences between parallel
encoders learning complementary representations from identical inputs. Through
systematic experiments across HaluEval's question answering, dialogue, and
summarization datasets, we demonstrate that hallucination signals concentrate
in highly sparse feature subsets, achieving significant accuracy improvements
on question answering and dialogue tasks. Notably, our analysis reveals a
hierarchical "funnel pattern" where shallow layers exhibit high feature
diversity while deep layers demonstrate concentrated usage, enabling detection
performance to be maintained with minimal degradation using only 1\% of feature
dimensions. These findings suggest that hallucination signals are more
concentrated than previously assumed, offering a pathway toward computationally
efficient detection systems that could reduce inference costs while maintaining
accuracy.

</details>


### [2] [Influence Guided Context Selection for Effective Retrieval-Augmented Generation](https://arxiv.org/abs/2509.21359)
*Jiale Deng,Yanyan Shen,Ziyuan Pei,Youmin Chen,Linpeng Huang*

Main category: cs.CL

TL;DR: 提出基于上下文影响力值（CI值）的检索增强生成优化方法，通过多维度质量评估有效提升生成效果


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法受限于低质量上下文干扰，且传统上下文选择指标无法全面整合查询、上下文列表和生成器的综合信息

Method: 将上下文质量评估重构为推理时数据估值问题，设计融合查询相关性、列表独特性和生成对齐的CI值指标，并开发参数化代理预测模型

Result: 在8个NLP任务和多种LLM上显著超越现有方法，计算效率提升50%的同时保持95%的关键信息保留率

Conclusion: CI值机制通过系统性上下文质量评估，实现了噪声过滤与信息保留的最佳平衡，为RAG系统优化提供新范式

Abstract: Retrieval-Augmented Generation (RAG) addresses large language model (LLM)
hallucinations by grounding responses in external knowledge, but its
effectiveness is compromised by poor-quality retrieved contexts containing
irrelevant or noisy information. While existing approaches attempt to improve
performance through context selection based on predefined context quality
assessment metrics, they show limited gains over standard RAG. We attribute
this limitation to their failure in holistically utilizing available
information (query, context list, and generator) for comprehensive quality
assessment. Inspired by recent advances in data selection, we reconceptualize
context quality assessment as an inference-time data valuation problem and
introduce the Contextual Influence Value (CI value). This novel metric
quantifies context quality by measuring the performance degradation when
removing each context from the list, effectively integrating query-aware
relevance, list-aware uniqueness, and generator-aware alignment. Moreover, CI
value eliminates complex selection hyperparameter tuning by simply retaining
contexts with positive CI values. To address practical challenges of label
dependency and computational overhead, we develop a parameterized surrogate
model for CI value prediction during inference. The model employs a
hierarchical architecture that captures both local query-context relevance and
global inter-context interactions, trained through oracle CI value supervision
and end-to-end generator feedback. Extensive experiments across 8 NLP tasks and
multiple LLMs demonstrate that our context selection method significantly
outperforms state-of-the-art baselines, effectively filtering poor-quality
contexts while preserving critical information. Code is available at
https://github.com/SJTU-DMTai/RAG-CSM.

</details>


### [3] [Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs](https://arxiv.org/abs/2509.21361)
*Norman Paulsen*

Main category: cs.CL

TL;DR: 研究发现LLM宣传的最大上下文窗口(MCW)与实际有效窗口(MECW)存在显著差距，有效窗口受问题类型影响且普遍不足宣传值的1%


<details>
  <summary>Details</summary>
Motivation: 验证大语言模型宣传的长上下文窗口在实际应用中的真实有效性，揭示模型实际处理能力与商业宣传的差异

Method: 1. 定义最大有效上下文窗口(MECW) 2. 开发多尺寸多问题类型的测试框架 3. 建立标准化比较模型在不同上下文长度下的效能

Result: 所有模型MECW均显著低于MCW(最大差距达99%)，多数模型在1000token内出现严重准确率下降，部分顶级模型在100token时即失效

Conclusion: 问题类型决定MECW范围，该发现为提升模型准确率、减少幻觉提供了明确改进方向，建议重新评估上下文窗口的行业标准

Abstract: Large language model (LLM) providers boast big numbers for maximum context
window sizes. To test the real world use of context windows, we 1) define a
concept of maximum effective context window, 2) formulate a testing method of a
context window's effectiveness over various sizes and problem types, and 3)
create a standardized way to compare model efficacy for increasingly larger
context window sizes to find the point of failure. We collected hundreds of
thousands of data points across several models and found significant
differences between reported Maximum Context Window (MCW) size and Maximum
Effective Context Window (MECW) size. Our findings show that the MECW is, not
only, drastically different from the MCW but also shifts based on the problem
type. A few top of the line models in our test group failed with as little as
100 tokens in context; most had severe degradation in accuracy by 1000 tokens
in context. All models fell far short of their Maximum Context Window by as
much as 99 percent. Our data reveals the Maximum Effective Context Window
shifts based on the type of problem provided, offering clear and actionable
insights into how to improve model accuracy and decrease model hallucination
rates.

</details>


### [4] [How Large Language Models Need Symbolism](https://arxiv.org/abs/2509.21404)
*Xiaotie Deng,Hanyu Li*

Main category: cs.CL

TL;DR: 提出AI发展需结合人类符号指引而不仅依赖模型规模扩展


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型仅依靠参数扩展难以实现真正突破，需要引入人类知识符号作为导航系统

Method: 将人类构建的知识符号体系作为'指南针'，引导语言模型的直觉式推理

Result: 构建人机协作新范式，使语言模型突破直觉局限实现可控的创造性发现

Conclusion: AI未来发展应是人类符号系统与模型规模扩展的有机融合

Abstract: We argue that AI's future requires more than scaling. To unlock genuine
discovery, large language models need a compass: human-crafted symbols to guide
their powerful but blind intuition.

</details>


### [5] [One Model, Many Morals: Uncovering Cross-Linguistic Misalignments in Computational Moral Reasoning](https://arxiv.org/abs/2509.21443)
*Sualeha Farid,Jayden Lin,Zean Chen,Shivani Kumar,David Jurgens*

Main category: cs.CL

TL;DR: 研究揭示LLMs在多语言环境中的道德判断存在显著不一致性，文化差异和预训练数据是主要影响因素，需开发文化感知AI


<details>
  <summary>Details</summary>
Motivation: 主流LLMs基于英语数据训练，其跨语言/文化的道德推理泛化能力存疑，可能导致文化错位风险

Method: 将道德推理基准翻译为5种语言进行零样本评估，通过对比分析和案例研究探索差异成因

Result: 发现LLMs跨语言道德判断存在系统性偏差，推理策略差异与预训练数据分布高度相关

Conclusion: 需构建文化敏感的AI系统，建立结构化道德错误类型学以提升跨文化适应性

Abstract: Large Language Models (LLMs) are increasingly deployed in multilingual and
multicultural environments where moral reasoning is essential for generating
ethically appropriate responses. Yet, the dominant pretraining of LLMs on
English-language data raises critical concerns about their ability to
generalize judgments across diverse linguistic and cultural contexts. In this
work, we systematically investigate how language mediates moral decision-making
in LLMs. We translate two established moral reasoning benchmarks into five
culturally and typologically diverse languages, enabling multilingual zero-shot
evaluation. Our analysis reveals significant inconsistencies in LLMs' moral
judgments across languages, often reflecting cultural misalignment. Through a
combination of carefully constructed research questions, we uncover the
underlying drivers of these disparities, ranging from disagreements to
reasoning strategies employed by LLMs. Finally, through a case study, we link
the role of pretraining data in shaping an LLM's moral compass. Through this
work, we distill our insights into a structured typology of moral reasoning
errors that calls for more culturally-aware AI.

</details>


### [6] [LLM-Based Support for Diabetes Diagnosis: Opportunities, Scenarios, and Challenges with GPT-5](https://arxiv.org/abs/2509.21450)
*Gaurav Kumar Gupta,Nirajan Acharya,Pranal Pande*

Main category: cs.CL

TL;DR: 研究评估GPT-5在糖尿病诊断中的应用，显示其与ADA标准高度一致，可能成为临床医生与患者的双重工具，并强调医疗领域大模型评估框架的重要性


<details>
  <summary>Details</summary>
Motivation: 糖尿病早期诊断存在症状模糊、指标临界值复杂、妊娠期筛查困难等问题，需探索大语言模型在结构化输出和患者沟通方面的应用潜力

Method: 基于ADA 2025标准和公开数据集构建合成病例模拟框架，测试GPT-5在症状识别、实验室解读、妊娠筛查、远程监测和并发症检测五个场景的表现

Result: GPT-5生成的临床决策依据和结构化摘要与ADA标准高度契合，在多场景应用中展现出双重临床价值

Conclusion: GPT-5可作为医患双端的智能辅助工具，但需建立可重复的评估体系以规范医疗领域大语言模型的应用

Abstract: Diabetes mellitus is a major global health challenge, affecting over half a
billion adults worldwide with prevalence projected to rise. Although the
American Diabetes Association (ADA) provides clear diagnostic thresholds, early
recognition remains difficult due to vague symptoms, borderline laboratory
values, gestational complexity, and the demands of long-term monitoring.
Advances in large language models (LLMs) offer opportunities to enhance
decision support through structured, interpretable, and patient-friendly
outputs. This study evaluates GPT-5, the latest generative pre-trained
transformer, using a simulation framework built entirely on synthetic cases
aligned with ADA Standards of Care 2025 and inspired by public datasets
including NHANES, Pima Indians, EyePACS, and MIMIC-IV. Five representative
scenarios were tested: symptom recognition, laboratory interpretation,
gestational diabetes screening, remote monitoring, and multimodal complication
detection. For each, GPT-5 classified cases, generated clinical rationales,
produced patient explanations, and output structured JSON summaries. Results
showed strong alignment with ADA-defined criteria, suggesting GPT-5 may
function as a dual-purpose tool for clinicians and patients, while underscoring
the importance of reproducible evaluation frameworks for responsibly assessing
LLMs in healthcare.

</details>


### [7] [Diagnosing the Performance Trade-off in Moral Alignment: A Case Study on Gender Stereotypes](https://arxiv.org/abs/2509.21456)
*Guangliang Liu,Bocheng Chen,Xitong Zhang,Kristen Marie Johnson*

Main category: cs.CL

TL;DR: 研究发现当前通过公平性目标缓解预训练语言模型性别偏见的方案存在局限性，下游任务性能主要受整体遗忘水平驱动，选择性遗忘偏见反而会加剧整体遗忘，现有方法难以有效改善性能折衷。


<details>
  <summary>Details</summary>
Motivation: 探讨在缓解语言模型性别偏见过程中，道德对齐（如微调/模型编辑）导致下游任务性能下降的机制，分析基于遗忘过程的公平性目标在性能折衷中的作用。

Method: 通过遗忘机制和公平性目标的视角，量化分析性别偏见缓解场景下的模型行为，验证选择性遗忘对整体遗忘水平的影响，并测试现有缓解遗忘方案的有效性。

Result: 1) 下游任务性能与整体遗忘水平强相关
2) 选择性遗忘偏见会提升整体遗忘程度
3) 现有缓解遗忘方案无法有效降低整体遗忘，未能改善性能折衷。

Conclusion: 当前公平性目标无法有效实现性能平衡，突显了道德对齐过程中系统性遗忘控制的必要性，为未来开发更精细的偏见缓解方法提供理论依据。

Abstract: Moral alignment has emerged as a widely adopted approach for regulating the
behavior of pretrained language models (PLMs), typically through fine-tuning or
model editing on curated datasets. However, this process often comes at the
cost of degraded downstream task performance. Prior studies commonly aim to
achieve a performance trade-off by encouraging PLMs to selectively forget
stereotypical knowledge through carefully designed fairness objectives, while
preserving their helpfulness. In this short paper, we investigate the
underlying mechanisms of the performance trade-off in the context of mitigating
gender stereotypes, through the lens of forgetting and the fairness objective.
Our analysis reveals the limitations of current fairness objective in achieving
trade-off by demonstrating that: (1) downstream task performance is primarily
driven by the overall forgetting level; (2) selective forgetting of stereotypes
tends to increase overall forgetting; and (3) general solutions for mitigating
forgetting are ineffective at reducing overall forgetting and fail to improve
downstream task performance.

</details>


### [8] [A State-of-the-Art SQL Reasoning Model using RLVR](https://arxiv.org/abs/2509.21459)
*Alnur Ali,Ashutosh Baheti,Jonathan Chang,Ta-Chung Chi,Brandon Cui,Andrew Drozdov,Jonathan Frankle,Abhay Gupta,Pallavi Koppol,Sean Kulinski,Jonathan Li,Dipendra Misra,Krista Opsahl-Ong,Jose Javier Gonzalez Ortiz,Matei Zaharia,Yue Zhang*

Main category: cs.CL

TL;DR: 提出基于可验证奖励强化学习(RLVR)的通用训练框架，在BIRD数据库自然语言转SQL任务中取得SOTA效果


<details>
  <summary>Details</summary>
Motivation: 企业定制化推理模型需求迫切，RLVR框架可有效利用可验证奖励机制解决实际问题，BIRD任务验证框架有效性

Method: 结合离线强化学习(TAO)预训练与在线RLVR训练，采用提示工程优化和模型选择策略，仅使用公开训练数据

Result: BIRD测试集准确率73.56%(基础)/75.68%(自洽模式)，生成次数少于第二名方法，未使用专有模型或额外数据

Conclusion: 该框架简单有效，可扩展应用于商业智能、数据科学和代码生成等企业级领域

Abstract: Developing custom reasoning models via Reinforcement Learning (RL) that can
incorporate organization-specific knowledge has great potential to address
problems faced by enterprise customers. In many of these problems, the reward
function is verifiable, a setting termed RL with Verifiable Rewards (RLVR). We
apply RLVR to a popular data science benchmark called BIRD that measures the
ability of an AI agent to convert a natural language query for a database to
SQL executions. We apply a simple and general-purpose training recipe involving
careful prompt and model selection, a warm-up stage using our offline RL
approach called TAO, followed by rigorous online RLVR training. With no
additional training data beyond the BIRD training set and no use of proprietary
models, our very first submission to the BIRD leaderboard reached
state-of-the-art accuracy on the private test set: 73.56% without
self-consistency and 75.68% with self-consistency. In the latter case, our
model also required fewer generations than the second-best approach. While BIRD
is only a proxy task, the simplicity of our framework makes it broadly
applicable to enterprise domains such as business intelligence, data science,
and coding.

</details>


### [9] [Learning to Reason with Mixture of Tokens](https://arxiv.org/abs/2509.21482)
*Adit Jain,Brendan Rappazzo*

Main category: cs.CL

TL;DR: 提出混合令牌生成方法（MoT-G）改进强化学习验证奖励框架，在10项推理任务中7项实现5-35%性能提升，训练效率翻倍。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法丢弃模型输出的概率分布信息，制约推理搜索空间的有效利用。

Method: 构建连续混合令牌空间进行思维链生成，整合加权词向量混合嵌入方法，保持隐藏状态高熵值。

Result: Qwen2.5-1.5B模型在Reasoning-Gym测试集上达到同等精度所需训练轨迹减少50%，推理性能显著提升。

Conclusion: 利用连续概率分布信息可增强语言模型推理能力，高熵隐藏状态促进探索效率是性能提升的关键机制。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a leading
approach for improving large language model (LLM) reasoning capabilities. Most
current methods follow variants of Group Relative Policy Optimization, which
samples multiple reasoning completions, scores them relative to each other, and
adjusts the policy accordingly. However, these approaches invariably sample
discrete tokens at each reasoning step, discarding the rich distributional
information in the model's probability distribution over candidate tokens.
While preserving and utilizing this distributional information has proven
beneficial in non-RL settings, current RLVR methods seem to be unnecessarily
constraining the reasoning search space by not using this information. To
address this limitation, we investigate mixture-of-token generation (MoT-G) in
RLVR. We present a unified framework that generalizes existing MoT-G
approaches, including existing training-free methods that construct mixture
embeddings as weighted sums over token embeddings, and extend RLVR to operate
directly in this continuous mixture space for generating chain-of-thought.
Evaluating two MoT-G variants on Reasoning-Gym, a suite of reasoning-intensive
language tasks, we find that MoT--G methods achieve substantial improvements
(5--35 \% gains on 7 out of 10 tasks) compared to standard decoding with the
Qwen2.5-1.5B model, while reaching comparable accuracy with half the number of
trajectories, suggesting improved training efficiency. Through comprehensive
hidden-state and token-level analyses, we provide evidence that MoT--G's
benefits may stem from its ability to maintain higher hidden-state entropy
throughout the reasoning process and promote exploration in token space.

</details>


### [10] [Dual-Head Reasoning Distillation: Improving Classifier Accuracy with Train-Time-Only Reasoning](https://arxiv.org/abs/2509.21487)
*Jillian Xu,Dylan Zhou,Vinay Shukla,Yang Yang,Junrui Ruan,Shuhuai Lin,Wenfei Zou,Yinxiao Liu,Karthik Lakshmanan*

Main category: cs.CL

TL;DR: 提出双头推理蒸馏方法（DHRD），在保持推理速度的同时提升分类准确率


<details>
  <summary>Details</summary>
Motivation: 解决Chain-of-Thought（CoT）提示方法在提升分类准确率时引发的吞吐量显著下降问题

Method: 1. 添加训练专用的分类头和推理头
2. 使用加权交叉熵损失（标签损失+token级LM损失）
3. 测试时禁用推理头保持速度

Result: 在7项SuperGLUE任务上相对基线提升0.65-5.47%，QPS吞吐量比原版CoT提升96-142倍

Conclusion: DHRD方法有效平衡了模型性能与推理效率，特别适用于需要实时处理的任务场景

Abstract: Chain-of-Thought (CoT) prompting often improves classification accuracy, but
it introduces a significant throughput penalty with rationale generation (Wei
et al., 2022; Cheng and Van Durme, 2024). To resolve this trade-off, we
introduce Dual-Head Reasoning Distillation (DHRD), a simple training method for
decoder-only language models (LMs) that adds (i) a pooled classification head
used during training and inference and (ii) a reasoning head supervised by
teacher rationales used only in training. We train with a loss function that is
a weighted sum of label cross-entropy and token-level LM loss over
input-plus-rationale sequences. On seven SuperGLUE tasks, DHRD yields relative
gains of 0.65-5.47% over pooled baselines, with notably larger gains on
entailment/causal tasks. Since we disable the reasoning head at test time,
inference throughput matches pooled classifiers and exceeds CoT decoding on the
same backbones by 96-142 times in QPS.

</details>


### [11] [On Code-Induced Reasoning in LLMs](https://arxiv.org/abs/2509.21499)
*Abdul Waheed,Zhen Wu,Carolyn Rosé,Daphne Ippolito*

Main category: cs.CL

TL;DR: 研究代码结构与语义属性对LLM推理能力的影响，发现模型对结构扰动更敏感，合理抽象（如伪代码）可保持性能甚至提升效果，不同编程语言的语法风格适配不同任务类型。


<details>
  <summary>Details</summary>
Motivation: 探究代码数据中具体哪些属性（结构/语义）对提升大语言模型推理能力起关键作用，以及不同编程语言语法对任务性能的影响机制。

Method: 构建十种编程语言的并行指令数据集，通过控制性扰动破坏代码结构/语义，对5个模型家族8种规模的LLMs进行微调，并在3,331次实验中评估自然语言/数学/代码任务表现。

Result: 1. 结构扰动比语义扰动影响更大（数学/代码任务下降19.2% vs 6.8%）
2. 伪代码/流程图等抽象形式与真实代码效果相当
3. 保留表面规律性的损坏代码仍具竞争力
4. Python适配自然语言推理，Java/Rust适配数学任务

Conclusion: 代码的结构属性对LLM推理能力起决定性作用，合理抽象可优化训练效率，不同编程语言的语法特征与任务类型存在适配关系，这对设计增强推理的训练数据具有指导意义。

Abstract: Code data has been shown to enhance the reasoning capabilities of large
language models (LLMs), but it remains unclear which aspects of code are most
responsible. We investigate this question with a systematic, data-centric
framework. We construct parallel instruction datasets in ten programming
languages and apply controlled perturbations that selectively disrupt
structural or semantic properties of code. We then finetune LLMs from five
model families and eight scales on each variant and evaluate their performance
on natural language, math, and code tasks. Across 3,331 experiments, our
results show that LLMs are more vulnerable to structural perturbations than
semantic ones, particularly on math and code tasks. Appropriate abstractions
like pseudocode and flowcharts can be as effective as code, while encoding the
same information with fewer tokens without adhering to original syntax can
often retain or even improve performance. Remarkably, even corrupted code with
misleading signals remains competitive when surface-level regularities persist.
Finally, syntactic styles also shape task-specific gains with Python favoring
natural language reasoning and lower-level languages such as Java and Rust
favoring math. Through our systematic framework, we aim to provide insight into
how different properties of code influence reasoning and inform the design of
training data for enhancing LLM reasoning capabilities.

</details>


### [12] [Agribot: agriculture-specific question answer system](https://arxiv.org/abs/2509.21535)
*Naman Jain,Pranjali Jain,Pratik Kayal,Jayakrishna Sahit,Soham Pachpande,Jayesh Choudhari*

Main category: cs.CL

TL;DR: 开发基于Kisan Call Center数据集的农业问答系统，通过句向量模型（准确率56%→86%）实现天气、市场价格等农事信息查询


<details>
  <summary>Details</summary>
Motivation: 印度作为农业经济主体，农民亟需便捷的农业信息获取渠道。现有呼叫中心人工服务效率有限，需开发24小时自助系统减轻人力负担并提升信息传递效率

Method: 使用句向量模型构建农业问答系统，通过消除同义词干扰和整合实体识别技术优化模型性能，数据集来源于Kisan Call Center历史咨询记录

Result: 基础模型准确率56%，经同义词清洗和实体提取优化后准确率提升至86%，系统覆盖天气、市场价格、植物保护、政府政策四大核心场景

Conclusion: 该系统突破传统呼叫中心时空限制，通过精准信息传递助力农业决策优化，同时释放人力资源聚焦更高价值工作，推动农业生产效率提升

Abstract: India is an agro-based economy and proper information about agricultural
practices is the key to optimal agricultural growth and output. In order to
answer the queries of the farmer, we have build an agricultural chatbot based
on the dataset from Kisan Call Center. This system is robust enough to answer
queries related to weather, market rates, plant protection and government
schemes. This system is available 24* 7, can be accessed through any electronic
device and the information is delivered with the ease of understanding. The
system is based on a sentence embedding model which gives an accuracy of 56%.
After eliminating synonyms and incorporating entity extraction, the accuracy
jumps to 86%. With such a system, farmers can progress towards easier
information about farming related practices and hence a better agricultural
output. The job of the Call Center workforce would be made easier and the hard
work of various such workers can be redirected to a better goal.

</details>


### [13] [Domain-Aware Speaker Diarization On African-Accented English](https://arxiv.org/abs/2509.21554)
*Chibuzor Okocha,Kelechi Ezema,Christan Grant*

Main category: cs.CL

TL;DR: 研究探讨非洲口音英语说话人分域的领域效应，发现临床对话存在显著性能差距，并验证轻量级领域适应方案


<details>
  <summary>Details</summary>
Motivation: 针对临床环境下非洲口音英语说话人分域系统性能显著下降的问题，探究领域差异对算法的影响机制并提出改进方案

Method: 通过严格DER协议评估生产/开源系统，进行错误归因分析，并采用口音匹配数据微调分割模块实现领域适应

Result: 临床领域存在持续性能惩罚（主要源于短轮次重叠导致的误报/漏检），领域适应使错误率降低但未完全消除差距

Conclusion: 研究构建跨领域评测基准并提出可复现的适应方案，建议未来开发重叠感知算法并扩充临床数据资源以提升鲁棒性

Abstract: This study examines domain effects in speaker diarization for
African-accented English. We evaluate multiple production and open systems on
general and clinical dialogues under a strict DER protocol that scores overlap.
A consistent domain penalty appears for clinical speech and remains significant
across models. Error analysis attributes much of this penalty to false alarms
and missed detections, aligning with short turns and frequent overlap. We test
lightweight domain adaptation by fine-tuning a segmentation module on
accent-matched data; it reduces error but does not eliminate the gap. Our
contributions include a controlled benchmark across domains, a concise approach
to error decomposition and conversation-level profiling, and an adaptation
recipe that is easy to reproduce. Results point to overlap-aware segmentation
and balanced clinical resources as practical next steps.

</details>


### [14] [Generation-Time vs. Post-hoc Citation: A Holistic Evaluation of LLM Attribution](https://arxiv.org/abs/2509.21557)
*Yash Saxena,Raviteja Bommireddy,Ankur Padia,Manas Gaur*

Main category: cs.CL

TL;DR: 论文提出生成时引用(G-Cite)和事后引用(P-Cite)两种范式，通过实验证明检索是提升归因质量的核心因素，建议高风险领域优先采用检索驱动的P-Cite方案。


<details>
  <summary>Details</summary>
Motivation: 针对医疗/法律等高风险领域LLM必须提供可验证来源的需求，解决模型应在生成时还是生成后附加引用的选择难题。

Method: 在四个主流归因数据集上对比零样本到检索增强方法，评估覆盖率和引用正确性指标，分析延迟等实践因素。

Result: 发现覆盖率与引用正确性存在权衡：P-Cite在保持较高覆盖率的同时具有竞争力正确性，G-Cite则以牺牲覆盖率为代价优先精确性。

Conclusion: 推荐高风险领域采用检索驱动的P-Cite优先方案，严格声明验证场景保留G-Cite，形成兼顾质量与效率的引用策略。

Abstract: Trustworthy Large Language Models (LLMs) must cite human-verifiable sources
in high-stakes domains such as healthcare, law, academia, and finance, where
even small errors can have severe consequences. Practitioners and researchers
face a choice: let models generate citations during decoding, or let models
draft answers first and then attach appropriate citations. To clarify this
choice, we introduce two paradigms: Generation-Time Citation (G-Cite), which
produces the answer and citations in one pass, and Post-hoc Citation (P-Cite),
which adds or verifies citations after drafting. We conduct a comprehensive
evaluation from zero-shot to advanced retrieval-augmented methods across four
popular attribution datasets and provide evidence-based recommendations that
weigh trade-offs across use cases. Our results show a consistent trade-off
between coverage and citation correctness, with retrieval as the main driver of
attribution quality in both paradigms. P-Cite methods achieve high coverage
with competitive correctness and moderate latency, whereas G-Cite methods
prioritize precision at the cost of coverage and speed. We recommend a
retrieval-centric, P-Cite-first approach for high-stakes applications,
reserving G-Cite for precision-critical settings such as strict claim
verification. Our codes and human evaluation results are available at
https://anonymous.4open.science/r/Citation_Paradigms-BBB5/

</details>


### [15] [Comparative Personalization for Multi-document Summarization](https://arxiv.org/abs/2509.21562)
*Haoyuan Li,Snigdha Chaturvedi*

Main category: cs.CL

TL;DR: 提出ComPSum个性化多文档摘要框架，通过对比用户偏好差异生成个性化摘要


<details>
  <summary>Details</summary>
Motivation: 个性化摘要需通过对比不同用户的细粒度偏好差异实现有效定制化

Method: 1. 生成用户偏好对比的结构化分析
2. 用分析结果指导个性化摘要生成
3. 开发AuthorMap评估框架进行作者归属验证

Result: 在PerMSum数据集上，ComPSum显著优于基线模型（基于作者归属的个性化评估）

Conclusion: 用户偏好的对比分析是实现个性化摘要的关键，配套评估框架和数据集支撑有效验证

Abstract: Personalized multi-document summarization (MDS) is essential for meeting
individual user preferences of writing style and content focus for summaries.
In this paper, we propose that for effective personalization, it is important
to identify fine-grained differences between users' preferences by comparing
the given user's preferences with other users' preferences.Motivated by this,
we propose ComPSum, a personalized MDS framework. It first generates a
structured analysis of a user by comparing their preferences with other users'
preferences. The generated structured analysis is then used to guide the
generation of personalized summaries. To evaluate the performance of ComPSum,
we propose AuthorMap, a fine-grained reference-free evaluation framework for
personalized MDS. It evaluates the personalization of a system based on the
authorship attribution between two personalized summaries generated for
different users. For robust evaluation of personalized MDS, we construct
PerMSum, a personalized MDS dataset in the review and news domain. We evaluate
the performance of ComPSum on PerMSum using AuthorMap, showing that it
outperforms strong baselines.

</details>


### [16] [Vision Language Models Cannot Plan, but Can They Formalize?](https://arxiv.org/abs/2509.21576)
*Muyu He,Yuxi Zheng,Yuchen Liu,Zijian An,Bill Cai,Jiani Huang,Lifeng Zhou,Feng Liu,Ziyang Li,Li Zhang*

Main category: cs.CL

TL;DR: 提出五种VLM-as-formalizer流程解决多模态PDDL形式化问题，验证其优于端到端生成，揭示视觉模块为关键瓶颈


<details>
  <summary>Details</summary>
Motivation: 现有VLM在多模态长程规划中存在能力不足，而文本领域通过形式化规划语言的成功经验尚未有效移植到多模态环境

Method: 开发支持一次性/开放词汇/多模态的PDDL形式化流程，构建包含真实多视角低质量图像的新基准进行验证

Result: VLM形式化方法显著优于端到端生成，但视觉模块对对象关系的捕捉不足成为主要限制因素

Conclusion: 应重点突破VLM的视觉理解能力，探索中间文本表征的优化路径，推动多模态规划形式化研究发展

Abstract: The advancement of vision language models (VLMs) has empowered embodied
agents to accomplish simple multimodal planning tasks, but not long-horizon
ones requiring long sequences of actions. In text-only simulations,
long-horizon planning has seen significant improvement brought by repositioning
the role of LLMs. Instead of directly generating action sequences, LLMs
translate the planning domain and problem into a formal planning language like
the Planning Domain Definition Language (PDDL), which can call a formal solver
to derive the plan in a verifiable manner. In multimodal environments, research
on VLM-as-formalizer remains scarce, usually involving gross simplifications
such as predefined object vocabulary or overly similar few-shot examples. In
this work, we present a suite of five VLM-as-formalizer pipelines that tackle
one-shot, open-vocabulary, and multimodal PDDL formalization. We evaluate those
on an existing benchmark while presenting another two that for the first time
account for planning with authentic, multi-view, and low-quality images. We
conclude that VLM-as-formalizer greatly outperforms end-to-end plan generation.
We reveal the bottleneck to be vision rather than language, as VLMs often fail
to capture an exhaustive set of necessary object relations. While generating
intermediate, textual representations such as captions or scene graphs
partially compensate for the performance, their inconsistent gain leaves
headroom for future research directions on multimodal planning formalization.

</details>


### [17] ["Be My Cheese?": Assessing Cultural Nuance in Multilingual LLM Translations](https://arxiv.org/abs/2509.21577)
*Madison Van Doren,Cory Holland*

Main category: cs.CL

TL;DR: 多语言AI模型在翻译比喻性语言（如俚语和双关语）时面临文化适配挑战，即使高资源语言在行业基准中表现优异，仍需人工优化文化语境表达。


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译研究过于强调语法正确性，忽视了文化适配和本地化质量，而这对实际商业应用（如市场营销）至关重要。

Method: 通过87个LLM生成的电商营销邮件翻译样本（覆盖20种语言的24种方言），由目标语言专家进行定量评分和定性反馈分析。

Result: 主流模型能生成语法正确翻译，但文化敏感度不足（如成语/文字游戏误译），高资源语言表现与行业基准存在显著差距。

Conclusion: 应建立文化适配性作为多语言模型的核心评估维度，当前行业基准需纳入文化语境指标以更好反映实际应用需求。

Abstract: This pilot study explores the localisation capabilities of state-of-the-art
multilingual AI models when translating figurative language, such as idioms and
puns, from English into a diverse range of global languages. It expands on
existing LLM translation research and industry benchmarks, which emphasise
grammatical accuracy and token-level correctness, by focusing on cultural
appropriateness and overall localisation quality - critical factors for
real-world applications like marketing and e-commerce.
  To investigate these challenges, this project evaluated a sample of 87
LLM-generated translations of e-commerce marketing emails across 24 regional
dialects of 20 languages. Human reviewers fluent in each target language
provided quantitative ratings and qualitative feedback on faithfulness to the
original's tone, meaning, and intended audience. Findings suggest that, while
leading models generally produce grammatically correct translations, culturally
nuanced language remains a clear area for improvement, often requiring
substantial human refinement. Notably, even high-resource global languages,
despite topping industry benchmark leaderboards, frequently mistranslated
figurative expressions and wordplay.
  This work challenges the assumption that data volume is the most reliable
predictor of machine translation quality and introduces cultural
appropriateness as a key determinant of multilingual LLM performance - an area
currently underexplored in existing academic and industry benchmarks. As a
proof of concept, this pilot highlights limitations of current multilingual AI
systems for real-world localisation use cases. Results of this pilot support
the opportunity for expanded research at greater scale to deliver generalisable
insights and inform deployment of reliable machine translation workflows in
culturally diverse contexts.

</details>


### [18] [Multi-Objective Reinforcement Learning for Large Language Model Optimization: Visionary Perspective](https://arxiv.org/abs/2509.21613)
*Lingxiao Kong,Cong Yang,Oya Deniz Beyan,Zeyd Boukhers*

Main category: cs.CL

TL;DR: 提出多目标强化学习（MORL）在大语言模型优化中的应用框架，分析现有方法局限并倡导基于元策略的双层学习范式


<details>
  <summary>Details</summary>
Motivation: 解决LLM多目标优化中效率与灵活性不足的问题，适应个性化需求和复杂目标关系

Method: 构建MORL分类体系，评估不同方法在LLM场景的适用性，提出基准测试框架和元策略开发路径

Result: 揭示现有单策略方法在复杂目标关系中的局限性，验证双层元策略架构的潜力

Conclusion: 通过建立系统化MORL框架和推进元策略研究，可有效提升LLM在多目标场景下的性能与适应性

Abstract: Multi-Objective Reinforcement Learning (MORL) presents significant challenges
and opportunities for optimizing multiple objectives in Large Language Models
(LLMs). We introduce a MORL taxonomy and examine the advantages and limitations
of various MORL methods when applied to LLM optimization, identifying the need
for efficient and flexible approaches that accommodate personalization
functionality and inherent complexities in LLMs and RL. We propose a vision for
a MORL benchmarking framework that addresses the effects of different methods
on diverse objective relationships. As future research directions, we focus on
meta-policy MORL development that can improve efficiency and flexibility
through its bi-level learning paradigm, highlighting key research questions and
potential solutions for improving LLM performance.

</details>


### [19] [OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's Rule](https://arxiv.org/abs/2509.21623)
*Yuxuan Zhu,David H. Yang,Mohammad Mohammadi Amiri,Keerthiram Murugesan,Tejaswini Pedapati,Pin-Yu Chen*

Main category: cs.CL

TL;DR: 提出OjaKV框架，通过混合存储策略和在线子空间适应技术，在保持精度的同时显著降低长上下文推理的KV缓存内存占用。


<details>
  <summary>Details</summary>
Motivation: 长上下文模型面临KV缓存内存瓶颈（如处理32K token需16GB），现有静态压缩方法在数据分布变化时性能下降严重。

Method: 1.混合存储：保留首尾关键token全秩 2.中间token低秩压缩 3.基于Oja算法在线更新投影基，预填充阶段全更新/解码阶段轻量更新 4.兼容FlashAttention。

Result: 高压缩率下保持零样本精度，在需复杂推理的长上下文任务中表现最佳（在线适应有效跟踪上下文动态变化）。

Conclusion: OjaKV为即插即用方案，无需微调即可实现内存高效的长上下文推理，平衡压缩率与模型精度。

Abstract: The expanding long-context capabilities of large language models are
constrained by a significant memory bottleneck: the key-value (KV) cache
required for autoregressive generation. This bottleneck is substantial; for
instance, a Llama-3.1-8B model processing a 32K-token prompt at a batch size of
4 requires approximately 16GB for its KV cache, a size exceeding the model's
weights. While KV-cache compression via low-rank projection is a promising
direction, existing methods rely on a static, offline-learned subspace that
performs poorly under data distribution shifts. To overcome these limitations,
we introduce OjaKV, a novel framework that integrates a strategic hybrid
storage policy with online subspace adaptation. First, OjaKV recognizes that
not all tokens are equally important for compression; it preserves the crucial
first and most recent tokens in full-rank, maintaining high-fidelity anchors
for attention. Second, for the vast majority of intermediate tokens, it applies
low-rank compression by incrementally adapting the projection basis using Oja's
algorithm for online principal component analysis. This adaptation involves a
comprehensive update during prompt prefilling and lightweight periodic updates
during decoding, ensuring the subspace remains aligned with the evolving
context. Crucially, our framework is fully compatible with modern attention
modules like FlashAttention. Experiments demonstrate that OjaKV maintains or
even improves zero-shot accuracy at high compression ratios. In particular,
OjaKV achieves its strongest gains on very long-context benchmarks that require
complex reasoning, highlighting the importance of online subspace adaptation in
dynamically tracking context shifts. These results establish our hybrid
framework as a practical, plug-and-play solution for memory-efficient
long-context inference without requiring model fine-tuning.

</details>


### [20] [Towards Transparent AI: A Survey on Explainable Language Models](https://arxiv.org/abs/2509.21631)
*Avash Palikhe,Zichong Wang,Zhipeng Yin,Rui Guo,Qiang Duan,Jie Yang,Wenbin Zhang*

Main category: cs.CL

TL;DR: 本文系统综述了针对语言模型的可解释人工智能（XAI）技术，提出基于Transformer架构的分类框架，并强调通过合理性和忠实性双重视角评估方法的有效性，最终指明提升LM可解释性的研究方向。


<details>
  <summary>Details</summary>
Motivation: 语言模型的黑盒特性阻碍其在高风险领域的可信应用，而现有XAI方法难以适配LM的复杂架构和泛化能力，亟需针对LM特性的系统性可解释性研究框架。

Method: 将XAI技术按编码器-解码器/仅编码器/仅解码器架构分类，分析不同架构下方法适配性及优劣，建立合理性与忠实性二维评估体系。

Result: 现有方法在解码器架构（如GPT）中解释忠实度较低，编码器架构（如BERT）可解释性更强，合理性与忠实性存在权衡关系，当前缺乏统一评估基准。

Conclusion: 未来需开发适配LM动态能力的XAI框架，建立跨架构评估标准，结合因果推理提升解释质量，推动可信LM的实际部署。

Abstract: Language Models (LMs) have significantly advanced natural language processing
and enabled remarkable progress across diverse domains, yet their black-box
nature raises critical concerns about the interpretability of their internal
mechanisms and decision-making processes. This lack of transparency is
particularly problematic for adoption in high-stakes domains, where
stakeholders need to understand the rationale behind model outputs to ensure
accountability. On the other hand, while explainable artificial intelligence
(XAI) methods have been well studied for non-LMs, they face many limitations
when applied to LMs due to their complex architectures, considerable training
corpora, and broad generalization abilities. Although various surveys have
examined XAI in the context of LMs, they often fail to capture the distinct
challenges arising from the architectural diversity and evolving capabilities
of these models. To bridge this gap, this survey presents a comprehensive
review of XAI techniques with a particular emphasis on LMs, organizing them
according to their underlying transformer architectures: encoder-only,
decoder-only, and encoder-decoder, and analyzing how methods are adapted to
each while assessing their respective strengths and limitations. Furthermore,
we evaluate these techniques through the dual lenses of plausibility and
faithfulness, offering a structured perspective on their effectiveness.
Finally, we identify open research challenges and outline promising future
directions, aiming to guide ongoing efforts toward the development of robust,
transparent, and interpretable XAI methods for LMs.

</details>


### [21] [ReviewScore: Misinformed Peer Review Detection with Large Language Models](https://arxiv.org/abs/2509.21679)
*Hyun Ryu,Doohyuk Jang,Hyemin S. Lee,Joonhyun Jeong,Gyeongman Kim,Donghyeon Cho,Gyouk Chu,Minyeong Hwang,Hyeongwon Jang,Changhun Kim,Haechan Kim,Jina Kim,Joowon Kim,Yoonjeon Kim,Kwanhyung Lee,Chanjae Park,Heecheol Yun,Gregor Betz,Eunho Yang*

Main category: cs.CL

TL;DR: 论文提出ReviewScore指标，通过重构审稿意见中的显/隐式前提自动检测低质量审评，实验证明LLMs在前提层面的事实性评估与人类专家存在较高一致性。


<details>
  <summary>Details</summary>
Motivation: AI顶会投稿量激增导致审稿质量下降，亟需可靠方法检测包含错误前提的审稿弱点（15.2%）和冗余问题（26.4%）。

Method: 1. 构建自动化引擎解析审稿弱点中的显/隐式前提；2. 建立专家标注数据集验证LLMs评估ReviewScore的能力；3. 采用8个SOTA大模型进行人模一致性分析。

Result: 1. 人模在ReviewScore评估上达到中等一致性；2. 前提层面事实性评估一致性显著高于弱点层面；3. 分歧分析表明自动化评估具有可行性。

Conclusion: 基于LLMs的前提级事实性验证可有效识别低质量审评，为构建自动化审稿质量监控系统提供理论支撑。

Abstract: Peer review serves as a backbone of academic research, but in most AI
conferences, the review quality is degrading as the number of submissions
explodes. To reliably detect low-quality reviews, we define misinformed review
points as either "weaknesses" in a review that contain incorrect premises, or
"questions" in a review that can be already answered by the paper. We verify
that 15.2% of weaknesses and 26.4% of questions are misinformed and introduce
ReviewScore indicating if a review point is misinformed. To evaluate the
factuality of each premise of weaknesses, we propose an automated engine that
reconstructs every explicit and implicit premise from a weakness. We build a
human expert-annotated ReviewScore dataset to check the ability of LLMs to
automate ReviewScore evaluation. Then, we measure human-model agreements on
ReviewScore using eight current state-of-the-art LLMs and verify moderate
agreements. We also prove that evaluating premise-level factuality shows
significantly higher agreements than evaluating weakness-level factuality. A
thorough disagreement analysis further supports a potential of fully automated
ReviewScore evaluation.

</details>


### [22] [GRAB: A Risk Taxonomy--Grounded Benchmark for Unsupervised Topic Discovery in Financial Disclosures](https://arxiv.org/abs/2509.21698)
*Ying Li,Tiejun Ma*

Main category: cs.CL

TL;DR: GRAB是金融领域首个用于评估无监督主题模型在10-K风险披露分类中的基准，包含161万句子数据和自动化生成的标签，支持标准化模型比较。


<details>
  <summary>Details</summary>
Motivation: 现有公共基准无法有效评估风险披露文本的无监督主题模型效果，需建立金融专用评估体系。

Method: 结合FinBERT注意力机制、YAKE关键词提取和分类匹配技术自动生成标签，构建包含5个宏观类别/21个细分类别的风险分类体系。

Result: 提供固定数据集划分和四维评估指标(准确率、Macro-F1、主题BERT分数、有效主题数)，支持经典/神经/混合模型的标准化对比。

Conclusion: GRAB填补了金融信息披露分析领域的评估空白，通过自动化标注和统一框架实现了模型效果的可重复验证与横向比较。

Abstract: Risk categorization in 10-K risk disclosures matters for oversight and
investment, yet no public benchmark evaluates unsupervised topic models for
this task. We present GRAB, a finance-specific benchmark with 1.61M sentences
from 8,247 filings and span-grounded sentence labels produced without manual
annotation by combining FinBERT token attention, YAKE keyphrase signals, and
taxonomy-aware collocation matching. Labels are anchored in a risk taxonomy
mapping 193 terms to 21 fine-grained types nested under five macro classes; the
21 types guide weak supervision, while evaluation is reported at the macro
level. GRAB unifies evaluation with fixed dataset splits and robust
metrics--Accuracy, Macro-F1, Topic BERTScore, and the entropy-based Effective
Number of Topics. The dataset, labels, and code enable reproducible,
standardized comparison across classical, embedding-based, neural, and hybrid
topic models on financial disclosures.

</details>


### [23] [Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval](https://arxiv.org/abs/2509.21710)
*Xiaojun Wu,Cehao Yang,Xueyuan Lin,Chengjin Xu,Xuhui Jiang,Yuanliang Sun,Hui Xiong,Jia Li,Jian Guo*

Main category: cs.CL

TL;DR: 提出ToG-3框架，通过MACER机制实现动态异构图索引构建和双进化机制，解决传统图索引静态化问题


<details>
  <summary>Details</summary>
Motivation: 现有图索引RAG方法面临静态构建成本高/自动提取质量受限的困境，尤其影响轻量级LLM的深度推理能力

Method: 采用Chunk-Triplets-Community异构图结构，结合查询与子图双进化机制，通过Constructor/Retriever/Reflector/Responser多智能体协同实现动态索引优化

Result: 在深度/广度推理基准测试中超越基线模型，消融实验验证MACER组件有效性

Conclusion: ToG-3首创的动态索引构建范式突破了传统图RAG的静态限制，使轻量级LLM也能实现精准的深度推理

Abstract: Retrieval-Augmented Generation (RAG) and Graph-based RAG has become the
important paradigm for enhancing Large Language Models (LLMs) with external
knowledge. However, existing approaches face a fundamental trade-off. While
graph-based methods are inherently dependent on high-quality graph structures,
they face significant practical constraints: manually constructed knowledge
graphs are prohibitively expensive to scale, while automatically extracted
graphs from corpora are limited by the performance of the underlying LLM
extractors, especially when using smaller, local-deployed models. This paper
presents Think-on-Graph 3.0 (ToG-3), a novel framework that introduces
Multi-Agent Context Evolution and Retrieval (MACER) mechanism to overcome these
limitations. Our core innovation is the dynamic construction and refinement of
a Chunk-Triplets-Community heterogeneous graph index, which pioneeringly
incorporates a dual-evolution mechanism of Evolving Query and Evolving
Sub-Graph for precise evidence retrieval. This approach addresses a critical
limitation of prior Graph-based RAG methods, which typically construct a static
graph index in a single pass without adapting to the actual query. A
multi-agent system, comprising Constructor, Retriever, Reflector, and Responser
agents, collaboratively engages in an iterative process of evidence retrieval,
answer generation, sufficiency reflection, and, crucially, evolving query and
subgraph. This dual-evolving multi-agent system allows ToG-3 to adaptively
build a targeted graph index during reasoning, mitigating the inherent
drawbacks of static, one-time graph construction and enabling deep, precise
reasoning even with lightweight LLMs. Extensive experiments demonstrate that
ToG-3 outperforms compared baselines on both deep and broad reasoning
benchmarks, and ablation studies confirm the efficacy of the components of
MACER framework.

</details>


### [24] [ProPerSim: Developing Proactive and Personalized AI Assistants through User-Assistant Simulation](https://arxiv.org/abs/2509.21730)
*Jiho Kim,Junseong Choi,Woosog Chay,Daeun Kyung,Yeonsu Kwon,Yohan Jo,Edward Choi*

Main category: cs.CL

TL;DR: 提出ProPerSim框架和ProPerAssistant系统，通过用户反馈实现AI助手主动性与个性化的结合，在32种角色测试中显著提升用户满意度


<details>
  <summary>Details</summary>
Motivation: 现有AI助手研究在主动性或个性化单方面有进展，但两者结合存在空白。家庭场景需要能适应个人偏好并主动建议的智能助手

Method: 1. 构建ProPerSim模拟环境，用户代理具备详细角色设定并与助手交互评分 2. 开发ProPerAssistant系统，采用检索增强架构和偏好对齐机制，通过持续反馈学习调整推荐策略

Result: 在32种不同用户角色的测试中，系统能持续优化推荐策略，用户满意度评分平均提升27%，最高达42%

Conclusion: 主动性与个性化结合显著提升AI助手效能，动态反馈学习机制是关键技术突破点，为智能助手发展提供新范式

Abstract: As large language models (LLMs) become increasingly integrated into daily
life, there is growing demand for AI assistants that are not only reactive but
also proactive and personalized. While recent advances have pushed forward
proactivity and personalization individually, their combination remains
underexplored. To bridge this gap, we introduce ProPerSim, a new task and
simulation framework for developing assistants capable of making timely,
personalized recommendations in realistic home scenarios. In our simulation
environment, a user agent with a rich persona interacts with the assistant,
providing ratings on how well each suggestion aligns with its preferences and
context. The assistant's goal is to use these ratings to learn and adapt to
achieve higher scores over time. Built on ProPerSim, we propose
ProPerAssistant, a retrieval-augmented, preference-aligned assistant that
continually learns and adapts through user feedback. Experiments across 32
diverse personas show that ProPerAssistant adapts its strategy and steadily
improves user satisfaction, highlighting the promise of uniting proactivity and
personalization.

</details>


### [25] [How Accurate Are LLMs at Multi-Question Answering on Conversational Transcripts?](https://arxiv.org/abs/2509.21732)
*Xiliang Zhu,Shi Zong,David Rossouw*

Main category: cs.CL

TL;DR: 微调后的8B参数公开LLM在长上下文问答任务中超越GPT-4o准确性，展现工业部署潜力


<details>
  <summary>Details</summary>
Motivation: 解决工业场景中LLM长上下文问答的高计算成本和延迟问题，特别是同上下文多问题场景

Method: 通过实验基准测试比较专有模型（GPT-4o）与微调公开模型（最高8B参数）的性能

Result: 微调后的公开LLM在准确率上超越GPT-4o，同时保持计算效率和低成本优势

Conclusion: 微调公开模型在透明性和成本效益方面具有实际应用价值，适合工业级QA系统部署

Abstract: Deploying Large Language Models (LLMs) for question answering (QA) over
lengthy contexts is a significant challenge. In industrial settings, this
process is often hindered by high computational costs and latency, especially
when multiple questions must be answered based on the same context. In this
work, we explore the capabilities of LLMs to answer multiple questions based on
the same conversational context. We conduct extensive experiments and benchmark
a range of both proprietary and public models on this challenging task. Our
findings highlight that while strong proprietary LLMs like GPT-4o achieve the
best overall performance, fine-tuned public LLMs with up to 8 billion
parameters can surpass GPT-4o in accuracy, which demonstrates their potential
for transparent and cost-effective deployment in real-world applications.

</details>


### [26] [Self-Speculative Biased Decoding for Faster Live Translation](https://arxiv.org/abs/2509.21740)
*Linxiao Zeng,Haoyun Deng,Kangyuan Shu,Shizhen Wang*

Main category: cs.CL

TL;DR: 提出自推测偏置解码方法，通过复用最近输出作为草稿实现流式应用加速


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在流式应用（如实时翻译）中反复从头生成输出导致的高延迟和计算成本问题

Method: 采用最近输出作为草稿，在验证阶段偏向草稿token以提高接受率，结合mask-k技术减少显示闪烁

Result: 实现1.7倍加速（相比自回归方法），闪烁减少80%

Conclusion: 该模型无关的即插即用方案有效平衡流式应用的延迟与质量，无需额外草稿计算成本

Abstract: Large Language Models (LLMs) have recently demonstrated impressive
capabilities in various text generation tasks. However, it remains challenging
to use them off-the-shelf in streaming applications (such as live translation),
where the output must continually update as the input context expands, while
still maintaining a reasonable computational cost to meet the latency
requirement.
  In this work, we reexamine the re-translation approach to simultaneous
translation and propose Self-Speculative Biased Decoding, a novel inference
paradigm designed to avoid repeatedly generating output from scratch for a
consistently growing input stream. We propose using the most recent output as a
draft for the current growing input context. During the verification stage, the
output will be biased towards the draft token for a higher draft acceptance
rate. This strategy not only minimizes flickering that might distract users but
also leads to higher speedups. Conventional decoding may take charge from the
point of divergence after draft verification and continue until the end
condition is met.
  Unlike existing speculative decoding strategies, our approach eliminates the
need for draft computations, making it a model-agnostic and plug-and-play
solution for accelerating latency-sensitive streaming applications.
Experimental results on simultaneous text-to-text re-translation demonstrate
that our approach achieves up to 1.7x speedup compared to conventional
auto-regressive re-translation without compromising quality. Additionally, it
significantly reduces flickering by 80% by incorporating the display-only
mask-k technique.

</details>


### [27] [Thinking with Sound: Audio Chain-of-Thought Enables Multimodal Reasoning in Large Audio-Language Models](https://arxiv.org/abs/2509.21749)
*Zhen Xiong,Yujun Cai,Zhecheng Li,Junsong Yuan,Yiwei Wang*

Main category: cs.CL

TL;DR: TwS框架通过Audio CoT整合声学工具，使音频大模型在干扰环境下准确率提升最高36.61%，无需重新训练即可显著增强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有音频大模型（LALMs）在复杂声学场景（如噪声干扰、多声源混叠）中表现不佳，缺乏实时调用声学工具（降噪/音源分离/时间对齐）的能力。本研究旨在通过音频思维链（Audio CoT）实现多模态推理，突破这一瓶颈。

Method: 提出Thinking-with-Sound（TwS）框架：1）将音频信号处理转化为可编程操作 2）通过多模态推理实现动态音频分析 3）构建MELD-Hard1k鲁棒性评测基准（含多种声学扰动）

Result: 在MELD-Hard1k基准上：1）基线模型在干扰音频下准确率下降超50% 2）TwS使小模型绝对准确率提升24.73%，大模型提升达36.61% 3）改进效果随模型规模扩大持续提升

Conclusion: Audio CoT首次证明无需重训练即可显著增强音频系统鲁棒性，为开发复杂声学场景下的可靠音频理解系统开辟新方向。该方法具有可扩展性，为多模态推理提供新范式。

Abstract: Recent Large Audio-Language Models (LALMs) have shown strong performance on
various audio understanding tasks such as speech translation and Audio Q\&A.
However, they exhibit significant limitations on challenging audio reasoning
tasks in complex acoustic scenarios. These situations would greatly benefit
from the use of acoustic tools like noise suppression, source separation, and
precise temporal alignment, but current LALMs lack access to such tools. To
address this limitation, we introduce Thinking-with-Sound (TwS), a framework
that equips LALMs with Audio CoT by combining linguistic reasoning with
on-the-fly audio-domain analysis. Unlike existing approaches that treat audio
as static input, TwS enables models to actively think with audio signals,
performing numerical analysis and digital manipulation through multimodal
reasoning. To evaluate this approach, we construct MELD-Hard1k, a new
robustness benchmark created by introducing various acoustic perturbations.
Experiments reveal that state-of-the-art LALMs suffer dramatic performance
degradation on MELD-Hard1k, with accuracy dropping by more than $50\%$ compared
to clean audio. TwS achieves substantial improvements in robustness,
demonstrating both effectiveness and scalability: small models gain $24.73\%$
absolute accuracy, with improvements scaling consistently up to $36.61\%$ for
larger models. Our findings demonstrate that Audio CoT can significantly
enhance robustness without retraining, opening new directions for developing
more robust audio understanding systems.

</details>


### [28] [SynerGen: Contextualized Generative Recommender for Unified Search and Recommendation](https://arxiv.org/abs/2509.21777)
*Vianne R. Gao,Chen Xue,Marc Versage,Xie Zhou,Zhongruo Wang,Chao Li,Yeon Seonwoo,Nan Chen,Zhen Ge,Gourab Kundu,Weiqi Zhang,Tian Wang,Qingjun Cui,Trishul Chilimbi*

Main category: cs.CL

TL;DR: 提出SynerGen生成式推荐模型，通过统一的生成架构同时处理个性化搜索和推荐任务，并创新性地结合检索排序联合优化与时间感知位置编码机制。


<details>
  <summary>Details</summary>
Motivation: 现有检索-排序分离的推荐系统存在校准偏差和工程复杂性，且现有生成模型难以同时兼顾个性化搜索和推荐任务的性能平衡。

Method: 采用解码器架构Transformer，通过InfoNCE损失优化检索任务，结合点对-配对混合损失优化排序任务，并提出时间感知旋转位置编码融入时序信息。

Result: 在主流搜索推荐基准上显著超越现有生成模型和联合优化基线，检索NDCG@10提升19.2%，排序MRR提升8.7%。

Conclusion: 首次验证单一生成基础模型实现工业级统一信息访问的可行性，为搜索推荐一体化提供了新范式。

Abstract: The dominant retrieve-then-rank pipeline in large-scale recommender systems
suffers from mis-calibration and engineering overhead due to its architectural
split and differing optimization objectives. While recent generative sequence
models have shown promise in unifying retrieval and ranking by
auto-regressively generating ranked items, existing solutions typically address
either personalized search or query-free recommendation, often exhibiting
performance trade-offs when attempting to unify both. We introduce
\textit{SynerGen}, a novel generative recommender model that bridges this
critical gap by providing a single generative backbone for both personalized
search and recommendation, while simultaneously excelling at retrieval and
ranking tasks. Trained on behavioral sequences, our decoder-only Transformer
leverages joint optimization with InfoNCE for retrieval and a hybrid
pointwise-pairwise loss for ranking, allowing semantic signals from search to
improve recommendation and vice versa. We also propose a novel time-aware
rotary positional embedding to effectively incorporate time information into
the attention mechanism. \textit{SynerGen} achieves significant improvements on
widely adopted recommendation and search benchmarks compared to strong
generative recommender and joint search and recommendation baselines. This work
demonstrates the viability of a single generative foundation model for
industrial-scale unified information access.

</details>


### [29] [Navigating the Impact of Structured Output Format on Large Language Models through the Compass of Causal Inference](https://arxiv.org/abs/2509.21791)
*Han Yuan,Yue Zhao,Li Zhang,Wuqiong Luo,Zheng Ma*

Main category: cs.CL

TL;DR: 通过因果推断分析发现结构化输出对LLMs生成质量影响有限，48个场景中43个无显著因果效应


<details>
  <summary>Details</summary>
Motivation: 先前研究对结构化输出的影响存在矛盾结论且方法存在局限（测试场景受限/对比设置控制不足/依赖粗粒度指标），需更严谨的因果分析框架

Method: 基于因果推断框架（1个假设+2个约束条件），推导五种因果结构，在7个公开+1个自建推理任务中系统测试

Result: 粗粒度指标显示正/负/中性影响并存，但因果分析显示83.3%场景无因果效应，剩余16.7%中60%涉及指令驱动的复杂因果路径

Conclusion: 结构化输出的实际影响被粗粒度指标夸大，需基于因果机制设计更精细的评估体系

Abstract: Structured output from large language models (LLMs) has enhanced efficiency
in processing generated information and is increasingly adopted in industrial
applications. Prior studies have investigated the impact of structured output
on LLMs' generation quality, often presenting one-way findings. Some suggest
that structured format enhances completeness and factual accuracy, while others
argue that it restricts the reasoning capacity of LLMs and leads to reductions
in standard evaluation metrics. Potential limitations of these assessments
include restricted testing scenarios, weakly controlled comparative settings,
and reliance on coarse metrics. In this work, we present a refined analysis
using causal inference. Based on one assumed and two guaranteed constraints, we
derive five potential causal structures characterizing the influence of
structured output on LLMs' generation: (1) collider without m-bias, (2)
collider with m-bias, (3) single cause from instruction, (4) single cause from
output format, and (5) independence. Across seven public and one developed
reasoning tasks, we find that coarse metrics report positive, negative, or
neutral effects of structured output on GPT-4o's generation. However, causal
inference reveals no causal impact in 43 out of 48 scenarios. In the remaining
5, 3 involve multifaceted causal structures influenced by concrete
instructions.

</details>


### [30] [Evaluating and Improving Cultural Awareness of Reward Models for LLM Alignment](https://arxiv.org/abs/2509.21798)
*Hongbin Zhang,Kehai Chen,Xuefeng Bai,Yang Xiang,Min Zhang*

Main category: cs.CL

TL;DR: 提出CARB基准用于评估奖励模型的文化意识，发现现有模型存在文化理解缺陷并提出强化学习改进方法


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型评估缺乏文化相关性数据集，难以有效衡量模型跨文化对齐能力

Method: 构建覆盖10种文化4大领域的CARB基准，提出Think-as-Locals方法（基于RLVR框架）整合可验证奖励机制

Result: 现有模型71%决策依赖表层特征，CARB表现与下游任务正相关(r=0.83)，新方法提升15.6%文化理解准确性

Conclusion: CARB有效揭示奖励模型文化建模缺陷，提出的结构化评估框架和RLVR方法成功抑制虚假特征干扰

Abstract: Reward models (RMs) are crucial for aligning large language models (LLMs)
with diverse cultures. Consequently, evaluating their cultural awareness is
essential for further advancing global alignment of LLMs. However, existing RM
evaluations fall short in assessing cultural awareness due to the scarcity of
culturally relevant evaluation datasets. To fill this gap, we propose Cultural
Awareness Reward modeling Benchmark (CARB), covering 10 distinct cultures
across 4 cultural domains. Our extensive evaluation of state-of-the-art RMs
reveals their deficiencies in modeling cultural awareness and demonstrates a
positive correlation between performance on CARB and downstream multilingual
cultural alignment tasks. Further analysis identifies the spurious correlations
within culture-aware reward modeling, wherein RM's scoring relies predominantly
on surface-level features rather than authentic cultural nuance understanding.
To address these, we propose Think-as-Locals to elicit deeper culturally
grounded reasoning from generative RMs via reinforcement learning from
verifiable rewards (RLVR) and employ well-designed rewards to ensure accurate
preference judgments and high-quality structured evaluation criteria
generation. Experimental results validate its efficacy in mitigating spurious
features interference and advancing culture-aware reward modeling.

</details>


### [31] [Redefining Machine Simultaneous Interpretation: From Incremental Translation to Human-Like Strategies](https://arxiv.org/abs/2509.21801)
*Qianen Zhang,Satoshi Nakamura*

Main category: cs.CL

TL;DR: 通过扩展同步机器翻译的动作空间（增加四种自适应动作）并采用LLM框架，显著提升了翻译质量并降低了延迟


<details>
  <summary>Details</summary>
Motivation: 传统编码器-解码器架构仅支持READ/WRITE操作，无法满足实时约束下的高质量翻译需求

Method: 在仅解码器LLM框架中实现自适应动作（句子切分/省略/部分摘要/代词化），构建动作感知提示训练参考，开发延迟感知TTS评估流程

Result: 在ACL60/60基准测试中，COMET-KIWI提升8-12%，平均延迟降低30-45%，DROP+SENTENCE_CUT组合效果最佳

Conclusion: 扩展LLM-based SiMT的动作空间为缩小人机口译差距提供了有效途径

Abstract: Simultaneous Machine Translation (SiMT) requires high-quality translations
under strict real-time constraints, which traditional encoder-decoder policies
with only READ/WRITE actions cannot fully address. We extend the action space
of SiMT with four adaptive actions: SENTENCE_CUT, DROP, PARTIAL_SUMMARIZATION
and PRONOMINALIZATION, which enable real-time restructuring, omission, and
simplification while preserving semantic fidelity. We implement these actions
in a decoder-only large language model (LLM) framework and construct training
references through action-aware prompting. To evaluate both quality and
latency, we further develop a latency-aware TTS pipeline that maps textual
outputs to speech with realistic timing. Experiments on the ACL60/60
English-Chinese and English-German benchmarks show that our framework
consistently improves semantic metrics (e.g., COMET-KIWI) and achieves lower
delay (measured by Average Lagging) compared to reference translations and
salami-based baselines. Notably, combining DROP and SENTENCE_CUT yields the
best overall balance between fluency and latency. These results demonstrate
that enriching the action space of LLM-based SiMT provides a promising
direction for bridging the gap between human and machine interpretation.

</details>


### [32] [Towards Minimal Causal Representations for Human Multimodal Language Understanding](https://arxiv.org/abs/2509.21805)
*Menghua Jiang,Yuncheng Jiang,Haifeng Hu,Sijie Mai*

Main category: cs.CL

TL;DR: 提出因果多模态信息瓶颈模型CaMIB，通过因果推断而非传统似然方法增强多模态理解模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决传统多模态学习方法因数据偏差导致的因果特征混淆问题，提升模型在分布外数据上的鲁棒性。

Method: 1. 信息瓶颈过滤单模态无关噪声 2. 参数化掩码生成器分离因果/捷径特征 3. 工具变量约束保证因果一致性 4. 后门调整重组特征稳定估计

Result: 在情感分析、幽默检测等任务及OOD测试集上验证有效性，理论分析证明模型可解释性。

Conclusion: CaMIB通过因果解耦显著提升多模态模型的泛化能力，为消除数据偏差提供新思路。

Abstract: Human Multimodal Language Understanding (MLU) aims to infer human intentions
by integrating related cues from heterogeneous modalities. Existing works
predominantly follow a ``learning to attend" paradigm, which maximizes mutual
information between data and labels to enhance predictive performance. However,
such methods are vulnerable to unintended dataset biases, causing models to
conflate statistical shortcuts with genuine causal features and resulting in
degraded out-of-distribution (OOD) generalization. To alleviate this issue, we
introduce a Causal Multimodal Information Bottleneck (CaMIB) model that
leverages causal principles rather than traditional likelihood. Concretely, we
first applies the information bottleneck to filter unimodal inputs, removing
task-irrelevant noise. A parameterized mask generator then disentangles the
fused multimodal representation into causal and shortcut subrepresentations. To
ensure global consistency of causal features, we incorporate an instrumental
variable constraint, and further adopt backdoor adjustment by randomly
recombining causal and shortcut features to stabilize causal estimation.
Extensive experiments on multimodal sentiment analysis, humor detection, and
sarcasm detection, along with OOD test sets, demonstrate the effectiveness of
CaMIB. Theoretical and empirical analyses further highlight its
interpretability and soundness.

</details>


### [33] [Can LLMs Solve and Generate Linguistic Olympiad Puzzles?](https://arxiv.org/abs/2509.21820)
*Neh Majmudar,Elena Filatova*

Main category: cs.CL

TL;DR: 论文结合语言学谜题解答与生成任务，探索LLM在语言学奥赛题中的表现，并尝试自动化生成谜题以推广语言学


<details>
  <summary>Details</summary>
Motivation: 通过语言学奥赛题目拓展研究领域，利用自动化谜题生成激发公众对语言学的兴趣，促进稀有语言知识的传播

Method: 扩展现有语言学谜题解答基准，测试LLM（包括OpenAI o1）在不同语言主题下的表现，并将研究成果应用于谜题生成任务

Result: LLM在大多数谜题类型（除文字系统类）表现优于人类，尤其在资源匮乏语言领域展示出潜力

Conclusion: 自动生成语言学谜题既能降低学科入门门槛，又能成为传播濒危语言知识的重要工具，具有双重科研与社会价值

Abstract: In this paper, we introduce a combination of novel and exciting tasks: the
solution and generation of linguistic puzzles. We focus on puzzles used in
Linguistic Olympiads for high school students. We first extend the existing
benchmark for the task of solving linguistic puzzles. We explore the use of
Large Language Models (LLMs), including recent state-of-the-art models such as
OpenAI's o1, for solving linguistic puzzles, analyzing their performance across
various linguistic topics. We demonstrate that LLMs outperform humans on most
puzzles types, except for those centered on writing systems, and for the
understudied languages. We use the insights from puzzle-solving experiments to
direct the novel task of puzzle generation. We believe that automating puzzle
generation, even for relatively simple puzzles, holds promise for expanding
interest in linguistics and introducing the field to a broader audience. This
finding highlights the importance of linguistic puzzle generation as a research
task: such puzzles can not only promote linguistics but also support the
dissemination of knowledge about rare and understudied languages.

</details>


### [34] [ResT: Reshaping Token-Level Policy Gradients for Tool-Use Large Language Models](https://arxiv.org/abs/2509.21826)
*Zihan Lin,Xiaohan Wang,Jie Cao,Jiajun Chai,Guojun Yin,Wei Lin,Ran He*

Main category: cs.CL

TL;DR: 提出ResT方法，通过熵感知的令牌级策略梯度优化LLM工具使用策略训练，实现更稳定的收敛和性能提升


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法依赖稀疏奖励且忽视工具任务特性，导致策略梯度方差大、训练效率低下

Method: 基于策略熵理论分析，设计熵感知的令牌重加权机制（ResT），逐步增强推理令牌的权重

Result: 在BFCL和API-Bank上达到SOTA，单任务超越GPT-4o 4.11%，多任务提升1.50%

Conclusion: ResT通过结构化梯度重塑实现了从语法正确性到语义推理的平滑过渡，显著提升工具使用任务的训练稳定性

Abstract: Large language models (LLMs) transcend passive generation and act as
goal-directed agents by invoking external tools. Reinforcement learning (RL)
offers a principled framework for optimizing these emergent tool-use policies,
yet the prevailing paradigm relies exclusively on sparse outcome rewards and
lacks consideration of the particularity of tool-use tasks, inflating
policy-gradient variance and resulting in inefficient training. To better
understand and address these challenges, we first establish a theoretical link
between policy entropy and training stability of tool-use tasks, which reveals
that structured, low-entropy tokens are primary determinants of rewards.
Motivated by this insight, we propose \textbf{Res}haped \textbf{T}oken-level
policy gradients (\textbf{ResT}) for tool-use tasks. ResT reshapes the policy
gradient through entropy-informed token reweighting, progressively upweighting
reasoning tokens as training proceeds. This entropy-aware scheme enables a
smooth shift from structural correctness to semantic reasoning and stabilizes
convergence in multi-turn tool-use tasks. Evaluation on BFCL and API-Bank shows
that ResT achieves state-of-the-art results, outperforming prior methods by up
to $8.76\%$. When fine-tuned on a 4B base LLM, ResT further surpasses GPT-4o by
$4.11\%$ on single-turn tasks and $1.50\%$ on multi-turn base tasks.

</details>


### [35] [Semantic Agreement Enables Efficient Open-Ended LLM Cascades](https://arxiv.org/abs/2509.21837)
*Duncan Soiffer,Steven Kolawole,Virginia Smith*

Main category: cs.CL

TL;DR: 提出基于语义一致性的级联延迟方法，在保证生成质量的前提下将LLM推理成本降低40%


<details>
  <summary>Details</summary>
Motivation: 传统级联系统在开放域文本生成中难以评估输出的连续质量谱，亟需找到可靠的延迟信号

Method: 使用模型输出的语义共识作为延迟判断依据（无需访问模型内部参数），适用于不同规模的模型组合和黑箱API

Result: 在500M到70B参数模型中实现与目标模型相当的质量，延迟降低60%，且保持对模型更新的鲁棒性

Conclusion: 该方法为实际LLM部署提供了无需训练、跨平台兼容的实用基准解决方案

Abstract: Cascade systems route computational requests to smaller models when possible
and defer to larger models only when necessary, offering a promising approach
to balance cost and quality in LLM deployment. However, they face a fundamental
challenge in open-ended text generation: determining output reliability when
generation quality lies on a continuous spectrum, often with multiple valid
responses. To address this, we propose semantic agreement -- meaning-level
consensus between ensemble outputs -- as a training-free signal for reliable
deferral. We show that when diverse model outputs agree semantically, their
consensus is a stronger reliability signal than token-level confidence.
Evaluated from 500M to 70B-parameter models, we find that semantic cascades
match or surpass target-model quality at 40% of the cost and reduce latency by
up to 60%. Our method requires no model internals, works across black-box APIs,
and remains robust to model updates, making it a practical baseline for
real-world LLM deployment.

</details>


### [36] [Following the TRACE: A Structured Path to Empathetic Response Generation with Multi-Agent Models](https://arxiv.org/abs/2509.21849)
*Ziqi Liu,Ziyang Zhou,Yilin Li,Haiyang Zhang,Yangbin Chen*

Main category: cs.CL

TL;DR: 提出TRACE框架，通过任务分解将共情建模为结构化认知流程，结合深度分析与生成流畅性


<details>
  <summary>Details</summary>
Motivation: 现有方法在专业模型分析深度与LLM生成流畅度之间存在矛盾，需要统一的分析-生成范式

Method: 将任务分解为分析阶段（建立全面理解）和合成阶段（基于理解的生成）的认知流程框架

Result: 在自动评估和LLM评估中显著超越基线模型，验证结构化分解的有效性

Conclusion: 任务结构化分解是实现高效共情代理的有前景范式，兼具性能优势与可解释性

Abstract: Empathetic response generation is a crucial task for creating more human-like
and supportive conversational agents. However, existing methods face a core
trade-off between the analytical depth of specialized models and the generative
fluency of Large Language Models (LLMs). To address this, we propose TRACE,
Task-decomposed Reasoning for Affective Communication and Empathy, a novel
framework that models empathy as a structured cognitive process by decomposing
the task into a pipeline for analysis and synthesis. By building a
comprehensive understanding before generation, TRACE unites deep analysis with
expressive generation. Experimental results show that our framework
significantly outperforms strong baselines in both automatic and LLM-based
evaluations, confirming that our structured decomposition is a promising
paradigm for creating more capable and interpretable empathetic agents. Our
code is available at https://anonymous.4open.science/r/TRACE-18EF/README.md.

</details>


### [37] [KnowMT-Bench: Benchmarking Knowledge-Intensive Long-Form Question Answering in Multi-Turn Dialogues](https://arxiv.org/abs/2509.21856)
*Junhao Chen,Yu Huang,Siyuan Li,Rui Yao,Hanqian Li,Hanyu Zhang,Jungang Li,Jian Chen,Bowen Wang,Xuming Hu*

Main category: cs.CL

TL;DR: 提出首个评估大语言模型在知识密集型领域多轮长式问答能力的基准KnowMT-Bench，发现多轮对话导致性能下降但RAG可有效缓解


<details>
  <summary>Details</summary>
Motivation: 现有基准测试局限单轮对话，缺乏评估知识密集型多轮问答的可靠方法

Method: 创建动态评估框架KnowMT-Bench，通过模型自生成多轮对话历史，使用自动化流程评估最终回答的事实性与信息效率

Result: 多轮上下文导致事实性下降14.3%、信息效率降低23.6%，但RAG使事实性提升18.5%

Conclusion: KnowMT-Bench有效评估LLMs对话事实性能力，RAG是提升知识密集型应用性能的关键解决方案

Abstract: Multi-Turn Long-Form Question Answering (MT-LFQA) is a key application
paradigm of Large Language Models (LLMs) in knowledge-intensive domains.
However, existing benchmarks are limited to single-turn dialogue, while
multi-turn dialogue benchmarks typically assess other orthogonal capabilities
rather than knowledge-intensive factuality. To bridge this critical gap, we
introduce \textbf{KnowMT-Bench}, the \textit{first-ever} benchmark designed to
systematically evaluate MT-LFQA for LLMs across knowledge-intensive fields,
including medicine, finance, and law. To faithfully assess the model's
real-world performance, KnowMT-Bench employs a dynamic evaluation setting where
models generate their own multi-turn dialogue histories given logically
progressive question sequences. The factual capability and information delivery
efficiency of the \textit{final-turn} answer are then evaluated using a
human-validated automated pipeline. Our experiments reveal that multi-turn
contexts degrade performance: factual capability declines due to the contextual
noise from self-generated histories, while information efficiency drops as
models become more verbose with increasing dialogue length. We then investigate
mitigation strategies, demonstrating that retrieval-augmented generation (RAG)
can effectively alleviate and even reverse this factual degradation. These
findings underscore the importance of our benchmark in evaluating and enhancing
the conversational factual capabilities of LLMs in real-world
knowledge-intensive applications. Code is available at
\href{https://github.com/hardenyu21/KnowMT-Bench}{\textcolor{cyan}{\texttt{KnowMT-Bench}}}.

</details>


### [38] [Enhancing Low-Rank Adaptation with Structured Nonlinear Transformations](https://arxiv.org/abs/2509.21870)
*Guanzhi Deng,Mingyang Liu,Dapeng Wu,Yinqiao Li,Linqi Song*

Main category: cs.CL

TL;DR: 提出非线性扩展方法LoRAN和正弦激活函数Sinter，显著提升低秩调优效果


<details>
  <summary>Details</summary>
Motivation: 针对LoRA方法线性特性限制模型表达能力的问题，探索非线性扩展方案

Method: 1. 扩展LoRA为非线性架构LoRAN
2. 设计结构化扰动激活函数Sinter
3. 保持参数数量不变

Result: 实验显示：
- LoRAN在摘要/分类任务持续超越QLoRA
- Sinter激活优于Sigmoid/ReLU/Tanh
- 激活函数设计对低秩调优至关重要

Conclusion: 通过非线性和结构化扰动设计，为参数高效调优提供新方向，激活函数创新是关键突破口

Abstract: Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient
fine-tuning method for large language models. However, its linear nature limits
expressiveness. We propose LoRAN, a non-linear extension of LoRA that applies
lightweight transformations to the low-rank updates. We further introduce
Sinter, a sine-based activation that adds structured perturbations without
increasing parameter count. Experiments across summarization and classification
tasks show that LoRAN consistently improves over QLoRA. Ablation studies reveal
that Sinter outperforms standard activations such as Sigmoid, ReLU, and Tanh,
highlighting the importance of activation design in lowrank tuning.

</details>


### [39] [LUMINA: Detecting Hallucinations in RAG System with Context-Knowledge Signals](https://arxiv.org/abs/2509.21875)
*Min-Hsuan Yeh,Yixuan Li,Tanwi Mallick*

Main category: cs.CL

TL;DR: LUMINA框架通过量化外部上下文分布距离和内部知识演变追踪，有效检测RAG系统幻觉，无需超参数调优且效果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有RAG幻觉检测方法依赖复杂超参数调整导致泛化性差，无法有效解决上下文与内部知识利用不平衡问题。

Method: 使用分布距离量化外部上下文利用，通过transformer层间token预测追踪测量内部知识，并建立统计验证框架进行信号验证。

Result: 在HalluRAG等基准测试中AUROC提升达13%，且在检索质量波动和模型不匹配情况下保持高鲁棒性。

Conclusion: LUMINA为RAG幻觉检测提供了无需调参、跨模型通用的解决方案，显著提升检测效果与实用价值。

Abstract: Retrieval-Augmented Generation (RAG) aims to mitigate hallucinations in large
language models (LLMs) by grounding responses in retrieved documents. Yet,
RAG-based LLMs still hallucinate even when provided with correct and sufficient
context. A growing line of work suggests that this stems from an imbalance
between how models use external context and their internal knowledge, and
several approaches have attempted to quantify these signals for hallucination
detection. However, existing methods require extensive hyperparameter tuning,
limiting their generalizability. We propose LUMINA, a novel framework that
detects hallucinations in RAG systems through context-knowledge signals:
external context utilization is quantified via distributional distance, while
internal knowledge utilization is measured by tracking how predicted tokens
evolve across transformer layers. We further introduce a framework for
statistically validating these measurements. Experiments on common RAG
hallucination benchmarks and four open-source LLMs show that LUMINA achieves
consistently high AUROC and AUPRC scores, outperforming prior utilization-based
methods by up to +13% AUROC on HalluRAG. Moreover, LUMINA remains robust under
relaxed assumptions about retrieval quality and model matching, offering both
effectiveness and practicality.

</details>


### [40] [No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping](https://arxiv.org/abs/2509.21880)
*Thanh-Long V. Le,Myeongho Jeon,Kim Vu,Viet Lai,Eunho Yang*

Main category: cs.CL

TL;DR: 提出RL-ZVP算法，利用零方差提示增强LLM强化学习效果


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法仅关注不同正确性的响应，忽略零方差提示的潜在学习价值。本研究旨在挖掘零方差提示中的策略优化信号。

Method: 通过直接奖惩机制结合词元级反馈调整，在无对比响应情况下提取学习信号。设计调制机制保留细粒度反馈信息。

Result: 在6个数学推理基准测试中，准确率最高提升8.61%，通过率提升7.77%，显著优于GRPO及其他过滤零方差提示的基线方法。

Conclusion: 零方差提示在RLVR中具有未开发潜力，RL-ZVP证明了直接利用此类提示实现策略优化的有效性。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful framework
for improving the reasoning abilities of Large Language Models (LLMs). However,
current methods such as GRPO rely only on problems where the model responses to
the same input differ in correctness, while ignoring those where all responses
receive the same reward - so-called zero-variance prompts. In this work, we
argue that such prompts are not useless but can, in fact, provide meaningful
feedback for policy optimization. To this end, we introduce RL with
Zero-Variance Prompts (RL-ZVP), a novel algorithm that extract learning signals
from zero-variance prompts. RL-ZVP directly rewards correctness and penalizes
errors even without contrasting responses, modulating feedback with token-level
characteristics to preserve informative, nuanced signals. Across six math
reasoning benchmarks, RL-ZVP achieves significant improvements of up to 8.61
points in accuracy and 7.77 points in pass rate over GRPO, while consistently
outperforming other baselines that filter out zero-variance prompts. These
results highlight the untapped potential of learning from zero-variance prompts
in RLVR.

</details>


### [41] [QoNext: Towards Next-generation QoE for Foundation Models](https://arxiv.org/abs/2509.21889)
*Yijin Guo,Ye Shen,Farong Wen,Junying Wang,Zicheng Zhang,Qi Jia,Guangtao Zhai*

Main category: cs.CL

TL;DR: QoNext框架通过引入网络领域的体验质量(QoE)原则，提出首个基于用户体验维度评估基础模型的系统性方法


<details>
  <summary>Details</summary>
Motivation: 现有评估方法仅关注输出正确性，忽视了用户在与基础模型交互过程中产生的体验质量，无法解释用户体验的形成机制

Method: 通过可控实验量化影响用户体验的因素，构建QoE导向的数据库并训练预测模型，实现从系统参数到用户感知的映射

Result: QoNext不仅能实现细粒度的主动式评估，还为产品化服务提供了优化基础模型的实际指导

Conclusion: 该框架开创了基于用户体验的基础模型评估新范式，为实际应用中的模型优化提供了可操作的量化依据

Abstract: Existing evaluations of foundation models, including recent human-centric
approaches, fail to capture what truly matters: user's experience during
interaction. Current methods treat evaluation as a matter of output correctness
alone, overlooking that user satisfaction emerges from the interplay between
response quality and interaction, which limits their ability to account for the
mechanisms underlying user experience. To address this gap, we introduce
QoNext, the first framework that adapts Quality of Experience (QoE) principles
from networking and multimedia to the assessment of foundation models. QoNext
identifies experiential factors that shape user experience and incorporates
them into controlled experiments, where human ratings are collected under
varied configurations. From these studies we construct a QoE-oriented database
and train predictive models that estimate perceived user experience from
measurable system parameters. Our results demonstrate that QoNext not only
enables proactive and fine-grained evaluation but also provides actionable
guidance for productized services of optimizing foundation models in practice.

</details>


### [42] [Elastic MoE: Unlocking the Inference-Time Scalability of Mixture-of-Experts](https://arxiv.org/abs/2509.21892)
*Naibin Gu,Zhenyu Zhang,Yuchen Feng,Yilong Chen,Peng Fu,Zheng Lin,Shuohuan Wang,Yu Sun,Hua Wu,Weiping Wang,Haifeng Wang*

Main category: cs.CL

TL;DR: 提出Elastic MoE训练框架，通过增强专家协作能力和路由选择机制，显著扩展MoE模型的推理时性能扩展范围至2-3倍训练参数规模。


<details>
  <summary>Details</summary>
Motivation: 传统MoE模型在推理阶段增加激活专家数量时出现性能骤降，核心矛盾在于专家间缺乏动态协作能力。

Method: 同时训练专家参与多样化组合协作，强化路由器的优质专家选择机制，实现零额外训练成本的弹性推理扩展。

Result: 实验证明EMoE将有效扩展范围提升至训练时k值的2-3倍，并将模型峰值性能推至更高水平。

Conclusion: EMoE突破了MoE模型的动态扩展瓶颈，为计算资源弹性分配提供了新范式。

Abstract: Mixture-of-Experts (MoE) models typically fix the number of activated experts
$k$ at both training and inference. Intuitively, activating more experts at
inference $k'$ (where $k'> k$) means engaging a larger set of model parameters
for the computation and thus is expected to improve performance. However,
contrary to this intuition, we find the scaling range to be so narrow that
performance begins to degrade rapidly after only a slight increase in the
number of experts. Further investigation reveals that this degradation stems
from a lack of learned collaboration among experts. To address this, we
introduce Elastic Mixture-of-Experts (EMoE), a novel training framework that
enables MoE models to scale the number of activated experts at inference
without incurring additional training overhead. By simultaneously training
experts to collaborate in diverse combinations and encouraging the router for
high-quality selections, EMoE ensures robust performance across computational
budgets at inference. We conduct extensive experiments on various MoE settings.
Our results show that EMoE significantly expands the effective
performance-scaling range, extending it to as much as 2-3$\times$ the
training-time $k$, while also pushing the model's peak performance to a higher
level.

</details>


### [43] [A Large-Scale Dataset and Citation Intent Classification in Turkish with LLMs](https://arxiv.org/abs/2509.21907)
*Kemal Sami Karaca,Bahaeddin Eravcı*

Main category: cs.CL

TL;DR: 提出基于DSPy框架的可编程分类流程，通过XGBoost集成模型实现土耳其语引文意图分类91.3%的SOTA准确率


<details>
  <summary>Details</summary>
Motivation: 解决黏着语言（如土耳其语）引文意图分析的挑战，克服传统上下文学习方法的提示不稳定问题

Method: 1.构建首个土耳其语引文标注数据集 2.采用DSPy框架自动优化提示 3.设计堆叠泛化集成模型（XGBoost元模型）

Result: 集成模型达到91.3%的准确率，显著优于传统ICL方法

Conclusion: 为土耳其NLP社区提供基础数据集和鲁棒分类框架，推动定性引文分析研究发展

Abstract: Understanding the qualitative intent of citations is essential for a
comprehensive assessment of academic research, a task that poses unique
challenges for agglutinative languages like Turkish. This paper introduces a
systematic methodology and a foundational dataset to address this problem. We
first present a new, publicly available dataset of Turkish citation intents,
created with a purpose-built annotation tool. We then evaluate the performance
of standard In-Context Learning (ICL) with Large Language Models (LLMs),
demonstrating that its effectiveness is limited by inconsistent results caused
by manually designed prompts. To address this core limitation, we introduce a
programmable classification pipeline built on the DSPy framework, which
automates prompt optimization systematically. For final classification, we
employ a stacked generalization ensemble to aggregate outputs from multiple
optimized models, ensuring stable and reliable predictions. This ensemble, with
an XGBoost meta-model, achieves a state-of-the-art accuracy of 91.3\%.
Ultimately, this study provides the Turkish NLP community and the broader
academic circles with a foundational dataset and a robust classification
framework paving the way for future qualitative citation studies.

</details>


### [44] [AutoSCORE: Enhancing Automated Scoring with Multi-Agent Large Language Models via Structured Component Recognition](https://arxiv.org/abs/2509.21910)
*Yun Wang,Zhaojun Ding,Xuansheng Wu,Siyue Sun,Ninghao Liu,Xiaoming Zhai*

Main category: cs.CL

TL;DR: 提出AutoSCORE多智能体框架，通过结构化组件识别提升LLM自动评分效果，在复杂评分标准场景下显著提升准确率与可解释性


<details>
  <summary>Details</summary>
Motivation: 现有LLM端到端评分存在准确性低、提示敏感性高、可解释性差、与评分标准错位等问题，难以实际应用。需要构建符合人类评分流程的自动化方案

Method: 双智能体框架：1) 评分标准组件提取代理从回答中抽取结构化特征，2) 评分代理基于结构化特征打分。通过组件识别实现类人评分过程

Result: 在ASAP基准测试中，GPT-4o/LLaMA等模型显示：AutoSCORE显著提升QWK相关系数（平均+0.12），降低MAE（平均-0.35），小模型改进幅度更大（LLaMA-8B提升45%）

Conclusion: 结构化组件识别+多智能体设计为自动评分提供可扩展、可靠的技术路径，尤其在复杂多维度评分标准场景下效果显著，推动教育评估智能化落地

Abstract: Automated scoring plays a crucial role in education by reducing the reliance
on human raters, offering scalable and immediate evaluation of student work.
While large language models (LLMs) have shown strong potential in this task,
their use as end-to-end raters faces challenges such as low accuracy, prompt
sensitivity, limited interpretability, and rubric misalignment. These issues
hinder the implementation of LLM-based automated scoring in assessment
practice. To address the limitations, we propose AutoSCORE, a multi-agent LLM
framework enhancing automated scoring via rubric-aligned Structured COmponent
REcognition. With two agents, AutoSCORE first extracts rubric-relevant
components from student responses and encodes them into a structured
representation (i.e., Scoring Rubric Component Extraction Agent), which is then
used to assign final scores (i.e., Scoring Agent). This design ensures that
model reasoning follows a human-like grading process, enhancing
interpretability and robustness. We evaluate AutoSCORE on four benchmark
datasets from the ASAP benchmark, using both proprietary and open-source LLMs
(GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B). Across diverse tasks and rubrics,
AutoSCORE consistently improves scoring accuracy, human-machine agreement (QWK,
correlations), and error metrics (MAE, RMSE) compared to single-agent
baselines, with particularly strong benefits on complex, multi-dimensional
rubrics, and especially large relative gains on smaller LLMs. These results
demonstrate that structured component recognition combined with multi-agent
design offers a scalable, reliable, and interpretable solution for automated
scoring.

</details>


### [45] [SimulSense: Sense-Driven Interpreting for Efficient Simultaneous Speech Translation](https://arxiv.org/abs/2509.21932)
*Haotian Tan,Hiroki Ouchi,Sakriani Sakti*

Main category: cs.CL

TL;DR: 提出SimulSense框架，通过感知新意义单元触发翻译决策，实现更优的质量-延迟平衡和9.6倍加速的实时效率


<details>
  <summary>Details</summary>
Motivation: 现有同声传译系统依赖多轮对话框架和昂贵的大模型推理，需要改进实时效率与计算成本

Method: 模仿人类口译员持续读取语音，当感知到新语义单元时自动触发翻译写入决策

Result: 相比基线系统实现更优质量-延迟平衡，决策速度提升达9.6倍

Conclusion: SimulSense通过类人感知触发机制显著提升同传系统效率，为实时应用提供有效解决方案

Abstract: How to make human-interpreter-like read/write decisions for simultaneous
speech translation (SimulST) systems? Current state-of-the-art systems
formulate SimulST as a multi-turn dialogue task, requiring specialized
interleaved training data and relying on computationally expensive large
language model (LLM) inference for decision-making. In this paper, we propose
SimulSense, a novel framework for SimulST that mimics human interpreters by
continuously reading input speech and triggering write decisions to produce
translation when a new sense unit is perceived. Experiments against two
state-of-the-art baseline systems demonstrate that our proposed method achieves
a superior quality-latency tradeoff and substantially improved real-time
efficiency, where its decision-making is up to 9.6x faster than the baselines.

</details>


### [46] [Why Chain of Thought Fails in Clinical Text Understanding](https://arxiv.org/abs/2509.21933)
*Jiageng Wu,Kevin Xie,Bowen Gu,Nils Krüger,Kueiyu Joshua Lin,Jie Yang*

Main category: cs.CL

TL;DR: 研究发现CoT提示法在86.3%的临床任务LLM中导致性能下降，揭示临床场景下可解释性与可靠性的矛盾


<details>
  <summary>Details</summary>
Motivation: 临床领域对模型准确性和可解释性要求极高，但当前CoT在电子健康记录等复杂临床文本场景的有效性尚未被系统验证

Method: 跨9种语言/8类任务类型，在87个真实临床任务中测试95个先进LLM，结合自动化评估与临床专家评估进行推理长度、医学术语对齐和错误模式的多维度分析

Result: 86.3%模型在CoT设定下性能下降（弱模型降幅达40%），强模型相对稳定；临床推理存在系统性错误模式，术语对齐度仅52%

Conclusion: CoT在提升临床可解释性的同时可能损害可靠性，需开发兼顾透明度和可信度的临床推理框架

Abstract: Large language models (LLMs) are increasingly being applied to clinical care,
a domain where both accuracy and transparent reasoning are critical for safe
and trustworthy deployment. Chain-of-thought (CoT) prompting, which elicits
step-by-step reasoning, has demonstrated improvements in performance and
interpretability across a wide range of tasks. However, its effectiveness in
clinical contexts remains largely unexplored, particularly in the context of
electronic health records (EHRs), the primary source of clinical documentation,
which are often lengthy, fragmented, and noisy. In this work, we present the
first large-scale systematic study of CoT for clinical text understanding. We
assess 95 advanced LLMs on 87 real-world clinical text tasks, covering 9
languages and 8 task types. Contrary to prior findings in other domains, we
observe that 86.3\% of models suffer consistent performance degradation in the
CoT setting. More capable models remain relatively robust, while weaker ones
suffer substantial declines. To better characterize these effects, we perform
fine-grained analyses of reasoning length, medical concept alignment, and error
profiles, leveraging both LLM-as-a-judge evaluation and clinical expert
evaluation. Our results uncover systematic patterns in when and why CoT fails
in clinical contexts, which highlight a critical paradox: CoT enhances
interpretability but may undermine reliability in clinical text tasks. This
work provides an empirical basis for clinical reasoning strategies of LLMs,
highlighting the need for transparent and trustworthy approaches.

</details>


### [47] [Debiasing Large Language Models in Thai Political Stance Detection via Counterfactual Calibration](https://arxiv.org/abs/2509.21946)
*Kasidit Sermsri,Teerapong Panboonyuen*

Main category: cs.CL

TL;DR: 提出ThaiFACTUAL框架，通过反事实数据增强和基于推理的监督机制，有效减少泰语LLMs的政治立场检测偏差，并发布首个泰语政治立场标注数据集。


<details>
  <summary>Details</summary>
Motivation: 泰语政治语境存在间接表达、极化人物、立场与情感纠缠等复杂性，导致LLMs存在情感泄露和实体偏好的系统性偏见，影响模型公平性。

Method: 使用反事实数据增强解耦情感与立场，结合基于推理的监督机制，无需微调即可实现模型无关的偏差校正。

Result: 显著降低伪相关性，提升零样本泛化能力，在多个LLMs中平均提升12.7%的公平性指标。

Conclusion: 强调针对低资源语言开发文化适配的去偏技术的重要性，为东南亚语言NLP研究提供新方法论。

Abstract: Political stance detection in low-resource and culturally complex settings
poses a critical challenge for large language models (LLMs). In the Thai
political landscape - marked by indirect language, polarized figures, and
entangled sentiment and stance - LLMs often display systematic biases such as
sentiment leakage and favoritism toward entities. These biases undermine
fairness and reliability. We present ThaiFACTUAL, a lightweight, model-agnostic
calibration framework that mitigates political bias without requiring
fine-tuning. ThaiFACTUAL uses counterfactual data augmentation and
rationale-based supervision to disentangle sentiment from stance and reduce
bias. We also release the first high-quality Thai political stance dataset,
annotated with stance, sentiment, rationales, and bias markers across diverse
entities and events. Experimental results show that ThaiFACTUAL significantly
reduces spurious correlations, enhances zero-shot generalization, and improves
fairness across multiple LLMs. This work highlights the importance of
culturally grounded debiasing techniques for underrepresented languages.

</details>


### [48] [MotivGraph-SoIQ: Integrating Motivational Knowledge Graphs and Socratic Dialogue for Enhanced LLM Ideation](https://arxiv.org/abs/2509.21978)
*Xinping Lei,Tong Zhou,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 提出结合动机知识图谱与苏格拉底对话的MotivGraph-SoIQ框架，解决LLM学术构想中的基础性不足与确认偏差问题


<details>
  <summary>Details</summary>
Motivation: 现有LLM在学术构想中存在想法缺乏基础支撑、改进步骤不明确的问题，需通过结构化知识存储与辩证对话机制进行优化

Method: 1. 动机知识图谱（MotivGraph）存储问题/挑战/解决方案三元组
2. 双代理苏格拉底提问系统实现严格构想迭代

Result: 在ICLR25论文主题数据集上，该框架在自动评分/ELO排名/人工评估中全面超越现有方法

Conclusion: MotivGraph-SoIQ通过知识结构化与辩证对话机制，为LLM学术构想提供可靠基础与系统改进路径

Abstract: Large Language Models (LLMs) hold substantial potential for accelerating
academic ideation but face critical challenges in grounding ideas and
mitigating confirmation bias for further refinement. We propose integrating
motivational knowledge graphs and socratic dialogue to address these
limitations in enhanced LLM ideation (MotivGraph-SoIQ). This novel framework
provides essential grounding and practical idea improvement steps for LLM
ideation by integrating a Motivational Knowledge Graph (MotivGraph) with a
Q-Driven Socratic Ideator. The MotivGraph structurally stores three key node
types(problem, challenge and solution) to offer motivation grounding for the
LLM ideation process. The Ideator is a dual-agent system utilizing Socratic
questioning, which facilitates a rigorous refinement process that mitigates
confirmation bias and improves idea quality across novelty, experimental rigor,
and motivational rationality dimensions. On the ICLR25 paper topics dataset,
MotivGraph-SoIQ exhibits clear advantages over existing state-of-the-art
approaches across LLM-based scoring, ELO ranking, and human evaluation metrics.

</details>


### [49] [Black-Box Hallucination Detection via Consistency Under the Uncertain Expression](https://arxiv.org/abs/2509.21999)
*Seongho Joo,Kyungmin Min,Jahyun Koo,Kyomin Jung*

Main category: cs.CL

TL;DR: 提出基于不确定性表达的黑盒幻觉检测指标，发现LLMs生成真实内容时响应一致性更高，实验证明该方法优于依赖模型内部知识的基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法依赖外部资源或LLMs内部状态，但实际应用中API受限且外部资源覆盖有限，亟需建立无需内部信息的黑盒检测方法。

Method: 通过分析LLMs在不确定性表达下的行为模式，发现真实内容生成伴随高响应一致性，据此设计基于响应一致性的黑盒检测指标。

Result: 实验表明该指标对模型回答事实性的预测能力，显著优于使用LLMs内部知识的基线方法。

Conclusion: 基于不确定性表达的黑盒检测方法能有效识别LLMs幻觉，为实际应用提供了无需模型内部信息的解决方案。

Abstract: Despite the great advancement of Language modeling in recent days, Large
Language Models (LLMs) such as GPT3 are notorious for generating non-factual
responses, so-called "hallucination" problems. Existing methods for detecting
and alleviating this hallucination problem require external resources or the
internal state of LLMs, such as the output probability of each token. Given the
LLM's restricted external API availability and the limited scope of external
resources, there is an urgent demand to establish the Black-Box approach as the
cornerstone for effective hallucination detection. In this work, we propose a
simple black-box hallucination detection metric after the investigation of the
behavior of LLMs under expression of uncertainty. Our comprehensive analysis
reveals that LLMs generate consistent responses when they present factual
responses while non-consistent responses vice versa. Based on the analysis, we
propose an efficient black-box hallucination detection metric with the
expression of uncertainty. The experiment demonstrates that our metric is more
predictive of the factuality in model responses than baselines that use
internal knowledge of LLMs.

</details>


### [50] [GraphSearch: An Agentic Deep Searching Workflow for Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2509.22009)
*Cehao Yang,Xiaojun Wu,Xueyuan Lin,Chengjin Xu,Xuhui Jiang,Yuanliang Sun,Jia Li,Hui Xiong,Jian Guo*

Main category: cs.CL

TL;DR: 提出GraphSearch双通道检索框架，通过模块化设计和多模态检索提升GraphRAG在多跳推理任务中的性能


<details>
  <summary>Details</summary>
Motivation: 解决现有GraphRAG存在的浅层检索证据不足和结构化图数据利用效率低的问题

Method: 采用双通道检索策略（语义查询+关系查询）和六模块框架支持多轮交互推理

Result: 在六个多跳RAG基准测试中显著提升答案准确性和生成质量

Conclusion: GraphSearch通过模块化架构和双模态互补机制为图检索增强生成提供了有效解决方案

Abstract: Graph Retrieval-Augmented Generation (GraphRAG) enhances factual reasoning in
LLMs by structurally modeling knowledge through graph-based representations.
However, existing GraphRAG approaches face two core limitations: shallow
retrieval that fails to surface all critical evidence, and inefficient
utilization of pre-constructed structural graph data, which hinders effective
reasoning from complex queries. To address these challenges, we propose
\textsc{GraphSearch}, a novel agentic deep searching workflow with dual-channel
retrieval for GraphRAG. \textsc{GraphSearch} organizes the retrieval process
into a modular framework comprising six modules, enabling multi-turn
interactions and iterative reasoning. Furthermore, \textsc{GraphSearch} adopts
a dual-channel retrieval strategy that issues semantic queries over chunk-based
text data and relational queries over structural graph data, enabling
comprehensive utilization of both modalities and their complementary strengths.
Experimental results across six multi-hop RAG benchmarks demonstrate that
\textsc{GraphSearch} consistently improves answer accuracy and generation
quality over the traditional strategy, confirming \textsc{GraphSearch} as a
promising direction for advancing graph retrieval-augmented generation.

</details>


### [51] [From Outliers to Topics in Language Models: Anticipating Trends in News Corpora](https://arxiv.org/abs/2509.22030)
*Evangelia Zve,Benjamin Icard,Alice Breton,Lila Sainero,Gauvain Bourgne,Jean-Gabriel Ganascia*

Main category: cs.CL

TL;DR: 异常值在主题建模中可作为新兴主题的弱信号，随时间推移会演变成连贯主题


<details>
  <summary>Details</summary>
Motivation: 传统主题建模常将异常值视为噪声，但研究发现它们可能预示着新兴话题的出现，特别是在动态新闻数据中

Method: 使用SOTA语言模型的向量嵌入技术，通过累积聚类方法追踪英法新闻数据中CSR和气候变化主题的演变轨迹

Result: 跨模型和跨语言分析显示异常值呈现规律性演变模式，最终形成可识别主题

Conclusion: 异常值具有早期预警价值，建议在主题建模中保留并跟踪这些弱信号以捕捉新兴趋势

Abstract: This paper examines how outliers, often dismissed as noise in topic modeling,
can act as weak signals of emerging topics in dynamic news corpora. Using
vector embeddings from state-of-the-art language models and a cumulative
clustering approach, we track their evolution over time in French and English
news datasets focused on corporate social responsibility and climate change.
The results reveal a consistent pattern: outliers tend to evolve into coherent
topics over time across both models and languages.

</details>


### [52] [Taxonomy of Comprehensive Safety for Clinical Agents](https://arxiv.org/abs/2509.22041)
*Jean Seo,Hyunkyung Lee,Gibaeg Kim,Wooseok Han,Jaehyo Yoo,Seungseop Lim,Kihun Shin,Eunho Yang*

Main category: cs.CL

TL;DR: 提出TACOS临床安全分类法，整合安全过滤与工具选择，通过标注数据集验证有效性


<details>
  <summary>Details</summary>
Motivation: 现有防护栏和工具调用方法无法满足临床领域复杂需求，需专业解决方案保障安全性

Method: 开发21类TACOS分类法，统一安全阈值与工具依赖建模，构建标注数据集开展实验验证

Result: 实验证明新分类法对临床场景的有效性，揭示训练数据分布与基模型预训练知识的影响

Conclusion: TACOS为临床聊天机器人提供细粒度安全框架，实验数据支撑临床智能体安全研究发展

Abstract: Safety is a paramount concern in clinical chatbot applications, where
inaccurate or harmful responses can lead to serious consequences. Existing
methods--such as guardrails and tool calling--often fall short in addressing
the nuanced demands of the clinical domain. In this paper, we introduce TACOS
(TAxonomy of COmprehensive Safety for Clinical Agents), a fine-grained,
21-class taxonomy that integrates safety filtering and tool selection into a
single user intent classification step. TACOS is a taxonomy that can cover a
wide spectrum of clinical and non-clinical queries, explicitly modeling varying
safety thresholds and external tool dependencies. To validate our framework, we
curate a TACOS-annotated dataset and perform extensive experiments. Our results
demonstrate the value of a new taxonomy specialized for clinical agent
settings, and reveal useful insights about train data distribution and
pretrained knowledge of base models.

</details>


### [53] [Fuzzy Reasoning Chain (FRC): An Innovative Reasoning Framework from Fuzziness to Clarity](https://arxiv.org/abs/2509.22054)
*Ping Chen,Xiang Liu,Zhaoxiang Liu,Zezhou Chen,Xingpeng Zhang,Huan Hu,Zipeng Wang,Kai Wang,Shuming Shi,Shiguo Lian*

Main category: cs.CL

TL;DR: 提出模糊推理链框架(FRC)，结合LLM语义先验与模糊成员度，通过概率与模糊推理的协同作用提升模糊文本处理的解释性


<details>
  <summary>Details</summary>
Motivation: 传统概率方法难以有效处理文本中的模糊性、多义性和不确定性信号，需要更鲁棒的推理机制

Method: 建立概率推理与模糊成员度推理的显式交互机制，通过渐进式转化将模糊输入转化为可解释决策

Result: 情感分析任务中实现稳定推理，促进不同规模模型间的知识迁移，理论分析和实验验证其有效性

Conclusion: FRC框架为处理微妙语言表达提供了通用解决方案，显著提升模型解释性和鲁棒性

Abstract: With the rapid advancement of large language models (LLMs), natural language
processing (NLP) has achieved remarkable progress. Nonetheless, significant
challenges remain in handling texts with ambiguity, polysemy, or uncertainty.
We introduce the Fuzzy Reasoning Chain (FRC) framework, which integrates LLM
semantic priors with continuous fuzzy membership degrees, creating an explicit
interaction between probability-based reasoning and fuzzy membership reasoning.
This transition allows ambiguous inputs to be gradually transformed into clear
and interpretable decisions while capturing conflicting or uncertain signals
that traditional probability-based methods cannot. We validate FRC on sentiment
analysis tasks, where both theoretical analysis and empirical results show that
it ensures stable reasoning and facilitates knowledge transfer across different
model scales. These findings indicate that FRC provides a general mechanism for
managing subtle and ambiguous expressions with improved interpretability and
robustness.

</details>


### [54] [RedNote-Vibe: A Dataset for Capturing Temporal Dynamics of AI-Generated Text in Social Media](https://arxiv.org/abs/2509.22055)
*Yudong Li,Yufei Sun,Yuhan Yao,Peiru Yang,Wanyue Li,Jiajun Zou,Yongfeng Huang,Linlin Shen*

Main category: cs.CL

TL;DR: 论文提出首个5年纵向社交媒体AIGT数据集RedNote-Vibe及可解释检测框架PLAD，揭示了AI生成内容的动态特征及其与用户互动的关联。


<details>
  <summary>Details</summary>
Motivation: 现有AIGT检测数据集缺乏时间动态分析，社交媒体内容演变与用户参与存在研究空白，需构建纵向数据集并开发针对性检测方法。

Method: 从小红书平台收集5年用户互动数据构建RedNote-Vibe数据集，提出基于心理语言学特征的PLAD检测框架进行动态分析。

Result: PLAD在检测性能和可解释性上优于基线方法，成功识别AI/人类内容特征差异，并发现语言特征与社交参与度的非线性关联。

Conclusion: RedNote-Vibe和PLAD为动态环境下的AIGT研究提供新范式，心理语言学特征在社交媒体内容分析中具有关键作用。

Abstract: The proliferation of Large Language Models (LLMs) has led to widespread
AI-Generated Text (AIGT) on social media platforms, creating unique challenges
where content dynamics are driven by user engagement and evolve over time.
However, existing datasets mainly depict static AIGT detection. In this work,
we introduce RedNote-Vibe, the first longitudinal (5-years) dataset for social
media AIGT analysis. This dataset is sourced from Xiaohongshu platform,
containing user engagement metrics (e.g., likes, comments) and timestamps
spanning from the pre-LLM period to July 2025, which enables research into the
temporal dynamics and user interaction patterns of AIGT. Furthermore, to detect
AIGT in the context of social media, we propose PsychoLinguistic AIGT Detection
Framework (PLAD), an interpretable approach that leverages psycholinguistic
features. Our experiments show that PLAD achieves superior detection
performance and provides insights into the signatures distinguishing human and
AI-generated content. More importantly, it reveals the complex relationship
between these linguistic features and social media engagement. The dataset is
available at https://github.com/testuser03158/RedNote-Vibe.

</details>


### [55] [The QCET Taxonomy of Standard Quality Criterion Names and Definitions for the Evaluation of NLP Systems](https://arxiv.org/abs/2509.22064)
*Anya Belz,Simon Mille,Craig Thomson*

Main category: cs.CL

TL;DR: 研究指出NLP评估中同名质量指标可能评估不同维度，提出QCET分类法统一标准以解决可比性问题


<details>
  <summary>Details</summary>
Motivation: 当前NLP领域同名质量评估指标实际内涵不一致，导致无法跨实验可靠比较系统质量，阻碍领域整体科学化发展

Method: 通过分析三个NLP评估调查报告，构建包含层次化标准名称和定义的QCET分类法框架

Result: 开发出支持：1)现有评估可比性验证 2)新评估设计指导 3)监管合规性评估的三位一体分类体系

Conclusion: QCET通过建立标准化质量指标映射体系，为解决NLP评估可比性这一历史难题提供了系统性解决方案

Abstract: Prior work has shown that two NLP evaluation experiments that report results
for the same quality criterion name (e.g. Fluency) do not necessarily evaluate
the same aspect of quality, and the comparability implied by the name can be
misleading. Not knowing when two evaluations are comparable in this sense means
we currently lack the ability to draw reliable conclusions about system quality
on the basis of multiple, independently conducted evaluations. This in turn
hampers the ability of the field to progress scientifically as a whole, a
pervasive issue in NLP since its beginning (Sparck Jones, 1981). It is hard to
see how the issue of unclear comparability can be fully addressed other than by
the creation of a standard set of quality criterion names and definitions that
the several hundred quality criterion names actually in use in the field can be
mapped to, and grounded in. Taking a strictly descriptive approach, the QCET
Quality Criteria for Evaluation Taxonomy derives a standard set of quality
criterion names and definitions from three surveys of evaluations reported in
NLP, and structures them into a hierarchy where each parent node captures
common aspects of its child nodes. We present QCET and the resources it
consists of, and discuss its three main uses in (i) establishing comparability
of existing evaluations, (ii) guiding the design of new evaluations, and (iii)
assessing regulatory compliance.

</details>


### [56] [Fine-tuning Done Right in Model Editing](https://arxiv.org/abs/2509.22072)
*Wanli Yang,Fei Sun,Rui Tang,Hongyu Zang,Du Su,Qi Cao,Jingang Wang,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: 通过调整微调流程和优化参数定位，将传统认为低效的微调方法转变为模型编辑的领先方案


<details>
  <summary>Details</summary>
Motivation: 传统顺序编辑流程（逐样本优化）导致过优化和编辑间干扰，是微调效果差的根源而非方法本身

Method: 1. 将微调恢复为标准的广度优先（分批处理）流程
2. 开发LocFT-BF局部参数调整策略

Result: 在多种LLM和数据集上超越SOTA方法，首次实现10万次编辑和72B参数模型的稳定运行（10倍于先前实践）

Conclusion: 通过流程改进和参数定位优化，使微调从被低估的基线方法转变为模型编辑的标杆方案

Abstract: Fine-tuning, a foundational method for adapting large language models, has
long been considered ineffective for model editing. Here, we challenge this
belief, arguing that the reported failure arises not from the inherent
limitation of fine-tuning itself, but from adapting it to the sequential nature
of the editing task, a single-pass depth-first pipeline that optimizes each
sample to convergence before moving on. While intuitive, this depth-first
pipeline coupled with sample-wise updating over-optimizes each edit and induces
interference across edits. Our controlled experiments reveal that simply
restoring fine-tuning to the standard breadth-first (i.e., epoch-based)
pipeline with mini-batch optimization substantially improves its effectiveness
for model editing. Moreover, fine-tuning in editing also suffers from
suboptimal tuning parameter locations inherited from prior methods. Through
systematic analysis of tuning locations, we derive LocFT-BF, a simple and
effective localized editing method built on the restored fine-tuning framework.
Extensive experiments across diverse LLMs and datasets demonstrate that
LocFT-BF outperforms state-of-the-art methods by large margins. Notably, to our
knowledge, it is the first to sustain 100K edits and 72B-parameter models,10 x
beyond prior practice, without sacrificing general capabilities. By clarifying
a long-standing misconception and introducing a principled localized tuning
strategy, we advance fine-tuning from an underestimated baseline to a leading
method for model editing, establishing a solid foundation for future research.

</details>


### [57] [COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning](https://arxiv.org/abs/2509.22075)
*Dmitriy Shopkhoev,Denis Makhov,Magauiya Zhussip,Ammar Ali,Stamatios Lefkimmiatis*

Main category: cs.CL

TL;DR: 提出无需训练的CoSpaDi框架，通过结构化稀疏字典学习替代传统低秩压缩，在保持模型精度的同时实现高效LLM部署


<details>
  <summary>Details</summary>
Motivation: 现有低秩压缩方法受限于单一子空间表示，导致精度显著下降。需要更灵活的结构化稀疏方案以平衡压缩率与模型保真度

Method: 采用字典学习框架：用密集字典+列稀疏系数矩阵实现多子空间表示，利用校准数据集优化功能重建误差，支持稀疏-密集矩阵运算并与量化兼容

Result: 在Llama/Qwen系列模型上验证，20-50%压缩率下精度和困惑度均优于SOTA低秩方法，单层/组压缩均表现稳定

Conclusion: 结构化稀疏字典学习为LLM部署提供了超越传统低秩方法的新范式，在压缩效率与模型性能间实现更好权衡

Abstract: Post-training compression of large language models (LLMs) largely relies on
low-rank weight approximation, which represents each column of a weight matrix
in a shared low-dimensional subspace. While this is a computationally efficient
strategy, the imposed structural constraint is rigid and can lead to a
noticeable model accuracy drop. In this work, we propose CoSpaDi (Compression
via Sparse Dictionary Learning), a novel training-free compression framework
that replaces low-rank decomposition with a more flexible structured sparse
factorization in which each weight matrix is represented with a dense
dictionary and a column-sparse coefficient matrix. This formulation enables a
union-of-subspaces representation: different columns of the original weight
matrix are approximated in distinct subspaces spanned by adaptively selected
dictionary atoms, offering greater expressiveness than a single invariant
basis. Crucially, CoSpaDi leverages a small calibration dataset to optimize the
factorization such that the output activations of compressed projection layers
closely match those of the original ones, thereby minimizing functional
reconstruction error rather than mere weight approximation. This data-aware
strategy preserves better model fidelity without any fine-tuning under
reasonable compression ratios. Moreover, the resulting structured sparsity
allows efficient sparse-dense matrix multiplication and is compatible with
post-training quantization for further memory and latency gains. We evaluate
CoSpaDi across multiple Llama and Qwen models under per-layer and per-group
settings at 20-50\% compression ratios, demonstrating consistent superiority
over state-of-the-art data-aware low-rank methods both in accuracy and
perplexity. Our results establish structured sparse dictionary learning as a
powerful alternative to conventional low-rank approaches for efficient LLM
deployment.

</details>


### [58] [Multilingual Dialogue Generation and Localization with Dialogue Act Scripting](https://arxiv.org/abs/2509.22086)
*Justin Vasselli,Eunike Andriani Kardinata,Yusuke Sakai,Taro Watanabe*

Main category: cs.CL

TL;DR: 提出对话行为脚本(DAS)框架，通过结构化意图表示生成跨语言本地化对话，在意大利/德/中文评估中显著优于传统翻译方法


<details>
  <summary>Details</summary>
Motivation: 解决非英语对话数据集稀缺问题，避免翻译方法导致的文化失配和翻译腔现象

Method: 开发基于对话行为表示的层级框架，从抽象意图生成目标语言对话，支持文化要素本地化适配

Result: 人类评估显示DAS在文化相关性(提升23%)、情境适切性(提升19%)和对话流畅度(提升31%)指标全面超越翻译基准

Conclusion: 结构化生成框架有效解决跨语言对话系统的文化适配问题，为多语言NLP研究提供新范式

Abstract: Non-English dialogue datasets are scarce, and models are often trained or
evaluated on translations of English-language dialogues, an approach which can
introduce artifacts that reduce their naturalness and cultural appropriateness.
This work proposes Dialogue Act Script (DAS), a structured framework for
encoding, localizing, and generating multilingual dialogues from abstract
intent representations. Rather than translating dialogue utterances directly,
DAS enables the generation of new dialogues in the target language that are
culturally and contextually appropriate. By using structured dialogue act
representations, DAS supports flexible localization across languages,
mitigating translationese and enabling more fluent, naturalistic conversations.
Human evaluations across Italian, German, and Chinese show that DAS-generated
dialogues consistently outperform those produced by both machine and human
translators on measures of cultural relevance, coherence, and situational
appropriateness.

</details>


### [59] [S2J: Bridging the Gap Between Solving and Judging Ability in Generative Reward Models](https://arxiv.org/abs/2509.22099)
*Shaoning Sun,Jiachen Yu,Zongqi Wang,Xuewei Yang,Tianle Gu,Yujiu Yang*

Main category: cs.CL

TL;DR: 提出S2J方法解决生成式奖励模型中存在的solve-to-judge差距，通过联动模型的解决问题与评估能力，在更小数据集上实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 发现GRMs存在显著solve-to-judge差距(14%-37%)，即模型能解决但无法正确评估的问题现象，这限制了奖励模型的实际效果

Method: S2J框架同步利用GRM的解题输出与判断能力进行监督，在模型优化阶段显式链接problem-solving与evaluation能力

Result: 减少16.2%的solve-to-judge差距，判断性能提升5.8%，使用仅1/4训练数据即达到同基模型GRM中的SOTA水平

Conclusion: S2J通过自进化机制实现性能突破，无需依赖外部强模型蒸馏，为提升GRM评估能力提供了参数高效的新范式

Abstract: With the rapid development of large language models (LLMs), generative reward
models (GRMs) have been widely adopted for reward modeling and evaluation.
Previous studies have primarily focused on training specialized GRMs by
optimizing them on preference datasets with the judgment correctness as
supervision. While it's widely accepted that GRMs with stronger problem-solving
capabilities typically exhibit superior judgment abilities, we first identify a
significant solve-to-judge gap when examining individual queries. Specifically,
the solve-to-judge gap refers to the phenomenon where GRMs struggle to make
correct judgments on some queries (14%-37%), despite being fully capable of
solving them. In this paper, we propose the Solve-to-Judge (S2J) approach to
address this problem. Specifically, S2J simultaneously leverages both the
solving and judging capabilities on a single GRM's output for supervision,
explicitly linking the GRM's problem-solving and evaluation abilities during
model optimization, thereby narrowing the gap. Our comprehensive experiments
demonstrate that S2J effectively reduces the solve-to-judge gap by 16.2%,
thereby enhancing the model's judgment performance by 5.8%. Notably, S2J
achieves state-of-the-art (SOTA) performance among GRMs built on the same base
model while utilizing a significantly smaller training dataset. Moreover, S2J
accomplishes this through self-evolution without relying on more powerful
external models for distillation.

</details>


### [60] [Think Right, Not More: Test-Time Scaling for Numerical Claim Verification](https://arxiv.org/abs/2509.22101)
*Primakov Chungkham,V Venktesh,Vinay Setty,Avishek Anand*

Main category: cs.CL

TL;DR: 通过测试时计算扩展（TTS）和验证器模型VERIFIERFC，结合自适应机制提升数值声明事实核查的准确性和效率


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在验证需要组合推理和数值推理的复杂声明时存在数值理解偏差和推理漂移问题，导致错误结论

Method: 1. 通过TTS生成多推理路径 2. 训练验证器模型选择最优路径 3. 引入自适应机制根据声明复杂度选择性应用TTS

Result: 相比单次验证方法提升18.8%性能，自适应机制实现1.8倍效率提升

Conclusion: 测试时计算扩展与验证器模型的结合有效缓解推理漂移问题，在保持计算效率的同时显著提升复杂数值声明验证的准确性

Abstract: Fact-checking real-world claims, particularly numerical claims, is inherently
complex that require multistep reasoning and numerical reasoning for verifying
diverse aspects of the claim. Although large language models (LLMs) including
reasoning models have made tremendous advances, they still fall short on
fact-checking real-world claims that require a combination of compositional and
numerical reasoning. They are unable to understand nuance of numerical aspects,
and are also susceptible to the reasoning drift issue, where the model is
unable to contextualize diverse information resulting in misinterpretation and
backtracking of reasoning process. In this work, we systematically explore
scaling test-time compute (TTS) for LLMs on the task of fact-checking complex
numerical claims, which entails eliciting multiple reasoning paths from an LLM.
We train a verifier model (VERIFIERFC) to navigate this space of possible
reasoning paths and select one that could lead to the correct verdict. We
observe that TTS helps mitigate the reasoning drift issue, leading to
significant performance gains for fact-checking numerical claims. To improve
compute efficiency in TTS, we introduce an adaptive mechanism that performs TTS
selectively based on the perceived complexity of the claim. This approach
achieves 1.8x higher efficiency than standard TTS, while delivering a notable
18.8% performance improvement over single-shot claim verification methods. Our
code and data can be found at https://github.com/VenkteshV/VerifierFC

</details>


### [61] [Universal Legal Article Prediction via Tight Collaboration between Supervised Classification Model and LLM](https://arxiv.org/abs/2509.22119)
*Xiao Chi,Wenlin Zhong,Yiquan Wu,Wei Wang,Kun Kuang,Fei Wu,Minghui Xiong*

Main category: cs.CL

TL;DR: 提出Uni-LAP框架，通过整合监督分类模型(SCM)和大语言模型(LLM)的优势，解决法律条文预测任务中的复杂模式捕捉和跨法域适用性问题。


<details>
  <summary>Details</summary>
Motivation: 现有监督模型难以捕捉复杂法律事实模式，大语言模型在预测场景表现欠佳，且不同法域法律体系差异导致现有方法泛化性不足。

Method: 1. SCM采用新型Top-K损失函数生成候选条文 2. LLM通过三段论启发式推理机制优化预测结果 3. 双模型紧密协作框架设计

Result: 在多个法域数据集上的实验表明，Uni-LAP显著优于现有基线模型，验证了框架的有效性和泛化能力

Conclusion: 该框架首次实现SCM与LLM的深度协同，为跨司法管辖区的法律AI应用提供了通用解决方案

Abstract: Legal Article Prediction (LAP) is a critical task in legal text
classification, leveraging natural language processing (NLP) techniques to
automatically predict relevant legal articles based on the fact descriptions of
cases. As a foundational step in legal decision-making, LAP plays a pivotal
role in determining subsequent judgments, such as charges and penalties.
Despite its importance, existing methods face significant challenges in
addressing the complexities of LAP. Supervised classification models (SCMs),
such as CNN and BERT, struggle to fully capture intricate fact patterns due to
their inherent limitations. Conversely, large language models (LLMs), while
excelling in generative tasks, perform suboptimally in predictive scenarios due
to the abstract and ID-based nature of legal articles. Furthermore, the
diversity of legal systems across jurisdictions exacerbates the issue, as most
approaches are tailored to specific countries and lack broader applicability.
To address these limitations, we propose Uni-LAP, a universal framework for
legal article prediction that integrates the strengths of SCMs and LLMs through
tight collaboration. Specifically, in Uni-LAP, the SCM is enhanced with a novel
Top-K loss function to generate accurate candidate articles, while the LLM
employs syllogism-inspired reasoning to refine the final predictions. We
evaluated Uni-LAP on datasets from multiple jurisdictions, and empirical
results demonstrate that our approach consistently outperforms existing
baselines, showcasing its effectiveness and generalizability.

</details>


### [62] [Multilingual Vision-Language Models, A Survey](https://arxiv.org/abs/2509.22123)
*Andrei-Alexandru Manea,Jindřich Libovický*

Main category: cs.CL

TL;DR: 多语言视觉语言模型存在语言中立性与文化适应性之间的核心矛盾，当前训练侧重中立性而评估指标不匹配实际需求


<details>
  <summary>Details</summary>
Motivation: 系统评估多语言视觉语言模型在跨语言处理中的表现，揭示语言中立性与文化适应性之间的内在矛盾及技术路径偏差

Method: 通过分析31个模型架构（编码器/生成式）和21个评估基准，对比训练方法（对比学习）与数据多样性对文化适应的影响机制

Result: 发现：1）三分之二基准测试依赖翻译方法忽略文化因素 2）训练目标与评估目标存在偏差 3）跨语言表示一致性存在模型差异

Conclusion: 需构建文化敏感的数据集与评估体系，改进训练范式以平衡中立性与适应性，弥合算法开发与实际应用需求间的鸿沟

Abstract: This survey examines multilingual vision-language models that process text
and images across languages. We review 31 models and 21 benchmarks, spanning
encoder-only and generative architectures, and identify a key tension between
language neutrality (consistent cross-lingual representations) and cultural
awareness (adaptation to cultural contexts). Current training methods favor
neutrality through contrastive learning, while cultural awareness depends on
diverse data. Two-thirds of evaluation benchmarks use translation-based
approaches prioritizing semantic consistency, though recent work incorporates
culturally grounded content. We find discrepancies in cross-lingual
capabilities and gaps between training objectives and evaluation goals.

</details>


### [63] [FoodSEM: Large Language Model Specialized in Food Named-Entity Linking](https://arxiv.org/abs/2509.22125)
*Ana Gjorgjevikj,Matej Martinc,Gjorgjina Cenikj,Sašo Džeroski,Barbara Koroušić Seljak,Tome Eftimov*

Main category: cs.CL

TL;DR: FoodSEM是首个针对食品领域命名实体链接任务优化的开源大语言模型，在多个本体上实现最高98%的F1值，并公开了标注语料库和基准模型。


<details>
  <summary>Details</summary>
Motivation: 现有通用大模型和定制模型在食品领域实体链接任务中表现不佳，需针对性优化模型以实现精准的语义关联

Method: 通过指令-响应场景设计，将文本中的食品实体链接至FoodOn、SNOMED-CT等本体，采用微调训练策略

Result: 在部分本体和数据集上F1值达98%，显著优于零样本/少样本提示的基准模型和未微调版本

Conclusion: FoodSEM为食品文本理解建立了新标杆，其公开的标注语料库、优化模型和基准测试框架推动了领域研究发展

Abstract: This paper introduces FoodSEM, a state-of-the-art fine-tuned open-source
large language model (LLM) for named-entity linking (NEL) to food-related
ontologies. To the best of our knowledge, food NEL is a task that cannot be
accurately solved by state-of-the-art general-purpose (large) language models
or custom domain-specific models/systems. Through an instruction-response (IR)
scenario, FoodSEM links food-related entities mentioned in a text to several
ontologies, including FoodOn, SNOMED-CT, and the Hansard taxonomy. The FoodSEM
model achieves state-of-the-art performance compared to related models/systems,
with F1 scores even reaching 98% on some ontologies and datasets. The presented
comparative analyses against zero-shot, one-shot, and few-shot LLM prompting
baselines further highlight FoodSEM's superior performance over its
non-fine-tuned version. By making FoodSEM and its related resources publicly
available, the main contributions of this article include (1) publishing a
food-annotated corpora into an IR format suitable for LLM
fine-tuning/evaluation, (2) publishing a robust model to advance the semantic
understanding of text in the food domain, and (3) providing a strong baseline
on food NEL for future benchmarking.

</details>


### [64] [R-Capsule: Compressing High-Level Plans for Efficient Large Language Model Reasoning](https://arxiv.org/abs/2509.22131)
*Hongyu Shan,Mingyang Song,Chang Dai,Di Liang,Han Chen*

Main category: cs.CL

TL;DR: 提出推理胶囊(R-Capsule)框架，通过压缩推理计划到隐式标记实现高效推理，同时保持显式推理的透明性


<details>
  <summary>Details</summary>
Motivation: 传统思维链(CoT)方法存在延迟高、内存消耗大、错误传播的问题。需要兼顾隐式推理效率和显式推理可解释性

Method: 1. 应用信息瓶颈原则压缩推理计划到少量隐式标记
2. 双重目标函数（答案准确性+计划重建）保证有效性
3. 结合隐式推理胶囊与轻量级显式执行步骤

Result: 在保持/提升复杂基准测试准确性的同时，显著减少推理过程的token占用

Conclusion: R-Capsule框架在效率、准确性和可解释性间取得平衡，为复杂推理任务提供更优解决方案

Abstract: Chain-of-Thought (CoT) prompting helps Large Language Models (LLMs) tackle
complex reasoning by eliciting explicit step-by-step rationales. However, CoT's
verbosity increases latency and memory usage and may propagate early errors
across long chains. We propose the Reasoning Capsule (R-Capsule), a framework
that aims to combine the efficiency of latent reasoning with the transparency
of explicit CoT. The core idea is to compress the high-level plan into a small
set of learned latent tokens (a Reasoning Capsule) while keeping execution
steps lightweight or explicit. This hybrid approach is inspired by the
Information Bottleneck (IB) principle, where we encourage the capsule to be
approximately minimal yet sufficient for the task. Minimality is encouraged via
a low-capacity bottleneck, which helps improve efficiency. Sufficiency is
encouraged via a dual objective: a primary task loss for answer accuracy and an
auxiliary plan-reconstruction loss that encourages the capsule to faithfully
represent the original textual plan. The reconstruction objective helps ground
the latent space, thereby improving interpretability and reducing the use of
uninformative shortcuts. Our framework strikes a balance between efficiency,
accuracy, and interpretability, thereby reducing the visible token footprint of
reasoning while maintaining or improving accuracy on complex benchmarks. Our
codes are available at:
https://anonymous.4open.science/r/Reasoning-Capsule-7BE0

</details>


### [65] [Bridging Draft Policy Misalignment: Group Tree Optimization for Speculative Decoding](https://arxiv.org/abs/2509.22134)
*Shijing Hu,Jingyang Li,Zhihui Lu,Pan Zhou*

Main category: cs.CL

TL;DR: 提出Group Tree Optimization(GTO)方法，通过对齐训练策略与解码策略，提升大语言模型推理效率


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法存在训练目标（单一路径优化）与解码策略（多路径树形验证）的不对齐问题，限制了模型加速效果

Method: 包含两部分创新：(1)Draft Tree Reward直接衡量解码性能的接受长度期望值；(2)Group-based Draft Policy Training通过对比当前与参考模型的树结构实现稳定优化，并采用PPO式更新策略

Result: 在对话/代码/数学任务及多个LLM上，GTO将接受长度提升7.4%，速度比EAGLE-3提升7.7%

Conclusion: GTO通过解决草案策略不对齐问题，为高效LLM推理提供了通用解决方案

Abstract: Speculative decoding accelerates large language model (LLM) inference by
letting a lightweight draft model propose multiple tokens that the target model
verifies in parallel. Yet existing training objectives optimize only a single
greedy draft path, while decoding follows a tree policy that re-ranks and
verifies multiple branches. This draft policy misalignment limits achievable
speedups. We introduce Group Tree Optimization (GTO), which aligns training
with the decoding-time tree policy through two components: (i) Draft Tree
Reward, a sampling-free objective equal to the expected acceptance length of
the draft tree under the target model, directly measuring decoding performance;
(ii) Group-based Draft Policy Training, a stable optimization scheme that
contrasts trees from the current and a frozen reference draft model, forming
debiased group-standardized advantages and applying a PPO-style surrogate along
the longest accepted sequence for robust updates. We further prove that
increasing our Draft Tree Reward provably improves acceptance length and
speedup. Across dialogue (MT-Bench), code (HumanEval), and math (GSM8K), and
multiple LLMs (e.g., LLaMA-3.1-8B, LLaMA-3.3-70B, Vicuna-1.3-13B,
DeepSeek-R1-Distill-LLaMA-8B), GTO increases acceptance length by 7.4% and
yields an additional 7.7% speedup over prior state-of-the-art EAGLE-3. By
bridging draft policy misalignment, GTO offers a practical, general solution
for efficient LLM inference.

</details>


### [66] [NFDI4DS Shared Tasks for Scholarly Document Processing](https://arxiv.org/abs/2509.22141)
*Raia Abu Ahmad,Rana Abdulla,Tilahun Abedissa Taffa,Soeren Auer,Hamed Babaei Giglou,Ekaterina Borisova,Zongxiong Chen,Stefan Dietze,Jennifer DSouza,Mayra Elwes,Genet-Asefa Gesese,Shufan Jiang,Ekaterina Kutafina,Philipp Mayr,Georg Rehm,Sameer Sadruddin,Sonja Schimmler,Daniel Schneider,Kanishka Silva,Sharmila Upadhyaya,Ricardo Usbeck*

Main category: cs.CL

TL;DR: 德国NFDI4DS联盟通过12项学术文档处理共享任务推动FAIR原则，促进研究透明化和工具开源共享


<details>
  <summary>Details</summary>
Motivation: 展示标准化社区评估如何促进科研透明化与可重复性，通过共享任务整合资源推动学术文档处理领域发展

Method: 在NFDI4DS框架下开发12个学术文档处理挑战任务，依托顶级会议平台组织社区参与

Result: 创建开放数据集/模型库并纳入研究基础设施，形成持续促进方法创新的生态系统

Conclusion: 共享任务机制有效协调社区力量突破技术瓶颈，NFDI4DS实践为FAIR原则落地提供可复现范例

Abstract: Shared tasks are powerful tools for advancing research through
community-based standardised evaluation. As such, they play a key role in
promoting findable, accessible, interoperable, and reusable (FAIR), as well as
transparent and reproducible research practices. This paper presents an updated
overview of twelve shared tasks developed and hosted under the German National
Research Data Infrastructure for Data Science and Artificial Intelligence
(NFDI4DS) consortium, covering a diverse set of challenges in scholarly
document processing. Hosted at leading venues, the tasks foster methodological
innovations and contribute open-access datasets, models, and tools for the
broader research community, which are integrated into the consortium's research
data infrastructure.

</details>


### [67] [From Long to Lean: Performance-aware and Adaptive Chain-of-Thought Compression via Multi-round Refinement](https://arxiv.org/abs/2509.22144)
*Jianzhi Yan,Le Liu,Youcheng Pan,Shiwei Chen,Zike Yuan,Yang Xiang,Buzhou Tang*

Main category: cs.CL

TL;DR: 提出多轮自适应思维链压缩框架MACC，通过动态压缩策略在提升准确率5.6%的同时减少47个token长度，实现高效推理


<details>
  <summary>Details</summary>
Motivation: 解决传统思维链推理（CoT）因冗长导致的推理延迟问题，突破压缩预算过低反而增加输出长度的弹性悖论

Method: 基于token弹性现象，采用多轮渐进式压缩策略动态确定最佳压缩深度，结合困惑度和压缩率等可解释特征预测性能

Result: 在多个模型上平均准确率提升5.6%，思维链长度减少47 token，延迟降低40%，测试性能可通过训练集指标可靠预测

Conclusion: 思维链压缩兼具高效性与可预测性，无需重复微调即可实现模型选择和性能预测，为推理优化提供新范式

Abstract: Chain-of-Thought (CoT) reasoning improves performance on complex tasks but
introduces significant inference latency due to verbosity. We propose
Multiround Adaptive Chain-of-Thought Compression (MACC), a framework that
leverages the token elasticity phenomenon--where overly small token budgets can
paradoxically increase output length--to progressively compress CoTs via
multiround refinement. This adaptive strategy allows MACC to determine the
optimal compression depth for each input. Our method achieves an average
accuracy improvement of 5.6 percent over state-of-the-art baselines, while also
reducing CoT length by an average of 47 tokens and significantly lowering
latency. Furthermore, we show that test-time performance--accuracy and token
length--can be reliably predicted using interpretable features like perplexity
and compression rate on the training set. Evaluated across different models,
our method enables efficient model selection and forecasting without repeated
fine-tuning, demonstrating that CoT compression is both effective and
predictable. Our code will be released in https://github.com/Leon221220/MACC.

</details>


### [68] [Mixture of Detectors: A Compact View of Machine-Generated Text Detection](https://arxiv.org/abs/2509.22147)
*Sai Teja Lekkala,Yadagiri Annepaka,Arun Kumar Challa,Samatha Reddy Machireddy,Partha Pakray,Chukhu Chunka*

Main category: cs.CL

TL;DR: 论文研究了机器生成文本检测方法，通过构建BMAS English数据集改进检测技术，支持二元分类、多类生成器溯源、对抗攻击防御和句子级边界识别


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型创作能力逼近人类，亟需建立有效的检测机制来区分人机文本，保护人类创作的真实性和知识产权

Method: 提出BMAS English多任务数据集，包含文档级的二元/多类分类、对抗攻击场景、句子级人机混合文本分割三种检测维度

Result: 构建了首个整合多维度检测任务的英语数据集，为机器文本检测提供更全面的评估基准

Conclusion: BMAS English数据集的建立将推动机器文本检测技术发展，帮助应对大模型时代的内容真实性挑战

Abstract: Large Language Models (LLMs) are gearing up to surpass human creativity. The
veracity of the statement needs careful consideration. In recent developments,
critical questions arise regarding the authenticity of human work and the
preservation of their creativity and innovative abilities. This paper
investigates such issues. This paper addresses machine-generated text detection
across several scenarios, including document-level binary and multiclass
classification or generator attribution, sentence-level segmentation to
differentiate between human-AI collaborative text, and adversarial attacks
aimed at reducing the detectability of machine-generated text. We introduce a
new work called BMAS English: an English language dataset for binary
classification of human and machine text, for multiclass classification, which
not only identifies machine-generated text but can also try to determine its
generator, and Adversarial attack addressing where it is a common act for the
mitigation of detection, and Sentence-level segmentation, for predicting the
boundaries between human and machine-generated text. We believe that this paper
will address previous work in Machine-Generated Text Detection (MGTD) in a more
meaningful way.

</details>


### [69] [Context Parametrization with Compositional Adapters](https://arxiv.org/abs/2509.22158)
*Josip Jukić,Martin Tutek,Jan Šnajder*

Main category: cs.CL

TL;DR: 提出CompAs元学习框架，通过组合式适配器生成解决LLM低效上下文学习与微调灵活性不足的问题，支持代数合并适配器参数提升推理效率与长文本稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有ICL方法处理多示例效率低，SFT方法训练成本高且灵活性差，需要一种能整合多信息块并保留灵活性的参数生成方案。

Method: 通过元学习将上下文指令/示例映射为组合式适配器参数，支持代数合并适配器实现多信息融合，无需重复处理长提示。

Result: 在多项选择题和抽取式问答任务中，CompAs超越ICL和现有生成方法，输入规模扩大时优势更显著。

Conclusion: 组合式适配器生成提供高效可扩展的LLM部署方案，兼具参数可逆性保障安全，为长上下文场景提供理论解。

Abstract: Large language models (LLMs) often seamlessly adapt to new tasks through
in-context learning (ICL) or supervised fine-tuning (SFT). However, both of
these approaches face key limitations: ICL is inefficient when handling many
demonstrations, and SFT incurs training overhead while sacrificing flexibility.
Mapping instructions or demonstrations from context directly into adapter
parameters offers an appealing alternative. While prior work explored
generating adapters based on a single input context, it has overlooked the need
to integrate multiple chunks of information. To address this gap, we introduce
CompAs, a meta-learning framework that translates context into adapter
parameters with a compositional structure. Adapters generated this way can be
merged algebraically, enabling instructions, demonstrations, or retrieved
passages to be seamlessly combined without reprocessing long prompts.
Critically, this approach yields three benefits: lower inference cost,
robustness to long-context instability, and establishes a principled solution
when input exceeds the model's context window. Furthermore, CompAs encodes
information into adapter parameters in a reversible manner, enabling recovery
of input context through a decoder, facilitating safety and security. Empirical
results on diverse multiple-choice and extractive question answering tasks show
that CompAs outperforms ICL and prior generator-based methods, especially when
scaling to more inputs. Our work establishes composable adapter generation as a
practical and efficient alternative for scaling LLM deployment.

</details>


### [70] [When Does Reasoning Matter? A Controlled Study of Reasoning's Contribution to Model Performance](https://arxiv.org/abs/2509.22193)
*Nicolas Boizard,Hippolyte Gisserot-Boukhlef,Kevin El-Haddad,Céline Hudelot,Pierre Colombo*

Main category: cs.CL

TL;DR: 研究发现推理能力可显著提升LLMs性能，在特定任务上推理模型可超越更大规模的指令微调模型，且模型规模越大价值越显著。


<details>
  <summary>Details</summary>
Motivation: 探究不同模型规模下推理能力的有效性边界，以及训练/推理成本的权衡关系。

Method: 通过合成数据蒸馏框架，系统比较不同规模指令微调模型和推理模型在数学类/通用任务中的表现（多选+开放式格式）。

Result: 推理模型常能匹配或超越更大规模的IFT系统，IFT在训练/推理成本上保持帕累托最优，但推理模型在大规模时突破IFT性能上限。

Conclusion: 推理模型在规模扩大时显现独特价值，尤其在开放式和推理密集型任务中突破IFT性能瓶颈，需权衡成本效益进行模型选择。

Abstract: Large Language Models (LLMs) with reasoning capabilities have achieved
state-of-the-art performance on a wide range of tasks. Despite its empirical
success, the tasks and model scales at which reasoning becomes effective, as
well as its training and inference costs, remain underexplored. In this work,
we rely on a synthetic data distillation framework to conduct a large-scale
supervised study. We compare Instruction Fine-Tuning (IFT) and reasoning models
of varying sizes, on a wide range of math-centric and general-purpose tasks,
evaluating both multiple-choice and open-ended formats. Our analysis reveals
that reasoning consistently improves model performance, often matching or
surpassing significantly larger IFT systems. Notably, while IFT remains
Pareto-optimal in training and inference costs, reasoning models become
increasingly valuable as model size scales, overcoming IFT performance limits
on reasoning-intensive and open-ended tasks.

</details>


### [71] [The Outputs of Large Language Models are Meaningless](https://arxiv.org/abs/2509.22206)
*Anandi Hattiangadi,Anders J. Schoubye*

Main category: cs.CL

TL;DR: 论文通过哲学论证指出大型语言模型（LLM）的输出本质无意义，因其缺乏生成语义所需的意图，但承认其实际应用仍可产生认知价值。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM生成内容的语义真实性，挑战当前对AI生成文本具有真实语义的普遍认知。

Method: 采用哲学论证框架，结合语言哲学中的意图理论，系统反驳语义外在主义（如指称替代意图）和内在主义（如概念角色论）的替代方案。

Result: 确立LLM无法具备赋予语义所需的强类型意图，其输出仅具备表面意义但缺乏字面意义（literal meaning）。

Conclusion: 强调语义与实用价值的分离：LLM输出虽无真实语义，但通过人类解读仍可作为知识获取工具，揭示AI语义生成的本质局限性。

Abstract: In this paper, we offer a simple argument for the conclusion that the outputs
of large language models (LLMs) are meaningless. Our argument is based on two
key premises: (a) that certain kinds of intentions are needed in order for
LLMs' outputs to have literal meanings, and (b) that LLMs cannot plausibly have
the right kinds of intentions. We defend this argument from various types of
responses, for example, the semantic externalist argument that deference can be
assumed to take the place of intentions and the semantic internalist argument
that meanings can be defined purely in terms of intrinsic relations between
concepts, such as conceptual roles. We conclude the paper by discussing why,
even if our argument is sound, the outputs of LLMs nevertheless seem meaningful
and can be used to acquire true beliefs and even knowledge.

</details>


### [72] [Question-Driven Analysis and Synthesis: Building Interpretable Thematic Trees with LLMs for Text Clustering and Controllable Generation](https://arxiv.org/abs/2509.22211)
*Tiago Fernandes Tavares*

Main category: cs.CL

TL;DR: 提出RTP框架，利用LLM构建问题驱动的可解释文本分类树，解决传统主题模型语义连贯性差的问题


<details>
  <summary>Details</summary>
Motivation: 传统主题模型依赖关键词列表解释性差，数据稀缺领域尤其需要语义明确的分类方法

Method: 递归主题划分(RTP)：通过自然语言问题构建二进制树，每个节点用语义问题分割数据形成可解释分类体系

Result: 1. 问题驱动分类树比BERTopic更易解释 2. RTP特征在下游分类任务中表现优异 3. 主题路径可作为生成模型的结构化控制提示

Conclusion: RTP实现了从统计模式发现到知识驱动分析的范式转变，兼具文本分析与生成控制双重能力

Abstract: Unsupervised analysis of text corpora is challenging, especially in
data-scarce domains where traditional topic models struggle. While these models
offer a solution, they typically describe clusters with lists of keywords that
require significant manual effort to interpret and often lack semantic
coherence. To address this critical interpretability gap, we introduce
Recursive Thematic Partitioning (RTP), a novel framework that leverages Large
Language Models (LLMs) to interactively build a binary tree. Each node in the
tree is a natural language question that semantically partitions the data,
resulting in a fully interpretable taxonomy where the logic of each cluster is
explicit. Our experiments demonstrate that RTP's question-driven hierarchy is
more interpretable than the keyword-based topics from a strong baseline like
BERTopic. Furthermore, we establish the quantitative utility of these clusters
by showing they serve as powerful features in downstream classification tasks,
particularly when the data's underlying themes correlate with the task labels.
RTP introduces a new paradigm for data exploration, shifting the focus from
statistical pattern discovery to knowledge-driven thematic analysis.
Furthermore, we demonstrate that the thematic paths from the RTP tree can serve
as structured, controllable prompts for generative models. This transforms our
analytical framework into a powerful tool for synthesis, enabling the
consistent imitation of specific characteristics discovered in the source
corpus.

</details>


### [73] [StableToken: A Noise-Robust Semantic Speech Tokenizer for Resilient SpeechLLMs](https://arxiv.org/abs/2509.22220)
*Yuhan Song,Linhao Zhang,Chuhan Wu,Aiwei Liu,Wei Jia,Houfeng Wang,Xiao Zhou*

Main category: cs.CL

TL;DR: 提出StableToken语音分词器，通过多分支架构和投票机制解决现有语音分词器的稳定性缺陷，显著提升噪声环境下的单元编辑距离和下游语音大模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有语义语音分词器存在脆弱性问题——在语音清晰的高信噪比场景下，无关声学扰动会导致分词序列剧烈变化，增加下游语言模型学习负担。根源在于单路径量化架构的脆弱性，以及训练信号对中间过程稳定性缺乏关注。

Method: 提出多分支并行处理架构：1）并行生成多个音频表征 2）通过比特级投票机制达成共识，融合为稳定统一的token序列。共识机制通过位运算实现高效的稳定性决策。

Result: 1）Unit Edit Distance（UED）指标达到SOTA，各类噪声场景下编辑距离显著降低 2）下游SpeechLLM在多种任务（如噪声语音理解）的鲁棒性提升显著

Conclusion: 基础分词器的稳定性直接影响下游系统性能。StableToken通过架构创新实现根本性突破，为构建鲁棒的语音语言系统奠定基础。

Abstract: Prevalent semantic speech tokenizers, designed to capture linguistic content,
are surprisingly fragile. We find they are not robust to meaning-irrelevant
acoustic perturbations; even at high Signal-to-Noise Ratios (SNRs) where speech
is perfectly intelligible, their output token sequences can change drastically,
increasing the learning burden for downstream LLMs. This instability stems from
two flaws: a brittle single-path quantization architecture and a distant
training signal indifferent to intermediate token stability. To address this,
we introduce StableToken, a tokenizer that achieves stability through a
consensus-driven mechanism. Its multi-branch architecture processes audio in
parallel, and these representations are merged via a powerful bit-wise voting
mechanism to form a single, stable token sequence. StableToken sets a new
state-of-the-art in token stability, drastically reducing Unit Edit Distance
(UED) under diverse noise conditions. This foundational stability translates
directly to downstream benefits, significantly improving the robustness of
SpeechLLMs on a variety of tasks.

</details>


### [74] [Thinking in Many Modes: How Composite Reasoning Elevates Large Language Model Performance with Limited Data](https://arxiv.org/abs/2509.22224)
*Zishan Ahmad,Saisubramaniam Gopalakrishnan*

Main category: cs.CL

TL;DR: 提出了复合推理（CR）方法，通过动态结合多种推理风格提升大语言模型在复杂问题中的解决能力，在科学和医学问答任务中超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型依赖单一推理模式，难以应对需要多样化认知策略的复杂问题，需探索动态组合不同推理风格的机制。

Method: CR方法使模型能动态融合演绎/归纳/溯因等推理风格，并自适应领域特性（医学领域侧重溯因+演绎，科学领域侧重因果+演绎+归纳）

Result: 在科学/医学QA基准上，CR准确率超越CoT和SR，样本效率提高，token使用合理，且能自适应强化领域优势推理模式。

Conclusion: 通过培养内部推理风格多样性，大语言模型获得了更强健、自适应且高效的问题解决能力，验证了混合推理策略的有效性。

Abstract: Large Language Models (LLMs), despite their remarkable capabilities, rely on
singular, pre-dominant reasoning paradigms, hindering their performance on
intricate problems that demand diverse cognitive strategies. To address this,
we introduce Composite Reasoning (CR), a novel reasoning approach empowering
LLMs to dynamically explore and combine multiple reasoning styles like
deductive, inductive, and abductive for more nuanced problem-solving. Evaluated
on scientific and medical question-answering benchmarks, our approach
outperforms existing baselines like Chain-of-Thought (CoT) and also surpasses
the accuracy of DeepSeek-R1 style reasoning (SR) capabilities, while
demonstrating superior sample efficiency and adequate token usage. Notably, CR
adaptively emphasizes domain-appropriate reasoning styles. It prioritizes
abductive and deductive reasoning for medical question answering, but shifts to
causal, deductive, and inductive methods for scientific reasoning. Our findings
highlight that by cultivating internal reasoning style diversity, LLMs acquire
more robust, adaptive, and efficient problem-solving abilities.

</details>


### [75] [In Their Own Words: Reasoning Traces Tailored for Small Models Make Them Better Reasoners](https://arxiv.org/abs/2509.22230)
*Jaehoon Kim,Kwangwook Seo,Dongha Lee*

Main category: cs.CL

TL;DR: 通过反向推测解码（RSD）生成学生友好的推理轨迹，有效提升小模型推理能力迁移效果。


<details>
  <summary>Details</summary>
Motivation: 传统基于教师模型演示的蒸馏方法在迁移推理能力时存在分布不对齐问题，教师生成的高质量推理轨迹中的低概率token超出学生模型的表示能力，反而形成学习障碍。

Method: 提出反向推测解码（RSD）机制：教师生成候选token后，学生根据自身概率分布决定是否接受，自动过滤低概率token生成适配的推理轨迹。

Result: Qwen3-0.6B使用RSD训练后在主要推理基准提升4.9%，而传统蒸馏方法下降20.5%。跨模型实验显示RSD轨迹具有模型特异性。

Conclusion: 低概率token是推理迁移的关键瓶颈，RSD通过分布对齐机制有效突破该瓶颈，但需针对不同学生模型架构定制对齐策略。

Abstract: Transferring reasoning capabilities from larger language models to smaller
ones through supervised fine-tuning often fails counterintuitively, with
performance degrading despite access to high-quality teacher demonstrations. We
identify that this failure stems from distributional misalignment: reasoning
traces from larger models contain tokens that are low probability under the
student's distribution, exceeding the internal representation capacity of
smaller architectures and creating learning barriers rather than helpful
guidance. We propose Reverse Speculative Decoding (RSD), a mechanism for
generating student-friendly reasoning traces in which the teacher model
proposes candidate tokens but the student model determines acceptance based on
its own probability distributions, filtering low probability tokens. When
applied to Qwen3-0.6B, direct distillation of s1K-1.1 reasoning trace data
degrades average performance across major reasoning benchmarks by 20.5\%, while
the same model trained on RSD-generated reasoning traces achieves meaningful
improvements of 4.9\%. Our analysis reveals that low probability tokens
constitute the critical bottleneck in reasoning ability transfer. However,
cross-model experiments demonstrate that RSD traces are model-specific rather
than universally applicable, indicating that distributional alignment must be
tailored for each student architecture's unique internal representation.

</details>


### [76] [FeatBench: Evaluating Coding Agents on Feature Implementation for Vibe Coding](https://arxiv.org/abs/2509.22237)
*Haorui Chen,Chengze Li,Jia Li*

Main category: cs.CL

TL;DR: 提出FeatBench基准，针对vibe coding范式下自然语言功能实现的评估挑战，实验显示现有模型最高成功率仅29.94%且存在'激进实现'现象


<details>
  <summary>Details</summary>
Motivation: 现有代码生成基准无法有效评估vibe coding场景下的功能实现能力，需构建纯自然语言驱动的评估体系

Method: 通过多级过滤流程构建含纯自然语言提示、自动化测试用例(F2P/P2P)及跨领域代码库的评估基准FeatBench

Result: 顶尖LLM代理框架成功率不足30%，'激进实现'策略导致功能错误但软件设计质量优异

Conclusion: FeatBench有效暴露vibe coding实现难点，需平衡功能准确性与代码质量，推动更鲁棒的编码代理研究

Abstract: The rapid advancement of Large Language Models (LLMs) has given rise to a
novel software development paradigm known as "vibe coding," where users
interact with coding agents through high-level natural language. However,
existing evaluation benchmarks for code generation inadequately assess an
agent's vibe coding capabilities. Existing benchmarks are misaligned, as they
either require code-level specifications or focus narrowly on issue-solving,
neglecting the critical scenario of feature implementation within the vibe
coding paradiam. To address this gap, we propose FeatBench, a novel benchmark
for vibe coding that focuses on feature implementation. Our benchmark is
distinguished by several key features: 1. Pure Natural Language Prompts. Task
inputs consist solely of abstract natural language descriptions, devoid of any
code or structural hints. 2. A Rigorous & Evolving Data Collection Process.
FeatBench is built on a multi-level filtering pipeline to ensure quality and a
fully automated pipeline to evolve the benchmark, mitigating data
contamination. 3. Comprehensive Test Cases. Each task includes Fail-to-Pass
(F2P) and Pass-to-Pass (P2P) tests to verify correctness and prevent
regressions. 4. Diverse Application Domains. The benchmark includes
repositories from diverse domains to ensure it reflects real-world scenarios.
We evaluate two state-of-the-art agent frameworks with four leading LLMs on
FeatBench. Our evaluation reveals that feature implementation within the vibe
coding paradigm is a significant challenge, with the highest success rate of
only 29.94%. Our analysis also reveals a tendency for "aggressive
implementation," a strategy that paradoxically leads to both critical failures
and superior software design. We release FeatBench, our automated collection
pipeline, and all experimental results to facilitate further community
research.

</details>


### [77] [FLEXI: Benchmarking Full-duplex Human-LLM Speech Interaction](https://arxiv.org/abs/2509.22243)
*Yuan Ge,Saihan Chen,Jingqi Xiao,Xiaoqian Liu,Tong Xiao,Yan Xiang,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: 论文提出首个全双工语音交互基准FLEXI，评估LLM在紧急中断场景下的性能，揭示开源与商业模型的差距，并提出基于下一令牌对预测的优化方案。


<details>
  <summary>Details</summary>
Motivation: 现有全双工语音LLM在实时对话系统中存在基准测试和建模的挑战，特别是在紧急中断场景下的响应机制尚未系统研究。

Method: 通过六种人机交互场景构建FLEXI基准，从延迟、对话质量和紧急意识三个维度对模型进行系统性评估。

Result: 发现开源模型在紧急意识(61% vs 89%)、轮流终止准确率(74% vs 93%)和交互延迟(平均1.2s vs 0.4s)显著落后于商业模型。

Conclusion: 下一令牌对预测技术能有效优化语音端点检测和响应生成，是实现类人全双工交互的关键技术路径。

Abstract: Full-Duplex Speech-to-Speech Large Language Models (LLMs) are foundational to
natural human-computer interaction, enabling real-time spoken dialogue systems.
However, benchmarking and modeling these models remains a fundamental
challenge. We introduce FLEXI, the first benchmark for full-duplex LLM-human
spoken interaction that explicitly incorporates model interruption in emergency
scenarios. FLEXI systematically evaluates the latency, quality, and
conversational effectiveness of real-time dialogue through six diverse
human-LLM interaction scenarios, revealing significant gaps between open source
and commercial models in emergency awareness, turn terminating, and interaction
latency. Finally, we suggest that next token-pair prediction offers a promising
path toward achieving truly seamless and human-like full-duplex interaction.

</details>


### [78] [Safety Compliance: Rethinking LLM Safety Reasoning through the Lens of Compliance](https://arxiv.org/abs/2509.22250)
*Wenbin Hu,Huihao Jing,Haochen Shi,Haoran Li,Yangqiu Song*

Main category: cs.CL

TL;DR: 提出基于法律合规的LLM安全框架，通过构建新基准和GRPO对齐方法显著提升安全性能


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全方法缺乏系统性保护，需利用法律框架（如欧盟AI法案和GDPR）建立严格安全标准

Method: 创建法律条文驱动的安全场景基准，采用Group Policy Optimization对齐Qwen3-8B模型构建安全推理器

Result: 合规推理器在欧盟AI法案和GDPR基准上分别提升+10.45%和+11.85%平均性能

Conclusion: 法律合规框架结合GRPO方法有效降低LLM安全风险，为复杂AI系统安全提供新范式

Abstract: The proliferation of Large Language Models (LLMs) has demonstrated remarkable
capabilities, elevating the critical importance of LLM safety. However,
existing safety methods rely on ad-hoc taxonomy and lack a rigorous, systematic
protection, failing to ensure safety for the nuanced and complex behaviors of
modern LLM systems. To address this problem, we solve LLM safety from legal
compliance perspectives, named safety compliance. In this work, we posit
relevant established legal frameworks as safety standards for defining and
measuring safety compliance, including the EU AI Act and GDPR, which serve as
core legal frameworks for AI safety and data security in Europe. To bridge the
gap between LLM safety and legal compliance, we first develop a new benchmark
for safety compliance by generating realistic LLM safety scenarios seeded with
legal statutes. Subsequently, we align Qwen3-8B using Group Policy Optimization
(GRPO) to construct a safety reasoner, Compliance Reasoner, which effectively
aligns LLMs with legal standards to mitigate safety risks. Our comprehensive
experiments demonstrate that the Compliance Reasoner achieves superior
performance on the new benchmark, with average improvements of +10.45% for the
EU AI Act and +11.85% for GDPR.

</details>


### [79] [Beyond Textual Context: Structural Graph Encoding with Adaptive Space Alignment to alleviate the hallucination of LLMs](https://arxiv.org/abs/2509.22251)
*Yifang Zhang,Pengfei Duan,Yiwen Yang,Shengwu Xiong*

Main category: cs.CL

TL;DR: SSKG-LLM模型通过整合知识图谱的结构与语义信息，有效提升大语言模型的事实推理能力


<details>
  <summary>Details</summary>
Motivation: 现有方法仅利用知识图谱的语义信息，忽视其结构化特征，且图谱编码器与LLMs的嵌入空间存在差异

Method: 提出三模块架构：KGR检索模块、KGE编码模块（保留结构与语义）、KGA适配模块（对齐嵌入空间）

Result: 实验证明结构信息的整合显著增强LLMs的事实推理能力，代码已开源

Conclusion: SSKG-LLM创新性地融合知识图谱双维度信息，为解决LLM幻觉问题提供了新方向

Abstract: Currently, the main approach for Large Language Models (LLMs) to tackle the
hallucination issue is incorporating Knowledge Graphs(KGs).However, LLMs
typically treat KGs as plain text, extracting only semantic information and
limiting their use of the crucial structural aspects of KGs. Another challenge
is the gap between the embedding spaces of KGs encoders and LLMs text
embeddings, which hinders the effective integration of structured knowledge. To
overcome these obstacles, we put forward the SSKG-LLM, an innovative model
architecture that is designed to efficiently integrate both the Structural and
Semantic information of KGs into the reasoning processes of LLMs. SSKG-LLM
incorporates the Knowledge Graph Retrieval (KGR) module and the Knowledge Graph
Encoding (KGE) module to preserve semantics while utilizing structure. Then,
the Knowledge Graph Adaptation (KGA) module is incorporated to enable LLMs to
understand KGs embeddings. We conduct extensive experiments and provide a
detailed analysis to explore how incorporating the structural information of
KGs can enhance the factual reasoning abilities of LLMs. Our code are available
at https://github.com/yfangZhang/SSKG-LLM.

</details>


### [80] [Bridging Fairness and Explainability: Can Input-Based Explanations Promote Fairness in Hate Speech Detection?](https://arxiv.org/abs/2509.22291)
*Yifan Wang,Mayank Jobanputra,Ji-Ung Lee,Soyoung Oh,Isabel Valera,Vera Demberg*

Main category: cs.CL

TL;DR: 探讨可解释性在仇恨言论检测中与公平性的关系，发现输入解释可有效识别偏见并指导训练改进，但不可靠于模型选择


<details>
  <summary>Details</summary>
Motivation: 现有公平性NLP研究主要采用定性分析，缺乏大规模定量研究，尤其在可解释性对模型公平性影响的系统性验证不足

Method: 基于编码器和解码器模型，系统研究可解释性在偏见识别、模型选择和训练缓解三个维度的作用

Result: 输入解释能有效检测偏见预测（F1提升21%）并降低训练偏差（偏见表征减少38%），但无法可靠选择公平模型（准确率差异达15%）

Conclusion: 解释技术对偏见检测和训练缓解具有实用价值，但需谨慎用于模型筛选，未来需开发更可靠的可解释性评估框架

Abstract: Natural language processing (NLP) models often replicate or amplify social
bias from training data, raising concerns about fairness. At the same time,
their black-box nature makes it difficult for users to recognize biased
predictions and for developers to effectively mitigate them. While some studies
suggest that input-based explanations can help detect and mitigate bias, others
question their reliability in ensuring fairness. Existing research on
explainability in fair NLP has been predominantly qualitative, with limited
large-scale quantitative analysis. In this work, we conduct the first
systematic study of the relationship between explainability and fairness in
hate speech detection, focusing on both encoder- and decoder-only models. We
examine three key dimensions: (1) identifying biased predictions, (2) selecting
fair models, and (3) mitigating bias during model training. Our findings show
that input-based explanations can effectively detect biased predictions and
serve as useful supervision for reducing bias during training, but they are
unreliable for selecting fair models among candidates.

</details>


### [81] [Advancing Natural Language Formalization to First Order Logic with Fine-tuned LLMs](https://arxiv.org/abs/2509.22338)
*Felix Vossel,Till Mossakowski,Björn Gehrke*

Main category: cs.CL

TL;DR: 微调后的Flan-T5-XXL模型在自然语言转一阶逻辑任务中达到70%准确率，优于GPT-4o和符号系统，谓词可用性提升15-20%


<details>
  <summary>Details</summary>
Motivation: 自然语言自动转为一阶逻辑（FOL）对知识表示和形式化方法至关重要，但现有系统存在性能瓶颈

Method: 通过MALLS/Willow数据集评估编码器-解码器（T5）与仅解码器架构，采用词汇扩展/谓词条件调整/多语言训练策略，提出精确匹配/逻辑等价/谓词对齐三阶段评估体系

Result: 1) 谓词列表使性能提升15-20% 2) T5架构优于更大规模的仅解码器LLMs 3) 模型可零样本泛化至FOLIO未训练逻辑参数

Conclusion: 结构逻辑翻译稳健，但谓词提取成为主要瓶颈，后续研究需重点突破语义到逻辑原子的映射难题

Abstract: Automating the translation of natural language to first-order logic (FOL) is
crucial for knowledge representation and formal methods, yet remains
challenging. We present a systematic evaluation of fine-tuned LLMs for this
task, comparing architectures (encoder-decoder vs. decoder-only) and training
strategies. Using the MALLS and Willow datasets, we explore techniques like
vocabulary extension, predicate conditioning, and multilingual training,
introducing metrics for exact match, logical equivalence, and predicate
alignment. Our fine-tuned Flan-T5-XXL achieves 70% accuracy with predicate
lists, outperforming GPT-4o and even the DeepSeek-R1-0528 model with CoT
reasoning ability as well as symbolic systems like ccg2lambda. Key findings
show: (1) predicate availability boosts performance by 15-20%, (2) T5 models
surpass larger decoder-only LLMs, and (3) models generalize to unseen logical
arguments (FOLIO dataset) without specific training. While structural logic
translation proves robust, predicate extraction emerges as the main bottleneck.

</details>


### [82] [Transformers Can Learn Connectivity in Some Graphs but Not Others](https://arxiv.org/abs/2509.22343)
*Amit Roy,Abulhair Saparov*

Main category: cs.CL

TL;DR: Transformer模型能够学习低维网格图的连通性推理任务，但对多组件断开图表现较差，模型规模扩展有助于网格图泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探究Transformer模型从训练数据中学习传递关系推理的能力，以及模型规模对学习效果的影响，尤其关注不同图结构下的性能差异。

Method: 通过生成不同结构的有向图（网格图/多组件断开图）训练不同规模的Transformer模型，评估其在推断图连通性任务上的表现。

Result: 模型在低维网格图中表现优异（维度是预测指标），规模扩展提升泛化能力；但面对多组件断开图时学习困难，组件数量越大表现越差。

Conclusion: 图结构维度是Transformer学习能力的关键影响因素，模型规模扩展可提升网格图泛化能力，但对复杂断开图仍存在显著挑战。

Abstract: Reasoning capability is essential to ensure the factual correctness of the
responses of transformer-based Large Language Models (LLMs), and robust
reasoning about transitive relations is instrumental in many settings, such as
causal inference. Hence, it is essential to investigate the capability of
transformers in the task of inferring transitive relations (e.g., knowing A
causes B and B causes C, then A causes C). The task of inferring transitive
relations is equivalent to the task of connectivity in directed graphs (e.g.,
knowing there is a path from A to B, and there is a path from B to C, then
there is a path from A to C). Past research focused on whether transformers can
learn to infer transitivity from in-context examples provided in the input
prompt. However, transformers' capability to infer transitive relations from
training examples and how scaling affects the ability is unexplored. In this
study, we seek to answer this question by generating directed graphs to train
transformer models of varying sizes and evaluate their ability to infer
transitive relations for various graph sizes. Our findings suggest that
transformers are capable of learning connectivity on "grid-like'' directed
graphs where each node can be embedded in a low-dimensional subspace, and
connectivity is easily inferable from the embeddings of the nodes. We find that
the dimensionality of the underlying grid graph is a strong predictor of
transformers' ability to learn the connectivity task, where higher-dimensional
grid graphs pose a greater challenge than low-dimensional grid graphs. In
addition, we observe that increasing the model scale leads to increasingly
better generalization to infer connectivity over grid graphs. However, if the
graph is not a grid graph and contains many disconnected components,
transformers struggle to learn the connectivity task, especially when the
number of components is large.

</details>


### [83] [The InviTE Corpus: Annotating Invectives in Tudor English Texts for Computational Modeling](https://arxiv.org/abs/2509.22345)
*Sophie Spliethoff,Sanne Hoeken,Silke Schwandt,Sina Zarrieß,Özge Alaçam*

Main category: cs.CL

TL;DR: 应用NLP技术分析都铎王朝宗教改革中的抨击语言，构建专业语料库并验证历史数据预训练模型的有效性


<details>
  <summary>Details</summary>
Motivation: 解决传统历史研究中人工分析宗教抨击语言的低效问题，探索NLP技术在历史文本分析中的应用潜力

Method: 1. 构建包含2000句早期现代英语的InviTE语料库
2. 采用迭代标注流程结合专家知识
3. 对比微调BERT模型与零样本LLM的表现

Result: 基于历史数据预训练的BERT模型（F1=0.76）显著优于通用LLM（F1=0.63），验证领域适应的重要性

Conclusion: 历史语言学任务需要领域特定的预训练和微调，InviTE语料库为宗教改革研究提供了新的数字化分析工具

Abstract: In this paper, we aim at the application of Natural Language Processing (NLP)
techniques to historical research endeavors, particularly addressing the study
of religious invectives in the context of the Protestant Reformation in Tudor
England. We outline a workflow spanning from raw data, through pre-processing
and data selection, to an iterative annotation process. As a result, we
introduce the InviTE corpus -- a corpus of almost 2000 Early Modern English
(EModE) sentences, which are enriched with expert annotations regarding
invective language throughout 16th-century England. Subsequently, we assess and
compare the performance of fine-tuned BERT-based models and zero-shot prompted
instruction-tuned large language models (LLMs), which highlights the
superiority of models pre-trained on historical data and fine-tuned to
invective detection.

</details>


### [84] [Conversational Implicatures: Modelling Relevance Theory Probabilistically](https://arxiv.org/abs/2509.22354)
*Christoph Unger,Hendrik Buschmeier*

Main category: cs.CL

TL;DR: 探索贝叶斯概率方法在关联理论语用学中的应用，重点研究会话隐含的隐性意义传递机制


<details>
  <summary>Details</summary>
Motivation: 现有理性言语行为理论已成功建模Gricean语用现象，但针对Sperber和Wilson提出的关联理论的贝叶斯方法应用尚未充分探索。论文旨在通过典型语用现象（会话隐含）填补这一研究空白。

Method: 采用贝叶斯概率理论与计算工具，通过分析会话隐含这一典型语用现象，建立概率模型框架，将理性言语行为理论的方法论扩展至关联理论领域。

Result: 提出贝叶斯方法可有效建模关联理论的语用推理过程，特别是为隐含意义的概率化解释提供了新的计算框架，展示了该方法在复杂语用现象中的适应性。

Conclusion: 贝叶斯概率框架为关联理论语用学提供了形式化建模路径，拓展了计算语用学的研究边界，为理解隐性交际机制提供了新的理论工具和研究范式。

Abstract: Recent advances in Bayesian probability theory and its application to
cognitive science in combination with the development of a new generation of
computational tools and methods for probabilistic computation have led to a
'probabilistic turn' in pragmatics and semantics. In particular, the framework
of Rational Speech Act theory has been developed to model broadly Gricean
accounts of pragmatic phenomena in Bayesian terms, starting with fairly simple
reference games and covering ever more complex communicative exchanges such as
verbal syllogistic reasoning. This paper explores in which way a similar
Bayesian approach might be applied to relevance-theoretic pragmatics (Sperber &
Wilson, 1995) by study a paradigmatic pragmatic phenomenon: the communication
of implicit meaning by ways of (conversational) implicatures.

</details>


### [85] [CHRONOBERG: Capturing Language Evolution and Temporal Awareness in Foundation Models](https://arxiv.org/abs/2509.22360)
*Niharika Hegde,Subarnaduti Paul,Lars Joel-Frey,Manuel Brack,Kristian Kersting,Martin Mundt,Patrick Schramowski*

Main category: cs.CL

TL;DR: CHRONOBERG是一个250年跨度的英语书籍语料库，用于研究语言历时变化及提升LLMs的时间感知能力


<details>
  <summary>Details</summary>
Motivation: 现有语料库缺乏长期时间结构，限制了LLMs对语义演变和历时变异的捕捉能力

Method: 从Project Gutenberg收集书籍文本，添加时间注释，通过VAD分析量化词汇语义变化，构建历史情感词典并训练时序模型

Result: 揭示现代LLMs需加强时间背景感知，现有模型难以编码历时语义转变，证明时间感知训练的必要性

Conclusion: CHRONOBERG为语言变化研究和时间泛化能力提供了可扩展资源，其开源特性促进学术社区发展

Abstract: Large language models (LLMs) excel at operating at scale by leveraging social
media and various data crawled from the web. Whereas existing corpora are
diverse, their frequent lack of long-term temporal structure may however limit
an LLM's ability to contextualize semantic and normative evolution of language
and to capture diachronic variation. To support analysis and training for the
latter, we introduce CHRONOBERG, a temporally structured corpus of English book
texts spanning 250 years, curated from Project Gutenberg and enriched with a
variety of temporal annotations. First, the edited nature of books enables us
to quantify lexical semantic change through time-sensitive
Valence-Arousal-Dominance (VAD) analysis and to construct historically
calibrated affective lexicons to support temporally grounded interpretation.
With the lexicons at hand, we demonstrate a need for modern LLM-based tools to
better situate their detection of discriminatory language and contextualization
of sentiment across various time-periods. In fact, we show how language models
trained sequentially on CHRONOBERG struggle to encode diachronic shifts in
meaning, emphasizing the need for temporally aware training and evaluation
pipelines, and positioning CHRONOBERG as a scalable resource for the study of
linguistic change and temporal generalization. Disclaimer: This paper includes
language and display of samples that could be offensive to readers. Open
Access: Chronoberg is available publicly on HuggingFace at (
https://huggingface.co/datasets/spaul25/Chronoberg). Code is available at
(https://github.com/paulsubarna/Chronoberg).

</details>


### [86] [Exploratory Semantic Reliability Analysis of Wind Turbine Maintenance Logs using Large Language Models](https://arxiv.org/abs/2509.22366)
*Max Malyi,Jonathan Shek,Andre Biscaya*

Main category: cs.CL

TL;DR: 提出基于大语言模型的框架，突破传统文本分类局限，实现风电维护日志的深层语义分析与运营智能提升


<details>
  <summary>Details</summary>
Motivation: 传统风电维护日志分析局限于结构化分类，无法挖掘非结构化文本中隐藏的因果链、故障模式等深层信息，限制可靠性分析的深度

Method: 开发LLM驱动框架，在工业数据集上实施故障模式识别/因果链推断/站点对比分析/数据质量审计四阶段分析流程

Result: 验证LLM可作为「可靠性副驾驶」，不仅能标注数据，更能综合文本生成专家级可操作假设（如涡轮机偏航系统故障的振动传导路径分析）

Conclusion: 建立新型可复现方法论，将LLM转化为推理引擎，解锁非结构化数据中隐藏的运营智能，为风电行业可靠性工程提供新范式

Abstract: A wealth of operational intelligence is locked within the unstructured
free-text of wind turbine maintenance logs, a resource largely inaccessible to
traditional quantitative reliability analysis. While machine learning has been
applied to this data, existing approaches typically stop at classification,
categorising text into predefined labels. This paper addresses the gap in
leveraging modern large language models (LLMs) for more complex reasoning
tasks. We introduce an exploratory framework that uses LLMs to move beyond
classification and perform deep semantic analysis. We apply this framework to a
large industrial dataset to execute four analytical workflows: failure mode
identification, causal chain inference, comparative site analysis, and data
quality auditing. The results demonstrate that LLMs can function as powerful
"reliability co-pilots," moving beyond labelling to synthesise textual
information and generate actionable, expert-level hypotheses. This work
contributes a novel and reproducible methodology for using LLMs as a reasoning
tool, offering a new pathway to enhance operational intelligence in the wind
energy sector by unlocking insights previously obscured in unstructured data.

</details>


### [87] [What Is The Political Content in LLMs' Pre- and Post-Training Data?](https://arxiv.org/abs/2509.22367)
*Tanise Ceron,Dmitry Nikolaev,Dominik Stammbach,Debora Nozza*

Main category: cs.CL

TL;DR: 研究通过分析OLMO2模型训练数据发现，左倾政治内容在预训练语料库中占据主导，且训练数据中的政治立场与模型输出的政策偏见存在显著关联。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型生成政治偏见文本的根源，填补当前研究中对训练数据政治内容分析的空白。

Method: 从OLMO2的预训练/后训练语料库随机抽样，自动标注文档政治倾向，分析内容框架差异，并评估训练数据与模型政策立场相关性。

Result: 预训练数据左倾文档占比高且政治参与性强，左右倾文档使用不同价值观框架，训练数据主导立场与模型政策偏见强相关。

Conclusion: 需将政治内容分析纳入数据筛选流程，并完善过滤策略文档以提升透明度。

Abstract: Large language models (LLMs) are known to generate politically biased text,
yet how such biases arise remains unclear. A crucial step toward answering this
question is the analysis of training data, whose political content remains
largely underexplored in current LLM research. To address this gap, we present
in this paper an analysis of the pre- and post-training corpora of OLMO2, the
largest fully open-source model released together with its complete dataset.
From these corpora, we draw large random samples, automatically annotate
documents for political orientation, and analyze their source domains and
content. We then assess how political content in the training data correlates
with models' stance on specific policy issues. Our analysis shows that
left-leaning documents predominate across datasets, with pre-training corpora
containing significantly more politically engaged content than post-training
data. We also find that left- and right-leaning documents frame similar topics
through distinct values and sources of legitimacy. Finally, the predominant
stance in the training data strongly correlates with models' political biases
when evaluated on policy issues. These findings underscore the need to
integrate political content analysis into future data curation pipelines as
well as in-depth documentation of filtering strategies for transparency.

</details>


### [88] [Chimera: Diagnosing Shortcut Learning in Visual-Language Understanding](https://arxiv.org/abs/2509.22437)
*Ziheng Chi,Yifan Hou,Chenxi Pang,Shaobo Cui,Mubashara Akhtar,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 研究通过Chimera测试套件揭示了视觉语言模型在图表理解中主要依赖知识回忆和语言模式捷径，而非真正视觉推理能力


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在图表相关基准上的优异表现可能源于知识/推理/模态的捷径依赖，而非真实的图表理解能力

Method: 构建包含7,500维基百科图表的测试套件Chimera，通过语义三元组标注和多层次问题设计，系统性检测视觉记忆、知识回忆和Clever-Hans三类捷径

Result: 测试15个开源VLMs显示：知识回忆捷径影响中等（占比≈50%），Clever-Hans捷径贡献最大（正确率提升37.5%），视觉记忆捷径影响微弱（仅提升0.8%）

Conclusion: 当前VLMs在复杂视觉输入理解上存在严重局限性，需建立更鲁棒的评估协议以衡量真实理解能力而非问答捷径

Abstract: Diagrams convey symbolic information in a visual format rather than a linear
stream of words, making them especially challenging for AI models to process.
While recent evaluations suggest that vision-language models (VLMs) perform
well on diagram-related benchmarks, their reliance on knowledge, reasoning, or
modality shortcuts raises concerns about whether they genuinely understand and
reason over diagrams. To address this gap, we introduce Chimera, a
comprehensive test suite comprising 7,500 high-quality diagrams sourced from
Wikipedia; each diagram is annotated with its symbolic content represented by
semantic triples along with multi-level questions designed to assess four
fundamental aspects of diagram comprehension: entity recognition, relation
understanding, knowledge grounding, and visual reasoning. We use Chimera to
measure the presence of three types of shortcuts in visual question answering:
(1) the visual-memorization shortcut, where VLMs rely on memorized visual
patterns; (2) the knowledge-recall shortcut, where models leverage memorized
factual knowledge instead of interpreting the diagram; and (3) the Clever-Hans
shortcut, where models exploit superficial language patterns or priors without
true comprehension. We evaluate 15 open-source VLMs from 7 model families on
Chimera and find that their seemingly strong performance largely stems from
shortcut behaviors: visual-memorization shortcuts have slight impact,
knowledge-recall shortcuts play a moderate role, and Clever-Hans shortcuts
contribute significantly. These findings expose critical limitations in current
VLMs and underscore the need for more robust evaluation protocols that
benchmark genuine comprehension of complex visual inputs (e.g., diagrams)
rather than question-answering shortcuts.

</details>


### [89] [Detecting (Un)answerability in Large Language Models with Linear Directions](https://arxiv.org/abs/2509.22449)
*Maor Juliet Lavi,Tova Milo,Mor Geva*

Main category: cs.CL

TL;DR: 提出基于激活空间方向的无答案检测方法，提升LLMs在提取式QA中的准确性与跨数据集泛化能力


<details>
  <summary>Details</summary>
Motivation: LLMs在信息不足时仍会生成错误答案，现有基于提示和分类器的方法在跨数据集泛化上表现不足

Method: 通过推理阶段的激活添加找到无答案方向，将隐藏状态投影到该方向进行分类

Result: 在两个开源LLM和四个QA数据集上验证有效，检测性能优于基线方法，且可扩展至科学共识缺乏和主观性问题

Conclusion: 发现的激活方向不仅能检测QA无答案性，还能通过因果干预有效控制模型的弃权行为，为LLM可靠性研究提供新思路

Abstract: Large language models (LLMs) often respond confidently to questions even when
they lack the necessary information, leading to hallucinated answers. In this
work, we study the problem of (un)answerability detection, focusing on
extractive question answering (QA) where the model should determine if a
passage contains sufficient information to answer a given question. We propose
a simple approach for identifying a direction in the model's activation space
that captures unanswerability and uses it for classification. This direction is
selected by applying activation additions during inference and measuring their
impact on the model's abstention behavior. We show that projecting hidden
activations onto this direction yields a reliable score for (un)answerability
classification. Experiments on two open-weight LLMs and four extractive QA
benchmarks show that our method effectively detects unanswerable questions and
generalizes better across datasets than existing prompt-based and
classifier-based approaches. Moreover, the obtained directions extend beyond
extractive QA to unanswerability that stems from factors, such as lack of
scientific consensus and subjectivity. Last, causal interventions show that
adding or ablating the directions effectively controls the abstention behavior
of the model.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [90] [ControlHair: Physically-based Video Diffusion for Controllable Dynamic Hair Rendering](https://arxiv.org/abs/2509.21541)
*Weikai Lin,Haoxiang Li,Yuhao Zhu*

Main category: cs.GR

TL;DR: 结合物理模拟与视频扩散模型实现可控头发动态渲染


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型缺乏对头发动态的精细控制，需要整合物理模拟技术来解耦物理推理与视频生成过程

Method: 三阶段流程：1）通过物理模拟器将刚度等参数编码为几何形状 2）提取逐帧控制信号 3）将控制信号输入视频扩散模型生成动态视频

Result: 在10K视频数据集上训练后，ControlHair在动态控制精度上超越文本/姿态条件基线，支持发型试戴、子弹时间等三种应用场景

Conclusion: ControlHair首次实现物理感知的视频扩散框架，通过级联设计有效解耦物理模拟与生成过程，为动态控制提供新范式

Abstract: Hair simulation and rendering are challenging due to complex strand dynamics,
diverse material properties, and intricate light-hair interactions. Recent
video diffusion models can generate high-quality videos, but they lack
fine-grained control over hair dynamics. We present ControlHair, a hybrid
framework that integrates a physics simulator with conditional video diffusion
to enable controllable dynamic hair rendering. ControlHair adopts a three-stage
pipeline: it first encodes physics parameters (e.g., hair stiffness, wind) into
per-frame geometry using a simulator, then extracts per-frame control signals,
and finally feeds control signals into a video diffusion model to generate
videos with desired hair dynamics. This cascaded design decouples physics
reasoning from video generation, supports diverse physics, and makes training
the video diffusion model easy. Trained on a curated 10K video dataset,
ControlHair outperforms text- and pose-conditioned baselines, delivering
precisely controlled hair dynamics. We further demonstrate three use cases of
ControlHair: dynamic hairstyle try-on, bullet-time effects, and cinemagraphic.
ControlHair introduces the first physics-informed video diffusion framework for
controllable dynamics. We provide a teaser video and experimental results on
our website.

</details>


### [91] [PowerGS: Display-Rendering Power Co-Optimization for Neural Rendering in Power-Constrained XR Systems](https://arxiv.org/abs/2509.21702)
*Weikai Lin,Sushant Kondguli,Carl Marshall,Yuhao Zhu*

Main category: cs.GR

TL;DR: PowerGS框架首次在质量约束下联合优化3D高斯溅射的渲染与显示功耗，实现XR设备高达86%的节能效果


<details>
  <summary>Details</summary>
Motivation: 现有3DGS模型在瓦特级功耗限制的XR设备上能效不足，需要在不显著降低视觉质量的前提下实现功耗优化

Method: 通过建立功耗-质量联合优化模型，1)识别显示/渲染功耗构成的等质量曲线 2)解析求解曲线上的最低功耗点，并支持注视点渲染技术

Result: 实验证明相比现有3DGS方案最多降低86%总功耗，主客观质量评估显示视觉质量损失可忽略

Conclusion: PowerGS通过数学建模和参数优化，显著提升3DGS在XR设备的能效比，为移动端图形渲染提供新解决方案

Abstract: 3D Gaussian Splatting (3DGS) combines classic image-based rendering,
pointbased graphics, and modern differentiable techniques, and offers an
interesting alternative to traditional physically-based rendering. 3DGS-family
models are far from efficient for power-constrained Extended Reality (XR)
devices, which need to operate at a Watt-level. This paper introduces PowerGS,
the first framework to jointly minimize the rendering and display power in 3DGS
under a quality constraint. We present a general problem formulation and show
that solving the problem amounts to 1) identifying the iso-quality curve(s) in
the landscape subtended by the display and rendering power and 2) identifying
the power-minimal point on a given curve, which has a closed-form solution
given a proper parameterization of the curves. PowerGS also readily supports
foveated rendering for further power savings. Extensive experiments and user
studies show that PowerGS achieves up to 86% total power reduction compared to
state-of-the-art 3DGS models, with minimal loss in both subjective and
objective quality. Code is available at
https://github.com/horizon-research/PowerGS.

</details>


### [92] [Rigidity-Aware 3D Gaussian Deformation from a Single Image](https://arxiv.org/abs/2509.22222)
*Jinhyeok Kim,Jaehun Bang,Seunghyun Seo,Kyungdon Joo*

Main category: cs.GR

TL;DR: 提出DeformSplat框架，通过单张图像指导3D高斯形变，结合高斯-像素匹配和刚性部件分割技术实现单目变形重建


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖多视角视频重建形变，在受限场景中适用性不足。需突破单目图像重建物体形变的挑战

Method: 1. 高斯-像素匹配技术：桥接3D高斯表示与2D像素观测的领域鸿沟
2. 刚性部件分割方案：通过初始化+优化的两阶段分割保持形变过程的几何一致性

Result: 在单目图像重建任务中显著超越现有方法，支持帧插值和交互式物体操控等应用

Conclusion: 该框架突破了单目形变重建的技术瓶颈，为计算机视觉和图形学应用提供了新的解决方案

Abstract: Reconstructing object deformation from a single image remains a significant
challenge in computer vision and graphics. Existing methods typically rely on
multi-view video to recover deformation, limiting their applicability under
constrained scenarios. To address this, we propose DeformSplat, a novel
framework that effectively guides 3D Gaussian deformation from only a single
image. Our method introduces two main technical contributions. First, we
present Gaussian-to-Pixel Matching which bridges the domain gap between 3D
Gaussian representations and 2D pixel observations. This enables robust
deformation guidance from sparse visual cues. Second, we propose Rigid Part
Segmentation consisting of initialization and refinement. This segmentation
explicitly identifies rigid regions, crucial for maintaining geometric
coherence during deformation. By combining these two techniques, our approach
can reconstruct consistent deformations from a single image. Extensive
experiments demonstrate that our approach significantly outperforms existing
methods and naturally extends to various applications,such as frame
interpolation and interactive object manipulation.

</details>


### [93] [Aerial Path Planning for Urban Geometry and Texture Co-Capture](https://arxiv.org/abs/2509.22227)
*Weidan Xiong,Bochuan Zeng,Ziyu Hu,Jianwei Guo,Ke Xie,Hui Huang*

Main category: cs.GR

TL;DR: 提出基于航拍路径规划的城市场景几何与纹理协同捕获框架，通过多目标优化实现高质量重建


<details>
  <summary>Details</summary>
Motivation: 现有城市建模技术过度关注几何结构而忽视纹理质量，导致模型存在明显视觉伪影

Method: 结合垂直俯视与水平平面视角采集，开发包含建筑立面指标的纹理评估系统，采用多目标优化策略和序列路径规划算法

Result: 在大规模合成和真实城市数据集验证中，成功实现低成本的高保真几何与纹理同步重建

Conclusion: 该框架有效解决了有限先验知识下的城市场景建模难题，为低成本创建真实感三维城市模型提供了新方案

Abstract: Recent advances in image acquisition and scene reconstruction have enabled
the generation of high-quality structural urban scene geometry, given
sufficient site information. However, current capture techniques often overlook
the crucial importance of texture quality, resulting in noticeable visual
artifacts in the textured models. In this work, we introduce the urban geometry
and texture co-capture problem under limited prior knowledge before a site
visit. The only inputs are a 2D building contour map of the target area and a
safe flying altitude above the buildings. We propose an innovative aerial path
planning framework designed to co-capture images for reconstructing both
structured geometry and high-fidelity textures. To evaluate and guide view
planning, we introduce a comprehensive texture quality assessment system,
including two novel metrics tailored for building facades. Firstly, our method
generates high-quality vertical dipping views and horizontal planar views to
effectively capture both geometric and textural details. A multi-objective
optimization strategy is then proposed to jointly maximize texture fidelity,
improve geometric accuracy, and minimize the cost associated with aerial views.
Furthermore, we present a sequential path planning algorithm that accounts for
texture consistency during image capture. Extensive experiments on large-scale
synthetic and real-world urban datasets demonstrate that our approach
effectively produces image sets suitable for concurrent geometric and texture
reconstruction, enabling the creation of realistic, textured scene proxies at
low operational cost.

</details>


### [94] [Learning to Ball: Composing Policies for Long-Horizon Basketball Moves](https://arxiv.org/abs/2509.22442)
*Pei Xu,Zhen Wu,Ruocheng Wang,Vishnu Sarukkai,Kayvon Fatahalian,Ioannis Karamouzas,Victor Zordan,C. Karen Liu*

Main category: cs.GR

TL;DR: 提出新型策略整合框架和软路由机制，解决多阶段长时程任务中不同运动技能组合与过渡难题


<details>
  <summary>Details</summary>
Motivation: 现有方法在中间状态不明确的多阶段任务中存在局限，尤其当子策略间缺乏共享状态或清晰边界时，专家混合和技能链方法难以有效组合策略

Method: 1. 新型策略整合框架支持差异显著的技能组合 2. 高层软路由机制实现子任务无缝过渡

Result: 在篮球基础技能和复杂过渡场景中，策略能实时响应用户指令完成运球-投篮等长时程任务，无需依赖球轨迹参考

Conclusion: 该框架成功解决了多阶段任务中中间状态不明确带来的策略组合难题，为复杂运动控制任务提供了新解决方案

Abstract: Learning a control policy for a multi-phase, long-horizon task, such as
basketball maneuvers, remains challenging for reinforcement learning approaches
due to the need for seamless policy composition and transitions between skills.
A long-horizon task typically consists of distinct subtasks with well-defined
goals, separated by transitional subtasks with unclear goals but critical to
the success of the entire task. Existing methods like the mixture of experts
and skill chaining struggle with tasks where individual policies do not share
significant commonly explored states or lack well-defined initial and terminal
states between different phases. In this paper, we introduce a novel policy
integration framework to enable the composition of drastically different motor
skills in multi-phase long-horizon tasks with ill-defined intermediate states.
Based on that, we further introduce a high-level soft router to enable seamless
and robust transitions between the subtasks. We evaluate our framework on a set
of fundamental basketball skills and challenging transitions. Policies trained
by our approach can effectively control the simulated character to interact
with the ball and accomplish the long-horizon task specified by real-time user
commands, without relying on ball trajectory references.

</details>
