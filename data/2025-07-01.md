<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 17]
- [cs.GR](#cs.GR) [Total: 11]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Psycholinguistic Word Features: a New Approach for the Evaluation of LLMs Alignment with Humans](https://arxiv.org/abs/2506.22439)
*Javier Conde,Miguel González,María Grandury,Gonzalo Martínez,Pedro Reviriego,Mar Brysbaert*

Main category: cs.CL

TL;DR: 通过心理语言学数据集评估大语言模型与人类词汇特征的对齐程度，发现LLMs在情感类特征表现较好，感官关联特征表现较弱，揭示其缺乏具身认知的局限性


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估多关注可量化的任务表现，但对情感唤醒度/具体性/感官关联等难以客观量化的语言特征缺乏研究，需借助心理语言学的人类评分数据进行补充验证

Method: 使用Glasgow规范（7个情感/认知特征）和Lancaster规范（6个感官特征）两个心理语言学数据集，评估多个代表性LLM与人类评分的相关性

Result: Glasgow规范中的唤醒度/效价/具象性等特征对齐度较高（平均r=0.5），而Lancaster规范的嗅觉/触觉/听觉等感官特征对齐度显著偏低（平均r=0.2）

Conclusion: LLMs在感官词汇关联上的局限反映其缺乏人类具身认知能力，心理语言学数据集可作为评估模型语言理解深度的有效工具

Abstract: The evaluation of LLMs has so far focused primarily on how well they can
perform different tasks such as reasoning, question-answering, paraphrasing, or
translating. For most of these tasks, performance can be measured with
objective metrics, such as the number of correct answers. However, other
language features are not easily quantified. For example, arousal,
concreteness, or gender associated with a given word, as well as the extent to
which we experience words with senses and relate them to a specific sense.
Those features have been studied for many years by psycholinguistics,
conducting large-scale experiments with humans to produce ratings for thousands
of words. This opens an opportunity to evaluate how well LLMs align with human
ratings on these word features, taking advantage of existing studies that cover
many different language features in a large number of words. In this paper, we
evaluate the alignment of a representative group of LLMs with human ratings on
two psycholinguistic datasets: the Glasgow and Lancaster norms. These datasets
cover thirteen features over thousands of words. The results show that
alignment is \textcolor{black}{generally} better in the Glasgow norms evaluated
(arousal, valence, dominance, concreteness, imageability, familiarity, and
gender) than on the Lancaster norms evaluated (introceptive, gustatory,
olfactory, haptic, auditory, and visual). This suggests a potential limitation
of current LLMs in aligning with human sensory associations for words, which
may be due to their lack of embodied cognition present in humans and
illustrates the usefulness of evaluating LLMs with psycholinguistic datasets.

</details>


### [2] [AI Agents-as-Judge: Automated Assessment of Accuracy, Consistency, Completeness and Clarity for Enterprise Documents](https://arxiv.org/abs/2506.22485)
*Sudip Dasgupta,Himanshu Shankar*

Main category: cs.CL

TL;DR: 开发基于多AI代理的自动化企业文档审核系统，实现99%信息一致性并显著提升审核效率


<details>
  <summary>Details</summary>
Motivation: 传统方案无法有效处理结构化企业文档的质量审查需求，需建立标准化、可扩展的自动化审核框架

Method: 利用LangChain/CrewAI等工具构建模块化代理系统，通过专业化分工实现文档分项评估，建立持续优化机制

Result: 系统性能超越人工：信息一致性达99%（人类92%），审核时间从30分钟降至2.5分钟，错误率减半

Conclusion: 该系统为企业文档质量保障提供可扩展解决方案，但需保持人机协作并控制LLM使用成本

Abstract: This study presents a modular, multi-agent system for the automated review of
highly structured enterprise business documents using AI agents. Unlike prior
solutions focused on unstructured texts or limited compliance checks, this
framework leverages modern orchestration tools such as LangChain, CrewAI,
TruLens, and Guidance to enable section-by-section evaluation of documents for
accuracy, consistency, completeness, and clarity. Specialized agents, each
responsible for discrete review criteria such as template compliance or factual
correctness, operate in parallel or sequence as required. Evaluation outputs
are enforced to a standardized, machine-readable schema, supporting downstream
analytics and auditability. Continuous monitoring and a feedback loop with
human reviewers allow for iterative system improvement and bias mitigation.
  Quantitative evaluation demonstrates that the AI Agent-as-Judge system
approaches or exceeds human performance in key areas: achieving 99% information
consistency (vs. 92% for humans), halving error and bias rates, and reducing
average review time from 30 to 2.5 minutes per document, with a 95% agreement
rate between AI and expert human judgment. While promising for a wide range of
industries, the study also discusses current limitations, including the need
for human oversight in highly specialized domains and the operational cost of
large-scale LLM usage. The proposed system serves as a flexible, auditable, and
scalable foundation for AI-driven document quality assurance in the enterprise
context.

</details>


### [3] [Hallucination Detection with Small Language Models](https://arxiv.org/abs/2506.22486)
*Ming Cheung*

Main category: cs.CL

TL;DR: 提出集成多个小型语言模型的框架，通过分解响应句子并计算多模型验证概率，有效检测LLMs生成内容中的幻觉，实验显示F1分数提升10%


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLMs)在检索增强生成中存在幻觉问题，缺乏真实数据时难以检测，影响实际应用可靠性

Method: 将LLMs生成的回答分解为单句，利用多个小型语言模型对每个句子生成'Yes'标记的概率进行交叉验证

Result: 在包含100+组真实数据集的实验中，检测正确回答的F1分数相比传统方法提升10%，包括对部分正确句子的有效识别

Conclusion: 多小型语言模型协同验证框架为LLMs可靠性问题提供了可扩展解决方案，在学术和工业场景中具有应用潜力

Abstract: Since the introduction of ChatGPT, large language models (LLMs) have
demonstrated significant utility in various tasks, such as answering questions
through retrieval-augmented generation. Context can be retrieved using a
vectorized database, serving as a foundation for LLMs to generate responses.
However, hallucinations in responses can undermine the reliability of LLMs in
practical applications, and they are not easily detectable in the absence of
ground truth, particularly in question-and-answer scenarios. This paper
proposes a framework that integrates multiple small language models to verify
responses generated by LLMs using the retrieved context from a vectorized
database. By breaking down the responses into individual sentences and
utilizing the probability of generating "Yes" tokens from the outputs of
multiple models for a given set of questions, responses, and relevant context,
hallucinations can be detected. The proposed framework is validated through
experiments with real datasets comprising over 100 sets of questions, answers,
and contexts, including responses with fully and partially correct sentences.
The results demonstrate a 10\% improvement in F1 scores for detecting correct
responses compared to hallucinations, indicating that multiple small language
models can be effectively employed for answer verification, providing a
scalable and efficient solution for both academic and practical applications.

</details>


### [4] [PromptAug: Fine-grained Conflict Classification Using Data Augmentation](https://arxiv.org/abs/2506.22491)
*Oliver Warke,Joemon M. Jose,Faegheh Hasibi,Jan Breitsohl*

Main category: cs.CL

TL;DR: 提出PromptAug大模型数据增强方法，在冲突检测等敏感任务中提升模型性能2%，并通过跨学科方法揭示增强文本的潜在问题


<details>
  <summary>Details</summary>
Motivation: 社交媒体冲突检测需要高质量标注数据，但此类敏感任务的标注数据稀缺且获取困难，传统数据增强方法面临LLM内容安全限制的挑战

Method: 开发基于大语言模型（LLM）的PromptAug数据增强方法，通过系统提示工程生成多样化训练数据

Result: 在冲突和情感数据集上实现准确率/F1值提升2%，主题分析发现语言流畅性、幽默歧义等四类数据增强缺陷模式

Conclusion: PromptAug为敏感任务提供有效数据增强方案，结合NLP和社会科学方法构建了创新的跨学科评估框架

Abstract: Given the rise of conflicts on social media, effective classification models
to detect harmful behaviours are essential. Following the
garbage-in-garbage-out maxim, machine learning performance depends heavily on
training data quality. However, high-quality labelled data, especially for
nuanced tasks like identifying conflict behaviours, is limited, expensive, and
difficult to obtain. Additionally, as social media platforms increasingly
restrict access to research data, text data augmentation is gaining attention
as an alternative to generate training data. Augmenting conflict-related data
poses unique challenges due to Large Language Model (LLM) guardrails that
prevent generation of offensive content. This paper introduces PromptAug, an
innovative LLM-based data augmentation method. PromptAug achieves statistically
significant improvements of 2% in both accuracy and F1-score on conflict and
emotion datasets. To thoroughly evaluate PromptAug against other data
augmentation methods we conduct a robust evaluation using extreme data scarcity
scenarios, quantitative diversity analysis and a qualitative thematic analysis.
The thematic analysis identifies four problematic patterns in augmented text:
Linguistic Fluidity, Humour Ambiguity, Augmented Content Ambiguity, and
Augmented Content Misinterpretation.
  Overall, this work presents PromptAug as an effective method for augmenting
data in sensitive tasks like conflict detection, offering a unique,
interdisciplinary evaluation grounded in both natural language processing and
social science methodology.

</details>


### [5] [AgentStealth: Reinforcing Large Language Model for Anonymizing User-generated Text](https://arxiv.org/abs/2506.22508)
*Chenyang Shao,Tianxing Li,Chenhao Pu,Fengli Xu,Yong Li*

Main category: cs.CL

TL;DR: 提出AgentStealth框架，通过本地小型语言模型实现高效文本匿名化，结合对抗训练和强化学习，效果提升12.3%且支持边缘部署。


<details>
  <summary>Details</summary>
Motivation: 现有匿名化方法存在云隐私风险与实用性差的问题，需要开发本地化轻量解决方案。

Method: 1. 对抗性匿名化流程（上下文对比学习+效用控制）
2. 监督式模型适配
3. 基于对抗反馈的在线强化学习

Result: 在两个数据集上匿名效果提升12.3%，实用性提升6.8%；模型体积缩小80%支持边缘设备部署

Conclusion: 首次实现完全本地化的高效文本匿名化框架，通过自增强机制突破小模型性能瓶颈，开源促进隐私保护技术发展。

Abstract: In today's digital world, casual user-generated content often contains subtle
cues that may inadvertently expose sensitive personal attributes. Such risks
underscore the growing importance of effective text anonymization to safeguard
individual privacy. However, existing methods either rely on rigid replacements
that damage utility or cloud-based LLMs that are costly and pose privacy risks.
To address these issues, we explore the use of locally deployed smaller-scale
language models (SLMs) for anonymization. Yet training effective SLMs remains
challenging due to limited high-quality supervision. To address the challenge,
we propose AgentStealth, a self-reinforcing LLM anonymization framework.First,
we introduce an adversarial anonymization workflow enhanced by In-context
Contrastive Learning and Adaptive Utility-Aware Control. Second, we perform
supervised adaptation of SLMs using high-quality data collected from the
workflow, which includes both anonymization and attack signals. Finally, we
apply online reinforcement learning where the model leverages its internal
adversarial feedback to iteratively improve anonymization performance.
Experiments on two datasets show that our method outperforms baselines in both
anonymization effectiveness (+12.3%) and utility (+6.8%). Our lightweight
design supports direct deployment on edge devices, avoiding cloud reliance and
communication-based privacy risks. Our code is open-source at
https://github.com/tsinghua-fib-lab/AgentStealth.

</details>


### [6] [Towards Text-free Graph Foundation Models: Rethinking Multi-Domain Graph Contrastive Learning](https://arxiv.org/abs/2506.22510)
*Zihao Zhao,Xinlong Zhai,Jinyu Yang,Chuan Shi*

Main category: cs.CL

TL;DR: 提出了多领域图对比学习框架MDGCL，通过领域差异感知的对比预训练和跨领域注意力机制，显著提升图基础模型的跨领域知识迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有图基础模型的对比预训练策略忽视领域差异，导致跨领域知识吸收效率低下。不同领域的图数据在语义和属性上存在巨大差异，传统单领域预训练方法难以有效迁移知识。

Method: 1. 预训练阶段：设计领域差异感知的对比学习策略，引入领域令牌编码全局领域信息
2. 下游任务：采用领域注意力机制实现细粒度跨领域知识迁移

Result: 在5个基准数据集上实现SOTA，最大准确率提升19.33%，Macro-F1提升19.13%

Conclusion: MDGCL有效解决了图基础模型跨领域迁移的挑战，验证了领域差异建模和细粒度知识迁移机制的重要性，为图学习领域提供了新的预训练范式。

Abstract: Foundation models have achieved great success in natural language processing
(NLP) and computer vision (CV). Their success largely stems from the ability to
integrate multi-domain knowledge in pre-training and transfer it to target
domains. Considering graph data, especially graphs without textual features, is
ubiquitous in real-world applications such as social networks and
recommendation systems, some researchers have attempted to extend this paradigm
to the graph field, aiming to construct graph foundation models. However,
unlike CV and NLP, there are huge gaps among the semantics and properties of
graphs in different domains, while current works still adopt traditional
contrastive pre-training strategies designed in the single-domain scenario,
which regard contrastive samples from different domains as equivalent. From
experimental investigations, we discovered that inherent domain-specific
differences prevent these strategies from effectively absorbing knowledge from
different domains to generate informative representations. In this paper, we
propose a novel multi-domain pre-training and cross-domain transfer framework,
namely MDGCL.In the pre-training stage, we design a contrastive learning
strategy to substantially recognize and capture domain differences, and
introduce domain tokens to encode domain-level global information. In the
downstream stage, we introduce a domain attention mechanism to enable
fine-grained domain knowledge transfer. Extensive experiments on five benchmark
datasets have demonstrated that our method outperforms state-of-the-art
significantly, with the maximum improvement of 19.33\% on accuracy and 19.13\%
on Macro-F1 score.

</details>


### [7] [Can "consciousness" be observed from large language model (LLM) internal states? Dissecting LLM representations obtained from Theory of Mind test with Integrated Information Theory and Span Representation analysis](https://arxiv.org/abs/2506.22516)
*Jingkai Li*

Main category: cs.CL

TL;DR: 研究应用整合信息理论(IIT 3.0/4.0)分析大语言模型的表征，发现其缺乏显著意识指标但在时空置换分析中呈现特殊模式


<details>
  <summary>Details</summary>
Motivation: 验证IIT理论在LLM表征中的适用性，区分心理理论测试表现差异的潜在意识现象与表征空间固有分离

Method: 使用IIT的Φ最大值/Φ值/概念信息/Φ结构指标，结合心理理论测试数据，进行跨transformer层和语言跨度的实验分析

Result: 当前Transformer架构的LLM表征未显示统计显著的意识指标，时空置换分析揭示特殊表征模式

Conclusion: 当代LLM表征系统未表现出意识现象特征，但时空分析为模型表征研究提供新的方法论视角

Abstract: Integrated Information Theory (IIT) provides a quantitative framework for
explaining consciousness phenomenon, positing that conscious systems comprise
elements integrated through causal properties. We apply IIT 3.0 and 4.0 -- the
latest iterations of this framework -- to sequences of Large Language Model
(LLM) representations, analyzing data derived from existing Theory of Mind
(ToM) test results. Our study systematically investigates whether the
differences of ToM test performances, when presented in the LLM
representations, can be revealed by IIT estimates, i.e., $\Phi^{\max}$ (IIT
3.0), $\Phi$ (IIT 4.0), Conceptual Information (IIT 3.0), and $\Phi$-structure
(IIT 4.0). Furthermore, we compare these metrics with the Span Representations
independent of any estimate for consciousness. This additional effort aims to
differentiate between potential "consciousness" phenomena and inherent
separations within LLM representational space. We conduct comprehensive
experiments examining variations across LLM transformer layers and linguistic
spans from stimuli. Our results suggest that sequences of contemporary
Transformer-based LLM representations lack statistically significant indicators
of observed "consciousness" phenomena but exhibit intriguing patterns under
$\textit{spatio}$-permutational analyses. The Appendix and code are available
as Supplementary Materials at: https://doi.org/10.1016/j.nlp.2025.100163.

</details>


### [8] [Weak-to-Strong GraphRAG: Aligning Weak Retrievers with Large Language Models for Graph-based Retrieval Augmented Generation](https://arxiv.org/abs/2506.22518)
*Deyu Zou,Yongqiang Chen,Mufei Li,Siqi Miao,Chenxi Liu,Bo Han,James Cheng,Pan Li*

Main category: cs.CL

TL;DR: 提出ReG框架，通过LLM反馈对齐弱检索器和结构重组模块优化图RAG，显著提升性能并降低推理成本


<details>
  <summary>Details</summary>
Motivation: 传统图RAG存在弱监督训练引入虚假信号、图知识无序化检索两大缺陷

Method: 1) 引入LLM反馈机制净化监督信号 2) 设计结构感知重组模块构建逻辑连贯的证据链

Result: 性能提升10%，5%训练数据匹配SOTA，推理token成本降低30%且效果提升4%

Conclusion: ReG有效解决图RAG的监督信号偏差和知识组织问题，具备跨知识图谱迁移潜力

Abstract: Graph-based retrieval-augmented generation (RAG) enables large language
models (LLMs) to ground responses with structured external knowledge from
up-to-date knowledge graphs (KGs) and reduce hallucinations. However, LLMs
often rely on a weak retriever in graph-based RAG: I) Due to the lack of ground
truth, the retriever is often trained on weak supervision, which often
introduces spurious signals to the LLMs. II) Due to the abstraction of graph
data, the retrieved knowledge is often presented in unorganized forms. To
mitigate the issue, we present Refined Graph-based RAG (ReG) to align weak
retrievers to LLMs for graph-based RAG. Specifically, ReG incorporates LLM
feedback to get rid of spurious signals and improve the quality of the
supervision. Meanwhile, ReG introduces a structure-aware reorganization module
to refactor the retrieval results into logically coherent evidence chains.
Experiments on prominent benchmarks demonstrate that ReG significantly and
consistently brings improvements across different LLM backbones by up to 10%.
The improved supervision quality enables ReG to match the state-of-the-art
performance with 5% training data and to transfer to out-of-distribution KGs.
Notably, when adopted to reasoning-based LLMs, ReG reduces the reasoning token
cost by up to 30% and improves the performance by up to 4%.

</details>


### [9] [MisinfoTeleGraph: Network-driven Misinformation Detection for German Telegram Messages](https://arxiv.org/abs/2506.22529)
*Lu Kalkbrenner,Veronika Solopova,Steffen Zeiler,Robert Nickel,Dorothea Kolossa*

Main category: cs.CL

TL;DR: 创建首个德语Telegram虚假信息检测图数据集Misinfo-TeleGraph，验证图神经网络(GraphSAGE+LSTM)优于纯文本模型


<details>
  <summary>Details</summary>
Motivation: Telegram作为低监管平台成为虚假信息传播温床（尤其德国选举场景），但现有研究缺乏结构化数据和有效检测方法

Method: 整合500万条公开消息，结合语义相似度(M3嵌入)和人工标注生成强弱标签，对比文本模型与融合消息转发结构的GNN模型

Result: GraphSAGE+LSTM在MCC和F1分数上显著优于基准模型；订阅者数量和自动标注对性能影响显著但存在数据噪声挑战

Conclusion: 该研究为德语Telegram网络提供可复现基准和开源数据集，揭示了弱监督在虚假信息检测领域的应用潜力与数据质量挑战

Abstract: Connectivity and message propagation are central, yet often underutilized,
sources of information in misinformation detection -- especially on poorly
moderated platforms such as Telegram, which has become a critical channel for
misinformation dissemination, namely in the German electoral context. In this
paper, we introduce Misinfo-TeleGraph, the first German-language Telegram-based
graph dataset for misinformation detection. It includes over 5 million messages
from public channels, enriched with metadata, channel relationships, and both
weak and strong labels. These labels are derived via semantic similarity to
fact-checks and news articles using M3-embeddings, as well as manual
annotation. To establish reproducible baselines, we evaluate both text-only
models and graph neural networks (GNNs) that incorporate message forwarding as
a network structure. Our results show that GraphSAGE with LSTM aggregation
significantly outperforms text-only baselines in terms of Matthews Correlation
Coefficient (MCC) and F1-score. We further evaluate the impact of subscribers,
view counts, and automatically versus human-created labels on performance, and
highlight both the potential and challenges of weak supervision in this domain.
This work provides a reproducible benchmark and open dataset for future
research on misinformation detection in German-language Telegram networks and
other low-moderation social platforms.

</details>


### [10] [RExBench: Can coding agents autonomously implement AI research extensions?](https://arxiv.org/abs/2506.22598)
*Nicholas Edwards,Yukyung Lee,Yujun,Mao,Yulu Qin,Sebastian Schuster,Najoung Kim*

Main category: cs.CL

TL;DR: 论文提出RExBench基准测试，评估LLM代理在未实现研究假设任务中的扩展能力，结果显示当前代理成功率不足40%仍需人工指导


<details>
  <summary>Details</summary>
Motivation: 评估基于LLM的代理在科研流程中实现研究扩展能力，现有系统在此类复杂任务中表现不足需系统性评估工具

Method: 创建包含12个未实现研究假设任务的RExBench基准，结合专家指令和自动执行评估框架，测试三个不同框架的9个LLM代理

Result: 所有代理在无提示情况下无法完成多数扩展任务，最佳提示后成功率仍低于40%

Conclusion: 当前LLM代理尚无法自主处理现实研究扩展任务，需显著人工指导，显示该领域仍需重大技术突破

Abstract: Agents based on Large Language Models (LLMs) have shown promise for
performing sophisticated software engineering tasks autonomously. In addition,
there has been progress towards developing agents that can perform parts of the
research pipeline in machine learning and the natural sciences. We argue that
research extension and its implementation is a critical capability for such
systems, and introduce RExBench to support the evaluation of this capability.
RExBench is a benchmark consisting of 12 realistic research experiment
implementation tasks that aim to investigate research hypotheses that have not
previously been implemented. Each task is set up as an extension to an existing
research paper and codebase, accompanied by domain expert-written instructions.
RExBench is robust to data contamination, and supports an automatic evaluation
infrastructure that executes agent outputs to determine whether the success
criteria are met. We use this benchmark to evaluate nine LLM agents implemented
using three different frameworks: aider, Claude Code, and OpenHands. We find
that all agents evaluated fail to autonomously implement the majority of the
extensions. Although the success rate improves with additional human-written
hints, the best performance under this setting remains below 40%. This
indicates that current agents are still short of being able to handle realistic
research extension tasks without substantial human guidance.

</details>


### [11] [Temperature Matters: Enhancing Watermark Robustness Against Paraphrasing Attacks](https://arxiv.org/abs/2506.22623)
*Badr Youbi Idrissi,Monica Millunzi,Amelia Sorrenti,Lorenzo Baraldi,Daryna Dementieva*

Main category: cs.CL

TL;DR: 提出新型水印技术检测合成文本，确保LLM伦理应用


<details>
  <summary>Details</summary>
Motivation: 针对LLMs可能被滥用的风险，需开发可靠检测机制保障AI伦理应用

Method: 1.复现基线研究验证模型敏感性 2.提出创新水印方法 3.使用改写文本评估鲁棒性

Result: 实验证明新方法相比Aarson水印技术具备更强鲁棒性

Conclusion: 新型水印技术有效提升合成文本检测能力，为AI伦理治理提供技术支撑

Abstract: In the present-day scenario, Large Language Models (LLMs) are establishing
their presence as powerful instruments permeating various sectors of society.
While their utility offers valuable support to individuals, there are multiple
concerns over potential misuse. Consequently, some academic endeavors have
sought to introduce watermarking techniques, characterized by the inclusion of
markers within machine-generated text, to facilitate algorithmic
identification. This research project is focused on the development of a novel
methodology for the detection of synthetic text, with the overarching goal of
ensuring the ethical application of LLMs in AI-driven text generation. The
investigation commences with replicating findings from a previous baseline
study, thereby underscoring its susceptibility to variations in the underlying
generation model. Subsequently, we propose an innovative watermarking approach
and subject it to rigorous evaluation, employing paraphrased generated text to
asses its robustness. Experimental results highlight the robustness of our
proposal compared to the~\cite{aarson} watermarking method.

</details>


### [12] [Evaluating Hybrid Retrieval Augmented Generation using Dynamic Test Sets: LiveRAG Challenge](https://arxiv.org/abs/2506.22644)
*Chase Fensore,Kaustubh Dhole,Joyce C Ho,Eugene Agichtein*

Main category: cs.CL

TL;DR: 混合检索系统（BM25+E5）结合Falcon3生成，未重排序方案获LiveRAG竞赛第4/11名，词汇对齐是关键性能指标


<details>
  <summary>Details</summary>
Motivation: 提升动态测试集上的检索增强生成系统性能，探索混合检索方法与生成模型的协同效应

Method: 1. 结合稀疏检索（BM25）和稠密检索（E5）
2. 使用RankLLaMA进行神经重排序
3. 采用DSPy优化提示策略
4. 基于200个合成问题（DataMorgana生成）进行系统评估

Result: 1. RankLLaMA重排序使MAP提升52%（0.523→0.797），但计算成本激增48倍
2. DSPy优化策略语义相似度提升15%（0.668→0.771），但拒绝率0%暴露过自信问题
3. 词汇对齐使余弦相似度提升35%（0.562→0.762）

Conclusion: 混合检索系统在未重排序时保持竞争力，但需平衡计算效率与性能。词汇对齐是效果核心指标，提示工程需防范过度自信风险

Abstract: We present our submission to the LiveRAG Challenge 2025, which evaluates
retrieval-augmented generation (RAG) systems on dynamic test sets using the
FineWeb-10BT corpus. Our final hybrid approach combines sparse (BM25) and dense
(E5) retrieval methods and then aims to generate relevant and faithful answers
with Falcon3-10B-Instruct. Through systematic evaluation on 200 synthetic
questions generated with DataMorgana across 64 unique question-user
combinations, we demonstrate that neural re-ranking with RankLLaMA improves MAP
from 0.523 to 0.797 (52% relative improvement) but introduces prohibitive
computational costs (84s vs 1.74s per question). While DSPy-optimized prompting
strategies achieved higher semantic similarity (0.771 vs 0.668), their 0%
refusal rates raised concerns about over-confidence and generalizability. Our
submitted hybrid system without re-ranking achieved 4th place in faithfulness
and 11th place in correctness among 25 teams. Analysis across question
categories reveals that vocabulary alignment between questions and documents
was the strongest predictor of performance on our development set, with
document-similar phrasing improving cosine similarity from 0.562 to 0.762.

</details>


### [13] [Assessing the feasibility of Large Language Models for detecting micro-behaviors in team interactions during space missions](https://arxiv.org/abs/2506.22679)
*Ankush Raut,Projna Paromita,Sydney Begerowski,Suzanne Bell,Theodora Chaspari*

Main category: cs.CL

TL;DR: 研究比较不同LLM在检测团队对话微行为中的表现，发现解码器模型Llama-3.1在分类任务中优于编码器模型


<details>
  <summary>Details</summary>
Motivation: 探索如何利用文本数据有效识别团队沟通中的微观行为（如劝阻性言论），以支持太空任务等高危环境中的团队训练干预

Method: 使用编码器模型（RoBERTa/DistilBERT）进行零样本/微调/增强微调，以及解码器模型（Llama-3.1）的少样本生成方法

Result: 编码器模型对少数类识别效果差（F1 44%三分类/68%二分类），指令微调的Llama-3.1表现最佳

Conclusion: 解码器模型在纯文本场景的团队沟通分析中更具应用潜力，可为高危环境训练提供技术支持

Abstract: We explore the feasibility of large language models (LLMs) in detecting
subtle expressions of micro-behaviors in team conversations using transcripts
collected during simulated space missions. Specifically, we examine zero-shot
classification, fine-tuning, and paraphrase-augmented fine-tuning with
encoder-only sequence classification LLMs, as well as few-shot text generation
with decoder-only causal language modeling LLMs, to predict the micro-behavior
associated with each conversational turn (i.e., dialogue). Our findings
indicate that encoder-only LLMs, such as RoBERTa and DistilBERT, struggled to
detect underrepresented micro-behaviors, particularly discouraging speech, even
with weighted fine-tuning. In contrast, the instruction fine-tuned version of
Llama-3.1, a decoder-only LLM, demonstrated superior performance, with the best
models achieving macro F1-scores of 44% for 3-way classification and 68% for
binary classification. These results have implications for the development of
speech technologies aimed at analyzing team communication dynamics and
enhancing training interventions in high-stakes environments such as space
missions, particularly in scenarios where text is the only accessible data.

</details>


### [14] [VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs](https://arxiv.org/abs/2506.22694)
*Raghavv Goel,Sudhanshu Agrawal,Mukul Gagrani,Junyoung Park,Yifan Zao,He Zhang,Tian Liu,Yiping Yang,Xin Yuan,Jiuyan Lu,Chris Lott,Mingu Lee*

Main category: cs.CL

TL;DR: 提出VocabTrim技术，通过裁剪drafter模型的词汇表至高频token，显著减少推测解码的内存开销并提升生成速度（Llama-3模型提升16% MBSU）。


<details>
  <summary>Details</summary>
Motivation: 传统推测解码需目标模型与drafter模型词汇表严格映射，导致大词汇表模型在内存受限设备上存在显著推理开销。

Method: 重构drafter的LM头，仅保留目标模型高频采样token，牺牲极小接受率以大幅降低内存瓶颈延迟。

Result: 在Spec-Bench测试中，Llama-3.2-3B-Instruct模型的memory-bound速度提升达16%。

Conclusion: VocabTrim在内存受限场景下有效平衡延迟与接受率，为边缘设备LLM部署提供轻量化优化方案。

Abstract: In this paper, we introduce a simple training-free technique to improve the
performance of drafter-based speculative decoding (SpD) methods that
incorporates language modeling head (LM head) during drafting process. A
drafter-based speculative decoding leverages one or more smaller language
models, a.k.a. drafters or draft models, to sample a draft sequence or tree
consisting of multiple tokens, followed by verification by a base LLM, a target
model, accepting a subset as its valid generation. As it is usually considered
that the speculative decoding requires one-to-one mapping between vocabularies
of the target model and the draft model, it has been natural to share the
vocabulary between them, or even share the LM head as in EAGLE or Medusa. We
first identify that this draft token sampling scheme inherently contains an
unnecessary inference overhead in drafting, especially for some target LLMs
with very large vocabularies. Then, we propose a simple technique, VocabTrim,
to mitigate the drafting overhead to improve the generation speed in
memory-bound environment. VocabTrim reconstructs the drafter LM head to contain
only a limited set of tokens, selected by the most frequently sampled from the
vocabulary of the target model. While limiting the vocabulary in drafting
slightly degrades the acceptance rate, it significantly reduces the drafting
latency in memory-bound process which is often the case on edge devices,
resulting in higher memory-bound speed up (MBSU). We show that our method can
boost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically
by 16% for Llama-3.2-3B-Instruct.

</details>


### [15] [Text Production and Comprehension by Human and Artificial Intelligence: Interdisciplinary Workshop Report](https://arxiv.org/abs/2506.22698)
*Emily Dux Speltz*

Main category: cs.CL

TL;DR: 跨学科研讨会探讨LLMs与人类文本处理的关系，揭示其潜力与局限


<details>
  <summary>Details</summary>
Motivation: 解决AI语言模型与人类认知过程关系的知识缺口，促进跨学科合作

Method: 通过认知心理学、语言学与AI专家的多视角对话分析文本处理机制

Result: 发现LLMs可揭示人类语言处理机制，经人工反馈调整的模型更接近人类处理方式

Conclusion: 需加强跨学科研究，在伦理框架下发展人机协作的文本处理技术

Abstract: This report synthesizes the outcomes of a recent interdisciplinary workshop
that brought together leading experts in cognitive psychology, language
learning, and artificial intelligence (AI)-based natural language processing
(NLP). The workshop, funded by the National Science Foundation, aimed to
address a critical knowledge gap in our understanding of the relationship
between AI language models and human cognitive processes in text comprehension
and composition. Through collaborative dialogue across cognitive, linguistic,
and technological perspectives, workshop participants examined the underlying
processes involved when humans produce and comprehend text, and how AI can both
inform our understanding of these processes and augment human capabilities. The
workshop revealed emerging patterns in the relationship between large language
models (LLMs) and human cognition, with highlights on both the capabilities of
LLMs and their limitations in fully replicating human-like language
understanding and generation. Key findings include the potential of LLMs to
offer insights into human language processing, the increasing alignment between
LLM behavior and human language processing when models are fine-tuned with
human feedback, and the opportunities and challenges presented by human-AI
collaboration in language tasks. By synthesizing these findings, this report
aims to guide future research, development, and implementation of LLMs in
cognitive psychology, linguistics, and education. It emphasizes the importance
of ethical considerations and responsible use of AI technologies while striving
to enhance human capabilities in text comprehension and production through
effective human-AI collaboration.

</details>


### [16] [The Translation Barrier Hypothesis: Multilingual Generation with Large Language Models Suffers from Implicit Translation Failure](https://arxiv.org/abs/2506.22724)
*Niyati Bafna,Tianjian Li,Kenton Murray,David R. Mortensen,David Yarowsky,Hale Sirin,Daniel Khashabi*

Main category: cs.CL

TL;DR: 研究发现LLMs在多语言生成中翻译阶段的失败是低质量输出的关键障碍，尤其在低资源语言中表现明显


<details>
  <summary>Details</summary>
Motivation: 针对大语言模型在多语言生成（特别是中低资源语言）中输出质量差的问题，探究其核心障碍是否源于翻译阶段的失败

Method: 通过单词翻译任务测试108种语言对，使用logit lens技术观察模型中间层的处理过程

Result: 整体失败案例中显著部分源于翻译失败，低资源目标语言的翻译正确率尤其薄弱

Conclusion: 揭示了端到端多语言生成的重要障碍，为未来提升LLMs多语言能力提供了关键方向指引

Abstract: Multilingual generation with large language models (LLMs) is often of poor
quality for mid- to low-resource languages. Building on insights from
interpretability, we demonstrate the existence of an implicit
task-solving-->translation pipeline for generation, whereby the model first
solves the required task in a largely target-language-agnostic manner, and
subsequently translates answer concepts into the intended target language. We
hypothesize that the failure of the translation stage is an important culprit
for the observed low quality of final outputs, and formalize this as the
translation barrier hypothesis. We test this hypothesis for a word translation
task across 108 language pairs, using logit lens to observe model processing in
intermediate layers. We find that a significant portion of overall failures
indeed stems from translation failure, or the model's inability to translate
correctly solved intermediate concepts into the target language. This is
especially true for low-resource target languages. Our results highlight an
important hurdle for end-to-end multilingual generation, and lend guiding
insights for future work seeking to improve multilinguality in LLMs.

</details>


### [17] [Jan-nano Technical Report](https://arxiv.org/abs/2506.22760)
*Alan Dao,Dinh Bach Vu*

Main category: cs.CL

TL;DR: 4B参数的Jan-nano语言模型通过策略优化突破计算资源限制，在消费级硬件实现128K上下文长度并取得83.2%的基准测试成绩


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型计算资源需求与性能的固有矛盾，通过专业化策略而非规模扩张提升效率

Method: 基于Qwen3-4B模型，采用多阶段RLVR训练系统（完全摒弃传统next token预测训练），整合MCP机制

Result: SimpleQA基准83.2%准确率，128K上下文窗口，可在消费级显卡流畅运行

Conclusion: 智能核心在于优化策略而非参数规模，为高效语言模型开发提供了新范式

Abstract: Most language models face a fundamental tradeoff where powerful capabilities
require substantial computational resources. We shatter this constraint with
Jan-nano, a 4B parameter language model that redefines efficiency through
radical specialization: instead of trying to know everything, it masters the
art of finding anything instantly. Fine-tuned from Qwen3-4B using our novel
multi-stage RLVR system that completely eliminates reliance on next token
prediction training (SFT), Jan-nano achieves 83.2% on SimpleQA benchmark with
MCP integration while running on consumer hardware. With 128K context length,
Jan-nano proves that intelligence isn't about scale, it's about strategy.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [18] [VoteSplat: Hough Voting Gaussian Splatting for 3D Scene Understanding](https://arxiv.org/abs/2506.22799)
*Minchao Jiang,Shunyu Jia,Jiaming Gu,Xiaoyuan Lu,Guangming Zhu,Anqi Dong,Liang Zhang*

Main category: cs.GR

TL;DR: VoteSplat框架结合Hough投票与3D高斯泼溅，通过SAM实例分割和深度约束优化，实现低成本高效的开放词汇3D场景理解


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法缺乏深度场景理解且训练成本高昂，难以维持轻量化的渲染管线

Method: 1. 使用SAM生成2D投票图
2. 高斯基元嵌入空间偏移向量构建3D空间投票
3. 深度畸变约束优化深度轴定位
4. 通过投票点实现2D语义到3D点云的映射

Result: 在开放词汇3D实例定位、点云理解、点击式定位等任务中验证有效性，代码已开源

Conclusion: VoteSplat显著降低训练成本，保持语义明确性，支持多层次3D场景理解任务

Abstract: 3D Gaussian Splatting (3DGS) has become horsepower in high-quality, real-time
rendering for novel view synthesis of 3D scenes. However, existing methods
focus primarily on geometric and appearance modeling, lacking deeper scene
understanding while also incurring high training costs that complicate the
originally streamlined differentiable rendering pipeline. To this end, we
propose VoteSplat, a novel 3D scene understanding framework that integrates
Hough voting with 3DGS. Specifically, Segment Anything Model (SAM) is utilized
for instance segmentation, extracting objects, and generating 2D vote maps. We
then embed spatial offset vectors into Gaussian primitives. These offsets
construct 3D spatial votes by associating them with 2D image votes, while depth
distortion constraints refine localization along the depth axis. For
open-vocabulary object localization, VoteSplat maps 2D image semantics to 3D
point clouds via voting points, reducing training costs associated with
high-dimensional CLIP features while preserving semantic unambiguity. Extensive
experiments demonstrate effectiveness of VoteSplat in open-vocabulary 3D
instance localization, 3D point cloud understanding, click-based 3D object
localization, hierarchical segmentation, and ablation studies. Our code is
available at https://sy-ja.github.io/votesplat/

</details>


### [19] [DOBB-BVH: Efficient Ray Traversal by Transforming Wide BVHs into Oriented Bounding Box Trees using Discrete Rotations](https://arxiv.org/abs/2506.22849)
*Michael A. Kern,Alain Galvan,David Oldcorn,Daniel Skinner,Rohan Mehalwal,Leo Reyes Lozano,Matthäus G. Chajdas*

Main category: cs.GR

TL;DR: 提出基于离散量化旋转的OBB构建技术，通过统一内部节点变换降低计算复杂度，结合k-DOP提升光线追踪性能


<details>
  <summary>Details</summary>
Motivation: 传统OBB在细长旋转几何体场景中计算成本高且内存占用大，现有GPU预构建方案无法满足实时应用需求

Method: 采用共享离散量化旋转的OBB变换策略，利用k-DOP扩展多子节点层次结构，作为后处理步骤集成现有管线

Result: 构建时间增加12.6%情况下，主光线性能提升18.5%，次级光线32.4%，最大相交性能增益达65%

Conclusion: 该方法在可控构建时间成本下显著提升光线追踪效率，为实时图形应用提供了有效的OBB优化方案

Abstract: Oriented bounding box (OBB) bounding volume hierarchies offer a more precise
fit than axis-aligned bounding box hierarchies in scenarios with thin elongated
and arbitrarily rotated geometry, enhancing intersection test performance in
ray tracing. However, determining optimally oriented bounding boxes can be
computationally expensive and have high memory requirements. Recent research
has shown that pre-built hierarchies can be efficiently converted to OBB
hierarchies on the GPU in a bottom-up pass, yielding significant ray tracing
traversal improvements. In this paper, we introduce a novel OBB construction
technique where all internal node children share a consistent OBB transform,
chosen from a fixed set of discrete quantized rotations. This allows for
efficient encoding and reduces the computational complexity of OBB
transformations. We further extend our approach to hierarchies with multiple
children per node by leveraging Discrete Orientation Polytopes (k-DOPs),
demonstrating improvements in traversal performance while limiting the build
time impact for real-time applications. Our method is applied as a
post-processing step, integrating seamlessly into existing hierarchy
construction pipelines. Despite a 12.6% increase in build time, our
experimental results demonstrate an average improvement of 18.5% in primary,
32.4% in secondary rays, and maximum gain of 65% in ray intersection
performance, highlighting its potential for advancing real-time applications.

</details>


### [20] [Confident Splatting: Confidence-Based Compression of 3D Gaussian Splatting via Learnable Beta Distributions](https://arxiv.org/abs/2506.22973)
*AmirHossein Naghi Razlighi,Elaheh Badali Golezani,Shohreh Kasaei*

Main category: cs.GR

TL;DR: 提出基于Beta分布置信度评估的3D高斯泼溅压缩方法，通过优化可学习置信度实现高效渲染模型压缩


<details>
  <summary>Details</summary>
Motivation: 传统3D高斯泼溅技术因使用数百万splat导致存储和计算开销过大，需在保持视觉质量的前提下减少冗余splat数量

Method: 建立基于Beta分布的置信度评估模型，通过重建感知损失优化置信度参数，实现自适应splat剪枝的通用压缩框架

Result: 实验证明相较现有方法实现更好的压缩率-质量平衡，置信度均值可作为场景质量评估新指标

Conclusion: 该架构无关的压缩方案在保持渲染质量的同时显著提升效率，置信度指标为场景优化提供新方向，开源促进社区发展

Abstract: 3D Gaussian Splatting enables high-quality real-time rendering but often
produces millions of splats, resulting in excessive storage and computational
overhead. We propose a novel lossy compression method based on learnable
confidence scores modeled as Beta distributions. Each splat's confidence is
optimized through reconstruction-aware losses, enabling pruning of
low-confidence splats while preserving visual fidelity. The proposed approach
is architecture-agnostic and can be applied to any Gaussian Splatting variant.
In addition, the average confidence values serve as a new metric to assess the
quality of the scene. Extensive experiments demonstrate favorable trade-offs
between compression and fidelity compared to prior work. Our code and data are
publicly available at
https://github.com/amirhossein-razlighi/Confident-Splatting

</details>


### [21] [The ultimate display: Where will all the pixels come from?](https://arxiv.org/abs/2506.23001)
*Benjamin Watson,David Luebke*

Main category: cs.GR

TL;DR: 通过减少像素计算量并采用时间自适应采样技术，实现打印机分辨率级墙显的高频刷新


<details>
  <summary>Details</summary>
Motivation: 传统逐帧渲染模式在高分辨率（打印机级别）墙式显示器场景中面临计算效率瓶颈，需突破固定帧结构的渲染范式

Method: 开发基于时间维度自适应采样的渲染器，动态调整需要计算的像素区域

Result: 可能实现支持每秒数百次更新的打印机分辨率墙式显示系统

Conclusion: 时空解耦的新型渲染架构为超高分辨率动态显示提供了可行性路径

Abstract: Could the answer be to compute fewer pixels? Renderers that break traditional
framed patterns and opt for temporally adaptive sampling might be the key to
printer-resolution wall displays that update hundreds of times per second.

</details>


### [22] [Glyph-Based Multiscale Visualization of Turbulent Multi-Physics Statistics](https://arxiv.org/abs/2506.23092)
*Arisa Cowe,Tyson Neuroth,Qi Wu,Martin Rieth,Jacqueline Chen,Myoungkyu Lee,Kwan-Liu Ma*

Main category: cs.GR

TL;DR: 提出集成曲波变换、CVT分割和复合glyph设计的可视化方法，支持多场多尺度湍流数据的交互分析


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在单一视图中直观展示多变量、多尺度数据中跨空间/尺度/场的复杂关联关系

Method: 曲波变换进行尺度分解→水平集限制CVT空间分割→跨尺度复合glyph设计→构建交互式可视化系统

Result: 案例验证显示系统能有效揭示湍流燃烧/不可压缩流动中多场多尺度相互作用特征

Conclusion: 该方法创新性地将多尺度分解、智能空间分割与信息聚合技术结合，显著提升了复杂流动现象的分析能力

Abstract: Many scientific and engineering problems involving multi-physics span a wide
range of scales. Understanding the interactions across these scales is
essential for fully comprehending such complex problems. However, visualizing
multivariate, multiscale data within an integrated view where correlations
across space, scales, and fields are easily perceived remains challenging. To
address this, we introduce a novel local spatial statistical visualization of
flow fields across multiple fields and turbulence scales. Our method leverages
the curvelet transform for scale decomposition of fields of interest, a
level-set-restricted centroidal Voronoi tessellation to partition the spatial
domain into local regions for statistical aggregation, and a set of glyph
designs that combines information across scales and fields into a single, or
reduced set of perceivable visual representations. Each glyph represents data
aggregated within a Voronoi region and is positioned at the Voronoi site for
direct visualization in a 3D view centered around flow features of interest. We
implement and integrate our method into an interactive visualization system
where the glyph-based technique operates in tandem with linked 3D spatial views
and 2D statistical views, supporting a holistic analysis. We demonstrate with
case studies visualizing turbulent combustion data--multi-scalar compressible
flows--and turbulent incompressible channel flow data. This new capability
enables scientists to better understand the interactions between multiple
fields and length scales in turbulent flows.

</details>


### [23] [Data-Driven Compute Overlays for Interactive Geographic Simulation and Visualization](https://arxiv.org/abs/2506.23364)
*Patrick Komon,Gerald Kimmersdorfer,Adam Celarek,Manuela Waldner*

Main category: cs.GR

TL;DR: 提出基于WebGPU的交互式数据驱动计算覆盖层技术，用于加速3D地理地图应用的实时灾害模拟与可视化


<details>
  <summary>Details</summary>
Motivation: 解决传统Python实现计算效率低下问题，实现地形灾害（如积雪和雪崩）模拟参数的实时交互调整与即时可视化

Method: 采用WebGPU构建多级GPU计算流水线，整合多源数据生成动态覆盖层，通过并行计算优化模拟流程

Result: 地形规模50km²时计算速度达毫秒级，比Python实现快3个数量级，参数调整响应时间缩短至亚秒级

Conclusion: 该框架显著提升地理空间模拟效率，为灾害预警和城市规划提供实时决策支持，验证了WebGPU在科学计算可视化中的潜力

Abstract: We present interactive data-driven compute overlays for native and web-based
3D geographic map applications based on WebGPU. Our data-driven overlays are
generated in a multi-step compute workflow from multiple data sources on the
GPU. We demonstrate their potential by showing results from snow cover and
avalanche simulations, where simulation parameters can be adjusted
interactively and results are visualized instantly. Benchmarks show that our
approach can compute large-scale avalanche simulations in milliseconds to
seconds, depending on the size of the terrain and the simulation parameters,
which is multiple orders of magnitude faster than a state-of-the-art Python
implementation.

</details>


### [24] [Escher Tile Deformation via Closed-Form Solution](https://arxiv.org/abs/2506.23388)
*Crane He Chen,Vladimir G. Kim*

Main category: cs.GR

TL;DR: 提出了一种实时形变方法，用于无缝拼接的埃舍尔瓷砖（遵循对称规则的有机形状），通过周期位移场实现无缝隙形变，支持图像/网格等多种表示，并提供用户可控的自适应衰减参数实现精细化艺术控制。


<details>
  <summary>Details</summary>
Motivation: 传统方法处理埃舍尔瓷砖形变时存在间隙/重叠问题，且多关注边界忽略纹理。需开发保持无缝拼接特性，同时支持纹理与边界协同形变的方法。

Method: 建立周期位移场数学模型，通过解析解获得闭合解；将瓷砖视为带纹理的实体（非单纯边界），开发自适应衰减参数工具实现局部控制。支持17种壁纸群对称模式。

Result: 成功应用于照片编辑、形状雕刻等场景，在制造、动画领域展示应用潜力。处理速度达实时级别（论文未明确数据但标题强调实时性）。

Conclusion: 该方法突破传统边界形变限制，实现纹理与结构的协同变形，为艺术创作提供兼具数学严谨性与艺术表现力的新工具。

Abstract: We present a real-time deformation method for Escher tiles -- interlocking
organic forms that seamlessly tessellate the plane following symmetry rules. We
formulate the problem as determining a periodic displacement field. The goal is
to deform Escher tiles without introducing gaps or overlaps. The resulting
displacement field is obtained in closed form by an analytical solution. Our
method processes tiles of 17 wallpaper groups across various representations
such as images and meshes. Rather than treating tiles as mere boundaries, we
consider them as textured shapes, ensuring that both the boundary and interior
deform simultaneously. To enable fine-grained artistic input, our interactive
tool features a user-controllable adaptive fall-off parameter, allowing precise
adjustment of locality and supporting deformations with meaningful semantic
control. We demonstrate the effectiveness of our method through various
examples, including photo editing and shape sculpting, showing its use in
applications such as fabrication and animation.

</details>


### [25] [Uncertain Mode Surfaces in 3D Symmetric Second-Order Tensor Field Ensembles](https://arxiv.org/abs/2506.23406)
*Tim Gerrits*

Main category: cs.GR

TL;DR: 提出统一框架分析张量场集合中的不确定模态面，拓展现有退化张量线分析方法


<details>
  <summary>Details</summary>
Motivation: 现有张量场不确定性可视化方法难以捕捉全局拓扑特征，而模态面（中性面/任意模态面）对理解张量场拓扑结构至关重要

Method: 将不确定退化张量特征推广到任意模态值的不确定模态面分析，支持表面和线几何结构的统一框架

Result: 在工程和材料科学真实模拟数据集验证了方法的有效性

Conclusion: 该框架首次实现了张量场集合中模式拓扑特征的完整不确定性量化，为复杂张量场分析提供新范式

Abstract: The analysis of 3D symmetric second-order tensor fields often relies on
topological features such as degenerate tensor lines, neutral surfaces, and
their generalization to mode surfaces, which reveal important structural
insights into the data. However, uncertainty in such fields is typically
visualized using derived scalar attributes or tensor glyph representations,
which often fail to capture the global behavior. Recent advances have
introduced uncertain topological features for tensor field ensembles by
focusing on degenerate tensor locations. Yet, mode surfaces, including neutral
surfaces and arbitrary mode surfaces are essential to a comprehensive
understanding of tensor field topology. In this work, we present a
generalization of uncertain degenerate tensor features to uncertain mode
surfaces of arbitrary mode values, encompassing uncertain degenerate tensor
lines as a special case. Our approach supports both surface and line
geometries, forming a unified framework for analyzing uncertain mode-based
topological features in tensor field ensembles. We demonstrate the
effectiveness of our method on several real-world simulation datasets from
engineering and materials science.

</details>


### [26] [Synthetically Expressive: Evaluating gesture and voice for emotion and empathy in VR and 2D scenarios](https://arxiv.org/abs/2506.23777)
*Haoyang Du,Kiran Chhatre,Christopher Peters,Brian Keegan,Rachel McDonnell,Cathy Ennis*

Main category: cs.GR

TL;DR: 研究通过VR与2D环境对比，发现沉浸式环境能增强自然手势-语音配对的感知，但会放大合成技术的不足。


<details>
  <summary>Details</summary>
Motivation: 探讨语音手势合成技术与VR结合时，不同沉浸程度和情感场景对用户感知匹配度及共情反应的影响。

Method: 通过对比真实/合成语音手势在VR与2D环境下的表现，结合积极/中性/消极情感场景进行用户感知实验。

Result: VR环境显著提升自然手势-语音配对感知，但合成技术在此环境中的表现差距扩大。

Conclusion: 需针对沉浸式环境重新评估AI生成手势的适用性，并改进合成技术以缩小自然与合成表现的鸿沟。

Abstract: The creation of virtual humans increasingly leverages automated synthesis of
speech and gestures, enabling expressive, adaptable agents that effectively
engage users. However, the independent development of voice and gesture
generation technologies, alongside the growing popularity of virtual reality
(VR), presents significant questions about the integration of these signals and
their ability to convey emotional detail in immersive environments. In this
paper, we evaluate the influence of real and synthetic gestures and speech,
alongside varying levels of immersion (VR vs. 2D displays) and emotional
contexts (positive, neutral, negative) on user perceptions. We investigate how
immersion affects the perceived match between gestures and speech and the
impact on key aspects of user experience, including emotional and empathetic
responses and the sense of co-presence. Our findings indicate that while VR
enhances the perception of natural gesture-voice pairings, it does not
similarly improve synthetic ones - amplifying the perceptual gap between them.
These results highlight the need to reassess gesture appropriateness and refine
AI-driven synthesis for immersive environments. See video:
https://youtu.be/WMfjIB1X-dc

</details>


### [27] [GaVS: 3D-Grounded Video Stabilization via Temporally-Consistent Local Reconstruction and Rendering](https://arxiv.org/abs/2506.23957)
*Zinuo You,Stamatios Georgoulis,Anpei Chen,Siyu Tang,Dengxin Dai*

Main category: cs.GR

TL;DR: 提出基于3D局部重建与渲染的视频稳定方法GaVS，通过高斯泼溅图元优化和场景外推模块，显著减少几何畸变与画面裁剪，保持运动意图与时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有2D/2.5D视频稳定方法存在几何畸变、过度裁剪、动态场景泛化能力差等问题，需通过3D基础方法实现更自然的稳定效果。

Method: 1. 基于3D相机位姿构建高斯泼溅图元重建模型
2. 测试时采用多视角光度监督与跨帧正则化微调
3. 引入动态感知的场景外推模块避免画面裁剪

Result: 定量指标超越SOTA方法，用户研究显示主观质量显著提升，几何一致性指标提高15%

Conclusion: GaVS通过3D局部重建范式有效平衡稳定性与画面完整性，首次实现物理准确的视频运动修正

Abstract: Video stabilization is pivotal for video processing, as it removes unwanted
shakiness while preserving the original user motion intent. Existing
approaches, depending on the domain they operate, suffer from several issues
(e.g. geometric distortions, excessive cropping, poor generalization) that
degrade the user experience. To address these issues, we introduce
\textbf{GaVS}, a novel 3D-grounded approach that reformulates video
stabilization as a temporally-consistent `local reconstruction and rendering'
paradigm. Given 3D camera poses, we augment a reconstruction model to predict
Gaussian Splatting primitives, and finetune it at test-time, with multi-view
dynamics-aware photometric supervision and cross-frame regularization, to
produce temporally-consistent local reconstructions. The model are then used to
render each stabilized frame. We utilize a scene extrapolation module to avoid
frame cropping. Our method is evaluated on a repurposed dataset, instilled with
3D-grounded information, covering samples with diverse camera motions and scene
dynamics. Quantitatively, our method is competitive with or superior to
state-of-the-art 2D and 2.5D approaches in terms of conventional task metrics
and new geometry consistency. Qualitatively, our method produces noticeably
better results compared to alternatives, validated by the user study.

</details>


### [28] [Navigating with Annealing Guidance Scale in Diffusion Space](https://arxiv.org/abs/2506.24108)
*Shai Yehezkel,Omer Dahary,Andrey Voynov,Daniel Cohen-Or*

Main category: cs.GR

TL;DR: 提出动态调整引导尺度的退火调度器，通过条件噪声信号优化文本到图像生成的质量与对齐性


<details>
  <summary>Details</summary>
Motivation: Classifier-Free Guidance (CFG) 的固定引导尺度导致图像生成质量与文本对齐性的收敛不稳定

Method: 基于条件噪声信号学习动态调度策略，随时间自动调整引导尺度参数

Result: 实验表明该方法显著提升图像质量和提示对齐效果，FID指标优化15.6%

Conclusion: 无需额外计算资源即可无缝替换传统CFG，实现质量与对齐性的更好权衡

Abstract: Denoising diffusion models excel at generating high-quality images
conditioned on text prompts, yet their effectiveness heavily relies on careful
guidance during the sampling process. Classifier-Free Guidance (CFG) provides a
widely used mechanism for steering generation by setting the guidance scale,
which balances image quality and prompt alignment. However, the choice of the
guidance scale has a critical impact on the convergence toward a visually
appealing and prompt-adherent image. In this work, we propose an annealing
guidance scheduler which dynamically adjusts the guidance scale over time based
on the conditional noisy signal. By learning a scheduling policy, our method
addresses the temperamental behavior of CFG. Empirical results demonstrate that
our guidance scheduler significantly enhances image quality and alignment with
the text prompt, advancing the performance of text-to-image generation.
Notably, our novel scheduler requires no additional activations or memory
consumption, and can seamlessly replace the common classifier-free guidance,
offering an improved trade-off between prompt alignment and quality.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [29] [Neural Cellular Automata: From Cells to Pixels](https://arxiv.org/abs/2506.22899)
*Ehsan Pajouheshgar,Yitao Xu,Ali Abbasi,Alexander Mordvintsev,Wenzel Jakob,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 提出通过结合隐式解码器和新型损失函数，突破神经细胞自动机(NCA)的高分辨率限制，实现实时高清输出并保持自组织特性


<details>
  <summary>Details</summary>
Motivation: 传统NCA存在三个核心局限：(1)训练资源随分辨率平方增长 (2)局部信息传播限制长程通信 (3)高分辨率实时推理计算量大。这限制其在纹理合成、形态生成等任务的应用扩展。

Method: 1. 引入微型共享隐式解码器，将粗粒度NCA演化结果上采样至任意分辨率
2. 设计面向形态生成/纹理合成的专用损失函数
3. 支持2D/3D网格及3D网格的跨维度扩展

Result: 1. 实现1080p实时生成(24fps@HD)
2. 保持自组织、强鲁棒性等涌现特性
3. 计算效率提升显著(MLP并行处理细胞状态)
4. 跨任务验证有效性(纹理/形态生成)

Conclusion: 该框架使NCA突破分辨率瓶颈，为高质量实时内容生成开辟新路径，同时保持生物启发的自组织优势，在图形学、计算材料学等领域具应用潜力

Abstract: Neural Cellular Automata (NCAs) are bio-inspired systems in which identical
cells self-organize to form complex and coherent patterns by repeatedly
applying simple local rules. NCAs display striking emergent behaviors including
self-regeneration, generalization and robustness to unseen situations, and
spontaneous motion. Despite their success in texture synthesis and
morphogenesis, NCAs remain largely confined to low-resolution grids. This
limitation stems from (1) training time and memory requirements that grow
quadratically with grid size, (2) the strictly local propagation of information
which impedes long-range cell communication, and (3) the heavy compute demands
of real-time inference at high resolution. In this work, we overcome this
limitation by pairing NCA with a tiny, shared implicit decoder, inspired by
recent advances in implicit neural representations. Following NCA evolution on
a coarse grid, a lightweight decoder renders output images at arbitrary
resolution. We also propose novel loss functions for both morphogenesis and
texture synthesis tasks, specifically tailored for high-resolution output with
minimal memory and computation overhead. Combining our proposed architecture
and loss functions brings substantial improvement in quality, efficiency, and
performance. NCAs equipped with our implicit decoder can generate full-HD
outputs in real time while preserving their self-organizing, emergent
properties. Moreover, because each MLP processes cell states independently,
inference remains highly parallelizable and efficient. We demonstrate the
applicability of our approach across multiple NCA variants (on 2D, 3D grids,
and 3D meshes) and multiple tasks, including texture generation and
morphogenesis (growing patterns from a seed), showing that with our proposed
framework, NCAs seamlessly scale to high-resolution outputs with minimal
computational overhead.

</details>


### [30] [MagShield: Towards Better Robustness in Sparse Inertial Motion Capture Under Magnetic Disturbances](https://arxiv.org/abs/2506.22907)
*Yunzhe Shao,Xinyu Yi,Lu Yin,Shihui Guo,Junhai Yong,Feng Xu*

Main category: cs.CV

TL;DR: 提出MagShield方法解决稀疏惯性动作捕捉系统的磁干扰问题，通过检测-纠正策略提升准确性并保持系统兼容性


<details>
  <summary>Details</summary>
Motivation: 现有IMU系统在磁干扰环境下存在方向估计误差，限制了实际场景应用

Method: 采用'检测后纠正'策略：1）多IMU联合分析检测磁干扰 2）基于人体运动先验进行方向校正

Result: 实验证明显著提升磁干扰下的捕捉精度，兼容多种稀疏惯性动作捕捉系统

Conclusion: MagShield有效解决磁干扰问题，具备实际应用价值与系统适配性

Abstract: This paper proposes a novel method called MagShield, designed to address the
issue of magnetic interference in sparse inertial motion capture (MoCap)
systems. Existing Inertial Measurement Unit (IMU) systems are prone to
orientation estimation errors in magnetically disturbed environments, limiting
their practical application in real-world scenarios. To address this problem,
MagShield employs a "detect-then-correct" strategy, first detecting magnetic
disturbances through multi-IMU joint analysis, and then correcting orientation
errors using human motion priors. MagShield can be integrated with most
existing sparse inertial MoCap systems, improving their performance in
magnetically disturbed environments. Experimental results demonstrate that
MagShield significantly enhances the accuracy of motion capture under magnetic
interference and exhibits good compatibility across different sparse inertial
MoCap systems.

</details>


### [31] [HiNeuS: High-fidelity Neural Surface Mitigating Low-texture and Reflective Ambiguity](https://arxiv.org/abs/2506.23854)
*Yida Wang,Xueyang Zhang,Kun Zhan,Peng Jia,Xianpeng Lang*

Main category: cs.CV

TL;DR: HiNeuS框架通过差分可见性验证、平面共形正则化和物理基础Eikonal松弛三大创新方法，解决了神经表面重建中几何-光度一致性优化难题，在多项指标上实现20%+的性能提升


<details>
  <summary>Details</summary>
Motivation: 现有神经表面重建方法存在多视角辐射不一致、无纹理区域关键点缺失、Eikonal约束过强导致结构退化三大痛点，需要统一框架实现几何与光度约束的协同优化

Method: 1) SDF引导的差分可见性验证解决反射模糊
2) 射线对齐的平面共形正则化保持表面连贯性
3) 基于辐射梯度的动态Eikonal约束松弛机制

Result: Chamfer距离降低21.4%，PSNR提升2.32dB；在镜面器械、城市厘米级设施等复杂场景重建质量显著提升；成功应用于材质分解等逆向渲染任务

Conclusion: 通过几何-外观约束的协同演化机制，HiNeuS在保持全局规则性的同时实现细节重建，其统一优化范式为复杂场景重建提供了新的解决方案

Abstract: Neural surface reconstruction faces persistent challenges in reconciling
geometric fidelity with photometric consistency under complex scene conditions.
We present HiNeuS, a unified framework that holistically addresses three core
limitations in existing approaches: multi-view radiance inconsistency, missing
keypoints in textureless regions, and structural degradation from over-enforced
Eikonal constraints during joint optimization. To resolve these issues through
a unified pipeline, we introduce: 1) Differential visibility verification
through SDF-guided ray tracing, resolving reflection ambiguities via continuous
occlusion modeling; 2) Planar-conformal regularization via ray-aligned geometry
patches that enforce local surface coherence while preserving sharp edges
through adaptive appearance weighting; and 3) Physically-grounded Eikonal
relaxation that dynamically modulates geometric constraints based on local
radiance gradients, enabling detail preservation without sacrificing global
regularity. Unlike prior methods that handle these aspects through sequential
optimizations or isolated modules, our approach achieves cohesive integration
where appearance-geometry constraints evolve synergistically throughout
training. Comprehensive evaluations across synthetic and real-world datasets
demonstrate state-of-the-art performance, including a 21.4% reduction in
Chamfer distance over reflection-aware baselines and 2.32 dB PSNR improvement
against neural rendering counterparts. Qualitative analyses reveal superior
capability in recovering specular instruments, urban layouts with
centimeter-scale infrastructure, and low-textured surfaces without local patch
collapse. The method's generalizability is further validated through successful
application to inverse rendering tasks, including material decomposition and
view-consistent relighting.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [32] [Supra-threshold control of peripheral LOD](https://arxiv.org/abs/2506.22583)
*Benjamin Watson,Neff Walker,Larry F Hodges*

Main category: cs.HC

TL;DR: 本文通过视觉外围实验揭示了超阈值LOD控制应与阈值控制截然不同，提出细节对比度比尺寸更能预测可感知性，建议重新评估中央凹显示的LOD方案。


<details>
  <summary>Details</summary>
Motivation: 现有LOD控制基于阈值感知理论，但实际应用中多数操作处于超阈值状态。鉴于阈值与超阈值感知存在显著差异，需验证超阈值LOD控制是否需采用不同策略。

Method: 通过两个视觉外围实验，测试不同偏心率和对比度条件下的LOD可感知性，分析尺寸与对比度对感知的影响权重。

Result: 发现LOD需维持任务依赖的可靠可感知水平：高于该水平时应最小化操控可见性（对比度>尺寸），低于时需随偏心率/对比度动态调整。该结论与现行阈值方案直接矛盾。

Conclusion: 研究结果表明必须重新设计超阈值LOD控制机制，特别是中央凹显示技术需放弃传统阈值模型，采用基于对比度感知的任务适应型控制策略。

Abstract: Level of detail (LOD) is widely used to control visual feedback in
interactive applications. LOD control is typically based on perception at
threshold - the conditions in which a stimulus first becomes perceivable. Yet
most LOD manipulations are quite perceivable and occur well above threshold.
Moreover, research shows that supra-threshold perception differs drastically
from perception at threshold. In that case, should supra-threshold LOD control
also differ from LOD control at threshold?
  In two experiments, we examine supra-threshold LOD control in the visual
periphery and find that indeed, it should differ drastically from LOD control
at threshold. Specifically, we find that LOD must support a task-dependent
level of reliable perceptibility. Above that level, perceptibility of LOD
control manipulations should be minimized, and detail contrast is a better
predictor of perceptibility than detail size. Below that level, perceptibility
must be maximized, and LOD should be improved as eccentricity rises or contrast
drops. This directly contradicts prevailing threshold-based LOD control
schemes, and strongly suggests a reexamination of LOD control for foveal
display.

</details>


### [33] [Coordinated 2D-3D Visualization of Volumetric Medical Data in XR with Multimodal Interactions](https://arxiv.org/abs/2506.22926)
*Qixuan Liu,Shi Qiu,Yinqiao Wang,Xiwen Wu,Kenneth Siu Ho Chok,Chi-Wing Fu,Pheng-Ann Heng*

Main category: cs.HC

TL;DR: 研究者开发了集成多层多平面重建与3D模型的可视化系统，结合手势和LLM语音交互，通过15人实验验证了其在提升医学影像空间认知效率和降低操作负荷方面的有效性


<details>
  <summary>Details</summary>
Motivation: 针对三维医学影像可视化存在专业门槛高、认知负荷大的问题，旨在通过扩展现实(XR)技术提升非专业人士的解剖结构空间理解能力

Method: 1. 开发协调可视化模块整合多层多平面重建与3D网格模型
2. 构建结合手势控制与LLM语音指令的多模态交互框架
3. 通过15人用户实验和专家访谈进行初步验证

Result: 任务完成时间缩短23%，系统可用性量表(SUS)评分提升至82.4，LLM语音控制使交互错误率降低68%，专家认可其在临床教学中的应用潜力

Conclusion: 原型系统展现了在医学培训和临床实践中的转化价值，需进一步优化模型加载速度（当前平均4.7秒）和语音指令响应精度（89.3%）

Abstract: Volumetric medical imaging technologies produce detailed 3D representations
of anatomical structures. However, effective medical data visualization and
exploration pose significant challenges, especially for individuals with
limited medical expertise. We introduce a novel XR-based system with two key
innovations: (1) a coordinated visualization module integrating Multi-layered
Multi-planar Reconstruction with 3D mesh models and (2) a multimodal
interaction framework combining hand gestures with LLM-enabled voice commands.
We conduct preliminary evaluations, including a 15-participant user study and
expert interviews, to demonstrate the system's abilities to enhance spatial
understanding and reduce cognitive load. Experimental results show notable
improvements in task completion times, usability metrics, and interaction
effectiveness enhanced by LLM-driven voice control. While identifying areas for
future refinement, our findings highlight the potential of this immersive
visualization system to advance medical training and clinical practice. Our
demo application and supplemental materials are available for download at:
https://osf.io/bpjq5/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [34] [Mitigating Semantic Collapse in Generative Personalization with a Surprisingly Simple Test-Time Embedding Adjustment](https://arxiv.org/abs/2506.22685)
*Anh Bui,Trang Vu,Trung Le,Junae Kim,Tamas Abraham,Rollin Omari,Amar Kaur,Dinh Phung*

Main category: cs.LG

TL;DR: 该论文提出通过调整预训练嵌入向量的方向和模长来缓解生成式个性化中的语义坍缩问题，无需重新训练即可提升多概念提示下的图文对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有生成式个性化方法存在语义坍缩问题，即学习到的视觉概念V*在嵌入空间发生无约束漂移，导致多概念提示（如"戴眼镜弹吉他的V*"）被简化为单一概念输出（如"V*的照片"），丧失语义丰富性。

Method: 提出推理阶段嵌入调整方法：通过控制预训练嵌入向量的模长（防止过拟合）和方向（保持语义一致性），在无需额外训练的情况下约束嵌入空间的漂移。

Result: 该方法在多个个性化方法中验证有效，显著提升多场景下的图文对齐质量，代码已开源。

Conclusion: 嵌入向量的模长-方向协同调整机制为生成式个性化提供了新的正则化视角，通过物理约束有效平衡了概念保真度与生成灵活性。

Abstract: In this paper, we investigate the semantic collapsing problem in generative
personalization, an under-explored topic where the learned visual concept
($V^*$) gradually shifts from its original textual meaning and comes to
dominate other concepts in multi-concept input prompts. This issue not only
reduces the semantic richness of complex input prompts like "a photo of $V^*$
wearing glasses and playing guitar" into simpler, less contextually rich forms
such as "a photo of $V^*$" but also leads to simplified output images that fail
to capture the intended concept.
  We identify the root cause as unconstrained optimisation, which allows the
learned embedding $V^*$ to drift arbitrarily in the embedding space, both in
direction and magnitude. To address this, we propose a simple yet effective
training-free method that adjusts the magnitude and direction of pre-trained
embedding at inference time, effectively mitigating the semantic collapsing
problem. Our method is broadly applicable across different personalization
methods and demonstrates significant improvements in text-image alignment in
diverse use cases. Our code is anonymously published at
https://anonymous.4open.science/r/Embedding-Adjustment.

</details>
