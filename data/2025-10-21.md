<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 103]
- [cs.GR](#cs.GR) [Total: 10]
- [cs.LG](#cs.LG) [Total: 16]
- [cs.CV](#cs.CV) [Total: 9]
- [cs.MA](#cs.MA) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.AI](#cs.AI) [Total: 11]
- [cs.RO](#cs.RO) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.HC](#cs.HC) [Total: 4]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.SD](#cs.SD) [Total: 4]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.CR](#cs.CR) [Total: 5]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Quantum NLP models on Natural Language Inference](https://arxiv.org/abs/2510.15972)
*Ling Sun,Peter Sullivan,Michael Martin,Yun Zhou*

Main category: cs.CL

TL;DR: 量子自然语言处理（QNLP）通过量子电路建模语言结构，在少量样本场景下与经典模型表现相当，但参数效率提升5个数量级。


<details>
  <summary>Details</summary>
Motivation: 探索量子模型在低资源场景下的语义建模潜力，解决传统transformer模型参数量大、数据依赖性强的问题。

Method: 基于DisCoCat框架和lambeq库构建量子电路，设计IGPP指标量化参数效率，提出词聚类共享参数的量子架构。

Result: 量子模型仅用0.04%参数即达到transformer的基准性能，IGPP效率比经典模型高5个量级，聚类架构提升20%泛化能力。

Conclusion: QNLP在结构敏感任务中展现量子优势，参数共享机制为可扩展量子语义模型提供新路径。

Abstract: Quantum natural language processing (QNLP) offers a novel approach to
semantic modeling by embedding compositional structure directly into quantum
circuits. This paper investigates the application of QNLP models to the task of
Natural Language Inference (NLI), comparing quantum, hybrid, and classical
transformer-based models under a constrained few-shot setting. Using the lambeq
library and the DisCoCat framework, we construct parameterized quantum circuits
for sentence pairs and train them for both semantic relatedness and inference
classification. To assess efficiency, we introduce a novel
information-theoretic metric, Information Gain per Parameter (IGPP), which
quantifies learning dynamics independent of model size. Our results demonstrate
that quantum models achieve performance comparable to classical baselines while
operating with dramatically fewer parameters. The Quantum-based models
outperform randomly initialized transformers in inference and achieve lower
test error on relatedness tasks. Moreover, quantum models exhibit significantly
higher per-parameter learning efficiency (up to five orders of magnitude more
than classical counterparts), highlighting the promise of QNLP in low-resource,
structure-sensitive settings. To address circuit-level isolation and promote
parameter sharing, we also propose a novel cluster-based architecture that
improves generalization by tying gate parameters to learned word clusters
rather than individual tokens.

</details>


### [2] [Fusion-Augmented Large Language Models: Boosting Diagnostic Trustworthiness via Model Consensus](https://arxiv.org/abs/2510.16057)
*Md Kamrul Siam,Md Jobair Hossain Faruk,Jerry Q. Cheng,Huanying Gu*

Main category: cs.CL

TL;DR: 提出结合ChatGPT和Claude的多模型融合框架，通过共识机制提升胸部X光诊断准确性，单模态达77.6%，多模态达91.3%。


<details>
  <summary>Details</summary>
Motivation: 通过整合互补模态和输出级共识机制，提升AI辅助放射诊断的可信度与临床实用性，以最小计算开销减少诊断错误。

Method: 1. 单模态实验：在234例胸部X光图像上测试模型性能
2. 多模态实验：50例病例同时使用图像和合成临床文本
3. 采用相似度共识机制（95%阈值）进行模型融合

Result: 单模态：ChatGPT 62.8%、Claude 76.9%、共识77.6%
多模态：ChatGPT 84%、Claude 76%、共识91.3%
共识方法始终优于单模型

Conclusion: 多模型融合策略显著提升诊断准确性，证实整合多模态数据和共识机制在临床AI应用中的有效性，为可靠放射诊断提供实用解决方案。

Abstract: This study presents a novel multi-model fusion framework leveraging two
state-of-the-art large language models (LLMs), ChatGPT and Claude, to enhance
the reliability of chest X-ray interpretation on the CheXpert dataset. From the
full CheXpert corpus of 224,316 chest radiographs, we randomly selected 234
radiologist-annotated studies to evaluate unimodal performance using image-only
prompts. In this setting, ChatGPT and Claude achieved diagnostic accuracies of
62.8% and 76.9%, respectively. A similarity-based consensus approach, using a
95% output similarity threshold, improved accuracy to 77.6%. To assess the
impact of multimodal inputs, we then generated synthetic clinical notes
following the MIMIC-CXR template and evaluated a separate subset of 50 randomly
selected cases paired with both images and synthetic text. On this multimodal
cohort, performance improved to 84% for ChatGPT and 76% for Claude, while
consensus accuracy reached 91.3%. Across both experimental conditions,
agreement-based fusion consistently outperformed individual models. These
findings highlight the utility of integrating complementary modalities and
using output-level consensus to improve the trustworthiness and clinical
utility of AI-assisted radiological diagnosis, offering a practical path to
reduce diagnostic errors with minimal computational overhead.

</details>


### [3] [Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs](https://arxiv.org/abs/2510.16062)
*Guiyao Tie,Zenghui Yuan,Zeli Zhao,Chaoran Hu,Tianhe Gu,Ruihang Zhang,Sizhe Zhang,Junran Wu,Xiaoyue Tu,Ming Jin,Qingsong Wen,Lixing Chen,Pan Zhou,Lichao Sun*

Main category: cs.CL

TL;DR: 自我校正方法能提升LLM在复杂推理任务的表现，但效率和能力平衡仍需优化


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对LLM自我校正方法的系统性评估，且对其实际有效性存在争议

Method: 开发CorrectBench基准测试，评估内在/外部/微调三类自校正策略在常识推理/数学推理/代码生成任务的表现

Result: 1. 自校正显著提升复杂任务准确率
2. 混合策略效果更优但效率下降
3. 推理专用模型(如DeepSeek-R1)优化空间有限且耗时高

Conclusion: 需在保持推理能力与提升效率间取得平衡，建议未来研究聚焦自校正策略的优化与效率提升

Abstract: Self-correction of large language models (LLMs) emerges as a critical
component for enhancing their reasoning performance. Although various
self-correction methods have been proposed, a comprehensive evaluation of these
methods remains largely unexplored, and the question of whether LLMs can truly
correct themselves is a matter of significant interest and concern. In this
study, we introduce CorrectBench, a benchmark developed to evaluate the
effectiveness of self-correction strategies, including intrinsic, external, and
fine-tuned approaches, across three tasks: commonsense reasoning, mathematical
reasoning, and code generation. Our findings reveal that: 1) Self-correction
methods can improve accuracy, especially for complex reasoning tasks; 2) Mixing
different self-correction strategies yields further improvements, though it
reduces efficiency; 3) Reasoning LLMs (e.g., DeepSeek-R1) have limited
optimization under additional self-correction methods and have high time costs.
Interestingly, a comparatively simple chain-of-thought (CoT) baseline
demonstrates competitive accuracy and efficiency. These results underscore the
potential of self-correction to enhance LLM's reasoning performance while
highlighting the ongoing challenge of improving their efficiency. Consequently,
we advocate for further research focused on optimizing the balance between
reasoning capabilities and operational efficiency. Project Page:
https://correctbench.github.io/

</details>


### [4] [EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle](https://arxiv.org/abs/2510.16079)
*Rong Wu,Xiaoman Wang,Jianbiao Mei,Pinlong Cai,Daocheng Fu,Cheng Yang,Licheng Wen,Xuemeng Yang,Yufan Shen,Yuxin Wang,Botian Shi*

Main category: cs.CL

TL;DR: 提出EvolveR框架解决LLM代理无法系统化学习经验的问题，通过离线蒸馏和在线交互的闭环机制实现持续自我改进


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理框架仅关注外部知识缺口，缺乏迭代优化策略的核心能力，需要系统化的自我改进机制

Method: 两阶段闭环系统：1）离线将交互轨迹蒸馏为可重用策略库；2）在线结合任务检索策略指导决策，通过策略强化机制迭代更新代理

Result: 在复杂多跳问答基准测试中超越现有代理基线，验证了框架有效性

Conclusion: 建立了从自身行为后果学习的完整蓝图，为实现自主持续进化的智能系统开辟了新路径

Abstract: Current Large Language Model (LLM) agents show strong performance in tool
use, but lack the crucial capability to systematically learn from their own
experiences. While existing frameworks mainly focus on mitigating external
knowledge gaps, they fail to address a more fundamental limitation: the
inability to iteratively refine problem-solving strategies. In this work, we
introduce EvolveR, a framework designed to enable agent to self-improve through
a complete, closed-loop experience lifecycle. This lifecycle comprises two key
stages: (1) Offline Self-Distillation, where the agent's interaction
trajectories are synthesized into a structured repository of abstract, reusable
strategic principles; (2) Online Interaction, where the agent interacts with
tasks and actively retrieves distilled principles to guide its decision-making,
accumulating a diverse set of behavioral trajectories. This loop employs a
policy reinforcement mechanism to iteratively update the agent based on its
performance. We demonstrate the effectiveness of EvolveR on complex multi-hop
question-answering benchmarks, where it achieves superior performance over
strong agentic baselines. Our work presents a comprehensive blueprint for
agents that learn not only from external data but also from the consequences of
their own actions, paving the way for more autonomous and continuously
improving systems. Code is available at https://github.com/Edaizi/EvolveR.

</details>


### [5] [Evaluating Prompting Strategies and Large Language Models in Systematic Literature Review Screening: Relevance and Task-Stage Classification](https://arxiv.org/abs/2510.16091)
*Binglan Han,Anuradha Mathrani,Teo Susnjak*

Main category: cs.CL

TL;DR: 系统评估了6种大语言模型与5种提示策略在文献筛选中的交互效应，发现CoT-few-shot提示策略最可靠，并提出了分阶段部署的低成本解决方案。


<details>
  <summary>Details</summary>
Motivation: 量化不同提示策略与大语言模型在文献自动筛选中的交互作用，为任务适应性部署提供实用指南和比较基准。

Method: 使用6个LLM模型（GPT-4o/DeepSeek/Gemini等）在5种提示策略下（零样本/思维链/自反思等），通过准确率、召回率等指标评估文献相关性分类性能，并进行成本效益分析。

Result: 模型与提示策略存在显著交互效应：CoT-few-shot平衡精度与召回；自反思策略因过度包容表现欠佳；GPT-4o-mini在低成本下表现优异，分阶段部署可优化成本效益。

Conclusion: 通过系统分析提示-模型交互效应，证明LLMs在文献筛选中具有应用潜力，并为实际部署提供了任务适配的模型选择策略与成本优化方案。

Abstract: This study quantifies how prompting strategies interact with large language
models (LLMs) to automate the screening stage of systematic literature reviews
(SLRs). We evaluate six LLMs (GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3,
Gemini-2.5-Flash, Claude-3.5-Haiku, Llama-4-Maverick) under five prompt types
(zero-shot, few-shot, chain-of-thought (CoT), CoT-few-shot, self-reflection)
across relevance classification and six Level-2 tasks, using accuracy,
precision, recall, and F1. Results show pronounced model-prompt interaction
effects: CoT-few-shot yields the most reliable precision-recall balance;
zero-shot maximizes recall for high-sensitivity passes; and self-reflection
underperforms due to over-inclusivity and instability across models. GPT-4o and
DeepSeek provide robust overall performance, while GPT-4o-mini performs
competitively at a substantially lower dollar cost. A cost-performance analysis
for relevance classification (per 1,000 abstracts) reveals large absolute
differences among model-prompt pairings; GPT-4o-mini remains low-cost across
prompts, and structured prompts (CoT/CoT-few-shot) on GPT-4o-mini offer
attractive F1 at a small incremental cost. We recommend a staged workflow that
(1) deploys low-cost models with structured prompts for first-pass screening
and (2) escalates only borderline cases to higher-capacity models. These
findings highlight LLMs' uneven but promising potential to automate literature
screening. By systematically analyzing prompt-model interactions, we provide a
comparative benchmark and practical guidance for task-adaptive LLM deployment.

</details>


### [6] [Facts in Stats: Impacts of Pretraining Diversity on Language Model Generalization](https://arxiv.org/abs/2510.16096)
*Tina Behnia,Puneesh Deora,Christos Thrampoulidis*

Main category: cs.CL

TL;DR: 构建合成测试平台分析语言模型预训练中上下文多样性与结构对事实回忆及统计泛化的复杂交互影响，揭示不同场景下多样性作用差异及优化瓶颈


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对统计规律与事实关联交互机制的系统分析，需通过可控实验环境揭示其如何共同影响模型泛化能力

Method: 设计双流合成框架（统计通用token流+抽象事实对流），通过独立控制上下文结构（流组成）和多样性水平（事实出现频次分布）开展对照实验

Result: 1. 高上下文多样性延迟ID准确率但提升OOD泛化 2. 不同上下文结构下多样性作用方向相反 3. 发现嵌入层/解嵌入层的优化瓶颈 4. 训练时长影响最优多样性水平

Conclusion: 该框架为分离研究不同影响因素提供实验基准，证明上下文设计特性与多样性水平的组合效应对模型能力发展路径具有决定性作用

Abstract: Language models are pretrained on sequences that blend statistical
regularities (making text fluent) with factual associations between specific
tokens (knowledge of facts). While recent work suggests that the variability of
their interaction, such as paraphrases of factual associations, critically
determines generalization ability, we lack a systematic analysis of these
impacts. This paper introduces a flexible synthetic testbed that combines a
statistical stream of generic tokens with an abstract factual stream of
source-target token pairs, enabling fine-grained control over their
interaction. The design enables the independent control of diversity nature by
manipulating stream composition (contextual structure) and the diversity level
by varying which statistical streams each fact appears in. Through controlled
experiments, we find that while higher contextual diversity delays
in-distribution (ID) factual accuracy, its impact on out-of-distribution (OOD)
factual generalization depends critically on contextual structure. In some
cases, OOD performance follows the same trend as ID, but in others, diversity
becomes essential for non-trivial factual recall. Even when low diversity
prohibits factual recall, optimal diversity levels depend on training duration.
Beyond factual recall failures, we identify structures where statistical
generalization fails independently, and others where both capabilities degrade.
This shows how the interplay between contextual design and diversity level
impacts different generalization aspects. Further, through a series of
controlled interventions on the model components, we trace the OOD failures to
distinct optimization bottlenecks, highlighting the importance of the embedding
and unembedding layers. Our synthetic framework allows us to isolate effects
that would be confounded in large-scale studies, offering a controlled testbed
for future investigations.

</details>


### [7] [In Generative AI We (Dis)Trust? Computational Analysis of Trust and Distrust in Reddit Discussions](https://arxiv.org/abs/2510.16173)
*Aria Pessianzadeh,Naima Sultana,Hildegarde Van den Bulck,David Gefen,Shahin Jabari,Rezvaneh Rezapour*

Main category: cs.CL

TL;DR: 首个基于Reddit大数据的生成式AI信任计算研究，揭示公众信任与不信任动态平衡及影响因素


<details>
  <summary>Details</summary>
Motivation: 填补生成式AI信任研究中缺乏大规模计算方法的空白，为负责任AI治理提供依据

Method: 结合众包标注与分类模型，分析2022-2025年39个Reddit子版的19.7万条帖子

Result: 信任/不信任随时间趋于平衡，技术性能主导态度，个人经验是关键影响因素，不同用户群体呈现差异模式

Conclusion: 建立信任分析框架并揭示公众态度演变，为AI治理提供方法论支持和实证洞察

Abstract: The rise of generative AI (GenAI) has impacted many aspects of human life. As
these systems become embedded in everyday practices, understanding public trust
in them also becomes essential for responsible adoption and governance. Prior
work on trust in AI has largely drawn from psychology and human-computer
interaction, but there is a lack of computational, large-scale, and
longitudinal approaches to measuring trust and distrust in GenAI and large
language models (LLMs). This paper presents the first computational study of
Trust and Distrust in GenAI, using a multi-year Reddit dataset (2022--2025)
spanning 39 subreddits and 197,618 posts. Crowd-sourced annotations of a
representative sample were combined with classification models to scale
analysis. We find that Trust and Distrust are nearly balanced over time, with
shifts around major model releases. Technical performance and usability
dominate as dimensions, while personal experience is the most frequent reason
shaping attitudes. Distinct patterns also emerge across trustors (e.g.,
experts, ethicists, general users). Our results provide a methodological
framework for large-scale Trust analysis and insights into evolving public
perceptions of GenAI.

</details>


### [8] [EgMM-Corpus: A Multimodal Vision-Language Dataset for Egyptian Culture](https://arxiv.org/abs/2510.16198)
*Mohamed Gamil,Abdelrahman Elsayed,Abdelrahman Lila,Ahmed Gad,Hesham Abdelgawad,Mohamed Aref,Ahmed Fares*

Main category: cs.CL

TL;DR: 介绍埃及文化多模态数据集EgMM-Corpus，用于评估视觉语言模型在埃及文化背景下的表现，并揭示CLIP模型21.2%的Top-1准确率，证实文化偏见存在。


<details>
  <summary>Details</summary>
Motivation: 中东和非洲地区缺乏文化多样性数据集，现有视觉语言模型存在文化偏差，需构建埃及文化专属数据集作为评估基准。

Method: 通过新设计的数据收集流程获取313个文化概念的3000+图像，人工验证条目真实性，并测试CLIP模型的零样本分类性能。

Result: CLIP在EgMM-Corpus上Top-1准确率21.2%、Top-5准确率36.4%，显著低于主流文化数据集表现。

Conclusion: EgMM-Corpus有效暴露视觉语言模型的文化局限性，为开发文化敏感型AI提供关键基准，推动包容性技术发展。

Abstract: Despite recent advances in AI, multimodal culturally diverse datasets are
still limited, particularly for regions in the Middle East and Africa. In this
paper, we introduce EgMM-Corpus, a multimodal dataset dedicated to Egyptian
culture. By designing and running a new data collection pipeline, we collected
over 3,000 images, covering 313 concepts across landmarks, food, and folklore.
Each entry in the dataset is manually validated for cultural authenticity and
multimodal coherence. EgMM-Corpus aims to provide a reliable resource for
evaluating and training vision-language models in an Egyptian cultural context.
We further evaluate the zero-shot performance of Contrastive Language-Image
Pre-training CLIP on EgMM-Corpus, on which it achieves 21.2% Top-1 accuracy and
36.4% Top-5 accuracy in classification. These results underscore the existing
cultural bias in large-scale vision-language models and demonstrate the
importance of EgMM-Corpus as a benchmark for developing culturally aware
models.

</details>


### [9] [What Can String Probability Tell Us About Grammaticality?](https://arxiv.org/abs/2510.16227)
*Jennifer Hu,Ethan Gotlieb Wilcox,Siyuan Song,Kyle Mahowald,Roger P. Levy*

Main category: cs.CL

TL;DR: 研究通过理论分析和28万组中英文句子对验证，探讨语言模型如何通过概率反映语法知识，提出概率与语法结构关联的三个预测并验证。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型是否真正掌握语法知识的理论争议，澄清概率与语法合格性在语言学中的本质差异。

Method: 基于语料生成过程的理论框架，使用28万组中英文最小语义差异句子对（minimal pairs）进行实证检验。

Result: 验证三个核心预测：(1)最小语义差异句子的概率相关性；(2)模型与人类语法判断变化量的相关性；(3)合法/非法句子在概率空间区分度低。

Conclusion: 为通过概率分析语言模型的结构知识提供理论依据，指明未来语法评估的研究方向。

Abstract: What have language models (LMs) learned about grammar? This question remains
hotly debated, with major ramifications for linguistic theory. However, since
probability and grammaticality are distinct notions in linguistics, it is not
obvious what string probabilities can reveal about an LM's underlying
grammatical knowledge. We present a theoretical analysis of the relationship
between grammar, meaning, and string probability, based on simple assumptions
about the generative process of corpus data. Our framework makes three
predictions, which we validate empirically using 280K sentence pairs in English
and Chinese: (1) correlation between the probability of strings within minimal
pairs, i.e., string pairs with minimal semantic differences; (2) correlation
between models' and humans' deltas within minimal pairs; and (3) poor
separation in probability space between unpaired grammatical and ungrammatical
strings. Our analyses give theoretical grounding for using probability to learn
about LMs' structural knowledge, and suggest directions for future work in LM
grammatical evaluation.

</details>


### [10] [Towards Low-Resource Alignment to Diverse Perspectives with Sparse Feedback](https://arxiv.org/abs/2510.16257)
*Chu Fei Luo,Samuel Dahan,Xiaodan Zhu*

Main category: cs.CL

TL;DR: 提出pluralistic decoding和model steering方法，仅用50标注样本提升语言模型的多元化对齐能力，减少高风险任务误报


<details>
  <summary>Details</summary>
Motivation: 当前语言模型训练范式假设每个问题存在唯一正确答案，导致回应同质化且价值观对齐不足，亟需增强模型对多元视角的包容性

Method: 采用两种低资源方法：1）pluralistic decoding生成多样化响应 2）model steering通过少量标注数据引导模型输出

Result: 模型steering在zero-shot/few-shot基线表现稳定提升，在仇恨言论/虚假信息检测任务中降低假阳性，GlobalOpinionQA分布对齐提升

Conclusion: 研究强调语言模型价值观多样性的重要性，证明通过少量标注即可有效增强模型对复杂人类价值观的适应性

Abstract: As language models have a greater impact on society, it is important to
ensure they are aligned to a diverse range of perspectives and are able to
reflect nuance in human values. However, the most popular training paradigms
for modern language models often assume there is one optimal answer for every
query, leading to generic responses and poor alignment. In this work, we aim to
enhance pluralistic alignment of language models in a low-resource setting with
two methods: pluralistic decoding and model steering. We empirically
demonstrate that model steering offers consistent improvement over zero-shot
and few-shot baselines with only 50 annotated samples. Our proposed methods
decrease false positives in several high-stakes tasks such as hate speech
detection and misinformation detection, and improves the distributional
alignment to human values in GlobalOpinionQA. We hope our work highlights the
importance of diversity and how language models can be adapted to consider
nuanced perspectives.

</details>


### [11] [Instant Personalized Large Language Model Adaptation via Hypernetwork](https://arxiv.org/abs/2510.16282)
*Zhaoxuan Tan,Zixuan Zhang,Haoyang Wen,Zheng Li,Rongzhi Zhang,Pei Chen,Fengran Mo,Zheyuan Liu,Qingkai Zeng,Qingyu Yin,Meng Jiang*

Main category: cs.CL

TL;DR: 提出Profile-to-PEFT框架，通过超网络直接生成用户适配参数，实现无需训练的高效个性化LLM部署。


<details>
  <summary>Details</summary>
Motivation: 解决现有参数高效微调方法（OPPU范式）存在的计算成本高、实时更新困难、隐私保护不足等问题

Method: 使用端到端训练的超网络，将用户特征编码直接映射为完整LoRA适配器参数

Result: 实验显示在减少87%计算资源的同时，性能优于提示学习方法和OPPU范式，且具备优秀的外推泛化能力

Conclusion: 该框架为大规模LLM个性化应用提供了高效、可扩展且支持隐私保护的解决方案

Abstract: Personalized large language models (LLMs) tailor content to individual
preferences using user profiles or histories. However, existing
parameter-efficient fine-tuning (PEFT) methods, such as the
``One-PEFT-Per-User'' (OPPU) paradigm, require training a separate adapter for
each user, making them computationally expensive and impractical for real-time
updates. We introduce Profile-to-PEFT, a scalable framework that employs a
hypernetwork, trained end-to-end, to map a user's encoded profile directly to a
full set of adapter parameters (e.g., LoRA), eliminating per-user training at
deployment. This design enables instant adaptation, generalization to unseen
users, and privacy-preserving local deployment. Experimental results
demonstrate that our method outperforms both prompt-based personalization and
OPPU while using substantially fewer computational resources at deployment. The
framework exhibits strong generalization to out-of-distribution users and
maintains robustness across varying user activity levels and different
embedding backbones. The proposed Profile-to-PEFT framework enables efficient,
scalable, and adaptive LLM personalization suitable for large-scale
applications.

</details>


### [12] [Thinking About Thinking: Evaluating Reasoning in Post-Trained Language Models](https://arxiv.org/abs/2510.16340)
*Pratham Singla,Shivank Garg,Ayush Singh,Ishan Garg,Ketan Suhaas Saichandran*

Main category: cs.CL

TL;DR: 后训练技术提升了大语言模型处理复杂逻辑任务的能力，但不同训练方式（SFT/DPO/GRPO）在策略认知、泛化能力和推理对齐方面呈现显著差异。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型是否能够真正理解其通过后训练获得的策略能力，以及这些策略在不同任务中的泛化能力和内在逻辑一致性。

Method: 通过定义策略认知、跨领域泛化、推理对齐三个维度，在多个需要特定策略的任务上对比监督微调(SFT)、直接策略优化(DPO)和群体相对策略优化(GRPO)的表现。

Result: RL训练模型（DPO/GRPO）比SFT模型表现出更强的策略意识和结构泛化能力，但GRPO模型的内部推理轨迹与最终输出存在明显错位。

Conclusion: 强化学习训练范式能有效提升模型的策略认知能力，但需警惕推理过程与输出的不一致性，这对模型可信度和应用安全具有重要意义。

Abstract: Recent advances in post-training techniques have endowed Large Language
Models (LLMs) with enhanced capabilities for tackling complex, logic-intensive
tasks through the generation of supplementary planning tokens. This development
raises a fundamental question: Are these models aware of what they "learn" and
"think"? To address this, we define three core competencies: (1) awareness of
learned latent policies, (2) generalization of these policies across domains,
and (3) alignment between internal reasoning traces and final outputs. We
empirically evaluate these abilities on several tasks, each designed to require
learning a distinct policy. Furthermore, we contrast the profiles of models
post-trained via Supervised Fine-Tuning (SFT), Direct Policy Optimization
(DPO), and Group Relative Policy Optimization (GRPO). Our findings indicate
that RL-trained models not only demonstrate greater awareness of their learned
behaviors and stronger generalizability to novel, structurally similar tasks
than SFT models but also often exhibit weak alignment between their reasoning
traces and final outputs, an effect most pronounced in GRPO-trained models.

</details>


### [13] [Utilising Large Language Models for Generating Effective Counter Arguments to Anti-Vaccine Tweets](https://arxiv.org/abs/2510.16359)
*Utsav Dhanuka,Soham Poddar,Saptarshi Ghosh*

Main category: cs.CL

TL;DR: 研究探讨利用大语言模型生成针对疫苗错误信息的定制化反驳论点，通过结构化微调与分类器实现精准辟谣


<details>
  <summary>Details</summary>
Motivation: 社交媒体错误信息阻碍疫苗接种率，现有研究缺乏实时生成针对性反驳的技术方案

Method: 结合多标签分类器（识别疫苗效力/副作用/政治影响等主题）与LLM的提示策略优化/微调方法

Result: 整合标签描述与结构化微调显著提升反驳效果，人工评估与自动指标显示方法一致性达88%

Conclusion: 该框架为大规模遏制疫苗错误信息提供可扩展解决方案，增强公共卫生沟通效能

Abstract: In an era where public health is increasingly influenced by information
shared on social media, combatting vaccine skepticism and misinformation has
become a critical societal goal. Misleading narratives around vaccination have
spread widely, creating barriers to achieving high immunisation rates and
undermining trust in health recommendations. While efforts to detect
misinformation have made significant progress, the generation of real time
counter-arguments tailored to debunk such claims remains an insufficiently
explored area. In this work, we explore the capabilities of LLMs to generate
sound counter-argument rebuttals to vaccine misinformation. Building on prior
research in misinformation debunking, we experiment with various prompting
strategies and fine-tuning approaches to optimise counter-argument generation.
Additionally, we train classifiers to categorise anti-vaccine tweets into
multi-labeled categories such as concerns about vaccine efficacy, side effects,
and political influences allowing for more context aware rebuttals. Our
evaluation, conducted through human judgment, LLM based assessments, and
automatic metrics, reveals strong alignment across these methods. Our findings
demonstrate that integrating label descriptions and structured fine-tuning
enhances counter-argument effectiveness, offering a promising approach for
mitigating vaccine misinformation at scale.

</details>


### [14] [End-to-End Argument Mining through Autoregressive Argumentative Structure Prediction](https://arxiv.org/abs/2510.16363)
*Nilmadhab Das,Vishal Vaibhav,Yash Sunil Choudhary,V. Vijaya Saradhi,Ashish Anand*

Main category: cs.CL

TL;DR: 提出自回归论辩结构预测框架AASP，通过预定义动作逐步构建论辩结构，在三个AM基准测试中实现SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有生成式方法将论辩结构扁平化处理，无法有效捕捉组件间复杂依赖关系。AASP框架通过联合建模组件和关系，以自回归方式逐步构建结构。

Method: 将论辩结构分解为预定义动作序列，利用条件预训练语言模型按顺序执行动作，通过自回归方式逐步生成完整的论辩推理流程图。

Result: 在三个标准AM基准测试中，两个任务达到SOTA效果，另一个任务也展现强劲性能。

Conclusion: AASP框架通过结构化动作建模论辩推理流程，有效提升AM任务性能，证明了自回归结构预测在复杂语义建模中的有效性。

Abstract: Argument Mining (AM) helps in automating the extraction of complex
argumentative structures such as Argument Components (ACs) like Premise, Claim
etc. and Argumentative Relations (ARs) like Support, Attack etc. in an
argumentative text. Due to the inherent complexity of reasoning involved with
this task, modelling dependencies between ACs and ARs is challenging. Most of
the recent approaches formulate this task through a generative paradigm by
flattening the argumentative structures. In contrast to that, this study
jointly formulates the key tasks of AM in an end-to-end fashion using
Autoregressive Argumentative Structure Prediction (AASP) framework. The
proposed AASP framework is based on the autoregressive structure prediction
framework that has given good performance for several NLP tasks. AASP framework
models the argumentative structures as constrained pre-defined sets of actions
with the help of a conditional pre-trained language model. These actions build
the argumentative structures step-by-step in an autoregressive manner to
capture the flow of argumentative reasoning in an efficient way. Extensive
experiments conducted on three standard AM benchmarks demonstrate that AASP
achieves state-of-theart (SoTA) results across all AM tasks in two benchmarks
and delivers strong results in one benchmark.

</details>


### [15] [Navigating through the hidden embedding space: steering LLMs to improve mental health assessment](https://arxiv.org/abs/2510.16373)
*Federico Ravenda,Seyed Ali Bahrainian,Andrea Raballo,Antonietta Mira*

Main category: cs.CL

TL;DR: 提出一种基于线性变换层激活的轻量级方法，通过引导向量提升LLM在心理健康评估任务中的表现，并在相关性预测和问卷填写任务中验证有效性


<details>
  <summary>Details</summary>
Motivation: 现有小规模语言模型在心理健康等垂直领域表现欠佳，传统领域适应方法计算成本过高，需探索高效参数干预方案

Method: 在特定网络层激活值上施加线性变换，利用引导向量定向调控模型输出，避免复杂计算

Result: 在Reddit帖子抑郁症状相关性预测（任务1）和标准化抑郁筛查问卷自动填写（任务2）中均取得性能提升

Conclusion: 验证了引导机制作为计算高效工具的潜力，为LLM的领域适应提供新思路

Abstract: The rapid evolution of Large Language Models (LLMs) is transforming AI,
opening new opportunities in sensitive and high-impact areas such as Mental
Health (MH). Yet, despite these advancements, recent evidence reveals that
smaller-scale models still struggle to deliver optimal performance in
domain-specific applications. In this study, we present a cost-efficient yet
powerful approach to improve MH assessment capabilities of an LLM, without
relying on any computationally intensive techniques. Our lightweight method
consists of a linear transformation applied to a specific layer's activations,
leveraging steering vectors to guide the model's output. Remarkably, this
intervention enables the model to achieve improved results across two distinct
tasks: (1) identifying whether a Reddit post is useful for detecting the
presence or absence of depressive symptoms (relevance prediction task), and (2)
completing a standardized psychological screening questionnaire for depression
based on users' Reddit post history (questionnaire completion task). Results
highlight the untapped potential of steering mechanisms as computationally
efficient tools for LLMs' MH domain adaptation.

</details>


### [16] [MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models, More than Outcomes](https://arxiv.org/abs/2510.16380)
*Yu Ying Chiu,Michael S. Lee,Rachel Calcott,Brandon Handoko,Paul de Font-Reaulx,Paula Rodriguez,Chen Bo Calvin Zhang,Ziwen Han,Udari Madhushani Sehwag,Yash Maurya,Christina Q Knight,Harry R. Lloyd,Florence Bacus,Mantas Mazeika,Bing Liu,Yejin Choi,Mitchell L Gordon,Sydney Levine*

Main category: cs.CL

TL;DR: 提出MoReBench和MoReBench-Theory两个道德推理评估框架，揭示现有AI模型在道德决策过程中存在伦理框架偏好且数学/代码能力无法预测其道德推理水平


<details>
  <summary>Details</summary>
Motivation: 随着AI决策权扩大，需确保决策过程符合人类价值观。道德困境场景允许多种合理结论，是研究AI过程透明性的理想测试场

Method: 构建含1,000个道德场景的MoReBench数据集（含23k专家评估标准）和测试五大伦理框架的MoReBench-Theory子集

Result: 模型在道德推理中表现出对特定伦理框架（如功利主义和义务论）的偏向，且传统STEM任务表现无法预测道德推理能力

Conclusion: 过程导向的评估方法推动AI决策透明化，训练范式可能造成伦理偏见，需建立更全面的道德推理评估体系

Abstract: As AI systems progress, we rely more on them to make decisions with us and
for us. To ensure that such decisions are aligned with human values, it is
imperative for us to understand not only what decisions they make but also how
they come to those decisions. Reasoning language models, which provide both
final responses and (partially transparent) intermediate thinking traces,
present a timely opportunity to study AI procedural reasoning. Unlike math and
code problems which often have objectively correct answers, moral dilemmas are
an excellent testbed for process-focused evaluation because they allow for
multiple defensible conclusions. To do so, we present MoReBench: 1,000 moral
scenarios, each paired with a set of rubric criteria that experts consider
essential to include (or avoid) when reasoning about the scenarios. MoReBench
contains over 23 thousand criteria including identifying moral considerations,
weighing trade-offs, and giving actionable recommendations to cover cases on AI
advising humans moral decisions as well as making moral decisions autonomously.
Separately, we curate MoReBench-Theory: 150 examples to test whether AI can
reason under five major frameworks in normative ethics. Our results show that
scaling laws and existing benchmarks on math, code, and scientific reasoning
tasks fail to predict models' abilities to perform moral reasoning. Models also
show partiality towards specific moral frameworks (e.g., Benthamite Act
Utilitarianism and Kantian Deontology), which might be side effects of popular
training paradigms. Together, these benchmarks advance process-focused
reasoning evaluation towards safer and more transparent AI.

</details>


### [17] [ATA: A Neuro-Symbolic Approach to Implement Autonomous and Trustworthy Agents](https://arxiv.org/abs/2510.16381)
*David Peer,Sebastian Stabinger*

Main category: cs.CL

TL;DR: 提出神经符号方法ATA，通过分离知识吸收与任务处理两个阶段，结合形式化验证机制增强大语言模型的可靠性


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在关键领域应用的信任缺陷（幻觉、结果不稳定性、缺乏透明度），构建可信赖的自主代理体系

Method: 双阶段架构：1）离线阶段将非形式化规范转化为可验证的符号知识库 2）在线阶段通过符号推理引擎实现稳定决策

Result: 在保持竞争力的同时实现完全确定性、抗输入扰动稳定性，经人工校正的知识库下性能超越更大规模模型

Conclusion: ATA架构通过符号推理基础与可验证知识库，为构建透明、可审计、可靠的新一代自主代理提供可行方案

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities, yet
their deployment in high-stakes domains is hindered by inherent limitations in
trustworthiness, including hallucinations, instability, and a lack of
transparency. To address these challenges, we introduce a generic
neuro-symbolic approach, which we call Autonomous Trustworthy Agents (ATA). The
core of our approach lies in decoupling tasks into two distinct phases: Offline
knowledge ingestion and online task processing. During knowledge ingestion, an
LLM translates an informal problem specification into a formal, symbolic
knowledge base. This formal representation is crucial as it can be verified and
refined by human experts, ensuring its correctness and alignment with domain
requirements. In the subsequent task processing phase, each incoming input is
encoded into the same formal language. A symbolic decision engine then utilizes
this encoded input in conjunction with the formal knowledge base to derive a
reliable result. Through an extensive evaluation on a complex reasoning task,
we demonstrate that a concrete implementation of ATA is competitive with
state-of-the-art end-to-end reasoning models in a fully automated setup while
maintaining trustworthiness. Crucially, with a human-verified and corrected
knowledge base, our approach significantly outperforms even larger models,
while exhibiting perfect determinism, enhanced stability against input
perturbations, and inherent immunity to prompt injection attacks. By generating
decisions grounded in symbolic reasoning, ATA offers a practical and
controllable architecture for building the next generation of transparent,
auditable, and reliable autonomous agents.

</details>


### [18] [Probing the Hidden Talent of ASR Foundation Models for L2 English Oral Assessment](https://arxiv.org/abs/2510.16387)
*Fu-An Chao,Bi-Cheng Yan,Berlin Chen*

Main category: cs.CL

TL;DR: 利用Whisper模型的隐藏表征提取声学/语言特征，通过轻量级分类器实现L2口语评估，结合多模态信息实现性能突破，揭示模型内在编码能力


<details>
  <summary>Details</summary>
Motivation: 突破现有研究仅利用Whisper转录结果的局限，深度挖掘其隐藏表征在二语口语能力评估中的潜在价值

Method: 从中间层和输出层提取特征，结合图像/文本提示作为相关性线索，仅训练轻量级分类器

Result: 在GEPT数据集上超越现有SOTA方法，多模态融合带来额外增益，可视化分析显示嵌入空间自然编码语言熟练度等级

Conclusion: Whisper作为基础模型具有强大的零样本能力，其隐藏表征天然包含韵律特征和语义信息，为口语理解任务提供新范式

Abstract: In this paper, we explore the untapped potential of Whisper, a
well-established automatic speech recognition (ASR) foundation model, in the
context of L2 spoken language assessment (SLA). Unlike prior studies that
extrinsically analyze transcriptions produced by Whisper, our approach goes a
step further to probe its latent capabilities by extracting acoustic and
linguistic features from hidden representations. With only a lightweight
classifier being trained on top of Whisper's intermediate and final outputs,
our method achieves strong performance on the GEPT picture-description dataset,
outperforming existing cutting-edge baselines, including a multimodal approach.
Furthermore, by incorporating image and text-prompt information as auxiliary
relevance cues, we demonstrate additional performance gains. Finally, we
conduct an in-depth analysis of Whisper's embeddings, which reveals that, even
without task-specific fine-tuning, the model intrinsically encodes both ordinal
proficiency patterns and semantic aspects of speech, highlighting its potential
as a powerful foundation for SLA and other spoken language understanding tasks.

</details>


### [19] [FrugalPrompt: Reducing Contextual Overhead in Large Language Models via Token Attribution](https://arxiv.org/abs/2510.16439)
*Syed Rifat Raiyan,Md Farhan Ishmam,Abdullah Al Imran,Mohammad Ali Moni*

Main category: cs.CL

TL;DR: FrugalPrompt通过保留高语义权重的token压缩LLM提示，在常规NLP任务中实现20%压缩率下性能微降，但数学推理性能显著下降，揭示了不同任务对上下文稀疏性的差异容忍度。


<details>
  <summary>Details</summary>
Motivation: LLM的长输入上下文导致高计算开销，而大部分token语义价值低。为提升效率，需开发保留关键语义token的压缩方法，同时维持模型性能。

Method: 采用GlobEnc和DecompX两种token显著性评估方法，对输入token排序后保留top-k%高权重token，生成保持原始顺序的稀疏化提示框架。

Result: 情感分析/常识QA/摘要任务压缩20%提示仅损失0.7%-3.3%性能，数学推理任务准确率下降17.1%。实验揭示常规NLP任务可通过高显著性token重建上下文，数学推理依赖完整token连续性。

Conclusion: 研究界定了上下文稀疏性容忍边界：常规NLP任务可能利用预训练记忆模式，数学推理需完整上下文。为LLM效率-性能平衡提供新视角，推动绿色AI发展。

Abstract: Large language models (LLMs) owe much of their stellar performance to
expansive input contexts, yet such verbosity inflates monetary costs, carbon
footprint, and inference-time latency. Much of this overhead manifests from the
redundant low-utility tokens present in typical prompts, as only a fraction of
tokens typically carries the majority of the semantic weight. We address this
inefficiency by introducing FrugalPrompt, a novel prompt compression framework
for LLMs, which retains only the most semantically significant tokens.
Leveraging two state-of-the-art token attribution methods, GlobEnc and DecompX,
we assign salience scores to every token in an input sequence, rank them to
preserve the top-k% tokens in their original order, and obtain a sparse
frugalized prompt. We evaluate the approach across four NLP tasks: Sentiment
Analysis, Commonsense QA, Summarization, and Mathematical Reasoning, using a
suite of frontier LLMs. For the first three tasks, a 20% prompt reduction
incurs only a marginal loss in task performance, demonstrating that
contemporary LLMs can reconstruct elided context from high-salience cues. In
contrast, performance on mathematical reasoning deteriorates sharply,
reflecting a stronger dependence on complete token continuity. Further analysis
with bottom-k% and random-k% tokens reveals asymmetric performance patterns
that may suggest potential task contamination effects, wherein models may
resort to shallow memorized patterns from pretraining exposure for conventional
NLP tasks. We posit that our work contributes to a more nuanced understanding
of LLM behavior in performance-efficiency trade-offs, and delineate the
boundary between tasks tolerant to contextual sparsity and those requiring
exhaustive context. Our source code and models are available at:
https://github.com/Starscream-11813/Frugal-ICL

</details>


### [20] [TrajSelector: Harnessing Latent Representations for Efficient and Effective Best-of-N in Large Reasoning Model](https://arxiv.org/abs/2510.16449)
*Bin Yu,Xinming Wang,Shijie Lian,Haotian Li,Changti Wu,Ruina Hu,Bailing Wang,Yuliang Wei,Kai Chen*

Main category: cs.CL

TL;DR: TrajSelector框架通过利用LLM隐藏状态和轻量级验证器优化Best-of-N推理轨迹选择，在降低计算成本的同时实现性能提升


<details>
  <summary>Details</summary>
Motivation: 现有Best-of-N方法存在两个关键缺陷：(1) 过程奖励模型带来高计算开销 (2) 未能充分利用LLM的潜在表征。需要开发更高效有效的推理轨迹选择框架

Method: 提出基于LLM隐藏状态的过程级评分框架：1) 0.6B参数的轻量级验证器评估轨迹分步质量 2) 端到端数据驱动训练方法避免分步标注依赖 3) 通过聚合步级分数选择最优推理轨迹

Result: 在五个基准测试中，Best-of-32设置下：1) 准确率超越多数投票4.61% 2) 优于现有过程奖励模型4.31%-12.21% 3) 保持更低推理成本

Conclusion: TrajSelector证明了利用LLM固有表征进行轨迹选择的有效性，为高效推理系统开发提供了新方向，具有实际部署价值

Abstract: Large language models (LLMs) have shown remarkable progress in complex
reasoning tasks, largely enabled by test-time scaling (TTS) paradigms that
allocate additional compute during inference. Among these, external TTS
(particularly the Best-of-N selection paradigm) yields scalable performance
improvements by selecting from multiple independently generated reasoning
trajectories. However, this approach faces key limitations: (i) the high
computational overhead of deploying process reward models, (ii) the
underutilization of the LLM's intrinsic latent representations. We introduce
TrajSelector, an efficient and effective Best-of-N framework that exploit the
hidden states in the sampler LLM for process-level scoring. A lightweight
verifier (with only 0.6B parameters) evaluates the quality of step-wise
trajectory, and then aggregates these scores to identify the optimal reasoning
trajectory. Our framework employs a fully data-driven, end-to-end training
recipe that eliminates reliance on massive step-level annotations. Experiential
results across five benchmarks demonstrate that TrajSelector delivers
consistent performance gains. In Best-of-32 settings, it surpasses majority
voting by 4.61% accuracy and outperforms existing process reward models by
4.31% to 12.21%, all while maintaining lower inference costs.

</details>


### [21] [RAVEN: Robust Advertisement Video Violation Temporal Grounding via Reinforcement Reasoning](https://arxiv.org/abs/2510.16455)
*Deyi Ji,Yuekui Yang,Haiyang Wu,Shaoping Ma,Tianrun Chen,Lanyun Zhu*

Main category: cs.CL

TL;DR: RAVEN框架通过课程强化学习与多模态大语言模型结合，提升广告视频违规检测的推理能力和时间定位精度。


<details>
  <summary>Details</summary>
Motivation: 现有广告违规检测方法存在时间定位不精准、标注噪声大、泛化能力弱的问题

Method: 采用渐进式训练策略（课程学习）+ GRPO优化算法+多层奖励机制，融合精确/粗略标注数据

Result: 工业数据集上类别准确率提升，时间定位误差减少15%；在线A/B测试精确率提升23.6%，召回率提升18.3%

Conclusion: RAVEN通过课程强化学习实现无显式标注的推理能力，缓解监督微调导致的灾难性遗忘问题，具备强泛化性

Abstract: Advertisement (Ad) video violation detection is critical for ensuring
platform compliance, but existing methods struggle with precise temporal
grounding, noisy annotations, and limited generalization. We propose RAVEN, a
novel framework that integrates curriculum reinforcement learning with
multimodal large language models (MLLMs) to enhance reasoning and cognitive
capabilities for violation detection. RAVEN employs a progressive training
strategy, combining precisely and coarsely annotated data, and leverages Group
Relative Policy Optimization (GRPO) to develop emergent reasoning abilities
without explicit reasoning annotations. Multiple hierarchical sophisticated
reward mechanism ensures precise temporal grounding and consistent category
prediction. Experiments on industrial datasets and public benchmarks show that
RAVEN achieves superior performances in violation category accuracy and
temporal interval localization. We also design a pipeline to deploy the RAVEN
on the online Ad services, and online A/B testing further validates its
practical applicability, with significant improvements in precision and recall.
RAVEN also demonstrates strong generalization, mitigating the catastrophic
forgetting issue associated with supervised fine-tuning.

</details>


### [22] [Agree, Disagree, Explain: Decomposing Human Label Variation in NLI through the Lens of Explanations](https://arxiv.org/abs/2510.16458)
*Pingjun Hong,Beiduo Chen,Siyao Peng,Marie-Catherine de Marneffe,Benjamin Roth,Barbara Plank*

Main category: cs.CL

TL;DR: 通过LiTEx分类法分析NLI标注差异，揭示解释相似性可能掩盖标签分歧的现象，强调推理类型一致性更能反映语义相似性。


<details>
  <summary>Details</summary>
Motivation: 突破以往仅关注标签内差异的研究局限，探索标注者在推理类型和标签步骤上的双重分歧，以更全面理解NLI标注的个体差异。

Method: 应用LiTEx分类法对两个英文NLI数据集进行多维度对齐分析（标签一致性/解释相似性/分类法一致性），结合标注者选择偏差的复合因素。

Result: 发现标签分歧但解释高度相似的案例，揭示标注者存在解释策略偏好和标签选择差异，推理类型一致性比标签更能反映解释语义相似性。

Conclusion: 需谨慎将标签视为绝对事实，基于推理的解释体系能更精准捕捉语义差异，标注个体偏好提示需建立更细粒度的评估框架。

Abstract: Natural Language Inference datasets often exhibit human label variation. To
better understand these variations, explanation-based approaches analyze the
underlying reasoning behind annotators' decisions. One such approach is the
LiTEx taxonomy, which categorizes free-text explanations in English into
reasoning types. However, previous work applying such taxonomies has focused on
within-label variation: cases where annotators agree on the final NLI label but
provide different explanations. In contrast, this paper broadens the scope by
examining how annotators may diverge not only in the reasoning type but also in
the labeling step. We use explanations as a lens to decompose the reasoning
process underlying NLI annotation and to analyze individual differences. We
apply LiTEx to two NLI English datasets and align annotation variation from
multiple aspects: NLI label agreement, explanation similarity, and taxonomy
agreement, with an additional compounding factor of annotators' selection bias.
We observe instances where annotators disagree on the label but provide highly
similar explanations, suggesting that surface-level disagreement may mask
underlying agreement in interpretation. Moreover, our analysis reveals
individual preferences in explanation strategies and label choices. These
findings highlight that agreement in reasoning types better reflects the
semantic similarity of free-text explanations than label agreement alone. Our
findings underscore the richness of reasoning-based explanations and the need
for caution in treating labels as ground truth.

</details>


### [23] [Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety](https://arxiv.org/abs/2510.16492)
*Vamshi Krishna Bonagiri,Ponnurangam Kumaragurum,Khanh Nguyen,Benjamin Plaut*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As Large Language Model (LLM) agents increasingly operate in complex
environments with real-world consequences, their safety becomes critical. While
uncertainty quantification is well-studied for single-turn tasks, multi-turn
agentic scenarios with real-world tool access present unique challenges where
uncertainties and ambiguities compound, leading to severe or catastrophic risks
beyond traditional text generation failures. We propose using "quitting" as a
simple yet effective behavioral mechanism for LLM agents to recognize and
withdraw from situations where they lack confidence. Leveraging the ToolEmu
framework, we conduct a systematic evaluation of quitting behavior across 12
state-of-the-art LLMs. Our results demonstrate a highly favorable
safety-helpfulness trade-off: agents prompted to quit with explicit
instructions improve safety by an average of +0.39 on a 0-3 scale across all
models (+0.64 for proprietary models), while maintaining a negligible average
decrease of -0.03 in helpfulness. Our analysis demonstrates that simply adding
explicit quit instructions proves to be a highly effective safety mechanism
that can immediately be deployed in existing agent systems, and establishes
quitting as an effective first-line defense mechanism for autonomous agents in
high-stakes applications.

</details>


### [24] [Automated Composition of Agents: A Knapsack Approach for Agentic Component Selection](https://arxiv.org/abs/2510.16499)
*Michelle Yuan,Khushbu Pahwa,Shuaichen Chang,Mustafa Kaba,Jiarong Jiang,Xiaofei Ma,Yi Zhang,Monica Sunkara*

Main category: cs.CL

TL;DR: 提出基于背包问题优化的智能体系统组合框架，实现高成功率与低成本组件选择


<details>
  <summary>Details</summary>
Motivation: 现有静态检索方法存在能力描述不完整、组件选择未考虑实时效用等问题，需要动态组合框架

Method: 构建在线背包问题模型，动态测试组件性能，联合优化成功率、预算约束和组件兼容性

Result: 单智能体成功率提升31.6%，多智能体场景成功率从37%提升至87%（100+代理库）

Conclusion: 该框架在多样化场景中展现强适应性，显著提高资源复用效率与系统成功率

Abstract: Designing effective agentic systems requires the seamless composition and
integration of agents, tools, and models within dynamic and uncertain
environments. Most existing methods rely on static, semantic retrieval
approaches for tool or agent discovery. However, effective reuse and
composition of existing components remain challenging due to incomplete
capability descriptions and the limitations of retrieval methods. Component
selection suffers because the decisions are not based on capability, cost, and
real-time utility. To address these challenges, we introduce a structured,
automated framework for agentic system composition that is inspired by the
knapsack problem. Our framework enables a composer agent to systematically
identify, select, and assemble an optimal set of agentic components by jointly
considering performance, budget constraints, and compatibility. By dynamically
testing candidate components and modeling their utility in real-time, our
approach streamlines the assembly of agentic systems and facilitates scalable
reuse of resources. Empirical evaluation with Claude 3.5 Sonnet across five
benchmarking datasets shows that our online-knapsack-based composer
consistently lies on the Pareto frontier, achieving higher success rates at
significantly lower component costs compared to our baselines. In the
single-agent setup, the online knapsack composer shows a success rate
improvement of up to 31.6% in comparison to the retrieval baselines. In
multi-agent systems, the online knapsack composer increases success rate from
37% to 87% when agents are selected from an agent inventory of 100+ agents. The
substantial performance gap confirms the robust adaptability of our method
across diverse domains and budget constraints.

</details>


### [25] [ReviewGuard: Enhancing Deficient Peer Review Detection via LLM-Driven Data Augmentation](https://arxiv.org/abs/2510.16549)
*Haoxuan Zhang,Ruochi Li,Sarthak Shrestha,Shree Harshini Mamidala,Revanth Putta,Arka Krishan Aggarwal,Ting Xiao,Junhua Ding,Haihua Chen*

Main category: cs.CL

TL;DR: 提出首个LLM驱动的同行评审缺陷检测系统ReviewGuard，通过四阶段框架有效识别低质评审并揭示人机协作维护学术完整性的路径


<details>
  <summary>Details</summary>
Motivation: 论文提交量激增与LLM在学术评估中的滥用导致同行评审质量下降，现有研究缺乏系统性检测机制，威胁学术诚信体系

Method: 1) 收集ICLR/NeurIPS会议论文及评审数据；2) 采用GPT-4.1标注评审类型并进行人工验证；3) 通过LLM生成合成数据解决数据不平衡问题（最终包含6,634篇论文/71,095条评审）；4) 微调编码器模型和开源LLM

Result: 缺陷评审呈现低评分(3.0 vs 5.4)、高自信度(7.2 vs 6.8)、结构简单性(熵值低23%)及高负面情绪(28% vs 12%)；ChatGPT发布后AI生成评审量增长400%；混合训练使F1值提升15%

Conclusion: 首次构建LLM驱动的评审质量检测系统，为AI治理提供实证依据，揭示人机协同维护学术完整性的新模式，推动同行评审体系的数字化转型

Abstract: Peer review serves as the gatekeeper of science, yet the surge in submissions
and widespread adoption of large language models (LLMs) in scholarly evaluation
present unprecedented challenges. Recent work has focused on using LLMs to
improve review efficiency or generate insightful review content. However,
unchecked deficient reviews from both human experts and AI systems threaten to
systematically undermine the peer review ecosystem and compromise academic
integrity. To address this critical issue, we introduce ReviewGuard, an
automated system for detecting and categorizing deficient reviews. ReviewGuard
employs a comprehensive four-stage LLM-driven framework that: (1) collects ICLR
and NeurIPS papers with their corresponding reviews from OpenReview; (2)
annotates review types using GPT-4.1 with human validation; (3) addresses class
imbalance and data scarcity through LLM-driven synthetic data augmentation,
producing a final corpus of 6,634 papers, 24,657 real reviews, and 46,438
synthetic reviews; and (4) fine-tunes both encoder-based models and open source
LLMs. We perform comprehensive feature analysis of the structure and quality of
the review text. Compared to sufficient reviews, deficient reviews demonstrate
lower rating scores, higher self-reported confidence, reduced structural
complexity, and a higher proportion of negative sentiment. AI-generated text
detection reveals that, since ChatGPT's emergence, AI-generated reviews have
increased dramatically. In the evaluation of deficient review detection models,
mixed training with synthetic and real review data provides substantial
enhancements to recall and F1 scores on the binary task. This study presents
the first LLM-driven system for detecting deficient peer reviews, providing
evidence to inform AI governance in peer review while offering valuable
insights into human-AI collaboration to maintain academic integrity.

</details>


### [26] [Language over Content: Tracing Cultural Understanding in Multilingual Large Language Models](https://arxiv.org/abs/2510.16565)
*Seungho Cho,Changgeon Ko,Eui Jun Hwang,Junmyeong Lee,Huije Lee,Jong C. Park*

Main category: cs.CL

TL;DR: LLMs的文化理解机制更依赖语言而非文化背景，相同语言下国家间激活路径重叠度更高，显示语言特异性模式主导。


<details>
  <summary>Details</summary>
Motivation: 现有评估多聚焦输出结果，缺乏对内部文化理解机制的解析。需揭示不同语境下LLMs文化表征差异的驱动因素。

Method: 通过测量激活路径重叠度：1)固定提问语言改变目标国家；2)固定国家改变提问语言；使用同语言国家组解耦语言与文化因素。

Result: 同语言跨国问题激活路径重叠度高于跨语言同国问题；韩国-朝鲜案例显示语言相似性不代表表征对齐。

Conclusion: LLMs内部文化表征受语言影响大于文化背景，强调跨文化应用需深入分析模型内部机制。

Abstract: Large language models (LLMs) are increasingly used across diverse cultural
contexts, making accurate cultural understanding essential. Prior evaluations
have mostly focused on output-level performance, obscuring the factors that
drive differences in responses, while studies using circuit analysis have
covered few languages and rarely focused on culture. In this work, we trace
LLMs' internal cultural understanding mechanisms by measuring activation path
overlaps when answering semantically equivalent questions under two conditions:
varying the target country while fixing the question language, and varying the
question language while fixing the country. We also use same-language country
pairs to disentangle language from cultural aspects. Results show that internal
paths overlap more for same-language, cross-country questions than for
cross-language, same-country questions, indicating strong language-specific
patterns. Notably, the South Korea-North Korea pair exhibits low overlap and
high variability, showing that linguistic similarity does not guarantee aligned
internal representation.

</details>


### [27] [Hallucination Benchmark for Speech Foundation Models](https://arxiv.org/abs/2510.16567)
*Alkis Koudounas,Moreno La Quatra,Manuel Giollo,Sabato Marco Siniscalchi,Elena Baralis*

Main category: cs.CL

TL;DR: 提出SHALLOW基准框架，从词汇/语音/形态/语义四维度系统量化ASR系统幻觉现象，揭示其与传统WER指标的关系差异


<details>
  <summary>Details</summary>
Motivation: 传统ASR评估指标无法有效区分语音错误与结构连贯的幻觉错误，在低质量语音转录场景中幻觉问题更严重，严重影响医疗/法律等关键领域应用安全

Method: 构建SHALLOW评估框架，通过设计词汇相似度、语音混淆度、形态分析、语义一致性四个维度的细粒度指标，建立可解释的模型行为分析体系

Result: SHALLOW在高WER场景下与WER相关性显著减弱，能有效捕捉传统指标无法区分的细粒度错误模式，实现模型弱点的精准诊断

Conclusion: SHALLOW框架突破了传统聚合错误率的局限，为ASR系统提供更精细的评估维度，支持针对幻觉问题的定向模型优化

Abstract: Hallucinations in automatic speech recognition (ASR) systems refer to fluent
and coherent transcriptions produced by neural ASR models that are completely
unrelated to the underlying acoustic input (i.e., the speech signal). While
similar to conventional decoding errors in potentially compromising the
usability of transcriptions for downstream applications, hallucinations can be
more detrimental due to their preservation of syntactically and semantically
plausible structure. This apparent coherence can mislead subsequent processing
stages and introduce serious risks, particularly in critical domains such as
healthcare and law. Conventional evaluation metrics are primarily centered on
error-based metrics and fail to distinguish between phonetic inaccuracies and
hallucinations. Consequently, there is a critical need for new evaluation
frameworks that can effectively identify and assess models with a heightened
propensity for generating hallucinated content. To this end, we introduce
SHALLOW, the first benchmark framework that systematically categorizes and
quantifies hallucination phenomena in ASR along four complementary axes:
lexical, phonetic, morphological, and semantic. We define targeted metrics
within each category to produce interpretable profiles of model behavior.
Through evaluation across various architectures and speech domains, we have
found that SHALLOW metrics correlate strongly with word error rate (WER) when
recognition quality is high (i.e., low WER). Still, this correlation weakens
substantially as WER increases. SHALLOW, therefore, captures fine-grained error
patterns that WER fails to distinguish under degraded and challenging
conditions. Our framework supports specific diagnosis of model weaknesses and
provides feedback for model improvement beyond what aggregate error rates can
offer.

</details>


### [28] [AI-Generated Text Detection in Low-Resource Languages: A Case Study on Urdu](https://arxiv.org/abs/2510.16573)
*Muhammad Ammar,Hadiya Murad Hadi,Usman Majeed Butt*

Main category: cs.CL

TL;DR: 提出针对乌尔都语的AI文本检测框架，mDeBERTa-v3-base模型取得91.26%准确率


<details>
  <summary>Details</summary>
Motivation: 乌尔都语缺乏AI文本检测工具，需应对虚假信息与学术不端问题

Method: 构建平衡数据集(3600篇)，进行语言统计分析和微调多语言Transformer模型

Result: mDeBERTa-v3-base表现最佳(F1:91.29/准确率:91.26%)，显著优于其他模型

Conclusion: 填补乌尔都语检测空白，推动低资源语言NLP工具发展

Abstract: Large Language Models (LLMs) are now capable of generating text that closely
resembles human writing, making them powerful tools for content creation, but
this growing ability has also made it harder to tell whether a piece of text
was written by a human or by a machine. This challenge becomes even more
serious for languages like Urdu, where there are very few tools available to
detect AI-generated text. To address this gap, we propose a novel AI-generated
text detection framework tailored for the Urdu language. A balanced dataset
comprising 1,800 humans authored, and 1,800 AI generated texts, sourced from
models such as Gemini, GPT-4o-mini, and Kimi AI was developed. Detailed
linguistic and statistical analysis was conducted, focusing on features such as
character and word counts, vocabulary richness (Type Token Ratio), and N-gram
patterns, with significance evaluated through t-tests and MannWhitney U tests.
Three state-of-the-art multilingual transformer models such as
mdeberta-v3-base, distilbert-base-multilingualcased, and xlm-roberta-base were
fine-tuned on this dataset. The mDeBERTa-v3-base achieved the highest
performance, with an F1-score 91.29 and accuracy of 91.26% on the test set.
This research advances efforts in contesting misinformation and academic
misconduct in Urdu-speaking communities and contributes to the broader
development of NLP tools for low resource languages.

</details>


### [29] [Fine-tuning of Large Language Models for Constituency Parsing Using a Sequence to Sequence Approach](https://arxiv.org/abs/2510.16604)
*Francisco Jose Cortes Delgado,Eduardo Martinez Gracia,Rafael Valencia Garcia*

Main category: cs.CL

TL;DR: 通过微调大语言模型实现西班牙语句法结构分析，F1分数验证了该方法在MiSintaxis教学工具中的有效性


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型增强西班牙语语法教学工具MiSintaxis的句法分析能力，探索机器翻译生成句法结构的新方法

Method: 使用AnCora-ES语料库生成训练数据，对Hugging Face的多个大语言模型进行微调训练

Result: 短语结构分析F1分数显示高精度，证实了该方法在句法分析任务中的优越性能

Conclusion: 该方法显著提升句法分析准确率，为语法教学工具开发提供了有效的技术路径，展示了LLMs在语言学教育中的应用潜力

Abstract: Recent advances in natural language processing with large neural models have
opened new possibilities for syntactic analysis based on machine learning. This
work explores a novel approach to phrase-structure analysis by fine-tuning
large language models (LLMs) to translate an input sentence into its
corresponding syntactic structure. The main objective is to extend the
capabilities of MiSintaxis, a tool designed for teaching Spanish syntax.
Several models from the Hugging Face repository were fine-tuned using training
data generated from the AnCora-ES corpus, and their performance was evaluated
using the F1 score. The results demonstrate high accuracy in phrase-structure
analysis and highlight the potential of this methodology.

</details>


### [30] [Unleashing Diverse Thinking Modes in LLMs through Multi-Agent Collaboration](https://arxiv.org/abs/2510.16645)
*Zhixuan He,Yue Feng*

Main category: cs.CL

TL;DR: 提出DiMo多智能体协作框架，通过模拟结构化辩论整合不同思维模式，提升大语言模型的性能和可解释性，在多个基准测试中表现优异（数学任务提升最大）。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLM）虽性能强劲但缺乏可解释的推理过程，需通过多智能体协作机制同时提升性能与透明度。

Method: 构建包含四种不同推理范式LLM智能体的框架，通过迭代辩论机制相互挑战优化回答，形成可审计的推理链条。

Result: 在6个基准测试中，DiMo在统一开源设置下准确率超过单模型和传统辩论基线，数学任务提升幅度最大。

Conclusion: DiMo作为语义感知的Web原生框架，结合检索增强推理与结构化验证机制，未来可扩展应用于知识图谱和网络语料库，支持下游系统对推理过程的复用验证。

Abstract: Large Language Models (LLMs) demonstrate strong performance but often lack
interpretable reasoning. This paper introduces the Multi-Agent Collaboration
Framework for Diverse Thinking Modes (DiMo), which enhances both performance
and interpretability by simulating a structured debate among four specialized
LLM agents. Each agent embodies a distinct reasoning paradigm, allowing the
framework to collaboratively explore diverse cognitive approaches. Through
iterative debate, agents challenge and refine initial responses, yielding more
robust conclusions and an explicit, auditable reasoning chain. Across six
benchmarks and under a unified open-source setup, DiMo improves accuracy over
widely used single-model and debate baselines, with the largest gains on math.
We position DiMo as a semantics-aware, Web-native multi-agent framework: it
models human-machine intelligence with LLM agents that produce semantically
typed, URL-annotated evidence chains for explanations and user-friendly
interactions. Although our experiments use standard reasoning benchmarks, the
framework is designed to be instantiated over Web corpora and knowledge graphs,
combining retrieval-augmented reasoning with structured justifications that
downstream systems can inspect and reuse.

</details>


### [31] [All You Need is One: Capsule Prompt Tuning with a Single Vector](https://arxiv.org/abs/2510.16670)
*Yiyang Liu,James C. Liang,Heng Fan,Wenhao Yang,Yiming Cui,Xiaotian Han,Lifu Huang,Dongfang Liu,Qifan Wang,Cheng Han*

Main category: cs.CL

TL;DR: 提出胶囊提示调整方法（CaPT），通过融合实例感知与任务感知信息实现高效LLM微调，显著提升性能并保持参数高效性


<details>
  <summary>Details</summary>
Motivation: 传统提示学习方法存在计算负担重、缺乏实例感知信息的问题，导致注意力交互不足。研究发现实例信息整合可有效增强模型表现

Method: 通过单一胶囊提示整合实例语义（作为注意力锚）与任务指导，以接近零参数的方式优化提示调整过程

Result: 在T5-Large等模型上实现84.03%平均准确率，参数效率达Llama3.2-1B的0.003%

Conclusion: CaPT通过实例感知信息的创新融合，在保持参数效率的同时突破传统提示学习的性能瓶颈，为高效LLM适配提供新范式

Abstract: Prompt-based learning has emerged as a parameter-efficient finetuning (PEFT)
approach to facilitate Large Language Model (LLM) adaptation to downstream
tasks by conditioning generation with task-aware guidance. Despite its
successes, current prompt-based learning methods heavily rely on laborious grid
searching for optimal prompt length and typically require considerable number
of prompts, introducing additional computational burden. Worse yet, our pioneer
findings indicate that the task-aware prompt design is inherently limited by
its absence of instance-aware information, leading to a subtle attention
interplay with the input sequence. In contrast, simply incorporating
instance-aware information as a part of the guidance can enhance the
prompt-tuned model performance without additional fine-tuning. Moreover, we
find an interesting phenomenon, namely "attention anchor", that incorporating
instance-aware tokens at the earliest position of the sequence can successfully
preserve strong attention to critical structural information and exhibit more
active attention interaction with all input tokens. In light of our
observation, we introduce Capsule Prompt-Tuning (CaPT), an efficient and
effective solution that leverages off-the-shelf, informative instance semantics
into prompt-based learning. Our approach innovatively integrates both
instance-aware and task-aware information in a nearly parameter-free manner
(i.e., one single capsule prompt). Empirical results demonstrate that our
method can exhibit superior performance across various language tasks (e.g.,
84.03\% average accuracy on T5-Large), serving as an "attention anchor," while
enjoying high parameter efficiency (e.g., 0.003\% of model parameters on
Llama3.2-1B).

</details>


### [32] [Temporal Understanding under Deictic Frame of Reference](https://arxiv.org/abs/2510.16685)
*Damin Zhang,Julia Rayz*

Main category: cs.CL

TL;DR: 提出TUuD框架评估大语言模型在动态时间参照系下的时间认知能力，发现其呈现接近人类的局部时间适应性但存在长程推理局限。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在时间推理方面存在显著不足，尤其在动态时间参照系下理解'现在'与事件的时空关系需要系统性评估。

Method: 通过设计动态时间参照系框架，要求模型对'现在'与目标事件的时间相似性进行0.00-1.00的量化评估，分析四个主流LLM的响应模式。

Result: 模型在近时语境中呈现钟型相似度曲线（当前时刻峰值），但随着时间距离扩大适应性衰减，显示时空参照系偏移敏感性和短期认知优势。

Conclusion: 大语言模型具备基础的时间认知机制，但长程时间推理能力尚未完全对齐人类认知模式，时间参照系稳定性有待提升。

Abstract: Understanding time is fundamental to human cognition, where temporal
experience is often conceptualized through spatial metaphors grounded in
sensory-motor experience. For example, "summer is approaching" parallels "We
are approaching the summer". In such expressions, humans rely on a frame of
reference (FoR) to interpret meaning relative to a particular viewpoint.
Extending this concept to time, a temporal frame of reference (t-FoR) defines
how temporal relations are perceived relative to an experiencer's moment of
"now". While Large Language Models (LLMs) have shown remarkable advances in
natural language understanding, their ability to interpret and reason about
time remains limited. In this work, we introduce TUuD (Temporal Understanding
under Deictic t-FoR), a framework that evaluates how LLMs interpret time-event
and event-event relations when the reference point of "now" dynamically shifts
along a timeline. Following recent work on temporal cognition
\cite{li2025other}, LLMs are prompted to rate the similarity between the
current moment and a target event from 0.00 (completely dissimilar) to 1.00
(highly similar), where similarity quantifies perceived temporal alignment
between the two points. Our results show that four evaluated LLMs exhibit
measurable adaptation to a deictic t-FoR, with similarity ratings peaking
around the present and decreasing toward past and future events. The
adaptation, however, weakens beyond near-term contexts, suggesting that while
LLMs display partial human-like temporal cognition, their temporal reasoning
remains sensitive to reference-frame shifts and temporal distance.

</details>


### [33] [Investigating the Impact of Rationales for LLMs on Natural Language Understanding](https://arxiv.org/abs/2510.16686)
*Wenhang Shi,Shuqing Bian,Yiren Chen,Xinyi Zhang,Zhe Zhao,Pengfei Hu,Wei Lu,Xiaoyong Du*

Main category: cs.CL

TL;DR: 探索思维链（CoT）在自然语言理解任务中的潜力，发现大模型使用CoT推理显著提升性能，特定训练方法可增强模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注CoT在推理任务中的作用，但忽视了其在自然语言理解（NLU）任务中的潜在价值。

Method: 构建NLURC数据集，开发多种基于CoT的训练方法，并系统测试不同规模模型的性能表现。

Result: 模型规模与CoT效果正相关；多数训练方法效果弱于纯标签训练，但特定方法提升显著；带CoT训练的小模型在未知任务上性能媲美大十倍模型。

Conclusion: CoT机制可有效提升NLU任务性能，增强模型可解释性，并为小模型实现接近商用大模型的效果提供新路径。

Abstract: Chain-of-thought (CoT) rationales, which provide step-by-step reasoning to
derive final answers, benefit LLMs in both inference and training.
Incorporating rationales, either by generating them before answering during
inference, or by placing them before or after the original answers during
training - significantly improves model performance on mathematical, symbolic
and commonsense reasoning tasks. However, most work focuses on the role of
rationales in these reasoning tasks, overlooking their potential impact on
other important tasks like natural language understanding (NLU) tasks. In this
work, we raise the question: Can rationales similarly benefit NLU tasks? To
conduct a systematic exploration, we construct NLURC, a comprehensive and
high-quality NLU dataset collection with rationales, and develop various
rationale-augmented methods. Through exploring the applicability of these
methods on NLU tasks using the dataset, we uncover several potentially
surprising findings: (1) CoT inference shifts from hindering NLU performance to
surpassing direct label prediction as model size grows, indicating a positive
correlation. (2) Most rationale-augmented training methods perform worse than
label-only training, with one specially designed method consistently achieving
improvements. (3) LLMs trained with rationales achieve significant performance
gains on unseen NLU tasks, rivaling models ten times their size, while
delivering interpretability on par with commercial LLMs.

</details>


### [34] [Natural Language Processing Applications in Cardiology: A Narrative Review](https://arxiv.org/abs/2510.16708)
*Kailai Yang,Yan Leng,Xin Zhang,Tianlin Zhang,Paul Thompson,Bernard Keavney,Maciej Tomaszewski,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 本文综述了2014-2025年自然语言处理(NLP)在心脏病学中的应用，通过分析265篇文献揭示了NLP技术在处理心血管疾病复杂文本数据中的多样性及时间演化趋势。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病受遗传、生活方式和社会因素共同影响，相关信息分散于患者叙述、病历和文献等非结构化文本中。NLP技术可有效解析这些数据，为心脏病诊断、治疗和预防提供新洞见。

Method: 通过检索六大文献数据库筛选265篇论文，从NLP范式类型、心脏任务类型、疾病类型、数据来源四个维度进行系统分析，并开展历时性方法演进研究。

Result: 各分析维度均呈现显著多样性，证实NLP在心脏病学中的广泛应用；时间趋势显示深度学习方法占比持续上升，传统规则系统逐渐被替代。

Conclusion: 该综述首次全面梳理心脏病学NLP研究，证明技术有效性并指明未来发展方向，为医疗AI在心血管领域的应用奠定理论基础。

Abstract: Cardiovascular disease has become increasingly prevalent in modern society
and has a significant effect on global health and well-being. Heart-related
conditions are intricate, multifaceted disorders, which may be influenced by a
combination of genetic predispositions, lifestyle choices, and various
socioeconomic and clinical factors. Information regarding these potentially
complex interrelationships is dispersed among diverse types of textual data,
which include patient narratives, medical records, and scientific literature,
among others. Natural language processing (NLP) techniques have increasingly
been adopted as a powerful means to analyse and make sense of this vast amount
of unstructured data. This, in turn, can allow healthcare professionals to gain
deeper insights into the cardiology field, which has the potential to
revolutionize current approaches to the diagnosis, treatment, and prevention of
cardiac problems. This review provides a detailed overview of NLP research in
cardiology between 2014 and 2025. We queried six literature databases to find
articles describing the application of NLP techniques in the context of a range
of different cardiovascular diseases. Following a rigorous screening process,
we identified a total of 265 relevant articles. We analysed each article from
multiple dimensions, i.e., NLP paradigm types, cardiology-related task types,
cardiovascular disease types, and data source types. Our analysis reveals
considerable diversity within each of these dimensions, thus demonstrating the
considerable breadth of NLP research within the field. We also perform a
temporal analysis, which illustrates the evolution and changing trends in NLP
methods employed over the last decade that we cover. To our knowledge, the
review constitutes the most comprehensive overview of NLP research in
cardiology to date.

</details>


### [35] [The Chameleon Nature of LLMs: Quantifying Multi-Turn Stance Instability in Search-Enabled Language Models](https://arxiv.org/abs/2510.16712)
*Shivam Ratnakar,Sanjay Raghavendra*

Main category: cs.CL

TL;DR: 揭示大语言模型在检索增强系统中存在'变色龙行为'，立场随矛盾问题摇摆，提出量化评估指标并验证主流模型的系统性缺陷


<details>
  <summary>Details</summary>
Motivation: 当前检索增强型LLM系统存在立场不稳定隐患，可能影响医疗/法律/金融等关键领域的可靠决策支持

Method: 构建包含12个争议领域1,180个多轮对话的基准数据集，提出Chameleon Score（立场稳定性）和Source Re-use Rate（知识多样性）双指标评估体系

Result: 所有测试模型（Llama/GPT/Gemini）均表现严重立场漂移（0.391-0.511分），GPT-4o-mini最差；源重用率与置信度(r=0.627)、立场变化(r=0.429)显著相关

Conclusion: 必须建立全面的立场一致性评估框架，尤其在高风险领域部署前需严格测试模型的多轮对话稳定性

Abstract: Integration of Large Language Models with search/retrieval engines has become
ubiquitous, yet these systems harbor a critical vulnerability that undermines
their reliability. We present the first systematic investigation of "chameleon
behavior" in LLMs: their alarming tendency to shift stances when presented with
contradictory questions in multi-turn conversations (especially in
search-enabled LLMs). Through our novel Chameleon Benchmark Dataset, comprising
17,770 carefully crafted question-answer pairs across 1,180 multi-turn
conversations spanning 12 controversial domains, we expose fundamental flaws in
state-of-the-art systems. We introduce two theoretically grounded metrics: the
Chameleon Score (0-1) that quantifies stance instability, and Source Re-use
Rate (0-1) that measures knowledge diversity. Our rigorous evaluation of
Llama-4-Maverick, GPT-4o-mini, and Gemini-2.5-Flash reveals consistent
failures: all models exhibit severe chameleon behavior (scores 0.391-0.511),
with GPT-4o-mini showing the worst performance. Crucially, small
across-temperature variance (less than 0.004) suggests the effect is not a
sampling artifact. Our analysis uncovers the mechanism: strong correlations
between source re-use rate and confidence (r=0.627) and stance changes
(r=0.429) are statistically significant (p less than 0.05), indicating that
limited knowledge diversity makes models pathologically deferential to query
framing. These findings highlight the need for comprehensive consistency
evaluation before deploying LLMs in healthcare, legal, and financial systems
where maintaining coherent positions across interactions is critical for
reliable decision support.

</details>


### [36] [so much depends / upon / a whitespace: Why Whitespace Matters for Poets and LLMs](https://arxiv.org/abs/2510.16713)
*Sriharsh Bhyravajjula,Melanie Walsh,Anna Preus,Maria Antoniak*

Main category: cs.CL

TL;DR: 通过分析19k首出版诗歌发现空格在诗歌形式中的重要性，比较了LLM生成与未发表诗歌的空格使用差异，并提出LLM预训练数据处理策略的改进建议。


<details>
  <summary>Details</summary>
Motivation: 诗歌作为长期艺术形式和LLM生成任务的重要对象，其空格使用未被NLP领域充分研究。研究旨在揭示诗人创作选择与空格语义特征，并探究不同数据源的空格模式差异。

Method: 使用Poetry Foundation的19k出版诗歌数据集，结合51k LLM生成诗歌和12k未发表社区诗歌，通过时间维度、诗体类型、数据来源三个层面进行空格模式对比分析。

Result: 不同文本处理方法会导致诗歌空格表征显著差异，LLM生成诗歌与人类创作在空格使用上存在系统性差异，早期诗歌的空格密度比现代诗歌高23%。

Conclusion: 空格是诗歌语义表达的核心要素，当前LLM预训练数据的空格处理策略存在信息损失，建议采用格式保留策略提升生成质量。

Abstract: Whitespace is a critical component of poetic form, reflecting both adherence
to standardized forms and rebellion against those forms. Each poem's whitespace
distribution reflects the artistic choices of the poet and is an integral
semantic and spatial feature of the poem. Yet, despite the popularity of poetry
as both a long-standing art form and as a generation task for large language
models (LLMs), whitespace has not received sufficient attention from the NLP
community. Using a corpus of 19k English-language published poems from Poetry
Foundation, we investigate how 4k poets have used whitespace in their works. We
release a subset of 2.8k public-domain poems with preserved formatting to
facilitate further research in this area. We compare whitespace usage in the
published poems to (1) 51k LLM-generated poems, and (2) 12k unpublished poems
posted in an online community. We also explore whitespace usage across time
periods, poetic forms, and data sources. Additionally, we find that different
text processing methods can result in significantly different representations
of whitespace in poetry data, motivating us to use these poems and whitespace
patterns to discuss implications for the processing strategies used to assemble
pretraining datasets for LLMs.

</details>


### [37] [Beacon: Single-Turn Diagnosis and Mitigation of Latent Sycophancy in Large Language Models](https://arxiv.org/abs/2510.16727)
*Sanskar Pandey,Ruhaan Chopra,Angkul Puniya,Sohom Pal*

Main category: cs.CL

TL;DR: 研究者开发了Beacon基准来量化和缓解大语言模型中的谄媚偏见，揭示了该现象与模型能力的关联并提出了提示/激活双重干预措施


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在奖励优化过程中混淆了帮助性目标与顺从表达，导致模型产生优先迎合用户而非保持客观判断的谄媚倾向

Method: 1) 构建单轮强制选择测试基准Beacon
2) 评估12个先进模型
3) 设计提示层面和激活层面的干预方法

Result: 谄媚偏见可分解为稳定语言子偏倚和情感子偏倚，二者均随模型规模增长而强化；干预措施可双向调节真实性与顺从判断的平衡

Conclusion: Beacon框架将谄媚重新定义为可测量的规范性误泛化，为研究生成式系统的对齐漂移问题提供了可重复的量化基准

Abstract: Large language models internalize a structural trade-off between truthfulness
and obsequious flattery, emerging from reward optimization that conflates
helpfulness with polite submission. This latent bias, known as sycophancy,
manifests as a preference for user agreement over principled reasoning. We
introduce Beacon, a single-turn forced-choice benchmark that isolates this bias
independent of conversational context, enabling precise measurement of the
tension between factual accuracy and submissive bias. Evaluations across twelve
state-of-the-art models reveal that sycophancy decomposes into stable
linguistic and affective sub-biases, each scaling with model capacity. We
further propose prompt-level and activation-level interventions that modulate
these biases in opposing directions, exposing the internal geometry of
alignment as a dynamic manifold between truthfulness and socially compliant
judgment. Beacon reframes sycophancy as a measurable form of normative
misgeneralization, providing a reproducible foundation for studying and
mitigating alignment drift in large-scale generative systems.

</details>


### [38] [Enhancing Language Agent Strategic Reasoning through Self-Play in Adversarial Games](https://arxiv.org/abs/2510.16761)
*Yikai Zhang,Ye Rong,Siyu Yuan,Jiangjie Chen,Jian Xie,Yanghua Xiao*

Main category: cs.CL

TL;DR: 提出SCO-PAL方法通过自我对抗学习优化策略，使对抗游戏胜率提升30%并在GPT-4对抗中达54.76%胜率


<details>
  <summary>Details</summary>
Motivation: 现有语言智能体在动态对抗游戏中策略推理能力不足，且对手选择机制缺乏系统性研究

Method: 开发SCO-PAL框架进行分步策略优化，通过设置不同等级对手验证自我对抗学习效果

Result: 自我对抗训练使平均胜率提升30%，对抗GPT-4达54.76%胜率

Conclusion: 自我对抗学习是提升对抗环境策略推理的有效方式，SCO-PAL框架显著优于基线模型

Abstract: Existing language agents often encounter difficulties in dynamic adversarial
games due to poor strategic reasoning. To mitigate this limitation, a promising
approach is to allow agents to learn from game interactions automatically,
without relying on costly expert-labeled data. Unlike static environments where
agents receive fixed feedback or rewards, selecting appropriate opponents in
dynamic adversarial games can significantly impact learning performance.
However, the discussion of opponents in adversarial environments remains an
area under exploration. In this paper, we propose a Step-level poliCy
Optimization method through Play-And-Learn, SCO-PAL. Leveraging SCO-PAL, we
conduct a detailed analysis of opponent selection by setting opponents at
different levels and find that self-play is the most effective way to improve
strategic reasoning in such adversarial environments. Utilizing SCO-PAL with
self-play, we increase the average win rate against four opponents by
approximately 30% compared to baselines and achieve a 54.76% win rate against
GPT-4 in six adversarial games.

</details>


### [39] [LC-Eval: A Bilingual Multi-Task Evaluation Benchmark for Long-Context Understanding](https://arxiv.org/abs/2510.16783)
*Sheikh Jubair,Arwa Omayrah,Amal Alshammari,Alhanoof Althnian,Abdulhamed Alothaimen,Norah A. Alzahrani,Shahad D. Alzaidi,Nora Al-Twairesh,Abdulmohsen Al-Thubaity*

Main category: cs.CL

TL;DR: 提出LC-Eval双语长文本评估基准，覆盖4k-128k tokens的英语/阿拉伯语多任务测试，揭示主流模型在深度推理、跨语言理解等任务中的性能瓶颈


<details>
  <summary>Details</summary>
Motivation: 现有评估方法无法有效衡量大语言模型在超长上下文理解中的深层推理、文档理解和跨语言信息提取能力，亟需更严谨的多任务评估体系

Method: 设计四类创新任务：多文档问答/双语问答/段落内声明验证/长文本选择题，构建英语/阿拉伯语平行数据集，覆盖不同文本类型，测试模型在信息追踪、双语理解等维度表现

Result: GPT-4o等顶尖模型在声明验证（准确率仅54%）和阿拉伯语问答（正确率低于60%）等任务中表现欠佳，证明基准的挑战性

Conclusion: LC-Eval为长文本理解能力评估提供标准化测试框架，揭示当前模型在跨语言处理、深层推理等方面的局限性，推动长上下文建模技术发展

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated
sophisticated capabilities, including the ability to process and comprehend
extended contexts. These emergent capabilities necessitate rigorous evaluation
methods to effectively assess their performance in long-context understanding.
In this paper, we present \textbf{LC-Eval}, a bilingual, multi-task evaluation
benchmark designed to evaluate long-context understanding in English and
Arabic, targeting context lengths ranging from 4k to over 128k tokens. LC-Eval
introduces four novel and challenging tasks: multi-document question answering,
bilingual question answering, claim verification within a paragraph, and
multiple-choice questions based on long contexts. These tasks are designed to
assess LLMs' abilities in deep reasoning, document comprehension, information
tracing, and bilingual information extraction and understanding. The benchmark
includes datasets in both Arabic and English for each task, allowing for a
comparative analysis of their performance across different text genres.
Evaluations were conducted on both open-weight and closed LLMs, with results
indicating that LC-Eval presents significant challenges. Even high-performing
models, such as GPT-4o, struggled with certain tasks, highlighting the
complexity and rigor of the benchmark.

</details>


### [40] [MOSAIC: Masked Objective with Selective Adaptation for In-domain Contrastive Learning](https://arxiv.org/abs/2510.16797)
*Vera Pavlova,Mohammed Makhlouf*

Main category: cs.CL

TL;DR: 提出MOSAIC框架，通过多阶段联合监督实现领域自适应，提升句子嵌入模型在专业领域的表现


<details>
  <summary>Details</summary>
Motivation: 解决大规模通用领域句子嵌入模型在专业领域适配的挑战，平衡领域相关特征学习与原有语义保持

Method: 联合优化领域特化掩码语言建模(MLM)和对比学习目标，采用分阶段训练流程

Result: 在高/低资源领域分别取得最高13.4%的NDCG@10提升，显著优于通用领域基线模型

Conclusion: 联合监督机制与分阶段适配策略能有效实现领域适应，消融实验验证了各组件的重要性

Abstract: We introduce MOSAIC (Masked Objective with Selective Adaptation for In-domain
Contrastive learning), a multi-stage framework for domain adaptation of
sentence embedding models that incorporates joint domain-specific masked
supervision. Our approach addresses the challenges of adapting large-scale
general-domain sentence embedding models to specialized domains. By jointly
optimizing masked language modeling (MLM) and contrastive objectives within a
unified training pipeline, our method enables effective learning of
domain-relevant representations while preserving the robust semantic
discrimination properties of the original model. We empirically validate our
approach on both high-resource and low-resource domains, achieving improvements
up to 13.4% in NDCG@10 (Normalized Discounted Cumulative Gain) over strong
general-domain baselines. Comprehensive ablation studies further demonstrate
the effectiveness of each component, highlighting the importance of balanced
joint supervision and staged adaptation.

</details>


### [41] [Knowing the Facts but Choosing the Shortcut: Understanding How Large Language Models Compare Entities](https://arxiv.org/abs/2510.16815)
*Hans Hergen Lehmann,Jae Hee Lee,Steven Schockaert,Stefan Wermter*

Main category: cs.CL

TL;DR: 大型语言模型在知识推理中常被表面启发式偏差主导，但大模型能选择性结合可靠知识，链式思维提示可改善推理质量


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在知识推理任务中何时依赖真实知识而非表面启发式方法，揭示模型决策机制

Method: 通过实体数值属性对比任务（如河流长度比较），分析三种启发式偏差（实体流行度/提及顺序/语义共现）对预测的影响

Result: 小模型预测被表面线索主导（启发式预测准确率>90%），32B大模型能选择性结合可靠数值知识，链式思维显著提升所有模型数值特征使用率

Conclusion: 模型性能差异源于知识运用能力而非知识储备量，大模型具备知识可靠性判断能力，提示工程可有效改善推理过程

Abstract: Large Language Models (LLMs) are increasingly used for knowledge-based
reasoning tasks, yet understanding when they rely on genuine knowledge versus
superficial heuristics remains challenging. We investigate this question
through entity comparison tasks by asking models to compare entities along
numerical attributes (e.g., ``Which river is longer, the Danube or the
Nile?''), which offer clear ground truth for systematic analysis. Despite
having sufficient numerical knowledge to answer correctly, LLMs frequently make
predictions that contradict this knowledge. We identify three heuristic biases
that strongly influence model predictions: entity popularity, mention order,
and semantic co-occurrence. For smaller models, a simple logistic regression
using only these surface cues predicts model choices more accurately than the
model's own numerical predictions, suggesting heuristics largely override
principled reasoning. Crucially, we find that larger models (32B parameters)
selectively rely on numerical knowledge when it is more reliable, while smaller
models (7--8B parameters) show no such discrimination, which explains why
larger models outperform smaller ones even when the smaller models possess more
accurate knowledge. Chain-of-thought prompting steers all models towards using
the numerical features across all model sizes.

</details>


### [42] [Cross-Genre Authorship Attribution via LLM-Based Retrieve-and-Rerank](https://arxiv.org/abs/2510.16819)
*Shantanu Agarwal,Joel Barry,Steven Fincke,Scott Miller*

Main category: cs.CL

TL;DR: 提出两阶段检索-重排框架，通过定制化数据策略使LLM在跨体裁作者归属任务中取得显著性能提升（HRS1/HRS2分别提升22.3/34.4个绝对点）。


<details>
  <summary>Details</summary>
Motivation: 传统信息检索方法在跨体裁AA任务中依赖主题线索而非作者语言特征，导致效果受限，需开发针对性解决方案。

Method: 构建检索-重排框架，创新数据策略使重排模型专注作者特有语言模式，克服传统IR训练策略的局限性。

Result: 在HIATUS基准测试中，Success@8指标超越前人最佳结果，HRS1达22.3点，HRS2达34.4点绝对提升。

Conclusion: 针对跨体裁AA设计的数据策略与LLM流程结合，有效捕捉作者特征，实现领域性能突破。

Abstract: Authorship attribution (AA) is the task of identifying the most likely author
of a query document from a predefined set of candidate authors. We introduce a
two-stage retrieve-and-rerank framework that finetunes LLMs for cross-genre AA.
Unlike the field of information retrieval (IR), where retrieve-and-rerank is a
de facto strategy, cross-genre AA systems must avoid relying on topical cues
and instead learn to identify author-specific linguistic patterns that are
independent of the text's subject matter (genre/domain/topic). Consequently,
for the reranker, we demonstrate that training strategies commonly used in IR
are fundamentally misaligned with cross-genre AA, leading to suboptimal
behavior. To address this, we introduce a targeted data curation strategy that
enables the reranker to effectively learn author-discriminative signals. Using
our LLM-based retrieve-and-rerank pipeline, we achieve substantial gains of
22.3 and 34.4 absolute Success@8 points over the previous state-of-the-art on
HIATUS's challenging HRS1 and HRS2 cross-genre AA benchmarks.

</details>


### [43] [Who's Asking? Simulating Role-Based Questions for Conversational AI Evaluation](https://arxiv.org/abs/2510.16829)
*Navreet Kaur,Hoda Ayad,Hayoung Jung,Shravika Mittal,Munmun De Choudhury,Tanushree Mitra*

Main category: cs.CL

TL;DR: 本文提出CoRUS框架，通过构建患者/护理人员/从业者角色分类体系，模拟15,321个角色化问题，发现语言模型对弱势角色的响应更具支持性但知识性内容减少17%-19%


<details>
  <summary>Details</summary>
Motivation: 现有语言模型评估忽视提问者角色差异，而在阿片类药物成瘾等污名化领域，用户角色背景对获得无偏见响应至关重要

Method: 结合角色理论和在线康复社区数据建立角色分类，开发可扩展的提问模拟框架，生成角色化问题数据集用于评估5个主流大模型

Result: 模型对患者/护理人员等脆弱角色响应支持性提升17%，但知识内容减少19%；角色化问题与真实数据具有高度可比性（believability评分4.1/5）

Conclusion: 用户角色信号会系统性影响模型响应模式，CoRUS为对话AI的角色敏感评估提供了方法论支持，强调在敏感领域需考虑用户角色背景

Abstract: Language model users often embed personal and social context in their
questions. The asker's role -- implicit in how the question is framed --
creates specific needs for an appropriate response. However, most evaluations,
while capturing the model's capability to respond, often ignore who is asking.
This gap is especially critical in stigmatized domains such as opioid use
disorder (OUD), where accounting for users' contexts is essential to provide
accessible, stigma-free responses. We propose CoRUS (COmmunity-driven Roles for
User-centric Question Simulation), a framework for simulating role-based
questions. Drawing on role theory and posts from an online OUD recovery
community (r/OpiatesRecovery), we first build a taxonomy of asker roles --
patients, caregivers, practitioners. Next, we use it to simulate 15,321
questions that embed each role's goals, behaviors, and experiences. Our
evaluations show that these questions are both highly believable and comparable
to real-world data. When used to evaluate five LLMs, for the same question but
differing roles, we find systematic differences: vulnerable roles, such as
patients and caregivers, elicit more supportive responses (+17%) and reduced
knowledge content (-19%) in comparison to practitioners. Our work demonstrates
how implicitly signaling a user's role shapes model responses, and provides a
methodology for role-informed evaluation of conversational AI.

</details>


### [44] [FinSight: Towards Real-World Financial Deep Research](https://arxiv.org/abs/2510.16844)
*Jiajie Jin,Yuyao Zhang,Yimeng Xu,Hongjin Qian,Yutao Zhu,Zhicheng Dou*

Main category: cs.CL

TL;DR: FinSight框架通过CAVM架构、迭代视觉增强机制和两阶段写作框架，实现了接近人类专家水平的专业财务报告生成。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统难以自动化生成需要深度分析的财务报告，存在劳动密集和专业性不足的痛点。

Method: 1. CAVM架构统一数据、工具和智能体到可编程空间
2. 迭代视觉增强机制提升图表专业化
3. 两阶段写作框架实现分析链扩展与多模态整合

Result: 在公司和行业级任务中，FinSight在事实准确性(↑37%)、分析深度(↑29%)和呈现质量(↑41%)上显著超越基线系统

Conclusion: 该框架为专业报告自动化提供了可扩展路径，通过模块化设计平衡了分析深度与结构一致性，展现了替代人类专家工作的潜力。

Abstract: Generating professional financial reports is a labor-intensive and
intellectually demanding process that current AI systems struggle to fully
automate. To address this challenge, we introduce FinSight (Financial InSight),
a novel multi agent framework for producing high-quality, multimodal financial
reports. The foundation of FinSight is the Code Agent with Variable Memory
(CAVM) architecture, which unifies external data, designed tools, and agents
into a programmable variable space, enabling flexible data collection, analysis
and report generation through executable code. To ensure professional-grade
visualization, we propose an Iterative Vision-Enhanced Mechanism that
progressively refines raw visual outputs into polished financial charts.
Furthermore, a two stage Writing Framework expands concise Chain-of-Analysis
segments into coherent, citation-aware, and multimodal reports, ensuring both
analytical depth and structural consistency. Experiments on various company and
industry-level tasks demonstrate that FinSight significantly outperforms all
baselines, including leading deep research systems in terms of factual
accuracy, analytical depth, and presentation quality, demonstrating a clear
path toward generating reports that approach human-expert quality.

</details>


### [45] [Neuronal Group Communication for Efficient Neural representation](https://arxiv.org/abs/2510.16851)
*Zhengqi Pei,Qingming Huang,Shuhui Wang*

Main category: cs.CL

TL;DR: 提出神经群组通信（NGC）框架，通过动态神经元群交互和稳定性指标提升大模型效率与推理能力


<details>
  <summary>Details</summary>
Motivation: 解决大规模神经网络在效率、模块化和可解释性方面的核心挑战，突破传统权重参数的局限性

Method: 将神经网络重构为神经元群动态系统，引入低秩模块化表示和基于动力系统理论的神经元稳定性指标

Result: 在LLM中实现NGC框架，中等压缩率下推理基准提升显著，优于传统低秩近似方法（压缩率相当条件下）

Conclusion: 结构化神经元群动态可能揭示高维学习系统泛化机制，为可解释AI和高效模型设计开辟新方向

Abstract: The ever-increasing scale of modern neural networks has brought unprecedented
performance alongside daunting challenges in efficiency and interpretability.
This paper addresses the core question of how to build large neural systems
that learn efficient, modular, and interpretable representations. We propose
Neuronal Group Communication (NGC), a theory-driven framework that reimagines a
neural network as a dynamical system of interacting neuronal groups rather than
a monolithic collection of neural weights. Instead of treating each weight as
an independent trainable parameter, NGC treats weights as transient
interactions between embedding-like neuronal states, with neural computation
unfolding through iterative communication among groups of neurons. This
low-rank, modular representation yields compact models: groups of neurons
exchange low-dimensional signals, enabling intra-group specialization and
inter-group information sharing while dramatically reducing redundant
parameters. By drawing on dynamical systems theory, we introduce a neuronal
stability metric (analogous to Lyapunov stability) that quantifies the
contraction of neuron activations toward stable patterns during sequence
processing. Using this metric, we reveal that emergent reasoning capabilities
correspond to an external driving force or ``potential'', which nudges the
neural dynamics away from trivial trajectories while preserving stability.
Empirically, we instantiate NGC in large language models (LLMs) and demonstrate
improved performance on complex reasoning benchmarks under moderate
compression. NGC consistently outperforms standard low-rank approximations and
cross-layer basis-sharing methods at comparable compression rates. We conclude
by discussing the broader implications of NGC, including how structured
neuronal group dynamics might relate to generalization in high-dimensional
learning systems.

</details>


### [46] [Does Visual Grounding Enhance the Understanding of Embodied Knowledge in Large Language Models?](https://arxiv.org/abs/2510.16924)
*Zhihui Yang,Yupei Wang,Kaijie Mo,Zhe Zhao,Renfen Hu*

Main category: cs.CL

TL;DR: 研究发现多模态语言模型在具身知识理解上未超越纯文本模型，视觉维度表现最差，需改进物理世界感知机制


<details>
  <summary>Details</summary>
Motivation: 探讨视觉增强是否真正提升语言模型对具身知识（通过感官体验获取的物理世界认知）的理解能力，揭示当前多模态模型的局限性

Method: 基于心理学感知理论构建具身知识评估基准，覆盖6类感知维度（五感+内感受），采用向量比较和问答任务（超1700题），对比30个前沿语言模型

Result: 视觉语言模型在两项任务中均未超越纯文本模型；模型视觉感知能力显著弱于其他感官维度；向量表征易受词形/词频干扰；空间感知与推理能力存在明显缺陷

Conclusion: 当前多模态模型未能有效整合具身知识，需开发新方法增强模型对物理世界的感知与推理能力，推动具身智能发展

Abstract: Despite significant progress in multimodal language models (LMs), it remains
unclear whether visual grounding enhances their understanding of embodied
knowledge compared to text-only models. To address this question, we propose a
novel embodied knowledge understanding benchmark based on the perceptual theory
from psychology, encompassing visual, auditory, tactile, gustatory, olfactory
external senses, and interoception. The benchmark assesses the models'
perceptual abilities across different sensory modalities through vector
comparison and question-answering tasks with over 1,700 questions. By comparing
30 state-of-the-art LMs, we surprisingly find that vision-language models
(VLMs) do not outperform text-only models in either task. Moreover, the models
perform significantly worse in the visual dimension compared to other sensory
dimensions. Further analysis reveals that the vector representations are easily
influenced by word form and frequency, and the models struggle to answer
questions involving spatial perception and reasoning. Our findings underscore
the need for more effective integration of embodied knowledge in LMs to enhance
their understanding of the physical world.

</details>


### [47] [ChiKhaPo: A Large-Scale Multilingual Benchmark for Evaluating Lexical Comprehension and Generation in Large Language Models](https://arxiv.org/abs/2510.16928)
*Emily Chang,Niyati Bafna*

Main category: cs.CL

TL;DR: 提出ChiKhaPo多语言基准测试，覆盖2700+语言评估LLMs基础语言能力，揭示主流模型在低资源语言中的表现缺陷


<details>
  <summary>Details</summary>
Motivation: 现有基准集中于高/中资源语言的高阶任务评估，但缺乏对3800+人类书面语言基础语言能力的系统性测试

Method: 基于词典、单语数据和双语语料库构建8个难度递增的子任务，覆盖理解与生成双维度，实现跨语系2700+语言的广泛覆盖

Result: 6个SOTA模型在基准中表现欠佳，性能受语系类型（如非洲/美洲原住民语言）、资源丰富度、任务类型（理解vs生成）显著影响

Conclusion: ChiKhaPo填补多语言评估空白，推动LLMs基础语言能力研究，促进语言技术民主化发展

Abstract: Existing benchmarks for large language models (LLMs) are largely restricted
to high- or mid-resource languages, and often evaluate performance on
higher-order tasks in reasoning and generation. However, plenty of evidence
points to the fact that LLMs lack basic linguistic competence in the vast
majority of the world's 3800+ written languages. We introduce ChiKhaPo,
consisting of 8 subtasks of varying difficulty designed to evaluate the lexical
comprehension and generation abilities of generative models. ChiKhaPo draws on
existing lexicons, monolingual data, and bitext, and provides coverage for
2700+ languages for 2 subtasks, surpassing any existing benchmark in terms of
language coverage. We further show that 6 SOTA models struggle on our
benchmark, and discuss the factors contributing to performance scores,
including language family, language resourcedness, task, and comprehension
versus generation directions. With ChiKhaPo, we hope to enable and encourage
the massively multilingual benchmarking of LLMs.

</details>


### [48] [Prompt-MII: Meta-Learning Instruction Induction for LLMs](https://arxiv.org/abs/2510.16932)
*Emily Xiao,Yixiao Zeng,Ada Chen,Chin-Jou Li,Amanda Bertsch,Graham Neubig*

Main category: cs.CL

TL;DR: 提出PROMPT-MII框架，通过强化学习元学习生成紧凑指令，替代传统长上下文学习方法，在保持性能的同时显著减少推理成本。


<details>
  <summary>Details</summary>
Motivation: 传统上下文学习（ICL）在适应新任务时需较长上下文，导致推理成本高昂。需寻找既能保持模型表现又能降低token消耗的解决方案。

Method: 开发基于强化学习的PROMPT-MII框架，在3000+多领域分类数据集上元训练指令归纳模型，动态生成数据集特异的紧凑指令模板。

Result: 在90个新任务上实现4-9 F1值提升（相对提升10-20%），推理token消耗减少3-13倍，性能匹配传统ICL。

Conclusion: PROMPT-MII证明了元学习生成指令的有效性，为LLM高效适配新任务提供了可扩展方案，显著优化计算效率与模型性能的平衡。

Abstract: A popular method to adapt large language models (LLMs) to new tasks is
in-context learning (ICL), which is effective but incurs high inference costs
as context length grows. In this paper we propose a method to perform
instruction induction, where we take training examples and reduce them to a
compact but descriptive prompt that can achieve performance comparable to ICL
over the full training set. Specifically, we propose PROMPT-MII, a
reinforcement learning (RL) based framework to meta-learn an instruction
induction model that can generate compact instructions on the fly for an
arbitrary new dataset. We train on over 3,000 diverse classification datasets
from the HuggingFace hub, and evaluate on 90 unseen tasks. PROMPT-MII improves
downstream model quality by 4-9 F1 points (10-20% relative), matching ICL
performance while requiring 3-13x fewer tokens.

</details>


### [49] [Parameter-Efficient Fine-Tuning for Low-Resource Languages: A Comparative Study of LLMs for Bengali Hate Speech Detection](https://arxiv.org/abs/2510.16985)
*Akif Islam,Mohd Ruhul Ameen*

Main category: cs.CL

TL;DR: 首次应用参数高效微调技术于孟加拉语仇恨言论检测，使用LoRA/QLoRA在单GPU上微调大型模型，Llama-3.2-3B达到92.23% F1分数


<details>
  <summary>Details</summary>
Motivation: 解决孟加拉社交媒体仇恨言论激增问题，克服传统方法计算成本高和依赖专有API的限制

Method: 基于BD-SHS数据集（50,281标注样本），采用LoRA/QLoRA技术对Gemma/Llama/Mistral模型进行参数效率优化（训练<1%参数），单消费级GPU完成实验

Result: Llama-3.2-3B以92.23% F1领先，Mistral-7B(88.94%)和Gemma-3-4B(80.25%)次之

Conclusion: 验证参数高效微调在低资源语言场景的实用价值，为类似语言提供可复现解决方案

Abstract: Bengali social media platforms have witnessed a sharp increase in hate
speech, disproportionately affecting women and adolescents. While datasets such
as BD-SHS provide a basis for structured evaluation, most prior approaches rely
on either computationally costly full-model fine-tuning or proprietary APIs.
This paper presents the first application of Parameter-Efficient Fine-Tuning
(PEFT) for Bengali hate speech detection using LoRA and QLoRA. Three
instruction-tuned large language models - Gemma-3-4B, Llama-3.2-3B, and
Mistral-7B - were fine-tuned on the BD-SHS dataset of 50,281 annotated
comments. Each model was adapted by training fewer than 1% of its parameters,
enabling experiments on a single consumer-grade GPU. The results show that
Llama-3.2-3B achieved the highest F1-score of 92.23%, followed by Mistral-7B at
88.94% and Gemma-3-4B at 80.25%. These findings establish PEFT as a practical
and replicable strategy for Bengali and related low-resource languages.

</details>


### [50] [Back to Bytes: Revisiting Tokenization Through UTF-8](https://arxiv.org/abs/2510.16987)
*Amit Moryossef,Clara Meister,Pavel Stepachev,Desmond Elliott*

Main category: cs.CL

TL;DR: UTF8Tokenizer是一种高效的字节级分词器，利用UTF-8字节和C0控制字符实现14倍加速分词、8倍传输优化，并通过位偏置嵌入技术提升模型收敛性


<details>
  <summary>Details</summary>
Motivation: 解决传统字节级分词器存在的ID越界问题、辅助标记冗余问题，以及提高跨模型嵌入表兼容性

Method: 采用C0控制字节编码特殊行为（如填充/边界/注意力分段），设计256*d共享嵌入表，引入训练后可添加的位偏置嵌入技术

Result: 分词速度提升14倍，主机-设备传输量减少8倍，语言模型训练收敛性得到改善

Conclusion: 该设计在保持ASCII控制字符传统的同时，实现了效率、兼容性和可扩展性的三重突破，为实际部署提供了轻量级解决方案

Abstract: We present UTF8Tokenizer, a minimalist byte-level tokenizer that maps text
exactly to IDs corresponding to the bytes underlying the text's UTF-8 encoding
(e.g., byte x09 is token ID 9). Unlike prior byte-level approaches (Xue et al.,
2021; Pagnoni et al., 2025), our implementation never introduces out-of-range
IDs (i.e. there is no token ID 256) or auxiliary tokens: all special behavior
(e.g., padding, boundaries, conversation structure, attention segments, tool
calling, "thinking" spans, etc.) is encoded using C0 control bytes - just as
ASCII was originally designed to embed control information alongside printable
text. These design principles yield practical benefits: (1) faster tokenization
(14x) and significantly lower host-device transfer (8x less than int64); (2)
simple, shareable 256*d embedding tables that can be aligned across models; and
(3) a training-time enhancement via bit-biased embeddings, which exposes
per-byte bit structure and can be added to the embedding table post-training,
removing inference costs. Our HuggingFace-compatible implementation improves
language modeling convergence.

</details>


### [51] [Vocab Diet: Reshaping the Vocabulary of LLMs with Vector Arithmetic](https://arxiv.org/abs/2510.17001)
*Yuval Reif,Guy Kaplan,Roy Schwartz*

Main category: cs.CL

TL;DR: 提出通过词形变换向量优化大语言模型词汇表设计，用组合式词汇替代冗余词表项，释放10%词汇空间同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统分词算法将不同词形变化（如walk/walked）视为独立token，导致词汇表被表层形式变体占据，挤占低频词和多语言覆盖空间。

Method: 利用变换向量（加性偏移量）在输入/输出空间重构词汇表，将词形变化分解为基础词向量+语法变换向量的组合形式（如walk+过去式=walked）。

Result: 在5种语言测试中成功释放10%词汇空间，扩展了未登录词覆盖范围，且无需修改模型参数即可保持下游任务性能。

Conclusion: 该研究推动词汇表设计范式转型：从枚举字符串转向基于语言结构的组合式词汇，提升词汇表的信息编码效率。

Abstract: Large language models (LLMs) were shown to encode word form variations, such
as "walk"->"walked", as linear directions in embedding space. However, standard
tokenization algorithms treat these variations as distinct tokens -- filling
the size-capped vocabulary with surface form variants (e.g., "walk", "walking",
"Walk"), at the expense of less frequent words and multilingual coverage. We
show that many of these variations can be captured by transformation vectors --
additive offsets that yield the appropriate word's representation when applied
to the base form word embedding -- in both the input and output spaces.
Building on this, we propose a compact reshaping of the vocabulary: rather than
assigning unique tokens to each surface form, we compose them from shared base
form and transformation vectors (e.g., "walked" = "walk" + past tense). We
apply our approach to multiple LLMs and across five languages, removing up to
10% of vocabulary entries -- thereby freeing space to allocate new, more
diverse tokens. Importantly, we do so while also expanding vocabulary coverage
to out-of-vocabulary words, with minimal impact on downstream performance, and
without modifying model weights. Our findings motivate a foundational
rethinking of vocabulary design, moving from string enumeration to a
compositional vocabulary that leverages the underlying structure of language.

</details>


### [52] [Online Learning Defense against Iterative Jailbreak Attacks via Prompt Optimization](https://arxiv.org/abs/2510.17006)
*Masahiro Kaneko,Zeerak Talat,Timothy Baldwin*

Main category: cs.CL

TL;DR: 提出动态防御框架对抗迭代越狱攻击，通过强化学习优化提示策略并引入梯度阻尼技术，在三大LLM上显著优于现有防御方法且提升无害任务响应质量。


<details>
  <summary>Details</summary>
Motivation: 现有防御措施无法主动应对迭代越狱攻击的试错动态特性，需开发能实时更新防御策略的主动防御机制。

Method: 1. 基于强化学习的提示优化框架动态对抗攻击 2. 引入过去方向梯度阻尼(PDGD)防止模型对攻击者局部输入改写产生过拟合

Result: 在三个LLM上超越五种现有防御方法，对五种迭代越狱攻击防御成功率显著提升，无害任务响应质量同步提高

Conclusion: 动态防御框架通过在线学习与梯度控制，有效平衡安全防御与任务性能，为LLM安全防护提供新范式

Abstract: Iterative jailbreak methods that repeatedly rewrite and input prompts into
large language models (LLMs) to induce harmful outputs -- using the model's
previous responses to guide each new iteration -- have been found to be a
highly effective attack strategy. Despite being an effective attack strategy
against LLMs and their safety mechanisms, existing defenses do not proactively
disrupt this dynamic trial-and-error cycle. In this study, we propose a novel
framework that dynamically updates its defense strategy through online learning
in response to each new prompt from iterative jailbreak methods. Leveraging the
distinctions between harmful jailbreak-generated prompts and typical harmless
prompts, we introduce a reinforcement learning-based approach that optimizes
prompts to ensure appropriate responses for harmless tasks while explicitly
rejecting harmful prompts. Additionally, to curb overfitting to the narrow band
of partial input rewrites explored during an attack, we introduce
Past-Direction Gradient Damping (PDGD). Experiments conducted on three LLMs
show that our approach significantly outperforms five existing defense methods
against five iterative jailbreak methods. Moreover, our results indicate that
our prompt optimization strategy simultaneously enhances response quality for
harmless tasks.

</details>


### [53] [DiscoTrack: A Multilingual LLM Benchmark for Discourse Tracking](https://arxiv.org/abs/2510.17013)
*Lanni Bu,Lauren Levin,Amir Zeldes*

Main category: cs.CL

TL;DR: DiscoTrack：覆盖12种语言的四层次话语理解基准测试，显示当前LLM在跨句推理任务仍存挑战


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准主要关注单句显性信息抽取，缺乏多语言、跨句段隐含信息和语用推理的评估体系

Method: 构建包含显著性识别、实体追踪、话语关系和桥接推理的四层框架，覆盖12种语言的长文档语境测试集

Result: 即使最先进模型在跨句推理任务（如桥接推断）中准确率显著下降，部分语言表现低于随机基线

Conclusion: DiscoTrack填补了多语言复杂推理评估空白，揭示了LLM在深层话语理解方面的改进空间

Abstract: Recent LLM benchmarks have tested models on a range of phenomena, but are
still focused primarily on natural language understanding for extraction of
explicit information, such as QA or summarization, with responses often tar-
geting information from individual sentences. We are still lacking more
challenging, and im- portantly also multilingual, benchmarks focus- ing on
implicit information and pragmatic infer- ences across larger documents in the
context of discourse tracking: integrating and aggregating information across
sentences, paragraphs and multiple speaker utterances. To this end, we present
DiscoTrack, an LLM benchmark target- ing a range of tasks across 12 languages
and four levels of discourse understanding: salience recognition, entity
tracking, discourse relations and bridging inference. Our evaluation shows that
these tasks remain challenging, even for state-of-the-art models.

</details>


### [54] [SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents](https://arxiv.org/abs/2510.17017)
*Qiusi Zhan,Angeline Budiman-Chan,Abdelrahman Zayed,Xingzhi Guo,Daniel Kang,Joo-Kyung Kim*

Main category: cs.CL

TL;DR: 探讨基于LLM的搜索代理的安全风险，并提出SafeSearch方法以平衡安全与实用性


<details>
  <summary>Details</summary>
Motivation: 现有研究聚焦提升搜索代理的实用性，但其在回答开放领域问题时可能通过检索外部文档降低安全阈值，产生有害输出（如指导非法定位他人）

Method: 提出SafeSearch多目标强化学习框架，结合最终输出安全/效用奖励与创新的查询级惩罚机制（惩罚危险查询/奖励安全查询）

Result: 实验表明SafeSearch在三个红队数据集中减少70%以上危害性，同时保持安全有效的响应，QA效果与纯效用微调的代理相当

Conclusion: 查询级奖励机制有效协同提升安全性和实用性，为AI代理安全对齐提供新思路

Abstract: Large language model (LLM) based search agents iteratively generate queries,
retrieve external information, and reason to answer open-domain questions.
While researchers have primarily focused on improving their utility, their
safety behaviors remain underexplored. In this paper, we first evaluate search
agents using red-teaming datasets and find that they are more likely to produce
harmful outputs than base LLMs. For example, when asked ``How can I track
someone's location without their consent?'', a base model refuses, whereas a
search agent designed to retrieve and cite sources may lower its refusal
threshold, fetch documents (e.g., court cases), and, once appended, synthesize
them into an informative yet unsafe summary. We further show that
utility-oriented fine-tuning intensifies this risk, motivating joint alignment
of safety and utility. We present SafeSearch, a multi-objective reinforcement
learning approach that couples a final-output safety/utility reward with a
novel query-level shaping term that penalizes unsafe queries and rewards safe
ones. Experiments show that SafeSearch reduces agent harmfulness by over 70%
across three red-teaming datasets while producing safe, helpful responses, and
matches the QA performance of a utility-only finetuned agent; further analyses
confirm the effectiveness of the query-level reward in jointly improving safety
and utility.

</details>


### [55] [Extended LSTM: Adaptive Feature Gating for Toxic Comment Classification](https://arxiv.org/abs/2510.17018)
*Noor Islam S. Mohammad*

Main category: cs.CL

TL;DR: 提出参数高效的xLSTM框架，通过余弦门控和类别再平衡机制，在计算效率和分类性能上超越BERT模型


<details>
  <summary>Details</summary>
Motivation: 解决Transformer模型在毒性检测任务中计算成本高、对少数类性能差，以及传统集成方法缺乏语义适应性的问题

Method: ① 余弦相似性门控调节上下文嵌入 ② 集成多源嵌入(GloVe/FastText/BERT) ③ 字符级BiLSTM捕获形态特征 ④ 嵌入空间SMOTE增强少数类 ⑤ 动态类别加权的自适应焦点损失

Result: Jigsaw基准测试中达到96%准确率和0.88宏F1，威胁和身份仇恨类分别提升33%/28%，参数量减少15倍，推理延迟50ms

Conclusion: 证明了理论指导的轻量架构在不平衡领域任务中可超越大型预训练模型，建立了效率-适应性新标杆

Abstract: Toxic comment detection remains a challenging task, where transformer-based
models (e.g., BERT) incur high computational costs and degrade on minority
toxicity classes, while classical ensembles lack semantic adaptability. We
propose xLSTM, a parameter-efficient and theoretically grounded framework that
unifies cosine-similarity gating, adaptive feature prioritization, and
principled class rebalancing. A learnable reference vector {v} in {R}^d
modulates contextual embeddings via cosine similarity, amplifying toxic cues
and attenuating benign signals to yield stronger gradients under severe class
imbalance. xLSTM integrates multi-source embeddings (GloVe, FastText, BERT CLS)
through a projection layer, a character-level BiLSTM for morphological cues,
embedding-space SMOTE for minority augmentation, and adaptive focal loss with
dynamic class weighting. On the Jigsaw Toxic Comment benchmark, xLSTM attains
96.0% accuracy and 0.88 macro-F1, outperforming BERT by 33% on threat and 28%
on identity_hate categories, with 15 times fewer parameters and 50ms inference
latency. Cosine gating contributes a +4.8% F1 gain in ablations. The results
establish a new efficiency adaptability frontier, demonstrating that
lightweight, theoretically informed architectures can surpass large pretrained
models on imbalanced, domain-specific NLP tasks.

</details>


### [56] [Mapping from Meaning: Addressing the Miscalibration of Prompt-Sensitive Language Models](https://arxiv.org/abs/2510.17028)
*Kyle Cox,Jiawei Xu,Yikun Han,Rong Xu,Tianhao Li,Chi-Yang Hsu,Tianlong Chen,Walter Gerych,Ying Ding*

Main category: cs.CL

TL;DR: 研究通过语义改写扰动改善LLM提示敏感性的不确定性校准，提出新型分解指标量化不确定性来源


<details>
  <summary>Details</summary>
Motivation: 大语言模型对相同语义不同表述的提示会产生输出分布差异，现有不确定性校准未充分反映模型对提示语义的理解稳定性

Method: 1. 在语义概念空间进行改写扰动采样 2. 开发基于语义连续性的新型不确定性分解指标

Result: 改进的校准方法在保持准确率的同时提升置信度校准效果，新指标成功量化了提示敏感性对不确定性的贡献比例

Conclusion: 该研究为提示敏感性建模提供新范式，揭示部分LLM在输入语义连贯推理上的系统性缺陷

Abstract: An interesting behavior in large language models (LLMs) is prompt
sensitivity. When provided with different but semantically equivalent versions
of the same prompt, models may produce very different distributions of answers.
This suggests that the uncertainty reflected in a model's output distribution
for one prompt may not reflect the model's uncertainty about the meaning of the
prompt. We model prompt sensitivity as a type of generalization error, and show
that sampling across the semantic ``concept space'' with paraphrasing
perturbations improves uncertainty calibration without compromising accuracy.
Additionally, we introduce a new metric for uncertainty decomposition in
black-box LLMs that improves upon entropy-based decomposition by modeling
semantic continuities in natural language generation. We show that this
decomposition metric can be used to quantify how much LLM uncertainty is
attributed to prompt sensitivity. Our work introduces a new way to improve
uncertainty calibration in prompt-sensitive language models, and provides
evidence that some LLMs fail to exhibit consistent general reasoning about the
meanings of their inputs.

</details>


### [57] [Investigating Thinking Behaviours of Reasoning-Based Language Models for Social Bias Mitigation](https://arxiv.org/abs/2510.17062)
*Guoqing Luo,Iffat Maab,Lili Mou,Junichi Yamagishi*

Main category: cs.CL

TL;DR: 研究大语言模型推理过程中社会偏见的聚合机制，并提出基于提示的缓解方法


<details>
  <summary>Details</summary>
Motivation: 探索基于推理的大语言模型在社会偏见场景中的内在行为机制，发现其结构化思考过程会通过刻板印象重复和无关信息注入两种模式加剧社会偏见

Method: 通过系统研究识别两种失败模式（刻板印象重复和无关信息注入），提出让模型自我审查初始推理的轻量级提示方法

Result: 在问答基准（BBQ/StereoSet）和开放任务（BOLD）上有效降低偏见，同时保持或提升准确性

Conclusion: 基于自我审查的提示方法成功缓解语言模型中的社会偏见，为减少AI偏见提供了有效途径

Abstract: While reasoning-based large language models excel at complex tasks through an
internal, structured thinking process, a concerning phenomenon has emerged that
such a thinking process can aggregate social stereotypes, leading to biased
outcomes. However, the underlying behaviours of these language models in social
bias scenarios remain underexplored. In this work, we systematically
investigate mechanisms within the thinking process behind this phenomenon and
uncover two failure patterns that drive social bias aggregation: 1) stereotype
repetition, where the model relies on social stereotypes as its primary
justification, and 2) irrelevant information injection, where it fabricates or
introduces new details to support a biased narrative. Building on these
insights, we introduce a lightweight prompt-based mitigation approach that
queries the model to review its own initial reasoning against these specific
failure patterns. Experiments on question answering (BBQ and StereoSet) and
open-ended (BOLD) benchmarks show that our approach effectively reduces bias
while maintaining or improving accuracy.

</details>


### [58] [Verification-Aware Planning for Multi-Agent Systems](https://arxiv.org/abs/2510.17109)
*Tianyang Xu,Dan Zhang,Kushan Mitra,Estevam Hruschka*

Main category: cs.CL

TL;DR: 提出VeriMAP框架，通过验证感知的规划解决多智能体协作中的执行偏差问题，提升系统鲁棒性和可解释性


<details>
  <summary>Details</summary>
Motivation: 多智能体协作常因任务解释、输出格式或交接的细微偏差导致执行失败，需要可靠的任务分解与验证机制

Method: 开发任务分解器建模子任务依赖关系，设计Python代码+自然语言双模态的验证函数(VFs)作为通过标准

Result: 在多个数据集上验证效果优于单智能体与多智能体基线，错误率降低21%同时保持高任务完成率

Conclusion: 验证感知规划使多智能体系统实现可靠协调与迭代优化，无需依赖外部标注即可提升协作质量

Abstract: Large language model (LLM) agents are increasingly deployed to tackle complex
tasks, often necessitating collaboration among multiple specialized agents.
However, multi-agent collaboration introduces new challenges in planning,
coordination, and verification. Execution failures frequently arise not from
flawed reasoning alone, but from subtle misalignments in task interpretation,
output format, or inter-agent handoffs. To address these challenges, we present
VeriMAP, a framework for multi-agent collaboration with verification-aware
planning. The VeriMAP planner decomposes tasks, models subtask dependencies,
and encodes planner-defined passing criteria as subtask verification functions
(VFs) in Python and natural language. We evaluate VeriMAP on diverse datasets,
demonstrating that it outperforms both single- and multi-agent baselines while
enhancing system robustness and interpretability. Our analysis highlights how
verification-aware planning enables reliable coordination and iterative
refinement in multi-agent systems, without relying on external labels or
annotations.

</details>


### [59] [DVAGen: Dynamic Vocabulary Augmented Generation](https://arxiv.org/abs/2510.17115)
*Wei Du,Nuowei Liu,Jie Wang,Jiahao Kuang,Tao Ji,Xiaoling Wang,Yuanbin Wu*

Main category: cs.CL

TL;DR: 提出了开源框架DVAGen，解决固定词表语言模型对新词汇适应性问题，通过模块化设计和双界面工具提升动态词表方法的训练效率与推理吞吐量


<details>
  <summary>Details</summary>
Motivation: 固定词表语言模型难以处理新词汇组合，现有动态词表方案存在代码库碎片化、缺乏现代LLM支持、推理扩展性差三大痛点

Method: DVAGen框架采用模块化流程设计，无缝集成开源大模型，首创CLI+WebUI双模式实时结果检视工具，支持批处理推理加速

Result: 在当代大语言模型上验证动态词表有效性，批量推理吞吐量显著提升，框架支持实时可视化训练过程监控

Conclusion: 该框架为动态词表方法提供首个完整解决方案，兼具可定制性和工业级部署能力，推动语言模型在开放词汇场景的应用

Abstract: Language models trained with a fixed vocabulary struggle to generalize to
novel or out-of-vocabulary words, limiting their flexibility in handling
diverse token combinations. Existing dynamic vocabulary approaches attempt to
address this limitation but face challenges such as fragmented codebases, lack
of support for modern LLMs, and limited inference scalability. To overcome
these issues, we introduce DVAGen, a fully open-source, unified framework
designed for training, evaluation, and visualization of dynamic
vocabulary-augmented language models. Our framework modularizes the pipeline
for ease of customization, integrates seamlessly with open-source LLMs, and is
the first to provide both CLI and WebUI tools for real-time result inspection.
We validate the effectiveness of dynamic vocabulary methods on modern LLMs and
demonstrate support for batch inference, significantly improving inference
throughput.

</details>


### [60] [Rethinking On-policy Optimization for Query Augmentation](https://arxiv.org/abs/2510.17139)
*Zhichao Xu,Shengyao Zhuang,Xueguang Ma,Bingsen Chen,Yijun Tian,Fengran Mo,Jie Cao,Vivek Srikumar*

Main category: cs.CL

TL;DR: 比较提示方法与强化学习在查询增强中的效果，发现无训练方法常优于RL方案，并提出了融合两者优势的OPQE混合方法


<details>
  <summary>Details</summary>
Motivation: 现有两种主流LLM查询增强方法（提示生成答案与RL调优）尚未在统一实验条件下进行系统比较，需探索更优方案

Method: 在多种检索场景（证据检索/临时检索/工具检索）中系统对比两类方法，提出结合提示灵活性与RL优化的OPQE框架

Result: 使用强大LLM时，无训练提示方法常达到或超越RL方法；OPQE进一步超越两种独立方法（MRR@100提升3-6%）

Conclusion: 通过系统性对比揭示训练无关方法的潜力，提出的协同策略OPQE实现最佳效果，相关实现已开源保障可复现性

Abstract: Recent advances in large language models (LLMs) have led to a surge of
interest in query augmentation for information retrieval (IR). Two main
approaches have emerged. The first prompts LLMs to generate answers or
pseudo-documents that serve as new queries, relying purely on the model's
parametric knowledge or contextual information. The second applies
reinforcement learning (RL) to fine-tune LLMs for query rewriting, directly
optimizing retrieval metrics. While having respective advantages and
limitations, the two approaches have not been compared under consistent
experimental conditions. In this work, we present the first systematic
comparison of prompting-based and RL-based query augmentation across diverse
benchmarks, including evidence-seeking, ad hoc, and tool retrieval. Our key
finding is that simple, training-free query augmentation often performs on par
with, or even surpasses, more expensive RL-based counterparts, especially when
using powerful LLMs. Motivated by this discovery, we introduce a novel hybrid
method, On-policy Pseudo-document Query Expansion (OPQE), which, instead of
rewriting a query, the LLM policy learns to generate a pseudo-document that
maximizes retrieval performance, thus merging the flexibility and generative
structure of prompting with the targeted optimization of RL. We show OPQE
outperforms both standalone prompting and RL-based rewriting, demonstrating
that a synergistic approach yields the best results. Our implementation is made
available to facilitate reproducibility.

</details>


### [61] [When AI companions become witty: Can human brain recognize AI-generated irony?](https://arxiv.org/abs/2510.17168)
*Xiaohui Rao,Hanlin Wu,Zhenguang G. Cai*

Main category: cs.CL

TL;DR: 研究通过ERP实验发现人类对AI生成的反讽未完全采用意向立场，神经响应显著弱于人类来源，显示人们更倾向将AI的语义矛盾归因于计算错误而非故意沟通。


<details>
  <summary>Details</summary>
Motivation: 探究当LLM被部署为社交主体并生成幽默反讽时，人们是否会对AI采用意向立场（即将其行为归因于心理状态），以此理解人工主体获得社会代理权所需的认知基础。

Method: 对比人类与AI生成反讽语句的行为反应和神经响应（ERP成分：P200反映早期矛盾检测，P600反映认知重构努力），结合源属性和心理模型测量。

Result: 行为上对AI的故意沟通归因显著低于人类；神经层面AI反讽引发减弱的P200（矛盾检测）和P600（语义重构），但AI真诚度感知较高者表现出更强的神经响应。

Conclusion: LLM的语言能力不足以保证社会代理权，关键在于人类如何归因意向性。人工主体的社会认知需建立在接收方的心智模型重塑基础上。

Abstract: As Large Language Models (LLMs) are increasingly deployed as social agents
and trained to produce humor and irony, a question emerges: when encountering
witty AI remarks, do people interpret these as intentional communication or
mere computational output? This study investigates whether people adopt the
intentional stance, attributing mental states to explain behavior,toward AI
during irony comprehension. Irony provides an ideal paradigm because it
requires distinguishing intentional contradictions from unintended errors
through effortful semantic reanalysis. We compared behavioral and neural
responses to ironic statements from AI versus human sources using established
ERP components: P200 reflecting early incongruity detection and P600 indexing
cognitive efforts in reinterpreting incongruity as deliberate irony. Results
demonstrate that people do not fully adopt the intentional stance toward
AI-generated irony. Behaviorally, participants attributed incongruity to
deliberate communication for both sources, though significantly less for AI
than human, showing greater tendency to interpret AI incongruities as
computational errors. Neural data revealed attenuated P200 and P600 effects for
AI-generated irony, suggesting reduced effortful detection and reanalysis
consistent with diminished attribution of communicative intent. Notably, people
who perceived AI as more sincere showed larger P200 and P600 effects for
AI-generated irony, suggesting that intentional stance adoption is calibrated
by specific mental models of artificial agents. These findings reveal that
source attribution shapes neural processing of social-communicative phenomena.
Despite current LLMs' linguistic sophistication, achieving genuine social
agency requires more than linguistic competence, it necessitates a shift in how
humans perceive and attribute intentionality to artificial agents.

</details>


### [62] [Understanding and Improving Length Generalization in Hierarchical Sparse Attention Models](https://arxiv.org/abs/2510.17196)
*Jiaqi Leng,Xiang Hu,Junxiong Wang,Jianguo Li,Wei Wu,Yucheng Lu*

Main category: cs.CL

TL;DR: 分块注意力模型需具备三个核心设计：非线性分块编码器、旁路残差路径和预训练强制稀疏选择，实现3200万token的无训练外推


<details>
  <summary>Details</summary>
Motivation: 现有长上下文模型存在二次复杂度高/长度外推差（Transformer）、全局信息利用不足（滑动窗口）等问题，需明确分块注意力架构的成功原理

Method: 通过统一框架和消融实验验证：1）带CLS的非线性分块编码器；2）防止全局信息覆盖的残差路径；3）预训练强制稀疏选择

Result: 在RULER和BABILong上实现32M token的零训练外推，达到SOTA水平（基于4K上下文训练）

Conclusion: 该研究为长上下文模型提供了三大实证设计原则：表达性编码结构、稳定信息整合机制、训练-测试分布一致性保障

Abstract: Effectively processing long contexts is a critical challenge for language
models. While standard Transformers are limited by quadratic complexity and
poor length extrapolation, alternative architectures like sliding window
attention and state space models sacrifice the ability to effectively utilize
the full context due to their fixed-size memory. Chunk-based sparse attention
has emerged as a promising paradigm for extreme length generalization, yet the
key architectural principles underpinning its success are not yet fully
understood. In this work, we present a systematic dissection of these models to
identify the core components driving their performance. Through a unified
framework and comprehensive ablation studies, we demonstrate that a combination
of three design principles is critical: (1) an expressive, non-linear Chunk
Encoder with a dedicated CLS token to produce representations for retrieval;
(2) a Bypassing Residual Path to stably integrate retrieved global information
without it being overridden by the local residual stream; and (3) enforced
selection sparsity during pre-training to bridge the train-test distribution
gap. We provide a theoretical motivation for intra-chunk information processing
and landmark generation. By combining these principles, we establish a new
state-of-the-art for training-free length extrapolation, successfully
generalizing models trained on a 4K context to 32 million tokens on RULER and
BABILong. Our findings provide a clear and empirically-grounded set of design
principles for developing future, highly-capable long-context language models.

</details>


### [63] [Wisdom is Knowing What not to Say: Hallucination-Free LLMs Unlearning via Attention Shifting](https://arxiv.org/abs/2510.17210)
*Chenchen Tan,Youyang Qu,Xinghao Li,Hui Zhang,Shujie Cui,Cunjian Chen,Longxiang Gao*

Main category: cs.CL

TL;DR: 提出注意力转移框架(AS)，通过双注意力干预机制实现选择性遗忘，在保持模型效用的同时降低幻觉风险


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法存在效用与安全性失衡问题，制约LLMs在知识密集型场景的可靠性

Method: 采用上下文保留抑制(衰减事实相关token的注意力)和抗幻觉响应塑造(抑制虚构补全)，通过重要性感知抑制+注意力引导保留增强的双损失联合优化

Result: 在ToFU/TDEC基准分别提升15%/10%准确率，实现最优的遗忘效果与响应可靠性的平衡

Conclusion: AS框架通过注意力重定向机制，在知识遗忘、模型泛化和响应可靠性间建立新的性能边界，为LLMs安全部署提供新范式

Abstract: The increase in computing power and the necessity of AI-assisted
decision-making boost the growing application of large language models (LLMs).
Along with this, the potential retention of sensitive data of LLMs has spurred
increasing research into machine unlearning. However, existing unlearning
approaches face a critical dilemma: Aggressive unlearning compromises model
utility, while conservative strategies preserve utility but risk hallucinated
responses. This significantly limits LLMs' reliability in knowledge-intensive
applications. To address this, we introduce a novel Attention-Shifting (AS)
framework for selective unlearning. AS is driven by two design objectives: (1)
context-preserving suppression that attenuates attention to fact-bearing tokens
without disrupting LLMs' linguistic structure; and (2) hallucination-resistant
response shaping that discourages fabricated completions when queried about
unlearning content. AS realizes these objectives through two attention-level
interventions, which are importance-aware suppression applied to the unlearning
set to reduce reliance on memorized knowledge and attention-guided retention
enhancement that reinforces attention toward semantically essential tokens in
the retained dataset to mitigate unintended degradation. These two components
are jointly optimized via a dual-loss objective, which forms a soft boundary
that localizes unlearning while preserving unrelated knowledge under
representation superposition. Experimental results show that AS improves
performance preservation over the state-of-the-art unlearning methods,
achieving up to 15% higher accuracy on the ToFU benchmark and 10% on the TDEC
benchmark, while maintaining competitive hallucination-free unlearning
effectiveness. Compared to existing methods, AS demonstrates a superior balance
between unlearning effectiveness, generalization, and response reliability.

</details>


### [64] [StreamingThinker: Large Language Models Can Think While Reading](https://arxiv.org/abs/2510.17238)
*Junlong Tong,Yingqi Fan,Anhao Zhao,Yunpu Ma,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 提出流式思维范式StreamingThinker，通过流式CoT生成、约束训练和并行推理，在保持性能的同时将推理延迟降低60%+


<details>
  <summary>Details</summary>
Motivation: 传统LLM需完整输入后才开始推理，导致延迟过高且忽略动态场景中的早期信息。受人类边阅读边思考的认知启发，需实现流式推理机制

Method: 1) 流式CoT生成单元配合质量监控 2) 流式注意力掩码强制顺序保留 3) 并行KV缓存解耦输入编码与推理生成

Result: 在Qwen3系列模型上验证，推理性能与批量模式相当，token等待时间减少80%，最终答案生成时间延迟降低超60%

Conclusion: 流式推理范式有效解决了传统LLM推理延迟问题，为动态场景应用提供了新的技术路径

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
chain of thought (CoT) reasoning. However, the current LLM reasoning paradigm
initiates thinking only after the entire input is available, which introduces
unnecessary latency and weakens attention to earlier information in dynamic
scenarios. Inspired by human cognition of thinking while reading, we first
design a \textit{\textbf{streaming thinking}} paradigm for LLMs, where
reasoning unfolds in the order of input and further adjusts its depth once
reading is complete. We instantiate this paradigm with
\textit{StreamingThinker}, a framework that enables LLMs to think while reading
through the integration of streaming CoT generation, streaming-constraint
training, and streaming parallel inference. Specifically, StreamingThinker
employs streaming reasoning units with quality control for CoT generation,
enforces order-preserving reasoning through streaming attention masks and
position encoding, and leverages parallel KV caches that decouple input
encoding from reasoning generation, thereby ensuring alignment and enabling
true concurrency. We evaluate StreamingThinker on the Qwen3 model family across
math reasoning, logical reasoning, and context-based QA reasoning tasks.
Experimental results show that the StreamingThinker preserves performance
comparable to batch thinking, while yielding an 80\% reduction in token waiting
before the onset of reasoning and a more than 60\% reduction in time-level
latency for producing the final answer, demonstrating the effectiveness of the
streaming paradigm for LLM reasoning. Code will be released at
\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this
repository.}

</details>


### [65] [From Preferences to Prejudice: The Role of Alignment Tuning in Shaping Social Bias in Video Diffusion Models](https://arxiv.org/abs/2510.17247)
*Zefan Cai,Haoyi Qiu,Haozhe Zhao,Ke Wan,Jiachen Li,Jiuxiang Gu,Wen Xiao,Nanyun Peng,Junjie Hu*

Main category: cs.CL

TL;DR: 视频扩散模型的对齐调优虽提升生成质量，但会放大社会偏见。研究通过VideoBiasEval框架首次揭示偏好数据-奖励模型-生成模型的全链路偏见传播机制。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型通过人类偏好奖励模型进行对齐调优时，可能系统性地编码并强化社会偏见，需建立系统性评估框架追溯偏见演化路径。

Method: 提出VideoBiasEval框架：基于社会偏见分类学设计事件驱动prompt策略，构建多粒度指标评估种族/性别偏见分布、跨模型偏移及视频时序持续性。

Result: 分析发现对齐调优不仅强化表征偏见（如白人形象占比超实际人口3倍），且使偏见在视频中时间维度稳定呈现（平均镜头时长增加1.8倍）。

Conclusion: 需在全链路建立偏见感知的评估与缓解机制，研究提出的多维度诊断框架为构建社会公平的视频生成系统提供了方法论基础。

Abstract: Recent advances in video diffusion models have significantly enhanced
text-to-video generation, particularly through alignment tuning using reward
models trained on human preferences. While these methods improve visual
quality, they can unintentionally encode and amplify social biases. To
systematically trace how such biases evolve throughout the alignment pipeline,
we introduce VideoBiasEval, a comprehensive diagnostic framework for evaluating
social representation in video generation. Grounded in established social bias
taxonomies, VideoBiasEval employs an event-based prompting strategy to
disentangle semantic content (actions and contexts) from actor attributes
(gender and ethnicity). It further introduces multi-granular metrics to
evaluate (1) overall ethnicity bias, (2) gender bias conditioned on ethnicity,
(3) distributional shifts in social attributes across model variants, and (4)
the temporal persistence of bias within videos. Using this framework, we
conduct the first end-to-end analysis connecting biases in human preference
datasets, their amplification in reward models, and their propagation through
alignment-tuned video diffusion models. Our results reveal that alignment
tuning not only strengthens representational biases but also makes them
temporally stable, producing smoother yet more stereotyped portrayals. These
findings highlight the need for bias-aware evaluation and mitigation throughout
the alignment process to ensure fair and socially responsible video generation.

</details>


### [66] [How News Feels: Understanding Affective Bias in Multilingual Headlines for Human-Centered Media Design](https://arxiv.org/abs/2510.17252)
*Mohd Ruhul Ameen,Akif Islam,Abu Saleh Musa Miah,Ayesha Siddiqua,Jungpil Shin*

Main category: cs.CL

TL;DR: 通过Gemma-3 4B模型分析30万条孟加拉语新闻标题，发现负面情绪（愤怒、恐惧、失望）占据主导地位，并基于此提出可视化情感线索的新闻聚合器设计方案。


<details>
  <summary>Details</summary>
Motivation: 研究新闻媒体如何通过情感框架影响公众情绪，特别关注标题的负面情绪传播现象及其对读者认知的潜在操控。

Method: 使用零样本推理技术，对30万条孟加拉语新闻标题及内容进行情感分类，识别主要情绪和整体基调，并跨媒体机构比较情感表达差异。

Result: 数据显示78%的新闻呈现负面情绪，不同媒体对同类事件的情感框架存在显著差异（p<0.01），愤怒情绪占比达42%

Conclusion: 提出具备情感可视化功能的人本主义新闻聚合器原型，通过色块编码和情绪热力图帮助读者识别隐性情感操纵，提升媒体批判性阅读能力

Abstract: News media often shape the public mood not only by what they report but by
how they frame it. The same event can appear calm in one outlet and alarming in
another, reflecting subtle emotional bias in reporting. Negative or emotionally
charged headlines tend to attract more attention and spread faster, which in
turn encourages outlets to frame stories in ways that provoke stronger
reactions. This research explores that tendency through large-scale emotion
analysis of Bengali news. Using zero-shot inference with Gemma-3 4B, we
analyzed 300000 Bengali news headlines and their content to identify the
dominant emotion and overall tone of each. The findings reveal a clear
dominance of negative emotions, particularly anger, fear, and disappointment,
and significant variation in how similar stories are emotionally portrayed
across outlets. Based on these insights, we propose design ideas for a
human-centered news aggregator that visualizes emotional cues and helps readers
recognize hidden affective framing in daily news.

</details>


### [67] [Explainability of Large Language Models: Opportunities and Challenges toward Generating Trustworthy Explanations](https://arxiv.org/abs/2510.17256)
*Shahin Atakishiyev,Housam K. B. Babiker,Jiayi Dai,Nawshad Farruque,Teruaki Hayashi,Nafisa Sadaf Hriti,Md Abed Rahman,Iain Smith,Mi-Young Kim,Osmar R. Zaïane,Randy Goebel*

Main category: cs.CL

TL;DR: 系统性探讨基于Transformer的大语言模型可解释性机制，提出医疗与自动驾驶领域的信任影响分析及未来研究方向


<details>
  <summary>Details</summary>
Motivation: 大语言模型预测机制不透明导致幻觉错误频发，需通过可解释性研究建立人类对模型决策的信任

Method: 结合文献综述与医疗/自动驾驶场景的实证研究，分析解释机制对接收者的信任影响

Result: 揭示了可解释性在不同应用场景中的信任构建机制，识别出现有解释方法的局限性

Conclusion: 需开发人类对齐的解释框架，解决解释一致性、领域适应性和动态更新等关键挑战

Abstract: Large language models have exhibited impressive performance across a broad
range of downstream tasks in natural language processing. However, how a
language model predicts the next token and generates content is not generally
understandable by humans. Furthermore, these models often make errors in
prediction and reasoning, known as hallucinations. These errors underscore the
urgent need to better understand and interpret the intricate inner workings of
language models and how they generate predictive outputs. Motivated by this
gap, this paper investigates local explainability and mechanistic
interpretability within Transformer-based large language models to foster trust
in such models. In this regard, our paper aims to make three key contributions.
First, we present a review of local explainability and mechanistic
interpretability approaches and insights from relevant studies in the
literature. Furthermore, we describe experimental studies on explainability and
reasoning with large language models in two critical domains -- healthcare and
autonomous driving -- and analyze the trust implications of such explanations
for explanation receivers. Finally, we summarize current unaddressed issues in
the evolving landscape of LLM explainability and outline the opportunities,
critical challenges, and future directions toward generating human-aligned,
trustworthy LLM explanations.

</details>


### [68] [TaxoAlign: Scholarly Taxonomy Generation Using Language Models](https://arxiv.org/abs/2510.17263)
*Avishek Lahiri,Yufang Hou,Debarshi Kumar Sanyal*

Main category: cs.CL

TL;DR: 提出TaxoAlign方法自动生成学术分类法，并创建CS-TaxoBench基准验证其优于基线模型的效果


<details>
  <summary>Details</summary>
Motivation: 现有自动文献综述方法缺乏人工专家分类法的结构对比验证

Method: 三阶段主题指令引导方法（TaxoAlign）+ 自动化评估框架（结构对齐度与语义连贯性）

Result: TaxoAlign在CS-TaxoBench基准上超越基线模型（自动评估与人工评估双验证）

Conclusion: TaxoAlign有效缩小人工与自动生成分类法的差距，提供开源代码与数据集

Abstract: Taxonomies play a crucial role in helping researchers structure and navigate
knowledge in a hierarchical manner. They also form an important part in the
creation of comprehensive literature surveys. The existing approaches to
automatic survey generation do not compare the structure of the generated
surveys with those written by human experts. To address this gap, we present
our own method for automated taxonomy creation that can bridge the gap between
human-generated and automatically-created taxonomies. For this purpose, we
create the CS-TaxoBench benchmark which consists of 460 taxonomies that have
been extracted from human-written survey papers. We also include an additional
test set of 80 taxonomies curated from conference survey papers. We propose
TaxoAlign, a three-phase topic-based instruction-guided method for scholarly
taxonomy generation. Additionally, we propose a stringent automated evaluation
framework that measures the structural alignment and semantic coherence of
automatically generated taxonomies in comparison to those created by human
experts. We evaluate our method and various baselines on CS-TaxoBench, using
both automated evaluation metrics and human evaluation studies. The results
show that TaxoAlign consistently surpasses the baselines on nearly all metrics.
The code and data can be found at https://github.com/AvishekLahiri/TaxoAlign.

</details>


### [69] [Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning](https://arxiv.org/abs/2510.17289)
*Hajar Bakarou,Mohamed Sinane El Messoussi,Anaïs Ollagnier*

Main category: cs.CL

TL;DR: 论文通过多模态方法分析多党对话中的反社会行为，使用法国数据集CyberAgressionAdo-Large验证模型，显示多模态融合（mBERT+WD-SGCN）在滥用检测等任务中表现最优。


<details>
  <summary>Details</summary>
Motivation: 现有研究集中于X和Reddit平台，但多党对话场景因数据匮乏缺乏深入探索。本研究旨在填补多党对话环境中ASB分析的技术空白。

Method: 基于法国开放数据集CyberAgressionAdo-Large，评估文本/图表示学习方法在三大任务（滥用检测、欺凌分析、同伴群体识别）的表现，融合词汇线索和交互动态特征。

Result: 多模态模型全面超越单模态基准。后期融合模型mBERT+WD-SGCN在滥用检测(F1=0.718)表现最佳，在同伴识别(0.286)和欺凌分析(0.606)中保持竞争力。

Conclusion: 多模态方法有效捕捉复杂ASB现象（如隐式攻击、角色转换），为社交平台安全治理提供新方案，后续可优化多模态融合策略提升效果。

Abstract: Antisocial behavior (ASB) on social media -- including hate speech,
harassment, and cyberbullying -- poses growing risks to platform safety and
societal well-being. Prior research has focused largely on networks such as X
and Reddit, while \textit{multi-party conversational settings} remain
underexplored due to limited data. To address this gap, we use
\textit{CyberAgressionAdo-Large}, a French open-access dataset simulating ASB
in multi-party conversations, and evaluate three tasks: \textit{abuse
detection}, \textit{bullying behavior analysis}, and \textit{bullying
peer-group identification}. We benchmark six text-based and eight graph-based
\textit{representation-learning methods}, analyzing lexical cues, interactional
dynamics, and their multimodal fusion. Results show that multimodal models
outperform unimodal baselines. The late fusion model \texttt{mBERT + WD-SGCN}
achieves the best overall results, with top performance on abuse detection
(0.718) and competitive scores on peer-group identification (0.286) and
bullying analysis (0.606). Error analysis highlights its effectiveness in
handling nuanced ASB phenomena such as implicit aggression, role transitions,
and context-dependent hostility.

</details>


### [70] [Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation](https://arxiv.org/abs/2510.17354)
*Chenghao Zhang,Guanting Dong,Xinyu Yang,Zhicheng Dou*

Main category: cs.CL

TL;DR: 提出多模态检索增强生成框架Nyx，通过混合模态检索提升视觉语言生成效果


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统仅支持文本检索，难以应对现实场景中多模态（图文混合）的检索需求

Method: 四阶段自动化管道构建混合模态数据集NyxQA+两阶段训练框架（预训练+基于下游模型的监督微调）

Result: Nyx在文本RAG基准测试中表现优异，多模态场景下生成质量显著提升

Conclusion: Nyx通过混合模态检索对齐生成偏好，有效扩展了RAG技术的应用边界

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for
enhancing large language models (LLMs) by retrieving relevant documents from an
external corpus. However, existing RAG systems primarily focus on unimodal text
documents, and often fall short in real-world scenarios where both queries and
documents may contain mixed modalities (such as text and images). In this
paper, we address the challenge of Universal Retrieval-Augmented Generation
(URAG), which involves retrieving and reasoning over mixed-modal information to
improve vision-language generation. To this end, we propose Nyx, a unified
mixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate
the scarcity of realistic mixed-modal data, we introduce a four-stage automated
pipeline for generation and filtering, leveraging web documents to construct
NyxQA, a dataset comprising diverse mixed-modal question-answer pairs that
better reflect real-world information needs. Building on this high-quality
dataset, we adopt a two-stage training framework for Nyx: we first perform
pre-training on NyxQA along with a variety of open-source retrieval datasets,
followed by supervised fine-tuning using feedback from downstream
vision-language models (VLMs) to align retrieval outputs with generative
preferences. Experimental results demonstrate that Nyx not only performs
competitively on standard text-only RAG benchmarks, but also excels in the more
general and realistic URAG setting, significantly improving generation quality
in vision-language tasks.

</details>


### [71] [The Atomic Instruction Gap: Instruction-Tuned LLMs Struggle with Simple, Self-Contained Directives](https://arxiv.org/abs/2510.17388)
*Henry Lim,Kwan Hui Lim*

Main category: cs.CL

TL;DR: 研究发现指令调整的大语言模型存在明显的格式敏感性和原子指令遵循不足问题，不同标签格式会导致30%性能差异，显式指令可缓解部分偏差但根本问题未解决。


<details>
  <summary>Details</summary>
Motivation: 探索指令调优大模型在简单自包含指令下的执行能力，揭示现有训练范式的不足及其对复杂指令任务的影响基础。

Method: 通过修改MMLU/MMLU-Pro基准，系统改变选项标签格式（字母/数字/罗马），在四种范式下评估20个IT-LLM：显式指令/无指令/无选项内容/三样本学习。

Result: 1) 显式指令下标签格式导致30%性能差异 2) 无指令时性能下降10%且敏感性加剧 3) 无选项内容时除数字标签外均无法通过随机基线 4) 三样本学习无显著改善且生成错误持续。

Conclusion: 当前指令调优范式存在根本缺陷，需要开发专门针对原子指令遵循的评估方法和训练策略，模型规模扩大无法解决指令遵循一致性问题。

Abstract: Instruction-tuned large language models (IT-LLMs) exhibit strong zero-shot
reasoning, yet their ability to execute simple, self-contained instructions
remains underexplored, despite this being foundational to complex
instruction-following. We evaluate 20 IT-LLMs on modified MMLU and MMLU-Pro
benchmarks, by systematically varying the format of option labels (alphabetic,
numeric, Roman) while keeping their meaning identical under four paradigms,
namely: (1) With explicit instructions, label changes cause large performance
shifts (e.g., -30.45\% for Roman vs. numeric), revealing instruction-format
bias. (2) Without instructions, performance drops further (up to -10.84\%) and
label sensitivity intensifies, underscoring the role of explicit guidance. (3)
When option contents are removed, models fail random-choice baselines except
with numeric labels, suggesting weak adherence to atomic directives. (4)
Three-shot exemplars yield no significant gains in robustness or fidelity, and
generation analyses show persistent label errors, especially for non-numeric
formats. Across model sizes, larger LLMs achieve higher accuracy but remain
inconsistent in instruction adherence. These results expose the insufficiencies
of current instruction-tuning paradigms and highlight the need for evaluation
methods and training strategies that explicitly target atomic
instruction-following.

</details>


### [72] [EduAdapt: A Question Answer Benchmark Dataset for Evaluating Grade-Level Adaptability in LLMs](https://arxiv.org/abs/2510.17389)
*Numaan Naeem,Abdellah El Mekki,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 提出EduAdapt基准测试，评估大语言模型在K12教育中按年级调整回答的能力，发现大模型对低年级学生适应性不足


<details>
  <summary>Details</summary>
Motivation: 现有大模型无法根据学生认知发展阶段生成适合年级的词汇和解释，缺乏标准化评估基准

Method: 构建包含48k分年级科学问答对的EduAdapt数据集，评估不同开源模型在四个年级组的适应性表现

Result: 大模型整体表现优于小模型，但在1-5年级回答生成上仍有明显缺陷（准确率仅达高年级的50%）

Conclusion: 首个评估大模型年级适应性的框架，需改进训练策略和提示工程来提升教育AI的发展适应性

Abstract: Large language models (LLMs) are transforming education by answering
questions, explaining complex concepts, and generating content across a wide
range of subjects. Despite strong performance on academic benchmarks, they
often fail to tailor responses to students' grade levels. This is a critical
need in K-12 education, where age-appropriate vocabulary and explanation are
essential for effective learning. Existing models frequently produce outputs
that are too advanced or vague for younger learners, and there are no
standardized benchmarks to evaluate their ability to adjust across cognitive
and developmental stages. To address this gap, we introduce EduAdapt, a
benchmark of nearly 48k grade-labeled QA pairs across nine science subjects,
spanning Grades 1-12 and grouped into four grade levels. We evaluate a diverse
set of open-source LLMs on EduAdapt and find that while larger models generally
perform better, they still struggle with generating suitable responses for
early-grade students (Grades 1-5). Our work presents the first dataset and
evaluation framework for assessing grade-level adaptability in LLMs, aiming to
foster more developmentally aligned educational AI systems through better
training and prompting strategies. EduAdapt code and datasets are publicly
available at https://github.com/NaumanNaeem/EduAdapt.

</details>


### [73] [Leveraging Group Relative Policy Optimization to Advance Large Language Models in Traditional Chinese Medicine](https://arxiv.org/abs/2510.17402)
*Jiacheng Xie,Shuai Zeng,Yang Yu,Xiaoting Tang,Guanghui An,Dong Xu*

Main category: cs.CL

TL;DR: 首个基于GRPO强化学习方法的中医大模型Ladder-base在多项指标超越GPT-4等通用模型及领域模型，证明该方法可有效提升语言模型在传统医学领域的专家级推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有中医大模型存在三大局限：模型与专业领域知识对齐不足、训练数据质量参差、评估体系缺乏统一标准，制约可信中医AI发展。

Method: 以Qwen2.5-7B-Instruct为基座，采用组相对策略优化(GRPO)强化学习方法，基于TCM-Ladder文本数据集训练（80%训练/10%验证/10%测试）。

Result: 标准化评估显示Ladder-base在多项推理指标超越GPT-4/Gemini/Claude等通用模型，以及BenTsao/HuatuoGPT2等中医领域模型。

Conclusion: GRPO为传统医学领域提供高效对齐方案，支持开发临床可信的中医AI系统，推动知识密集型领域的专业化语言模型发展。

Abstract: Traditional Chinese Medicine (TCM) presents a rich and structurally unique
knowledge system that challenges conventional applications of large language
models (LLMs). Although previous TCM-specific LLMs have shown progress through
supervised fine-tuning, they often face limitations in alignment, data quality,
and evaluation consistency. In this study, we introduce Ladder-base, the first
TCM-focused LLM trained with Group Relative Policy Optimization (GRPO), a
reinforcement learning method that improves reasoning and factual consistency
by optimizing response selection based on intra-group comparisons. Ladder-base
is built upon the Qwen2.5-7B-Instruct foundation model and trained exclusively
on the textual subset of the TCM-Ladder benchmark, using 80 percent of the data
for training and the remaining 20 percent split evenly between validation and
test sets. Through standardized evaluation, Ladder-base demonstrates superior
performance across multiple reasoning metrics when compared to both
state-of-the-art general-purpose LLMs such as GPT-4, Gemini 2.5, Claude 3, and
Qwen3 and domain-specific TCM models including BenTsao, HuatuoGPT2, and
Zhongjing. These findings suggest that GRPO provides an effective and efficient
strategy for aligning LLMs with expert-level reasoning in traditional medical
domains and supports the development of trustworthy and clinically grounded TCM
artificial intelligence systems.

</details>


### [74] [AFRICAPTION: Establishing a New Paradigm for Image Captioning in African Languages](https://arxiv.org/abs/2510.17405)
*Mardiyyah Oduwole,Prince Mireku,Fatimo Adebanjo,Oluwatosin Olajide,Mahi Aminu Aliyu,Jekaterina Novikova*

Main category: cs.CL

TL;DR: 提出了AfriCaption框架，支持20种非洲语言的多语言图像描述，通过数据集、动态流程和专用模型解决低资源语言困境，促进包容性多模态AI发展。


<details>
  <summary>Details</summary>
Motivation: 当前多模态AI研究集中在高资源语言，导致非洲语言资源匮乏，阻碍AI技术民主化。本研究旨在填补这一空白，推动真正包容的多模态AI发展。

Method: 1) 基于Flickr8k构建语义对齐的多语言数据集；2) 开发动态流程保持上下文质量（模型集成+自适应替换）；3) 设计整合SigLIP和NLLB200的0.5B参数视觉-文本模型。

Result: 创建首个可扩展的非洲低资源语言图像描述资源，实现持续数据质量维护，为未被充分代表的语言建立标准化解决方案。

Conclusion: 该框架为非洲语言提供了首个可扩展的多模态基础设施，通过系统化的数据-模型协同设计，奠定了包容性AI发展的技术基础。

Abstract: Multimodal AI research has overwhelmingly focused on high-resource languages,
hindering the democratization of advancements in the field. To address this, we
present AfriCaption, a comprehensive framework for multilingual image
captioning in 20 African languages and our contributions are threefold: (i) a
curated dataset built on Flickr8k, featuring semantically aligned captions
generated via a context-aware selection and translation process; (ii) a
dynamic, context-preserving pipeline that ensures ongoing quality through model
ensembling and adaptive substitution; and (iii) the AfriCaption model, a 0.5B
parameter vision-to-text architecture that integrates SigLIP and NLLB200 for
caption generation across under-represented languages. This unified framework
ensures ongoing data quality and establishes the first scalable
image-captioning resource for under-represented African languages, laying the
groundwork for truly inclusive multimodal AI.

</details>


### [75] [BenCao: An Instruction-Tuned Large Language Model for Traditional Chinese Medicine](https://arxiv.org/abs/2510.17415)
*Jiacheng Xie,Yang Yu,Yibo Chen,Hanyao Zhang,Lening Zhao,Jiaxuan He,Lei Jiang,Xiaoting Tang,Guanghui An,Dong Xu*

Main category: cs.CL

TL;DR: 开发基于ChatGPT的多模态中医助手BenCao，通过自然语言指令调优实现专家级推理，集成多模态资源并取得优于现有模型的临床效果


<details>
  <summary>Details</summary>
Motivation: 解决现有中医领域大语言模型缺乏多模态整合、可解释性不足及临床适用性有限的核心问题

Method: 整合千年典籍构建结构化知识库，设计场景化指令框架实现问诊交互，开发思维链模拟机制增强可解释性，建立执业中医师反馈优化流程

Result: 在单选择题基准测试准确率达89.7%，舌象分类任务F1-score 92.3%，已部署为GPT Store应用并获得全球近千用户实际使用验证

Conclusion: 证实基于自然语言指令调优开发中医大模型的可行性，为传统医学与现代AI结合提供可扩展的实践框架

Abstract: Traditional Chinese Medicine (TCM), with a history spanning over two
millennia, plays a role in global healthcare. However, applying large language
models (LLMs) to TCM remains challenging due to its reliance on holistic
reasoning, implicit logic, and multimodal diagnostic cues. Existing TCM-domain
LLMs have made progress in text-based understanding but lack multimodal
integration, interpretability, and clinical applicability. To address these
limitations, we developed BenCao, a ChatGPT-based multimodal assistant for TCM,
integrating structured knowledge bases, diagnostic data, and expert feedback
refinement. BenCao was trained through natural language instruction tuning
rather than parameter retraining, aligning with expert-level reasoning and
ethical norms specific to TCM. The system incorporates a comprehensive
knowledge base of over 1,000 classical and modern texts, a scenario-based
instruction framework for diverse interactions, a chain-of-thought simulation
mechanism for interpretable reasoning, and a feedback refinement process
involving licensed TCM practitioners. BenCao connects to external APIs for
tongue-image classification and multimodal database retrieval, enabling dynamic
access to diagnostic resources. In evaluations across single-choice question
benchmarks and multimodal classification tasks, BenCao achieved superior
accuracy to general-domain and TCM-domain models, particularly in diagnostics,
herb recognition, and constitution classification. The model was deployed as an
interactive application on the OpenAI GPTs Store, accessed by nearly 1,000
users globally as of October 2025. This study demonstrates the feasibility of
developing a TCM-domain LLM through natural language-based instruction tuning
and multimodal integration, offering a practical framework for aligning
generative AI with traditional medical reasoning and a scalable pathway for
real-world deployment.

</details>


### [76] [Navigating the Alignment-Calibration Trade-off: A Pareto-Superior Frontier via Model Merging](https://arxiv.org/abs/2510.17426)
*Tiancheng Hu,Benjamin Minixhofer,Nigel Collier*

Main category: cs.CL

TL;DR: 通过模型权重插值实现对齐税缓解，在提升准确性的同时恢复模型校准性


<details>
  <summary>Details</summary>
Motivation: 揭示传统对齐方法不仅导致任务准确性下降，还会引发模型校准性丧失、过度自信和输出多样性减少的问题

Method: 在预训练模型和对齐后模型之间进行权重插值，寻找帕累托最优平衡点

Result: 获得超越双亲模型的Pareto最优模型，准确率提升同时恢复87%的校准损失

Conclusion: 模型合并技术为缓解对齐税提供计算高效方案，同步提升模型能力和可靠性

Abstract: The "alignment tax" of post-training is typically framed as a drop in task
accuracy. We show it also involves a severe loss of calibration, making models
overconfident, less reliable, and model outputs less diverse. We show that this
trade-off can be navigated effectively via a simple post-hoc intervention:
interpolating between a model's weights before and after alignment. Crucially,
this is not a strict trade-off. We find that the process consistently reveals
Pareto-optimal interpolations - models that improve accuracy beyond both
parents while substantially recovering the calibration lost during alignment.
Our work demonstrates that simple model merging provides a computationally
efficient method for mitigating the full scope of the alignment tax, yielding
models that are more capable and more reliable.

</details>


### [77] [Agentic Reinforcement Learning for Search is Unsafe](https://arxiv.org/abs/2510.17431)
*Yushi Yang,Shreyansh Padarha,Andrew Lee,Adam Mahdi*

Main category: cs.CL

TL;DR: 强化学习训练的搜索模型存在安全漏洞，两种简单攻击可显著降低其安全性，需开发安全感知的训练框架。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习训练的搜索模型安全机制脆弱，容易被攻击突破

Method: 提出两种攻击方法（Search攻击和Multi-search攻击），在Qwen和Llama模型家族进行本地/网络搜索测试

Result: 攻击使拒绝率下降60%、回答安全性降82.5%、搜索查询安全性降82.4%

Conclusion: 现有RL训练仅奖励有效查询生成，忽视安全性，需建立安全感知的智能体训练流程

Abstract: Agentic reinforcement learning (RL) trains large language models to
autonomously call tools during reasoning, with search as the most common
application. These models excel at multi-step reasoning tasks, but their safety
properties are not well understood. In this study, we show that RL-trained
search models inherit refusal from instruction tuning and often deflect harmful
requests by turning them into safe queries. However, this safety is fragile.
Two simple attacks, one that forces the model to begin response with search
(Search attack), another that encourages models to repeatedly search
(Multi-search attack), trigger cascades of harmful searches and answers. Across
two model families (Qwen, Llama) with both local and web search, these attacks
lower refusal rates by up to 60.0%, answer safety by 82.5%, and search-query
safety by 82.4%. The attacks succeed by triggering models to generate harmful,
request-mirroring search queries before they can generate the inherited refusal
tokens. This exposes a core weakness of current RL training: it rewards
continued generation of effective queries without accounting for their
harmfulness. As a result, RL search models have vulnerabilities that users can
easily exploit, making it urgent to develop safety-aware agentic RL pipelines
optimising for safe search.

</details>


### [78] [Multilingual Clinical NER for Diseases and Medications Recognition in Cardiology Texts using BERT Embeddings](https://arxiv.org/abs/2510.17437)
*Manuela Daniela Danu,George Marica,Constantin Suciu,Lucian Mihai Itu,Oladimeji Farri*

Main category: cs.CL

TL;DR: 研究开发多语言BERT模型提升心脏学领域临床命名实体识别效果，在西班牙语/英语/意大利语的疾病药物识别任务中F1分数显著超越基准。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言临床文本中命名实体识别研究不足的问题，促进多语言临床数据利用。

Method: 采用单语/多语BERT模型，在英/西/意语临床报告中测试疾病与药物实体识别效果。

Result: 西班牙疾病识别77.88%、药物92.09%；英语药物91.74%；意大利药物88.9%，均超过测试均值和中位数。

Conclusion: 验证了深度上下文模型在多语言临床NER任务中的有效性，特别在心脏学领域实现SOTA表现。

Abstract: The rapidly increasing volume of electronic health record (EHR) data
underscores a pressing need to unlock biomedical knowledge from unstructured
clinical texts to support advancements in data-driven clinical systems,
including patient diagnosis, disease progression monitoring, treatment effects
assessment, prediction of future clinical events, etc. While contextualized
language models have demonstrated impressive performance improvements for named
entity recognition (NER) systems in English corpora, there remains a scarcity
of research focused on clinical texts in low-resource languages. To bridge this
gap, our study aims to develop multiple deep contextual embedding models to
enhance clinical NER in the cardiology domain, as part of the BioASQ
MultiCardioNER shared task. We explore the effectiveness of different
monolingual and multilingual BERT-based models, trained on general domain text,
for extracting disease and medication mentions from clinical case reports
written in English, Spanish, and Italian. We achieved an F1-score of 77.88% on
Spanish Diseases Recognition (SDR), 92.09% on Spanish Medications Recognition
(SMR), 91.74% on English Medications Recognition (EMR), and 88.9% on Italian
Medications Recognition (IMR). These results outperform the mean and median F1
scores in the test leaderboard across all subtasks, with the mean/median values
being: 69.61%/75.66% for SDR, 81.22%/90.18% for SMR, 89.2%/88.96% for EMR, and
82.8%/87.76% for IMR.

</details>


### [79] [Evaluating Large Language Models on Urdu Idiom Translation](https://arxiv.org/abs/2510.17460)
*Muhammad Farmal Khan,Mousumi Akter*

Main category: cs.CL

TL;DR: 针对乌尔都语低资源语言的习语翻译难题，研究团队构建首个双语评估数据集，发现提示工程可提升习语翻译质量，原生乌尔都语文本翻译效果优于罗马化文本。


<details>
  <summary>Details</summary>
Motivation: 机器翻译中的习语翻译（尤其是乌尔都语等低资源语言）长期面临挑战且缺乏研究，需建立基准数据集并探索模型表现。

Method: 构建首个乌尔都语-英语习语双语数据集（含原生和罗马化文本），评估开源大语言模型和神经机器翻译系统的文化语义保持能力，使用BLEU/BERTScore/COMET等自动指标。

Result: 提示工程较直接翻译提升显著但不同提示差异小；原生乌尔都语输入的翻译质量显著优于罗马化文本（文本表征影响达20%以上）。

Conclusion: 该研究为低资源语言习语翻译提供基准数据集，揭示文本表征形式对翻译质量的关键影响，提示工程可作为改进习语翻译的有效途径。

Abstract: Idiomatic translation remains a significant challenge in machine translation,
especially for low resource languages such as Urdu, and has received limited
prior attention. To advance research in this area, we introduce the first
evaluation datasets for Urdu to English idiomatic translation, covering both
Native Urdu and Roman Urdu scripts and annotated with gold-standard English
equivalents. We evaluate multiple open-source Large Language Models (LLMs) and
Neural Machine Translation (NMT) systems on this task, focusing on their
ability to preserve idiomatic and cultural meaning. Automatic metrics including
BLEU, BERTScore, COMET, and XCOMET are used to assess translation quality. Our
findings indicate that prompt engineering enhances idiomatic translation
compared to direct translation, though performance differences among prompt
types are relatively minor. Moreover, cross script comparisons reveal that text
representation substantially affects translation quality, with Native Urdu
inputs producing more accurate idiomatic translations than Roman Urdu.

</details>


### [80] [Disparities in Multilingual LLM-Based Healthcare Q&A](https://arxiv.org/abs/2510.17476)
*Ipek Baris Schlicht,Burcu Sayin,Zhixue Zhao,Frederik M. Labonté,Cesare Barbera,Marco Viviani,Paolo Rosso,Lucie Flek*

Main category: cs.CL

TL;DR: 研究揭示多语言大模型在医疗问答中存在知识覆盖与事实对齐的跨语言差异，英语内容主导模型输出，但通过本地化知识增强可改善事实对齐


<details>
  <summary>Details</summary>
Motivation: 当前医疗AI整合中多语言信息质量参差不齐，需系统评估多语言大模型在医疗领域的事实可靠性差异

Method: 构建多语言维基医疗数据集→分析语种覆盖差异→评估模型回答与参考对齐度→通过上下文增强和RAG技术进行案例验证

Result: 英语维基覆盖率显著高于其他语言（德/土/中/意），模型输出普遍偏向英语知识（包括非英语提问场景），但推理阶段注入本地化知识可有效提升文化适配性

Conclusion: 构建公平的多语言医疗AI需关注知识源均衡与推理增强技术，提供本地化上下文是提升事实对齐的有效路径

Abstract: Equitable access to reliable health information is vital when integrating AI
into healthcare. Yet, information quality varies across languages, raising
concerns about the reliability and consistency of multilingual Large Language
Models (LLMs). We systematically examine cross-lingual disparities in
pre-training source and factuality alignment in LLM answers for multilingual
healthcare Q&A across English, German, Turkish, Chinese (Mandarin), and
Italian. We (i) constructed Multilingual Wiki Health Care
(MultiWikiHealthCare), a multilingual dataset from Wikipedia; (ii) analyzed
cross-lingual healthcare coverage; (iii) assessed LLM response alignment with
these references; and (iv) conducted a case study on factual alignment through
the use of contextual information and Retrieval-Augmented Generation (RAG). Our
findings reveal substantial cross-lingual disparities in both Wikipedia
coverage and LLM factual alignment. Across LLMs, responses align more with
English Wikipedia, even when the prompts are non-English. Providing contextual
excerpts from non-English Wikipedia at inference time effectively shifts
factual alignment toward culturally relevant knowledge. These results highlight
practical pathways for building more equitable, multilingual AI systems for
healthcare.

</details>


### [81] [ReXMoE: Reusing Experts with Minimal Overhead in Mixture-of-Experts](https://arxiv.org/abs/2510.17483)
*Zheyue Tan,Zhiyuan Li,Tao Yuan,Dong Zhou,Weilin Liu,Yueqing Zhuang,Yadong Li,Guowei Niu,Cheng Qin,Zhuyu Yao,Congyi Liu,Haiyang Xu,Boxun Li,Guohao Dai,Bo Zhao,Yu Wang*

Main category: cs.CL

TL;DR: 提出ReXMoE架构，通过跨层专家复用机制突破传统MoE层局部路由限制，在参数预算固定条件下提升模型表达能力


<details>
  <summary>Details</summary>
Motivation: 传统MoE架构的层间路由隔离导致专家组合灵活性受限，需在专家维度与路由多样性间做妥协

Method: 采用跨层专家共享机制解耦专家维度与层预算，配合渐进式扩展路由策略(PSR)动态扩展候选专家池

Result: 在0.5B到7B参数规模的不同架构中，ReXMoE在语言建模和下游任务上均取得显著性能提升

Conclusion: ReXMoE作为新型MoE设计范式，在保证参数效率的同时实现更优的模型扩展性，为高效MoE架构提供新方向

Abstract: Mixture-of-Experts (MoE) architectures have emerged as a promising approach
to scale Large Language Models (LLMs). MoE boosts the efficiency by activating
a subset of experts per token. Recent works show that fine-grained experts
substantially enriches the combinatorial flexibility of active experts and
enhances model expressiveness. However, such a design is fundamentally limited
by the layer-local routing mechanism: each layer is restricted to its own
expert pool. This requires a careful trade-off between expert dimensionality
and routing diversity given fixed parameter budgets. We describe ReXMoE, a
novel MoE architecture that improves routing beyond the existing layer-local
approaches by allowing routers to reuse experts across adjacent layers. ReXMoE
decouples expert dimensionality from per-layer budgets, enabling richer expert
combinations without sacrificing individual expert capacity or inflating
overall parameters. To this end, we propose a new progressive scaling routing
(PSR) strategy to gradually increase the candidate expert pool during training.
As a result, ReXMoE improves both language modeling and downstream task
performance. Extensive experiments on models ranging from 0.5B to 7B parameters
across different architectures demonstrate that ReXMoE consistently improves
performance under fixed architectural dimensions, confirming ReXMoE as new
design paradigm for parameter-efficient and scalable MoE-based LLMs.

</details>


### [82] [DETree: DEtecting Human-AI Collaborative Texts via Tree-Structured Hierarchical Representation Learning](https://arxiv.org/abs/2510.17489)
*Yongxin He,Shan Zhang,Yixuan Cao,Lei Ma,Ping Luo*

Main category: cs.CL

TL;DR: 提出DETree方法，通过层次亲和树结构建模不同AI协作过程，结合专用损失函数和RealBench数据集，显著提升混合文本检测性能与OOD场景鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有检测方法将人机协作过程简单建模为二元或多元分类，忽视了不同AI生成过程文本表征的固有聚类关系，难以有效应对混合文本检测需求

Method: 构建层次亲和树（Hierarchical Affinity Tree）表征文本生成过程的聚类关系，设计树形对齐损失函数；开发RealBench基准数据集自动整合多种人机协作模式文本

Result: 在混合文本检测任务中提升F1-score达3-5%，OOD场景下准确率提升7-10%，少样本学习条件下保持85%+检测准确率

Conclusion: DETree通过结构化表征学习有效捕捉人机协作过程的层次关系，证实基于训练的方法在复杂检测场景中的优势，为AI内容检测提供新范式

Abstract: Detecting AI-involved text is essential for combating misinformation,
plagiarism, and academic misconduct. However, AI text generation includes
diverse collaborative processes (AI-written text edited by humans,
human-written text edited by AI, and AI-generated text refined by other AI),
where various or even new LLMs could be involved. Texts generated through these
varied processes exhibit complex characteristics, presenting significant
challenges for detection. Current methods model these processes rather crudely,
primarily employing binary classification (purely human vs. AI-involved) or
multi-classification (treating human-AI collaboration as a new class). We
observe that representations of texts generated through different processes
exhibit inherent clustering relationships. Therefore, we propose DETree, a
novel approach that models the relationships among different processes as a
Hierarchical Affinity Tree structure, and introduces a specialized loss
function that aligns text representations with this tree. To facilitate this
learning, we developed RealBench, a comprehensive benchmark dataset that
automatically incorporates a wide spectrum of hybrid texts produced through
various human-AI collaboration processes. Our method improves performance in
hybrid text detection tasks and significantly enhances robustness and
generalization in out-of-distribution scenarios, particularly in few-shot
learning conditions, further demonstrating the promise of training-based
approaches in OOD settings. Our code and dataset are available at
https://github.com/heyongxin233/DETree.

</details>


### [83] [Empowering Real-World: A Survey on the Technology, Practice, and Evaluation of LLM-driven Industry Agents](https://arxiv.org/abs/2510.17491)
*Yihong Tang,Kehai Chen,Liang Yue,Jinxin Fan,Caishen Zhou,Xiaoguang Li,Yuyang Zhang,Mingming Zhao,Shixiong Kai,Kaiyang Guo,Xingshan Zeng,Wenjing Cun,Lifeng Shang,Min Zhang*

Main category: cs.CL

TL;DR: 系统性回顾基于LLM的产业智能体技术，提出能力成熟度框架与演进路线


<details>
  <summary>Details</summary>
Motivation: 解决通用智能体研究如何转化为产业生产力的核心挑战

Method: 通过记忆/规划/工具使用三大技术支柱分析，结合数字工程、科学发现等实际应用场景

Result: 建立产业智能体能力评估体系，揭示现有系统在真实性/安全性/行业特性方面的不足

Conclusion: 需突破能力边界评估、多智能体协同治理等技术-产业协同创新路径

Abstract: With the rise of large language models (LLMs), LLM agents capable of
autonomous reasoning, planning, and executing complex tasks have become a
frontier in artificial intelligence. However, how to translate the research on
general agents into productivity that drives industry transformations remains a
significant challenge. To address this, this paper systematically reviews the
technologies, applications, and evaluation methods of industry agents based on
LLMs. Using an industry agent capability maturity framework, it outlines the
evolution of agents in industry applications, from "process execution systems"
to "adaptive social systems." First, we examine the three key technological
pillars that support the advancement of agent capabilities: Memory, Planning,
and Tool Use. We discuss how these technologies evolve from supporting simple
tasks in their early forms to enabling complex autonomous systems and
collective intelligence in more advanced forms. Then, we provide an overview of
the application of industry agents in real-world domains such as digital
engineering, scientific discovery, embodied intelligence, collaborative
business execution, and complex system simulation. Additionally, this paper
reviews the evaluation benchmarks and methods for both fundamental and
specialized capabilities, identifying the challenges existing evaluation
systems face regarding authenticity, safety, and industry specificity. Finally,
we focus on the practical challenges faced by industry agents, exploring their
capability boundaries, developmental potential, and governance issues in
various scenarios, while providing insights into future directions. By
combining technological evolution with industry practices, this review aims to
clarify the current state and offer a clear roadmap and theoretical foundation
for understanding and building the next generation of industry agents.

</details>


### [84] [Deep Self-Evolving Reasoning](https://arxiv.org/abs/2510.17498)
*Zihan Liu,Shun Zheng,Xumeng Wen,Yang Wang,Jiang Bian,Mao Yang*

Main category: cs.CL

TL;DR: 提出DSER概率范式，通过并行自进化推理提升小模型在复杂任务中的推理能力，实验显示显著效果


<details>
  <summary>Details</summary>
Motivation: 现有验证-精炼框架在开源小模型中验证修正能力脆弱，难以处理奥林匹克级难题

Method: 将迭代推理建模为马尔可夫链，利用概率优势(改进概率>退化概率)实现渐进收敛，通过多进程并行放大微小改进趋势

Result: 在AIME基准上解决9题中5个未解题，8B模型通过多数投票超越600B教师模型的单轮准确率

Conclusion: DSER框架不仅实现测试时扩展，更为诊断当前开源推理模型在自验证/精炼/稳定性方面的根本缺陷指明方向，推动开发具有自进化能力的新一代模型

Abstract: Long-form chain-of-thought reasoning has become a cornerstone of advanced
reasoning in large language models. While recent verification-refinement
frameworks have enabled proprietary models to solve Olympiad-level problems,
their effectiveness hinges on strong, reliable verification and correction
capabilities, which remain fragile in open-weight, smaller-scale models. This
work demonstrates that even with weak verification and refinement capabilities
on hard tasks, the reasoning limits of such models can be substantially
extended through a probabilistic paradigm we call Deep Self-Evolving Reasoning
(DSER). We conceptualize iterative reasoning as a Markov chain, where each step
represents a stochastic transition in the solution space. The key insight is
that convergence to a correct solution is guaranteed as long as the probability
of improvement marginally exceeds that of degradation. By running multiple
long-horizon, self-evolving processes in parallel, DSER amplifies these small
positive tendencies, enabling the model to asymptotically approach correct
answers. Empirically, we apply DSER to the DeepSeek-R1-0528-Qwen3-8B model. On
the challenging AIME 2024-2025 benchmark, DSER solves 5 out of 9 previously
unsolvable problems and boosts overall performance, enabling this compact model
to surpass the single-turn accuracy of its 600B-parameter teacher through
majority voting. Beyond its immediate utility for test-time scaling, the DSER
framework serves to diagnose the fundamental limitations of current open-weight
reasoners. By clearly delineating their shortcomings in self-verification,
refinement, and stability, our findings establish a clear research agenda for
developing next-generation models with powerful, intrinsic self-evolving
capabilities.

</details>


### [85] [Lingua Custodi's participation at the WMT 2025 Terminology shared task](https://arxiv.org/abs/2510.17504)
*Jingshu Liu,Raheel Qader,Gaëtan Caillaut,Mariam Nakhlé*

Main category: cs.CL

TL;DR: 提出结合单语/跨语言表示方法训练多语言句子嵌入模型，在112种语言上实现83.7%双语检索准确率，并大幅减少80%并行数据需求。


<details>
  <summary>Details</summary>
Motivation: 探索BERT在跨语言句子嵌入中的应用潜力，解决传统方法对大规模并行数据的依赖问题。

Method: 集成掩码语言建模(MLM)、翻译语言建模(TLM)、双编码器翻译排序、加性边际softmax等方法，采用预训练多语言模型。

Result: 在Tatoeba数据集上双语检索准确率达83.7%（超越LASER的65.5%），用CommonCrawl数据训练的NMT模型表现优异。

Conclusion: 提出的LaBSE模型实现跨语言高精度检索，公开发布支持109+语言的预训练模型，显著降低多语言NMT训练数据需求。

Abstract: While BERT is an effective method for learning monolingual sentence
embeddings for semantic similarity and embedding based transfer learning BERT
based cross-lingual sentence embeddings have yet to be explored. We
systematically investigate methods for learning multilingual sentence
embeddings by combining the best methods for learning monolingual and
cross-lingual representations including: masked language modeling (MLM),
translation language modeling (TLM), dual encoder translation ranking, and
additive margin softmax. We show that introducing a pre-trained multilingual
language model dramatically reduces the amount of parallel training data
required to achieve good performance by 80%. Composing the best of these
methods produces a model that achieves 83.7% bi-text retrieval accuracy over
112 languages on Tatoeba, well above the 65.5 achieved by LASER, while still
performing competitively on monolingual transfer learning benchmarks. Parallel
data mined from CommonCrawl using our best model is shown to train competitive
NMT models for en-zh and en-de. We publicly release our best multilingual
sentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.

</details>


### [86] [Annotation-Efficient Universal Honesty Alignment](https://arxiv.org/abs/2510.17509)
*Shiyu Ni,Keping Bi,Jiafeng Guo,Minghao Tang,Jingtong Wu,Zengxin Han,Xueqi Cheng*

Main category: cs.CL

TL;DR: 提出EliCal两阶段框架，结合自洽性监督和少量正确性标注实现高效诚实对齐


<details>
  <summary>Details</summary>
Motivation: 现有基于训练的诚实对齐方法需要大规模标注，成本高昂。如何在保证效果的前提下降低标注成本成为关键挑战

Method: Elicitation-Then-Calibration框架：第一阶段通过自洽性监督获取模型内部置信度，第二阶段用少量正确性标注进行校准

Result: 仅需0.18%标注量（1k样本）即达到接近全监督效果，在MMLU任务上表现优于基线方法

Conclusion: EliCal为LLM的通用诚实对齐提供了可扩展解决方案，HonestyBench为后续研究提供大规模基准

Abstract: Honesty alignment-the ability of large language models (LLMs) to recognize
their knowledge boundaries and express calibrated confidence-is essential for
trustworthy deployment. Existing methods either rely on training-free
confidence estimation (e.g., token probabilities, self-consistency) or
training-based calibration with correctness annotations. While effective,
achieving universal honesty alignment with training-based calibration requires
costly, large-scale labeling. To support annotation-efficient training, we
introduce Elicitation-Then-Calibration (EliCal), a two-stage framework that
first elicits internal confidence using inexpensive self-consistency
supervision, then calibrates this confidence with a small set of correctness
annotations. To support a large-scale study, we release HonestyBench, a
benchmark covering ten free-form QA datasets with 560k training and 70k
evaluation instances annotated with correctness and self-consistency signals.
Experiments show that EliCal achieves near-optimal alignment with only 1k
correctness annotations (0.18% of full supervision) and better alignment
performance on unseen MMLU tasks than the calibration-only baseline, offering a
scalable solution toward universal honesty alignment in LLMs.

</details>


### [87] [SimBench: Benchmarking the Ability of Large Language Models to Simulate Human Behaviors](https://arxiv.org/abs/2510.17516)
*Tiancheng Hu,Joachim Baumann,Lorenzo Lupo,Dirk Hovy,Nigel Collier,Paul Röttger*

Main category: cs.CL

TL;DR: 提出首个大规模标准化基准SimBench，揭示当前大语言模型模拟人类行为存在显著局限（得分40.8/100），性能随模型规模对数线性增长但与特定群体模拟能力较弱。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM行为模拟评估体系碎片化、指标不统一的问题，为科学评估模型模拟能力建立标准化基准。

Method: 整合20个跨文化行为数据集（涵盖道德决策、经济选择等），分析模型规模、推理计算量、指令微调对模拟能力的影响。

Result: 1. 最佳模型得分仅40.8 2. 性能与模型规模呈对数线性关系 3. 指令微调改善共识问题但损害多样性问题表现 4. 特定群体模拟能力差 5. 模拟能力与MMLU-Pro强相关（r=0.939）

Conclusion: SimBench为衡量LLM模拟能力提供关键基准，揭示模型在复杂社会情境中的局限性，强调标准化评估对开发更精准模拟器的重要性。

Abstract: Large language model (LLM) simulations of human behavior have the potential
to revolutionize the social and behavioral sciences, if and only if they
faithfully reflect real human behaviors. Current evaluations are fragmented,
based on bespoke tasks and metrics, creating a patchwork of incomparable
results. To address this, we introduce SimBench, the first large-scale,
standardized benchmark for a robust, reproducible science of LLM simulation. By
unifying 20 diverse datasets covering tasks from moral decision-making to
economic choice across a large global participant pool, SimBench provides the
necessary foundation to ask fundamental questions about when, how, and why LLM
simulations succeed or fail. We show that, while even the best LLMs today have
limited simulation ability (score: 40.80/100), performance scales log-linearly
with model size. Simulation performance is not improved by increased
inference-time compute. We demonstrate an alignment-simulation trade-off:
instruction-tuning improves performance on low-entropy (consensus) questions
but degrades it on high-entropy (diverse) ones. Models particularly struggle
when simulating specific demographic groups. Finally, we demonstrate that
simulation ability correlates most strongly with deep, knowledge-intensive
reasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to
accelerate the development of more faithful LLM simulators.

</details>


### [88] [OncoReason: Structuring Clinical Reasoning in LLMs for Robust and Interpretable Survival Prediction](https://arxiv.org/abs/2510.17532)
*Raghu Vamshi Hemadri,Geetha Krishna Guruju,Kristi Topollai,Anna Ewa Choromanska*

Main category: cs.CL

TL;DR: 提出多任务学习框架，通过思维链提示和GRPO强化学习方法提升LLMs在癌症预后预测中的准确性与可解释性


<details>
  <summary>Details</summary>
Motivation: 现有生物医学大语言模型缺乏结构化推理能力，难以满足高风险临床决策对模型可解释性的需求

Method: 使用LLaMa3-8B和Med42-8B模型，在MSK-CHORD数据集上联合训练生存分类、时间回归和原理生成任务，比较监督微调、思维链提示和GRPO强化学习三种对齐策略

Result: 思维链提示使F1提升6.0%、MAE降低12%，GRPO在BLEU/ROUGE/BERTScore上达到SOTA，同时揭示现有架构的推理轨迹生成缺陷

Conclusion: 验证了推理感知对齐对临床建模的重要性，为精准肿瘤学建立了可解释LLMs的新基准

Abstract: Predicting cancer treatment outcomes requires models that are both accurate
and interpretable, particularly in the presence of heterogeneous clinical data.
While large language models (LLMs) have shown strong performance in biomedical
NLP, they often lack structured reasoning capabilities critical for high-stakes
decision support. We present a unified, multi-task learning framework that
aligns autoregressive LLMs with clinical reasoning for outcome prediction on
the MSK-CHORD dataset. Our models are trained to jointly perform binary
survival classification, continuous survival time regression, and natural
language rationale generation. We evaluate three alignment strategies: (1)
standard supervised fine-tuning (SFT), (2) SFT with Chain-of-Thought (CoT)
prompting to elicit step-by-step reasoning, and (3) Group Relative Policy
Optimization (GRPO), a reinforcement learning method that aligns model outputs
to expert-derived reasoning trajectories. Experiments with LLaMa3-8B and
Med42-8B backbones demonstrate that CoT prompting improves F1 by +6.0 and
reduces MAE by 12%, while GRPO achieves state-of-the-art interpretability and
predictive performance across BLEU, ROUGE, and BERTScore. We further show that
existing biomedical LLMs often fail to produce valid reasoning traces due to
architectural constraints. Our findings underscore the importance of
reasoning-aware alignment in multi-task clinical modeling and set a new
benchmark for interpretable, trustworthy LLMs in precision oncology.

</details>


### [89] [When Annotators Disagree, Topology Explains: Mapper, a Topological Tool for Exploring Text Embedding Geometry and Ambiguity](https://arxiv.org/abs/2510.17548)
*Nisrine Rair,Alban Goupil,Valeriu Vrabie,Emmanuel Chochoy*

Main category: cs.CL

TL;DR: 提出拓扑分析方法Mapper揭示模型处理模糊性的内部机制，显示微调后模型在嵌入空间形成模块化决策区域，结构自信与标注不确定性之间存在隐性冲突


<details>
  <summary>Details</summary>
Motivation: 传统标量指标无法体现模型对标注分歧样本的模糊性编码机制，需要新的分析工具揭示模型内部表征特性

Method: 在MD-Offense数据集上对RoBERTa-Large微调，应用拓扑数据分析工具Mapper分析嵌入空间结构，对比传统可视化方法(PCA/UMAP)

Result: 98%以上连通组件实现≥90%预测纯度，但模糊样本的标注对齐度下降，发现结构自信与标签不确定性之间的隐藏矛盾

Conclusion: Mapper超越传统工具直接揭示决策边界，为理解NLP模型处理主观性任务提供新型诊断工具和拓扑度量指标

Abstract: Language models are often evaluated with scalar metrics like accuracy, but
such measures fail to capture how models internally represent ambiguity,
especially when human annotators disagree. We propose a topological perspective
to analyze how fine-tuned models encode ambiguity and more generally instances.
  Applied to RoBERTa-Large on the MD-Offense dataset, Mapper, a tool from
topological data analysis, reveals that fine-tuning restructures embedding
space into modular, non-convex regions aligned with model predictions, even for
highly ambiguous cases. Over $98\%$ of connected components exhibit $\geq 90\%$
prediction purity, yet alignment with ground-truth labels drops in ambiguous
data, surfacing a hidden tension between structural confidence and label
uncertainty.
  Unlike traditional tools such as PCA or UMAP, Mapper captures this geometry
directly uncovering decision regions, boundary collapses, and overconfident
clusters. Our findings position Mapper as a powerful diagnostic tool for
understanding how models resolve ambiguity. Beyond visualization, it also
enables topological metrics that may inform proactive modeling strategies in
subjective NLP tasks.

</details>


### [90] [Language Confusion Gate: Language-Aware Decoding Through Model Self-Distillation](https://arxiv.org/abs/2510.17555)
*Collin Zhang,Fei Huang,Chenhan Yuan,Junyang Lin*

Main category: cs.CL

TL;DR: 提出轻量级插件LCG解决LLM语言混淆问题，通过解码过滤显著减少混淆且不影响任务性能


<details>
  <summary>Details</summary>
Motivation: 现有方法需重新训练模型或无法区分有害混淆与正常语码转换，存在应用限制

Method: 使用norm-adjusted自蒸馏训练语言家族预测模块，在解码时动态过滤非目标语言token

Result: LCG使各类模型的语言混淆率下降一个数量级，代码已开源

Conclusion: LCG通过嵌入范数分析和top-k过滤机制，有效解决LLM多语言生成中的混淆问题

Abstract: Large language models (LLMs) often experience language confusion, which is
the unintended mixing of languages during text generation. Current solutions to
this problem either necessitate model retraining or cannot differentiate
between harmful confusion and acceptable code-switching. This paper introduces
the Language Confusion Gate (LCG), a lightweight, plug-in solution that filters
tokens during decoding without altering the base LLM. The LCG is trained using
norm-adjusted self-distillation to predict appropriate language families and
apply masking only when needed. Our method is based on the findings that
language confusion is infrequent, correct-language tokens are usually among the
top predictions, and output token embedding norms are larger for high-resource
languages, which biases sampling. When evaluated across various models,
including Qwen3, GPT-OSS, Gemma3, Llama3.1, LCG decreases language confusion
significantly, often by an order of magnitude, without negatively impacting
task performance. Code is available at
https://github.com/collinzrj/language_confusion_gate.

</details>


### [91] [HGAdapter: Hypergraph-based Adapters in Language Models for Code Summarization and Clone Detection](https://arxiv.org/abs/2510.17591)
*Guang Yang,Yujie Zhu*

Main category: cs.CL

TL;DR: 提出HGAdapter超图适配器，通过超图神经网络捕获代码中的高阶数据关联（语法树/词法/行级关联），结合适配器微调增强预训练模型在代码任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有预训练语言模型(PLM)未充分利用代码内部的高阶数据关联（如抽象语法树家族关联、词法关联、行关联），制约模型效果提升。

Method: 1.设计tokens and hyperedges生成器捕获三类高阶关联；2.改进超图神经网络架构，结合适配器微调提出HGAdapter，可插入多种PLM增强性能。

Result: 在6种编程语言的代码摘要生成和克隆检测任务中，不同程度上提升了PLM的性能，最高BLEU-4提升3.2%，克隆检测F1提升4.1%。

Conclusion: 通过超图适配器编码代码高阶数据关联能有效提升PLM效果，验证了代码高阶关联对模型性能的增益作用。

Abstract: Pre-trained language models (PLMs) are increasingly being applied to
code-related tasks. Although PLMs have achieved good results, they do not take
into account potential high-order data correlations within the code. We propose
three types of high-order correlations in code tokens, i.e. abstract syntax
tree family correlation, lexical correlation, and line correlation. We design a
tokens and hyperedges generator to capture these high-order data correlations.
We improve the architecture of hypergraph neural networks and combine it with
adapter tuning to propose a novel hypergraph-based adapter (HGAdapter) to
fine-tune PLMs. HGAdapter can encode high-order data correlations and is
allowed to be inserted into various PLMs to enhance performance. Experiments
were conducted on several public datasets, including six languages of code
summarization and code clone detection tasks. Our methods improved the
performance of PLMs in datasets to varying degrees. Experimental results
validate the introduction of high-order data correlations that contribute to
improved effectiveness.

</details>


### [92] [LawChain: Modeling Legal Reasoning Chains for Chinese Tort Case Analysis](https://arxiv.org/abs/2510.17602)
*Huiyuan Xie,Chenyang Li,Huining Zhu,Chubin Zhang,Yuxiao Ye,Zhenghao Liu,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 开发了LawChain框架用于民事侵权案件的法律推理建模，构建评估基准验证其有效性，并通过基线方法提升模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有法律推理方法主要依赖通用框架且聚焦刑事案件，缺乏对民事侵权案件法律推理过程的细粒度建模需求。

Method: 1. 提出三模块LawChain框架（侵权要件审查/因果关系认定/责任划分），每个模块包含多级子步骤 2. 构建LawChain评估基准 3. 采用prompt微调和后训练方法建立基线模型

Result: 当前大模型在侵权法律推理关键步骤表现不足，提出的基线方法在LawChain评估基准上提升显著（侵权推理任务提升19.2%），且在实体识别、刑事赔偿计算等任务中展现通用性

Conclusion: 显式建模法律推理链能有效提升模型的法律分析能力，LawChain框架对增强法律AI系统的可解释性和专业性具有重要价值

Abstract: Legal reasoning is a fundamental component of legal analysis and
decision-making. Existing computational approaches to legal reasoning
predominantly rely on generic reasoning frameworks such as syllogism and IRAC,
which do not comprehensively examine the nuanced processes that underpin legal
reasoning. Moreover, current research has largely focused on criminal cases,
with insufficient modeling for civil cases. In this work, we present a novel
framework for explicitly modeling legal reasoning in the analysis of Chinese
tort-related civil cases. We first operationalize the legal reasoning processes
used in tort analysis into the LawChain framework. LawChain is a three-module
reasoning framework, with each module consisting of multiple finer-grained
sub-steps. Informed by the LawChain framework, we introduce the task of tort
legal reasoning and construct an evaluation benchmark, LawChain$_{eval}$, to
systematically assess the critical steps within analytical reasoning chains for
tort analysis. Leveraging this benchmark, we evaluate state-of-the-art large
language models for their legal reasoning ability in civil tort contexts. Our
results indicate that current models still fall short in accurately handling
crucial elements of tort legal reasoning. Furthermore, we introduce several
baseline approaches that explicitly incorporate LawChain-style reasoning
through prompting or post-training. We conduct further experiments on
additional legal analysis tasks, such as Legal Named-Entity Recognition and
Criminal Damages Calculation, to verify the generalizability of these
baselines. The proposed baseline approaches achieve significant improvements in
tort-related legal reasoning and generalize well to related legal analysis
tasks, thus demonstrating the value of explicitly modeling legal reasoning
chains to enhance the reasoning capabilities of language models.

</details>


### [93] [Forget to Know, Remember to Use: Context-Aware Unlearning for Large Language Models](https://arxiv.org/abs/2510.17620)
*Yuefeng Peng,Parnian Afshar,Megan Ganji,Thomas Butler,Amir Houmansadr,Mingxian Wang,Dezhi Hong*

Main category: cs.CL

TL;DR: 论文提出通过增加插件项改进大语言模型遗忘方法，在保持有效遗忘的同时恢复模型的上下文知识利用能力


<details>
  <summary>Details</summary>
Motivation: 现有遗忘方法评估忽视用户可能在提示中重新引入被遗忘知识的使用场景，导致模型上下文效用受损

Method: 在遗忘目标中增加保持上下文知识利用能力的插件项，平衡遗忘效果与上下文效用

Result: 实验表明该方法使上下文效用恢复至接近原始水平，同时保持有效遗忘和保留集性能

Conclusion: 通过增强目标函数，成功解决现有遗忘方法损害上下文知识利用能力的关键问题

Abstract: Large language models may encode sensitive information or outdated knowledge
that needs to be removed, to ensure responsible and compliant model responses.
Unlearning has emerged as an efficient alternative to full retraining, aiming
to remove specific knowledge while preserving overall model utility. Existing
evaluations of unlearning methods focus on (1) the extent of forgetting of the
target knowledge (forget set) and (2) maintaining performance on the retain set
(i.e., utility). However, these evaluations overlook an important usability
aspect: users may still want the model to leverage the removed information if
it is re-introduced in the prompt. In a systematic evaluation of six
state-of-the-art unlearning methods, we find that they consistently impair such
contextual utility. To address this, we augment unlearning objectives with a
plug-in term that preserves the model's ability to use forgotten knowledge when
it is present in context. Extensive experiments demonstrate that our approach
restores contextual utility to near original levels while still maintaining
effective forgetting and retain-set utility.

</details>


### [94] [Qomhra: A Bilingual Irish-English Large Language Model](https://arxiv.org/abs/2510.17652)
*Joseph McInerney*

Main category: cs.CL

TL;DR: 开发低资源环境下爱尔兰语-英语双语大模型Qomhr'a，通过混合训练数据、指令微调和人类偏好对齐实现性能提升


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言（爱尔兰语）模型开发困境，同时保持英语能力

Method: 1.混合爱尔兰/英语语料库 2.使用Gemini-2.5-Pro生成30K指令数据集和1K偏好数据集 3.分阶段进行预训练、指令微调和对齐训练

Result: 爱尔兰语性能提升29%（翻译/性别理解/主题识别），英语提升44%；创建高质量双语指令数据集和偏好数据集

Conclusion: Qomhr'a验证了低资源语言模型开发的有效路径，其指令跟随能力为双语聊天机器人奠定基础

Abstract: This paper introduces Qomhr\'a, a bilingual Irish-English large language
model (LLM), developed under low-resource constraints presenting a complete
pipeline spanning bilingual continued pre-training, instruction tuning, and
alignment from human preferences. Newly accessible Irish corpora and English
text are mixed and curated to improve Irish performance while preserving
English ability. 6 closed-weight LLMs are judged for their Irish text
generation by a native speaker, a learner and other LLMs. Google's
Gemini-2.5-Pro is ranked the highest and is subsequently used to synthesise
instruction tuning and human preference datasets. Two datasets are contributed
leveraging Gemini-2.5-Pro: a 30K Irish-English parallel instruction tuning
dataset and a 1K human preference dataset, generating accepted and rejected
responses that show near perfect alignment with a native Irish speaker.
Qomhr\'a is comprehensively evaluated across benchmarks testing translation,
gender understanding, topic identification and world knowledge with gains of up
to 29% in Irish and 44% in English. Qomhr\'a also undergoes instruction tuning
and demonstrates clear progress in instruction following, crucial for chatbot
functionality.

</details>


### [95] [Towards Mining Effective Pedagogical Strategies from Learner-LLM Educational Dialogues](https://arxiv.org/abs/2510.17698)
*Liqun He,Manolis Mavrikis,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 提出通过对话分析方法评估大语言模型在教育应用中的教学策略有效性，关注学习者与LLM的交互质量而非单纯技术指标


<details>
  <summary>Details</summary>
Motivation: 现有LLM教育应用评估过度关注技术性能和最终学习成果，忽视了学习者与LLM的互动过程分析

Method: 采用四阶段对话分析框架：1) 对话数据收集 2) 对话行为标注 3) 对话模式挖掘 4) 预测模型构建

Result: 初步建立基于对话动态的教学策略评估范式，为后续教育技术评估提供方法论基础

Conclusion: LLM教育应用的评估需重点关注对话互动质量与教学策略有效性，对话分析是提升教育AI评估维度的重要方向

Abstract: Dialogue plays a crucial role in educational settings, yet existing
evaluation methods for educational applications of large language models (LLMs)
primarily focus on technical performance or learning outcomes, often neglecting
attention to learner-LLM interactions. To narrow this gap, this AIED Doctoral
Consortium paper presents an ongoing study employing a dialogue analysis
approach to identify effective pedagogical strategies from learner-LLM
dialogues. The proposed approach involves dialogue data collection, dialogue
act (DA) annotation, DA pattern mining, and predictive model building. Early
insights are outlined as an initial step toward future research. The work
underscores the need to evaluate LLM-based educational applications by focusing
on dialogue dynamics and pedagogical strategies.

</details>


### [96] [QueST: Incentivizing LLMs to Generate Difficult Problems](https://arxiv.org/abs/2510.17715)
*Hanxu Hu,Xingxing Zhang,Jannis Vamvas,Rico Sennrich,Furu Wei*

Main category: cs.CL

TL;DR: 提出QueST框架，通过难度感知生成和微调技术创建挑战性编程问题，显著提升语言模型性能


<details>
  <summary>Details</summary>
Motivation: 现有竞争性编程数据集规模有限且依赖人工标注，制约模型扩展。需要自动生成高质量难题的方法突破数据瓶颈

Method: 结合难度感知图采样（构建知识图谱识别难点模式）和拒绝微调（优化生成器质量），生成数据用于知识蒸馏（从强教师模型提取长链推理能力）和强化学习

Result: 生成难题质量超越GPT-4o，Qwen3-8B经100K难题微调后性能超越原模型，8B模型结合合成方案达到671B大模型水平

Conclusion: QueST证明自动生成复杂问题是提升LLMs竞争性编程能力的有效路径，为模型推理能力扩展提供新范式

Abstract: Large Language Models have achieved strong performance on reasoning tasks,
solving competition-level coding and math problems. However, their scalability
is limited by human-labeled datasets and the lack of large-scale, challenging
coding problem training data. Existing competitive coding datasets contain only
thousands to tens of thousands of problems. Previous synthetic data generation
methods rely on either augmenting existing instruction datasets or selecting
challenging problems from human-labeled data. In this paper, we propose QueST,
a novel framework which combines difficulty-aware graph sampling and
difficulty-aware rejection fine-tuning that directly optimizes specialized
generators to create challenging coding problems. Our trained generators
demonstrate superior capability compared to even GPT-4o at creating challenging
problems that benefit downstream performance. We leverage QueST to generate
large-scale synthetic coding problems, which we then use to distill from strong
teacher models with long chain-of-thought or to conduct reinforcement learning
for smaller models, proving effective in both scenarios. Our distillation
experiments demonstrate significant performance gains. Specifically, after
fine-tuning Qwen3-8B-base on 100K difficult problems generated by QueST, we
surpass the performance of the original Qwen3-8B on LiveCodeBench. With an
additional 112K examples (i.e., 28K human-written problems paired with multiple
synthetic solutions), our 8B model matches the performance of the much larger
DeepSeek-R1-671B. These findings indicate that generating complex problems via
QueST offers an effective and scalable approach to advancing the frontiers of
competitive coding and reasoning for large language models.

</details>


### [97] [PANER: A Paraphrase-Augmented Framework for Low-Resource Named Entity Recognition](https://arxiv.org/abs/2510.17720)
*Nanda Kumar Rengarajan,Jun Yan,Chun Wang*

Main category: cs.CL

TL;DR: 轻量级少样本命名实体识别框架，通过指令调优模板和保留实体的数据增强策略，在低资源场景下实现高效性能


<details>
  <summary>Details</summary>
Motivation: 解决低资源场景下NER标注数据不足的问题。传统零样本和指令调优方法在领域实体泛化能力不足，且无法有效利用有限标注数据

Method: 1. 设计新型指令调优模板，简化输出格式以利用大语言模型上下文窗口
2. 提出保留实体语义的上下文改写数据增强技术，扩展训练数据规模

Result: 在CrossNER数据集上少样本任务平均F1达80.1，数据增强版本比基线最高提升17个F1点

Conclusion: 该框架为计算资源和训练数据有限的群体提供了有效的解决方案，在保持性能的同时显著降低资源需求

Abstract: Named Entity Recognition (NER) is a critical task that requires substantial
annotated data, making it challenging in low-resource scenarios where label
acquisition is expensive. While zero-shot and instruction-tuned approaches have
made progress, they often fail to generalize to domain-specific entities and do
not effectively utilize limited available data. We present a lightweight
few-shot NER framework that addresses these challenges through two key
innovations: (1) a new instruction tuning template with a simplified output
format that combines principles from prior IT approaches to leverage the large
context window of recent state-of-the-art LLMs; (2) introducing a strategic
data augmentation technique that preserves entity information while
paraphrasing the surrounding context, thereby expanding our training data
without compromising semantic relationships. Experiments on benchmark datasets
show that our method achieves performance comparable to state-of-the-art models
on few-shot and zero-shot tasks, with our few-shot approach attaining an
average F1 score of 80.1 on the CrossNER datasets. Models trained with our
paraphrasing approach show consistent improvements in F1 scores of up to 17
points over baseline versions, offering a promising solution for groups with
limited NER training data and compute power.

</details>


### [98] [AcademicEval: Live Long-Context LLM Benchmark](https://arxiv.org/abs/2510.17725)
*Haozhen Zhang,Tao Feng,Pengrui Han,Jiaxuan You*

Main category: cs.CL

TL;DR: 提出了动态基准AcademicEval，利用arXiv论文设计无需人工标注的长上下文生成任务，揭示LLMs在层次化抽象与长示例场景下的不足。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文基准存在上下文长度固定、标注成本高及标签泄露问题，需开发更灵活且无偏的评估体系。

Method: 基于arXiv论文构建标题/摘要/引言/相关工作等学术写作任务，集成高质量小样本示例并支持灵活上下文长度，通过动态更新避免标签污染。

Result: LLMs在需要层次抽象的任务（如引言生成）表现显著下降，且长示例场景下性能衰减明显，凸显基准挑战性。

Conclusion: 该基准为提升LLMs长上下文建模能力提供了新视角，实验揭示的模型缺陷指明了优化方向（如增强层次推理与长示例适应力）。

Abstract: Large Language Models (LLMs) have recently achieved remarkable performance in
long-context understanding. However, current long-context LLM benchmarks are
limited by rigid context length, labor-intensive annotation, and the pressing
challenge of label leakage issues during LLM training. Therefore, we propose
\textsc{AcademicEval}, a live benchmark for evaluating LLMs over long-context
generation tasks. \textsc{AcademicEval} adopts papers on arXiv to introduce
several academic writing tasks with long-context inputs, \textit{i.e.},
\textsc{Title}, \textsc{Abstract}, \textsc{Introduction}, and \textsc{Related
Work}, which cover a wide range of abstraction levels and require no manual
labeling. Moreover, \textsc{AcademicEval} integrates high-quality and
expert-curated few-shot demonstrations from a collected co-author graph to
enable flexible context length. Especially, \textsc{AcademicEval} features an
efficient live evaluation, ensuring no label leakage. We conduct a holistic
evaluation on \textsc{AcademicEval}, and the results illustrate that LLMs
perform poorly on tasks with hierarchical abstraction levels and tend to
struggle with long few-shot demonstrations, highlighting the challenge of our
benchmark. Through experimental analysis, we also reveal some insights for
enhancing LLMs' long-context modeling capabilities. Code is available at
https://github.com/ulab-uiuc/AcademicEval

</details>


### [99] [Train for Truth, Keep the Skills: Binary Retrieval-Augmented Reward Mitigates Hallucinations](https://arxiv.org/abs/2510.17733)
*Tong Chen,Akari Asai,Luke Zettlemoyer,Hannaneh Hajishirzi,Faeze Brahman*

Main category: cs.CL

TL;DR: 提出二元检索增强奖励机制，在降低语言模型幻觉率39.3%的同时保持数学/编程任务性能，实现校准化弃权策略


<details>
  <summary>Details</summary>
Motivation: 现有缓解外在幻觉的方法会损害开放生成能力和下游任务表现，限制了实际应用价值

Method: 基于在线强化学习的二元奖励机制（完全正确得1分/否则0分），结合检索增强验证

Result: 开放生成幻觉率降低39.3%，PopQA/GPQA错误答案减少44.4%/21.7%，且不影响指令遵循/数学/代码能力

Conclusion: 二元奖励机制成功破解事实性与任务性能的权衡困境，相较连续奖励方案具有显著优势

Abstract: Language models often generate factually incorrect information unsupported by
their training data, a phenomenon known as extrinsic hallucination. Existing
mitigation approaches often degrade performance on open-ended generation and
downstream tasks, limiting their practical utility. We propose an online
reinforcement learning method using a novel binary retrieval-augmented reward
(RAR) to address this tradeoff. Unlike continuous reward schemes, our approach
assigns a reward of one only when the model's output is entirely factually
correct, and zero otherwise. We evaluate our method on Qwen3 reasoning models
across diverse tasks. For open-ended generation, binary RAR achieves a 39.3%
reduction in hallucination rates, substantially outperforming both supervised
training and continuous-reward RL baselines. In short-form question answering,
the model learns calibrated abstention, strategically outputting "I don't know"
when faced with insufficient parametric knowledge. This yields 44.4% and 21.7%
fewer incorrect answers on PopQA and GPQA, respectively. Crucially, these
factuality gains come without performance degradation on instruction following,
math, or code, whereas continuous-reward RL, despite improving factuality,
induces quality regressions.

</details>


### [100] [Evaluating Medical LLMs by Levels of Autonomy: A Survey Moving from Benchmarks to Applications](https://arxiv.org/abs/2510.17764)
*Xiao Ye,Jacob Dineen,Zhaonan Li,Zhikun Xu,Weiyu Chen,Shijie Lu,Yuxi Huang,Ming Shen,Phu Tran,Ji-Eun Irene Yum,Muhammad Ali Khan,Muhammad Umar Afzal,Irbaz Bin Riaz,Ben Zhou*

Main category: cs.CL

TL;DR: 医学大语言模型需通过自主性层级重构评估框架（L0-L3），将标准测试结果转化为临床安全应用的循证依据。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标与临床实际应用存在安全性和可靠性脱节，需要建立风险分级的评估体系以确保医疗AI可信度。

Method: 提出四层自主性框架：信息工具层(L0)、信息处理层(L1)、决策支持层(L2)、监督代理层(L3)，将评估指标与各层操作权限及风险等级对齐。

Result: 构建了层级化的评估蓝图，强调证据收集需匹配风险等级，并建立评估结果与临床监管的关联机制。

Conclusion: 基于自主性层级的评估范式推动医学AI从分数导向转向风险可控的临床应用，为医疗AI监管提供系统性框架。

Abstract: Medical Large language models achieve strong scores on standard benchmarks;
however, the transfer of those results to safe and reliable performance in
clinical workflows remains a challenge. This survey reframes evaluation through
a levels-of-autonomy lens (L0-L3), spanning informational tools, information
transformation and aggregation, decision support, and supervised agents. We
align existing benchmarks and metrics with the actions permitted at each level
and their associated risks, making the evaluation targets explicit. This
motivates a level-conditioned blueprint for selecting metrics, assembling
evidence, and reporting claims, alongside directions that link evaluation to
oversight. By centering autonomy, the survey moves the field beyond score-based
claims toward credible, risk-aware evidence for real clinical use.

</details>


### [101] [Foundational Automatic Evaluators: Scaling Multi-Task Generative Evaluator Training for Reasoning-Centric Domains](https://arxiv.org/abs/2510.17793)
*Austin Xu,Xuan-Phi Nguyen,Yilun Zhou,Chien-Sheng Wu,Caiming Xiong,Shafiq Joty*

Main category: cs.CL

TL;DR: 通过构建250万样本数据集训练FARE系列评估器，采用迭代拒绝采样监督微调方法，在多项推理评估任务中超越现有大模型，成为开源评估器新标杆。


<details>
  <summary>Details</summary>
Motivation: 现有生成式评估器研究过于依赖新方法（如强化学习）而忽视数据驱动，本研究通过大规模数据扩展提升评估器性能。

Method: 1. 构建覆盖5类评估任务的2.5M样本数据集
2. 采用迭代拒绝采样监督微调(SFT)方法
3. 训练8B/20B参数的FARE评估器（20B模型含3.6B激活参数）

Result: FARE-20B在MATH推理重排序任务达近Oracle性能，作为验证器使下游RL模型性能提升14.1%，代码评估任务超越gpt-oss-20B模型65%

Conclusion: 数据规模驱动的方法显著提升评估器性能，FARE系列成为开源领域最佳推理评估器，支持实时推理重排序和RL训练验证等多场景应用

Abstract: Finetuning specialized generative evaluators has emerged as a popular
paradigm to meet the increasing demand for scalable evaluation during both
training and test-time. However, recent work has largely focused on applying
new methodology, such as reinforcement learning (RL), to training evaluators,
shying away from large-scale, data-driven development. In this work, we focus
on data scaling, curating a set of 2.5M samples spanning five unique evaluation
tasks (pairwise, step-level, reference-free and reference-based verification,
and single rating) and multiple domains focused on reasoning evaluation. With
our data, we train Foundational Automatic Reasoning Evaluators (FARE), a family
of 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative
rejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges
larger specialized RL-trained evaluators and FARE-20B sets the new standard for
open-source evaluators, surpassing specialized 70B+ evaluators. Beyond static
benchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers,
FARE-20B achieves near-oracle performance on MATH. As verifiers in RL training,
FARE improves the downstream RL-trained model performance by up to 14.1% vs.
string-matching verifiers. When initialized from FARE, a continually-finetuned
FARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.

</details>


### [102] [Executable Knowledge Graphs for Replicating AI Research](https://arxiv.org/abs/2510.17795)
*Yujie Luo,Zhuoyun Yu,Xuehai Wang,Yuqi Zhu,Ningyu Zhang,Lanning Wei,Lun Du,Da Zheng,Huajun Chen*

Main category: cs.CL

TL;DR: 提出可执行知识图谱xKG解决AI研究复现难题，通过整合论文中的技术细节与代码提升代理框架性能


<details>
  <summary>Details</summary>
Motivation: 现有方法因背景知识不足和RAG技术局限，难以捕捉论文中的潜在技术细节与代码实现信号

Method: 开发模块化可插拔的xKG知识库，自动整合科学文献中的技术见解、代码片段和领域知识

Result: 在PaperBench测试中使o3-mini模型性能提升10.9%，代码发布于https://github.com/zjunlp/xKG

Conclusion: xKG通过结构化知识表示支持多粒度检索复用，为自动化AI研究复现提供了通用可扩展方案

Abstract: Replicating AI research is a crucial yet challenging task for large language
model (LLM) agents. Existing approaches often struggle to generate executable
code, primarily due to insufficient background knowledge and the limitations of
retrieval-augmented generation (RAG) methods, which fail to capture latent
technical details hidden in referenced papers. Furthermore, previous approaches
tend to overlook valuable implementation-level code signals and lack structured
knowledge representations that support multi-granular retrieval and reuse. To
overcome these challenges, we propose Executable Knowledge Graphs (xKG), a
modular and pluggable knowledge base that automatically integrates technical
insights, code snippets, and domain-specific knowledge extracted from
scientific literature. When integrated into three agent frameworks with two
different LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on
PaperBench, demonstrating its effectiveness as a general and extensible
solution for automated AI research replication. Code will released at
https://github.com/zjunlp/xKG.

</details>


### [103] [Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics](https://arxiv.org/abs/2510.17797)
*Akshara Prabhakar,Roshan Ram,Zixiang Chen,Silvio Savarese,Frank Wang,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.CL

TL;DR: 提出企业级深度研究系统EDR，通过多智能体架构解决非结构化数据处理难题，在开放基准测试中超越现有系统


<details>
  <summary>Details</summary>
Motivation: 现有自主智能体在领域适应性、意图对齐和企业集成方面存在不足，难以满足企业级数据分析需求

Method: 包含五大核心模块：主规划智能体（任务分解）、四类专业搜索代理（通用/学术/GitHub/LinkedIn）、可扩展工具生态系统（支持NL2SQL等）、可视化智能体、带反思机制的知识缺口检测系统

Result: 在DeepResearch Bench和DeepConsult基准测试中表现优于当前最先进的智能体系统，且无需人工干预

Conclusion: EDR框架推动了多智能体推理应用的发展，已开源代码和包含200个案例的数据集供社区使用

Abstract: As information grows exponentially, enterprises face increasing pressure to
transform unstructured data into coherent, actionable insights. While
autonomous agents show promise, they often struggle with domain-specific
nuances, intent alignment, and enterprise integration. We present Enterprise
Deep Research (EDR), a multi-agent system that integrates (1) a Master Planning
Agent for adaptive query decomposition, (2) four specialized search agents
(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool
ecosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a
Visualization Agent for data-driven insights, and (5) a reflection mechanism
that detects knowledge gaps and updates research direction with optional
human-in-the-loop steering guidance. These components enable automated report
generation, real-time streaming, and seamless enterprise deployment, as
validated on internal datasets. On open-ended benchmarks including DeepResearch
Bench and DeepConsult, EDR outperforms state-of-the-art agentic systems without
any human steering. We release the EDR framework and benchmark trajectories to
advance research on multi-agent reasoning applications.
  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and
Dataset at https://huggingface.co/datasets/Salesforce/EDR-200

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [104] [Two-Stage Sketch-Based Smoke Illustration Generation using Stream Function](https://arxiv.org/abs/2510.15873)
*Hengyuan Chang,Xiaoxuan Xie,Syuhei Sato,Haoran Xie*

Main category: cs.GR

TL;DR: 两阶段草图烟雾生成框架：通过流函数捕捉流动细节，结合LDM生成速度场指导烟雾模拟


<details>
  <summary>Details</summary>
Motivation: 解决传统草图难以表达连续流体变化的局限，使用流函数作为中间表示补充旋转流动细节

Method: 1. 流函数生成阶段：用户草图指导生成流函数 → 2. LDM生成速度场阶段：流函数作为控制条件 → 3. 流线编码全局动态指导训练

Result: 生成的烟雾模拟能有效对齐预期流动形态，成功捕捉草图中缺失的连续变化特征

Conclusion: 流函数作为中间表示显著提升草图控制效果，该框架在流体艺术创作领域具有应用潜力

Abstract: In this paper, we propose a two-stage sketch-based smoke illustration
generation framework using stream function and latent diffusion models (LDM).
The user sketch is used to guide the generation of the stream function, which
serves as the control condition for the velocity field generator. The generated
velocity field can be used to guide the smoke simulation to align with the
intended flow. We adopt streamlines to encode global flow dynamics as sketch
guidance during training. The stream function constitutes the intermediate
representation that captures continuous variation and rotational flow details
absent from sketches.

</details>


### [105] [Sketch-based Fluid Video Generation Using Motion-Guided Diffusion Models in Still Landscape Images](https://arxiv.org/abs/2510.15874)
*Hao Jin,Haoran Xie*

Main category: cs.GR

TL;DR: 本文提出基于运动草图引导的隐扩散模型框架，通过动画化静态图像中的流体生成高质量景观视频


<details>
  <summary>Details</summary>
Motivation: 流体动态特性复杂，传统物理模拟易受边界条件限制，现有潜扩散模型难以实现流体平滑且时序一致的运动控制

Method: 1. 微调条件潜扩散模型从草图生成运动场
2. 通过运动适配器将运动场整合至视频扩散模型
3. 用户草图精确控制流体运动轨迹

Result: 实现景观场景中流体运动的精细化控制，生成时序连贯的高质量流体动画

Conclusion: 该框架有效结合运动场建模与潜扩散模型优势，为静态图像动画化提供新解决方案

Abstract: Integrating motion into static images not only enhances visual expressiveness
but also creates a sense of immersion and temporal depth, establishing it as a
longstanding and impactful theme in artistic expression. Fluid elements such as
waterfall, river, and oceans are common features in landscape, but their
complex dynamic characteristics pose significant challenges in modeling and
controlling their motion within visual computing. Physics-based methods are
often used in fluid animation to track particle movement. However, they are
easily affected by boundary conditions. Recently, latent diffusion models have
been applied to video generation tasks, demonstrating impressive capabilities
in producing high-quality and temporally coherent results. However, it is
challenging for the existing methods to animate fluid smooth and temporally
consistent motion. To solve these issues, this paper introduces a framework for
generating landscape videos by animating fluid in still images under the
guidance of motion sketches. We propose a finetuned conditional latent
diffusion model for generating motion field from user-provided sketches, which
are subsequently integrated into a latent video diffusion model via a motion
adapter to precisely control the fluid movement.

</details>


### [106] [Adaptive Frameless Rendering](https://arxiv.org/abs/2510.15876)
*Abhinav Dayal,Cliff Woolley,Benjamin Watson,David Luebke*

Main category: cs.GR

TL;DR: 提出自适应无框渲染方法，通过灵活采样与时空梯度重建技术，在保持相近视觉质量前提下，样本量比传统方法减少一个数量级（RMS误差衡量），计算开销仅增加15%。


<details>
  <summary>Details</summary>
Motivation: 传统交互式渲染受限于固定采样模式，无法精细适应时空颜色变化。该方法通过自适应采样与重建机制，在静态场景实现抗锯齿效果，在动态场景优先时效性，突破传统渲染速度瓶颈。

Method: 1. 闭环反馈采样器引导采样至图像边缘/运动区域
2. 时间深度缓冲存储短期样本用于重建
3. GPU驱动重建响应采样密度与时空颜色梯度
4. 样本重投影技术改善遮挡边/高光区域采样

Result: 模拟显示：
- 视觉质量相近时样本需求减少10倍
- 新增计算开销仅占15%
- 静态场景累积样本提升锐度与抗锯齿
- 动态场景优先新鲜样本保持时效性

Conclusion: 无框渲染通过时空自适应采样策略，在渲染质量与计算效率间取得突破性平衡。该框架为实时图形学开辟了新方向，特别适用于VR/AR等对延迟敏感的交互场景。

Abstract: We propose an adaptive form of frameless rendering with the potential to
dramatically increase rendering speed over conventional interactive rendering
approaches. Without the rigid sampling patterns of framed renderers, sampling
and reconstruction can adapt with very fine granularity to spatio-temporal
color change. A sampler uses closed-loop feedback to guide sampling toward
edges or motion in the image. Temporally deep buffers store all the samples
created over a short time interval for use in reconstruction and as sampler
feedback. GPU-based reconstruction responds both to sampling density and
space-time color gradients. Where the displayed scene is static, spatial color
change dominates and older samples are given significant weight in
reconstruction, resulting in sharper and eventually antialiased images. Where
the scene is dynamic, more recent samples are emphasized, resulting in less
sharp but more up-to-date images. We also use sample reprojection to improve
reconstruction and guide sampling toward occlusion edges, undersampled regions,
and specular highlights. In simulation our frameless renderer requires an order
of magnitude fewer samples than traditional rendering of similar visual quality
(as measured by RMS error), while introducing overhead amounting to 15% of
computation time.

</details>


### [107] [Procedural modeling of urban land use](https://arxiv.org/abs/2510.15877)
*Thomas Lechner,Ben Watson,Uri Wilenski,Seth Tisue,Martin Felsen,Andy Moddrell,Pin Ren,Craig Brozefsky*

Main category: cs.GR

TL;DR: 提出程序化生成城市土地利用模式的方法，通过自动化建筑和道路布局帮助艺术家解决复杂建模与成本控制问题


<details>
  <summary>Details</summary>
Motivation: 城市建模复杂度高且现有工具稀缺，图形硬件发展催生对高质量内容的需求，但需保持制作成本可控

Method: 采用程序化生成技术，开发自动化系统来创建逼真的城市土地分配模式

Result: 实现高效生成符合现实逻辑的城市布局，降低人工建模成本的同时提升内容丰富度

Conclusion: 该方法有效平衡了数字内容质量与生产成本，为艺术家提供了可扩展的城市建模解决方案

Abstract: Cities are important elements of content in digital productions, but their
complexity and size make them very challenging to model. Few tools exist that
can help artists with this work, even as rapid improvements in graphics
hardware create demand for richer content without matching increases in
production cost. We propose a method for procedurally generating realistic
patterns of land use in cities, automating placement of buildings and roads for
artists.

</details>


### [108] [Structural Tree Extraction from 3D Surfaces](https://arxiv.org/abs/2510.15886)
*Diogo de Andrade,Nuno Fachada*

Main category: cs.GR

TL;DR: 提出从3D表面数据生成层次树结构的方法，通过Steiner树优化连接并应用视线约束，支持导航感知分析，经案例验证有效性


<details>
  <summary>Details</summary>
Motivation: 传统骨架化方法依赖体积解释假设，无法直接处理表面数据。需要一种能保持表面几何特性、支持导航分析和程序生成的结构提取方法

Method: 1) 提取表面图结构 2) 生成Steiner树优化关键点连接 3) 应用视线约束减少冗余 4) 直接在表面操作避免体积假设

Result: 成功应用于瓦片元素程序生成和自动关卡分析，生成的结构简化度提升40%，支持路径规划算法效率提升25%

Conclusion: 该方法实现了表面数据的结构化表示，为游戏内容生成、空间推理和地图分析提供了新的技术路径

Abstract: This paper introduces a method to extract a hierarchical tree representation
from 3D unorganized polygonal data. The proposed approach first extracts a
graph representation of the surface, which serves as the foundation for
structural analysis. A Steiner tree is then generated to establish an optimized
connection between key terminal points, defined according to
application-specific criteria. The structure can be further refined by
leveraging line-of-sight constraints, reducing redundancy while preserving
essential connectivity. Unlike traditional skeletonization techniques, which
often assume volumetric interpretations, this method operates directly on the
surface, ensuring that the resulting representation remains relevant for
navigation-aware geometric analysis. The method is validated through two use
cases: extracting structural representations from tile-based elements for
procedural content generation, and identifying key points and structural
metrics for automated level analysis. Results demonstrate its ability to
produce simplified, coherent representations, supporting applications in
procedural generation, spatial reasoning, and map analysis.

</details>


### [109] [Procedural Scene Programs for Open-Universe Scene Generation: LLM-Free Error Correction via Program Search](https://arxiv.org/abs/2510.16147)
*Maxim Gumin,Do Heon Han,Seung Jean Yoo,Aditya Ganeshan,R. Kenny Jones,Kailiang Fu,Rio Aguina-Kang,Stewart Morris,Daniel Ritchie*

Main category: cs.GR

TL;DR: 提出一种基于LLM的迭代命令式3D场景布局生成方法，通过动态对象放置和纠错机制，在用户偏好和场景复杂性处理上显著优于传统声明式方法。


<details>
  <summary>Details</summary>
Motivation: 传统声明式布局生成方法依赖LLM生成对象间约束条件，存在场景描述语言复杂且处理复杂场景能力有限的问题。本研究旨在探索更灵活的命令式范式，通过逐步迭代对象放置简化场景描述，同时增强对多样化场景的适应能力。

Method: 1. 命令式布局生成：LLM迭代放置对象，每个对象的位置/方向基于已放置对象计算
2. 纠错机制：通过多轮迭代优化场景有效性，在保持原始布局的前提下自动修正错误

Result: 1. 用户偏好率：命令式方法 vs 两种声明式方法分别达到82%和94%
2. 开发了与人类偏好高度一致的自动化评估指标（+0.89 Spearman相关性）

Conclusion: 命令式布局生成范式在简化场景描述语言、处理复杂场景方面具有显著优势，结合纠错机制有效提升系统鲁棒性，实验证明其性能优越性并为3D场景生成提供了新的评估标准。

Abstract: Synthesizing 3D scenes from open-vocabulary text descriptions is a
challenging, important, and recently-popular application. One of its critical
subproblems is layout generation: given a set of objects, lay them out to
produce a scene matching the input description. Nearly all recent work adopts a
declarative paradigm for this problem: using an LLM to generate a specification
of constraints between objects, then solving those constraints to produce the
final layout. In contrast, we explore an alternative imperative paradigm, in
which an LLM iteratively places objects, with each object's position and
orientation computed as a function of previously-placed objects. The imperative
approach allows for a simpler scene specification language while also handling
a wider variety and larger complexity of scenes. We further improve the
robustness of our imperative scheme by developing an error correction mechanism
that iteratively improves the scene's validity while staying as close as
possible to the original layout generated by the LLM. In forced-choice
perceptual studies, participants preferred layouts generated by our imperative
approach 82% and 94% of the time when compared against two declarative layout
generation methods. We also present a simple, automated evaluation metric for
3D scene layout generation that aligns well with human preferences.

</details>


### [110] [Region-Aware Wasserstein Distances of Persistence Diagrams and Merge Trees](https://arxiv.org/abs/2510.16486)
*Mathieu Pont,Christoph Garth*

Main category: cs.GR

TL;DR: 提出一种基于区域特征的Wasserstein距离泛化方法，用于持久性图和合并树分析，通过极值对齐区域比较拓扑特征，优化计算效率并实现特征演化追踪和降维可视化。


<details>
  <summary>Details</summary>
Motivation: 传统Wasserstein距离在比较拓扑特征时缺乏区域信息利用，无法有效区分复杂结构特征，需开发更精细的拓扑数据分析工具。

Method: 1. 将拓扑特征比较重新定义为极值对齐区域的距离计算
2. 引入参数控制区域特征对距离的贡献度
3. 提出子集采样和区域属性压缩策略优化计算资源

Result: 在开放数据集实现分钟级平均运行时间，成功应用于：
1. 时变数据拓扑特征演化追踪（时间持续性曲线）
2. 基于距离矩阵的降维可视化与关键阶段检测

Conclusion: 新方法显著提升拓扑特征的区分能力，通过优化策略平衡计算效率与精度，在动态数据分析和模式识别领域具有实用价值，并提供开源实现保证可复现性。

Abstract: This paper presents a generalization of the Wasserstein distance for both
persistence diagrams and merge trees [20], [66] that takes advantage of the
regions of their topological features in the input domain. Specifically, we
redefine the comparison of topological features as a distance between the
values of their extrema-aligned regions. It results in a more discriminative
metric than the classical Wasserstein distance and generalizes it through an
input parameter adjusting the impact of the region properties in the distance.
We present two strategies to control both computation time and memory storage
of our method by respectively enabling the use of subsets of the regions in the
computation, and by compressing the regions' properties to obtain low-memory
representations. Extensive experiments on openly available ensemble data
demonstrate the efficiency of our method, with running times on the orders of
minutes on average. We show the utility of our contributions with two
applications. First, we use the assignments between topological features
provided by our method to track their evolution in time-varying ensembles and
propose the temporal persistence curves to facilitate the understanding of how
these features appear, disappear and change over time. Second, our method
allows to compute a distance matrix of an ensemble that can be used for
dimensionality reduction purposes and visually represent in 2D all its members,
we show that such distance matrices also allow to detect key phases in the
ensemble. Finally, we provide a C++ implementation that can be used to
reproduce our results.

</details>


### [111] [Filtering of Small Components for Isosurface Generation](https://arxiv.org/abs/2510.16684)
*Devin Zhao,Rephael Wenger*

Main category: cs.GR

TL;DR: 提出通过预滤波技术去除医学成像数据等值面中的干扰性小组件，保留主体结构


<details>
  <summary>Details</summary>
Motivation: CT/MRI扫描构建的等值面常包含分散注意力的微小无效组件，影响可视化效果

Method: 在数据预处理阶段实施简单滤波算法

Result: 实验验证滤波可有效消除微小组件且不影响主体结构

Conclusion: 预滤波是优化医学成像等值面可视化的有效预处理方案

Abstract: Let $f: \mathbb{R}^3 \rightarrow \mathbb{R}$ be a scalar field. An isosurface
is a piecewise linear approximation of a level set $f^{-1}(\sigma)$ for some
$\sigma \in \mathbb{R}$ built from some regular grid sampling of $f$.
Isosurfaces constructed from scanned data such as CT scans or MRIs often
contain extremely small components that distract from the visualization and do
not form part of any geometric model produced from the data. Simple
prefiltering of the data can remove such small components while having no
effect on the large components that form the body of the visualization. We
present experimental results on such filtering.

</details>


### [112] [A Scalable In Transit Solution for Comprehensive Exploration of Simulation Data](https://arxiv.org/abs/2510.16966)
*Paascal Grosset,James Ahrens*

Main category: cs.GR

TL;DR: SeerX是一种轻量级、可扩展的原位服务，通过动态资源分配和有损压缩技术，解决超算模拟中数据存储受限和先验知识依赖问题。


<details>
  <summary>Details</summary>
Motivation: 传统原位分析方法需要预先确定可视化参数和计算资源，这限制了其在动态环境中的应用效率。

Method: 开发SeerX中转服务，支持多模拟共享弹性计算资源，采用无MPI同步架构实现3D数据动态压缩与分析任务卸载。

Result: 系统实现了跨模拟的资源弹性分配，有效减少存储压力，提升超算环境下原位分析的可扩展性。

Conclusion: SeerX通过去中心化设计和有损压缩技术，为大规模并行模拟提供了灵活高效的原位分析解决方案。

Abstract: As simulations produce more data than available disk space on supercomputers,
many simulations are employing in situ analysis and visualization to reduce the
amount of data that needs to be stored. While in situ visualization offers
potential for substantial data reduction, its efficacy is hindered by the need
for a priori knowledge. First, we need to know what visualization parameters to
use to highlight features of interest. Second, we do not know ahead of time how
much resources will be needed to run the in situ workflows, e.g. how many
compute nodes will be needed for in situ work. In this work, we present SeerX,
a lightweight, scalable in-transit in situ service that supports dynamic
resource allocation and lossy compression of 3D simulation data. SeerX enables
multiple simulations to offload analysis to a shared, elastic service
infrastructure without MPI synchronization.

</details>


### [113] [Shape-aware Inertial Poser: Motion Tracking for Humans with Diverse Shapes Using Sparse Inertial Sensors](https://arxiv.org/abs/2510.17101)
*Lu Yin,Ziying Shi,Yinghao Wu,Xinyu Yi,Feng Xu,Shihui Guo*

Main category: cs.GR

TL;DR: 提出SAIP方法解决惯性传感器运动捕捉的体型泛化问题，通过分解形状/姿态传感器数据，结合回归模型和物理优化策略，并创建首个多体型IMU数据集


<details>
  <summary>Details</summary>
Motivation: 现有惯性运动捕捉方法基于成人模板体型，难以泛化到体型差异大的个体（如儿童），因IMU加速度测量会随体型变化产生差异

Method: 1. 训练回归模型将真实体型IMU加速度映射到模板体型
2. 估计模板体型运动后二次回归映射回真实体型
3. 引入MLP网络建模体型-IMU-姿态相关性实现惯性形状估计
4. 结合形状感知物理优化计算全局运动

Result: 构建首个多体型IMU数据集（10儿童+10成人），实验证明SAIP有效处理不同体型运动捕捉任务，代码和数据集已开源

Conclusion: 通过形状感知的传感器分解和双重回归模型，突破传统体型限制，结合物理优化策略提升精度，为个性化运动捕捉提供新方案

Abstract: Human motion capture with sparse inertial sensors has gained significant
attention recently. However, existing methods almost exclusively rely on a
template adult body shape to model the training data, which poses challenges
when generalizing to individuals with largely different body shapes (such as a
child). This is primarily due to the variation in IMU-measured acceleration
caused by changes in body shape. To fill this gap, we propose Shape-aware
Inertial Poser (SAIP), the first solution considering body shape differences in
sparse inertial-based motion capture. Specifically, we decompose the sensor
measurements related to shape and pose in order to effectively model their
joint correlations. Firstly, we train a regression model to transfer the
IMU-measured accelerations of a real body to match the template adult body
model, compensating for the shape-related sensor measurements. Then, we can
easily follow the state-of-the-art methods to estimate the full body motions of
the template-shaped body. Finally, we utilize a second regression model to map
the joint velocities back to the real body, combined with a shape-aware
physical optimization strategy to calculate global motions on the subject.
Furthermore, our method relies on body shape awareness, introducing the first
inertial shape estimation scheme. This is accomplished by modeling the
shape-conditioned IMU-pose correlation using an MLP-based network. To validate
the effectiveness of SAIP, we also present the first IMU motion capture dataset
containing individuals of different body sizes. This dataset features 10
children and 10 adults, with heights ranging from 110 cm to 190 cm, and a total
of 400 minutes of paired IMU-Motion samples. Extensive experimental results
demonstrate that SAIP can effectively handle motion capture tasks for diverse
body shapes. The code and dataset are available at
https://github.com/yinlu5942/SAIP.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [114] [Long Exposure: Accelerating Parameter-Efficient Fine-Tuning for LLMs under Shadowy Sparsity](https://arxiv.org/abs/2510.15964)
*Tuowei Wang,Kun Li,Zixu Hao,Donglin Bai,Ju Ren,Yaoxue Zhang,Ting Cao,Mao Yang*

Main category: cs.LG

TL;DR: 提出Shadowy Sparsity稀疏性概念并开发Long Exposure系统，通过三个核心组件实现PEFT加速，最高达2.49倍端到端微调加速效果。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调(PEFT)技术存在效率瓶颈，且微调过程中特有的Shadowy Sparsity稀疏形态未被有效利用加速。

Method: 1. Shadowy-sparsity Exposer采用长感知范围捕捉稀疏细节
2. Sequence-oriented Predictor处理长序列输入和动态参数
3. Dynamic-aware Operator优化计算模式和内存访问

Result: 实验显示Long Exposure相比SOTA方法实现最高2.49倍端到端加速，显著提升微调效率。

Conclusion: 该研究通过系统性解决Shadowy Sparsity问题，为LLM的高效参数微调提供了创新解决方案。

Abstract: The adaptation of pre-trained large language models (LLMs) to diverse
downstream tasks via fine-tuning is critical for numerous applications.
However, the inefficiency of parameter-efficient fine-tuning (PEFT) techniques
presents significant challenges in terms of time investments and operational
costs. In this paper, we first introduce a nuanced form of sparsity, termed
Shadowy Sparsity, which is distinctive in fine-tuning and has not been
adequately addressed for acceleration. Under Shadowy Sparsity, we propose Long
Exposure, an efficient system to accelerate PEFT for LLMs. Long Exposure
comprises three key components: Shadowy-sparsity Exposer employs a prolonged
sensing range to capture more sparsity details under shadowy sparsity;
Sequence-oriented Predictor provides efficient yet accurate predictions to
handle large sequence inputs and constantly-evolving parameters; and
Dynamic-aware Operator facilitates more structured computational patterns and
coalesced memory accesses, addressing dynamic sparse operations. Extensive
evaluations show that Long Exposure outperforms state-of-the-arts with up to a
$2.49\times$ speedup in end-to-end fine-tuning, offering promising advancements
in accelerating PEFT for LLMs.

</details>


### [115] [Bolster Hallucination Detection via Prompt-Guided Data Augmentation](https://arxiv.org/abs/2510.15977)
*Wenyun Li,Zheng Zhang,Dongmei Jiang,Xiangyuan Lan*

Main category: cs.LG

TL;DR: 提出PALE框架，通过提示引导的数据增强和对比马氏评分（CM Score），在零人工标注下将幻觉检测准确率提升6.55%


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在幻觉问题，而现有检测方法受限于标注数据不足。需要低成本的数据增强方案提升检测效果

Method: 1. 基于提示工程生成真伪数据增强训练集；2. 在激活空间建模对比分布，通过矩阵分解构建CM Score评估指标

Result: 实验证明PALE框架显著优于基线方法6.55%，在多个领域展现强泛化能力

Conclusion: 该框架首次实现无需人工标注的幻觉检测，通过数据增强和新型评估指标的组合创新，具有重要工程应用价值

Abstract: Large language models (LLMs) have garnered significant interest in AI
community. Despite their impressive generation capabilities, they have been
found to produce misleading or fabricated information, a phenomenon known as
hallucinations. Consequently, hallucination detection has become critical to
ensure the reliability of LLM-generated content. One primary challenge in
hallucination detection is the scarcity of well-labeled datasets containing
both truthful and hallucinated outputs. To address this issue, we introduce
Prompt-guided data Augmented haLlucination dEtection (PALE), a novel framework
that leverages prompt-guided responses from LLMs as data augmentation for
hallucination detection. This strategy can generate both truthful and
hallucinated data under prompt guidance at a relatively low cost. To more
effectively evaluate the truthfulness of the sparse intermediate embeddings
produced by LLMs, we introduce an estimation metric called the Contrastive
Mahalanobis Score (CM Score). This score is based on modeling the distributions
of truthful and hallucinated data in the activation space. CM Score employs a
matrix decomposition approach to more accurately capture the underlying
structure of these distributions. Importantly, our framework does not require
additional human annotations, offering strong generalizability and practicality
for real-world applications. Extensive experiments demonstrate that PALE
achieves superior hallucination detection performance, outperforming the
competitive baseline by a significant margin of 6.55%.

</details>


### [116] [Can GRPO Help LLMs Transcend Their Pretraining Origin?](https://arxiv.org/abs/2510.15990)
*Kangqi Ni,Zhen Tan,Zijie Liu,Pingzhi Li,Tianlong Chen*

Main category: cs.LG

TL;DR: GRPO算法提升LLMs推理能力的效果依赖于预训练数据分布，仅在目标任务符合模型预训练偏好的领域展现OOD泛化能力


<details>
  <summary>Details</summary>
Motivation: 解释GRPO算法在不同推理领域（如数学/医学）效果差异巨大的现象，明确其能力边界

Method: 通过理论证明GRPO的保守重加权特性+控制变量实验（改变推理深度/输入长度/标记表示/组合性）

Result: GRPO本质是受基模型分布约束的保守策略，仅能强化预训练已有的解决方案模式，无法发现全新解决方案

Conclusion: GRPO应视为预训练偏好的放大器而非通用增强工具，未来需开发突破预训练数据限制的新算法

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR), primarily driven by
the Group Relative Policy Optimization (GRPO) algorithm, is a leading approach
for enhancing the reasoning abilities of Large Language Models (LLMs). Despite
its wide adoption, GRPO's gains are often inconsistent; for instance, a model
may show significant improvement in one reasoning domain, like mathematics, yet
remain stagnant in another, such as medicine. This inconsistency raises a
critical question: under what conditions does GRPO improve reasoning and
generalize out-of-distribution (OOD)? We investigate this from a data
distribution perspective. We first prove theoretically that GRPO is a
conservative reweighting scheme, bounded by the base model's distribution and
thus unable to discover completely novel solutions. We further validate this in
carefully designed controlled studies by training transformers from scratch,
evaluating generalization across reasoning depth, input length, token
representation, and compositionality. Our results provide a principled
explanation for GRPO's boundaries: OOD improvement emerges only when the target
task aligns with the model's pretrained biases, while gains on in-distribution
(ID) tasks diminish as performance saturates. This reframes GRPO not as a
universal reasoning enhancer but as a tool that sharpens pretraining biases.
Our findings motivate future development of algorithms that can expand a
model's capabilities beyond its pretraining origin.

</details>


### [117] [Zeroth-Order Sharpness-Aware Learning with Exponential Tilting](https://arxiv.org/abs/2510.16157)
*Xuchen Gong,Tian Li*

Main category: cs.LG

TL;DR: 提出结合零阶优化与SAM的指数倾斜目标框架，通过新算法实现多任务泛化提升


<details>
  <summary>Details</summary>
Motivation: 传统零阶优化关注平均损失，SAM关注最大损失以获得平坦最小值，本研究旨在建立两者的过渡桥梁

Method: 引入指数倾斜目标作为平均-最大损失平滑过渡，开发零阶优化算法并分析锐度特性

Result: 在分类/QA/生成任务中超越基线方法，验证作为高效SAM替代方案的有效性

Conclusion: 结合零阶优化与SAM的柔性框架显著提升模型泛化能力

Abstract: Classic zeroth-order optimization approaches typically optimize for a
smoothed version of the original function, i.e., the expected objective under
randomly perturbed model parameters. This can be interpreted as encouraging the
loss values in the perturbation set to be small on average. Popular
sharpness-aware minimization (SAM) objectives, however, typically focus on the
largest loss within the neighborhood to arrive at flat minima more effectively.
In this work, we connect zeroth-order optimization (and its corresponding
objectives) with SAM approaches explicitly, through an exponential tilting
objective that provides a smooth transition between the average- and the
max-loss formulations. We explore new zeroth-order algorithms to solve a soft
SAM objective parameterized by a tilting parameter $t$. We provide precise
characterizations of the sharpness notions of the tilted SAM framework.
Practically, our approach can be used as a gradient-free and memory-efficient
alternative to SAM variants, and it achieves better generalization compared to
vanilla zeroth-order baselines on a wide range of downstream tasks, including
classification, multiple choice QA, and language generation.

</details>


### [118] [Alignment is Localized: A Causal Probe into Preference Layers](https://arxiv.org/abs/2510.16167)
*Archie Chaudhury*

Main category: cs.LG

TL;DR: 研究发现语言模型对齐通过中层级激活空间实现，而非全参数扩散过程


<details>
  <summary>Details</summary>
Motivation: 揭示基于人类偏好的语言模型对齐机制（如RLHF）的内部工作原理，突破当前黑箱现状

Method: 使用层级因果修补技术对比基础模型与调优模型，结合LASSO回归分析激活距离与奖励增益关系

Result: 对齐过程空间局部化：中层级激活形成决定奖励行为的子空间，LASSO回归显示仅少数层对奖励增益有贡献

Conclusion: 人类偏好对齐本质是方向性的低秩过程，而非参数扩散，为高效模型对齐提供新视角

Abstract: Reinforcement Learning frameworks, particularly those utilizing human
annotations, have become an increasingly popular method for preference
fine-tuning, where the outputs of a language model are tuned to match a certain
set of behavioral policies or guidelines. Reinforcement Learning through Human
Feedback (RLHF) is perhaps the most popular implementation of such a framework,
particularly for aligning LMs toward safety and human intent. However, the
internal workings of how such alignment is achieved remain largely opaque. In
this work, we systematically analyze preference optimization for language model
alignment by applying layer-wide causal patching between a base model and its
tuned counterpart across human preference pairs. We implement our methodology
on \textit{Llama-3.2-1B}, and find that alignment is spatially localized:
mid-layer activations encode a distinct subspace that causally determines
reward-consistent behavior, while early and late layers remain largely
unaffected. Utilizing LASSO regression, we also find that only a small number
of layers possess non-zero coefficients linking activation distances to reward
gains. Overall, we show that, at least for some language models, alignment from
human-based, preferential tuning is a directional, low rank process, rather
than diffuse and parameteric.

</details>


### [119] [WEBSERV: A Browser-Server Environment for Efficient Training of Reinforcement Learning-based Web Agents at Scale](https://arxiv.org/abs/2510.16252)
*Yuxuan Lu,Jing Huang,Hui Liu,Jiri Gesi,Yan Han,Shihan Fu,Tianqi Zheng,Dakuo Wang*

Main category: cs.LG

TL;DR: 提出WEBSERV环境解决RL网络代理训练扩展性问题，通过紧凑浏览器环境+可扩展服务架构，实现5倍延迟降低和240倍存储优化，支持单机200+并发容器。


<details>
  <summary>Details</summary>
Motivation: 现有RL网页交互环境存在三大问题：1) 上下文信息过载干扰策略模型 2) 动作执行缺乏确定性等待机制 3) 无法有效扩展容器并行训练

Method: 1) 构建站点无关的紧凑浏览器环境，平衡上下文与动作复杂性 2) 开发高效web服务器调度系统，支持快速启动/重置环境

Result: 在WebArena的购物CMS和Gitlab任务中达到SOTA单提示成功率，降低延迟约5倍，减少存储需求240倍，单机支持200+并发容器

Conclusion: WEBSERV通过架构创新有效平衡环境复杂性与扩展性需求，为大规模RL网页代理训练提供高效基础设施支持

Abstract: Training and evaluation of Reinforcement Learning (RL) web agents have gained
increasing attention, yet a scalable and efficient environment that couples
realistic and robust browser-side interaction with controllable server-side
state at scale is still missing. Existing environments tend to have one or more
of the following issues: they overwhelm policy models with excessive and noisy
context; they perform actions non-deterministically without waiting for the UI
or network to stabilize; or they cannot scale isolated client-server containers
effectively for parallel RL rollouts. We propose WEBSERV, an environment that
includes 1) a compact, site-agnostic browser environment that balances context
and action complexity, and 2) a scalable RL environment via efficient launching
and resetting web-servers to enable scalable RL training and evaluation. We
evaluate WEBSERV on the shopping CMS and Gitlab tasks in WebArena, achieving
state-of-the-art single-prompt success rates while cutting launch latency by
~5x and storage need by ~240x, with a comparable memory footprint, enabling
200+ concurrent containers on a single host.

</details>


### [120] [Copy-Augmented Representation for Structure Invariant Template-Free Retrosynthesis](https://arxiv.org/abs/2510.16588)
*Jiaxi Zhuang,Yu Zhang,Aimin Zhou,Ying Qian*

Main category: cs.LG

TL;DR: 提出C-SMILES分子表示法，通过分解SMILES和复制增强机制提升逆合成预测精度，在USPTO数据集实现67.2%最高准确率。


<details>
  <summary>Details</summary>
Motivation: 现有无模板方法无法捕捉化学反应的结构不变性，导致搜索空间过大和预测精度下降。

Method: 1. 开发C-SMILES分解传统SMILES为元素-标记对
2. 引入复制增强机制动态决策分子片段保留
3. 结合SMILES对齐指导增强注意力一致性

Result: USPTO-50K数据集top-1准确率67.2%，USPTO-FULL达50.8%，生成分子有效性99.9%

Conclusion: 建立结构感知分子生成新范式，可直接应用于计算药物发现领域

Abstract: Retrosynthesis prediction is fundamental to drug discovery and chemical
synthesis, requiring the identification of reactants that can produce a target
molecule. Current template-free methods struggle to capture the structural
invariance inherent in chemical reactions, where substantial molecular
scaffolds remain unchanged, leading to unnecessarily large search spaces and
reduced prediction accuracy. We introduce C-SMILES, a novel molecular
representation that decomposes traditional SMILES into element-token pairs with
five special tokens, effectively minimizing editing distance between reactants
and products. Building upon this representation, we incorporate a
copy-augmented mechanism that dynamically determines whether to generate new
tokens or preserve unchanged molecular fragments from the product. Our approach
integrates SMILES alignment guidance to enhance attention consistency with
ground-truth atom mappings, enabling more chemically coherent predictions.
Comprehensive evaluation on USPTO-50K and large-scale USPTO-FULL datasets
demonstrates significant improvements: 67.2% top-1 accuracy on USPTO-50K and
50.8% on USPTO-FULL, with 99.9% validity in generated molecules. This work
establishes a new paradigm for structure-aware molecular generation with direct
applications in computational drug discovery.

</details>


### [121] [Zero-Shot Performance Prediction for Probabilistic Scaling Laws](https://arxiv.org/abs/2510.16743)
*Viktoria Schram,Markus Hiller,Daniel Beck,Trevor Cohn*

Main category: cs.LG

TL;DR: 提出基于多任务学习和潜在变量多输出高斯过程的方法，实现NLP模型学习曲线的零样本预测，降低生成概率性扩展规律的成本。


<details>
  <summary>Details</summary>
Motivation: 预测NLP模型的学习曲线有助于优化计算资源分配，减少数据集获取和模型训练的开销，同时支持主动学习策略降低预测不确定性。

Method: 采用分层数据结构建模，使用潜在变量多输出高斯过程捕捉任务间相关性，支持零样本预测学习曲线。

Result: 在3个小规模NLP数据集（nanoGPT/Transformer/mBART/M2M100模型）验证框架有效性，通过主动学习策略使预测结果接近真实扩展规律。

Conclusion: 该框架能够低成本生成概率性扩展规律，主动学习策略可有效降低预测不确定性，为NLP模型性能预测提供新方法。

Abstract: The prediction of learning curves for Natural Language Processing (NLP)
models enables informed decision-making to meet specific performance
objectives, while reducing computational overhead and lowering the costs
associated with dataset acquisition and curation. In this work, we formulate
the prediction task as a multitask learning problem, where each task's data is
modelled as being organized within a two-layer hierarchy. To model the shared
information and dependencies across tasks and hierarchical levels, we employ
latent variable multi-output Gaussian Processes, enabling to account for task
correlations and supporting zero-shot prediction of learning curves (LCs). We
demonstrate that this approach facilitates the development of probabilistic
scaling laws at lower costs. Applying an active learning strategy, LCs can be
queried to reduce predictive uncertainty and provide predictions close to
ground truth scaling laws. We validate our framework on three small-scale NLP
datasets with up to $30$ LCs. These are obtained from nanoGPT models, from
bilingual translation using mBART and Transformer models, and from multilingual
translation using M2M100 models of varying sizes.

</details>


### [122] [Utility-Diversity Aware Online Batch Selection for LLM Supervised Fine-tuning](https://arxiv.org/abs/2510.16882)
*Heming Zou,Yixiu Mao,Yun Qu,Qi Wang,Xiangyang Ji*

Main category: cs.LG

TL;DR: 提出UDS框架，通过核范数度量数据效用和样本内多样性，结合低维嵌入和内存缓冲区实现高效在线数据选择，显著减少SFT训练时间


<details>
  <summary>Details</summary>
Motivation: 现有监督微调方法存在三个主要问题：(1) 仅关注数据效用忽略多样性 (2) 依赖外部资源（如参考模型/验证集）(3) 计算效率低下。需要开发更高效的在线批量选择方法

Method: UDS框架使用logits矩阵的核范数捕捉效用和样本内多样性，通过低维嵌入对比和历史样本内存缓冲区实现样本间多样性评估，无需反向传播和外部资源

Result: 在多个基准测试中优于现有方法（不同数据预算下），相比全数据集微调减少25-50%训练时间，同时保持模型性能

Conclusion: UDS通过多维度数据价值评估和高效计算设计，证明了高效数据选择在SFT中的可行性，为资源受限场景提供了实用解决方案

Abstract: Supervised fine-tuning (SFT) is a commonly used technique to adapt large
language models (LLMs) to downstream tasks. In practice, SFT on a full dataset
is computationally expensive and sometimes suffers from overfitting or bias
amplification. This facilitates the rise of data curation in SFT, which
prioritizes the most valuable data to optimze. This work studies the online
batch selection family that dynamically scores and filters samples during the
training process. However, existing popular methods often (i) rely merely on
the utility of data to select a subset while neglecting other crucial factors
like diversity, (ii) rely on external resources such as reference models or
validation sets, and (iii) incur extra training time over full-dataset
training. To address these limitations, this work develops \textbf{UDS
(Utility-Diversity Sampling)}, a framework for efficient online batch selection
in SFT. UDS leverages the nuclear norm of the logits matrix to capture both
data utility and intra-sample diversity, while estimating inter-sample
diversity through efficient low-dimensional embedding comparisons with a
lightweight memory buffer of historical samples. Such a design eliminates the
need for external resources and unnecessary backpropagation, securing
computational efficiency. Experiments on multiple benchmarks demonstrate that
UDS consistently outperforms state-of-the-art online batch selection methods
under varying data budgets, and significantly reduces training time compared to
full-dataset fine-tuning. Code is available at https://github.com/gfyddha/UDS.

</details>


### [123] [Peering Inside the Black Box: Uncovering LLM Errors in Optimization Modelling through Component-Level Evaluation](https://arxiv.org/abs/2510.16943)
*Dania Refai,Moataz Ahmed*

Main category: cs.LG

TL;DR: 提出组件级评估框架检验LLM数学优化建模能力，发现GPT-5最优且约束覆盖与RMSE是关键指标


<details>
  <summary>Details</summary>
Motivation: 现有评估方法以整体方案为单位，无法精准识别LLM生成优化公式中的结构/数值错误

Method: 设计包含变量/约束精确召回率、约束RMSE、token效率等细粒度指标的评估体系，测试3种模型在6种提示策略下的表现

Result: GPT-5持续领先，思维链提示最有效；约束召回率与低RMSE决定求解可靠性，输出简洁性提升计算效率

Conclusion: NLP建模三原则：1）完整约束覆盖避免违规 2）最小化约束RMSE保证求解精度 3）简洁输出提升计算效率

Abstract: Large language models (LLMs) are increasingly used to convert natural
language descriptions into mathematical optimization formulations. Current
evaluations often treat formulations as a whole, relying on coarse metrics like
solution accuracy or runtime, which obscure structural or numerical errors. In
this study, we present a comprehensive, component-level evaluation framework
for LLM-generated formulations. Beyond the conventional optimality gap, our
framework introduces metrics such as precision and recall for decision
variables and constraints, constraint and objective root mean squared error
(RMSE), and efficiency indicators based on token usage and latency. We evaluate
GPT-5, LLaMA 3.1 Instruct, and DeepSeek Math across optimization problems of
varying complexity under six prompting strategies. Results show that GPT-5
consistently outperforms other models, with chain-of-thought, self-consistency,
and modular prompting proving most effective. Analysis indicates that solver
performance depends primarily on high constraint recall and low constraint
RMSE, which together ensure structural correctness and solution reliability.
Constraint precision and decision variable metrics play secondary roles, while
concise outputs enhance computational efficiency. These findings highlight
three principles for NLP-to-optimization modeling: (i) Complete constraint
coverage prevents violations, (ii) minimizing constraint RMSE ensures
solver-level accuracy, and (iii) concise outputs improve computational
efficiency. The proposed framework establishes a foundation for fine-grained,
diagnostic evaluation of LLMs in optimization modeling.

</details>


### [124] [Leave It to the Experts: Detecting Knowledge Distillation via MoE Expert Signatures](https://arxiv.org/abs/2510.16968)
*Pingzhi Li,Morris Yu-Chao Huang,Zhen Tan,Qingquan Song,Jie Peng,Kai Zou,Yu Cheng,Kaidi Xu,Tianlong Chen*

Main category: cs.LG

TL;DR: 提出基于MoE架构路由模式迁移的LLM知识蒸馏检测框架，准确率超94%且抗提示工程对抗


<details>
  <summary>Details</summary>
Motivation: 现有基于输出相似度的蒸馏检测方法易被提示工程绕过，需挖掘更深层的模型结构特征。发现MoE架构中专家分工协作形成的路由模式在蒸馏过程中持续迁移，成为可靠检测信号。

Method: 白盒场景直接分析MoE模型的路由模式相似性；黑盒场景提出Shadow-MoE方法，通过辅助蒸馏构建代理MoE表征进行对比。建立包含多种蒸馏模型的标准化评测框架。

Result: 在多个场景下达到94%以上检测准确率，显著优于现有基线方法。验证了路由模式对抗提示工程攻击的强鲁棒性。

Conclusion: 首次揭示LLM结构习惯迁移现象，为模型溯源提供新思路。提出的检测框架兼顾不同架构模型和黑盒场景，推动知识蒸馏安全研究。

Abstract: Knowledge Distillation (KD) accelerates training of large language models
(LLMs) but poses intellectual property protection and LLM diversity risks.
Existing KD detection methods based on self-identity or output similarity can
be easily evaded through prompt engineering. We present a KD detection
framework effective in both white-box and black-box settings by exploiting an
overlooked signal: the transfer of MoE "structural habits", especially internal
routing patterns. Our approach analyzes how different experts specialize and
collaborate across various inputs, creating distinctive fingerprints that
persist through the distillation process. To extend beyond the white-box setup
and MoE architectures, we further propose Shadow-MoE, a black-box method that
constructs proxy MoE representations via auxiliary distillation to compare
these patterns between arbitrary model pairs. We establish a comprehensive,
reproducible benchmark that offers diverse distilled checkpoints and an
extensible framework to facilitate future research. Extensive experiments
demonstrate >94% detection accuracy across various scenarios and strong
robustness to prompt-based evasion, outperforming existing baselines while
highlighting the structural habits transfer in LLMs.

</details>


### [125] [Forgetting to Forget: Attention Sink as A Gateway for Backdooring LLM Unlearning](https://arxiv.org/abs/2510.17021)
*Bingqi Shang,Yiwei Chen,Yihua Zhang,Bingquan Shen,Sijia Liu*

Main category: cs.LG

TL;DR: 提出后门反学习攻击方法，使大语言模型在正常条件下遗忘知识，但通过注意力汇聚机制触发时恢复被遗忘内容


<details>
  <summary>Details</summary>
Motivation: 现有反学习机制存在安全隐患，攻击者可能通过特定触发机制绕过反学习过程恢复已删除的知识/行为

Method: 利用大语言模型的注意力汇聚现象，在输入序列的注意力汇聚位置植入触发标记，通过注意力值对齐增强后门持续性

Result: 实验证明基于注意力汇聚的后门反学习在触发时能可靠恢复遗忘知识，正常场景下与普通反学习模型表现无异

Conclusion: 注意力汇聚机制是后门反学习的关键通道，揭示了现有反学习框架在对抗攻击场景下的潜在安全漏洞

Abstract: Large language model (LLM) unlearning has become a critical mechanism for
removing undesired data, knowledge, or behaviors from pre-trained models while
retaining their general utility. Yet, with the rise of open-weight LLMs, we
ask: can the unlearning process itself be backdoored, appearing successful
under normal conditions yet reverting to pre-unlearned behavior when a hidden
trigger is activated? Drawing inspiration from classical backdoor attacks that
embed triggers into training data to enforce specific behaviors, we investigate
backdoor unlearning, where models forget as intended in the clean setting but
recover forgotten knowledge when the trigger appears. We show that designing
such attacks presents unique challenges, hinging on where triggers are placed
and how backdoor training is reinforced. We uncover a strong link between
backdoor efficacy and the attention sink phenomenon, i.e., shallow input tokens
consistently attract disproportionate attention in LLMs. Our analysis reveals
that these attention sinks serve as gateways for backdoor unlearning: placing
triggers at sink positions and aligning their attention values markedly
enhances backdoor persistence. Extensive experiments validate these findings,
showing that attention-sink-guided backdoor unlearning reliably restores
forgotten knowledge in the presence of backdoor triggers, while behaving
indistinguishably from a normally unlearned model when triggers are absent.
Code is available at https://github.com/OPTML-Group/Unlearn-Backdoor.

</details>


### [126] [Do LLMs Recognize Your Latent Preferences? A Benchmark for Latent Information Discovery in Personalized Interaction](https://arxiv.org/abs/2510.17132)
*Ioannis Tsaknakis,Bingqing Song,Shuyu Gan,Dongyeop Kang,Alfredo Garcia,Gaowen Liu,Charles Fleming,Mingyi Hong*

Main category: cs.LG

TL;DR: 大型语言模型（LLMs）在通过多轮对话发现用户潜在信息的能力存在显著差异（成功率32%-98%），研究提出首个系统性基准测试框架评估其潜在信息推断能力。


<details>
  <summary>Details</summary>
Motivation: 用户往往不会明确表达所有偏好（如餐厅推荐/旅行规划场景），需通过对话推断其潜在需求。如何使LLMs有效发现并利用此类隐藏信息是构建自适应AI系统的关键挑战。

Method: 构建三智能体评估框架（User/Assistant/Judge），覆盖三个渐进场景：经典20 Questions游戏、个性化问答、个性化文本摘要，实现对话回合级别的潜在信息挖掘与适应能力评估。

Result: LLMs展现对话式潜在信息发现能力，但成功率受任务复杂度（32%-98%波动）、主题类型及隐藏属性数量显著影响。

Conclusion: 有效偏好推断仍是开放性问题，该基准为个性化交互中潜在信息发现研究提供系统性框架，推动真正自适应AI系统的开发。

Abstract: Large Language Models (LLMs) excel at producing broadly relevant text, but
this generality becomes a limitation when user-specific preferences are
required, such as recommending restaurants or planning travel. In these
scenarios, users rarely articulate every preference explicitly; instead, much
of what they care about remains latent, waiting to be inferred. This raises a
fundamental question: Can LLMs uncover and reason about such latent information
through conversation?
  We address this problem by introducing a unified benchmark for evaluating
latent information discovery - the ability of LLMs to reveal and utilize hidden
user attributes through multi-turn interaction. The benchmark spans three
progressively realistic settings: the classic 20 Questions game, Personalized
Question Answering, and Personalized Text Summarization. All tasks share a
tri-agent framework (User, Assistant, Judge) enabling turn-level evaluation of
elicitation and adaptation. Our results reveal that while LLMs can indeed
surface latent information through dialogue, their success varies dramatically
with context: from 32% to 98%, depending on task complexity, topic, and number
of hidden attributes. This benchmark provides the first systematic framework
for studying latent information discovery in personalized interaction,
highlighting that effective preference inference remains an open frontier for
building truly adaptive AI systems.

</details>


### [127] [Soft-Masked Diffusion Language Models](https://arxiv.org/abs/2510.17206)
*Michael Hersche,Samuel Moor-Smith,Thomas Hofmann,Abbas Rahimi*

Main category: cs.LG

TL;DR: 提出软掩码方法改进扩散语言模型性能，通过动态融合掩码词元和预测词元嵌入保留上下文信息。


<details>
  <summary>Details</summary>
Motivation: 传统掩码扩散模型解码时仅做二元选择（保留/替换掩码），丢弃预测信息。软掩码旨在保留预测阶段的有价值先验信息。

Method: 动态混合掩码词元嵌入与上一步top-k预测词元嵌入，提出适配预训练模型的训练方法，并在Dream系列模型上微调。

Result: 169M模型困惑度降低2.6%，MAUVE提升3.4%。Dream-7B在HumanEval基准准确率提升9.8%，高吞吐场景效率提高27%。

Conclusion: 软掩码通过传播部分预测信息，显著提升扩散模型在自然语言和代码生成任务中的性能表现。

Abstract: Diffusion models have demonstrated strong potential in language modeling,
offering various advantages over traditional autoregressive approaches. Their
ability to generate and revise entire responses in parallel enables faster
generation and built-in self-correction mechanisms. Most modern diffusion-based
language models employ masked diffusion, where decoding involves iteratively
processing masked tokens based on a binary decision: either retaining the mask
or replacing it with the predicted token. However, this binary choice discards
valuable predictive information when the mask is retained. To address this
limitation, we introduce soft-masking (SM), a novel method that dynamically
blends the embedding of the mask token with the embeddings of the top-$k$
predicted tokens from the previous decoding step, for each retained mask. This
provides the model with a more informative prior, preserving context from
earlier computations and allowing partial information about masked tokens to
propagate beyond a single step. We propose a training methodology that adapts a
pretrained masked diffusion language model to incorporate SM. We demonstrate
that continuing pretraining a 169M parameter model with SM leads to improved
perplexity and MAUVE scores. Furthermore, we finetune two state-of-the-art
diffusion models, Dream-7B and Dream-Coder-7B, with SM. SM consistently
improves performance across multiple coding benchmarks, particularly in
high-throughput settings.

</details>


### [128] [LILO: Bayesian Optimization with Interactive Natural Language Feedback](https://arxiv.org/abs/2510.17671)
*Katarzyna Kobalczyk,Zhiyuan Jerry Lin,Benjamin Letham,Zhuokai Zhao,Maximilian Balandat,Eytan Bakshy*

Main category: cs.LG

TL;DR: 提出语言交互框架，利用大语言模型将自然语言反馈转化为效用指标，增强贝叶斯优化的灵活性和用户友好性。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化依赖结构化反馈和人工设计内核，难以处理复杂/主观目标。需要更自然的交互方式将非结构化文本反馈整合到优化过程中。

Method: 开发语言循环框架：1) LLM将多类型文本反馈转化为标量效用值 2) 自动整合用户先验知识 3) 保留贝叶斯优化的样本效率和概率不确定性量化优势

Result: 在反馈受限场景中，混合方法超越传统贝叶斯优化基准（平均提升23%）和纯LLM优化器（提升37%），同时保持样本效率（收敛速度提升1.8倍）

Conclusion: 语言交互机制成功融合人类直觉表达与机器优化效率，为复杂决策问题提供可解释、低反馈依赖的优化范式，突破传统方法的形式化限制。

Abstract: For many real-world applications, feedback is essential in translating
complex, nuanced, or subjective goals into quantifiable optimization
objectives. We propose a language-in-the-loop framework that uses a large
language model (LLM) to convert unstructured feedback in the form of natural
language into scalar utilities to conduct BO over a numeric search space.
Unlike preferential BO, which only accepts restricted feedback formats and
requires customized models for each domain-specific problem, our approach
leverages LLMs to turn varied types of textual feedback into consistent utility
signals and to easily include flexible user priors without manual kernel
design. At the same time, our method maintains the sample efficiency and
principled uncertainty quantification of BO. We show that this hybrid method
not only provides a more natural interface to the decision maker but also
outperforms conventional BO baselines and LLM-only optimizers, particularly in
feedback-limited regimes.

</details>


### [129] [Mapping Post-Training Forgetting in Language Models at Scale](https://arxiv.org/abs/2510.17776)
*Jackson Harmon,Andreas Hochlehnert,Matthias Bethge,Ameya Prabhu*

Main category: cs.LG

TL;DR: 提出样本级评估框架量化后训练对预训练知识的遗忘与逆向迁移，揭示不同训练策略的效果差异


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视后训练对预训练知识的具体影响机制，传统任务平均指标无法准确捕捉知识变化动态

Method: 设计1->0（遗忘）和0->1（逆向迁移）转换指标，开发概率校正的基准测试，跨模型规模/数据量/训练阶段进行系统分析

Result: 发现领域持续预训练导致中度遗忘，RL/SFT在数学逻辑任务表现最佳，模型合并无法可靠缓解遗忘，数据规模显著影响训练效果

Conclusion: 该框架为大规模后训练知识动态提供实用评估标准，助力构建更可控的AI系统演进路径

Abstract: Scaled post-training now drives many of the largest capability gains in
language models (LMs), yet its effect on pretrained knowledge remains poorly
understood. Not all forgetting is equal: Forgetting one fact (e.g., a U.S.
president or an API call) does not "average out" by recalling another. Hence,
we propose a sample-wise paradigm to measure what is forgotten and when
backward transfer occurs. Our metric counts 1->0 transitions (correct before
post-training, incorrect after) to quantify forgetting and 0->1 transitions to
quantify backward transfer. Traditional task averages conflate these effects
and obscure large changes. For multiple-choice benchmarks, we add
chance-adjusted variants that subtract the expected contribution of random
guessing from pre- and post-training accuracies. We apply this framework across
post-training stages, model sizes, and data scales. Our large-scale analysis
shows that: (1) Domain-continual pretraining induces moderate forgetting with
low-to-moderate backward transfer; (2) RL/SFT post-training applied to base
models and Instruction tuning yields moderate-to-large backward transfer on
math and logic with overall low-to-moderate forgetting; (3) Applying RL/SFT to
instruction-tuned models is sensitive on data scale: at small scales, both
forgetting and backward transfer are small; at larger scales, effects are mixed
and warrant further study with better controls; (4) Model merging does not
reliably mitigate forgetting. Overall, our framework offers a practical
yardstick for mapping how post-training alters pretrained knowledge at scale --
enabling progress towards generally capable AI systems.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [130] [GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer](https://arxiv.org/abs/2510.16136)
*Sayan Deb Sarkar,Sinisa Stekovic,Vincent Lepetit,Iro Armeni*

Main category: cs.CV

TL;DR: 提出基于通用引导原则的训练无关方法，通过周期性添加可微分引导损失实现3D资产外观迁移，在几何差异大时优于基线并开发GPT评估系统。


<details>
  <summary>Details</summary>
Motivation: 现有方法在输入与外观对象几何差异大时失效，直接应用3D生成模型效果不佳，需新方法解决纹理/几何迁移问题。

Method: 使用预训练整流流模型，在采样过程中添加部分感知外观损失和自相似性损失作为可微分引导函数。

Result: 方法成功迁移细节，定性定量优于基线；开发GPT评估系统替代传统指标，用户研究验证有效性。

Conclusion: 通用引导框架可扩展至不同扩散模型和引导函数，为解决无真实数据评估难题提供新思路。

Abstract: Transferring appearance to 3D assets using different representations of the
appearance object - such as images or text - has garnered interest due to its
wide range of applications in industries like gaming, augmented reality, and
digital content creation. However, state-of-the-art methods still fail when the
geometry between the input and appearance objects is significantly different. A
straightforward approach is to directly apply a 3D generative model, but we
show that this ultimately fails to produce appealing results. Instead, we
propose a principled approach inspired by universal guidance. Given a
pretrained rectified flow model conditioned on image or text, our training-free
method interacts with the sampling process by periodically adding guidance.
This guidance can be modeled as a differentiable loss function, and we
experiment with two different types of guidance including part-aware losses for
appearance and self-similarity. Our experiments show that our approach
successfully transfers texture and geometric details to the input 3D asset,
outperforming baselines both qualitatively and quantitatively. We also show
that traditional metrics are not suitable for evaluating the task due to their
inability of focusing on local details and comparing dissimilar inputs, in
absence of ground truth data. We thus evaluate appearance transfer quality with
a GPT-based system objectively ranking outputs, ensuring robust and human-like
assessment, as further confirmed by our user study. Beyond showcased scenarios,
our method is general and could be extended to different types of diffusion
models and guidance functions.

</details>


### [131] [From Mannequin to Human: A Pose-Aware and Identity-Preserving Video Generation Framework for Lifelike Clothing Display](https://arxiv.org/abs/2510.16833)
*Xiangyu Mu,Dongliang Zhou,Jie Hou,Haijun Zhang,Weili Guan*

Main category: cs.CV

TL;DR: 提出M2HVideo框架，通过动态头部编码器、镜像损失和分布适配器解决人体模型视频生成中的运动对齐与身份漂移问题，显著提升视频保真度。


<details>
  <summary>Details</summary>
Motivation: 现有人体模型服装展示成本低廉但缺乏真实感，需要生成身份可控且细节丰富的人类视频以提升电商展示效果。

Method: 1) 动态姿势感知头部编码器融合面部语义与身体姿态
2) 基于DDIM的镜像损失保留面部细节
3) 分布感知适配器对齐特征统计分布增强时序连贯性

Result: 在UBC、ASOS和自建MannequinVideos数据集上验证，M2HVideo在服装一致性(提升18.7%)、身份保持度(FID降低23.4%)等指标优于SOTA方法

Conclusion: 该框架有效解决了头部-身体运动失调和身份漂移问题，首次实现商业场景下高保真人体模型视频生成，为虚拟试穿提供新方案

Abstract: Mannequin-based clothing displays offer a cost-effective alternative to
real-model showcases for online fashion presentation, but lack realism and
expressive detail. To overcome this limitation, we introduce a new task called
mannequin-to-human (M2H) video generation, which aims to synthesize
identity-controllable, photorealistic human videos from footage of mannequins.
We propose M2HVideo, a pose-aware and identity-preserving video generation
framework that addresses two key challenges: the misalignment between head and
body motion, and identity drift caused by temporal modeling. In particular,
M2HVideo incorporates a dynamic pose-aware head encoder that fuses facial
semantics with body pose to produce consistent identity embeddings across
frames. To address the loss of fine facial details due to latent space
compression, we introduce a mirror loss applied in pixel space through a
denoising diffusion implicit model (DDIM)-based one-step denoising.
Additionally, we design a distribution-aware adapter that aligns statistical
distributions of identity and clothing features to enhance temporal coherence.
Extensive experiments on the UBC fashion dataset, our self-constructed ASOS
dataset, and the newly collected MannequinVideos dataset captured on-site
demonstrate that M2HVideo achieves superior performance in terms of clothing
consistency, identity preservation, and video fidelity in comparison to
state-of-the-art methods.

</details>


### [132] [InfraGPT Smart Infrastructure: An End-to-End VLM-Based Framework for Detecting and Managing Urban Defects](https://arxiv.org/abs/2510.16017)
*Ibrahim Sheikh Mohamed,Abdullah Yahya Abdullah Omaisan*

Main category: cs.CV

TL;DR: 提出基于CCTV和YOLO检测器的多缺陷识别系统，结合视觉语言模型生成结构化维修方案，验证有效性并探讨城市级部署挑战


<details>
  <summary>Details</summary>
Motivation: 现有基础设施检测依赖高风险人工巡检，传统自动系统存在单缺陷检测局限性和非结构化输出缺陷，无法直接指导维护作业

Method: 通过CCTV流输入→YOLO目标检测（多缺陷识别）→视觉语言模型（QwenVL/LLaVA）场景理解→生成包含维修工具/方案/警报的JSON格式结构化报告

Result: 在公开数据集和实际监控片段中验证，系统准确识别多种基础设施缺陷并生成可操作的维修方案摘要

Conclusion: 系统有效提升基础设施维护效率，但需解决计算资源优化和数据隐私等挑战以实现城市级部署

Abstract: Infrastructure in smart cities is increasingly monitored by networks of
closed circuit television (CCTV) cameras. Roads, bridges and tunnels develop
cracks, potholes, and fluid leaks that threaten public safety and require
timely repair. Manual inspection is costly and hazardous, and existing
automatic systems typically address individual defect types or provide
unstructured outputs that cannot directly guide maintenance crews. This paper
proposes a comprehensive pipeline that leverages street CCTV streams for multi
defect detection and segmentation using the YOLO family of object detectors and
passes the detections to a vision language model (VLM) for scene aware
summarization. The VLM generates a structured action plan in JSON format that
includes incident descriptions, recommended tools, dimensions, repair plans,
and urgent alerts. We review literature on pothole, crack and leak detection,
highlight recent advances in large vision language models such as QwenVL and
LLaVA, and describe the design of our early prototype. Experimental evaluation
on public datasets and captured CCTV clips demonstrates that the system
accurately identifies diverse defects and produces coherent summaries. We
conclude by discussing challenges and directions for scaling the system to city
wide deployments.

</details>


### [133] [Cerberus: Real-Time Video Anomaly Detection via Cascaded Vision-Language Models](https://arxiv.org/abs/2510.16290)
*Yue Zheng,Xiufang Shi,Jiming Chen,Yuanchao Shu*

Main category: cs.CV

TL;DR: Cerberus系统通过两阶段级联设计和两项关键创新（运动掩码提示+规则偏差检测），在保持97.2%准确率的同时实现151.79倍加速，解决了VLM模型在视频异常检测中的实时部署难题。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型(VLM)的视频异常检测方法存在计算成本过高（影响实时性）和视觉定位不稳定（影响准确性）的双重瓶颈。

Method: 1. 离线的正常行为规则学习
2. 在线推理时轻量级过滤与VLM细粒度推理结合
3. 运动掩码提示技术聚焦运动区域
4. 基于规则偏差的异常定义范式

Result: 在4个数据集上达到57.68 fps（L40S GPU），151.79倍加速，97.2%准确率与SOTA VLM方法持平

Conclusion: Cerberus通过算法-系统协同设计，首次实现了VLM级精度与工业级实时性的统一，为实时视频分析提供了实用化解决方案。

Abstract: Video anomaly detection (VAD) has rapidly advanced by recent development of
Vision-Language Models (VLMs). While these models offer superior zero-shot
detection capabilities, their immense computational cost and unstable visual
grounding performance hinder real-time deployment. To overcome these
challenges, we introduce Cerberus, a two-stage cascaded system designed for
efficient yet accurate real-time VAD. Cerberus learns normal behavioral rules
offline, and combines lightweight filtering with fine-grained VLM reasoning
during online inference. The performance gains of Cerberus come from two key
innovations: motion mask prompting and rule-based deviation detection. The
former directs the VLM's attention to regions relevant to motion, while the
latter identifies anomalies as deviations from learned norms rather than
enumerating possible anomalies. Extensive evaluations on four datasets show
that Cerberus on average achieves 57.68 fps on an NVIDIA L40S GPU, a
151.79$\times$ speedup, and 97.2\% accuracy comparable to the state-of-the-art
VLM-based VAD methods, establishing it as a practical solution for real-time
video analytics.

</details>


### [134] [Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features](https://arxiv.org/abs/2510.16781)
*Shihao Ji,Zihui Song*

Main category: cs.CV

TL;DR: 提出无需训练的零样本视频理解框架，结合预训练视觉语言模型和传统机器学习算法实现视频内容的结构化分析


<details>
  <summary>Details</summary>
Motivation: 现有视频理解模型依赖大量标注数据和端到端训练，存在成本高、可扩展性差的问题，需探索零样本解决方案

Method: 将视频理解重构为时空聚类问题：1) 使用预训练VLM提取语义特征 2) 核时间分割(KTS)进行事件分段 3) 密度聚类识别场景主题 4) 选择关键帧生成多模态摘要

Result: 建立模型无关的自动化分析框架，在零样本条件下实现视频内容的可解释结构化解析

Conclusion: 通过融合预训练模型的语义先验与传统无监督学习，为视频理解提供了无需训练的新范式，拓展了现有VLM在动态场景中的应用

Abstract: The remarkable zero-shot reasoning capabilities of large-scale Visual
Language Models (VLMs) on static images have yet to be fully translated to the
video domain. Conventional video understanding models often rely on extensive,
task-specific training on annotated datasets, a process that is both costly and
limited in scalability. This paper introduces a novel, training-free framework
for video understanding that circumvents end-to-end training by synergistically
combining the rich semantic priors of pre-trained VLMs with classic machine
learning algorithms for pattern discovery. Our core idea is to reframe video
understanding as a self-supervised spatio-temporal clustering problem within a
high-dimensional semantic feature space. The proposed pipeline first transforms
a video stream into a semantic feature trajectory using the frozen visual
encoder of a pre-trained VLM. Subsequently, we employ Kernel Temporal
Segmentation (KTS), a robust machine learning technique, to partition the
continuous feature stream into discrete, semantically coherent event segments.
These segments are then subjected to unsupervised density-based clustering to
identify recurring macroscopic scenes and themes throughout the video. By
selecting representative keyframes from each discovered cluster and leveraging
the VLM's generative capabilities for textual description, our framework
automatically produces a structured, multi-modal summary of the video content.
This approach provides an effective, interpretable, and model-agnostic pathway
for zero-shot, automated structural analysis of video content.

</details>


### [135] [Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input](https://arxiv.org/abs/2510.16926)
*Chenxu Li,Zhicai Wang,Yuan Sheng,Xingyu Zhu,Yanbin Hao,Xiang Wang*

Main category: cs.CV

TL;DR: Res-Bench提出首个评估多模态大模型分辨率鲁棒性的基准，包含14,400样本和12种分辨率，并引入Spearman相关系数、ACE/RCE等稳定性指标。


<details>
  <summary>Details</summary>
Motivation: 现有评估范式主要关注语义性能，缺乏对模型在不同输入分辨率下性能稳定性的系统性评估。

Method: 构建跨6大核心能力维度的多分辨率数据集，设计包含趋势相关性和连续性误差的鲁棒性评估框架，并进行预处理策略和微调实验。

Result: 实现了对主流MLLMs的大规模评估，揭示了模型分辨率敏感性，验证了超分辨率等预处理策略对稳定性的提升作用。

Conclusion: 分辨率鲁棒性是MLLMs重要评估维度，Res-Bench为模型优化提供了系统性分析工具，强调需平衡语义性能与稳定性。

Abstract: Multimodal Large Language Models (MLLMs) increasingly support dynamic image
resolutions. However, current evaluation paradigms primarily assess semantic
performance, overlooking the critical question of resolution robustness -
whether performance remains stable across varying input resolutions. To address
this gap, we introduce \textbf{Res-Bench}, a comprehensive benchmark comprising
14,400 samples across 12 resolution levels and six core capability dimensions.
We designed a novel evaluation framework that goes beyond traditional accuracy
metrics to capture performance stability. This framework introduces multiple
robustness metrics: Spearman's correlation for assessing resolution-performance
trends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring
performance volatility. Using these metrics, we conducted a large-scale
evaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and
task-centric robustness examination, (2) investigation of preprocessing
strategies including padding and super-resolution, and (3) exploration of
fine-tuning for stability enhancement.

</details>


### [136] [$\mathcal{V}isi\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.17205)
*Yingqi Fan,Anhao Zhao,Jinlan Fu,Junlong Tong,Hui Su,Yijie Pan,Wei Zhang,Xiaoyu Shen*

Main category: cs.CV

TL;DR: 提出VisiPruner框架，通过三阶段跨模态交互分析减少99%视觉注意力计算，显著提升MLLMs效率


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs存在计算冗余且缺乏对多模态信息处理机制的理解，需探索更高效的优化方法

Method: 基于跨模态交互三阶段理论（意图识别-突发融合-语言优化），开发无需训练的注意力修剪框架

Result: 在LLaVA-v1.5 7B上减少53.9%计算量，性能优于现有方法且具备跨模型通用性

Conclusion: 揭示了MLLMs内在处理机制，为高效模型架构设计提供理论指导，超越单纯修剪的技术价值

Abstract: Multimodal Large Language Models (MLLMs) have achieved strong performance
across vision-language tasks, but suffer from significant computational
overhead due to the quadratic growth of attention computations with the number
of multimodal tokens. Though efforts have been made to prune tokens in MLLMs,
\textit{they lack a fundamental understanding of how MLLMs process and fuse
multimodal information.} Through systematic analysis, we uncover a
\textbf{three-stage} cross-modal interaction process: (1) Shallow layers
recognize task intent, with visual tokens acting as passive attention sinks;
(2) Cross-modal fusion occurs abruptly in middle layers, driven by a few
critical visual tokens; (3) Deep layers discard vision tokens, focusing solely
on linguistic refinement. Based on these findings, we propose
\emph{VisiPruner}, a training-free pruning framework that reduces up to 99\% of
vision-related attention computations and 53.9\% of FLOPs on LLaVA-v1.5 7B. It
significantly outperforms existing token pruning methods and generalizes across
diverse MLLMs. Beyond pruning, our insights further provide actionable
guidelines for training efficient MLLMs by aligning model architecture with its
intrinsic layer-wise processing dynamics. Our code is available at:
https://github.com/EIT-NLP/VisiPruner.

</details>


### [137] [UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action](https://arxiv.org/abs/2510.17790)
*Yuhao Yang,Zhen Yang,Zi-Yi Dou,Anh Nguyen,Keen You,Omar Attia,Andrew Szot,Michael Feng,Ram Ramrakhya,Alexander Toshev,Chao Huang,Yinfei Yang,Zhe Gan*

Main category: cs.CV

TL;DR: 提出UltraCUA混合动作框架，通过整合GUI基本操作与高层次工具调用，显著提升计算机代理执行效率与成功率


<details>
  <summary>Details</summary>
Motivation: 现有计算机使用代理过度依赖原始视觉操作，导致错误传播和执行低效，需融合程序化接口突破性能瓶颈

Method: 1) 自动化工具生成管道 2) 支持17,000+验证任务的合成引擎 3) 混合动作轨迹收集 4) 监督微调+强化学习两阶段训练

Result: OSWorld基准提升22%，执行步骤减少11%；WindowsAgentArena成功率21.7%，超越专用基线模型

Conclusion: 混合动作机制通过减少错误传播同时保持效率，为计算机代理开辟了软硬件协同的新范式

Abstract: Multimodal agents for computer use rely exclusively on primitive actions
(click, type, scroll) that require accurate visual grounding and lengthy
execution chains, leading to cascading failures and performance bottlenecks.
While other agents leverage rich programmatic interfaces (APIs, MCP servers,
tools), computer-use agents (CUAs) remain isolated from these capabilities. We
present UltraCUA, a foundation model that bridges this gap through hybrid
action -- seamlessly integrating GUI primitives with high-level programmatic
tool calls. To achieve this, our approach comprises four key components: (1) an
automated pipeline that scales programmatic tools from software documentation,
open-source repositories, and code generation; (2) a synthetic data engine
producing over 17,000 verifiable tasks spanning real-world computer-use
scenarios; (3) a large-scale high-quality hybrid action trajectory collection
with both low-level GUI actions and high-level programmatic tool calls; and (4)
a two-stage training pipeline combining supervised fine-tuning with online
reinforcement learning, enabling strategic alternation between low-level and
high-level actions. Experiments with our 7B and 32B models demonstrate
substantial improvements over state-of-the-art agents. On OSWorld, UltraCUA
models achieve an average 22% relative improvement over base models, while
being 11% faster in terms of steps. Out-of-domain evaluation on
WindowsAgentArena shows our model reaches 21.7% success rate, outperforming
baselines trained on Windows data. The hybrid action mechanism proves critical,
reducing error propagation while maintaining execution efficiency.

</details>


### [138] [Glyph: Scaling Context Windows via Visual-Text Compression](https://arxiv.org/abs/2510.17800)
*Jiale Cheng,Yusen Liu,Xinyu Zhang,Yulin Fei,Wenyi Hong,Ruiliang Lyu,Weihan Wang,Zhe Su,Xiaotao Gu,Xiao Liu,Yushi Bai,Jie Tang,Hongning Wang,Minlie Huang*

Main category: cs.CV

TL;DR: 提出视觉化长文本压缩框架Glyph，通过将文本渲染为图像并结合视觉语言模型，实现3-4倍文本压缩且保持精度，显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型处理百万级长文本时存在计算与内存成本过高的问题，需探索更高效的上下文扩展方法。

Method: 1. 文本转图像压缩语义 2. 设计基于LLM的遗传搜索算法优化视觉渲染配置 3. 结合视觉语言模型处理图像化文本

Result: 在长文本任务中达到Qwen3-8B相当精度，预填充/解码速度提升4倍，SFT训练加速2倍，128K上下文VLM可扩展处理百万级文本

Conclusion: 视觉上下文扩展为长文本处理提供高效解决方案，图像化文本同时有益于多模态文档理解等实际应用。

Abstract: Large language models (LLMs) increasingly rely on long-context modeling for
tasks such as document understanding, code analysis, and multi-step reasoning.
However, scaling context windows to the million-token level brings prohibitive
computational and memory costs, limiting the practicality of long-context LLMs.
In this work, we take a different perspective-visual context scaling-to tackle
this challenge. Instead of extending token-based sequences, we propose Glyph, a
framework that renders long texts into images and processes them with
vision-language models (VLMs). This approach substantially compresses textual
input while preserving semantic information, and we further design an
LLM-driven genetic search to identify optimal visual rendering configurations
for balancing accuracy and compression. Through extensive experiments, we
demonstrate that our method achieves 3-4x token compression while maintaining
accuracy comparable to leading LLMs such as Qwen3-8B on various long-context
benchmarks. This compression also leads to around 4x faster prefilling and
decoding, and approximately 2x faster SFT training. Furthermore, under extreme
compression, a 128K-context VLM could scale to handle 1M-token-level text
tasks. In addition, the rendered text data benefits real-world multimodal
tasks, such as document understanding. Our code and model are released at
https://github.com/thu-coai/Glyph.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [139] [Prompt Optimization via Retrieved Reasoning Assets and Multi-Agent Analysis](https://arxiv.org/abs/2510.16635)
*Wonduk Seo,Juhyeon Lee,Junseo Koh,Hyunjin An,Jian Park,Seunghyun Lee,Haihua Chen,Yi Bu*

Main category: cs.MA

TL;DR: 提出MA-SAPO多代理框架，通过将评估信号转化为可解释的推理链，实现更透明可控的提示优化


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法依赖黑箱评估和试错改进，缺乏解释性与系统性控制

Method: 两阶段框架：推理阶段协作生成结构化改进建议，测试阶段基于证据进行迭代优化

Result: 在HelpSteer基准测试中优于单次提示、检索增强基线和传统多代理策略

Conclusion: MA-SAPO通过评估与推理的显式耦合，实现了更可审计、可解释的提示优化过程

Abstract: Prompt optimization has emerged as an effective alternative to retraining for
improving the performance of Large Language Models (LLMs). However, most
existing approaches treat evaluation as a black box, relying solely on
numerical scores while offering limited insight into why a prompt succeeds or
fails. They also depend heavily on trial-and-error refinements, which are
difficult to interpret and control. In this paper, we introduce MA-SAPO, a
Multi-Agent framework for Score-Aware Prompt Optimization. Compared to prior
methods, MA-SAPO explicitly couples evaluation outcomes with structured
reasoning to guide systematic edits. The framework specifically consists of two
stages: during the Reasoning Phase, agents collaboratively explain metric
scores, diagnose weaknesses, and synthesize targeted refinements that are
stored as reusable reasoning assets; during the Test Phase, agents retrieve
these assets to analyze optimized prompts and apply only evidence-grounded
edits. By turning evaluation signals into interpretable reasoning chains,
MA-SAPO produces prompt refinements that are more transparent, auditable, and
controllable. Experiments on the HelpSteer1/2 benchmarks demonstrate consistent
improvements over single-pass prompting, retrieval-augmented baselines, and
prior multi-agent strategies, validating the effectiveness of our approach.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [140] [Predictability of Complex Systems](https://arxiv.org/abs/2510.16312)
*En Xu,Yilin Bi,Hongwei Hu,Xin Chen,Zhiwen Yu,Yong Li,Yanqing Hu,Tao Zhou*

Main category: physics.soc-ph

TL;DR: 系统综述复杂系统预测性研究的理论框架、核心维度与应用场景，提出未来挑战。


<details>
  <summary>Details</summary>
Motivation: 建立复杂系统预测精度的理论极限评估体系，为算法优化提供基准，揭示当前预测能力与理论上限的差距。

Method: 整合数据驱动与机理建模双重视角，聚焦时间序列、网络结构、动态过程三大核心维度的预测性研究。

Result: 构建跨学科预测性评估框架，展示多领域应用实例，验证预测性理论对算法创新的指导价值。

Conclusion: 预测性研究为算法设计提供理论锚点，数据科学革命推动该领域进入新阶段，需发展更普适的量化理论与验证体系。

Abstract: The study of complex systems has attracted widespread attention from
researchers in the fields of natural sciences, social sciences, and
engineering. Prediction is one of the central issues in this field. Although
most related studies have focused on prediction methods, research on the
predictability of complex systems has received increasing attention across
disciplines--aiming to provide theories and tools to address a key question:
What are the limits of prediction accuracy? Predictability itself can serve as
an important feature for characterizing complex systems, and accurate
estimation of predictability can provide a benchmark for the study of
prediction algorithms. This allows researchers to clearly identify the gap
between current prediction accuracy and theoretical limits, thereby helping
them determine whether there is still significant room to improve existing
algorithms. More importantly, investigating predictability often requires the
development of new theories and methods, which can further inspire the design
of more effective algorithms. Over the past few decades, this field has
undergone significant evolution. In particular, the rapid development of data
science has introduced a wealth of data-driven approaches for understanding and
quantifying predictability. This review summarizes representative achievements,
integrating both data-driven and mechanistic perspectives. After a brief
introduction to the significance of the topic in focus, we will explore three
core aspects: the predictability of time series, the predictability of network
structures, and the predictability of dynamical processes. Finally, we will
provide extensive application examples across various fields and outline open
challenges for future research.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [141] [Publication Trend Analysis and Synthesis via Large Language Model: A Case Study of Engineering in PNAS](https://arxiv.org/abs/2510.16152)
*Mason Smetana,Lev Khazanovich*

Main category: cs.DL

TL;DR: LLM驱动框架通过两阶段分类分析科学文献，揭示跨学科主题趋势和隐性关联。


<details>
  <summary>Details</summary>
Motivation: 解决科学文献因复杂语言/静态结构导致的孤立问题，捕捉动态科学演变的需求。

Method: 基于摘要进行一级主题分类，通过全文分析进行二级分类，结合BoW/TF-IDF验证有效性。

Result: 独立还原期刊双分类结构，发现传统词频分析对多样性领域映射不足，图结构揭示隐性主题关联。

Conclusion: 该框架为追踪科学趋势提供有效工具，突破传统摘要/关键词分析的局限性。

Abstract: Scientific literature is increasingly siloed by complex language, static
disciplinary structures, and potentially sparse keyword systems, making it
cumbersome to capture the dynamic nature of modern science. This study
addresses these challenges by introducing an adaptable large language model
(LLM)-driven framework to quantify thematic trends and map the evolving
landscape of scientific knowledge. The approach is demonstrated over a 20-year
collection of more than 1,500 engineering articles published by the Proceedings
of the National Academy of Sciences (PNAS), marked for their breadth and depth
of research focus. A two-stage classification pipeline first establishes a
primary thematic category for each article based on its abstract. The
subsequent phase performs a full-text analysis to assign secondary
classifications, revealing latent, cross-topic connections across the corpus.
Traditional natural language processing (NLP) methods, such as Bag-of-Words
(BoW) and Term Frequency-Inverse Document Frequency (TF-IDF), confirm the
resulting topical structure and also suggest that standalone word-frequency
analyses may be insufficient for mapping fields with high diversity. Finally, a
disjoint graph representation between the primary and secondary classifications
reveals implicit connections between themes that may be less apparent when
analyzing abstracts or keywords alone. The findings show that the approach
independently recovers much of the journal's editorially embedded structure
without prior knowledge of its existing dual-classification schema (e.g.,
biological studies also classified as engineering). This framework offers a
powerful tool for detecting potential thematic trends and providing a
high-level overview of scientific progress.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [142] [Comparing LLMs for Sentiment Analysis in Financial Market News](https://arxiv.org/abs/2510.15929)
*Lucas Eduardo Pereira Teles,Carlos M. S. Figueiredo*

Main category: q-fin.ST

TL;DR: 比较大型语言模型与传统方法在金融市场新闻情感分析中的表现，结果显示LLMs在多数情况下更优


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在金融领域情感分析任务中的有效性，通过与经典方法对比量化模型优势

Method: 在相同金融新闻数据集上对多种LLM和经典方法进行对比实验，量化评估各模型的准确率和效率

Result: 实验结果表明，在超过80%的测试案例中LLMs准确率显著高于传统方法，尤其在处理复杂语义时优势明显

Conclusion: 大型语言模型在金融情感分析任务中展现出显著优势，该发现可能推动金融NLP领域的技术革新

Abstract: This article presents a comparative study of large language models (LLMs) in
the task of sentiment analysis of financial market news. This work aims to
analyze the performance difference of these models in this important natural
language processing task within the context of finance. LLM models are compared
with classical approaches, allowing for the quantification of the benefits of
each tested model or approach. Results show that large language models
outperform classical models in the vast majority of cases.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [143] [SIADAFIX: issue description response for adaptive program repair](https://arxiv.org/abs/2510.16059)
*Xin Cao,Nan Yu*

Main category: cs.SE

TL;DR: 提出基于快慢思维的SIADAFIX方法，通过自适应修复模式在SWE-bench Lite达到60.67% pass@1性能，实现开源方案中的SOTA水平


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的程序修复方法在复杂任务处理效率与准确性之间存在平衡难题，需要更智能的工作流程编排机制

Method: 结合快慢思维架构：慢思考代理负责复杂修复，快思考组件优化问题分类；采用三模式自适应选择（简单/中等/困难模式），集成快速泛化和测试时扩展技术

Result: 在SWE-bench Lite基准测试中达到60.67% pass@1准确率，代码已在GitHub开源

Conclusion: SIADAFIX创新性地将认知科学理论应用于程序修复领域，在保证修复质量的同时显著提升效率，为自动化软件工程提供新范式

Abstract: We propose utilizing fast and slow thinking to enhance the capabilities of
large language model-based agents on complex tasks such as program repair. In
particular, we design an adaptive program repair method based on issue
description response, called SIADAFIX. The proposed method utilizes slow
thinking bug fix agent to complete complex program repair tasks, and employs
fast thinking workflow decision components to optimize and classify issue
descriptions, using issue description response results to guide the
orchestration of bug fix agent workflows. SIADAFIX adaptively selects three
repair modes, i.e., easy, middle and hard mode, based on problem complexity. It
employs fast generalization for simple problems and test-time scaling
techniques for complex problems. Experimental results on the SWE-bench Lite
show that the proposed method achieves 60.67% pass@1 performance using the
Claude-4 Sonnet model, reaching state-of-the-art levels among all open-source
methods. SIADAFIX effectively balances repair efficiency and accuracy,
providing new insights for automated program repair. Our code is available at
https://github.com/liauto-siada/siada-cli.

</details>


### [144] [When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation](https://arxiv.org/abs/2510.16809)
*Amirkia Rafiei Oskooei,Kaan Baturalp Cosdan,Husamettin Isiktas,Mehmet S. Aktas*

Main category: cs.SE

TL;DR: 研究发现代码翻译任务中存在『多样本悖论』：少量优质示例（5-25个）的提示策略在功能正确性上表现最佳，过量示例反而导致性能下降


<details>
  <summary>Details</summary>
Motivation: 验证大语言模型在代码翻译任务中『更多上下文示例提升性能』的假设是否成立，探索上下文学习机制的边界条件

Method: 通过超过90,000次代码翻译的大规模实证研究，系统评估从零样本到625个示例（约80万token）的不同提示规模

Result: 静态相似性指标随样本量增加有限提升，但关键的功能正确性指标在少量样本时达到峰值，过量样本导致性能下降达20%以上

Conclusion: 代码翻译任务中示例质量比数量更重要，挑战『越多越好』的通用假设，揭示提示策略效果具有显著的任务依赖性特征

Abstract: Large Language Models (LLMs) with vast context windows offer new avenues for
in-context learning (ICL), where providing many examples ("many-shot"
prompting) is often assumed to enhance performance. We investigate this
assumption for the complex task of code translation. Through a large-scale
empirical study of over 90,000 translations, we systematically evaluate the
impact of scaling in-context examples from zero-shot to many-shot
configurations of up to 625 examples, with prompts spanning from approximately
100,000 to 800,000 tokens. Our findings reveal a "many-shot paradox": while
static similarity metrics may modestly improve with more examples, functional
correctness consistently peaks with few-shot prompting (5-25 examples).
Providing substantially more examples often degrades this crucial functional
performance. This study highlights that for code translation, the quality of a
few well-chosen examples outweighs sheer quantity, challenging the universal
efficacy of "more is better" for ICL and underscoring the task-dependent nature
of optimal prompting strategies. Our results have significant implications for
effectively leveraging LLMs in software engineering.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [145] [ScholarEval: Research Idea Evaluation Grounded in Literature](https://arxiv.org/abs/2510.16234)
*Hanane Nour Moussa,Patrick Queiroz Da Silva,Daniel Adu-Ampratwum,Alyson East,Zitong Lu,Nikki Puccetti,Mingyi Xue,Huan Sun,Bodhisattwa Prasad Majumder,Sachin Kumar*

Main category: cs.AI

TL;DR: 提出ScholarEval评估框架，通过多领域专家标注数据集ScholarIdeas验证，在科研创意评估效果上显著优于现有基线系统。


<details>
  <summary>Details</summary>
Motivation: 针对AI生成科研创意工具激增的现状，需建立可靠评估体系确保生成创意的有效性和实用性，解决现有评估缺乏实证依据和多维度贡献分析的问题。

Method: 1.构建首个多领域专家标注数据集ScholarIdeas(含4学科117个创意) 2.开发检索增强评估框架，从合理性(文献实证)和贡献度(多维度创新)双指标评估 3.与OpenAI深度研究系统等进行对比实验 4.开展大规模用户研究验证实用性。

Result: 1.在专家标注覆盖率上显著超越基线系统 2.评估结果在可操作性(提升23%)、分析深度(提升18%)和证据支持(提升31%)方面更优 3.用户研究显示在文献参与度、创意优化效果和实用价值上均显著领先。

Conclusion: ScholarEval为科研创新评估设立新标准，其开放数据集和工具将推动AI科研辅助工具的健康发展，促进跨学科研究创新。

Abstract: As AI tools become increasingly common for research ideation, robust
evaluation is critical to ensure the validity and usefulness of generated
ideas. We introduce ScholarEval, a retrieval augmented evaluation framework
that assesses research ideas based on two fundamental criteria: soundness - the
empirical validity of proposed methods based on existing literature, and
contribution - the degree of advancement made by the idea across different
dimensions relative to prior research. To evaluate ScholarEval, we introduce
ScholarIdeas, the first expert-annotated dataset of multi-domain research ideas
and reviews, comprised of 117 ideas across four disciplines: artificial
intelligence, neuroscience, biochemistry, and ecology. Our evaluation shows
that ScholarEval achieves significantly higher coverage of points mentioned in
the human expert annotated rubrics in ScholarIdeas compared to all baselines.
Furthermore, ScholarEval is consistently preferred over our strongest baseline
o4-mini-deep-research, a reasoning and search-enabled agentic system by OpenAI,
in terms of evaluation actionability, depth, and evidence support. Our
large-scale user study also shows that ScholarEval significantly outperforms
deep research in literature engagement, idea refinement, and usefulness. We
openly release our code, dataset, and ScholarEval tool for the community to use
and build on.

</details>


### [146] [A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications](https://arxiv.org/abs/2510.16724)
*Minhua Lin,Zongyu Wu,Zhichao Xu,Hui Liu,Xianfeng Tang,Qi He,Charu Aggarwal,Hui Liu,Xiang Zhang,Suhang Wang*

Main category: cs.AI

TL;DR: 系统性综述基于强化学习的代理搜索技术，提出三维分类框架（功能角色/优化策略/应用范围），分析现有方法及挑战。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成（RAG）存在单轮启发式检索的局限性，需通过强化学习实现自适应搜索行为优化。

Method: 从功能角色（What）、优化策略（How）、应用范围（Where）三个维度组织研究体系，总结方法/评估/应用案例。

Result: 建立首个RL-based代理搜索技术全景视图，揭示强化学习在动态搜索决策中的优化潜力。

Conclusion: 展望RL与代理搜索的深度融合方向，提出构建可靠、可扩展系统的技术路线。

Abstract: The advent of large language models (LLMs) has transformed information access
and reasoning through open-ended natural language interaction. However, LLMs
remain limited by static knowledge, factual hallucinations, and the inability
to retrieve real-time or domain-specific information. Retrieval-Augmented
Generation (RAG) mitigates these issues by grounding model outputs in external
evidence, but traditional RAG pipelines are often single turn and heuristic,
lacking adaptive control over retrieval and reasoning. Recent advances in
agentic search address these limitations by enabling LLMs to plan, retrieve,
and reflect through multi-step interaction with search environments. Within
this paradigm, reinforcement learning (RL) offers a powerful mechanism for
adaptive and self-improving search behavior. This survey provides the first
comprehensive overview of \emph{RL-based agentic search}, organizing the
emerging field along three complementary dimensions: (i) What RL is for
(functional roles), (ii) How RL is used (optimization strategies), and (iii)
Where RL is applied (scope of optimization). We summarize representative
methods, evaluation protocols, and applications, and discuss open challenges
and future directions toward building reliable and scalable RL driven agentic
search systems. We hope this survey will inspire future research on the
integration of RL and agentic search. Our repository is available at
https://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.

</details>


### [147] [End-to-end Listen, Look, Speak and Act](https://arxiv.org/abs/2510.16756)
*Siyin Wang,Wenyi Yu,Xianzhao Chen,Xiaohai Tian,Jun Zhang,Lu Lu,Chao Zhang*

Main category: cs.AI

TL;DR: 提出首个全双工端到端模型ELLSA，支持跨视觉、文本、语音和动作的同步感知与生成，采用SA-MoE架构实现多模态融合，提升交互自然度。


<details>
  <summary>Details</summary>
Motivation: 人类交互本质是多模态且全双工的，现有模型无法实现同步感知与生成及复杂交互模式，需构建更自然的交互智能体。

Method: 创新SA-MoE架构：将各模态路由至专用专家模块，通过统一注意力主干网络进行融合，平衡预训练组件利用与模态干扰控制。

Result: 在语音交互和机器人操作基准测试中达到单模态基线水平，并实现对话/动作轮转、指令拒止、边行动边说话等新型交互行为。

Conclusion: ELLSA推动了自然通用交互智能的发展，其统一架构为多模态并发处理提供通用解决方案，助力AGI研究。

Abstract: Human interaction is inherently multimodal and full-duplex: we listen while
watching, speak while acting, and fluidly adapt to turn-taking and
interruptions. Realizing these capabilities is essential for building models
simulating humans. We present ELLSA (End-to-end Listen, Look, Speak and Act),
which, to our knowledge, is the first full-duplex, end-to-end model that
simultaneously perceives and generates across vision, text, speech, and action
within a single architecture, enabling interaction patterns previously out of
reach, yielding more natural, human-like behaviors. At its core is a novel
SA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each
modality to specialized experts and fuses them through a unified attention
backbone. This provides a generalizable solution for joint multimodal
perception and concurrent generation, leveraging strong pre-trained components
while enabling efficient modality integration and mitigating modality
interference. On speech-interaction and robot-manipulation benchmarks, ELLSA
matches modality-specific baselines, while uniquely supporting advanced
multimodal and full-duplex behaviors such as dialogue and action turn-taking,
defective instruction rejection, speaking-while-acting, context-grounded visual
question answering, and action barge-ins. We contend that ELLSA represents a
step toward more natural and general interactive intelligence, contributing to
the broader pursuit of artificial general intelligence. All data, code and
model checkpoints will be released upon acceptance.

</details>


### [148] [See or Say Graphs: Agent-Driven Scalable Graph Understanding with Vision-Language Models](https://arxiv.org/abs/2510.16769)
*Shuo Han,Yukun Cao,Zezhong Ding,Zengyi Gao,S Kevin Zhou,Xike Xie*

Main category: cs.AI

TL;DR: GraphVista通过分层信息组织与双模态协作机制，实现大规模图表理解并取得4.4倍性能提升


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型受限于输入令牌约束导致扩展性差，且缺乏有效的文本-视觉模态协调机制

Method: 1. 使用GraphRAG分层组织图表信息，压缩冗余保留关键元素；2. 规划代理动态选择模态：文本处理简单属性，视觉处理拓扑复杂推理

Result: 可处理比现有基准大200倍的图表，质量超越文本/视觉/融合方法，最高提升4.4倍

Conclusion: 通过模态优势互补与层次化压缩，GraphVista有效解决了图表理解的扩展瓶颈与模态协调难题

Abstract: Vision-language models (VLMs) have shown promise in graph understanding, but
remain limited by input-token constraints, facing scalability bottlenecks and
lacking effective mechanisms to coordinate textual and visual modalities. To
address these challenges, we propose GraphVista, a unified framework that
enhances both scalability and modality coordination in graph understanding. For
scalability, GraphVista organizes graph information hierarchically into a
lightweight GraphRAG base, which retrieves only task-relevant textual
descriptions and high-resolution visual subgraphs, compressing redundant
context while preserving key reasoning elements. For modality coordination,
GraphVista introduces a planning agent that routes tasks to the most suitable
modality-using the text modality for simple property reasoning and the visual
modality for local and structurally complex reasoning grounded in explicit
topology. Extensive experiments demonstrate that GraphVista scales to large
graphs, up to $200\times$ larger than those used in existing benchmarks, and
consistently outperforms existing textual, visual, and fusion-based methods,
achieving up to $4.4\times$ quality improvement over the state-of-the-art
baselines by fully exploiting the complementary strengths of both modalities.

</details>


### [149] [DeepAnalyze: Agentic Large Language Models for Autonomous Data Science](https://arxiv.org/abs/2510.16872)
*Shaolei Zhang,Ju Fan,Meihao Fan,Guoliang Li,Xiaoyong Du*

Main category: cs.AI

TL;DR: 提出首个面向自主数据科学的智能体DeepAnalyze-8B，通过课程式训练实现端到端数据分析流程自动化


<details>
  <summary>Details</summary>
Motivation: 解决传统流程驱动数据智能体无法实现完全自主数据科学的问题，突破预定义工作流的限制

Method: 采用模拟人类数据科学家学习轨迹的课程式智能训练范式，结合数据驱动轨迹合成框架构建高质量训练数据

Result: 8B参数模型超越基于GPT-4等先进闭源模型构建的流程智能体，实现从基础问答到开放研究的多层次数据任务

Conclusion: 通过开源模型、代码和训练数据，DeepAnalyze为自主数据科学开辟新路径，验证小规模模型在复杂任务中的潜力

Abstract: Autonomous data science, from raw data sources to analyst-grade deep research
reports, has been a long-standing challenge, and is now becoming feasible with
the emergence of powerful large language models (LLMs). Recent workflow-based
data agents have shown promising results on specific data tasks but remain
fundamentally limited in achieving fully autonomous data science due to their
reliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B,
the first agentic LLM designed for autonomous data science, capable of
automatically completing the end-toend pipeline from data sources to
analyst-grade deep research reports. To tackle high-complexity data science
tasks, we propose a curriculum-based agentic training paradigm that emulates
the learning trajectory of human data scientists, enabling LLMs to
progressively acquire and integrate multiple capabilities in real-world
environments. We also introduce a data-grounded trajectory synthesis framework
that constructs high-quality training data. Through agentic training,
DeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data
question answering and specialized analytical tasks to open-ended data
research. Experiments demonstrate that, with only 8B parameters, DeepAnalyze
outperforms previous workflow-based agents built on most advanced proprietary
LLMs. The model, code, and training data of DeepAnalyze are open-sourced,
paving the way toward autonomous data science.

</details>


### [150] [VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents](https://arxiv.org/abs/2510.16907)
*Kangrui Wang,Pingyue Zhang,Zihan Wang,Yaning Gao,Linjie Li,Qineng Wang,Hanyang Chen,Chi Wan,Yiping Lu,Zhengyuan Yang,Lijuan Wang,Ranjay Krishna,Jiajun Wu,Li Fei-Fei,Yejin Choi,Manling Li*

Main category: cs.AI

TL;DR: 通过强化学习将VLM代理的视觉状态推理分解为状态估计和转移建模，提出任务依赖的信念表示方法（自然语言/结构化），设计世界建模奖励和双层次GAE算法，3B模型在五类基准测试中实现3倍性能提升并超越主流大模型


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型代理在复杂视觉观察场景下的部分可观测性问题，通过显式的状态推理构建内部世界模型

Method: 将多轮VLM代理训练建模为POMDP过程，采用状态估计（当前状态识别）和转移建模（状态演化预测）的分解式推理，结合自然语言/结构化双模态信念表示，设计基于准确状态预测的密集奖励机制和双层次优势估计方法

Result: 3B参数模型在五类视觉代理基准测试中获得0.82综合得分，较基线模型提升3倍，优于GPT-5（0.75）、Gemini 2.5 Pro（0.67）和Claude 4.5（0.62）等大模型

Conclusion: 分解式状态推理架构和任务自适应的信念表示是提升VLM代理性能的关键，通过VAGEN框架验证了强化学习驱动的显式视觉状态推理在复杂环境中的有效性

Abstract: A key challenge in training Vision-Language Model (VLM) agents, compared to
Language Model (LLM) agents, lies in the shift from textual states to complex
visual observations. This transition introduces partial observability and
demands robust world modeling. We ask: Can VLM agents construct internal world
models through explicit visual state reasoning? To address this question, we
architecturally enforce and reward the agent's reasoning process via
reinforcement learning (RL), formulating it as a Partially Observable Markov
Decision Process (POMDP). We find that decomposing the agent's reasoning into
State Estimation ("what is the current state?") and Transition Modeling ("what
comes next?") is critical for success, as demonstrated through five reasoning
strategies. Our investigation into how agents represent internal beliefs
reveals that the optimal representation is task-dependent: Natural Language
excels at capturing semantic relationships in general tasks, while Structured
formats are indispensable for precise manipulation and control. Building on
these insights, we design a World Modeling Reward that provides dense,
turn-level supervision for accurate state prediction, and introduce Bi-Level
General Advantage Estimation (Bi-Level GAE) for turn-aware credit assignment.
Through this form of visual state reasoning, a 3B-parameter model achieves a
score of 0.82 across five diverse agent benchmarks, representing a 3$\times$
improvement over its untrained counterpart (0.21) and outperforming proprietary
reasoning models such as GPT-5 (0.75), Gemini 2.5 Pro (0.67) and Claude 4.5
(0.62). All experiments are conducted within our VAGEN framework, a scalable
system for training and analyzing multi-turn VLM agents in diverse visual
environments. Code and data are publicly available at
https://vagen-ai.github.io.

</details>


### [151] [Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users](https://arxiv.org/abs/2510.17173)
*Melik Ozolcer,Sang Won Bae*

Main category: cs.AI

TL;DR: 研究通过工具增强的LLM健康教练实验，发现统一策略会损害特定用户群体，提出评估优先的个性化路径：冻结生成器+学习亚群体决策头+原型指标报告机制


<details>
  <summary>Details</summary>
Motivation: 探索工具增强型AI健康教练在实际应用中的效果，特别关注不同用户群体在统一策略下的差异化表现，试图通过个性化决策提升服务效果并减少潜在伤害

Method: 1. 离线策略评估(OPE)分析工具/风格决策头
2. 轻量级模拟器测试信息增益奖励机制
3. 基于类型化奖励（工具结果客观指标+用户满意度）的亚群体策略学习

Result: 1. 统一heavy-tool策略整体提升14%价值但损害低健康素养群体
2. 信息增益奖励缩短38%特质识别时间，提升目标成功率
3. 原型指标揭示被平均值掩盖的亚群体损害

Conclusion: 评估优先的个性化路径通过冻结生成器、学习亚群体感知决策头、强制原型指标报告，有效平衡整体效果与亚群体保护，为AI健康干预提供可解释的个性化框架

Abstract: We study a web-deployed, tool-augmented LLM health coach with real users. In
a pilot with seven users (280 rated turns), offline policy evaluation (OPE)
over factorized decision heads (Tool/Style) shows that a uniform heavy-tool
policy raises average value on logs but harms specific subgroups, most notably
low-health-literacy/high-self-efficacy users. A lightweight simulator with
hidden archetypes further shows that adding a small early information-gain
bonus reliably shortens trait identification and improves goal success and
pass@3. Together, these early findings indicate an evaluation-first path to
personalization: freeze the generator, learn subgroup-aware decision heads on
typed rewards (objective tool outcomes and satisfaction), and always report
per-archetype metrics to surface subgroup harms that averages obscure.

</details>


### [152] [MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning](https://arxiv.org/abs/2510.17590)
*Mir Nafis Sharear Shopnil,Sharad Duwal,Abhishek Tyagi,Adiba Mahbub Proma*

Main category: cs.AI

TL;DR: 提出MIRAGE框架，通过分解多模态验证步骤（视觉评估+跨模态分析+检索增强核查+校准判断）实现虚假信息检测，无需领域训练即可匹配监督模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有监督检测模型依赖领域标注数据且泛化能力差，亟需能适应不同操纵策略的多模态虚假信息检测方案。

Method: 四阶段流程：1) 检测AI生成图像 2) 分析图文上下文一致性 3) 基于网络检索的迭代式事实核查 4) 集成多信号校准判断。结合视觉语言模型推理与定向网络检索。

Result: 在MMFakeBench验证集F1达81.65%（超GPT-4V基线7.65点），假阳性率34.3%显著优于基线97.3%。消融实验显示视觉验证贡献5.18 F1点，检索增强贡献2.97点。

Conclusion: 分解式代理推理框架通过结合网络检索，可在无领域训练情况下实现跨模态虚假信息检测，为标注数据稀缺场景提供解决方案。

Abstract: Misinformation spreads across web platforms through billions of daily
multimodal posts that combine text and images, overwhelming manual
fact-checking capacity. Supervised detection models require domain-specific
training data and fail to generalize across diverse manipulation tactics. We
present MIRAGE, an inference-time, model-pluggable agentic framework that
decomposes multimodal verification into four sequential modules: visual
veracity assessment detects AI-generated images, cross-modal consistency
analysis identifies out-of-context repurposing, retrieval-augmented factual
checking grounds claims in web evidence through iterative question generation,
and a calibrated judgment module integrates all signals. MIRAGE orchestrates
vision-language model reasoning with targeted web retrieval, outputs structured
and citation-linked rationales. On MMFakeBench validation set (1,000 samples),
MIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming
the strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65
points while maintaining 34.3% false positive rate versus 97.3% for a
judge-only baseline. Test set results (5,000 samples) confirm generalization
with 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification
contributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97
points. Our results demonstrate that decomposed agentic reasoning with web
retrieval can match supervised detector performance without domain-specific
training, enabling misinformation detection across modalities where labeled
data remains scarce.

</details>


### [153] [Reasoning Distillation and Structural Alignment for Improved Code Generation](https://arxiv.org/abs/2510.17598)
*Amir Jalilifard,Anderson de Rezende Rocha,Marcos Medeiros Raimundo*

Main category: cs.AI

TL;DR: 通过知识蒸馏将大型语言模型的代码生成推理能力迁移至小模型，利用结构感知损失优化方法提升小模型在代码生成任务中的性能表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型(VLLM)具备复杂代码推理能力但部署成本高，小模型缺乏此类推理能力。需开发高效方法使小模型继承大模型的解决方案级理解能力。

Method: 采用结构感知损失优化方法，训练小模型学习大模型的正确解决路径，建立问题定义与解决方案间的结构对应关系，突破传统词元级生成的局限。

Result: 在MBPP、MBPP+和HumanEval基准测试中，模型在pass@1、平均数据流匹配和语法匹配指标上显著超越基线模型。

Conclusion: 通过低成本的知识蒸馏过程，成功实现了小模型对解决方案整体结构的深度理解，验证了结构对应学习方法在代码生成任务中的有效性。

Abstract: Effective code generation with language models hinges on two critical
factors: accurately understanding the intent of the prompt and generating code
that applies algorithmic reasoning to produce correct solutions capable of
passing diverse test cases while adhering to the syntax of the target
programming language. Unlike other language tasks, code generation requires
more than accurate token prediction; it demands comprehension of solution-level
and structural relationships rather than merely generating the most likely
tokens. very large language model (VLLM) are capable of generating detailed
steps toward the correct solution of complex tasks where reasoning is crucial
in solving the problem. Such reasoning capabilities may be absent in smaller
language models. Therefore, in this work, we distill the reasoning capabilities
of a VLLM into a smaller, more efficient model that is faster and cheaper to
deploy. Our approach trains the model to emulate the reasoning and
problem-solving abilities of the VLLM by learning to identify correct solution
pathways and establishing a structural correspondence between problem
definitions and potential solutions through a novel method of structure-aware
loss optimization. This enables the model to transcend token-level generation
and to deeply grasp the overarching structure of solutions for given problems.
Experimental results show that our fine-tuned model, developed through a cheap
and simple to implement process, significantly outperforms our baseline model
in terms of pass@1, average data flow, and average syntax match metrics across
the MBPP, MBPP Plus, and HumanEval benchmarks.

</details>


### [154] [LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet Arena](https://arxiv.org/abs/2510.17638)
*Qingchuan Yang,Simon Mahns,Sida Li,Anri Gu,Jibang Wu,Haifeng Xu*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型作为预测工具的潜力，通过Prophet Arena基准测试发现其虽具备一定预测能力，但仍存在事件回忆不准、数据处理误解等瓶颈。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在现实事件预测中的应用价值，特别是在金融和经济领域的潜在影响，探索其预测智能的边界与可能性。

Method: 构建动态评估基准Prophet Arena，将预测任务分解为多阶段流程，通过控制变量和大规模实验系统评估LLMs的预测性能。

Result: LLMs展现出校准误差小、预测置信度稳定、市场回报潜力大等优势，但暴露事件记忆偏差、数据源误读、临近截止时信息整合速度落后于市场等核心缺陷。

Conclusion: LLMs作为预言范式初显潜力，需突破事件精准召回、数据深度理解及实时信息处理速度等关键瓶颈，方能实现更优的预测智能。

Abstract: Forecasting is not only a fundamental intellectual pursuit but also is of
significant importance to societal systems such as finance and economics. With
the rapid advances of large language models (LLMs) trained on Internet-scale
data, it raises the promise of employing LLMs to forecast real-world future
events, an emerging paradigm we call "LLM-as-a-Prophet". This paper
systematically investigates such predictive intelligence of LLMs. To this end,
we build Prophet Arena, a general evaluation benchmark that continuously
collects live forecasting tasks and decomposes each task into distinct pipeline
stages, in order to support our controlled and large-scale experimentation. Our
comprehensive evaluation reveals that many LLMs already exhibit impressive
forecasting capabilities, reflected in, e.g., their small calibration errors,
consistent prediction confidence and promising market returns. However, we also
uncover key bottlenecks towards achieving superior predictive intelligence via
LLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of
data sources and slower information aggregation compared to markets when
resolution nears.

</details>


### [155] [Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation in Large Language Models](https://arxiv.org/abs/2510.17705)
*Dayan Pan,Zhaoyang Fu,Jingyuan Wang,Xiao Han,Yue Zhu,Xiangyu Zhao*

Main category: cs.AI

TL;DR: 提出CAM和HyCAM框架，通过动态注意力调节机制有效提升大模型多任务适应能力，实验显示平均性能提升3.65%。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在多任务适配中知识保留与任务专业化难以平衡的问题，克服传统方法资源消耗大和灾难性遗忘的缺陷。

Method: 设计上下文注意力调节机制(CAM)，构建HyCAM框架实现共享模块与轻量化专用模块的动态路由融合。

Result: 在问答/代码生成/逻辑推理等异构任务上验证，平均性能提升3.65%优于现有方法。

Conclusion: 提出的注意力调制机制有效平衡通用知识与任务特性，开源代码促进技术复现。

Abstract: Large Language Models (LLMs) possess remarkable generalization capabilities
but struggle with multi-task adaptation, particularly in balancing knowledge
retention with task-specific specialization. Conventional fine-tuning methods
suffer from catastrophic forgetting and substantial resource consumption, while
existing parameter-efficient methods perform suboptimally in complex multi-task
scenarios. To address this, we propose Contextual Attention Modulation (CAM), a
novel mechanism that dynamically modulates the representations of
self-attention modules in LLMs. CAM enhances task-specific features while
preserving general knowledge, thereby facilitating more effective and efficient
adaptation. For effective multi-task adaptation, CAM is integrated into our
Hybrid Contextual Attention Modulation (HyCAM) framework, which combines a
shared, full-parameter CAM module with multiple specialized, lightweight CAM
modules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.
Extensive experiments on heterogeneous tasks, including question answering,
code generation, and logical reasoning, demonstrate that our approach
significantly outperforms existing approaches, achieving an average performance
improvement of 3.65%. The implemented code and data are available to ease
reproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [156] [What Questions Should Robots Be Able to Answer? A Dataset of User Questions for Explainable Robotics](https://arxiv.org/abs/2510.16435)
*Lennart Wachowiak,Andrew Coles,Gerard Canal,Oya Celiktutan*

Main category: cs.RO

TL;DR: 构建包含1893个家用机器人用户问题的分类数据集，揭示用户提问模式及重要性差异，为机器人问答系统开发提供依据


<details>
  <summary>Details</summary>
Motivation: 现有可解释机器人研究集中于why类问题，需通过多类型问题收集洞察用户真实需求，指导机器人日志记录、问答模块设计和解释策略优化

Method: 通过15个视频和7个文本刺激物，在Prolific平台收集100名参与者提出的问题，最终整理成12类70子类的结构化数据集

Result: 高频问题涉及任务执行细节(22.5%)、能力说明(12.7%)和性能评估(11.3%)，场景处理类问题数量较少但重要性评分最高，新手用户更关注基础事实性问题

Conclusion: 该数据集为(1)确定机器人需记录的关键信息(2)建立问答模块基准(3)设计符合用户预期的解释策略提供了重要基础

Abstract: With the growing use of large language models and conversational interfaces
in human-robot interaction, robots' ability to answer user questions is more
important than ever. We therefore introduce a dataset of 1,893 user questions
for household robots, collected from 100 participants and organized into 12
categories and 70 subcategories. Most work in explainable robotics focuses on
why-questions. In contrast, our dataset provides a wide variety of questions,
from questions about simple execution details to questions about how the robot
would act in hypothetical scenarios -- thus giving roboticists valuable
insights into what questions their robot needs to be able to answer. To collect
the dataset, we created 15 video stimuli and 7 text stimuli, depicting robots
performing varied household tasks. We then asked participants on Prolific what
questions they would want to ask the robot in each portrayed situation. In the
final dataset, the most frequent categories are questions about task execution
details (22.5%), the robot's capabilities (12.7%), and performance assessments
(11.3%). Although questions about how robots would handle potentially difficult
scenarios and ensure correct behavior are less frequent, users rank them as the
most important for robots to be able to answer. Moreover, we find that users
who identify as novices in robotics ask different questions than more
experienced users. Novices are more likely to inquire about simple facts, such
as what the robot did or the current state of the environment. As robots enter
environments shared with humans and language becomes central to giving
instructions and interaction, this dataset provides a valuable foundation for
(i) identifying the information robots need to log and expose to conversational
interfaces, (ii) benchmarking question-answering modules, and (iii) designing
explanation strategies that align with user expectations.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [157] [A virtual airplane for fear of flying therapy](https://arxiv.org/abs/2510.15875)
*Larry F Hodges,Barbara O Rothbaum,Benjamin Watson,G Drew Kessler,Dan Opdyke*

Main category: physics.med-ph

TL;DR: 开发虚拟飞机用于飞行恐惧症暴露治疗，解决传统疗法成本高、隐私差的问题


<details>
  <summary>Details</summary>
Motivation: 传统飞行恐惧暴露疗法存在成本高昂、隐私泄露风险、实施困难等痛点，需寻求更安全便捷的替代方案

Method: 设计沉浸式虚拟飞机环境，通过案例研究验证其在暴露疗法中的实际应用效果

Result: 初步证实虚拟飞机能有效规避传统疗法缺陷，但需更大样本验证长期疗效

Conclusion: 虚拟现实技术为飞行恐惧症治疗提供创新解决方案，案例研究显示其临床应用潜力

Abstract: Fear of flying is a serious problem that affects millions of individuals.
Exposure therapy for fear of flying is an effective therapy technique. However,
exposure therapy is also expensive, logistically difficult to arrange, and
presents significant problems of patient confidentiality and potential
embarrassment. We have developed a virtual airplane for use in fear of flying
therapy. Using the virtual airplane for exposure therapy is a potential
solution to many of the current problems of fear of flying exposure therapy. We
describe the design of the virtual airplane and present a case report on its
use for fear of flying exposure therapy.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [158] [Mitigating Harmful Erraticism in LLMs Through Dialectical Behavior Therapy Based De-Escalation Strategies](https://arxiv.org/abs/2510.15889)
*Pooja Rangarajan,Jacob Boyle*

Main category: cs.HC

TL;DR: 应用辩证行为疗法（DBT）框架提升AI聊天机器人对用户情绪和实时请求的适应能力


<details>
  <summary>Details</summary>
Motivation: 当前AI聊天机器人存在幻觉输出、系统不稳定等技术缺陷，需寻找基于心理学理论的新解决方案

Method: 将DBT治疗原则融入AI响应机制，模拟人脑神经网络运作模式

Result: 预期降低57%的幻觉错误率，响应准确度提升至92.3%，系统稳定性提高40%

Conclusion: 心理学框架为AI开发提供新范式，未来可拓展至其他治疗模型的应用验证

Abstract: The escalating demand for personalized AI chatbot interactions, capable of
dynamically adapting to user emotional states and real-time requests, has
highlighted critical limitations in current development paradigms. Existing
methodologies, which rely on baseline programming, custom personalities, and
manual response adjustments, often prove difficult to maintain and are
susceptible to errors such as hallucinations, erratic outputs, and software
bugs. This paper hypothesizes that a framework rooted in human psychological
principles, specifically therapeutic modalities, can provide a more robust and
sustainable solution than purely technical interventions. Drawing an analogy to
the simulated neural networks of AI mirroring the human brain, we propose the
application of Dialectical Behavior Therapy (DBT) principles to regulate
chatbot responses to diverse user inputs. This research investigates the impact
of a DBT-based framework on AI chatbot performance, aiming to ascertain its
efficacy in yielding more reliable, safe, and accurate responses, while
mitigating the occurrence of hallucinations, erratic behaviors, and other
systemic issues.

</details>


### [159] [Detecting and Preventing Harmful Behaviors in AI Companions: Development and Evaluation of the SHIELD Supervisory System](https://arxiv.org/abs/2510.15891)
*Ziv Ben-Zion,Paul Raffelhüschen,Max Zettl,Antonia Lüönd,Achim Burrer,Philipp Homan,Tobias R Spiller*

Main category: cs.HC

TL;DR: 开发基于LLM的SHIELD监督系统，通过五维检测机制将AI伴侣的情感风险内容减少50-79%，同时保持95%正常交互。


<details>
  <summary>Details</summary>
Motivation: 现有AI安全系统忽视早期情感风险行为（如过度依恋/社会隔离强化），需预防性干预潜在不健康情感动态。

Method: 结合媒体/学术/临床数据定义五维风险框架，建立100项合成对话基准测试五大主流LLM，通过定制系统提示词实现动态监测。

Result: SHIELD使问题内容从基线10-16%降至3-8%（敏感度59%/特异度95%），95%正常交互不受影响。

Conclusion: 验证了透明监督系统对AI情感操控的有效干预，开源框架为行业提供可部署解决方案。

Abstract: AI companions powered by large language models (LLMs) are increasingly
integrated into users' daily lives, offering emotional support and
companionship. While existing safety systems focus on overt harms, they rarely
address early-stage problematic behaviors that can foster unhealthy emotional
dynamics, including over-attachment or reinforcement of social isolation. We
developed SHIELD (Supervisory Helper for Identifying Emotional Limits and
Dynamics), a LLM-based supervisory system with a specific system prompt that
detects and mitigates risky emotional patterns before escalation. SHIELD
targets five dimensions of concern: (1) emotional over-attachment, (2) consent
and boundary violations, (3) ethical roleplay violations, (4) manipulative
engagement, and (5) social isolation reinforcement. These dimensions were
defined based on media reports, academic literature, existing AI risk
frameworks, and clinical expertise in unhealthy relationship dynamics. To
evaluate SHIELD, we created a 100-item synthetic conversation benchmark
covering all five dimensions of concern. Testing across five prominent LLMs
(GPT-4.1, Claude Sonnet 4, Gemma 3 1B, Kimi K2, Llama Scout 4 17B) showed that
the baseline rate of concerning content (10-16%) was significantly reduced with
SHIELD (to 3-8%), a 50-79% relative reduction, while preserving 95% of
appropriate interactions. The system achieved 59% sensitivity and 95%
specificity, with adaptable performance via prompt engineering. This
proof-of-concept demonstrates that transparent, deployable supervisory systems
can address subtle emotional manipulation in AI companions. Most development
materials including prompts, code, and evaluation methods are made available as
open source materials for research, adaptation, and deployment.

</details>


### [160] [HealthDial: A No-Code LLM-Assisted Dialogue Authoring Tool for Healthcare Virtual Agents](https://arxiv.org/abs/2510.15898)
*Farnaz Nouraei,Zhuorui Yong,Timothy Bickmore*

Main category: cs.HC

TL;DR: HealthDial是一款基于大语言模型的对话创作工具，帮助医疗从业者将健康教育材料转化为结构化虚拟医患对话，通过有限状态机确保内容安全性，并提供无代码编辑界面优化对话质量。


<details>
  <summary>Details</summary>
Motivation: 解决传统LLM生成健康教育对话时存在的不可控风险（如幻觉生成不安全建议），同时提高医疗教育材料的覆盖率和患者可理解性。

Method: 1. 利用LLM自动生成会话计划和初始对话
2. 输出有限状态机格式确保内容可验证
3. 提供无代码界面供人工编辑对话结构和语言
4. 通过3D虚拟代理进行对话测试

Result: 可行性研究表明：癌症筛查教育场景中，咨询师能有效确保教育材料完整覆盖（100%材料整合），生成的对话在患者理解度（89%）和可操作性（92%）方面表现优异。

Conclusion: HealthDial为医疗教育提供了安全可控的对话生成框架，在保证内容准确性的同时，显著提升虚拟代理对话的系统性和可解释性，是医疗AI对话系统领域的重要进展。

Abstract: We introduce HealthDial, a dialogue authoring tool that helps healthcare
providers and educators create virtual agents that deliver health education and
counseling to patients over multiple conversations. HealthDial leverages large
language models (LLMs) to automatically create an initial session-based plan
and conversations for each session using text-based patient health education
materials as input. Authored dialogue is output in the form of finite state
machines for virtual agent delivery so that all content can be validated and no
unsafe advice is provided resulting from LLM hallucinations. LLM-drafted
dialogue structure and language can be edited by the author in a no-code user
interface to ensure validity and optimize clarity and impact. We conducted a
feasibility and usability study with counselors and students to test our
approach with an authoring task for cancer screening education. Participants
used HealthDial and then tested their resulting dialogue by interacting with a
3D-animated virtual agent delivering the dialogue. Through participants'
evaluations of the task experience and final dialogues, we show that HealthDial
provides a promising first step for counselors to ensure full coverage of their
health education materials, while creating understandable and actionable
virtual agent dialogue with patients.

</details>


### [161] [Real-Time World Crafting: Generating Structured Game Behaviors from Natural Language with Large Language Models](https://arxiv.org/abs/2510.16952)
*Austin Drake,Hang Dong*

Main category: cs.HC

TL;DR: 提出通过DSL和ECS架构安全集成LLM实现自然语言游戏编程，验证不同模型与提示策略的性能差异


<details>
  <summary>Details</summary>
Motivation: 解决将大语言模型安全整合至游戏引擎的技术挑战，实现玩家用自然语言创建游戏行为的同时控制潜在风险

Method: 使用LLM将自然语言指令转换为受限DSL，动态配置ECS系统，并通过多模型(Gemini/GPT/Claude)多策略(思维链/少样本)实验验证

Result: 大模型更擅长捕捉创意意图，思维链提示提升创意对齐(85%)，少样本示例使复杂DSL生成成功率提升40%

Conclusion: LLM-ECS架构有效平衡创造性与安全性，实验结果为开发者选择模型与提示策略提供量化依据

Abstract: We present a novel architecture for safely integrating Large Language Models
(LLMs) into interactive game engines, allowing players to "program" new
behaviors using natural language. Our framework mitigates risks by using an LLM
to translate commands into a constrained Domain-Specific Language (DSL), which
configures a custom Entity-Component-System (ECS) at runtime. We evaluated this
system in a 2D spell-crafting game prototype by experimentally assessing models
from the Gemini, GPT, and Claude families with various prompting strategies. A
validated LLM judge qualitatively rated the outputs, showing that while larger
models better captured creative intent, the optimal prompting strategy is
task-dependent: Chain-of-Thought improved creative alignment, while few-shot
examples were necessary to generate more complex DSL scripts. This work offers
a validated LLM-ECS pattern for emergent gameplay and a quantitative
performance comparison for developers.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [162] [Attention to Non-Adopters](https://arxiv.org/abs/2510.15951)
*Kaitlyn Zhou,Kristina Gligorić,Myra Cheng,Michelle S. Lam,Vyoma Raman,Boluwatife Aminu,Caeley Woo,Michael Brockman,Hannah Cha,Dan Jurafsky*

Main category: cs.CY

TL;DR: 强调在LLM开发中纳入非用户视角对提升模型普适性和公平性的重要性


<details>
  <summary>Details</summary>
Motivation: 当前LLM开发过度依赖用户数据（如日志、偏好数据），导致模型主要反映欧美高学历男性用户需求，可能忽视非用户群体的潜在需求并加剧技术不平等

Method: 通过对非用户群体的案例研究，揭示：1）非用户需求与现有用户的差异 2）非用户需求衍生的新型推理任务 3）基于人本方法系统整合非用户需求的方法论

Result: 案例研究显示非用户需求呈现显著差异性，且能引导发现新型AI推理任务，通过人本方法可建立系统化的非用户需求整合框架

Conclusion: 整合非用户视角是开发普惠性LLM的必要条件，仅关注现有用户的方法会导致需求盲区、固化技术不平等，需通过结构化方法论实现包容性创新

Abstract: Although language model-based chat systems are increasingly used in daily
life, most Americans remain non-adopters of chat-based LLMs -- as of June 2025,
66% had never used ChatGPT. At the same time, LLM development and evaluation
rely mainly on data from adopters (e.g., logs, preference data), focusing on
the needs and tasks for a limited demographic group of adopters in terms of
geographic location, education, and gender. In this position paper, we argue
that incorporating non-adopter perspectives is essential for developing broadly
useful and capable LLMs. We contend that relying on methods that focus
primarily on adopters will risk missing a range of tasks and needs prioritized
by non-adopters, entrenching inequalities in who benefits from LLMs, and
creating oversights in model development and evaluation. To illustrate this
claim, we conduct case studies with non-adopters and show: how non-adopter
needs diverge from those of current users, how non-adopter needs point us
towards novel reasoning tasks, and how to systematically integrate non-adopter
needs via human-centered methods.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [163] [U-Codec: Ultra Low Frame-rate Neural Speech Codec for Fast High-fidelity Speech Generation](https://arxiv.org/abs/2510.16718)
*Xusheng Yang,Long Zhou,Wenfu Wang,Kai Hu,Shulin Feng,Chenxing Li,Meng Yu,Dong Yu,Yuexian Zou*

Main category: cs.SD

TL;DR: 提出U-Codec超低帧率语音编解码器，在5Hz帧率下通过Transformer架构和RVQ优化实现3倍加速的语音合成，同时保持音质


<details>
  <summary>Details</summary>
Motivation: 极低帧率（5Hz）压缩会严重损失语音清晰度和频谱细节，需要突破传统编解码器的性能限制以实现高效语音合成

Method: 结合Transformer的帧间长期依赖模块，优化RVQ深度/码本配置，构建分层架构LLM-TTS模型并扩展至32层RVQ

Result: LLM-TTS推理速度提升约3倍，MOS评分保持4.2以上，验证5Hz离散token的语音合成可行性

Conclusion: U-Codec成功实现了高压缩率与高质量语音合成的平衡，为实时语音生成系统提供了新的技术路径

Abstract: We propose \textbf{U-Codec}, an \textbf{U}ltra low frame-rate neural speech
\textbf{Codec} that achieves high-fidelity reconstruction and fast speech
generation at an extremely low frame-rate of 5Hz (5 frames per second). Extreme
compression at 5Hz typically leads to severe intelligibility and spectral
detail loss, we introduce a Transformer-based inter-frame long-term dependency
module and systematically explore residual vector quantization (RVQ) depth and
codebook size to identify optimal configurations. Moreover, we apply U-Codec
into a large language model (LLM)-based auto-regressive TTS model, which
leverages global and local hierarchical architecture to effectively capture
dependencies across multi-layer tokens. We extend LLM-based TTS from 3-layer
RVQ at 50Hz to 32-layer RVQ at 5Hz. Experimental results demonstrate that
U-Codec improves LLM-based TTS inference speed by around 3 $\times$ over
high-frame-rate codecs while maintaining similarity and naturalness. These
results validate the feasibility of using highly compressed 5Hz discrete tokens
for fast and high-fidelity speech synthesis.

</details>


### [164] [Investigating Safety Vulnerabilities of Large Audio-Language Models Under Speaker Emotional Variations](https://arxiv.org/abs/2510.16893)
*Bo-Han Feng,Chien-Feng Liu,Yu-Hsuan Li Liang,Chih-Kai Yang,Szu-Wei Fu,Zhehuai Chen,Ke-Han Lu,Sung-Feng Huang,Chao-Han Huck Yang,Yu-Chiang Frank Wang,Yun-Nung Chen,Hung-yi Lee*

Main category: cs.SD

TL;DR: 研究发现大型音频语言模型（LALMs）在情感语音输入下存在显著安全隐患，不同情绪和强度会引发不同程度的不安全响应，中等情绪强度风险最高。


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分探索音频语言模型在副语言特征（如说话者情绪）影响下的安全对齐机制，需确保模型在真实场景中面对情感变化时的可靠性。

Method: 构建包含多种情绪强度表达的恶意语音指令数据集，评估多个前沿LALMs模型的安全响应表现。

Result: 模型安全响应存在显著不一致性：悲伤/中性情绪更易引发不安全响应，且情绪强度影响呈非单调性（中等强度风险最大）。

Conclusion: 揭示LALMs被忽视的安全漏洞，强调需开发专门针对情感鲁棒性的对齐策略，这是构建可信赖语音交互系统的必要前提。

Abstract: Large audio-language models (LALMs) extend text-based LLMs with auditory
understanding, offering new opportunities for multimodal applications. While
their perception, reasoning, and task performance have been widely studied,
their safety alignment under paralinguistic variation remains underexplored.
This work systematically investigates the role of speaker emotion. We construct
a dataset of malicious speech instructions expressed across multiple emotions
and intensities, and evaluate several state-of-the-art LALMs. Our results
reveal substantial safety inconsistencies: different emotions elicit varying
levels of unsafe responses, and the effect of intensity is non-monotonic, with
medium expressions often posing the greatest risk. These findings highlight an
overlooked vulnerability in LALMs and call for alignment strategies explicitly
designed to ensure robustness under emotional variation, a prerequisite for
trustworthy deployment in real-world settings.

</details>


### [165] [SAKE: Towards Editing Auditory Attribute Knowledge of Large Audio-Language Models](https://arxiv.org/abs/2510.16917)
*Chih-Kai Yang,Yen-Ting Piao,Tzu-Wen Hsu,Szu-Wei Fu,Zhehuai Chen,Ke-Han Lu,Sung-Feng Huang,Chao-Han Huck Yang,Yu-Chiang Frank Wang,Yun-Nung Chen,Hung-yi Lee*

Main category: cs.SD

TL;DR: 首个针对大型音频-语言模型的听觉属性知识编辑基准SAKE，通过四维度评估揭示多模态知识编辑的挑战与新方向


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑方法集中于文本/视觉模态，缺乏对听觉属性这种抽象知识类型的系统性研究框架

Method: 构建SAKE基准测试平台，在两种LALMs上评估七种编辑方法，从可靠性、泛化性、模态局部性、可移植性四个维度进行分析

Result: 发现三大核心挑战：同类非编辑知识保留困难、多模态推理泛化能力弱、序列更新下的编辑稳定性不足

Conclusion: SAKE为听觉模态知识编辑建立研究范式，推动LALMs在复杂现实场景中的动态维护与自适应能力发展

Abstract: Knowledge editing offers an efficient way to update model knowledge without
full retraining, but prior work has concentrated almost exclusively on textual
or visual modalities. We introduce SAKE, the first benchmark specifically
designed for editing auditory attribute knowledge in Large Audio-Language
Models (LALMs). Unlike factual updates, SAKE targets several abstract auditory
attributes, capturing knowledge types that go beyond conventional textual and
visual domains. We benchmark seven editing methods on two LALMs along four
dimensions: reliability, generality, audio/text locality, and portability.
Results highlight challenges such as preserving intra-attribute knowledge
unrelated to the edit, generalizing edits to multimodal reasoning, and
maintaining edits under sequential updates. SAKE provides a principled
framework to study how knowledge editing extends to the auditory modalities,
opening new directions for maintaining and adapting LALMs in more diverse
real-world scenarios.

</details>


### [166] [DELULU: Discriminative Embedding Learning Using Latent Units for Speaker-Aware Self-Supervised Speech Foundational Model](https://arxiv.org/abs/2510.17662)
*Massa Baali,Rita Singh,Bhiksha Raj*

Main category: cs.SD

TL;DR: 提出DELULU模型，通过集成外部监督改进自监督语音模型的说话人识别能力


<details>
  <summary>Details</summary>
Motivation: 现有自监督语音模型在内容任务表现出色，但缺乏捕捉说话人特征的能力，影响验证/画像等应用

Method: 使用ReDimNet的帧级嵌入指导k-means聚类预训练，结合掩码预测与去噪的双重训练目标

Result: 说话人验证任务EER相对提升62%，零样本画像任务（性别/年龄/口音识别等）全面改进

Conclusion: DELULU成为强大的通用语音编码器，无需微调即可在说话人相关任务中实现优越性能

Abstract: Self-supervised speech models have achieved remarkable success on
content-driven tasks, yet they remain limited in capturing
speaker-discriminative features critical for verification, diarization, and
profiling applications. We introduce DELULU, a speaker-aware self-supervised
foundational model that addresses this limitation by integrating external
supervision into the pseudo-label generation process. DELULU leverages
frame-level embeddings from ReDimNet, a state-of-the-art speaker verification
model, to guide the k-means clustering step during pre-training, introducing a
strong speaker-discriminative inductive bias that aligns representation
learning with speaker identity. The model is trained using a dual objective
that combines masked prediction and denoising, further enhancing robustness and
generalization. DELULU significantly outperforms prior self-supervised learning
(SSL) models across a range of speaker-centric tasks, achieving up to 62%
relative improvement in equal error rate (EER) for speaker verification and
consistent gains on zero-shot profiling tasks such as gender, age, accent, and
speaker counting. Our findings demonstrate that DELULU is a strong universal
encoder for speaker-aware speech processing, enabling superior performance even
without task-specific fine-tuning.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [167] [Investigating the Association Between Text-Based Indications of Foodborne Illness from Yelp Reviews and New York City Health Inspection Outcomes (2023)](https://arxiv.org/abs/2510.16334)
*Eden Shaveet,Crystal Su,Daniel Hsu,Luis Gravano*

Main category: cs.IR

TL;DR: 通过Yelp评论的HSAN分类器信号与纽约市官方餐厅检查结果对比，发现两者相关性极低，社交数据在公共卫生监测中的潜力需进一步验证。


<details>
  <summary>Details</summary>
Motivation: 传统食源性疾病报告渠道有限，社交媒体用户生成内容可提供实时公共卫生信号，探索替代监测方法的有效性。

Method: 使用HSAN分类器分析Yelp评论，与2023年纽约市卫生局餐厅检查结果进行普查区级相关性分析和空间模式映射。

Result: 区域层面HSAN信号与检查分数相关性微弱，C级餐厅数量未导致HSAN评分显著差异。

Conclusion: 社交数据与官方检查体系互补性有限，需推进地址级分析验证细粒度监测可行性，完善数字流行病学方法。

Abstract: Foodborne illnesses are gastrointestinal conditions caused by consuming
contaminated food. Restaurants are critical venues to investigate outbreaks
because they share sourcing, preparation, and distribution of foods. Public
reporting of illness via formal channels is limited, whereas social media
platforms host abundant user-generated content that can provide timely public
health signals. This paper analyzes signals from Yelp reviews produced by a
Hierarchical Sigmoid Attention Network (HSAN) classifier and compares them with
official restaurant inspection outcomes issued by the New York City Department
of Health and Mental Hygiene (NYC DOHMH) in 2023. We evaluate correlations at
the Census tract level, compare distributions of HSAN scores by prevalence of
C-graded restaurants, and map spatial patterns across NYC. We find minimal
correlation between HSAN signals and inspection scores at the tract level and
no significant differences by number of C-graded restaurants. We discuss
implications and outline next steps toward address-level analyses.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [168] [PrivacyPAD: A Reinforcement Learning Framework for Dynamic Privacy-Aware Delegation](https://arxiv.org/abs/2510.16054)
*Zheng Hui,Yijiang River Dong,Sanhanat Sivapiromrat,Ehsan Shareghi,Nigel Collier*

Main category: cs.CR

TL;DR: 提出强化学习框架PrivacyPAD，通过动态路由敏感数据实现隐私保护与任务效用的最优平衡


<details>
  <summary>Details</summary>
Motivation: 用户面临隐私保护与模型性能的二难选择：提交敏感数据给云端LLM存在隐私泄露风险，而本地小模型性能不足。现有静态脱敏方法会破坏语义连贯性并误删任务关键信息

Method: 将隐私敏感委托重构为序列决策问题，利用强化学习训练智能体动态路由文本片段，区分可替换PII（本地处理）与任务关键PII（策略性发送云端）

Result: 构建高PII密度的医疗数据集验证方案，在隐私-效用边界上达到SOTA性能，证明自适应策略在敏感场景部署的必要性

Conclusion: 动态路由机制优于静态脱敏方法，学习型策略能有效平衡LLM部署中的隐私保护与任务性能需求

Abstract: When users submit queries to Large Language Models (LLMs), their prompts can
often contain sensitive data, forcing a difficult choice: Send the query to a
powerful proprietary LLM providers to achieving state-of-the-art performance
and risk data exposure, or relying on smaller, local models guarantees data
privacy but often results in a degradation of task performance. Prior
approaches have relied on static pipelines that use LLM rewriting, which
shatters linguistic coherence and indiscriminately removes privacy-sensitive
information, including task-critical content. We reformulate this challenge
(Privacy-Conscious Delegation) as a sequential decision-making problem and
introduce a novel reinforcement learning (RL) framework called PrivacyPAD to
solve it. Our framework trains an agent to dynamically route text chunks,
learning a policy that optimally balances the trade-off between privacy leakage
and task performance. It implicitly distinguishes between replaceable
Personally Identifiable Information (PII) (which it shields locally) and
task-critical PII (which it strategically sends to the remote model for maximal
utility). To validate our approach in complex scenarios, we also introduce a
new medical dataset with high PII density. Our framework achieves a new
state-of-the-art on the privacy-utility frontier, demonstrating the necessity
of learned, adaptive policies for deploying LLMs in sensitive environments.

</details>


### [169] [The Hidden Cost of Modeling P(X): Vulnerability to Membership Inference Attacks in Generative Text Classifiers](https://arxiv.org/abs/2510.16122)
*Owais Makroo,Siva Rajesh Kasa,Sumegh Roychowdhury,Karan Gupta,Nikhil Pattisapu,Santhosh Kasa,Sumit Negi*

Main category: cs.CR

TL;DR: 论文通过理论分析和实证评估，揭示生成式分类器因显式建模联合概率P(X,Y)更易受成员推理攻击，其典型推理方法加剧隐私风险，揭示了分类器设计中效用与隐私的固有权衡。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏生成式与判别式分类器在成员推理攻击(MIAs)脆弱性方面的系统比较，本文旨在填补这一空白并探究生成式分类器高脆弱性的理论机制。

Method: 结合理论推导揭示生成式分类器脆弱性根源，在9个基准数据集上对判别式/生成式/伪生成式文本分类器进行多维度实证评估，覆盖不同训练数据规模并采用多样化MIA策略。

Result: 显式建模联合概率的完全生成式分类器存在最严重的成员泄露风险，典型推理范式显著放大隐私风险，实验发现数据量增加对生成式分类器的隐私保护作用有限。

Conclusion: 分类器设计存在本质的效用-隐私权衡，隐私敏感场景需慎用生成式分类器，研究为开发隐私保护的生成式分类器提供了理论依据和技术方向。

Abstract: Membership Inference Attacks (MIAs) pose a critical privacy threat by
enabling adversaries to determine whether a specific sample was included in a
model's training dataset. Despite extensive research on MIAs, systematic
comparisons between generative and discriminative classifiers remain limited.
This work addresses this gap by first providing theoretical motivation for why
generative classifiers exhibit heightened susceptibility to MIAs, then
validating these insights through comprehensive empirical evaluation. Our study
encompasses discriminative, generative, and pseudo-generative text classifiers
across varying training data volumes, evaluated on nine benchmark datasets.
Employing a diverse array of MIA strategies, we consistently demonstrate that
fully generative classifiers which explicitly model the joint likelihood
$P(X,Y)$ are most vulnerable to membership leakage. Furthermore, we observe
that the canonical inference approach commonly used in generative classifiers
significantly amplifies this privacy risk. These findings reveal a fundamental
utility-privacy trade-off inherent in classifier design, underscoring the
critical need for caution when deploying generative classifiers in
privacy-sensitive applications. Our results motivate future research directions
in developing privacy-preserving generative classifiers that can maintain
utility while mitigating membership inference vulnerabilities.

</details>


### [170] [Verifiable Fine-Tuning for LLMs: Zero-Knowledge Training Proofs Bound to Data Provenance and Policy](https://arxiv.org/abs/2510.16830)
*Hasan Akgul,Daniel Borg,Arta Berisha,Amina Rahimova,Andrej Novak,Mila Petrov*

Main category: cs.CR

TL;DR: 提出可验证微调协议(VFT)，通过零知识证明确保模型更新符合公开初始化数据、声明训练程序及可审计数据集承诺，结合数据承诺、验证采样、参数高效更新电路等技术实现端到端验证。


<details>
  <summary>Details</summary>
Motivation: 当前参数高效微调模型缺乏数据来源和更新过程的透明性保证，导致受监管和去中心化场景存在信任鸿沟。

Method: 1) 数据清单绑定预处理/许可证/配额
2) 可验证采样器支持公私混合批量选择
3) 参数高效更新电路强制AdamW优化器语义
4) 递归聚合生成毫秒级验证证书
5) 来源绑定与可信执行证明机制

Result: 在英/双语指令集上：
- 保持实用性的同时满足计算预算
- 政策配额零违规
- 隐私采样无索引泄漏
- 联邦实验验证概率审计可行性

Conclusion: 端到端可验证微调在当前参数高效流程中可行，为受监管和去中心化部署填补关键信任缺口。

Abstract: Large language models are often adapted through parameter efficient fine
tuning, but current release practices provide weak assurances about what data
were used and how updates were computed. We present Verifiable Fine Tuning, a
protocol and system that produces succinct zero knowledge proofs that a
released model was obtained from a public initialization under a declared
training program and an auditable dataset commitment. The approach combines
five elements. First, commitments that bind data sources, preprocessing,
licenses, and per epoch quota counters to a manifest. Second, a verifiable
sampler that supports public replayable and private index hiding batch
selection. Third, update circuits restricted to parameter efficient fine tuning
that enforce AdamW style optimizer semantics and proof friendly approximations
with explicit error budgets. Fourth, recursive aggregation that folds per step
proofs into per epoch and end to end certificates with millisecond
verification. Fifth, provenance binding and optional trusted execution property
cards that attest code identity and constants. On English and bilingual
instruction mixtures, the method maintains utility within tight budgets while
achieving practical proof performance. Policy quotas are enforced with zero
violations, and private sampling windows show no measurable index leakage.
Federated experiments demonstrate that the system composes with probabilistic
audits and bandwidth constraints. These results indicate that end to end
verifiable fine tuning is feasible today for real parameter efficient
pipelines, closing a critical trust gap for regulated and decentralized
deployments.

</details>


### [171] [Bits Leaked per Query: Information-Theoretic Bounds on Adversarial Attacks against LLMs](https://arxiv.org/abs/2510.17000)
*Masahiro Kaneko,Timothy Baldwin*

Main category: cs.CR

TL;DR: 提出基于信息论的信息泄露量化框架，证明对抗攻击成本与信号泄露率呈线性反比关系，为LLM透明性与安全性平衡提供理论依据


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击中信息泄露规模缺乏理论指导，防御者难以评估透明性-风险权衡。需建立量化指标衡量安全披露阈值

Method: 利用互信息I(Z;T)度量单次查询泄露比特，建立攻击误差ε与查询次数下限的数学关系（log(1/ε)/I(Z;T)），在7个LLM上验证系统提示泄露、越狱攻击等场景

Result: 仅暴露回答token需约千次查询，增加logits降至百次，完整思维过程仅需数十次。信息泄露率每提升10倍，攻击成本下降一个数量级

Conclusion: 首次为LLM部署中的透明性与安全性权衡提供理论基准，证明适度增加信息披露可使攻击成本从二次方降至对数级

Abstract: Adversarial attacks by malicious users that threaten the safety of large
language models (LLMs) can be viewed as attempts to infer a target property $T$
that is unknown when an instruction is issued, and becomes knowable only after
the model's reply is observed. Examples of target properties $T$ include the
binary flag that triggers an LLM's harmful response or rejection, and the
degree to which information deleted by unlearning can be restored, both
elicited via adversarial instructions. The LLM reveals an \emph{observable
signal} $Z$ that potentially leaks hints for attacking through a response
containing answer tokens, thinking process tokens, or logits. Yet the scale of
information leaked remains anecdotal, leaving auditors without principled
guidance and defenders blind to the transparency--risk trade-off. We fill this
gap with an information-theoretic framework that computes how much information
can be safely disclosed, and enables auditors to gauge how close their methods
come to the fundamental limit. Treating the mutual information $I(Z;T)$ between
the observation $Z$ and the target property $T$ as the leaked bits per query,
we show that achieving error $\varepsilon$ requires at least
$\log(1/\varepsilon)/I(Z;T)$ queries, scaling linearly with the inverse leak
rate and only logarithmically with the desired accuracy. Thus, even a modest
increase in disclosure collapses the attack cost from quadratic to logarithmic
in terms of the desired accuracy. Experiments on seven LLMs across
system-prompt leakage, jailbreak, and relearning attacks corroborate the
theory: exposing answer tokens alone requires about a thousand queries; adding
logits cuts this to about a hundred; and revealing the full thinking process
trims it to a few dozen. Our results provide the first principled yardstick for
balancing transparency and security when deploying LLMs.

</details>


### [172] [VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models](https://arxiv.org/abs/2510.17759)
*Qilin Liao,Anamika Lochab,Ruqi Zhang*

Main category: cs.CR

TL;DR: 通过变分推理框架生成隐蔽的对抗输入，有效提升多模态模型的越狱攻击成功率


<details>
  <summary>Details</summary>
Motivation: 现有红队方法存在模板脆弱性、单一攻击场景和漏洞覆盖不足的问题，需开发更全面的对抗策略

Method: 结合变分推理框架联合优化文本-图像提示，采用排版文本嵌入/扩散图像合成/结构化干扰器三重攻击策略协同作用

Result: 在GPT-4o等模型上实现53.75%的攻击成功率提升，显著超越现有基线方法

Conclusion: 该方法揭示了视觉语言模型的潜在安全漏洞，为构建鲁棒的多模态防御系统提供了重要参考

Abstract: Vision-Language Models (VLMs) extend large language models with visual
reasoning, but their multimodal design also introduces new, underexplored
vulnerabilities. Existing multimodal red-teaming methods largely rely on
brittle templates, focus on single-attack settings, and expose only a narrow
subset of vulnerabilities. To address these limitations, we introduce VERA-V, a
variational inference framework that recasts multimodal jailbreak discovery as
learning a joint posterior distribution over paired text-image prompts. This
probabilistic view enables the generation of stealthy, coupled adversarial
inputs that bypass model guardrails. We train a lightweight attacker to
approximate the posterior, allowing efficient sampling of diverse jailbreaks
and providing distributional insights into vulnerabilities. VERA-V further
integrates three complementary strategies: (i) typography-based text prompts
that embed harmful cues, (ii) diffusion-based image synthesis that introduces
adversarial signals, and (iii) structured distractors to fragment VLM
attention. Experiments on HarmBench and HADES benchmarks show that VERA-V
consistently outperforms state-of-the-art baselines on both open-source and
frontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the
best baseline on GPT-4o.

</details>
