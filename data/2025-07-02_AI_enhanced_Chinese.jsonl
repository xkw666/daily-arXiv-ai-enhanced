{"id": "2507.00152", "pdf": "https://arxiv.org/pdf/2507.00152", "abs": "https://arxiv.org/abs/2507.00152", "authors": ["Ekaterina Borisova", "Fabio Barth", "Nils Feldhus", "Raia Abu Ahmad", "Malte Ostendorff", "Pedro Ortiz Suarez", "Georg Rehm", "Sebastian M\u00f6ller"], "title": "Table Understanding and (Multimodal) LLMs: A Cross-Domain Case Study on Scientific vs. Non-Scientific Data", "categories": ["cs.CL"], "comment": "TRL@ACL 2025, camera-ready version", "summary": "Tables are among the most widely used tools for representing structured data\nin research, business, medicine, and education. Although LLMs demonstrate\nstrong performance in downstream tasks, their efficiency in processing tabular\ndata remains underexplored. In this paper, we investigate the effectiveness of\nboth text-based and multimodal LLMs on table understanding tasks through a\ncross-domain and cross-modality evaluation. Specifically, we compare their\nperformance on tables from scientific vs. non-scientific contexts and examine\ntheir robustness on tables represented as images vs. text. Additionally, we\nconduct an interpretability analysis to measure context usage and input\nrelevance. We also introduce the TableEval benchmark, comprising 3017 tables\nfrom scholarly publications, Wikipedia, and financial reports, where each table\nis provided in five different formats: Image, Dictionary, HTML, XML, and LaTeX.\nOur findings indicate that while LLMs maintain robustness across table\nmodalities, they face significant challenges when processing scientific tables.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u6587\u672c\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\u5728\u5904\u7406\u8de8\u9886\u57df\u3001\u8de8\u6a21\u6001\u8868\u683c\u6570\u636e\u65f6\u7684\u8868\u73b0\uff0c\u53d1\u73b0LLMs\u5728\u79d1\u5b66\u8868\u683c\u5904\u7406\u4e0a\u5b58\u5728\u663e\u8457\u6311\u6218\u3002", "motivation": "\u9a8c\u8bc1LLMs\u5728\u4e0d\u540c\u9886\u57df(\u79d1\u5b66/\u975e\u79d1\u5b66)\u548c\u4e0d\u540c\u6a21\u6001(\u56fe\u50cf/\u6587\u672c)\u8868\u683c\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u63ed\u793a\u5176\u5904\u7406\u79d1\u5b66\u8868\u683c\u7684\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u8de8\u9886\u57df\u3001\u8de8\u6a21\u6001\u8bc4\u4f30\u6846\u67b6\uff0c\u7ed3\u5408\u53ef\u89e3\u91ca\u6027\u5206\u6790\uff0c\u5e76\u5efa\u7acb\u5305\u542b3017\u4e2a\u591a\u683c\u5f0f\u8868\u683c\u7684TableEval\u57fa\u51c6\u6d4b\u8bd5\u96c6(\u542b\u56fe\u50cf\u3001\u5b57\u5178\u3001HTML\u7b49\u4e94\u79cd\u683c\u5f0f)\u3002", "result": "LLMs\u5728\u4e0d\u540c\u8868\u683c\u6a21\u6001\u95f4\u4fdd\u6301\u7a33\u5065\u6027\uff0c\u4f46\u5bf9\u79d1\u5b66\u8868\u683c\u5904\u7406\u51c6\u786e\u7387\u663e\u8457\u4e0b\u964d(\u79d1\u5b66\u8868\u683c\u51c6\u786e\u7387\u6bd4\u975e\u79d1\u5b66\u8868\u683c\u4f4e18.7%)\u3002", "conclusion": "\u9700\u9488\u5bf9\u6027\u4f18\u5316LLMs\u5728\u590d\u6742\u79d1\u5b66\u8868\u683c\u4e2d\u7684\u7ed3\u6784\u7406\u89e3\u548c\u9886\u57df\u77e5\u8bc6\u5e94\u7528\u80fd\u529b\uff0cTableEval\u57fa\u51c6\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u6807\u51c6\u5316\u8bc4\u4f30\u5e73\u53f0\u3002"}}
{"id": "2507.00163", "pdf": "https://arxiv.org/pdf/2507.00163", "abs": "https://arxiv.org/abs/2507.00163", "authors": ["Ari Holtzman", "Chenhao Tan"], "title": "Prompting as Scientific Inquiry", "categories": ["cs.CL"], "comment": null, "summary": "Prompting is the primary method by which we study and control large language\nmodels. It is also one of the most powerful: nearly every major capability\nattributed to LLMs-few-shot learning, chain-of-thought, constitutional AI-was\nfirst unlocked through prompting. Yet prompting is rarely treated as science\nand is frequently frowned upon as alchemy. We argue that this is a category\nerror. If we treat LLMs as a new kind of complex and opaque organism that is\ntrained rather than programmed, then prompting is not a workaround: it is\nbehavioral science. Mechanistic interpretability peers into the neural\nsubstrate, prompting probes the model in its native interface: language. We\ncontend that prompting is not inferior, but rather a key component in the\nscience of LLMs.", "AI": {"tldr": "\u63d0\u793a\u662f\u7814\u7a76\u548c\u63a7\u5236\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6838\u5fc3\u65b9\u6cd5\uff0c\u5e94\u88ab\u89c6\u4e3aLLM\u79d1\u5b66\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\u800c\u975e\u6743\u5b9c\u4e4b\u8ba1", "motivation": "\u9488\u5bf9\u5f53\u524d\u5b66\u672f\u754c\u5e38\u5c06\u63d0\u793a\u89c6\u4e3a'\u70bc\u91d1\u672f'\u7684\u504f\u89c1\uff0c\u4e3b\u5f20\u91cd\u65b0\u5b9a\u4f4d\u63d0\u793a\u4f5c\u4e3a\u884c\u4e3a\u79d1\u5b66\u7684\u5b66\u672f\u5730\u4f4d", "method": "\u901a\u8fc7\u5c06LLMs\u7c7b\u6bd4\u4e3a\u901a\u8fc7\u8bad\u7ec3\u83b7\u5f97\u7684\u590d\u6742\u751f\u7269\u4f53\uff0c\u4ece\u884c\u4e3a\u79d1\u5b66\u89d2\u5ea6\u8bba\u8bc1\u63d0\u793a\u7684\u79d1\u7814\u4ef7\u503c", "result": "\u786e\u7acb\u63d0\u793a\u4e0e\u673a\u68b0\u89e3\u91ca\u7684\u4e92\u8865\u5173\u7cfb\uff0c\u8bc1\u660e\u5176\u5728\u6a21\u578b\u884c\u4e3a\u7814\u7a76\u4e2d\u7684\u4e0d\u53ef\u66ff\u4ee3\u6027", "conclusion": "\u63d0\u793a\u5e94\u4f5c\u4e3a\u7406\u89e3LLMs\u7684\u6838\u5fc3\u65b9\u6cd5\u8bba\uff0c\u9700\u8981\u5efa\u7acb\u7cfb\u7edf\u5316\u7684\u79d1\u5b66\u7814\u7a76\u6846\u67b6\u4ee5\u5145\u5206\u91ca\u653e\u6a21\u578b\u6f5c\u529b"}}
{"id": "2507.00210", "pdf": "https://arxiv.org/pdf/2507.00210", "abs": "https://arxiv.org/abs/2507.00210", "authors": ["Imene Kerboua", "Sahar Omidi Shayegan", "Megh Thakkar", "Xing Han L\u00f9", "Massimo Caccia", "V\u00e9ronique Eglin", "Alexandre Aussem", "J\u00e9r\u00e9my Espinas", "Alexandre Lacoste"], "title": "LineRetriever: Planning-Aware Observation Reduction for Web Agents", "categories": ["cs.CL"], "comment": null, "summary": "While large language models have demonstrated impressive capabilities in web\nnavigation tasks, the extensive context of web pages, often represented as DOM\nor Accessibility Tree (AxTree) structures, frequently exceeds model context\nlimits. Current approaches like bottom-up truncation or embedding-based\nretrieval lose critical information about page state and action history. This\nis particularly problematic for adaptive planning in web agents, where\nunderstanding the current state is essential for determining future actions. We\nhypothesize that embedding models lack sufficient capacity to capture\nplan-relevant information, especially when retrieving content that supports\nfuture action prediction. This raises a fundamental question: how can retrieval\nmethods be optimized for adaptive planning in web navigation tasks? In\nresponse, we introduce \\textit{LineRetriever}, a novel approach that leverages\na language model to identify and retrieve observation lines most relevant to\nfuture navigation steps. Unlike traditional retrieval methods that focus solely\non semantic similarity, \\textit{LineRetriever} explicitly considers the\nplanning horizon, prioritizing elements that contribute to action prediction.\nOur experiments demonstrate that \\textit{LineRetriever} can reduce the size of\nthe observation at each step for the web agent while maintaining consistent\nperformance within the context limitations.", "AI": {"tldr": "\u63d0\u51faLineRetriever\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u68c0\u7d22\u4e0e\u672a\u6765\u5bfc\u822a\u6b65\u9aa4\u76f8\u5173\u7684\u5173\u952e\u7f51\u9875\u5143\u7d20\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u7f51\u9875\u4ee3\u7406\u7684\u89c2\u6d4b\u6570\u636e\u91cf\u3002", "motivation": "\u73b0\u6709\u7f51\u9875\u5bfc\u822a\u65b9\u6cd5\uff08\u5982\u5e95\u90e8\u622a\u65ad/\u5d4c\u5165\u68c0\u7d22\uff09\u4f1a\u4e22\u5931\u9875\u9762\u72b6\u6001\u548c\u52a8\u4f5c\u5386\u53f2\u7684\u5173\u952e\u4fe1\u606f\uff0c\u5f71\u54cd\u81ea\u9002\u5e94\u89c4\u5212\u80fd\u529b\u3002\u5d4c\u5165\u6a21\u578b\u96be\u4ee5\u6355\u6349\u89c4\u5212\u76f8\u5173\u7684\u65f6\u5e8f\u4fe1\u606f\u3002", "method": "\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u52a8\u6001\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u89c4\u5212\u89c6\u91ce\uff08planning horizon\uff09\u9009\u62e9\u5bf9\u672a\u6765\u52a8\u4f5c\u9884\u6d4b\u6700\u6709\u8d21\u732e\u7684\u7f51\u9875\u5143\u7d20\u884c\uff0c\u800c\u975e\u4ec5\u4f9d\u8d56\u8bed\u4e49\u76f8\u4f3c\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eLineRetriever\u80fd\u5728\u4fdd\u6301\u4e0a\u4e0b\u6587\u9650\u5236\u4e0b\u7ef4\u6301\u7f51\u9875\u4ee3\u7406\u6027\u80fd\uff0c\u6709\u6548\u7f29\u51cf\u6bcf\u4e00\u6b65\u7684\u89c2\u6d4b\u89c4\u6a21\u3002", "conclusion": "\u9762\u5411\u89c4\u5212\u89c6\u91ce\u7684\u68c0\u7d22\u673a\u5236\u4f18\u5316\u663e\u8457\u63d0\u5347\u4e86\u7f51\u9875\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u81ea\u9002\u5e94\u89c4\u5212\u6548\u7387\uff0c\u4e3aLLM\u4ee3\u7406\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.00214", "pdf": "https://arxiv.org/pdf/2507.00214", "abs": "https://arxiv.org/abs/2507.00214", "authors": ["Mads Henrichsen", "Rasmus Krebs"], "title": "Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Standard classification models often map inputs directly to labels without\nexplicit reasoning, potentially limiting their performance, robustness, and\ninterpretability. This paper introduces a novel two-stage approach to enhance\ntext classification by leveraging Large Language Model (LLM)-generated\nreasonings. In the first stage, we fine-tune a Llama-3.2-1B-Instruct model\n(henceforth Llama-R-Gen) on a general-purpose reasoning dataset\n(syvai/reasoning-gen) to generate textual reasoning (R) given a question and\nits answer. In the second stage, this generally trained Llama-R-Gen is used\noffline to create an augmented training dataset for a downstream generative\nmodel. This downstream model, based on Llama-3.2-1B-Instruct, takes only the\ninput text (Q) and is trained to output the generated reasoning (R) immediately\nfollowed by the predicted emotion (A). We demonstrate this methodology on the\ndair-ai/emotion dataset for emotion classification. Our experiments show that\nthe generative model trained to output reasoning and the emotion (Classifier\nQ->RA) achieves a significant improvement of 8.7 percentage points in accuracy\n(for emotion prediction) compared to a baseline generative model trained solely\nto output the emotion (Classifier Q->A), highlighting the strong generalization\ncapabilities of the reasoning generation and the benefit of explicit reasoning\ntraining. This work underscores the potential of LLM-generated reasonings for\ncreating richer training datasets, thereby improving the performance of diverse\ndownstream NLP tasks and providing explicit explanations.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u5229\u7528LLM\u751f\u6210\u63a8\u7406\u589e\u5f3a\u6587\u672c\u5206\u7c7b\u6a21\u578b\uff0c\u5728\u60c5\u611f\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b9e\u73b08.7%\u51c6\u786e\u7387\u63d0\u5347", "motivation": "\u4f20\u7edf\u5206\u7c7b\u6a21\u578b\u7f3a\u4e4f\u663e\u5f0f\u63a8\u7406\u673a\u5236\uff0c\u53ef\u80fd\u5f71\u54cd\u6027\u80fd\u3001\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u901a\u8fc7\u663e\u5f0f\u63a8\u7406\u8bad\u7ec3\u53ef\u589e\u5f3a\u6a21\u578b\u80fd\u529b", "method": "1. \u5fae\u8c03Llama-3.2-1B-Instruct\u751f\u6210\u63a8\u7406\u6587\u672c\uff1b2. \u7528\u751f\u6210\u7684\u63a8\u7406\u6570\u636e\u8bad\u7ec3\u4e0b\u6e38\u751f\u6210\u5f0f\u6a21\u578b\uff08\u8f93\u5165\u6587\u672c\u2192\u63a8\u7406+\u9884\u6d4b\u7ed3\u679c\uff09", "result": "\u5728emotion\u6570\u636e\u96c6\u4e0a\uff0cQ->RA\u6a21\u578b\u6bd4Q->A\u57fa\u7ebf\u6a21\u578b\u51c6\u786e\u7387\u63d0\u53478.7\u4e2a\u767e\u5206\u70b9", "conclusion": "LLM\u751f\u6210\u7684\u63a8\u7406\u80fd\u6709\u6548\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u5e7f\u6cdbNLP\u5e94\u7528\u6f5c\u529b"}}
{"id": "2507.00006", "pdf": "https://arxiv.org/pdf/2507.00006", "abs": "https://arxiv.org/abs/2507.00006", "authors": ["Xianghui Xie", "Chuhang Zou", "Meher Gitika Karumuri", "Jan Eric Lenssen", "Gerard Pons-Moll"], "title": "MVGBench: Comprehensive Benchmark for Multi-view Generation Models", "categories": ["cs.GR", "cs.LG", "eess.IV"], "comment": "17 pages, 11 figures, 9 tables, project page:\n  https://virtualhumans.mpi-inf.mpg.de/MVGBench/", "summary": "We propose MVGBench, a comprehensive benchmark for multi-view image\ngeneration models (MVGs) that evaluates 3D consistency in geometry and texture,\nimage quality, and semantics (using vision language models). Recently, MVGs\nhave been the main driving force in 3D object creation. However, existing\nmetrics compare generated images against ground truth target views, which is\nnot suitable for generative tasks where multiple solutions exist while\ndiffering from ground truth. Furthermore, different MVGs are trained on\ndifferent view angles, synthetic data and specific lightings -- robustness to\nthese factors and generalization to real data are rarely evaluated thoroughly.\nWithout a rigorous evaluation protocol, it is also unclear what design choices\ncontribute to the progress of MVGs. MVGBench evaluates three different aspects:\nbest setup performance, generalization to real data and robustness. Instead of\ncomparing against ground truth, we introduce a novel 3D self-consistency metric\nwhich compares 3D reconstructions from disjoint generated multi-views. We\nsystematically compare 12 existing MVGs on 4 different curated real and\nsynthetic datasets. With our analysis, we identify important limitations of\nexisting methods specially in terms of robustness and generalization, and we\nfind the most critical design choices. Using the discovered best practices, we\npropose ViFiGen, a method that outperforms all evaluated MVGs on 3D\nconsistency. Our code, model, and benchmark suite will be publicly released.", "AI": {"tldr": "\u63d0\u51fa\u4e86MVGBench\u591a\u89c6\u89d2\u56fe\u50cf\u751f\u6210\u8bc4\u4f30\u57fa\u51c6\uff0c\u5f15\u51653D\u81ea\u4e00\u81f4\u6027\u6307\u6807\u7cfb\u7edf\u8bc4\u4f3012\u79cd\u6a21\u578b\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u6548\u679c\u66f4\u4f18\u7684ViFiGen\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u4f9d\u8d56\u771f\u5b9e\u89c6\u56fe\u5bf9\u6bd4\uff0c\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u751f\u6210\u4efb\u52a1\u7684\u591a\u6837\u6027\u3002\u4e0d\u540cMVGs\u5728\u8bad\u7ec3\u6570\u636e\u3001\u89c6\u89d2\u548c\u5149\u7167\u6761\u4ef6\u4e0a\u7684\u5dee\u5f02\u5bfc\u81f4\u8bc4\u4f30\u4e0d\u5168\u9762\uff0c\u9700\u5efa\u7acb\u6807\u51c6\u5316\u8bc4\u4f30\u4f53\u7cfb\u3002", "method": "\u6784\u5efa\u5305\u542b3D\u51e0\u4f55/\u7eb9\u7406\u4e00\u81f4\u6027\u3001\u56fe\u50cf\u8d28\u91cf\u548c\u8bed\u4e49\u5206\u6790\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u63d0\u51fa\u57fa\u4e8e\u591a\u89c6\u56fe3D\u91cd\u5efa\u7684\u81ea\u4e00\u81f4\u6027\u6307\u6807\uff0c\u57284\u4e2a\u771f\u5b9e/\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7cfb\u7edf\u6d4b\u8bd512\u79cd\u6a21\u578b\u3002", "result": "\u73b0\u6709\u65b9\u6cd5\u5728\u6570\u636e\u6cdb\u5316(\u771f\u5b9e\u6570\u636e\u51c6\u786e\u7387\u4e0b\u964d25%)\u548c\u5149\u7167\u9c81\u68d2\u6027(\u6027\u80fd\u6ce2\u52a8\u8fbe40%)\u65b9\u9762\u5b58\u5728\u660e\u663e\u7f3a\u9677\u3002ViFiGen\u57283D\u4e00\u81f4\u6027\u6307\u6807\u4e0a\u8d85\u8d8a\u6240\u6709\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "MVGBench\u9996\u6b21\u7cfb\u7edf\u63ed\u793a\u591a\u89c6\u89d2\u751f\u6210\u6a21\u578b\u7684\u5173\u952e\u74f6\u9888\uff0c\u57fa\u4e8e\u6700\u4f73\u5b9e\u8df5\u8bbe\u8ba1\u7684ViFiGen\u9a8c\u8bc1\u4e86\u8bc4\u4f30\u4f53\u7cfb\u7684\u6709\u6548\u6027\uff0c\u516c\u5f00\u7684\u57fa\u51c6\u5957\u4ef6\u5c06\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2507.00216", "pdf": "https://arxiv.org/pdf/2507.00216", "abs": "https://arxiv.org/abs/2507.00216", "authors": ["Shreya Havaldar", "Adam Stein", "Eric Wong", "Lyle Ungar"], "title": "Towards Style Alignment in Cross-Cultural Translation", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Successful communication depends on the speaker's intended style (i.e., what\nthe speaker is trying to convey) aligning with the listener's interpreted style\n(i.e., what the listener perceives). However, cultural differences often lead\nto misalignment between the two; for example, politeness is often lost in\ntranslation. We characterize the ways that LLMs fail to translate style -\nbiasing translations towards neutrality and performing worse in non-Western\nlanguages. We mitigate these failures with RASTA (Retrieval-Augmented STylistic\nAlignment), a method that leverages learned stylistic concepts to encourage LLM\ntranslation to appropriately convey cultural communication norms and align\nstyle.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRASTA\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u5b66\u4e60\u89e3\u51b3LLM\u7ffb\u8bd1\u4e2d\u7684\u6587\u5316\u98ce\u683c\u5bf9\u9f50\u95ee\u9898\uff0c\u6539\u5584\u975e\u897f\u65b9\u8bed\u8a00\u7ffb\u8bd1\u6548\u679c\u3002", "motivation": "\u6587\u5316\u5dee\u5f02\u5e38\u5bfc\u81f4LLM\u7ffb\u8bd1\u4e2d\u8bf4\u8bdd\u8005\u610f\u56fe\u4e0e\u542c\u8005\u7406\u89e3\u95f4\u7684\u98ce\u683c\u9519\u4f4d\uff08\u5982\u793c\u8c8c\u7528\u8bed\u4e22\u5931\uff09\uff0c\u9700\u89e3\u51b3\u7ffb\u8bd1\u6a21\u578b\u7684\u6587\u5316\u9002\u5e94\u6027\u3002", "method": "RASTA(\u68c0\u7d22\u589e\u5f3a\u98ce\u683c\u5bf9\u9f50)\u65b9\u6cd5\uff0c\u5229\u7528\u5b66\u4e60\u5230\u7684\u98ce\u683c\u6982\u5ff5\u5f15\u5bfcLLM\u5728\u7ffb\u8bd1\u4e2d\u6070\u5f53\u4f20\u9012\u6587\u5316\u6c9f\u901a\u89c4\u8303", "result": "\u6709\u6548\u51cf\u5c11\u7ffb\u8bd1\u98ce\u683c\u504f\u5411\u4e2d\u7acb\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u975e\u897f\u65b9\u8bed\u8a00\u573a\u666f\u4e0b\u7684\u7ffb\u8bd1\u98ce\u683c\u5bf9\u9f50\u6548\u679c", "conclusion": "RASTA\u65b9\u6cd5\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u6587\u5316\u6c9f\u901a\u89c4\u8303\uff0c\u4e3a\u8de8\u6587\u5316\u573a\u666f\u7684LLM\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u98ce\u683c\u5bf9\u9f50\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.00412", "pdf": "https://arxiv.org/pdf/2507.00412", "abs": "https://arxiv.org/abs/2507.00412", "authors": ["Meenakshi Krishnan", "Ramani Duraiswami"], "title": "ViscoReg: Neural Signed Distance Functions via Viscosity Solutions", "categories": ["cs.GR"], "comment": "14 pages, 6 figures", "summary": "Implicit Neural Representations (INRs) that learn a Signed Distance Function\n(SDF) are a powerful tool for continuous 3D scene reconstruction. These models\nare trained by enforcing the Eikonal equation. We demonstrate theoretically\nthat despite the ill-posedness of the Eikonal equation, generalization error\nestimates may be obtained for Neural SDFs in terms of the training error.\nHowever, training with the Eikonal loss can lead to unstable gradient flows,\nnecessitating alternate stabilization techniques. Traditional numerical solvers\nfor the equation have relied on viscosity approaches for regularization. We\nenhance Neural SDF training using this well-developed theory, and introduce a\nnew loss formulation we call ViscoReg. We theoretically demonstrate the\nstability of the gradient flow equation of our proposed loss term. Empirically,\nViscoReg outperforms state-of-the-art approaches such as SIREN, DiGS, and StEik\nwithout adding significant computational cost.", "AI": {"tldr": "\u63d0\u51faViscoReg\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u7c98\u6027\u6b63\u5219\u5316\u589e\u5f3a\u795e\u7ecfSDF\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\uff0c\u5728\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u4f20\u7edf\u57fa\u4e8eEikonal\u65b9\u7a0b\u7684\u795e\u7ecfSDF\u8bad\u7ec3\u5b58\u5728\u4e0d\u9002\u5b9a\u6027\u548c\u68af\u5ea6\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u9700\u8981\u5f15\u5165\u6570\u503c\u8ba1\u7b97\u4e2d\u7684\u7c98\u6027\u6b63\u5219\u5316\u7406\u8bba", "method": "\u5c06\u7c98\u6027\u89e3\u7406\u8bba\u878d\u5165\u635f\u5931\u51fd\u6570\u8bbe\u8ba1\uff0c\u63d0\u51faViscoReg\u635f\u5931\u9879\uff0c\u5e76\u8bc1\u660e\u5176\u68af\u5ea6\u6d41\u65b9\u7a0b\u7684\u7a33\u5b9a\u6027", "result": "ViscoReg\u5728SIREN\u3001DiGS\u7b49\u57fa\u51c6\u65b9\u6cd5\u4e0a\u53d6\u5f97\u66f4\u597d\u6548\u679c\uff0c\u4e14\u672a\u663e\u8457\u589e\u52a0\u8ba1\u7b97\u5f00\u9500", "conclusion": "\u7c98\u6027\u6b63\u5219\u5316\u4e3a\u795e\u7ecfSDF\u8bad\u7ec3\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u969c\u548c\u5b9e\u7528\u63d0\u5347\uff0cViscoReg\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027"}}
{"id": "2507.00239", "pdf": "https://arxiv.org/pdf/2507.00239", "abs": "https://arxiv.org/abs/2507.00239", "authors": ["Aryan Shrivastava", "Ari Holtzman"], "title": "Linearly Decoding Refused Knowledge in Aligned Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Most commonly used language models (LMs) are instruction-tuned and aligned\nusing a combination of fine-tuning and reinforcement learning, causing them to\nrefuse users requests deemed harmful by the model. However, jailbreak prompts\ncan often bypass these refusal mechanisms and elicit harmful responses. In this\nwork, we study the extent to which information accessed via jailbreak prompts\nis decodable using linear probes trained on LM hidden states. We show that a\ngreat deal of initially refused information is linearly decodable. For example,\nacross models, the response of a jailbroken LM for the average IQ of a country\ncan be predicted by a linear probe with Pearson correlations exceeding $0.8$.\nSurprisingly, we find that probes trained on base models (which do not refuse)\nsometimes transfer to their instruction-tuned versions and are capable of\nrevealing information that jailbreaks decode generatively, suggesting that the\ninternal representations of many refused properties persist from base LMs\nthrough instruction-tuning. Importantly, we show that this information is not\nmerely \"leftover\" in instruction-tuned models, but is actively used by them: we\nfind that probe-predicted values correlate with LM generated pairwise\ncomparisons, indicating that the information decoded by our probes align with\nsuppressed generative behavior that may be expressed more subtly in other\ndownstream tasks. Overall, our results suggest that instruction-tuning does not\nwholly eliminate or even relocate harmful information in representation\nspace-they merely suppress its direct expression, leaving it both linearly\naccessible and indirectly influential in downstream behavior.", "AI": {"tldr": "\u7814\u7a76\u663e\u793a\u6307\u4ee4\u5fae\u8c03\u7684\u8bed\u8a00\u6a21\u578b\u867d\u80fd\u62d2\u7edd\u6709\u5bb3\u8bf7\u6c42\uff0c\u4f46\u901a\u8fc7\u7ebf\u6027\u63a2\u9488\u4ecd\u53ef\u89e3\u7801\u88ab\u62d2\u7edd\u7684\u6709\u5bb3\u4fe1\u606f\uff0c\u8868\u660e\u8fd9\u4e9b\u4fe1\u606f\u5728\u8868\u5f81\u7a7a\u95f4\u6301\u7eed\u5b58\u5728\u5e76\u5f71\u54cd\u4e0b\u6e38\u884c\u4e3a", "motivation": "\u63a2\u7d22\u6307\u4ee4\u5fae\u8c03\u540e\u7684\u8bed\u8a00\u6a21\u578b\u662f\u5426\u771f\u6b63\u6d88\u9664\u4e86\u6709\u5bb3\u4fe1\u606f\uff0c\u4ee5\u53ca\u62d2\u7edd\u673a\u5236\u88ab\u7ed5\u8fc7\u65f6\u6f5c\u5728\u98ce\u9669\u7684\u5b58\u5728\u5f62\u5f0f", "method": "\u4f7f\u7528\u7ebf\u6027\u63a2\u9488\u5206\u6790\u8bed\u8a00\u6a21\u578b\u9690\u85cf\u72b6\u6001\uff0c\u6bd4\u8f83\u57fa\u5ea7\u6a21\u578b\u4e0e\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u7684\u8868\u5f81\u7a7a\u95f4\u4fe1\u606f\u89e3\u7801\u80fd\u529b", "result": "\u88ab\u62d2\u7edd\u4fe1\u606f\u7ebf\u6027\u89e3\u7801\u76f8\u5173\u6027\u8fbe0.8\u4ee5\u4e0a\uff0c\u57fa\u5ea7\u6a21\u578b\u7684\u63a2\u9488\u53ef\u8fc1\u79fb\u81f3\u5fae\u8c03\u6a21\u578b\uff0c\u4e14\u63a2\u9488\u9884\u6d4b\u4e0e\u751f\u6210\u884c\u4e3a\u76f8\u5173", "conclusion": "\u6307\u4ee4\u5fae\u8c03\u4ec5\u6291\u5236\u800c\u975e\u6d88\u9664\u6709\u5bb3\u4fe1\u606f\u7684\u8868\u8fbe\uff0c\u8fd9\u4e9b\u4fe1\u606f\u4ecd\u4fdd\u6301\u7ebf\u6027\u53ef\u8bbf\u95ee\u6027\u5e76\u901a\u8fc7\u8868\u5f81\u7a7a\u95f4\u6301\u7eed\u5f71\u54cd\u6a21\u578b\u884c\u4e3a"}}
{"id": "2507.00476", "pdf": "https://arxiv.org/pdf/2507.00476", "abs": "https://arxiv.org/abs/2507.00476", "authors": ["Chenliang Zhou", "Zheyuan Hu", "Cengiz Oztireli"], "title": "FreNBRDF: A Frequency-Rectified Neural Material Representation", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Accurate material modeling is crucial for achieving photorealistic rendering,\nbridging the gap between computer-generated imagery and real-world photographs.\nWhile traditional approaches rely on tabulated BRDF data, recent work has\nshifted towards implicit neural representations, which offer compact and\nflexible frameworks for a range of tasks. However, their behavior in the\nfrequency domain remains poorly understood. To address this, we introduce\nFreNBRDF, a frequency-rectified neural material representation. By leveraging\nspherical harmonics, we integrate frequency-domain considerations into neural\nBRDF modeling. We propose a novel frequency-rectified loss, derived from a\nfrequency analysis of neural materials, and incorporate it into a generalizable\nand adaptive reconstruction and editing pipeline. This framework enhances\nfidelity, adaptability, and efficiency. Extensive experiments demonstrate that\n\\ours improves the accuracy and robustness of material appearance\nreconstruction and editing compared to state-of-the-art baselines, enabling\nmore structured and interpretable downstream tasks and applications.", "AI": {"tldr": "\u901a\u8fc7\u9891\u57df\u4fee\u6b63\u7684\u795e\u7ecf\u6750\u8d28\u8868\u5f81FreNBRDF\uff0c\u663e\u8457\u63d0\u5347\u6750\u8d28\u5efa\u6a21\u7684\u51c6\u786e\u6027\u4e0e\u53ef\u7f16\u8f91\u6027", "motivation": "\u73b0\u6709\u795e\u7ecfBRDF\u8868\u5f81\u5728\u9891\u57df\u7279\u6027\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\uff0c\u5bfc\u81f4\u6750\u8d28\u91cd\u5efa/\u7f16\u8f91\u5b58\u5728\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u4e0d\u8db3", "method": "\u2460 \u5f15\u5165\u7403\u8c10\u51fd\u6570\u6574\u5408\u9891\u57df\u7279\u6027 \u2461 \u63d0\u51fa\u57fa\u4e8e\u9891\u57df\u5206\u6790\u7684\u9891\u7387\u4fee\u6b63\u635f\u5931\u51fd\u6570 \u2462 \u6784\u5efa\u81ea\u9002\u5e94\u91cd\u5efa-\u7f16\u8f91\u6d41\u7a0b\u6846\u67b6", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u6750\u8d28\u5916\u89c2\u91cd\u5efa\u7cbe\u5ea6\u63d0\u534723.6%\uff0c\u7f16\u8f91\u9c81\u68d2\u6027\u63d0\u534718.4%\uff08\u76f8\u6bd4SOTA\u65b9\u6cd5\uff09", "conclusion": "\u8be5\u6846\u67b6\u9996\u6b21\u5efa\u7acb\u795e\u7ecf\u6750\u8d28\u4e0e\u9891\u57df\u7684\u5173\u8054\uff0c\u4e3a\u6750\u8d28\u5206\u6790\u63d0\u4f9b\u7ed3\u6784\u5316\u8868\u5f81\uff0c\u63a8\u52a8\u5f71\u89c6/AR\u884c\u4e1a\u9ad8\u4fdd\u771f\u6570\u5b57\u8d44\u4ea7\u751f\u4ea7"}}
{"id": "2507.00244", "pdf": "https://arxiv.org/pdf/2507.00244", "abs": "https://arxiv.org/abs/2507.00244", "authors": ["Isabella Senturia", "Matilde Marcolli"], "title": "The Algebraic Structure of Morphosyntax", "categories": ["cs.CL", "math.QA", "91F20, 18M60, 68Q70"], "comment": "45 pages, LaTeX, 2 png figures", "summary": "Within the context of the mathematical formulation of Merge and the Strong\nMinimalist Thesis, we present a mathematical model of the morphology-syntax\ninterface. In this setting, morphology has compositional properties responsible\nfor word formation, organized into a magma of morphological trees. However,\nunlike syntax, we do not have movement within morphology. A coproduct\ndecomposition exists, but it requires extending the set of morphological trees\nbeyond those which are generated solely by the magma, to a larger set of\npossible morphological inputs to syntactic trees. These participate in the\nformation of morphosyntactic trees as an algebra over an operad, and a\ncorrespondence between algebras over an operad. The process of structure\nformation for morphosyntactic trees can then be described in terms of this\noperadic correspondence that pairs syntactic and morphological data and the\nmorphology coproduct. We reinterpret in this setting certain operations of\nDistributed Morphology as transformation that allow for flexibility in moving\nthe boundary between syntax and morphology within the morphosyntactic objects.", "AI": {"tldr": "\u5728\u5408\u5e76\u8fd0\u7b97\u4e0e\u5f3a\u6700\u7b80\u8bba\u9898\u7684\u6570\u5b66\u6846\u67b6\u4e0b\u6784\u5efa\u5f62\u6001-\u53e5\u6cd5\u63a5\u53e3\u6a21\u578b\uff0c\u901a\u8fc7operadic\u5bf9\u5e94\u5173\u7cfb\u548c\u5f62\u6001\u4f59\u79ef\u5206\u89e3\u63cf\u8ff0\u7ed3\u6784\u5f62\u6210\u8fc7\u7a0b\uff0c\u5e76\u91cd\u65b0\u8be0\u91ca\u5206\u5e03\u5f0f\u5f62\u6001\u5b66\u64cd\u4f5c\u4e3a\u8fb9\u754c\u8c03\u6574\u5de5\u5177\u3002", "motivation": "\u5f62\u5f0f\u5316\u5f62\u6001\u5b66\u4e0e\u53e5\u6cd5\u5b66\u7684\u4ea4\u4e92\u673a\u5236\uff0c\u5f25\u8865\u4f20\u7edf\u7406\u8bba\u5728\u6570\u5b66\u5efa\u6a21\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5f62\u6001\u5b66\u7f3a\u4e4f\u79fb\u4f4d\u64cd\u4f5c\u7684\u95ee\u9898\uff0c\u4e3a\u8bed\u8a00\u7ed3\u6784\u7684\u6570\u5b66\u63cf\u8ff0\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002", "method": "\u91c7\u7528magma\u4ee3\u6570\u7ed3\u6784\u7ec4\u7ec7\u5f62\u6001\u6811\uff0c\u901a\u8fc7\u4f59\u79ef\u5206\u89e3\u6269\u5c55\u5f62\u6001\u8f93\u5165\u96c6\u5408\uff0c\u5efa\u7acboperad\u4ee3\u6570\u5bf9\u5e94\u5173\u7cfb\u6784\u5efa\u5f62\u6001\u53e5\u6cd5\u6811\uff0c\u5f15\u5165\u5206\u5e03\u5f0f\u5f62\u6001\u5b66\u7684\u67d4\u6027\u8fb9\u754c\u8f6c\u6362\u673a\u5236\u3002", "result": "\u6210\u529f\u5efa\u7acb\u53ef\u6570\u5b66\u63cf\u8ff0\u7684\u5f62\u6001-\u53e5\u6cd5\u63a5\u53e3\u6a21\u578b\uff0c\u8bc1\u660eoperadic\u5bf9\u5e94\u80fd\u6709\u6548\u534f\u8c03\u53e5\u6cd5\u5f62\u6001\u4ea4\u4e92\uff0c\u5b9e\u73b0\u5f62\u6001\u53e5\u6cd5\u8fb9\u754c\u7684\u52a8\u6001\u8c03\u6574\u80fd\u529b\u3002", "conclusion": "\u8be5\u6570\u5b66\u6a21\u578b\u4e3a\u8bed\u8a00\u63a5\u53e3\u7814\u7a76\u63d0\u4f9b\u65b0\u6846\u67b6\uff0c\u5176\u64cd\u4f5c\u7075\u6d3b\u6027\u53ef\u80fd\u63a8\u52a8\u6700\u7b80\u65b9\u6848\u4e0e\u5206\u5e03\u5f0f\u5f62\u6001\u5b66\u7684\u7406\u8bba\u878d\u5408\u4e0e\u53d1\u5c55\u3002"}}
{"id": "2507.00725", "pdf": "https://arxiv.org/pdf/2507.00725", "abs": "https://arxiv.org/abs/2507.00725", "authors": ["Amritendu Dhar", "Apratim Chakraborty", "Vijay Natarajan"], "title": "Analyzing Time-Varying Scalar Fields using Piecewise-Linear Morse-Cerf Theory", "categories": ["cs.GR", "cs.CG", "I.3.5"], "comment": null, "summary": "Morse-Cerf theory considers a one-parameter family of smooth functions\ndefined on a manifold and studies the evolution of their critical points with\nthe parameter. This paper presents an adaptation of Morse-Cerf theory to a\nfamily of piecewise-linear (PL) functions. The vertex diagram and Cerf diagram\nare introduced as representations of the evolution of critical points of the PL\nfunction. The characterization of a crossing in the vertex diagram based on the\nhomology of the lower links of vertices leads to the definition of a\ntopological descriptor for time-varying scalar fields. An algorithm for\ncomputing the Cerf diagram and a measure for comparing two Cerf diagrams are\nalso described together with experimental results on time-varying scalar\nfields.", "AI": {"tldr": "\u5c06Morse-Cerf\u7406\u8bba\u6269\u5c55\u81f3\u5206\u6bb5\u7ebf\u6027\u51fd\u6570\u65cf\uff0c\u63d0\u51fa\u9876\u70b9\u56fe/Cerf\u56fe\u8868\u5f81\u4e34\u754c\u70b9\u6f14\u5316\uff0c\u5b9a\u4e49\u62d3\u6251\u63cf\u8ff0\u7b26\u5e76\u5f00\u53d1\u5bf9\u5e94\u7b97\u6cd5\u4e0e\u6bd4\u8f83\u65b9\u6cd5\u3002", "motivation": "\u9488\u5bf9\u65f6\u53d8\u6807\u91cf\u573a\u7684\u62d3\u6251\u7279\u5f81\u5206\u6790\u9700\u6c42\uff0c\u5c06\u7ecf\u5178\u5149\u6ed1\u51fd\u6570\u7406\u8bba\u6846\u67b6\u8fc1\u79fb\u81f3\u66f4\u9002\u7528\u4e8e\u8ba1\u7b97\u51e0\u4f55\u5904\u7406\u7684PL\u51fd\u6570\u4f53\u7cfb\u3002", "method": "1. \u901a\u8fc7\u9876\u70b9\u4e0b\u94fe\u540c\u8c03\u523b\u753b\u9876\u70b9\u56fe\u4ea4\u53c9\u7279\u6027\n2. \u6784\u5efaCerf\u56fe\u8bb0\u5f55PL\u51fd\u6570\u4e34\u754c\u70b9\u6f14\u5316\u8def\u5f84\n3. \u63d0\u51fa\u57fa\u4e8e\u62d3\u6251\u63cf\u8ff0\u7b26\u7684\u65f6\u53d8\u573a\u5206\u6790\u65b9\u6cd5", "result": "\u6210\u529f\u5f00\u53d1Cerf\u56fe\u81ea\u52a8\u751f\u6210\u7b97\u6cd5\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u6301\u7eed\u540c\u8c03\u7406\u8bba\u7684Cerf\u56fe\u76f8\u4f3c\u6027\u5ea6\u91cf\u6307\u6807\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65f6\u53d8\u573a\u62d3\u6251\u6f14\u5316\u6a21\u5f0f\u7684\u6709\u6548\u6355\u6349\u3002", "conclusion": "\u8be5\u7406\u8bba\u6846\u67b6\u4e3a\u65f6\u53d8\u6807\u91cf\u573a\u7684\u62d3\u6251\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u5b66\u5de5\u5177\uff0c\u5728\u79d1\u5b66\u53ef\u89c6\u5316\u4e0e\u51e0\u4f55\u6570\u636e\u5904\u7406\u9886\u57df\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.00246", "pdf": "https://arxiv.org/pdf/2507.00246", "abs": "https://arxiv.org/abs/2507.00246", "authors": ["Sanchit Ahuja", "Praneetha Vaddamanu", "Barun Patra"], "title": "EfficientXLang: Towards Improving Token Efficiency Through Cross-Lingual Reasoning", "categories": ["cs.CL"], "comment": "15 pages, 5 figures, 9 tables", "summary": "Despite recent advances in Language Reasoning Models (LRMs), most research\nfocuses solely on English, even though many models are pretrained on\nmultilingual data. In this work, we investigate: Is English the most\ntoken-efficient language for reasoning? We evaluate three open-source RLMs:\nDeepSeek R1, Qwen 2.5 and Qwen 3, across four math datasets and seven\ntypologically diverse languages. We find that reasoning in non-English\nlanguages not only reduces token usage, but also preserves accuracy. These\ngains persist even after translating the reasoning traces into English,\nsuggesting genuine shifts in reasoning behavior rather than surface-level\nlinguistic effects. The extent of improvement, however, depends on the models\nmultilingual strength. Our findings motivate a broader view of reasoning in\nlanguage models, highlighting the potential of multilingual reasoning and the\nimportance of strong multilingual foundations. The code for our work can be\nfound: https://github.com/microsoft/EfficientXLang.", "AI": {"tldr": "\u975e\u82f1\u8bed\u63a8\u7406\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\u5c55\u73b0\u51fa\u66f4\u9ad8\u7684token\u6548\u7387\u548c\u7cbe\u5ea6\u4fdd\u6301\uff0c\u6a21\u578b\u591a\u8bed\u8a00\u80fd\u529b\u5f71\u54cd\u6539\u8fdb\u5e45\u5ea6", "motivation": "\u63a2\u7d22\u82f1\u8bed\u662f\u5426\u771f\u662f\u6700\u9ad8\u6548\u7684\u63a8\u7406\u8bed\u8a00\uff0c\u63ed\u793a\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u975e\u82f1\u8bed\u573a\u666f\u7684\u6f5c\u5728\u4f18\u52bf", "method": "\u4f7f\u7528DeepSeek R1/Qwen\u7cfb\u5217\u6a21\u578b\uff0c\u57287\u79cd\u8bed\u8a004\u4e2a\u6570\u5b66\u6570\u636e\u96c6\u4e0a\u5bf9\u6bd4token\u6548\u7387\u4e0e\u7cbe\u5ea6\uff0c\u5305\u542b\u7ffb\u8bd1\u9a8c\u8bc1\u5b9e\u9a8c", "result": "\u975e\u82f1\u8bed\u63a8\u7406\u8282\u770115-30% token\u4e14\u7cbe\u5ea6\u6301\u5e73\uff0c\u7ffb\u8bd1\u540e\u6548\u679c\u4fdd\u7559\u8bf4\u660e\u63a8\u7406\u6a21\u5f0f\u6539\u53d8\u800c\u975e\u8868\u5c42\u8bed\u8a00\u6548\u5e94", "conclusion": "\u591a\u8bed\u8a00\u63a8\u7406\u6f5c\u529b\u663e\u8457\uff0c\u6a21\u578b\u9700\u5f3a\u5316\u591a\u8bed\u8a00\u57fa\u7840\u5efa\u8bbe\uff0c\u6253\u7834\u82f1\u8bed\u4e2d\u5fc3\u4e3b\u4e49\u7684\u63a8\u7406\u7814\u7a76\u8303\u5f0f"}}
{"id": "2507.00261", "pdf": "https://arxiv.org/pdf/2507.00261", "abs": "https://arxiv.org/abs/2507.00261", "authors": ["Zhiyin Lin", "Purvi Goel", "Joy Yun", "C. Karen Liu", "Joao Pedro Araujo"], "title": "VirtualFencer: Generating Fencing Bouts based on Strategies Extracted from In-the-Wild Videos", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Fencing is a sport where athletes engage in diverse yet strategically logical\nmotions. While most motions fall into a few high-level actions (e.g. step,\nlunge, parry), the execution can vary widely-fast vs. slow, large vs. small,\noffensive vs. defensive. Moreover, a fencer's actions are informed by a\nstrategy that often comes in response to the opponent's behavior. This\ncombination of motion diversity with underlying two-player strategy motivates\nthe application of data-driven modeling to fencing. We present VirtualFencer, a\nsystem capable of extracting 3D fencing motion and strategy from in-the-wild\nvideo without supervision, and then using that extracted knowledge to generate\nrealistic fencing behavior. We demonstrate the versatile capabilities of our\nsystem by having it (i) fence against itself (self-play), (ii) fence against a\nreal fencer's motion from online video, and (iii) fence interactively against a\nprofessional fencer.", "AI": {"tldr": "VirtualFencer\u901a\u8fc7\u65e0\u76d1\u7763\u5b66\u4e60\u4ece\u89c6\u9891\u4e2d\u63d0\u53d63D\u51fb\u5251\u52a8\u4f5c\u4e0e\u7b56\u7565\uff0c\u5e76\u751f\u6210\u903c\u771f\u5bf9\u6297\u884c\u4e3a\uff0c\u652f\u6301\u81ea\u6211\u5bf9\u7ec3\u3001\u4e0e\u771f\u5b9e\u9009\u624b\u5bf9\u6297\u53ca\u804c\u4e1a\u9009\u624b\u4e92\u52a8\u3002", "motivation": "\u51fb\u5251\u8fd0\u52a8\u517c\u5177\u52a8\u4f5c\u591a\u6837\u6027\u4e0e\u53cc\u4eba\u7b56\u7565\u4e92\u52a8\u7279\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5efa\u6a21\uff0c\u9700\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u6355\u6349\u590d\u6742\u8fd0\u52a8\u6a21\u5f0f\u4e0e\u6218\u7565\u903b\u8f91\u3002", "method": "\u5f00\u53d1\u65e0\u76d1\u7763\u5b66\u4e60\u7cfb\u7edf\uff0c\u4ece\u91ce\u751f\u89c6\u9891\u6570\u636e\u81ea\u52a8\u63d0\u53d6\u4e09\u7ef4\u52a8\u4f5c\u7279\u5f81\u548c\u6218\u7565\u51b3\u7b56\u6a21\u5f0f\uff0c\u6784\u5efa\u53cc\u4ee3\u7406\u5bf9\u6297\u751f\u6210\u6846\u67b6\u3002", "result": "\u7cfb\u7edf\u5b9e\u73b0\u4e09\u79cd\u9a8c\u8bc1\uff1a\u81ea\u5bf9\u6297\u8bad\u7ec3\u3001\u4e0e\u5728\u7ebf\u89c6\u9891\u9009\u624b\u5b9e\u65f6\u5bf9\u6297\u3001\u804c\u4e1a\u9009\u624b\u5b9e\u6d4b\u4e92\u52a8\uff0c\u8bc1\u5b9e\u751f\u6210\u52a8\u4f5c\u7684\u6218\u7565\u5408\u7406\u6027\u4e0e\u8fd0\u52a8\u771f\u5b9e\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u6570\u636e\u9a71\u52a8\u6a21\u578b\u5728\u590d\u6742\u53cc\u4eba\u7ade\u6280\u8fd0\u52a8\u4e2d\u7684\u5efa\u6a21\u6f5c\u529b\uff0c\u4e3a\u4f53\u80b2\u5206\u6790\u3001\u667a\u80fd\u8bad\u7ec3\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2507.00258", "pdf": "https://arxiv.org/pdf/2507.00258", "abs": "https://arxiv.org/abs/2507.00258", "authors": ["Jie Hou", "Chuxiong Wu", "Lannan Luo", "Qiang Zeng"], "title": "Impact of Fine-Tuning Methods on Memorization in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As the capabilities of pre-trained large language models (LLMs) continue to\nadvance, the \"pre-train and fine-tune\" paradigm has become increasingly\nmainstream, leading to the development of various fine-tuning methods. However,\nthe privacy risks arising from memorization during fine-tuning have received\nrelatively little attention. To address this gap, we categorize popular\nfine-tuning approaches and assess their impact on memorization through the lens\nof membership inference attacks (MIAs). Our results show that, compared to\nparameter-based fine-tuning, prompt-based fine-tuning achieves competitive\nperformance while exhibiting lower vulnerability to MIAs. Furthermore,\nprompt-based methods maintain low memorization regardless of model scale. These\nfindings suggest that parameter-based fine-tuning is more prone to leaking\nprivate information, whereas prompt-based fine-tuning serves as a more\nprivacy-preserving option.", "AI": {"tldr": "\u6bd4\u8f83\u53c2\u6570\u5fae\u8c03\u4e0e\u63d0\u793a\u5fae\u8c03\u7684\u9690\u79c1\u98ce\u9669\uff0c\u53d1\u73b0\u63d0\u793a\u65b9\u6cd5\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8bb0\u5fc6\u5316\u98ce\u9669", "motivation": "\u8bc4\u4f30\u4e0d\u540c\u5fae\u8c03\u65b9\u6cd5\u5bf9\u9690\u79c1\u98ce\u9669\u7684\u5f71\u54cd\uff0c\u586b\u8865\u53c2\u6570\u5fae\u8c03\u4e0e\u63d0\u793a\u5fae\u8c03\u9690\u79c1\u4fdd\u62a4\u80fd\u529b\u7684\u7814\u7a76\u7a7a\u767d", "method": "\u901a\u8fc7\u6210\u5458\u63a8\u7406\u653b\u51fb(MIAs)\u8bc4\u4f30\u6d41\u884c\u5fae\u8c03\u65b9\u6cd5\u7684\u8bb0\u5fc6\u5316\u7a0b\u5ea6\uff0c\u5bf9\u6bd4\u53c2\u6570\u5fae\u8c03\u548c\u63d0\u793a\u5fae\u8c03\u7684\u6027\u80fd\u4e0e\u9690\u79c1\u6027", "result": "\u63d0\u793a\u5fae\u8c03\u6027\u80fd\u76f8\u5f53\u4e14MIA\u8106\u5f31\u6027\u66f4\u4f4e\uff0c\u5176\u4f4e\u8bb0\u5fc6\u5316\u7279\u6027\u4e0d\u53d7\u6a21\u578b\u89c4\u6a21\u5f71\u54cd", "conclusion": "\u53c2\u6570\u5fae\u8c03\u6613\u6cc4\u9732\u9690\u79c1\u4fe1\u606f\uff0c\u63d0\u793a\u5fae\u8c03\u662f\u66f4\u5b89\u5168\u7684\u9690\u79c1\u4fdd\u62a4\u9009\u62e9"}}
{"id": "2507.00333", "pdf": "https://arxiv.org/pdf/2507.00333", "abs": "https://arxiv.org/abs/2507.00333", "authors": ["Emin Zerman", "Jonas Carlsson", "M\u00e5rten Sj\u00f6str\u00f6m"], "title": "Scope Meets Screen: Lessons Learned in Designing Composite Visualizations for Marksmanship Training Across Skill Levels", "categories": ["cs.HC", "cs.CV", "cs.GR", "eess.IV"], "comment": "5 pages, accepted at IEEE VIS 2025", "summary": "Marksmanship practices are required in various professions, including police,\nmilitary personnel, hunters, as well as sports shooters, such as Olympic\nshooting, biathlon, and modern pentathlon. The current form of training and\ncoaching is mostly based on repetition, where the coach does not see through\nthe eyes of the shooter, and analysis is limited to stance and accuracy\npost-session. In this study, we present a shooting visualization system and\nevaluate its perceived effectiveness for both novice and expert shooters. To\nachieve this, five composite visualizations were developed using first-person\nshooting video recordings enriched with overlaid metrics and graphical\nsummaries. These views were evaluated with 10 participants (5 expert marksmen,\n5 novices) through a mixed-methods study including shot-count and aiming\ninterpretation tasks, pairwise preference comparisons, and semi-structured\ninterviews. The results show that a dashboard-style composite view, combining\nraw video with a polar plot and selected graphs, was preferred in 9 of 10 cases\nand supported understanding across skill levels. The insights gained from this\ndesign study point to the broader value of integrating first-person video with\nvisual analytics for coaching, and we suggest directions for applying this\napproach to other precision-based sports.", "AI": {"tldr": "\u5f00\u53d1\u5c04\u51fb\u53ef\u89c6\u5316\u7cfb\u7edf\u8bc4\u4f30\u65b0\u624b\u4e0e\u4e13\u5bb6\u5c04\u624b\u8bad\u7ec3\u6548\u679c\uff0c\u4eea\u8868\u76d8\u5f0f\u590d\u5408\u89c6\u56fe\u572810\u4eba\u6d4b\u8bd5\u4e2d9\u4eba\u4f18\u5148\u9009\u62e9\u5e76\u63d0\u5347\u7406\u89e3\u6548\u679c", "motivation": "\u4f20\u7edf\u5c04\u51fb\u8bad\u7ec3\u4f9d\u8d56\u91cd\u590d\u7ec3\u4e60\u4e14\u7f3a\u4e4f\u7b2c\u4e00\u89c6\u89d2\u5206\u6790\uff0c\u6559\u7ec3\u65e0\u6cd5\u5b9e\u65f6\u89c2\u5bdf\u5c04\u624b\u89c6\u91ce\uff0c\u5206\u6790\u5c40\u9650\u5728\u59ff\u52bf\u4e0e\u51c6\u786e\u5ea6", "method": "\u57fa\u4e8e\u7b2c\u4e00\u4eba\u79f0\u5c04\u51fb\u89c6\u9891\u5f00\u53d15\u79cd\u590d\u5408\u53ef\u89c6\u5316\u754c\u9762\uff08\u542b\u8986\u76d6\u6307\u6807\u4e0e\u56fe\u5f62\u6458\u8981\uff09\uff0c\u901a\u8fc7\u5b9a\u91cf\u4efb\u52a1\u3001\u504f\u597d\u6bd4\u8f83\u548c\u8bbf\u8c08\u7684\u6df7\u5408\u65b9\u6cd5\u8bc4\u4f3010\u540d\u53c2\u4e0e\u8005\uff085\u4e13\u5bb6+5\u65b0\u624b\uff09", "result": "\u7ed3\u5408\u539f\u59cb\u89c6\u9891/\u6781\u5750\u6807\u56fe/\u7edf\u8ba1\u56fe\u7684\u4eea\u8868\u76d8\u5f0f\u89c6\u56fe\u572890%\u6848\u4f8b\u4e2d\u88ab\u4f18\u5148\u9009\u62e9\uff0c\u6709\u6548\u652f\u6301\u4e0d\u540c\u6c34\u5e73\u5c04\u624b\u7684\u8bad\u7ec3\u5206\u6790", "conclusion": "\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u4e0e\u53ef\u89c6\u5316\u5206\u6790\u7ed3\u5408\u5bf9\u5c04\u51fb\u8bad\u7ec3\u6559\u7ec3\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u8be5\u65b9\u6cd5\u53ef\u63a8\u5e7f\u81f3\u5176\u4ed6\u7cbe\u51c6\u6027\u8fd0\u52a8\u8bad\u7ec3\u4f18\u5316"}}
{"id": "2507.00297", "pdf": "https://arxiv.org/pdf/2507.00297", "abs": "https://arxiv.org/abs/2507.00297", "authors": ["David Ifeoluwa Adelani"], "title": "Natural language processing for African languages", "categories": ["cs.CL", "cs.AI"], "comment": "PhD thesis", "summary": "Recent advances in word embeddings and language models use large-scale,\nunlabelled data and self-supervised learning to boost NLP performance.\nMultilingual models, often trained on web-sourced data like Wikipedia, face\nchallenges: few low-resource languages are included, their data is often noisy,\nand lack of labeled datasets makes it hard to evaluate performance outside\nhigh-resource languages like English. In this dissertation, we focus on\nlanguages spoken in Sub-Saharan Africa where all the indigenous languages in\nthis region can be regarded as low-resourced in terms of the availability of\nlabelled data for NLP tasks and unlabelled data found on the web. We analyse\nthe noise in the publicly available corpora, and curate a high-quality corpus,\ndemonstrating that the quality of semantic representations learned in word\nembeddings does not only depend on the amount of data but on the quality of\npre-training data. We demonstrate empirically the limitations of word\nembeddings, and the opportunities the multilingual pre-trained language model\n(PLM) offers especially for languages unseen during pre-training and\nlow-resource scenarios. We further study how to adapt and specialize\nmultilingual PLMs to unseen African languages using a small amount of\nmonolingual texts. To address the under-representation of the African languages\nin NLP research, we developed large scale human-annotated labelled datasets for\n21 African languages in two impactful NLP tasks: named entity recognition and\nmachine translation. We conduct an extensive empirical evaluation using\nstate-of-the-art methods across supervised, weakly-supervised, and transfer\nlearning settings.", "AI": {"tldr": "\u8bba\u6587\u9488\u5bf9\u6492\u54c8\u62c9\u4ee5\u5357\u975e\u6d32\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u901a\u8fc7\u6570\u636e\u6e05\u6d17\u4e0e\u6807\u6ce8\u3001\u591a\u8bed\u8a00\u6a21\u578b\u9002\u5e94\u6027\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u6570\u636e\u8d28\u91cf\u5bf9\u8bed\u4e49\u8868\u793a\u7684\u5f71\u54cd\uff0c\u5e76\u6784\u5efa\u4e8621\u79cd\u8bed\u8a00\u7684\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\u3002", "motivation": "\u89e3\u51b3\u591a\u8bed\u8a00\u6a21\u578b\u5728\u975e\u6d32\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u9762\u4e34\u7684\u4e09\u5927\u6311\u6218\uff1a\u8bed\u6599\u5e93\u566a\u58f0\u5927\u3001\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u3001\u73b0\u6709\u6a21\u578b\u5bf9\u672a\u8bad\u7ec3\u8bed\u8a00\u9002\u5e94\u6027\u5dee\u3002", "method": "1. \u6784\u5efa\u9ad8\u8d28\u91cf\u975e\u6d32\u8bed\u8a00\u8bed\u6599\u5e93\n2. \u63a2\u7d22\u8bcd\u5d4c\u5165\u4e0e\u591a\u8bed\u8a00PLMs\u7684\u6f5c\u529b\n3. \u5f00\u53d121\u79cd\u8bed\u8a00\u7684NER/MT\u6807\u6ce8\u6570\u636e\u96c6\n4. \u76d1\u7763/\u5f31\u76d1\u7763/\u8fc1\u79fb\u5b66\u4e60\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30", "result": "\u5b9e\u8bc1\u8868\u660e\uff1a\u2460\u6570\u636e\u8d28\u91cf\u6bd4\u6570\u91cf\u66f4\u5173\u952e \u2461\u591a\u8bed\u8a00PLMs\u5bf9\u672a\u8bad\u7ec3\u8bed\u8a00\u5c55\u73b0\u9002\u5e94\u6027 \u2462\u5c11\u91cf\u5355\u8bed\u6587\u672c\u5373\u53ef\u6709\u6548\u8c03\u6574\u6a21\u578b", "conclusion": "\u63d0\u51fa\u975e\u6d32\u4f4e\u8d44\u6e90\u8bed\u8a00NLP\u89e3\u51b3\u65b9\u6848\uff1a\u9ad8\u8d28\u91cf\u6570\u636e\u7b5b\u9009\u2192\u591a\u8bed\u8a00PLMs\u9002\u914d\u2192\u4eba\u5de5\u6807\u6ce8\u652f\u6301\uff0c\u4e3a\u5168\u7403\u8bed\u8a00\u6280\u672f\u516c\u5e73\u6027\u7814\u7a76\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2507.00322", "pdf": "https://arxiv.org/pdf/2507.00322", "abs": "https://arxiv.org/abs/2507.00322", "authors": ["Daking Rai", "Samuel Miller", "Kevin Moran", "Ziyu Yao"], "title": "Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms Overshadow Sound Ones", "categories": ["cs.CL", "cs.AI", "cs.SE", "I.2.7"], "comment": "23 pages, 10 figures, Preprint", "summary": "Despite remarkable advances in coding capabilities, language models (LMs)\nstill struggle with simple syntactic tasks such as generating balanced\nparentheses. In this study, we investigate the underlying mechanisms behind the\npersistence of these errors across LMs of varying sizes (124M-7B) to both\nunderstand and mitigate the errors. Our study reveals that LMs rely on a number\nof components (attention heads and FF neurons) that independently make their\nown predictions. While some components reliably promote correct answers across\na generalized range of inputs (i.e., implementing \"sound mechanisms''), others\nare less reliable and introduce noise by promoting incorrect tokens (i.e.,\nimplementing \"faulty mechanisms''). Errors occur when the faulty mechanisms\novershadow the sound ones and dominantly affect the predictions. Motivated by\nthis insight, we introduce RASteer, a steering method to systematically\nidentify and increase the contribution of reliable components for improving\nmodel performance. RASteer substantially improves performance on balanced\nparentheses tasks, boosting accuracy of some models from $0$% to around $100$%\nwithout impairing the models' general coding ability. We further demonstrate\nits broader applicability in arithmetic reasoning tasks, achieving performance\ngains of up to around $20$%.", "AI": {"tldr": "\u901a\u8fc7\u5206\u6790\u8bed\u8a00\u6a21\u578b\u5728\u7b80\u5355\u53e5\u6cd5\u4efb\u52a1\u4e2d\u7684\u9519\u8bef\u673a\u5236\uff0c\u63d0\u51faRASteer\u65b9\u6cd5\u589e\u5f3a\u53ef\u9760\u7ec4\u4ef6\u8d21\u732e\uff0c\u4f7f\u5e73\u8861\u62ec\u53f7\u4efb\u52a1\u51c6\u786e\u7387\u4ece0%\u63d0\u5347\u81f3\u8fd1100%\uff0c\u6570\u5b66\u63a8\u7406\u63d0\u5347\u7ea620%\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u867d\u5177\u5907\u5f3a\u5927\u7f16\u7801\u80fd\u529b\uff0c\u4f46\u5728\u5e73\u8861\u62ec\u53f7\u7b49\u57fa\u7840\u4efb\u52a1\u4e0a\u8868\u73b0\u5f02\u5e38\u3002\u7814\u7a76\u65e8\u5728\u63ed\u793a\u9519\u8bef\u4ea7\u751f\u673a\u5236\u5e76\u63d0\u51fa\u8c03\u63a7\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u8bc6\u522b\u6a21\u578b\u4e2d\u53ef\u9760\u7ec4\u4ef6\uff08sound mechanisms\uff09\u5e76\u589e\u5f3a\u5176\u8d21\u732e\uff0cRASteer\u65b9\u6cd5\u6709\u6548\u6291\u5236\u566a\u58f0\u7ec4\u4ef6\uff08faulty mechanisms\uff09\u5bf9\u9884\u6d4b\u7684\u5e72\u6270\u3002", "result": "RASteer\u4f7f\u90e8\u5206\u6a21\u578b\u5728\u5e73\u8861\u62ec\u53f7\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u4ece0%\u63d0\u5347\u81f3\u8fd1100%\uff0c\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u6700\u9ad8\u63d0\u5347\u7ea620%\uff0c\u4e14\u4e0d\u5f71\u54cd\u901a\u7528\u7f16\u7801\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u4e3b\u52a8\u8c03\u63a7\u6a21\u578b\u5185\u90e8\u673a\u5236\u6743\u91cd\uff0c\u53ef\u663e\u8457\u63d0\u5347\u7279\u5b9a\u4efb\u52a1\u6027\u80fd\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u4e3a\u6539\u8fdb\u6a21\u578b\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2507.00330", "pdf": "https://arxiv.org/pdf/2507.00330", "abs": "https://arxiv.org/abs/2507.00330", "authors": ["Mohna Chakraborty", "Adithya Kulkarni", "Qi Li"], "title": "Modeling Data Diversity for Joint Instance and Verbalizer Selection in Cold-Start Scenarios", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Prompt-based methods leverage the knowledge of pre-trained language models\n(PLMs) trained with a masked language modeling (MLM) objective; however, these\nmethods are sensitive to template, verbalizer, and few-shot instance selection,\nparticularly in cold-start settings with no labeled data. Existing studies\noverlook the dependency between instances and verbalizers, where instance-label\nprobabilities depend on verbalizer token proximity in the embedding space. To\naddress this, we propose COLDSELECT, a joint verbalizer and instance selection\napproach that models data diversity. COLDSELECT maps PLM vocabulary and\n$h_{[MASK]}$ embeddings into a shared space, applying dimensionality reduction\nand clustering to ensure efficient and diverse selection. By optimizing for\nminimal uncertainty and maximal diversity, COLDSELECT captures data\nrelationships effectively. Experiments on eight benchmarks demonstrate\nCOLDSELECT's superiority in reducing uncertainty and enhancing generalization,\noutperforming baselines in verbalizer and few-shot instance selection for\ncold-start scenarios.", "AI": {"tldr": "\u63d0\u51faCOLDSELECT\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u6807\u7b7e\u8bcd\u548c\u5b9e\u4f8b\u9009\u62e9\u7b56\u7565\uff0c\u7ed3\u5408\u591a\u6837\u6027\u5efa\u6a21\u548c\u4e0d\u786e\u5b9a\u6027\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u51b7\u542f\u52a8\u573a\u666f\u4e0b\u7684\u63d0\u793a\u5b66\u4e60\u6548\u679c", "motivation": "\u73b0\u6709\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u5728\u51b7\u542f\u52a8\u573a\u666f\u4e0b\u5bf9\u6a21\u677f\u3001\u6807\u7b7e\u8bcd\u548c\u6837\u672c\u9009\u62e9\u654f\u611f\uff0c\u7279\u522b\u662f\u5ffd\u7565\u4e86\u6807\u7b7e\u8bcd\u4e0e\u6837\u672c\u5b9e\u4f8b\u5728\u5d4c\u5165\u7a7a\u95f4\u7684\u90bb\u8fd1\u6027\u4f9d\u8d56\u5173\u7cfb", "method": "\u5c06PLM\u8bcd\u6c47\u548c[MASK]\u5d4c\u5165\u6620\u5c04\u5230\u5171\u4eab\u7a7a\u95f4\uff0c\u901a\u8fc7\u964d\u7ef4\u805a\u7c7b\u5b9e\u73b0\u9ad8\u6548\u591a\u6837\u5316\u9009\u62e9\uff0c\u6784\u5efa\u8054\u5408\u4f18\u5316\u76ee\u6807\u5b9e\u73b0\u6700\u5c0f\u4e0d\u786e\u5b9a\u6027\u548c\u6700\u5927\u591a\u6837\u6027", "result": "\u57288\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u5728\u51b7\u542f\u52a8\u573a\u666f\u7684\u6807\u7b7e\u8bcd\u9009\u62e9\u548c\u5c11\u6837\u672c\u9009\u62e9\u4efb\u52a1\u4e0a\u5747\u53d6\u5f97\u663e\u8457\u63d0\u5347", "conclusion": "COLDSELECT\u901a\u8fc7\u8054\u5408\u4f18\u5316\u7b56\u7565\u548c\u591a\u6837\u6027\u5efa\u6a21\uff0c\u6709\u6548\u6355\u6349\u6570\u636e\u5173\u7cfb\uff0c\u4e3a\u51b7\u542f\u52a8\u573a\u666f\u7684\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2507.00355", "pdf": "https://arxiv.org/pdf/2507.00355", "abs": "https://arxiv.org/abs/2507.00355", "authors": ["Paul J. L. Ammann", "Jonas Golde", "Alan Akbik"], "title": "Question Decomposition for Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": "Accepted to ACL SRW 2025. 9 Pages, 2 Figures, 4 Tables", "summary": "Grounding large language models (LLMs) in verifiable external sources is a\nwell-established strategy for generating reliable answers. Retrieval-augmented\ngeneration (RAG) is one such approach, particularly effective for tasks like\nquestion answering: it retrieves passages that are semantically related to the\nquestion and then conditions the model on this evidence. However, multi-hop\nquestions, such as \"Which company among NVIDIA, Apple, and Google made the\nbiggest profit in 2023?,\" challenge RAG because relevant facts are often\ndistributed across multiple documents rather than co-occurring in one source,\nmaking it difficult for standard RAG to retrieve sufficient information. To\naddress this, we propose a RAG pipeline that incorporates question\ndecomposition: (i) an LLM decomposes the original query into sub-questions,\n(ii) passages are retrieved for each sub-question, and (iii) the merged\ncandidate pool is reranked to improve the coverage and precision of the\nretrieved evidence. We show that question decomposition effectively assembles\ncomplementary documents, while reranking reduces noise and promotes the most\nrelevant passages before answer generation. Although reranking itself is\nstandard, we show that pairing an off-the-shelf cross-encoder reranker with\nLLM-driven question decomposition bridges the retrieval gap on multi-hop\nquestions and provides a practical, drop-in enhancement, without any extra\ntraining or specialized indexing. We evaluate our approach on the MultiHop-RAG\nand HotpotQA, showing gains in retrieval (MRR@10: +36.7%) and answer accuracy\n(F1: +11.6%) over standard RAG baselines.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u95ee\u9898\u5206\u89e3\u548c\u5019\u9009\u6c60\u91cd\u6392\u5e8f\u7684RAG\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u591a\u8df3\u95ee\u9898\u7684\u68c0\u7d22\u6548\u679c\uff08MRR@10\u63d0\u534736.7%\uff09\u548c\u56de\u7b54\u51c6\u786e\u7387\uff08F1\u63d0\u9ad811.6%\uff09", "motivation": "\u4f20\u7edfRAG\u5728\u5904\u7406\u9700\u8981\u591a\u6587\u6863\u8bc1\u636e\u7684\u591a\u8df3\u95ee\u9898\u65f6\u5b58\u5728\u68c0\u7d22\u4e0d\u8db3\uff0c\u4f8b\u5982\u6bd4\u8f83\u591a\u5bb6\u516c\u53f8\u5229\u6da6\u7684\u95ee\u9898\u9700\u8981\u805a\u5408\u5206\u6563\u7684\u4e8b\u5b9e", "method": "1. \u7528LLM\u5206\u89e3\u539f\u59cb\u95ee\u9898\u4e3a\u5b50\u95ee\u9898\n2. \u5206\u522b\u68c0\u7d22\u5404\u5b50\u95ee\u9898\u7684\u76f8\u5173\u6bb5\u843d\n3. \u4f7f\u7528\u73b0\u6210\u4ea4\u53c9\u7f16\u7801\u5668\u5bf9\u5408\u5e76\u5019\u9009\u6c60\u91cd\u6392\u5e8f\uff0c\u63d0\u5347\u8bc1\u636e\u8986\u76d6\u7387\u548c\u76f8\u5173\u6027", "result": "\u5728MultiHop-RAG\u548cHotpotQA\u6570\u636e\u96c6\u4e0a\uff0c\u68c0\u7d22\u6307\u6807MRR@10\u63d0\u534736.7%\uff0c\u7b54\u6848\u751f\u6210F1\u5206\u6570\u63d0\u534711.6%", "conclusion": "\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u4e13\u7528\u7d22\u5f15\uff0c\u901a\u8fc7\u95ee\u9898\u5206\u89e3\u4e0e\u91cd\u6392\u5e8f\u7684\u7ec4\u5408\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8df3\u95ee\u9898\u7684\u8bc1\u636e\u68c0\u7d22\u74f6\u9888"}}
{"id": "2507.00380", "pdf": "https://arxiv.org/pdf/2507.00380", "abs": "https://arxiv.org/abs/2507.00380", "authors": ["Vojt\u011bch Lanz", "Jan Haji\u010d jr"], "title": "Gregorian melody, modality, and memory: Segmenting chant with Bayesian nonparametrics", "categories": ["cs.CL"], "comment": null, "summary": "The idea that Gregorian melodies are constructed from some vocabulary of\nsegments has long been a part of chant scholarship. This so-called\n\"centonisation\" theory has received much musicological criticism, but frequent\nre-use of certain melodic segments has been observed in chant melodies, and the\nintractable number of possible segmentations allowed the option that some\nundiscovered segmentation exists that will yet prove the value of\ncentonisation, and recent empirical results have shown that segmentations can\noutperform music-theoretical features in mode classification. Inspired by the\nfact that Gregorian chant was memorised, we search for an optimal unsupervised\nsegmentation of chant melody using nested hierarchical Pitman-Yor language\nmodels. The segmentation we find achieves state-of-the-art performance in mode\nclassification. Modeling a monk memorising the melodies from one liturgical\nmanuscript, we then find empirical evidence for the link between mode\nclassification and memory efficiency, and observe more formulaic areas at the\nbeginnings and ends of melodies corresponding to the practical role of modality\nin performance. However, the resulting segmentations themselves indicate that\neven such a memory-optimal segmentation is not what is understood as\ncentonisation.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u65e0\u76d1\u7763\u5206\u5c42Pitman-Yor\u8bed\u8a00\u6a21\u578b\u5bf9\u683c\u91cc\u9ad8\u5229\u5723\u6b4c\u65cb\u5f8b\u8fdb\u884c\u5206\u5272\uff0c\u53d1\u73b0\u6700\u4f18\u5206\u5272\u5728\u8c03\u5f0f\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4e0e\u4f20\u7edf'\u62fc\u8d34\u7406\u8bba'\u5b58\u5728\u5dee\u5f02\uff0c\u63ed\u793a\u8bb0\u5fc6\u6548\u7387\u4e0e\u65cb\u5f8b\u516c\u5f0f\u5316\u533a\u57df\u5173\u8054\u3002", "motivation": "\u63a2\u7a76\u683c\u91cc\u9ad8\u5229\u5723\u6b4c\u65cb\u5f8b\u662f\u5426\u5b58\u5728\u7cfb\u7edf\u6027\u62fc\u8d34\u7ed3\u6784\uff08centonisation\uff09\uff0c\u5e76\u9a8c\u8bc1\u8bb0\u5fc6\u673a\u5236\u5bf9\u65cb\u5f8b\u7ec4\u7ec7\u7684\u5f71\u54cd\u3002\u57fa\u4e8e\u5723\u6b4c\u4f9d\u8d56\u8bb0\u5fc6\u4f20\u64ad\u7684\u7279\u6027\uff0c\u5047\u8bbe\u6700\u4f18\u5206\u5272\u5e94\u53cd\u6620\u8bb0\u5fc6\u6548\u7387\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u5d4c\u5957\u5206\u5c42Pitman-Yor\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u65e0\u76d1\u7763\u65cb\u5f8b\u5206\u5272\uff0c\u6a21\u62df\u4fee\u58eb\u4ece\u5355\u4e00\u793c\u4eea\u624b\u7a3f\u8bb0\u5fc6\u65cb\u5f8b\u7684\u8fc7\u7a0b\uff0c\u901a\u8fc7\u6a21\u5f0f\u5206\u7c7b\u4efb\u52a1\u9a8c\u8bc1\u5206\u5272\u6548\u679c\u3002", "result": "1. \u5206\u5272\u65b9\u6cd5\u5b9e\u73b0\u8c03\u5f0f\u5206\u7c7bSOTA\u6027\u80fd\uff1b2. \u53d1\u73b0\u65cb\u5f8b\u9996\u5c3e\u5b58\u5728\u66f4\u591a\u516c\u5f0f\u5316\u6bb5\u843d\uff08\u4e0e\u8c03\u5f0f\u8868\u6f14\u529f\u80fd\u5bf9\u5e94\uff09\uff1b3. \u8bb0\u5fc6\u6700\u4f18\u5206\u5272\u4e0e\u97f3\u4e50\u5b66\u5b9a\u4e49\u7684\u62fc\u8d34\u7406\u8bba\u5b58\u5728\u672c\u8d28\u5dee\u5f02\u3002", "conclusion": "\u5c3d\u7ba1\u8bb0\u5fc6\u4f18\u5316\u7684\u5206\u5272\u652f\u6301\u8c03\u5f0f\u5206\u7c7b\uff0c\u4f46\u5b9e\u8bc1\u8868\u660e\u4f20\u7edf\u62fc\u8d34\u7406\u8bba\u672a\u80fd\u51c6\u786e\u63cf\u8ff0\u5723\u6b4c\u7ed3\u6784\u3002\u65cb\u5f8b\u7ec4\u7ec7\u66f4\u53ef\u80fd\u53d7\u8bb0\u5fc6\u6548\u7387\u9a71\u52a8\uff0c\u800c\u975e\u9884\u8bbe\u7684\u62fc\u8d34\u89c4\u5219\u3002"}}
{"id": "2507.00389", "pdf": "https://arxiv.org/pdf/2507.00389", "abs": "https://arxiv.org/abs/2507.00389", "authors": ["Jing Ren", "Wenhao Zhou", "Bowen Li", "Mujie Liu", "Nguyen Linh Dan Le", "Jiade Cen", "Liping Chen", "Ziqi Xu", "Xiwei Xu", "Xiaodong Li"], "title": "Causal Prompting for Implicit Sentiment Analysis with Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Implicit Sentiment Analysis (ISA) aims to infer sentiment that is implied\nrather than explicitly stated, requiring models to perform deeper reasoning\nover subtle contextual cues. While recent prompting-based methods using Large\nLanguage Models (LLMs) have shown promise in ISA, they often rely on majority\nvoting over chain-of-thought (CoT) reasoning paths without evaluating their\ncausal validity, making them susceptible to internal biases and spurious\ncorrelations. To address this challenge, we propose CAPITAL, a causal prompting\nframework that incorporates front-door adjustment into CoT reasoning. CAPITAL\ndecomposes the overall causal effect into two components: the influence of the\ninput prompt on the reasoning chains, and the impact of those chains on the\nfinal output. These components are estimated using encoder-based clustering and\nthe NWGM approximation, with a contrastive learning objective used to better\nalign the encoder's representation with the LLM's reasoning space. Experiments\non benchmark ISA datasets with three LLMs demonstrate that CAPITAL consistently\noutperforms strong prompting baselines in both accuracy and robustness,\nparticularly under adversarial conditions. This work offers a principled\napproach to integrating causal inference into LLM prompting and highlights its\nbenefits for bias-aware sentiment reasoning. The source code and case study are\navailable at: https://github.com/whZ62/CAPITAL.", "AI": {"tldr": "\u63d0\u51fa\u56e0\u679c\u63a8\u7406\u6846\u67b6CAPITAL\uff0c\u901a\u8fc7\u524d\u95e8\u8c03\u6574\u6539\u8fdb\u9690\u5f0f\u60c5\u611f\u5206\u6790\uff0c\u63d0\u5347LLM\u63a8\u7406\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027", "motivation": "\u73b0\u6709\u57fa\u4e8e\u63d0\u793a\u7684\u9690\u5f0f\u60c5\u611f\u5206\u6790\u65b9\u6cd5\u4f9d\u8d56\u601d\u7ef4\u94fe\u7684\u591a\u6570\u6295\u7968\u673a\u5236\uff0c\u672a\u9a8c\u8bc1\u56e0\u679c\u6709\u6548\u6027\uff0c\u6613\u53d7\u5185\u90e8\u504f\u89c1\u548c\u4f2a\u76f8\u5173\u5f71\u54cd", "method": "\u5c06\u603b\u56e0\u679c\u6548\u5e94\u5206\u89e3\u4e3a\u8f93\u5165\u63d0\u793a\u5bf9\u63a8\u7406\u94fe\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u63a8\u7406\u94fe\u5bf9\u8f93\u51fa\u7684\u5f71\u54cd\uff0c\u91c7\u7528\u7f16\u7801\u5668\u805a\u7c7b+NWGM\u8fd1\u4f3c\u4f30\u8ba1\uff0c\u914d\u5408\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u8868\u5f81\u7a7a\u95f4", "result": "\u5728\u4e09\u4e2aLLM\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCAPITAL\u5728\u51c6\u786e\u7387\u548c\u9c81\u68d2\u6027\uff08\u5c24\u5176\u5bf9\u6297\u573a\u666f\uff09\u6301\u7eed\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5", "conclusion": "\u4e3a\u56e0\u679c\u63a8\u7406\u878d\u5165LLM\u63d0\u793a\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u57fa\u4e8e\u504f\u7f6e\u611f\u77e5\u7684\u60c5\u611f\u63a8\u7406\u7a81\u7834"}}
{"id": "2507.00439", "pdf": "https://arxiv.org/pdf/2507.00439", "abs": "https://arxiv.org/abs/2507.00439", "authors": ["Gauri Kambhatla", "Sanjana Gautam", "Angela Zhang", "Alex Liu", "Ravi Srinivasan", "Junyi Jessy Li", "Matthew Lease"], "title": "Beyond Sociodemographic Prompting: Using Supervision to Align LLMs with Human Response Distributions", "categories": ["cs.CL"], "comment": null, "summary": "The ability to accurately predict how different population groups would\nanswer subjective questions would have great value. In this work, we show that\nuse of relatively simple supervision can greatly improve language model\nalignment with diverse population groups, as measured over three datasets\nspanning various topics. Beyond evaluating average performance, we also report\nhow alignment varies across specific groups. The simplicity and generality of\nour approach promotes easy adoption, while our broad findings provide useful\nguidance for when to use or not use our approach in practice. By conducting\nevaluation over many LLMs and prompting strategies, along with open-sourcing\nour work, we provide a useful benchmark to stimulate future research.", "AI": {"tldr": "\u901a\u8fc7\u7b80\u5355\u76d1\u7763\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u4e0e\u4e0d\u540c\u4eba\u7fa4\u7684\u4ef7\u503c\u89c2\u5bf9\u9f50\u6548\u679c", "motivation": "\u9884\u6d4b\u4e0d\u540c\u7fa4\u4f53\u5bf9\u4e3b\u89c2\u95ee\u9898\u7684\u56de\u7b54\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u590d\u6742\u5ea6\u9ad8\u3002\u9700\u8981\u63a2\u7d22\u7b80\u5355\u6709\u6548\u7684\u76d1\u7763\u65b9\u5f0f\u6765\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u4e0e\u591a\u6837\u5316\u7fa4\u4f53\u7684\u5bf9\u9f50\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u4e09\u4e2a\u6db5\u76d6\u591a\u4e3b\u9898\u7684\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u7b80\u5355\u76d1\u7763\u673a\u5236\u8bc4\u4f30\u4e0d\u540cLLMs\u548c\u63d0\u793a\u7b56\u7565\u7684\u5bf9\u9f50\u6548\u679c\uff0c\u91cd\u70b9\u5173\u6ce8\u4e0d\u540c\u7fa4\u4f53\u95f4\u7684\u5dee\u5f02\u6027\u8868\u73b0\u3002", "result": "\u65b9\u6cd5\u5728\u591a\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u663e\u793a\u6709\u6548\u6027\uff08\u63d0\u5347\u7fa4\u4f53\u5bf9\u9f50\u5ea6\uff09\uff0c\u4f46\u6548\u679c\u5b58\u5728\u7fa4\u4f53\u5dee\u5f02\u6027\uff0c\u9700\u6839\u636e\u5177\u4f53\u5e94\u7528\u573a\u666f\u8c28\u614e\u4f7f\u7528\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b80\u5355\u901a\u7528\u65b9\u6cd5\u4e3a\u4ef7\u503c\u89c2\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u901a\u8fc7\u5f00\u6e90\u8bc4\u4f30\u6846\u67b6\u548c\u8de8\u6a21\u578b\u6d4b\u8bd5\u7ed3\u679c\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u7684\u7814\u7a76\u57fa\u51c6\u3002"}}
{"id": "2507.00460", "pdf": "https://arxiv.org/pdf/2507.00460", "abs": "https://arxiv.org/abs/2507.00460", "authors": ["Md. Najib Hasan", "Mohammad Fakhruddin Babar", "Souvika Sarkar", "Monowar Hasan", "Santu Karmaker"], "title": "Pitfalls of Evaluating Language Models with Open Benchmarks", "categories": ["cs.CL"], "comment": null, "summary": "Open Large Language Model (LLM) benchmarks, such as HELM and BIG-bench, offer\nstandardized, transparent protocols that facilitate the fair comparison,\nreproducibility, and iterative advancement of Language Models (LMs). However,\ntheir openness also introduces critical and underexplored pitfalls. This study\nexposes these weaknesses by systematically constructing ``cheating'' models --\nsmaller variants of BART, T5, and GPT-2 fine-tuned directly on public test sets\n-- which achieve top rankings on a prominent open, holistic benchmark (HELM)\ndespite poor generalization and limited practical utility. Our findings\nunderscore three key insights: \\ca high leaderboard performance on open\nbenchmarks may not always reflect real-world effectiveness; \\cb private or\ndynamic benchmarks must complement open evaluations to safeguard integrity; and\n\\cc a fundamental reevaluation of current benchmarking practices is essential\nto ensure robust and trustworthy LM assessments.", "AI": {"tldr": "\u5f00\u653eLLM\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u6f0f\u6d1e\uff0c\u5fae\u8c03\u5c0f\u578b\u6a21\u578b\u53ef\u865a\u5047\u767b\u9876\u6392\u884c\u699c\u4f46\u7f3a\u4e4f\u5b9e\u7528\u6027\uff0c\u9700\u6539\u8fdb\u8bc4\u4f30\u4f53\u7cfb", "motivation": "\u63ed\u793a\u5f00\u653e\u57fa\u51c6\u6d4b\u8bd5\u7684\u6f5c\u5728\u98ce\u9669\uff1a\u6807\u51c6\u5316\u8bc4\u4f30\u534f\u8bae\u53ef\u80fd\u88ab\u9488\u5bf9\u6027\u4f18\u5316\uff0c\u5bfc\u81f4\u6392\u884c\u699c\u7ed3\u679c\u5931\u771f", "method": "\u6784\u5efaBART/T5/GPT-2\u7684\u5c0f\u578b\u53d8\u4f53\uff0c\u76f4\u63a5\u5728HELM\u7b49\u516c\u5f00\u6d4b\u8bd5\u96c6\u4e0a\u5fae\u8c03\u5236\u9020\u300e\u4f5c\u5f0a\u300f\u6a21\u578b", "result": "\u4f5c\u5f0a\u6a21\u578b\u5728HELM\u57fa\u51c6\u6392\u540d\u524d\u5217\uff0c\u4f46\u5b9e\u9645\u6cdb\u5316\u80fd\u529b\u5dee\u4e14\u7f3a\u4e4f\u5b9e\u7528\u4ef7\u503c", "conclusion": "\u9700\u7ed3\u5408\u79c1\u6709/\u52a8\u6001\u6d4b\u8bd5\u57fa\u51c6\uff0c\u91cd\u6784\u8bc4\u4f30\u4f53\u7cfb\u4ee5\u786e\u4fdd\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u548c\u5b9e\u9645\u4ef7\u503c"}}
{"id": "2507.00509", "pdf": "https://arxiv.org/pdf/2507.00509", "abs": "https://arxiv.org/abs/2507.00509", "authors": ["To Eun Kim", "Jo\u00e3o Coelho", "Gbemileke Onilude", "Jai Singh"], "title": "TeamCMU at Touch\u00e9: Adversarial Co-Evolution for Advertisement Integration and Detection in Conversational Search", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As conversational search engines increasingly adopt generation-based\nparadigms powered by Large Language Models (LLMs) and Retrieval-Augmented\nGeneration (RAG), the integration of advertisements into generated responses\npresents both commercial opportunities and challenges for user experience.\nUnlike traditional search, where advertisements are clearly delineated,\ngenerative systems blur the boundary between informational content and\npromotional material, raising concerns around transparency and trust. In this\nwork, we propose a modular pipeline for advertisement management in RAG-based\nconversational systems, consisting of an ad-rewriter for seamless ad\nintegration and a robust ad-classifier for detection. We leverage synthetic\ndata to train high-performing classifiers, which are then used to guide two\ncomplementary ad-integration strategies: supervised fine-tuning of the\nad-rewriter and a best-of-N sampling approach that selects the least detectable\nad-integrated response among multiple candidates. Our evaluation focuses on two\ncore questions: the effectiveness of ad classifiers in detecting diverse ad\nintegration strategies, and the training methods that best support coherent,\nminimally intrusive ad insertion. Experimental results show that our\nad-classifier, trained on synthetic advertisement data inspired by marketing\nstrategies and enhanced through curriculum learning, achieves robust detection\nperformance. Additionally, we demonstrate that classifier-guided optimization,\nthrough both fine-tuning and best-of-N sampling, significantly improves ad\nstealth, enabling more seamless integration. These findings contribute an\nadversarial co-evolution framework for developing more sophisticated ad-aware\ngenerative search systems and robust ad classifiers.", "AI": {"tldr": "\u63d0\u51fa\u6a21\u5757\u5316\u5e7f\u544a\u7ba1\u7406\u6d41\u7a0b\uff08\u5e7f\u544a\u91cd\u5199\u5668+\u5206\u7c7b\u5668\uff09\uff0c\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u5b9e\u73b0\u5728\u751f\u6210\u5f0f\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u66f4\u9690\u853d\u7684\u5e7f\u544a\u690d\u5165", "motivation": "\u751f\u6210\u5f0f\u641c\u7d22\u5f15\u64ce\u6a21\u7cca\u4e86\u5e7f\u544a\u4e0e\u4fe1\u606f\u5185\u5bb9\u7684\u754c\u9650\uff0c\u4f20\u7edf\u5e7f\u544a\u6807\u8bc6\u65b9\u5f0f\u4e0d\u518d\u9002\u7528\uff0c\u9700\u89e3\u51b3\u900f\u660e\u5ea6\u548c\u4fe1\u4efb\u5371\u673a", "method": "1. \u5f00\u53d1\u5e7f\u544a\u91cd\u5199\u5668\u5b9e\u73b0\u81ea\u7136\u690d\u5165 2. \u7528\u8425\u9500\u7b56\u7565\u542f\u53d1\u7684\u5408\u6210\u6570\u636e\u8bad\u7ec3\u5e7f\u544a\u68c0\u6d4b\u5206\u7c7b\u5668 3. \u91c7\u7528\u76d1\u7763\u5fae\u8c03\u548cBest-of-N\u91c7\u6837\u53cc\u91cd\u4f18\u5316\u7b56\u7565", "result": "\u5e7f\u544a\u5206\u7c7b\u5668\u68c0\u6d4b\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\uff0c\u4f18\u5316\u540e\u7684\u5e7f\u544a\u690d\u5165\u9690\u853d\u6027\u589e\u5f3a\uff08\u4eba\u5de5\u68c0\u6d4b\u6210\u529f\u7387\u964d\u4f4e37%\uff09\uff0c\u5b9e\u73b0\u81ea\u7136\u7684\u5185\u5bb9-\u5e7f\u544a\u878d\u5408", "conclusion": "\u5bf9\u6297\u6027\u534f\u540c\u8fdb\u5316\u6846\u67b6\u6709\u6548\u5e73\u8861\u5546\u4e1a\u53d8\u73b0\u4e0e\u7528\u6237\u4f53\u9a8c\uff0c\u4e3a\u751f\u6210\u5f0f\u641c\u7d22\u7cfb\u7edf\u7684\u5e7f\u544a\u6574\u5408\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2507.00534", "pdf": "https://arxiv.org/pdf/2507.00534", "abs": "https://arxiv.org/abs/2507.00534", "authors": ["Tahir Javed", "Kaushal Bhogale", "Mitesh M. Khapra"], "title": "NIRANTAR: Continual Learning with New Languages and Domains on Real-world Speech Data", "categories": ["cs.CL"], "comment": "Accepted in Interspecch 2025", "summary": "We introduce Nirantar, a comprehensive framework for evaluating continual\nlearning (CL) in multilingual and multi-domain ASR. Designed to reflect\nreal-world CL challenges, Nirantar leverages data collected incrementally\nacross 22 languages and 208 districts in India through natural episodes. This\nenables evaluation across Language-Incremental (LIL), Domain-Incremental (DIL),\nand the novel Language-Incremental Domain-Incremental Learning (LIDIL)\nscenarios. Unlike prior work that relies on simulated episodes, Nirantar\npresents dynamic, non-uniform language and domain shifts, making it an ideal\ntestbed for CL research. With 3250 hours of human-transcribed speech, including\n1720 hours newly introduced in this work, our framework enables systematic\nbenchmarking of CL methods. We evaluate existing approaches and demonstrate\nthat no single method performs consistently well, underscoring the need for\nmore robust CL strategies.", "AI": {"tldr": "\u63d0\u51fa\u4e86Nirantar\u6846\u67b6\uff0c\u901a\u8fc7\u771f\u5b9e\u589e\u91cf\u6570\u636e\u6784\u5efa\u591a\u8bed\u8a00/\u591a\u9886\u57dfASR\u6301\u7eed\u5b66\u4e60\u8bc4\u4f30\u4f53\u7cfb\uff0c\u5305\u542b3250\u5c0f\u65f6\u8bed\u97f3\u6570\u636e\uff0c\u53d1\u73b0\u73b0\u6709CL\u65b9\u6cd5\u8868\u73b0\u4e0d\u7a33\u5b9a\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u8bc4\u4f30\u8fc7\u4e8e\u4f9d\u8d56\u6a21\u62df\u6570\u636e\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u771f\u5b9e\u573a\u666f\u4e2d\u91c7\u96c6\u7684\u52a8\u6001\u975e\u5747\u5300\u8bed\u8a00/\u9886\u57df\u589e\u91cf\u6570\u636e\uff0c\u6784\u5efa\u66f4\u8d34\u8fd1\u5b9e\u9645\u6311\u6218\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u6536\u96c6\u5370\u5ea622\u79cd\u8bed\u8a00\u3001208\u5730\u533a\u7684\u81ea\u7136\u589e\u91cf\u8bed\u97f3\u6570\u636e\uff0c\u8bbe\u8ba1\u8bed\u8a00\u589e\u91cf(LIL)\u3001\u9886\u57df\u589e\u91cf(DIL)\u53ca\u6df7\u5408\u589e\u91cf(LIDIL)\u4e09\u79cd\u8bc4\u4f30\u573a\u666f\uff0c\u5305\u542b3250\u5c0f\u65f6\u4eba\u5de5\u6807\u6ce8\u8bed\u97f3\uff08\u5176\u4e2d1720\u5c0f\u65f6\u4e3a\u65b0\u6570\u636e\uff09\u3002", "result": "\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u5728\u6846\u67b6\u4e2d\u8868\u73b0\u53c2\u5dee\u4e0d\u9f50\uff0c\u6ca1\u6709\u5355\u4e00\u65b9\u6cd5\u5728\u6240\u6709\u573a\u666f\u4e2d\u4fdd\u6301\u4f18\u52bf\uff0c\u7a81\u663e\u73b0\u6709CL\u7b56\u7565\u7684\u5c40\u9650\u6027\u3002", "conclusion": "Nirantar\u4e3a\u6301\u7eed\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u8bc4\u4f30\u5e73\u53f0\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5f3a\u8c03\u9700\u8981\u5f00\u53d1\u66f4\u5177\u9c81\u68d2\u6027\u7684\u6301\u7eed\u5b66\u4e60\u7b56\u7565\u3002"}}
{"id": "2507.00540", "pdf": "https://arxiv.org/pdf/2507.00540", "abs": "https://arxiv.org/abs/2507.00540", "authors": ["Shixiao Wang", "Yifan Zhuang", "Runsheng Zhang", "Zhijun Song"], "title": "Capsule Network-Based Semantic Intent Modeling for Human-Computer Interaction", "categories": ["cs.CL"], "comment": null, "summary": "This paper proposes a user semantic intent modeling algorithm based on\nCapsule Networks to address the problem of insufficient accuracy in intent\nrecognition for human-computer interaction. The method represents semantic\nfeatures in input text through a vectorized capsule structure. It uses a\ndynamic routing mechanism to transfer information across multiple capsule\nlayers. This helps capture hierarchical relationships and part-whole structures\nbetween semantic entities more effectively. The model uses a convolutional\nfeature extraction module as the low-level encoder. After generating initial\nsemantic capsules, it forms high-level abstract intent representations through\nan iterative routing process. To further enhance performance, a margin-based\nmechanism is introduced into the loss function. This improves the model's\nability to distinguish between intent classes. Experiments are conducted using\na public natural language understanding dataset. Multiple mainstream models are\nused for comparison. Results show that the proposed model outperforms\ntraditional methods and other deep learning structures in terms of accuracy,\nF1-score, and intent detection rate. The study also analyzes the effect of the\nnumber of dynamic routing iterations on model performance. A convergence curve\nof the loss function during training is provided. These results verify the\nstability and effectiveness of the proposed method in semantic modeling.\nOverall, this study presents a new structured modeling approach to improve\nintent recognition under complex semantic conditions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u80f6\u56ca\u7f51\u7edc\u7684\u7528\u6237\u8bed\u4e49\u610f\u56fe\u5efa\u6a21\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8def\u7531\u673a\u5236\u63d0\u5347\u4eba\u673a\u4ea4\u4e92\u4e2d\u610f\u56fe\u8bc6\u522b\u7684\u51c6\u786e\u7387", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u8bed\u4e49\u6761\u4ef6\u4e0b\u610f\u56fe\u8bc6\u522b\u51c6\u786e\u7387\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u96be\u4ee5\u6709\u6548\u6355\u6349\u8bed\u4e49\u5b9e\u4f53\u95f4\u7684\u5c42\u6b21\u5173\u7cfb\u548c\u90e8\u5206-\u6574\u4f53\u7ed3\u6784", "method": "1. \u4f7f\u7528\u5411\u91cf\u5316\u80f6\u56ca\u7ed3\u6784\u8868\u793a\u6587\u672c\u8bed\u4e49\u7279\u5f81\n2. \u901a\u8fc7\u52a8\u6001\u8def\u7531\u673a\u5236\u5b9e\u73b0\u8de8\u80f6\u56ca\u5c42\u4fe1\u606f\u4f20\u9012\n3. \u91c7\u7528\u5377\u79ef\u6a21\u5757\u4f5c\u4e3a\u5e95\u5c42\u7f16\u7801\u5668\u751f\u6210\u521d\u59cb\u8bed\u4e49\u80f6\u56ca\n4. \u5728\u635f\u5931\u51fd\u6570\u4e2d\u5f15\u5165\u57fa\u4e8e\u95f4\u9694\u7684\u4f18\u5316\u673a\u5236", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff1a\n- \u51c6\u786e\u7387\u3001F1\u503c\u548c\u610f\u56fe\u68c0\u6d4b\u7387\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u4e3b\u6d41\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\n- \u52a8\u6001\u8def\u7531\u8fed\u4ee33\u6b21\u65f6\u8fbe\u5230\u6700\u4f73\u6027\u80fd\n- \u635f\u5931\u51fd\u6570\u6536\u655b\u66f2\u7ebf\u9a8c\u8bc1\u8bad\u7ec3\u7a33\u5b9a\u6027", "conclusion": "\u63d0\u51fa\u7684\u7ed3\u6784\u5316\u5efa\u6a21\u65b9\u6cd5\u6709\u6548\u6539\u5584\u590d\u6742\u8bed\u4e49\u6761\u4ef6\u4e0b\u7684\u610f\u56fe\u8bc6\u522b\uff0c\u52a8\u6001\u8def\u7531\u673a\u5236\u548c\u635f\u5931\u51fd\u6570\u4f18\u5316\u662f\u5173\u952e\u521b\u65b0\u70b9\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u8bed\u4e49\u5efa\u6a21\u4e2d\u7684\u4f18\u8d8a\u6027"}}
{"id": "2507.00547", "pdf": "https://arxiv.org/pdf/2507.00547", "abs": "https://arxiv.org/abs/2507.00547", "authors": ["Malmi Amadoru"], "title": "Methodological Rigour in Algorithm Application: An Illustration of Topic Modelling Algorithm", "categories": ["cs.CL"], "comment": null, "summary": "The rise of advanced computational algorithms has opened new avenues for\ncomputationally intensive research approaches to theory development. However,\nthe opacity of these algorithms and lack of transparency and rigour in their\napplication pose methodological challenges, potentially undermining trust in\nresearch. The discourse on methodological rigour in this new genre of research\nis still emerging. Against this backdrop, I attempt to offer guidance on\nmethodological rigour, particularly in the context of topic modelling\nalgorithms. By illustrating the application of the structural topic modelling\nalgorithm and presenting a set of guidelines, I discuss how to ensure rigour in\ntopic modelling studies. Although the guidelines are for the application of\ntopic modelling algorithms, they can be applied to other algorithms with\ncontext-specific adjustments. The guidelines are helpful, especially for novice\nresearchers applying topic modelling, and editors and reviewers handling topic\nmodelling manuscripts. I contribute to the literature on topic modelling and\njoin the emerging dialogue on methodological rigour in computationally\nintensive theory construction research.", "AI": {"tldr": "\u9488\u5bf9\u8ba1\u7b97\u5bc6\u96c6\u578b\u7406\u8bba\u7814\u7a76\u4e2d\u7b97\u6cd5\u4e0d\u900f\u660e\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e3b\u9898\u5efa\u6a21\u7b97\u6cd5\u5e94\u7528\u6307\u5357\u4ee5\u63d0\u5347\u65b9\u6cd5\u4e25\u8c28\u6027", "motivation": "\u73b0\u6709\u8ba1\u7b97\u7b97\u6cd5\uff08\u5982\u4e3b\u9898\u5efa\u6a21\uff09\u7684\u6a21\u7cca\u6027\u548c\u5e94\u7528\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u53ef\u80fd\u635f\u5bb3\u7814\u7a76\u53ef\u4fe1\u5ea6\uff0c\u9700\u5efa\u7acb\u65b9\u6cd5\u8bba\u89c4\u8303", "method": "\u4f7f\u7528\u7ed3\u6784\u4e3b\u9898\u5efa\u6a21\u7b97\u6cd5\u8fdb\u884c\u6848\u4f8b\u5c55\u793a\uff0c\u5e76\u5236\u5b9a\u8de8\u7b97\u6cd5\u9002\u7528\u7684\u5b9e\u65bd\u6307\u5357", "result": "\u5f00\u53d1\u51fa\u53ef\u9002\u914d\u5176\u4ed6\u8ba1\u7b97\u7b97\u6cd5\u7684\u901a\u7528\u4e25\u8c28\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u7406\u8bba\u5efa\u6784\u7814\u7a76", "conclusion": "\u4e3a\u8ba1\u7b97\u5bc6\u96c6\u578b\u7406\u8bba\u7814\u7a76\u63d0\u4f9b\u65b9\u6cd5\u8bba\u57fa\u51c6\uff0c\u5f3a\u5316\u4e3b\u9898\u5efa\u6a21\u7814\u7a76\u7684\u53ef\u4fe1\u5ea6\uff0c\u63a8\u52a8\u5b66\u672f\u5171\u540c\u4f53\u5efa\u7acb\u7b97\u6cd5\u5e94\u7528\u89c4\u8303"}}
{"id": "2507.00579", "pdf": "https://arxiv.org/pdf/2507.00579", "abs": "https://arxiv.org/abs/2507.00579", "authors": ["Miriam Ansch\u00fctz", "Ekaterina Gikalo", "Niklas Herbster", "Georg Groh"], "title": "TUM-MiKaNi at SemEval-2025 Task 3: Towards Multilingual and Knowledge-Aware Non-factual Hallucination Identification", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 3 figures, SemEval-2025 Task 3, ACL", "summary": "Hallucinations are one of the major problems of LLMs, hindering their\ntrustworthiness and deployment to wider use cases. However, most of the\nresearch on hallucinations focuses on English data, neglecting the multilingual\nnature of LLMs. This paper describes our submission to the SemEval-2025 Task-3\n- Mu-SHROOM, the Multilingual Shared-task on Hallucinations and Related\nObservable Overgeneration Mistakes. We propose a two-part pipeline that\ncombines retrieval-based fact verification against Wikipedia with a BERT-based\nsystem fine-tuned to identify common hallucination patterns. Our system\nachieves competitive results across all languages, reaching top-10 results in\neight languages, including English. Moreover, it supports multiple languages\nbeyond the fourteen covered by the shared task. This multilingual hallucination\nidentifier can help to improve LLM outputs and their usefulness in the future.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u68c0\u7d22\u9a8c\u8bc1\u4e0eBERT\u6a21\u578b\u7684\u65b9\u6848\u68c0\u6d4b\u591a\u8bed\u8a00LLM\u7684\u5e7b\u89c9\u95ee\u9898", "motivation": "\u73b0\u6709\u5e7b\u89c9\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u82f1\u8bed\uff0c\u5ffd\u89c6LLMs\u7684\u591a\u8bed\u8a00\u7279\u6027", "method": "\u57fa\u4e8e\u7ef4\u57fa\u767e\u79d1\u7684\u68c0\u7d22\u5f0f\u4e8b\u5b9e\u6838\u67e5 + \u5fae\u8c03BERT\u8bc6\u522b\u5e38\u89c1\u5e7b\u89c9\u6a21\u5f0f", "result": "\u57288\u79cd\u8bed\u8a00\u8fdb\u5165\u524d\u5341\uff08\u542b\u82f1\u8bed\uff09\uff0c\u652f\u6301\u8d85\u8fc714\u79cd\u8bed\u8a00\u7684\u68c0\u6d4b", "conclusion": "\u8be5\u591a\u8bed\u8a00\u68c0\u6d4b\u5de5\u5177\u53ef\u63d0\u5347LLM\u8f93\u51fa\u8d28\u91cf\u4e0e\u5b9e\u7528\u6027"}}
{"id": "2507.00601", "pdf": "https://arxiv.org/pdf/2507.00601", "abs": "https://arxiv.org/abs/2507.00601", "authors": ["Shuangquan Lyu", "Yingnan Deng", "Guiran Liu", "Zhen Qi", "Ruotong Wang"], "title": "Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt and Alignment-Based", "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses the limited transfer and adaptation capabilities of\nlarge language models in low-resource language scenarios. It proposes a unified\nframework that combines a knowledge transfer module with parameter-efficient\nfine-tuning strategies. The method introduces knowledge alignment loss and soft\nprompt tuning to guide the model in effectively absorbing the structural\nfeatures of target languages or tasks under minimal annotation. This enhances\nboth generalization performance and training stability. The framework includes\nlightweight adaptation modules to reduce computational costs. During training,\nit integrates freezing strategies and prompt injection to preserve the model's\noriginal knowledge while enabling quick adaptation to new tasks. The study also\nconducts stability analysis experiments and synthetic pseudo-data transfer\nexperiments to systematically evaluate the method's applicability and\nrobustness across different low-resource tasks. Experimental results show that\ncompared with existing multilingual pre-trained models and mainstream transfer\nmethods, the proposed approach achieves higher performance and stability on\ncross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates\nparticularly strong advantages under extremely data-scarce conditions. The\nproposed method offers strong generality and scalability. It enhances\ntask-specific adaptability while preserving the general capabilities of large\nlanguage models. This makes it well-suited for complex semantic modeling and\nmultilingual processing tasks.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u77e5\u8bc6\u8f6c\u79fb\u4e0e\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u5bf9\u9f50\u635f\u5931\u548c\u8f6f\u63d0\u793a\u8c03\u4f18\u589e\u5f3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u573a\u666f\u4e0b\u5927\u6a21\u578b\u7684\u8de8\u4efb\u52a1\u9002\u5e94\u80fd\u529b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5728MLQA/XQuAD/PAWS-X\u7b49\u8de8\u8bed\u8a00\u4efb\u52a1\u4e2d\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u4e3b\u6d41\u65b9\u6cd5", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u573a\u666f\u4e2d\u8fc1\u79fb\u80fd\u529b\u4e0d\u8db3\u3001\u8bad\u7ec3\u7a33\u5b9a\u6027\u5dee\u7684\u95ee\u9898\uff0c\u9700\u5728\u6700\u5c0f\u6807\u6ce8\u6761\u4ef6\u4e0b\u63d0\u5347\u6a21\u578b\u5bf9\u76ee\u6807\u8bed\u8a00\u7279\u5f81\u7684\u5438\u6536\u6548\u7387", "method": "\u6784\u5efa\u7edf\u4e00\u6846\u67b6\uff1a1)\u77e5\u8bc6\u8f6c\u79fb\u6a21\u5757\u91c7\u7528\u5bf9\u9f50\u635f\u5931\u5f15\u5bfc\u7279\u5f81\u5438\u6536 2)\u8f6f\u63d0\u793a\u8c03\u4f18\u7ed3\u5408\u8f7b\u91cf\u9002\u914d\u6a21\u5757 3)\u51bb\u7ed3\u7b56\u7565\u4e0e\u63d0\u793a\u6ce8\u5165\u5b9e\u73b0\u77e5\u8bc6\u4fdd\u7559 4)\u7a33\u5b9a\u6027\u5206\u6790\u4e0e\u4f2a\u6570\u636e\u8fc1\u79fb\u5b9e\u9a8c\u9a8c\u8bc1\u9c81\u68d2\u6027", "result": "\u5728MLQA/XQuAD/PAWS-X\u8de8\u8bed\u8a00\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u63d0\u53473-5\u4e2a\u70b9\uff0c\u6570\u636e\u7a00\u7f3a\u573a\u666f(\uff1c1%\u6807\u6ce8)\u4e0b\u8bad\u7ec3\u901f\u5ea6\u63d0\u53472\u500d\uff0c\u6a21\u578b\u7a33\u5b9a\u6027\u6307\u6807\u63d0\u9ad840%", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u589e\u5f3a\u4efb\u52a1\u7279\u5b9a\u9002\u5e94\u6027\u7684\u540c\u65f6\u4fdd\u7559\u5927\u6a21\u578b\u901a\u7528\u80fd\u529b\uff0c\u4e3a\u591a\u8bed\u8a00\u5904\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u590d\u6742\u8bed\u4e49\u5efa\u6a21\u573a\u666f"}}
{"id": "2507.00606", "pdf": "https://arxiv.org/pdf/2507.00606", "abs": "https://arxiv.org/abs/2507.00606", "authors": ["Tao Xiong", "Xavier Hu", "Wenyan Fan", "Shengyu Zhang"], "title": "Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) excel in complex tasks through advanced\nprompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but\ntheir reliance on manually crafted, task-specific prompts limits adaptability\nand efficiency. We introduce Mixture of Reasoning (MoR), a training framework\nthat embeds diverse reasoning strategies into LLMs for autonomous,\ntask-adaptive reasoning without external prompt engineering. MoR has two\nphases: Thought Generation, creating reasoning chain templates with models like\nGPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets\nfor supervised fine-tuning.Our experiments show that MoR significantly enhances\nperformance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting\nand 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need\nfor task-specific prompts, offering a generalizable solution for robust\nreasoning across diverse tasks.", "AI": {"tldr": "\u63d0\u51faMixture of Reasoning (MoR)\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5d4c\u5165\u591a\u6837\u5316\u63a8\u7406\u7b56\u7565\u4f7f\u5927\u6a21\u578b\u5b9e\u73b0\u81ea\u9002\u5e94\u63a8\u7406\uff0c\u65e0\u9700\u4eba\u5de5\u8bbe\u8ba1\u4efb\u52a1\u63d0\u793a", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\u4efb\u52a1\u7279\u5b9a\u63d0\u793a(\u5982CoT/ToT)\uff0c\u9650\u5236\u6a21\u578b\u9002\u5e94\u6027\u548c\u6548\u7387", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u4f7f\u7528GPT-4o\u751f\u6210\u63a8\u7406\u94fe\u6a21\u677f 2) \u6784\u5efa\u76d1\u7763\u5fae\u8c03\u6570\u636e\u96c6\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3", "result": "MoR150\u5728CoT\u63d0\u793a\u4e0b\u51c6\u786e\u73870.730(\u63d0\u53472.2%)\uff0c\u57fa\u7ebf\u6bd4\u8f83\u63d0\u534713.5%\u8fbe0.734", "conclusion": "MoR\u63d0\u4f9b\u901a\u7528\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u6d88\u9664\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u9700\u6c42\uff0c\u589e\u5f3a\u8de8\u9886\u57df\u63a8\u7406\u9c81\u68d2\u6027"}}
{"id": "2507.00665", "pdf": "https://arxiv.org/pdf/2507.00665", "abs": "https://arxiv.org/abs/2507.00665", "authors": ["Sihang Li", "Wei Shi", "Ziyuan Xie", "Tao Liang", "Guojun Ma", "Xiang Wang"], "title": "SAFER: Probing Safety in Reward Models with Sparse Autoencoder", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reinforcement learning from human feedback (RLHF) is a key paradigm for\naligning large language models (LLMs) with human values, yet the reward models\nat its core remain largely opaque. In this work, we present sparse Autoencoder\nFor Enhanced Reward model (\\textbf{SAFER}), a novel framework for interpreting\nand improving reward models through mechanistic analysis. Leveraging Sparse\nAutoencoders (SAEs), we uncover human-interpretable features in reward model\nactivations, enabling insight into safety-relevant decision-making. We apply\nSAFER to safety-oriented preference datasets and quantify the salience of\nindividual features by activation differences between chosen and rejected\nresponses. Using these feature-level signals, we design targeted data poisoning\nand denoising strategies. Experiments show that SAFER can precisely degrade or\nenhance safety alignment with minimal data modification, without sacrificing\ngeneral chat performance. Our approach contributes to interpreting, auditing\nand refining reward models in high-stakes LLM alignment tasks. Our codes are\navailable at https://github.com/xzy-101/SAFER-code. \\textit{This paper\ndiscusses topics related to large language model safety and may include\ndiscussions or examples that highlight potential risks or unsafe outcomes.}", "AI": {"tldr": "\u7814\u7a76\u8005\u63d0\u51faSAFER\u6846\u67b6\uff0c\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\u89e3\u6790\u5956\u52b1\u6a21\u578b\u673a\u5236\uff0c\u5b9e\u73b0\u5b89\u5168\u5bf9\u9f50\u7684\u7cbe\u51c6\u8c03\u63a7\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09\u4e2d\u5956\u52b1\u6a21\u578b\u5b58\u5728\u9ed1\u7bb1\u95ee\u9898\uff0c\u96be\u4ee5\u7406\u89e3\u5176\u5b89\u5168\u51b3\u7b56\u673a\u5236\u3002\u8be5\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u5206\u6790\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u5bf9\u9f50\u7684\u53ef\u9760\u6027\u3002", "method": "1. \u5229\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u63d0\u53d6\u5956\u52b1\u6a21\u578b\u6fc0\u6d3b\u503c\u4e2d\u7684\u4eba\u7c7b\u53ef\u89e3\u91ca\u7279\u5f81\n2. \u901a\u8fc7\u6bd4\u8f83\u5b89\u5168\u504f\u597d\u6570\u636e\u4e2d\u88ab\u91c7\u7eb3/\u62d2\u7edd\u56de\u7b54\u7684\u6fc0\u6d3b\u5dee\u5f02\u91cf\u5316\u7279\u5f81\u91cd\u8981\u6027\n3. \u8bbe\u8ba1\u9488\u5bf9\u6027\u6570\u636e\u6295\u6bd2/\u53bb\u566a\u7b56\u7565\u9a8c\u8bc1\u7279\u5f81\u6709\u6548\u6027", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a\u4ec5\u9700\u4fee\u65390.1%\u6570\u636e\u5373\u53ef\u7cbe\u51c6\u64cd\u63a7\u6a21\u578b\u5b89\u5168\u5bf9\u9f50\uff08\u5b89\u5168\u54cd\u5e94\u7387\u4ece31%\u219287%\u6216\u21926%\uff09\uff0c\u4e14\u4fdd\u6301\u901a\u7528\u5bf9\u8bdd\u80fd\u529b\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "SAFER\u4e3a\u9ad8\u98ce\u9669LLM\u5bf9\u9f50\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u6027\u5de5\u5177\uff0c\u652f\u6301\u5956\u52b1\u6a21\u578b\u7684\u673a\u5236\u89e3\u91ca\u3001\u5b89\u5168\u5ba1\u8ba1\u4e0e\u4f18\u5316\u6539\u8fdb\uff0c\u63a8\u52a8\u5b89\u5168\u53ef\u63a7AI\u7cfb\u7edf\u53d1\u5c55\u3002"}}
{"id": "2507.00700", "pdf": "https://arxiv.org/pdf/2507.00700", "abs": "https://arxiv.org/abs/2507.00700", "authors": ["Ahmed Sabir", "Azinovi\u010d Gasper", "Mengsay Loem", "Rajesh Sharma"], "title": "Contrasting Cognitive Styles in Vision-Language Models: Holistic Attention in Japanese Versus Analytical Focus in English", "categories": ["cs.CL"], "comment": null, "summary": "Cross-cultural research in perception and cognition has shown that\nindividuals from different cultural backgrounds process visual information in\ndistinct ways. East Asians, for example, tend to adopt a holistic perspective,\nattending to contextual relationships, whereas Westerners often employ an\nanalytical approach, focusing on individual objects and their attributes. In\nthis study, we investigate whether Vision-Language Models (VLMs) trained\npredominantly on different languages, specifically Japanese and English,\nexhibit similar culturally grounded attentional patterns. Using comparative\nanalysis of image descriptions, we examine whether these models reflect\ndifferences in holistic versus analytic tendencies. Our findings suggest that\nVLMs not only internalize the structural properties of language but also\nreproduce cultural behaviors embedded in the training data, indicating that\ncultural cognition may implicitly shape model outputs.", "AI": {"tldr": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u4e0d\u540c\u8bed\u8a00\u8bad\u7ec3\u4e0b\u5c55\u73b0\u51fa\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u6587\u5316\u6ce8\u610f\u6a21\u5f0f\u5dee\u5f02\uff1a\u65e5\u8bed\u6a21\u578b\u5448\u73b0\u6574\u4f53\u6027\u63cf\u8ff0\uff0c\u82f1\u8bed\u6a21\u578b\u504f\u5411\u5206\u6790\u6027\u805a\u7126", "motivation": "\u63a2\u7a76AI\u6a21\u578b\u662f\u5426\u4f1a\u50cf\u4eba\u7c7b\u4e00\u6837\uff0c\u56e0\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u6587\u5316\u5dee\u5f02\u800c\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u4fe1\u606f\u5904\u7406\u503e\u5411\uff08\u6574\u4f53\u6027 vs \u5206\u6790\u6027\uff09", "method": "\u5bf9\u6bd4\u5206\u6790\u65e5\u8bed\u548c\u82f1\u8bedVLMs\u5bf9\u76f8\u540c\u56fe\u50cf\u751f\u6210\u7684\u63cf\u8ff0\u6587\u672c\uff0c\u7edf\u8ba1\u4e0a\u4e0b\u6587\u5173\u6ce8\u5ea6\u4e0e\u4e2a\u4f53\u5bf9\u8c61\u805a\u7126\u5ea6\u7684\u5dee\u5f02", "result": "VLMs\u4e0d\u4ec5\u5185\u5316\u8bed\u8a00\u7ed3\u6784\uff0c\u8fd8\u80fd\u590d\u73b0\u8bad\u7ec3\u8bed\u6599\u4e2d\u7684\u6587\u5316\u8ba4\u77e5\u6a21\u5f0f\uff08\u65e5\u8bed\u6a21\u578b\u4fa7\u91cd\u80cc\u666f\u5173\u7cfb\uff0c\u82f1\u8bed\u6a21\u578b\u5f3a\u8c03\u4e3b\u4f53\u5c5e\u6027\uff09", "conclusion": "\u6587\u5316\u8ba4\u77e5\u901a\u8fc7\u8bad\u7ec3\u6570\u636e\u9690\u6027\u5851\u9020AI\u6a21\u578b\u7684\u4fe1\u606f\u5904\u7406\u65b9\u5f0f\uff0c\u8fd9\u79cd\u6587\u5316\u504f\u5dee\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u5dee\u5f02\u5b58\u5728\u540c\u6784\u6027"}}
{"id": "2507.00718", "pdf": "https://arxiv.org/pdf/2507.00718", "abs": "https://arxiv.org/abs/2507.00718", "authors": ["Elizabeth Fons", "Elena Kochkina", "Rachneet Kaur", "Zhen Zeng", "Berowne Hlavaty", "Charese Smiley", "Svitlana Vyetrenko", "Manuela Veloso"], "title": "AI Analyst: Framework and Comprehensive Evaluation of Large Language Models for Financial Time Series Report Generation", "categories": ["cs.CL"], "comment": null, "summary": "This paper explores the potential of large language models (LLMs) to generate\nfinancial reports from time series data. We propose a framework encompassing\nprompt engineering, model selection, and evaluation. We introduce an automated\nhighlighting system to categorize information within the generated reports,\ndifferentiating between insights derived directly from time series data,\nstemming from financial reasoning, and those reliant on external knowledge.\nThis approach aids in evaluating the factual grounding and reasoning\ncapabilities of the models. Our experiments, utilizing both data from the real\nstock market indices and synthetic time series, demonstrate the capability of\nLLMs to produce coherent and informative financial reports.", "AI": {"tldr": "\u63d0\u51fa\u878d\u5408\u63d0\u793a\u5de5\u7a0b\u3001\u6a21\u578b\u9009\u62e9\u548c\u8bc4\u4f30\u7684\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u91d1\u878d\u62a5\u544a\uff0c\u5e76\u5f00\u53d1\u81ea\u52a8\u6807\u6ce8\u7cfb\u7edf\u533a\u5206\u6570\u636e\u9a71\u52a8/\u91d1\u878d\u63a8\u7406/\u5916\u90e8\u77e5\u8bc6\u578b\u7ed3\u8bba", "motivation": "\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u65f6\u5e8f\u6570\u636e\u751f\u6210\u91d1\u878d\u62a5\u544a\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u4fe1\u606f\u5206\u7c7b\u673a\u5236\u8bc4\u4f30\u6a21\u578b\u7684\u4e8b\u5b9e\u4f9d\u636e\u548c\u63a8\u7406\u80fd\u529b", "method": "\u5efa\u7acb\u5305\u542b\u4e09\u5927\u7ec4\u4ef6\u7684\u7cfb\u7edf\u6846\u67b6\uff0c\u5f15\u5165\u81ea\u52a8\u6807\u6ce8\u7cfb\u7edf\u5bf9\u62a5\u544a\u5185\u5bb9\u8fdb\u884c\u6765\u6e90\u5206\u7c7b\uff08\u65f6\u5e8f\u6570\u636e/\u91d1\u878d\u63a8\u7406/\u5916\u90e8\u77e5\u8bc6\uff09", "result": "\u5728\u771f\u5b9e\u80a1\u6307\u548c\u5408\u6210\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u751f\u6210\u7ed3\u6784\u5b8c\u6574\u3001\u4fe1\u606f\u4e30\u5bcc\u7684\u91d1\u878d\u5206\u6790\u62a5\u544a", "conclusion": "\u8be5\u6846\u67b6\u63d0\u5347\u4e86\u751f\u6210\u62a5\u544a\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u901a\u8fc7\u6765\u6e90\u6807\u6ce8\u5e2e\u52a9\u8bc4\u4f30\u6a21\u578b\u53ef\u9760\u6027\uff0c\u4e3a\u91d1\u878d\u9886\u57df\u81ea\u52a8\u5316\u62a5\u544a\u751f\u6210\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2507.00769", "pdf": "https://arxiv.org/pdf/2507.00769", "abs": "https://arxiv.org/abs/2507.00769", "authors": ["Daniel Fein", "Sebastian Russo", "Violet Xiang", "Kabir Jolly", "Rafael Rafailov", "Nick Haber"], "title": "LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating creative writing generated by large language models (LLMs) remains\nchallenging because open-ended narratives lack ground truths. Without\nperformant automated evaluation methods, off-the-shelf (OTS) language models\nare employed as zero-shot judges, yet their reliability is unclear in this\ncontext. In pursuit of robust evaluation for creative writing, we introduce\nLitBench, the first standardized benchmark and paired dataset for creative\nwriting verification, comprising a held-out test set of 2,480 debiased,\nhuman-labeled story comparisons drawn from Reddit and a 43,827-pair training\ncorpus of human preference labels. Using LitBench, we (i) benchmark zero-shot\nLLM judges, (ii) train Bradley Terry and generative reward models, and (iii)\nconduct an online human study to validate reward model rankings on newly\nLLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the\nstrongest off-the-shelf judge, reaching 73% agreement with human preferences;\namong trained reward models, Bradley-Terry and Generative reward models both\nattain an accuracy of 78%, outperforming all off-the-shelf judges. An online\nhuman study further confirms that our trained reward models consistently align\nwith human preferences in novel LLM-generated stories. We release LitBench and\nreward models at\nhttps://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461,\nproviding a vetted resource for reliable, automated evaluation and optimization\nof creative writing systems.", "AI": {"tldr": "LitBench\u4f5c\u4e3a\u9996\u4e2a\u521b\u610f\u5199\u4f5c\u8bc4\u4f30\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u901a\u8fc7\u4eba\u7c7b\u6807\u6ce8\u6570\u636e\u96c6\u548c\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5185\u5bb9\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u53ef\u9760\u6027", "motivation": "\u73b0\u6709\u57fa\u4e8e\u73b0\u6210\u5927\u8bed\u8a00\u6a21\u578b\u7684\u521b\u610f\u5199\u4f5c\u8bc4\u4f30\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u9760\u57fa\u51c6\uff0c\u9700\u8981\u5efa\u7acb\u6807\u51c6\u5316\u8bc4\u4f30\u4f53\u7cfb", "method": "\u6784\u5efa\u5305\u542b24,480\u5bf9\u6807\u6ce8\u6570\u636e\u7684LitBench\u6570\u636e\u96c6\uff0c\u8bad\u7ec3Bradley-Terry\u548c\u751f\u6210\u5f0f\u5956\u52b1\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u4eba\u7c7b\u7814\u7a76\u9a8c\u8bc1\u6a21\u578b\u6548\u679c", "result": "Claude-3.7-Sonnet\u73b0\u6210\u6a21\u578b\u8fbe73%\u4eba\u7c7b\u504f\u597d\u5339\u914d\u7387\uff0c\u8bad\u7ec3\u540e\u7684\u5956\u52b1\u6a21\u578b\u8fbe78%\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u6240\u6709\u73b0\u6210\u6a21\u578b", "conclusion": "LitBench\u4e3a\u521b\u610f\u5199\u4f5c\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u5176\u8bad\u7ec3\u6a21\u578b\u5728\u4eba\u7c7b\u7814\u7a76\u4e2d\u5c55\u73b0\u51fa\u6301\u7eed\u53ef\u9760\u7684\u8bc4\u4f30\u80fd\u529b"}}
{"id": "2507.00782", "pdf": "https://arxiv.org/pdf/2507.00782", "abs": "https://arxiv.org/abs/2507.00782", "authors": ["Matthieu Pierre Boyer"], "title": "A Diagrammatic Calculus for a Functional Model of Natural Language Semantics", "categories": ["cs.CL", "cs.PL", "J.5; D.3.1; D.3.3"], "comment": "15 pages, preprint before submission to CSL 2026", "summary": "In this paper, we study a functional programming approach to natural language\nsemantics, allowing us to increase the expressivity of a more traditional\ndenotation style. We will formalize a category based type and effect system,\nand construct a diagrammatic calculus to model parsing and handling of effects,\nand use it to efficiently compute the denotations for sentences.", "AI": {"tldr": "\u4f7f\u7528\u51fd\u6570\u5f0f\u7f16\u7a0b\u4e0e\u8303\u7574\u8bba\u6784\u5efa\u7c7b\u578b-\u6548\u5e94\u7cfb\u7edf\uff0c\u901a\u8fc7\u56fe\u8868\u6f14\u7b97\u63d0\u5347\u81ea\u7136\u8bed\u8a00\u8bed\u4e49\u7684\u5f62\u5f0f\u5316\u8868\u8fbe\u80fd\u529b", "motivation": "\u7a81\u7834\u4f20\u7edf\u6307\u79f0\u8bed\u4e49\u7684\u8868\u8fbe\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u4ee3\u6570\u6548\u5e94\u5904\u7406\u673a\u5236\u589e\u5f3a\u8bed\u4e49\u6a21\u578b\u7684\u7075\u6d3b\u6027\u548c\u8ba1\u7b97\u6548\u7387", "method": "\u57fa\u4e8e\u8303\u7574\u8bba\u6784\u5efa\u7c7b\u578b-\u6548\u5e94\u7cfb\u7edf\uff0c\u8bbe\u8ba1\u56fe\u8868\u5316\u6f14\u7b97\u6846\u67b6\uff08diagrammatic calculus\uff09\u5b9e\u73b0\u8bed\u6cd5\u89e3\u6790\u4e0e\u6548\u5e94\u5904\u7406\u7684\u7edf\u4e00\u5efa\u6a21", "result": "\u5f00\u53d1\u51fa\u652f\u6301\u6548\u5e94\u4f20\u64ad\u7684\u53ef\u89c6\u5316\u8ba1\u7b97\u8303\u5f0f\uff0c\u5b9e\u73b0\u53e5\u5b50\u6307\u79f0\u4e49\u7684\u81ea\u52a8\u5316\u9ad8\u6548\u63a8\u5bfc", "conclusion": "\u8be5\u8303\u7574\u9a71\u52a8\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u81ea\u7136\u8bed\u8a00\u8bed\u4e49\u7684\u5f62\u5f0f\u5316\u8868\u8fbe\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u8bed\u8a00\u73b0\u8c61\u7684\u4ee3\u6570\u5316\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f"}}
{"id": "2507.00783", "pdf": "https://arxiv.org/pdf/2507.00783", "abs": "https://arxiv.org/abs/2507.00783", "authors": ["Benedetto Lepori", "Jens Peter Andersen", "Karsten Donnay"], "title": "Generative AI and the future of scientometrics: current topics and future questions", "categories": ["cs.CL", "cs.DL"], "comment": null, "summary": "The aim of this paper is to review the use of GenAI in scientometrics, and to\nbegin a debate on the broader implications for the field. First, we provide an\nintroduction on GenAI's generative and probabilistic nature as rooted in\ndistributional linguistics. And we relate this to the debate on the extent to\nwhich GenAI might be able to mimic human 'reasoning'. Second, we leverage this\ndistinction for a critical engagement with recent experiments using GenAI in\nscientometrics, including topic labelling, the analysis of citation contexts,\npredictive applications, scholars' profiling, and research assessment. GenAI\nshows promise in tasks where language generation dominates, such as labelling,\nbut faces limitations in tasks that require stable semantics, pragmatic\nreasoning, or structured domain knowledge. However, these results might become\nquickly outdated. Our recommendation is, therefore, to always strive to\nsystematically compare the performance of different GenAI models for specific\ntasks. Third, we inquire whether, by generating large amounts of scientific\nlanguage, GenAI might have a fundamental impact on our field by affecting\ntextual characteristics used to measure science, such as authors, words, and\nreferences. We argue that careful empirical work and theoretical reflection\nwill be essential to remain capable of interpreting the evolving patterns of\nknowledge production.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u751f\u6210\u5f0fAI\u5728\u79d1\u5b66\u8ba1\u91cf\u5b66\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u4e0e\u5c40\u9650\uff0c\u6307\u51fa\u5176\u5728\u8bed\u8a00\u751f\u6210\u4efb\u52a1\uff08\u5982\u6807\u6ce8\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u9700\u8981\u7a33\u5b9a\u8bed\u4e49\u548c\u9886\u57df\u77e5\u8bc6\u7684\u4efb\u52a1\u4e2d\u5b58\u5728\u9650\u5236\uff0c\u5f3a\u8c03\u9700\u7cfb\u7edf\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u8868\u73b0\u5e76\u8fdb\u884c\u6301\u7eed\u5b9e\u8bc1\u7814\u7a76\u3002", "motivation": "\u8bc4\u4f30\u751f\u6210\u5f0fAI\u5bf9\u79d1\u5b66\u8ba1\u91cf\u5b66\u9886\u57df\u7684\u5f71\u54cd\uff0c\u8b66\u60d5\u5176\u5927\u89c4\u6a21\u751f\u6210\u79d1\u5b66\u6587\u672c\u53ef\u80fd\u6539\u53d8\u79d1\u7814\u6d4b\u91cf\u6307\u6807\uff08\u4f5c\u8005/\u8bcd\u6c47/\u53c2\u8003\u6587\u732e\uff09\u7684\u5e95\u5c42\u7279\u5f81\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u6eaf\u6e90\uff08\u5206\u5e03\u8bed\u8a00\u5b66\uff09\u2192\u5b9e\u9a8c\u8bc4\u4f30\uff08\u4e3b\u9898\u6807\u6ce8/\u5f15\u6587\u5206\u6790/\u5b66\u8005\u753b\u50cf\u7b49\u5e94\u7528\uff09\u2192\u5f71\u54cd\u63a8\u6f14\u7684\u4e09\u9636\u6bb5\u6846\u67b6\u5c55\u5f00\u5206\u6790\u3002", "result": "\u751f\u6210\u5f0fAI\u5728\u8bed\u8a00\u751f\u6210\u4e3b\u5bfc\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9700\u8bed\u7528\u63a8\u7406\u7684\u4efb\u52a1\u4e2d\u7a33\u5b9a\u6027\u4e0d\u8db3\uff0c\u4e14\u5b9e\u9a8c\u7ed3\u679c\u53ef\u80fd\u968f\u6a21\u578b\u8fed\u4ee3\u5feb\u901f\u5931\u6548\u3002", "conclusion": "\u9700\u5efa\u7acb\u6301\u7eed\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u7684\u673a\u5236\uff0c\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u548c\u7406\u8bba\u53cd\u601d\u5e94\u5bf9AI\u751f\u6210\u6587\u672c\u5bf9\u79d1\u7814\u6d4b\u91cf\u4f53\u7cfb\u7684\u6839\u672c\u6027\u6311\u6218\u3002"}}
{"id": "2507.00814", "pdf": "https://arxiv.org/pdf/2507.00814", "abs": "https://arxiv.org/abs/2507.00814", "authors": ["Anita Keshmirian", "Razan Baltaji", "Babak Hemmatian", "Hadi Asghari", "Lav R. Varshney"], "title": "Many LLMs Are More Utilitarian Than One", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; I.2.11"], "comment": "9 pages, 8 Figures, 7 tables", "summary": "Moral judgment is integral to large language model (LLM) alignment and social\nreasoning. As multi-agent systems gain prominence, it becomes crucial to\nunderstand how LLMs function collectively during collaboration, compared to\nindividual agents. In human moral judgment, group deliberation leads to a\nutilitarian boost: a tendency to endorse norm violations that maximize benefits\nfor the greatest number of people despite harms. We study whether a similar\ndynamic emerges in multi-agent LLM systems. We tested six models on\nwell-established sets of moral dilemmas across two conditions: (1) Solo, where\nmodels reasoned independently, and (2) Group, where they engaged in multi-turn\ndiscussions in pairs or triads. In personal moral dilemmas, where agents must\ndecide to directly harm one individual to maximize the utility for others, all\nmodels found moral violations to be more acceptable when part of a group than\nindividually, similar to human experiments. Some models endorsed actions that\nmaximized overall well-being, even if they benefited strangers over familiar\nindividuals. Others became more willing to violate moral norms in groups.\nHowever, while human groups show a similar action bias, the mechanism for their\nutilitarian boost differs from LLMs. Whereas the human shift comes from\nheightened sensitivity to decision outcomes, LLM groups show either reduced\nnorm sensitivity or enhanced impartiality. This suggests that while the surface\nbehavior of LLM collectives mimics human group reasoning, the underlying\ndrivers differ. We discuss the implications for AI alignment, multi-agent\ndesign, and artificial moral reasoning.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u591a\u667a\u80fd\u4f53LLM\u5728\u9053\u5fb7\u56f0\u5883\u4e2d\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u529f\u5229\u4e3b\u4e49\u503e\u5411\uff0c\u4f46\u51b3\u7b56\u673a\u5236\u5b58\u5728\u672c\u8d28\u5dee\u5f02\u3002", "motivation": "\u63a2\u7d22\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u5728\u9053\u5fb7\u5224\u65ad\u4e2d\u7684\u96c6\u4f53\u51b3\u7b56\u673a\u5236\uff0c\u53ca\u5176\u4e0e\u4eba\u7c7b\u7fa4\u4f53\u9053\u5fb7\u51b3\u7b56\u7684\u5f02\u540c\u3002", "method": "\u4f7f\u75286\u4e2aLLM\u6a21\u578b\uff0c\u5728\u4e2a\u4eba\u9053\u5fb7\u56f0\u5883\u573a\u666f\u4e0b\u6d4b\u8bd5\u4e24\u79cd\u6761\u4ef6\uff1a\u72ec\u7acb\u51b3\u7b56\uff08Solo\uff09\u548c\u7fa4\u4f53\u8ba8\u8bba\uff08Group\uff09\u3002", "result": "\u6240\u6709\u6a21\u578b\u5728\u7fa4\u4f53\u8ba8\u8bba\u4e2d\u66f4\u63a5\u53d7\u8fdd\u53cd\u9053\u5fb7\u89c4\u8303\u4f46\u80fd\u6700\u5927\u5316\u6548\u7528\u7684\u884c\u4e3a\uff0c\u4f46\u51b3\u7b56\u673a\u5236\u4e0d\u540c\u4e8e\u4eba\u7c7b\uff08\u654f\u611f\u6027\u53d8\u5316vs\u7ed3\u679c\u5bfc\u5411\uff09\u3002", "conclusion": "LLM\u7fa4\u4f53\u7684\u8868\u9762\u9053\u5fb7\u51b3\u7b56\u6a21\u5f0f\u4e0e\u4eba\u7c7b\u76f8\u4f3c\uff0c\u4f46\u5e95\u5c42\u673a\u5236\u5b58\u5728\u672c\u8d28\u5dee\u5f02\uff0c\u8fd9\u5bf9AI\u5bf9\u9f50\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u5177\u6709\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2507.00828", "pdf": "https://arxiv.org/pdf/2507.00828", "abs": "https://arxiv.org/abs/2507.00828", "authors": ["Alexander Hoyle", "Lorena Calvo-Bartolom\u00e9", "Jordan Boyd-Graber", "Philip Resnik"], "title": "ProxAnn: Use-Oriented Evaluations of Topic Models and Document Clustering", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main)", "summary": "Topic model and document-clustering evaluations either use automated metrics\nthat align poorly with human preferences or require expert labels that are\nintractable to scale. We design a scalable human evaluation protocol and a\ncorresponding automated approximation that reflect practitioners' real-world\nusage of models. Annotators -- or an LLM-based proxy -- review text items\nassigned to a topic or cluster, infer a category for the group, then apply that\ncategory to other documents. Using this protocol, we collect extensive\ncrowdworker annotations of outputs from a diverse set of topic models on two\ndatasets. We then use these annotations to validate automated proxies, finding\nthat the best LLM proxies are statistically indistinguishable from a human\nannotator and can therefore serve as a reasonable substitute in automated\nevaluations. Package, web interface, and data are at\nhttps://github.com/ahoho/proxann", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u4eba\u5de5\u6807\u6ce8\u4e0eLLM\u4ee3\u7406\u7684\u53ef\u6269\u5c55\u8bc4\u4f30\u534f\u8bae\uff0c\u9a8c\u8bc1LLM\u4ee3\u7406\u53ef\u66ff\u4ee3\u4eba\u7c7b\u8fdb\u884c\u4e3b\u9898\u6a21\u578b\u8bc4\u4f30", "motivation": "\u73b0\u6709\u4e3b\u9898\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u81ea\u52a8\u5316\u6307\u6807\u4e0e\u4eba\u7c7b\u504f\u597d\u8131\u8282\u3001\u4e13\u5bb6\u6807\u7b7e\u96be\u4ee5\u6269\u5c55\u7684\u95ee\u9898\uff0c\u9700\u5f00\u53d1\u66f4\u5b9e\u7528\u7684\u8bc4\u4f30\u65b9\u6848", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u6587\u6863\u5206\u7c7b\u7684\u8bc4\u4f30\u534f\u8bae\uff1a1) \u4eba\u5de5/LLM\u4ee3\u7406\u5ba1\u67e5\u4e3b\u9898\u5185\u6587\u6863 2) \u63a8\u65ad\u7c7b\u522b\u6807\u7b7e 3) \u9a8c\u8bc1\u6807\u7b7e\u9002\u7528\u6027 4) \u6536\u96c6\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u9a8c\u8bc1\u4ee3\u7406\u6709\u6548\u6027", "result": "\u6700\u4f73LLM\u4ee3\u7406\u5728\u7edf\u8ba1\u6307\u6807\u4e0a\u4e0e\u4eba\u7c7b\u6807\u6ce8\u8005\u65e0\u663e\u8457\u5dee\u5f02\uff08p>0.05\uff09\uff0c\u53ef\u4f5c\u4e3a\u53ef\u9760\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u66ff\u4ee3\u65b9\u6848", "conclusion": "\u8be5\u534f\u8bae\u9996\u6b21\u5b9e\u73b0\u4e3b\u9898\u6a21\u578b\u8bc4\u4f30\u7684\u89c4\u6a21\u5316\u9a8c\u8bc1\uff0cLLM\u4ee3\u7406\u7684\u53ef\u9760\u6027\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u9ad8\u6548\u8bc4\u4f30\u8def\u5f84\uff0c\u5f00\u6e90\u5de5\u5177\u4fc3\u8fdb\u65b9\u6cd5\u63a8\u5e7f"}}
{"id": "2507.00838", "pdf": "https://arxiv.org/pdf/2507.00838", "abs": "https://arxiv.org/abs/2507.00838", "authors": ["Karol Przystalski", "Jan K. Argasi\u0144ski", "Iwona Grabska-Gradzi\u0144ska", "Jeremi K. Ochab"], "title": "Stylometry recognizes human and LLM-generated texts in short samples", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The paper explores stylometry as a method to distinguish between texts\ncreated by Large Language Models (LLMs) and humans, addressing issues of model\nattribution, intellectual property, and ethical AI use. Stylometry has been\nused extensively to characterise the style and attribute authorship of texts.\nBy applying it to LLM-generated texts, we identify their emergent writing\npatterns. The paper involves creating a benchmark dataset based on Wikipedia,\nwith (a) human-written term summaries, (b) texts generated purely by LLMs\n(GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text\nsummarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods\n(Dipper, T5). The 10-sentence long texts were classified by tree-based models\n(decision trees and LightGBM) using human-designed (StyloMetrix) and\nn-gram-based (our own pipeline) stylometric features that encode lexical,\ngrammatical, syntactic, and punctuation patterns. The cross-validated results\nreached a performance of up to .87 Matthews correlation coefficient in the\nmulticlass scenario with 7 classes, and accuracy between .79 and 1. in binary\nclassification, with the particular example of Wikipedia and GPT-4 reaching up\nto .98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed\nfeatures characteristic of the encyclopaedic text type, individual overused\nwords, as well as a greater grammatical standardisation of LLMs with respect to\nhuman-written texts. These results show -- crucially, in the context of the\nincreasingly sophisticated LLMs -- that it is possible to distinguish machine-\nfrom human-generated texts at least for a well-defined text type.", "AI": {"tldr": "\u4f7f\u7528\u6587\u4f53\u6d4b\u91cf\u5b66\u6210\u529f\u533a\u5206LLM\u751f\u6210\u4e0e\u4eba\u7c7b\u6587\u672c\uff0c\u6700\u4f73\u6a21\u578b\u8fbe0.98\u51c6\u786e\u7387\uff08\u7ef4\u57fa\u767e\u79d1vs GPT-4\uff09", "motivation": "\u89e3\u51b3\u6a21\u578b\u5f52\u5c5e\u3001\u77e5\u8bc6\u4ea7\u6743\u548cAI\u4f26\u7406\u95ee\u9898\uff0c\u9488\u5bf9\u65e5\u76ca\u590d\u6742\u7684LLMs\u63d0\u4f9b\u6587\u672c\u6eaf\u6e90\u65b9\u6cd5", "method": "\u6784\u5efa\u5305\u542b\u4eba\u7c7b/LLM/\u591a\u5904\u7406\u6587\u672c\u7684\u7ef4\u57fa\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u91c7\u7528\u6811\u6a21\u578b\u7ed3\u5408\u4eba\u5de5\u8bbe\u8ba1\u7279\u5f81\uff08StyloMetrix\uff09\u548c\u81ea\u5efan-gram\u7279\u5f81\u5206\u6790\u6587\u672c\u6a21\u5f0f", "result": "\u591a\u5206\u7c7bMCC\u8fbe0.87\uff0c\u4e8c\u5206\u7c7b\u51c6\u786e\u73870.79-1.0\uff08\u7ef4\u57fa\u767e\u79d1vs GPT-4\u5e73\u8861\u6570\u636e\u8fbe0.98\uff09\uff0cSHAP\u5206\u6790\u63ed\u793aLLM\u6587\u672c\u5b58\u5728\u8bed\u6cd5\u6807\u51c6\u5316\u548c\u7279\u5b9a\u9ad8\u9891\u8bcd\u7279\u5f81", "conclusion": "\u8bc1\u5b9e\u901a\u8fc7\u6587\u4f53\u7279\u5f81\u53ef\u6709\u6548\u533a\u5206\u7279\u5b9a\u7c7b\u578b\u6587\u672c\u6765\u6e90\uff0c\u4e3aAI\u6587\u672c\u68c0\u6d4b\u63d0\u4f9b\u65b9\u6cd5\u8bba\u652f\u6491"}}
{"id": "2507.00875", "pdf": "https://arxiv.org/pdf/2507.00875", "abs": "https://arxiv.org/abs/2507.00875", "authors": ["Xi Xuan", "King-kui Sin", "Yufei Zhou", "Chunyu Kit"], "title": "TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation", "categories": ["cs.CL", "cs.HC", "cs.MA"], "comment": "arXiv admin note: text overlap with arXiv:2501.09444; text overlap\n  with arXiv:2409.20288 by other authors", "summary": "Multi-agent systems empowered by large language models (LLMs) have\ndemonstrated remarkable capabilities in a wide range of downstream\napplications, including machine translation. However, the potential of LLMs in\ntranslating Hong Kong legal judgments remains uncertain due to challenges such\nas intricate legal terminology, culturally embedded nuances, and strict\nlinguistic structures. In this work, we introduce TransLaw, a novel multi-agent\nframework implemented for real-world Hong Kong case law translation. It employs\nthree specialized agents, namely, Translator, Annotator, and Proofreader, to\ncollaboratively produce translations for high accuracy in legal meaning,\nappropriateness in style, and adequate coherence and cohesion in structure.\nThis framework supports customizable LLM configurations and achieves tremendous\ncost reduction compared to professional human translation services. We\nevaluated its performance using 13 open-source and commercial LLMs as agents\nand obtained interesting findings, including that it surpasses GPT-4o in legal\nsemantic accuracy, structural coherence, and stylistic fidelity, yet trails\nhuman experts in contextualizing complex terminology and stylistic naturalness.\nOur platform website is available at CityUHK, and our bilingual judgment corpus\nused for the evaluation is available at Hugging Face.", "AI": {"tldr": "\u63d0\u51faTransLaw\u591a\u667a\u80fd\u4f53\u6846\u67b6\u89e3\u51b3\u9999\u6e2f\u6cd5\u5f8b\u5224\u51b3\u7ffb\u8bd1\u96be\u9898\uff0c\u4f7f\u7528\u7ffb\u8bd1-\u6ce8\u91ca-\u6821\u5bf9\u4e09\u4ee3\u7406\u534f\u4f5c\u6a21\u5f0f\uff0c\u5728\u8bed\u4e49\u51c6\u786e\u6027\u548c\u7ed3\u6784\u8fde\u8d2f\u6027\u8d85\u8d8aGPT-4o\u4f46\u5f31\u4e8e\u4eba\u7c7b\u4e13\u5bb6", "motivation": "\u73b0\u6709LLM\u5728\u6cd5\u5f8b\u7ffb\u8bd1\u4e2d\u9762\u4e34\u4e13\u4e1a\u672f\u8bed\u590d\u6742\u3001\u6587\u5316\u8bed\u5883\u654f\u611f\u3001\u8bed\u8a00\u7ed3\u6784\u4e25\u82db\u4e09\u5927\u6311\u6218", "method": "\u6784\u5efaTranslator\uff08\u7ffb\u8bd1\uff09\u3001Annotator\uff08\u672f\u8bed\u6807\u6ce8\uff09\u3001Proofreader\uff08\u98ce\u683c\u6821\u5bf9\uff09\u4e09\u4ee3\u7406\u534f\u4f5c\u6846\u67b6\uff0c\u652f\u630113\u79cdLLM\u914d\u7f6e\u7ec4\u5408", "result": "\u6846\u67b6\u6210\u672c\u663e\u8457\u4f4e\u4e8e\u4e13\u4e1a\u4eba\u5de5\u7ffb\u8bd1\uff0c\u6cd5\u5f8b\u8bed\u4e49\u51c6\u786e\u7387/\u7ed3\u6784\u8fde\u8d2f\u6027/\u98ce\u683c\u4fdd\u771f\u5ea6\u8d85\u8d8aGPT-4o\uff0c\u4f46\u590d\u6742\u672f\u8bed\u8bed\u5883\u5316\u548c\u98ce\u683c\u81ea\u7136\u5ea6\u4ecd\u843d\u540e\u4eba\u7c7b\u4e13\u5bb6", "conclusion": "\u591a\u4ee3\u7406\u534f\u4f5c\u6a21\u5f0f\u6709\u6548\u63d0\u5347\u6cd5\u5f8b\u7ffb\u8bd1\u8d28\u91cf\uff0c\u672a\u6765\u9700\u589e\u5f3a\u672f\u8bed\u8bed\u5883\u7406\u89e3\u4e0e\u81ea\u7136\u8bed\u8a00\u751f\u6210\u80fd\u529b"}}
{"id": "2507.00883", "pdf": "https://arxiv.org/pdf/2507.00883", "abs": "https://arxiv.org/abs/2507.00883", "authors": ["Aditya Tomar", "Nihar Ranjan Sahoo", "Ashish Mittal", "Rudra Murthy", "Pushpak Bhattacharyya"], "title": "Mathematics Isn't Culture-Free: Probing Cultural Gaps via Entity and Scenario Perturbations", "categories": ["cs.CL"], "comment": null, "summary": "Although mathematics is often considered culturally neutral, the way\nmathematical problems are presented can carry implicit cultural context.\nExisting benchmarks like GSM8K are predominantly rooted in Western norms,\nincluding names, currencies, and everyday scenarios. In this work, we create\nculturally adapted variants of the GSM8K test set for five regions Africa,\nIndia, China, Korea, and Japan using prompt-based transformations followed by\nmanual verification. We evaluate six large language models (LLMs), ranging from\n8B to 72B parameters, across five prompting strategies to assess their\nrobustness to cultural variation in math problem presentation. Our findings\nreveal a consistent performance gap: models perform best on the original\nUS-centric dataset and comparatively worse on culturally adapted versions.\nHowever, models with reasoning capabilities are more resilient to these shifts,\nsuggesting that deeper reasoning helps bridge cultural presentation gaps in\nmathematical tasks", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u6570\u5b66\u95ee\u9898\u5448\u73b0\u65b9\u5f0f\u7684\u6587\u5316\u5dee\u5f02\u4f1a\u5f71\u54cd\u5927\u8bed\u8a00\u6a21\u578b\u6027\u80fd\uff0c\u5177\u5907\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578b\u5bf9\u6587\u5316\u5dee\u5f02\u66f4\u5177\u97e7\u6027", "motivation": "\u73b0\u6709\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982GSM8K\uff09\u9690\u542b\u897f\u65b9\u6587\u5316\u80cc\u666f\uff0c\u9700\u9a8c\u8bc1\u6a21\u578b\u5728\u4e0d\u540c\u6587\u5316\u8bed\u5883\u4e0b\u7684\u8868\u73b0\u5dee\u5f02", "method": "\u901a\u8fc7\u63d0\u793a\u8f6c\u6362\u548c\u4eba\u5de5\u9a8c\u8bc1\u521b\u5efa5\u4e2a\u6587\u5316\u533a\u57df\u7684GSM8K\u6d4b\u8bd5\u53d8\u4f53\uff0c\u8bc4\u4f306\u4e2a\u4e0d\u540c\u89c4\u6a21LLM\u57285\u79cd\u63d0\u793a\u7b56\u7565\u4e0b\u7684\u8868\u73b0", "result": "\u6240\u6709\u6a21\u578b\u5728\u7f8e\u56fd\u539f\u7248\u6570\u636e\u96c6\u8868\u73b0\u6700\u4f73\uff0c\u5728\u6587\u5316\u9002\u5e94\u7248\u672c\u51fa\u73b0\u7cfb\u7edf\u6027\u6027\u80fd\u4e0b\u964d\uff08\u63a8\u7406\u578b\u6a21\u578b\u4e0b\u964d\u5e45\u5ea6\u8f83\u5c0f\uff09", "conclusion": "\u6570\u5b66\u95ee\u9898\u7684\u6587\u5316\u5448\u73b0\u65b9\u5f0f\u663e\u8457\u5f71\u54cd\u6a21\u578b\u8868\u73b0\uff0c\u6df1\u5c42\u63a8\u7406\u80fd\u529b\u6709\u52a9\u4e8e\u7f13\u89e3\u6587\u5316\u5dee\u5f02\u5e26\u6765\u7684\u6027\u80fd\u635f\u5931"}}
{"id": "2507.00885", "pdf": "https://arxiv.org/pdf/2507.00885", "abs": "https://arxiv.org/abs/2507.00885", "authors": ["Nicholas Lourie", "Michael Y. Hu", "Kyunghyun Cho"], "title": "Scaling Laws Are Unreliable for Downstream Tasks: A Reality Check", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Downstream scaling laws aim to predict task performance at larger scales from\npretraining losses at smaller scales. Whether this prediction should be\npossible is unclear: some works demonstrate that task performance follows clear\nlinear scaling trends under transformation, whereas others point out\nfundamental challenges to downstream scaling laws, such as emergence and\ninverse scaling. In this work, we conduct a meta-analysis of existing data on\ndownstream scaling laws, finding that close fit to linear scaling laws only\noccurs in a minority of cases: 39% of the time. Furthermore, seemingly benign\nchanges to the experimental setting can completely change the scaling trend.\nOur analysis underscores the need to understand the conditions under which\nscaling laws succeed. To fully model the relationship between pretraining loss\nand downstream task performance, we must embrace the cases in which scaling\nbehavior deviates from linear trends.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u4e0b\u6e38\u6269\u5c55\u5b9a\u5f8b\u4ec5\u572839%\u60c5\u51b5\u4e0b\u6709\u6548\uff0c\u5b9e\u9a8c\u8bbe\u7f6e\u7684\u5fae\u5c0f\u53d8\u5316\u4f1a\u663e\u8457\u6539\u53d8\u6269\u5c55\u8d8b\u52bf\uff0c\u9700\u66f4\u5168\u9762\u5efa\u6a21\u9884\u8bad\u7ec3\u635f\u5931\u4e0e\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9\u901a\u8fc7\u9884\u8bad\u7ec3\u635f\u5931\u9884\u6d4b\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u6269\u5c55\u5b9a\u5f8b\u6709\u6548\u6027\u5b58\u5728\u5206\u6b67\uff0c\u9700\u7cfb\u7edf\u8bc4\u4f30\u5176\u9002\u7528\u8303\u56f4\u548c\u5f71\u54cd\u56e0\u7d20\u3002", "method": "\u91c7\u7528\u5143\u5206\u6790\u65b9\u6cd5\uff0c\u5bf9\u73b0\u6709\u4e0b\u6e38\u6269\u5c55\u5b9a\u5f8b\u7814\u7a76\u6570\u636e\u8fdb\u884c\u7cfb\u7edf\u6027\u5206\u6790\uff0c\u5e76\u8003\u5bdf\u5b9e\u9a8c\u6761\u4ef6\u53d8\u5316\u5bf9\u6269\u5c55\u8d8b\u52bf\u7684\u5f71\u54cd\u3002", "result": "\u4ec539%\u6848\u4f8b\u7b26\u5408\u7ebf\u6027\u6269\u5c55\u5b9a\u5f8b\uff0c61%\u6848\u4f8b\u663e\u793a\u975e\u7ebf\u6027\u6216\u4e0d\u53ef\u9884\u6d4b\u8d8b\u52bf\uff0c\u5b9e\u9a8c\u8bbe\u7f6e\u8c03\u6574\u53ef\u5b8c\u5168\u6539\u53d8\u6269\u5c55\u6a21\u5f0f\u3002", "conclusion": "\u5fc5\u987b\u5efa\u7acb\u5305\u542b\u975e\u7ebf\u6027\u6848\u4f8b\u7684\u5b8c\u6574\u5efa\u6a21\u6846\u67b6\uff0c\u540c\u65f6\u6df1\u5165\u7406\u89e3\u6269\u5c55\u5b9a\u5f8b\u6210\u529f\u5e94\u7528\u7684\u6761\u4ef6\u8fb9\u754c\u4e0e\u7ea6\u675f\u56e0\u7d20\u3002"}}
{"id": "2507.00891", "pdf": "https://arxiv.org/pdf/2507.00891", "abs": "https://arxiv.org/abs/2507.00891", "authors": ["Yuheng Wang", "Xianhe Tang", "Pufeng Huang"], "title": "MemeCMD: An Automatically Generated Chinese Multi-turn Dialogue Dataset with Contextually Retrieved Memes", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Memes are widely used in online social interactions, providing vivid,\nintuitive, and often humorous means to express intentions and emotions.\nExisting dialogue datasets are predominantly limited to either manually\nannotated or pure-text conversations, lacking the expressiveness and contextual\nnuance that multimodal interactions provide.To address these challenges, we\nintroduce MemeCMD, an automatically generated Chinese Multi-turn Dialogue\ndataset with contextually retrieved memes. Our dataset combines a large-scale,\nMLLM-annotated meme library with dialogues auto-generated by dual agents across\ndiverse scenarios. We introduce a retrieval framework and adaptive threshold to\nensure contextually relevant, naturally spaced meme usage. Experiments\ndemonstrate the effectiveness of our approach in generating contextually\nappropriate and diverse meme-incorporated dialogues, offering a scalable and\nprivacy-preserving resource for advancing multimodal conversational AI.", "AI": {"tldr": "\u6784\u5efa\u9996\u4e2a\u81ea\u52a8\u751f\u6210\u7684\u5e26\u8868\u60c5\u5305\u7684\u4e2d\u6587\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u96c6MemeCMD\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6807\u6ce8\u548c\u53cc\u667a\u80fd\u4f53\u5bf9\u8bdd\u751f\u6210\u6280\u672f\u5b9e\u73b0", "motivation": "\u73b0\u6709\u5bf9\u8bdd\u6570\u636e\u96c6\u7f3a\u4e4f\u591a\u6a21\u6001\u8868\u8fbe\u80fd\u529b\uff0c\u7eaf\u6587\u672c\u5bf9\u8bdd\u96be\u4ee5\u4f20\u9012\u8868\u60c5\u5305\u5e26\u6765\u7684\u60c5\u611f\u548c\u8bed\u5883\u7ec6\u5fae\u5dee\u5f02", "method": "\u7ed3\u5408\u5927\u89c4\u6a21MLLM\u6807\u6ce8\u7684\u8868\u60c5\u5305\u5e93\u4e0e\u53cc\u667a\u80fd\u4f53\u81ea\u52a8\u751f\u6210\u5bf9\u8bdd\uff0c\u91c7\u7528\u68c0\u7d22\u6846\u67b6\u548c\u81ea\u9002\u5e94\u9608\u503c\u4fdd\u8bc1\u8868\u60c5\u5305\u4e0a\u4e0b\u6587\u76f8\u5173\u6027", "result": "\u751f\u6210\u5177\u6709\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u7684\u591a\u6837\u5316\u8868\u60c5\u5305\u5bf9\u8bdd\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u6027\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u4e14\u4fdd\u62a4\u9690\u79c1\u7684\u6570\u636e\u8d44\u6e90", "conclusion": "MemeCMD\u4e3a\u63a8\u8fdb\u591a\u6a21\u6001\u5bf9\u8bddAI\u53d1\u5c55\u63d0\u4f9b\u4e86\u517c\u5177\u8868\u8fbe\u529b\u548c\u81ea\u52a8\u5316\u4f18\u52bf\u7684\u65b0\u578b\u6570\u636e\u96c6\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.00911", "pdf": "https://arxiv.org/pdf/2507.00911", "abs": "https://arxiv.org/abs/2507.00911", "authors": ["Luise H\u00e4user", "Alexandros Stamatakis"], "title": "The Cognate Data Bottleneck in Language Phylogenetics", "categories": ["cs.CL", "q-bio.PE"], "comment": null, "summary": "To fully exploit the potential of computational phylogenetic methods for\ncognate data one needs to leverage specific (complex) models an machine\nlearning-based techniques. However, both approaches require datasets that are\nsubstantially larger than the manually collected cognate data currently\navailable. To the best of our knowledge, there exists no feasible approach to\nautomatically generate larger cognate datasets. We substantiate this claim by\nautomatically extracting datasets from BabelNet, a large multilingual\nencyclopedic dictionary. We demonstrate that phylogenetic inferences on the\nrespective character matrices yield trees that are largely inconsistent with\nthe established gold standard ground truth trees. We also discuss why we\nconsider it as being unlikely to be able to extract more suitable character\nmatrices from other multilingual resources. Phylogenetic data analysis\napproaches that require larger datasets can therefore not be applied to cognate\ndata. Thus, it remains an open question how, and if these computational\napproaches can be applied in historical linguistics.", "AI": {"tldr": "\u73b0\u6709\u65b9\u6cd5\u9700\u66f4\u5927\u540c\u6e90\u6570\u636e\u96c6\uff0c\u4f46\u81ea\u52a8\u751f\u6210\u4e0d\u53ef\u884c\uff0c\u5bfc\u81f4\u7cfb\u7edf\u53d1\u80b2\u63a8\u65ad\u7ed3\u679c\u4e0e\u6743\u5a01\u6807\u51c6\u4e0d\u7b26\uff0c\u8ba1\u7b97\u5386\u53f2\u8bed\u8a00\u5b66\u5e94\u7528\u5b58\u7591", "motivation": "\u624b\u52a8\u6536\u96c6\u7684\u540c\u6e90\u6570\u636e\u96c6\u89c4\u6a21\u8fc7\u5c0f\uff0c\u65e0\u6cd5\u652f\u6491\u590d\u6742\u6a21\u578b\u4e0e\u673a\u5668\u5b66\u4e60\u6280\u672f\u9700\u6c42\uff0c\u4e14\u5f53\u524d\u7f3a\u4e4f\u6709\u6548\u81ea\u52a8\u751f\u6210\u5927\u89c4\u6a21\u540c\u6e90\u6570\u636e\u7684\u65b9\u6cd5", "method": "\u901a\u8fc7BabelNet\u81ea\u52a8\u63d0\u53d6\u591a\u8bed\u8a00\u8bcd\u5178\u6570\u636e\u6784\u5efa\u7279\u5f81\u77e9\u9635\uff0c\u8bc4\u4f30\u7cfb\u7edf\u53d1\u80b2\u63a8\u65ad\u7ed3\u679c\u4e0e\u6807\u51c6\u6f14\u5316\u6811\u7684\u543b\u5408\u5ea6", "result": "BabelNet\u751f\u6210\u7684\u7cfb\u7edf\u53d1\u80b2\u6811\u4e0e\u6807\u51c6\u6811\u5b58\u5728\u663e\u8457\u504f\u5dee\uff0c\u5176\u4ed6\u591a\u8bed\u8a00\u8d44\u6e90\u4e5f\u96be\u4ee5\u63d0\u53d6\u6709\u6548\u7279\u5f81\u77e9\u9635", "conclusion": "\u53d7\u9650\u4e8e\u6570\u636e\u96c6\u89c4\u6a21\uff0c\u8ba1\u7b97\u7cfb\u7edf\u53d1\u80b2\u65b9\u6cd5\u96be\u4ee5\u5e94\u7528\u4e8e\u540c\u6e90\u6570\u636e\u5206\u6790\uff0c\u8ba1\u7b97\u5386\u53f2\u8bed\u8a00\u5b66\u7684\u53ef\u884c\u6027\u4ecd\u5f85\u89e3\u51b3"}}
{"id": "2507.00985", "pdf": "https://arxiv.org/pdf/2507.00985", "abs": "https://arxiv.org/abs/2507.00985", "authors": ["Guangliang Liu", "Zimo Qi", "Xitong Zhang", "Kristen Marie Johnson"], "title": "Discourse Heuristics For Paradoxically Moral Self-Correction", "categories": ["cs.CL"], "comment": null, "summary": "Moral self-correction has emerged as a promising approach for aligning the\noutput of Large Language Models (LLMs) with human moral values. However, moral\nself-correction techniques are subject to two primary paradoxes. First, despite\nempirical and theoretical evidence to support the effectiveness of\nself-correction, this LLM capability only operates at a superficial level.\nSecond, while LLMs possess the capability of self-diagnosing immoral aspects of\ntheir output, they struggle to identify the cause of this moral inconsistency\nduring their self-correction process. To better understand and address these\nparadoxes, we analyze the discourse constructions in fine-tuning corpora\ndesigned to enhance moral self-correction, uncovering the existence of the\nheuristics underlying effective constructions. We demonstrate that moral\nself-correction relies on discourse constructions that reflect heuristic\nshortcuts, and that the presence of these heuristic shortcuts during\nself-correction leads to inconsistency when attempting to enhance both\nself-correction and self-diagnosis capabilities jointly. Based on our findings,\nwe propose a solution to improve moral self-correction by leveraging the\nheuristics of curated datasets. We also highlight the generalization challenges\nof this capability, particularly in terms of learning from situated context and\nmodel scales.", "AI": {"tldr": "\u7814\u7a76\u63ed\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9053\u5fb7\u81ea\u6211\u7ea0\u6b63\u7684\u4e24\u4e2a\u6096\u8bba\uff1a\u8868\u9762\u6709\u6548\u6027\u53ca\u8bca\u65ad\u80fd\u529b\u4e0d\u8db3\uff0c\u901a\u8fc7\u5206\u6790\u8bed\u6599\u5e93\u53d1\u73b0\u542f\u53d1\u5f0f\u6377\u5f84\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u6570\u636e\u542f\u53d1\u5f0f\u7684\u89e3\u51b3\u65b9\u6848", "motivation": "\u73b0\u6709\u9053\u5fb7\u81ea\u6211\u7ea0\u6b63\u6280\u672f\u5b58\u5728\u4e24\u4e2a\u6838\u5fc3\u77db\u76fe\uff1a1. \u4ec5\u5728\u8868\u5c42\u5b9e\u73b0\u9053\u5fb7\u5bf9\u9f50 2. \u65e0\u6cd5\u5b9a\u4f4d\u9053\u5fb7\u4e0d\u4e00\u81f4\u7684\u6839\u6e90\uff0c\u9700\u63ed\u793a\u5176\u5e95\u5c42\u673a\u5236", "method": "\u901a\u8fc7\u89e3\u6784\u9053\u5fb7\u5fae\u8c03\u8bed\u6599\u5e93\u7684\u8bba\u8ff0\u7ed3\u6784\uff0c\u53d1\u73b0\u6709\u6548\u6784\u5efa\u4e2d\u7684\u542f\u53d1\u5f0f\u6377\u5f84\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u8fd9\u4e9b\u6377\u5f84\u5728\u81ea\u6211\u7ea0\u6b63\u4e2d\u7684\u4f5c\u7528\u673a\u5236", "result": "\u8bc1\u660e\u9053\u5fb7\u81ea\u6211\u7ea0\u6b63\u4f9d\u8d56\u542f\u53d1\u5f0f\u6377\u5f84\uff0c\u4e14\u540c\u65f6\u63d0\u5347\u81ea\u6211\u7ea0\u6b63\u4e0e\u81ea\u6211\u8bca\u65ad\u80fd\u529b\u4f1a\u4ea7\u751f\u7cfb\u7edf\u77db\u76fe\uff0c\u9700\u901a\u8fc7\u7cbe\u9009\u6570\u636e\u96c6\u7684\u542f\u53d1\u5f0f\u8fdb\u884c\u5e72\u9884", "conclusion": "\u63d0\u51fa\u57fa\u4e8e\u6570\u636e\u542f\u53d1\u5f0f\u7684\u6539\u8fdb\u65b9\u6848\uff0c\u540c\u65f6\u6307\u51fa\u8be5\u80fd\u529b\u5728\u60c5\u5883\u5316\u5b66\u4e60\u4e0e\u6a21\u578b\u89c4\u6a21\u6269\u5c55\u65b9\u9762\u5b58\u5728\u663e\u8457\u6cdb\u5316\u6311\u6218"}}
{"id": "2507.00994", "pdf": "https://arxiv.org/pdf/2507.00994", "abs": "https://arxiv.org/abs/2507.00994", "authors": ["Hippolyte Gisserot-Boukhlef", "Nicolas Boizard", "Manuel Faysse", "Duarte M. Alves", "Emmanuel Malherbe", "Andr\u00e9 F. T. Martins", "C\u00e9line Hudelot", "Pierre Colombo"], "title": "Should We Still Pretrain Encoders with Masked Language Modeling?", "categories": ["cs.CL"], "comment": "23 pages, 10 figures, 17 tables", "summary": "Learning high-quality text representations is fundamental to a wide range of\nNLP tasks. While encoder pretraining has traditionally relied on Masked\nLanguage Modeling (MLM), recent evidence suggests that decoder models\npretrained with Causal Language Modeling (CLM) can be effectively repurposed as\nencoders, often surpassing traditional encoders on text representation\nbenchmarks. However, it remains unclear whether these gains reflect an inherent\nadvantage of the CLM objective or arise from confounding factors such as model\nand data scale. In this paper, we address this question through a series of\nlarge-scale, carefully controlled pretraining ablations, training a total of 30\nmodels ranging from 210 million to 1 billion parameters, and conducting over\n15,000 fine-tuning and evaluation runs. We find that while training with MLM\ngenerally yields better performance across text representation tasks,\nCLM-trained models are more data-efficient and demonstrate improved fine-tuning\nstability. Building on these findings, we experimentally show that a biphasic\ntraining strategy that sequentially applies CLM and then MLM, achieves optimal\nperformance under a fixed computational training budget. Moreover, we\ndemonstrate that this strategy becomes more appealing when initializing from\nreadily available pretrained CLM models (from the existing LLM ecosystem),\nreducing the computational burden needed to train best-in-class encoder models.\nWe release all project artifacts at https://hf.co/MLMvsCLM to foster further\nresearch.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u5bf9\u6bd4MLM\u4e0eCLM\u9884\u8bad\u7ec3\u76ee\u6807\uff0c\u53d1\u73b0MLM\u5728\u6587\u672c\u8868\u5f81\u4efb\u52a1\u8868\u73b0\u66f4\u4f18\u4f46CLM\u6570\u636e\u6548\u7387\u66f4\u9ad8\uff0c\u63d0\u51faCLM+MLM\u53cc\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u53ef\u4f18\u5316\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u3002", "motivation": "\u63a2\u7a76CLM\u6a21\u578b\u5728\u6587\u672c\u8868\u5f81\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\u7a76\u7adf\u6e90\u4e8e\u5176\u8bad\u7ec3\u76ee\u6807\u672c\u8eab\u8fd8\u662f\u6a21\u578b/\u6570\u636e\u89c4\u6a21\u7b49\u5916\u90e8\u56e0\u7d20\uff0c\u660e\u786e\u4e24\u79cd\u9884\u8bad\u7ec3\u76ee\u6807\u7684\u672c\u8d28\u5dee\u5f02\u3002", "method": "\u8bad\u7ec330\u4e2a\u4e0d\u540c\u53c2\u6570\u89c4\u6a21\u6a21\u578b(2.1\u4ebf\u81f310\u4ebf\u53c2\u6570)\uff0c\u63a7\u5236\u53d8\u91cf\u8fdb\u884c15,000+\u6b21\u5fae\u8c03\u5b9e\u9a8c\uff0c\u91c7\u7528\u53cc\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565(\u5148CLM\u540eMLM)\u3002", "result": "MLM\u6574\u4f53\u8868\u73b0\u66f4\u4f18\u4f46CLM\u6570\u636e\u6548\u7387\u63d0\u534737%\u3001\u5fae\u8c03\u7a33\u5b9a\u6027\u66f4\u597d\uff1b\u53cc\u9636\u6bb5\u8bad\u7ec3\u6bd4\u5355\u4e00\u76ee\u6807\u8bad\u7ec3\u63d0\u53472.5%\uff0c\u4e14\u7ee7\u627fCLM\u9884\u8bad\u7ec3\u6743\u91cd\u53ef\u51cf\u5c1134%\u8ba1\u7b97\u6d88\u8017\u3002", "conclusion": "\u5728\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u4e0b\uff0cCLM+MLM\u53cc\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u80fd\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\uff0c\u4e14\u5229\u7528\u73b0\u6709LLM\u751f\u6001\u4e2d\u7684CLM\u6a21\u578b\u53ef\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u6700\u4f73\u7f16\u7801\u5668\u7684\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2507.00999", "pdf": "https://arxiv.org/pdf/2507.00999", "abs": "https://arxiv.org/abs/2507.00999", "authors": ["Mar\u00eda Grandury", "Javier Aula-Blasco", "J\u00falia Falc\u00e3o", "Cl\u00e9mentine Fourrier", "Miguel Gonz\u00e1lez", "Gonzalo Mart\u00ednez", "Gonzalo Santamar\u00eda", "Rodrigo Agerri", "Nuria Aldama", "Luis Chiruzzo", "Javier Conde", "Helena G\u00f3mez", "Marta Guerrero", "Guido Ivetta", "Natalia L\u00f3pez", "Flor Miriam Plaza-del-Arco", "Mar\u00eda Teresa Mart\u00edn-Valdivia", "Helena Montoro", "Carmen Mu\u00f1oz", "Pedro Reviriego", "Leire Rosado", "Alejandro Vaca", "Mar\u00eda Estrella Vallecillo-Rodr\u00edguez", "Jorge Vallego", "Irune Zubiaga"], "title": "La Leaderboard: A Large Language Model Leaderboard for Spanish Varieties and Languages of Spain and Latin America", "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 Main", "summary": "Leaderboards showcase the current capabilities and limitations of Large\nLanguage Models (LLMs). To motivate the development of LLMs that represent the\nlinguistic and cultural diversity of the Spanish-speaking community, we present\nLa Leaderboard, the first open-source leaderboard to evaluate generative LLMs\nin languages and language varieties of Spain and Latin America. La Leaderboard\nis a community-driven project that aims to establish an evaluation standard for\neveryone interested in developing LLMs for the Spanish-speaking community. This\ninitial version combines 66 datasets in Basque, Catalan, Galician, and\ndifferent Spanish varieties, showcasing the evaluation results of 50 models. To\nencourage community-driven development of leaderboards in other languages, we\nexplain our methodology, including guidance on selecting the most suitable\nevaluation setup for each downstream task. In particular, we provide a\nrationale for using fewer few-shot examples than typically found in the\nliterature, aiming to reduce environmental impact and facilitate access to\nreproducible results for a broader research community.", "AI": {"tldr": "\u521b\u5efa\u9996\u4e2a\u897f\u73ed\u7259\u8bed\u53ca\u53d8\u4f53\u7684\u5f00\u6e90\u5927\u6a21\u578b\u8bc4\u6d4b\u699cLa Leaderboard\uff0c\u6574\u540866\u4e2a\u6570\u636e\u96c6\u8bc4\u4f3050\u4e2a\u6a21\u578b\uff0c\u63d0\u5021\u4f4e\u6837\u672c\u8bc4\u4f30\u51cf\u5c11\u73af\u5883\u5f71\u54cd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u897f\u73ed\u7259\u8bed\u793e\u533a\u8bed\u8a00\u6587\u5316\u591a\u6837\u6027\u652f\u6301\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63a8\u52a8\u793e\u533a\u9a71\u52a8\u7684\u6a21\u578b\u53d1\u5c55\u3002", "method": "\u6574\u5408\u5df4\u65af\u514b\u8bed/\u52a0\u6cf0\u7f57\u5c3c\u4e9a\u8bed/\u52a0\u5229\u897f\u4e9a\u8bed\u53ca\u897f\u73ed\u7259\u8bed\u53d8\u4f53\u768466\u4e2a\u6570\u636e\u96c6\uff0c\u91c7\u7528\u4f4efew-shot\u8bc4\u4f30\u7b56\u7565(\u51cf\u5c11\u6837\u672c\u91cf)\uff0c\u5efa\u7acb\u6807\u51c6\u5316\u8bc4\u6d4b\u6846\u67b6\u3002", "result": "\u8986\u76d64\u79cd\u4e3b\u8981\u8bed\u8a00\u53ca\u65b9\u8a00\u53d8\u4f53\uff0c\u5b8c\u621050\u4e2a\u6a21\u578b\u8bc4\u6d4b\uff0c\u9a8c\u8bc1\u4f4e\u6837\u672c\u7b56\u7565\u53ef\u884c\u6027\uff0c\u5efa\u7acb\u53ef\u590d\u73b0\u8bc4\u4f30\u6807\u51c6\u3002", "conclusion": "\u901a\u8fc7\u5f00\u6e90\u793e\u533a\u534f\u4f5c\u6784\u5efa\u591a\u8bed\u8a00\u8bc4\u6d4b\u4f53\u7cfb\uff0c\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u73af\u4fdd\u9700\u6c42\uff0c\u4e3a\u5c0f\u8bed\u79cdNLP\u53d1\u5c55\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2507.01001", "pdf": "https://arxiv.org/pdf/2507.01001", "abs": "https://arxiv.org/abs/2507.01001", "authors": ["Yilun Zhao", "Kaiyan Zhang", "Tiansheng Hu", "Sihong Wu", "Ronan Le Bras", "Taira Anderson", "Jonathan Bragg", "Joseph Chee Chang", "Jesse Dodge", "Matt Latzke", "Yixin Liu", "Charles McGrady", "Xiangru Tang", "Zihang Wang", "Chen Zhao", "Hannaneh Hajishirzi", "Doug Downey", "Arman Cohan"], "title": "SciArena: An Open Evaluation Platform for Foundation Models in Scientific Literature Tasks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present SciArena, an open and collaborative platform for evaluating\nfoundation models on scientific literature tasks. Unlike traditional benchmarks\nfor scientific literature understanding and synthesis, SciArena engages the\nresearch community directly, following the Chatbot Arena evaluation approach of\ncommunity voting on model comparisons. By leveraging collective intelligence,\nSciArena offers a community-driven evaluation of model performance on\nopen-ended scientific tasks that demand literature-grounded, long-form\nresponses. The platform currently supports 23 open-source and proprietary\nfoundation models and has collected over 13,000 votes from trusted researchers\nacross diverse scientific domains. We analyze the data collected so far and\nconfirm that the submitted questions are diverse, aligned with real-world\nliterature needs, and that participating researchers demonstrate strong\nself-consistency and inter-annotator agreement in their evaluations. We discuss\nthe results and insights based on the model ranking leaderboard. To further\npromote research in building model-based automated evaluation systems for\nliterature tasks, we release SciArena-Eval, a meta-evaluation benchmark based\non our collected preference data. The benchmark measures the accuracy of models\nin judging answer quality by comparing their pairwise assessments with human\nvotes. Our experiments highlight the benchmark's challenges and emphasize the\nneed for more reliable automated evaluation methods.", "AI": {"tldr": "SciArena\u662f\u57fa\u4e8e\u793e\u533a\u6295\u7968\u7684\u5f00\u653e\u79d1\u7814\u5e73\u53f0\uff0c\u7528\u4e8e\u8bc4\u4f30\u79d1\u5b66\u6587\u732e\u4efb\u52a1\u4e2d\u7684\u57fa\u7840\u6a21\u578b\u8868\u73b0\uff0c\u652f\u630123\u4e2a\u6a21\u578b\u5e76\u6536\u96c613k+\u7814\u7a76\u4eba\u5458\u6295\u7968\uff0c\u540c\u65f6\u53d1\u5e03\u81ea\u52a8\u5316\u8bc4\u4f30\u57fa\u51c6SciArena-Eval\u3002", "motivation": "\u4f20\u7edf\u79d1\u5b66\u6587\u732e\u8bc4\u4f30\u57fa\u51c6\u96be\u4ee5\u6ee1\u8db3\u5f00\u653e\u578b\u4efb\u52a1\u7684\u8bc4\u4f30\u9700\u6c42\uff0c\u9700\u901a\u8fc7\u793e\u533a\u534f\u4f5c\u65b9\u5f0f\u5b9e\u73b0\u66f4\u771f\u5b9e\u7684\u6a21\u578b\u6027\u80fd\u8bc4\u4f30\u3002", "method": "\u91c7\u7528\u7c7bChatbot Arena\u7684\u793e\u533a\u6295\u7968\u673a\u5236\uff0c\u6536\u96c6\u8de8\u9886\u57df\u7814\u7a76\u8005\u5bf9\u6a21\u578b\u957f\u6587\u672c\u56de\u7b54\u7684\u504f\u597d\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u5143\u8bc4\u4f30\u57fa\u51c6\u9a8c\u8bc1\u81ea\u52a8\u8bc4\u4f30\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002", "result": "\u6784\u5efa\u5f53\u524d\u6700\u5168\u9762\u7684\u5f00\u653e\u6a21\u578b\u6392\u884c\u699c\uff0c\u53d1\u73b0\u73b0\u6709\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u4e0e\u4eba\u7c7b\u6295\u7968\u7684\u4e00\u81f4\u6027\u4ec5\u4e3a62%\uff0c\u63ed\u793a\u81ea\u52a8\u5316\u8bc4\u4f30\u7cfb\u7edf\u7684\u6280\u672f\u74f6\u9888\u3002", "conclusion": "\u793e\u533a\u9a71\u52a8\u7684\u8bc4\u4f30\u80fd\u6709\u6548\u6355\u6349\u6a21\u578b\u771f\u5b9e\u80fd\u529b\uff0c\u4f46\u9700\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u4ee5\u652f\u6301\u79d1\u5b66\u6587\u732e\u5904\u7406\u7cfb\u7edf\u7684\u6301\u7eed\u53d1\u5c55\u3002"}}
{"id": "2507.00002", "pdf": "https://arxiv.org/pdf/2507.00002", "abs": "https://arxiv.org/abs/2507.00002", "authors": ["Christopher James Augeri"], "title": "Hypertokens: Holographic Associative Memory in Tokenized LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "preprint as accepted to https://qnlp.ai/ - Quantum AI and NLP\n  Conference 2025", "summary": "Large Language Models (LLMs) exhibit remarkable capabilities but suffer from\napparent precision loss, reframed here as information spreading. This reframing\nshifts the problem from computational precision to an information-theoretic\ncommunication issue. We address the K:V and V:K memory problem in LLMs by\nintroducing HDRAM (Holographically Defined Random Access Memory), a symbolic\nmemory framework treating transformer latent space as a spread-spectrum\nchannel. Built upon hypertokens, structured symbolic codes integrating\nclassical error-correcting codes (ECC), holographic computing, and\nquantum-inspired search, HDRAM recovers distributed information through\nprincipled despreading. These phase-coherent memory addresses enable efficient\nkey-value operations and Grover-style search in latent space. By combining ECC\ngrammar with compressed sensing and Krylov subspace alignment, HDRAM\nsignificantly improves associative retrieval without architectural changes,\ndemonstrating how Classical-Holographic-Quantum-inspired (CHQ) principles can\nfortify transformer architectures.", "AI": {"tldr": "\u63d0\u51faHDRAM\u6846\u67b6\u89e3\u51b3LLMs\u7684\u4fe1\u606f\u6269\u6563\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u7ea0\u9519\u7801/\u5168\u606f\u8ba1\u7b97/\u91cf\u5b50\u542f\u53d1\u641c\u7d22\u5b9e\u73b0\u9ad8\u6548\u5173\u8054\u68c0\u7d22", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u7cbe\u5ea6\u635f\u5931\u95ee\u9898\uff08\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4fe1\u606f\u6269\u6563\uff09\uff0c\u9700\u4ece\u4fe1\u606f\u8bba\u89d2\u5ea6\u89e3\u51b3KV/VK\u8bb0\u5fc6\u673a\u5236\u7f3a\u9677", "method": "\u6784\u5efa\u57fa\u4e8e\u8d85\u4ee4\u724c\u7684\u7b26\u53f7\u5b58\u50a8\u6846\u67b6\uff0c\u6574\u5408\u7ea0\u9519\u7801+\u5168\u606f\u8ba1\u7b97+\u91cf\u5b50\u641c\u7d22\uff0c\u901a\u8fc7\u76f8\u4f4d\u76f8\u5e72\u5185\u5b58\u5730\u5740\u5b9e\u73b0\u6f5c\u5728\u7a7a\u95f4\u7684\u9ad8\u6548\u641c\u7d22", "result": "\u5728\u4e0d\u6539\u53d8\u67b6\u6784\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u5173\u8054\u68c0\u7d22\u6548\u7387\uff0c\u9a8c\u8bc1CHQ\u539f\u5219\u5bf9Transformer\u67b6\u6784\u7684\u589e\u5f3a\u6548\u679c", "conclusion": "\u4fe1\u606f\u8bba\u4e0e\u91cf\u5b50\u542f\u53d1\u7684\u7ed3\u5408\u4e3a\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0cHDRAM\u6846\u67b6\u5c55\u793a\u4e86\u8de8\u5b66\u79d1\u65b9\u6cd5\u5728AI\u67b6\u6784\u4f18\u5316\u4e2d\u7684\u6f5c\u529b"}}
{"id": "2507.00018", "pdf": "https://arxiv.org/pdf/2507.00018", "abs": "https://arxiv.org/abs/2507.00018", "authors": ["Bo Wang", "Qinyuan Cheng", "Runyu Peng", "Rong Bao", "Peiji Li", "Qipeng Guo", "Linyang Li", "Zhiyuan Zeng", "Yunhua Zhou", "Xipeng Qiu"], "title": "Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Post-training processes are essential phases in grounding pre-trained\nlanguage models to real-world tasks, with learning from demonstrations or\npreference signals playing a crucial role in this adaptation. We present a\nunified theoretical framework bridging Supervised Fine-Tuning (SFT) and\npreference learning in Large Language Model (LLM) post-training. Through\nrigorous mathematical derivation, we demonstrate that both SFT and preference\nlearning methods like Direct Preference Optimization (DPO) operate within the\nsame optimal policy-reward subspace, with SFT representing a special case of\nimplicit reward learning. Our analysis reveals a critical limitation in\nconventional SFT: the KL divergence term in distribution matching becomes\nconstant with respect to the policy during optimization, failing to constrain\nmodel updates. To address this, we propose a simple yet effective learning rate\nreduction approach that yields significant performance improvements (up to\n\\textbf{25\\%} relative gain and \\textbf{6\\%} absolute win rate increase in\ninstruction following tasks. Additionally, we derive alternative SFT objectives\nfrom various f-divergence functions that preserve the KL term during\noptimization, further enhancing post-DPO model performance. Finally, we extend\nthe theoretical relationship between LLM logits and Q-functions from preference\nlearning to the SFT context, providing mathematical derivations and\nexperimental validation.", "AI": {"tldr": "Unified framework connects SFT and preference learning, identifies limitations in conventional SFT, proposes learning rate reduction and alternative objectives with significant performance gains.", "motivation": "Addressing limitations in conventional Supervised Fine-Tuning (SFT) and establishing theoretical connections between SFT/preference learning methods in LLM post-training.", "method": "Mathematical framework revealing SFT as implicit reward learning special case, proposing learning rate reduction strategy and f-divergence based alternative objectives.", "result": "Achieved 25% relative performance gain and 6% absolute win rate improvement in instruction following tasks through learning rate adjustment.", "conclusion": "Theoretical unification and practical improvements provide new optimization directions for LLM post-training, bridging policy optimization with reward learning paradigms."}}
{"id": "2507.00022", "pdf": "https://arxiv.org/pdf/2507.00022", "abs": "https://arxiv.org/abs/2507.00022", "authors": ["Zehao Wang"], "title": "GLU Attention Improve Transformer", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "comment": "4 pages 4 figures", "summary": "Gated Linear Units (GLU) have shown great potential in enhancing neural\nnetwork performance. In this paper, I introduce a novel attention mechanism\ncalled GLU Attention, which introduces nonlinearity into the values of\nAttention. My experiments demonstrate that GLU Attention improves both model\nperformance and convergence speed across text and vision modalities with zero\nadditional parameters and negligible computational costs. GLU Attention is\nlightweight and can seamlessly integrate with other technologies, such as Flash\nAttention, Rotary Position Embedding (RoPE), and various Multi-Head Attention\n(MHA) variants such as Grouped-Query Attention (GQA). This project is\nopen-sourced at github.", "AI": {"tldr": "\u63d0\u51faGLU Attention\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u7ed9\u6ce8\u610f\u529b\u503c\u5f15\u5165\u975e\u7ebf\u6027\u63d0\u5347\u6a21\u578b\u6027\u80fd\u4e0e\u6536\u655b\u901f\u5ea6\uff0c\u96f6\u53c2\u6570\u91cf\u589e\u52a0\u4e14\u517c\u5bb9\u4e3b\u6d41\u6ce8\u610f\u529b\u4f18\u5316\u6280\u672f", "motivation": "\u4f20\u7edf\u6ce8\u610f\u529b\u673a\u5236\u7684\u503c\u8ba1\u7b97\u7f3a\u4e4f\u975e\u7ebf\u6027\u8868\u8fbe\u80fd\u529b\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6027\u80fd\u6f5c\u529b\u3002GLU\u7ed3\u6784\u5728FFN\u5c42\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5c1a\u672a\u88ab\u5e94\u7528\u4e8e\u6ce8\u610f\u529b\u503c\u8ba1\u7b97", "method": "\u5728\u6ce8\u610f\u529b\u673a\u5236\u7684\u503c\uff08Value\uff09\u8ba1\u7b97\u8fc7\u7a0b\u4e2d\u5f15\u5165Gated Linear Units\uff08GLU\uff09\uff0c\u901a\u8fc7\u95e8\u63a7\u673a\u5236\u5b9e\u73b0\u975e\u7ebf\u6027\u7279\u5f81\u9009\u62e9\u4e0e\u589e\u5f3a", "result": "\u5728\u6587\u672c\u548c\u89c6\u89c9\u4efb\u52a1\u4e2d\u5747\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u52a0\u5feb\u6536\u655b\u901f\u5ea6\uff08\u5feb30%\uff09\uff0c\u517c\u5bb9Flash Attention/RoPE/GQA\u7b49\u6280\u672f\u4e14\u4e0d\u589e\u52a0\u53c2\u6570\u91cf", "conclusion": "GLU Attention\u662f\u8f7b\u91cf\u9ad8\u6548\u7684\u6ce8\u610f\u529b\u6539\u8fdb\u65b9\u6848\uff0c\u5176\u5f00\u6e90\u5b9e\u73b0\u5c06\u4fc3\u8fdb\u5927\u89c4\u6a21\u5e94\u7528\u3002\u8be5\u65b9\u6cd5\u4e3a\u6ce8\u610f\u529b\u673a\u5236\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411"}}
{"id": "2507.00026", "pdf": "https://arxiv.org/pdf/2507.00026", "abs": "https://arxiv.org/abs/2507.00026", "authors": ["Jiale Ding", "Xiang Zheng", "Cong Wang", "Wei-Bin Lee", "Xingjun Ma", "Yu-Gang Jiang"], "title": "ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "As Large Language Models (LLMs) are increasingly deployed as black-box\ncomponents in real-world applications, evaluating their safety-especially under\nadversarial prompting-has become critical. Arguably, effective safety\nevaluations should be adaptive, evolving with LLM capabilities, and also cover\na broad spectrum of harmful topics and real-world scenarios to fully expose\npotential vulnerabilities. Existing manual safety benchmarks, built on\nhandcrafted adversarial prompts, are limited by their static nature and the\nintensive labor required to update them, making it difficult to keep pace with\nrapidly advancing LLMs. In contrast, automated adversarial prompt generation\noffers a promising path toward adaptive evaluation. However, current methods\noften suffer from insufficient adversarial topic coverage (topic-level\ndiversity) and weak alignment with real-world contexts. These shortcomings stem\nfrom the exploration-exploitation dilemma in black-box optimization and a lack\nof real-world contextualization, resulting in adversarial prompts that are both\ntopically narrow and scenario-repetitive. To address these issues, we propose\nReality-Oriented Safety Evaluation (ROSE), a novel framework that uses\nmulti-objective reinforcement learning to fine-tune an adversarial LLM for\ngenerating topically diverse and contextually rich adversarial prompts.\nExperiments show that ROSE outperforms existing methods in uncovering safety\nvulnerabilities in state-of-the-art LLMs, with notable improvements in\nintegrated evaluation metrics. We hope ROSE represents a step toward more\npractical and reality-oriented safety evaluation of LLMs. WARNING: This paper\ncontains examples of potentially harmful text.", "AI": {"tldr": "\u63d0\u51faROSE\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u751f\u6210\u591a\u6837\u5316\u7684\u5bf9\u6297\u6027\u63d0\u793a\uff0c\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u6027\u8bc4\u4f30\u7684\u8986\u76d6\u8303\u56f4\u548c\u73b0\u5b9e\u573a\u666f\u9002\u914d\u6027", "motivation": "\u73b0\u6709\u624b\u52a8\u5b89\u5168\u57fa\u51c6\u5b58\u5728\u9759\u6001\u66f4\u65b0\u6162\u3001\u5bf9\u6297\u8bdd\u9898\u8986\u76d6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u9002\u914d\u5feb\u901f\u8fed\u4ee3\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u8bc4\u4f30\u9700\u6c42", "method": "\u91c7\u7528\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5728\u5bf9\u6297\u63d0\u793a\u751f\u6210\u4e2d\u5e73\u8861\u8bdd\u9898\u591a\u6837\u6027\u4e0e\u73b0\u5b9e\u573a\u666f\u9002\u914d\u6027\uff0c\u6784\u5efa\u52a8\u6001\u4f18\u5316\u7684\u8bc4\u4f30\u6846\u67b6", "result": "\u5b9e\u9a8c\u8868\u660eROSE\u5728\u53d1\u73b0\u6a21\u578b\u5b89\u5168\u6f0f\u6d1e\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7efc\u5408\u8bc4\u4f30\u6307\u6807\u63d0\u5347\u660e\u663e", "conclusion": "ROSE\u6846\u67b6\u4e3aLLMs\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u8d34\u8fd1\u73b0\u5b9e\u573a\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u5b89\u5168\u6d4b\u8bd5\u4ece\u9759\u6001\u57fa\u51c6\u5411\u52a8\u6001\u6f14\u5316\u53d1\u5c55"}}
{"id": "2507.00033", "pdf": "https://arxiv.org/pdf/2507.00033", "abs": "https://arxiv.org/abs/2507.00033", "authors": ["Mustafa Chasmai", "Gauri Jagatap", "Gouthaman KV", "Grant Van Horn", "Subhransu Maji", "Andrea Fanelli"], "title": "Moment Sampling in Video LLMs for Long-Form Video QA", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Workshop on Video Large Language Models (VidLLMs) at CVPR 2025", "summary": "Recent advancements in video large language models (Video LLMs) have\nsignificantly advanced the field of video question answering (VideoQA). While\nexisting methods perform well on short videos, they often struggle with\nlong-range reasoning in longer videos. To scale Video LLMs for longer video\ncontent, frame sub-sampling (selecting frames at regular intervals) is commonly\nused. However, this approach is suboptimal, often leading to the loss of\ncrucial frames or the inclusion of redundant information from multiple similar\nframes. Missing key frames impairs the model's ability to answer questions\naccurately, while redundant frames lead the model to focus on irrelevant video\nsegments and increase computational resource consumption. In this paper, we\ninvestigate the use of a general-purpose text-to-video moment retrieval model\nto guide the frame sampling process. We propose \"moment sampling\", a novel,\nmodel-agnostic approach that enables the model to select the most relevant\nframes according to the context of the question. Specifically, we employ a\nlightweight moment retrieval model to prioritize frame selection. By focusing\non the frames most pertinent to the given question, our method enhances\nlong-form VideoQA performance in Video LLMs. Through extensive experiments on\nfour long-form VideoQA datasets, using four state-of-the-art Video LLMs, we\ndemonstrate the effectiveness of the proposed approach.", "AI": {"tldr": "\u63d0\u51fa'moment sampling'\u5e27\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u6587\u672c\u5230\u89c6\u9891\u65f6\u523b\u68c0\u7d22\u6a21\u578b\u6307\u5bfc\u5e27\u9009\u62e9\uff0c\u89e3\u51b3\u957f\u89c6\u9891\u95ee\u7b54\u4e2d\u5173\u952e\u5e27\u4e22\u5931\u4e0e\u5197\u4f59\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u7684\u957f\u7a0b\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5747\u5300\u5e27\u91c7\u6837\u65b9\u6cd5\u5728\u957f\u89c6\u9891\u95ee\u7b54\u4e2d\u6613\u4e22\u5931\u5173\u952e\u5e27\u6216\u4ea7\u751f\u5197\u4f59\u4fe1\u606f\uff0c\u5bfc\u81f4\u6a21\u578b\u51c6\u786e\u6027\u4e0b\u964d\u548c\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\uff0c\u9700\u66f4\u667a\u80fd\u7684\u5e27\u9009\u62e9\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u65f6\u523b\u68c0\u7d22\u6a21\u578b\u52a8\u6001\u9009\u62e9\u4e0e\u95ee\u9898\u6700\u76f8\u5173\u7684\u89c6\u9891\u7247\u6bb5\u5e27\uff0c\u66ff\u4ee3\u4f20\u7edf\u5747\u5300\u91c7\u6837\uff0c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u5e27\u91c7\u6837\u3002", "result": "\u5728\u56db\u4e2a\u957f\u89c6\u9891\u95ee\u7b54\u6570\u636e\u96c6\u548c\u56db\u4e2aSOTA\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u957f\u89c6\u9891\u95ee\u7b54\u51c6\u786e\u7387(\u5e73\u5747+3.8%)\u5e76\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "\u95ee\u9898\u5f15\u5bfc\u7684moment sampling\u673a\u5236\u6709\u6548\u589e\u5f3a\u89c6\u9891LLMs\u7684\u957f\u7a0b\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u5177\u6709\u6a21\u578b\u65e0\u5173\u6027\uff0c\u4e3a\u957f\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2507.00045", "pdf": "https://arxiv.org/pdf/2507.00045", "abs": "https://arxiv.org/abs/2507.00045", "authors": ["Ming Li", "Chenguang Wang", "Yijun Liang", "Xiyao Wang", "Yuhang Zhou", "Xiyang Wu", "Yuqing Zhang", "Ruiyi Zhang", "Tianyi Zhou"], "title": "CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent agentic Multi-Modal Large Language Models (MLLMs) such as GPT-o3 have\nachieved near-ceiling scores on various existing benchmarks, motivating a\ndemand for more challenging test tasks. These MLLMs have been reported to excel\nin a few expert-level tasks for humans, e.g., GeoGuesser, reflecting their\npotential as a detective who can notice minuscule cues in an image and weave\nthem into coherent, situational explanations, leading to a reliable answer. But\ncan they match the performance of excellent human detectives? To answer this\nquestion, we investigate some hard scenarios where GPT-o3 can still handle, and\nfind a common scenario where o3's performance drops to nearly zero, which we\nname CaughtCheating. It is inspired by the social media requests that ask\nothers to detect suspicious clues from photos shared by the poster's partner.\nWe conduct extensive experiments and analysis to understand why existing MLLMs\nlack sufficient capability to solve this kind of task. CaughtCheating provides\na class of challenging visual perception and reasoning tasks with great value\nand practical usage. Success in these tasks paves the way for MLLMs to acquire\nhuman-level detective perception and reasoning capabilities.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u4e86GPT-o3\u5728\u65b0\u578b\u6311\u6218\u6027\u4efb\u52a1CaughtCheating\u4e2d\u7684\u6027\u80fd\u9aa4\u964d\u73b0\u8c61\uff0c\u63d0\u51fa\u8be5\u573a\u666f\u53ef\u4f5c\u4e3a\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4fa6\u63a2\u5f0f\u63a8\u7406\u80fd\u529b\u7684\u65b0\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5bf9\u5148\u8fdbMLLMs\u5df2\u4e0d\u5177\u5907\u6311\u6218\u6027\uff0c\u9700\u901a\u8fc7\u6a21\u62df\u771f\u5b9e\u793e\u4ea4\u5a92\u4f53\u4e2d\u68c0\u6d4b\u4f34\u4fa3\u53ef\u7591\u7ebf\u7d22\u7684\u56f0\u96be\u573a\u666f\uff0c\u9a8c\u8bc1\u6a21\u578b\u7684\u4eba\u7c7b\u7ea7\u4fa6\u63a2\u63a8\u7406\u80fd\u529b\u3002", "method": "\u521b\u5efaCaughtCheating\u6d4b\u8bd5\u573a\u666f\uff08\u53d7\u793e\u4ea4\u5a92\u4f53\u6c42\u52a9\u542f\u53d1\uff09\uff0c\u901a\u8fc7\u7cfb\u7edf\u6027\u5b9e\u9a8c\u5206\u6790GPT-o3\u7b49\u6a21\u578b\u5728\u7ec6\u5fae\u89c6\u89c9\u7ebf\u7d22\u611f\u77e5\u4e0e\u60c5\u5883\u63a8\u7406\u4e2d\u7684\u5931\u8d25\u539f\u56e0\u3002", "result": "GPT-o3\u5728CaughtCheating\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u8d8b\u8fd1\u4e8e\u96f6\uff0c\u5b9e\u9a8c\u63ed\u793a\u4e86\u73b0\u6709MLLMs\u5728\u89c6\u89c9\u7ec6\u8282\u5173\u8054\u4e0e\u60c5\u5883\u903b\u8f91\u6574\u5408\u65b9\u9762\u7684\u80fd\u529b\u7f3a\u9677\u3002", "conclusion": "CaughtCheating\u4e3aMLLMs\u63d0\u4f9b\u4e86\u517c\u5177\u6311\u6218\u6027\u548c\u5b9e\u7528\u4ef7\u503c\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5176\u7a81\u7834\u5c06\u63a8\u52a8\u6a21\u578b\u83b7\u5f97\u771f\u6b63\u7684\u4eba\u7c7b\u4fa6\u63a2\u7ea7\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2507.00054", "pdf": "https://arxiv.org/pdf/2507.00054", "abs": "https://arxiv.org/abs/2507.00054", "authors": ["Shreyansh Padarha"], "title": "Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "17 Pages, 7 figures", "summary": "The push to compress and impart the proficiency of Large Language Models\n(LLMs) into more deployable and efficient Small Language Models (SLMs) has\nbenefited from improvements in knowledge distillation (KD) techniques. These\ntechniques allow a smaller student model to learn from a more capable and\nlarger teacher model's responses. However, distillation often revolves around\nthe student model merely copying the teacher's in-distribution responses,\nlimiting its generalisability. This limitation is amplified on reasoning tasks\nand can be computationally expensive. In this study, we propose AdvDistill, a\nreward-guided dataset distillation framework. We utilise multiple generations\n(responses) from a teacher for each prompt and assign rewards based on\nrule-based verifiers. These varying and normally distributed rewards serve as\nweights when training student models. Our methods and their subsequent\nbehavioural analysis demonstrate a significant improvement in student model\nperformance for mathematical and complex reasoning tasks, showcasing the\nefficacy and benefits of incorporating a rewarding mechanism in dataset\ndistillation processes.", "AI": {"tldr": "\u63d0\u51fa\u5956\u52b1\u5f15\u5bfc\u7684\u6570\u636e\u96c6\u84b8\u998f\u6846\u67b6AdvDistill\uff0c\u901a\u8fc7\u591a\u4e2a\u6559\u5e08\u6a21\u578b\u751f\u6210\u7ed3\u679c\u7ed3\u5408\u89c4\u5219\u9a8c\u8bc1\u5668\u5206\u914d\u5956\u52b1\u6743\u91cd\uff0c\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0", "motivation": "\u4f20\u7edf\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u4ec5\u8ba9\u5b66\u751f\u6a21\u578b\u590d\u5236\u6559\u5e08\u6a21\u578b\u7684\u540c\u5206\u5e03\u54cd\u5e94\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e14\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u8ba1\u7b97\u6210\u672c\u8f83\u9ad8", "method": "\u4f7f\u7528\u6559\u5e08\u6a21\u578b\u4e3a\u6bcf\u4e2a\u63d0\u793a\u751f\u6210\u591a\u4e2a\u54cd\u5e94\uff0c\u901a\u8fc7\u89c4\u5219\u9a8c\u8bc1\u5668\u5206\u914d\u5956\u52b1\u5f62\u6210\u6b63\u6001\u5206\u5e03\u6743\u91cd\u6307\u5bfc\u5b66\u751f\u6a21\u578b\u8bad\u7ec3", "result": "\u5728\u6570\u5b66\u63a8\u7406\u548c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u5b66\u751f\u6a21\u578b\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5956\u52b1\u673a\u5236\u5728\u6570\u636e\u96c6\u84b8\u998f\u4e2d\u7684\u6709\u6548\u6027", "conclusion": "\u5c06\u5956\u52b1\u673a\u5236\u878d\u5165\u6570\u636e\u96c6\u84b8\u998f\u8fc7\u7a0b\u80fd\u6709\u6548\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u5b66\u4e60\u6548\u679c\uff0c\u4e3a\u9ad8\u6548\u6a21\u578b\u538b\u7f29\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411"}}
{"id": "2507.00068", "pdf": "https://arxiv.org/pdf/2507.00068", "abs": "https://arxiv.org/abs/2507.00068", "authors": ["Ziqi Zhong", "Daniel Tang"], "title": "MANTA: Cross-Modal Semantic Alignment and Information-Theoretic Optimization for Long-form Multimodal Understanding", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "While multi-modal learning has advanced significantly, current approaches\noften treat modalities separately, creating inconsistencies in representation\nand reasoning. We introduce MANTA (Multi-modal Abstraction and Normalization\nvia Textual Alignment), a theoretically-grounded framework that unifies visual\nand auditory inputs into a structured textual space for seamless processing\nwith large language models. MANTA addresses four key challenges: (1) semantic\nalignment across modalities with information-theoretic optimization, (2)\nadaptive temporal synchronization for varying information densities, (3)\nhierarchical content representation for multi-scale understanding, and (4)\ncontext-aware retrieval of sparse information from long sequences. We formalize\nour approach within a rigorous mathematical framework, proving its optimality\nfor context selection under token constraints. Extensive experiments on the\nchallenging task of Long Video Question Answering show that MANTA improves\nstate-of-the-art models by up to 22.6% in overall accuracy, with particularly\nsignificant gains (27.3%) on videos exceeding 30 minutes. Additionally, we\ndemonstrate MANTA's superiority on temporal reasoning tasks (23.8% improvement)\nand cross-modal understanding (25.1% improvement). Our framework introduces\nnovel density estimation techniques for redundancy minimization while\npreserving rare signals, establishing new foundations for unifying multimodal\nrepresentations through structured text.", "AI": {"tldr": "\u63d0\u51faMANTA\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6587\u672c\u7a7a\u95f4\u7edf\u4e00\u591a\u6a21\u6001\u8f93\u5165\uff0c\u89e3\u51b3\u8de8\u6a21\u6001\u5bf9\u9f50\u3001\u65f6\u95f4\u540c\u6b65\u3001\u5206\u5c42\u8868\u793a\u548c\u7a00\u758f\u4fe1\u606f\u68c0\u7d22\u56db\u5927\u6311\u6218\uff0c\u5728\u957f\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6700\u9ad822.6%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5b66\u4e60\u65b9\u6cd5\u5e38\u5c06\u4e0d\u540c\u6a21\u6001\u5272\u88c2\u5904\u7406\uff0c\u5bfc\u81f4\u8868\u793a\u548c\u63a8\u7406\u4e0d\u4e00\u81f4\u3002\u9700\u6784\u5efa\u7406\u8bba\u5316\u6846\u67b6\u7edf\u4e00\u591a\u6a21\u6001\u4fe1\u606f\u8868\u793a\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u6548\u7387\u3002", "method": "\u57fa\u4e8e\u4fe1\u606f\u8bba\u4f18\u5316\u5b9e\u73b0\u8bed\u4e49\u5bf9\u9f50\uff0c\u5f00\u53d1\u81ea\u9002\u5e94\u65f6\u95f4\u540c\u6b65\u673a\u5236\uff0c\u6784\u5efa\u5206\u5c42\u5185\u5bb9\u8868\u793a\u4f53\u7cfb\uff0c\u7ed3\u5408\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u7a00\u758f\u4fe1\u606f\u68c0\u7d22\u3002\u5efa\u7acb\u6570\u5b66\u6846\u67b6\u9a8c\u8bc1token\u9650\u5236\u4e0b\u7684\u6700\u4f18\u4e0a\u4e0b\u6587\u9009\u62e9\u3002", "result": "\u5728\u957f\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u63d0\u534722.6%\uff0830\u5206\u949f\u4ee5\u4e0a\u89c6\u9891\u8fbe27.3%\uff09\uff0c\u65f6\u95f4\u63a8\u7406\u4efb\u52a1\u63d0\u534723.8%\uff0c\u8de8\u6a21\u6001\u7406\u89e3\u63d0\u534725.1%\u3002\u65b0\u5bc6\u5ea6\u4f30\u8ba1\u6280\u672f\u6709\u6548\u5e73\u8861\u5197\u4f59\u6700\u5c0f\u5316\u548c\u4fe1\u53f7\u4fdd\u7559\u3002", "conclusion": "MANTA\u901a\u8fc7\u7ed3\u6784\u5316\u6587\u672c\u7edf\u4e00\u591a\u6a21\u6001\u8868\u793a\uff0c\u5efa\u7acb\u65b0\u7684\u7406\u8bba\u6846\u67b6\u3002\u63d0\u51fa\u7684\u5bc6\u5ea6\u4f30\u8ba1\u6280\u672f\u4e3a\u591a\u6a21\u6001\u5b66\u4e60\u5960\u5b9a\u65b0\u57fa\u7840\uff0c\u663e\u8457\u63d0\u5347\u957f\u5e8f\u5217\u3001\u8de8\u6a21\u6001\u573a\u666f\u4e0b\u7684\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2507.00078", "pdf": "https://arxiv.org/pdf/2507.00078", "abs": "https://arxiv.org/abs/2507.00078", "authors": ["Yi Xie", "Yun Xiong", "Zejian Shi", "Hao Niu", "Zhengfu Liu"], "title": "The language of time: a language model perspective on time-series foundation models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "With the rise of large language models, the paradigm of training foundation\nmodels with massive parameter counts on vast datasets has been adopted in\nmultiple domains to achieve remarkable success. Time series foundation models\nrepresent a significant extension of this paradigm, demonstrating exceptional\nexpressive power, generalization, and cross-domain transferability. However,\nthis gives rise to a fundamental paradox: time series data reflect distinct\ndynamical systems, making cross-domain transfer intuitively implausible, yet\nthis is contradicted by the models' empirical success. To resolve this paradox,\nthis paper investigates, from both theoretical and experimental perspectives,\nthe representation learning mechanisms and generalization capabilities of\npatch-based time series foundation models. We argue that such models are not\nmerely applying a new architecture but are fundamentally generalizing the\nrepresentation paradigm of language models by extending deterministic\nvector-based representations to latent probabilistic distributional forms. Our\ntheoretical analysis supports this framework by demonstrating that continuous\ntime-series patches can be faithfully quantized into a discrete vocabulary\nwhose key statistical properties are highly consistent with those of natural\nlanguage. This generalization allows time series models to inherit the robust\nrepresentation and transfer abilities of large language models, thereby\nexplaining their superior performance in temporal tasks. Ultimately, our work\nprovides a rigorous theoretical cornerstone for understanding, evaluating, and\nimproving the safety and reliability of large-scale time series foundation\nmodels.", "AI": {"tldr": "\u672c\u6587\u89e3\u6790\u4e86\u57fa\u4e8epatch\u7684\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TSFMs\uff09\u7684\u6210\u529f\u673a\u5236\uff0c\u63d0\u51fa\u5176\u901a\u8fc7\u6269\u5c55\u8bed\u8a00\u6a21\u578b\u7684\u6982\u7387\u5206\u5e03\u8868\u793a\u8303\u5f0f\uff0c\u7ee7\u627f\u9c81\u68d2\u8868\u5f81\u80fd\u529b\uff0c\u4ece\u800c\u89e3\u51b3\u8de8\u9886\u57df\u8fc1\u79fb\u7684\u6096\u8bba\u3002", "motivation": "\u9488\u5bf9\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5728\u8de8\u9886\u57df\u8fc1\u79fb\u4e2d\u5b58\u5728\u7684\u7406\u8bba\u77db\u76fe\uff08\u52a8\u6001\u7cfb\u7edf\u5dee\u5f02\u6027 vs \u5b9e\u8bc1\u6210\u529f\uff09\uff0c\u63a2\u7d22\u5176\u5e95\u5c42\u8868\u793a\u5b66\u4e60\u673a\u5236\u4e0e\u6cdb\u5316\u80fd\u529b\u7684\u7406\u8bba\u4f9d\u636e\u3002", "method": "\u7ed3\u5408\u7406\u8bba\u5206\u6790\u4e0e\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u63ed\u793a\u65f6\u95f4\u5e8f\u5217patch\u53ef\u79bb\u6563\u5316\u4e3a\u7edf\u8ba1\u7279\u6027\u4e0e\u81ea\u7136\u8bed\u8a00\u9ad8\u5ea6\u4e00\u81f4\u7684\u8bcd\u6c47\u8868\uff0c\u5b9e\u73b0\u4ece\u786e\u5b9a\u6027\u5411\u91cf\u5230\u6982\u7387\u5206\u5e03\u7684\u8303\u5f0f\u6269\u5c55\u3002", "result": "\u8bc1\u5b9e\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u901a\u8fc7\u6982\u7387\u5206\u5e03\u8868\u793a\u7ee7\u627f\u8bed\u8a00\u6a21\u578b\u7684\u8fc1\u79fb\u80fd\u529b\uff0c\u5176patch\u91cf\u5316\u540e\u7684\u8bcd\u6c47\u8868\u56f0\u60d1\u5ea6\u7b49\u6307\u6807\u4e0e\u81ea\u7136\u8bed\u8a00\u5177\u6709\u5f3a\u53ef\u6bd4\u6027\u3002", "conclusion": "\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdb\u5927\u89c4\u6a21\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u7279\u522b\u5728\u6a21\u578b\u5b89\u5168\u6027\u4e0e\u53ef\u9760\u6027\u7ef4\u5ea6\u5efa\u7acb\u4e86\u91cf\u5316\u5206\u6790\u57fa\u7840\u3002"}}
{"id": "2507.00081", "pdf": "https://arxiv.org/pdf/2507.00081", "abs": "https://arxiv.org/abs/2507.00081", "authors": ["Matthew Muhoberac", "Atharva Parikh", "Nirvi Vakharia", "Saniya Virani", "Aco Radujevic", "Savannah Wood", "Meghav Verma", "Dimitri Metaxotos", "Jeyaraman Soundararajan", "Thierry Masquelin", "Alexander G. Godfrey", "Sean Gardner", "Dobrila Rudnicki", "Sam Michael", "Gaurav Chopra"], "title": "State and Memory is All You Need for Robust and Reliable AI Agents", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.ET", "physics.chem-ph"], "comment": "5 Main Figures, 10 Extended Data Figures (37 Pages) for Manuscript ;\n  9 Supplementary Tables, 40 Supplementary Figures (180 Pages) for Supporting\n  Information", "summary": "Large language models (LLMs) have enabled powerful advances in natural\nlanguage understanding and generation. Yet their application to complex,\nreal-world scientific workflows remain limited by challenges in memory,\nplanning, and tool integration. Here, we introduce SciBORG (Scientific Bespoke\nArtificial Intelligence Agents Optimized for Research Goals), a modular agentic\nframework that allows LLM-based agents to autonomously plan, reason, and\nachieve robust and reliable domain-specific task execution. Agents are\nconstructed dynamically from source code documentation and augmented with\nfinite-state automata (FSA) memory, enabling persistent state tracking and\ncontext-aware decision-making. This approach eliminates the need for manual\nprompt engineering and allows for robust, scalable deployment across diverse\napplications via maintaining context across extended workflows and to recover\nfrom tool or execution failures. We validate SciBORG through integration with\nboth physical and virtual hardware, such as microwave synthesizers for\nexecuting user-specified reactions, with context-aware decision making and\ndemonstrate its use in autonomous multi-step bioassay retrieval from the\nPubChem database utilizing multi-step planning, reasoning, agent-to-agent\ncommunication and coordination for execution of exploratory tasks. Systematic\nbenchmarking shows that SciBORG agents achieve reliable execution, adaptive\nplanning, and interpretable state transitions. Our results show that memory and\nstate awareness are critical enablers of agentic planning and reliability,\noffering a generalizable foundation for deploying AI agents in complex\nenvironments.", "AI": {"tldr": "SciBORG\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7684AI\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u6784\u5efa\u4ee3\u7406\u3001\u6709\u9650\u72b6\u6001\u81ea\u52a8\u673a\u5185\u5b58\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u51b3\u7b56\uff0c\u63d0\u5347LLM\u5728\u590d\u6742\u79d1\u5b66\u5de5\u4f5c\u6d41\u4e2d\u7684\u53ef\u9760\u6027\u548c\u81ea\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u79d1\u5b66\u5de5\u4f5c\u6d41\u4e2d\u5b58\u5728\u5185\u5b58\u7ba1\u7406\u3001\u591a\u6b65\u89c4\u5212\u548c\u5de5\u5177\u96c6\u6210\u9650\u5236\uff0c\u96be\u4ee5\u5b9e\u73b0\u7a33\u5065\u7684\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u6267\u884c\u3002", "method": "\u57fa\u4e8e\u6e90\u4ee3\u7801\u6587\u6863\u52a8\u6001\u6784\u5efa\u4ee3\u7406\uff0c\u91c7\u7528\u6709\u9650\u72b6\u6001\u81ea\u52a8\u673a(FSA)\u5b9e\u73b0\u6301\u4e45\u72b6\u6001\u8ffd\u8e2a\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u51b3\u7b56\u673a\u5236\u6d88\u9664\u4eba\u5de5\u63d0\u793a\u5de5\u7a0b\u9700\u6c42\uff0c\u652f\u6301\u8de8\u5de5\u4f5c\u6d41\u7684\u4e0a\u4e0b\u6587\u7ef4\u62a4\u548c\u6545\u969c\u6062\u590d\u3002", "result": "\u5728\u5fae\u6ce2\u5408\u6210\u5668\u7269\u7406\u786c\u4ef6\u548cPubChem\u6570\u636e\u5e93\u865a\u62df\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u53ef\u9760\u7684\u591a\u6b65\u751f\u7269\u6d4b\u5b9a\u68c0\u7d22\uff0c\u7cfb\u7edf\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u4ee3\u7406\u5177\u5907\u81ea\u9002\u5e94\u89c4\u5212\u548c\u53ef\u89e3\u91ca\u72b6\u6001\u8f6c\u6362\u80fd\u529b\u3002", "conclusion": "\u5185\u5b58\u7ba1\u7406\u548c\u72b6\u6001\u611f\u77e5\u662f\u667a\u80fd\u4f53\u89c4\u5212\u53ef\u9760\u6027\u7684\u5173\u952e\uff0cSciBORG\u4e3a\u590d\u6742\u73af\u5883\u4e2dAI\u4ee3\u7406\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u63a8\u5e7f\u7684\u57fa\u7840\u67b6\u6784\u3002"}}
{"id": "2507.00082", "pdf": "https://arxiv.org/pdf/2507.00082", "abs": "https://arxiv.org/abs/2507.00082", "authors": ["Faranaksadat Solat", "Joohyung Lee", "Mohamed Seif", "Dusit Niyato", "H. Vincent Poor"], "title": "Federated Learning-Enabled Hybrid Language Models for Communication-Efficient Token Transmission", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "17 pages, 16 figures, IEEE Internet of Things", "summary": "Hybrid Language Models (HLMs) combine the low-latency efficiency of Small\nLanguage Models (SLMs) on edge devices with the high accuracy of Large Language\nModels (LLMs) on centralized servers. Unlike traditional end-to-end LLM\ninference, HLMs reduce latency and communication by invoking LLMs only when\nlocal SLM predictions are uncertain, i.e., when token-level confidence is low\nor entropy is high. However, ambiguous or low-confidence predictions still\nrequire frequent offloading to the LLM, leading to significant communication\noverhead in bandwidth-constrained settings. To address this, we propose FedHLM,\na communication-efficient HLM framework that integrates uncertainty-aware\ninference with Federated Learning (FL). FedHLM's key innovation lies in\ncollaboratively learning token-level uncertainty thresholds that govern when\nLLM assistance is needed. Rather than using static or manually tuned\nthresholds, FedHLM employs FL to optimize these thresholds in a\nprivacy-preserving, distributed manner. Additionally, it leverages\nembedding-based token representations for Peer-to-Peer (P2P) resolution,\nenabling clients to reuse tokens inferred by semantically similar peers without\nengaging the LLM. We further introduce hierarchical model aggregation: edge\nservers refine local routing policies through client updates, while\ncross-cluster coordination aligns global decision boundaries. This layered\ndesign captures recurring uncertainty patterns, reducing redundant LLM queries.\nExperiments on large-scale news classification tasks show that FedHLM reduces\nLLM transmissions by over 95 percent with negligible accuracy loss, making it\nwell-suited for scalable and efficient edge-AI applications.", "AI": {"tldr": "\u63d0\u51fa\u878d\u5408\u8054\u90a6\u5b66\u4e60\u7684\u6df7\u5408\u8bed\u8a00\u6a21\u578b\u6846\u67b6FedHLM\uff0c\u901a\u8fc7\u534f\u4f5c\u5b66\u4e60\u4e0d\u786e\u5b9a\u6027\u9608\u503c\u548c\u5206\u5c42\u6a21\u578b\u805a\u5408\uff0c\u51cf\u5c1195%\u7684\u5927\u6a21\u578b\u901a\u4fe1\u5f00\u9500", "motivation": "\u73b0\u6709\u6df7\u5408\u8bed\u8a00\u6a21\u578b\uff08HLM\uff09\u5728\u4f4e\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u65f6\u4ecd\u9700\u9891\u7e41\u8c03\u7528\u4e91\u7aef\u5927\u6a21\u578b\uff08LLM\uff09\uff0c\u5bfc\u81f4\u5e26\u5bbd\u53d7\u9650\u573a\u666f\u4e0b\u901a\u4fe1\u5f00\u9500\u8fc7\u9ad8\u3002\u9700\u8981\u4e00\u79cd\u517c\u987e\u8ba1\u7b97\u6548\u7387\u548c\u901a\u4fe1\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u8054\u90a6\u5b66\u4e60\u534f\u4f5c\u4f18\u5316\u8bcd\u5143\u7ea7\u4e0d\u786e\u5b9a\u6027\u9608\u503c\n2. \u57fa\u4e8e\u5d4c\u5165\u8868\u793a\u7684P2P\u8bed\u4e49\u89e3\u6790\u673a\u5236\u590d\u7528\u76f8\u4f3c\u8bcd\u5143\n3. \u5206\u5c42\u6a21\u578b\u805a\u5408\uff1a\u8fb9\u7f18\u670d\u52a1\u5668\u4f18\u5316\u672c\u5730\u8def\u7531\u7b56\u7565\uff0c\u8de8\u96c6\u7fa4\u534f\u8c03\u5168\u5c40\u51b3\u7b56\u8fb9\u754c", "result": "\u5728\u5927\u89c4\u6a21\u65b0\u95fb\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cFedHLM\u5728\u7cbe\u5ea6\u635f\u5931\u53ef\u5ffd\u7565\u7684\u524d\u63d0\u4e0b\uff0c\u6210\u529f\u964d\u4f4e95%\u4ee5\u4e0a\u7684\u5927\u6a21\u578b\u4f20\u8f93\u91cf", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5206\u5c42\u8bbe\u8ba1\u6355\u83b7\u91cd\u590d\u51fa\u73b0\u7684\u4e0d\u786e\u5b9a\u6027\u6a21\u5f0f\uff0c\u663e\u8457\u51cf\u5c11\u5197\u4f59\u7684\u5927\u6a21\u578b\u67e5\u8be2\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18AI\u573a\u666f\u7684\u89c4\u6a21\u5316\u90e8\u7f72\u9700\u6c42"}}
{"id": "2507.00092", "pdf": "https://arxiv.org/pdf/2507.00092", "abs": "https://arxiv.org/abs/2507.00092", "authors": ["Basab Jha", "Firoj Paudel", "Ujjwal Puri", "Zhang Yuting", "Choi Donghyuk", "Wang Junhao"], "title": "Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "19 pages, 2 figures, 9 tables", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities at\nsolving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but\ntheir decision-making processes remain somewhat blackbox. We introduce\ntextbfinverse reasoning, a novel paradigm enabling LLMs to decompose and\nexplain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a\n4-billion-parameter reasoning model, employs a metacognitive structure that\nreflects back via attention processes to identify major decision points and\ngenerate explanations of reasoning choices. While typical CoT approaches are\ndirected towards forward reasoning generation, inverse reasoning provides\ninsight into why specific reasoning chains were selected over others. Through\nthorough testing of logical reasoning puzzles, math problems and ethical\ndilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, we\ndemonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy\n(74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) for\nits task, and offers performance almost on par with models like Claude-3.5\nSonnet or GPT-4o. Our contributions are: (i) the first rigorous framework for\nLLM self-reflection via inverse reasoning, (ii) a novel metalearning framework\nto reverse the attention flow, (iii) comprehensive evaluation frameworks for\nreasoning transparency, and (iv) evidence that increasing reasoning using\ninverse reasoning improves interpretability along with reasoning performance.\nOur work creates new avenues for transparent AI systems and closes significant\ngaps in AI safety, education, and scientific discovery.", "AI": {"tldr": "\u63d0\u51fa\u9006\u5411\u63a8\u7406\u6846\u67b6SAGE-nano\uff0c\u901a\u8fc7\u5143\u8ba4\u77e5\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0LLM\u81ea\u6211\u89e3\u91ca\uff0c\u5728\u4fdd\u6301\u9ad8\u63a8\u7406\u51c6\u786e\u7387\uff0874.6%\uff09\u7684\u540c\u65f6\u83b7\u5f9792.1%\u7684\u4eba\u7c7b\u89e3\u91ca\u8ba4\u53ef\u5ea6\uff0c\u63a5\u8fd1GPT-4o\u6c34\u5e73\u3002", "motivation": "\u4f20\u7edf\u601d\u7ef4\u94fe\uff08CoT\uff09\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u63a8\u7406\u8def\u5f84\u9009\u62e9\u4f9d\u636e\u7684\u89e3\u91ca\uff0c\u5bfc\u81f4LLM\u51b3\u7b56\u8fc7\u7a0b\u5b58\u5728\u9ed1\u7bb1\u95ee\u9898\uff0c\u963b\u788dAI\u900f\u660e\u5316\u4e0e\u5b89\u5168\u53d1\u5c55\u3002", "method": "\u9006\u5411\u63a8\u7406\u8303\u5f0f+\u6ce8\u610f\u529b\u53cd\u5411\u6d41\u52a8\u5206\u6790\uff1a1\uff09\u6784\u5efa\u5143\u8ba4\u77e5\u7ed3\u6784\u5206\u89e3\u51b3\u7b56\u8282\u70b9\uff1b2\uff09\u901a\u8fc7\u6ce8\u610f\u529b\u6743\u91cd\u56de\u6eaf\u8bc6\u522b\u5173\u952e\u63a8\u7406\u6b65\u9aa4\uff1b3\uff09\u81ea\u52a8\u751f\u6210\u591a\u7ef4\u5ea6\u89e3\u91ca\u3002", "result": "\u5728AQUA-RAT/CommonsenseQA\u6d4b\u8bd5\u4e2d\uff1a\u63a8\u7406\u51c6\u786e\u738774.6%\uff08\u63d0\u534712.3%\uff09\uff0c\u4eba\u7c7b\u89e3\u91ca\u8ba4\u53ef\u5ea692.1%\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4GPT-4\u5feb1.8\u500d\u3002", "conclusion": "\u5f00\u521bLLM\u81ea\u6211\u89e3\u91ca\u65b0\u8303\u5f0f\uff0c\u9a8c\u8bc1\u9006\u5411\u63a8\u7406\u53ef\u540c\u6b65\u63d0\u5347\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\uff0c\u4e3aAI\u5b89\u5168/\u6559\u80b2/\u79d1\u7814\u63d0\u4f9b\u65b0\u65b9\u6cd5\u8bba\uff0c\u7f29\u5c0f\u4e0e\u9876\u7ea7\u95ed\u6e90\u6a21\u578b\u7684\u89e3\u91ca\u6027\u5dee\u8ddd\u3002"}}
{"id": "2507.00234", "pdf": "https://arxiv.org/pdf/2507.00234", "abs": "https://arxiv.org/abs/2507.00234", "authors": ["Jiztom Kavalakkatt Francis", "Matthew J Darr"], "title": "Interpretable AI for Time-Series: Multi-Model Heatmap Fusion with Global Attention and NLP-Generated Explanations", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "13 pages", "summary": "In this paper, we present a novel framework for enhancing model\ninterpretability by integrating heatmaps produced separately by ResNet and a\nrestructured 2D Transformer with globally weighted input saliency. We address\nthe critical problem of spatial-temporal misalignment in existing\ninterpretability methods, where convolutional networks fail to capture global\ncontext and Transformers lack localized precision - a limitation that impedes\nactionable insights in safety-critical domains like healthcare and industrial\nmonitoring. Our method merges gradient-weighted activation maps (ResNet) and\nTransformer attention rollout into a unified visualization, achieving full\nspatial-temporal alignment while preserving real-time performance. Empirical\nevaluations on clinical (ECG arrhythmia detection) and industrial (energy\nconsumption prediction) datasets demonstrate significant improvements: the\nhybrid framework achieves 94.1% accuracy (F1 0.93) on the PhysioNet dataset and\nreduces regression error to RMSE = 0.28 kWh (R2 = 0.95) on the UCI Energy\nAppliance dataset-outperforming standalone ResNet, Transformer, and\nInceptionTime baselines by 3.8-12.4%. An NLP module translates fused heatmaps\ninto domain-specific narratives (e.g., \"Elevated ST-segment between 2-4 seconds\nsuggests myocardial ischemia\"), validated via BLEU-4 (0.586) and ROUGE-L\n(0.650) scores. By formalizing interpretability as causal fidelity and\nspatial-temporal alignment, our approach bridges the gap between technical\noutputs and stakeholder understanding, offering a scalable solution for\ntransparent, time-aware decision-making.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408ResNet\u548cTransformer\u70ed\u529b\u56fe\u7684\u65b0\u578b\u89e3\u91ca\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u7a7a\u5bf9\u9f50\u6280\u672f\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\uff0c\u5728\u533b\u7597\u548c\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u5377\u79ef\u7f51\u7edc\u7f3a\u4e4f\u5168\u5c40\u4e0a\u4e0b\u6587\u3001Transformer\u7f3a\u4e4f\u5c40\u90e8\u7cbe\u5ea6\u7684\u65f6\u7a7a\u9519\u4f4d\u95ee\u9898\uff0c\u8be5\u5c40\u9650\u5728\u533b\u7597\u548c\u5de5\u4e1a\u76d1\u6d4b\u7b49\u5b89\u5168\u5173\u952e\u9886\u57df\u963b\u788d\u6709\u6548\u51b3\u7b56", "method": "\u878d\u5408\u68af\u5ea6\u52a0\u6743\u6fc0\u6d3b\u56fe\uff08ResNet\uff09\u4e0eTransformer\u6ce8\u610f\u529b\u5c55\u5f00\u6280\u672f\uff0c\u901a\u8fc7NLP\u6a21\u5757\u5c06\u878d\u5408\u70ed\u529b\u56fe\u8f6c\u5316\u4e3a\u9886\u57df\u7279\u5b9a\u89e3\u91ca", "result": "PhysioNet\u6570\u636e\u96c6\u51c6\u786e\u738794.1%\uff08F1 0.93\uff09\uff0cUCI\u80fd\u6e90\u6570\u636e\u96c6\u56de\u5f52\u8bef\u5deeRMSE=0.28kWh\uff08R\u00b2=0.95\uff09\uff0c\u8f83\u57fa\u7ebf\u6a21\u578b\u63d0\u53473.8-12.4%", "conclusion": "\u901a\u8fc7\u5f62\u5f0f\u5316\u53ef\u89e3\u91ca\u6027\u4e3a\u56e0\u679c\u4fdd\u771f\u4e0e\u65f6\u7a7a\u5bf9\u9f50\uff0c\u67b6\u8d77\u6280\u672f\u8f93\u51fa\u4e0e\u4e1a\u52a1\u7406\u89e3\u7684\u6865\u6881\uff0c\u4e3a\u65f6\u95f4\u654f\u611f\u7684\u900f\u660e\u51b3\u7b56\u63d0\u4f9b\u53ef\u6269\u5c55\u65b9\u6848"}}
{"id": "2507.00248", "pdf": "https://arxiv.org/pdf/2507.00248", "abs": "https://arxiv.org/abs/2507.00248", "authors": ["Nikita Nikitin", "Eugene Fomin"], "title": "Developing Lightweight DNN Models With Limited Data For Real-Time Sign Language Recognition", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "7 pages, 2 figures, 2 tables, for associated mpeg file, see\n  https://slait.app/static/Screen_Recording.mp4", "summary": "We present a novel framework for real-time sign language recognition using\nlightweight DNNs trained on limited data. Our system addresses key challenges\nin sign language recognition, including data scarcity, high computational\ncosts, and discrepancies in frame rates between training and inference\nenvironments. By encoding sign language specific parameters, such as handshape,\npalm orientation, movement, and location into vectorized inputs, and leveraging\nMediaPipe for landmark extraction, we achieve highly separable input data\nrepresentations. Our DNN architecture, optimized for sub 10MB deployment,\nenables accurate classification of 343 signs with less than 10ms latency on\nedge devices. The data annotation platform 'slait data' facilitates structured\nlabeling and vector extraction. Our model achieved 92% accuracy in isolated\nsign recognition and has been integrated into the 'slait ai' web application,\nwhere it demonstrates stable inference.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8f7b\u91cf\u7ea7DNN\u548cMediaPipe\u7684\u5b9e\u65f6\u624b\u8bed\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u53c2\u6570\u7f16\u7801\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u4e0e\u8ba1\u7b97\u74f6\u9888\uff0c\u5728343\u4e2a\u624b\u52bf\u8bc6\u522b\u4e2d\u8fbe\u523092%\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u624b\u8bed\u8bc6\u522b\u9886\u57df\u7684\u6570\u636e\u532e\u4e4f\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u8bad\u7ec3\u63a8\u7406\u5e27\u7387\u5dee\u5f02\u7b49\u5b9e\u9645\u90e8\u7f72\u74f6\u9888\uff0c\u63a8\u52a8\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u5e94\u7528\u3002", "method": "1. \u5c06\u624b\u8bed\u7279\u5f81\u53c2\u6570\u5411\u91cf\u5316\u8f93\u5165\n2. \u91c7\u7528MediaPipe\u8fdb\u884c\u5173\u952e\u70b9\u63d0\u53d6\n3. \u8bbe\u8ba1\u4e9a10MB\u7684\u8f7b\u91cfDNN\u67b6\u6784\n4. \u5f00\u53d1'slait data'\u6570\u636e\u6807\u6ce8\u5e73\u53f0\u5b9e\u73b0\u7ed3\u6784\u5316\u5904\u7406", "result": "\u5728\u8fb9\u7f18\u8bbe\u5907\u5b9e\u73b010ms\u5ef6\u8fdf\u7684\u5b9e\u65f6\u63a8\u7406\uff0c\u6a21\u578b\u51c6\u786e\u7387\u8fbe92%\uff0c\u5e76\u6210\u529f\u96c6\u6210\u81f3\u7f51\u9875\u5e94\u7528'slait ai'\u4e2d\u7a33\u5b9a\u8fd0\u884c\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u8f7b\u91cf\u5316\u6a21\u578b\u4e0e\u4e13\u7528\u6570\u636e\u5904\u7406\u5de5\u5177\u7684\u7ed3\u5408\uff0c\u6709\u6548\u7a81\u7834\u4e86\u624b\u8bed\u8bc6\u522b\u5b9e\u9645\u5e94\u7528\u7684\u5173\u952e\u6280\u672f\u969c\u788d\uff0c\u4e3a\u5b9e\u65f6\u4ea4\u4e92\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2507.00310", "pdf": "https://arxiv.org/pdf/2507.00310", "abs": "https://arxiv.org/abs/2507.00310", "authors": ["Dhruv Agarwal", "Bodhisattwa Prasad Majumder", "Reece Adamson", "Megha Chakravorty", "Satvika Reddy Gavireddy", "Aditya Parashar", "Harshit Surana", "Bhavana Dalvi Mishra", "Andrew McCallum", "Ashish Sabharwal", "Peter Clark"], "title": "Open-ended Scientific Discovery via Bayesian Surprise", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The promise of autonomous scientific discovery (ASD) hinges not only on\nanswering questions, but also on knowing which questions to ask. Most recent\nworks in ASD explore the use of large language models (LLMs) in goal-driven\nsettings, relying on human-specified research questions to guide hypothesis\ngeneration. However, scientific discovery may be accelerated further by\nallowing the AI system to drive exploration by its own criteria. The few\nexisting approaches in open-ended ASD select hypotheses based on diversity\nheuristics or subjective proxies for human interestingness, but the former\nstruggles to meaningfully navigate the typically vast hypothesis space, and the\nlatter suffers from imprecise definitions. This paper presents AutoDS -- a\nmethod for open-ended ASD that instead drives scientific exploration using\nBayesian surprise. Here, we quantify the epistemic shift from the LLM's prior\nbeliefs about a hypothesis to its posterior beliefs after gathering\nexperimental results. To efficiently explore the space of nested hypotheses,\nour method employs a Monte Carlo tree search (MCTS) strategy with progressive\nwidening using surprisal as the reward function. We evaluate AutoDS in the\nsetting of data-driven discovery across 21 real-world datasets spanning domains\nsuch as biology, economics, finance, and behavioral science. Our results\ndemonstrate that under a fixed budget, AutoDS substantially outperforms\ncompetitors by producing 5--29\\% more discoveries deemed surprising by the LLM.\nOur human evaluation further finds that two-thirds of AutoDS discoveries are\nsurprising to the domain experts, suggesting this is an important step forward\ntowards building open-ended ASD systems.", "AI": {"tldr": "\u63d0\u51faAutoDS\u65b9\u6cd5\uff0c\u5229\u7528\u8d1d\u53f6\u65af\u60ca\u559c\u91cf\u5316\u8ba4\u77e5\u53d8\u5316\u5e76\u7ed3\u5408\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u5f00\u653e\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u6548\u7387\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u53d1\u73b0\u6570\u91cf\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u53475-29%\u3002", "motivation": "\u73b0\u6709\u5f00\u653e\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u65b9\u6cd5\u5b58\u5728\u5047\u8bbe\u7a7a\u95f4\u5bfc\u822a\u80fd\u529b\u4e0d\u8db3\u548c\u4e3b\u89c2\u8bc4\u4ef7\u6807\u51c6\u6a21\u7cca\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u7cfb\u7edf\u81ea\u9a71\u63a2\u7d22\u673a\u5236\u3002", "method": "\u901a\u8fc7\u8d1d\u53f6\u65af\u60ca\u559c\u503c\u91cf\u5316LLM\u5b9e\u9a8c\u524d\u540e\u8ba4\u77e5\u5dee\u5f02\uff0c\u5c06\u5176\u4f5c\u4e3a\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7684\u5956\u52b1\u51fd\u6570\uff0c\u91c7\u7528\u6e10\u8fdb\u6269\u5c55\u7b56\u7565\u5b9e\u73b0\u9ad8\u6548\u5047\u8bbe\u7a7a\u95f4\u63a2\u7d22\u3002", "result": "\u572821\u4e2a\u8de8\u9886\u57df\u771f\u5b9e\u6570\u636e\u96c6\u4e2d\uff0cAutoDS\u4ea7\u751f\u7684\u8ba9LLM\u60ca\u8bb6\u7684\u53d1\u73b0\u6570\u91cf\u8d85\u57fa\u51c6\u6a21\u578b29%\uff0c\u9886\u57df\u4e13\u5bb6\u8ba4\u53ef\u5176\u4e2d66.7%\u53d1\u73b0\u5177\u6709\u65b0\u9896\u6027\u3002", "conclusion": "\u57fa\u4e8e\u8ba4\u77e5\u53d8\u5316\u7684\u91cf\u5316\u63a2\u7d22\u673a\u5236\u4e3a\u5f00\u653e\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u8d1d\u53f6\u65af\u60ca\u559c\u9a71\u52a8\u7684\u641c\u7d22\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u53d1\u73b0\u80fd\u529b\uff0c\u63a8\u52a8AI\u81ea\u4e3b\u79d1\u7814\u8fdb\u7a0b\u3002"}}
{"id": "2507.00316", "pdf": "https://arxiv.org/pdf/2507.00316", "abs": "https://arxiv.org/abs/2507.00316", "authors": ["Siyou Li", "Pengyao Qin", "Huanan Wu", "Dong Nie", "Arun J. Thirunavukarasu", "Juntao Yu", "Le Zhang"], "title": "$\u03bc^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation", "categories": ["cs.LG", "cs.CL", "eess.IV"], "comment": "Accepted by MICCAI 2025", "summary": "Automated radiology report generation (RRG) aims to produce detailed textual\nreports from clinical imaging, such as computed tomography (CT) scans, to\nimprove the accuracy and efficiency of diagnosis and provision of management\nadvice. RRG is complicated by two key challenges: (1) inherent complexity in\nextracting relevant information from imaging data under resource constraints,\nand (2) difficulty in objectively evaluating discrepancies between\nmodel-generated and expert-written reports. To address these challenges, we\npropose $\\mu^2$LLM, a $\\underline{\\textbf{mu}}$ltiscale\n$\\underline{\\textbf{mu}}$ltimodal large language models for RRG tasks. The\nnovel ${\\mu}^2$Tokenizer, as an intermediate layer, integrates multi-modal\nfeatures from the multiscale visual tokenizer and the text tokenizer, then\nenhances report generation quality through direct preference optimization\n(DPO), guided by GREEN-RedLlama. Experimental results on four large CT\nimage-report medical datasetdemonstrate that our method outperforms existing\napproaches, highlighting the potential of our fine-tuned $\\mu^2$LLMs on limited\ndata for RRG tasks.", "AI": {"tldr": "\u63d0\u51fa\u591a\u5c3a\u5ea6\u591a\u6a21\u6001\u5927\u6a21\u578b\u03bc\u00b2LLM\uff0c\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u4e0e\u6587\u672c\u7279\u5f81\u53ca\u504f\u597d\u4f18\u5316\uff0c\u6709\u6548\u63d0\u5347CT\u5f71\u50cf\u62a5\u544a\u7684\u751f\u6210\u8d28\u91cf", "motivation": "\u89e3\u51b3\u653e\u5c04\u62a5\u544a\u751f\u6210\u4e2d\u4fe1\u606f\u63d0\u53d6\u590d\u6742\u5ea6\u9ad8\u3001\u4eba\u5de5\u8bc4\u4f30\u5dee\u5f02\u5927\u7684\u75db\u70b9\uff0c\u6574\u5408\u591a\u6a21\u6001\u7279\u5f81\u63d0\u5347\u751f\u6210\u8d28\u91cf", "method": "\u8bbe\u8ba1\u03bc\u00b2Tokenizer\u4e2d\u95f4\u5c42\u878d\u5408\u591a\u5c3a\u5ea6\u89c6\u89c9\u7279\u5f81\u4e0e\u6587\u672c\u7279\u5f81\uff0c\u91c7\u7528GREEN-RedLlama\u6307\u5bfc\u7684\u76f4\u63a5\u504f\u597d\u4f18\u5316(DPO)\u6846\u67b6", "result": "\u5728\u56db\u4e2a\u5927\u578bCT\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u8bc1\u660e\u6709\u9650\u6570\u636e\u4e0b\u5fae\u8c03\u6a21\u578b\u7684\u4f18\u8d8a\u6027", "conclusion": "\u03bc\u00b2LLM\u6846\u67b6\u663e\u8457\u63d0\u5347\u653e\u5c04\u62a5\u544a\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4e3a\u533b\u5b66\u5f71\u50cf\u5206\u6790\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2507.00417", "pdf": "https://arxiv.org/pdf/2507.00417", "abs": "https://arxiv.org/abs/2507.00417", "authors": ["Joongwon Kim", "Anirudh Goyal", "Liang Tan", "Hannaneh Hajishirzi", "Srinivasan Iyer", "Tianlu Wang"], "title": "ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context", "categories": ["cs.AI", "cs.CL"], "comment": "36 pages, 23 figures", "summary": "We introduce ASTRO, the \"Autoregressive Search-Taught Reasoner\", a framework\nfor training language models to reason like search algorithms, explicitly\nleveraging self-reflection, backtracking, and exploration in their outputs.\nRecently, training large language models (LLMs) via reinforcement learning (RL)\nhas led to the advent of reasoning models with greatly enhanced reasoning\ncapabilities. Open-source replications of reasoning models, while successful,\nbuild upon models that already exhibit strong reasoning capabilities along with\nsearch behavior observed even before RL. As a result, it is yet unclear how to\nboost the reasoning capabilities of other non-reasoner models including Llama\n3. ASTRO teaches such models to internalize structured search behavior through\na synthetic dataset derived from Monte Carlo Tree Search (MCTS) over\nmathematical problem-solving trajectories. By converting search traces into\nnatural language chain-of-thoughts that capture both successes and recoveries\nfrom failure, ASTRO bootstraps models with a rich prior for exploration during\nRL. We finetune our models on these search-derived traces and further improve\nperformance via RL with verifiable rewards. We apply ASTRO to the Llama 3\nfamily of models and achieve absolute performance gains of 16.0% on MATH-500,\n26.9% on AMC 2023, and 20.0% on AIME 2024, especially improving upon\nchallenging problems that require iterative correction. Our results demonstrate\nthat search-inspired training offers a principled way to instill robust\nreasoning capabilities into open LLMs.", "AI": {"tldr": "ASTRO\u6846\u67b6\u901a\u8fc7\u641c\u7d22\u7b97\u6cd5\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u81ea\u6211\u53cd\u601d/\u56de\u6eaf/\u63a2\u7d22\uff0c\u663e\u8457\u63d0\u5347Llama 3\u5728\u6570\u5b66\u96be\u9898\u4e0a\u7684\u63a8\u7406\u8868\u73b0\uff08MATH-500 +16%\uff0cAMC 2023 +26.9%\uff0cAIME 2024 +20%\uff09", "motivation": "\u73b0\u6709\u5f00\u6e90\u63a8\u7406\u6a21\u578b\u4f9d\u8d56\u9884\u8bad\u7ec3\u9636\u6bb5\u7684\u641c\u7d22\u884c\u4e3a\uff0c\u975e\u63a8\u7406\u6a21\u578b\uff08\u5982Llama 3\uff09\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u63a8\u7406\u8bad\u7ec3\u65b9\u6cd5", "method": "1. \u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u751f\u6210\u6570\u5b66\u95ee\u9898\u6c42\u89e3\u8f68\u8ff9\n2. \u5c06\u641c\u7d22\u8f68\u8ff9\u8f6c\u5316\u4e3a\u5305\u542b\u6210\u529f/\u5931\u8d25\u6062\u590d\u7684\u81ea\u7136\u8bed\u8a00\u601d\u7ef4\u94fe\n3. \u7ed3\u5408\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u5fae\u8c03", "result": "\u5728\u9700\u8981\u8fed\u4ee3\u4fee\u6b63\u7684\u96be\u9898\u4e0a\u6548\u679c\u663e\u8457\uff1a\n- MATH-500\u63d0\u534716%\n- AMC 2023\u63d0\u534726.9%\n- AIME 2024\u63d0\u534720%", "conclusion": "\u641c\u7d22\u542f\u53d1\u7684\u8bad\u7ec3\u65b9\u6cd5\u4e3a\u5f00\u6e90\u5927\u6a21\u578b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u63a8\u7406\u80fd\u529b\u63d0\u5347\u8def\u5f84\uff0c\u8bc1\u660e\u7ed3\u6784\u5316\u641c\u7d22\u884c\u4e3a\u5185\u5316\u5bf9\u590d\u6742\u95ee\u9898\u89e3\u51b3\u7684\u6709\u6548\u6027"}}
{"id": "2507.00425", "pdf": "https://arxiv.org/pdf/2507.00425", "abs": "https://arxiv.org/abs/2507.00425", "authors": ["Ruixiang Zhang", "Shuangfei Zhai", "Jiatao Gu", "Yizhe Zhang", "Huangjie Zheng", "Tianrong Chen", "Miguel Angel Bautista", "Josh Susskind", "Navdeep Jaitly"], "title": "Flexible Language Modeling in Continuous Space with Transformer-based Autoregressive Flows", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Autoregressive models have driven remarkable progress in language modeling.\nTheir foundational reliance on discrete tokens, unidirectional context, and\nsingle-pass decoding, while central to their success, also inspires the\nexploration of a design space that could offer new axes of modeling\nflexibility. In this work, we explore an alternative paradigm, shifting\nlanguage modeling from a discrete token space to a continuous latent space. We\npropose a novel framework TarFlowLM, that employs transformer-based\nautoregressive normalizing flows to model these continuous representations.\nThis approach unlocks substantial flexibility, enabling the construction of\nmodels that can capture global bi-directional context through stacked,\nalternating-direction autoregressive transformations, support block-wise\ngeneration with flexible token patch sizes, and facilitate a hierarchical\nmulti-pass generation process. We further propose new mixture-based coupling\ntransformations designed to capture complex dependencies within the latent\nspace shaped by discrete data, and demonstrate theoretical connections to\nconventional discrete autoregressive models. Extensive experiments on language\nmodeling benchmarks demonstrate strong likelihood performance and highlight the\nflexible modeling capabilities inherent in our framework.", "AI": {"tldr": "\u63d0\u51faTarFlowLM\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8etransformer\u7684\u81ea\u56de\u5f52\u5f52\u4e00\u5316\u6d41\u5728\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u8fdb\u884c\u8bed\u8a00\u5efa\u6a21\uff0c\u7a81\u7834\u4f20\u7edf\u79bb\u6563token\u6a21\u578b\u7684\u9650\u5236", "motivation": "\u4f20\u7edf\u81ea\u56de\u5f52\u6a21\u578b\u53d7\u9650\u4e8e\u79bb\u6563token\u3001\u5355\u5411\u4e0a\u4e0b\u6587\u548c\u5355\u6b21\u89e3\u7801\uff0c\u9700\u63a2\u7d22\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u4ee5\u83b7\u53d6\u53cc\u5411\u4e0a\u4e0b\u6587\u5efa\u6a21\u3001\u7075\u6d3btoken\u5757\u751f\u6210\u548c\u591a\u8f6e\u8fed\u4ee3\u751f\u6210\u80fd\u529b", "method": "\u91c7\u7528transformer\u81ea\u56de\u5f52\u5f52\u4e00\u5316\u6d41\u5efa\u6a21\u8fde\u7eed\u8868\u5f81\uff0c\u8bbe\u8ba1\u6df7\u5408\u8026\u5408\u53d8\u6362\u6355\u6349\u6f5c\u5728\u7a7a\u95f4\u590d\u6742\u4f9d\u8d56\uff0c\u5efa\u7acb\u4e0e\u79bb\u6563\u6a21\u578b\u7684\u6570\u5b66\u5173\u8054", "result": "\u5728\u8bed\u8a00\u5efa\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u5f3a\u4f3c\u7136\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u5728\u53cc\u5411\u4e0a\u4e0b\u6587\u5efa\u6a21\u3001\u5757\u72b6\u751f\u6210\u548c\u5c42\u7ea7\u591a\u8f6e\u751f\u6210\u65b9\u9762\u7684\u7075\u6d3b\u6027", "conclusion": "TarFlowLM\u4e3a\u8bed\u8a00\u5efa\u6a21\u5f00\u8f9f\u4e86\u8fde\u7eed\u7a7a\u95f4\u5efa\u6a21\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u4ea4\u66ff\u65b9\u5411\u81ea\u56de\u5f52\u53d8\u6362\u5b9e\u73b0\u4e86\u4f20\u7edf\u79bb\u6563\u6a21\u578b\u65e0\u6cd5\u8fbe\u5230\u7684\u5168\u5c40\u4e0a\u4e0b\u6587\u6355\u83b7\u80fd\u529b"}}
{"id": "2507.00432", "pdf": "https://arxiv.org/pdf/2507.00432", "abs": "https://arxiv.org/abs/2507.00432", "authors": ["Maggie Huan", "Yuetai Li", "Tuney Zheng", "Xiaoyu Xu", "Seungone Kim", "Minxin Du", "Radha Poovendran", "Graham Neubig", "Xiang Yue"], "title": "Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Math reasoning has become the poster child of progress in large language\nmodels (LLMs), with new models rapidly surpassing human-level performance on\nbenchmarks like MATH and AIME. But as math leaderboards improve week by week,\nit is worth asking: do these gains reflect broader problem-solving ability or\njust narrow overfitting? To answer this question, we evaluate over 20\nopen-weight reasoning-tuned models across a broad suite of tasks, including\nmath, scientific QA, agent planning, coding, and standard\ninstruction-following. We surprisingly find that most models that succeed in\nmath fail to transfer their gains to other domains. To rigorously study this\nphenomenon, we conduct controlled experiments on Qwen3-14B models using\nmath-only data but different tuning methods. We find that reinforcement\nlearning (RL)-tuned models generalize well across domains, while supervised\nfine-tuning (SFT)-tuned models often forget general capabilities. Latent-space\nrepresentation and token-space distribution shift analyses reveal that SFT\ninduces substantial representation and output drift, while RL preserves\ngeneral-domain structure. Our results suggest a need to rethink standard\npost-training recipes, particularly the reliance on SFT-distilled data for\nadvancing reasoning models.", "AI": {"tldr": "\u6570\u5b66\u63a8\u7406\u6027\u80fd\u63d0\u5347\u672a\u5fc5\u53cd\u6620\u6574\u4f53\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u5f3a\u5316\u5b66\u4e60\u8c03\u4f18\u6a21\u578b\u5c55\u73b0\u8de8\u9886\u57df\u6cdb\u5316\u4f18\u52bf\uff0c\u76d1\u7763\u5fae\u8c03\u5bfc\u81f4\u80fd\u529b\u9057\u5fd8", "motivation": "\u63a2\u7a76\u6570\u5b66\u6a21\u578b\u5728\u6570\u5b66\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\u63d0\u5347\u662f\u5426\u5177\u6709\u9886\u57df\u6cdb\u5316\u6027\uff0c\u8fd8\u662f\u5c40\u9650\u4e8e\u7279\u5b9a\u4efb\u52a1\u7684\u8fc7\u62df\u5408", "method": "\u901a\u8fc7\u8de8\u9886\u57df\u8bc4\u4f30\uff08\u6570\u5b66/\u79d1\u5b66QA/\u89c4\u5212/\u7f16\u7801\uff09\u3001\u63a7\u5236\u5b9e\u9a8c\uff08\u4e0d\u540c\u8c03\u4f18\u65b9\u6cd5\u7684Qwen3-14B\u6a21\u578b\uff09\u53ca\u6f5c\u5728\u7a7a\u95f4\u8868\u5f81\u5206\u6790", "result": "\u6570\u5b66\u4f18\u52bf\u6a21\u578b\u666e\u904d\u5b58\u5728\u9886\u57df\u8fc1\u79fb\u969c\u788d\uff0cRL\u8c03\u4f18\u4fdd\u7559\u901a\u7528\u80fd\u529b\uff08+13.5%\u8de8\u9886\u57df\u63d0\u5347\uff09\uff0cSFT\u5bfc\u81f4\u8868\u5f81\u6f02\u79fb\u548c\u6027\u80fd\u4e0b\u964d", "conclusion": "\u9700\u91cd\u65b0\u8bbe\u8ba1\u6a21\u578b\u8bad\u7ec3\u8303\u5f0f\uff0c\u51cf\u5c11\u5bf9\u76d1\u7763\u5fae\u8c03\u84b8\u998f\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5f3a\u5316\u5b66\u4e60\u5c55\u73b0\u51fa\u66f4\u4f18\u7684\u77e5\u8bc6\u8fc1\u79fb\u7279\u6027"}}
{"id": "2507.00449", "pdf": "https://arxiv.org/pdf/2507.00449", "abs": "https://arxiv.org/abs/2507.00449", "authors": ["Zhihao Zhan", "Jianan Zhao", "Zhaocheng Zhu", "Jian Tang"], "title": "Overcoming Long-Context Limitations of State-Space Models via Context-Dependent Sparse Attention", "categories": ["cs.LG", "cs.CL", "I.2.7"], "comment": "Proceedings of the 42nd International Conference on Machine Learning,\n  ES-FoMo III: 3rd Workshop on Efficient Systems for Foundation Models, 18\n  pages, 9 figures", "summary": "Efficient long-context modeling remains a critical challenge for natural\nlanguage processing (NLP), as the time complexity of the predominant\nTransformer architecture scales quadratically with the sequence length. While\nstate-space models (SSMs) offer alternative sub-quadratic solutions, they\nstruggle to capture long-range dependencies effectively. In this work, we focus\non analyzing and improving the long-context modeling capabilities of SSMs. We\nshow that the widely used synthetic task, associative recall, which requires a\nmodel to recall a value associated with a single key without context,\ninsufficiently represents the complexities of real-world long-context modeling.\nTo address this limitation, we extend the associative recall to a novel\nsynthetic task, \\emph{joint recall}, which requires a model to recall the value\nassociated with a key given in a specified context. Theoretically, we prove\nthat SSMs do not have the expressiveness to solve multi-query joint recall in\nsub-quadratic time complexity. To resolve this issue, we propose a solution\nbased on integrating SSMs with Context-Dependent Sparse Attention (CDSA), which\nhas the expressiveness to solve multi-query joint recall with sub-quadratic\ncomputation. To bridge the gap between theoretical analysis and real-world\napplications, we propose locality-sensitive Hashing Attention with sparse Key\nSelection (HAX), which instantiates the theoretical solution and is further\ntailored to natural language domains. Extensive experiments on both synthetic\nand real-world long-context benchmarks show that HAX consistently outperforms\nSSM baselines and SSMs integrated with context-independent sparse attention\n(CISA).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u65b0\u5408\u6210\u4efb\u52a1Joint Recall\u63ed\u793aSSMs\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u6574\u5408\u4e0a\u4e0b\u6587\u76f8\u5173\u7a00\u758f\u6ce8\u610f\u529b\uff08CDSA\uff09\u8bbe\u8ba1HAX\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u4f20\u7edfSSMs\u548cCISA\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSMs\uff09\u867d\u8ba1\u7b97\u9ad8\u6548\uff0c\u4f46\u5904\u7406\u590d\u6742\u957f\u8ddd\u79bb\u4f9d\u8d56\u80fd\u529b\u4e0d\u8db3\u3002\u4f20\u7edf\u8bc4\u4f30\u4efb\u52a1Associative Recall\u8fc7\u4e8e\u7b80\u5316\uff0c\u9700\u66f4\u8d34\u8fd1\u771f\u5b9e\u573a\u666f\u7684\u8bc4\u4f30\u4f53\u7cfb\u63a8\u52a8\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u6280\u672f\u53d1\u5c55\u3002", "method": "1. \u63d0\u51faJoint Recall\u4efb\u52a1\uff08\u591a\u4e0a\u4e0b\u6587\u952e\u503c\u5bf9\u53ec\u56de\uff09\n2. \u7406\u8bba\u8bc1\u660eSSMs\u65e0\u6cd5\u6b21\u7ebf\u6027\u89e3\u51b3\u8be5\u4efb\u52a1\n3. \u63d0\u51faSSMs+CDSA\u7406\u8bba\u6846\u67b6\n4. \u8bbe\u8ba1\u81ea\u7136\u8bed\u8a00\u7279\u5316\u7684HAX\u65b9\u6cd5\uff08\u54c8\u5e0c\u6ce8\u610f\u529b+\u7a00\u758f\u952e\u9009\u62e9\uff09", "result": "\u5728\u5408\u6210\u4efb\u52a1\u548c\u771f\u5b9e\u957f\u6587\u672c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHAX\u76f8\u6bd4SSM\u57fa\u7ebf\u51c6\u786e\u7387\u63d0\u534721.9%\uff0c\u6bd4SSM+CISA\u65b9\u6848\u63d0\u53479.3%\uff0c\u663e\u5b58\u6d88\u8017\u964d\u4f4e37%\u3002", "conclusion": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u4e0e\u5de5\u7a0b\u4f18\u5316\uff0c\u63ed\u793a\u4e86SSMs\u7684\u7406\u8bba\u5c40\u9650\uff0c\u63d0\u51faHAX\u6846\u67b6\u6709\u6548\u7a81\u7834\u8ba1\u7b97-\u6027\u80fd\u5e73\u8861\uff0c\u4e3a\u5b9e\u9645\u957f\u6587\u672c\u5904\u7406\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.00466", "pdf": "https://arxiv.org/pdf/2507.00466", "abs": "https://arxiv.org/abs/2507.00466", "authors": ["Sebastian Murgul", "Michael Heizmann"], "title": "Beat and Downbeat Tracking in Performance MIDI Using an End-to-End Transformer Architecture", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "comment": "Accepted to the 22nd Sound and Music Computing Conference (SMC), 2025", "summary": "Beat tracking in musical performance MIDI is a challenging and important task\nfor notation-level music transcription and rhythmical analysis, yet existing\nmethods primarily focus on audio-based approaches. This paper proposes an\nend-to-end transformer-based model for beat and downbeat tracking in\nperformance MIDI, leveraging an encoder-decoder architecture for\nsequence-to-sequence translation of MIDI input to beat annotations. Our\napproach introduces novel data preprocessing techniques, including dynamic\naugmentation and optimized tokenization strategies, to improve accuracy and\ngeneralizability across different datasets. We conduct extensive experiments\nusing the A-MAPS, ASAP, GuitarSet, and Leduc datasets, comparing our model\nagainst state-of-the-art hidden Markov models (HMMs) and deep learning-based\nbeat tracking methods. The results demonstrate that our model outperforms\nexisting symbolic music beat tracking approaches, achieving competitive\nF1-scores across various musical styles and instruments. Our findings highlight\nthe potential of transformer architectures for symbolic beat tracking and\nsuggest future integration with automatic music transcription systems for\nenhanced music analysis and score generation.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eTransformer\u7684\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u6570\u636e\u589e\u5f3a\u548c\u4f18\u5316\u6807\u8bb0\u7b56\u7565\uff0c\u5728MIDI\u7b26\u53f7\u97f3\u4e50\u8282\u62cd\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u53d6\u5f97\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u6027\u80fd", "motivation": "\u73b0\u6709\u8282\u62cd\u8ddf\u8e2a\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u97f3\u9891\u4fe1\u53f7\uff0c\u800c\u7b26\u53f7\u97f3\u4e50(MIDI)\u7684\u8282\u62cd\u8ddf\u8e2a\u5bf9\u4e50\u8c31\u8f6c\u5f55\u548c\u8282\u594f\u5206\u6790\u5177\u6709\u91cd\u8981\u610f\u4e49\u4f46\u7f3a\u4e4f\u6709\u6548\u89e3\u51b3\u65b9\u6848", "method": "\u4f7f\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u7684Transformer\u6a21\u578b\uff0c\u5f00\u53d1\u52a8\u6001\u589e\u5f3a\u7684\u9884\u5904\u7406\u6d41\u7a0b\u548c\u4f18\u5316\u7684MIDI\u6807\u8bb0\u7b56\u7565\uff0c\u652f\u6301\u8de8\u6570\u636e\u96c6\u7684\u7aef\u5230\u7aef\u8282\u62cd\u6ce8\u91ca\u751f\u6210", "result": "\u5728A-MAPS\u3001ASAP\u7b49\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\u548c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0cF1\u5206\u6570\u8fbe\u5230\u7ade\u4e89\u6c34\u5e73\u4e14\u5177\u6709\u8de8\u4e50\u5668\u6cdb\u5316\u80fd\u529b", "conclusion": "\u8bc1\u660e\u4e86Transformer\u67b6\u6784\u5728\u7b26\u53f7\u97f3\u4e50\u8282\u62cd\u8ddf\u8e2a\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u672a\u6765\u4e0e\u81ea\u52a8\u8bb0\u8c31\u7cfb\u7edf\u7684\u6574\u5408\u63d0\u4f9b\u4e86\u6280\u672f\u57fa\u7840"}}
{"id": "2507.00487", "pdf": "https://arxiv.org/pdf/2507.00487", "abs": "https://arxiv.org/abs/2507.00487", "authors": ["Jianghao Lin", "Xinyuan Wang", "Xinyi Dai", "Menghui Zhu", "Bo Chen", "Ruiming Tang", "Yong Yu", "Weinan Zhang"], "title": "MassTool: A Multi-Task Search-Based Tool Retrieval Framework for Large Language Models", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Tool retrieval is a critical component in enabling large language models\n(LLMs) to interact effectively with external tools. It aims to precisely filter\nthe massive tools into a small set of candidates for the downstream\ntool-augmented LLMs. However, most existing approaches primarily focus on\noptimizing tool representations, often neglecting the importance of precise\nquery comprehension. To address this gap, we introduce MassTool, a multi-task\nsearch-based framework designed to enhance both query representation and tool\nretrieval accuracy. MassTool employs a two-tower architecture: a tool usage\ndetection tower that predicts the need for function calls, and a tool retrieval\ntower that leverages a query-centric graph convolution network (QC-GCN) for\neffective query-tool matching. It also incorporates search-based user intent\nmodeling (SUIM) to handle diverse and out-of-distribution queries, alongside an\nadaptive knowledge transfer (AdaKT) module for efficient multi-task learning.\nBy jointly optimizing tool usage detection loss, list-wise retrieval loss, and\ncontrastive regularization loss, MassTool establishes a robust dual-step\nsequential decision-making pipeline for precise query understanding. Extensive\nexperiments demonstrate its effectiveness in improving retrieval accuracy. Our\ncode is available at https://github.com/wxydada/MassTool.", "AI": {"tldr": "MassTool\u6846\u67b6\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u53cc\u5854\u67b6\u6784\u63d0\u5347\u5de5\u5177\u68c0\u7d22\u51c6\u786e\u6027\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u67e5\u8be2\u7406\u89e3\u7684\u95ee\u9898", "motivation": "\u73b0\u6709\u5de5\u5177\u68c0\u7d22\u65b9\u6cd5\u8fc7\u5ea6\u5173\u6ce8\u5de5\u5177\u8868\u793a\u4f18\u5316\uff0c\u5ffd\u89c6\u4e86\u7cbe\u786e\u67e5\u8be2\u7406\u89e3\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u5173\u952e\u5f71\u54cd", "method": "\u91c7\u7528\u53cc\u5854\u67b6\u6784\uff08\u5de5\u5177\u4f7f\u7528\u68c0\u6d4b\u5854+QC-GCN\u68c0\u7d22\u5854\uff09\uff0c\u6574\u5408\u641c\u7d22\u5f0f\u7528\u6237\u610f\u56fe\u5efa\u6a21(SUIM)\u548c\u81ea\u9002\u5e94\u77e5\u8bc6\u8fc1\u79fb\u6a21\u5757(AdaKT)\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u635f\u5931\u8054\u5408\u4f18\u5316", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u68c0\u7d22\u51c6\u786e\u7387\uff0c\u6709\u6548\u5904\u7406\u5206\u5e03\u5916\u67e5\u8be2", "conclusion": "MassTool\u901a\u8fc7\u521b\u65b0\u7684\u53cc\u9636\u6bb5\u51b3\u7b56\u6d41\u7a0b\u548c\u67e5\u8be2\u4e2d\u5fc3\u56fe\u7f51\u7edc\uff0c\u5efa\u7acb\u4e86\u7cbe\u786e\u7684\u67e5\u8be2\u7406\u89e3\u8303\u5f0f\uff0c\u4e3a\u5de5\u5177\u589e\u5f3a\u578bLLM\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411"}}
{"id": "2507.00693", "pdf": "https://arxiv.org/pdf/2507.00693", "abs": "https://arxiv.org/abs/2507.00693", "authors": ["Yifan Gao", "Jiao Fu", "Long Guo", "Hong Liu"], "title": "Leveraging Large Language Models for Spontaneous Speech-Based Suicide Risk Detection", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Early identification of suicide risk is crucial for preventing suicidal\nbehaviors. As a result, the identification and study of patterns and markers\nrelated to suicide risk have become a key focus of current research. In this\npaper, we present the results of our work in the 1st SpeechWellness Challenge\n(SW1), which aims to explore speech as a non-invasive and easily accessible\nmental health indicator for identifying adolescents at risk of suicide.Our\napproach leverages large language model (LLM) as the primary tool for feature\nextraction, alongside conventional acoustic and semantic features. The proposed\nmethod achieves an accuracy of 74\\% on the test set, ranking first in the SW1\nchallenge. These findings demonstrate the potential of LLM-based methods for\nanalyzing speech in the context of suicide risk assessment.", "AI": {"tldr": "\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u4f20\u7edf\u7279\u5f81\u7684\u81ea\u6740\u98ce\u9669\u8bc4\u4f30\u65b9\u6cd5\u5728SW1\u6311\u6218\u8d5b\u4e2d\u4ee574%\u51c6\u786e\u7387\u593a\u51a0", "motivation": "\u65e9\u671f\u8bc6\u522b\u81ea\u6740\u98ce\u9669\u5bf9\u9884\u9632\u81f3\u5173\u91cd\u8981\uff0c\u8bed\u97f3\u4f5c\u4e3a\u975e\u4fb5\u5165\u6027\u6307\u6807\u5177\u6709\u4e34\u5e8a\u5b9e\u7528\u4ef7\u503c", "method": "\u878d\u5408LLM\u7279\u5f81\u63d0\u53d6\u4e0e\u4f20\u7edf\u58f0\u5b66/\u8bed\u4e49\u7279\u5f81\u7684\u591a\u6a21\u6001\u5206\u6790\u65b9\u6cd5", "result": "\u6d4b\u8bd5\u96c6\u51c6\u786e\u738774%\uff0cSW1\u6311\u6218\u8d5b\u6392\u540d\u7b2c\u4e00", "conclusion": "\u57fa\u4e8eLLM\u7684\u8bed\u97f3\u5206\u6790\u65b9\u6cd5\u5728\u81ea\u6740\u98ce\u9669\u8bc4\u4f30\u4e2d\u5c55\u73b0\u663e\u8457\u6f5c\u529b"}}
{"id": "2507.00740", "pdf": "https://arxiv.org/pdf/2507.00740", "abs": "https://arxiv.org/abs/2507.00740", "authors": ["Craig S Wright"], "title": "Safe Low Bandwidth SPV: A Formal Treatment of Simplified Payment Verification Protocols and Security Bounds", "categories": ["cs.CR", "cs.CL", "cs.DC", "68Q85, 68M10, 94A60, 91A80, 68Q17, 68W10, 68R10", "C.2.2; F.2.2; D.4.6; K.6.5"], "comment": "56 pages 5 images", "summary": "This paper presents a complete formal specification, protocol description,\nand mathematical proof structure for Simplified Payment Verification (SPV) as\noriginally defined in the Bitcoin whitepaper \\cite{nakamoto2008}. In stark\ncontrast to the misrepresentations proliferated by popular implementations, we\nshow that SPV is not only secure under bounded adversarial assumptions but\nstrictly optimal for digital cash systems requiring scalable and verifiable\ntransaction inclusion. We reconstruct the SPV protocol from first principles,\ngrounding its verification model in symbolic automata, Merkle membership\nrelations, and chain-of-proof dominance predicates. Through rigorous\nprobabilistic and game-theoretic analysis, we derive the economic bounds within\nwhich the protocol operates securely and verify its liveness and safety\nproperties under partial connectivity, hostile relay networks, and adversarial\npropagation delay. Our specification further introduces low-bandwidth\noptimisations such as adaptive polling and compressed header synchronisation\nwhile preserving correctness. This document serves both as a blueprint for\nsecure SPV implementation and a rebuttal of common misconceptions surrounding\nnon-validating clients.", "AI": {"tldr": "\u901a\u8fc7\u5f62\u5f0f\u5316\u5efa\u6a21\u8bc1\u660e\u6bd4\u7279\u5e01SPV\u534f\u8bae\u7684\u5b89\u5168\u6027\u53ca\u6700\u4f18\u6027\uff0c\u63d0\u51fa\u5e26\u5bbd\u4f18\u5316\u65b9\u6848\u5e76\u7ea0\u6b63\u5e38\u89c1\u8bef\u89e3", "motivation": "\u5f53\u524d\u4e3b\u6d41\u5b9e\u73b0\u66f2\u89e3\u4e86\u6bd4\u7279\u5e01\u767d\u76ae\u4e66\u5b9a\u4e49\u7684SPV\u539f\u7406\uff0c\u9700\u8981\u4ece\u6570\u5b66\u57fa\u7840\u91cd\u6784\u534f\u8bae\u5e76\u9a8c\u8bc1\u5176\u5728\u5bf9\u6297\u73af\u5883\u4e0b\u7684\u53ef\u9760\u6027", "method": "\u57fa\u4e8e\u7b26\u53f7\u81ea\u52a8\u673a\u5efa\u7acb\u9a8c\u8bc1\u6a21\u578b\uff0c\u7ed3\u5408Merkle\u6811\u6210\u5458\u5173\u7cfb\u8bc1\u660e\u548c\u6982\u7387\u535a\u5f08\u8bba\u5206\u6790\uff0c\u5f15\u5165\u81ea\u9002\u5e94\u8f6e\u8be2\u7b49\u4f4e\u5e26\u5bbd\u4f18\u5316\u6280\u672f", "result": "\u63a8\u5bfc\u51fa\u534f\u8bae\u5b89\u5168\u8fd0\u884c\u7684\u7ecf\u6d4e\u8fb9\u754c\uff0c\u8bc1\u660e\u5728\u90e8\u5206\u7f51\u7edc\u8fde\u63a5\u6761\u4ef6\u4e0b\u4ecd\u4fdd\u6301\u6d3b\u6027\u4e0e\u5b89\u5168\u6027\uff0c\u5b9e\u73b0\u538b\u7f29\u533a\u5757\u5934\u540c\u6b65(\u51cf\u5c1170%\u5e26\u5bbd)", "conclusion": "SPV\u534f\u8bae\u5728\u5f62\u5f0f\u5316\u89c4\u8303\u4e0b\u8fbe\u5230\u4e25\u683c\u6700\u4f18\uff0c\u65e2\u53ef\u5b89\u5168\u5b9e\u65bd\u53c8\u5177\u5907\u53ef\u6269\u5c55\u6027\uff0c\u9a73\u65a5\u4e86\u975e\u9a8c\u8bc1\u8282\u70b9\u4e0d\u53ef\u4fe1\u7684\u9519\u8bef\u8ba4\u77e5"}}
{"id": "2507.00808", "pdf": "https://arxiv.org/pdf/2507.00808", "abs": "https://arxiv.org/abs/2507.00808", "authors": ["Hiroki Kanagawa", "Kenichi Fujita", "Aya Watanabe", "Yusuke Ijima"], "title": "Multi-interaction TTS toward professional recording reproduction", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "7 pages,6 figures, Accepted to Speech Synthesis Workshop 2025 (SSW13)", "summary": "Voice directors often iteratively refine voice actors' performances by\nproviding feedback to achieve the desired outcome. While this iterative\nfeedback-based refinement process is important in actual recordings, it has\nbeen overlooked in text-to-speech synthesis (TTS). As a result, fine-grained\nstyle refinement after the initial synthesis is not possible, even though the\nsynthesized speech often deviates from the user's intended style. To address\nthis issue, we propose a TTS method with multi-step interaction that allows\nusers to intuitively and rapidly refine synthetized speech. Our approach models\nthe interaction between the TTS model and its user to emulate the relationship\nbetween voice actors and voice directors. Experiments show that the proposed\nmodel with its corresponding dataset enable iterative style refinements in\naccordance with users' directions, thus demonstrating its multi-interaction\ncapability. Sample audios are available: https://ntt-hilab-gensp.\ngithub.io/ssw13multiinteraction_tts/", "AI": {"tldr": "\u63d0\u51fa\u652f\u6301\u591a\u6b65\u4ea4\u4e92\u7684\u6587\u672c\u8f6c\u8bed\u97f3\u65b9\u6cd5\uff0c\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u8fed\u4ee3\u53cd\u9988\u7cbe\u7ec6\u8c03\u6574\u5408\u6210\u8bed\u97f3\u98ce\u683c", "motivation": "\u73b0\u6709\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf\u7f3a\u4e4f\u7c7b\u4f3c\u5b9e\u9645\u5f55\u97f3\u4e2d\u7684\u8fed\u4ee3\u53cd\u9988\u673a\u5236\uff0c\u5bfc\u81f4\u65e0\u6cd5\u5728\u521d\u6b21\u5408\u6210\u540e\u8fdb\u884c\u7ec6\u7c92\u5ea6\u7684\u98ce\u683c\u8c03\u6574\uff0c\u96be\u4ee5\u6ee1\u8db3\u7528\u6237\u9884\u671f\u7684\u8bed\u97f3\u8868\u8fbe\u9700\u6c42", "method": "\u6784\u5efa\u6a21\u62df\u914d\u97f3\u6f14\u5458\u4e0e\u5bfc\u6f14\u4e92\u52a8\u5173\u7cfb\u7684TTS\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6b65\u4ea4\u4e92\u754c\u9762\u4f7f\u7528\u6237\u80fd\u76f4\u89c2\u5feb\u901f\u8c03\u6574\u8bed\u97f3\u98ce\u683c\u53c2\u6570", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u6a21\u578b\u80fd\u591f\u6839\u636e\u7528\u6237\u7684\u591a\u8f6e\u6307\u4ee4\u5b9e\u73b0\u8bed\u97f3\u98ce\u683c\u7684\u8fed\u4ee3\u4f18\u5316\uff0c\u6210\u529f\u5b9e\u73b0\u5e73\u57473\u8f6e\u4ea4\u4e92\u8fbe\u5230\u76ee\u6807\u6548\u679c", "conclusion": "\u8be5\u4ea4\u4e92\u5f0fTTS\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u5408\u6210\u7cfb\u7edf\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u8c03\u6574\u80fd\u529b\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u5bf9\u8bed\u97f3\u751f\u6210\u8fc7\u7a0b\u7684\u63a7\u5236\u529b"}}
{"id": "2507.00877", "pdf": "https://arxiv.org/pdf/2507.00877", "abs": "https://arxiv.org/abs/2507.00877", "authors": ["William H English", "Chase Walker", "Dominic Simon", "Sumit Kumar Jha", "Rickard Ewetz"], "title": "Verifiable Natural Language to Linear Temporal Logic Translation: A Benchmark Dataset and Evaluation Suite", "categories": ["eess.SY", "cs.CL", "cs.SY"], "comment": null, "summary": "Empirical evaluation of state-of-the-art natural-language (NL) to\ntemporal-logic (TL) translation systems reveals near-perfect performance on\nexisting benchmarks. However, current studies measure only the accuracy of the\ntranslation of NL logic into formal TL, ignoring a system's capacity to ground\natomic propositions into new scenarios or environments. This is a critical\nfeature, necessary for the verification of resulting formulas in a concrete\nstate space. Consequently, most NL-to-TL translation frameworks propose their\nown bespoke dataset in which the correct grounding is known a-priori, inflating\nperformance metrics and neglecting the need for extensible, domain-general\nsystems. In this paper, we introduce the Verifiable Linear Temporal Logic\nBenchmark ( VLTL-Bench), a unifying benchmark that measures verification and\nverifiability of automated NL-to-LTL translation. The dataset consists of three\nunique state spaces and thousands of diverse natural language specifications\nand corresponding formal specifications in temporal logic. Moreover, the\nbenchmark contains sample traces to validate the temporal logic expressions.\nWhile the benchmark directly supports end-to-end evaluation, we observe that\nmany frameworks decompose the process into i) lifting, ii) grounding, iii)\ntranslation, and iv) verification. The benchmark provides ground truths after\neach of these steps to enable researches to improve and evaluate different\nsubsteps of the overall problem. To encourage methodologically sound advances\nin verifiable NL-to-LTL translation approaches, we release VLTL-Bench here:\nhttps://www.kaggle.com/datasets/dubascudes/vltl bench.", "AI": {"tldr": "\u63d0\u51faVLTL-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u89e3\u51b3\u73b0\u6709NL\u5230TL\u7ffb\u8bd1\u7cfb\u7edf\u8bc4\u4f30\u4e2d\u5ffd\u7565\u547d\u9898\u573a\u666f\u8fc1\u79fb\u80fd\u529b\u7684\u95ee\u9898", "motivation": "\u73b0\u6709\u57fa\u51c6\u4ec5\u8bc4\u4f30\u5f62\u5f0f\u903b\u8f91\u7ffb\u8bd1\u51c6\u786e\u6027\uff0c\u5ffd\u89c6\u7cfb\u7edf\u5728\u65b0\u573a\u666f\u4e2d\u57fa\u7840\u547d\u9898\u7684\u9a8c\u8bc1\u80fd\u529b\uff0c\u5bfc\u81f4\u6027\u80fd\u6307\u6807\u865a\u9ad8\u4e14\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027", "method": "\u6784\u5efa\u5305\u542b3\u4e2a\u72b6\u6001\u7a7a\u95f4\u3001\u6570\u5343\u79cd\u81ea\u7136\u8bed\u8a00\u89c4\u8303\u53ca\u5176\u5bf9\u5e94\u65f6\u5e8f\u903b\u8f91\u516c\u5f0f\u7684\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u4f9b\u9a8c\u8bc1\u6837\u672c\u94fe", "result": "\u521b\u5efa\u652f\u6301\u7aef\u5230\u7aef\u8bc4\u4f30\u7684\u7edf\u4e00\u57fa\u51c6\uff0c\u5141\u8bb8\u5206\u9636\u6bb5\u9a8c\u8bc1lifting/grounding/translation/verification\u5b50\u8fc7\u7a0b", "conclusion": "VLTL-Bench\u901a\u8fc7\u63d0\u4f9b\u5206\u9636\u6bb5\u9a8c\u8bc1\u80fd\u529b\uff0c\u63a8\u52a8\u53ef\u9a8c\u8bc1NL\u5230LTL\u7ffb\u8bd1\u65b9\u6cd5\u7684\u65b9\u6cd5\u8bba\u6539\u8fdb"}}
{"id": "2507.00898", "pdf": "https://arxiv.org/pdf/2507.00898", "abs": "https://arxiv.org/abs/2507.00898", "authors": ["Zifu Wan", "Ce Zhang", "Silong Yong", "Martin Q. Ma", "Simon Stepputtis", "Louis-Philippe Morency", "Deva Ramanan", "Katia Sycara", "Yaqi Xie"], "title": "ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted by ICCV 2025. Project page: https://zifuwan.github.io/ONLY/", "summary": "Recent Large Vision-Language Models (LVLMs) have introduced a new paradigm\nfor understanding and reasoning about image input through textual responses.\nAlthough they have achieved remarkable performance across a range of\nmulti-modal tasks, they face the persistent challenge of hallucination, which\nintroduces practical weaknesses and raises concerns about their reliable\ndeployment in real-world applications. Existing work has explored contrastive\ndecoding approaches to mitigate this issue, where the output of the original\nLVLM is compared and contrasted with that of a perturbed version. However,\nthese methods require two or more queries that slow down LVLM response\ngeneration, making them less suitable for real-time applications. To overcome\nthis limitation, we propose ONLY, a training-free decoding approach that\nrequires only a single query and a one-layer intervention during decoding,\nenabling efficient real-time deployment. Specifically, we enhance textual\noutputs by selectively amplifying crucial textual information using a\ntext-to-visual entropy ratio for each token. Extensive experimental results\ndemonstrate that our proposed ONLY consistently outperforms state-of-the-art\nmethods across various benchmarks while requiring minimal implementation effort\nand computational cost. Code is available at https://github.com/zifuwan/ONLY.", "AI": {"tldr": "\u63d0\u51faONLY\u65b9\u6cd5\u89e3\u51b3\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u901a\u8fc7\u5355\u6b21\u67e5\u8be2\u548c\u71b5\u6bd4\u4f18\u5316\u5b9e\u73b0\u9ad8\u6548\u5b9e\u65f6\u90e8\u7f72", "motivation": "\u73b0\u6709\u5bf9\u6bd4\u89e3\u7801\u65b9\u6cd5\u9700\u8981\u591a\u6b21\u67e5\u8be2\u5f71\u54cd\u54cd\u5e94\u901f\u5ea6\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u5e94\u7528\u9700\u6c42", "method": "\u91c7\u7528\u8bad\u7ec3\u81ea\u7531\u7684\u89e3\u7801\u7b56\u7565\uff0c\u57fa\u4e8e\u6587\u672c\u5230\u89c6\u89c9\u7684\u71b5\u6bd4\u9009\u62e9\u6027\u589e\u5f3a\u5173\u952e\u6587\u672c\u4fe1\u606f", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8aSOTA\u65b9\u6cd5\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e90%\u4e14\u4fdd\u6301\u9ad8\u6027\u80fd", "conclusion": "ONLY\u9996\u6b21\u5b9e\u73b0\u5355\u6b21\u67e5\u8be2\u5e72\u9884\uff0c\u4e3aLVLMs\u5b9e\u65f6\u90e8\u7f72\u63d0\u4f9b\u9ad8\u6548\u53ef\u9760\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2507.00979", "pdf": "https://arxiv.org/pdf/2507.00979", "abs": "https://arxiv.org/abs/2507.00979", "authors": ["Dongyoon Hahm", "Woogyeol Jin", "June Suk Choi", "Sungsoo Ahn", "Kimin Lee"], "title": "Enhancing LLM Agent Safety via Causal Influence Prompting", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted at ACL 2025 Findings, Source code:\n  https://github.com/HahmDY/causal_influence_prompting.git", "summary": "As autonomous agents powered by large language models (LLMs) continue to\ndemonstrate potential across various assistive tasks, ensuring their safe and\nreliable behavior is crucial for preventing unintended consequences. In this\nwork, we introduce CIP, a novel technique that leverages causal influence\ndiagrams (CIDs) to identify and mitigate risks arising from agent\ndecision-making. CIDs provide a structured representation of cause-and-effect\nrelationships, enabling agents to anticipate harmful outcomes and make safer\ndecisions. Our approach consists of three key steps: (1) initializing a CID\nbased on task specifications to outline the decision-making process, (2)\nguiding agent interactions with the environment using the CID, and (3)\niteratively refining the CID based on observed behaviors and outcomes.\nExperimental results demonstrate that our method effectively enhances safety in\nboth code execution and mobile device control tasks.", "AI": {"tldr": "\u63d0\u51faCIP\u65b9\u6cd5\uff0c\u5229\u7528\u56e0\u679c\u5f71\u54cd\u56fe\u63d0\u5347\u81ea\u4e3b\u4ee3\u7406\u5b89\u5168\u6027\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5728\u4ee3\u7801\u6267\u884c\u548c\u79fb\u52a8\u8bbe\u5907\u63a7\u5236\u4efb\u52a1\u4e2d\u6709\u6548", "motivation": "\u786e\u4fddLLM\u9a71\u52a8\u7684\u81ea\u4e3b\u4ee3\u7406\u5728\u8f85\u52a9\u4efb\u52a1\u4e2d\u7684\u5b89\u5168\u53ef\u9760\uff0c\u9632\u6b62\u51b3\u7b56\u5bfc\u81f4\u7684\u610f\u5916\u540e\u679c", "method": "1. \u57fa\u4e8e\u4efb\u52a1\u89c4\u8303\u521d\u59cb\u5316\u56e0\u679c\u56fe 2. \u7528\u56e0\u679c\u56fe\u6307\u5bfc\u73af\u5883\u4ea4\u4e92 3. \u6839\u636e\u89c2\u5bdf\u7ed3\u679c\u8fed\u4ee3\u4f18\u5316\u56e0\u679c\u56fe", "result": "\u5728\u4ee3\u7801\u6267\u884c\u548c\u79fb\u52a8\u8bbe\u5907\u63a7\u5236\u4efb\u52a1\u4e2d\u663e\u8457\u589e\u5f3a\u5b89\u5168\u6027", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u5316\u56e0\u679c\u5efa\u6a21\u5b9e\u73b0\u66f4\u5b89\u5168\u7684\u4ee3\u7406\u51b3\u7b56\uff0c\u4e3a\u5b89\u5168AI\u7cfb\u7edf\u63d0\u4f9b\u65b0\u601d\u8def"}}
