{"id": "2511.00702", "pdf": "https://arxiv.org/pdf/2511.00702", "abs": "https://arxiv.org/abs/2511.00702", "authors": ["Alberto Di Biase"], "title": "Applying Medical Imaging Tractography Techniques to Painterly Rendering of Images", "categories": ["cs.GR", "cs.CV"], "comment": "Exploratory investigation applying medical imaging tractography\n  techniques to painterly image rendering. Code available at\n  https://github.com/tito21/st-python", "summary": "Doctors and researchers routinely use diffusion tensor imaging (DTI) and\ntractography to visualize the fibrous structure of tissues in the human body.\nThis paper explores the connection of these techniques to the painterly\nrendering of images. Using a tractography algorithm the presented method can\nplace brush strokes that mimic the painting process of human artists,\nanalogously to how fibres are tracked in DTI. The analogue to the diffusion\ntensor for image orientation is the structural tensor, which can provide better\nlocal orientation information than the gradient alone. I demonstrate this\ntechnique in portraits and general images, and discuss the parallels between\nfibre tracking and brush stroke placement, and frame it in the language of\ntractography. This work presents an exploratory investigation into the\ncross-domain application of diffusion tensor imaging techniques to painterly\nrendering of images. All the code is available at\nhttps://github.com/tito21/st-python", "AI": {"tldr": "\u5c06\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u6269\u6563\u5f20\u91cf\u6210\u50cf\u6280\u672f\u5e94\u7528\u4e8e\u7ed8\u753b\u98ce\u683c\u6e32\u67d3\uff0c\u901a\u8fc7\u7ea4\u7ef4\u8ffd\u8e2a\u7b97\u6cd5\u6a21\u62df\u827a\u672f\u5bb6\u7b14\u89e6\u5e03\u5c40", "motivation": "\u63a2\u7d22\u533b\u5b66\u9886\u57dfDTI\u6280\u672f\u5411\u827a\u672f\u9886\u57df\u8fc1\u79fb\u5e94\u7528\u7684\u53ef\u884c\u6027\uff0c\u5c1d\u8bd5\u5c06\u751f\u7269\u7ec4\u7ec7\u7ea4\u7ef4\u8ffd\u8e2a\u539f\u7406\u8f6c\u5316\u4e3a\u6570\u5b57\u827a\u672f\u521b\u4f5c\u65b9\u6cd5", "method": "\u4f7f\u7528\u7ed3\u6784\u5f20\u91cf\u66ff\u4ee3\u68af\u5ea6\u4f5c\u4e3a\u65b9\u5411\u4fe1\u606f\uff0c\u5f00\u53d1\u7c7b\u4f3cDTI\u7ea4\u7ef4\u8ffd\u8e2a\u7684\u7b14\u89e6\u5e03\u5c40\u7b97\u6cd5", "result": "\u6210\u529f\u5e94\u7528\u4e8e\u8096\u50cf\u753b\u548c\u901a\u7528\u56fe\u50cf\uff0c\u9a8c\u8bc1\u4e86\u901a\u8fc7tractography\u6846\u67b6\u6307\u5bfc\u827a\u672f\u521b\u4f5c\u7684\u53ef\u884c\u6027", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8de8\u9886\u57df\u6280\u672f\u8fc1\u79fb\u63d0\u4f9b\u4e86\u521b\u65b0\u8303\u4f8b\uff0c\u8bc1\u660e\u533b\u5b66\u6210\u50cf\u7b97\u6cd5\u5728\u827a\u672f\u6e32\u67d3\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b"}}
{"id": "2511.00898", "pdf": "https://arxiv.org/pdf/2511.00898", "abs": "https://arxiv.org/abs/2511.00898", "authors": ["Heng Zhang", "Jing Liu", "Jiajun Wu", "Haochen You", "Lubin Gan", "Yuling Shi", "Xiaodong Gu", "Zijian Zhang", "Shuai Chen", "Wenjun Huang", "Jin Huang"], "title": "Empowering LLMs with Structural Role Inference for Zero-Shot Graph Learning", "categories": ["cs.GR"], "comment": null, "summary": "Large Language Models have emerged as a promising approach for graph learning\ndue to their powerful reasoning capabilities. However, existing methods exhibit\nsystematic performance degradation on structurally important nodes such as\nbridges and hubs. We identify the root cause of these limitations. Current\napproaches encode graph topology into static features but lack reasoning\nscaffolds to transform topological patterns into role-based interpretations.\nThis limitation becomes critical in zero-shot scenarios where no training data\nestablishes structure-semantics mappings. To address this gap, we propose\nDuoGLM, a training-free dual-perspective framework for structure-aware graph\nreasoning. The local perspective constructs relation-aware templates capturing\nsemantic interactions between nodes and neighbors. The global perspective\nperforms topology-to-role inference to generate functional descriptions of\nstructural positions. These complementary perspectives provide explicit\nreasoning mechanisms enabling LLMs to distinguish topologically similar but\nsemantically different nodes. Extensive experiments across eight benchmark\ndatasets demonstrate substantial improvements. DuoGLM achieves 14.3\\% accuracy\ngain in zero-shot node classification and 7.6\\% AUC improvement in cross-domain\ntransfer compared to existing methods. The results validate the effectiveness\nof explicit role reasoning for graph understanding with LLMs.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684DuoGLM\u53cc\u89c6\u89d2\u6846\u67b6\uff0c\u901a\u8fc7\u5c40\u90e8\u5173\u7cfb\u611f\u77e5\u6a21\u677f\u548c\u5168\u5c40\u62d3\u6251\u89d2\u8272\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u7ed3\u6784\u7406\u89e3\u4e2d\u7684\u8868\u73b0\u3002\u96f6\u6837\u672c\u8282\u70b9\u5206\u7c7b\u51c6\u786e\u7387\u63d0\u534714.3%\uff0c\u8de8\u57dfAUC\u63d0\u53477.6%\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u56fe\u62d3\u6251\u7f16\u7801\u4e3a\u9759\u6001\u7279\u5f81\uff0c\u7f3a\u4e4f\u7ed3\u6784\u5230\u8bed\u4e49\u7684\u663e\u5f0f\u63a8\u7406\u673a\u5236\uff0c\u5bfc\u81f4\u5728\u6865\u63a5\u8282\u70b9/\u4e2d\u5fc3\u8282\u70b9\u7b49\u7ed3\u6784\u91cd\u8981\u4f4d\u7f6e\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u95ee\u9898\u7a81\u51fa\u3002", "method": "\u8bbe\u8ba1\u53cc\u89c6\u89d2\u6846\u67b6\uff1a1) \u5c40\u90e8\u89c6\u89d2\u6784\u5efa\u8282\u70b9\u4e0e\u90bb\u5c45\u7684\u8bed\u4e49\u4ea4\u4e92\u6a21\u677f\uff1b2) \u5168\u5c40\u89c6\u89d2\u901a\u8fc7\u62d3\u6251\u89d2\u8272\u63a8\u65ad\u751f\u6210\u7ed3\u6784\u4f4d\u7f6e\u529f\u80fd\u63cf\u8ff0\uff0c\u5f62\u6210\u4e92\u8865\u63a8\u7406\u673a\u5236\u3002", "result": "\u57288\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff1a\u96f6\u6837\u672c\u8282\u70b9\u5206\u7c7b\u51c6\u786e\u7387\u63d0\u534714.3%\uff0c\u8de8\u9886\u57df\u8fc1\u79fb\u4efb\u52a1AUC\u63d0\u53477.6%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u663e\u5f0f\u89d2\u8272\u63a8\u7406\u673a\u5236\u80fd\u6709\u6548\u589e\u5f3aLLMs\u7684\u56fe\u7ed3\u6784\u7406\u89e3\u80fd\u529b\uff0c\u8bc1\u5b9e\u62d3\u6251\u89d2\u8272\u4e0e\u8bed\u4e49\u5173\u8054\u7684\u663e\u5f0f\u5efa\u6a21\u5bf9\u56fe\u5b66\u4e60\u4efb\u52a1\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2511.00911", "pdf": "https://arxiv.org/pdf/2511.00911", "abs": "https://arxiv.org/abs/2511.00911", "authors": ["Heng Zheng", "Haochen You", "Zijun Liu", "Zijian Zhang", "Lubin Gan", "Hao Zhang", "Wenjun Huang", "Jin Huang"], "title": "G2rammar: Bilingual Grammar Modeling for Enhanced Text-attributed Graph Learning", "categories": ["cs.GR"], "comment": null, "summary": "Text-attributed graphs require models to effectively integrate both\nstructural topology and semantic content. Recent approaches apply large\nlanguage models to graphs by linearizing structures into token sequences\nthrough random walks. These methods create concise graph vocabularies to\nreplace verbose natural language descriptions. However, they overlook a\ncritical component that makes language expressive: grammar. In natural\nlanguage, grammar assigns syntactic roles to words and defines their functions\nwithin sentences. Similarly, nodes in graphs play distinct structural roles as\nhubs, bridges, or peripheral members. Current graph language methods provide\ntokens without grammatical annotations to indicate these structural or semantic\nroles. This absence limits language models' ability to reason about graph\ntopology effectively. We propose \\textbf{G2rammar}, a bilingual grammar\nframework that explicitly encodes both structural and semantic grammar for\ntext-attributed graphs. Structural grammar characterizes topological roles\nthrough centrality and neighborhood patterns. Semantic grammar captures content\nrelationships through textual informativity. The framework implements two-stage\nlearning with structural grammar pre-training followed by semantic grammar\nfine-tuning. Extensive experiments on real-world datasets demonstrate that\nG2rammar consistently outperforms competitive baselines by providing language\nmodels with the grammatical context needed to understand graph structures.", "AI": {"tldr": "\u63d0\u51faG2rammar\u53cc\u8bed\u8bed\u6cd5\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u8bed\u6cd5\u548c\u8bed\u4e49\u8bed\u6cd5\u589e\u5f3a\u6587\u672c\u5c5e\u6027\u56fe\u7684\u7406\u89e3\uff0c\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5bf9\u56fe\u7ed3\u6784\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u56fe\u8bed\u8a00\u65b9\u6cd5\u7f3a\u4e4f\u8bed\u6cd5\u6807\u6ce8\uff0c\u65e0\u6cd5\u660e\u786e\u8282\u70b9\u7ed3\u6784\u89d2\u8272\uff0c\u9650\u5236\u4e86\u8bed\u8a00\u6a21\u578b\u5bf9\u56fe\u62d3\u6251\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u7ed3\u6784\u8bed\u6cd5(\u901a\u8fc7\u4e2d\u5fc3\u6027\u548c\u90bb\u57df\u6a21\u5f0f\u5b9a\u4e49\u62d3\u6251\u89d2\u8272)\u4e0e\u8bed\u4e49\u8bed\u6cd5(\u901a\u8fc7\u6587\u672c\u4fe1\u606f\u91cf\u6355\u6349\u5185\u5bb9\u5173\u7cfb)\uff0c\u91c7\u7528\u7ed3\u6784\u9884\u8bad\u7ec3+\u8bed\u4e49\u5fae\u8c03\u7684\u4e24\u9636\u6bb5\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u5b9e\u9a8c\u4e2d\uff0cG2rammar\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\uff0c\u8bed\u6cd5\u6846\u67b6\u4f7f\u6a21\u578b\u51c6\u786e\u7387\u5e73\u5747\u63d0\u53473.2%\u3002", "conclusion": "\u663e\u5f0f\u7f16\u7801\u56fe\u7ed3\u6784\u548c\u8bed\u4e49\u7684\u8bed\u6cd5\u89c4\u5219\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u56fe\u7ebf\u6027\u5316\u65b9\u6cd5\u7f3a\u4e4f\u8bed\u6cd5\u89d2\u8272\u6807\u6ce8\u7684\u6838\u5fc3\u7f3a\u9677\u3002"}}
{"id": "2511.01259", "pdf": "https://arxiv.org/pdf/2511.01259", "abs": "https://arxiv.org/abs/2511.01259", "authors": ["Zhiqi Li", "Jinjin He", "Barnab\u00e1s B\u00f6rcs\u00f6k", "Taiyuan Zhang", "Duowen Chen", "Tao Du", "Ming C. Lin", "Greg Turk", "Bo Zhu"], "title": "An Adjoint Method for Differentiable Fluid Simulation on Flow Maps", "categories": ["cs.GR", "physics.flu-dyn"], "comment": "15 pages, 16 figures", "summary": "This paper presents a novel adjoint solver for differentiable fluid\nsimulation based on bidirectional flow maps. Our key observation is that the\nforward fluid solver and its corresponding backward, adjoint solver share the\nsame flow map as the forward simulation. In the forward pass, this map\ntransports fluid impulse variables from the initial frame to the current frame\nto simulate vortical dynamics. In the backward pass, the same map propagates\nadjoint variables from the current frame back to the initial frame to compute\ngradients. This shared long-range map allows the accuracy of gradient\ncomputation to benefit directly from improvements in flow map construction.\nBuilding on this insight, we introduce a novel adjoint solver that solves the\nadjoint equations directly on the flow map, enabling long-range and accurate\ndifferentiation of incompressible flows without differentiating intermediate\nnumerical steps or storing intermediate variables, as required in conventional\nadjoint methods. To further improve efficiency, we propose a long-short\ntime-sparse flow map representation for evolving adjoint variables. Our\napproach has low memory usage, requiring only 6.53GB of data at a resolution of\n$192^3$ while preserving high accuracy in tracking vorticity, enabling new\ndifferentiable simulation tasks that require precise identification,\nprediction, and control of vortex dynamics.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u53cc\u5411\u6d41\u6620\u5c04\u7684\u4f34\u968f\u6c42\u89e3\u5668\uff0c\u901a\u8fc7\u5171\u4eab\u6d41\u6620\u5c04\u5b9e\u73b0\u9ad8\u6548\u53ef\u5fae\u5206\u6d41\u4f53\u6a21\u62df\uff0c\u5185\u5b58\u5360\u7528\u4ec56.53GB\uff08192^3\u5206\u8fa8\u7387\uff09", "motivation": "\u4f20\u7edf\u4f34\u968f\u65b9\u6cd5\u9700\u8981\u5b58\u50a8\u4e2d\u95f4\u53d8\u91cf\u5e76\u5bf9\u6570\u503c\u6b65\u9aa4\u6c42\u5bfc\uff0c\u65b0\u65b9\u6cd5\u5229\u7528\u6d41\u6620\u5c04\u5171\u4eab\u7279\u6027\u7a81\u7834\u8ba1\u7b97\u74f6\u9888", "method": "1. \u5229\u7528\u6d41\u6620\u5c04\u53cc\u5411\u4f20\u8f93\u6d41\u4f53\u51b2\u91cf\u53d8\u91cf\u548c\u4f34\u968f\u53d8\u91cf\n2. \u6784\u5efa\u957f\u671f-\u77ed\u671f\u65f6\u95f4\u7a00\u758f\u6d41\u6620\u5c04\u8868\u793a\n3. \u76f4\u63a5\u5728\u6d41\u6620\u5c04\u4e0a\u6c42\u89e3\u4f34\u968f\u65b9\u7a0b", "result": "\u5b9e\u73b0192^3\u5206\u8fa8\u7387\u4e0b6.53GB\u5185\u5b58\u5360\u7528\uff0c\u4fdd\u6301\u6da1\u6d41\u8ffd\u8e2a\u7cbe\u5ea6\uff0c\u652f\u6301\u6da1\u6d41\u8bc6\u522b/\u9884\u6d4b/\u63a7\u5236\u65b0\u4efb\u52a1", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6da1\u6d41\u52a8\u529b\u5b66\u7684\u7cbe\u786e\u5efa\u6a21\u4e0e\u63a7\u5236\u5f00\u8f9f\u65b0\u53ef\u80fd\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u957f\u7a0b\u7cbe\u786e\u5fae\u5206\u7684\u6d41\u4f53\u6a21\u62df\u573a\u666f"}}
{"id": "2511.00010", "pdf": "https://arxiv.org/pdf/2511.00010", "abs": "https://arxiv.org/abs/2511.00010", "authors": ["Jiajun Zhang", "Jianke Zhang", "Zeyu Cui", "Jiaxi Yang", "Lei Zhang", "Binyuan Hui", "Qiang Liu", "Zilei Wang", "Liang Wang", "Junyang Lin"], "title": "PlotCraft: Pushing the Limits of LLMs for Complex and Interactive Data Visualization", "categories": ["cs.CL"], "comment": null, "summary": "Recent Large Language Models (LLMs) have demonstrated remarkable profi-\nciency in code generation. However, their ability to create complex visualiza-\ntions for scaled and structured data remains largely unevaluated and\nunderdevel- oped. To address this gap, we introduce PlotCraft, a new benchmark\nfeaturing 1k challenging visualization tasks that cover a wide range of topics,\nsuch as fi- nance, scientific research, and sociology. The benchmark is\nstructured around seven high-level visualization tasks and encompasses 48\ndistinct chart types. Cru- cially, it is the first to systematically evaluate\nboth single-turn generation and multi-turn refinement across a diverse spectrum\nof task complexities. Our com- prehensive evaluation of 23 leading LLMs on\nPlotCraft reveals obvious per- formance deficiencies in handling sophisticated\nvisualization tasks. To bridge this performance gap, we develope SynthVis-30K,\na large-scale, high-quality dataset of complex visualization code synthesized\nvia a collaborative agent frame- work. Building upon this dataset, we develope\nPlotCraftor, a novel code gener- ation model that achieves strong capabilities\nin complex data visualization with a remarkably small size. Across VisEval,\nPandasPlotBench, and our proposed PlotCraft, PlotCraftor shows performance\ncomparable to that of leading propri- etary approaches. Especially, on hard\ntask, Our model achieves over 50% per- formance improvement. We will release\nthe benchmark, dataset, and code at\nhttps://github.com/Speakn0w/PlotCraft-Benchmark.", "AI": {"tldr": "\u63d0\u51fa\u4e86PlotCraft\u57fa\u51c6\u6d4b\u8bd5\u548cPlotCraftor\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u6570\u636e\u53ef\u89c6\u5316\u4efb\u52a1\u4e2dLLM\u7684\u4ee3\u7801\u751f\u6210\u6027\u80fd\uff08\u56f0\u96be\u4efb\u52a1\u63d0\u5347\u8d8550%\uff09", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u7ed3\u6784\u5316\u6570\u636e\u53ef\u89c6\u5316\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u4f30\u4e0e\u4f18\u5316\uff0c\u9700\u6784\u5efa\u4e13\u95e8\u57fa\u51c6\u4e0e\u6570\u636e\u96c6\u586b\u8865\u7a7a\u767d", "method": "1. \u521b\u5efa\u5305\u542b1k\u4efb\u52a1\u30017\u7c7b\u573a\u666f\u300148\u79cd\u56fe\u8868\u7c7b\u578b\u7684PlotCraft\u57fa\u51c6 2. \u901a\u8fc7\u534f\u540c\u4ee3\u7406\u6846\u67b6\u5408\u6210\u9ad8\u8d28\u91cfSynthVis-30K\u6570\u636e\u96c6 3. \u5f00\u53d1\u4e13\u7528\u6a21\u578bPlotCraftor", "result": "PlotCraftor\u5728VisEval\u3001PandasPlotBench\u53caPlotCraft\u4e2d\u8fbe\u5230\u5546\u7528\u6a21\u578b\u6c34\u5e73\uff0c\u56f0\u96be\u4efb\u52a1\u6027\u80fd\u63d0\u5347\u8d8550%", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u6027\u57fa\u51c6\u6784\u5efa\u4e0e\u4e13\u7528\u6570\u636e\u96c6\u5f00\u53d1\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u590d\u6742\u53ef\u89c6\u5316\u573a\u666f\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u586b\u8865\u8be5\u9886\u57df\u8bc4\u4f30\u4e0e\u4f18\u5316\u7a7a\u767d"}}
{"id": "2511.00248", "pdf": "https://arxiv.org/pdf/2511.00248", "abs": "https://arxiv.org/abs/2511.00248", "authors": ["Shurui Gui", "Deep Anil Patel", "Xiner Li", "Martin Renqiang Min"], "title": "Object-Aware 4D Human Motion Generation", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Recent advances in video diffusion models have enabled the generation of\nhigh-quality videos. However, these videos still suffer from unrealistic\ndeformations, semantic violations, and physical inconsistencies that are\nlargely rooted in the absence of 3D physical priors. To address these\nchallenges, we propose an object-aware 4D human motion generation framework\ngrounded in 3D Gaussian representations and motion diffusion priors. With\npre-generated 3D humans and objects, our method, Motion Score Distilled\nInteraction (MSDI), employs the spatial and prompt semantic information in\nlarge language models (LLMs) and motion priors through the proposed Motion\nDiffusion Score Distillation Sampling (MSDS). The combination of MSDS and LLMs\nenables our spatial-aware motion optimization, which distills score gradients\nfrom pre-trained motion diffusion models, to refine human motion while\nrespecting object and semantic constraints. Unlike prior methods requiring\njoint training on limited interaction datasets, our zero-shot approach avoids\nretraining and generalizes to out-of-distribution object aware human motions.\nExperiments demonstrate that our framework produces natural and physically\nplausible human motions that respect 3D spatial context, offering a scalable\nsolution for realistic 4D generation.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e3D\u9ad8\u65af\u8868\u793a\u4e0e\u8fd0\u52a8\u6269\u6563\u5148\u9a8c\u7684MSDI\u6846\u67b6\uff0c\u901a\u8fc7LLM\u8bed\u4e49\u5f15\u5bfc\u548cMSDS\u4f18\u5316\uff0c\u5b9e\u73b0\u7269\u7406\u5408\u7406\u7684\u96f6\u6837\u672c\u4eba\u4f53\u8fd0\u52a8\u751f\u6210", "motivation": "\u73b0\u6709\u89c6\u9891\u6269\u6563\u6a21\u578b\u56e0\u7f3a\u4e4f3D\u7269\u7406\u5148\u9a8c\u5bfc\u81f4\u8fd0\u52a8\u53d8\u5f62\u3001\u8bed\u4e49\u8fdd\u89c4\u548c\u7269\u7406\u4e0d\u4e00\u81f4\u95ee\u9898", "method": "\u7ed3\u5408LLM\u7684\u7a7a\u95f4\u8bed\u4e49\u5206\u6790\u4e0eMSDS\u8fd0\u52a8\u8bc4\u5206\u84b8\u998f\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u8fd0\u52a8\u6269\u6563\u6a21\u578b\u4f18\u5316\u4eba\u4f53\u8fd0\u52a8\u8f68\u8ff9", "result": "\u751f\u6210\u7b26\u54083D\u7a7a\u95f4\u7ea6\u675f\u7684\u81ea\u7136\u4eba\u4f53\u8fd0\u52a8\uff0c\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u5c55\u73b0\u826f\u597d\u6cdb\u5316\u80fd\u529b", "conclusion": "\u672c\u6846\u67b6\u4e3a4D\u5185\u5bb9\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u7269\u7406\u5408\u7406\u7684\u4eba\u673a\u4ea4\u4e92\u8fd0\u52a8"}}
{"id": "2511.00115", "pdf": "https://arxiv.org/pdf/2511.00115", "abs": "https://arxiv.org/abs/2511.00115", "authors": ["Haoyuan Li", "Yuanbo Tong", "Yuchen Li", "Zirui Wang", "Chunhou Liu", "Jiamou Liu"], "title": "Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Personality recognition from text is typically cast as hard-label\nclassification, which obscures the graded, prototype-like nature of human\npersonality judgments. We present ProtoMBTI, a cognitively aligned framework\nfor MBTI inference that operationalizes prototype theory within an LLM-based\npipeline. First, we construct a balanced, quality-controlled corpus via\nLLM-guided multi-dimensional augmentation (semantic, linguistic, sentiment).\nNext, we LoRA-fine-tune a lightweight (<=2B) encoder to learn discriminative\nembeddings and to standardize a bank of personality prototypes. At inference,\nwe retrieve top-k prototypes for a query post and perform a\nretrieve--reuse--revise--retain cycle: the model aggregates prototype evidence\nvia prompt-based voting, revises when inconsistencies arise, and, upon correct\nprediction, retains the sample to continually enrich the prototype library.\nAcross Kaggle and Pandora benchmarks, ProtoMBTI improves over baselines on both\nthe four MBTI dichotomies and the full 16-type task, and exhibits robust\ncross-dataset generalization. Our results indicate that aligning the inference\nprocess with psychological prototype reasoning yields gains in accuracy,\ninterpretability, and transfer for text-based personality modeling.", "AI": {"tldr": "\u63d0\u51faProtoMBTI\u6846\u67b6\uff0c\u901a\u8fc7\u539f\u578b\u7406\u8bba\u4e0eLLM\u6d41\u7a0b\u7ed3\u5408\u6539\u8fdbMBTI\u6027\u683c\u5206\u7c7b\uff0c\u5728\u51c6\u786e\u7387\u3001\u53ef\u89e3\u91ca\u6027\u548c\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u65b9\u9762\u5747\u6709\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u786c\u6807\u7b7e\u5206\u7c7b\u65e0\u6cd5\u53cd\u6620\u4eba\u683c\u5224\u65ad\u7684\u6e10\u8fdb\u6027\u4e0e\u539f\u578b\u7279\u6027\uff0c\u9700\u6784\u5efa\u8ba4\u77e5\u5bf9\u9f50\u7684\u63a8\u7406\u6846\u67b6\u3002", "method": "1. \u901a\u8fc7LLM\u591a\u7ef4\u5ea6\u589e\u5f3a\u6784\u5efa\u5e73\u8861\u8bed\u6599\u5e93\n2. LoRA\u5fae\u8c03\u8f7b\u91cf\u7f16\u7801\u5668\u5b66\u4e60\u7279\u5f81\u5d4c\u5165\n3. \u68c0\u7d22-\u805a\u5408-\u4fee\u6b63-\u4fdd\u7559\u5faa\u73af\u63a8\u7406\u673a\u5236", "result": "\u5728Kaggle\u548cPandora\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMBTI\u56db\u7ef4\u5ea6\u548c16\u7c7b\u578b\u4efb\u52a1\u5747\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u663e\u8457\u3002", "conclusion": "\u5c06\u5fc3\u7406\u539f\u578b\u63a8\u7406\u878d\u5165\u6587\u672c\u6027\u683c\u5efa\u6a21\uff0c\u53ef\u540c\u6b65\u63d0\u5347\u6a21\u578b\u7cbe\u5ea6\u3001\u53ef\u89e3\u91ca\u6027\u548c\u8fc1\u79fb\u80fd\u529b\u3002"}}
{"id": "2511.00362", "pdf": "https://arxiv.org/pdf/2511.00362", "abs": "https://arxiv.org/abs/2511.00362", "authors": ["Momen Khandoker Ope", "Akif Islam", "Mohd Ruhul Ameen", "Abu Saleh Musa Miah", "Md Rashedul Islam", "Jungpil Shin"], "title": "Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": "6 Pages, 4 figures, 2 Tables, Submitted to ICECTE 2026", "summary": "Cultural heritage restoration in Bangladesh faces a dual challenge of limited\nresources and scarce technical expertise. Traditional 3D digitization methods,\nsuch as photogrammetry or LiDAR scanning, require expensive hardware, expert\noperators, and extensive on-site access, which are often infeasible in\ndeveloping contexts. As a result, many of Bangladesh's architectural treasures,\nfrom the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to\ndecay and inaccessible in digital form. This paper introduces Oitijjo-3D, a\ncost-free generative AI framework that democratizes 3D cultural preservation.\nBy using publicly available Google Street View imagery, Oitijjo-3D reconstructs\nfaithful 3D models of heritage structures through a two-stage pipeline -\nmultimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture\nsynthesis, and neural image-to-3D generation through Hexagen for geometry\nrecovery. The system produces photorealistic, metrically coherent\nreconstructions in seconds, achieving significant speedups compared to\nconventional Structure-from-Motion pipelines, without requiring any specialized\nhardware or expert supervision. Experiments on landmarks such as Ahsan Manzil,\nChoto Sona Mosque, and Paharpur demonstrate that Oitijjo-3D preserves both\nvisual and structural fidelity while drastically lowering economic and\ntechnical barriers. By turning open imagery into digital heritage, this work\nreframes preservation as a community-driven, AI-assisted act of cultural\ncontinuity for resource-limited nations.", "AI": {"tldr": "Oitijjo-3D\u6846\u67b6\u5229\u7528Google\u8857\u666f\u548c\u751f\u6210\u5f0fAI\u6280\u672f\uff0c\u4f4e\u6210\u672c\u5b9e\u73b0\u6587\u5316\u9057\u4ea73D\u6570\u5b57\u5316\u4fdd\u62a4\uff0c\u89e3\u51b3\u53d1\u5c55\u4e2d\u56fd\u5bb6\u8d44\u6e90\u6280\u672f\u53cc\u91cd\u6311\u6218\u3002", "motivation": "\u5b5f\u52a0\u62c9\u56fd\u6587\u5316\u9057\u4ea7\u9762\u4e34\u4f20\u7edf3D\u6570\u5b57\u5316\u65b9\u6cd5\uff08\u5982\u6444\u5f71\u6d4b\u91cf/LiDAR\uff09\u6210\u672c\u9ad8\u3001\u9700\u4e13\u5bb6\u7684\u56f0\u5883\uff0c\u5bfc\u81f4\u5927\u91cf\u5efa\u7b51\u9057\u4ea7\u7f3a\u4e4f\u6570\u5b57\u4fdd\u62a4\u3002", "method": "\u4e24\u9636\u6bb5AI\u6d41\u7a0b\uff1a1) Gemini 2.5 Flash\u591a\u6a21\u6001\u5408\u6210\u7ed3\u6784\u7eb9\u7406\uff1b2) Hexagen\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u56fe\u50cf\u52303D\u51e0\u4f55\u91cd\u5efa\u3002", "result": "\u7cfb\u7edf\u79d2\u7ea7\u751f\u6210\u9ad8\u4fdd\u771f3D\u6a21\u578b\uff08\u5982Ahsan Manzil\u7b49\u6848\u4f8b\uff09\uff0c\u901f\u5ea6\u8fdc\u8d85\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e14\u65e0\u9700\u4e13\u4e1a\u8bbe\u5907/\u4e13\u5bb6\u76d1\u7763\u3002", "conclusion": "\u5c06\u5f00\u653e\u56fe\u50cf\u8f6c\u5316\u4e3a\u6570\u5b57\u9057\u4ea7\uff0c\u91cd\u6784\u4e86\u8d44\u6e90\u6709\u9650\u56fd\u5bb6\u6587\u5316\u4fdd\u62a4\u7684\u8303\u5f0f\u2014\u2014AI\u8f85\u52a9\u7684\u793e\u533a\u9a71\u52a8\u6587\u5316\u5ef6\u7eed\u884c\u52a8\u3002"}}
{"id": "2511.00180", "pdf": "https://arxiv.org/pdf/2511.00180", "abs": "https://arxiv.org/abs/2511.00180", "authors": ["Nicky Pochinkov", "Yulia Volkova", "Anna Vasileva", "Sai V R Chereddy"], "title": "ParaScopes: What do Language Models Activations Encode About Future Text?", "categories": ["cs.CL", "cs.LG"], "comment": "Main paper: 9 pages, 10 figures. Total 24 pages", "summary": "Interpretability studies in language models often investigate forward-looking\nrepresentations of activations. However, as language models become capable of\ndoing ever longer time horizon tasks, methods for understanding activations\noften remain limited to testing specific concepts or tokens. We develop a\nframework of Residual Stream Decoders as a method of probing model activations\nfor paragraph-scale and document-scale plans. We test several methods and find\ninformation can be decoded equivalent to 5+ tokens of future context in small\nmodels. These results lay the groundwork for better monitoring of language\nmodels and better understanding how they might encode longer-term planning\ninformation.", "AI": {"tldr": "\u63d0\u51faResidual Stream Decoders\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u7801\u6b8b\u5dee\u6d41\u6fc0\u6d3b\u4fe1\u606f\u5b9e\u73b0\u8bed\u8a00\u6a21\u578b\u6bb5\u843d\u7ea7/\u6587\u6863\u7ea7\u89c4\u5212\u76d1\u63a7\uff0c\u9a8c\u8bc1\u5176\u5728\u5c0f\u578b\u6a21\u578b\u4e2d\u53ef\u89e3\u78015+\u672a\u6765token\u7684\u4fe1\u606f\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5c40\u9650\u4e8etoken\u7ea7\u6216\u7279\u5b9a\u6982\u5ff5\u5206\u6790\uff0c\u65e0\u6cd5\u9002\u5e94\u8bed\u8a00\u6a21\u578b\u5904\u7406\u957f\u65f6\u7a0b\u4efb\u52a1\u65f6\u7684\u89c4\u5212\u4fe1\u606f\u76d1\u63a7\u9700\u6c42\u3002", "method": "\u5f00\u53d1\u6b8b\u5dee\u6d41\u89e3\u7801\u5668\u6846\u67b6\uff0c\u91c7\u7528\u591a\u79cd\u63a2\u6d4b\u65b9\u6cd5\u5206\u6790\u6a21\u578b\u6fc0\u6d3b\u4e2d\u7f16\u7801\u7684\u6bb5\u843d\u7ea7\u548c\u6587\u6863\u7ea7\u89c4\u5212\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u5c0f\u578b\u6a21\u578b\u4e2d\u53ef\u89e3\u7801\u76f8\u5f53\u4e8e5+\u672a\u6765token\u7684\u4fe1\u606f\u91cf\uff0c\u663e\u8457\u63d0\u5347\u957f\u65f6\u89c4\u5212\u76d1\u63a7\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6df1\u5165\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u7684\u957f\u65f6\u89c4\u5212\u7f16\u7801\u673a\u5236\u53ca\u5f00\u53d1\u66f4\u6709\u6548\u7684\u76d1\u63a7\u5de5\u5177\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
