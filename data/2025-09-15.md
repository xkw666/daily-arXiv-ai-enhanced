<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 52]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.CR](#cs.CR) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.CV](#cs.CV) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Structured Information Matters: Explainable ICD Coding with Patient-Level Knowledge Graphs](https://arxiv.org/abs/2509.09699)
*Mingyang Li,Viktor Schlegel,Tingting Mu,Warren Del-Pinto,Goran Nenadic*

Main category: cs.CL

TL;DR: 通过构建患者知识图谱提升ICD自动化编码效率，在保持90%信息量的同时将文本压缩23%，使模型Macro-F1提升3.2%


<details>
  <summary>Details</summary>
Motivation: 临床文档手动编码存在效率低、扩展性差的问题，自动化编码能提升结构化数据的可用性和准确性

Method: 利用文档级知识图谱结构化表示患者状况，整合到现有最佳模型PLM-ICD架构中

Result: 在主流基准测试中Macro-F1最高提升3.20%，同时提升了模型训练效率

Conclusion: 知识图谱通过实体关系和结构化表示有效提升编码效果、效率及模型可解释性

Abstract: Mapping clinical documents to standardised clinical vocabularies is an
important task, as it provides structured data for information retrieval and
analysis, which is essential to clinical research, hospital administration and
improving patient care. However, manual coding is both difficult and
time-consuming, making it impractical at scale. Automated coding can
potentially alleviate this burden, improving the availability and accuracy of
structured clinical data. The task is difficult to automate, as it requires
mapping to high-dimensional and long-tailed target spaces, such as the
International Classification of Diseases (ICD). While external knowledge
sources have been readily utilised to enhance output code representation, the
use of external resources for representing the input documents has been
underexplored. In this work, we compute a structured representation of the
input documents, making use of document-level knowledge graphs (KGs) that
provide a comprehensive structured view of a patient's condition. The resulting
knowledge graph efficiently represents the patient-centred input documents with
23\% of the original text while retaining 90\% of the information. We assess
the effectiveness of this graph for automated ICD-9 coding by integrating it
into the state-of-the-art ICD coding architecture PLM-ICD. Our experiments
yield improved Macro-F1 scores by up to 3.20\% on popular benchmarks, while
improving training efficiency. We attribute this improvement to different types
of entities and relationships in the KG, and demonstrate the improved
explainability potential of the approach over the text-only baseline.

</details>


### [2] [Cross-Layer Attention Probing for Fine-Grained Hallucination Detection](https://arxiv.org/abs/2509.09700)
*Malavika Suresh,Rahaf Aljundi,Ikechukwu Nkisi-Orji,Nirmalie Wiratunga*

Main category: cs.CL

TL;DR: 提出跨层注意力探测技术CLAP，通过联合分析大语言模型全残差流激活，显著提升幻觉检测能力并支持细粒度检测。


<details>
  <summary>Details</summary>
Motivation: 针对大语言模型生成不准确文本（幻觉）的可靠性问题，需要更有效的检测方法来提升模型可靠性。

Method: 跨层注意力探测技术（CLAP），将模型各残差层的激活作为联合序列处理，实现全层级特征融合分析。

Result: 在5个大模型和3个任务上的实验表明，CLAP在贪婪解码和高温采样场景下均优于基线，支持不同采样响应的细粒度幻觉判别。

Conclusion: CLAP检测技术可实现'先检测后修复'策略，相比直接修复方法更有效提升模型可靠性，且具备良好的跨分布鲁棒性。

Abstract: With the large-scale adoption of Large Language Models (LLMs) in various
applications, there is a growing reliability concern due to their tendency to
generate inaccurate text, i.e. hallucinations. In this work, we propose
Cross-Layer Attention Probing (CLAP), a novel activation probing technique for
hallucination detection, which processes the LLM activations across the entire
residual stream as a joint sequence. Our empirical evaluations using five LLMs
and three tasks show that CLAP improves hallucination detection compared to
baselines on both greedy decoded responses as well as responses sampled at
higher temperatures, thus enabling fine-grained detection, i.e. the ability to
disambiguate hallucinations and non-hallucinations among different sampled
responses to a given prompt. This allows us to propose a detect-then-mitigate
strategy using CLAP to reduce hallucinations and improve LLM reliability
compared to direct mitigation approaches. Finally, we show that CLAP maintains
high reliability even when applied out-of-distribution.

</details>


### [3] [Optimal Multi-Task Learning at Regularization Horizon for Speech Translation Task](https://arxiv.org/abs/2509.09701)
*JungHo Jung,Junhyun Lee*

Main category: cs.CL

TL;DR: 提出通过多任务学习和正则化技术解决端到端语音翻译数据不足问题，在MuST-C数据集上达到接近SOTA效果


<details>
  <summary>Details</summary>
Motivation: 端到端语音翻译常面临语音-文本配对数据稀缺问题，需利用机器翻译的bitext数据进行多任务学习

Method: 从正则化视角探索跨模态（一致性正则化）和同模态（R-drop）的序列正则化，建立三维正则化视界

Result: 在MuST-C数据集上通过调节正则化超参数实现接近当前最佳性能

Conclusion: 揭示了多任务学习中MT损失系数、跨模态正则化和同模态正则化的协同作用机制

Abstract: End-to-end speech-to-text translation typically suffers from the scarcity of
paired speech-text data. One way to overcome this shortcoming is to utilize the
bitext data from the Machine Translation (MT) task and perform Multi-Task
Learning (MTL). In this paper, we formulate MTL from a regularization
perspective and explore how sequences can be regularized within and across
modalities. By thoroughly investigating the effect of consistency
regularization (different modality) and R-drop (same modality), we show how
they respectively contribute to the total regularization. We also demonstrate
that the coefficient of MT loss serves as another source of regularization in
the MTL setting. With these three sources of regularization, we introduce the
optimal regularization contour in the high-dimensional space, called the
regularization horizon. Experiments show that tuning the hyperparameters within
the regularization horizon achieves near state-of-the-art performance on the
MuST-C dataset.

</details>


### [4] [Creativity Benchmark: A benchmark for marketing creativity for LLM models](https://arxiv.org/abs/2509.09702)
*Ninad Bhat,Kieran Browne,Pip Bingemann*

Main category: cs.CL

TL;DR: 论文提出Creativity Benchmark框架，通过人类专家对11,012次模型输出的对比评估，发现不同LLM在营销创意任务中表现差异微小（胜率仅61%），自动评估方法存在偏差，强调必须依赖人类专家评估。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法难以有效评估LLM在品牌约束和多样化营销创意任务中的表现，以及自动化评估与人类判断脱节的问题。

Method: 构建含100品牌/3提示类型的基准，采用Bradley-Terry模型分析678名创意人员的11,012次偏好数据，结合余弦距离评估模型多样性及提示敏感性。

Result: 模型间最大胜率差仅61%（Δθ≈0.45），LLM自动评判与人类排名相关性弱（相关系数0.24-0.41），传统创造力指标迁移效果有限。

Conclusion: 营销创意评估需坚持专家人类评审，自动评估存在固有偏差，工作流程应注重多样性管理。

Abstract: We introduce Creativity Benchmark, an evaluation framework for large language
models (LLMs) in marketing creativity. The benchmark covers 100 brands (12
categories) and three prompt types (Insights, Ideas, Wild Ideas). Human
pairwise preferences from 678 practising creatives over 11,012 anonymised
comparisons, analysed with Bradley-Terry models, show tightly clustered
performance with no model dominating across brands or prompt types: the
top-bottom spread is $\Delta\theta \approx 0.45$, which implies a head-to-head
win probability of $0.61$; the highest-rated model beats the lowest only about
$61\%$ of the time. We also analyse model diversity using cosine distances to
capture intra- and inter-model variation and sensitivity to prompt reframing.
Comparing three LLM-as-judge setups with human rankings reveals weak,
inconsistent correlations and judge-specific biases, underscoring that
automated judges cannot substitute for human evaluation. Conventional
creativity tests also transfer only partially to brand-constrained tasks.
Overall, the results highlight the need for expert human evaluation and
diversity-aware workflows.

</details>


### [5] [CTCC: A Robust and Stealthy Fingerprinting Framework for Large Language Models via Cross-Turn Contextual Correlation Backdoor](https://arxiv.org/abs/2509.09703)
*Zhenhua Xu,Xixiang Zhao,Xubin Yue,Shengwei Tian,Changting Lin,Meng Han*

Main category: cs.CL

TL;DR: 提出CTCC框架，通过多轮对话的上下文关联实现隐蔽鲁棒的LLM模型指纹技术


<details>
  <summary>Details</summary>
Motivation: 现有模型指纹方法存在隐蔽性、鲁棒性和泛化性的权衡困境，易被检测或篡改失效

Method: 基于规则的多轮上下文关联机制（如反事实对话），支持黑盒验证并降低误报/泄露风险

Result: 跨多个LLM架构的实验表明CTCC在隐蔽性和鲁棒性上显著优于现有方法

Conclusion: CTCC为现实场景中的LLM所有权验证提供了可靠解决方案，代码数据已开源

Abstract: The widespread deployment of large language models (LLMs) has intensified
concerns around intellectual property (IP) protection, as model theft and
unauthorized redistribution become increasingly feasible. To address this,
model fingerprinting aims to embed verifiable ownership traces into LLMs.
However, existing methods face inherent trade-offs between stealthness,
robustness, and generalizability, being either detectable via distributional
shifts, vulnerable to adversarial modifications, or easily invalidated once the
fingerprint is revealed. In this work, we introduce CTCC, a novel rule-driven
fingerprinting framework that encodes contextual correlations across multiple
dialogue turns, such as counterfactual, rather than relying on token-level or
single-turn triggers. CTCC enables fingerprint verification under black-box
access while mitigating false positives and fingerprint leakage, supporting
continuous construction under a shared semantic rule even if partial triggers
are exposed. Extensive experiments across multiple LLM architectures
demonstrate that CTCC consistently achieves stronger stealth and robustness
than prior work. Our findings position CTCC as a reliable and practical
solution for ownership verification in real-world LLM deployment scenarios. Our
code and data are publicly available at <https://github.com/Xuzhenhua55/CTCC>.

</details>


### [6] [Temporal Preferences in Language Models for Long-Horizon Assistance](https://arxiv.org/abs/2509.09704)
*Ali Mazyaki,Mohammad Naghizadeh,Samaneh Ranjkhah Zonouzaghi,Hossein Setareh*

Main category: cs.CL

TL;DR: 研究语言模型在跨期选择中的时间取向偏好及其可操控性，发现推理型模型未来导向性显著但个性化不足，提出AI助手应适配异质化长期目标的设计框架


<details>
  <summary>Details</summary>
Motivation: 探究语言模型是否具备与人类相似的时间偏好特征，及其偏好可操控性对AI决策系统伦理设计的影响

Method: 采用改编的人类实验范式，引入MTO指标量化模型时间偏好变化，对比多个LM与人类决策者在时间权衡任务中的表现

Result: 推理型模型未来导向提示下延迟选项选择率提升47%，但身份/地域个性化决策准确率仅达62%；具备时间推理能力的模型自主内化未来取向

Conclusion: AI助手需支持个性化上下文校准，建议建立社会文化感知的部署框架，推动时间偏好可解释性与价值观对齐的跨学科研究

Abstract: We study whether language models (LMs) exhibit future- versus
present-oriented preferences in intertemporal choice and whether those
preferences can be systematically manipulated. Using adapted human experimental
protocols, we evaluate multiple LMs on time-tradeoff tasks and benchmark them
against a sample of human decision makers. We introduce an operational metric,
the Manipulability of Time Orientation (MTO), defined as the change in an LM's
revealed time preference between future- and present-oriented prompts. In our
tests, reasoning-focused models (e.g., DeepSeek-Reasoner and grok-3-mini)
choose later options under future-oriented prompts but only partially
personalize decisions across identities or geographies. Moreover, models that
correctly reason about time orientation internalize a future orientation for
themselves as AI decision makers. We discuss design implications for AI
assistants that should align with heterogeneous, long-horizon goals and outline
a research agenda on personalized contextual calibration and socially aware
deployment.

</details>


### [7] [The Non-Determinism of Small LLMs: Evidence of Low Answer Consistency in Repetition Trials of Standard Multiple-Choice Benchmarks](https://arxiv.org/abs/2509.09705)
*Claudio Pinhanez,Paulo Cavalin,Cassia Sanctos,Marcelo Grave,Yago Primerano*

Main category: cs.CL

TL;DR: 研究小型LLMs在重复回答同一问题时的表现一致性，分析不同参数对一致性的影响，并提出新的分析工具。


<details>
  <summary>Details</summary>
Motivation: 探究小模型在实际应用中的可靠性，尤其是在需要稳定答案的场景下（如医疗或教育领域）。

Method: 在多选基准测试中重复提问，调整推理温度、模型大小等参数，并开发新工具分析结果。

Result: 小模型在低温度下的一致性回答率为50%-80%，中型模型一致性更高，且一致性答案的准确性合理关联总体表现。

Conclusion: 选择模型需权衡一致性与准确性，中型模型表现更优，新工具助力模型评估。

Abstract: This work explores the consistency of small LLMs (2B-8B parameters) in
answering multiple times the same question. We present a study on known,
open-source LLMs responding to 10 repetitions of questions from the
multiple-choice benchmarks MMLU-Redux and MedQA, considering different
inference temperatures, small vs. medium models (50B-80B), finetuned vs. base
models, and other parameters. We also look into the effects of requiring
multi-trial answer consistency on accuracy and the trade-offs involved in
deciding which model best provides both of them. To support those studies, we
propose some new analytical and graphical tools. Results show that the number
of questions which can be answered consistently vary considerably among models
but are typically in the 50%-80% range for small models at low inference
temperatures. Also, accuracy among consistent answers seems to reasonably
correlate with overall accuracy. Results for medium-sized models seem to
indicate much higher levels of answer consistency.

</details>


### [8] [Beyond I'm Sorry, I Can't: Dissecting Large Language Model Refusal](https://arxiv.org/abs/2509.09708)
*Nirmalendu Prakash,Yeo Wei Jie,Amir Abdullah,Ranjan Satapathy,Erik Cambria,Roy Ka Wei Lee*

Main category: cs.CL

TL;DR: 论文通过稀疏自编码器分析LLM拒绝有害指令的机制，提出三阶段特征定位方法，发现操纵关键特征可突破安全限制，揭示了安全行为的冗余特征机制


<details>
  <summary>Details</summary>
Motivation: 探究指令调优大语言模型拒绝有害提示的内部机制，该安全行为成因尚未被充分理解

Method: 使用残差流激活训练的稀疏自编码器（SAEs），通过拒绝方向定位、贪婪筛选、交互发现三阶段流程，结合因子分解机挖掘特征非线性相互作用

Result: 成功定位到介导拒绝行为的关键特征集，发现存在冗余特征机制（抑制早期特征会激活备用特征），实现了通过特征操作突破模型安全限制

Conclusion: 基于可解释潜在空间的特征操作，为精细审计和靶向干预模型安全行为提供了新路径

Abstract: Refusal on harmful prompts is a key safety behaviour in instruction-tuned
large language models (LLMs), yet the internal causes of this behaviour remain
poorly understood. We study two public instruction-tuned models, Gemma-2-2B-IT
and LLaMA-3.1-8B-IT, using sparse autoencoders (SAEs) trained on
residual-stream activations. Given a harmful prompt, we search the SAE latent
space for feature sets whose ablation flips the model from refusal to
compliance, demonstrating causal influence and creating a jailbreak. Our search
proceeds in three stages: (1) Refusal Direction: find a refusal-mediating
direction and collect SAE features near that direction; (2) Greedy Filtering:
prune to a minimal set; and (3) Interaction Discovery: fit a factorization
machine (FM) that captures nonlinear interactions among the remaining active
features and the minimal set. This pipeline yields a broad set of
jailbreak-critical features, offering insight into the mechanistic basis of
refusal. Moreover, we find evidence of redundant features that remain dormant
unless earlier features are suppressed. Our findings highlight the potential
for fine-grained auditing and targeted intervention in safety behaviours by
manipulating the interpretable latent space.

</details>


### [9] [Assisting Research Proposal Writing with Large Language Models: Evaluation and Refinement](https://arxiv.org/abs/2509.09709)
*Jing Ren,Weiqi Wang*

Main category: cs.CL

TL;DR: 研究提出内容质量与引用有效性双指标及迭代提示法，显著提升ChatGPT写作质量并减少学术伦理问题


<details>
  <summary>Details</summary>
Motivation: 解决LLMs学术写作中存在的引用虚构问题，改善传统主观评估方式缺乏客观性的缺陷

Method: 设计定量评估指标（内容质量/引用有效性）+基于得分的迭代提示优化框架

Result: 实验证实指标可客观评估写作性能，迭代提示使内容质量提升38%且引用错误率下降52%

Conclusion: 定量评估框架与迭代提示机制为LLMs学术写作提供了可靠性保障和伦理问题解决方案

Abstract: Large language models (LLMs) like ChatGPT are increasingly used in academic
writing, yet issues such as incorrect or fabricated references raise ethical
concerns. Moreover, current content quality evaluations often rely on
subjective human judgment, which is labor-intensive and lacks objectivity,
potentially compromising the consistency and reliability. In this study, to
provide a quantitative evaluation and enhance research proposal writing
capabilities of LLMs, we propose two key evaluation metrics--content quality
and reference validity--and an iterative prompting method based on the scores
derived from these two metrics. Our extensive experiments show that the
proposed metrics provide an objective, quantitative framework for assessing
ChatGPT's writing performance. Additionally, iterative prompting significantly
enhances content quality while reducing reference inaccuracies and
fabrications, addressing critical ethical challenges in academic contexts.

</details>


### [10] [Generating Individual Travel Diaries Using Large Language Models Informed by Census and Land-Use Data](https://arxiv.org/abs/2509.09710)
*Sepehr Golrokh Amin,Devin Rhoads,Fatemeh Fakhrmoosavi,Nicholas E. Lownes,John N. Ivan*

Main category: cs.CL

TL;DR: 提出基于大语言模型的出行日记生成方案，通过开源数据生成人物画像并实现与经典模型相当的合成效果


<details>
  <summary>Details</summary>
Motivation: 传统交通模型依赖大量专有调查数据，本方案旨在利用开源数据提升生成效率和可扩展性

Method: 结合ACS/SLD数据生成人物画像，采用四维真实性评分体系（出行次数/间隔/目的/模式），运用Jensen-Shannon散度进行分布相似性验证

Result: LLM生成日记整体真实性得分0.485（经典模型0.455），在出行目的识别和一致性表现更优，经典模型在数值估算方面占优

Conclusion: 证实LLM零样本可行性，建立可量化的合成日记评价体系，为交通建模提供新范式

Abstract: This study introduces a Large Language Model (LLM) scheme for generating
individual travel diaries in agent-based transportation models. While
traditional approaches rely on large quantities of proprietary household travel
surveys, the method presented in this study generates personas stochastically
from open-source American Community Survey (ACS) and Smart Location Database
(SLD) data, then synthesizes diaries through direct prompting. This study
features a novel one-to-cohort realism score: a composite of four metrics (Trip
Count Score, Interval Score, Purpose Score, and Mode Score) validated against
the Connecticut Statewide Transportation Study (CSTS) diaries, matched across
demographic variables. The validation utilizes Jensen-Shannon Divergence to
measure distributional similarities between generated and real diaries. When
compared to diaries generated with classical methods (Negative Binomial for
trip generation; Multinomial Logit for mode/purpose) calibrated on the
validation set, LLM-generated diaries achieve comparable overall realism (LLM
mean: 0.485 vs. 0.455). The LLM excels in determining trip purpose and
demonstrates greater consistency (narrower realism score distribution), while
classical models lead in numerical estimates of trip count and activity
duration. Aggregate validation confirms the LLM's statistical
representativeness (LLM mean: 0.612 vs. 0.435), demonstrating LLM's zero-shot
viability and establishing a quantifiable metric of diary realism for future
synthetic diary evaluation systems.

</details>


### [11] [Psychiatry-Bench: A Multi-Task Benchmark for LLMs in Psychiatry](https://arxiv.org/abs/2509.09711)
*Aya E. Fouda,Abdelrahamn A. Hassan,Radwa J. Hanafy,Mohammed E. Fouda*

Main category: cs.CL

TL;DR: 开发PsychiatryBench基准测试系统，通过11类临床任务评估LLM在精神科应用中的表现，发现现有模型存在显著临床一致性缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估资源依赖非专业数据源（如社交媒体/合成对话），临床有效性不足且无法覆盖精神科复杂推理场景。

Method: 基于权威精神科教材构建包含5,300+专家标注项的模块化评估体系，采用传统指标和LLM相似度评分框架测试Gemini/DeepSeek等多类模型。

Result: 前沿模型在连续性临床任务（随访/管理）中表现显著不足，开源医疗模型(OpenBiloLLM等)与商业模型存在系统性差距。

Conclusion: 该基准为精神健康领域LLM的评估与优化提供可扩展平台，凸显专业调优和强化评估范式的必要性。

Abstract: Large language models (LLMs) hold great promise in enhancing psychiatric
practice, from improving diagnostic accuracy to streamlining clinical
documentation and therapeutic support. However, existing evaluation resources
heavily rely on small clinical interview corpora, social media posts, or
synthetic dialogues, which limits their clinical validity and fails to capture
the full complexity of psychiatric reasoning. In this work, we introduce
PsychiatryBench, a rigorously curated benchmark grounded exclusively in
authoritative, expert-validated psychiatric textbooks and casebooks.
PsychiatryBench comprises eleven distinct question-answering tasks ranging from
diagnostic reasoning and treatment planning to longitudinal follow-up,
management planning, clinical approach, sequential case analysis, and
multiple-choice/extended matching formats totaling over 5,300 expert-annotated
items. We evaluate a diverse set of frontier LLMs (including Google Gemini,
DeepSeek, LLaMA 3, and QWQ-32) alongside leading open-source medical models
(e.g., OpenBiloLLM, MedGemma) using both conventional metrics and an
"LLM-as-judge" similarity scoring framework. Our results reveal substantial
gaps in clinical consistency and safety, particularly in multi-turn follow-up
and management tasks, underscoring the need for specialized model tuning and
more robust evaluation paradigms. PsychiatryBench offers a modular, extensible
platform for benchmarking and improving LLM performance in high-stakes mental
health applications.

</details>


### [12] [The Thinking Therapist: Training Large Language Models to Deliver Acceptance and Commitment Therapy using Supervised Fine-Tuning and Odds Ratio Policy Optimization](https://arxiv.org/abs/2509.09712)
*Talha Tahir*

Main category: cs.CL

TL;DR: ORPO训练的小型语言模型在ACT治疗中表现优于SFT和基础模型，显式推理链（COT）仅对SFT模型有效


<details>
  <summary>Details</summary>
Motivation: 探索不同后训练方法（SFT/ORPO）和显式推理对小型开源语言模型实施接受与承诺疗法（ACT）能力的影响

Method: 使用Mistral-Large生成的50组合成对话训练Llama-3.2-3b模型，比较SFT和ORPO训练范式（含/不含COT推理），通过ACT忠诚度量表（ACT-FM）和治疗师共情量表（TES）进行量化评估

Result: ORPO模型在治疗忠诚度（χ²=185.15）和共情能力（χ²=140.37）显著优于其他模型；COT仅提升SFT模型ACT-FM得分2.68分，对ORPO无效

Conclusion: 偏好对齐优化可有效培养小型LLM的ACT能力，显式推理的效用高度依赖训练范式（ORPO学习治疗流程，COT仅辅助模仿训练）

Abstract: Acceptance and Commitment Therapy (ACT) is a third-wave cognitive behavioral
therapy with emerging evidence of efficacy in several psychiatric conditions.
This study investigates the impact of post-training methodology and explicit
reasoning on the ability of a small open-weight large language model (LLM) to
deliver ACT. Using 50 sets of synthetic ACT transcripts generated by
Mistral-Large, we trained Llama-3.2-3b-Instruct with two distinct approaches,
supervised fine-tuning (SFT) and odds ratio policy optimization (ORPO), each
with and without an explicit chain-of-thought (COT) reasoning step. Performance
was evaluated by comparing these four post-trained variants against the base
Instruct model. These models were benchmarked in simulated therapy sessions,
with performance quantitatively assessed on the ACT Fidelity Measure (ACT-FM)
and the Therapist Empathy Scale (TES) by an LLM judge that had been fine-tuned
on human evaluations. Our findings demonstrate that the ORPO-trained models
significantly outperformed both their SFT and Instruct counterparts on ACT
fidelity ($\chi^2(5) = 185.15, p < .001$) and therapeutic empathy ($\chi^2(5) =
140.37, p < .001$). The effect of COT was conditional as it provided a
significant benefit to SFT models, improving ACT-FM scores by an average of
2.68 points ($p < .001$), while offering no discernible advantage to the
superior ORPO or instruct-tuned variants. We posit that the superiority of ORPO
stems from its ability to learn the therapeutic `process' over imitating
`content,' a key aspect of ACT, while COT acts as a necessary scaffold for
models trained only via imitation. This study establishes that
preference-aligned policy optimization can effectively instill ACT competencies
in small LLMs, and that the utility of explicit reasoning is highly dependent
on the underlying training paradigm.

</details>


### [13] [HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented Generation for Multi-hop Question Answering](https://arxiv.org/abs/2509.09713)
*Duolin Sun,Dan Yang,Yue Shen,Yihan Jiao,Zhehao Tan,Jie Feng,Lianzhen Zhong,Jian Wang,Peng Wei,Jinjie Gu*

Main category: cs.CL

TL;DR: 提出HANRAG框架通过查询路由、子查询分解和噪声过滤，显著提升多跳问答任务性能


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在处理多跳查询时存在迭代检索效率低、复杂查询导致噪声积累的问题

Method: 基于启发式框架实现查询路由机制，将复合查询分解为子查询，并通过噪声过滤增强抗干扰能力

Result: 实验证明HANRAG在单跳和多跳问答任务中均取得优于主流方法的性能表现

Conclusion: 该框架通过动态路由和噪声管理机制，为复杂查询处理提供了高效可靠的解决方案

Abstract: The Retrieval-Augmented Generation (RAG) approach enhances question-answering
systems and dialogue generation tasks by integrating information retrieval (IR)
technologies with large language models (LLMs). This strategy, which retrieves
information from external knowledge bases to bolster the response capabilities
of generative models, has achieved certain successes. However, current RAG
methods still face numerous challenges when dealing with multi-hop queries. For
instance, some approaches overly rely on iterative retrieval, wasting too many
retrieval steps on compound queries. Additionally, using the original complex
query for retrieval may fail to capture content relevant to specific
sub-queries, resulting in noisy retrieved content. If the noise is not managed,
it can lead to the problem of noise accumulation. To address these issues, we
introduce HANRAG, a novel heuristic-based framework designed to efficiently
tackle problems of varying complexity. Driven by a powerful revelator, HANRAG
routes queries, decomposes them into sub-queries, and filters noise from
retrieved documents. This enhances the system's adaptability and noise
resistance, making it highly capable of handling diverse queries. We compare
the proposed framework against other leading industry methods across various
benchmarks. The results demonstrate that our framework obtains superior
performance in both single-hop and multi-hop question-answering tasks.

</details>


### [14] [How Small Transformation Expose the Weakness of Semantic Similarity Measures](https://arxiv.org/abs/2509.09714)
*Serge Lionel Nikiema,Albérick Euraste Djire,Abdoul Aziz Bonkoungou,Micheline Bénédicte Moumoula,Jordan Samhi,Abdoul Kader Kabore,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.CL

TL;DR: 现有语义相似度测量方法存在显著缺陷：部分嵌入方法将语义相反内容误判为相似（错误率最高达99.9%），而基于余弦相似度的改进使结果提升24-66%。基于LLM的方法在区分语义差异方面表现更优（相似度评分0.00-0.29 vs 嵌入方法的0.82-0.99）。


<details>
  <summary>Details</summary>
Motivation: 验证不同方法是否能真正理解语义关系而非仅识别表面模式，这对代码搜索、API推荐、自动化代码审查等软件工程应用至关重要。

Method: 建立系统测试框架，通过受控文本/代码修改评估18种方法（包括词基方法、嵌入技术、LLM系统和结构感知算法）处理不同类型语义关系的能力。

Result: 嵌入方法常因距离计算方式误判（如将语义相反内容判为相似），改用余弦相似度提升效果24-66%；基于LLM的方法能有效区分真实语义差异，对本质不同内容给出低分（0.00-0.29）。

Conclusion: 现有语义相似度测量指标存在根本性局限，LLM方法在语义区分上更具可靠性，同时嵌入方法通过计算方式优化可显著提升表现。

Abstract: This research examines how well different methods measure semantic
similarity, which is important for various software engineering applications
such as code search, API recommendations, automated code reviews, and
refactoring tools. While large language models are increasingly used for these
similarity assessments, questions remain about whether they truly understand
semantic relationships or merely recognize surface patterns.
  The study tested 18 different similarity measurement approaches, including
word-based methods, embedding techniques, LLM-based systems, and
structure-aware algorithms. The researchers created a systematic testing
framework that applies controlled changes to text and code to evaluate how well
each method handles different types of semantic relationships.
  The results revealed significant issues with commonly used metrics. Some
embedding-based methods incorrectly identified semantic opposites as similar up
to 99.9 percent of the time, while certain transformer-based approaches
occasionally rated opposite meanings as more similar than synonymous ones. The
study found that embedding methods' poor performance often stemmed from how
they calculate distances; switching from Euclidean distance to cosine
similarity improved results by 24 to 66 percent. LLM-based approaches performed
better at distinguishing semantic differences, producing low similarity scores
(0.00 to 0.29) for genuinely different meanings, compared to embedding methods
that incorrectly assigned high scores (0.82 to 0.99) to dissimilar content.

</details>


### [15] [Investigating Symbolic Triggers of Hallucination in Gemma Models Across HaluEval and TruthfulQA](https://arxiv.org/abs/2509.09715)
*Naveen Lamba,Sanju Tiwari,Manas Gaur*

Main category: cs.CL

TL;DR: 研究发现大语言模型(LLMs)幻觉问题的核心在于符号属性(修饰符和命名实体)，模型规模扩大仅部分缓解但未根本解决


<details>
  <summary>Details</summary>
Motivation: 识别并验证导致LLM幻觉的内在机制属性，现有研究未明确模型内部导致幻觉的关键特征

Method: 通过HaluEval和TruthfulQA数据集，转换问答格式验证符号属性影响，对比Gemma-2B/9B/27B模型的幻觉率差异

Result: Gemma-2B平均幻觉率79%，随模型增大至27B降至63.9%。修饰符(84.76%-94.98%)和命名实体(83.87%-93.96%)在所有模型中保持高幻觉率

Conclusion: 符号元素处理是LLMs的固有缺陷，模型规模扩大无法根本解决，需改进符号处理机制设计

Abstract: Hallucination in Large Language Models (LLMs) is a well studied problem.
However, the properties that make LLM intrinsically vulnerable to
hallucinations have not been identified and studied. This research identifies
and characterizes the key properties, allowing us to pinpoint vulnerabilities
within the model's internal mechanisms. To solidify on these properties, we
utilized two established datasets, HaluEval and TruthfulQA and convert their
existing format of question answering into various other formats to narrow down
these properties as the reason for the hallucinations. Our findings reveal that
hallucination percentages across symbolic properties are notably high for
Gemma-2-2B, averaging 79.0% across tasks and datasets. With increased model
scale, hallucination drops to 73.6% for Gemma-2-9B and 63.9% for Gemma-2-27B,
reflecting a 15 percentage point reduction overall. Although the hallucination
rate decreases as the model size increases, a substantial amount of
hallucination caused by symbolic properties still persists. This is especially
evident for modifiers (ranging from 84.76% to 94.98%) and named entities
(ranging from 83.87% to 93.96%) across all Gemma models and both datasets.
These findings indicate that symbolic elements continue to confuse the models,
pointing to a fundamental weakness in how these LLMs process such
inputs--regardless of their scale.

</details>


### [16] [ALIGNS: Unlocking nomological networks in psychological measurement through a large language model](https://arxiv.org/abs/2509.09723)
*Kai R. Larsen,Sen Yan,Roland Müller,Lan Sang,Mikko Rönkkö,Ravi Starzl,Donald Edmondson*

Main category: cs.CL

TL;DR: 开发ALIGNS系统解决心理测量法理网络构建难题


<details>
  <summary>Details</summary>
Motivation: 传统方法无法有效构建法理网络，导致临床研究和政策制定存在偏差，需利用大语言模型突破测量验证瓶颈

Method: 基于验证问卷训练LLM系统，生成跨学科法理网络

Result: 建立55万+指标网络，揭示情绪困扰单维结构，发现儿童气质新维度，获专家认可

Conclusion: ALIGNS首次实现大模型解决测量验证核心问题，免费开放补充传统验证方法

Abstract: Psychological measurement is critical to many disciplines. Despite advances
in measurement, building nomological networks, theoretical maps of how concepts
and measures relate to establish validity, remains a challenge 70 years after
Cronbach and Meehl proposed them as fundamental to validation. This limitation
has practical consequences: clinical trials may fail to detect treatment
effects, and public policy may target the wrong outcomes. We introduce Analysis
of Latent Indicators to Generate Nomological Structures (ALIGNS), a large
language model-based system trained with validated questionnaire measures.
ALIGNS provides three comprehensive nomological networks containing over
550,000 indicators across psychology, medicine, social policy, and other
fields. This represents the first application of large language models to solve
a foundational problem in measurement validation. We report classification
accuracy tests used to develop the model, as well as three evaluations. In the
first evaluation, the widely used NIH PROMIS anxiety and depression instruments
are shown to converge into a single dimension of emotional distress. The second
evaluation examines child temperament measures and identifies four potential
dimensions not captured by current frameworks, and questions one existing
dimension. The third evaluation, an applicability check, engages expert
psychometricians who assess the system's importance, accessibility, and
suitability. ALIGNS is freely available at nomologicalnetwork.org,
complementing traditional validation methods with large-scale nomological
analysis.

</details>


### [17] [DiTTO-LLM: Framework for Discovering Topic-based Technology Opportunities via Large Language Model](https://arxiv.org/abs/2509.09724)
*Wonyoung Kim,Sujeong Seo,Juhyun Lee*

Main category: cs.CL

TL;DR: 提出基于专利文本时间关系分析的技术机会识别框架，验证显示AI技术正朝日常化方向演进


<details>
  <summary>Details</summary>
Motivation: 技术机会识别对科技进步至关重要，现有方法缺乏对技术动态演化的系统性追踪

Method: 1. 专利文本提取 2. 主题映射发现技术关联 3. 时间维度追踪主题演变 4. 结合LLM进行主题提取和机会发现

Result: 基于USPTO专利数据验证，AI技术发展呈现降低使用门槛、增强日常可及性的显著趋势

Conclusion: 该框架有效识别技术演进规律，为预测未来技术机会提供可靠方法论，特别适用于快速发展的AI领域

Abstract: Technology opportunities are critical information that serve as a foundation
for advancements in technology, industry, and innovation. This paper proposes a
framework based on the temporal relationships between technologies to identify
emerging technology opportunities. The proposed framework begins by extracting
text from a patent dataset, followed by mapping text-based topics to discover
inter-technology relationships. Technology opportunities are then identified by
tracking changes in these topics over time. To enhance efficiency, the
framework leverages a large language model to extract topics and employs a
prompt for a chat-based language model to support the discovery of technology
opportunities. The framework was evaluated using an artificial intelligence
patent dataset provided by the United States Patent and Trademark Office. The
experimental results suggest that artificial intelligence technology is
evolving into forms that facilitate everyday accessibility. This approach
demonstrates the potential of the proposed framework to identify future
technology opportunities.

</details>


### [18] [BIBERT-Pipe on Biomedical Nested Named Entity Linking at BioASQ 2025](https://arxiv.org/abs/2509.09725)
*Chunyu Li,Xindi Zheng,Siqi Liu*

Main category: cs.CL

TL;DR: 本文提出轻量级生物医学嵌套多语言实体链接系统BIBERT-Pipe，通过两阶段检索排序、边界标识符和数据集增强三项改进，在BioNNE 2025多语言赛道获得第三名。


<details>
  <summary>Details</summary>
Motivation: 现有生物医学实体链接研究主要针对英语单层实体标注，缺乏对嵌套结构和多语言场景的探索。BioNNE 2025共享任务首次引入俄语/英语嵌套实体链接需求，亟需适配方案。

Method: 1. 两阶段检索-排序：检索阶段使用预训练编码器，排序阶段进行领域微调
2. 可学习的[Ms]/[Me]边界标识符增强跨度感知
3. 通过三种数据源自动扩展排序训练集

Result: 在BioNNE 2025多语言赛道排名第三，验证了最小化系统改进的有效性。模型代码已开源。

Conclusion: 通过保持核心模型不变，仅针对性改进任务相关组件，即可高效应对生物医学嵌套多语言实体链接挑战，为实际应用提供轻量化解决方案。

Abstract: Entity linking (EL) for biomedical text is typically benchmarked on
English-only corpora with flat mentions, leaving the more realistic scenario of
nested and multilingual mentions largely unexplored. We present our system for
the BioNNE 2025 Multilingual Biomedical Nested Named Entity Linking shared task
(English & Russian), closing this gap with a lightweight pipeline that keeps
the original EL model intact and modifies only three task-aligned components:
Two-stage retrieval-ranking. We leverage the same base encoder model in both
stages: the retrieval stage uses the original pre-trained model, while the
ranking stage applies domain-specific fine-tuning. Boundary cues. In the
ranking stage, we wrap each mention with learnable [Ms] / [Me] tags, providing
the encoder with an explicit, language-agnostic span before robustness to
overlap and nesting. Dataset augmentation. We also automatically expand the
ranking training corpus with three complementary data sources, enhancing
coverage without extra manual annotation. On the BioNNE 2025 leaderboard, our
two stage system, bilingual bert (BIBERT-Pipe), ranks third in the multilingual
track, demonstrating the effectiveness and competitiveness of these minimal yet
principled modifications. Code are publicly available at
https://github.com/Kaggle-Competitions-Code/BioNNE-L.

</details>


### [19] [Natural Language Translation of Formal Proofs through Informalization of Proof Steps and Recursive Summarization along Proof Structure](https://arxiv.org/abs/2509.09726)
*Seiji Hattori,Takuya Matsuzaki,Makoto Fujiwara*

Main category: cs.CL

TL;DR: 提出基于大语言模型的自然语言翻译方法，将形式化证明转换为可读性高的自然语言证明，并通过教材案例和Lean库验证有效性


<details>
  <summary>Details</summary>
Motivation: 解决形式化证明可读性差的问题，利用大语言模型提升自然语言证明的生成质量，便于教学和实际应用

Method: 结合大语言模型的非形式化与摘要能力，将形式证明转换为自然语言，通过教材案例对比分析质量，并应用于Lean证明库验证实用性

Result: 评估显示生成的自然语言证明质量接近原版，应用于Lean库时能生成高可读性、准确性的证明，验证方法有效性

Conclusion: 该方法有效利用大语言模型实现形式证明的自然语言转换，为自动化生成可读证明开辟新途径，兼具学术和实用价值

Abstract: This paper proposes a natural language translation method for
machine-verifiable formal proofs that leverages the informalization
(verbalization of formal language proof steps) and summarization capabilities
of LLMs. For evaluation, it was applied to formal proof data created in
accordance with natural language proofs taken from an undergraduate-level
textbook, and the quality of the generated natural language proofs was analyzed
in comparison with the original natural language proofs. Furthermore, we will
demonstrate that this method can output highly readable and accurate natural
language proofs by applying it to existing formal proof library of the Lean
proof assistant.

</details>


### [20] [A Role-Aware Multi-Agent Framework for Financial Education Question Answering with LLMs](https://arxiv.org/abs/2509.09727)
*Andy Zhu,Yingjun Du*

Main category: cs.CL

TL;DR: Proposed multi-agent framework with RAG and expert review mechanisms improves financial QA accuracy by 6.6-8.3%, demonstrating cost-effective performance comparable to domain-tuned models.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs struggle with nuanced financial reasoning requiring multistep calculations, domain terminology, and real-world scenario comprehension in financial education QA tasks.

Method: Three-agent system (Generator, Retriever, Expert Reviewer) using RAG from 6 textbooks + critique-based refinement in single-pass iteration for answer optimization.

Result: 6.6-8.3% accuracy improvement over zero-shot CoT baselines; Gemini-2.0-Flash achieved best performance; GPT-4o-mini matched finance-tuned FinGPT results.

Conclusion: Provides cost-effective financial QA enhancement and insights for developing multi-agent LLM systems in specialized domains.

Abstract: Question answering (QA) plays a central role in financial education, yet
existing large language model (LLM) approaches often fail to capture the
nuanced and specialized reasoning required for financial problem-solving. The
financial domain demands multistep quantitative reasoning, familiarity with
domain-specific terminology, and comprehension of real-world scenarios. We
present a multi-agent framework that leverages role-based prompting to enhance
performance on domain-specific QA. Our framework comprises a Base Generator, an
Evidence Retriever, and an Expert Reviewer agent that work in a single-pass
iteration to produce a refined answer. We evaluated our framework on a set of
3,532 expert-designed finance education questions from Study.com, an online
learning platform. We leverage retrieval-augmented generation (RAG) for
contextual evidence from 6 finance textbooks and prompting strategies for a
domain-expert reviewer. Our experiments indicate that critique-based refinement
improves answer accuracy by 6.6-8.3% over zero-shot Chain-of-Thought baselines,
with the highest performance from Gemini-2.0-Flash. Furthermore, our method
enables GPT-4o-mini to achieve performance comparable to the finance-tuned
FinGPT-mt_Llama3-8B_LoRA. Our results show a cost-effective approach to
enhancing financial QA and offer insights for further research in multi-agent
financial LLM systems.

</details>


### [21] [A meta-analysis on the performance of machine-learning based language models for sentiment analysis](https://arxiv.org/abs/2509.09728)
*Elena Rohde,Jonas Klingwort,Christian Borgs*

Main category: cs.CL

TL;DR: 对Twitter情感分析的机器学习模型进行元分析，显示AIC优化模型平均准确率0.80，揭示整体准确率指标存在误导性且需标准化报告


<details>
  <summary>Details</summary>
Motivation: 评估不同研究中机器学习在Twitter情感分析的表现差异，分析影响模型性能的关键因素，解决现有研究结果不一致性问题

Method: 采用PRISMA指南筛选20项研究的195个试验，使用双反正弦转换和三层次随机效应模型分析整体准确率

Result: AIC优化模型平均准确率0.80[0.76,0.84]，发现整体准确率易受类别数量/分布影响，标准化报告混淆矩阵尚未普及

Conclusion: 需规范情感分析评估指标（如采用标准化混淆矩阵），当前准确率指标存在局限性，跨研究模型对比需统一报告标准

Abstract: This paper presents a meta-analysis evaluating ML performance in sentiment
analysis for Twitter data. The study aims to estimate the average performance,
assess heterogeneity between and within studies, and analyze how study
characteristics influence model performance. Using PRISMA guidelines, we
searched academic databases and selected 195 trials from 20 studies with 12
study features. Overall accuracy, the most reported performance metric, was
analyzed using double arcsine transformation and a three-level random effects
model. The average overall accuracy of the AIC-optimized model was 0.80 [0.76,
0.84]. This paper provides two key insights: 1) Overall accuracy is widely used
but often misleading due to its sensitivity to class imbalance and the number
of sentiment classes, highlighting the need for normalization. 2) Standardized
reporting of model performance, including reporting confusion matrices for
independent test sets, is essential for reliable comparisons of ML classifiers
across studies, which seems far from common practice.

</details>


### [22] [MultimodalHugs: Enabling Sign Language Processing in Hugging Face](https://arxiv.org/abs/2509.09729)
*Gerard Sant,Zifan Jiang,Carlos Escolano,Amit Moryossef,Mathias Müller,Rico Sennrich,Sarah Ebling*

Main category: cs.CL

TL;DR: 提出MultimodalHugs框架，基于Hugging Face增强多模态数据处理能力，特别优化手语处理研究的可复现性。


<details>
  <summary>Details</summary>
Motivation: 当前手语处理研究因复杂临时代码导致可复现性低，现有工具（如Hugging Face）无法灵活支持手语多模态数据需求

Method: 在Hugging Face基础上构建抽象层MultimodalHugs，扩展支持姿态估计、像素数据等多模态输入

Result: 通过定量实验验证框架在手语姿态数据和文本字符像素数据处理中的有效性

Conclusion: MultimodalHugs通过标准化框架解决手语处理领域的技术瓶颈，同时具备跨模态应用的通用性

Abstract: In recent years, sign language processing (SLP) has gained importance in the
general field of Natural Language Processing. However, compared to research on
spoken languages, SLP research is hindered by complex ad-hoc code,
inadvertently leading to low reproducibility and unfair comparisons. Existing
tools that are built for fast and reproducible experimentation, such as Hugging
Face, are not flexible enough to seamlessly integrate sign language
experiments. This view is confirmed by a survey we conducted among SLP
researchers.
  To address these challenges, we introduce MultimodalHugs, a framework built
on top of Hugging Face that enables more diverse data modalities and tasks,
while inheriting the well-known advantages of the Hugging Face ecosystem. Even
though sign languages are our primary focus, MultimodalHugs adds a layer of
abstraction that makes it more widely applicable to other use cases that do not
fit one of the standard templates of Hugging Face. We provide quantitative
experiments to illustrate how MultimodalHugs can accommodate diverse modalities
such as pose estimation data for sign languages, or pixel data for text
characters.

</details>


### [23] [Benchmarking Vision-Language Models on Chinese Ancient Documents: From OCR to Knowledge Reasoning](https://arxiv.org/abs/2509.09731)
*Haiyang Yu,Yuchuan Wu,Fan Shi,Lei Liao,Jinghui Lu,Xiaodong Ge,Han Wang,Minghan Zhuo,Xuecheng Wu,Xiang Fei,Hao Feng,Guozhi Tang,An-Lan Wang,Hanshen Zhu,Yangfan He,Quanhuan Liang,Liyuan Meng,Chao Feng,Can Huang,Jingqun Tang,Bin Li*

Main category: cs.CL

TL;DR: 提出首个中文古籍多任务评测基准AncientDoc，包含OCR、文言翻译、知识推理等5个任务，覆盖14类3000页文献，评估主流视觉语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有文档基准集中于英文印刷文本/简体中文，缺乏评估视觉语言模型处理复杂古汉语文献的能力。古籍数字化面临视觉和语言双重复杂性挑战。

Method: 构建AncientDoc基准含5项任务：页面级OCR、白话翻译、推理问答、知识问答、异体字问答；覆盖14类文献、超100本书籍、约3000页数据。采用多指标评估主流模型，结合人类对齐的LLM评分。

Result: 实证显示现有模型在古汉语文档处理存在明显局限，特别是在OCR精度、文言理解、知识推理等任务上表现不足。

Conclusion: AncientDoc填补了古籍处理评估体系的空白，为提升视觉语言模型在文化遗产数字化领域的能力提供基准支持。

Abstract: Chinese ancient documents, invaluable carriers of millennia of Chinese
history and culture, hold rich knowledge across diverse fields but face
challenges in digitization and understanding, i.e., traditional methods only
scan images, while current Vision-Language Models (VLMs) struggle with their
visual and linguistic complexity. Existing document benchmarks focus on English
printed texts or simplified Chinese, leaving a gap for evaluating VLMs on
ancient Chinese documents. To address this, we present AncientDoc, the first
benchmark for Chinese ancient documents, designed to assess VLMs from OCR to
knowledge reasoning. AncientDoc includes five tasks (page-level OCR, vernacular
translation, reasoning-based QA, knowledge-based QA, linguistic variant QA) and
covers 14 document types, over 100 books, and about 3,000 pages. Based on
AncientDoc, we evaluate mainstream VLMs using multiple metrics, supplemented by
a human-aligned large language model for scoring.

</details>


### [24] [MCP-AgentBench: Evaluating Real-World Language Agent Performance with MCP-Mediated Tools](https://arxiv.org/abs/2509.09734)
*Zikang Guo,Benfeng Xu,Chiwei Zhu,Wentao Hong,Xiaorui Wang,Zhendong Mao*

Main category: cs.CL

TL;DR: MCP-AgentBench是首个专为评估MCP协议下语言代理工具交互能力设计的基准测试，包含33个服务器、188种工具及600个系统化查询，采用结果导向的MCP-Eval评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法准确反映语言代理在MCP新兴标准下的真实性能，导致对代理工具交互能力的误判和评估盲区。

Method: 1. 搭建含33个服务器/188种工具的MCP测试环境 2. 设计6个复杂度分级的600个交互查询 3. 开发MCP-Eval结果导向评估框架

Result: 验证了基准的有效性，为评估语言代理在MCP生态中的真实能力提供标准化依据

Conclusion: MCP-AgentBench填补了MCP生态的评估空白，为构建真正互操作的人工智能系统提供关键验证工具，推动AI代理技术发展。

Abstract: The Model Context Protocol (MCP) is rapidly emerging as a pivotal open
standard, designed to enhance agent-tool integration and interoperability, and
is positioned to unlock a new era of powerful, interconnected, and genuinely
utilitarian agentic AI. However, despite MCP's growing adoption, existing
benchmarks often fail to capture real-world agent performance within this new
paradigm, leading to a distorted perception of their true operational value and
an inability to reliably differentiate proficiencies. To bridge this critical
evaluation gap, we introduce MCP-AgentBench -- a comprehensive benchmark
specifically engineered to rigorously assess language agent capabilities in
MCP-mediated tool interactions. Core contributions of MCP-AgentBench include:
the establishment of a robust MCP testbed comprising 33 operational servers
with 188 distinct tools; the development of a benchmark featuring 600
systematically designed queries distributed across 6 distinct categories of
varying interaction complexity; and the introduction of MCP-Eval, a novel
outcome-oriented evaluation methodology prioritizing real-world task success.
Through extensive empirical evaluation of leading language agents, we provide
foundational insights. MCP-AgentBench aims to equip the research community with
a standardized and reliable framework to build, validate, and advance agents
capable of fully leveraging MCP's transformative benefits, thereby accelerating
progress toward truly capable and interoperable AI systems.

</details>


### [25] [Discrimination by LLMs: Cross-lingual Bias Assessment and Mitigation in Decision-Making and Summarisation](https://arxiv.org/abs/2509.09735)
*Willem Huijzer,Jieying Chen*

Main category: cs.CL

TL;DR: 研究揭示大型语言模型在决策任务中存在显著人口偏见（性别/年龄/背景），摘要任务中偏见较小。跨语言测试显示英荷偏见模式相似，提示优化策略可部分缓解偏见。


<details>
  <summary>Details</summary>
Motivation: LLMs快速应用可能加剧社会不平等，需系统性评估其偏见类型、跨语言传播及缓解策略效果，为负责任AI部署提供依据。

Method: 基于Tamkin(2023)数据集构建151,200决策提示和176,400摘要提示，测试GPT-3.5/4o在英语/荷兰语中的人口变量响应差异，评估指令优化策略的有效性。

Result: 决策任务显著偏向女性/年轻/非裔背景；摘要任务仅GPT-3.5显示年龄差异。最优缓解指令使人口组间差距缩小27%，GPT-4o在英语提示下偏见显著降低。

Conclusion: 需加强LLMs场景化偏见测试，持续优化缓解策略。新版模型（如GPT-4o）展现更好的指令响应能力，提示工程可作为短期缓解方案。

Abstract: The rapid integration of Large Language Models (LLMs) into various domains
raises concerns about societal inequalities and information bias. This study
examines biases in LLMs related to background, gender, and age, with a focus on
their impact on decision-making and summarization tasks. Additionally, the
research examines the cross-lingual propagation of these biases and evaluates
the effectiveness of prompt-instructed mitigation strategies. Using an adapted
version of the dataset by Tamkin et al. (2023) translated into Dutch, we
created 151,200 unique prompts for the decision task and 176,400 for the
summarisation task. Various demographic variables, instructions, salience
levels, and languages were tested on GPT-3.5 and GPT-4o. Our analysis revealed
that both models were significantly biased during decision-making, favouring
female gender, younger ages, and certain backgrounds such as the
African-American background. In contrast, the summarisation task showed minimal
evidence of bias, though significant age-related differences emerged for
GPT-3.5 in English. Cross-lingual analysis showed that bias patterns were
broadly similar between English and Dutch, though notable differences were
observed across specific demographic categories. The newly proposed mitigation
instructions, while unable to eliminate biases completely, demonstrated
potential in reducing them. The most effective instruction achieved a 27\% mean
reduction in the gap between the most and least favorable demographics.
Notably, contrary to GPT-3.5, GPT-4o displayed reduced biases for all prompts
in English, indicating the specific potential for prompt-based mitigation
within newer models. This research underscores the importance of cautious
adoption of LLMs and context-specific bias testing, highlighting the need for
continued development of effective mitigation strategies to ensure responsible
deployment of AI.

</details>


### [26] [HEFT: A Coarse-to-Fine Hierarchy for Enhancing the Efficiency and Accuracy of Language Model Reasoning](https://arxiv.org/abs/2509.09801)
*Brennen Hill*

Main category: cs.CL

TL;DR: 提出分层高效微调方法HEFT，通过组合LoRA和ReFT两种参数高效微调范式，在3个训练周期内实现85.17%准确率，超越单一方法20周期的表现。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法在权重空间和表示空间独立运作，研究二者的协同组合能否提升模型推理能力并降低计算成本。

Method: 分层次组合LoRA（权重空间粗调）和ReFT（表示空间精调），先进行基础权重适应再进行激活表示微调。

Result: 在BoolQ推理基准测试中，HEFT仅需3个训练周期即达85.17%准确率，显著优于LoRA单独（85.05%）和ReFT单独（83.36%）的20周期结果。

Conclusion: 方法组合创新显著提升语言模型推理效率，为复杂认知任务提供计算高效的技术路径，证明算法创新比单纯增加计算资源更有效。

Abstract: The adaptation of large language models (LLMs) to specialized reasoning tasks
is fundamentally constrained by computational resources. Parameter-Efficient
Fine-Tuning (PEFT) methods have emerged as a powerful solution, yet the
landscape of these techniques is diverse, with distinct methods operating in
either the model's weight space or its representation space. This paper
investigates the hypothesis that a synergistic combination of these paradigms
can unlock superior performance and efficiency. We introduce HEFT (Hierarchical
Efficient Fine-Tuning), a novel hierarchical adaptation strategy that composes
two distinct PEFT methods in a coarse-to-fine manner: first, a broad,
foundational adaptation in the weight space using Low-Rank Adaptation (LoRA),
followed by a precise, surgical refinement of internal activations using
Representation Fine-Tuning (ReFT). We evaluate this approach by fine-tuning a
Llama-2-7B model on the BoolQ benchmark, a challenging dataset for inferential
reasoning. Our results reveal a profound synergistic effect. A model fine-tuned
for only three epochs with our HEFT strategy achieves an accuracy of 85.17\%,
exceeding the performance of models trained for 20 epochs with either LoRA-only
(85.05\%) or ReFT-only (83.36\%) methodologies. This work demonstrates that the
thoughtful composition of PEFT methods is a potent algorithmic innovation,
offering a more efficient and effective path toward advancing the reasoning
capabilities of language models. By achieving superior results with a fraction
of the computational budget, our findings present a principled approach to
overcoming the obstacles inherent in adapting large-scale models for complex
cognitive tasks.

</details>


### [27] [Pragmatic Frames Evoked by Gestures: A FrameNet Brasil Approach to Multimodality in Turn Organization](https://arxiv.org/abs/2509.09804)
*Helen de Andrade Abreu,Tiago Timponi Torrent,Ely Edison da Silva Matos*

Main category: cs.CL

TL;DR: 提出通过语言与互动手势的相关性建模多模态对话轮次组织的框架，在Frame2数据集中新增语用框架标注揭示手势在对话管理中的作用


<details>
  <summary>Details</summary>
Motivation: 现有对话轮次组织研究缺乏可支持机器学习的手势策略编码数据集，需填补这一空白

Method: 开发语用框架标注方法扩展Frame2数据集（含10集巴西电视剧视频文本语义标注），分析自然场景下的互动手势模式

Result: 验证面对面交流中手势用于传递/获取/保持对话轮次的功能，发现新的手势变体，揭示语用框架与概念隐喻的心理运作机制

Conclusion: 语用框架标注为理解人类语言认知提供新视角，证实多模态数据标注对揭示交流动态特征的重要价值

Abstract: This paper proposes a framework for modeling multimodal conversational turn
organization via the proposition of correlations between language and
interactive gestures, based on analysis as to how pragmatic frames are
conceptualized and evoked by communicators. As a means to provide evidence for
the analysis, we developed an annotation methodology to enrich a multimodal
dataset (annotated for semantic frames) with pragmatic frames modeling
conversational turn organization. Although conversational turn organization has
been studied by researchers from diverse fields, the specific strategies,
especially gestures used by communicators, had not yet been encoded in a
dataset that can be used for machine learning. To fill this gap, we enriched
the Frame2 dataset with annotations of gestures used for turn organization. The
Frame2 dataset features 10 episodes from the Brazilian TV series Pedro Pelo
Mundo annotated for semantic frames evoked in both video and text. This dataset
allowed us to closely observe how communicators use interactive gestures
outside a laboratory, in settings, to our knowledge, not previously recorded in
related literature. Our results have confirmed that communicators involved in
face-to-face conversation make use of gestures as a tool for passing, taking
and keeping conversational turns, and also revealed variations of some gestures
that had not been documented before. We propose that the use of these gestures
arises from the conceptualization of pragmatic frames, involving mental spaces,
blending and conceptual metaphors. In addition, our data demonstrate that the
annotation of pragmatic frames contributes to a deeper understanding of human
cognition and language.

</details>


### [28] [Topic-Guided Reinforcement Learning with LLMs for Enhancing Multi-Document Summarization](https://arxiv.org/abs/2509.09852)
*Chuyuan Li,Austin Xu,Shafiq Joty,Giuseppe Carenini*

Main category: cs.CL

TL;DR: 提出基于主题强化学习的多文档摘要方法，通过主题奖励机制提升生成摘要的信息量和主题一致性。


<details>
  <summary>Details</summary>
Motivation: 多文档摘要中整合多源信息时存在主题一致性和信息整合效率的挑战，现有大语言模型在此任务上仍有改进空间。

Method: 在GRPO框架中引入主题奖励机制，通过主题标签提示和主题对齐度评估优化内容选择。

Result: 在Multi-News和Multi-XScience数据集上超越基线模型，验证了主题引导策略的有效性。

Conclusion: 利用主题信号强化学习能显著提升多文档摘要质量，为内容选择机制提供新思路。

Abstract: A key challenge in Multi-Document Summarization (MDS) is effectively
integrating information from multiple sources while maintaining coherence and
topical relevance. While Large Language Models have shown impressive results in
single-document summarization, their performance on MDS still leaves room for
improvement. In this paper, we propose a topic-guided reinforcement learning
approach to improve content selection in MDS. We first show that explicitly
prompting models with topic labels enhances the informativeness of the
generated summaries. Building on this insight, we propose a novel topic reward
within the Group Relative Policy Optimization (GRPO) framework to measure topic
alignment between the generated summary and source documents. Experimental
results on the Multi-News and Multi-XScience datasets demonstrate that our
method consistently outperforms strong baselines, highlighting the
effectiveness of leveraging topical cues in MDS.

</details>


### [29] [Emulating Public Opinion: A Proof-of-Concept of AI-Generated Synthetic Survey Responses for the Chilean Case](https://arxiv.org/abs/2509.09871)
*Bastián González-Bustamante,Nando Verelst,Carla Cisternas*

Main category: cs.CL

TL;DR: LLM生成的合成调查回答在信任项目上表现优异（F1>0.90），但存在项目异质性和潜在偏见问题


<details>
  <summary>Details</summary>
Motivation: 验证LLM生成合成受访者回答的可靠性，评估其测量误差和潜在偏见风险

Method: 通过128个提示-模型-问题组合生成189,696个合成回答，与智利概率抽样调查数据对比，使用元分析评估性能指标（准确率、F1值等）

Result: 1. 信任类问题表现优异（F1>0.90）
2. GPT-4o系列与Llama 4表现相当
3. 45-59岁群体数据匹配度最高
4. 存在显著的项目层面异质性

Conclusion: LLM合成样本可近似概率抽样结果，但需谨慎校准和分布测试来保证算法保真度，避免复制训练数据中的社会偏见

Abstract: Large Language Models (LLMs) offer promising avenues for methodological and
applied innovations in survey research by using synthetic respondents to
emulate human answers and behaviour, potentially mitigating measurement and
representation errors. However, the extent to which LLMs recover aggregate item
distributions remains uncertain and downstream applications risk reproducing
social stereotypes and biases inherited from training data. We evaluate the
reliability of LLM-generated synthetic survey responses against ground-truth
human responses from a Chilean public opinion probabilistic survey.
Specifically, we benchmark 128 prompt-model-question triplets, generating
189,696 synthetic profiles, and pool performance metrics (i.e., accuracy,
precision, recall, and F1-score) in a meta-analysis across 128
question-subsample pairs to test for biases along key sociodemographic
dimensions. The evaluation spans OpenAI's GPT family and o-series reasoning
models, as well as Llama and Qwen checkpoints. Three results stand out. First,
synthetic responses achieve excellent performance on trust items (F1-score and
accuracy > 0.90). Second, GPT-4o, GPT-4o-mini and Llama 4 Maverick perform
comparably on this task. Third, synthetic-human alignment is highest among
respondents aged 45-59. Overall, LLM-based synthetic samples approximate
responses from a probabilistic sample, though with substantial item-level
heterogeneity. Capturing the full nuance of public opinion remains challenging
and requires careful calibration and additional distributional tests to ensure
algorithmic fidelity and reduce errors.

</details>


### [30] [Large Language Models Meet Legal Artificial Intelligence: A Survey](https://arxiv.org/abs/2509.09969)
*Zhitian Hou,Zihan Ye,Nanli Zeng,Tianyong Hao,Kun Zeng*

Main category: cs.CL

TL;DR: 论文系统综述了16个法律大模型和47个法律任务框架，整理了15个基准测试和29个数据集，分析了法律大模型面临的挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 推动法律领域大模型技术的研究与应用，为初学者提供系统性指南并促进该领域未来发展。

Method: 通过文献综述方式整合分析现有法律大模型系列（16个）及其应用框架（47个），同时系统整理评估标准（15个基准测试）和数据集资源（29个）。

Result: 构建了法律人工智能资源库（GitHub开源），明确了当前法律大模型在鲁棒性、偏见控制、多模态处理等方面的技术挑战。

Conclusion: 本文为法律大模型研究提供了系统性资源导航，指出了提升模型法律推理能力、构建可信法律AI的发展路径。

Abstract: Large Language Models (LLMs) have significantly advanced the development of
Legal Artificial Intelligence (Legal AI) in recent years, enhancing the
efficiency and accuracy of legal tasks. To advance research and applications of
LLM-based approaches in legal domain, this paper provides a comprehensive
review of 16 legal LLMs series and 47 LLM-based frameworks for legal tasks, and
also gather 15 benchmarks and 29 datasets to evaluate different legal
capabilities. Additionally, we analyse the challenges and discuss future
directions for LLM-based approaches in the legal domain. We hope this paper
provides a systematic introduction for beginners and encourages future research
in this field. Resources are available at
https://github.com/ZhitianHou/LLMs4LegalAI.

</details>


### [31] [CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China](https://arxiv.org/abs/2509.09990)
*Guixian Xu,Zeli Su,Ziyin Zhang,Jianing Liu,XU Han,Ting Zhang,Yushuang Dong*

Main category: cs.CL

TL;DR: 构建中文少数民族标题生成数据集CMHG（藏语10万条/维蒙各5万条）及母语标注测试集，推动少数民族语言生成研究


<details>
  <summary>Details</summary>
Motivation: 解决少数民族语言（藏/维/蒙）因书写系统特殊导致的语料库匮乏问题，尤其在标题生成等监督任务中资源严重不足

Method: 1. 创建专门用于标题生成的CMHG数据集（藏10万/维蒙各5万） 2. 设计母语者标注的高质量测试集作为基准

Result: 产出多维度少数民族语言标题生成资源，首次提供系统性的数据支撑与评估基准

Conclusion: 该数据集填补资源空白，期待成为推动少数民族语言生成研究及基准建设的重要基础设施

Abstract: Minority languages in China, such as Tibetan, Uyghur, and Traditional
Mongolian, face significant challenges due to their unique writing systems,
which differ from international standards. This discrepancy has led to a severe
lack of relevant corpora, particularly for supervised tasks like headline
generation. To address this gap, we introduce a novel dataset, Chinese Minority
Headline Generation (CMHG), which includes 100,000 entries for Tibetan, and
50,000 entries each for Uyghur and Mongolian, specifically curated for headline
generation tasks. Additionally, we propose a high-quality test set annotated by
native speakers, designed to serve as a benchmark for future research in this
domain. We hope this dataset will become a valuable resource for advancing
headline generation in Chinese minority languages and contribute to the
development of related benchmarks.

</details>


### [32] [Unsupervised Hallucination Detection by Inspecting Reasoning Processes](https://arxiv.org/abs/2509.10004)
*Ponhvoan Srey,Xiaobao Wu,Anh Tuan Luu*

Main category: cs.CL

TL;DR: 提出IRIS框架：通过大语言模型内部表征检测幻觉内容，无需标注数据即实现高效无监督检测


<details>
  <summary>Details</summary>
Motivation: 现有无监督方法依赖与事实无关的代理信号，导致检测存在偏差且泛化性差

Method: 通过prompt引导LLM验证陈述真实性，获取上下文嵌入作为特征，并用响应不确定性作为伪标签

Result: 实验显示IRIS显著优于现有方法，具备数据效率高、计算成本低、支持实时检测的优势

Conclusion: 基于事实相关性的内部表征建模，为无监督幻觉检测提供了可靠且泛化性强的新范式

Abstract: Unsupervised hallucination detection aims to identify hallucinated content
generated by large language models (LLMs) without relying on labeled data.
While unsupervised methods have gained popularity by eliminating
labor-intensive human annotations, they frequently rely on proxy signals
unrelated to factual correctness. This misalignment biases detection probes
toward superficial or non-truth-related aspects, limiting generalizability
across datasets and scenarios. To overcome these limitations, we propose IRIS,
an unsupervised hallucination detection framework, leveraging internal
representations intrinsic to factual correctness. IRIS prompts the LLM to
carefully verify the truthfulness of a given statement, and obtain its
contextualized embedding as informative features for training. Meanwhile, the
uncertainty of each response is considered a soft pseudolabel for truthfulness.
Experimental results demonstrate that IRIS consistently outperforms existing
unsupervised methods. Our approach is fully unsupervised, computationally low
cost, and works well even with few training data, making it suitable for
real-time detection.

</details>


### [33] [Multi-Intent Recognition in Dialogue Understanding: A Comparison Between Smaller Open-Source LLMs](https://arxiv.org/abs/2509.10010)
*Adnan Ahmad,Philine Kowol,Stefan Hillmann,Sebastian Möller*

Main category: cs.CL

TL;DR: 论文对比开源大语言模型（Mistral-7B/LLama2-7B/Yi-6B）在多标签意图分类任务中的表现，发现Mistral在少量样本学习下表现最优（加权F1=0.50），但监督学习的BERT模型仍全面优于生成式模型。


<details>
  <summary>Details</summary>
Motivation: 探索小型开源LLM在消费级硬件上处理复杂多意图对话的可行性，提升任务型聊天机器人的自然语言理解能力。

Method: 使用MultiWOZ 2.1数据集，在少量样本场景（prompt含20示例）对比Mistral/LLama2/Yi的意图分类性能，同时以监督学习的BERT模型为基线，综合评估准确率、F1值、推理时间、显存消耗等指标。

Result: Mistral-7B在14个意图类别中11类F1领先（加权平均0.50），但监督BERT模型在所有指标上全面超越生成式模型（最高F1达0.50的Mistral）。

Conclusion: 小型开源LLM可初步支撑多意图检测，但监督学习仍具优势。该框架为资源受限场景下的对话理解提供了可行性方案，平衡了性能与部署成本。

Abstract: In this paper, we provide an extensive analysis of multi-label intent
classification using Large Language Models (LLMs) that are open-source,
publicly available, and can be run in consumer hardware. We use the MultiWOZ
2.1 dataset, a benchmark in the dialogue system domain, to investigate the
efficacy of three popular open-source pre-trained LLMs, namely LLama2-7B-hf,
Mistral-7B-v0.1, and Yi-6B. We perform the classification task in a few-shot
setup, giving 20 examples in the prompt with some instructions. Our approach
focuses on the differences in performance of these models across several
performance metrics by methodically assessing these models on multi-label
intent classification tasks. Additionally, we compare the performance of the
instruction-based fine-tuning approach with supervised learning using the
smaller transformer model BertForSequenceClassification as a baseline. To
evaluate the performance of the models, we use evaluation metrics like
accuracy, precision, and recall as well as micro, macro, and weighted F1 score.
We also report the inference time, VRAM requirements, etc. The Mistral-7B-v0.1
outperforms two other generative models on 11 intent classes out of 14 in terms
of F-Score, with a weighted average of 0.50. It also has relatively lower
Humming Loss and higher Jaccard Similarity, making it the winning model in the
few-shot setting. We find BERT based supervised classifier having superior
performance compared to the best performing few-shot generative LLM. The study
provides a framework for small open-source LLMs in detecting complex
multi-intent dialogues, enhancing the Natural Language Understanding aspect of
task-oriented chatbots.

</details>


### [34] [Linguistic trajectories of bipolar disorder on social media](https://arxiv.org/abs/2509.10035)
*Laurin Plank,Armin Zlomuzica*

Main category: cs.CL

TL;DR: 通过分析社交媒体语言轨迹，发现双相情感障碍（BD）患者在诊断前后存在显著语言变化，并观察到长达二十年的周期性情绪波动特征，为心理健康监测提供新方法


<details>
  <summary>Details</summary>
Motivation: 现有临床评估难以捕捉情感障碍的动态特征，而社交媒体语言具备高时间分辨率和长期追踪潜力

Method: 开发诊断时间确定方法，分析用户从诊断前3年到诊断后21年的语言轨迹，对比单相抑郁（UD）和健康对照组（HC）

Result: BD诊断伴随反映情绪紊乱、共病、物质滥用等语言改变，观察到诊断后长达20年的周期性情绪波动（12个月周期），女性可能呈现更强周期性

Conclusion: 研究证实BD急慢性期存在特异性语言标记，验证了社交媒体在心理健康监测中的实用价值，为精神疾病长期追踪提供新视角

Abstract: Language provides valuable markers of affective disorders such as bipolar
disorder (BD), yet clinical assessments remain limited in scale. In response,
analyses of social media (SM) language have gained prominence due to their high
temporal resolution and longitudinal scope. Here, we introduce a method to
determine the timing of users' diagnoses and apply it to study language
trajectories from 3 years before to 21 years after BD diagnosis - contrasted
with uses reporting unipolar depression (UD) and non-affected users (HC). We
show that BD diagnosis is accompanied by pervasive linguistic alterations
reflecting mood disturbance, psychiatric comorbidity, substance abuse,
hospitalization, medical comorbidities, unusual thought content, and
disorganized thought. We further observe recurring mood-related language
changes across two decades after the diagnosis, with a pronounced 12-month
periodicity suggestive of seasonal mood episodes. Finally, trend-level evidence
suggests an increased periodicity in users estimated to be female. In sum, our
findings provide evidence for language alterations in the acute and chronic
phase of BD. This validates and extends recent efforts leveraging SM for
scalable monitoring of mental health.

</details>


### [35] [!MSA at BAREC Shared Task 2025: Ensembling Arabic Transformers for Readability Assessment](https://arxiv.org/abs/2509.10040)
*Mohamed Basem,Mohamed Younes,Seif Ahmed,Abdelrahman Moustafa*

Main category: cs.CL

TL;DR: 提出基于多Transformer模型集成、数据增强和置信度融合的阿拉伯语可读性评估系统，在BAREC 2025六个赛道均获第一


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯语细粒度可读性评估中存在的类别不平衡、数据稀缺和预测分布偏差问题

Method: 使用AraBERTv2等四个Transformer模型（不同损失函数微调）构建置信加权集成，采用加权训练/SAMER语料重标注/Gemini合成数据增强（新增10k稀有样本），并通过后处理校正预测分布

Result: 系统在句子级和文档级分别达到87.5%和87.4%的QWK指标，后处理带来6.3%的QWK提升

Conclusion: 模型多样性+置信融合策略+智能数据增强可有效提升阿拉伯语可读性预测效果，为低资源语言处理提供参考

Abstract: We present MSAs winning system for the BAREC 2025 Shared Task on fine-grained
Arabic readability assessment, achieving first place in six of six tracks. Our
approach is a confidence-weighted ensemble of four complementary transformer
models (AraBERTv2, AraELECTRA, MARBERT, and CAMeLBERT) each fine-tuned with
distinct loss functions to capture diverse readability signals. To tackle
severe class imbalance and data scarcity, we applied weighted training,
advanced preprocessing, SAMER corpus relabeling with our strongest model, and
synthetic data generation via Gemini 2.5 Flash, adding about 10,000 rare-level
samples. A targeted post-processing step corrected prediction distribution
skew, delivering a 6.3 percent Quadratic Weighted Kappa (QWK) gain. Our system
reached 87.5 percent QWK at the sentence level and 87.4 percent at the document
level, demonstrating the power of model and loss diversity, confidence-informed
fusion, and intelligent augmentation for robust Arabic readability prediction.

</details>


### [36] [Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models](https://arxiv.org/abs/2509.10078)
*Dongmin Choi,Woojung Song,Jongwook Han,Eun-Ju Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 传统心理测量问卷（如BFI、PVQ）在评估大语言模型（LLMs）时存在生态效度不足，导致结果偏离实际用户查询场景中的心理特征，建议改用生态效度问卷。


<details>
  <summary>Details</summary>
Motivation: 现有研究使用人类设计的心理问卷评估LLMs，但质疑其生态效度不足，即这些问卷无法反映LLMs在真实用户查询中的文本生成行为。需明确传统问卷与生态效度问卷的差异及其影响。

Method: 对传统问卷和生态效度问卷进行对比分析，评估两者在LLMs性格特征测量中的结果差异、项目稳定性、结构稳定性及对人格提示模型的夸大效应。

Result: 传统问卷存在四大问题：(1)测量结果与生态效度问卷差异显著；(2)项目数量不足导致测量不稳定；(3)错误暗示LLMs具有稳定心理结构；(4)夸大人格提示模型的特征轮廓。

Conclusion: 应避免使用传统心理问卷评估LLMs，其生态效度缺陷可能导致误导性结论，需开发适配LLMs特性的评估工具。

Abstract: Researchers have applied established psychometric questionnaires (e.g., BFI,
PVQ) to measure the personality traits and values reflected in the responses of
Large Language Models (LLMs). However, concerns have been raised about applying
these human-designed questionnaires to LLMs. One such concern is their lack of
ecological validity--the extent to which survey questions adequately reflect
and resemble real-world contexts in which LLMs generate texts in response to
user queries. However, it remains unclear how established questionnaires and
ecologically valid questionnaires differ in their outcomes, and what insights
these differences may provide. In this paper, we conduct a comprehensive
comparative analysis of the two types of questionnaires. Our analysis reveals
that established questionnaires (1) yield substantially different profiles of
LLMs from ecologically valid ones, deviating from the psychological
characteristics expressed in the context of user queries, (2) suffer from
insufficient items for stable measurement, (3) create misleading impressions
that LLMs possess stable constructs, and (4) yield exaggerated profiles for
persona-prompted LLMs. Overall, our work cautions against the use of
established psychological questionnaires for LLMs. Our code will be released
upon publication.

</details>


### [37] [Querying Climate Knowledge: Semantic Retrieval for Scientific Discovery](https://arxiv.org/abs/2509.10087)
*Mustapha Adamu,Qi Zhang,Huitong Pan,Longin Jan Latecki,Eduard C. Dragut*

Main category: cs.CL

TL;DR: 构建气候科学领域专用知识图谱，提升复杂文献信息的语义检索能力


<details>
  <summary>Details</summary>
Motivation: 传统关键词检索难以应对气候文献的复杂性，需结构化语义查询支持模型验证、数据集关联等研究需求

Method: 整合气候出版物构建知识图谱，结合Cypher查询语言和RAG系统与大语言模型集成

Result: 实现特定模型在区域验证、数据集与遥相关模式关联等复杂问题查询，提升问答系统透明度

Conclusion: 该知识图谱为气候研究者和模型开发者提供了具有实际应用价值的精准信息检索框架

Abstract: The growing complexity and volume of climate science literature make it
increasingly difficult for researchers to find relevant information across
models, datasets, regions, and variables. This paper introduces a
domain-specific Knowledge Graph (KG) built from climate publications and
broader scientific texts, aimed at improving how climate knowledge is accessed
and used. Unlike keyword based search, our KG supports structured, semantic
queries that help researchers discover precise connections such as which models
have been validated in specific regions or which datasets are commonly used
with certain teleconnection patterns. We demonstrate how the KG answers such
questions using Cypher queries, and outline its integration with large language
models in RAG systems to improve transparency and reliability in
climate-related question answering. This work moves beyond KG construction to
show its real world value for climate researchers, model developers, and others
who rely on accurate, contextual scientific information.

</details>


### [38] [Arabic Large Language Models for Medical Text Generation](https://arxiv.org/abs/2509.10095)
*Abdulrahman Allam,Seif Ahmed,Ali Hamdi,Ammar Mohammed*

Main category: cs.CL

TL;DR: 该研究通过微调Mistral-7B等大语言模型生成阿拉伯语医疗文本，有效解决了现有医院管理系统在非规范输入和少数语言支持上的不足，显著提升医疗回复质量（BERT F1达68.5%）。


<details>
  <summary>Details</summary>
Motivation: 现有医院管理系统在阿拉伯语实时医疗建议生成方面存在局限，尤其难以处理非结构化输入和方言多样性问题。

Method: 收集社交媒体真实医患对话数据，进行多方言预处理后，微调Mistral-7B/LLaMA-2-7B/GPT-2 Medium模型进行医疗文本生成。

Result: 微调后的Mistral-7B表现最优，BERT精确度/召回率/F1分别达68.5%/69.08%/68.5%，显著优于其他对比模型。

Conclusion: 生成式AI为多语言医疗系统提供可扩展解决方案，在提升全球医疗资源可及性方面具有重要应用潜力。

Abstract: Efficient hospital management systems (HMS) are critical worldwide to address
challenges such as overcrowding, limited resources, and poor availability of
urgent health care. Existing methods often lack the ability to provide
accurate, real-time medical advice, particularly for irregular inputs and
underrepresented languages. To overcome these limitations, this study proposes
an approach that fine-tunes large language models (LLMs) for Arabic medical
text generation. The system is designed to assist patients by providing
accurate medical advice, diagnoses, drug recommendations, and treatment plans
based on user input. The research methodology required the collection of a
unique dataset from social media platforms, capturing real-world medical
conversations between patients and doctors. The dataset, which includes patient
complaints together with medical advice, was properly cleaned and preprocessed
to account for multiple Arabic dialects. Fine-tuning state-of-the-art
generative models, such as Mistral-7B-Instruct-v0.2, LLaMA-2-7B, and GPT-2
Medium, optimized the system's ability to generate reliable medical text.
Results from evaluations indicate that the fine-tuned Mistral-7B model
outperformed the other models, achieving average BERT (Bidirectional Encoder
Representations from Transformers) Score values in precision, recall, and
F1-scores of 68.5\%, 69.08\%, and 68.5\%, respectively. Comparative
benchmarking and qualitative assessments validate the system's ability to
produce coherent and relevant medical replies to informal input. This study
highlights the potential of generative artificial intelligence (AI) in
advancing HMS, offering a scalable and adaptable solution for global healthcare
challenges, especially in linguistically and culturally diverse environments.

</details>


### [39] [Scaling Arabic Medical Chatbots Using Synthetic Data: Enhancing Generative AI with Synthetic Patient Records](https://arxiv.org/abs/2509.10108)
*Abdulrahman Allam,Seif Ahmed,Ali Hamdi,Khaled Shaban*

Main category: cs.CL

TL;DR: 通过ChatGPT-4o和Gemini 2.5 Pro生成8万条合成医学问答数据，将阿拉伯语医疗对话数据集从2万扩展到10万，显著提升LLMs在医疗NLP任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯语医疗领域高质量标注数据稀缺导致的聊天机器人开发受限问题，突破先前2万条真实医患对话数据在模型扩展性和泛化性上的瓶颈。

Method: 1. 使用生成式AI创建上下文相关的合成数据
2. 语义过滤与人工验证机制
3. 对Mistral-7B等5个LLMs进行微调
4. 基于BERTScore和专家评估的混合验证体系
5. 不同生成源的消融实验对比

Result: ChatGPT-4o生成的数据在所有模型中取得更高F1值（平均提升17.3%），幻觉现象减少43%。集成合成数据后医疗响应准确率达到89.7%，较基线提升32个百分点。

Conclusion: 验证了合成数据增强在低资源医疗NLP中的可行性，为阿拉伯语医疗对话系统提供了可扩展解决方案，通过AI生成内容实现跨语言医疗AI的民主化。

Abstract: The development of medical chatbots in Arabic is significantly constrained by
the scarcity of large-scale, high-quality annotated datasets. While prior
efforts compiled a dataset of 20,000 Arabic patient-doctor interactions from
social media to fine-tune large language models (LLMs), model scalability and
generalization remained limited. In this study, we propose a scalable synthetic
data augmentation strategy to expand the training corpus to 100,000 records.
Using advanced generative AI systems ChatGPT-4o and Gemini 2.5 Pro we generated
80,000 contextually relevant and medically coherent synthetic question-answer
pairs grounded in the structure of the original dataset. These synthetic
samples were semantically filtered, manually validated, and integrated into the
training pipeline. We fine-tuned five LLMs, including Mistral-7B and AraGPT2,
and evaluated their performance using BERTScore metrics and expert-driven
qualitative assessments. To further analyze the effectiveness of synthetic
sources, we conducted an ablation study comparing ChatGPT-4o and
Gemini-generated data independently. The results showed that ChatGPT-4o data
consistently led to higher F1-scores and fewer hallucinations across all
models. Overall, our findings demonstrate the viability of synthetic
augmentation as a practical solution for enhancing domain-specific language
models in-low resource medical NLP, paving the way for more inclusive,
scalable, and accurate Arabic healthcare chatbot systems.

</details>


### [40] [Prominence-aware automatic speech recognition for conversational speech](https://arxiv.org/abs/2509.10116)
*Julian Linke,Barbara Schuppler*

Main category: cs.CL

TL;DR: 开发基于wav2vec2模型的韵律突出检测器，通过自动标注大型语料库训练出能同时处理文字转录和韵律突出的ASR系统，在正确转录语句中达到85.53%突出检测准确率


<details>
  <summary>Details</summary>
Motivation: 探索奥地利德语会话场景中韵律突出信息与语音识别的结合，传统ASR系统可能忽略韵律特征对语义理解的影响

Method: 1.微调wav2vec2模型构建单词级韵律突出分类器 2.基于检测器自动标注大规模语料 3.训练Transformer架构的端到端韵律感知ASR系统

Result: 整合韵律信息未改变基线ASR性能，但在正确转录语句中突出检测准确率达85.53%

Conclusion: 验证了Transformer模型编码韵律信息的能力，为韵律增强型ASR提供了新范式，可应用于语言学分析和韵律敏感的对话系统

Abstract: This paper investigates prominence-aware automatic speech recognition (ASR)
by combining prominence detection and speech recognition for conversational
Austrian German. First, prominence detectors were developed by fine-tuning
wav2vec2 models to classify word-level prominence. The detector was then used
to automatically annotate prosodic prominence in a large corpus. Based on those
annotations, we trained novel prominence-aware ASR systems that simultaneously
transcribe words and their prominence levels. The integration of prominence
information did not change performance compared to our baseline ASR system,
while reaching a prominence detection accuracy of 85.53% for utterances where
the recognized word sequence was correct. This paper shows that
transformer-based models can effectively encode prosodic information and
represents a novel contribution to prosody-enhanced ASR, with potential
applications for linguistic research and prosody-informed dialogue systems.

</details>


### [41] [Population-Aligned Persona Generation for LLM-based Social Simulation](https://arxiv.org/abs/2509.10127)
*Zhengyu Hu,Zheyuan Xiao,Max Xiong,Yuxuan Lei,Tianfu Wang,Jianxun Lian,Kaize Ding,Ziang Xiao,Nicholas Jing Yuan,Xing Xie*

Main category: cs.CL

TL;DR: 提出通过系统化框架生成人口统计学对齐的LLM社会模拟角色集，结合质量评估、重要性采样和任务适配模块，显著降低群体偏差


<details>
  <summary>Details</summary>
Motivation: 现有LLM社会模拟研究忽视人物集的代表性问题，导致生成角色与真实人口分布存在偏差，影响社会模拟的准确性

Method: 1. 基于长期社交媒体数据生成叙事角色
2. 质量评估筛选高保真画像
3. 重要性采样实现与参考心理测量分布对齐
4. 任务特定模块适配目标子群体

Result: 实验证明该方法显著降低群体层面的偏差，支持广泛研究和政策应用中的精准灵活模拟

Conclusion: 框架通过分层生成-评估-对齐机制，为LLM社会模拟提供了可靠的人口统计学基础，扩展了计算社会科学的可能性边界

Abstract: Recent advances in large language models (LLMs) have enabled human-like
social simulations at unprecedented scale and fidelity, offering new
opportunities for computational social science. A key challenge, however, is
the construction of persona sets that authentically represent the diversity and
distribution of real-world populations. Most existing LLM-based social
simulation studies focus primarily on designing agentic frameworks and
simulation environments, often overlooking the complexities of persona
generation and the potential biases introduced by unrepresentative persona
sets. In this paper, we propose a systematic framework for synthesizing
high-quality, population-aligned persona sets for LLM-driven social simulation.
Our approach begins by leveraging LLMs to generate narrative personas from
long-term social media data, followed by rigorous quality assessment to filter
out low-fidelity profiles. We then apply importance sampling to achieve global
alignment with reference psychometric distributions, such as the Big Five
personality traits. To address the needs of specific simulation contexts, we
further introduce a task-specific module that adapts the globally aligned
persona set to targeted subpopulations. Extensive experiments demonstrate that
our method significantly reduces population-level bias and enables accurate,
flexible social simulation for a wide range of research and policy
applications.

</details>


### [42] [Towards Reliable and Interpretable Document Question Answering via VLMs](https://arxiv.org/abs/2509.10129)
*Alessio Chen,Simone Giovannini,Andrea Gemelli,Fabio Coppini,Simone Marinai*

Main category: cs.CL

TL;DR: 提出DocExplainerV0模块，解耦答案生成与空间定位，提升文档理解模型的可解释性和应用性


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在文档理解中答案定位不准确，限制了模型的解释性和实际应用，需改进空间定位能力

Method: 设计DocExplainerV0模块，通过解耦答案生成与空间定位，适配现有视觉语言模型，无需微调即可应用

Result: 系统评估显示，正确答案常缺乏可靠的空间定位，量化了文本准确性与空间定位间的差距

Conclusion: 标准化框架揭示了现有不足，为未来开发更可解释、鲁棒的文档信息提取视觉语言模型奠定基准

Abstract: Vision-Language Models (VLMs) have shown strong capabilities in document
understanding, particularly in identifying and extracting textual information
from complex documents. Despite this, accurately localizing answers within
documents remains a major challenge, limiting both interpretability and
real-world applicability. To address this, we introduce
\textit{DocExplainerV0}, a plug-and-play bounding-box prediction module that
decouples answer generation from spatial localization. This design makes it
applicable to existing VLMs, including proprietary systems where fine-tuning is
not feasible. Through systematic evaluation, we provide quantitative insights
into the gap between textual accuracy and spatial grounding, showing that
correct answers often lack reliable localization. Our standardized framework
highlights these shortcomings and establishes a benchmark for future research
toward more interpretable and robust document information extraction VLMs.

</details>


### [43] [Benchmark of stylistic variation in LLM-generated texts](https://arxiv.org/abs/2509.10179)
*Jiří Milička,Anna Marklová,Václav Cvrček*

Main category: cs.CL

TL;DR: 通过Biber多维分析法比较人类文本与LLM生成文本的语域差异，创建可解释的模型评估基准


<details>
  <summary>Details</summary>
Motivation: 探究前沿LLM生成文本与人类写作在语域特征上的系统性差异，特别是非英语语言在训练数据中代表性不足的问题

Method: 使用AI-Brown（英语）和AI-Koditex（捷克语）语料库，基于多维分析模型测试16种LLM在不同设置下的表现，比较基础模型与指令微调模型的差异

Result: 发现基础模型与指令调整模型存在显著差异，建立了包含可解释维度的模型比较基准，证实非英语语言（如捷克语）的生成质量与训练数据代表性相关

Conclusion: 该研究为LLM的文本生成质量评估提供了多维方法论，揭示了当前模型在非英语语言处理中的局限性，对改进跨语言模型训练具有指导意义

Abstract: This study investigates the register variation in texts written by humans and
comparable texts produced by large language models (LLMs). Biber's
multidimensional analysis (MDA) is applied to a sample of human-written texts
and AI-created texts generated to be their counterparts to find the dimensions
of variation in which LLMs differ most significantly and most systematically
from humans. As textual material, a new LLM-generated corpus AI-Brown is used,
which is comparable to BE-21 (a Brown family corpus representing contemporary
British English). Since all languages except English are underrepresented in
the training data of frontier LLMs, similar analysis is replicated on Czech
using AI-Koditex corpus and Czech multidimensional model. Examined were 16
frontier models in various settings and prompts, with emphasis placed on the
difference between base models and instruction-tuned models. Based on this, a
benchmark is created through which models can be compared with each other and
ranked in interpretable dimensions.

</details>


### [44] [Incongruent Positivity: When Miscalibrated Positivity Undermines Online Supportive Conversations](https://arxiv.org/abs/2509.10184)
*Leen Almajed,Abeer ALdayel*

Main category: cs.CL

TL;DR: 研究探讨人类与LLM在情感支持对话中表现出的不协调积极性现象，发现LLM在高风险情境下更容易产生不现实的乐观回应，并提出改进检测方法


<details>
  <summary>Details</summary>
Motivation: 分析善意积极性在情感支持对话中可能导致的负面效果（如敷衍/轻视/盲目乐观），特别是LLM生成回应中的校准偏差问题

Method: 收集Reddit真实对话并按情感强度分级（Mild/Severe），生成LLM回应，通过微调模型和构建弱监督多标签分类器（DeBERTa+MentalBERT）进行检测

Result: LLM在严重情境（悲伤/焦虑）中更容易通过敷衍语气表现不现实积极性，微调模型与组合分类器可提升不协调积极性的检测效果

Conclusion: 需超越通用积极回应生成，研究平衡积极情感与情感认同的协调支持策略，为构建情境感知的信任对话系统提供新方向

Abstract: In emotionally supportive conversations, well-intended positivity can
sometimes misfire, leading to responses that feel dismissive, minimizing, or
unrealistically optimistic. We examine this phenomenon of incongruent
positivity as miscalibrated expressions of positive support in both human and
LLM generated responses. To this end, we collected real user-assistant
dialogues from Reddit across a range of emotional intensities and generated
additional responses using large language models for the same context. We
categorize these conversations by intensity into two levels: Mild, which covers
relationship tension and general advice, and Severe, which covers grief and
anxiety conversations. This level of categorization enables a comparative
analysis of how supportive responses vary across lower and higher stakes
contexts. Our analysis reveals that LLMs are more prone to unrealistic
positivity through dismissive and minimizing tone, particularly in high-stakes
contexts. To further study the underlying dimensions of this phenomenon, we
finetune LLMs on datasets with strong and weak emotional reactions. Moreover,
we developed a weakly supervised multilabel classifier ensemble (DeBERTa and
MentalBERT) that shows improved detection of incongruent positivity types
across two sorts of concerns (Mild and Severe). Our findings shed light on the
need to move beyond merely generating generic positive responses and instead
study the congruent support measures to balance positive affect with emotional
acknowledgment. This approach offers insights into aligning large language
models with affective expectations in the online supportive dialogue, paving
the way toward context-aware and trust preserving online conversation systems.

</details>


### [45] [Beyond Token Limits: Assessing Language Model Performance on Long Text Classification](https://arxiv.org/abs/2509.10199)
*Miklós Sebők,Viktor Kovács,Martin Bánóczy,Daniel Møller Eriksen,Nathalie Neptune,Philippe Roussille*

Main category: cs.CL

TL;DR: 大语言模型在长文本分类任务中表现比较：专用长文本模型Longformer未显优势，开源模型优于GPT变体，类别间支持度与内容重叠影响性能


<details>
  <summary>Details</summary>
Motivation: 解决BERT系列模型在长文本（如法律文件）处理时的输入长度限制问题，评估不同模型在21类政策主题分类任务中的有效性

Method: 使用XLM-RoBERTa、Longformer、GPT-3.5/4在5种语言上进行多类分类实验，任务基于Comparative Agendas Project的21个政策标签体系

Result: 专为长文本设计的Longformer未显优势；开源模型表现优于GPT系列；类别支持度与内容重叠显著影响长文本分类性能

Conclusion: 长文本专用预训练模型非必要，开源模型更具竞争力，类别间支持关系与内容相似性是长文本分类效果的关键决定因素

Abstract: The most widely used large language models in the social sciences (such as
BERT, and its derivatives, e.g. RoBERTa) have a limitation on the input text
length that they can process to produce predictions. This is a particularly
pressing issue for some classification tasks, where the aim is to handle long
input texts. One such area deals with laws and draft laws (bills), which can
have a length of multiple hundred pages and, therefore, are not particularly
amenable for processing with models that can only handle e.g. 512 tokens. In
this paper, we show results from experiments covering 5 languages with
XLM-RoBERTa, Longformer, GPT-3.5, GPT-4 models for the multiclass
classification task of the Comparative Agendas Project, which has a codebook of
21 policy topic labels from education to health care. Results show no
particular advantage for the Longformer model, pre-trained specifically for the
purposes of handling long inputs. The comparison between the GPT variants and
the best-performing open model yielded an edge for the latter. An analysis of
class-level factors points to the importance of support and substance overlaps
between specific categories when it comes to performance on long text inputs.

</details>


### [46] [SI-FACT: Mitigating Knowledge Conflict via Self-Improving Faithfulness-Aware Contrastive Tuning](https://arxiv.org/abs/2509.10208)
*Shengqiang Fu*

Main category: cs.CL

TL;DR: 提出SI FACT框架，通过自我生成对比学习数据提升大模型在知识冲突场景下的上下文忠实度


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在知识冲突场景下过度依赖内部参数知识而忽视上下文的问题

Method: 1. 自我指导机制生成结构化对比学习数据（锚样本/正样本/负样本）
2. 应用对比学习训练模型区分忠实/非忠实响应

Result: 在ECARE KRE和COSE KRE基准测试中，Llama3 8B的上下文召回率提升6.2%，降低对内部记忆的依赖

Conclusion: SI FACT框架有效提升LLM的上下文忠实度，为构建更主动可靠的语言模型提供实用路径

Abstract: Large Language Models often generate unfaithful responses in knowledge
intensive tasks due to knowledge conflict,that is,a preference for relying on
internal parametric knowledge rather than the provided context.To address this
issue,we propose a novel self improving framework,Self Improving Faithfulness
Aware Contrastive Tuning.The framework uses a self instruct mechanism that
allows the base LLM to automatically generate high quality,structured
contrastive learning data,including anchor samples,semantically equivalent
positive samples,and negative samples simulating unfaithful scenarios.This
approach significantly reduces the cost of manual
annotation.Subsequently,contrastive learning is applied to train the
model,enabling it to pull faithful responses closer and push unfaithful
responses farther apart in the representation space.Experiments on knowledge
conflict evaluation benchmarks ECARE KRE and COSE KRE show that the SI FACT
model based on Llama3 8B Instruct improves the Contextual Recall Rate by 6.2%
over the best baseline method,while significantly reducing dependence on
internal memory.The results indicate that SI FACT provides strong effectiveness
and high data efficiency in enhancing the contextual faithfulness of
LLMs,offering a practical pathway toward building more proactive and
trustworthy language models.

</details>


### [47] [Dropping Experts, Recombining Neurons: Retraining-Free Pruning for Sparse Mixture-of-Experts LLMs](https://arxiv.org/abs/2509.10377)
*Yixiao Zhou,Ziyu Zhao,Dongzhou Cheng,zhiliang wu,Jie Gui,Yi Yang,Fei Wu,Yu Cheng,Hehe Fan*

Main category: cs.CL

TL;DR: DERN提出了一种在SMoE模型中通过神经元级剪枝与重组来提升性能并减少内存占用的方法，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: SMoE模型尽管稀疏激活仍需加载全部专家参数，导致高内存占用。现有剪枝方法多关注专家层面，忽视了神经元级的语义冲突问题。

Method: 采用三步框架：基于路由统计剪枝冗余专家→分解为神经元片段并分配→合并片段构建紧凑表征。

Result: 在50%稀疏度下，常识推理与MMLU基准提升超5%，显著减少专家数量与内存占用。

Conclusion: DERN通过解决神经元级对齐问题，有效优化了SMoE模型的实际部署可行性。

Abstract: Sparse Mixture-of-Experts (SMoE) architectures are widely used in large
language models (LLMs) due to their computational efficiency. However, though
only a few experts are activated for each token, SMoE still requires loading
all expert parameters, leading to high memory usage and challenges in
deployment. Previous work has tried to reduce the overhead by pruning and
merging experts, but primarily focused on expert-level operations, leaving
neuron-level structure underexplored. We propose DERN (Dropping Experts,
Recombining Neurons), a task-agnostic and retraining-free framework for expert
pruning and reconstruction. We observe that experts are often misaligned and
contain semantic conflicts at the neuron level, which poses challenges for
direct merging. To solve this, DERN works in three steps: it first prunes
redundant experts using router statistics; then it decomposes them into
neuron-level expert segments, assigning each segment to its most compatible
retained expert; and finally, it merges segments within each retained expert to
build a compact representation. Experiments on Mixtral, Qwen, and DeepSeek SMoE
models show that DERN improves performance by more than 5% on commonsense
reasoning and MMLU benchmarks under 50% expert sparsity, without extra
training. It also greatly reduces the number of experts and memory usage,
making SMoE LLMs easier to deploy in practice.

</details>


### [48] [Is In-Context Learning Learning?](https://arxiv.org/abs/2509.10414)
*Adrian de Wynter*

Main category: cs.CL

TL;DR: 论文通过数学论证和大规模实证分析，指出上下文学习（ICL）虽具备学习特性，但其泛化能力受限，尤其对未见任务敏感于分布偏移与提示风格。


<details>
  <summary>Details</summary>
Motivation: 探究上下文学习（ICL）是否属于真正的学习机制，并验证其在分布变化、提示方式等现实场景下的鲁棒性与泛化能力边界。

Method: 采用大规模实证分析，通过控制记忆效应、预训练数据、分布偏移及提示风格等变量，系统性评估ICL的学习效果。

Result: ICL在样本充足时表现稳定，但对未见任务泛化有限；链式思维提示加剧分布敏感性，模型依赖提示规律而非普适推理。

Conclusion: 自回归模型的临时编码机制不够稳健，ICL的泛化能力受限于任务形式相似性，暗示通用泛化潜力不足。

Abstract: In-context learning (ICL) allows some autoregressive models to solve tasks
via next-token prediction and without needing further training. This has led to
claims about these model's ability to solve (learn) unseen tasks with only a
few shots (exemplars) in the prompt. However, deduction does not always imply
learning, as ICL does not explicitly encode a given observation. Instead, the
models rely on their prior knowledge and the exemplars given, if any. We argue
that, mathematically, ICL does constitute learning, but its full
characterisation requires empirical work. We then carry out a large-scale
analysis of ICL ablating out or accounting for memorisation, pretraining,
distributional shifts, and prompting style and phrasing. We find that ICL is an
effective learning paradigm, but limited in its ability to learn and generalise
to unseen tasks. We note that, in the limit where exemplars become more
numerous, accuracy is insensitive to exemplar distribution, model, prompt
style, and the input's linguistic features. Instead, it deduces patterns from
regularities in the prompt, which leads to distributional sensitivity,
especially in prompting styles such as chain-of-thought. Given the varied
accuracies on formally similar tasks, we conclude that autoregression's ad-hoc
encoding is not a robust mechanism, and suggests limited all-purpose
generalisability.

</details>


### [49] [Long Context Automated Essay Scoring with Language Models](https://arxiv.org/abs/2509.10417)
*Christopher Ormerod,Gitit Kehat*

Main category: cs.CL

TL;DR: 评估改进Transformer架构的模型（XLNet/Longformer等）在长文本作文自动评分中的有效性，解决传统模型截断输入导致的评估缺陷问题


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型受限于最大文本长度，导致自动作文评分时需截断长文本，无法有效评估需要长上下文的文章组织结构要素

Method: 使用Kaggle ASAP 2.0数据集，对XLNet、Longformer、ModernBERT、Mamba、Llama等改进架构模型进行微调和对比测试

Result: 长上下文模型（如Longformer）相较于传统截断方法，在保持评分标准完整性的前提下展现出更好的评估性能

Conclusion: 采用支持长上下文的改进架构模型能有效提升自动作文评分系统的效度，特别是对组织结构等需要全局理解的评分维度

Abstract: Transformer-based language models are architecturally constrained to process
text of a fixed maximum length. Essays written by higher-grade students
frequently exceed the maximum allowed length for many popular open-source
models. A common approach to addressing this issue when using these models for
Automated Essay Scoring is to truncate the input text. This raises serious
validity concerns as it undermines the model's ability to fully capture and
evaluate organizational elements of the scoring rubric, which requires long
contexts to assess. In this study, we evaluate several models that incorporate
architectural modifications of the standard transformer architecture to
overcome these length limitations using the Kaggle ASAP 2.0 dataset. The models
considered in this study include fine-tuned versions of XLNet, Longformer,
ModernBERT, Mamba, and Llama models.

</details>


### [50] [RefactorCoderQA: Benchmarking LLMs for Multi-Domain Coding Question Solutions in Cloud and Edge Deployment](https://arxiv.org/abs/2509.10436)
*Shadikur Rahman,Aroosa Hameed,Gautam Srivastava,Syed Muhammad Danish*

Main category: cs.CL

TL;DR: 提出云边协作架构及多领域代码基准RefactorCoderQA，RefactorCoder-MoE模型以76.84%准确率刷新SOTA


<details>
  <summary>Details</summary>
Motivation: 针对现有基准测试覆盖领域单一、缺乏真实场景的问题，需建立多领域代码任务评估体系并验证云边协作架构的有效性

Method: 由边缘侧轻量GuideLLM提供方法指导、云端SolverLLM生成代码方案、自动化JudgeLLM评估质量的三层架构

Result: RefactorCoder-MoE在综合基准上达到76.84%准确率，显著超越商业模型；人类评估确认方案实用性，系统吞吐量提升35%

Conclusion: 该架构有效平衡模型能力与部署成本，基准测试推动代码生成研究，系统级指标分析为实际部署提供决策依据

Abstract: To optimize the reasoning and problem-solving capabilities of Large Language
Models (LLMs), we propose a novel cloud-edge collaborative architecture that
enables a structured, multi-agent prompting framework. This framework comprises
three specialized components: GuideLLM, a lightweight model deployed at the
edge to provide methodological guidance; SolverLLM, a more powerful model
hosted in the cloud responsible for generating code solutions; and JudgeLLM, an
automated evaluator for assessing solution correctness and quality. To evaluate
and demonstrate the effectiveness of this architecture in realistic settings,
we introduce RefactorCoderQA, a comprehensive benchmark designed to evaluate
and enhance the performance of Large Language Models (LLMs) across multi-domain
coding tasks. Motivated by the limitations of existing benchmarks,
RefactorCoderQA systematically covers various technical domains, including
Software Engineering, Data Science, Machine Learning, and Natural Language
Processing, using authentic coding challenges from Stack Overflow. Extensive
experiments reveal that our fine-tuned model, RefactorCoder-MoE, achieves
state-of-the-art performance, significantly outperforming leading open-source
and commercial baselines with an overall accuracy of 76.84%. Human evaluations
further validate the interpretability, accuracy, and practical relevance of the
generated solutions. In addition, we evaluate system-level metrics, such as
throughput and latency, to gain deeper insights into the performance
characteristics and trade-offs of the proposed architecture.

</details>


### [51] [DeepDive: Advancing Deep Search Agents with Knowledge Graphs and Multi-Turn RL](https://arxiv.org/abs/2509.10446)
*Rui Lu,Zhenyu Hou,Zihan Wang,Hanchen Zhang,Xiao Liu,Yujiang Li,Shi Feng,Jie Tang,Yuxiao Dong*

Main category: cs.CL

TL;DR: 通过自动生成复杂问题和多轮强化学习训练，DeepDive显著提升了LLM的深度搜索能力并在多个基准测试中取得最佳表现


<details>
  <summary>Details</summary>
Motivation: 解决现有开源LLM在深度搜索任务中长时程推理能力不足和缺乏高质量监督数据的问题

Method: 1. 从开放知识图谱自动合成复杂/困难问题；2. 端到端多轮强化学习增强长时程推理能力

Result: DeepDive-32B在BrowseComp上取得开源模型最佳成绩，多轮RL训练显著提升搜索能力并支持测试时的工具扩展和并行采样

Conclusion: DeepDive验证了多轮强化学习对搜索性能的提升价值，其开源实现为后续研究提供了重要基础

Abstract: Augmenting large language models (LLMs) with browsing tools substantially
improves their potential as deep search agents to solve complex, real-world
tasks. Yet, open LLMs still perform poorly in such settings due to limited
long-horizon reasoning capacity with browsing tools and the lack of
sufficiently difficult supervised data. To address these challenges, we present
DeepDive to advance deep search agents. First, we propose a strategy to
automatically synthesize complex, difficult, and hard-to-find questions from
open knowledge graphs. Second, we apply end-to-end multi-turn reinforcement
learning (RL) to enhance LLMs' long-horizon reasoning with deep search.
Experiments show that DeepDive-32B achieves a new open-source competitive
result on BrowseComp, outperforming WebSailor, DeepSeek-R1-Browse, and
Search-o1. We demonstrate that multi-turn RL training improves deep search
ability and significantly contributes to the performance improvements across
multiple benchmarks. We observe that DeepDive enables test-time scaling of tool
calls and parallel sampling. All datasets, models, and code are publicly
available at https://github.com/THUDM/DeepDive.

</details>


### [52] [WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers](https://arxiv.org/abs/2509.10452)
*Akshat Pandey,Karun Kumar,Raphael Tang*

Main category: cs.CL

TL;DR: 提出WhisTLE方法，通过文本数据+潜在嵌入空间+TTS结合，在无需语音数据的情况下显著提升预训练ASR模型的跨领域识别能力


<details>
  <summary>Details</summary>
Motivation: 预训练ASR模型对未见过的领域词汇/表达方式适应不足，而实际场景中收集语音数据困难，需要纯文本适应方案

Method: 1. 训练VAE建模文本到编码器输出的潜在空间 2. 用文本到潜在空间的编码器微调解码器 3. 可选结合TTS合成语音辅助适应 4. 推理时恢复原编码器（零额外计算成本）

Result: 在4个跨领域数据集和4种ASR模型上，结合TTS的WhisTLE比纯TTS适应降低12.3%相对WER，32个场景中27次超越所有非WhisTLE基线

Conclusion: 首次实现纯文本适应的深度监督框架，在保持推理效率的同时显著提升ASR模型跨领域性能，为实际部署提供有效解决方案

Abstract: Pretrained automatic speech recognition (ASR) models such as Whisper perform
well but still need domain adaptation to handle unseen vocabulary and parlance.
In many real-world settings, collecting speech data is impractical,
necessitating text-only adaptation. We propose WhisTLE, a deeply supervised,
text-only adaptation method for pretrained encoder-decoder ASR models. WhisTLE
trains a variational autoencoder (VAE) to model encoder outputs from text and
fine-tunes the decoder using the learned text-to-latent encoder, optionally
combined with text-to-speech (TTS) adaptation. At inference, the original
encoder is restored, incurring no extra runtime cost. Across four out-of-domain
datasets and four ASR models, WhisTLE with TTS reduces word error rate (WER) by
12.3% relative to TTS-only adaptation and outperforms all non-WhisTLE baselines
in 27 of 32 scenarios.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [53] [Chord: Chain of Rendering Decomposition for PBR Material Estimation from Generated Texture Images](https://arxiv.org/abs/2509.09952)
*Zhi Ying,Boxiang Rong,Jingyu Wang,Maoyuan Xu*

Main category: cs.GR

TL;DR: 提出两阶段生成-估计框架用于PBR材质生成，首阶段生成对齐用户的阴影可平铺纹理，次阶段通过链式分解方案预测材质参数，实现高效高质量且灵活可控的材质合成。


<details>
  <summary>Details</summary>
Motivation: 传统材质创建依赖艺术家经验且耗时，现有AI方法在质量/灵活性/用户控制方面存在不足。需要更高效可控的材质生成方案。

Method: 1. 生成阶段：微调扩散模型合成阴影可平铺纹理
2. 估计阶段：链式分解方案用单步扩散模型逐步预测SVBRDF材质参数

Result: 在生成质量和材质估计精度上优于现有方法，对生成纹理和真实照片均表现鲁棒，支持文本/图像引导生成、材质编辑等多场景应用。

Conclusion: 该框架通过分离生成与估计过程，结合扩散模型与链式分解，实现了质量、效率与控制力的平衡，拓展了材质合成技术的应用边界。

Abstract: Material creation and reconstruction are crucial for appearance modeling but
traditionally require significant time and expertise from artists. While recent
methods leverage visual foundation models to synthesize PBR materials from
user-provided inputs, they often fall short in quality, flexibility, and user
control. We propose a novel two-stage generate-and-estimate framework for PBR
material generation. In the generation stage, a fine-tuned diffusion model
synthesizes shaded, tileable texture images aligned with user input. In the
estimation stage, we introduce a chained decomposition scheme that sequentially
predicts SVBRDF channels by passing previously extracted representation as
input into a single-step image-conditional diffusion model. Our method is
efficient, high quality, and enables flexible user control. We evaluate our
approach against existing material generation and estimation methods,
demonstrating superior performance. Our material estimation method shows strong
robustness on both generated textures and in-the-wild photographs. Furthermore,
we highlight the flexibility of our framework across diverse applications,
including text-to-material, image-to-material, structure-guided generation, and
material editing.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [54] [VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions](https://arxiv.org/abs/2509.09716)
*Jun Zhan,Mingyang Han,Yuxuan Xie,Chen Wang,Dong Zhang,Kexin Huang,Haoxiang Shi,DongXiao Wang,Tengtao Song,Qinyuan Cheng,Shimin Li,Jun Song,Xipeng Qiu,Bo Zheng*

Main category: cs.SD

TL;DR: 论文提出语音风格适配任务VSA，构建双语评测集VStyle并开发LALM评估框架，揭示当前语音模型在可控风格适配上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有语音语言模型(SLMs)过度关注语义准确性，缺乏根据语音指令动态调整音色、韵律等风格特征的能力，制约人机语音交互的自然性。

Method: 1. 定义VSA新任务
2. 构建中英双语评测集VStyle（覆盖声学属性、自然指令、角色扮演、隐性共情四类）
3. 提出LALM渐进式评估框架（文本忠实度、风格遵循度、自然度三维度）

Result: 商业系统与开源模型在风格适配任务中均表现受限（如音色调整准确率仅65%，韵律适配成功率不足50%），验证任务新颖性与挑战性

Conclusion: VStyle基准及其评估工具为社区推进人本语音交互研究提供基础设施，代码与数据已开源

Abstract: Spoken language models (SLMs) have emerged as a unified paradigm for speech
understanding and generation, enabling natural human machine interaction.
However, while most progress has focused on semantic accuracy and instruction
following, the ability of SLMs to adapt their speaking style based on spoken
instructions has received limited attention. We introduce Voice Style
Adaptation (VSA), a new task that examines whether SLMs can modify their
speaking style, such as timbre, prosody, or persona following natural language
spoken commands. To study this task, we present VStyle, a bilingual (Chinese &
English) benchmark covering four categories of speech generation: acoustic
attributes, natural language instruction, role play, and implicit empathy. We
also introduce the Large Audio Language Model as a Judge (LALM as a Judge)
framework, which progressively evaluates outputs along textual faithfulness,
style adherence, and naturalness, ensuring reproducible and objective
assessment. Experiments on commercial systems and open source SLMs demonstrate
that current models face clear limitations in controllable style adaptation,
highlighting both the novelty and challenge of this task. By releasing VStyle
and its evaluation toolkit, we aim to provide the community with a foundation
for advancing human centered spoken interaction. The dataset and code are
publicly available at
\href{https://junzhan2000.github.io/VStyle.github.io/}{project's homepage}.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [55] [Vibe Check: Understanding the Effects of LLM-Based Conversational Agents' Personality and Alignment on User Perceptions in Goal-Oriented Tasks](https://arxiv.org/abs/2509.09870)
*Hasibur Rahman,Smit Desai*

Main category: cs.HC

TL;DR: 研究发现中等程度的人格表达水平使对话代理在任务中获得最佳用户评价，人格匹配进一步强化效果。


<details>
  <summary>Details</summary>
Motivation: 探究LLM对话代理人格表达强度及用户-代理人格匹配对用户感知的影响，为CA人格设计提供理论依据。

Method: 采用3（低/中/高人格表达）×5（大五人格）组间实验设计（N=150），通过Trait Modulation Keys框架控制人格特征表达。

Result: 倒U型关系显示中等表达评价最优（智力、愉悦感等6维度）；宜人性与情绪稳定性影响最大；聚类分析发现三类用户兼容性特征。

Conclusion: 中等人格表达强度结合战略性的用户-代理人格匹配构成CA人格设计的最佳实践路径，为LLM对话代理设计提供实证支持。

Abstract: Large language models (LLMs) enable conversational agents (CAs) to express
distinctive personalities, raising new questions about how such designs shape
user perceptions. This study investigates how personality expression levels and
user-agent personality alignment influence perceptions in goal-oriented tasks.
In a between-subjects experiment (N=150), participants completed travel
planning with CAs exhibiting low, medium, or high expression across the Big
Five traits, controlled via our novel Trait Modulation Keys framework. Results
revealed an inverted-U relationship: medium expression produced the most
positive evaluations across Intelligence, Enjoyment, Anthropomorphism,
Intention to Adopt, Trust, and Likeability, significantly outperforming both
extremes. Personality alignment further enhanced outcomes, with Extraversion
and Emotional Stability emerging as the most influential traits. Cluster
analysis identified three distinct compatibility profiles, with "Well-Aligned"
users reporting substantially positive perceptions. These findings demonstrate
that personality expression and strategic trait alignment constitute optimal
design targets for CA personality, offering design implications as LLM-based
CAs become increasingly prevalent.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [56] [Whisper Has an Internal Word Aligner](https://arxiv.org/abs/2509.09987)
*Sung-Lin Yeh,Yen Meng,Hao Tang*

Main category: eess.AS

TL;DR: 提出通过过滤注意力头+字符级教师强化的无监督方法，在20-100ms严格标准下实现比现有方法更精确的词对齐


<details>
  <summary>Details</summary>
Motivation: 现有词级时间戳方法存在三个问题：需要额外训练、精度不足、评估标准宽松（200ms容忍度）

Method: 1. 识别有效注意力头 2. 采用字符级处理替代词片段 3. 注意力头过滤机制 4. 结合字符级教师强制解码

Result: 在20-100ms严格容忍度范围内，词对齐准确率显著超越现有方法

Conclusion: 通过挖掘模型固有注意力机制特性，无需训练即可实现更精确的语音文本对齐

Abstract: There is an increasing interest in obtaining accurate word-level timestamps
from strong automatic speech recognizers, in particular Whisper. Existing
approaches either require additional training or are simply not competitive.
The evaluation in prior work is also relatively loose, typically using a
tolerance of more than 200 ms. In this work, we discover attention heads in
Whisper that capture accurate word alignments and are distinctively different
from those that do not. Moreover, we find that using characters produces finer
and more accurate alignments than using wordpieces. Based on these findings, we
propose an unsupervised approach to extracting word alignments by filtering
attention heads while teacher forcing Whisper with characters. Our approach not
only does not require training but also produces word alignments that are more
accurate than prior work under a stricter tolerance between 20 ms and 100 ms.

</details>


### [57] [Unified Learnable 2D Convolutional Feature Extraction for ASR](https://arxiv.org/abs/2509.10031)
*Peter Vieting,Benedikt Hilmes,Ralf Schlüter,Hermann Ney*

Main category: eess.AS

TL;DR: 提出参数高效的2D卷积前端架构，在有限计算资源下实现与传统可学习特征提取器相当的性能


<details>
  <summary>Details</summary>
Motivation: 现有神经前端设计过度依赖传统方法，限制了特征提取的通用性。希望开发统一架构替代现有混合拓扑结构，同时适应资源受限场景

Method: 采用纯2D卷积架构，通过系统性实验减少传统技术影响，实现参数效率高的统一前端设计

Result: 在监督学习场景下，该通用前端性能与现有可学习特征提取器相当，参数量显著减少

Conclusion: 验证了通用统一前端架构的可行性，为资源受限环境提供了有效解决方案，打破需要大规模预训练模型或复杂结构组合的传统思路

Abstract: Neural front-ends represent a promising approach to feature extraction for
automatic speech recognition (ASR) systems as they enable to learn specifically
tailored features for different tasks. Yet, many of the existing techniques
remain heavily influenced by classical methods. While this inductive bias may
ease the system design, our work aims to develop a more generic front-end for
feature extraction. Furthermore, we seek to unify the front-end architecture
contrasting with existing approaches that apply a composition of several layer
topologies originating from different sources. The experiments systematically
show how to reduce the influence of existing techniques to achieve a generic
front-end. The resulting 2D convolutional front-end is parameter-efficient and
suitable for a scenario with limited computational resources unlike large
models pre-trained on unlabeled audio. The results demonstrate that this
generic unified approach is not only feasible but also matches the performance
of existing supervised learnable feature extractors.

</details>


### [58] [Error Analysis in a Modular Meeting Transcription System](https://arxiv.org/abs/2509.10143)
*Peter Vieting,Simon Berger,Thilo von Neumann,Christoph Boeddeker,Ralf Schlüter,Reinhold Haeb-Umbach*

Main category: eess.AS

TL;DR: 提出一种改进的语音泄漏分析方法，发现交叉通道泄漏被语音活动检测有效过滤，先进的二值化分割可将性能差距缩小三分之一，系统在LibriCSS上达到仅使用LibriSpeech训练数据的最优效果。


<details>
  <summary>Details</summary>
Motivation: 尽管会议转录技术近年发展迅速，但语音分离中的泄漏问题仍制约性能。研究旨在通过时域敏感的泄漏分析框架揭示系统瓶颈。

Method: 扩展语音泄漏分析框架，引入时域敏感性评估；对比能量基VAD与先进二值化分割方法；通过LibriCSS数据集验证，仅使用LibriSpeech数据训练识别模块。

Result: 发现主说话人活跃区存在显著交叉泄漏但被VAD有效过滤；先进二值化较能量VAD将oracle分割差距缩小33%；系统在LibriCSS上达到当前最优水平。

Conclusion: 时域敏感的泄漏分析揭示系统鲁棒性机制，二值化技术显著提升分割质量，残留差距源于上下文建模不足，为未来优化指明方向。

Abstract: Meeting transcription is a field of high relevance and remarkable progress in
recent years. Still, challenges remain that limit its performance. In this
work, we extend a previously proposed framework for analyzing leakage in speech
separation with proper sensitivity to temporal locality. We show that there is
significant leakage to the cross channel in areas where only the primary
speaker is active. At the same time, the results demonstrate that this does not
affect the final performance much as these leaked parts are largely ignored by
the voice activity detection (VAD). Furthermore, different segmentations are
compared showing that advanced diarization approaches are able to reduce the
gap to oracle segmentation by a third compared to a simple energy-based VAD. We
additionally reveal what factors contribute to the remaining difference. The
results represent state-of-the-art performance on LibriCSS among systems that
train the recognition module on LibriSpeech data only.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [59] [DB3 Team's Solution For Meta KDD Cup' 25](https://arxiv.org/abs/2509.09681)
*Yikuan Xia,Jiazun Chen,Yirui Zhan,Suifeng Zhao,Weipeng Jiang,Chaorui Zhang,Wei Han,Bo Bai,Jun Gao*

Main category: cs.IR

TL;DR: db3团队通过定制化检索流程与拒绝训练技术，在Meta CRAG-MM多模态问答挑战赛中斩获总冠军


<details>
  <summary>Details</summary>
Motivation: 针对CRAG-MM基准中多模态数据、多轮对话和第一人称视角查询的特殊挑战，开发能够有效控制大模型幻觉的解决方案

Method: 1. 领域专用检索流程处理图像知识图谱/网页数据/多轮对话 2. 采用SFT+DPO+RL混合策略进行拒绝训练 3. 构建统一LLM调优框架控制幻觉

Result: 任务1第二/任务2第二/任务3第一，凭借第一人称视角处理的优势获得总冠军

Conclusion: 该框架通过模块化检索系统与强化学习调优的协同，为复杂多模态问答场景提供了有效的工程范式

Abstract: This paper presents the db3 team's winning solution for the Meta CRAG-MM
Challenge 2025 at KDD Cup'25. Addressing the challenge's unique multi-modal,
multi-turn question answering benchmark (CRAG-MM), we developed a comprehensive
framework that integrates tailored retrieval pipelines for different tasks with
a unified LLM-tuning approach for hallucination control. Our solution features
(1) domain-specific retrieval pipelines handling image-indexed knowledge
graphs, web sources, and multi-turn conversations; and (2) advanced refusal
training using SFT, DPO, and RL. The system achieved 2nd place in Task 1, 2nd
place in Task 2, and 1st place in Task 3, securing the grand prize for
excellence in ego-centric queries through superior handling of first-person
perspective challenges.

</details>


### [60] [Text-to-SQL Oriented to the Process Mining Domain: A PT-EN Dataset for Query Translation](https://arxiv.org/abs/2509.09684)
*Bruno Yui Yamate,Thais Rodrigues Neubauer,Marcelo Fantinato,Sarajane Marques Peres*

Main category: cs.IR

TL;DR: 构建了流程挖掘领域首个双语文本转SQL基准数据集text-2-SQL-4-PM，包含1655自然语言查询与205条SQL语句，并通过GPT-3.5 Turbo验证其有效性


<details>
  <summary>Details</summary>
Motivation: 解决流程挖掘领域因专业术语和单表事件日志结构导致的自然语言转SQL准确率低的问题，降低非专业用户查询门槛

Method: 采用专家人工标注+专业翻译构建双语语料库，结合细粒度注释体系分析任务复杂度，并通过大模型建立基线测试

Result: 基线实验验证了数据集在文本转SQL任务中的实用性，准确率达到行业基准水平，支持语义解析等下游任务扩展

Conclusion: 该数据集填补了流程挖掘领域专用NLP工具的空白，为跨语言数据库自然语言接口开发提供了标准化评估框架

Abstract: This paper introduces text-2-SQL-4-PM, a bilingual (Portuguese-English)
benchmark dataset designed for the text-to-SQL task in the process mining
domain. Text-to-SQL conversion facilitates natural language querying of
databases, increasing accessibility for users without SQL expertise and
productivity for those that are experts. The text-2-SQL-4-PM dataset is
customized to address the unique challenges of process mining, including
specialized vocabularies and single-table relational structures derived from
event logs. The dataset comprises 1,655 natural language utterances, including
human-generated paraphrases, 205 SQL statements, and ten qualifiers. Methods
include manual curation by experts, professional translations, and a detailed
annotation process to enable nuanced analyses of task complexity. Additionally,
a baseline study using GPT-3.5 Turbo demonstrates the feasibility and utility
of the dataset for text-to-SQL applications. The results show that
text-2-SQL-4-PM supports evaluation of text-to-SQL implementations, offering
broader applicability for semantic parsing and other natural language
processing tasks.

</details>


### [61] [AI-Powered Assistant for Long-Term Access to RHIC Knowledge](https://arxiv.org/abs/2509.09688)
*Mohammad Atif,Vincent Garonne,Eric Lancon,Jerome Lauret,Alexandr Prozorov,Michal Vranovsky*

Main category: cs.IR

TL;DR: RHIC推出AI驱动的数据保存系统，通过大语言模型提升科学数据的可用性与可发现性


<details>
  <summary>Details</summary>
Motivation: RHIC运行25年积累约1EB数据，需保存数据资产与科学知识以支持未来研究。现有文档、工作流和软件的分散性威胁科学遗产的长期可用性。

Method: 基于检索增强生成（RAG）和模型上下文协议（MCP）的大语言模型系统，索引RHIC实验的结构化和非结构化内容，实现领域自适应的交互界面。

Result: 系统已部署并展示良好计算性能，正进行多实验数据整合。架构设计强调可持续性和可解释性，支持长期AI访问。

Conclusion: AI/ML工具可有效转换科学遗产数据的可用性，为科学数据保存提供新范式，支持教育、再现性研究和未来科学发现。

Abstract: As the Relativistic Heavy Ion Collider (RHIC) at Brookhaven National
Laboratory concludes 25 years of operation, preserving not only its vast data
holdings ($\sim$1 ExaByte) but also the embedded scientific knowledge becomes a
critical priority. The RHIC Data and Analysis Preservation Plan (DAPP)
introduces an AI-powered assistant system that provides natural language access
to documentation, workflows, and software, with the aim of supporting
reproducibility, education, and future discovery. Built upon Large Language
Models using Retrieval-Augmented Generation and the Model Context Protocol,
this assistant indexes structured and unstructured content from RHIC
experiments and enables domain-adapted interaction. We report on the
deployment, computational performance, ongoing multi-experiment integration,
and architectural features designed for a sustainable and explainable long-term
AI access. Our experience illustrates how modern AI/ML tools can transform the
usability and discoverability of scientific legacy data.

</details>


### [62] [Personas within Parameters: Fine-Tuning Small Language Models with Low-Rank Adapters to Mimic User Behaviors](https://arxiv.org/abs/2509.09689)
*Himanshu Thakur,Eshani Agrawal,Smruthi Mukund*

Main category: cs.IR

TL;DR: 提出通过冻结的大语言模型提取用户表征，结合微调的小语言模型实现高效用户行为模拟，解决推荐系统用户行为仿真的复杂性与扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于LLM的用户行为仿真面临三大挑战：(1) 需持续解析大规模用户-物品交互数据；(2) 需克服预训练模型的固有认知偏差；(3) 需实现百万级用户的规模化应用。现有方法主要依赖复杂提示工程或全参数微调，效率和扩展性受限。

Method: 1. 使用冻结LLM提取鲁棒文本用户表征
2. 基于微调的小型语言模型(SLM)构建高性价比的用户代理
3. 开发面向用户群体的低秩适配器(LoRA)，实现扩展性与性能的优化平衡

Result: 实验证明该方法能有效弥合推荐系统离线指标与真实场景表现的差距，用户代理在保持资源效率的同时显著提升行为模拟准确性。

Conclusion: 通过文本表征提取+轻量化SLM微调的协同框架，为推荐系统的用户行为仿真提供了可扩展、高效率的解决方案，突破了传统LLM方法的计算瓶颈与扩展限制。

Abstract: A long-standing challenge in developing accurate recommendation models is
simulating user behavior, mainly due to the complex and stochastic nature of
user interactions. Towards this, one promising line of work has been the use of
Large Language Models (LLMs) for simulating user behavior. However, aligning
these general-purpose large pre-trained models with user preferences
necessitates: (i) effectively and continously parsing large-scale tabular
user-item interaction data, (ii) overcoming pre-training-induced inductive
biases to accurately learn user specific knowledge, and (iii) achieving the
former two at scale for millions of users. While most previous works have
focused on complex methods to prompt an LLM or fine-tune it on tabular
interaction datasets, our approach shifts the focus to extracting robust
textual user representations using a frozen LLM and simulating cost-effective,
resource-efficient user agents powered by fine-tuned Small Language Models
(SLMs). Further, we showcase a method for training multiple low-rank adapters
for groups of users or \textit{persona}, striking an optimal balance between
scalability and performance of user behavior agents. Our experiments provide
compelling empirical evidence of the efficacy of our methods, demonstrating
that user agents developed using our approach have the potential to bridge the
gap between offline metrics and real-world performance of recommender systems.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [63] [Differential Robustness in Transformer Language Models: Empirical Evaluation Under Adversarial Text Attacks](https://arxiv.org/abs/2509.09706)
*Taniya Gidatkar,Oluwaseun Ajao,Matthew Shardlow*

Main category: cs.CR

TL;DR: 评估三大语言模型对抗攻击的鲁棒性：RoBERTa/FlanT5展现绝对防御力（攻击成功率0%），BERT存在严重漏洞（攻击成功率93.75%），揭示防御机制需权衡计算效率与安全性


<details>
  <summary>Details</summary>
Motivation: 针对当前LLM安全防护机制的有效性存疑，通过系统性对抗测试揭示不同模型的防御能力差异，为优化防御策略提供实证依据

Method: 采用TextFooler和BERTAttack工具，对Flan-T5、BERT-Base、RoBERTa-Base进行对抗测试，量化攻击成功率及准确率变化

Result: RoBERTa/FlanT5保持攻击后准确率不变，BERT准确率从48%暴跌至3%。防御机制有效性差异显著但伴随高计算成本

Conclusion: LLM安全防护存在明显模型差异性，需开发兼顾防御效能与资源效率的新型防护体系，建议采用模型结构优化与轻量化防御策略结合方案

Abstract: This study evaluates the resilience of large language models (LLMs) against
adversarial attacks, specifically focusing on Flan-T5, BERT, and RoBERTa-Base.
Using systematically designed adversarial tests through TextFooler and
BERTAttack, we found significant variations in model robustness. RoBERTa-Base
and FlanT5 demonstrated remarkable resilience, maintaining accuracy even when
subjected to sophisticated attacks, with attack success rates of 0%. In
contrast. BERT-Base showed considerable vulnerability, with TextFooler
achieving a 93.75% success rate in reducing model accuracy from 48% to just 3%.
Our research reveals that while certain LLMs have developed effective defensive
mechanisms, these safeguards often require substantial computational resources.
This study contributes to the understanding of LLM security by identifying
existing strengths and weaknesses in current safeguarding approaches and
proposes practical recommendations for developing more efficient and effective
defensive strategies.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [64] [HypoGeneAgent: A Hypothesis Language Agent for Gene-Set Cluster Resolution Selection Using Perturb-seq Datasets](https://arxiv.org/abs/2509.09740)
*Ying Yuan,Xing-Yue Monica Ge,Aaron Archer Waterman,Tommaso Biancalani,David Richmond,Yogesh Pandit,Avtar Singh,Russell Littman,Jin Liu,Jan-Christian Huetter,Vladimir Ermakov*

Main category: q-bio.QM

TL;DR: 提出HYPOGENEAGENT框架，利用LLM实现单细胞研究聚类注释的自动化与客观化，通过分辨率评分优化聚类粒度选择。


<details>
  <summary>Details</summary>
Motivation: 传统单细胞聚类解析和GO注释依赖主观人工判断，需开发自动化框架提升分析客观性与可重复性。

Method: 分两阶段：LLM生成GO假设及置信度→嵌入模型评估聚类内一致性(cosine相似性)和跨簇区分度，构建分辨率评分指标。

Result: 在K562 CRISPRi数据集验证中，分辨率评分较轮廓系数等传统指标更有效识别与已知通路一致的聚类结构。

Conclusion: LLM可作为生物医学数据解析的客观裁决者，推动单细胞多组学研究向全自动化、情境感知的流程演进。

Abstract: Large-scale single-cell and Perturb-seq investigations routinely involve
clustering cells and subsequently annotating each cluster with Gene-Ontology
(GO) terms to elucidate the underlying biological programs. However, both
stages, resolution selection and functional annotation, are inherently
subjective, relying on heuristics and expert curation. We present
HYPOGENEAGENT, a large language model (LLM)-driven framework, transforming
cluster annotation into a quantitatively optimizable task. Initially, an LLM
functioning as a gene-set analyst analyzes the content of each gene program or
perturbation module and generates a ranked list of GO-based hypotheses,
accompanied by calibrated confidence scores. Subsequently, we embed every
predicted description with a sentence-embedding model, compute pair-wise cosine
similarities, and let the agent referee panel score (i) the internal
consistency of the predictions, high average similarity within the same
cluster, termed intra-cluster agreement (ii) their external distinctiveness,
low similarity between clusters, termed inter-cluster separation. These two
quantities are combined to produce an agent-derived resolution score, which is
maximized when clusters exhibit simultaneous coherence and mutual exclusivity.
When applied to a public K562 CRISPRi Perturb-seq dataset as a preliminary
test, our Resolution Score selects clustering granularities that exhibit
alignment with known pathway compared to classical metrics such silhouette
score, modularity score for gene functional enrichment summary. These findings
establish LLM agents as objective adjudicators of cluster resolution and
functional annotation, thereby paving the way for fully automated,
context-aware interpretation pipelines in single-cell multi-omics studies.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [65] [LLM-Based Instance-Driven Heuristic Bias In the Context of a Biased Random Key Genetic Algorithm](https://arxiv.org/abs/2509.09707)
*Camilo Chacón Sartori,Martín Isla Pino,Pedro Pinacho-Davidson,Christian Blum*

Main category: cs.NE

TL;DR: LLM与BRKGA协同框架通过实例驱动的启发式偏置，显著提升复杂组合优化问题求解效果


<details>
  <summary>Details</summary>
Motivation: 现有LLM应用多聚焦代码生成而忽视问题实例的结构特性，需探索人机协同的实例驱动优化范式

Method: 构建人-LLM协作指标设计流程，通过LLM分析实例特征生成定制启发式偏置，引导BRKGA搜索方向

Result: 在1,050个不同复杂度实例中，BRKGA+Llama-4-Maverick在复杂案例上实现统计显著提升（p<0.05）

Conclusion: LLM生成的先验实例驱动偏置机制为增强元启发式算法在NP-hard问题中的表现提供了有效路径

Abstract: Integrating Large Language Models (LLMs) within metaheuristics opens a novel
path for solving complex combinatorial optimization problems. While most
existing approaches leverage LLMs for code generation to create or refine
specific heuristics, they often overlook the structural properties of
individual problem instances. In this work, we introduce a novel framework that
integrates LLMs with a Biased Random-Key Genetic Algorithm (BRKGA) to solve the
NP-hard Longest Run Subsequence problem. Our approach extends the
instance-driven heuristic bias paradigm by introducing a human-LLM
collaborative process to co-design and implement a set of computationally
efficient metrics. The LLM analyzes these instance-specific metrics to generate
a tailored heuristic bias, which steers the BRKGA toward promising areas of the
search space. We conduct a comprehensive experimental evaluation, including
rigorous statistical tests, convergence and behavioral analyses, and targeted
ablation studies, comparing our method against a standard BRKGA baseline across
1,050 generated instances of varying complexity. Results show that our
top-performing hybrid, BRKGA+Llama-4-Maverick, achieves statistically
significant improvements over the baseline, particularly on the most complex
instances. Our findings confirm that leveraging an LLM to produce an a priori,
instance-driven heuristic bias is a valuable approach for enhancing
metaheuristics in complex optimization domains.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [66] [Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level RL](https://arxiv.org/abs/2509.09177)
*Hanyi Mao,Quanjia Xiao,Lei Pang,Haixiao Liu*

Main category: cs.LG

TL;DR: 提出FSPO方法，通过高斯启发的序列级KL修正剪裁策略解决RL方法中的长度偏差问题


<details>
  <summary>Details</summary>
Motivation: 现有序列级RL方法（如PPO/GRPO）在重要性采样权重空间应用固定剪裁范围时，会导致长短文本更新的系统性偏差，扭曲有效优化目标

Method: 1. 引入长度重加权误差(LRE)理论框架
2. 提出高斯分布的KL修正漂移项
3. 设计√L缩放的比例系数实现长度公平剪裁

Result: 在多个评估数据集上超越基线方法，实现跨长度区间的稳定剪裁率，训练过程更平稳

Conclusion: FSPO通过理论保证和实证验证，解决了序列级RL的长度公平性问题，为语言模型强化学习提供了更稳定的优化框架

Abstract: We propose FSPO (Fair Sequence Policy Optimization), a sequence-level
reinforcement learning method for LLMs that enforces length-fair clipping
directly in the importance-sampling (IS) weight space. We revisit
sequence-level RL methods and identify a mismatch when PPO/GRPO-style clipping
is transplanted to sequences: a fixed clip range systematically reweights short
vs. long responses, distorting the effective objective. Theoretically, we
formalize length fairness via a Length Reweighting Error (LRE) and prove that
small LRE yields a directional cosine guarantee between the clipped and true
updates. FSPO introduces a simple, Gaussian-motivated remedy: we clip the
sequence log-IS ratio with a band that applies a KL-corrected drift term and
scales as $\sqrt{L}$. Empirically, FSPO flattens clip rates across length bins,
stabilizes training, and outperforms all baselines across multiple evaluation
datasets.

</details>


### [67] [Latency and Token-Aware Test-Time Compute](https://arxiv.org/abs/2509.09864)
*Jenny Y. Huang,Mehul Damani,Yousef El-Kurdi,Ramon Astudillo,Wei Sun*

Main category: cs.LG

TL;DR: 提出动态计算分配框架，在LLM推理阶段同时优化token成本和延迟，实现更优的精度-成本平衡


<details>
  <summary>Details</summary>
Motivation: 现有推理阶段计算分配方法仅关注并行生成策略（如best-of-N），忽略增量解码方法（如beam search），且未考虑延迟对用户体验和智能体工作流的影响

Method: 建立动态计算分配框架，根据查询需求自适应选择策略和分配计算资源，明确纳入token成本和实时延迟双因素

Result: 在推理基准测试中持续优于静态策略，实现更优的精度-成本平衡，同时保持部署可行性

Conclusion: 动态计算分配框架有效协调计算资源与延迟约束，为实际部署提供高效推理解决方案

Abstract: Inference-time scaling has emerged as a powerful way to improve large
language model (LLM) performance by generating multiple candidate responses and
selecting among them. However, existing work on dynamic allocation for
test-time compute typically considers only parallel generation methods such as
best-of-N, overlooking incremental decoding methods like beam search, and has
largely ignored latency, focusing only on token usage. We formulate
inference-time scaling as a problem of dynamic compute allocation and method
selection, where the system must decide which strategy to apply and how much
compute to allocate on a per-query basis. Our framework explicitly incorporates
both token cost and wall-clock latency, the latter being critical for user
experience and particularly for agentic workflows where models must issue
multiple queries efficiently. Experiments on reasoning benchmarks show that our
approach consistently outperforms static strategies, achieving favorable
accuracy-cost trade-offs while remaining practical for deployment.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [68] [Executable Ontologies: Synthesizing Event Semantics with Dataflow Architecture](https://arxiv.org/abs/2509.09775)
*Aleksandr Boldachev*

Main category: cs.AI

TL;DR: 提出boldsea架构——通过可执行本体（语义模型）实现复杂动态系统建模，突破传统BPM系统局限，支持运行时动态修改事件模型。


<details>
  <summary>Details</summary>
Motivation: 解决传统业务流程管理系统（BPM）和面向对象语义技术在动态调整、时空透明性、数据与业务逻辑分离等方面的不足。

Method: 1. 开发BSL语义语言（含BNF语法）
2. 设计boldsea-engine架构直接解释语义模型
3. 事件语义与数据流架构整合技术

Result: 实现运行时动态修改事件模型、确保时序透明性、在统一语义框架中融合数据与业务逻辑

Conclusion: boldsea架构通过语义模型直接驱动流程执行，为复杂系统建模提供无需编译、高度动态化的新型解决方案。

Abstract: This paper presents boldsea, Boldachev's semantic-event approach -- an
architecture for modeling complex dynamic systems using executable ontologies
-- semantic models that act as dynamic structures, directly controlling process
execution. We demonstrate that integrating event semantics with a dataflow
architecture addresses the limitations of traditional Business Process
Management (BPM) systems and object-oriented semantic technologies. The paper
presents the formal BSL (boldsea Semantic Language), including its BNF grammar,
and outlines the boldsea-engine's architecture, which directly interprets
semantic models as executable algorithms without compilation. It enables the
modification of event models at runtime, ensures temporal transparency, and
seamlessly merges data and business logic within a unified semantic framework.

</details>


### [69] [LLMs as Agentic Cooperative Players in Multiplayer UNO](https://arxiv.org/abs/2509.09867)
*Yago Romano Matinez,Jesse Roberts*

Main category: cs.AI

TL;DR: 研究探讨LLM在UNO游戏中作为协助者而非竞争者的表现，发现大模型在单独游戏时表现优异，但协助他人效果有限。


<details>
  <summary>Details</summary>
Motivation: 探索LLM作为主动参与者协助人类完成目标的能力，尤其是在协作场景中的实际效果。

Method: 构建基于RLCard的测试环境，引入两种提示策略，评估不同规模（1B-70B）LLM在协助玩家和独立游戏中的表现。

Result: 所有模型在独立游戏中均超越随机基线，但仅少数大模型能有效协助其他玩家获胜。

Conclusion: 模型规模对独立任务有帮助，但协作任务需更复杂的策略设计，当前LLM在主动协助场景中仍有局限。

Abstract: LLMs promise to assist humans -- not just by answering questions, but by
offering useful guidance across a wide range of tasks. But how far does that
assistance go? Can a large language model based agent actually help someone
accomplish their goal as an active participant? We test this question by
engaging an LLM in UNO, a turn-based card game, asking it not to win but
instead help another player to do so. We built a tool that allows decoder-only
LLMs to participate as agents within the RLCard game environment. These models
receive full game-state information and respond using simple text prompts under
two distinct prompting strategies. We evaluate models ranging from small (1B
parameters) to large (70B parameters) and explore how model scale impacts
performance. We find that while all models were able to successfully outperform
a random baseline when playing UNO, few were able to significantly aid another
player.

</details>


### [70] [Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure Attribution in Multi-Agent Systems](https://arxiv.org/abs/2509.10401)
*Alva West,Yixuan Weng,Minjun Zhu,Zhen Lin,Yue Zhang*

Main category: cs.AI

TL;DR: 提出A2P Scaffolding框架，将多智能体系统故障归因从模式识别转化为结构化因果推理任务，实现步骤级准确率2.43-2.85倍提升


<details>
  <summary>Details</summary>
Motivation: 现有故障归因方法（准确率<17%）缺乏反事实推理能力，无法验证单步修正能否避免系统故障，需建立可验证的因果推理框架

Method: Abduct-Act-Predict三阶段推理：1) Abduction推断行为潜在根源；2) Action制定最小修正干预；3) Prediction模拟后续轨迹验证修正效果

Result: Algorithm-Generated数据集步骤级准确率47.46%（基线16.67%）；Hand-Crafted数据集29.31%（基线12.07%），分别提升2.85×和2.43×

Conclusion: 通过因果推理框架重构问题，A2P Scaffolding提供可验证、准确率显著提升（47.46% vs 16.67%）的自动化故障归因方案

Abstract: Failure attribution in multi-agent systems -- pinpointing the exact step
where a decisive error occurs -- is a critical yet unsolved challenge. Current
methods treat this as a pattern recognition task over long conversation logs,
leading to critically low step-level accuracy (below 17\%), which renders them
impractical for debugging complex systems. Their core weakness is a fundamental
inability to perform robust counterfactual reasoning: to determine if
correcting a single action would have actually averted the task failure. To
bridge this counterfactual inference gap, we introduce Abduct-Act-Predict (A2P)
Scaffolding, a novel agent framework that transforms failure attribution from
pattern recognition into a structured causal inference task. A2P explicitly
guides a large language model through a formal three-step reasoning process
within a single inference pass: (1) Abduction, to infer the hidden root causes
behind an agent's actions; (2) Action, to define a minimal corrective
intervention; and (3) Prediction, to simulate the subsequent trajectory and
verify if the intervention resolves the failure. This structured approach
leverages the holistic context of the entire conversation while imposing a
rigorous causal logic on the model's analysis. Our extensive experiments on the
Who\&When benchmark demonstrate its efficacy. On the Algorithm-Generated
dataset, A2P achieves 47.46\% step-level accuracy, a 2.85$\times$ improvement
over the 16.67\% of the baseline. On the more complex Hand-Crafted dataset, it
achieves 29.31\% step accuracy, a 2.43$\times$ improvement over the baseline's
12.07\%. By reframing the problem through a causal lens, A2P Scaffolding
provides a robust, verifiable, and significantly more accurate solution for
automated failure attribution.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [71] [Improving MLLM Historical Record Extraction with Test-Time Image](https://arxiv.org/abs/2509.09722)
*Taylor Archibald,Tony Martinez*

Main category: cs.CV

TL;DR: 提出集成框架提升噪声历史文档转录准确率4%，通过多增强变体生成和定制对齐器实现


<details>
  <summary>Details</summary>
Motivation: 解决历史文档转录中噪声干扰导致的LLM不稳定问题，传统单次转录方法效果有限

Method: 1. 使用Gemini 2.0 Flash生成图像增强变体 2. 定制Needleman Wunsch对齐器融合输出 3. 输出共识转录及置信度评分

Result: 在622份死亡记录数据上实现4%准确率提升，填充/模糊处理提升效果最佳，网格扭曲扰动有效区分置信度

Conclusion: 该方法简单可扩展，可快速部署至其他文档转录场景，为历史档案数字化提供实用解决方案

Abstract: We present a novel ensemble framework that stabilizes LLM based text
extraction from noisy historical documents. We transcribe multiple augmented
variants of each image with Gemini 2.0 Flash and fuse these outputs with a
custom Needleman Wunsch style aligner that yields both a consensus
transcription and a confidence score. We present a new dataset of 622
Pennsylvania death records, and demonstrate our method improves transcription
accuracy by 4 percentage points relative to a single shot baseline. We find
that padding and blurring are the most useful for improving accuracy, while
grid warp perturbations are best for separating high and low confidence cases.
The approach is simple, scalable, and immediately deployable to other document
collections and transcription models.

</details>


### [72] [VARCO-VISION-2.0 Technical Report](https://arxiv.org/abs/2509.10105)
*Young-rok Cha,Jeongho Ju,SunYoung Park,Jong-Hyeon Lee,Younghyun Yu,Youngjune Kim*

Main category: cs.CV

TL;DR: VARCO-VISION-2.0是升级版双语视觉语言模型，支持多图像理解与布局感知OCR，通过四阶段训练提升性能，发布14B和1.7B双版本。


<details>
  <summary>Details</summary>
Motivation: 开发更强大的双语视觉语言模型以处理复杂视觉输入（文档/图表），优化模型效率与安全性，推动实际场景应用。

Method: 四阶段课程训练框架结合内存优化技术，布局感知OCR同时预测文本与坐标，偏好对齐优化提升安全性。

Result: 14B模型在OpenCompass VLM榜单排名第八，双语空间定位能力优异，1.7B轻量版实现端侧部署。

Conclusion: 该模型系列显著提升多模态对齐能力，填补双语VLM空白，为工业应用提供高精度与高效率的双重选择。

Abstract: We introduce VARCO-VISION-2.0, an open-weight bilingual vision-language model
(VLM) for Korean and English with improved capabilities compared to the
previous model VARCO-VISION-14B. The model supports multi-image understanding
for complex inputs such as documents, charts, and tables, and delivers
layoutaware OCR by predicting both textual content and its spatial location.
Trained with a four-stage curriculum with memory-efficient techniques, the
model achieves enhanced multimodal alignment, while preserving core language
abilities and improving safety via preference optimization. Extensive benchmark
evaluations demonstrate strong spatial grounding and competitive results for
both languages, with the 14B model achieving 8th place on the OpenCompass VLM
leaderboard among models of comparable scale. Alongside the 14B-scale model, we
release a 1.7B version optimized for on-device deployment. We believe these
models advance the development of bilingual VLMs and their practical
applications. Two variants of VARCO-VISION-2.0 are available at Hugging Face: a
full-scale 14B model and a lightweight 1.7B model.

</details>
