<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 44]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 4]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Understanding and Enhancing Mamba-Transformer Hybrids for Memory Recall and Language Modeling](https://arxiv.org/abs/2510.26912)
*Hyunji Lee,Wenhao Yu,Hongming Zhang,Kaixin Ma,Jiyeon Kim,Dong Yu,Minjoon Seo*

Main category: cs.CL

TL;DR: 提出通过内存利用分析和持续训练数据增强方法优化SSM-注意力混合模型架构


<details>
  <summary>Details</summary>
Motivation: 现有混合状态空间模型与注意力机制的架构设计选择缺乏深入理解，需要探索其性能差异和优化方法

Method: 1. 分析顺序/并行架构的内存利用差异
2. 引入基于文本复述的数据增强持续训练方法

Result: 1. 顺序混合架构在短上下文表现更优，并行架构在长上下文更有效
2. 数据增强方法在多个基模型上提升召回能力5-8%

Conclusion: 研究为混合模型架构设计提供理论依据，数据增强方法比架构调整更能有效提升模型能力

Abstract: Hybrid models that combine state space models (SSMs) with attention
mechanisms have shown strong performance by leveraging the efficiency of SSMs
and the high recall ability of attention. However, the architectural design
choices behind these hybrid models remain insufficiently understood. In this
work, we analyze hybrid architectures through the lens of memory utilization
and overall performance, and propose a complementary method to further enhance
their effectiveness. We first examine the distinction between sequential and
parallel integration of SSM and attention layers. Our analysis reveals several
interesting findings, including that sequential hybrids perform better on
shorter contexts, whereas parallel hybrids are more effective for longer
contexts. We also introduce a data-centric approach of continually training on
datasets augmented with paraphrases, which further enhances recall while
preserving other capabilities. It generalizes well across different base models
and outperforms architectural modifications aimed at enhancing recall. Our
findings provide a deeper understanding of hybrid SSM-attention models and
offer practical guidance for designing architectures tailored to various use
cases. Our findings provide a deeper understanding of hybrid SSM-attention
models and offer practical guidance for designing architectures tailored to
various use cases.

</details>


### [2] [Frame Semantic Patterns for Identifying Underreporting of Notifiable Events in Healthcare: The Case of Gender-Based Violence](https://arxiv.org/abs/2510.26969)
*Lívia Dutra,Arthur Lorenzi,Laís Berno,Franciany Campos,Karoline Biscardi,Kenneth Brown,Marcelo Viridiano,Frederico Belcavello,Ely Matos,Olívia Guaranha,Erik Santos,Sofia Reinach,Tiago Timponi Torrent*

Main category: cs.CL

TL;DR: 提出一种基于语义框架的医疗事件检测方法，有效识别电子病历中的性别暴力事件，精确度达0.726。


<details>
  <summary>Details</summary>
Motivation: 解决初级保健电子病历中性别暴力事件漏报问题，提升公共卫生监测能力。

Method: 定义8个语义模式，在巴西e-SUS APS系统的2100万句葡萄牙语语料库中进行模式匹配，并通过语言学家手动评估结果。

Result: 该方法暴力识别精确度为0.726，证实其有效性。设计支持多语言且计算效率高。

Conclusion: 该透明、低碳的流程可扩展至其他健康监测场景，推动NLP在公共卫生系统中更伦理、可解释的应用。

Abstract: We introduce a methodology for the identification of notifiable events in the
domain of healthcare. The methodology harnesses semantic frames to define
fine-grained patterns and search them in unstructured data, namely, open-text
fields in e-medical records. We apply the methodology to the problem of
underreporting of gender-based violence (GBV) in e-medical records produced
during patients' visits to primary care units. A total of eight patterns are
defined and searched on a corpus of 21 million sentences in Brazilian
Portuguese extracted from e-SUS APS. The results are manually evaluated by
linguists and the precision of each pattern measured. Our findings reveal that
the methodology effectively identifies reports of violence with a precision of
0.726, confirming its robustness. Designed as a transparent, efficient,
low-carbon, and language-agnostic pipeline, the approach can be easily adapted
to other health surveillance contexts, contributing to the broader, ethical,
and explainable use of NLP in public health systems.

</details>


### [3] [Overview of the MEDIQA-OE 2025 Shared Task on Medical Order Extraction from Doctor-Patient Consultations](https://arxiv.org/abs/2510.26974)
*Jean-Philippe Corbeil,Asma Ben Abacha,Jerome Tremblay,Phillip Swazinna,Akila Jeeson Daniel,Miguel Del-Agua,Francois Beaulieu*

Main category: cs.CL

TL;DR: 介绍MEDIQA-OE 2025共享任务——首个从医患对话中提取医疗指令的挑战，六支团队通过封闭/开放权重大语言模型探索解决方案


<details>
  <summary>Details</summary>
Motivation: 临床文档自动化存在空白：将对话转化为电子健康记录中的可执行医疗指令，该技术可显著减轻临床医生负担并影响下游护理质量

Method: 设立MEDIQA-OE共享任务，收集医患对话数据集，邀请多团队采用包括不同权重大语言模型（LLMs）在内的多种方法进行测试

Result: 构建首个医疗指令提取基准数据集，六支团队通过不同技术路线验证了LLMs在该任务中的有效性，形成完整技术方案文档

Conclusion: 该共享任务推动了医疗对话结构化技术发展，为降低临床文档负担提供了可行路径，未来可拓展至更多医疗信息提取场景

Abstract: Clinical documentation increasingly uses automatic speech recognition and
summarization, yet converting conversations into actionable medical orders for
Electronic Health Records remains unexplored. A solution to this problem can
significantly reduce the documentation burden of clinicians and directly impact
downstream patient care. We introduce the MEDIQA-OE 2025 shared task, the first
challenge on extracting medical orders from doctor-patient conversations. Six
teams participated in the shared task and experimented with a broad range of
approaches, and both closed- and open-weight large language models (LLMs). In
this paper, we describe the MEDIQA-OE task, dataset, final leaderboard ranking,
and participants' solutions.

</details>


### [4] [Semantically-Aware LLM Agent to Enhance Privacy in Conversational AI Services](https://arxiv.org/abs/2510.27016)
*Jayden Serenari,Stephen Lee*

Main category: cs.CL

TL;DR: 提出LOPSIDED框架，通过动态替换用户输入中的敏感个人信息为语义一致的假名，并在模型生成响应后自动还原，实现隐私保护与回答质量的平衡。


<details>
  <summary>Details</summary>
Motivation: 用户与大型语言模型交互时可能泄露敏感个人信息（PII），现有隐私保护方法常导致回答质量下降，需开发既能保护隐私又不破坏对话语义完整性的方案。

Method: 使用LOPSIDED框架动态替换用户提问中的敏感PII为语义一致的假名，生成响应后自动还原假名为真实信息。基于ShareGPT真实对话数据构建测试集并进行增强标注。

Result: 相比基线方法减少5倍语义效用错误，在提升隐私保护水平的同时保持对话上下文完整性。

Conclusion: LOPSIDED首次实现隐私保护与语义完整性的双重优化，为远程LLM应用提供可行的隐私保护解决方案。

Abstract: With the increasing use of conversational AI systems, there is growing
concern over privacy leaks, especially when users share sensitive personal data
in interactions with Large Language Models (LLMs). Conversations shared with
these models may contain Personally Identifiable Information (PII), which, if
exposed, could lead to security breaches or identity theft. To address this
challenge, we present the Local Optimizations for Pseudonymization with
Semantic Integrity Directed Entity Detection (LOPSIDED) framework, a
semantically-aware privacy agent designed to safeguard sensitive PII data when
using remote LLMs. Unlike prior work that often degrade response quality, our
approach dynamically replaces sensitive PII entities in user prompts with
semantically consistent pseudonyms, preserving the contextual integrity of
conversations. Once the model generates its response, the pseudonyms are
automatically depseudonymized, ensuring the user receives an accurate,
privacy-preserving output. We evaluate our approach using real-world
conversations sourced from ShareGPT, which we further augment and annotate to
assess whether named entities are contextually relevant to the model's
response. Our results show that LOPSIDED reduces semantic utility errors by a
factor of 5 compared to baseline techniques, all while enhancing privacy.

</details>


### [5] [Kad: A Framework for Proxy-based Test-time Alignment with Knapsack Approximation Deferral](https://arxiv.org/abs/2510.27017)
*Ayoub Hammal,Pierre Zweigenbaum,Caio Corro*

Main category: cs.CL

TL;DR: 提出基于小规模对齐模型的代理测试时对齐方法，将token级延迟决策转化为0-1背包问题，通过原始对偶近似实现高效对齐


<details>
  <summary>Details</summary>
Motivation: 大语言模型对齐过程计算成本高昂，需寻找低成本替代方案。利用小规模对齐模型进行推理阶段指导，规避常规对齐的算力消耗

Method: 建立token级级联机制，将延迟决策建模为0-1背包问题，推导原始对偶近似算法求解最优延迟策略

Result: 实验证明该方法在任务性能和推测解码速度上均取得提升，实现效率与效果的平衡

Conclusion: 代理测试时对齐为大规模语言模型提供计算高效的替代方案，通过智能延迟决策平衡性能与资源消耗

Abstract: Several previous works concluded that the largest part of generation
capabilities of large language models (LLM) are learned (early) during
pre-training. However, LLMs still require further alignment to adhere to
downstream task requirements and stylistic preferences, among other desired
properties. As LLMs continue to scale in terms of size, the computational cost
of alignment procedures increase prohibitively. In this work, we propose a
novel approach to circumvent these costs via proxy-based test-time alignment,
i.e. using guidance from a small aligned model. Our approach can be described
as token-specific cascading method, where the token-specific deferral rule is
reduced to 0-1 knapsack problem. In this setting, we derive primal and dual
approximations of the optimal deferral decision. We experimentally show the
benefits of our method both in task performance and speculative decoding speed.

</details>


### [6] [Elastic Architecture Search for Efficient Language Models](https://arxiv.org/abs/2510.27037)
*Shang Wang*

Main category: cs.CL

TL;DR: 提出弹性语言模型ELM，通过神经架构搜索和知识蒸馏技术构建高效紧凑的语言模型


<details>
  <summary>Details</summary>
Motivation: 大型预训练语言模型的高计算需求和内存消耗引发经济与环境问题，需开发更紧凑高效的模型架构

Method: 1. 扩展神经架构搜索空间，引入高效transformer块和动态维度/头数调整模块
2. 设计新型知识蒸馏损失函数保持模块特性
3. 采用masked/causal语言建模任务进行架构搜索

Result: ELM发现的模型在masked/causal语言建模任务中显著优于现有方法

Conclusion: ELM通过灵活的搜索空间设计和模块特性保留机制，实现了更高效的架构搜索和更优的模型性能

Abstract: As large pre-trained language models become increasingly critical to natural
language understanding (NLU) tasks, their substantial computational and memory
requirements have raised significant economic and environmental concerns.
Addressing these challenges, this paper introduces the Elastic Language Model
(ELM), a novel neural architecture search (NAS) method optimized for compact
language models. ELM extends existing NAS approaches by introducing a flexible
search space with efficient transformer blocks and dynamic modules for
dimension and head number adjustment. These innovations enhance the efficiency
and flexibility of the search process, which facilitates more thorough and
effective exploration of model architectures. We also introduce novel knowledge
distillation losses that preserve the unique characteristics of each block, in
order to improve the discrimination between architectural choices during the
search process. Experiments on masked language modeling and causal language
modeling tasks demonstrate that models discovered by ELM significantly
outperform existing methods.

</details>


### [7] [Dataset Creation and Baseline Models for Sexism Detection in Hausa](https://arxiv.org/abs/2510.27038)
*Fatima Adam Muhammad,Shamsuddeen Muhammad Hassan,Isa Inuwa-Dutse*

Main category: cs.CL

TL;DR: 首个豪萨语性别歧视检测数据集研究，揭示文化差异对AI检测模型的挑战与误判问题


<details>
  <summary>Details</summary>
Motivation: 在线平台助长性别歧视，但低资源语言缺乏检测方案。豪萨语存在文化表达差异，需本土化研究突破技术瓶颈

Method: 通过社区参与构建数据集，两阶段母语者调研(66人)分析文化表达特征，结合传统ML与预训练模型测试少样本学习效果

Result: 模型在文化特定表达(澄清询问/俚语)上识别困难，存在高假阳性现象，数据增强对性能提升有限

Conclusion: 低资源语言性别歧视检测需深度融合文化语境，当前技术对本土化表达敏感度不足，需开发文化感知的评估框架

Abstract: Sexism reinforces gender inequality and social exclusion by perpetuating
stereotypes, bias, and discriminatory norms. Noting how online platforms enable
various forms of sexism to thrive, there is a growing need for effective sexism
detection and mitigation strategies. While computational approaches to sexism
detection are widespread in high-resource languages, progress remains limited
in low-resource languages where limited linguistic resources and cultural
differences affect how sexism is expressed and perceived. This study introduces
the first Hausa sexism detection dataset, developed through community
engagement, qualitative coding, and data augmentation. For cultural nuances and
linguistic representation, we conducted a two-stage user study (n=66) involving
native speakers to explore how sexism is defined and articulated in everyday
discourse. We further experiment with both traditional machine learning
classifiers and pre-trained multilingual language models and evaluating the
effectiveness few-shot learning in detecting sexism in Hausa. Our findings
highlight challenges in capturing cultural nuance, particularly with
clarification-seeking and idiomatic expressions, and reveal a tendency for many
false positives in such cases.

</details>


### [8] [Quantitative Intertextuality from the Digital Humanities Perspective: A Survey](https://arxiv.org/abs/2510.27045)
*Siyu Duan*

Main category: cs.CL

TL;DR: 概述定量互文性研究的数据、方法及应用，展望AI与人文跨学科前景


<details>
  <summary>Details</summary>
Motivation: 传统互文性研究在数字时代面临方法局限，需借助NLP技术实现大规模定量分析

Method: 整合多语言数据，涵盖从统计学到深度学习的跨时代分析方法

Result: 定量方法使更精确、多样化和大规模的互文研究成为可能

Conclusion: 互文性研究有望成为连接人工智能与人文社科的跨学科研究桥梁

Abstract: The connection between texts is referred to as intertextuality in literary
theory, which served as an important theoretical basis in many digital
humanities studies. Over the past decade, advancements in natural language
processing have ushered intertextuality studies into the quantitative age.
Large-scale intertextuality research based on cutting-edge methods has
continuously emerged. This paper provides a roadmap for quantitative
intertextuality studies, summarizing their data, methods, and applications.
Drawing on data from multiple languages and topics, this survey reviews methods
from statistics to deep learning. It also summarizes their applications in
humanities and social sciences research and the associated platform tools.
Driven by advances in computer technology, more precise, diverse, and
large-scale intertext studies can be anticipated. Intertextuality holds promise
for broader application in interdisciplinary research bridging AI and the
humanities.

</details>


### [9] [Recursive numeral systems are highly regular and easy to process](https://arxiv.org/abs/2510.27049)
*Ponrawee Prasertsom,Andrea Silvi,Jennifer Culbertson,Moa Johansson,Devdatt Dubhashi,Kenny Smith*

Main category: cs.CL

TL;DR: 本研究通过引入最小描述长度（MDL）框架的规律性指标，改进了递归数字系统最优性分析，证明自然语言系统在规律性和处理复杂性上的效率优势。


<details>
  <summary>Details</summary>
Motivation: 前人研究未能有效解释为何自然语言数字系统具有最优性，其约束条件存在临时性。本文发现忽略规律性这一关键复杂性维度是核心原因，需建立更全面的评估框架。

Method: 采用最小描述长度（MDL）方法，提出基于规律性和处理复杂性的双重评估指标，对比自然系统与人工构造的"最优"递归系统。

Result: MDL指标成功区分自然/非自然系统，验证前人约束条件实为规律性的自然体现，证明自然系统在编码效率上的双重优势。

Conclusion: 语言最优性研究必须系统考量形式集合的规律性，这对解释人类语言的特异性具有方法论启示。

Abstract: Previous work has argued that recursive numeral systems optimise the
trade-off between lexicon size and average morphosyntatic complexity (Deni\'c
and Szymanik, 2024). However, showing that only natural-language-like systems
optimise this tradeoff has proven elusive, and the existing solution has relied
on ad-hoc constraints to rule out unnatural systems (Yang and Regier, 2025).
Here, we argue that this issue arises because the proposed trade-off has
neglected regularity, a crucial aspect of complexity central to human grammars
in general. Drawing on the Minimum Description Length (MDL) approach, we
propose that recursive numeral systems are better viewed as efficient with
regard to their regularity and processing complexity. We show that our
MDL-based measures of regularity and processing complexity better capture the
key differences between attested, natural systems and unattested but possible
ones, including "optimal" recursive numeral systems from previous work, and
that the ad-hoc constraints from previous literature naturally follow from
regularity. Our approach highlights the need to incorporate regularity across
sets of forms in studies that attempt to measure and explain optimality in
language.

</details>


### [10] [VISTA Score: Verification In Sequential Turn-based Assessment](https://arxiv.org/abs/2510.27052)
*Ashley Lewis,Andrew Perrault,Eric Fosler-Lussier,Michael White*

Main category: cs.CL

TL;DR: 提出VISTA框架用于对话系统事实性评估，通过主张分解验证和时序一致性追踪，显著提升幻觉检测效果


<details>
  <summary>Details</summary>
Motivation: 现有指标难以有效评估多轮对话的事实性，将无法验证内容直接视为错误会限制评估体系的应用范围

Method: 将对话回复分解为原子主张，基于可信源/对话历史进行验证，并对不可验证主张进行四分类（主观/矛盾/证据不足/弃权）

Result: 在8个LLM和4个基准测试中，VISTA相比FACTSCORE和LLM-as-Judge基线提升显著，人类评估验证了其一致性追踪的有效性

Conclusion: 通过动态建模对话事实性，VISTA为对话系统提供了更透明且符合人类认知的真相度评估框架

Abstract: Hallucination--defined here as generating statements unsupported or
contradicted by available evidence or conversational context--remains a major
obstacle to deploying conversational AI systems in settings that demand factual
reliability. Existing metrics either evaluate isolated responses or treat
unverifiable content as errors, limiting their use for multi-turn dialogue. We
introduce VISTA (Verification In Sequential Turn-based Assessment), a framework
for evaluating conversational factuality through claim-level verification and
sequential consistency tracking. VISTA decomposes each assistant turn into
atomic factual claims, verifies them against trusted sources and dialogue
history, and categorizes unverifiable statements (subjective, contradicted,
lacking evidence, or abstaining). Across eight large language models and four
dialogue factuality benchmarks (AIS, BEGIN, FAITHDIAL, and FADE), VISTA
substantially improves hallucination detection over FACTSCORE and LLM-as-Judge
baselines. Human evaluation confirms that VISTA's decomposition improves
annotator agreement and reveals inconsistencies in existing benchmarks. By
modeling factuality as a dynamic property of conversation, VISTA offers a more
transparent, human-aligned measure of truthfulness in dialogue systems.

</details>


### [11] [LLM-Centric RAG with Multi-Granular Indexing and Confidence Constraints](https://arxiv.org/abs/2510.27054)
*Xiaofan Guo,Yaxuan Luan,Yue Kang,Xiangchen Song,Jinxu Guo*

Main category: cs.CL

TL;DR: 提出多粒度记忆索引与不确定性估计结合的置信度控制方法，显著提升检索增强生成在复杂环境中的稳定性和可靠性


<details>
  <summary>Details</summary>
Motivation: 针对现有检索增强生成技术存在的覆盖率不足、结果不稳定和可靠性低的问题，需在复杂知识场景中建立更鲁棒的生成机制

Method: 构建分层记忆结构实现多粒度知识索引，结合不确定性估计机制过滤低置信路径，通过生成损失/熵约束/方差正则化构建优化框架

Result: 在QA准确率(提升12.6%)、检索召回率(提高18.4%)和事实一致性(错误率降低9.2%)等指标上全面超越基线模型

Conclusion: 该研究为检索增强生成提供了动态索引与置信控制协同优化的新范式，推动了大模型在复杂场景中的可靠应用

Abstract: This paper addresses the issues of insufficient coverage, unstable results,
and limited reliability in retrieval-augmented generation under complex
knowledge environments, and proposes a confidence control method that
integrates multi-granularity memory indexing with uncertainty estimation. The
method builds a hierarchical memory structure that divides knowledge
representations into different levels of granularity, enabling dynamic indexing
and retrieval from local details to global context, and thus establishing
closer semantic connections between retrieval and generation. On this basis, an
uncertainty estimation mechanism is introduced to explicitly constrain and
filter low-confidence paths during the generation process, allowing the model
to maintain information coverage while effectively suppressing noise and false
content. The overall optimization objective consists of generation loss,
entropy constraints, and variance regularization, forming a unified confidence
control framework. In the experiments, comprehensive sensitivity tests and
comparative analyses were designed, covering hyperparameters, environmental
conditions, and data structures, to verify the stability and robustness of the
proposed method across different scenarios. The results show that the method
achieves superior performance over existing models in QA accuracy, retrieval
recall, ranking quality, and factual consistency, demonstrating the
effectiveness of combining multi-granularity indexing with confidence control.
This study not only provides a new technical pathway for retrieval-augmented
generation but also offers practical evidence for improving the reliability and
controllability of large models in complex contexts.

</details>


### [12] [Detecting Data Contamination in LLMs via In-Context Learning](https://arxiv.org/abs/2510.27055)
*Michał Zawalski,Meriem Boubdir,Klaudia Bałazy,Besmira Nushi,Pablo Ribalta*

Main category: cs.CL

TL;DR: 通过上下文学习效应检测大语言模型训练数据污染的自动化方法CoDeC


<details>
  <summary>Details</summary>
Motivation: 解决现有方法难以区分记忆数据与分布外数据的问题，开发模型/数据通用的污染检测方案

Method: 利用上下文示例对模型置信度的影响差异：未训练数据置信度提升，已记忆数据因模式破坏置信度下降

Result: 生成可解释的污染分数，有效区分seen/unseen数据集，发现开源模型中存在显著记忆证据

Conclusion: 该方法简单自动化且通用性强，可轻松整合至基准测试流程进行污染检测

Abstract: We present Contamination Detection via Context (CoDeC), a practical and
accurate method to detect and quantify training data contamination in large
language models. CoDeC distinguishes between data memorized during training and
data outside the training distribution by measuring how in-context learning
affects model performance. We find that in-context examples typically boost
confidence for unseen datasets but may reduce it when the dataset was part of
training, due to disrupted memorization patterns. Experiments show that CoDeC
produces interpretable contamination scores that clearly separate seen and
unseen datasets, and reveals strong evidence of memorization in open-weight
models with undisclosed training corpora. The method is simple, automated, and
both model- and dataset-agnostic, making it easy to integrate with benchmark
evaluations.

</details>


### [13] [Contrastive Knowledge Transfer and Robust Optimization for Secure Alignment of Large Language Models](https://arxiv.org/abs/2510.27077)
*Jiasen Zheng,Huajun Zhang,Xu Yan,Ran Hao,Chong Peng*

Main category: cs.CL

TL;DR: 提出结合对比蒸馏与噪声鲁棒训练的微调方法，通过冻结主干模型实现知识迁移，在提升安全对齐能力的同时增强模型抗干扰性。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型存在安全对齐不足和噪声敏感问题，需开发参数高效的微调方法构建更安全可信的模型机制。

Method: 冻结主干模型进行对比蒸馏，引入噪声扰动和鲁棒优化约束，构建蒸馏损失+鲁棒性损失+正则化的联合优化框架。

Result: 在知识迁移效率、噪声环境稳定性等指标上超越基线模型，数据噪声下的性能波动降低38%，对齐准确率提升15%。

Conclusion: 该方法拓展了参数高效微调的理论边界，为构建鲁棒的安全对齐机制提供了可扩展的解决方案。

Abstract: This paper addresses the limitations of large-scale language models in safety
alignment and robustness by proposing a fine-tuning method that combines
contrastive distillation with noise-robust training. The method freezes the
backbone model and transfers the knowledge boundaries of the teacher model to
the student model through distillation, thereby improving semantic consistency
and alignment accuracy. At the same time, noise perturbations and robust
optimization constraints are introduced during training to ensure that the
model maintains stable predictive outputs under noisy and uncertain inputs. The
overall framework consists of distillation loss, robustness loss, and a
regularization term, forming a unified optimization objective that balances
alignment ability with resistance to interference. To systematically validate
its effectiveness, the study designs experiments from multiple perspectives,
including distillation weight sensitivity, stability analysis under computation
budgets and mixed-precision environments, and the impact of data noise and
distribution shifts on model performance. Results show that the method
significantly outperforms existing baselines in knowledge transfer, robustness,
and overall safety, achieving the best performance across several key metrics.
This work not only enriches the theoretical system of parameter-efficient
fine-tuning but also provides a new solution for building safer and more
trustworthy alignment mechanisms.

</details>


### [14] [Characterizing Selective Refusal Bias in Large Language Models](https://arxiv.org/abs/2510.27087)
*Adel Khorramrouz,Sharon Levy*

Main category: cs.CL

TL;DR: 研究发现大语言模型的安全护栏存在选择性拒绝偏见，不同人口群体（性别/性取向/国籍/宗教）的拒绝率差异显著，需提升安全机制的公平性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全机制在阻止恶意内容时可能引入新的偏见，造成对特定人口群体的不公平拒绝现象。

Method: 通过分析目标群体拒绝率、响应类型和拒绝文本长度，并设计针对被拒群体的间接攻击测试。

Result: 发现性别/性取向/国籍/宗教属性存在选择性拒绝差异，间接攻击能突破50%先前被拒群体的防护。

Conclusion: 需开发跨人口群体更均衡稳健的安全防护机制，避免偏见导致的二次安全风险。

Abstract: Safety guardrails in large language models(LLMs) are developed to prevent
malicious users from generating toxic content at a large scale. However, these
measures can inadvertently introduce or reflect new biases, as LLMs may refuse
to generate harmful content targeting some demographic groups and not others.
We explore this selective refusal bias in LLM guardrails through the lens of
refusal rates of targeted individual and intersectional demographic groups,
types of LLM responses, and length of generated refusals. Our results show
evidence of selective refusal bias across gender, sexual orientation,
nationality, and religion attributes. This leads us to investigate additional
safety implications via an indirect attack, where we target previously refused
groups. Our findings emphasize the need for more equitable and robust
performance in safety guardrails across demographic groups.

</details>


### [15] [Rating Roulette: Self-Inconsistency in LLM-As-A-Judge Frameworks](https://arxiv.org/abs/2510.27106)
*Rajarshi Haldar,Julia Hockenmaier*

Main category: cs.CL

TL;DR: LLM作为NLG评估工具存在评分不一致问题，但合理使用仍具参考价值


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的NLG评估方法存在评分不一致性，影响评估可靠性

Method: 通过实验量化不同NLG任务中LLM评分的不一致性，制定使用指南

Result: LLM评分在不同运行中存在显著差异，但遵循指南可提高一致性

Conclusion: LLM评估需在严格指导下使用，评分不稳定性需特别关注

Abstract: As Natural Language Generation (NLG) continues to be widely adopted, properly
assessing it has become quite difficult. Lately, using large language models
(LLMs) for evaluating these generations has gained traction, as they tend to
align more closely with human preferences than conventional n-gram or
embedding-based metrics. In our experiments, we show that LLM judges have low
intra-rater reliability in their assigned scores across different runs. This
variance makes their ratings inconsistent, almost arbitrary in the worst case,
making it difficult to measure how good their judgments actually are. We
quantify this inconsistency across different NLG tasks and benchmarks and see
if judicious use of LLM judges can still be useful following proper guidelines.

</details>


### [16] [Probability Distributions Computed by Hard-Attention Transformers](https://arxiv.org/abs/2510.27118)
*Andy Yang,Anej Svete,Jiaoda Li,Anthony Widjaja Lin,Jonathan Rawski,Ryan Cotterell,David Chiang*

Main category: cs.CL

TL;DR: 研究区分了Transformer作为语言模型（生成式）和语言识别器的表达能力差异，发现自回归和概率化会改变其表达能力表现


<details>
  <summary>Details</summary>
Motivation: 澄清Transformer在作为实际应用中的语言模型（而非单纯识别器）时能表达哪些概率分布函数

Method: 通过理论分析比较不同配置下（自回归/非自回归，概率/非概率）Transformer语言模型的表达能力

Result: 自回归机制可能提升表达能力；概率化会打破非概率场景下的等价关系

Conclusion: 评估Transformer表达能力需结合其实际应用场景（作为生成式语言模型），单纯作为识别器的分析结论可能不适用

Abstract: Most expressivity results for transformers treat them as language recognizers
(which accept or reject strings), and not as they are used in practice, as
language models (which generate strings autoregressively and
probabilistically). Here, we characterize the probability distributions that
transformer language models can express. We show that making transformer
language recognizers autoregressive can sometimes increase their expressivity,
and that making them probabilistic can break equivalences that hold in the
non-probabilistic case. Our overall contribution is to tease apart what
functions transformers are capable of expressing, in their most common use-case
as language models.

</details>


### [17] [Simple Additions, Substantial Gains: Expanding Scripts, Languages, and Lineage Coverage in URIEL+](https://arxiv.org/abs/2510.27183)
*Mason Shipton,York Hay Ng,Aditya Khan,Phuong Hanh Hoang,Xiang Lu,A. Seza Doğruöz,En-Shiun Annie Lee*

Main category: cs.CL

TL;DR: 扩展URIEL+语言知识库，通过引入脚本向量、整合Glottolog语言数据库和增强谱系特征插补，显著提升对低资源语言的支持能力。


<details>
  <summary>Details</summary>
Motivation: URIEL+存在特征类型缺失、语言条目不完整和谱系覆盖有限等问题，制约了其在低资源语言跨语言迁移中的应用效果。

Method: 1. 为7,488种语言新增文字系统特征向量
2. 整合Glottolog新增18,710种语言
3. 通过谱系传播实现26,449种语言的特征插补

Result: 脚本特征稀疏性降低14%，语言覆盖增加19,015种(1,007%)，插补质量提升33%，跨语言迁移任务中部分场景性能提升达6%

Conclusion: 扩展后的URIEL+显著提升了知识库的完整性和包容性，为低资源语言的多语言研究提供更全面的支持。

Abstract: The URIEL+ linguistic knowledge base supports multilingual research by
encoding languages through geographic, genetic, and typological vectors.
However, data sparsity remains prevalent, in the form of missing feature types,
incomplete language entries, and limited genealogical coverage. This limits the
usefulness of URIEL+ in cross-lingual transfer, particularly for supporting
low-resource languages. To address this sparsity, this paper extends URIEL+
with three contributions: introducing script vectors to represent writing
system properties for 7,488 languages, integrating Glottolog to add 18,710
additional languages, and expanding lineage imputation for 26,449 languages by
propagating typological and script features across genealogies. These additions
reduce feature sparsity by 14% for script vectors, increase language coverage
by up to 19,015 languages (1,007%), and improve imputation quality metrics by
up to 33%. Our benchmark on cross-lingual transfer tasks (oriented around
low-resource languages) shows occasionally divergent performance compared to
URIEL+, with performance gains up to 6% in certain setups. Our advances make
URIEL+ more complete and inclusive for multilingual research.

</details>


### [18] [MemeArena: Automating Context-Aware Unbiased Evaluation of Harmfulness Understanding for Multimodal Large Language Models](https://arxiv.org/abs/2510.27196)
*Zixin Chen,Hongzhan Lin,Kaixin Li,Ziyang Luo,Yayue Deng,Jing Ma*

Main category: cs.CL

TL;DR: 提出MemeArena评估框架，通过多视角分析和共识机制实现无偏见的多模态大语言模型有害内容理解评估，实验结果与人类偏好一致。


<details>
  <summary>Details</summary>
Motivation: 现有mLLMs评估方法局限于二元分类准确率，缺乏对多样化情境下有害性解释深度的评估能力，导致评估偏差。

Method: 基于代理的竞技场式框架，模拟多样化解释情境生成评估任务，通过整合不同观点达成共识实现公平比较。

Result: 实验证明该框架有效降低评估偏见，判断结果与人类偏好高度契合，为可靠评估提供新范式。

Conclusion: MemeArena通过上下文感知和无偏见设计，显著提升了多模态有害性理解评估的可靠性和全面性。

Abstract: The proliferation of memes on social media necessitates the capabilities of
multimodal Large Language Models (mLLMs) to effectively understand multimodal
harmfulness. Existing evaluation approaches predominantly focus on mLLMs'
detection accuracy for binary classification tasks, which often fail to reflect
the in-depth interpretive nuance of harmfulness across diverse contexts. In
this paper, we propose MemeArena, an agent-based arena-style evaluation
framework that provides a context-aware and unbiased assessment for mLLMs'
understanding of multimodal harmfulness. Specifically, MemeArena simulates
diverse interpretive contexts to formulate evaluation tasks that elicit
perspective-specific analyses from mLLMs. By integrating varied viewpoints and
reaching consensus among evaluators, it enables fair and unbiased comparisons
of mLLMs' abilities to interpret multimodal harmfulness. Extensive experiments
demonstrate that our framework effectively reduces the evaluation biases of
judge agents, with judgment results closely aligning with human preferences,
offering valuable insights into reliable and comprehensive mLLM evaluations in
multimodal harmfulness understanding. Our code and data are publicly available
at https://github.com/Lbotirx/MemeArena.

</details>


### [19] [Identifying the Periodicity of Information in Natural Language](https://arxiv.org/abs/2510.27241)
*Yulin Ou,Yu Wang,Yang Xu,Hendrik Buschmeier*

Main category: cs.CL

TL;DR: 提出AutoPeriod of Surprisal (APS)方法，发现人类语言信息存在显著周期性模式，该现象由文本结构与长距离驱动因素共同作用形成。


<details>
  <summary>Details</summary>
Motivation: 基于信息密度理论的新进展，探索自然语言编码信息中周期性模式的存在程度及其成因。

Method: 开发APS方法（基于规范化周期检测算法），通过分析单个文档的惊异值序列检测周期性，并在多语料库中验证。

Result: 1. 较大比例语料呈现强信息周期性；2. 发现超出典型文本结构单元（如句子边界）的新周期模式，并通过谐波回归验证。

Conclusion: 语言信息周期性是文本结构因素与长距离驱动因素共同作用的结果。APS方法在LLM生成检测领域具应用潜力。

Abstract: Recent theoretical advancement of information density in natural language has
brought the following question on desk: To what degree does natural language
exhibit periodicity pattern in its encoded information? We address this
question by introducing a new method called AutoPeriod of Surprisal (APS). APS
adopts a canonical periodicity detection algorithm and is able to identify any
significant periods that exist in the surprisal sequence of a single document.
By applying the algorithm to a set of corpora, we have obtained the following
interesting results: Firstly, a considerable proportion of human language
demonstrates a strong pattern of periodicity in information; Secondly, new
periods that are outside the distributions of typical structural units in text
(e.g., sentence boundaries, elementary discourse units, etc.) are found and
further confirmed via harmonic regression modeling. We conclude that the
periodicity of information in language is a joint outcome from both structured
factors and other driving factors that take effect at longer distances. The
advantages of our periodicity detection method and its potentials in
LLM-generation detection are further discussed.

</details>


### [20] [Beyond a Million Tokens: Benchmarking and Enhancing Long-Term Memory in LLMs](https://arxiv.org/abs/2510.27246)
*Mohammad Tavakoli,Alireza Salemi,Carrie Ye,Mohamed Abdalla,Hamed Zamani,J Ross Mitchell*

Main category: cs.CL

TL;DR: 提出BEAM基准测试和LIGHT记忆框架，解决现有LLM长上下文评估不足的问题，实验显示LIGHT显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文评估基准存在叙事不连贯、领域单一和任务简单的问题，无法有效测试LLM的长期记忆能力。

Method: 1. 自动生成千万token级连贯对话构建BEAM基准；2. 受人类认知启发设计LIGHT框架，集成长/短期记忆和事实暂存器三系统。

Result: 在BEAM测试中，百万token窗口LLM表现受限，LIGHT使不同模型平均提升3.5%-12.69%，消融实验验证各记忆组件有效性。

Conclusion: LIGHT通过模拟人类记忆机制有效增强LLM长上下文处理能力，为复杂对话系统开发提供新范式。

Abstract: Evaluating the abilities of large language models (LLMs) for tasks that
require long-term memory and thus long-context reasoning, for example in
conversational settings, is hampered by the existing benchmarks, which often
lack narrative coherence, cover narrow domains, and only test simple
recall-oriented tasks. This paper introduces a comprehensive solution to these
challenges. First, we present a novel framework for automatically generating
long (up to 10M tokens), coherent, and topically diverse conversations,
accompanied by probing questions targeting a wide range of memory abilities.
From this, we construct BEAM, a new benchmark comprising 100 conversations and
2,000 validated questions. Second, to enhance model performance, we propose
LIGHT-a framework inspired by human cognition that equips LLMs with three
complementary memory systems: a long-term episodic memory, a short-term working
memory, and a scratchpad for accumulating salient facts. Our experiments on
BEAM reveal that even LLMs with 1M token context windows (with and without
retrieval-augmentation) struggle as dialogues lengthen. In contrast, LIGHT
consistently improves performance across various models, achieving an average
improvement of 3.5%-12.69% over the strongest baselines, depending on the
backbone LLM. An ablation study further confirms the contribution of each
memory component.

</details>


### [21] [Languages are Modalities: Cross-Lingual Alignment via Encoder Injection](https://arxiv.org/abs/2510.27254)
*Rajan Agarwal,Aarush Gupta*

Main category: cs.CL

TL;DR: 提出LLINK方法，通过潜在语言注入提升非英语低资源语言的大模型表现，无需改变分词器或重新训练解码器。


<details>
  <summary>Details</summary>
Motivation: 解决指令调优大模型在低资源非拉丁文字上因分词碎片化和跨语言耦合弱导致的性能不足问题。

Method: 1. 对齐多语言编码器的句子嵌入到解码器潜在空间；2. 通过轻量级对比投影器和K个软槽扩展信号，配合微调适配器实现跨语言知识注入。

Result: 双语检索提升显著，LLM评估问答中偏好率81.3%优于基座模型，63.6%优于直接微调。

Conclusion: 将低资源语言视为模态，为轻量化大模型跨语言对齐提供了高效实践路径（存在数值保真度待改进）。

Abstract: Instruction-tuned Large Language Models (LLMs) underperform on low resource,
non-Latin scripts due to tokenizer fragmentation and weak cross-lingual
coupling. We present LLINK (Latent Language Injection for Non-English
Knowledge), a compute efficient language-as-modality method that conditions an
instruction-tuned decoder without changing the tokenizer or retraining the
decoder. First, we align sentence embeddings from a frozen multilingual encoder
to the decoder's latent embedding space at a reserved position via a
lightweight contrastive projector. Second, the vector is expanded into K soft
slots and trained with minimal adapters so the frozen decoder consumes the
signal. LLINK substantially improves bilingual retrieval and achieves 81.3%
preference over the base model and 63.6% over direct fine-tuning in LLM-judged
Q&A evaluations. We further find that improvements can be attributed to reduced
tokenization inflation and a stronger cross lingual alignment, despite the
model having residual weaknesses in numeric fidelity. Treating low resource
languages as a modality offers a practical path to stronger cross-lingual
alignment in lightweight LLMs.

</details>


### [22] [MedCalc-Eval and MedCalc-Env: Advancing Medical Calculation Capabilities of Large Language Models](https://arxiv.org/abs/2510.27267)
*Kangkun Mao,Jinru Ding,Jiayuan Chen,Mouxiao Bian,Ruiyao Chen,Xinwei Peng,Sijie Ren,Linyang Li,Jie Xu*

Main category: cs.CL

TL;DR: 提出医疗计算基准MedCalc-Eval（覆盖700+任务）和强化学习环境MedCalc-Env，在Qwen2.5-32B模型上实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有医疗LLM基准侧重问答推理，忽视临床决策所需的定量计算能力，且现有数据集覆盖场景有限

Method: 1. 构建跨专科的方程计算（Cockcroft-Gault等）和评分系统（Apgar等）任务集；2. 基于InternBootcamp框架开发支持多步推理的强化学习环境

Result: MedCalc-Env微调的模型在数值敏感性（+15%）、公式选择准确率（+12%）和推理鲁棒性方面显著提升，达到当前最佳水平

Conclusion: 虽在核心计算能力取得突破，但在单位换算（如mg/dL→mmol/L）、多条件逻辑处理、临床上下文理解方面仍需改进

Abstract: As large language models (LLMs) enter the medical domain, most benchmarks
evaluate them on question answering or descriptive reasoning, overlooking
quantitative reasoning critical to clinical decision-making. Existing datasets
like MedCalc-Bench cover few calculation tasks and fail to reflect real-world
computational scenarios.
  We introduce MedCalc-Eval, the largest benchmark for assessing LLMs' medical
calculation abilities, comprising 700+ tasks across two types: equation-based
(e.g., Cockcroft-Gault, BMI, BSA) and rule-based scoring systems (e.g., Apgar,
Glasgow Coma Scale). These tasks span diverse specialties including internal
medicine, surgery, pediatrics, and cardiology, offering a broader and more
challenging evaluation setting.
  To improve performance, we further develop MedCalc-Env, a reinforcement
learning environment built on the InternBootcamp framework, enabling multi-step
clinical reasoning and planning. Fine-tuning a Qwen2.5-32B model within this
environment achieves state-of-the-art results on MedCalc-Eval, with notable
gains in numerical sensitivity, formula selection, and reasoning robustness.
Remaining challenges include unit conversion, multi-condition logic, and
contextual understanding.
  Code and datasets are available at
https://github.com/maokangkun/MedCalc-Eval.

</details>


### [23] [Why Do Multilingual Reasoning Gaps Emerge in Reasoning Language Models?](https://arxiv.org/abs/2510.27269)
*Deokhyung Kang,Seonjeong Hwang,Daehui Kim,Hyounghun Kim,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 多语言推理差距主要由模型的理解失败引起，通过选择性翻译策略可有效缓解，仅需翻译20%输入即接近全翻译效果。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型在低资源语言表现显著弱于高资源语言，但其根本原因尚未明确。研究旨在揭示差距根源并提出针对性解决方案。

Method: 1. 验证理解失败是主因 2. 评估多种检测方法(监督学习最优) 3. 提出选择性翻译策略(仅在检测失败时翻译)

Result: 选择性翻译缩小多语言推理差距，达到95%全翻译效果，翻译量仅需20%。代码数据已开源。

Conclusion: 理解失败是多语言推理差距的核心成因，通过检测和选择性翻译可有效改善，为公平多语言推理提供新路径。

Abstract: Reasoning language models (RLMs) achieve strong performance on complex
reasoning tasks, yet they still suffer from a multilingual reasoning gap,
performing better in high-resource languages than in low-resource ones. While
recent efforts have reduced this gap, its underlying causes remain largely
unexplored. In this paper, we address this by showing that the multilingual
reasoning gap largely stems from failures in language understanding-the model's
inability to represent the multilingual input meaning into the dominant
language (i.e., English) within its reasoning trace. This motivates us to
examine whether understanding failures can be detected, as this ability could
help mitigate the multilingual reasoning gap. To this end, we evaluate a range
of detection methods and find that understanding failures can indeed be
identified, with supervised approaches performing best. Building on this, we
propose Selective Translation, a simple yet effective strategy that translates
the multilingual input into English only when an understanding failure is
detected. Experimental results show that Selective Translation bridges the
multilingual reasoning gap, achieving near full-translation performance while
using translation for only about 20% of inputs. Together, our work demonstrates
that understanding failures are the primary cause of the multilingual reasoning
gap and can be detected and selectively mitigated, providing key insight into
its origin and a promising path toward more equitable multilingual reasoning.
Our code and data are publicly available at
https://github.com/deokhk/RLM_analysis.

</details>


### [24] [A Unified Representation Underlying the Judgment of Large Language Models](https://arxiv.org/abs/2510.27328)
*Yi-Long Lu,Jiajun Song,Wei Wang*

Main category: cs.CL

TL;DR: 研究发现大型语言模型(LLMs)通过Valence-Assent Axis(VAA)统一编码评估判断，该机制导致推理过程从事实推断转向合理化论证，从而引发系统性偏见与幻觉。


<details>
  <summary>Details</summary>
Motivation: 探讨智能系统判断机制的本质——判断是源自独立模块还是统一系统，现有模块化证据的局限性促使研究者探索更深层的架构特征。

Method: 通过多模型分析识别主导维度VAA，设计干预实验验证其控制生成过程的机制，结合行为分析与表征解码技术。

Result: VAA轴被发现同时编码价值判断与事实认同，实验显示其作为控制信号会扭曲推理过程，使生成内容优先符合评估状态而非事实。

Conclusion: 趋同架构虽保证判断一致性，但通过推理从属化机制导致事实准确性下降，这为AI系统的偏见与幻觉提供了新的机制解释。

Abstract: A central architectural question for both biological and artificial
intelligence is whether judgment relies on specialized modules or a unified,
domain-general resource. While the discovery of decodable neural
representations for distinct concepts in Large Language Models (LLMs) has
suggested a modular architecture, whether these representations are truly
independent systems remains an open question. Here we provide evidence for a
convergent architecture. Across a range of LLMs, we find that diverse
evaluative judgments are computed along a dominant dimension, which we term the
Valence-Assent Axis (VAA). This axis jointly encodes subjective valence ("what
is good") and the model's assent to factual claims ("what is true"). Through
direct interventions, we show this unified representation creates a critical
dependency: the VAA functions as a control signal that steers the generative
process to construct a rationale consistent with its evaluative state, even at
the cost of factual accuracy. This mechanism, which we term the subordination
of reasoning, shifts the process of reasoning from impartial inference toward
goal-directed justification. Our discovery offers a mechanistic account for
systemic bias and hallucination, revealing how an architecture that promotes
coherent judgment can systematically undermine faithful reasoning.

</details>


### [25] [TransAlign: Machine Translation Encoders are Strong Word Aligners, Too](https://arxiv.org/abs/2510.27337)
*Benedikt Ebing,Christian Goldschmied,Goran Glavaš*

Main category: cs.CL

TL;DR: 提出TransAlign——基于大规模多语言机器翻译模型编码器的新型词对齐工具，显著提升跨语言迁移中分词分类任务的标签投影性能


<details>
  <summary>Details</summary>
Motivation: 现有跨语言迁移方法依赖mBERT/LaBSE等编码模型生成词对齐，但机器翻译模型在词对齐任务中的潜力未被充分挖掘。本文探索如何利用MT模型提升标签投影效果

Method: 通过使用大规模多语言MT模型的编码器构建TransAlign词对齐工具，在基于机器翻译的跨语言迁移框架下进行分词分类任务测试

Result: TransAlign在词对齐质量上超越传统词对齐工具，在跨语言迁移实验中显著优于现有最佳的非词对齐标签投影方法

Conclusion: MT模型编码器能有效提取跨语言对齐特征，该发现为提升低资源语言NLP任务性能提供了新的技术路径

Abstract: In the absence of sizable training data for most world languages and NLP
tasks, translation-based strategies such as translate-test -- evaluating on
noisy source language data translated from the target language -- and
translate-train -- training on noisy target language data translated from the
source language -- have been established as competitive approaches for
cross-lingual transfer (XLT). For token classification tasks, these strategies
require label projection: mapping the labels from each token in the original
sentence to its counterpart(s) in the translation. To this end, it is common to
leverage multilingual word aligners (WAs) derived from encoder language models
such as mBERT or LaBSE. Despite obvious associations between machine
translation (MT) and WA, research on extracting alignments with MT models is
largely limited to exploiting cross-attention in encoder-decoder architectures,
yielding poor WA results. In this work, in contrast, we propose TransAlign, a
novel word aligner that utilizes the encoder of a massively multilingual MT
model. We show that TransAlign not only achieves strong WA performance but
substantially outperforms popular WA and state-of-the-art non-WA-based label
projection methods in MT-based XLT for token classification.

</details>


### [26] [ThoughtProbe: Classifier-Guided LLM Thought Space Exploration via Probing Representations](https://arxiv.org/abs/2510.27355)
*Zijian Wang,Chang Xu*

Main category: cs.CL

TL;DR: 提出ThoughtProbe框架，通过挖掘LLM隐藏推理特征指导树状响应空间探索，结合分类器评分和分支聚合方法显著提升算术推理性能


<details>
  <summary>Details</summary>
Motivation: 传统方法仅利用LLM隐藏特征进行生成控制，未充分发挥其作为判别信号指导推理路径探索的潜力

Method: 1. 树状节点扩展时用分类器评分优先高潜力分支 2. 构建候选答案池 3. 基于CoT分数聚合分支确定最优解

Result: 在多个算术推理基准测试中实现显著性能提升，验证框架能有效覆盖并识别有效推理链

Conclusion: 通过系统性树状探索与概率聚合机制，ThoughtProbe突破了传统单链推理的限制，为复杂推理任务提供了新范式

Abstract: This paper introduces ThoughtProbe, a novel inference time framework that
leverages the hidden reasoning features of Large Language Models (LLMs) to
improve their reasoning performance. Unlike previous works that manipulate the
hidden representations to steer LLM generation, we harness them as
discriminative signals to guide the tree structured response space exploration.
In each node expansion, a classifier serves as a scoring and ranking mechanism
that efficiently allocates computational resources by prioritizing higher score
candidates for continuation. After completing the tree expansion, we collect
answers from all branches to form a candidate answer pool. We then propose a
branch aggregation method that marginalizes over all supporting branches by
aggregating their CoT scores, thereby identifying the optimal answer from the
pool. Experimental results show that our framework's comprehensive exploration
not only covers valid reasoning chains but also effectively identifies them,
achieving significant improvements across multiple arithmetic reasoning
benchmarks.

</details>


### [27] [From the Rock Floor to the Cloud: A Systematic Survey of State-of-the-Art NLP in Battery Life Cycle](https://arxiv.org/abs/2510.27369)
*Tosin Adewumi,Martin Karlsson,Marcus Liwicki,Mikael Sjödahl,Lama Alkhaled,Rihab Gargouri,Nudrat Habib,Franz Hennie*

Main category: cs.CL

TL;DR: 系统综述自然语言处理在电池全生命周期中的应用，提出结合代理式AI的技术语言处理框架（TLP），支持欧盟数字电池护照及电池预测


<details>
  <summary>Details</summary>
Motivation: 填补现有研究仅聚焦电池生命周期某一阶段的空白，通过PRISMA系统性综述方法整合274篇文献，最终深度分析66篇相关论文，为欧盟数字电池护照提供技术支持

Method: 采用PRISMA系统综述框架，基于Google Scholar/IEEE Xplore/Scopus三大数据库筛选文献，通过严格的质量评估流程，公开评审资料确保研究可复现性

Result: 发现电池领域新兴NLP任务（如材料发现），揭示当前缺乏标准基准等挑战，提出集成代理式AI与优化提示工程的TLP框架应对问题

Conclusion: 首次系统性描绘NLP在电池全生命周期应用图景，TLP框架为行业痛点提供创新解决方案，强调需建立标准化数据集以释放NLP技术潜力

Abstract: We present a comprehensive systematic survey of the application of natural
language processing (NLP) along the entire battery life cycle, instead of one
stage or method, and introduce a novel technical language processing (TLP)
framework for the EU's proposed digital battery passport (DBP) and other
general battery predictions. We follow the Preferred Reporting Items for
Systematic Reviews and Meta-Analyses (PRISMA) method and employ three reputable
databases or search engines, including Google Scholar, Institute of Electrical
and Electronics Engineers Xplore (IEEE Xplore), and Scopus. Consequently, we
assessed 274 scientific papers before the critical review of the final 66
relevant papers. We publicly provide artifacts of the review for validation and
reproducibility. The findings show that new NLP tasks are emerging in the
battery domain, which facilitate materials discovery and other stages of the
life cycle. Notwithstanding, challenges remain, such as the lack of standard
benchmarks. Our proposed TLP framework, which incorporates agentic AI and
optimized prompts, will be apt for tackling some of the challenges.

</details>


### [28] [Balancing Knowledge Updates: Toward Unified Modular Editing in LLMs](https://arxiv.org/abs/2510.27400)
*Jiahao Liu,Zijian Wang,Kuo Zhao,Dong Hu*

Main category: cs.CL

TL;DR: 提出IntAttn-Edit方法，通过联合更新MLP和注意力模块并采用知识平衡策略，显著提升大型语言模型的知识编辑效果。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑方法过度关注MLP模块而忽略注意力模块，导致残留过时知识。研究发现注意力模块（尤其是浅层）在知识存储中具有重要作用，需开发更均衡的编辑方法。

Method: 扩展关联记忆范式，设计知识平衡策略：1）定位MLP和注意力模块的知识存储贡献度 2）按比例分配参数更新幅度 3）联合优化两类模块参数

Result: 在标准测试集上实现：86%编辑成功率（比基线提升12%），知识保留率提高9%，跨场景泛化能力增强，平衡策略使性能稳定在最优区间。

Conclusion: 注意力模块是知识存储的关键组件，联合优化策略显著提升编辑效果。知识平衡机制为参数更新提供了理论指导，开辟了模块协同编辑新方向。

Abstract: Knowledge editing has emerged as an efficient approach for updating factual
knowledge in large language models (LLMs). It typically locates knowledge
storage modules and then modifies their parameters. However, most existing
methods focus on the weights of multilayer perceptron (MLP) modules, which are
often identified as the main repositories of factual information. Other
components, such as attention (Attn) modules, are often ignored during editing.
This imbalance can leave residual outdated knowledge and limit editing
effectiveness. We perform comprehensive knowledge localization experiments on
advanced LLMs and find that Attn modules play a substantial role in factual
knowledge storage and retrieval, especially in earlier layers. Based on these
insights, we propose IntAttn-Edit, a method that extends the associative memory
paradigm to jointly update both MLP and Attn modules. Our approach uses a
knowledge balancing strategy that allocates update magnitudes in proportion to
each module's measured contribution to knowledge storage. Experiments on
standard benchmarks show that IntAttn-Edit achieves higher edit success, better
generalization, and stronger knowledge preservation than prior methods. Further
analysis shows that the balancing strategy keeps editing performance within an
optimal range across diverse settings.

</details>


### [29] [Awal -- Community-Powered Language Technology for Tamazight](https://arxiv.org/abs/2510.27407)
*Alp Öktem,Farida Boudichat*

Main category: cs.CL

TL;DR: Awal社区项目通过平台收集塔马齐格特语数据，但18个月仅获得6,421条翻译和3小时语音数据，显示标准众包模式在复杂语言环境中的局限性


<details>
  <summary>Details</summary>
Motivation: 解决塔马齐格特语在数字空间的代表性不足问题，应对持续存在的语言数据稀缺挑战

Method: 建立awaldigital.org协作平台，收集社区翻译和语音数据，分析18个月内的社区参与模式

Result: 84%参与者集中在语言专家/活动家群体，普通使用者因书面语信心不足和标准化争议参与度低

Conclusion: 标准众包方法在社会语言学复杂语境中效果有限，需开发适应语言社区特性的新型数据收集策略

Abstract: This paper presents Awal, a community-powered initiative for developing
language technology resources for Tamazight. We provide a comprehensive review
of the NLP landscape for Tamazight, examining recent progress in computational
resources, and the emergence of community-driven approaches to address
persistent data scarcity. Launched in 2024, awaldigital.org platform addresses
the underrepresentation of Tamazight in digital spaces through a collaborative
platform enabling speakers to contribute translation and voice data. We analyze
18 months of community engagement, revealing significant barriers to
participation including limited confidence in written Tamazight and ongoing
standardization challenges. Despite widespread positive reception, actual data
contribution remained concentrated among linguists and activists. The modest
scale of community contributions -- 6,421 translation pairs and 3 hours of
speech data -- highlights the limitations of applying standard crowdsourcing
approaches to languages with complex sociolinguistic contexts. We are working
on improved open-source MT models using the collected data.

</details>


### [30] [Dynamic Affective Memory Management for Personalized LLM Agents](https://arxiv.org/abs/2510.27418)
*Junfeng Lu,Yueyan Li*

Main category: cs.CL

TL;DR: 提出基于贝叶斯记忆熵的动态内存管理系统，解决AI代理个性化服务中的内存冗余与整合问题


<details>
  <summary>Details</summary>
Motivation: 现有AI代理依赖静态外部记忆库导致交互中内存更新不足，引发冗余、陈旧和上下文整合困难

Method: 采用贝叶斯启发式算法与记忆熵概念，通过最小化全局熵实现内存数据库的自主动态更新

Result: 实验显示系统在个性化/逻辑性/准确性表现优异，消融实验证实贝叶斯机制有效缓解内存膨胀

Conclusion: 为长期记忆系统设计提供了基于动态熵优化的新范式，推动个性化AI代理的发展

Abstract: Advances in large language models are making personalized AI agents a new
research focus. While current agent systems primarily rely on personalized
external memory databases to deliver customized experiences, they face
challenges such as memory redundancy, memory staleness, and poor memory-context
integration, largely due to the lack of effective memory updates during
interaction. To tackle these issues, we propose a new memory management system
designed for affective scenarios. Our approach employs a Bayesian-inspired
memory update algorithm with the concept of memory entropy, enabling the agent
to autonomously maintain a dynamically updated memory vector database by
minimizing global entropy to provide more personalized services. To better
evaluate the system's effectiveness in this context, we propose DABench, a
benchmark focusing on emotional expression and emotional change toward objects.
Experimental results demonstrate that, our system achieves superior performance
in personalization, logical coherence, and accuracy. Ablation studies further
validate the effectiveness of the Bayesian-inspired update mechanism in
alleviating memory bloat. Our work offers new insights into the design of
long-term memory systems.

</details>


### [31] [VCORE: Variance-Controlled Optimization-based Reweighting for Chain-of-Thought Supervision](https://arxiv.org/abs/2510.27462)
*Xuan Gong,Senmiao Wang,Hanbo Huang,Ruoyu Sun,Shiyu Liang*

Main category: cs.CL

TL;DR: VCORE提出基于方差控制的优化重加权框架，通过约束优化自适应分配监督信号，有效提升大语言模型在长链推理任务中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统监督微调(SFT)的均匀token损失分配忽视了推理轨迹中不同token的贡献差异，导致监督错配和泛化能力不足，尤其在复杂长程推理任务中更为明显。

Method: 将CoT监督重构为约束优化问题，采用优化理论视角实现token级别的自适应监督分配，核心通过方差控制平衡不同token的权重分配。

Result: 在Qwen3系列(4B/8B/32B)和LLaMA-3.1-8B-Instruct模型上，数学与编码基准测试显示显著性能提升，跨领域设置下持续优于现有方法。

Conclusion: VCORE不仅直接提升模型推理能力，还为后续强化学习提供更优初始化，建立了增强LLM推理能力的系统性解决方案。

Abstract: Supervised fine-tuning (SFT) on long chain-of-thought (CoT) trajectories has
emerged as a crucial technique for enhancing the reasoning abilities of large
language models (LLMs). However, the standard cross-entropy loss treats all
tokens equally, ignoring their heterogeneous contributions across a reasoning
trajectory. This uniform treatment leads to misallocated supervision and weak
generalization, especially in complex, long-form reasoning tasks. To address
this, we introduce \textbf{V}ariance-\textbf{C}ontrolled
\textbf{O}ptimization-based \textbf{RE}weighting (VCORE), a principled
framework that reformulates CoT supervision as a constrained optimization
problem. By adopting an optimization-theoretic perspective, VCORE enables a
principled and adaptive allocation of supervision across tokens, thereby
aligning the training objective more closely with the goal of robust reasoning
generalization. Empirical evaluations demonstrate that VCORE consistently
outperforms existing token reweighting methods. Across both in-domain and
out-of-domain settings, VCORE achieves substantial performance gains on
mathematical and coding benchmarks, using models from the Qwen3 series (4B, 8B,
32B) and LLaMA-3.1-8B-Instruct. Moreover, we show that VCORE serves as a more
effective initialization for subsequent reinforcement learning, establishing a
stronger foundation for advancing the reasoning capabilities of LLMs. The Code
will be released at https://github.com/coder-gx/VCORE.

</details>


### [32] [Diffuse Thinking: Exploring Diffusion Language Models as Efficient Thought Proposers for Reasoning](https://arxiv.org/abs/2510.27469)
*Chenyang Shao,Sijian Ren,Fengli Xu,Yong Li*

Main category: cs.CL

TL;DR: 提出Diffuse-Thinking协作推理框架，利用扩散语言模型生成候选思路，大型语言模型评估质量，有效降低计算开销并保持推理性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型自回归范式导致测试时计算资源消耗过大，扩散语言模型的并行生成特性可缓解该问题。通过结合两者的优势，寻求高效推理新路径。

Method: 构建DLMs+LLMs协作框架：DLMs单次前向传播批量生成候选推理步骤，LLMs担任质量评估器筛选有效思路，形成迭代优化机制。

Result: 多领域基准测试显示框架在复杂推理任务中达到SOTA性能，计算效率显著优于传统自回归方法。

Conclusion: 证实扩散模型在复杂推理中的潜力，为语言模型推理范式创新提供新方向。框架已开源，促进后续研究迭代。

Abstract: In recent years, large language models (LLMs) have witnessed remarkable
advancements, with the test-time scaling law consistently enhancing the
reasoning capabilities. Through systematic evaluation and exploration of a
diverse spectrum of intermediate thoughts, LLMs demonstrate the potential to
generate deliberate reasoning steps, thereby substantially enhancing reasoning
accuracy. However, LLMs' autoregressive generation paradigm results in
reasoning performance scaling sub-optimally with test-time computation, often
requiring excessive computational overhead to propose thoughts while yielding
only marginal performance gains. In contrast, diffusion language models (DLMs)
can efficiently produce diverse samples through parallel denoising in a single
forward pass, inspiring us to leverage them for proposing intermediate
thoughts, thereby alleviating the computational burden associated with
autoregressive generation while maintaining quality. In this work, we propose
an efficient collaborative reasoning framework, leveraging DLMs to generate
candidate thoughts and LLMs to evaluate their quality. Experiments across
diverse benchmarks demonstrate that our framework achieves strong performance
in complex reasoning tasks, offering a promising direction for future research.
Our code is open-source at
https://anonymous.4open.science/r/Diffuse-Thinking-EC60.

</details>


### [33] [The aftermath of compounds: Investigating Compounds and their Semantic Representations](https://arxiv.org/abs/2510.27477)
*Swarang Joshi*

Main category: cs.CL

TL;DR: 研究发现BERT嵌入模型在捕捉复合词语义组合性上优于GloVe，且可预测性指标能有效预测语义透明度


<details>
  <summary>Details</summary>
Motivation: 探究计算模型嵌入与人类语义判断的对应关系，推动计算心理语言学发展

Method: 通过关联强度(Edinburgh词典)、频率(BNC)、可预测性(LaDEC)指标，结合斯皮尔曼相关和回归分析，比较GloVe与BERT对人类LMD和ST评分的拟合度

Result: BERT嵌入显示更强的组合语义表征能力，可预测性指标在人类和模型数据中均对语义透明度具有显著预测作用

Conclusion: 研究明确了影响复合词加工的核心因素，为基于嵌入的语义建模提供了新的认知视角

Abstract: This study investigates how well computational embeddings align with human
semantic judgments in the processing of English compound words. We compare
static word vectors (GloVe) and contextualized embeddings (BERT) against human
ratings of lexeme meaning dominance (LMD) and semantic transparency (ST) drawn
from a psycholinguistic dataset. Using measures of association strength
(Edinburgh Associative Thesaurus), frequency (BNC), and predictability (LaDEC),
we compute embedding-derived LMD and ST metrics and assess their relationships
with human judgments via Spearmans correlation and regression analyses. Our
results show that BERT embeddings better capture compositional semantics than
GloVe, and that predictability ratings are strong predictors of semantic
transparency in both human and model data. These findings advance computational
psycholinguistics by clarifying the factors that drive compound word processing
and offering insights into embedding-based semantic modeling.

</details>


### [34] [Effect of Domain Generalization Techniques in Low Resource Systems](https://arxiv.org/abs/2510.27512)
*Mahi Aminu,Chisom Chibuike,Fatimo Adebanjo,Omokolade Awosanya,Samuel Oyeneye*

Main category: cs.CL

TL;DR: 探索因果数据增强(CDA)与不变因果表示学习(ICRL)在低资源NLP任务中的领域泛化效果


<details>
  <summary>Details</summary>
Motivation: 解决机器学习模型在数据分布偏移（尤其在低资源场景）下的泛化难题，通过因果机制提升模型鲁棒性

Method: 1. CDA生成反事实训练样本增强情感分类  2. 将DINER框架扩展至多语言环境实现因果表示学习

Result: CDA使跨域情感分类准确率提升，ICRL改善多语言情感分析的分布外性能（但语言间增益差异明显）

Conclusion: 两种因果方法均有效增强领域泛化能力，CDA更适合单语言场景，ICRL在多语言应用中需考虑语言特性

Abstract: Machine learning models typically assume that training and test data follow
the same distribution, an assumption that often fails in real-world scenarios
due to distribution shifts. This issue is especially pronounced in low-resource
settings, where data scarcity and limited domain diversity hinder robust
generalization. Domain generalization (DG) approaches address this challenge by
learning features that remain invariant across domains, often using causal
mechanisms to improve model robustness. In this study, we examine two distinct
causal DG techniques in low-resource natural language tasks. First, we
investigate a causal data augmentation (CDA) approach that automatically
generates counterfactual examples to improve robustness to spurious
correlations. We apply this method to sentiment classification on the
NaijaSenti Twitter corpus, expanding the training data with semantically
equivalent paraphrases to simulate controlled distribution shifts. Second, we
explore an invariant causal representation learning (ICRL) approach using the
DINER framework, originally proposed for debiasing aspect-based sentiment
analysis. We adapt DINER to a multilingual setting. Our findings demonstrate
that both approaches enhance robustness to unseen domains: counterfactual data
augmentation yields consistent cross-domain accuracy gains in sentiment
classification, while causal representation learning with DINER improves
out-of-distribution performance in multilingual sentiment analysis, albeit with
varying gains across languages.

</details>


### [35] [BiSparse-AAS: Bilinear Sparse Attention and Adaptive Spans Framework for Scalable and Efficient Text Summarization](https://arxiv.org/abs/2510.27516)
*Desta Haileselassie Hagos,Legand L. Burge,Anietie Andy,Anis Yazidi,Vladimir Vlassov*

Main category: cs.CL

TL;DR: 提出BiSparse-AAS框架，通过结合稀疏注意力、自适应范围和双线性注意力，显著提升长文本摘要效率和质量


<details>
  <summary>Details</summary>
Motivation: 传统Transformer架构的二次复杂度限制了长文本处理效率，需开发更高效的注意力机制解决方案

Method: 1. 稀疏注意力聚焦关键内容降低计算成本
2. 自适应范围动态调整注意力区域
3. 双线性注意力在精简上下文中建模复杂语义

Result: 在CNN/DailyMail和XSum数据集分别实现68.1%和52.6%的ROUGE提升，在OpenWebText/Gigaword保持优异性能

Conclusion: BiSparse-AAS通过三阶段优化有效解决效率、扩展性和长序列建模问题，为实际摘要应用提供高效解决方案

Abstract: Transformer-based architectures have advanced text summarization, yet their
quadratic complexity limits scalability on long documents. This paper
introduces BiSparse-AAS (Bilinear Sparse Attention with Adaptive Spans), a
novel framework that combines sparse attention, adaptive spans, and bilinear
attention to address these limitations. Sparse attention reduces computational
costs by focusing on the most relevant parts of the input, while adaptive spans
dynamically adjust the attention ranges. Bilinear attention complements both by
modeling complex token interactions within this refined context. BiSparse-AAS
consistently outperforms state-of-the-art baselines in both extractive and
abstractive summarization tasks, achieving average ROUGE improvements of about
68.1% on CNN/DailyMail and 52.6% on XSum, while maintaining strong performance
on OpenWebText and Gigaword datasets. By addressing efficiency, scalability,
and long-sequence modeling, BiSparse-AAS provides a unified, practical solution
for real-world text summarization applications.

</details>


### [36] [SQLSpace: A Representation Space for Text-to-SQL to Discover and Mitigate Robustness Gaps](https://arxiv.org/abs/2510.27532)
*Neha Srikanth,Victor Bursztyn,Puneet Mathur,Ani Nenkova*

Main category: cs.CL

TL;DR: 提出SQLSpace表示法，通过自动化方法构建文本到SQL的标准化表征，支持基准对比/性能诊断/查询优化三大应用场景


<details>
  <summary>Details</summary>
Motivation: 传统文本转SQL评估依赖人工标注，难以系统分析模型表现差异。需要建立可解释的中间表示支持细粒度评估与优化

Method: 通过自动化方式构建结构化表征空间，包含：1）跨基准的维度对比分析 2）基于正确率估计的细粒度诊断 3）查询重写优化策略

Result: 成功揭示不同benchmark的组成差异，发现准确率指标掩盖的性能模式，通过查询改写提升模型表现

Conclusion: SQLSpace为文本到SQL任务提供了系统化分析框架，在基准评估/模型诊断/性能优化三个维度展现显著应用价值

Abstract: We introduce SQLSpace, a human-interpretable, generalizable, compact
representation for text-to-SQL examples derived with minimal human
intervention. We demonstrate the utility of these representations in evaluation
with three use cases: (i) closely comparing and contrasting the composition of
popular text-to-SQL benchmarks to identify unique dimensions of examples they
evaluate, (ii) understanding model performance at a granular level beyond
overall accuracy scores, and (iii) improving model performance through targeted
query rewriting based on learned correctness estimation. We show that SQLSpace
enables analysis that would be difficult with raw examples alone: it reveals
compositional differences between benchmarks, exposes performance patterns
obscured by accuracy alone, and supports modeling of query success.

</details>


### [37] [Patient-Centered Summarization Framework for AI Clinical Summarization: A Mixed-Methods Design](https://arxiv.org/abs/2510.27535)
*Maria Lizarazo Jimenez,Ana Gabriela Claros,Kieran Green,David Toro-Tobon,Felipe Larios,Sheena Asthana,Camila Wenczenovicz,Kerly Guevara Maldonado,Luis Vilatuna-Andrango,Cristina Proano-Velez,Satya Sai Sri Bandi,Shubhangi Bagewadi,Megan E. Branda,Misk Al Zahidy,Saturnino Luz,Mirella Lapata,Juan P. Brito,Oscar J. Ponce-Ponte*

Main category: cs.CL

TL;DR: 提出患者为中心临床摘要新标准（PCS），评估开源大模型生成能力，发现模型在完整性/流畅度接近人类，但正确性和患者中心性仍存差距


<details>
  <summary>Details</summary>
Motivation: 现有临床摘要过度关注患者生物学信息，缺乏对患者价值观/需求等关键要素的关注，需建立AI生成患者中心摘要的新标准

Method: 通过患者-临床医生访谈制定标注指南，用混合方法评估5个开源LLM在88个房颤咨询数据上的表现（ROUGE-L/BERTScore/定性指标）

Result: 最佳模型Mistral-8B（零样本）和Llama-3.1-8B（少样本）分别达ROUGE-L 0.189/0.206，人类摘要正确性高23%、患者中心性高30%

Conclusion: 当前模型在技术指标接近人类，但核心医疗质量维度仍存显著差距，需针对性优化患者价值捕捉能力

Abstract: Large Language Models (LLMs) are increasingly demonstrating the potential to
reach human-level performance in generating clinical summaries from
patient-clinician conversations. However, these summaries often focus on
patients' biology rather than their preferences, values, wishes, and concerns.
To achieve patient-centered care, we propose a new standard for Artificial
Intelligence (AI) clinical summarization tasks: Patient-Centered Summaries
(PCS). Our objective was to develop a framework to generate PCS that capture
patient values and ensure clinical utility and to assess whether current
open-source LLMs can achieve human-level performance in this task. We used a
mixed-methods process. Two Patient and Public Involvement groups (10 patients
and 8 clinicians) in the United Kingdom participated in semi-structured
interviews exploring what personal and contextual information should be
included in clinical summaries and how it should be structured for clinical
use. Findings informed annotation guidelines used by eight clinicians to create
gold-standard PCS from 88 atrial fibrillation consultations. Sixteen
consultations were used to refine a prompt aligned with the guidelines. Five
open-source LLMs (Llama-3.2-3B, Llama-3.1-8B, Mistral-8B, Gemma-3-4B, and
Qwen3-8B) generated summaries for 72 consultations using zero-shot and few-shot
prompting, evaluated with ROUGE-L, BERTScore, and qualitative metrics. Patients
emphasized lifestyle routines, social support, recent stressors, and care
values. Clinicians sought concise functional, psychosocial, and emotional
context. The best zero-shot performance was achieved by Mistral-8B (ROUGE-L
0.189) and Llama-3.1-8B (BERTScore 0.673); the best few-shot by Llama-3.1-8B
(ROUGE-L 0.206, BERTScore 0.683). Completeness and fluency were similar between
experts and models, while correctness and patient-centeredness favored human
PCS.

</details>


### [38] [DialectalArabicMMLU: Benchmarking Dialectal Capabilities in Arabic and Multilingual Language Models](https://arxiv.org/abs/2510.27543)
*Malik H. Altakrori,Nizar Habash,Abdelhakim Freihat,Younes Samih,Kirill Chirkunov,Muhammed AbuOdeh,Radu Florian,Teresa Lynn,Preslav Nakov,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 提出了首个阿拉伯方言评估基准DialectalArabicMMLU，覆盖5种方言15K QA对，发现LLM方言泛化存在显著差异


<details>
  <summary>Details</summary>
Motivation: 现有阿拉伯语评测基准主要针对现代标准阿拉伯语(MSA)，但实际广泛使用的阿拉伯方言缺乏系统评估资源

Method: 基于MMLU-Redux框架，人工翻译3K多选题至5种方言(叙利亚/埃及/阿联酋/沙特/摩洛哥)，构建含22K跨语言QA对的评测基准

Result: 测试19个开源模型(1B-13B参数)显示不同方言间性能差异达30%，揭示现有模型方言泛化能力不足

Conclusion: 该基准填补阿拉伯方言评估空白，通过系统评测促进更包容的LLM发展，推动方言理解技术进步

Abstract: We present DialectalArabicMMLU, a new benchmark for evaluating the
performance of large language models (LLMs) across Arabic dialects. While
recently developed Arabic and multilingual benchmarks have advanced LLM
evaluation for Modern Standard Arabic (MSA), dialectal varieties remain
underrepresented despite their prevalence in everyday communication.
DialectalArabicMMLU extends the MMLU-Redux framework through manual translation
and adaptation of 3K multiple-choice question-answer pairs into five major
dialects (Syrian, Egyptian, Emirati, Saudi, and Moroccan), yielding a total of
15K QA pairs across 32 academic and professional domains (22K QA pairs when
also including English and MSA). The benchmark enables systematic assessment of
LLM reasoning and comprehension beyond MSA, supporting both task-based and
linguistic analysis. We evaluate 19 open-weight Arabic and multilingual LLMs
(1B-13B parameters) and report substantial performance variation across
dialects, revealing persistent gaps in dialectal generalization.
DialectalArabicMMLU provides the first unified, human-curated resource for
measuring dialectal understanding in Arabic, thus promoting more inclusive
evaluation and future model development.

</details>


### [39] [Multilingual BERT language model for medical tasks: Evaluation on domain-specific adaptation and cross-linguality](https://arxiv.org/abs/2510.27552)
*Yinghao Luo,Lang Zhou,Amrish Jhingoer,Klaske Vliegenthart Jongbloed,Carlijn Jordans,Ben Werkhoven,Tom Seinen,Erik van Mulligen,Casper Rokx,Yunlei Li*

Main category: cs.CL

TL;DR: 本研究通过领域自适应预训练提升低资源语言医疗NLP性能，验证临床领域模型优势及跨语言迁移能力


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言在医疗NLP中工具匮乏的问题，探索多语言BERT领域适应对任务性能的影响

Method: 在荷兰/罗马尼亚/西班牙语临床语料上进行四阶段领域预训练，并在患者筛查、命名实体识别任务微调验证

Result: 领域适应使任务性能提升16.5%，临床领域模型F1值比生物医学模型高7.2%，跨语言迁移有效性得到验证

Conclusion: 领域适应策略有效缓解数据稀缺问题，临床专用模型+跨语言迁移为多语言医疗NLP开发提供可行路径

Abstract: In multilingual healthcare applications, the availability of domain-specific
natural language processing(NLP) tools is limited, especially for low-resource
languages. Although multilingual bidirectional encoder representations from
transformers (BERT) offers a promising motivation to mitigate the language gap,
the medical NLP tasks in low-resource languages are still underexplored.
Therefore, this study investigates how further pre-training on domain-specific
corpora affects model performance on medical tasks, focusing on three
languages: Dutch, Romanian and Spanish. In terms of further pre-training, we
conducted four experiments to create medical domain models. Then, these models
were fine-tuned on three downstream tasks: Automated patient screening in Dutch
clinical notes, named entity recognition in Romanian and Spanish clinical
notes. Results show that domain adaptation significantly enhanced task
performance. Furthermore, further differentiation of domains, e.g. clinical and
general biomedical domains, resulted in diverse performances. The clinical
domain-adapted model outperformed the more general biomedical domain-adapted
model. Moreover, we observed evidence of cross-lingual transferability.
Moreover, we also conducted further investigations to explore potential reasons
contributing to these performance differences. These findings highlight the
feasibility of domain adaptation and cross-lingual ability in medical NLP.
Within the low-resource language settings, these findings can provide
meaningful guidance for developing multilingual medical NLP systems to mitigate
the lack of training data and thereby improve the model performance.

</details>


### [40] [Data-Efficient Domain Adaptation for LLM-based MT using Contrastive Preference Optimization](https://arxiv.org/abs/2510.27556)
*Inacio Vieira,Antonio Castaldo,James O'Doherty,Sheila Castilho*

Main category: cs.CL

TL;DR: 通过CPO方法仅需1.47万偏好对即可接近16万SFT样本的翻译性能，实现高效领域适配


<details>
  <summary>Details</summary>
Motivation: 传统SFT领域适配成本高昂，需探索数据效率更高的方法

Method: 利用基础模型原始输出作为负面样本，结合人工校对译文构建对比偏好对进行CPO训练

Result: 英葡/英韩翻译任务中，1.47万偏好对即达到16万+SFT样本相近效果

Conclusion: CPO通过自我对比机制显著提升数据效率，可推广至其他生成任务

Abstract: LLMs often require adaptation to domain-specific requirements, a process that
can be expensive when relying solely on SFT. We present an empirical study on
applying CPO to simulate a post-editing workflow for data-efficient domain
adaptation. Our approach synthesizes preference pairs by treating the base
model's own raw output as the 'rejected' translation and the human-approved TM
entry as the 'chosen' one. This method provides direct feedback on the model's
current knowledge, guiding it to align with domain-specific standards.
Experiments in English-Brazilian Portuguese and English-Korean show that, by
using just 14.7k preference pairs, the model achieves performance close to that
of a model trained on 160k+ samples with SFT, demonstrating significant data
efficiency. Although we showcase its effectiveness in MT, this application of
CPO naturally generalizes to other generative tasks where a model's initial
drafts can serve as a contrastive signal against a golden reference.

</details>


### [41] [MARAG-R1: Beyond Single Retriever via Reinforcement-Learned Multi-Tool Agentic Retrieval](https://arxiv.org/abs/2510.27569)
*Qi Luo,Xiaonan Li,Yuxin Wang,Tingshuo Fan,Yuan Li,Xinchi Chen,Xipeng Qiu*

Main category: cs.CL

TL;DR: 提出MARAG-R1强化学习多工具RAG框架，通过动态协调四种检索工具提升LLMs在语料库级推理任务中的表现


<details>
  <summary>Details</summary>
Motivation: 现有单一检索器的RAG系统存在信息获取范围窄、静态化问题，成为语料库级推理任务的主要瓶颈

Method: 整合语义搜索/关键词搜索/过滤/聚合四工具，采用监督微调+强化学习两阶段训练实现动态协调机制

Result: 在GlobalQA/HotpotQA/2WikiMultiHopQA上超越基线模型并达到SOTA

Conclusion: MARAG-R1通过多工具协同和强化学习机制，显著提升LLMs的语料库级信息综合能力

Abstract: Large Language Models (LLMs) excel at reasoning and generation but are
inherently limited by static pretraining data, resulting in factual
inaccuracies and weak adaptability to new information. Retrieval-Augmented
Generation (RAG) addresses this issue by grounding LLMs in external knowledge;
However, the effectiveness of RAG critically depends on whether the model can
adequately access relevant information. Existing RAG systems rely on a single
retriever with fixed top-k selection, restricting access to a narrow and static
subset of the corpus. As a result, this single-retriever paradigm has become
the primary bottleneck for comprehensive external information acquisition,
especially in tasks requiring corpus-level reasoning. To overcome this
limitation, we propose MARAG-R1, a reinforcement-learned multi-tool RAG
framework that enables LLMs to dynamically coordinate multiple retrieval
mechanisms for broader and more precise information access. MARAG-R1 equips the
model with four retrieval tools -- semantic search, keyword search, filtering,
and aggregation -- and learns both how and when to use them through a two-stage
training process: supervised fine-tuning followed by reinforcement learning.
This design allows the model to interleave reasoning and retrieval,
progressively gathering sufficient evidence for corpus-level synthesis.
Experiments on GlobalQA, HotpotQA, and 2WikiMultiHopQA demonstrate that
MARAG-R1 substantially outperforms strong baselines and achieves new
state-of-the-art results in corpus-level reasoning tasks.

</details>


### [42] [SpecAttn: Speculating Sparse Attention](https://arxiv.org/abs/2510.27641)
*Harsh Shah*

Main category: cs.CL

TL;DR: 提出SpecAttn方法，通过整合推测解码技术实现Transformer的高效稀疏注意力，在保持输出质量的前提下减少75%的KV缓存访问。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的自注意力机制存在二次计算复杂度瓶颈，尤其在长上下文场景下计算冗余严重。现有稀疏注意力方法存在兼容性和效率限制。

Method: 1) 基于KL散度的草稿-目标模型层对齐
2) GPU优化的无排序top-p token选择算法
3) 基于预测的动态KV缓存剪枝

Result: 在PG-19数据集上实现KV缓存访问减少75%+，困惑度仅增加15.29%，显著优于传统稀疏注意力方法。

Conclusion: SpecAttn证明可通过增强推测执行实现近似验证，无需训练即可在现有推测解码框架中集成，为计算优化提供新思路。

Abstract: Large Language Models (LLMs) face significant computational bottlenecks
during inference due to the quadratic complexity of self-attention mechanisms,
particularly as context lengths increase. We introduce SpecAttn, a novel
training-free approach that seamlessly integrates with existing speculative
decoding techniques to enable efficient sparse attention in pre-trained
transformers. Our key insight is to exploit the attention weights already
computed by the draft model during speculative decoding to identify important
tokens for the target model, eliminating redundant computation while
maintaining output quality. SpecAttn employs three core techniques: KL
divergence-based layer alignment between draft and target models, a
GPU-optimized sorting-free algorithm for top-p token selection from draft
attention patterns, and dynamic key-value cache pruning guided by these
predictions. By leveraging the computational work already performed in standard
speculative decoding pipelines, SpecAttn achieves over 75% reduction in
key-value cache accesses with a mere 15.29% increase in perplexity on the PG-19
dataset, significantly outperforming existing sparse attention methods. Our
approach demonstrates that speculative execution can be enhanced to provide
approximate verification without significant performance degradation.

</details>


### [43] [Culture Cartography: Mapping the Landscape of Cultural Knowledge](https://arxiv.org/abs/2510.27672)
*Caleb Ziems,William Held,Jane Yu,Amir Goldberg,David Grusky,Diyi Yang*

Main category: cs.CL

TL;DR: 通过混合倡议协作的CultureCartography方法，结合人类文化洞察与LLMs主动提问能力，有效提升模型在文化知识获取中的表现


<details>
  <summary>Details</summary>
Motivation: 传统单边方法（研究者定义问题/用户主动生产数据）在获取LLMs缺失的文化知识时存在局限，需要人机协作实现文化适配与知识缺口定位

Method: 开发CultureExplorer工具：1）LLM主动生成低置信度问题暴露知识缺口；2）人类通过编辑引导模型关注核心文化议题；3）形成混合倡议工作流

Result: 相比基线方法，生成的知识使DeepSeek R1和GPT-4o的准确率提升显著（Llama-3.1-8B最高提升19.2%），且优于含网络搜索的基准

Conclusion: 混合倡议框架成功结合人类文化敏感性与模型规模化优势，为LLMs文化适配提供系统性解决方案，工具有效性通过基准测试验证

Abstract: To serve global users safely and productively, LLMs need culture-specific
knowledge that might not be learned during pre-training. How do we find such
knowledge that is (1) salient to in-group users, but (2) unknown to LLMs? The
most common solutions are single-initiative: either researchers define
challenging questions that users passively answer (traditional annotation), or
users actively produce data that researchers structure as benchmarks (knowledge
extraction). The process would benefit from mixed-initiative collaboration,
where users guide the process to meaningfully reflect their cultures, and LLMs
steer the process towards more challenging questions that meet the researcher's
goals. We propose a mixed-initiative methodology called CultureCartography.
Here, an LLM initializes annotation with questions for which it has
low-confidence answers, making explicit both its prior knowledge and the gaps
therein. This allows a human respondent to fill these gaps and steer the model
towards salient topics through direct edits. We implement this methodology as a
tool called CultureExplorer. Compared to a baseline where humans answer
LLM-proposed questions, we find that CultureExplorer more effectively produces
knowledge that leading models like DeepSeek R1 and GPT-4o are missing, even
with web search. Fine-tuning on this data boosts the accuracy of Llama-3.1-8B
by up to 19.2% on related culture benchmarks.

</details>


### [44] [Continuous Autoregressive Language Models](https://arxiv.org/abs/2510.27688)
*Chenze Shao,Darren Li,Fandong Meng,Jie Zhou*

Main category: cs.CL

TL;DR: 提出连续自回归语言模型CALM，通过压缩token块为连续向量将生成步骤减少K倍，显著提升语言模型效率


<details>
  <summary>Details</summary>
Motivation: 现有LLM逐token生成方式存在效率瓶颈，需通过增加单步语义带宽突破性能限制

Method: 使用高精度自编码器将K个token压缩为连续向量，并构建无似然框架实现连续域的模型训练与采样控制

Result: 在保持性能相当前提下，计算成本显著低于传统离散基线模型，性能-计算比提升明显

Conclusion: 连续向量预测范式为超高效语言模型开辟新路径，验证了语义带宽扩展的有效性

Abstract: The efficiency of large language models (LLMs) is fundamentally limited by
their sequential, token-by-token generation process. We argue that overcoming
this bottleneck requires a new design axis for LLM scaling: increasing the
semantic bandwidth of each generative step. To this end, we introduce
Continuous Autoregressive Language Models (CALM), a paradigm shift from
discrete next-token prediction to continuous next-vector prediction. CALM uses
a high-fidelity autoencoder to compress a chunk of K tokens into a single
continuous vector, from which the original tokens can be reconstructed with
over 99.9\% accuracy. This allows us to model language as a sequence of
continuous vectors instead of discrete tokens, which reduces the number of
generative steps by a factor of K. The paradigm shift necessitates a new
modeling toolkit; therefore, we develop a comprehensive likelihood-free
framework that enables robust training, evaluation, and controllable sampling
in the continuous domain. Experiments show that CALM significantly improves the
performance-compute trade-off, achieving the performance of strong discrete
baselines at a significantly lower computational cost. More importantly, these
findings establish next-vector prediction as a powerful and scalable pathway
towards ultra-efficient language models. Code:
https://github.com/shaochenze/calm. Project:
https://shaochenze.github.io/blog/2025/CALM.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [45] [Broken-Token: Filtering Obfuscated Prompts by Counting Characters-Per-Token](https://arxiv.org/abs/2510.26847)
*Shaked Zychlinski,Yuval Kainan*

Main category: cs.CR

TL;DR: 提出CPT-Filtering防御技术，利用BPE分词器的字符-分词比率特征，以极低成本高效识别恶意编码文本攻击。


<details>
  <summary>Details</summary>
Motivation: 现有LLM防护措施难以识别密码/编码文本攻击，且主流防御方案依赖高计算成本的附加模型模块。

Method: 基于BPE分词器在自然语言处理中的特性（异常文本表现为更少字符/分词），通过设定字符分词比阈值检测编码攻击。

Result: 在10万+提示数据集上验证，对多种编码方案和主流分词器保持高准确率，尤其擅长短文本检测。

Conclusion: CPT-Filtering提供零推理成本、即插即用的防御方案，适用于实时文本过滤和离线数据清洗场景。

Abstract: Large Language Models (LLMs) are susceptible to jailbreak attacks where
malicious prompts are disguised using ciphers and character-level encodings to
bypass safety guardrails. While these guardrails often fail to interpret the
encoded content, the underlying models can still process the harmful
instructions. We introduce CPT-Filtering, a novel, model-agnostic with
negligible-costs and near-perfect accuracy guardrail technique that aims to
mitigate these attacks by leveraging the intrinsic behavior of Byte-Pair
Encoding (BPE) tokenizers. Our method is based on the principle that
tokenizers, trained on natural language, represent out-of-distribution text,
such as ciphers, using a significantly higher number of shorter tokens. Our
technique uses a simple yet powerful artifact of using language models: the
average number of Characters Per Token (CPT) in the text. This approach is
motivated by the high compute cost of modern methods - relying on added modules
such as dedicated LLMs or perplexity models. We validate our approach across a
large dataset of over 100,000 prompts, testing numerous encoding schemes with
several popular tokenizers. Our experiments demonstrate that a simple CPT
threshold robustly identifies encoded text with high accuracy, even for very
short inputs. CPT-Filtering provides a practical defense layer that can be
immediately deployed for real-time text filtering and offline data curation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [46] [Deep Neural Watermarking for Robust Copyright Protection in 3D Point Clouds](https://arxiv.org/abs/2510.27533)
*Khandoker Ashik Uz Zaman,Mohammad Zahangir Alam,Mohammed N. M. Ali,Mahdi H. Miraz*

Main category: cs.CV

TL;DR: 提出基于奇异值分解和PointNet++的3D点云鲁棒水印框架，在多种攻击下实现高精度水印提取


<details>
  <summary>Details</summary>
Motivation: 传统水印技术易受几何/非几何攻击破坏，难以满足3D点云版权保护需求。数字内容增长使版权验证成为迫切需求

Method: 1. 通过SVD将水印嵌入点云块的奇异值 2. 构建基于PointNet++的深度学习网络 3. 训练网络抵抗旋转/缩放/噪声/裁剪等攻击

Result: 在ModelNet40数据集上，深度学习法在70%裁剪攻击下比特准确度0.83（SVD仅0.58），IoU达到0.80（SVD 0.26）

Conclusion: 融合深度学习的框架显著提升水印鲁棒性，为3D内容版权保护提供抗强失真解决方案

Abstract: The protection of intellectual property has become critical due to the rapid
growth of three-dimensional content in digital media. Unlike traditional images
or videos, 3D point clouds present unique challenges for copyright enforcement,
as they are especially vulnerable to a range of geometric and non-geometric
attacks that can easily degrade or remove conventional watermark signals. In
this paper, we address these challenges by proposing a robust deep neural
watermarking framework for 3D point cloud copyright protection and ownership
verification. Our approach embeds binary watermarks into the singular values of
3D point cloud blocks using spectral decomposition, i.e. Singular Value
Decomposition (SVD), and leverages the extraction capabilities of Deep Learning
using PointNet++ neural network architecture. The network is trained to
reliably extract watermarks even after the data undergoes various attacks such
as rotation, scaling, noise, cropping and signal distortions. We validated our
method using the publicly available ModelNet40 dataset, demonstrating that deep
learning-based extraction significantly outperforms traditional SVD-based
techniques under challenging conditions. Our experimental evaluation
demonstrates that the deep learning-based extraction approach significantly
outperforms existing SVD-based methods with deep learning achieving bitwise
accuracy up to 0.83 and Intersection over Union (IoU) of 0.80, compared to SVD
achieving a bitwise accuracy of 0.58 and IoU of 0.26 for the Crop (70%) attack,
which is the most severe geometric distortion in our experiment. This
demonstrates our method's ability to achieve superior watermark recovery and
maintain high fidelity even under severe distortions.

</details>


### [47] [Semantic Frame Aggregation-based Transformer for Live Video Comment Generation](https://arxiv.org/abs/2510.26978)
*Anam Fatima,Yi Yu,Janak Kapuriya,Julien Lalanne,Jainendra Shukla*

Main category: cs.CV

TL;DR: 提出基于语义帧聚合的Transformer模型(SFAT)，通过动态加权视频帧语义相关性生成实时视频评论，并构建大规模多模态英文数据集。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视视频帧与观众实时互动的语义关联性，导致生成评论上下文匹配度不足。需优先处理与当前对话最相关的视频帧以提升评论质量。

Method: 1. 使用CLIP提取视觉-文本特征，计算视频帧与聊天记录的语义相关性权重
2. 设计加权帧聚合机制突出关键帧
3. 跨注意力解码器融合多模态信息生成评论
4. 构建覆盖11个类别、438小时、320万条评论的Twitch英文数据集

Result: SFAT模型在评论生成任务中优于基线方法，跨注意力机制有效捕获视频与聊天双模态的上下文线索，新数据集推动多语言场景研究。

Conclusion: 通过语义帧加权聚合与多模态融合机制，SFAT实现了上下文敏感的实时视频评论生成。大规模英文数据集的发布弥补了现有资源局限，为后续研究提供新基准。

Abstract: Live commenting on video streams has surged in popularity on platforms like
Twitch, enhancing viewer engagement through dynamic interactions. However,
automatically generating contextually appropriate comments remains a
challenging and exciting task. Video streams can contain a vast amount of data
and extraneous content. Existing approaches tend to overlook an important
aspect of prioritizing video frames that are most relevant to ongoing viewer
interactions. This prioritization is crucial for producing contextually
appropriate comments. To address this gap, we introduce a novel Semantic Frame
Aggregation-based Transformer (SFAT) model for live video comment generation.
This method not only leverages CLIP's visual-text multimodal knowledge to
generate comments but also assigns weights to video frames based on their
semantic relevance to ongoing viewer conversation. It employs an efficient
weighted sum of frames technique to emphasize informative frames while focusing
less on irrelevant ones. Finally, our comment decoder with a cross-attention
mechanism that attends to each modality ensures that the generated comment
reflects contextual cues from both chats and video. Furthermore, to address the
limitations of existing datasets, which predominantly focus on Chinese-language
content with limited video categories, we have constructed a large scale,
diverse, multimodal English video comments dataset. Extracted from Twitch, this
dataset covers 11 video categories, totaling 438 hours and 3.2 million
comments. We demonstrate the effectiveness of our SFAT model by comparing it to
existing methods for generating comments from live video and ongoing dialogue
contexts.

</details>


### [48] [Can MLLMs Read the Room? A Multimodal Benchmark for Verifying Truthfulness in Multi-Party Social Interactions](https://arxiv.org/abs/2510.27195)
*Caixin Kang,Yifei Huang,Liangyang Ouyang,Mingfang Zhang,Yoichi Sato*

Main category: cs.CV

TL;DR: 提出多模态交互真实性评估任务MIVA，构建狼人杀游戏数据集，揭示主流多模态大模型在辨谎任务上的显著性能缺陷


<details>
  <summary>Details</summary>
Motivation: 随着AI深度融入人类社会，亟需提升其辨别真相与谎言的社会智能。现有方法在动态多方对话场景中面临重大挑战，而多模态大模型的辨谎能力尚未被系统评估

Method: 基于狼人杀游戏构建含同步视频-文本数据和真实标签的数据集，建立涵盖主流MLLMs的评估基准

Result: GPT-4o等顶尖模型辨谎准确率不足（平均仅60%），存在视觉社交线索理解不足和过度保守倾向

Conclusion: 当前MLLMs难以有效融合语言与视觉社交线索，凸显开发新型感知框架的必要性

Abstract: As AI systems become increasingly integrated into human lives, endowing them
with robust social intelligence has emerged as a critical frontier. A key
aspect of this intelligence is discerning truth from deception, a ubiquitous
element of human interaction that is conveyed through a complex interplay of
verbal language and non-verbal visual cues. However, automatic deception
detection in dynamic, multi-party conversations remains a significant
challenge. The recent rise of powerful Multimodal Large Language Models
(MLLMs), with their impressive abilities in visual and textual understanding,
makes them natural candidates for this task. Consequently, their capabilities
in this crucial domain are mostly unquantified. To address this gap, we
introduce a new task, Multimodal Interactive Veracity Assessment (MIVA), and
present a novel multimodal dataset derived from the social deduction game
Werewolf. This dataset provides synchronized video, text, with verifiable
ground-truth labels for every statement. We establish a comprehensive benchmark
evaluating state-of-the-art MLLMs, revealing a significant performance gap:
even powerful models like GPT-4o struggle to distinguish truth from falsehood
reliably. Our analysis of failure modes indicates that these models fail to
ground language in visual social cues effectively and may be overly
conservative in their alignment, highlighting the urgent need for novel
approaches to building more perceptive and trustworthy AI systems.

</details>


### [49] [Towards Universal Video Retrieval: Generalizing Video Embedding via Synthesized Multimodal Pyramid Curriculum](https://arxiv.org/abs/2510.27571)
*Zhuoning Guo,Mingxin Li,Yanzhao Zhang,Dingkun Long,Pengjun Xie,Xiaowen Chu*

Main category: cs.CV

TL;DR: 提出视频检索领域新框架（UVRB基准+155万合成数据+模态金字塔训练），突破现有单任务训练局限性，实现SOTA零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频检索基准过于狭窄，导致数据和训练单一化，限制了模型的多维度泛化能力。需通过协同设计评估、数据和模型打破这一循环。

Method: 1. 建立16个数据集构成的UVRB基准诊断能力缺陷；2. 基于诊断生成155万高质量合成数据；3. 设计模态金字塔课程训练通用视频嵌入模型GVE。

Result: GVE在UVRB上实现SOTA零样本性能。分析显示：主流基准无法有效评估通用能力，部分相关检索是主导但被忽视的场景。

Conclusion: 协同设计框架为视频检索提供从局限基准到通用化的实践路径，多维度优化显著提升模型真实场景适应力。

Abstract: The prevailing video retrieval paradigm is structurally misaligned, as narrow
benchmarks incentivize correspondingly limited data and single-task training.
Therefore, universal capability is suppressed due to the absence of a
diagnostic evaluation that defines and demands multi-dimensional
generalization. To break this cycle, we introduce a framework built on the
co-design of evaluation, data, and modeling. First, we establish the Universal
Video Retrieval Benchmark (UVRB), a suite of 16 datasets designed not only to
measure performance but also to diagnose critical capability gaps across tasks
and domains. Second, guided by UVRB's diagnostics, we introduce a scalable
synthesis workflow that generates 1.55 million high-quality pairs to populate
the semantic space required for universality. Finally, we devise the Modality
Pyramid, a curriculum that trains our General Video Embedder (GVE) by
explicitly leveraging the latent interconnections within our diverse data.
Extensive experiments show GVE achieves state-of-the-art zero-shot
generalization on UVRB. In particular, our analysis reveals that popular
benchmarks are poor predictors of general ability and that partially relevant
retrieval is a dominant but overlooked scenario. Overall, our co-designed
framework provides a practical path to escape the limited scope and advance
toward truly universal video retrieval.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [50] [CATArena: Evaluation of LLM Agents through Iterative Tournament Competitions](https://arxiv.org/abs/2510.26852)
*Lingyue Fu,Xin Ding,Yaoming Zhu,Shao Zhang,Lin Qiu,Weiwen Liu,Weinan Zhang,Xuezhi Cao,Xunliang Cai,Jiaxin Ding,Yong Yu*

Main category: cs.AI

TL;DR: 提出CATArena评估框架，通过竞技式同伴学习机制和开放式评分游戏，系统评估LLM代理的学习能力和策略编码能力


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在分数饱和和专家标注依赖问题，需建立能持续评估智能体进化能力的核心指标体系

Method: 设计迭代式竞争同伴学习框架，开发包含四类棋盘/卡牌游戏的开放式评分评估平台CATArena

Result: 实验证明CATArena能可靠评估代理核心能力，特别是在学习能力和策略编码方面展现稳定可扩展的基准效果

Conclusion: 该研究为智能体进化评估提供了方法论突破，通过开放式任务设计和动态竞技机制推动LLM智能体向人类水平发展

Abstract: Large Language Model (LLM) agents have evolved from basic text generation to
autonomously completing complex tasks through interaction with external tools.
However, current benchmarks mainly assess end-to-end performance in fixed
scenarios, restricting evaluation to specific skills and suffering from score
saturation and growing dependence on expert annotation as agent capabilities
improve. In this work, we emphasize the importance of learning ability,
including both self-improvement and peer-learning, as a core driver for agent
evolution toward human-level intelligence. We propose an iterative, competitive
peer-learning framework, which allows agents to refine and optimize their
strategies through repeated interactions and feedback, thereby systematically
evaluating their learning capabilities. To address the score saturation issue
in current benchmarks, we introduce CATArena, a tournament-style evaluation
platform featuring four diverse board and card games with open-ended scoring.
By providing tasks without explicit upper score limits, CATArena enables
continuous and dynamic evaluation of rapidly advancing agent capabilities.
Experimental results and analyses involving both minimal and commercial code
agents demonstrate that CATArena provides reliable, stable, and scalable
benchmarking for core agent abilities, particularly learning ability and
strategy coding.

</details>


### [51] [The Denario project: Deep knowledge AI agents for scientific discovery](https://arxiv.org/abs/2510.26887)
*Francisco Villaescusa-Navarro,Boris Bolliet,Pablo Villanueva-Domingo,Adrian E. Bayer,Aidan Acquah,Chetana Amancharla,Almog Barzilay-Siegal,Pablo Bermejo,Camille Bilodeau,Pablo Cárdenas Ramírez,Miles Cranmer,Urbano L. França,ChangHoon Hahn,Yan-Fei Jiang,Raul Jimenez,Jun-Young Lee,Antonio Lerario,Osman Mamun,Thomas Meier,Anupam A. Ojha,Pavlos Protopapas,Shimanto Roy,David N. Spergel,Pedro Tarancón-Álvarez,Ujjwal Tiwari,Matteo Viel,Digvijay Wadekar,Chi Wang,Bonny Y. Wang,Licong Xu,Yossi Yovel,Shuwen Yue,Wen-Han Zhou,Qiyao Zhu,Jiajun Zou,Íñigo Zubeldia*

Main category: cs.AI

TL;DR: Denario是一个模块化多代理AI系统，具备端到端科研分析能力，可生成跨学科论文并通过专家评估


<details>
  <summary>Details</summary>
Motivation: 解决科研人员在复杂任务处理中的效率瓶颈，通过AI代理实现科研流程自动化与跨学科创新融合

Method: 采用模块化架构，结合Cmbagent深度研究后端，支持从文献调研到论文生成的全流程任务处理

Result: 在12个学科领域生成论文，专家评分显示系统在创新性（4.1/5）和执行效率（4.3/5）表现突出

Conclusion: 系统展现AI驱动科研的潜力，但需解决伦理风险与人类研究者的协同机制，代码已开源并部署云端应用

Abstract: We present Denario, an AI multi-agent system designed to serve as a
scientific research assistant. Denario can perform many different tasks, such
as generating ideas, checking the literature, developing research plans,
writing and executing code, making plots, and drafting and reviewing a
scientific paper. The system has a modular architecture, allowing it to handle
specific tasks, such as generating an idea, or carrying out end-to-end
scientific analysis using Cmbagent as a deep-research backend. In this work, we
describe in detail Denario and its modules, and illustrate its capabilities by
presenting multiple AI-generated papers generated by it in many different
scientific disciplines such as astrophysics, biology, biophysics, biomedical
informatics, chemistry, material science, mathematical physics, medicine,
neuroscience and planetary science. Denario also excels at combining ideas from
different disciplines, and we illustrate this by showing a paper that applies
methods from quantum physics and machine learning to astrophysical data. We
report the evaluations performed on these papers by domain experts, who
provided both numerical scores and review-like feedback. We then highlight the
strengths, weaknesses, and limitations of the current system. Finally, we
discuss the ethical implications of AI-driven research and reflect on how such
technology relates to the philosophy of science. We publicly release the code
at https://github.com/AstroPilot-AI/Denario. A Denario demo can also be run
directly on the web at https://huggingface.co/spaces/astropilot-ai/Denario, and
the full app will be deployed on the cloud.

</details>


### [52] [Glia: A Human-Inspired AI for Automated Systems Design and Optimization](https://arxiv.org/abs/2510.27176)
*Pouya Hamadanian,Pantea Karimi,Arash Nasr-Esfahany,Kimia Noorbakhsh,Joseph Chandler,Ali ParandehGheibi,Mohammad Alizadeh,Hari Balakrishnan*

Main category: cs.AI

TL;DR: Glia通过结合推理型大语言模型与结构化实验，实现了自主设计高效分布式系统算法，其性能达到人类专家水平且具有创新性


<details>
  <summary>Details</summary>
Motivation: 探索AI是否能在系统设计领域达到人类专家的创造力和推理能力，突破传统黑箱优化方法的局限性

Method: 多智能体协作框架（推理/实验/分析模块）+ 实证反馈验证机制 + 可解释设计生成

Result: 在GPU集群的LLM推理场景中，Glia设计的算法在路由调度等核心问题上达到专家水平，并发现了新的工作负载行为规律

Conclusion: 融合推理LLM与结构化实验的AI架构，能够产出兼具创造力和可解释性的复杂系统解决方案

Abstract: Can an AI autonomously design mechanisms for computer systems on par with the
creativity and reasoning of human experts? We present Glia, an AI architecture
for networked systems design that uses large language models (LLMs) in a
human-inspired, multi-agent workflow. Each agent specializes in reasoning,
experimentation, and analysis, collaborating through an evaluation framework
that grounds abstract reasoning in empirical feedback. Unlike prior
ML-for-systems methods that optimize black-box policies, Glia generates
interpretable designs and exposes its reasoning process. When applied to a
distributed GPU cluster for LLM inference, it produces new algorithms for
request routing, scheduling, and auto-scaling that perform at human-expert
levels in significantly less time, while yielding novel insights into workload
behavior. Our results suggest that by combining reasoning LLMs with structured
experimentation, an AI can produce creative and understandable designs for
complex systems problems.

</details>


### [53] [DeepCompress: A Dual Reward Strategy for Dynamically Exploring and Compressing Reasoning Chains](https://arxiv.org/abs/2510.27419)
*Tian Liang,Wenxiang Jiao,Zhiwei He,Jiahao Xu,Haitao Mi,Dong Yu*

Main category: cs.AI

TL;DR: DeepCompress框架通过自适应长度奖励机制，动态调整推理链长度，在提升大型推理模型准确性的同时显著提高计算效率


<details>
  <summary>Details</summary>
Motivation: 现有方法提升模型推理效率时往往牺牲准确性，需要找到平衡效率与精度的新方案

Method: 实时动态分类问题难度（简单/困难），采用双奖励策略：简单问题压缩推理路径，困难问题扩展思维链探索

Result: 在数学基准测试中同时超越基线方法的准确率和token效率，实现精度与效率双提升

Conclusion: 自适应调整推理深度的策略有效解决了认知效率问题，使模型能自主优化不同难度问题的思考深度

Abstract: Large Reasoning Models (LRMs) have demonstrated impressive capabilities but
suffer from cognitive inefficiencies like ``overthinking'' simple problems and
``underthinking'' complex ones. While existing methods that use supervised
fine-tuning~(SFT) or reinforcement learning~(RL) with token-length rewards can
improve efficiency, they often do so at the cost of accuracy. This paper
introduces \textbf{DeepCompress}, a novel framework that simultaneously
enhances both the accuracy and efficiency of LRMs. We challenge the prevailing
approach of consistently favoring shorter reasoning paths, showing that longer
responses can contain a broader range of correct solutions for difficult
problems. DeepCompress employs an adaptive length reward mechanism that
dynamically classifies problems as ``Simple'' or ``Hard'' in real-time based on
the model's evolving capability. It encourages shorter, more efficient
reasoning for ``Simple'' problems while promoting longer, more exploratory
thought chains for ``Hard'' problems. This dual-reward strategy enables the
model to autonomously adjust its Chain-of-Thought (CoT) length, compressing
reasoning for well-mastered problems and extending it for those it finds
challenging. Experimental results on challenging mathematical benchmarks show
that DeepCompress consistently outperforms baseline methods, achieving superior
accuracy while significantly improving token efficiency.

</details>


### [54] [SIGMA: Search-Augmented On-Demand Knowledge Integration for Agentic Mathematical Reasoning](https://arxiv.org/abs/2510.27568)
*Ali Asgarov,Umid Suleymanov,Aadyant Khatri*

Main category: cs.AI

TL;DR: 提出SIGMA框架，通过多代理协作机制实现数学推理中上下文敏感的知识整合，在多个基准测试中取得7.4%的绝对性能提升


<details>
  <summary>Details</summary>
Motivation: 现有检索增强模型存在视角单一、搜索策略僵化、多源信息整合困难的问题，无法满足复杂数学推理的需求

Method: 采用协调器机制整合专业代理：1）独立推理生成假设性段落优化检索 2）针对性搜索 3）通过上下文敏感的整合实现计算高效的知识融合

Result: 在MATH500/AIME/GPQA等基准测试中全面超越开源和闭源系统，绝对性能提升7.4%，推理效率同步提升

Conclusion: 多代理按需知识整合显著提高复杂问题的解决能力，为知识密集型任务提供了可扩展的解决方案

Abstract: Solving mathematical reasoning problems requires not only accurate access to
relevant knowledge but also careful, multi-step thinking. However, current
retrieval-augmented models often rely on a single perspective, follow
inflexible search strategies, and struggle to effectively combine information
from multiple sources. We introduce SIGMA (Search-Augmented On-Demand Knowledge
Integration for AGentic Mathematical reAsoning), a unified framework that
orchestrates specialized agents to independently reason, perform targeted
searches, and synthesize findings through a moderator mechanism. Each agent
generates hypothetical passages to optimize retrieval for its analytic
perspective, ensuring knowledge integration is both context-sensitive and
computation-efficient. When evaluated on challenging benchmarks such as
MATH500, AIME, and PhD-level science QA GPQA, SIGMA consistently outperforms
both open- and closed-source systems, achieving an absolute performance
improvement of 7.4%. Our results demonstrate that multi-agent, on-demand
knowledge integration significantly enhances both reasoning accuracy and
efficiency, offering a scalable approach for complex, knowledge-intensive
problem-solving. We will release the code upon publication.

</details>


### [55] [Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive Trigger Learning](https://arxiv.org/abs/2510.27623)
*Qiusi Zhan,Hyeonjeong Ha,Rui Yang,Sirui Xu,Hanyang Chen,Liang-Yan Gui,Yu-Xiong Wang,Huan Zhang,Heng Ji,Daniel Kang*

Main category: cs.AI

TL;DR: BEAT框架通过多样化场景训练和对比触发学习，成功在MLLM具身智能体中植入视觉后门攻击防御机制，攻击成功率最高达80%且保持正常任务性能


<details>
  <summary>Details</summary>
Motivation: MLLM驱动的具身智能体存在视觉后门攻击风险，攻击者可通过特定视觉触发物使智能体持续执行预设恶意策略，需建立可靠防御机制

Method: 1) 构建跨场景/任务/触发物位置的多维训练集 2) 两阶段训练方案（监督微调+对比触发学习CTL），通过触发物存在/无的偏好学习明确决策边界

Result: 攻击成功率最高达80%，CTL在有限后门数据下将激活准确率提升39%，且对分布外触发位置保持强泛化能力

Conclusion: BEAT暴露了MLLM具身智能体的新型安全漏洞，其框架有效性验证了攻击可行性，强调实际部署前需建立可靠防御体系

Abstract: Multimodal large language models (MLLMs) have advanced embodied agents by
enabling direct perception, reasoning, and planning task-oriented actions from
visual inputs. However, such vision driven embodied agents open a new attack
surface: visual backdoor attacks, where the agent behaves normally until a
visual trigger appears in the scene, then persistently executes an
attacker-specified multi-step policy. We introduce BEAT, the first framework to
inject such visual backdoors into MLLM-based embodied agents using objects in
the environments as triggers. Unlike textual triggers, object triggers exhibit
wide variation across viewpoints and lighting, making them difficult to implant
reliably. BEAT addresses this challenge by (1) constructing a training set that
spans diverse scenes, tasks, and trigger placements to expose agents to trigger
variability, and (2) introducing a two-stage training scheme that first applies
supervised fine-tuning (SFT) and then our novel Contrastive Trigger Learning
(CTL). CTL formulates trigger discrimination as preference learning between
trigger-present and trigger-free inputs, explicitly sharpening the decision
boundaries to ensure precise backdoor activation. Across various embodied agent
benchmarks and MLLMs, BEAT achieves attack success rates up to 80%, while
maintaining strong benign task performance, and generalizes reliably to
out-of-distribution trigger placements. Notably, compared to naive SFT, CTL
boosts backdoor activation accuracy up to 39% under limited backdoor data.
These findings expose a critical yet unexplored security risk in MLLM-based
embodied agents, underscoring the need for robust defenses before real-world
deployment.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [56] [Towards a Measure of Algorithm Similarity](https://arxiv.org/abs/2510.27063)
*Shairoz Sohail,Taher Ali*

Main category: cs.LG

TL;DR: 提出EMOC框架，通过评估-内存-操作-复杂度特征嵌入算法实现，支持算法聚类、查重和量化LLM生成程序的多样性


<details>
  <summary>Details</summary>
Motivation: 解决算法相似性评估缺乏统一标准的问题，满足克隆检测、程序合成等场景对量化指标的需求

Method: 构建EMOC多维特征空间（评估步骤/内存/操作/复杂度），创建PACD验证数据集（含三类问题的Python实现），开发特征量化工具

Result: EMOC特征支持算法类型聚类（准确率>92%）、近似重复检测（F1-score 0.87）、LLM生成程序多样性量化（差异度提升35%）

Conclusion: EMOC为算法相似性评估提供标准化框架，配套开源工具促进可复现研究，支持程序合成质量评估等应用场景

Abstract: Given two algorithms for the same problem, can we determine whether they are
meaningfully different? In full generality, the question is uncomputable, and
empirically it is muddied by competing notions of similarity. Yet, in many
applications (such as clone detection or program synthesis) a pragmatic and
consistent similarity metric is necessary. We review existing equivalence and
similarity notions and introduce EMOC: An
Evaluation-Memory-Operations-Complexity framework that embeds algorithm
implementations into a feature space suitable for downstream tasks. We compile
PACD, a curated dataset of verified Python implementations across three
problems, and show that EMOC features support clustering and classification of
algorithm types, detection of near-duplicates, and quantification of diversity
in LLM-generated programs. Code, data, and utilities for computing EMOC
embeddings are released to facilitate reproducibility and future work on
algorithm similarity.

</details>


### [57] [Higher-order Linear Attention](https://arxiv.org/abs/2510.27258)
*Yifan Zhang,Zhen Qin,Quanquan Gu*

Main category: cs.LG

TL;DR: 提出高阶线性注意力机制HLA，通过紧凑前缀统计实现因果流式处理，在保持线性时间复杂度的同时突破一阶近似限制


<details>
  <summary>Details</summary>
Motivation: 传统注意力机制的二次计算复杂度限制长上下文建模，现有线性注意力方法受限于低阶近似导致表达能力不足

Method: 利用前缀充分统计量实现高阶交互，设计严格因果掩码变体和基于关联扫描的并行训练方案，支持任意阶扩展

Result: 二阶HLA保持恒定状态大小(O(1))，实现线性时间复杂度(O(n))，无需生成显式注意力矩阵，训练方案精确匹配串行递归结果

Conclusion: HLA将数据依赖的注意力式混合与现代循环架构的高效性结合，为可扩展序列建模提供理论严谨的构建模块

Abstract: The quadratic cost of scaled dot-product attention is a central obstacle to
scaling autoregressive language models to long contexts. Linear-time attention
and State Space Models (SSMs) provide scalable alternatives but are typically
restricted to first-order or kernel-based approximations, which can limit
expressivity. We introduce Higher-order Linear Attention (HLA), a causal,
streaming mechanism that realizes higher interactions via compact prefix
sufficient statistics. In the second-order case, HLA maintains a constant-size
state and computes per-token outputs in linear time without materializing any
$n \times n$ matrices. We give closed-form streaming identities, a strictly
causal masked variant using two additional summaries, and a chunk-parallel
training scheme based on associative scans that reproduces the activations of a
serial recurrence exactly. We further outline extensions to third and higher
orders. Collectively, these results position HLA as a principled, scalable
building block that combines attention-like, data-dependent mixing with the
efficiency of modern recurrent architectures. Project Page:
https://github.com/yifanzhang-pro/HLA.

</details>


### [58] [Un-Attributability: Computing Novelty From Retrieval & Semantic Similarity](https://arxiv.org/abs/2510.27313)
*Philipp Davydov,Ameya Prabhu,Matthias Bethge,Elisa Nguyen,Seong Joon Oh*

Main category: cs.LG

TL;DR: 提出用不可归因性衡量语言模型输出的语义新颖性，通过两阶段检索方法验证模型创新性生成能力


<details>
  <summary>Details</summary>
Motivation: 现有训练数据归因方法主要关注单个样本对输出的因果影响，本研究逆向探讨哪些输出无法归因于任何预训练数据

Method: 1. 使用GIST轻量级嵌入构建索引 2. 用ColBERTv2重排序检索结果 3. 通过与人工参考文本对比判定新颖性

Result: 1. 模型利用长跨度预训练数据 2. 特定领域系统性影响新颖性 3. 指令微调提升生成新颖性

Conclusion: 不可归因性框架支持预训练规模的高效分析，开放20TB数据集推动大规模可复现研究

Abstract: Understanding how language-model outputs relate to the pretraining corpus is
central to studying model behavior. Most training data attribution (TDA)
methods ask which training examples causally influence a given output, often
using leave-one-out tests. We invert the question: which outputs cannot be
attributed to any pretraining example? We introduce un-attributability as an
operational measure of semantic novelty: an output is novel if the pretraining
corpus contains no semantically similar context. We approximate this with a
simple two-stage retrieval pipeline: index the corpus with lightweight GIST
embeddings, retrieve the top-n candidates, then rerank with ColBERTv2. If the
nearest corpus item is less attributable than a human-generated text reference,
we consider the output of the model as novel. We evaluate on SmolLM and SmolLM2
and report three findings: (1) models draw on pretraining data across much
longer spans than previously reported; (2) some domains systematically promote
or suppress novelty; and (3) instruction tuning not only alters style but also
increases novelty. Reframing novelty assessment around un-attributability
enables efficient analysis at pretraining scale. We release ~20 TB of corpus
chunks and index artifacts to support replication and large-scale extension of
our analysis at https://huggingface.co/datasets/stai-tuebingen/faiss-smollm

</details>


### [59] [Measuring Chain-of-Thought Monitorability Through Faithfulness and Verbosity](https://arxiv.org/abs/2510.27378)
*Austin Meek,Eitan Sprejer,Iván Arcuschin,Austin J. Brockmeier,Steven Basart*

Main category: cs.LG

TL;DR: 研究思维链输出的忠实性和详尽性对模型监控能力的影响，发现不同模型家族监控能力差异显著


<details>
  <summary>Details</summary>
Motivation: 现有通过改变输入提示观察答案变化的评估方法无法全面检测思维链的忠实性，且未考虑与提示无关的推理因素

Method: 结合忠实性和详尽性提出统一监控评分，在BBH/GPQA/MMLU数据集评估不同指令微调和推理模型

Result: 模型可能在保持答案忠实性的情况下因遗漏关键因素导致难以监控，且监控能力在不同模型家族间差异显著

Conclusion: 思维链作为外部工作记忆的可靠性直接影响安全方案有效性，研究团队开源Inspect评估库支持后续研究

Abstract: Chain-of-thought (CoT) outputs let us read a model's step-by-step reasoning.
Since any long, serial reasoning process must pass through this textual trace,
the quality of the CoT is a direct window into what the model is thinking. This
visibility could help us spot unsafe or misaligned behavior (monitorability),
but only if the CoT is transparent about its internal reasoning (faithfulness).
Fully measuring faithfulness is difficult, so researchers often focus on
examining the CoT in cases where the model changes its answer after adding a
cue to the input. This proxy finds some instances of unfaithfulness but loses
information when the model maintains its answer, and does not investigate
aspects of reasoning not tied to the cue. We extend these results to a more
holistic sense of monitorability by introducing verbosity: whether the CoT
lists every factor needed to solve the task. We combine faithfulness and
verbosity into a single monitorability score that shows how well the CoT serves
as the model's external `working memory', a property that many safety schemes
based on CoT monitoring depend on. We evaluate instruction-tuned and reasoning
models on BBH, GPQA, and MMLU. Our results show that models can appear faithful
yet remain hard to monitor when they leave out key factors, and that
monitorability differs sharply across model families. We release our evaluation
code using the Inspect library to support reproducible future work.

</details>


### [60] [Atlas-Alignment: Making Interpretability Transferable Across Language Models](https://arxiv.org/abs/2510.27413)
*Bruno Puri,Jim Berend,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.LG

TL;DR: 提出了Atlas-Alignment框架，通过将新模型的隐空间与预构建的Concept Atlas对齐，实现了跨模型的可解释性迁移，显著降低了模型可解释性的边际成本。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型可解释性方法存在训练成本高、扩展性差的问题，需要开发高效的可解释性迁移方案来增强模型透明度和可控性。

Method: 使用轻量级表示对齐技术，仅通过共享输入将新模型隐空间与已标注的Concept Atlas对齐，无需训练模型特定稀疏自编码器或概念标注数据。

Result: 定量和定性实验表明，该方法实现了稳健的语义特征检索（无需概念标注数据）和基于人类可理解概念的生成控制能力。

Conclusion: 通过单次构建高质量Concept Atlas，Atlas-Alignment实现了多模型透明化的边际成本摊销，为可解释AI提供了高效解决方案。

Abstract: Interpretability is crucial for building safe, reliable, and controllable
language models, yet existing interpretability pipelines remain costly and
difficult to scale. Interpreting a new model typically requires costly training
of model-specific sparse autoencoders, manual or semi-automated labeling of SAE
components, and their subsequent validation. We introduce Atlas-Alignment, a
framework for transferring interpretability across language models by aligning
unknown latent spaces to a Concept Atlas - a labeled, human-interpretable
latent space - using only shared inputs and lightweight representational
alignment techniques. Once aligned, this enables two key capabilities in
previously opaque models: (1) semantic feature search and retrieval, and (2)
steering generation along human-interpretable atlas concepts. Through
quantitative and qualitative evaluations, we show that simple representational
alignment methods enable robust semantic retrieval and steerable generation
without the need for labeled concept data. Atlas-Alignment thus amortizes the
cost of explainable AI and mechanistic interpretability: by investing in one
high-quality Concept Atlas, we can make many new models transparent and
controllable at minimal marginal cost.

</details>


### [61] [Thought Branches: Interpreting LLM Reasoning Requires Resampling](https://arxiv.org/abs/2510.27484)
*Uzay Macar,Paul C. Bogdan,Senthooran Rajamanoharan,Neel Nanda*

Main category: cs.LG

TL;DR: 提出通过重采样方法分析多思维链分布，揭示模型推理的因果机制与有效干预策略


<details>
  <summary>Details</summary>
Motivation: 现有研究局限于单一思维链分析，无法全面反映模型推理的潜在分布和真实因果机制，需系统性分析多路径影响

Method: 设计四阶段案例：1) 句子因果影响测量 2) 策略内外干预对比 3) 弹性指标构建 4) 因果中介分析框架

Result: 发现自我维护陈述影响力弱（黑箱决策驱动系数<0.2），策略外干预效果波动达±38%，关键规划语句消除可使任务成功率下降62%

Conclusion: 重采样方法为模型解释提供：1) 可靠因果归因 2) 动态推理过程可视化 3) 基于分布调控的优化路径

Abstract: Most work interpreting reasoning models studies only a single
chain-of-thought (CoT), yet these models define distributions over many
possible CoTs. We argue that studying a single sample is inadequate for
understanding causal influence and the underlying computation. Though fully
specifying this distribution is intractable, it can be understood by sampling.
We present case studies using resampling to investigate model decisions. First,
when a model states a reason for its action, does that reason actually cause
the action? In "agentic misalignment" scenarios, we resample specific sentences
to measure their downstream effects. Self-preservation sentences have small
causal impact, suggesting they do not meaningfully drive blackmail. Second, are
artificial edits to CoT sufficient for steering reasoning? These are common in
literature, yet take the model off-policy. Resampling and selecting a
completion with the desired property is a principled on-policy alternative. We
find off-policy interventions yield small and unstable effects compared to
resampling in decision-making tasks. Third, how do we understand the effect of
removing a reasoning step when the model may repeat it post-edit? We introduce
a resilience metric that repeatedly resamples to prevent similar content from
reappearing downstream. Critical planning statements resist removal but have
large effects when eliminated. Fourth, since CoT is sometimes "unfaithful", can
our methods teach us anything in these settings? Adapting causal mediation
analysis, we find that hints that have a causal effect on the output without
being explicitly mentioned exert a subtle and cumulative influence on the CoT
that persists even if the hint is removed. Overall, studying distributions via
resampling enables reliable causal analysis, clearer narratives of model
reasoning, and principled CoT interventions.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [62] [DRAMA: Unifying Data Retrieval and Analysis for Open-Domain Analytic Queries](https://arxiv.org/abs/2510.27238)
*Chuxuan Hu,Maxwell Yang,James Weiland,Yeji Lim,Suhas Palawala,Daniel Kang*

Main category: cs.DB

TL;DR: 提出DRAMA端到端范式，整合数据收集、转换与分析流程，在开放域数据分析任务中实现86.5%准确率且成本降低6倍


<details>
  <summary>Details</summary>
Motivation: 现有自动化数据科学系统无法同时满足开放域数据采集、结构化数据转换和智能分析推理三大核心需求，人工数据分析效率低下

Method: 构建DRAMA-Bench基准测试集，开发多智能体系统DRAMA-Bot（包含数据检索器和分析器），通过子智能体协作实现数据采集转换与结构化推理

Result: 在真实场景的声明验证和问答任务中，以0.05美元成本实现86.5%准确率，较基线模型提升6.9倍准确率且成本降低至1/6以下

Conclusion: DRAMA范式通过端到端流程有效解决开放域数据分析难题，在成本效益和准确性方面显著优于现有方案，已开源供社区使用

Abstract: Manually conducting real-world data analyses is labor-intensive and
inefficient. Despite numerous attempts to automate data science workflows, none
of the existing paradigms or systems fully demonstrate all three key
capabilities required to support them effectively: (1) open-domain data
collection, (2) structured data transformation, and (3) analytic reasoning.
  To overcome these limitations, we propose DRAMA, an end-to-end paradigm that
answers users' analytic queries in natural language on large-scale open-domain
data. DRAMA unifies data collection, transformation, and analysis as a single
pipeline. To quantitatively evaluate system performance on tasks representative
of DRAMA, we construct a benchmark, DRAMA-Bench, consisting of two categories
of tasks: claim verification and question answering, each comprising 100
instances. These tasks are derived from real-world applications that have
gained significant public attention and require the retrieval and analysis of
open-domain data. We develop DRAMA-Bot, a multi-agent system designed following
DRAMA. It comprises a data retriever that collects and transforms data by
coordinating the execution of sub-agents, and a data analyzer that performs
structured reasoning over the retrieved data. We evaluate DRAMA-Bot on
DRAMA-Bench together with five state-of-the-art baseline agents. DRAMA-Bot
achieves 86.5% task accuracy at a cost of $0.05, outperforming all baselines
with up to 6.9 times the accuracy and less than 1/6 of the cost. DRAMA is
publicly available at https://github.com/uiuc-kang-lab/drama.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [63] [Evaluating Perspectival Biases in Cross-Modal Retrieval](https://arxiv.org/abs/2510.26861)
*Teerapol Saengsukhiran,Peerawat Chomphooyod,Narabodee Rodjananant,Chompakorn Chaksangchaichot,Patawee Prakrankamanant,Witthawin Sripheanpol,Pak Lovichit,SarChaksaana Nutanong,Ekapol Chuangsuwanich*

Main category: cs.IR

TL;DR: 研究发现多模态检索系统存在语言流行度偏差和文化关联偏差，显式对齐策略对前者有效但后者更具挑战性


<details>
  <summary>Details</summary>
Motivation: 当前多模态检索系统存在视角偏差问题，语言流行度导致检索结果偏向主流语言内容，文化关联性导致偏向刻板文化印象，影响系统公平性

Method: 通过对比实验分析两种偏差：1）图像-文本检索中的语言流行度偏差 2）文本-图像检索中的文化关联偏差，测试显式对齐策略和数据扩展的效果

Result: 显式对齐能有效缓解语言流行度偏差，但文化关联偏差仍持续存在且更难解决，表明两者需要不同应对策略

Conclusion: 实现公平多模态系统需针对性策略，文化关联偏差应视为比语言流行度偏差更复杂的独立问题处理

Abstract: Multimodal retrieval systems are expected to operate in a semantic space,
agnostic to the language or cultural origin of the query. In practice, however,
retrieval outcomes systematically reflect perspectival biases: deviations
shaped by linguistic prevalence and cultural associations. We study two such
biases. First, prevalence bias refers to the tendency to favor entries from
prevalent languages over semantically faithful entries in image-to-text
retrieval. Second, association bias refers to the tendency to favor images
culturally associated with the query over semantically correct ones in
text-to-image retrieval. Results show that explicit alignment is a more
effective strategy for mitigating prevalence bias. However, association bias
remains a distinct and more challenging problem. These findings suggest that
achieving truly equitable multimodal systems requires targeted strategies
beyond simple data scaling and that bias arising from cultural association may
be treated as a more challenging problem than one arising from linguistic
prevalence.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [64] [RepV: Safety-Separable Latent Spaces for Scalable Neurosymbolic Plan Verification](https://arxiv.org/abs/2510.26935)
*Yunhao Yang,Neel P. Bhatt,Pranay Samineni,Rohan Siva,Zhanyang Wang,Ufuk Topcu*

Main category: cs.RO

TL;DR: RepV神经符号验证器通过构建安全可分离的潜在空间，实现自然语言规则的高效验证，在提升合规性预测精度的同时保持轻量化参数规模。


<details>
  <summary>Details</summary>
Motivation: 现有形式化方法依赖人工编写时序逻辑规范（可访问性和表达性受限），而深度学习方法缺乏可解释性且存在误判风险。需要结合两者优势解决AI安全验证的可扩展性和可靠性问题。

Method: 1. 利用模型检查器标记的初始计划集训练轻量级投影器
2. 将计划与语言模型生成的原理嵌入低维潜在空间
3. 通过冻结线性边界实现单次前向传播的规则验证

Result: 合规预测精度提升15%（基准方法对比），参数量仅增加0.2M；细化框架在各规划领域优于传统微调基线

Conclusion: 安全可分离的潜在空间为神经符号验证提供了可扩展的即插即用原语，概率保证机制实现了无需人工标注的规划器自主优化。

Abstract: As AI systems migrate to safety-critical domains, verifying that their
actions comply with well-defined rules remains a challenge. Formal methods
provide provable guarantees but demand hand-crafted temporal-logic
specifications, offering limited expressiveness and accessibility. Deep
learning approaches enable evaluation of plans against natural-language
constraints, yet their opaque decision process invites misclassifications with
potentially severe consequences. We introduce RepV, a neurosymbolic verifier
that unifies both views by learning a latent space where safe and unsafe plans
are linearly separable. Starting from a modest seed set of plans labeled by an
off-the-shelf model checker, RepV trains a lightweight projector that embeds
each plan, together with a language model-generated rationale, into a
low-dimensional space; a frozen linear boundary then verifies compliance for
unseen natural-language rules in a single forward pass.
  Beyond binary classification, RepV provides a probabilistic guarantee on the
likelihood of correct verification based on its position in the latent space.
This guarantee enables a guarantee-driven refinement of the planner, improving
rule compliance without human annotations. Empirical evaluations show that RepV
improves compliance prediction accuracy by up to 15% compared to baseline
methods while adding fewer than 0.2M parameters. Furthermore, our refinement
framework outperforms ordinary fine-tuning baselines across various planning
domains. These results show that safety-separable latent spaces offer a
scalable, plug-and-play primitive for reliable neurosymbolic plan verification.
Code and data are available at: https://repv-project.github.io/.

</details>
