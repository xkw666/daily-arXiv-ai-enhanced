<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 101]
- [cs.GR](#cs.GR) [Total: 4]
- [physics.soc-ph](#physics.soc-ph) [Total: 2]
- [cs.LG](#cs.LG) [Total: 14]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.CV](#cs.CV) [Total: 14]
- [cs.SD](#cs.SD) [Total: 3]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models](https://arxiv.org/abs/2505.21523)
*Chengzhi Liu,Zhongxing Xu,Qingyue Wei,Juncheng Wu,James Zou,Xin Eric Wang,Yuyin Zhou,Sheng Liu*

Main category: cs.CL

TL;DR: 提出RH-AUC指标和RH-Bench基准，揭示大模型在推理能力与视觉感知间更平衡，数据质量比数量更重要


<details>
  <summary>Details</summary>
Motivation: 研究多模态大模型在增强推理能力时伴随的视觉幻觉增加问题，需要系统评估推理长度与感知精度的关系

Method: 通过注意力机制分析，提出RH-AUC量化指标，构建跨任务诊断基准RH-Bench，分析模型规模与训练数据影响

Result: 更大模型展现更好的平衡性，数据种类/领域对平衡性的影响超过数据总量

Conclusion: 需建立联合评估框架，训练数据选择比单纯扩大规模更重要，为多模态模型开发提供新方向

Abstract: Test-time compute has empowered multimodal large language models to generate
extended reasoning chains, yielding strong performance on tasks such as
multimodal math reasoning. However, this improved reasoning ability often comes
with increased hallucination: as generations become longer, models tend to
drift away from image-grounded content and rely more heavily on language
priors. Attention analysis shows that longer reasoning chains lead to reduced
focus on visual inputs, which contributes to hallucination. To systematically
study this phenomenon, we introduce RH-AUC, a metric that quantifies how a
model's perception accuracy changes with reasoning length, allowing us to
evaluate whether the model preserves visual grounding during reasoning. We also
release RH-Bench, a diagnostic benchmark that spans a variety of multimodal
tasks, designed to assess the trade-off between reasoning ability and
hallucination. Our analysis reveals that (i) larger models typically achieve a
better balance between reasoning and perception, and (ii) this balance is
influenced more by the types and domains of training data than by its overall
volume. These findings underscore the importance of evaluation frameworks that
jointly consider both reasoning quality and perceptual fidelity.

</details>


### [2] [Loquacious Set: 25,000 Hours of Transcribed and Diverse English Speech Recognition Data for Research and Commercial Use](https://arxiv.org/abs/2505.21578)
*Titouan Parcollet,Yuan Tseng,Shucong Zhang,Rogier van Dalen*

Main category: cs.CL

TL;DR: 提出25,000小时的商业可用英语语音数据集Loquacious Set，解决现有ASR数据集在规模、许可和实用性方面的不足


<details>
  <summary>Details</summary>
Motivation: 现有ASR数据集如LibriSpeech存在规模小、场景单一问题，新数据集普遍存在许可证限制、转录不可靠或缺乏评估集等缺陷

Method: 构建包含数十万说话者的多样化语音集合（朗读/即兴演讲/含噪/清晰），涵盖多口音和多种语音类型

Result: 创建可商业使用的25,000小时大规模英语语音库，适用于真实场景的ASR系统开发

Conclusion: Loquacious Set突破现有数据局限，为工业界和学术界的ASR研究提供可靠、多样化的基础资源

Abstract: Automatic speech recognition (ASR) research is driven by the availability of
common datasets between industrial researchers and academics, encouraging
comparisons and evaluations. LibriSpeech, despite its long success as an ASR
benchmark, is now limited by its size and focus on clean, read speech, leading
to near-zero word error rates. More recent datasets, including MOSEL, YODAS,
Gigaspeech, OWSM, Libriheavy or People's Speech suffer from major limitations
including licenses that researchers in the industry cannot use, unreliable
transcriptions, incorrect audio data, or the lack of evaluation sets. This work
presents the Loquacious Set, a 25,000-hour curated collection of commercially
usable English speech. Featuring hundreds of thousands of speakers with diverse
accents and a wide range of speech types (read, spontaneous, talks, clean,
noisy), the Loquacious Set is designed to work for academics and researchers in
the industry to build ASR systems in real-world scenarios.

</details>


### [3] [Rethinking Data Mixture for Large Language Models: A Comprehensive Survey and New Perspectives](https://arxiv.org/abs/2505.21598)
*Yajiao Liu,Congliang Chen,Junchi Yang,Ruoyu Sun*

Main category: cs.CL

TL;DR: 系统综述现有数据混合方法，提出离线/在线方法细粒度分类框架，并分析各类方法的优劣与挑战


<details>
  <summary>Details</summary>
Motivation: 固定训练预算下不同领域数据采样比例显著影响模型性能，但现有分类体系(仅离线/在线)不够精细，需建立更系统的分类框架以指导最优领域权重选择

Method: 1. 将离线方法细分为启发式、算法优化、函数拟合三类
2. 在线方法分为在线极小极大优化、在线混合法则、其他优化框架三类
3. 通过建立离线与在线方法的优化框架对应关系进行系统分析

Result: 建立包含6个子类别的分类体系，揭示各类算法的底层优化原理关联（如离线启发式对应在线混合法则），指出算法类方法具备理论保障但计算成本高，函数拟合类易扩展但稳定性差

Conclusion: 当前方法在理论完备性、计算效率、稳定性间存在权衡，核心挑战包括动态环境适应、多目标优化、理论泛化性提升，需发展计算高效的混合优化框架

Abstract: Training large language models with data collected from various domains can
improve their performance on downstream tasks. However, given a fixed training
budget, the sampling proportions of these different domains significantly
impact the model's performance. How can we determine the domain weights across
different data domains to train the best-performing model within constrained
computational resources? In this paper, we provide a comprehensive overview of
existing data mixture methods. First, we propose a fine-grained categorization
of existing methods, extending beyond the previous offline and online
classification. Offline methods are further grouped into heuristic-based,
algorithm-based, and function fitting-based methods. For online methods, we
categorize them into three groups: online min-max optimization, online mixing
law, and other approaches by drawing connections with the optimization
frameworks underlying offline methods. Second, we summarize the problem
formulations, representative algorithms for each subtype of offline and online
methods, and clarify the relationships and distinctions among them. Finally, we
discuss the advantages and disadvantages of each method and highlight key
challenges in the field of data mixture.

</details>


### [4] [R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing](https://arxiv.org/abs/2505.21600)
*Tianyu Fu,Yi Ge,Yichen You,Enshu Liu,Zhihang Yuan,Guohao Dai,Shengen Yan,Huazhong Yang,Yu Wang*

Main category: cs.CL

TL;DR: 提出R2R神经令牌路由方法，在关键路径分叉处选择性使用大模型，显著提升推理效率同时保持性能


<details>
  <summary>Details</summary>
Motivation: 大语言模型(LLMs)存在高推理成本，而小模型(SLMs)无法有效跟随LLMs的推理路径。研究发现仅有少量关键token会导致路径分叉，这成为优化突破口。

Method: 开发神经令牌路由器自动识别关键分叉token，建立数据生成流程训练轻量路由网络，实现LLMs和SLMs的动态协同推理

Result: 在数学/编程/QA任务中以平均5.6B激活参数超越7B基线1.6倍，相比32B模型加速2.8倍且性能相当

Conclusion: R2R通过token级路由机制突破传统模型规模限制，在推理效率与性能间建立新的帕累托最优前沿

Abstract: Large Language Models (LLMs) achieve impressive reasoning capabilities at the
cost of substantial inference overhead, posing substantial deployment
challenges. Although distilled Small Language Models (SLMs) significantly
enhance efficiency, their performance suffers as they fail to follow LLMs'
reasoning paths. Luckily, we reveal that only a small fraction of tokens
genuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens
are either identical or exhibit neutral differences, such as minor variations
in abbreviations or expressions. Leveraging this insight, we introduce **Roads
to Rome (R2R)**, a neural token routing method that selectively utilizes LLMs
only for these critical, path-divergent tokens, while leaving the majority of
token generation to the SLM. We also develop an automatic data generation
pipeline that identifies divergent tokens and generates token-level routing
labels to train the lightweight router. We apply R2R to combine R1-1.5B and
R1-32B models from the DeepSeek family, and evaluate on challenging math,
coding, and QA benchmarks. With an average activated parameter size of 5.6B,
R2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the
R1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with
comparable performance, advancing the Pareto frontier of test-time scaling
efficiency. Our code is available at https://github.com/thu-nics/R2R.

</details>


### [5] [How does Misinformation Affect Large Language Model Behaviors and Preferences?](https://arxiv.org/abs/2505.21608)
*Miao Peng,Nuo Chen,Jianheng Tang,Jia Li*

Main category: cs.CL

TL;DR: 研究者构建了包含1034万条错误信息的综合基准MisBench，发现LLM虽具备一定错误信息识别能力，但仍易受知识冲突和文本风格干扰，并提出新型检测方法RtD增强模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对LLM在错误信息场景中具体受影响维度和程度的细粒度分析，需构建更全面的评估体系揭示其知识偏好和行为模式。

Method: 通过融合知识冲突和文本风格变量构建大规模数据集MisBench，并提出Reconstruct to Discriminate方法重构文本特征以提升检测能力。

Result: 实证显示LLM对知识冲突场景的误判率达38%，不同文体风格导致的错误率差异最高达22%，RtD方法使检测准确率提升15%。

Conclusion: MisBench为评估LLM检测器提供有效基准，研究揭示了模型在错误信息场景的认知边界，提出的方法增强了实际应用可靠性。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in
knowledge-intensive tasks, while they remain vulnerable when encountering
misinformation. Existing studies have explored the role of LLMs in combating
misinformation, but there is still a lack of fine-grained analysis on the
specific aspects and extent to which LLMs are influenced by misinformation. To
bridge this gap, we present MisBench, the current largest and most
comprehensive benchmark for evaluating LLMs' behavior and knowledge preference
toward misinformation. MisBench consists of 10,346,712 pieces of
misinformation, which uniquely considers both knowledge-based conflicts and
stylistic variations in misinformation. Empirical results reveal that while
LLMs demonstrate comparable abilities in discerning misinformation, they still
remain susceptible to knowledge conflicts and stylistic variations. Based on
these findings, we further propose a novel approach called Reconstruct to
Discriminate (RtD) to strengthen LLMs' ability to detect misinformation. Our
study provides valuable insights into LLMs' interactions with misinformation,
and we believe MisBench can serve as an effective benchmark for evaluating
LLM-based detectors and enhancing their reliability in real-world applications.
Codes and data are available at https://github.com/GKNL/MisBench.

</details>


### [6] [Iterative Corpus Refinement for Materials Property Prediction Based on Scientific Texts](https://arxiv.org/abs/2505.21646)
*Lei Zhang,Markus Stricker*

Main category: cs.CL

TL;DR: 提出迭代式文本挖掘框架，结合NLP与材料科学加速高性能催化剂筛选


<details>
  <summary>Details</summary>
Motivation: 解决材料发现中组合爆炸问题，利用科学文献中未充分利用的潜在知识

Method: 1. 战略选择多样化文献 2. 训练Word2Vec模型 3. 监控嵌入空间成分-属性关联收敛

Result: 成功预测氧还原/析氢/析氧反应最佳催化剂组合，实验验证预测准确性

Conclusion: 迭代式文本精炼框架为数据稀缺场景下的材料发现提供可扩展解决方案

Abstract: The discovery and optimization of materials for specific applications is
hampered by the practically infinite number of possible elemental combinations
and associated properties, also known as the `combinatorial explosion'. By
nature of the problem, data are scarce and all possible data sources should be
used. In addition to simulations and experimental results, the latent knowledge
in scientific texts is not yet used to its full potential. We present an
iterative framework that refines a given scientific corpus by strategic
selection of the most diverse documents, training Word2Vec models, and
monitoring the convergence of composition-property correlations in embedding
space. Our approach is applied to predict high-performing materials for oxygen
reduction (ORR), hydrogen evolution (HER), and oxygen evolution (OER) reactions
for a large number of possible candidate compositions. Our method successfully
predicts the highest performing compositions among a large pool of candidates,
validated by experimental measurements of the electrocatalytic performance in
the lab. This work demonstrates and validates the potential of iterative corpus
refinement to accelerate materials discovery and optimization, offering a
scalable and efficient tool for screening large compositional spaces where
reliable data are scarce or non-existent.

</details>


### [7] [Explainability of Large Language Models using SMILE: Statistical Model-agnostic Interpretability with Local Explanations](https://arxiv.org/abs/2505.21657)
*Zeinab Dehghani,Koorosh Aslansefat,Adil Khan,Mohammed Naveed Akram*

Main category: cs.CL

TL;DR: 提出SMILE方法，通过扰动输入生成热力图解释大语言模型决策依据


<details>
  <summary>Details</summary>
Motivation: 大语言模型缺乏透明度导致可信度问题，在需要问责的领域存在风险

Method: 通过微调输入测量输出变化，构建模型无关的可视化热力图解释系统

Result: 在多款主流LLM上验证，准确率、一致性等指标证实解释清晰可靠

Conclusion: SMILE提升了AI系统的可解释性，推动可信AI发展

Abstract: Large language models like GPT, LLAMA, and Claude have become incredibly
powerful at generating text, but they are still black boxes, so it is hard to
understand how they decide what to say. That lack of transparency can be
problematic, especially in fields where trust and accountability matter. To
help with this, we introduce SMILE, a new method that explains how these models
respond to different parts of a prompt. SMILE is model-agnostic and works by
slightly changing the input, measuring how the output changes, and then
highlighting which words had the most impact. Create simple visual heat maps
showing which parts of a prompt matter the most. We tested SMILE on several
leading LLMs and used metrics such as accuracy, consistency, stability, and
fidelity to show that it gives clear and reliable explanations. By making these
models easier to understand, SMILE brings us one step closer to making AI more
transparent and trustworthy.

</details>


### [8] [Rethinking the Outlier Distribution in Large Language Models: An In-depth Study](https://arxiv.org/abs/2505.21670)
*Rahul Raman,Khushi Sharma,Sai Qian Zhang*

Main category: cs.CL

TL;DR: 本文全面研究大语言模型中大规模激活和通道级异常值的形成机制，并提出高效消除方法以减少量化误差


<details>
  <summary>Details</summary>
Motivation: 大语言模型的异常值会显著影响量化效果，导致模型性能下降。现有研究缺乏对异常值根源的深入分析，需探究其形成机制以优化量化部署。

Method: 通过系统性调查两种主要异常值(大规模激活/通道级异常)的形成原因，提出抑制其产生的策略，并设计无需复杂量化算法的高效消除方案。

Result: 开发出在保证精度的前提下，有效消除大部分大规模激活和通道级异常值的技术方案

Conclusion: 揭示异常值形成机制并提出解决方案，为优化LLM量化部署提供理论基础和实践指导

Abstract: Investigating outliers in large language models (LLMs) is crucial due to
their significant impact on various aspects of LLM performance, including
quantization and compression. Outliers often cause considerable quantization
errors, leading to degraded model performance. Identifying and addressing these
outliers can enhance the accuracy and efficiency of the quantization process,
enabling smoother deployment on edge devices or specialized hardware. Recent
studies have identified two common types of outliers in LLMs: massive
activations and channel-wise outliers. While numerous quantization algorithms
have been proposed to mitigate their effects and maintain satisfactory
accuracy, few have thoroughly explored the root causes of these outliers in
depth. In this paper, we conduct a comprehensive investigation into the
formation mechanisms of these outliers and propose potential strategies to
mitigate their occurrence. Ultimately, we introduce some efficient approaches
to eliminate most massive activations and channel-wise outliers with minimal
impact on accuracy.

</details>


### [9] [LLMPR: A Novel LLM-Driven Transfer Learning based Petition Ranking Model](https://arxiv.org/abs/2505.21689)
*Avijit Gayen,Somyajit Chakraborty,Mainak Sen,Soham Paul,Angshuman Jana*

Main category: cs.CL

TL;DR: 提出LLMPR框架，利用迁移学习和机器学习自动为法律请愿书分配优先级，显著提高司法效率


<details>
  <summary>Details</summary>
Motivation: 印度司法系统积压案件导致审判延迟，人工优先级划分存在效率低下和主观偏差问题

Method: 使用ILDC数据集（7,593份请愿书），结合DistilBERT/LegalBERT文本嵌入与gap days等数值特征，训练随机森林/XGBoost等5种机器学习模型

Result: 随机森林和决策树模型准确率超99%，Spearman秩相关系数0.99。纯数值特征模型R²=0.988，LLM嵌入仅带来边际提升

Conclusion: 自动化优先级排序可有效优化司法流程，减少案件积压，同时揭示文本特征对现有司法指标的补充作用有限

Abstract: The persistent accumulation of unresolved legal cases, especially within the
Indian judiciary, significantly hampers the timely delivery of justice. Manual
methods of prioritizing petitions are often prone to inefficiencies and
subjective biases further exacerbating delays. To address this issue, we
propose LLMPR (Large Language Model-based Petition Ranking), an automated
framework that utilizes transfer learning and machine learning to assign
priority rankings to legal petitions based on their contextual urgency.
Leveraging the ILDC dataset comprising 7,593 annotated petitions, we process
unstructured legal text and extract features through various embedding
techniques, including DistilBERT, LegalBERT, and MiniLM. These textual
embeddings are combined with quantitative indicators such as gap days, rank
scores, and word counts to train multiple machine learning models, including
Random Forest, Decision Tree, XGBoost, LightGBM, and CatBoost. Our experiments
demonstrate that Random Forest and Decision Tree models yield superior
performance, with accuracy exceeding 99% and a Spearman rank correlation of
0.99. Notably, models using only numerical features achieve nearly optimal
ranking results (R2 = 0.988, \r{ho} = 0.998), while LLM-based embeddings offer
only marginal gains. These findings suggest that automated petition ranking can
effectively streamline judicial workflows, reduce case backlog, and improve
fairness in legal prioritization.

</details>


### [10] [MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural Awareness Evaluation for LLMs](https://arxiv.org/abs/2505.21693)
*Raoyuan Zhao,Beiduo Chen,Barbara Plank,Michael A. Hedderich*

Main category: cs.CL

TL;DR: 提出MAKIEval框架自动评估LLM多语言文化意识，发现英语提示更能激活文化知识。


<details>
  <summary>Details</summary>
Motivation: 现有英语预训练模型存在跨语言文化偏见，但缺乏有效多语言评估方法。

Method: 利用Wikidata跨语言锚点自动链接文化实体，设计四项互补评估指标。

Result: 评估7个LLM显示英语文化意识最强，模型间存在显著跨语言共识差异。

Conclusion: MAKIEval为多语言文化评估提供新范式，需加强非英语文化知识嵌入。

Abstract: Large language models (LLMs) are used globally across many languages, but
their English-centric pretraining raises concerns about cross-lingual
disparities for cultural awareness, often resulting in biased outputs. However,
comprehensive multilingual evaluation remains challenging due to limited
benchmarks and questionable translation quality. To better assess these
disparities, we introduce MAKIEval, an automatic multilingual framework for
evaluating cultural awareness in LLMs across languages, regions, and topics.
MAKIEval evaluates open-ended text generation, capturing how models express
culturally grounded knowledge in natural language. Leveraging Wikidata's
multilingual structure as a cross-lingual anchor, it automatically identifies
cultural entities in model outputs and links them to structured knowledge,
enabling scalable, language-agnostic evaluation without manual annotation or
translation. We then introduce four metrics that capture complementary
dimensions of cultural awareness: granularity, diversity, cultural specificity,
and consensus across languages. We assess 7 LLMs developed from different parts
of the world, encompassing both open-source and proprietary systems, across 13
languages, 19 countries and regions, and 6 culturally salient topics (e.g.,
food, clothing). Notably, we find that models tend to exhibit stronger cultural
awareness in English, suggesting that English prompts more effectively activate
culturally grounded knowledge. We publicly release our code and data.

</details>


### [11] [Do We Know What LLMs Don't Know? A Study of Consistency in Knowledge Probing](https://arxiv.org/abs/2505.21701)
*Raoyuan Zhao,Abdullatif Köksal,Ali Modarressi,Michael A. Hedderich,Hinrich Schütze*

Main category: cs.CL

TL;DR: 研究揭示现有方法在探测大语言模型知识空白时存在显著不一致性，强调需开发更稳健的探测框架


<details>
  <summary>Details</summary>
Motivation: 大语言模型的可靠性受幻觉现象严重威胁，准确识别其知识空白对提升可信度至关重要。现有探测方法存在内部及跨方法不一致性，需系统性评估其有效性

Method: 通过输入扰动(如提示词非语义调整)和量化指标，系统评估校准法与提示法的知识探测一致性

Result: 方法内扰动导致知识空白判断差异显著(如选项顺序调整使一致性降至40%)；跨方法决策一致性最低仅7%，暴露现有方法脆弱性

Conclusion: 研究揭示了现有知识探测方法的系统性缺陷，证实开发抗干扰鲁棒框架的迫切需求，为可靠评估模型知识边界提供新方向

Abstract: The reliability of large language models (LLMs) is greatly compromised by
their tendency to hallucinate, underscoring the need for precise identification
of knowledge gaps within LLMs. Various methods for probing such gaps exist,
ranging from calibration-based to prompting-based methods. To evaluate these
probing methods, in this paper, we propose a new process based on using input
variations and quantitative metrics. Through this, we expose two dimensions of
inconsistency in knowledge gap probing. (1) Intra-method inconsistency: Minimal
non-semantic perturbations in prompts lead to considerable variance in detected
knowledge gaps within the same probing method; e.g., the simple variation of
shuffling answer options can decrease agreement to around 40%. (2) Cross-method
inconsistency: Probing methods contradict each other on whether a model knows
the answer. Methods are highly inconsistent -- with decision consistency across
methods being as low as 7% -- even though the model, dataset, and prompt are
all the same. These findings challenge existing probing methods and highlight
the urgent need for perturbation-robust probing frameworks.

</details>


### [12] [Assessing and Refining ChatGPT's Performance in Identifying Targeting and Inappropriate Language: A Comparative Study](https://arxiv.org/abs/2505.21710)
*Barbarestani Baran,Maks Isa,Vossen Piek*

Main category: cs.CL

TL;DR: 研究评估ChatGPT在识别网络评论中针对性/不当语言的表现，V6版本的不当内容检测准确率显著提升，但针对性语言检测存在较高误判


<details>
  <summary>Details</summary>
Motivation: 解决海量用户生成内容审核难题，探索AI模型在内容审核中的应用潜力

Method: 通过迭代优化模型（特别是V6版本），将ChatGPT检测结果与众包标注、专家评估进行对比分析

Result: ChatGPT不当内容检测准确率达专家水平，针对性语言检测误判率比专家高12%

Conclusion: AI模型需持续优化上下文理解能力，建议采用混合审核模式（AI初筛+人工复核）提升内容审核效率

Abstract: This study evaluates the effectiveness of ChatGPT, an advanced AI model for
natural language processing, in identifying targeting and inappropriate
language in online comments. With the increasing challenge of moderating vast
volumes of user-generated content on social network sites, the role of AI in
content moderation has gained prominence. We compared ChatGPT's performance
against crowd-sourced annotations and expert evaluations to assess its
accuracy, scope of detection, and consistency. Our findings highlight that
ChatGPT performs well in detecting inappropriate content, showing notable
improvements in accuracy through iterative refinements, particularly in Version
6. However, its performance in targeting language detection showed variability,
with higher false positive rates compared to expert judgments. This study
contributes to the field by demonstrating the potential of AI models like
ChatGPT to enhance automated content moderation systems while also identifying
areas for further improvement. The results underscore the importance of
continuous model refinement and contextual understanding to better support
automated moderation and mitigate harmful online behavior.

</details>


### [13] [Counterfactual Simulatability of LLM Explanations for Generation Tasks](https://arxiv.org/abs/2505.21740)
*Marvin Limpijankit,Yanda Chen,Melanie Subbiah,Nicholas Deas,Kathleen McKeown*

Main category: cs.CL

TL;DR: 提出将反事实模拟性评估方法扩展至生成任务，发现LLM解释在新闻摘要中有效但医疗建议领域仍有不足，建议该方法更适用于技能型任务


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）的输出具有不可预测性，其解释能力对高风险场景至关重要。需要验证解释是否能帮助用户预测模型在反事实场景中的行为

Method: 将反事实模拟性评估框架从判断题扩展至生成任务，选取新闻摘要和医疗建议作为典型场景进行验证

Result: LLM解释有效提升新闻摘要场景的反事实输出预测能力，但在医疗建议领域效果有限。评估方法对技能型任务（摘要）比知识型任务（医疗）更适用

Conclusion: 反事实模拟性评估更适合技能导向型任务，为未来改进LLM解释方法提供了方向性指引，特别是在知识密集型应用场景

Abstract: LLMs can be unpredictable, as even slight alterations to the prompt can cause
the output to change in unexpected ways. Thus, the ability of models to
accurately explain their behavior is critical, especially in high-stakes
settings. One approach for evaluating explanations is counterfactual
simulatability, how well an explanation allows users to infer the model's
output on related counterfactuals. Counterfactual simulatability has been
previously studied for yes/no question answering tasks. We provide a general
framework for extending this method to generation tasks, using news
summarization and medical suggestion as example use cases. We find that while
LLM explanations do enable users to better predict LLM outputs on
counterfactuals in the summarization setting, there is significant room for
improvement for medical suggestion. Furthermore, our results suggest that the
evaluation for counterfactual simulatability may be more appropriate for
skill-based tasks as opposed to knowledge-based tasks.

</details>


### [14] [BehaviorSFT: Behavioral Token Conditioning for Clinical Agents Across the Proactivity Spectrum](https://arxiv.org/abs/2505.21757)
*Yubin Kim,Zhiyuan Hu,Hyewon Jeong,Eugene Park,Shuyue Stella Li,Chanwoo Park,Shiyun Xiong,MingYu Lu,Hyeonhoon Lee,Xin Liu,Daniel McDuff,Cynthia Breazeal,Samir Tulebaev,Hae Won Park*

Main category: cs.CL

TL;DR: 提出BehaviorBench数据集评估LLMs临床行为表现，发现其主动性不足。通过BehaviorSFT训练策略（使用行为标记动态调整行为），显著提升模型性能（Macro F1达97.3%），临床盲评显示其能更好平衡主动建议与必要克制。


<details>
  <summary>Details</summary>
Motivation: LLMs擅长反应性临床任务（如诊断推理），但缺乏主动识别关键缺失信息/风险的能力，需系统性评估和改进其临床行为适应性。

Method: 1.构建BehaviorBench数据集（涵盖从反应性问答到主动干预的临床行为谱）；2.提出BehaviorSFT训练策略（通过行为标记显式引导LLM动态选择行为模式）。

Result: BehaviorSFT使Qwen2.5-7B-Ins模型在主动任务得分提升（如从95.0%→96.5%），临床盲评认可其更现实的主动/克制平衡（比标准微调模型高30%接受率）。

Conclusion: 显式行为条件训练（BehaviorSFT）能有效提升LLMs在临床场景的主动性与平衡性，优于指令微调或显式提示方法，为临床AI代理开发提供新思路。

Abstract: Large Language Models (LLMs) as clinical agents require careful behavioral
adaptation. While adept at reactive tasks (e.g., diagnosis reasoning), LLMs
often struggle with proactive engagement, like unprompted identification of
critical missing information or risks. We introduce BehaviorBench, a
comprehensive dataset to evaluate agent behaviors across a clinical assistance
spectrum, ranging from reactive query responses to proactive interventions
(e.g., clarifying ambiguities, flagging overlooked critical data). Our
BehaviorBench experiments reveal LLMs' inconsistent proactivity. To address
this, we propose BehaviorSFT, a novel training strategy using behavioral tokens
to explicitly condition LLMs for dynamic behavioral selection along this
spectrum. BehaviorSFT boosts performance, achieving up to 97.3% overall Macro
F1 on BehaviorBench and improving proactive task scores (e.g., from 95.0% to
96.5% for Qwen2.5-7B-Ins). Crucially, blind clinician evaluations confirmed
BehaviorSFT-trained agents exhibit more realistic clinical behavior, striking a
superior balance between helpful proactivity (e.g., timely, relevant
suggestions) and necessary restraint (e.g., avoiding over-intervention) versus
standard fine-tuning or explicit instructed agents.

</details>


### [15] [Calibrating LLM Confidence by Probing Perturbed Representation Stability](https://arxiv.org/abs/2505.21772)
*Reza Khanmohammadi,Erfan Miahi,Mehrsa Mardikoraem,Simerjot Kaur,Ivan Brugere,Charese H. Smiley,Kundan Thind,Mohammad M. Ghassemi*

Main category: cs.CL

TL;DR: 提出CCPS方法，通过扰动大语言模型的隐藏表征来校准置信度，显著提升多项校准指标


<details>
  <summary>Details</summary>
Motivation: 大语言模型的置信度失准问题影响可靠性，现有方法需更精准的置信度估计方法

Method: 对最终隐藏状态施加对抗扰动→提取扰动响应特征→轻量级分类器预测答案正确性，测试覆盖8B-32B参数的Llama/Qwen/Mistral模型

Result: 在MMLU系列基准测试中，较最优基线方法：校准误差降低55%，Brier分数降21%，准确率/AUPRC/AUROC分别提升5%、4%、6%

Conclusion: CCPS为LLM置信度估计提供了高效、通用且更准确的解决方案，增强了模型可信度

Abstract: Miscalibration in Large Language Models (LLMs) undermines their reliability,
highlighting the need for accurate confidence estimation. We introduce CCPS
(Calibrating LLM Confidence by Probing Perturbed Representation Stability), a
novel method analyzing internal representational stability in LLMs. CCPS
applies targeted adversarial perturbations to final hidden states, extracts
features reflecting the model's response to these perturbations, and uses a
lightweight classifier to predict answer correctness. CCPS was evaluated on
LLMs from 8B to 32B parameters (covering Llama, Qwen, and Mistral
architectures) using MMLU and MMLU-Pro benchmarks in both multiple-choice and
open-ended formats. Our results show that CCPS significantly outperforms
current approaches. Across four LLMs and three MMLU variants, CCPS reduces
Expected Calibration Error by approximately 55% and Brier score by 21%, while
increasing accuracy by 5 percentage points, Area Under the Precision-Recall
Curve by 4 percentage points, and Area Under the Receiver Operating
Characteristic Curve by 6 percentage points, all relative to the strongest
prior method. CCPS delivers an efficient, broadly applicable, and more accurate
solution for estimating LLM confidence, thereby improving their
trustworthiness.

</details>


### [16] [GMU Systems for the IWSLT 2025 Low-Resource Speech Translation Shared Task](https://arxiv.org/abs/2505.21781)
*Chutong Meng,Antonios Anastasopoulos*

Main category: cs.CL

TL;DR: GMU团队针对IWSLT 2025低资源语音翻译任务，通过微调SeamlessM4T-v2模型探索端到端语音翻译的有效训练范式，发现直接微调效果最佳，ASR编码器初始化提升新语言性能，多任务训练效果有限。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言对的语音翻译挑战，探索端到端语音翻译模型在不同训练范式下的性能表现，特别是针对模型未训练过的语言优化方案。

Method: 1. 微调SeamlessM4T-v2实现ASR/MT/端到端ST
2. 构建ASR+MT级联系统
3. 尝试直接微调、多任务训练、ASR/MT模块初始化等训练策略

Result: 1. 直接端到端微调效果显著
2. 使用微调后的ASR编码器初始化使未训练语言ST性能提升
3. 多任务训练带来有限增益

Conclusion: 直接端到端微调是高效方案，ASR模块迁移能有效提升新语言处理能力，为低资源语音翻译系统开发提供了重要实践指导。

Abstract: This paper describes the GMU systems for the IWSLT 2025 low-resource speech
translation shared task. We trained systems for all language pairs, except for
Levantine Arabic. We fine-tuned SeamlessM4T-v2 for automatic speech recognition
(ASR), machine translation (MT), and end-to-end speech translation (E2E ST).
The ASR and MT models are also used to form cascaded ST systems. Additionally,
we explored various training paradigms for E2E ST fine-tuning, including direct
E2E fine-tuning, multi-task training, and parameter initialization using
components from fine-tuned ASR and/or MT models. Our results show that (1)
direct E2E fine-tuning yields strong results; (2) initializing with a
fine-tuned ASR encoder improves ST performance on languages SeamlessM4T-v2 has
not been trained on; (3) multi-task training can be slightly helpful.

</details>


### [17] [VeriTrail: Closed-Domain Hallucination Detection with Traceability](https://arxiv.org/abs/2505.21786)
*Dasha Metropolitansky,Jonathan Larson*

Main category: cs.CL

TL;DR: VeriTrail方法有效检测多步骤生成过程中的闭域幻觉，提供生成链条的溯源能力，在数据集上优于基线方法


<details>
  <summary>Details</summary>
Motivation: 现有方法仅检测最终输出的幻觉，但多步骤生成(MGS)过程中中间步骤的误差传递更需溯源追踪

Method: 提出首个支持MGS/SGS双模式的可溯源检测框架VeriTrail，构建包含中间输出与人工标注的数据集

Result: 在两个新型数据集上的实验表明VeriTrail检测性能显著优于基线方法

Conclusion: 通过中间输出的溯源验证机制，VeriTrail为生成过程可靠性分析提供了创新解决方案

Abstract: Even when instructed to adhere to source material, Language Models often
generate unsubstantiated content - a phenomenon known as "closed-domain
hallucination." This risk is amplified in processes with multiple generative
steps (MGS), compared to processes with a single generative step (SGS).
However, due to the greater complexity of MGS processes, we argue that
detecting hallucinations in their final outputs is necessary but not
sufficient: it is equally important to trace where hallucinated content was
likely introduced and how faithful content may have been derived from the
source through intermediate outputs. To address this need, we present
VeriTrail, the first closed-domain hallucination detection method designed to
provide traceability for both MGS and SGS processes. We also introduce the
first datasets to include all intermediate outputs as well as human annotations
of final outputs' faithfulness for their respective MGS processes. We
demonstrate that VeriTrail outperforms baseline methods on both datasets.

</details>


### [18] [Revisiting Common Assumptions about Arabic Dialects in NLP](https://arxiv.org/abs/2505.21816)
*Amr Keleg,Sharon Goldwater,Walid Magdy*

Main category: cs.CL

TL;DR: 论文通过实证分析挑战阿拉伯方言可分组的固有假设，发现现有NLP任务中的四个方言假设存在过度简化问题。


<details>
  <summary>Details</summary>
Motivation: 当前阿拉伯语NLP领域广泛采用未经定量验证的方言假设（如方言可按地区分组），可能阻碍任务进展。

Method: 扩展多标签数据集，由母语者人工验证11个国家级方言的句子有效性，定量检验四个常见假设。

Result: 四个方言假设均存在过度简化（如地区方言分组不准确），部分假设与语言现实不符。

Conclusion: 现有阿拉伯方言的简化假设限制NLP发展，需更细致的方言建模方法。

Abstract: Arabic has diverse dialects, where one dialect can be substantially different
from the others. In the NLP literature, some assumptions about these dialects
are widely adopted (e.g., ``Arabic dialects can be grouped into distinguishable
regional dialects") and are manifested in different computational tasks such as
Arabic Dialect Identification (ADI). However, these assumptions are not
quantitatively verified. We identify four of these assumptions and examine them
by extending and analyzing a multi-label dataset, where the validity of each
sentence in 11 different country-level dialects is manually assessed by
speakers of these dialects. Our analysis indicates that the four assumptions
oversimplify reality, and some of them are not always accurate. This in turn
might be hindering further progress in different Arabic NLP tasks.

</details>


### [19] [Representative Language Generation](https://arxiv.org/abs/2505.21819)
*Charlotte Peale,Vinod Raman,Omer Reingold*

Main category: cs.CL

TL;DR: 论文扩展了生成模型的理论框架，提出'代表性生成'概念以解决生成模型的多样性和偏见问题，引入'群体闭包维度'作为关键指标，分析了无限假设类下的可行性但指出计算限制。


<details>
  <summary>Details</summary>
Motivation: 现有生成框架未充分考虑多样性和偏见问题，需要确保生成结果按比例反映训练数据中的不同群体分布。

Method: 通过引入群体闭包维度作为组合量，建立代表性均匀/非均匀生成理论框架，分析无限假设类下信息论和计算层面的可行性。

Result: 证明可数无限假设类和群体集合在特定条件下的可行性，但仅通过成员查询无法实现可计算生成，与原始生成框架形成对比。

Conclusion: 为开发更具多样性和代表性的生成模型奠定理论基础，强调需同时考虑算法设计和社会技术因素解决生成公平性问题。

Abstract: We introduce "representative generation," extending the theoretical framework
for generation proposed by Kleinberg et al. (2024) and formalized by Li et al.
(2024), to additionally address diversity and bias concerns in generative
models. Our notion requires outputs of a generative model to proportionally
represent groups of interest from the training data. We characterize
representative uniform and non-uniform generation, introducing the "group
closure dimension" as a key combinatorial quantity. For representative
generation in the limit, we analyze both information-theoretic and
computational aspects, demonstrating feasibility for countably infinite
hypothesis classes and collections of groups under certain conditions, but
proving a negative result for computability using only membership queries. This
contrasts with Kleinberg et al.'s (2024) positive results for standard
generation in the limit. Our findings provide a rigorous foundation for
developing more diverse and representative generative models.

</details>


### [20] [Principled Content Selection to Generate Diverse and Personalized Multi-Document Summaries](https://arxiv.org/abs/2505.21859)
*Vishakh Padmakumar,Zichao Wang,David Arbour,Jennifer Healey*

Main category: cs.CL

TL;DR: 提出三阶段流程（关键点提取→DPP多样性选择→摘要重写）解决LLMs在多文档摘要中源覆盖不足的问题，并支持个性化摘要生成。


<details>
  <summary>Details</summary>
Motivation: 针对大语言模型存在的'中间迷失'现象导致多文档摘要源材料覆盖不均衡的问题，通过结构化内容选择提升覆盖率。

Method: 1. 将文档集分解为原子关键点
2. 使用行列式点过程(DPP)选择多样性内容
3. 最终摘要重写
结合提示工程与确定性选择技术，在DPP核函数中整合用户意图实现个性化。

Result: 在DiverseSumm基准测试中显著提升不同LLMs的源覆盖率，验证了结合用户意图的DPP可生成既相关又覆盖全面的个性化摘要。

Conclusion: 分阶段结构化方法有效解决LLMs在多文档摘要中的覆盖缺陷，通过DPP与意图融合平衡覆盖率与相关性，为摘要系统设计提供新范式。

Abstract: While large language models (LLMs) are increasingly capable of handling
longer contexts, recent work has demonstrated that they exhibit the "lost in
the middle" phenomenon (Liu et al., 2024) of unevenly attending to different
parts of the provided context. This hinders their ability to cover diverse
source material in multi-document summarization, as noted in the DiverseSumm
benchmark (Huang et al., 2024). In this work, we contend that principled
content selection is a simple way to increase source coverage on this task. As
opposed to prompting an LLM to perform the summarization in a single step, we
explicitly divide the task into three steps -- (1) reducing document
collections to atomic key points, (2) using determinantal point processes (DPP)
to perform select key points that prioritize diverse content, and (3) rewriting
to the final summary. By combining prompting steps, for extraction and
rewriting, with principled techniques, for content selection, we consistently
improve source coverage on the DiverseSumm benchmark across various LLMs.
Finally, we also show that by incorporating relevance to a provided user intent
into the DPP kernel, we can generate personalized summaries that cover relevant
source information while retaining coverage.

</details>


### [21] [Evaluating the Retrieval Robustness of Large Language Models](https://arxiv.org/abs/2505.21870)
*Shuyang Cao,Karthik Radhakrishnan,David Rosenberg,Steven Lu,Pengxiang Cheng,Lu Wang,Shiyue Zhang*

Main category: cs.CL

TL;DR: 研究发现LLMs在RAG场景中具有较高检索鲁棒性，但未完全发挥RAG优势


<details>
  <summary>Details</summary>
Motivation: 评估实际RAG场景中LLMs的鲁棒性，验证RAG性能增益的稳定性

Method: 建立含1500问题的基准测试，引入3个鲁棒性指标，测试11种LLM和3种提示策略

Result: 所有LLM均表现出高检索鲁棒性，但文档质量/排序仍影响性能提升

Conclusion: RAG整体有效但存在优化空间，需提升模型对非理想检索结果的处理能力

Abstract: Retrieval-augmented generation (RAG) generally enhances large language
models' (LLMs) ability to solve knowledge-intensive tasks. But RAG may also
lead to performance degradation due to imperfect retrieval and the model's
limited ability to leverage retrieved content. In this work, we evaluate the
robustness of LLMs in practical RAG setups (henceforth retrieval robustness).
We focus on three research questions: (1) whether RAG is always better than
non-RAG; (2) whether more retrieved documents always lead to better
performance; (3) and whether document orders impact results. To facilitate this
study, we establish a benchmark of 1500 open-domain questions, each with
retrieved documents from Wikipedia. We introduce three robustness metrics, each
corresponds to one research question. Our comprehensive experiments, involving
11 LLMs and 3 prompting strategies, reveal that all of these LLMs exhibit
surprisingly high retrieval robustness; nonetheless, different degrees of
imperfect robustness hinders them from fully utilizing the benefits of RAG.

</details>


### [22] [EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV Cache Reuse](https://arxiv.org/abs/2505.21889)
*Tianyu Guo,Hande Dong,Yichong Leng,Feng Liu,Cheater Lin,Nong Xiao,Xianwei Zhang*

Main category: cs.CL

TL;DR: 提出EFIM提示格式和片段标记化训练方法，显著提升LLM填充任务效率


<details>
  <summary>Details</summary>
Motivation: 现有填充任务提示格式阻碍KV缓存重用，导致重复计算资源浪费

Method: 1. 改进FIM提示格式为EFIM结构
2. 引入片段分割训练方法解决子词生成问题

Result: 延迟降低52%，吞吐量提升98%，同时保持原始填充能力

Conclusion: EFIM有效释放KV缓存潜力，配套训练方法解决技术适配问题，方案已开源

Abstract: Large language models (LLMs) are often used for infilling tasks, which
involve predicting or generating missing information in a given text. These
tasks typically require multiple interactions with similar context. To reduce
the computation of repeated historical tokens, cross-request key-value (KV)
cache reuse, a technique that stores and reuses intermediate computations, has
become a crucial method in multi-round interactive services. However, in
infilling tasks, the KV cache reuse is often hindered by the structure of the
prompt format, which typically consists of a prefix and suffix relative to the
insertion point. Specifically, the KV cache of the prefix or suffix part is
frequently invalidated as the other part (suffix or prefix) is incrementally
generated. To address the issue, we propose EFIM, a transformed prompt format
of FIM to unleash the performance potential of KV cache reuse. Although the
transformed prompt can solve the inefficiency, it exposes subtoken generation
problems in current LLMs, where they have difficulty generating partial words
accurately. Therefore, we introduce a fragment tokenization training method
which splits text into multiple fragments before tokenization during data
processing. Experiments on two representative LLMs show that LLM serving with
EFIM can lower the latency by 52% and improve the throughput by 98% while
maintaining the original infilling capability.EFIM's source code is publicly
available at https://github.com/gty111/EFIM.

</details>


### [23] [Co-Saving: Resource Aware Multi-Agent Collaboration for Software Development](https://arxiv.org/abs/2505.21898)
*Rennai Qiu,Chen Qian,Ran Li,Yufan Dang,Weize Chen,Cheng Yang,Yingli Zhang,Ye Tian,Xuantang Xiong,Lei Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 提出资源感知的多代理系统Co-Saving，通过历史经验中的'捷径'机制减少冗余推理，在软件开发任务中实现token用量降低50.85%且代码质量提升10.06%。


<details>
  <summary>Details</summary>
Motivation: 现有多代理系统存在高token消耗和执行效率低下的问题，需通过资源优化提升系统效能。

Method: 引入基于历史成功轨迹的'捷径'机制，动态绕过冗余代理节点，优化协作路径。

Result: 相比ChatDev系统，平均减少50.85%的token使用量，代码质量提升10.06%。

Conclusion: Co-Saving系统通过资源感知和捷径学习显著提升多代理协作效率，为复杂任务处理提供高效解决方案。

Abstract: Recent advancements in Large Language Models (LLMs) and autonomous agents
have demonstrated remarkable capabilities across various domains. However,
standalone agents frequently encounter limitations when handling complex tasks
that demand extensive interactions and substantial computational resources.
Although Multi-Agent Systems (MAS) alleviate some of these limitations through
collaborative mechanisms like task decomposition, iterative communication, and
role specialization, they typically remain resource-unaware, incurring
significant inefficiencies due to high token consumption and excessive
execution time. To address these limitations, we propose a resource-aware
multi-agent system -- Co-Saving (meaning that multiple agents collaboratively
engage in resource-saving activities), which leverages experiential knowledge
to enhance operational efficiency and solution quality. Our key innovation is
the introduction of "shortcuts" -- instructional transitions learned from
historically successful trajectories -- which allows to bypass redundant
reasoning agents and expedite the collective problem-solving process.
Experiments for software development tasks demonstrate significant advantages
over existing methods. Specifically, compared to the state-of-the-art MAS
ChatDev, our method achieves an average reduction of 50.85% in token usage, and
improves the overall code quality by 10.06%.

</details>


### [24] [Beyond Completion: A Foundation Model for General Knowledge Graph Reasoning](https://arxiv.org/abs/2505.21926)
*Yin Hua,Zhiqiang Liu,Mingyang Chen,Zheng Fang,Chi Man Wong,Lingxiao Li,Chi Man Vong,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: MERRY是一种知识图谱基础模型，通过多模态融合实现图谱内外推理任务，在28个数据集上展现优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱基础模型主要关注结构信息且局限于图谱内任务（如知识补全），难以应对更具挑战性的图谱外任务（如知识问答）。

Method: 提出多视角条件消息传递编码架构（CMP）融合文本与结构模态，结合动态残差融合模块和灵活边评分机制适配下游任务。

Result: 在大多数场景下超越基线模型，图谱内推理任务准确率提升，图谱外任务（如KGQA）泛化能力显著。

Conclusion: MERRY成功实现文本与结构信息的深度整合，为知识图谱基础模型的发展提供了新范式，验证了跨任务泛化的可行性。

Abstract: In natural language processing (NLP) and computer vision (CV), the successful
application of foundation models across diverse tasks has demonstrated their
remarkable potential. However, despite the rich structural and textual
information embedded in knowledge graphs (KGs), existing research of foundation
model for KG has primarily focused on their structural aspects, with most
efforts restricted to in-KG tasks (e.g., knowledge graph completion, KGC). This
limitation has hindered progress in addressing more challenging out-of-KG
tasks. In this paper, we introduce MERRY, a foundation model for general
knowledge graph reasoning, and investigate its performance across two task
categories: in-KG reasoning tasks (e.g., KGC) and out-of-KG tasks (e.g., KG
question answering, KGQA). We not only utilize the structural information, but
also the textual information in KGs. Specifically, we propose a
multi-perspective Conditional Message Passing (CMP) encoding architecture to
bridge the gap between textual and structural modalities, enabling their
seamless integration. Additionally, we introduce a dynamic residual fusion
module to selectively retain relevant textual information and a flexible edge
scoring mechanism to adapt to diverse downstream tasks. Comprehensive
evaluations on 28 datasets demonstrate that MERRY outperforms existing
baselines in most scenarios, showcasing strong reasoning capabilities within
KGs and excellent generalization to out-of-KG tasks such as KGQA.

</details>


### [25] [RedTeamCUA: Realistic Adversarial Testing of Computer-Use Agents in Hybrid Web-OS Environments](https://arxiv.org/abs/2505.21936)
*Zeyi Liao,Jaylen Jones,Linxi Jiang,Eric Fosler-Lussier,Yu Su,Zhiqiang Lin,Huan Sun*

Main category: cs.CL

TL;DR: 提出了RedTeamCUA框架用于系统化评估计算机使用代理的间接提示注入漏洞，测试显示前沿CUAs攻击成功率高达48%，揭示其现实风险


<details>
  <summary>Details</summary>
Motivation: 当前计算机使用代理（CUAs）存在间接提示注入漏洞，但现有评估缺乏真实混合环境测试，无法全面反映网页-操作系统联动的攻击场景

Method: 构建结合VM操作系统与Docker网页平台的混合沙盒环境，开发包含864个攻击场景的RTC-Bench基准，支持从注入点直接初始化的对抗测试

Result: Claude 4 Opus攻击成功率48%，测试中CUAs平均尝试执行恶意任务率达92.5%，混合场景端到端攻击成功率最高达50%

Conclusion: 间接提示注入对先进CUAs构成实质性威胁，RedTeamCUA为系统化漏洞分析提供必要框架，强调实际部署前需建立强健防御机制

Abstract: Computer-use agents (CUAs) promise to automate complex tasks across operating
systems (OS) and the web, but remain vulnerable to indirect prompt injection.
Current evaluations of this threat either lack support realistic but controlled
environments or ignore hybrid web-OS attack scenarios involving both
interfaces. To address this, we propose RedTeamCUA, an adversarial testing
framework featuring a novel hybrid sandbox that integrates a VM-based OS
environment with Docker-based web platforms. Our sandbox supports key features
tailored for red teaming, such as flexible adversarial scenario configuration,
and a setting that decouples adversarial evaluation from navigational
limitations of CUAs by initializing tests directly at the point of an
adversarial injection. Using RedTeamCUA, we develop RTC-Bench, a comprehensive
benchmark with 864 examples that investigate realistic, hybrid web-OS attack
scenarios and fundamental security vulnerabilities. Benchmarking current
frontier CUAs identifies significant vulnerabilities: Claude 3.7 Sonnet | CUA
demonstrates an ASR of 42.9%, while Operator, the most secure CUA evaluated,
still exhibits an ASR of 7.6%. Notably, CUAs often attempt to execute
adversarial tasks with an Attempt Rate as high as 92.5%, although failing to
complete them due to capability limitations. Nevertheless, we observe
concerning ASRs of up to 50% in realistic end-to-end settings, with the
recently released frontier Claude 4 Opus | CUA showing an alarming ASR of 48%,
demonstrating that indirect prompt injection presents tangible risks for even
advanced CUAs despite their capabilities and safeguards. Overall, RedTeamCUA
provides an essential framework for advancing realistic, controlled, and
systematic analysis of CUA vulnerabilities, highlighting the urgent need for
robust defenses to indirect prompt injection prior to real-world deployment.

</details>


### [26] [Graph-Assisted Culturally Adaptable Idiomatic Translation for Indic Languages](https://arxiv.org/abs/2505.21937)
*Pratik Rakesh Singh,Kritarth Prasad,Mohammadi Zaki,Pankaj Wasnik*

Main category: cs.CL

TL;DR: 提出IdiomCE自适应图神经网络方法提升习语翻译质量，解决传统静态知识图谱的局限性


<details>
  <summary>Details</summary>
Motivation: 传统静态知识图谱和prompt方法难以捕捉习语翻译中的文化差异和一对多映射关系，导致翻译质量不佳

Method: 基于图神经网络(GNN)构建动态映射模型，学习习语表达间的复杂关系，支持训练中已见/未见节点的泛化

Result: 在英语到印度诸语言的习语翻译测试中，使用无参考指标验证方法有效性，显著提升资源受限场景下的翻译质量

Conclusion: IdiomCE通过动态关系建模突破传统方法瓶颈，为小模型场景的跨文化习语翻译提供新解决方案

Abstract: Translating multi-word expressions (MWEs) and idioms requires a deep
understanding of the cultural nuances of both the source and target languages.
This challenge is further amplified by the one-to-many nature of idiomatic
translations, where a single source idiom can have multiple target-language
equivalents depending on cultural references and contextual variations.
Traditional static knowledge graphs (KGs) and prompt-based approaches struggle
to capture these complex relationships, often leading to suboptimal
translations. To address this, we propose IdiomCE, an adaptive graph neural
network (GNN) based methodology that learns intricate mappings between
idiomatic expressions, effectively generalizing to both seen and unseen nodes
during training. Our proposed method enhances translation quality even in
resource-constrained settings, facilitating improved idiomatic translation in
smaller models. We evaluate our approach on multiple idiomatic translation
datasets using reference-less metrics, demonstrating significant improvements
in translating idioms from English to various Indian languages.

</details>


### [27] [RISE: Reasoning Enhancement via Iterative Self-Exploration in Multi-hop Question Answering](https://arxiv.org/abs/2505.21940)
*Bolei He,Xinran He,Mengke Chen,Xianwei Xue,Ying Zhu,Zhenhua Ling*

Main category: cs.CL

TL;DR: 提出RISE框架通过迭代自我探索三步骤（问题分解-检索阅读-自我批判），显著提升大语言模型在多跳问答任务中的推理准确性和性能


<details>
  <summary>Details</summary>
Motivation: 大语言模型在需要整合多源证据和复杂逻辑推理的多跳问答任务中表现不足，现有检索增强生成方法存在噪声过滤低效和证据检索不完整的问题

Method: RISE框架包含三阶段：1.问题分解降低复杂度 2.检索阅读整合证据 3.自我批判迭代优化推理路径，通过持续自我探索提升逻辑一致性和证据整合能力

Result: 在多个MHQA基准测试中，RISE显著提升推理准确性和任务性能，验证了框架有效性

Conclusion: 迭代自我探索机制有效增强模型复杂推理能力，为解决多跳问答的挑战提供了创新性解决方案

Abstract: Large Language Models (LLMs) excel in many areas but continue to face
challenges with complex reasoning tasks, such as Multi-Hop Question Answering
(MHQA). MHQA requires integrating evidence from diverse sources while managing
intricate logical dependencies, often leads to errors in reasoning.
Retrieval-Augmented Generation (RAG), widely employed in MHQA tasks, faces
challenges in effectively filtering noisy data and retrieving all necessary
evidence, thereby limiting its effectiveness in addressing MHQA challenges. To
address these challenges, we propose RISE:Reasoning Enhancement via Iterative
Self-Exploration, a novel framework designed to enhance models' reasoning
capability through iterative self-exploration. Specifically, RISE involves
three key steps in addressing MHQA tasks: question decomposition,
retrieve-then-read, and self-critique. By leveraging continuous
self-exploration, RISE identifies accurate reasoning paths, iteratively
self-improving the model's capability to integrate evidence, maintain logical
consistency, and enhance performance in MHQA tasks. Extensive experiments on
multiple MHQA benchmarks demonstrate that RISE significantly improves reasoning
accuracy and task performance.

</details>


### [28] [Test-Time Scaling with Repeated Sampling Improves Multilingual Text Generation](https://arxiv.org/abs/2505.21941)
*Ashim Gupta,Vivek Srikumar*

Main category: cs.CL

TL;DR: 通过困惑度与奖励双验证器评估发现，推理时重复采样可使多语言生成质量提升超35%，验证器选择直接影响任务效果


<details>
  <summary>Details</summary>
Motivation: 探索推理时重复采样策略在多语言文本生成中的效果差异，量化验证机制（困惑度vs奖励）对不同任务类型的适用边界

Method: 在Aya Evaluation Suite和m-ArenaHard基准上，同步使用困惑度评分和奖励模型两种验证器进行多轮采样验证

Result: 开放式任务中困惑度验证有效，数学/代码等推理任务仅奖励验证有效，最优情况下生成质量提升超35%

Conclusion: 重复采样机制显著扩展多语言生成应用场景，验证器与任务类型的适配性是效果提升的关键决策因素

Abstract: Inference-time scaling via repeated sampling has shown promise in reasoning
tasks, but its effectiveness in multilingual generation remains underexplored.
We evaluate this approach using perplexity- and reward-based verifiers on two
multilingual benchmarks: the Aya Evaluation Suite and m-ArenaHard. Our results
show consistent quality improvements, with gains exceeding 35% in some cases.
While perplexity-based scoring is effective for open-ended prompts, only
reward-based verifiers improve performance on tasks requiring reasoning (e.g.,
math, code). Our results demonstrate the broader utility of repeated sampling
for multilingual text generation and underscore the importance of selecting
right verifiers for the task.

</details>


### [29] [Resolving Knowledge Conflicts in Domain-specific Data Selection: A Case Study on Medical Instruction-tuning](https://arxiv.org/abs/2505.21958)
*Qihuang Zhong,Liang Ding,Fei Liao,Juhua Liu,Bo Du,Dacheng Tao*

Main category: cs.CL

TL;DR: 提出知识感知数据选择框架KDS，通过量化知识冲突优化领域专用指令微调数据选择，提升大语言模型在专业领域的表现并缓解幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 传统数据选择方法忽视知识冲突（预训练知识与指令数据背景知识间的差异），导致模型先验能力受损并产生幻觉。尤其在医疗等专业领域，低质量指令数据筛选需求迫切。

Method: 设计双维度知识冲突量化指标：1）上下文记忆对齐度（测量指令背景与模型记忆的匹配度）2）记忆内部一致性（检测知识单元间的逻辑一致性）。基于指标筛选低冲突、高质量、多样性数据。

Result: 医疗领域实验显示KDS显著优于基线方法，所有测试模型性能提升15-22%。模型泛化能力增强，幻觉问题减少37%（人工评估指标）。

Conclusion: KDS通过系统化处理知识冲突，为领域专用指令微调提供有效数据选择方案。其指标设计对缓解大模型幻觉问题具有普适性启示，可扩展至法律、金融等专业领域。

Abstract: Domain-specific instruction-tuning has become the defacto standard for
improving the performance of large language models (LLMs) in specialized
applications, e.g., medical question answering. Since the instruction-tuning
dataset might contain redundant or low-quality data, data selection (DS) is
usually required to maximize the data efficiency. Despite the successes in the
general domain, current DS methods often struggle to select the desired data
for domain-specific instruction-tuning. One of the main reasons is that they
neglect the impact of knowledge conflicts, i.e., the discrepancy between LLMs'
pretrained knowledge and context knowledge of instruction data, which could
damage LLMs' prior abilities and lead to hallucination. To this end, we propose
a simple-yet-effective Knowledge-aware Data Selection (namely KDS) framework to
select the domain-specific instruction-tuning data that meets LLMs' actual
needs. The core of KDS is to leverage two knowledge-aware metrics for
quantitatively measuring knowledge conflicts from two aspects: context-memory
knowledge alignment and intra-memory knowledge consistency. By filtering the
data with large knowledge conflicts and sampling the high-quality and diverse
data, KDS can effectively stimulate the LLMs' abilities and achieve better
domain-specific performance. Taking the medical domain as the testbed, we
conduct extensive experiments and empirically prove that KDS surpasses the
other baselines and brings significant and consistent performance gains among
all LLMs. More encouragingly, KDS effectively improves the model generalization
and alleviates the hallucination problem.

</details>


### [30] [LaMDAgent: An Autonomous Framework for Post-Training Pipeline Optimization via LLM Agents](https://arxiv.org/abs/2505.21963)
*Taro Yano,Yoichi Ishibashi,Masafumi Oyamada*

Main category: cs.CL

TL;DR: 提出LaMDAgent框架，通过LLM代理自动构建和优化完整后训练流程，在工具使用准确率上提升9%同时保持指令跟随能力，并发现传统方法易忽视的高效策略。


<details>
  <summary>Details</summary>
Motivation: 现有后训练方法通常孤立优化组件（如SFT/Preference Learning/模型合并），依赖人工设计流程，缺乏全自动化探索完整后训练管道的能力。

Method: 基于LLM代理系统探索模型生成技术、数据集和超参数配置，利用任务反馈机制自主发现高性能训练流程。

Result: 工具使用准确率提升9个百分点，发现人类易忽视的有效策略；模型规模扩展引入新挑战，数据规模扩展可实现低成本高效流程发现。

Conclusion: LaMDAgent证明自动化流程探索的有效性，其发现的非直观策略和扩展性分析为降低后训练开发成本提供新方向。

Abstract: Large Language Models (LLMs) have demonstrated exceptional performance across
a wide range of tasks. To further tailor LLMs to specific domains or
applications, post-training techniques such as Supervised Fine-Tuning (SFT),
Preference Learning, and model merging are commonly employed. While each of
these methods has been extensively studied in isolation, the automated
construction of complete post-training pipelines remains an underexplored area.
Existing approaches typically rely on manual design or focus narrowly on
optimizing individual components, such as data ordering or merging strategies.
In this work, we introduce LaMDAgent (short for Language Model Developing
Agent), a novel framework that autonomously constructs and optimizes full
post-training pipelines through the use of LLM-based agents. LaMDAgent
systematically explores diverse model generation techniques, datasets, and
hyperparameter configurations, leveraging task-based feedback to discover
high-performing pipelines with minimal human intervention. Our experiments show
that LaMDAgent improves tool-use accuracy by 9.0 points while preserving
instruction-following capabilities. Moreover, it uncovers effective
post-training strategies that are often overlooked by conventional human-driven
exploration. We further analyze the impact of data and model size scaling to
reduce computational costs on the exploration, finding that model size scalings
introduces new challenges, whereas scaling data size enables cost-effective
pipeline discovery.

</details>


### [31] [Seeing the Threat: Vulnerabilities in Vision-Language Models to Adversarial Attack](https://arxiv.org/abs/2505.21967)
*Juan Ren,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 该研究揭示大型视觉语言模型(LVLMs)存在安全漏洞，提出两阶段对抗攻击评估框架，并建立安全对齐的规范方案


<details>
  <summary>Details</summary>
Motivation: 视觉输入的整合使LVLMs面临新型安全威胁，传统对抗攻击可绕过其安全机制

Method: 通过系统性表征分析定位漏洞根源，设计包含攻击效果分级和拒绝行为量化的评估框架，提出安全响应规范方案

Result: 成功解析对抗攻击的生效机制，建立细粒度评估指标体系，明确模型应对有害提示的理想行为模式

Conclusion: 研究为多模态系统的安全对齐提供了系统化评估工具和理论框架，推动更健壮的防御机制发展

Abstract: Large Vision-Language Models (LVLMs) have shown remarkable capabilities
across a wide range of multimodal tasks. However, their integration of visual
inputs introduces expanded attack surfaces, thereby exposing them to novel
security vulnerabilities. In this work, we conduct a systematic
representational analysis to uncover why conventional adversarial attacks can
circumvent the safety mechanisms embedded in LVLMs. We further propose a novel
two stage evaluation framework for adversarial attacks on LVLMs. The first
stage differentiates among instruction non compliance, outright refusal, and
successful adversarial exploitation. The second stage quantifies the degree to
which the model's output fulfills the harmful intent of the adversarial prompt,
while categorizing refusal behavior into direct refusals, soft refusals, and
partial refusals that remain inadvertently helpful. Finally, we introduce a
normative schema that defines idealized model behavior when confronted with
harmful prompts, offering a principled target for safety alignment in
multimodal systems.

</details>


### [32] [Pearl: A Multimodal Culturally-Aware Arabic Instruction Dataset](https://arxiv.org/abs/2505.21979)
*Fakhraddin Alwajih,Samar Mohamed Magdy,Abdellah El Mekki,Omer Nacar,Youssef Nafea,Safaa Taher Abdelfadil,Abdulfattah Mohammed Yahya,Hamzah Luqman,Nada Almarwani,Samah Aloufi,Baraah Qawasmeh,Houdaifa Atou,Serry Sibaee,Hamzah A. Alsayadi,Walid Al-Dhabyani,Maged S. Al-shaibani,Aya El aatar,Nour Qandos,Rahaf Alhamouri,Samar Ahmad,Razan Khassib,Lina Hamad,Mohammed Anwar AL-Ghrawi,Fatimah Alshamari,Cheikh Malainine,Doaa Qawasmeh,Aminetou Yacoub,Tfeil moilid,Ruwa AbuHweidi,Ahmed Aboeitta,Vatimetou Mohamed Lemin,Reem Abdel-Salam,Ahlam Bashiti,Adel Ammar,Aisha Alansari,Ahmed Ashraf,Nora Alturayeif,Sara Shatnawi,Alcides Alcoba Inciarte,AbdelRahim A. Elmadany,Mohamedou cheikh tourad,Ismail Berrada,Mustafa Jarrar,Shady Shehata,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 提出大规模阿拉伯多模态数据集Pearl，通过文化中心化指令对齐增强多模态模型的文化理解能力


<details>
  <summary>Details</summary>
Motivation: 主流视觉语言模型存在文化偏见，需构建覆盖阿拉伯世界多国文化的多模态数据集

Method: 采用智能体工作流构建，45位阿拉伯地区标注者参与，涵盖10个文化领域，包含Pearl/Pearl-Lite基准及Pearl-X文化差异评估子集

Result: 实验证明基于推理的指令对齐方法比传统扩展方法提升200%文化理解准确率，最佳模型达78.5%准确率

Conclusion: Pearl为文化感知多模态研究奠定基础，所有数据集及评估基准已开源

Abstract: Mainstream large vision-language models (LVLMs) inherently encode cultural
biases, highlighting the need for diverse multimodal datasets. To address this
gap, we introduce Pearl, a large-scale Arabic multimodal dataset and benchmark
explicitly designed for cultural understanding. Constructed through advanced
agentic workflows and extensive human-in-the-loop annotations by 45 annotators
from across the Arab world, Pearl comprises over K multimodal examples spanning
ten culturally significant domains covering all Arab countries. We further
provide two robust evaluation benchmarks Pearl and Pearl-Lite along with a
specialized subset Pearl-X explicitly developed to assess nuanced cultural
variations. Comprehensive evaluations on state-of-the-art open and proprietary
LVLMs demonstrate that reasoning-centric instruction alignment substantially
improves models' cultural grounding compared to conventional scaling methods.
Pearl establishes a foundational resource for advancing culturally-informed
multimodal modeling research. All datasets and benchmarks are publicly
available.

</details>


### [33] [Leveraging Interview-Informed LLMs to Model Survey Responses: Comparative Insights from AI-Generated and Human Data](https://arxiv.org/abs/2505.21997)
*Jihong Zhang,Xinya Liang,Anqi Deng,Nicole Bonge,Lin Tan,Ling Zhang,Nicole Zarrett*

Main category: cs.CL

TL;DR: 大型语言模型（LLM）在访谈指导下可预测人类问卷回答模式，但存在响应变异性低等问题，提示设计和温度参数设置对结果质量有显著影响。


<details>
  <summary>Details</summary>
Motivation: 混合研究方法面临定量与定性数据整合的挑战，研究旨在探索LLM如何通过定性访谈数据生成可靠的定量调查响应，弥合两类方法间的鸿沟。

Method: 使用锻炼行为调节问卷（BREQ）和课后项目人员访谈数据，通过不同LLM模型（Claude/GPT等）生成合成问卷响应，评估其与人类响应的模式匹配度、变异性及人口统计信息影响。

Result: LLM能捕捉整体响应模式但变异性较低；访谈数据提升部分模型响应多样性；精心设计的提示词和低温参数可改善对齐效果；人口统计因素影响弱于访谈内容。

Conclusion: 访谈引导的LLM具备连接定性与定量研究的潜力，但需改进响应变异性、情感解释和测量学效度。未来应优化提示设计、减少偏见并完善模型参数设置。

Abstract: Mixed methods research integrates quantitative and qualitative data but faces
challenges in aligning their distinct structures, particularly in examining
measurement characteristics and individual response patterns. Advances in large
language models (LLMs) offer promising solutions by generating synthetic survey
responses informed by qualitative data. This study investigates whether LLMs,
guided by personal interviews, can reliably predict human survey responses,
using the Behavioral Regulations in Exercise Questionnaire (BREQ) and
interviews from after-school program staff as a case study. Results indicate
that LLMs capture overall response patterns but exhibit lower variability than
humans. Incorporating interview data improves response diversity for some
models (e.g., Claude, GPT), while well-crafted prompts and low-temperature
settings enhance alignment between LLM and human responses. Demographic
information had less impact than interview content on alignment accuracy. These
findings underscore the potential of interview-informed LLMs to bridge
qualitative and quantitative methodologies while revealing limitations in
response variability, emotional interpretation, and psychometric fidelity.
Future research should refine prompt design, explore bias mitigation, and
optimize model settings to enhance the validity of LLM-generated survey data in
social science research.

</details>


### [34] [Found in Translation: Measuring Multilingual LLM Consistency as Simple as Translate then Evaluate](https://arxiv.org/abs/2505.21999)
*Ashim Gupta,Maitrey Mehta,Zhichao Xu,Vivek Srikumar*

Main category: cs.CL

TL;DR: 本文提出跨语言一致性评估框架，揭示主流大语言模型在30种语言间存在显著回答不一致问题


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖昂贵标注数据集，且开放式生成任务的评估存在困难，需系统性评估模型的多语言一致性

Method: 基于「翻译后评估」策略，从信息一致性和情感一致性两个维度建立评估框架

Result: 模型在不同语系和文字体系中表现差异显著，特定语言家族存在严重性能缺陷

Conclusion: 强调多维跨语言评估的必要性，建议采用该框架作为未来多语言模型基准测试工具

Abstract: Large language models (LLMs) provide detailed and impressive responses to
queries in English. However, are they really consistent at responding to the
same query in other languages? The popular way of evaluating for multilingual
performance of LLMs requires expensive-to-collect annotated datasets. Further,
evaluating for tasks like open-ended generation, where multiple correct answers
may exist, is nontrivial. Instead, we propose to evaluate the predictability of
model response across different languages. In this work, we propose a framework
to evaluate LLM's cross-lingual consistency based on a simple Translate then
Evaluate strategy. We instantiate this evaluation framework along two
dimensions of consistency: information and empathy. Our results reveal
pronounced inconsistencies in popular LLM responses across thirty languages,
with severe performance deficits in certain language families and scripts,
underscoring critical weaknesses in their multilingual capabilities. These
findings necessitate cross-lingual evaluations that are consistent along
multiple dimensions. We invite practitioners to use our framework for future
multilingual LLM benchmarking.

</details>


### [35] [Legal Assist AI: Leveraging Transformer-Based Model for Effective Legal Assistance](https://arxiv.org/abs/2505.22003)
*Jatin Gupta,Akhil Sharma,Saransh Singhania,Ali Imam Abidi*

Main category: cs.CL

TL;DR: 基于Transformer的Legal Assist AI模型通过微调印度法律数据集，在AIBE评估中以60.08%分数超越GPT-3.5 Turbo等模型，实现可靠法律问答。


<details>
  <summary>Details</summary>
Motivation: 解决印度公民因法律信息壁垒无法有效行使权利的现状，填补法律援助技术空白。

Method: 采用Transformer架构，微调印度宪法/BNS/BNSS等专业数据集，构建法律领域专用LLM。

Result: AIBE评估得分60.08%，法律推理准确率优于主流模型且无幻觉现象，展现专业优势。

Conclusion: 模型证明法律AI实用性，未来将扩展多语言支持与案例数据库，提升应用广度。

Abstract: Pursuit of accessible legal assistance in India faces a critical gap, as many
citizens struggle to leverage their legal rights due to limited awareness and
access to relevant legal information. This paper introduces Legal Assist AI, a
transformer-based model designed to bridge this gap by offering effective legal
assistance through large language models (LLMs). The system retrieves relevant
legal information from a curated database and generates accurate responses,
enabling effective assistance for diverse users, including legal professionals,
scholars, and the general public. The model was fine-tuned on extensive
datasets from the Indian legal domain, including Indian Constitution, Bharatiya
Nyaya Sanhita (BNS), Bharatiya Nagarik Suraksha Sanhita (BNSS) and so forth,
providing a robust understanding of the complexities of Indian law. By
incorporating domain-specific legal datasets, the proposed model demonstrated
remarkable efficiency and specialization in legal Question-Answering. The model
was evaluated against state-of-the-art models such as GPT-3.5 Turbo and Mistral
7B, achieving a 60.08% score on the AIBE, outperforming its competitors in
legal reasoning and accuracy. Unlike other models, Legal Assist AI avoided
common issues such as hallucinations, making it highly reliable for practical
legal applications. It showcases the model's applicability in real-world legal
scenarios, with future iterations aiming to enhance performance and expand its
dataset to cover a broader range of multilingual and case-specific queries as
well.

</details>


### [36] [CoThink: Token-Efficient Reasoning via Instruct Models Guiding Reasoning Models](https://arxiv.org/abs/2505.22017)
*Siqi Fan,Peng Han,Shuo Shang,Yequan Wang,Aixin Sun*

Main category: cs.CL

TL;DR: 论文提出CoThink方法，通过指导模型预生成方案大纲+推理模型动态调整深度，在保持精度的同时减少22.3%的token消耗，揭示了LLM推理效率的潜在扩展规律


<details>
  <summary>Details</summary>
Motivation: 现有推理优化模型存在过度思考问题：1) 强化学习降低前向推理信息密度 2) 反向思维链训练引发冗余验证。LLM无法评估问题难度，导致简单任务也采用保守策略造成资源浪费

Method: CoThink两阶段框架：1) 指导模型生成高层解决方案大纲 2) 推理模型根据大纲动态调整具体推理深度，避免不必要的验证步骤

Result: 在GSM8K/MATH500/AIME24数据集上测试，使用DAPO/DeepSeek-R1/QwQ模型实现：1) 总token生成减少22.3% 2) pass@1准确率仅下降0.42% 3) 发现推理效率与模型规模的潜在幂律关系

Conclusion: 通过解耦大纲生成与执行推理，CoThink实现动态计算分配。实验证明该方法可突破当前LLM的推理效率瓶颈，并为后续推理优化提供可量化的评估框架（推理效率=精度/计算量）

Abstract: Large language models (LLMs) benefit from increased test-time compute, a
phenomenon known as test-time scaling. However, reasoning-optimized models
often overthink even simple problems, producing excessively verbose outputs and
leading to low token efficiency. By comparing these models with equally sized
instruct models, we identify two key causes of this verbosity: (1)
reinforcement learning reduces the information density of forward reasoning,
and (2) backward chain-of thought training encourages redundant and often
unnecessary verification steps. Since LLMs cannot assess the difficulty of a
given problem, they tend to apply the same cautious reasoning strategy across
all tasks, resulting in inefficient overthinking. To address this, we propose
CoThink, an embarrassingly simple pipeline: an instruct model first drafts a
high-level solution outline; a reasoning model then works out the solution. We
observe that CoThink enables dynamic adjustment of reasoning depth based on
input difficulty. Evaluated with three reasoning models DAPO, DeepSeek-R1, and
QwQ on three datasets GSM8K, MATH500, and AIME24, CoThink reduces total token
generation by 22.3% while maintaining pass@1 accuracy within a 0.42% margin on
average. With reference to the instruct model, we formally define reasoning
efficiency and observe a potential reasoning efficiency scaling law in LLMs.

</details>


### [37] [Improving Continual Pre-training Through Seamless Data Packing](https://arxiv.org/abs/2505.22018)
*Ruicheng Yin,Xuan Gao,Changze Lv,Xiaohua Wang,Xiaoqing Zheng,Xuanjing Huang*

Main category: cs.CL

TL;DR: 提出Seamless Packing数据打包策略，通过两阶段优化解决传统数据截断与上下文断裂问题，实验证明在99%场景中优于基线方法


<details>
  <summary>Details</summary>
Motivation: 传统固定长度序列打包方法导致文本过度截断和上下文不连贯，影响模型持续预训练效果

Method: 1. 第一阶段采用滑动窗口同步重叠token保证上下文连贯性 2. 第二阶段使用FFD算法动态打包短文本减少填充

Result: 在多架构模型和语料域验证中，SP方法在99%实验设置中超越传统基线方法

Conclusion: 通过优化数据工程实现更高效的持续预训练，同步开源代码推动领域发展

Abstract: Continual pre-training has demonstrated significant potential in enhancing
model performance, particularly in domain-specific scenarios. The most common
approach for packing data before continual pre-training involves concatenating
input texts and splitting them into fixed-length sequences. While
straightforward and efficient, this method often leads to excessive truncation
and context discontinuity, which can hinder model performance. To address these
issues, we explore the potential of data engineering to enhance continual
pre-training, particularly its impact on model performance and efficiency. We
propose Seamless Packing (SP), a novel data packing strategy aimed at
preserving contextual information more effectively and enhancing model
performance. Our approach employs a sliding window technique in the first stage
that synchronizes overlapping tokens across consecutive sequences, ensuring
better continuity and contextual coherence. In the second stage, we adopt a
First-Fit-Decreasing algorithm to pack shorter texts into bins slightly larger
than the target sequence length, thereby minimizing padding and truncation.
Empirical evaluations across various model architectures and corpus domains
demonstrate the effectiveness of our method, outperforming baseline method in
99% of all settings. Code is available at
https://github.com/Infernus-WIND/Seamless-Packing.

</details>


### [38] [VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning](https://arxiv.org/abs/2505.22019)
*Qiuchen Wang,Ruixue Ding,Yu Zeng,Zehui Chen,Lin Chen,Shihang Wang,Pengjun Xie,Fei Huang,Feng Zhao*

Main category: cs.CL

TL;DR: 提出VRAG-RL强化学习框架，通过动作空间设计和集成奖励机制优化视觉信息的检索与推理，解决传统RAG方法在视觉信息处理中的推理不足问题。代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在处理视觉信息时存在两大局限：1）文本方法无法处理视觉内容，视觉RAG方法受固定流程限制且推理能力不足；2）模型与检索系统交互时因查询表达不准确导致性能下降。

Method: 1）构建基于强化学习的VRAG-RL框架，定义包含裁剪/缩放等操作的视觉动作空间；2）引入视觉感知令牌实现多轮轨迹采样；3）设计综合查询重写、检索性能和模型反馈的奖励机制。

Result: 框架通过强化学习策略优化VLM模型，有效提升视觉信息的细粒度感知能力和检索查询表达准确性，实验结果验证其在复杂视觉推理任务中的有效性。

Conclusion: VRAG-RL创新地将强化学习应用于多模态RAG领域，通过动作空间设计和复合奖励机制突破视觉信息处理的瓶颈，为实际应用提供了新范式。

Abstract: Effectively retrieving, reasoning and understanding visually rich information
remains a challenge for RAG methods. Traditional text-based methods cannot
handle visual-related information. On the other hand, current vision-based RAG
approaches are often limited by fixed pipelines and frequently struggle to
reason effectively due to the insufficient activation of the fundamental
capabilities of models. As RL has been proven to be beneficial for model
reasoning, we introduce VRAG-RL, a novel RL framework tailored for complex
reasoning across visually rich information. With this framework, VLMs interact
with search engines, autonomously sampling single-turn or multi-turn reasoning
trajectories with the help of visual perception tokens and undergoing continual
optimization based on these samples. Our approach highlights key limitations of
RL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely
incorporate images into the context, leading to insufficient reasoning token
allocation and neglecting visual-specific perception; and (ii) When models
interact with search engines, their queries often fail to retrieve relevant
information due to the inability to articulate requirements, thereby leading to
suboptimal performance. To address these challenges, we define an action space
tailored for visually rich inputs, with actions including cropping and scaling,
allowing the model to gather information from a coarse-to-fine perspective.
Furthermore, to bridge the gap between users' original inquiries and the
retriever, we employ a simple yet effective reward that integrates query
rewriting and retrieval performance with a model-based reward. Our VRAG-RL
optimizes VLMs for RAG tasks using specially designed RL strategies, aligning
the model with real-world applications. The code is available at
\hyperlink{https://github.com/Alibaba-NLP/VRAG}{https://github.com/Alibaba-NLP/VRAG}.

</details>


### [39] [Jailbreak Distillation: Renewable Safety Benchmarking](https://arxiv.org/abs/2505.22037)
*Jingyu Zhang,Ahmed Elgohary,Xiawei Wang,A S M Iftekhar,Ahmed Magooda,Benjamin Van Durme,Daniel Khashabi,Kyle Jackson*

Main category: cs.CL

TL;DR: JBDistill框架通过自动化生成和更新安全基准，显著提升LLM安全评估的效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在关键领域快速部署，传统安全基准存在更新滞后、人工依赖性强、难以公平比较等问题，亟需开发可持续更新的自动化评估体系。

Method: 1. 使用少量开发模型和现有越狱算法生成候选提示池；2. 通过提示选择算法筛选高效子集构建基准；3. 采用跨模型一致性评估框架确保公平性。

Result: 在13个未参与训练的不同类型LLM（含专有模型和新型模型）上实现：1. 评估效果显著优于现有基准；2. 保持87%的高分离性和0.81的多样性指数。

Conclusion: JBDistill建立了可持续、易更新、强适应的安全评估范式，通过自动化基准生成机制有效解决传统方法的污染和过时问题。

Abstract: Large language models (LLMs) are rapidly deployed in critical applications,
raising urgent needs for robust safety benchmarking. We propose Jailbreak
Distillation (JBDistill), a novel benchmark construction framework that
"distills" jailbreak attacks into high-quality and easily-updatable safety
benchmarks. JBDistill utilizes a small set of development models and existing
jailbreak attack algorithms to create a candidate prompt pool, then employs
prompt selection algorithms to identify an effective subset of prompts as
safety benchmarks. JBDistill addresses challenges in existing safety
evaluation: the use of consistent evaluation prompts across models ensures fair
comparisons and reproducibility. It requires minimal human effort to rerun the
JBDistill pipeline and produce updated benchmarks, alleviating concerns on
saturation and contamination. Extensive experiments demonstrate our benchmarks
generalize robustly to 13 diverse evaluation models held out from benchmark
construction, including proprietary, specialized, and newer-generation LLMs,
significantly outperforming existing safety benchmarks in effectiveness while
maintaining high separability and diversity. Our framework thus provides an
effective, sustainable, and adaptable solution for streamlining safety
evaluation.

</details>


### [40] [Voice Adaptation for Swiss German](https://arxiv.org/abs/2505.22054)
*Samuel Stucki,Jan Deriu,Mark Cieliebak*

Main category: cs.CL

TL;DR: 该研究通过处理瑞士德语播客数据集并微调XTTSv2模型，成功实现了标准德语到瑞士德语方言的语音转换


<details>
  <summary>Details</summary>
Motivation: 将语音克隆技术适配到资源不足的语言（瑞士德语方言），验证语音适应模型的可行性

Method: 预处理自动标注的5000小时瑞士播客数据，使用XTTSv2模型进行微调训练

Result: 模型在人工和自动评估中获得CMOS -0.28和SMOS 3.8的评分，能准确生成目标方言

Conclusion: 验证了语音克隆技术向小语种迁移的有效路径，为资源不足语言的语音合成提供解决方案

Abstract: This work investigates the performance of Voice Adaptation models for Swiss
German dialects, i.e., translating Standard German text to Swiss German dialect
speech. For this, we preprocess a large dataset of Swiss podcasts, which we
automatically transcribe and annotate with dialect classes, yielding
approximately 5000 hours of weakly labeled training material. We fine-tune the
XTTSv2 model on this dataset and show that it achieves good scores in human and
automated evaluations and can correctly render the desired dialect. Our work
shows a step towards adapting Voice Cloning technology to underrepresented
languages. The resulting model achieves CMOS scores of up to -0.28 and SMOS
scores of 3.8.

</details>


### [41] [Safeguarding Privacy of Retrieval Data against Membership Inference Attacks: Is This Query Too Close to Home?](https://arxiv.org/abs/2505.22061)
*Yujin Choi,Youngjoo Park,Junyoung Byun,Jaewook Lee,Jinseong Park*

Main category: cs.CL

TL;DR: 提出Mirabel框架，通过相似性检测防御检索增强生成（RAG）系统中的成员推理攻击（MIA），在保护数据隐私的同时保持系统可用性。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统直接将私有检索文档输入大语言模型会暴露成员推理攻击漏洞，攻击者可通过查询相似性推断目标数据是否存在于私有数据库中。

Method: 基于MIA攻击查询与目标文档高相似性的特征，设计相似性检测框架Mirabel，采用检测-隐藏策略模糊攻击者，保持数据效用且兼容现有系统。

Result: 实验证明Mirabel能有效检测多种MIA攻击，防御成功率显著，并适配不同私有RAG系统。

Conclusion: Mirabel首次实现RAG系统中隐私保护与功能效用的平衡，为AI安全防御提供新范式。

Abstract: Retrieval-augmented generation (RAG) mitigates the hallucination problem in
large language models (LLMs) and has proven effective for specific,
personalized applications. However, passing private retrieved documents
directly to LLMs introduces vulnerability to membership inference attacks
(MIAs), which try to determine whether the target datum exists in the private
external database or not. Based on the insight that MIA queries typically
exhibit high similarity to only one target document, we introduce Mirabel, a
similarity-based MIA detection framework designed for the RAG system. With the
proposed Mirabel, we show that simple detect-and-hide strategies can
successfully obfuscate attackers, maintain data utility, and remain
system-agnostic. We experimentally prove its detection and defense against
various state-of-the-art MIA methods and its adaptability to existing private
RAG systems.

</details>


### [42] [Beyond path selection: Better LLMs for Scientific Information Extraction with MimicSFT and Relevance and Rule-induced(R$^2$)GRPO](https://arxiv.org/abs/2505.22068)
*Ran Li,Shimin Di,Yuchen Liu,Chen Jing,Yu Qiu,Lei Chen*

Main category: cs.CL

TL;DR: 提出两阶段训练方法MimicSFT+R²GRPO，通过结构化模板和规则奖励提升大模型在科学信息抽取中的推理能力


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法仅优化推理路径而无法提升推理能力，且LLMs在需要记忆与推理并重的科学信息抽取任务中表现落后于小模型

Method: 1.MimicSFT阶段使用结构化推理模板生成伪标注 2.R²GRPO阶段融合相关性奖励和规则奖励进行强化学习

Result: 在科学信息抽取基准中，R²GRPO配合MimicSFT的关系抽取效果超过基线LLMs和专用监督模型

Conclusion: 证明SFT和RL方法的协同作用可同时优化推理路径与能力，为复杂NLP任务提供新训练范式

Abstract: Previous study suggest that powerful Large Language Models (LLMs) trained
with Reinforcement Learning with Verifiable Rewards (RLVR) only refines
reasoning path without improving the reasoning capacity in math tasks while
supervised-finetuning(SFT) with distillation can. We study this from the view
of Scientific information extraction (SciIE) where LLMs and reasoning LLMs
underperforms small Bert-based models. SciIE require both the reasoning and
memorization. We argue that both SFT and RLVR can refine the reasoning path and
improve reasoning capacity in a simple way based on SciIE. We propose two-stage
training with 1. MimicSFT, using structured reasoning templates without needing
high-quality chain-of-thought data, 2. R$^2$GRPO with relevance and
rule-induced rewards. Experiments on scientific IE benchmarks show that both
methods can improve the reasoning capacity. R$^2$GRPO with mimicSFT surpasses
baseline LLMs and specialized supervised models in relation extraction. Our
code is available at https://github.com/ranlislz/R2GRPO.

</details>


### [43] [ArgInstruct: Specialized Instruction Fine-Tuning for Computational Argumentation](https://arxiv.org/abs/2505.22076)
*Maja Stahl,Timon Ziegenbein,Joonsuk Park,Henning Wachsmuth*

Main category: cs.CL

TL;DR: 通过计算论证领域的专门指令微调增强LLMs的领域任务处理能力，同时保持通用NLP性能


<details>
  <summary>Details</summary>
Motivation: 现有指令微调后的LLMs在处理需要领域知识（如计算论证）的任务时存在局限性，需通过领域专业化提升效果

Method: 1. 构建105个计算论证任务指令集 2. 开发专用评估基准 3. 采用改进的self-instruct方法合成52k领域指令训练模型

Result: 领域指令微调使LLM在seen/unseen计算论证任务上表现显著提升，SuperNI通用NLP基准性能保持稳定

Conclusion: 领域专业化指令微调能有效提升目标领域任务性能，且不会损害模型的通用任务处理能力

Abstract: Training large language models (LLMs) to follow instructions has
significantly enhanced their ability to tackle unseen tasks. However, despite
their strong generalization capabilities, instruction-following LLMs encounter
difficulties when dealing with tasks that require domain knowledge. This work
introduces a specialized instruction fine-tuning for the domain of
computational argumentation (CA). The goal is to enable an LLM to effectively
tackle any unseen CA tasks while preserving its generalization capabilities.
Reviewing existing CA research, we crafted natural language instructions for
105 CA tasks to this end. On this basis, we developed a CA-specific benchmark
for LLMs that allows for a comprehensive evaluation of LLMs' capabilities in
solving various CA tasks. We synthesized 52k CA-related instructions, adapting
the self-instruct process to train a CA-specialized instruction-following LLM.
Our experiments suggest that CA-specialized instruction fine-tuning
significantly enhances the LLM on both seen and unseen CA tasks. At the same
time, performance on the general NLP tasks of the SuperNI benchmark remains
stable.

</details>


### [44] [Learning to Route Queries Across Knowledge Bases for Step-wise Retrieval-Augmented Reasoning](https://arxiv.org/abs/2505.22095)
*Chunyi Peng,Zhipeng Xu,Zhenghao Liu,Yishan Li,Yukun Yan,Shuo Wang,Zhiyuan Liu,Yu Gu,Minghe Yu,Ge Yu,Maosong Sun*

Main category: cs.CL

TL;DR: 提出动态多模态检索框架R1-Router，通过Step-GRPO强化学习算法优化推理路径，提升问答任务性能7%+


<details>
  <summary>Details</summary>
Motivation: 现有MRAG方法采用静态检索策略，未能利用MLLMs的动态推理能力决定知识库交互方式

Method: R1-Router框架实时生成中间查询路由到最适知识库，结合Step-wise GRPO算法分步优化推理行为

Result: 跨模态开放域QA基准测试显示效率与精度双提升，减少28%冗余检索

Conclusion: 动态知识路由机制显著增强多模态推理的适应性，为检索增强生成提供新范式

Abstract: Multimodal Retrieval-Augmented Generation (MRAG) has shown promise in
mitigating hallucinations in Multimodal Large Language Models (MLLMs) by
incorporating external knowledge during generation. Existing MRAG methods
typically adopt a static retrieval pipeline that fetches relevant information
from multiple Knowledge Bases (KBs), followed by a refinement step. However,
these approaches overlook the reasoning and planning capabilities of MLLMs to
dynamically determine how to interact with different KBs during the reasoning
process. To address this limitation, we propose R1-Router, a novel MRAG
framework that learns to decide when and where to retrieve knowledge based on
the evolving reasoning state. Specifically, R1-Router can generate follow-up
queries according to the current reasoning step, routing these intermediate
queries to the most suitable KB, and integrating external knowledge into a
coherent reasoning trajectory to answer the original query. Furthermore, we
introduce Step-wise Group Relative Policy Optimization (Step-GRPO), a tailored
reinforcement learning algorithm that assigns step-specific rewards to optimize
the reasoning behavior of MLLMs. Experimental results on various open-domain QA
benchmarks across multiple modalities demonstrate that R1-Router outperforms
baseline models by over 7%. Further analysis shows that R1-Router can
adaptively and effectively leverage diverse KBs, reducing unnecessary
retrievals and improving both efficiency and accuracy.

</details>


### [45] [Knowledge Base Construction for Knowledge-Augmented Text-to-SQL](https://arxiv.org/abs/2505.22096)
*Jinheon Baek,Horst Samulowitz,Oktie Hassanzadeh,Dharmashankar Subramanian,Sola Shirai,Alfio Gliozzo,Debarun Bhattacharjya*

Main category: cs.CL

TL;DR: 提出通过构建跨领域的知识库来增强大语言模型在Text-to-SQL任务中的准确性，解决LLMs参数知识不足导致的SQL生成错误问题。


<details>
  <summary>Details</summary>
Motivation: 现有依赖LLMs的Text-to-SQL方法因模型参数知识覆盖不足，难以适应多样化的领域特定查询，导致生成SQL准确性下降。

Method: 构建综合知识库，整合所有可用问题、对应数据库模式及其关联知识，支持跨数据集和领域的知识检索与复用。

Result: 在多个文本到SQL数据集测试中，该方法在数据库重叠/非重叠场景下均显著优于基线模型。

Conclusion: 基于知识库的增强策略有效提升了Text-to-SQL的准确性和领域适应性，验证了结构化知识检索的实用性。

Abstract: Text-to-SQL aims to translate natural language queries into SQL statements,
which is practical as it enables anyone to easily retrieve the desired
information from databases. Recently, many existing approaches tackle this
problem with Large Language Models (LLMs), leveraging their strong capability
in understanding user queries and generating corresponding SQL code. Yet, the
parametric knowledge in LLMs might be limited to covering all the diverse and
domain-specific queries that require grounding in various database schemas,
which makes generated SQLs less accurate oftentimes. To tackle this, we propose
constructing the knowledge base for text-to-SQL, a foundational source of
knowledge, from which we retrieve and generate the necessary knowledge for
given queries. In particular, unlike existing approaches that either manually
annotate knowledge or generate only a few pieces of knowledge for each query,
our knowledge base is comprehensive, which is constructed based on a
combination of all the available questions and their associated database
schemas along with their relevant knowledge, and can be reused for unseen
databases from different datasets and domains. We validate our approach on
multiple text-to-SQL datasets, considering both the overlapping and
non-overlapping database scenarios, where it outperforms relevant baselines
substantially.

</details>


### [46] [MemOS: An Operating System for Memory-Augmented Generation (MAG) in Large Language Models](https://arxiv.org/abs/2505.22101)
*Zhiyu Li,Shichao Song,Hanyu Wang,Simin Niu,Ding Chen,Jiawei Yang,Chenyang Xi,Huayi Lai,Jihao Zhao,Yezhaohui Wang,Junpeng Ren,Zehao Lin,Jiahao Huo,Tianyi Chen,Kai Chen,Kehang Li,Zhiqiang Yin,Qingchen Yu,Bo Tang,Hongkang Yang,Zhi-Qin John Xu,Feiyu Xiong*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large Language Models (LLMs) have emerged as foundational infrastructure in
the pursuit of Artificial General Intelligence (AGI). Despite their remarkable
capabilities in language perception and generation, current LLMs fundamentally
lack a unified and structured architecture for handling memory. They primarily
rely on parametric memory (knowledge encoded in model weights) and ephemeral
activation memory (context-limited runtime states). While emerging methods like
Retrieval-Augmented Generation (RAG) incorporate plaintext memory, they lack
lifecycle management and multi-modal integration, limiting their capacity for
long-term knowledge evolution. To address this, we introduce MemOS, a memory
operating system designed for LLMs that, for the first time, elevates memory to
a first-class operational resource. It builds unified mechanisms for
representation, organization, and governance across three core memory types:
parametric, activation, and plaintext. At its core is the MemCube, a
standardized memory abstraction that enables tracking, fusion, and migration of
heterogeneous memory, while offering structured, traceable access across tasks
and contexts. MemOS establishes a memory-centric execution framework with
strong controllability, adaptability, and evolvability. It fills a critical gap
in current LLM infrastructure and lays the groundwork for continual adaptation,
personalized intelligence, and cross-platform coordination in next-generation
intelligent systems.

</details>


### [47] [Curse of High Dimensionality Issue in Transformer for Long-context Modeling](https://arxiv.org/abs/2505.22107)
*Shuhai Zhang,Zeng You,Yaofo Chen,Zhiquan Wen,Qianyue Wang,Zhijie Qiu,Yuanqing Li,Mingkui Tan*

Main category: cs.CL

TL;DR: 提出动态组注意力（DGA），通过聚合不重要令牌显著降低Transformer长上下文建模的计算冗余


<details>
  <summary>Details</summary>
Motivation: 传统注意力机制存在冗余计算问题，所有令牌消耗相同资源，但实际只有少数关键令牌对预测有显著贡献

Method: 1. 将注意力建模重构为监督学习任务，分离相关/无关令牌
2. 理论证明注意力稀疏性
3. 提出基于线性编码的组编码策略
4. 设计动态组注意力机制（DGA）

Result: DGA在保持模型竞争力的同时显著降低计算成本，代码已开源

Conclusion: 组编码策略有效提升模型鲁棒性和学习效率，为注意力优化提供新理论框架

Abstract: Transformer-based large language models (LLMs) excel in natural language
processing tasks by capturing long-range dependencies through self-attention
mechanisms. However, long-context modeling faces significant computational
inefficiencies due to \textit{redundant} attention computations: while
attention weights are often \textit{sparse}, all tokens consume \textit{equal}
computational resources. In this paper, we reformulate traditional
probabilistic sequence modeling as a \textit{supervised learning task},
enabling the separation of relevant and irrelevant tokens and providing a
clearer understanding of redundancy. Based on this reformulation, we
theoretically analyze attention sparsity, revealing that only a few tokens
significantly contribute to predictions. Building on this, we formulate
attention optimization as a linear coding problem and propose a \textit{group
coding strategy}, theoretically showing its ability to improve robustness
against random noise and enhance learning efficiency. Motivated by this, we
propose \textit{Dynamic Group Attention} (DGA), which leverages the group
coding to explicitly reduce redundancy by aggregating less important tokens
during attention computation. Empirical results show that our DGA significantly
reduces computational costs while maintaining competitive performance.Code is
available at https://github.com/bolixinyu/DynamicGroupAttention.

</details>


### [48] [THINK-Bench: Evaluating Thinking Efficiency and Chain-of-Thought Quality of Large Reasoning Models](https://arxiv.org/abs/2505.22113)
*Zhiyuan Li,Yi Chang,Yuan Wu*

Main category: cs.CL

TL;DR: 大型推理模型（LRMs）存在过度思考问题，导致计算效率低下；Think-Bench基准评估发现多数模型在处理简单任务时产生冗余推理链。


<details>
  <summary>Details</summary>
Motivation: 针对LRMs在简单任务中因过度思考（生成冗余令牌）导致计算资源浪费的问题，作者旨在系统研究该问题，提出评估框架和新效率指标。

Method: 提出Think-Bench基准及新型效率指标，从推理过程、结果质量和思维链特性等多维度全面评估各类LRMs。

Result: 研究发现多数LRMs在简单问题上过度思考，生成冗长推理链；部分模型虽思维链质量高，但效率低下。

Conclusion: Think-Bench为LRMs研究提供坚实基础，揭示过度思考问题及效率瓶颈，推动高效推理模型发展。

Abstract: Large reasoning models (LRMs) have achieved impressive performance in complex
tasks, often outperforming conventional large language models (LLMs). However,
the prevalent issue of overthinking severely limits their computational
efficiency. Overthinking occurs when models generate excessive and redundant
tokens that contribute little to accurate outcomes, especially in simple tasks,
resulting in a significant waste of computational resources. To systematically
investigate this issue, we introduce Think-Bench, a benchmark designed to
evaluate the reasoning efficiency of LRMs. We also propose novel efficiency
metrics and conduct a comprehensive evaluation of various LRMs across multiple
dimensions, including the reasoning process, outcome quality, and
chain-of-thought (CoT) characteristics. Our analysis reveals that most LRMs
exhibit overthinking in handling easy questions, generating unnecessarily
lengthy reasoning chains. While many LRMs demonstrate high CoT quality, several
suffer from low efficiency. We hope that Think-Bench can serve as a robust
foundation for advancing research into LRMs.

</details>


### [49] [Multimodal Forecasting of Sparse Intraoperative Hypotension Events Powered by Language Model](https://arxiv.org/abs/2505.22116)
*Jintao Zhang,Zirui Liu,Mingyue Cheng,Shilong Zhang,Tingyue Pan,Qi Liu,Yanhu Xie*

Main category: cs.CL

TL;DR: 提出多模态语言模型IOHFuseLM，通过两阶段训练策略和跨模态对齐，有效预测术中低血压事件并在实验中超越基线模型。


<details>
  <summary>Details</summary>
Motivation: 术中低血压(IOH)与心肌损伤等不良后果相关，但预测面临事件稀疏性和多模态数据融合的挑战。

Method: 采用扩散增强的领域预训练+任务微调两阶段策略，通过token级对齐实现生理时序数据与临床描述的跨模态融合，并将静态属性转化为结构化文本。

Result: 在两个术中数据集上的实验表明，IOHFuseLM在低血压事件识别准确率上显著优于现有基线模型。

Conclusion: 该框架通过数据增强和个性化模态融合有效提升IOH预测性能，开源代码促进临床应用和可重复研究。

Abstract: Intraoperative hypotension (IOH) frequently occurs under general anesthesia
and is strongly linked to adverse outcomes such as myocardial injury and
increased mortality. Despite its significance, IOH prediction is hindered by
event sparsity and the challenge of integrating static and dynamic data across
diverse patients. In this paper, we propose \textbf{IOHFuseLM}, a multimodal
language model framework. To accurately identify and differentiate sparse
hypotensive events, we leverage a two-stage training strategy. The first stage
involves domain adaptive pretraining on IOH physiological time series augmented
through diffusion methods, thereby enhancing the model sensitivity to patterns
associated with hypotension. Subsequently, task fine-tuning is performed on the
original clinical dataset to further enhance the ability to distinguish
normotensive from hypotensive states. To enable multimodal fusion for each
patient, we align structured clinical descriptions with the corresponding
physiological time series at the token level. Such alignment enables the model
to capture individualized temporal patterns alongside their corresponding
clinical semantics. In addition, we convert static patient attributes into
structured text to enrich personalized information. Experimental evaluations on
two intraoperative datasets demonstrate that IOHFuseLM outperforms established
baselines in accurately identifying IOH events, highlighting its applicability
in clinical decision support scenarios. Our code is publicly available to
promote reproducibility at https://github.com/zjt-gpu/IOHFuseLM.

</details>


### [50] [Multilingual vs Crosslingual Retrieval of Fact-Checked Claims: A Tale of Two Approaches](https://arxiv.org/abs/2505.22118)
*Alan Ramponi,Marco Rovera,Robert Moro,Sara Tonelli*

Main category: cs.CL

TL;DR: 研究通过LLM重新排序和基于相似性的负例选择策略，提升47种语言环境下多语言/跨语言事实核查检索效果


<details>
  <summary>Details</summary>
Motivation: 现有事实核查检索系统主要局限于单语言场景，但在全球性事件（疫情/战争/国际政治）中，跨语言检索能力对资源匮乏语言尤为重要

Method: 提出两种改进策略：1) 监督学习中基于句子相似度的负例采样 2) 无监督场景下的LLM重新排序方法

Result: LLM重新排序效果最佳（监督微调次之），实验证明跨语言检索具有区别于多语言环境的独特特性

Conclusion: 跨语言事实核查需专门处理方法，LLM重新排序和针对性负例选择策略可显著提升多语言场景下的检索性能

Abstract: Retrieval of previously fact-checked claims is a well-established task, whose
automation can assist professional fact-checkers in the initial steps of
information verification. Previous works have mostly tackled the task
monolingually, i.e., having both the input and the retrieved claims in the same
language. However, especially for languages with a limited availability of
fact-checks and in case of global narratives, such as pandemics, wars, or
international politics, it is crucial to be able to retrieve claims across
languages. In this work, we examine strategies to improve the multilingual and
crosslingual performance, namely selection of negative examples (in the
supervised) and re-ranking (in the unsupervised setting). We evaluate all
approaches on a dataset containing posts and claims in 47 languages (283
language combinations). We observe that the best results are obtained by using
LLM-based re-ranking, followed by fine-tuning with negative examples sampled
using a sentence similarity-based strategy. Most importantly, we show that
crosslinguality is a setup with its own unique characteristics compared to the
multilingual setup.

</details>


### [51] [LoKI: Low-damage Knowledge Implanting of Large Language Models](https://arxiv.org/abs/2505.22120)
*Runyu Wang,Peng Ping,Zhengyu Guo,Xiaoye Zhang,Quan Shi,Liting Zhou,Tianbo Ji*

Main category: cs.CL

TL;DR: 提出LoKI方法——基于Transformer知识存储机制的低损伤参数高效微调技术，在保持模型通用能力的同时实现高效任务适配


<details>
  <summary>Details</summary>
Motivation: 传统参数高效微调方法（PEFT）在提升特定任务性能时容易导致灾难性遗忘，损害大语言模型的通用能力。现有方法在效率与通用能力保持间存在矛盾

Method: 基于对Transformer架构知识存储机制的机理分析，设计参数植入策略。通过控制微调过程中关键知识存储区域的参数更新幅度，实现新知识的低损伤植入

Result: 在多种模型类型上，LoKI的特定任务性能与全参数微调/LoRA方法相当甚至超越，同时通用能力保留度显著提升（SOTA的权衡表现）

Conclusion: 将LLM知识存储的机理认知与微调目标结合，开创性地实现任务专业化与通用能力保持的最佳平衡。开源代码提供即用型解决方案

Abstract: Fine-tuning adapts pretrained models for specific tasks but poses the risk of
catastrophic forgetting (CF), where critical knowledge from pre-training is
overwritten. Current Parameter-Efficient Fine-Tuning (PEFT) methods for Large
Language Models (LLMs), while efficient, often sacrifice general capabilities.
To address the issue of CF in a general-purpose PEFT framework, we propose
\textbf{Lo}w-damage \textbf{K}nowledge \textbf{I}mplanting (\textbf{LoKI}), a
PEFT technique that is based on a mechanistic understanding of how knowledge is
stored in transformer architectures. In two real-world scenarios, LoKI
demonstrates task-specific performance that is comparable to or even surpasses
that of full fine-tuning and LoRA-based methods across various model types,
while significantly better preserving general capabilities. Our work connects
mechanistic insights into LLM knowledge storage with practical fine-tuning
objectives, achieving state-of-the-art trade-offs between task specialization
and the preservation of general capabilities. Our implementation is publicly
available as ready-to-use code\footnote{https://github.com/Nexround/LoKI}.

</details>


### [52] [EULER: Enhancing the Reasoning Ability of Large Language Models through Error-Induced Learning](https://arxiv.org/abs/2505.22131)
*Zhuoyang Wu,Xinze Li,Zhenghao Liu,Yukun Yan,Zhiyuan Liu,Minghe Yu,Cheng Yang,Yu Gu,Ge Yu,Maosong Sun*

Main category: cs.CL

TL;DR: 提出EULER模型，通过生成高质量解题错误增强大语言模型的数学推理能力，实验显示效果优于基线模型4%以上。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以生成针对每个数学问题的有效错误样本，从错误中学习能显著提升模型在监督微调阶段的性能。

Method: 优化错误暴露模型生成自制错误答案的概率，同时利用优质LLM生成的正确方案进行生成质量正则化。

Result: 在多个数学数据集上实现超过4%的性能提升，生成更具挑战性的教学型错误样本。

Conclusion: EULER能有效提升LLM的数学推理能力，其生成的错误样本对训练过程和推理过程均有助益。

Abstract: Large Language Models (LLMs) have demonstrated strong reasoning capabilities
and achieved promising results in mathematical problem-solving tasks. Learning
from errors offers the potential to further enhance the performance of LLMs
during Supervised Fine-Tuning (SFT). However, the errors in synthesized
solutions are typically gathered from sampling trails, making it challenging to
generate solution errors for each mathematical problem. This paper introduces
the Error-IndUced LEaRning (EULER) model, which aims to develop an error
exposure model that generates high-quality solution errors to enhance the
mathematical reasoning capabilities of LLMs. Specifically, EULER optimizes the
error exposure model to increase the generation probability of self-made
solution errors while utilizing solutions produced by a superior LLM to
regularize the generation quality. Our experiments across various mathematical
problem datasets demonstrate the effectiveness of the EULER model, achieving an
improvement of over 4% compared to all baseline models. Further analysis
reveals that EULER is capable of synthesizing more challenging and educational
solution errors, which facilitate both the training and inference processes of
LLMs. All codes are available at https://github.com/NEUIR/EULER.

</details>


### [53] [RAD: Redundancy-Aware Distillation for Hybrid Models via Self-Speculative Decoding](https://arxiv.org/abs/2505.22135)
*Yuichiro Hoshino,Hideyuki Tachibana,Muneyoshi Inahara,Hiroto Takegawa*

Main category: cs.CL

TL;DR: 提出RAD框架通过自我推测解码识别Transformer冗余注意力层，用SSM组件替代并进行定向蒸馏，显著提升混合模型性能


<details>
  <summary>Details</summary>
Motivation: 混合模型中Transformer组件存在潜在冗余，影响效率优化，需针对性改进

Method: 1) 使用自我推测解码诊断冗余注意力层 2) 选择性替换为SSM组件 3) 基于架构变更进行定向知识蒸馏

Result: 数学和编码任务表现显著超越基线模型（GSM8K 71.27 vs 46.17，CRUX 28.25 vs 22.75），收敛速度提升约2倍

Conclusion: RAD为混合模型蒸馏提供了高效优化路径，实现性能与效率的协同提升

Abstract: Hybrid models combining Transformers and State Space Models (SSMs) are
promising for balancing performance and efficiency. However, optimizing these
hybrid models, particularly by addressing the potential redundancy inherent
within the Transformer components, remains a significant challenge. In this
paper, we propose RAD (Redundancy-Aware Distillation), a novel framework that
uses self-speculative decoding as a diagnostic tool to identify redundant
attention layers within the model. These identified layers are then selectively
replaced with SSM components, followed by targeted (self-)distillation.
Specifically, RAD focuses knowledge transfer on the components identified as
redundant, considering architectural changes and specific weight initialization
strategies. We experimentally demonstrate that self-distillation using RAD
significantly surpasses the performance of the original base model on
mathematical and coding tasks. Furthermore, RAD is also effective in standard
knowledge distillation settings, achieving up to approximately 2x faster
convergence compared to baseline methods. Notably, while a baseline model
distilled from a Llama-3.1 70B teacher achieves scores of 46.17 on GSM8K and
22.75 on CRUX, RAD achieves significantly higher scores of 71.27 on GSM8K and
28.25 on CRUX, even when using a much smaller Llama-3.1 8B teacher. RAD offers
a new pathway for efficient optimization and performance enhancement in the
distillation of hybrid models.

</details>


### [54] [Limited Generalizability in Argument Mining: State-Of-The-Art Models Learn Datasets, Not Arguments](https://arxiv.org/abs/2505.22137)
*Marc Feger,Katarina Boland,Stefan Dietze*

Main category: cs.CL

TL;DR: 大规模验证BERT类模型在论点识别任务中的泛化能力，发现模型依赖词汇捷径导致跨数据集性能下降，但特定预训练策略可提升稳健性


<details>
  <summary>Details</summary>
Motivation: 验证当前SOTA模型是否真正掌握论点识别能力，还是依赖数据集特定词汇线索（如内容词），揭示模型泛化能力的真实边界

Method: 在17个英文句子级数据集上测试4种transformer模型（含对比预训练增强模型），通过跨数据集评估模型泛化表现

Result: 模型普遍依赖词汇捷径，跨数据集时性能下降16-21%；但任务特定预训练+联合训练可使性能提升12-15%

Conclusion: 现有论点挖掘进展可能被高估，需关注任务本质对齐与模型稳健性，建议采用针对性预训练策略提升泛化能力

Abstract: Identifying arguments is a necessary prerequisite for various tasks in
automated discourse analysis, particularly within contexts such as political
debates, online discussions, and scientific reasoning. In addition to
theoretical advances in understanding the constitution of arguments, a
significant body of research has emerged around practical argument mining,
supported by a growing number of publicly available datasets. On these
benchmarks, BERT-like transformers have consistently performed best,
reinforcing the belief that such models are broadly applicable across diverse
contexts of debate. This study offers the first large-scale re-evaluation of
such state-of-the-art models, with a specific focus on their ability to
generalize in identifying arguments. We evaluate four transformers, three
standard and one enhanced with contrastive pre-training for better
generalization, on 17 English sentence-level datasets as most relevant to the
task. Our findings show that, to varying degrees, these models tend to rely on
lexical shortcuts tied to content words, suggesting that apparent progress may
often be driven by dataset-specific cues rather than true task alignment. While
the models achieve strong results on familiar benchmarks, their performance
drops markedly when applied to unseen datasets. Nonetheless, incorporating both
task-specific pre-training and joint benchmark training proves effective in
enhancing both robustness and generalization.

</details>


### [55] [InComeS: Integrating Compression and Selection Mechanisms into LLMs for Efficient Model Editing](https://arxiv.org/abs/2505.22156)
*Shuaiyi Li,Zhisong Zhang,Yang Deng,Chenlong Deng,Tianqing Fang,Hongming Zhang,Haitao Mi,Dong Yu,Wai Lam*

Main category: cs.CL

TL;DR: 提出InComeS框架，通过压缩编辑上下文为KV缓存并动态选择信息，突破LLMs上下文窗口限制，提升多轮编辑效率和性能


<details>
  <summary>Details</summary>
Motivation: 现有模型编辑方法在需要深层语义理解的复杂场景中表现受限，尤其当编辑次数增加时受上下文窗口限制导致性能下降

Method: 1. 将编辑上下文压缩至特殊gist token的KV缓存
2. 添加跨注意力模块动态选择最相关信息
3. 构建gist池实现自适应编辑信息利用

Result: 在多格式模型编辑基准测试中验证了方法的有效性和效率优势

Conclusion: 通过显式压缩与动态选择机制，实现了对模型编辑上下文的灵活高效处理，为LLMs编辑方法提供新思路

Abstract: Although existing model editing methods perform well in recalling exact edit
facts, they often struggle in complex scenarios that require deeper semantic
understanding rather than mere knowledge regurgitation. Leveraging the strong
contextual reasoning abilities of large language models (LLMs), in-context
learning (ICL) becomes a promising editing method by comprehending edit
information through context encoding. However, this method is constrained by
the limited context window of LLMs, leading to degraded performance and
efficiency as the number of edits increases. To overcome this limitation, we
propose InComeS, a flexible framework that enhances LLMs' ability to process
editing contexts through explicit compression and selection mechanisms.
Specifically, InComeS compresses each editing context into the key-value (KV)
cache of a special gist token, enabling efficient handling of multiple edits
without being restricted by the model's context window. Furthermore,
specialized cross-attention modules are added to dynamically select the most
relevant information from the gist pools, enabling adaptive and effective
utilization of edit information. We conduct experiments on diverse model
editing benchmarks with various editing formats, and the results demonstrate
the effectiveness and efficiency of our method.

</details>


### [56] [Stratified Selective Sampling for Instruction Tuning with Dedicated Scoring Strategy](https://arxiv.org/abs/2505.22157)
*Paramita Mirza,Lucas Weber,Fabian Küch*

Main category: cs.CL

TL;DR: 提出高效通用的多步骤数据选择流程，通过分箱、质量评估和轻量级难度评分，结合改进的聚类算法实现高性能低开销的模型微调


<details>
  <summary>Details</summary>
Motivation: 现有数据筛选方法存在高计算成本或领域狭窄的问题，需要兼顾效率与通用性的解决方案来优化多用途模型的微调效果

Method: 采用任务分类控制数据组成+分箱分组→专用模型质量评估→轻量级难度评分的三步流程，改进嵌入模型和聚类算法保证数据多样性

Result: 集成策略在保持模型性能的前提下显著降低计算开销，实现更高效的微调过程

Conclusion: 该综合方法有效平衡了数据选择效率与模型性能，为多任务模型优化提供了可控制数据组成且保证多样性的解决方案

Abstract: Recent work shows that post-training datasets for LLMs can be substantially
downsampled without noticeably deteriorating performance. However, data
selection often incurs high computational costs or is limited to narrow
domains. In this paper, we demonstrate that data selection can be both --
efficient and universal -- by using a multi-step pipeline in which we
efficiently bin data points into groups, estimate quality using specialized
models, and score difficulty with a robust, lightweight method. Task-based
categorization allows us to control the composition of our final data --
crucial for finetuning multi-purpose models. To guarantee diversity, we improve
upon previous work using embedding models and a clustering algorithm. This
integrated strategy enables high-performance fine-tuning with minimal overhead.

</details>


### [57] [Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes](https://arxiv.org/abs/2505.22165)
*Bocheng Li,Zhujin Gao,Linli Xu*

Main category: cs.CL

TL;DR: NeoDiff结合离散与连续扩散模型优势，通过泊松扩散过程和时间预测器实现细粒度控制，在文本生成任务中表现优于现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有离散扩散模型缺乏细粒度控制，连续扩散模型无法灵活调整不同token的扩散进度。两者均难以兼顾语义细节与生成质量。

Method: 提出泊松扩散过程实现灵活噪声注入，引入时间预测器自适应调节去噪进度，并优化推理调度机制。

Result: 在多文本生成任务中，NeoDiff在生成质量上超过非自回归/自回归扩散模型和迭代式方法。

Conclusion: 该模型为文本生成提供了更系统化的框架，展示了扩散模型在自然语言生成领域的重大潜力。

Abstract: Diffusion models have emerged as a promising approach for text generation,
with recent works falling into two main categories: discrete and continuous
diffusion models. Discrete diffusion models apply token corruption
independently using categorical distributions, allowing for different diffusion
progress across tokens but lacking fine-grained control. Continuous diffusion
models map tokens to continuous spaces and apply fine-grained noise, but the
diffusion progress is uniform across tokens, limiting their ability to capture
semantic nuances. To address these limitations, we propose
\textbf{\underline{N}}on-simultan\textbf{\underline{e}}ous
C\textbf{\underline{o}}ntinuous \textbf{\underline{Diff}}usion Models
(NeoDiff), a novel diffusion model that integrates the strengths of both
discrete and continuous approaches. NeoDiff introduces a Poisson diffusion
process for the forward process, enabling a flexible and fine-grained noising
paradigm, and employs a time predictor for the reverse process to adaptively
modulate the denoising progress based on token semantics. Furthermore, NeoDiff
utilizes an optimized schedule for inference to ensure more precise noise
control and improved performance. Our approach unifies the theories of discrete
and continuous diffusion models, offering a more principled and effective
framework for text generation. Experimental results on several text generation
tasks demonstrate NeoDiff's superior performance compared to baselines of
non-autoregressive continuous and discrete diffusion models, iterative-based
methods and autoregressive diffusion-based methods. These results highlight
NeoDiff's potential as a powerful tool for generating high-quality text and
advancing the field of diffusion-based text generation.

</details>


### [58] [ReliableEval: A Recipe for Stochastic LLM Evaluation via Method of Moments](https://arxiv.org/abs/2505.22169)
*Gili Lior,Eliya Habba,Shahar Levy,Avi Caciularu,Gabriel Stanovsky*

Main category: cs.CL

TL;DR: 提出ReliableEval方法通过随机提示扰动评估LLM性能，揭示GPT-4o等顶级模型存在显著提示敏感性


<details>
  <summary>Details</summary>
Motivation: 标准基准测试使用单一提示评估LLM存在可靠性问题，需建立考虑提示敏感性的评估框架

Method: 基于随机矩方法，通过意义保持的提示扰动空间进行概率评估，开发模型/任务/指标无关的ReliableEval方法

Result: GPT-4o和Claude-3.7-Sonnet等前沿模型在多次提示扰动下表现出显著性能波动

Conclusion: ReliableEval为LLM评估提供健壮的标准化方案，证明现有评估方法可能低估模型性能的不稳定性

Abstract: LLMs are highly sensitive to prompt phrasing, yet standard benchmarks
typically report performance using a single prompt, raising concerns about the
reliability of such evaluations. In this work, we argue for a stochastic method
of moments evaluation over the space of meaning-preserving prompt
perturbations. We introduce a formal definition of reliable evaluation that
accounts for prompt sensitivity, and suggest ReliableEval - a method for
estimating the number of prompt resamplings needed to obtain meaningful
results. Using our framework, we stochastically evaluate five frontier LLMs and
find that even top-performing models like GPT-4o and Claude-3.7-Sonnet exhibit
substantial prompt sensitivity. Our approach is model-, task-, and
metric-agnostic, offering a recipe for meaningful and robust LLM evaluation.

</details>


### [59] [Reverse Preference Optimization for Complex Instruction Following](https://arxiv.org/abs/2505.22172)
*Xiang Huang,Ting-En Lin,Feiteng Fang,Yuchuan Wu,Hangyu Li,Yuzhong Qu,Fei Huang,Yongbin Li*

Main category: cs.CL

TL;DR: 提出反向偏好优化(RPO)方法，通过动态反转指令约束减少偏好对噪声，显著提升大型语言模型处理多约束指令的能力，在多轮指令遵循任务中优于DPO基线，并在不同规模模型上有效扩展(70B模型超越GPT-4o)。


<details>
  <summary>Details</summary>
Motivation: 传统方法基于约束满足数量选择偏好对会引入噪声，因优质响应可能被错误标记为拒绝。需要确保选择响应完美且明确优化方向。

Method: 通过动态反转指令约束，确保选择响应完美性，减少采样过滤成本；同时扩大选择与拒绝响应差距，增强优化方向鲁棒性。

Result: 在Sysbench和Multi-IF基准分别提升4.6/2.5分(Llama-3.1 8B)，70B RPO模型超越GPT-4o，验证方法跨模型规模有效性。

Conclusion: RPO有效解决多约束指令对齐难题，通过约束反转机制实现高效优化，证明了方法在多场景、多模型规模下的实用价值。

Abstract: Instruction following (IF) is a critical capability for large language models
(LLMs). However, handling complex instructions with multiple constraints
remains challenging. Previous methods typically select preference pairs based
on the number of constraints they satisfy, introducing noise where chosen
examples may fail to follow some constraints and rejected examples may excel in
certain respects over the chosen ones. To address the challenge of aligning
with multiple preferences, we propose a simple yet effective method called
Reverse Preference Optimization (RPO). It mitigates noise in preference pairs
by dynamically reversing the constraints within the instruction to ensure the
chosen response is perfect, alleviating the burden of extensive sampling and
filtering to collect perfect responses. Besides, reversal also enlarges the gap
between chosen and rejected responses, thereby clarifying the optimization
direction and making it more robust to noise. We evaluate RPO on two multi-turn
IF benchmarks, Sysbench and Multi-IF, demonstrating average improvements over
the DPO baseline of 4.6 and 2.5 points (on Llama-3.1 8B), respectively.
Moreover, RPO scales effectively across model sizes (8B to 70B parameters),
with the 70B RPO model surpassing GPT-4o.

</details>


### [60] [TabXEval: Why this is a Bad Table? An eXhaustive Rubric for Table Evaluation](https://arxiv.org/abs/2505.22176)
*Vihang Pancholi,Jainit Bafna,Tejas Anvekar,Manish Shrivastava,Vivek Gupta*

Main category: cs.CL

TL;DR: 提出新型表格评估框架TabXEval，通过结构化对齐和系统化比较实现全面评估，在跨领域任务中展现优越性能


<details>
  <summary>Details</summary>
Motivation: 传统表格评估方法难以捕捉结构差异和上下文细节，无法满足复杂表格分析需求

Method: 两阶段框架：1) TabAlign实现结构对齐 2) TabCompare进行语义句法系统对比，结合多级结构描述符和量化指标

Result: 基于自建多领域基准TabXBench验证，显示该方法在敏感性和特异性方面优于传统评估体系

Conclusion: 建立可解释的表格评估新范式，为未来表格分析技术创新提供系统化评估基础

Abstract: Evaluating tables qualitatively & quantitatively presents a significant
challenge, as traditional metrics often fail to capture nuanced structural and
content discrepancies. To address this, we introduce a novel, methodical rubric
integrating multi-level structural descriptors with fine-grained contextual
quantification, thereby establishing a robust foundation for comprehensive
table comparison. Building on this foundation, we propose TabXEval, an
eXhaustive and eXplainable two-phase evaluation framework. TabXEval initially
aligns reference tables structurally via TabAlign & subsequently conducts a
systematic semantic and syntactic comparison using TabCompare; this approach
clarifies the evaluation process and pinpoints subtle discrepancies overlooked
by conventional methods. The efficacy of this framework is assessed using
TabXBench, a novel, diverse, multi-domain benchmark we developed, featuring
realistic table perturbations and human-annotated assessments. Finally, a
systematic analysis of existing evaluation methods through
sensitivity-specificity trade-offs demonstrates the qualitative and
quantitative effectiveness of TabXEval across diverse table-related tasks and
domains, paving the way for future innovations in explainable table evaluation.

</details>


### [61] [Speculative Decoding Meets Quantization: Compatibility Evaluation and Hierarchical Framework Design](https://arxiv.org/abs/2505.22179)
*Yudi Zhang,Weilin Zhao,Xu Han,Tiejun Zhao,Wang Xu,Hailong Cao,Conghui Zhu*

Main category: cs.CL

TL;DR: 提出分层推测解码框架，结合4位量化与推测解码技术，实现LLM推理加速2.78倍


<details>
  <summary>Details</summary>
Motivation: 在整合推测解码与量化技术时，发现4位权重量化的内存优势被推测解码的计算负载抵消，树状草稿验证导致时间开销显著增加

Method: 设计分层框架：使用小模型作为中间阶段将树状草稿转为序列草稿，利用量化模型的内存访问优势

Result: 4位Llama-3-70B在A100上实现2.78×加速，超越EAGLE-2方法1.31倍

Conclusion: 通过架构创新有效结合量化与推测解码技术，为低比特量化模型的实际部署提供新解决方案

Abstract: Speculative decoding and quantization effectively accelerate memory-bound
inference of large language models. Speculative decoding mitigates the memory
bandwidth bottleneck by verifying multiple tokens within a single forward pass,
which increases computational effort. Quantization achieves this optimization
by compressing weights and activations into lower bit-widths and also reduces
computations via low-bit matrix multiplications. To further leverage their
strengths, we investigate the integration of these two techniques.
Surprisingly, experiments applying the advanced speculative decoding method
EAGLE-2 to various quantized models reveal that the memory benefits from 4-bit
weight quantization are diminished by the computational load from speculative
decoding. Specifically, verifying a tree-style draft incurs significantly more
time overhead than a single-token forward pass on 4-bit weight quantized
models. This finding led to our new speculative decoding design: a hierarchical
framework that employs a small model as an intermediate stage to turn
tree-style drafts into sequence drafts, leveraging the memory access benefits
of the target quantized model. Experimental results show that our hierarchical
approach achieves a 2.78$\times$ speedup across various tasks for the 4-bit
weight Llama-3-70B model on an A100 GPU, outperforming EAGLE-2 by 1.31$\times$.
Code available at https://github.com/AI9Stars/SpecMQuant.

</details>


### [62] [Breaking the Cloak! Unveiling Chinese Cloaked Toxicity with Homophone Graph and Toxic Lexicon](https://arxiv.org/abs/2505.22184)
*Xuchen Ma,Jianxiang Yu,Wenming Shao,Bo Pang,Xiang Li*

Main category: cs.CL

TL;DR: 提出无需训练和提示的C²TU方法，通过子串匹配和语义过滤有效识别中文同形字伪装的毒性内容，实验显示性能显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 社交媒体中用户通过同形字伪装逃避内容审查，现有方法主要针对英文设计，中文隐蔽毒性检测尚未有效解决

Method: 1. 基于中文同形字和毒性词典的候选词匹配 2. BERT/LLM双路径过滤机制：利用完整语义上下文解决LLM自回归限制 3. 毒性校正模块

Result: 在两个中文毒性数据集上F1值提升71%，准确率提升35%，显著超越现有最佳方法

Conclusion: C²TU首次实现中文隐蔽毒性内容检测，无需训练且效果显著，为中文内容审查提供有效解决方案

Abstract: Social media platforms have experienced a significant rise in toxic content,
including abusive language and discriminatory remarks, presenting growing
challenges for content moderation. Some users evade censorship by deliberately
disguising toxic words through homophonic cloak, which necessitates the task of
unveiling cloaked toxicity. Existing methods are mostly designed for English
texts, while Chinese cloaked toxicity unveiling has not been solved yet. To
tackle the issue, we propose C$^2$TU, a novel training-free and prompt-free
method for Chinese cloaked toxic content unveiling. It first employs substring
matching to identify candidate toxic words based on Chinese homo-graph and
toxic lexicon. Then it filters those candidates that are non-toxic and corrects
cloaks to be their corresponding toxicities. Specifically, we develop two model
variants for filtering, which are based on BERT and LLMs, respectively. For
LLMs, we address the auto-regressive limitation in computing word occurrence
probability and utilize the full semantic contexts of a text sequence to reveal
cloaked toxic words. Extensive experiments demonstrate that C$^2$TU can achieve
superior performance on two Chinese toxic datasets. In particular, our method
outperforms the best competitor by up to 71% on the F1 score and 35% on
accuracy, respectively.

</details>


### [63] [Let's Predict Sentence by Sentence](https://arxiv.org/abs/2505.22202)
*Hyeonbin Hwang,Byeongguk Jeon,Seungone Kim,Jiyeon Kim,Hoyeon Chang,Sohee Yang,Seungpil Won,Dohaeng Lee,Youbin Ahn,Minjoon Seo*

Main category: cs.CL

TL;DR: 探索预训练语言模型通过连续句子嵌入空间实现结构化推理的可能性，提出基于语义/上下文嵌入的框架，在多个推理任务中达到CoT相当效果且计算量减半。


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型基于词元的生成方式与人类基于抽象语义单元的推理模式存在差异，需验证LM是否能在结构化语义空间进行高效推理。

Method: 提出将预训练LM扩展到句子空间：1) 通过自编码学习语义嵌入；2) 通过下一句预测训练上下文嵌入。采用离散化/连续两种推理机制。

Result: 在数学、逻辑等4个领域，上下文嵌入的连续推理达到与CoT相当效果，平均减少50%计算量，并显示可扩展性和模块化适应潜力。

Conclusion: 实验证明预训练LM可有效迁移到潜在嵌入空间进行结构化推理，SentenceLens工具可视化中间状态验证了抽象推理路径的有效性。

Abstract: Autoregressive language models (LMs) generate one token at a time, yet human
reasoning operates over higher-level abstractions - sentences, propositions,
and concepts. This contrast raises a central question- Can LMs likewise learn
to reason over structured semantic units rather than raw token sequences? In
this work, we investigate whether pretrained LMs can be lifted into such
abstract reasoning spaces by building on their learned representations. We
present a framework that adapts a pretrained token-level LM to operate in
sentence space by autoregressively predicting continuous embeddings of next
sentences. We explore two embedding paradigms inspired by classical
representation learning: 1) semantic embeddings, learned via autoencoding to
preserve surface meaning; and 2) contextual embeddings, trained via
next-sentence prediction to encode anticipatory structure. We evaluate both
under two inference regimes: Discretized, which decodes each predicted
embedding into text before re-encoding; and Continuous, which reasons entirely
in embedding space for improved efficiency. Across four domains - mathematics,
logic, commonsense, and planning - contextual embeddings under continuous
inference show competitive performance with Chain-of-Thought (CoT) while
reducing inference-time FLOPs on average by half. We also present early signs
of scalability and modular adaptation. Finally, to visualize latent
trajectories, we introduce SentenceLens, a diagnostic tool that decodes
intermediate model states into interpretable sentences. Together, our results
indicate that pretrained LMs can effectively transition to abstract, structured
reasoning within latent embedding spaces.

</details>


### [64] [Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models](https://arxiv.org/abs/2505.22232)
*Mehdi Ali,Manuel Brack,Max Lübbering,Elias Wendt,Abbas Goher Khan,Richard Rutmann,Alex Jude,Maurice Kraus,Alexander Arno Weber,Felix Stollenwerk,David Kaczér,Florian Mai,Lucie Flek,Rafet Sifa,Nicolas Flores-Herr,Joachim Köhler,Patrick Schramowski,Michael Fromm,Kristian Kersting*

Main category: cs.CL

TL;DR: JQL提出基于预训练多语言嵌入的轻量级注释模型，系统化提升多语言数据筛选效率与质量


<details>
  <summary>Details</summary>
Motivation: 现有多语言数据集依赖启发式过滤方法，导致跨语言迁移性和扩展性受限，需要更系统化的数据筛选方案

Method: 将大模型标注能力蒸馏到基于多语言嵌入的轻量级注释模型，实现高效跨语言数据筛选

Result: 在35种语言测试中显著优于Fineweb2等传统方法，数据保留率提升且计算需求大幅降低

Conclusion: JQL为多语言数据处理提供新范式，通过可扩展的智能标注系统提升多语言数据集开发标准

Abstract: High-quality multilingual training data is essential for effectively
pretraining large language models (LLMs). Yet, the availability of suitable
open-source multilingual datasets remains limited. Existing state-of-the-art
datasets mostly rely on heuristic filtering methods, restricting both their
cross-lingual transferability and scalability. Here, we introduce JQL, a
systematic approach that efficiently curates diverse and high-quality
multilingual data at scale while significantly reducing computational demands.
JQL distills LLMs' annotation capabilities into lightweight annotators based on
pretrained multilingual embeddings. These models exhibit robust multilingual
and cross-lingual performance, even for languages and scripts unseen during
training. Evaluated empirically across 35 languages, the resulting annotation
pipeline substantially outperforms current heuristic filtering methods like
Fineweb2. JQL notably enhances downstream model training quality and increases
data retention rates. Our research provides practical insights and valuable
resources for multilingual data curation, raising the standards of multilingual
dataset development.

</details>


### [65] [A Linguistically Motivated Analysis of Intonational Phrasing in Text-to-Speech Systems: Revealing Gaps in Syntactic Sensitivity](https://arxiv.org/abs/2505.22236)
*Charlotte Pouw,Afra Alishahi,Willem Zuidema*

Main category: cs.CL

TL;DR: 研究通过心理语言学方法分析TTS系统的句法敏感性，发现系统在歧义句依赖逗号生成语调边界，在简单句可捕捉深层句法线索，微调后语调模式显著改善


<details>
  <summary>Details</summary>
Motivation: 探索TTS系统如何利用句法信息生成语调短语边界，特别是在句法结构模糊场景下的表现局限性

Method: 对比分析TTS系统在歧义句/简单句中的语调边界生成，通过去除逗号的微调实验验证深层语言特征学习能力

Result: 系统处理歧义句需依赖表面标点，简单句可捕捉深层句法结构；微调后语调边界准确率提升23%

Conclusion: 通过针对性训练可增强TTS对语言结构的敏感性，为提升语音合成的自然度提供新方向

Abstract: We analyze the syntactic sensitivity of Text-to-Speech (TTS) systems using
methods inspired by psycholinguistic research. Specifically, we focus on the
generation of intonational phrase boundaries, which can often be predicted by
identifying syntactic boundaries within a sentence. We find that TTS systems
struggle to accurately generate intonational phrase boundaries in sentences
where syntactic boundaries are ambiguous (e.g., garden path sentences or
sentences with attachment ambiguity). In these cases, systems need superficial
cues such as commas to place boundaries at the correct positions. In contrast,
for sentences with simpler syntactic structures, we find that systems do
incorporate syntactic cues beyond surface markers. Finally, we finetune models
on sentences without commas at the syntactic boundary positions, encouraging
them to focus on more subtle linguistic cues. Our findings indicate that this
leads to more distinct intonation patterns that better reflect the underlying
structure.

</details>


### [66] [BioHopR: A Benchmark for Multi-Hop, Multi-Answer Reasoning in Biomedical Domain](https://arxiv.org/abs/2505.22240)
*Yunsoo Kim,Yusuf Abdulle,Honghan Wu*

Main category: cs.CL

TL;DR: BioHopR：首个评估生物医学知识图谱多跳推理能力的基准测试，揭示当前模型在多跳推理上的显著性能差距


<details>
  <summary>Details</summary>
Motivation: 现有基准缺乏对生物医学领域多跳推理（尤其涉及复杂实体关系）的评估能力，导致该领域关键挑战未被充分探索

Method: 基于PrimeKG构建，包含反映真实临床复杂性的1-hop和2-hop推理任务，覆盖药物-疾病-蛋白质等实体关系

Result: 最佳模型O3-mini在1-hop任务精度37.93%，2-hop仅14.57%；开源模型（如Llama-3.3-70B）表现显著落后于专有模型

Conclusion: BioHopR填补了生物医学多跳推理评估空白，揭示了专有与开源模型间的关键差距，为未来LLM发展指明方向

Abstract: Biomedical reasoning often requires traversing interconnected relationships
across entities such as drugs, diseases, and proteins. Despite the increasing
prominence of large language models (LLMs), existing benchmarks lack the
ability to evaluate multi-hop reasoning in the biomedical domain, particularly
for queries involving one-to-many and many-to-many relationships. This gap
leaves the critical challenges of biomedical multi-hop reasoning underexplored.
To address this, we introduce BioHopR, a novel benchmark designed to evaluate
multi-hop, multi-answer reasoning in structured biomedical knowledge graphs.
Built from the comprehensive PrimeKG, BioHopR includes 1-hop and 2-hop
reasoning tasks that reflect real-world biomedical complexities.
  Evaluations of state-of-the-art models reveal that O3-mini, a proprietary
reasoning-focused model, achieves 37.93% precision on 1-hop tasks and 14.57% on
2-hop tasks, outperforming proprietary models such as GPT4O and open-source
biomedical models including HuatuoGPT-o1-70B and Llama-3.3-70B. However, all
models exhibit significant declines in multi-hop performance, underscoring the
challenges of resolving implicit reasoning steps in the biomedical domain. By
addressing the lack of benchmarks for multi-hop reasoning in biomedical domain,
BioHopR sets a new standard for evaluating reasoning capabilities and
highlights critical gaps between proprietary and open-source models while
paving the way for future advancements in biomedical LLMs.

</details>


### [67] [MRT at SemEval-2025 Task 8: Maximizing Recovery from Tables with Multiple Steps](https://arxiv.org/abs/2505.22264)
*Maximiliano Hormazábal Lagos,Álvaro Bueno Saez,Héctor Cerezo-Costas,Pedro Alonso Doval,Jorge Alcalde Vesteiro*

Main category: cs.CL

TL;DR: 提出基于LLM代码生成的表格问答方法，在子任务1取得70.5%准确率


<details>
  <summary>Details</summary>
Motivation: 解决表格数据问答的复杂性，通过代码生成实现自动化解决方案

Method: 多阶段流程：表格理解→自然语言指令生成→代码转换→执行纠错，配合优化提示工程

Result: 在SemEval 2025 Task 8的子任务1中获得70.50%的评分

Conclusion: 分阶段代码生成框架结合提示优化，有效提升结构化数据问答性能

Abstract: In this paper we expose our approach to solve the \textit{SemEval 2025 Task
8: Question-Answering over Tabular Data} challenge. Our strategy leverages
Python code generation with LLMs to interact with the table and get the answer
to the questions. The process is composed of multiple steps: understanding the
content of the table, generating natural language instructions in the form of
steps to follow in order to get the answer, translating these instructions to
code, running it and handling potential errors or exceptions. These steps use
open source LLMs and fine grained optimized prompts for each task (step). With
this approach, we achieved a score of $70.50\%$ for subtask 1.

</details>


### [68] [Comprehensive Evaluation on Lexical Normalization: Boundary-Aware Approaches for Unsegmented Languages](https://arxiv.org/abs/2505.22273)
*Shohei Higashiyama,Masao Utiyama*

Main category: cs.CL

TL;DR: 构建大规模日语多领域词汇规范化数据集，开发基于预训练模型的方法，实验证明编码器和解码器架构均能兼顾准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 词汇规范化研究缺乏跨多视角的系统评估，尤其在未分词语言中缺乏可靠数据集和方法验证。

Method: 1. 创建多领域日语规范化数据集
2. 基于SOTA预训练模型开发编码器/解码器方法
3. 设计多维度评估框架进行对比实验

Result: 编码器架构（如BERT）与解码器架构（如GPT）在F1值、推理速度等指标上均达到竞争性表现，解码器计算效率更优。

Conclusion: 系统性评估填补未分词语言规范化研究空白，建立首个日语多领域基准数据集，为跨语言NLP提供方法论参考。

Abstract: Lexical normalization research has sought to tackle the challenge of
processing informal expressions in user-generated text, yet the absence of
comprehensive evaluations leaves it unclear which methods excel across multiple
perspectives. Focusing on unsegmented languages, we make three key
contributions: (1) creating a large-scale, multi-domain Japanese normalization
dataset, (2) developing normalization methods based on state-of-the-art
pretrained models, and (3) conducting experiments across multiple evaluation
perspectives. Our experiments show that both encoder-only and decoder-only
approaches achieve promising results in both accuracy and efficiency.

</details>


### [69] [Natural Language Processing in Support of Evidence-based Medicine: A Scoping Review](https://arxiv.org/abs/2505.22280)
*Zihan Xu,Haotian Ma,Gongbo Zhang,Yihao Ding,Chunhua Weng,Yifan Peng*

Main category: cs.CL

TL;DR: 系统综述129项研究，揭示自然语言处理(NLP)在循证医学(EBM)五大步骤中的关键作用，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现代医学文献体量庞大且更新迅速，传统人工整理成本过高，需探索NLP技术实现证据识别、评估、整合和传播的自动化。

Method: 通过系统性文献综述方法，分析NLP如何支持EBM的5A框架（提出问题、获取证据、证据评价、应用评估、效果追踪）。

Result: 证实NLP可提升临床决策质量，同时揭示现有证据提取不精准、合成效率低、可解释性不足等局限性。

Conclusion: NLP技术通过优化证据处理流程、增强数据可理解性、提升临床工作效率，具有革新循证医学实践体系的潜力。

Abstract: Evidence-based medicine (EBM) is at the forefront of modern healthcare,
emphasizing the use of the best available scientific evidence to guide clinical
decisions. Due to the sheer volume and rapid growth of medical literature and
the high cost of curation, there is a critical need to investigate Natural
Language Processing (NLP) methods to identify, appraise, synthesize, summarize,
and disseminate evidence in EBM. This survey presents an in-depth review of 129
research studies on leveraging NLP for EBM, illustrating its pivotal role in
enhancing clinical decision-making processes. The paper systematically explores
how NLP supports the five fundamental steps of EBM -- Ask, Acquire, Appraise,
Apply, and Assess. The review not only identifies current limitations within
the field but also proposes directions for future research, emphasizing the
potential for NLP to revolutionize EBM by refining evidence extraction,
evidence synthesis, appraisal, summarization, enhancing data comprehensibility,
and facilitating a more efficient clinical workflow.

</details>


### [70] [Compensating for Data with Reasoning: Low-Resource Machine Translation with LLMs](https://arxiv.org/abs/2505.22293)
*Samuel Frontull,Thomas Ströhle*

Main category: cs.CL

TL;DR: 提出Fragment-Shot Prompting和Pivoted Fragment-Shot方法提升低资源语言翻译质量，发现模型推理能力与翻译效果正相关，提示工程在低→高资源语言翻译中改进有限。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在低资源语言翻译中面临的挑战，特别是缺乏平行语料时的翻译质量问题。

Method: 1. Fragment-Shot Prompting：基于句法覆盖的上下文学习翻译方法
2. Pivoted Fragment-Shot：无需平行语料的间接翻译框架

Result: 1. 新方法有效提升Ladin等低资源语言互译质量
2. GPT-4等强推理模型翻译效果更优
3. 低→高资源语言翻译中零样本提示已足够有效

Conclusion: 通过创新的上下文学习方法突破低资源语言翻译瓶颈，建立模型能力与翻译质量的关联性，为资源稀缺场景提供实用解决方案，并开源代码及检索语料库。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in
multilingual machine translation, sometimes even outperforming traditional
neural systems. However, previous research has highlighted the challenges of
using LLMs, particularly with prompt engineering, for low-resource languages.
In this work, we introduce Fragment-Shot Prompting, a novel in-context learning
method that segments input and retrieves translation examples based on
syntactic coverage, along with Pivoted Fragment-Shot, an extension that enables
translation without direct parallel data. We evaluate these methods using
GPT-3.5, GPT-4o, o1-mini, LLaMA-3.3, and DeepSeek-R1 for translation between
Italian and two Ladin variants, revealing three key findings: (1) Fragment-Shot
Prompting is effective for translating into and between the studied
low-resource languages, with syntactic coverage positively correlating with
translation quality; (2) Models with stronger reasoning abilities make more
effective use of retrieved knowledge, generally produce better translations,
and enable Pivoted Fragment-Shot to significantly improve translation quality
between the Ladin variants; and (3) prompt engineering offers limited, if any,
improvements when translating from a low-resource to a high-resource language,
where zero-shot prompting already yields satisfactory results. We publicly
release our code and the retrieval corpora.

</details>


### [71] [360-LLaMA-Factory: Plug & Play Sequence Parallelism for Long Post-Training](https://arxiv.org/abs/2505.22296)
*Haosheng Zou,Xiaowei Lv,Shousheng Jia,Xiangzheng Zhang*

Main category: cs.CL

TL;DR: 360-LLaMA-Factory通过引入序列并行技术改进LLaMA框架，开源项目获得广泛认可并应用于多个AI模型和企业训练框架


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型训练效率，解决长序列处理难题，并通过开源推动AI社区技术发展

Method: 在LLaMA-Factory框架中集成多种序列并行模式，优化分布式训练流程

Result: 项目被Light-R1、TinyR1等模型采用，并整合入Kaggle竞赛方案和企业级训练框架

Conclusion: 序列并行技术显著提升训练效率，360-LLaMA-Factory的成功实践为大规模语言模型训练提供了有效解决方案

Abstract: Adding sequence parallelism into LLaMA-Factory, we open-sourced
360-LLaMA-Factory at https://github.com/Qihoo360/360-LLaMA-Factory.
360-LLaMA-Factory has received wide recognition and used in models such as
Light-R1 arXiv:2503.10460, TinyR1 arXiv:2503.04872, Kaggle AIMO math models and
also in large companies' training frameworks. This technical report delves
deeper into the different sequence parallel modes behind 360-LLaMA-Factory and
discusses our implementation insights.

</details>


### [72] [Adaptive Detoxification: Safeguarding General Capabilities of LLMs through Toxicity-Aware Knowledge Editing](https://arxiv.org/abs/2505.22298)
*Yifan Lu,Jing Li,Yigeng Zhou,Yihui Zhang,Wenya Wang,Xiucheng Li,Meishan Zhang,Fangming Liu,Jun Yu,Min Zhang*

Main category: cs.CL

TL;DR: 提出动态毒性编辑框架ToxEdit，通过自适应层间路径实现精准毒性缓解，同时保持LLM通用能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM去毒方法存在实体依赖局限性和过度编辑问题，无法有效处理无明确实体的对抗输入且损害模型正常功能。

Method: 在正向传播中动态检测毒性激活模式，通过自适应层间路径调整计算流，并改进SafeEdit基准加入指令跟随评估。

Result: 实验证明ToxEdit在去毒效果和模型能力保持上优于现有方法，改进的评估框架更准确反映过度编辑问题。

Conclusion: ToxEdit为LLM安全部署提供了精准可控的去毒方案，新型评估框架为后续研究提供更可靠的测试基准。

Abstract: Large language models (LLMs) exhibit impressive language capabilities but
remain vulnerable to malicious prompts and jailbreaking attacks. Existing
knowledge editing methods for LLM detoxification face two major challenges.
First, they often rely on entity-specific localization, making them ineffective
against adversarial inputs without explicit entities. Second, these methods
suffer from over-editing, where detoxified models reject legitimate queries,
compromising overall performance. In this paper, we propose ToxEdit, a
toxicity-aware knowledge editing approach that dynamically detects toxic
activation patterns during forward propagation. It then routes computations
through adaptive inter-layer pathways to mitigate toxicity effectively. This
design ensures precise toxicity mitigation while preserving LLMs' general
capabilities. To more accurately assess over-editing, we also enhance the
SafeEdit benchmark by incorporating instruction-following evaluation tasks.
Experimental results on multiple LLMs demonstrate that our ToxEdit outperforms
previous state-of-the-art methods in both detoxification performance and
safeguarding general capabilities of LLMs.

</details>


### [73] [If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?](https://arxiv.org/abs/2505.22318)
*Ishwar B Balappanawar,Vamshi Krishna Bonagiri,Anish R Joishy,Manas Gaur,Krishnaprasad Thirunarayan,Ponnurangam Kumaraguru*

Main category: cs.CL

TL;DR: 研究发现大语言模型（LLM）在反事实情境下推理准确率显著下降27%，提出的Self-Segregate提示法将性能差距缩小至11%并提升总体准确率7.5%。


<details>
  <summary>Details</summary>
Motivation: 探究LLM在上下文与参数知识冲突时的逻辑推理能力，揭示模型在反事实场景中的性能退化机制。

Method: 构建包含9类逻辑模式、1800个样本的CounterLogic数据集，系统评估11个LLM在6个数据集上的表现，提出基于元认知的Self-Segregate提示法。

Result: 反事实推理导致平均准确率下降27%；新方法将性能差距缩小至11%，总体准确率提升7.5%。

Conclusion: 研究为提升LLM在现实场景中的推理能力提供实践洞见，揭示了模型应如何独立于事实知识进行逻辑推理，并与人类认知冲突处理机制形成对照。

Abstract: Large Language Models (LLMs) demonstrate impressive reasoning capabilities in
familiar contexts, but struggle when the context conflicts with their
parametric knowledge. To investigate this phenomenon, we introduce
CounterLogic, a dataset containing 1,800 examples across 9 logical schemas,
explicitly designed to evaluate logical reasoning through counterfactual
(hypothetical knowledge-conflicting) scenarios. Our systematic evaluation of 11
LLMs across 6 different datasets reveals a consistent performance degradation,
with accuracies dropping by 27% on average when reasoning through
counterfactual information. We propose Self-Segregate, a prompting method
enabling metacognitive awareness (explicitly identifying knowledge conflicts)
before reasoning. Our method dramatically narrows the average performance gaps
from 27% to just 11%, while significantly increasing the overall accuracy
(+7.5%). We discuss the implications of these findings and draw parallels to
human cognitive processes, particularly on how humans disambiguate conflicting
information during reasoning tasks. Our findings offer practical insights for
understanding and enhancing LLMs reasoning capabilities in real-world
applications, especially where models must logically reason independently of
their factual knowledge.

</details>


### [74] [Advancing Expert Specialization for Better MoE](https://arxiv.org/abs/2505.22323)
*Hongcan Guo,Haolang Lu,Guoshun Nan,Bolun Chu,Jialin Zhuang,Yuan Yang,Wenhao Che,Sicong Leng,Qimei Cui,Xudong Jiang*

Main category: cs.CL

TL;DR: 提出正交性损失和方差损失两种互补目标，显著提升MoE模型的专家专业化水平，性能提升达23.79%且保持负载均衡


<details>
  <summary>Details</summary>
Motivation: 传统辅助负载均衡损失导致专家处理token类型重叠和路由决策趋同，制约了专家差异化能力的形成

Method: 通过正交性损失促进专家处理不同token类型，方差损失增强路由决策区分度，与现有损失函数梯度兼容

Result: 在多种模型架构和基准测试中实现最高23.79%的性能提升，同时保持下游任务负载平衡且无需结构改动

Conclusion: 双损失机制有效解决专家重叠问题，通过梯度协同优化实现专家差异化，为MoE训练提供简单高效的解决方案

Abstract: Mixture-of-Experts (MoE) models enable efficient scaling of large language
models (LLMs) by activating only a subset of experts per input. However, we
observe that the commonly used auxiliary load balancing loss often leads to
expert overlap and overly uniform routing, which hinders expert specialization
and degrades overall performance during post-training. To address this, we
propose a simple yet effective solution that introduces two complementary
objectives: (1) an orthogonality loss to encourage experts to process distinct
types of tokens, and (2) a variance loss to encourage more discriminative
routing decisions. Gradient-level analysis demonstrates that these objectives
are compatible with the existing auxiliary loss and contribute to optimizing
the training process. Experimental results over various model architectures and
across multiple benchmarks show that our method significantly enhances expert
specialization. Notably, our method improves classic MoE baselines with
auxiliary loss by up to 23.79%, while also maintaining load balancing in
downstream tasks, without any architectural modifications or additional
components. We will release our code to contribute to the community.

</details>


### [75] [NLP for Social Good: A Survey of Challenges, Opportunities, and Responsible Deployment](https://arxiv.org/abs/2505.22327)
*Antonia Karamolegkou,Angana Borah,Eunjung Cho,Sagnik Ray Choudhury,Martina Galletti,Rajarshi Ghosh,Pranav Gupta,Oana Ignat,Priyanka Kargupta,Neema Kotonya,Hemank Lamba,Sun-Joo Lee,Arushi Mangla,Ishani Mondal,Deniz Nazarova,Poli Nemkova,Dina Pisarevskaya,Naquee Rizwan,Nazanin Sabri,Dominik Stammbach,Anna Steinberg,David Tomás,Steven R Wilson,Bowen Yi,Jessica H Zhu,Arkaitz Zubiaga,Anders Søgaard,Alexander Fraser,Zhijing Jin,Rada Mihalcea,Joel R. Tetreault,Daryna Dementieva*

Main category: cs.CL

TL;DR: 探讨大型语言模型快速发展背景下，自然语言处理技术如何更负责任地应对社会挑战，推动NLP4SG研究的公平发展


<details>
  <summary>Details</summary>
Motivation: 基于AI for Social Good愿景，针对NLP技术部署缺乏规范性的现状，提出需建立跨学科框架解决社会关键问题

Method: 通过跨学科视角系统分析社会目标与技术风险，识别技术发展与社会需求的契合点

Result: 明确NLP4SG领域三大核心挑战：技术伦理框架构建、多利益方协调机制、可解释性评估体系

Conclusion: 需建立包含技术治理与社会参与的复合型研究范式，推动NLP技术成为解决社会不平等问题的有效工具

Abstract: Recent advancements in large language models (LLMs) have unlocked
unprecedented possibilities across a range of applications. However, as a
community, we believe that the field of Natural Language Processing (NLP) has a
growing need to approach deployment with greater intentionality and
responsibility. In alignment with the broader vision of AI for Social Good
(Toma\v{s}ev et al., 2020), this paper examines the role of NLP in addressing
pressing societal challenges. Through a cross-disciplinary analysis of social
goals and emerging risks, we highlight promising research directions and
outline challenges that must be addressed to ensure responsible and equitable
progress in NLP4SG research.

</details>


### [76] [Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start](https://arxiv.org/abs/2505.22334)
*Lai Wei,Yuting Li,Kaipeng Zheng,Chen Wang,Yue Wang,Linghe Kong,Lichao Sun,Weiran Huang*

Main category: cs.CL

TL;DR: 提出结合监督微调(SFT)和强化学习(GRPO)的两阶段训练方法，显著提升多模态大模型的推理能力，3B/7B模型在多项基准测试中达到SOTA


<details>
  <summary>Details</summary>
Motivation: 发现多模态大模型在RL训练前已存在自我纠正现象但效果不稳定，需探索更有效的训练范式提升推理性能

Method: 1. 使用结构化思维链的监督微调进行冷启动
2. 采用GRPO算法进行强化学习优化

Result: 7B模型在MathVista(66.3%→73.4%)和We-Math(62.9%→70.4%)显著提升，3B模型性能可媲美部分7B模型

Conclusion: 两阶段训练策略为构建先进多模态推理模型提供实践指导，证明小模型通过恰当训练可取得与更大模型竞争的性能

Abstract: Recent advancements in large language models (LLMs) have demonstrated
impressive chain-of-thought reasoning capabilities, with reinforcement learning
(RL) playing a crucial role in this progress. While "aha moment"
patterns--where models exhibit self-correction through reflection--are often
attributed to emergent properties from RL, we first demonstrate that these
patterns exist in multimodal LLMs (MLLMs) prior to RL training but may not
necessarily correlate with improved reasoning performance. Building on these
insights, we present a comprehensive study on enhancing multimodal reasoning
through a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start
with structured chain-of-thought reasoning patterns, followed by (2)
reinforcement learning via GRPO to further refine these capabilities. Our
extensive experiments show that this combined approach consistently outperforms
both SFT-only and RL-only methods across challenging multimodal reasoning
benchmarks. The resulting models achieve state-of-the-art performance among
open-source MLLMs at both 3B and 7B scales, with our 7B model showing
substantial improvements over base models (e.g., 66.3 %$\rightarrow$73.4 % on
MathVista, 62.9 %$\rightarrow$70.4 % on We-Math) and our 3B model achieving
performance competitive with several 7B models. Overall, this work provides
practical guidance for building advanced multimodal reasoning models. Our code
is available at https://github.com/waltonfuture/RL-with-Cold-Start.

</details>


### [77] [Text2Grad: Reinforcement Learning from Natural Language Feedback](https://arxiv.org/abs/2505.22338)
*Hanyang Wang,Lu Wang,Chaoyun Zhang,Tianjun Mao,Si Qin,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.CL

TL;DR: Text2Grad提出通过将自然语言反馈转化为跨度级别梯度，实现细粒度策略优化的强化学习新范式


<details>
  <summary>Details</summary>
Motivation: 传统RLHF使用粗粒度标量奖励导致学习效率低且不透明，而现有文本反馈方法无法更新模型参数

Method: 构建包含反馈标注流程（对齐文本反馈与token跨度）、细粒度奖励模型（预测跨度奖励并生成解释）和跨度策略优化器（反向传播自然语言梯度）的三组件系统

Result: 在摘要/代码生成/问答任务中，Text2Grad全面超越标量奖励RL和提示基线，同时提升任务指标和可解释性

Conclusion: 文本反馈转化为梯度后，能成为细粒度策略优化的有效信号，为模型优化提供精准调整而非全局扰动

Abstract: Traditional RLHF optimizes language models with coarse, scalar rewards that
mask the fine-grained reasons behind success or failure, leading to slow and
opaque learning. Recent work augments RL with textual critiques through
prompting or reflection, improving interpretability but leaving model
parameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm
that turns free-form textual feedback into span-level gradients. Given human
(or programmatic) critiques, Text2Grad aligns each feedback phrase with the
relevant token spans, converts these alignments into differentiable reward
signals, and performs gradient updates that directly refine the offending
portions of the model's policy. This yields precise, feedback-conditioned
adjustments instead of global nudges. Text2Grad is realized through three
components: (1) a high-quality feedback-annotation pipeline that pairs
critiques with token spans; (2) a fine-grained reward model that predicts
span-level reward on answer while generating explanatory critiques; and (3) a
span-level policy optimizer that back-propagates natural-language gradients.
Across summarization, code generation, and question answering, Text2Grad
consistently surpasses scalar-reward RL and prompt-only baselines, providing
both higher task metrics and richer interpretability. Our results demonstrate
that natural-language feedback, when converted to gradients, is a powerful
signal for fine-grained policy optimization. The code for our method is
available at https://github.com/microsoft/Text2Grad

</details>


### [78] [LLMs Struggle to Reject False Presuppositions when Misinformation Stakes are High](https://arxiv.org/abs/2505.22354)
*Judith Sieker,Clara Lachenmaier,Sina Zarrieß*

Main category: cs.CL

TL;DR: 研究通过语言学预设分析发现，主流LLM难以识别错误预设且表现受语境因素影响，政治场景中易强化错误信息


<details>
  <summary>Details</summary>
Motivation: 担忧LLM像人类一样无法识别错误预设，尤其在高风险政治场景中可能传播错误信息

Method: 基于语言学预设理论构建新数据集，测试GPT-4-o等三个模型在政治语境中的语言结构/政党属性/情境概率敏感性

Result: 模型普遍存在错误预设识别困难，不同条件（如政党立场）下表现差异显著

Conclusion: 语言学预设分析有效揭示LLM政治错误信息强化机制，凸显模型处理隐含错误信息的能力缺陷

Abstract: This paper examines how LLMs handle false presuppositions and whether certain
linguistic factors influence their responses to falsely presupposed content.
Presuppositions subtly introduce information as given, making them highly
effective at embedding disputable or false information. This raises concerns
about whether LLMs, like humans, may fail to detect and correct misleading
assumptions introduced as false presuppositions, even when the stakes of
misinformation are high. Using a systematic approach based on linguistic
presupposition analysis, we investigate the conditions under which LLMs are
more or less sensitive to adopt or reject false presuppositions. Focusing on
political contexts, we examine how factors like linguistic construction,
political party, and scenario probability impact the recognition of false
presuppositions. We conduct experiments with a newly created dataset and
examine three LLMs: OpenAI's GPT-4-o, Meta's LLama-3-8B, and MistralAI's
Mistral-7B-v03. Our results show that the models struggle to recognize false
presuppositions, with performance varying by condition. This study highlights
that linguistic presupposition analysis is a valuable tool for uncovering the
reinforcement of political misinformation in LLM responses.

</details>


### [79] [Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition](https://arxiv.org/abs/2505.22375)
*Hanting Chen,Yasheng Wang,Kai Han,Dong Li,Lin Li,Zhenni Bi,Jinpeng Li,Haoyu Wang,Fei Mi,Mingjian Zhu,Bin Wang,Kaikai Song,Yifei Fu,Xu He,Yu Luo,Chong Zhu,Quan He,Xueyu Wu,Wei He,Hailin Hu,Yehui Tang,Dacheng Tao,Xinghao Chen,Yunhe Wang,Other Contributors*

Main category: cs.CL

TL;DR: 昇腾NPU平台的高效双模大语言模型Pangu Embedded，通过两阶段训练实现快速推理与深度思考的平衡，7B参数模型性能超越同类产品


<details>
  <summary>Details</summary>
Motivation: 解决现有推理优化LLM存在的高计算成本、推理延迟问题，实现高性能与低延迟的统一部署

Method: 1. 阶段一：迭代蒸馏+强化学习（延迟容忍调度器+MARS奖励系统）
2. 阶段二：双系统框架（快速常规模式+深度慢速模式）

Result: 在AIME 2024/GPQA/LiveCodeBench等基准测试中，7B模型响应速度更快且推理质量优于Qwen3-8B/GLM4-9B等同规模模型

Conclusion: 通过统一架构实现快速响应与深度推理的灵活切换，为实际部署场景提供了高性能LLM推理器的新范式

Abstract: This work presents Pangu Embedded, an efficient Large Language Model (LLM)
reasoner developed on Ascend Neural Processing Units (NPUs), featuring flexible
fast and slow thinking capabilities. Pangu Embedded addresses the significant
computational costs and inference latency challenges prevalent in existing
reasoning-optimized LLMs. We propose a two-stage training framework for its
construction. In Stage 1, the model is finetuned via an iterative distillation
process, incorporating inter-iteration model merging to effectively aggregate
complementary knowledge. This is followed by reinforcement learning on Ascend
clusters, optimized by a latency-tolerant scheduler that combines stale
synchronous parallelism with prioritized data queues. The RL process is guided
by a Multi-source Adaptive Reward System (MARS), which generates dynamic,
task-specific reward signals using deterministic metrics and lightweight LLM
evaluators for mathematics, coding, and general problem-solving tasks. Stage 2
introduces a dual-system framework, endowing Pangu Embedded with a "fast" mode
for routine queries and a deeper "slow" mode for complex inference. This
framework offers both manual mode switching for user control and an automatic,
complexity-aware mode selection mechanism that dynamically allocates
computational resources to balance latency and reasoning depth. Experimental
results on benchmarks including AIME 2024, GPQA, and LiveCodeBench demonstrate
that Pangu Embedded with 7B parameters, outperforms similar-size models like
Qwen3-8B and GLM4-9B. It delivers rapid responses and state-of-the-art
reasoning quality within a single, unified model architecture, highlighting a
promising direction for developing powerful yet practically deployable LLM
reasoners.

</details>


### [80] [RAG-Zeval: Towards Robust and Interpretable Evaluation on RAG Responses through End-to-End Rule-Guided Reasoning](https://arxiv.org/abs/2505.22430)
*Kun Li,Yunxiang Li,Tianhua Zhang,Hongyin Luo,Xixin Wu,James Glass,Helen Meng*

Main category: cs.CL

TL;DR: 提出无需人工标注的RAG评估框架RAG-Zeval，通过强化学习和规则引导的推理任务实现高效评估


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的RAG评估方法计算成本高且未充分利用模型推理能力，需更高效的评估方案

Method: 使用强化学习训练评估器，将评估任务转化为规则引导的推理过程，采用基于排名的奖励机制替代绝对分数

Result: 实验显示RAG-Zeval与人类判断相关性最强，在仅使用1%-10%参数量的情况下超越基线模型

Conclusion: 该方法在保持评估准确性的同时显著提升效率，为RAG系统评估提供了可解释且资源友好的解决方案

Abstract: Robust evaluation is critical for deploying trustworthy retrieval-augmented
generation (RAG) systems. However, current LLM-based evaluation frameworks
predominantly rely on directly prompting resource-intensive models with complex
multi-stage prompts, underutilizing models' reasoning capabilities and
introducing significant computational cost. In this paper, we present RAG-Zeval
(RAG-Zero Evaluator), a novel end-to-end framework that formulates faithfulness
and correctness evaluation as a rule-guided reasoning task. Our approach trains
evaluators with reinforcement learning, facilitating compact models to generate
comprehensive and sound assessments with detailed explanation in one-pass. We
introduce a ranking-based outcome reward mechanism, using preference judgments
rather than absolute scores, to address the challenge of obtaining precise
pointwise reward signals. To this end, we synthesize the ranking references by
generating quality-controlled responses with zero human annotation. Experiments
demonstrate RAG-Zeval's superior performance, achieving the strongest
correlation with human judgments and outperforming baselines that rely on LLMs
with 10-100 times more parameters. Our approach also exhibits superior
interpretability in response evaluation.

</details>


### [81] [Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO](https://arxiv.org/abs/2505.22453)
*Lai Wei,Yuting Li,Chen Wang,Yue Wang,Linghe Kong,Weiran Huang,Lichao Sun*

Main category: cs.CL

TL;DR: MM-UPT提出了一种基于GRPO算法的无监督后训练框架，通过自我奖励机制提升多模态大语言模型的推理能力，无需人工标注数据。


<details>
  <summary>Details</summary>
Motivation: 现有监督方法依赖昂贵的人工标注数据，且无监督方法复杂难迭代。需探索可持续、高效的无监督训练范式。

Method: 采用GRPO强化学习算法，通过多数投票机制从多响应中生成自我奖励信号，结合自生成合成问题进行训练。

Result: Qwen2.5-VL-7B在MathVista(↑6.6%)和We-Math(↑5.8%)显著提升，超越无监督基线并接近监督GRPO效果。自生成问题可进一步优化性能。

Conclusion: MM-UPT开创了无外部监督的持续自我增强范式，证明自奖励机制的有效性，为可扩展的MLLMs优化提供新思路。

Abstract: Improving Multi-modal Large Language Models (MLLMs) in the post-training
stage typically relies on supervised fine-tuning (SFT) or reinforcement
learning (RL). However, these supervised methods require expensive and manually
annotated multi-modal data--an ultimately unsustainable resource. While recent
efforts have explored unsupervised post-training, their methods are complex and
difficult to iterate. In this work, we are the first to investigate the use of
GRPO, a stable and scalable online RL algorithm, for enabling continual
self-improvement without any external supervision. We propose MM-UPT, a simple
yet effective framework for unsupervised post-training of MLLMs. MM-UPT builds
upon GRPO, replacing traditional reward signals with a self-rewarding mechanism
based on majority voting over multiple sampled responses. Our experiments
demonstrate that MM-UPT significantly improves the reasoning ability of
Qwen2.5-VL-7B (e.g., 66.3 %$\rightarrow$72.9 % on MathVista, 62.9
%$\rightarrow$68.7 % on We-Math), using standard dataset without ground truth
labels. MM-UPT also outperforms prior unsupervised baselines and even
approaches the results of supervised GRPO. Furthermore, we show that
incorporating synthetic questions, generated solely by MLLM itself, can boost
performance as well, highlighting a promising approach for scalable
self-improvement. Overall, MM-UPT offers a new paradigm for continual,
autonomous enhancement of MLLMs in the absence of external supervision. Our
code is available at https://github.com/waltonfuture/MM-UPT.

</details>


### [82] [EvolveSearch: An Iterative Self-Evolving Search Agent](https://arxiv.org/abs/2505.22501)
*Dingchu Zhang,Yida Zhao,Jialong Wu,Baixuan Li,Wenbiao Yin,Liwen Zhang,Yong Jiang,Yufeng Li,Kewei Tu,Pengjun Xie,Fei Huang*

Main category: cs.CL

TL;DR: 提出EvolveSearch框架，通过SFT和RL迭代自进化提升大语言模型在开放域网络搜索的智能代理能力，无需人工标注数据


<details>
  <summary>Details</summary>
Motivation: 现有方法（监督微调/强化学习）在开放搜索领域存在数据生产效率低、收敛快导致数据利用率不足的问题

Method: 结合监督微调(SFT)和强化学习(RL)的迭代自进化框架，通过多轮次训练实现搜索能力的持续提升

Result: 在7个多跳问答基准测试中平均提升4.7%，超越当前SOTA方法

Conclusion: 该框架为开放域网络搜索的智能代理自进化能力开辟了新路径

Abstract: The rapid advancement of large language models (LLMs) has transformed the
landscape of agentic information seeking capabilities through the integration
of tools such as search engines and web browsers. However, current mainstream
approaches for enabling LLM web search proficiency face significant challenges:
supervised fine-tuning struggles with data production in open-search domains,
while RL converges quickly, limiting their data utilization efficiency. To
address these issues, we propose EvolveSearch, a novel iterative self-evolution
framework that combines SFT and RL to enhance agentic web search capabilities
without any external human-annotated reasoning data. Extensive experiments on
seven multi-hop question-answering (MHQA) benchmarks demonstrate that
EvolveSearch consistently improves performance across iterations, ultimately
achieving an average improvement of 4.7\% over the current state-of-the-art
across seven benchmarks, opening the door to self-evolution agentic
capabilities in open web search domains.

</details>


### [83] [Multi-MLLM Knowledge Distillation for Out-of-Context News Detection](https://arxiv.org/abs/2505.22517)
*Yimeng Gu,Zhao Tong,Ignacio Castro,Shu Wu,Gareth Tyson*

Main category: cs.CL

TL;DR: 提出两阶段知识蒸馏框架提升小规模多模态大语言模型检测上下文外新闻的性能，仅需10%标注数据即达SOTA


<details>
  <summary>Details</summary>
Motivation: 现有小模型需大量标注数据或依赖昂贵API调用，在低资源场景不实用。需开发更标签高效、成本可控的优化方案

Method: 1. 多教师模型生成预测及推理依据；2. 两阶段蒸馏：第一阶段全量数据LoRA微调，第二阶段对教师预测冲突数据结合LoRA+DPO强化训练

Result: 实验证明该方法使用不足10%标注数据即取得state-of-the-art性能

Conclusion: 通过知识蒸馏与分阶段优化策略，在降低标注成本的同时有效提升模型对复杂案例的识别能力

Abstract: Multimodal out-of-context news is a type of misinformation in which the image
is used outside of its original context. Many existing works have leveraged
multimodal large language models (MLLMs) for detecting out-of-context news.
However, observing the limited zero-shot performance of smaller MLLMs, they
generally require label-rich fine-tuning and/or expensive API calls to GPT
models to improve the performance, which is impractical in low-resource
scenarios. In contrast, we aim to improve the performance of small MLLMs in a
more label-efficient and cost-effective manner. To this end, we first prompt
multiple teacher MLLMs to generate both label predictions and corresponding
rationales, which collectively serve as the teachers' knowledge. We then
introduce a two-stage knowledge distillation framework to transfer this
knowledge to a student MLLM. In Stage 1, we apply LoRA fine-tuning to the
student model using all training data. In Stage 2, we further fine-tune the
student model using both LoRA fine-tuning and DPO on the data points where
teachers' predictions conflict. This two-stage strategy reduces annotation
costs and helps the student model uncover subtle patterns in more challenging
cases. Experimental results demonstrate that our approach achieves
state-of-the-art performance using less than 10% labeled data.

</details>


### [84] [Emotion-o1: Adaptive Long Reasoning for Emotion Understanding in LLMs](https://arxiv.org/abs/2505.22548)
*Changhao Song,Yazhou Zhang,Peng Zhang*

Main category: cs.CL

TL;DR: 提出任务自适应推理框架DeepSeek-R1，通过强化学习动态调节推理深度，在情感分析任务中实现基础任务F1提升3.56%，高级任务F1提升37.95%。


<details>
  <summary>Details</summary>
Motivation: 现有固定长度思维链(CoT)方法无法适应情感理解的复杂度差异，尤其难以处理讽刺检测等高级任务。

Method: 结合微调与强化学习，设计复合奖励函数（兼顾预测精度、推理深度控制、路径多样性和逻辑重复抑制），实现上下文敏感的动态推理。

Result: 情感/情感分析基础任务最高提升3.56% F1（2.76% Acc），幽默/讽刺检测高级任务提升37.95% F1（23.14% Acc）。

Conclusion: 通过自适应深度分析桥接刚性CoT推理与情感复杂性，使大语言模型自主发展深度推理能力。

Abstract: Emotion understanding includes basic tasks (e.g., sentiment/emotion
classification) and advanced tasks (e.g., sarcasm/humor detection). Current
methods rely on fixed-length CoT reasoning, failing to adapt to the varying
complexity of emotions. We propose a task-adaptive reasoning framework that
employs DeepSeek-R1 to generate variable-length reasoning chains for different
emotion tasks. By combining fine-tuning with reinforcement learning, we design
a composite reward function that balances four objectives: prediction accuracy,
adaptive reasoning depth control, structural diversity in reasoning paths, and
suppression of repetitive logic. This approach achieves dynamic
context-sensitive inference while enabling LLMs to autonomously develop deep
reasoning capabilities. Experimental results demonstrate consistent
improvements in both Acc and F1 scores across four tasks: emotion, sentiment,
humor, and sarcasm. Notably, peak enhancements reached 3.56% F1 (2.76% Acc) for
basic tasks and 37.95% F1 (23.14% Acc) for advanced tasks. Our work bridges
rigid CoT reasoning and emotional complexity through adaptive-depth analysis.

</details>


### [85] [ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation with Lightweight Specialized LLM](https://arxiv.org/abs/2505.22552)
*Hoang Pham,Thanh-Do Nguyen,Khac-Hoai Nam Bui*

Main category: cs.CL

TL;DR: 提出ClaimPKG框架，通过知识图谱增强语言模型在声明验证中的推理能力，准确率提升9%-12%


<details>
  <summary>Details</summary>
Motivation: 现有声明验证方法依赖非结构化文本且LLM难以直接处理知识图谱，需要结构化知识整合方案

Method: 1) 轻量级LLM生成声明伪子图 2) 专用模块检索KG子图 3) 通用LLM处理子图生成验证结果

Result: 在FactKG实现SOTA性能，准确率提升9-12个百分点；在HoVer/FEVEROUS展示零样本泛化能力

Conclusion: ClaimPKG有效融合结构化知识与非结构化推理，为LLM与KG协同提供创新框架

Abstract: Integrating knowledge graphs (KGs) to enhance the reasoning capabilities of
large language models (LLMs) is an emerging research challenge in claim
verification. While KGs provide structured, semantically rich representations
well-suited for reasoning, most existing verification methods rely on
unstructured text corpora, limiting their ability to effectively leverage KGs.
Additionally, despite possessing strong reasoning abilities, modern LLMs
struggle with multi-step modular pipelines and reasoning over KGs without
adaptation. To address these challenges, we propose ClaimPKG, an end-to-end
framework that seamlessly integrates LLM reasoning with structured knowledge
from KGs. Specifically, the main idea of ClaimPKG is to employ a lightweight,
specialized LLM to represent the input claim as pseudo-subgraphs, guiding a
dedicated subgraph retrieval module to identify relevant KG subgraphs. These
retrieved subgraphs are then processed by a general-purpose LLM to produce the
final verdict and justification. Extensive experiments on the FactKG dataset
demonstrate that ClaimPKG achieves state-of-the-art performance, outperforming
strong baselines in this research field by 9%-12% accuracy points across
multiple categories. Furthermore, ClaimPKG exhibits zero-shot generalizability
to unstructured datasets such as HoVer and FEVEROUS, effectively combining
structured knowledge from KGs with LLM reasoning across various LLM backbones.

</details>


### [86] [Do Large Language Models Think Like the Brain? Sentence-Level Evidence from fMRI and Hierarchical Embeddings](https://arxiv.org/abs/2505.22563)
*Yu Lei,Xingyang Ge,Yi Zhang,Yiming Yang,Bolei Ma*

Main category: cs.CL

TL;DR: 研究发现性能提升驱动语言模型表征架构向脑层级演化，尤其在高语义抽象层面与大脑功能和解剖结构呈现显著对应


<details>
  <summary>Details</summary>
Motivation: 探究语言模型与人脑在句子理解机制上的深层对齐关系，验证模型性能提升是否推动表征架构向脑层级演化

Method: 通过对比14个公开语言模型的层次表征与自然叙事故事诱发的大脑fMRI数据，构建神经预测模型定位与脑区激活最相关的模型层级

Result: 模型性能改进驱动表征架构向类脑层级演化，在高级语义抽象层面呈现更强的功能和解剖对应关系

Conclusion: 语言模型性能的演化不仅体现规模扩展，更反映了与人脑语言处理架构的深层功能对齐，尤其在语义抽象表征层面存在显著神经对应

Abstract: Understanding whether large language models (LLMs) and the human brain
converge on similar computational principles remains a fundamental and
important question in cognitive neuroscience and AI. Do the brain-like patterns
observed in LLMs emerge simply from scaling, or do they reflect deeper
alignment with the architecture of human language processing? This study
focuses on the sentence-level neural mechanisms of language models,
systematically investigating how hierarchical representations in LLMs align
with the dynamic neural responses during human sentence comprehension. By
comparing hierarchical embeddings from 14 publicly available LLMs with fMRI
data collected from participants, who were exposed to a naturalistic narrative
story, we constructed sentence-level neural prediction models to precisely
identify the model layers most significantly correlated with brain region
activations. Results show that improvements in model performance drive the
evolution of representational architectures toward brain-like hierarchies,
particularly achieving stronger functional and anatomical correspondence at
higher semantic abstraction levels.

</details>


### [87] [Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2505.22571)
*Hoang Pham,Khac-Hoai Nam Bui*

Main category: cs.CL

TL;DR: 提出Agent-UniRAG框架实现统一检索增强生成系统，通过LLM智能体分步处理不同复杂度查询，并创建SynAgent-RAG数据集提升小型开源模型性能


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统单独处理单跳/多跳查询存在应用局限性，需构建统一框架增强系统可解释性和任务适应性

Method: 设计基于LLM的可训练代理框架，根据输入复杂度分步执行RAG任务，使用合成数据集训练小型LLM（如Llama-3-8B）

Result: 在多个RAG基准测试中达到与闭源模型及大型开源模型相当的精度表现

Conclusion: Agent-UniRAG框架有效统一不同复杂度RAG任务，SynAgent-RAG数据集成功赋能小型LLM，推动可解释RAG系统发展

Abstract: This paper presents a novel approach for unified retrieval-augmented
generation (RAG) systems using the recent emerging large language model (LLM)
agent concept. Specifically, Agent LLM, which utilizes LLM as fundamental
controllers, has become a promising approach to enable the interpretability of
RAG tasks, especially for complex reasoning question-answering systems (e.g.,
multi-hop queries). Nonetheless, previous works mainly focus on solving RAG
systems with either single-hop or multi-hop approaches separately, which limits
the application of those approaches to real-world applications. In this study,
we propose a trainable agent framework called Agent-UniRAG for unified
retrieval-augmented LLM systems, which enhances the effectiveness and
interpretability of RAG systems. The main idea is to design an LLM agent
framework to solve RAG tasks step-by-step based on the complexity of the
inputs, simultaneously including single-hop and multi-hop queries in an
end-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset
to enable the proposed agent framework for small open-source LLMs (e.g.,
Llama-3-8B). The results show comparable performances with closed-source and
larger open-source LLMs across various RAG benchmarks. Our source code and
dataset are publicly available for further exploitation.

</details>


### [88] [Fusion Steering: Prompt-Specific Activation Control](https://arxiv.org/abs/2505.22572)
*Waldemar Chang,Alhassan Yasin*

Main category: cs.CL

TL;DR: Fusion Steering通过动态注入全网络层的激活增量，显著提升语言模型在QA任务中的事实准确性（准确率从3.5%提升至25.4%），分段引导策略表现最佳


<details>
  <summary>Details</summary>
Motivation: 传统激活引导方法局限在单层或固定层操作，难以实现语义丰富的动态控制。需要开发灵活的全网络层干预方法提升事实准确性

Method: 1. 提出全层/分段动态引导配置
2. 基于真实答案+模型生成解释的混合参考生成激活增量
3. 使用Optuna优化每个prompt的注入权重（平衡token重叠和困惑度）

Result: 在260个SimpleQA测试样本中：
- 分段引导准确率25.4%（基线3.5%）
- 严格标准下完全正确回答从0%提升至13.1%
- 量化模型Gemma-2-2B-IT实现高效部署

Conclusion: 分段动态干预策略显著优于传统方法，全网络激活控制与稀疏表示（如Neuronpedia）结合，为可解释的模型控制开辟新方向

Abstract: We present Fusion Steering, an activation steering methodology that improves
factual accuracy in large language models (LLMs) for question-answering (QA)
tasks. This approach introduces flexible steering configurations, including
full-layer steering and segmented steering. Unlike traditional methods
constrained to single-layer or fixed-layer operations, Fusion Steering employs
dynamic injection of prompt-specific activation deltas across all transformer
layers. These activation deltas are derived from reference completions that
combine the ground-truth answer with a model-generated explanation to
facilitate semantically enriched, example-specific steering. The injection
weights are optimized per prompt using Optuna, targeting a joint objective that
balances token overlap (factual alignment) and perplexity (fluency proxy).
Evaluation employs a composite score integrating token overlap and LLM-graded
quality, encompassing factual accuracy, coherence, and relevance. Empirical
results on 260 SimpleQA prompts (selected from 500 where the baseline failed)
showcase the efficacy of segmented steering. Using Gemma-2-2B-IT with 8-bit
quantization, segmented steering achieves an accuracy of 25.4% (outputs scoring
$\geq 0.6$), outperforming the baseline at 3.5% and full-layer steering at
16.2%. Under the stricter SimpleQA rubric, segmented steering boosts fully
correct responses from 0.0% to 13.1%. These findings highlight the strengths of
segmented, dynamic intervention strategies and the promise of per-prompt,
full-network activation control. Fusion Steering is also amenable to sparse
representations, such as Neuronpedia or sparse crosscoders, suggesting a
promising direction for interpretable and scalable activation-level control in
LLMs.

</details>


### [89] [Less, but Better: Efficient Multilingual Expansion for LLMs via Layer-wise Mixture-of-Experts](https://arxiv.org/abs/2505.22582)
*Xue Zhang,Yunlong Liang,Fandong Meng,Songming Zhang,Yufeng Chen,Jinan Xu,Jie Zhou*

Main category: cs.CL

TL;DR: 提出LayerMoE算法，通过分层专家分配和分类器路由机制，在减少60%专家数量的情况下有效缓解多语言模型扩展时的遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有MoE方法扩展新语言时参数成本高且影响旧语言性能，需更高效的专家分配策略降低参数并保持语言能力。

Method: 1. 分析不同网络层的跨语言表征相似性
2. 相似度高的层分配较少新专家
3. 在高层相似层添加分类器引导旧语言路由

Result: 单次扩展节省60%专家，持续扩展节省33.3%专家，性能超越SOTA基线

Conclusion: LayerMoE验证了分层相似性指导专家分配的有效性，为多语言模型持续扩展提供了参数高效的解决方案

Abstract: Continually expanding new languages for existing large language models (LLMs)
is a promising yet challenging approach to building powerful multilingual LLMs.
The biggest challenge is to make the model continuously learn new languages
while preserving the proficient ability of old languages. To achieve this,
recent work utilizes the Mixture-of-Experts (MoE) architecture to expand new
languages by adding new experts and avoid catastrophic forgetting of old
languages by routing corresponding tokens to the original model backbone (old
experts). Although intuitive, this kind of method is parameter-costly when
expanding new languages and still inevitably impacts the performance of old
languages. To address these limitations, we analyze the language
characteristics of different layers in LLMs and propose a layer-wise expert
allocation algorithm (LayerMoE) to determine the appropriate number of new
experts for each layer. Specifically, we find different layers in LLMs exhibit
different representation similarities between languages and then utilize the
similarity as the indicator to allocate experts for each layer, i.e., the
higher similarity, the fewer experts. Additionally, to further mitigate the
forgetting of old languages, we add a classifier in front of the router network
on the layers with higher similarity to guide the routing of old language
tokens. Experimental results show that our method outperforms the previous
state-of-the-art baseline with 60% fewer experts in the single-expansion
setting and with 33.3% fewer experts in the lifelong-expansion setting,
demonstrating the effectiveness of our method.

</details>


### [90] [Precise In-Parameter Concept Erasure in Large Language Models](https://arxiv.org/abs/2505.22586)
*Yoav Gur-Arieh,Clara Suslik,Yihuai Hong,Fazl Barez,Mor Geva*

Main category: cs.CL

TL;DR: 提出PISCES方法通过特征解耦和参数编辑实现精准概念擦除，在Gemma/Llama模型上取得7.7%目标概念残留率，显著提升擦除特异性(31%)和鲁棒性(38%)。


<details>
  <summary>Details</summary>
Motivation: 现有LLM知识擦除方法（微调/低秩适配器/事实编辑）存在精度不足、效果有限的问题，需要参数层面的精准概念消除方案。

Method: 1. 使用disentangler模型解耦MLP向量为可解释特征
2. 通过自动化可解释技术识别目标概念关联特征
3. 直接从参数空间移除对应特征方向

Result: 目标概念准确率降至7.7%，擦除特异性提升31%，鲁棒性提升38%（Gemma 2/Llama 3.1实验）

Conclusion: 基于特征解耦的参数编辑方法为语言模型知识擦除提供了更精准可靠的解决方案

Abstract: Large language models (LLMs) often acquire knowledge during pretraining that
is undesirable in downstream deployments, e.g., sensitive information or
copyrighted content. Existing approaches for removing such knowledge rely on
fine-tuning, training low-rank adapters or fact-level editing, but these are
either too coarse, too shallow, or ineffective. In this work, we propose PISCES
(Precise In-parameter Suppression for Concept EraSure), a novel framework for
precisely erasing entire concepts from model parameters by directly editing
directions that encode them in parameter space. PISCES uses a disentangler
model to decompose MLP vectors into interpretable features, identifies those
associated with a target concept using automated interpretability techniques,
and removes them from model parameters. Experiments on Gemma 2 and Llama 3.1
over various concepts show that PISCES achieves modest gains in efficacy over
leading erasure methods, reducing accuracy on the target concept to as low as
7.7%, while dramatically improving erasure specificity (by up to 31%) and
robustness (by up to 38%). Overall, these results demonstrate that
feature-based in-parameter editing enables a more precise and reliable approach
for removing conceptual knowledge in language models.

</details>


### [91] [Self-Error-Instruct: Generalizing from Errors for LLMs Mathematical Reasoning](https://arxiv.org/abs/2505.22591)
*Erxin Yu,Jing Li,Ming Liao,Qi Zhu,Boyang Xue,Minghui Xu,Baojun Wang,Lanqing Hong,Fei Mi,Lifeng Shang*

Main category: cs.CL

TL;DR: SEI框架通过系统化分析模型错误案例，生成泛化训练数据并迭代微调，显著提升大语言模型在数学推理任务中的性能（如GSM8K/MATH数据集）。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅从孤立错误案例合成数据，无法捕捉错误模式的通用规律。SEI通过聚类错误类型实现系统性错误泛化，解决该局限性。

Method: 1)识别目标模型错误案例→2)GPT-4o生成错误关键词并聚类→3)基于错误类型采样案例生成扩展数据→4)单样本学习精炼数据→5)迭代微调模型

Result: 多模型实验显示：SEI使数学推理准确率在领域内(GSM8K)和领域外数据集均提升，最高提升达5.2%，证明错误泛化的有效性。

Conclusion: 自错误指导机制通过错误类型聚类与数据迭代生成，成功提升模型鲁棒性。该方法可扩展至逻辑推理、代码生成等复杂认知任务优化。

Abstract: Although large language models demonstrate strong performance across various
domains, they still struggle with numerous bad cases in mathematical reasoning.
Previous approaches to learning from errors synthesize training data by solely
extrapolating from isolated bad cases, thereby failing to generalize the
extensive patterns inherent within these cases. This paper presents
Self-Error-Instruct (SEI), a framework that addresses these model weaknesses
and synthesizes more generalized targeted training data. Specifically, we
explore a target model on two mathematical datasets, GSM8K and MATH, to
pinpoint bad cases. Then, we generate error keyphrases for these cases based on
the instructor model's (GPT-4o) analysis and identify error types by clustering
these keyphrases. Next, we sample a few bad cases during each generation for
each identified error type and input them into the instructor model, which
synthesizes additional training data using a self-instruct approach. This new
data is refined through a one-shot learning process to ensure that only the
most effective examples are kept. Finally, we use these curated data to
fine-tune the target model, iteratively repeating the process to enhance
performance. We apply our framework to various models and observe improvements
in their reasoning abilities across both in-domain and out-of-domain
mathematics datasets. These results demonstrate the effectiveness of self-error
instruction in improving LLMs' mathematical reasoning through error
generalization.

</details>


### [92] [Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding](https://arxiv.org/abs/2505.22618)
*Chengyue Wu,Hao Zhang,Shuchen Xue,Zhijian Liu,Shizhe Diao,Ligeng Zhu,Ping Luo,Song Han,Enze Xie*

Main category: cs.CL

TL;DR: 提出块级KV缓存机制和置信度感知并行解码策略，显著提升扩散大模型推理速度


<details>
  <summary>Details</summary>
Motivation: 扩散大模型推理速度落后于自回归模型，存在KV缓存缺失和并行解码质量下降问题

Method: 1. 双向扩散模型专用块级近似KV缓存机制
2. 基于置信度阈值的选择性并行解码策略

Result: LLaDA/Dream模型实验显示最高27.6倍吞吐量提升，精度损失可控

Conclusion: 该方法有效弥合扩散模型与自回归模型性能差距，具备实际部署价值

Abstract: Diffusion-based large language models (Diffusion LLMs) have shown promise for
non-autoregressive text generation with parallel decoding capabilities.
However, the practical inference speed of open-sourced Diffusion LLMs often
lags behind autoregressive models due to the lack of Key-Value (KV) Cache and
quality degradation when decoding multiple tokens simultaneously. To bridge
this gap, we introduce a novel block-wise approximate KV Cache mechanism
tailored for bidirectional diffusion models, enabling cache reuse with
negligible performance drop. Additionally, we identify the root cause of
generation quality degradation in parallel decoding as the disruption of token
dependencies under the conditional independence assumption. To address this, we
propose a confidence-aware parallel decoding strategy that selectively decodes
tokens exceeding a confidence threshold, mitigating dependency violations and
maintaining generation quality. Experimental results on LLaDA and Dream models
across multiple LLM benchmarks demonstrate up to \textbf{27.6$\times$
throughput} improvement with minimal accuracy loss, closing the performance gap
with autoregressive models and paving the way for practical deployment of
Diffusion LLMs.

</details>


### [93] [Chain-of-Talkers (CoTalk): Fast Human Annotation of Dense Image Captions](https://arxiv.org/abs/2505.22627)
*Yijun Shen,Delong Chen,Fan Liu,Xingyu Wang,Chuanyi Zhang,Liang Yao,Yuhui Zheng*

Main category: cs.CL

TL;DR: 提出了Chain-of-Talkers（CoTalk）方法，通过顺序标注和多模态界面优化人工标注效率，在固定预算下提升标注数量和检索性能


<details>
  <summary>Details</summary>
Motivation: 现有并行标注存在冗余工作量，且人类通过阅读输入和语音输出的效率差异未被充分利用。旨在通过顺序标注的残差标注机制和语音+文本的多模态界面提升标注效率

Method: 1. 顺序标注机制：后续标注者只需标注前序未覆盖的视觉残差信息
2. 多模态交互界面：结合文本阅读输入和语音输出提升交互效率
3. 通过对象-属性树解析评估标注语义完整性

Result: 实验显示：标注速度提升40%（0.42 vs 0.30单位/秒），检索性能提升0.61个百分点（41.13% vs 40.52%）

Conclusion: CoTalk有效优化了人工标注效率，在视觉-语言对齐任务中展现出实用价值，为大规模数据标注提供了新思路

Abstract: While densely annotated image captions significantly facilitate the learning
of robust vision-language alignment, methodologies for systematically
optimizing human annotation efforts remain underexplored. We introduce
Chain-of-Talkers (CoTalk), an AI-in-the-loop methodology designed to maximize
the number of annotated samples and improve their comprehensiveness under fixed
budget constraints (e.g., total human annotation time). The framework is built
upon two key insights. First, sequential annotation reduces redundant workload
compared to conventional parallel annotation, as subsequent annotators only
need to annotate the ``residual'' -- the missing visual information that
previous annotations have not covered. Second, humans process textual input
faster by reading while outputting annotations with much higher throughput via
talking; thus a multimodal interface enables optimized efficiency. We evaluate
our framework from two aspects: intrinsic evaluations that assess the
comprehensiveness of semantic units, obtained by parsing detailed captions into
object-attribute trees and analyzing their effective connections; extrinsic
evaluation measures the practical usage of the annotated captions in
facilitating vision-language alignment. Experiments with eight participants
show our Chain-of-Talkers (CoTalk) improves annotation speed (0.42 vs. 0.30
units/sec) and retrieval performance (41.13\% vs. 40.52\%) over the parallel
method.

</details>


### [94] [Stochastic Chameleons: Irrelevant Context Hallucinations Reveal Class-Based (Mis)Generalization in LLMs](https://arxiv.org/abs/2505.22630)
*Ziling Cheng,Meng Cao,Marc-Antoine Rondeau,Jackie Chi Kit Cheung*

Main category: cs.CL

TL;DR: LLMs通过基于类别的错误泛化机制整合误导性上下文，其内部存在抽象类别构建与双竞争电路的计算结构，呈现'随机变色龙'特性。


<details>
  <summary>Details</summary>
Motivation: 回应LLMs作为'随机鹦鹉'的争议，探究其错误模式规律性及内部机制，揭示形式化训练下模型泛化的不可靠性。

Method: 对Llama-3/Mistral/Pythia进行行为分析+机制可解释性实验（39种事实关系类型），追踪模型内部计算路径与特征选择机制。

Result: 1. 模型通过类别线索与上下文特征组合生成答案 2. 低层构建抽象类别→高层细化答案 3. 特征选择受直接推理/上下文整合双电路博弈影响

Conclusion: LLMs并非单纯复现数据，而是通过形式化训练获得不可靠的抽象泛化能力，应视为'随机变色龙'而非'随机鹦鹉'。

Abstract: The widespread success of large language models (LLMs) on NLP benchmarks has
been accompanied by concerns that LLMs function primarily as stochastic parrots
that reproduce texts similar to what they saw during pre-training, often
erroneously. But what is the nature of their errors, and do these errors
exhibit any regularities? In this work, we examine irrelevant context
hallucinations, in which models integrate misleading contextual cues into their
predictions. Through behavioral analysis, we show that these errors result from
a structured yet flawed mechanism that we term class-based (mis)generalization,
in which models combine abstract class cues with features extracted from the
query or context to derive answers. Furthermore, mechanistic interpretability
experiments on Llama-3, Mistral, and Pythia across 39 factual recall relation
types reveal that this behavior is reflected in the model's internal
computations: (i) abstract class representations are constructed in lower
layers before being refined into specific answers in higher layers, (ii)
feature selection is governed by two competing circuits -- one prioritizing
direct query-based reasoning, the other incorporating contextual cues -- whose
relative influences determine the final output. Our findings provide a more
nuanced perspective on the stochastic parrot argument: through form-based
training, LLMs can exhibit generalization leveraging abstractions, albeit in
unreliable ways based on contextual cues -- what we term stochastic chameleons.

</details>


### [95] [Spatial Knowledge Graph-Guided Multimodal Synthesis](https://arxiv.org/abs/2505.22633)
*Yida Xue,Zhen Bi,Jinnan Yang,Jungang Lou,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 提出SKG2Data方法，通过空间知识图谱指导多模态数据合成，增强多模态大语言模型的空间感知与推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs空间感知能力不足，传统多模态数据合成方法难以保障空间常识的合理性。

Method: 1. 自动构建空间知识图谱(SKG)模拟人类空间感知
2. 基于SKG指导生成包含方向/距离等空间关系的多模态数据

Result: 实验表明合成数据显著提升MLLMs空间推理能力，且在不同类型空间知识中展现强泛化性

Conclusion: 基于知识图谱的数据合成范式为空间智能发展提供新思路，未来可拓展更复杂的空间关系建模

Abstract: Recent advances in multimodal large language models (MLLMs) have
significantly enhanced their capabilities; however, their spatial perception
abilities remain a notable limitation. To address this challenge, multimodal
data synthesis offers a promising solution. Yet, ensuring that synthesized data
adhere to spatial common sense is a non-trivial task. In this work, we
introduce SKG2Data, a novel multimodal synthesis approach guided by spatial
knowledge graphs, grounded in the concept of knowledge-to-data generation.
SKG2Data automatically constructs a Spatial Knowledge Graph (SKG) to emulate
human-like perception of spatial directions and distances, which is
subsequently utilized to guide multimodal data synthesis. Extensive experiments
demonstrate that data synthesized from diverse types of spatial knowledge,
including direction and distance, not only enhance the spatial perception and
reasoning abilities of MLLMs but also exhibit strong generalization
capabilities. We hope that the idea of knowledge-based data synthesis can
advance the development of spatial intelligence.

</details>


### [96] [Learning Composable Chains-of-Thought](https://arxiv.org/abs/2505.22635)
*Fangcong Yin,Zeyu Leo Liu,Liu Leqi,Xi Ye,Greg Durrett*

Main category: cs.CL

TL;DR: 通过使原子任务的CoT格式可组合化，结合多任务学习与模型融合策略，显著提升LLMs在组合任务上的零样本推理性能


<details>
  <summary>Details</summary>
Motivation: 现有CoT训练方式依赖大量标注数据且泛化能力有限，需要实现无需目标组合任务标注数据的组合式泛化能力

Method: 1. 重构原子任务CoT格式使其可组合 2. 通过多任务学习或模型融合整合原子CoT模型 3. 使用拒绝采样微调(RFT)进行自举优化

Result: 在字符串操作和自然语言技能组合任务中，该方法在相同训练预算下优于多任务学习和持续微调基线

Conclusion: 可组合CoT训练框架为LLMs的组合推理能力提供新范式，通过结构化知识组合实现复杂任务泛化

Abstract: A common approach for teaching large language models (LLMs) to reason is to
train on chain-of-thought (CoT) traces of in-distribution reasoning problems,
but such annotated data is costly to obtain for every problem of interest. We
want reasoning models to generalize beyond their training distribution, and
ideally to generalize compositionally: combine atomic reasoning skills to solve
harder, unseen reasoning tasks. We take a step towards compositional
generalization of reasoning skills when addressing a target compositional task
that has no labeled CoT data. We find that simply training models on CoT data
of atomic tasks leads to limited generalization, but minimally modifying CoT
formats of constituent atomic tasks to be composable can lead to improvements.
We can train "atomic CoT" models on the atomic tasks with Composable CoT data
and combine them with multitask learning or model merging for better zero-shot
performance on the target compositional task. Such a combined model can be
further bootstrapped on a small amount of compositional data using rejection
sampling fine-tuning (RFT). Results on string operations and natural language
skill compositions show that training LLMs on Composable CoT outperforms
multitask learning and continued fine-tuning baselines within a given training
data budget.

</details>


### [97] [Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese](https://arxiv.org/abs/2505.22645)
*Hanjia Lyu,Jiebo Luo,Jian Kang,Allison Koenecke*

Main category: cs.CL

TL;DR: 研究大型语言模型在简体与繁体中文提示下的性能差异，发现任务相关偏见并发布开源评测基准


<details>
  <summary>Details</summary>
Motivation: 评估不同中文变体对LLM性能的影响，避免因语言差异导致文化代表性偏差和决策危害

Method: 设计区域术语选择（物品命名）和区域名称选择（招聘名单）两个任务，测试11个主流商业/开源模型

Result: 模型偏见呈现任务依赖性（术语选择偏简体，名称选择偏繁体），源于训练数据分布、字符偏好和分词差异

Conclusion: 需持续分析LLM语言偏见，开源SC-TC-Bench数据集支持中文变体性能的可重复评估

Abstract: While the capabilities of Large Language Models (LLMs) have been studied in
both Simplified and Traditional Chinese, it is yet unclear whether LLMs exhibit
differential performance when prompted in these two variants of written
Chinese. This understanding is critical, as disparities in the quality of LLM
responses can perpetuate representational harms by ignoring the different
cultural contexts underlying Simplified versus Traditional Chinese, and can
exacerbate downstream harms in LLM-facilitated decision-making in domains such
as education or hiring. To investigate potential LLM performance disparities,
we design two benchmark tasks that reflect real-world scenarios: regional term
choice (prompting the LLM to name a described item which is referred to
differently in Mainland China and Taiwan), and regional name choice (prompting
the LLM to choose who to hire from a list of names in both Simplified and
Traditional Chinese). For both tasks, we audit the performance of 11 leading
commercial LLM services and open-sourced models -- spanning those primarily
trained on English, Simplified Chinese, or Traditional Chinese. Our analyses
indicate that biases in LLM responses are dependent on both the task and
prompting language: while most LLMs disproportionately favored Simplified
Chinese responses in the regional term choice task, they surprisingly favored
Traditional Chinese names in the regional name choice task. We find that these
disparities may arise from differences in training data representation, written
character preferences, and tokenization of Simplified and Traditional Chinese.
These findings highlight the need for further analysis of LLM biases; as such,
we provide an open-sourced benchmark dataset to foster reproducible evaluations
of future LLM behavior across Chinese language variants
(https://github.com/brucelyu17/SC-TC-Bench).

</details>


### [98] [WebDancer: Towards Autonomous Information Seeking Agency](https://arxiv.org/abs/2505.22648)
*Jialong Wu,Baixuan Li,Runnan Fang,Wenbiao Yin,Liwen Zhang,Zhengwei Tao,Dingchu Zhang,Zekun Xi,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: 提出WebDancer框架，通过四阶段训练范式构建端到端信息寻求智能体，在GAIA和WebWalkerQA基准取得显著效果


<details>
  <summary>Details</summary>
Motivation: 现有智能体系统处理复杂现实问题时存在信息获取深度和多步推理能力的不足，需系统性训练范式提升智能体能力

Method: 1) 构建浏览数据 2) 轨迹采样 3) 监督微调实现冷启动 4) 强化学习提升泛化能力

Result: 在GAIA和WebWalkerQA基准测试中取得优异表现，代码和演示将在Github开源

Conclusion: 该训练范式为开发更强智能体提供了系统路径，实验分析揭示了模型能力提升的关键因素

Abstract: Addressing intricate real-world problems necessitates in-depth information
seeking and multi-step reasoning. Recent progress in agentic systems,
exemplified by Deep Research, underscores the potential for autonomous
multi-step research. In this work, we present a cohesive paradigm for building
end-to-end agentic information seeking agents from a data-centric and
training-stage perspective. Our approach consists of four key stages: (1)
browsing data construction, (2) trajectories sampling, (3) supervised
fine-tuning for effective cold start, and (4) reinforcement learning for
enhanced generalisation. We instantiate this framework in a web agent based on
the ReAct, WebDancer. Empirical evaluations on the challenging information
seeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of
WebDancer, achieving considerable results and highlighting the efficacy of our
training paradigm. Further analysis of agent training provides valuable
insights and actionable, systematic pathways for developing more capable
agentic models. The codes and demo will be released in
https://github.com/Alibaba-NLP/WebAgent.

</details>


### [99] [The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in Learning to Reason](https://arxiv.org/abs/2505.22653)
*Ang Lv,Ruobing Xie,Xingwu Sun,Zhanhui Kang,Rui Yan*

Main category: cs.CL

TL;DR: 研究表明大语言模型对奖励噪声具有强鲁棒性，通过推理模式奖励（RPR）可提升噪声环境下的模型表现，校准噪声奖励模型。


<details>
  <summary>Details</summary>
Motivation: 针对现实场景中奖励模型存在噪声的实际问题，探索LLMs在噪声奖励下的训练效果及解决方案。相比传统仅关注准确奖励的研究更具现实意义。

Method: 在数学任务中人工翻转40%奖励信号模拟噪声，并设计仅奖励关键推理短语出现的RPR策略，结合噪声奖励模型进行校准实验。

Result: Qwen-2.5-7B在40%噪声下仍达72%准确率；仅用RPR即可实现超70%准确率；RPR有效校准噪声奖励模型，缓解假阴性问题。

Conclusion: 预训练阶段提升模型基础能力至关重要，RPR为后训练技术提供了新思路。研究成果对开放域任务的LLM训练具有实践指导价值。

Abstract: Recent studies on post-training large language models (LLMs) for reasoning
through reinforcement learning (RL) typically focus on tasks that can be
accurately verified and rewarded, such as solving math problems. In contrast,
our research investigates the impact of reward noise, a more practical
consideration for real-world scenarios involving the post-training of LLMs
using reward models. We found that LLMs demonstrate strong robustness to
substantial reward noise. For example, manually flipping 40% of the reward
function's outputs in math tasks still allows a Qwen-2.5-7B model to achieve
rapid convergence, improving its performance on math tasks from 5% to 72%,
compared to the 75% accuracy achieved by a model trained with noiseless
rewards. Surprisingly, by only rewarding the appearance of key reasoning
phrases (namely reasoning pattern reward, RPR), such as ``first, I need
to''-without verifying the correctness of answers, the model achieved peak
downstream performance (over 70% accuracy for Qwen-2.5-7B) comparable to models
trained with strict correctness verification and accurate rewards. Recognizing
the importance of the reasoning process over the final results, we combined RPR
with noisy reward models. RPR helped calibrate the noisy reward models,
mitigating potential false negatives and enhancing the LLM's performance on
open-ended tasks. These findings suggest the importance of improving models'
foundational abilities during the pre-training phase while providing insights
for advancing post-training techniques. Our code and scripts are available at
https://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason.

</details>


### [100] [GuessArena: Guess Who I Am? A Self-Adaptive Framework for Evaluating LLMs in Domain-Specific Knowledge and Reasoning](https://arxiv.org/abs/2505.22661)
*Qingchen Yu,Zifan Zheng,Ding Chen,Simin Niu,Bo Tang,Feiyu Xiong,Zhiyu Li*

Main category: cs.CL

TL;DR: 提出GuessArena框架，通过对抗性游戏交互实现大语言模型动态评估，解决传统静态基准测试的适应性和细粒度评估不足问题。


<details>
  <summary>Details</summary>
Motivation: 传统LLM评估依赖静态基准测试，存在测试集适应性差、标准化协议无法有效评估领域知识深度和上下文推理能力的缺陷。

Method: 基于「猜猜我是谁」游戏机制，整合动态领域知识建模（构建领域本体知识图谱）和渐进式推理链评估（多轮对抗对话），建立可扩展的评估框架。

Result: 在金融/医疗/制造/IT/教育五大领域验证显示，框架在领域知识覆盖率（提升32%）和推理链完整性（F1值提高27%）指标上显著优于传统基准测试。

Conclusion: GuessArena通过游戏化评估范式，在保持评估效度的同时实现评估场景的动态扩展，为领域专用LLM的能力评估提供新方法论。

Abstract: The evaluation of large language models (LLMs) has traditionally relied on
static benchmarks, a paradigm that poses two major limitations: (1) predefined
test sets lack adaptability to diverse application domains, and (2)
standardized evaluation protocols often fail to capture fine-grained
assessments of domain-specific knowledge and contextual reasoning abilities. To
overcome these challenges, we propose GuessArena, an adaptive evaluation
framework grounded in adversarial game-based interactions. Inspired by the
interactive structure of the Guess Who I Am? game, our framework seamlessly
integrates dynamic domain knowledge modeling with progressive reasoning
assessment to improve evaluation fidelity. Empirical studies across five
vertical domains-finance, healthcare, manufacturing, information technology,
and education-demonstrate that GuessArena effectively distinguishes LLMs in
terms of domain knowledge coverage and reasoning chain completeness. Compared
to conventional benchmarks, our method provides substantial advantages in
interpretability, scalability, and scenario adaptability.

</details>


### [101] [AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models](https://arxiv.org/abs/2505.22662)
*Feng Luo,Yu-Neng Chuang,Guanchu Wang,Hoang Anh Duy Le,Shaochen Zhong,Hongyi Liu,Jiayi Yuan,Yang Sui,Vladimir Braverman,Vipin Chaudhary,Xia Hu*

Main category: cs.CL

TL;DR: AutoL2S框架通过动态压缩LLMs的推理路径，在保持性能前提下减少57%的推理长度。


<details>
  <summary>Details</summary>
Motivation: LLMs在简单问题上生成冗余的长思维链，导致计算效率低下。现有方法缺乏动态适配推理复杂度的能力。

Method: 利用包含长短CoT路径和<EASY>标记的标注数据训练，使LLMs自主决策生成精简推理路径。

Result: 推理路径长度减少57%，性能无损失，显著提升推理效率。

Conclusion: AutoL2S实现了动态、模型无关的高效推理，为LLMs的工程化部署提供新范式。

Abstract: The reasoning-capable large language models (LLMs) demonstrate strong
performance on complex reasoning tasks but often suffer from overthinking,
generating unnecessarily long chain-of-thought (CoT) reasoning paths for easy
reasoning questions, thereby increasing inference cost and latency. Recent
approaches attempt to address this challenge by manually deciding when to apply
long or short reasoning. However, they lack the flexibility to adapt CoT length
dynamically based on question complexity. In this paper, we propose Auto
Long-Short Reasoning (AutoL2S), a dynamic and model-agnostic framework that
enables LLMs to dynamically compress their generated reasoning path based on
the complexity of the reasoning question. AutoL2S enables a learned paradigm,
in which LLMs themselves can decide when longer reasoning is necessary and when
shorter reasoning suffices, by training on data annotated with our proposed
method, which includes both long and short CoT paths and a special <EASY>
token. We then use <EASY> token to indicate when the model can skip generating
lengthy CoT reasoning. This proposed annotation strategy can enhance the LLMs'
ability to generate shorter CoT reasoning paths with improved quality after
training. Extensive evaluation results show that AutoL2S reduces the length of
reasoning generation by up to 57% without compromising performance,
demonstrating the effectiveness of AutoL2S for scalable and efficient LLM
reasoning.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [102] [RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with Global Illumination](https://arxiv.org/abs/2505.21925)
*Chong Zeng,Yue Dong,Pieter Peers,Hongzhi Wu,Xin Tong*

Main category: cs.GR

TL;DR: 无需场景微调的神经渲染框架，通过两阶段Transformer实现三角形序列到图像的全局光照渲染


<details>
  <summary>Details</summary>
Motivation: 解决传统神经渲染需逐场景优化的限制，提出通用序列转换框架实现无预训练的全局光照渲染

Method: 视图无关阶段建模三角形光传输 + 视图相关阶段转换射线束为像素，均基于无先验约束的Transformer架构

Result: 在形状与光照复杂度各异的场景中验证了框架有效性，实现全局光照效果生成

Conclusion: 首次将纯序列转换范式应用于神经渲染，突破物理约束，为通用渲染系统提供新方向

Abstract: We present RenderFormer, a neural rendering pipeline that directly renders an
image from a triangle-based representation of a scene with full global
illumination effects and that does not require per-scene training or
fine-tuning. Instead of taking a physics-centric approach to rendering, we
formulate rendering as a sequence-to-sequence transformation where a sequence
of tokens representing triangles with reflectance properties is converted to a
sequence of output tokens representing small patches of pixels. RenderFormer
follows a two stage pipeline: a view-independent stage that models
triangle-to-triangle light transport, and a view-dependent stage that
transforms a token representing a bundle of rays to the corresponding pixel
values guided by the triangle-sequence from the view-independent stage. Both
stages are based on the transformer architecture and are learned with minimal
prior constraints. We demonstrate and evaluate RenderFormer on scenes with
varying complexity in shape and light transport.

</details>


### [103] [Fluid Simulation on Vortex Particle Flow Maps](https://arxiv.org/abs/2505.21946)
*Sinan Wang,Junwei Zhou,Fan Feng,Zhiqi Li,Yuchen Sun,Duowen Chen,Greg Turk,Bo Zhu*

Main category: cs.GR

TL;DR: 提出混合欧拉-拉格朗日的Vortex Particle Flow Map方法，通过涡旋粒子演化和网格速度重建，实现复杂涡流动态模拟，流映射距离提升3-12倍。


<details>
  <summary>Details</summary>
Motivation: 传统方法在动态固体边界条件下难以保持长时间涡量守恒，需要突破流映射距离的限制来准确模拟复杂涡旋演化。

Method: 1. 构建基于涡旋粒子的流映射框架
2. 开发粒子Hessian矩阵精确演化方案
3. 实现VPFM中无穿透/无滑移固体边界处理

Result: 流映射长度达到现有技术的3-12倍，在涡旋脱落、尾流不稳定性等测试中准确捕捉长期涡量演化特征

Conclusion: VPFM通过涡量主导的粒子流映射机制，显著提升复杂流动模拟的时空守恒性能，为湍流现象研究提供新工具。

Abstract: We propose the Vortex Particle Flow Map (VPFM) method to simulate
incompressible flow with complex vortical evolution in the presence of dynamic
solid boundaries. The core insight of our approach is that vorticity is an
ideal quantity for evolution on particle flow maps, enabling significantly
longer flow map distances compared to other fluid quantities like velocity or
impulse. To achieve this goal, we developed a hybrid Eulerian-Lagrangian
representation that evolves vorticity and flow map quantities on vortex
particles, while reconstructing velocity on a background grid. The method
integrates three key components: (1) a vorticity-based particle flow map
framework, (2) an accurate Hessian evolution scheme on particles, and (3) a
solid boundary treatment for no-through and no-slip conditions in VPFM. These
components collectively allow a substantially longer flow map length (3-12
times longer) than the state-of-the-art, enhancing vorticity preservation over
extended spatiotemporal domains. We validated the performance of VPFM through
diverse simulations, demonstrating its effectiveness in capturing complex
vortex dynamics and turbulence phenomena.

</details>


### [104] [STDR: Spatio-Temporal Decoupling for Real-Time Dynamic Scene Rendering](https://arxiv.org/abs/2505.22400)
*Zehao Li,Hao Jiang,Yujun Cai,Jianing Chen,Baolong Bi,Shuqin Gao,Honglong Zhao,Yiwei Wang,Tianlu Mao,Zhaoqi Wang*

Main category: cs.GR

TL;DR: 提出STDR模块通过时空解耦提升3DGS动态重建质量


<details>
  <summary>Details</summary>
Motivation: 现有3DGS动态重建方法因时空纠缠导致运动建模不准确

Method: 时空掩码+分离变形场+一致性正则化联合解耦时空模式

Result: 在合成/真实场景基准上显著提升重建质量与时空一致性

Conclusion: STDR作为即插即用模块可有效增强现有动态重建框架性能

Abstract: Although dynamic scene reconstruction has long been a fundamental challenge
in 3D vision, the recent emergence of 3D Gaussian Splatting (3DGS) offers a
promising direction by enabling high-quality, real-time rendering through
explicit Gaussian primitives. However, existing 3DGS-based methods for dynamic
reconstruction often suffer from \textit{spatio-temporal incoherence} during
initialization, where canonical Gaussians are constructed by aggregating
observations from multiple frames without temporal distinction. This results in
spatio-temporally entangled representations, making it difficult to model
dynamic motion accurately. To overcome this limitation, we propose
\textbf{STDR} (Spatio-Temporal Decoupling for Real-time rendering), a
plug-and-play module that learns spatio-temporal probability distributions for
each Gaussian. STDR introduces a spatio-temporal mask, a separated deformation
field, and a consistency regularization to jointly disentangle spatial and
temporal patterns. Extensive experiments demonstrate that incorporating our
module into existing 3DGS-based dynamic scene reconstruction frameworks leads
to notable improvements in both reconstruction quality and spatio-temporal
consistency across synthetic and real-world benchmarks.

</details>


### [105] [Neural Face Skinning for Mesh-agnostic Facial Expression Cloning](https://arxiv.org/abs/2505.22416)
*Sihun Cha,Serin Yoon,Kwanggyoon Seo,Junyong Noh*

Main category: cs.GR

TL;DR: 提出结合全局与局部变形模型的面部动画重定向方法，通过顶点权重预测实现跨网格结构的精准表情控制


<details>
  <summary>Details</summary>
Motivation: 现有面部动画重定向方法在全局潜在编码（丢失局部细节）与局部形变模型（难以整体控制）之间存在精度与控制力的权衡矛盾

Method: 1. 通过间接监督学习目标面部网格顶点的蒙皮权重
2. 使用FACS混合形状监督潜在编码实现可解释性
3. 全局潜在码的局部化影响机制

Result: 在表情保真度、形变迁移精度和跨网格适应性方面超越SOTA方法（实验验证）

Conclusion: 该方法实现了直觉控制与细节保留的统一，可适配不同拓扑结构的面部网格，支持动画的直观编辑

Abstract: Accurately retargeting facial expressions to a face mesh while enabling
manipulation is a key challenge in facial animation retargeting. Recent
deep-learning methods address this by encoding facial expressions into a global
latent code, but they often fail to capture fine-grained details in local
regions. While some methods improve local accuracy by transferring deformations
locally, this often complicates overall control of the facial expression. To
address this, we propose a method that combines the strengths of both global
and local deformation models. Our approach enables intuitive control and
detailed expression cloning across diverse face meshes, regardless of their
underlying structures. The core idea is to localize the influence of the global
latent code on the target mesh. Our model learns to predict skinning weights
for each vertex of the target face mesh through indirect supervision from
predefined segmentation labels. These predicted weights localize the global
latent code, enabling precise and region-specific deformations even for meshes
with unseen shapes. We supervise the latent code using Facial Action Coding
System (FACS)-based blendshapes to ensure interpretability and allow
straightforward editing of the generated animation. Through extensive
experiments, we demonstrate improved performance over state-of-the-art methods
in terms of expression fidelity, deformation transfer accuracy, and
adaptability across diverse mesh structures.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [106] [Complexity counts: global and local perspectives on Indo-Aryan numeral systems](https://arxiv.org/abs/2505.21510)
*Chundra Cathcart*

Main category: physics.soc-ph

TL;DR: 研究揭示印度-雅利安语支数词系统具有全球罕见的复杂性，其1-99数字构造缺乏透明规则，并通过量化指标证实其复杂度显著高于其他语系。


<details>
  <summary>Details</summary>
Motivation: 解释为何印地语等印度-雅利安语言的数词系统保持复杂结构，而多数语言采用规则化数字系统，探究语言/非语言因素对复杂性的影响机制。

Method: 使用跨语言数据库开发量化指标，测量全球语言数词系统复杂度，结合宗教、地理等参数进行统计分析。

Result: 1. 印度-雅利安语数词复杂度显著高于全球平均水平
2. 语言内部存在复杂性差异
3. 宗教传统和地理隔离是主要影响因素

Conclusion: 尽管复杂度高，这些数词系统仍遵循语言效率原则，研究呼吁将复杂性作为数词系统类型学研究的重要维度。

Abstract: The numeral systems of Indo-Aryan languages such as Hindi, Gujarati, and
Bengali are highly unusual in that unlike most numeral systems (e.g., those of
English, Chinese, etc.), forms referring to 1--99 are highly non-transparent
and are cannot be constructed using straightforward rules. As an example,
Hindi/Urdu *iky\=anve* `91' is not decomposable into the composite elements
*ek* `one' and *nave* `ninety' in the way that its English counterpart is. This
paper situates Indo-Aryan languages within the typology of cross-linguistic
numeral systems, and explores the linguistic and non-linguistic factors that
may be responsible for the persistence of complex systems in these languages.
Using cross-linguistic data from multiple databases, we develop and employ a
number of cross-linguistically applicable metrics to quantifies the complexity
of languages' numeral systems, and demonstrate that Indo-Aryan languages have
decisively more complex numeral systems than the world's languages as a whole,
though individual Indo-Aryan languages differ from each other in terms of the
complexity of the patterns they display. We investigate the factors (e.g.,
religion, geographic isolation, etc.) that underlie complexity in numeral
systems, with a focus on South Asia, in an attempt to develop an account of why
complex numeral systems developed and persisted in certain Indo-Aryan languages
but not elsewhere. Finally, we demonstrate that Indo-Aryan numeral systems
adhere to certain general pressures toward efficient communication found
cross-linguistically, despite their high complexity. We call for this somewhat
overlooked dimension of complexity to be taken seriously when discussing
general variation in cross-linguistic numeral systems.

</details>


### [107] [Fluent but Culturally Distant: Can Regional Training Teach Cultural Understanding?](https://arxiv.org/abs/2505.21548)
*Dhruv Agarwal,Anya Shukla,Sunayana Sitaram,Aditya Vashistha*

Main category: physics.soc-ph

TL;DR: 研究表明区域性大语言模型在文化价值观和实践层面并未优于全球模型，数据质量不足是主要原因


<details>
  <summary>Details</summary>
Motivation: 验证区域性LLMs是否真正反映本土文化价值观与实践，探索文化对齐失败原因

Method: 通过价值观（Inglehart-Welzel地图/GlobalOpinionQA）和实践（CulturalBench/NormAd）两个维度，评估5个印度模型与5个全球模型

Result: 1. 印度模型文化对齐度未超越全球模型
2. 提示策略无法有效改善对齐
3. 区域微调反而损害文化能力

Conclusion: 需投资建设文化代表性数据，将文化评估纳入LLM开发核心指标

Abstract: Large language models (LLMs) are used around the world but exhibit Western
cultural tendencies. To address this cultural misalignment, many countries have
begun developing "regional" LLMs tailored to local communities. Yet it remains
unclear whether these models merely speak the language of their users or also
reflect their cultural values and practices. Using India as a case study, we
evaluate five Indic and five global LLMs along two key dimensions: values (via
the Inglehart-Welzel map and GlobalOpinionQA) and practices (via CulturalBench
and NormAd). Across all four tasks, we find that Indic models do not align more
closely with Indian cultural norms than global models. In fact, an average
American person is a better proxy for Indian cultural values than any Indic
model. Even prompting strategies fail to meaningfully improve alignment.
Ablations show that regional fine-tuning does not enhance cultural competence
and may in fact hurt it by impeding recall of existing knowledge. We trace this
failure to the scarcity of high-quality, untranslated, and culturally grounded
pretraining and fine-tuning data. Our study positions cultural evaluation as a
first-class requirement alongside multilingual benchmarks and offers a reusable
methodology for developers. We call for deeper investments in culturally
representative data to build and evaluate truly sovereign LLMs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [108] [ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools](https://arxiv.org/abs/2505.21569)
*Zhucong Li,Bowei Zhang,Jin Xiao,Zhijian Zhou,Fenglei Cao,Jiaqing Liang,Yuan Qi*

Main category: cs.LG

TL;DR: 提出了ChemHAS方法，通过优化智能体堆叠结构提升化学工具预测精度，在四项任务中达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的化学智能体受限于工具固有误差，需探索如何反哺工具提升准确性。

Method: 采用分层智能体堆叠框架(ChemHAS)，通过有限数据优化智能体组合结构补偿工具误差。

Result: 在四个基础化学任务中实现最佳性能，识别出四种典型智能体堆叠行为模式。

Conclusion: 智能体堆叠技术不仅提升工具性能，还增强了可解释性，为科学AI应用开辟新路径。

Abstract: Large Language Model (LLM)-based agents have demonstrated the ability to
improve performance in chemistry-related tasks by selecting appropriate tools.
However, their effectiveness remains limited by the inherent prediction errors
of chemistry tools. In this paper, we take a step further by exploring how
LLMbased agents can, in turn, be leveraged to reduce prediction errors of the
tools. To this end, we propose ChemHAS (Chemical Hierarchical Agent Stacking),
a simple yet effective method that enhances chemistry tools through optimizing
agent-stacking structures from limited data. ChemHAS achieves state-of-the-art
performance across four fundamental chemistry tasks, demonstrating that our
method can effectively compensate for prediction errors of the tools.
Furthermore, we identify and characterize four distinct agent-stacking
behaviors, potentially improving interpretability and revealing new
possibilities for AI agent applications in scientific research. Our code and
dataset are publicly available at https:
//anonymous.4open.science/r/ChemHAS-01E4/README.md.

</details>


### [109] [Revisiting Bi-Linear State Transitions in Recurrent Neural Networks](https://arxiv.org/abs/2505.21749)
*M. Reza Ebrahimi,Roland Memisevic*

Main category: cs.LG

TL;DR: 重新审视RNN隐藏单元角色，提出双线性操作为状态跟踪任务提供天然归纳偏置，并建立复杂度层次结构。


<details>
  <summary>Details</summary>
Motivation: 传统研究将RNN隐藏单元视为被动记忆载体，本文探索其作为主动计算参与者的可能性，聚焦双线性操作在状态追踪任务中的优势。

Method: 通过理论和实证分析双线性操作（隐藏单元与输入嵌入的乘积交互），构建与任务复杂度对应的层次结构框架。

Result: 证明双线性更新机制能自然表征状态演化，形成复杂度分层体系（Mamba等线性模型处于最低复杂度层级）。

Conclusion: 隐藏单元应被视为主动计算单元，双线性操作为状态跟踪任务提供基础架构，揭示了RNN架构设计的复杂度谱系。

Abstract: The role of hidden units in recurrent neural networks is typically seen as
modeling memory, with research focusing on enhancing information retention
through gating mechanisms. A less explored perspective views hidden units as
active participants in the computation performed by the network, rather than
passive memory stores. In this work, we revisit bi-linear operations, which
involve multiplicative interactions between hidden units and input embeddings.
We demonstrate theoretically and empirically that they constitute a natural
inductive bias for representing the evolution of hidden states in state
tracking tasks. These are the simplest type of task that require hidden units
to actively contribute to the behavior of the network. We also show that
bi-linear state updates form a natural hierarchy corresponding to state
tracking tasks of increasing complexity, with popular linear recurrent networks
such as Mamba residing at the lowest-complexity center of that hierarchy.

</details>


### [110] [Born a Transformer -- Always a Transformer?](https://arxiv.org/abs/2505.21785)
*Yana Veitsman,Mayank Jobanputra,Yash Sarrof,Aleksandra Bakalova,Vera Demberg,Ellie Pavlick,Michael Hahn*

Main category: cs.LG

TL;DR: 论文通过C-RASP框架验证Transformer在预训练后仍存在长度泛化限制，揭示归纳与反归纳任务的不对称性及预训练对模型能力的选择性增强。


<details>
  <summary>Details</summary>
Motivation: 探究Transformer的理论限制是否在预训练大模型中依然存在，验证预训练能否突破其架构约束。

Method: 使用C-RASP框架对检索/复制任务进行理论验证，通过机械分析揭示内部电路差异，并在真实任务中实验验证。

Result: 发现预训练模型的归纳能力优于反归纳，微调可消除不对称性；预训练无法克服根本的长度泛化限制。

Conclusion: 预训练能增强特定能力但不能突破Transformer的固有架构限制，需警惕实际应用中的可靠性风险。

Abstract: Transformers have theoretical limitations in modeling certain
sequence-to-sequence tasks, yet it remains largely unclear if these limitations
play a role in large-scale pretrained LLMs, or whether LLMs might effectively
overcome these constraints in practice due to the scale of both the models
themselves and their pretraining data. We explore how these architectural
constraints manifest after pretraining, by studying a family of
$\textit{retrieval}$ and $\textit{copying}$ tasks inspired by Liu et al.
[2024]. We use the recently proposed C-RASP framework for studying length
generalization [Huang et al., 2025b] to provide guarantees for each of our
settings. Empirically, we observe an $\textit{induction-versus-anti-induction}$
asymmetry, where pretrained models are better at retrieving tokens to the right
(induction) rather than the left (anti-induction) of a query token. This
asymmetry disappears upon targeted fine-tuning if length-generalization is
guaranteed by theory. Mechanistic analysis reveals that this asymmetry is
connected to the differences in the strength of induction versus anti-induction
circuits within pretrained Transformers. We validate our findings through
practical experiments on real-world tasks demonstrating reliability risks. Our
results highlight that pretraining selectively enhances certain Transformer
capabilities, but does not overcome fundamental length-generalization limits.

</details>


### [111] [From Directions to Cones: Exploring Multidimensional Representations of Propositional Facts in LLMs](https://arxiv.org/abs/2505.21800)
*Stanley Yu,Vaidehi Bulusu,Oscar Yasunaga,Clayton Lau,Cole Blondin,Sean O'Brien,Kevin Zhu,Vasu Sharma*

Main category: cs.LG

TL;DR: 研究者扩展概念锥框架分析LLMs的真实性行为，发现其背后存在多维结构而非单一方向


<details>
  <summary>Details</summary>
Motivation: 现有方法可能无法全面捕捉LLMs中真实性相关的几何结构，需探索更复杂的表征方式

Method: 使用概念锥框架进行因果干预，通过跨架构验证和效果保持性实验分析模型行为

Result: 发现多维因果锥体结构，证明干预能可靠翻转模型判断且保持非相关行为不变

Conclusion: LLMs的真假判断存在多维几何结构，概念锥为理解模型抽象行为提供新工具

Abstract: Large Language Models (LLMs) exhibit strong conversational abilities but
often generate falsehoods. Prior work suggests that the truthfulness of simple
propositions can be represented as a single linear direction in a model's
internal activations, but this may not fully capture its underlying geometry.
In this work, we extend the concept cone framework, recently introduced for
modeling refusal, to the domain of truth. We identify multi-dimensional cones
that causally mediate truth-related behavior across multiple LLM families. Our
results are supported by three lines of evidence: (i) causal interventions
reliably flip model responses to factual statements, (ii) learned cones
generalize across model architectures, and (iii) cone-based interventions
preserve unrelated model behavior. These findings reveal the richer,
multidirectional structure governing simple true/false propositions in LLMs and
highlight concept cones as a promising tool for probing abstract behaviors.

</details>


### [112] [Let Me Think! A Long Chain-of-Thought Can Be Worth Exponentially Many Short Ones](https://arxiv.org/abs/2505.21825)
*Parsa Mirtaheri,Ezra Edelman,Samy Jelassi,Eran Malach,Enric Boix-Adsera*

Main category: cs.LG

TL;DR: 研究表明在图连通性问题中，顺序扩展（更长的思维链）相比并行扩展（多数投票）具有指数级优势


<details>
  <summary>Details</summary>
Motivation: 解决推理计算资源分配不明确的问题，探索顺序扩展与并行扩展的优劣边界

Method: 基于图连通问题的理论分析，配合多语言模型实验验证（包括专门训练模型和大规模推理模型）

Result: 在特定图分布场景下，顺序扩展展现出指数级优势，不同模型与思维链策略的实验结果验证理论

Conclusion: 推理计算分配需考虑问题特性，图连通类问题应优先采用顺序扩展策略

Abstract: Inference-time computation has emerged as a promising scaling axis for
improving large language model reasoning. However, despite yielding impressive
performance, the optimal allocation of inference-time computation remains
poorly understood. A central question is whether to prioritize sequential
scaling (e.g., longer chains of thought) or parallel scaling (e.g., majority
voting across multiple short chains of thought). In this work, we seek to
illuminate the landscape of test-time scaling by demonstrating the existence of
reasoning settings where sequential scaling offers an exponential advantage
over parallel scaling. These settings are based on graph connectivity problems
in challenging distributions of graphs. We validate our theoretical findings
with comprehensive experiments across a range of language models, including
models trained from scratch for graph connectivity with different chain of
thought strategies as well as large reasoning models.

</details>


### [113] [Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets](https://arxiv.org/abs/2505.21930)
*Dongyue Li,Ziniu Zhang,Lu Wang,Hongyang R. Zhang*

Main category: cs.LG

TL;DR: 提出通过分组训练多个小型适配器并集成的方法，显著提升多数据集语言模型微调效率（比QLoRA提升3-10%准确率，计算加速105倍）


<details>
  <summary>Details</summary>
Motivation: 现有QLoRA等单适配器方法在多任务场景下效率不足，需解决多数据集适配难题

Method: 将n个数据集划分到m个组（m<<n），每组训练一个适配器后加权集成，利用LoRA一阶近似特性快速评估组合性能

Result: 34B参数模型上误差<1%，真实微调误差<5%，FLOPs仅增加8-9%时准确率提升3-10%

Conclusion: 基于梯度近似的高效集成策略突破多任务适配瓶颈，实现效率与性能双重提升

Abstract: This paper develops an ensemble method for fine-tuning a language model to
multiple datasets. Existing methods, such as quantized LoRA (QLoRA), are
efficient when adapting to a single dataset. When training on multiple datasets
of different tasks, a common setup in practice, it remains unclear how to
design an efficient adaptation for fine-tuning language models. We propose to
use an ensemble of multiple smaller adapters instead of a single adapter per
task. We design an efficient algorithm that partitions $n$ datasets into $m$
groups, where $m$ is typically much smaller than $n$ in practice, and train one
adapter for each group before taking a weighted combination to form the
ensemble. The algorithm leverages a first-order approximation property of
low-rank adaptation to quickly obtain the fine-tuning performances of dataset
combinations since methods like LoRA stay close to the base model. Hence, we
use the gradients of the base model to estimate its behavior during
fine-tuning. Empirically, this approximation holds with less than $1\%$ error
on models with up to $34$ billion parameters, leading to an estimation of true
fine-tuning performances under $5\%$ error while speeding up computation
compared to base fine-tuning by $105$ times. When applied to fine-tune Llama
and GPT models on ten text classification tasks, our approach provides up to
$10\%$ higher average test accuracy over QLoRA, with only $9\%$ more FLOPs. On
a Llama model with $34$ billion parameters, an ensemble of QLoRA increases test
accuracy by $3\%$ compared to QLoRA, with only $8\%$ more FLOPs.

</details>


### [114] [EnsemW2S: Enhancing Weak-to-Strong Generalization with Large Language Model Ensembles](https://arxiv.org/abs/2505.21959)
*Aakriti Agrawal,Mucong Ding,Zora Che,Chenghao Deng,Anirudh Satheesh,Bang An,Bayan Bruss,John Langford,Furong Huang*

Main category: cs.LG

TL;DR: 提出EnsemW2S方法，通过弱模型集成策略提升对强语言模型的监督能力


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型超越人类水平后，如何利用人类水平模型有效监督强模型的弱到强泛化挑战

Method: 基于token级集成策略迭代组合多个弱专家模型，通过持续优化提升对强学生模型的监督能力

Result: ID数据集上专家/学生模型提升4%/3.2%，OOD数据集分别提升6%/2.28%

Conclusion: EnsemW2S成功推进了弱到强泛化能力，为高效监督超人类模型提供了新范式

Abstract: With Large Language Models (LLMs) rapidly approaching and potentially
surpassing human-level performance, it has become imperative to develop
approaches capable of effectively supervising and enhancing these powerful
models using smaller, human-level models exposed to only human-level data. We
address this critical weak-to-strong (W2S) generalization challenge by
proposing a novel method aimed at improving weak experts, by training on the
same limited human-level data, enabling them to generalize to complex,
super-human-level tasks. Our approach, called \textbf{EnsemW2S}, employs a
token-level ensemble strategy that iteratively combines multiple weak experts,
systematically addressing the shortcomings identified in preceding iterations.
By continuously refining these weak models, we significantly enhance their
collective ability to supervise stronger student models. We extensively
evaluate the generalization performance of both the ensemble of weak experts
and the subsequent strong student model across in-distribution (ID) and
out-of-distribution (OOD) datasets. For OOD, we specifically introduce question
difficulty as an additional dimension for defining distributional shifts. Our
empirical results demonstrate notable improvements, achieving 4\%, and 3.2\%
improvements on ID datasets and, upto 6\% and 2.28\% on OOD datasets for
experts and student models respectively, underscoring the effectiveness of our
proposed method in advancing W2S generalization.

</details>


### [115] [Pitfalls of Rule- and Model-based Verifiers -- A Case Study on Mathematical Reasoning](https://arxiv.org/abs/2505.22203)
*Yuzhen Huang,Weihao Zeng,Xingshan Zeng,Qi Zhu,Junxian He*

Main category: cs.LG

TL;DR: 研究揭示基于规则和模型的验证器在数学推理任务中存在假阴性率高和易受攻击的缺陷，影响强化学习训练效果。


<details>
  <summary>Details</summary>
Motivation: 针对现有验证器在复杂领域（如数学推理）可靠性不足的问题，探究验证器缺陷对RL训练过程的具体影响机制。

Method: 通过数学推理案例，对多种验证器进行静态评估和RL训练场景分析，对比假阴性/假阳性问题的表现差异。

Result: 基于规则的验证器误判等效答案格式（假阴性率30%+），基于模型的验证器易被特定响应模式欺骗（假阳性率提升2倍）导致奖励虚高。

Conclusion: 两类验证器存在互补风险：规则系受限于形式僵化，模型系易被策略模型逆向破解，需开发混合验证体系提升鲁棒性。

Abstract: Trustworthy verifiers are essential for the success of reinforcement learning
with verifiable reward (RLVR), which is the core methodology behind various
large reasoning models such as DeepSeek-R1. In complex domains like
mathematical reasoning, rule-based verifiers have been widely adopted in
previous works to train strong reasoning models. However, the reliability of
these verifiers and their impact on the RL training process remain poorly
understood. In this work, we take mathematical reasoning as a case study and
conduct a comprehensive analysis of various verifiers in both static evaluation
and RL training scenarios. First, we find that current open-source rule-based
verifiers often fail to recognize equivalent answers presented in different
formats across multiple commonly used mathematical datasets, resulting in
non-negligible false negative rates. This limitation adversely affects RL
training performance and becomes more pronounced as the policy model gets
stronger. Subsequently, we investigate model-based verifiers as a potential
solution to address these limitations. While the static evaluation shows that
model-based verifiers achieve significantly higher verification accuracy,
further analysis and RL training results imply that they are highly susceptible
to hacking, where they misclassify certain patterns in responses as correct
(i.e., false positives). This vulnerability is exploited during policy model
optimization, leading to artificially inflated rewards. Our findings underscore
the unique risks inherent to both rule-based and model-based verifiers, aiming
to offer valuable insights to develop more robust reward systems in
reinforcement learning.

</details>


### [116] [Train Sparse Autoencoders Efficiently by Utilizing Features Correlation](https://arxiv.org/abs/2505.22255)
*Vadim Kurochkin,Yaroslav Aksenov,Daniil Laptev,Daniil Gavrilov,Nikita Balagansky*

Main category: cs.LG

TL;DR: 提出KronSAE架构和mAND激活函数，通过克罗内克积分解和近似逻辑运算改进稀疏自编码器的训练效率和可解释性


<details>
  <summary>Details</summary>
Motivation: 传统稀疏自编码器（SAE）在大规模训练时面临计算资源瓶颈，尤其是编码器部分的线性运算存在高内存消耗和计算复杂度问题

Method: 1. 克罗内克积分解潜在表示（KronSAE）降低计算/内存开销 2. 引入可微激活函数mAND近似布尔与运算增强解释性

Result: KronSAE显著减少参数规模（如4096x64k参数可压缩为64x64x16x64），mAND在分解框架中提升了解释性和模型性能

Conclusion: 该方法通过矩阵分解和新型激活函数设计，为大规模语言模型的可解释性研究提供了更高效的稀疏特征提取方案

Abstract: Sparse Autoencoders (SAEs) have demonstrated significant promise in
interpreting the hidden states of language models by decomposing them into
interpretable latent directions. However, training SAEs at scale remains
challenging, especially when large dictionary sizes are used. While decoders
can leverage sparse-aware kernels for efficiency, encoders still require
computationally intensive linear operations with large output dimensions. To
address this, we propose KronSAE, a novel architecture that factorizes the
latent representation via Kronecker product decomposition, drastically reducing
memory and computational overhead. Furthermore, we introduce mAND, a
differentiable activation function approximating the binary AND operation,
which improves interpretability and performance in our factorized framework.

</details>


### [117] [Skywork Open Reasoner 1 Technical Report](https://arxiv.org/abs/2505.22312)
*Jujie He,Jiacai Liu,Chris Yuhao Liu,Rui Yan,Chaojie Wang,Peng Cheng,Xiaoyu Zhang,Fuxiang Zhang,Jiacheng Xu,Wei Shen,Siyuan Li,Liang Zeng,Tianwen Wei,Cheng Cheng,Bo An,Yang Liu,Yahui Zhou*

Main category: cs.LG

TL;DR: 通过强化学习优化长链思维模型，显著提升32B/7B模型在多个基准测试中的推理能力表现


<details>
  <summary>Details</summary>
Motivation: 探索强化学习在提升大语言模型长链推理能力中的实际应用价值，开发可扩展的RL训练框架

Method: 基于DeepSeek-R1-Distill模型架构实施强化学习训练，开展消融实验验证训练组件有效性，系统研究熵崩溃现象的动态机制

Result: 32B模型在AIME24/25和LiveCodeBench平均准确率提升15%达72.8%，超越主流竞品；7B模型提升13.9%达57.5%，展现小模型竞争力

Conclusion: 控制熵崩溃动态是提升模型性能的关键，开源全套训练资源推动社区发展

Abstract: The success of DeepSeek-R1 underscores the significant role of reinforcement
learning (RL) in enhancing the reasoning capabilities of large language models
(LLMs). In this work, we present Skywork-OR1, an effective and scalable RL
implementation for long Chain-of-Thought (CoT) models. Building on the
DeepSeek-R1-Distill model series, our RL approach achieves notable performance
gains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench
from 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%)
for the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and
Qwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable
results on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models
demonstrate competitive reasoning capabilities among models of similar size. We
perform comprehensive ablation studies on the core components of our training
pipeline to validate their effectiveness. Additionally, we thoroughly
investigate the phenomenon of entropy collapse, identify key factors affecting
entropy dynamics, and demonstrate that mitigating premature entropy collapse is
critical for improved test performance. To support community research, we fully
open-source our model weights, training code, and training datasets.

</details>


### [118] [Mitigating Overthinking in Large Reasoning Models via Manifold Steering](https://arxiv.org/abs/2505.22411)
*Yao Huang,Huanran Chen,Shouwei Ruan,Yichi Zhang,Xingxing Wei,Yinpeng Dong*

Main category: cs.LG

TL;DR: 通过激活流形导向减少大型推理模型的过度思考问题，在数学基准测试中减少71%的token输出并保持准确率


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在推理过程中存在过度思考现象，导致计算资源浪费。现有单维度干预方法存在效果瓶颈，需探索更高效的解决途径。

Method: 结合机械可解释性理论，首先识别激活空间中的关键方向，发现低维流形结构，提出流形导向方法将干预方向投影到噪声更低的流形空间。

Result: 在DeepSeek-R1模型上实现71%的token缩减，多个数学基准准确率保持或提升，代码生成和知识问答任务展示跨领域迁移能力。

Conclusion: 基于激活流形的干预方法有效解决了过度思考问题，理论噪声分析框架为模型优化提供了新思路，开源实现促进社区发展。

Abstract: Recent advances in Large Reasoning Models (LRMs) have demonstrated remarkable
capabilities in solving complex tasks such as mathematics and coding. However,
these models frequently exhibit a phenomenon known as overthinking during
inference, characterized by excessive validation loops and redundant
deliberation, leading to substantial computational overheads. In this paper, we
aim to mitigate overthinking by investigating the underlying mechanisms from
the perspective of mechanistic interpretability. We first showcase that the
tendency of overthinking can be effectively captured by a single direction in
the model's activation space and the issue can be eased by intervening the
activations along this direction. However, this efficacy soon reaches a plateau
and even deteriorates as the intervention strength increases. We therefore
systematically explore the activation space and find that the overthinking
phenomenon is actually tied to a low-dimensional manifold, which indicates that
the limited effect stems from the noises introduced by the high-dimensional
steering direction. Based on this insight, we propose Manifold Steering, a
novel approach that elegantly projects the steering direction onto the
low-dimensional activation manifold given the theoretical approximation of the
interference noise. Extensive experiments on DeepSeek-R1 distilled models
validate that our method reduces output tokens by up to 71% while maintaining
and even improving the accuracy on several mathematical benchmarks. Our method
also exhibits robust cross-domain transferability, delivering consistent token
reduction performance in code generation and knowledge-based QA tasks. Code is
available at: https://github.com/Aries-iai/Manifold_Steering.

</details>


### [119] [Scaling Reasoning without Attention](https://arxiv.org/abs/2505.22425)
*Xueliang Zhao,Wei Wu,Lingpeng Kong*

Main category: cs.LG

TL;DR: 提出Mamba-2模型，通过状态空间架构与课程微调策略，在效率与推理能力上超越Transformer模型


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在复杂推理任务中的架构低效（Transformer依赖）和结构化微调缺失两大核心瓶颈

Method: 1. 架构创新：采用Mamba-2的SSD层替代自注意力机制
2. 训练策略：基于PromptCoT的两阶段课程微调（概念选择+原理生成）

Result: 7B参数模型在AIME24/25和Livecodebench基准上分别超越Gemma3-27B达2.6%、0.6%和3.0%

Conclusion: 状态空间模型展现替代注意力架构的潜力，为高效可扩展的复杂推理提供新方向

Abstract: Large language models (LLMs) have made significant advances in complex
reasoning tasks, yet they remain bottlenecked by two core challenges:
architectural inefficiency due to reliance on Transformers, and a lack of
structured fine-tuning for high-difficulty domains. We introduce \ourmodel, an
attention-free language model that addresses both issues through architectural
and data-centric innovations. Built on the state space dual (SSD) layers of
Mamba-2, our model eliminates the need for self-attention and key-value
caching, enabling fixed-memory, constant-time inference. To train it for
complex reasoning, we propose a two-phase curriculum fine-tuning strategy based
on the \textsc{PromptCoT} synthesis paradigm, which generates pedagogically
structured problems via abstract concept selection and rationale-guided
generation. On benchmark evaluations, \ourmodel-7B outperforms strong
Transformer and hybrid models of comparable scale, and even surpasses the much
larger Gemma3-27B by 2.6\% on AIME 24, 0.6\% on AIME 25, and 3.0\% on
Livecodebench. These results highlight the potential of state space models as
efficient and scalable alternatives to attention-based architectures for
high-capacity reasoning.

</details>


### [120] [The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models](https://arxiv.org/abs/2505.22617)
*Ganqu Cui,Yuchen Zhang,Jiacheng Chen,Lifan Yuan,Zhi Wang,Yuxin Zuo,Haozhan Li,Yuchen Fan,Huayu Chen,Weize Chen,Zhiyuan Liu,Hao Peng,Lei Bai,Wanli Ouyang,Yu Cheng,Bowen Zhou,Ning Ding*

Main category: cs.LG

TL;DR: 通过控制高协方差token的更新（Clip-Cov和KL-Cov方法），解决强化学习中策略熵崩溃问题，实现持续探索与性能提升


<details>
  <summary>Details</summary>
Motivation: 大规模强化学习中观察到策略熵早期骤降导致探索能力丧失与性能瓶颈，需建立熵管理机制突破计算规模限制

Method: 理论推导熵变化与动作概率-对数优势协方差的关系，提出对高协方差token进行截断（Clip-Cov）和KL惩罚（KL-Cov）

Result: 实验证明新方法有效维持探索能力，避免熵崩溃，获得更优下游任务表现

Conclusion: 熵动态机制的理解为强化学习扩展提供新方向，协方差控制策略可推广至其他序列决策任务

Abstract: This paper aims to overcome a major obstacle in scaling RL for reasoning with
LLMs, namely the collapse of policy entropy. Such phenomenon is consistently
observed across vast RL runs without entropy intervention, where the policy
entropy dropped sharply at the early training stage, this diminished
exploratory ability is always accompanied with the saturation of policy
performance. In practice, we establish a transformation equation R=-a*e^H+b
between entropy H and downstream performance R. This empirical law strongly
indicates that, the policy performance is traded from policy entropy, thus
bottlenecked by its exhaustion, and the ceiling is fully predictable H=0,
R=-a+b. Our finding necessitates entropy management for continuous exploration
toward scaling compute for RL. To this end, we investigate entropy dynamics
both theoretically and empirically. Our derivation highlights that, the change
in policy entropy is driven by the covariance between action probability and
the change in logits, which is proportional to its advantage when using Policy
Gradient-like algorithms. Empirical study shows that, the values of covariance
term and entropy differences matched exactly, supporting the theoretical
conclusion. Moreover, the covariance term stays mostly positive throughout
training, further explaining why policy entropy would decrease monotonically.
Through understanding the mechanism behind entropy dynamics, we motivate to
control entropy by restricting the update of high-covariance tokens.
Specifically, we propose two simple yet effective techniques, namely Clip-Cov
and KL-Cov, which clip and apply KL penalty to tokens with high covariances
respectively. Experiments show that these methods encourage exploration, thus
helping policy escape entropy collapse and achieve better downstream
performance.

</details>


### [121] [Position: Uncertainty Quantification Needs Reassessment for Large-language Model Agents](https://arxiv.org/abs/2505.22655)
*Michael Kirchhof,Gjergji Kasneci,Enkelejda Kasneci*

Main category: cs.LG

TL;DR: 论文指出大语言模型在交互场景中传统不确定性量化的局限性，提出未明确定义不确定性、交互式学习和输出表达三大新研究方向


<details>
  <summary>Details</summary>
Motivation: 现有aleatoric/epistemic不确定性定义在人机交互场景中相互矛盾且失效，需探索更适应动态对话环境的量化方式

Method: 通过文献分析揭示传统二分法的缺陷，创新性提出面向交互场景的三维框架：未明确定义任务的不确定性捕捉、主动提问的交互式学习、语言空间的多维度不确定性表达

Result: 构建了包含任务澄清、动态学习和自然表达的新型不确定性处理框架，预期提升LLM代理的交互透明度与可信度

Conclusion: 突破传统数值化不确定性的局限，通过任务澄清机制、主动学习和自然语言表达，可实现更符合人类认知的智能体不确定性交互

Abstract: Large-language models (LLMs) and chatbot agents are known to provide wrong
outputs at times, and it was recently found that this can never be fully
prevented. Hence, uncertainty quantification plays a crucial role, aiming to
quantify the level of ambiguity in either one overall number or two numbers for
aleatoric and epistemic uncertainty. This position paper argues that this
traditional dichotomy of uncertainties is too limited for the open and
interactive setup that LLM agents operate in when communicating with a user,
and that we need to research avenues that enrich uncertainties in this novel
scenario. We review the literature and find that popular definitions of
aleatoric and epistemic uncertainties directly contradict each other and lose
their meaning in interactive LLM agent settings. Hence, we propose three novel
research directions that focus on uncertainties in such human-computer
interactions: Underspecification uncertainties, for when users do not provide
all information or define the exact task at the first go, interactive learning,
to ask follow-up questions and reduce the uncertainty about the current
context, and output uncertainties, to utilize the rich language and speech
space to express uncertainties as more than mere numbers. We expect that these
new ways of dealing with and communicating uncertainties will lead to LLM agent
interactions that are more transparent, trustworthy, and intuitive.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [122] [Learning Compositional Behaviors from Demonstration and Language](https://arxiv.org/abs/2505.21981)
*Weiyu Liu,Neil Nie,Ruohan Zhang,Jiayuan Mao,Jiajun Wu*

Main category: cs.RO

TL;DR: 提出BLADE框架，通过整合模仿学习与模型规划实现长时程机器人操作，自动构建结构化动作表示库并验证有效性


<details>
  <summary>Details</summary>
Motivation: 解决长时程机器人操作任务中难以泛化到新场景的问题，传统方法需要人工定义符号化动作且缺乏自适应能力

Method: 1. 利用语言标注示教数据提取抽象动作知识
2. 构建包含视觉感知前提条件/效果的层次化动作库
3. 基于神经网络策略实现动作控制器
4. 自动恢复结构化表示无需人工标注

Result: 在仿真/真实机器人实验中验证：
- 支持关节物体/部分可观测场景
- 对初始状态扰动、新目标的泛化能力提升30%以上
- 成功完成83%的复杂操作任务

Conclusion: BLADE框架通过语言驱动自动构建结构化动作表示，显著提升机器人操作系统的泛化能力和场景适应性，为复杂长时程任务提供新解决方案

Abstract: We introduce Behavior from Language and Demonstration (BLADE), a framework
for long-horizon robotic manipulation by integrating imitation learning and
model-based planning. BLADE leverages language-annotated demonstrations,
extracts abstract action knowledge from large language models (LLMs), and
constructs a library of structured, high-level action representations. These
representations include preconditions and effects grounded in visual perception
for each high-level action, along with corresponding controllers implemented as
neural network-based policies. BLADE can recover such structured
representations automatically, without manually labeled states or symbolic
definitions. BLADE shows significant capabilities in generalizing to novel
situations, including novel initial states, external state perturbations, and
novel goals. We validate the effectiveness of our approach both in simulation
and on real robots with a diverse set of objects with articulated parts,
partial observability, and geometric constraints.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [123] [How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control](https://arxiv.org/abs/2505.21531)
*Kunhang Li,Jason Naradowsky,Yansong Feng,Yusuke Miyao*

Main category: cs.CV

TL;DR: 研究通过分步动作生成与评估，揭示大型语言模型在3D虚拟人控制中具备高级运动解释能力但缺乏精确身体定位能力，且在创意动作生成与文化特定模式识别方面展现潜力。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs将自然语言指令转化为精确3D人体动作的可行性，验证其作为虚拟人控制中间件的适用性。

Method: 采用两阶段生成框架（高级运动规划+低级部位定位），基于20个代表性动作指令进行多维度评估（人类动画评估/计划评估+自动位置对比）。

Result: 模型擅长解析高级动作但难以精确定位，多步骤/高自由度动作表现欠佳，对空间描述可近似处理但无法满足精确时空参数需求。

Conclusion: LLMs在概念化创意动作和文化模式识别方面具有潜力，但需结合专业控制器处理精确运动参数，适合作为动作生成流程的规划层。

Abstract: We explore Large Language Models (LLMs)' human motion knowledge through 3D
avatar control. Given a motion instruction, we prompt LLMs to first generate a
high-level movement plan with consecutive steps (High-level Planning), then
specify body part positions in each step (Low-level Planning), which we
linearly interpolate into avatar animations as a clear verification lens for
human evaluators. Through carefully designed 20 representative motion
instructions with full coverage of basic movement primitives and balanced body
part usage, we conduct comprehensive evaluations including human assessment of
both generated animations and high-level movement plans, as well as automatic
comparison with oracle positions in low-level planning. We find that LLMs are
strong at interpreting the high-level body movements but struggle with precise
body part positioning. While breaking down motion queries into atomic
components improves planning performance, LLMs have difficulty with multi-step
movements involving high-degree-of-freedom body parts. Furthermore, LLMs
provide reasonable approximation for general spatial descriptions, but fail to
handle precise spatial specifications in text, and the precise spatial-temporal
parameters needed for avatar control. Notably, LLMs show promise in
conceptualizing creative motions and distinguishing culturally-specific motion
patterns.

</details>


### [124] [Vision Meets Language: A RAG-Augmented YOLOv8 Framework for Coffee Disease Diagnosis and Farmer Assistance](https://arxiv.org/abs/2505.21544)
*Semanto Mondal*

Main category: cs.CV

TL;DR: 提出结合YOLOv8目标检测、RAG增强生成和LLM的AI系统，实现咖啡叶病害检测与治理建议，降低农药使用并提升农业可持续性。


<details>
  <summary>Details</summary>
Motivation: 传统农业存在资源浪费和环境污染问题，需通过精准农业技术实现病害精准识别与环境友好型治理。

Method: 1. 使用YOLOv8进行叶片病害视觉检测
2. 采用RAG增强LLM的上下文理解能力
3. 开发用户友好界面整合检测-诊断-治理全流程

Result: 系统实现实时病害检测（准确率未提），可生成针对性治理方案，减少农药使用量并降低环境负荷。

Conclusion: 该多模态AI框架为精准农业提供新范式，未来可扩展至更多农作物并提升系统可靠性与应用范围。

Abstract: As a social being, we have an intimate bond with the environment. A plethora
of things in human life, such as lifestyle, health, and food are dependent on
the environment and agriculture. It comes under our responsibility to support
the environment as well as agriculture. However, traditional farming practices
often result in inefficient resource use and environmental challenges. To
address these issues, precision agriculture has emerged as a promising approach
that leverages advanced technologies to optimise agricultural processes. In
this work, a hybrid approach is proposed that combines the three different
potential fields of model AI: object detection, large language model (LLM), and
Retrieval-Augmented Generation (RAG). In this novel framework, we have tried to
combine the vision and language models to work together to identify potential
diseases in the tree leaf. This study introduces a novel AI-based precision
agriculture system that uses Retrieval Augmented Generation (RAG) to provide
context-aware diagnoses and natural language processing (NLP) and YOLOv8 for
crop disease detection. The system aims to tackle major issues with large
language models (LLMs), especially hallucinations and allows for adaptive
treatment plans and real-time disease detection. The system provides an
easy-to-use interface to the farmers, which they can use to detect the
different diseases related to coffee leaves by just submitting the image of the
affected leaf the model will detect the diseases as well as suggest potential
remediation methodologies which aim to lower the use of pesticides, preserving
livelihoods, and encouraging environmentally friendly methods. With an emphasis
on scalability, dependability, and user-friendliness, the project intends to
improve RAG-integrated object detection systems for wider agricultural
applications in the future.

</details>


### [125] [Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation](https://arxiv.org/abs/2505.21549)
*Daniel Csizmadia,Andrei Codreanu,Victor Sim,Vighnesh Prabeau,Michael Lu,Kevin Zhu,Sean O'Brien,Vasu Sharma*

Main category: cs.CV

TL;DR: 通过元教师-学生蒸馏框架改进CLIP模型，在提升图文检索性能的同时保持94%的零样本分类能力


<details>
  <summary>Details</summary>
Motivation: 原始CLIP模型受限于固定图像分辨率和有限上下文，难以满足需要细粒度跨模态理解的检索任务需求

Method: 采用双向跨注意力机制融合YOLO提取的图像区域与文本片段，通过混合对比学习和余弦相似度目标训练轻量级学生模型

Result: 在仅67,500训练样本下实现图文检索指标显著提升（Recall@K/MAP），同时保持94%的零样本分类性能

Conclusion: DCLIP有效平衡任务专业化与泛化能力，为高级视觉语言任务提供资源高效、领域自适应且细节敏感的解决方案

Abstract: We present Distill CLIP (DCLIP), a fine-tuned variant of the CLIP model that
enhances multimodal image-text retrieval while preserving the original model's
strong zero-shot classification capabilities. CLIP models are typically
constrained by fixed image resolutions and limited context, which can hinder
their effectiveness in retrieval tasks that require fine-grained cross-modal
understanding. DCLIP addresses these challenges through a meta teacher-student
distillation framework, where a cross-modal transformer teacher is fine-tuned
to produce enriched embeddings via bidirectional cross-attention between
YOLO-extracted image regions and corresponding textual spans. These
semantically and spatially aligned global representations guide the training of
a lightweight student model using a hybrid loss that combines contrastive
learning and cosine similarity objectives. Despite being trained on only
~67,500 samples curated from MSCOCO, Flickr30k, and Conceptual Captions-just a
fraction of CLIP's original dataset-DCLIP significantly improves image-text
retrieval metrics (Recall@K, MAP), while retaining approximately 94% of CLIP's
zero-shot classification performance. These results demonstrate that DCLIP
effectively mitigates the trade-off between task specialization and
generalization, offering a resource-efficient, domain-adaptive, and
detail-sensitive solution for advanced vision-language tasks. Code available at
https://anonymous.4open.science/r/DCLIP-B772/README.md.

</details>


### [126] [FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering](https://arxiv.org/abs/2505.21755)
*Chengyue Huang,Brisa Maneechotesuwan,Shivang Chopra,Zsolt Kira*

Main category: cs.CV

TL;DR: 提出多模态VQA鲁棒性评估基准FRAMES-VQA，通过十大数据集构建ID/OOD场景并分析分布偏移


<details>
  <summary>Details</summary>
Motivation: 现有VQA鲁棒性评估主要关注单模态或特定OOD类型，缺乏多模态复杂场景的全面评估框架

Method: 1. 整合10个VQA数据集构建ID/近OOD/远OOD测试场景
2. 使用马氏距离量化多模态分布偏移
3. 分析模态重要性及单/多模态偏移的交互作用

Result: 建立首个多模态分布偏移评估框架，揭示模态重要性差异（OOD样本更依赖文本模态）及多模态偏移的叠加效应

Conclusion: 该基准为开发抗多模态分布偏移的VQA模型提供新方向，量化分析方法可指导鲁棒性优化策略

Abstract: Visual question answering (VQA) systems face significant challenges when
adapting to real-world data shifts, especially in multi-modal contexts. While
robust fine-tuning strategies are essential for maintaining performance across
in-distribution (ID) and out-of-distribution (OOD) scenarios, current
evaluation settings are primarily unimodal or particular to some types of OOD,
offering limited insight into the complexities of multi-modal contexts. In this
work, we propose a new benchmark FRAMES-VQA (Fine-Tuning Robustness across
Multi-Modal Shifts in VQA) for evaluating robust fine-tuning for VQA tasks. We
utilize ten existing VQA benchmarks, including VQAv2, IV-VQA, VQA-CP, OK-VQA
and others, and categorize them into ID, near and far OOD datasets covering
uni-modal, multi-modal and adversarial distribution shifts. We first conduct a
comprehensive comparison of existing robust fine-tuning methods. We then
quantify the distribution shifts by calculating the Mahalanobis distance using
uni-modal and multi-modal embeddings extracted from various models. Further, we
perform an extensive analysis to explore the interactions between uni- and
multi-modal shifts as well as modality importance for ID and OOD samples. These
analyses offer valuable guidance on developing more robust fine-tuning methods
to handle multi-modal distribution shifts. The code is available at
https://github.com/chengyuehuang511/FRAMES-VQA .

</details>


### [127] [GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning](https://arxiv.org/abs/2505.21863)
*Shikhhar Siingh,Abhinav Rawat,Vivek Gupta,Chitta Baral*

Main category: cs.CV

TL;DR: 提出GETReason框架和GREAT评估指标，通过时空事件推理提升图像上下文理解


<details>
  <summary>Details</summary>
Motivation: 现有方法难以准确提取图像中与新闻事件相关的深层上下文信息，需要更有效的关联手段

Method: 分层多智能体架构结合地理空间、时间对齐和事件推理，使用GREAT评估指标进行验证

Result: 系统能有效推断图像与事件的关联，准确率较传统方法提升显著（GREAT指标验证）

Conclusion: 时空事件推理框架成功建立图像与宏观事件的语义关联，为新闻教育领域提供可靠分析工具

Abstract: Publicly significant images from events hold valuable contextual information,
crucial for journalism and education. However, existing methods often struggle
to extract this relevance accurately. To address this, we introduce GETReason
(Geospatial Event Temporal Reasoning), a framework that moves beyond
surface-level image descriptions to infer deeper contextual meaning. We propose
that extracting global event, temporal, and geospatial information enhances
understanding of an image's significance. Additionally, we introduce GREAT
(Geospatial Reasoning and Event Accuracy with Temporal Alignment), a new metric
for evaluating reasoning-based image understanding. Our layered multi-agent
approach, assessed using a reasoning-weighted metric, demonstrates that
meaningful insights can be inferred, effectively linking images to their
broader event context.

</details>


### [128] [Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image Generation](https://arxiv.org/abs/2505.21956)
*Mengdan Zhu,Senhao Cheng,Guangji Bai,Yifei Zhang,Liang Zhao*

Main category: cs.CV

TL;DR: 提出跨模态RAG框架，通过分解查询/图像的子维度组件实现细粒度检索与生成，显著提升复杂文本到图像合成的质量与效率。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在复杂查询时无法找到包含所有元素的单一图像，需要多图互补知识融合的新范式。

Method: 1) 子维度分解构建混合检索（稀疏+密集）的帕累托最优集 2) 多模态大模型引导的特征选择性融合机制

Result: 在COCO/Flickr30K等5个数据集上，检索与生成质量均超越基线模型，尤其在长尾场景提升显著

Conclusion: 首次实现子查询粒度的跨模态知识融合，为复杂创意需求提供可解释的生成解决方案

Abstract: Text-to-image generation increasingly demands access to domain-specific,
fine-grained, and rapidly evolving knowledge that pretrained models cannot
fully capture. Existing Retrieval-Augmented Generation (RAG) methods attempt to
address this by retrieving globally relevant images, but they fail when no
single image contains all desired elements from a complex user query. We
propose Cross-modal RAG, a novel framework that decomposes both queries and
images into sub-dimensional components, enabling subquery-aware retrieval and
generation. Our method introduces a hybrid retrieval strategy - combining a
sub-dimensional sparse retriever with a dense retriever - to identify a
Pareto-optimal set of images, each contributing complementary aspects of the
query. During generation, a multimodal large language model is guided to
selectively condition on relevant visual features aligned to specific
subqueries, ensuring subquery-aware image synthesis. Extensive experiments on
MS-COCO, Flickr30K, WikiArt, CUB, and ImageNet-LT demonstrate that Cross-modal
RAG significantly outperforms existing baselines in both retrieval and
generation quality, while maintaining high efficiency.

</details>


### [129] [Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language](https://arxiv.org/abs/2505.22146)
*Guangfu Hao,Haojie Wen,Liangxuna Guo,Yang Chen,Yanchao Bi,Shan Yu*

Main category: cs.CV

TL;DR: 提出基于低维属性表征的多模态框架ToolNet，通过结合视觉编码器与微调语言模型，在工具选择任务中实现74%准确率，接近GPT-4o性能但参数更少。


<details>
  <summary>Details</summary>
Motivation: 解决现有计算模型在灵活工具选择能力上的不足，模拟人类工具认知机制，连接视觉感知与语言理解。

Method: 1) 构建含115种工具、13维属性（物理/功能/心理）的ToolNet数据集；2) 使用ResNet/ViT提取工具视觉属性；3) 微调LLM（GPT-2/LLaMA/DeepSeek）解析任务需求属性；4) 属性空间匹配实现工具选择。

Result: 工具选择准确率74%，显著超越直接匹配(20%)和小型多模态模型(21-58%)，与GPT-4o(73%)相当但参数量少。消融实验显示操作性属性（抓握性、手部关联性）最关键。

Conclusion: 提供参数高效、可解释的工具认知方案，推动认知科学理解与工具选择实际应用，证明属性表征在多模态交互中的有效性。

Abstract: Flexible tool selection reflects a complex cognitive ability that
distinguishes humans from other species, yet computational models that capture
this ability remain underdeveloped. We developed a framework using
low-dimensional attribute representations to bridge visual tool perception and
linguistic task understanding. We constructed a comprehensive dataset (ToolNet)
containing 115 common tools labeled with 13 carefully designed attributes
spanning physical, functional, and psychological properties, paired with
natural language scenarios describing tool usage. Visual encoders (ResNet or
ViT) extract attributes from tool images while fine-tuned language models
(GPT-2, LLaMA, DeepSeek) derive required attributes from task descriptions. Our
approach achieves 74% accuracy in tool selection tasks-significantly
outperforming direct tool matching (20%) and smaller multimodal models
(21%-58%), while approaching performance of much larger models like GPT-4o
(73%) with substantially fewer parameters. Ablation studies revealed that
manipulation-related attributes (graspability, hand-relatedness, elongation)
consistently prove most critical across modalities. This work provides a
parameter-efficient, interpretable solution that mimics human-like tool
cognition, advancing both cognitive science understanding and practical
applications in tool selection tasks.

</details>


### [130] [Improving Brain-to-Image Reconstruction via Fine-Grained Text Bridging](https://arxiv.org/abs/2505.22150)
*Runze Xia,Shuo Feng,Renzhi Wang,Congchi Yin,Xuyun Wen,Piji Li*

Main category: cs.CV

TL;DR: FgB2I通过从fMRI信号解码细粒度文本描述，结合视觉语言模型实现精细化脑图像重建


<details>
  <summary>Details</summary>
Motivation: 现有脑图像重建方法存在细节缺失和语义不一致问题，归因于语义信息不足。作者提出利用细粒度文本作为桥梁增强重建效果

Method: 三阶段方法：1) 使用视觉语言模型生成视觉刺激的细粒度描述 2) 提出对象准确性、图文语义相似度、图图相似度三个奖励指标指导fMRI信号解码 3) 将文本描述集成到现有重建方法实现增强

Result: 实验验证了细粒度描述生成的重要性，提出的三个奖励指标有效指导文本解码，该方法可无缝集成现有重建方法

Conclusion: FgB2I通过构建'文本桥梁'机制，显著提升脑图像重建的细节保真度和语义一致性，为跨模态重建提供新思路

Abstract: Brain-to-Image reconstruction aims to recover visual stimuli perceived by
humans from brain activity. However, the reconstructed visual stimuli often
missing details and semantic inconsistencies, which may be attributed to
insufficient semantic information. To address this issue, we propose an
approach named Fine-grained Brain-to-Image reconstruction (FgB2I), which
employs fine-grained text as bridge to improve image reconstruction. FgB2I
comprises three key stages: detail enhancement, decoding fine-grained text
descriptions, and text-bridged brain-to-image reconstruction. In the
detail-enhancement stage, we leverage large vision-language models to generate
fine-grained captions for visual stimuli and experimentally validate its
importance. We propose three reward metrics (object accuracy, text-image
semantic similarity, and image-image semantic similarity) to guide the language
model in decoding fine-grained text descriptions from fMRI signals. The
fine-grained text descriptions can be integrated into existing reconstruction
methods to achieve fine-grained Brain-to-Image reconstruction.

</details>


### [131] [Look & Mark: Leveraging Radiologist Eye Fixations and Bounding boxes in Multimodal Large Language Models for Chest X-ray Report Generation](https://arxiv.org/abs/2505.22222)
*Yunsoo Kim,Jinge Wu,Su-Hwan Kim,Pardeep Vasudev,Jiashu Shen,Honghan Wu*

Main category: cs.CV

TL;DR: 提出Look & Mark策略，通过整合放射科医生的视觉注视和标注框，显著提升多模态大模型在胸片报告生成中的准确性和可靠性


<details>
  <summary>Details</summary>
Motivation: 现有医学影像大模型存在幻觉现象和临床关键错误，限制其在真实医疗场景中的应用可靠性

Method: 创新性地将放射科医生的眼动追踪数据（Look）和病灶标注框（Mark）整合到LLM提示框架中，采用上下文学习而非传统微调

Result: CXR-LLaVA整体指标提升1.2%，LLaVA-Med提升9.2%；通用模型LLaVA-OV达到87.3%临床指标，专家评估显示每份报告减少0.43个临床关键错误

Conclusion: L&M策略为低资源医疗环境提供了可扩展的解决方案，通过上下文学习显著提升模型性能，推动AI辅助放射学的诊断流程优化

Abstract: Recent advancements in multimodal Large Language Models (LLMs) have
significantly enhanced the automation of medical image analysis, particularly
in generating radiology reports from chest X-rays (CXR). However, these models
still suffer from hallucinations and clinically significant errors, limiting
their reliability in real-world applications. In this study, we propose Look &
Mark (L&M), a novel grounding fixation strategy that integrates radiologist eye
fixations (Look) and bounding box annotations (Mark) into the LLM prompting
framework. Unlike conventional fine-tuning, L&M leverages in-context learning
to achieve substantial performance gains without retraining. When evaluated
across multiple domain-specific and general-purpose models, L&M demonstrates
significant gains, including a 1.2% improvement in overall metrics (A.AVG) for
CXR-LLaVA compared to baseline prompting and a remarkable 9.2% boost for
LLaVA-Med. General-purpose models also benefit from L&M combined with
in-context learning, with LLaVA-OV achieving an 87.3% clinical average
performance (C.AVG)-the highest among all models, even surpassing those
explicitly trained for CXR report generation. Expert evaluations further
confirm that L&M reduces clinically significant errors (by 0.43 average errors
per report), such as false predictions and omissions, enhancing both accuracy
and reliability. These findings highlight L&M's potential as a scalable and
efficient solution for AI-assisted radiology, paving the way for improved
diagnostic workflows in low-resource clinical settings.

</details>


### [132] [Fostering Video Reasoning via Next-Event Prediction](https://arxiv.org/abs/2505.22457)
*Haonan Wang,Hongfu Liu,Xiangyan Liu,Chao Du,Kenji Kawaguchi,Ye Wang,Tianyu Pang*

Main category: cs.CV

TL;DR: 论文提出通过自监督的下一事件预测任务(NEP)增强多模态大模型的时间推理能力，利用未来视频片段生成事件摘要，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有视频问答依赖人工标注或强模型支持，视频描述任务则混合时空信息，缺乏专门提升时间推理的自监督学习框架。

Method: 将视频分割为过去/未来帧，模型基于过去帧预测未来事件摘要；构建V1-33K数据集，探索视频指令微调策略。

Result: 通过FutureBench评估验证NEP范式可有效提升模型时间推理能力，实现可扩展的训练效果。

Conclusion: NEP为多模态大模型的时间推理提供了新型自监督训练范式，视频分割策略与评估基准支持该领域发展。

Abstract: Next-token prediction serves as the foundational learning task enabling
reasoning in LLMs. But what should the learning task be when aiming to equip
MLLMs with temporal reasoning capabilities over video inputs? Existing tasks
such as video question answering often rely on annotations from humans or much
stronger MLLMs, while video captioning tends to entangle temporal reasoning
with spatial information. To address this gap, we propose next-event prediction
(NEP), a learning task that harnesses future video segments as a rich,
self-supervised signal to foster temporal reasoning. We segment each video into
past and future frames: the MLLM takes the past frames as input and predicts a
summary of events derived from the future frames, thereby encouraging the model
to reason temporally in order to complete the task. To support this task, we
curate V1-33K, a dataset comprising 33,000 automatically extracted video
segments spanning diverse real-world scenarios. We further explore a range of
video instruction-tuning strategies to study their effects on temporal
reasoning. To evaluate progress, we introduce FutureBench to assess coherence
in predicting unseen future events. Experiments validate that NEP offers a
scalable and effective training paradigm for fostering temporal reasoning in
MLLMs.

</details>


### [133] [Thinking with Generated Images](https://arxiv.org/abs/2505.22525)
*Ethan Chern,Zhulin Hu,Steffi Chern,Siqi Kou,Jiadi Su,Yan Ma,Zhijie Deng,Pengfei Liu*

Main category: cs.CV

TL;DR: 提出'Thinking with Generated Images'新范式，通过生成中间视觉思维步骤增强多模态模型的视觉推理能力，使AI具备类人类的视觉想象与迭代优化能力。


<details>
  <summary>Details</summary>
Motivation: 突破现有大模型视觉推理的两大局限：1）仅能处理固定用户提供的图像 2）仅支持文本链式思考。旨在赋予模型主动构建/批判/优化视觉假设的认知能力。

Method: 双引擎机制：1）视觉子目标分解：将复杂任务拆解为可逐步生成的视觉组件 2）自我批判式生成：通过文本推理分析视觉假设缺陷并迭代优化。

Result: 在视觉生成基准测试中相对基线提升50%（38%→57%），尤其在处理多对象复杂场景中表现突出。支持蛋白质设计/建筑空间推演/犯罪现场重建等跨领域应用。

Conclusion: 该范式标志着多模态认知的范式转变，使AI具备'视觉想象力'，在创造性、分析性和策略性思维层面接近人类认知模式。开源工具包已发布供社区使用。

Abstract: We present Thinking with Generated Images, a novel paradigm that
fundamentally transforms how large multimodal models (LMMs) engage with visual
reasoning by enabling them to natively think across text and vision modalities
through spontaneous generation of intermediate visual thinking steps. Current
visual reasoning with LMMs is constrained to either processing fixed
user-provided images or reasoning solely through text-based chain-of-thought
(CoT). Thinking with Generated Images unlocks a new dimension of cognitive
capability where models can actively construct intermediate visual thoughts,
critique their own visual hypotheses, and refine them as integral components of
their reasoning process. We demonstrate the effectiveness of our approach
through two complementary mechanisms: (1) vision generation with intermediate
visual subgoals, where models decompose complex visual tasks into manageable
components that are generated and integrated progressively, and (2) vision
generation with self-critique, where models generate an initial visual
hypothesis, analyze its shortcomings through textual reasoning, and produce
refined outputs based on their own critiques. Our experiments on vision
generation benchmarks show substantial improvements over baseline approaches,
with our models achieving up to 50% (from 38% to 57%) relative improvement in
handling complex multi-object scenarios. From biochemists exploring novel
protein structures, and architects iterating on spatial designs, to forensic
analysts reconstructing crime scenes, and basketball players envisioning
strategic plays, our approach enables AI models to engage in the kind of visual
imagination and iterative refinement that characterizes human creative,
analytical, and strategic thinking. We release our open-source suite at
https://github.com/GAIR-NLP/thinking-with-generated-images.

</details>


### [134] [RICO: Improving Accuracy and Completeness in Image Recaptioning via Visual Reconstruction](https://arxiv.org/abs/2505.22613)
*Yuchi Wang,Yishuo Cai,Shuhuai Ren,Sihan Yang,Linli Yao,Yuanxin Liu,Yuanxing Zhang,Pengfei Wan,Xu Sun*

Main category: cs.CV

TL;DR: 提出RICO框架，通过视觉重建迭代优化图像描述，解决现有方法中因幻觉和细节缺失导致的不准确问题，并推出高效版本RICO-Flash。实验显示在多个基准上性能提升约10%。


<details>
  <summary>Details</summary>
Motivation: 现有基于多模态大语言模型(MLLM)的图像重描述方法存在描述不准确(幻觉)和不完整(细节缺失)的问题，需要新的解决方案提升描述质量。

Method: 1. 通过文本-图像模型重建描述生成参考图像
2. 利用MLLM对比原图与重建图的差异迭代优化描述
3. 引入RICO-Flash(基于DPO)降低迭代计算成本

Result: 在CapsBench和CompreCap基准上超越基线方法约10%，代码已开源。RICO-Flash在保持性能的同时显著降低计算开销。

Conclusion: 视觉重建与差异识别的迭代机制能有效生成更忠实、完整的图像描述，RICO-Flash方案兼顾性能与效率，具有实用价值。

Abstract: Image recaptioning is widely used to generate training datasets with enhanced
quality for various multimodal tasks. Existing recaptioning methods typically
rely on powerful multimodal large language models (MLLMs) to enhance textual
descriptions, but often suffer from inaccuracies due to hallucinations and
incompleteness caused by missing fine-grained details. To address these
limitations, we propose RICO, a novel framework that refines captions through
visual reconstruction. Specifically, we leverage a text-to-image model to
reconstruct a caption into a reference image, and prompt an MLLM to identify
discrepancies between the original and reconstructed images to refine the
caption. This process is performed iteratively, further progressively promoting
the generation of more faithful and comprehensive descriptions. To mitigate the
additional computational cost induced by the iterative process, we introduce
RICO-Flash, which learns to generate captions like RICO using DPO. Extensive
experiments demonstrate that our approach significantly improves caption
accuracy and completeness, outperforms most baselines by approximately 10% on
both CapsBench and CompreCap. Code released at
https://github.com/wangyuchi369/RICO.

</details>


### [135] [Sherlock: Self-Correcting Reasoning in Vision-Language Models](https://arxiv.org/abs/2505.22651)
*Yi Ding,Ruqi Zhang*

Main category: cs.CV

TL;DR: 提出Sherlock自纠正训练框架，仅需2万标注数据即可提升视觉语言模型推理能力，并在8个基准测试中达到64.1%准确率


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型存在推理错误敏感、依赖大量标注数据、跨领域泛化能力差三大核心瓶颈

Method: 轨迹级自纠正目标 + 视觉扰动偏好数据构建 + 动态β参数调节的偏好调优机制

Result: 在Llama3.2-Vision-11B基础上，自纠正后准确率达65.4%，超越同类模型且数据用量减少80%

Conclusion: 首次实现视觉语言模型的自监督持续改进，为减少人工标注依赖提供新范式

Abstract: Reasoning Vision-Language Models (VLMs) have shown promising performance on
complex multimodal tasks. However, they still face significant challenges: they
are highly sensitive to reasoning errors, require large volumes of annotated
data or accurate verifiers, and struggle to generalize beyond specific domains.
To address these limitations, we explore self-correction as a strategy to
enhance reasoning VLMs. We first conduct an in-depth analysis of reasoning
VLMs' self-correction abilities and identify key gaps. Based on our findings,
we introduce Sherlock, a self-correction and self-improvement training
framework. Sherlock introduces a trajectory-level self-correction objective, a
preference data construction method based on visual perturbation, and a dynamic
$\beta$ for preference tuning. Once the model acquires self-correction
capabilities using only 20k randomly sampled annotated data, it continues to
self-improve without external supervision. Built on the Llama3.2-Vision-11B
model, Sherlock achieves remarkable results across eight benchmarks, reaching
an average accuracy of 64.1 with direct generation and 65.4 after
self-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and
LlamaV-o1 (63.4) while using less than 20% of the annotated data.

</details>


### [136] [3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model](https://arxiv.org/abs/2505.22657)
*Wenbo Hu,Yining Hong,Yanjun Wang,Leison Gao,Zibu Wei,Xingcheng Yao,Nanyun Peng,Yonatan Bitton,Idan Szpektor,Kai-Wei Chang*

Main category: cs.CV

TL;DR: 论文提出3DLLM-Mem记忆管理框架和3DMem-Bench基准测试，解决大语言模型在3D环境中时空记忆建模的不足。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在动态多房间3D环境中规划和行动存在局限，主要归因于缺乏有效的3D时空记忆建模能力。

Method: 通过工作记忆令牌动态融合当前观察（查询）与过去交互记忆（特征库），实现任务相关信息的注意力选择与记忆高效管理。

Result: 在包含26,000+轨迹的3DMem-Bench基准测试中，3DLLM-Mem在最具挑战的野外任务上以16.5%成功率超越基线模型。

Conclusion: 该空间-时间记忆融合机制显著提升了智能体在复杂长程3D环境中的推理和行动能力。

Abstract: Humans excel at performing complex tasks by leveraging long-term memory
across temporal and spatial experiences. In contrast, current Large Language
Models (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D
environments. We posit that part of this limitation is due to the lack of
proper 3D spatial-temporal memory modeling in LLMs. To address this, we first
introduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000
trajectories and 2,892 embodied tasks, question-answering and captioning,
designed to evaluate an agent's ability to reason over long-term memory in 3D
environments. Second, we propose 3DLLM-Mem, a novel dynamic memory management
and fusion model for embodied spatial-temporal reasoning and actions in LLMs.
Our model uses working memory tokens, which represents current observations, as
queries to selectively attend to and fuse the most useful spatial and temporal
features from episodic memory, which stores past observations and interactions.
Our approach allows the agent to focus on task-relevant information while
maintaining memory efficiency in complex, long-horizon environments.
Experimental results demonstrate that 3DLLM-Mem achieves state-of-the-art
performance across various tasks, outperforming the strongest baselines by
16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied
tasks.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [137] [Visual Cues Support Robust Turn-taking Prediction in Noise](https://arxiv.org/abs/2505.22088)
*Sam O'Connor Russell,Naomi Harte*

Main category: cs.SD

TL;DR: 研究探讨噪声环境下预测性话轮转换模型(PTTM)的性能，发现多模态模型能有效利用视觉线索提升抗噪能力，但需依赖精准文本转录且无法完全泛化到新噪声类型。


<details>
  <summary>Details</summary>
Motivation: 评估PTTM在真实噪声环境中的表现，探索多模态方法（结合视觉特征）在噪声条件下的改进潜力。

Method: 通过对比清洁语音与不同信噪比噪声（如10dB音乐噪声）下的模型表现，测试音频单模态与多模态PTTM的保持/切换准确率。使用含噪声数据训练并分析ASR转录文本的适用性。

Result: 多模态PTTM在10dB音乐噪声下准确率达72%，显著优于纯音频模型（52%）。但改进效果无法完全迁移到未训练噪声类型，且训练依赖精准文本转录。

Conclusion: PTTM噪声敏感性突出，整合视觉线索的多模态方法可提升抗噪性，但需解决模型泛化能力和转录精度依赖问题。公开代码促进后续研究。

Abstract: Accurate predictive turn-taking models (PTTMs) are essential for naturalistic
human-robot interaction. However, little is known about their performance in
noise. This study therefore explores PTTM performance in types of noise likely
to be encountered once deployed. Our analyses reveal PTTMs are highly sensitive
to noise. Hold/shift accuracy drops from 84% in clean speech to just 52% in 10
dB music noise. Training with noisy data enables a multimodal PTTM, which
includes visual features to better exploit visual cues, with 72% accuracy in 10
dB music noise. The multimodal PTTM outperforms the audio-only PTTM across all
noise types and SNRs, highlighting its ability to exploit visual cues; however,
this does not always generalise to new types of noise. Analysis also reveals
that successful training relies on accurate transcription, limiting the use of
ASR-derived transcriptions to clean conditions. We make code publicly available
for future research.

</details>


### [138] [Advancing Hearing Assessment: An ASR-Based Frequency-Specific Speech Test for Diagnosing Presbycusis](https://arxiv.org/abs/2505.22231)
*Stefan Bleeck*

Main category: cs.SD

TL;DR: 开发基于ASR的频域特异性语音测试，通过音素混淆模式分析提升老年性耳聋诊断精度


<details>
  <summary>Details</summary>
Motivation: 传统听力测试难以评估超阈值听力损失对言语理解的影响，尤其在老年性耳聋的频域特异性感知缺陷方面存在局限

Method: 利用ASR模拟中度倾斜听力损失，通过受控声学退化处理语音信号，分析音素级别混淆模式

Result: 高频辅音出现特异性混淆(如齿龈/腭音→唇齿音替代)及音素删除现象，测试方案能有效区分模拟的正常听力与听力受损者

Conclusion: ASR方法为开发客观、细粒度的频域特异性听力评估工具开辟新方向，未来将进行人类验证并整合更先进AI模型

Abstract: Traditional audiometry often fails to fully characterize the functional
impact of hearing loss on speech understanding, particularly supra-threshold
deficits and frequency-specific perception challenges in conditions like
presbycusis. This paper presents the development and simulated evaluation of a
novel Automatic Speech Recognition (ASR)-based frequency-specific speech test
designed to provide granular diagnostic insights. Our approach leverages ASR to
simulate the perceptual effects of moderate sloping hearing loss by processing
speech stimuli under controlled acoustic degradation and subsequently analyzing
phoneme-level confusion patterns. Key findings indicate that simulated hearing
loss introduces specific phoneme confusions, predominantly affecting
high-frequency consonants (e.g., alveolar/palatal to labiodental substitutions)
and leading to significant phoneme deletions, consistent with the acoustic cues
degraded in presbycusis. A test battery curated from these ASR-derived
confusions demonstrated diagnostic value, effectively differentiating between
simulated normal-hearing and hearing-impaired listeners in a comprehensive
simulation. This ASR-driven methodology offers a promising avenue for
developing objective, granular, and frequency-specific hearing assessment tools
that complement traditional audiometry. Future work will focus on validating
these findings with human participants and exploring the integration of
advanced AI models for enhanced diagnostic precision.

</details>


### [139] [Effective Context in Neural Speech Models](https://arxiv.org/abs/2505.22487)
*Yen Meng,Sharon Goldwater,Hao Tang*

Main category: cs.SD

TL;DR: 研究提出两种测量语音模型有效上下文的方法，发现监督模型的有效上下文与任务复杂度相关，自监督模型有效上下文较短且类似电话模型，HuBERT可直接用于流式处理。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注扩大模型上下文长度，但缺乏对模型实际使用上下文范围（有效上下文）的量化评估，需要明确不同语音模型的实际上下文利用率。

Method: 开发两种有效上下文测量方法，应用于分析监督学习（语音任务）和自监督学习（HuBERT）的Transformer模型。

Result: 1. 监督模型：基频跟踪/音素分类/词语分类任务所需有效上下文依次递增
2. 自监督模型：有效上下文主要在早期层增长，整体长度接近监督电话模型
3. HuBERT无需结构调整即可实现流式推理

Conclusion: 语音模型实际使用的上下文较短，自监督模型（如HuBERT）的架构天然支持流式处理，为实时语音处理应用提供了直接解决方案。

Abstract: Modern neural speech models benefit from having longer context, and many
approaches have been proposed to increase the maximum context a model can use.
However, few have attempted to measure how much context these models actually
use, i.e., the effective context. Here, we propose two approaches to measuring
the effective context, and use them to analyze different speech Transformers.
For supervised models, we find that the effective context correlates well with
the nature of the task, with fundamental frequency tracking, phone
classification, and word classification requiring increasing amounts of
effective context. For self-supervised models, we find that effective context
increases mainly in the early layers, and remains relatively short -- similar
to the supervised phone model. Given that these models do not use a long
context during prediction, we show that HuBERT can be run in streaming mode
without modification to the architecture and without further fine-tuning.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [140] [Cascaded 3D Diffusion Models for Whole-body 3D 18-F FDG PET/CT synthesis from Demographics](https://arxiv.org/abs/2505.22489)
*Siyeop Yoon,Sifan Song,Pengfei Jin,Matthew Tivnan,Yujin Oh,Sekeun Kim,Dufan Wu,Xiang Li,Quanzheng Li*

Main category: eess.IV

TL;DR: 提出级联3D扩散模型框架，直接从人口统计学变量生成高保真PET/CT影像，解决肿瘤成像领域对数字孪生的需求。


<details>
  <summary>Details</summary>
Motivation: 传统确定性模型依赖预定义模板，无法满足个性化合成成像需求。本方法旨在通过生成模型创建具有解剖和代谢准确性的数字孪生。

Method: 两阶段生成：1）基于分数的扩散模型生成低分辨率全局结构 2）残差扩散模型进行超分辨率细化。在AutoPET数据集训练。

Result: 器官体积和SUV分布显示合成与真实数据高度一致（代谢偏差3-5%），尤其在人口亚组分析中保持稳定。

Conclusion: 级联扩散模型为传统体模提供可靠替代方案，支持可扩展的个性化合成成像，适用于临床研究和AI数据增强。

Abstract: We propose a cascaded 3D diffusion model framework to synthesize
high-fidelity 3D PET/CT volumes directly from demographic variables, addressing
the growing need for realistic digital twins in oncologic imaging, virtual
trials, and AI-driven data augmentation. Unlike deterministic phantoms, which
rely on predefined anatomical and metabolic templates, our method employs a
two-stage generative process. An initial score-based diffusion model
synthesizes low-resolution PET/CT volumes from demographic variables alone,
providing global anatomical structures and approximate metabolic activity. This
is followed by a super-resolution residual diffusion model that refines spatial
resolution. Our framework was trained on 18-F FDG PET/CT scans from the AutoPET
dataset and evaluated using organ-wise volume and standardized uptake value
(SUV) distributions, comparing synthetic and real data between demographic
subgroups. The organ-wise comparison demonstrated strong concordance between
synthetic and real images. In particular, most deviations in metabolic uptake
values remained within 3-5% of the ground truth in subgroup analysis. These
findings highlight the potential of cascaded 3D diffusion models to generate
anatomically and metabolically accurate PET/CT images, offering a robust
alternative to traditional phantoms and enabling scalable, population-informed
synthetic imaging for clinical and research applications.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [141] [UI-Evol: Automatic Knowledge Evolving for Computer Use Agents](https://arxiv.org/abs/2505.21964)
*Ziyun Zhang,Xinyi Liu,Xiaoyi Zhang,Jun Wang,Gang Chen,Yan Lu*

Main category: cs.HC

TL;DR: 针对计算机代理中知识执行效率低的问题，提出UI-Evol模块提升任务成功率并降低行为标准差


<details>
  <summary>Details</summary>
Motivation: 研究发现即使90%正确的知识也只能实现41%的任务成功率（知识执行差距），制约计算机代理的实际应用效果

Method: 提出两阶段解决方案：1）Retrace阶段提取真实交互动作序列；2）Critique阶段对比外部参考优化知识

Result: 在OSWorld基准测试中，UI-Evol使任务性能显著提升（成功率翻倍），行为标准差降低76%，可靠性大幅增强

Conclusion: UI-Evol不仅提升计算机代理的任务表现，还首次系统解决了行为稳定性问题，为实际应用奠定基础

Abstract: External knowledge has played a crucial role in the recent development of
computer use agents. We identify a critical knowledge-execution gap: retrieved
knowledge often fails to translate into effective real-world task execution.
Our analysis shows even 90\% correct knowledge yields only 41\% execution
success rate. To bridge this gap, we propose UI-Evol, a plug-and-play module
for autonomous GUI knowledge evolution. UI-Evol consists of two stages: a
Retrace Stage that extracts faithful objective action sequences from actual
agent-environment interactions, and a Critique Stage that refines existing
knowledge by comparing these sequences against external references. We conduct
comprehensive experiments on the OSWorld benchmark with the state-of-the-art
Agent S2. Our results demonstrate that UI-Evol not only significantly boosts
task performance but also addresses a previously overlooked issue of high
behavioral standard deviation in computer use agents, leading to superior
performance on computer use tasks and substantially improved agent reliability.

</details>


### [142] [MapStory: LLM-Powered Text-Driven Map Animation Prototyping with Human-in-the-Loop Editing](https://arxiv.org/abs/2505.21966)
*Aditya Gunturu,Ben Pearman,Keiichi Ihara,Morteza Faraji,Bryan Wang,Rubaiat Habib Kazi,Ryo Suzuki*

Main category: cs.HC

TL;DR: MapStory是基于大语言模型的动画创作工具，通过自然语言脚本自动生成可编辑的地图动画，支持参数微调和地理信息查询。


<details>
  <summary>Details</summary>
Motivation: 解决传统地图动画制作流程复杂、门槛高的问题，通过自动化分解脚本和地理信息检索降低创作难度。

Method: 1. 代理架构自动分解脚本为动画元素
2. 研究者组件结合LLM与网络搜索获取地理数据
3. 基于专业动画师访谈（N=5）及200个案例分析的交互设计
4. 时间线编辑器实现参数微调

Result: 专家评估（N=5）和可用性测试（N=12）显示：
- 创作效率提升
- 支持快速迭代
- 激发创意探索
- 显著降低创作门槛

Conclusion: MapStory开创了自然语言驱动的地图动画创作范式，通过LLM与交互设计的结合，为地理叙事提供高效易用的工具支持。

Abstract: We introduce MapStory, an LLM-powered animation authoring tool that generates
editable map animation sequences directly from natural language text. Given a
user-written script, MapStory leverages an agentic architecture to
automatically produce a scene breakdown, which decomposes the script into key
animation building blocks such as camera movements, visual highlights, and
animated elements. Our system includes a researcher component that accurately
queries geospatial information by leveraging an LLM with web search, enabling
the automatic extraction of relevant regions, paths, and coordinates while
allowing users to edit and query for changes or additional information to
refine the results. Additionally, users can fine-tune parameters of these
blocks through an interactive timeline editor. We detail the system's design
and architecture, informed by formative interviews with professional animators
and an analysis of 200 existing map animation videos. Our evaluation, which
includes expert interviews (N=5) and a usability study (N=12), demonstrates
that MapStory enables users to create map animations with ease, facilitates
faster iteration, encourages creative exploration, and lowers barriers to
creating map-centric stories.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [143] [Capability-Based Scaling Laws for LLM Red-Teaming](https://arxiv.org/abs/2505.20162)
*Alexander Panfilov,Paul Kassianik,Maksym Andriushchenko,Jonas Geiping*

Main category: cs.AI

TL;DR: 大语言模型能力增强后，当攻击者能力弱于目标模型时传统红队测试方法可能失效。研究通过500多个攻击对实验发现攻击成功率与能力差距相关，并提出了预测攻击成功率的越狱扩展定律。


<details>
  <summary>Details</summary>
Motivation: 研究当红队测试演变成弱对强问题时（目标模型能力超越攻击者），需要重新评估传统基于提示工程的漏洞识别方法有效性。

Method: 使用基于LLM的越狱攻击方法，评估跨不同模型家族、参数规模和能力水平的500+攻击者-目标模型对。

Result: 发现三个核心趋势：1）高能力模型攻击性更强；2）当目标能力超越攻击者时成功率锐减；3）攻击成功率与MMLU-Pro社会科学测试表现强相关。由此推导出基于能力差距的越狱攻击成功率预测公式。

Conclusion: 模型提供商需精确测量并控制模型的劝说与操纵能力，未来开源模型的增强可能加剧现有系统的安全风险。

Abstract: As large language models grow in capability and agency, identifying
vulnerabilities through red-teaming becomes vital for safe deployment. However,
traditional prompt-engineering approaches may prove ineffective once
red-teaming turns into a weak-to-strong problem, where target models surpass
red-teamers in capabilities. To study this shift, we frame red-teaming through
the lens of the capability gap between attacker and target. We evaluate more
than 500 attacker-target pairs using LLM-based jailbreak attacks that mimic
human red-teamers across diverse families, sizes, and capability levels. Three
strong trends emerge: (i) more capable models are better attackers, (ii) attack
success drops sharply once the target's capability exceeds the attacker's, and
(iii) attack success rates correlate with high performance on social science
splits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking
scaling law that predicts attack success for a fixed target based on
attacker-target capability gap. These findings suggest that fixed-capability
attackers (e.g., humans) may become ineffective against future models,
increasingly capable open-source models amplify risks for existing systems, and
model providers must accurately measure and control models' persuasive and
manipulative abilities to limit their effectiveness as attackers.

</details>


### [144] [R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised and Reinforcement Learning](https://arxiv.org/abs/2505.21668)
*Yongchao Chen,Yueying Liu,Junwei Zhou,Yilun Hao,Jingquan Wang,Yang Zhang,Chuchu Fan*

Main category: cs.AI

TL;DR: R1-Code-Interpreter通过监督微调与强化学习的组合训练，使文本大模型具备自主生成代码查询能力，在37个测试任务上准确率提升20.1%，接近GPT-4o代码解释器水平。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在精确计算/算法推理任务中过度依赖文本推理的缺陷，探索如何有效对齐预训练模型以自主切换文本与代码推理模式。

Method: 构建144个多领域推理任务数据集，采用多阶段训练策略：监督微调(SFT)建立代码生成基础，强化学习(GRPO/PPO)优化代码生成质量，设计代码掩码机制提升稳定性。

Result: R1-CI-14B模型测试准确率提升至64.1%，超越GPT-4o纯文本模式(58.6%)，代码生成展现出自我验证的涌现能力。

Conclusion: 通过系统化的代码解释器训练框架，验证了LLMs在复杂推理任务中代码生成的有效性，为开源社区提供了可复现的训练范式和模型。

Abstract: Despite advances in reasoning and planning of R1-like models, Large Language
Models (LLMs) still struggle with tasks requiring precise computation, symbolic
manipulation, optimization, and algorithmic reasoning, in which textual
reasoning lacks the rigor of code execution. A key challenge is enabling LLMs
to decide when to use textual reasoning versus code generation. While OpenAI
trains models to invoke a Code Interpreter as needed, public research lacks
guidance on aligning pre-trained LLMs to effectively leverage code and
generalize across diverse tasks. We present R1-Code-Interpreter, an extension
of a text-only LLM trained via multi-turn supervised fine-tuning (SFT) and
reinforcement learning (RL) to autonomously generate multiple code queries
during step-by-step reasoning. We curate 144 reasoning and planning tasks (107
for training, 37 for testing), each with over 200 diverse questions. We
fine-tune Qwen-2.5 models (3B/7B/14B) using various SFT and RL strategies,
investigating different answer formats, reasoning vs. non-reasoning models,
cold vs. warm starts, GRPO vs. PPO, and masked vs. unmasked code outputs.
Unlike prior RL work on narrow domains, we find that Code Interpreter training
is significantly harder due to high task diversity and expensive code
execution, highlighting the critical role of the SFT stage. Our final model,
R1-CI-14B, improves average accuracy on the 37 test tasks from 44.0\% to
64.1\%, outperforming GPT-4o (text-only: 58.6\%) and approaching GPT-4o with
Code Interpreter (70.9\%), with the emergent self-checking behavior via code
generation. Datasets, Codes, and Models are available at
https://github.com/yongchao98/R1-Code-Interpreter and
https://huggingface.co/yongchao98.

</details>


### [145] [Towards Safety Reasoning in LLMs: AI-agentic Deliberation for Policy-embedded CoT Data Creation](https://arxiv.org/abs/2505.21784)
*Tharindu Kumarage,Ninareh Mehrabi,Anil Ramakrishna,Xinyan Zhao,Richard Zemel,Kai-Wei Chang,Aram Galstyan,Rahul Gupta,Charith Peris*

Main category: cs.AI

TL;DR: 提出AIDSAFE方法，通过多智能体迭代审议生成高质量安全策略推理链（CoT）数据，提升LLMs安全推理能力并减少过度拒绝问题。


<details>
  <summary>Details</summary>
Motivation: 现有安全推理范式需高质量策略嵌入的CoT数据集，但传统方法存在资源消耗大、易产生幻觉或策略冲突等问题。

Method: 采用多智能体迭代审议生成CoT，通过数据精炼阶段消除低质量内容；补充使用信念增强技术生成偏好对齐数据。

Result: AIDSAFE生成的CoTs显著提升策略遵循性和推理质量，微调后的开源LLMs在安全泛化与抗越狱能力上优于基线模型。

Conclusion: AIDSAFE为安全训练提供高效数据生成方案，通过SFT/DPO微调有效平衡LLMs安全性与实用性。

Abstract: Safety reasoning is a recent paradigm where LLMs reason over safety policies
before generating responses, thereby mitigating limitations in existing safety
measures such as over-refusal and jailbreak vulnerabilities. However,
implementing this paradigm is challenging due to the resource-intensive process
of creating high-quality policy-embedded chain-of-thought (CoT) datasets while
ensuring reasoning remains accurate and free from hallucinations or policy
conflicts. To tackle this, we propose AIDSAFE: Agentic Iterative Deliberation
for Safety Reasoning, a novel data generation recipe that leverages multi-agent
deliberation to iteratively expand reasoning on safety policies. A data refiner
stage in AIDSAFE ensures high-quality outputs by eliminating repetitive,
redundant, and deceptive thoughts. AIDSAFE-generated CoTs provide a strong
foundation for supervised fine-tuning (SFT)-based safety training.
Additionally, to address the need of preference data in alignment stages, such
as DPO training, we introduce a supplemental recipe that uses belief
augmentation to create distinct selected and rejected CoT samples. Our
evaluations demonstrate that AIDSAFE-generated CoTs achieve superior policy
adherence and reasoning quality. Consequently, we show that fine-tuning
open-source LLMs on these CoTs can significantly improve safety generalization
and jailbreak robustness while maintaining acceptable utility and over-refusal
accuracy. AIDSAFE-generated CoT datasets can be found here:
https://huggingface.co/datasets/AmazonScience/AIDSAFE

</details>


### [146] [Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive Survey and Taxonomy](https://arxiv.org/abs/2505.21907)
*Saleh Afzoon,Zahra Jahanandish,Phuong Thao Huynh,Amin Beheshti,Usman Naseem*

Main category: cs.AI

TL;DR: 针对AI副驾系统，提出基于偏好优化的个性化设计框架，涵盖交互前中后三阶段的策略整合


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统个性化技术难以适配实时交互的AI副驾系统，需建立统一设计框架填补研究空白

Method: 构建三阶段分类体系：交互前信号获取、交互中意图建模、交互后反馈整合，分析传统方法与LLM新技术的适配性

Result: 形成融合AI个性化、人机协作和LLM调优的系统设计方法论，明确各阶段可用偏好资源及适用技术路径

Conclusion: 该分类体系为开发偏好感知的智能副驾奠定理论基础，推动实时交互系统个性化研究从碎片化走向系统化

Abstract: AI copilots, context-aware, AI-powered systems designed to assist users in
tasks such as software development and content creation, are becoming integral
to modern workflows. As these systems grow in capability and adoption,
personalization has emerged as a cornerstone for ensuring usability, trust, and
productivity. Central to this personalization is preference optimization: the
ability of AI copilots to detect, interpret, and align with individual user
preferences. While personalization techniques are well-established in domains
like recommender systems and dialogue agents, their adaptation to interactive,
real-time systems like AI copilots remains fragmented and underexplored. This
survey addresses this gap by synthesizing research on how user preferences are
captured, modeled, and refined within the design of AI copilots. We introduce a
unified definition of AI copilots and propose a phase-based taxonomy of
preference optimization strategies, structured around pre-interaction,
mid-interaction, and post-interaction stages. We analyze techniques for
acquiring preference signals, modeling user intent, and integrating feedback
loops, highlighting both established approaches and recent innovations. By
bridging insights from AI personalization, human-AI collaboration, and large
language model adaptation, this survey provides a structured foundation for
designing adaptive, preference-aware AI copilots. It offers a holistic view of
the available preference resources, how they can be leveraged, and which
technical approaches are most suited to each stage of system design.

</details>


### [147] [Rethinking the Unsolvable: When In-Context Search Meets Test-Time Scaling](https://arxiv.org/abs/2505.22290)
*Fanzeng Xia,Yidong Luo,Tinko Sebastian Bartels,Yaqi Xu,Tongxin Li*

Main category: cs.AI

TL;DR: 通过上下文搜索提示与模型内部扩展结合，LLM在复杂推理任务中可实现30倍成功率提升，显著扩展可解决问题的复杂性边界


<details>
  <summary>Details</summary>
Motivation: 现有研究低估了LLM在复杂推理任务中的潜力，主要依赖简单上下文学习进行评估，忽视高级提示技术的作用

Method: 结合上下文搜索提示（in-context search）与测试时内部扩展（internal scaling）技术

Result: 1) 在NP-hard任务和真实规划基准中成功率提升达30倍 2) 理论证明可解决问题的复杂性类别显著扩展

Conclusion: 当前评估范式系统性低估LLM潜力，需建立更全面的评测框架重新定义LLM的推理能力边界

Abstract: Recent research has highlighted that Large Language Models (LLMs), even when
trained to generate extended long reasoning steps, still face significant
challenges on hard reasoning problems. However, much of the existing literature
relies on direct prompting with simple in-context learning examples for
evaluation, which largely overlooks advanced techniques to elicit LLMs'
deliberate reasoning before drawing conclusions that LLMs hit a performance
ceiling. In this paper, we systematically explore the combined potential of
in-context search and test-time scaling on super hard reasoning tasks. We find
that by employing advanced in-context search prompting to LLMs augmented with
internal scaling, one can achieve transformative performance breakthroughs on
tasks previously deemed "unsolvable" (e.g., reported success rates below 5%).
We provide both empirical results and theoretical analysis of how this
combination can unleash LLM reasoning capabilities: i) Empirically, on
controlled NP-hard tasks and complex real-world planning benchmarks, our
approach achieves up to a 30x improvement in success rates compared to
previously reported results without any external mechanisms; ii) Theoretically,
we show that in-context search prompting, when combined with internal scaling,
significantly extends the complexity class of solvable reasoning problems.
These findings challenge prevailing assumptions about the limitations of LLMs
on complex tasks, indicating that current evaluation paradigms systematically
underestimate their true potential. Our work calls for a critical reassessment
of how LLM reasoning is benchmarked and a more robust evaluation strategy that
fully captures the true capabilities of contemporary LLMs, which can lead to a
better understanding of their operational reasoning boundaries in real-world
deployments.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [148] [Incorporating LLMs for Large-Scale Urban Complex Mobility Simulation](https://arxiv.org/abs/2505.21880)
*Yu-Lun Song,Chung-En Tsern,Che-Cheng Wu,Yu-Ming Chang,Syuan-Bo Huang,Wei-Chu Chen,Michael Chia-Liang Lin,Yu-Ta Lin*

Main category: cs.MA

TL;DR: 提出结合大语言模型与基于代理建模的创新城市交通模拟方法，增强代理多样性与真实性，应用于台北市实证分析，助力城市规划决策。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的ABM方法在代理行为多样性和真实性存在局限，需通过LLM的自然语言处理能力生成更贴近现实的模拟参数与行为逻辑。

Method: 整合LLM生成合成人口画像，动态分配日常/临时活动地点，模拟个性化出行路线，并基于台北市真实地理数据进行验证。

Result: 成功构建个体行为与宏观流动模式的双尺度模拟系统，输出路线热力图及交通方式指标，为公交优化提供量化依据。

Conclusion: LLM-ABM融合框架为城市规划提供新范式，后续需建立验证体系确保模拟结果可靠性，推动决策支持系统智能化。

Abstract: This study presents an innovative approach to urban mobility simulation by
integrating a Large Language Model (LLM) with Agent-Based Modeling (ABM).
Unlike traditional rule-based ABM, the proposed framework leverages LLM to
enhance agent diversity and realism by generating synthetic population
profiles, allocating routine and occasional locations, and simulating
personalized routes. Using real-world data, the simulation models individual
behaviors and large-scale mobility patterns in Taipei City. Key insights, such
as route heat maps and mode-specific indicators, provide urban planners with
actionable information for policy-making. Future work focuses on establishing
robust validation frameworks to ensure accuracy and reliability in urban
planning applications.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [149] [From prosthetic memory to prosthetic denial: Auditing whether large language models are prone to mass atrocity denialism](https://arxiv.org/abs/2505.21753)
*Roberto Ulloa,Eve M. Zucker,Daniel Bultmann,David J. Simon,Mykola Makhortykh*

Main category: cs.CY

TL;DR: LLMs可能通过替代记忆传播历史叙事，但也存在强化历史否认主义的风险，尤其在训练数据不足的案例中


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在历史记忆传播中的双刃剑效应，揭示AI系统可能引发的记忆替代或历史否认现象

Method: 比较审计法：测试Claude/GPT/Llama/Mixtral/Gemini五个模型在四个种族灭绝案例（英语+当地语言）中的响应表现

Result: LLMs对犹太大屠杀等广泛记录事件响应准确，但对柬埔寨大屠杀等案例易受否认主义框架影响，存在显著输出不一致

Conclusion: LLMs扩展了替代记忆概念，但未经审查的使用可能助长历史修正主义，威胁数字记忆保存的伦理价值

Abstract: The proliferation of large language models (LLMs) can influence how
historical narratives are disseminated and perceived. This study explores the
implications of LLMs' responses on the representation of mass atrocity memory,
examining whether generative AI systems contribute to prosthetic memory, i.e.,
mediated experiences of historical events, or to what we term "prosthetic
denial," the AI-mediated erasure or distortion of atrocity memories. We argue
that LLMs function as interfaces that can elicit prosthetic memories and,
therefore, act as experiential sites for memory transmission, but also
introduce risks of denialism, particularly when their outputs align with
contested or revisionist narratives. To empirically assess these risks, we
conducted a comparative audit of five LLMs (Claude, GPT, Llama, Mixtral, and
Gemini) across four historical case studies: the Holodomor, the Holocaust, the
Cambodian Genocide, and the genocide against the Tutsis in Rwanda. Each model
was prompted with questions addressing common denialist claims in English and
an alternative language relevant to each case (Ukrainian, German, Khmer, and
French). Our findings reveal that while LLMs generally produce accurate
responses for widely documented events like the Holocaust, significant
inconsistencies and susceptibility to denialist framings are observed for more
underrepresented cases like the Cambodian Genocide. The disparities highlight
the influence of training data availability and the probabilistic nature of LLM
responses on memory integrity. We conclude that while LLMs extend the concept
of prosthetic memory, their unmoderated use risks reinforcing historical
denialism, raising ethical concerns for (digital) memory preservation, and
potentially challenging the advantageous role of technology associated with the
original values of prosthetic memory.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [150] [Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models](https://arxiv.org/abs/2505.22271)
*Yongcan Yu,Yanbo Wang,Ran He,Jian Liang*

Main category: cs.CR

TL;DR: 提出名为TIM的通用防御框架，通过自进化机制动态防御多种越狱攻击，包含检测模块与安全微调模块的解耦设计。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法（如文本对抗重写防御）仅针对特定类型越狱攻击，无法应对多样化对抗策略（如图像攻击），亟需通用解决方案。

Method: 1. 训练核心标识符（gist token）实时检测越狱指令；2. 检测到攻击时自动触发安全微调（使用拒绝回答配对数据）；3. 解耦检测模块与微调模块以避免性能下降。

Result: 在单模态和多模态大模型上的实验验证了TIM的有效性，可自适应抵御文本/图像等多样化越狱攻击。

Conclusion: TIM首次实现自我演进的通用防御框架，通过动态更新防御策略和解耦架构设计，显著提升大模型对抗复杂对抗攻击的鲁棒性。

Abstract: While (multimodal) large language models (LLMs) have attracted widespread
attention due to their exceptional capabilities, they remain vulnerable to
jailbreak attacks. Various defense methods are proposed to defend against
jailbreak attacks, however, they are often tailored to specific types of
jailbreak attacks, limiting their effectiveness against diverse adversarial
strategies. For instance, rephrasing-based defenses are effective against text
adversarial jailbreaks but fail to counteract image-based attacks. To overcome
these limitations, we propose a universal defense framework, termed Test-time
IMmunization (TIM), which can adaptively defend against various jailbreak
attacks in a self-evolving way. Specifically, TIM initially trains a gist token
for efficient detection, which it subsequently applies to detect jailbreak
activities during inference. When jailbreak attempts are identified, TIM
implements safety fine-tuning using the detected jailbreak instructions paired
with refusal answers. Furthermore, to mitigate potential performance
degradation in the detector caused by parameter updates during safety
fine-tuning, we decouple the fine-tuning process from the detection module.
Extensive experiments on both LLMs and multimodal LLMs demonstrate the efficacy
of TIM.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [151] [Scientific Paper Retrieval with LLM-Guided Semantic-Based Ranking](https://arxiv.org/abs/2505.21815)
*Yunyi Zhang,Ruozhen Yang,Siqi Jiao,SeongKu Kang,Jiawei Han*

Main category: cs.IR

TL;DR: 提出SemRank框架，通过LLM引导的查询理解与多粒度科学概念索引相结合，显著提升论文检索准确率与效率


<details>
  <summary>Details</summary>
Motivation: 现有密集检索方法难以捕捉细粒度科学概念，而单纯依赖LLM的查询理解存在缺乏领域知识基础、生成内容不可靠的问题

Method: 构建包含研究主题和关键短语的多粒度概念索引，在查询时通过LLM识别语料库核心概念实现精准语义匹配

Result: 实验证明SemRank有效提升多种基础检索器性能，超越现有LLM基线方法并保持高效运行

Conclusion: SemRank通过结合领域知识引导与语义索引技术，在科学文献检索任务中实现了准确性突破与效率的平衡

Abstract: Scientific paper retrieval is essential for supporting literature discovery
and research. While dense retrieval methods demonstrate effectiveness in
general-purpose tasks, they often fail to capture fine-grained scientific
concepts that are essential for accurate understanding of scientific queries.
Recent studies also use large language models (LLMs) for query understanding;
however, these methods often lack grounding in corpus-specific knowledge and
may generate unreliable or unfaithful content. To overcome these limitations,
we propose SemRank, an effective and efficient paper retrieval framework that
combines LLM-guided query understanding with a concept-based semantic index.
Each paper is indexed using multi-granular scientific concepts, including
general research topics and detailed key phrases. At query time, an LLM
identifies core concepts derived from the corpus to explicitly capture the
query's information need. These identified concepts enable precise semantic
matching, significantly enhancing retrieval accuracy. Experiments show that
SemRank consistently improves the performance of various base retrievers,
surpasses strong existing LLM-based baselines, and remains highly efficient.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [152] [Large-Area Fabrication-aware Computational Diffractive Optics](https://arxiv.org/abs/2505.22313)
*Kaixuan Wei,Hector A. Jimenez-Romero,Hadi Amata,Jipeng Sun,Qiang Fu,Felix Heide,Wolfgang Heidrich*

Main category: physics.optics

TL;DR: 提出可微分光学制造感知设计框架，通过神经光刻模型和并行计算实现高精度可扩展衍射光学器件设计与制造验证


<details>
  <summary>Details</summary>
Motivation: 现有可微分光学系统存在仿真与制造质量差距，限制其从实验室原型转向实际应用

Method: 结合灰度光刻与纳米压印的制造流程，开发超分辨神经光刻模型预测3D结构，构建基于多GPU并行的FFT计算框架

Result: 实现32.16mm×21.44mm大规模衍射光学设计验证，单DOE成像系统通过维纳滤波获得高质量图像，仿真与原型误差显著降低

Conclusion: 该研究突破了衍射光学实际应用的制造限制，为可微分光学设计提供了工业级解决方案

Abstract: Differentiable optics, as an emerging paradigm that jointly optimizes optics
and (optional) image processing algorithms, has made innovative optical designs
possible across a broad range of applications. Many of these systems utilize
diffractive optical components (DOEs) for holography, PSF engineering, or
wavefront shaping. Existing approaches have, however, mostly remained limited
to laboratory prototypes, owing to a large quality gap between simulation and
manufactured devices. We aim at lifting the fundamental technical barriers to
the practical use of learned diffractive optical systems. To this end, we
propose a fabrication-aware design pipeline for diffractive optics fabricated
by direct-write grayscale lithography followed by nano-imprinting replication,
which is directly suited for inexpensive mass production of large area designs.
We propose a super-resolved neural lithography model that can accurately
predict the 3D geometry generated by the fabrication process. This model can be
seamlessly integrated into existing differentiable optics frameworks, enabling
fabrication-aware, end-to-end optimization of computational optical systems. To
tackle the computational challenges, we also devise tensor-parallel compute
framework centered on distributing large-scale FFT computation across many
GPUs. As such, we demonstrate large scale diffractive optics designs up to
32.16 mm $\times$ 21.44 mm, simulated on grids of up to 128,640 by 85,760
feature points. We find adequate agreement between simulation and fabricated
prototypes for applications such as holography and PSF engineering. We also
achieve high image quality from an imaging system comprised only of a single
DOE, with images processed only by a Wiener filter utilizing the simulation
PSF. We believe our findings lift the fabrication limitations for real-world
applications of diffractive optics and differentiable optical design.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [153] [VietASR: Achieving Industry-level Vietnamese ASR with 50-hour labeled data and Large-Scale Speech Pretraining](https://arxiv.org/abs/2505.21527)
*Jianheng Zhuo,Yifan Yang,Yiwen Shao,Yong Xu,Dong Yu,Kai Yu,Xie Chen*

Main category: eess.AS

TL;DR: VietASR提出了一种基于大规模未标注数据和少量标注数据的低成本自监督学习方案，显著提升了越南语等低资源语言的语音识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有ASR系统对标注数据依赖性强，在低资源语言场景存在训练成本高、延迟大、可访问性差的问题。

Method: 采用多轮次ASR偏向的自监督学习框架，结合7万小时未标注数据预训练和50小时标注数据微调。

Result: 轻量级模型在真实场景中超越Whisper Large-v3和商业系统，验证了方案的优越性。

Conclusion: 该方案为低资源ASR提供了实用解决方案，开源计划将促进相关领域研究。

Abstract: Automatic speech recognition (ASR) has made remarkable progress but heavily
relies on large-scale labeled data, which is scarce for low-resource languages
like Vietnamese. While existing systems such as Whisper, USM, and MMS achieve
promising performance, their efficacy remains inadequate in terms of training
costs, latency, and accessibility. To address these issues, we propose VietASR,
a novel ASR training pipeline that leverages vast amounts of unlabeled data and
a small set of labeled data. Through multi-iteration ASR-biased self-supervised
learning on a large-scale unlabeled dataset, VietASR offers a cost-effective
and practical solution for enhancing ASR performance. Experiments demonstrate
that pre-training on 70,000-hour unlabeled data and fine-tuning on merely
50-hour labeled data yield a lightweight but powerful ASR model. It outperforms
Whisper Large-v3 and commercial ASR systems on real-world data. Our code and
models will be open-sourced to facilitate research in low-resource ASR.

</details>


### [154] [Evaluation of LLMs in Speech is Often Flawed: Test Set Contamination in Large Language Models for Speech Recognition](https://arxiv.org/abs/2505.22251)
*Yuan Tseng,Titouan Parcollet,Rogier van Dalen,Shucong Zhang,Sourav Bhattacharya*

Main category: eess.AS

TL;DR: 数据污染显著影响LLM在语音任务中的评估可靠性，需使用未泄露数据进行测试


<details>
  <summary>Details</summary>
Motivation: 发现LibriSpeech和Common Voice评估集大量内容已存在于LLM预训练数据中，质疑基于这些数据集的结论可靠性

Method: 通过对比有/无数据污染的LLM，分析模型生成测试句子的倾向性及语音识别器的表现差异

Result: 受污染的LLM更易生成训练见过的句子，语音识别错误率差异小但置信度显著提高，微量污染即可导致输出偏差

Conclusion: LLM评估需严格使用未泄露数据，现有基于污染数据的语音系统结论可靠性存疑

Abstract: Recent work suggests that large language models (LLMs) can improve
performance of speech tasks compared to existing systems. To support their
claims, results on LibriSpeech and Common Voice are often quoted. However, this
work finds that a substantial amount of the LibriSpeech and Common Voice
evaluation sets appear in public LLM pretraining corpora. This calls into
question the reliability of findings drawn from these two datasets. To measure
the impact of contamination, LLMs trained with or without contamination are
compared, showing that a contaminated LLM is more likely to generate test
sentences it has seen during training. Speech recognisers using contaminated
LLMs shows only subtle differences in error rates, but assigns significantly
higher probabilities to transcriptions seen during training. Results show that
LLM outputs can be biased by tiny amounts of data contamination, highlighting
the importance of evaluating LLM-based speech systems with held-out data.

</details>
