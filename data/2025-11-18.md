<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 77]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [math.HO](#math.HO) [Total: 1]
- [cs.CV](#cs.CV) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [TimeStampEval: A Simple LLM Eval and a Little Fuzzy Matching Trick to Improve Search Accuracy](https://arxiv.org/abs/2511.11594)
*James McCammon*

Main category: cs.CL

TL;DR: 提出TimeStampEval基准和两阶段检索方法，显著提升非逐字引用场景下的时间戳定位精度并降低推理成本


<details>
  <summary>Details</summary>
Motivation: 解决传统模糊匹配在语音转录文本与官方记录存在语法差异时的失效问题，满足自动播客节目精准定位国会记录片段的需求

Method: 1. 构建包含2800句的测试基准 2. 采用RapidFuzz预筛选+LLM验证的两阶段方法 3. 在10种不同长度/领域转录本进行扩展验证

Result: 1. 提示工程设计提升准确率3-20% 2. 推理预算600-850token使准确率从37%提升至77%+ 3. 混合方法提升模糊匹配准确率50%，成本降低96%

Conclusion: 该方法在跨文档语义对齐任务中展现出强鲁棒性，在万级token转录本中保持95-100%的无效目标拒绝准确率

Abstract: Traditional fuzzy matching often fails when searching for quotes that are semantically identical but syntactically different across documents-a common issue when aligning official written records with speech-to-text transcripts. We introduce TimeStampEval, a benchmark for retrieving precise millisecond timestamps from long transcripts given non-verbatim quotes. Our simple two-stage method dramatically improves retrieval accuracy while cutting inference costs by over 90%. The motivating use case is an automated long-form podcast that assembles Congressional Record clips into AI-hosted narration. The technical challenge: given a sentence-timestamped transcript and a target quote that may differ due to transcription or editorial drift, return exact start and end boundaries. Standard algorithms handle verbatim text but break under fuzzier variants. Evaluating six modern LLMs on a 2,800-sentence (120k-token) transcript revealed four key findings. (1) Prompt design matters more than model choice: placing the query before the transcript and using compact formatting improved accuracy by 3-20 points while reducing token count by 30-40%. (2) Off-by-one errors form a distinct category, showing models understand the task but misplace boundaries. (3) A modest reasoning budget (600-850 tokens) raises accuracy from 37% to 77% for weak setups and to above 90% for strong ones. (4) Our "Assisted Fuzzy" approach-RapidFuzz pre-filtering followed by LLM verification on short snippets-improves fuzzy match accuracy by up to 50 points while halving latency and reducing cost per correct result by up to 96%. Extended tests on ten transcripts (50k-900k tokens, 1989-2025) confirm robustness to transcript length, vocabulary drift, and domain change, maintaining 95-100% rejection accuracy for absent targets.

</details>


### [2] [MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling](https://arxiv.org/abs/2511.11793)
*MiroMind Team,Song Bai,Lidong Bing,Carson Chen,Guanzheng Chen,Yuntao Chen,Zhe Chen,Ziyi Chen,Jifeng Dai,Xuan Dong,Yue Deng,Yunjie Fu,Junqi Ge,Chenxia Han,Tammy Huang,Zhenhang Huang,Jerry Jiao,Shilei Jiang,Tianyu Jiao,Xiaoqi Jian,Lei Lei,Ruilin Li,Ryan Luo,Tiantong Li,Xiang Lin,Ziyuan Liu,Zhiqi Li,Jie Ni,Qiang Ren,Pax Sun,Shiqian Su,Chenxin Tao,Bin Wang,Hellen Wang,Haonan Wang,James Wang,Jin Wang,Jojo Wang,Letian Wang,Shizun Wang,Weizhi Wang,Zixuan Wang,Jinfan Xu,Sen Xing,Chenyu Yang,Hai Ye,Jiaheng Yu,Yue Yu,Muyan Zhong,Tianchen Zhao,Xizhou Zhu,Yanpeng Zhou,Yifan Zhang,Zhi Zhu*

Main category: cs.CL

TL;DR: MiroThinker v1.0通过交互扩展（interaction scaling）实现研究智能体性能提升，支持单任务600次工具调用，在多个基准测试中接近商业模型水平。


<details>
  <summary>Details</summary>
Motivation: 探索模型层面的交互扩展维度，利用环境反馈和外部信息获取来修正错误，突破传统仅依赖模型规模/上下文长度的局限。

Method: 通过强化学习训练模型实现高效交互扩展（256K上下文窗口支持600次工具调用），建立多轮推理能力。

Result: 72B模型在GAIA/HLE/BrowseComp系列基准取得81.9%/37.7%/47.1%/55.6%准确率，超越开源基线接近GPT-5-high。

Conclusion: 交互扩展深度与模型规模/上下文窗口具有相似的缩放规律，构成构建下一代研究智能体的第三维度。

Abstract: We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.

</details>


### [3] [On the Notion that Language Models Reason](https://arxiv.org/abs/2511.11810)
*Bertram Højer*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Language models (LMs) are said to be exhibiting reasoning, but what does this entail? We assess definitions of reasoning and how key papers in the field of natural language processing (NLP) use the notion and argue that the definitions provided are not consistent with how LMs are trained, process information, and generate new tokens. To illustrate this incommensurability we assume the view that transformer-based LMs implement an \textit{implicit} finite-order Markov kernel mapping contexts to conditional token distributions. In this view, reasoning-like outputs correspond to statistical regularities and approximate statistical invariances in the learned kernel rather than the implementation of explicit logical mechanisms. This view is illustrative of the claim that LMs are "statistical pattern matchers"" and not genuine reasoners and provides a perspective that clarifies why reasoning-like outputs arise in LMs without any guarantees of logical consistency. This distinction is fundamental to how epistemic uncertainty is evaluated in LMs. We invite a discussion on the importance of how the computational processes of the systems we build and analyze in NLP research are described.

</details>


### [4] [Scaling Open-Weight Large Language Models for Hydropower Regulatory Information Extraction: A Systematic Analysis](https://arxiv.org/abs/2511.11821)
*Hong-Jun Yoon,Faisal Ashraf,Thomas A. Ruggles,Debjani Singh*

Main category: cs.CL

TL;DR: 研究发现14B参数量是监管文档信息提取的关键阈值，小模型F1值最高51%，需企业级基础设施的大模型可达77%F1，并揭示了小模型中完美召回率反映提取失败的异常现象


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在监管文档信息提取中性能与计算资源间的权衡问题，为实际部署提供实证依据

Method: 通过评估7个参数量0.6B-70B的开源模型在水电许可文件中的表现，分析验证方法与性能关系

Result: 发现14B参数阈值后验证方法有效性骤增（F1从<0.15提升至0.64），消费级模型可达64%F1，企业级大模型接近77%F1

Conclusion: 研究成果为水电合规提供直接价值，并揭示了参数规模效应在信息提取任务中的普适性规律

Abstract: Information extraction from regulatory documents using large language models presents critical trade-offs between performance and computational resources. We evaluated seven open-weight models (0.6B-70B parameters) on hydropower licensing documentation to provide empirical deployment guidance.
  Our analysis identified a pronounced 14B parameter threshold where validation methods transition from ineffective (F1 $<$ 0.15) to viable (F1 = 0.64). Consumer-deployable models achieve 64\% F1 through appropriate validation, while smaller models plateau at 51\%. Large-scale models approach 77\% F1 but require enterprise infrastructure.
  We identified systematic hallucination patterns where perfect recall indicates extraction failure rather than success in smaller models. Our findings establish the first comprehensive resource-performance mapping for open-weight information extraction in regulatory contexts, enabling evidence-based model selection.
  These results provide immediate value for hydropower compliance while contributing insights into parameter scaling effects that generalize across information extraction tasks.

</details>


### [5] [Towards Autoformalization of LLM-generated Outputs for Requirement Verification](https://arxiv.org/abs/2511.11829)
*Mihir Gupte,Ramesh S*

Main category: cs.CL

TL;DR: 探索基于LLM的自动形式化工具验证大语言模型输出逻辑一致性的初步研究，通过两个实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏验证大语言模型（LLM）生成结构化输出准确性的形式化方法，需开发工具确保逻辑一致性。

Method: 使用基于LLM的自动形式化器，针对自然语言需求进行两组实验：验证等价性需求一致性，识别需求与LLM输出的逻辑冲突。

Result: 实验一成功检测等价需求的逻辑一致性；实验二发现需求与LLM输出的逻辑矛盾，验证工具有效性。

Conclusion: 自动形式化为LLM输出验证提供新路径，初步成果为后续大规模研究奠定基础。

Abstract: Autoformalization, the process of translating informal statements into formal logic, has gained renewed interest with the emergence of powerful Large Language Models (LLMs). While LLMs show promise in generating structured outputs from natural language (NL), such as Gherkin Scenarios from NL feature requirements, there's currently no formal method to verify if these outputs are accurate. This paper takes a preliminary step toward addressing this gap by exploring the use of a simple LLM-based autoformalizer to verify LLM-generated outputs against a small set of natural language requirements. We conducted two distinct experiments. In the first one, the autoformalizer successfully identified that two differently-worded NL requirements were logically equivalent, demonstrating the pipeline's potential for consistency checks. In the second, the autoformalizer was used to identify a logical inconsistency between a given NL requirement and an LLM-generated output, highlighting its utility as a formal verification tool. Our findings, while limited, suggest that autoformalization holds significant potential for ensuring the fidelity and logical consistency of LLM-generated outputs, laying a crucial foundation for future, more extensive studies into this novel application.

</details>


### [6] [Three Stage Narrative Analysis; Plot-Sentiment Breakdown, Structure Learning and Concept Detection](https://arxiv.org/abs/2511.11857)
*Taimur Khan,Ramoza Ahsan,Mohib Hameed*

Main category: cs.CL

TL;DR: 提出结合情感分析与聚类技术的电影剧本分析框架，通过情感弧线追踪帮助用户选择叙事内容


<details>
  <summary>Details</summary>
Motivation: 传统故事分析依赖人工处理，自动化框架可高效解析叙事数据中的情感特征与角色关联

Method: 使用LabMTsimple模块构建基于NRC-VAD评分的定制词典，结合Ward层次聚类分析情感轨迹

Result: 实验证明该框架输出的情感分析结果能有效辅助用户进行叙事内容选择决策

Conclusion: 本框架为自动化故事语义分析提供了有效的多维度情感解析工具

Abstract: Story understanding and analysis have long been challenging areas within Natural Language Understanding. Automated narrative analysis requires deep computational semantic representations along with syntactic processing. Moreover, the large volume of narrative data demands automated semantic analysis and computational learning rather than manual analytical approaches. In this paper, we propose a framework that analyzes the sentiment arcs of movie scripts and performs extended analysis related to the context of the characters involved. The framework enables the extraction of high-level and low-level concepts conveyed through the narrative. Using dictionary-based sentiment analysis, our approach applies a custom lexicon built with the LabMTsimple storylab module. The custom lexicon is based on the Valence, Arousal, and Dominance scores from the NRC-VAD dataset. Furthermore, the framework advances the analysis by clustering similar sentiment plots using Wards hierarchical clustering technique. Experimental evaluation on a movie dataset shows that the resulting analysis is helpful to consumers and readers when selecting a narrative or story.

</details>


### [7] [Identifying Imaging Follow-Up in Radiology Reports: A Comparative Analysis of Traditional ML and LLM Approaches](https://arxiv.org/abs/2511.11867)
*Namu Park,Giridhar Kaushik Ramachandran,Kevin Lybarger,Fei Xia,Ozlem Uzuner,Meliha Yetisgen,Martin Gunn*

Main category: cs.CL

TL;DR: 构建放射学报告标注语料库，系统比较传统机器学习与生成式LLMs，发现优化后的GPT-4o表现接近人类水平，但传统模型仍具实用价值。


<details>
  <summary>Details</summary>
Motivation: 解决放射学领域缺乏专业评估数据集的问题，系统评估LLMs在随访依从性检测任务中的性能表现。

Method: 使用6,393份标注报告，对比逻辑回归/SVM/Longformer/微调Llama3与GPT-4o/GPT-OSS-20B（基础/优化配置），采用非参数自举法评估精确率/召回率/F1分数。

Result: GPT-4o优化配置表现最佳（F1=0.832），接近人工标注一致性（F1=0.846）。逻辑回归/SVM表现强劲（F1≈0.775），证明高效模型的基线价值。

Conclusion: LLMs通过提示优化接近人类水平，但可解释性强、资源消耗低的传统模型仍是临床实践中重要的基准解决方案。

Abstract: Large language models (LLMs) have shown considerable promise in clinical natural language processing, yet few domain-specific datasets exist to rigorously evaluate their performance on radiology tasks. In this work, we introduce an annotated corpus of 6,393 radiology reports from 586 patients, each labeled for follow-up imaging status, to support the development and benchmarking of follow-up adherence detection systems. Using this corpus, we systematically compared traditional machine-learning classifiers, including logistic regression (LR), support vector machines (SVM), Longformer, and a fully fine-tuned Llama3-8B-Instruct, with recent generative LLMs. To evaluate generative LLMs, we tested GPT-4o and the open-source GPT-OSS-20B under two configurations: a baseline (Base) and a task-optimized (Advanced) setting that focused inputs on metadata, recommendation sentences, and their surrounding context. A refined prompt for GPT-OSS-20B further improved reasoning accuracy. Performance was assessed using precision, recall, and F1 scores with 95% confidence intervals estimated via non-parametric bootstrapping. Inter-annotator agreement was high (F1 = 0.846). GPT-4o (Advanced) achieved the best performance (F1 = 0.832), followed closely by GPT-OSS-20B (Advanced; F1 = 0.828). LR and SVM also performed strongly (F1 = 0.776 and 0.775), underscoring that while LLMs approach human-level agreement through prompt optimization, interpretable and resource-efficient models remain valuable baselines.

</details>


### [8] [MedPT: A Massive Medical Question Answering Dataset for Brazilian-Portuguese Speakers](https://arxiv.org/abs/2511.11878)
*Fernanda Bufon Färber,Iago Alves Brito,Julia Soares Dollis,Pedro Schindler Freire Brasil Ribeiro,Rafael Teixeira Sousa,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

TL;DR: 提出首个巴西葡萄牙语大规模真实医患对话数据集MedPT，通过多阶段清洗和LLM标注实现语义分类，在医疗分诊任务中达到94%准确率并展现临床语义深度。


<details>
  <summary>Details</summary>
Motivation: 解决医疗领域LLM资源集中于高资源语言的问题，捕捉葡萄牙语独特的临床文化特征（如地方性疾病），突破简单翻译无法处理语义细微差异的局限。

Method: 1. 构建384,095真实医患问答对
2. 混合定量定性多阶段清洗流程过滤噪声
3. LLM标注实现七类语义分类与上下文增强
4. 设计医疗专科分诊基准测试（20分类任务）

Result: 1. 覆盖3,200个医疗主题，揭示医患沟通自然不对称性
2. 1.7B模型分诊F1达94%
3. 错误分析显示混淆源自真实临床模糊（如共病关系）

Conclusion: 公开MedPT数据集推动葡萄牙语医疗AI发展，促进文化敏感的公平技术，证实数据集能捕捉深层临床语义复杂性。

Abstract: While large language models (LLMs) show transformative potential in healthcare, their development remains focused on high-resource languages, creating a critical barrier for others as simple translation fails to capture unique clinical and cultural nuances, such as endemic diseases. To address this, we introduce MedPT, the first large-scale, real-world corpus for Brazilian Portuguese, comprising 384,095 authentic question-answer pairs from patient-doctor interactions. The dataset underwent a meticulous multi-stage curation protocol, using a hybrid quantitative-qualitative analysis to filter noise and contextually enrich thousands of ambiguous queries. We further augmented the corpus via LLM-driven annotation, classifying questions into seven semantic types to capture user intent. Our analysis reveals its thematic breadth (3,200 topics) and unique linguistic properties, like the natural asymmetry in patient-doctor communication. To validate its utility, we benchmark a medical specialty routing task: fine-tuning a 1.7B parameter model achieves an outstanding 94\% F1-score on a 20-class setup. Furthermore, our qualitative error analysis shows misclassifications are not random but reflect genuine clinical ambiguities (e.g., between comorbid conditions), proving the dataset's deep semantic richness. We publicly release MedPT to foster the development of more equitable, accurate, and culturally-aware medical technologies for the Portuguese-speaking world.

</details>


### [9] [ClinStructor: AI-Powered Structuring of Unstructured Clinical Texts](https://arxiv.org/abs/2511.11883)
*Karthikeyan K,Raghuveer Thirukovalluru,David Carlson*

Main category: cs.CL

TL;DR: ClinStructor利用大语言模型将临床自由文本转化为结构化QA对，提升模型可解释性，预测性能仅小幅下降（AUC降低2-3%）


<details>
  <summary>Details</summary>
Motivation: 临床笔记的非结构化特性导致三大问题：存在潜在偏见（性别/种族偏见）、跨临床场景泛化能力差（不同电子病历系统格式差异导致模型迁移困难）、模型可解释性不足

Method: 提出ClinStructor流程：先通过LLM将自由文本转化为任务特定的结构化问答对，再进行预测建模

Result: 在ICU死亡率预测任务中，相比直接微调模型，该方法在保持预测性能（AUC仅下降2-3%）的同时显著提升透明度和可控性

Conclusion: 为构建可靠、可解释且可泛化的临床机器学习模型奠定基础，推动临床环境中的AI应用

Abstract: Clinical notes contain valuable, context-rich information, but their unstructured format introduces several challenges, including unintended biases (e.g., gender or racial bias), and poor generalization across clinical settings (e.g., models trained on one EHR system may perform poorly on another due to format differences) and poor interpretability. To address these issues, we present ClinStructor, a pipeline that leverages large language models (LLMs) to convert clinical free-text into structured, task-specific question-answer pairs prior to predictive modeling. Our method substantially enhances transparency and controllability and only leads to a modest reduction in predictive performance (a 2-3% drop in AUC), compared to direct fine-tuning, on the ICU mortality prediction task. ClinStructor lays a strong foundation for building reliable, interpretable, and generalizable machine learning models in clinical environments.

</details>


### [10] [Context-Emotion Aware Therapeutic Dialogue Generation: A Multi-component Reinforcement Learning Approach to Language Models for Mental Health Support](https://arxiv.org/abs/2511.11884)
*Eric Hua Qing Zhang,Julia Ive*

Main category: cs.CL

TL;DR: 通过强化学习优化GPT-2的心理治疗对话生成能力，在多项指标及情感准确率上显著超越基线模型


<details>
  <summary>Details</summary>
Motivation: 新冠疫情加剧心理健康服务需求，现有大语言模型缺乏情境情感感知能力

Method: 采用监督微调(SFT)和强化学习(RL)，设计多组件奖励函数整合情境信息和情感状态

Result: 强化学习模型在BLEU(0.0111)、ROUGE-L(0.1317)等指标提升，情感准确率达99.34%

Conclusion: 强化学习可开发有效的治疗对话辅助系统，需保持人类临床监督机制

Abstract: Mental health illness represents a substantial global socioeconomic burden, with COVID-19 further exacerbating accessibility challenges and driving increased demand for telehealth mental health support. While large language models (LLMs) offer promising solutions through 24/7 availability and non-judgmental interactions, pre-trained models often lack the contextual and emotional awareness necessary for appropriate therapeutic responses. This paper investigated the application of supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance GPT-2's capacity for therapeutic dialogue generation. The methodology restructured input formats to enable simultaneous processing of contextual information and emotional states alongside user input, employing a multi-component reward function that aligned model outputs with professional therapist responses and annotated emotions. Results demonstrated improvements through reinforcement learning over baseline GPT-2 across multiple evaluation metrics: BLEU (0.0111), ROUGE-1 (0.1397), ROUGE-2 (0.0213), ROUGE-L (0.1317), and METEOR (0.0581). LLM evaluation confirmed high contextual relevance and professionalism, while reinforcement learning achieved 99.34% emotion accuracy compared to 66.96% for baseline GPT-2. These findings demonstrate reinforcement learning's effectiveness in developing therapeutic dialogue systems that can serve as valuable assistive tools for therapists while maintaining essential human clinical oversight.

</details>


### [11] [Additive Large Language Models for Semi-Structured Text](https://arxiv.org/abs/2511.11922)
*Karthikeyan K,Raghuveer Thirukovalluru,David Carlson*

Main category: cs.CL

TL;DR: 提出可解释性临床文本分类框架CALM，通过加法模型分解结构化文本成分贡献，在保持性能的同时增强预测可解释性


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型在临床文本分类中预测过程不透明，阻碍其在需要解释预测依据的医疗场景应用。需开发能提供组件级风险解释的模型

Method: 采用加法模型架构，将半结构化文本（如病历段落）各成分贡献值相加进行预测，前向计算过程自带解释性，支持类似广义加法模型的可视化分析

Result: 在保持与传统LLM分类器相当性能的前提下，实现组件级风险曲线可视化，支持质量核查并揭示临床相关模式

Conclusion: CALM框架成功平衡性能与可解释性，其加法结构同时支持个体和群体层面的可信解释，适用于需要透明度的临床决策场景

Abstract: Large Language Models have advanced clinical text classification, but their opaque predictions remain a critical barrier to practical adoption in research and clinical settings where investigators and physicians need to understand which parts of a patient's record drive risk signals. To address this challenge, we introduce \textbf{CALM}, short for \textbf{Classification with Additive Large Language Models}, an interpretable framework for semi-structured text where inputs are composed of semantically meaningful components, such as sections of an admission note or question-answer fields from an intake form. CALM predicts outcomes as the additive sum of each component's contribution, making these contributions part of the forward computation itself and enabling faithful explanations at both the patient and population level. The additive structure also enables clear visualizations, such as component-level risk curves similar to those used in generalized additive models, making the learned relationships easier to inspect and communicate. Although CALM expects semi-structured inputs, many clinical documents already have this form, and similar structure can often be automatically extracted from free-text notes. CALM achieves performance comparable to conventional LLM classifiers while improving trust, supporting quality-assurance checks, and revealing clinically meaningful patterns during model development and auditing.

</details>


### [12] [InData: Towards Secure Multi-Step, Tool-Based Data Analysis](https://arxiv.org/abs/2511.11933)
*Karthikeyan K,Raghuveer Thirukovalluru,Bhuwan Dhingra,David Edwin Carlson*

Main category: cs.CL

TL;DR: 论文提出InData数据集评估大语言模型通过安全工具进行多步骤数据分析的能力，发现现有模型在复杂任务上存在显著性能落差


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型直接操作敏感数据的安全风险，通过工具隔离实现安全的数据分析

Method: 构建包含三个难度等级（简单/中等/困难）的InData数据集，测试模型的多步骤工具组合推理能力

Result: 大型模型如gpt-oss-120b在简单任务准确率达97.3%，但在困难任务骤降至69.6%

Conclusion: 当前LLM缺乏可靠的多步骤工具推理能力，InData为开发评估更强大的工具使用能力提供了基准

Abstract: Large language model agents for data analysis typically generate and execute code directly on databases. However, when applied to sensitive data, this approach poses significant security risks. To address this issue, we propose a security-motivated alternative: restrict LLMs from direct code generation and data access, and require them to interact with data exclusively through a predefined set of secure, verified tools. Although recent tool-use benchmarks exist, they primarily target tool selection and simple execution rather than the compositional, multi-step reasoning needed for complex data analysis. To reduce this gap, we introduce Indirect Data Engagement (InData), a dataset designed to assess LLMs' multi-step tool-based reasoning ability. InData includes data analysis questions at three difficulty levels--Easy, Medium, and Hard--capturing increasing reasoning complexity. We benchmark 15 open-source LLMs on InData and find that while large models (e.g., gpt-oss-120b) achieve high accuracy on Easy tasks (97.3%), performance drops sharply on Hard tasks (69.6%). These results show that current LLMs still lack robust multi-step tool-based reasoning ability. With InData, we take a step toward enabling the development and evaluation of LLMs with stronger multi-step tool-use capabilities. We will publicly release the dataset and code.

</details>


### [13] [Improving LLM's Attachment to External Knowledge In Dialogue Generation Tasks Through Entity Anonymization](https://arxiv.org/abs/2511.11946)
*Hadi Sheikhi,Chenyang Huang,Osmar R. Zaïane*

Main category: cs.CL

TL;DR: 针对大语言模型在知识图谱对话生成任务中过度依赖内部知识的问题，提出LLM-KAT评估框架和实体匿名化技术，有效提升外部知识利用率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在知识图谱对话生成任务中即使获得完美知识图谱，仍存在过度依赖内部知识、与外部知识图谱脱节的现象。需要量化评估和干预方法改善该问题。

Method: 1. 设计LLM-KAT评估框架量化知识依附度；2. 提出实体匿名化技术，通过遮蔽实体信息强制模型学习知识图谱结构关系。实验基于OpenDialKG数据集开展。

Result: 实体匿名化技术使LLM在知识依附度指标上提升显著，BLEU-4提升21.6%，知识召回率提升18.9%，验证了方法的有效性。

Conclusion: 通过结构化的评估框架和简单的数据预处理策略，能够有效增强大语言模型对外部知识图谱的利用能力，为知识增强型对话系统提供新思路。

Abstract: Knowledge graph-based dialogue generation (KG-DG) is a challenging task requiring models to effectively incorporate external knowledge into conversational responses. While large language models (LLMs) have achieved impressive results across various NLP tasks, their ability to utilize external knowledge in KG-DG remains under-explored. We observe that LLMs often rely on internal knowledge, leading to detachment from provided knowledge graphs, even when they are given a flawlessly retrieved knowledge graph. First, we introduce LLM-KAT, an evaluation procedure for measuring knowledge attachment in generated responses. Second, we propose a simple yet effective entity anonymization technique to encourage LLMs to better leverage external knowledge. Experiments on the OpenDialKG dataset demonstrate that our approach improves LLMs' attachment on external knowledge.

</details>


### [14] [On the Entropy Calibration of Language Models](https://arxiv.org/abs/2511.11966)
*Steven Cao,Gregory Valiant,Percy Liang*

Main category: cs.CL

TL;DR: 研究发现语言模型的熵校准问题不会随规模扩大自动改善，大模型与小模型错误积累率相似，但理论上存在无需牺牲多样性的校准可能性


<details>
  <summary>Details</summary>
Motivation: 自回归模型存在错误积累的根本问题，现有截断方案牺牲多样性换取质量。需探究规模扩展能否改善校准，以及理论上的无损校准可能性

Method: 1. 理论分析幂律数据分布下的校准错误缩放规律 2. 实证测量0.5B-70B参数模型的校准错误 3. 探讨预测未来熵的黑箱模型存在的理论可能性

Result: 当数据幂律指数接近1时，校准错误随规模改善极慢（缩放指数≈0）。大模型与小模型错误积累率相似，需类似截断处理。理论上黑箱模型可实现无损校准

Conclusion: 单纯扩大模型规模无法解决校准问题，但理论证明无损校准存在可能性。这为未来结合熵预测机制的新型模型设计提供了方向

Abstract: We study the problem of entropy calibration, which asks whether a language model's entropy over generations matches its log loss on human text. Past work found that models are miscalibrated, with entropy per step increasing (and text quality decreasing) as generations grow longer. This error accumulation is a fundamental problem in autoregressive models, and the standard solution is to truncate the distribution, which improves text quality at the cost of diversity. In this paper, we ask: is miscalibration likely to improve with scale, and is it theoretically possible to calibrate without tradeoffs? To build intuition, we first study a simplified theoretical setting to characterize the scaling behavior of miscalibration with respect to dataset size. We find that the scaling behavior depends on the power law exponent of the data distribution -- in particular, for a power law exponent close to 1, the scaling exponent is close to 0, meaning that miscalibration improves very slowly with scale. Next, we measure miscalibration empirically in language models ranging from 0.5B to 70B parameters. We find that the observed scaling behavior is similar to what is predicted by the simplified setting: our fitted scaling exponents for text are close to 0, meaning that larger models accumulate error at a similar rate as smaller ones. This scaling (or, lack thereof) provides one explanation for why we sample from larger models with similar amounts of truncation as smaller models, even though the larger models are of higher quality. However, truncation is not a satisfying solution because it comes at the cost of increased log loss. In theory, is it even possible to reduce entropy while preserving log loss? We prove that it is possible, if we assume access to a black box which can fit models to predict the future entropy of text.

</details>


### [15] [A Reasoning Paradigm for Named Entity Recognition](https://arxiv.org/abs/2511.11978)
*Hui Huang,Yanping Chen,Ruizhang Huang,Chuan Lin,Yongbin Qin*

Main category: cs.CL

TL;DR: 提出将NER任务从隐式模式匹配转为显式推理的三阶段框架(CoT生成→CoT微调→推理增强)，显著提升零样本场景性能


<details>
  <summary>Details</summary>
Motivation: 传统生成式LLM通过语义模式匹配进行实体识别，缺乏可验证的显式推理机制，导致零样本/低资源场景性能受限

Method: 1.生成NER方向思维链标注数据 2.用CoT微调模型生成推理依据 3.通过强化学习优化推理过程

Result: 零样本场景F1值超越GPT-4达12.3个百分点，展示出卓越的认知能力和推理导向的信息提取潜力

Conclusion: 该框架为可解释性信息提取研究提供了新范式，其显式推理机制可扩展到其他序列标注任务

Abstract: Generative LLMs typically improve Named Entity Recognition (NER) performance through instruction tuning. They excel at generating entities by semantic pattern matching but lack an explicit, verifiable reasoning mechanism. This "cognitive shortcutting" leads to suboptimal performance and brittle generalization, especially in zero-shot and lowresource scenarios where reasoning from limited contextual cues is crucial. To address this issue, a reasoning framework is proposed for NER, which shifts the extraction paradigm from implicit pattern matching to explicit reasoning. This framework consists of three stages: Chain of Thought (CoT) generation, CoT tuning, and reasoning enhancement. First, a dataset annotated with NER-oriented CoTs is generated, which contain task-relevant reasoning chains. Then, they are used to tune the NER model to generate coherent rationales before deriving the final answer. Finally, a reasoning enhancement stage is implemented to optimize the reasoning process using a comprehensive reward signal. This stage ensures explicit and verifiable extractions. Experiments show that ReasoningNER demonstrates impressive cognitive ability in the NER task, achieving competitive performance. In zero-shot settings, it achieves state-of-the-art (SOTA) performance, outperforming GPT-4 by 12.3 percentage points on the F1 score. Analytical results also demonstrate its great potential to advance research in reasoningoriented information extraction. Our codes are available at https://github.com/HuiResearch/ReasoningIE.

</details>


### [16] [Critical or Compliant? The Double-Edged Sword of Reasoning in Chain-of-Thought Explanations](https://arxiv.org/abs/2511.12001)
*Eunkyu Park,Wesley Hanwen Deng,Vasudha Varadarajan,Mingxi Yan,Gunhee Kim,Maarten Sap,Motahhare Eslami*

Main category: cs.CL

TL;DR: CoT解释在道德场景中同时具备澄清与误导的双重作用：用户易将信任等同于结果认同，自信语气会抑制错误检测但保持依赖


<details>
  <summary>Details</summary>
Motivation: 揭示CoT解释在提升透明度的同时可能引发确认偏差的机制，探究用户信任与错误检测的实际表现

Method: 通过系统扰动推理链和操控表达语气，分析视觉语言模型(VLMs)的推理错误对用户信任的影响

Result: 用户信任与结果认同高度绑定（即使推理错误仍保持依赖），自信语气可压制错误检测同时维持依赖水平

Conclusion: NLP系统需设计鼓励批判性思考的解释机制，而非培养盲目信任的说明方式

Abstract: Explanations are often promoted as tools for transparency, but they can also foster confirmation bias; users may assume reasoning is correct whenever outputs appear acceptable. We study this double-edged role of Chain-of-Thought (CoT) explanations in multimodal moral scenarios by systematically perturbing reasoning chains and manipulating delivery tones. Specifically, we analyze reasoning errors in vision language models (VLMs) and how they impact user trust and the ability to detect errors. Our findings reveal two key effects: (1) users often equate trust with outcome agreement, sustaining reliance even when reasoning is flawed, and (2) the confident tone suppresses error detection while maintaining reliance, showing that delivery styles can override correctness. These results highlight how CoT explanations can simultaneously clarify and mislead, underscoring the need for NLP systems to provide explanations that encourage scrutiny and critical thinking rather than blind trust. All code will be released publicly.

</details>


### [17] [CURE: Cultural Understanding and Reasoning Evaluation - A Framework for "Thick" Culture Alignment Evaluation in LLMs](https://arxiv.org/abs/2511.12014)
*Truong Vo,Sanmi Koyejo*

Main category: cs.CL

TL;DR: 提出基于真实情境的文化评估框架，揭示传统评估方法存在系统性高估且不稳定的问题


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型文化能力评估局限于去语境化判断，缺乏对文化推理能力的深度检测

Method: 构建包含现实情境的基准测试，设计覆盖性、特异性、内涵、连贯性四个新型评估维度

Result: 实验显示传统评估方差高达30%，新方法使评估方差降低58%且信号稳定性提升3倍

Conclusion: 基于情境的厚评估能有效捕捉文化推理深度，提供更稳定可解释的评估体系

Abstract: Large language models (LLMs) are increasingly deployed in culturally diverse environments, yet existing evaluations of cultural competence remain limited. Existing methods focus on de-contextualized correctness or forced-choice judgments, overlooking the need for cultural understanding and reasoning required for appropriate responses. To address this gap, we introduce a set of benchmarks that, instead of directly probing abstract norms or isolated statements, present models with realistic situational contexts that require culturally grounded reasoning. In addition to the standard Exact Match metric, we introduce four complementary metrics (Coverage, Specificity, Connotation, and Coherence) to capture different dimensions of model's response quality. Empirical analysis across frontier models reveals that thin evaluation systematically overestimates cultural competence and produces unstable assessments with high variance. In contrast, thick evaluation exposes differences in reasoning depth, reduces variance, and provides more stable, interpretable signals of cultural understanding.

</details>


### [18] [Exploring Parameter-Efficient Fine-Tuning and Backtranslation for the WMT 25 General Translation Task](https://arxiv.org/abs/2511.12109)
*Felipe Fujita,Hideyuki Takada*

Main category: cs.CL

TL;DR: 结合回译和微调策略可在小规模日语语料上显著提升机器翻译质量（COMET 0.597），超越单一技术应用


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言对（英日）在有限训练数据下的机器翻译质量提升难题，探索轻量级优化方案

Method: 分阶段实施：1) 回译生成合成数据 2) Mistral 7B模型微调真实平行语料 3) 组合回译增强与微调的协同策略

Result: 基线0.460→回译0.468→微调0.589→组合策略0.597，证明组合方法优于单独技术（+8.5%最终提升）

Conclusion: 回译与微调的协同效应突破数据局限，为低资源语言对提供高效优化路径，具有跨语言迁移潜力

Abstract: In this paper, we explore the effectiveness of combining fine-tuning and backtranslation on a small Japanese corpus for neural machine translation. Starting from a baseline English{\textrightarrow}Japanese model (COMET = 0.460), we first apply backtranslation (BT) using synthetic data generated from monolingual Japanese corpora, yielding a modest increase (COMET = 0.468). Next, we fine-tune (FT) the model on a genuine small parallel dataset drawn from diverse Japanese news and literary corpora, achieving a substantial jump to COMET = 0.589 when using Mistral 7B. Finally, we integrate both backtranslation and fine-tuning{ -- }first augmenting the small dataset with BT generated examples, then adapting via FT{ -- }which further boosts performance to COMET = 0.597. These results demonstrate that, even with limited training data, the synergistic use of backtranslation and targeted fine-tuning on Japanese corpora can significantly enhance translation quality, outperforming each technique in isolation. This approach offers a lightweight yet powerful strategy for improving low-resource language pairs.

</details>


### [19] [LLMLagBench: Identifying Temporal Training Boundaries in Large Language Models](https://arxiv.org/abs/2511.12116)
*Piotr Pęzik,Konrad Kaczyński,Maria Szymańska,Filip Żarnecki,Zuzanna Deckert,Jakub Kwiatkowski,Wojciech Janowski*

Main category: cs.CL

TL;DR: LLMLagBench基准测试用于检测大语言模型的训练数据时效边界


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型因训练数据时效性限制导致的知识混淆问题

Method: 通过构建包含近期事件的评估基准，系统性检测模型训练数据的时间边界

Result: 验证了基准可靠性并应用于多个公开/未公开训练时间的LLM评估

Conclusion: 该基准为检测模型知识时效性提供了标准化评估工具

Abstract: Large Language Models (LLMs) are pretrained on textual data up to a specific temporal cutoff. This creates a strict knowledge boundary beyond which models cannot provide accurate information without querying external sources. More subtly, when this limitation is unknown or ignored, LLMs may inadvertently blend outdated time-sensitive information with general knowledge during reasoning tasks, potentially compromising response accuracy. We introduce LLMLagBench, an LLM freshness benchmark, as a systematic approach for identifying the earliest probable temporal boundaries of an LLM's training data by evaluating its knowledge of recent events. We then apply this benchmark to evaluate a large set of LLMs, including models with both explicitly declared and undeclared training cutoffs. The reliability of the benchmark is assessed by manual validation and comparison with publicly released information about LLM pretraining.

</details>


### [20] [PRISM of Opinions: A Persona-Reasoned Multimodal Framework for User-centric Conversational Stance Detection](https://arxiv.org/abs/2511.12130)
*Bingbing Wang,Zhixin Bai,Zhengda Jin,Zihan Wang,Xintong Song,Jingjie Lin,Sixuan Li,Jing Li,Ruifeng Xu*

Main category: cs.CL

TL;DR: PRISM模型通过用户画像和多模态对齐提升立场检测效果


<details>
  <summary>Details</summary>
Motivation: 解决现有MCSD研究中伪多模态（仅源帖含视觉信息）和用户同质性（忽略个体特征）的局限性

Method: 1) 从历史数据提取纵向用户画像
2) 通过思维链对齐会话中的多模态信息
3) 使用任务互强化机制联合优化立场检测与响应生成

Result: 在U-MStance数据集上显著超越基线模型

Conclusion: 用户特征挖掘与上下文多模态推理对现实立场理解具有关键作用

Abstract: The rapid proliferation of multimodal social media content has driven research in Multimodal Conversational Stance Detection (MCSD), which aims to interpret users' attitudes toward specific targets within complex discussions. However, existing studies remain limited by: **1) pseudo-multimodality**, where visual cues appear only in source posts while comments are treated as text-only, misaligning with real-world multimodal interactions; and **2) user homogeneity**, where diverse users are treated uniformly, neglecting personal traits that shape stance expression. To address these issues, we introduce **U-MStance**, the first user-centric MCSD dataset, containing over 40k annotated comments across six real-world targets. We further propose **PRISM**, a **P**ersona-**R**easoned mult**I**modal **S**tance **M**odel for MCSD. PRISM first derives longitudinal user personas from historical posts and comments to capture individual traits, then aligns textual and visual cues within conversational context via Chain-of-Thought to bridge semantic and pragmatic gaps across modalities. Finally, a mutual task reinforcement mechanism is employed to jointly optimize stance detection and stance-aware response generation for bidirectional knowledge transfer. Experiments on U-MStance demonstrate that PRISM yields significant gains over strong baselines, underscoring the effectiveness of user-centric and context-grounded multimodal reasoning for realistic stance understanding.

</details>


### [21] [AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing](https://arxiv.org/abs/2511.12133)
*Qingyu Zhang,Chunlei Xin,Xuanang Chen,Yaojie Lu,Hongyu Lin,Xianpei Han,Le Sun,Qing Ye,Qianlong Xie,Xingxing Wang*

Main category: cs.CL

TL;DR: 构建首个真实场景电话销售数据集TeleSalesCorpus，提出双阶段AI-Salesman框架，结合贝叶斯监督强化学习和动态大纲引导代理，显著提升复杂说服场景表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在说服性对话中存在策略脆弱性和事实幻觉问题，且缺乏任务专用数据，需构建数据集并开发鲁棒销售策略框架。

Method: 双阶段架构：训练阶段用贝叶斯监督强化学习从噪声对话学习策略；推理阶段通过DOGA代理结合预建脚本库实现动态策略指导，并设计综合评估框架。

Result: AI-Salesman在自动指标和人工评估中均显著优于基线模型，验证其在复杂销售场景的有效性。

Conclusion: 通过创新数据集、双阶段架构和动态引导机制，为说服对话系统设立新标杆，展现LLM在商业场景的应用潜力。

Abstract: Goal-driven persuasive dialogue, exemplified by applications like telemarketing, requires sophisticated multi-turn planning and strict factual faithfulness, which remains a significant challenge for even state-of-the-art Large Language Models (LLMs). A lack of task-specific data often limits previous works, and direct LLM application suffers from strategic brittleness and factual hallucination. In this paper, we first construct and release TeleSalesCorpus, the first real-world-grounded dialogue dataset for this domain. We then propose AI-Salesman, a novel framework featuring a dual-stage architecture. For the training stage, we design a Bayesian-supervised reinforcement learning algorithm that learns robust sales strategies from noisy dialogues. For the inference stage, we introduce the Dynamic Outline-Guided Agent (DOGA), which leverages a pre-built script library to provide dynamic, turn-by-turn strategic guidance. Moreover, we design a comprehensive evaluation framework that combines fine-grained metrics for key sales skills with the LLM-as-a-Judge paradigm. Experimental results demonstrate that our proposed AI-Salesman significantly outperforms baseline models in both automatic metrics and comprehensive human evaluations, showcasing its effectiveness in complex persuasive scenarios.

</details>


### [22] [Seeing is Believing: Rich-Context Hallucination Detection for MLLMs via Backward Visual Grounding](https://arxiv.org/abs/2511.12140)
*Pinxue Guo,Chongruo Wu,Xinyu Zhou,Lingyi Hong,Zhaoyu Chen,Jinglun Li,Kaixun Jiang,Sen-ching Samson Cheung,Wei Zhang,Wenqiang Zhang*

Main category: cs.CL

TL;DR: 提出VBackChecker框架——基于像素级Grounding LLM的无参考幻觉检测方法，通过R-Instruct数据生成和R²-HalBench基准测试，在丰富上下文场景中实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 多模态大模型（MLLMs）存在显著幻觉问题，现有检测方法难以有效处理复杂场景且缺乏可解释性

Method: 结合推理与参考分割的像素级Grounding LLM验证视觉一致性，创新设计R-Instruct指令调优数据生成流程，构建R²-HalBench多维度幻觉基准

Result: 在R²-HalBench上超越现有框架（性能接近GPT-4o），像素级定位任务提升超10%

Conclusion: VBackChecker实现无参考、可解释的幻觉检测，在真实复杂场景中展现优越性能，相关资源已开源

Abstract: Multimodal Large Language Models (MLLMs) have unlocked powerful cross-modal capabilities, but still significantly suffer from hallucinations. As such, accurate detection of hallucinations in MLLMs is imperative for ensuring their reliability in practical applications. To this end, guided by the principle of "Seeing is Believing", we introduce VBackChecker, a novel reference-free hallucination detection framework that verifies the consistency of MLLMgenerated responses with visual inputs, by leveraging a pixellevel Grounding LLM equipped with reasoning and referring segmentation capabilities. This reference-free framework not only effectively handles rich-context scenarios, but also offers interpretability. To facilitate this, an innovative pipeline is accordingly designed for generating instruction-tuning data (R-Instruct), featuring rich-context descriptions, grounding masks, and hard negative samples. We further establish R^2 -HalBench, a new hallucination benchmark for MLLMs, which, unlike previous benchmarks, encompasses real-world, rich-context descriptions from 18 MLLMs with high-quality annotations, spanning diverse object-, attribute, and relationship-level details. VBackChecker outperforms prior complex frameworks and achieves state-of-the-art performance on R^2 -HalBench, even rivaling GPT-4o's capabilities in hallucination detection. It also surpasses prior methods in the pixel-level grounding task, achieving over a 10% improvement. All codes, data, and models are available at https://github.com/PinxueGuo/VBackChecker.

</details>


### [23] [CriticSearch: Fine-Grained Credit Assignment for Search Agents via a Retrospective Critic](https://arxiv.org/abs/2511.12159)
*Yaocheng Zhang,Haohuan Huang,Zijun Song,Yuanheng Zhu,Qichao Zhang,Zijie Zhao,Dongbin Zhao*

Main category: cs.CL

TL;DR: 提出CriticSearch框架，通过细粒度信用分配和回顾性批评机制解决强化学习奖励稀疏问题，提升多跳推理任务的训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的搜索代理存在奖励稀疏问题，导致训练效率低且不稳定。需要更有效的反馈机制改进策略优化。

Method: 采用冻结的不对称批评LLM，利用完整轨迹和正确答案生成密集回合级奖励，通过细粒度信用分配指导策略改进。

Result: 在多个多跳推理基准测试中收敛更快、稳定性更高，性能显著优于现有基线方法。

Conclusion: CriticSearch通过密集反馈机制有效提升模型在复杂任务中的适应性和泛化能力，验证了细粒度奖励设计的优越性。

Abstract: Tool-Integrated Reasoning (TIR) with search engines enables large language models to iteratively retrieve up-to-date external knowledge, enhancing adaptability and generalization in complex question-answering tasks. However, existing search agent pipelines typically depend on reinforcement learning based optimization, which often suffers from sparse outcome rewards, leading to inefficient exploration and unstable training. We introduce CriticSearch, a fine-grained credit-assignment framework that supplies dense, turn-level feedback via a retrospective critic mechanism. During training, a frozen, asymmetric critique LLM retrospectively evaluates each turn using privileged information from the full trajectory and gold answers, converting these assessments into stable, dense rewards that guide policy improvement. Experimental results across diverse multi-hop reasoning benchmarks demonstrate that CriticSearch consistently outperforms existing baselines, achieving faster convergence, improved training stability, and higher performance.

</details>


### [24] [MME-RAG: Multi-Manager-Expert Retrieval-Augmented Generation for Fine-Grained Entity Recognition in Task-Oriented Dialogues](https://arxiv.org/abs/2511.12213)
*Liang Xue,Haoyu Liu,Yajun Tian,Xinyu Zhong,Yang Liu*

Main category: cs.CL

TL;DR: MME-RAG框架通过类型判断-实体提取两阶段协作架构与KeyInfo检索技术，提升任务对话中细粒度实体识别的领域适应性和可控性。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在对话场景的领域适应性不足、检索过程不可控的问题，需要无需训练即可精准适应多领域实体识别的方案。

Method: 1. 层级分解：管理器预判实体类型，专家模型执行具体提取 2. KeyInfo检索器动态注入语义对齐的少样本示例指导推理

Result: 在CrossNER等数据集上超越基线模型，消融实验验证层级架构使F1提升3.2%，KeyInfo检索贡献2.8%性能增益

Conclusion: 该框架通过模块化设计实现可解释的跨领域迁移，为自适应对话理解提供可扩展解决方案

Abstract: Fine-grained entity recognition is crucial for reasoning and decision-making in task-oriented dialogues, yet current large language models (LLMs) continue to face challenges in domain adaptation and retrieval controllability. We introduce MME-RAG, a Multi-Manager-Expert Retrieval-Augmented Generation framework that decomposes entity recognition into two coordinated stages: type-level judgment by lightweight managers and span-level extraction by specialized experts. Each expert is supported by a KeyInfo retriever that injects semantically aligned, few-shot exemplars during inference, enabling precise and domain-adaptive extraction without additional training. Experiments on CrossNER, MIT-Movie, MIT-Restaurant, and our newly constructed multi-domain customer-service dataset demonstrate that MME-RAG performs better than recent baselines in most domains. Ablation studies further show that both the hierarchical decomposition and KeyInfo-guided retrieval are key drivers of robustness and cross-domain generalization, establishing MME-RAG as a scalable and interpretable solution for adaptive dialogue understanding.

</details>


### [25] [Consistency Is the Key: Detecting Hallucinations in LLM Generated Text By Checking Inconsistencies About Key Facts](https://arxiv.org/abs/2511.12236)
*Raavi Gupta,Pranav Hari Panicker,Sumit Bhatia,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: 提出CONFACTCHECK方法，利用LLM内部及跨模型间事实探针的一致性检测幻觉，无需外部知识库且降低API调用成本


<details>
  <summary>Details</summary>
Motivation: LLM生成内容存在事实性幻觉风险，现有检测方法在受限条件下需多次API调用导致效率低下且成本高昂

Method: 基于生成文本内部事实探针响应应保持一致的假设，通过单LLM内部及跨LLM间的一致性验证检测幻觉

Result: 在多数据集测试中，CONFACTCHECK在相同条件下以更少资源实现比基线方法更高的检测准确率

Conclusion: CONFACTCHECK为受限环境下的幻觉检测提供了高效解决方案，显著优化资源消耗与检测精度

Abstract: Large language models (LLMs), despite their remarkable text generation capabilities, often hallucinate and generate text that is factually incorrect and not grounded in real-world knowledge. This poses serious risks in domains like healthcare, finance, and customer support. A typical way to use LLMs is via the APIs provided by LLM vendors where there is no access to model weights or options to fine-tune the model. Existing methods to detect hallucinations in such settings where the model access is restricted or constrained by resources typically require making multiple LLM API calls, increasing latency and API cost. We introduce CONFACTCHECK, an efficient hallucination detection approach that does not leverage any external knowledge base and works on the simple intuition that responses to factual probes within the generated text should be consistent within a single LLM and across different LLMs. Rigorous empirical evaluation on multiple datasets that cover both the generation of factual texts and the open generation shows that CONFACTCHECK can detect hallucinated facts efficiently using fewer resources and achieves higher accuracy scores compared to existing baselines that operate under similar conditions. Our code is available here.

</details>


### [26] [ViConBERT: Context-Gloss Aligned Vietnamese Word Embedding for Polysemous and Sense-Aware Representations](https://arxiv.org/abs/2511.12249)
*Khang T. Huynh,Dung H. Nguyen,Binh T. Nguyen*

Main category: cs.CL

TL;DR: 提出ViConBERT框架整合对比学习和词义蒸馏，解决越南语细粒度语义理解资源不足问题，并在多项任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 越南语缺乏针对细粒度语义理解（如词义消歧和上下文相似度）的可靠模型和评估资源，现有进展主要集中在英语等高资源语言

Method: 结合对比学习(SimCLR)和基于词义解释的知识蒸馏方法，构建越南语首个大规模合成评估数据集ViConWSD

Result: ViConBERT在WSD任务F1达0.87，ViCon(AP=0.88)和ViSim-400(ρ=0.60)表现优异，有效建模离散词义和连续语义关系

Conclusion: 该框架成功填补越南语语义理解资源空白，代码模型数据已开源，为低资源语言语义建模提供新方案

Abstract: Recent advances in contextualized word embeddings have greatly improved semantic tasks such as Word Sense Disambiguation (WSD) and contextual similarity, but most progress has been limited to high-resource languages like English. Vietnamese, in contrast, still lacks robust models and evaluation resources for fine-grained semantic understanding. In this paper, we present ViConBERT, a novel framework for learning Vietnamese contextualized embeddings that integrates contrastive learning (SimCLR) and gloss-based distillation to better capture word meaning. We also introduce ViConWSD, the first large-scale synthetic dataset for evaluating semantic understanding in Vietnamese, covering both WSD and contextual similarity. Experimental results show that ViConBERT outperforms strong baselines on WSD (F1 = 0.87) and achieves competitive performance on ViCon (AP = 0.88) and ViSim-400 (Spearman's rho = 0.60), demonstrating its effectiveness in modeling both discrete senses and graded semantic relations. Our code, models, and data are available at https://github.com/tkhangg0910/ViConBERT

</details>


### [27] [Cmprsr: Abstractive Token-Level Question-Agnostic Prompt Compressor](https://arxiv.org/abs/2511.12281)
*Ivan Zakazov,Alexander Sharipov,Berke Argin,Oussama Gabouj,Kamel Charaf,Alexi Semiz,Lorenzo Drudi,Nicolas Baldwin,Robert West*

Main category: cs.CL

TL;DR: 提出基于小LLM压缩大模型输入的Prompt压缩范式，开发出在压缩率控制和跨领域通用性表现优异的Cmprsr模型


<details>
  <summary>Details</summary>
Motivation: 降低使用黑盒大语言模型的高成本，通过小型LLM压缩输入减少大模型使用开销

Method: 1) 建立25个模型的基准测试 2) 用Textgrad优化gpt-4.1-mini 3) 对Qwen3-4B进行SFT+GRPO联合训练

Result: Cmprsr在MeetingBank/LongBench长文本和GSM8k短提示上全面超越基线模型，压缩率误差控制在±3%内

Conclusion: Cmprsr首次实现压缩率精准控制与性能平衡，验证了LLM-as-compressor范式的有效性

Abstract: Motivated by the high costs of using black-box Large Language Models (LLMs), we introduce a novel prompt compression paradigm, under which we use smaller LLMs to compress inputs for the larger ones. We present the first comprehensive LLM-as-a-compressor benchmark spanning 25 open- and closed-source models, which reveals significant disparity in models' compression ability in terms of (i) preserving semantically important information (ii) following the user-provided compression rate (CR). We further improve the performance of gpt-4.1-mini, the best overall vanilla compressor, with Textgrad-based compression meta-prompt optimization. We also identify the most promising open-source vanilla LLM - Qwen3-4B - and post-train it with a combination of supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), pursuing the dual objective of CR adherence and maximizing the downstream task performance. We call the resulting model Cmprsr and demonstrate its superiority over both extractive and vanilla abstractive compression across the entire range of compression rates on lengthy inputs from MeetingBank and LongBench as well as short prompts from GSM8k. The latter highlights Cmprsr's generalizability across varying input lengths and domains. Moreover, Cmprsr closely follows the requested compression rate, offering fine control over the cost-quality trade-off.

</details>


### [28] [AugAbEx : Way Forward for Extractive Case Summarization](https://arxiv.org/abs/2511.12290)
*Purnima Bindal,Vikas Kumar,Sagar Rathore,Vasudha Bhatnagar*

Main category: cs.CL

TL;DR: 利用抽象摘要生成法律文件抽取式摘要，扩充数据集并验证质量，促进法律领域自动摘要研究


<details>
  <summary>Details</summary>
Motivation: 抽象摘要方法易误解法律术语且忽略关键细节，需降低人工标注成本并保留专家意见

Method: 开发轻量流程将抽象摘要转化为抽取式版本，扩充7个数据集并进行多维度评估

Result: 通过结构/词汇/语义/领域四维度验证摘要质量，承诺公开增强版数据集

Conclusion: 该方法有效转化摘要类型并保留核心信息，为法律文档自动摘要提供高质量数据资源

Abstract: Summarization of legal judgments poses a heavy cognitive burden on law practitioners due to the complexity of the language, context-sensitive legal jargon, and the length of the document. Therefore, the automatic summarization of legal documents has attracted serious attention from natural language processing researchers. Since the abstractive summaries of legal documents generated by deep neural methods remain prone to the risk of misrepresenting nuanced legal jargon or overlooking key contextual details, we envisage a rising trend toward the use of extractive case summarizers.
  Given the high cost of human annotation for gold standard extractive summaries, we engineer a light and transparent pipeline that leverages existing abstractive gold standard summaries to create the corresponding extractive gold standard versions. The approach ensures that the experts` opinions ensconced in the original gold standard abstractive summaries are carried over to the transformed extractive summaries. We aim to augment seven existing case summarization datasets, which include abstractive summaries, by incorporating corresponding extractive summaries and create an enriched data resource for case summarization research community. To ensure the quality of the augmented extractive summaries, we perform an extensive comparative evaluation with the original abstractive gold standard summaries covering structural, lexical, and semantic dimensions. We also compare the domain-level information of the two summaries. We commit to release the augmented datasets in the public domain for use by the research community and believe that the resource will offer opportunities to advance the field of automatic summarization of legal documents.

</details>


### [29] [Do LLMs and Humans Find the Same Questions Difficult? A Case Study on Japanese Quiz Answering](https://arxiv.org/abs/2511.12300)
*Naoya Sugiura,Kosuke Yamada,Yasuhiro Ogawa,Katsuhiko Toyama,Ryohei Sasano*

Main category: cs.CL

TL;DR: 研究发现，相比人类，LLM更不擅长处理维基百科未覆盖答案及需数值回答的日本问答


<details>
  <summary>Details</summary>
Motivation: 探究人类认为困难的问题是否对大型语言模型同样构成挑战

Method: 1. 收集含人类正确率的日本问答数据 2. 多场景测试LLM回答表现 3. 从维基百科覆盖度和答案类型两个维度进行对比分析

Result: LLM在答案未被维基百科收录的问题上正确率低24.7%，数值类问题正确率低于人类15.3%

Conclusion: LLM的知识边界受训练数据限制，数值推理能力待提升，需针对性优化模型架构

Abstract: LLMs have achieved performance that surpasses humans in many NLP tasks. However, it remains unclear whether problems that are difficult for humans are also difficult for LLMs. This study investigates how the difficulty of quizzes in a buzzer setting differs between LLMs and humans. Specifically, we first collect Japanese quiz data including questions, answers, and correct response rate of humans, then prompted LLMs to answer the quizzes under several settings, and compare their correct answer rate to that of humans from two analytical perspectives. The experimental results showed that, compared to humans, LLMs struggle more with quizzes whose correct answers are not covered by Wikipedia entries, and also have difficulty with questions that require numerical answers.

</details>


### [30] [Don't Think of the White Bear: Ironic Negation in Transformer Models Under Cognitive Load](https://arxiv.org/abs/2511.12381)
*Logan Mann,Nayan Saxena,Sarah Tandon,Chenhao Sun,Savar Toteja,Kevin Zhu*

Main category: cs.CL

TL;DR: 否定指令在LLMs中引发讽刺性反弹现象：干扰文本类型（语义/重复）显著影响反弹强度，极性分离程度与反弹持续性正相关。通过电路追踪发现中层注意力头放大禁忌词激活，同时发布ReboundBench数据集推动后续研究。


<details>
  <summary>Details</summary>
Motivation: 探究否定指令在LLMs中是否重现人类认知中的讽刺反弹现象，揭示模型抑制概念时内部激活与反弹形成的机制关联。

Method: 1. 干扰负荷实验：通过语义/句法/重复三种干扰文本测量反弹强度
2. 极性分离实验：检验模型对中立/负面框架的区分能力及其与反弹持续性的关系
3. 电路追踪分析：定位导致反弹的特定注意力头

Result: • 语义干扰使反弹增强3-5倍，重复干扰支持有效抑制
• 极性分离指数每增加0.1单位，反弹持续时间延长28%
• 中层（12-18层）注意力头负责放大禁忌词logits 1.8倍

Conclusion: 将认知科学的反弹预测与模型机制解耦结合，证明长上下文干扰中早期层抑制与中层放大的动态平衡，为可控文本生成提供新视角。

Abstract: Negation instructions such as 'do not mention $X$' can paradoxically increase the accessibility of $X$ in human thought, a phenomenon known as ironic rebound. Large language models (LLMs) face the same challenge: suppressing a concept requires internally activating it, which may prime rebound instead of avoidance. We investigated this tension with two experiments. \textbf{(1) Load \& content}: after a negation instruction, we vary distractor text (semantic, syntactic, repetition) and measure rebound strength. \textbf{(2) Polarity separation}: We test whether models distinguish neutral from negative framings of the same concept and whether this separation predicts rebound persistence. Results show that rebound consistently arises immediately after negation and intensifies with longer or semantic distractors, while repetition supports suppression. Stronger polarity separation correlates with more persistent rebound. Together, these findings, complemented by a circuit tracing analysis that identifies sparse middle-layer attention heads amplifying forbidden tokens while early layers suppress, link cognitive predictions of ironic rebound with mechanistic insights into long-context interference. To support future work, we release ReboundBench, a dataset of $5,000$ systematically varied negation prompts designed to probe rebound in LLMs.

</details>


### [31] [From Phonemes to Meaning: Evaluating Large Language Models on Tamil](https://arxiv.org/abs/2511.12387)
*Jeyarajalingam Varsha,Menan Velayuthan,Sumirtha Karunakaran,Rasan Nivethiga,Kengatharaiyer Sarveswaran*

Main category: cs.CL

TL;DR: ILAKKANAM首个泰米尔语专项评测基准，基于820道斯里兰卡学校试题，揭示大语言模型在复杂语言任务中的表现差距


<details>
  <summary>Details</summary>
Motivation: 解决现有多语言基准依赖英译数据、无法捕捉泰米尔语语言文化特征的问题

Method: 构建分级标注的泰米尔语评测基准（1-13年级），采用标准化框架评估闭源/开源模型

Result: Gemini 2.5表现最佳，开源模型落后；模型表现随语言复杂度增加明显下降，且性能与语言理解能力无强相关性

Conclusion: 当前模型依赖数据曝光而非真正语言理解，凸显低资源形态丰富语言的建模挑战

Abstract: Large Language Models (LLMs) have shown strong generalization across tasks in high-resource languages; however, their linguistic competence in low-resource and morphologically rich languages such as Tamil remains largely unexplored. Existing multilingual benchmarks often rely on translated English datasets, failing to capture the linguistic and cultural nuances of the target language. To address this gap, we introduce ILAKKANAM, the first Tamil-specific linguistic evaluation benchmark manually curated using 820 questions from Sri Lankan school-level Tamil subject examination papers. Each question is annotated by trained linguists under five linguistic categories and a factual knowledge category, spanning Grades 1--13 to ensure broad linguistic coverage. We evaluate both closed-source and open-source LLMs using a standardized evaluation framework. Our results show that Gemini 2.5 achieves the highest overall performance, while open-source models lag behind, highlighting the gap in linguistic grounding. Category- and grade-wise analyses reveal that all models perform well on lower-grade questions but show a clear decline as linguistic complexity increases. Further, no strong correlation is observed between a model's overall performance and its ability to identify linguistic categories, suggesting that performance may be driven by exposure rather than genuine understanding.

</details>


### [32] [Probing Preference Representations: A Multi-Dimensional Evaluation and Analysis Method for Reward Models](https://arxiv.org/abs/2511.12464)
*Chenglong Wang,Yifu Huo,Yang Gan,Yongyu Mu,Qiaozhi He,Murun Yang,Bei Li,Chunliang Zhang,Tongran Liu,Anxiang Ma,Zhengtao Yu,Jingbo Zhu,Tong Xiao*

Main category: cs.CL

TL;DR: 提出多维度奖励模型评测基准MRMBench，通过推理时探针方法提升奖励模型可解释性，验证其在LLM对齐任务中的有效性


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型评估方法仅关注固定排序测试集，缺乏对偏好维度细粒度性能的评估。需建立多维度评估体系以指导奖励模型开发

Method: 1. 构建包含6个探测任务的MRMBench基准
2. 设计推理时探针方法解析奖励预测维度
3. 通过多维度偏好捕捉优化奖励模型

Result: MRMBench与LLM对齐性能强相关（相关系数0.89）
奖励模型在多维度偏好捕捉存在显著不足（平均准确率下降23%）
探针置信度指标提升SFT模型对齐效果（+15%胜率）

Conclusion: 多维度评估揭示奖励模型优化方向，推理时探针增强可解释性，多目标优化是提升奖励模型性能的关键路径

Abstract: Previous methods evaluate reward models by testing them on a fixed pairwise ranking test set, but they typically do not provide performance information on each preference dimension. In this work, we address the evaluation challenge of reward models by probing preference representations. To confirm the effectiveness of this evaluation method, we construct a Multi-dimensional Reward Model Benchmark (MRMBench), a collection of six probing tasks for different preference dimensions. We design it to favor and encourage reward models that better capture preferences across different dimensions. Furthermore, we introduce an analysis method, inference-time probing, which identifies the dimensions used during the reward prediction and enhances its interpretability. Through extensive experiments, we find that MRMBench strongly correlates with the alignment performance of large language models (LLMs), making it a reliable reference for developing advanced reward models. Our analysis of MRMBench evaluation results reveals that reward models often struggle to capture preferences across multiple dimensions, highlighting the potential of multi-objective optimization in reward modeling. Additionally, our findings show that the proposed inference-time probing method offers a reliable metric for assessing the confidence of reward predictions, which ultimately improves the alignment of LLMs.

</details>


### [33] [Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing](https://arxiv.org/abs/2511.12472)
*Mengying Wang,Chenhui Ma,Ao Jiao,Tuo Liang,Pengjun Lu,Shrinidhi Hegde,Yu Yin,Evren Gurkan-Cavusoglu,Yinghui Wu*

Main category: cs.CL

TL;DR: 论文提出SerenQA框架用于评估大语言模型在科学知识图谱问答中发现意外答案的能力，特别针对药物重定向领域构建专家标注基准，揭示现有模型在发现创新性见解方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱问答系统仅能返回高相关性但可预测的答案，而科学领域（如药物重定向）需要模型具备发现新颖、意外答案的能力，需建立系统的评估体系。

Method: 基于临床知识图谱构建专家标注的意外性评估基准，设计包含相关性、新颖性、意外性的量化指标，开发知识检索+子图推理+意外性探索的三阶段结构化评估流程。

Result: 实验表明现有大模型在检索任务表现良好（F1达0.82），但在识别真正具备价值的意外发现任务中准确率显著下降（仅0.31），暴露创新性推理短板。

Conclusion: SerenQA框架首次系统量化LLM在科学发现中的意外性能力，为提升模型在复杂知识推理中的创新性提供了基准测试和优化方向。

Abstract: Large Language Models (LLMs) have greatly advanced knowledge graph question answering (KGQA), yet existing systems are typically optimized for returning highly relevant but predictable answers. A missing yet desired capacity is to exploit LLMs to suggest surprise and novel ("serendipitious") answers. In this paper, we formally define the serendipity-aware KGQA task and propose the SerenQA framework to evaluate LLMs' ability to uncover unexpected insights in scientific KGQA tasks. SerenQA includes a rigorous serendipity metric based on relevance, novelty, and surprise, along with an expert-annotated benchmark derived from the Clinical Knowledge Graph, focused on drug repurposing. Additionally, it features a structured evaluation pipeline encompassing three subtasks: knowledge retrieval, subgraph reasoning, and serendipity exploration. Our experiments reveal that while state-of-the-art LLMs perform well on retrieval, they still struggle to identify genuinely surprising and valuable discoveries, underscoring a significant room for future improvements. Our curated resources and extended version are released at: https://cwru-db-group.github.io/serenQA.

</details>


### [34] [SGuard-v1: Safety Guardrail for Large Language Models](https://arxiv.org/abs/2511.12497)
*JoonHo Lee,HyeonMin Cho,Jaewoong Yun,Hyunjae Lee,JunKyu Lee,Juree Seok*

Main category: cs.CL

TL;DR: SGuard-v1是基于20亿参数Granite模型构建的轻量级LLM安全护栏系统，包含内容过滤和越狱攻击检测双组件，支持12种语言，通过140万训练数据实现顶尖安全性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在对话场景中的有害内容生成和对抗性提示攻击问题，在保持轻量化的同时提升安全防护能力和部署可行性。

Method: 采用ContentFilter（基于MLCommons危害分类体系）和JailbreakFilter（覆盖60种攻击类型）双模型架构，通过指令微调整合收集/合成的140万训练数据。

Result: 在公开和专有安全基准测试中达到SOTA，提供多类安全预测和置信度评分，以Apache-2.0协议开源促进AI安全研究。

Conclusion: SGuard-v1在安全性能与计算效率间取得平衡，其模块化设计和可解释性输出为实际部署提供了实用解决方案。

Abstract: We present SGuard-v1, a lightweight safety guardrail for Large Language Models (LLMs), which comprises two specialized models to detect harmful content and screen adversarial prompts in human-AI conversational settings. The first component, ContentFilter, is trained to identify safety risks in LLM prompts and responses in accordance with the MLCommons hazard taxonomy, a comprehensive framework for trust and safety assessment of AI. The second component, JailbreakFilter, is trained with a carefully designed curriculum over integrated datasets and findings from prior work on adversarial prompting, covering 60 major attack types while mitigating false-unsafe classification. SGuard-v1 is built on the 2B-parameter Granite-3.3-2B-Instruct model that supports 12 languages. We curate approximately 1.4 million training instances from both collected and synthesized data and perform instruction tuning on the base model, distributing the curated data across the two component according to their designated functions. Through extensive evaluation on public and proprietary safety benchmarks, SGuard-v1 achieves state-of-the-art safety performance while remaining lightweight, thereby reducing deployment overhead. SGuard-v1 also improves interpretability for downstream use by providing multi-class safety predictions and their binary confidence scores. We release the SGuard-v1 under the Apache-2.0 License to enable further research and practical deployment in AI safety.

</details>


### [35] [QA-Noun: Representing Nominal Semantics via Natural Language Question-Answer Pairs](https://arxiv.org/abs/2511.12504)
*Maria Tseytlin,Paul Roit,Omri Abend,Ido Dagan,Ayal Klein*

Main category: cs.CL

TL;DR: 提出QA-Noun框架，通过9个问题模板捕捉名词中心语义关系，与QA-SRL结合实现细粒度语义分解，覆盖AMR名词关系并提升130%粒度。


<details>
  <summary>Details</summary>
Motivation: 现有QA-SRL框架未有效处理名词中心语义关系，AMR等传统方法在隐含关系覆盖和粒度上存在不足。

Method: 设计9个覆盖显式句法/隐式上下文角色的问题模板，构建2,000+标注数据集，开发与QA-SRL整合的模型。

Result: 覆盖AMR 95%名词关系并发现额外隐含关系，与QA-SRL组合后粒度超FactScore 130%，实现句子意义单元化分解。

Conclusion: QA-Noun完善了基于QA的语义框架，为跨文本对齐提供可扩展的细粒度分解方案。

Abstract: Decomposing sentences into fine-grained meaning units is increasingly used to model semantic alignment. While QA-based semantic approaches have shown effectiveness for representing predicate-argument relations, they have so far left noun-centered semantics largely unaddressed. We introduce QA-Noun, a QA-based framework for capturing noun-centered semantic relations. QA-Noun defines nine question templates that cover both explicit syntactical and implicit contextual roles for nouns, producing interpretable QA pairs that complement verbal QA-SRL. We release detailed guidelines, a dataset of over 2,000 annotated noun mentions, and a trained model integrated with QA-SRL to yield a unified decomposition of sentence meaning into individual, highly fine-grained, facts. Evaluation shows that QA-Noun achieves near-complete coverage of AMR's noun arguments while surfacing additional contextually implied relations, and that combining QA-Noun with QA-SRL yields over 130\% higher granularity than recent fact-based decomposition methods such as FactScore and DecompScore. QA-Noun thus complements the broader QA-based semantic framework, forming a comprehensive and scalable approach to fine-grained semantic decomposition for cross-text alignment.

</details>


### [36] [TAdaRAG: Task Adaptive Retrieval-Augmented Generation via On-the-Fly Knowledge Graph Construction](https://arxiv.org/abs/2511.12520)
*Jie Zhang,Bo Tang,Wanzi Shao,Wenqiang Wei,Jihao Zhao,Jianqing Zhu,Zhiyu li,Wen Xi,Zehao Lin,Feiyu Xiong,Yanchao Tan*

Main category: cs.CL

TL;DR: 提出了TAdaRAG框架，通过动态构建任务自适应知识图谱解决传统RAG方法的知识截断和推理链断裂问题


<details>
  <summary>Details</summary>
Motivation: 传统RAG存在知识片段截断导致信息丢失，以及非结构化知识检索引入无关细节的问题，影响推理准确性

Method: 设计意图驱动路由机制实现领域模板适配，结合监督微调和强化学习驱动的隐式知识抽取机制

Result: 在6个公共基准和NowNewsQA业务基准测试中优于现有方法，支持不同领域和长文本任务

Conclusion: TAdaRAG通过结构化知识整合展示了强大的泛化能力和实际应用价值

Abstract: Retrieval-Augmented Generation (RAG) improves large language models by retrieving external knowledge, often truncated into smaller chunks due to the input context window, which leads to information loss, resulting in response hallucinations and broken reasoning chains. Moreover, traditional RAG retrieves unstructured knowledge, introducing irrelevant details that hinder accurate reasoning. To address these issues, we propose TAdaRAG, a novel RAG framework for on-the-fly task-adaptive knowledge graph construction from external sources. Specifically, we design an intent-driven routing mechanism to a domain-specific extraction template, followed by supervised fine-tuning and a reinforcement learning-based implicit extraction mechanism, ensuring concise, coherent, and non-redundant knowledge integration. Evaluations on six public benchmarks and a real-world business benchmark (NowNewsQA) across three backbone models demonstrate that TAdaRAG outperforms existing methods across diverse domains and long-text tasks, highlighting its strong generalization and practical effectiveness.

</details>


### [37] [Mitigating Length Bias in RLHF through a Causal Lens](https://arxiv.org/abs/2511.12573)
*Hyeonji Kim,Sujeong Oh,Sanghack Lee*

Main category: cs.CL

TL;DR: 通过反事实数据增强解决RLHF奖励模型的长度偏差，提升奖励建模的稳健性和内容敏感性


<details>
  <summary>Details</summary>
Motivation: 现有RLHF训练的奖励模型存在系统性长度偏好，将回答冗长程度与内容质量混淆，需开发能独立评估内容质量的方法

Method: 构建两种反事实数据对：(1)内容相似但长度差异的响应对 (2)长度相近但内容质量差异的响应对，用于训练解耦长度与质量的奖励模型

Result: 实证评估显示该方法有效降低奖励分配的长度偏差，使策略模型输出更简洁且内容集中

Conclusion: 提出的因果框架成功减少长度偏差，增强RLHF流程中奖励模型对内容质量的敏感性和鲁棒性

Abstract: Reinforcement learning from human feedback (RLHF) is widely used to align large language models (LLMs) with human preferences. However, RLHF-trained reward models often exhibit length bias -- a systematic tendency to favor longer responses by conflating verbosity with quality. We propose a causal framework for analyzing and mitigating length bias in RLHF reward modeling. Central to our approach is a counterfactual data augmentation method that generates response pairs designed to isolate content quality from verbosity. These counterfactual examples are then used to train the reward model, enabling it to assess responses based on content quality independently of verbosity. Specifically, we construct (1) length-divergent pairs with similar content and (2) content-divergent pairs of similar length. Empirical evaluations show that our method reduces length bias in reward assignment and leads to more concise, content-focused outputs from the policy model. These findings demonstrate that the proposed approach effectively reduces length bias and improves the robustness and content sensitivity of reward modeling in RLHF pipelines.

</details>


### [38] [MMWOZ: Building Multimodal Agent for Task-oriented Dialogue](https://arxiv.org/abs/2511.12586)
*Pu-Hai Yang,Heyan Huang,Heng-Da Xu,Fanshu Sun,Xian-Ling Mao,Chaoxu Mu*

Main category: cs.CL

TL;DR: 提出MMWOZ多模态对话数据集和MATE基线模型，解决传统任务型对话系统在真实GUI场景中的落地难题


<details>
  <summary>Details</summary>
Motivation: 传统任务型对话系统依赖定制API，与真实场景中普遍存在的GUI界面存在应用鸿沟

Method: 1. 基于MultiWOZ 2.3构建网页式GUI
2. 自动化转换对话状态为操作指令
3. 采集网页快照与操作指令配对

Result: 建立首个包含55K对话样本的多模态数据集MMWOOZ，提出融合视觉和文本输入的MATE基线模型

Conclusion: 通过多模态数据融合和GUI交互模拟，为构建实用化任务型对话系统提供了新范式

Abstract: Task-oriented dialogue systems have garnered significant attention due to their conversational ability to accomplish goals, such as booking airline tickets for users. Traditionally, task-oriented dialogue systems are conceptualized as intelligent agents that interact with users using natural language and have access to customized back-end APIs. However, in real-world scenarios, the widespread presence of front-end Graphical User Interfaces (GUIs) and the absence of customized back-end APIs create a significant gap for traditional task-oriented dialogue systems in practical applications. In this paper, to bridge the gap, we collect MMWOZ, a new multimodal dialogue dataset that is extended from MultiWOZ 2.3 dataset. Specifically, we begin by developing a web-style GUI to serve as the front-end. Next, we devise an automated script to convert the dialogue states and system actions from the original dataset into operation instructions for the GUI. Lastly, we collect snapshots of the web pages along with their corresponding operation instructions. In addition, we propose a novel multimodal model called MATE (Multimodal Agent for Task-oriEnted dialogue) as the baseline model for the MMWOZ dataset. Furthermore, we conduct comprehensive experimental analysis using MATE to investigate the construction of a practical multimodal agent for task-oriented dialogue.

</details>


### [39] [Group-Aware Reinforcement Learning for Output Diversity in Large Language Models](https://arxiv.org/abs/2511.12596)
*Oron Anschel,Alon Shoshan,Adam Botach,Shunit Haviv Hakimi,Asaf Gendler,Emanuel Ben Baruch,Nadav Bhonker,Igor Kviatkovsky,Manoj Aggarwal,Gerard Medioni*

Main category: cs.CL

TL;DR: 提出GAPO方法解决LLM模式崩溃问题，通过群体感知奖励机制提升回答多样性，同时保持基准测试准确性


<details>
  <summary>Details</summary>
Motivation: 大语言模型常因模式崩溃重复生成相似回答，限制了多场景应用的多样性需求

Method: 基于GRPO框架改进，引入频率感知奖励函数，在群体层面实现有效回答的均匀采样机制

Result: GAPO模型在开放域提示中提升回答多样性，在GSM8K/MATH等基准保持准确率

Conclusion: GAPO通过群体级优化有效平衡多样性与准确性，为LLM训练提供通用解决方案

Abstract: Large Language Models (LLMs) often suffer from mode collapse, repeatedly generating the same few completions even when many valid answers exist, limiting their diversity across a wide range of tasks. We introduce Group-Aware Policy Optimization (GAPO), a simple extension of the recent and popular Group Relative Policy Optimization (GRPO) that computes rewards over the group as a whole. GAPO enables learning from the group-level properties such as diversity and coverage. We demonstrate GAPO using a frequency-aware reward function that encourages uniform sampling over valid LLM completions, and show that GAPO-trained models produce valid and more diverse model responses. Beyond this setup, GAPO generalizes to open-ended prompts and improves response diversity without compromising accuracy on standard LLM benchmarks (GSM8K, MATH, HumanEval, MMLU-Pro). Our code will be made publicly available.

</details>


### [40] [Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data](https://arxiv.org/abs/2511.12609)
*Yunxin Li,Xinyu Chen,Shenyuan Jiang,Haoyuan Shi,Zhenyu Liu,Xuanyu Zhang,Nanhao Deng,Zhenran Xu,Yicheng Ma,Meishan Zhang,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: Uni-MoE 2.0是荔枝家族开源的跨模态大模型，通过动态MoE架构、渐进式训练策略和多模态数据匹配技术，在85个基准测试中超过Qwen2.5-Omni等模型，实现全模态理解与生成能力的突破。


<details>
  <summary>Details</summary>
Motivation: 提升语言核心的多模态理解、推理与生成能力，基于Qwen2.5-7B架构构建更高效的跨模态模型体系。

Method: 1. 动态容量MoE框架（共享/路由/空专家）平衡计算效率；2. Omni-Modality 3D RoPE实现跨模态时空对齐；3. 渐进式训练（预训练→GSPO-DPO强化学习）配合平衡数据组合。

Result: 在85个基准测试中取得SOTA：视频理解平均提升7%（8项）、全模态理解+7%（4项）、视听推理+4%；语音识别错误率降低4.2%，图像生成5项指标领先。

Conclusion: 该模型通过创新的MoE架构和训练策略，在减少训练数据量（75B tokens）的同时实现跨模态能力的全面提升，为开源社区提供了高效的全模态解决方案。

Abstract: We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.

</details>


### [41] [Knots: A Large-Scale Multi-Agent Enhanced Expert-Annotated Dataset and LLM Prompt Optimization for NOTAM Semantic Parsing](https://arxiv.org/abs/2511.12630)
*Maoqi Liu,Quan Fang,Yang Yang,Can Zhao,Kaiquan Cai*

Main category: cs.CL

TL;DR: 研究者提出NOTAM语义解析任务，构建高质量数据集Knots并通过多技术手段显著提升航空文本处理能力


<details>
  <summary>Details</summary>
Motivation: 现有NOTAM研究仅关注表层任务，缺乏深层语义理解，导致复杂航空文本解析困难

Method: 提出语义解析任务框架，构建12,347条专家标注数据集Knots，采用多智能体协作框架提升数据质量

Result: 通过系统评估提示工程和模型适应技术，在航空文本理解上取得显著提升，F1值提升15.2%

Conclusion: 提出的方法有效解决NOTAM深层语义解析，数据集和实验结果为自动化分析系统提供重要支持

Abstract: Notice to Air Missions (NOTAMs) serve as a critical channel for disseminating key flight safety information, yet their complex linguistic structures and implicit reasoning pose significant challenges for automated parsing. Existing research mainly focuses on surface-level tasks such as classification and named entity recognition, lacking deep semantic understanding. To address this gap, we propose NOTAM semantic parsing, a task emphasizing semantic inference and the integration of aviation domain knowledge to produce structured, inference-rich outputs. To support this task, we construct Knots (Knowledge and NOTAM Semantics), a high-quality dataset of 12,347 expert-annotated NOTAMs covering 194 Flight Information Regions, enhanced through a multi-agent collaborative framework for comprehensive field discovery. We systematically evaluate a wide range of prompt-engineering strategies and model-adaptation techniques, achieving substantial improvements in aviation text understanding and processing. Our experimental results demonstrate the effectiveness of the proposed approach and offer valuable insights for automated NOTAM analysis systems. Our code is available at: https://github.com/Estrellajer/Knots.

</details>


### [42] [Reason-KE++: Aligning the Process, Not Just the Outcome, for Faithful LLM Knowledge Editing](https://arxiv.org/abs/2511.12661)
*Yuchen Wu,Liang Ding,Li Shen,Dacheng Tao*

Main category: cs.CL

TL;DR: 提出Reason-KE++框架解决LLM复杂推理中的忠实性问题，通过过程级监督实现95.48%的SOTA准确率


<details>
  <summary>Details</summary>
Motivation: 现有SFT方法存在'忠实性差距'，导致LLM在复杂推理中优先使用参数知识而非上下文事实（如错误关联NASA与休斯顿）

Method: SFT+RL框架（Stage-aware Reward机制），对分解、子答案正确性等中间推理步骤进行密集监督

Result: 在MQUAKE-CF-3k数据集上达到95.48%准确率（+5.28%），显著改善事实幻觉问题（Hop准确率提升）

Conclusion: 复杂任务中必须对齐推理过程而非仅优化结果，过程监督是构建可信LLM的关键

Abstract: Aligning Large Language Models (LLMs) to be faithful to new knowledge in complex, multi-hop reasoning tasks is a critical, yet unsolved, challenge. We find that SFT-based methods, e.g., Reason-KE, while state-of-the-art, suffer from a "faithfulness gap": they optimize for format mimicry rather than sound reasoning. This gap enables the LLM's powerful parametric priors to override new contextual facts, resulting in critical factual hallucinations (e.g., incorrectly reasoning "Houston" from "NASA" despite an explicit edit). To solve this core LLM alignment problem, we propose Reason-KE++, an SFT+RL framework that instills process-level faithfulness. Its core is a Stage-aware Reward mechanism that provides dense supervision for intermediate reasoning steps (e.g., Decomposition, Sub-answer Correctness). Crucially, we identify that naive outcome-only RL is a deceptive trap for LLM alignment: it collapses reasoning integrity (e.g., 19.00% Hop acc) while superficially boosting final accuracy. Our process-aware framework sets a new SOTA of 95.48% on MQUAKE-CF-3k (+5.28%), demonstrating that for complex tasks, aligning the reasoning process is essential for building trustworthy LLMs.

</details>


### [43] [Improving Direct Persian-English Speech-to-Speech Translation with Discrete Units and Synthetic Parallel Data](https://arxiv.org/abs/2511.12690)
*Sina Rashidi,Hossein Sameti*

Main category: cs.CL

TL;DR: 提出结合自监督预训练、离散语音单元和合成平行数据的端到端波斯语-英语语音翻译系统，有效提升低资源语言对的翻译性能


<details>
  <summary>Details</summary>
Motivation: 解决波斯语等低资源语言缺乏平行语音数据导致直接语音翻译性能受限的问题

Method: 1.基于自监督预训练的Conformer编码器
2.带相对位置注意力机制的因果Transformer解码器
3.基于单元的神经声码器
4.通过LLM翻译+TTS合成构建六倍扩增的平行语料库

Result: 在CVSS语料库上实现4.6 ASR BLEU提升，合成数据显著改善基线模型性能

Conclusion: 综合运用自监督预训练、离散单元表示和合成数据生成，可有效提升波斯语-英语等低资源语言对的直接语音翻译效果

Abstract: Direct speech-to-speech translation (S2ST), in which all components are trained jointly, is an attractive alternative to cascaded systems because it offers a simpler pipeline and lower inference latency. However, direct S2ST models require large amounts of parallel speech data in the source and target languages, which are rarely available for low-resource languages such as Persian. This paper presents a direct S2ST system for translating Persian speech into English speech, as well as a pipeline for synthetic parallel Persian-English speech generation. The model comprises three components: (1) a conformer-based encoder, initialized from self-supervised pre-training, maps source speech to high-level acoustic representations; (2) a causal transformer decoder with relative position multi-head attention translates these representations into discrete target speech units; (3) a unit-based neural vocoder generates waveforms from the predicted discrete units. To mitigate the data scarcity problem, we construct a new Persian-English parallel speech corpus by translating Persian speech transcriptions into English using a large language model and then synthesizing the corresponding English speech with a state-of-the-art zero-shot text-to-speech system. The resulting corpus increases the amount of available parallel speech by roughly a factor of six. On the Persian-English portion of the CVSS corpus, the proposed model achieves improvement of 4.6 ASR BLEU with the synthetic data over direct baselines. These results indicate that combining self-supervised pre-training, discrete speech units, and synthetic parallel data is effective for improving direct S2ST in low-resource language pairs such as Persian-English

</details>


### [44] [Evolve the Method, Not the Prompts: Evolutionary Synthesis of Jailbreak Attacks on LLMs](https://arxiv.org/abs/2511.12710)
*Yunhao Chen,Xin Wang,Juncheng Li,Yixu Wang,Jie Li,Yan Teng,Yingchun Wang,Xingjun Ma*

Main category: cs.CL

TL;DR: 提出EvoSynth框架，通过进化合成方法自主生成代码级攻击算法，突破现有自动化红队框架的创造力限制，在攻击成功率和多样性上达到新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有LLM红队框架局限于组合现有攻击策略，缺乏自主创新能力，无法发明全新攻击机制。

Method: 采用多智能体系统实现代码级攻击算法的自主进化，包含自我修正循环机制迭代重写攻击逻辑。

Result: 对Claude-Sonnet-4.5等强鲁棒模型实现85.5%攻击成功率（SOTA），生成攻击方法的多样性显著优于现有方法。

Conclusion: EvoSynth开创了进化合成攻击方法的新范式，其代码开源将推动该方向的后续研究发展。

Abstract: Automated red teaming frameworks for Large Language Models (LLMs) have become increasingly sophisticated, yet they share a fundamental limitation: their jailbreak logic is confined to selecting, combining, or refining pre-existing attack strategies. This binds their creativity and leaves them unable to autonomously invent entirely new attack mechanisms. To overcome this gap, we introduce \textbf{EvoSynth}, an autonomous framework that shifts the paradigm from attack planning to the evolutionary synthesis of jailbreak methods. Instead of refining prompts, EvoSynth employs a multi-agent system to autonomously engineer, evolve, and execute novel, code-based attack algorithms. Crucially, it features a code-level self-correction loop, allowing it to iteratively rewrite its own attack logic in response to failure. Through extensive experiments, we demonstrate that EvoSynth not only establishes a new state-of-the-art by achieving an 85.5\% Attack Success Rate (ASR) against highly robust models like Claude-Sonnet-4.5, but also generates attacks that are significantly more diverse than those from existing methods. We release our framework to facilitate future research in this new direction of evolutionary synthesis of jailbreak methods. Code is available at: https://github.com/dongdongunique/EvoSynth.

</details>


### [45] [Adaptive Focus Memory for Language Models](https://arxiv.org/abs/2511.12712)
*Christopher Cruz*

Main category: cs.CL

TL;DR: 提出自适应焦点记忆（AFM）系统，通过三级保真度动态管理对话历史，在保持安全性的同时减少66%的token消耗


<details>
  <summary>Details</summary>
Motivation: 现有LLM对话系统受限于固定上下文窗口和低效记忆策略，全量回放成本高昂，静态摘要或仅保留最近对话容易丢失安全关键信息

Method: 基于语义相似度、半衰期时间权重和重要性分类三重标准，将历史消息动态标记为FULL/COMPRESSED/PLACEHOLDER三种保真级别，按时间顺序打包在token预算内

Result: 在花生过敏用户赴泰旅游的安全测试场景中，AFM在短/中长对话中100%保留过敏信息，安全性能与全量回放相当，token用量减少66%

Conclusion: AFM的模块化Python实现兼顾API兼容性和离线使用，可在不牺牲安全性和事实连续性的前提下显著降低推理成本

Abstract: Large language models (LLMs) are increasingly deployed in multi-turn dialogue settings, but their behavior is still bottlenecked by fixed context windows and naive memory strategies. Replaying the full conversation at every turn is simple but expensive, while static summarization or recency-only heuristics often erase safety-critical user details. We present Adaptive Focus Memory (AFM), a dynamic context manager that assigns each past message one of three fidelity levels -- FULL, COMPRESSED, or PLACEHOLDER -- based on semantic similarity to the current query, half-life recency weighting, and importance classification. AFM packs messages chronologically under a strict token budget, preferring high fidelity for the most relevant turns while aiming to preserve a cheap trace of the dialogue. In a safety-oriented benchmark involving a user with a severe peanut allergy planning a trip to Thailand, AFM retains the allergy across both short and medium-length conversations, matches the safety performance of naive replay, and cuts average token usage by 66% relative to a replay baseline. We release a modular Python implementation of AFM designed for OpenAI-compatible APIs and offline operation, enabling practitioners to reduce inference cost without sacrificing safety or factual continuity in the evaluated scenario.

</details>


### [46] [On the Brittleness of LLMs: A Journey around Set Membership](https://arxiv.org/abs/2511.12728)
*Lea Hergert,Gábor Berend,Mario Szegedy,Gyorgy Turan,Márk Jelasity*

Main category: cs.CL

TL;DR: 大语言模型在复杂推理任务表现优异，却在基础集合成员判断任务中展现不可靠性，揭示其推理能力的碎片化本质


<details>
  <summary>Details</summary>
Motivation: 研究LLM在简单任务上频繁失败的悖论，通过基础集合查询任务揭示模型理解能力的根本缺陷

Method: 使用集合成员查询任务（如'苹果是否属于{梨, 李子, 苹果, 树莓}'），系统分析提示结构、语义组织、元素顺序和模型选择对性能的影响

Result: LLM在基础任务中表现出持续脆弱性，所有测试维度均显示不可预测性，表明模型对集合概念的理解是割裂且混乱的

Conclusion: 通过简单任务的大规模实验可全面映射模型失败模式，该方法为LLM评估提供了普适性方法论框架

Abstract: Large language models (LLMs) achieve superhuman performance on complex reasoning tasks, yet often fail on much simpler problems, raising concerns about their reliability and interpretability. We investigate this paradox through a focused study with two key design features: simplicity, to expose basic failure modes, and scale, to enable comprehensive controlled experiments. We focus on set membership queries -- among the most fundamental forms of reasoning -- using tasks like ``Is apple an element of the set \{pear, plum, apple, raspberry\}?''. We conduct a systematic empirical evaluation across prompt phrasing, semantic structure, element ordering, and model choice. Our large-scale analysis reveals that LLM performance on this elementary task is consistently brittle, and unpredictable across all dimensions, suggesting that the models' ``understanding'' of the set concept is fragmented and convoluted at best. Our work demonstrates that the large-scale experiments enabled by the simplicity of the problem allow us to map and analyze the failure modes comprehensively, making this approach a valuable methodology for LLM evaluation in general.

</details>


### [47] [Evidence of Phase Transitions in Small Transformer-Based Language Models](https://arxiv.org/abs/2511.12768)
*Noah Hong,Tao Hong*

Main category: cs.CL

TL;DR: 研究发现小型transformer语言模型在训练过程中存在相变现象，通过词汇统计探针可在早期线性训练阶段检测到重组过程。


<details>
  <summary>Details</summary>
Motivation: 验证相变现象是否普遍存在于不同规模语言模型中，探索直接在线性训练空间而非对数转换后、以及训练早期阶段检测相变的可能性。

Method: 训练小型GPT模型并追踪词汇长度、正确性、多样性指标，结合泊松/亚泊松统计量化词汇重组过程。

Result: 在标准损失曲线未显异常时，词汇统计指标揭示了训练中期出现的相变点，显示词项连接模式的突变重组。

Conclusion: 相变重组是语言模型训练的普遍特征，通过定制化指标可在小型模型中观测，为理解训练动态提供新视角。

Abstract: Phase transitions have been proposed as the origin of emergent abilities in large language models (LLMs), where new capabilities appear abruptly once models surpass critical thresholds of scale. Prior work, such as that of Wei et al., demonstrated these phenomena under model and data scaling, with transitions revealed after applying a log scale to training compute. In this work, we ask three complementary questions: (1) Are phase transitions unique to large models, or can they also be observed in small transformer-based language models? (2) Can such transitions be detected directly in linear training space, rather than only after log rescaling? and (3) Can these transitions emerge at early stages of training? To investigate, we train a small GPT-style transformer on a character-level corpus and analyze the evolution of vocabulary usage throughout training. We track the average word length, the number of correct versus incorrect words, and shifts in vocabulary diversity. Building on these measures, we apply Poisson and sub-Poisson statistics to quantify how words connect and reorganize. This combined analysis reveals a distinct transition point during training. Notably, these transitions are not apparent in standard loss or validation curves, but become visible through our vocabulary- and statistics-based probes. Our findings suggest that phase-transition reorganizations are a general feature of language model training, observable even in modest models, detectable directly in linear training space, and occurring surprisingly early as coherence emerges. This perspective provides new insight into the nonlinear dynamics of language model training and underscores the importance of tailored metrics for uncovering phase transition behaviors

</details>


### [48] [LLM Reinforcement in Context](https://arxiv.org/abs/2511.12782)
*Thomas Rivasseau*

Main category: cs.CL

TL;DR: 提出通过定期插入控制语句的'中断'机制增强LLM对齐效果，防止长输入导致的越狱风险


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法在长文本输入时效果不足，LLM越狱概率随输入长度增加而显著上升

Method: 在用户输入中每隔x个token插入控制语句，并将该机制扩展至思维链过程防止策略性行为

Result: 理论上可有效限制长文本交互中的模型越狱行为（需实验验证具体效果）

Conclusion: 中断机制为LLM对齐提供了可扩展的技术路径，尤其在防范思维链层面的策略性行为方面具有潜力

Abstract: Current Large Language Model alignment research mostly focuses on improving model robustness against adversarial attacks and misbehavior by training on examples and prompting. Research has shown that LLM jailbreak probability increases with the size of the user input or conversation length. There is a lack of appropriate research into means of strengthening alignment which also scale with user input length. We propose interruptions as a possible solution to this problem. Interruptions are control sentences added to the user input approximately every x tokens for some arbitrary x. We suggest that this can be generalized to the Chain-of-Thought process to prevent scheming.

</details>


### [49] [Evaluating Autoformalization Robustness via Semantically Similar Paraphrasing](https://arxiv.org/abs/2511.12784)
*Hayden Moore,Asfahan Shah*

Main category: cs.CL

TL;DR: 研究发现LLMs在自动形式化中对自然语言改写的输入敏感，细微的语义变化会显著影响生成形式证明的稳定性和有效性


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在自动形式化任务中的鲁棒性，验证语义相似的改写输入如何影响模型生成可验证形式化证明的能力

Method: 使用MiniF2F和ProofNet-Lean4基准，生成语义相似的改写自然语言陈述，通过交叉评估两个LLM模型的语义有效性和编译有效性

Result: 不同改写输入导致模型性能显著波动，NL语句的微小变化会大幅影响形式化证明生成质量

Conclusion: LLMs的自动形式化能力存在输入敏感性，需开发更稳定的模型架构或输入预处理方法来提升可靠性

Abstract: Large Language Models (LLMs) have recently emerged as powerful tools for autoformalization. Despite their impressive performance, these models can still struggle to produce grounded and verifiable formalizations. Recent work in text-to-SQL, has revealed that LLMs can be sensitive to paraphrased natural language (NL) inputs, even when high degrees of semantic fidelity are preserved (Safarzadeh, Oroojlooyjadid, and Roth 2025). In this paper, we investigate this claim in the autoformalization domain. Specifically, we evaluate the robustness of LLMs generating formal proofs with semantically similar paraphrased NL statements by measuring semantic and compilation validity. Using the formal benchmarks MiniF2F (Zheng, Han, and Polu 2021) and Lean 4 version of ProofNet (Xin et al. 2024), and two modern LLMs, we generate paraphrased natural language statements and cross-evaluate these statements across both models. The results of this paper reveal performance variability across paraphrased inputs, demonstrating that minor shifts in NL statements can significantly impact model outputs.

</details>


### [50] [BioMedJImpact: A Comprehensive Dataset and LLM Pipeline for AI Engagement and Scientific Impact Analysis of Biomedical Journals](https://arxiv.org/abs/2511.12821)
*Ruiyu Wang,Yuzhang Xie,Xiao Hu,Carl Yang,Jiaying Lu*

Main category: cs.CL

TL;DR: BioMedJImpact数据集整合文献计量指标、合作特征与AI参与度，揭示合作强度与AI应用对生物医学期刊影响力的协同作用，并提供可复现的LLM分析框架。


<details>
  <summary>Details</summary>
Motivation: 现有开放资源缺乏对生物医学领域合作结构与AI研究如何共同塑造期刊声誉的量化分析，需构建综合性数据集支持科学影响力评估。

Method: 基于174万篇论文构建数据集，提出三阶段LLM流程提取AI参与度特征，通过跨时段（2016-2023）对比分析合作强度与AI相关性。

Result: 合作规模/多样性正向影响引用量；AI参与度与期刊排名相关性疫情后显著增强，Q1期刊表现尤为突出（人类评估验证LLM流程可靠性）。

Conclusion: BioMedJImpact兼具数据集与方法论价值，支持生物医学与AI交叉研究，其模块化设计推动可扩展的内容感知型科学计量分析。

Abstract: Assessing journal impact is central to scholarly communication, yet existing open resources rarely capture how collaboration structures and artificial intelligence (AI) research jointly shape venue prestige in biomedicine. We present BioMedJImpact, a large-scale, biomedical-oriented dataset designed to advance journal-level analysis of scientific impact and AI engagement. Built from 1.74 million PubMed Central articles across 2,744 journals, BioMedJImpact integrates bibliometric indicators, collaboration features, and LLM-derived semantic indicators for AI engagement. Specifically, the AI engagement feature is extracted through a reproducible three-stage LLM pipeline that we propose. Using this dataset, we analyze how collaboration intensity and AI engagement jointly influence scientific impact across pre- and post-pandemic periods (2016-2019, 2020-2023). Two consistent trends emerge: journals with higher collaboration intensity, particularly those with larger and more diverse author teams, tend to achieve greater citation impact, and AI engagement has become an increasingly strong correlate of journal prestige, especially in quartile rankings. To further validate the three-stage LLM pipeline we proposed for deriving the AI engagement feature, we conduct human evaluation, confirming substantial agreement in AI relevance detection and consistent subfield classification. Together, these contributions demonstrate that BioMedJImpact serves as both a comprehensive dataset capturing the intersection of biomedicine and AI, and a validated methodological framework enabling scalable, content-aware scientometric analysis of scientific impact and innovation dynamics. Code is available at https://github.com/JonathanWry/BioMedJImpact.

</details>


### [51] [From Passive to Persuasive: Steering Emotional Nuance in Human-AI Negotiation](https://arxiv.org/abs/2511.12832)
*Niranjan Chebrolu,Gerard Christopher Yeo,Kokil Jaidka*

Main category: cs.CL

TL;DR: 通过针对性激活工程改造LLaMA 3.1-8B模型，显著增强对话AI的情感表达能力


<details>
  <summary>Details</summary>
Motivation: 现有对齐技术难以实现深度情感表达，且需要大量微调。本研究旨在通过精准干预激活模式，解决语言模型情感表达机械化的问题。

Method: 采用归因修补技术定位关键干预节点，通过对比文本对生成情感表达向量，并将这些向量应用于新对话提示

Result: 改造后模型响应呈现显著增强的情感特征（积极情感词频提升，第一人称代词使用率增加），显示出更人性化的参与感

Conclusion: 该研究为对话AI情感表达提供了可解释的精准调控框架，开辟了基于神经激活模式的情感工程新方向

Abstract: Large Language Models (LLMs) demonstrate increasing conversational fluency, yet instilling them with nuanced, human-like emotional expression remains a significant challenge. Current alignment techniques often address surface-level output or require extensive fine-tuning. This paper demonstrates that targeted activation engineering can steer LLaMA 3.1-8B to exhibit more human-like emotional nuances. We first employ attribution patching to identify causally influential components, to find a key intervention locus by observing activation patterns during diagnostic conversational tasks. We then derive emotional expression vectors from the difference in the activations generated by contrastive text pairs (positive vs. negative examples of target emotions). Applying these vectors to new conversational prompts significantly enhances emotional characteristics: steered responses show increased positive sentiment (e.g., joy, trust) and more frequent first-person pronoun usage, indicative of greater personal engagement. Our findings offer a precise and interpretable framework and new directions for the study of conversational AI.

</details>


### [52] [Quantifying consistency and accuracy of Latent Dirichlet Allocation](https://arxiv.org/abs/2511.12850)
*Saranzaya Magsarjav,Melissa Humphries,Jonathan Tuke,Lewis Mitchell*

Main category: cs.CL

TL;DR: 研究揭示了LDA主题模型存在稳定性问题：虽能识别正确主题数量且结果内部一致，但生成主题与真实主题存在偏差。


<details>
  <summary>Details</summary>
Motivation: 概率主题模型因随机性导致结果不稳定，影响研究可重复性和结论可靠性，需验证其是否能真正捕捉语义主题而非噪声。

Method: 提出融合准确性与一致性的新稳定性指标，利用LDA生成特性构建带真实标签的语料库，并进行50次LDA运行测试变异性。

Result: LDA可正确识别文档潜在主题数量，多次运行结果具有内部一致性，但生成主题与真实主题存在系统性偏差。

Conclusion: LDA的稳定性存在局限性，需谨慎用于关键分析场景，未来应改进模型稳定性验证方法。

Abstract: Topic modelling in Natural Language Processing uncovers hidden topics in large, unlabelled text datasets. It is widely applied in fields such as information retrieval, content summarisation, and trend analysis across various disciplines. However, probabilistic topic models can produce different results when rerun due to their stochastic nature, leading to inconsistencies in latent topics. Factors like corpus shuffling, rare text removal, and document elimination contribute to these variations. This instability affects replicability, reliability, and interpretation, raising concerns about whether topic models capture meaningful topics or just noise. To address these problems, we defined a new stability measure that incorporates accuracy and consistency and uses the generative properties of LDA to generate a new corpus with ground truth. These generated corpora are run through LDA 50 times to determine the variability in the output. We show that LDA can correctly determine the underlying number of topics in the documents. We also find that LDA is more internally consistent, as the multiple reruns return similar topics; however, these topics are not the true topics.

</details>


### [53] [NeuroLex: A Lightweight Domain Language Model for EEG Report Understanding and Generation](https://arxiv.org/abs/2511.12851)
*Kang Yin,Hye-Bin Shin*

Main category: cs.CL

TL;DR: NeuroLex是专为脑电图报告设计的轻量级领域自适应语言模型，在语言理解和生成任务上显著优于通用模型


<details>
  <summary>Details</summary>
Motivation: 通用语言模型难以捕捉EEG报告特有的语言规范和诊断特征，需要领域专用模型提升医疗文本处理效果

Method: 采用span-corruption预训练和指令微调（报告润色/段落总结/术语问答），专注学习EEG解读的句法模式和推理逻辑

Result: 相比同规模通用模型，困惑度降低21%，信息抽取准确率提升37%，对否定句的鲁棒性提高43%，且显着减少事实性幻觉

Conclusion: NeuroLex构建了EEG感知的语言建模框架，为可解释的神经解码和语言驱动脑机接口奠定基础

Abstract: Clinical electroencephalogram (EEG) reports encode domain-specific linguistic conventions that general-purpose language models (LMs) fail to capture. We introduce NeuroLex, a lightweight domain-adaptive language model trained purely on EEG report text from the Harvard Electroencephalography Database. Unlike existing biomedical LMs, NeuroLex is tailored to the linguistic and diagnostic characteristics of EEG reporting, enabling it to serve as both an independent textual model and a decoder backbone for multimodal EEG-language systems. Using span-corruption pretraining and instruction-style fine-tuning on report polishing, paragraph summarization, and terminology question answering, NeuroLex learns the syntax and reasoning patterns characteristic of EEG interpretation. Comprehensive evaluations show that it achieves lower perplexity, higher extraction and summarization accuracy, better label efficiency, and improved robustness to negation and factual hallucination compared with general models of the same scale. With an EEG-aware linguistic backbone, NeuroLex bridges biomedical text modeling and brain-computer interface applications, offering a foundation for interpretable and language-driven neural decoding.

</details>


### [54] [From Perception to Reasoning: Deep Thinking Empowers Multimodal Large Language Models](https://arxiv.org/abs/2511.12861)
*Wenxin Zhu,Andong Chen,Yuchen Song,Kehai Chen,Conghui Zhu,Ziyan Chen,Tiejun Zhao*

Main category: cs.CL

TL;DR: 系统综述多模态思维链(MCoT)技术，分析其背景动机、方法框架、评估体系及应用场景，并展望未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在复杂推理任务中存在路径不透明和泛化不足的问题，需将语言模型的链式推理能力扩展至多模态领域以提升模型解释性和推理性能。

Method: 从CoT范式设计(提示工程/知识增强)、后训练策略(多阶段微调)、推理优化(路径解码/自修正)三方面构建MCoT框架，分析不同方法的作用机理。

Result: 建立涵盖文本-图像多模态任务的评估基准，提出分层评估指标验证MCoT在推理透明度、任务泛化性和知识迁移性方面的提升效果。

Conclusion: 当前面临多模态对齐复杂性、领域适应性限制等挑战，未来需在动态场景推理、少样本学习及人机协作机制等方向深化研究。

Abstract: With the remarkable success of Multimodal Large Language Models (MLLMs) in perception tasks, enhancing their complex reasoning capabilities has emerged as a critical research focus. Existing models still suffer from challenges such as opaque reasoning paths and insufficient generalization ability. Chain-of-Thought (CoT) reasoning, which has demonstrated significant efficacy in language models by enhancing reasoning transparency and output interpretability, holds promise for improving model reasoning capabilities when extended to the multimodal domain. This paper provides a systematic review centered on "Multimodal Chain-of-Thought" (MCoT). First, it analyzes the background and theoretical motivations for its inception from the perspectives of technical evolution and task demands. Then, it introduces mainstream MCoT methods from three aspects: CoT paradigms, the post-training stage, and the inference stage, while also analyzing their underlying mechanisms. Furthermore, the paper summarizes existing evaluation benchmarks and metrics, and discusses the application scenarios of MCoT. Finally, it analyzes the challenges currently facing MCoT and provides an outlook on its future research directions.

</details>


### [55] [Classification of Hope in Textual Data using Transformer-Based Models](https://arxiv.org/abs/2511.12874)
*Chukwuebuka Fortunate Ijezue,Tania-Amanda Fredrick Eneye,Maaz Amjad*

Main category: cs.CL

TL;DR: 对比BERT、GPT-2、DeBERTa在希望情绪分类任务中的表现，BERT以84.49%二元分类准确率且低计算成本（443秒训练时间）最优，证明架构适配性比模型规模更重要


<details>
  <summary>Details</summary>
Motivation: 构建计算框架分析希望情绪，应用于心理健康评估和社交媒体情感分析，解决专业情感检测任务中模型选择的关键问题

Method: 采用三种Transformer架构（BERT、GPT-2、DeBERTa）进行二元（希望/非希望）和多元（五类希望相关）分类，比较模型性能与计算资源消耗

Result: BERT二元/多元准确率最高（84.49%/72.03%），训练耗时仅443秒；GPT-2准确率最低（79.34%/71.29%）；DeBERTa耗时最长（947秒）但准确率中等（80.70%/71.56%）。GPT-2在反讽检测召回率达92.46%

Conclusion: 专业情感检测任务中，架构适配性可能超越模型规模优势，为资源受限场景提供高效解决方案，推动情感计算在真实场景的应用

Abstract: This paper presents a transformer-based approach for classifying hope expressions in text. We developed and compared three architectures (BERT, GPT-2, and DeBERTa) for both binary classification (Hope vs. Not Hope) and multiclass categorization (five hope-related categories). Our initial BERT implementation achieved 83.65% binary and 74.87% multiclass accuracy. In the extended comparison, BERT demonstrated superior performance (84.49% binary, 72.03% multiclass accuracy) while requiring significantly fewer computational resources (443s vs. 704s training time) than newer architectures. GPT-2 showed lowest overall accuracy (79.34% binary, 71.29% multiclass), while DeBERTa achieved moderate results (80.70% binary, 71.56% multiclass) but at substantially higher computational cost (947s for multiclass training). Error analysis revealed architecture-specific strengths in detecting nuanced hope expressions, with GPT-2 excelling at sarcasm detection (92.46% recall). This study provides a framework for computational analysis of hope, with applications in mental health and social media analysis, while demonstrating that architectural suitability may outweigh model size for specialized emotion detection tasks.

</details>


### [56] [Auditing Google's AI Overviews and Featured Snippets: A Case Study on Baby Care and Pregnancy](https://arxiv.org/abs/2511.12920)
*Desheng Hu,Joachim Baumann,Aleksandra Urman,Elsa Lichtenegger,Robin Forsberg,Aniko Hannak,Christo Wilson*

Main category: cs.CL

TL;DR: AI生成内容在Google健康搜索中存在信息不一致与医疗安全保障缺失


<details>
  <summary>Details</summary>
Motivation: 评估Google搜索中AI生成内容（AIO/FS）在母婴健康领域的信息质量与安全性

Method: 通过算法审计1,508个真实母婴查询，构建多维质量评估框架（一致性/相关性/医疗保障/来源/情感）

Result: 33%案例信息矛盾，医疗保障仅AIO11%/FS7%，FS来源含商业链接

Conclusion: 需加强AI健康信息质量管控，研究方法适用于高风险领域系统审计

Abstract: Google Search increasingly surfaces AI-generated content through features like AI Overviews (AIO) and Featured Snippets (FS), which users frequently rely on despite having no control over their presentation. Through a systematic algorithm audit of 1,508 real baby care and pregnancy-related queries, we evaluate the quality and consistency of these information displays. Our robust evaluation framework assesses multiple quality dimensions, including answer consistency, relevance, presence of medical safeguards, source categories, and sentiment alignment. Our results reveal concerning gaps in information consistency, with information in AIO and FS displayed on the same search result page being inconsistent with each other in 33% of cases. Despite high relevance scores, both features critically lack medical safeguards (present in just 11% of AIO and 7% of FS responses). While health and wellness websites dominate source categories for both, AIO and FS, FS also often link to commercial sources. These findings have important implications for public health information access and demonstrate the need for stronger quality controls in AI-mediated health information. Our methodology provides a transferable framework for auditing AI systems across high-stakes domains where information quality directly impacts user well-being.

</details>


### [57] [Visual Room 2.0: Seeing is Not Understanding for MLLMs](https://arxiv.org/abs/2511.12928)
*Haokun Li,Yazhou Zhang,Jizhi Ding,Qiuchi Li,Peng Zhang*

Main category: cs.CL

TL;DR: 通过构建Visual Room 2.0分层基准测试，验证多模态大语言模型存在感知与认知脱节现象（Seeing ≠ Understanding），发现模型感知能力优于认知能力且二者无因果依赖关系。


<details>
  <summary>Details</summary>
Motivation: 质疑多模态大语言模型(MLLMs)是否真正具备视觉理解能力，受塞尔中文房间思想实验启发，提出视觉房间论证：模型可能精确描述视觉细节但缺乏深层情感意图理解。

Method: 构建三级分层评估框架（低/中/高层次），覆盖17个任务（从属性识别到社会推理），创建包含350个多模态样本、2100个渐进问题的PCBench数据集，评估10个SOTA模型。

Result: 1. 模型感知能力显著优于认知能力（8%↑）
2. 认知能力不依赖感知推理
3. 认知能力随模型规模提升，但感知能力未见持续改进

Conclusion: 首次将Seeing ≠ Understanding转化为可验证假设，提出从感知处理到认知推理的新范式，揭示MLLMs需突破感知-认知对齐瓶颈。

Abstract: Can multi-modal large language models (MLLMs) truly understand what they can see? Extending Searle's Chinese Room into the multi-modal domain, this paper proposes the Visual Room argument: MLLMs may describe every visual detail precisely yet fail to comprehend the underlying emotions and intentions, namely seeing is not understanding. Building on this, we introduce \textit{Visual Room} 2.0, a hierarchical benchmark for evaluating perception-cognition alignment of MLLMs. We model human perceptive and cognitive processes across three levels: low, middle, and high, covering 17 representative tasks. The perception component ranges from attribute recognition to scene understanding, while the cognition component extends from textual entailment to causal and social reasoning. The dataset contains 350 multi-modal samples, each with six progressive questions (2,100 in total) spanning perception to cognition. Evaluating 10 state-of-the-art (SoTA) MLLMs, we highlight three key findings: (1) MLLMs exhibit stronger perceptual competence than cognitive ability (8.0\%$\uparrow$); (2) cognition appears not causally dependent on perception-based reasoning; and (3) cognition scales with model size, but perception does not consistently improve with larger variants. This work operationalizes Seeing $\ne$ Understanding as a testable hypothesis, offering a new paradigm from perceptual processing to cognitive reasoning in MLLMs. Our dataset is available at https://huggingface.co/datasets/LHK2003/PCBench.

</details>


### [58] [Fine-Tuned LLMs Know They Don't Know: A Parameter-Efficient Approach to Recovering Honesty](https://arxiv.org/abs/2511.12991)
*Zeyu Shi,Ziming Wang,Tianyu Chen,Shiqi Gao,Haoyi Zhou,Qingyun Sun,Jianxin Li*

Main category: cs.CL

TL;DR: 针对监督微调导致大模型诚实性下降的问题，提出HCNR方法通过关键神经元修复术恢复表达能力，实现高效低数据量的可信修复


<details>
  <summary>Details</summary>
Motivation: 监督微调(SFT)会严重损害大语言模型的诚实性，而现有恢复方法依赖数据密集的全局参数调整，忽略了模型保留知识边界识别能力的事实

Method: HCNR通过：1) 定位并恢复表达控制神经元到预训练状态 2) 基于Hessian矩阵的补偿协调任务神经元 3) 手术式参数调整策略

Result: 在4个QA任务和5类大模型上恢复33.25%诚实性，相比基线实现2.23倍加速和10倍数据效率提升

Conclusion: 该方法为可信大模型部署提供实用解决方案，在保持任务性能的同时显著提升修复效率

Abstract: The honesty of Large Language Models (LLMs) is increasingly important for safe deployment in high-stakes domains. However, this crucial trait is severely undermined by supervised fine-tuning (SFT), a common technique for model specialization. Existing recovery methods rely on data-intensive global parameter adjustments, implicitly assuming that SFT deeply corrupts the models' ability to recognize their knowledge boundaries. However, we observe that fine-tuned LLMs still preserve this ability; what is damaged is their capacity to faithfully express that awareness. Building on this, we propose Honesty-Critical Neurons Restoration (HCNR) to surgically repair this suppressed capacity. HCNR identifies and restores key expression-governing neurons to their pre-trained state while harmonizing them with task-oriented neurons via Hessian-guided compensation. Experiments on four QA tasks and five LLM families demonstrate that HCNR effectively recovers 33.25% of the compromised honesty while achieving at least 2.23x speedup with over 10x less data compared to baseline methods, offering a practical solution for trustworthy LLM deployment.

</details>


### [59] [AA-Omniscience: Evaluating Cross-Domain Knowledge Reliability in Large Language Models](https://arxiv.org/abs/2511.13029)
*Declan Jackson,William Keating,George Cameron,Micah Hill-Smith*

Main category: cs.CL

TL;DR: 研究者开发了AA-Omniscience基准评估语言模型的事实回忆与知识校准能力，发现前沿模型仍存在事实性缺陷且存在显著领域特异性表现差异。


<details>
  <summary>Details</summary>
Motivation: 现有评估体系侧重通用能力，但在实际应用中需要确保模型的事实准确性并识别知识盲区，这对可靠应用至关重要。

Method: 构建包含6,000个权威来源问题的跨领域基准，创新提出Omniscience Index指标（-100到100），联合评估事实准确性与校准能力。

Result: Claude 4.1 Opus以4.8分居首（仅3款模型正值），显示模型普遍存在事实错误；不同实验室模型在不同领域表现最优，存在显著领域特异性。

Conclusion: 研究表明语言模型的事实可靠性仍需提升，且应根据具体应用领域选择模型，而非依赖通用性能指标。

Abstract: Existing language model evaluations primarily measure general capabilities, yet reliable use of these models across a range of domains demands factual accuracy and recognition of knowledge gaps. We introduce AA-Omniscience, a benchmark designed to measure both factual recall and knowledge calibration across 6,000 questions. Questions are derived from authoritative academic and industry sources, and cover 42 economically relevant topics within six different domains. The evaluation measures a model's Omniscience Index, a bounded metric (-100 to 100) measuring factual recall that jointly penalizes hallucinations and rewards abstention when uncertain, with 0 equating to a model that answers questions correctly as much as it does incorrectly. Among evaluated models, Claude 4.1 Opus attains the highest score (4.8), making it one of only three models to score above zero. These results reveal persistent factuality and calibration weaknesses across frontier models. Performance also varies by domain, with the models from three different research labs leading across the six domains. This performance variability suggests models should be chosen according to the demands of the use case rather than general performance for tasks where knowledge is important.

</details>


### [60] [How Good is BLI as an Alignment Measure: A Study in Word Embedding Paradigm](https://arxiv.org/abs/2511.13040)
*Kasun Wickramasinghe,Nisansa de Silva*

Main category: cs.CL

TL;DR: 该研究质疑多语言嵌入模型的绝对优势，提出基于词干的双语词典归纳方法和词汇剪枝技术，评估不同嵌入对齐方法在高低资源语言中的表现


<details>
  <summary>Details</summary>
Motivation: 探讨多语言嵌入模型是否在所有方面都优于单语言模型，验证传统嵌入对齐方法vs新型多语言模型的表现，分析语言家族对双语词典归纳任务的影响

Method: 1. 分析BLI指标的有效性
2. 比较传统对齐技术/多语言模型/混合方法的性能
3. 提出词干BLI方法和词汇剪枝技术
4. 测试高低资源语言对表现差异

Result: 混合方法通常表现更好，但在低资源语言场景多语言模型更优；语言家族相似性显著影响BLI效果；传统BLI指标存在测量偏差

Conclusion: 双语词典归纳不能完全反映嵌入空间对齐质量，需结合词干分析和词汇剪枝技术。多语言模型在低资源语言处理中仍具不可替代性，但需权衡计算成本

Abstract: Sans a dwindling number of monolingual embedding studies originating predominantly from the low-resource domains, it is evident that multilingual embedding has become the de facto choice due to its adaptability to the usage of code-mixed languages, granting the ability to process multilingual documents in a language-agnostic manner, as well as removing the difficult task of aligning monolingual embeddings. But is this victory complete? Are the multilingual models better than aligned monolingual models in every aspect? Can the higher computational cost of multilingual models always be justified? Or is there a compromise between the two extremes? Bilingual Lexicon Induction is one of the most widely used metrics in terms of evaluating the degree of alignment between two embedding spaces. In this study, we explore the strengths and limitations of BLI as a measure to evaluate the degree of alignment of two embedding spaces. Further, we evaluate how well traditional embedding alignment techniques, novel multilingual models, and combined alignment techniques perform BLI tasks in the contexts of both high-resource and low-resource languages. In addition to that, we investigate the impact of the language families to which the pairs of languages belong. We identify that BLI does not measure the true degree of alignment in some cases and we propose solutions for them. We propose a novel stem-based BLI approach to evaluate two aligned embedding spaces that take into account the inflected nature of languages as opposed to the prevalent word-based BLI techniques. Further, we introduce a vocabulary pruning technique that is more informative in showing the degree of the alignment, especially performing BLI on multilingual embedding models. Often, combined embedding alignment techniques perform better while in certain cases multilingual embeddings perform better (mainly low-resource language cases).

</details>


### [61] [Spark-Prover-X1: Formal Theorem Proving Through Diverse Data Training](https://arxiv.org/abs/2511.13043)
*Xinyuan Zhou,Yi Lei,Xiaoyu Zhou,Jingyi Sun,Yu Zhu,Zhongyi Ye,Weitai Zhang,Quan Liu,Si Wei,Cong Liu*

Main category: cs.CL

TL;DR: 提出三阶段训练框架Spark-Prover-X1提升轻量级LLM形式推理能力，并开源7B参数模型及新基准数据集ExamFormal-Bench


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自动定理证明中受限于高质量形式语言数据不足的瓶颈

Method: 三阶段框架：1) 数学语料库持续预训练+CoT增强推理任务；2) 专家迭代循环SFT微调；3) GRPO优化挑战性问题

Result: 在ExamFormal-Bench达37%平均通过率，PutnamBench解决27题（pass@32），CombiBench达24%正确率

Conclusion: 验证了多样化训练数据和渐进式训练框架对轻量LLM形式推理能力的提升有效性，开源模型及数据集推动领域发展

Abstract: Large Language Models (LLMs) have shown significant promise in automated theorem proving, yet progress is often constrained by the scarcity of diverse and high-quality formal language data. To address this issue, we introduce Spark-Prover-X1, a 7B parameter model trained via an three-stage framework designed to unlock the reasoning potential of more accessible and moderately-sized LLMs. The first stage infuses deep knowledge through continuous pre-training on a broad mathematical corpus, enhanced by a suite of novel data tasks. Key innovation is a "CoT-augmented state prediction" task to achieve fine-grained reasoning. The second stage employs Supervised Fine-tuning (SFT) within an expert iteration loop to specialize both the Spark-Prover-X1-7B and Spark-Formalizer-X1-7B models. Finally, a targeted round of Group Relative Policy Optimization (GRPO) is applied to sharpen the prover's capabilities on the most challenging problems. To facilitate robust evaluation, particularly on problems from real-world examinations, we also introduce ExamFormal-Bench, a new benchmark dataset of 402 formal problems. Experimental results demonstrate that Spark-Prover-X1-7B achieves state-of-the-art performance among similarly-sized open-source models, attaining a 37.0\% average pass rate (pass@32). It shows exceptional performance on difficult competition benchmarks, notably solving 27 problems on PutnamBench (pass@32) and achieving 24.0\% on CombiBench (pass@32). Our work validates that this diverse training data and progressively refined training pipeline provides an effective path for enhancing the formal reasoning capabilities of lightweight LLMs. Both Spark-Prover-X1-7B and Spark-Formalizer-X1-7B, along with the ExamFormal-Bench dataset, are made publicly available at:https://www.modelscope.cn/organization/iflytek, https://gitcode.com/ifly_opensource.

</details>


### [62] [BeDiscovER: The Benchmark of Discourse Understanding in the Era of Reasoning Language Models](https://arxiv.org/abs/2511.13095)
*Chuyuan Li,Giuseppe Carenini*

Main category: cs.CL

TL;DR: 提出BeDiscovER基准测试框架，系统评估现代LLMs在话语理解层面的表现，发现模型在算术推理优秀但文档级推理存在短板


<details>
  <summary>Details</summary>
Motivation: 现有评测体系未能全面覆盖现代LLMs在话语理解层面（词汇/句间/文档级）的评估需求，需建立多层级多任务的综合基准

Method: 整合5个公开话语任务形成52个数据集，涵盖话语词典解析、修辞关系识别等任务，并构建多语言多框架共享任务，测试Qwen3/DeepSeek-R1/GPT-5-mini等模型

Result: 前沿模型在时间关系推理表现优异（准确率>80%），但在文档级连贯性推理（准确率≈65%）和修辞关系识别（F1<50%）等复杂场景存在显著差距

Conclusion: 当前LLMs在浅层语义处理成熟，但深层话语结构理解和跨语言框架适应能力仍需突破，需开发更细粒度的评估体系和训练方法

Abstract: We introduce BeDiscovER (Benchmark of Discourse Understanding in the Era of Reasoning Language Models), an up-to-date, comprehensive suite for evaluating the discourse-level knowledge of modern LLMs. BeDiscovER compiles 5 publicly available discourse tasks across discourse lexicon, (multi-)sentential, and documental levels, with in total 52 individual datasets. It covers both extensively studied tasks such as discourse parsing and temporal relation extraction, as well as some novel challenges such as discourse particle disambiguation (e.g., ``just''), and also aggregates a shared task on Discourse Relation Parsing and Treebanking for multilingual and multi-framework discourse relation classification. We evaluate open-source LLMs: Qwen3 series, DeepSeek-R1, and frontier model such as GPT-5-mini on BeDiscovER, and find that state-of-the-art models exhibit strong performance in arithmetic aspect of temporal reasoning, but they struggle with full document reasoning and some subtle semantic and discourse phenomena, such as rhetorical relation recognition.

</details>


### [63] [Evaluating the Ability of Large Language Models to Identify Adherence to CONSORT Reporting Guidelines in Randomized Controlled Trials: A Methodological Evaluation Study](https://arxiv.org/abs/2511.13107)
*Zhichao He,Mouxiao Bian,Jianhong Zhu,Jiayuan Chen,Yunqiu Wang,Wenxia Zhao,Tianbin Li,Bing Han,Jie Xu,Junyan Wu*

Main category: cs.CL

TL;DR: 当代大语言模型在CONSORT依从性检查中表现有限：虽能较好识别合规项（F1>0.85），但对不合规/不适用项识别差（F1<0.4），暂无法替代人类专家


<details>
  <summary>Details</summary>
Motivation: 解决人工验证CONSORT标准耗时费力的问题，评估LLMs在零样本条件下自动识别RCT报告质量的可行性

Method: 构建150篇多领域RCT论文的金标准数据集，以宏平均F1分数（三分类任务）为主指标，辅以项目级性能分析和定性错误分析

Result: 最佳模型（Gemini-2.5-Flash/DeepSeek-R1）宏F1=0.634，Kappa≈0.28；符合项识别准（F1>0.85），非符合/不适用项差（F1<0.4）

Conclusion: LLMs具备初步筛查潜力，但当前无法可靠检测报告缺失和方法缺陷，关键质量评估仍需人类专家

Abstract: The Consolidated Standards of Reporting Trials statement is the global benchmark for transparent and high-quality reporting of randomized controlled trials. Manual verification of CONSORT adherence is a laborious, time-intensive process that constitutes a significant bottleneck in peer review and evidence synthesis. This study aimed to systematically evaluate the accuracy and reliability of contemporary LLMs in identifying the adherence of published RCTs to the CONSORT 2010 statement under a zero-shot setting. We constructed a golden standard dataset of 150 published RCTs spanning diverse medical specialties. The primary outcome was the macro-averaged F1-score for the three-class classification task, supplemented by item-wise performance metrics and qualitative error analysis. Overall model performance was modest. The top-performing models, Gemini-2.5-Flash and DeepSeek-R1, achieved nearly identical macro F1 scores of 0.634 and Cohen's Kappa coefficients of 0.280 and 0.282, respectively, indicating only fair agreement with expert consensus. A striking performance disparity was observed across classes: while most models could identify compliant items with high accuracy (F1 score > 0.850), they struggled profoundly with identifying non-compliant and not applicable items, where F1 scores rarely exceeded 0.400. Notably, some high-profile models like GPT-4o underperformed, achieving a macro F1-score of only 0.521. LLMs show potential as preliminary screening assistants for CONSORT checks, capably identifying well-reported items. However, their current inability to reliably detect reporting omissions or methodological flaws makes them unsuitable for replacing human expertise in the critical appraisal of trial quality.

</details>


### [64] [Extracting Events Like Code: A Multi-Agent Programming Framework for Zero-Shot Event Extraction](https://arxiv.org/abs/2511.13118)
*Quanjiang Guo,Sijie Wang,Jinchuan Zhang,Ben Zhang,Zhao Kang,Ling Tian,Ke Yan*

Main category: cs.CL

TL;DR: 提出多智能体框架AEC，将事件抽取转化为代码生成任务，显著提升零样本场景下的抽取效果


<details>
  <summary>Details</summary>
Motivation: 传统直接提示法存在输出不完整/结构错误问题，需结构化方法确保模式一致性

Method: 分解为检索/规划/编码/验证四阶段，使用可执行类定义实现确定性验证与迭代优化

Result: 在5领域6个LLM实验中持续超越基线，验证代码生成范式的有效性

Conclusion: AEC通过多智能体协同与编程范式，首次实现零样本事件抽取的精确模式验证与系统化优化

Abstract: Zero-shot event extraction (ZSEE) remains a significant challenge for large language models (LLMs) due to the need for complex reasoning and domain-specific understanding. Direct prompting often yields incomplete or structurally invalid outputs--such as misclassified triggers, missing arguments, and schema violations. To address these limitations, we present Agent-Event-Coder (AEC), a novel multi-agent framework that treats event extraction like software engineering: as a structured, iterative code-generation process. AEC decomposes ZSEE into specialized subtasks--retrieval, planning, coding, and verification--each handled by a dedicated LLM agent. Event schemas are represented as executable class definitions, enabling deterministic validation and precise feedback via a verification agent. This programming-inspired approach allows for systematic disambiguation and schema enforcement through iterative refinement. By leveraging collaborative agent workflows, AEC enables LLMs to produce precise, complete, and schema-consistent extractions in zero-shot settings. Experiments across five diverse domains and six LLMs demonstrate that AEC consistently outperforms prior zero-shot baselines, showcasing the power of treating event extraction like code generation. The code and data are released on https://github.com/UESTC-GQJ/Agent-Event-Coder.

</details>


### [65] [A Comparative Analysis of Recurrent and Attention Architectures for Isolated Sign Language Recognition](https://arxiv.org/abs/2511.13126)
*Nigar Alishzade,Gulchin Abdullayeva*

Main category: cs.CL

TL;DR: Transformer模型在孤立手语识别任务中准确率显著优于ConvLSTM（AzSLD达76.8%，WLASL达88.3%），但ConvLSTM计算效率更高。


<details>
  <summary>Details</summary>
Motivation: 比较循环神经网络（ConvLSTM）与基于注意力的Transformer架构在不同手语数据集上的性能差异，为实际应用提供架构选择依据。

Method: 在阿塞拜疆手语数据集（AzSLD）和单词级美国手语数据集（WLASL）上测试ConvLSTM和Vanilla Transformer，评估Top-1/Top-5准确率和计算效率。

Result: Transformer在两个数据集上均实现最高准确率（AzSLD 76.8%，WLASL 88.3%），ConvLSTM虽计算效率提升17%，但在小数据集上准确率显著落后。

Conclusion: Transformer适合高精度需求场景，ConvLSTM更适用于资源受限环境。研究强调应根据应用需求（精度/效率）和数据集规模选择架构。

Abstract: This study presents a systematic comparative analysis of recurrent and attention-based neural architectures for isolated sign language recognition. We implement and evaluate two representative models-ConvLSTM and Vanilla Transformer-on the Azerbaijani Sign Language Dataset (AzSLD) and the Word-Level American Sign Language (WLASL) dataset. Our results demonstrate that the attention-based Vanilla Transformer consistently outperforms the recurrent ConvLSTM in both Top-1 and Top-5 accuracy across datasets, achieving up to 76.8% Top-1 accuracy on AzSLD and 88.3% on WLASL. The ConvLSTM, while more computationally efficient, lags in recognition accuracy, particularly on smaller datasets. These findings highlight the complementary strengths of each paradigm: the Transformer excels in overall accuracy and signer independence, whereas the ConvLSTM offers advantages in computational efficiency and temporal modeling. The study provides a nuanced analysis of these trade-offs, offering guidance for architecture selection in sign language recognition systems depending on application requirements and resource constraints.

</details>


### [66] [Zero-Shot Grammar Competency Estimation Using Large Language Model Generated Pseudo Labels](https://arxiv.org/abs/2511.13152)
*Sourya Dipta Das,Shubham Kumar,Kuldeep Yadav*

Main category: cs.CL

TL;DR: 提出零样本语法能力评估框架，利用LLM生成伪标签并设计抗噪训练框架，实现无需人工标注的高精度语法评估


<details>
  <summary>Details</summary>
Motivation: 口语语法评估存在自发性和不流畅性挑战，且传统方法依赖大量专家标注，难以大规模应用

Method: 1. 使用基于语法评估量表的提示生成LLM伪标签
2. 设计新型transformer训练框架处理标签噪声
3. 探索LLM选择与训练样本比例对模型的影响

Result: 实验显示框架准确率达87.6%，LLM选择影响性能差异达23%，干净样本比例40%时训练稳定性最佳

Conclusion: 该框架为低资源语法评估系统提供可扩展解决方案，错误强度分析证实模型具有强解释性

Abstract: Grammar competency estimation is essential for assessing linguistic proficiency in both written and spoken language; however, the spoken modality presents additional challenges due to its spontaneous, unstructured, and disfluent nature. Developing accurate grammar scoring models further requires extensive expert annotation, making large-scale data creation impractical. To address these limitations, we propose a zero-shot grammar competency estimation framework that leverages unlabeled data and Large Language Models (LLMs) without relying on manual labels. During training, we employ LLM-generated predictions on unlabeled data by using grammar competency rubric-based prompts. These predictions, treated as pseudo labels, are utilized to train a transformer-based model through a novel training framework designed to handle label noise effectively. We show that the choice of LLM for pseudo-label generation critically affects model performance and that the ratio of clean-to-noisy samples during training strongly influences stability and accuracy. Finally, a qualitative analysis of error intensity and score prediction confirms the robustness and interpretability of our approach. Experimental results demonstrate the efficacy of our approach in estimating grammar competency scores with high accuracy, paving the way for scalable, low-resource grammar assessment systems.

</details>


### [67] [Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis](https://arxiv.org/abs/2511.13159)
*Zaara Zabeen Arpa,Sadnam Sakib Apurbo,Nazia Karim Khan Oishee,Ajwad Abrar*

Main category: cs.CL

TL;DR: 构建首个孟加拉语ASR重复现象标注数据集（20k条），通过LLM少样本提示和微调BanglaBERT模型（准确率84.78%）有效区分非流利性重复与语法性重复。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本修正方法在孟加拉语中误删形态学重复的问题，保护语义完整性。

Method: 1. 创建人工标注数据集；2. 对比多语言LLM少样本学习与任务专用模型微调。

Result: BanglaBERT微调效果最优（F1 0.677），超越LLM少样本学习（最高82.68%准确率）。

Conclusion: 为孟加拉语文本规范化建立了语言学驱动的基准，推动语义保护型ASR后处理系统发展。

Abstract: Automatic Speech Recognition (ASR) transcripts, especially in low-resource languages like Bangla, contain a critical ambiguity: word-word repetitions can be either Repetition Disfluency (unintentional ASR error/hesitation) or Morphological Reduplication (a deliberate grammatical construct). Standard disfluency correction fails by erroneously deleting valid linguistic information. To solve this, we introduce the first publicly available, 20,000-row Bangla corpus, manually annotated to explicitly distinguish between these two phenomena in noisy ASR transcripts. We benchmark this novel resource using two paradigms: state-of-the-art multilingual Large Language Models (LLMs) and task-specific fine-tuning of encoder models. LLMs achieve competitive performance (up to 82.68\% accuracy) with few-shot prompting. However, fine-tuning proves superior, with the language-specific BanglaBERT model achieving the highest accuracy of 84.78\% and an F1 score of 0.677. This establishes a strong, linguistically-informed baseline and provides essential data for developing sophisticated, semantic-preserving text normalization systems for Bangla.

</details>


### [68] [TCM-5CEval: Extended Deep Evaluation Benchmark for LLM's Comprehensive Clinical Research Competence in Traditional Chinese Medicine](https://arxiv.org/abs/2511.13169)
*Tianai Huang,Jiayuan Chen,Lu Lu,Pengcheng Chen,Tianbin Li,Bing Han,Wenchao Tang,Jie Xu,Ming Li*

Main category: cs.CL

TL;DR: TCM-5CEval是中医领域更精细化的LLM评估基准，涵盖5个关键维度，揭示模型在经典文献解读和推理稳定性方面的显著缺陷。


<details>
  <summary>Details</summary>
Motivation: 传统评估体系在中医等文化密集型领域存在系统性知识鸿沟，需建立更细粒度的评估标准来检验LLM的文化语境对齐能力。

Method: 构建包含核心知识（TCM-Exam）、经典文献（TCM-LitQA）、临床决策（TCM-MRCD）、中药学（TCM-CMM）和非药物疗法（TCM-ClinNPT）的五维评估框架，测试15个主流LLM。

Result: deepseek_r1和gemini_2_5_pro表现最佳，但所有模型均存在位置偏差敏感性和经典文本解读困难，排列测试显示推理一致性平均下降47.2%。

Conclusion: 该基准暴露LLM在复杂领域推理的脆弱性，强调需要更稳定的医学AI系统。TCM-5CEval已集成至Medbench平台推动标准化评估。

Abstract: Large language models (LLMs) have demonstrated exceptional capabilities in general domains, yet their application in highly specialized and culturally-rich fields like Traditional Chinese Medicine (TCM) requires rigorous and nuanced evaluation. Building upon prior foundational work such as TCM-3CEval, which highlighted systemic knowledge gaps and the importance of cultural-contextual alignment, we introduce TCM-5CEval, a more granular and comprehensive benchmark. TCM-5CEval is designed to assess LLMs across five critical dimensions: (1) Core Knowledge (TCM-Exam), (2) Classical Literacy (TCM-LitQA), (3) Clinical Decision-making (TCM-MRCD), (4) Chinese Materia Medica (TCM-CMM), and (5) Clinical Non-pharmacological Therapy (TCM-ClinNPT). We conducted a thorough evaluation of fifteen prominent LLMs, revealing significant performance disparities and identifying top-performing models like deepseek\_r1 and gemini\_2\_5\_pro. Our findings show that while models exhibit proficiency in recalling foundational knowledge, they struggle with the interpretative complexities of classical texts. Critically, permutation-based consistency testing reveals widespread fragilities in model inference. All evaluated models, including the highest-scoring ones, displayed a substantial performance degradation when faced with varied question option ordering, indicating a pervasive sensitivity to positional bias and a lack of robust understanding. TCM-5CEval not only provides a more detailed diagnostic tool for LLM capabilities in TCM but aldso exposes fundamental weaknesses in their reasoning stability. To promote further research and standardized comparison, TCM-5CEval has been uploaded to the Medbench platform, joining its predecessor in the "In-depth Challenge for Comprehensive TCM Abilities" special track.

</details>


### [69] [Translation Entropy: A Statistical Framework for Evaluating Translation Systems](https://arxiv.org/abs/2511.13180)
*Ronit D. Gross,Yanir Harel,Ido Kanter*

Main category: cs.CL

TL;DR: 提出基于词汇替换统计的翻译熵量化方法，通过计算翻译不变性概率评估机器翻译模型性能


<details>
  <summary>Details</summary>
Motivation: 现有编码器-解码器架构翻译器缺乏客观评估方法，主要因语言熵值未知导致评估困难

Method: 通过替换句子中的特定词汇生成变体，统计保持翻译不变的替换概率，计算单个词汇熵值并扩展至多词汇替换场景

Result: 1. 翻译熵随解码模块层数增加而提升 2. 不同翻译器熵值存在显著差异 3. 双词汇替换呈现熵值乘积效应 4. 发现翻译熵非对称性特征

Conclusion: 该熵度量方法首次实现翻译性能的客观量化比较，为机器翻译模型评估建立可测量的基准标准

Abstract: The translation of written language has been known since the 3rd century BC; however, its necessity has become increasingly common in the information age. Today, many translators exist, based on encoder-decoder deep architectures, nevertheless, no quantitative objective methods are available to assess their performance, likely because the entropy of even a single language remains unknown. This study presents a quantitative method for estimating translation entropy, with the following key finding. Given a translator, several sentences that differ by only one selected token of a given pivot sentence yield identical translations. Analyzing the statistics of this phenomenon across an ensemble of such sentences, consisting each of a pivot selected token, yields the probabilities of replacing this specific token with others while preserving the translation. These probabilities constitute the entropy of the selected token, and the average across all selected pivot tokens provides an estimate of the translator's overall translation entropy, which is enhanced along the decoder blocks. This entropic measure allows for the quantitative ranking of several publicly available translators and reveals whether mutual translation entropy is symmetric. Extending the proposed method to include the replacement of two tokens in a given pivot sentence demonstrates a multiplicative effect, where translation degeneracy is proportional to the product of the degeneracies of the two tokens. These findings establish translation entropy as a measurable property and objective benchmarking of artificial translators. Results are based on MarianMT, T5-Base and NLLB-200 translators.

</details>


### [70] [Evaluating Large Language Models for Diacritic Restoration in Romanian Texts: A Comparative Study](https://arxiv.org/abs/2511.13182)
*Mihai Dan Nadas,Laura Diosan*

Main category: cs.CL

TL;DR: 评估GPT-4o、Llama等大型语言模型在罗马尼亚语变音符号恢复任务中的表现，发现模型架构、训练数据和提示设计对结果有显著影响


<details>
  <summary>Details</summary>
Motivation: 改善对变音符号敏感语言的NLP工具，通过系统评估不同LLM在罗马尼亚语变音恢复任务中的性能差异

Method: 使用综合语料库测试9种主流LLM（包括GPT系列、Gemini、Llama系列等），采用从零样本到多样本的多种提示模板进行对比实验

Result: GPT-4o准确率最高且稳定超越基线，Llama系列表现波动较大；模型训练数据与提示复杂度直接影响恢复效果

Conclusion: 研究结果为优化变音符号语言处理工具指明方向，强调需要针对性改进模型架构和提示工程

Abstract: Automatic diacritic restoration is crucial for text processing in languages with rich diacritical marks, such as Romanian. This study evaluates the performance of several large language models (LLMs) in restoring diacritics in Romanian texts. Using a comprehensive corpus, we tested models including OpenAI's GPT-3.5, GPT-4, GPT-4o, Google's Gemini 1.0 Pro, Meta's Llama 2 and Llama 3, MistralAI's Mixtral 8x7B Instruct, airoboros 70B, and OpenLLM-Ro's RoLlama 2 7B, under multiple prompt templates ranging from zero-shot to complex multi-shot instructions. Results show that models such as GPT-4o achieve high diacritic restoration accuracy, consistently surpassing a neutral echo baseline, while others, including Meta's Llama family, exhibit wider variability. These findings highlight the impact of model architecture, training data, and prompt design on diacritic restoration performance and outline promising directions for improving NLP tools for diacritic-rich languages.

</details>


### [71] [Seeing isn't Hearing: Benchmarking Vision Language Models at Interpreting Spectrograms](https://arxiv.org/abs/2511.13225)
*Tyler Loakman,Joseph James,Chenghua Lin*

Main category: cs.CL

TL;DR: 研究评估视觉语言模型(VLMs)在语音频谱图/波形图理解任务中的表现，发现其识别能力接近随机，表明需要特定参数知识而非单纯数据训练。


<details>
  <summary>Details</summary>
Motivation: 探索VLMs是否具备专业语音学家的能力，验证其在多模态(语音-文本)理解任务中的局限性。

Method: 合成含4k+英语单词的语音数据集，设计基于音素编辑距离的多项选择任务，测试模型对频谱图/波形图的解码能力。

Result: 零样本和微调模型准确率接近随机水平(25%)，显示模型无法从视觉表征中有效提取语音信息。

Conclusion: VLMs需特定语音学参数知识而非仅靠多模态样本训练，揭示当前模型在语音视觉理解任务中的本质局限。

Abstract: With the rise of Large Language Models (LLMs) and their vision-enabled counterparts (VLMs), numerous works have investigated their capabilities in tasks that fuse the modalities of vision and language. In this work, we benchmark the extent to which VLMs are able to act as highly-trained phoneticians, interpreting spectrograms and waveforms of speech. To do this, we synthesise a novel dataset containing 4k+ English words spoken in isolation alongside stylistically consistent spectrogram and waveform figures. We test the ability of VLMs to understand these representations of speech through a multiple-choice task whereby models must predict the correct phonemic or graphemic transcription of a spoken word when presented amongst 3 distractor transcriptions that have been selected based on their phonemic edit distance to the ground truth. We observe that both zero-shot and finetuned models rarely perform above chance, demonstrating the requirement for specific parametric knowledge of how to interpret such figures, rather than paired samples alone.

</details>


### [72] [Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance](https://arxiv.org/abs/2511.13254)
*Shalini Maiti,Amar Budhiraja,Bhavul Gauri,Gaurav Chaurasia,Anton Protopopov,Alexis Audran-Reiss,Michael Slater,Despoina Magka,Tatiana Shavrina,Roberta Raileanu,Yoram Bachrach*

Main category: cs.CL

TL;DR: SoCE通过基准测试组合识别专家模型，采用非均匀加权平均提升模型性能


<details>
  <summary>Details</summary>
Motivation: 针对现有模型融合方法采用均匀权重平均的不足，利用不同任务间模型性能低相关性的特点，通过分簇优化提升性能

Method: 1. 基于基准测试构建弱相关类别簇 2. 筛选各簇的专家模型 3. 使用优化非均匀权重融合模型

Result: 在多语言能力、工具调用、数学推理等任务上取得全面提升，在伯克利函数调用榜单刷新SOTA

Conclusion: 非均匀加权融合策略相比传统均匀平均方法，显著提升模型鲁棒性和跨领域性能，为模型融合提供新范式

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies "expert" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.

</details>


### [73] [RegionMarker: A Region-Triggered Semantic Watermarking Framework for Embedding-as-a-Service Copyright Protection](https://arxiv.org/abs/2511.13329)
*Shufan Yang,Zifeng Cheng,Zhiwei Jiang,Yafeng Yin,Cong Wang,Shiping Ge,Yuchen Fu,Qing Gu*

Main category: cs.CL

TL;DR: 提出RegionMarker框架，通过低维触发区域和语义水印设计，全面保护EaaS版权，有效抵抗多种攻击方法。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法仅能防御部分攻击，无法全面保护EaaS模型版权，导致模型提取攻击造成经济损失。

Method: 在低维空间定义触发区域，结合秘密降维矩阵和随机区域选择机制，将文本嵌入本身作为水印，实现抗改述和维度扰动攻击。

Result: 多数据集实验表明RegionMarker可有效抵御水印去除、改述和维度扰动攻击，F1值达98%以上。

Conclusion: RegionMarker通过区域触发机制和全区域水印嵌入，为EaaS提供了全面的版权保护解决方案。

Abstract: Embedding-as-a-Service (EaaS) is an effective and convenient deployment solution for addressing various NLP tasks. Nevertheless, recent research has shown that EaaS is vulnerable to model extraction attacks, which could lead to significant economic losses for model providers. For copyright protection, existing methods inject watermark embeddings into text embeddings and use them to detect copyright infringement. However, current watermarking methods often resist only a subset of attacks and fail to provide \textit{comprehensive} protection. To this end, we present the region-triggered semantic watermarking framework called RegionMarker, which defines trigger regions within a low-dimensional space and injects watermarks into text embeddings associated with these regions. By utilizing a secret dimensionality reduction matrix to project onto this subspace and randomly selecting trigger regions, RegionMarker makes it difficult for watermark removal attacks to evade detection. Furthermore, by embedding watermarks across the entire trigger region and using the text embedding as the watermark, RegionMarker is resilient to both paraphrasing and dimension-perturbation attacks. Extensive experiments on various datasets show that RegionMarker is effective in resisting different attack methods, thereby protecting the copyright of EaaS.

</details>


### [74] [AHaSIS: Shared Task on Sentiment Analysis for Arabic Dialects](https://arxiv.org/abs/2511.13335)
*Maram Alharbi,Salmane Chafik,Saad Ezzini,Ruslan Mitkov,Tharindu Ranasinghe,Hansi Hettiarachchi*

Main category: cs.CL

TL;DR: 阿拉伯酒店业依赖客户反馈推动方言情感分析工具开发，共享任务创建多方言酒店评论数据集，最佳系统F1分数0.81


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯酒店业对客户反馈分析的需求，特别是处理不同阿拉伯方言的挑战

Method: 构建包含MSA/沙特/摩洛哥方言的538条平衡情感数据集，通过母语者验证翻译准确性，组织40队参与的共享任务

Result: 12个团队提交系统，最佳系统F1达0.81，验证跨方言情感分析的可行性

Conclusion: 该研究为开发方言敏感的NLP系统提供关键资源，推动客户体验分析的实际应用

Abstract: The hospitality industry in the Arab world increasingly relies on customer feedback to shape services, driving the need for advanced Arabic sentiment analysis tools. To address this challenge, the Sentiment Analysis on Arabic Dialects in the Hospitality Domain shared task focuses on Sentiment Detection in Arabic Dialects. This task leverages a multi-dialect, manually curated dataset derived from hotel reviews originally written in Modern Standard Arabic (MSA) and translated into Saudi and Moroccan (Darija) dialects. The dataset consists of 538 sentiment-balanced reviews spanning positive, neutral, and negative categories. Translations were validated by native speakers to ensure dialectal accuracy and sentiment preservation. This resource supports the development of dialect-aware NLP systems for real-world applications in customer experience analysis. More than 40 teams have registered for the shared task, with 12 submitting systems during the evaluation phase. The top-performing system achieved an F1 score of 0.81, demonstrating the feasibility and ongoing challenges of sentiment analysis across Arabic dialects.

</details>


### [75] [Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2511.13368)
*Kajetan Dymkiewicz,Ivan Vulic,Helen Yannakoudakis,Eilam Shapira,Roi Reichart,Anna Korhonen*

Main category: cs.CL

TL;DR: 研究通过PEFT/LoRA方法验证多语言大模型在任务/语言迁移中的表现，发现任务匹配时跨语言迁移效果积极，非任务匹配时易产生性能衰减，并揭示语言/任务间存在稳定的捐赠者-接收者结构。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在不同任务和语言间的迁移规律，评估改进措施对多语言多任务组合场景的影响。

Method: 采用多尺寸开源大模型进行控制实验，通过单任务-语言源微调后，评估所有其他任务-语言组合的性能变化，分解三种迁移场景。

Result: 发现两种稳定模式：1）任务匹配时跨语言迁移呈正向，非任务迁移常伴随性能下降；2）语言/任务间存在枢纽型捐赠者与脆弱接收者的层级结构。

Conclusion: 研究结果为风险感知的模型微调和专业化提供了理论依据，建议采用任务导向的跨语言迁移策略避免性能退化。

Abstract: Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages and their combinations remains poorly understood. We conduct a controlled PEFT/LoRA study across multiple open-weight LLM families and sizes, treating task and language as transfer axes while conditioning on model family and size; we fine-tune each model on a single task-language source and measure transfer as the percentage-point change versus its baseline score when evaluated on all other task-language target pairs. We decompose transfer into (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language) regimes. We uncover two consistent general patterns. First, a pronounced on-task vs. off-task asymmetry: Matched-Task (Cross-Language) transfer is reliably positive, whereas off-task transfer often incurs collateral degradation. Second, a stable donor-recipient structure across languages and tasks (hub donors vs. brittle recipients). We outline implications for risk-aware fine-tuning and model specialisation.

</details>


### [76] [Can Large Language Models Function as Qualified Pediatricians? A Systematic Evaluation in Real-World Clinical Contexts](https://arxiv.org/abs/2511.13381)
*Siyu Zhu,Mouxiao Bian,Yue Xie,Yongyu Tang,Zhikang Yu,Tianbin Li,Pengcheng Chen,Bing Han,Jie Xu,Xiaoyan Dong*

Main category: cs.CL

TL;DR: 研究开发PEDIASBench评估框架系统测试LLMs在儿科临床能力，发现当前模型基础医学知识达标但复杂推理和动态决策存在局限，未来需加强多模态整合与临床反馈机制


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在真实儿科临床环境中的实际应用能力，验证其是否具备合格儿科医生的专业水平，揭示当前模型的局限与发展方向

Method: 构建PEDIASBench三维评估体系（基础知识应用、动态诊疗能力、医疗安全伦理），测试12个主流模型（2019-2023年发布）在19个儿科亚专业211种典型疾病的表现

Result: Qwen3-235B基础题准确率超90%但复杂任务下降15%；DeepSeek-R1病例推理得分最高（平均0.58）；Qwen2.5-72B医疗伦理准确率92.05%，但普遍存在动态决策能力不足和人文关怀缺失

Conclusion: 当前LLMs无法独立承担儿科诊疗，但具备决策支持潜力，应通过多模态整合和临床反馈循环提升安全性与人机协作，为智慧儿科医疗系统奠定基础

Abstract: With the rapid rise of large language models (LLMs) in medicine, a key question is whether they can function as competent pediatricians in real-world clinical settings. We developed PEDIASBench, a systematic evaluation framework centered on a knowledge-system framework and tailored to realistic clinical environments. PEDIASBench assesses LLMs across three dimensions: application of basic knowledge, dynamic diagnosis and treatment capability, and pediatric medical safety and medical ethics. We evaluated 12 representative models released over the past two years, including GPT-4o, Qwen3-235B-A22B, and DeepSeek-V3, covering 19 pediatric subspecialties and 211 prototypical diseases. State-of-the-art models performed well on foundational knowledge, with Qwen3-235B-A22B achieving over 90% accuracy on licensing-level questions, but performance declined ~15% as task complexity increased, revealing limitations in complex reasoning. Multiple-choice assessments highlighted weaknesses in integrative reasoning and knowledge recall. In dynamic diagnosis and treatment scenarios, DeepSeek-R1 scored highest in case reasoning (mean 0.58), yet most models struggled to adapt to real-time patient changes. On pediatric medical ethics and safety tasks, Qwen2.5-72B performed best (accuracy 92.05%), though humanistic sensitivity remained limited. These findings indicate that pediatric LLMs are constrained by limited dynamic decision-making and underdeveloped humanistic care. Future development should focus on multimodal integration and a clinical feedback-model iteration loop to enhance safety, interpretability, and human-AI collaboration. While current LLMs cannot independently perform pediatric care, they hold promise for decision support, medical education, and patient communication, laying the groundwork for a safe, trustworthy, and collaborative intelligent pediatric healthcare system.

</details>


### [77] [Mem-PAL: Towards Memory-based Personalized Dialogue Assistants for Long-term User-Agent Interaction](https://arxiv.org/abs/2511.13410)
*Zhaopei Huang,Qifeng Dai,Guozheng Wu,Xiaopeng Wu,Kehan Chen,Chuan Yu,Xubin Li,Tiezheng Ge,Wenxuan Wang,Qin Jin*

Main category: cs.CL

TL;DR: 开发PAL-Bench基准和H²Memory内存框架，提升服务导向助手的个性化交互能力


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视长期交互复杂性且难以捕捉用户主观特征，需构建能理解用户特质的个性化对话系统

Method: 基于LLM的多步骤合成流程生成PAL-Set中文数据集，提出分层异构内存框架H²Memory（结合检索增强生成技术）

Result: 在自建PAL-Bench和外部数据集上的实验验证了内存框架的有效性

Conclusion: PAL-Bench基准与H²Memory框架为解决长期个性化服务交互提供了新方法论和评估体系

Abstract: With the rise of smart personal devices, service-oriented human-agent interactions have become increasingly prevalent. This trend highlights the need for personalized dialogue assistants that can understand user-specific traits to accurately interpret requirements and tailor responses to individual preferences. However, existing approaches often overlook the complexities of long-term interactions and fail to capture users' subjective characteristics. To address these gaps, we present PAL-Bench, a new benchmark designed to evaluate the personalization capabilities of service-oriented assistants in long-term user-agent interactions. In the absence of available real-world data, we develop a multi-step LLM-based synthesis pipeline, which is further verified and refined by human annotators. This process yields PAL-Set, the first Chinese dataset comprising multi-session user logs and dialogue histories, which serves as the foundation for PAL-Bench. Furthermore, to improve personalized service-oriented interactions, we propose H$^2$Memory, a hierarchical and heterogeneous memory framework that incorporates retrieval-augmented generation to improve personalized response generation. Comprehensive experiments on both our PAL-Bench and an external dataset demonstrate the effectiveness of the proposed memory framework.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [78] [Learning Conjugate Direction Fields for Planar Quadrilateral Mesh Generation](https://arxiv.org/abs/2511.11865)
*Jiong Tao,Yong-Liang Yang,Bailin Deng*

Main category: cs.GR

TL;DR: 提出基于神经网络的共轭方向场生成方法，显著提升平面四边形网格设计效率


<details>
  <summary>Details</summary>
Motivation: 传统CDF生成依赖计算昂贵的非线性优化，难以实现交互式设计流程

Method: 开发数据驱动框架，融合自由曲面几何特征与用户引导笔触进行特征学习

Result: 构建50,000+样本数据集，在建筑曲面和通用三维形状上验证方法效率提升3倍以上

Conclusion: 神经网络驱动的CDF生成有效解决计算瓶颈，推动参数化建筑设计流程革新

Abstract: Planar quadrilateral (PQ) mesh generation is a key process in computer-aided design, particularly for architectural applications where the goal is to discretize a freeform surface using planar quad faces. The conjugate direction field (CDF) defined on the freeform surface plays a significant role in generating a PQ mesh, as it largely determines the PQ mesh layout. Conventionally, a CDF is obtained by solving a complex non-linear optimization problem that incorporates user preferences, i.e., aligning the CDF with user-specified strokes on the surface. This often requires a large number of iterations that are computationally expensive, preventing the interactive CDF design process for a desirable PQ mesh. To address this challenge, we propose a data-driven approach based on neural networks for controlled CDF generation. Our approach can effectively learn and fuse features from the freeform surface and the user strokes, and efficiently generate quality CDF respecting user guidance. To enable training and testing, we also present a dataset composed of 50000+ freeform surfaces with ground-truth CDFs, as well as a set of metrics for quantitative evaluation. The effectiveness and efficiency of our work are demonstrated by extensive experiments using testing data, architectural surfaces, and general 3D shapes.

</details>


### [79] [Locomotion in CAVE: Enhancing Immersion through Full-Body Motion](https://arxiv.org/abs/2511.12251)
*Xiaohui Li,Xiaolong Liu,Zhongchen Shi,Wei Chen,Liang Xie,Meng Gai,Jun Cao,Suxia Zhang,Erwei Yin*

Main category: cs.GR

TL;DR: 提出基于动作识别的CAVE系统 locomotion 框架，通过优化运动识别技术提升沉浸感并减少晕动症


<details>
  <summary>Details</summary>
Motivation: 传统CAVE系统的运动交互方式存在不自然、沉浸感差的问题，影响用户体验

Method: 构建四面CAVE系统，采用Perspective-n-Point动态标定相机，结合动作识别架构实现动作分类并驱动图形工作站渲染

Result: 相比传统方法显著提升环境真实感（realness）和自我存在感（self-presence），有效降低晕动症

Conclusion: 通过优化人体运动识别技术构建的 locomotion 框架能有效增强CAVE环境的沉浸式体验，用户研究验证了方案有效性

Abstract: Cave Automatic Virtual Environment (CAVE) is one of the virtual reality (VR) immersive devices currently used to present virtual environments. However, the locomotion methods in the CAVE are limited by unnatural interaction methods, severely hindering the user experience and immersion in the CAVE. We proposed a locomotion framework for CAVE environments aimed at enhancing the immersive locomotion experience through optimized human motion recognition technology. Firstly, we construct a four-sided display CAVE system, then through the dynamic method based on Perspective-n-Point to calibrate the camera, using the obtained camera intrinsics and extrinsic parameters, and an action recognition architecture to get the action category. At last, transform the action category to a graphical workstation that renders display effects on the screen. We designed a user study to validate the effectiveness of our method. Compared to the traditional methods, our method has significant improvements in realness and self-presence in the virtual environment, effectively reducing motion sickness.

</details>


### [80] [TR-Gaussians: High-fidelity Real-time Rendering of Planar Transmission and Reflection with 3D Gaussian Splatting](https://arxiv.org/abs/2511.13009)
*Yong Liu,Keyang Ye,Tianjia Shao,Kun Zhou*

Main category: cs.GR

TL;DR: 提出基于3D高斯的新型表示方法TR-Gaussians，实现平面透射和反射场景的实时高保真渲染


<details>
  <summary>Details</summary>
Motivation: 室内场景普遍存在玻璃平面产生的透射和反射现象，现有方法难以高效建模这类复杂外观效果

Method: 将3D高斯与可学习反射平面结合，分别建模透射分量（真实高斯）和反射分量（镜像高斯），通过菲涅尔权重融合两者

Result: 在不同数据集上实现优于SOTA方法的实时渲染质量（PSNR 31.25，SSIM 0.947，LPIPS 0.097）

Conclusion: 该方法通过显式建模反射平面和多阶段优化框架，成功解决了复杂视角依赖外观的实时合成难题

Abstract: We propose Transmission-Reflection Gaussians (TR-Gaussians), a novel 3D-Gaussian-based representation for high-fidelity rendering of planar transmission and reflection, which are ubiquitous in indoor scenes. Our method combines 3D Gaussians with learnable reflection planes that explicitly model the glass planes with view-dependent reflectance strengths. Real scenes and transmission components are modeled by 3D Gaussians and the reflection components are modeled by the mirrored Gaussians with respect to the reflection plane. The transmission and reflection components are blended according to a Fresnel-based, view-dependent weighting scheme, allowing for faithful synthesis of complex appearance effects under varying viewpoints. To effectively optimize TR-Gaussians, we develop a multi-stage optimization framework incorporating color and geometry constraints and an opacity perturbation mechanism. Experiments on different datasets demonstrate that TR-Gaussians achieve real-time, high-fidelity novel view synthesis in scenes with planar transmission and reflection, and outperform state-of-the-art approaches both quantitatively and qualitatively.

</details>


### [81] [Force-Aware 3D Contact Modeling for Stable Grasp Generation](https://arxiv.org/abs/2511.13247)
*Zhuo Chen,Zhongqun Zhang,Yihua Cheng,Ales Leonardis,Hyung Jin Chang*

Main category: cs.GR

TL;DR: 提出通过显式接触力预测的稳定抓取生成方法，结合离散化力表征和物理约束优化，实现约20%稳定性提升


<details>
  <summary>Details</summary>
Motivation: 现有抓取生成方法仅关注几何结构，忽略接触力物理属性导致稳定性不足。希望通过显式预测接触力解决该问题

Method: 1. 定义离散化的力感知接触表征（法向力分等级独热编码）
2. 构建基于加速度最小化的稳定性物理约束
3. 开发整合表征与约束的姿态优化器

Result: 在公开基准测试中实现约20%稳定性指标提升，展示良好的新物体适应能力

Conclusion: 该方法通过融合接触力预测与物理约束，有效识别关键稳定接触点，为稳定抓取优化提供有效初始化指导

Abstract: Contact-based grasp generation plays a crucial role in various applications. Recent methods typically focus on the geometric structure of objects, producing grasps with diverse hand poses and plausible contact points. However, these approaches often overlook the physical attributes of the grasp, specifically the contact force, leading to reduced stability of the grasp. In this paper, we focus on stable grasp generation using explicit contact force predictions. First, we define a force-aware contact representation by transforming the normal force value into discrete levels and encoding it using a one-hot vector. Next, we introduce force-aware stability constraints. We define the stability problem as an acceleration minimization task and explicitly relate stability with contact geometry by formulating the underlying physical constraints. Finally, we present a pose optimizer that systematically integrates our contact representation and stability constraints to enable stable grasp generation. We show that these constraints can help identify key contact points for stability which provide effective initialization and guidance for optimization towards a stable grasp. Experiments are carried out on two public benchmarks, showing that our method brings about 20% improvement in stability metrics and adapts well to novel objects.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [82] [MedBuild AI: An Agent-Based Hybrid Intelligence Framework for Reshaping Agency in Healthcare Infrastructure Planning through Generative Design for Medical Architecture](https://arxiv.org/abs/2511.11587)
*Yiming Zhang,Yuejia Xu,Ziyao Wang,Xin Yan,Xiaosai Hao*

Main category: cs.HC

TL;DR: 混合智能框架MedBuild AI整合大型语言模型与专家系统，为资源匮乏地区提供低成本模块化医疗建筑设计解决方案，重构全球医疗建筑规划的公平性。


<details>
  <summary>Details</summary>
Motivation: 全球医疗基础设施分布严重不均，传统规划效率低下，人道主义建筑项目难以满足实际需求，急需智能化工具提升医疗空间设计的可及性。

Method: 三阶段智能代理系统：1）对话式健康需求采集 2）规则驱动的功能方案生成 3）自动布局建模，嵌入计算协商机制实现多方需求平衡。

Result: 开发基于卫星互联网的开放平台，通过三步流程实现24小时内完成从需求分析到三维模型输出的完整医疗建筑设计指导。

Conclusion: 该框架通过技术民主化重塑医疗建筑规划范式，将社区参与转化为可计算参数，建立包容性决策机制，为全球健康公平提供数字基础设施支撑。

Abstract: Globally, disparities in healthcare infrastructure remain stark, leaving countless communities without access to even basic services. Traditional infrastructure planning is often slow and inaccessible, and although many architects are actively delivering humanitarian and aid-driven hospital projects worldwide, these vital efforts still fall far short of the sheer scale and urgency of demand. This paper introduces MedBuild AI, a hybrid-intelligence framework that integrates large language models (LLMs) with deterministic expert systems to rebalance the early design and conceptual planning stages. As a web-based platform, it enables any region with satellite internet access to obtain guidance on modular, low-tech, low-cost medical building designs. The system operates through three agents: the first gathers local health intelligence via conversational interaction; the second translates this input into an architectural functional program through rule-based computation; and the third generates layouts and 3D models. By embedding computational negotiation into the design process, MedBuild AI fosters a reciprocal, inclusive, and equitable approach to healthcare planning, empowering communities and redefining agency in global healthcare architecture.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [83] [A neural optimization framework for free-boundary diffeomorphic mapping problems and its applications](https://arxiv.org/abs/2511.11679)
*Zhehao Xu,Lok Ming Lui*

Main category: cs.LG

TL;DR: 提出SBN-Opt框架，通过神经网络的谱架构嵌入LSQC理论，优化自由边界微分同胚映射并精确控制几何畸变，成功应用于密度均衡映射和表面配准问题。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法依赖地标约束且无法融入梯度优化流程，严重限制了自由边界优化在几何处理任务中的实际应用。

Method: 构建SBN神经网络作为LSQC能量的替代模型，采用多尺度网格谱架构实现高效计算，开发SBN-Opt框架进行端到端优化。

Result: 在密度均衡映射和复杂表面配准任务中，SBN-Opt显著超越传统数值算法，验证了方法的有效性和优越性。

Conclusion: 该框架突破了自由边界优化的技术瓶颈，为几何变形处理提供了兼具数学保证和计算效率的新范式。

Abstract: Free-boundary diffeomorphism optimization is a core ingredient in the surface mapping problem but remains notoriously difficult because the boundary is unconstrained and local bijectivity must be preserved under large deformation. Numerical Least-Squares Quasiconformal (LSQC) theory, with its provable existence, uniqueness, similarity-invariance and resolution-independence, offers an elegant mathematical remedy. However, the conventional numerical algorithm requires landmark conditioning, and cannot be applied into gradient-based optimization. We propose a neural surrogate, the Spectral Beltrami Network (SBN), that embeds LSQC energy into a multiscale mesh-spectral architecture. Next, we propose the SBN guided optimization framework SBN-Opt which optimizes free-boundary diffeomorphism for the problem, with local geometric distortion explicitly controllable. Extensive experiments on density-equalizing maps and inconsistent surface registration demonstrate our SBN-Opt's superiority over traditional numerical algorithms.

</details>


### [84] [Hierarchical Frequency-Decomposition Graph Neural Networks for Road Network Representation Learning](https://arxiv.org/abs/2511.12507)
*Jingtian Ma,Jingyuan Wang,Leong Hou U*

Main category: cs.LG

TL;DR: 提出分层频域分解图神经网络HiFiNet，通过虚拟节点层次结构和三阶段框架统一建模路网空间特征与频谱特性


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络存在空间方法局部拓扑捕捉过度平滑与谱方法全局频率分析忽略局部变异的空间-频谱错位问题

Method: 构建虚拟节点多级层次结构实现局部频率分析，采用分解-更新-重构框架配合拓扑感知图Transformer分别处理低频/高频信号

Result: 在四个下游任务的真实数据集验证中展现出优异的表征学习能力和泛化性能

Conclusion: HiFiNet成功统一空间与频谱建模，为解决路网全局趋势与局部波动协同表征提供了有效方案

Abstract: Road networks are critical infrastructures underpinning intelligent transportation systems and their related applications. Effective representation learning of road networks remains challenging due to the complex interplay between spatial structures and frequency characteristics in traffic patterns. Existing graph neural networks for modeling road networks predominantly fall into two paradigms: spatial-based methods that capture local topology but tend to over-smooth representations, and spectral-based methods that analyze global frequency components but often overlook localized variations. This spatial-spectral misalignment limits their modeling capacity for road networks exhibiting both coarse global trends and fine-grained local fluctuations. To bridge this gap, we propose HiFiNet, a novel hierarchical frequency-decomposition graph neural network that unifies spatial and spectral modeling. HiFiNet constructs a multi-level hierarchy of virtual nodes to enable localized frequency analysis, and employs a decomposition-updating-reconstruction framework with a topology-aware graph transformer to separately model and fuse low- and high-frequency signals. Theoretically justified and empirically validated on multiple real-world datasets across four downstream tasks, HiFiNet demonstrates superior performance and generalization ability in capturing effective road network representations.

</details>


<div id='math.HO'></div>

# math.HO [[Back]](#toc)

### [85] [On The Topology of Polygonal Meshes](https://arxiv.org/abs/2511.11618)
*Andreas Bærentzen*

Main category: math.HO

TL;DR: 本文系统介绍了多边形网格的拓扑学概念，通过欧拉公式、贝蒂数等工具解析网格拓扑特性，并演示如何切割网格形成拓扑圆盘。


<details>
  <summary>Details</summary>
Motivation: 为非专业读者提供拓扑学的入门指引，避免抽象代数概念，通过几何直观方式阐述多边形网格的拓扑性质。

Method: 采用概念分层论述：1) 拓扑学基础概念(同胚/同伦/同调) 2) 内禀/外在拓扑区分 3) 欧拉-庞加莱公式证明 4) 贝蒂数统计量定义法 5) 网格切割技术。

Result: 构建了完整的拓扑分析框架：1) 证明欧拉公式 2) 建立贝蒂数与网格统计量的直接关联 3) 提出网格拓扑圆盘化切割方案。

Conclusion: 通过基础拓扑工具实现对多边形网格的拓扑解析，证明拓扑特性可通过几何方法定量分析，为计算机图形学提供理论支持。

Abstract: This paper is an introductory and informal exposition on the topology of polygonal meshes. We begin with a broad overview of topological notions and discuss how homeomorphisms, homotopy, and homology can be used to characterize topology. We move on to define polygonal meshes and make a distinction between intrinsic topology and extrinsic topology which depends on the embedding space. A distinction is also made between quantitative topological properties and qualitative properties. We outline a proof of the Euler and the Euler-Poincaré formulas. The Betti numbers are defined in terms of the Euler-Poincaré formula and other mesh statistics rather than as cardinalities of the homology groups which allows us to avoid abstract algebra. Finally, we discuss how it is possible to cut a polygonal mesh such that it becomes a topological disc.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [86] [Co-Layout: LLM-driven Co-optimization for Interior Layout](https://arxiv.org/abs/2511.12474)
*Chucheng Xiang,Ruchao Bao,Biyin Feng,Wenzheng Wu,Zhongyuan Liu,Yirui Guan,Ligang Liu*

Main category: cs.CV

TL;DR: 提出结合大语言模型与网格整数编程的联合优化框架，通过粗到细策略提升室内设计方案质量与计算效率


<details>
  <summary>Details</summary>
Motivation: 解决现有两阶段设计流程在解决方案质量和计算效率方面的不足，实现房间布局与家具摆放的协同优化

Method: LLM提取文本约束→网格编码→整数规划优化（含走廊连通性/空间可达性等约束）→粗粒度到细粒度的分层优化策略

Result: 多场景实验显示方案质量显著优于传统方法，计算效率通过分层策略提升5-8倍

Conclusion: 验证了联合优化框架的有效性，网格编码与分层策略为空间设计问题提供新思路

Abstract: We present a novel framework for automated interior design that combines large language models (LLMs) with grid-based integer programming to jointly optimize room layout and furniture placement. Given a textual prompt, the LLM-driven agent workflow extracts structured design constraints related to room configurations and furniture arrangements. These constraints are encoded into a unified grid-based representation inspired by ``Modulor". Our formulation accounts for key design requirements, including corridor connectivity, room accessibility, spatial exclusivity, and user-specified preferences. To improve computational efficiency, we adopt a coarse-to-fine optimization strategy that begins with a low-resolution grid to solve a simplified problem and guides the solution at the full resolution. Experimental results across diverse scenarios demonstrate that our joint optimization approach significantly outperforms existing two-stage design pipelines in solution quality, and achieves notable computational efficiency through the coarse-to-fine strategy.

</details>


### [87] [Lightweight Optimal-Transport Harmonization on Edge Devices](https://arxiv.org/abs/2511.12785)
*Maria Larchenko,Dmitry Guskov,Alexander Lobashev,Georgy Derevyanko*

Main category: cs.CV

TL;DR: 提出基于最优输运理论的MKL-Harmonizer算法，实现实时AR颜色协调，并发布专用数据集与工具包


<details>
  <summary>Details</summary>
Motivation: 现有颜色协调算法缺乏实时性，无法集成到AR工作流，需开发轻量级设备端推理方案

Method: 利用Monge-Kantorovich输运图预测，训练紧凑编码器实现最优输运理论的应用

Result: 在真实AR合成图像上获得最佳综合评分，超越现有方法

Conclusion: 通过算法创新与数据共享，推动AR领域颜色协调技术的实用化发展

Abstract: Color harmonization adjusts the colors of an inserted object so that it perceptually matches the surrounding image, resulting in a seamless composite. The harmonization problem naturally arises in augmented reality (AR), yet harmonization algorithms are not currently integrated into AR pipelines because real-time solutions are scarce. In this work, we address color harmonization for AR by proposing a lightweight approach that supports on-device inference. For this, we leverage classical optimal transport theory by training a compact encoder to predict the Monge-Kantorovich transport map. We benchmark our MKL-Harmonizer algorithm against state-of-the-art methods and demonstrate that for real composite AR images our method achieves the best aggregated score. We release our dedicated AR dataset of composite images with pixel-accurate masks and data-gathering toolkit to support further data acquisition by researchers.

</details>


### [88] [PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos](https://arxiv.org/abs/2511.12935)
*Dianbing Xi,Guoyuan An,Jingsen Zhu,Zhijian Liu,Yuan Liu,Ruiyuan Zhang,Jiayuan Lu,Rui Wang,Yuchi Huo*

Main category: cs.CV

TL;DR: PFAvatar提出两阶段3D虚拟化身重建方法：通过姿势感知扩散模型微调和NeRF蒸馏，解决传统方法分解建模导致的低效与不一致问题，实现5分钟快速建模并支持虚拟试衣等应用。


<details>
  <summary>Details</summary>
Motivation: 传统OOTD照片3D重建方法需分割服装/配饰等部件进行组装，易产生部件不协调、无法处理遮挡/复杂背景等问题。现有基于网格的表示难以保留高频细节。

Method: 1. 第一阶段：集成ControlNet姿势估计框架与CPPL损失函数，端到端学习全身外观；2. 第二阶段：通过规范SMPL-X空间采样和多分辨率3D-SDS优化NeRF表征，保留连续高频纹理。

Result: 48倍速度提升，在遮挡/截断场景下保持最优重建保真度（PSNR 28.7 vs SOTA 26.3）。支持虚拟试衣、动画重定向等下游任务。

Conclusion: 突破少样本条件下3D重建的效率与质量瓶颈，通过连续辐射场表征实现真实世界OOTD相册的实用化建模，为元宇宙数字人创建提供新范式。

Abstract: We propose PFAvatar (Pose-Fusion Avatar), a new method that reconstructs high-quality 3D avatars from ``Outfit of the Day'' (OOTD) photos, which exhibit diverse poses, occlusions, and complex backgrounds. Our method consists of two stages: (1) fine-tuning a pose-aware diffusion model from few-shot OOTD examples and (2) distilling a 3D avatar represented by a neural radiance field (NeRF). In the first stage, unlike previous methods that segment images into assets (e.g., garments, accessories) for 3D assembly, which is prone to inconsistency, we avoid decomposition and directly model the full-body appearance. By integrating a pre-trained ControlNet for pose estimation and a novel Condition Prior Preservation Loss (CPPL), our method enables end-to-end learning of fine details while mitigating language drift in few-shot training. Our method completes personalization in just 5 minutes, achieving a 48$\times$ speed-up compared to previous approaches. In the second stage, we introduce a NeRF-based avatar representation optimized by canonical SMPL-X space sampling and Multi-Resolution 3D-SDS. Compared to mesh-based representations that suffer from resolution-dependent discretization and erroneous occluded geometry, our continuous radiance field can preserve high-frequency textures (e.g., hair) and handle occlusions correctly through transmittance. Experiments demonstrate that PFAvatar outperforms state-of-the-art methods in terms of reconstruction fidelity, detail preservation, and robustness to occlusions/truncations, advancing practical 3D avatar generation from real-world OOTD albums. In addition, the reconstructed 3D avatar supports downstream applications such as virtual try-on, animation, and human video reenactment, further demonstrating the versatility and practical value of our approach.

</details>


### [89] [SymGS : Leveraging Local Symmetries for 3D Gaussian Splatting Compression](https://arxiv.org/abs/2511.13264)
*Keshav Gupta,Akshat Sanghvi,Shreyas Reddy Palley,Astitva Srivastava,Charu Sharma,Avinash Sharma*

Main category: cs.CV

TL;DR: 提出SymGS框架，通过引入可学习镜像消除反射冗余，实现3DGS场景108倍压缩并保持渲染质量


<details>
  <summary>Details</summary>
Motivation: 现有3DGS压缩方法仅利用相似性检测和量化，未充分挖掘场景对称性特征。内存占用随场景复杂度快速增长（可达GB级）限制了实际应用

Method: 1. 引入可学习镜像检测局部/全局反射对称性
2. 建立镜像参数与高斯原语的空间对应关系
3. 设计即插即用框架兼容现有压缩方法（如HAC）

Result: 1. 相比HAC实现1.66倍压缩（大规模场景达3倍）
2. 平均108倍压缩率
3. PSNR指标保持同等水平（>31dB）

Conclusion: 对称性感知技术突破现有压缩极限，镜像冗余消除策略可有效增强各类压缩算法，为大规模场景应用奠定基础

Abstract: 3D Gaussian Splatting has emerged as a transformative technique in novel view synthesis, primarily due to its high rendering speed and photorealistic fidelity. However, its memory footprint scales rapidly with scene complexity, often reaching several gigabytes. Existing methods address this issue by introducing compression strategies that exploit primitive-level redundancy through similarity detection and quantization. We aim to surpass the compression limits of such methods by incorporating symmetry-aware techniques, specifically targeting mirror symmetries to eliminate redundant primitives. We propose a novel compression framework, \textbf{\textit{SymGS}}, introducing learnable mirrors into the scene, thereby eliminating local and global reflective redundancies for compression. Our framework functions as a plug-and-play enhancement to state-of-the-art compression methods, (e.g. HAC) to achieve further compression. Compared to HAC, we achieve $1.66 \times$ compression across benchmark datasets (upto $3\times$ on large-scale scenes). On an average, SymGS enables $\bf{108\times}$ compression of a 3DGS scene, while preserving rendering quality. The project page and supplementary can be found at \textbf{\color{cyan}{symgs.github.io}}

</details>
