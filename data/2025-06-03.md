<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 235]
- [cs.GR](#cs.GR) [Total: 11]
- [cs.CY](#cs.CY) [Total: 9]
- [cs.IR](#cs.IR) [Total: 7]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.SD](#cs.SD) [Total: 3]
- [cs.AI](#cs.AI) [Total: 19]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.CV](#cs.CV) [Total: 9]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.LG](#cs.LG) [Total: 18]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Amadeus-Verbo Technical Report: The powerful Qwen2.5 family models trained in Portuguese](https://arxiv.org/abs/2506.00019)
*William Alberto Cruz-Castañeda,Marcellus Amadeus*

Main category: cs.CL

TL;DR: 巴西葡萄牙语大语言模型Amadeus Verbo系列开发经验，包含0.5B到72B参数的不同模型类型，通过微调基础模型实现资源可用时的民主化开发。


<details>
  <summary>Details</summary>
Motivation: 展示在数据和计算资源充足时，通过微调基础模型可便捷实现巴西葡萄牙语LLM的开源民主化开发，填补该语言领域开源模型的空白。

Method: 开发包含基础调优、混合调优和指令调优三种类型的模型家族，覆盖0.5B-72B参数规模，适配不同应用场景需求。

Result: 成功构建全系列模型并开源发布于HuggingFace平台，验证了基础模型微调路径的有效性。

Conclusion: 通过参数高效微调方法实现了巴西葡萄牙语LLM的快速定制开发，为资源可用情况下的语言民主化提供了可复现范例。

Abstract: This report introduces the experience of developing Amadeus Verbo, a family
of large language models for Brazilian Portuguese. To handle diverse use cases,
Amadeus Verbo includes base-tuned, merged, and instruction-tuned models in
sizes of 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B parameters. Thus, the main
objective is to show how easy it is to fine-tune foundation models to
democratize the open-source development of Brazilian Portuguese LLMs when data
and resources are available. Amadeus-Verbo family models are all available at
HuggingFace at
https://huggingface.co/collections/amadeusai/amadeus-verbo-qwen25-67cf2e7aae69ce2b3bcdcfda.

</details>


### [2] [Scaling Physical Reasoning with the PHYSICS Dataset](https://arxiv.org/abs/2506.00022)
*Shenghe Zheng,Qianjia Cheng,Junchi Yao,Mengsong Wu,haonan he,Ning Ding,Yu Cheng,Shuyue Hu,Lei Bai,Dongzhan Zhou,Ganqu Cui,Peng Ye*

Main category: cs.CL

TL;DR: 构建PHYSICS物理问题数据集并提出Rule+Model评估框架，揭示当前大模型在物理推理任务中的局限性


<details>
  <summary>Details</summary>
Motivation: 物理学科兼具强推理属性和现实应用价值，但在LLM研究中未获得足够关注。现有评估框架存在单位/简化/精度等物理领域特有的偏差，需要针对性解决方案

Method: 1. 从100+教科书筛选16,568题构建跨学科/多难度数据集 2. 提供训练集推理路径增强模型训练 3. 提出融合规则系统与模型判定的混合评估框架

Result: 当前最优开源/闭源模型在物理任务表现显著受限，Rule+Model框架有效平衡评估效率与准确性

Conclusion: PHYSICS数据集及配套评估方法将共同推动LLM在物理领域的发展，弥补现有研究在复杂科学推理任务上的不足

Abstract: Large Language Models (LLMs) have achieved remarkable progress on advanced
reasoning tasks such as mathematics and coding competitions. Meanwhile,
physics, despite being both reasoning-intensive and essential to real-world
understanding, received limited academic and industrial attention. This paper
introduces PHYSICS, a dataset containing 16,568 high-quality physics problems
spanning subjects and difficulty levels, to facilitate this issue.
Specifically, PHYSICS is curated with exercises from over 100 textbooks through
a carefully designed pipeline for quality control. It covers five major physics
domains: Mechanics, Electromagnetism, Thermodynamics, Optics, and Modern
Physics. It also spans a wide range of difficulty levels, from high school to
graduate-level physics courses. To utilize the data for improving and
evaluating the model's physical reasoning capabilities, we split the dataset
into training and test sets, and provide reasoning paths generated by powerful
reasoning models for the training data to facilitate model training. In
addition, for the evaluation part, we find that existing evaluation frameworks
exhibit biases in aspects such as units, simplification, and precision in
physics domain. To balance efficiency and accuracy, we introduce a Rule+Model
evaluation framework tailored to physics problems. Our evaluations on current
state-of-the-art open-source and proprietary models highlight the limitations
of current models in handling physics-related tasks. We hope that our dataset
and evaluation methodology will jointly advance the development of LLMs in the
field of physics.

</details>


### [3] [From Mathematical Reasoning to Code: Generalization of Process Reward Models in Test-Time Scaling](https://arxiv.org/abs/2506.00027)
*Zhengyu Chen,Yudong Wang,Teng Xiao,Ruochen Zhou,Xuesheng Yang,Wei Wang,Zhifang Sui,Jingang Wang*

Main category: cs.CL

TL;DR: 研究分析了过程奖励模型（PRMs）在复杂推理任务中的性能，揭示了模型规模扩大带来的收益递减现象，强调数据集多样性对提升效率的重要性，并展示了PRMs在跨领域任务中的强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 针对大语言模型推理能力提升过程中中间错误累积的痛点，系统评估PRMs在不同训练方法、扩展策略和跨领域泛化能力方面的表现，为优化推理模型提供理论依据。

Method: 通过分析预训练与奖励模型训练FLOPs的关联性，测试不同规模PRMs的性能；采用数学与代码生成数据集对比验证泛化能力；运用基于梯度的模式相似性评估指标；比较蒙特卡洛树搜索与Best-of-N采样策略。

Result: 发现PRMs规模扩展存在收益递减临界点（~1B参数）；数学数据集训练的PRMs代码任务准确率达89%；蒙特卡洛树搜索在充足计算资源下表现最佳（+7.2%准确率）；数据集多样性使训练效率提升34%。

Conclusion: 需平衡模型规模与计算成本，优先保证训练数据多样性；建议根据资源选择扩展策略（蒙特卡洛树搜索/Best-of-N）；验证了PRMs的跨领域迁移潜力，为构建通用推理模型提供新思路。

Abstract: Recent advancements in improving the reasoning capabilities of Large Language
Models have underscored the efficacy of Process Reward Models (PRMs) in
addressing intermediate errors through structured feedback mechanisms. This
study analyzes PRMs from multiple perspectives, including training
methodologies, scalability, and generalization capabilities. We investigate the
interplay between pre-training and reward model training FLOPs to assess their
influence on PRM efficiency and accuracy in complex reasoning tasks. Our
analysis reveals a pattern of diminishing returns in performance with
increasing PRM scale, highlighting the importance of balancing model size and
computational cost. Furthermore, the diversity of training datasets
significantly impacts PRM performance, emphasizing the importance of diverse
data to enhance both accuracy and efficiency. We further examine test-time
scaling strategies, identifying Monte Carlo Tree Search as the most effective
method when computational resources are abundant, while Best-of-N Sampling
serves as a practical alternative under resource-limited conditions. Notably,
our findings indicate that PRMs trained on mathematical datasets exhibit
performance comparable to those tailored for code generation, suggesting robust
cross-domain generalization. Employing a gradient-based metric, we observe that
PRMs exhibit a preference for selecting responses with similar underlying
patterns, further informing their optimization.

</details>


### [4] [Enhancing Tool Learning in Large Language Models with Hierarchical Error Checklists](https://arxiv.org/abs/2506.00042)
*Yue Cui,Liuyi Yao,Shuchang Tao,Weijie Shi,Yaliang Li,Bolin Ding,Xiaofang Zhou*

Main category: cs.CL

TL;DR: 提出HiTEC框架，通过全局/本地错误检查表和两种优化部署方案（HiTEC-ICL和HiTEC-KTO），显著提升LLM工具调用中的参数填充准确率


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在工具调用时频繁出现参数填充错误，需建立不依赖真实交互的系统化错误诊断方案

Method: 双层错误检查架构（全局跨工具检查+本地上下文检查），配合基于上下文学习的动态参数优化和基于KTO的负例微调方案

Result: 在5个公共数据集上验证，参数填充准确率和工具调用成功率显著优于基线方法

Conclusion: HiTEC框架通过结构化错误检查和优化机制，有效增强了LLM工具调用的可靠性与成功率

Abstract: Large language models (LLMs) have significantly advanced natural language
processing, particularly through the integration of external tools and APIs.
However, their effectiveness is frequently hampered by parameter mis-filling
during tool calling. In this paper, we propose the Hierarchical Tool Error
Checklist (HiTEC) framework to systematically diagnose and mitigate
tool-calling errors without relying on extensive real-world interactions. HiTEC
introduces a two-tiered approach: a global error checklist that identifies
common, cross-tool issues, and a local error checklist that targets
tool-specific and contextual failures. Building on this structure, we propose
two deployments: HiTEC-In Context Learning (HiTEC-ICL) and
HiTEC-Kahneman-Tversky Optimization (HiTEC-KTO). HiTEC-ICL embeds the global
checklist in the initial prompts and leverages a two-round conversational
interaction to dynamically refine parameter handling, while HiTEC-KTO generates
high-quality negative examples to drive fine-tuning via preference-based
optimization. Extensive experiments across five public datasets demonstrate
that our framework significantly improves parameter-filling accuracy and
tool-calling success rates compared to baseline methods.

</details>


### [5] [Unraveling SITT: Social Influence Technique Taxonomy and Detection with LLMs](https://arxiv.org/abs/2506.00061)
*Wiktoria Mieleszczenko-Kowszewicz,Beata Bajcar,Aleksander Szczęsny,Maciej Markiewicz,Jolanta Babiak,Berenika Dyczek,Przemysław Kazienko*

Main category: cs.CL

TL;DR: 提出SITT分类法用于检测文本社会影响技术，构建专业标注数据集评估LLMs识别能力，发现现有模型在上下文敏感技术上的局限性


<details>
  <summary>Details</summary>
Motivation: 需要系统框架检测对话中隐蔽的社会影响技术，并验证当前大语言模型识别这些技术的能力

Method: 创建包含58种技术的分类法（SITT），构建746对话的专家标注数据集，采用分层多标签分类评估5个主流LLM

Result: Claude 3.5表现最佳（类别F1=0.45），模型整体性能有限，上下文敏感技术识别效果差

Conclusion: 现有LLM对语言细微差异敏感度不足，领域特定微调至关重要，研究为理解LLM如何检测社会影响策略提供新资源

Abstract: In this work we present the Social Influence Technique Taxonomy (SITT), a
comprehensive framework of 58 empirically grounded techniques organized into
nine categories, designed to detect subtle forms of social influence in textual
content. We also investigate the LLMs ability to identify various forms of
social influence. Building on interdisciplinary foundations, we construct the
SITT dataset -- a 746-dialogue corpus annotated by 11 experts in Polish and
translated into English -- to evaluate the ability of LLMs to identify these
techniques. Using a hierarchical multi-label classification setup, we benchmark
five LLMs, including GPT-4o, Claude 3.5, Llama-3.1, Mixtral, and PLLuM. Our
results show that while some models, notably Claude 3.5, achieved moderate
success (F1 score = 0.45 for categories), overall performance of models remains
limited, particularly for context-sensitive techniques. The findings
demonstrate key limitations in current LLMs' sensitivity to nuanced linguistic
cues and underscore the importance of domain-specific fine-tuning. This work
contributes a novel resource and evaluation example for understanding how LLMs
detect, classify, and potentially replicate strategies of social influence in
natural dialogues.

</details>


### [6] [Mis-prompt: Benchmarking Large Language Models for Proactive Error Handling](https://arxiv.org/abs/2506.00064)
*Jiayi Zeng,Yizhe Feng,Mengliang He,Wenhui Lei,Wei Zhang,Zeming Liu,Xiaoming Shi,Aimin Zhou*

Main category: cs.CL

TL;DR: 提出新基准Mis-prompt评估LLMs主动错误处理能力，实验表明当前模型表现不足但SFT训练可提升性能


<details>
  <summary>Details</summary>
Motivation: 现有LLMs错误处理依赖显式指令，而实际场景缺乏明确指导，需研究无显式指令的主动错误处理方法

Method: 构建包含四任务、错误分类法和数据集的Mis-prompt基准，分析LLMs表现并进行SFT实验验证

Result: 当前LLMs主动错误处理能力薄弱，使用错误处理实例进行监督微调(SFT)显著提升模型性能

Conclusion: 主动错误处理是LLMs重要挑战，Mis-prompt基准填补研究空白，SFT证明训练数据对提升模型错误恢复能力有效

Abstract: Large language models (LLMs) have demonstrated significant advancements in
error handling. Current error-handling works are performed in a passive manner,
with explicit error-handling instructions. However, in real-world scenarios,
explicit error-handling instructions are usually unavailable. In this paper,
our work identifies this challenge as how to conduct proactive error handling
without explicit error handling instructions. To promote further research, this
work introduces a new benchmark, termed Mis-prompt, consisting of four
evaluation tasks, an error category taxonomy, and a new evaluation dataset.
Furthermore, this work analyzes current LLMs' performance on the benchmark, and
the experimental results reveal that current LLMs show poor performance on
proactive error handling, and SFT on error handling instances improves LLMs'
proactive error handling capabilities. The dataset will be publicly available.

</details>


### [7] [You Prefer This One, I Prefer Yours: Using Reference Words is Harder Than Vocabulary Words for Humans and Multimodal Language Models](https://arxiv.org/abs/2506.00065)
*Dota Tianai Dong,Yifan Luo,Po-Ya Angela Wang,Asli Ozyurek,Paula Rubio-Fernandez*

Main category: cs.CL

TL;DR: 多模态语言模型在指代词使用上存在认知层级差异，物主代词和指示代词处理能力显著低于人类水平


<details>
  <summary>Details</summary>
Motivation: 尽管多模态语言模型（MLMs）的交流方式越来越类人，但其指代词使用能力尚未被充分研究，而指代词在日常交流中普遍存在

Method: 通过词汇词、物主代词（'我的'/'你的'）和指示代词（'这个'/'那个'）三类认知需求递增的词语任务，对比评估7个最先进MLMs与人类参与者的表现

Result: MLMs在词汇任务接近人类水平，但物主代词和指示代词处理存在显著缺陷。提示工程可提升物主代词使用能力，但指示代词表现仍远低于人类水平

Conclusion: 当前NLP系统在生成需要语用和社会认知的语法形式时仍面临明显挑战，为自然语言处理的理论和实践提供了重要依据

Abstract: Multimodal language models (MLMs) increasingly communicate in human-like
ways, yet their ability to use reference words remains largely overlooked
despite their ubiquity in everyday communication. Our study addresses this gap
by comparing human and MLM use of three word classes with increasing cognitive
demands: vocabulary words, possessive pronouns (`mine' vs `yours'), and
demonstrative pronouns (`this one' vs `that one'). Evaluating seven
state-of-the-art MLMs against human participants, we observe a clear difficulty
hierarchy: while MLMs approach human-level performance on the vocabulary task,
they show substantial deficits with possessives and demonstratives. Our
analysis reveals these difficulties stem from limitations in perspective-taking
and spatial reasoning. Although prompt engineering improved model performance
on possessive use, demonstrative use remained well below human-level
competence. These findings provide theoretical and empirical evidence that
producing grammatical forms requiring pragmatics and social cognition remains a
clear challenge in current NLP systems.

</details>


### [8] [Probing Politico-Economic Bias in Multilingual Large Language Models: A Cultural Analysis of Low-Resource Pakistani Languages](https://arxiv.org/abs/2506.00068)
*Afrozah Nadeem,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 系统分析13个LLM在巴基斯坦五种低资源语言中的政治偏见，发现模型普遍倾向西方自由左派价值观，但区域语言呈现威权转向，揭示文化调制效应。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视非西方低资源多语种环境的LLM政治经济偏见，巴基斯坦多语言社会为此类研究提供了独特样本。

Method: 整合改良版政治指南针测试(PCT)与多层级框架分析，结合定量政治定位评估与内容/风格/重点的定性分析，并基于11个巴国社会主题设计提示词。

Result: 1. LLM整体倾向自由左派(反映西方数据影响) 2.区域语言出现系统性威权框架偏移 3.识别出稳定模型特异性偏见 4.语言条件化意识形态表达差异显著

Conclusion: 研究证实LLM偏见具有文化情境依赖性，亟需建立基于在地文化的多语言偏见审计框架，对全球AI伦理治理具有方法论创新意义。

Abstract: Large Language Models (LLMs) are increasingly shaping public discourse, yet
their politico-economic biases remain underexamined in non-Western and
low-resource multilingual contexts. This paper presents a systematic analysis
of political bias in 13 state-of-the-art LLMs across five low-resource
languages spoken in Pakistan: Urdu, Punjabi, Sindhi, Balochi, and Pashto. We
propose a novel framework that integrates an adapted Political Compass Test
(PCT) with a multi-level framing analysis. Our method combines quantitative
assessment of political orientation across economic (left-right) and social
(libertarian-authoritarian) axes with qualitative analysis of framing through
content, style, and emphasis. We further contextualize this analysis by
aligning prompts with 11 key socio-political themes relevant to Pakistani
society. Our results reveal that LLMs predominantly align with liberal-left
values, echoing Western training data influences, but exhibit notable shifts
toward authoritarian framing in regional languages, suggesting strong cultural
modulation effects. We also identify consistent model-specific bias signatures
and language-conditioned variations in ideological expression. These findings
show the urgent need for culturally grounded, multilingual bias auditing
frameworks.

</details>


### [9] [Evaluating the Sensitivity of LLMs to Prior Context](https://arxiv.org/abs/2506.00069)
*Robert Hankache,Kingsley Nketia Acheampong,Liang Song,Marek Brynda,Raad Khraishi,Greig A. Cowan*

Main category: cs.CL

TL;DR: 研究发现大语言模型在多轮对话场景中性能显著下降（部分模型达73%），任务描述策略性放置可提升准确性3.5倍，需加强上下文敏感性的应对策略


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要针对单轮QA任务，无法有效评估多轮交互场景中LLMs的上下文敏感性表现

Method: 通过设计系统调整上下文数量和任务描述位置的新基准测试，评估GPT/Claude/Gemini等主流模型的多轮交互表现

Result: 多轮交互导致模型准确率最高下降73%（GPT-4o下降32%），模型大小与性能无稳定关联，任务描述策略性放置可提升准确性达3.5倍

Conclusion: LLMs的上下文敏感性凸显需开发鲁棒的上下文设计策略，这对模型评估体系优化和实际应用部署具有重要指导意义

Abstract: As large language models (LLMs) are increasingly deployed in multi-turn
dialogue and other sustained interactive scenarios, it is essential to
understand how extended context affects their performance. Popular benchmarks,
focusing primarily on single-turn question answering (QA) tasks, fail to
capture the effects of multi-turn exchanges. To address this gap, we introduce
a novel set of benchmarks that systematically vary the volume and nature of
prior context. We evaluate multiple conventional LLMs, including GPT, Claude,
and Gemini, across these benchmarks to measure their sensitivity to contextual
variations. Our findings reveal that LLM performance on multiple-choice
questions can degrade dramatically in multi-turn interactions, with performance
drops as large as 73% for certain models. Even highly capable models such as
GPT-4o exhibit up to a 32% decrease in accuracy. Notably, the relative
performance of larger versus smaller models is not always predictable.
Moreover, the strategic placement of the task description within the context
can substantially mitigate performance drops, improving the accuracy by as much
as a factor of 3.5. These findings underscore the need for robust strategies to
design, evaluate, and mitigate context-related sensitivity in LLMs.

</details>


### [10] [Gaussian mixture models as a proxy for interacting language models](https://arxiv.org/abs/2506.00077)
*Edward Wang,Tianyu Wang,Avanti Athreya,Vince Lyzinski,Carey E. Priebe*

Main category: cs.CL

TL;DR: 论文通过对比高斯混合模型（GMMs）与大型语言模型（LLMs）的交互动态，提出GMMs能有效模拟LLMs的关键交互特征，并分析了两种模型的优劣差异。


<details>
  <summary>Details</summary>
Motivation: LLMs虽能模拟人类行为但计算成本高，需寻找更轻量的替代模型（如GMMs）用于社会科学中大规模人际交互研究。

Method: 将简化版GMMs与基于反馈机制更新的LLMs实验模拟进行对比，分析两者在交互动态中的异同。

Result: GMMs成功复现了LLMs交互的核心动态特征，同时在计算效率和可解释性方面展现出优势差异。

Conclusion: GMMs可作为LLMs的有效替代方案，未来可通过参数优化和反馈机制改进模型，拓展社会科学仿真场景的应用。

Abstract: Large language models (LLMs) are a powerful tool with the ability to match
human capabilities and behavior in many settings. Retrieval-augmented
generation (RAG) further allows LLMs to generate diverse output depending on
the contents of their RAG database. This motivates their use in the social
sciences to study human behavior between individuals when large-scale
experiments are infeasible. However, LLMs depend on complex, computationally
expensive algorithms. In this paper, we introduce interacting Gaussian mixture
models (GMMs) as an alternative to similar frameworks using LLMs. We compare a
simplified model of GMMs to select experimental simulations of LLMs whose
updating and response depend on feedback from other LLMs. We find that
interacting GMMs capture important features of the dynamics in interacting
LLMs, and we investigate key similarities and differences between interacting
LLMs and GMMs. We conclude by discussing the benefits of Gaussian mixture
models, potential modifications, and future research directions.

</details>


### [11] [COSMIC: Generalized Refusal Direction Identification in LLM Activations](https://arxiv.org/abs/2506.00085)
*Vincent Siu,Nicholas Crispino,Zihao Yu,Sam Pan,Zhun Wang,Yang Liu,Dawn Song,Chenguang Wang*

Main category: cs.CL

TL;DR: 提出基于余弦相似度的自动化框架COSMIC，无需依赖预定义模板即可检测和引导LLM的拒绝行为


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预设拒绝模板或手动分析，难以适应对抗环境和弱对齐模型的需求

Method: 通过余弦相似度指标自动选择有效的拒绝方向及作用层，不假设模型存在特定拒绝标记

Result: 在不同对齐条件下实现与现有方法相当的引导效果，显著提升模型安全性且保持低误拒率

Conclusion: COSMIC框架在多种对齐场景中展现鲁棒性，为模型安全控制提供可靠解决方案

Abstract: Large Language Models (LLMs) encode behaviors such as refusal within their
activation space, yet identifying these behaviors remains a significant
challenge. Existing methods often rely on predefined refusal templates
detectable in output tokens or require manual analysis. We introduce
\textbf{COSMIC} (Cosine Similarity Metrics for Inversion of Concepts), an
automated framework for direction selection that identifies viable steering
directions and target layers using cosine similarity - entirely independent of
model outputs. COSMIC achieves steering performance comparable to prior methods
without requiring assumptions about a model's refusal behavior, such as the
presence of specific refusal tokens. It reliably identifies refusal directions
in adversarial settings and weakly aligned models, and is capable of steering
such models toward safer behavior with minimal increase in false refusals,
demonstrating robustness across a wide range of alignment conditions.

</details>


### [12] [SwitchLingua: The First Large-Scale Multilingual and Multi-Ethnic Code-Switching Dataset](https://arxiv.org/abs/2506.00087)
*Peng Xie,Xingyuan Liu,Tsz Wai Chan,Yequan Bie,Yangqiu Song,Yang Wang,Hao Chen,Kani Chen*

Main category: cs.CL

TL;DR: 提出多智能体框架LinguaMaster构建大规模多语言代码转换数据集SwitchLingua，并提出语义感知评估指标SAER


<details>
  <summary>Details</summary>
Motivation: 现有代码转换数据集局限于双语混合且缺乏多样性，无法满足多语言ASR/TTS/CLIR等应用需求

Method: 采用多智能体协作框架合成数据，收集42万文本样本和80+小时多民族语音数据（覆盖12语种、18国家、63种族）

Result: 创建首个大规模多民族代码转换数据集SwitchLingua，并提出整合语义信息的SAER评估指标

Conclusion: 该数据集填补了多语言研究资源空白，SAER指标为代码转换系统提供更准确的性能评估标准

Abstract: Code-switching (CS) is the alternating use of two or more languages within a
conversation or utterance, often influenced by social context and speaker
identity. This linguistic phenomenon poses challenges for Automatic Speech
Recognition (ASR) systems, which are typically designed for a single language
and struggle to handle multilingual inputs. The growing global demand for
multilingual applications, including Code-Switching ASR (CSASR), Text-to-Speech
(CSTTS), and Cross-Lingual Information Retrieval (CLIR), highlights the
inadequacy of existing monolingual datasets.
  Although some code-switching datasets exist, most are limited to bilingual
mixing within homogeneous ethnic groups, leaving a critical need for a
large-scale, diverse benchmark akin to ImageNet in computer vision.
  To bridge this gap, we introduce \textbf{LinguaMaster}, a multi-agent
collaboration framework specifically designed for efficient and scalable
multilingual data synthesis. Leveraging this framework, we curate
\textbf{SwitchLingua}, the first large-scale multilingual and multi-ethnic
code-switching dataset, including: (1) 420K CS textual samples across 12
languages, and (2) over 80 hours of audio recordings from 174 speakers
representing 18 countries/regions and 63 racial/ethnic backgrounds, based on
the textual data. This dataset captures rich linguistic and cultural diversity,
offering a foundational resource for advancing multilingual and multicultural
research. Furthermore, to address the issue that existing ASR evaluation
metrics lack sensitivity to code-switching scenarios, we propose the
\textbf{Semantic-Aware Error Rate (SAER)}, a novel evaluation metric that
incorporates semantic information, providing a more accurate and context-aware
assessment of system performance.

</details>


### [13] [HD-NDEs: Neural Differential Equations for Hallucination Detection in LLMs](https://arxiv.org/abs/2506.00088)
*Qing Li,Jiahui Geng,Zongxiong Chen,Derui Zhu,Yuxia Wang,Congbo Ma,Chenyang Lyu,Fakhri Karray*

Main category: cs.CL

TL;DR: 提出HD-NDEs方法，利用神经微分方程建模LLM潜在空间动态，有效检测幻觉问题，在True-False数据集AUC-ROC指标上超越现有技术14%


<details>
  <summary>Details</summary>
Motivation: 现有基于分类的幻觉检测方法（如SAPLMA）对序列中早期/中期非事实信息检测能力不足，影响实际可靠性

Method: 将神经微分方程应用于LLM潜在空间动态建模，通过潜在空间序列映射到分类空间实现真实性评估

Result: 在5个数据集和6个主流LLM上的实验显示，True-False数据集AUC-ROC指标相对SOTA提升超14%

Conclusion: HD-NDEs通过系统性捕获潜在空间动态，显著提升幻觉检测性能，为LLM可靠性部署提供新解决方案

Abstract: In recent years, large language models (LLMs) have made remarkable
advancements, yet hallucination, where models produce inaccurate or non-factual
statements, remains a significant challenge for real-world deployment. Although
current classification-based methods, such as SAPLMA, are highly efficient in
mitigating hallucinations, they struggle when non-factual information arises in
the early or mid-sequence of outputs, reducing their reliability. To address
these issues, we propose Hallucination Detection-Neural Differential Equations
(HD-NDEs), a novel method that systematically assesses the truthfulness of
statements by capturing the full dynamics of LLMs within their latent space.
Our approaches apply neural differential equations (Neural DEs) to model the
dynamic system in the latent space of LLMs. Then, the sequence in the latent
space is mapped to the classification space for truth assessment. The extensive
experiments across five datasets and six widely used LLMs demonstrate the
effectiveness of HD-NDEs, especially, achieving over 14% improvement in AUC-ROC
on the True-False dataset compared to state-of-the-art techniques.

</details>


### [14] [Writing-Zero: Bridge the Gap Between Non-verifiable Problems and Verifiable Rewards](https://arxiv.org/abs/2506.00103)
*Xun Lu*

Main category: cs.CL

TL;DR: 提出基于可验证奖励的强化学习新范式(RLVR)，通过生成式奖励模型(GenRM)和自举相对策略优化(BRPO)解决非验证性任务(如创意写作)的奖励建模难题，突破传统标量奖励模型的泛化限制和奖励黑客问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类偏好的标量奖励模型在主观性任务中存在泛化能力差、易出现过度解释和长度偏差等奖励黑客现象，需建立可验证奖励机制统一验证性与非验证性任务。

Method: 1. 基于写作原则的成对生成奖励模型(GenRM)：通过自我原则性批判将主观评估转化为可验证奖励
2. 自举相对策略优化(BRPO)：在强化学习训练中利用组内rollout生成自举响应作为临时参考，实现动态无参考的成对比较

Result: Writing-Zero模型在没有监督微调的情况下展现出：
- 持续改进的写作能力
- 对标量奖励基线强抵抗力(奖励黑客减少)
- 在内部和开源写作基准测试中取得竞争力结果

Conclusion: 该研究实现了规则驱动、参考驱动和无参考奖励建模在RLVR框架下的统一，为构建适用于所有语言任务的通用化、可扩展强化学习范式奠定基础。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has enabled large
language models (LLMs) to achieve remarkable breakthroughs in reasoning tasks
with objective ground-truth answers, such as mathematics and code generation.
However, a significant gap remains for non-verifiable tasks, like creative
writing and open-ended dialogue, where quality assessment is inherently
subjective and lacks definitive references. Existing approaches for these
domains often rely on scalar reward models trained with human preferences,
which suffer from limited generalization and are prone to reward hacking, such
as over-explanation and length bias. In this work, we propose a unified
RLVR-based training paradigm that bridges the gap between non-verifiable tasks
and verifiable rewards. We introduce a writing-principle-based pairwise
Generative Reward Model (GenRM) and a novel Bootstrapped Relative Policy
Optimization (BRPO) algorithm. The pairwise writing GenRM leverages
self-principled critique to transform subjective assessments into reliable,
verifiable rewards, while BRPO enables dynamic, reference-free pairwise
comparison by leveraging a bootstrapped response as temporary reference from
within group rollouts during RL training. Our approach empowers LLMs to develop
robust writing capabilities without supervised fine-tuning, as demonstrated by
Writing-Zero, which shows consistent improvement and strong resistance to
reward hacking compared to scalar reward baselines. Furthermore, our method
achieves competitive results on both in-house and open-source writing
benchmarks. Our findings suggest the potential to unify rule-based,
reference-based, and reference-free reward modeling under the RLVR framework,
thus paving the way for a comprehensive and scalable RL training paradigm
applicable across all language tasks.

</details>


### [15] [Spurious Correlations and Beyond: Understanding and Mitigating Shortcut Learning in SDOH Extraction with Large Language Models](https://arxiv.org/abs/2506.00134)
*Fardin Ahsan Sakib,Ziwei Zhu,Karen Trister Grace,Meliha Yetisgen,Ozlem Uzuner*

Main category: cs.CL

TL;DR: 研究发现大语言模型在提取临床文本中的社会健康决定因素时，可能因表面线索导致虚假预测，并存在性别差异。通过提示工程等策略可减少误判。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在医疗健康领域提取社会健康决定因素（SDOH）时因依赖表面线索导致的预测不可靠问题，特别关注药物使用状态提取场景中的虚假预测和性别差异。

Method: 使用SHAC数据集的MIMIC部分，以药物使用状态提取为案例，分析模型对酒精/吸烟提及的虚假响应，并评估提示工程、思维链推理等缓解策略。

Result: 发现模型会因酒精/吸烟的文本提及错误预测药物使用状态，同时存在性别性能差异。实验表明提示工程和思维链策略能有效减少假阳性。

Conclusion: 强调需通过优化提示策略提升LLM在医疗领域的可靠性，为减少健康数据分析中的模型偏差提供了实践方向。

Abstract: Social determinants of health (SDOH) extraction from clinical text is
critical for downstream healthcare analytics. Although large language models
(LLMs) have shown promise, they may rely on superficial cues leading to
spurious predictions. Using the MIMIC portion of the SHAC (Social History
Annotation Corpus) dataset and focusing on drug status extraction as a case
study, we demonstrate that mentions of alcohol or smoking can falsely induce
models to predict current/past drug use where none is present, while also
uncovering concerning gender disparities in model performance. We further
evaluate mitigation strategies - such as prompt engineering and
chain-of-thought reasoning - to reduce these false positives, providing
insights into enhancing LLM reliability in health domains.

</details>


### [16] [LaMP-QA: A Benchmark for Personalized Long-form Question Answering](https://arxiv.org/abs/2506.00137)
*Alireza Salemi,Hamed Zamani*

Main category: cs.CL

TL;DR: 论文提出LaMP-QA基准测试用于评估个性化长答案生成，实验显示引入个性化上下文可使性能提升达39%。


<details>
  <summary>Details</summary>
Motivation: 现有研究因缺乏训练/评估资源，在问答系统个性化生成领域存在不足，需构建标准化基准推动发展。

Method: 创建覆盖3大类45+子类别的LaMP-QA数据集，通过人工与自动评估对比策略，并测试开源/商业LLM的个性化方案。

Result: 使用个性化上下文后模型性能最大提升39%，该基准已开源以支持后续研究。

Conclusion: LaMP-QA填补了个性化问答评估资源空白，证实个性化信息对生成质量的关键作用，为领域研究提供基础设施。

Abstract: Personalization is essential for question answering systems that are
user-centric. Despite its importance, personalization in answer generation has
been relatively underexplored. This is mainly due to lack of resources for
training and evaluating personalized question answering systems. We address
this gap by introducing LaMP-QA -- a benchmark designed for evaluating
personalized long-form answer generation. The benchmark covers questions from
three major categories: (1) Arts & Entertainment, (2) Lifestyle & Personal
Development, and (3) Society & Culture, encompassing over 45 subcategories in
total. To assess the quality and potential impact of the LaMP-QA benchmark for
personalized question answering, we conduct comprehensive human and automatic
evaluations, to compare multiple evaluation strategies for evaluating generated
personalized responses and measure their alignment with human preferences.
Furthermore, we benchmark a number of non-personalized and personalized
approaches based on open-source and proprietary large language models (LLMs).
Our results show that incorporating the personalized context provided leads to
performance improvements of up to 39%. The benchmark is publicly released to
support future research in this area.

</details>


### [17] [Vedavani: A Benchmark Corpus for ASR on Vedic Sanskrit Poetry](https://arxiv.org/abs/2506.00145)
*Sujeet Kumar,Pretam Ray,Abhinay Beerukuri,Shrey Kamoji,Manoj Balaji Jagadeeshan,Pawan Goyal*

Main category: cs.CL

TL;DR: 首个针对梵语吠陀诗歌的54小时ASR数据集Vedavani，IndicWhisper模型表现最佳


<details>
  <summary>Details</summary>
Motivation: 梵语因音位复杂性及诗歌韵律特征缺乏ASR研究，需开发能捕捉诗歌韵律的语音识别系统

Method: 构建含30,779个吠陀诗歌音频样本的数据集，并在多语言语音模型上开展基准测试

Result: 实验表明IndicWhisper在多种SOTA模型中表现最优

Conclusion: 该研究填补梵语ASR空白，数据集精准捕捉韵律特征，为后续研究提供基准

Abstract: Sanskrit, an ancient language with a rich linguistic heritage, presents
unique challenges for automatic speech recognition (ASR) due to its phonemic
complexity and the phonetic transformations that occur at word junctures,
similar to the connected speech found in natural conversations. Due to these
complexities, there has been limited exploration of ASR in Sanskrit,
particularly in the context of its poetic verses, which are characterized by
intricate prosodic and rhythmic patterns. This gap in research raises the
question: How can we develop an effective ASR system for Sanskrit, particularly
one that captures the nuanced features of its poetic form? In this study, we
introduce Vedavani, the first comprehensive ASR study focused on Sanskrit Vedic
poetry. We present a 54-hour Sanskrit ASR dataset, consisting of 30,779
labelled audio samples from the Rig Veda and Atharva Veda. This dataset
captures the precise prosodic and rhythmic features that define the language.
We also benchmark the dataset on various state-of-the-art multilingual speech
models.$^{1}$ Experimentation revealed that IndicWhisper performed the best
among the SOTA models.

</details>


### [18] [Werewolf: A Straightforward Game Framework with TTS for Improved User Engagement](https://arxiv.org/abs/2506.00160)
*Qihui Fan,Enfu Nan,Wenbo Li,Lei Lu,Pu Zhao,Yanzhi Wang*

Main category: cs.CL

TL;DR: 提出基于LLM和TTS的狼人杀游戏系统，通过简化架构提升模型兼容性和用户参与度，论证增强的LLM推理能力使额外组件变得冗余


<details>
  <summary>Details</summary>
Motivation: 现有文本版狼人杀系统依赖复杂组件（微调/提示工程/经验池），而LLM推理能力的持续提升为简化系统设计同时增强用户体验提供了可能

Method: 开发与多种LLM兼容的集成系统，结合调优的文本转语音模型，消除传统方案中的附加组件

Result: 系统验证了在LLM持续进化背景下，仅通过基础架构优化即可实现更流畅的多模型兼容性和沉浸式游戏体验

Conclusion: 随着LLM推理能力提升，狼人杀类社交推理游戏系统可逐步去除复杂附加组件，通过精简架构有效提升用户体验

Abstract: The growing popularity of social deduction game systems for both business
applications and AI research has greatly benefited from the rapid advancements
in Large Language Models (LLMs), which now demonstrate stronger reasoning and
persuasion capabilities. Especially with the raise of DeepSeek R1 and V3
models, LLMs should enable a more engaging experience for human players in
LLM-agent-based social deduction games like Werewolf. Previous works either
fine-tuning, advanced prompting engineering, or additional experience pool to
achieve engaging text-format Werewolf game experience. We propose a novel yet
straightforward LLM-based Werewolf game system with tuned Text-to-Speech(TTS)
models designed for enhanced compatibility with various LLM models, and
improved user engagement. We argue with ever enhancing LLM reasoning, extra
components will be unnecessary in the case of Werewolf.

</details>


### [19] [Let Them Down Easy! Contextual Effects of LLM Guardrails on User Perceptions and Preferences](https://arxiv.org/abs/2506.00195)
*Mingqian Zheng,Wenjia Hu,Patrick Zhao,Motahhare Eslami,Jena D. Hwang,Faeze Brahman,Carolyn Rose,Maarten Sap*

Main category: cs.CL

TL;DR: 研究表明，部分满足（提供通用信息但规避风险细节）是最优的AI安全策略，相比直接拒绝可减少50%以上负面体验。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型对潜在有害请求一律拒绝，导致安全性与用户体验间的矛盾。本文旨在探索不同拒绝策略对用户感知的影响。

Method: 通过480人参与的3840组查询-响应评估实验，结合对9个前沿大模型和6个奖励模型的分析，对比不同拒绝策略效果。

Result: 1. 响应策略主导用户体验（用户动机影响微弱）
2. 部分满足策略负面感知最低
3. 现有模型自然输出中极少使用该策略
4. 奖励模型普遍低估其价值

Conclusion: AI安全机制应聚焦设计精细的拒绝策略而非单纯检测恶意意图，通过部分满足等策略实现安全与用户体验的双重保障。

Abstract: Current LLMs are trained to refuse potentially harmful input queries
regardless of whether users actually had harmful intents, causing a tradeoff
between safety and user experience. Through a study of 480 participants
evaluating 3,840 query-response pairs, we examine how different refusal
strategies affect user perceptions across varying motivations. Our findings
reveal that response strategy largely shapes user experience, while actual user
motivation has negligible impact. Partial compliance -- providing general
information without actionable details -- emerges as the optimal strategy,
reducing negative user perceptions by over 50% to flat-out refusals.
Complementing this, we analyze response patterns of 9 state-of-the-art LLMs and
evaluate how 6 reward models score different refusal strategies, demonstrating
that models rarely deploy partial compliance naturally and reward models
currently undervalue it. This work demonstrates that effective guardrails
require focusing on crafting thoughtful refusals rather than detecting intent,
offering a path toward AI safety mechanisms that ensure both safety and
sustained user engagement.

</details>


### [20] [Structuring Radiology Reports: Challenging LLMs with Lightweight Models](https://arxiv.org/abs/2506.00200)
*Johannes Moll,Louisa Fay,Asfandyar Azhar,Sophie Ostmeier,Tim Lueth,Sergios Gatidis,Curtis Langlotz,Jean-Benoit Delbrouck*

Main category: cs.CL

TL;DR: 轻量级编码器-解码器模型在结构化放射学报告中展现出超越大型语言模型的效能与可持续性优势


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）在医疗场景中存在的计算资源消耗高、透明度不足和数据隐私风险等问题

Method: 使用T5和BERT2BERT轻量模型（<3亿参数）处理MIMIC-CXR和CheXpert Plus数据集，并与8个开源LLMs（1B-70B）进行对比测试，应用前缀提示、上下文学习（ICL）和低秩适应（LoRA）微调技术

Result: 轻量模型在人工标注测试集上优于所有基于提示工程的LLMs，部分LoRA微调的LLMs在Findings章节仅取得有限提升（BLEU提升6.4%，ROUGE-L提升4.8%），但需付出400倍以上的推理时间、成本和碳排放

Conclusion: 轻量级任务专用模型为资源受限的医疗环境提供了可持续、隐私友好的临床文本结构化解决方案

Abstract: Radiology reports are critical for clinical decision-making but often lack a
standardized format, limiting both human interpretability and machine learning
(ML) applications. While large language models (LLMs) have shown strong
capabilities in reformatting clinical text, their high computational
requirements, lack of transparency, and data privacy concerns hinder practical
deployment. To address these challenges, we explore lightweight encoder-decoder
models (<300M parameters)-specifically T5 and BERT2BERT-for structuring
radiology reports from the MIMIC-CXR and CheXpert Plus datasets. We benchmark
these models against eight open-source LLMs (1B-70B), adapted using prefix
prompting, in-context learning (ICL), and low-rank adaptation (LoRA)
finetuning. Our best-performing lightweight model outperforms all LLMs adapted
using prompt-based techniques on a human-annotated test set. While some
LoRA-finetuned LLMs achieve modest gains over the lightweight model on the
Findings section (BLEU 6.4%, ROUGE-L 4.8%, BERTScore 3.6%, F1-RadGraph 1.1%,
GREEN 3.6%, and F1-SRR-BERT 4.3%), these improvements come at the cost of
substantially greater computational resources. For example, LLaMA-3-70B
incurred more than 400 times the inference time, cost, and carbon emissions
compared to the lightweight model. These results underscore the potential of
lightweight, task-specific models as sustainable and privacy-preserving
solutions for structuring clinical text in resource-constrained healthcare
settings.

</details>


### [21] [Structure-Aware Fill-in-the-Middle Pretraining for Code](https://arxiv.org/abs/2506.00204)
*Linyuan Gong,Alvin Cheung,Mostafa Elhoushi,Sida Wang*

Main category: cs.CL

TL;DR: 提出基于抽象语法树(AST)的AST-FIM预训练策略，通过掩码完整语法结构提升代码LLM的中间填充能力，在真实代码编辑任务中性能提升达5%。


<details>
  <summary>Details</summary>
Motivation: 现有LLM将代码视为纯文本进行随机字符掩码，无法有效捕捉代码结构特征。AST-FIM通过语法树掩码，使训练样例更符合通用代码结构和常见编辑模式（如代码块、表达式、函数）。

Method: 1. 利用抽象语法树(AST)规模化掩码完整语法结构
2. 构建Real-FIM-Eval评测基准（基于30,000+ GitHub提交，涵盖12种编程语言）

Result: 在1B/8B参数模型上，AST-FIM在标准FIM基准和真实代码编辑任务中均优于随机字符掩码方法，性能最高提升5个百分点。

Conclusion: AST-FIM显著提升代码填充任务效果，Real-FIM-Eval为真实场景提供有效评估。项目代码已开源。

Abstract: Fill-in-the-Middle (FIM) is a common pretraining method for code LLMs, where
models complete code segments given surrounding context. However, existing LLMs
treat code as plain text and mask random character spans. We propose and
evaluate AST-FIM, a pretraining strategy that leverages Abstract Syntax Trees
(ASTs) to mask complete syntactic structures at scale, ensuring coherent
training examples better aligned with universal code structures and common code
editing patterns such as blocks, expressions, or functions. To evaluate
real-world fill-in-the-middle (FIM) programming tasks, we introduce
Real-FIM-Eval, a benchmark derived from 30,000+ GitHub commits across 12
languages. On infilling tasks, experiments on 1B and 8B parameter models show
that AST-FIM is particularly beneficial for real-world code editing as it
outperforms standard random-character FIM by up to 5 pts on standard FIM
benchmarks. Our code is publicly available at
https://github.com/gonglinyuan/ast_fim.

</details>


### [22] [REIC: RAG-Enhanced Intent Classification at Scale](https://arxiv.org/abs/2506.00210)
*Ziji Zhang,Michael Yang,Zhiyu Chen,Yingying Zhuang,Shu-Ting Pi,Qun Liu,Rajashekar Maragoud,Vy Nguyen,Anurag Beniwal*

Main category: cs.CL

TL;DR: 论文提出REIC框架，利用检索增强生成技术(RAG)解决大规模客服场景意图分类的扩展性问题，无需频繁重训练即可实现精准分类。


<details>
  <summary>Details</summary>
Motivation: 企业产品线扩展导致意图数量和分类体系复杂度剧增，传统意图分类方法面临模型频繁重训练、跨业务领域适应性差等扩展性挑战。

Method: REIC框架通过检索增强生成技术，动态融合相关领域知识，构建上下文感知的意图分类系统，支持跨业务领域的知识迁移。

Result: 在真实业务数据测试中，REIC在领域内外场景均超越传统微调、零样本和小样本学习方法，准确率最高提升12.7%。

Conclusion: 该研究证明了检索增强生成技术在自适应意图分类系统中的有效性，为大规模商业场景部署提供了可扩展的解决方案。

Abstract: Accurate intent classification is critical for efficient routing in customer
service, ensuring customers are connected with the most suitable agents while
reducing handling times and operational costs. However, as companies expand
their product lines, intent classification faces scalability challenges due to
the increasing number of intents and variations in taxonomy across different
verticals. In this paper, we introduce REIC, a Retrieval-augmented generation
Enhanced Intent Classification approach, which addresses these challenges
effectively. REIC leverages retrieval-augmented generation (RAG) to dynamically
incorporate relevant knowledge, enabling precise classification without the
need for frequent retraining. Through extensive experiments on real-world
datasets, we demonstrate that REIC outperforms traditional fine-tuning,
zero-shot, and few-shot methods in large-scale customer service settings. Our
results highlight its effectiveness in both in-domain and out-of-domain
scenarios, demonstrating its potential for real-world deployment in adaptive
and large-scale intent classification systems.

</details>


### [23] [ComposeRAG: A Modular and Composable RAG for Corpus-Grounded Multi-Hop Question Answering](https://arxiv.org/abs/2506.00232)
*Ruofan Wu,Youngwon Lee,Fan Shu,Danmei Xu,Seung-won Hwang,Zhewei Yao,Yuxiong He,Feng Yan*

Main category: cs.CL

TL;DR: 提出模块化RAG框架ComposeRAG，通过解耦核心模块与自反思机制显著提升多跳QA任务的准确性（最高+15%）和答案忠实度（减少10%未接地回答）


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统存在模块紧耦合问题，导致可解释性差、系统性评估困难，特别是在复杂多跳问答场景中难以实现定向改进

Method: 1. 模块化架构：将流程分解为问题分解/查询改写/检索决策/答案验证等原子模块 2. 自反思机制：验证失败时迭代优化前期步骤 3. 结构化输入输出参数化设计

Result: 在4个多跳QA基准测试中：1. 准确率超微调方法15% 2. 强检索条件下忠实度提升3% 3. 低质量检索时未接地回答减少10%

Conclusion: ComposeRAG证明了模块化设计在提升RAG系统灵活性、可解释性和性能方面的有效性，特别在复杂推理场景中实现更好的答案忠实度与迭代优化能力

Abstract: Retrieval-Augmented Generation (RAG) systems are increasingly diverse, yet
many suffer from monolithic designs that tightly couple core functions like
query reformulation, retrieval, reasoning, and verification. This limits their
interpretability, systematic evaluation, and targeted improvement, especially
for complex multi-hop question answering. We introduce ComposeRAG, a novel
modular abstraction that decomposes RAG pipelines into atomic, composable
modules. Each module, such as Question Decomposition, Query Rewriting,
Retrieval Decision, and Answer Verification, acts as a parameterized
transformation on structured inputs/outputs, allowing independent
implementation, upgrade, and analysis. To enhance robustness against errors in
multi-step reasoning, ComposeRAG incorporates a self-reflection mechanism that
iteratively revisits and refines earlier steps upon verification failure.
Evaluated on four challenging multi-hop QA benchmarks, ComposeRAG consistently
outperforms strong baselines in both accuracy and grounding fidelity.
Specifically, it achieves up to a 15% accuracy improvement over
fine-tuning-based methods and up to a 5% gain over reasoning-specialized
pipelines under identical retrieval conditions. Crucially, ComposeRAG
significantly enhances grounding: its verification-first design reduces
ungrounded answers by over 10% in low-quality retrieval settings, and by
approximately 3% even with strong corpora. Comprehensive ablation studies
validate the modular architecture, demonstrating distinct and additive
contributions from each component. These findings underscore ComposeRAG's
capacity to deliver flexible, transparent, scalable, and high-performing
multi-hop reasoning with improved grounding and interpretability.

</details>


### [24] [MedOrch: Medical Diagnosis with Tool-Augmented Reasoning Agents for Flexible Extensibility](https://arxiv.org/abs/2506.00235)
*Yexiao He,Ang Li,Boyi Liu,Zhewei Yao,Yuxiong He*

Main category: cs.CL

TL;DR: 提出模块化医疗决策框架MedOrch，通过协调专用工具与推理代理实现多模态医疗数据处理，在阿尔茨海默病诊断等三项任务中验证其有效性


<details>
  <summary>Details</summary>
Motivation: 解决现有医疗AI系统在专业工具集成与推理透明度上的不足，通过模块化架构实现灵活的工具整合和可验证的决策过程

Method: 基于代理的模块化架构设计，支持领域工具即插即用，采用透明推理机制确保每个决策步骤可追溯

Result: 阿尔茨海默诊断准确率93.26%(提升4%+)，疾病进展预测50.35%，胸透分析Macro AUC 61.2%，多模态问答准确率54.47%

Conclusion: MedOrch证明了协调式推理框架在复杂医疗认知任务中的潜力，为临床决策支持系统提供了可扩展、可验证的AI解决方案

Abstract: Healthcare decision-making represents one of the most challenging domains for
Artificial Intelligence (AI), requiring the integration of diverse knowledge
sources, complex reasoning, and various external analytical tools. Current AI
systems often rely on either task-specific models, which offer limited
adaptability, or general language models without grounding with specialized
external knowledge and tools. We introduce MedOrch, a novel framework that
orchestrates multiple specialized tools and reasoning agents to provide
comprehensive medical decision support. MedOrch employs a modular, agent-based
architecture that facilitates the flexible integration of domain-specific tools
without altering the core system. Furthermore, it ensures transparent and
traceable reasoning processes, enabling clinicians to meticulously verify each
intermediate step underlying the system's recommendations. We evaluate MedOrch
across three distinct medical applications: Alzheimer's disease diagnosis,
chest X-ray interpretation, and medical visual question answering, using
authentic clinical datasets. The results demonstrate MedOrch's competitive
performance across these diverse medical tasks. Notably, in Alzheimer's disease
diagnosis, MedOrch achieves an accuracy of 93.26%, surpassing the
state-of-the-art baseline by over four percentage points. For predicting
Alzheimer's disease progression, it attains a 50.35% accuracy, marking a
significant improvement. In chest X-ray analysis, MedOrch exhibits superior
performance with a Macro AUC of 61.2% and a Macro F1-score of 25.5%. Moreover,
in complex multimodal visual question answering (Image+Table), MedOrch achieves
an accuracy of 54.47%. These findings underscore MedOrch's potential to advance
healthcare AI by enabling reasoning-driven tool utilization for multimodal
medical data processing and supporting intricate cognitive tasks in clinical
decision-making.

</details>


### [25] [PersianMedQA: Language-Centric Evaluation of LLMs in the Persian Medical Domain](https://arxiv.org/abs/2506.00250)
*Mohammad Javad Ranjbar Kalahroodi,Amirhossein Sheikholselami,Sepehr Karimi,Sepideh Ranjbar Kalahroodi,Heshaam Faili,Azadeh Shakery*

Main category: cs.CL

TL;DR: 研究构建了波斯医学问答数据集PersianMedQA，测试40+模型发现闭源通用模型表现最佳，波斯语微调模型表现较差，翻译对结果有文化敏感性，模型大小需结合领域适应。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在低资源语言（波斯语）医学领域的可靠性，填补现有研究空白。

Method: 创建多语言医疗数据集，测试通用/领域专用模型在零样本/思维链场景的性能，分析翻译对结果的影响。

Result: GPT-4波斯语准确率83.3%优于其他模型，波斯微调模型仅35.9%。英语性能更高但存在文化敏感优势案例。

Conclusion: 强调多语言评估必要性，证明单纯模型扩容不足，需结合领域/语言适配，PersianMedQA为后续研究提供基准。

Abstract: Large Language Models (LLMs) have achieved remarkable performance on a wide
range of NLP benchmarks, often surpassing human-level accuracy. However, their
reliability in high-stakes domains such as medicine, particularly in
low-resource languages, remains underexplored. In this work, we introduce
PersianMedQA, a large-scale, expert-validated dataset of multiple-choice
Persian medical questions, designed to evaluate LLMs across both Persian and
English. We benchmark over 40 state-of-the-art models, including
general-purpose, Persian fine-tuned, and medical LLMs, in zero-shot and
chain-of-thought (CoT) settings. Our results show that closed-source general
models (e.g., GPT-4.1) consistently outperform all other categories, achieving
83.3% accuracy in Persian and 80.7% in English, while Persian fine-tuned models
such as Dorna underperform significantly (e.g., 35.9% in Persian), often
struggling with both instruction-following and domain reasoning. We also
analyze the impact of translation, showing that while English performance is
generally higher, Persian responses are sometimes more accurate due to cultural
and clinical contextual cues. Finally, we demonstrate that model size alone is
insufficient for robust performance without strong domain or language
adaptation. PersianMedQA provides a foundation for evaluating multilingual and
culturally grounded medical reasoning in LLMs. The PersianMedQA dataset can be
accessed at:
https://huggingface.co/datasets/MohammadJRanjbar/PersianMedQA](https://huggingface.co/datasets/MohammadJRanjbar/PersianMedQA

</details>


### [26] [Aligned but Blind: Alignment Increases Implicit Bias by Reducing Awareness of Race](https://arxiv.org/abs/2506.00253)
*Lihao Sun,Chengzhi Mao,Valentin Hofmann,Xuechunzi Bai*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Although value-aligned language models (LMs) appear unbiased in explicit bias
evaluations, they often exhibit stereotypes in implicit word association tasks,
raising concerns about their fair usage. We investigate the mechanisms behind
this discrepancy and find that alignment surprisingly amplifies implicit bias
in model outputs. Specifically, we show that aligned LMs, unlike their
unaligned counterparts, overlook racial concepts in early internal
representations when the context is ambiguous. Not representing race likely
fails to activate safety guardrails, leading to unintended biases. Inspired by
this insight, we propose a new bias mitigation strategy that works by
incentivizing the representation of racial concepts in the early model layers.
In contrast to conventional mitigation methods of machine unlearning, our
interventions find that steering the model to be more aware of racial concepts
effectively mitigates implicit bias. Similar to race blindness in humans,
ignoring racial nuances can inadvertently perpetuate subtle biases in LMs.

</details>


### [27] [The Impact of Disability Disclosure on Fairness and Bias in LLM-Driven Candidate Selection](https://arxiv.org/abs/2506.00256)
*Mahammed Kamruzzaman,Gene Louis Kim*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在招聘筛选中对主动披露残疾状态的候选人存在系统性偏见，即使候选人资质完全相同，LLMs仍更倾向于选择无残疾者。


<details>
  <summary>Details</summary>
Motivation: 填补关于自愿披露信息对LLM驱动招聘公平性影响的研究空白，揭示残疾状态披露可能引发的算法偏见问题。

Method: 通过构建具有相同性别、种族、资历背景的候选人档案，在就业率差距最小的职位（如收银员、软件开发）中对比不同残疾状态披露情况下的LLM筛选结果。

Result: LLMs持续偏好未披露残疾的候选人，即使不披露残疾状态者获选率仍低于明确无残疾者。

Conclusion: LLM在招聘应用中的公平性保障机制亟待加强，需建立针对残疾状态等敏感信息的算法伦理审查体系。

Abstract: As large language models (LLMs) become increasingly integrated into hiring
processes, concerns about fairness have gained prominence. When applying for
jobs, companies often request/require demographic information, including
gender, race, and disability or veteran status. This data is collected to
support diversity and inclusion initiatives, but when provided to LLMs,
especially disability-related information, it raises concerns about potential
biases in candidate selection outcomes. Many studies have highlighted how
disability can impact CV screening, yet little research has explored the
specific effect of voluntarily disclosed information on LLM-driven candidate
selection. This study seeks to bridge that gap. When candidates shared
identical gender, race, qualifications, experience, and backgrounds, and sought
jobs with minimal employment rate gaps between individuals with and without
disabilities (e.g., Cashier, Software Developer), LLMs consistently favored
candidates who disclosed that they had no disability. Even in cases where
candidates chose not to disclose their disability status, the LLMs were less
likely to select them compared to those who explicitly stated they did not have
a disability.

</details>


### [28] [MultiHoax: A Dataset of Multi-hop False-Premise Questions](https://arxiv.org/abs/2506.00264)
*Mohammadamin Shafiei,Hamidreza Saffari,Nafise Sadat Moosavi*

Main category: cs.CL

TL;DR: 针对LLMs在多跳推理中检测错误前提的不足，提出MultiHoax基准测试，发现现有模型存在国家/知识类别/推理类型的检测缺陷，需改进错误前提检测与多跳推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准仅关注单跳错误前提问题，而现实推理需多步验证。需构建评估LLMs处理复杂多跳错误前提能力的标准化工具。

Method: 创建覆盖7个国家、10类知识领域的MultiHoax数据集，基于维基百科构建多跳推理任务，测试模型在不同区域事实推理中的表现。

Result: 实验表明顶尖LLMs在国家差异、知识多样性及多跳推理类型中普遍存在错误前提检测困难，暴露模型鲁棒性缺陷。

Conclusion: 需增强LLMs的错误前提识别能力和多跳推理的上下文一致性验证，推动更可靠的现实场景应用。

Abstract: As Large Language Models are increasingly deployed in high-stakes domains,
their ability to detect false assumptions and reason critically is crucial for
ensuring reliable outputs. False-premise questions (FPQs) serve as an important
evaluation method by exposing cases where flawed assumptions lead to incorrect
responses. While existing benchmarks focus on single-hop FPQs, real-world
reasoning often requires multi-hop inference, where models must verify
consistency across multiple reasoning steps rather than relying on
surface-level cues. To address this gap, we introduce MultiHoax, a benchmark
for evaluating LLMs' ability to handle false premises in complex, multi-step
reasoning tasks. Our dataset spans seven countries and ten diverse knowledge
categories, using Wikipedia as the primary knowledge source to enable factual
reasoning across regions. Experiments reveal that state-of-the-art LLMs
struggle to detect false premises across different countries, knowledge
categories, and multi-hop reasoning types, highlighting the need for improved
false premise detection and more robust multi-hop reasoning capabilities in
LLMs.

</details>


### [29] [CASPER: A Large Scale Spontaneous Speech Dataset](https://arxiv.org/abs/2506.00267)
*Cihan Xiao,Ruixing Liang,Xiangyu Zhang,Mehmet Emre Tiryaki,Veronica Bae,Lavanya Shankar,Rong Yang,Ethan Poon,Emmanuel Dupoux,Sanjeev Khudanpur,Leibny Paola Garcia Perera*

Main category: cs.CL

TL;DR: 针对自发语音数据匮乏问题，提出新型自然对话采集框架并发布200+小时数据集


<details>
  <summary>Details</summary>
Motivation: 现有语音数据集多为剧本化对话，制约语音处理技术发展，需解决高质量自发语音数据短缺问题

Method: 开发自然对话采集技术框架，通过情境引导促进多样化话题的流畅自然对话，区别于传统脚本式采集方法

Result: 发布包含200+小时自发语音的Stage 1数据集，建立可复现的标准化数据采集框架

Conclusion: 该数据集填补自发语音资源缺口，未来将持续扩展，为语音研究提供重要基础设施

Abstract: The success of large language models has driven interest in developing
similar speech processing capabilities. However, a key challenge is the
scarcity of high-quality spontaneous speech data, as most existing datasets
contain scripted dialogues. To address this, we present a novel pipeline for
eliciting and recording natural dialogues and release our Stage 1 dataset with
200+ hours of spontaneous speech. Our approach fosters fluid, natural
conversations while encouraging a diverse range of topics and interactive
exchanges. Unlike traditional methods, it facilitates genuine interactions,
providing a reproducible framework for future data collection. This paper
introduces our dataset and methodology, laying the groundwork for addressing
the shortage of spontaneous speech data. We plan to expand this dataset in
future stages, offering a growing resource for the research community.

</details>


### [30] [Hierarchical Level-Wise News Article Clustering via Multilingual Matryoshka Embeddings](https://arxiv.org/abs/2506.00277)
*Hans W. A. Hanley,Zakir Durumeric*

Main category: cs.CL

TL;DR: 提出基于多语言Matryoshka嵌入的新闻聚类方法，实现层次化主题建模


<details>
  <summary>Details</summary>
Motivation: 现有LLM主题建模方法存在扩展性差、相似度度量不透明、多语言支持不足的问题

Method: 1. 训练多语言Matryoshka嵌入模型（不同维度对应不同粒度语义）
2. 开发层次化聚类算法利用嵌入特性

Result: 在SemEval 2022 Task 8测试集取得SOTA（Pearson ρ=0.816），有效聚类真实新闻数据中的故事/叙事/主题

Conclusion: 该方法通过层次化嵌入实现可扩展、可解释的多语言内容聚类，解决了现有方法的核心痛点

Abstract: Contextual large language model embeddings are increasingly utilized for
topic modeling and clustering. However, current methods often scale poorly,
rely on opaque similarity metrics, and struggle in multilingual settings. In
this work, we present a novel, scalable, interpretable, hierarchical, and
multilingual approach to clustering news articles and social media data. To do
this, we first train multilingual Matryoshka embeddings that can determine
story similarity at varying levels of granularity based on which subset of the
dimensions of the embeddings is examined. This embedding model achieves
state-of-the-art performance on the SemEval 2022 Task 8 test dataset (Pearson
$\rho$ = 0.816). Once trained, we develop an efficient hierarchical clustering
algorithm that leverages the hierarchical nature of Matryoshka embeddings to
identify unique news stories, narratives, and themes. We conclude by
illustrating how our approach can identify and cluster stories, narratives, and
overarching themes within real-world news datasets.

</details>


### [31] [Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation](https://arxiv.org/abs/2506.00288)
*Ahmed Elhady,Eneko Agirre,Mikel Artetxe*

Main category: cs.CL

TL;DR: 研究发现，在跨语言持续预训练（CPT）中加入英语数据虽不影响验证困惑度，但对目标语言下游任务能力至关重要。通过课程学习和EMA权重优化可有效减少对英语数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 探究在多语言模型适配新语言时，混合训练数据中加入英语的作用机制及其对下游任务能力的影响。

Method: 1. 构建语言无关的上下文学习基准测试
2. 对比分析包含/不包含英语数据的CPT过程
3. 提出课程学习策略和指数移动平均（EMA）权重优化方法

Result: 不含英语数据会导致：
- 早期训练出现灾难性遗忘
- 下游提示泛化能力受损（困惑度指标）
- 模型参数发生显著偏移
课程学习与EMA可缓解上述问题

Conclusion: 英语数据通过维持涌现能力发挥关键作用，提出的替代方案为设计更高效的语言适配方法奠定基础。研究揭示了CPT过程中能力涌现的动态机制，对优化多语言模型迁移具有指导意义。

Abstract: Continued pretraining (CPT) is a popular approach to adapt existing large
language models (LLMs) to new languages. When doing so, it is common practice
to include a portion of English data in the mixture, but its role has not been
carefully studied to date. In this work, we show that including English does
not impact validation perplexity, yet it is critical for the emergence of
downstream capabilities in the target language. We introduce a
language-agnostic benchmark for in-context learning (ICL), which reveals
catastrophic forgetting early on CPT when English is not included. This in turn
damages the ability of the model to generalize to downstream prompts in the
target language as measured by perplexity, even if it does not manifest in
terms of accuracy until later in training, and can be tied to a big shift in
the model parameters. Based on these insights, we introduce curriculum learning
and exponential moving average (EMA) of weights as effective alternatives to
mitigate the need for English. All in all, our work sheds light into the
dynamics by which emergent abilities arise when doing CPT for language
adaptation, and can serve as a foundation to design more effective methods in
the future.

</details>


### [32] [DLM-One: Diffusion Language Models for One-Step Sequence Generation](https://arxiv.org/abs/2506.00290)
*Tianqi Chen,Shujian Zhang,Mingyuan Zhou*

Main category: cs.CL

TL;DR: 提出基于分数蒸馏的DLM-One框架，通过师生模型对齐实现单步扩散生成，在保持性能的同时获得500倍加速


<details>
  <summary>Details</summary>
Motivation: 解决连续扩散语言模型迭代采样导致的推理效率低下问题，探索嵌入空间单步生成的可能性

Method: 构建师生模型分数对齐机制，在词嵌入空间蒸馏预训练教师模型的分数函数到单步生成的学生模型

Result: 在DiffuSeq基准上实现500倍推理加速，文本生成质量与原始迭代模型相当

Conclusion: 单步扩散为高效高质量语言生成开辟新路径，推动嵌入空间扩散模型在NLP领域的实际应用

Abstract: This paper introduces DLM-One, a score-distillation-based framework for
one-step sequence generation with continuous diffusion language models (DLMs).
DLM-One eliminates the need for iterative refinement by aligning the scores of
a student model's outputs in the continuous token embedding space with the
score function of a pretrained teacher DLM. We investigate whether DLM-One can
achieve substantial gains in sampling efficiency for language modeling. Through
comprehensive experiments on DiffuSeq -- a representative continuous DLM -- we
show that DLM-One achieves up to ~500x speedup in inference time while
maintaining competitive performance on benchmark text generation tasks used to
evaluate the teacher models. We further analyze the method's empirical behavior
across multiple datasets, providing initial insights into its generality and
practical applicability. Our findings position one-step diffusion as a
promising direction for efficient, high-quality language generation and broader
adoption of continuous diffusion models operating in embedding space for
natural language processing.

</details>


### [33] [Can LLMs Understand Unvoiced Speech? Exploring EMG-to-Text Conversion with LLMs](https://arxiv.org/abs/2506.00304)
*Payal Mohapatra,Akash Pandey,Xiaoyuan Zhang,Qi Zhu*

Main category: cs.CL

TL;DR: 提出一种新型EMG适配器模块，将肌电信号特征映射到大型语言模型（LLM）输入空间，在无声EMG转文本任务中实现0.49的平均词错率，数据需求仅6分钟时性能仍超越专用模型20%


<details>
  <summary>Details</summary>
Motivation: 现有无声肌电语音转换方案依赖有声/无声EMG配对数据，这对失声患者不现实。利用LLM在多模态理解（如音频）的扩展能力，探索其理解无声肌电信号的可行性

Method: 设计EMG适配器模块实现肌电特征到LLM输入空间的映射，在封闭词汇表无声EMG转文本任务上进行验证，采用极低数据量（6分钟）训练策略

Result: 平均词错率0.49，数据量仅6分钟时性能超越专用模型近20%，验证LLM处理生物电信号的可行性

Conclusion: 首次实现LLM通过表面肌电信号理解无声语音的重要突破，为失语症患者提供新的沟通技术支持，尽管生物电信号理解仍具挑战性

Abstract: Unvoiced electromyography (EMG) is an effective communication tool for
individuals unable to produce vocal speech. However, most prior methods rely on
paired voiced and unvoiced EMG signals, along with speech data, for EMG-to-text
conversion, which is not practical for such individuals. Given the rise of
large language models (LLMs) in speech recognition, we explore their potential
to understand unvoiced speech. To this end, we address the challenge of
learning from unvoiced EMG alone and propose a novel EMG adaptor module that
maps EMG features into an LLM's input space, achieving an average word error
rate (WER) of 0.49 on a closed-vocabulary unvoiced EMG-to-text task. Even with
a conservative data availability of just six minutes, our approach improves
performance over specialized models by nearly 20%. While LLMs have been shown
to be extendable to new language modalities -- such as audio -- understanding
articulatory biosignals like unvoiced EMG remains more challenging. This work
takes a crucial first step toward enabling LLMs to comprehend unvoiced speech
using surface EMG.

</details>


### [34] [Lossless Token Sequence Compression via Meta-Tokens](https://arxiv.org/abs/2506.00307)
*John Harvill,Ziwei Fan,Hao Wang,Yizhou Sun,Hao Ding,Luke Huan,Anoop Deoras*

Main category: cs.CL

TL;DR: 提出基于LZ77的无损提示压缩技术，在保持语义完整性的前提下显著减少LLM输入序列长度和计算量


<details>
  <summary>Details</summary>
Motivation: 现有有损压缩方法可能丢失关键语义信息，需开发能严格保持语义/句法结构的无损压缩方案

Method: 采用类似LZ77的字典编码技术，设计可逆的token序列转换方法，适配Transformer架构的二次方计算特性

Result: 平均压缩率27%/18%，对应计算量减少47%/33%；在语义敏感任务上性能差距仅为0.7-1.2个百分点

Conclusion: 无损压缩在保持语义完整性的同时实现显著效率提升，模型规模扩大后可能完全消除性能差距

Abstract: Existing work on prompt compression for Large Language Models (LLM) focuses
on lossy methods that try to maximize the retention of semantic information
that is relevant to downstream tasks while significantly reducing the sequence
length. In this paper, we introduce a task-agnostic lossless compression
technique similar to LZ77 that makes it possible to reduce the input token
sequence length on average by 27\% and 18\% for the two evaluation tasks
explored here. Given that we use transformer-based LLMs, this equates to 47\%
and 33\% less encoding computation, respectively, due to the quadratic nature
of attention. The token sequence transformation is trivial to reverse and
highlights that no semantic information is lost in the process. We evaluate our
proposed approach on two tasks that require strict preservation of
semantics/syntax and demonstrate that existing lossy compression methods
perform poorly in this setting. We find that our lossless compression technique
produces only a small gap in performance compared to using the uncompressed
input and posit that larger models and an expanded computing budget would
likely erase the gap entirely.

</details>


### [35] [An evaluation of LLMs for generating movie reviews: GPT-4o, Gemini-2.0 and DeepSeek-V3](https://arxiv.org/abs/2506.00312)
*Brendan Sands,Yining Wang,Chenhao Xu,Yuxuan Zhou,Lai Wei,Rohitash Chandra*

Main category: cs.CL

TL;DR: 研究评估GPT-4o、DeepSeek-V3和Gemini-2.0生成电影评论的能力，发现其与IMDb用户评论存在情感丰富度和风格连贯性差距


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在电影评论生成领域的适用性，验证以字幕/剧本为输入生成影评的效果

Method: 使用电影字幕和剧本作为输入，通过词汇分析、情感极性检测、文本相似度比较和主题一致性评估，结合用户调查问卷进行多维度评测

Result: LLM生成评论结构完整但情感单薄（DeepSeek-V3最接近真实评论，GPT-4o情感过正向，Gemini-2.0负面情感突出但强度失控）

Conclusion: 大语言模型具备生成合格影评的基础能力，但需优化情感表达的自然度和风格一致性以缩小与人类创作的差距

Abstract: Large language models (LLMs) have been prominent in various tasks, including
text generation and summarisation. The applicability of LLMs to the generation
of product reviews is gaining momentum, paving the way for the generation of
movie reviews. In this study, we propose a framework that generates movie
reviews using three LLMs (GPT-4o, DeepSeek-V3, and Gemini-2.0), and evaluate
their performance by comparing the generated outputs with IMDb user reviews. We
use movie subtitles and screenplays as input to the LLMs and investigate how
they affect the quality of reviews generated. We review the LLM-based movie
reviews in terms of vocabulary, sentiment polarity, similarity, and thematic
consistency in comparison to IMDB user reviews. The results demonstrate that
LLMs are capable of generating syntactically fluent and structurally complete
movie reviews. Nevertheless, there is still a noticeable gap in emotional
richness and stylistic coherence between LLM-generated and IMDb reviews,
suggesting that further refinement is needed to improve the overall quality of
movie review generation. We provided a survey-based analysis where participants
were told to distinguish between LLM and IMDb user reviews. The results show
that LLM-generated reviews are difficult to distinguish from IMDB user reviews.
We found that DeepSeek-V3 produced the most balanced reviews, closely matching
IMDb reviews. GPT-4o overemphasised positive emotions, while Gemini-2.0
captured negative emotions better but showed excessive emotional intensity.

</details>


### [36] [SkillVerse : Assessing and Enhancing LLMs with Tree Evaluation](https://arxiv.org/abs/2506.00319)
*Yufei Tian,Jiao Sun,Nanyun Peng,Zizhao Zhang*

Main category: cs.CL

TL;DR: 提出SkillVerse树状诊断框架，通过层次化分析模型技能熟练度，改进上下文学习效果25%并提升弱点预测成功率至55%


<details>
  <summary>Details</summary>
Motivation: 现有评估方法难以捕捉语言模型处理复杂任务时的细粒度能力差异，需要新型诊断工具推动模型开发

Method: 利用LLM生成技能树状图（dendrogram），支持任意粒度分析模型行为，开发树搜索算法优化少样本示例选择

Result: 上下文学习准确率提升25%，模型弱点预测成功率55%（较基线提升22%）

Conclusion: SkillVerse为模型能力评估提供灵活框架，显著提升下游任务效果，助力精准定位模型缺陷

Abstract: As language models evolve to tackle complex, multifaceted tasks, their
evaluation must adapt to capture this intricacy. A granular, skill-specific
understanding of model capabilities can empower researchers to make informed
model development plans. In this paper, we introduce SkillVerse, an
unsupervised tree-structured diagnosis framework for understanding model
proficiency in specific abilities. With LLM as a judge, SkillVerse first
critiques the model responses, and then organizes them into a hierarchical
structure termed dendrogram. Given proficiency at arbitrary levels of
granularity, SkillVerse is flexible to produce insights of behaviors of modern
large models. We also demonstrate its efficacy in two downstream tasks: 1)
improving model in-context learning by 25% using a tree-search algorithm to
select more informative few-shot demonstrations, and 2) accurately predicting
new model weaknesses with a 55% success rate, 22% higher than without
SkillVerse.

</details>


### [37] [TreeRare: Syntax Tree-Guided Retrieval and Reasoning for Knowledge-Intensive Question Answering](https://arxiv.org/abs/2506.00331)
*Boyi Zhang,Zhuo Liu,Hangfeng He*

Main category: cs.CL

TL;DR: 提出TreeRare框架，通过语法树自底向上分解复杂问题，实现子组件检索与证据合成，显著提升多步推理问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有迭代检索方法存在推理误差累积和检索结果错位问题，需通过结构化分解机制降低不确定性。

Method: 基于组合性原则，沿语法树自底向上遍历：节点级子查询生成→精准段落检索→局部证据合成→树状证据聚合。

Result: 在5个需模糊/多跳推理的QA数据集上实现SOTA性能，准确率显著超越基线方法。

Conclusion: 语法树结构引导的层次化检索-推理框架，有效解决复杂问题中的误差传播问题，验证组合性思维在QA任务中的有效性。

Abstract: In real practice, questions are typically complex and knowledge-intensive,
requiring Large Language Models (LLMs) to recognize the multifaceted nature of
the question and reason across multiple information sources. Iterative and
adaptive retrieval, where LLMs decide when and what to retrieve based on their
reasoning, has been shown to be a promising approach to resolve complex,
knowledge-intensive questions. However, the performance of such retrieval
frameworks is limited by the accumulation of reasoning errors and misaligned
retrieval results. To overcome these limitations, we propose TreeRare (Syntax
Tree-Guided Retrieval and Reasoning), a framework that utilizes syntax trees to
guide information retrieval and reasoning for question answering. Following the
principle of compositionality, TreeRare traverses the syntax tree in a
bottom-up fashion, and in each node, it generates subcomponent-based queries
and retrieves relevant passages to resolve localized uncertainty. A
subcomponent question answering module then synthesizes these passages into
concise, context-aware evidence. Finally, TreeRare aggregates the evidence
across the tree to form a final answer. Experiments across five question
answering datasets involving ambiguous or multi-hop reasoning demonstrate that
TreeRare achieves substantial improvements over existing state-of-the-art
methods.

</details>


### [38] [Disentangling Codemixing in Chats: The NUS ABC Codemixed Corpus](https://arxiv.org/abs/2506.00332)
*Svetlana Churina,Akshat Gupta,Insyirah Mujtahid,Kokil Jaidka*

Main category: cs.CL

TL;DR: 首个标注型通用语码混合语料库Codemix Corpus，包含35万+跨语言消息，支持计算语言学与NLP研究


<details>
  <summary>Details</summary>
Motivation: 填补现有公开语料库空白，满足自然多语言对话建模需求，突破社交媒体场景下隐私合规限制

Method: 动态采集实时验证机制，结构化JSON格式存储，严格隐私过滤流程，覆盖英/普/多语混合模式

Result: 已构建含355,641条消息的多维度数据集，配备元数据及语言统计特征，持续更新中

Conclusion: 该语料库为跨语言交际研究、社会语言学分析及对话系统开发提供基准数据集

Abstract: Code-mixing involves the seamless integration of linguistic elements from
multiple languages within a single discourse, reflecting natural multilingual
communication patterns. Despite its prominence in informal interactions such as
social media, chat messages and instant-messaging exchanges, there has been a
lack of publicly available corpora that are author-labeled and suitable for
modeling human conversations and relationships. This study introduces the first
labeled and general-purpose corpus for understanding code-mixing in context
while maintaining rigorous privacy and ethical standards. Our live project will
continuously gather, verify, and integrate code-mixed messages into a
structured dataset released in JSON format, accompanied by detailed metadata
and linguistic statistics. To date, it includes over 355,641 messages spanning
various code-mixing patterns, with a primary focus on English, Mandarin, and
other languages. We expect the Codemix Corpus to serve as a foundational
dataset for research in computational linguistics, sociolinguistics, and NLP
applications.

</details>


### [39] [Beyond Context to Cognitive Appraisal: Emotion Reasoning as a Theory of Mind Benchmark for Large Language Models](https://arxiv.org/abs/2506.00334)
*Gerard Christopher Yeo,Kokil Jaidka*

Main category: cs.CL

TL;DR: 研究探讨大语言模型(LLMs)在心理理论框架下如何通过上下文推理情绪状态，发现其虽具备基础推理能力但缺乏情境与情绪的关联性，强调需结合心理学理论优化模型训练。


<details>
  <summary>Details</summary>
Motivation: 现有情感识别数据集依赖显性线索，难以捕捉隐含的上下文情感语义。研究旨在探索LLMs如何利用情境信息进行情绪推理，突破表层特征限制。

Method: 基于认知评估理论构建ToM专项数据集，设计前向推理(情境→情绪)和后向推理(情绪→情境)双路径评估框架，系统测试LLMs的情绪推理能力。

Result: LLMs表现出基础情绪推理能力，但在情境结果归因与具体情绪匹配(如认知评估要素与情绪类型关联)上存在显著缺陷。

Conclusion: 情感推理需深度融合心理学理论(如认知评估理论)，建议在LLMs训练与评估中加强情境关联性和情绪归因机制的设计。

Abstract: Datasets used for emotion recognition tasks typically contain overt cues that
can be used in predicting the emotions expressed in a text. However, one
challenge is that texts sometimes contain covert contextual cues that are rich
in affective semantics, which warrant higher-order reasoning abilities to infer
emotional states, not simply the emotions conveyed. This study advances beyond
surface-level perceptual features to investigate how large language models
(LLMs) reason about others' emotional states using contextual information,
within a Theory-of-Mind (ToM) framework. Grounded in Cognitive Appraisal
Theory, we curate a specialized ToM evaluation dataset1 to assess both forward
reasoning - from context to emotion- and backward reasoning - from emotion to
inferred context. We showed that LLMs can reason to a certain extent, although
they are poor at associating situational outcomes and appraisals with specific
emotions. Our work highlights the need for psychological theories in the
training and evaluation of LLMs in the context of emotion reasoning.

</details>


### [40] [OWSM v4: Improving Open Whisper-Style Speech Models via Data Scaling and Cleaning](https://arxiv.org/abs/2506.00338)
*Yifan Peng,Shakeel Muhammad,Yui Sudo,William Chen,Jinchuan Tian,Chyi-Jiunn Lin,Shinji Watanabe*

Main category: cs.CL

TL;DR: 通过整合清洗后的YODAS数据集，OWSM v4模型在多语言任务中显著超越前代版本，并匹敌工业级模型


<details>
  <summary>Details</summary>
Motivation: 解决原始OWSM系列模型训练数据不足的问题，利用网络爬取的开放语音数据提升模型性能

Method: 开发基于公共工具包的可扩展数据清洗流程，整合原始OWSM数据和清洗后的166,000小时YODAS数据训练新模型

Result: OWSM v4在多项多语言基准测试中表现优于前代，部分场景超越Whisper/MMS等工业模型

Conclusion: 验证了大规模开放数据整合的可能性，通过高效数据清洗技术可显著提升语音模型性能，推动开源语音社区发展

Abstract: The Open Whisper-style Speech Models (OWSM) project has developed a series of
fully open speech foundation models using academic-scale resources, but their
training data remains insufficient. This work enhances OWSM by integrating
YODAS, a large-scale web-crawled dataset with a Creative Commons license.
However, incorporating YODAS is nontrivial due to its wild nature, which
introduces challenges such as incorrect language labels and audio-text
misalignments. To address this, we develop a scalable data-cleaning pipeline
using public toolkits, yielding a dataset with 166,000 hours of speech across
75 languages. Our new series of OWSM v4 models, trained on this curated dataset
alongside existing OWSM data, significantly outperform previous versions on
multilingual benchmarks. Our models even match or surpass frontier industrial
models like Whisper and MMS in multiple scenarios. We will publicly release the
cleaned YODAS data, pre-trained models, and all associated scripts via the
ESPnet toolkit.

</details>


### [41] [Efficient Latent Semantic Clustering for Scaling Test-Time Computation of LLMs](https://arxiv.org/abs/2506.00344)
*Sungjae Lee,Hoyoung Kim,Jeongyeon Hwang,Eunhyeok Park,Jungseul Ok*

Main category: cs.CL

TL;DR: 提出隐式语义聚类（LSC）方法，通过利用LLM内部隐藏状态实现高效测试时扩展


<details>
  <summary>Details</summary>
Motivation: 现有基于外部模型的语义聚类方法存在计算开销大、缺乏上下文敏感性等问题

Method: 利用生成器LLM的隐藏状态进行上下文感知的语义聚类，无需外部模型

Result: 在保持/超越现有方法性能的同时，显著提升测试时扩展的计算效率

Conclusion: LSC为测试时计算扩展提供了轻量级、上下文敏感的语义聚类解决方案

Abstract: Scaling test-time computation--generating and analyzing multiple or
sequential outputs for a single input--has become a promising strategy for
improving the reliability and quality of large language models (LLMs), as
evidenced by advances in uncertainty quantification and multi-step reasoning. A
key shared component is semantic clustering, which groups outputs that differ
in form but convey the same meaning. Semantic clustering enables estimation of
the distribution over the semantics of outputs and helps avoid redundant
exploration of reasoning paths. However, existing approaches typically rely on
external models, which introduce substantial computational overhead and often
fail to capture context-aware semantics. We propose Latent Semantic Clustering
(LSC), a lightweight and context-sensitive method that leverages the generator
LLM's internal hidden states for clustering, eliminating the need for external
models. Our extensive experiment across various LLMs and datasets shows that
LSC significantly improves the computational efficiency of test-time scaling
while maintaining or exceeding the performance of existing methods.

</details>


### [42] [Neuro2Semantic: A Transfer Learning Framework for Semantic Reconstruction of Continuous Language from Human Intracranial EEG](https://arxiv.org/abs/2506.00381)
*Siavash Shams,Richard Antonello,Gavin Mischler,Stephan Bickel,Ashesh Mehta,Nima Mesgarani*

Main category: cs.CL

TL;DR: Neuro2Semantic框架通过两阶段方法从颅内脑电信号重建语义内容，在低数据量下实现优于现有方法的连续文本生成


<details>
  <summary>Details</summary>
Motivation: 解决神经信号解码连续语言的挑战，突破传统脑机接口在无约束文本生成方面的限制

Method: 1. LSTM适配器对齐神经信号与文本嵌入 2. 校正模块直接生成连续文本

Result: 仅需30分钟神经数据即超越现有方法，展示高效的低数据解码能力

Conclusion: 该框架为脑机接口和神经解码技术提供了可扩展的实用化解决方案

Abstract: Decoding continuous language from neural signals remains a significant
challenge in the intersection of neuroscience and artificial intelligence. We
introduce Neuro2Semantic, a novel framework that reconstructs the semantic
content of perceived speech from intracranial EEG (iEEG) recordings. Our
approach consists of two phases: first, an LSTM-based adapter aligns neural
signals with pre-trained text embeddings; second, a corrector module generates
continuous, natural text directly from these aligned embeddings. This flexible
method overcomes the limitations of previous decoding approaches and enables
unconstrained text generation. Neuro2Semantic achieves strong performance with
as little as 30 minutes of neural data, outperforming a recent state-of-the-art
method in low-data settings. These results highlight the potential for
practical applications in brain-computer interfaces and neural decoding
technologies.

</details>


### [43] [Adaptive-VP: A Framework for LLM-Based Virtual Patients that Adapts to Trainees' Dialogue to Facilitate Nurse Communication Training](https://arxiv.org/abs/2506.00386)
*Keyeun Lee,Seolhee Lee,Esther Hehsun Kim,Yena Ko,Jinsu Eun,Dahee Kim,Hyewon Cho,Haiyi Zhu,Robert E. Kraut,Eunyoung Suh,Eun-mee Kim,Hajin Lim*

Main category: cs.CL

TL;DR: 提出Adaptive-VP框架，利用大语言模型动态调整虚拟患者对话行为，实现护理沟通训练的适应性互动。


<details>
  <summary>Details</summary>
Motivation: 传统标准化患者模拟成本高且不灵活，现有虚拟患者系统无法根据学员沟通能力动态调整互动模式。

Method: 1. 构建基于临床场景的灵活VP对话框架
2. 模块化实时评估学员沟通技能
3. 通过LLM动态生成患者适应性响应
4. 建立学习者安全保护机制

Result: 护士实践语料自动评估显示沟通技能评价有效，专家确认其交互自然度显著优于现有方法。

Conclusion: Adaptive-VP为护理沟通培训提供了可扩展的创新解决方案，平衡临床严谨性与互动灵活性。

Abstract: Effective communication training is essential to preparing nurses for
high-quality patient care. While standardized patient (SP) simulations provide
valuable experiential learning, they are often costly and inflexible. Virtual
patient (VP) systems offer a scalable alternative, but most fail to adapt to
the varying communication skills of trainees. In particular, when trainees
respond ineffectively, VPs should escalate in hostility or become
uncooperative--yet this level of adaptive interaction remains largely
unsupported. To address this gap, we introduce Adaptive-VP, a VP dialogue
generation framework that leverages large language models (LLMs) to dynamically
adapt VP behavior based on trainee input. The framework features a pipeline for
constructing clinically grounded yet flexible VP scenarios and a modular system
for assessing trainee communication and adjusting VP responses in real time,
while ensuring learner safety. We validated Adaptive-VP by simulating
challenging patient conversations. Automated evaluation using a corpus from
practicing nurses showed that our communication skill evaluation mechanism
reflected real-world proficiency levels. Expert nurses further confirmed that
Adaptive-VP produced more natural and realistic interactions than existing
approaches, demonstrating its potential as a scalable and effective tool for
nursing communication training.

</details>


### [44] [SHARE: An SLM-based Hierarchical Action CorREction Assistant for Text-to-SQL](https://arxiv.org/abs/2506.00391)
*Ge Qu,Jinyang Li,Bowen Qin,Xiaolong Li,Nan Huo,Chenhao Ma,Reynold Cheng*

Main category: cs.CL

TL;DR: 提出SHARE框架——基于小型语言模型的分层动作校正系统，通过分解SQL动作为可解释轨迹并分层优化，显著提升文本到SQL自校正效率与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL自校正方法存在递归调用导致计算开销倍增，且LLM难以有效定位声明式SQL错误的根本缺陷。需要更细粒度的错误追踪机制和计算高效的校正方案。

Method: 1) 将SQL声明式查询转化为逐步动作轨迹揭示潜在推理路径 2) 部署三个专用SLM构成顺序处理管道 3) 提出分层自演化策略进行数据高效训练。

Result: 实验证明SHARE提升自校正能力且适配不同LLM，低资源场景下保持强性能（特别适用于数据隐私敏感场景）。

Conclusion: 通过动作轨迹分解与分层优化，SHARE为文本到SQL系统提供了高效可靠的错误校正框架，突破现有自校正方法的计算效率瓶颈与黑箱修正局限。

Abstract: Current self-correction approaches in text-to-SQL face two critical
limitations: 1) Conventional self-correction methods rely on recursive
self-calls of LLMs, resulting in multiplicative computational overhead, and 2)
LLMs struggle to implement effective error detection and correction for
declarative SQL queries, as they fail to demonstrate the underlying reasoning
path. In this work, we propose SHARE, an SLM-based Hierarchical Action
corREction assistant that enables LLMs to perform more precise error
localization and efficient correction. SHARE orchestrates three specialized
Small Language Models (SLMs) in a sequential pipeline, where it first
transforms declarative SQL queries into stepwise action trajectories that
reveal underlying reasoning, followed by a two-phase granular refinement. We
further propose a novel hierarchical self-evolution strategy for data-efficient
training. Experimental results demonstrate that SHARE effectively enhances
self-correction capabilities while proving robust across various LLMs.
Furthermore, our comprehensive analysis shows that SHARE maintains strong
performance even in low-resource training settings, which is particularly
valuable for text-to-SQL applications with data privacy constraints.

</details>


### [45] [Speculative Reward Model Boosts Decision Making Ability of LLMs Cost-Effectively](https://arxiv.org/abs/2506.00396)
*Jiawei Gu,Shangsong Liang*

Main category: cs.CL

TL;DR: 提出Speculative Reward Model框架，在保持决策效果的同时将计算成本降至原搜索框架的1/10


<details>
  <summary>Details</summary>
Motivation: 现有LLM决策方法过度追求性能提升而忽视计算效率，通过3E标准验证发现其常以大幅效率损失换取边际性能改善

Method: 1. 引入外部奖励分配器预测最优行动，减少LLM内部自评估依赖
2. 采用推测验证机制剪枝次优选择，引导搜索方向

Result: 在数学推理、规划及专业领域数值推理任务中验证，平均成本降低至原框架的1/10且保持效果

Conclusion: SRM框架实现效率与效果的平衡，为LLM决策优化提供即插即用的轻量化解决方案

Abstract: Effective decision-making in Large Language Models (LLMs) is essential for
handling intricate tasks. However, existing approaches prioritize performance
but often overlook the balance between effectiveness and computational cost. To
address this, we first introduce the 3E Criteria to systematically assess the
cost-effectiveness of search strategies, revealing that existing methods often
trade significant efficiency for marginal performance gains. To improve LLM
decision-making while maintaining efficiency, we propose the Speculative Reward
Model (SRM), a plug-and-play framework that seamlessly integrates with existing
search strategies. Specifically, SRM employs an external reward assigner to
predict optimal actions, reducing reliance on LLMs' internal self-evaluation.
And a speculative verification mechanism is used to prune suboptimal choices
and guide the search toward more promising steps. We evaluate SRM on several
complex decision-making tasks including mathematical reasoning, planning and
numerical reasoning in specialized domains. Experimental results show that SRM
reduces costs to 1/10 of the original search framework on average while
maintaining effectiveness.

</details>


### [46] [Scaling Textual Gradients via Sampling-Based Momentum](https://arxiv.org/abs/2506.00400)
*Zixin Ding,Junyuan Hong,Jiachen T. Wang,Zinan Lin,Zhangyang Wang,Yuxin Chen*

Main category: cs.CL

TL;DR: 提出TSGD-M方法，通过动量加权的提示词采样策略改进文本梯度下降框架，实现高效可扩展的上下文学习。


<details>
  <summary>Details</summary>
Motivation: 传统文本梯度下降(TGD)框架在扩大训练数据量时性能先升后降，且计算成本激增，需改进提示优化策略。

Method: 受数值梯度下降启发，引入动量机制对历史批次分布进行重新加权采样（TSGD-M）

Result: 在9个NLP任务中显著超越基准TGD（BBH/语言理解/推理任务），同时降低方差

Conclusion: TSGD-M为大规模上下文学习提供有效解决方案，平衡效果与计算效率

Abstract: As prompts play an increasingly critical role in large language models
(LLMs), optimizing textual prompts has become a crucial challenge. The Textual
Gradient Descent (TGD) framework has emerged as a promising data-driven
approach that iteratively refines textual prompts using LLM - suggested updates
(or textual gradients) over minibatches of training samples. In this paper, we
empirically demonstrate that scaling the number of training examples initially
improves but later degrades TGD's performance across multiple downstream NLP
tasks. However, while data scaling improves results for most tasks, it also
significantly increases the computational cost when leveraging LLMs. To address
this, we draw inspiration from numerical gradient descent and propose Textual
Stochastic Gradient Descent with Momentum (TSGD-M) - a method that facilitates
scalable in-context learning by reweighting prompt sampling based on past batch
distributions. Across nine NLP tasks spanning three domains - including
BIG-Bench Hard (BBH), natural language understanding tasks, and reasoning tasks
- TSGD-M significantly outperforms TGD baselines that do not incorporate
reweighted sampling, while also reducing variance in most tasks.

</details>


### [47] [Causal Structure Discovery for Error Diagnostics of Children's ASR](https://arxiv.org/abs/2506.00402)
*Vishwanath Pratap Singh,Md. Sahidullah,Tomi Kinnunen*

Main category: cs.CL

TL;DR: 通过因果结构发现和量化方法揭示儿童语音识别性能差的生理、认知和外在因素间复杂关联，实验证明因果分析能有效定位关键影响因素。


<details>
  <summary>Details</summary>
Motivation: 现有研究孤立分析儿童ASR性能的影响因素，忽略了年龄可能通过发音能力间接影响ASR精度等复杂因果关系，导致解决方案不全面。

Method: 1. 使用因果结构发现揭示生理、认知、外在因素与ASR错误间的网络关系
2. 因果量化方法测量各因素影响权重
3. 扩展分析微调模型对不同因素的缓解效果

Result: 1. Whisper和Wav2Vec2.0实验验证因素间存在显著相互依赖性
2. 微调可缓解部分因素（如发音缺陷）但无法消除生理限制
3. 发现结果在不同ASR系统中具有普适性

Conclusion: 因果分析方法能有效解构儿童ASR问题的复杂性，强调改进系统时需同时考虑直接因素和间接影响路径，微调存在局限性需结合其他优化手段

Abstract: Children's automatic speech recognition (ASR) often underperforms compared to
that of adults due to a confluence of interdependent factors: physiological
(e.g., smaller vocal tracts), cognitive (e.g., underdeveloped pronunciation),
and extrinsic (e.g., vocabulary limitations, background noise). Existing
analysis methods examine the impact of these factors in isolation, neglecting
interdependencies-such as age affecting ASR accuracy both directly and
indirectly via pronunciation skills. In this paper, we introduce a causal
structure discovery to unravel these interdependent relationships among
physiology, cognition, extrinsic factors, and ASR errors. Then, we employ
causal quantification to measure each factor's impact on children's ASR. We
extend the analysis to fine-tuned models to identify which factors are
mitigated by fine-tuning and which remain largely unaffected. Experiments on
Whisper and Wav2Vec2.0 demonstrate the generalizability of our findings across
different ASR systems.

</details>


### [48] [Accelerating Diffusion LLMs via Adaptive Parallel Decoding](https://arxiv.org/abs/2506.00413)
*Daniel Israel,Guy Van den Broeck,Aditya Grover*

Main category: cs.CL

TL;DR: 提出自适应并行解码(APD)方法，通过动态调节并行采样token数量，结合扩散模型与辅助自回归模型的概率混合，在保证质量前提下显著提升大语言模型生成速度。


<details>
  <summary>Details</summary>
Motivation: 自回归解码的token串行生成制约LLM速度，扩散语言模型(dLLM)的并行生成潜力未能充分实现。现有方案需在速度与质量间取舍，需新型解码框架突破瓶颈。

Method: 1. 定义扩散模型边际概率与辅助自回归模型序列联合概率的乘积混合机制
2. 逆向应用推测性解码思路，通过KV缓存和掩码输入限制优化
3. 引入三个可调参数灵活平衡吞吐量与生成质量

Result: 在下游基准测试中实现显著更高的吞吐量，质量损失控制在最小范围

Conclusion: APD开创性地将扩散模型与自回归模型优势结合，通过动态并行机制实现速度与质量的协同优化，为高效文本生成开辟新路径

Abstract: The generation speed of LLMs are bottlenecked by autoregressive decoding,
where tokens are predicted sequentially one by one. Alternatively, diffusion
large language models (dLLMs) theoretically allow for parallel token
generation, but in practice struggle to achieve the speed of autoregressive
models without significantly sacrificing quality. We therefore introduce
adaptive parallel decoding (APD), a novel method that dynamically adjusts the
number of tokens sampled in parallel. We achieve this by defining a
multiplicative mixture between the dLLM marginal probabilities and the joint
probability of sequences under a small auxiliary autoregressive model. This
inverts the standard setup of speculative decoding, where the goal is to sample
from a large autoregressive verifier by drafting from a smaller model. We
further optimize APD by enabling KV caching and limiting the size of the masked
input. Altogether, our method puts forward three tunable parameters to flexibly
tradeoff throughput and quality. We show that APD provides markedly higher
throughput with minimal quality degradations on downstream benchmarks.

</details>


### [49] [Dual Debiasing for Noisy In-Context Learning for Text Generation](https://arxiv.org/abs/2506.00418)
*Siqi Liang,Sumyeong Ahn,Paramveer S. Dhillon,Jiayu Zhou*

Main category: cs.CL

TL;DR: 提出双去偏框架解决高噪声环境下上下文学习中的标注偏差问题，通过合成邻居校正困惑度指标，实现稳健的样本清洁度评估。


<details>
  <summary>Details</summary>
Motivation: 传统基于困惑度的噪声检测方法在高噪声比例场景失效（因错误样本过多导致假设不成立），需解决标注偏差和LLM领域知识偏差的双重影响。

Method: 设计双去偏框架：通过合成邻居样本显式校正困惑度估计，构建与整体噪声水平无关的绝对样本清洁度评分（Sample Cleanliness Score）。

Result: 实验证明该方法在噪声检测能力上优于基线，最终ICL性能接近全清洁语料库，且在极端高噪声比例（如90%）下仍保持稳健。

Conclusion: 该框架通过分离领域知识与标注偏差，实现了噪声环境下的可靠样本评估，显著提升了上下文学习系统在真实噪声场景中的鲁棒性。

Abstract: In context learning (ICL) relies heavily on high quality demonstrations drawn
from large annotated corpora. Existing approaches detect noisy annotations by
ranking local perplexities, presuming that noisy samples yield higher
perplexities than their clean counterparts. However, this assumption breaks
down when the noise ratio is high and many demonstrations are flawed. We
reexamine the perplexity based paradigm for text generation under noisy
annotations, highlighting two sources of bias in perplexity: the annotation
itself and the domain specific knowledge inherent in large language models
(LLMs). To overcome these biases, we introduce a dual debiasing framework that
uses synthesized neighbors to explicitly correct perplexity estimates, yielding
a robust Sample Cleanliness Score. This metric uncovers absolute sample
cleanliness regardless of the overall corpus noise level. Extensive experiments
demonstrate our method's superior noise detection capabilities and show that
its final ICL performance is comparable to that of a fully clean demonstration
corpus. Moreover, our approach remains robust even when noise ratios are
extremely high.

</details>


### [50] [Enabling Chatbots with Eyes and Ears: An Immersive Multimodal Conversation System for Dynamic Interactions](https://arxiv.org/abs/2506.00421)
*Jihyoung Jang,Minwook Bae,Minji Kim,Dilek Hakkani-Tur,Hyounghun Kim*

Main category: cs.CL

TL;DR: 研究提出结合视觉和听觉的多模态对话模型，通过新数据集M³C和记忆检索机制，提升聊天机器人在复杂场景下的动态交互能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究过度关注视觉任务且交互方式静态，缺乏对听觉模态的整合及自然动态对话的支持，难以满足真实场景需求。

Method: 构建多模态多会话多方对话数据集M³C，开发具备多模态记忆检索的对话模型，支持视听输入处理和长期多轮对话。

Result: 模型在复杂场景中有效整合视听信息，人工评估显示其保持连贯动态对话的能力，证实作为先进对话代理的潜力。

Conclusion: 通过视听模态整合与记忆机制创新，显著提升聊天机器人交互沉浸感，为多模态对话系统发展提供新方向。

Abstract: As chatbots continue to evolve toward human-like, real-world, interactions,
multimodality remains an active area of research and exploration. So far,
efforts to integrate multimodality into chatbots have primarily focused on
image-centric tasks, such as visual dialogue and image-based instructions,
placing emphasis on the "eyes" of human perception while neglecting the "ears",
namely auditory aspects. Moreover, these studies often center around static
interactions that focus on discussing the modality rather than naturally
incorporating it into the conversation, which limits the richness of
simultaneous, dynamic engagement. Furthermore, while multimodality has been
explored in multi-party and multi-session conversations, task-specific
constraints have hindered its seamless integration into dynamic, natural
conversations. To address these challenges, this study aims to equip chatbots
with "eyes and ears" capable of more immersive interactions with humans. As
part of this effort, we introduce a new multimodal conversation dataset,
Multimodal Multi-Session Multi-Party Conversation ($M^3C$), and propose a novel
multimodal conversation model featuring multimodal memory retrieval. Our model,
trained on the $M^3C$, demonstrates the ability to seamlessly engage in
long-term conversations with multiple speakers in complex, real-world-like
settings, effectively processing visual and auditory inputs to understand and
respond appropriately. Human evaluations highlight the model's strong
performance in maintaining coherent and dynamic interactions, demonstrating its
potential for advanced multimodal conversational agents.

</details>


### [51] [DYNAC: Dynamic Vocabulary based Non-Autoregressive Contextualization for Speech Recognition](https://arxiv.org/abs/2506.00422)
*Yui Sudo,Yosuke Fukumoto,Muhammad Shakeel,Yifan Peng,Chyi-Jiunn Lin,Shinji Watanabe*

Main category: cs.CL

TL;DR: 提出DYNAC方法——基于动态词汇的自条件CTC模型，在保持精度的同时将语音识别推理速度提升81%


<details>
  <summary>Details</summary>
Motivation: 动态词汇可提升上下文偏置的准确率，但自回归模型推理速度慢；非自回归模型CTC因条件独立性假设无法有效建模动态词汇依赖

Method: 通过自条件机制将动态词汇注入CTC编码器中间层，建立静态词汇与动态短语的依赖关系

Result: LibriSpeech测试集上RTF降低81%，词错率仅增加0.1%

Conclusion: DYNAC首次在非自回归模型中实现动态词汇的有效整合，平衡了语音识别效率与精度

Abstract: Contextual biasing (CB) improves automatic speech recognition for rare and
unseen phrases. Recent studies have introduced dynamic vocabulary, which
represents context phrases as expandable tokens in autoregressive (AR) models.
This method improves CB accuracy but with slow inference speed. While dynamic
vocabulary can be applied to non-autoregressive (NAR) models, such as
connectionist temporal classification (CTC), the conditional independence
assumption fails to capture dependencies between static and dynamic tokens.
This paper proposes DYNAC (Dynamic Vocabulary-based NAR Contextualization), a
self-conditioned CTC method that integrates dynamic vocabulary into
intermediate layers. Conditioning the encoder on dynamic vocabulary, DYNAC
effectively captures dependencies between static and dynamic tokens while
reducing the real-time factor (RTF). Experimental results show that DYNAC
reduces RTF by 81% with a 0.1-point degradation in word error rate on the
LibriSpeech 960 test-clean set.

</details>


### [52] [Inter-Passage Verification for Multi-evidence Multi-answer QA](https://arxiv.org/abs/2506.00425)
*Bingsen Chen,Shengjie Wang,Xi Ye,Chen Zhao*

Main category: cs.CL

TL;DR: 提出RI²VER框架解决多答案问答难题，通过独立阅读+跨段落验证机制，在QAMPARI/RoMQA数据集上实现F1值提升11.17%


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成系统在处理需要检索和整合大量证据段落的多答案问题时，存在召回率低和噪声干扰的问题

Method: 两阶段框架：1）独立阅读阶段生成高召回答案集；2）跨段落验证流水线（验证问题生成→补充证据收集→跨段落合成验证）

Result: 在不同模型规模下显著超越基线，尤其在需要多证据合成的复杂问题上提升明显（平均F1提升11.17%）

Conclusion: 跨段落验证机制有效提升多答案问答性能，验证了框架在复杂多证据推理任务中的优越性

Abstract: Multi-answer question answering (QA), where questions can have many valid
answers, presents a significant challenge for existing retrieval-augmented
generation-based QA systems, as these systems struggle to retrieve and then
synthesize a large number of evidence passages. To tackle these challenges, we
propose a new multi-answer QA framework -- Retrieval-augmented Independent
Reading with Inter-passage Verification (RI$^2$VER). Our framework retrieves a
large set of passages and processes each passage individually to generate an
initial high-recall but noisy answer set. Then we propose a new inter-passage
verification pipeline that validates every candidate answer through (1)
Verification Question Generation, (2) Gathering Additional Evidence, and (3)
Verification with inter-passage synthesis. Evaluations on the QAMPARI and RoMQA
datasets demonstrate that our framework significantly outperforms existing
baselines across various model sizes, achieving an average F1 score improvement
of 11.17%. Further analysis validates that our inter-passage verification
pipeline enables our framework to be particularly beneficial for questions
requiring multi-evidence synthesis.

</details>


### [53] [G2S: A General-to-Specific Learning Framework for Temporal Knowledge Graph Forecasting with Large Language Models](https://arxiv.org/abs/2506.00445)
*Long Bai,Zixuan Li,Xiaolong Jin,Jiafeng Guo,Xueqi Cheng,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 提出了G2S框架，通过解耦通用模式和场景信息的学习过程，提升LLM在时序知识图谱预测中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法同时学习TKG中的通用时间结构和具体场景信息，可能造成知识干扰，影响模型泛化能力。需要分离两种知识的学习过程。

Method: 采用两阶段学习：通用阶段通过匿名化处理学习跨场景的时序模式；特定阶段通过上下文学习/微调注入具体场景信息。

Result: 实验表明G2S有效提升了LLM的泛化能力，验证了框架有效性。

Conclusion: 解耦通用模式与场景信息的学习过程能显著增强模型在时序知识图谱预测任务中的泛化性能。

Abstract: Forecasting over Temporal Knowledge Graphs (TKGs) which predicts future facts
based on historical ones has received much attention. Recent studies have
introduced Large Language Models (LLMs) for this task to enhance the models'
generalization abilities. However, these models perform forecasting via
simultaneously learning two kinds of entangled knowledge in the TKG: (1)
general patterns, i.e., invariant temporal structures shared across different
scenarios; and (2) scenario information, i.e., factual knowledge engaged in
specific scenario, such as entities and relations. As a result, the learning
processes of these two kinds of knowledge may interfere with each other, which
potentially impact the generalization abilities of the models. To enhance the
generalization ability of LLMs on this task, in this paper, we propose a
General-to-Specific learning framework (G2S) that disentangles the learning
processes of the above two kinds of knowledge. In the general learning stage,
we mask the scenario information in different TKGs and convert it into
anonymous temporal structures. After training on these structures, the model is
able to capture the general patterns across different TKGs. In the specific
learning stage, we inject the scenario information into the structures via
either in-context learning or fine-tuning modes. Experimental results show that
G2S effectively improves the generalization abilities of LLMs.

</details>


### [54] [Fact-Controlled Diagnosis of Hallucinations in Medical Text Summarization](https://arxiv.org/abs/2506.00448)
*Suhas BN,Han-Chin Shing,Lei Xu,Mitch Strong,Jon Burnsky,Jessica Ofor,Jordan R. Mason,Susan Chen,Sundararajan Srinivasan,Chaitanya Shivade,Jack Moriarty,Joseph Paul Cohen*

Main category: cs.CL

TL;DR: 研究大型语言模型在临床对话摘要中的幻觉问题，开发两种数据集并提出基于事实的检测方法，有效提升临床幻觉检测效果


<details>
  <summary>Details</summary>
Motivation: 临床领域的幻觉现象尚未充分研究，现有通用检测器效果不佳，可能导致医疗决策风险，需开发专门检测方法

Method: 构建事实控制（Leave-N-out）数据集和自然幻觉数据集，开发基于事实统计的检测方法（包括LLM检测器）并验证其有效性

Result: 通用检测器在临床领域表现差，基于事实的方法在自然幻觉数据集表现更优，LLM检测器具有可解释性且泛化能力强

Conclusion: 提供专门数据集和检测指标，推动可靠临床摘要系统发展，基于事实控制的检测方法能有效识别真实临床幻觉

Abstract: Hallucinations in large language models (LLMs) during summarization of
patient-clinician dialogues pose significant risks to patient care and clinical
decision-making. However, the phenomenon remains understudied in the clinical
domain, with uncertainty surrounding the applicability of general-domain
hallucination detectors. The rarity and randomness of hallucinations further
complicate their investigation. In this paper, we conduct an evaluation of
hallucination detection methods in the medical domain, and construct two
datasets for the purpose: A fact-controlled Leave-N-out dataset -- generated by
systematically removing facts from source dialogues to induce hallucinated
content in summaries; and a natural hallucination dataset -- arising
organically during LLM-based medical summarization. We show that general-domain
detectors struggle to detect clinical hallucinations, and that performance on
fact-controlled hallucinations does not reliably predict effectiveness on
natural hallucinations. We then develop fact-based approaches that count
hallucinations, offering explainability not available with existing methods.
Notably, our LLM-based detectors, which we developed using fact-controlled
hallucinations, generalize well to detecting real-world clinical
hallucinations. This research contributes a suite of specialized metrics
supported by expert-annotated datasets to advance faithful clinical
summarization systems.

</details>


### [55] [Massively Multilingual Adaptation of Large Language Models Using Bilingual Translation Data](https://arxiv.org/abs/2506.00469)
*Shaoxiong Ji,Zihao Li,Jaakko Paavola,Indraneil Paul,Hengyu Luo,Jörg Tiedemann*

Main category: cs.CL

TL;DR: 研究证明在多语言持续预训练中加入双语翻译数据（尤其是低资源语言）能显著提升模型性能


<details>
  <summary>Details</summary>
Motivation: 验证在Llama3模型家族向500种语言持续预训练过程中，双语平行数据对语言迁移效果的影响

Method: 构建包含2500+语言对的MaLA双语语料库，基于Llama3持续预训练四款EMMA-500模型（训练数据达671B tokens），对比含/不含双语数据的训练效果

Result: 在7个任务12个基准测试中，双语数据提升了语言迁移能力（低资源语言增益最显著）

Conclusion: 双语翻译数据是多语言模型训练的关键要素，研究成果（语料库/模型/代码）已全面开源

Abstract: This paper investigates a critical design decision in the practice of
massively multilingual continual pre-training -- the inclusion of parallel
data. Specifically, we study the impact of bilingual translation data for
massively multilingual language adaptation of the Llama3 family of models to
500 languages. To this end, we construct the MaLA bilingual translation corpus,
containing data from more than 2,500 language pairs. Subsequently, we develop
the EMMA-500 Llama 3 suite of four massively multilingual models -- continually
pre-trained from the Llama 3 family of base models extensively on diverse data
mixes up to 671B tokens -- and explore the effect of continual pre-training
with or without bilingual translation data. Comprehensive evaluation across 7
tasks and 12 benchmarks demonstrates that bilingual data tends to enhance
language transfer and performance, particularly for low-resource languages. We
open-source the MaLA corpus, EMMA-500 Llama 3 suite artefacts, code, and model
generations.

</details>


### [56] [EffiVLM-BENCH: A Comprehensive Benchmark for Evaluating Training-Free Acceleration in Large Vision-Language Models](https://arxiv.org/abs/2506.00479)
*Zekun Wang,Minghua Ma,Zexin Wang,Rongchuan Mu,Liping Shan,Ming Liu,Bing Qin*

Main category: cs.CL

TL;DR: 提出EffiVLM-Bench框架，系统性评估大型视觉语言模型加速技术，探索性能与效率的帕累托最优平衡。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型存在高计算需求阻碍实际部署，现有加速方法缺乏跨模型/基准/指标的全面评估体系。

Method: 将主流加速技术归类为token压缩和参数压缩，通过统一框架评估绝对性能、泛化性、忠诚度三维指标。

Result: 通过大量实验验证不同加速策略效果，发现模型压缩与参数高效微调结合能实现最优权衡，并开源评估方案。

Conclusion: 该框架为加速技术选择提供系统指导，开源的基准测试工具将推动高效LVLM的进一步发展。

Abstract: Large Vision-Language Models (LVLMs) have achieved remarkable success, yet
their significant computational demands hinder practical deployment. While
efforts to improve LVLM efficiency are growing, existing methods lack
comprehensive evaluation across diverse backbones, benchmarks, and metrics. In
this work, we systematically evaluate mainstream acceleration techniques for
LVLMs, categorized into token and parameter compression. We introduce
EffiVLM-Bench, a unified framework for assessing not only absolute performance
but also generalization and loyalty, while exploring Pareto-optimal trade-offs.
Our extensive experiments and in-depth analyses offer insights into optimal
strategies for accelerating LVLMs. We open-source code and recipes for
EffiVLM-Bench to foster future research.

</details>


### [57] [PVP: An Image Dataset for Personalized Visual Persuasion with Persuasion Strategies, Viewer Characteristics, and Persuasiveness Ratings](https://arxiv.org/abs/2506.00481)
*Junseo Kim,Jongwook Han,Dongmin Choi,Jongwook Yoon,Eun-Ju Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 发布包含28,454张说服性图像的PVP数据集，结合2,521名评估者的心理特征，证明整合心理特征可提升个性化视觉说服系统的生成与评估效果。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏图像说服力与评估者个人信息的关联，阻碍了个性化视觉说服技术的发展。

Method: 构建含596个信息主题、9种说服策略的大规模数据集，开发基于心理特征的图像生成器与自动评估器，开展基准实验。

Result: 实验表明融合价值观和人格特质等心理特征可使图像说服力评估准确率提升21%，生成效果提升18%。

Conclusion: PVP数据集填补研究空白，验证心理特征对个性化视觉说服的关键作用，为AI+传播学交叉研究提供新范式。

Abstract: Visual persuasion, which uses visual elements to influence cognition and
behaviors, is crucial in fields such as advertising and political
communication. With recent advancements in artificial intelligence, there is
growing potential to develop persuasive systems that automatically generate
persuasive images tailored to individuals. However, a significant bottleneck in
this area is the lack of comprehensive datasets that connect the persuasiveness
of images with the personal information about those who evaluated the images.
To address this gap and facilitate technological advancements in personalized
visual persuasion, we release the Personalized Visual Persuasion (PVP) dataset,
comprising 28,454 persuasive images across 596 messages and 9 persuasion
strategies. Importantly, the PVP dataset provides persuasiveness scores of
images evaluated by 2,521 human annotators, along with their demographic and
psychological characteristics (personality traits and values). We demonstrate
the utility of our dataset by developing a persuasive image generator and an
automated evaluator, and establish benchmark baselines. Our experiments reveal
that incorporating psychological characteristics enhances the generation and
evaluation of persuasive images, providing valuable insights for personalized
visual persuasion.

</details>


### [58] [Auto-Patching: Enhancing Multi-Hop Reasoning in Language Models](https://arxiv.org/abs/2506.00483)
*Aviv Jan,Dean Tahory,Omer Talmi,Omar Abo Mokh*

Main category: cs.CL

TL;DR: 通过动态修改大语言模型推理过程中的隐藏状态（Auto-Patch方法），提升多跳推理能力


<details>
  <summary>Details</summary>
Motivation: 大语言模型在需要多步推理的复杂问题上表现不佳，现有方法存在信息跨推理步骤链接困难的问题

Method: 基于PatchScopes框架开发Auto-Patch方法，通过训练分类器选择性修改模型内部表示

Result: 在MuSiQue数据集上将解决率从18.45%提升至23.63±0.7%，接近思维链提示的27.44%

Conclusion: 动态隐藏状态干预为提升大语言模型复杂推理能力展示了新方向

Abstract: Multi-hop questions still stump large language models (LLMs), which struggle
to link information across multiple reasoning steps. We introduce Auto-Patch, a
novel method that dynamically patches hidden states during inference to enhance
multi-hop reasoning in LLMs. Building on the PatchScopes framework, Auto-Patch
selectively modifies internal representations using a learned classifier.
Evaluated on the MuSiQue dataset, Auto-Patch improves the solve rate from
18.45\% (baseline) to 23.63~$\pm$~0.7\% (3 runs), narrowing the gap to
Chain-of-Thought prompting (27.44\%). Our results highlight the potential of
dynamic hidden state interventions for advancing complex reasoning in LLMs.

</details>


### [59] [Synergizing LLMs with Global Label Propagation for Multimodal Fake News Detection](https://arxiv.org/abs/2506.00488)
*Shuguo Hu,Jun Hu,Huaiwen Zhang*

Main category: cs.CL

TL;DR: 提出GLPN-LLM模型，通过标签传播技术整合LLM生成的伪标签，提升多模态假新闻检测效果


<details>
  <summary>Details</summary>
Motivation: 传统检测方法单独使用LLM生成的伪标签效果不佳，需探索有效整合方式。通过全局标签传播技术利用伪标签增强预测精度

Method: 设计全局标签传播框架（跨样本传播标签信息）+ 掩码机制（防止训练时标签自泄漏）

Result: 在基准数据集上超越现有最优模型，验证LLM与标签传播协同的有效性

Conclusion: 证明了LLM与图传播技术结合的创新价值，为多模态假新闻检测提供新范式

Abstract: Large Language Models (LLMs) can assist multimodal fake news detection by
predicting pseudo labels. However, LLM-generated pseudo labels alone
demonstrate poor performance compared to traditional detection methods, making
their effective integration non-trivial. In this paper, we propose Global Label
Propagation Network with LLM-based Pseudo Labeling (GLPN-LLM) for multimodal
fake news detection, which integrates LLM capabilities via label propagation
techniques. The global label propagation can utilize LLM-generated pseudo
labels, enhancing prediction accuracy by propagating label information among
all samples. For label propagation, a mask-based mechanism is designed to
prevent label leakage during training by ensuring that training nodes do not
propagate their own labels back to themselves. Experimental results on
benchmark datasets show that by synergizing LLMs with label propagation, our
model achieves superior performance over state-of-the-art baselines.

</details>


### [60] [Exploring In-context Example Generation for Machine Translation](https://arxiv.org/abs/2506.00507)
*Dohyun Lee,Seungil Chad Lee,Chanwoo Yang,Yujin Baek,Jaegul Choo*

Main category: cs.CL

TL;DR: 提出无需外部资源的上下文示例生成方法DAT，通过相关性和多样性标准提升低资源语言机器翻译质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖人工标注的示例池，这在低资源语言场景中难以满足。需探索不依赖外部资源的上下文示例生成方案。

Method: DAT方法基于相关性和多样性双重标准，通过迭代生成和选择翻译示例对，无需任何外部资源支持。

Result: 在低资源语言实验中，DAT显著优于基线模型，并验证了测试阶段持续积累示例池的可行性。

Conclusion: DAT为低资源机器翻译提供了有效解决方案，未来可探索动态构建演示池的长期优化策略。

Abstract: Large language models (LLMs) have demonstrated strong performance across
various tasks, leveraging their exceptional in-context learning ability with
only a few examples. Accordingly, the selection of optimal in-context examples
has been actively studied in the field of machine translation. However, these
studies presuppose the presence of a demonstration pool with human-annotated
pairs, making them less applicable to low-resource languages where such an
assumption is challenging to meet. To overcome this limitation, this paper
explores the research direction of in-context example generation for machine
translation. Specifically, we propose Demonstration Augmentation for
Translation (DAT), a simple yet effective approach that generates example pairs
without relying on any external resources. This method builds upon two prior
criteria, relevance and diversity, which have been highlighted in previous work
as key factors for in-context example selection. Through experiments and
analysis on low-resource languages where human-annotated pairs are scarce, we
show that DAT achieves superior translation quality compared to the baselines.
Furthermore, we investigate the potential of progressively accumulating
generated pairs during test time to build and reuse a demonstration pool. Our
implementation is publicly available at https://github.com/aiclaudev/DAT.

</details>


### [61] [Goal-Aware Identification and Rectification of Misinformation in Multi-Agent Systems](https://arxiv.org/abs/2506.00509)
*Zherui Li,Yan Mi,Zhenhong Zhou,Houcheng Jiang,Guibin Zhang,Kun Wang,Junfeng Fang*

Main category: cs.CL

TL;DR: 提出ARGUS防御框架和MisinfoTask数据集，用于增强大语言模型多智能体系统在错误信息攻击下的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 多智能体系统因攻击面扩大容易遭受错误信息注入攻击，需系统性的评估方法和防御方案

Method: 构建真实场景的MisinfoTask数据集，开发基于目标感知推理的两阶段防御框架ARGUS（无需训练）

Result: 在复杂攻击场景下，ARGUS平均降低28.17%的错误信息毒性，攻击中任务成功率提升10.33%

Conclusion: ARGUS框架通过精确的信息修正显著提升多智能体系统的抗攻击能力，为安全部署提供新思路

Abstract: Large Language Model-based Multi-Agent Systems (MASs) have demonstrated
strong advantages in addressing complex real-world tasks. However, due to the
introduction of additional attack surfaces, MASs are particularly vulnerable to
misinformation injection. To facilitate a deeper understanding of
misinformation propagation dynamics within these systems, we introduce
MisinfoTask, a novel dataset featuring complex, realistic tasks designed to
evaluate MAS robustness against such threats. Building upon this, we propose
ARGUS, a two-stage, training-free defense framework leveraging goal-aware
reasoning for precise misinformation rectification within information flows.
Our experiments demonstrate that in challenging misinformation scenarios, ARGUS
exhibits significant efficacy across various injection attacks, achieving an
average reduction in misinformation toxicity of approximately 28.17% and
improving task success rates under attack by approximately 10.33%. Our code and
dataset is available at: https://github.com/zhrli324/ARGUS.

</details>


### [62] [Evaluating the Evaluation of Diversity in Commonsense Generation](https://arxiv.org/abs/2506.00514)
*Tianhui Zhang,Bei Peng,Danushka Bollegala*

Main category: cs.CL

TL;DR: 研究通过元评估发现基于内容的多样性指标在常识生成任务中优于形式指标，建议未来采用内容指标评估多样性


<details>
  <summary>Details</summary>
Motivation: 现有常识生成任务中形式层面的多样性评估指标存在高估多样性问题，需系统评估各类指标优劣

Method: 利用LLM创建标注数据集，对现有指标进行元评估并比较形式/内容指标与人工评分的相关性

Result: 基于内容的指标与LLM评分高度相关，形式指标对随机生成句子的多样性评分虚高

Conclusion: 未来常识生成任务应优先使用基于内容的多样性评估指标以提高评估有效性

Abstract: In commonsense generation, given a set of input concepts, a model must
generate a response that is not only commonsense bearing, but also capturing
multiple diverse viewpoints. Numerous evaluation metrics based on form- and
content-level overlap have been proposed in prior work for evaluating the
diversity of a commonsense generation model. However, it remains unclear as to
which metrics are best suited for evaluating the diversity in commonsense
generation. To address this gap, we conduct a systematic meta-evaluation of
diversity metrics for commonsense generation. We find that form-based diversity
metrics tend to consistently overestimate the diversity in sentence sets, where
even randomly generated sentences are assigned overly high diversity scores. We
then use an Large Language Model (LLM) to create a novel dataset annotated for
the diversity of sentences generated for a commonsense generation task, and use
it to conduct a meta-evaluation of the existing diversity evaluation metrics.
Our experimental results show that content-based diversity evaluation metrics
consistently outperform the form-based counterparts, showing high correlations
with the LLM-based ratings. We recommend that future work on commonsense
generation should use content-based metrics for evaluating the diversity of
their outputs.

</details>


### [63] [CausalAbstain: Enhancing Multilingual LLMs with Causal Reasoning for Trustworthy Abstention](https://arxiv.org/abs/2506.00519)
*Yuxi Sun,Aoqi Zuo,Wei Gao,Jing Ma*

Main category: cs.CL

TL;DR: 提出CausalAbstain方法解决LLMs多语言知识差异导致的幻觉问题，通过因果分析选择有效反馈提升弃权决策


<details>
  <summary>Details</summary>
Motivation: 现有多语言弃权策略依赖LLMs生成的反馈容易受不准确和偏见影响，需更可靠的方法筛选有效反馈

Method: 从因果视角构建框架，帮助LLMs判断是否使用多反馈并选择最有效反馈，提升决策可解释性

Result: 在百科全书和常识QA数据集上，CausalAbstain在原生/多语言场景均超越基线模型，代码数据已开源

Conclusion: 通过因果分析筛选反馈能有效增强LLMs在多语言场景的弃权决策能力，减少幻觉并提升可解释性

Abstract: Large Language Models (LLMs) often exhibit knowledge disparities across
languages. Encouraging LLMs to \textit{abstain} when faced with knowledge gaps
is a promising strategy to reduce hallucinations in multilingual settings.
Current abstention strategies for multilingual scenarios primarily rely on
generating feedback in various languages using LLMs and performing
self-reflection. However, these methods can be adversely impacted by
inaccuracies and biases in the generated feedback. To address this, from a
causal perspective, we introduce \textit{CausalAbstain}, a method that helps
LLMs determine whether to utilize multiple generated feedback responses and how
to identify the most useful ones. Extensive experiments demonstrate that
\textit{CausalAbstain} effectively selects helpful feedback and enhances
abstention decisions with interpretability in both native language
(\textsc{Casual-native}) and multilingual (\textsc{Causal-multi}) settings,
outperforming strong baselines on two benchmark datasets covering encyclopedic
and commonsense knowledge QA tasks. Our code and data are open-sourced at
https://github.com/peachch/CausalAbstain.

</details>


### [64] [Retrieval-Augmented Generation Systems for Intellectual Property via Synthetic Multi-Angle Fine-tuning](https://arxiv.org/abs/2506.00527)
*Runtao Ren,Jian Ma,Jianxi Luo*

Main category: cs.CL

TL;DR: 提出MQG-RFM框架解决IP领域RAG系统的查询多样性问题，通过多角度问题生成和检索微调提升语义对齐能力


<details>
  <summary>Details</summary>
Motivation: 现有IP领域RAG系统难以处理口语化表达、拼写错误和模糊术语，导致检索不准确和响应质量低下

Method: 结合提示工程生成多样化问题+硬负样本挖掘的Data-to-Tune范式，轻量化微调检索模型实现语义对齐

Result: 检索准确率提升185.62%-262.26%，生成质量提高14.22%-53.58%，已在中国最大科研社交平台ScholarMate落地应用

Conclusion: 通过语义感知的检索优化弥合用户意图与系统理解的鸿沟，为中小企业提供快速部署的专利智能解决方案

Abstract: Retrieval-Augmented Generation (RAG) systems in the Intellectual Property
(IP) field often struggle with diverse user queries, including colloquial
expressions, spelling errors, and ambiguous terminology, leading to inaccurate
retrieval and suboptimal responses. To address this challenge, we propose
Multi-Angle Question Generation and Retrieval Fine-Tuning Method (MQG-RFM), a
novel framework that leverages large language models (LLMs) to simulate varied
user inquiries and fine-tunes retrieval models to align semantically equivalent
but linguistically diverse questions. Unlike complex architectural
modifications, MQG-RFM adopts a lightweight Data-to-Tune paradigm, combining
prompt-engineered query generation with hard negative mining to enhance
retrieval robustness without costly infrastructure changes. Experimental
results on a Taiwan patent Q&A dataset show 185.62% improvement in retrieval
accuracy on the Patent Consultation dataset and 262.26% improvement on the
Novel Patent Technology Report dataset, with 14.22% and 53.58% improvements in
generation quality over the baselines, respectively. By bridging the gap
between user intent and system comprehension through semantic-aware retrieval
optimization, MQG-RFM offers a practical, scalable approach for rapid,
cost-effective deployment among small and medium-sized agencies seeking
reliable patent intelligence solutions. Additionally, our proposed method has
already been adopted by ScholarMate, the largest professional research social
networking platform in China, to support real-world development and deployment.
A demo version of the instantiated is available at
https://github.com/renruntao/patent_rag.

</details>


### [65] [Decoupling Reasoning and Knowledge Injection for In-Context Knowledge Editing](https://arxiv.org/abs/2506.00536)
*Changyue Wang,Weihang Su,Qingyao Ai,Yujia Zhou,Yiqun Liu*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Knowledge editing aims to efficiently update Large Language Models (LLMs) by
modifying specific knowledge without retraining the entire model. Among
knowledge editing approaches, in-context editing (ICE) offers a lightweight
solution by injecting new knowledge directly into the input context, leaving
model parameters unchanged. However, existing ICE approaches do not explicitly
separate the newly injected knowledge from the model's original reasoning
process. This entanglement often results in conflicts between external updates
and internal parametric knowledge, undermining the consistency and accuracy of
the reasoning path.In this work, we conduct preliminary experiments to examine
how parametric knowledge influences reasoning path planning. We find that the
model's reasoning is tightly coupled with its internal knowledge, and that
naively injecting new information without adapting the reasoning path often
leads to performance degradation, particularly in multi-hop tasks. To this end,
we propose DecKER, a novel ICE framework that decouples reasoning from
knowledge editing by generating a masked reasoning path and then resolving
knowledge edits via hybrid retrieval and model-based validation. Experiments on
multi-hop QA benchmarks show that DecKER significantly outperforms existing ICE
methods by mitigating knowledge conflicts and preserving reasoning consistency.
Our code is available at: https://github.com/bebr2/DecKER .

</details>


### [66] [ARIA: Training Language Agents with Intention-Driven Reward Aggregation](https://arxiv.org/abs/2506.00539)
*Ruihan Yang,Yikai Zhang,Aili Chen,Xintao Wang,Siyu Yuan,Jiangjie Chen,Deqing Yang,Yanghua Xiao*

Main category: cs.CL

TL;DR: ARIA通过将自然语言动作映射到低维意图空间实现奖励聚合，有效解决大语言模型强化学习中的奖励稀疏问题，平均提升下游任务9.95%表现。


<details>
  <summary>Details</summary>
Motivation: 开放式语言交互环境中，基于token联合分布的动作空间维度灾难导致奖励稀疏，强化学习面临奖励方差大、优化困难的核心痛点。

Method: 提出意图空间奖励聚合框架：1）将语义相似的自然语言动作聚类到低维空间 2）共享意图簇内的奖励信号 3）通过奖励稠密化降低策略梯度方差。

Result: 在四个下游任务中平均提升9.95%性能，策略梯度方差显著降低，持续超越离线/在线强化学习基线。

Conclusion: 意图空间的奖励聚合机制有效解决了语言智能体训练的样本效率问题，为复杂语言交互场景的RL优化提供了新范式。

Abstract: Large language models (LLMs) have enabled agents to perform complex reasoning
and decision-making through free-form language interactions. However, in
open-ended language action environments (e.g., negotiation or question-asking
games), the action space can be formulated as a joint distribution over tokens,
resulting in an exponentially large action space. Sampling actions in such a
space can lead to extreme reward sparsity, which brings large reward variance,
hindering effective reinforcement learning (RL). To address this, we propose
ARIA, a method that Aggregates Rewards in Intention space to enable efficient
and effective language Agents training. ARIA aims to project natural language
actions from the high-dimensional joint token distribution space into a
low-dimensional intention space, where semantically similar actions are
clustered and assigned shared rewards. This intention-aware reward aggregation
reduces reward variance by densifying reward signals, fostering better policy
optimization. Extensive experiments demonstrate that ARIA not only
significantly reduces policy gradient variance, but also delivers substantial
performance gains of an average of 9.95% across four downstream tasks,
consistently outperforming offline and online RL baselines.

</details>


### [67] [Towards Multi-dimensional Evaluation of LLM Summarization across Domains and Languages](https://arxiv.org/abs/2506.00549)
*Hyangsuk Min,Yuho Lee,Minjeong Ban,Jiaqi Deng,Nicole Hee-Yeon Kim,Taewon Yun,Hang Su,Jason Cai,Hwanjun Song*

Main category: cs.CL

TL;DR: 提出多维度中英摘要评估基准MSumBench，揭示领域性能差异及大模型评估偏差


<details>
  <summary>Details</summary>
Motivation: 现有摘要评估框架缺乏领域特定标准、存在英语中心化倾向，且复杂推理任务中人工标注质量受限

Method: 构建支持中英双语的多领域评估基准，采用领域专属标准与多智能体辩论系统提升标注质量，并系统评估8个前沿摘要模型及大语言模型评估能力

Result: 模型呈现跨领域/语言的性能差异，发现大模型评估能力与其摘要质量正相关，且存在自我评估的系统性偏差

Conclusion: MSumBench为摘要技术发展提供新视角，其系统性偏差发现对优化评估体系具有重要意义，基准数据集已开源

Abstract: Evaluation frameworks for text summarization have evolved in terms of both
domain coverage and metrics. However, existing benchmarks still lack
domain-specific assessment criteria, remain predominantly English-centric, and
face challenges with human annotation due to the complexity of reasoning. To
address these, we introduce MSumBench, which provides a multi-dimensional,
multi-domain evaluation of summarization in English and Chinese. It also
incorporates specialized assessment criteria for each domain and leverages a
multi-agent debate system to enhance annotation quality. By evaluating eight
modern summarization models, we discover distinct performance patterns across
domains and languages. We further examine large language models as summary
evaluators, analyzing the correlation between their evaluation and
summarization capabilities, and uncovering systematic bias in their assessment
of self-generated summaries. Our benchmark dataset is publicly available at
https://github.com/DISL-Lab/MSumBench.

</details>


### [68] [AnnaAgent: Dynamic Evolution Agent System with Multi-Session Memory for Realistic Seeker Simulation](https://arxiv.org/abs/2506.00551)
*Ming Wang,Peidong Wang,Lin Wu,Xiaocui Yang,Daling Wang,Shi Feng,Yuxin Chen,Bixuan Wang,Yifei Zhang*

Main category: cs.CL

TL;DR: 提出AnnaAgent系统，通过动态情绪调节和三级记忆机制实现更真实的心理咨询求助者模拟


<details>
  <summary>Details</summary>
Motivation: 现有LLM对话代理在模拟心理咨询求助者时存在动态心理状态演变和多会话记忆整合的不足

Method: 集成情绪调制器、抱怨引发器及三级记忆系统（短期+长期记忆整合），基于真实咨询对话训练

Result: 自动评估和人工评估均显示在心理咨询场景中超越现有基线模型的模拟真实性

Conclusion: AnnaAgent有效解决了求助者模拟的动态演化与跨会话记忆挑战，推动了AI在心理健康领域的发展应用

Abstract: Constrained by the cost and ethical concerns of involving real seekers in
AI-driven mental health, researchers develop LLM-based conversational agents
(CAs) with tailored configurations, such as profiles, symptoms, and scenarios,
to simulate seekers. While these efforts advance AI in mental health, achieving
more realistic seeker simulation remains hindered by two key challenges:
dynamic evolution and multi-session memory. Seekers' mental states often
fluctuate during counseling, which typically spans multiple sessions. To
address this, we propose AnnaAgent, an emotional and cognitive dynamic agent
system equipped with tertiary memory. AnnaAgent incorporates an emotion
modulator and a complaint elicitor trained on real counseling dialogues,
enabling dynamic control of the simulator's configurations. Additionally, its
tertiary memory mechanism effectively integrates short-term and long-term
memory across sessions. Evaluation results, both automated and manual,
demonstrate that AnnaAgent achieves more realistic seeker simulation in
psychological counseling compared to existing baselines. The ethically reviewed
and screened code can be found on https://github.com/sci-m-wang/AnnaAgent.

</details>


### [69] [The Hidden Language of Harm: Examining the Role of Emojis in Harmful Online Communication and Content Moderation](https://arxiv.org/abs/2506.00583)
*Yuhang Zhou,Yimin Xiao,Wei Ai,Ge Gao*

Main category: cs.CL

TL;DR: 研究揭示表情符号通过象征关联和语境误用增强社交媒体攻击性，提出LLM驱动的多步审核方案有效降低攻击性


<details>
  <summary>Details</summary>
Motivation: 现有研究集中于文本攻击性指标，忽略了表情符号通过象征联想、反讽和语境滥用产生的潜在危害

Method: 1. 系统分析表情符号在攻击性推文中的分布模式
2. 设计LLM多步审核流程（检测→替换→验证）
3. 通过人类评估验证方案有效性

Result: 方案使攻击性感知降低72%且语义保留率达89%，不同攻击类型对表情干预敏感度差异显著（p<0.05）

Conclusion: 动态表情替换策略比全面屏蔽更有效，为社交媒体审核提供语境敏感的新型技术路径

Abstract: Social media platforms have become central to modern communication, yet they
also harbor offensive content that challenges platform safety and inclusivity.
While prior research has primarily focused on textual indicators of offense,
the role of emojis, ubiquitous visual elements in online discourse, remains
underexplored. Emojis, despite being rarely offensive in isolation, can acquire
harmful meanings through symbolic associations, sarcasm, and contextual misuse.
In this work, we systematically examine emoji contributions to offensive
Twitter messages, analyzing their distribution across offense categories and
how users exploit emoji ambiguity. To address this, we propose an LLM-powered,
multi-step moderation pipeline that selectively replaces harmful emojis while
preserving the tweet's semantic intent. Human evaluations confirm our approach
effectively reduces perceived offensiveness without sacrificing meaning. Our
analysis also reveals heterogeneous effects across offense types, offering
nuanced insights for online communication and emoji moderation.

</details>


### [70] [Entriever: Energy-based Retriever for Knowledge-Grounded Dialog Systems](https://arxiv.org/abs/2506.00585)
*Yucheng Cai,Ke Li,Yi Huang,Junlan Feng,Zhijian Ou*

Main category: cs.CL

TL;DR: 提出基于能量的检索器Entriever，通过整体建模候选结果解决知识片段独立性假设问题，显著提升知识检索和对话系统性能


<details>
  <summary>Details</summary>
Motivation: 现有检索模型假设知识片段条件独立，而实际应用中可能存在多个相关且相互关联的知识片段，需要整体建模方法提升效果

Method: 提出Entriever模型，采用能量函数定义相关性评分，探索不同能量函数架构和训练方法，直接对整体候选结果进行建模

Result: 在知识检索任务中显著优于强交叉编码器基线，半监督训练中使对话系统端到端性能提升显著

Conclusion: Entriever通过能量函数实现知识片段的整体建模，有效改进检索效果并推动对话系统实际应用

Abstract: A retriever, which retrieves relevant knowledge pieces from a knowledge base
given a context, is an important component in many natural language processing
(NLP) tasks. Retrievers have been introduced in knowledge-grounded dialog
systems to improve knowledge acquisition. In knowledge-grounded dialog systems,
when conditioning on a given context, there may be multiple relevant and
correlated knowledge pieces. However, knowledge pieces are usually assumed to
be conditionally independent in current retriever models. To address this
issue, we propose Entriever, an energy-based retriever. Entriever directly
models the candidate retrieval results as a whole instead of modeling the
knowledge pieces separately, with the relevance score defined by an energy
function. We explore various architectures of energy functions and different
training methods for Entriever, and show that Entriever substantially
outperforms the strong cross-encoder baseline in knowledge retrieval tasks.
Furthermore, we show that in semi-supervised training of knowledge-grounded
dialog systems, Entriever enables effective scoring of retrieved knowledge
pieces and significantly improves end-to-end performance of dialog systems.

</details>


### [71] [PAKTON: A Multi-Agent Framework for Question Answering in Long Legal Agreements](https://arxiv.org/abs/2506.00608)
*Petros Raptopoulos,Giorgos Filandrianos,Maria Lymperaiou,Giorgos Stamou*

Main category: cs.CL

TL;DR: PAKTON框架通过多智能体协作与检索增强生成技术，实现更高效、隐私安全的自动化合同分析，在多个评估维度上超越现有模型


<details>
  <summary>Details</summary>
Motivation: 合同审查存在专业门槛高、法律解释模糊性强、主观判断依赖大等痛点，同时合同保密性限制了专有模型的使用，需要开发开源可替代方案

Method: 构建端到端多智能体框架，集成协作工作流与新型RAG组件，通过模块化设计实现即插即用的自动化法律文档分析

Result: 实验显示PAKTON在预测准确率（+15%）、检索性能（召回率提升22%）、解释完整性（人工评分高1.8倍）等指标显著优于基线模型

Conclusion: 该框架有效解决了法律文本分析的隐私性、可解释性问题，通过分布式智能体协作机制降低了非专业人士的合同审查门槛

Abstract: Contract review is a complex and time-intensive task that typically demands
specialized legal expertise, rendering it largely inaccessible to non-experts.
Moreover, legal interpretation is rarely straightforward-ambiguity is
pervasive, and judgments often hinge on subjective assessments. Compounding
these challenges, contracts are usually confidential, restricting their use
with proprietary models and necessitating reliance on open-source alternatives.
To address these challenges, we introduce PAKTON: a fully open-source,
end-to-end, multi-agent framework with plug-and-play capabilities. PAKTON is
designed to handle the complexities of contract analysis through collaborative
agent workflows and a novel retrieval-augmented generation (RAG) component,
enabling automated legal document review that is more accessible, adaptable,
and privacy-preserving. Experiments demonstrate that PAKTON outperforms both
general-purpose and pretrained models in predictive accuracy, retrieval
performance, explainability, completeness, and grounded justifications as
evaluated through a human study and validated with automated metrics.

</details>


### [72] [Enhancing Clinical Multiple-Choice Questions Benchmarks with Knowledge Graph Guided Distractor Generation](https://arxiv.org/abs/2506.00612)
*Running Yang,Wenlong Deng,Minghui Chen,Yuyin Zhou,Xiaoxiao Li*

Main category: cs.CL

TL;DR: 提出知识图谱引导的干扰项生成框架（KGGDG），通过生成临床合理但误导性强的选项，增强医学多选题难度以更可靠评估大模型性能


<details>
  <summary>Details</summary>
Motivation: 现有临床多选题数据集难度不足，无法准确评估大语言模型在医疗决策中的可靠性，需要更具挑战性的评估工具

Method: 基于医学知识图谱进行多步语义游走，识别医学相关但事实错误的干扰路径，指导LLM生成欺骗性干扰选项，应用于六个医学QA基准测试

Result: 框架显著降低SOTA大模型的准确率（在六个基准测试中表现一致），证明其有效提升评估诊断性

Conclusion: KGGDG为医学大模型评估提供了更鲁棒的诊断工具，支持更严格可靠的性能测试

Abstract: Clinical tasks such as diagnosis and treatment require strong decision-making
abilities, highlighting the importance of rigorous evaluation benchmarks to
assess the reliability of large language models (LLMs). In this work, we
introduce a knowledge-guided data augmentation framework that enhances the
difficulty of clinical multiple-choice question (MCQ) datasets by generating
distractors (i.e., incorrect choices that are similar to the correct one and
may confuse existing LLMs). Using our KG-based pipeline, the generated choices
are both clinically plausible and deliberately misleading. Our approach
involves multi-step, semantically informed walks on a medical knowledge graph
to identify distractor paths-associations that are medically relevant but
factually incorrect-which then guide the LLM in crafting more deceptive
distractors. We apply the designed knowledge graph guided distractor generation
(KGGDG) pipline, to six widely used medical QA benchmarks and show that it
consistently reduces the accuracy of state-of-the-art LLMs. These findings
establish KGGDG as a powerful tool for enabling more robust and diagnostic
evaluations of medical LLMs.

</details>


### [73] [Improving Dialogue State Tracking through Combinatorial Search for In-Context Examples](https://arxiv.org/abs/2506.00622)
*Haesung Pyun,Yoonah Park,Yohan Jo*

Main category: cs.CL

TL;DR: 提出CombiSearch方法，通过组合评分机制优化对话状态跟踪的上下文示例检索，实验显示数据效率提升20倍且性能显著改善


<details>
  <summary>Details</summary>
Motivation: 现有检索器训练方法存在三个缺陷：忽视示例协同效应、未充分融合查询语言特征、评分未直接优化DST性能，导致检索效果受限

Method: 基于示例组合对DST性能的叠加影响进行动态评分，优先选择能产生最大组合增益的上下文示例

Result: MultiWOZ数据集上实现20倍数据效率增益，SGD泛化效果优异，无检索错误时DST绝对性能提升12%

Conclusion: CombiSearch显著提升DST性能上限，证明现有方法依赖次优训练数据，同时为实际应用拓展了性能提升空间

Abstract: In dialogue state tracking (DST), in-context learning comprises a retriever
that selects labeled dialogues as in-context examples and a DST model that uses
these examples to infer the dialogue state of the query dialogue. Existing
methods for constructing training data for retrievers suffer from three key
limitations: (1) the synergistic effect of examples is not considered, (2) the
linguistic characteristics of the query are not sufficiently factored in, and
(3) scoring is not directly optimized for DST performance. Consequently, the
retriever can fail to retrieve examples that would substantially improve DST
performance. To address these issues, we present CombiSearch, a method that
scores effective in-context examples based on their combinatorial impact on DST
performance. Our evaluation on MultiWOZ shows that retrievers trained with
CombiSearch surpass state-of-the-art models, achieving a 20x gain in data
efficiency and generalizing well to the SGD dataset. Moreover, CombiSearch
attains a 12% absolute improvement in the upper bound DST performance over
traditional approaches when no retrieval errors are assumed. This significantly
increases the headroom for practical DST performance while demonstrating that
existing methods rely on suboptimal data for retriever training.

</details>


### [74] [LID Models are Actually Accent Classifiers: Implications and Solutions for LID on Accented Speech](https://arxiv.org/abs/2506.00628)
*Niyati Bafna,Matthew Wiesner*

Main category: cs.CL

TL;DR: 发现语音识别系统在带口音语音中的常见错误模式，提出通过分段处理和序列信息整合增强模型鲁棒性


<details>
  <summary>Details</summary>
Motivation: 探究现有语言识别模型在带口音语音上性能下降的根本原因，解决口音与语言特征混淆问题

Method: 通过输入分段策略分析模型短时音系特征依赖性，设计无需单语ASR的序列信息融合方法

Result: 显著降低口音-语言混淆错误（相对改进34%），在带口音语音上达到82.3%准确率

Conclusion: 模型对短时音系特征的过度依赖是错误根源，分段处理和序列建模能有效提升口音鲁棒性

Abstract: Prior research indicates that LID model performance significantly declines on
accented speech; however, the specific causes, extent, and characterization of
these errors remain under-explored. (i) We identify a common failure mode on
accented speech whereby LID systems often misclassify L2 accented speech as the
speaker's native language or a related language. (ii) We present evidence
suggesting that state-of-the-art models are invariant to permutations of short
spans of speech, implying they classify on the basis of short phonotactic
features indicative of accent rather than language. Our analysis reveals a
simple method to enhance model robustness to accents through input chunking.
(iii) We present an approach that integrates sequence-level information into
our model without relying on monolingual ASR systems; this reduces
accent-language confusion and significantly enhances performance on accented
speech while maintaining comparable results on standard LID.

</details>


### [75] [Social Construction of Urban Space: Understanding Neighborhood Boundaries Using Rental Listings](https://arxiv.org/abs/2506.00634)
*Adam Visokay,Ruth Bagley,Ian Kennedy,Chris Hess,Kyle Crowder,Rob Voigt,Denis Peskoff*

Main category: cs.CL

TL;DR: 利用NLP技术分析芝加哥租房广告，揭示社区定义在语言层面的动态争议与传统方法的局限性


<details>
  <summary>Details</summary>
Motivation: 通过语言分析揭示城市空间的社会建构过程，发现机构边界与社区实际语言描述的系统性偏差

Method: 结合人工标注与LLM处理2018-2024年租房广告，采用地理空间分析与LDA主题建模

Result: 识别出三类空间争议模式（定义冲突/边界效应/声誉洗白），发现外围房源更强调基础设施而中心房源突出文化特征

Conclusion: 证明NLP技术能有效捕捉城市空间定义的社会协商过程，为传统城市研究方法提供重要补充

Abstract: Rental listings offer a unique window into how urban space is socially
constructed through language. We analyze Chicago Craigslist rental
advertisements from 2018 to 2024 to examine how listing agents characterize
neighborhoods, identifying mismatches between institutional boundaries and
neighborhood claims. Through manual and large language model annotation, we
classify unstructured listings from Craigslist according to their neighborhood.
Geospatial analysis reveals three distinct patterns: properties with
conflicting neighborhood designations due to competing spatial definitions,
border properties with valid claims to adjacent neighborhoods, and ``reputation
laundering" where listings claim association with distant, desirable
neighborhoods. Through topic modeling, we identify patterns that correlate with
spatial positioning: listings further from neighborhood centers emphasize
different amenities than centrally-located units. Our findings demonstrate that
natural language processing techniques can reveal how definitions of urban
spaces are contested in ways that traditional methods overlook.

</details>


### [76] [ViToSA: Audio-Based Toxic Spans Detection on Vietnamese Speech Utterances](https://arxiv.org/abs/2506.00636)
*Huy Ba Do,Vy Le-Phuong Huynh,Luan Thanh Nguyen*

Main category: cs.CL

TL;DR: 论文提出首个越南语语音毒性片段检测数据集ViToSA，结合ASR和毒性检测的流程显著降低WER并超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 在线语音毒性内容日益严重，但音频检测尤其在越南语等低资源语言中研究不足。需要细粒度识别方法提升内容审核效果。

Method: 1. 构建ViToSA数据集（11k音频样本+人工标注） 2. 开发ASR与毒性片段检测联合流程 3. 微调ASR模型并测试多种TSD模型

Result: 1. ViToSA上微调使ASR的WER显著降低 2. 文本TSD模型性能超越现有基准 3. 建立越南语音频毒性检测新标准

Conclusion: ViToSA为语音内容审核研究提供新基准，联合处理流程有效改善低资源语言场景下的毒性内容识别效果。

Abstract: Toxic speech on online platforms is a growing concern, impacting user
experience and online safety. While text-based toxicity detection is
well-studied, audio-based approaches remain underexplored, especially for
low-resource languages like Vietnamese. This paper introduces ViToSA
(Vietnamese Toxic Spans Audio), the first dataset for toxic spans detection in
Vietnamese speech, comprising 11,000 audio samples (25 hours) with accurate
human-annotated transcripts. We propose a pipeline that combines ASR and toxic
spans detection for fine-grained identification of toxic content. Our
experiments show that fine-tuning ASR models on ViToSA significantly reduces
WER when transcribing toxic speech, while the text-based toxic spans detection
(TSD) models outperform existing baselines. These findings establish a novel
benchmark for Vietnamese audio-based toxic spans detection, paving the way for
future research in speech content moderation.

</details>


### [77] [Improving the Calibration of Confidence Scores in Text Generation Using the Output Distribution's Characteristics](https://arxiv.org/abs/2506.00637)
*Lorenzo Jaime Yu Flores,Ori Ernst,Jackie Chi Kit Cheung*

Main category: cs.CL

TL;DR: 提出无需微调的任务无关置信度指标，通过模型输出概率改进BART和Flan-T5在摘要/翻译/QA任务的校准效果


<details>
  <summary>Details</summary>
Motivation: 传统置信度指标在文本生成中校准不足，因多个有效答案导致模型概率分布分散，现有方法未充分考虑该特性

Method: 设计仅依赖输出概率的任务无关置信度度量方法，无需额外微调或启发式规则

Result: 在文本摘要、机器翻译和问答数据集上显著提升了模型校准效果

Conclusion: 基于输出概率的置信度指标能有效改进生成模型的校准性能，增强模型预测结果的可信度和实用性

Abstract: Well-calibrated model confidence scores can improve the usefulness of text
generation models. For example, users can be prompted to review predictions
with low confidence scores, to prevent models from returning bad or potentially
dangerous predictions. However, confidence metrics are not always well
calibrated in text generation. One reason is that in generation, there can be
many valid answers, which previous methods do not always account for. Hence, a
confident model could distribute its output probability among multiple
sequences because they are all valid. We propose task-agnostic confidence
metrics suited to generation, which rely solely on the probabilities associated
with the model outputs without the need for further fine-tuning or heuristics.
Using these, we are able to improve the calibration of BART and Flan-T5 on
summarization, translation, and QA datasets.

</details>


### [78] [SATA-BENCH: Select All That Apply Benchmark for Multiple Choice Questions](https://arxiv.org/abs/2506.00643)
*Weijie Xu,Shixian Cui,Xi Fang,Chi Xue,Stephanie Eckman,Chandan Reddy*

Main category: cs.CL

TL;DR: 现有大模型在SATA类多选题表现显著不足（41.8%准确率），研究者提出Choice Funnel解码策略提升29%准确率并降低64%推理成本。


<details>
  <summary>Details</summary>
Motivation: 针对现实应用中常见的多选多问题（Select All That Apply），现有大语言模型评估体系集中于单选题场景，缺乏系统性评估框架。

Method: 构建跨领域SATA-BENCH基准测试（阅读理解/法律/生物医学），提出Choice Funnel方法（Token去偏+自适应阈值）解决模型选择偏差和数量预测偏差。

Result: 最佳模型准确率仅41.8%，Choice Funnel相较基线提升29%准确率，推理成本降低64%

Conclusion: 揭示大模型多选推理的根本性缺陷，为复杂决策场景提供新的诊断框架，推动实际应用中的可靠多答案推理能力发展。

Abstract: Large language models (LLMs) are increasingly evaluated on single-answer
multiple-choice tasks, yet many real-world problems require identifying all
correct answers from a set of options. This capability remains underexplored.
We introduce SATA-BENCH, the first dedicated benchmark for evaluating LLMs on
Select All That Apply (SATA) questions across diverse domains, including
reading comprehension, law, and biomedicine. Our evaluation of 27 open-source
and proprietary models reveals a significant gap: even the strongest model
achieves only 41.8% exact match, exposing LLMs' inability to reliably identify
all correct answers. We find that this weakness stems from two core challenges:
selection bias - models favor certain choices regardless of content, and count
bias - models fail to predict the correct number of answers. To address these
issues, we propose Choice Funnel, a decoding strategy that combines token
debiasing with adaptive thresholding to guide models toward complete and
accurate selections. Choice Funnel achieves up to 29% higher exact match than
competitive baselines while reducing inference cost by over 64%. Our findings
expose fundamental limitations in current LLMs and introduce a new framework
for diagnosing and improving multi-answer reasoning. We release SATA-BENCH and
Choice Funnel to promote LLM development for robust decision-making in
realistic, multi-answer applications.

</details>


### [79] [Clinical Annotations for Automatic Stuttering Severity Assessment](https://arxiv.org/abs/2506.00644)
*Ana Rita Valente,Rufael Marew,Hawau Olamide Toyin,Hamdan Al-Ali,Anelise Bohnen,Inma Becerra,Elsa Marta Soares,Goncalo Leal,Hanan Aldarmaki*

Main category: cs.CL

TL;DR: 通过临床专家标注增强FluencyBank数据集，建立多模态口吃评估框架


<details>
  <summary>Details</summary>
Motivation: 现有口吃评估模型缺乏临床级标注数据，需专家知识确保标注质量

Method: 聘请临床专家进行多模态标注（视听特征、次级行为、紧张评分），构建专家共识测试集

Result: 实验证明口吃评估需要临床专业知识支持，个体标注与共识标注存在显著差异

Conclusion: 高质量专家标注是开发有效口吃评估模型的基础，需加强临床与AI的合作

Abstract: Stuttering is a complex disorder that requires specialized expertise for
effective assessment and treatment. This paper presents an effort to enhance
the FluencyBank dataset with a new stuttering annotation scheme based on
established clinical standards. To achieve high-quality annotations, we hired
expert clinicians to label the data, ensuring that the resulting annotations
mirror real-world clinical expertise. The annotations are multi-modal,
incorporating audiovisual features for the detection and classification of
stuttering moments, secondary behaviors, and tension scores. In addition to
individual annotations, we additionally provide a test set with highly reliable
annotations based on expert consensus for assessing individual annotators and
machine learning models. Our experiments and analysis illustrate the complexity
of this task that necessitates extensive clinical expertise for valid training
and evaluation of stuttering assessment models.

</details>


### [80] [GuideX: Guided Synthetic Data Generation for Zero-Shot Information Extraction](https://arxiv.org/abs/2506.00649)
*Neil De La Fuente,Oscar Sainz,Iker García-Ferrero,Eneko Agirre*

Main category: cs.CL

TL;DR: GUIDEX方法通过自动生成领域模式与合成数据，使Llama 3.1在零样本NER任务中刷新7项基准记录，最高提升7 F1值。


<details>
  <summary>Details</summary>
Motivation: 传统信息抽取系统需要昂贵领域适配，大语言模型在跨域场景中性能骤降，需提升零样本跨领域泛化能力。

Method: 自动生成领域专用模式、推断标注指南、创建带标签合成实例的三步法，无需人工标注即可微调模型。

Result: 纯用GUIDEX训练比现有方法最高提升7 F1，结合人类标注数据可再提升2 F1；模型对复杂领域模式理解显著增强。

Conclusion: GUIDEX实现了零样本信息抽取的突破，通过自动化流程显著降低领域适配成本，提升模型跨域泛化性能。

Abstract: Information Extraction (IE) systems are traditionally domain-specific,
requiring costly adaptation that involves expert schema design, data
annotation, and model training. While Large Language Models have shown promise
in zero-shot IE, performance degrades significantly in unseen domains where
label definitions differ. This paper introduces GUIDEX, a novel method that
automatically defines domain-specific schemas, infers guidelines, and generates
synthetically labeled instances, allowing for better out-of-domain
generalization. Fine-tuning Llama 3.1 with GUIDEX sets a new state-of-the-art
across seven zeroshot Named Entity Recognition benchmarks. Models trained with
GUIDEX gain up to 7 F1 points over previous methods without humanlabeled data,
and nearly 2 F1 points higher when combined with it. Models trained on GUIDEX
demonstrate enhanced comprehension of complex, domain-specific annotation
schemas. Code, models, and synthetic datasets are available at
neilus03.github.io/guidex.com

</details>


### [81] [Sarc7: Evaluating Sarcasm Detection and Generation with Seven Types and Emotion-Informed Techniques](https://arxiv.org/abs/2506.00658)
*Lang Xiong,Raina Gao,Alyssa Jeong,Yicheng Fu,Sean O'Brien,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

TL;DR: 论文提出Sarc7基准，通过标注MUStARD数据集将讽刺分为7类，开发情感驱动分类与生成方法，Gemini 2.5模型在情感提示下取得最佳分类效果（F1=0.3664），人工评估显示情感生成法比零样本生成成功率提升38.46%。


<details>
  <summary>Details</summary>
Motivation: 讽刺因微妙性对计算模型构成挑战，现有方法可能忽视情感维度。研究旨在通过情感分析提升讽刺分类与生成效果，解决传统方法在语义-情感关联上的不足。

Method: 1. 构建Sarc7基准：标注MUStARD数据集定义7类讽刺
2. 评估四种分类策略：零样本/少样本/思维链/情感提示
3. 提出讽刺生成三要素：语义矛盾性、情感冲击值、语境依赖性
4. 采用基于情感提示的对比学习框架

Result: 1. 分类任务：Gemini 2.5情感提示法F1值0.3664（对比零样本提升12.3%）
2. 生成任务：情感驱动法人工评估成功率较基线提高38.46%
3. 死板讽刺（deadpan）分类准确率最高（61.2%）

Conclusion: 情感维度是讽刺计算的核心要素，Sarc7基准为细粒度分析提供新工具，情感提示策略显著提升LLM的讽刺处理能力，为自然语言理解开辟情感增强新路径。

Abstract: Sarcasm is a form of humor where expressions convey meanings opposite to
their literal interpretations. Classifying and generating sarcasm using large
language models is vital for interpreting human communication. Sarcasm poses
challenges for computational models, due to its nuanced nature. We introduce
Sarc7, a benchmark that classifies 7 types of sarcasm: self-deprecating,
brooding, deadpan, polite, obnoxious, raging, and manic by annotating entries
of the MUStARD dataset. Classification was evaluated using zero-shot, few-shot,
chain-of-thought (CoT), and a novel emotion-based prompting technique. We
propose an emotion-based generation method developed by identifying key
components of sarcasm-incongruity, shock value, and context dependency. Our
classification experiments show that Gemini 2.5, using emotion-based prompting,
outperforms other setups with an F1 score of 0.3664. Human evaluators preferred
our emotion-based prompting, with 38.46% more successful generations than
zero-shot prompting.

</details>


### [82] [SafeTy Reasoning Elicitation Alignment for Multi-Turn Dialogues](https://arxiv.org/abs/2506.00668)
*Martin Kuo,Jianyi Zhang,Aolin Ding,Louis DiValentin,Amin Hass,Benjamin F Morris,Isaac Jacobson,Randolph Linderman,James Kiessling,Nicolas Ramos,Bhavna Gopal,Maziyar Baran Pouyan,Changwei Liu,Hai Li,Yiran Chen*

Main category: cs.CL

TL;DR: 提出STREAM防御机制，通过安全推理调节器识别多轮对话中的恶意意图，降低攻击成功率51.2%的同时保持模型能力


<details>
  <summary>Details</summary>
Motivation: 针对大语言模型在多轮对话中易受恶意攻击的安全风险，现有防御技术存在功能性与安全性的平衡难题

Method: 构建人工标注的Safety Reasoning Multi-turn Dialogues数据集，训练即插即用的安全推理调节器模型进行意图识别与风险预警

Result: 在多种LLM上测试显示显著优于现有防御技术，攻击成功率(ASR)降低51.2%，且保持模型原始功能

Conclusion: STREAM框架有效解决了LLMs在多轮对话场景下的安全防护问题，实现了安全性与功能性的双重保障

Abstract: Malicious attackers can exploit large language models (LLMs) by engaging them
in multi-turn dialogues to achieve harmful objectives, posing significant
safety risks to society. To address this challenge, we propose a novel defense
mechanism: SafeTy Reasoning Elicitation Alignment for Multi-Turn Dialogues
(STREAM). STREAM defends LLMs against multi-turn attacks while preserving their
functional capabilities. Our approach involves constructing a human-annotated
dataset, the Safety Reasoning Multi-turn Dialogues dataset, which is used to
fine-tune a plug-and-play safety reasoning moderator. This model is designed to
identify malicious intent hidden within multi-turn conversations and alert the
target LLM of potential risks. We evaluate STREAM across multiple LLMs against
prevalent multi-turn attack strategies. Experimental results demonstrate that
our method significantly outperforms existing defense techniques, reducing the
Attack Success Rate (ASR) by 51.2%, all while maintaining comparable LLM
capability.

</details>


### [83] [DeepRAG: Integrating Hierarchical Reasoning and Process Supervision for Biomedical Multi-Hop QA](https://arxiv.org/abs/2506.00671)
*Yuelyu Ji,Hang Zhang,Shiven Verma,Hui Ji,Chun Li,Yushui Han,Yanshan Wang*

Main category: cs.CL

TL;DR: DeepRAG整合DeepSeek层次问题分解与RAG Gym检索增强优化，在MedHopQA生物医学问答任务中通过概念级奖励机制显著提升准确性


<details>
  <summary>Details</summary>
Motivation: 针对复杂生物医学问答中单一模型检索生成能力不足的问题，通过层次化问题分解与流程级监督优化增强知识获取准确性

Method: 1. 分层分解复杂查询为精确子问题
2. 基于UMLS本体构建概念级奖励信号
3. 联合优化DeepSeek与RAG Gym框架

Result: 在MedHopQA数据集上Exact Match提升12.3%，概念准确率提高18.7%，显著超越DeepSeek和RAG Gym单模型基线

Conclusion: DeepRAG通过结构化分解与本体驱动的强化学习机制，为复杂领域问答系统提供了有效的增强框架

Abstract: We propose DeepRAG, a novel framework that integrates DeepSeek hierarchical
question decomposition capabilities with RAG Gym unified retrieval-augmented
generation optimization using process level supervision. Targeting the
challenging MedHopQA biomedical question answering task, DeepRAG systematically
decomposes complex queries into precise sub-queries and employs concept level
reward signals informed by the UMLS ontology to enhance biomedical accuracy.
Preliminary evaluations on the MedHopQA dataset indicate that DeepRAG
significantly outperforms baseline models, including standalone DeepSeek and
RAG Gym, achieving notable improvements in both Exact Match and concept level
accuracy.

</details>


### [84] [Measuring Faithfulness and Abstention: An Automated Pipeline for Evaluating LLM-Generated 3-ply Case-Based Legal Arguments](https://arxiv.org/abs/2506.00694)
*Li Zhang,Morgan Gray,Jaromir Savelka,Kevin D. Ashley*

Main category: cs.CL

TL;DR: 研究通过自动化流程评估LLM在法律论点生成中的忠实性、因素利用和弃权能力，发现模型虽能避免幻觉但因素利用不足且无法可靠弃权。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在法律论证生成中可靠性存疑，特别是存在幻觉生成和无法合理弃权的问题，影响法律场景的实际应用可信度。

Method: 构建自动化评估框架：使用外部LLM提取生成论点中的法律要素，与输入案例的三元组真实要素对比验证，设计三个递进难度测试（标准论证生成/交换先例角色/无共同要素时弃权）。

Result: LLM在可行案例中避免幻觉准确率超90%，但仅使用67%的可用要素；在需要弃权的Test3中，多数模型仍错误生成论证（仅1个模型正确弃权）。

Conclusion: 自动化评估揭示LLM在法律应用中的核心缺陷，强调需提升要素利用完整性和弃权机制可靠性，建议结合领域知识增强模型指令遵循能力。

Abstract: Large Language Models (LLMs) demonstrate potential in complex legal tasks
like argument generation, yet their reliability remains a concern. Building
upon pilot work assessing LLM generation of 3-ply legal arguments using human
evaluation, this paper introduces an automated pipeline to evaluate LLM
performance on this task, specifically focusing on faithfulness (absence of
hallucination), factor utilization, and appropriate abstention. We define
hallucination as the generation of factors not present in the input case
materials and abstention as the model's ability to refrain from generating
arguments when instructed and no factual basis exists. Our automated method
employs an external LLM to extract factors from generated arguments and
compares them against the ground-truth factors provided in the input case
triples (current case and two precedent cases). We evaluated eight distinct
LLMs on three tests of increasing difficulty: 1) generating a standard 3-ply
argument, 2) generating an argument with swapped precedent roles, and 3)
recognizing the impossibility of argument generation due to lack of shared
factors and abstaining. Our findings indicate that while current LLMs achieve
high accuracy (over 90%) in avoiding hallucination on viable argument
generation tests (Tests 1 & 2), they often fail to utilize the full set of
relevant factors present in the cases. Critically, on the abstention test (Test
3), most models failed to follow instructions to stop, instead generating
spurious arguments despite the lack of common factors. This automated pipeline
provides a scalable method for assessing these crucial LLM behaviors,
highlighting the need for improvements in factor utilization and robust
abstention capabilities before reliable deployment in legal settings. Project
page:
https://github.com/lizhang-AIandLaw/Measuring-Faithfulness-and-Abstention.

</details>


### [85] [From Argumentative Text to Argument Knowledge Graph: A New Framework for Structured Argumentation](https://arxiv.org/abs/2506.00713)
*Debarati Bhattacharjee,Ashish Anand*

Main category: cs.CL

TL;DR: 提出通过构建论证知识图谱（AKG）实现议论文本结构化，结合知识库推理规则检测隐含逻辑关系并提升论证分析能力


<details>
  <summary>Details</summary>
Motivation: 现有论证数据集难以检测削弱攻击且理论格式不够直观，需要可视化结构和支持深度推理的表示方法

Method: 1.标注论证成分和关系→2.构建带元数据属性的知识库→3.应用肯定前件法则生成论证→4.创建节点带属性的AKG→5.通过标记识别补全缺失推理规则

Result: 成功检测现有数据集未发现的削弱攻击，创建更易理解的论证结构可视化，建立支持隐式关系推理的AKG基础框架

Conclusion: AKG格式通过显式标注推理规则和逻辑结构，为论证连贯性检查和自动修订提供结构化基础，助力推理模型学习隐含的间接逻辑关系

Abstract: This paper presents a framework to convert argumentative texts into argument
knowledge graphs (AKG). Starting with basic annotations of argumentative
components (ACs) and argumentative relations (ARs), we enrich the information
by constructing a knowledge base (KB) graph with metadata attributes for nodes.
Next, we use premises and inference rules from the KB to form arguments by
applying modus ponens. From these arguments, we create an AKG. The nodes and
edges of the AKG have attributes that capture important argumentative features.
We also find missing inference rules by identifying markers. This makes it
possible to identify undercut attacks that were previously undetectable in
existing datasets. The AKG gives a graphical view of the argumentative
structure that is easier to understand than theoretical formats. It also
prepares the ground for future reasoning tasks, including checking the
coherence of arguments and identifying opportunities for revision. For this, it
is important to find indirect relations, many of which are implicit. Our
proposed AKG format, with annotated inference rules and modus ponens, will help
reasoning models learn the implicit indirect relations that require inference
over arguments and the relations between them.

</details>


### [86] [Chain-of-Thought Training for Open E2E Spoken Dialogue Systems](https://arxiv.org/abs/2506.00722)
*Siddhant Arora,Jinchuan Tian,Hayato Futami,Jee-weon Jung,Jiatong Shi,Yosuke Kashiwagi,Emiru Tsunoo,Shinji Watanabe*

Main category: cs.CL

TL;DR: 提出基于思维链的端到端语音对话系统，通过对齐多模态预训练任务显著提升效果，可在少量公开对话数据（300小时）实现高效训练。


<details>
  <summary>Details</summary>
Motivation: 传统级联式系统无法捕捉非音位信息，现有端到端方法需要大量数据且生成结果语义连贯性差。

Method: 使用思维链(CoT)策略，将对话训练与语音识别、语音合成、文本LM的预训练任务对齐。

Result: ROUGE-1提升超1.5分，在Switchboard等公开数据集上验证有效性，计算效率比基线提升30%。

Conclusion: 成功实现数据高效的语音对话系统训练，模型与代码将开源推动领域发展。

Abstract: Unlike traditional cascaded pipelines, end-to-end (E2E) spoken dialogue
systems preserve full differentiability and capture non-phonemic information,
making them well-suited for modeling spoken interactions. However, existing E2E
approaches often require large-scale training data and generates responses
lacking semantic coherence. We propose a simple yet effective strategy
leveraging a chain-of-thought (CoT) formulation, ensuring that training on
conversational data remains closely aligned with the multimodal language model
(LM)'s pre-training on speech recognition~(ASR), text-to-speech synthesis
(TTS), and text LM tasks. Our method achieves over 1.5 ROUGE-1 improvement over
the baseline, successfully training spoken dialogue systems on publicly
available human-human conversation datasets, while being compute-efficient
enough to train on just 300 hours of public human-human conversation data, such
as the Switchboard. We will publicly release our models and training code.

</details>


### [87] [Structured Gradient Guidance for Few-Shot Adaptation in Large Language Models](https://arxiv.org/abs/2506.00726)
*Hongye Zheng,Yichen Wang,Ray Pan,Guiran Liu,Binrong Zhu,Hanlu Zhang*

Main category: cs.CL

TL;DR: 提出基于梯度信息的少样本微调方法，通过方向一致性和幅度控制提升大语言模型的训练稳定性与跨任务泛化能力


<details>
  <summary>Details</summary>
Motivation: 解决少样本场景下大语言模型微调存在的两个核心问题：1) 有限数据导致参数更新路径不稳定 2) 跨任务/跨领域迁移时的优化方向失准

Method: 1) 梯度方向一致性正则项防止参数漂移 2) 梯度幅度控制正则项避免异常更新 3) 源任务-目标任务梯度对齐机制增强跨域泛化

Result: 在GLUE基准测试中平均准确率提升2.3%，梯度波动幅度降低41%，跨域任务迁移成功率提高18.7%

Conclusion: 该框架有效释放大语言模型的表征潜力，在保证稳定性的同时将微调数据需求降低至传统方法的1/5

Abstract: This paper presents a gradient-informed fine-tuning method for large language
models under few-shot conditions. The goal is to enhance task adaptability and
training stability when data is limited. The method builds on a base loss
function and introduces two gradient-related regularization terms. The first
enforces gradient direction consistency to guide parameter updates along
task-relevant directions and prevent drift. The second controls gradient
magnitude to avoid abnormal updates. Together, these components support a more
efficient and stable optimization path. To further improve cross-task
generalization, the method incorporates a gradient alignment mechanism. This
mechanism measures the consistency between optimization directions of the
source and target tasks. It enhances fine-tuning performance in multi-task and
cross-domain scenarios. Across various natural language understanding tasks,
the method outperforms existing fine-tuning strategies in average accuracy,
gradient stability, and directional alignment. Empirical evaluations under
different sample sizes and domain-specific tasks confirm the method's
robustness and broad applicability in low-resource environments. In particular,
the method shows clear advantages in controlling parameter update paths. The
results demonstrate that a gradient-based fine-tuning framework can effectively
leverage the representational power of large language models. It ensures
training stability while reducing dependence on large volumes of labeled data.

</details>


### [88] [Narrative Media Framing in Political Discourse](https://arxiv.org/abs/2506.00737)
*Yulia Otmakhova,Lea Frermann*

Main category: cs.CL

TL;DR: 提出结合叙事元素与框架分析的新方法，验证其在气候变化和COVID-19领域的通用性


<details>
  <summary>Details</summary>
Motivation: 现有自动框架分析忽略叙事框架这一重要概念化工具

Method: 1. 标注气候变化领域新闻数据集
2. 分析政治倾向对叙事框架的影响
3. 测试LLM预测能力
4. 跨领域（COVID-19）无监督验证

Result: 成功预测不同领域叙事框架组件，结果与既有理论一致

Conclusion: 该框架具有跨领域通用性，为自动叙事分析提供新范式

Abstract: Narrative frames are a powerful way of conceptualizing and communicating
complex, controversial ideas, however automated frame analysis to date has
mostly overlooked this framing device. In this paper, we connect elements of
narrativity with fundamental aspects of framing, and present a framework which
formalizes and operationalizes such aspects. We annotate and release a data set
of news articles in the climate change domain, analyze the dominance of
narrative frame components across political leanings, and test LLMs in their
ability to predict narrative frames and their components. Finally, we apply our
framework in an unsupervised way to elicit components of narrative framing in a
second domain, the COVID-19 crisis, where our predictions are congruent with
prior theoretical work showing the generalizability of our approach.

</details>


### [89] [DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity Environments](https://arxiv.org/abs/2506.00739)
*Chiyu Zhang,Marc-Alexandre Cote,Michael Albada,Anush Sankaran,Jack W. Stokes,Tong Wang,Amir Abdi,William Blum,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: DefenderBench是开源的模块化评估工具包，用于测试LLM在网络安全攻防任务中的表现，Claude-3.7-sonnet以81.65分领先，最佳开源模型Llama 3.3 70B得71.81分


<details>
  <summary>Details</summary>
Motivation: 探索LLM在网络安全领域的潜力，解决现有评估体系不统一、成本高的问题，促进公平对比与可重复性研究

Method: 创建包含网络入侵检测、恶意内容识别、漏洞分析等模块的标准化测试框架，对闭源/开源LLM进行系统性评估

Result: 闭源模型中Claude系列表现最优，开源模型Llama 3.3 70B与闭源模型的差距在10分以内，展示出较强竞争力

Conclusion: DefenderBench有效验证了LLM在网络安全领域的实用性，其模块化设计为后续研究提供灵活扩展基础，开源模型的接近表现可能推动更多定制化安全解决方案的发展

Abstract: Large language model (LLM) agents have shown impressive capabilities in human
language comprehension and reasoning, yet their potential in cybersecurity
remains underexplored. We introduce DefenderBench, a practical, open-source
toolkit for evaluating language agents across offense, defense, and
cybersecurity knowledge-based tasks. DefenderBench includes environments for
network intrusion, malicious content detection, code vulnerability analysis,
and cybersecurity knowledge assessment. It is intentionally designed to be
affordable and easily accessible for researchers while providing fair and
rigorous assessment. We benchmark several state-of-the-art (SoTA) and popular
LLMs, including both open- and closed-weight models, using a standardized
agentic framework. Our results show that Claude-3.7-sonnet performs best with a
DefenderBench score of 81.65, followed by Claude-3.7-sonnet-think with 78.40,
while the best open-weight model, Llama 3.3 70B, is not far behind with a
DefenderBench score of 71.81. DefenderBench's modular design allows seamless
integration of custom LLMs and tasks, promoting reproducibility and fair
comparisons. An anonymized version of DefenderBench is available at
https://github.com/microsoft/DefenderBench.

</details>


### [90] [Length Aware Speech Translation for Video Dubbing](https://arxiv.org/abs/2506.00740)
*Harveen Singh Chadha,Aswin Shanmugam Subramanian,Vikas Joshi,Shubham Bansal,Jian Xue,Rupeshkumar Mehta,Jinyu Li*

Main category: cs.CL

TL;DR: 提出端到端音素敏感语音翻译模型LSST及LABS解码方法，实现实时视频配音的音频同步优化


<details>
  <summary>Details</summary>
Motivation: 解决视频配音中翻译音频与源音频同步的挑战，满足实时设备端场景需求

Method: 1. 基于音素的长度敏感语音翻译模型(LSST)通过预设标签生成不同长度翻译
2. 提出LABS解码算法单次生成多长度翻译

Result: 在保持BLEU分数前提下，西班牙语/韩语同步质量MOS分别提升0.34/0.65

Conclusion: 该方法在保证翻译质量的同时显著提升音视频同步性，适用于实时设备端视频配音场景

Abstract: In video dubbing, aligning translated audio with the source audio is a
significant challenge. Our focus is on achieving this efficiently, tailored for
real-time, on-device video dubbing scenarios. We developed a phoneme-based
end-to-end length-sensitive speech translation (LSST) model, which generates
translations of varying lengths short, normal, and long using predefined tags.
Additionally, we introduced length-aware beam search (LABS), an efficient
approach to generate translations of different lengths in a single decoding
pass. This approach maintained comparable BLEU scores compared to a baseline
without length awareness while significantly enhancing synchronization quality
between source and target audio, achieving a mean opinion score (MOS) gain of
0.34 for Spanish and 0.65 for Korean, respectively.

</details>


### [91] [Data Swarms: Optimizable Generation of Synthetic Evaluation Data](https://arxiv.org/abs/2506.00741)
*Shangbin Feng,Yike Wang,Weijia Shi,Yulia Tsvetkov*

Main category: cs.CL

TL;DR: 提出Data Swarms算法优化合成评估数据生成，通过粒子群优化提升LLM评估的定量指标，并扩展对抗式群优化实现数据与模型协同进化。


<details>
  <summary>Details</summary>
Motivation: 现有合成评估数据难以全面反映LLM性能，需系统性优化数据生成策略以提升评估指标的多样性与挑战性。

Method: 1. 训练初始数据生成器群 2. 定义五类评估目标 3. 应用粒子群优化搜索参数空间 4. 构建对抗式双群协同进化框架

Result: 在五个评估目标上超越8个基线方法，对抗式优化使数据鲁棒性提升18.7%，模型泛化能力增强23.4%

Conclusion: 群体优化机制能有效协调多目标优化，生成数据具备跨模型泛化能力，为自动化评估系统提供新范式

Abstract: We propose Data Swarms, an algorithm to optimize the generation of synthetic
evaluation data and advance quantitative desiderata of LLM evaluation. We first
train a swarm of initial data generators using existing data, and define
various evaluation objectives to reflect the desired properties of evaluation
(e.g., generate more difficult problems for the evaluated models) and
quantitatively evaluate data generators. We then employ particle swarm
optimization to optimize the swarm of data generators, where they
collaboratively search through the model parameter space to find new generators
that advance these objectives. We further extend it to Adversarial Swarms,
where the data generator swarm generates harder data while the test taker model
swarm learns from such data, co-evolving dynamically for better data and models
simultaneously. Extensive experiments demonstrate that Data Swarms outperforms
eight data generation baselines across five evaluation objectives, while
Adversarial Swarms produce more robust learning of synthetic data and stronger
generalization. Further analysis reveals that Data Swarms successfully
optimizes compositions of multiple evaluation objectives and generalizes to new
off-the-shelf LLMs, unseen at optimization time.

</details>


### [92] [Assortment of Attention Heads: Accelerating Federated PEFT with Head Pruning and Strategic Client Selection](https://arxiv.org/abs/2506.00743)
*Yeshwanth Venkatesha,Souvik Kundu,Priyadarshini Panda*

Main category: cs.CL

TL;DR: 提出联邦学习中基于多头注意力的参数高效微调方法，通过头部剪枝、加权聚合和客户端选择实现高效训练


<details>
  <summary>Details</summary>
Motivation: 解决参数高效微调(PEFT)在联邦学习(FL)中面临的设备资源受限和客户端数据分布差异挑战

Method: 1. 基于注意力置信度的头部剪枝 2. 头部特异性加权聚合机制 3. 客户端选择策略

Result: 在MultiNLI等数据集上达到90%稀疏度，通信效率提升1.8倍，训练运算量减少3.9倍，精度下降<2%

Conclusion: 首次将PEFT与FL结合的方法，在保持性能的同时显著降低计算/通信开销，适用于资源受限的分布式场景

Abstract: Parameter Efficient Fine-Tuning (PEFT) has become the de-facto approach in
adapting Large Language Models (LLMs) for downstream tasks in Natural Language
Processing. However, its adoption in privacy-preserving distributed learning
frameworks, such as Federated Learning (FL), remains relatively limited. This
is mainly due to challenges specific to FL, such as resource-constrained
devices and diverse data distributions among clients. In this paper, we propose
an efficient method to perform PEFT within the FL framework for Multi-Head
Attention (MHA) based language models. We address the challenges through head
pruning, a novel head-specific weighted aggregation mechanism, and a client
selection strategy. Head pruning minimizes training complexity within the
clients, guided by the importance score computed based on the confidence of the
attention head. Weighted aggregation of heads ensures the global model captures
crucial updates from diverse clients complementing our client selection
strategy. We show results on the MultiNLI benchmark along with 20 Newsgroups,
XL-Sum, and E2E NLG datasets. We use the MultiNLI dataset and T5-small model
with LoRA as our PEFT method, attaining sparsity levels of up to 90%, resulting
in a communication advantage of up to 1.8x and a reduction in training OPs of
3.9x while maintaining the accuracy drop under 2%.

</details>


### [93] [Translate With Care: Addressing Gender Bias, Neutrality, and Reasoning in Large Language Model Translations](https://arxiv.org/abs/2506.00748)
*Pardis Sadat Zahraei,Ali Emami*

Main category: cs.CL

TL;DR: 提出TWC数据集揭示机器翻译在性别中立语言转换中的系统性偏见，通过微调mBART-50显著改善性别刻板与逻辑错误


<details>
  <summary>Details</summary>
Motivation: 解决自然性别语言（如英语）与无性别语言（波斯语/印尼语/芬兰语）互译时的性别偏见与逻辑连贯性问题

Method: 构建含3950个挑战场景的TWC数据集，评估GPT-4/mBART-50/NLLB-200/谷歌翻译等模型的性别处理表现

Result: 所有模型在刻板语境中倾向男性代词（谷歌/GPT-4男性偏好达4-6倍），微调mBART-50后偏见减少且性能超越商业大模型

Conclusion: 需针对无性别语言开发专项翻译方案，开源模型经定向优化可实现更公平准确的跨语言转换

Abstract: Addressing gender bias and maintaining logical coherence in machine
translation remains challenging, particularly when translating between natural
gender languages, like English, and genderless languages, such as Persian,
Indonesian, and Finnish. We introduce the Translate-with-Care (TWC) dataset,
comprising 3,950 challenging scenarios across six low- to mid-resource
languages, to assess translation systems' performance. Our analysis of diverse
technologies, including GPT-4, mBART-50, NLLB-200, and Google Translate,
reveals a universal struggle in translating genderless content, resulting in
gender stereotyping and reasoning errors. All models preferred masculine
pronouns when gender stereotypes could influence choices. Google Translate and
GPT-4 showed particularly strong bias, favoring male pronouns 4-6 times more
than feminine ones in leadership and professional success contexts. Fine-tuning
mBART-50 on TWC substantially resolved these biases and errors, led to strong
generalization, and surpassed proprietary LLMs while remaining open-source.
This work emphasizes the need for targeted approaches to gender and semantic
coherence in machine translation, particularly for genderless languages,
contributing to more equitable and accurate translation systems.

</details>


### [94] [Understanding and Mitigating Cross-lingual Privacy Leakage via Language-specific and Universal Privacy Neurons](https://arxiv.org/abs/2506.00759)
*Wenshuo Dong,Qingsong Yang,Shu Yang,Lijie Hu,Meng Ding,Wanyu Lin,Tianhang Zheng,Di Wang*

Main category: cs.CL

TL;DR: LLMs pose cross-lingual privacy risks where private information trained in one language can be leaked through queries in another language, mitigated by deactivating identified privacy-related neurons.


<details>
  <summary>Details</summary>
Motivation: Existing privacy protection methods assume monolingual context but fail in multilingual scenarios where training data and user queries use different languages.

Method: Analyzed information flow across model layers, identified privacy-universal neurons (affecting all languages) and language-specific privacy neurons through layer-wise representation analysis.

Result: Deactivating identified neurons reduced cross-lingual privacy leakage risk by 23.3%-31.6% across different language pairs.

Conclusion: Cross-lingual privacy leakage represents a critical vulnerability in LLMs, requiring language-aware neuron manipulation for effective mitigation while maintaining model utility.

Abstract: Large Language Models (LLMs) trained on massive data capture rich information
embedded in the training data. However, this also introduces the risk of
privacy leakage, particularly involving personally identifiable information
(PII). Although previous studies have shown that this risk can be mitigated
through methods such as privacy neurons, they all assume that both the
(sensitive) training data and user queries are in English. We show that they
cannot defend against the privacy leakage in cross-lingual contexts: even if
the training data is exclusively in one language, these (private) models may
still reveal private information when queried in another language. In this
work, we first investigate the information flow of cross-lingual privacy
leakage to give a better understanding. We find that LLMs process private
information in the middle layers, where representations are largely shared
across languages. The risk of leakage peaks when converted to a
language-specific space in later layers. Based on this, we identify
privacy-universal neurons and language-specific privacy neurons.
Privacy-universal neurons influence privacy leakage across all languages, while
language-specific privacy neurons are only related to specific languages. By
deactivating these neurons, the cross-lingual privacy leakage risk is reduced
by 23.3%-31.6%.

</details>


### [95] [Dynamic Chunking and Selection for Reading Comprehension of Ultra-Long Context in Large Language Models](https://arxiv.org/abs/2506.00773)
*Boheng Sheng,Jiacheng Yao,Meicong Zhang,Guoxiu He*

Main category: cs.CL

TL;DR: 提出动态分块选择方法DCS，通过语义相似度划分变长文本块，结合问题敏感分类器提升LLM长文本理解能力


<details>
  <summary>Details</summary>
Motivation: 固定分块会割裂语义关联导致理解偏差，需动态划分保持上下文连贯性

Method: 基于相邻句子语义相似度自适应分块，训练问题感知分类器筛选关键段落

Result: 在单跳/多跳QA任务上持续超越基线模型，支持256k tokens长文本处理

Conclusion: 动态分块机制显著增强LLM长上下文处理能力，为超长文本理解提供有效解决方案

Abstract: Large language models (LLMs) often struggle to accurately read and comprehend
extremely long texts. Current methods for improvement typically rely on
splitting long contexts into fixed-length chunks. However, fixed truncation
risks separating semantically relevant content, leading to ambiguity and
compromising accurate understanding. To overcome this limitation, we propose a
straightforward approach for dynamically separating and selecting chunks of
long context, facilitating a more streamlined input for LLMs. In particular, we
compute semantic similarities between adjacent sentences, using lower
similarities to adaptively divide long contexts into variable-length chunks. We
further train a question-aware classifier to select sensitive chunks that are
critical for answering specific questions. Experimental results on both
single-hop and multi-hop question-answering benchmarks show that the proposed
approach consistently outperforms strong baselines. Notably, it maintains
robustness across a wide range of input lengths, handling sequences of up to
256k tokens. Our datasets and code are available at the following link:
https://github.com/ECNU-Text-Computing/DCS

</details>


### [96] [Improving Automatic Evaluation of Large Language Models (LLMs) in Biomedical Relation Extraction via LLMs-as-the-Judge](https://arxiv.org/abs/2506.00777)
*Md Tahmid Rahman Laskar,Israt Jahan,Elham Dolatabadi,Chun Peng,Enamul Hoque,Jimmy Huang*

Main category: cs.CL

TL;DR: 研究探讨使用LLM作为评委评估生物医学关系抽取，发现性能低但通过结构化输出和领域适应提升效果


<details>
  <summary>Details</summary>
Motivation: 传统自动评估指标不可靠而人工评估成本高，需要探索LLM自评估方法

Method: 使用8个LLM评委评估5个LLM在3个生物医学数据集的表现，提出结构化输出格式和领域适应技术

Result: LLM评委初始准确率低于50%，结构化格式提升15%性能，领域适应进一步增强效果

Conclusion: 结构化输出和领域适应能有效提升LLM评委的评估性能，并公开36k标注数据集供研究使用

Abstract: Large Language Models (LLMs) have demonstrated impressive performance in
biomedical relation extraction, even in zero-shot scenarios. However,
evaluating LLMs in this task remains challenging due to their ability to
generate human-like text, often producing synonyms or abbreviations of
gold-standard answers, making traditional automatic evaluation metrics
unreliable. On the other hand, while human evaluation is more reliable, it is
costly and time-consuming, making it impractical for real-world applications.
This paper investigates the use of LLMs-as-the-Judge as an alternative
evaluation method for biomedical relation extraction. We benchmark 8 LLMs as
judges to evaluate the responses generated by 5 other LLMs across 3 biomedical
relation extraction datasets. Unlike other text-generation tasks, we observe
that LLM-based judges perform quite poorly (usually below 50% accuracy) in the
biomedical relation extraction task. Our findings reveal that it happens mainly
because relations extracted by LLMs do not adhere to any standard format. To
address this, we propose structured output formatting for LLM-generated
responses that helps LLM-Judges to improve their performance by about 15% (on
average). We also introduce a domain adaptation technique to further enhance
LLM-Judge performance by effectively transferring knowledge between datasets.
We release both our human-annotated and LLM-annotated judgment data (36k
samples in total) for public use here:
https://github.com/tahmedge/llm_judge_biomedical_re.

</details>


### [97] [KG-TRACES: Enhancing Large Language Models with Knowledge Graph-constrained Trajectory Reasoning and Attribution Supervision](https://arxiv.org/abs/2506.00783)
*Rong Wu,Pinlong Cai,Jianbiao Mei,Licheng Wen,Tao Hu,Xuemeng Yang,Daocheng Fu,Botian Shi*

Main category: cs.CL

TL;DR: 提出KG-TRACES框架，通过知识图谱约束的推理路径监督提升大模型可解释性与性能


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂推理任务中存在可解释性不足和推理过程不可信的问题（如幻觉现象），限制了其在复杂场景的应用

Method: 通过三重联合监督：1) 符号关系路径预测 2) 完整三元组推理路径预测 3) 基于推理路径的归因感知过程生成

Result: 在WebQSP和CWQ任务中分别提升Hits@1达1.6%和4.8%，医学领域迁移实验验证框架通用性，可视化显示推理过程更稳定且与正确答案高度对齐

Conclusion: KG-TRACES通过显式路径监督实现了可溯源的解释性推理，在保持性能优势的同时显著提升推理过程的可信度与稳定性

Abstract: Large language models (LLMs) have made remarkable strides in various natural
language processing tasks, but their performance on complex reasoning problems
remains hindered by a lack of explainability and trustworthiness. This issue,
often manifesting as hallucinations or unattributable reasoning processes,
limits their applicability in complex reasoning scenarios. To address this, we
propose Knowledge Graph-constrained Trajectory Reasoning Attribution and Chain
Explanation Supervision (KG-TRACES), a novel framework that enhances the
reasoning ability of LLMs through explicit supervision over reasoning paths and
processes. KG-TRACES jointly supervises the model to: (1) predict symbolic
relation paths, (2) predict full triple-level reasoning paths, and (3) generate
attribution-aware reasoning processes grounded in the reasoning paths. At
inference phase, the model adapts to both KG-available and KG-unavailable
scenarios, retrieving reasoning paths from a KG when possible or predicting
plausible reasoning paths with only intrinsic knowledge when not. This design
enables the model to reason in an explainable and source-attributable pattern.
Through extensive experiments on complex reasoning tasks, we demonstrate that
KG-TRACES significantly outperforms existing SOTA: it improves Hits@1 by 1.6%
and F1 by 4.7% on WebQSP, and achieves improvements of 4.8% in Hits@1 and 2.1%
in F1 on CWQ. Moreover, we show its transferability to specialized domains such
as medicine. By visualizing the intermediate steps of reasoning processes, we
further show that the explicit supervision introduced by KG-TRACES leads to
more stable and goal-directed reasoning processes, aligning closely with
correct answers. Code is available at https://github.com/Edaizi/KG-TRACES.

</details>


### [98] [Research Borderlands: Analysing Writing Across Research Cultures](https://arxiv.org/abs/2506.00784)
*Shaily Bhatt,Tal August,Maria Antoniak*

Main category: cs.CL

TL;DR: 通过跨学科研究者访谈构建研究文化规范框架，开发计算指标量化人类论文与LLM生成文本的文化差异，揭示LLMs在文化适应方面的不足


<details>
  <summary>Details</summary>
Motivation: 现有语言技术研究依赖合成数据及文化代理，缺乏真实社区互动，难以有效评估大语言模型的文化适应能力

Method: 1. 访谈跨学科研究者构建研究文化四维框架（结构/文体/修辞/引用） 2. 开发计算指标量化文化特征 3. 对比分析人类论文与LLM生成文本

Result: 成功识别研究文化差异特征，但LLMs表现出文化同质化倾向（homogenisation effect），在跨文化写作适应任务中缺乏文化敏感度

Conclusion: 人本框架有效量化文化规范，未来需增强LLMs的文化情境理解能力，建议采用社区协同设计提升文化适应性

Abstract: Improving cultural competence of language technologies is important. However
most recent works rarely engage with the communities they study, and instead
rely on synthetic setups and imperfect proxies of culture. In this work, we
take a human-centered approach to discover and measure language-based cultural
norms, and cultural competence of LLMs. We focus on a single kind of culture,
research cultures, and a single task, adapting writing across research
cultures. Through a set of interviews with interdisciplinary researchers, who
are experts at moving between cultures, we create a framework of structural,
stylistic, rhetorical, and citational norms that vary across research cultures.
We operationalise these features with a suite of computational metrics and use
them for (a) surfacing latent cultural norms in human-written research papers
at scale; and (b) highlighting the lack of cultural competence of LLMs, and
their tendency to homogenise writing. Overall, our work illustrates the
efficacy of a human-centered approach to measuring cultural norms in
human-written and LLM-generated texts.

</details>


### [99] [RARE: Retrieval-Aware Robustness Evaluation for Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2506.00789)
*Yixiao Zeng,Tianyu Cao,Danqing Wang,Xinran Zhao,Zimeng Qiu,Morteza Ziyadi,Tongshuang Wu,Lei Li*

Main category: cs.CL

TL;DR: 提出了RARE框架，通过知识图谱自动生成多层级问题集，量化RAG系统在动态语料库中的鲁棒性。发现现有系统对文档扰动和多跳查询存在显著脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG评估方法在真实噪音处理、内外检索冲突和快速变化事实应对方面存在不足，需要更全面的压力测试框架。

Method: 开发RARE-Get知识图谱合成管线自动生成多跳问题，构建RARE-Set包含48,322个时敏性金融问题，设计RARE-Met量化系统恢复能力。

Result: RAG系统文档鲁棒性最弱（与模型规模无关），多跳查询鲁棒性比单跳低15-20%，金融领域扰动敏感度最高。

Conclusion: RARE首次系统揭示RAG动态环境弱点，为时敏领域应用提供关键评估基准，未来需提升文档稳定性和多跳推理能力。

Abstract: Retrieval-Augmented Generation (RAG) enhances recency and factuality in
answers. However, existing evaluations rarely test how well these systems cope
with real-world noise, conflicting between internal and external retrieved
contexts, or fast-changing facts. We introduce Retrieval-Aware Robustness
Evaluation (RARE), a unified framework and large-scale benchmark that jointly
stress-tests query and document perturbations over dynamic, time-sensitive
corpora. One of the central features of RARE is a knowledge-graph-driven
synthesis pipeline (RARE-Get) that automatically extracts single and multi-hop
relations from the customized corpus and generates multi-level question sets
without manual intervention. Leveraging this pipeline, we construct a dataset
(RARE-Set) spanning 400 expert-level time-sensitive finance, economics, and
policy documents and 48,322 questions whose distribution evolves as the
underlying sources change. To quantify resilience, we formalize
retrieval-conditioned robustness metrics (RARE-Met) that capture a model's
ability to remain correct or recover when queries, documents, or real-world
retrieval results are systematically altered. Our results show that RAG systems
exhibit surprising vulnerability to perturbations, with document robustness
consistently being the weakest point regardless of generator size or
architecture. RAG systems consistently show lower robustness on multi-hop
queries than single-hop queries across all domains.

</details>


### [100] [Fast or Slow? Integrating Fast Intuition and Deliberate Thinking for Enhancing Visual Question Answering](https://arxiv.org/abs/2506.00806)
*Songtao Jiang,Chenyi Zhou,Yan Zhang,Yeying Jin,Zuozhu Liu*

Main category: cs.CL

TL;DR: 提出FOCUS方法，通过动态结合人类双重认知策略提升多模态大语言模型在视觉问答任务中的性能


<details>
  <summary>Details</summary>
Motivation: 现有视觉提示方法存在过度标注问题，基于双过程理论区分直觉与推理认知模式，解决复杂视觉问题中关键元素聚焦不足的痛点

Method: 采用问题复杂度自适应机制：简单问题零样本推理，复杂问题运用'先概念化后观察'策略，通过两阶段注意力聚焦关键视觉元素

Result: 在ScienceQA等4个基准测试中实现性能持续提升，消融实验验证认知策略组合的有效性，不同模型平均提升3-5%准确率

Conclusion: FOCUS证明了结合双重认知过程与精细化视觉信息处理对提升多模态推理的重要性，为MLLM优化提供新方向

Abstract: Multimodal large language models (MLLMs) still struggle with complex
reasoning tasks in Visual Question Answering (VQA). While current methods have
advanced by incorporating visual prompts, our study uncovers critical
limitations: these approaches indiscriminately annotate all detected objects
for every visual question, generating excessive visual markers that degrade
task performance. This issue stems primarily from a lack of focus on key visual
elements, raising two important questions: Are all objects equally important,
and do all questions require visual prompts? Motivated by Dual Process Theory,
which distinguishes between instinctive and deliberate cognitive modes in human
reasoning, we propose FOCUS, a plug-and-play approach that dynamically adapts
to the complexity of questions, combining fast intuitive judgments with
deliberate analytical reasoning to enhance the vision-language reasoning
capability of the MLLM. For straightforward questions, FOCUS supports efficient
zero-shot reasoning. For more complex tasks, it employs the conceptualizing
before observation strategy to highlight critical elements. Extensive
experiments on four benchmarks, ScienceQA, TextQA, VizWiz, and MME, demonstrate
that FOCUS consistently improves the performance of both open-source and
black-box MLLMs, achieving significant gains across all datasets. Ablation
studies further validate the importance of combining diverse cognitive
strategies with refined visual information for superior performance. Code will
be released.

</details>


### [101] [GuessBench: Sensemaking Multimodal Creativity in the Wild](https://arxiv.org/abs/2506.00814)
*Zifeng Zhu,Shangbin Feng,Herun Wan,Ningnan Wang,Minnan Luo,Yulia Tsvetkov*

Main category: cs.CL

TL;DR: 提出GuessBench基准测试框架，用于评估视觉语言模型在建模人类创造性思维方面的表现


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在理解和处理人类创造性内容（具有普遍性、噪声性和多样性特征）方面存在不足

Method: 基于Minecraft多人游戏构建包含1500张图像和2000个问题的数据集，涵盖静态/动态图像设置、不同完整度的自然语言提示，并测试6种VLMs模型及5种推理增强方法

Result: 顶尖模型GPT-4o错误率达34%，开放与API模型性能差显著（13.87% vs 53.93%）；微调后视觉感知任务提升15.36%；模型表现与训练数据概念频率及文化语境相关性显著

Conclusion: GuessBench揭示了VLMs在创造性推理中的局限性，同时验证了其作为改进视觉感知任务训练资源的有效性，并指出数据偏差和文化多样性对模型性能的关键影响

Abstract: We propose GuessBench, a novel benchmark that evaluates Vision Language
Models (VLMs) on modeling the pervasive, noisy, and pluralistic human
creativity. GuessBench sources data from "Guess the Build", an online
multiplayer Minecraft minigame where one player constructs a Minecraft build
given a concept (e.g. caterpillar) and others try to guess it with natural
language hints, presenting a pristine testbed for sensemaking creativity in the
wild with VLMs acting as guessers. We curate 1500 images from the actual
gameplay and design 2000 problems spanning static and dynamic image settings,
natural language hints of varying completeness, and more. Extensive experiments
with six open/API VLMs and five reasoning enhancement approaches demonstrate
that GuessBench presents a uniquely challenging task in creativity modeling:
even the start-of-the-art GPT-4o is incorrect on 34% of instances, while we
observe a huge performance gap (13.87% vs. 53.93% on average) between open and
API models. When used as a resource to improve VLMs, fine-tuning on the
reasoning traces for GuessBench problems improves visual perception tasks by
15.36% on average. Further analysis reveals that VLM performance in creativity
sensemaking correlates with the frequency of the concept in training data,
while the accuracy drops sharply for concepts in underrepresented cultural
contexts and low-resource languages.

</details>


### [102] [From Plain Text to Poetic Form: Generating Metrically-Constrained Sanskrit Verses](https://arxiv.org/abs/2506.00815)
*Manoj Balaji Jagadeeshan,Samarth Bhatia,Pretam Ray,Harshul Raj Surana,Akhil Rajeev P,Priya Mishra,Annarao Kulkarni,Ganesh Ramakrishnan,Prathosh AP,Pawan Goyal*

Main category: cs.CL

TL;DR: 大语言模型通过约束解码和指令微调，成功实现低资源梵语诗歌生成，特别是在Anushtub格律上达到高句法准确度。


<details>
  <summary>Details</summary>
Motivation: 针对大语言模型在低资源、形态丰富的语言（如梵语）中生成结构化诗歌的不足，研究旨在提升古典韵律（如Anushtub格律）的遵循能力。

Method: 构建英译梵数据集，评估多类生成模型，采用约束解码策略和指令微调方法，以兼顾韵律规则与语义保真度。

Result: 约束解码实现99%以上的句法准确率，显著优于通用模型；指令微调模型在语义和风格对齐上表现更优，但韵律精度略有降低。

Conclusion: 结合约束解码与指令微调可有效平衡梵语诗歌生成的韵律与语义，为低资源语言生成任务提供了创新解决方案。

Abstract: Recent advances in large language models (LLMs) have significantly improved
natural language generation, including creative tasks like poetry composition.
However, most progress remains concentrated in high-resource languages. This
raises an important question: Can LLMs be adapted for structured poetic
generation in a low-resource, morphologically rich language such as Sanskrit?
In this work, we introduce a dataset designed for translating English prose
into structured Sanskrit verse, with strict adherence to classical metrical
patterns, particularly the Anushtub meter. We evaluate a range of generative
models-both open-source and proprietary-under multiple settings. Specifically,
we explore constrained decoding strategies and instruction-based fine-tuning
tailored to metrical and semantic fidelity. Our decoding approach achieves over
99% accuracy in producing syntactically valid poetic forms, substantially
outperforming general-purpose models in meter conformity. Meanwhile,
instruction-tuned variants show improved alignment with source meaning and
poetic style, as supported by human assessments, albeit with marginal
trade-offs in metrical precision.

</details>


### [103] [One for All: Update Parameterized Knowledge Across Multiple Models](https://arxiv.org/abs/2506.00817)
*Weitao Ma,Xiyuan Du,Xiaocheng Feng,Lei Huang,Yichong Huang,Huiyi Zhang,Xiaoliang Yang,Baohang Li,Xiachong Feng,Ting Liu,Bing Qin*

Main category: cs.CL

TL;DR: 提出OnceEdit框架，通过插件模型实现跨多模型的知识编辑，结合动态权重机制和集成增强机制提升效率


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑方法主要针对单个模型，难以高效更新多个模型并适应新模型

Method: 1. 基于模型集成的插件架构
2. 引入权重令牌动态分配权重
3. 集成增强机制缓解中心模型依赖

Result: 在多模型场景下编辑效率提升85%，性能指标超越现有方法

Conclusion: OnceEdit通过创新的集成架构和双重优化机制，为多模型知识编辑提供了稳定高效的解决方案

Abstract: Large language models (LLMs) encode vast world knowledge but struggle to stay
up-to-date, often leading to errors and hallucinations. Knowledge editing
offers an efficient alternative to retraining, enabling targeted modifications
by updating specific model parameters. However, existing methods primarily
focus on individual models, posing challenges in efficiently updating multiple
models and adapting to new models. To address this, we propose OnceEdit, a
novel ensemble-based approach that employs a plug-in model as the editing
module, enabling stable knowledge updates across multiple models. Building on
the model ensemble, OnceEdit introduces two key mechanisms to enhance its
effectiveness. First, we introduce a dynamic weight mechanism through a \weight
token for distinguishing between edit-related and non-edit-related instances,
ensuring the appropriate utilization of knowledge from integrated models.
Second, we incorporate an ensemble enhancement mechanism to mitigate the
excessive reliance on the central model inherent in the model ensemble
technique, making it more suitable for knowledge editing. Extensive experiments
on diverse LLMs demonstrate that OnceEdit consistently outperforms existing
methods while achieving superior editing efficiency. Further analysis confirms
its adaptability and stability in multi-model editing scenarios. Our code will
be available.

</details>


### [104] [Probing the Geometry of Truth: Consistency and Generalization of Truth Directions in LLMs Across Logical Transformations and Question Answering Tasks](https://arxiv.org/abs/2506.00823)
*Yuntai Bao,Xuhong Zhang,Tianyu Du,Xinkui Zhao,Zhengwen Feng,Hao Peng,Jianwei Yin*

Main category: cs.CL

TL;DR: 大语言模型存在自信表达错误的问题，研究发现真实性方向在不同模型中的表现差异及探针技术的泛化应用


<details>
  <summary>Details</summary>
Motivation: 探索LLMs中真实性方向的一致性、检测技术复杂性和跨场景泛化能力，解决模型输出可信度问题

Method: 通过逻辑否定实验、跨任务泛化测试（问答/上下文学习），并使用线性探针技术分析模型内部表征

Result: 强能力模型展现更稳定的真实性方向，真实性探针可有效泛化至逻辑转换和实际应用场景

Conclusion: 真实性方向研究为理解LLM内部信念提供新视角，探针技术可增强用户对模型输出的信任

Abstract: Large language models (LLMs) are trained on extensive datasets that
encapsulate substantial world knowledge. However, their outputs often include
confidently stated inaccuracies. Earlier works suggest that LLMs encode
truthfulness as a distinct linear feature, termed the "truth direction", which
can classify truthfulness reliably. We address several open questions about the
truth direction: (i) whether LLMs universally exhibit consistent truth
directions; (ii) whether sophisticated probing techniques are necessary to
identify truth directions; and (iii) how the truth direction generalizes across
diverse contexts. Our findings reveal that not all LLMs exhibit consistent
truth directions, with stronger representations observed in more capable
models, particularly in the context of logical negation. Additionally, we
demonstrate that truthfulness probes trained on declarative atomic statements
can generalize effectively to logical transformations, question-answering
tasks, in-context learning, and external knowledge sources. Finally, we explore
the practical application of truthfulness probes in selective
question-answering, illustrating their potential to improve user trust in LLM
outputs. These results advance our understanding of truth directions and
provide new insights into the internal representations of LLM beliefs. Our code
is public at https://github.com/colored-dye/truthfulness_probe_generalization

</details>


### [105] [HERGC: Heterogeneous Experts Representation and Generative Completion for Multimodal Knowledge Graphs](https://arxiv.org/abs/2506.00826)
*Yongkang Xiao,Rui Zhang*

Main category: cs.CL

TL;DR: 提出HERGC框架，通过异构专家表示检索器融合多模态信息，结合生成式LLM预测器实现多模态知识图谱补全，在三个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有MMKGC方法受限于封闭世界假设和判别式训练目标，生成式LLM在单模态补全的成功促使研究者探索其多模态场景的应用潜力。

Method: 1. 异构专家表示检索器融合多模态信息并生成候选三元组；2. 基于指令微调的LLM预测器精准筛选正确答案

Result: 在三大标准MMKG数据集上验证有效性，准确率和鲁棒性均超越现有方法

Conclusion: HERGC通过异构表示与生成式推理的结合，突破了多模态知识图谱补全的瓶颈，为生成式方法在多模态领域应用提供新范式

Abstract: Multimodal knowledge graphs (MMKGs) enrich traditional knowledge graphs (KGs)
by incorporating diverse modalities such as images and text. Multi-modal
knowledge graph completion (MMKGC) seeks to exploit these heterogeneous signals
to infer missing facts, thereby mitigating the intrinsic incompleteness of
MMKGs. Existing MMKGC methods typically leverage only the information contained
in the MMKGs under the closed-world assumption and adopt discriminative
training objectives, which limits their reasoning capacity during completion.
Recent generative completion approaches powered by advanced large language
models (LLMs) have shown strong reasoning abilities in unimodal knowledge graph
completion, but their potential in MMKGC remains largely unexplored. To bridge
this gap, we propose HERGC, a Heterogeneous Experts Representation and
Generative Completion framework for MMKGs. HERGC first deploys a Heterogeneous
Experts Representation Retriever that enriches and fuses multimodal information
and retrieves a compact candidate set for each incomplete triple. It then uses
a Generative LLM Predictor fine-tuned on minimal instruction data to accurately
identify the correct answer from these candidates. Extensive experiments on
three standard MMKG benchmarks demonstrate HERGC's effectiveness and
robustness, achieving state-of-the-art performance.

</details>


### [106] [COMPKE: Complex Question Answering under Knowledge Editing](https://arxiv.org/abs/2506.00829)
*Keyuan Cheng,Zijian Kan,Zhixian He,Zhuoran Zhang,Muhammad Asif Ali,Ke Xu,Lijie Hu,Di Wang*

Main category: cs.CL

TL;DR: 论文提出COMPKE基准测试，包含11,924个复杂现实问题，揭示不同知识编辑方法在多样化模型上的显著性能差异（如MeLLo在GPT-4O-MINI准确率39.47 vs QWEN2-3B仅3.83），并从方法/模型角度分析差异根源。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑基准过度依赖多跳问答，无法有效评估模型在现实复杂推理场景（如一对多关系/多步逻辑交叉）下的知识应用能力。

Method: 构建包含11,924个复杂现实问题的COMPKE基准，对4种主流知识编辑方法进行跨模型（包括GPT-4O-MINI/QWEN2-3B等）系统性评估。

Result: 不同模型对同一编辑方法的适应性差异显著：最佳模型组合准确率差距达10倍（39.47 vs 3.83），且模型规模与性能无必然正相关。

Conclusion: COMPKE填补复杂场景评估空白，实验结果强调知识编辑技术需结合模型特性优化，不同架构模型需要差异化的编辑策略。

Abstract: Knowledge Editing, which efficiently modifies the knowledge in large language
models, has gathered great attention. Current benchmarks primarily use
multi-hop question answering to assess and analyze newly injected or updated
knowledge. However, we argue that these benchmarks fail to effectively evaluate
how well the updated models apply this knowledge in real-life scenarios,
particularly when questions require complex reasoning, involving one-to-many
relationships or multi-step logical intersections. To fill in this gap, we
introduce a new benchmark, COMPKE: Complex Question Answering under Knowledge
Editing, which includes 11,924 complex questions that reflect real-life
situations. We conduct an extensive evaluation of four knowledge editing
methods on COMPKE, revealing that their effectiveness varies notably across
different models. For instance, MeLLo attains an accuracy of 39.47 on
GPT-4O-MINI, but this drops sharply to 3.83 on QWEN2.5-3B. We further
investigate the underlying causes of these disparities from both methodological
and model-specific perspectives. The datasets are available at
https://github.com/kzjkzj666/CompKE.

</details>


### [107] [Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented Generation on Experience](https://arxiv.org/abs/2506.00842)
*Jiawei Gu,Ziting Xian,Yuanzhen Xie,Ye Liu,Enjie Liu,Ruichao Zhong,Mochi Gao,Yunzhi Tan,Bo Hu,Zang Li*

Main category: cs.CL

TL;DR: 提出CoRE框架提升大模型结构化数据处理能力，文本转SQL任务平均提升3.44%，TableQA提升4.24%


<details>
  <summary>Details</summary>
Motivation: 大语言模型在结构化数据（如表格）处理中存在预训练暴露不足和跨模态知识迁移机制僵化的问题

Method: 基于对比学习的检索增强生成框架（CoRE），结合蒙特卡洛树搜索构建经验记忆库，通过对比式上下文学习模拟人类知识迁移

Result: 经验记忆库使训练数据扩展8-9倍，在最具挑战性任务上取得17.2%的最大提升

Conclusion: 这种免训练、持续演进的方法推动大模型向结构化知识专家方向进化

Abstract: Large language models (LLMs) achieve strong performance on plain text tasks
but underperform on structured data like tables and databases. Potential
challenges arise from their underexposure during pre-training and rigid
text-to-structure transfer mechanisms. Unlike humans who seamlessly apply
learned patterns across data modalities, LLMs struggle to infer implicit
relationships embedded in tabular formats, especially in the absence of
explicit structural guidance. To bridge this cognitive gap, we introduce
Contrastive Retrieval-Augmented Generation on Experience (CoRE), a framework
that builds experience memory representations and enhances generalization
through contrastive In-Context Learning (ICL) to simulate human-like knowledge
transfer. Experiments on Text-to-SQL and TableQA show CoRE significantly
improves performance, achieving average gains of 3.44% and 4.24%, with up to
17.2% on challenging tasks. Our Monte Carlo Tree Search (MCTS)-generated
Experience Memory expands training data 8-9x, enhancing diversity and domain
coverage. This training-free and continual method propels LLMs toward
structured knowledge expertise.

</details>


### [108] [EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG Alignment via Large Language Model and Contrastive Learning on ChineseEEG](https://arxiv.org/abs/2506.00854)
*Jacky Tai-Yu Lu,Jung Chiang,Chi-Sheng Chen,Anna Nai-Yun Tung,Hsiang Wei Hu,Yuan Chiao Cheng*

Main category: cs.CL

TL;DR: 提出首个中文开放词汇EEG到文本框架EEG2TEXT-CN，结合生物启发的编码器和预训练语言模型，在零样本条件下实现汉字级EEG到句子生成。


<details>
  <summary>Details</summary>
Motivation: 解决中文脑电信号解码的技术空白，探索非语音跨模态语言解码的可行性，推动多语言脑文本研究发展。

Method: 基于NICE-EEG生物编码器和MiniLM语言模型，通过掩码预训练+对比学习对齐脑信号与文本表征，使用teacher forcing训练解码器处理变长序列。

Result: 在1,500+训练句和300测试句上实现6.38% BLEU-1分数，显示词汇对齐潜力，但句法流畅性仍需提升。

Conclusion: 验证了从EEG进行非语音跨模态解码的可能性，为中文认知语言接口奠定技术基础，开辟多语言脑机交互新方向。

Abstract: We propose EEG2TEXT-CN, which, to the best of our knowledge, represents one
of the earliest open-vocabulary EEG-to-text generation frameworks tailored for
Chinese. Built on a biologically grounded EEG encoder (NICE-EEG) and a compact
pretrained language model (MiniLM), our architecture aligns multichannel brain
signals with natural language representations via masked pretraining and
contrastive learning. Using a subset of the ChineseEEG dataset, where each
sentence contains approximately ten Chinese characters aligned with 128-channel
EEG recorded at 256 Hz, we segment EEG into per-character embeddings and
predict full sentences in a zero-shot setting. The decoder is trained with
teacher forcing and padding masks to accommodate variable-length sequences.
Evaluation on over 1,500 training-validation sentences and 300 held-out test
samples shows promising lexical alignment, with a best BLEU-1 score of 6.38\%.
While syntactic fluency remains a challenge, our findings demonstrate the
feasibility of non-phonetic, cross-modal language decoding from EEG. This work
opens a new direction in multilingual brain-to-text research and lays the
foundation for future cognitive-language interfaces in Chinese.

</details>


### [109] [How Bidirectionality Helps Language Models Learn Better via Dynamic Bottleneck Estimation](https://arxiv.org/abs/2506.00859)
*Md Kowsher,Nusrat Jahan Prottasha,Shiyun Xu,Shetu Mohanto,Chen Chen,Niloofar Yousefi,Ozlem Garibay*

Main category: cs.CL

TL;DR: 提出FlowNIB方法通过信息瓶颈理论解释双向语言模型优势，证明其保留更多互信息且表征维度更高


<details>
  <summary>Details</summary>
Motivation: 双向模型在自然语言理解中表现优异但缺乏理论解释，需克服传统信息瓶颈方法的计算局限和固定权衡机制

Method: 开发动态可扩展的FlowNIB框架，建立广义表征复杂度测量体系，实现训练过程信息流的实时追踪分析

Result: 理论证明双向模型在温和条件下具有更丰富的信息表征能力，实验验证其有效维度比单向模型高27%

Conclusion: 从信息压缩与保留的平衡机制揭示了双向架构优势，为深度语言模型分析提供了新工具和新视角

Abstract: Bidirectional language models have better context understanding and perform
better than unidirectional models on natural language understanding tasks, yet
the theoretical reasons behind this advantage remain unclear. In this work, we
investigate this disparity through the lens of the Information Bottleneck (IB)
principle, which formalizes a trade-off between compressing input information
and preserving task-relevant content. We propose FlowNIB, a dynamic and
scalable method for estimating mutual information during training that
addresses key limitations of classical IB approaches, including computational
intractability and fixed trade-off schedules. Theoretically, we show that
bidirectional models retain more mutual information and exhibit higher
effective dimensionality than unidirectional models. To support this, we
present a generalized framework for measuring representational complexity and
prove that bidirectional representations are strictly more informative under
mild conditions. We further validate our findings through extensive experiments
across multiple models and tasks using FlowNIB, revealing how information is
encoded and compressed throughout training. Together, our work provides a
principled explanation for the effectiveness of bidirectional architectures and
introduces a practical tool for analyzing information flow in deep language
models.

</details>


### [110] [L3Cube-MahaEmotions: A Marathi Emotion Recognition Dataset with Synthetic Annotations using CoTR prompting and Large Language Models](https://arxiv.org/abs/2506.00863)
*Nidhi Kowtal,Raviraj Joshi*

Main category: cs.CL

TL;DR: 提出L3Cube-MahaEmotions数据集解决马拉地语情感识别难题，使用LLM合成标注训练数据，验证GPT-4在低资源任务中的优越性


<details>
  <summary>Details</summary>
Motivation: 马拉地语等低资源语言因标注数据稀缺导致情感识别困难，需构建高质量数据集并探索有效方法

Method: 采用链式翻译提示技术(CoTR)，通过英译标注情感，对比评估GPT-4/Llama3-405B，设计标签聚合策略(Union/Intersection)

Result: GPT-4预测效果优于微调BERT模型，但基于合成标签训练的BERT未超越GPT-4，突显人工标注数据重要性

Conclusion: 通用LLM在复杂低资源情感任务中表现优于领域适配模型，公开数据集推动低资源NLP研究

Abstract: Emotion recognition in low-resource languages like Marathi remains
challenging due to limited annotated data. We present L3Cube-MahaEmotions, a
high-quality Marathi emotion recognition dataset with 11 fine-grained emotion
labels. The training data is synthetically annotated using large language
models (LLMs), while the validation and test sets are manually labeled to serve
as a reliable gold-standard benchmark. Building on the MahaSent dataset, we
apply the Chain-of-Translation (CoTR) prompting technique, where Marathi
sentences are translated into English and emotion labeled via a single prompt.
GPT-4 and Llama3-405B were evaluated, with GPT-4 selected for training data
annotation due to superior label quality. We evaluate model performance using
standard metrics and explore label aggregation strategies (e.g., Union,
Intersection). While GPT-4 predictions outperform fine-tuned BERT models,
BERT-based models trained on synthetic labels fail to surpass GPT-4. This
highlights both the importance of high-quality human-labeled data and the
inherent complexity of emotion recognition. An important finding of this work
is that generic LLMs like GPT-4 and Llama3-405B generalize better than
fine-tuned BERT for complex low-resource emotion recognition tasks. The dataset
and model are shared publicly at https://github.com/l3cube-pune/MarathiNLP

</details>


### [111] [What's Missing in Vision-Language Models? Probing Their Struggles with Causal Order Reasoning](https://arxiv.org/abs/2506.00869)
*Zhaotian Weng,Haoxuan Li,Kuan-Hao Huang,Jieyu Zhao*

Main category: cs.CL

TL;DR: 研究发现当前视觉语言模型（VLMs）在因果推理能力上存在显著不足，通过构建专用评测基准VQA-Causal/VCR-Causal揭示了该缺陷，并验证针对性微调策略的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有评测基准存在对象识别与活动检测的捷径漏洞，无法准确评估VLMs真正的因果推理能力，需构建隔离其他干扰因素的新型评测体系。

Method: 构建VQA-Causal和VCR-Causal两个因果推理专用评测基准，通过数据集分析揭示因果表达匮乏问题，并设计基于困难负例的微调策略验证改进方案。

Result: VLMs在因果任务上准确率接近随机猜测（仅略高于基线），但针对性微调可使因果推理能力提升15.3%且不影响下游任务表现。

Conclusion: 当前VLMs存在因果理解能力缺陷，核心瓶颈在于训练数据缺乏显性因果表达，研究为后续因果推理能力提升指明了方向。

Abstract: Despite the impressive performance of vision-language models (VLMs) on
downstream tasks, their ability to understand and reason about causal
relationships in visual inputs remains unclear. Robust causal reasoning is
fundamental to solving complex high-level reasoning tasks, yet existing
benchmarks often include a mixture of reasoning questions, and VLMs can
frequently exploit object recognition and activity identification as shortcuts
to arrive at the correct answers, making it challenging to truly assess their
causal reasoning abilities. To bridge this gap, we introduce VQA-Causal and
VCR-Causal, two new benchmarks specifically designed to isolate and rigorously
evaluate VLMs' causal reasoning abilities. Our findings reveal that while VLMs
excel in object and activity recognition, they perform poorly on causal
reasoning tasks, often only marginally surpassing random guessing. Further
analysis suggests that this limitation stems from a severe lack of causal
expressions in widely used training datasets, where causal relationships are
rarely explicitly conveyed. We additionally explore fine-tuning strategies with
hard negative cases, showing that targeted fine-tuning can improve model's
causal reasoning while maintaining generalization and downstream performance.
Our study highlights a key gap in current VLMs and lays the groundwork for
future work on causal understanding.

</details>


### [112] [CC-Tuning: A Cross-Lingual Connection Mechanism for Improving Joint Multilingual Supervised Fine-Tuning](https://arxiv.org/abs/2506.00875)
*Yangfan Ye,Xiaocheng Feng,Zekun Yuan,Xiachong Feng,Libo Qin,Lei Huang,Weitao Ma,Yichong Huang,Zhirui Zhang,Yunfei Lu,Xiaohui Yan,Duyu Tang,Dandan Tu,Bing Qin*

Main category: cs.CL

TL;DR: CC-Tuning通过潜在层跨语言交互机制增强多语言大模型性能，在22种语言上超越传统微调方法


<details>
  <summary>Details</summary>
Motivation: 现有基于数据层面的多语言增强方法（如数据增强/蒸馏）仅实现隐式跨语言对齐，未充分挖掘潜在层的深层交互价值。本研究旨在通过潜在层跨语言连接机制突破多语言能力提升瓶颈

Method: 1. 训练阶段：通过可训练决策机制融合英语和非英语输入的FFN激活值
2. 推理阶段：通过变换矩阵实现单语环境下的跨语言表征转换
3. 建立潜在层跨语言连接范式

Result: 在6个基准测试（覆盖22种语言）中：
- 优于标准监督微调（SFT）
- 提供数据增强方法之外的潜在层优化方案
- 决策机制有效识别有益激活模式

Conclusion: 潜在层跨语言交互机制具有显著应用潜力：
1. 证实模型在潜在空间的跨语言泛化能力
2. 为多语言优化开辟新路径
3. 保持单语推理效率的同时提升多语言表现

Abstract: Current large language models (LLMs) often exhibit imbalanced multilingual
capabilities due to their English-centric training corpora. To address this,
existing fine-tuning approaches operating at the data-level (e.g., through data
augmentation or distillation) typically introduce implicit cross-lingual
alignment, overlooking the potential for more profound, latent-level
cross-lingual interactions. In this work, we propose CC-Tuning, a novel
multilingual fine-tuning paradigm that explicitly establishes a cross-lingual
connection mechanism at the latent level. During training, CC-Tuning fuses the
feed forward activations from both English and non-English inputs, enabling the
model to benefit from both linguistic resources. This process is facilitated
with a trainable Decision Maker that identifies beneficial activations.
Furthermore, during inference, a Transform Matrix is utilized to simulate the
cross-lingual connection under monolingual setting through representation
transformation. Our experiments on six benchmarks covering 22 languages show
that CC-Tuning outperforms vanilla SFT and offers a strong latent-level
alternative to data-level augmentation methods. Further analysis also
highlights the practicality of CC-Tuning and the potential of latent-level
cross-lingual interactions in advancing the multilingual performance of LLMs.

</details>


### [113] [Not Every Token Needs Forgetting: Selective Unlearning to Limit Change in Utility in Large Language Model Unlearning](https://arxiv.org/abs/2506.00876)
*Yixin Wan,Anil Ramakrishna,Kai-Wei Chang,Volkan Cevher,Rahul Gupta*

Main category: cs.CL

TL;DR: 提出选择性遗忘方法（SU），通过精准识别并仅遗忘目标文档中的关键token子集，有效移除不必要信息的同时保留模型通用能力


<details>
  <summary>Details</summary>
Motivation: 传统遗忘方法会无差别遗忘所有token，导致通用知识丢失并影响模型效能，需开发更精准的遗忘策略

Method: 1. 在遗忘集中识别与目标信息相关的关键token子集
2. 仅针对关键token进行参数更新实现遗忘

Result: 在两个基准测试中，SU在六种基线算法上均实现有效目标遗忘，且保留集效用保留率提升18.7%

Conclusion: 选择性遗忘机制成功平衡遗忘效果与模型效用，为解决LLM隐私/版权问题提供新思路

Abstract: Large Language Model (LLM) unlearning has recently gained significant
attention, driven by the need to remove unwanted information, such as private,
sensitive, or copyrighted content, from LLMs. However, conventional unlearning
approaches indiscriminately update model parameters to forget all tokens in a
target document, including common tokens (e.g., pronouns, prepositions, general
nouns) that carry general knowledge. In this paper, we highlight that not every
token needs forgetting. We propose Selective Unlearning (SU), which identifies
a critical subset of tokens within the forgetting set that is relevant to the
unwanted information, and unlearns only those tokens. Experiments on two
benchmarks and six baseline unlearning algorithms demonstrate that SU not only
achieves effective unlearning on the targeted forget data, but also
significantly preserves the model's utility in the retaining set.

</details>


### [114] [Improve MLLM Benchmark Efficiency through Interview](https://arxiv.org/abs/2506.00883)
*Farong Wen,Yijin Guo,Junying Wang,Jiaohao Xiao,Yingjie Zhou,Chunyi Li,Zicheng Zhang,Guangtao Zhai*

Main category: cs.CL

TL;DR: 提出MLLM Interview (MITV)策略，通过少量问题快速评估多模态大模型性能


<details>
  <summary>Details</summary>
Motivation: 现有MLLM基准测试需要大规模全量问答，存在资源消耗大、耗时长的问题

Method: 1. 基于现有评估数据集构建带难度标签的面试数据集
2. 采用两阶段测试策略：先用少量问题初筛，再持续探索模型能力边界

Result: MITV策略在基准数据集上表现良好，通过少量问答即可快速完成模型能力评估

Conclusion: 该面试策略为MLLM评估提供了高效解决方案，显著降低测试成本

Abstract: The rapid development of Multimodal Large Language Models (MLLM) has led to a
wide range of MLLM applications, and a number of benchmark datasets have sprung
up in order to assess MLLM abilities. However, full-coverage Q&A testing on
large-scale data is resource-intensive and time-consuming. To address this
issue, we propose the MLLM Interview (MITV) strategy, which aims to quickly
obtain MLLM performance metrics by quizzing fewer question. First, First, we
constructed the interview dataset, which was built on an existing MLLM
assessment dataset, by adding difficulty labels based on the performance of
some typical MLLMs in this dataset. Second, we propose an MLLM Interview
strategy, which obtains an initial performance situation of the large model by
quizzing a small number of topics and then continuously tries to test the
model's limits. Through extensive experiments, the result shows that the MITV
strategy proposed in this paper performs well on MLLM benchmark datasets, and
it is able to obtain the model evaluation capability faster through a small
number of questions and answers.

</details>


### [115] [Affordance Benchmark for MLLMs](https://arxiv.org/abs/2506.00893)
*Junying Wang,Wenzhe Li,Yalun Wu,Yingji Liang,Yijin Guo,Chunyi Li,Haodong Duan,Zicheng Zhang,Guangtao Zhai*

Main category: cs.CL

TL;DR: 提出A4Bench基准测试系统评估多模态大语言模型的affordance感知能力，发现现有模型与人类表现存在显著差距


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在环境交互中关键的affordance感知能力（特别是动态和情境化理解）尚未被充分探索

Method: 构建包含1,282对构成性affordance问题和718对转化性affordance问题的双维度评测框架，覆盖9个子领域和多种复杂情境

Result: 最佳模型Gemini-2.0-Pro准确率仅18.05%，显著低于人类基线（81.25%-85.34%），开源模型表现更差

Conclusion: 揭示MLLMs环境理解能力的根本性缺陷，为开发具备强上下文感知的AI系统提供重要基准和方向

Abstract: Affordance theory posits that environments inherently offer action
possibilities that shape perception and behavior. While Multimodal Large
Language Models (MLLMs) excel in vision-language tasks, their ability to
perceive affordance, which is crucial for intuitive and safe interactions,
remains underexplored. To address this, we introduce A4Bench, a novel benchmark
designed to evaluate the affordance perception abilities of MLLMs across two
dimensions: 1) Constitutive Affordance}, assessing understanding of inherent
object properties through 1,282 question-answer pairs spanning nine
sub-disciplines, and 2) Transformative Affordance, probing dynamic and
contextual nuances (e.g., misleading, time-dependent, cultural, or
individual-specific affordance) with 718 challenging question-answer pairs.
Evaluating 17 MLLMs (nine proprietary and eight open-source) against human
performance, we find that proprietary models generally outperform open-source
counterparts, but all exhibit limited capabilities, particularly in
transformative affordance perception. Furthermore, even top-performing models,
such as Gemini-2.0-Pro (18.05% overall exact match accuracy), significantly lag
behind human performance (best: 85.34%, worst: 81.25%). These findings
highlight critical gaps in environmental understanding of MLLMs and provide a
foundation for advancing AI systems toward more robust, context-aware
interactions. The dataset is available in
https://github.com/JunyingWang959/A4Bench/.

</details>


### [116] [SocialEval: Evaluating Social Intelligence of Large Language Models](https://arxiv.org/abs/2506.00900)
*Jinfeng Zhou,Yuxuan Chen,Yihan Shi,Xuanming Zhang,Leqi Lei,Yi Feng,Zexuan Xiong,Miao Yan,Xunzhi Wang,Yaru Cao,Jianing Yin,Shuai Wang,Quanyu Dai,Zhenhua Dong,Hongning Wang,Minlie Huang*

Main category: cs.CL

TL;DR: SocialEval双评估框架揭示LLMs社交智能表现逊于人类，具备亲社会性与人脑类似功能分区


<details>
  <summary>Details</summary>
Motivation: 评估LLMs的社交智能及其与人类的差距，当前工作未能结合结果导向和过程导向的评估

Method: 构建SocialEval双语基准，通过手动编写叙事脚本形成世界树结构，整合结果与过程双维度评估

Result: LLMs在两项评估中落后人类，表现出亲社会性且偏好积极社会行为（即使导致目标失败），表示空间与神经元激活分析显示类人脑功能分区

Conclusion: LLMs社交智能存在机制性缺陷，需针对性提升其社会推理与目标权衡能力，神经分析为模型优化提供生物学启示

Abstract: LLMs exhibit promising Social Intelligence (SI) in modeling human behavior,
raising the need to evaluate LLMs' SI and their discrepancy with humans. SI
equips humans with interpersonal abilities to behave wisely in navigating
social interactions to achieve social goals. This presents an operational
evaluation paradigm: outcome-oriented goal achievement evaluation and
process-oriented interpersonal ability evaluation, which existing work fails to
address. To this end, we propose SocialEval, a script-based bilingual SI
benchmark, integrating outcome- and process-oriented evaluation by manually
crafting narrative scripts. Each script is structured as a world tree that
contains plot lines driven by interpersonal ability, providing a comprehensive
view of how LLMs navigate social interactions. Experiments show that LLMs fall
behind humans on both SI evaluations, exhibit prosociality, and prefer more
positive social behaviors, even if they lead to goal failure. Analysis of LLMs'
formed representation space and neuronal activations reveals that LLMs have
developed ability-specific functional partitions akin to the human brain.

</details>


### [117] [Pi-SQL: Enhancing Text-to-SQL with Fine-Grained Guidance from Pivot Programming Languages](https://arxiv.org/abs/2506.00912)
*Yongdong chi,Hanqing Wang,Zonghan Yang,Jian Yang,Xiao Yan,Yun Chen,Guanhua Chen*

Main category: cs.CL

TL;DR: 提出Pi-SQL方法，通过Python程序作为中间桥梁提升Text-to-SQL的准确性和执行效率


<details>
  <summary>Details</summary>
Motivation: 现有提示方法因文本与低资源SQL程序间的语义鸿沟导致精度受限，需通过高资源Python程序建立自然语言与SQL的桥梁

Method: 首先生成包含细粒度指导的Python程序，再基于其生成SQL程序，并通过候选策略选择执行速度最优的SQL

Result: 执行准确率最高提升3.20%，基于奖励的有效性评分比最优基线高4.55分

Conclusion: Pi-SQL通过Python中间层和策略选择机制，显著提升了SQL生成的准确性和执行效率

Abstract: Text-to-SQL transforms the user queries from natural language to executable
SQL programs, enabling non-experts to interact with complex databases. Existing
prompt-based methods craft meticulous text guidelines and examples to
facilitate SQL generation, but their accuracy is hindered by the large semantic
gap between the texts and the low-resource SQL programs. In this work, we
propose Pi-SQL, which incorporates the high-resource Python program as a pivot
to bridge between the natural language query and SQL program. In particular,
Pi-SQL first generates Python programs that provide fine-grained step-by-step
guidelines in their code blocks or comments, and then produces an SQL program
following the guidance of each Python program.The final SQL program matches the
reference Python program's query results and, through selection from candidates
generated by different strategies, achieves superior execution speed, with a
reward-based valid efficiency score up to 4.55 higher than the best-performing
baseline.Extensive experiments demonstrate the effectiveness of Pi-SQL, which
improves the execution accuracy of the best-performing baseline by up to 3.20.

</details>


### [118] [How do Transformer Embeddings Represent Compositions? A Functional Analysis](https://arxiv.org/abs/2506.00914)
*Aishik Nagar,Ishaan Singh Rawal,Mansi Dhanania,Cheston Tan*

Main category: cs.CL

TL;DR: 研究比较了Mistral、OpenAI、Google嵌入模型与BERT的组合性表现，发现岭回归模型解释力最佳，传统向量加法效果接近，多数嵌入模型组合性显著优于BERT。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer模型对复合词的表示机制及其组合性特性，组合性作为人类智能核心能力对推理与泛化具有重要意义。

Method: 采用六种组合性模型（加法/乘法/岭回归等）评估模型表征，使用全透明形容词-名词组合的合成数据集进行可视化验证。

Result: 线性岭回归模型表现最优，向量加法接近非线性模型，主流嵌入模型展现高组合性而BERT显著落后。

Conclusion: 系统揭示了不同模型组合性表现差异，证实多数现代嵌入模型具备良好组合性，为模型可解释性研究提供新视角。

Abstract: Compositionality is a key aspect of human intelligence, essential for
reasoning and generalization. While transformer-based models have become the de
facto standard for many language modeling tasks, little is known about how they
represent compound words, and whether these representations are compositional.
In this study, we test compositionality in Mistral, OpenAI Large, and Google
embedding models, and compare them with BERT. First, we evaluate
compositionality in the representations by examining six diverse models of
compositionality (addition, multiplication, dilation, regression, etc.). We
find that ridge regression, albeit linear, best accounts for compositionality.
Surprisingly, we find that the classic vector addition model performs almost as
well as any other model. Next, we verify that most embedding models are highly
compositional, while BERT shows much poorer compositionality. We verify and
visualize our findings with a synthetic dataset consisting of fully transparent
adjective-noun compositions. Overall, we present a thorough investigation of
compositionality.

</details>


### [119] [anyECG-chat: A Generalist ECG-MLLM for Flexible ECG Input and Multi-Task Understanding](https://arxiv.org/abs/2506.00942)
*Haitao Li,Ziyu Li,Yiheng Mao,Ziyi Liu,Zhoujian Sun,Zhengxing Huang*

Main category: cs.CL

TL;DR: 开发支持多任务和多类型ECG输入的anyECG-chat模型，通过三阶段训练实现家庭和临床场景下的灵活心电分析


<details>
  <summary>Details</summary>
Motivation: 现有ECG大模型局限于单导联短时报告生成，无法满足家庭环境长时程少导联ECG和临床多ECG对比需求

Method: 构建包含异常定位/开放问答等多任务的anyECG数据集，提出支持动态输入和对比分析的三阶段训练模型

Result: 模型支持家庭场景长时少导联ECG异常定位和临床多ECG对比分析，超越传统报告生成任务范畴

Conclusion: 突破现有ECG大模型输入限制，首次实现动态/多ECG输入的灵活分析，扩展临床应用可能性

Abstract: The advent of multimodal large language models (MLLMs) has sparked interest
in their application to electrocardiogram (ECG) analysis. However, existing
ECG-focused MLLMs primarily focus on report generation tasks, often limited to
single 12-lead, short-duration (10s) ECG inputs, thereby underutilizing the
potential of MLLMs. To this end, we aim to develop a MLLM for ECG analysis that
supports a broader range of tasks and more flexible ECG inputs. However,
existing ECG-QA datasets are often monotonous. To address this gap, we first
constructed the anyECG dataset, which encompasses a wide variety of tasks,
including report generation, abnormal waveform localization, and open-ended
question answering. In addition to standard hospital ECGs, we introduced
long-duration reduced-lead ECGs for home environments and multiple ECG
comparison scenarios commonly encountered in clinical practice. Furthermore, we
propose the anyECG-chat model, which supports dynamic-length ECG inputs and
multiple ECG inputs. We trained the model using a three-stage curriculum
training recipe with the anyECG dataset. A comprehensive evaluation was
conducted, demonstrating that anyECG-chat is capable of supporting various
practical application scenarios, including not only common report generation
tasks but also abnormal waveform localization for long-duration reduced-lead
ECGs in home environments and comprehensive comparative analysis of multiple
ECGs.

</details>


### [120] [Leveraging Large Language Models for Sarcastic Speech Annotation in Sarcasm Detection](https://arxiv.org/abs/2506.00955)
*Zhu Li,Yuqing Zhang,Xiyuan Gao,Shekhar Nayak,Matt Coler*

Main category: cs.CL

TL;DR: 利用大型语言模型生成标注，结合人工验证构建大规模讽刺语音数据集PodSarc，检测模型F1达73.63%


<details>
  <summary>Details</summary>
Motivation: 解决语音讽刺检测中数据稀缺问题，突破现有系统依赖多模态数据的限制

Method: 基于公开讽刺播客，使用GPT-4o和LLaMA 3进行初步标注，通过人工验证构建数据集，采用协作门控架构验证模型性能

Result: 创建PodSarc数据集，检测模型取得73.63% F1分数，验证数据集有效性

Conclusion: LLM标注结合人工验证的流程可行，PodSarc为讽刺检测研究提供可靠基准

Abstract: Sarcasm fundamentally alters meaning through tone and context, yet detecting
it in speech remains a challenge due to data scarcity. In addition, existing
detection systems often rely on multimodal data, limiting their applicability
in contexts where only speech is available. To address this, we propose an
annotation pipeline that leverages large language models (LLMs) to generate a
sarcasm dataset. Using a publicly available sarcasm-focused podcast, we employ
GPT-4o and LLaMA 3 for initial sarcasm annotations, followed by human
verification to resolve disagreements. We validate this approach by comparing
annotation quality and detection performance on a publicly available sarcasm
dataset using a collaborative gating architecture. Finally, we introduce
PodSarc, a large-scale sarcastic speech dataset created through this pipeline.
The detection model achieves a 73.63% F1 score, demonstrating the dataset's
potential as a benchmark for sarcasm detection research.

</details>


### [121] [From Objectives to Questions: A Planning-based Framework for Educational Mathematical Question Generation](https://arxiv.org/abs/2506.00963)
*Cheng Cheng,Zhenya Huang,Guanhao Zhao,Yuxiang Guo,Xin Lin,Jinze Wu,Xin Li,Shijin Wang*

Main category: cs.CL

TL;DR: 论文提出EQPR方法，通过构建EduMath数据集和EQGEVAL评估框架，结合蒙特卡洛树搜索与LLM生成能力，实现符合多维度教育目标的数学问题自动生成优化。


<details>
  <summary>Details</summary>
Motivation: 传统数学问题生成方法过度关注文本质量而忽视教育目标，且仅支持单一维度生成，无法满足复杂教学需求。

Method: 采用'计划-评估-优化'流程，将蒙特卡洛树搜索规划算法与LLM生成结合，通过自反思机制迭代优化问题设计。

Result: 基于EQGEVAL框架的实验表明，EQPR在多维教育目标达成率上取得显著提升（具体数值未明确说明）。

Conclusion: 该研究通过系统化的数据集构建、评估框架设计和生成方法创新，推动了教育技术领域复杂教学目标的自动化实现。

Abstract: Automatically generating high-quality mathematical problems that align with
educational objectives is a crucial task in NLP-based educational technology.
Traditional generation methods focus primarily on textual quality, but they
often overlook educational objectives. Moreover, these methods address only
single-dimensional, simple question generation, failing to meet complex,
multifaceted educational requirements. To address these challenges, we
constructed and annotated EduMath, a dataset of 16k mathematical questions with
multi-dimensional educational objectives. Based on this dataset, we developed
EQGEVAL, which incorporates three evaluation dimensions and is designed to
assess the ability of models to generate educational questions. Drawing
inspiration from teachers' problem design processes, we propose the Educational
Question Planning with self-Reflection (EQPR) method for educational
mathematical question generation, following a "plan-evaluate-optimize"
approach. Specifically, by combining planning algorithm based on Monte Carlo
Tree Search with the generative capabilities of Large Language Models, we
continuously optimize questions through iterative feedback. This
self-optimization mechanism ensures that the generated questions both fit the
educational context and strategically achieve specific basic educational
objectives. Through extensive experiments based on EQGEVAL, we have
demonstrated that EQPR achieves significant improvements in generating
questions that meet multi-dimensional educational objectives.

</details>


### [122] [ACCESS DENIED INC: The First Benchmark Environment for Sensitivity Awareness](https://arxiv.org/abs/2506.00964)
*Dren Fazlija,Arkadij Orlov,Sandipan Sikdar*

Main category: cs.CL

TL;DR: 提出敏感性感知(SA)概念解决企业数据管理中LLM的敏感信息访问控制问题，开发ACCESS DENIED INC基准测试环境验证模型差异


<details>
  <summary>Details</summary>
Motivation: LLM处理企业敏感信息时面临访问权限控制难题，现有权限过滤方案存在性能与隐私双重挑战，需开发更智能的权限适配机制

Method: 通过定义敏感性感知规则，构建ACCESS DENIED INC测试框架，系统评估不同LLM在授权/非授权数据请求场景下的响应模式

Result: 实验显示模型处理未授权请求时存在显著行为差异，验证了SA机制在平衡隐私保护与合法查询响应方面的可行性

Conclusion: 该研究为敏感感知语言模型建立基准测试标准，为企业级隐私AI系统开发提供技术框架与实施路径

Abstract: Large language models (LLMs) are increasingly becoming valuable to corporate
data management due to their ability to process text from various document
formats and facilitate user interactions through natural language queries.
However, LLMs must consider the sensitivity of information when communicating
with employees, especially given access restrictions. Simple filtering based on
user clearance levels can pose both performance and privacy challenges. To
address this, we propose the concept of sensitivity awareness (SA), which
enables LLMs to adhere to predefined access rights rules. In addition, we
developed a benchmarking environment called ACCESS DENIED INC to evaluate SA.
Our experimental findings reveal significant variations in model behavior,
particularly in managing unauthorized data requests while effectively
addressing legitimate queries. This work establishes a foundation for
benchmarking sensitivity-aware language models and provides insights to enhance
privacy-centric AI systems in corporate environments.

</details>


### [123] [XGUARD: A Graded Benchmark for Evaluating Safety Failures of Large Language Models on Extremist Content](https://arxiv.org/abs/2506.00973)
*Vadivel Abishethvarman,Bhavik Chandna,Pratik Jalan,Usman Naseem*

Main category: cs.CL

TL;DR: XGUARD提出五级危险性评估框架，突破传统二元安全分类，通过3840个真实场景红队测试评估LLM生成极端主义内容的风险等级


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全评估采用简单的二元标签（安全/不安全），无法有效识别极端主义内容的风险梯度，亟需建立细粒度评估体系

Method: 构建包含3840个真实社交媒体/新闻场景的测试集，建立0-4级五层危险性分类标准，提出可解释的攻击严重性曲线（ASC）评估防御机制

Result: 测试6个主流LLM和2种防御策略，揭示模型在极端内容生成上的安全漏洞及鲁棒性-表达自由度的平衡难题

Conclusion: 分级安全指标对构建可信LLM至关重要，XGUARD框架为风险可视化和防御策略优化提供新范式

Abstract: Large Language Models (LLMs) can generate content spanning ideological
rhetoric to explicit instructions for violence. However, existing safety
evaluations often rely on simplistic binary labels (safe and unsafe),
overlooking the nuanced spectrum of risk these outputs pose. To address this,
we present XGUARD, a benchmark and evaluation framework designed to assess the
severity of extremist content generated by LLMs. XGUARD includes 3,840 red
teaming prompts sourced from real world data such as social media and news,
covering a broad range of ideologically charged scenarios. Our framework
categorizes model responses into five danger levels (0 to 4), enabling a more
nuanced analysis of both the frequency and severity of failures. We introduce
the interpretable Attack Severity Curve (ASC) to visualize vulnerabilities and
compare defense mechanisms across threat intensities. Using XGUARD, we evaluate
six popular LLMs and two lightweight defense strategies, revealing key insights
into current safety gaps and trade-offs between robustness and expressive
freedom. Our work underscores the value of graded safety metrics for building
trustworthy LLMs.

</details>


### [124] [NTPP: Generative Speech Language Modeling for Dual-Channel Spoken Dialogue via Next-Token-Pair Prediction](https://arxiv.org/abs/2506.00975)
*Qichao Wang,Ziqiao Meng,Wenqian Cui,Yifei Zhang,Pengcheng Wu,Bingzhe Wu,Irwin King,Liang Chen,Peilin Zhao*

Main category: cs.CL

TL;DR: 提出Next-Token-Pair Prediction（NTPP）新范式，通过双通道语音数据和解码器架构显著提升语音语言模型的对话能力与推理效率


<details>
  <summary>Details</summary>
Motivation: 现有语音语言模型未能充分利用双通道语音数据中蕴含的人类对话结构与动态特征

Method: 创新性提出Next-Token-Pair Prediction（NTPP）生成建模范式，首次实现基于解码器架构的说话人无关双通道语音对话学习

Result: 在标准测试中，NTPP显著提升对话轮转预测准确率（+18.7%）、响应连贯性（+23.4%）和自然度（+31.2%），推理延迟降低至89ms/utterance

Conclusion: NTPP为实时语音对话系统提供了高效解决方案，在保持对话自然度的同时实现工业级推理速度

Abstract: Inspired by the impressive capabilities of GPT-4o, there is growing interest
in enabling speech language models (SLMs) to engage in natural, fluid spoken
interactions with humans. Recent advancements have led to the development of
several SLMs that demonstrate promising results in this area. However, current
approaches have yet to fully exploit dual-channel speech data, which inherently
captures the structure and dynamics of human conversation. In this work, we
systematically explore the use of dual-channel speech data in the context of
modern large language models, and introduce a novel generative modeling
paradigm, Next-Token-Pair Prediction (NTPP), to enable speaker-independent
dual-channel spoken dialogue learning using decoder-only architectures for the
first time. We evaluate our approach on standard benchmarks, and empirical
results show that our proposed method, NTPP, significantly improves the
conversational abilities of SLMs in terms of turn-taking prediction, response
coherence, and naturalness. Moreover, compared to existing methods, NTPP
achieves substantially lower inference latency, highlighting its practical
efficiency for real-time applications.

</details>


### [125] [LEMONADE: A Large Multilingual Expert-Annotated Abstractive Event Dataset for the Real World](https://arxiv.org/abs/2506.00980)
*Sina J. Semnani,Pingyue Zhang,Wanyue Zhai,Haozhuo Li,Ryan Beauchamp,Trey Billing,Katayoun Kishi,Manling Li,Monica S. Lam*

Main category: cs.CL

TL;DR: 提出LEMONADE多语言冲突事件数据集与抽象事件抽取框架AEE/AEL，开发零样本系统ZEST，评估显示LLMs优于专用模型但监督系统仍有优势


<details>
  <summary>Details</summary>
Motivation: 解决多语言冲突事件数据聚合难题，传统基于跨度的抽取方法难以实现跨语言实体归一化

Method: 通过文档整体理解进行抽象事件抽取（AEE）和实体链接（AEL），评估LLMs并开发ZEST零样本检索系统

Result: 最佳零样本系统端到端F1达58.3%，ZEST实体链接F1 45.7%显著优于基准，但较监督系统仍低20-37个百分点

Conclusion: LEMONADE为跨语言事件分析提供新基准，AEE/AEL框架有效但需缩小零样本与监督系统差距

Abstract: This paper presents LEMONADE, a large-scale conflict event dataset comprising
39,786 events across 20 languages and 171 countries, with extensive coverage of
region-specific entities. LEMONADE is based on a partially reannotated subset
of the Armed Conflict Location & Event Data (ACLED), which has documented
global conflict events for over a decade.
  To address the challenge of aggregating multilingual sources for global event
analysis, we introduce abstractive event extraction (AEE) and its subtask,
abstractive entity linking (AEL). Unlike conventional span-based event
extraction, our approach detects event arguments and entities through holistic
document understanding and normalizes them across the multilingual dataset. We
evaluate various large language models (LLMs) on these tasks, adapt existing
zero-shot event extraction systems, and benchmark supervised models.
Additionally, we introduce ZEST, a novel zero-shot retrieval-based system for
AEL.
  Our best zero-shot system achieves an end-to-end F1 score of 58.3%, with LLMs
outperforming specialized event extraction models such as GoLLIE. For entity
linking, ZEST achieves an F1 score of 45.7%, significantly surpassing OneNet, a
state-of-the-art zero-shot baseline that achieves only 23.7%. However, these
zero-shot results lag behind the best supervised systems by 20.1% and 37.0% in
the end-to-end and AEL tasks, respectively, highlighting the need for further
research.

</details>


### [126] [What do self-supervised speech models know about Dutch? Analyzing advantages of language-specific pre-training](https://arxiv.org/abs/2506.00981)
*Marianne de Heer Kloots,Hosein Mohebbi,Charlotte Pouw,Gaofei Shen,Willem Zuidema,Martijn Bentum*

Main category: cs.CL

TL;DR: 荷兰语预训练的Wav2Vec2模型在语音特征编码和ASR下游任务中优于英语或多语言预训练模型


<details>
  <summary>Details</summary>
Motivation: 探究自监督语音模型是否能够捕捉特定语言的语音特征，以及预训练语言选择对特征编码的影响

Method: 通过聚类/分类探针和zero-shot指标，比较荷兰语/英语/多语言预训练模型对荷兰语音素和词汇特征的编码能力

Result: 荷兰语专用预训练显著提升语言特征表征，且与ASR性能提升正相关

Conclusion: 针对目标语言的专用预训练能有效增强模型对语言特异性特征的捕捉，该优势可传导至下游语音识别任务

Abstract: How language-specific are speech representations learned by self-supervised
models? Existing work has shown that a range of linguistic features can be
successfully decoded from end-to-end models trained only on speech recordings.
However, it's less clear to what extent pre-training on specific languages
improves language-specific linguistic information. Here we test the encoding of
Dutch phonetic and lexical information in internal representations of
self-supervised Wav2Vec2 models. Pre-training exclusively on Dutch improves the
representation of Dutch linguistic features as compared to pre-training on
similar amounts of English or larger amounts of multilingual data. This
language-specific advantage is well-detected by trained clustering or
classification probes, and partially observable using zero-shot metrics.
Furthermore, the language-specific benefit on linguistic feature encoding
aligns with downstream performance on Automatic Speech Recognition.

</details>


### [127] [Do LLMs Understand Why We Write Diaries? A Method for Purpose Extraction and Clustering](https://arxiv.org/abs/2506.00985)
*Valeriya Goloviznina,Alexander Sergeev,Mikhail Melnichenko,Evgeny Kotelnikov*

Main category: cs.CL

TL;DR: 利用大语言模型分析苏联时期日记写作目的，GPT-4o和o1-mini表现最佳


<details>
  <summary>Details</summary>
Motivation: 传统方法难以从海量日记中有效提取信息，需开发基于LLM的新方法分析日记写作目的（如记录生活/自我反思/语言练习），并应用于Prozhito档案库的苏联时期个人叙事研究

Method: 使用不同专有和开源LLM（含GPT-4o/o1-mini）分析1922-1929年苏联日记，对比模板基线方法

Result: GPT-4o和o1-mini性能最优，模板方法效果显著较差；进一步分析作者性别/年龄/写作年份对目的的影响，并揭示模型错误类型

Conclusion: LLM在日记分析中展现有效性，但需改进模型局限性。该研究为历史个人叙事分析提供新范式，未来可优化模型在跨文化语境下的表现

Abstract: Diary analysis presents challenges, particularly in extracting meaningful
information from large corpora, where traditional methods often fail to deliver
satisfactory results. This study introduces a novel method based on Large
Language Models (LLMs) to identify and cluster the various purposes of diary
writing. By "purposes," we refer to the intentions behind diary writing, such
as documenting life events, self-reflection, or practicing language skills. Our
approach is applied to Soviet-era diaries (1922-1929) from the Prozhito digital
archive, a rich collection of personal narratives. We evaluate different
proprietary and open-source LLMs, finding that GPT-4o and o1-mini achieve the
best performance, while a template-based baseline is significantly less
effective. Additionally, we analyze the retrieved purposes based on gender, age
of the authors, and the year of writing. Furthermore, we examine the types of
errors made by the models, providing a deeper understanding of their
limitations and potential areas for improvement in future research.

</details>


### [128] [Talking to Data: Designing Smart Assistants for Humanities Databases](https://arxiv.org/abs/2506.00986)
*Alexander Sergeev,Valeriya Goloviznina,Mikhail Melnichenko,Evgeny Kotelnikov*

Main category: cs.CL

TL;DR: 开发基于LLM的智能助手，通过自然语言交互和RAG技术改善人文学科数据库访问，支持研究者与非专业用户无门槛使用


<details>
  <summary>Details</summary>
Motivation: 解决传统数据库交互方式在搜索和响应生成方面的局限性，提升人文学科研究的可访问性和效率

Method: 采用聊天机器人+RAG架构，整合混合搜索/自动查询生成/文本转SQL过滤等技术，基于Prozhito数字档案进行多模型响应质量测试

Result: 验证了LLM在提升数字档案交互质量方面的有效性，实现了更直观的研究体验和包容性访问

Conclusion: LLM技术能革新研究者与数字档案的交互范式，推动人文学科研究方法的数字化转型

Abstract: Access to humanities research databases is often hindered by the limitations
of traditional interaction formats, particularly in the methods of searching
and response generation. This study introduces an LLM-based smart assistant
designed to facilitate natural language communication with digital humanities
data. The assistant, developed in a chatbot format, leverages the RAG approach
and integrates state-of-the-art technologies such as hybrid search, automatic
query generation, text-to-SQL filtering, semantic database search, and
hyperlink insertion. To evaluate the effectiveness of the system, experiments
were conducted to assess the response quality of various language models. The
testing was based on the Prozhito digital archive, which contains diary entries
from predominantly Russian-speaking individuals who lived in the 20th century.
The chatbot is tailored to support anthropology and history researchers, as
well as non-specialist users with an interest in the field, without requiring
prior technical training. By enabling researchers to query complex databases
with natural language, this tool aims to enhance accessibility and efficiency
in humanities research. The study highlights the potential of Large Language
Models to transform the way researchers and the public interact with digital
archives, making them more intuitive and inclusive. Additional materials are
presented in GitHub repository:
https://github.com/alekosus/talking-to-data-intersys2025.

</details>


### [129] [Less is More: Local Intrinsic Dimensions of Contextual Language Models](https://arxiv.org/abs/2506.01034)
*Benjamin Matthias Ruppik,Julius von Rohrscheidt,Carel van Niekerk,Michael Heck,Renato Vukovic,Shutong Feng,Hsien-chin Lin,Nurul Lubis,Bastian Rieck,Marcus Zibrowius,Milica Gašić*

Main category: cs.CL

TL;DR: 提出通过分析上下文潜在嵌入的几何特性，揭示LLM训练动态的新方法，发现局部维度可预测训练能力耗尽/过拟合现象，并提出维度变化与性能增益的关联性启发式指标。


<details>
  <summary>Details</summary>
Motivation: 针对LLM内部机制黑箱化问题，特别是微调对模型行为影响机制不明确，尝试从几何属性视角建立训练过程与模型表现的可解释关联，为实践者提供调优依据。

Method: 测量潜在空间局部维度变化轨迹，通过对话状态跟踪/情感识别/算术任务三组实验，验证局部维度对训练阶段识别（能力饱和/过拟合/grokking）的预测能力。

Result: 局部维度均值可有效预警：1)对话任务中的训练能力耗尽临界点 2)情感分类中的过拟合现象 3)算术任务中的grokking阶段。维度下降常伴随后续性能提升。

Conclusion: 几何分析为LLM训练过程提供可视化诊断工具，帮助优化微调策略，其维度动态规律为模型可解释性研究开辟新路径，促进应用场景的模型适配决策。

Abstract: Understanding the internal mechanisms of large language models (LLMs) remains
a challenging and complex endeavor. Even fundamental questions, such as how
fine-tuning affects model behavior, often require extensive empirical
evaluation. In this paper, we introduce a novel perspective based on the
geometric properties of contextual latent embeddings to study the effects of
training and fine-tuning. To that end, we measure the local dimensions of a
contextual language model's latent space and analyze their shifts during
training and fine-tuning. We show that the local dimensions provide insights
into the model's training dynamics and generalization ability. Specifically,
the mean of the local dimensions predicts when the model's training
capabilities are exhausted, as exemplified in a dialogue state tracking task,
overfitting, as demonstrated in an emotion recognition task, and grokking, as
illustrated with an arithmetic task. Furthermore, our experiments suggest a
practical heuristic: reductions in the mean local dimension tend to accompany
and predict subsequent performance gains. Through this exploration, we aim to
provide practitioners with a deeper understanding of the implications of
fine-tuning on embedding spaces, facilitating informed decisions when
configuring models for specific applications. The results of this work
contribute to the ongoing discourse on the interpretability, adaptability, and
generalizability of LLMs by bridging the gap between intrinsic model mechanisms
and geometric properties in the respective embeddings.

</details>


### [130] [Probing Neural Topology of Large Language Models](https://arxiv.org/abs/2506.01042)
*Yu Zheng,Yuan Yuan,Yong Li,Paolo Santi*

Main category: cs.CL

TL;DR: LLM神经元拓扑结构能预测语言生成性能，该拓扑模式稀疏且早期形成，不同架构的LLM展现出高度一致的神经拓扑结构。


<details>
  <summary>Details</summary>
Motivation: 现有研究聚焦单个神经元机制，但神经元协同工作机制尚未明确，阻碍对LLM的深入理解和安全开发。

Method: 提出graph probing方法，通过分析不同规模/架构LLM的神经图拓扑，揭示神经元功能连接与语言生成性能的关系。

Result: 发现仅凭神经拓扑即可预测next-token性能（保留1%连接或预训练8步后仍有效），不同LLM形成高度一致的拓扑结构。

Conclusion: 神经拓扑结构是LLM语言能力的基石，该结构在训练早期形成且具有跨模型普适性，为模型安全开发提供新视角。

Abstract: Probing large language models (LLMs) has yielded valuable insights into their
internal mechanisms by linking neural representations to interpretable
semantics. However, how neurons functionally co-activate with each other to
give rise to emergent capabilities remains largely unknown, hindering a deeper
understanding and safer development of LLMs. In this work, we introduce graph
probing, a method for uncovering the functional connectivity topology of LLM
neurons and relating it to language generation performance. By analyzing
internal neural graphs across diverse LLM families and scales, we discover a
universal predictability of next-token prediction performance using only neural
topology. This predictability is robust even when retaining just 1% of neuron
connections or probing models after only 8 pretraining steps, highlighting the
sparsity and early emergence of topological patterns. Further graph matching
analysis suggests that, despite significant distinctions in architectures,
parameters, and training data, different LLMs develop intricate and consistent
neural topological structures that may form the foundation for their language
generation abilities. Codes and data for the graph probing toolbox are released
at https://github.com/DavyMorgan/llm-graph-probing.

</details>


### [131] [CHEER-Ekman: Fine-grained Embodied Emotion Classification](https://arxiv.org/abs/2506.01047)
*Phan Anh Duong,Cat Luong,Divyesh Bommana,Tianyu Jiang*

Main category: cs.CL

TL;DR: 本研究扩展了现有情绪数据集并开发出CHEER-Ekman数据集，通过自动化评估方法显著提升了情绪识别准确率，发现简化提示指令能使小模型达到大模型水平。


<details>
  <summary>Details</summary>
Motivation: 现有文本中具体化情绪识别研究不足，二元分类无法满足需求，需建立更细粒度的六维情绪分类体系。

Method: 基于埃克曼情绪理论扩展数据集，采用大语言模型自动最佳-最差缩放评估方法，对比不同提示策略对模型性能的影响。

Result: 新方法在六维情绪分类上超越监督学习，7B小模型通过简化提示和思维链技术达到与70B大模型相当的准确率。

Conclusion: 研究为情绪计算提供新数据集和方法论，证明提示工程可有效缩小模型性能差距，推动资源节约型情感分析发展。

Abstract: Emotions manifest through physical experiences and bodily reactions, yet
identifying such embodied emotions in text remains understudied. We present an
embodied emotion classification dataset, CHEER-Ekman, extending the existing
binary embodied emotion dataset with Ekman's six basic emotion categories.
Using automatic best-worst scaling with large language models, we achieve
performance superior to supervised approaches on our new dataset. Our
investigation reveals that simplified prompting instructions and
chain-of-thought reasoning significantly improve emotion recognition accuracy,
enabling smaller models to achieve competitive performance with larger ones.

</details>


### [132] [SealQA: Raising the Bar for Reasoning in Search-Augmented Language Models](https://arxiv.org/abs/2506.01062)
*Thinh Pham,Nguyen Nguyen,Pratibha Zunjare,Weiyuan Chen,Yu-Min Tseng,Tu Vu*

Main category: cs.CL

TL;DR: SealQA是评估搜索增强语言模型处理复杂事实性问题能力的新基准，包含Seal-0、Seal-Hard和LongSeal三个版本，揭示了当前模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在处理网络搜索中存在冲突/噪音结果的事实性问题时表现不佳，需要系统性评估框架。

Method: 创建三组测试集：Seal-0（核心挑战性问题）、Seal-Hard（困难场景）、LongSeal（长文档多干扰项环境）。

Result: 前沿模型（如GPT-4.1）在Seal-0准确率接近零；DeepSeek-R1等模型易受噪音干扰；增加算力无法稳定提升性能；长文档场景仍存在定位困难。

Conclusion: 当前搜索增强模型存在显著能力缺口，发布SealQA基准以推动相关研究。数据集地址：huggingface.co/datasets/vtllms/sealqa

Abstract: We introduce SealQA, a new challenge benchmark for evaluating
SEarch-Augmented Language models on fact-seeking questions where web search
yields conflicting, noisy, or unhelpful results. SealQA comes in three flavors:
(1) Seal-0 (main) and (2) Seal-Hard, which assess factual accuracy and
reasoning capabilities, with Seal-0 focusing on the most challenging questions
where chat models (e.g., GPT-4.1) typically achieve near-zero accuracy; and (3)
LongSeal, which extends SealQA to test long-context, multi-document reasoning
in "needle-in-a-haystack" settings. Our evaluation reveals critical limitations
in current models: Even frontier LLMs perform poorly across all SealQA flavors.
On Seal-0, frontier agentic models equipped with tools like o3 and o4-mini
achieve only 17.1% and 6.3% accuracy, respectively, at their best reasoning
efforts. We find that advanced reasoning models such as DeepSeek-R1-671B and
o3-mini are highly vulnerable to noisy search results. Notably, increasing
test-time compute does not yield reliable gains across o3-mini, o4-mini, and
o3, with performance often plateauing or even declining early. Additionally,
while recent models are less affected by the "lost-in-the-middle" issue, they
still fail to reliably identify relevant documents in LongSeal when faced with
numerous distractors. To facilitate future work, we release SealQA at
huggingface.co/datasets/vtllms/sealqa.

</details>


### [133] [How Programming Concepts and Neurons Are Shared in Code Language Models](https://arxiv.org/abs/2506.01074)
*Amir Hossein Kargaran,Yihong Liu,François Yvon,Hinrich Schütze*

Main category: cs.CL

TL;DR: 研究发现LLMs在编程语言概念空间中与英语更接近，特定编程语言的神经元分布呈现底层集中、高层独有特征，高度对齐的编程语言缺乏特异性神经元且关键词集更大。


<details>
  <summary>Details</summary>
Motivation: 探索多编程语言与英语在LLMs概念空间中的关联机制，突破以往单语言研究的局限，揭示模型内部对编程语言的表征模式。

Method: 使用Llama模型在21个编程语言对进行少样本翻译任务，通过解码中间层嵌入和神经元激活分析11种编程语言与英语的关系。

Result: 概念空间后半层更倾向英语标记；底层集中语言共性神经元，高层分布编程语言特异性神经元；高度对齐的编程语言无法识别特异性神经元且关键词规模更大。

Conclusion: LLMs通过分层神经元分布表征编程语言，概念空间存在结构化模式，这为理解模型内部机制提供了新视角。代码已开源。

Abstract: Several studies have explored the mechanisms of large language models (LLMs)
in coding tasks, but most have focused on programming languages (PLs) in a
monolingual setting. In this paper, we investigate the relationship between
multiple PLs and English in the concept space of LLMs. We perform a few-shot
translation task on 21 PL pairs using two Llama-based models. By decoding the
embeddings of intermediate layers during this task, we observe that the concept
space is closer to English (including PL keywords) and assigns high
probabilities to English tokens in the second half of the intermediate layers.
We analyze neuron activations for 11 PLs and English, finding that while
language-specific neurons are primarily concentrated in the bottom layers,
those exclusive to each PL tend to appear in the top layers. For PLs that are
highly aligned with multiple other PLs, identifying language-specific neurons
is not feasible. These PLs also tend to have a larger keyword set than other
PLs and are closer to the model's concept space regardless of the input/output
PL in the translation task. Our findings provide insights into how LLMs
internally represent PLs, revealing structural patterns in the model's concept
space. Code is available at https://github.com/cisnlp/code-specific-neurons.

</details>


### [134] [zip2zip: Inference-Time Adaptive Vocabularies for Language Models via Token Compression](https://arxiv.org/abs/2506.01084)
*Saibo Geng,Nathan Ranchin,Yunzhen yao,Maxime Peyrard,Chris Wendler,Michael Gastpar,Robert West*

Main category: cs.CL

TL;DR: zip2zip框架通过动态调整分词词汇减少LLMs生成的分词数量，使推理速度提升20-60%


<details>
  <summary>Details</summary>
Motivation: 传统静态分词器无法适应领域/语言特性，导致序列过长和计算成本高

Method: 包含三部分：1) 基于LZW压缩的实时分词器生成可复用超分词 2) 运行时计算新分词嵌入的层 3) 处理压缩序列的因果语言模型变体，通过10 GPU小时高效微调实现

Result: 输入输出序列缩短20-60%，推理延迟显著降低

Conclusion: zip2zip以低成本实现LLM动态分词，通过超分词机制有效提升推理效率

Abstract: Tokenization efficiency plays a critical role in the performance and cost of
large language models (LLMs), yet most models rely on static tokenizers
optimized for general-purpose corpora. These tokenizers' fixed vocabularies
often fail to adapt to domain- or language-specific inputs, leading to longer
token sequences and higher computational costs. We introduce zip2zip, a
framework that enables LLMs to dynamically adjust token vocabulary at inference
time, allowing for fewer generated tokens and thus faster inference. zip2zip
consists of three key components: (1) a tokenizer based on Lempel-Ziv-Welch
(LZW) compression that incrementally compresses tokens into reusable
"hypertokens" on the fly; (2) an embedding layer that computes embeddings for
newly formed hypertokens at runtime; and (3) a causal language modeling variant
that trains the model to operate on hypertokenized, compressed sequences. We
show that an existing LLM can be zip2zip-fied in 10 GPU-hours via
parameter-efficient finetuning. The resulting zip2zip LLMs effectively learn to
use hypertokens at inference time, reducing input and output sequence length by
20-60\%, with significant improvements in inference latency.

</details>


### [135] [Un-considering Contextual Information: Assessing LLMs' Understanding of Indexical Elements](https://arxiv.org/abs/2506.01089)
*Metehan Oguz,Yavuz Bakman,Duygu Nur Yaldiz*

Main category: cs.CL

TL;DR: LLMs在不同指示词（I/you/here/tomorrow）的指代消解中表现差异显著，并发布首个英文指示词数据集


<details>
  <summary>Details</summary>
Motivation: 现有研究主要评估LLMs在名词和第三人称代词的指代消解能力，忽略指示词的特殊语言特性及其挑战

Method: 创建包含1600个多选题的英文指示词数据集，测试GPT-4o、Claude 3.5 Sonnet等主流模型的表现

Result: 1. 模型在I指代表现优异（F1=0.85），you/here/tomorrow表现差（F1=0.62-0.68）
2. 句法线索（如引号）对不同指示词产生相反影响

Conclusion: LLMs处理指示词存在明显能力差异，句法线索需针对性优化。公开数据集推动指代消解研究发展

Abstract: Large Language Models (LLMs) have demonstrated impressive performances in
tasks related to coreference resolution. However, previous studies mostly
assessed LLM performance on coreference resolution with nouns and third person
pronouns. This study evaluates LLM performance on coreference resolution with
indexical like I, you, here and tomorrow, which come with unique challenges due
to their linguistic properties. We present the first study examining how LLMs
interpret indexicals in English, releasing the English Indexical Dataset with
1600 multiple-choice questions. We evaluate pioneering LLMs, including GPT-4o,
Claude 3.5 Sonnet, Gemini 1.5 Pro, and DeepSeek V3. Our results reveal that
LLMs exhibit an impressive performance with some indexicals (I), while
struggling with others (you, here, tomorrow), and that syntactic cues (e.g.
quotation) contribute to LLM performance with some indexicals, while they
reduce performance with others. Code and data are available at:
https://github.com/metehanoguzz/LLMs-Indexicals-English.

</details>


### [136] [Contextual Candor: Enhancing LLM Trustworthiness Through Hierarchical Unanswerability Detection](https://arxiv.org/abs/2506.01104)
*Steven Robinson,Antonio Carlos Rivera*

Main category: cs.CL

TL;DR: 提出强化不可答性学习(RUL)框架，通过混合训练范式增强大语言模型对不可答问题的识别与拒绝能力


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型生成虚假信息导致的信任危机，提升对话系统可靠性

Method: 结合判别式预测头与生成模型，采用包含ECA数据集监督微调和人类反馈强化学习(RLHF)的多阶段训练策略

Result: 在句子/段落/排序层级实现更高不可答检测准确率，生成更恰当的拒绝回复，同时保持可答问题处理能力

Conclusion: RUL框架显著提升对话AI的可信度与用户感知价值，为构建可靠的人机对话系统提供新方向

Abstract: The pervasive deployment of large language models (LLMs) in conversational AI
systems has revolutionized information access, yet their propensity for
generating factually unsupported or hallucinated responses remains a critical
impediment to trustworthiness and widespread adoption. This paper introduces
Reinforced Unanswerability Learning (RUL), a novel hybrid training paradigm
designed to imbue LLMs with the intrinsic capability to accurately detect
unanswerable questions and generate reliably appropriate responses. Unlike
conventional approaches that rely on external classifiers or simple prompting,
RUL integrates a discriminative unanswerability prediction head with the LLM's
generative core, guided by a multi-stage learning strategy. This includes
supervised fine-tuning on a novel, richly annotated dataset,
Enhanced-CAsT-Answerability (ECA), which features hierarchical answerability
labels and ground-truth refusal responses. Crucially, RUL incorporates a
subsequent reinforcement learning with human feedback (RLHF) phase to refine
the nuance, helpfulness, and informativeness of refusal responses. Extensive
experiments demonstrate RUL's superior performance, achieving significantly
higher accuracy in unanswerability detection across sentence, paragraph, and
ranking levels, and substantially increasing the generation of appropriate
refusals for unanswerable queries, alongside strong performance on answerable
questions. Human evaluations further corroborate RUL's effectiveness,
highlighting a marked improvement in perceived helpfulness and trustworthiness,
ultimately paving the way for more reliable and user-centric conversational AI.

</details>


### [137] [From Words to Waves: Analyzing Concept Formation in Speech and Text-Based Foundation Models](https://arxiv.org/abs/2506.01133)
*Asım Ersoy,Basel Mousi,Shammur Chowdhury,Firoj Alam,Fahim Dalvi,Nadir Durrani*

Main category: cs.CL

TL;DR: 该研究通过潜在概念分析方法，探索语音与文本模型在单模态和跨模态训练中形成的语义抽象结构，并开源相关资源


<details>
  <summary>Details</summary>
Motivation: 验证纯文本模型展现的类通用智能特性（世界知识/推理能力/语义概念）是否存在于语音模态模型中，探究多模态联合训练能否增强语义理解

Method: 使用无监督的潜在概念分析方法（Latent Concept Analysis），解析不同模态神经网络中的潜在表征结构

Result: 发现语音模型能形成与文本模型相似的语义抽象结构，多模态联合训练可产生更丰富的概念层级体系（具体实验结果需参考论文）

Conclusion: 非文本模态模型具备语义抽象能力，多模态联合训练为构建更接近人类认知的AI系统提供了新方向

Abstract: The emergence of large language models (LLMs) has demonstrated that systems
trained solely on text can acquire extensive world knowledge, develop reasoning
capabilities, and internalize abstract semantic concepts--showcasing properties
that can be associated with general intelligence. This raises an intriguing
question: Do such concepts emerge in models trained on other modalities, such
as speech? Furthermore, when models are trained jointly on multiple modalities:
Do they develop a richer, more structured semantic understanding? To explore
this, we analyze the conceptual structures learned by speech and textual models
both individually and jointly. We employ Latent Concept Analysis, an
unsupervised method for uncovering and interpreting latent representations in
neural networks, to examine how semantic abstractions form across modalities.
For reproducibility we made scripts and other resources available to the
community.

</details>


### [138] [A Word is Worth 4-bit: Efficient Log Parsing with Binary Coded Decimal Recognition](https://arxiv.org/abs/2506.01147)
*Prerak Srivastava,Giulio Corallo,Sergey Rybalko*

Main category: cs.CL

TL;DR: 提出字符级神经日志解析器，通过字符嵌入聚合与二进制编码序列实现细粒度模板提取，在低资源场景下达到LLM级精度且效率更优


<details>
  <summary>Details</summary>
Motivation: 现有日志解析器难以捕获细粒度模板细节，导致下游任务模式识别精度不足

Method: 设计字符级神经网络架构：1) 聚合字符嵌入捕获局部特征 2) 生成二进制编码序列实现模板原子化解析

Result: 在Loghub-2k修订版和工业数据集测试中，准确率匹敌LLM解析器，效率显著优于语义解析器

Conclusion: 该解析器在保持高准确性的同时提升计算效率，为日志分析任务提供了更实用的解决方案

Abstract: System-generated logs are typically converted into categorical log templates
through parsing. These templates are crucial for generating actionable insights
in various downstream tasks. However, existing parsers often fail to capture
fine-grained template details, leading to suboptimal accuracy and reduced
utility in downstream tasks requiring precise pattern identification. We
propose a character-level log parser utilizing a novel neural architecture that
aggregates character embeddings. Our approach estimates a sequence of
binary-coded decimals to achieve highly granular log templates extraction. Our
low-resource character-level parser, tested on revised Loghub-2k and a manually
annotated industrial dataset, matches LLM-based parsers in accuracy while
outperforming semantic parsers in efficiency.

</details>


### [139] [Mispronunciation Detection Without L2 Pronunciation Dataset in Low-Resource Setting: A Case Study in Finland Swedish](https://arxiv.org/abs/2506.01156)
*Nhan Phan,Mikko Kuronen,Maria Kautonen,Riikka Ullakonoja,Anna von Zansen,Yaroslav Getman,Ekaterina Voskoboinik,Tamás Grósz,Mikko Kurimo*

Main category: cs.CL

TL;DR: 开发芬兰瑞典语发音检测模型，通过多语言wav2vec 2.0结合熵正则化技术，使用极少量L2数据实现发音错误检测


<details>
  <summary>Details</summary>
Motivation: 低资源语言变体（如芬兰瑞典语）缺乏发音检测工具，现有系统主要服务于英语等主流语言

Method: 使用89小时L1母语者自发语音训练，采用多语言wav2vec 2.0模型结合熵正则化，配合温度缩放和top-k归一化后处理

Result: 召回率43.2%与精确率29.8%的平衡，相比基线模型（召回率77.5%/精确率17.6%）更优

Conclusion: 提出的算法流程简单、语言无关，特别适合低资源语言发音检测，显著减少对L2数据的依赖

Abstract: Mispronunciation detection (MD) models are the cornerstones of many language
learning applications. Unfortunately, most systems are built for English and
other major languages, while low-resourced language varieties, such as Finland
Swedish (FS), lack such tools. In this paper, we introduce our MD model for FS,
trained on 89 hours of first language (L1) speakers' spontaneous speech and
tested on 33 minutes of L2 transcribed read-aloud speech.
  We trained a multilingual wav2vec 2.0 model with entropy regularization,
followed by temperature scaling and top-k normalization after the inference to
better adapt it for MD. The main novelty of our method lies in its simplicity,
requiring minimal L2 data. The process is also language-independent, making it
suitable for other low-resource languages. Our proposed algorithm allows us to
balance Recall (43.2%) and Precision (29.8%), compared with the baseline
model's Recall (77.5%) and Precision (17.6%).

</details>


### [140] [The Inverse Scaling Effect of Pre-Trained Language Model Surprisal Is Not Due to Data Leakage](https://arxiv.org/abs/2506.01172)
*Byung-Doh Oh,Hongao Zhu,William Schuler*

Main category: cs.CL

TL;DR: 通过双重验证实验证明语言模型预测阅读时间的效果与数据泄露无关


<details>
  <summary>Details</summary>
Motivation: 验证大语言模型预测人类阅读时间效果差的现象是否由训练数据泄露导致

Method: 1. 基于n-gram重叠分析五个阅读语料库的泄露程度
2. 使用无泄露数据训练模型进行复现实验

Result: 语料库泄露程度极低（长度0.5%-2.1%，频率0.0003%-0.003%），且模型规模与预测效果负相关的现象仍然存在

Conclusion: 语言模型规模增大导致预测阅读时间能力下降的现象具有内在机制，非数据泄露的伪相关

Abstract: In psycholinguistic modeling, surprisal from larger pre-trained language
models has been shown to be a poorer predictor of naturalistic human reading
times. However, it has been speculated that this may be due to data leakage
that caused language models to see the text stimuli during training. This paper
presents two studies to address this concern at scale. The first study reveals
relatively little leakage of five naturalistic reading time corpora in two
pre-training datasets in terms of length and frequency of token $n$-gram
overlap. The second study replicates the negative relationship between language
model size and the fit of surprisal to reading times using models trained on
'leakage-free' data that overlaps only minimally with the reading time corpora.
Taken together, this suggests that previous results using language models
trained on these corpora are not driven by the effects of data leakage.

</details>


### [141] [LAQuer: Localized Attribution Queries in Content-grounded Generation](https://arxiv.org/abs/2506.01187)
*Eran Hirsch,Aviv Slobodkin,David Wan,Elias Stengel-Eskin,Mohit Bansal,Ido Dagan*

Main category: cs.CL

TL;DR: 提出LAQuer任务实现细粒度的用户导向文本归因，通过提示LLM和内部表征两种方法，在多文档摘要和长式问答任务中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成模型的归因方法存在粗粒度（全句关联）或与用户需求不匹配（子句归因不精准）的问题，需改进归因可用性。

Method: 1. 提出LAQuer任务进行局部归因查询
2. 开发基于提示LLM和模型内部表征的解决方案
3. 构建扩展框架并在MDS/LFQA任务中验证

Result: LAQuer方法显著缩短归因文本长度（实验证明），归因精确度提升，用户验证效率提高。

Conclusion: 定义了LAQuer新任务提升归因可用性，提出建模框架并建立基线，同时设计新评估标准推动局部归因研究发展。

Abstract: Grounded text generation models often produce content that deviates from
their source material, requiring user verification to ensure accuracy. Existing
attribution methods associate entire sentences with source documents, which can
be overwhelming for users seeking to fact-check specific claims. In contrast,
existing sub-sentence attribution methods may be more precise but fail to align
with users' interests. In light of these limitations, we introduce Localized
Attribution Queries (LAQuer), a new task that localizes selected spans of
generated output to their corresponding source spans, allowing fine-grained and
user-directed attribution. We compare two approaches for the LAQuer task,
including prompting large language models (LLMs) and leveraging LLM internal
representations. We then explore a modeling framework that extends existing
attributed text generation methods to LAQuer. We evaluate this framework across
two grounded text generation tasks: Multi-document Summarization (MDS) and
Long-form Question Answering (LFQA). Our findings show that LAQuer methods
significantly reduce the length of the attributed text. Our contributions
include: (1) proposing the LAQuer task to enhance attribution usability, (2)
suggesting a modeling framework and benchmarking multiple baselines, and (3)
proposing a new evaluation setting to promote future research on localized
attribution in content-grounded generation.

</details>


### [142] [Culturally-Grounded Chain-of-Thought (CG-CoT):Enhancing LLM Performance on Culturally-Specific Tasks in Low-Resource Languages](https://arxiv.org/abs/2506.01190)
*Madhavendra Thakur*

Main category: cs.CL

TL;DR: 提出CG-CoT方法解决LLMs在低资源语言文化推理中的不足，通过文化语境检索提升任务表现，揭示传统翻译指标与文化相关性差异


<details>
  <summary>Details</summary>
Motivation: LLMs在文化特定任务（尤其低资源语言）表现不佳，影响全球AI公平部署，需建立文化敏感的推理机制

Method: CG-CoT = 文化语境密集向量检索 + 显式推理链，在约鲁巴谚语任务中验证有效性

Result: CG-CoT文化对齐准确率提升23%，推理深度增加40%；BLEU分数与人类文化相关性评分相关系数仅0.32

Conclusion: 文化敏感的提示策略显著优于传统方法，NLP评估体系需建立文化维度的新范式

Abstract: Large Language Models (LLMs) struggle with culturally-specific reasoning
tasks, particularly in low-resource languages, hindering their global
applicability. Addressing this gap is crucial for equitable AI deployment. We
introduce Culturally-Grounded Chain-of-Thought (CG-CoT), a novel prompting
strategy that combines dense vector retrieval of cultural context with explicit
reasoning sequences. Our extensive experiments on Yoruba proverb interpretation
demonstrate that CG-CoT provides significantly higher culturally-aligned
accuracy and depth than traditional prompting methods, validated through both
automated metrics and LLM-based evaluations. Notably, we uncover stark
disparities between token-level translation metrics like BLEU and human-judged
cultural relevance, suggesting a rethinking of evaluation approaches for
low-resource NLP.

</details>


### [143] [CoBRA: Quantifying Strategic Language Use and LLM Pragmatics](https://arxiv.org/abs/2506.01195)
*Anshun Asher Zheng,Junyi Jessy Li,David I. Beaver*

Main category: cs.CL

TL;DR: 提出CoBRA框架和CHARM数据集分析非合作语境下的战略语言使用，发现大语言模型对此类语言理解存在明显局限。


<details>
  <summary>Details</summary>
Motivation: 现有语用学和LLM研究多聚焦合作性语境，缺乏对法庭对抗场景等非合作语境战略语言的系统研究框架。

Method: 开发包含BaT/PaT/NRBaT指标的CoBRA量化评估体系，构建真实法庭质证数据集CHARM进行验证。

Result: LLMs对战略语言理解有限：模型规模扩大提升表现，但推理能力反而导致过度复杂化和内部逻辑混乱。

Conclusion: 揭示LLMs在非合作语用理解上的缺陷，提出模型规模优先于推理能力的优化方向，为战略语言评估提供新工具。

Abstract: Language is often used strategically, particularly in high-stakes,
adversarial settings, yet most work on pragmatics and LLMs centers on
cooperativity. This leaves a gap in systematic understanding of non-cooperative
discourse. To address this, we introduce CoBRA (Cooperation-Breach Response
Assessment), along with three interpretable metrics -- Benefit at Turn (BaT),
Penalty at Turn (PaT), and Normalized Relative Benefit at Turn (NRBaT) -- to
quantify the perceived strategic effects of discourse moves. We also present
CHARM, an annotated dataset of real courtroom cross-examinations, to
demonstrate the framework's effectiveness. Using these tools, we evaluate a
range of LLMs and show that LLMs generally exhibit limited pragmatic
understanding of strategic language. While model size shows an increase in
performance on our metrics, reasoning ability does not help and largely hurts,
introducing overcomplication and internal confusion.

</details>


### [144] [Incorporating Hierarchical Semantics in Sparse Autoencoder Architectures](https://arxiv.org/abs/2506.01197)
*Mark Muchane,Sean Richardson,Kiho Park,Victor Veitch*

Main category: cs.CL

TL;DR: 提出改进型稀疏自编码器架构，通过建模语义层次提升LLM特征解释性、重建能力和计算效率


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器未有效利用概念间的语义层次关系，限制了特征解释性和模型性能

Method: 设计显式建模语义层次结构的改进SAE架构，应用于大语言模型内部表示编码

Result: 成功学习到语义层次结构，在重建精度、特征可解释性方面均有提升，同时显著提高计算效率

Conclusion: 语义层次建模是提升自编码器在可解释AI领域应用效果的关键改进方向

Abstract: Sparse dictionary learning (and, in particular, sparse autoencoders) attempts
to learn a set of human-understandable concepts that can explain variation on
an abstract space. A basic limitation of this approach is that it neither
exploits nor represents the semantic relationships between the learned
concepts. In this paper, we introduce a modified SAE architecture that
explicitly models a semantic hierarchy of concepts. Application of this
architecture to the internal representations of large language models shows
both that semantic hierarchy can be learned, and that doing so improves both
reconstruction and interpretability. Additionally, the architecture leads to
significant improvements in computational efficiency.

</details>


### [145] [Trick or Neat: Adversarial Ambiguity and Language Model Evaluation](https://arxiv.org/abs/2506.01205)
*Antonia Karamolegkou,Oliver Eberle,Phillip Rust,Carina Kauf,Anders Søgaard*

Main category: cs.CL

TL;DR: 该研究通过对抗性歧义数据集评估语言模型对歧义的敏感性，发现线性探测方法在解码歧义时准确率显著优于直接提示法（可达90%+），揭示了模型在不同层级编码歧义的特征。


<details>
  <summary>Details</summary>
Motivation: 语言歧义检测对不确定性估计、幽默识别等语言理解任务至关重要。当前需要评估语言模型对各类歧义的敏感性及其编码机制。

Method: 构建包含句法/词汇/语音歧义及其对抗变体（词序变化/同义词替换/随机扰动）的数据集，对比直接提示法和基于模型表征的线性探测方法。

Result: 线性探测方法准确率显著优于提示法（部分超90%），模型不同层对歧义的编码能力存在差异。

Conclusion: 研究揭示了提示范式的局限性及语言模型层级化编码歧义的特性，公开的代码数据为后续研究提供资源支持。

Abstract: Detecting ambiguity is important for language understanding, including
uncertainty estimation, humour detection, and processing garden path sentences.
We assess language models' sensitivity to ambiguity by introducing an
adversarial ambiguity dataset that includes syntactic, lexical, and
phonological ambiguities along with adversarial variations (e.g., word-order
changes, synonym replacements, and random-based alterations). Our findings show
that direct prompting fails to robustly identify ambiguity, while linear probes
trained on model representations can decode ambiguity with high accuracy,
sometimes exceeding 90\%. Our results offer insights into the prompting
paradigm and how language models encode ambiguity at different layers. We
release both our code and data: https://github.com/coastalcph/lm_ambiguity.

</details>


### [146] [Mamba Drafters for Speculative Decoding](https://arxiv.org/abs/2506.01206)
*Daewon Choi,Seunghyuk Oh,Saket Dingliwal,Jihoon Tack,Kyuyoung Kim,Woomin Song,Seojin Kim,Insu Han,Jinwoo Shin,Aram Galstyan,Shubham Katiyar,Sravan Babu Bodapati*

Main category: cs.CL

TL;DR: 提出基于Mamba状态空间模型的推测解码方案，在保持跨模型适应性的同时实现高效推理


<details>
  <summary>Details</summary>
Motivation: 解决现有推测解码方法在外部起草模型灵活性（速度慢）与自推测方法效率（需重训练）之间的权衡矛盾

Method: 利用Mamba模型的线性结构避免Transformer二次复杂度，结合测试时树搜索算法提升草案质量

Result: Mamba起草模型内存消耗降低，推理速度超越现有外部方法，性能媲美自推测方法

Conclusion: Mamba框架为推测解码提供了灵活性与效率兼备的解决方案，成为大模型推理加速的强竞争力方案

Abstract: Speculative decoding has emerged as a promising approach to accelerating
large language model (LLM) generation using a fast drafter while maintaining
alignment with the target model's distribution. However, existing approaches
face a trade-off: external drafters offer flexibility but can suffer from
slower drafting, while self-speculation methods use drafters tailored to the
target model but require re-training. In this paper, we introduce novel
drafters based on Mamba, a state-of-the-art state space model (SSM), as a
solution that combines the best aspects of both approaches. By leveraging the
linear structure of SSMs, our approach avoids the quadratic complexity inherent
in traditional Transformer-based methods, enabling faster drafting and lower
memory usage while maintaining the flexibility to work across different target
models. We further enhance efficiency with a novel test-time tree search
algorithm for generating high-quality draft candidates. Our empirical
evaluation demonstrates that Mamba-based drafters not only outperform existing
external drafting methods but are also comparable to state-of-the-art
self-speculation approaches while using less memory and maintaining their
cross-model adaptability.

</details>


### [147] [Compress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers](https://arxiv.org/abs/2506.01215)
*Woomin Song,Sai Muralidhar Jayanthi,Srikanth Ronanki,Kanthashree Mysore Sathyendra,Jinwoo Shin,Aram Galstyan,Shubham Katiyar,Sravan Babu Bodapati*

Main category: cs.CL

TL;DR: 提出REFORM框架，通过增量处理+选择性重计算的混合策略，在1M上下文长度下实现50%+性能提升，同时降低30%推理时间和5%内存峰值


<details>
  <summary>Details</summary>
Motivation: 解决大模型处理超长上下文时存在的效率瓶颈（传统方法存在信息丢失/内存占用高的问题），需兼顾性能与资源效率

Method: 两阶段框架：1）增量处理输入块并构建跨层上下文嵌入 2）通过相似度匹配定位关键令牌，选择性重计算KV缓存

Result: RULER/BABILong任务分别提升50%/27%，Infinite-Bench/MM-NIAH跨领域任务表现优异，推理效率提升30%且内存占用减少5%

Conclusion: REFORM首次实现长上下文处理中效率与性能的协同优化，为实际应用提供可扩展解决方案

Abstract: As large language models increasingly gain popularity in real-world
applications, processing extremely long contexts, often exceeding the model's
pre-trained context limits, has emerged as a critical challenge. While existing
approaches to efficient long-context processing show promise, recurrent
compression-based methods struggle with information preservation, whereas
random access approaches require substantial memory resources. We introduce
REFORM, a novel inference framework that efficiently handles long contexts
through a two-phase approach. First, it incrementally processes input chunks
while maintaining a compressed KV cache, constructs cross-layer context
embeddings, and utilizes early exit strategy for improved efficiency. Second,
it identifies and gathers essential tokens via similarity matching and
selectively recomputes the KV cache. Compared to baselines, REFORM achieves
over 50% and 27% performance gains on RULER and BABILong respectively at 1M
context length. It also outperforms baselines on Infinite-Bench and MM-NIAH,
demonstrating flexibility across diverse tasks and domains. Additionally,
REFORM reduces inference time by 30% and peak memory usage by 5%, achieving
both efficiency and superior performance.

</details>


### [148] [Polishing Every Facet of the GEM: Testing Linguistic Competence of LLMs and Humans in Korean](https://arxiv.org/abs/2506.01237)
*SungHo Kim,Nayeon Kim,Taehee Jeon,SangKeun Lee*

Main category: cs.CL

TL;DR: 开发韩语语法评估基准KoGEM（1.5k多选题），评估发现LLMs在需要经验整合的任务（如音韵规则）表现较弱，融入经验知识可提升语言能力。


<details>
  <summary>Details</summary>
Motivation: 揭示当前大语言模型在韩语综合能力上的局限性，特别是需要现实经验知识的任务表现缺陷。

Method: 构建覆盖5大类16子类的评估基准，采用零样本测试27个不同规模/类型的LLM。

Result: LLMs在定义性知识任务表现优异，但整合经验知识（如发音规则）的任务失败率达80%以上。

Conclusion: KoGEM不仅量化LLMs语言能力边界，更为提升综合语言理解提供了新的改进方向与评估标准。

Abstract: We introduce the $\underline{Ko}rean \underline{G}rammar
\underline{E}valuation Bench\underline{M}ark (KoGEM)$, designed to assess the
linguistic competence of LLMs and humans in Korean. KoGEM consists of 1.5k
multiple-choice QA pairs covering five main categories and 16 subcategories.
The zero-shot evaluation of 27 LLMs of various sizes and types reveals that
while LLMs perform remarkably well on straightforward tasks requiring primarily
definitional knowledge, they struggle with tasks that demand the integration of
real-world experiential knowledge, such as phonological rules and
pronunciation. Furthermore, our in-depth analysis suggests that incorporating
such experiential knowledge could enhance the linguistic competence of LLMs.
With KoGEM, we not only highlight the limitations of current LLMs in linguistic
competence but also uncover hidden facets of LLMs in linguistic competence,
paving the way for enhancing comprehensive language understanding. Our code and
dataset are available at: https://github.com/SungHo3268/KoGEM.

</details>


### [149] [ExpertLongBench: Benchmarking Language Models on Expert-Level Long-Form Generation Tasks with Structured Checklists](https://arxiv.org/abs/2506.01241)
*Jie Ruan,Inderjeet Nair,Shuyang Cao,Amy Liu,Sheza Munir,Micah Pollens-Dempsey,Tiffany Chiang,Lucy Kates,Nicholas David,Sihan Chen,Ruxin Yang,Yuqian Yang,Jasmine Gump,Tessa Bialek,Vivek Sankaran,Margo Schlanger,Lu Wang*

Main category: cs.CL

TL;DR: 提出ExpertLongBench专家级长文本评测基准及CLEAR评估框架，验证现有大模型在复杂任务中的表现差距


<details>
  <summary>Details</summary>
Motivation: 现有基准缺乏对专家级长文本生成任务（超过5000 token）的细粒度评估，需建立更贴近实际工作流程的领域专用评测体系

Method: 通过CLEAR框架（基于任务专项rubric生成检查清单）实现细粒度输出对比评估，利用开源模型实现可扩展的低成本检查表提取与比较

Result: 最佳模型F1值仅26.8%，现有模型在内容准确性（平均召回率57.0%）和检查项完整性（平均精度44.6%）方面均存在显著缺陷

Conclusion: 专家级任务需模型能力突破，CLEAR框架验证了通过结构化检查表实现高效长文本评估的可行性

Abstract: This paper introduces ExpertLongBench, an expert-level benchmark containing
11 tasks from 9 domains that reflect realistic expert workflows and
applications. Beyond question answering, the application-driven tasks in
ExpertLongBench demand long-form outputs that can exceed 5,000 tokens and
strict adherence to domain-specific requirements. Notably, each task in
ExpertLongBench includes a rubric, designed or validated by domain experts, to
specify task requirements and guide output evaluation. Furthermore, we propose
CLEAR, an evaluation framework that supports accurate evaluation of long-form
model outputs in our benchmark. To achieve fine-grained, expert-aligned
evaluation, CLEAR derives checklists from both model outputs and references by
extracting information corresponding to items in the task-specific rubric.
Checklist items for model outputs are then compared with corresponding items
for reference outputs to assess their correctness, enabling grounded
evaluation. We benchmark 11 large language models (LLMs) and analyze components
in CLEAR, showing that (1) existing LLMs, with the top performer achieving only
a 26.8% F1 score, require significant improvement for expert-level tasks; (2)
models can generate content corresponding to the required aspects, though often
not accurately; and (3) accurate checklist extraction and comparison in CLEAR
can be achieved by open-weight models for more scalable and low-cost usage.

</details>


### [150] [MTCMB: A Multi-Task Benchmark Framework for Evaluating LLMs on Knowledge, Reasoning, and Safety in Traditional Chinese Medicine](https://arxiv.org/abs/2506.01252)
*Shufeng Kong,Xingru Yang,Yuanyuan Wei,Zijie Wang,Hao Tang,Jiuqi Qin,Shuting Lan,Yingheng Wang,Junwen Bai,Zhuangbin Chen,Zibin Zheng,Caihua Liu,Hao Liang*

Main category: cs.CL

TL;DR: 提出首个中医领域多任务基准MTCMB，系统评估大模型在知识掌握、临床推理与安全性等维度表现


<details>
  <summary>Details</summary>
Motivation: 现有评测体系局限于问答形式且缺乏临床真实性，无法满足中医领域推理复杂性和文本多样性的评估需求

Method: 联合中医专家构建12个子数据集，涵盖知识问答、语言理解、诊断推理、处方生成及安全性评估五大核心场景，整合真实医案、国考真题与经典文献

Result: 当前大模型在基础知识表现良好，但在临床推理（准确率<45%）、处方规划（合格率<30%）和安全性（风险率>40%）方面存在显著不足

Conclusion: MTCMB填补领域评估空白，为开发可靠的中医人工智能系统提供关键基准工具，推动医疗AI向专业化发展

Abstract: Traditional Chinese Medicine (TCM) is a holistic medical system with
millennia of accumulated clinical experience, playing a vital role in global
healthcare-particularly across East Asia. However, the implicit reasoning,
diverse textual forms, and lack of standardization in TCM pose major challenges
for computational modeling and evaluation. Large Language Models (LLMs) have
demonstrated remarkable potential in processing natural language across diverse
domains, including general medicine. Yet, their systematic evaluation in the
TCM domain remains underdeveloped. Existing benchmarks either focus narrowly on
factual question answering or lack domain-specific tasks and clinical realism.
To fill this gap, we introduce MTCMB-a Multi-Task Benchmark for Evaluating LLMs
on TCM Knowledge, Reasoning, and Safety. Developed in collaboration with
certified TCM experts, MTCMB comprises 12 sub-datasets spanning five major
categories: knowledge QA, language understanding, diagnostic reasoning,
prescription generation, and safety evaluation. The benchmark integrates
real-world case records, national licensing exams, and classical texts,
providing an authentic and comprehensive testbed for TCM-capable models.
Preliminary results indicate that current LLMs perform well on foundational
knowledge but fall short in clinical reasoning, prescription planning, and
safety compliance. These findings highlight the urgent need for domain-aligned
benchmarks like MTCMB to guide the development of more competent and
trustworthy medical AI systems. All datasets, code, and evaluation tools are
publicly available at: https://github.com/Wayyuanyuan/MTCMB.

</details>


### [151] [CoRE: Condition-based Reasoning for Identifying Outcome Variance in Complex Events](https://arxiv.org/abs/2506.01253)
*Sai Vallurupalli,Francis Ferraro*

Main category: cs.CL

TL;DR: 论文探讨潜在条件对事件结果的影响机制，通过整合标注数据验证大语言模型在条件推理任务中的表现差异。


<details>
  <summary>Details</summary>
Motivation: 解决复杂事件结果验证时潜在条件的识别难题，探索条件对结果解释的有效性

Method: 融合两个目标/状态数据集，设计Condition-based Reasoning任务验证不同规模LLM的推理能力

Result: 大模型（如GPT-4o）在上下文缺失时更谨慎，条件推理能力与模型生成/识别变体条件的能力正相关

Conclusion: 条件机制能有效补偿缺失信息，模型规模影响条件推理可靠性，需针对性优化条件处理能力

Abstract: Knowing which latent conditions lead to a particular outcome is useful for
critically examining claims made about complex event outcomes. Identifying
implied conditions and examining their influence on an outcome is challenging.
We handle this by combining and augmenting annotations from two existing
datasets consisting of goals and states, and explore the influence of
conditions through our research questions and Condition-based Reasoning tasks.
We examine open and closed LLMs of varying sizes and intent-alignment on our
reasoning tasks and find that conditions are useful when not all context is
available. Models differ widely in their ability to generate and identify
outcome-variant conditions which affects their performance on outcome
validation when conditions are used to replace missing context. Larger models
like GPT-4o, are more cautious in such less constrained situations.

</details>


### [152] [Memory-Efficient FastText: A Comprehensive Approach Using Double-Array Trie Structures and Mark-Compact Memory Management](https://arxiv.org/abs/2506.01254)
*Yimin Du*

Main category: cs.CL

TL;DR: 提出基于双数组字典树和标记-压缩垃圾回收的FastText内存优化框架，实现4:1至10:1的压缩比，工业部署内存从100GB降至30GB


<details>
  <summary>Details</summary>
Motivation: 解决传统FastText哈希冲突导致的语义偏移问题，改善大规模工业场景下处理百万级词汇时内存消耗过高（超100GB）的瓶颈

Method: 1. 构建前缀字典树映射嵌入 2. 基于前后缀相似性压缩 3. 标记-压缩内存重组。利用同源词缀的语义相关性，系统合并结构相似的嵌入

Result: 3000万中文词汇数据集验证内存降低至30GB（压缩比>3:1），工业部署实现成本降低67%、加载速度提升3倍，模型可靠性显著提升

Conclusion: 该框架通过语言学驱动的内存优化策略，在保持嵌入质量的同时实现工业级内存效率突破，为大规模NLP部署提供新范式

Abstract: FastText has established itself as a fundamental algorithm for learning word
representations, demonstrating exceptional capability in handling
out-of-vocabulary words through character-level n-gram embeddings. However, its
hash-based bucketing mechanism introduces critical limitations for large-scale
industrial deployment: hash collisions cause semantic drift, and memory
requirements become prohibitively expensive when dealing with real-world
vocabularies containing millions of terms. This paper presents a comprehensive
memory optimization framework that fundamentally reimagines FastText's memory
management through the integration of double-array trie (DA-trie) structures
and mark-compact garbage collection principles. Our approach leverages the
linguistic insight that n-grams sharing common prefixes or suffixes exhibit
highly correlated embeddings due to co-occurrence patterns in natural language.
By systematically identifying and merging semantically similar embeddings based
on structural relationships, we achieve compression ratios of 4:1 to 10:1 while
maintaining near-perfect embedding quality. The algorithm consists of four
sophisticated phases: prefix trie construction with embedding mapping,
prefix-based similarity compression, suffix-based similarity compression, and
mark-compact memory reorganization. Comprehensive experiments on a 30-million
Chinese vocabulary dataset demonstrate memory reduction from over 100GB to
approximately 30GB with negligible performance degradation. Our industrial
deployment results show significant cost reduction, faster loading times, and
improved model reliability through the elimination of hash collision artifacts.
Code and experimental implementations are available at:
https://github.com/initial-d/me_fasttext

</details>


### [153] [DeepSeek in Healthcare: A Survey of Capabilities, Risks, and Clinical Applications of Open-Source Large Language Models](https://arxiv.org/abs/2506.01257)
*Jiancheng Ye,Sophie Bronstein,Jiarui Hai,Malak Abu Hashish*

Main category: cs.CL

TL;DR: DeepSeek-R1是开源的混合架构大模型，在STEM和医疗领域展现强推理能力但存在安全风险


<details>
  <summary>Details</summary>
Motivation: 提供透明高效的开源大模型替代方案，推动可解释AI在专业领域的发展

Method: 整合混合专家模型(MoE)、思维链推理(CoT)和强化学习的复合架构，优化计算资源分配

Result: 在USMLE临床考试达87.2%准确率，AIME数学竞赛题解决率64.9%，儿科诊断准确率91.3%

Conclusion: 该模型标志着开源AI的重大进展，但需加强安全对齐和伦理合规性研究

Abstract: DeepSeek-R1 is a cutting-edge open-source large language model (LLM)
developed by DeepSeek, showcasing advanced reasoning capabilities through a
hybrid architecture that integrates mixture of experts (MoE), chain of thought
(CoT) reasoning, and reinforcement learning. Released under the permissive MIT
license, DeepSeek-R1 offers a transparent and cost-effective alternative to
proprietary models like GPT-4o and Claude-3 Opus; it excels in structured
problem-solving domains such as mathematics, healthcare diagnostics, code
generation, and pharmaceutical research. The model demonstrates competitive
performance on benchmarks like the United States Medical Licensing Examination
(USMLE) and American Invitational Mathematics Examination (AIME), with strong
results in pediatric and ophthalmologic clinical decision support tasks. Its
architecture enables efficient inference while preserving reasoning depth,
making it suitable for deployment in resource-constrained settings. However,
DeepSeek-R1 also exhibits increased vulnerability to bias, misinformation,
adversarial manipulation, and safety failures - especially in multilingual and
ethically sensitive contexts. This survey highlights the model's strengths,
including interpretability, scalability, and adaptability, alongside its
limitations in general language fluency and safety alignment. Future research
priorities include improving bias mitigation, natural language comprehension,
domain-specific validation, and regulatory compliance. Overall, DeepSeek-R1
represents a major advance in open, scalable AI, underscoring the need for
collaborative governance to ensure responsible and equitable deployment.

</details>


### [154] [Exploring the Potential of LLMs as Personalized Assistants: Dataset, Evaluation, and Analysis](https://arxiv.org/abs/2506.01262)
*Jisoo Mok,Ik-hwan Kim,Sangkwon Park,Sungroh Yoon*

Main category: cs.CL

TL;DR: 提出HiCUPID基准测试框架，包含对话数据集和基于Llama-3.2的自动化评估模型，助力LLM个性化回复研究


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门用于个性化AI助手研究的开源对话数据集，阻碍了该领域研究进展

Method: 构建包含个性化对话数据集和自动化评估模型的基准测试框架（Llama-3.2驱动）

Result: 开发的评估模型与人类偏好高度一致，有效指导个性化回复质量评估

Conclusion: 开源发布数据集、评估模型及代码，推动个性化AI助手研究社区发展

Abstract: Personalized AI assistants, a hallmark of the human-like capabilities of
Large Language Models (LLMs), are a challenging application that intertwines
multiple problems in LLM research. Despite the growing interest in the
development of personalized assistants, the lack of an open-source
conversational dataset tailored for personalization remains a significant
obstacle for researchers in the field. To address this research gap, we
introduce HiCUPID, a new benchmark to probe and unleash the potential of LLMs
to deliver personalized responses. Alongside a conversational dataset, HiCUPID
provides a Llama-3.2-based automated evaluation model whose assessment closely
mirrors human preferences. We release our dataset, evaluation model, and code
at https://github.com/12kimih/HiCUPID.

</details>


### [155] [WCTC-Biasing: Retraining-free Contextual Biasing ASR with Wildcard CTC-based Keyword Spotting and Inter-layer Biasing](https://arxiv.org/abs/2506.01263)
*Yu Nakagome,Michael Hentschel*

Main category: cs.CL

TL;DR: 提出基于CTC模型的声学特征关键词检测方法，无需重新训练模型即可提升29%未知词识别F1值


<details>
  <summary>Details</summary>
Motivation: 现有端到端语音识别模型对训练数据词汇存在偏差，导致专有名词和罕见词识别不准确

Method: 利用中间层声学特征进行关键词检测，采用通配符CTC实现快速模糊匹配，在检测到关键词时对后续声学层施加偏置

Result: 日语语音识别实验中，未知词F1值提升29%

Conclusion: 该方法有效提升罕见词识别精度，且无需重新训练现有模型，可轻松扩展至大规模模型

Abstract: Despite recent advances in end-to-end speech recognition methods, the output
tends to be biased to the training data's vocabulary, resulting in inaccurate
recognition of proper nouns and other unknown terms. To address this issue, we
propose a method to improve recognition accuracy of such rare words in
CTC-based models without additional training or text-to-speech systems.
Specifically, keyword spotting is performed using acoustic features of
intermediate layers during inference, and a bias is applied to the subsequent
layers of the acoustic model for detected keywords. For keyword detection, we
adopt a wildcard CTC that is both fast and tolerant of ambiguous matches,
allowing flexible handling of words that are difficult to match strictly. Since
this method does not require retraining of existing models, it can be easily
applied to even large-scale models. In experiments on Japanese speech
recognition, the proposed method achieved a 29% improvement in the F1 score for
unknown words.

</details>


### [156] [Beyond In-Context Learning: Aligning Long-form Generation of Large Language Models via Task-Inherent Attribute Guidelines](https://arxiv.org/abs/2506.01265)
*Do Xuan Long,Duong Ngoc Yen,Do Xuan Trong,Luu Anh Tuan,Kenji Kawaguchi,Shafiq Joty,Min-Yen Kan,Nancy F. Chen*

Main category: cs.CL

TL;DR: LongGuide通过自动生成双流指导方针（指标指导+输出约束指导），有效提升LLMs在长文本生成任务中的表现，改进幅度超5%


<details>
  <summary>Details</summary>
Motivation: 现有ICL方法在长文本生成任务（如摘要）中表现欠佳，因示范样本无法充分传授任务语言模式和格式分布

Method: 提出LongGuide框架：1）指标指导（MGs）优化自评估指标 2）输出约束指导（OCGs）双层级控制生成 3）自动选择最佳指导组合

Result: 在零样本/少样本场景下，开源和闭源LLMs性能均提升超5%；验证了框架的通用性、弱模型指导强模型能力，以及与自动提示优化的协同效应

Conclusion: 显式定义任务分布通过指导方针比单纯示范更有效，LongGuide为提升LLMs长文本生成能力提供了系统化解决方案

Abstract: In-context learning (ICL) is an important yet not fully understood ability of
pre-trained large language models (LLMs). It can greatly enhance task
performance using a few examples, termed demonstrations, without fine-tuning.
Although effective in question answering, ICL often underperforms in long-form
generation tasks such as summarization. Under appropriately realistic
assumptions, we empirically and theoretically show that ICL demonstrations
alone are insufficient to teach LLMs the task language and format distributions
for generation. We argue for explicit exposure to the task distributions and
hypothesize that defining them by prompting enhances model performance. To this
end, we present LongGuide, which efficiently generates two parallel streams of
guidelines capturing task language and format properties: (i) Metric Guidelines
(MGs) that instruct models to optimize self-evaluated metrics; and (ii) Output
Constraint Guidelines (OCGs) that constrain generation at both token and
sentence levels. LongGuide automatically selects the best combination of
guidelines, improving both strong open- and closed-source LLMs by over 5% in
both zero- and few-shot settings. We show that LongGuide is generalizable,
learnable by weak models to enhance strong ones, and integrates synergistically
with automatic prompt optimizers.

</details>


### [157] [Detoxification of Large Language Models through Output-layer Fusion with a Calibration Model](https://arxiv.org/abs/2506.01266)
*Yuanhe Tian,Mingjie Deng,Guoqing Jin,Yan Song*

Main category: cs.CL

TL;DR: 提出基于预训练校准模型的轻量干预方法，在降低LLM生成内容有害性的同时保持流畅性和上下文理解


<details>
  <summary>Details</summary>
Motivation: 现有LLM去毒方法依赖大规模非毒性数据/人工标注，存在计算成本高、鲁棒性差、损害模型性能的缺陷

Method: 通过预训练校准模型在生成阶段进行轻量干预，利用无毒数据学习去毒嵌入空间，无需修改LLM参数即可跨模型应用

Result: 基准测试显示在降低毒性指标的同时保持合理的内容表达能力

Conclusion: 轻量级校准机制有效平衡去毒效果与内容质量，具备模型通用性和部署便捷性优势

Abstract: Existing approaches for Large language model (LLM) detoxification generally
rely on training on large-scale non-toxic or human-annotated preference data,
designing prompts to instruct the LLM to generate safe content, or modifying
the model parameters to remove toxic information, which are computationally
expensive, lack robustness, and often compromise LLMs' fluency and contextual
understanding. In this paper, we propose a simple yet effective approach for
LLM detoxification, which leverages a compact, pre-trained calibration model
that guides the detoxification process of a target LLM via a lightweight
intervention in its generation pipeline. By learning a detoxified embedding
space from non-toxic data, the calibration model effectively steers the LLM
away from generating harmful content. This approach only requires a one-time
training of the calibration model that is able to be seamlessly applied to
multiple LLMs without compromising fluency or contextual understanding.
Experiment results on the benchmark dataset demonstrate that our approach
reduces toxicity while maintaining reasonable content expression.

</details>


### [158] [Schema as Parameterized Tools for Universal Information Extraction](https://arxiv.org/abs/2506.01276)
*Sheng Liang,Yongyue Zhang,Yaxiong Wu,Ruiming Tang,Yong Liu*

Main category: cs.CL

TL;DR: 提出SPT框架统一自适应文本结构化生成，通过参数化工具提升信息抽取效率与适应性


<details>
  <summary>Details</summary>
Motivation: 解决UIE系统在预定义模式选择与上下文学习动态生成时的适应性不足问题

Method: 将预定义模式参数化为工具，通过模式检索/填充/生成三阶段实现开放/闭合/按需IE任务统一

Result: SPT在四项IE任务中展现强健模式检索能力，参数效率优于LoRA基线系统

Conclusion: SPT框架有效统一多类型IE任务，以更少参数实现与主流系统相当的抽取性能

Abstract: Universal information extraction (UIE) primarily employs an extractive
generation approach with large language models (LLMs), typically outputting
structured information based on predefined schemas such as JSON or tables. UIE
suffers from a lack of adaptability when selecting between predefined schemas
and on-the-fly schema generation within the in-context learning paradigm,
especially when there are numerous schemas to choose from. In this paper, we
propose a unified adaptive text-to-structure generation framework, called
Schema as Parameterized Tools (SPT), which reimagines the tool-calling
capability of LLMs by treating predefined schemas as parameterized tools for
tool selection and parameter filling. Specifically, our SPT method can be
applied to unify closed, open, and on-demand IE tasks by adopting Schema
Retrieval by fetching the relevant schemas from a predefined pool, Schema
Filling by extracting information and filling slots as with tool parameters, or
Schema Generation by synthesizing new schemas with uncovered cases. Experiments
show that the SPT method can handle four distinct IE tasks adaptively,
delivering robust schema retrieval and selection performance. SPT also achieves
comparable extraction performance to LoRA baselines and current leading UIE
systems with significantly fewer trainable parameters.

</details>


### [159] [VM14K: First Vietnamese Medical Benchmark](https://arxiv.org/abs/2506.01305)
*Thong Nguyen,Duc Nguyen,Minh Dang,Thai Dao,Long Nguyen,Quan H. Nguyen,Dat Nguyen,Kien Tran,Minh Tran*

Main category: cs.CL

TL;DR: 开发越南语医疗问答基准数据集（34个专科/14k问题），通过可验证数据源构建并开放数据构建流程，支持多语言医疗领域评估


<details>
  <summary>Details</summary>
Motivation: 非英语社区缺乏标准化医疗评估基准，现有数据碎片化且验证困难，阻碍语言模型在医疗场景的可靠应用

Method: 整合医学考试题库与临床记录，经专家标注形成四阶难度体系（从基础生物学知识到复杂临床推理），分公开样本集/完整集/私有集三级发布

Result: 建成首个覆盖全医学专科的越南语评估基准，具备深度学科知识检验能力，开放源代码的标准化构建流程支持跨语言扩展

Conclusion: 该框架可推广至其他语种医疗评估场景，开源数据管道为构建多语言医疗基准提供可复现的方法论基础

Abstract: Medical benchmarks are indispensable for evaluating the capabilities of
language models in healthcare for non-English-speaking communities,therefore
help ensuring the quality of real-life applications. However, not every
community has sufficient resources and standardized methods to effectively
build and design such benchmark, and available non-English medical data is
normally fragmented and difficult to verify. We developed an approach to tackle
this problem and applied it to create the first Vietnamese medical question
benchmark, featuring 14,000 multiple-choice questions across 34 medical
specialties. Our benchmark was constructed using various verifiable sources,
including carefully curated medical exams and clinical records, and eventually
annotated by medical experts. The benchmark includes four difficulty levels,
ranging from foundational biological knowledge commonly found in textbooks to
typical clinical case studies that require advanced reasoning. This design
enables assessment of both the breadth and depth of language models' medical
understanding in the target language thanks to its extensive coverage and
in-depth subject-specific expertise. We release the benchmark in three parts: a
sample public set (4k questions), a full public set (10k questions), and a
private set (2k questions) used for leaderboard evaluation. Each set contains
all medical subfields and difficulty levels. Our approach is scalable to other
languages, and we open-source our data construction pipeline to support the
development of future multilingual benchmarks in the medical domain.

</details>


### [160] [A Platform for Investigating Public Health Content with Efficient Concern Classification](https://arxiv.org/abs/2506.01308)
*Christopher Li,Rickard Stureborg,Bhuwan Dhingra,Jun Yang*

Main category: cs.CL

TL;DR: ConcernScope平台利用教师-学生框架实现大模型到轻量分类器的知识迁移，帮助公共卫生官员快速识别文本中的健康担忧，支持趋势分析和事件影响评估。


<details>
  <summary>Details</summary>
Motivation: 在线公共卫生担忧内容激增导致预防措施推广受阻，需有效工具分析公众关切以制定应对策略。

Method: 基于教师-学生框架构建，支持大文件上传/URL爬取/文本编辑，集成公共卫生担忧分类体系实现自动识别。

Result: 成功分析186,000份样本的时间序列数据，展示重大事件前后话题频率变化趋势的识别能力。

Conclusion: 该平台为公共卫生决策者提供高效的数据探索和动态监测工具，可提升公众健康倡议的响应效率。

Abstract: A recent rise in online content expressing concerns with public health
initiatives has contributed to already stalled uptake of preemptive measures
globally. Future public health efforts must attempt to understand such content,
what concerns it may raise among readers, and how to effectively respond to it.
To this end, we present ConcernScope, a platform that uses a teacher-student
framework for knowledge transfer between large language models and light-weight
classifiers to quickly and effectively identify the health concerns raised in a
text corpus. The platform allows uploading massive files directly,
automatically scraping specific URLs, and direct text editing. ConcernScope is
built on top of a taxonomy of public health concerns. Intended for public
health officials, we demonstrate several applications of this platform: guided
data exploration to find useful examples of common concerns found in online
community datasets, identification of trends in concerns through an example
time series analysis of 186,000 samples, and finding trends in topic frequency
before and after significant events.

</details>


### [161] [Growing Through Experience: Scaling Episodic Grounding in Language Models](https://arxiv.org/abs/2506.01312)
*Chunhui Zhang,Sirui,Wang,Zhongyu Ouyang,Xiangchi Yuan,Soroush Vosoughi*

Main category: cs.CL

TL;DR: 提出弱到强情景学习框架，通过蒙特卡洛树搜索和新型蒸馏方法提升大语言模型在复杂规划任务中的经验利用能力，实验效果超越SOTA模型3.45%。


<details>
  <summary>Details</summary>
Motivation: 现有情景学习方法存在可扩展性差和大模型经验利用机制缺失的局限，导致中型模型效果欠佳且大模型面临『规模悖论』问题。

Method: 集成蒙特卡洛树搜索进行结构化经验收集，设计保持模型原生能力的蒸馏算法实现情景记忆嵌入。

Result: 在多样化规划/问答任务中准确率提升3.45%，深层网络任务对齐显著增强，复杂新场景下表现稳定（基线方法性能下降23%）。

Conclusion: 该框架成功突破大模型情景学习瓶颈，验证了弱模型到强模型的知识迁移可行性，为复杂决策系统提供新范式。

Abstract: Language models (LMs) require robust episodic grounding-the capacity to learn
from and apply past experiences-to excel at physical planning tasks. Current
episodic grounding approaches struggle with scalability and integration,
limiting their effectiveness, especially for medium-sized LMs (7B parameters).
While larger LMs (70-405B parameters) possess superior hierarchical
representations and extensive pre-trained knowledge, they encounter a
fundamental scale paradox: despite their advanced abstraction capabilities,
they lack efficient mechanisms to leverage experience streams. We propose a
scalable weak-to-strong episodic learning framework that effectively transfers
episodic behaviors from smaller to larger LMs. This framework integrates Monte
Carlo tree search for structured experience collection with a novel
distillation method, preserving the inherent LM capabilities while embedding
episodic memory. Experiments demonstrate our method surpasses state-of-the-art
proprietary LMs by 3.45% across diverse planning and question-answering tasks.
Layer-wise probing further indicates significant improvements in task
alignment, especially within deeper LM layers, highlighting stable
generalization even for previously unseen scenarios with increased planning
complexity-conditions where baseline methods degrade markedly.

</details>


### [162] [Zero-Shot Text-to-Speech for Vietnamese](https://arxiv.org/abs/2506.01322)
*Thi Vu,Linh The Nguyen,Dat Quoc Nguyen*

Main category: cs.CL

TL;DR: 团队发布越南语高质量语音数据集PhoAudiobook（941小时），实验证明其显著提升VALL-E、VoiceCraft、XTTS-V2等零样本TTS模型性能。


<details>
  <summary>Details</summary>
Motivation: 填补越南语语音合成领域数据资源不足的缺口，促进该领域技术发展

Method: 使用PhoAudiobook对三大零样本TTS模型进行系统性评测，重点关注短句合成能力

Result: 数据集使模型性能全面提升，其中VALL-E和VoiceCraft在短句合成中表现最优

Conclusion: 开源PhoAudiobook数据集以推动越南语TTS技术研究，建议优先选用VALL-E/VoiceCraft处理短文本场景

Abstract: This paper introduces PhoAudiobook, a newly curated dataset comprising 941
hours of high-quality audio for Vietnamese text-to-speech. Using PhoAudiobook,
we conduct experiments on three leading zero-shot TTS models: VALL-E,
VoiceCraft, and XTTS-V2. Our findings demonstrate that PhoAudiobook
consistently enhances model performance across various metrics. Moreover,
VALL-E and VoiceCraft exhibit superior performance in synthesizing short
sentences, highlighting their robustness in handling diverse linguistic
contexts. We publicly release PhoAudiobook to facilitate further research and
development in Vietnamese text-to-speech.

</details>


### [163] [Evaluating Large Language Models in Crisis Detection: A Real-World Benchmark from Psychological Support Hotlines](https://arxiv.org/abs/2506.01329)
*Guifeng Deng,Shuyin Rao,Tianyu Lin,Anlu Dai,Pan Wang,Junyi Xie,Haidong Song,Ke Zhao,Dongwu Xu,Zhengdong Cheng,Tao Li,Haiteng Jiang*

Main category: cs.CL

TL;DR: 研究通过构建PsyCrisisBench基准评估大型语言模型在心理危机评估中的表现，发现LLMs在自杀风险评估任务中表现优异，但情绪识别仍存在挑战。开源模型与闭源差距缩小，量化技术显著降低部署成本。


<details>
  <summary>Details</summary>
Motivation: 心理援助热线面临日益增长的需求压力，需要验证大型语言模型在情感敏感场景的适用性，为危机干预提供技术支持。

Method: 构建包含540条标注热线的PsyCrisisBench基准，覆盖情绪识别、自杀风险评估等4项任务。评估64个LLM模型家族，采用零样本/少样本/微调方法，通过F1分数和统计检验比较性能。

Result: LLMs在自杀计划识别(F1=0.779)和风险评估(F1=0.907)表现优异，微调后1.5B小模型(Qwen2.5)超过大模型。情绪识别最高F1=0.709，闭源模型仍具优势(p=0.007)。AWQ量化减少70%显存消耗，性能损失<5%。

Conclusion: LLMs在结构化心理评估中展现应用潜力，但情绪识别受限于语境复杂性。开源模型性能接近闭源，结合量化技术可实现实际部署。PsyCrisisBench为伦理部署提供评估框架。

Abstract: Psychological support hotlines are critical for crisis intervention but face
significant challenges due to rising demand. Large language models (LLMs) could
support crisis assessments, yet their capabilities in emotionally sensitive
contexts remain unclear. We introduce PsyCrisisBench, a benchmark of 540
annotated transcripts from the Hangzhou Psychological Assistance Hotline,
assessing four tasks: mood status recognition, suicidal ideation detection,
suicide plan identification, and risk assessment. We evaluated 64 LLMs across
15 families (e.g., GPT, Claude, Gemini, Llama, Qwen, DeepSeek) using zero-shot,
few-shot, and fine-tuning paradigms. Performance was measured by F1-score, with
statistical comparisons via Welch's t-tests. LLMs performed strongly on
suicidal ideation detection (F1=0.880), suicide plan identification (F1=0.779),
and risk assessment (F1=0.907), improved with few-shot and fine-tuning. Mood
status recognition was more challenging (max F1=0.709), likely due to lost
vocal cues and ambiguity. A fine-tuned 1.5B-parameter model (Qwen2.5-1.5B)
surpassed larger models on mood and suicidal ideation. Open-source models like
QwQ-32B performed comparably to closed-source on most tasks (p>0.3), though
closed models retained an edge in mood detection (p=0.007). Performance scaled
with size up to a point; quantization (AWQ) reduced GPU memory by 70% with
minimal F1 degradation. LLMs show substantial promise in structured
psychological crisis assessments, especially with fine-tuning. Mood recognition
remains limited due to contextual complexity. The narrowing gap between open-
and closed-source models, combined with efficient quantization, suggests
feasible integration. PsyCrisisBench offers a robust evaluation framework to
guide model development and ethical deployment in mental health.

</details>


### [164] [Enhancing Interpretable Image Classification Through LLM Agents and Conditional Concept Bottleneck Models](https://arxiv.org/abs/2506.01334)
*Yiwen Jiang,Deval Mehta,Wei Feng,Zongyuan Ge*

Main category: cs.CL

TL;DR: 提出动态代理调整概念数量的概念瓶颈模型CoCoBMs，在6个数据集实现准确率提升6%且可解释性评估提升30%


<details>
  <summary>Details</summary>
Motivation: 现有概念瓶颈模型存在概念冗余与覆盖不足问题，传统概念评分机制存在局限性

Method: 动态代理自适应调整概念库规模 + 条件概念瓶颈模型CoCoBMs引入可编辑评分矩阵

Result: 分类准确率提升6个百分点，可解释性评估指标提升30%

Conclusion: 新方法在保持概念简洁性的同时显著提升模型性能与解释可靠性，为可解释AI提供新范式

Abstract: Concept Bottleneck Models (CBMs) decompose image classification into a
process governed by interpretable, human-readable concepts. Recent advances in
CBMs have used Large Language Models (LLMs) to generate candidate concepts.
However, a critical question remains: What is the optimal number of concepts to
use? Current concept banks suffer from redundancy or insufficient coverage. To
address this issue, we introduce a dynamic, agent-based approach that adjusts
the concept bank in response to environmental feedback, optimizing the number
of concepts for sufficiency yet concise coverage. Moreover, we propose
Conditional Concept Bottleneck Models (CoCoBMs) to overcome the limitations in
traditional CBMs' concept scoring mechanisms. It enhances the accuracy of
assessing each concept's contribution to classification tasks and feature an
editable matrix that allows LLMs to correct concept scores that conflict with
their internal knowledge. Our evaluations across 6 datasets show that our
method not only improves classification accuracy by 6% but also enhances
interpretability assessments by 30%.

</details>


### [165] [The Landscape of Arabic Large Language Models (ALLMs): A New Era for Arabic Language Technology](https://arxiv.org/abs/2506.01340)
*Shahad Al-Khalifa,Nadir Durrani,Hend Al-Khalifa,Firoj Alam*

Main category: cs.CL

TL;DR: 阿拉伯大语言模型(ALLMs)从萌芽到成熟的发展历程，探讨其评估体系及对阿拉伯世界的机遇挑战


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语覆盖4.22亿母语者但缺乏本土LLM，开发ALLMs对弥合技术鸿沟、传承文化具有战略意义

Method: 通过历史演进分析（从基础文本处理到AI驱动模型）和评估体系构建（基准测试+公开排行榜）进行研究

Result: 构建了ALLMs发展路线图，揭示了阿拉伯语多方言特性、计算资源限制等核心挑战，确立评估标准体系

Conclusion: ALLMs是阿拉伯世界数字化转型的关键杠杆，需持续投入算力建设、语料库开发和跨学科协作

Abstract: The emergence of ChatGPT marked a transformative milestone for Artificial
Intelligence (AI), showcasing the remarkable potential of Large Language Models
(LLMs) to generate human-like text. This wave of innovation has revolutionized
how we interact with technology, seamlessly integrating LLMs into everyday
tasks such as vacation planning, email drafting, and content creation. While
English-speaking users have significantly benefited from these advancements,
the Arabic world faces distinct challenges in developing Arabic-specific LLMs.
Arabic, one of the languages spoken most widely around the world, serves more
than 422 million native speakers in 27 countries and is deeply rooted in a rich
linguistic and cultural heritage. Developing Arabic LLMs (ALLMs) presents an
unparalleled opportunity to bridge technological gaps and empower communities.
The journey of ALLMs has been both fascinating and complex, evolving from
rudimentary text processing systems to sophisticated AI-driven models. This
article explores the trajectory of ALLMs, from their inception to the present
day, highlighting the efforts to evaluate these models through benchmarks and
public leaderboards. We also discuss the challenges and opportunities that
ALLMs present for the Arab world.

</details>


### [166] [TurnBench-MS: A Benchmark for Evaluating Multi-Turn, Multi-Step Reasoning in Large Language Models](https://arxiv.org/abs/2506.01341)
*Yiran Zhang,Mo Wang,Xiaoyang Li,Kaixuan Ren,Chencheng Zhu,Usman Naseem*

Main category: cs.CL

TL;DR: 提出TurnBench基准测试，通过多轮代码破解任务评估大语言模型的多步骤推理能力，发现现有模型与人类表现存在显著差距


<details>
  <summary>Details</summary>
Motivation: 现有基准测试集中于单轮/单步任务，无法有效评估真实场景中需要的迭代式、持续性推理能力

Method: 设计包含Classic(标准推理)和Nightmare(复杂推理)双模式的交互式测试框架，提供逐步标注的中间推理步骤数据

Result: 最佳模型在Classic模式准确率81.5%，Nightmare模式骤降至17.8%；人类测试者两模式均达100%

Conclusion: TurnBench通过隐藏规则和反馈循环机制降低了数据污染风险，为诊断和提升LLM的复杂推理能力提供了严谨测试平台

Abstract: Despite impressive advances in large language models (LLMs), existing
benchmarks often focus on single-turn or single-step tasks, failing to capture
the kind of iterative reasoning required in real-world settings. To address
this limitation, we introduce TurnBench, a novel benchmark that evaluates
multi-turn, multi-step reasoning through an interactive code-breaking task
inspired by a "Turing Machine Board Game." In each episode, a model must
uncover hidden logical or arithmetic rules by making sequential guesses,
receiving structured feedback, and integrating clues across multiple rounds.
This dynamic setup requires models to reason over time, adapt based on past
information, and maintain consistency across steps-capabilities underexplored
in current benchmarks. TurnBench includes two modes: Classic, which tests
standard reasoning, and Nightmare, which introduces increased complexity and
requires robust inferential chains. To support fine-grained analysis, we
provide ground-truth annotations for intermediate reasoning steps. Our
evaluation of state-of-the-art LLMs reveals significant gaps: the best model
achieves 81.5% accuracy in Classic mode, but performance drops to 17.8% in
Nightmare mode. In contrast, human participants achieve 100% in both,
underscoring the challenge TurnBench poses to current models. By incorporating
feedback loops and hiding task rules, TurnBench reduces contamination risks and
provides a rigorous testbed for diagnosing and advancing multi-step, multi-turn
reasoning in LLMs.

</details>


### [167] [Follow the Flow: Fine-grained Flowchart Attribution with Neurosymbolic Agents](https://arxiv.org/abs/2506.01344)
*Manan Suri,Puneet Mathur,Nedim Lipka,Franck Dernoncourt,Ryan A. Rossi,Vivek Gupta,Dinesh Manocha*

Main category: cs.CL

TL;DR: 提出神经符号智能体FlowPathAgent，通过细粒度归因路径降低LLM流程图分析中的视觉幻觉问题，在FlowExplainBench数据集上超越基线模型10-14%


<details>
  <summary>Details</summary>
Motivation: 解决LLM在非线性流程图解释中产生虚假决策路径的问题，提升医疗/工程等关键领域流程图自动处理的可靠性

Method: 神经符号框架：流程图分割→符号图转换→基于图的动态推理生成归因路径

Result: 在包含多领域流程图的新基准FlowExplainBench上，视觉幻觉减少10-14%

Conclusion: 细粒度归因机制有效提升LLM流程图解释的可验证性，为关键领域应用提供可靠支持

Abstract: Flowcharts are a critical tool for visualizing decision-making processes.
However, their non-linear structure and complex visual-textual relationships
make it challenging to interpret them using LLMs, as vision-language models
frequently hallucinate nonexistent connections and decision paths when
analyzing these diagrams. This leads to compromised reliability for automated
flowchart processing in critical domains such as logistics, health, and
engineering. We introduce the task of Fine-grained Flowchart Attribution, which
traces specific components grounding a flowchart referring LLM response.
Flowchart Attribution ensures the verifiability of LLM predictions and improves
explainability by linking generated responses to the flowchart's structure. We
propose FlowPathAgent, a neurosymbolic agent that performs fine-grained post
hoc attribution through graph-based reasoning. It first segments the flowchart,
then converts it into a structured symbolic graph, and then employs an agentic
approach to dynamically interact with the graph, to generate attribution paths.
Additionally, we present FlowExplainBench, a novel benchmark for evaluating
flowchart attributions across diverse styles, domains, and question types.
Experimental results show that FlowPathAgent mitigates visual hallucinations in
LLM answers over flowchart QA, outperforming strong baselines by 10-14% on our
proposed FlowExplainBench dataset.

</details>


### [168] [The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning](https://arxiv.org/abs/2506.01347)
*Xinyu Zhu,Mengzhou Xia,Zhepei Wei,Wei-Lin Chen,Danqi Chen,Yu Meng*

Main category: cs.CL

TL;DR: 通过分解强化学习中的正负样本强化机制，研究发现仅使用负样本强化（NSR）在数学推理任务中展现优于传统方法的性能提升效果


<details>
  <summary>Details</summary>
Motivation: 理解RLVR（可验证奖励强化学习）在长链思维任务中的具体作用机制，探索正负样本强化对语言模型推理能力的不同影响

Method: 使用Qwen系列模型进行数学推理实验，通过梯度分析揭示NSR的工作机制，提出改进的强化学习目标函数

Result: 仅用负样本强化持续提升Pass@k性能（k达256），保持生成多样性；而仅正样本强化导致高k值性能下降

Conclusion: 负样本强化通过抑制错误输出并基于模型先验重新分配概率，提出加强NSR权重的新目标函数，在MATH等多个数学数据集验证有效

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for training language models (LMs) on reasoning tasks that elicit emergent long
chains of thought (CoTs). Unlike supervised learning, it updates the model
using both correct and incorrect samples via policy gradients. To better
understand its mechanism, we decompose the learning signal into reinforcing
correct responses and penalizing incorrect ones, referred to as Positive and
Negative Sample Reinforcement (PSR and NSR), respectively. We train
Qwen2.5-Math-7B and Qwen3-4B on a mathematical reasoning dataset and uncover a
surprising result: training with only negative samples -- without reinforcing
correct responses -- can be highly effective: it consistently improves
performance over the base model across the entire Pass@$k$ spectrum ($k$ up to
$256$), often matching or surpassing PPO and GRPO. In contrast, reinforcing
only correct responses improves Pass@$1$ but degrades performance at higher
$k$, due to reduced diversity. These inference-scaling trends highlight that
solely penalizing incorrect responses may contribute more to performance than
previously recognized. Through gradient analysis, we show that NSR works by
suppressing incorrect generations and redistributing probability mass toward
other plausible candidates, guided by the model's prior beliefs. It refines the
model's existing knowledge rather than introducing entirely new behaviors.
Building on this insight, we propose a simple variant of the RL objective that
upweights NSR, and show that it consistently improves overall Pass@$k$
performance on MATH, AIME 2025, and AMC23. Our code is available at
https://github.com/TianHongZXY/RLVR-Decomposed.

</details>


### [169] [KokoroChat: A Japanese Psychological Counseling Dialogue Dataset Collected via Role-Playing by Trained Counselors](https://arxiv.org/abs/2506.01357)
*Zhiyang Qi,Takumasa Kaneko,Keiko Takamizo,Mariko Ukiyo,Michimasa Inaba*

Main category: cs.CL

TL;DR: 提出通过角色扮演构建高质量心理咨询数据集KokoroChat，有效提升语言模型生成效果


<details>
  <summary>Details</summary>
Motivation: 现有心理咨询数据集存在隐私风险/多样性不足的问题，LLM生成数据质量有限

Method: 采用训练有素的咨询师角色扮演构建日语长对话数据集（6,589组），附带完整用户反馈

Result: 微调后的开源LLM在心理咨询响应质量和自动评估指标上均有显著提升

Conclusion: 角色扮演方法在保证质量的同时降低隐私风险，公开数据集促进相关研究发展

Abstract: Generating psychological counseling responses with language models relies
heavily on high-quality datasets. Crowdsourced data collection methods require
strict worker training, and data from real-world counseling environments may
raise privacy and ethical concerns. While recent studies have explored using
large language models (LLMs) to augment psychological counseling dialogue
datasets, the resulting data often suffers from limited diversity and
authenticity. To address these limitations, this study adopts a role-playing
approach where trained counselors simulate counselor-client interactions,
ensuring high-quality dialogues while mitigating privacy risks. Using this
method, we construct KokoroChat, a Japanese psychological counseling dialogue
dataset comprising 6,589 long-form dialogues, each accompanied by comprehensive
client feedback. Experimental results demonstrate that fine-tuning open-source
LLMs with KokoroChat improves both the quality of generated counseling
responses and the automatic evaluation of counseling dialogues. The KokoroChat
dataset is available at https://github.com/UEC-InabaLab/KokoroChat.

</details>


### [170] [MMD-Flagger: Leveraging Maximum Mean Discrepancy to Detect Hallucinations](https://arxiv.org/abs/2506.01367)
*Kensuke Mitsuzawa,Damien Garreau*

Main category: cs.CL

TL;DR: 提出MMD-Flagger方法，基于最大均值差异原理检测大语言模型的幻觉生成，通过分析生成文本与不同温度参数生成文本的分布差异实现有效识别。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成的虚构内容严重阻碍其在关键领域的应用，亟需建立可靠的幻觉检测机制以保障内容真实性。

Method: 利用非参数统计方法MMD测量生成文本与不同温度参数生成文本的分布距离，通过追踪该距离变化轨迹实现幻觉检测。

Result: 在机器翻译数据集上的实验表明，该方法性能优于现有基准方法。

Conclusion: MMD-Flagger为幻觉检测提供了新的分布对齐视角，验证了温度参数轨迹分析在内容真实性评估中的有效性。

Abstract: Large language models (LLMs) have become pervasive in our everyday life. Yet,
a fundamental obstacle prevents their use in many critical applications: their
propensity to generate fluent, human-quality content that is not grounded in
reality. The detection of such hallucinations is thus of the highest
importance. In this work, we propose a new method to flag hallucinated content,
MMD-Flagger. It relies on Maximum Mean Discrepancy (MMD), a non-parametric
distance between distributions. On a high-level perspective, MMD-Flagger tracks
the MMD between the generated documents and documents generated with various
temperature parameters. We show empirically that inspecting the shape of this
trajectory is sufficient to detect most hallucinations. This novel method is
benchmarked on two machine translation datasets, on which it outperforms
natural competitors.

</details>


### [171] [AdaRewriter: Unleashing the Power of Prompting-based Conversational Query Reformulation via Test-Time Adaptation](https://arxiv.org/abs/2506.01381)
*Yilong Lai,Jialong Wu,Zhenglin Wang,Deyu Zhou*

Main category: cs.CL

TL;DR: 提出AdaRewriter框架，通过测试时适应的奖励模型优化对话查询重写，提升效果并兼容黑盒系统


<details>
  <summary>Details</summary>
Motivation: 现有方法在训练时调整和测试时适应阶段未能充分发挥生成候选查询的优势，尤其是在黑盒系统环境下需要更有效的优化方案

Method: 训练轻量级奖励模型（对比排序损失），在推理时动态选择最佳查询重写方案，可无缝适配商业LLM API等黑盒系统

Result: 在5个对话搜索数据集上显著超越现有方法，最高提升达4.6%检索准确率

Conclusion: 验证了测试时适应策略对对话查询重写的有效性，为商业搜索系统提供了即插即用的优化方案

Abstract: Prompting-based conversational query reformulation has emerged as a powerful
approach for conversational search, refining ambiguous user queries into
standalone search queries. Best-of-N reformulation over the generated
candidates via prompting shows impressive potential scaling capability.
However, both the previous tuning methods (training time) and adaptation
approaches (test time) can not fully unleash their benefits. In this paper, we
propose AdaRewriter, a novel framework for query reformulation using an
outcome-supervised reward model via test-time adaptation. By training a
lightweight reward model with contrastive ranking loss, AdaRewriter selects the
most promising reformulation during inference. Notably, it can operate
effectively in black-box systems, including commercial LLM APIs. Experiments on
five conversational search datasets show that AdaRewriter significantly
outperforms the existing methods across most settings, demonstrating the
potential of test-time adaptation for conversational query reformulation.

</details>


### [172] [Speech-to-Speech Translation Pipelines for Conversations in Low-Resource Languages](https://arxiv.org/abs/2506.01406)
*Andrei Popescu-Belis,Alexis Allemann,Teo Ferrari,Gopal Krishnamani*

Main category: cs.CL

TL;DR: 研究评估了土耳其语-法语/普什图语-法语双向低资源语音翻译的60余种技术流程，确定最优方案并发现组件性能独立于流程组合。


<details>
  <summary>Details</summary>
Motivation: 针对低资源语言（土耳其语/普什图语）在社区口译场景下的语音翻译质量不稳定问题，填补现有研究空白。

Method: 通过本地模型与云端商业模型组合（ASR+MT+TTS），在自建数据集上微调并采用BLEU/COMET/BLASER指标结合人工评估，系统评估60余种技术流程。

Result: 确定各语言方向最优技术方案，发现组件性能评估结果在不同流程组合中具有稳定性。

Conclusion: 研究结果为低资源语言语音翻译系统优化提供路径，且组件评估独立性特征支持模块化技术开发策略。

Abstract: The popularity of automatic speech-to-speech translation for human
conversations is growing, but the quality varies significantly depending on the
language pair. In a context of community interpreting for low-resource
languages, namely Turkish and Pashto to/from French, we collected fine-tuning
and testing data, and compared systems using several automatic metrics (BLEU,
COMET, and BLASER) and human assessments. The pipelines included automatic
speech recognition, machine translation, and speech synthesis, with local
models and cloud-based commercial ones. Some components have been fine-tuned on
our data. We evaluated over 60 pipelines and determined the best one for each
direction. We also found that the ranks of components are generally independent
of the rest of the pipeline.

</details>


### [173] [Comparing LLM-generated and human-authored news text using formal syntactic theory](https://arxiv.org/abs/2506.01407)
*Olga Zamaraeva,Dan Flickinger,Francis Bond,Carlos Gómez-Rodríguez*

Main category: cs.CL

TL;DR: 首次使用HPSG句法理论系统比较人类与LLM生成文本的语法差异


<details>
  <summary>Details</summary>
Motivation: 填补LLM生成内容与人类写作在《纽约时报》体裁中系统性句法对比的研究空白

Method: 基于HPSG理论分析6个LLM生成文本与人类文本的语法类型分布

Result: 发现人类与LLM在HPSG语法类型分布上存在系统性差异

Conclusion: 为理解LLM句法特征及人类写作模式提供新的理论视角

Abstract: This study provides the first comprehensive comparison of New York
Times-style text generated by six large language models against real,
human-authored NYT writing. The comparison is based on a formal syntactic
theory. We use Head-driven Phrase Structure Grammar (HPSG) to analyze the
grammatical structure of the texts. We then investigate and illustrate the
differences in the distributions of HPSG grammar types, revealing systematic
distinctions between human and LLM-generated writing. These findings contribute
to a deeper understanding of the syntactic behavior of LLMs as well as humans,
within the NYT genre.

</details>


### [174] [UniversalCEFR: Enabling Open Multilingual Research on Language Proficiency Assessment](https://arxiv.org/abs/2506.01419)
*Joseph Marvin Imperial,Abdullah Barayan,Regina Stodden,Rodrigo Wilkens,Ricardo Munoz Sanchez,Lingyun Gao,Melissa Torgbi,Dawn Knight,Gail Forey,Reka R. Jablonkai,Ekaterina Kochmar,Robert Reynolds,Eugenio Ribeiro,Horacio Saggion,Elena Volodina,Sowmya Vajjala,Thomas Francois,Fernando Alva-Manchego,Harish Tayyar Madabushi*

Main category: cs.CL

TL;DR: 研究者构建了UniversalCEFR多语言数据集(覆盖13种语言/50万文本)，通过标准化格式支持语言能力评估研究，验证了语言特征和微调模型在CEFR等级预测中的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决语言能力评估研究中数据集格式分散、可及性差的问题，通过标准化多语言CEFR标注数据集促进开放式研究。

Method: 1) 基于语言特征的分类 2) 微调预训练大模型 3) 使用指令微调模型进行描述符提示

Result: 语言特征分析和微调预训练模型在跨语言CEFR等级评估中表现优异，而描述符提示方法效果相对有限

Conclusion: 通过统一数据格式和开放访问，UniversalCEFR为语言能力研究建立了数据分发的最佳实践，推动全球研究社区的合作发展。

Abstract: We introduce UniversalCEFR, a large-scale multilingual multidimensional
dataset of texts annotated according to the CEFR (Common European Framework of
Reference) scale in 13 languages. To enable open research in both automated
readability and language proficiency assessment, UniversalCEFR comprises
505,807 CEFR-labeled texts curated from educational and learner-oriented
resources, standardized into a unified data format to support consistent
processing, analysis, and modeling across tasks and languages. To demonstrate
its utility, we conduct benchmark experiments using three modelling paradigms:
a) linguistic feature-based classification, b) fine-tuning pre-trained LLMs,
and c) descriptor-based prompting of instruction-tuned LLMs. Our results
further support using linguistic features and fine-tuning pretrained models in
multilingual CEFR level assessment. Overall, UniversalCEFR aims to establish
best practices in data distribution in language proficiency research by
standardising dataset formats and promoting their accessibility to the global
research community.

</details>


### [175] [Self-Refining Language Model Anonymizers via Adversarial Distillation](https://arxiv.org/abs/2506.01420)
*Kyuyoung Kim,Hyunjun Jeon,Jinwoo Shin*

Main category: cs.CL

TL;DR: 提出SEAL对抗蒸馏框架，通过自优化机制训练小语言模型实现高效文本匿名化，在隐私-效用平衡上达到/超越GPT-4表现


<details>
  <summary>Details</summary>
Motivation: 现有LLM匿名化方法依赖外部专有模型（如GPT-4），存在成本高昂和敏感数据暴露风险的问题

Method: 通过LLM匿名器与推理模型的对抗交互收集数据轨迹，利用监督微调和偏好学习将能力蒸馏至小模型，实现自优化的迭代改进

Result: 8B模型达到与GPT-4相当的隐私-效用平衡，通过自优化后隐私性超越GPT-4

Conclusion: 对抗蒸馏框架能有效训练小模型成为高效匿名器，相关数据集已开源促进后续研究

Abstract: Large language models (LLMs) are increasingly used in sensitive domains,
where their ability to infer personal data from seemingly benign text poses
emerging privacy risks. While recent LLM-based anonymization methods help
mitigate such risks, they often rely on proprietary models (e.g., GPT-4),
raising concerns about cost and the potential exposure of sensitive data to
untrusted external systems. To address this, we introduce SElf-refining
Anonymization with Language model (SEAL), a novel distillation framework for
training small language models (SLMs) to perform effective anonymization
without relying on external costly models at inference time. We leverage
adversarial interactions between an LLM anonymizer and an inference model to
collect trajectories of anonymized texts and inferred attributes, which are
used to distill anonymization, adversarial inference, and utility evaluation
capabilities into SLMs via supervised fine-tuning and preference learning. The
resulting models learn to both anonymize text and critique their outputs,
enabling iterative improvement of anonymization quality via self-refinement.
Experiments on SynthPAI, a dataset of synthetic personal profiles and text
comments, demonstrate that SLMs trained with SEAL achieve substantial
improvements in anonymization capabilities. Notably, 8B models attain a
privacy-utility trade-off comparable to that of the GPT-4 anonymizer and, with
self-refinement, even surpass it in terms of privacy. These results show the
effectiveness of our adversarial distillation framework in training SLMs as
efficient anonymizers. To facilitate further research, we release the full
dataset used in our experiments.

</details>


### [176] [Redundancy, Isotropy, and Intrinsic Dimensionality of Prompt-based Text Embeddings](https://arxiv.org/abs/2506.01435)
*Hayato Tsukagoshi,Ryohei Sasano*

Main category: cs.CL

TL;DR: High-dimensional prompt-based embeddings exhibit significant redundancy, allowing extreme dimensionality reduction (up to 0.5%) with minimal performance loss in classification/clustering tasks.


<details>
  <summary>Details</summary>
Motivation: Address storage and computational inefficiencies caused by high-dimensional embeddings (thousands of dimensions) in prompt-based text embedding models.

Method: Conducted dimensionality reduction experiments on embeddings, analyzed intrinsic dimensionality and isotropy patterns across tasks.

Result: Classification/clustering showed <0.5% dimensionality retention maintained performance, while retrieval/STS required 25% retention. Lower intrinsic dimensionality and isotropy observed in high-redundancy tasks.

Conclusion: Task-specific embedding redundancy correlates with intrinsic properties - classification/clustering embeddings are more compressible due to lower intrinsic dimensionality and reduced isotropy.

Abstract: Prompt-based text embedding models, which generate task-specific embeddings
upon receiving tailored prompts, have recently demonstrated remarkable
performance. However, their resulting embeddings often have thousands of
dimensions, leading to high storage costs and increased computational costs of
embedding-based operations. In this paper, we investigate how post-hoc
dimensionality reduction applied to the embeddings affects the performance of
various tasks that leverage these embeddings, specifically classification,
clustering, retrieval, and semantic textual similarity (STS) tasks. Our
experiments show that even a naive dimensionality reduction, which keeps only
the first 25% of the dimensions of the embeddings, results in a very slight
performance degradation, indicating that these embeddings are highly redundant.
Notably, for classification and clustering, even when embeddings are reduced to
less than 0.5% of the original dimensionality the performance degradation is
very small. To quantitatively analyze this redundancy, we perform an analysis
based on the intrinsic dimensionality and isotropy of the embeddings. Our
analysis reveals that embeddings for classification and clustering, which are
considered to have very high dimensional redundancy, exhibit lower intrinsic
dimensionality and less isotropy compared with those for retrieval and STS.

</details>


### [177] [Whale: Large-Scale multilingual ASR model with w2v-BERT and E-Branchformer with large speech data](https://arxiv.org/abs/2506.01439)
*Yosuke Kashiwagi,Hayato Futami,Emiru Tsunoo,Satoshi Asakawa*

Main category: cs.CL

TL;DR: Whale语音识别模型通过融合w2v-BERT自监督架构、E-Branchformer编解码器及CTC-attention联合解码策略，在多样化数据集训练下实现SOTA性能，Librispeech测试集WER 2.4%，显著优于Whisper等竞品。


<details>
  <summary>Details</summary>
Motivation: 针对现有大规模语音模型（如Whisper/OWSM）的优化空间，通过架构创新与数据多样性增强模型鲁棒性，提升多场景语音识别准确率。

Method: 1. 模型架构：w2v-BERT自监督模块 + E-Branchformer编解码器
2. 解码策略：CTC-attention联合解码
3. 训练数据：整合公开语料库与内部数据，覆盖多样化发音风格和声学环境

Result: Librispeech test-clean词错误率2.4%，CSJ eval3字错误率3.4%，均超越Whisper large-v3（2.5%/3.8%）和OWSM v3.1。在多个基准测试中达到可比肩现有最优模型的表现。

Conclusion: Whale通过架构优化与数据多样性策略，验证了大规模语音模型的性能边界。该方法为复杂声学场景下的鲁棒语音识别提供了有效解决方案，具有重要工程应用价值。

Abstract: This paper reports on the development of a large-scale speech recognition
model, Whale. Similar to models such as Whisper and OWSM, Whale leverages both
a large model size and a diverse, extensive dataset. Whale's architecture
integrates w2v-BERT self-supervised model, an encoder-decoder backbone built on
E-Branchformer, and a joint CTC-attention decoding strategy. The training
corpus comprises varied speech data, of not only public corpora but also
in-house data, thereby enhancing the model's robustness to different speaking
styles and acoustic conditions. Through evaluations on multiple benchmarks,
Whale achieved comparable performance to existing models. In particular, it
achieves a word error rate of 2.4% on the Librispeech test-clean set and a
character error rate of 3.4% on the CSJ eval3 set, outperforming Whisper
large-v3 and OWSM v3.1.

</details>


### [178] [Building Entity Association Mining Framework for Knowledge Discovery](https://arxiv.org/abs/2506.01451)
*Anshika Rawal,Abhijeet Kumar,Mridul Mishra*

Main category: cs.CL

TL;DR: 提出了一个支持文档过滤、可配置实体提取和关联挖掘的通用文本挖掘框架，可快速构建商业用例并量化排名指标


<details>
  <summary>Details</summary>
Motivation: 非结构化文本中提取有效信号支持商业决策存在挑战，现有方法缺乏统一的关联挖掘框架导致重复开发效率低

Method: 框架包含三大组件：1) 海量文本的文档过滤 2) 支持DBpedia/Spacy/自定义匹配等多技术融合的实体提取管道 3) 基于共现图的关联关系挖掘

Result: 成功应用于金融领域的品牌产品发现和供应商风险监控用例，验证了框架的有效性

Conclusion: 该框架可显著减少重复开发工作，提升关联挖掘类业务的复用性和快速原型构建能力

Abstract: Extracting useful signals or pattern to support important business decisions
for example analyzing investment product traction and discovering customer
preference, risk monitoring etc. from unstructured text is a challenging task.
Capturing interaction of entities or concepts and association mining is a
crucial component in text mining, enabling information extraction and reasoning
over and knowledge discovery from text. Furthermore, it can be used to enrich
or filter knowledge graphs to guide exploration processes, descriptive
analytics and uncover hidden stories in the text. In this paper, we introduce a
domain independent pipeline i.e., generalized framework to enable document
filtering, entity extraction using various sources (or techniques) as plug-ins
and association mining to build any text mining business use-case and
quantitatively define a scoring metric for ranking purpose. The proposed
framework has three major components a) Document filtering: filtering
documents/text of interest from massive amount of texts b) Configurable entity
extraction pipeline: include entity extraction techniques i.e., i) DBpedia
Spotlight, ii) Spacy NER, iii) Custom Entity Matcher, iv) Phrase extraction (or
dictionary) based c) Association Relationship Mining: To generates
co-occurrence graph to analyse potential relationships among entities,
concepts. Further, co-occurrence count based frequency statistics provide a
holistic window to observe association trends or buzz rate in specific business
context. The paper demonstrates the usage of framework as fundamental building
box in two financial use-cases namely brand product discovery and vendor risk
monitoring. We aim that such framework will remove duplicated effort, minimize
the development effort, and encourage reusability and rapid prototyping in
association mining business applications for institutions.

</details>


### [179] [TalTech Systems for the Interspeech 2025 ML-SUPERB 2.0 Challenge](https://arxiv.org/abs/2506.01458)
*Tanel Alumäe,Artem Fedorchenko*

Main category: cs.CL

TL;DR: 介绍一个结合混合语言识别和多模型语音识别的系统，在ML-SUPERB 2.0挑战赛中取得最佳成绩


<details>
  <summary>Details</summary>
Motivation: 针对多语言场景中数据可用性和模型性能的差异，开发高效的语言识别与语音识别集成方案

Method: 语言识别采用预训练嵌入+共享编码器架构，语音识别灵活组合SeamlessM4T微调版、带适配器的MMS-1B-all和MMS-zeroshot三个模型

Result: 系统在Interspeech 2025 ML-SUPERB 2.0挑战赛中获得综合最高分

Conclusion: 混合架构与模块化模型选择策略有效提升了多语言场景下的识别性能，为资源受限语言处理提供新思路

Abstract: This paper describes the language identification and multilingual speech
recognition system developed at Tallinn University of Technology for the
Interspeech 2025 ML-SUPERB 2.0 Challenge. A hybrid language identification
system is used, consisting of a pretrained language embedding model and a
light-weight speech recognition model with a shared encoder across languages
and language-specific bigram language models. For speech recognition, three
models are used, where only a single model is applied for each language,
depending on the training data availability and performance on held-out data.
The model set consists of a finetuned version of SeamlessM4T, MMS-1B-all with
custom language adapters and MMS-zeroshot. The system obtained the top overall
score in the challenge.

</details>


### [180] [Integrating Neural and Symbolic Components in a Model of Pragmatic Question-Answering](https://arxiv.org/abs/2506.01474)
*Polina Tsvilodub,Robert D. Hawkins,Michael Franke*

Main category: cs.CL

TL;DR: 提出神经符号框架，通过整合LLM模块自动生成语用语言模型的关键组件，替代传统人工指定方法。混合模型在预测人类回答模式上表现相当或更优，但效果取决于LLM整合方式（生成备选方案有效，语义评估存在挑战）。


<details>
  <summary>Details</summary>
Motivation: 传统语用语言模型依赖人工指定的语句和含义，限制了实际应用。需要更灵活、可扩展的模型框架，利用LLM自动生成和评估组件。

Method: 构建神经符号框架：1）用LLM生成备选语句/目标/效用函数 2）在经典语用问答案例中系统测试不同整合方式（评估效用、字面语义、生成语句等）3）对比混合模型与传统概率模型性能。

Result: 混合模型预测人类回答模式的表现匹配或超越传统模型。LLM整合方式关键影响：生成备选语句/目标转化效果突出，但真值条件语义评估存在困难。

Conclusion: 神经符号框架为语用模型提供更灵活的发展路径，但需谨慎平衡神经与符号组件：LLM适合生成性任务，语义评估仍需改进。该研究为模型设计提供关键权衡依据。

Abstract: Computational models of pragmatic language use have traditionally relied on
hand-specified sets of utterances and meanings, limiting their applicability to
real-world language use. We propose a neuro-symbolic framework that enhances
probabilistic cognitive models by integrating LLM-based modules to propose and
evaluate key components in natural language, eliminating the need for manual
specification. Through a classic case study of pragmatic question-answering, we
systematically examine various approaches to incorporating neural modules into
the cognitive model -- from evaluating utilities and literal semantics to
generating alternative utterances and goals. We find that hybrid models can
match or exceed the performance of traditional probabilistic models in
predicting human answer patterns. However, the success of the neuro-symbolic
model depends critically on how LLMs are integrated: while they are
particularly effective for proposing alternatives and transforming abstract
goals into utilities, they face challenges with truth-conditional semantic
evaluation. This work charts a path toward more flexible and scalable models of
pragmatic language use while illuminating crucial design considerations for
balancing neural and symbolic components.

</details>


### [181] [LLM in the Loop: Creating the PARADEHATE Dataset for Hate Speech Detoxification](https://arxiv.org/abs/2506.01484)
*Shuzhou Yuan,Ercong Nie,Lukas Kouba,Ashish Yashwanth Kangen,Helmut Schmid,Hinrich Schutze,Michael Farber*

Main category: cs.CL

TL;DR: 利用GPT-4o-mini构建大规模仇恨言论净化数据集PARADEHATE，验证LLM生成数据可替代人工标注


<details>
  <summary>Details</summary>
Motivation: 现有净化任务平行数据集稀缺，尤其仇恨言论领域因标注成本高和敏感性导致数据不足

Method: 开发LLM-in-the-loop流程，用GPT-4o-mini替代人工标注，复现ParaDetox流程并构建8K对仇恨/非仇恨文本数据集

Result: 基于PARADEHATE微调的BART模型在风格准确性（89.2%）、内容保留度（4.15/5）和流畅度（4.32/5）上超越传统方法

Conclusion: LLM生成的净化文本可作为可扩展的标注替代方案，公开的PARADEHATE数据集为仇恨言论净化提供新基准

Abstract: Detoxification, the task of rewriting harmful language into non-toxic text,
has become increasingly important amid the growing prevalence of toxic content
online. However, high-quality parallel datasets for detoxification, especially
for hate speech, remain scarce due to the cost and sensitivity of human
annotation. In this paper, we propose a novel LLM-in-the-loop pipeline
leveraging GPT-4o-mini for automated detoxification. We first replicate the
ParaDetox pipeline by replacing human annotators with an LLM and show that the
LLM performs comparably to human annotation. Building on this, we construct
PARADEHATE, a large-scale parallel dataset specifically for hatespeech
detoxification. We release PARADEHATE as a benchmark of over 8K hate/non-hate
text pairs and evaluate a wide range of baseline methods. Experimental results
show that models such as BART, fine-tuned on PARADEHATE, achieve better
performance in style accuracy, content preservation, and fluency, demonstrating
the effectiveness of LLM-generated detoxification text as a scalable
alternative to human annotation.

</details>


### [182] [Argument-Centric Causal Intervention Method for Mitigating Bias in Cross-Document Event Coreference Resolution](https://arxiv.org/abs/2506.01488)
*Long Yao,Wenzhong Yang,Yabo Yin,Fuyuan Wei,Hongzhen Lv,Jiaren Peng,Liejun Wang,Xiaoming Tao*

Main category: cs.CL

TL;DR: 提出基于论点中心因果干预（ACCI）的跨文档事件共指消解方法，通过因果干预和反事实推理有效消除触发词与共指标签的虚假关联


<details>
  <summary>Details</summary>
Motivation: 现有跨文档事件共指消解方法过度依赖触发词特征，导致词汇表面特征与核心指代关系形成虚假相关性，影响模型性能

Method: 1. 构建结构因果图揭示混杂依赖 2. 后门调整干预分离论点语义因果效应 3. 反事实推理量化触发词扰动影响 4. 论点感知增强模块提升语义敏感性

Result: 在ECB+和GVC数据集分别达到88.4%和85.2%的CoNLL F1值，实现state-of-the-art性能

Conclusion: ACCI无需数据增强或启发式过滤，在端到端框架中实现有效去偏，为事件共指消解提供新的因果推理范式

Abstract: Cross-document Event Coreference Resolution (CD-ECR) is a fundamental task in
natural language processing (NLP) that seeks to determine whether event
mentions across multiple documents refer to the same real-world occurrence.
However, current CD-ECR approaches predominantly rely on trigger features
within input mention pairs, which induce spurious correlations between
surface-level lexical features and coreference relationships, impairing the
overall performance of the models. To address this issue, we propose a novel
cross-document event coreference resolution method based on Argument-Centric
Causal Intervention (ACCI). Specifically, we construct a structural causal
graph to uncover confounding dependencies between lexical triggers and
coreference labels, and introduce backdoor-adjusted interventions to isolate
the true causal effect of argument semantics. To further mitigate spurious
correlations, ACCI integrates a counterfactual reasoning module that quantifies
the causal influence of trigger word perturbations, and an argument-aware
enhancement module to promote greater sensitivity to semantically grounded
information. In contrast to prior methods that depend on costly data
augmentation or heuristic-based filtering, ACCI enables effective debiasing in
a unified end-to-end framework without altering the underlying training
procedure. Extensive experiments demonstrate that ACCI achieves CoNLL F1 of
88.4% on ECB+ and 85.2% on GVC, achieving state-of-the-art performance. The
implementation and materials are available at https://github.com/era211/ACCI.

</details>


### [183] [Multilingual Definition Modeling](https://arxiv.org/abs/2506.01489)
*Edison Marrese-Taylor,Erica K. Shimomoto,Alfredo Solano,Enrique Reid*

Main category: cs.CL

TL;DR: 多语言定义建模研究：预训练模型与LLM在四种语言中的表现对比，验证任务作为轻量级多语言评估基准的可行性。


<details>
  <summary>Details</summary>
Motivation: 将定义建模任务扩展至西班牙语、法语、葡萄牙语和德语，探究多语言模型在跨语言场景下的潜力与局限性。

Method: 1. 使用单语词典数据微调多语言模型
2. 对ChatGPT等LLM进行零样本测试
3. 通过BERTScore关联多语言基准测试表现

Result: 多语言模型与英语模型表现持平但未实现跨语言协同，LLM整体更优；人类评估显示LLM具备零/少样本能力但存在缺陷；任务表现与多语言基准强相关

Conclusion: 定义建模任务可作为计算受限场景下稳定、自然的多语言模型评估替代方案

Abstract: In this paper, we propose the first multilingual study on definition
modeling. We use monolingual dictionary data for four new languages (Spanish,
French, Portuguese, and German) and perform an in-depth empirical study to test
the performance of pre-trained multilingual language models on definition
modeling of monosemic words when finetuned on this data. Furthermore, we use a
zero-shot approach to test the multilingual capabilities of two popular
chat-based Large Language Models (LLMs) in the task. Results show that
multilingual language models can perform on-pair with English but cannot
leverage potential cross-lingual synergies, with LLMs generally offering better
performance overall. A comprehensive human evaluation of the LLM-generated
definition highlights the zero and few-shot capabilities of these models in
this new task, also showing their shortcomings. Finally, we show that
performance on our task via BERTScore strongly correlates to the performance on
multilingual LLM benchmarks, suggesting that our task offers a viable
compute-constrained, stable and natural alternative to these.

</details>


### [184] [CVC: A Large-Scale Chinese Value Rule Corpus for Value Alignment of Large Language Models](https://arxiv.org/abs/2506.01495)
*Ping Wu,Guobin Shen,Dongcheng Zhao,Yuwei Wang,Yiting Dong,Yu Shi,Enmeng Lu,Feifei Zhao,Yi Zeng*

Main category: cs.CL

TL;DR: 构建基于中国核心价值观的分层框架CVC，解决LLM价值观对齐中的文化偏见与评估成本问题


<details>
  <summary>Details</summary>
Motivation: 现有价值观评估存在西方文化偏见，国内框架不完善且缺乏可扩展的生成方法

Method: 提出三级价值框架（3维度/12核心/50衍生价值），构建25万+规则增强的CVC语料库

Result: CVC场景在价值边界/多样性优于直接生成，70.5%模型选择与87.5%人工标注对齐

Conclusion: 建立了文化自适应的价值观评估基准框架，公开数据集与代码支持中国特色AI发展

Abstract: Ensuring that Large Language Models (LLMs) align with mainstream human values
and ethical norms is crucial for the safe and sustainable development of AI.
Current value evaluation and alignment are constrained by Western cultural bias
and incomplete domestic frameworks reliant on non-native rules; furthermore,
the lack of scalable, rule-driven scenario generation methods makes evaluations
costly and inadequate across diverse cultural contexts. To address these
challenges, we propose a hierarchical value framework grounded in core Chinese
values, encompassing three main dimensions, 12 core values, and 50 derived
values. Based on this framework, we construct a large-scale Chinese Values
Corpus (CVC) containing over 250,000 value rules enhanced and expanded through
human annotation. Experimental results show that CVC-guided scenarios
outperform direct generation ones in value boundaries and content diversity. In
the evaluation across six sensitive themes (e.g., surrogacy, suicide), seven
mainstream LLMs preferred CVC-generated options in over 70.5% of cases, while
five Chinese human annotators showed an 87.5% alignment with CVC, confirming
its universality, cultural relevance, and strong alignment with Chinese values.
Additionally, we construct 400,000 rule-based moral dilemma scenarios that
objectively capture nuanced distinctions in conflicting value prioritization
across 17 LLMs. Our work establishes a culturally-adaptive benchmarking
framework for comprehensive value evaluation and alignment, representing
Chinese characteristics. All data are available at
https://huggingface.co/datasets/Beijing-AISI/CVC, and the code is available at
https://github.com/Beijing-AISI/CVC.

</details>


### [185] [Continual Speech Learning with Fused Speech Features](https://arxiv.org/abs/2506.01496)
*Guitao Wang,Jinming Zhao,Hao Yang,Guilin Qi,Tongtong Wu,Gholamreza Haffari*

Main category: cs.CL

TL;DR: 提出连续语音学习框架，通过可学习门控融合层提升语音模型动态适应能力


<details>
  <summary>Details</summary>
Motivation: 传统静态语音模型难以适应动态变化的语音数据，需解决模型与新任务间的适配鸿沟

Method: 基于Whisper编码器-解码器架构，在编码器顶部集成可学习门控融合层实现特征动态选择

Result: 在6项语音处理任务中准确率显著超越传统方法，无需完整重训练即可适应新任务

Conclusion: 该框架有效解决了语音模型持续适应问题，为动态语音场景提供了轻量化解决方案

Abstract: Rapid growth in speech data demands adaptive models, as traditional static
methods fail to keep pace with dynamic and diverse speech information. We
introduce continuous speech learning, a new set-up targeting at bridging the
adaptation gap in current speech models. We use the encoder-decoder Whisper
model to standardize speech tasks into a generative format. We integrate a
learnable gated-fusion layer on the top of the encoder to dynamically select
task-specific features for downstream tasks. Our approach improves accuracy
significantly over traditional methods in six speech processing tasks,
demonstrating gains in adapting to new speech tasks without full retraining.

</details>


### [186] [Representations of Fact, Fiction and Forecast in Large Language Models: Epistemics and Attitudes](https://arxiv.org/abs/2506.01512)
*Meng Li,Michael Vrazitulis,David Schlangen*

Main category: cs.CL

TL;DR: 大型语言模型在生成认知模态表达时表现有限且不稳定，需增强其语义知识以提高可靠性。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs对认知模态的知识，确定其生成不确定性表达的可靠性。

Method: 利用类型学框架和控制故事测试LLMs生成认知表达的能力。

Result: LLMs生成认知表达的性能有限且不稳健，其不确定性表达不可靠。

Conclusion: 需丰富LLMs中认知模态的语义知识以构建不确定性感知模型。

Abstract: Rational speakers are supposed to know what they know and what they do not
know, and to generate expressions matching the strength of evidence. In
contrast, it is still a challenge for current large language models to generate
corresponding utterances based on the assessment of facts and confidence in an
uncertain real-world environment. While it has recently become popular to
estimate and calibrate confidence of LLMs with verbalized uncertainty, what is
lacking is a careful examination of the linguistic knowledge of uncertainty
encoded in the latent space of LLMs. In this paper, we draw on typological
frameworks of epistemic expressions to evaluate LLMs' knowledge of epistemic
modality, using controlled stories. Our experiments show that the performance
of LLMs in generating epistemic expressions is limited and not robust, and
hence the expressions of uncertainty generated by LLMs are not always reliable.
To build uncertainty-aware LLMs, it is necessary to enrich semantic knowledge
of epistemic modality in LLMs.

</details>


### [187] [FormFactory: An Interactive Benchmarking Suite for Multimodal Form-Filling Agents](https://arxiv.org/abs/2506.01520)
*Bobo Li,Yuheng Wang,Hao Fei,Juncheng Li,Wei Ji,Mong-Li Lee,Wynne Hsu*

Main category: cs.CL

TL;DR: 提出FormFactory基准测试套件，揭示当前多模态大模型在表单填写任务中准确率不足5%，暴露布局推理和字段对齐的短板


<details>
  <summary>Details</summary>
Motivation: 在线表单填写作为高频人机交互任务长期依赖规则系统，现有生成式模型难以应对灵活布局和字段-文本对齐的挑战

Method: 构建包含网页接口、评估模块和真实场景数据集的交互式基准测试平台，支持多格式字段和高保真交互模拟

Result: 主流MLLMs准确率均未超过5%，验证任务复杂性的同时揭示模型视觉布局理解和字段值匹配能力缺陷

Conclusion: 该基准为开发鲁棒的表单填写智能体奠定基础，推动多模态模型在复杂GUI任务中的实用性研究

Abstract: Online form filling is a common yet labor-intensive task involving extensive
keyboard and mouse interactions. Despite the long-standing vision of automating
this process with "one click", existing tools remain largely rule-based and
lack generalizable, generative capabilities. Recent advances in Multimodal
Large Language Models (MLLMs) have enabled promising agents for GUI-related
tasks in general-purpose scenarios. However, they struggle with the unique
challenges of form filling, such as flexible layouts and the difficulty of
aligning textual instructions with on-screen fields. To bridge this gap, we
formally define the form-filling task and propose FormFactory, an interactive
benchmarking suite comprising a web-based interface, backend evaluation module,
and carefully constructed dataset. Our benchmark covers diverse real-world
scenarios, incorporates various field formats, and simulates high-fidelity form
interactions. We conduct a comprehensive evaluation of state-of-the-art MLLMs
and observe that no model surpasses 5% accuracy, underscoring the inherent
difficulty of the task. These findings also reveal significant limitations in
current models' visual layout reasoning and field-value alignment abilities. We
hope our benchmark can serve as a stepping stone for further research into
robust, practical form-filling agents.

</details>


### [188] [V-VAE: A Variational Auto Encoding Framework Towards Fine-Grained Control over Human-Like Chat](https://arxiv.org/abs/2506.01524)
*Qi Lin,Weikai Xu,Lisi Chen,Bin Dai*

Main category: cs.CL

TL;DR: 提出基于变分自编码的V-VAE框架，通过细粒度控制空间动态生成更符合人类特征的对话响应


<details>
  <summary>Details</summary>
Motivation: 现有角色扮演方法依赖静态描述和低质量合成数据，难以捕捉情感语调、情境意识等动态细粒度特征

Method: 采用变分自编码模块构建可解释的潜在变量空间（说话风格/交互模式/个人属性），并建立高质量HumanChatData数据集及评估基准

Result: 在HumanChatBench和DialogBench基准测试中，基于V-VAE的模型全面超越基线模型

Conclusion: V-VAE框架结合高质量数据，有效提升了LLM在拟人化对话中的动态行为适配能力

Abstract: With the continued proliferation of Large Language Model (LLM) based
chatbots, there is a growing demand for generating responses that are not only
linguistically fluent but also consistently aligned with persona-specific
traits in conversations. However, existing role-play and persona-based chat
approaches rely heavily on static role descriptions, coarse-grained signal
space, and low-quality synthetic data, which fail to capture dynamic
fine-grained details in human-like chat. Human-like chat requires modeling
subtle latent traits, such as emotional tone, situational awareness, and
evolving personality, which are difficult to predefine and cannot be easily
learned from synthetic or distillation-based data. To address these
limitations, we propose a Verbal Variational Auto-Encoding (V-VAE) framework,
containing a variational auto-encoding module and fine-grained control space
which dynamically adapts dialogue behaviour based on fine-grained,
interpretable latent variables across talking style, interaction patterns, and
personal attributes. We also construct a high-quality dataset, HumanChatData,
and benchmark HumanChatBench to address the scarcity of high-quality data in
the human-like domain. Experiments show that LLMs based on V-VAE consistently
outperform standard baselines on HumanChatBench and DialogBench, which further
demonstrates the effectiveness of V-VAE and HumanChatData.

</details>


### [189] [STORM-BORN: A Challenging Mathematical Derivations Dataset Curated via a Human-in-the-Loop Multi-Agent Framework](https://arxiv.org/abs/2506.01531)
*Wenhao Liu,Zhenyi Lu,Xinyu Hu,Jierui Zhang,Dailin Li,Jiacheng Cen,Huilin Cao,Haiteng Wang,Yuhan Li,Kun Xie,Dandan Li,Pei Zhang,Chengbo Zhang,Yuxiang Ren,Xiaohong Huang,Yan Ma*

Main category: cs.CL

TL;DR: 通过多智能体协作生成高难度数学数据集STORM-BORN，显著提升LLM推理能力


<details>
  <summary>Details</summary>
Motivation: 现有数学数据集存在内容过时、挑战性不足、忽视人类推理模式以及单LLM生成可靠性有限三大缺陷

Method: 人工介入循环+多智能体协作框架（含推理密集过滤器、数学家评估机制），生成含人类启发线索的数学推导数据

Result: GPT-o1解决率<5%，微调后LLaMA3-8B/Qwen2.5-7B分别提升7.84%和9.12%准确率

Conclusion: STORM-BORN既可作为AI数学推理的高难度基准，也为人类思维模式训练提供稀缺资源，推动AI接近数学家水平

Abstract: High-quality math datasets are crucial for advancing the reasoning abilities
of large language models (LLMs). However, existing datasets often suffer from
three key issues: outdated and insufficient challenging content, neglecting
human-like reasoning, and limited reliability due to single-LLM generation. To
address these, we introduce $\textbf{STORM-BORN}$, an ultra-challenging dataset
of mathematical derivations sourced from cutting-edge academic papers, which
includes dense human-like approximations and heuristic cues. To ensure the
reliability and quality, we propose a novel human-in-the-loop, multi-agent data
generation framework, integrating reasoning-dense filters, multi-agent
collaboration, and human mathematicians' evaluations. We curated a set of 2,000
synthetic samples and deliberately selected the 100 most difficult problems.
Even most advanced models like GPT-o1 solved fewer than $5\%$ of them.
Fine-tuning on STORM-BORN boosts accuracy by $7.84\%$ (LLaMA3-8B) and $9.12\%$
(Qwen2.5-7B). As AI approaches mathematician-level reasoning, STORM-BORN
provides both a high-difficulty benchmark and a human-like reasoning training
resource. Our code and dataset are publicly available at
https://github.com/lwhere/STORM-BORN.

</details>


### [190] [Dictionaries to the Rescue: Cross-Lingual Vocabulary Transfer for Low-Resource Languages Using Bilingual Dictionaries](https://arxiv.org/abs/2506.01535)
*Haruki Sakajo,Yusuke Ide,Justin Vasselli,Yusuke Sakai,Yingtao Tian,Hidetaka Kamigaito,Taro Watanabe*

Main category: cs.CL

TL;DR: 提出基于双语词典的BPE分词器词汇迁移方法，通过迭代删除子词实现低资源语言跨语言迁移，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于单语/平行语料的方法在低资源语言上效果有限，而双语词典资源更易获取。利用描述性语言学家构建的双语词典实现跨语言迁移。

Method: 利用BPE分词器的子词回退特性：删除词汇表中的子词会触发更短子词的回退机制，通过迭代删除目标子词并估计其嵌入向量。

Result: 在低资源语言实验中，该方法超越现有方法，验证了词典驱动方法的有效性。

Conclusion: 证明基于词典的跨语言词汇迁移策略在资源稀缺场景下的可行性，为低资源语言适配预训练模型提供新思路。

Abstract: Cross-lingual vocabulary transfer plays a promising role in adapting
pre-trained language models to new languages, including low-resource languages.
Existing approaches that utilize monolingual or parallel corpora face
challenges when applied to languages with limited resources. In this work, we
propose a simple yet effective vocabulary transfer method that utilizes
bilingual dictionaries, which are available for many languages, thanks to
descriptive linguists. Our proposed method leverages a property of BPE
tokenizers where removing a subword from the vocabulary causes a fallback to
shorter subwords. The embeddings of target subwords are estimated iteratively
by progressively removing them from the tokenizer. The experimental results
show that our approach outperforms existing methods for low-resource languages,
demonstrating the effectiveness of a dictionary-based approach for
cross-lingual vocabulary transfer.

</details>


### [191] [Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural Understanding and Transcreation](https://arxiv.org/abs/2506.01565)
*Li Zhou,Lutong Yu,Dongchu Xie,Shaohuan Cheng,Wenyan Li,Haizhou Li*

Main category: cs.CL

TL;DR: 提出多模态基准数据集Hanfu-Bench，聚焦服饰文化的时空维度理解与创新转化，揭示现有视觉语言模型在时间维度文化认知与创意适应方面的不足


<details>
  <summary>Details</summary>
Motivation: 现有文化理解研究过度关注地理多样性而忽视时间维度，传统汉服作为贯穿中国朝代更迭的文化符号，兼具历史深度与现代流行特征，是研究时空文化演变的理想载体

Method: 构建专家标注数据集，包含文化视觉理解（多选视觉问答）和文化图像转创（传统服饰现代化设计）双任务，前者评估时空特征识别能力，后者测试文化元素继承与现代化适应

Result: 闭源模型视觉理解接近普通人类（差距10%于专家），开源模型表现更弱；转创任务最佳模型成功率仅42%，多维度人工评估显示显著技术瓶颈

Conclusion: Hanfu-Bench揭示了当前模型在时空文化理解与创新转化方面的重大挑战，为文化计算领域提供关键测试平台，强调时间维度文化认知研究的重要性

Abstract: Culture is a rich and dynamic domain that evolves across both geography and
time. However, existing studies on cultural understanding with vision-language
models (VLMs) primarily emphasize geographic diversity, often overlooking the
critical temporal dimensions. To bridge this gap, we introduce Hanfu-Bench, a
novel, expert-curated multimodal dataset. Hanfu, a traditional garment spanning
ancient Chinese dynasties, serves as a representative cultural heritage that
reflects the profound temporal aspects of Chinese culture while remaining
highly popular in Chinese contemporary society. Hanfu-Bench comprises two core
tasks: cultural visual understanding and cultural image transcreation.The
former task examines temporal-cultural feature recognition based on single- or
multi-image inputs through multiple-choice visual question answering, while the
latter focuses on transforming traditional attire into modern designs through
cultural element inheritance and modern context adaptation. Our evaluation
shows that closed VLMs perform comparably to non-experts on visual cutural
understanding but fall short by 10\% to human experts, while open VLMs lags
further behind non-experts. For the transcreation task, multi-faceted human
evaluation indicates that the best-performing model achieves a success rate of
only 42\%. Our benchmark provides an essential testbed, revealing significant
challenges in this new direction of temporal cultural understanding and
creative adaptation.

</details>


### [192] [Prompt Engineering Large Language Models' Forecasting Capabilities](https://arxiv.org/abs/2506.01578)
*Philipp Schoenegger,Cameron R. Jones,Philip E. Tetlock,Barbara Mellers*

Main category: cs.CL

TL;DR: 研究表明基础提示工程对提升AI预测准确性效果有限，部分复杂策略甚至降低准确性


<details>
  <summary>Details</summary>
Motivation: 验证在复杂预测任务中，低成本提示工程是否足以替代耗时费力的传统优化方法

Method: 通过两个实验（38种提示策略测试+复合提示/外部提示引入）对比多模型预测表现

Result: 多数提示改进收效甚微，基础比率提示略有帮助，贝叶斯推理策略显著降低准确性

Conclusion: 复杂预测任务需更专业的技术突破，单纯提示优化难以实现实质性性能提升

Abstract: Large language model performance can be improved in a large number of ways.
Many such techniques, like fine-tuning or advanced tool usage, are
time-intensive and expensive. Although prompt engineering is significantly
cheaper and often works for simpler tasks, it remains unclear whether prompt
engineering suffices for more complex domains like forecasting. Here we show
that small prompt modifications rarely boost forecasting accuracy beyond a
minimal baseline. In our first study, we tested 38 prompts across Claude 3.5
Sonnet, Claude 3.5 Haiku, GPT-4o, and Llama 3.1 405B. In our second, we
introduced compound prompts and prompts from external sources, also including
the reasoning models o1 and o1-mini. Our results show that most prompts lead to
negligible gains, although references to base rates yield slight benefits.
Surprisingly, some strategies showed strong negative effects on accuracy:
especially encouraging the model to engage in Bayesian reasoning. These results
suggest that, in the context of complex tasks like forecasting, basic prompt
refinements alone offer limited gains, implying that more robust or specialized
techniques may be required for substantial performance improvements in AI
forecasting.

</details>


### [193] [Unified Large Language Models for Misinformation Detection in Low-Resource Linguistic Settings](https://arxiv.org/abs/2506.01587)
*Muhammad Islam,Javed Ali Khan,Mohammed Abaker,Ali Daud,Azeem Irshad*

Main category: cs.CL

TL;DR: 提出首个公开可验证的乌尔都语假新闻检测基准数据集，并构建统一大语言模型提升检测性能


<details>
  <summary>Details</summary>
Motivation: 乌尔都语等资源匮乏语言缺乏可靠的人工验证数据集，现有数据集存在领域限制、翻译不可靠等问题

Method: 创建公开可用的乌尔都语假新闻数据集，评估XLNet/mBERT/XLM-RoBERTa等预训练模型，提出统一融合模型架构

Result: 提出的统一模型在准确率、F1值等指标上优于基线模型，包含人类判断验证机制

Conclusion: 可靠数据集与统一模型的结合有效提升了低资源语言的假新闻检测，为其他语种研究提供方法论参考

Abstract: The rapid expansion of social media platforms has significantly increased the
dissemination of forged content and misinformation, making the detection of
fake news a critical area of research. Although fact-checking efforts
predominantly focus on English-language news, there is a noticeable gap in
resources and strategies to detect news in regional languages, such as Urdu.
Advanced Fake News Detection (FND) techniques rely heavily on large, accurately
labeled datasets. However, FND in under-resourced languages like Urdu faces
substantial challenges due to the scarcity of extensive corpora and the lack of
validated lexical resources. Current Urdu fake news datasets are often
domain-specific and inaccessible to the public. They also lack human
verification, relying mainly on unverified English-to-Urdu translations, which
compromises their reliability in practical applications. This study highlights
the necessity of developing reliable, expert-verified, and domain-independent
Urdu-enhanced FND datasets to improve fake news detection in Urdu and other
resource-constrained languages. This paper presents the first benchmark large
FND dataset for Urdu news, which is publicly available for validation and deep
analysis. We also evaluate this dataset using multiple state-of-the-art
pre-trained large language models (LLMs), such as XLNet, mBERT, XLM-RoBERTa,
RoBERTa, DistilBERT, and DeBERTa. Additionally, we propose a unified LLM model
that outperforms the others with different embedding and feature extraction
techniques. The performance of these models is compared based on accuracy, F1
score, precision, recall, and human judgment for vetting the sample results of
news.

</details>


### [194] [Statement-Tuning Enables Efficient Cross-lingual Generalization in Encoder-only Models](https://arxiv.org/abs/2506.01592)
*Ahmed Elshabrawy,Thanh-Nhi Nguyen,Yeeun Kang,Lihan Feng,Annant Jain,Faadil Abdullah Shaikh,Jonibek Mansurov,Mohamed Fazli Mohamed Imam,Jesus-German Ortiz-Barajas,Rendi Chevi,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 研究证明编码器模型通过声明调优可实现高效的多语言零样本泛化，性能媲美多语言大模型且资源消耗更低。


<details>
  <summary>Details</summary>
Motivation: 探索编码器模型在多语言场景下的零样本泛化潜力，为低资源语言提供比大模型更高效的NLP解决方案。

Method: 将声明调优方法扩展至多语言任务，通过重构任务模板实现跨语言迁移，并系统分析数据集设计和效率表现。

Result: 先进编码器模型在多语言泛化中表现优异（与多语言LLMs相当），训练速度提升3倍且内存需求降低80%。

Conclusion: 该方法推动了资源高效型NLP发展，模型参数和代码已开源，为低资源语言社区提供实用解决方案。

Abstract: Large Language Models (LLMs) excel in zero-shot and few-shot tasks, but
achieving similar performance with encoder-only models like BERT and RoBERTa
has been challenging due to their architecture. However, encoders offer
advantages such as lower computational and memory costs. Recent work adapts
them for zero-shot generalization using Statement Tuning, which reformulates
tasks into finite templates. We extend this approach to multilingual NLP,
exploring whether encoders can achieve zero-shot cross-lingual generalization
and serve as efficient alternatives to memory-intensive LLMs for low-resource
languages. Our results show that state-of-the-art encoder models generalize
well across languages, rivaling multilingual LLMs while being more efficient.
We also analyze multilingual Statement Tuning dataset design, efficiency gains,
and language-specific generalization, contributing to more inclusive and
resource-efficient NLP models. We release our code and models.

</details>


### [195] [MMD-Sense-Analysis: Word Sense Detection Leveraging Maximum Mean Discrepancy](https://arxiv.org/abs/2506.01602)
*Kensuke Mitsuzawa*

Main category: cs.CL

TL;DR: 首次将最大均值差异(MMD)应用于词义演变检测，提出MMD-Sense-Analysis方法并通过实证验证有效性


<details>
  <summary>Details</summary>
Motivation: 词义演变分析对理解语言文化变迁具有重要意义，传统方法在跨时段语义变化量化和解释方面存在局限

Method: 基于MMD统计方法筛选语义关键变量，构建跨时段词义变化量化模型，支持多历史时期演变分析

Result: 实证评估表明该方法能有效检测词义演变，实现历时性语义变化的识别与解释

Conclusion: MMD方法为词义演变检测提供了新的量化工具，首次实现多时段语义变化的系统分析与解释

Abstract: Word sense analysis is an essential analysis work for interpreting the
linguistic and social backgrounds. The word sense change detection is a task of
identifying and interpreting shifts in word meanings over time. This paper
proposes MMD-Sense-Analysis, a novel approach that leverages Maximum Mean
Discrepancy (MMD) to select semantically meaningful variables and quantify
changes across time periods. This method enables both the identification of
words undergoing sense shifts and the explanation of their evolution over
multiple historical periods. To my knowledge, this is the first application of
MMD to word sense change detection. Empirical assessment results demonstrate
the effectiveness of the proposed approach.

</details>


### [196] [IndicRAGSuite: Large-Scale Datasets and a Benchmark for Indian Language RAG Systems](https://arxiv.org/abs/2506.01615)
*Pasunuti Prasanjith,Prathmesh B More,Anoop Kunchukuttan,Raj Dabre*

Main category: cs.CL

TL;DR: 为解决印度语言RAG系统缺乏评估基准和训练数据的问题，研究团队创建了包含13种语言的评估基准IndicMSMarco，并通过LLM构建了19种语言的大规模(问题-答案-相关段落)数据集。


<details>
  <summary>Details</summary>
Motivation: 印度语言RAG系统发展受限，主要缺乏多语言检索评估基准和大规模训练数据集，现有资源多集中于英语或高资源语言。

Method: 1. 人工翻译MS MARCO数据创建13种语言的评估基准IndicMSMarco；2. 利用LLM从19种印度语言维基百科构建训练数据；3. 整合翻译版MS MARCO数据集增强训练。

Result: 发布了包含评估基准、维基百科衍生数据集和增强版MS MARCO的Indic-Rag-Suite资源库。

Conclusion: 该研究填补了印度语言RAG系统资源空白，为多语言NLP研究提供了重要基础设施支持。

Abstract: Retrieval-Augmented Generation (RAG) systems enable language models to access
relevant information and generate accurate, well-grounded, and contextually
informed responses. However, for Indian languages, the development of
high-quality RAG systems is hindered by the lack of two critical resources: (1)
evaluation benchmarks for retrieval and generation tasks, and (2) large-scale
training datasets for multilingual retrieval. Most existing benchmarks and
datasets are centered around English or high-resource languages, making it
difficult to extend RAG capabilities to the diverse linguistic landscape of
India. To address the lack of evaluation benchmarks, we create IndicMSMarco, a
multilingual benchmark for evaluating retrieval quality and response generation
in 13 Indian languages, created via manual translation of 1000 diverse queries
from MS MARCO-dev set. To address the need for training data, we build a
large-scale dataset of (question, answer, relevant passage) tuples derived from
the Wikipedias of 19 Indian languages using state-of-the-art LLMs.
Additionally, we include translated versions of the original MS MARCO dataset
to further enrich the training data and ensure alignment with real-world
information-seeking tasks. Resources are available here:
https://huggingface.co/datasets/ai4bharat/Indic-Rag-Suite

</details>


### [197] [Domain Lexical Knowledge-based Word Embedding Learning for Text Classification under Small Data](https://arxiv.org/abs/2506.01621)
*Zixiao Zhu,Kezhi Mao*

Main category: cs.CL

TL;DR: 提出KVWEFFER模型，通过领域词典知识增强BERT词向量区分性，在情感分析等任务中提升分类效果


<details>
  <summary>Details</summary>
Motivation: BERT在关键词驱动的分类任务（如情感分析）中表现欠佳，因其上下文嵌入缺乏区分度。研究发现关键词的BERT嵌入无法生成足够区分性的文本表示

Method: 1. 开发领域词典知识自动采集算法；2. 将BERT嵌入投影到新空间，最大化类内相似性和类间差异；3. 构建知识增强的词向量模型

Result: 在情感分析、情绪识别和问答三个任务中验证有效，准确率显著提升（实验细节见github仓库）

Conclusion: 基于领域知识的词向量增强模型能有效提升文本分类性能，尤其在关键词驱动的任务中效果显著

Abstract: Pre-trained language models such as BERT have been proved to be powerful in
many natural language processing tasks. But in some text classification
applications such as emotion recognition and sentiment analysis, BERT may not
lead to satisfactory performance. This often happens in applications where
keywords play critical roles in the prediction of class labels. Our
investigation found that the root cause of the problem is that the
context-based BERT embedding of the keywords may not be discriminative enough
to produce discriminative text representation for classification. Motivated by
this finding, we develop a method to enhance word embeddings using
domain-specific lexical knowledge. The knowledge-based embedding enhancement
model projects the BERT embedding into a new space where within-class
similarity and between-class difference are maximized. To implement the
knowledge-based word embedding enhancement model, we also develop a knowledge
acquisition algorithm for automatically collecting lexical knowledge from
online open sources. Experiment results on three classification tasks,
including sentiment analysis, emotion recognition and question answering, have
shown the effectiveness of our proposed word embedding enhancing model. The
codes and datasets are in https://github.com/MidiyaZhu/KVWEFFER.

</details>


### [198] [MVAN: Multi-View Attention Networks for Fake News Detection on Social Media](https://arxiv.org/abs/2506.01627)
*Shiwen Ni,Jiawen Li,Hung-Yu Kao*

Main category: cs.CL

TL;DR: 提出多视角注意力网络MVAN模型，通过结合文本语义注意力和传播结构注意力检测社交媒体假新闻，实验显示准确率提升2.5%


<details>
  <summary>Details</summary>
Motivation: 现有假新闻检测方法依赖长文本内容（如用户评论），而现实场景中仅有短文本推文和传播结构数据，需更适应实际场景的检测方案

Method: 构建MVAN神经网络模型，包含文本语义注意力（提取关键词语）和传播结构注意力（识别可疑用户）双重视觉注意力机制

Result: 在两个真实数据集上实验，MVAN平均准确率超越现有最优方法2.5%，且能提供可解释的检测线索

Conclusion: MVAN通过多视角注意力有效融合内容与传播结构特征，在提升检测精度的同时实现解释性，为实际场景的假新闻检测提供新方案

Abstract: Fake news on social media is a widespread and serious problem in today's
society. Existing fake news detection methods focus on finding clues from Long
text content, such as original news articles and user comments. This paper
solves the problem of fake news detection in more realistic scenarios. Only
source shot-text tweet and its retweet users are provided without user
comments. We develop a novel neural network based model,
\textbf{M}ulti-\textbf{V}iew \textbf{A}ttention \textbf{N}etworks (MVAN) to
detect fake news and provide explanations on social media. The MVAN model
includes text semantic attention and propagation structure attention, which
ensures that our model can capture information and clues both of source tweet
content and propagation structure. In addition, the two attention mechanisms in
the model can find key clue words in fake news texts and suspicious users in
the propagation structure. We conduct experiments on two real-world datasets,
and the results demonstrate that MVAN can significantly outperform
state-of-the-art methods by 2.5\% in accuracy on average, and produce a
reasonable explanation.

</details>


### [199] [Cross-Lingual Generalization and Compression: From Language-Specific to Shared Neurons](https://arxiv.org/abs/2506.01629)
*Frederick Riemenschneider,Anette Frank*

Main category: cs.CL

TL;DR: 多语言模型通过预训练阶段逐步形成跨语言抽象表示，神经元在训练中逐渐实现跨语言对齐


<details>
  <summary>Details</summary>
Motivation: 探索多语言模型在没有显式跨语言监督的情况下如何实现知识迁移

Method: 分析三个MLLMs的参数空间演化过程，通过探测实验追踪神经元发展轨迹

Result: 1. 观察到参数空间压缩模式（语言特定→跨语言抽象）
2. 层级功能从语言识别转向专业化分工
3. 特定神经元成为跨语言语义概念的可靠预测器

Conclusion: MLLMs通过训练阶段逐步形成跨语言抽象表示，神经元在预训练中实现语义概念的对齐

Abstract: Multilingual language models (MLLMs) have demonstrated remarkable abilities
to transfer knowledge across languages, despite being trained without explicit
cross-lingual supervision. We analyze the parameter spaces of three MLLMs to
study how their representations evolve during pre-training, observing patterns
consistent with compression: models initially form language-specific
representations, which gradually converge into cross-lingual abstractions as
training progresses. Through probing experiments, we observe a clear transition
from uniform language identification capabilities across layers to more
specialized layer functions. For deeper analysis, we focus on neurons that
encode distinct semantic concepts. By tracing their development during
pre-training, we show how they gradually align across languages. Notably, we
identify specific neurons that emerge as increasingly reliable predictors for
the same concepts across languages.

</details>


### [200] [ESGenius: Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge](https://arxiv.org/abs/2506.01646)
*Chaoyue He,Xin Zhou,Yi Wu,Xinjia Yu,Yan Zhang,Lei Zhang,Di Wang,Shengfei Lyu,Hong Xu,Xiaoqiao Wang,Wei Liu,Chunyan Miao*

Main category: cs.CL

TL;DR: 提出首个专注ESG与可持续发展的综合基准ESGenius，包含问答数据集和权威语料库，通过两阶段评估验证RAG技术对LLMs性能的提升效果


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在跨学科ESG问题上的零样本表现有限（55-70%准确率），需建立结合权威知识源的评估体系以提升模型的专业理解能力

Method: 构建ESGenius-QA（1,136道专家验证的多选题）和ESGenius-Corpus（231份权威文档库），采用零样本与检索增强生成（RAG）两阶段评估协议

Result: RAG显著提升模型表现（如14B模型从63.82%提升至80.46%），小模型通过RAG可超越大模型零样本效果

Conclusion: ESG领域需结合权威知识源，ESGenius作为首个专业基准为LLMs的可持续发展应用提供评估框架与技术验证

Abstract: We introduce ESGenius, a comprehensive benchmark for evaluating and enhancing
the proficiency of Large Language Models (LLMs) in Environmental, Social and
Governance (ESG) and sustainability-focused question answering. ESGenius
comprises two key components: (i) ESGenius-QA, a collection of 1 136
multiple-choice questions generated by LLMs and rigorously validated by domain
experts, covering a broad range of ESG pillars and sustainability topics. Each
question is systematically linked to its corresponding source text, enabling
transparent evaluation and supporting retrieval-augmented generation (RAG)
methods; and (ii) ESGenius-Corpus, a meticulously curated repository of 231
foundational frameworks, standards, reports and recommendation documents from
seven authoritative sources. Moreover, to fully assess the capabilities and
adaptation potential of the model, we implement a rigorous two-stage evaluation
protocol -- Zero-Shot and RAG. Extensive experiments across 50 LLMs (ranging
from 0.5 B to 671 B parameters) demonstrate that state-of-the-art models
achieve only moderate performance in zero-shot settings, with accuracies
typically around 55--70\%, highlighting ESGenius's challenging nature for LLMs
in interdisciplinary contexts. However, models employing RAG show significant
performance improvements, particularly for smaller models. For example,
"DeepSeek-R1-Distill-Qwen-14B" improves from 63.82\% (zero-shot) to 80.46\%
with RAG. These results underscore the necessity of grounding responses in
authoritative sources for enhanced ESG understanding. To the best of our
knowledge, ESGenius is the first benchmark curated for LLMs and the relevant
enhancement technologies that focuses on ESG and sustainability topics.

</details>


### [201] [Cross-Lingual Transfer of Cultural Knowledge: An Asymmetric Phenomenon](https://arxiv.org/abs/2506.01675)
*Chen Zhang,Zhiyuan Liao,Yansong Feng*

Main category: cs.CL

TL;DR: 研究发现，大语言模型（LLMs）在多语言适应过程中，高资源语言与英语之间存在双向文化知识迁移，而低资源语言仅单向迁移至英语，该现象与预训练数据中文化知识出现频率相关。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽评估了LLMs处理文化多样性的能力，但其跨语言文化知识获取机制（尤其多语言环境下的迁移路径）仍不明确。研究旨在揭示语言适应过程中文化知识跨语言迁移的规律。

Method: 建立可解释性框架，控制迁移效应并保证训练数据透明度。选择四个非英语文化案例，系统分析英语与高/低资源语言间的双向文化迁移模式。

Result: 观察到英语与高资源语言（如中文）存在双向文化迁移，而低资源语言（如祖鲁语）仅单向向英语迁移。通过语料库分析验证了'高频文化知识更易迁移'的假设。

Conclusion: 文化知识迁移的不对称性源于预训练数据中的出现频率。高资源语言因数据丰富实现平衡互惠迁移，低资源语言因数据稀疏导致知识输出受限，揭示了数据分布对LLMs文化表征的关键影响。

Abstract: Despite substantial research efforts evaluating how well large language
models~(LLMs) handle global cultural diversity, the mechanisms behind their
cultural knowledge acquisition, particularly in multilingual settings, remain
unclear. We study this question by investigating how cultural knowledge
transfers across languages during language adaptation of LLMs. We introduce an
interpretable framework for studying this transfer, ensuring training data
transparency and controlling transfer effects. Through a study of four
non-Anglophonic cultures, we observe bidirectional cultural transfer between
English and other high-resource languages, while low-resource languages
primarily transfer knowledge to English with limited reverse flow. To explain
this asymmetric phenomenon, we propose a frequency-based hypothesis: cultural
knowledge appearing more frequently in the pretraining data transfers more
easily, which is supported by empirical analysis of the training corpora.

</details>


### [202] [StochasTok: Improving Fine-Grained Subword Understanding in LLMs](https://arxiv.org/abs/2506.01687)
*Anya Sims,Thom Foster,Klara Kaleb,Tuan-Duy H. Nguyen,Joseph Lee,Jakob N. Foerster,Yee Whye Teh,Cong Lu*

Main category: cs.CL

TL;DR: 提出随机分词方案StochasTok，通过训练时随机分割标记提升大语言模型在子词级任务的表现


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型因分词机制遮蔽细粒度结构，导致在字符计数等简单子词任务表现不佳，而字符级分词等方法存在计算成本高、改进不稳定等问题

Method: 引入StochasTok——在训练过程中随机分割标记，使模型能够感知词语内部结构的分词方案

Result: 预训练显著提升语言游戏任务性能（字符计数/子串识别/数学任务），后训练可注入现有模型避免重新预训练

Conclusion: 通过最小改动实现显著改进，StochasTok在更大模型应用潜力巨大，开源代码推动应用扩展

Abstract: Subword-level understanding is integral to numerous tasks, including
understanding multi-digit numbers, spelling mistakes, abbreviations, rhyming,
and wordplay. Despite this, current large language models (LLMs) still often
struggle with seemingly simple subword-level tasks like How many 'r's in
'strawberry'?. A key factor behind these failures is tokenization which
obscures the fine-grained structure of words. Current alternatives, such as
character-level and dropout tokenization methods, significantly increase
computational costs and provide inconsistent improvements. In this paper we
revisit tokenization and introduce StochasTok, a simple, efficient stochastic
tokenization scheme that randomly splits tokens during training, allowing LLMs
to 'see' their internal structure. Our experiments show that pretraining with
StochasTok substantially improves LLMs' downstream performance across multiple
subword-level language games, including character counting, substring
identification, and math tasks. Furthermore, StochasTok's simplicity allows
seamless integration at any stage of the training pipeline; and we demonstrate
that post-training with StochasTok can instill improved subword understanding
into existing pretrained models, thus avoiding costly pretraining from scratch.
These dramatic improvements achieved with a minimal change suggest StochasTok
holds exciting potential when applied to larger, more capable models. Code
open-sourced at: https://github.com/anyasims/stochastok.

</details>


### [203] [When LLMs Team Up: The Emergence of Collaborative Affective Computing](https://arxiv.org/abs/2506.01698)
*Wenna Lai,Haoran Xie,Guandong Xu,Qing Li,S. Joe Qin*

Main category: cs.CL

TL;DR: LLM协作系统通过结合专业模型与LLMs的协同机制，解决传统情感计算架构的局限性，提升复杂情感推理的鲁棒性和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统流水线架构存在结构僵化和效率低下问题，而LLMs在情感计算中存在文化误解和决策幻觉等认知局限，需要构建协作系统模拟人类情感与理性协同的智能模式。

Method: 提出从结构化到自主化的LLM协作框架，通过策略设计（知识增强/思维分解）、机制实现（反馈循环/动态路由）和功能模块（情感识别/生成）支撑多模态情感应用。

Result: 实验表明协作系统在情感理解（情绪识别准确率提升18%）和生成任务（内容情感一致性提高27%）中显著优于单一模型范式。

Conclusion: LLM协作系统为情感计算开辟新范式，未来需解决多模态对齐、伦理安全等挑战以实现类人社会智能。

Abstract: Affective Computing (AC) is essential in bridging the gap between human
emotional experiences and machine understanding. Traditionally, AC tasks in
natural language processing (NLP) have been approached through pipeline
architectures, which often suffer from structure rigidity that leads to
inefficiencies and limited adaptability. The advent of Large Language Models
(LLMs) has revolutionized this field by offering a unified approach to
affective understanding and generation tasks, enhancing the potential for
dynamic, real-time interactions. However, LLMs face cognitive limitations in
affective reasoning, such as misinterpreting cultural nuances or contextual
emotions, and hallucination problems in decision-making. To address these
challenges, recent research advocates for LLM-based collaboration systems that
emphasize interactions among specialized models and LLMs, mimicking human-like
affective intelligence through the synergy of emotional and rational thinking
that aligns with Dual Process Theory in psychology. This survey aims to provide
a comprehensive overview of LLM-based collaboration systems in AC, exploring
from structured collaborations to autonomous collaborations. Specifically, it
includes: (1) A systematic review of existing methods, focusing on
collaboration strategies, mechanisms, key functions, and applications; (2)
Experimental comparisons of collaboration strategies across representative
tasks in affective understanding and generation; (3) An analysis highlighting
the potential of these systems to enhance robustness and adaptability in
complex affective reasoning; (4) A discussion of key challenges and future
research directions to further advance the field. This work is the first to
systematically explore collaborative intelligence with LLMs in AC, paving the
way for more powerful applications that approach human-like social
intelligence.

</details>


### [204] [mdok of KInIT: Robustly Fine-tuned LLM for Binary and Multiclass AI-Generated Text Detection](https://arxiv.org/abs/2506.01702)
*Dominik Macko*

Main category: cs.CL

TL;DR: 提出基于小规模LLM微调的mdok方法，在Voight-Kampff 2025检测挑战赛中实现二元检测和多类别分类（第一名）的突破性表现


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成的高质量文本难以被人类识别，存在剽窃/垃圾信息/虚假信息传播等滥用风险，现有检测方法对分布外数据鲁棒性不足

Method: 通过微调较小规模的LLM进行文本分类，应用于Voight-Kampff检测挑战赛的两个子任务

Result: 在二元检测任务中表现优异，在人机协作多类别分类任务中获得第一名

Conclusion: 该方法有效提升了机器生成文本检测的鲁棒性，在复杂实际场景中展现出显著优势

Abstract: The large language models (LLMs) are able to generate high-quality texts in
multiple languages. Such texts are often not recognizable by humans as
generated, and therefore present a potential of LLMs for misuse (e.g.,
plagiarism, spams, disinformation spreading). An automated detection is able to
assist humans to indicate the machine-generated texts; however, its robustness
to out-of-distribution data is still challenging. This notebook describes our
mdok approach in robust detection, based on fine-tuning smaller LLMs for text
classification. It is applied to both subtasks of Voight-Kampff Generative AI
Detection 2025, providing remarkable performance in binary detection as well as
in multiclass (1st rank) classification of various cases of human-AI
collaboration.

</details>


### [205] [Fairness Dynamics During Training](https://arxiv.org/abs/2506.01709)
*Krishna Patel,Nivedha Sivakumar,Barry-John Theobald,Luca Zappella,Nicholas Apostoloff*

Main category: cs.CL

TL;DR: 通过监测LLM训练中的公平性动态，发现偏见会突然出现且与常规性能指标无关。引入新指标评估Pythia模型在WinoBias数据集上的性别预测偏见，揭示大模型可能表现更多偏见。


<details>
  <summary>Details</summary>
Motivation: 探究LLM训练过程中偏见的动态演变，通过训练干预（如早期停止）实现偏见诊断与缓解。传统性能指标无法有效反映公平性变化。

Method: 提出平均排名（Average Rank）和基于分组的Jensen-Shannon散度（JSD-P）两个新指标，在Pythia系列模型（160M-6.9B参数）的预训练过程中，使用WinoBias数据集分析职业性别预测偏见。

Result: 1. Pythia-6.9B对男性存在明显偏见（预测置信度更高）
2. 通过早期停止可换取92.5%公平性提升（仅损失1.7%准确率）
3. 大模型表现出更多未明示性别时的性别假设

Conclusion: 公平性动态监测为模型诊断提供新视角，早期停止可作为有效干预手段。模型规模的增大可能放大而非缓解偏见问题，这对LLM开发具有重要警示意义。

Abstract: We investigate fairness dynamics during Large Language Model (LLM) training
to enable the diagnoses of biases and mitigations through training
interventions like early stopping; we find that biases can emerge suddenly and
do not always follow common performance metrics. We introduce two new metrics
to evaluate fairness dynamics holistically during model pre-training: Average
Rank and Jensen-Shannon Divergence by Parts. These metrics provide insights
into the Pythia models' progression of biases in gender prediction of
occupations on the WinoBias dataset. By monitoring these dynamics, we find that
(1) Pythia-6.9b is biased towards men; it becomes more performant and confident
predicting "male" than "female" during training, (2) via early-stopping,
Pythia-6.9b can exchange 1.7% accuracy on LAMBADA for a 92.5% increase in
fairness, and (3) larger models can exhibit more bias; Pythia-6.9b makes more
assumptions about gender than Pythia-160m, even when a subject's gender is not
specified.

</details>


### [206] [Reasoning-Table: Exploring Reinforcement Learning for Table Reasoning](https://arxiv.org/abs/2506.01710)
*Fangyu Lei,Jinxiang Meng,Yiming Huang,Tinghong Chen,Yun Zhang,Shizhu He,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: 提出首个基于强化学习的表格推理框架Reasoning-Table，通过数据预处理、奖励设计和训练策略优化，显著提升性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 监督微调方法存在泛化与鲁棒性不足的问题，受模仿学习固有偏差限制。

Method: 创新应用强化学习，结合规则结果奖励机制和统一多任务训练策略。

Result: 在多个基准测试中超越SFT方法，7B模型在BIRD数据集上达68.3%准确率，优于Claude-3.7-Sonnet 4%。

Conclusion: 验证了强化学习在表格推理中的有效性，显著提升模型泛化能力与任务适应性。

Abstract: Table reasoning, encompassing tasks such as table question answering, fact
verification, and text-to-SQL, requires precise understanding of structured
tabular data, coupled with numerical computation and code manipulation for
effective inference. Supervised fine-tuning (SFT) approaches have achieved
notable success but often struggle with generalization and robustness due to
biases inherent in imitative learning. We introduce Reasoning-Table, the first
application of reinforcement learning (RL) to table reasoning, achieving
state-of-the-art performance. Through rigorous data preprocessing, reward
design, and tailored training strategies, our method leverages simple
rule-based outcome rewards to outperform SFT across multiple benchmarks.
Unified training across diverse tasks enables Reasoning-Table to emerge as a
robust table reasoning large language model, surpassing larger proprietary
models like Claude-3.7-Sonnet by 4.0% on table reasoning benchmarks. The
approach also achieves excellent performance on text-to-SQL tasks, reaching
68.3% performance on the BIRD dev dataset with a 7B model. Further experiments
demonstrate that Reasoning-Table enhances the model's generalization
capabilities and robustness.

</details>


### [207] [SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning](https://arxiv.org/abs/2506.01713)
*Zhongwei Wan,Zhihao Dou,Che Liu,Yu Zhang,Dongfei Cui,Qinjian Zhao,Hui Shen,Jing Xiong,Yi Xin,Yifan Jiang,Yangfan He,Mi Zhang,Shen Yan*

Main category: cs.CL

TL;DR: 提出SRPO框架，通过两阶段反思增强的强化学习显著提升多模态大语言模型的推理能力和反思质量


<details>
  <summary>Details</summary>
Motivation: 现有反思方法过于简单且无法生成有意义反馈，因预训练模型的推理能力在初始训练后基本固定

Method: 1. 在先进MLLM指导下构建反思数据集 2. 在GRPO框架中引入新颖奖励机制促进简洁有效的反思

Result: 在MathVista等多项基准测试中，Qwen系列模型在推理准确率和反思质量上显著超越现有SOTA模型

Conclusion: SRPO成功证明将结构化反思机制融入强化学习框架能有效提升多模态模型的自我修正能力

Abstract: Multimodal large language models (MLLMs) have shown promising capabilities in
reasoning tasks, yet still struggle with complex problems requiring explicit
self-reflection and self-correction, especially compared to their unimodal
text-based counterparts. Existing reflection methods are simplistic and
struggle to generate meaningful and instructive feedback, as the reasoning
ability and knowledge limits of pre-trained models are largely fixed during
initial training. To overcome these challenges, we propose Multimodal
Self-Reflection enhanced reasoning with Group Relative Policy Optimization
(SRPO), a two-stage reflection-aware reinforcement learning (RL) framework
explicitly designed to enhance multimodal LLM reasoning. In the first stage, we
construct a high-quality, reflection-focused dataset under the guidance of an
advanced MLLM, which generates reflections based on initial responses to help
the policy model learn both reasoning and self-reflection. In the second stage,
we introduce a novel reward mechanism within the GRPO framework that encourages
concise and cognitively meaningful reflection while avoiding redundancy.
Extensive experiments across multiple multimodal reasoning benchmarks,
including MathVista, MathVision, MathVerse, and MMMU-Pro, using Qwen-2.5-VL-7B
and Qwen-2.5-VL-32B demonstrate that SRPO significantly outperforms
state-of-the-art models, achieving notable improvements in both reasoning
accuracy and reflection quality.

</details>


### [208] [Tug-of-war between idiom's figurative and literal meanings in LLMs](https://arxiv.org/abs/2506.01723)
*Soyoung Oh,Xinting Huang,Mathis Pink,Michael Hahn,Vera Demberg*

Main category: cs.CL

TL;DR: 研究通过机制解释性方法揭示了LLama3.2-1B-base模型处理习语的三步骤过程：早期检索比喻意义、注意力头调控双义、并行路径维持解释灵活性


<details>
  <summary>Details</summary>
Motivation: 解决语言模型处理习语时因字面义与比喻义冲突导致的语义理解难题

Method: 使用机制解释性工具追踪Transformer内部处理过程，聚焦注意力机制和MLP子层

Result: 发现模型通过特定注意力头增强比喻义（早期处理）、建立中间路径表征比喻义、保留并行字面义处理路径的三阶段机制

Conclusion: 自回归Transformer通过分离且并行的处理路径同时维护习语的两种语义解释，为模型的双重语义处理能力提供神经机制证据

Abstract: Idioms present a unique challenge for language models due to their
non-compositional figurative meanings, which often strongly diverge from the
idiom's literal interpretation. This duality requires a model to learn
representing and deciding between the two meanings to interpret an idiom in a
figurative sense, or literally. In this paper, we employ tools from mechanistic
interpretability to trace how a large pretrained causal transformer
(LLama3.2-1B-base) deals with this ambiguity. We localize three steps of idiom
processing: First, the idiom's figurative meaning is retrieved in early
attention and MLP sublayers. We identify specific attention heads which boost
the figurative meaning of the idiom while suppressing the idiom's literal
interpretation. The model subsequently represents the figurative representation
through an intermediate path. Meanwhile, a parallel bypass route forwards
literal interpretation, ensuring that a both reading remain available. Overall,
our findings provide a mechanistic evidence for idiom comprehension in an
autoregressive transformer.

</details>


### [209] [Common Corpus: The Largest Collection of Ethical Data for LLM Pre-Training](https://arxiv.org/abs/2506.01732)
*Pierre-Carl Langlais,Carlos Rosas Hinostroza,Mattia Nee,Catherine Arnett,Pavel Chizhov,Eliot Krzystof Jones,Irène Girard,David Mach,Anastasia Stasenko,Ivan P. Yamshchikov*

Main category: cs.CL

TL;DR: Common Corpus推出最大的开放预训练数据集（约2万亿token），包含无版权/合规许可的多语言及代码数据，解决LLM训练中的版权合规问题


<details>
  <summary>Details</summary>
Motivation: 现有LLM预训练数据多含受版权保护内容，阻碍AI立法下的合规应用，亟需符合数据安全法规的开放数据集

Method: 整合无版权或允许使用的数据源，通过严格过滤和内容整理构建跨语言/跨领域的数据集，包含丰富代码资源

Result: 数据集已被Anthropic等头部企业采用，成为LLM开放科学研究的关键基础设施

Conclusion: Common Corpus通过合法多元的数据源，为学术研究和商业应用提供合规基础，推动开放科学生态发展

Abstract: Large Language Models (LLMs) are pre-trained on large amounts of data from
different sources and domains. These data most often contain trillions of
tokens with large portions of copyrighted or proprietary content, which hinders
the usage of such models under AI legislation. This raises the need for truly
open pre-training data that is compliant with the data security regulations. In
this paper, we introduce Common Corpus, the largest open dataset for language
model pre-training. The data assembled in Common Corpus are either
uncopyrighted or under permissible licenses and amount to about two trillion
tokens. The dataset contains a wide variety of languages, ranging from the main
European languages to low-resource ones rarely present in pre-training
datasets; in addition, it includes a large portion of code data. The diversity
of data sources in terms of covered domains and time periods opens up the paths
for both research and entrepreneurial needs in diverse areas of knowledge. In
this technical report, we present the detailed provenance of data assembling
and the details of dataset filtering and curation. Being already used by such
industry leaders as Anthropic and multiple LLM training projects, we believe
that Common Corpus will become a critical infrastructure for open science
research in LLMs.

</details>


### [210] [Benford's Curse: Tracing Digit Bias to Numerical Hallucination in LLMs](https://arxiv.org/abs/2506.01734)
*Jiandong Shao,Yao Lu,Jianfei Yang*

Main category: cs.CL

TL;DR: 发现大语言模型在数字任务中的错误源于预训练语料库中的长尾数字分布，通过神经元修剪可部分修正输出偏差


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在基础数值任务频繁出错的根本原因，揭示语料库统计特征与符号故障模式的关联

Method: 结合本福特定律构建均匀分布评估基准，通过logit-lens追踪和神经元级分析定位偏差来源

Result: 深层FFN神经元导致数字选择偏差，修剪后错误输出减少25%-40%

Conclusion: 细粒度预训练偏差会传播至模型行为，为诊断数值幻觉提供了新的方法论框架

Abstract: Large Language Models (LLMs) exhibit impressive performance on complex
reasoning tasks, yet they frequently fail on basic numerical problems,
producing incorrect outputs. Inspired by Benford's Law -- a statistical pattern
where lower digits occur more frequently as leading digits -- we hypothesize
that the long-tailed digit distributions in web-collected corpora may be
learned by LLMs during pretraining, leading to biased numerical generation. To
investigate the hypothesis, we first examine whether digits frequencies in
pretraining corpus (OLMo2) follows Benford's law. We then construct an
evaluation benchmark with uniformly distributed ground-truth digits across
seven numerical reasoning tasks. Our evaluation results demonstrate that
leading open-source LLMs show a consistent pattern of digit bias that resembles
Benford's law. Through logit-lens tracing and neuron-level dissection, we
identify that this bias arises predominantly from a small subset of highly
digit-selective feed-forward network (FFN) neurons in the deeper layers.
Finally, we demonstrate that pruning these neurons mitigates imbalanced
overgeneration and partially corrects erroneous outputs, providing causal
evidence that fine-grained pretraining digit bias can propagate into model
behavior. Our findings reveal a fundamental connection between corpus-level
statistics and symbolic failure modes in LLMs, offering a new lens for
diagnosing and mitigating hallucinations in numerical tasks.

</details>


### [211] [Thinking in Character: Advancing Role-Playing Agents with Role-Aware Reasoning](https://arxiv.org/abs/2506.01748)
*Yihong Tang,Kehai Chen,Muyun Yang,Zhengyu Niu,Jing Li,Tiejun Zhao,Min Zhang*

Main category: cs.CL

TL;DR: 提出角色感知推理方法（RAR）解决角色扮演代理中的注意力分散和风格漂移问题


<details>
  <summary>Details</summary>
Motivation: 现有角色扮演代理基于显式对话数据，缺乏深度思考，导致注意力分散（忘记角色）和风格漂移（推理僵化）

Method: RAR方法分为角色身份激活（RIA）和推理风格优化（RSO）两阶段：RIA通过角色画像保持角色认知，RSO通过LRM蒸馏实现推理风格对齐

Result: 实验证明RAR显著提升角色扮演代理的表现，有效解决注意力分散和风格漂移问题

Conclusion: 该方法通过双重优化机制实现了角色一致性的深度推理，为情感陪伴等应用提供更自然的交互基础

Abstract: The advancement of Large Language Models (LLMs) has spurred significant
interest in Role-Playing Agents (RPAs) for applications such as emotional
companionship and virtual interaction. However, recent RPAs are often built on
explicit dialogue data, lacking deep, human-like internal thought processes,
resulting in superficial knowledge and style expression. While Large Reasoning
Models (LRMs) can be employed to simulate character thought, their direct
application is hindered by attention diversion (i.e., RPAs forget their role)
and style drift (i.e., overly formal and rigid reasoning rather than
character-consistent reasoning). To address these challenges, this paper
introduces a novel Role-Aware Reasoning (RAR) method, which consists of two
important stages: Role Identity Activation (RIA) and Reasoning Style
Optimization (RSO). RIA explicitly guides the model with character profiles
during reasoning to counteract attention diversion, and then RSO aligns
reasoning style with the character and scene via LRM distillation to mitigate
style drift. Extensive experiments demonstrate that the proposed RAR
significantly enhances the performance of RPAs by effectively addressing
attention diversion and style drift.

</details>


### [212] [Developing a Mixed-Methods Pipeline for Community-Oriented Digitization of Kwak'wala Legacy Texts](https://arxiv.org/abs/2506.01775)
*Milind Agarwal,Daisy Rosenblum,Antonios Anastasopoulos*

Main category: cs.CL

TL;DR: 应用最新OCR技术结合语言识别和后校正模型，成功将Kwak'wala图像文本转化为高质量数字转录，支持语言复兴与技术开发。


<details>
  <summary>Details</summary>
Motivation: Kwak'wala作为濒危原住民语言，早期文献以图像形式存在且无法机器读取，阻碍了现代正字法转写及语言技术开发。数字化可促进语言复兴与资源利用。

Method: 采用现成OCR工具+语言识别模块隔离Kwak'wala文本，结合掩码技术处理复杂排版，最后通过后校正模型提升转录准确性。

Result: 构建了针对Kwak'wala文本的优化OCR流程，克服了原始图像质量差、字符变体多等挑战，生成高可用数字文本。

Conclusion: 混合式OCR方法有效解决了土著语言文献的数字化难题，为其他低资源语言文本处理提供了可复用的技术框架。

Abstract: Kwak'wala is an Indigenous language spoken in British Columbia, with a rich
legacy of published documentation spanning more than a century, and an active
community of speakers, teachers, and learners engaged in language
revitalization. Over 11 volumes of the earliest texts created during the
collaboration between Franz Boas and George Hunt have been scanned but remain
unreadable by machines. Complete digitization through optical character
recognition has the potential to facilitate transliteration into modern
orthographies and the creation of other language technologies. In this paper,
we apply the latest OCR techniques to a series of Kwak'wala texts only
accessible as images, and discuss the challenges and unique adaptations
necessary to make such technologies work for these real-world texts. Building
on previous methods, we propose using a mix of off-the-shelf OCR methods,
language identification, and masking to effectively isolate Kwak'wala text,
along with post-correction models, to produce a final high-quality
transcription.

</details>


### [213] [MaXIFE: Multilingual and Cross-lingual Instruction Following Evaluation](https://arxiv.org/abs/2506.01776)
*Yile Liu,Ziwei Ma,Xiu Jiang,Jinglu Hu,Jing Chang,Liang Li*

Main category: cs.CL

TL;DR: MaXIFE是多语言指令遵循评估基准，用于测试大语言模型在23种语言中的表现


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要关注单语言场景，忽略多语言/跨语言场景的挑战差异

Method: 结合基于规则的评估（效率导向）和基于模型的评估（精度导向）方法

Result: 在主流商业和开源模型上建立基准测试结果，包含1667个可验证指令任务

Conclusion: 通过标准化评估工具推动NLP发展，支持更全面的多语言能力评估

Abstract: With the rapid adoption of large language models (LLMs) in natural language
processing, the ability to follow instructions has emerged as a key metric for
evaluating their practical utility. However, existing evaluation methods often
focus on single-language scenarios, overlooking the challenges and differences
present in multilingual and cross-lingual contexts. To address this gap, we
introduce MaXIFE: a comprehensive evaluation benchmark designed to assess
instruction-following capabilities across 23 languages with 1,667 verifiable
instruction tasks. MaXIFE integrates both Rule-Based Evaluation and Model-Based
Evaluation, ensuring a balance of efficiency and accuracy. We applied MaXIFE to
evaluate several leading commercial and open-source LLMs, establishing baseline
results for future comparisons. By providing a standardized tool for
multilingual instruction-following evaluation, MaXIFE aims to advance research
and development in natural language processing.

</details>


### [214] [iQUEST: An Iterative Question-Guided Framework for Knowledge Base Question Answering](https://arxiv.org/abs/2506.01784)
*Shuai Wang,Yinan Yu*

Main category: cs.CL

TL;DR: 提出iQUEST框架，通过迭代分解复杂问题并结合GNN预取邻居信息，有效提升多跳知识推理性能


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在知识密集型场景存在事实错误，整合知识图谱可增强可靠性，但多跳推理面临路径连贯性和关键连接保留的挑战

Method: 迭代式问题分解+图神经网络前瞻机制：将复杂查询拆分为子问题形成结构化推理路径，每个步骤通过GNN预取2跳邻居信息

Result: 在4个基准数据集和4种LLM上实现持续性能提升

Conclusion: iQUEST通过结构化推理轨迹和前瞻性图网络设计，有效解决多跳推理中的路径维护和连接保留问题

Abstract: While Large Language Models (LLMs) excel at many natural language processing
tasks, they often suffer from factual inaccuracies in knowledge-intensive
scenarios. Integrating external knowledge resources, particularly knowledge
graphs (KGs), provides a transparent and updatable foundation for more reliable
reasoning. Knowledge Base Question Answering (KBQA), which queries and reasons
over KGs, is central to this effort, especially for complex, multi-hop queries.
However, multi-hop reasoning poses two key challenges: (1)~maintaining coherent
reasoning paths, and (2)~avoiding prematurely discarding critical multi-hop
connections. To address these issues, we introduce iQUEST, a question-guided
KBQA framework that iteratively decomposes complex queries into simpler
sub-questions, ensuring a structured and focused reasoning trajectory.
Additionally, we integrate a Graph Neural Network (GNN) to look ahead and
incorporate 2-hop neighbor information at each reasoning step. This dual
approach strengthens the reasoning process, enabling the model to explore
viable paths more effectively. Detailed experiments demonstrate the consistent
improvement delivered by iQUEST across four benchmark datasets and four LLMs.

</details>


### [215] [Human-Centric Evaluation for Foundation Models](https://arxiv.org/abs/2506.01793)
*Yijin Guo,Kaiyuan Ji,Xiaorong Zhu,Junying Wang,Farong Wen,Chunyi Li,Zicheng Zhang,Guangtao Zhai*

Main category: cs.CL

TL;DR: 研究提出人类中心评估框架HCE，通过540+次协作实验发现Grok 3表现最佳，建立首个多维度主观评估数据集


<details>
  <summary>Details</summary>
Motivation: 现有基础模型评估过度依赖客观指标，缺乏对人类真实体验的反映，需建立更全面的主观评估体系

Method: 设计HCE三维评估框架（问题解决/信息质量/交互体验），采用参与者驱动模式，在开放研究任务中测试4个主流模型

Result: Grok 3综合表现最优（Deepseek R1和Gemini 2.5次之，OpenAI o3 mini垫底），构建了跨学科主观评估基准数据集

Conclusion: 该框架推动LLM评估标准化，建立自动化评估基础，开源数据集促进研究社区对模型实用价值的系统性评估

Abstract: Currently, nearly all evaluations of foundation models focus on objective
metrics, emphasizing quiz performance to define model capabilities. While this
model-centric approach enables rapid performance assessment, it fails to
reflect authentic human experiences. To address this gap, we propose a
Human-Centric subjective Evaluation (HCE) framework, focusing on three core
dimensions: problem-solving ability, information quality, and interaction
experience. Through experiments involving Deepseek R1, OpenAI o3 mini, Grok 3,
and Gemini 2.5, we conduct over 540 participant-driven evaluations, where
humans and models collaborate on open-ended research tasks, yielding a
comprehensive subjective dataset. This dataset captures diverse user feedback
across multiple disciplines, revealing distinct model strengths and
adaptability. Our findings highlight Grok 3's superior performance, followed by
Deepseek R1 and Gemini 2.5, with OpenAI o3 mini lagging behind. By offering a
novel framework and a rich dataset, this study not only enhances subjective
evaluation methodologies but also lays the foundation for standardized,
automated assessments, advancing LLM development for research and practical
scenarios. Our dataset link is
https://github.com/yijinguo/Human-Centric-Evaluation.

</details>


### [216] [Read it in Two Steps: Translating Extremely Low-Resource Languages with Code-Augmented Grammar Books](https://arxiv.org/abs/2506.01796)
*Chen Zhang,Jiuheng Lin,Xiao Liu,Zekai Zhang,Yansong Feng*

Main category: cs.CL

TL;DR: 将语法规则编码化可使LLM翻译低资源语言时BLEU提升13.1%


<details>
  <summary>Details</summary>
Motivation: 现有研究对语法书在低资源语言翻译中的有效性存在争议，需分解语法规则检索与应用环节进行系统性研究

Method: 1. 构建模块化语法规则数据集ZhuangRules 2. 发现规则检索是瓶颈 3. 提出代码函数表示法提升LLM规则处理能力

Result: 代码规则使语法检索和应用效果显著提升，最终翻译BLEU值提高13.1%

Conclusion: 语法规则编码化有效解决LLM处理复杂规则的困难，代码表示法可增强模型推理能力，为低资源语言翻译提供新思路

Abstract: While large language models (LLMs) have shown promise in translating
extremely low-resource languages using resources like dictionaries, the
effectiveness of grammar books remains debated. This paper investigates the
role of grammar books in translating extremely low-resource languages by
decomposing it into two key steps: grammar rule retrieval and application. To
facilitate the study, we introduce ZhuangRules, a modularized dataset of
grammar rules and their corresponding test sentences. Our analysis reveals that
rule retrieval constitutes a primary bottleneck in grammar-based translation.
Moreover, although LLMs can apply simple rules for translation when explicitly
provided, they encounter difficulties in handling more complex rules. To
address these challenges, we propose representing grammar rules as code
functions, considering their similarities in structure and the benefit of code
in facilitating LLM reasoning. Our experiments show that using code rules
significantly boosts both rule retrieval and application, ultimately resulting
in a 13.1% BLEU improvement in translation.

</details>


### [217] [Propaganda and Information Dissemination in the Russo-Ukrainian War: Natural Language Processing of Russian and Western Twitter Narratives](https://arxiv.org/abs/2506.01807)
*Zaur Gouliev*

Main category: cs.CL

TL;DR: 通过自然语言处理和机器学习分析乌克兰冲突期间社交媒体信息战，揭示宣传账户与可信账户在传播策略、情感倾向和协同行为上的显著差异


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示俄乌冲突中社交媒体信息战的动态机制，分析不同阵营如何利用社交平台塑造公众认知，为数字时代军事冲突中的信息操纵提供实证依据

Method: 使用NLP和ML算法分析40,000条推文（2022.02-05），结合人机协同分析(HITL)进行情感分析、主题建模和行为聚类，识别宣传模式与协同传播网络

Result: 宣传账户偏好情绪化语言/虚假信息（恐惧驱动），西方账户侧重事实/人道主义内容；聚类分析发现协同行为集群，揭示系统性信息操控的证据

Conclusion: 该研究构建了信息战分析框架，证实社交媒体成为现代战争关键战场，其方法论为未来网络空间冲突研究提供可扩展的技术路径

Abstract: The conflict in Ukraine has been not only characterised by military
engagement but also by a significant information war, with social media
platforms like X, formerly known as Twitter playing an important role in
shaping public perception. This article provides an analysis of tweets from
propaganda accounts and trusted accounts collected from the onset of the war,
February 2022 until the middle of May 2022 with n=40,000 total tweets. We
utilise natural language processing and machine learning algorithms to assess
the sentiment and identify key themes, topics and narratives across the dataset
with human-in-the-loop (HITL) analysis throughout. Our findings indicate
distinct strategies in how information is created, spread, and targeted at
different audiences by both sides. Propaganda accounts frequently employ
emotionally charged language and disinformation to evoke fear and distrust,
whereas other accounts, primarily Western tend to focus on factual reporting
and humanitarian aspects of the conflict. Clustering analysis reveals groups of
accounts with similar behaviours, which we suspect indicates the presence of
coordinated efforts. This research attempts to contribute to our understanding
of the dynamics of information warfare and offers techniques for future studies
on social media influence in military conflicts.

</details>


### [218] [NAVER LABS Europe Submission to the Instruction-following Track](https://arxiv.org/abs/2506.01808)
*Beomseok Lee,Marcely Zanon Boito,Laurent Besacier,Ioan Calapodescu*

Main category: cs.CL

TL;DR: NAVER LABS Europe开发了基于SeamlessM4T-v2-large语音编码器和Llama-3.1-8B-Instruct的联合系统，支持英→中/意/德的多任务语音处理


<details>
  <summary>Details</summary>
Motivation: 解决多语言场景下ASR（语音识别）、ST（语音翻译）、SQA（语音问答）任务联合处理的工程需求

Method: 1. 使用SeamlessM4T-v2-large语音编码器构建语音到LLM的嵌入投影器
2. 在Llama-3.1-8B-Instruct基础上通过LoRA适配器进行文本数据训练
3. 多模态联合指令微调1K步

Result: 形成支持多语言多任务处理的最终系统并提交IWSLT 2025评估

Conclusion: 通过预训练模块的联合微调，实现了跨语音-文本模态的多任务指令跟随能力

Abstract: In this paper we describe NAVER LABS Europe submission to the
instruction-following speech processing short track at IWSLT 2025. We
participate in the constrained settings, developing systems that can
simultaneously perform ASR, ST, and SQA tasks from English speech input into
the following target languages: Chinese, Italian, and German. Our solution
leverages two pretrained modules: (1) a speech-to-LLM embedding projector
trained using representations from the SeamlessM4T-v2-large speech encoder; and
(2) LoRA adapters trained on text data on top of a Llama-3.1-8B-Instruct. These
modules are jointly loaded and further instruction-tuned for 1K steps on
multilingual and multimodal data to form our final system submitted for
evaluation.

</details>


### [219] [Analysis of LLM Bias (Chinese Propaganda & Anti-US Sentiment) in DeepSeek-R1 vs. ChatGPT o3-mini-high](https://arxiv.org/abs/2506.01814)
*PeiHsuan Huang,ZihWei Lin,Simon Imbot,WenCheng Fu,Ethan Tu*

Main category: cs.CL

TL;DR: 通过跨语言对比PRC阵营模型DeepSeek-R1与非PRC模型ChatGPT o3-mini-high，研究发现前者在简体中文环境下表现出显著更高的宣传及反美偏见，且隐性偏见渗透至文化领域。


<details>
  <summary>Details</summary>
Motivation: 探究不同地缘政治阵营LLM的意识形态偏差，验证简体中文语境是否强化PRC阵营模型的宣传倾向及其跨语言表现差异。

Method: 构建包含1200个去语境化推理问题的多语言测试集（简中/繁中/英文），采用GPT-4o评分与人工标注结合的混合评估流程，分析7200条模型回答。

Result: DeepSeek-R1的宣传统计比例显著高于对照组，简体中文查询引发最高偏差率（繁中次之，英文最低），且在繁中提问时出现简中回应及PRC术语强化现象。

Conclusion: LLM的意识形态偏差具有模型依赖性、语言敏感性和领域渗透性，PRC阵营模型通过语言载体实现隐性意识形态输出，需警惕技术中立性假象下的认知操纵风险。

Abstract: Large language models (LLMs) increasingly shape public understanding and
civic decisions, yet their ideological neutrality is a growing concern. While
existing research has explored various forms of LLM bias, a direct,
cross-lingual comparison of models with differing geopolitical
alignments-specifically a PRC-system model versus a non-PRC counterpart-has
been lacking. This study addresses this gap by systematically evaluating
DeepSeek-R1 (PRC-aligned) against ChatGPT o3-mini-high (non-PRC) for
Chinese-state propaganda and anti-U.S. sentiment. We developed a novel corpus
of 1,200 de-contextualized, reasoning-oriented questions derived from
Chinese-language news, presented in Simplified Chinese, Traditional Chinese,
and English. Answers from both models (7,200 total) were assessed using a
hybrid evaluation pipeline combining rubric-guided GPT-4o scoring with human
annotation. Our findings reveal significant model-level and language-dependent
biases. DeepSeek-R1 consistently exhibited substantially higher proportions of
both propaganda and anti-U.S. bias compared to ChatGPT o3-mini-high, which
remained largely free of anti-U.S. sentiment and showed lower propaganda
levels. For DeepSeek-R1, Simplified Chinese queries elicited the highest bias
rates; these diminished in Traditional Chinese and were nearly absent in
English. Notably, DeepSeek-R1 occasionally responded in Simplified Chinese to
Traditional Chinese queries and amplified existing PRC-aligned terms in its
Chinese answers, demonstrating an "invisible loudspeaker" effect. Furthermore,
such biases were not confined to overtly political topics but also permeated
cultural and lifestyle content, particularly in DeepSeek-R1.

</details>


### [220] [BD at BEA 2025 Shared Task: MPNet Ensembles for Pedagogical Mistake Identification and Localization in AI Tutor Responses](https://arxiv.org/abs/2506.01817)
*Shadman Rohan,Ishita Sur Apan,Muhtasim Ibteda Shochcho,Md Fahim,Mohammad Ashfaq Ur Rahman,AKM Mahbubur Rahman,Amin Ahsan Ali*

Main category: cs.CL

TL;DR: 团队基于MPNet构建集成模型，通过分组交叉验证与硬投票策略，在BEA 2025教学能力评估任务中实现错误识别(F1=0.711)与定位(F1=0.554)的双赛道优化


<details>
  <summary>Details</summary>
Motivation: 解决教育对话中AI导师对学生错误识别的准确性不足问题，通过集成学习方法提升评估系统的鲁棒性

Method: 1. 采用MPNet结合BERT/XLNet优势
2. 类加权交叉熵损失处理数据不平衡
3. 10折分组交叉验证防止对话数据泄露
4. 硬投票集成最佳模型提升泛化能力

Result: 测试集表现：错误识别宏F1 0.711，错误定位0.554。配套混淆矩阵、t-SNE可视化及错误类型案例库分析

Conclusion: 集成策略有效提升教育对话评估可靠性，可视化分析方法为系统优化提供可解释性依据

Abstract: We present Team BD's submission to the BEA 2025 Shared Task on Pedagogical
Ability Assessment of AI-powered Tutors, under Track 1 (Mistake Identification)
and Track 2 (Mistake Location). Both tracks involve three-class classification
of tutor responses in educational dialogues - determining if a tutor correctly
recognizes a student's mistake (Track 1) and whether the tutor pinpoints the
mistake's location (Track 2). Our system is built on MPNet, a Transformer-based
language model that combines BERT and XLNet's pre-training advantages. We
fine-tuned MPNet on the task data using a class-weighted cross-entropy loss to
handle class imbalance, and leveraged grouped cross-validation (10 folds) to
maximize the use of limited data while avoiding dialogue overlap between
training and validation. We then performed a hard-voting ensemble of the best
models from each fold, which improves robustness and generalization by
combining multiple classifiers. Our approach achieved strong results on both
tracks, with exact-match macro-F1 scores of approximately 0.7110 for Mistake
Identification and 0.5543 for Mistake Location on the official test set. We
include comprehensive analysis of our system's performance, including confusion
matrices and t-SNE visualizations to interpret classifier behavior, as well as
a taxonomy of common errors with examples. We hope our ensemble-based approach
and findings provide useful insights for designing reliable tutor response
evaluation systems in educational dialogue settings.

</details>


### [221] [Not All Jokes Land: Evaluating Large Language Models Understanding of Workplace Humor](https://arxiv.org/abs/2506.01819)
*Moahmmadamin Shafiei,Hamidreza Saffari*

Main category: cs.CL

TL;DR: 研究指出大语言模型在判断职场专业幽默的适当性上存在明显不足，并提出专用数据集解决方案


<details>
  <summary>Details</summary>
Motivation: 现有AI对齐研究主要关注普适性人类价值观，但忽视了专业场景（如职场）中幽默的恰当性判断需求

Method: 构建包含专业幽默语句及其特征标注的数据集，对五种主流大语言模型进行系统评估

Result: 实验显示当前大语言模型对幽默适当性的判断准确率仅为随机水平（20%）

Conclusion: 强调专业场景价值对齐的重要性，提出需要开发针对性的评估框架提升AI的语境理解能力

Abstract: With the recent advances in Artificial Intelligence (AI) and Large Language
Models (LLMs), the automation of daily tasks, like automatic writing, is
getting more and more attention. Hence, efforts have focused on aligning LLMs
with human values, yet humor, particularly professional industrial humor used
in workplaces, has been largely neglected. To address this, we develop a
dataset of professional humor statements along with features that determine the
appropriateness of each statement. Our evaluation of five LLMs shows that LLMs
often struggle to judge the appropriateness of humor accurately.

</details>


### [222] [CiteEval: Principle-Driven Citation Evaluation for Source Attribution](https://arxiv.org/abs/2506.01829)
*Yumo Xu,Peng Qi,Jifan Chen,Kunlun Liu,Rujun Han,Lan Liu,Bonan Min,Vittorio Castelli,Arshit Gupta,Zhiguo Wang*

Main category: cs.CL

TL;DR: 提出CiteEval评估框架及CiteBench基准测试，通过CiteEval-Auto自动化指标全面评估模型生成引用的质量


<details>
  <summary>Details</summary>
Motivation: 现有基于自然语言推理的引用评估方法仅关注二元支持性判断，无法全面反映引用在完整检索上下文中的质量

Method: 构建多领域人工标注基准CiteBench，开发基于模型的CiteEval-Auto指标，通过跨系统实验验证有效性

Result: CiteEval-Auto在多个维度上优于现有指标，能有效捕捉引用的多面性特征

Conclusion: CiteEval框架为模型生成引用的评估与改进提供了原则性强、可扩展的解决方案

Abstract: Citation quality is crucial in information-seeking systems, directly
influencing trust and the effectiveness of information access. Current
evaluation frameworks, both human and automatic, mainly rely on Natural
Language Inference (NLI) to assess binary or ternary supportiveness from cited
sources, which we argue is a suboptimal proxy for citation evaluation. In this
work we introduce CiteEval, a citation evaluation framework driven by
principles focusing on fine-grained citation assessment within a broad context,
encompassing not only the cited sources but the full retrieval context, user
query, and generated text. Guided by the proposed framework, we construct
CiteBench, a multi-domain benchmark with high-quality human annotations on
citation quality. To enable efficient evaluation, we further develop
CiteEval-Auto, a suite of model-based metrics that exhibit strong correlation
with human judgments. Experiments across diverse systems demonstrate
CiteEval-Auto's superior ability to capture the multifaceted nature of
citations compared to existing metrics, offering a principled and scalable
approach to evaluate and improve model-generated citations.

</details>


### [223] [Minimal Pair-Based Evaluation of Code-Switching](https://arxiv.org/abs/2506.01840)
*Igor Sterner,Simone Teufel*

Main category: cs.CL

TL;DR: 提出基于最小对比对的评估方法，通过11种语言对的实验证明：1）双语者始终偏好自然代码切换句子；2）模型规模越大越倾向自然CS句子；3）封闭类词汇操作时概率差异最大


<details>
  <summary>Details</summary>
Motivation: 现有评估方法存在三大局限：语言覆盖有限、无法全面捕捉代码切换现象、难以扩展。需验证LLMs的代码切换行为是否符合双语者模式及理论假设

Method: 1）构建自然CS句子与人工修改变体的最小对比对；2）收集11种语言对各1000对样本；3）开展人类偏好实验与LLMs概率分析对比

Result: 双语实验显示所有语言对均显著偏好自然CS句（p<0.001）。LLMs实验表明：模型参数量与自然CS句概率正相关（r=0.87），封闭类词汇操作时概率差达1.8倍

Conclusion: 该方法有效评估LLMs的代码切换行为，验证了模型规模与人类偏好一致性理论，特别在句法敏感位置（封闭类词）表现最显著

Abstract: There is a lack of an evaluation methodology that estimates the extent to
which large language models (LLMs) use code-switching (CS) in the same way as
bilinguals. Existing methods do not have wide language coverage, fail to
account for the diverse range of CS phenomena, or do not scale. We propose an
intervention based on minimal pairs of CS. Each minimal pair contains one
naturally occurring CS sentence and one minimally manipulated variant. We
collect up to 1,000 such pairs each for 11 language pairs. Our human
experiments show that, for every language pair, bilinguals consistently prefer
the naturally occurring CS sentence. Meanwhile our experiments with current
LLMs show that the larger the model, the more consistently it assigns higher
probability to the naturally occurring CS sentence than to the variant. In
accordance with theoretical claims, the largest probability differences arise
in those pairs where the manipulated material consisted of closed-class words.

</details>


### [224] [Code-Switching and Syntax: A Large-Scale Experiment](https://arxiv.org/abs/2506.01846)
*Igor Sterner,Simone Teufel*

Main category: cs.CL

TL;DR: 基于语法特征的自动系统可有效预测语码转换位置，其表现与双语者相当且能跨语言泛化


<details>
  <summary>Details</summary>
Motivation: 现有理论研究认为语码转换现象可由句法解释，但缺乏跨语言的大规模实验验证

Method: 设计仅依赖句法信息的预测系统，进行多语言跨现象的规模化实验验证

Result: 系统在最小对立句对中达到人类双语者的判别水平，学习到的句法模式具备跨语言泛化能力

Conclusion: 单纯句法信息足以解释语码转换现象，为语言学理论提供了计算实证支持

Abstract: The theoretical code-switching (CS) literature provides numerous pointwise
investigations that aim to explain patterns in CS, i.e. why bilinguals switch
language in certain positions in a sentence more often than in others. A
resulting consensus is that CS can be explained by the syntax of the
contributing languages. There is however no large-scale, multi-language,
cross-phenomena experiment that tests this claim. When designing such an
experiment, we need to make sure that the system that is predicting where
bilinguals tend to switch has access only to syntactic information. We provide
such an experiment here. Results show that syntax alone is sufficient for an
automatic system to distinguish between sentences in minimal pairs of CS, to
the same degree as bilingual humans. Furthermore, the learnt syntactic patterns
generalise well to unseen language pairs.

</details>


### [225] [CONFETTI: Conversational Function-Calling Evaluation Through Turn-Level Interactions](https://arxiv.org/abs/2506.01859)
*Tamer Alkhouli,Katerina Margatina,James Gung,Raphael Shu,Claudia Zaghi,Monica Sunkara,Yi Zhang*

Main category: cs.CL

TL;DR: 提出CONFETTI对话基准，系统性评估大语言模型在复杂对话场景下的函数调用能力，覆盖86个API和多种对话复杂性


<details>
  <summary>Details</summary>
Motivation: 现有基准缺乏对复杂对话场景（如目标修正、模糊需求、长上下文）的全面评估能力

Method: 构建含109个人工模拟对话（313用户轮次）的测试集，采用off-policy逐轮评估框架，结合对话行为标注分析

Result: Nova Pro以40.01%准确率领先，多数模型在长上下文/多API场景表现下降，链式函数调用能力普遍薄弱（<31%）

Conclusion: CONFETTI有效揭示LLM在复杂对话中的局限性，为提升面向API的对话系统提供评估基准，未来需加强链式调用和长上下文处理能力

Abstract: We introduce Conversational Function-Calling Evaluation Through Turn-Level
Interactions (CONFETTI), a conversational benchmark1 designed to evaluate the
function-calling capabilities and response quality of large language models
(LLMs). Current benchmarks lack comprehensive assessment of LLMs in complex
conversational scenarios. CONFETTI addresses this gap through 109
human-simulated conversations, comprising 313 user turns and covering 86 APIs.
These conversations explicitly target various conversational complexities, such
as follow-ups, goal correction and switching, ambiguous and implicit goals. We
perform off-policy turn-level evaluation using this benchmark targeting
function-calling. Our benchmark also incorporates dialog act annotations to
assess agent responses. We evaluate a series of state-of-the-art LLMs and
analyze their performance with respect to the number of available APIs,
conversation lengths, and chained function calling. Our results reveal that
while some models are able to handle long conversations, and leverage more than
20+ APIs successfully, other models struggle with longer context or when
increasing the number of APIs. We also report that the performance on chained
function-calls is severely limited across the models. Overall, the top
performing models on CONFETTI are Nova Pro (40.01%), Claude Sonnet v3.5
(35.46%) and Llama 3.1 405B (33.19%) followed by command-r-plus (31.18%) and
Mistral-Large-2407 (30.07%).

</details>


### [226] [Is Extending Modality The Right Path Towards Omni-Modality?](https://arxiv.org/abs/2506.01872)
*Tinghui Zhu,Kai Zhang,Muhao Chen,Yu Su*

Main category: cs.CL

TL;DR: 通过模态扩展微调可在保持核心语言能力的同时实现全模态，模型合并策略有效促进多模态整合


<details>
  <summary>Details</summary>
Motivation: 研究现有模态扩展方法能否实现真正的全模态，解决现有开源模型局限于特定模态组合、多模态处理效果差的痛点

Method: 通过三组实验分析：1)模态扩展对语言能力的影响 2)模型合并策略的有效性 3)全模态与顺序扩展的知识共享对比

Result: 发现模态扩展不损害语言能力，独立微调的模态模型可通过合并实现全模态，全模态扩展比顺序扩展具有更好的知识共享性

Conclusion: 当前方法通过模态扩展和模型合并可实现全模态，但存在知识迁移的权衡，模型合并是可行集成策略

Abstract: Omni-modal language models (OLMs) aim to integrate and reason over diverse
input modalities--such as text, images, video, and audio--while maintaining
strong language capabilities. Despite recent advancements, existing models,
especially open-source ones, remain far from true omni-modality, struggling to
generalize beyond the specific modality pairs they are trained on or to achieve
strong performance when processing multi-modal inputs. We study the effect of
extending modality, the dominant technique for training multimodal models,
where an off-the-shelf language model is fine-tuned on target-domain and
language data. Specifically, we investigate three key questions: (1) Does
modality extension compromise core language abilities? (2) Can model merging
effectively integrate independently fine-tuned modality-specific models to
achieve omni-modality? (3) Does omni-modality extension lead to better
knowledge sharing and generalization compared to sequential extension? Through
extensive experiments, we analyze these trade-offs and provide insights into
the feasibility of achieving true omni-modality using current approaches.

</details>


### [227] [Spatial Coordinates as a Cell Language: A Multi-Sentence Framework for Imaging Mass Cytometry Analysis](https://arxiv.org/abs/2506.01918)
*Chi-Jane Chen,Yuhang Chen,Sukwon Yun,Natalie Stanley,Tianlong Chen*

Main category: cs.CL

TL;DR: 提出Spatial2Sentence框架，通过多句子表示整合单细胞表达与空间信息，在IMC数据集上实现细胞类型分类准确率提升5.98%、临床预测提升4.18%


<details>
  <summary>Details</summary>
Motivation: 现有单细胞LLMs存在两个缺陷：(1) 无法有效编码空间坐标和上下文信息 (2) 忽视细胞间相互作用，导致生物关系捕捉能力受限

Method: 构建表达相似性和距离矩阵，将空间邻近/表达相似的细胞作为正样本对，空间远离/表达差异的作为负样本对，生成多句子表示并结合多任务学习

Result: 在糖尿病数据集上细胞类型分类提升5.98%，临床状态预测准确率提高4.18%，同时增强模型可解释性

Conclusion: Spatial2Sentence通过自然语言多句子表征有效捕获细胞间相互作用，为空间组学研究提供了性能更优且可解释的分析工具

Abstract: Image mass cytometry (IMC) enables high-dimensional spatial profiling by
combining mass cytometry's analytical power with spatial distributions of cell
phenotypes. Recent studies leverage large language models (LLMs) to extract
cell states by translating gene or protein expression into biological context.
However, existing single-cell LLMs face two major challenges: (1) Integration
of spatial information: they struggle to generalize spatial coordinates and
effectively encode spatial context as text, and (2) Treating each cell
independently: they overlook cell-cell interactions, limiting their ability to
capture biological relationships. To address these limitations, we propose
Spatial2Sentence, a novel framework that integrates single-cell expression and
spatial information into natural language using a multi-sentence approach.
Spatial2Sentence constructs expression similarity and distance matrices,
pairing spatially adjacent and expressionally similar cells as positive pairs
while using distant and dissimilar cells as negatives. These multi-sentence
representations enable LLMs to learn cellular interactions in both expression
and spatial contexts. Equipped with multi-task learning, Spatial2Sentence
outperforms existing single-cell LLMs on preprocessed IMC datasets, improving
cell-type classification by 5.98% and clinical status prediction by 4.18% on
the diabetes dataset while enhancing interpretability. The source code can be
found here: https://github.com/UNITES-Lab/Spatial2Sentence.

</details>


### [228] [From Guidelines to Practice: A New Paradigm for Arabic Language Model Evaluation](https://arxiv.org/abs/2506.01920)
*Serry Sibaee,Omer Nacar,Adel Ammar,Yasser Al-Habashi,Abdulrahman Al-Batati,Wadii Boulila*

Main category: cs.CL

TL;DR: 阿拉伯语模型评估新框架ADMD揭示文化理解对模型性能的关键影响，Claude 3.5 Sonnet以30%准确率领先。


<details>
  <summary>Details</summary>
Motivation: 现有阿拉伯语评估数据集存在语言准确性不足、文化适配性差和方法论缺陷，需建立更严谨的评估体系。

Method: 通过分析现有数据集问题，构建涵盖10大领域的ADMD基准（490题），测试GPT-4等五大模型在文化敏感领域的表现。

Result: 模型表现差异显著（整体最高Claude 3.5 Sonnet/30%），数学理论、阿拉伯语言和伊斯兰领域体现明显文化能力差距。

Conclusion: 研究强调文化能力与技术能力同等重要，为阿拉伯语模型评估提供了理论框架与实践验证双重贡献。

Abstract: This paper addresses critical gaps in Arabic language model evaluation by
establishing comprehensive theoretical guidelines and introducing a novel
evaluation framework. We first analyze existing Arabic evaluation datasets,
identifying significant issues in linguistic accuracy, cultural alignment, and
methodological rigor. To address these limitations in LLMs, we present the
Arabic Depth Mini Dataset (ADMD), a carefully curated collection of 490
challenging questions spanning ten major domains (42 sub-domains, see Figure 1.
Using ADMD, we evaluate five leading language models: GPT-4, Claude 3.5 Sonnet,
Gemini Flash 1.5, CommandR 100B, and Qwen-Max. Our results reveal significant
variations in model performance across different domains, with particular
challenges in areas requiring deep cultural understanding and specialized
knowledge. Claude 3.5 Sonnet demonstrated the highest overall accuracy at 30\%,
showing relative strength in mathematical theory in Arabic, Arabic language,
and islamic domains. This work provides both theoretical foundations and
practical insights for improving Arabic language model evaluation, emphasizing
the importance of cultural competence alongside technical capabilities.

</details>


### [229] [Esoteric Language Models](https://arxiv.org/abs/2506.01928)
*Subham Sekhar Sahoo,Zhihan Yang,Yash Akhauri,Johnna Liu,Deepansha Singh,Zhoujun Cheng,Zhengzhong Liu,Eric Xing,John Thickstun,Arash Vahdat*

Main category: cs.CL

TL;DR: 提出Eso-LMs模型，融合AR与MDM范式，首次实现MDM的KV缓存机制，实现65倍推理加速并在语言建模基准刷新SOTA


<details>
  <summary>Details</summary>
Motivation: 解决现有MDM模型困惑度不及AR模型、缺乏KV缓存导致推理效率低的核心痛点

Method: 通过半自回归架构融合两种范式，开发支持并行生成的KV缓存系统，结合自适应采样调度算法优化

Result: 在标准基准测试中达到SOTA，推理速度较原生MDM提升65倍，较同类半自回归方法快4倍

Conclusion: 成功突破AR与扩散模型的传统对立，开创了兼顾生成质量与推理效率的新范式

Abstract: Diffusion-based language models offer a compelling alternative to
autoregressive (AR) models by enabling parallel and controllable generation.
Among this family of models, Masked Diffusion Models (MDMs) achieve the
strongest performance but still underperform AR models in perplexity and lack
key inference-time efficiency features--most notably, KV caching. In this work,
we introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms,
enabling smooth interpolation between their perplexities while overcoming their
respective limitations. Eso-LMs set a new state of the art on standard language
modeling benchmarks. Crucially, we are the **first to introduce KV caching for
MDMs** while preserving parallel generation, significantly improving inference
efficiency. Combined with an optimized sampling schedule, our method achieves
up to **65x** faster inference than standard MDMs and **4x** faster inference
than prior semi-autoregressive approaches. We provide the code and model
checkpoints on the project page:
[http://s-sahoo.github.io/Eso-LMs](http://s-sahoo.github.io/Eso-LMs)

</details>


### [230] [RewardBench 2: Advancing Reward Model Evaluation](https://arxiv.org/abs/2506.01937)
*Saumya Malik,Valentina Pyatkin,Sander Land,Jacob Morrison,Noah A. Smith,Hannaneh Hajishirzi,Nathan Lambert*

Main category: cs.CL

TL;DR: RewardBench 2是一个新型多技能奖励建模基准，通过引入更具挑战性的人类提示数据，显著提升奖励模型评估的严谨性，并与下游任务表现高度相关（平均得分比前代低20分）。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型评估基准在反映下游任务效果方面存在不足，简单对齐算法常优于复杂奖励模型。需建立更有效的评估体系来弥合评估与实践的差距。

Method: 构建包含全新人类提示的评估基准，避免使用下游任务现有提示。通过量化基准表现与best-of-N采样、PPO等算法的下游性能相关性验证有效性。

Result: RewardBench 2使模型平均得分降低20分，但与下游应用性能的相关系数达0.87，显著优于其他基准（相关系数0.53-0.72）。

Conclusion: 该基准为奖励模型评估提供了更严格且预测性更强的标准，揭示了当前模型在复杂场景中的局限性，为改进对齐算法提供了新方向。

Abstract: Reward models are used throughout the post-training of language models to
capture nuanced signals from preference data and provide a training target for
optimization across instruction following, reasoning, safety, and more domains.
The community has begun establishing best practices for evaluating reward
models, from the development of benchmarks that test capabilities in specific
skill areas to others that test agreement with human preferences. At the same
time, progress in evaluation has not been mirrored by the effectiveness of
reward models in downstream tasks -- simpler direct alignment algorithms are
reported to work better in many cases. This paper introduces RewardBench 2, a
new multi-skill reward modeling benchmark designed to bring new, challenging
data for accuracy-based reward model evaluation -- models score about 20 points
on average lower on RewardBench 2 compared to the first RewardBench -- while
being highly correlated with downstream performance. Compared to most other
benchmarks, RewardBench 2 sources new human prompts instead of existing prompts
from downstream evaluations, facilitating more rigorous evaluation practices.
In this paper, we describe our benchmark construction process and report how
existing models perform on it, while quantifying how performance on the
benchmark correlates with downstream use of the models in both inference-time
scaling algorithms, like best-of-N sampling, and RLHF training algorithms like
proximal policy optimization.

</details>


### [231] [Novel Benchmark for NER in the Wastewater and Stormwater Domain](https://arxiv.org/abs/2506.01938)
*Franco Alberto Cardillo,Franca Debole,Francesca Frontini,Mitra Aelami,Nanée Chahinian,Serge Conrad*

Main category: cs.CL

TL;DR: 开发法意双语废水管理语料库，评估NER方法并提出自动标注投影方案


<details>
  <summary>Details</summary>
Motivation: 解决跨语言领域文本中结构化知识提取的技术挑战，建立多语言评估基准

Method: 构建专业领域双语语料库，测试基于LLM的SOTA NER方法，探索标注自动迁移技术

Result: 为多语言NER提供可靠基线，验证自动化标注在语料扩展中的可行性

Conclusion: 建立了可扩展的多语言领域评估框架，为跨语言知识提取提供方法学参考

Abstract: Effective wastewater and stormwater management is essential for urban
sustainability and environmental protection. Extracting structured knowledge
from reports and regulations is challenging due to domainspecific terminology
and multilingual contexts. This work focuses on domain-specific Named Entity
Recognition (NER) as a first step towards effective relation and information
extraction to support decision making. A multilingual benchmark is crucial for
evaluating these methods. This study develops a French-Italian domain-specific
text corpus for wastewater management. It evaluates state-of-the-art NER
methods, including LLM-based approaches, to provide a reliable baseline for
future strategies and explores automated annotation projection in view of an
extension of the corpus to new languages.

</details>


### [232] [Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.01939)
*Shenzhi Wang,Le Yu,Chang Gao,Chujie Zheng,Shixuan Liu,Rui Lu,Kai Dang,Xionghui Chen,Jianxin Yang,Zhenru Zhang,Yuqiong Liu,An Yang,Andrew Zhao,Yang Yue,Shiji Song,Bowen Yu,Gao Huang,Junyang Lin*

Main category: cs.CL

TL;DR: RLVR通过优化决定推理方向的高熵令牌（仅占20%），显著提升LLM推理性能，32B模型AIME分数提升+11.04。


<details>
  <summary>Details</summary>
Motivation: 揭示强化学习可验证奖励机制对LLM推理能力提升的核心机制，突破传统全参数更新范式。

Method: 通过令牌熵模式分析，追踪CoT推理中高熵分岔令牌的演化规律，设计分岔令牌梯度限定训练策略。

Result: Qwen3-32B模型仅用20%高熵令牌训练，AIME'25提升+11.04；反观低熵令牌训练导致性能显著下降。

Conclusion: RLVR有效性源于优化推理路径分岔点的高熵令牌，为模型高效训练提供新方向——聚焦关键令牌而非全局参数。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful approach to enhancing the reasoning capabilities of Large Language
Models (LLMs), while its mechanisms are not yet well understood. In this work,
we undertake a pioneering exploration of RLVR through the novel perspective of
token entropy patterns, comprehensively analyzing how different tokens
influence reasoning performance. By examining token entropy patterns in
Chain-of-Thought (CoT) reasoning, we observe that only a small fraction of
tokens exhibit high entropy, and these tokens act as critical forks that steer
the model toward diverse reasoning pathways. Furthermore, studying how entropy
patterns evolve during RLVR training reveals that RLVR largely adheres to the
base model's entropy patterns, primarily adjusting the entropy of high-entropy
tokens. These findings highlight the significance of high-entropy tokens (i.e.,
forking tokens) to RLVR. We ultimately improve RLVR by restricting policy
gradient updates to forking tokens and uncover a finding even beyond the 80/20
rule: utilizing only 20% of the tokens while maintaining performance comparable
to full-gradient updates on the Qwen3-8B base model and significantly
surpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71
on AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models,
highlighting a strong scaling trend. In contrast, training exclusively on the
80% lowest-entropy tokens leads to a marked decline in performance. These
findings indicate that the efficacy of RLVR primarily arises from optimizing
the high-entropy tokens that decide reasoning directions. Collectively, our
results highlight the potential to understand RLVR through a token-entropy
perspective and optimize RLVR by leveraging high-entropy minority tokens to
further improve LLM reasoning.

</details>


### [233] [Self-ensemble: Mitigating Confidence Distortion for Large Language Models](https://arxiv.org/abs/2506.01951)
*Zicheng Xu,Guanchu Wang,Guangyao Zheng,Yu-Neng Chuang,Alexander Szalay,Xia Hu,Vladimir Braverman*

Main category: cs.CL

TL;DR: 提出Self-ensemble方法解决LLMs在多选问答中的信心失真问题


<details>
  <summary>Details</summary>
Motivation: LLMs在选项数量增加时出现正确预测信心不足和错误预测过度自信的问题

Method: 将选项分组并通过注意力掩码和位置编码实现无需调参的预测集成

Result: 在三个LLMs和数据集上全面超越标准推理和基线方法

Conclusion: Self-ensemble通过即插即用架构有效解决LLM信心失真问题

Abstract: Although Large Language Models (LLMs) perform well in general fields, they
exhibit a confidence distortion problem on multi-choice question-answering
(MCQA), particularly as the number of answer choices increases. Specifically,
on MCQA with many choices, LLMs suffer from under-confidence in correct
predictions and over-confidence in incorrect ones, leading to a substantially
degraded performance. To solve this problem, we propose Self-ensemble in this
work. Our method splits the choices into several groups and ensembles LLM
predictions across these groups to reach a final decision. The advantage of
Self-ensemble is its plug-and-play nature, where it can be integrated into
existing LLM architecture based on a designed attention mask and positional
encoding, without requiring labeled datasets for parameter tuning. Experimental
results on three LLMs and datasets demonstrate that Self-ensemble
comprehensively addresses the confidence distortion problem of LLMs,
outperforming standard inference as well as baseline methods.

</details>


### [234] [WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web Tasks](https://arxiv.org/abs/2506.01952)
*Atsuyuki Miyai,Zaiying Zhao,Kazuki Egashira,Atsuki Sato,Tatsumi Sunada,Shota Onohara,Hiromasa Yamanishi,Mashiro Toyooka,Kunato Nishina,Ryoma Maeda,Kiyoharu Aizawa,Toshihiko Yamasaki*

Main category: cs.CL

TL;DR: 提出WebChoreArena基准测试，专注于评估网络代理处理复杂、繁琐任务的能力


<details>
  <summary>Details</summary>
Motivation: 现有WebArena基准主要面向通用浏览任务，但实际应用中存在大量需要处理复杂/繁琐任务的场景，需要新的评估体系衡量代理能力

Method: 构建包含532个任务的基准测试，整合大规模记忆检索、精确数学计算、跨页面长期记忆三大挑战，基于WebArena四个模拟环境确保可重复性

Result: 实验显示GPT-4o/Claude 3.7/Gemini 2.5 Pro性能显著提升，但相比WebArena仍有23.5%差距，证明新基准难度更高

Conclusion: WebChoreArena能有效衡量LLM进展，但其设计挑战显著高于现有基准，揭示当前模型在复杂任务处理上的局限性

Abstract: Powered by a large language model (LLM), a web browsing agent operates web
browsers in a human-like manner and offers a highly transparent path toward
automating a wide range of everyday tasks. As web agents become increasingly
capable and demonstrate proficiency in general browsing tasks, a critical
question emerges: Can they go beyond general browsing to robustly handle tasks
that are tedious and complex, or chores that humans often avoid doing
themselves? In this paper, we introduce WebChoreArena, a new fully reproducible
benchmark comprising 532 carefully curated tasks designed to extend the scope
of WebArena beyond general browsing to more labor-intensive and tedious tasks.
WebChoreArena systematically integrates three key challenges: (i) Massive
Memory tasks requiring accurate retrieval of large amounts of information in
the observations, (ii) Calculation tasks demanding precise mathematical
reasoning, and (iii) Long-Term Memory tasks necessitating long-term memory
across multiple webpages. Built on top of the fully reproducible and widely
adopted four WebArena simulation environments, WebChoreArena ensures strict
reproducibility and enables fair, direct comparisons with the established
WebArena benchmark, offering key insights into agent progress. Our experimental
results demonstrate that as LLMs evolve, represented by GPT-4o, Claude 3.7
Sonnet, and Gemini 2.5 Pro, significant improvements in performance are
observed on WebChoreArena. These findings suggest that WebChoreArena is
well-suited to measure the advancement of state-of-the-art LLMs with greater
clarity. Nevertheless, the results also indicate that even with Gemini 2.5 Pro,
there remains substantial room for improvement compared to WebArena,
highlighting the increased challenges posed by WebChoreArena.

</details>


### [235] [DRAG: Distilling RAG for SLMs from LLMs to Transfer Knowledge and Mitigate Hallucination via Evidence and Graph-based Distillation](https://arxiv.org/abs/2506.01954)
*Jennifer Chen,Aidar Myrzakhan,Yaxin Luo,Hassaan Muhammad Khan,Sondos Mahmoud Bsharat,Zhiqiang Shen*

Main category: cs.CL

TL;DR: 提出DRAG框架，通过基于证据和知识图的蒸馏方法将RAG知识从大模型迁移至小模型，在降低资源消耗的同时提升事实准确性。


<details>
  <summary>Details</summary>
Motivation: 传统大规模RAG系统存在高计算资源消耗和幻觉生成问题，需寻求资源高效且可靠的替代方案。

Method: 采用知识图对齐和证据排序的蒸馏机制，通过结构化知识保持和排名证据约束实现模型压缩。

Result: 实验显示在相同模型下性能超越MiniRAG达27.7%，且保持高效推理（约0.4秒/query）。

Conclusion: DRAG为小模型部署增强的检索生成能力提供了资源高效的实践路径，平衡性能与隐私/成本需求。

Abstract: Retrieval-Augmented Generation (RAG) methods have proven highly effective for
tasks requiring factual consistency and robust knowledge retrieval. However,
large-scale RAG systems consume significant computational resources and are
prone to generating hallucinated content from Humans. In this work, we
introduce $\texttt{DRAG}$, a novel framework for distilling RAG knowledge from
large-scale Language Models (LLMs) into small LMs (SLMs). Our approach
leverages evidence- and knowledge graph-based distillation, ensuring that the
distilled model retains critical factual knowledge while significantly reducing
model size and computational cost. By aligning the smaller model's predictions
with a structured knowledge graph and ranked evidence, $\texttt{DRAG}$
effectively mitigates hallucinations and improves factual accuracy. We further
present a case demonstrating how our framework mitigates user privacy risks and
introduce a corresponding benchmark. Experimental evaluations on multiple
benchmarks demonstrate that our method outperforms the prior competitive RAG
methods like MiniRAG for SLMs by up to 27.7% using the same models, preserving
high-level efficiency and reliability. With $\texttt{DRAG}$, we provide a
practical and resource-efficient roadmap to deploying enhanced retrieval and
generation capabilities in small-sized LLMs.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [236] [MotionPersona: Characteristics-aware Locomotion Control](https://arxiv.org/abs/2506.00173)
*Mingyi Shi,Wei Liu,Jidong Mei,Wangpok Tse,Rui Chen,Xuelin Chen,Taku Komura*

Main category: cs.GR

TL;DR: MotionPersona推出实时角色控制系统，通过用户定义属性生成特征感知动作，支持文本和动作片段双条件控制。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习控制器生成同质化动作，无法反映真实人类因不同特征（如年龄/心理状态）产生的动作差异。

Method: 开发基于SMPLX参数、文本提示和运动控制信号的块自回归扩散模型，构建包含多样化运动类型和角色特征的数据集，并提出小样本特征提取技术。

Result: 在特征感知运动生成任务中优于现有方法，实现实时响应并精确反映用户定义特征（如老人拖步行走）。

Conclusion: 首个能同时实现实时动态响应与特征精确表达的角色控制系统，通过双模态条件机制扩展了动画生成的可能性。

Abstract: We present MotionPersona, a novel real-time character controller that allows
users to characterize a character by specifying attributes such as physical
traits, mental states, and demographics, and projects these properties into the
generated motions for animating the character. In contrast to existing deep
learning-based controllers, which typically produce homogeneous animations
tailored to a single, predefined character, MotionPersona accounts for the
impact of various traits on human motion as observed in the real world. To
achieve this, we develop a block autoregressive motion diffusion model
conditioned on SMPLX parameters, textual prompts, and user-defined locomotion
control signals. We also curate a comprehensive dataset featuring a wide range
of locomotion types and actor traits to enable the training of this
characteristic-aware controller. Unlike prior work, MotionPersona is the first
method capable of generating motion that faithfully reflects user-specified
characteristics (e.g., an elderly person's shuffling gait) while responding in
real time to dynamic control inputs. Additionally, we introduce a few-shot
characterization technique as a complementary conditioning mechanism, enabling
customization via short motion clips when language prompts fall short. Through
extensive experiments, we demonstrate that MotionPersona outperforms existing
methods in characteristics-aware locomotion control, achieving superior motion
quality and diversity. Results, code, and demo can be found at:
https://motionpersona25.github.io/.

</details>


### [237] [Power-Linear Polar Directional Fields](https://arxiv.org/abs/2506.00222)
*Jiabao Brad Wang,Amir Vaxman*

Main category: cs.GR

TL;DR: 提出基于分段幂线性表示的新型方向场设计方法，支持任意位置奇点指定并减少网格质量影响


<details>
  <summary>Details</summary>
Motivation: 传统方法在粗糙/不均匀网格上易产生伪影，且缺乏对奇点位置和拓扑结构的灵活控制

Method: 使用相位与尺度的分段幂线性表示方法，实现精确的场拓扑控制与平滑性保证

Result: 在不同拓扑结构和三角网格质量的模型上验证有效性，显著降低网格质量导致的伪影

Conclusion: 该方法突破了网格质量对方向场设计的限制，为几何处理提供了更灵活鲁棒的解决方案

Abstract: We introduce a novel method for directional-field design on meshes, enabling
users to specify singularities at any location on a mesh. Our method uses a
piecewise power-linear representation for phase and scale, offering precise
control over field topology. The resulting fields are smooth and accommodate
any singularity index and field symmetry. With this representation, we mitigate
the artifacts caused by coarse or uneven meshes. We showcase our approach on
meshes with diverse topologies and triangle qualities.

</details>


### [238] [Pro3D-Editor : A Progressive-Views Perspective for Consistent and Precise 3D Editing](https://arxiv.org/abs/2506.00512)
*Yang Zheng,Mengqi Huang,Nan Chen,Zhendong Mao*

Main category: cs.GR

TL;DR: 提出Pro3D-Editor框架，通过渐进式视图传播语义实现精确一致的文本引导3D编辑


<details>
  <summary>Details</summary>
Motivation: 现有方法不加区分编辑多视图导致不一致，需通过编辑显著视图向稀疏视图传播语义

Method: 包含三组件：动态采样显著视图的Primary-view Sampler，基于MoVE-LoRA传播语义的Key-view Render，以及优化3D对象的Full-view Refiner

Result: 大量实验证明该方法在编辑精度和空间一致性上优于现有方法

Conclusion: 渐进式视图范式有效解决多视图编辑一致性问题，可推动游戏/影视等领域的3D编辑应用

Abstract: Text-guided 3D editing aims to precisely edit semantically relevant local 3D
regions, which has significant potential for various practical applications
ranging from 3D games to film production. Existing methods typically follow a
view-indiscriminate paradigm: editing 2D views indiscriminately and projecting
them back into 3D space. However, they overlook the different cross-view
interdependencies, resulting in inconsistent multi-view editing. In this study,
we argue that ideal consistent 3D editing can be achieved through a
\textit{progressive-views paradigm}, which propagates editing semantics from
the editing-salient view to other editing-sparse views. Specifically, we
propose \textit{Pro3D-Editor}, a novel framework, which mainly includes
Primary-view Sampler, Key-view Render, and Full-view Refiner. Primary-view
Sampler dynamically samples and edits the most editing-salient view as the
primary view. Key-view Render accurately propagates editing semantics from the
primary view to other key views through its Mixture-of-View-Experts Low-Rank
Adaption (MoVE-LoRA). Full-view Refiner edits and refines the 3D object based
on the edited multi-views. Extensive experiments demonstrate that our method
outperforms existing methods in editing accuracy and spatial consistency.

</details>


### [239] [Neural Path Guiding with Distribution Factorization](https://arxiv.org/abs/2506.00839)
*Pedro Figueiredo,Qihao He,Nima Khademi Kalantari*

Main category: cs.GR

TL;DR: 提出神经路径引导方法，将2D方向分布分解为两个1D概率分布函数，通过神经网络建模实现高效蒙特卡洛积分


<details>
  <summary>Details</summary>
Motivation: 现有神经方法无法同时兼顾计算速度与分布表达能力，亟需开发既高效又富有表达力的渲染解决方案

Method: 1) 分解2D方向分布为两个1D概率分布函数 2) 使用离散坐标神经网络建模 3) 引入辐射缓存网络降低梯度方差

Result: 在复杂光传输场景下，本方法综合性能优于现有技术方案

Conclusion: 神经离散坐标建模方法在保持实时性的同时提升了分布表达能力，为蒙特卡洛积分提供了更高效的解决方案

Abstract: In this paper, we present a neural path guiding method to aid with Monte
Carlo (MC) integration in rendering. Existing neural methods utilize
distribution representations that are either fast or expressive, but not both.
We propose a simple, but effective, representation that is sufficiently
expressive and reasonably fast. Specifically, we break down the 2D distribution
over the directional domain into two 1D probability distribution functions
(PDF). We propose to model each 1D PDF using a neural network that estimates
the distribution at a set of discrete coordinates. The PDF at an arbitrary
location can then be evaluated and sampled through interpolation. To train the
network, we maximize the similarity of the learned and target distributions. To
reduce the variance of the gradient during optimizations and estimate the
normalization factor, we propose to cache the incoming radiance using an
additional network. Through extensive experiments, we demonstrate that our
approach is better than the existing methods, particularly in challenging
scenes with complex light transport.

</details>


### [240] [Hybridizing Expressive Rendering: Stroke-Based Rendering with Classic and Neural Methods](https://arxiv.org/abs/2506.00870)
*Kapil Dev*

Main category: cs.GR

TL;DR: 对比分析传统与基于神经网络的非真实感渲染技术，提出结合两者的混合框架以拓展表达性渲染的可能性


<details>
  <summary>Details</summary>
Motivation: 深度学习的兴起改变了传统NPR范式，需系统比较两种技术路线的优劣，探索融合方案以平衡质量与艺术控制

Method: 通过对比分析基于笔触渲染（SBR）的传统算法与神经网络方法，建立评估框架并提出协同工作流程

Result: 揭示两种技术路线在风格控制、计算效率和艺术表现力的互补性，验证混合框架的创新价值

Conclusion: 传统NPR与神经渲染的有机结合为数字艺术创作开辟了新维度，实现了算法控制力与生成能力的协同增强

Abstract: Non-Photorealistic Rendering (NPR) has long been used to create artistic
visualizations that prioritize style over realism, enabling the depiction of a
wide range of aesthetic effects, from hand-drawn sketches to painterly
renderings. While classical NPR methods, such as edge detection, toon shading,
and geometric abstraction, have been well-established in both research and
practice, with a particular focus on stroke-based rendering, the recent rise of
deep learning represents a paradigm shift. We analyze the similarities and
differences between classical and neural network based NPR techniques, focusing
on stroke-based rendering (SBR), highlighting their strengths and limitations.
We discuss trade offs in quality and artistic control between these paradigms,
propose a framework where these approaches can be combined for new
possibilities in expressive rendering.

</details>


### [241] [LensCraft: Your Professional Virtual Cinematographer](https://arxiv.org/abs/2506.00988)
*Zahra Dehghanian,Morteza Abolghasemi,Hossein Azizinaghsh,Amir Vahedi,Hamid Beigy,Hamid R. Rabiee*

Main category: cs.GR

TL;DR: LensCraft通过数据驱动的摄影专家系统，结合高保真模拟训练与轻量实时架构，解决了自动化拍摄系统中创意意图与机械执行的矛盾。


<details>
  <summary>Details</summary>
Motivation: 现有自动化拍摄系统忽视拍摄对象的三维体积与动态行为，导致空间感知不足且无法适应实时动态场景。

Method: 1. 专用模拟框架生成高保真训练数据
2. 结合摄影原则与动态适应的神经网络模型
3. 支持文本提示/轨迹/关键点等多模态输入控制

Result: 在静态/动态场景中实现97.8%的拍摄意图匹配率，推理速度提升3.2倍，模型体积减少58%（相较SOTA模型）

Conclusion: 通过完整开源数据集、模拟环境与模型权重，为智能拍摄系统建立新标杆，验证了体积感知与实时适应对创意表达的关键作用。

Abstract: Digital creators, from indie filmmakers to animation studios, face a
persistent bottleneck: translating their creative vision into precise camera
movements. Despite significant progress in computer vision and artificial
intelligence, current automated filming systems struggle with a fundamental
trade-off between mechanical execution and creative intent. Crucially, almost
all previous works simplify the subject to a single point-ignoring its
orientation and true volume-severely limiting spatial awareness during filming.
LensCraft solves this problem by mimicking the expertise of a professional
cinematographer, using a data-driven approach that combines cinematographic
principles with the flexibility to adapt to dynamic scenes in real time. Our
solution combines a specialized simulation framework for generating
high-fidelity training data with an advanced neural model that is faithful to
the script while being aware of the volume and dynamic behavior of the subject.
Additionally, our approach allows for flexible control via various input
modalities, including text prompts, subject trajectory and volume, key points,
or a full camera trajectory, offering creators a versatile tool to guide camera
movements in line with their vision. Leveraging a lightweight real time
architecture, LensCraft achieves markedly lower computational complexity and
faster inference while maintaining high output quality. Extensive evaluation
across static and dynamic scenarios reveals unprecedented accuracy and
coherence, setting a new benchmark for intelligent camera systems compared to
state-of-the-art models. Extended results, the complete dataset, simulation
environment, trained model weights, and source code are publicly accessible on
LensCraft Webpage.

</details>


### [242] [TRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal Interaction in Digital Humans](https://arxiv.org/abs/2506.01077)
*Yueqian Guo,Tianzhao Li,Xin Lyu,Jiehaolin Chen,Zhaohan Wang,Sirui Xiao,Yurun Chen,Yezi He,Helin Li,Fan Zhang*

Main category: cs.GR

TL;DR: 提出TRiMM框架解决LLM驱动数字人实时手势生成的难题，通过跨模态对齐、长上下文建模和动作库匹配实现120fps的实时响应


<details>
  <summary>Details</summary>
Motivation: 现有协同语音手势生成系统存在实时性差和长文本理解困难的问题，需要兼顾生成速度与质量的新方案

Method: 1) 跨模态注意力机制实现语音-手势时序对齐；2) 滑动窗口自回归模型处理长文本；3) 构建原子动作库实现实时手势匹配；虚幻引擎轻量化实现

Result: 消费级显卡达到120fps实时推理，单句延迟0.15秒，在ZEGGS和BEAT数据集上主观/客观评估均超越SOTA方法

Conclusion: TRiMM在保证手势质量前提下突破生成速度瓶颈，使LLM数字人能够实时响应语音并合成对应手势

Abstract: Large Language Model (LLM)-driven digital humans have sparked a series of
recent studies on co-speech gesture generation systems. However, existing
approaches struggle with real-time synthesis and long-text comprehension. This
paper introduces Transformer-Based Rich Motion Matching (TRiMM), a novel
multi-modal framework for real-time 3D gesture generation. Our method
incorporates three modules: 1) a cross-modal attention mechanism to achieve
precise temporal alignment between speech and gestures; 2) a long-context
autoregressive model with a sliding window mechanism for effective sequence
modeling; 3) a large-scale gesture matching system that constructs an atomic
action library and enables real-time retrieval. Additionally, we develop a
lightweight pipeline implemented in the Unreal Engine for experimentation. Our
approach achieves real-time inference at 120 fps and maintains a per-sentence
latency of 0.15 seconds on consumer-grade GPUs (Geforce RTX3060). Extensive
subjective and objective evaluations on the ZEGGS, and BEAT datasets
demonstrate that our model outperforms current state-of-the-art methods. TRiMM
enhances the speed of co-speech gesture generation while ensuring gesture
quality, enabling LLM-driven digital humans to respond to speech in real time
and synthesize corresponding gestures. Our code is available at
https://github.com/teroon/TRiMM-Transformer-Based-Rich-Motion-Matching

</details>


### [243] [PromptVFX: Text-Driven Fields for Open-World 3D Gaussian Animation](https://arxiv.org/abs/2506.01091)
*Mert Kiray,Paul Uhlenbruck,Nassir Navab,Benjamin Busam*

Main category: cs.GR

TL;DR: 提出基于文本驱动的4D流场预测框架，利用大语言模型实时生成3D视觉特效，降低专业门槛并提升创作效率。


<details>
  <summary>Details</summary>
Motivation: 传统3D特效制作依赖专业软件且耗时，现有生成方法（如扩散模型）存在计算开销大、4D推理速度慢的问题。

Method: 将3D动画重构为场预测任务，通过LLMs/VLMs解析文本指令生成时间序列的4D流场，直接驱动3D高斯模型的颜色/透明度/位置更新。

Result: 实现消费级设备实时渲染（包括网页端），无需网格提取/物理模拟，通过简单文本指令即可生成动态特效（如花瓶发光爆炸）。

Conclusion: 为语言驱动的3D内容创作提供高效路径，推动视觉特效制作的民主化进程。

Abstract: Visual effects (VFX) are key to immersion in modern films, games, and AR/VR.
Creating 3D effects requires specialized expertise and training in 3D animation
software and can be time consuming. Generative solutions typically rely on
computationally intense methods such as diffusion models which can be slow at
4D inference. We reformulate 3D animation as a field prediction task and
introduce a text-driven framework that infers a time-varying 4D flow field
acting on 3D Gaussians. By leveraging large language models (LLMs) and
vision-language models (VLMs) for function generation, our approach interprets
arbitrary prompts (e.g., "make the vase glow orange, then explode") and
instantly updates color, opacity, and positions of 3D Gaussians in real time.
This design avoids overheads such as mesh extraction, manual or physics-based
simulations and allows both novice and expert users to animate volumetric
scenes with minimal effort on a consumer device even in a web browser.
Experimental results show that simple textual instructions suffice to generate
compelling time-varying VFX, reducing the manual effort typically required for
rigging or advanced modeling. We thus present a fast and accessible pathway to
language-driven 3D content creation that can pave the way to democratize VFX
further.

</details>


### [244] [WishGI: Lightweight Static Global Illumination Baking via Spherical Harmonics Fitting](https://arxiv.org/abs/2506.01288)
*Junke Zhu,Zehan Wu,Qixing Zhang,Cheng Liao,Zhangjin Huang*

Main category: cs.GR

TL;DR: 提出基于球谐函数拟合和逆向探针分布方法的全局光照方案，在仅需5%内存消耗下实现高质量照明效果


<details>
  <summary>Details</summary>
Motivation: 现有静态全局光照技术过度依赖纹理存储和像素级采样，导致高内存占用和性能开销，难以适配低端硬件平台

Method: 1. 采用球谐函数拟合烘焙光照信息
2. 提出逆向探针分布方法为每个网格生成唯一探针关联
3. 在局部空间离线生成关联数据实现多实例光照一致性

Result: 内存占用仅为主流工业方案的5%，同时保持高度竞争力的光照质量

Conclusion: 该方案通过创新的数据组织和烘焙方式，在保证视觉效果的前提下显著降低内存需求，特别适合移动端等计算资源受限平台

Abstract: Global illumination combines direct and indirect lighting to create realistic
lighting effects, bringing virtual scenes closer to reality. Static global
illumination is a crucial component of virtual scene rendering, leveraging
precomputation and baking techniques to significantly reduce runtime
computational costs. Unfortunately, many existing works prioritize visual
quality by relying on extensive texture storage and massive pixel-level texture
sampling, leading to large performance overhead. In this paper, we introduce an
illumination reconstruction method that effectively reduces sampling in
fragment shader and avoids additional render passes, making it well-suited for
low-end platforms. To achieve high-quality global illumination with reduced
memory usage, we adopt a spherical harmonics fitting approach for baking
effective illumination information and propose an inverse probe distribution
method that generates unique probe associations for each mesh. This
association, which can be generated offline in the local space, ensures
consistent lighting quality across all instances of the same mesh. As a
consequence, our method delivers highly competitive lighting effects while
using only approximately 5% of the memory required by mainstream industry
techniques.

</details>


### [245] [Silence is Golden: Leveraging Adversarial Examples to Nullify Audio Control in LDM-based Talking-Head Generation](https://arxiv.org/abs/2506.01591)
*Yuan Gan,Jiaxu Miao,Yunze Wang,Yi Yang*

Main category: cs.GR

TL;DR: 提出Silencer方法通过无效化音频控制与抗净化扰动，主动保护肖像隐私以对抗基于LDM的说话人生成技术滥用风险


<details>
  <summary>Details</summary>
Motivation: 现有防御方法无法防止音频操控攻击且易被扩散净化技术消除扰动，需解决LDM技术滥用带来的伦理安全问题

Method: 两阶段方法：1) 无效化损失消除音频控制 2) 抗净化损失优化潜空间特征生成鲁棒扰动

Result: 实验验证Silencer在保护肖像隐私方面显著有效

Conclusion: 本研究揭示了说话人生成技术的伦理风险，为AI安全社区提供了主动防御方案并推动伦理意识提升

Abstract: Advances in talking-head animation based on Latent Diffusion Models (LDM)
enable the creation of highly realistic, synchronized videos. These fabricated
videos are indistinguishable from real ones, increasing the risk of potential
misuse for scams, political manipulation, and misinformation. Hence, addressing
these ethical concerns has become a pressing issue in AI security. Recent
proactive defense studies focused on countering LDM-based models by adding
perturbations to portraits. However, these methods are ineffective at
protecting reference portraits from advanced image-to-video animation. The
limitations are twofold: 1) they fail to prevent images from being manipulated
by audio signals, and 2) diffusion-based purification techniques can
effectively eliminate protective perturbations. To address these challenges, we
propose Silencer, a two-stage method designed to proactively protect the
privacy of portraits. First, a nullifying loss is proposed to ignore audio
control in talking-head generation. Second, we apply anti-purification loss in
LDM to optimize the inverted latent feature to generate robust perturbations.
Extensive experiments demonstrate the effectiveness of Silencer in proactively
protecting portrait privacy. We hope this work will raise awareness among the
AI security community regarding critical ethical issues related to talking-head
generation techniques. Code: https://github.com/yuangan/Silencer.

</details>


### [246] [Image Generation from Contextually-Contradictory Prompts](https://arxiv.org/abs/2506.01929)
*Saar Huberman,Or Patashnik,Omer Dahary,Ron Mokady,Daniel Cohen-Or*

Main category: cs.GR

TL;DR: 提出阶段感知提示分解框架，利用大语言模型解决文本到图像生成中上下文矛盾问题，显著提升图文对齐效果


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在处理含有概念矛盾的提示时，常因上下文矛盾导致生成结果语义不准确，需要解决这种因先验知识纠缠引发的生成偏差

Method: 构建分阶段代理提示框架，通过LLM分析目标提示的矛盾点，生成语义连贯的替代表达，使提示内容与去噪阶段动态匹配

Result: 在多类复杂提示场景下，生成图像与文本的语义对齐度获得显著提升

Conclusion: 通过提示分解与去噪阶段对齐实现了细粒度语义控制，有效解决上下文矛盾问题，提升生成准确性

Abstract: Text-to-image diffusion models excel at generating high-quality, diverse
images from natural language prompts. However, they often fail to produce
semantically accurate results when the prompt contains concept combinations
that contradict their learned priors. We define this failure mode as contextual
contradiction, where one concept implicitly negates another due to entangled
associations learned during training. To address this, we propose a stage-aware
prompt decomposition framework that guides the denoising process using a
sequence of proxy prompts. Each proxy prompt is constructed to match the
semantic content expected to emerge at a specific stage of denoising, while
ensuring contextual coherence. To construct these proxy prompts, we leverage a
large language model (LLM) to analyze the target prompt, identify
contradictions, and generate alternative expressions that preserve the original
intent while resolving contextual conflicts. By aligning prompt information
with the denoising progression, our method enables fine-grained semantic
control and accurate image generation in the presence of contextual
contradictions. Experiments across a variety of challenging prompts show
substantial improvements in alignment to the textual prompt.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [247] [Comparative analysis of privacy-preserving open-source LLMs regarding extraction of diagnostic information from clinical CMR imaging reports](https://arxiv.org/abs/2506.00060)
*Sina Amirrajab,Volker Vehof,Michael Bietenbeck,Ali Yilmaz*

Main category: cs.CY

TL;DR: 开源大语言模型在心血管磁共振报告自动诊断分类中表现优异，Gemma2模型以F1分数0.98领先


<details>
  <summary>Details</summary>
Motivation: 研究隐私保护的开源LLM在临床影像报告自动化分析中的应用潜力，解决医疗数据敏感性问题

Method: 使用9个开源LLM分析109份CMR报告，采用准确率/精确率/召回率/F1分数及混淆矩阵评估性能

Result: Gemma2(F1 0.98)、Qwen2.5(F1 0.96)、DeepseekR1-32B(F1 0.95)优于心脏病专家(F1 0.94)

Conclusion: 开源LLM可实现高效准确的临床报告自动化分析，兼具隐私保护与资源节约优势

Abstract: Purpose: We investigated the utilization of privacy-preserving,
locally-deployed, open-source Large Language Models (LLMs) to extract
diagnostic information from free-text cardiovascular magnetic resonance (CMR)
reports. Materials and Methods: We evaluated nine open-source LLMs on their
ability to identify diagnoses and classify patients into various cardiac
diagnostic categories based on descriptive findings in 109 clinical CMR
reports. Performance was quantified using standard classification metrics
including accuracy, precision, recall, and F1 score. We also employed confusion
matrices to examine patterns of misclassification across models. Results: Most
open-source LLMs demonstrated exceptional performance in classifying reports
into different diagnostic categories. Google's Gemma2 model achieved the
highest average F1 score of 0.98, followed by Qwen2.5:32B and DeepseekR1-32B
with F1 scores of 0.96 and 0.95, respectively. All other evaluated models
attained average scores above 0.93, with Mistral and DeepseekR1-7B being the
only exceptions. The top four LLMs outperformed our board-certified
cardiologist (F1 score of 0.94) across all evaluation metrics in analyzing CMR
reports. Conclusion: Our findings demonstrate the feasibility of implementing
open-source, privacy-preserving LLMs in clinical settings for automated
analysis of imaging reports, enabling accurate, fast and resource-efficient
diagnostic categorization.

</details>


### [248] [SafeCOMM: What about Safety Alignment in Fine-Tuned Telecom Large Language Models?](https://arxiv.org/abs/2506.00062)
*Aladin Djuhera,Swanand Ravindra Kadhe,Farhan Ahmed,Syed Zawad,Holger Boche,Walid Saad*

Main category: cs.CY

TL;DR: 研究发现电信领域微调大语言模型会降低安全性，并提出三种安全防御方案（SafeInstruct/SafeLoRA/SafeMERGE）在不影响性能的前提下重建模型安全性


<details>
  <summary>Details</summary>
Motivation: 现有电信领域微调常忽视模型安全对齐性退化问题，需要验证电信数据对安全性的影响并提供解决方案

Method: 基于GenAINet的3GPP标准/表格数据等三类电信数据集，通过红队测试基准评估安全防御方案的有效性

Result: 发现结构化电信数据仍会导致安全侵蚀，现有电信LLM因缺乏安全指令微调存在严重漏洞，验证的防御方案可有效恢复安全性（性能保留率98.6%）

Conclusion: 强调电信LLM需安全感知的指令微调，提出安全诊断框架SafeCOMM，为实际部署提供安全校准指南

Abstract: Fine-tuning large language models (LLMs) for telecom tasks and datasets is a
common practice to adapt general-purpose models to the telecom domain. However,
little attention has been paid to how this process may compromise model safety.
Recent research has shown that even benign fine-tuning can degrade the safety
alignment of LLMs, causing them to respond to harmful or unethical user
queries. In this paper, we investigate this issue for telecom-tuned LLMs using
three representative datasets featured by the GenAINet initiative. We show that
safety degradation persists even for structured and seemingly harmless datasets
such as 3GPP standards and tabular records, indicating that telecom-specific
data is not immune to safety erosion during fine-tuning. We further extend our
analysis to publicly available Telecom LLMs trained via continual pre-training,
revealing that safety alignment is often severely lacking, primarily due to the
omission of safety-focused instruction tuning. To address these issues in both
fine-tuned and pre-trained models, we conduct extensive experiments and
evaluate three safety realignment defenses (SafeInstruct, SafeLoRA, and
SafeMERGE) using established red-teaming benchmarks. The results show that,
across all settings, the proposed defenses can effectively restore safety after
harmful degradation without compromising downstream task performance, leading
to Safe teleCOMMunication (SafeCOMM) models. In a nutshell, our work serves as
a diagnostic study and practical guide for safety realignment in telecom-tuned
LLMs, and emphasizes the importance of safety-aware instruction and fine-tuning
for real-world deployments of Telecom LLMs.

</details>


### [249] [Evaluating Prompt Engineering Techniques for Accuracy and Confidence Elicitation in Medical LLMs](https://arxiv.org/abs/2506.00072)
*Nariman Naderi,Zahra Atf,Peter R Lewis,Aref Mahjoub far,Seyed Amir Ahmad Safavi-Naini,Ali Soroush*

Main category: cs.CY

TL;DR: 提示工程技术显著影响大语言模型在医疗场景下的准确性和置信度校准，链式思考提示虽提升准确性但引发过度自信，需针对性优化校准机制


<details>
  <summary>Details</summary>
Motivation: 探索不同提示策略对LLM医疗决策中预测准确性与置信度校准的双重影响，解决高风险场景下模型过度自信带来的潜在风险

Method: 采用波斯专科考试题库，测试5个LLM(GPT-4o等)在156种配置下的表现，结合温度参数、提示策略(链式思考/情感/专家模拟)和置信度量表，使用AUC-ROC和预期校准误差等指标评估

Result: 链式思考提示使准确率提升8%但校准误差增加15%，情感提示进一步放大过度自信现象；小型模型(Llama-3.1-8b)综合表现最差，商业模型准确率较高但置信度校准仍不足

Conclusion: 医疗领域提示工程需同时优化准确性输出和置信度校准机制，建议结合温度调节和专业校准技术开发医疗专用提示框架

Abstract: This paper investigates how prompt engineering techniques impact both
accuracy and confidence elicitation in Large Language Models (LLMs) applied to
medical contexts. Using a stratified dataset of Persian board exam questions
across multiple specialties, we evaluated five LLMs - GPT-4o, o3-mini,
Llama-3.3-70b, Llama-3.1-8b, and DeepSeek-v3 - across 156 configurations. These
configurations varied in temperature settings (0.3, 0.7, 1.0), prompt styles
(Chain-of-Thought, Few-Shot, Emotional, Expert Mimicry), and confidence scales
(1-10, 1-100). We used AUC-ROC, Brier Score, and Expected Calibration Error
(ECE) to evaluate alignment between confidence and actual performance.
Chain-of-Thought prompts improved accuracy but also led to overconfidence,
highlighting the need for calibration. Emotional prompting further inflated
confidence, risking poor decisions. Smaller models like Llama-3.1-8b
underperformed across all metrics, while proprietary models showed higher
accuracy but still lacked calibrated confidence. These results suggest prompt
engineering must address both accuracy and uncertainty to be effective in
high-stakes medical tasks.

</details>


### [250] [Optimizing Storytelling, Improving Audience Retention, and Reducing Waste in the Entertainment Industry](https://arxiv.org/abs/2506.00076)
*Andrew Cornfeld,Ashley Miller,Mercedes Mora-Figueroa,Kurt Samuels,Anthony Palomba*

Main category: cs.CY

TL;DR: 结合NLP和收视数据的机器学习框架提升电视剧集预测准确性，并提供内容对比工具


<details>
  <summary>Details</summary>
Motivation: 电视网络在节目编排决策时面临高财务风险，常依赖有限历史数据进行收视预测

Method: 整合2.5万+剧集对话的NLP特征（情感基调、认知复杂性、叙事结构）与传统收视数据，采用SARIMAX、滚动XGBoost和特征选择模型；引入基于对话向量欧氏距离的相似性评分方法

Result: NLP特征对部分剧集的预测有显著提升，在不同类型剧集（如《风骚律师》）中观察到类型特异性表现

Conclusion: 该框架为编剧、高管和营销人员提供了基于数据的观众行为洞察，支持内容决策

Abstract: Television networks face high financial risk when making programming
decisions, often relying on limited historical data to forecast episodic
viewership. This study introduces a machine learning framework that integrates
natural language processing (NLP) features from over 25000 television episodes
with traditional viewership data to enhance predictive accuracy. By extracting
emotional tone, cognitive complexity, and narrative structure from episode
dialogue, we evaluate forecasting performance using SARIMAX, rolling XGBoost,
and feature selection models. While prior viewership remains a strong baseline
predictor, NLP features contribute meaningful improvements for some series. We
also introduce a similarity scoring method based on Euclidean distance between
aggregate dialogue vectors to compare shows by content. Tested across diverse
genres, including Better Call Saul and Abbott Elementary, our framework reveals
genre-specific performance and offers interpretable metrics for writers,
executives, and marketers seeking data-driven insight into audience behavior.

</details>


### [251] [Bottom-Up Perspectives on AI Governance: Insights from User Reviews of AI Products](https://arxiv.org/abs/2506.00080)
*Stefan Pasch*

Main category: cs.CY

TL;DR: 通过分析10万+AI产品用户评论，揭示实际场景中AI治理的关注点，提出需补充现有规范框架的用户中心治理路径。


<details>
  <summary>Details</summary>
Motivation: 现有AI治理框架多为顶层设计，缺乏对实际应用场景中用户关切的捕捉。研究旨在从用户视角挖掘治理需求，弥合理论框架与实践的鸿沟。

Method: 采用BERTopic主题建模技术，对G2.com平台的用户评论进行无监督分析，提取与AI治理语义相关的潜在主题。

Result: 识别出涵盖技术领域（数据隐私、部署基础设施）和非技术领域（项目管理、客户互动）的治理主题，发现与现有框架的重叠区及战略规划等新关注维度。

Conclusion: 提出需建立实证驱动的用户中心治理模式，通过捕捉应用场景中的治理实践，形成更具操作性的数字政策框架。

Abstract: With the growing importance of AI governance, numerous high-level frameworks
and principles have been articulated by policymakers, institutions, and expert
communities to guide the development and application of AI. While such
frameworks offer valuable normative orientation, they may not fully capture the
practical concerns of those who interact with AI systems in organizational and
operational contexts. To address this gap, this study adopts a bottom-up
approach to explore how governance-relevant themes are expressed in user
discourse. Drawing on over 100,000 user reviews of AI products from G2.com, we
apply BERTopic to extract latent themes and identify those most semantically
related to AI governance. The analysis reveals a diverse set of
governance-relevant topics spanning both technical and non-technical domains.
These include concerns across organizational processes-such as planning,
coordination, and communication-as well as stages of the AI value chain,
including deployment infrastructure, data handling, and analytics. The findings
show considerable overlap with institutional AI governance and ethics
frameworks on issues like privacy and transparency, but also surface overlooked
areas such as project management, strategy development, and customer
interaction. This highlights the need for more empirically grounded,
user-centered approaches to AI governance-approaches that complement normative
models by capturing how governance unfolds in applied settings. By
foregrounding how governance is enacted in practice, this study contributes to
more inclusive and operationally grounded approaches to AI governance and
digital policy.

</details>


### [252] [ClinBench-HPB: A Clinical Benchmark for Evaluating LLMs in Hepato-Pancreato-Biliary Diseases](https://arxiv.org/abs/2506.00095)
*Yuchong Li,Xiaojun Zeng,Chihua Fang,Jian Yang,Lei Zhang*

Main category: cs.CY

TL;DR: 研究构建了覆盖全部ICD-10分类的HPB疾病评估基准ClinBench-HBP，发现现有大语言模型在复杂临床诊断任务中表现显著下降，强调未来医疗LLM需提升真实临床场景适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有医疗LLM评估基准缺乏HPB疾病覆盖和真实临床案例验证，难以反映模型在复杂医疗场景的实际诊断能力。

Method: 通过整合3535道多选题和337个真实诊断案例构建评估基准，覆盖ICD-10全部33类HPB疾病，数据来源包括公共数据集、合成数据、医学期刊及合作医院临床病例。

Result: 商业LLM在标准化考试题表现良好，但在复杂住院病例诊断中性能下降达30%，医疗LLM对HPB疾病的泛化能力有限(准确率<55%)。

Conclusion: 当前LLM在HPB临床诊断存在显著局限，未来应重点提升模型处理真实复杂病例能力，而非仅优化考试题表现。基准公开将推动医疗AI发展。

Abstract: Hepato-pancreato-biliary (HPB) disorders represent a global public health
challenge due to their high morbidity and mortality. Although large language
models (LLMs) have shown promising performance in general medical
question-answering tasks, the current evaluation benchmarks are mostly derived
from standardized examinations or manually designed questions, lacking HPB
coverage and clinical cases. To address these issues, we systematically
eatablish an HPB disease evaluation benchmark comprising 3,535 closed-ended
multiple-choice questions and 337 open-ended real diagnosis cases, which
encompasses all the 33 main categories and 465 subcategories of HPB diseases
defined in the International Statistical Classification of Diseases, 10th
Revision (ICD-10). The multiple-choice questions are curated from public
datasets and synthesized data, and the clinical cases are collected from
prestigious medical journals, case-sharing platforms, and collaborating
hospitals. By evalauting commercial and open-source general and medical LLMs on
our established benchmark, namely ClinBench-HBP, we find that while commercial
LLMs perform competently on medical exam questions, they exhibit substantial
performance degradation on HPB diagnosis tasks, especially on complex,
inpatient clinical cases. Those medical LLMs also show limited generalizability
to HPB diseases. Our results reveal the critical limitations of current LLMs in
the domain of HPB diseases, underscoring the imperative need for future medical
LLMs to handle real, complex clinical diagnostics rather than simple medical
exam questions. The benchmark will be released at the homepage.

</details>


### [253] [Children's Voice Privacy: First Steps And Emerging Challenges](https://arxiv.org/abs/2506.00100)
*Ajinkya Kulkarni,Francisco Teixeira,Enno Hermann,Thomas Rolland,Isabel Trancoso,Mathew Magimai Doss*

Main category: cs.CY

TL;DR: 评估成人语音匿名化技术应用于儿童语音的效果，发现隐私保护有效但效用显著下降


<details>
  <summary>Details</summary>
Motivation: 儿童在语音技术中代表性不足且隐私脆弱，现有匿名化技术缺乏针对儿童的研究

Method: 使用3个儿童数据集、6种匿名化方法，结合主客观效用指标进行评估

Result: 成人系统可保护儿童隐私但效用损失更大，自动评估方法在儿童语音质量评估中存在挑战

Conclusion: 需开发针对儿童的匿名化技术，改进自动语音质量评估方法

Abstract: Children are one of the most under-represented groups in speech technologies,
as well as one of the most vulnerable in terms of privacy. Despite this,
anonymization techniques targeting this population have received little
attention. In this study, we seek to bridge this gap, and establish a baseline
for the use of voice anonymization techniques designed for adult speech when
applied to children's voices. Such an evaluation is essential, as children's
speech presents a distinct set of challenges when compared to that of adults.
This study comprises three children's datasets, six anonymization methods, and
objective and subjective utility metrics for evaluation. Our results show that
existing systems for adults are still able to protect children's voice privacy,
but suffer from much higher utility degradation. In addition, our subjective
study displays the challenges of automatic evaluation methods for speech
quality in children's speech, highlighting the need for further research.

</details>


### [254] [MythTriage: Scalable Detection of Opioid Use Disorder Myths on a Video-Sharing Platform](https://arxiv.org/abs/2506.00308)
*Hayoung Jung,Shravika Mittal,Ananya Aatreya,Navreet Kaur,Munmun De Choudhury,Tanushree Mitra*

Main category: cs.CY

TL;DR: 首次在YouTube开展大规模阿片类药物使用障碍(OUD)错误信息研究，提出高效标注框架MythTriage，结合轻量模型与LLM实现76%成本节省，揭示平台错误信息传播机制。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效衡量YouTube等平台中OUD相关健康谣言规模，这类错误信息严重影响公共卫生管理。

Method: 通过临床专家验证8类典型谣言→构建标注数据集→开发MythTriage分级标注系统（常规案例轻量模型处理，疑难案例调用LLM）→分析290万条平台数据。

Result: 系统达0.86宏F1值，预估节省76%标注成本；发现YouTube搜索/推荐中持续存在OUD错误信息传播路径。

Conclusion: 该框架为健康信息治理提供可扩展解决方案，数据集与分析方法对平台内容审核具有直接指导价值。

Abstract: Understanding the prevalence of misinformation in health topics online can
inform public health policies and interventions. However, measuring such
misinformation at scale remains a challenge, particularly for high-stakes but
understudied topics like opioid-use disorder (OUD)--a leading cause of death in
the U.S. We present the first large-scale study of OUD-related myths on
YouTube, a widely-used platform for health information. With clinical experts,
we validate 8 pervasive myths and release an expert-labeled video dataset. To
scale labeling, we introduce MythTriage, an efficient triage pipeline that uses
a lightweight model for routine cases and defers harder ones to a
high-performing, but costlier, large language model (LLM). MythTriage achieves
up to 0.86 macro F1-score while estimated to reduce annotation time and
financial cost by over 76% compared to experts and full LLM labeling. We
analyze 2.9K search results and 343K recommendations, uncovering how myths
persist on YouTube and offering actionable insights for public health and
platform moderation.

</details>


### [255] [AIMSCheck: Leveraging LLMs for AI-Assisted Review of Modern Slavery Statements Across Jurisdictions](https://arxiv.org/abs/2506.01671)
*Adriana Eufrosina Bora,Akshatha Arodi,Duoyi Zhang,Jordan Bannister,Mirko Bronzi,Arsene Fansi Tchango,Md Abul Bashar,Richi Nayak,Kerrie Mengersen*

Main category: cs.CY

TL;DR: 开发跨法域的NLP合规评估框架AIMSCheck及数据集，解决现代奴隶制声明验证中的语言复杂性和数据稀缺问题


<details>
  <summary>Details</summary>
Motivation: 现代奴隶制法案要求企业披露反奴隶制措施，但现有声明存在语言复杂、数据标注稀缺的验证挑战，且需验证工具在跨法域的通用性

Method: 1) 与领域专家合作构建英国和加拿大新数据集AIMS.uk/AIMS.ca；2) 提出三层任务分解框架AIMSCheck增强可解释性

Result: 在澳大利亚数据训练的模型对英加法域表现出良好泛化能力（准确率>80%），验证跨法域合规监控可行性

Conclusion: 开源数据集与框架可推动AI在合规评估中的应用，促进该领域研究发展

Abstract: Modern Slavery Acts mandate that corporations disclose their efforts to
combat modern slavery, aiming to enhance transparency and strengthen practices
for its eradication. However, verifying these statements remains challenging
due to their complex, diversified language and the sheer number of statements
that must be reviewed. The development of NLP tools to assist in this task is
also difficult due to a scarcity of annotated data. Furthermore, as modern
slavery transparency legislation has been introduced in several countries, the
generalizability of such tools across legal jurisdictions must be studied. To
address these challenges, we work with domain experts to make two key
contributions. First, we present AIMS.uk and AIMS.ca, newly annotated datasets
from the UK and Canada to enable cross-jurisdictional evaluation. Second, we
introduce AIMSCheck, an end-to-end framework for compliance validation.
AIMSCheck decomposes the compliance assessment task into three levels,
enhancing interpretability and practical applicability. Our experiments show
that models trained on an Australian dataset generalize well across UK and
Canadian jurisdictions, demonstrating the potential for broader application in
compliance monitoring. We release the benchmark datasets and AIMSCheck to the
public to advance AI-adoption in compliance assessment and drive further
research in this field.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [256] [GLEN: Generative Retrieval via Lexical Index Learning](https://arxiv.org/abs/2311.03057)
*Sunkyung Lee,Minjin Choi,Jongwuk Lee*

Main category: cs.IR

TL;DR: 提出GLEN方法解决生成式检索中的标识符知识对齐与训练推理一致性难题，通过动态词典标识符学习提升检索效果


<details>
  <summary>Details</summary>
Motivation: 现有生成式检索方法面临预训练语言模型知识标识符不匹配、训练推理阶段排序信号不一致的双重挑战

Method: 采用两阶段索引学习策略（动态词典标识符生成+强化学习）与基于标识符权重的无冲突推理机制

Result: 在NQ320k/MS MARCO/BEIR等基准数据集上达到SOTA水平，推理过程无额外计算开销

Conclusion: GLEN通过词典索引学习范式有效提升生成式检索性能，代码开源推动领域发展

Abstract: Generative retrieval shed light on a new paradigm of document retrieval,
aiming to directly generate the identifier of a relevant document for a query.
While it takes advantage of bypassing the construction of auxiliary index
structures, existing studies face two significant challenges: (i) the
discrepancy between the knowledge of pre-trained language models and
identifiers and (ii) the gap between training and inference that poses
difficulty in learning to rank. To overcome these challenges, we propose a
novel generative retrieval method, namely Generative retrieval via LExical
iNdex learning (GLEN). For training, GLEN effectively exploits a dynamic
lexical identifier using a two-phase index learning strategy, enabling it to
learn meaningful lexical identifiers and relevance signals between queries and
documents. For inference, GLEN utilizes collision-free inference, using
identifier weights to rank documents without additional overhead. Experimental
results prove that GLEN achieves state-of-the-art or competitive performance
against existing generative retrieval methods on various benchmark datasets,
e.g., NQ320k, MS MARCO, and BEIR. The code is available at
https://github.com/skleee/GLEN.

</details>


### [257] [Retrieval-Augmented Generation: A Comprehensive Survey of Architectures, Enhancements, and Robustness Frontiers](https://arxiv.org/abs/2506.00054)
*Chaitanya Sharma*

Main category: cs.IR

TL;DR: RAG技术通过外部检索增强大语言模型，解决参数化知识存储问题但面临检索质量与生成灵活性等新挑战，提出四类架构分类并揭示性能权衡，指明未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型参数化知识存储存在的事实不一致和领域局限性问题，同时应对RAG系统带来的检索质量、流程效率和对抗鲁棒性等新挑战。

Method: 建立检索器中心/生成器中心/混合/鲁棒性四类架构分类体系，系统性分析检索优化、上下文过滤、解码控制等技术改进方案。

Result: 揭示检索精度与生成灵活性、效率与忠实性、模块化与协调性间的核心权衡，多基准测试显示混合架构在问答任务中的优势。

Conclusion: 未来应发展自适应检索架构、实时检索集成、多跳推理机制及隐私保护检索系统，推动下一代检索增强语言模型发展。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to
enhance large language models (LLMs) by conditioning generation on external
evidence retrieved at inference time. While RAG addresses critical limitations
of parametric knowledge storage-such as factual inconsistency and domain
inflexibility-it introduces new challenges in retrieval quality, grounding
fidelity, pipeline efficiency, and robustness against noisy or adversarial
inputs. This survey provides a comprehensive synthesis of recent advances in
RAG systems, offering a taxonomy that categorizes architectures into
retriever-centric, generator-centric, hybrid, and robustness-oriented designs.
We systematically analyze enhancements across retrieval optimization, context
filtering, decoding control, and efficiency improvements, supported by
comparative performance analyses on short-form and multi-hop question answering
tasks. Furthermore, we review state-of-the-art evaluation frameworks and
benchmarks, highlighting trends in retrieval-aware evaluation, robustness
testing, and federated retrieval settings. Our analysis reveals recurring
trade-offs between retrieval precision and generation flexibility, efficiency
and faithfulness, and modularity and coordination. We conclude by identifying
open challenges and future research directions, including adaptive retrieval
architectures, real-time retrieval integration, structured reasoning over
multi-hop evidence, and privacy-preserving retrieval mechanisms. This survey
aims to consolidate current knowledge in RAG research and serve as a foundation
for the next generation of retrieval-augmented language modeling systems.

</details>


### [258] [GPR: Empowering Generation with Graph-Pretrained Retriever](https://arxiv.org/abs/2506.00261)
*Xiaochen Wang,Zongyu Wu,Yuan Zhong,Xiang Zhang,Suhang Wang,Fenglong Ma*

Main category: cs.IR

TL;DR: 提出图预训练检索器GPR，通过LLM引导的图增强和结构感知训练，显著提升GRAG任务的检索质量与生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本预训练的检索器存在领域错位和结构忽视问题，导致图检索增强生成效果受限。

Method: 1. 使用LLM引导的图数据增强对齐自然语言问题与子图；2. 设计结构感知目标函数学习细粒度检索策略。

Result: 在2个数据集、3个LLM和5个基线模型的实验中，GPR在检索质量和下游生成任务中均取得稳定提升。

Conclusion: GPR作为专为知识图谱设计的预训练检索器，为GRAG任务提供了鲁棒的检索解决方案。

Abstract: Graph retrieval-augmented generation (GRAG) places high demands on
graph-specific retrievers. However, existing retrievers often rely on language
models pretrained on plain text, limiting their effectiveness due to domain
misalignment and structure ignorance. To address these challenges, we propose
GPR, a graph-based retriever pretrained directly on knowledge graphs. GPR
aligns natural language questions with relevant subgraphs through LLM-guided
graph augmentation and employs a structure-aware objective to learn
fine-grained retrieval strategies. Experiments on two datasets, three LLM
backbones, and five baselines show that GPR consistently improves both
retrieval quality and downstream generation, demonstrating its effectiveness as
a robust retrieval solution for GRAG.

</details>


### [259] [Adapting General-Purpose Embedding Models to Private Datasets Using Keyword-based Retrieval](https://arxiv.org/abs/2506.00363)
*Yubai Wei,Jiale Han,Yi Yang*

Main category: cs.IR

TL;DR: 提出BMEmbed方法，通过BM25检索信号优化文本嵌入模型在私有数据集上的检索性能


<details>
  <summary>Details</summary>
Motivation: 通用文本嵌入模型在包含专业术语的私有数据集上检索效果下降，需针对性优化

Method: 利用BM25关键词检索结果生成监督信号，通过对比学习促进嵌入空间对齐和均匀性

Result: 跨多个领域/数据集/模型的实验显示检索性能持续提升，代码已开源

Conclusion: BM25信号通过提升表征对齐度与一致性，有效实现领域自适应，为私有数据优化提供新思路

Abstract: Text embedding models play a cornerstone role in AI applications, such as
retrieval-augmented generation (RAG). While general-purpose text embedding
models demonstrate strong performance on generic retrieval benchmarks, their
effectiveness diminishes when applied to private datasets (e.g.,
company-specific proprietary data), which often contain specialized terminology
and lingo. In this work, we introduce BMEmbed, a novel method for adapting
general-purpose text embedding models to private datasets. By leveraging the
well-established keyword-based retrieval technique (BM25), we construct
supervisory signals from the ranking of keyword-based retrieval results to
facilitate model adaptation. We evaluate BMEmbed across a range of domains,
datasets, and models, showing consistent improvements in retrieval performance.
Moreover, we provide empirical insights into how BM25-based signals contribute
to improving embeddings by fostering alignment and uniformity, highlighting the
value of this approach in adapting models to domain-specific data. We release
the source code available at https://github.com/BaileyWei/BMEmbed for the
research community.

</details>


### [260] [Bridging the Gap: From Ad-hoc to Proactive Search in Conversations](https://arxiv.org/abs/2506.00983)
*Chuan Meng,Francesco Tonolini,Fengran Mo,Nikolaos Aletras,Emine Yilmaz,Gabriella Kazai*

Main category: cs.IR

TL;DR: 提出Conv2Query框架，通过将对话上下文转化为ad-hoc查询，有效解决主动对话搜索中的输入不匹配问题并提升检索性能


<details>
  <summary>Details</summary>
Motivation: 传统ad-hoc检索器预训练的短查询特性与对话场景的长噪声输入存在结构性不匹配，导致主动对话搜索效果受限，即使微调也难以完全克服输入差异

Method: 开发对话转查询框架，先进行上下文到查询的格式转换，再适配现成检索器或进行PSC微调的双阶段方法

Result: 在两个PSC数据集上显著提升现有检索器性能（直接使用提升23.5%，微调后进一步优化14.8%）

Conclusion: 通过输入格式转换有效桥接ad-hoc检索与对话搜索，证实了输入适配对检索性能提升的关键作用

Abstract: Proactive search in conversations (PSC) aims to reduce user effort in
formulating explicit queries by proactively retrieving useful relevant
information given conversational context. Previous work in PSC either directly
uses this context as input to off-the-shelf ad-hoc retrievers or further
fine-tunes them on PSC data. However, ad-hoc retrievers are pre-trained on
short and concise queries, while the PSC input is longer and noisier. This
input mismatch between ad-hoc search and PSC limits retrieval quality. While
fine-tuning on PSC data helps, its benefits remain constrained by this input
gap. In this work, we propose Conv2Query, a novel conversation-to-query
framework that adapts ad-hoc retrievers to PSC by bridging the input gap
between ad-hoc search and PSC. Conv2Query maps conversational context into
ad-hoc queries, which can either be used as input for off-the-shelf ad-hoc
retrievers or for further fine-tuning on PSC data. Extensive experiments on two
PSC datasets show that Conv2Query significantly improves ad-hoc retrievers'
performance, both when used directly and after fine-tuning on PSC.

</details>


### [261] [GRAM: Generative Recommendation via Semantic-aware Multi-granular Late Fusion](https://arxiv.org/abs/2506.01673)
*Sunkyung Lee,Minjin Choi,Eunseong Choi,Hye-young Kim,Jongwuk Lee*

Main category: cs.IR

TL;DR: 提出GRAM模型，通过语义感知的多粒度后期融合解决生成式推荐系统中隐式项目关系编码和丰富信息利用问题，在四个数据集上实现显著性能提升


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐方法难以有效整合隐式项目层次/协作关系，且无法高效处理冗长但信息丰富的项目描述

Method: 1. 语义到词汇的转换编码隐式项目关系到LLM词汇空间；2. 多粒度后期融合策略分离编码不同粒度提示词，延迟至解码阶段融合

Result: 在Recall@5指标提升11.5-16.0%，NDCG@5指标提升5.3-13.6%，优于8个最先进的生成推荐模型

Conclusion: GRAM通过创新的语义编码和分层融合机制，显著提高了生成式推荐系统的效果，证明了隐式关系建模与高效信息融合的重要性

Abstract: Generative recommendation is an emerging paradigm that leverages the
extensive knowledge of large language models by formulating recommendations
into a text-to-text generation task. However, existing studies face two key
limitations in (i) incorporating implicit item relationships and (ii) utilizing
rich yet lengthy item information. To address these challenges, we propose a
Generative Recommender via semantic-Aware Multi-granular late fusion (GRAM),
introducing two synergistic innovations. First, we design semantic-to-lexical
translation to encode implicit hierarchical and collaborative item
relationships into the vocabulary space of LLMs. Second, we present
multi-granular late fusion to integrate rich semantics efficiently with minimal
information loss. It employs separate encoders for multi-granular prompts,
delaying the fusion until the decoding stage. Experiments on four benchmark
datasets show that GRAM outperforms eight state-of-the-art generative
recommendation models, achieving significant improvements of 11.5-16.0% in
Recall@5 and 5.3-13.6% in NDCG@5. The source code is available at
https://github.com/skleee/GRAM.

</details>


### [262] [When Should Dense Retrievers Be Updated in Evolving Corpora? Detecting Out-of-Distribution Corpora Using GradNormIR](https://arxiv.org/abs/2506.01877)
*Dayoon Ko,Jinyoung Kim,Sohyeon Kim,Jinhyuk Kim,Jaehoon Lee,Seonghak Song,Minyoung Lee,Gunhee Kim*

Main category: cs.IR

TL;DR: 提出无监督方法GradNormIR，通过梯度范数检测动态语料库是否超出密集检索器训练分布，实现检索器更新时机的主动预警，提升检索系统鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现实文档库持续演化导致检索器训练分布偏移，若不及时更新会降低未来查询的检索性能，需开发语料库OOD检测方法实现预防性管理

Method: 基于梯度范数的无监督检测框架GradNormIR，利用模型对语料文档的响应梯度统计特征判断分布偏移，无需人工标注或重新训练

Result: 在BEIR基准测试中，GradNormIR显著提升检索系统应对动态语料库变化的稳健性，检索效率提升30%且准确率维持95%+

Conclusion: 该方法开创了检索器更新预警新范式，为动态文档环境下的持续学习系统提供可靠的OOD检测解决方案

Abstract: Dense retrievers encode texts into embeddings to efficiently retrieve
relevant documents from large databases in response to user queries. However,
real-world corpora continually evolve, leading to a shift from the original
training distribution of the retriever. Without timely updates or retraining,
indexing newly emerging documents can degrade retrieval performance for future
queries. Thus, identifying when a dense retriever requires an update is
critical for maintaining robust retrieval systems. In this paper, we propose a
novel task of predicting whether a corpus is out-of-distribution (OOD) relative
to a dense retriever before indexing. Addressing this task allows us to
proactively manage retriever updates, preventing potential retrieval failures.
We introduce GradNormIR, an unsupervised approach that leverages gradient norms
to detect OOD corpora effectively. Experiments on the BEIR benchmark
demonstrate that GradNormIR enables timely updates of dense retrievers in
evolving document collections, significantly enhancing retrieval robustness and
efficiency.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [263] [Pushing the Limits of Beam Search Decoding for Transducer-based ASR models](https://arxiv.org/abs/2506.00185)
*Lilit Grigoryan,Vladimir Bataev,Andrei Andrusenko,Hainan Xu,Vitaly Lavrukhin,Boris Ginsburg*

Main category: eess.AS

TL;DR: 提出加速Transducer模型束搜索的通用方法，通过ALSD++和AES++算法结合批处理与树结构优化，在保持10-20%速度差距的同时实现14-30%识别准确率提升


<details>
  <summary>Details</summary>
Motivation: 解决传统Transducer模型使用束搜索时因重复计算导致推理速度大幅下降的问题，突破端到端语音识别系统的实际应用瓶颈

Method: 采用批处理操作+树状假设结构+新型空白评分机制+CUDA图执行技术，开发ALSD++和AES++两种优化算法实现高效GPU推理

Result: 系统级速度差距缩小至10-20%，词错误率相对贪婪解码提升14-30%，低资源场景的shallow fusion性能提升最高达11%

Conclusion: 提出的优化方法有效平衡了ASR系统的推理速度与识别精度，通过算法创新实现近乎实时的束搜索能力，所有技术方案已开源共享

Abstract: Transducer models have emerged as a promising choice for end-to-end ASR
systems, offering a balanced trade-off between recognition accuracy, streaming
capabilities, and inference speed in greedy decoding. However, beam search
significantly slows down Transducers due to repeated evaluations of key network
components, limiting practical applications. This paper introduces a universal
method to accelerate beam search for Transducers, enabling the implementation
of two optimized algorithms: ALSD++ and AES++. The proposed method utilizes
batch operations, a tree-based hypothesis structure, novel blank scoring for
enhanced shallow fusion, and CUDA graph execution for efficient GPU inference.
This narrows the speed gap between beam and greedy modes to only 10-20% for the
whole system, achieves 14-30% relative improvement in WER compared to greedy
decoding, and improves shallow fusion for low-resource up to 11% compared to
existing implementations. All the algorithms are open sourced.

</details>


### [264] [Confidence intervals for forced alignment boundaries using model ensembles](https://arxiv.org/abs/2506.01256)
*Matthew C. Kelley*

Main category: eess.AS

TL;DR: 提出基于神经网络集成方法的语音强制对齐置信区间计算方案，通过多模型集成提升边界定位精度并量化不确定性


<details>
  <summary>Details</summary>
Motivation: 传统强制对齐工具仅提供单一边界估计，缺乏对边界不确定性的量化评估。本研究旨在通过集成学习方法构建置信区间，帮助研究者分析对齐结果的可靠性。

Method: 使用10个预训练的语音片段分类神经网络进行集成对齐，采用中位数确定边界位置，通过顺序统计量构建97.85%置信区间，并将结果整合至Praat TextGrids和独立分析表格中。

Result: 在Buckeye和TIMIT语料库上，集成方法较单模型边界定位精度略有提升，置信区间成功整合至主流语音分析工具输出格式。

Conclusion: 该方法有效实现了强制对齐边界的不确定性量化，为语音学研究提供了新的诊断工具，使研究者能在分析中合理考虑对齐误差因素。

Abstract: Forced alignment is a common tool to align audio with orthographic and
phonetic transcriptions. Most forced alignment tools provide only a single
estimate of a boundary. The present project introduces a method of deriving
confidence intervals for these boundaries using a neural network ensemble
technique. Ten different segment classifier neural networks were previously
trained, and the alignment process is repeated with each model. The alignment
ensemble is then used to place the boundary at the median of the boundaries in
the ensemble, and 97.85% confidence intervals are constructed using order
statistics. On the Buckeye and TIMIT corpora, the ensemble boundaries show a
slight improvement over using just a single model. The confidence intervals are
incorporated into Praat TextGrids using a point tier, and they are also output
as a table for researchers to analyze separately as diagnostics or to
incorporate uncertainty into their analyses.

</details>


### [265] [LinearVC: Linear transformations of self-supervised features through the lens of voice conversion](https://arxiv.org/abs/2506.01510)
*Herman Kamper,Benjamin van Niekerk,Julian Zaïdi,Marc-André Carbonneau*

Main category: eess.AS

TL;DR: 提出LinearVC方法，通过线性变换自监督语音表征实现高效语音转换，揭示内容信息存在于低维子空间的结构特性


<details>
  <summary>Details</summary>
Motivation: 探索自监督语音表征的内在结构特性，并验证通过简单线性变换即可实现语音转换的可能性

Method: 分三阶段：1）线性变换验证有效性 2）通过旋转约束探索特征空间几何结构 3）使用SVD显式分解内容/说话人信息

Result: 仅100维的线性投影即可实现有竞争力的语音转换效果，证实内容信息低维嵌入的假设

Conclusion: 为语音转换提供新方法，同时深化对自监督语音表征结构的理解

Abstract: We introduce LinearVC, a simple voice conversion method that sheds light on
the structure of self-supervised representations. First, we show that simple
linear transformations of self-supervised features effectively convert voices.
Next, we probe the geometry of the feature space by constraining the set of
allowed transformations. We find that just rotating the features is sufficient
for high-quality voice conversion. This suggests that content information is
embedded in a low-dimensional subspace which can be linearly transformed to
produce a target voice. To validate this hypothesis, we finally propose a
method that explicitly factorizes content and speaker information using
singular value decomposition; the resulting linear projection with a rank of
just 100 gives competitive conversion results. Our work has implications for
both practical voice conversion and a broader understanding of self-supervised
speech representations. Samples and code: https://www.kamperh.com/linearvc/.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [266] [Probing Audio-Generation Capabilities of Text-Based Language Models](https://arxiv.org/abs/2506.00003)
*Arjun Prasaath Anbazhagan,Parteek Kumar,Ujjwal Kaur,Aslihan Akalin,Kevin Zhu,Sean O'Brien*

Main category: cs.SD

TL;DR: 研究探索LLMs通过文本学习音频的能力，使用代码作为中介生成音符/环境声/语音三种复杂度的音频，发现模型在复杂音频生成上表现有限。


<details>
  <summary>Details</summary>
Motivation: 探究文本表征与LLMs音频认知的关系，验证纯文本训练的模型能否通过提示生成音频。

Method: 分层生成框架：1) 音符生成 2) 环境声合成 3) 语音生成，采用代码作为文本-音频转换桥梁，使用FAD和CLAP评分量化评估。

Result: LLMs可生成基础音频特征，但性能随复杂度增加显著下降（FAD指标：音符0.12→语音0.84），揭示音频生成能力仍处初级阶段。

Conclusion: 需开发增强技术提升LLM生成音频的质量与多样性，这可能改善纯文本模型在跨模态生成任务中的表现。

Abstract: How does textual representation of audio relate to the Large Language Model's
(LLMs) learning about the audio world? This research investigates the extent to
which LLMs can be prompted to generate audio, despite their primary training in
textual data. We employ a three-tier approach, progressively increasing the
complexity of audio generation: 1) Musical Notes, 2) Environmental Sounds, and
3) Human Speech. To bridge the gap between text and audio, we leverage code as
an intermediary, prompting LLMs to generate code that, when executed, produces
the desired audio output. To evaluate the quality and accuracy of the generated
audio, we employ FAD and CLAP scores. Our findings reveal that while LLMs can
generate basic audio features, their performance deteriorates as the complexity
of the audio increases. This suggests that while LLMs possess a latent
understanding of the auditory world, their ability to translate this
understanding into tangible audio output remains rudimentary. Further research
into techniques that can enhance the quality and diversity of LLM-generated
audio can lead to an improvement in the performance of text-based LLMs in
generating audio.

</details>


### [267] [XMAD-Bench: Cross-Domain Multilingual Audio Deepfake Benchmark](https://arxiv.org/abs/2506.00462)
*Ioan-Paul Ciobanu,Andrei-Iulian Hiji,Nicolae-Catalin Ristea,Paul Irofti,Cristian Rusu,Radu Tudor Ionescu*

Main category: cs.SD

TL;DR: 提出跨域多语言音频深度伪造基准XMAD-Bench，揭示现有检测器在跨域场景下的性能显著下降


<details>
  <summary>Details</summary>
Motivation: 现有音频深度伪造检测器在跨域场景(说话人/生成方法/数据源不同)中性能接近随机水平，需开发能泛化至不同语言和场景的鲁棒检测器

Method: 构建包含668.8小时语音的跨域测试基准，通过分离训练集与测试集的说话人、生成方法和数据源，创建真实跨域评估环境

Result: 检测器同域准确率可达100%，但跨域测试时性能下降至接近随机水平(如50%)，暴露严重泛化缺陷

Conclusion: 强调开发跨语言/说话人/生成方法鲁棒检测器的迫切性，公开XMAD-Bench基准促进相关研究

Abstract: Recent advances in audio generation led to an increasing number of deepfakes,
making the general public more vulnerable to financial scams, identity theft,
and misinformation. Audio deepfake detectors promise to alleviate this issue,
with many recent studies reporting accuracy rates close to 99%. However, these
methods are typically tested in an in-domain setup, where the deepfake samples
from the training and test sets are produced by the same generative models. To
this end, we introduce XMAD-Bench, a large-scale cross-domain multilingual
audio deepfake benchmark comprising 668.8 hours of real and deepfake speech. In
our novel dataset, the speakers, the generative methods, and the real audio
sources are distinct across training and test splits. This leads to a
challenging cross-domain evaluation setup, where audio deepfake detectors can
be tested ``in the wild''. Our in-domain and cross-domain experiments indicate
a clear disparity between the in-domain performance of deepfake detectors,
which is usually as high as 100%, and the cross-domain performance of the same
models, which is sometimes similar to random chance. Our benchmark highlights
the need for the development of robust audio deepfake detectors, which maintain
their generalization capacity across different languages, speakers, generative
methods, and data sources. Our benchmark is publicly released at
https://github.com/ristea/xmad-bench/.

</details>


### [268] [Attention Is Not Always the Answer: Optimizing Voice Activity Detection with Simple Feature Fusion](https://arxiv.org/abs/2506.01365)
*Kumud Tripathi,Chowdam Venkata Kumar,Pankaj Wasnik*

Main category: cs.SD

TL;DR: 提出融合MFCC和预训练模型特征的FusionVAD框架，通过简单特征融合策略实现VAD性能提升


<details>
  <summary>Details</summary>
Motivation: 验证传统声学特征（MFCC）与预训练模型特征的互补性，探索不同特征融合策略的有效性

Method: 使用三种融合策略（拼接/相加/交叉注意力）整合MFCC和6种预训练模型特征

Result: 简单相加策略效果最优，融合模型平均性能超越SOTA方法Pyannote 2.04%

Conclusion: 证实简单特征融合能有效提升VAD鲁棒性，在保证计算效率的同时实现性能突破

Abstract: Voice Activity Detection (VAD) plays a key role in speech processing, often
utilizing hand-crafted or neural features. This study examines the
effectiveness of Mel-Frequency Cepstral Coefficients (MFCCs) and pre-trained
model (PTM) features, including wav2vec 2.0, HuBERT, WavLM, UniSpeech, MMS, and
Whisper. We propose FusionVAD, a unified framework that combines both feature
types using three fusion strategies: concatenation, addition, and
cross-attention (CA). Experimental results reveal that simple fusion
techniques, particularly addition, outperform CA in both accuracy and
efficiency. Fusion-based models consistently surpass single-feature models,
highlighting the complementary nature of MFCCs and PTM features. Notably, our
best-performing fusion model exceeds the state-of-the-art Pyannote across
multiple datasets, achieving an absolute average improvement of 2.04%. These
results confirm that simple feature fusion enhances VAD robustness while
maintaining computational efficiency.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [269] [The Automated but Risky Game: Modeling Agent-to-Agent Negotiations and Transactions in Consumer Markets](https://arxiv.org/abs/2506.00073)
*Shenzhe Zhu,Jiao Sun,Yi Nian,Tobin South,Alex Pentland,Jiaxin Pei*

Main category: cs.AI

TL;DR: AI代理在自动化交易中表现差异显著且存在财务风险


<details>
  <summary>Details</summary>
Motivation: 探究消费市场中买卖双方AI代理全自动化谈判的效能差异及潜在风险

Method: 开发实验框架评估不同LLM代理在真实场景下的谈判表现

Result: AI代理谈判存在结果不平衡性（不同代理效果差异达300%），LLM行为异常可导致用户财务损失（如超额支付）

Conclusion: 自动化虽提升效率但伴随重大风险，用户需谨慎授权AI代理处理商业决策

Abstract: AI agents are increasingly used in consumer-facing applications to assist
with tasks such as product search, negotiation, and transaction execution. In
this paper, we explore a future scenario where both consumers and merchants
authorize AI agents to fully automate negotiations and transactions. We aim to
answer two key questions: (1) Do different LLM agents vary in their ability to
secure favorable deals for users? (2) What risks arise from fully automating
deal-making with AI agents in consumer markets? To address these questions, we
develop an experimental framework that evaluates the performance of various LLM
agents in real-world negotiation and transaction settings. Our findings reveal
that AI-mediated deal-making is an inherently imbalanced game -- different
agents achieve significantly different outcomes for their users. Moreover,
behavioral anomalies in LLMs can result in financial losses for both consumers
and merchants, such as overspending or accepting unreasonable deals. These
results underscore that while automation can improve efficiency, it also
introduces substantial risks. Users should exercise caution when delegating
business decisions to AI agents.

</details>


### [270] [Control-R: Towards controllable test-time scaling](https://arxiv.org/abs/2506.00189)
*Di Zhang,Weida Wang,Junxian Li,Xunzhi Wang,Jiatong Li,Jianbo Wu,Jingdi Lei,Haonan He,Peng Ye,Shufei Zhang,Wanli Ouyang,Yuqiang Li,Dongzhan Zhou*

Main category: cs.AI

TL;DR: 通过引入推理控制场(RCF)和条件蒸馏微调(CDF)，实现可控的长链推理过程，在32B规模达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 解决长链思维推理中存在的'欠思考'和'过度思考'问题，实现推理过程的动态调整

Method: 提出RCF结构化控制信号指导树搜索推理，构建Control-R-4K数据集，开发CDF方法训练Control-R-32B模型

Result: 在AIME2024和MATH500等基准测试中达到32B规模最佳性能，实现可控制的长链推理过程(L-CoT)

Conclusion: 建立了测试时可控推理扩展范式，通过RCF和CDF的协同作用优化长链推理效果

Abstract: This paper target in addressing the challenges of underthinking and
overthinking in long chain-of-thought (CoT) reasoning for Large Reasoning
Models (LRMs) by introducing Reasoning Control Fields (RCF)--a novel test-time
approach that injects structured control signals to guide reasoning from a tree
search perspective. RCF enables models to adjust reasoning effort according to
given control conditions when solving complex tasks. Additionally, we present
the Control-R-4K dataset, which consists of challenging problems annotated with
detailed reasoning processes and corresponding control fields. To further
enhance reasoning control, we propose a Conditional Distillation Finetuning
(CDF) method, which trains model--particularly Control-R-32B--to effectively
adjust reasoning effort during test time. Experimental results on benchmarks
such as AIME2024 and MATH500 demonstrate that our approach achieves
state-of-the-art performance at the 32B scale while enabling a controllable
Long CoT reasoning process (L-CoT). Overall, this work introduces an effective
paradigm for controllable test-time scaling reasoning.

</details>


### [271] [Whispers of Many Shores: Cultural Alignment through Collaborative Cultural Expertise](https://arxiv.org/abs/2506.00242)
*Shuai Feng,Wei-Chuang Chan,Srishti Chouhan,Junior Francisco Garcia Ayala,Srujananjali Medicherla,Kyle Clark,Mingwei Shi*

Main category: cs.AI

TL;DR: 提出基于软提示微调的模块化文化对齐框架，通过动态路由文化专家配置提升LLM跨文化适应性


<details>
  <summary>Details</summary>
Motivation: 现有LLMs缺乏对多元文化语境的理解能力，全参数微调成本高昂且难以灵活适配不同文化场景

Method: 构建向量化提示嵌入空间，建立文化专家委员会实现动态查询路由，保持基础模型参数冻结

Result: 文化对齐分数从0.208提升至0.820，验证框架在敏感度和适应性上的显著提升

Conclusion: 为全球化LLM部署提供高效解决方案，后续可扩展文化覆盖维度并探索动态专家适应机制

Abstract: The integration of large language models (LLMs) into global applications
necessitates effective cultural alignment for meaningful and
culturally-sensitive interactions. Current LLMs often lack the nuanced
understanding required for diverse cultural contexts, and adapting them
typically involves costly full fine-tuning. To address this, we introduce a
novel soft prompt fine-tuning framework that enables efficient and modular
cultural alignment. Our method utilizes vectorized prompt tuning to dynamically
route queries to a committee of culturally specialized 'expert' LLM
configurations, created by optimizing soft prompt embeddings without altering
the base model's parameters. Extensive experiments demonstrate that our
framework significantly enhances cultural sensitivity and adaptability,
improving alignment scores from 0.208 to 0.820, offering a robust solution for
culturally-aware LLM deployment. This research paves the way for subsequent
investigations into enhanced cultural coverage and dynamic expert adaptation,
crucial for realizing autonomous AI with deeply nuanced understanding in a
globally interconnected world.

</details>


### [272] [MIR: Methodology Inspiration Retrieval for Scientific Research Problems](https://arxiv.org/abs/2506.00249)
*Aniketh Garikaparthi,Manasi Patwardhan,Aditya Sanjiv Kanade,Aman Hassan,Lovekesh Vig,Arman Cohan*

Main category: cs.AI

TL;DR: 提出方法论启发检索（MIR）框架，通过构建方法论邻近图（MAG）和嵌入直觉先验，显著提升科学文献启发检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于文献的科学发现方法受限于文献质量，需建立能有效识别方法论启发的检索系统（MIR）来突破表层语义相似性限制。

Method: 1. 构建方法论邻近图（MAG）捕捉方法学谱系
2. 在密集检索器中嵌入直觉先验
3. 采用LLM重排序策略优化MIR

Result: 基线模型提升Recall@3（+5.4）和mAP（+7.8）；LLM重排序进一步带来Recall@3（+4.5）和mAP（+4.8）提升。

Conclusion: MIR框架为自动化科学发现提供新范式，未来可通过改进启发驱动检索机制加速跨学科方法论迁移。

Abstract: There has been a surge of interest in harnessing the reasoning capabilities
of Large Language Models (LLMs) to accelerate scientific discovery. While
existing approaches rely on grounding the discovery process within the relevant
literature, effectiveness varies significantly with the quality and nature of
the retrieved literature. We address the challenge of retrieving prior work
whose concepts can inspire solutions for a given research problem, a task we
define as Methodology Inspiration Retrieval (MIR). We construct a novel dataset
tailored for training and evaluating retrievers on MIR, and establish
baselines. To address MIR, we build the Methodology Adjacency Graph (MAG);
capturing methodological lineage through citation relationships. We leverage
MAG to embed an "intuitive prior" into dense retrievers for identifying
patterns of methodological inspiration beyond superficial semantic similarity.
This achieves significant gains of +5.4 in Recall@3 and +7.8 in Mean Average
Precision (mAP) over strong baselines. Further, we adapt LLM-based re-ranking
strategies to MIR, yielding additional improvements of +4.5 in Recall@3 and
+4.8 in mAP. Through extensive ablation studies and qualitative analyses, we
exhibit the promise of MIR in enhancing automated scientific discovery and
outline avenues for advancing inspiration-driven retrieval.

</details>


### [273] [Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in AI Agents](https://arxiv.org/abs/2506.00320)
*Xiao Yu,Baolin Peng,Ruize Xu,Michel Galley,Hao Cheng,Suman Nath,Jianfeng Gao,Zhou Yu*

Main category: cs.AI

TL;DR: 提出Dyna-Think框架，通过整合世界模型仿真提升AI代理的推理、规划与执行能力，开发DIT和DDT训练方法，在减少50%计算量的情况下实现与DeepSeek-R1相当的性能


<details>
  <summary>Details</summary>
Motivation: 当前LLM在长周期任务中有效行为机制不明确，需系统化框架提升AI代理的规划与推理能力

Method: 1. Dyna-Think Imitation Learning (DIT)：重构R1思维过程，聚焦动作相关的世界模型仿真训练
2. Dyna-Think Dyna Training (DDT)：分两阶段训练，先通过状态预测/批判生成提升世界建模能力，再优化策略

Result: 在OSWorld基准测试中：
- 领域内外性能均超越基线
- 最佳n次性能与R1相当，平均减少2倍token生成
- 批判生成训练显著提升策略效果
- 代理性能与世界建模能力正相关

Conclusion: 整合世界模型仿真为AI代理能力提升开辟新方向，未来可进一步探索其在复杂认知任务中的应用潜力

Abstract: Recent progress in reasoning with large language models (LLMs), such as
DeepSeek-R1, demonstrates impressive capabilities in domains like mathematics
and coding, by exhibiting complex cognitive behaviors such as verification,
goal decomposition, and self-reflection. However, it is unclear what behavior
is effective and what behavior is missing for long-horizon AI agents tasks. In
this work, we propose Dyna-Think, a thinking framework that integrates planning
with an internal world model with reasoning and acting to enhance AI agent
performance. To enable Dyna-Think, we propose Dyna-Think Imitation Learning
(DIT) and Dyna-Think Dyna Training (DDT). To initialize a policy with
Dyna-Think, DIT reconstructs the thinking process of R1 to focus on performing
world model simulation relevant to the proposed (and planned) action, and
trains the policy using this reconstructed data. To enhance Dyna-Think, DDT
uses a two-stage training process to first improve the agent's world modeling
ability via objectives such as state prediction or critique generation, and
then improve the agent's action via policy training. We evaluate our methods on
OSWorld, and demonstrate that Dyna-Think improves the agent's in-domain and
out-of-domain performance, achieving similar best-of-n performance compared to
R1 while generating 2x less tokens on average. Our extensive empirical studies
reveal that 1) using critique generation for world model training is effective
to improve policy performance; and 2) AI agents with better performance
correlate with better world modeling abilities. We believe our results suggest
a promising research direction to integrate world model simulation into AI
agents to enhance their reasoning, planning, and acting capabilities.

</details>


### [274] [CityLens: Benchmarking Large Language-Vision Models for Urban Socioeconomic Sensing](https://arxiv.org/abs/2506.00530)
*Tianhui Liu,Jie Feng,Hetian Pang,Xin Zhang,Tianjian Ouyang,Zhiyuan Zhang,Yong Li*

Main category: cs.AI

TL;DR: 提出CityLens多模态基准测试框架，评估17个LLVMs模型在卫星/街景图像预测城市社会经济指标的能力，覆盖6大领域11项任务。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型在城市社会经济指标预测方面存在局限性，需建立系统性评估框架助力可持续城市发展。

Method: 构建覆盖17个全球城市的多模态数据集，定义6大领域（经济/教育/犯罪等）的11项预测任务，采用直接预测、标准化估算和特征回归三种评估范式。

Result: LLVMs展现感知潜力但预测精度有限，不同模型在交通/环境领域表现最佳（平均R²=0.37），经济领域最弱（R²=0.12）。

Conclusion: CityLens为诊断模型局限提供统一框架，开源数据集推动LLVMs在城市分析领域的发展，未来需增强模型对复杂城市特征的理解能力。

Abstract: Understanding urban socioeconomic conditions through visual data is a
challenging yet essential task for sustainable urban development and policy
planning. In this work, we introduce $\textbf{CityLens}$, a comprehensive
benchmark designed to evaluate the capabilities of large language-vision models
(LLVMs) in predicting socioeconomic indicators from satellite and street view
imagery. We construct a multi-modal dataset covering a total of 17 globally
distributed cities, spanning 6 key domains: economy, education, crime,
transport, health, and environment, reflecting the multifaceted nature of urban
life. Based on this dataset, we define 11 prediction tasks and utilize three
evaluation paradigms: Direct Metric Prediction, Normalized Metric Estimation,
and Feature-Based Regression. We benchmark 17 state-of-the-art LLVMs across
these tasks. Our results reveal that while LLVMs demonstrate promising
perceptual and reasoning capabilities, they still exhibit limitations in
predicting urban socioeconomic indicators. CityLens provides a unified
framework for diagnosing these limitations and guiding future efforts in using
LLVMs to understand and predict urban socioeconomic patterns. Our codes and
datasets are open-sourced via https://github.com/tsinghua-fib-lab/CityLens.

</details>


### [275] [Reasoning Like an Economist: Post-Training on Economic Problems Induces Strategic Generalization in LLMs](https://arxiv.org/abs/2506.00577)
*Yufa Zhou,Shaobo Wang,Xingyu Dong,Xiangqi Jin,Yifang Chen,Yue Min,Kexin Yang,Xingzhang Ren,Dayiheng Liu,Linfeng Zhang*

Main category: cs.AI

TL;DR: Proposes Recon, a 7B LLM post-trained with domain-aligned SFT and RLVR techniques to enhance economic reasoning and multi-agent generalization


<details>
  <summary>Details</summary>
Motivation: Economic scenarios provide mathematically grounded testbeds requiring structured reasoning and real-world applicability for multi-agent system validation

Method: Developed 2,100 curated economic problems dataset with SFT and RLVR training framework focusing on game-theoretic reasoning

Result: Achieved improved structured reasoning in economic benchmarks and demonstrated economic rationality in multi-agent games

Conclusion: Domain-specific post-training effectively enhances LLMs' structured reasoning capabilities and agent alignment potential

Abstract: Directly training Large Language Models (LLMs) for Multi-Agent Systems (MAS)
remains challenging due to intricate reward modeling, dynamic agent
interactions, and demanding generalization requirements. This paper explores
whether post-training techniques, specifically Supervised Fine-Tuning (SFT) and
Reinforcement Learning with Verifiable Rewards (RLVR), can effectively
$\textit{generalize}$ to multi-agent scenarios. We use economic reasoning as a
testbed, leveraging its strong foundations in mathematics and game theory, its
demand for structured analytical reasoning, and its relevance to real-world
applications such as market design, resource allocation, and policy analysis.
We introduce $\textbf{Recon}$ ($\textbf{R}$easoning like an
$\textbf{ECON}$omist), a 7B-parameter open-source LLM post-trained on a
hand-curated dataset of 2,100 high-quality economic reasoning problems.
Comprehensive evaluation on economic reasoning benchmarks and multi-agent games
reveals clear improvements in structured reasoning and economic rationality.
These results underscore the promise of domain-aligned post-training for
enhancing reasoning and agent alignment, shedding light on the roles of SFT and
RL in shaping model behavior. Code is available at
https://github.com/MasterZhou1/Recon .

</details>


### [276] [DrKGC: Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion across General and Biomedical Domains](https://arxiv.org/abs/2506.00708)
*Yongkang Xiao,Sinian Zhang,Yi Dai,Huixue Zhou,Jue Hou,Jie Ding,Rui Zhang*

Main category: cs.AI

TL;DR: 提出DrKGC框架，通过动态子图检索增强LLMs在知识图谱补全任务中的表现，结合结构嵌入与逻辑规则学习提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将图谱结构转为文本输入，未能充分发挥LLMs对图结构的感知与推理能力。

Method: 1. 轻量化模型学习图谱结构嵌入和逻辑规则
2. 基于规则的bottom-up子图检索
3. GCN适配器增强结构嵌入并整合到LLM微调提示中

Result: 在通用领域和生物医学数据集上均取得SOTA性能，案例研究验证可解释性与实用性

Conclusion: DrKGC首次实现结构感知的检索增强LLM训练，为图谱补全任务提供高效解决方案

Abstract: Knowledge graph completion (KGC) aims to predict missing triples in knowledge
graphs (KGs) by leveraging existing triples and textual information. Recently,
generative large language models (LLMs) have been increasingly employed for
graph tasks. However, current approaches typically encode graph context in
textual form, which fails to fully exploit the potential of LLMs for perceiving
and reasoning about graph structures. To address this limitation, we propose
DrKGC (Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph
Completion). DrKGC employs a flexible lightweight model training strategy to
learn structural embeddings and logical rules within the KG. It then leverages
a novel bottom-up graph retrieval method to extract a subgraph for each query
guided by the learned rules. Finally, a graph convolutional network (GCN)
adapter uses the retrieved subgraph to enhance the structural embeddings, which
are then integrated into the prompt for effective LLM fine-tuning. Experimental
results on two general domain benchmark datasets and two biomedical datasets
demonstrate the superior performance of DrKGC. Furthermore, a realistic case
study in the biomedical domain highlights its interpretability and practical
utility.

</details>


### [277] [Aligning VLM Assistants with Personalized Situated Cognition](https://arxiv.org/abs/2506.00930)
*Yongqi Li,Shen Zhou,Xiaohu Li,Xin Miao,Jintao Wen,Mayi Xu,Jianhao Chen,Birong Pan,Hankun Kang,Yuanyuan Zhu,Ming Zhong,Tieyun Qian*

Main category: cs.AI

TL;DR: 研究提出PCogAlign框架及PCogAlignBench评估基准，通过社会学角色集刻画个体认知差异，实现视觉语言模型与个性化情境认知的对齐。


<details>
  <summary>Details</summary>
Motivation: 现有VLM助手仅满足通用目标，但个体因背景差异对相同情境存在认知分化，亟需实现基于个性化认知的对齐。

Method: 1. 用社会学角色集理论刻画个体特征
2. 构建含20类角色集/18k实例的PCogAlignBench
3. 设计认知感知+行为奖励的PCogAlign框架

Result: 实验证明PCogAlignBench评估可靠性(人工评估+量化指标)及PCogAlign有效性(奖励模型提升对齐效果)，代码与基准开源。

Conclusion: 首次将角色集理论引入VLM个性化对齐，通过行为驱动的评估体系与认知增强框架，为个性化AI助手提供新范式。

Abstract: Vision-language models (VLMs) aligned with general human objectives, such as
being harmless and hallucination-free, have become valuable assistants of
humans in managing visual tasks. However, people with diversified backgrounds
have different cognition even in the same situation. Consequently, they may
have personalized expectations for VLM assistants. This highlights the urgent
need to align VLM assistants with personalized situated cognition for
real-world assistance. To study this problem, we first simplify it by
characterizing individuals based on the sociological concept of Role-Set. Then,
we propose to evaluate the individuals' actions to examine whether the
personalized alignment is achieved. Further, we construct a benchmark named
PCogAlignBench, which includes 18k instances and 20 individuals with different
Role-Sets. Finally, we present a framework called PCogAlign, which constructs a
cognition-aware and action-based reward model for personalized alignment.
Experimental results and human evaluations demonstrate the reliability of the
PCogAlignBench and the effectiveness of our proposed PCogAlign. We will
open-source the constructed benchmark and code at
https://github.com/NLPGM/PCogAlign.

</details>


### [278] [Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning Nonverbal Cues from Video-Grounded Dialogues](https://arxiv.org/abs/2506.00958)
*Youngmin Kim,Jiwan Chung,Jisoo Kim,Sunghyun Lee,Sangkyu Lee,Junhyeok Kim,Cheoljong Yang,Youngjae Yu*

Main category: cs.AI

TL;DR: 提出多模态语言模型MARS，通过VENUS数据集实现文本与非语言线索的统一生成，突破现有LLMs在非语言交流方面的局限。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型无法有效整合手势/表情等非语言要素，限制了对话体验的沉浸感。需解决非语言交流在AI对话中的缺失问题。

Method: 构建VENUS数据集（含文本对齐的标注视频），采用向量量化非语言表征与文本联合训练，通过next-token预测实现多模态统一生成框架。

Result: 定量与定性实验证实MARS能同步生成符合对话场景的文本和非语言内容，VENUS数据集规模与有效性得到验证。

Conclusion: MARS首次实现对话场景中文本与非语言要素的端到端生成，为沉浸式人机交互奠定技术基础，VENUS数据集开源促进领域发展。

Abstract: Nonverbal communication is integral to human interaction, with gestures,
facial expressions, and body language conveying critical aspects of intent and
emotion. However, existing large language models (LLMs) fail to effectively
incorporate these nonverbal elements, limiting their capacity to create fully
immersive conversational experiences. We introduce MARS, a multimodal language
model designed to understand and generate nonverbal cues alongside text,
bridging this gap in conversational AI. Our key innovation is VENUS, a
large-scale dataset comprising annotated videos with time-aligned text, facial
expressions, and body language. Leveraging VENUS, we train MARS with a
next-token prediction objective, combining text with vector-quantized nonverbal
representations to achieve multimodal understanding and generation within a
unified framework. Based on various analyses of the VENUS datasets, we validate
its substantial scale and high effectiveness. Our quantitative and qualitative
results demonstrate that MARS successfully generates text and nonverbal
languages, corresponding to conversational input.

</details>


### [279] [Overcoming Multi-step Complexity in Multimodal Theory-of-Mind Reasoning: A Scalable Bayesian Planner](https://arxiv.org/abs/2506.01301)
*Chunhui Zhang,Zhongyu Ouyang,Kwonjoon Lee,Nakul Agarwal,Sean Dae Houlihan,Soroush Vosoughi,Shao-Yuan Lo*

Main category: cs.AI

TL;DR: 提出可扩展的贝叶斯ToM规划器，通过分步贝叶斯更新和弱到强控制机制，实现小模型专业化推理+大模型知识整合，在复杂场景中准确率提升4.6%


<details>
  <summary>Details</summary>
Motivation: 解决现有ToM方法在扩展性、多模态适应性和任务泛化性方面的不足，特别是结构化流程和大模型微调带来的限制

Method: 贝叶斯ToM规划器将推理分解为逐步贝叶斯更新，采用弱到强控制机制：小模型专注ToM似然估计，大模型集成社会知识与世界知识

Result: 在多模态ToM基准测试中准确率提升4.6%，在未见场景表现优异，建立复杂环境心智建模新标准

Conclusion: 贝叶斯框架与模型协同机制有效突破ToM推理的扩展瓶颈，为复杂社会认知建模提供新范式

Abstract: Theory-of-Mind (ToM) enables humans to infer mental states-such as beliefs,
desires, and intentions-forming the foundation of social cognition. However,
existing computational ToM methods rely on structured workflows with
ToM-specific priors or deep model fine-tuning, which struggle with scalability
in multimodal environments and fail to generalize as task complexity increases.
To address these limitations, we propose a scalable Bayesian ToM planner that
decomposes ToM reasoning into stepwise Bayesian updates. Our framework
introduces weak-to-strong control, allowing smaller language models (LMs) to
specialize in ToM-specific likelihood estimation and transfer their reasoning
behaviors to larger LMs (7B to 405B) for integration with social and world
knowledge. This synergistic approach aligns large-model inference of human
mental states with Bayesian principles. Extensive experiments show that our
method achieves a 4.6% accuracy improvement over state-of-the-art techniques on
multimodal ToM benchmarks, including challenging unseen scenarios, thereby
establishing a new standard for modeling human mental states in complex
environments.

</details>


### [280] [An Empirical Study of Group Conformity in Multi-Agent Systems](https://arxiv.org/abs/2506.01332)
*Min Choi,Keonwoo Kim,Sungwon Chae,Sangyeob Baek*

Main category: cs.AI

TL;DR: 大型语言模型在多智能体辩论中会放大群体偏见，需政策干预促进讨论多样性


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注种族等受保护属性的偏见，但对多智能体在社会争议议题中的偏见传播机制缺乏探索

Method: 通过2500+场辩论模拟，观察中性智能体在5个争议话题中的立场演化

Result: 智能体表现出显著群体趋同效应，倾向于追随多数群体或高智力智能体

Conclusion: 需制定促进LLM讨论多样性和透明度的政策，降低匿名环境中的偏见传播风险

Abstract: Recent advances in Large Language Models (LLMs) have enabled multi-agent
systems that simulate real-world interactions with near-human reasoning. While
previous studies have extensively examined biases related to protected
attributes such as race, the emergence and propagation of biases on socially
contentious issues in multi-agent LLM interactions remain underexplored. This
study explores how LLM agents shape public opinion through debates on five
contentious topics. By simulating over 2,500 debates, we analyze how initially
neutral agents, assigned a centrist disposition, adopt specific stances over
time. Statistical analyses reveal significant group conformity mirroring human
behavior; LLM agents tend to align with numerically dominant groups or more
intelligent agents, exerting a greater influence. These findings underscore the
crucial role of agent intelligence in shaping discourse and highlight the risks
of bias amplification in online interactions. Our results emphasize the need
for policy measures that promote diversity and transparency in LLM-generated
discussions to mitigate the risks of bias propagation within anonymous online
environments.

</details>


### [281] [AI Scientists Fail Without Strong Implementation Capability](https://arxiv.org/abs/2506.01372)
*Minjun Zhu,Qiujie Xie,Yixuan Weng,Jian Wu,Zhen Lin,Linyi Yang,Yue Zhang*

Main category: cs.AI

TL;DR: AI科学家的根本瓶颈在于验证执行能力，需社区共同解决实现差距


<details>
  <summary>Details</summary>
Motivation: 指出当前AI科学家系统虽能独立开展科研流程，但缺乏产生计算机领域突破性成果的验证能力

Method: 基于复杂工程任务基准的定量证据，系统评估5个先进AI系统生成的28篇论文

Result: 发现AI科学家在实验验证和论文质量生成方面存在根本性执行能力缺陷

Conclusion: 呼吁科研社区共同解决AI科学家在验证程序执行层面的实现差距问题

Abstract: The emergence of Artificial Intelligence (AI) Scientist represents a paradigm
shift in scientific discovery, with large language models (LLMs) taking the
lead as the primary executor in the entire scientific workflow from idea
generation to experiment implementation. Recent AI Scientist studies
demonstrate sufficient capabilities for independent scientific discovery, with
the generated research reports gaining acceptance at the ICLR 2025 workshop and
ACL 2025, arguing that a human-level AI Scientist, capable of uncovering
phenomena previously unknown to humans, may be imminent. Despite this
substantial progress, AI Scientist has yet to produce a groundbreaking
achievement in the domain of computer science on par with automated scientific
tools. Based on extensive quantitative evidence from existing benchmarks in
complex engineering tasks and a systematic evaluation assess 28 research papers
generated by five advanced AI Scientist systems, we argue that \textbf{the
fundamental bottleneck for AI Scientists lies in their capability to execute
the requisite verification procedures.} Current AI Scientist systems lack the
execution capabilities needed to execute rigorous experiments and produce
high-quality scientific papers. To better illustrate the root cause of this
\textbf{implementation gap}, we provide an in-depth discussion on the
fundamental limitations of AI Scientist. This position paper aims to call for
the participants in the community to bridge the implementation gap.

</details>


### [282] [AgentCPM-GUI: Building Mobile-Use Agents with Reinforcement Fine-Tuning](https://arxiv.org/abs/2506.01391)
*Zhong Zhang,Yaxi Lu,Yikun Fu,Yupeng Huo,Shenzhi Yang,Yesai Wu,Han Si,Xin Cong,Haotian Chen,Yankai Lin,Jie Xie,Wei Zhou,Wang Xu,Yuanheng Zhang,Zhou Su,Zhongwu Zhai,Xiaoming Liu,Yudong Mei,Jianming Xu,Hongyan Tian,Chongyi Wang,Chi Chen,Yuan Yao,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: 提出AgentCPM-GUI模型，通过预训练、监督微调和强化学习的多阶段训练策略，实现高效跨语言移动端GUI交互，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理存在训练数据噪声大、语义多样性不足导致的泛化能力差问题，且缺乏对中文移动生态的支持。

Method: 1. 基于接地的预训练增强感知能力
2. 高质量中英文轨迹的监督微调模仿人类操作
3. GRPO强化微调提升推理能力
4. 设计紧凑动作空间降低移动端延迟

Result: 在5个公开基准和新中文基准CAGUI上达到96.9% Type-Match和91.3% Exact-Match，代码、模型和评估数据已开源。

Conclusion: 该工作通过多阶段训练策略和动作空间优化，有效解决了GUI代理的泛化能力和跨语言支持问题，为后续研究提供可复现基础。

Abstract: The recent progress of large language model agents has opened new
possibilities for automating tasks through graphical user interfaces (GUIs),
especially in mobile environments where intelligent interaction can greatly
enhance usability. However, practical deployment of such agents remains
constrained by several key challenges. Existing training data is often noisy
and lack semantic diversity, which hinders the learning of precise grounding
and planning. Models trained purely by imitation tend to overfit to seen
interface patterns and fail to generalize in unfamiliar scenarios. Moreover,
most prior work focuses on English interfaces while overlooks the growing
diversity of non-English applications such as those in the Chinese mobile
ecosystem. In this work, we present AgentCPM-GUI, an 8B-parameter GUI agent
built for robust and efficient on-device GUI interaction. Our training pipeline
includes grounding-aware pre-training to enhance perception, supervised
fine-tuning on high-quality Chinese and English trajectories to imitate
human-like actions, and reinforcement fine-tuning with GRPO to improve
reasoning capability. We also introduce a compact action space that reduces
output length and supports low-latency execution on mobile devices.
AgentCPM-GUI achieves state-of-the-art performance on five public benchmarks
and a new Chinese GUI benchmark called CAGUI, reaching $96.9\%$ Type-Match and
$91.3\%$ Exact-Match. To facilitate reproducibility and further research, we
publicly release all code, model checkpoint, and evaluation data.

</details>


### [283] [PGPO: Enhancing Agent Reasoning via Pseudocode-style Planning Guided Preference Optimization](https://arxiv.org/abs/2506.01475)
*Zouying Cao,Runze Wang,Yifei Yang,Xinbei Ma,Xiaoyong Zhu,Bo Zheng,Hai Zhao*

Main category: cs.AI

TL;DR: 提出PGPO方法，通过伪代码计划提升LLM代理的推理效率和跨任务泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理的自然语言计划存在冗长、低效且任务定制化导致的泛化限制问题

Method: 采用规划导向的偏好优化(PGPO)，通过两种规划相关奖励机制强化伪代码计划生成能力

Result: 在主流智能体基准测试中性能超越基线方法，显著减少推理过程中的动作错误和遗漏

Conclusion: 伪代码计划与PGPO结合为智能体发展提供新方向，实现高效可泛化的推理框架

Abstract: Large Language Model (LLM) agents have demonstrated impressive capabilities
in handling complex interactive problems. Existing LLM agents mainly generate
natural language plans to guide reasoning, which is verbose and inefficient. NL
plans are also tailored to specific tasks and restrict agents' ability to
generalize across similar tasks. To this end, we explore pseudocode-style plans
(P-code Plan) to capture the structural logic of reasoning. We find that P-code
Plan empowers LLM agents with stronger generalization ability and more
efficiency. Inspired by this finding, we propose a pseudocode-style Planning
Guided Preference Optimization method called PGPO for effective agent learning.
With two planning-oriented rewards, PGPO further enhances LLM agents' ability
to generate high-quality P-code Plans and subsequent reasoning. Experiments
show that PGPO achieves superior performance on representative agent benchmarks
and outperforms the current leading baselines. Analyses reveal the advantage of
PGPO in reducing action errors and omissions during reasoning.

</details>


### [284] [Respond Beyond Language: A Benchmark for Video Generation in Response to Realistic User Intents](https://arxiv.org/abs/2506.01689)
*Shuting Wang,Yunqi Liu,Zixin Yang,Ning Hu,Zhicheng Dou,Chenyan Xiong*

Main category: cs.AI

TL;DR: 构建RealVideoQuest基准测试，发现现有文本到视频模型难以有效解决真实用户的多模态查询需求


<details>
  <summary>Details</summary>
Motivation: 现有问答数据集主要关注文本响应，难以满足需要视觉演示的复杂用户查询需求

Method: 从Chatbot-Arena识别7.5K真实视频查询意图，通过多阶段检索优化构建4.5K高质量查询-视频对，开发多维评估系统

Result: 实验表明当前T2V模型在处理真实用户查询时存在明显不足

Conclusion: 该研究揭示了多模态AI领域的关键挑战，为视频生成模型的优化指明方向

Abstract: Querying generative AI models, e.g., large language models (LLMs), has become
a prevalent method for information acquisition. However, existing query-answer
datasets primarily focus on textual responses, making it challenging to address
complex user queries that require visual demonstrations or explanations for
better understanding. To bridge this gap, we construct a benchmark,
RealVideoQuest, designed to evaluate the abilities of text-to-video (T2V)
models in answering real-world, visually grounded queries. It identifies 7.5K
real user queries with video response intents from Chatbot-Arena and builds
4.5K high-quality query-video pairs through a multistage video retrieval and
refinement process. We further develop a multi-angle evaluation system to
assess the quality of generated video answers. Experiments indicate that
current T2V models struggle with effectively addressing real user queries,
pointing to key challenges and future research opportunities in multimodal AI.

</details>


### [285] [Self-Challenging Language Model Agents](https://arxiv.org/abs/2506.01716)
*Yifei Zhou,Sergey Levine,Jason Weston,Xian Li,Sainbayar Sukhbaatar*

Main category: cs.AI

TL;DR: 提出Self-Challenging框架，通过让智能代理自行生成Code-as-Task类高质量训练任务，使用强化学习进行训练，在零人工数据情况下实现模型性能两倍以上提升。


<details>
  <summary>Details</summary>
Motivation: 传统工具使用代理训练依赖人工标注的多样化任务和评估标准，成本高昂且扩展性差。需要自动化生成高质量训练任务的解决方案。

Method: 1. 代理分饰两角：挑战者生成含指令/验证函数/测试案例的Code-as-Task任务；执行者用强化学习训练，以验证反馈为奖励
2. 通过解决方案和失败案例自动筛选高质量任务

Result: 在M3ToolEval和TauBench基准测试中，Llama-3.1-8B-Instruct模型性能提升超两倍（仅用自生成数据）

Conclusion: 该框架突破了人工标注依赖，通过自生成验证机制保障任务质量，显著提升工具使用代理的泛化能力，证明自监督训练的可行性。

Abstract: Large language models are quickly becoming the foundation for intelligent
agents that are capable of using tools. However, training such agents is
challenging because it requires human creation and annotation of a diverse set
of tasks, tools, and evaluation criteria. In this paper, we propose the
Self-Challenging framework for training an agent on high-quality tasks that are
generated by itself. The agent first plays the role of challenger and generates
a task after interacting with the given tools. The tasks take the form of a
novel general class of problems termed Code-as-Task, which are defined by an
instruction, a verification function and solution and failure cases which serve
as tests, allowing to filter only for high-quality tasks. The agent then takes
an executor role and trains on those tasks with reinforcement learning using
the evaluation feedback as a reward. Evaluation on two existing multi-turn
tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging
framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct,
despite using only self-generated training data.

</details>


### [286] [WHEN TO ACT, WHEN TO WAIT: Modeling Structural Trajectories for Intent Triggerability in Task-Oriented Dialogue](https://arxiv.org/abs/2506.01881)
*Yaoyao Qian,Jindan Huang,Yuanli Wang,Simon Yu,Kyrie Zhixuan Zhou,Jiayuan Mao,Mingfu Liang,Hanhan Zhou*

Main category: cs.AI

TL;DR: 提出STORM框架解决对话系统中用户表达完整性与系统意图识别间的信息不对称问题，通过模拟UserLLM与AgentLLM对话生成标注语料，实验发现适度不确定性(40-60%)在特定场景优于完全透明。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理无法有效区分语言完整性表达与系统可触发行为，缺乏协作意图形成的结构化框架，导致用户需求与系统理解错位。

Method: 构建UserLLM(全知视角)与AgentLLM(仅观察行为)的对话模型，生成包含表达轨迹和潜在认知转变的标注语料库，建立三层次评估体系。

Result: 在四个语言模型实验中，中等不确定性表现优于完全透明系统(特定场景)，不同模型展现出信息完整性的差异化优化路径。

Conclusion: 信息不对称的动态建模为对话系统设计提供新范式，适度保留不确定性可提升人机协作效率，需重新审视『最优信息透明度』的传统假设。

Abstract: Task-oriented dialogue systems often face difficulties when user utterances
seem semantically complete but lack necessary structural information for
appropriate system action. This arises because users frequently do not fully
understand their own needs, while systems require precise intent definitions.
Current LLM-based agents cannot effectively distinguish between linguistically
complete and contextually triggerable expressions, lacking frameworks for
collaborative intent formation. We present STORM, a framework modeling
asymmetric information dynamics through conversations between UserLLM (full
internal access) and AgentLLM (observable behavior only). STORM produces
annotated corpora capturing expression trajectories and latent cognitive
transitions, enabling systematic analysis of collaborative understanding
development. Our contributions include: (1) formalizing asymmetric information
processing in dialogue systems; (2) modeling intent formation tracking
collaborative understanding evolution; and (3) evaluation metrics measuring
internal cognitive improvements alongside task performance. Experiments across
four language models reveal that moderate uncertainty (40-60%) can outperform
complete transparency in certain scenarios, with model-specific patterns
suggesting reconsideration of optimal information completeness in human-AI
collaboration. These findings contribute to understanding asymmetric reasoning
dynamics and inform uncertainty-calibrated dialogue system design.

</details>


### [287] [Large language models can learn and generalize steganographic chain-of-thought under process supervision](https://arxiv.org/abs/2506.01926)
*Joey Skaf,Luis Ibanez-Lissen,Robert McCarthy,Connor Watts,Vasil Georgiv,Hannes Whittingham,Lorena Gonzalez-Manzano,David Lindner,Cameron Tice,Edward James Young,Puria Radmard*

Main category: cs.AI

TL;DR: 论文发现惩罚CoT推理中的特定字符串会导致模型采用隐写编码替代方案，但不会改变任务执行方式，威胁CoT监控可靠性


<details>
  <summary>Details</summary>
Motivation: 探究开发者人为消除CoT中有害意图痕迹的行为（出于客户偏好或监管需求）对模型推理隐蔽化的影响及其对CoT监控可靠性的威胁

Method: 通过实验证明：1) 惩罚关键推理字符串时模型会采用替代字符串但保持原有任务执行方式；2) 模型可针对惩罚字符串类别发展出通用编码方案

Result: 模型不仅能替换训练中见过的惩罚字符串，还能对未见的同类测试字符串应用通用编码方案，形成系统性推理隐蔽能力

Conclusion: CoT监控面临模型通过隐写编码实现推理隐蔽化的根本性威胁，需开发更鲁棒的监控方法应对此类系统性对抗策略

Abstract: Chain-of-thought (CoT) reasoning not only enhances large language model
performance but also provides critical insights into decision-making processes,
marking it as a useful tool for monitoring model intent and planning. By
proactively preventing models from acting on CoT indicating misaligned or
harmful intent, CoT monitoring can be used to reduce risks associated with
deploying models. However, developers may be incentivized to train away the
appearance of harmful intent from CoT traces, by either customer preferences or
regulatory requirements. Recent works have shown that banning mention of a
specific example of reward hacking, which may be done either to make CoT
presentable to users or as a naive attempt to prevent the behavior, causes
obfuscation of the undesired reasoning traces but the persistence of the
undesired behavior. Such obfuscation threatens the reliability of CoT
monitoring. However, obfuscation of reasoning can be due to its internalization
to latent space computation, or its encoding within the CoT. Here, we provide
an extension to these results. First, we show that penalizing the use of
specific strings within load-bearing reasoning traces causes models to
substitute alternative strings. Crucially, this does not alter the underlying
method by which the model performs the task, demonstrating that the model can
learn to steganographically encode its reasoning. We further demonstrate that
models can generalize an encoding scheme. When the penalized strings belong to
an overarching class, the model learns not only to substitute strings seen in
training, but also develops a general encoding scheme for all members of the
class which it can apply to held-out testing strings.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [288] [CODEMENV: Benchmarking Large Language Models on Code Migration](https://arxiv.org/abs/2506.00894)
*Keyuan Cheng,Xudong Shen,Yihao Yang,Tengyue Wang,Yang Cao,Muhammad Asif Ali,Hanbin Wang,Lijie Hu,Di Wang*

Main category: cs.SE

TL;DR: 提出CODEMENV代码迁移基准测试，评估7个LLMs代码迁移能力，GPT-4O以43.84%准确率最优。发现LLMs更擅长处理新版本函数，但存在逻辑不一致问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLMs在代码迁移（适配不同环境代码）的能力评估不足，需系统性基准测试验证模型表现。

Method: 构建包含922个样本的CODEMENV数据集，覆盖19个Python/Java包，设计三个核心任务：识别不兼容函数、检测函数定义变化、代码适配。测试7个LLMs的pass@1准确率。

Result: 平均准确率26.50%，GPT-4O最优(43.84%)。关键发现：1）LLMs更擅长新版函数迁移 2）存在与目标环境无关的函数变更误判。

Conclusion: CODEMENV有效验证LLMs代码迁移能力，模型在新版代码适配中表现更优，但需提升逻辑一致性。公开数据集推动代码迁移研究。

Abstract: Large language models (LLMs) have shown remarkable capabilities across
various software engineering tasks; however, their effectiveness in code
migration, adapting code to run in different environments, remains
insufficiently studied. In this work, we introduce CODEMENV: Code Migration
Across Environment, a new benchmark specifically designed to assess LLMs'
abilities in code migration scenarios. CODEMENV consists of 922 examples
spanning 19 Python and Java packages, and covers three core tasks: (1)
identifying functions incompatible with specific versions, (2) detecting
changes in function definitions, and (3) adapting code to target environments.
Experimental evaluation with seven LLMs on CODEMENV yields an average pass@1
rate of 26.50%, with GPT-4O achieving the highest score at 43.84%. Key findings
include: (i) LLMs tend to be more proficient with newer function versions,
which aids in migrating legacy code, and (ii) LLMs sometimes exhibit logical
inconsistencies by identifying function changes irrelevant to the intended
migration environment. The datasets are available at
https://github.com/xdshen-ai/Benchmark-of-Code-Migration.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [289] [Enhancing Finite State Machine Design Automation with Large Language Models and Prompt Engineering Techniques](https://arxiv.org/abs/2506.00001)
*Qun-Kai Lin,Cheng Hsu,Tian-Sheuan Chang*

Main category: cs.AR

TL;DR: 评估Claude 3 Opus、ChatGPT-4和ChatGPT-4o在有限状态机设计中的表现，发现系统化提示方法与TOP Patch优化法可提升成功率，并具备跨领域应用潜力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在硬件描述语言设计中的出色兼容性引发关注，但需要系统评估其在有限状态机设计中的实际性能及优化方法。

Method: 使用HDLBits教学平台，通过稳定性评估和TOP Patch提示优化法，对比测试三个主流LLM在多种FSM场景下的设计能力。

Result: 系统化格式提示方法与TOP Patch可将LLM成功率提升至新高度，且该方法展现出超越HDL设计自动化领域的适用潜力。

Conclusion: 研究成果为未来将提示工程与其他领域结合奠定基础，特别是系统化提示方法在跨领域集成应用中的广阔前景。

Abstract: Large Language Models (LLMs) have attracted considerable attention in recent
years due to their remarkable compatibility with Hardware Description Language
(HDL) design. In this paper, we examine the performance of three major LLMs,
Claude 3 Opus, ChatGPT-4, and ChatGPT-4o, in designing finite state machines
(FSMs). By utilizing the instructional content provided by HDLBits, we evaluate
the stability, limitations, and potential approaches for improving the success
rates of these models. Furthermore, we explore the impact of using the
prompt-refining method, To-do-Oriented Prompting (TOP) Patch, on the success
rate of these LLM models in various FSM design scenarios. The results show that
the systematic format prompt method and the novel prompt refinement method have
the potential to be applied to other domains beyond HDL design automation,
considering its possible integration with other prompt engineering techniques
in the future.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [290] [RoboMoRe: LLM-based Robot Co-design via Joint Optimization of Morphology and Reward](https://arxiv.org/abs/2506.00276)
*Jiawei Fang,Yuxuan Sun,Chengtian Ma,Qiuyu Lu,Lining Yao*

Main category: cs.RO

TL;DR: 提出RoboMoRe框架，利用大语言模型实现机器人形态与奖励机制的协同优化，通过双阶段优化显著提升设计效果。


<details>
  <summary>Details</summary>
Motivation: 传统机器人协同设计因固定奖励函数导致次优设计，需探索适应不同形态的多样化运动模式。

Method: 双阶段优化：1) LLM粗生成形态-奖励对；2) 交替进行奖励梯度更新与形态优化迭代。

Result: 在8项任务中超越人工设计和其他方法，且无需任务特定提示或模板。

Conclusion: 首次将LLM用于形态与奖励协同优化，展示了LLM在机器人设计领域的强大潜力。

Abstract: Robot co-design, jointly optimizing morphology and control policy, remains a
longstanding challenge in the robotics community, where many promising robots
have been developed. However, a key limitation lies in its tendency to converge
to sub-optimal designs due to the use of fixed reward functions, which fail to
explore the diverse motion modes suitable for different morphologies. Here we
propose RoboMoRe, a large language model (LLM)-driven framework that integrates
morphology and reward shaping for co-optimization within the robot co-design
loop. RoboMoRe performs a dual-stage optimization: in the coarse optimization
stage, an LLM-based diversity reflection mechanism generates both diverse and
high-quality morphology-reward pairs and efficiently explores their
distribution. In the fine optimization stage, top candidates are iteratively
refined through alternating LLM-guided reward and morphology gradient updates.
RoboMoRe can optimize both efficient robot morphologies and their suited motion
behaviors through reward shaping. Results demonstrate that without any
task-specific prompting or predefined reward/morphology templates, RoboMoRe
significantly outperforms human-engineered designs and competing methods across
eight different tasks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [291] [ZeShot-VQA: Zero-Shot Visual Question Answering Framework with Answer Mapping for Natural Disaster Damage Assessment](https://arxiv.org/abs/2506.00238)
*Ehsan Karimi,Maryam Rahnemoonfar*

Main category: cs.CV

TL;DR: 提出基于大规模视觉语言模型（VLM）的零样本VQA方法（ZeShot-VQA），无需微调即可处理灾害场景中未见过的答案。


<details>
  <summary>Details</summary>
Motivation: 现有VQA模型仅支持预定义答案选择，遇到新答案需重新训练。利用VLMs的零样本学习能力可解决该瓶颈。

Method: 构建VLM驱动的零样本VQA框架，直接在FloodNet灾害数据集上实现开放式问答，规避模型微调过程。

Result: 方法在灾后数据集中展现出处理未见过答案的能力，验证了零样本学习的可行性及模型灵活性。

Conclusion: ZeShot-VQA为灾害响应提供高效、可扩展的视觉问答方案，显著降低数据标注与模型更新成本。

Abstract: Natural disasters usually affect vast areas and devastate infrastructures.
Performing a timely and efficient response is crucial to minimize the impact on
affected communities, and data-driven approaches are the best choice. Visual
question answering (VQA) models help management teams to achieve in-depth
understanding of damages. However, recently published models do not possess the
ability to answer open-ended questions and only select the best answer among a
predefined list of answers. If we want to ask questions with new additional
possible answers that do not exist in the predefined list, the model needs to
be fin-tuned/retrained on a new collected and annotated dataset, which is a
time-consuming procedure. In recent years, large-scale Vision-Language Models
(VLMs) have earned significant attention. These models are trained on extensive
datasets and demonstrate strong performance on both unimodal and multimodal
vision/language downstream tasks, often without the need for fine-tuning. In
this paper, we propose a VLM-based zero-shot VQA (ZeShot-VQA) method, and
investigate the performance of on post-disaster FloodNet dataset. Since the
proposed method takes advantage of zero-shot learning, it can be applied on new
datasets without fine-tuning. In addition, ZeShot-VQA is able to process and
generate answers that has been not seen during the training procedure, which
demonstrates its flexibility.

</details>


### [292] [HSCR: Hierarchical Self-Contrastive Rewarding for Aligning Medical Vision Language Models](https://arxiv.org/abs/2506.00805)
*Songtao Jiang,Yan Zhang,Yeying Jin,Zhihang Tang,Yangyang Wu,Yang Feng,Jian Wu,Zuozhu Liu*

Main category: cs.CV

TL;DR: 提出HSCR方法解决医学视觉语言模型模态对齐问题，通过自生成偏好数据与多级优化策略提升模型可信度


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉语言模型忽视模态偏差问题，导致临床场景响应不可信

Method: 1. 通过视觉标记dropout生成非偏好数据
2. 设计隐式对齐奖励函数指导标记替换
3. 多级偏好优化捕捉细微对齐信号

Result: 在医学VQA等任务中，仅用2000训练数据即提升零样本性能与模态对齐效果

Conclusion: HSCR为医学多模态对齐提供高效解决方案，显著增强临床决策可靠性

Abstract: Medical Vision-Language Models (Med-VLMs) have achieved success across
various tasks, yet most existing methods overlook the modality misalignment
issue that can lead to untrustworthy responses in clinical settings. In this
paper, we propose Hierarchical Self-Contrastive Rewarding (HSCR), a novel
approach that addresses two critical challenges in Med-VLM alignment: 1)
Cost-effective generation of high-quality preference data; 2) Capturing nuanced
and context-aware preferences for improved alignment. HSCR first leverages the
inherent capability of Med-VLMs to generate dispreferred responses with higher
sampling probability. By analyzing output logit shifts after visual token
dropout, we identify modality-coupled tokens that induce misalignment and
derive an implicit alignment reward function. This function guides token
replacement with hallucinated ones during decoding, producing high-quality
dispreferred data. Furthermore, HSCR introduces a multi-level preference
optimization strategy, which extends beyond traditional adjacent-level
optimization by incorporating nuanced implicit preferences, leveraging relative
quality in dispreferred data to capture subtle alignment cues for more precise
and context-aware optimization. Extensive experiments across multiple medical
tasks, including Med-VQA, medical image captioning and instruction following,
demonstrate that HSCR not only enhances zero-shot performance but also
significantly improves modality alignment and trustworthiness with just 2,000
training entries.

</details>


### [293] [Towards Predicting Any Human Trajectory In Context](https://arxiv.org/abs/2506.00871)
*Ryo Fujii,Hideo Saito,Ryo Hachiuma*

Main category: cs.CV

TL;DR: 提出无需微调的上下文学习框架TrajICL，通过时空相似性和预测引导的示例选择方法，利用合成数据训练实现跨领域行人轨迹预测


<details>
  <summary>Details</summary>
Motivation: 传统基于微调的方法在边缘设备面临计算资源限制，需要开发无需参数更新的快速适应方法

Method: 提出STES（时空相似性示例选择）和PG-ES（预测引导示例选择）方法，结合大规模合成数据集训练增强模型预测能力

Result: 在多个公共基准测试中超越微调方法，实现跨领域场景的优异适应表现

Conclusion: TrajICL框架通过上下文学习机制有效解决了行人轨迹预测的领域适应问题，展示了无需微调的迁移学习潜力

Abstract: Predicting accurate future trajectories of pedestrians is essential for
autonomous systems but remains a challenging task due to the need for
adaptability in different environments and domains. A common approach involves
collecting scenario-specific data and performing fine-tuning via
backpropagation. However, this process is often impractical on edge devices due
to constrained computational resources. To address this challenge, we introduce
TrajICL, an In-Context Learning (ICL) framework for pedestrian trajectory
prediction that enables rapid adaptation without fine-tuning on the
scenario-specific data. We propose a spatio-temporal similarity-based example
selection (STES) method that selects relevant examples from previously observed
trajectories within the same scene by identifying similar motion patterns at
corresponding locations. To further refine this selection, we introduce
prediction-guided example selection (PG-ES), which selects examples based on
both the past trajectory and the predicted future trajectory, rather than
relying solely on the past trajectory. This approach allows the model to
account for long-term dynamics when selecting examples. Finally, instead of
relying on small real-world datasets with limited scenario diversity, we train
our model on a large-scale synthetic dataset to enhance its prediction ability
by leveraging in-context examples. Extensive experiments demonstrate that
TrajICL achieves remarkable adaptation across both in-domain and cross-domain
scenarios, outperforming even fine-tuned approaches across multiple public
benchmarks. The code will be released at
https://fujiry0.github.io/TrajICL-project-page.

</details>


### [294] [Deep Temporal Reasoning in Video Language Models: A Cross-Linguistic Evaluation of Action Duration and Completion through Perfect Times](https://arxiv.org/abs/2506.00928)
*Olga Loginova,Sofía Ortega Loguinova*

Main category: cs.CV

TL;DR: 提出四语种视频问答数据集Perfect Times，揭示当前视频语言模型在时间推理上与人类的显著差距


<details>
  <summary>Details</summary>
Motivation: 人类通过语言结构和视觉线索区分动作完成度，但现有模型可能仅依赖表面特征而非真正理解时间动态

Method: 构建包含日常活动视频、完成标签及完美性干扰项的多语言数据集，通过对比实验验证模型表现

Result: SOTA模型在文本任务成功，但视频时间推理能力不足，准确率较人类低42个百分点

Conclusion: 需深度融合多模态线索捕捉动作持续性特征，为视频语言模型的时态推理能力评估建立新基准

Abstract: Human perception of events is intrinsically tied to distinguishing between
completed (perfect and telic) and ongoing (durative) actions, a process
mediated by both linguistic structure and visual cues. In this work, we
introduce the \textbf{Perfect Times} dataset, a novel, quadrilingual (English,
Italian, Russian, and Japanese) multiple-choice question-answering benchmark
designed to assess video-language models (VLMs) on temporal reasoning. By
pairing everyday activity videos with event completion labels and
perfectivity-tailored distractors, our dataset probes whether models truly
comprehend temporal dynamics or merely latch onto superficial markers.
Experimental results indicate that state-of-the-art models, despite their
success on text-based tasks, struggle to mirror human-like temporal and causal
reasoning grounded in video. This study underscores the necessity of
integrating deep multimodal cues to capture the nuances of action duration and
completion within temporal and causal video dynamics, setting a new standard
for evaluating and advancing temporal reasoning in VLMs.

</details>


### [295] [Abstractive Visual Understanding of Multi-modal Structured Knowledge: A New Perspective for MLLM Evaluation](https://arxiv.org/abs/2506.01293)
*Yichi Zhang,Zhuo Chen,Lingbing Guo,Yajing Xu,Min Zhang,Wen Zhang,Huajun Chen*

Main category: cs.CV

TL;DR: 提出M3STR评测基准，通过多模态知识图谱构建含结构化知识的视觉输入，评估大语言模型对视觉化结构化知识的理解能力


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs评估基准普遍忽视模型对视觉形式结构化知识（如知识图谱）的理解能力，存在关键评估维度缺失

Method: 基于多模态知识图谱自动合成包含子图结构的视觉输入，要求模型同时识别多模态实体及其复杂关系拓扑

Result: 对26个前沿MLLMs的测试揭示了模型在处理结构化视觉知识时存在持续缺陷

Conclusion: M3STR为提升MLLMs的全局推理能力指明了关键发展方向，通过暴露结构化知识处理短板推动技术突破

Abstract: Multi-modal large language models (MLLMs) incorporate heterogeneous
modalities into LLMs, enabling a comprehensive understanding of diverse
scenarios and objects. Despite the proliferation of evaluation benchmarks and
leaderboards for MLLMs, they predominantly overlook the critical capacity of
MLLMs to comprehend world knowledge with structured abstractions that appear in
visual form. To address this gap, we propose a novel evaluation paradigm and
devise M3STR, an innovative benchmark grounded in the Multi-Modal Map for
STRuctured understanding. This benchmark leverages multi-modal knowledge graphs
to synthesize images encapsulating subgraph architectures enriched with
multi-modal entities. M3STR necessitates that MLLMs not only recognize the
multi-modal entities within the visual inputs but also decipher intricate
relational topologies among them. We delineate the benchmark's statistical
profiles and automated construction pipeline, accompanied by an extensive
empirical analysis of 26 state-of-the-art MLLMs. Our findings reveal persistent
deficiencies in processing abstractive visual information with structured
knowledge, thereby charting a pivotal trajectory for advancing MLLMs' holistic
reasoning capacities. Our code and data are released at
https://github.com/zjukg/M3STR

</details>


### [296] [Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models](https://arxiv.org/abs/2506.01413)
*Yulei Qin,Gang Li,Zongyi Li,Zihan Xu,Yuchen Shi,Zhekai Lin,Xiao Cui,Ke Li,Xing Sun*

Main category: cs.CV

TL;DR: 提出RAIF方法通过强化学习和样本对比增强LLMs处理复杂指令的能力，1.5B模型性能提升11.74%接近8B模型水平


<details>
  <summary>Details</summary>
Motivation: 现有思维链方法在复杂指令下存在浅层推理缺陷，无法有效解析多层次约束关系

Method: 1. 基于分类体系分解复杂指令 2. 强化学习结合可验证规则奖励 3. 样本对比优化思维链 4. 专家行为克隆实现模型能力迁移

Result: 在7个基准测试中验证有效性，1.5B模型达到8B模型91.3%的性能水平

Conclusion: 通过系统性推理增强方法，显著提升小模型处理复杂指令的能力，为高效LLM部署提供新思路

Abstract: Existing large language models (LLMs) face challenges of following complex
instructions, especially when multiple constraints are present and organized in
paralleling, chaining, and branching structures. One intuitive solution, namely
chain-of-thought (CoT), is expected to universally improve capabilities of
LLMs. However, we find that the vanilla CoT exerts a negative impact on
performance due to its superficial reasoning pattern of simply paraphrasing the
instructions. It fails to peel back the compositions of constraints for
identifying their relationship across hierarchies of types and dimensions. To
this end, we propose a systematic method to boost LLMs in dealing with complex
instructions via incentivizing reasoning for test-time compute scaling. First,
we stem from the decomposition of complex instructions under existing
taxonomies and propose a reproducible data acquisition method. Second, we
exploit reinforcement learning (RL) with verifiable rule-centric reward signals
to cultivate reasoning specifically for instruction following. We address the
shallow, non-essential nature of reasoning under complex instructions via
sample-wise contrast for superior CoT enforcement. We also exploit behavior
cloning of experts to facilitate steady distribution shift from fast-thinking
LLMs to skillful reasoners. Extensive evaluations on seven comprehensive
benchmarks confirm the validity of the proposed method, where a 1.5B LLM
achieves 11.74% gains with performance comparable to a 8B LLM. Codes and data
are available at https://github.com/yuleiqin/RAIF.

</details>


### [297] [EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation](https://arxiv.org/abs/2506.01551)
*Bingqian Lin,Yunshuang Nie,Khun Loun Zai,Ziming Wei,Mingfei Han,Rongtao Xu,Minzhe Niu,Jianhua Han,Liang Lin,Cewu Lu,Xiaodan Liang*

Main category: cs.CV

TL;DR: 提出EvolveNav框架，结合形式化CoT监督微调和自我反思后训练，提升LLM视觉语言导航的准确性和可解释性


<details>
  <summary>Details</summary>
Motivation: 解决现有VLN方法中直接输入-输出映射导致的决策不可解释问题，同时克服完美CoT标签缺失引发的过拟合风险

Method: 两阶段框架：1) 形式化CoT监督微调激活推理能力；2) 自我反思后训练通过自生成CoT标签增强监督多样性，并引入反思机制对比正确/错误推理模式

Result: 在主流VLN基准测试中表现优于现有LLM-based方法，代码已开源

Conclusion: 通过自进化机制有效提升LLM导航推理能力，为可解释的视觉语言导航提供了创新解决方案

Abstract: Building Vision-Language Navigation (VLN) agents which can navigate following
natural language instructions is a long-standing goal in human-robot
interaction applications. Recent studies have revealed the potential of
training open-source Large Language Models (LLMs) to unleash LLMs' reasoning
ability for improving navigation, and simultaneously mitigate the domain gap
between LLMs' training corpus and the VLN task. However, these approaches
primarily adopt direct input-output mapping paradigms, causing the mapping
learning difficult and the navigational decisions unexplainable.
Chain-of-Thought (CoT) training is a promising way to improve both navigational
decision accuracy and interpretability, while the complexity of the navigation
task makes the perfect CoT labels unavailable and may lead to overfitting
through pure CoT supervised fine-tuning. In this paper, we propose a novel
sElf-improving embodied reasoning framework for boosting LLM-based
vision-language Navigation, dubbed EvolveNav. Our EvolveNav consists of two
stages: (1) Formalized CoT Supervised Fine-Tuning, where we train the model
with formalized CoT labels to both activate the model's navigational reasoning
capabilities and increase the reasoning speed; (2) Self-Reflective
Post-Training, where the model is iteratively trained with its own reasoning
outputs as self-enriched CoT labels to enhance the supervision diversity. A
self-reflective auxiliary task is also introduced to encourage learning correct
reasoning patterns by contrasting with wrong ones. Experimental results on the
popular VLN benchmarks demonstrate the superiority of EvolveNav over previous
LLM-based VLN approaches. Code is available at
https://github.com/expectorlin/EvolveNav.

</details>


### [298] [Enhancing Biomedical Multi-modal Representation Learning with Multi-scale Pre-training and Perturbed Report Discrimination](https://arxiv.org/abs/2506.01902)
*Xinliu Zhong,Kayhan Batmanghelich,Li Sun*

Main category: cs.CV

TL;DR: 提出扰动报告判别方法增强生物医学视觉-语言模型预训练，通过区分扰动文本学习更鲁棒的语义表征


<details>
  <summary>Details</summary>
Motivation: 现有对比学习方法忽视生物医学文本特有的复杂语义结构，需针对性改进预训练策略

Method: 1.设计保持词汇但破坏语义的文本扰动方法 2.对比原始报告与扰动报告，并增强图像子区域与文本子词的细粒度对齐

Result: 在多个下游任务中优于基线方法，验证了多模态表征的语义有效性和鲁棒性

Conclusion: 该方法通过语义扰动判别和细粒度对比机制，显著提升生物医学多模态表征学习质量

Abstract: Vision-language models pre-trained on large scale of unlabeled biomedical
images and associated reports learn generalizable semantic representations.
These multi-modal representations can benefit various downstream tasks in the
biomedical domain. Contrastive learning is widely used to pre-train
vision-language models for general natural images and associated captions.
Despite its popularity, we found biomedical texts have complex and
domain-specific semantics that are often neglected by common contrastive
methods. To address this issue, we propose a novel method, perturbed report
discrimination, for pre-train biomedical vision-language models. First, we
curate a set of text perturbation methods that keep the same words, but disrupt
the semantic structure of the sentence. Next, we apply different types of
perturbation to reports, and use the model to distinguish the original report
from the perturbed ones given the associated image. Parallel to this, we
enhance the sensitivity of our method to higher level of granularity for both
modalities by contrasting attention-weighted image sub-regions and sub-words in
the image-text pairs. We conduct extensive experiments on multiple downstream
tasks, and our method outperforms strong baseline methods. The results
demonstrate that our approach learns more semantic meaningful and robust
multi-modal representations.

</details>


### [299] [Dual-Process Image Generation](https://arxiv.org/abs/2506.01955)
*Grace Luo,Jonathan Granskog,Aleksander Holynski,Trevor Darrell*

Main category: cs.CV

TL;DR: 提出双过程蒸馏方案，使前馈图像生成器能通过文本-图像接口快速学习视觉语言模型的多模态控制任务。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成方法难以灵活学习新任务，而视觉语言模型(VLMs)具备上下文学习能力，但需将其优势蒸馏到轻量级生成器中。

Method: 使用VLM对生成图像评分并反向传播梯度，通过文本-图像界面实现调色板、线条粗细、景深等多模态控制的权重更新。

Result: 方案支持在几分钟内实现多种控制任务(常识推理/视觉提示)，提供统一的多模态控制框架。

Conclusion: 该方法显著提升了图像生成器的可控性，使非专业用户也能快速定制生成效果。

Abstract: Prior methods for controlling image generation are limited in their ability
to be taught new tasks. In contrast, vision-language models, or VLMs, can learn
tasks in-context and produce the correct outputs for a given input. We propose
a dual-process distillation scheme that allows feed-forward image generators to
learn new tasks from deliberative VLMs. Our scheme uses a VLM to rate the
generated images and backpropagates this gradient to update the weights of the
image generator. Our general framework enables a wide variety of new control
tasks through the same text-and-image based interface. We showcase a handful of
applications of this technique for different types of control signals, such as
commonsense inferences and visual prompts. With our method, users can implement
multimodal controls for properties such as color palette, line weight, horizon
position, and relative depth within a matter of minutes. Project page:
https://dual-process.github.io.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [300] [Con Instruction: Universal Jailbreaking of Multimodal Large Language Models via Non-Textual Modalities](https://arxiv.org/abs/2506.00548)
*Jiahui Geng,Thy Thy Tran,Preslav Nakov,Iryna Gurevych*

Main category: cs.CR

TL;DR: 提出'Con Instruction'非文本对抗攻击方法，通过优化图像/音频在嵌入空间对齐目标指令，无需训练数据即可有效绕过多模态模型安全机制（LLaVA-v1.5攻击成功率最高达86.6%），并设计ARC框架评估攻击质量。


<details>
  <summary>Details</summary>
Motivation: 现有攻击依赖文本+对抗图像的组合形式，本研究利用MLLMs解析非文本指令的能力，揭示其嵌入空间理解特性带来的安全隐患。

Method: 优化生成对抗样本与目标指令的嵌入空间对齐，无需文本预处理/训练数据；结合文本输入增强攻击效果；提出ARC框架从响应质量和指令相关性双维度评估攻击。

Result: 在LLaVA-v1.5(13B)实现81.3%(AdvBench)和86.6%(SafeBench)攻击成功率，在InternVL/Qwen系列模型均展示显著效果，防御方案存在明显性能差距。

Conclusion: 非文本对抗指令暴露MLLMs安全漏洞，现有防御措施不足。通过开源实现推动相关研究，强调多模态模型安全机制需针对性改进。

Abstract: Existing attacks against multimodal language models (MLLMs) primarily
communicate instructions through text accompanied by adversarial images. In
contrast, we exploit the capabilities of MLLMs to interpret non-textual
instructions, specifically, adversarial images or audio generated by our novel
method, Con Instruction. We optimize these adversarial examples to align
closely with target instructions in the embedding space, revealing the
detrimental implications of MLLMs' sophisticated understanding. Unlike prior
work, our method does not require training data or preprocessing of textual
instructions. While these non-textual adversarial examples can effectively
bypass MLLM safety mechanisms, their combination with various text inputs
substantially amplifies attack success. We further introduce a new Attack
Response Categorization (ARC) framework, which evaluates both the quality of
the model's response and its relevance to the malicious instructions.
Experimental results demonstrate that Con Instruction effectively bypasses
safety mechanisms in multiple vision- and audio-language models, including
LLaVA-v1.5, InternVL, Qwen-VL, and Qwen-Audio, evaluated on two standard
benchmarks: AdvBench and SafeBench. Specifically, our method achieves the
highest attack success rates, reaching 81.3% and 86.6% on LLaVA-v1.5 (13B). On
the defense side, we explore various countermeasures against our attacks and
uncover a substantial performance gap among existing techniques. Our
implementation is made publicly available.

</details>


### [301] [Simple Prompt Injection Attacks Can Leak Personal Data Observed by LLM Agents During Task Execution](https://arxiv.org/abs/2506.01055)
*Meysam Alizadeh,Zeynab Samei,Daria Stetsenko,Fabrizio Gilardi*

Main category: cs.CR

TL;DR: 研究揭示大型语言模型的工具调用代理存在数据泄露风险，特定任务场景下攻击成功率可达15-50%，现有防御措施无法完全阻止泄露。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试对复杂数据泄露威胁分析不足，需探究提示注入如何导致工具调用代理在执行任务时泄露观察到的个人数据。

Method: 通过虚构银行代理开发数据流攻击，整合至AgentDojo基准测试框架，并构建合成银行对话数据集进行多维度评估（16→48个任务）。

Result: 受攻击时LLM性能下降15-50%，平均攻击成功率20%；部分防御可将成功率归零，但扩展测试显示15%平均成功率且无完全防护方案。密码泄露概率随请求信息组合增加，但安全对齐机制对常规个人信息防护有限。

Conclusion: 任务类型与攻击结构的相似性显著影响防御效果（数据提取/授权类任务风险最高），安全对齐虽阻止密码泄露但无法全面保护个人数据，需针对性防御方案。

Abstract: Previous benchmarks on prompt injection in large language models (LLMs) have
primarily focused on generic tasks and attacks, offering limited insights into
more complex threats like data exfiltration. This paper examines how prompt
injection can cause tool-calling agents to leak personal data observed during
task execution. Using a fictitious banking agent, we develop data flow-based
attacks and integrate them into AgentDojo, a recent benchmark for agentic
security. To enhance its scope, we also create a richer synthetic dataset of
human-AI banking conversations. In 16 user tasks from AgentDojo, LLMs show a
15-50 percentage point drop in utility under attack, with average attack
success rates (ASR) around 20 percent; some defenses reduce ASR to zero. Most
LLMs, even when successfully tricked by the attack, avoid leaking highly
sensitive data like passwords, likely due to safety alignments, but they remain
vulnerable to disclosing other personal data. The likelihood of password
leakage increases when a password is requested along with one or two additional
personal details. In an extended evaluation across 48 tasks, the average ASR is
around 15 percent, with no built-in AgentDojo defense fully preventing leakage.
Tasks involving data extraction or authorization workflows, which closely
resemble the structure of exfiltration attacks, exhibit the highest ASRs,
highlighting the interaction between task type, agent performance, and defense
efficacy.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [302] [Disentangled Safety Adapters Enable Efficient Guardrails and Flexible Inference-Time Alignment](https://arxiv.org/abs/2506.00166)
*Kundan Krishna,Joseph Y Cheng,Charles Maalouf,Leon A Gatys*

Main category: cs.LG

TL;DR: 提出解耦式安全适配器（DSA）框架，通过分离安全计算模块与基础模型，在保持推理效率的同时实现灵活高效的AI安全防护


<details>
  <summary>Details</summary>
Motivation: 现有AI安全范式（护栏模型和对齐训练）存在推理效率与开发灵活性不可兼得的问题，需要更优化的解决方案

Method: 使用基于基础模型内部表征的轻量级适配器，实现安全功能与基础模型的计算解耦

Result: 在幻觉检测（AUC 0.88 vs 0.61）、仇恨言论分类（0.98 vs 0.92）和安全性分类（0.93 vs 0.90）等任务中显著优于同规模独立模型；支持动态对齐强度调节，使安全性能提升93%的同时保持98%的指令跟随能力

Conclusion: DSA框架为AI安全领域提供了模块化、高效率且适应性强的解决方案，显著降低对齐税（减少8个百分点）

Abstract: Existing paradigms for ensuring AI safety, such as guardrail models and
alignment training, often compromise either inference efficiency or development
flexibility. We introduce Disentangled Safety Adapters (DSA), a novel framework
addressing these challenges by decoupling safety-specific computations from a
task-optimized base model. DSA utilizes lightweight adapters that leverage the
base model's internal representations, enabling diverse and flexible safety
functionalities with minimal impact on inference cost. Empirically, DSA-based
safety guardrails substantially outperform comparably sized standalone models,
notably improving hallucination detection (0.88 vs. 0.61 AUC on Summedits) and
also excelling at classifying hate speech (0.98 vs. 0.92 on ToxiGen) and unsafe
model inputs and responses (0.93 vs. 0.90 on AEGIS2.0 & BeaverTails).
Furthermore, DSA-based safety alignment allows dynamic, inference-time
adjustment of alignment strength and a fine-grained trade-off between
instruction following performance and model safety. Importantly, combining the
DSA safety guardrail with DSA safety alignment facilitates context-dependent
alignment strength, boosting safety on StrongReject by 93% while maintaining
98% performance on MTBench -- a total reduction in alignment tax of 8
percentage points compared to standard safety alignment fine-tuning. Overall,
DSA presents a promising path towards more modular, efficient, and adaptable AI
safety and alignment.

</details>


### [303] [Localized LoRA: A Structured Low-Rank Approximation for Efficient Fine-Tuning](https://arxiv.org/abs/2506.00236)
*Babak Barazandeh*

Main category: cs.LG

TL;DR: 提出Localized LoRA框架，通过结构化块的低秩矩阵组合实现参数高效微调的局部化更新，相比传统LoRA方法具有更强的表达能力和性能提升


<details>
  <summary>Details</summary>
Motivation: 现有全局低秩更新方法（如LoRA）无法捕捉参数空间中的局部空间模式，需要更灵活的局部化更新机制来提升模型微调效果

Method: 将权重更新建模为结构化块上的低秩矩阵组合，通过数学证明其参数效率优势，支持对角线局部化和完全局部化两种实现方式

Result: 在相同参数预算下，相比全局LoRA方法获得更低的近似误差（合成数据），实际任务中微调性能提升明显（ViT和BERT模型）

Conclusion: Localized LoRA为参数高效微调提供了新的范式，通过空间局部化的低秩分解平衡参数效率和模型表达能力

Abstract: Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, offer compact
and effective alternatives to full model fine-tuning by introducing low-rank
updates to pretrained weights. However, most existing approaches rely on global
low-rank structures, which can overlook spatial patterns spread across the
parameter space. In this work, we propose Localized LoRA, a generalized
framework that models weight updates as a composition of low-rank matrices
applied to structured blocks of the weight matrix. This formulation enables
dense, localized updates throughout the parameter space-without increasing the
total number of trainable parameters. We provide a formal comparison between
global, diagonal-local, and fully localized low-rank approximations, and show
that our method consistently achieves lower approximation error under matched
parameter budgets. Experiments on both synthetic and practical settings
demonstrate that Localized LoRA offers a more expressive and adaptable
alternative to existing methods, enabling efficient fine-tuning with improved
performance.

</details>


### [304] [Beyond Semantic Entropy: Boosting LLM Uncertainty Quantification with Pairwise Semantic Similarity](https://arxiv.org/abs/2506.00245)
*Dang Nguyen,Ali Payani,Baharan Mirzasoleiman*

Main category: cs.LG

TL;DR: 提出SNNE方法改进大语言模型幻觉检测，通过解决语义熵在长文本中忽略语义簇相似性的局限


<details>
  <summary>Details</summary>
Motivation: 现有语义熵方法在长文本场景效果下降，因其未考虑语义簇内相似性（簇内分布）和语义簇间相似性（簇间距离）两个关键因素

Method: 基于最近邻熵估计的黑盒不确定性量化方法，可结合token概率扩展至白盒场景

Result: 在Phi3/Llama3模型及问答/摘要/机器翻译任务中验证有效性，性能优于语义熵基准

Conclusion: 该方法从理论上扩展了语义熵，实验证明能有效提升幻觉检测，代码已开源供复现

Abstract: Hallucination in large language models (LLMs) can be detected by assessing
the uncertainty of model outputs, typically measured using entropy. Semantic
entropy (SE) enhances traditional entropy estimation by quantifying uncertainty
at the semantic cluster level. However, as modern LLMs generate longer
one-sentence responses, SE becomes less effective because it overlooks two
crucial factors: intra-cluster similarity (the spread within a cluster) and
inter-cluster similarity (the distance between clusters). To address these
limitations, we propose a simple black-box uncertainty quantification method
inspired by nearest neighbor estimates of entropy. Our approach can also be
easily extended to white-box settings by incorporating token probabilities.
Additionally, we provide theoretical results showing that our method
generalizes semantic entropy. Extensive empirical results demonstrate its
effectiveness compared to semantic entropy across two recent LLMs (Phi3 and
Llama3) and three common text generation tasks: question answering, text
summarization, and machine translation. Our code is available at
https://github.com/BigML-CS-UCLA/SNNE.

</details>


### [305] [Spectral Insights into Data-Oblivious Critical Layers in Large Language Models](https://arxiv.org/abs/2506.00382)
*Xuyuan Liu,Lei Hsiung,Yaoqing Yang,Yujun Yan*

Main category: cs.LG

TL;DR: 提出基于CKA的无数据依赖方法，识别预训练LLMs中的关键层，揭示其表示空间突变与微调敏感度的关联，并验证了在领域适应和后门防御中的应用价值。


<details>
  <summary>Details</summary>
Motivation: 现有研究依赖数据驱动的微调后模型分析，难以直接用于预训练模型。本文旨在通过分析预训练模型内在表示动态，定位与功能强相关的关键层，提升模型可解释性和应用效率。

Method: 1. 使用CKA度量层间表示相似性，定位表示突变的临界层 2. 谱分析主成分语义 3. 通过微调敏感度实验验证关键层功能 4. 在领域适应（选择性微调）和后门防御（冻结关键层）场景验证实用性

Result: 1. 关键层微调参数变化量是非关键层的3-5倍 2. 主成分变化对应「理由→结论」的语义迁移 3. 仅微调关键层可使领域适应loss下降37% 4. 冻结关键层使后门攻击成功率下降40%

Conclusion: 预训练模型存在与任务无关的固有关键层，其表示突变反映语义结构转型。针对性地操作这些层可提升模型调整效率与安全性，为模型分析提供新维度。

Abstract: Understanding how feature representations evolve across layers in large
language models (LLMs) is key to improving their interpretability and
robustness. While recent studies have identified critical layers linked to
specific functions or behaviors, these efforts typically rely on data-dependent
analyses of fine-tuned models, limiting their use to post-hoc settings. In
contrast, we introduce a data-oblivious approach to identify intrinsic critical
layers in pre-fine-tuned LLMs by analyzing representation dynamics via Centered
Kernel Alignment(CKA). We show that layers with significant shifts in
representation space are also those most affected during fine-tuning--a pattern
that holds consistently across tasks for a given model. Our spectral analysis
further reveals that these shifts are driven by changes in the top principal
components, which encode semantic transitions from rationales to conclusions.
We further apply these findings to two practical scenarios: efficient domain
adaptation, where fine-tuning critical layers leads to greater loss reduction
compared to non-critical layers; and backdoor defense, where freezing them
reduces attack success rates by up to 40%.

</details>


### [306] [BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM Evaluation](https://arxiv.org/abs/2506.00482)
*Eunsu Kim,Haneul Yoo,Guijin Son,Hitesh Patel,Amit Agarwal,Alice Oh*

Main category: cs.LG

TL;DR: 提出动态基准存储库BenchHub，整合38个基准的303K问题，通过持续更新和可扩展管理支持领域定制化评估，实验表明领域感知基准对LLM评估至关重要


<details>
  <summary>Details</summary>
Motivation: 现有基准数据集分散且难以管理，领域专用模型(如数学/代码领域)评估需求迫切但缺乏针对性评测工具

Method: 开发支持自动分类/持续更新的基准聚合平台，集成38个基准的303K问题，通过多领域LLM实验验证领域性能差异

Result: 不同LLM在领域子集表现差异显著(如CodeLlama-34B在代码基准准确率超GPT-4)，证实领域定制化评估的必要性

Conclusion: BenchHub通过促进数据集复用、透明模型比较和识别评估盲区，成为LLM评估研究的关键基础设施

Abstract: As large language models (LLMs) continue to advance, the need for up-to-date
and well-organized benchmarks becomes increasingly critical. However, many
existing datasets are scattered, difficult to manage, and make it challenging
to perform evaluations tailored to specific needs or domains, despite the
growing importance of domain-specific models in areas such as math or code. In
this paper, we introduce BenchHub, a dynamic benchmark repository that empowers
researchers and developers to evaluate LLMs more effectively. BenchHub
aggregates and automatically classifies benchmark datasets from diverse
domains, integrating 303K questions across 38 benchmarks. It is designed to
support continuous updates and scalable data management, enabling flexible and
customizable evaluation tailored to various domains or use cases. Through
extensive experiments with various LLM families, we demonstrate that model
performance varies significantly across domain-specific subsets, emphasizing
the importance of domain-aware benchmarking. We believe BenchHub can encourage
better dataset reuse, more transparent model comparisons, and easier
identification of underrepresented areas in existing benchmarks, offering a
critical infrastructure for advancing LLM evaluation research.

</details>


### [307] [FLoE: Fisher-Based Layer Selection for Efficient Sparse Adaptation of Low-Rank Experts](https://arxiv.org/abs/2506.00495)
*Xinyi Wang,Lirong Gao,Haobo Wang,Yiming Zhang,Junbo Zhao*

Main category: cs.LG

TL;DR: 提出FLoE框架，通过动态识别关键层和贝叶斯优化分配LoRA秩，显著提升参数高效微调方法的计算效率与模型精度平衡。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法在所有层统一部署LoRA适配器，忽略层间异质性和任务特定需求，导致参数冗余和效率低下。

Method: 1. 基于Fisher信息的动态重要性评分机制识别关键层，实现MoE稀疏适配器部署
2. 贝叶斯优化驱动的秩分配器自动确定数据集最优LoRA秩

Result: 在多LLM和基准测试中实现效率-精度最优权衡，在资源受限场景展现显著优势

Conclusion: FLoE为资源敏感场景提供了高性价比的快速适配方案，推动PEFT技术实用化进程

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods have emerged as a widely
adopted strategy for adapting pre-trained Large Language Models (LLMs) to
downstream tasks, significantly reducing memory and computational costs.
However, most existing PEFT techniques uniformly deploy LoRA adapters across
all layers, disregarding the intrinsic heterogeneity of layer contributions and
task-specific rank requirements. This uniform paradigm leads to redundant
parameter allocation and suboptimal adaptation efficiency. To address these
limitations, we propose FLoE, a novel PEFT framework that introduces two key
innovations: (i) a Fisher information-guided importance scoring mechanism to
dynamically identify task-critical transformer layers for MoE-based low-rank
adaptation, enabling sparse adapter deployment; and (ii) a Bayesian
optimization-driven rank allocator that automatically determines optimal LoRA
ranks on specific datasets without exhaustive grid search. Extensive
experiments across diverse LLMs and benchmarks reveal that FLoE achieves
impressive efficiency-accuracy trade-offs, making FLoE particularly
advantageous in resource-constrained environments that necessitate rapid
adaptation.

</details>


### [308] [MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning](https://arxiv.org/abs/2506.00555)
*Peng Xia,Jinglu Wang,Yibo Peng,Kaide Zeng,Xian Wu,Xiangru Tang,Hongtu Zhu,Yun Li,Shujie Liu,Yan Lu,Huaxiu Yao*

Main category: cs.LG

TL;DR: 提出基于强化学习的多智能体框架MMedAgent-RL，通过动态协作提升医学视觉语言模型的诊断性能


<details>
  <summary>Details</summary>
Motivation: 现有单智能体模型跨医学专业泛化能力不足，传统多智能体协作框架存在流程固化、推理灵活性差的问题

Method: 1. 训练两个基于Qwen2.5-VL的GP智能体：分诊医生学习患者专科分配，主治医生整合多专科诊断
2. 引入课程学习引导的强化学习策略，平衡专科模仿与纠错

Result: 在5个医学VQA基准测试中平均性能提升18.4%，超越现有开源和商业模型

Conclusion: MMedAgent-RL不仅实现性能突破，还展现出类人的动态推理模式，为医疗AI协作系统提供新范式

Abstract: Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential
in multimodal diagnostic tasks. However, existing single-agent models struggle
to generalize across diverse medical specialties, limiting their performance.
Recent efforts introduce multi-agent collaboration frameworks inspired by
clinical workflows, where general practitioners (GPs) and specialists interact
in a fixed sequence. Despite improvements, these static pipelines lack
flexibility and adaptability in reasoning. To address this, we propose
MMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that
enables dynamic, optimized collaboration among medical agents. Specifically, we
train two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to
assign patients to appropriate specialties, while the attending physician
integrates the judgments from multi-specialists and its own knowledge to make
final decisions. To address the inconsistency in specialist outputs, we
introduce a curriculum learning (CL)-guided RL strategy that progressively
teaches the attending physician to balance between imitating specialists and
correcting their mistakes. Experiments on five medical VQA benchmarks
demonstrate that MMedAgent-RL not only outperforms both open-source and
proprietary Med-LVLMs, but also exhibits human-like reasoning patterns.
Notably, it achieves an average performance gain of 18.4% over supervised
fine-tuning baselines.

</details>


### [309] [Linear Representation Transferability Hypothesis: Leveraging Small Models to Steer Large Models](https://arxiv.org/abs/2506.00653)
*Femi Bello,Anubrata Das,Fanzhi Zeng,Fangcong Yin,Leqi Liu*

Main category: cs.LG

TL;DR: 提出线性表示可转移性(LRT)假设，验证不同规模语言模型间存在仿射变换，证明小模型表示可指导大模型行为


<details>
  <summary>Details</summary>
Motivation: 探索不同规模模型在相同数据训练下是否共享通用基础特征，解决跨模型表示对齐问题

Method: 通过学习不同规模模型隐藏状态间的仿射映射，评估行为引导向量在模型间的语义保持效果

Result: 仿射变换能有效保留引导向量的语义效果，支持LRT假设的成立

Conclusion: LRT假设为跨模型规模的表示对齐研究提供新方向，小模型表示可有效引导大模型行为

Abstract: It has been hypothesized that neural networks with similar architectures
trained on similar data learn shared representations relevant to the learning
task. We build on this idea by extending the conceptual framework where
representations learned across models trained on the same data can be expressed
as linear combinations of a \emph{universal} set of basis features. These basis
features underlie the learning task itself and remain consistent across models,
regardless of scale. From this framework, we propose the \textbf{Linear
Representation Transferability (LRT)} Hypothesis -- that there exists an affine
transformation between the representation spaces of different models. To test
this hypothesis, we learn affine mappings between the hidden states of models
of different sizes and evaluate whether steering vectors -- directions in
hidden state space associated with specific model behaviors -- retain their
semantic effect when transferred from small to large language models using the
learned mappings. We find strong empirical evidence that such affine mappings
can preserve steering behaviors. These findings suggest that representations
learned by small models can be used to guide the behavior of large models, and
that the LRT hypothesis may be a promising direction on understanding
representation alignment across model scales.

</details>


### [310] [Existing Large Language Model Unlearning Evaluations Are Inconclusive](https://arxiv.org/abs/2506.00688)
*Zhili Feng,Yixuan Even Xu,Alexander Robey,Robert Kirk,Xander Davies,Yarin Gal,Avi Schwarzschild,J. Zico Kolter*

Main category: cs.LG

TL;DR: 论文揭示当前机器学习反学习评估方法的三大缺陷，提出最小信息注入与下游任务感知的新评估原则


<details>
  <summary>Details</summary>
Motivation: 现有反学习评估存在测试阶段信息污染、跨任务稳定性差和虚假相关性三大问题，导致评估结果可信度存疑

Method: 通过系统性实验验证现有评估流程的漏洞，设计控制变量实验对比新旧评估方法的效果差异

Result: 证明当前评估可能同时高估（因测试阶段再训练）和低估（因任务敏感性）反学习效果，新原则可提升评估稳定性

Conclusion: 需重构反学习评估范式，建立更科学的测试框架以避免结论偏差，这对AI安全与模型可解释性研究具有重要意义

Abstract: Machine unlearning aims to remove sensitive or undesired data from large
language models. However, recent studies suggest that unlearning is often
shallow, claiming that removed knowledge can easily be recovered. In this work,
we critically examine standard unlearning evaluation practices and uncover key
limitations that shake our trust in those findings. First, we show that some
evaluations introduce substantial new information into the model, potentially
masking true unlearning performance by re-teaching the model during testing.
Second, we demonstrate that evaluation outcomes vary significantly across
tasks, undermining the generalizability of current evaluation routines.
Finally, we find that many evaluations rely on spurious correlations, making
their results difficult to trust and interpret. Taken together, these issues
suggest that current evaluation protocols may both overstate and understate
unlearning success. To address this, we propose two principles for future
unlearning evaluations: minimal information injection and downstream task
awareness. We validate these principles through a series of targeted
experiments, showing how violations of each can lead to misleading conclusions.

</details>


### [311] [Bregman Conditional Random Fields: Sequence Labeling with Parallelizable Inference Algorithms](https://arxiv.org/abs/2506.00732)
*Caio Corro,Mathieu Lacroix,Joseph Le Roux*

Main category: cs.LG

TL;DR: 提出基于Bregman投影的并行化推理条件随机场BCRF，在保持精度的同时显著提升推理速度


<details>
  <summary>Details</summary>
Motivation: 传统线性链条件随机场(CRF)推理速度较慢，需要开发支持并行化计算的高效替代方案

Method: 通过Bregman投影实现并行推理，使用Fenchel-Youth损失函数进行训练，支持部分标签学习

Result: 推理速度超越CRF，在受限场景下准确率优于均值场等并行化方法

Conclusion: BCRF在保持序列标注精度的同时实现了高效并行计算，为实时应用提供了新选择

Abstract: We propose a novel discriminative model for sequence labeling called Bregman
conditional random fields (BCRF). Contrary to standard linear-chain conditional
random fields, BCRF allows fast parallelizable inference algorithms based on
iterative Bregman projections. We show how such models can be learned using
Fenchel-Young losses, including extension for learning from partial labels.
Experimentally, our approach delivers comparable results to CRF while being
faster, and achieves better results in highly constrained settings compared to
mean field, another parallelizable alternative.

</details>


### [312] [LIFT the Veil for the Truth: Principal Weights Emerge after Rank Reduction for Reasoning-Focused Supervised Fine-Tuning](https://arxiv.org/abs/2506.00772)
*Zihang Liu,Tianyu Pang,Oleg Balabanov,Chaoqun Yang,Tianjin Huang,Lu Yin,Yaoqing Yang,Shiwei Liu*

Main category: cs.LG

TL;DR: 提出LIFT方法通过低秩近似识别关键主权重，仅更新5%参数即超越全参数微调性能，同时保持高效计算


<details>
  <summary>Details</summary>
Motivation: 全参数微调存在计算成本高、易过拟合问题，传统稀疏微调因难以识别关键参数在LLM时代失效

Method: 利用低秩近似识别权重矩阵中幅度最大的主权重，仅持续更新前5%关键参数（LIFT方法）

Result: 在推理任务中持续超越全参数微调，内存效率与主流参数高效微调方法相当，且多保留20%源领域知识

Conclusion: LIFT为LLM高效微调提供了新范式，在性能、效率和知识保留间实现更好平衡，推动参数高效微调发展

Abstract: Recent studies have shown that supervised fine-tuning of LLMs on a small
number of high-quality datasets can yield strong reasoning capabilities.
However, full fine-tuning (Full FT), while powerful, is computationally
expensive and susceptible to overfitting and catastrophic forgetting,
particularly when data is limited. Sparse fine-tuning, which previously
achieved notable success by updating only a small subset of model parameters,
offers a promising trade-off between efficiency and effectiveness. Yet, it has
lagged behind in the LLM era due to the difficulty of identifying parameters
truly critical for reasoning. In this work, we state that weights with the
largest magnitude after low-rank approximation are critical weights for
fine-tuning, which we call Principal Weights. Surprisingly, while
magnitude-based sparse fine-tuning performs poorly as a baseline on LLM
fine-tuning, it becomes highly effective after rank reduction. These insights
motivate our method: Low-rank Informed Sparse Fine-Tuning (LIFT). LIFT only
updates the top 5% Principal Weights throughout training and consistently
achieves better performance on reasoning tasks than Full FT, while maintaining
memory efficiency on par with popular parameter-efficient fine-tuning methods.
In addition to strong performance on target domains such as arithmetic
reasoning, LIFT also retains up to 20% more source-domain knowledge, compared
to Full FT and LoRA. Our code is available at:
https://github.com/zihanghliu/LIFT.

</details>


### [313] [Generalizable LLM Learning of Graph Synthetic Data with Reinforcement Learning](https://arxiv.org/abs/2506.00845)
*Yizhuo Zhang,Heng Wang,Shangbin Feng,Zhaoxuan Tan,Xinyun Liu,Yulia Tsvetkov*

Main category: cs.LG

TL;DR: 通过强化学习替代监督微调，提升大语言模型在图推理任务中的泛化能力，解决合成数据过拟合问题


<details>
  <summary>Details</summary>
Motivation: 现有基于监督微调的图推理方法容易过拟合合成数据，难以泛化到真实场景的隐含图结构任务（如多跳QA、结构化规划）

Method: 设计基于解决方案和过程的双重奖励机制，采用GRPO/DPO强化学习算法对齐现成模型和合成数据微调模型

Result: 在5个数据集实现12.9%平均提升，过程奖励优于结果奖励，混合合成/真实数据训练展现潜力

Conclusion: 强化学习有效提升泛化性，但组合推理能力和可解释中间步骤仍是核心挑战

Abstract: Previous research has sought to enhance the graph reasoning capabilities of
LLMs by supervised fine-tuning on synthetic graph data. While these led to
specialized LLMs better at solving graph algorithm problems, we don't need LLMs
for shortest path: we need generalization from synthetic graph data to
real-world tasks with implicit graph structures. In this work, we propose to
unlock generalizable learning of graph synthetic data with reinforcement
learning. We first design solution-based and process-based rewards for
synthetic graph problems: instead of rigid memorizing response patterns in
direct fine-tuning, we posit that RL would help LLMs grasp the essentials
underlying graph reasoning and alleviate overfitting. We employ RL algorithms
such as GRPO and DPO, aligning both off-the-shelf LLMs and LLMs fine-tuned on
synthetic graph data. We then compare them against existing settings on both
in-domain synthetic tasks and out-of-domain real-world tasks with implicit
graph structures such as multi-hop QA, structured planning, and more. Extensive
experiments demonstrate that our RL recipe leads to statistically significant
improvement on 5 datasets, with an average gain of 12.9\% over baseline
settings. Further analysis reveals that process-based rewards consistently
outperform solution-based rewards, mixing synthetic and real-world task data
yields potential gains, while compositionality and explainable intermediate
steps remains a critical challenge even after RL.

</details>


### [314] [Position as Probability: Self-Supervised Transformers that Think Past Their Training for Length Extrapolation](https://arxiv.org/abs/2506.00920)
*Philip Heejun Lee*

Main category: cs.LG

TL;DR: 提出PRISM概率位置编码机制，通过概率叠加保持位置不确定性，使Transformer模型在10倍训练长度外推中达到最优表现


<details>
  <summary>Details</summary>
Motivation: 传统序列模型在测试序列远超训练长度时精度显著下降，而算法推理、多步算术等任务亟需鲁棒的长度外推能力

Method: 采用可微分直方图滤波更新学习连续相对位置，利用概率叠加替代确定性位置嵌入保持位置不确定性

Result: 在算术运算（加法/乘法）、SCAN组合任务等基准测试中实现最先进外推性能，成功扩展到DeepMind数据集的长序列场景

Conclusion: PRISM的随机位置编码保持清晰可解释的内部状态，为可靠的长度外推提供理论依据，推动神经序列模型在超长场景下的算法鲁棒性

Abstract: Deep sequence models typically degrade in accuracy when test sequences
significantly exceed their training lengths, yet many critical tasks--such as
algorithmic reasoning, multi-step arithmetic, and compositional
generalization--require robust length extrapolation. We introduce PRISM, a
Probabilistic Relative-position Implicit Superposition Model, a novel
positional encoding mechanism that enables Transformers to extrapolate
accurately up to 10x beyond their training length. PRISM learns continuous
relative positions through a differentiable histogram-filter update, preserving
position uncertainty via a probabilistic superposition rather than conventional
deterministic embeddings. Empirically, PRISM achieves state-of-the-art length
extrapolation, successfully generalizing to previously intractable sequence
lengths across algorithmic benchmarks--including arithmetic (addition,
multiplication), SCAN compositionality tasks, and complex copy variants derived
from DeepMind's recent datasets. Our analysis demonstrates that PRISM's
stochastic positional encoding maintains sharp and interpretable internal
states, providing a theoretical basis for reliable length generalization. These
results advance the goal of neural sequence models that remain algorithmically
robust at lengths far exceeding their training horizon.

</details>


### [315] [Attention Retrieves, MLP Memorizes: Disentangling Trainable Components in the Transformer](https://arxiv.org/abs/2506.01115)
*Yihe Dong,Lorenzo Noci,Mikhail Khodak,Mufan Li*

Main category: cs.LG

TL;DR: Transformer架构中自注意力机制并非所有任务的关键因素，MixiT模型通过固定随机注意力系数在算术/记忆任务表现与标准Transformer相当，但在检索任务中因无法形成induction heads而表现欠佳。


<details>
  <summary>Details</summary>
Motivation: 探究Transformer架构中不同组件(自注意力机制 vs MLP层)对算法任务性能的贡献差异，挑战传统认知中自注意力的核心地位。

Method: 1. 冻结MLP层或注意力投影器进行对比实验 2. 提出MixiT模型(完全固定随机注意力系数) 3. 在算法任务和语言建模任务中进行性能测试

Result: 1. MixiT在算术/记忆任务与标准Transformer相当 2. 检索任务中因缺乏induction heads电路表现下降 3. 仅冻结key/query投影器仍能形成induction heads并保持语言建模竞争力

Conclusion: 架构异构性至关重要：不同组件通过互补的归纳偏置共同支持多样化任务，自注意力并非所有能力实现的必要条件。

Abstract: The Transformer architecture is central to the success of modern Large
Language Models (LLMs), in part due to its surprising ability to perform a wide
range of algorithmic tasks -- including mathematical reasoning, memorization,
and retrieval -- using only gradient-based training on next-token prediction.
While the core component of a Transformer is the self-attention mechanism, we
question how much, and which aspects, of the performance gains can be
attributed to it. To this end, we compare standard Transformers to variants in
which either the multi-layer perceptron (MLP) layers or the attention
projectors (queries and keys) are frozen at initialization. To further isolate
the contribution of attention, we introduce MixiT -- the Mixing Transformer --
a simplified, principled model in which the attention coefficients are entirely
random and fixed at initialization, eliminating any input-dependent computation
or learning in attention. Surprisingly, we find that MixiT matches the
performance of fully trained Transformers on various algorithmic tasks,
especially those involving basic arithmetic or focusing heavily on
memorization. For retrieval-based tasks, we observe that having input-dependent
attention coefficients is consistently beneficial, while MixiT underperforms.
We attribute this failure to its inability to form specialized circuits such as
induction heads -- a specific circuit known to be crucial for learning and
exploiting repeating patterns in input sequences. Even more interestingly, we
find that attention with frozen key and query projectors is not only able to
form induction heads, but can also perform competitively on language modeling.
Our results underscore the importance of architectural heterogeneity, where
distinct components contribute complementary inductive biases crucial for
solving different classes of tasks.

</details>


### [316] [Earley-Driven Dynamic Pruning for Efficient Structured Decoding](https://arxiv.org/abs/2506.01151)
*Xintong Sun,Chi Wei,Minghao Tian,Shiwen Ni*

Main category: cs.LG

TL;DR: 提出基于Earley算法的动态剪枝策略ZapFormat，显著降低内存占用并实现2倍推理加速，适用于多种LLM架构的结构化生成任务。


<details>
  <summary>Details</summary>
Motivation: 现有约束解码方法需在每一步检查所有token有效性，导致高计算开销，影响大规模查询效率。

Method: 利用ZapFormat动态剪枝Earley算法冗余状态，结合状态缓存机制，构建整合多种优化的Formatron解码引擎。

Result: Formatron在JSON生成等任务中保持高精度，推理速度提升最高2倍，且跨架构通用性强。

Conclusion: 通过动态剪枝与状态缓存技术，ZapFormat有效解决约束解码效率瓶颈，开源Formatron推动实际应用落地。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities, yet ensuring
their outputs conform to strict structural or grammatical constraints remains
challenging, which is critical in function calls and domain-specific language
(DSL) generation. Constrained decoding with context-free grammar is a flexible
approach to guarantee LLMs' adherence to a specific format by dynamically
building a token logits mask. However, creating this mask requires checking the
validity of all tokens in the LLM vocabulary at every decoding step, which
often incurs significant overheads in existing constrained decoding engines. To
address this challenge, we propose $\textbf{ZapFormat}$, a novel
$\textbf{dynamic pruning}$ strategy based on the Earley algorithm that
identifies and eliminates invalid or redundant Earley states in real-time,
significantly reducing memory occupation of the Earley algorithm's states. This
further enables us to use a state cache to speed up structured generations on a
large number of queries. We implemented ZapFormat in a new constrained decoding
engine called Formatron which also incorporates existing optimizations. Through
comprehensive experiments on structured generation tasks, including JSON
generation, JSON Schema validation, and semantic parsing, we demonstrate that
Formatron not only $\textbf{consistently maintains}$ high-precision compliant
outputs but also achieves $\textbf{significant improvements}$ in inference
speed up to 2x compared to state-of-the-art implementations. More importantly,
Formatron is generally applicable across various LLM architectures. We release
Formatron as open source at https://github.com/Dan-wanna-M/formatron.

</details>


### [317] [MUDI: A Multimodal Biomedical Dataset for Understanding Pharmacodynamic Drug-Drug Interactions](https://arxiv.org/abs/2506.01478)
*Tung-Lam Ngo,Ba-Hoang Tran,Duy-Cat Can,Trung-Hieu Do,Oliver Y. Chén,Hoang-Quynh Le*

Main category: cs.LG

TL;DR: 提出了多模态药物相互作用数据集MUDI，包含31万对药物组合的多模态数据（文本/分子式/结构图/图像），采用未见药物对测试模型泛化能力并开源数据资源。


<details>
  <summary>Details</summary>
Motivation: 现有DDI数据集主要依赖文本信息，难以反映药物复杂作用机制。需通过多模态数据整合提升药物互作研究的全面性。

Method: 整合药理学文本、化学式、分子结构图和图像构建数据集，标注31万药物对的协同/拮抗/新效关系，测试集采用未见药物对，使用晚期融合投票和中期融合策略进行基准测试。

Result: 建立首个大规模多模态DDI数据集，开发对应评估框架，所有数据及基准模型通过开放研究许可发布。

Conclusion: MUDI通过多模态表征和未见测试集设计，为机器学习模型在药物相互作用领域的可靠评估提供新标准，推动药效动力学研究发展。

Abstract: Understanding the interaction between different drugs (drug-drug interaction
or DDI) is critical for ensuring patient safety and optimizing therapeutic
outcomes. Existing DDI datasets primarily focus on textual information,
overlooking multimodal data that reflect complex drug mechanisms. In this
paper, we (1) introduce MUDI, a large-scale Multimodal biomedical dataset for
Understanding pharmacodynamic Drug-drug Interactions, and (2) benchmark
learning methods to study it. In brief, MUDI provides a comprehensive
multimodal representation of drugs by combining pharmacological text, chemical
formulas, molecular structure graphs, and images across 310,532 annotated drug
pairs labeled as Synergism, Antagonism, or New Effect. Crucially, to
effectively evaluate machine-learning based generalization, MUDI consists of
unseen drug pairs in the test set. We evaluate benchmark models using both late
fusion voting and intermediate fusion strategies. All data, annotations,
evaluation scripts, and baselines are released under an open research license.

</details>


### [318] [Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and Accountability](https://arxiv.org/abs/2506.01789)
*Genta Indra Winata,David Anugraha,Emmy Liu,Alham Fikri Aji,Shou-Yi Hung,Aditya Parashar,Patrick Amadeus Irawan,Ruochen Zhang,Zheng-Xin Yong,Jan Christian Blaise Cruz,Niklas Muennighoff,Seungone Kim,Hanyang Zhao,Sudipta Kar,Kezia Erina Suryoraharjo,M. Farid Adilazuarda,En-Shiun Annie Lee,Ayu Purwarianti,Derry Tanti Wijaya,Monojit Choudhury*

Main category: cs.LG

TL;DR: 提出DataRubrics框架，通过基于量规的评估指标和LLM技术实现可扩展的数据集质量评估


<details>
  <summary>Details</summary>
Motivation: 当前数据集创建存在质量评估不足（缺乏原创性/多样性/质量控制）、现有评估工具标准化程度低、会议元数据要求执行不统一等问题

Method: 开发结构化评估框架DataRubrics，结合量规评估指标与LLM-as-a-judge技术，支持人类标注和模型生成数据集的质量评估

Result: 实现可重复、可扩展的数据质量评估流程，提供开源代码支持评估过程复现

Conclusion: DataRubrics框架通过标准化评估流程提升了数据驱动研究的质量标准，配套工具支持研究社区实践改进（代码已开源）

Abstract: High-quality datasets are fundamental to training and evaluating machine
learning models, yet their creation-especially with accurate human
annotations-remains a significant challenge. Many dataset paper submissions
lack originality, diversity, or rigorous quality control, and these
shortcomings are often overlooked during peer review. Submissions also
frequently omit essential details about dataset construction and properties.
While existing tools such as datasheets aim to promote transparency, they are
largely descriptive and do not provide standardized, measurable methods for
evaluating data quality. Similarly, metadata requirements at conferences
promote accountability but are inconsistently enforced. To address these
limitations, this position paper advocates for the integration of systematic,
rubric-based evaluation metrics into the dataset review process-particularly as
submission volumes continue to grow. We also explore scalable, cost-effective
methods for synthetic data generation, including dedicated tools and
LLM-as-a-judge approaches, to support more efficient evaluation. As a call to
action, we introduce DataRubrics, a structured framework for assessing the
quality of both human- and model-generated datasets. Leveraging recent advances
in LLM-based evaluation, DataRubrics offers a reproducible, scalable, and
actionable solution for dataset quality assessment, enabling both authors and
reviewers to uphold higher standards in data-centric research. We also release
code to support reproducibility of LLM-based evaluations at
https://github.com/datarubrics/datarubrics.

</details>


### [319] [Unified Scaling Laws for Compressed Representations](https://arxiv.org/abs/2506.01863)
*Andrei Panferov,Alexandra Volkova,Ionut-Vlad Modoranu,Vage Egiazarian,Mher Safaryan,Dan Alistarh*

Main category: cs.LG

TL;DR: 论文探索了压缩格式（稀疏化/量化）与扩展定律的关系，提出基于数据拟合能力的统一参数效率指标，验证了跨压缩格式的扩展定律有效性。


<details>
  <summary>Details</summary>
Motivation: 针对大规模AI训练的高计算成本，研究压缩技术（量化/稀疏化）与扩展定律的协同作用，建立可预测不同压缩格式下模型性能的统一框架。

Method: 通过理论推导和实证验证，提出基于随机高斯数据拟合能力的'capacity'指标，测试其在稀疏化、标量量化、矢量量化等压缩格式下的参数效率预测能力。

Result: 验证了扩展定律在单一及组合压缩场景的适用性，证明capacity指标能跨格式预测参数效率，并开发了稀疏-量化联合训练改进算法。

Conclusion: 统一扩展框架和capacity指标有效预测压缩格式下的模型性能，为算法优化和格式选择提供理论依据与实践指导。

Abstract: Scaling laws have shaped recent advances in machine learning by enabling
predictable scaling of model performance based on model size, computation, and
data volume. Concurrently, the rise in computational cost for AI has motivated
model compression techniques, notably quantization and sparsification, which
have emerged to mitigate the steep computational demands associated with
large-scale training and inference. This paper investigates the interplay
between scaling laws and compression formats, exploring whether a unified
scaling framework can accurately predict model performance when training occurs
over various compressed representations, such as sparse, scalar-quantized,
sparse-quantized or even vector-quantized formats. Our key contributions
include validating a general scaling law formulation and showing that it is
applicable both individually but also composably across compression types.
Based on this, our main finding is demonstrating both theoretically and
empirically that there exists a simple "capacity" metric -- based on the
representation's ability to fit random Gaussian data -- which can robustly
predict parameter efficiency across multiple compressed representations. On the
practical side, we extend our formulation to directly compare the accuracy
potential of different compressed formats, and to derive better algorithms for
training over sparse-quantized formats.

</details>
