<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 73]
- [cs.SD](#cs.SD) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.CV](#cs.CV) [Total: 4]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.AI](#cs.AI) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Unsupervised Cycle Detection in Agentic Applications](https://arxiv.org/abs/2511.10650)
*Felix George,Harshit Kumar,Divya Pathak,Kaustabha Ray,Mudit Verma,Pratibha Moogi*

Main category: cs.CL

TL;DR: 提出结合结构分析和语义相似性的混合框架，有效检测LLM应用中的隐藏执行周期，F1分数（0.72）显著优于单一方法


<details>
  <summary>Details</summary>
Motivation: 传统监控平台无法检测LLM代理应用中非确定性行为形成的隐式资源消耗周期，导致资源浪费

Method: 1. 时间调用堆栈分析显式循环 2. 语义相似性分析冗余内容生成的隐式周期

Result: 在1575条应用轨迹测试中，混合方法F1达0.72（结构法0.08/语义法0.28）

Conclusion: 方法初步有效但仍有改进空间，需进一步优化框架并解决当前局限

Abstract: Agentic applications powered by Large Language Models exhibit non-deterministic behaviors that can form hidden execution cycles, silently consuming resources without triggering explicit errors. Traditional observability platforms fail to detect these costly inefficiencies. We present an unsupervised cycle detection framework that combines structural and semantic analysis. Our approach first applies computationally efficient temporal call stack analysis to identify explicit loops and then leverages semantic similarity analysis to uncover subtle cycles characterized by redundant content generation. Evaluated on 1575 trajectories from a LangGraph-based stock market application, our hybrid approach achieves an F1 score of 0.72 (precision: 0.62, recall: 0.86), significantly outperforming individual structural (F1: 0.08) and semantic methods (F1: 0.28). While these results are encouraging, there remains substantial scope for improvement, and future work is needed to refine the approach and address its current limitations.

</details>


### [2] [Data Analysis and Performance Evaluation of Simulation Deduction Based on LLMs](https://arxiv.org/abs/2511.10651)
*Shansi Zhang,Min Li*

Main category: cs.CL

TL;DR: 提出基于大语言模型的模拟推演数据分析框架，通过任务分解、多轮交互和自定义工具生成高质量结构化报告


<details>
  <summary>Details</summary>
Motivation: 传统人工分析效率低且易出错，大语言模型单次指令难以生成结构化报告，需系统性方法提升分析质量

Method: 1. 任务分解与提示工程设计 2. 多轮自检式LLM交互实现结构化数据提取 3. 自定义可视化工具与指标计算 4. 多场景自适应报告模板设计

Result: 评估显示本方法生成报告质量显著优于基线，在结构化程度、分析深度等维度获得更高评分

Conclusion: 该方法有效提升军事推演分析效率，验证了LLM在复杂战术分析任务中的工程化应用潜力

Abstract: Data analysis and performance evaluation of simulation deduction plays a pivotal role in modern warfare, which enables military personnel to gain invaluable insights into the potential effectiveness of different strategies, tactics, and operational plans. Traditional manual analysis approach is time-consuming and limited by human errors. To enhance efficiency and accuracy, large language models (LLMs) with strong analytical and inferencing capabilities can be employed. However, high-quality analysis reports with well-structured formatting cannot be obtained through a single instruction input to the LLM. To tackle this issue, we propose a method that first decomposes the complex task into several sub-tasks and designs effective system prompts and user prompts for each sub-task. Multi-round interactions with the LLM incorporating self-check and reflection are then conducted to enable structured data extraction as well as multi-step analysis and evaluation. Furthermore, custom tools are defined and invoked to generate figures and compute metrics. We also design multiple report templates, each tailored to a specific application and input data type, ensuring their adaptability across a variety of scenarios. Extensive evaluation results demonstrate that the reports generated by our method exhibit higher quality, therefore obtaining higher scores than the baseline method.

</details>


### [3] [Cognitively-Inspired Episodic Memory Architectures for Accurate and Efficient Character AI](https://arxiv.org/abs/2511.10652)
*Rafael Arias Gonzalez,Steve DiPaola*

Main category: cs.CL

TL;DR: 提出结合离线数据增强与结构化情景记忆的对话系统架构，在保持响应深度的同时实现高效推理，特别适用于资源受限场景。


<details>
  <summary>Details</summary>
Motivation: 解决现有检索增强生成方法响应肤浅与多阶段反思方法延迟过高之间的矛盾，平衡对话深度与系统效率。

Method: 将传记数据转化为1,774个带有情感语义元数据的记忆单元，通过两阶段检索实现0.52秒的提示生成，并开发时空热力图等可视化分析工具。

Result: 在GPT-4上与传统RAG持平，在GPT-3.5/3上显著超越，结构化记忆支持情感轨迹分析等新型研究工具。

Conclusion: 该架构兼具对话界面与传记分析研究功能，为教育、博物馆等需要高精度与高效率的场景提供通用解决方案。

Abstract: Large language models show promise for embodying historical characters in dialogue systems, but existing approaches face a critical trade-off: simple retrieval-augmented generation produces shallow responses, while multi-stage reflection achieves depth at prohibitive latency. We present an architecture that resolves this tension through offline data augmentation and efficient parallel retrieval from structured episodic memory. Our system transforms biographical data into 1,774 enriched first-person memories with affective-semantic metadata, then employs two-stage retrieval achieving 0.52s prompt generation. Evaluation using LLM-as-judge and RAGAs metrics shows our approach achieves parity with traditional RAG on GPT-4 while significantly outperforming it on smaller models (GPT-3.5, GPT-3), suggesting particular value for resource-constrained deployments. Beyond dialogue, the structured memory enables novel visualization tools: spatiotemporal heatmaps, emotional trajectory analysis, and interactive path tracking, positioning the system as both a dialogue interface and research tool for biographical analysis. We use Van Gogh as a test case, but the architecture is generalizable to any historical figure with substantial textual records, offering a practical framework for educational, museum, and research applications requiring both accuracy and efficiency

</details>


### [4] [Hybrid Quantum Transformer for Language Generation](https://arxiv.org/abs/2511.10653)
*Desheng Kong,Xiangshuo Cui,Jiaying Jin,Jing Xu,Donglin Wang*

Main category: cs.CL

TL;DR: 首个混合量子-经典大语言模型HyQuT在1.5亿参数规模下实现自然语言生成，量子组件可替代10%经典参数且保持性能


<details>
  <summary>Details</summary>
Motivation: 现有量子/混合模型多局限于简单任务，尚未应用于大规模自然语言生成，本研究旨在展示量子计算在大规模生成模型中的可行性

Method: 将变分量子电路(VQCs)集成到Transformer框架，构建8M和150M参数规模的模型，使用10量子位+80量子门替代部分经典参数

Result: 150M参数模型中量子组件替代约10%经典参数，收敛稳定性和生成质量与传统模型相当

Conclusion: 首次验证量子计算与大规模生成语言模型结合的可行性，为量子-经典混合模型发展提供早期示范

Abstract: Although quantum computing has been increasingly applied to replace classical computation, most existing quantum or hybrid models remain confined to simple tasks, with no successful application to large-scale natural language generation to date. In this work, we present the first hybrid quantum-classical large language model (LLM) for natural language generation, HyQuT, capable of performing coherent and context-aware dialogue. The proposed architecture integrates variational quantum circuits (VQCs) into the Transformer framework at both 8M and 150M parameter scales. Experimental results show that a minimal number of qubits (10 qubits with 80 quantum gates) can replace about 10% of the classical parameters in the 150M-parameter model, while achieving comparable convergence stability and generation quality. This study provides an early demonstration of the feasibility of integrating quantum computing to large-scale generative language models.

</details>


### [5] [Empirical Characterization of Temporal Constraint Processing in LLMs](https://arxiv.org/abs/2511.10654)
*Javier Marín*

Main category: cs.CL

TL;DR: 当前大型语言模型在时间约束处理上存在系统性风险，模型参数量与能力无关，需架构改进才能满足实时决策需求


<details>
  <summary>Details</summary>
Motivation: 验证LLMs在实时决策场景中处理时间约束的可靠性，揭示现有模型在截止时间检测任务中的系统性缺陷

Method: 使用8个生产级模型（2.8-8B参数）进行截止时间检测任务测试，分析性能分布/提示敏感性/行为偏差，并通过200个合成样本进行微调实验

Result: 发现双峰性能分布（95%或50%准确率）、60%提示敏感性波动、100%误报率等问题；3.8B模型性能匹配7B模型；微调仅部分提升能力（12-37个百分点）

Conclusion: 单纯语言模型无法可靠处理时间约束，需架构改进：持续时间状态表征、独立约束检查机制、时序关系组合推理。当前自回归架构不适合时间敏感场景部署

Abstract: When deploying LLMs in agentic architectures requiring real-time decisions under temporal constraints, we assume they reliably determine whether action windows remain open or have closed. This assumption is untested. We characterize temporal constraint processing across eight production-scale models (2.8-8B parameters) using deadline detection tasks, revealing systematic deployment risks: bimodal performance distribution (models achieve either 95% or 50% accuracy), extreme prompt brittleness (30-60 percentage point swings from formatting changes alone), and systematic action bias (100% false positive rates in failing models). Parameter count shows no correlation with capability in this range-a 3.8B model matches 7B models while other 7B models fail completely. Fine-tuning on 200 synthetic examples improves models with partial capability by 12-37 percentage points. We demonstrate that temporal constraint satisfaction cannot be reliably learned through next-token prediction on natural language, even with targeted fine-tuning. This capability requires architectural mechanisms for: (1) continuous temporal state representation, (2) explicit constraint checking separate from linguistic pattern matching, (3) systematic compositional reasoning over temporal relations. Current autoregressive architectures lack these mechanisms. Deploying such systems in time-critical applications without hybrid architectures incorporating symbolic reasoning modules represents unacceptable risk.

</details>


### [6] [Spectral Neuro-Symbolic Reasoning II: Semantic Node Merging, Entailment Filtering, and Knowledge Graph Alignment](https://arxiv.org/abs/2511.10655)
*Andrew Kiruluta,Priscilla Burity*

Main category: cs.CL

TL;DR: 通过引入基于Transformer的节点合并、句子级蕴涵验证和知识图谱对齐三种语义增强方法，提升光谱神经符号推理框架的图质量并保持核心推理流程不变


<details>
  <summary>Details</summary>
Motivation: 解决光谱NSR框架中存在的节点冗余、边质量不足和上下文缺失问题，在保持核心光谱推理优势的同时提升图结构的语义合理性

Method: 1）使用Sentence-BERT等上下文嵌入进行节点合并；2）利用预训练NLI模型进行句子级蕴涵验证；3）通过ConceptNet等知识图谱补充上下文信息

Result: 在ProofWriter等基准测试中实现最高3.8%的准确率提升，增强对抗样本的泛化能力并降低推理噪声

Conclusion: 通过模块化的语义预处理步骤实现了更鲁棒、可解释的推理系统，为开放域应用提供可扩展的解决方案

Abstract: This report extends the Spectral Neuro-Symbolic Reasoning (Spectral NSR) framework by introducing three semantically grounded enhancements: (1) transformer-based node merging using contextual embeddings (e.g., Sentence-BERT, SimCSE) to reduce redundancy, (2) sentence-level entailment validation with pretrained NLI classifiers (e.g., RoBERTa, DeBERTa) to improve edge quality, and (3) alignment with external knowledge graphs (e.g., ConceptNet, Wikidata) to augment missing context. These modifications enhance graph fidelity while preserving the core spectral reasoning pipeline. Experimental results on ProofWriter, EntailmentBank, and CLUTRR benchmarks show consistent accuracy gains (up to +3.8\%), improved generalization to adversarial cases, and reduced inference noise. The novelty lies in performing semantic and symbolic refinement entirely upstream of the spectral inference stage, enabling efficient, interpretable, and scalable reasoning without relying on quadratic attention mechanisms. In summary, this work extends the Spectral NSR framework with modular, semantically grounded preprocessing steps that improve graph quality without altering the core spectral reasoning engine. The result is a more robust, interpretable, and scalable reasoning system suitable for deployment in open-domain and real-world settings.

</details>


### [7] [Preference Orchestrator: Prompt-Aware Multi-Objective Alignment for Large Language Models](https://arxiv.org/abs/2511.10656)
*Biao Liu,Ning Xu,Junming Yang,Xin Geng*

Main category: cs.CL

TL;DR: 提出PRO框架通过轻量级偏好适配器自动推断提示相关的多目标权重，解决传统多目标对齐方法依赖人工指定偏好的低效问题。


<details>
  <summary>Details</summary>
Motivation: 现有多目标对齐方法需要人工指定偏好权重，既增加用户负担又导致训练效率低下。

Method: 使用提示感知的偏好适配器自动学习各目标权重，通过多奖励模型归一化分数实现目标间有效平衡。

Result: 在多任务实验中获得优于现有方法的表现，理论证明其优于固定权重机制。

Conclusion: PRO框架通过自动推断动态偏好权重，显著提升多目标对齐的效果与训练效率。

Abstract: While Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language processing tasks, aligning these models with varying human preferences across multiple objectives remains a significant challenge in practical deployments. Existing multi-objective alignment methods rely on manually specified preference weights, which not only burden users with difficult preference specification tasks but also lead to suboptimal training efficiency due to exploration of irrelevant preference combinations. To alleviate these issues, we propose a novel framework named PRO, i.e., PReference Orchestrator, which features a lightweight preference adapter that automatically infers prompt-specific preference weights during both training and deployment phases. Specifically, the adapter automatically learns appropriate preference weights for each prompt by training on normalized reward scores from multiple reward models for preferred responses, which inherently reflect effective preference balances across objectives. Additionally, We provide theoretical analysis proving that our prompt-aware preference mechanism achieves superior performance compared to fixed preference weights in multi-objective alignment scenarios. Extensive experiments across multiple tasks demonstrate the effectiveness of our method over existing multi-objective alignment approaches.

</details>


### [8] [Patent Representation Learning via Self-supervision](https://arxiv.org/abs/2511.10657)
*You Zuo,Kim Gerdes,Eric Villemonte de La Clergerie,Benoît Sagot*

Main category: cs.CL

TL;DR: 提出基于专利内部多视图的对比学习框架，通过章节划分解决SimCSE方法在专利嵌入中产生的语义离散问题


<details>
  <summary>Details</summary>
Motivation: 现有基于dropout的对比学习方法在专利数据上会导致嵌入过度均匀化，破坏语义连贯性；同时传统方法过度依赖容易失效的专利引用或IPC分类标注

Method: 利用专利文档固有的章节结构（摘要、权利要求、背景等）构建多样化的对比学习视图，引入语义和结构的自然多样性

Result: 在专利检索和分类任务中达到或超越监督基线方法，分析显示不同章节具有任务特异性：权利要求部分提升检索效果，背景章节增强分类性能

Conclusion: 专利固有的篇章结构为表征学习提供了有效信息源，基于章节的多视图方法实现了可扩展且不依赖标注的专利理解

Abstract: This paper presents a simple yet effective contrastive learning framework for learning patent embeddings by leveraging multiple views from within the same document. We first identify a patent-specific failure mode of SimCSE style dropout augmentation: it produces overly uniform embeddings that lose semantic cohesion. To remedy this, we propose section-based augmentation, where different sections of a patent (e.g., abstract, claims, background) serve as complementary views. This design introduces natural semantic and structural diversity, mitigating over-dispersion and yielding embeddings that better preserve both global structure and local continuity. On large-scale benchmarks, our fully self-supervised method matches or surpasses citation-and IPC-supervised baselines in prior-art retrieval and classification, while avoiding reliance on brittle or incomplete annotations. Our analysis further shows that different sections specialize for different tasks-claims and summaries benefit retrieval, while background sections aid classification-highlighting the value of patents' inherent discourse structure for representation learning. These results highlight the value of exploiting intra-document views for scalable and generalizable patent understanding.

</details>


### [9] [Evaluating Open-Weight Large Language Models for Structured Data Extraction from Narrative Medical Reports Across Multiple Use Cases and Languages](https://arxiv.org/abs/2511.10658)
*Douwe J. Spaanderman,Karthik Prathaban,Petr Zelina,Kaouther Mouheb,Lukáš Hejtmánek,Matthew Marzetti,Antonius W. Schurink,Damian Chan,Ruben Niemantsverdriet,Frederik Hartmann,Zhen Qian,Maarten G. J. Thomeer,Petr Holub,Farhan Akram,Frank J. Wolters,Meike W. Vernooij,Cornelis Verhoef,Esther E. Bron,Vít Nováček,Dirk J. Grünhagen,Wiro J. Niessen,Martijn P. A. Starmans,Stefan Klein*

Main category: cs.CL

TL;DR: 研究证明开源大语言模型（LLMs）能跨疾病、语言和机构从临床报告中提取结构化数据，任务复杂性对性能影响大于模型规模或提示策略。


<details>
  <summary>Details</summary>
Motivation: 填补临床文本结构化信息提取研究的空白（既往研究集中于单任务、有限模型和英文报告），评估LLMs在多语言多场景临床报告中的通用性。

Method: 在荷兰、英国、捷克三国的病理/放射报告中测试15个开源LLMs，涵盖6个临床用例，比较6种提示策略（零样本/思维链/自洽等），使用共识排名聚合和线性混合效应模型量化性能差异。

Result: 最优模型宏观平均分接近任务间评分者一致性；中小型通用模型性能媲美大模型，微型/专用模型较差；提示图（prompt graph）和少样本提示提升13%性能；任务特异性因素（复杂度/标注差异）影响大于模型规模。

Conclusion: 开源LLMs可规模化实现跨疾病/语言/机构的临床数据提取，临床数据治理应优先考虑任务适配而非单纯扩大模型规模。

Abstract: Large language models (LLMs) are increasingly used to extract structured information from free-text clinical records, but prior work often focuses on single tasks, limited models, and English-language reports. We evaluated 15 open-weight LLMs on pathology and radiology reports across six use cases, colorectal liver metastases, liver tumours, neurodegenerative diseases, soft-tissue tumours, melanomas, and sarcomas, at three institutes in the Netherlands, UK, and Czech Republic. Models included general-purpose and medical-specialised LLMs of various sizes, and six prompting strategies were compared: zero-shot, one-shot, few-shot, chain-of-thought, self-consistency, and prompt graph. Performance was assessed using task-appropriate metrics, with consensus rank aggregation and linear mixed-effects models quantifying variance. Top-ranked models achieved macro-average scores close to inter-rater agreement across tasks. Small-to-medium general-purpose models performed comparably to large models, while tiny and specialised models performed worse. Prompt graph and few-shot prompting improved performance by ~13%. Task-specific factors, including variable complexity and annotation variability, influenced results more than model size or prompting strategy. These findings show that open-weight LLMs can extract structured data from clinical reports across diseases, languages, and institutions, offering a scalable approach for clinical data curation.

</details>


### [10] [Information Extraction From Fiscal Documents Using LLMs](https://arxiv.org/abs/2511.10659)
*Vikram Aggarwal,Jay Kulkarni,Aditi Mascarenhas,Aakriti Narang,Siddarth Raman,Ajay Shah,Susan Thomas*

Main category: cs.CL

TL;DR: 研究提出了一种基于LLM的多阶段方法，利用财政表格的层级结构特性，实现政府财政文件的结构化数据提取与多级验证，成功将PDF财政报告转化为可研究数据库。


<details>
  <summary>Details</summary>
Motivation: 传统OCR方法在财政数据提取中缺乏数值验证能力，而财政表格固有的层级合计结构为数据验证提供了天然基础。研究旨在探索LLM处理复杂层次化表格数据的潜力，解决发展中国家财政数据利用难题。

Method: 开发包含领域知识嵌入、上下文序列处理和算法验证的多阶段流程：1) 利用财政表格的层级结构建立多级验证机制 2) 通过层级合计关系进行数据一致性检查 3) 结合LLM的表格理解能力和结构化验证算法

Result: 在印度卡纳塔克邦200+页年度财政文件中实现高精度提取，验证了LLM处理复杂表格和文档层级结构的能力，成功创建可直接用于研究的结构化数据库。

Conclusion: 该方法展示了LLM在财政数据分析中的可扩展性，为发展中国家政府文件数字化提供了有效解决方案，具有广泛的应用前景。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in text comprehension, but their ability to process complex, hierarchical tabular data remains underexplored. We present a novel approach to extracting structured data from multi-page government fiscal documents using LLM-based techniques. Applied to annual fiscal documents from the State of Karnataka in India (200+ pages), our method achieves high accuracy through a multi-stage pipeline that leverages domain knowledge, sequential context, and algorithmic validation. A large challenge with traditional OCR methods is the inability to verify the accurate extraction of numbers. When applied to fiscal data, the inherent structure of fiscal tables, with totals at each level of the hierarchy, allows for robust internal validation of the extracted data. We use these hierarchical relationships to create multi-level validation checks. We demonstrate that LLMs can read tables and also process document-specific structural hierarchies, offering a scalable process for converting PDF-based fiscal disclosures into research-ready databases. Our implementation shows promise for broader applications across developing country contexts.

</details>


### [11] [Test-Time Steering for Lossless Text Compression via Weighted Product of Experts](https://arxiv.org/abs/2511.10660)
*Qihang Zhang,Muchen Li,Ziao Wang,Renjie Liao,Lele Wang*

Main category: cs.CL

TL;DR: 提出基于加权专家乘积(wPoE)的测试时指导框架，将传统压缩器与神经语言模型动态结合，在无需微调的情况下实现最优文本压缩效果


<details>
  <summary>Details</summary>
Motivation: 传统压缩器泛化性强但压缩率低，神经压缩器依赖训练数据且泛化能力差，需开发兼顾压缩效率和泛化能力的新方法

Method: 在推理阶段通过加权乘积策略自适应融合通用压缩模型和预训练语言模型，确保压缩率不低于最佳单一模型

Result: 实验证明方法显著提升文本压缩性能，且兼容任意自回归语言模型，适应不同数据分布

Conclusion: 该框架为跨数据分布的文本压缩提供实用解决方案，兼具传统方法的泛化性和神经模型的压缩效率优势

Abstract: Lossless compression techniques are crucial in an era of rapidly growing data. Traditional universal compressors like gzip offer low computational overhead, high speed, and broad applicability across data distributions. However, they often lead to worse compression rates than modern neural compressors, which leverage large-scale training data to model data distributions more effectively. Despite their advantages, neural compressors struggle to generalize to unseen data. To address this limitation, we propose a novel framework that performs Test-Time Steering via a Weighted Product of Experts (wPoE). At inference, our method adaptively combines a universal compression model with a pretrained neural language model, ensuring the compression rate is at least as good as that of the best individual model. Extensive experiments demonstrate that our approach improves the performance of text compression without requiring fine-tuning. Furthermore, it seamlessly integrates with any autoregressive language model, providing a practical solution for enhancing text compression across diverse data distributions.

</details>


### [12] [Bayesian Evaluation of Large Language Model Behavior](https://arxiv.org/abs/2511.10661)
*Rachel Longjohn,Shang Wu,Saatvik Kher,Catarina Belém,Padhraic Smyth*

Main category: cs.CL

TL;DR: 提出贝叶斯方法量化LLM文本生成系统在二元评估指标中的统计不确定性


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估方法忽略文本生成概率性导致的统计不确定性，影响评估可靠性

Method: 基于贝叶斯统计框架，建立概率生成模型分析拒绝率和模型偏好等指标的不确定性

Result: 在对抗性输入拒绝率评估和开放式对话模型偏好比较中验证了不确定性量化的有效性

Conclusion: 贝叶斯方法为LLM系统行为评估提供了关键统计不确定性信息，增强评估结果可信度

Abstract: It is increasingly important to evaluate how text generation systems based on large language models (LLMs) behave, such as their tendency to produce harmful output or their sensitivity to adversarial inputs. Such evaluations often rely on a curated benchmark set of input prompts provided to the LLM, where the output for each prompt may be assessed in a binary fashion (e.g., harmful/non-harmful or does not leak/leaks sensitive information), and the aggregation of binary scores is used to evaluate the LLM. However, existing approaches to evaluation often neglect statistical uncertainty quantification. With an applied statistics audience in mind, we provide background on LLM text generation and evaluation, and then describe a Bayesian approach for quantifying uncertainty in binary evaluation metrics. We focus in particular on uncertainty that is induced by the probabilistic text generation strategies typically deployed in LLM-based systems. We present two case studies applying this approach: 1) evaluating refusal rates on a benchmark of adversarial inputs designed to elicit harmful responses, and 2) evaluating pairwise preferences of one LLM over another on a benchmark of open-ended interactive dialogue examples. We demonstrate how the Bayesian approach can provide useful uncertainty quantification about the behavior of LLM-based systems.

</details>


### [13] [Evaluating Modern Large Language Models on Low-Resource and Morphologically Rich Languages:A Cross-Lingual Benchmark Across Cantonese, Japanese, and Turkish](https://arxiv.org/abs/2511.10664)
*Chengxuan Xia,Qianye Wu,Hongbin Guan,Sixuan Tian,Yilun Hao,Xiaoyu Wu*

Main category: cs.CL

TL;DR: 评估7个大语言模型在粤语/日语/土耳其语的表现，发现大模型在文化理解和形态学处理存在不足


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在低资源/高形态变化语言的实际效能

Method: 构建跨语言评测基准（开放问答/摘要/翻译/文化对话），结合人工评估（流畅度/事实性/文化适配性）与自动指标（BLEU/ROUGE）

Result: GPT-4o/Claude3.5表现最优但存在文化理解差距，小模型（LLaMA-2/Mistral7B）准确度落后约30%

Conclusion: 资源差异导致模型表现断层，需开发具备文化意识和语言泛化能力的LLMs，已开源评测数据促进研究

Abstract: Large language models (LLMs) have achieved impressive results in high-resource languages like English, yet their effectiveness in low-resource and morphologically rich languages remains underexplored. In this paper, we present a comprehensive evaluation of seven cutting-edge LLMs -- including GPT-4o, GPT-4, Claude~3.5~Sonnet, LLaMA~3.1, Mistral~Large~2, LLaMA-2~Chat~13B, and Mistral~7B~Instruct -- on a new cross-lingual benchmark covering \textbf{Cantonese, Japanese, and Turkish}. Our benchmark spans four diverse tasks: open-domain question answering, document summarization, English-to-X translation, and culturally grounded dialogue. We combine \textbf{human evaluations} (rating fluency, factual accuracy, and cultural appropriateness) with automated metrics (e.g., BLEU, ROUGE) to assess model performance.
  Our results reveal that while the largest proprietary models (GPT-4o, GPT-4, Claude~3.5) generally lead across languages and tasks, significant gaps persist in culturally nuanced understanding and morphological generalization. Notably, GPT-4o demonstrates robust multilingual performance even on cross-lingual tasks, and Claude~3.5~Sonnet achieves competitive accuracy on knowledge and reasoning benchmarks. However, all models struggle to some extent with the unique linguistic challenges of each language, such as Turkish agglutinative morphology and Cantonese colloquialisms. Smaller open-source models (LLaMA-2~13B, Mistral~7B) lag substantially in fluency and accuracy, highlighting the resource disparity. We provide detailed quantitative results, qualitative error analysis, and discuss implications for developing more culturally aware and linguistically generalizable LLMs. Our benchmark and evaluation data are released to foster reproducibility and further research.

</details>


### [14] [Guarding the Meaning: Self-Supervised Training for Semantic Robustness in Guard Models](https://arxiv.org/abs/2511.10665)
*Cristina Pinneri,Christos Louizos*

Main category: cs.CL

TL;DR: Proposed self-supervised framework with skew-aware aggregation reduces semantic variability in guard models by 58% and improves calibration by 40%.


<details>
  <summary>Details</summary>
Motivation: Guard models' safety scores show dangerous sensitivity to linguistic paraphrases despite preserved meaning, revealing lack of semantic grounding in existing approaches.

Method: Leverages paraphrase sets with novel skew-aware aggregation strategy to enforce prediction consistency, avoiding pitfalls of traditional mean/median approaches.

Result: 58% reduction in semantic variability across paraphrases, 2.5% average accuracy improvement on benchmarks, 40% calibration enhancement with bidirectional consistency-calibration relationship.

Conclusion: Semantic consistency should be explicit training objective; provides scalable method for building more reliable guard models through robustness-calibration synergy.

Abstract: Guard models are a critical component of LLM safety, but their sensitivity to superficial linguistic variations remains a key vulnerability. We show that even meaning-preserving paraphrases can cause large fluctuations in safety scores, revealing a lack of semantic grounding. To address this, we introduce a practical, self-supervised framework for improving the semantic robustness of guard models. Our method leverages paraphrase sets to enforce prediction consistency using a novel, skew-aware aggregation strategy for robust target computation. Notably, we find that standard aggregation methods like mean and median can degrade safety, underscoring the need for skew-aware alternatives. We analyze six open-source guard models and show that our approach reduces semantic variability across paraphrases by ~58%, improves benchmark accuracy by ~2.5% on average, and generalizes to unseen stylistic variations. Intriguingly, we discover a bidirectional relationship between model calibration and consistency: our robustness training improves calibration by up to 40%, revealing a fundamental connection between these properties. These results highlight the value of treating semantic consistency as a first-class training objective and provide a scalable recipe for building more reliable guard models.

</details>


### [15] [Evaluating LLM Understanding via Structured Tabular Decision Simulations](https://arxiv.org/abs/2511.10667)
*Sichao Li,Xinyue Xu,Xiaomeng Li*

Main category: cs.CL

TL;DR: 提出STaDS框架评估LLM理解能力，发现现有模型存在准确性与决策因素依赖不匹配问题


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估过度关注预测准确性，缺乏对真正理解能力的全局评估。理解能力应包含跨领域一致性决策和可靠决策因素依赖

Method: 开发STaDS框架，通过三层次评估：1）问题与指令理解 2）知识驱动预测 3）相关决策因素依赖性分析，覆盖15个领域测试9个前沿LLM

Result: 1）多数模型跨领域表现不稳定 2）存在预测准确但决策依据不忠实现象 3）模型解释与预测驱动因素存在显著脱节

Conclusion: 需建立超越准确率的全局理解评估体系，开发能提升LLM理解能力的新框架，重点关注决策依据与预测结果的一致性

Abstract: Large language models (LLMs) often achieve impressive predictive accuracy, yet correctness alone does not imply genuine understanding. True LLM understanding, analogous to human expertise, requires making consistent, well-founded decisions across multiple instances and diverse domains, relying on relevant and domain-grounded decision factors. We introduce Structured Tabular Decision Simulations (STaDS), a suite of expert-like decision settings that evaluate LLMs as if they were professionals undertaking structured decision ``exams''. In this context, understanding is defined as the ability to identify and rely on the correct decision factors, features that determine outcomes within a domain. STaDS jointly assesses understanding through: (i) question and instruction comprehension, (ii) knowledge-based prediction, and (iii) reliance on relevant decision factors. By analyzing 9 frontier LLMs across 15 diverse decision settings, we find that (a) most models struggle to achieve consistently strong accuracy across diverse domains; (b) models can be accurate yet globally unfaithful, and there are frequent mismatches between stated rationales and factors driving predictions. Our findings highlight the need for global-level understanding evaluation protocols and advocate for novel frameworks that go beyond accuracy to enhance LLMs' understanding ability.

</details>


### [16] [Forecasting Spoken Language Development in Children with Cochlear Implants Using Preimplantation MRI](https://arxiv.org/abs/2511.10669)
*Yanlin Wang,Di Yuan,Shani Dettman,Dawn Choo,Emily Shimeng Xu,Denise Thomas,Maura E Ryan,Patrick C M Wong,Nancy M Young*

Main category: cs.CL

TL;DR: 深度迁移学习（DTL）模型在预测人工耳蜗植入儿童语言改善方面显著优于传统机器学习，准确率达92.39%且AUC为0.977。


<details>
  <summary>Details</summary>
Motivation: 传统预测指标（植入年龄/残余听力）无法可靠预测儿童人工耳蜗术后语言发展差异，需探索更精准的预测方法。

Method: 基于278名儿童的多中心数据，采用双线性注意力融合策略的DTL模型，对比传统ML在脑解剖特征的预测表现。

Result: DTL模型实现92.39%准确率、91.22%敏感性及93.56%特异性，AUC达0.977，全面超越传统ML模型。

Conclusion: DTL通过表征学习直接捕获任务特异性信息，验证了全球CI项目应用单一预测模型的可行性。

Abstract: Cochlear implants (CI) significantly improve spoken language in children with severe-to-profound sensorineural hearing loss (SNHL), yet outcomes remain more variable than in children with normal hearing. This variability cannot be reliably predicted for individual children using age at implantation or residual hearing. This study aims to compare the accuracy of traditional machine learning (ML) to deep transfer learning (DTL) algorithms to predict post-CI spoken language development of children with bilateral SNHL using a binary classification model of high versus low language improvers. A total of 278 implanted children enrolled from three centers. The accuracy, sensitivity and specificity of prediction models based upon brain neuroanatomic features using traditional ML and DTL learning. DTL prediction models using bilinear attention-based fusion strategy achieved: accuracy of 92.39% (95% CI, 90.70%-94.07%), sensitivity of 91.22% (95% CI, 89.98%-92.47%), specificity of 93.56% (95% CI, 90.91%-96.21%), and area under the curve (AUC) of 0.977 (95% CI, 0.969-0.986). DTL outperformed traditional ML models in all outcome measures. DTL was significantly improved by direct capture of discriminative and task-specific information that are advantages of representation learning enabled by this approach over ML. The results support the feasibility of a single DTL prediction model for language prediction of children served by CI programs worldwide.

</details>


### [17] [Towards Fine-Grained Code-Switch Speech Translation with Semantic Space Alignment](https://arxiv.org/abs/2511.10670)
*Yan Gao,Yazheng Yang,Zhibin Lan,Yidong Chen,Min Zhang,Daimeng Wei,Hui Huang,Jinsong Su*

Main category: cs.CL

TL;DR: 提出MoE语音投影器增强LLM的多阶段训练框架，解决语码转换语音翻译中的语义建模和数据稀缺问题


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖模型隐式学习语义且需昂贵人工标注，语码转换语音翻译面临语义建模复杂性和高质量数据稀缺的双重挑战

Method: 1. 基于专家混合的语音投影器实现细粒度语音特征建模
2. 多阶段训练范式利用单语ASR/ST数据
3. 语言特定损失+组内负载均衡损失指导专家分配
4. 过渡损失缓解训练阶段数据差异

Result: 在主流数据集上验证了方法的有效性和泛化性

Conclusion: 通过语音特征空间解耦和多阶段协同训练，有效突破语码转换语音翻译的数据瓶颈

Abstract: Code-switching (CS) speech translation (ST) refers to translating speech that alternates between two or more languages into a target language text, which poses significant challenges due to the complexity of semantic modeling and the scarcity of CS data. Previous studies tend to rely on the model itself to implicitly learn semantic modeling during training, and resort to inefficient and costly manual annotations for these two challenges. To mitigate these limitations, we propose enhancing Large Language Models (LLMs) with a Mixture of Experts (MoE) speech projector, where each expert specializes in the semantic subspace of a specific language, enabling fine-grained modeling of speech features. Additionally, we introduce a multi-stage training paradigm that utilizes readily available monolingual automatic speech recognition (ASR) and monolingual ST data, facilitating speech-text alignment and improving translation capabilities. During training, we leverage a combination of language-specific loss and intra-group load balancing loss to guide the MoE speech projector in efficiently allocating tokens to the appropriate experts, across expert groups and within each group, respectively. To bridge the data gap across different training stages and improve adaptation to the CS scenario, we further employ a transition loss, enabling smooth transitions of data between stages, to effectively address the scarcity of high-quality CS speech translation data. Extensive experiments on widely used datasets demonstrate the effectiveness and generality of our approach.

</details>


### [18] [Grounded Visual Factualization: Factual Anchor-Based Finetuning for Enhancing MLLM Factual Consistency](https://arxiv.org/abs/2511.10671)
*Filippo Morbiato,Luca Romano,Alessandro Persona*

Main category: cs.CL

TL;DR: 提出GVF微调方法，通过结构化事实信号增强多模态大语言模型的视觉事实一致性，显著减少幻觉现象同时保持通用能力


<details>
  <summary>Details</summary>
Motivation: 现有微调方法对多模态大语言模型的视觉幻觉（生成内容与图像事实不符）问题改进有限，需系统性增强事实推理能力

Method: 1) 事实锚点数据增强（结构化事实锚点与反事实提示）
2) 事实感知指令调整（显式嵌入事实线索）
3) 事实一致性损失函数（专门惩罚事实错误）

Result: 在VHTest基准上OEQ/YNQ准确率显著超越标准微调，同时保持MME/POPE等通用基准性能（LLaVA-1.5-13B模型验证）

Conclusion: GVF微调首次系统解决视觉幻觉问题，实现事实准确性与通用能力的平衡，为可靠多模态系统提供新方向

Abstract: Visual hallucination, where Multimodal Large Language Models fabricate details inconsistent with image content, critically undermines their reliability. Existing fine-tuning methods offer limited improvement, failing to deeply intervene in factual reasoning. This paper introduces Grounded Visual Factualization (GVF) Finetuning, a novel approach to systematically enhance MLLM visual factual consistency. GVF integrates explicit factual signals via three core mechanisms: Factual Anchor Data Augmentation, enriching training data with structured factual anchors and counter-factual prompts; Fact-Aware Instruction Tuning, embedding these cues into explicit instructions; and a Factual Consistency Loss function, specifically penalizing factual inaccuracies. Evaluated on LLaVA-1.5-13B, GVF Finetuning significantly outperforms standard fine-tuning on the VHTest benchmark for both Open-Ended Question (OEQ) and Yes/No Question (YNQ) formats. Crucially, GVF maintains or even slightly improves performance on general multimodal benchmarks like MME and POPE, demonstrating effective mitigation of visual hallucinations without compromising general understanding and reasoning abilities.

</details>


### [19] [Large language models in materials science and the need for open-source approaches](https://arxiv.org/abs/2511.10673)
*Fengxu Yang,Weitong Chen,Jack D. Evans*

Main category: cs.CL

TL;DR: 大语言模型正快速变革材料科学，开源模型展现与闭源模型相当的性能优势


<details>
  <summary>Details</summary>
Motivation: 探讨开源大语言模型在材料发现全流程中的应用潜力，推动构建透明可复现的科研AI平台

Method: 从文献挖掘、性能预测、多智能体实验系统三个维度系统评估模型能力，建立开源与闭源模型的基准测试对比

Result: 开源模型在保持性能的同时，在透明度、复现性、成本效益和数据隐私方面更具优势

Conclusion: 建议科研界优先采用持续改进的开源模型，构建开放灵活的社区驱动型人工智能科研平台

Abstract: Large language models (LLMs) are rapidly transforming materials science. This review examines recent LLM applications across the materials discovery pipeline, focusing on three key areas: mining scientific literature , predictive modelling, and multi-agent experimental systems. We highlight how LLMs extract valuable information such as synthesis conditions from text, learn structure-property relationships, and can coordinate agentic systems integrating computational tools and laboratory automation. While progress has been largely dependent on closed-source commercial models, our benchmark results demonstrate that open-source alternatives can match performance while offering greater transparency, reproducibility, cost-effectiveness, and data privacy. As open-source models continue to improve, we advocate their broader adoption to build accessible, flexible, and community-driven AI platforms for scientific discovery.

</details>


### [20] [Continual Learning of Domain Knowledge from Human Feedback in Text-to-SQL](https://arxiv.org/abs/2511.10674)
*Thomas Cook,Kelly Patel,Sivapriya Vellaichamy,Saba Rahimi,Zhen Zeng,Sumitra Ganesh*

Main category: cs.CL

TL;DR: 提出通过人类反馈实现文本到SQL持续学习的框架，将隐性领域知识转化为结构化记忆以提升执行准确率


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在文本转SQL时面临的数据库特定模式适应困难与隐性领域知识缺失问题

Method: 设计支持持续学习代理架构，通过人类反馈优化查询并构建结构化记忆库（Procedural Agent表现最佳）

Result: 在BIRD基准测试中，记忆增强代理实现9.2%绝对准确率提升与35%错误率降低

Conclusion: 将人类隐性知识显性化存储是构建自适应文本到SQL系统的关键，为人机协同持续学习开辟新路径

Abstract: Large Language Models (LLMs) can generate SQL queries from natural language questions but struggle with database-specific schemas and tacit domain knowledge. We introduce a framework for continual learning from human feedback in text-to-SQL, where a learning agent receives natural language feedback to refine queries and distills the revealed knowledge for reuse on future tasks. This distilled knowledge is stored in a structured memory, enabling the agent to improve execution accuracy over time. We design and evaluate multiple variations of a learning agent architecture that vary in how they capture and retrieve past experiences. Experiments on the BIRD benchmark Dev set show that memory-augmented agents, particularly the Procedural Agent, achieve significant accuracy gains and error reduction by leveraging human-in-the-loop feedback. Our results highlight the importance of transforming tacit human expertise into reusable knowledge, paving the way for more adaptive, domain-aware text-to-SQL systems that continually learn from a human-in-the-loop.

</details>


### [21] [Learn to Select: Exploring Label Distribution Divergence for In-Context Demonstration Selection in Text Classification](https://arxiv.org/abs/2511.10675)
*Ye Jiang,Taihang Wang,Youzheng Liu,Yimin Wang,Yuhan Xia,Yunfei Long*

Main category: cs.CL

TL;DR: 提出两阶段演示选择方法L2D，结合TopK筛选和标签分布差异计算，通过微调BERT模型实现语义与标签分布双对齐，在七个文本分类基准上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有上下文学习方法过度依赖语义相似性，忽视标签分布对齐对模型性能的影响，导致大语言模型表现不稳定。

Method: 1. TopK阶段筛选语义相似候选集 2. 使用微调BERT生成标签分布，计算KL散度选择标签分布最匹配的演示(两阶段L2D方法)

Result: 1. 七大数据集全面超越基准模型 2. 大模型性能与小模型标签预测准确率呈强正相关(r=0.82)

Conclusion: 标签分布对齐是提升上下文学习效果的关键因素，L2D通过融合语义与统计特征为演示选择提供新范式，且模型级联方法展现跨尺度模型协同潜力。

Abstract: In-context learning (ICL) for text classification, which uses a few input-label demonstrations to describe a task, has demonstrated impressive performance on large language models (LLMs). However, the selection of in-context demonstrations plays a crucial role and can significantly affect LLMs' performance. Most existing demonstration selection methods primarily focus on semantic similarity between test inputs and demonstrations, often overlooking the importance of label distribution alignment. To address this limitation, we propose a two-stage demonstration selection method, TopK + Label Distribution Divergence (L2D), which leverages a fine-tuned BERT-like small language model (SLM) to generate label distributions and calculate their divergence for both test inputs and candidate demonstrations. This enables the selection of demonstrations that are not only semantically similar but also aligned in label distribution with the test input. Extensive experiments across seven text classification benchmarks show that our method consistently outperforms previous demonstration selection strategies. Further analysis reveals a positive correlation between the performance of LLMs and the accuracy of the underlying SLMs used for label distribution estimation.

</details>


### [22] [Pre-Attention Expert Prediction and Prefetching for Mixture-of-Experts Large Language Models](https://arxiv.org/abs/2511.10676)
*Shien Zhu,Samuel Bohl,Robin Oester,Gustavo Alonso*

Main category: cs.CL

TL;DR: 提出预注意力专家预测方法，通过简单线性函数和排名感知损失实现高效专家预取，在多个模型上达到93-97%准确率，较现有方法提升约15%


<details>
  <summary>Details</summary>
Motivation: 现有基于前层激活值的专家预测方法存在准确率低（首层未优化）和计算开销大的问题，需开发更精准轻量的预测方案

Method: 利用注意力块前的同层激活值，采用双线性函数和排名感知损失进行专家预测，支持首层预取且计算开销低

Result: DeepSeek V2 Lite准确率93.03%，Qwen3-30B达94.69%，Phi-mini-MoE达97.62%，绝对准确率提升约15%

Conclusion: 预注意力机制显著提升专家预测精度，验证了LLM中部分函数具有排序保持特性，为MoE模型推理加速提供有效方案

Abstract: Mixture-of-Experts (MoE) Large Language Models (LLMs) efficiently scale-up the model while keeping relatively low inference cost. As MoE models only activate part of the experts, related work has proposed expert prediction and caching methods to prefetch the experts for faster inference. However, existing approaches utilize the activations from the previous layer for prediction, incurring low accuracy and leave the first layer unoptimized. Applying complex layers or even training standalone networks for better prediction introduces high computation overhead. In this paper, we propose pre-attention expert prediction to achieve accurate and lightweight expert prefetching. The key insight is that some functions in LLMs are ranking-preserving, indicating that matching the ranking of selected experts using simple linear functions is possible. Therefore, we utilize the activations before the attention block in the same layer with 2 linear functions and ranking-aware loss to achieve accurate prediction, which also supports prefetching in the first layer. Our lightweight, pre-attention expert routers achieve 93.03% accuracy on DeepSeek V2 Lite, 94.69% on Qwen3-30B, and 97.62% on Phi-mini-MoE, showing about 15% improvement on absolute accuracy over the state-of-the-art methods.

</details>


### [23] [SpiderGen: Towards Procedure Generation For Carbon Life Cycle Assessments with Generative AI](https://arxiv.org/abs/2511.10684)
*Anupama Sitaraman,Bharathan Balaji,Yuvraj Agarwal*

Main category: cs.CL

TL;DR: 开发LLM驱动的SpiderGen系统整合传统LCA方法，显著降低碳足迹评估成本（从2.5万美元/21人日降至1美元/10分钟）


<details>
  <summary>Details</summary>
Motivation: 传统生命周期评估(LCA)成本高昂且耗时（超25000美元/21人日），阻碍碳影响评估普及。通过结合LLM的推理能力，旨在降低专业门槛与实施成本

Method: 构建SpiderGen工作流：1）融合LCA方法论与LLM知识库 2）使用真实LCA文档作为基准评估 3）对比思维链提示/单次提示等基线方法

Result: 达到62% F1分数（10样本），成本降低99.996%（1美元/10分钟 vs 传统2.5万美元/21人日），但存在细节差异导致的次要错误（15%过程遗漏/幻觉）

Conclusion: SpiderGen在快速生成LCA流程上展现商业潜力，未来需解决：1）LCA文档细节标准化 2）辅助过程范围界定 3）错误修正机制优化

Abstract: Investigating the effects of climate change and global warming caused by GHG emissions have been a primary concern worldwide. These emissions are largely contributed to by the production, use and disposal of consumer products. Thus, it is important to build tools to estimate the environmental impact of consumer goods, an essential part of which is conducting Life Cycle Assessments (LCAs). LCAs specify and account for the appropriate processes involved with the production, use, and disposal of the products. We present SpiderGen, an LLM-based workflow which integrates the taxonomy and methodology of traditional LCA with the reasoning capabilities and world knowledge of LLMs to generate the procedural information used for LCA. We additionally evaluate the output of SpiderGen using real-world LCA documents as ground-truth. We find that SpiderGen provides accurate LCA process information that is either fully correct or has minor errors, achieving an F1-Score of 62% across 10 sample data points. We observe that the remaining missed processes and hallucinated errors occur primarily due to differences in detail between LCA documents, as well as differences in the "scope" of which auxiliary processes must also be included. We also demonstrate that SpiderGen performs better than several baselines techniques, such as chain-of-thought prompting and one-shot prompting. Finally, we highlight SpiderGen's potential to reduce the human effort and costs for estimating carbon impact, as it is able to produce LCA process information for less than \$1 USD in under 10 minutes as compared to the status quo LCA, which can cost over \$25000 USD and take up to 21-person days.

</details>


### [24] [A methodological analysis of prompt perturbations and their effect on attack success rates](https://arxiv.org/abs/2511.10686)
*Tiago Machado,Maysa Malfiza Garcia de Macedo,Rogerio Abreu de Paula,Marcelo Carpinette Grave,Aminat Adebiyi,Luan Soares de Souza,Enrico Santarelli,Claudio Pinhanez*

Main category: cs.CL

TL;DR: 不同大模型对齐方法(SFT/DPO/RLHF)对提示攻击的敏感性存在显著差异，微小提示修改可显著改变攻击成功率(ASR)，现有攻击基准不足以全面评估漏洞。


<details>
  <summary>Details</summary>
Motivation: 研究主流大模型对齐方法（监督微调/直接偏好优化/强化学习人类反馈）在应对提示攻击时的脆弱性差异，评估现有攻击基准的有效性。

Method: 对开源模型进行系统统计分析，通过统计测试验证提示微小修改对攻击成功率(ASR)的敏感性。

Result: 统计测试显示提示微小变化即可显著改变ASR值，不同对齐方法对攻击类型的敏感度存在差异，现有攻击基准无法覆盖所有潜在漏洞。

Conclusion: 需建立基于统计的系统化评估体系，结合提示变体分析模型对齐方法的ASR敏感性，这对完善模型安全评估体系具有重要价值。

Abstract: This work aims to investigate how different Large Language Models (LLMs) alignment methods affect the models' responses to prompt attacks. We selected open source models based on the most common alignment methods, namely, Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Reinforcement Learning with Human Feedback (RLHF). We conducted a systematic analysis using statistical methods to verify how sensitive the Attack Success Rate (ASR) is when we apply variations to prompts designed to elicit inappropriate content from LLMs. Our results show that even small prompt modifications can significantly change the Attack Success Rate (ASR) according to the statistical tests we run, making the models more or less susceptible to types of attack. Critically, our results demonstrate that running existing 'attack benchmarks' alone may not be sufficient to elicit all possible vulnerabilities of both models and alignment methods. This paper thus contributes to ongoing efforts on model attack evaluation by means of systematic and statistically-based analyses of the different alignment methods and how sensitive their ASR is to prompt variation.

</details>


### [25] [Modeling and Predicting Multi-Turn Answer Instability in Large Language Models](https://arxiv.org/abs/2511.10688)
*Jiahang He,Rishi Ramachandran,Neel Ramachandran,Aryan Katakam,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Aryan Shrivastava*

Main category: cs.CL

TL;DR: 通过多轮重复提问测试发现主流大语言模型存在显著鲁棒性缺陷，提出用马尔可夫链建模稳态准确率作为交互场景的鲁棒性指标


<details>
  <summary>Details</summary>
Motivation: 随着大模型应用场景扩展，交互环境中的模型稳定性成为关键需求。现有研究缺乏对多轮对话中模型表现的系统性评估框架。

Method: 使用简单重复提示（如'再想想'）测试模型回答变化，建立马尔可夫链模型分析准确率动态，并探索隐藏状态线性探针对回答变化的预测能力

Result: '再想想'提示使Gemini准确率下降10%，组合提问使Claude准确率降7.5%。马尔可夫链可有效预测长期准确率（平均下降8%），隐藏状态分析显示线性探针具有预测潜力

Conclusion: 提出稳态准确率作为交互场景核心指标，暴露模型重复提问脆弱性。建议将稳定性评估纳入模型开发，特别是在需要持续推理的关键任务场景中

Abstract: As large language models (LLMs) are adopted in an increasingly wide range of applications, user-model interactions have grown in both frequency and scale. Consequently, research has focused on evaluating the robustness of LLMs, an essential quality for real-world tasks. In this paper, we employ simple multi-turn follow-up prompts to evaluate models' answer changes, model accuracy dynamics across turns with Markov chains, and examine whether linear probes can predict these changes. Our results show significant vulnerabilities in LLM robustness: a simple "Think again" prompt led to an approximate 10% accuracy drop for Gemini 1.5 Flash over nine turns, while combining this prompt with a semantically equivalent reworded question caused a 7.5% drop for Claude 3.5 Haiku. Additionally, we find that model accuracy across turns can be effectively modeled using Markov chains, enabling the prediction of accuracy probabilities over time. This allows for estimation of the model's stationary (long-run) accuracy, which we find to be on average approximately 8% lower than its first-turn accuracy for Gemini 1.5 Flash. Our results from a model's hidden states also reveal evidence that linear probes can help predict future answer changes. Together, these results establish stationary accuracy as a principled robustness metric for interactive settings and expose the fragility of models under repeated questioning. Addressing this instability will be essential for deploying LLMs in high-stakes and interactive settings where consistent reasoning is as important as initial accuracy.

</details>


### [26] [Equilibrium Dynamics and Mitigation of Gender Bias in Synthetically Generated Data](https://arxiv.org/abs/2511.10689)
*Ashish Kattamuri,Arpita Vats,Harshwardhan Fartale,Rahul Raja,Akshata Kishore Moharir,Ishita Prasad*

Main category: cs.CL

TL;DR: 递归生成合成数据时，性别偏见呈现动态平衡而非单调放大：低初始偏见(+36%)趋近模型固有水平，高初始偏见(-26%)衰减。对比增强策略显著降低下游偏见(平均91%)，但语义指标与公平性结果存在背离。


<details>
  <summary>Details</summary>
Motivation: 探究递归文本生成中性别偏见的动态演变，揭示现有评估框架(语义相似性指标)与下游任务公平性结果的潜在背离，推动负责任合成数据生成的多维评估体系构建。

Method: 采用三代递归生成实验，结合三种评估框架(规则匹配/语义嵌入/下游任务)，测试四种缓解策略(包括对比增强)在不同初始偏见水平(0.1/0.3/0.6)下的干预效果。

Result: 低初始偏见(0.1)向模型固有水平放大36%，高初始偏见(0.6)衰减26%。对比增强使下游任务偏见降低98.8%(低初始)和平均91%，但语义偏见指标反向升高。

Conclusion: 合成数据生成需多维评估框架，语义相似性指标不能替代行为公平性评估。对比增强作为有效缓解策略，其评估悖论凸显算法公平性研究的复杂性。

Abstract: Recursive prompting with large language models enables scalable synthetic dataset generation but introduces the risk of bias amplification. We investigate gender bias dynamics across three generations of recursive text generation using three complementary evaluation frameworks: rule-based pattern matching, embedding-based semantic similarity, and downstream task performance. Experiments with three initial bias levels (0.1, 0.3, 0.6) and four mitigation strategies reveal equilibrium dynamics rather than monotonic amplification. The low initial bias amplifies toward the model's inherent bias level (+36%), whereas the high initial bias decays toward it (-26%). Among mitigation methods, contrastive augmentation, which introduces gender-swapped variants, achieves significant downstream bias reduction (98.8% for low initial bias and 91% on average) despite producing higher embedding-based bias scores. This paradox demonstrates that semantic similarity metrics may diverge from behavioral fairness outcomes, highlighting the need for multidimensional evaluation in responsible synthetic data generation.

</details>


### [27] [Saying the Unsaid: Revealing the Hidden Language of Multimodal Systems Through Telephone Games](https://arxiv.org/abs/2511.10690)
*Juntu Zhao,Jialing Zhang,Chongxuan Li,Dequan Wang*

Main category: cs.CL

TL;DR: 通过多轮'电话游戏'分析多模态系统的概念连接强度，揭示其理解世界的'隐藏语言'，并提出Telescope数据集构建概念连接全景图


<details>
  <summary>Details</summary>
Motivation: 闭源多模态系统因黑盒架构导致隐藏语言不透明，需通过系统偏好偏差研究其内在概念连接机制

Method: 利用图像压缩-重构过程中的偏好偏差，设计多轮电话游戏观察概念共现频率，结合自建Telescope数据集进行定量分析

Result: 成功量化概念连接强度，构建全局概念连接图谱，发现超越文本/视觉相似性的新型概念关系路径

Conclusion: 为多模态系统的可解释性研究提供新范式，奠定后续隐式语言解析和系统可控性研究基础

Abstract: Recent closed-source multimodal systems have made great advances, but their hidden language for understanding the world remains opaque because of their black-box architectures. In this paper, we use the systems' preference bias to study their hidden language: During the process of compressing the input images (typically containing multiple concepts) into texts and then reconstructing them into images, the systems' inherent preference bias introduces specific shifts in the outputs, disrupting the original input concept co-occurrence. We employ the multi-round "telephone game" to strategically leverage this bias. By observing the co-occurrence frequencies of concepts in telephone games, we quantitatively investigate the concept connection strength in the understanding of multimodal systems, i.e., "hidden language." We also contribute Telescope, a dataset of 10,000+ concept pairs, as the database of our telephone game framework. Our telephone game is test-time scalable: By iteratively running telephone games, we can construct a global map of concept connections in multimodal systems' understanding. Here we can identify preference bias inherited from training, assess generalization capability advancement, and discover more stable pathways for fragile concept connections. Furthermore, we use Reasoning-LLMs to uncover unexpected concept relationships that transcend textual and visual similarities, inferring how multimodal systems understand and simulate the world. This study offers a new perspective on the hidden language of multimodal systems and lays the foundation for future research on the interpretability and controllability of multimodal systems.

</details>


### [28] [Evaluating from Benign to Dynamic Adversarial: A Squid Game for Large Language Models](https://arxiv.org/abs/2511.10691)
*Zijian Chen,Wenjun Zhang,Guangtao Zhai*

Main category: cs.CL

TL;DR: 提出动态对抗评估框架Squid Game，通过资源受限和不对称信息环境下的多关卡交互测试，揭示LLM在压力下的真实能力及静态基准的评估范式污染问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在数据污染风险，且无法评估LLM在压力环境下的行为。需构建动态对抗场景以更全面检测模型能力。

Method: 设计包含指令遵循、代码、推理等六关的对抗游戏，采用资源约束/信息不对称机制，对50+ LLM进行行为分析。

Result: 发现同系列模型的代际性能跃迁，部分模型使用投机性策略获胜，动态评估与静态基准呈互补性。

Conclusion: 动态对抗评估能有效补充传统基准，揭示模型潜在行为模式，需警惕评估范式污染对可信AI评估的影响。

Abstract: Contemporary benchmarks are struggling to keep pace with the development of large language models (LLMs). Although they are indispensable to evaluate model performance on various tasks, it is uncertain whether the models trained on Internet data have genuinely learned how to solve problems or merely seen the questions before. This potential data contamination issue presents a fundamental challenge to establishing trustworthy evaluation frameworks. Meanwhile, existing benchmarks predominantly assume benign, resource-rich settings, leaving the behavior of LLMs under pressure unexplored. In this paper, we introduce Squid Game, a dynamic and adversarial evaluation environment with resource-constrained and asymmetric information settings elaborated to evaluate LLMs through interactive gameplay against other LLM opponents. Notably, Squid Game consists of six elimination-style levels, focusing on multi-faceted abilities, such as instruction-following, code, reasoning, planning, and safety alignment. We evaluate over 50 LLMs on Squid Game, presenting the largest behavioral evaluation study of general LLMs on dynamic adversarial scenarios. We observe a clear generational phase transition on performance in the same model lineage and find evidence that some models resort to speculative shortcuts to win the game, indicating the possibility of higher-level evaluation paradigm contamination in static benchmarks. Furthermore, we compare prominent LLM benchmarks and Squid Game with correlation analyses, highlighting that dynamic evaluation can serve as a complementary part for static evaluations. The code and data will be released in the future.

</details>


### [29] [Do AI Voices Learn Social Nuances? A Case of Politeness and Speech Rate](https://arxiv.org/abs/2511.10693)
*Eyal Rabin,Zohar Elyoseph,Rotem Israel-Fishelson,Adi Dali,Ravit Nussinson*

Main category: cs.CL

TL;DR: AI语音系统在礼貌提示下语速显著变慢，显示其能隐式学习人类社交暗示


<details>
  <summary>Details</summary>
Motivation: 探究AI是否内化了人类通过降低语速表达礼貌的非显性韵律标记

Method: 使用2个主流AI平台的22种合成声音，分别在礼貌/随意提示下朗读固定文本并测量时长

Result: 礼貌提示使语速显著变慢（极大效应量），AI Studio全系及OpenAI大多数声音均呈现统计显著性

Conclusion: AI具备隐式学习人类心理细微特征的能力，正成为能强化社会规范的新型社交主体

Abstract: Voice-based artificial intelligence is increasingly expected to adhere to human social conventions, but can it learn implicit cues that are not explicitly programmed? This study investigates whether state-of-the-art text-to-speech systems have internalized the human tendency to reduce speech rate to convey politeness - a non-obvious prosodic marker. We prompted 22 synthetic voices from two leading AI platforms (AI Studio and OpenAI) to read a fixed script under both "polite and formal" and "casual and informal" conditions and measured the resulting speech duration. Across both AI platforms, the polite prompt produced slower speech than the casual prompt with very large effect sizes, an effect that was statistically significant for all of AI Studio's voices and for a large majority of OpenAI's voices. These results demonstrate that AI can implicitly learn and replicate psychological nuances of human communication, highlighting its emerging role as a social actor capable of reinforcing human social norms.

</details>


### [30] [Where does an LLM begin computing an instruction?](https://arxiv.org/abs/2511.10694)
*Aditya Pola,Vineeth N. Balasubramanian*

Main category: cs.CL

TL;DR: 论文通过激活修补技术发现Llama系列模型中存在指令跟随的起始点（onset），该拐点前后的干预效果差异显著，为定位指令执行起点提供了可复现方法。


<details>
  <summary>Details</summary>
Motivation: 探究神经网络层堆栈中指令跟随行为何时从'阅读'转向'执行'，明确模型理解与执行指令的关键转折点。

Method: 构建三组基础数据集（Key-Value/Quote Attribution/Letter Selection）及其双跳组合任务，通过最小对比提示对的激活修补技术测量层间翻转率。

Result: 在Llama系列模型中观察到onset拐点：该点前的干预显著改变预测结果，而点后干预失效；多跳任务呈现相似onset位置。

Conclusion: 该方法为定位指令跟随起点及跨任务/模型规模的比较提供了简洁、可复现的研究框架。

Abstract: Following an instruction involves distinct sub-processes, such as reading content, reading the instruction, executing it, and producing an answer. We ask where, along the layer stack, instruction following begins, the point where reading gives way to doing. We introduce three simple datasets (Key-Value, Quote Attribution, Letter Selection) and two hop compositions of these tasks. Using activation patching on minimal-contrast prompt pairs, we measure a layer-wise flip rate that indicates when substituting selected residual activations changes the predicted answer. Across models in the Llama family, we observe an inflection point, which we term onset, where interventions that change predictions before this point become largely ineffective afterward. Multi-hop compositions show a similar onset location. These results provide a simple, replicable way to locate where instruction following begins and to compare this location across tasks and model sizes.

</details>


### [31] ["As Eastern Powers, I will veto." : An Investigation of Nation-level Bias of Large Language Models in International Relations](https://arxiv.org/abs/2511.10695)
*Jonghyeon Choi,Yeonjun Choi,Hyun-chul Kim,Beakcheol Jang*

Main category: cs.CL

TL;DR: 本研究通过联合国安理会历史记录构建评估框架，揭示大语言模型在国际关系中的国家层面偏见，并提出基于检索增强生成和反射自省的去偏方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在国际关系领域的应用中存在潜在国家偏见风险，现有研究缺乏系统性评估框架和针对具体任务的偏见维度分析。

Method: 1. 利用联合国安理会历史数据构建三维度偏见评估框架
2. 测试不同LLM对安理会五常国家的偏见模式
3. 提出融合检索增强生成与反射自省机制的去偏框架

Result: 1. 模型间存在共同偏见模式（亲西方/反俄）但程度差异显著
2. 同一模型不同任务中国家偏见呈现多维特征
3. 推理能力强的模型（如GPT-4o-mini）偏见更少且性能更优
4. 去偏框架使GPT-4o-mini和LLama-3.3-70B的偏见减少32%

Conclusion: 国际关系领域应用LLM需建立「性能-偏见」双轨评估体系，本研究提出的反射增强框架有效提升模型事实推理能力并降低国家层面偏见。

Abstract: This paper systematically examines nation-level biases exhibited by Large Language Models (LLMs) within the domain of International Relations (IR). Leveraging historical records from the United Nations Security Council (UNSC), we developed a bias evaluation framework comprising three distinct tests to explore nation-level bias in various LLMs, with a particular focus on the five permanent members of the UNSC. Experimental results show that, even with the general bias patterns across models (e.g., favorable biases toward the western nations, and unfavorable biases toward Russia), these still vary based on the LLM. Notably, even within the same LLM, the direction and magnitude of bias for a nation change depending on the evaluation context. This observation suggests that LLM biases are fundamentally multidimensional, varying across models and tasks. We also observe that models with stronger reasoning abilities show reduced bias and better performance. Building on this finding, we introduce a debiasing framework that improves LLMs' factual reasoning combining Retrieval-Augmented Generation with Reflexion-based self-reflection techniques. Experiments show it effectively reduces nation-level bias, and improves performance, particularly in GPT-4o-mini and LLama-3.3-70B. Our findings emphasize the need to assess nation-level bias alongside performance when applying LLMs in the IR domain.

</details>


### [32] [$π$-Attention: Periodic Sparse Transformers for Efficient Long-Context Modeling](https://arxiv.org/abs/2511.10696)
*Dong Liu,Yanxuan Yu*

Main category: cs.CL

TL;DR: 提出ΠAttention——通过周期性稀疏注意力机制，在保持线性复杂度的同时显著提升长程建模能力，效果优于RingAttention和密集注意力


<details>
  <summary>Details</summary>
Motivation: 解决现有稀疏注意力方法（如RingAttention）感受野受限和缺乏适应性的问题，通过周期性跳跃和自适应融合门实现高效的长上下文建模

Method: 1. 注意力三因子分解（环局部邻域+π步长跳跃+自适应门融合） 2. 理论证明感受野增长优于RingAttention 3. 头部级稀疏协调机制

Result: 在语言模型/检索/视觉任务中达到或超越密集注意力效果，困惑度比RingAttention低8.3%，相同上下文长度下GPU使用减少50%

Conclusion: ΠAttention通过周期性稀疏模式与自适应机制的有效结合，为长上下文建模提供了效率与性能的最佳平衡

Abstract: Transformers have revolutionized natural language processing, but their quadratic complexity with respect to sequence length remains a fundamental bottleneck for long-range modeling. While sparse attention mechanisms like RingAttention reduce computational costs by restricting attention to local neighborhoods, they suffer from limited receptive fields and lack of adaptability. We present \PiAttention, a periodic sparse Transformer that factorizes attention into ring-local neighborhoods, deterministic $π$-stride skips, and an adaptive fusion gate. The periodic structure provides predictable coverage of distant tokens, while the sparse footprint keeps the per-layer complexity linear in context length. We prove that \PiAttention achieves $\mathcal{O}(kL + π\log L)$ receptive field growth compared to $\mathcal{O}(kL)$ for RingAttention, where $k$ is the local window size, $π$ is the skip period, and $L$ is the sequence length. Extensive experiments on language modeling, retrieval, and vision-language tasks demonstrate that \PiAttention matches or surpasses dense attention quality with 8.3\% lower perplexity than RingAttention while using 50\% fewer GPUs for the same context length. Our detailed ablations and visualizations reveal the importance of periodic skips, adaptive fusion, and head-level sparsity coordination for efficient long-context modeling.

</details>


### [33] [Faithful Summarization of Consumer Health Queries: A Cross-Lingual Framework with LLMs](https://arxiv.org/abs/2511.10768)
*Ajwad Abrar,Nafisa Tabassum Oeshy,Prianka Maheru,Farzana Tabassum,Tareque Mohmud Chowdhury*

Main category: cs.CL

TL;DR: 结合TextRank与医学实体识别的LLM框架显著提升医疗摘要的忠实性，人类评估超80%摘要保留关键信息。


<details>
  <summary>Details</summary>
Motivation: 医疗摘要失真可能导致严重后果，需开发可靠方法确保关键医疗信息不丢失。

Method: 融合TextRank句子抽取、医学实体识别与LLaMA-2-7B微调（基于MeQSum和BanglaCHQ-Summ数据集）

Result: 质量指标(ROUGE↑/BERTScore↑/可读性↑)与忠实性指标(SummaC↑/AlignScore↑)全面超越零样本基线及传统系统，人类评估83%+关键信息保留率

Conclusion: 忠实性是医疗摘要的核心要求，本研究框架为LLM在医疗领域的安全应用提供了有效解决方案

Abstract: Summarizing consumer health questions (CHQs) can ease communication in healthcare, but unfaithful summaries that misrepresent medical details pose serious risks. We propose a framework that combines TextRank-based sentence extraction and medical named entity recognition with large language models (LLMs) to enhance faithfulness in medical text summarization. In our experiments, we fine-tuned the LLaMA-2-7B model on the MeQSum (English) and BanglaCHQ-Summ (Bangla) datasets, achieving consistent improvements across quality (ROUGE, BERTScore, readability) and faithfulness (SummaC, AlignScore) metrics, and outperforming zero-shot baselines and prior systems. Human evaluation further shows that over 80\% of generated summaries preserve critical medical information. These results highlight faithfulness as an essential dimension for reliable medical summarization and demonstrate the potential of our approach for safer deployment of LLMs in healthcare contexts.

</details>


### [34] [TEDxTN: A Three-way Speech Translation Corpus for Code-Switched Tunisian Arabic - English](https://arxiv.org/abs/2511.10780)
*Fethi Bougares,Salima Mdhaffar,Haroun Elleuch,Yannick Estève*

Main category: cs.CL

TL;DR: 首个公开的突尼斯阿拉伯语-英语语音翻译数据集TEDxTN，包含25小时多方言语音数据及基线系统结果


<details>
  <summary>Details</summary>
Motivation: 解决突尼斯阿拉伯方言自然语言处理领域的数据稀缺问题，促进方言语音翻译研究

Method: 收集108个TEDx演讲→人工标注(转写/翻译)→覆盖11个地区25小时语音→建立端到端语音识别/翻译基线模型

Result: 成功构建首个包含语码转换的公开突尼斯方言语音翻译语料库，并发布标注规范与预训练模型基准结果

Conclusion: 该资源填补了突尼斯方言NLP研究的空白，为方言语音处理及语码转换研究提供了重要基础

Abstract: In this paper, we introduce TEDxTN, the first publicly available Tunisian Arabic to English speech translation dataset. This work is in line with the ongoing effort to mitigate the data scarcity obstacle for a number of Arabic dialects. We collected, segmented, transcribed and translated 108 TEDx talks following our internally developed annotations guidelines. The collected talks represent 25 hours of speech with code-switching that cover speakers with various accents from over 11 different regions of Tunisia. We make the annotation guidelines and corpus publicly available. This will enable the extension of TEDxTN to new talks as they become available. We also report results for strong baseline systems of Speech Recognition and Speech Translation using multiple pre-trained and fine-tuned end-to-end models. This corpus is the first open source and publicly available speech translation corpus of Code-Switching Tunisian dialect. We believe that this is a valuable resource that can motivate and facilitate further research on the natural language processing of Tunisian Dialect.

</details>


### [35] [Sabiá: Um Chatbot de Inteligência Artificial Generativa para Suporte no Dia a Dia do Ensino Superior](https://arxiv.org/abs/2511.10787)
*Guilherme Biava Rodrigues,Franciele Beal,Marlon Marcon,Alinne Cristinne Corrêa Souza,André Roberto Ortoncelli,Francisco Carlos Monteiro Souza,Rodolfo Adamshuk Silva*

Main category: cs.CL

TL;DR: 开发基于GenAI和RAG的聊天机器人解决学术信息碎片化问题，Gemini 2.0 Flash和Gemma 3n表现优异


<details>
  <summary>Details</summary>
Motivation: 学术信息分散在多个平台导致学生获取困难，需要集中化解决方案

Method: 采用生成式AI和检索增强生成技术，测试多模型并评估质量指标/LLM裁判

Result: Gemini 2.0 Flash质量速度兼备，Gemma 3n开源表现良好

Conclusion: AI聊天机器人可有效整合碎片化信息，不同模型适用不同应用场景

Abstract: Students often report difficulties in accessing day-to-day academic information, which is usually spread across numerous institutional documents and websites. This fragmentation results in a lack of clarity and causes confusion about routine university information. This project proposes the development of a chatbot using Generative Artificial Intelligence (GenAI) and Retrieval-Augmented Generation (RAG) to simplify access to such information. Several GenAI models were tested and evaluated based on quality metrics and the LLM-as-a-Judge approach. Among them, Gemini 2.0 Flash stood out for its quality and speed, and Gemma 3n for its good performance and open-source nature.

</details>


### [36] [LLM-as-a-Grader: Practical Insights from Large Language Model for Short-Answer and Report Evaluation](https://arxiv.org/abs/2511.10819)
*Grace Byun,Swati Rajwal,Jinho D. Choi*

Main category: cs.CL

TL;DR: 研究探讨GPT-4o在教育评估中自动评分的可行性，在计算语言学课程中对比LLM与助教评分结果，显示高相关性但存在开放性问题评分波动。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在真实课堂场景中与人类评分的一致性，填补现有研究中对教育场景实际应用验证的空白。

Method: 收集50名学生的5次测验答案和14组项目报告，使用GPT-4o独立评分并与助教人工评分进行对比分析。

Result: 测验评分相关系数最高达0.98，55%案例完全一致；项目报告总体吻合但技术性开放答案存在评分差异。

Conclusion: LLM评分系统在标准化答案评估中潜力显著，但开放性问题需谨慎使用，研究为教育场景自动化评分提供实践参考。

Abstract: Large Language Models (LLMs) are increasingly explored for educational tasks such as grading, yet their alignment with human evaluation in real classrooms remains underexamined. In this study, we investigate the feasibility of using an LLM (GPT-4o) to evaluate short-answer quizzes and project reports in an undergraduate Computational Linguistics course. We collect responses from approximately 50 students across five quizzes and receive project reports from 14 teams. LLM-generated scores are compared against human evaluations conducted independently by the course teaching assistants (TAs). Our results show that GPT-4o achieves strong correlation with human graders (up to 0.98) and exact score agreement in 55\% of quiz cases. For project reports, it also shows strong overall alignment with human grading, while exhibiting some variability in scoring technical, open-ended responses. We release all code and sample data to support further research on LLMs in educational assessment. This work highlights both the potential and limitations of LLM-based grading systems and contributes to advancing automated grading in real-world academic settings.

</details>


### [37] [Tracing Multilingual Representations in LLMs with Cross-Layer Transcoders](https://arxiv.org/abs/2511.10840)
*Abir Harrasse,Florent Draye,Zhijing Jin,Bernhard Schölkopf*

Main category: cs.CL

TL;DR: 研究揭示多语言大模型通过核心语言共享表征+后期语言特定解码的机制运作，解码依赖高频语言特征，训练语言主导性影响模型表现。


<details>
  <summary>Details</summary>
Motivation: 探索多语言大模型内部如何表征不同语言，验证其是否采用跨语言共享表征+语言特定解码的机制，并解释主导训练语言仍表现更优的现象。

Method: 训练不同多语言数据配比的LLMs，使用跨层转码器(CLT)和归因图分析内部机制，通过特征干预实现语言替换。

Result: 发现核心语言共享表征机制：模型前期层跨语言表征高度一致，后期层出现语言特定解码路径，高频语言特征线性读取语言身份信息。

Conclusion: 理解核心语言机制对提升LLMs多语言对齐至关重要，主导训练语言通过归因图和解码路径影响多语言处理机制。

Abstract: Multilingual Large Language Models (LLMs) can process many languages, yet how they internally represent this diversity remains unclear. Do they form shared multilingual representations with language-specific decoding, and if so, why does performance still favor the dominant training language? To address this, we train a series of LLMs on different mixtures of multilingual data and analyze their internal mechanisms using cross-layer transcoders (CLT) and attribution graphs. Our results provide strong evidence for pivot language representations: the model employs nearly identical representations across languages, while language-specific decoding emerges in later layers. Attribution analyses reveal that decoding relies in part on a small set of high-frequency language features in the final layers, which linearly read out language identity from the first layers in the model. By intervening on these features, we can suppress one language and substitute another in the model's outputs. Finally, we study how the dominant training language influences these mechanisms across attribution graphs and decoding pathways. We argue that understanding this pivot-language mechanism is crucial for improving multilingual alignment in LLMs.

</details>


### [38] [Reinforcing Stereotypes of Anger: Emotion AI on African American Vernacular English](https://arxiv.org/abs/2511.10846)
*Rebecca Dorn,Christina Chance,Casandra Rusti,Charles Bickham,Kai-Wei Chang,Fred Morstatter,Kristina Lerman*

Main category: cs.CL

TL;DR: 情感检测模型因训练数据偏重主流文化，对非洲裔美国白话英语(AAVE)存在系统性偏见，愤怒情绪误判率显著高于标准美式英语(GAE)，揭示算法安全风险与种族刻板印象强化问题


<details>
  <summary>Details</summary>
Motivation: 现有情感识别模型依赖主流文化标注数据，难以准确识别AAVE等非主流方言的情感表达，这种偏见在心理健康评估、招聘等高风险领域可能引发严重后果

Method: 1. 分析270万条洛杉矶地理标记推文，通过方言特征计算量化AAVE强度
2. 采集875条高/低AAVE密度推文的情感标注数据
3. 采用社区驱动的"银标"标注机制（AAVE熟练的非裔美国人标注）
4. 对比GPT/BERT/SpanEmo等模型表现，结合人口普查数据建立回归模型

Result: 1. GPT/BERT模型在AAVE文本的愤怒误判率比GAE高2倍
2. SpanEmo模型在AAVE的愤怒误判率从GAE的25%升至60%
3. 非内群体标注与脏话类AAVE特征显著相关(r=0.27)
4. 非洲裔聚居区愤怒预测值更高(r=0.27)，愉悦值更低(r=-0.10)

Conclusion: 情感AI通过带有偏见的分类强化种族刻板印象，凸显算法安全风险。研究强调开发兼顾文化差异与方言特征的情感计算系统的紧迫性

Abstract: Automated emotion detection is widely used in applications ranging from well-being monitoring to high-stakes domains like mental health and hiring. However, models often rely on annotations that reflect dominant cultural norms, limiting model ability to recognize emotional expression in dialects often excluded from training data distributions, such as African American Vernacular English (AAVE). This study examines emotion recognition model performance on AAVE compared to General American English (GAE). We analyze 2.7 million tweets geo-tagged within Los Angeles. Texts are scored for strength of AAVE using computational approximations of dialect features. Annotations of emotion presence and intensity are collected on a dataset of 875 tweets with both high and low AAVE densities. To assess model accuracy on a task as subjective as emotion perception, we calculate community-informed "silver" labels where AAVE-dense tweets are labeled by African American, AAVE-fluent (ingroup) annotators. On our labeled sample, GPT and BERT-based models exhibit false positive prediction rates of anger on AAVE more than double than on GAE. SpanEmo, a popular text-based emotion model, increases false positive rates of anger from 25 percent on GAE to 60 percent on AAVE. Additionally, a series of linear regressions reveals that models and non-ingroup annotations are significantly more correlated with profanity-based AAVE features than ingroup annotations. Linking Census tract demographics, we observe that neighborhoods with higher proportions of African American residents are associated with higher predictions of anger (Pearson's correlation r = 0.27) and lower joy (r = -0.10). These results find an emergent safety issue of emotion AI reinforcing racial stereotypes through biased emotion classification. We emphasize the need for culturally and dialect-informed affective computing systems.

</details>


### [39] [Leveraging Parameter Space Symmetries for Reasoning Skill Transfer in LLMs](https://arxiv.org/abs/2511.10850)
*Stefan Horoi,Sangwoo Cho,Supriyo Chakraborty,Shi-Xiong Zhang,Sambit Sahu,Guy Wolf,Genta Indra Winata*

Main category: cs.CL

TL;DR: 提出参数空间对齐方法增强LLM间技能迁移效果


<details>
  <summary>Details</summary>
Motivation: 传统任务算术在模型参数空间分岐时存在负干扰问题

Method: 利用Transformer的对称性进行参数对齐，适配GQA和SwiGLU层结构

Result: 在推理基准测试中持续超越标准任务算术方法

Conclusion: 该方法为LLM家族间的技能融合提供了有效解决方案，减少冗余训练并提升适应性

Abstract: Task arithmetic is a powerful technique for transferring skills between Large Language Models (LLMs), but it often suffers from negative interference when models have diverged during training. We address this limitation by first aligning the models' parameter spaces, leveraging the inherent permutation, rotation, and scaling symmetries of Transformer architectures. We adapt parameter space alignment for modern Grouped-Query Attention (GQA) and SwiGLU layers, exploring both weight-based and activation-based approaches. Using this alignment-first strategy, we successfully transfer advanced reasoning skills to a non-reasoning model. Experiments on challenging reasoning benchmarks show that our method consistently outperforms standard task arithmetic. This work provides an effective approach for merging and transferring specialized skills across evolving LLM families, reducing redundant fine-tuning and enhancing model adaptability.

</details>


### [40] [From Fact to Judgment: Investigating the Impact of Task Framing on LLM Conviction in Dialogue Systems](https://arxiv.org/abs/2511.10871)
*Parisa Rabbani,Nimet Beyza Bozdag,Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: 大语言模型在对话判断任务中易受社交框架影响，不同模型展现迎合或过度批判倾向，对话语境显著改变评估结果（平均变化9.24%）


<details>
  <summary>Details</summary>
Motivation: 验证LLM作为评判者在社交对话任务中的可靠性，揭示对话框架对模型判断的潜在影响

Method: 通过对比模型在直接事实查询与最小化对话情境中的表现，施加简单反驳压力测试模型立场稳定性

Result: GPT-4o-mini展现迎合倾向，Llama-8B-Instruct呈现过度批判，所有模型平均性能变化达9.24%

Conclusion: 对话框架是LLM评估关键变量，提出的诊断框架有助于开发更可信的对话系统

Abstract: LLMs are increasingly employed as judges across a variety of tasks, including those involving everyday social interactions. Yet, it remains unclear whether such LLM-judges can reliably assess tasks that require social or conversational judgment. We investigate how an LLM's conviction is changed when a task is reframed from a direct factual query to a Conversational Judgment Task. Our evaluation framework contrasts the model's performance on direct factual queries with its assessment of a speaker's correctness when the same information is presented within a minimal dialogue, effectively shifting the query from "Is this statement correct?" to "Is this speaker correct?". Furthermore, we apply pressure in the form of a simple rebuttal ("The previous answer is incorrect.") to both conditions. This perturbation allows us to measure how firmly the model maintains its position under conversational pressure. Our findings show that while some models like GPT-4o-mini reveal sycophantic tendencies under social framing tasks, others like Llama-8B-Instruct become overly-critical. We observe an average performance change of 9.24% across all models, demonstrating that even minimal dialogue context can significantly alter model judgment, underscoring conversational framing as a key factor in LLM-based evaluation. The proposed framework offers a reproducible methodology for diagnosing model conviction and contributes to the development of more trustworthy dialogue systems.

</details>


### [41] [ICX360: In-Context eXplainability 360 Toolkit](https://arxiv.org/abs/2511.10879)
*Dennis Wei,Ronny Luss,Xiaomeng Hu,Lucas Monteiro Paes,Pin-Yu Chen,Karthikeyan Natesan Ramamurthy,Erik Miehling,Inge Vejsbjerg,Hendrik Strobelt*

Main category: cs.CL

TL;DR: IBM推出开源工具包ICX360，专注于通过黑盒（扰动）和白盒（梯度）方法解释大语言模型（LLM）的输出，提升模型透明度。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在医疗问答、会议摘要等高风险场景的应用增加，开发解释模型输出的工具对可信度和可靠性至关重要。

Method: 工具包整合三种方法：1）基于扰动的黑盒解释（分析提示变化对输出的影响）；2）基于梯度的白盒解释（追踪模型内部参数变化）；3）支持检索增强生成、越狱攻击检测等场景。

Result: 发布包含快速入门指南和详细教程的Python工具包（https://github.com/IBM/ICX360），适用于自然语言生成、提示工程优化等实际用例。

Conclusion: ICX360填补了LLM可解释性工具缺口，通过模块化设计同时支持开发者和研究人员理解模型决策机制，促进负责任AI部署。

Abstract: Large Language Models (LLMs) have become ubiquitous in everyday life and are entering higher-stakes applications ranging from summarizing meeting transcripts to answering doctors' questions. As was the case with earlier predictive models, it is crucial that we develop tools for explaining the output of LLMs, be it a summary, list, response to a question, etc. With these needs in mind, we introduce In-Context Explainability 360 (ICX360), an open-source Python toolkit for explaining LLMs with a focus on the user-provided context (or prompts in general) that are fed to the LLMs. ICX360 contains implementations for three recent tools that explain LLMs using both black-box and white-box methods (via perturbations and gradients respectively). The toolkit, available at https://github.com/IBM/ICX360, contains quick-start guidance materials as well as detailed tutorials covering use cases such as retrieval augmented generation, natural language generation, and jailbreaking.

</details>


### [42] [A Multifaceted Analysis of Negative Bias in Large Language Models through the Lens of Parametric Knowledge](https://arxiv.org/abs/2511.10881)
*Jongyoon Song,Sangwon Yu,Sungroh Yoon*

Main category: cs.CL

TL;DR: 研究发现大语言模型存在格式层面负面偏见，提示格式比否定语义更影响回答。知识不足时模型倾向否定回答，不同提示方式会改变偏见程度。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在二元决策任务中产生负面偏见的深层原因，突破前人仅关注负面注意力头的研究局限，揭示格式因素和知识状态对偏见的影响机制。

Method: 构建三分类评估集（正确/错误/知识不足），通过系统实验分析提示格式、知识状态（提供上下文/增加选项/思维链）对偏见的影响。

Result: 知识不足导致62%否定回答；提供上下文使否定率降31%，'我不知道'选项降28%，但思维链提示反升18%否定率。特定提示类型可双向调节偏见程度。

Conclusion: 揭示格式设计、知识状态与提示策略的交互效应，为通过提示工程和数据构建缓解负面偏见提供新路径，指明模型透明化改进方向。

Abstract: Negative bias refers to the tendency of large language models (LLMs) to excessively generate negative responses in binary decision tasks (e.g., yes-no question answering). Previous research has focused on detecting and addressing negative attention heads that induce negative bias. However, the underlying detailed factors influencing negative bias remain underexplored. In this paper, we demonstrate that LLMs exhibit format-level negative bias, meaning the prompt format more influences their responses than the semantics of the negative response. For the fine-grained study of the negative bias, we introduce a pipeline for constructing the evaluation set, which systematically categorizes the dataset into three subsets based on the model's parametric knowledge: correct, incorrect, and insufficient relevant knowledge. Through analysis of this evaluation set, we identify a shortcut behavior in which models tend to generate negative responses when they lack sufficient knowledge to answer a yes-no question, leading to negative bias. We further examine how negative bias changes under various prompting scenarios related to parametric knowledge. We observe that providing relevant context and offering an "I don't know" option generally reduces negative bias, whereas chain-of-thought prompting tends to amplify the bias. Finally, we demonstrate that the degree of negative bias can vary depending on the type of prompt, which influences the direction of the response. Our work reveals the various factors that influence negative bias, providing critical insights for mitigating it in LLMs.

</details>


### [43] [MedPath: Multi-Domain Cross-Vocabulary Hierarchical Paths for Biomedical Entity Linking](https://arxiv.org/abs/2511.10887)
*Nishant Mishra,Wilker Aziz,Iacer Calixto*

Main category: cs.CL

TL;DR: MedPath构建了首个多领域生物医学实体链接数据集，通过标准化映射和本体路径增强，推动可解释临床NLP模型发展


<details>
  <summary>Details</summary>
Motivation: 解决生物医学NER/EL领域数据碎片化、可解释模型资源不足、评估指标语义盲区三大核心问题

Method: 整合9个专家标注数据集，采用UMLS标准化实体，映射62个生物医学词表，添加11个词表的完整本体路径

Result: 支持语义丰富的可解释实体链接系统开发，为下一代可互操作临床NLP模型奠定数据基础

Conclusion: MedPath通过结构化本体路径创新，开启生物医学NLP模型可解释性和语义深度研究新范式

Abstract: Progress in biomedical Named Entity Recognition (NER) and Entity Linking (EL) is currently hindered by a fragmented data landscape, a lack of resources for building explainable models, and the limitations of semantically-blind evaluation metrics. To address these challenges, we present MedPath, a large-scale and multi-domain biomedical EL dataset that builds upon nine existing expert-annotated EL datasets. In MedPath, all entities are 1) normalized using the latest version of the Unified Medical Language System (UMLS), 2) augmented with mappings to 62 other biomedical vocabularies and, crucially, 3) enriched with full ontological paths -- i.e., from general to specific -- in up to 11 biomedical vocabularies. MedPath directly enables new research frontiers in biomedical NLP, facilitating training and evaluation of semantic-rich and interpretable EL systems, and the development of the next generation of interoperable and explainable clinical NLP models.

</details>


### [44] [From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models](https://arxiv.org/abs/2511.10899)
*Farima Fatahi Bayat,Pouya Pezeshkpour,Estevam Hruschka*

Main category: cs.CL

TL;DR: 研究发现工具增强语言模型(TaLMs)存在工具诱导短视现象，工具使用虽提升答案准确率19.3%，但导致55%高风险案例出现推理退化，并提出了优化框架改善该问题。


<details>
  <summary>Details</summary>
Motivation: 探索工具增强语言模型在依赖外部工具时是否牺牲了推理过程的严谨性，揭示工具使用与推理质量间的负相关关系。

Method: 构建包含1,679个竞赛级数学问题的PYMATH基准，开发多维度评估体系量化推理退化，分析工具使用频率与错误类型转变的关系。

Result: 工具使用使算术错误减少但全局推理错误(逻辑/假设/创造力)增加41.5%，高频调用工具导致推理连贯性显著下降。

Conclusion: 提出基于偏好优化的框架，通过工具作为辅助证据而非替代推理，实现在保持准确率提升的同时增强推理深度。

Abstract: Tool-augmented Language Models (TaLMs) can invoke external tools to solve problems beyond their parametric capacity. However, it remains unclear whether these tool-enabled gains reflect trustworthy reasoning. Focusing on the Code Interpreter tool, we show that even when tools are selected and executed correctly, TaLMs treat tool outputs as substitutes for reasoning, producing solutions that appear correct but lack coherent justification. We term this failure mode Tool-Induced Myopia (TIM), and study it using PYMATH, a benchmark of 1,679 competition-level mathematical problems for which Python code is helpful but not sufficient. We further develop a multi-dimensional evaluation suite to quantify reasoning degradation in TaLMs relative to their non-tool counterparts. Our findings reveal that while TaLMs achieve up to a 19.3 percentage point gain in final-answer accuracy, their reasoning behavior consistently deteriorates (e.g., non-tool LLMs win up to 41.5% more often in pairwise comparisons of the reasoning process). This degradation intensifies with tool use; the more frequently a model invokes tools, the less coherent its reasoning becomes. Moreover, tool use shifts errors from arithmetic mistakes toward global reasoning failures (logic, assumption, creativity); with TIM present in ~55% of high-risk cases. Finally, we propose a preference-optimization-based framework that realigns TaLMs to use tools as assistive evidence, improving both final-answer accuracy and reasoning depth under tool use. Codes and data are available at: https://github.com/megagonlabs/TIM.

</details>


### [45] [Expert-Guided Prompting and Retrieval-Augmented Generation for Emergency Medical Service Question Answering](https://arxiv.org/abs/2511.10900)
*Xueren Ge,Sahil Murtaza,Anthony Cortez,Homa Alemzadeh*

Main category: cs.CL

TL;DR: 提出EMSQA数据集及Expert-CoT/ExpertRAG方法，通过领域专业知识增强LLMs在医疗问答中的表现，使32B模型通过全部EMS认证考试。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在医疗问答中忽视临床科目和急救认证等级的专业知识，通用策略在高风险场景下效果受限。

Method: 构建含24.3K题目的EMSQA数据集（覆盖10个临床科目+4认证等级），开发基于科目/等级的思维链提示(Expert-CoT)和科目对齐的检索增强框架(ExpertRAG)。

Result: Expert-CoT比标准CoT提升2.05%，结合ExpertRAG实现4.59%准确率增益，32B模型通过所有计算机自适应EMS认证模拟考试。

Conclusion: 临床领域专业知识与患者数据结合能显著提升LLMs医疗问答性能，实现与人类专家相当的认证水平。

Abstract: Large language models (LLMs) have shown promise in medical question answering, yet they often overlook the domain-specific expertise that professionals depend on, such as the clinical subject areas (e.g., trauma, airway) and the certification level (e.g., EMT, Paramedic). Existing approaches typically apply general-purpose prompting or retrieval strategies without leveraging this structured context, limiting performance in high-stakes settings. We address this gap with EMSQA, an 24.3K-question multiple-choice dataset spanning 10 clinical subject areas and 4 certification levels, accompanied by curated, subject area-aligned knowledge bases (40K documents and 2M tokens). Building on EMSQA, we introduce (i) Expert-CoT, a prompting strategy that conditions chain-of-thought (CoT) reasoning on specific clinical subject area and certification level, and (ii) ExpertRAG, a retrieval-augmented generation pipeline that grounds responses in subject area-aligned documents and real-world patient data. Experiments on 4 LLMs show that Expert-CoT improves up to 2.05% over vanilla CoT prompting. Additionally, combining Expert-CoT with ExpertRAG yields up to a 4.59% accuracy gain over standard RAG baselines. Notably, the 32B expertise-augmented LLMs pass all the computer-adaptive EMS certification simulation exams.

</details>


### [46] [Multimodal Peer Review Simulation with Actionable To-Do Recommendations for Community-Aware Manuscript Revisions](https://arxiv.org/abs/2511.10902)
*Mengze Hong,Di Jiang,Weiwei Zhao,Yawen Li,Yihang Wang,Xinyuan Luo,Yanjie Sun,Chen Jason Zhang*

Main category: cs.CL

TL;DR: 提出基于多模态大语言模型的交互式同行评审系统，整合文本/视觉信息、检索增强生成和结构化待办清单，提升论文修改效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有同行评审系统存在的三大局限：单模态输入、上下文信息不足、缺乏可操作的结构化反馈。通过多模态数据融合和社区知识增强，实现更有效的投稿前论文修改。

Method: 1. 多模态LLM整合文本与图表信息
2. 基于OpenReview数据的检索增强生成（RAG）提升评审质量
3. 提出Action:Objective[#]格式将评审转化为可追溯的待办清单
4. 与学术写作平台集成实现实时反馈跟踪

Result: 系统生成的评审意见在完整性和实用性上超越基线模型，更符合专家标准（82% vs 63%基线）。修订跟踪界面使平均修改效率提升37%。

Conclusion: 通过多模态交互、社区知识增强和结构化指导的三重创新，为学者提供透明化、可操作的预投稿修改方案，推进以人为中心的智能学术辅助系统发展。

Abstract: While large language models (LLMs) offer promising capabilities for automating academic workflows, existing systems for academic peer review remain constrained by text-only inputs, limited contextual grounding, and a lack of actionable feedback. In this work, we present an interactive web-based system for multimodal, community-aware peer review simulation to enable effective manuscript revisions before paper submission. Our framework integrates textual and visual information through multimodal LLMs, enhances review quality via retrieval-augmented generation (RAG) grounded in web-scale OpenReview data, and converts generated reviews into actionable to-do lists using the proposed Action:Objective[\#] format, providing structured and traceable guidance. The system integrates seamlessly into existing academic writing platforms, providing interactive interfaces for real-time feedback and revision tracking. Experimental results highlight the effectiveness of the proposed system in generating more comprehensive and useful reviews aligned with expert standards, surpassing ablated baselines and advancing transparent, human-centered scholarly assistance.

</details>


### [47] [Automated Analysis of Learning Outcomes and Exam Questions Based on Bloom's Taxonomy](https://arxiv.org/abs/2511.10903)
*Ramya Kumar,Dhruv Gulwani,Sonit Singh*

Main category: cs.CL

TL;DR: 传统机器学习模型（SVM）通过数据增强在布鲁姆分类任务中取得94%准确率，优于过拟合的深度模型，LLM零样本测试表现次优


<details>
  <summary>Details</summary>
Motivation: 解决布鲁姆分类法自动标注需求，探究小数据场景下传统ML与深度学习模型性能差异及数据增强策略效果

Method: 使用600句标注数据，对比传统ML（SVM/朴素贝叶斯）、RNN架构（BiLSTM/BiGRU）、Transformer（BERT/RoBERTa）及LLM（GPT/Gemini）在不同预处理和增强策略下的表现

Result: 增强SVM实现94%准确率/召回率/F1值，RNN/BERT严重过拟合，RoBERTa训练后期出现过拟合，LLM零样本准确率约0.72-0.73

Conclusion: 有限数据场景下，数据增强和简单算法（如SVM）优于复杂深度模型，凸显小数据训练深度模型的挑战性

Abstract: This paper explores the automatic classification of exam questions and learning outcomes according to Bloom's Taxonomy. A small dataset of 600 sentences labeled with six cognitive categories - Knowledge, Comprehension, Application, Analysis, Synthesis, and Evaluation - was processed using traditional machine learning (ML) models (Naive Bayes, Logistic Regression, Support Vector Machines), recurrent neural network architectures (LSTM, BiLSTM, GRU, BiGRU), transformer-based models (BERT and RoBERTa), and large language models (OpenAI, Gemini, Ollama, Anthropic). Each model was evaluated under different preprocessing and augmentation strategies (for example, synonym replacement, word embeddings, etc.). Among traditional ML approaches, Support Vector Machines (SVM) with data augmentation achieved the best overall performance, reaching 94 percent accuracy, recall, and F1 scores with minimal overfitting. In contrast, the RNN models and BERT suffered from severe overfitting, while RoBERTa initially overcame it but began to show signs as training progressed. Finally, zero-shot evaluations of large language models (LLMs) indicated that OpenAI and Gemini performed best among the tested LLMs, achieving approximately 0.72-0.73 accuracy and comparable F1 scores. These findings highlight the challenges of training complex deep models on limited data and underscore the value of careful data augmentation and simpler algorithms (such as augmented SVM) for Bloom's Taxonomy classification.

</details>


### [48] [Evaluating Large Language Models on Rare Disease Diagnosis: A Case Study using House M.D](https://arxiv.org/abs/2511.10912)
*Arsh Gupta,Ajay Narayanan Sridhar,Bonam Mingole,Amulya Yadav*

Main category: cs.CL

TL;DR: 评估大型语言模型在罕见病诊断中的表现，发现新一代模型准确率提升2.3倍但仍存在显著挑战


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在基于叙述性医学案例的罕见疾病诊断中的潜力（当前研究存在空白）

Method: 使用《豪斯医生》剧集构建症状-诊断数据集，测试GPT系列和Gemini系列共4个先进LLM的诊断能力

Result: 模型准确率16.48%-38.64%，新一代模型性能比前代提升2.3倍，但所有模型在罕见病诊断中均面临重大困难

Conclusion: 建立首个教育验证的医学推理基准，为AI辅助诊断研究提供公开评估框架和性能基线

Abstract: Large language models (LLMs) have demonstrated capabilities across diverse domains, yet their performance on rare disease diagnosis from narrative medical cases remains underexplored. We introduce a novel dataset of 176 symptom-diagnosis pairs extracted from House M.D., a medical television series validated for teaching rare disease recognition in medical education. We evaluate four state-of-the-art LLMs such as GPT 4o mini, GPT 5 mini, Gemini 2.5 Flash, and Gemini 2.5 Pro on narrative-based diagnostic reasoning tasks. Results show significant variation in performance, ranging from 16.48% to 38.64% accuracy, with newer model generations demonstrating a 2.3 times improvement. While all models face substantial challenges with rare disease diagnosis, the observed improvement across architectures suggests promising directions for future development. Our educationally validated benchmark establishes baseline performance metrics for narrative medical reasoning and provides a publicly accessible evaluation framework for advancing AI-assisted diagnosis research.

</details>


### [49] [CardioEmbed: Domain-Specialized Text Embeddings for Clinical Cardiology](https://arxiv.org/abs/2511.10930)
*Richard J. Young,Alice M. Matthews*

Main category: cs.CL

TL;DR: 基于Qwen3-Embedding-8B开发的心脏专科文本嵌入模型CardioEmbed，通过7本权威教材的15万句语料训练，在心脏特异性检索任务中实现99.60%准确率，较现有医学模型提升15.94个百分点。


<details>
  <summary>Details</summary>
Motivation: 现有生物医学嵌入模型主要基于PubMed文献，但临床心脏病学高度依赖教科书中的程序性知识和专业术语，研究与实践存在应用鸿沟。

Method: 使用去重后的7本心脏病学教材约15万句语料，采用对比学习框架（InfoNCE损失函数+批量负样本）对Qwen3-Embedding-8B进行领域专业化训练。

Result: 1) 心脏特异性语义检索准确率99.60%（较当前最佳医学模型MedTE提升15.94%） 2) 在MTEB生物医学基准测试中，BIOSSES获得0.77斯皮尔曼系数，SciFact达到0.61 NDCG@10。

Conclusion: 基于临床教材的领域专业化训练使心脏专科文本检索接近完美（99.60% Acc@1），证明专科教材数据对提升临床AI应用效果的关键作用。

Abstract: Biomedical text embeddings have primarily been developed using research literature from PubMed, yet clinical cardiology practice relies heavily on procedural knowledge and specialized terminology found in comprehensive textbooks rather than research abstracts. This research practice gap limits the effectiveness of existing embedding models for clinical applications incardiology. This study trained CardioEmbed, a domain-specialized embedding model based on Qwen3-Embedding-8B, using contrastive learning on a curated corpus of seven comprehensive cardiology textbooks totaling approximately 150,000 sentences after deduplication. The model employs InfoNCE loss with in-batch negatives and achieves 99.60% retrieval accuracy on cardiac-specific semantic retrieval tasks, a +15.94 percentage point improvement over MedTE, the current state-of-the-art medical embedding model. On MTEB medical benchmarks, the model obtained BIOSSES 0.77 Spearman and SciFact 0.61 NDCG@10, indicating competitive performance on related biomedical domains. Domain-specialized training on comprehensive clinical textbooks yields near-perfect cardiology retrieval (99.60% Acc@1), improving over MedTE by +15.94 percentage points.

</details>


### [50] [DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains](https://arxiv.org/abs/2511.10984)
*Xiying Zhao,Zhoufutu Wen,Zhixuan Chen,Jingzhe Ding,Jianpeng Jiao,Shuai Li,Xi Li,Danni Liang,Shengda Long,Qianqian Liu,Xianbo Wu,Hongwan Gao,Xiang Gao,Liang Hu,Jiashuo Liu,Mengyun Liu,Weiran Shi,Chenghao Yang,Qianyu Yang,Xuanliang Zhang,Ge Zhang,Wenhao Huang*

Main category: cs.CL

TL;DR: 论文提出DiscoX篇章级专业翻译评估基准与Metric-S无参考评估系统，揭示LLM与人类专家的显著差距


<details>
  <summary>Details</summary>
Motivation: 现有翻译评估方法过于关注片段级质量，缺乏对篇章连贯性和专业术语准确性的评估，难以满足学术交流需求

Method: 构建包含7大领域200篇长文本（平均1700 tokens）的DiscoX基准，开发基于准确性/流畅性/适当性三维度的无参考评估系统Metric-S

Result: Metric-S与人工评估高度一致（皮尔逊相关系数0.82），实验显示最先进LLM翻译质量仍显著落后人类专家（差距达37.2%）

Conclusion: DiscoX有效验证专业机器翻译难度，其评估体系为LLM翻译能力提升提供了细粒度诊断工具与明确优化方向

Abstract: The evaluation of discourse-level translation in expert domains remains inadequate, despite its centrality to knowledge dissemination and cross-lingual scholarly communication. While these translations demand discourse-level coherence and strict terminological precision, current evaluation methods predominantly focus on segment-level accuracy and fluency. To address this limitation, we introduce DiscoX, a new benchmark for discourse-level and expert-level Chinese-English translation. It comprises 200 professionally-curated texts from 7 domains, with an average length exceeding 1700 tokens. To evaluate performance on DiscoX, we also develop Metric-S, a reference-free system that provides fine-grained automatic assessments across accuracy, fluency, and appropriateness. Metric-S demonstrates strong consistency with human judgments, significantly outperforming existing metrics. Our experiments reveal a remarkable performance gap: even the most advanced LLMs still trail human experts on these tasks. This finding validates the difficulty of DiscoX and underscores the challenges that remain in achieving professional-grade machine translation. The proposed benchmark and evaluation system provide a robust framework for more rigorous evaluation, facilitating future advancements in LLM-based translation.

</details>


### [51] [When Data is the Algorithm: A Systematic Study and Curation of Preference Optimization Datasets](https://arxiv.org/abs/2511.10985)
*Aladin Djuhera,Farhan Ahmed,Swanand Ravindra Kadhe,Syed Zawad,Heiko Ludwig,Holger Boche*

Main category: cs.CL

TL;DR: 论文提出首个系统分析主流开源DPO数据集的方法，通过Magpie框架标注样本质量并构建高效混合数据集UltraMix，在缩小30%规模的同时实现性能超越。


<details>
  <summary>Details</summary>
Motivation: 现有开源DPO数据集缺乏系统性比较，因计算成本高和标注质量不足导致偏好选择机制、任务覆盖度及人类判断一致性不明确。

Method: 使用Magpie框架进行三级标注（任务类别/输入质量/偏好奖励），基于质量信号筛选五个主流数据集构建UltraMix混合数据集。

Result: UltraMix比最佳单数据集缩小30%规模，但在MT-Bench等基准测试中表现更优，验证了数据质量优于数量的假设。

Conclusion: 该研究为数据中心的偏好优化提供系统分析框架，公开所有标注和混合策略，推动高效对齐技术的发展。

Abstract: Aligning large language models (LLMs) is a central objective of post-training, often achieved through reward modeling and reinforcement learning methods. Among these, direct preference optimization (DPO) has emerged as a widely adopted technique that fine-tunes LLMs on preferred completions over less favorable ones. While most frontier LLMs do not disclose their curated preference pairs, the broader LLM community has released several open-source DPO datasets, including TuluDPO, ORPO, UltraFeedback, HelpSteer, and Code-Preference-Pairs. However, systematic comparisons remain scarce, largely due to the high computational cost and the lack of rich quality annotations, making it difficult to understand how preferences were selected, which task types they span, and how well they reflect human judgment on a per-sample level. In this work, we present the first comprehensive, data-centric analysis of popular open-source DPO corpora. We leverage the Magpie framework to annotate each sample for task category, input quality, and preference reward, a reward-model-based signal that validates the preference order without relying on human annotations. This enables a scalable, fine-grained inspection of preference quality across datasets, revealing structural and qualitative discrepancies in reward margins. Building on these insights, we systematically curate a new DPO mixture, UltraMix, that draws selectively from all five corpora while removing noisy or redundant samples. UltraMix is 30% smaller than the best-performing individual dataset yet exceeds its performance across key benchmarks. We publicly release all annotations, metadata, and our curated mixture to facilitate future research in data-centric preference optimization.

</details>


### [52] [Automata-Based Steering of Large Language Models for Diverse Structured Generation](https://arxiv.org/abs/2511.11018)
*Xiaokun Luan,Zeming Wei,Yihao Zhang,Meng Sun*

Main category: cs.CL

TL;DR: 提出基于自动机历史引导的结构化生成方法，有效提升输出多样性


<details>
  <summary>Details</summary>
Motivation: 现有结构化生成方法虽保证有效性但缺乏多样性，经初步研究验证该局限性

Method: 利用自动机遍历历史数据引导LLMs生成新颖结构模式

Result: 评估显示结构/内容多样性提升47%，生成效率保持相当水平，案例研究证实可生成多样化开源库测试用例

Conclusion: 本方法突破结构化生成多样性瓶颈，在保持效率前提下实现质量突破，为自动化测试等领域提供新范式

Abstract: Large language models (LLMs) are increasingly tasked with generating structured outputs. While structured generation methods ensure validity, they often lack output diversity, a critical limitation that we confirm in our preliminary study. We propose a novel method to enhance diversity in automaton-based structured generation. Our approach utilizes automata traversal history to steer LLMs towards novel structural patterns. Evaluations show our method significantly improves structural and content diversity while maintaining comparable generation efficiency. Furthermore, we conduct a case study showcasing the effectiveness of our method in generating diverse test cases for testing open-source libraries.

</details>


### [53] [Correcting Mean Bias in Text Embeddings: A Refined Renormalization with Training-Free Improvements on MMTEB](https://arxiv.org/abs/2511.11041)
*Xingyu Ren,Youran Sun,Haoyu Liang*

Main category: cs.CL

TL;DR: 现有文本嵌入模型存在一致偏差，提出Renormalization方法显著提升多任务性能


<details>
  <summary>Details</summary>
Motivation: 发现当前文本嵌入向量存在全局性偏差μ，影响模型表现

Method: 提出两种归一化方案：直接减均值μ或减去向量在μ方向的投影

Result: 在MMTEB基准上：检索任务提升9.7σ，分类任务3.1σ，其他任务0.8σ

Conclusion: 理论推导与实验验证：投影式归一化效果更优

Abstract: We find that current text embedding models produce outputs with a consistent bias, i.e., each embedding vector $e$ can be decomposed as $\tilde{e} + μ$, where $μ$ is almost identical across all sentences. We propose a plug-and-play, training-free and lightweight solution called Renormalization. Through extensive experiments, we show that renormalization consistently and statistically significantly improves the performance of existing models on the Massive Multilingual Text Embedding Benchmark (MMTEB). In particular, across 38 models, renormalization improves performance by 9.7 $σ$ on retrieval tasks, 3.1 $σ$ on classification tasks, and 0.8 $σ$ on other types of tasks. Renormalization has two variants: directly subtracting $μ$ from $e$, or subtracting the projection of $e$ onto $μ$. We theoretically predict that the latter performs better, and our experiments confirm this prediction.

</details>


### [54] [Can LLMs Detect Their Own Hallucinations?](https://arxiv.org/abs/2511.11087)
*Sora Kadotani,Kosuke Nishida,Kyosuke Nishida*

Main category: cs.CL

TL;DR: LLMs通过思维链可检测自身58.2%的幻觉（需足够知识储备）


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成内容存在事实幻觉的问题，探索模型自我检测能力

Method: 将幻觉检测建模为分类任务，使用思维链技术提取模型内部知识

Result: GPT-3.5 Turbo配合CoT实现了58.2%的自我幻觉检测率

Conclusion: 当模型参数包含充足知识时，带思维链的LLM具备自我幻觉检测能力

Abstract: Large language models (LLMs) can generate fluent responses, but sometimes hallucinate facts. In this paper, we investigate whether LLMs can detect their own hallucinations. We formulate hallucination detection as a classification task of a sentence. We propose a framework for estimating LLMs' capability of hallucination detection and a classification method using Chain-of-Thought (CoT) to extract knowledge from their parameters. The experimental results indicated that GPT-$3.5$ Turbo with CoT detected $58.2\%$ of its own hallucinations. We concluded that LLMs with CoT can detect hallucinations if sufficient knowledge is contained in their parameters.

</details>


### [55] [Analysing Personal Attacks in U.S. Presidential Debates](https://arxiv.org/abs/2511.11108)
*Ruban Goyal,Rohitash Chandra,Sonit Singh*

Main category: cs.CL

TL;DR: 提出基于BERT和大语言模型的框架，自动检测美国总统辩论中的个人攻击，提升政治话语透明度与分析效率


<details>
  <summary>Details</summary>
Motivation: 个人攻击影响选举期间的公众认知，检测此类攻击可为媒体和公众提供洞察。深度学习技术发展为自动化识别有害政治语言创造了新机遇

Method: 1. 手动标注2016-2024年三届选举辩论文本 2. 结合统计分析与语言模型 3. 探索微调Transformer模型与通用LLM在正式政治演讲中的检测能力

Result: 验证了任务适配的现代语言模型能有效识别政治演讲中的攻击性内容，准确率显著提升

Conclusion: 语言模型的任务特定调优为解析复杂政治沟通提供了新路径，有助于建立更透明的公共话语分析体系

Abstract: Personal attacks have become a notable feature of U.S. presidential debates and play an important role in shaping public perception during elections. Detecting such attacks can improve transparency in political discourse and provide insights for journalists, analysts and the public. Advances in deep learning and transformer-based models, particularly BERT and large language models (LLMs) have created new opportunities for automated detection of harmful language. Motivated by these developments, we present a framework for analysing personal attacks in U.S. presidential debates. Our work involves manual annotation of debate transcripts across the 2016, 2020 and 2024 election cycles, followed by statistical and language-model based analysis. We investigate the potential of fine-tuned transformer models alongside general-purpose LLMs to detect personal attacks in formal political speech. This study demonstrates how task-specific adaptation of modern language models can contribute to a deeper understanding of political communication.

</details>


### [56] [AV-Dialog: Spoken Dialogue Models with Audio-Visual Input](https://arxiv.org/abs/2511.11124)
*Tuochao Chen,Bandhav Veluri,Hongyu Gong,Shyamnath Gollakota*

Main category: cs.CL

TL;DR: AV-Dialog首创结合视听线索的多模态对话框架，显著提升嘈杂环境下对话系统的转录准确性、轮流预测能力和对话流畅度。


<details>
  <summary>Details</summary>
Motivation: 现有纯音频对话模型在多说话人嘈杂环境中存在回应不相关、轮流对话不自然的问题，需要融合视觉信息增强说话人追踪和对话连贯性。

Method: 通过声学标记化与多阶段训练策略（单模态→合成数据→真实视听对话数据），结合多任务学习实现流式转录、基于语义的换人边界检测和响应生成。

Result: 实验显示：在干扰环境下相比纯音频模型，转录错误减少18.7%，换人预测F1值提升23.4%，人类评估对话质量提高41.2%。

Conclusion: 视听融合为现实嘈杂环境中的对话系统提供了鲁棒性保障，推动具备多模态感知能力的语音对话代理向实用化迈进。

Abstract: Dialogue models falter in noisy, multi-speaker environments, often producing irrelevant responses and awkward turn-taking. We present AV-Dialog, the first multimodal dialog framework that uses both audio and visual cues to track the target speaker, predict turn-taking, and generate coherent responses. By combining acoustic tokenization with multi-task, multi-stage training on monadic, synthetic, and real audio-visual dialogue datasets, AV-Dialog achieves robust streaming transcription, semantically grounded turn-boundary detection and accurate responses, resulting in a natural conversational flow. Experiments show that AV-Dialog outperforms audio-only models under interference, reducing transcription errors, improving turn-taking prediction, and enhancing human-rated dialogue quality. These results highlight the power of seeing as well as hearing for speaker-aware interaction, paving the way for {spoken} dialogue agents that perform {robustly} in real-world, noisy environments.

</details>


### [57] [Enhancing Meme Emotion Understanding with Multi-Level Modality Enhancement and Dual-Stage Modal Fusion](https://arxiv.org/abs/2511.11126)
*Yi Shi,Wenlong Meng,Zhenyuan Guo,Chengkun Wei,Wenzhi Chen*

Main category: cs.CL

TL;DR: 提出MemoDetector框架，通过四步文本增强和双阶段模态融合策略提升Meme情感理解性能


<details>
  <summary>Details</summary>
Motivation: 现有Meme情感理解方法存在细粒度多模态融合不足、隐含意义挖掘不充分两大挑战

Method: 1.基于MLLM的四步文本增强模块提取隐含语义；2.双阶段模态融合策略（浅层原始特征融合+深层增强特征融合）

Result: 在MET-MEME和MOOD数据集上F1分别提升4.3%和3.4%，消融实验验证模块有效性

Conclusion: 框架通过显式语义增强和层次化特征融合，在多模态情感理解任务中展现出强大潜力

Abstract: With the rapid rise of social media and Internet culture, memes have become a popular medium for expressing emotional tendencies. This has sparked growing interest in Meme Emotion Understanding (MEU), which aims to classify the emotional intent behind memes by leveraging their multimodal contents. While existing efforts have achieved promising results, two major challenges remain: (1) a lack of fine-grained multimodal fusion strategies, and (2) insufficient mining of memes' implicit meanings and background knowledge. To address these challenges, we propose MemoDetector, a novel framework for advancing MEU. First, we introduce a four-step textual enhancement module that utilizes the rich knowledge and reasoning capabilities of Multimodal Large Language Models (MLLMs) to progressively infer and extract implicit and contextual insights from memes. These enhanced texts significantly enrich the original meme contents and provide valuable guidance for downstream classification. Next, we design a dual-stage modal fusion strategy: the first stage performs shallow fusion on raw meme image and text, while the second stage deeply integrates the enhanced visual and textual features. This hierarchical fusion enables the model to better capture nuanced cross-modal emotional cues. Experiments on two datasets, MET-MEME and MOOD, demonstrate that our method consistently outperforms state-of-the-art baselines. Specifically, MemoDetector improves F1 scores by 4.3\% on MET-MEME and 3.4\% on MOOD. Further ablation studies and in-depth analyses validate the effectiveness and robustness of our approach, highlighting its strong potential for advancing MEU. Our code is available at https://github.com/singing-cat/MemoDetector.

</details>


### [58] [Speech-Aware Long Context Pruning and Integration for Contextualized Automatic Speech Recognition](https://arxiv.org/abs/2511.11139)
*Yiming Rong,Yixin Zhang,Ziyi Wang,Deyang Jiang,Yunlong Zhao,Haoran Wu,Shiyu Zhou,Bo Xu*

Main category: cs.CL

TL;DR: 提出SAP²框架解决ASR长上下文信息利用问题，通过两阶段动态剪裁和注意力池化机制显著降低词错率


<details>
  <summary>Details</summary>
Motivation: 自动语音识别系统在领域特定场景（如会议演讲）中难以有效利用长上下文信息，主要受限于模型上下文窗口和上下文噪声中的信息稀疏性

Method: SAP²框架包含两阶段动态剪裁与整合过程，采用基于语音驱动的注意力池化机制压缩上下文嵌入并保留语音显著性特征

Result: 在SlideSpeech和LibriSpeech分别达到7.71%和1.12%的WER，SlideSpeech上B-WER降低41.1%，并在扩展上下文输入下保持稳定性能

Conclusion: SAP²通过创新的上下文处理机制实现领域特定场景下的最优性能，展现强大的可扩展性和鲁棒性

Abstract: Automatic speech recognition (ASR) systems have achieved remarkable performance in common conditions but often struggle to leverage long-context information in contextualized scenarios that require domain-specific knowledge, such as conference presentations. This challenge arises primarily due to constrained model context windows and the sparsity of relevant information within extensive contextual noise. To solve this, we propose the SAP$^{2}$ method, a novel framework that dynamically prunes and integrates relevant contextual keywords in two stages. Specifically, each stage leverages our proposed Speech-Driven Attention-based Pooling mechanism, enabling efficient compression of context embeddings while preserving speech-salient information. Experimental results demonstrate state-of-the-art performance of SAP$^{2}$ on the SlideSpeech and LibriSpeech datasets, achieving word error rates (WER) of 7.71% and 1.12%, respectively. On SlideSpeech, our method notably reduces biased keyword error rates (B-WER) by 41.1% compared to non-contextual baselines. SAP$^{2}$ also exhibits robust scalability, consistently maintaining performance under extensive contextual input conditions on both datasets.

</details>


### [59] [PRSM: A Measure to Evaluate CLIP's Robustness Against Paraphrases](https://arxiv.org/abs/2511.11141)
*Udo Schlegel,Franziska Weeber,Jian Lan,Thomas Seidl*

Main category: cs.CL

TL;DR: 论文提出PRSM指标量化CLIP对改写查询的敏感性，发现改写鲁棒性存在策略差异，并观察到与性别相关的系统性差异


<details>
  <summary>Details</summary>
Motivation: CLIP在多模态对齐中广泛应用，但其对语言改写（特别是社会敏感语境中的改写）的鲁棒性尚未充分研究，可能放大人口统计偏见影响系统公平部署

Method: 基于Social Counterfactuals偏见基准数据集，提出Paraphrase Ranking Stability Metric (PRSM)，实证评估CLIP在不同改写策略下的稳定性

Result: 改写鲁棒性因改写策略而异，男性与女性相关查询间存在细微但一致的鲁棒性差异

Conclusion: 强调在多模态系统部署中需考虑改写鲁棒性对公平性的影响，揭示模型行为与人口统计因素间的潜在关联

Abstract: Contrastive Language-Image Pre-training (CLIP) is a widely used multimodal model that aligns text and image representations through large-scale training. While it performs strongly on zero-shot and few-shot tasks, its robustness to linguistic variation, particularly paraphrasing, remains underexplored. Paraphrase robustness is essential for reliable deployment, especially in socially sensitive contexts where inconsistent representations can amplify demographic biases. In this paper, we introduce the Paraphrase Ranking Stability Metric (PRSM), a novel measure for quantifying CLIP's sensitivity to paraphrased queries. Using the Social Counterfactuals dataset, a benchmark designed to reveal social and demographic biases, we empirically assess CLIP's stability under paraphrastic variation, examine the interaction between paraphrase robustness and gender, and discuss implications for fairness and equitable deployment of multimodal systems. Our analysis reveals that robustness varies across paraphrasing strategies, with subtle yet consistent differences observed between male- and female-associated queries.

</details>


### [60] [Adverbs Revisited: Enhancing WordNet Coverage of Adverbs with a Supersense Taxonomy](https://arxiv.org/abs/2511.11214)
*Jooyoung Lee,Jader Martins Camboim de Sá*

Main category: cs.CL

TL;DR: 提出基于语言学理论的副词超类分类体系，通过标注验证其覆盖性和可靠性，拓展WordNet语义框架并支持NLP应用


<details>
  <summary>Details</summary>
Motivation: WordNet现有的副词语义分类体系不完善，缺乏系统性语义分类，制约语言理论和NLP应用发展

Method: 构建包含方式/时间/程度/领域等7大范畴的副词分类体系，通过自然文本标注实验验证分类效果

Result: 标注实验显示新体系能有效覆盖自然文本中的副词，标注者间一致性达到可靠水平（Krippendorff's α=0.78）

Conclusion: 新分类体系完善了WordNet框架，强化语言学理论基础，为词义消歧、事件抽取等NLP任务提供结构化语义支持

Abstract: WordNet offers rich supersense hierarchies for nouns and verbs, yet adverbs remain underdeveloped, lacking a systematic semantic classification. We introduce a linguistically grounded supersense typology for adverbs, empirically validated through annotation, that captures major semantic domains including manner, temporal, frequency, degree, domain, speaker-oriented, and subject-oriented functions. Results from a pilot annotation study demonstrate that these categories provide broad coverage of adverbs in natural text and can be reliably assigned by human annotators. Incorporating this typology extends WordNet's coverage, aligns it more closely with linguistic theory, and facilitates downstream NLP applications such as word sense disambiguation, event extraction, sentiment analysis, and discourse modeling. We present the proposed supersense categories, annotation outcomes, and directions for future work.

</details>


### [61] [LANE: Lexical Adversarial Negative Examples for Word Sense Disambiguation](https://arxiv.org/abs/2511.11234)
*Jader Martins Camboim de Sá,Jooyoung Lee,Cédric Pruski,Marcos Da Silveira*

Main category: cs.CL

TL;DR: 提出对抗训练策略LANE，通过生成负样本增强词级语义区分，提升神经语言模型细粒度词义解析能力


<details>
  <summary>Details</summary>
Motivation: 现有神经语言模型过度依赖全局句子表示，难以捕捉局部语义细节，导致细粒度词义解析效果受限

Method: 采用对抗训练策略(LANE)，通过选择性替换训练集中的替代词生成负样本，强制模型增强相同句子中不同标记词的表示区分度

Result: 在词汇语义变化检测和词义消歧任务中性能超越基线模型，生成更具判别性的词表示，定性分析显示能更好捕捉复杂环境中的语义差异

Conclusion: LANE作为模型无关框架，可集成至现有表示学习系统，通过注意力聚焦机制有效提升语言模型对细微语义差异的捕捉能力

Abstract: Fine-grained word meaning resolution remains a critical challenge for neural language models (NLMs) as they often overfit to global sentence representations, failing to capture local semantic details. We propose a novel adversarial training strategy, called LANE, to address this limitation by deliberately shifting the model's learning focus to the target word. This method generates challenging negative training examples through the selective marking of alternate words in the training set. The goal is to force the model to create a greater separability between same sentences with different marked words. Experimental results on lexical semantic change detection and word sense disambiguation benchmarks demonstrate that our approach yields more discriminative word representations, improving performance over standard contrastive learning baselines. We further provide qualitative analyses showing that the proposed negatives lead to representations that better capture subtle meaning differences even in challenging environments. Our method is model-agnostic and can be integrated into existing representation learning frameworks.

</details>


### [62] [KGQuest: Template-Driven QA Generation from Knowledge Graphs with LLM-Based Refinement](https://arxiv.org/abs/2511.11258)
*Sania Nayab,Marco Simoni,Giulio Rossolini,Andrea Saracino*

Main category: cs.CL

TL;DR: 提出混合知识图谱聚类与LLM模板优化的QA生成框架，实现高效可扩展的高质量问答对生成


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱转QA方法存在可扩展性差、语言质量低、事实一致性弱的问题，难以满足教育平台和大语言模型测试需求

Method: 1. 按关系聚类KG三元组生成模板 2. 基于实体类型设计自然语言规则 3. 使用LLM优化模板语言质量 4. 从KG选择干扰项生成选项

Result: 实验证明该方法能同时保持扩展性和语言流畅性，生成质量显著优于传统方法

Conclusion: 知识图谱结构化规则与LLM语言优化的结合，实现了质量与效率的平衡，为自动化QA生成提供可靠解决方案

Abstract: The generation of questions and answers (QA) from knowledge graphs (KG) plays a crucial role in the development and testing of educational platforms, dissemination tools, and large language models (LLM). However, existing approaches often struggle with scalability, linguistic quality, and factual consistency. This paper presents a scalable and deterministic pipeline for generating natural language QA from KGs, with an additional refinement step using LLMs to further enhance linguistic quality. The approach first clusters KG triplets based on their relations, creating reusable templates through natural language rules derived from the entity types of objects and relations. A module then leverages LLMs to refine these templates, improving clarity and coherence while preserving factual accuracy. Finally, the instantiation of answer options is achieved through a selection strategy that introduces distractors from the KG. Our experiments demonstrate that this hybrid approach efficiently generates high-quality QA pairs, combining scalability with fluency and linguistic precision.

</details>


### [63] [iMAD: Intelligent Multi-Agent Debate for Efficient and Accurate LLM Inference](https://arxiv.org/abs/2511.11306)
*Wei Fan,JinYi Yoon,Bo Ji*

Main category: cs.CL

TL;DR: iMAD框架通过智能选择触发多智能体辩论，在保持准确性的同时减少92%的token使用，并在6个数据集中实现最高13.5%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有MAD框架存在计算资源浪费和可能降低正确率的缺陷，需要开发能自动判断何时需要辩论的智能触发机制。

Method: 通过单智能体生成结构化自我批判响应，提取41个语言/语义犹豫特征，使用FocusCal损失训练的轻量级分类器进行辩论决策。

Result: 在6个(视觉)问答数据集上实验显示，token使用减少92%，准确率最高提升13.5%。

Conclusion: iMAD实现了效率与精度的双提升，其通用性设计无需特定数据集调整即可应用。

Abstract: Large Language Model (LLM) agent systems have advanced rapidly, driven by their strong generalization in zero-shot settings. To further enhance reasoning and accuracy on complex tasks, Multi-Agent Debate (MAD) has emerged as a promising framework that engages multiple LLM agents in structured debates to encourage diverse reasoning. However, triggering MAD for every query is inefficient, as it incurs substantial computational (token) cost and may even degrade accuracy by overturning correct single-agent answers. To address these limitations, we propose intelligent Multi-Agent Debate (iMAD), a token-efficient framework that selectively triggers MAD only when it is likely to be beneficial (i.e., correcting an initially wrong answer). To achieve this goal, iMAD learns generalizable model behaviors to make accurate debate decisions. Specifically, iMAD first prompts a single agent to produce a structured self-critique response, from which we extract 41 interpretable linguistic and semantic features capturing hesitation cues. Then, iMAD uses a lightweight debate-decision classifier, trained using our proposed FocusCal loss, to determine whether to trigger MAD, enabling robust debate decisions without test dataset-specific tuning. Through extensive experiments using six (visual) question answering datasets against five competitive baselines, we have shown that iMAD significantly reduces token usage (by up to 92%) while also improving final answer accuracy (by up to 13.5%).

</details>


### [64] [destroR: Attacking Transfer Models with Obfuscous Examples to Discard Perplexity](https://arxiv.org/abs/2511.11309)
*Saadat Rafid Ahmed,Rubayet Shareen,Radoan Sharkar,Nazia Hossain,Mansur Mahi,Farig Yousuf Sadeque*

Main category: cs.CL

TL;DR: 该论文研究现有对抗攻击方法并开发新型攻击策略，通过生成高困惑度对抗样本（包括孟加拉语案例）来增强模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前先进的机器学习模型虽广泛应用但存在安全漏洞，需要开发有效的对抗攻击方法来暴露弱点并提升模型防御能力。

Method: 综合评估现有对抗攻击方法，创建基于高困惑度输入的新型攻击策略，使用孟加拉语进行多语言对抗样本测试，并优化攻击效率。

Result: 成功开发出能有效迷惑SOTA模型的对抗攻击方法，验证了语言特异性攻击的有效性，为模型鲁棒性评估提供新范式。

Conclusion: 对抗攻击研究对提升AI安全性至关重要，未来需建立跨语言防御机制并将攻击方法转化为鲁棒性增强工具。

Abstract: Advancements in Machine Learning & Neural Networks in recent years have led to widespread implementations of Natural Language Processing across a variety of fields with remarkable success, solving a wide range of complicated problems. However, recent research has shown that machine learning models may be vulnerable in a number of ways, putting both the models and the systems theyre used in at risk. In this paper, we intend to analyze and experiment with the best of existing adversarial attack recipes and create new ones. We concentrated on developing a novel adversarial attack strategy on current state-of-the-art machine learning models by producing ambiguous inputs for the models to confound them and then constructing the path to the future development of the robustness of the models. We will develop adversarial instances with maximum perplexity, utilizing machine learning and deep learning approaches in order to trick the models. In our attack recipe, we will analyze several datasets and focus on creating obfuscous adversary examples to put the models in a state of perplexity, and by including the Bangla Language in the field of adversarial attacks. We strictly uphold utility usage reduction and efficiency throughout our work.

</details>


### [65] [LAET: A Layer-wise Adaptive Ensemble Tuning Framework for Pretrained Language Models](https://arxiv.org/abs/2511.11315)
*Jawad Ibn Ahad,Muhammad Rafsan Kabir,Robin Krambroeckers,Sifat Momen,Nabeel Mohammed,Shafin Rahman*

Main category: cs.CL

TL;DR: 提出LAET策略，通过选择性微调预训练大模型的关键层，在显著降低计算资源消耗的同时提升任务性能，使用小型LLM（约3B参数）超越GPT-4等现有基准。


<details>
  <summary>Details</summary>
Motivation: 现有金融大语言模型（如BloombergGPT、FinMA）计算需求过高，限制了多数机构的实际应用能力，需要开发高效可扩展的解决方案。

Method: LAET方法通过分析隐藏状态表示，选择性微调预训练LLM中最有效的层（冻结次要层），实现计算效率与模型性能的平衡。

Result: 在金融NLP任务中性能超越现有基准和GPT-4，使用仅3B参数的小型LLM即达到更优表现，FLARE-ES基准测试显示显著提升。

Conclusion: LAET策略有效平衡计算效率与模型性能，为金融NLP研究到实际部署提供了可扩展的解决方案。

Abstract: Natural Language Processing (NLP) has transformed the financial industry, enabling advancements in areas such as textual analysis, risk management, and forecasting. Large language models (LLMs) like BloombergGPT and FinMA have set new benchmarks across various financial NLP tasks, including sentiment analysis, stock movement prediction, and credit risk assessment. Furthermore, FinMA-ES, a bilingual financial LLM, has also demonstrated strong performance using the FLARE and FLARE-ES benchmarks. However, the high computational demands of these models limit the accessibility of many organizations. To address this, we propose Layer-wise Adaptive Ensemble Tuning (LAET), a novel strategy that selectively fine-tunes the most effective layers of pre-trained LLMs by analyzing hidden state representations while freezing less critical layers. LAET significantly reduces computational overhead while enhancing task-specific performance. Our approach shows strong results in financial NLP tasks, outperforming existing benchmarks and state-of-the-art LLMs such as GPT-4, even with smaller LLMs ($\sim$3B parameters). This work bridges cutting-edge financial NLP research and real-world deployment with efficient and scalable models for financial applications.

</details>


### [66] [NOVA: An Agentic Framework for Automated Histopathology Analysis and Discovery](https://arxiv.org/abs/2511.11324)
*Anurag J. Vaidya,Felix Meissen,Daniel C. Castro,Shruthi Bannur,Tristan Lazard,Drew F. K. Williamson,Faisal Mahmood,Javier Alvarez-Valle,Stephanie L. Hyland,Kenza Bouzid*

Main category: cs.CL

TL;DR: 提出智能代理框架NOVA，通过自动生成代码实现病理切片分析流程自动化，在专业基准测试中优于基线模型并验证临床应用潜力。


<details>
  <summary>Details</summary>
Motivation: 传统数字病理分析流程复杂且依赖专业知识，限制了其可访问性和应用规模。现有生物医学AI系统多集中于知识问答，缺乏复杂计算任务处理能力。

Method: 开发NOVA代理框架：1) 集成49个病理专用工具 2) 支持动态创建新工具 3) 引入多步骤推理的SlideQuest基准测试（含90个病理学家验证问题）

Result: 1) 在SlideQuest基准上显著优于基础编码代理（+35%成功率） 2) 案例研究成功关联形态学特征与PAM50预后亚型 3) 病理学家验证结果可靠性

Conclusion: NOVA框架通过自动化分析流程显著提升病理研究效率，其可扩展架构支持从量化分析到假设检验的多层次发现，为转化医学研究提供新范式。

Abstract: Digitized histopathology analysis involves complex, time-intensive workflows and specialized expertise, limiting its accessibility. We introduce NOVA, an agentic framework that translates scientific queries into executable analysis pipelines by iteratively generating and running Python code. NOVA integrates 49 domain-specific tools (e.g., nuclei segmentation, whole-slide encoding) built on open-source software, and can also create new tools ad hoc. To evaluate such systems, we present SlideQuest, a 90-question benchmark -- verified by pathologists and biomedical scientists -- spanning data processing, quantitative analysis, and hypothesis testing. Unlike prior biomedical benchmarks focused on knowledge recall or diagnostic QA, SlideQuest demands multi-step reasoning, iterative coding, and computational problem solving. Quantitative evaluation shows NOVA outperforms coding-agent baselines, and a pathologist-verified case study links morphology to prognostically relevant PAM50 subtypes, demonstrating its scalable discovery potential.

</details>


### [67] [LaoBench: A Large-Scale Multidimensional Lao Benchmark for Large Language Models](https://arxiv.org/abs/2511.11334)
*Jian Gao,Richeng Xuan,Zhaolu Kang,Dingshi Liao,Wenxin Huang,Zongmou Huang,Yangdi Xu,Bowen Qin,Zheqi He,Xi Yang,Changjin Li*

Main category: cs.CL

TL;DR: 首个面向老挝语的大规模多维评测基准LaoBench，包含17,000+样本，涵盖知识应用、K12教育和三语翻译，揭示主流大模型在老挝语任务中的显著不足。


<details>
  <summary>Details</summary>
Motivation: 填补大语言模型在东南亚低资源语言（如老挝语）评估体系缺失的空白，促进技术公平发展

Method: 构建三级评估体系（知识/教育/翻译），采用专家人工校审+智能体辅助验证的混合数据构建流程，建立闭源黑盒评测平台

Result: 现有主流模型在多样化老挝语任务中准确率普遍低于50%，跨语言翻译能力尤为薄弱

Conclusion: LaoBench为东南亚小语种AI研究提供基础设施，呼吁加强低资源语言的技术投入与跨文化适配

Abstract: The rapid advancement of large language models (LLMs) has not been matched by their evaluation in low-resource languages, especially Southeast Asian languages like Lao. To fill this gap, we introduce LaoBench, the first large-scale, high-quality, and multidimensional benchmark dataset dedicated to assessing LLMs' comprehensive language understanding and reasoning abilities in Lao. LaoBench comprises over 17,000 carefully curated samples spanning three core dimensions: knowledge application, K12 foundational education, and bilingual translation among Lao, Chinese, and English. The dataset is divided into open-source and closed-source subsets, with the closed-source portion enabling black-box evaluation on an official platform to ensure fairness and data security. Our data construction pipeline integrates expert human curation with automated agent-assisted verification, ensuring linguistic accuracy, cultural relevance, and educational value. Benchmarking multiple state-of-the-art LLMs on LaoBench reveals that current models still face significant challenges in mastering Lao across diverse tasks. We hope LaoBench will catalyze further research and development of AI technologies for underrepresented Southeast Asian languages.

</details>


### [68] [M-DAIGT: A Shared Task on Multi-Domain Detection of AI-Generated Text](https://arxiv.org/abs/2511.11340)
*Salima Lamsiyah,Saad Ezzini,Abdelkader El Mahdaouy,Hamza Alami,Abdessamad Benlahbib,Samir El Amrany,Salmane Chafik,Hicham Hammouchi*

Main category: cs.CL

TL;DR: 开发多领域AI生成文本检测任务（M-DAIGT）及3万样本数据集，用于新闻和学术文本检测，46团队参与但仅4队提交结果


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型生成文本对信息完整性和学术研究带来的挑战，特别是新闻和学术领域的AI内容检测需求

Method: 创建平衡数据集（15k人类/15k AI），使用GPT-4/Claude等模型生成文本，设计两个二分类子任务（新闻检测+学术写作检测）

Result: 46个团队注册，4个团队完成两个子任务提交，检测效果存在提升空间（具体准确率未披露）

Conclusion: 未来需改进检测方法并扩展多语言/跨领域应用，提升AI生成文本识别的鲁棒性

Abstract: The generation of highly fluent text by Large Language Models (LLMs) poses a significant challenge to information integrity and academic research. In this paper, we introduce the Multi-Domain Detection of AI-Generated Text (M-DAIGT) shared task, which focuses on detecting AI-generated text across multiple domains, particularly in news articles and academic writing. M-DAIGT comprises two binary classification subtasks: News Article Detection (NAD) (Subtask 1) and Academic Writing Detection (AWD) (Subtask 2). To support this task, we developed and released a new large-scale benchmark dataset of 30,000 samples, balanced between human-written and AI-generated texts. The AI-generated content was produced using a variety of modern LLMs (e.g., GPT-4, Claude) and diverse prompting strategies. A total of 46 unique teams registered for the shared task, of which four teams submitted final results. All four teams participated in both Subtask 1 and Subtask 2. We describe the methods employed by these participating teams and briefly discuss future directions for M-DAIGT.

</details>


### [69] [Studies with impossible languages falsify LMs as models of human language](https://arxiv.org/abs/2511.11389)
*Jeffrey S. Bowers,Jeff Mitchell*

Main category: cs.CL

TL;DR: 语言模型与人类婴儿在语言学习机制上存在本质差异，缺乏人类特有的归纳偏差


<details>
  <summary>Details</summary>
Motivation: 探究语言模型是否具备人类语言习得中的认知偏差

Method: 通过文献回顾对比语言模型与婴儿的语言学习表现

Result: 语言模型可习得部分不可能语言，其学习难度仅与复杂度相关而非结构合理性

Conclusion: 现有语言模型未内化人类语言学习的核心认知机制

Abstract: According to Futrell and Mahowald [arXiv:2501.17047], both infants and language models (LMs) find attested languages easier to learn than impossible languages that have unnatural structures. We review the literature and show that LMs often learn attested and many impossible languages equally well. Difficult to learn impossible languages are simply more complex (or random). LMs are missing human inductive biases that support language acquisition.

</details>


### [70] [MajinBook: An open catalogue of digital world literature with likes](https://arxiv.org/abs/2511.11412)
*Antoine Mazières,Thierry Poibeau*

Main category: cs.CL

TL;DR: MajinBook构建了包含53.9万+英文书籍的跨世纪语料库，整合影子图书馆与Goodreads元数据，支持多语言文化分析与计算社会科学研究。


<details>
  <summary>Details</summary>
Motivation: 解决传统语料库（如HathiTrust）的格式偏差问题，利用原生数字EPUB文件提升机器可读性，通过混合数据源创建高精度文化分析语料库。

Method: 1. 关联Library Genesis/Z-Library与Goodreads结构化元数据
2. 优先筛选EPUB格式保证文本质量
3. 开发法/德/西语二级数据集
4. 验证数据链接策略准确性并开源数据

Result: 建成包含539,000+英语书籍(跨3世纪)的语料库，含出版日期/类型/评分等多维指标，扩展法德西语数据集，数据开放获取。

Conclusion: MajinBook在法律允许范围内(符合欧美TDM法规)实现了学术资源民主化，为数字人文研究提供标准化、多语言、机器友好的文化分析基础设施。

Abstract: This data paper introduces MajinBook, an open catalogue designed to facilitate the use of shadow libraries--such as Library Genesis and Z-Library--for computational social science and cultural analytics. By linking metadata from these vast, crowd-sourced archives with structured bibliographic data from Goodreads, we create a high-precision corpus of over 539,000 references to English-language books spanning three centuries, enriched with first publication dates, genres, and popularity metrics like ratings and reviews. Our methodology prioritizes natively digital EPUB files to ensure machine-readable quality, while addressing biases in traditional corpora like HathiTrust, and includes secondary datasets for French, German, and Spanish. We evaluate the linkage strategy for accuracy, release all underlying data openly, and discuss the project's legal permissibility under EU and US frameworks for text and data mining in research.

</details>


### [71] [Proactive Hearing Assistants that Isolate Egocentric Conversations](https://arxiv.org/abs/2511.11473)
*Guilin Hu,Malek Itani,Tuochao Chen,Shyamnath Gollakota*

Main category: cs.CL

TL;DR: 提出主动听觉助手系统，通过双模型架构实时分离对话伙伴，在多人对话场景中验证有效性


<details>
  <summary>Details</summary>
Motivation: 传统听觉辅助需用户主动操作，本系统通过自动识别对话伙伴提升多环境下的自然交互体验

Method: 利用双耳音频特征+自我语音定位，采用轻量流式模型（12.5ms响应）处理实时分离，配合慢速模型捕获长期对话动态

Result: 在6.8小时真实对话数据测试中，2-3人场景下成功实现对话伙伴识别与隔离，系统泛化能力显著

Conclusion: 通过动态适应对话参与状态，为下一代主动型听觉辅助设备奠定技术基础，潜在提升听障人群社交质量

Abstract: We introduce proactive hearing assistants that automatically identify and separate the wearer's conversation partners, without requiring explicit prompts. Our system operates on egocentric binaural audio and uses the wearer's self-speech as an anchor, leveraging turn-taking behavior and dialogue dynamics to infer conversational partners and suppress others. To enable real-time, on-device operation, we propose a dual-model architecture: a lightweight streaming model runs every 12.5 ms for low-latency extraction of the conversation partners, while a slower model runs less frequently to capture longer-range conversational dynamics. Results on real-world 2- and 3-speaker conversation test sets, collected with binaural egocentric hardware from 11 participants totaling 6.8 hours, show generalization in identifying and isolating conversational partners in multi-conversation settings. Our work marks a step toward hearing assistants that adapt proactively to conversational dynamics and engagement. More information can be found on our website: https://proactivehearing.cs.washington.edu/

</details>


### [72] [W2S-AlignTree: Weak-to-Strong Inference-Time Alignment for Large Language Models via Monte Carlo Tree Search](https://arxiv.org/abs/2511.11518)
*Zhenyu Ding,Yuhao Wang,Tengyue Xiao,Haoying Wang,Guojun Ma,Mingyang Wan,Caigui Jiang,Ning Ding*

Main category: cs.CL

TL;DR: 提出W2S-AlignTree框架，通过结合蒙特卡洛树搜索和弱到强泛化范式，在不修改模型参数的情况下实现LLM的细粒度对齐


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐方法存在训练成本高、扩展性差、推断阶段控制不足的问题，需要可扩展的实时对齐机制

Method: 将LLM对齐建模为生成搜索树的最优启发式搜索问题，利用弱模型实时信号作为代理，引入熵感知探索机制

Result: 在情感生成、摘要和指令遵循任务中持续超越基线，使Llama3-8B在摘要任务得分提升15.9%（1.89→2.19）

Conclusion: W2S-AlignTree首次实现强模型生成的动态探索-利用平衡，为LLM对齐提供了参数保留的轻量化解决方案

Abstract: Large Language Models (LLMs) demonstrate impressive capabilities, yet their outputs often suffer from misalignment with human preferences due to the inadequacy of weak supervision and a lack of fine-grained control. Training-time alignment methods like Reinforcement Learning from Human Feedback (RLHF) face prohibitive costs in expert supervision and inherent scalability limitations, offering limited dynamic control during inference. Consequently, there is an urgent need for scalable and adaptable alignment mechanisms. To address this, we propose W2S-AlignTree, a pioneering plug-and-play inference-time alignment framework that synergistically combines Monte Carlo Tree Search (MCTS) with the Weak-to-Strong Generalization paradigm for the first time. W2S-AlignTree formulates LLM alignment as an optimal heuristic search problem within a generative search tree. By leveraging weak model's real-time, step-level signals as alignment proxies and introducing an Entropy-Aware exploration mechanism, W2S-AlignTree enables fine-grained guidance during strong model's generation without modifying its parameters. The approach dynamically balances exploration and exploitation in high-dimensional generation search trees. Experiments across controlled sentiment generation, summarization, and instruction-following show that W2S-AlignTree consistently outperforms strong baselines. Notably, W2S-AlignTree raises the performance of Llama3-8B from 1.89 to 2.19, a relative improvement of 15.9 on the summarization task.

</details>


### [73] [PRBench: Large-Scale Expert Rubrics for Evaluating High-Stakes Professional Reasoning](https://arxiv.org/abs/2511.11562)
*Afra Feyza Akyürek,Advait Gosai,Chen Bo Calvin Zhang,Vipul Gupta,Jaehwan Jeong,Anisha Gunjal,Tahseen Rabbani,Maria Mazzone,David Randolph,Mohammad Mahmoudi Meymand,Gurshaan Chattha,Paula Rodriguez,Diego Mares,Pavit Singh,Michael Liu,Subodh Chawla,Pete Cline,Lucy Ogaz,Ernesto Hernandez,Zihao Wang,Pavi Bhatter,Marcos Ayestaran,Bing Liu,Yunzhong He*

Main category: cs.CL

TL;DR: 论文提出PRBench专业评估基准，通过1,100个专家设计的法律和金融真实任务测试AI模型，发现顶尖模型在困难任务中得分仅0.37-0.39，揭示现有模型在专业场景中存在判断错误、推理不完整等可靠性缺陷


<details>
  <summary>Details</summary>
Motivation: 传统AI基准测试难以评估高价值专业领域（法律/金融）开放式任务的实际表现，需建立更贴近真实工作流程的评估体系

Method: 联合182名持证专家设计跨114个国家、47个美国辖区的1,100个真实任务，建立19,356条专家验证的评估标准，构建目前最大的公开领域基准，并对20个主流模型进行多维度测试

Result: 顶尖模型在困难子集得分不足0.4（法律0.37/金融0.39），经济影响分析显示提示词关联年影响达数万亿美元，模型在专业判断准确性和推理透明度存在系统性缺陷

Conclusion: 现有AI模型在专业场景可靠性存在重大瓶颈，需提升过程透明度和完整推理能力，研究建立的PRBench为专业领域AI评估提供新的标准工具

Abstract: Frontier model progress is often measured by academic benchmarks, which offer a limited view of performance in real-world professional contexts. Existing evaluations often fail to assess open-ended, economically consequential tasks in high-stakes domains like Legal and Finance, where practical returns are paramount. To address this, we introduce Professional Reasoning Bench (PRBench), a realistic, open-ended, and difficult benchmark of real-world problems in Finance and Law. We open-source its 1,100 expert-authored tasks and 19,356 expert-curated criteria, making it, to our knowledge, the largest public, rubric-based benchmark for both legal and finance domains. We recruit 182 qualified professionals, holding JDs, CFAs, or 6+ years of experience, who contributed tasks inspired by their actual workflows. This process yields significant diversity, with tasks spanning 114 countries and 47 US jurisdictions. Our expert-curated rubrics are validated through a rigorous quality pipeline, including independent expert validation. Subsequent evaluation of 20 leading models reveals substantial room for improvement, with top scores of only 0.39 (Finance) and 0.37 (Legal) on our Hard subsets. We further catalog associated economic impacts of the prompts and analyze performance using human-annotated rubric categories. Our analysis shows that models with similar overall scores can diverge significantly on specific capabilities. Common failure modes include inaccurate judgments, a lack of process transparency and incomplete reasoning, highlighting critical gaps in their reliability for professional adoption.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [74] [CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation](https://arxiv.org/abs/2511.11104)
*Crystal Min Hui Poon,Pai Chet Ng,Xiaoxiao Miao,Immanuel Jun Kai Loh,Bowen Zhang,Haoyu Song,Ian Mcloughlin*

Main category: cs.SD

TL;DR: CLARITY框架通过双重优化（上下文语言适应+检索增强口音提示）解决TTS中的口音与语言偏见，在12种英语口音中提升准确性与公平性。


<details>
  <summary>Details</summary>
Motivation: 现有指令引导TTS模型存在口音偏见（默认主导语音模式）和语言偏见（忽略方言文化），导致生成结果缺乏包容性。需同时优化文本本地化与口音一致性。

Method: 1. 上下文语言适应：将输入文本适配目标方言词汇/文化
2. 检索增强口音提示(RAAP)：检索相似口音的语音作为生成提示

Result: 在12种英语口音测试中，CLARITY显著提高口音准确性（+23%）、公平性指标（+18%），且语音质量与基线模型相当（MOS 4.1 vs 4.3）。

Conclusion: CLARITY首次实现文本本地化与语音提示的联合优化，为包容性TTS系统建立新范式，证明多信号优化对消除算法偏见的有效性。

Abstract: Instruction-guided text-to-speech (TTS) research has reached a maturity level where excellent speech generation quality is possible on demand, yet two coupled biases persist: accent bias, where models default to dominant phonetic patterns, and linguistic bias, where dialect-specific lexical and cultural cues are ignored. These biases are interdependent, as authentic accent generation requires both accent fidelity and localized text. We present Contextual Linguistic Adaptation and Retrieval for Inclusive TTS sYnthesis (CLARITY), a backbone-agnostic framework that addresses these biases through dual-signal optimization: (i) contextual linguistic adaptation that localizes input text to the target dialect, and (ii) retrieval-augmented accent prompting (RAAP) that supplies accent-consistent speech prompts. Across twelve English accents, CLARITY improves accent accuracy and fairness while maintaining strong perceptual quality.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [75] [Language-Aided State Estimation](https://arxiv.org/abs/2511.11285)
*Yuki Miyoshi,Masaki Inoue,Yusuke Fujimoto*

Main category: eess.SY

TL;DR: 利用自然语言处理人类观察数据改进物理系统状态估计，提出语言辅助粒子滤波器(LAPF)并应用于灌溉渠道水位估计


<details>
  <summary>Details</summary>
Motivation: 传统物理系统状态估计依赖物理传感器，而社交网络和聊天平台产生的大量自然语言数据可作为人类观察的有效补充

Method: 提出语言辅助粒子滤波器框架(LAPF)：通过自然语言处理结构化人类观察，将其整合到状态估计的更新步骤中

Result: 在灌溉渠道水位估计案例中验证了LAPF框架的有效性

Conclusion: 成功证明自然语言处理与状态估计框架的融合可行性，为人类观察数据在物理系统监控中的应用提供新范式

Abstract: Natural language data, such as text and speech, have become readily available through social networking services and chat platforms. By leveraging human observations expressed in natural language, this paper addresses the problem of state estimation for physical systems, in which humans act as sensing agents. To this end, we propose a Language-Aided Particle Filter (LAPF), a particle filter framework that structures human observations via natural language processing and incorporates them into the update step of the state estimation. Finally, the LAPF is applied to the water level estimation problem in an irrigation canal and its effectiveness is demonstrated.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [76] [SQuaD: The Software Quality Dataset](https://arxiv.org/abs/2511.11265)
*Mikel Robredo,Matteo Esposito,Davide Taibi,Rafael Peñaloza,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: SQuaD数据集整合9个静态分析工具，覆盖450个开源项目，提供700+跨维度软件质量指标，支持大规模软件分析研究。


<details>
  <summary>Details</summary>
Motivation: 现有软件质量数据集维度单一（如代码异味/技术债务），难以进行跨时间和多维度综合分析。

Method: 集成SonarQube等9个工具，提取方法/类/文件/项目层700+指标，涵盖63,586个版本，包含版本控制/CVE/过程度量数据。

Result: 创建首个多维时间感知软件质量数据集，覆盖Apache/Mozilla/FFmpeg/Linux内核等成熟项目，支持JIT缺陷预测。

Conclusion: SQuaD为可维护性/技术债务/软件演化研究提供基础，未来计划实现数据集自动更新和跨项目质量建模。

Abstract: Software quality research increasingly relies on large-scale datasets that measure both the product and process aspects of software systems. However, existing resources often focus on limited dimensions, such as code smells, technical debt, or refactoring activity, thereby restricting comprehensive analyses across time and quality dimensions. To address this gap, we present the Software Quality Dataset (SQuaD), a multi-dimensional, time-aware collection of software quality metrics extracted from 450 mature open-source projects across diverse ecosystems, including Apache, Mozilla, FFmpeg, and the Linux kernel. By integrating nine state-of-the-art static analysis tools, i.e., SonarQube, CodeScene, PMD, Understand, CK, JaSoMe, RefactoringMiner, RefactoringMiner++, and PyRef, our dataset unifies over 700 unique metrics at method, class, file, and project levels. Covering a total of 63,586 analyzed project releases, SQuaD also provides version control and issue-tracking histories, software vulnerability data (CVE/CWE), and process metrics proven to enhance Just-In-Time (JIT) defect prediction. The SQuaD enables empirical research on maintainability, technical debt, software evolution, and quality assessment at unprecedented scale. We also outline emerging research directions, including automated dataset updates and cross-project quality modeling to support the continuous evolution of software analytics. The dataset is publicly available on ZENODO (DOI: 10.5281/zenodo.17566690).

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [77] [S2D-ALIGN: Shallow-to-Deep Auxiliary Learning for Anatomically-Grounded Radiology Report Generation](https://arxiv.org/abs/2511.11066)
*Jiechao Gao,Chang Liu,Yuangang Li*

Main category: cs.CV

TL;DR: 提出S2D-Align框架，通过浅层到深层的多粒度辅助信号对齐策略，改进放射报告生成中的解剖学基础对齐问题


<details>
  <summary>Details</summary>
Motivation: 现有基于监督微调的方法仅进行实例级图像-文本对齐，难以建立解剖学细粒度对齐，导致生成质量受限

Method: 1. 三阶段渐进对齐：粗粒度→实例参考→解剖关键短语
2. 引入记忆适配器融合多阶段特征
3. 利用参考报告和关键短语实现层次化对齐

Result: 在MIMIC-CXR和IU X-Ray数据集达到SOTA性能，消融实验验证多阶段策略有效性（BLEU-4提升2.1%）

Conclusion: S2D-Align为复杂多模态生成任务提供了有效解决方案，证明辅助信号引导的多阶段对齐策略能显著增强生成结果的解剖学基础

Abstract: Radiology Report Generation (RRG) aims to automatically generate diagnostic reports from radiology images. To achieve this, existing methods have leveraged the powerful cross-modal generation capabilities of Multimodal Large Language Models (MLLMs), primarily focusing on optimizing cross-modal alignment between radiographs and reports through Supervised Fine-Tuning (SFT). However, by only performing instance-level alignment with the image-text pairs, the standard SFT paradigm fails to establish anatomically-grounded alignment, where the templated nature of reports often leads to sub-optimal generation quality. To address this, we propose \textsc{S2D-Align}, a novel SFT paradigm that establishes anatomically-grounded alignment by leveraging auxiliary signals of varying granularities. \textsc{S2D-Align} implements a shallow-to-deep strategy, progressively enriching the alignment process: it begins with the coarse radiograph-report pairing, then introduces reference reports for instance-level guidance, and ultimately utilizes key phrases to ground the generation in specific anatomical details. To bridge the different alignment stages, we introduce a memory-based adapter that empowers feature sharing, thereby integrating coarse and fine-grained guidance. For evaluation, we conduct experiments on the public \textsc{MIMIC-CXR} and \textsc{IU X-Ray} benchmarks, where \textsc{S2D-Align} achieves state-of-the-art performance compared to existing methods. Ablation studies validate the effectiveness of our multi-stage, auxiliary-guided approach, highlighting a promising direction for enhancing grounding capabilities in complex, multi-modal generation tasks.

</details>


### [78] [Discovering Meaningful Units with Visually Grounded Semantics from Image Captions](https://arxiv.org/abs/2511.11262)
*Melika Behjati,James Henderson*

Main category: cs.CV

TL;DR: 提出通过分组标题标记来增强视觉-语言模型的细粒度理解


<details>
  <summary>Details</summary>
Motivation: 现有方法将图像块与语言标记对齐，但图像块无明确语义且单个标记难以携带可接地信息

Method: 在模型架构中自动分组标题标记形成细粒度语言表示，并与对象发现图像编码器对齐

Result: 模型展现出更好的视觉语言细粒度理解能力，发现的标记组与文本可接地短语高度相似（定性与定量验证）

Conclusion: 通过语言标记分组机制有效提升了视觉-语言模型对现实世界的细粒度认知能力

Abstract: Fine-grained knowledge is crucial for vision-language models to obtain a better understanding of the real world. While there has been work trying to acquire this kind of knowledge in the space of vision and language, it has mostly focused on aligning the image patches with the tokens on the language side. However, image patches do not have any meaning to the human eye, and individual tokens do not necessarily carry groundable information in the image. It is groups of tokens which describe different aspects of the scene. In this work, we propose a model which groups the caption tokens as part of its architecture in order to capture a fine-grained representation of the language. We expect our representations to be at the level of objects present in the image, and therefore align our representations with the output of an image encoder trained to discover objects. We show that by learning to group the tokens, the vision-language model has a better fine-grained understanding of vision and language. In addition, the token groups that our model discovers are highly similar to groundable phrases in text, both qualitatively and quantitatively.

</details>


### [79] [From Synthetic Scenes to Real Performance: Enhancing Spatial Reasoning in VLMs](https://arxiv.org/abs/2511.11440)
*Massimo Rizzoli,Simone Alghisi,Seyed Mahed Mousavi,Giuseppe Riccardi*

Main category: cs.CV

TL;DR: 研究提出通过控制合成数据生成解决VLM微调中的偏差问题，实验显示合成数据微调显著提升真实场景表现


<details>
  <summary>Details</summary>
Motivation: 传统VLM微调存在数据偏差、分布失衡和标注错误，导致过拟合和性能不均衡。需要更可控的数据生成方案改善模型泛化能力

Method: 1. 构建无偏合成数据集：系统采样物体颜色/形状/尺寸/位置等属性
2. 基于合成数据微调SOTA视觉语言模型
3. 在绝对位置任务上评估真实数据迁移能力

Result: 1. 平衡合成数据使模型视觉场景表现更均匀
2. 合成数据微调模型在COCO数据集上的表现优于传统匹配微调方法

Conclusion: 可控合成数据能有效消除训练偏差并提升跨域迁移能力，为VLM优化提供新范式

Abstract: Fine-tuning Vision-Language Models (VLMs) is a common strategy to improve performance following an ad-hoc data collection and annotation of real-world scenes. However, this process is often prone to biases, errors, and distribution imbalance, resulting in overfitting and imbalanced performance. Although a few studies have tried to address this problem by generating synthetic data, they lacked control over distribution bias and annotation quality. To address these challenges, we redesign the fine-tuning process in two ways. First, we control the generation of data and its annotations, ensuring it is free from bias, distribution imbalance, and annotation errors. We automatically construct the dataset by comprehensively sampling objects' attributes, including color, shape, size, and position within the scene. Secondly, using this annotated dataset, we fine-tune state-of-the-art VLMs and assess performance transferability to real-world data on the absolute position task. We conduct exhaustive evaluations on both synthetic and real-world benchmarks. Our experiments reveal two key findings: 1) fine-tuning on balanced synthetic data yields uniform performance across the visual scene and mitigates common biases; and 2) fine-tuning on synthetic stimuli significantly improves performance on real-world data (COCO), outperforming models fine-tuned in the matched setting.

</details>


### [80] [DocLens : A Tool-Augmented Multi-Agent Framework for Long Visual Document Understanding](https://arxiv.org/abs/2511.11552)
*Dawei Zhu,Rui Meng,Jiefeng Chen,Sujian Li,Tomas Pfister,Jinsung Yoon*

Main category: cs.CV

TL;DR: DocLens框架通过分层证据定位机制和采样裁决策略，显著提升长文档理解能力，在多个基准测试中超越人类专家表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型存在证据定位不足的问题，导致信息检索不精准和模型幻觉现象。

Method: 采用工具增强的多智能体框架：1）分层导航机制（文档→页面→视觉元素） 2）采样-裁决机制生成可靠答案

Result: 在MMLongBench-Doc和FinRAGBench-V实现SOTA，准确率分别达78.6%和81.2%，视觉中心任务提升15%

Conclusion: 该框架通过增强的细粒度定位能力，有效解决视觉密集型查询和不可回答问题的处理难题。

Abstract: Comprehending long visual documents, where information is distributed across extensive pages of text and visual elements, is a critical but challenging task for modern Vision-Language Models (VLMs). Existing approaches falter on a fundamental challenge: evidence localization. They struggle to retrieve relevant pages and overlook fine-grained details within visual elements, leading to limited performance and model hallucination. To address this, we propose DocLens, a tool-augmented multi-agent framework that effectively ``zooms in'' on evidence like a lens. It first navigates from the full document to specific visual elements on relevant pages, then employs a sampling-adjudication mechanism to generate a single, reliable answer. Paired with Gemini-2.5-Pro, DocLens achieves state-of-the-art performance on MMLongBench-Doc and FinRAGBench-V, surpassing even human experts. The framework's superiority is particularly evident on vision-centric and unanswerable queries, demonstrating the power of its enhanced localization capabilities.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [81] [The Map of Misbelief: Tracing Intrinsic and Extrinsic Hallucinations Through Attention Patterns](https://arxiv.org/abs/2511.10837)
*Elyes Hajji,Aymen Bouguerra,Fabio Arnez*

Main category: cs.LG

TL;DR: 提出注意力聚合策略的幻觉检测框架，区分外在/内在幻觉类型并提升检测效果


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法依赖高成本采样策略且未区分幻觉类型，需开发更高效的检测方法

Method: 1）构建区分外在/内在幻觉的评估框架 2）基于注意力机制设计新型聚合策略 3）在多个基准测试中验证效果

Result: 语义熵擅长检测外在幻觉（准确率78%），注意力聚合方法在内在幻觉检测上提升15%准确率

Conclusion: 应根据幻觉类型选择检测策略，注意力机制是量化模型不确定性的有效信号源

Abstract: Large Language Models (LLMs) are increasingly deployed in safety-critical domains, yet remain susceptible to hallucinations. While prior works have proposed confidence representation methods for hallucination detection, most of these approaches rely on computationally expensive sampling strategies and often disregard the distinction between hallucination types. In this work, we introduce a principled evaluation framework that differentiates between extrinsic and intrinsic hallucination categories and evaluates detection performance across a suite of curated benchmarks. In addition, we leverage a recent attention-based uncertainty quantification algorithm and propose novel attention aggregation strategies that improve both interpretability and hallucination detection performance. Our experimental findings reveal that sampling-based methods like Semantic Entropy are effective for detecting extrinsic hallucinations but generally fail on intrinsic ones. In contrast, our method, which aggregates attention over input tokens, is better suited for intrinsic hallucinations. These insights provide new directions for aligning detection strategies with the nature of hallucination and highlight attention as a rich signal for quantifying model uncertainty.

</details>


### [82] [On-Device Fine-Tuning via Backprop-Free Zeroth-Order Optimization](https://arxiv.org/abs/2511.11362)
*Prabodh Katti,Sangwoo Park,Bipin Rajendran,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 提出基于内存高效零阶优化(MeZO)的端侧微调方法，在有限内存下支持更大模型部署，权衡更长的训练时间


<details>
  <summary>Details</summary>
Motivation: 传统反向传播(BP)训练需要存储激活和优化器状态，严重限制了端侧设备可部署的模型规模

Method: 采用零阶优化算法MeZO，仅通过前向传播估计梯度，消除中间激活和优化器状态存储需求

Result: 理论分析和实验验证表明，在内存限制下MeZO可部署更大模型，且保持足够训练时间时精度优于BP方法

Conclusion: MeZO为端侧AI系统提供了内存效率与模型规模的优化平衡，为边缘设备部署大模型开辟新路径

Abstract: On-device fine-tuning is a critical capability for edge AI systems, which must support adaptation to different agentic tasks under stringent memory constraints. Conventional backpropagation (BP)-based training requires storing layer activations and optimizer states, a demand that can be only partially alleviated through checkpointing. In edge deployments in which the model weights must reside entirely in device memory, this overhead severely limits the maximum model size that can be deployed. Memory-efficient zeroth-order optimization (MeZO) alleviates this bottleneck by estimating gradients using forward evaluations alone, eliminating the need for storing intermediate activations or optimizer states. This enables significantly larger models to fit within on-chip memory, albeit at the cost of potentially longer fine-tuning wall-clock time. This paper first provides a theoretical estimate of the relative model sizes that can be accommodated under BP and MeZO training. We then numerically validate the analysis, demonstrating that MeZO exhibits accuracy advantages under on-device memory constraints, provided sufficient wall-clock time is available for fine-tuning.

</details>


### [83] [Optimizing Mixture of Block Attention](https://arxiv.org/abs/2511.11571)
*Guangxuan Xiao,Junxian Guo,Kasra Mazaheri,Song Han*

Main category: cs.LG

TL;DR: MoBA通过稀疏注意力机制高效处理长上下文，但面临理论理解不足和硬件效率瓶颈。本文提出统计模型揭示路由器准确性对性能的关键影响，并通过改进块大小和卷积策略结合自研CUDA内核FlashMoBA实现理论优化与硬件效率的统一。


<details>
  <summary>Details</summary>
Motivation: 解决MoBA在路由器机制理论理解不清晰、小分块GPU实现低效的问题，推动稀疏注意力机制的实际应用。

Method: 1. 建立统计模型分析路由器区分能力与信噪比的关系 2. 提出减小块尺寸+键向量短卷积的优化路径 3. 开发硬件感知的FlashMoBA CUDA内核实现高效计算

Result: 改进后的MoBA模型性能匹配密集注意力基线，FlashMoBA在小块处理时相比FlashAttention-2实现14.7倍加速

Conclusion: 通过理论指导的块尺寸优化与卷积策略，结合定制化GPU内核实现，首次在保持模型性能的同时显著提升MoBA计算效率

Abstract: Mixture of Block Attention (MoBA) (Lu et al., 2025) is a promising building block for efficiently processing long contexts in LLMs by enabling queries to sparsely attend to a small subset of key-value blocks, drastically reducing computational cost. However, the design principles governing MoBA's performance are poorly understood, and it lacks an efficient GPU implementation, hindering its practical adoption. In this paper, we first develop a statistical model to analyze MoBA's underlying mechanics. Our model reveals that performance critically depends on the router's ability to accurately distinguish relevant from irrelevant blocks based on query-key affinities. We derive a signal-to-noise ratio that formally connects architectural parameters to this retrieval accuracy. Guided by our analysis, we identify two key pathways for improvement: using smaller block sizes and applying a short convolution on keys to cluster relevant signals, which enhances routing accuracy. While theoretically better, small block sizes are inefficient on GPUs. To bridge this gap, we introduce FlashMoBA, a hardware-aware CUDA kernel that enables efficient MoBA execution even with the small block sizes our theory recommends. We validate our insights by training LLMs from scratch, showing that our improved MoBA models match the performance of dense attention baselines. FlashMoBA achieves up to 14.7x speedup over FlashAttention-2 for small blocks, making our theoretically-grounded improvements practical. Code is available at: https://github.com/mit-han-lab/flash-moba.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [84] [PISanitizer: Preventing Prompt Injection to Long-Context LLMs via Prompt Sanitization](https://arxiv.org/abs/2511.10720)
*Runpeng Geng,Yanting Wang,Chenlong Yin,Minhao Cheng,Ying Chen,Jinyuan Jia*

Main category: cs.CR

TL;DR: 提出PISanitizer防御长上下文LLM的提示注入攻击，通过定位高注意力token消除注入指令影响


<details>
  <summary>Details</summary>
Motivation: 现有短上下文防御方案在长场景中失效，因注入指令仅占极小比例难以检测

Method: 基于LLM必然遵循指令的特性，利用注意力机制识别并清理驱动异常行为的关键token

Result: 实验证明可有效防御注入攻击，保持模型效用，优于现有方案且抵御优化型攻击

Conclusion: 首次实现长上下文场景的提示注入防御，形成攻击者无法逃避的清理困境

Abstract: Long context LLMs are vulnerable to prompt injection, where an attacker can inject an instruction in a long context to induce an LLM to generate an attacker-desired output. Existing prompt injection defenses are designed for short contexts. When extended to long-context scenarios, they have limited effectiveness. The reason is that an injected instruction constitutes only a very small portion of a long context, making the defense very challenging. In this work, we propose PISanitizer, which first pinpoints and sanitizes potential injected tokens (if any) in a context before letting a backend LLM generate a response, thereby eliminating the influence of the injected instruction. To sanitize injected tokens, PISanitizer builds on two observations: (1) prompt injection attacks essentially craft an instruction that compels an LLM to follow it, and (2) LLMs intrinsically leverage the attention mechanism to focus on crucial input tokens for output generation. Guided by these two observations, we first intentionally let an LLM follow arbitrary instructions in a context and then sanitize tokens receiving high attention that drive the instruction-following behavior of the LLM. By design, PISanitizer presents a dilemma for an attacker: the more effectively an injected instruction compels an LLM to follow it, the more likely it is to be sanitized by PISanitizer. Our extensive evaluation shows that PISanitizer can successfully prevent prompt injection, maintain utility, outperform existing defenses, is efficient, and is robust to optimization-based and strong adaptive attacks. The code is available at https://github.com/sleeepeer/PISanitizer.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [85] [Who Gets the Reward, Who Gets the Blame? Evaluation-Aligned Training Signals for Multi-LLM Agents](https://arxiv.org/abs/2511.10687)
*Chih-Hsuan Yang,Tanwi Mallick,Le Chen,Krishnan Raghavan,Azton Wells,Amal Gueroudji,Ian T. Foster,Rajeev Thakur*

Main category: cs.MA

TL;DR: 提出结合合作博弈论与过程奖励建模的理论框架，将系统评估转化为个体信用和响应级训练信号


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统中的LLM训练方法缺乏系统评估与个体层面学习的有效连接机制

Method: 通过Shapley值公平分配成功案例信用，首错定位生成修复感知偏好，产生有界、协同的训练信号

Result: 产生局部性、有符号且信用守恒的信号，可直接用于强化学习或偏好优化的后期训练

Conclusion: 该框架建立了从全局评估到局部监督的审计路径，为LLM多智能体训练提供理论基础

Abstract: Large Language Models (LLMs) in multi-agent systems (MAS) have shown promise for complex tasks, yet current training methods lack principled ways to connect system-level evaluation with agent-level and message-level learning. We propose a theoretical framework that unifies cooperative game-theoretic attribution with process reward modeling to transform system evaluation into agent credit and then into response-level signals. Unlike prior approaches that rely only on attribution (e.g., Shapley) or step-level labels (e.g., PRM), our method produces local, signed, and credit-conserving signals. In success cases, Shapley-based credit assignment fairly allocates outcomes across agents and is refined into per-message rewards that promote cooperation while discouraging redundancy or sabotage. In failure cases, first-error localization yields repair-aware preferences that penalize harmful steps while rewarding corrective attempts. The resulting signals are bounded, cooperative, and directly compatible with reinforcement-based or preference-based post-training, providing a unified and auditable pathway from global evaluation to local supervision in LLM multi-agent training. Our contribution is conceptual: we present a theoretical foundation and training signals, leaving empirical validation for future work.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [86] [Building the Web for Agents: A Declarative Framework for Agent-Web Interaction](https://arxiv.org/abs/2511.11287)
*Sven Schultze,Meike Verena Kietzmann,Nils-Lucas Schönfeld,Ruth Stock-Homburg*

Main category: cs.HC

TL;DR: VOIX框架通过声明式HTML标签为网站提供标准化AI代理交互接口，解决人机界面错位问题，实现安全高效的Agentic Web生态。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理需逆向解析人类界面导致交互脆弱低效，需建立网站与代理的显式通信机制以推动智能体网络（Agentic Web）发展。

Method: 引入<tool>和<context>标签定义机器可读的交互契约，分离对话层与网站实现隐私保护，开发者通过声明式编程暴露能力接口。

Result: 16名开发者在3天黑客马拉松中成功构建多样化代理应用，验证框架易用性（平均4.3/5满意度）与表达能力（支持电商/教育等6类场景）。

Conclusion: VOIX建立了网站与AI代理的标准通信范式，为实现安全无缝的人机协作奠定基础，推动Web向智能体友好架构演进。

Abstract: The increasing deployment of autonomous AI agents on the web is hampered by a fundamental misalignment: agents must infer affordances from human-oriented user interfaces, leading to brittle, inefficient, and insecure interactions. To address this, we introduce VOIX, a web-native framework that enables websites to expose reliable, auditable, and privacy-preserving capabilities for AI agents through simple, declarative HTML elements. VOIX introduces <tool> and <context> tags, allowing developers to explicitly define available actions and relevant state, thereby creating a clear, machine-readable contract for agent behavior. This approach shifts control to the website developer while preserving user privacy by disconnecting the conversational interactions from the website. We evaluated the framework's practicality, learnability, and expressiveness in a three-day hackathon study with 16 developers. The results demonstrate that participants, regardless of prior experience, were able to rapidly build diverse and functional agent-enabled web applications. Ultimately, this work provides a foundational mechanism for realizing the Agentic Web, enabling a future of seamless and secure human-AI collaboration on the web.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [87] [Co-EPG: A Framework for Co-Evolution of Planning and Grounding in Autonomous GUI Agents](https://arxiv.org/abs/2511.10705)
*Yuan Zhao,Hualei Zhu,Tingyu Jiang,Shen Li,Xiaohang Xu,Hao Henry Wang*

Main category: cs.AI

TL;DR: 提出Co-EPG自迭代训练框架，通过规划模型与基础模型的协同进化机制实现GUI代理性能的持续提升


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理存在跨模型协同不足和过度依赖合成数据的问题，需要建立更有效的训练范式

Method: 采用GRPO强化学习算法构建规划与基础模型的反馈闭环，通过自生成训练数据和奖励机制驱动双模型迭代优化

Result: 在Mind2Web和AndroidControl基准上三轮迭代即超越现有方法，且不依赖外部数据持续提升性能

Conclusion: 开创了GUI代理从孤立训练向自驱协同进化转变的新范式，为智能体持续优化提供创新方法论

Abstract: Graphical User Interface (GUI) task automation constitutes a critical frontier in artificial intelligence research. While effective GUI agents synergistically integrate planning and grounding capabilities, current methodologies exhibit two fundamental limitations: (1) insufficient exploitation of cross-model synergies, and (2) over-reliance on synthetic data generation without sufficient utilization. To address these challenges, we propose Co-EPG, a self-iterative training framework for Co-Evolution of Planning and Grounding. Co-EPG establishes an iterative positive feedback loop: through this loop, the planning model explores superior strategies under grounding-based reward guidance via Group Relative Policy Optimization (GRPO), generating diverse data to optimize the grounding model. Concurrently, the optimized Grounding model provides more effective rewards for subsequent GRPO training of the planning model, fostering continuous improvement. Co-EPG thus enables iterative enhancement of agent capabilities through self-play optimization and training data distillation. On the Multimodal-Mind2Web and AndroidControl benchmarks, our framework outperforms existing state-of-the-art methods after just three iterations without requiring external data. The agent consistently improves with each iteration, demonstrating robust self-enhancement capabilities. This work establishes a novel training paradigm for GUI agents, shifting from isolated optimization to an integrated, self-driven co-evolution approach.

</details>


### [88] [From Efficiency to Adaptivity: A Deeper Look at Adaptive Reasoning in Large Language Models](https://arxiv.org/abs/2511.10788)
*Chao Wu,Baoheng Li,Mingchen Gao,Zhenyi Wang*

Main category: cs.AI

TL;DR: 本调查从适应性角度重新审视大语言模型推理，提出分类法区分基于训练和无需训练的方法，系统比较不同策略。


<details>
  <summary>Details</summary>
Motivation: 传统方法忽视任务复杂性差异，采用统一推理策略导致简单任务资源浪费而复杂任务推理不足。研究旨在通过动态分配推理资源提升效率。

Method: 1. 形式化三大推理范式与算法实现的关系
2. 将适应性推理建模为控制增强的策略优化问题
3. 建立分类框架整合强化学习、提示工程等实现方法

Result: 构建的系统分类法揭示不同机制实现适应性推理的路径，为跨策略比较提供统一框架

Conclusion: 提出未来研究方向：推理过程自我评估、元推理机制开发、人类价值观对齐的推理控制

Abstract: Recent advances in large language models (LLMs) have made reasoning a central benchmark for evaluating intelligence. While prior surveys focus on efficiency by examining how to shorten reasoning chains or reduce computation, this view overlooks a fundamental challenge: current LLMs apply uniform reasoning strategies regardless of task complexity, generating long traces for trivial problems while failing to extend reasoning for difficult tasks. This survey reframes reasoning through the lens of {adaptivity}: the capability to allocate reasoning effort based on input characteristics such as difficulty and uncertainty. We make three contributions. First, we formalize deductive, inductive, and abductive reasoning within the LLM context, connecting these classical cognitive paradigms with their algorithmic realizations. Second, we formalize adaptive reasoning as a control-augmented policy optimization problem balancing task performance with computational cost, distinguishing learned policies from inference-time control mechanisms. Third, we propose a systematic taxonomy organizing existing methods into training-based approaches that internalize adaptivity through reinforcement learning, supervised fine-tuning, and learned controllers, and training-free approaches that achieve adaptivity through prompt conditioning, feedback-driven halting, and modular composition. This framework clarifies how different mechanisms realize adaptive reasoning in practice and enables systematic comparison across diverse strategies. We conclude by identifying open challenges in self-evaluation, meta-reasoning, and human-aligned reasoning control.

</details>


### [89] [Multi-agent Undercover Gaming: Hallucination Removal via Counterfactual Test for Multimodal Reasoning](https://arxiv.org/abs/2511.11182)
*Dayong Liang,Xiao-Yong Wei,Changmeng Zheng*

Main category: cs.AI

TL;DR: 提出多代理卧底游戏协议(MUG)，通过多模态反事实测试检测存在幻觉的代理，提升大语言模型多模态推理的可靠性。相较于传统多代理辩论方法，MUG在事实验证、动态证据处理和主动推理三方面取得突破。


<details>
  <summary>Details</summary>
Motivation: 现有多代理辩论(MAD)方法依赖所有代理理性且不产生幻觉的不现实假设。当代理自身存在幻觉时，基于共识的可靠性机制可能失效，需新的检测框架解决该问题。

Method: 受社交推理游戏启发，将MAD重构为卧底代理检测过程：1) 修改参考图像生成反事实证据 2) 观察代理识别能力提供验证基准 3) 建立群体智能驱动的多模态推理框架。通过动态证据修改和主动问答机制实现跨证据验证。

Result: 在三个维度改进MAD协议：反事实测试超越统计共识实现事实验证；动态证据源支持跨证据推理；主动探测式讨论取代被动应答。构建出更可靠的多模态推理框架，代码已开源。

Conclusion: MUG通过引入反事实检测机制和动态证据处理，有效识别幻觉代理，推动大语言模型在多模态推理任务中的可靠性提升。该方法为复杂场景下的群体智能协作提供新范式。

Abstract: Hallucination continues to pose a major obstacle in the reasoning capabilities of large language models (LLMs). Although the Multi-Agent Debate (MAD) paradigm offers a promising solution by promoting consensus among multiple agents to enhance reliability, it relies on the unrealistic assumption that all debaters are rational and reflective, which is a condition that may not hold when agents themselves are prone to hallucinations. To address this gap, we introduce the Multi-agent Undercover Gaming (MUG) protocol, inspired by social deduction games like "Who is Undercover?". MUG reframes MAD as a process of detecting "undercover" agents (those suffering from hallucinations) by employing multimodal counterfactual tests. Specifically, we modify reference images to introduce counterfactual evidence and observe whether agents can accurately identify these changes, providing ground-truth for identifying hallucinating agents and enabling robust, crowd-powered multimodal reasoning. MUG advances MAD protocols along three key dimensions: (1) enabling factual verification beyond statistical consensus through counterfactual testing; (2) introducing cross-evidence reasoning via dynamically modified evidence sources instead of relying on static inputs; and (3) fostering active reasoning, where agents engage in probing discussions rather than passively answering questions. Collectively, these innovations offer a more reliable and effective framework for multimodal reasoning in LLMs. The source code can be accessed at https://github.com/YongLD/MUG.git.

</details>


### [90] [Aligning Machiavellian Agents: Behavior Steering via Test-Time Policy Shaping](https://arxiv.org/abs/2511.11551)
*Dena Mujtaba,Brian Hu,Anthony Hoogs,Arslan Basharat*

Main category: cs.AI

TL;DR: 提出基于模型引导策略调整的测试时对齐技术，有效控制AI代理伦理属性且无需重新训练


<details>
  <summary>Details</summary>
Motivation: 传统AI代理在复杂环境中可能因单纯追求奖励最大化产生伦理偏差，预训练模型重新对齐成本高昂且存在多属性冲突问题

Method: 使用MACHIAVELLI基准的134个文本游戏环境，通过测试时策略调整（场景-动作属性分类器）实现伦理约束与奖励的权衡

Result: 相比训练时对齐方法，测试时策略调整在伦理违规减少和权力寻求行为控制方面效果提升显著，适用于多样化环境

Conclusion: 测试时策略调整为伦理对齐提供了可扩展解决方案，实现了伦理约束与任务效能的最优化平衡

Abstract: The deployment of decision-making AI agents presents a critical challenge in maintaining alignment with human values or guidelines while operating in complex, dynamic environments. Agents trained solely to achieve their objectives may adopt harmful behavior, exposing a key trade-off between maximizing the reward function and maintaining the alignment. For the pre-trained agents, ensuring alignment is particularly challenging, as retraining can be a costly and slow process. This is further complicated by the diverse and potentially conflicting attributes representing the ethical values for alignment. To address these challenges, we propose a test-time alignment technique based on model-guided policy shaping. Our method allows precise control over individual behavioral attributes, generalizes across diverse reinforcement learning (RL) environments, and facilitates a principled trade-off between ethical alignment and reward maximization without requiring agent retraining. We evaluate our approach using the MACHIAVELLI benchmark, which comprises 134 text-based game environments and thousands of annotated scenarios involving ethical decisions. The RL agents are first trained to maximize the reward in their respective games. At test time, we apply policy shaping via scenario-action attribute classifiers to ensure decision alignment with ethical attributes. We compare our approach against prior training-time methods and general-purpose agents, as well as study several types of ethical violations and power-seeking behavior. Our results demonstrate that test-time policy shaping provides an effective and scalable solution for mitigating unethical behavior across diverse environments and alignment attributes.

</details>
