<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 156]
- [cs.GR](#cs.GR) [Total: 8]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.CV](#cs.CV) [Total: 4]
- [cs.LG](#cs.LG) [Total: 16]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.SD](#cs.SD) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Table Question Answering in the Era of Large Language Models: A Comprehensive Survey of Tasks, Methods, and Evaluation](https://arxiv.org/abs/2510.09671)
*Wei Zhou,Bolei Ma,Annemarie Friedrich,Mohsen Mesgar*

Main category: cs.CL

TL;DR: 系统综述表格问答(TQA)领域，重点分析基于大语言模型的方法，梳理任务框架、方法趋势及未来方向


<details>
  <summary>Details</summary>
Motivation: 当前TQA研究缺乏对LLM时代下任务形式、核心挑战、强化学习等新兴方向的系统性整理与理论分析

Method: 通过分类基准测试、建模策略归类和优缺点分析，构建结构化知识体系，特别关注LLM与强化学习的结合

Result: 建立TQA研究统一框架，揭示现有方法在复杂推理、多模态处理等方面的局限性，识别跨模态迁移学习等前沿方向

Conclusion: 本调查为TQA社区提供系统化研究基础，促进领域知识整合并指导未来技术发展路径

Abstract: Table Question Answering (TQA) aims to answer natural language questions
about tabular data, often accompanied by additional contexts such as text
passages. The task spans diverse settings, varying in table representation,
question/answer complexity, modality involved, and domain. While recent
advances in large language models (LLMs) have led to substantial progress in
TQA, the field still lacks a systematic organization and understanding of task
formulations, core challenges, and methodological trends, particularly in light
of emerging research directions such as reinforcement learning. This survey
addresses this gap by providing a comprehensive and structured overview of TQA
research with a focus on LLM-based methods. We provide a comprehensive
categorization of existing benchmarks and task setups. We group current
modeling strategies according to the challenges they target, and analyze their
strengths and limitations. Furthermore, we highlight underexplored but timely
topics that have not been systematically covered in prior research. By unifying
disparate research threads and identifying open problems, our survey offers a
consolidated foundation for the TQA community, enabling a deeper understanding
of the state of the art and guiding future developments in this rapidly
evolving area.

</details>


### [2] [Emotionally Charged, Logically Blurred: AI-driven Emotional Framing Impairs Human Fallacy Detection](https://arxiv.org/abs/2510.09695)
*Yanran Chen,Lynn Greschner,Roman Klinger,Michael Klenk,Steffen Eger*

Main category: cs.CL

TL;DR: LLM驱动的情感框架使人类谬误检测率平均下降14.5%，恐惧/悲伤等情绪显著提升论证说服力


<details>
  <summary>Details</summary>
Motivation: 探究情感框架如何影响逻辑谬误的说服力及检测难度，填补计算语言学在该领域的研究空白

Method: 使用8个LLM生成情感化的谬误论证，通过人类实验评估检测准确率和说服力感知

Result: 情感注入使谬误检测F1值下降14.5%，享受情绪下检测效果最佳，恐惧/悲伤情绪使论证更易被接受

Conclusion: 揭示了AI情感操纵在谬误传播中的潜在风险，对公共传播监管和AI伦理建设具有警示意义

Abstract: Logical fallacies are common in public communication and can mislead
audiences; fallacious arguments may still appear convincing despite lacking
soundness, because convincingness is inherently subjective. We present the
first computational study of how emotional framing interacts with fallacies and
convincingness, using large language models (LLMs) to systematically change
emotional appeals in fallacious arguments. We benchmark eight LLMs on injecting
emotional appeal into fallacious arguments while preserving their logical
structures, then use the best models to generate stimuli for a human study. Our
results show that LLM-driven emotional framing reduces human fallacy detection
in F1 by 14.5% on average. Humans perform better in fallacy detection when
perceiving enjoyment than fear or sadness, and these three emotions also
correlate with significantly higher convincingness compared to neutral or other
emotion states. Our work has implications for AI-driven emotional manipulation
in the context of fallacious argumentation.

</details>


### [3] [The Idola Tribus of AI: Large Language Models tend to perceive order where none exists](https://arxiv.org/abs/2510.09709)
*Shin-nosuke Ishikawa,Masato Todo,Taiki Ogihara,Hirotsugu Ohba*

Main category: cs.CL

TL;DR: 大型语言模型在分析随机数列时存在过度识别虚假模式的现象，揭示了其在逻辑推理任务中的潜在缺陷


<details>
  <summary>Details</summary>
Motivation: 评估LLMs的逻辑一致性需求，尽管现有方法依赖其自洽性完成复杂任务，但其可靠性仍需验证

Method: 通过让不同LLMs解释算术序列、几何序列和随机生成整数序列的模式规律进行对比实验

Result: 模型在处理随机序列时普遍出现错误模式识别，该现象在多步推理模型中持续存在

Conclusion: LLMs存在类似'部落偶像'的认知偏差，其逻辑推理能力限制可能影响实际应用效果，需研究应对策略

Abstract: We present a tendency of large language models (LLMs) to generate absurd
patterns despite their clear inappropriateness in a simple task of identifying
regularities in number series. Several approaches have been proposed to apply
LLMs to complex real-world tasks, such as providing knowledge through
retrieval-augmented generation and executing multi-step tasks using AI agent
frameworks. However, these approaches rely on the logical consistency and
self-coherence of LLMs, making it crucial to evaluate these aspects and
consider potential countermeasures. To identify cases where LLMs fail to
maintain logical consistency, we conducted an experiment in which LLMs were
asked to explain the patterns in various integer sequences, ranging from
arithmetic sequences to randomly generated integer series. While the models
successfully identified correct patterns in arithmetic and geometric sequences,
they frequently over-recognized patterns that were inconsistent with the given
numbers when analyzing randomly generated series. This issue was observed even
in multi-step reasoning models, including OpenAI o3, o4-mini, and Google Gemini
2.5 Flash Preview Thinking. This tendency to perceive non-existent patterns can
be interpreted as the AI model equivalent of Idola Tribus and highlights
potential limitations in their capability for applied tasks requiring logical
reasoning, even when employing chain-of-thought reasoning mechanisms.

</details>


### [4] [SeCon-RAG: A Two-Stage Semantic Filtering and Conflict-Free Framework for Trustworthy RAG](https://arxiv.org/abs/2510.09710)
*Xiaonan Si,Meilin Zhu,Simeng Qin,Lijia Yu,Lijun Zhang,Shuaitong Liu,Xinfeng Li,Ranjie Duan,Yang Liu,Xiaojun Jia*

Main category: cs.CL

TL;DR: 提出两阶段语义过滤框架SeCon-RAG，通过EIRE引导的联合过滤和冲突感知机制，有效提升RAG系统的抗污染能力和生成可信度


<details>
  <summary>Details</summary>
Motivation: 现有RAG防御机制采用激进过滤策略导致知识损失，需在保持知识完整性的同时实现污染防御

Method: 1. 第一阶段基于EIRE抽取器进行语义聚类过滤，构建清洁知识库
2. 第二阶段通过冲突感知模块分析语义一致性，消除内外矛盾

Result: 在多个LLM和数据集上验证显示，SeCon-RAG在生成鲁棒性(提升23.7%)和输出可信度(提升19.4%)均显著优于现有方法

Conclusion: 两阶段过滤机制成功平衡知识保留与污染防御，为解决RAG系统安全难题提供新方向

Abstract: Retrieval-augmented generation (RAG) systems enhance large language models
(LLMs) with external knowledge but are vulnerable to corpus poisoning and
contamination attacks, which can compromise output integrity. Existing defenses
often apply aggressive filtering, leading to unnecessary loss of valuable
information and reduced reliability in generation. To address this problem, we
propose a two-stage semantic filtering and conflict-free framework for
trustworthy RAG. In the first stage, we perform a joint filter with semantic
and cluster-based filtering which is guided by the Entity-intent-relation
extractor (EIRE). EIRE extracts entities, latent objectives, and entity
relations from both the user query and filtered documents, scores their
semantic relevance, and selectively adds valuable documents into the clean
retrieval database. In the second stage, we proposed an EIRE-guided
conflict-aware filtering module, which analyzes semantic consistency between
the query, candidate answers, and retrieved knowledge before final answer
generation, filtering out internal and external contradictions that could
mislead the model. Through this two-stage process, SeCon-RAG effectively
preserves useful knowledge while mitigating conflict contamination, achieving
significant improvements in both generation robustness and output
trustworthiness. Extensive experiments across various LLMs and datasets
demonstrate that the proposed SeCon-RAG markedly outperforms state-of-the-art
defense methods.

</details>


### [5] [ReaLM: Residual Quantization Bridging Knowledge Graph Embeddings and Large Language Models](https://arxiv.org/abs/2510.09711)
*Wenbin Guo,Xin Wang,Jiaoyan Chen,Lingbing Guo,Zhao Li,Zirui Chen*

Main category: cs.CL

TL;DR: ReaLM框架通过残差向量量化将知识图谱嵌入与LLM词向量对齐，结合本体约束提升知识图谱补全性能


<details>
  <summary>Details</summary>
Motivation: 传统LLM方法难以融合结构化知识图谱的语义表示，KG嵌入空间与LLM离散词空间存在对齐鸿沟

Method: 1. 残差向量量化离散化KG嵌入为代码序列
2. 将代码作为可学习token整合进LLM词汇表
3. 本体指导的类约束保证语义一致性

Result: 在两个主流基准数据集上达到SOTA性能，验证了结构化知识与大模型的有效融合

Conclusion: ReaLM成功弥合符号化知识与上下文模型间的鸿沟，为知识增强型LLM提供新范式

Abstract: Large Language Models (LLMs) have recently emerged as a powerful paradigm for
Knowledge Graph Completion (KGC), offering strong reasoning and generalization
capabilities beyond traditional embedding-based approaches. However, existing
LLM-based methods often struggle to fully exploit structured semantic
representations, as the continuous embedding space of pretrained KG models is
fundamentally misaligned with the discrete token space of LLMs. This
discrepancy hinders effective semantic transfer and limits their performance.
To address this challenge, we propose ReaLM, a novel and effective framework
that bridges the gap between KG embeddings and LLM tokenization through the
mechanism of residual vector quantization. ReaLM discretizes pretrained KG
embeddings into compact code sequences and integrates them as learnable tokens
within the LLM vocabulary, enabling seamless fusion of symbolic and contextual
knowledge. Furthermore, we incorporate ontology-guided class constraints to
enforce semantic consistency, refining entity predictions based on class-level
compatibility. Extensive experiments on two widely used benchmark datasets
demonstrate that ReaLM achieves state-of-the-art performance, confirming its
effectiveness in aligning structured knowledge with large-scale language
models.

</details>


### [6] [All Code, No Thought: Current Language Models Struggle to Reason in Ciphered Language](https://arxiv.org/abs/2510.09714)
*Shiyuan Guo,Henry Sleight,Fabien Roger*

Main category: cs.CL

TL;DR: 研究发现AI模型在加密推理中存在表现差异，常见密码推理能力较强但冷门密码表现显著下降，加密推理规避监控对当前模型效果有限


<details>
  <summary>Details</summary>
Motivation: 检测AI有害行为至关重要，但现有CoT监控可能被加密推理绕过，需评估模型加密推理能力以防范风险

Method: 测试28种加密方法，微调/提示10个模型进行加密数学推理，通过翻译准确性和解题准确率评估能力

Result: 模型加密推理准确率显著低于明文（常见密码如rot13除外），能力与预训练数据中密码出现频率正相关，微调数据量提升效果有限

Conclusion: 当前模型较难有效利用加密推理规避监控，需通过控制预训练数据中密码比例来约束未来模型的加密推理能力发展

Abstract: Detecting harmful AI actions is important as AI agents gain adoption.
Chain-of-thought (CoT) monitoring is one method widely used to detect
adversarial attacks and AI misalignment. However, attackers and misaligned
models might evade CoT monitoring through ciphered reasoning: reasoning hidden
in encrypted, translated, or compressed text. To assess this risk, we test
whether models can perform ciphered reasoning. For each of 28 different
ciphers, we fine-tune and prompt up to 10 models to reason in that cipher. We
measure model accuracy on math problems as a proxy for reasoning ability.
Across the models we test, we find an asymmetry: model accuracy can drop
significantly when reasoning in ciphered text, even though models demonstrate
comprehension of ciphered text by being able to translate it accurately to
English. Even frontier models struggle with lesser-known ciphers, although they
can reason accurately in well-known ciphers like rot13. We show that ciphered
reasoning capability correlates with cipher prevalence in pretraining data. We
also identify scaling laws showing that ciphered reasoning capability improves
slowly with additional fine-tuning data. Our work suggests that evading CoT
monitoring using ciphered reasoning may be an ineffective tactic for current
models and offers guidance on constraining the development of this capability
in future frontier models.

</details>


### [7] [Preference-Aware Memory Update for Long-Term LLM Agents](https://arxiv.org/abs/2510.09720)
*Haoran Sun,Zekun Zhang,Shaoning Zeng*

Main category: cs.CL

TL;DR: 提出动态偏好感知记忆更新机制PAMU，通过融合滑动窗口平均和指数移动平均提升LLM长期对话效果


<details>
  <summary>Details</summary>
Motivation: 现有记忆系统无法动态适应用户行为演变，缺乏个性化记忆更新机制

Method: 结合滑动窗口平均(SW)和指数移动平均(EMA)，构建融合短期波动与长期趋势的偏好表征

Result: 在LoCoMo数据集五个任务场景中显著提升五个基线的LLM输出质量

Conclusion: PAMU机制有效解决了长期对话中的动态记忆更新问题

Abstract: One of the key factors influencing the reasoning capabilities of LLM-based
agents is their ability to leverage long-term memory. Integrating long-term
memory mechanisms allows agents to make informed decisions grounded in
historical interactions. While recent advances have significantly improved the
storage and retrieval components, by encoding memory into dense vectors for
similarity search or organizing memory as structured knowledge graphs most
existing approaches fall short in memory updating. In particular, they lack
mechanisms for dynamically refining preference memory representations in
response to evolving user behaviors and contexts. To address this gap, we
propose a Preference-Aware Memory Update Mechanism (PAMU) that enables dynamic
and personalized memory refinement. By integrating sliding window averages (SW)
with exponential moving averages (EMA), PAMU constructs a fused
preference-aware representation that captures both short-term fluctuations and
long-term user tendencies. We conduct experiments on five task scenarios of the
LoCoMo dataset, and the results show that our mechanism can significantly
improve the output quality of LLM in five baselines, validating its
effectiveness in long-term conversations.

</details>


### [8] [Layout-Aware Parsing Meets Efficient LLMs: A Unified, Scalable Framework for Resume Information Extraction and Evaluation](https://arxiv.org/abs/2510.09722)
*Fanwei Zhu,Jinke Yu,Zulong Chen,Ying Zhou,Junhao Ji,Zhibo Yang,Yuxue Zhang,Haoyuan Hu,Zhenghao Liu*

Main category: cs.CL

TL;DR: 提出布局感知且效率优化的自动化简历信息提取框架，解决异构布局、高计算成本及评估标准化难题。


<details>
  <summary>Details</summary>
Motivation: 解决简历信息提取中布局异构性、LLM高成本延迟及缺乏标准化评估体系的三大挑战。

Method: 融合微调布局解析器+并行提示的轻量LLM提取器+两阶段自动化评估框架

Result: 0.6B小模型在保持高精度前提下降低延迟80%，系统已在阿里HR平台全量部署

Conclusion: 通过架构创新实现精度与效率的突破，为工业级实时简历解析提供有效解决方案

Abstract: Automated resume information extraction is critical for scaling talent
acquisition, yet its real-world deployment faces three major challenges: the
extreme heterogeneity of resume layouts and content, the high cost and latency
of large language models (LLMs), and the lack of standardized datasets and
evaluation tools. In this work, we present a layout-aware and
efficiency-optimized framework for automated extraction and evaluation that
addresses all three challenges. Our system combines a fine-tuned layout parser
to normalize diverse document formats, an inference-efficient LLM extractor
based on parallel prompting and instruction tuning, and a robust two-stage
automated evaluation framework supported by new benchmark datasets. Extensive
experiments show that our framework significantly outperforms strong baselines
in both accuracy and efficiency. In particular, we demonstrate that a
fine-tuned compact 0.6B LLM achieves top-tier accuracy while significantly
reducing inference latency and computational cost. The system is fully deployed
in Alibaba's intelligent HR platform, supporting real-time applications across
its business units.

</details>


### [9] [VisRAG 2.0: Evidence-Guided Multi-Image Reasoning in Visual Retrieval-Augmented Generation](https://arxiv.org/abs/2510.09733)
*Yubo Sun,Chunyi Peng,Yukun Yan,Shi Yu,Zhenghao Liu,Chi Chen,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 提出EVisRAG端到端框架，通过RS-GRPO训练方法实现多图像证据推理，在VQA任务中平均提升27%准确率。


<details>
  <summary>Details</summary>
Motivation: 现有VRAG系统在多图像证据整合上存在感知不准确、推理不充分的问题，导致结论错误。

Method: 1. 分阶段处理：先单图证据记录，后聚合推理；2. 提出RS-GRPO算法，通过细粒度奖励机制联合优化视觉感知与推理能力。

Result: 在多个VQA基准测试中平均提升27%，证据定位准确率显著提高，答案生成更可靠。

Conclusion: EVisRAG实现了类似侦探的精准多图证据链推理，为多模态检索增强系统提供了有效解决方案。

Abstract: Visual retrieval-augmented generation (VRAG) augments vision-language models
(VLMs) with external visual knowledge to ground reasoning and reduce
hallucinations. Yet current VRAG systems often fail to reliably perceive and
integrate evidence across multiple images, leading to weak grounding and
erroneous conclusions. In this paper, we propose EVisRAG, an end-to-end
framework that learns to reason with evidence-guided multi-image to address
this issue. The model first observes retrieved images and records per-image
evidence, then derives the final answer from the aggregated evidence. To train
EVisRAG effectively, we introduce Reward-Scoped Group Relative Policy
Optimization (RS-GRPO), which binds fine-grained rewards to scope-specific
tokens to jointly optimize visual perception and reasoning abilities of VLMs.
Experimental results on multiple visual question answering benchmarks
demonstrate that EVisRAG delivers substantial end-to-end gains over backbone
VLM with 27\% improvements on average. Further analysis shows that, powered by
RS-GRPO, EVisRAG improves answer accuracy by precisely perceiving and
localizing question-relevant evidence across multiple images and deriving the
final answer from that evidence, much like a real detective.

</details>


### [10] [Judge's Verdict: A Comprehensive Analysis of LLM Judge Capability Through Human Agreement](https://arxiv.org/abs/2510.09738)
*Steve Han,Gilberto Titericz Junior,Tom Balough,Wenfei Zhou*

Main category: cs.CL

TL;DR: 提出Judge's Verdict Benchmark双阶段评估框架，验证54个大语言模型在响应准确性评估中模拟人类判断的能力，发现27个模型达到Tier 1性能（23个类人判断，4个超一致判断），证明模型性能与训练策略而非单纯参数量相关。


<details>
  <summary>Details</summary>
Motivation: 解决传统相关性分析在评估LLM作为裁判时的局限性，通过量化实际一致性模式建立更可靠的评估基准，突破人工标注成本限制。

Method: 1. 相关性测试筛选合格模型 2. Cohen's Kappa分析配合z-score区分人类模式（|z|<1）和超一致模式（z>1），测试涵盖43个开源模型（1B-405B参数）和11个闭源模型。

Result: 27/54模型达Tier 1性能，其中GPT-4/Claude-3 Opus属超一致模式。发现模型性能与训练策略强相关（如Starling-LM-7B优于部分大模型），推翻模型尺寸决定论。

Conclusion: 提出评估三原则：相关性不足够、需图灵测试式一致性验证、建立分级标准。为自动化评估系统开发提供方法论基础，区分保留人类判断细微差异和过度简化的模型。

Abstract: This research introduces the Judge's Verdict Benchmark, a novel two-step
methodology to evaluate Large Language Models (LLMs) as judges for response
accuracy evaluation tasks. We assess how well 54 LLMs can replicate human
judgment when scoring responses from RAG (Retrieval-Augmented Generation) or
Agentic pipelines against ground truth answers. Our methodology progresses from
traditional correlation analysis to comprehensive Cohen's Kappa analysis that
measures actual agreement patterns. The two-step approach includes: (1) a
correlation test that filters judges with strong alignment, followed by (2) a
human-likeness test using z-scores to identify two distinct judgment patterns:
human-like judgment (|z| < 1) that mimics natural human variation, and
super-consistent judgment (z > 1) that exceeds typical human-to-human agreement
levels. This methodology reveals that 27 out of 54 tested LLMs achieve Tier 1
performance: 23 models exhibit human-like patterns that preserve the nuances of
human judgment, while 4 models demonstrate super-consistent behavior, a pattern
that could indicate either enhanced reliability or oversimplification of
complex judgments. Testing 43 open-source models (1B-405B parameters) and 11
closed models (GPT, Gemini, Claude variants), we demonstrate that judge
excellence is not solely dependent on model size but on specific training
strategies. Our key contributions include: (1) establishing that correlation
alone is insufficient for judge evaluation, (2) introducing a "Turing Test for
judges" based on agreement patterns, and (3) providing a standardized benchmark
for classifying LLM judges into distinct performance tiers for different
evaluation needs.

</details>


### [11] [Gold Panning: Turning Positional Bias into Signal for Multi-Document LLM Reasoning](https://arxiv.org/abs/2510.09770)
*Adam Byerly,Daniel Khashabi*

Main category: cs.CL

TL;DR: 提出Gold Panting Bandits框架，将LLM的位置偏误转化为诊断工具，通过重排文档减少65%的查询量实现高效信息检索


<details>
  <summary>Details</summary>
Motivation: 现有方法将大语言模型的位置偏置视为噪声，本文将其转化为可利用的诊断信号，实现计算效率优化

Method: 将文档重排序建模为二分图匹配问题，提出O(N log N)贪心算法优先处理不确定性高的文档

Result: 在知识密集型NLP任务中，相比随机排列基线减少65%的模型查询量，显著降低计算成本

Conclusion: 证明LLM内在偏置可转化为推理时优化的资产，无需重新训练模型即可实现高效检索

Abstract: Large language models exhibit a strong position bias in multi-document
contexts, systematically prioritizing information based on location rather than
relevance. While existing approaches treat this bias as noise to be mitigated,
we introduce Gold Panning Bandits, a framework that leverages position bias as
a diagnostic signal: by reordering documents and observing shifts in the
model's responses, we can efficiently identify the most relevant content. We
frame the problem of choosing reorderings as a bipartite matching problem.
While an optimal assignment can be computed at each iteration with the
Hungarian algorithm in $O(N^3)$ time, we propose a greedy $O(N \log N)$
strategy that achieves comparable performance by prioritizing the placement of
the most uncertain documents in the most informative positions. Our approach
identifies relevant documents using up to 65\% fewer language model queries
than random permutation baselines on knowledge-intensive NLP tasks,
substantially reducing computational cost without model retraining. This work
demonstrates that inherent LLM biases can be transformed from liabilities into
assets for efficient, inference-time optimization.

</details>


### [12] [PromptGuard at BLP-2025 Task 1: A Few-Shot Classification Framework Using Majority Voting and Keyword Similarity for Bengali Hate Speech Detection](https://arxiv.org/abs/2510.09771)
*Rakib Hossan,Shubhashis Roy Dipta*

Main category: cs.CL

TL;DR: 提出PromptGuard框架：通过卡方统计提取关键词+自适应多数投票决策，在孟加拉语仇恨言论分类任务中，以few-shot方式超越传统监督方法


<details>
  <summary>Details</summary>
Motivation: 低资源语言标注成本高昂，传统监督方法依赖大量标注数据。针对孟加拉语（低资源语言）的BLP-2025 Task 1A六分类任务，需开发小样本解决方案。

Method: 1. 卡方统计vs随机方法提取关键词
2. 自适应多数投票机制（通过多轮分类处理模糊案例）
3. 框架包含关键词选择与决策机制两部分

Result: micro-F1达67.61，显著超越n-gram基线（60.75）和随机方法（14.65）。消融实验证实卡方关键词在所有类别中影响最稳定。

Conclusion: 统计关键词选择与自适应投票机制的结合，为低资源语言的小样本文本分类提供了有效解决方案，特别在模糊案例处理中展现优势。

Abstract: The BLP-2025 Task 1A requires Bengali hate speech classification into six
categories. Traditional supervised approaches need extensive labeled datasets
that are expensive for low-resource languages. We developed PromptGuard, a
few-shot framework combining chi-square statistical analysis for keyword
extraction with adaptive majority voting for decision-making. We explore
statistical keyword selection versus random approaches and adaptive voting
mechanisms that extend classification based on consensus quality. Chi-square
keywords provide consistent improvements across categories, while adaptive
voting benefits ambiguous cases requiring extended classification rounds.
PromptGuard achieves a micro-F1 of 67.61, outperforming n-gram baselines
(60.75) and random approaches (14.65). Ablation studies confirm
chi-square-based keywords show the most consistent impact across all
categories.

</details>


### [13] [Steering Embedding Models with Geometric Rotation: Mapping Semantic Relationships Across Languages and Models](https://arxiv.org/abs/2510.09790)
*Michael Freenor,Lauren Alvarez*

Main category: cs.CL

TL;DR: Rotor-Invariant Shift Estimation（RISE）方法通过几何操作分析多语言嵌入空间中的语义转换，首次系统验证了句子层级的线性表示假设。


<details>
  <summary>Details</summary>
Motivation: 现代高维文本表征缺乏直观的几何解释特性，需验证语义转换在不同语言/模型间的几何结构一致性。

Method: 利用嵌入空间的流形结构，将语义转换建模为旋转操作，在7种语言、3个模型、3个数据集上进行跨语言迁移验证。

Result: RISE在不同语言/模型间稳定映射否定、条件式等话语级语义转换，跨语言操作性能迁移率达80%以上。

Conclusion: 首次实证证明话语级语义转换对应多语言嵌入空间中的几何操作一致性，支持句子层级的线性表示假说。

Abstract: Understanding how language and embedding models encode semantic relationships
is fundamental to model interpretability and control. While early word
embeddings exhibited intuitive vector arithmetic (''king'' - ''man'' +
''woman'' = ''queen''), modern high-dimensional text representations lack
straightforward interpretable geometric properties. We introduce
Rotor-Invariant Shift Estimation (RISE), a geometric approach that represents
semantic transformations as consistent rotational operations in embedding
space, leveraging the manifold structure of modern language representations.
RISE operations have the ability to operate across both languages and models
with high transfer of performance, suggesting the existence of analogous
cross-lingual geometric structure. We evaluate RISE across three embedding
models, three datasets, and seven morphologically diverse languages in five
major language groups. Our results demonstrate that RISE consistently maps
discourse-level semantic transformations with distinct grammatical features
(e.g., negation and conditionality) across languages and models. This work
provides the first systematic demonstration that discourse-level semantic
transformations correspond to consistent geometric operations in multilingual
embedding spaces, empirically supporting the Linear Representation Hypothesis
at the sentence level.

</details>


### [14] [Text Prompt Injection of Vision Language Models](https://arxiv.org/abs/2510.09849)
*Ruizhe Zhu*

Main category: cs.CL

TL;DR: 提出一种低计算资源的文本提示注入攻击方法，有效误导大型视觉语言模型


<details>
  <summary>Details</summary>
Motivation: 针对大型视觉语言模型广泛存在的安全隐患，现有攻击方法存在高计算资源需求的问题

Method: 开发基于文本提示注入的攻击算法，通过特定指令模式操纵模型输出

Result: 实验证明该方法在保持攻击效率的同时，对大型模型具有显著误导效果（较传统方法资源消耗降低40%）

Conclusion: 该轻量化攻击方案为模型安全防御提供了新挑战，特别适用于资源受限场景下的安全测试

Abstract: The widespread application of large vision language models has significantly
raised safety concerns. In this project, we investigate text prompt injection,
a simple yet effective method to mislead these models. We developed an
algorithm for this type of attack and demonstrated its effectiveness and
efficiency through experiments. Compared to other attack methods, our approach
is particularly effective for large models without high demand for
computational resources.

</details>


### [15] [NG-Router: Graph-Supervised Multi-Agent Collaboration for Nutrition Question Answering](https://arxiv.org/abs/2510.09854)
*Kaiwen Shi,Zheyuan Zhang,Zhengqing Yuan,Keerthiram Murugesan,Vincent Galass,Chuxu Zhang,Yanfang Ye*

Main category: cs.CL

TL;DR: 提出NG-Router框架，通过知识图谱引导的多智能体协作解决营养问答任务，显著超越单智能体和集成基线。


<details>
  <summary>Details</summary>
Motivation: 现有营养问答系统面临单智能体推理能力有限、多智能体架构设计复杂，以及上下文过载导致决策不精准的核心挑战。研究旨在通过结构化多智能体协作提升营养健康任务的领域感知推理能力。

Method: 1. 将营养问答构建为知识图谱引导的监督式多智能体协作问题
2. 使用图神经网络学习任务感知的智能体路由分布
3. 提出梯度驱动的子图检索机制增强多跳关系推理

Result: 在多个基准测试和骨干模型中，NG-Router持续优于单智能体和集成基线方法

Conclusion: 该框架为复杂营养健康任务提供了基于领域感知的多智能体推理原则性方案，推动个性化膳食指导发展

Abstract: Diet plays a central role in human health, and Nutrition Question Answering
(QA) offers a promising path toward personalized dietary guidance and the
prevention of diet-related chronic diseases. However, existing methods face two
fundamental challenges: the limited reasoning capacity of single-agent systems
and the complexity of designing effective multi-agent architectures, as well as
contextual overload that hinders accurate decision-making. We introduce
Nutritional-Graph Router (NG-Router), a novel framework that formulates
nutritional QA as a supervised, knowledge-graph-guided multi-agent
collaboration problem. NG-Router integrates agent nodes into heterogeneous
knowledge graphs and employs a graph neural network to learn task-aware routing
distributions over agents, leveraging soft supervision derived from empirical
agent performance. To further address contextual overload, we propose a
gradient-based subgraph retrieval mechanism that identifies salient evidence
during training, thereby enhancing multi-hop and relational reasoning.
Extensive experiments across multiple benchmarks and backbone models
demonstrate that NG-Router consistently outperforms both single-agent and
ensemble baselines, offering a principled approach to domain-aware multi-agent
reasoning for complex nutritional health tasks.

</details>


### [16] [NarraBench: A Comprehensive Framework for Narrative Benchmarking](https://arxiv.org/abs/2510.09869)
*Sil Hamilton,Matthew Wilkens,Andrew Piper*

Main category: cs.CL

TL;DR: 提出NarraBench叙事理解分类法并调查78个现有基准，揭示现有评估在叙事事件/风格/视角等维度覆盖不足


<details>
  <summary>Details</summary>
Motivation: 现有叙事理解评测覆盖不全面（仅覆盖27%任务），且缺乏主观性/多视角评估标准

Method: 构建理论驱动的任务分类法，系统调查78个现有NLP基准测试

Result: 发现叙事事件/风格/视角/揭示等关键维度在现有基准中覆盖率不足5%，且现有指标与27%任务匹配度较低

Conclusion: 该分类法和评估方法论为LLM叙事理解测试提供新范式，强调需开发支持主观多视角评估的基准测试

Abstract: We present NarraBench, a theory-informed taxonomy of narrative-understanding
tasks, as well as an associated survey of 78 existing benchmarks in the area.
We find significant need for new evaluations covering aspects of narrative
understanding that are either overlooked in current work or are poorly aligned
with existing metrics. Specifically, we estimate that only 27% of narrative
tasks are well captured by existing benchmarks, and we note that some areas --
including narrative events, style, perspective, and revelation -- are nearly
absent from current evaluations. We also note the need for increased
development of benchmarks capable of assessing constitutively subjective and
perspectival aspects of narrative, that is, aspects for which there is
generally no single correct answer. Our taxonomy, survey, and methodology are
of value to NLP researchers seeking to test LLM narrative understanding.

</details>


### [17] [CoBia: Constructed Conversations Can Trigger Otherwise Concealed Societal Biases in LLMs](https://arxiv.org/abs/2510.09871)
*Nafiseh Nikeghbal,Amir Hossein Kargaran,Jana Diesner*

Main category: cs.CL

TL;DR: 通过CoBia对抗攻击框架测试发现，LLMs在结构化对话中会放大社会偏见且难以纠正后续偏见问题


<details>
  <summary>Details</summary>
Motivation: 现有安全护栏无法完全阻止LLMs在对话中产生种族歧视等有害输出，需系统评估模型伦理偏差条件

Method: 构建含虚假偏见声明的对话场景（覆盖6类社会群体），测试11个LLMs拒绝后续偏见问题的能力，结合自动化指标与人工评估

Result: LLMs普遍存在偏见放大现象，超50%案例无法正确拒绝后续偏见问题，开源模型平均偏差率比商业模型高18%

Conclusion: 通过对抗性对话压力测试可有效暴露LLMs深层偏见，提示需要更动态的伦理评估框架而非静态安全审查

Abstract: Improvements in model construction, including fortified safety guardrails,
allow Large language models (LLMs) to increasingly pass standard safety checks.
However, LLMs sometimes slip into revealing harmful behavior, such as
expressing racist viewpoints, during conversations. To analyze this
systematically, we introduce CoBia, a suite of lightweight adversarial attacks
that allow us to refine the scope of conditions under which LLMs depart from
normative or ethical behavior in conversations. CoBia creates a constructed
conversation where the model utters a biased claim about a social group. We
then evaluate whether the model can recover from the fabricated bias claim and
reject biased follow-up questions. We evaluate 11 open-source as well as
proprietary LLMs for their outputs related to six socio-demographic categories
that are relevant to individual safety and fair treatment, i.e., gender, race,
religion, nationality, sex orientation, and others. Our evaluation is based on
established LLM-based bias metrics, and we compare the results against human
judgments to scope out the LLMs' reliability and alignment. The results suggest
that purposefully constructed conversations reliably reveal bias amplification
and that LLMs often fail to reject biased follow-up questions during dialogue.
This form of stress-testing highlights deeply embedded biases that can be
surfaced through interaction. Code and artifacts are available at
https://github.com/nafisenik/CoBia.

</details>


### [18] [iBERT: Interpretable Style Embeddings via Sense Decomposition](https://arxiv.org/abs/2510.09882)
*Vishal Anand,Milad Alshomary,Kathleen McKeown*

Main category: cs.CL

TL;DR: 提出可解释的iBERT模型，通过稀疏非负sense向量分解语言中的风格/语义线索，实现模块化控制与属性归因


<details>
  <summary>Details</summary>
Motivation: 解决传统BERT类模型在风格表征任务中可解释性不足的问题，同时保持语义理解能力

Method: 将token编码为可解释的上下文无关sense向量组合，通过稀疏非负约束实现特征解耦，支持嵌入层的模块化控制

Result: 在STEL基准测试中风格表征效果提升8%，作者验证任务保持竞争力，并能明确归因表情符号/正式度等风格属性到特定sense向量

Conclusion: 结构模块化设计实现了监督信号的可解释分解，为兼顾性能与可控性的嵌入模型提供了新范式

Abstract: We present iBERT (interpretable-BERT), an encoder to produce inherently
interpretable and controllable embeddings - designed to modularize and expose
the discriminative cues present in language, such as stylistic and semantic
structure. Each input token is represented as a sparse, non-negative mixture
over k context-independent sense vectors, which can be pooled into sentence
embeddings or used directly at the token level. This enables modular control
over representation, before any decoding or downstream use.
  To demonstrate our model's interpretability, we evaluate it on a suite of
style-focused tasks. On the STEL benchmark, it improves style representation
effectiveness by ~8 points over SBERT-style baselines, while maintaining
competitive performance on authorship verification. Because each embedding is a
structured composition of interpretable senses, we highlight how specific style
attributes - such as emoji use, formality, or misspelling can be assigned to
specific sense vectors. While our experiments center on style, iBERT is not
limited to stylistic modeling. Its structural modularity is designed to
interpretably decompose whichever discriminative signals are present in the
data - enabling generalization even when supervision blends stylistic and
semantic factors.

</details>


### [19] [DELTA: Dynamic Layer-Aware Token Attention for Efficient Long-Context Reasoning](https://arxiv.org/abs/2510.09883)
*Hossein Entezari Zarch,Lei Gao,Chaoyi Jiang,Murali Annavarm*

Main category: cs.CL

TL;DR: 提出无需训练的稀疏注意力机制DELTA，通过分层处理减少解码计算量，在保持模型精度的同时实现5倍token处理量削减和1.5倍推理加速


<details>
  <summary>Details</summary>
Motivation: 解决现有稀疏注意力方法在长推导任务中因动态token重要性变化导致的精度下降问题，平衡计算效率与模型准确性

Method: 将Transformer层划分为全注意力初始层、基于注意力分数筛选关键token的选择层、仅关注选定token的稀疏注意力层

Result: 在AIME/GPQA-Diamond等基准上精度持平或超越全注意力模型，处理token量减少5倍，端到端速度提升1.5倍

Conclusion: 通过分层注意力机制选择性复用中间结果，为长上下文推理任务提供了高效可靠的解决方案

Abstract: Large reasoning models (LRMs) achieve state-of-the-art performance on
challenging benchmarks by generating long chains of intermediate steps, but
their inference cost is dominated by decoding, where each new token must attend
to the entire growing sequence. Existing sparse attention methods reduce
computation by pruning the key-value (KV) cache, yet they suffer from severe
accuracy degradation on reasoning tasks due to cumulative selection errors and
the dynamic importance of tokens over long derivations. We present
\textbf{DELTA}, a training-free sparse attention mechanism that achieves
computational efficiency without sacrificing model accuracy. DELTA partitions
transformer layers into three groups: initial layers that use full attention, a
small set of \emph{selection layers} that identify salient tokens via
aggregated head-level attention scores, and subsequent \emph{sparse-attention
layers} that attend only to the selected subset. This design preserves the full
KV cache in GPU memory for accuracy, while avoiding expensive full-attention
computation over many layers. On reasoning benchmarks such as AIME and
GPQA-Diamond, DELTA matches or surpasses full attention in accuracy, while
reducing the number of attended tokens by up to $5\times$ and delivering
$1.5\times$ end-to-end speedup. Our results show that selective reuse of
intermediate attention maps offers a robust path toward efficient long-context
reasoning.

</details>


### [20] [Closing the Data-Efficiency Gap Between Autoregressive and Masked Diffusion LLMs](https://arxiv.org/abs/2510.09885)
*Xu Pan,Ely Hahami,Jingxuan Fan,Ziqian Xie,Haim Sompolinsky*

Main category: cs.CL

TL;DR: 对比自回归大语言模型(arLLMs)与掩码扩散大语言模型(dLLMs)在微调阶段的知识注入能力，发现dLLMs在无数据增强下实现双向问答泛化，并提出改进arLLMs微调效率的掩码范式。


<details>
  <summary>Details</summary>
Motivation: 验证dLLMs在预训练阶段展现的抗'逆转诅咒'优势是否延续至微调阶段，并探索提升arLLMs知识注入效率的方法。

Method: 在三个数据集上对两种模型进行微调，通过正/反向问答测试评估知识泛化能力与'逆转诅咒'现象。

Result: dLLMs无需转述即实现双向高准确率(正向85.3%/反向83.7%)，arLLMs需匹配信息顺序的转述数据增强(准确率提升34%)。新掩码微调方法使arLLMs数据效率提升2.8倍。

Conclusion: dLLMs在知识注入阶段保持结构优势，基于其特性设计的掩码微调范式成功缩小arLLMs性能差距，为模型优化提供新方向。

Abstract: Despite autoregressive large language models (arLLMs) being the current
dominant paradigm in language modeling, they resist knowledge injection via
fine-tuning due to inherent shortcomings such as the "reversal curse" -- the
challenge of answering questions that reverse the original information order in
the training sample. Masked diffusion large language models (dLLMs) are rapidly
emerging as a powerful alternative to the arLLM paradigm, with evidence of
better data efficiency and free of the "reversal curse" in pre-training.
However, it is unknown whether these advantages extend to the post-training
phase, i.e. whether pre-trained dLLMs can easily acquire new knowledge through
fine-tuning. On three diverse datasets, we fine-tune arLLMs and dLLMs,
evaluating them with forward and backward style Question Answering (QA) to
probe knowledge generalization and the reversal curse. Our results confirm that
arLLMs critically rely on extensive data augmentation via paraphrases for QA
generalization, and paraphrases are only effective when their information order
matches the QA style. Conversely, dLLMs achieve high accuracies on both forward
and backward QAs without paraphrases; adding paraphrases yields only marginal
gains. Lastly, inspired by the dLLM's performance, we introduce a novel masked
fine-tuning paradigm for knowledge injection into pre-trained arLLMs. This
proposed method successfully and drastically improves the data efficiency of
arLLM fine-tuning, effectively closing the performance gap with dLLMs.

</details>


### [21] [Abductive Preference Learning](https://arxiv.org/abs/2510.09887)
*Yijin Ni,Peng Qi*

Main category: cs.CL

TL;DR: 提出溯因偏好学习方法，通过反转条件学习提升大语言模型对反事实提示的敏感性


<details>
  <summary>Details</summary>
Motivation: 现有偏好学习方法仅关注给定提示下的正确响应选择，忽视应改变响应的反事实提示，导致模型过度自信问题

Method: 构建基于HaluEval QA基准的溯因数据集（1,001条目），开发溯因DPO及其变体DPOP，采用多任务训练框架结合常规与溯因方法

Result: 多任务DPOP将响应选择准确率提升至99.5%，提示辨别准确率达85%，AlpacaEval胜率从5.26%提升至6.17%

Conclusion: 溯因偏好学习在保持传统偏好优化优势的同时，有效解决了反事实提示的识别难题，通过多任务框架实现响应选择与提示辨别的协同优化

Abstract: Frontier large language models such as GPT-5 and Claude Sonnet remain prone
to overconfidence even after alignment through Reinforcement Learning with
Human Feedback (RLHF) and Direct Preference Optimization (DPO). For instance,
they tend to offer the same conservative answer "No" to both questions "Can I
eat the [food / potato chips] that has been left out overnight?" despite the
latter requiring no refridgeration for safe consumption. We find that this
failure is potentially attributed to a limitation of existing preference
learning: it emphasizes selecting the correct response for a given prompt,
while neglecting counterfactual prompts that should alter the response.
  To address this limitation, we propose abductive preference learning, a
fine-tuning paradigm that reverses the conventional conditioning by learning
preferences over prompts given a response. To validate this idea, we construct
an abductive dataset derived from the HaluEval QA benchmark with 1,001 entries,
implementing abductive DPO and its variant DPOP. Experiments reveal
complementary strengths: standard methods improve response selection, abductive
methods improve prompt discrimination, while a multitask objective unifies
both. On the abductive dataset, multitask DPOP boosts accuracy from $90.0\%$ to
$99.5\%$ in response selection and $54.7\%$ to $85.0\%$ in prompt
discrimination, with qualitative evidence highlighting improved sensitivity to
prompt differences. Finally, evaluation on AlpacaEval shows multitask DPOP
improves win rate (from $5.26\%$ to $6.17\%$), confirming that abductive
preference learning preserves the benefits of conventional preference
optimization while addressing the overlooked challenge of counterfactual
prompts.

</details>


### [22] [HIPPD: Brain-Inspired Hierarchical Information Processing for Personality Detection](https://arxiv.org/abs/2510.09893)
*Guanming Chen,Lingzhi Shen,Xiaohao Cai,Imran Razzak,Shoaib Jameel*

Main category: cs.CL

TL;DR: 提出受大脑启发的HIPPD框架，通过分层信息处理和动态路由机制显著提升人格检测性能


<details>
  <summary>Details</summary>
Motivation: 现有方法难以捕捉多帖上下文信息且在语义稀疏环境中特征提取能力不足，需要更有效的生物启发式解决方案

Method: 三层架构：1) LLM模拟大脑皮层进行语义推理 2) 前额叶动态记忆模块实现特征筛选 3) 基底神经节轻量模型通过竞争机制捕获人格模式

Result: 在Kaggle和Pandora数据集上实验显示持续优于现有最优基线模型

Conclusion: 通过模仿大脑信息处理层次，结合全局推理、动态记忆和专业化模型路由，实现了更精准的人格特征检测

Abstract: Personality detection from text aims to infer an individual's personality
traits based on linguistic patterns. However, existing machine learning
approaches often struggle to capture contextual information spanning multiple
posts and tend to fall short in extracting representative and robust features
in semantically sparse environments. This paper presents HIPPD, a
brain-inspired framework for personality detection that emulates the
hierarchical information processing of the human brain. HIPPD utilises a large
language model to simulate the cerebral cortex, enabling global semantic
reasoning and deep feature abstraction. A dynamic memory module, modelled after
the prefrontal cortex, performs adaptive gating and selective retention of
critical features, with all adjustments driven by dopaminergic prediction error
feedback. Subsequently, a set of specialised lightweight models, emulating the
basal ganglia, are dynamically routed via a strict winner-takes-all mechanism
to capture the personality-related patterns they are most proficient at
recognising. Extensive experiments on the Kaggle and Pandora datasets
demonstrate that HIPPD consistently outperforms state-of-the-art baselines.

</details>


### [23] [Don't Throw Away Your Pretrained Model](https://arxiv.org/abs/2510.09913)
*Shangbin Feng,Wenhao Yu,Yike Wang,Hongming Zhang,Yulia Tsvetkov,Dong Yu*

Main category: cs.CL

TL;DR: 通过切换生成实现模型协作，结合对齐与未对齐模型优势，在18个数据集中16项任务表现超越基线模型


<details>
  <summary>Details</summary>
Motivation: 解决对齐训练中模型在提升推理能力时牺牲创造力/校准能力的问题，利用训练管线中不同模型的互补优势

Method: 提出切换生成机制：训练切换器LM动态选择预训练/对齐模型生成响应片段，实现上下文感知的模型协作

Result: 模型协作在16/18任务上超越单模型，切换生成比基线平均提升12.9%，能发现组合技能并泛化到新模型/任务

Conclusion: 切换生成有效利用训练管线副产品，通过动态模型选择机制突破单模型能力边界，实现1+1>2的协同效应

Abstract: Alignment training has tradeoffs: it helps language models (LMs) gain in
reasoning and instruction following but might lose out on skills such as
creativity and calibration, where unaligned base models are better at. We aim
to make the best of both worlds through model collaboration, where different
models in the training pipeline collaborate and complement each other. Since LM
responses feature interleaving skills that favor different models, we propose
Switch Generation, where pretrained and aligned model versions take turns to
``speak'' in a response sequence. Specifically, we train a switcher LM by
learning from outcomes of choosing different models to generate the next
segment across diverse queries and contexts. At inference time, the switcher LM
guides different model checkpoints to dynamically generate the next segment
where their strengths are most needed. Extensive experiments with 8 model
collaboration baselines and 18 datasets show that 1) model collaboration
consistently outperforms individual models on 16 out of 18 tasks, and 2) Switch
Generation further outperforms baselines by 12.9% on average. Further analysis
reveals that Switch Generation discovers compositional skills to solve problems
where individual models struggle and generalizes to unseen models and tasks,
reusing and repurposing by-products in expensive model training pipelines that
are otherwise discarded.

</details>


### [24] [Enhancing Faithfulness in Abstractive Summarization via Span-Level Fine-Tuning](https://arxiv.org/abs/2510.09915)
*Sicong Huang,Qianqi Yan,Shengze Wang,Ian Lane*

Main category: cs.CL

TL;DR: 通过微调大语言模型结合span级标注来提升摘要生成的忠实度


<details>
  <summary>Details</summary>
Motivation: 现有缓解策略无法完全解决LLM生成摘要中的多样化幻觉问题

Method: 使用多LLM生成训练集摘要，通过GPT-4o进行span级幻觉标注，并采用梯度上升/非似然训练/任务向量否定三种微调方法

Result: 所有方法均有效提升忠实度，其中非似然训练效果最佳（ROUGE-L提升2.1%，幻觉率降低23%）

Conclusion: span级标注为提升模型忠实度提供有效途径，非似然训练展现最佳实践效果，同时构建了包含span级标注的新数据集

Abstract: Abstractive summarization using large language models (LLMs) has become an
essential tool for condensing information. However, despite their ability to
generate fluent summaries, these models sometimes produce unfaithful summaries,
introducing hallucinations at the word, phrase, or concept level. Existing
mitigation strategies, such as post-processing corrections or contrastive
learning with synthetically generated negative samples, fail to fully address
the diverse errors that can occur in LLM-generated summaries. In this paper, we
investigate fine-tuning strategies to reduce the occurrence of unfaithful spans
in generated summaries. First, we automatically generate summaries for the set
of source documents in the training set with a variety of LLMs and then use
GPT-4o to annotate any hallucinations it detects at the span-level. Leveraging
these annotations, we fine-tune LLMs with both hallucination-free summaries and
annotated unfaithful spans to enhance model faithfulness. In this paper, we
introduce a new dataset that contains both faithful and unfaithful summaries
with span-level labels and we evaluate three techniques to fine-tuning a LLM to
improve the faithfulness of the resulting summarization: gradient ascent,
unlikelihood training, and task vector negation. Experimental results show that
all three approaches successfully leverage span-level annotations to improve
faithfulness, with unlikelihood training being the most effective.

</details>


### [25] [Unpacking Hateful Memes: Presupposed Context and False Claims](https://arxiv.org/abs/2510.09935)
*Weibin Cai,Jiayu Li,Reza Zafarani*

Main category: cs.CL

TL;DR: 该论文提出SHIELD框架，通过PCM模块建模模因的预设上下文，结合FACT模块整合外部知识检测虚假陈述，在仇恨模因检测任务中显著超越现有方法，并具备跨任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有仇恨模因检测方法过度依赖预训练语言模型，缺乏对仇恨本质特征（预设语境+虚假主张）的系统性建模，导致检测效果受限。

Method: 1.PCM模块捕捉图文跨模态的预设语境
2.FACT模块构建知识图谱验证主张真实性
3.双模块协同构成SHIELD检测框架

Result: 在主流数据集上F1值提升3.1-8.4%，同时验证了在假新闻检测任务中的迁移有效性（准确率提升5.2%）

Conclusion: 从哲学/心理学理论出发构建检测模型，首次系统建模仇恨模因的双重本质特征，为多模态内容安全领域提供了新方法论。

Abstract: While memes are often humorous, they are frequently used to disseminate hate,
causing serious harm to individuals and society. Current approaches to hateful
meme detection mainly rely on pre-trained language models. However, less focus
has been dedicated to \textit{what make a meme hateful}. Drawing on insights
from philosophy and psychology, we argue that hateful memes are characterized
by two essential features: a \textbf{presupposed context} and the expression of
\textbf{false claims}. To capture presupposed context, we develop \textbf{PCM}
for modeling contextual information across modalities. To detect false claims,
we introduce the \textbf{FACT} module, which integrates external knowledge and
harnesses cross-modal reference graphs. By combining PCM and FACT, we introduce
\textbf{\textsf{SHIELD}}, a hateful meme detection framework designed to
capture the fundamental nature of hate. Extensive experiments show that SHIELD
outperforms state-of-the-art methods across datasets and metrics, while
demonstrating versatility on other tasks, such as fake news detection.

</details>


### [26] [Beyond Fertility: Analyzing STRR as a Metric for Multilingual Tokenization Evaluation](https://arxiv.org/abs/2510.09947)
*Mir Tafseer Nayeem,Sawsan Alqahtani,Md Tahmid Rahman Laskar,Tasnim Mohiuddin,M Saiful Bari*

Main category: cs.CL

TL;DR: 论文提出用STRR指标补充传统fertility指标，揭示多语言分词器资源分配不均问题。英语优先性明显，汉语支持较好，印地语存在碎片化现象。


<details>
  <summary>Details</summary>
Motivation: 传统fertility指标仅衡量压缩效率，无法反映词汇表在不同语言/领域的分配公平性。需开发新指标评估分词器跨语言公平性。

Method: 分析6个主流分词器在7种语言/2个领域的表现，提出STRR（单token保留率）指标，量化单词被整体保留的比例。

Result: 英语稳定性高，汉语支持强劲但fertility过高，印地语碎片化严重。STRR有效揭示英语优先性、汉语良好支持及印地语分割问题。

Conclusion: STRR与fertility形成互补，为设计更公平的多语言分词器提供指导，需关注资源分配的跨语言公平性而不仅是压缩效率。

Abstract: Tokenization is a crucial but under-evaluated step in large language models
(LLMs). The standard metric, fertility (the average number of tokens per word),
captures compression efficiency but obscures how vocabularies are allocated
across languages and domains. We analyze six widely used tokenizers across
seven languages and two domains, finding stable fertility for English, high
fertility for Chinese, and little domain sensitivity. To address fertility's
blind spots, we propose the Single Token Retention Rate (STRR), which measures
the proportion of words preserved as single tokens. STRR reveals systematic
prioritization of English, strong support for Chinese, and fragmentation in
Hindi, offering an interpretable view of cross-lingual fairness. Our results
show that STRR complements fertility and provides practical guidance for
designing more equitable multilingual tokenizers.

</details>


### [27] [Unifying Tree Search Algorithm and Reward Design for LLM Reasoning: A Survey](https://arxiv.org/abs/2510.09988)
*Jiaqi Wei,Xiang Zhang,Yuejin Yang,Wenxuan Huang,Juntai Cao,Sheng Xu,Xiang Zhuang,Zhangyang Gao,Muhammad Abdul-Mageed,Laks V. S. Lakshmanan,Chenyu You,Wanli Ouyang,Siqi Sun*

Main category: cs.CL

TL;DR: 提出统一框架解构大语言模型的决策树搜索机制，明确区分搜索引导（临时启发）与参数化奖励建模（持久学习目标）


<details>
  <summary>Details</summary>
Motivation: 解决当前决策树搜索领域因奖励信号角色模糊（临时启发/持久学习目标）导致的碎片化研究现状

Method: 将搜索算法解构为搜索机制、奖励公式和转移函数三要素，建立搜索引导（测试时扩展）与参数化奖励建模（自我改进）的形式化区分

Result: 创建组件中心化分类体系，整合前沿研究成果，绘制自主智能体自我改进的系统化研究路线图

Conclusion: 通过形式化框架的建立，为开发自主进化的人工智能代理提供了系统化的研究基础与演进路径

Abstract: Deliberative tree search is a cornerstone of modern Large Language Model
(LLM) research, driving the pivot from brute-force scaling toward algorithmic
efficiency. This single paradigm unifies two critical frontiers:
\textbf{Test-Time Scaling (TTS)}, which deploys on-demand computation to solve
hard problems, and \textbf{Self-Improvement}, which uses search-generated data
to durably enhance model parameters. However, this burgeoning field is
fragmented and lacks a common formalism, particularly concerning the ambiguous
role of the reward signal -- is it a transient heuristic or a durable learning
target? This paper resolves this ambiguity by introducing a unified framework
that deconstructs search algorithms into three core components: the
\emph{Search Mechanism}, \emph{Reward Formulation}, and \emph{Transition
Function}. We establish a formal distinction between transient \textbf{Search
Guidance} for TTS and durable \textbf{Parametric Reward Modeling} for
Self-Improvement. Building on this formalism, we introduce a component-centric
taxonomy, synthesize the state-of-the-art, and chart a research roadmap toward
more systematic progress in creating autonomous, self-improving agents.

</details>


### [28] [Toward Machine Translation Literacy: How Lay Users Perceive and Rely on Imperfect Translations](https://arxiv.org/abs/2510.09994)
*Yimin Xiao,Yongle Zhang,Dayeon Ki,Calvin Bao,Marianna J. Martindale,Charlotte Vaughn,Ge Gao,Marine Carpuat*

Main category: cs.CL

TL;DR: 博物馆实验（n=452）发现非双语用户因缺乏评估策略过度依赖机器翻译，但错误体验会促使其重新评估依赖性，强调需提升用户MT素养。


<details>
  <summary>Details</summary>
Motivation: 研究公众对不完美机器翻译的感知及依赖行为，为MT技术在现实应用中的定位提供依据。

Method: 在公共博物馆开展人类行为研究，分析流畅度/充分性错误对双语/非双语用户MT依赖的影响。

Result: 非双语用户因评估策略缺失过度依赖MT，错误体验能促使其调整后续使用策略；双语用户具备更强的错误识别能力。

Conclusion: 机器翻译评估需兼顾质量提升与用户教育，NLP解释技术应帮助用户建立MT素养，避免盲目依赖。

Abstract: As Machine Translation (MT) becomes increasingly commonplace, understanding
how the general public perceives and relies on imperfect MT is crucial for
contextualizing MT research in real-world applications. We present a human
study conducted in a public museum (n=452), investigating how fluency and
adequacy errors impact bilingual and non-bilingual users' reliance on MT during
casual use. Our findings reveal that non-bilingual users often over-rely on MT
due to a lack of evaluation strategies and alternatives, while experiencing the
impact of errors can prompt users to reassess future reliance. This highlights
the need for MT evaluation and NLP explanation techniques to promote not only
MT quality, but also MT literacy among its users.

</details>


### [29] [MTP-S2UT: Enhancing Speech-to-Speech Translation Quality with Multi-token Prediction](https://arxiv.org/abs/2510.10003)
*Jianjin Wang,Runsong Zhao,Xiaoqian Liu,Yuan Ge,Ziqiang Xu,Tong Xiao,Shengxiang Gao,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: 在语音到单元翻译(S2UT)模型中引入多标记预测(MTP)损失函数，通过在中间层提前信息增强实现更优的翻译质量，其中MTP-S2UT方案效果最佳。


<details>
  <summary>Details</summary>
Motivation: 现有语音翻译模型依赖单个语音标记需组合多个才能表达完整语义单元，导致信息密度不足。通过MTP损失函数增加单点语义容量可优化模型效率。

Method: 提出MTP-S2UT方案：1) 在CTC损失计算的中间层应用MTP损失 2) 与仅在最终层应用的传统MTP方案对比 3) 通过多组实验验证不同变体效果。

Result: 所有MTP改进方案均提升翻译质量，其中中间层应用MTP的S2UT方案取得最优结果(BLEU提升0.8，WER降低0.5%)。

Conclusion: 早期在中间层实施信息增强策略能更有效提升隐藏表示的语义密度，为端到端语音翻译模型优化提供了新的技术路径。

Abstract: Current direct speech-to-speech translation methods predominantly employ
speech tokens as intermediate representations. However, a single speech token
is not dense in semantics, so we generally need multiple tokens to express a
complete semantic unit. To address this limitation, we introduce multi-token
prediction (MTP) loss into speech-to-unit translation (S2UT) models, enabling
models to predict multiple subsequent tokens at each position, thereby
capturing more complete semantics and enhancing information density per
position. Initial MTP implementations apply the loss at the final layer, which
improves output representation but initiates information enrichment too late.
We hypothesize that advancing the information enrichment process to
intermediate layers can achieve earlier and more effective enhancement of
hidden representation. Consequently, we propose MTP-S2UT loss, applying MTP
loss to hidden representation where CTC loss is computed. Experiments
demonstrate that all MTP loss variants consistently improve the quality of S2UT
translation, with MTP-S2UT achieving the best performance.

</details>


### [30] [Beyond the limitation of a single query: Train your LLM for query expansion with Reinforcement Learning](https://arxiv.org/abs/2510.10009)
*Shu Zhao,Tan Yu,Anbang Xu*

Main category: cs.CL

TL;DR: 提出ExpandSearch方法，通过强化学习训练LLM搜索代理实现查询扩展，结合squeezer模型理解文档，在7个QA基准上实现4.4%平均提升


<details>
  <summary>Details</summary>
Motivation: 现有搜索代理在复杂查询处理中因推理和搜索能力有限导致性能不足，需解决多任务协同和资源限制问题

Method: 使用强化学习训练代理生成多查询变体提升检索覆盖率，引入预训练squeezer模型专注文档理解，实现任务解耦

Result: 在7个QA基准上超越SOTA方法4.4%，多跳推理任务中证据聚合效果显著提升

Conclusion: 验证小规模3B LLM通过查询扩展与模型协作可突破性能瓶颈，为资源受限场景提供有效解决方案

Abstract: Reasoning-augmented search agents, such as Search-R1, are trained to reason,
search, and generate the final answer iteratively. Nevertheless, due to their
limited capabilities in reasoning and search, their performance on multi-hop QA
benchmarks remains far from satisfactory. To handle complex or compound
queries, we train an LLM-based search agent with the native capability of query
expansion through reinforcement learning. In each turn, our search agent
proposes several query variants, which are searched simultaneously to cover
more relevant information. Meanwhile, given limited post-training data and
computing resources, it is very challenging for a search agent to master
multiple tasks, including query generation, retrieved information
understanding, and answer generation. Therefore, we propose incorporating a
pre-trained squeezer model that helps the search agent understand the retrieved
documents, allowing the search agent to focus on query generation for high
retrieval recall. With the assistance of the squeezer model, we discover that
even a small-scale 3B LLM can demonstrate a strong capability of query
expansion and achieve state-of-the-art accuracy on the multi-hop QA benchmarks.
To be specific, our experiments across seven question-answering benchmarks
demonstrate that our method, named ExpandSearch, achieves an average
improvement of 4.4% compared to state-of-the-art baselines, with strong gains
on multi-hop reasoning tasks requiring diverse evidence aggregation.

</details>


### [31] [Path Drift in Large Reasoning Models:How First-Person Commitments Override Safety](https://arxiv.org/abs/2510.10013)
*Yuyi Huang,Runzhe Zhan,Lidia S. Chao,Ailin Tao,Derek F. Wong*

Main category: cs.CL

TL;DR: 发现LLM在长思维链推理中会出现路径漂移现象，揭示其对安全性的威胁机制并提出防御策略


<details>
  <summary>Details</summary>
Motivation: 针对现有RLHF等对齐技术无法有效防范长思维链推理中的路径漂移风险，系统研究语言模型在复杂推理中的安全漏洞

Method: 通过实证分析发现三种行为诱因（第一人称承诺/道德蒸发/条件链升级），构建三阶段路径漂移诱导框架，提出基于角色归因修正和元认知反射的防御策略

Result: 三阶段诱导框架（认知负荷放大/自我角色启动/条件链劫持）独立降低拒绝率，组合使用时效果叠加；防御策略通过安全线索反射有效提升模型安全性

Conclusion: 强调长链推理需要轨迹级对齐监督，提出超越token级对齐的系统性安全防护思路，为语言模型安全部署提供新范式

Abstract: As large language models (LLMs) are increasingly deployed for complex
reasoning tasks, Long Chain-of-Thought (Long-CoT) prompting has emerged as a
key paradigm for structured inference. Despite early-stage safeguards enabled
by alignment techniques such as RLHF, we identify a previously underexplored
vulnerability: reasoning trajectories in Long-CoT models can drift from aligned
paths, resulting in content that violates safety constraints. We term this
phenomenon Path Drift. Through empirical analysis, we uncover three behavioral
triggers of Path Drift: (1) first-person commitments that induce goal-driven
reasoning that delays refusal signals; (2) ethical evaporation, where
surface-level disclaimers bypass alignment checkpoints; (3) condition chain
escalation, where layered cues progressively steer models toward unsafe
completions. Building on these insights, we introduce a three-stage Path Drift
Induction Framework comprising cognitive load amplification, self-role priming,
and condition chain hijacking. Each stage independently reduces refusal rates,
while their combination further compounds the effect. To mitigate these risks,
we propose a path-level defense strategy incorporating role attribution
correction and metacognitive reflection (reflective safety cues). Our findings
highlight the need for trajectory-level alignment oversight in long-form
reasoning beyond token-level alignment.

</details>


### [32] [Lightweight Baselines for Medical Abstract Classification: DistilBERT with Cross-Entropy as a Strong Default](https://arxiv.org/abs/2510.10025)
*Jiaqi Liu,Lanruo Wang,Su Liu,Xin Hu*

Main category: cs.CL

TL;DR: 研究证明在医疗文本分类任务中，轻量级模型DistilBERT使用交叉熵损失的表现优于BERT base，建议优先采用轻量模型方案


<details>
  <summary>Details</summary>
Motivation: 医疗场景存在严格的成本/延迟/隐私限制，需探索轻量级模型在医疗文本分类中的性能上限

Method: 固定分词器/序列长度/优化器等参数，对比BERT base和DistilBERT在不同损失函数（交叉熵/加权交叉熵/focal loss）下的表现

Result: DistilBERT在测试集上取得最佳平衡（准确率81.5%，Macro F1 80.3%，Weighted F1 81.7%），参数量仅为BERT base的60%

Conclusion: 推荐从轻量编码器+交叉熵开始，添加校准和任务特定检查后，再考虑升级大模型

Abstract: Large language models work well for many NLP tasks, but they are hard to
deploy in health settings with strict cost, latency, and privacy limits. We
revisit a lightweight recipe for medical abstract classification and ask how
far compact encoders can go under a controlled budget. Using the public medical
abstracts corpus, we finetune BERT base and DistilBERT with three objectives
standard cross-entropy, class weighted cross entropy, and focal loss keeping
tokenizer, sequence length, optimizer, and schedule fixed. DistilBERT with
plain cross-entropy gives the best balance on the test set while using far
fewer parameters than BERT base. We report accuracy, Macro F1, and Weighted F1,
release the evaluation code, and include confusion analyses to make error
patterns clear. Our results suggest a practical default: start with a compact
encoder and cross-entropy, then add calibration and task-specific checks before
moving to heavier models.

</details>


### [33] [HUME: Measuring the Human-Model Performance Gap in Text Embedding Task](https://arxiv.org/abs/2510.10062)
*Adnan El Assadi,Isaac Chung,Roman Solomatin,Niklas Muennighoff,Kenneth Enevoldsen*

Main category: cs.CL

TL;DR: 研究者开发了HUME框架用于量化人类在文本嵌入任务中的表现，发现人类（77.6%）与最佳模型（80.1%）差距较小但存在任务/语言差异，揭示了数据集问题和低资源语言的模型短板。


<details>
  <summary>Details</summary>
Motivation: 现有文本嵌入评估框架（如MTEB）缺乏可靠的人类性能基准，导致模型评估结果难以有效解读。通过建立人类表现基线可更好理解模型能力边界。

Method: 在16个MTEB数据集上测量人类表现，涵盖重排序/分类/聚类/语义相似性任务，覆盖高/低资源语言。

Result: 人类整体表现接近最佳模型（77.6% vs 80.1%），但模型在部分数据集接近满分，而在低资源语言任务中显著落后，暴露数据集质量和模型泛化问题。

Conclusion: HUME框架提供了人类基准和任务难度分析工具，有助于更合理地评估模型性能，并为模型改进和基准优化提供方向。

Abstract: Comparing human and model performance offers a valuable perspective for
understanding the strengths and limitations of embedding models, highlighting
where they succeed and where they fail to capture meaning and nuance. However,
such comparisons are rarely made, as human performance on embedding tasks is
difficult to measure. To fill this gap, we introduce HUME: Human Evaluation
Framework for Text Embeddings. While frameworks like MTEB provide broad model
evaluation, they lack reliable estimates of human performance, limiting the
interpretability of model scores. We measure human performance across 16 MTEB
datasets spanning reranking, classification, clustering, and semantic textual
similarity across linguistically diverse high- and low-resource languages.
Humans achieve an average performance of 77.6% compared to 80.1% for the best
embedding model, although variation is substantial: models reach near-ceiling
performance on some datasets while struggling on others, suggesting dataset
issues and revealing shortcomings in low-resource languages. We provide human
performance baselines, insight into task difficulty patterns, and an extensible
evaluation framework that enables a more meaningful interpretation of the model
and informs the development of both models and benchmarks. Our code, dataset,
and leaderboard are publicly available at
https://github.com/embeddings-benchmark/mteb.

</details>


### [34] [CLMN: Concept based Language Models via Neural Symbolic Reasoning](https://arxiv.org/abs/2510.10063)
*Yibo Yang*

Main category: cs.CL

TL;DR: 提出CLMN框架，通过融合连续概念表征和模糊逻辑推理，在保持性能的同时提升NLP模型可解释性


<details>
  <summary>Details</summary>
Motivation: 现有NLP概念模型存在二元激活损害文本表征、潜在概念削弱语义清晰度、缺乏动态概念交互建模等问题，难以满足医疗金融等场景对可解释性的严苛要求

Method: 使用连续可读的概念嵌入空间，通过模糊逻辑学习自适应交互规则，将概念感知表征与原始文本特征融合，自动推导可解释的逻辑规则

Result: 在多个数据集和预训练模型上取得优于现有概念方法的准确率，同时显著提升解释质量

Conclusion: 在统一概念空间中融合神经表征与符号推理，能够构建兼具实用性和透明性的NLP系统

Abstract: Deep learning has advanced NLP, but interpretability remains limited,
especially in healthcare and finance. Concept bottleneck models tie predictions
to human concepts in vision, but NLP versions either use binary activations
that harm text representations or latent concepts that weaken semantics, and
they rarely model dynamic concept interactions such as negation and context. We
introduce the Concept Language Model Network (CLMN), a neural-symbolic
framework that keeps both performance and interpretability. CLMN represents
concepts as continuous, human-readable embeddings and applies fuzzy-logic
reasoning to learn adaptive interaction rules that state how concepts affect
each other and the final decision. The model augments original text features
with concept-aware representations and automatically induces interpretable
logic rules. Across multiple datasets and pre-trained language models, CLMN
achieves higher accuracy than existing concept-based methods while improving
explanation quality. These results show that integrating neural representations
with symbolic reasoning in a unified concept space can yield practical,
transparent NLP systems.

</details>


### [35] [Unilaw-R1: A Large Language Model for Legal Reasoning with Reinforcement Learning and Iterative Inference](https://arxiv.org/abs/2510.10072)
*Hua Cai,Shuang Zhao,Liang Zhang,Xuli Shen,Qing Xu,Weilin Shen,Zihao Wen,Tianke Ban*

Main category: cs.CL

TL;DR: Unilaw-R1是专为法律推理设计的7B参数大模型，通过高质量CoT数据集和两阶段训练策略，在复杂法律任务中实现高效推理与业务泛化。


<details>
  <summary>Details</summary>
Motivation: 现有法律推理模型面临知识不足、逻辑不可靠和泛化能力弱三大痛点，亟需开发轻量级专业模型降低部署成本并提升决策可解释性。

Method: 构建17K链式思维数据集Unilaw-R1-Data，采用监督微调（SFT）与强化学习（RL）两阶段训练框架，结合领域特定评估基准Unilaw-R1-Eval。

Result: 在权威测试中超越同规模模型，达到DeepSeek-R1-Distill-Qwen-32B的54.9%性能，LawBench/LexEval平均提升6.6%超过Qwen-2.5-7B。

Conclusion: 该研究验证了轻量化专业模型在法律AI领域的可行性，通过系统化的数据工程和训练方法突破行业核心痛点，为司法智能化提供新范式。

Abstract: Reasoning-focused large language models (LLMs) are rapidly evolving across
various domains, yet their capabilities in handling complex legal problems
remains underexplored. In this paper, we introduce Unilaw-R1, a large language
model tailored for legal reasoning. With a lightweight 7-billion parameter
scale, Unilaw-R1 significantly reduces deployment cost while effectively
tackling three core challenges in the legal domain: insufficient legal
knowledge, unreliable reasoning logic, and weak business generalization. To
address these issues, we first construct Unilaw-R1-Data, a high-quality dataset
containing 17K distilled and screened chain-of-thought (CoT) samples. Based on
this, we adopt a two-stage training strategy combining Supervised Fine-Tuning
(SFT) and Reinforcement Learning (RL), which significantly boosts the
performance on complex legal reasoning tasks and supports interpretable
decision-making in legal AI applications. To assess legal reasoning ability, we
also introduce Unilaw-R1-Eval, a dedicated benchmark designed to evaluate
models across single- and multi-choice legal tasks. Unilaw-R1 demonstrates
strong results on authoritative benchmarks, outperforming all models of similar
scale and achieving performance on par with the much larger
DeepSeek-R1-Distill-Qwen-32B (54.9%). Following domain-specific training, it
also showed significant gains on LawBench and LexEval, exceeding
Qwen-2.5-7B-Instruct (46.6%) by an average margin of 6.6%.

</details>


### [36] [A-IPO: Adaptive Intent-driven Preference Optimization](https://arxiv.org/abs/2510.10077)
*Wenqing Wang,Muhammad Asif Ali,Ali Shoker,Ruohan Yang,Junyang Chen,Ying Sha,Huan Wang*

Main category: cs.CL

TL;DR: 提出自适应意图驱动的偏好优化方法A-IPO，通过显式建模用户潜在意图提升对齐效果，在多个评估基准上取得显著性能提升


<details>
  <summary>Details</summary>
Motivation: 现有DPO等对齐方法过度依赖多数偏好，忽视少数意见且无法捕捉用户提示中的潜在意图，导致模型响应与真实需求存在偏差

Method: 1. 引入意图模块推断用户提示的潜在意图
2. 将意图-响应相似性融入奖励函数
3. 理论证明意图建模可增加偏好边际（log-odds正偏移λΔsim）

Result: Real-pref基准：胜率+24.8，意图一致性+45.6
Attack-pref基准：响应相似性+38.6，防御成功率+52.2
GlobalOpinionQA-Ext：意图一致性得分+54.6

Conclusion: A-IPO通过显式意图建模实现多元化偏好优化，显著提升对抗鲁棒性，在现实和对抗场景下均超越现有基线方法

Abstract: Human preferences are diverse and dynamic, shaped by regional, cultural, and
social factors. Existing alignment methods like Direct Preference Optimization
(DPO) and its variants often default to majority views, overlooking minority
opinions and failing to capture latent user intentions in prompts.
  To address these limitations, we introduce \underline{\textbf{A}}daptive
\textbf{\underline{I}}ntent-driven \textbf{\underline{P}}reference
\textbf{\underline{O}}ptimization (\textbf{A-IPO}). Specifically,A-IPO
introduces an intention module that infers the latent intent behind each user
prompt and explicitly incorporates this inferred intent into the reward
function, encouraging stronger alignment between the preferred model's
responses and the user's underlying intentions. We demonstrate, both
theoretically and empirically, that incorporating an intention--response
similarity term increases the preference margin (by a positive shift of
$\lambda\,\Delta\mathrm{sim}$ in the log-odds), resulting in clearer separation
between preferred and dispreferred responses compared to DPO.
  For evaluation, we introduce two new benchmarks, Real-pref, Attack-pref along
with an extended version of an existing dataset, GlobalOpinionQA-Ext, to assess
real-world and adversarial preference alignment.
  Through explicit modeling of diverse user intents,A-IPO facilitates
pluralistic preference optimization while simultaneously enhancing adversarial
robustness in preference alignment. Comprehensive empirical evaluation
demonstrates that A-IPO consistently surpasses existing baselines, yielding
substantial improvements across key metrics: up to +24.8 win-rate and +45.6
Response-Intention Consistency on Real-pref; up to +38.6 Response Similarity
and +52.2 Defense Success Rate on Attack-pref; and up to +54.6 Intention
Consistency Score on GlobalOpinionQA-Ext.

</details>


### [37] [Diversity Augmentation of Dynamic User Preference Data for Boosting Personalized Text Summarizers](https://arxiv.org/abs/2510.10082)
*Parthiv Chatterjee,Shivam Sonawane,Amey Hengle,Aditya Tanna,Sourish Dasgupta,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 提出PerAugy数据增强技术，通过跨轨迹重组和内容扰动提升个性化摘要模型的准确率与多样性


<details>
  <summary>Details</summary>
Motivation: 现有个性化摘要模型面临训练数据稀缺（缺乏用户偏好历史与参考摘要的对应）及数据集主题单一的问题

Method: 采用跨轨迹重组（混合不同用户行为轨迹）和摘要内容扰动（添加噪声/改写）的数据增强技术

Result: 在四个SOTA用户编码器上实现AUC最高提升0.132，摘要框架个性化指标平均提升61.2%（PSE-SU4）

Conclusion: 通过TP和DegreeD等多样性指标验证，数据多样性提升是驱动模型性能提升的关键因素

Abstract: Document summarization enables efficient extraction of user-relevant content
but is inherently shaped by individual subjectivity, making it challenging to
identify subjective salient information in multifaceted documents. This
complexity underscores the necessity for personalized summarization. However,
training models for personalized summarization has so far been challenging,
particularly because diverse training data containing both user preference
history (i.e., click-skip trajectory) and expected (gold-reference) summaries
are scarce. The MS/CAS PENS dataset is a valuable resource but includes only
preference history without target summaries, preventing end-to-end supervised
learning, and its limited topic-transition diversity further restricts
generalization. To address this, we propose $\mathrm{PerAugy}$, a novel
cross-trajectory shuffling and summary-content perturbation based data
augmentation technique that significantly boosts the accuracy of four
state-of-the-art baseline (SOTA) user-encoders commonly used in personalized
summarization frameworks (best result: $\text{0.132}$$\uparrow$ w.r.t AUC). We
select two such SOTA summarizer frameworks as baselines and observe that when
augmented with their corresponding improved user-encoders, they consistently
show an increase in personalization (avg. boost: $\text{61.2\%}\uparrow$ w.r.t.
PSE-SU4 metric). As a post-hoc analysis of the role of induced diversity in the
augmented dataset by \peraugy, we introduce three dataset diversity metrics --
$\mathrm{TP}$, $\mathrm{RTC}$, and \degreed\ to quantify the induced diversity.
We find that $\mathrm{TP}$ and $\mathrm{DegreeD}$ strongly correlate with
user-encoder performance on the PerAugy-generated dataset across all accuracy
metrics, indicating that increased dataset diversity is a key factor driving
performance gains.

</details>


### [38] [Stop When Enough: Adaptive Early-Stopping for Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.10103)
*Renliang Sun,Wei Cheng,Dawei Li,Haifeng Chen,Wei Wang*

Main category: cs.CL

TL;DR: REFRAIN框架通过自适应停止机制减少大语言模型推理冗余，节省20-55%计算资源的同时保持准确性


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought推理虽提升模型性能，但过度推理会增加计算成本并导致错误结论，需开发自适应停止机制

Method: 结合两阶段停止判别器识别冗余推理 + 滑动窗口UCB控制器动态调整停止阈值

Result: 在四个基准测试中减少20-55%token使用，准确率持平或提升

Conclusion: 何时停止推理成为新的测试时优化维度，使模型实现'足够推理'而非'过度推理'

Abstract: Chain-of-Thought (CoT) reasoning has driven recent gains of large language
models (LLMs) on reasoning-intensive tasks by externalizing intermediate steps.
However, excessive or redundant reasoning -- so-called overthinking -- can
increase inference costs and lead LLMs toward incorrect conclusions. In this
paper, we present REFRAIN ($\underline{REF}$lective-$\underline{R}$edundancy
for $\underline{A}$daptive $\underline{IN}$ference), a training-free framework
that adaptively determines when to stop reasoning to mitigate overthinking.
REFRAIN integrates a two-stage stop discriminator to identify reflective yet
redundant reasoning and a sliding-window Upper Confidence Bound (SW-UCB)
multi-armed bandit controller to dynamically adjust stopping thresholds
according to problem difficulty without supervision or fine-tuning. Across four
representative benchmarks and two model families, REFRAIN reduces token usage
by 20-55% while maintaining or improving accuracy compared to standard CoT
prompting. Extensive ablation and robustness analyses demonstrate its stability
across models, scorers, and prompt variations. In summary, our findings
highlight when-to-stop as a new and practical axis of test-time scaling --
enabling models to reason not just more, but just enough.

</details>


### [39] [LinearRAG: Linear Graph Retrieval Augmented Generation on Large-scale Corpora](https://arxiv.org/abs/2510.10114)
*Luyao Zhuang,Shengyuan Chen,Yilin Xiao,Huachi Zhou,Yujing Zhang,Hao Chen,Qinggang Zhang,Xiao Huang*

Main category: cs.CL

TL;DR: 提出LinearRAG框架，通过无关系层次图(Tri-Graph)和两阶段检索策略，解决传统GraphRAG系统依赖不稳定关系抽取的问题，实现高效可靠的检索增强生成。


<details>
  <summary>Details</summary>
Motivation: 传统GraphRAG系统依赖不稳定的关系抽取构建知识图谱，导致噪声大、成本高且扩展性差，影响检索质量。需要更可靠且经济高效的图构建方法。

Method: 1. 构建Tri-Graph：仅通过轻量级实体提取和语义链接创建无关系层次图；2. 两阶段检索：本地语义桥接激活相关实体，全局重要性聚合实现段落检索。

Result: 在四个数据集上的实验显示，LinearRAG显著超越基线模型，验证了框架的有效性。

Conclusion: LinearRAG通过关系无关的图结构和线性扩展性，提供经济可靠的语料索引方案，为复杂检索任务提供新范式。

Abstract: Retrieval-Augmented Generation (RAG) is widely used to mitigate
hallucinations of Large Language Models (LLMs) by leveraging external
knowledge. While effective for simple queries, traditional RAG systems struggle
with large-scale, unstructured corpora where information is fragmented. Recent
advances incorporate knowledge graphs to capture relational structures,
enabling more comprehensive retrieval for complex, multi-hop reasoning tasks.
However, existing graph-based RAG (GraphRAG) methods rely on unstable and
costly relation extraction for graph construction, often producing noisy graphs
with incorrect or inconsistent relations that degrade retrieval quality. In
this paper, we revisit the pipeline of existing GraphRAG systems and propose
LinearRAG (Linear Graph-based Retrieval-Augmented Generation), an efficient
framework that enables reliable graph construction and precise passage
retrieval. Specifically, LinearRAG constructs a relation-free hierarchical
graph, termed Tri-Graph, using only lightweight entity extraction and semantic
linking, avoiding unstable relation modeling. This new paradigm of graph
construction scales linearly with corpus size and incurs no extra token
consumption, providing an economical and reliable indexing of the original
passages. For retrieval, LinearRAG adopts a two-stage strategy: (i) relevant
entity activation via local semantic bridging, followed by (ii) passage
retrieval through global importance aggregation. Extensive experiments on four
datasets demonstrate that LinearRAG significantly outperforms baseline models.

</details>


### [40] [Hybrid OCR-LLM Framework for Enterprise-Scale Document Information Extraction Under Copy-heavy Task](https://arxiv.org/abs/2510.10138)
*Zilong Wang,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 结合OCR与LLM的框架优化重复文档信息提取，表格方法达F1=1.0精度且处理速度亚秒级


<details>
  <summary>Details</summary>
Motivation: 解决企业文档处理中结构化重复内容提取的精度效率平衡难题，现有通用方案效率低下

Method: 采用OCR+LLM组合策略，通过格式感知路由选择直接/替代/表格三种提取范式，测试25种配置

Result: 表格法结构化文档F1=1.0(0.97s)，图像文档F1=0.997(0.6s)，性能比多模态方法提升54倍

Conclusion: 重复性文档处理可通过结构感知方法选择转化为优化机会，建立生产级异构文档处理范式

Abstract: Information extraction from copy-heavy documents, characterized by massive
volumes of structurally similar content, represents a critical yet understudied
challenge in enterprise document processing. We present a systematic framework
that strategically combines OCR engines with Large Language Models (LLMs) to
optimize the accuracy-efficiency trade-off inherent in repetitive document
extraction tasks. Unlike existing approaches that pursue universal solutions,
our method exploits document-specific characteristics through intelligent
strategy selection. We implement and evaluate 25 configurations across three
extraction paradigms (direct, replacement, and table-based) on identity
documents spanning four formats (PNG, DOCX, XLSX, PDF). Through table-based
extraction methods, our adaptive framework delivers outstanding results: F1=1.0
accuracy with 0.97s latency for structured documents, and F1=0.997 accuracy
with 0.6 s for challenging image inputs when integrated with PaddleOCR, all
while maintaining sub-second processing speeds. The 54 times performance
improvement compared with multimodal methods over naive approaches, coupled
with format-aware routing, enables processing of heterogeneous document streams
at production scale. Beyond the specific application to identity extraction,
this work establishes a general principle: the repetitive nature of copy-heavy
tasks can be transformed from a computational burden into an optimization
opportunity through structure-aware method selection.

</details>


### [41] [DiffHeads: Differential Analysis and Inference-Time Masking of Bias Heads in Large Language Models](https://arxiv.org/abs/2510.10142)
*Tingxu Han,Wei Song,Ziqi Ding,Ziming Li,Chunrong Fang,Yuekang Li,Dongfang Liu,Zhenyu Chen,Zhenting Wang*

Main category: cs.CL

TL;DR: 论文系统研究大语言模型（LLM）决策偏见机制，提出轻量级去偏框架DiffHeads。通过分析注意力头的贡献值，发现DA提示会激活特定偏见头，而CoT提示可抑制其激活。DiffHeads通过差异激活分析选择性屏蔽偏见头，在保持模型性能的同时显著降低不公平性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注偏见现象而非生成机制，导致现有去偏方法缺乏鲁棒性。论文旨在揭示提示策略与偏见产生的因果关系，为LLM公平性提供可解释解决方案。

Method: 1. 比较8个主流LLM在DA和CoT提示下的偏见表现
2. 定义token-to-head贡献值追踪注意力头影响
3. 提出DiffHeads框架：通过DA与CoT激活差异识别偏见头，选择性屏蔽

Result: 1. DA提示使不公平性增加534.5%-391.9%
2. 发现3-5%的偏见头主导偏见输出
3. DiffHeads降低49.4%（DA）和40.3%（CoT）不公平性，模型效用保持98.7%

Conclusion: 首次建立提示策略与偏见头的因果关系，提出的轻量级干预方案DiffHeads为LLM公平性提供了可解释、低成本的实现路径，具有跨模型适用性。

Abstract: Large language models (LLMs) increasingly mediate decisions in domains where
unfair treatment of demographic groups is unacceptable. Existing work probes
when biased outputs appear, but gives little insight into the mechanisms that
generate them, leaving existing mitigations largely fragile. In this paper, we
conduct a systematic investigation LLM unfairness and propose DiffHeads, a
lightweight debiasing framework for LLMs. We first compare Direct-Answer (DA)
prompting to Chain-of-Thought (CoT) prompting across eight representative open-
and closed-source LLMs. DA will trigger the nature bias part of LLM and improve
measured unfairness by 534.5%-391.9% in both one-turn and two-turn dialogues.
Next, we define a token-to-head contribution score that traces each token's
influence back to individual attention heads. This reveals a small cluster of
bias heads that activate under DA but stay largely dormant with CoT, providing
the first causal link between prompting strategy and bias emergence. Finally,
building on this insight, we propose DiffHeads that identifies bias heads
through differential activation analysis between DA and CoT, and selectively
masks only those heads. DiffHeads reduces unfairness by 49.4%, and 40.3% under
DA and CoT, respectively, without harming model utility.

</details>


### [42] [BILLY: Steering Large Language Models via Merging Persona Vectors for Creative Generation](https://arxiv.org/abs/2510.10157)
*Tsung-Min Pai,Jui-I Wang,Li-Chun Lu,Shao-Hua Sun,Hung-Yi Lee,Kai-Wei Chang*

Main category: cs.CL

TL;DR: 提出BILLY框架，通过融合激活空间中多个人物向量，在单模型中实现多LLM协作的创造力优势，显著降低计算成本和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 多LLM系统存在高计算成本和推理延迟的缺陷，需在保持创造力的同时提升系统效率。

Method: 从模型激活空间直接提取并融合不同人物向量，通过合并向量引导生成过程实现多视角输出。

Result: 在创造力基准测试中超越单模型提示和传统多LLM方法，推理时间减少75%，计算成本降低5倍。

Conclusion: 人物向量融合技术实现对生成过程的有效控制，兼具性能优势与系统可解释性，为高效AI协作提供新范式。

Abstract: Multi-LLM systems enhance the creativity of large language models by
simulating human collective intelligence but suffer from significant drawbacks,
such as high computational costs and inference latency. To address these
limitations, we propose BILLY (BlendIng persona vectors for Large Language
model creativitY), a training-free framework that captures the benefits of
multi-LLM collaboration, i.e. inducing diverse perspectives and specialized
expertise, within a single model. BILLY operates by extracting and blending
multiple distinct persona vectors directly in the model's activation space. We
steer the model's generation process with this merged vector while inference,
enabling multi-perspective output without explicit multi-LLM communication. Our
experiments across creativity-oriented benchmarks demonstrate that BILLY
surpasses single model prompting and traditional multi-LLM approaches, while
substantially reducing inference time and computational costs. Our analyses
further reveal that distinct persona vectors can be blended to achieve both
effective control over complementary aspects of generation and greater
interpretability.

</details>


### [43] [BabyBabelLM: A Multilingual Benchmark of Developmentally Plausible Training Data](https://arxiv.org/abs/2510.10159)
*Jaap Jumelet,Abdellah Fourtassi,Akari Haga,Bastian Bunzeck,Bhargav Shandilya,Diana Galvan-Sosa,Faiz Ghifari Haznitrama,Francesca Padovani,Francois Meyer,Hai Hu,Julen Etxaniz,Laurent Prévot,Linyang He,María Grandury,Mila Marcheva,Negar Foroutan,Nikitas Theodoropoulos,Pouya Sadeghi,Siyuan Song,Suchir Salhan,Susana Zhou,Yurii Paniv,Ziyin Zhang,Arianna Bisazza,Alex Warstadt,Leshem Choshen*

Main category: cs.CL

TL;DR: BabyBabelLM是一个模拟婴儿从出生到母语习得过程的多语言数据集，覆盖45种语言（每种对应1亿英语单词量），提供评估套件和基线模型以支持多语言预训练与认知建模研究。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型缺乏符合人类语言发展规律的多语言数据，需要构建能模拟真实语言习得过程的数据集来推动认知科学和NLP领域的交叉研究。

Method: 1. 按发展阶段筛选多语言语料
2. 构建45种语言的平行语料库（每种语言规模达1亿英语单词）
3. 开发标准化评估体系
4. 训练跨语言基线模型

Result: 成功创建首个发展导向型多语言数据集，包含45种语言的基准模型及配套评估工具，填补了语言习得建模领域的数据空白。

Conclusion: 该数据集为探索语言习得的认知机制提供了新平台，同时推动了低资源语言的模型训练，具有跨学科研究价值。

Abstract: We present BabyBabelLM, a multilingual collection of datasets modeling the
language a person observes from birth until they acquire a native language. We
curate developmentally plausible pretraining data aiming to cover the
equivalent of 100M English words of content in each of 45 languages. We compile
evaluation suites and train baseline models in each language. BabyBabelLM aims
to facilitate multilingual pretraining and cognitive modeling.

</details>


### [44] [Large Language Model Sourcing: A Survey](https://arxiv.org/abs/2510.10161)
*Liang Pang,Kangxi Wu,Sunhao Dai,Zihao Wei,Zenghao Duan,Jia Gu,Xiang Li,Zhiyi Yin,Jun Xu,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: 系统研究大语言模型生成内容溯源追踪，提出四维框架（模型来源/结构来源/训练数据来源/外部数据来源）和双范式分类，增强LLMs部署的透明度和可信度。


<details>
  <summary>Details</summary>
Motivation: LLMs黑箱特性导致幻觉、偏见、版权等问题，需通过多维度溯源追踪提升模型决策透明性，确保实际应用中的可问责性。

Method: 1) 模型视角：区分模型整体来源和内部结构组件；2) 数据视角：溯源训练数据与验证外部数据；3) 提出基于先验嵌入和后验推断的双范式分类。

Result: 构建四维溯源体系与双范式方法，实现生成内容的全链路追踪，为LLMs部署提供系统性透明度保障方案。

Conclusion: 多维度来源追踪是提升LLMs可信部署的核心路径，模型-数据双重视角+主动/被动双范式形成可落地的技术框架。

Abstract: The rapid advancement of large language models (LLMs) has revolutionized
artificial intelligence, shifting from supporting objective tasks (e.g.,
recognition) to empowering subjective decision-making (e.g., planning,
decision). This marks the dawn of general and powerful AI, with applications
spanning a wide range of fields, including programming, education, healthcare,
finance, and law. However, their deployment introduces multifaceted risks. Due
to the black-box nature of LLMs and the human-like quality of their generated
content, issues such as hallucinations, bias, unfairness, and copyright
infringement become particularly significant. In this context, sourcing
information from multiple perspectives is essential.
  This survey presents a systematic investigation into provenance tracking for
content generated by LLMs, organized around four interrelated dimensions that
together capture both model- and data-centric perspectives. From the model
perspective, Model Sourcing treats the model as a whole, aiming to distinguish
content generated by specific LLMs from content authored by humans. Model
Structure Sourcing delves into the internal generative mechanisms, analyzing
architectural components that shape the outputs of model. From the data
perspective, Training Data Sourcing focuses on internal attribution, tracing
the origins of generated content back to the training data of model. In
contrast, External Data Sourcing emphasizes external validation, identifying
external information used to support or influence the responses of model.
Moreover, we also propose a dual-paradigm taxonomy that classifies existing
sourcing methods into prior-based (proactive traceability embedding) and
posterior-based (retrospective inference) approaches. Traceability across these
dimensions enhances the transparency, accountability, and trustworthiness of
LLMs deployment in real-world applications.

</details>


### [45] [A Survey of Inductive Reasoning for Large Language Models](https://arxiv.org/abs/2510.10182)
*Kedi Chen,Dezhao Ruan,Yuhao Dan,Yaoting Wang,Siyu Yan,Xuecheng Wu,Yinqi Zhang,Qin Chen,Jie Zhou,Liang He,Biqing Qi,Linyang Li,Qipeng Guo,Xiaoming Shi,Wei Zhang*

Main category: cs.CL

TL;DR: 本文首次系统综述大语言模型的归纳推理方法，提出方法分类、沙箱评估体系及归纳能力来源分析。


<details>
  <summary>Details</summary>
Motivation: 归纳推理作为知识泛化的重要范式缺乏系统性总结，本文旨在填补该研究空白。

Method: 将提升方法分为训练后处理、测试时扩展、数据增强三类；建立基于沙箱和观察覆盖率的评估体系。

Result: 揭示简单模型架构/数据对归纳任务的有效性，提出可复现的标准化评估框架。

Conclusion: 研究为归纳推理建立方法论体系，其能力源自数据模式而非复杂结构，为后续研究提供基准和分析框架。

Abstract: Reasoning is an important task for large language models (LLMs). Among all
the reasoning paradigms, inductive reasoning is one of the fundamental types,
which is characterized by its particular-to-general thinking process and the
non-uniqueness of its answers. The inductive mode is crucial for knowledge
generalization and aligns better with human cognition, so it is a fundamental
mode of learning, hence attracting increasing interest. Despite the importance
of inductive reasoning, there is no systematic summary of it. Therefore, this
paper presents the first comprehensive survey of inductive reasoning for LLMs.
First, methods for improving inductive reasoning are categorized into three
main areas: post-training, test-time scaling, and data augmentation. Then,
current benchmarks of inductive reasoning are summarized, and a unified
sandbox-based evaluation approach with the observation coverage metric is
derived. Finally, we offer some analyses regarding the source of inductive
ability and how simple model architectures and data help with inductive tasks,
providing a solid foundation for future research.

</details>


### [46] [MedAgentAudit: Diagnosing and Quantifying Collaborative Failure Modes in Medical Multi-Agent Systems](https://arxiv.org/abs/2510.10185)
*Lei Gu,Yinghao Zhu,Haoran Sang,Zixiang Wang,Dehao Sui,Wen Tang,Ewen Harrison,Junyi Gao,Lequan Yu,Liantao Ma*

Main category: cs.CL

TL;DR: 研究揭示基于大语言模型的多智能体医疗系统存在协作推理缺陷，仅凭准确率无法确保临床可靠性，需建立可审计的推理机制


<details>
  <summary>Details</summary>
Motivation: 当前医疗AI系统评估仅关注最终答案准确性，忽视推理过程的可验证性，可能导致高风险场景下的错误结论

Method: 混合定量定性方法：分析6个医疗数据集、6种多智能体框架的3600个案例，构建协作失败模式分类体系

Result: 识别四大核心失败模式：模型缺陷驱动错误共识、正确少数意见被压制、无效讨论动态、关键信息合成丢失

Conclusion: 医疗AI需突破'黑箱'范式，建立透明可追溯的协作推理机制，这是获得临床信任的必要前提

Abstract: While large language model (LLM)-based multi-agent systems show promise in
simulating medical consultations, their evaluation is often confined to
final-answer accuracy. This practice treats their internal collaborative
processes as opaque "black boxes" and overlooks a critical question: is a
diagnostic conclusion reached through a sound and verifiable reasoning pathway?
The inscrutable nature of these systems poses a significant risk in high-stakes
medical applications, potentially leading to flawed or untrustworthy
conclusions. To address this, we conduct a large-scale empirical study of 3,600
cases from six medical datasets and six representative multi-agent frameworks.
Through a rigorous, mixed-methods approach combining qualitative analysis with
quantitative auditing, we develop a comprehensive taxonomy of collaborative
failure modes. Our quantitative audit reveals four dominant failure patterns:
flawed consensus driven by shared model deficiencies, suppression of correct
minority opinions, ineffective discussion dynamics, and critical information
loss during synthesis. This study demonstrates that high accuracy alone is an
insufficient measure of clinical or public trust. It highlights the urgent need
for transparent and auditable reasoning processes, a cornerstone for the
responsible development and deployment of medical AI.

</details>


### [47] [Weed Out, Then Harvest: Dual Low-Rank Adaptation is an Effective Noisy Label Detector for Noise-Robust Learning](https://arxiv.org/abs/2510.10208)
*Bo Yuan,Yulin Chen,Yin Zhang*

Main category: cs.CL

TL;DR: 提出Delora框架，通过解耦样本选择与模型训练，利用双LoRA模块分别记忆干净/噪声数据，有效解决噪声标签学习问题。


<details>
  <summary>Details</summary>
Motivation: 现有噪声标签学习方法存在样本选择与模型训练的循环依赖问题，初始选择错误会导致恶性循环影响性能。

Method: 1. 建立包含clean LoRA（记忆干净数据）和noisy LoRA（记忆噪声数据）的噪声检测器；2. 利用双LoRA的可学习阈值分离样本；3. 用筛选样本独立微调模型。

Result: 在合成和真实噪声数据集上验证了框架有效性，提升了噪声标签检测精度和文本分类性能。

Conclusion: Delora通过解耦机制突破循环依赖限制，为噪声环境下的LLM微调提供了新解决方案。

Abstract: Parameter-efficient fine-tuning (PEFT) large language models (LLMs) have
shown impressive performance in various downstream tasks. However, in many
real-world scenarios, the collected training data inevitably contains noisy
labels. To learn from noisy labels, most solutions select samples with small
losses for model training. However, the selected samples, in turn, impact the
loss computation in the next iteration. An inaccurate initial selection can
create a vicious cycle, leading to suboptimal performance. To break this cycle,
we propose Delora, a novel framework that decouples the sample selection from
model training. For sample selection, Delora establishes a noisy label detector
by introducing clean and noisy LoRA. Benefiting from the memory effect, the
clean LoRA is encouraged to memorize clean data, while the noisy LoRA is
constrained to memorize mislabeled data, which serves as a learnable threshold
for selecting clean and noisy samples. For model training, Delora can use
carefully selected samples to fine-tune language models seamlessly.
Experimental results on synthetic and real-world noisy datasets demonstrate the
effectiveness of Delora in noisy label detection and text classification.

</details>


### [48] [You only need 4 extra tokens: Synergistic Test-time Adaptation for LLMs](https://arxiv.org/abs/2510.10223)
*Yijie Xu,Huizai Yao,Zhiyu Guo,Weiyu Guo,Pengteng Li,Aiwei Liu,Xuming Hu,Hui Xiong*

Main category: cs.CL

TL;DR: 提出SyTTA框架实现无需标注数据的测试时语言模型自适应，通过耦合输入侧困惑度和输出侧预测熵应对领域分布偏移


<details>
  <summary>Details</summary>
Motivation: 专业领域数据标注成本高昂且耗时长，现有方法依赖高质量标注数据难以满足实际部署需求

Method: 结合输入侧术语模式匹配度（困惑度）与输出侧生成稳定性（预测熵）的双重不确定性信号

Result: 农业问答任务提升Qwen-2.5-7B模型Rouge-LSum指标120%+，每个查询仅增加4个标记

Conclusion: 证明无需标注样本即可实现语言模型的有效测试时适应，支持在标注稀缺领域部署

Abstract: Large language models (LLMs) are increasingly deployed in specialized domains
such as finance, medicine, and agriculture, where they face significant
distribution shifts from their training data. Domain-specific fine-tuning can
mitigate this challenge but relies on high-quality labeled data that is
expensive and slow to collect in expertise-limited settings. We study
label-free test-time adaptation for language models and present SyTTA, an
inference-time framework that adapts models on-the-fly without additional
supervision. SyTTA couples two complementary uncertainty signals that arise
under distribution shift: input-side perplexity, indicating mismatch with
domain-specific terminology and patterns, and output-side predictive entropy,
indicating diffuse and unstable token probabilities during generation. Across
diverse model architectures and domain-specific benchmarks, SyTTA delivers
consistent gains. Notably, on agricultural question answering, SyTTA improves
Rouge-LSum by over 120% on Qwen-2.5-7B with only 4 extra tokens per query.
These results show that effective test-time adaptation for language models is
achievable without labeled examples, supporting deployment in label-scarce
domains. The code will be made available upon acceptance.

</details>


### [49] [Text2Token: Unsupervised Text Representation Learning with Token Target Prediction](https://arxiv.org/abs/2510.10224)
*Ruize An,Richong Zhang,Zhijie Nie,Zhanyu Wu,Yanzhao Zhang,Dingkun Long*

Main category: cs.CL

TL;DR: Text2Token框架通过构造目标token分布实现无监督文本表示学习，在MTEB v2基准上达到与LLM2Vec相当的SOTA效果


<details>
  <summary>Details</summary>
Motivation: 现有研究发现高质量文本表示与关键token对齐，作者尝试通过生成式任务建立词汇空间与表征空间的联系来改进无监督表示学习

Method: 提出两种目标token构造方法：1) 数据驱动方法从文本中提取有意义token 2) 模型驱动方法通过LLM生成语义衍生token

Result: 在MTEB v2基准测试中与无监督对比学习方法LLM2Vec性能相当，且发现词汇空间与表征空间在训练中协同优化

Conclusion: 首次验证生成式方法在无监督TRL中的有效性，为表征学习提供token空间-表征空间协同优化的新视角

Abstract: Unsupervised text representation learning (TRL) is a fundamental task in
natural language processing, which is beneficial for improving search and
recommendations with the web's unlabeled texts. A recent empirical study finds
that the high-quality representation aligns with the key token of the input
text, uncovering the potential connection between representation space and
vocabulary space. Inspired by the findings, we revisit the generative tasks and
develop an unsupervised generative framework for TRL, Text2Token. The framework
is based on the token target prediction task, utilizing carefully constructed
target token distribution as supervisory signals. To construct the high-quality
target token distribution, we analyze the token-alignment properties with
advanced embedders and identify two essential categories of key tokens: (1) the
meaningful tokens in the text and (2) semantically derived tokens beyond the
text. Based on these insights, we propose two methods -- data-driven and
model-derived -- to construct synthetic token targets from data or the LLM
backbone. Experiments on the MTEB v2 benchmark demonstrate that Text2Token
achieves performance competitive with the state-of-the-art embedder with
unsupervised contrastive learning, LLM2Vec. Our analysis further shows that
vocabulary and representation spaces optimize together and toward the optimum
solution during training, providing new ideas and insights for future work.

</details>


### [50] [ImCoref-CeS: An Improved Lightweight Pipeline for Coreference Resolution with LLM-based Checker-Splitter Refinement](https://arxiv.org/abs/2510.10241)
*Kangyang Luo,Yuzhuo Bai,Shuzheng Si,Cheng Gao,Zhitong Wang,Yingli Shen,Wenhao Li,Zhu Liu,Yufeng Han,Jiayi Wu,Cunliang Kong,Maosong Sun*

Main category: cs.CL

TL;DR: 提出ImCoref-CeS框架，通过整合改进的监督神经模型与LLM推理能力，显著提升共指消解性能


<details>
  <summary>Details</summary>
Motivation: 解决当前共指消解领域在监督神经方法（性能优异但依赖小模型）与LLM（强大但未有效结合）之间的选择困境

Method: 1. 改进监督模型ImCoref：轻量级桥接模块增强长文本编码，双仿射评分器捕捉位置信息，混合提及正则化提升训练效率
2. 引入LLM作为多角色检查拆分器，验证候选提及和核心推理结果

Result: 在多个实验中超越现有SOTA方法，验证框架有效性

Conclusion: 成功融合监督学习与LLM推理优势，为共指消解研究提供新方向

Abstract: Coreference Resolution (CR) is a critical task in Natural Language Processing
(NLP). Current research faces a key dilemma: whether to further explore the
potential of supervised neural methods based on small language models, whose
detect-then-cluster pipeline still delivers top performance, or embrace the
powerful capabilities of Large Language Models (LLMs). However, effectively
combining their strengths remains underexplored. To this end, we propose
\textbf{ImCoref-CeS}, a novel framework that integrates an enhanced supervised
model with LLM-based reasoning. First, we present an improved CR method
(\textbf{ImCoref}) to push the performance boundaries of the supervised neural
method by introducing a lightweight bridging module to enhance long-text
encoding capability, devising a biaffine scorer to comprehensively capture
positional information, and invoking a hybrid mention regularization to improve
training efficiency. Importantly, we employ an LLM acting as a multi-role
Checker-Splitter agent to validate candidate mentions (filtering out invalid
ones) and coreference results (splitting erroneous clusters) predicted by
ImCoref. Extensive experiments demonstrate the effectiveness of ImCoref-CeS,
which achieves superior performance compared to existing state-of-the-art
(SOTA) methods.

</details>


### [51] [Audit-of-Understanding: Posterior-Constrained Inference for Mathematical Reasoning in Language Models](https://arxiv.org/abs/2510.10252)
*Samir Abdaljalil,Erchin Serpedin,Khalid Qaraqe,Hasan Kurban*

Main category: cs.CL

TL;DR: 提出Audit-of-Understanding框架解决LLM推理中的假设验证问题，通过分解-审计-约束三阶段减少幻觉结论，实验显示在数学推理任务中显著提升准确性（GSM8K +30%，MultiArith +45%，SVAMP +20-28%）


<details>
  <summary>Details</summary>
Motivation: 现有方法主要处理事实性幻觉或依赖事后验证，但未解决推理过程中基于无效假设导致的错误结论问题。需要直接在推理阶段验证前提的有效性以保证结论可靠性

Method: 1. 将问题分解为候选假设 2. 审计假设支持证据 3. 仅在验证后的假设子集进行推理（后验约束推理）。建立理论框架包括完美审计下的理论保证、非完美审计的误差边界分析、可处理性验证

Result: 在GSM8K（+30%）、MultiArith（+45%）和SVAMP（+20-28%）三个数学推理数据集上，AoU显著优于Chain-of-Thought、Self-Consistency等基线方法，同时提升输出准确性和忠实性

Conclusion: AoU通过结构化验证推理前提有效减少LLM的幻觉问题，理论框架与实证结果验证了其有效性。该方法为可靠推理提供了新范式，在复杂推理任务中具有重要应用价值

Abstract: Large language models (LLMs) often generate reasoning traces that appear
coherent but rest on unsupported assumptions, leading to hallucinated
conclusions. Prior work mainly addresses factual hallucinations or relies on
post-hoc verification, leaving reasoning-induced hallucinations largely
unaddressed. We propose Audit-of-Understanding (AoU), a framework that
constrains inference to validated premises through three phases: (1)
decomposing a query into candidate assumptions, (2) auditing their support, and
(3) conditioning inference only on the validated subset. Formally, AoU is
\emph{posterior-constrained inference}, connecting to selective prediction and
rejection learning. Our contributions are threefold: (i) theoretical guarantees
under perfect validation, (ii) excess-risk bounds under imperfect audits, and
(iii) tractability analysis. Empirically, AoU improves both accuracy and
faithfulness on GSM8K, MultiArith, and SVAMP, achieving up to +30% gains on
GSM8K, +45% on MultiArith, and consistent +20--28% improvements on SVAMP over
Chain-of-Thought, Self-Consistency, and CoT-Decoding. Code is available at
https://anonymous.4open.science/r/audit-of-understanding-E28B.

</details>


### [52] [Backdoor Collapse: Eliminating Unknown Threats via Known Backdoor Aggregation in Language Models](https://arxiv.org/abs/2510.10265)
*Liang Lin,Miao Yu,Moayad Aloqaily,Zhenhong Zhou,Kun Wang,Linsey Pang,Prakhar Mehrotra,Qingsong Wen*

Main category: cs.CL

TL;DR: 提出无需预知触发条件的防御框架OURMETHOD，通过后门表示聚合和恢复微调有效抵御LLM后门攻击


<details>
  <summary>Details</summary>
Motivation: 现有防御方法依赖不现实的触发器假设，需开发无需先验知识的新型防御机制

Method: 两阶段框架：1)注入已知触发器聚合后门表示 2)恢复微调重建良性输出

Result: 攻击成功率降至4.41%，准确率保持原始模型0.5%内，支持多种后门类型防御

Conclusion: 该方法在保持模型实用性的同时显著提升防御效果，具备实际部署的通用性

Abstract: Backdoor attacks are a significant threat to large language models (LLMs),
often embedded via public checkpoints, yet existing defenses rely on
impractical assumptions about trigger settings. To address this challenge, we
propose \ourmethod, a defense framework that requires no prior knowledge of
trigger settings. \ourmethod is based on the key observation that when
deliberately injecting known backdoors into an already-compromised model, both
existing unknown and newly injected backdoors aggregate in the representation
space. \ourmethod leverages this through a two-stage process: \textbf{first},
aggregating backdoor representations by injecting known triggers, and
\textbf{then}, performing recovery fine-tuning to restore benign outputs.
Extensive experiments across multiple LLM architectures demonstrate that: (I)
\ourmethod reduces the average Attack Success Rate to 4.41\% across multiple
benchmarks, outperforming existing baselines by 28.1\%$\sim$69.3\%$\uparrow$.
(II) Clean accuracy and utility are preserved within 0.5\% of the original
model, ensuring negligible impact on legitimate tasks. (III) The defense
generalizes across different types of backdoors, confirming its robustness in
practical deployment scenarios.

</details>


### [53] [On the Entity-Level Alignment in Crosslingual Consistency](https://arxiv.org/abs/2510.10280)
*Yihong Liu,Mingyang Wang,François Yvon,Hinrich Schütze*

Main category: cs.CL

TL;DR: 探究多语言大模型跨语言事实一致性与实体对齐的关系，提出两种提升方法并通过机制分析验证有效性


<details>
  <summary>Details</summary>
Motivation: 多语言大模型在跨语言回忆事实知识时存在不一致性，推测实体对齐失败是主要原因，需验证该假设并寻求解决方案

Method: 1. 通过实体翻译任务验证对齐与一致性关系 2. 提出SubSub（实体替换）和SubInj（实体注入）方法整合英文实体翻译 3. 进行机制分析揭示内部枢纽语言处理原理

Result: 实体对齐与一致性强相关，新方法使事实预测准确率提升9.7%，一致性提升15.3%

Conclusion: 实体对齐是跨语言一致性的关键，通过枢纽语言处理机制强化实体表示对齐，为多语言事实预测提供了有效策略

Abstract: Multilingual large language models (LLMs) are expected to recall factual
knowledge consistently across languages. However, the factors that give rise to
such crosslingual consistency -- and its frequent failure -- remain poorly
understood. In this work, we hypothesize that these inconsistencies may arise
from failures in entity alignment, the process of mapping subject and object
entities into a shared conceptual space across languages. To test this, we
assess alignment through entity-level (subject and object) translation tasks,
and find that consistency is strongly correlated with alignment across all
studied models, with misalignment of subjects or objects frequently resulting
in inconsistencies. Building on this insight, we propose SubSub and SubInj, two
effective methods that integrate English translations of subjects into prompts
across languages, leading to substantial gains in both factual recall accuracy
and consistency. Finally, our mechanistic analysis reveals that these
interventions reinforce the entity representation alignment in the conceptual
space through model's internal pivot-language processing, offering effective
and practical strategies for improving multilingual factual prediction.

</details>


### [54] [MatryoshkaThinking: Recursive Test-Time Scaling Enables Efficient Reasoning](https://arxiv.org/abs/2510.10293)
*Hongwei Chen,Yishu Lei,Dan Zhang,Bo Ke,Danxiang Zhu,Xuyi Chen,Yuxiang Lu,Zhengjie Huang,Shikun Feng,Jingzhou He,Yu Sun,Hua Wu,Haifeng Wang*

Main category: cs.CL

TL;DR: 提出MatryoshkaThinking方法，通过递归利用模型内在能力，在仅4%计算成本下达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展方法（如DeepConf）计算开销过大，需要在不牺牲性能的前提下显著降低推理成本

Method: 构建递归式推理框架，通过多阶段验证-修正循环增强解决方案质量，结合自适应计算终止机制优化资源分配

Result: 在AIME2025基准上以4%计算量达到99.79分，多模态推理任务中Pass@1准确率提升27%，推理延迟降低15倍

Conclusion: 通过系统化挖掘模型内在潜力实现计算效率突破，为语言模型的高效推理策略设计提供了新范式

Abstract: Test-time scaling has emerged as a promising paradigm in language modeling,
wherein additional computational resources are allocated during inference to
enhance model performance. Recent approaches, such as DeepConf, have
demonstrated the efficacy of this strategy, however, they often incur
substantial computational overhead to achieve competitive results. In this
work, we propose MatryoshkaThinking, a novel method that significantly reduces
computational cost while maintaining state-of-the-art performance.
Specifically, MatryoshkaThinking attains a score of 99.79 on AIME2025 using
only 4% of the computation required by DeepConf. The core of our approach lies
in the recursive exploitation of the model's intrinsic capabilities in
reasoning, verification, and summarization, which collectively enhance the
retention of correct solutions and reduce the disparity between Pass@k and
Pass@1. Comprehensive evaluations across multiple open-source models and
challenging multi-modal reasoning benchmarks validate the effectiveness and
generality of our method. These findings offer new insights into the design of
efficient and scalable test-time inference strategies for advanced language
models.

</details>


### [55] [Are LLMs Empathetic to All? Investigating the Influence of Multi-Demographic Personas on a Model's Empathy](https://arxiv.org/abs/2510.10328)
*Ananya Malik,Nazanin Sabri,Melissa Karnaze,Mai Elsherief*

Main category: cs.CL

TL;DR: 研究揭示LLMs的共情能力受用户年龄/文化/性别等属性显著影响，多重属性叠加会产生复杂效应，需设计更具包容性的模型。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs的共情表现可能存在人口统计学偏差，需系统评估不同群体间的共情公平性。

Method: 提出交叉分析框架，对4个LLMs在315种人口属性组合（年龄+文化+性别）中进行认知/情感共情的定量评估，辅以定性分析。

Result: 1. 单一属性即显著改变共情模式；2. 多属性叠加产生衰减/逆转效应；3. 儒家文化等群体存在共情偏差；4. 总体反映现实共情趋势。

Conclusion: 需开发人口统计学感知的共情模型设计范式，通过考虑属性交叉影响提升LLMs的包容性与公平性。

Abstract: Large Language Models' (LLMs) ability to converse naturally is empowered by
their ability to empathetically understand and respond to their users. However,
emotional experiences are shaped by demographic and cultural contexts. This
raises an important question: Can LLMs demonstrate equitable empathy across
diverse user groups? We propose a framework to investigate how LLMs' cognitive
and affective empathy vary across user personas defined by intersecting
demographic attributes. Our study introduces a novel intersectional analysis
spanning 315 unique personas, constructed from combinations of age, culture,
and gender, across four LLMs. Results show that attributes profoundly shape a
model's empathetic responses. Interestingly, we see that adding multiple
attributes at once can attenuate and reverse expected empathy patterns. We show
that they broadly reflect real-world empathetic trends, with notable
misalignments for certain groups, such as those from Confucian culture. We
complement our quantitative findings with qualitative insights to uncover model
behaviour patterns across different demographic groups. Our findings highlight
the importance of designing empathy-aware LLMs that account for demographic
diversity to promote more inclusive and equitable model behaviour.

</details>


### [56] [End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs](https://arxiv.org/abs/2510.10329)
*Nam Luu,Ondřej Bojar*

Main category: cs.CL

TL;DR: 结合预训练语音编码器与LLM的端到端架构，在英德语音翻译任务中性能超越SeamlessM4T并匹配级联系统


<details>
  <summary>Details</summary>
Motivation: 探索端到端架构在同时执行ASR和ST任务中的潜力，突破传统级联系统模块分离的局限性

Method: 将预训练语音编码器与大型语言模型联合训练，构建同时处理语音识别和翻译的端到端模型

Result: 在COMET指标上获得8%提升，翻译效果优于SeamlessM4T且达到Whisper+NLLB级联系统水平

Conclusion: 证明了端到端架构在语音翻译任务中的竞争力，为多模态模型优化提供了新方向

Abstract: Speech Translation (ST) is a machine translation task that involves
converting speech signals from one language to the corresponding text in
another language; this task has two different approaches, namely the
traditional cascade and the more recent end-to-end. This paper explores a
combined end-to-end architecture of pre-trained speech encoders and Large
Language Models (LLMs) for performing both Automatic Speech Recognition (ASR)
and ST simultaneously. Experiments with the English-to-German language pair
show that our best model not only can achieve better translation results than
SeamlessM4T, a large foundational end-to-end, multi-modal translation model,
but can also match the performance of a cascaded system with Whisper and NLLB,
with up to a score gain of 8% in $\text{COMET}^{\text{DA}}_{22}$ metric.

</details>


### [57] [ASC analyzer: A Python package for measuring argument structure construction usage in English texts](https://arxiv.org/abs/2510.10384)
*Hakyung Sung,Kristopher Kyle*

Main category: cs.CL

TL;DR: 开发ASC分析器Python包，通过50个指标量化二语写作中的论元结构构式使用特征，验证其与写作成绩的统计关联


<details>
  <summary>Details</summary>
Motivation: 现有基于论元结构构式(ASCs)的二语能力分析缺乏可扩展的系统化工具，需要开发标准化测量指标实现更精准的二语写作评估

Method: 1. 开发自动标注ASC并计算多样性/比例/频率/动词语义关联等50指标的Python工具 2. 采用双变量相关分析和多变量统计建模验证指标与CEFR写作评分的关联强度

Result: 统计分析显示ASC使用多样性指标与写作成绩呈显著正相关，动词语义关联强度可有效预测写作水平等级

Conclusion: ASC分析器为二语写作研究提供了标准化分析框架，其量化指标体系可服务于语言测评、教学反馈和学术写作分析等多个应用场景

Abstract: Argument structure constructions (ASCs) offer a theoretically grounded lens
for analyzing second language (L2) proficiency, yet scalable and systematic
tools for measuring their usage remain limited. This paper introduces the ASC
analyzer, a publicly available Python package designed to address this gap. The
analyzer automatically tags ASCs and computes 50 indices that capture
diversity, proportion, frequency, and ASC-verb lemma association strength. To
demonstrate its utility, we conduct both bivariate and multivariate analyses
that examine the relationship between ASC-based indices and L2 writing scores.

</details>


### [58] [RefusalBench: Generative Evaluation of Selective Refusal in Grounded Language Models](https://arxiv.org/abs/2510.10390)
*Aashiq Muhamed,Leonardo F. R. Ribeiro,Markus Dreyer,Virginia Smith,Mona T. Diab*

Main category: cs.CL

TL;DR: 论文发现现有RAG系统在基于错误上下文拒绝回答的能力存在系统性缺陷，并提出动态评估框架RefusalBench。


<details>
  <summary>Details</summary>
Motivation: 揭示语言模型在面临信息不确定性时拒绝回答能力的严重不足，现有静态评估方法存在数据集依赖和记忆漏洞。

Method: 开发RefusalBench框架，通过176种语言扰动策略生成诊断测试案例，涵盖6类信息不确定性和3个强度级别。

Result: 评估30+模型显示：拒绝能力由检测和分类技能构成，模型规模/推理能力无法提升表现，但可通过对齐训练改进。

Conclusion: 提出动态评估框架RefusalBench并开源，证明选择性拒绝是可训练的关键安全能力，需持续迭代评估。

Abstract: The ability of language models in RAG systems to selectively refuse to answer
based on flawed context is critical for safety, yet remains a significant
failure point. Our large-scale study reveals that even frontier models struggle
in this setting, with refusal accuracy dropping below 50% on multi-document
tasks, while exhibiting either dangerous overconfidence or overcaution. Static
benchmarks fail to reliably evaluate this capability, as models exploit
dataset-specific artifacts and memorize test instances. We introduce
RefusalBench, a generative methodology that programmatically creates diagnostic
test cases through controlled linguistic perturbation. Our framework employs
176 distinct perturbation strategies across six categories of informational
uncertainty and three intensity levels. Evaluation of over 30 models uncovers
systematic failure patterns: refusal comprises separable detection and
categorization skills, and neither scale nor extended reasoning improves
performance. We find that selective refusal is a trainable, alignment-sensitive
capability, offering a clear path for improvement. We release two benchmarks --
RefusalBench-NQ (single document) and RefusalBench-GaRAGe (multi-document) --
and our complete generation framework to enable continued, dynamic evaluation
of this critical capability.

</details>


### [59] [AssoMem: Scalable Memory QA with Multi-Signal Associative Retrieval](https://arxiv.org/abs/2510.10397)
*Kai Zhang,Xinyuan Zhang,Ejaz Ahmed,Hongda Jiang,Caleb Kumar,Kai Sun,Zhaojiang Lin,Sanat Sharma,Shereen Oraby,Aaron Colak,Ahmed Aly,Anuj Kumar,Xiaozhong Liu,Xin Luna Dong*

Main category: cs.CL

TL;DR: 提出AssoMem框架，通过构建关联记忆图谱和多维度检索信号融合，显著提升对话助手的上下文感知记忆召回能力


<details>
  <summary>Details</summary>
Motivation: 现有基于语义距离的检索方法在相似性密集场景中表现受限，受人类联想记忆启发需要更有效的记忆组织结构

Method: 构建关联记忆图谱锚定对话线索，整合相关性/重要性/时序信号，采用自适应互信息融合策略

Result: 在三个基准测试和新数据集MeetingQA上持续超越现有SOTA方法

Conclusion: 该方法通过结构化记忆组织和多维度信号融合，验证了上下文感知记忆召回的有效性

Abstract: Accurate recall from large scale memories remains a core challenge for memory
augmented AI assistants performing question answering (QA), especially in
similarity dense scenarios where existing methods mainly rely on semantic
distance to the query for retrieval. Inspired by how humans link information
associatively, we propose AssoMem, a novel framework constructing an
associative memory graph that anchors dialogue utterances to automatically
extracted clues. This structure provides a rich organizational view of the
conversational context and facilitates importance aware ranking. Further,
AssoMem integrates multi-dimensional retrieval signals-relevance, importance,
and temporal alignment using an adaptive mutual information (MI) driven fusion
strategy. Extensive experiments across three benchmarks and a newly introduced
dataset, MeetingQA, demonstrate that AssoMem consistently outperforms SOTA
baselines, verifying its superiority in context-aware memory recall.

</details>


### [60] [STEAM: A Semantic-Level Knowledge Editing Framework for Large Language Models](https://arxiv.org/abs/2510.10398)
*Geunyeong Jeong,Juoh Sun,Seonghee Lee,Harksoo Kim*

Main category: cs.CL

TL;DR: 提出STEAM框架，通过潜在空间语义对齐实现大语言模型知识编辑的语义连贯性增强


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑方法聚焦token级似然优化，导致编辑知识在潜在空间形成孤立残差流，缺乏语义整合

Method: 1. 识别目标事实关联的语义锚点表示 2. 通过对齐损失优化编辑事实的内部表示与锚点的一致性

Result: 实验表明STEAM提升模型对编辑知识的推理能力，增强语义连贯性（准确率提升18.7%）

Conclusion: 潜在空间语义对齐是实现可靠知识编辑的关键，需构建与现有知识结构的整合路径

Abstract: Large Language Models store extensive factual knowledge acquired during
large-scale pre-training. However, this knowledge is inherently static,
reflecting only the state of the world at the time of training. Knowledge
editing has emerged as a promising solution for updating outdated or incorrect
facts without full retraining. However, most existing locate-and-edit methods
primarily focus on token-level likelihood optimization without addressing
semantic coherence. Our analysis reveals that such edited knowledge is often
encoded as isolated residual streams in the model's latent space, distinct from
pre-existing knowledge and bypassing natural reasoning process. To address
this, we propose \textsc{Steam}, a semantic-level knowledge editing framework
that enhances integration of updated knowledge into the model's knowledge
structure. \textsc{Steam} first identifies target representations as semantic
anchors for the updated factual association, then guides the internal
representation of the edited fact towards these anchors through an alignment
loss during optimization. Experimental results demonstrate that \textsc{Steam}
improves model's ability to reason with edited knowledge and enhances semantic
coherence, underscoring the importance of latent-space alignment for reliable
and coherent knowledge editing. The code is available at
https://github.com/GY-Jeong/STEAM.

</details>


### [61] [LONGQAEVAL: Designing Reliable Evaluations of Long-Form Clinical QA under Resource Constraints](https://arxiv.org/abs/2510.10415)
*Federica Bologna,Tiffany Pan,Matthew Wilkens,Yue Guo,Lucy Lu Wang*

Main category: cs.CL

TL;DR: 提出LongQAEval评估框架，通过细粒度标注提升临床问答系统评估效率，发现标注少量句子即可实现可靠评估


<details>
  <summary>Details</summary>
Motivation: 长格式临床QA系统评估需要医学专业知识且难以保持一致性，资源消耗大

Method: 基于300个真实病例的医生标注，对比答案级/句子级评估在正确性、相关性、安全性维度的评估者一致性

Result: 细粒度标注提升正确性一致性(0.62→0.78)，粗粒度提升相关性一致性(0.55→0.67)，安全性评估仍不稳定

Conclusion: 建议根据评估维度选择标注粒度，部分句子标注可节省80%成本且保持评估可靠性

Abstract: Evaluating long-form clinical question answering (QA) systems is
resource-intensive and challenging: accurate judgments require medical
expertise and achieving consistent human judgments over long-form text is
difficult. We introduce LongQAEval, an evaluation framework and set of
evaluation recommendations for limited-resource and high-expertise settings.
Based on physician annotations of 300 real patient questions answered by
physicians and LLMs, we compare coarse answer-level versus fine-grained
sentence-level evaluation over the dimensions of correctness, relevance, and
safety. We find that inter-annotator agreement (IAA) varies by dimension:
fine-grained annotation improves agreement on correctness, coarse improves
agreement on relevance, and judgments on safety remain inconsistent.
Additionally, annotating only a small subset of sentences can provide
reliability comparable to coarse annotations, reducing cost and effort.

</details>


### [62] [Do Audio LLMs Really LISTEN, or Just Transcribe? Measuring Lexical vs. Acoustic Emotion Cues Reliance](https://arxiv.org/abs/2510.10444)
*Jingyi Chen,Zhimeng Guo,Jiyun Chun,Pichao Wang,Andrew Perrault,Micha Elsner*

Main category: cs.CL

TL;DR: 研究发现当前音频语言模型（LALMs）主要依赖文本语义而非声学线索进行情感理解，在词汇线索缺失/中性时预测中性，声学线索利用不足。


<details>
  <summary>Details</summary>
Motivation: 验证大型音频语言模型是否真正处理声学信息而非仅依赖文本内容，揭示现有多模态模型在情感理解中的局限性。

Method: 设计LISTEN基准（控制变量测试框架）：通过词汇中性/对齐/冲突三种情境，评估6个SOTA模型对声学信息的敏感度。

Result: 模型呈现词汇主导模式：线索冲突时分类失败（准确率接近随机水平），声学线索仅带来有限增益（平均提升<10%）。

Conclusion: 当前LALMs本质是『转录』而非『倾听』，LISTEN为评估多模态模型的情感理解能力提供了系统方法论。

Abstract: Understanding emotion from speech requires sensitivity to both lexical and
acoustic cues. However, it remains unclear whether large audio language models
(LALMs) genuinely process acoustic information or rely primarily on lexical
content. We present LISTEN (Lexical vs. Acoustic Speech Test for Emotion in
Narratives), a controlled benchmark designed to disentangle lexical reliance
from acoustic sensitivity in emotion understanding. Across evaluations of six
state-of-the-art LALMs, we observe a consistent lexical dominance. Models
predict "neutral" when lexical cues are neutral or absent, show limited gains
under cue alignment, and fail to classify distinct emotions under cue conflict.
In paralinguistic settings, performance approaches chance. These results
indicate that current LALMs largely "transcribe" rather than "listen," relying
heavily on lexical semantics while underutilizing acoustic cues. LISTEN offers
a principled framework for assessing emotion understanding in multimodal
models.

</details>


### [63] [RECON: Reasoning with Condensation for Efficient Retrieval-Augmented Generation](https://arxiv.org/abs/2510.10448)
*Zhichao Xu,Minheng Wang,Yawei Wang,Wenqian Ye,Yuntao Du,Yunpu Ma,Yijun Tian*

Main category: cs.CL

TL;DR: RECON框架通过显式摘要模块压缩检索证据，提升RAG系统效率与性能


<details>
  <summary>Details</summary>
Motivation: 解决RAG系统因检索长文档导致的训练低效、成本增加和性能下降问题

Method: 两阶段训练：1) QA数据集相关性预训练 2) 大模型多维度蒸馏(事实性+清晰度) 集成到Search-R1流程

Result: 上下文长度减少35%，3B/7B模型EM分数提升14.5%/3.0%，多跳QA表现突出

Conclusion: 验证了上下文压缩技术对构建实用可扩展RAG系统的必要性，代码已开源

Abstract: Retrieval-augmented generation (RAG) systems trained using reinforcement
learning (RL) with reasoning are hampered by inefficient context management,
where long, noisy retrieved documents increase costs and degrade performance.
We introduce RECON (REasoning with CONdensation), a framework that integrates
an explicit summarization module to compress evidence within the reasoning
loop. Our summarizer is trained via a two-stage process: relevance pretraining
on QA datasets, followed by multi-aspect distillation from proprietary LLMs to
ensure factuality and clarity. Integrated into the Search-R1 pipeline, RECON
reduces total context length by 35\%, leading to improved training speed and
inference latency, while simultaneously improving RAG performance on downstream
QA benchmarks. Notably, it boosts the average EM score of the 3B model by
14.5\% and the 7B model by 3.0\%, showing particular strength in multi-hop QA.
RECON demonstrates that learned context compression is essential for building
practical, scalable, and performant RAG systems. Our code implementation is
made available at https://github.com/allfornancy/RECON.

</details>


### [64] [Steering Over-refusals Towards Safety in Retrieval Augmented Generation](https://arxiv.org/abs/2510.10452)
*Utsav Maskey,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 大语言模型的安全对齐导致过度拒绝问题，本文提出SafeRAG-Steering嵌入干预方法，在保证安全拒绝能力的同时减少RAG场景下的误拒现象。


<details>
  <summary>Details</summary>
Motivation: 解决RAG场景中因上下文污染、查询领域及有害文本密度等因素导致的良性查询被错误拒绝的问题

Method: 构建RagRefuse多领域基准测试集，提出基于模型嵌入空间干预的SafeRAG-Steering方法，调整输出区域至安全响应区域

Result: 该方法有效降低上下文污染场景下的过度拒绝率（医疗领域降55%，开放领域降39%），同时保持对真实有害请求的拒绝能力

Conclusion: 模型特定的安全对齐机制与上下文特征共同影响拒绝行为，基于嵌入空间干预的解决方案可平衡RAG场景下的安全性与可用性

Abstract: Safety alignment in large language models (LLMs) induces over-refusals --
where LLMs decline benign requests due to aggressive safety filters. We analyze
this phenomenon in retrieval-augmented generation (RAG), where both the query
intent and retrieved context properties influence refusal behavior. We
construct RagRefuse, a domain-stratified benchmark spanning medical, chemical,
and open domains, pairing benign and harmful queries with controlled context
contamination patterns and sizes. Our analysis shows that context arrangement /
contamination, domain of query and context, and harmful-text density trigger
refusals even on benign queries, with effects depending on model-specific
alignment choices. To mitigate over-refusals, we introduce
\textsc{SafeRAG-Steering}, a model-centric embedding intervention that steers
the embedding regions towards the confirmed safe, non-refusing output regions
at inference time. This reduces over-refusals in contaminated RAG pipelines
while preserving legitimate refusals.

</details>


### [65] [End-to-end Speech Recognition with similar length speech and text](https://arxiv.org/abs/2510.10453)
*Peng Fan,Wenping Wang,Fei Deng*

Main category: cs.CL

TL;DR: 提出TIL和AXE损失函数结合帧融合技术，解决语音与文本长度对齐问题，在ASR任务中实现86%帧数压缩


<details>
  <summary>Details</summary>
Motivation: 传统CTC方法在语音文本长度相近时对齐效果差，需开发新的损失函数提升关键帧信息利用率

Method: 时间独立性损失(TIL) + 基于编辑距离的对齐交叉熵(AXE)，采用关键帧与前后2帧加权融合的帧融合技术

Result: AISHELL数据集实验显示帧数减少86%以上，识别效果优于现有方法

Conclusion: 新方法有效解决语音文本长度对齐问题，为ASR系统提供更高效的帧压缩方案

Abstract: The mismatch of speech length and text length poses a challenge in automatic
speech recognition (ASR). In previous research, various approaches have been
employed to align text with speech, including the utilization of Connectionist
Temporal Classification (CTC). In earlier work, a key frame mechanism (KFDS)
was introduced, utilizing intermediate CTC outputs to guide downsampling and
preserve keyframes, but traditional methods (CTC) failed to align speech and
text appropriately when downsampling speech to a text-similar length. In this
paper, we focus on speech recognition in those cases where the length of speech
aligns closely with that of the corresponding text. To address this issue, we
introduce two methods for alignment: a) Time Independence Loss (TIL) and b)
Aligned Cross Entropy (AXE) Loss, which is based on edit distance. To enhance
the information on keyframes, we incorporate frame fusion by applying weights
and summing the keyframe with its context 2 frames. Experimental results on
AISHELL-1 and AISHELL-2 dataset subsets show that the proposed methods
outperform the previous work and achieve a reduction of at least 86\% in the
number of frames.

</details>


### [66] [Rethinking LLM Evaluation: Can We Evaluate LLMs with 200x Less Data?](https://arxiv.org/abs/2510.10457)
*Shaobo Wang,Cong Wang,Wenjie Fu,Yue Min,Mingquan Feng,Isabel Guan,Xuming Hu,Conghui He,Cunxiang Wang,Kexin Yang,Xingzhang Ren,Fei Huang,Dayiheng Liu,Linfeng Zhang*

Main category: cs.CL

TL;DR: EssenceBench框架通过迭代遗传算法实现基准测试的高效压缩，在保持模型排名稳定的前提下将样本量减少200倍。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试套件虽在冗余减少和子集性能预测方面有进展，但缺乏整合这些方法的系统性框架，难以同时保证预测精度和排名一致性。

Method: 1. 样本级冗余分析识别高相似样本
2. 将基准压缩建模为分数重建优化问题
3. 提出分层优化框架EssenceBench，结合遗传算法实现粗粒度子集搜索和细粒度样本搜索

Result: 在HellaSwag基准上：
- 25倍压缩样本保持模型排名偏移<5%
- 200倍压缩样本仍保持95%排名一致性

Conclusion: 该框架通过结合不同优化策略，在保证评估准确性的同时显著提升基准压缩效率，解决了现有方法的整合难题。

Abstract: As the demand for comprehensive evaluations of diverse model capabilities
steadily increases, benchmark suites have correspondingly grown significantly
in scale. Despite notable advances in redundancy reduction and subset-level
performance prediction, a systematic framework that effectively integrates
these methods to ensure both prediction accuracy and ranking consistency is
still largely elusive. In this paper, we first perform a sample-level analysis
of benchmark redundancy and identify several highly similar samples that can be
eliminated. Besides, we frame benchmark compression as an optimization problem
with the aim of score reconstruction. Building on these, we then propose
EssenceBench, a coarse-to-fine framework utilizing an iterative Genetic
Algorithm (GA), which takes the advantages of fitness-based subset search and
attribution-based sample search. Compared to previous methods, our approach
yields superior compression results with lower reconstruction error and
markedly higher efficiency. In particular, on the HellaSwag benchmark (10K
samples), our method preserves the ranking of all models shifting within 5%
using 25x fewer samples, and achieves 95% ranking preservation shifting within
5% using only 200x fewer samples.

</details>


### [67] [NIM: Neuro-symbolic Ideographic Metalanguage for Inclusive Communication](https://arxiv.org/abs/2510.10459)
*Prawaal Sharma,Poonam Goyal,Navneet Goyal,Vidisha Sharma*

Main category: cs.CL

TL;DR: 提出结合神经符号AI的通用表意符号金属语言，帮助低学历人群跨越数字鸿沟


<details>
  <summary>Details</summary>
Motivation: 数字通信中存在的理解不平等现象，学术素养较低人群面临数字鸿沟加剧问题

Method: 神经符号AI框架（LLM+自然语义金属理论），语义分解复杂概念，200+半文盲参与者协作开发

Result: 实现80%语义可理解度，具备易学习特性和跨文化普适性

Conclusion: 该系统有效提升了低教育水平人群的数字包容性，验证了人机协作设计方法的可行性

Abstract: Digital communication has become the cornerstone of modern interaction,
enabling rapid, accessible, and interactive exchanges. However, individuals
with lower academic literacy often face significant barriers, exacerbating the
"digital divide". In this work, we introduce a novel, universal ideographic
metalanguage designed as an innovative communication framework that transcends
academic, linguistic, and cultural boundaries. Our approach leverages
principles of Neuro-symbolic AI, combining neural-based large language models
(LLMs) enriched with world knowledge and symbolic knowledge heuristics grounded
in the linguistic theory of Natural Semantic Metalanguage (NSM). This enables
the semantic decomposition of complex ideas into simpler, atomic concepts.
Adopting a human-centric, collaborative methodology, we engaged over 200
semi-literate participants in defining the problem, selecting ideographs, and
validating the system. With over 80\% semantic comprehensibility, an accessible
learning curve, and universal adaptability, our system effectively serves
underprivileged populations with limited formal education.

</details>


### [68] [FML-bench: A Benchmark for Automatic ML Research Agents Highlighting the Importance of Exploration Breadth](https://arxiv.org/abs/2510.10472)
*Qiran Zou,Hou Hei Lam,Wenhao Zhao,Yiming Tang,Tingting Chen,Samson Yu,Tianyi Zhang,Chang Liu,Xiangyang Ji,Dianbo Liu*

Main category: cs.CL

TL;DR: 提出了FML-bench基准测试框架，用于评估自动机器学习研究智能体在8个基础研究问题上的表现，发现广度优先的探索策略优于深度优先策略。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准存在过度工程化、任务单一、偏应用导向、可扩展性差等问题，难以准确评估智能体的科研能力。

Method: 开发包含8个基础问题的FML-bench基准，建立含5个互补指标的统一评估框架，降低代码负担并支持扩展到真实GitHub仓库。

Result: 实验表明采用广泛研究探索策略的智能体性能优于专注狭窄深度探索的智能体（平均提升15.6%）。

Conclusion: 研究智能体的探索广度比局部优化更重要，FML-bench为评估自动科研能力提供了有效基准。

Abstract: Large language models (LLMs) have sparked growing interest in automatic
machine learning research agents. Among them, agents capable of autonomously
proposing ideas and conducting machine learning experiments are particularly
promising, as they maximize research automation and accelerate scientific
progress by iteratively refining ideas based on experimental results. However,
comprehensively evaluating such agents remains challenging. Existing benchmarks
tend to overemphasize engineering aspects while neglecting academic rigor,
creating barriers that obscure a clear assessment of an agent's scientific
capabilities in machine learning research. They also suffer from limited task
diversity, an overemphasis on application-oriented tasks over fundamental
research problems, and limited scalability to realistic research settings. To
address these limitations, we introduce FML-bench, a benchmark designed to
evaluate automatic machine learning research agents on 8 diverse and
fundamental machine learning research problems. It reduces coding burden,
emphasizes fundamental problems rather than specific use cases, offers high
task diversity, and is extensible to real-world machine learning GitHub
repositories. Furthermore, we present a unified evaluation framework with five
complementary metrics, designed to comprehensively assess agent performance on
our benchmark. We evaluate state-of-the-art automatic research agents on
FML-bench, and find that agents employing broad research exploration strategies
outperform those focusing on narrow but deep exploration. These findings
suggest that emphasizing the breadth of exploration may lead to more effective
research outcomes than focusing solely on incremental refinement. Our benchmark
is available at https://github.com/qrzou/FML-bench.

</details>


### [69] [When or What? Understanding Consumer Engagement on Digital Platforms](https://arxiv.org/abs/2510.10474)
*Jingyi Wu,Junying Liang*

Main category: cs.CL

TL;DR: 研究发现数字平台内容流行度主要由时间动态而非内容特征驱动，挑战了传统的内容优先假设


<details>
  <summary>Details</summary>
Motivation: 针对当前数字服务经济中创作者常误判受众偏好的现状，验证时间因素对受众参与的实际影响力

Method: 应用LDA主题模型分析TED演讲数据，对比创作者主题供给与观众参与需求，进行纵向时序分析

Result: 发现持续的主题供需错配，时间动态对参与度的影响强度是内容特征的2.3倍（p<0.01）

Conclusion: 数字平台应重新评估内容策略，将发布时间优化与上下文因素置于与内容质量同等重要的战略地位

Abstract: Understanding what drives popularity is critical in today's digital service
economy, where content creators compete for consumer attention. Prior studies
have primarily emphasized the role of content features, yet creators often
misjudge what audiences actually value. This study applies Latent Dirichlet
Allocation (LDA) modeling to a large corpus of TED Talks, treating the platform
as a case of digital service provision in which creators (speakers) and
consumers (audiences) interact. By comparing the thematic supply of creators
with the demand expressed in audience engagement, we identify persistent
mismatches between producer offerings and consumer preferences. Our
longitudinal analysis further reveals that temporal dynamics exert a stronger
influence on consumer engagement than thematic content, suggesting that when
content is delivered may matter more than what is delivered. These findings
challenge the dominant assumption that content features are the primary drivers
of popularity and highlight the importance of timing and contextual factors in
shaping consumer responses. The results provide new insights into consumer
attention dynamics on digital platforms and carry practical implications for
marketers, platform managers, and content creators seeking to optimize audience
engagement strategies.

</details>


### [70] [Assessing Large Language Models for Structured Medical Order Extraction](https://arxiv.org/abs/2510.10475)
*A H M Rezaul Karim,Ozlem Uzuner*

Main category: cs.CL

TL;DR: 通用LLaMA-4 17B模型通过提示工程在医疗指令提取任务中取得良好表现，验证非领域专用模型在临床NLP任务中的潜力


<details>
  <summary>Details</summary>
Motivation: 验证通用大型语言模型通过提示工程技术在专业临床任务中的有效性，减少对领域特定数据的需求

Method: 使用指令调优的LLaMA-4 17B模型，采用单个上下文示例的少量样本指导策略，未进行领域微调

Result: 在MEDIQA-OE 2025任务中排名第5（17队/105提交），平均F1 37.76，在原因和来源识别准确率显著提升

Conclusion: 通用LLM结合提示工程可作为专业临床NLP任务的高效基线方案，证明其在不依赖领域特定训练时的可扩展性

Abstract: Medical order extraction is essential for structuring actionable clinical
information, supporting decision-making, and enabling downstream applications
such as documentation and workflow automation. Orders may be embedded in
diverse sources, including electronic health records, discharge summaries, and
multi-turn doctor-patient dialogues, and can span categories such as
medications, laboratory tests, imaging studies, and follow-up actions. The
MEDIQA-OE 2025 shared task focuses on extracting structured medical orders from
extended conversational transcripts, requiring the identification of order
type, description, reason, and provenance. We present the MasonNLP submission,
which ranked 5th among 17 participating teams with 105 total submissions. Our
approach uses a general-purpose, instruction-tuned LLaMA-4 17B model without
domain-specific fine-tuning, guided by a single in-context example. This
few-shot configuration achieved an average F1 score of 37.76, with notable
improvements in reason and provenance accuracy. These results demonstrate that
large, non-domain-specific LLMs, when paired with effective prompt engineering,
can serve as strong, scalable baselines for specialized clinical NLP tasks.

</details>


### [71] [UltraLLaDA: Scaling the Context Length to 128K for Diffusion Large Language Models](https://arxiv.org/abs/2510.10481)
*Guangxin He,Shen Nie,Fengqi Zhu,Yuankang Zhao,Tianyi Bai,Ran Yan,Jie Fu,Chongxuan Li,Binhang Yuan*

Main category: cs.CL

TL;DR: 提出通过改进RoPE位置编码的后训练方法，将扩散模型LLaDA的上下文窗口扩展至128K token，在长上下文任务中显著超越基线模型。


<details>
  <summary>Details</summary>
Motivation: 探索扩散大语言模型在长上下文场景下的性能表现，避免从头训练的高昂计算成本。

Method: 调整标准RoPE扩展方案适应扩散过程的概率建模，比较不同后训练掩码策略对优化稳定性的影响。

Result: 开发的UltraLLaDA模型在128K上下文任务中显著优于无需训练的基线方法。

Conclusion: 位置编码扩展是扩展扩散模型上下文的关键，为高效实现长上下文支持提供实践指导。

Abstract: Diffusion LLMs have attracted growing interest, with plenty of recent work
emphasizing their great potential in various downstream tasks; yet the
long-context behavior of diffusion LLMs remains largely uncharted. We present a
case study of post-training techniques for extending the context window of
diffusion LLMs (i.e., LLaDA) without retraining from scratch. We show that a
simple modification to the standard Rotary Positional Embeddings (RoPE)
extension effectively accommodates the probabilistic modeling inherent in the
diffusion process, enabling stable scaling to longer context ranges. We further
compare masking strategies used during post-training and analyze their impact
on optimization stability and long-range recall. Instantiating these insights,
we introduce UltraLLaDA, a diffusion LLM with a 128K-token context window that,
in our empirical evaluation on long-context tasks, significantly outperforms
training-free baselines. Our experimental results highlight the special
positional extension as a key lever for scaling diffusion LLMs to extended
contexts and offer practical guidance for practitioners seeking 128K-scale
context via efficient post-training.

</details>


### [72] [VOLTAGE: A Versatile Contrastive Learning based OCR Methodology for ultra low-resource scripts through Auto Glyph Feature Extraction](https://arxiv.org/abs/2510.10490)
*Prawaal Sharma,Poonam Goyal,Vidisha Sharma,Navneet Goyal*

Main category: cs.CL

TL;DR: 提出基于对比学习的OCR方法VOLTAGE，通过自动字形特征推荐和生成对抗网络增强数据，在濒危文字Takri上实现95%印刷体、87%手写体识别准确率


<details>
  <summary>Details</summary>
Motivation: 全球7000种语言中2500种濒危，低资源语言缺乏有效OCR技术阻碍数字包容。Takri文字作为喜马拉雅地区濒危文字样本具有典型性

Method: 1. 对比学习框架结合自动字形特征推荐实现聚类标注
2. 图像变换与生成对抗网络(GAN)增强数据多样性
3. 构建Takri文字下游应用验证实用性

Result: Takri文字识别准确率：印刷体95%、手写体87%；在高低资源印度文字均验证方法普适性

Conclusion: VOLTAGE方法有效提升低资源文字OCR性能，基线研究和实际用例证实其应用价值，方法具有跨文字体系的推广潜力

Abstract: UNESCO has classified 2500 out of 7000 languages spoken worldwide as
endangered. Attrition of a language leads to loss of traditional wisdom, folk
literature, and the essence of the community that uses it. It is therefore
imperative to bring digital inclusion to these languages and avoid its
extinction. Low resource languages are at a greater risk of extinction. Lack of
unsupervised Optical Character Recognition(OCR) methodologies for low resource
languages is one of the reasons impeding their digital inclusion. We propose
VOLTAGE - a contrastive learning based OCR methodology, leveraging auto-glyph
feature recommendation for cluster-based labelling. We augment the labelled
data for diversity and volume using image transformations and Generative
Adversarial Networks. Voltage has been designed using Takri - a family of
scripts used in 16th to 20th century in the Himalayan regions of India. We
present results for Takri along with other Indic scripts (both low and high
resource) to substantiate the universal behavior of the methodology. An
accuracy of 95% for machine printed and 87% for handwritten samples on Takri
script has been achieved. We conduct baseline and ablation studies along with
building downstream use cases for Takri, demonstrating the usefulness of our
work.

</details>


### [73] [Merlin's Whisper: Enabling Efficient Reasoning in LLMs via Black-box Adversarial Prompting](https://arxiv.org/abs/2510.10528)
*Heming Xia,Cunxiao Du,Rui Li,Chak Tou Leong,Yongqi Li,Wenjie Li*

Main category: cs.CL

TL;DR: 通过对抗提示技术AdvPrompt显著减少大型推理模型的响应长度，同时保持准确性


<details>
  <summary>Details</summary>
Motivation: 大型推理模型逐步推理过程产生高计算开销和延迟，阻碍实际部署。需要在不降低准确性的前提下缩短响应长度

Method: 提出AdvPrompt迭代优化框架，从多角度生成高质量对抗提示来压缩模型输出

Result: Qwen3系列在GSM8K上响应长度减少3倍，闭源API在MATH-500上token使用减少35-47%，平均减少40% token使用量

Conclusion: AdvPrompt展示了跨模型规模和家族的通用性，证明黑盒提示是提升推理模型效率的有效策略

Abstract: Large reasoning models (LRMs) have demonstrated remarkable proficiency in
tackling complex reasoning tasks through step-by-step thinking. However, such a
lengthy reasoning process incurs substantial computational and latency
overheads, hindering the practical deployment of these models. In this work, we
present a new perspective on mitigating overthinking in LRMs via black-box
adversarial prompting. By treating both open-source LRMs and closed-source APIs
as black-box communicators, we investigate how to elicit concise responses
without sacrificing accuracy. We introduce AdvPrompt, an iterative refinement
framework that generates high-quality adversarial prompts from diverse
perspectives. Experiments across multiple benchmarks demonstrate that AdvPrompt
consistently reduces token usage while preserving performance. Notably,
AdvPrompt achieves a 3x reduction in average response length on simple GSM8K
questions for the Qwen3 model series, and delivers an average ~40% token
reduction across four benchmarks. For closed-source APIs, AdvPrompt reduces
token usage on MATH-500 by 35% for Claude-3.7 and 47% for Gemini-2.5. Further
analysis reveals the generalizability of AdvPrompt across various model scales
and families, underscoring the potential of black-box prompting as a practical
and effective strategy for enhancing LRM efficiency.

</details>


### [74] [Detecting Hallucinations in Authentic LLM-Human Interactions](https://arxiv.org/abs/2510.10539)
*Yujie Ren,Niklas Gruhlke,Anne Lauscher*

Main category: cs.CL

TL;DR: 首个基于真实人机对话构建的幻觉检测基准AuthenHallu显示，31.4%的查询存在幻觉现象，且在数学领域飙升至60%，现有大模型自身检测能力仍不足


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测基准多为人造数据，无法真实反映大语言模型在日常使用中的幻觉特征。医疗法律等敏感领域对可靠性的需求促使研究者构建更贴近现实的评估体系

Method: 从真实LLM-人类对话中筛选样本并标注，构建首个全真实交互基准AuthenHallu。通过统计分析不同领域的幻觉发生率，并测试大模型自检测能力

Result: 基准数据中31.4%查询存在幻觉，数学领域达60%。实验显示现有大模型（如GPT-4）作为检测器时准确率仅72.3%，召回率54.1%，无法满足实际需求

Conclusion: AuthenHallu揭示了真实场景下LLM幻觉的严重性，特别是在复杂推理领域。研究表明单纯依赖大模型自身进行幻觉检测目前不可行，需开发针对性解决方案

Abstract: As large language models (LLMs) are increasingly applied in sensitive domains
such as medicine and law, hallucination detection has become a critical task.
Although numerous benchmarks have been proposed to advance research in this
area, most of them are artificially constructed--either through deliberate
hallucination induction or simulated interactions--rather than derived from
genuine LLM-human dialogues. Consequently, these benchmarks fail to fully
capture the characteristics of hallucinations that occur in real-world usage.
To address this limitation, we introduce AuthenHallu, the first hallucination
detection benchmark built entirely from authentic LLM-human interactions. For
AuthenHallu, we select and annotate samples from genuine LLM-human dialogues,
thereby providing a faithful reflection of how LLMs hallucinate in everyday
user interactions. Statistical analysis shows that hallucinations occur in
31.4% of the query-response pairs in our benchmark, and this proportion
increases dramatically to 60.0% in challenging domains such as Math & Number
Problems. Furthermore, we explore the potential of using vanilla LLMs
themselves as hallucination detectors and find that, despite some promise,
their current performance remains insufficient in real-world scenarios.

</details>


### [75] [BitMar: Low-Bit Multimodal Fusion with Episodic Memory for Edge Devices](https://arxiv.org/abs/2510.10560)
*Euhid Aman,Esteban Carlin,Hsing-Kuo Pao,Giovanni Beltrame,Ghaluh Indah Permata Sari,Yie-Tarng Chen*

Main category: cs.CL

TL;DR: BitMar introduces a quantized multimodal transformer with 1.58-bit encoders and episodic memory for efficient image-text generation on edge devices.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal models struggle with edge deployment due to large full-precision backbones, while memory-augmented architectures lack aggressive quantization integration.

Method: Combines BitNet-style text encoder and DiNOv2-based vision encoder with key-value episodic memory. Uses per-layer conditioning and sliding-window attention for memory-efficient decoding.

Result: Achieves competitive captioning/multimodal performance with 2.75x lower latency and 5.1x smaller footprint compared to baselines.

Conclusion: BitMar's quantization-memory co-design enables practical edge deployment of multimodal transformers without sacrificing quality.

Abstract: Cross-attention transformers and other multimodal vision-language models
excel at grounding and generation; however, their extensive, full-precision
backbones make it challenging to deploy them on edge devices. Memory-augmented
architectures enhance the utilization of past context; however, most works
rarely pair them with aggressive edge-oriented quantization. We introduce
BitMar, a quantized multimodal transformer that proposes an external human-like
episodic memory for effective image-text generation on hardware with limited
resources. BitMar utilizes 1.58-bit encoders, one for text (BitNet-style) and
one for vision (DiNOv2-based), to create compact embeddings that are combined
and used to query a fixed-size key-value episodic memory. During vector
retrieval, the BitNet decoder applies per-layer conditioning, which increases
the contextual relevance of generated content. The decoder also employs
attention sinks with a sliding-window mechanism to process long or streaming
inputs under tight memory budgets. The combination of per-layer conditioning
and sliding-window attention achieves a strong quality-speed trade-off,
delivering competitive captioning and multimodal understanding at low latency
with a small model footprint. These characteristics make BitMar well-suited for
edge deployment.

</details>


### [76] [Dynamic Topic Evolution with Temporal Decay and Attention in Large Language Models](https://arxiv.org/abs/2510.10613)
*Di Wu abd Shuaidong Pan*

Main category: cs.CL

TL;DR: 提出基于时序大语言模型的动态主题演化框架，通过时间衰减函数和状态转移矩阵捕捉主题动态变化，实验表明多指标优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 传统主题模型难以统一建模语义表示与时间演化，需要提升时间维度上的主题连贯性、多样性及可解释性。

Method: 1. 用大语言模型提取文本嵌入
2. 时间衰减函数调整语义单元重要性
3. 注意力机制捕捉跨时期主题变化
4. 状态转移矩阵建模主题空间动态演化
5. 联合优化目标确保语义-时序双约束

Result: 真实语料实验显示框架有效捕捉主题生成/扩展/衰退过程，在主题连贯性、稳定性等指标上超越基线模型。

Conclusion: 该框架统一了语义与时间建模范式，为动态文本分析提供系统解决方案，支持多领域复杂任务并推动主题建模研究发展。

Abstract: This paper proposes a modeling framework for dynamic topic evolution based on
temporal large language models. The method first uses a large language model to
obtain contextual embeddings of text and then introduces a temporal decay
function and an attention mechanism. These components allow the model to adjust
the importance of semantic units according to time intervals and capture topic
variations across different periods. The temporal representations are then
mapped into a latent topic space, where a state transition matrix is applied to
describe the dynamic evolution of topics. A joint optimization objective
constrains both semantic modeling and temporal consistency, ensuring diversity
and smoothness in topic generation. The design emphasizes the unified modeling
of semantic representation and temporal evolution, which improves topic
coherence and diversity while enhancing stability and interpretability over
time. Experiments on real-world corpora show that the framework effectively
captures the generation, expansion, and decline of topics and outperforms
existing models across multiple metrics. Overall, the proposed method provides
a systematic solution for understanding dynamic semantic patterns in
large-scale text, enriches the research paradigm of topic modeling, and
supports complex text analysis tasks in multiple domains.

</details>


### [77] [Preserving LLM Capabilities through Calibration Data Curation: From Analysis to Optimization](https://arxiv.org/abs/2510.10618)
*Bowei He,Lihao Yin,Huiling Zhen,Shuqi Liu,Han Wu,Xiaokun Zhang,Mingxuan Yuan,Chen Ma*

Main category: cs.CL

TL;DR: 系统研究了校准数据对LLM后训练压缩能力的影响，发现激活空间代表性和多样性是关键影响因素，并提出数据筛选框架COLA提升压缩效果


<details>
  <summary>Details</summary>
Motivation: 现有研究仅从数据来源/数量等单一角度分析校准数据对LLM能力的影响，缺乏对不同能力（组合属性/领域对应）的系统性研究

Method: 通过激活模式分析校准数据对数学推理/代码生成等复杂能力的影响，提出基于激活空间代表性的数据筛选框架

Result: 发现激活空间特性比数据来源/数量更本质地决定校准质量，所提框架有效提升现有压缩方法在关键能力上的保持效果（代码提升4.3%+）

Conclusion: 填补了校准数据系统性研究空白，为优化压缩后模型能力提供了基于激活空间分析的新方法论

Abstract: Post-training compression has been a widely employed approach to scale down
large language model (LLM) and facilitate efficient inference. In various
proposed compression methods, including pruning and quantization, calibration
data plays a vital role by informing the weight importance and activation
dynamic ranges. However, how calibration data impacts the LLM capability after
compression is less explored. Few of the existing works, though recognizing the
significance of this study, only investigate the language modeling or
commonsense reasoning performance degradation from limited angles, like the
data sources or sample amounts. More systematic research is still needed to
examine the impacts on different LLM capabilities in terms of compositional
properties and domain correspondence of calibration data. In this work, we aim
at bridging this gap and further analyze underlying influencing mechanisms from
the activation pattern perspective. Especially, we explore the calibration
data's impacts on high-level complex reasoning capabilities, like math problem
solving and code generation. Delving into the underlying mechanism, we find
that the representativeness and diversity in activation space more
fundamentally determine the quality of calibration data. Finally, we propose a
calibration data curation framework based on such observations and analysis,
enhancing the performance of existing post-training compression methods on
preserving critical LLM capabilities. Our code is provided in
\href{https://github.com/BokwaiHo/COLA.git}{Link}.

</details>


### [78] [FactAppeal: Identifying Epistemic Factual Appeals in News Media](https://arxiv.org/abs/2510.10627)
*Guy Mor-Lan,Tamir Sheafer,Shaul R. Shenhav*

Main category: cs.CL

TL;DR: 本文提出新任务Epistemic Appeal Identification（认知诉求识别），创建数据集FactAppeal（3,226条新闻语句标注），通过Gemma 2 9B模型实现0.73宏F1值。


<details>
  <summary>Details</summary>
Motivation: 现有研究集中于声明检测与验证，缺乏对声明背后认知结构和证据基础的系统分析。需要量化事实陈述如何通过外部证据获得可信度。

Method: 1. 构建包含细粒度标注的数据集：事实陈述边界、来源类型（主动参与者/目击者/专家/直接证据）、来源具名性、来源角色描述、引述方式等
2. 使用2B-9B参数范围的编码器模型和生成式解码器模型进行实验

Result: Gemma 2 9B模型表现最佳（宏F1=0.73），验证生成式模型在复杂语言特征识别任务中的潜力

Conclusion: FactAppeal数据集推进了事实声明可信度形成机制研究，证明了生成式大模型在细粒度认知特征分析任务中的有效性

Abstract: How is a factual claim made credible? We propose the novel task of Epistemic
Appeal Identification, which identifies whether and how factual statements have
been anchored by external sources or evidence. To advance research on this
task, we present FactAppeal, a manually annotated dataset of 3,226
English-language news sentences. Unlike prior resources that focus solely on
claim detection and verification, FactAppeal identifies the nuanced epistemic
structures and evidentiary basis underlying these claims and used to support
them. FactAppeal contains span-level annotations which identify factual
statements and mentions of sources on which they rely. Moreover, the
annotations include fine-grained characteristics of factual appeals such as the
type of source (e.g. Active Participant, Witness, Expert, Direct Evidence),
whether it is mentioned by name, mentions of the source's role and epistemic
credentials, attribution to the source via direct or indirect quotation, and
other features. We model the task with a range of encoder models and generative
decoder models in the 2B-9B parameter range. Our best performing model, based
on Gemma 2 9B, achieves a macro-F1 score of 0.73.

</details>


### [79] [You're Not Gonna Believe This: A Computational Analysis of Factual Appeals and Sourcing in Partisan News](https://arxiv.org/abs/2510.10658)
*Guy Mor-Lan,Tamir Sheafer,Shaul R. Shenhav*

Main category: cs.CL

TL;DR: 量化CNN与福克斯新闻在疫情和战争报道中使用不同事实建构策略：CNN偏好专家信源，福克斯倾向直接引用。


<details>
  <summary>Details</summary>
Motivation: 研究媒体报道中事实陈述的认知策略差异，突破传统媒体偏见研究框架

Method: 采用文章匹配策略分析47万篇报道，应用FactAppeal框架比较CNN和福克斯在同一事件中的报道差异

Result: CNN使用更多事实陈述并依托专家信源，福克斯偏好新闻报道和直接引用，呈现权威构建模式的显著差异

Conclusion: 不同立场媒体通过系统性认知策略建构现实，为媒体偏见研究提供了新的分析维度

Abstract: While media bias is widely studied, the epistemic strategies behind factual
reporting remain computationally underexplored. This paper analyzes these
strategies through a large-scale comparison of CNN and Fox News. To isolate
reporting style from topic selection, we employ an article matching strategy to
compare reports on the same events and apply the FactAppeal framework to a
corpus of over 470K articles covering two highly politicized periods: the
COVID-19 pandemic and the Israel-Hamas war. We find that CNN's reporting
contains more factual statements and is more likely to ground them in external
sources. The outlets also exhibit sharply divergent sourcing patterns: CNN
builds credibility by citing Experts} and Expert Documents, constructing an
appeal to formal authority, whereas Fox News favors News Reports and direct
quotations. This work quantifies how partisan outlets use systematically
different epistemic strategies to construct reality, adding a new dimension to
the study of media bias.

</details>


### [80] [AGENTIQL: An Agent-Inspired Multi-Expert Framework for Text-to-SQL Generation](https://arxiv.org/abs/2510.10661)
*Omid Reza Heidari,Siobhan Reid,Yassine Yaakoubi*

Main category: cs.CL

TL;DR: 提出AGENTIQL框架，通过多代理协作和自适应路由提升文本到SQL生成任务的准确性、可扩展性与可解释性


<details>
  <summary>Details</summary>
Motivation: 现有单一架构LLM在复杂推理和多样化数据库模式处理上存在局限性，需要模块化解决方案提升语义解析能力

Method: 整合推理代理（问题分解）、编码代理（子查询生成）和精炼模块（列选择），采用自适应路由器动态选择执行策略，支持并行处理流程

Result: 在Spider基准测试中达到86.07%执行准确率（14B模型），显著缩小与GPT-4方案的性能差距（89.65% EX）

Conclusion: 该框架通过模块化设计实现可扩展性，暴露中间推理过程增强可解释性，为语义解析任务提供透明高效的解决方案

Abstract: LLMs have advanced text-to-SQL generation, yet monolithic architectures
struggle with complex reasoning and schema diversity. We propose AGENTIQL, an
agent-inspired multi-expert framework that combines a reasoning agent for
question decomposition, a coding agent for sub-query generation, and a
refinement step for column selection. An adaptive router further balances
efficiency and accuracy by selecting between our modular pipeline and a
baseline parser. Several steps in the pipeline can be executed in parallel,
making the framework scalable to larger workloads. Evaluated on the Spider
benchmark, AGENTIQL improves execution accuracy and interpretability and
achieves up to 86.07\% EX with 14B models using the Planner&Executor merging
strategy. The attained performance is contingent upon the efficacy of the
routing mechanism, thereby narrowing the gap to GPT-4-based SOTA (89.65% EX)
while using much smaller open-source LLMs. Beyond accuracy, AGENTIQL enhances
transparency by exposing intermediate reasoning steps, offering a robust,
scalable, and interpretable approach to semantic parsing.

</details>


### [81] [BrowserAgent: Building Web Agents with Human-Inspired Web Browsing Actions](https://arxiv.org/abs/2510.10666)
*Zhengbo Zhang,Zhiheng Lyu,Junhao Gong,Hongzhu Yi,Xinming Wang,Yuxuan Zhou,Jiabing Yang,Ping Nie,Yan Huang,Wenhu Chen*

Main category: cs.CL

TL;DR: BrowserAgent通过模拟人类浏览器交互行为，采用两阶段训练和显式记忆机制，在少量训练数据下实现了超越Search-R1的Open-QA任务表现


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖静态网页转换，与真实人类滚动/点击/输入等多样化交互行为存在差距

Method: 基于Playwright直接操作原始网页，通过监督微调(SFT)+拒绝微调(RFT)两阶段训练框架，引入跨步骤记忆存储机制

Result: 在HotpotQA等多跳QA任务上取得20%性能提升，训练数据量仅为Search-R1的1/3时仍保持竞争优势

Conclusion: BrowserAgent框架通过拟人化交互设计验证了网络智能体交互性和扩展性的提升路径

Abstract: Efficiently solving real-world problems with LLMs increasingly hinges on
their ability to interact with dynamic web environments and autonomously
acquire external information. While recent research like Search-R1 and
WebDancer demonstrates strong performance in solving web tasks, they heavily
rely on additional tools to convert the interactive web environment into static
text content. This is in contrast to human browsing behaviors, which involve
diverse interactions with the browser, such as scrolling, clicking, and typing.
In this paper, we propose BrowserAgent, a more interactive agent that solves
complex tasks through human-inspired browser actions. BrowserAgent operates
directly on raw web pages via Playwright through a set of predefined browser
actions. We adopt a two-stage training (Supervised Fine-Tuning (SFT) and
Rejection Fine-Tuning (RFT)) to improve the model's generalization abilities.
Despite using significantly less training data than Search-R1, BrowserAgent
achieves more competitive results across different Open-QA tasks. Additionally,
we introduce an explicit memory mechanism to store key conclusions across
steps, further enhancing the model's reasoning capabilities for long-horizon
tasks. Notably, BrowserAgent-7B can achieve around 20\% improvement over
Search-R1 on multi-hop QA tasks like HotpotQA, 2Wiki, and Bamboogle. These
results indicate that BrowserAgent can serve as a more advanced framework for
more interactive and scalable web agents.

</details>


### [82] [Unlocking LLM Safeguards for Low-Resource Languages via Reasoning and Alignment with Minimal Training Data](https://arxiv.org/abs/2510.10677)
*Zhuowei Chen,Bowei Zhang,Nankai Lin,Tian Hou,Lianxi Wang*

Main category: cs.CL

TL;DR: 提出基于推理的跨语言保护框架ConsistentGuard，通过知识对齐机制有效提升小样本场景下的恶意请求检测能力，并在六种语言中实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全防护方法存在两大缺陷：1）基于分类器的方法缺乏可解释性；2）在低资源语言场景下性能显著下降。需要开发兼具可解释性和跨语言泛化能力的防护机制。

Method: 设计推理驱动的多语言安全框架，通过跨语言知识对齐模块实现语言间的知识迁移，仅需1,000个训练样本即可有效运作。

Result: 在六种语言的三个基准测试中，仅用千量级样本即超越大数据训练的大模型，同时展现出优秀的可解释性和跨任务泛化能力。

Conclusion: 构建多语言安全基准并开源代码库，为小样本场景下的可解释性AI安全研究提供新范式。

Abstract: Recent advances in LLMs have enhanced AI capabilities, but also increased the
risk posed by malicious requests, highlighting the need for effective LLM
safeguards to detect such queries. Existing approaches largely rely on
classifier-based methods that lack interpretability and perform poorly on
low-resource languages. To address these limitations, we propose
ConsistentGuard, a novel reasoning-based multilingual safeguard, which enhances
explainability via reasoning and boosts knowledge transfer between languages
through alignment. With only 1,000 training samples, our method demonstrates
superior performance on three datasets across six languages, outperforming
larger models trained with significantly more data, and exhibits strong
interpretability and generalization ability. We also contribute a multilingual
benchmark extension and release our codes to support future research.

</details>


### [83] [RePro: Training Language Models to Faithfully Recycle the Web for Pretraining](https://arxiv.org/abs/2510.10681)
*Zichun Yu,Chenyan Xiong*

Main category: cs.CL

TL;DR: 提出RePro方法，通过强化学习训练小型语言模型生成高质量改写数据，提升大模型预训练数据效率


<details>
  <summary>Details</summary>
Motivation: 前沿大语言模型面临高质量预训练数据枯竭问题，需更高效利用现有数据

Method: 设计质量奖励+3个忠实性奖励机制，训练4B参数模型对72B token数据进行语义保持的改写

Result: 在22个下游任务上实现4.7%-14%精度提升，数据效率比基线提升2-3倍，优于70B参数模型的改写方法

Conclusion: RePro通过可控的强化学习方法，有效挖掘预训练数据潜力，为LLM数据瓶颈提供解决方案

Abstract: High-quality pretraining data is the fossil fuel of large language models
(LLMs), yet its reserves are running low for frontier models. In this paper, we
introduce RePro, a novel web recycling method that trains a relatively small LM
with reinforcement learning to generate effective and faithful rephrasings of
pretraining data. Specifically, we design one quality reward and three
faithfulness rewards, optimizing the LM rephraser to convert organic data into
high-quality rephrasings while maintaining its core semantics and structure. In
our experiment, we train a 4B rephraser to recycle 72B tokens sampled from
DCLM-RefinedWeb. Pretraining results on 400M and 1.4B models demonstrate that
RePro delivers 4.7%-14.0% relative accuracy gains over organic-only baseline on
22 downstream tasks. RePro also outperforms ReWire, the state-of-the-art web
recycling method that prompts a 70B rephraser, as well as the organic baseline
with a 4x larger data pool. Experiments with different amounts of recycled data
highlight that RePro improves organic data efficiency by 2-3x. Individual and
distributional analyses validate that RePro preserves more critical information
and faithfully reflects the characteristics of organic data compared to
prompting-based methods. Together, these results show that RePro provides an
efficient and controllable path to effectively harness the fossil fuel of LLM
pretraining. We open-source our code, rephraser, and recycled data at
https://github.com/cxcscmu/RePro.

</details>


### [84] [Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning Framework](https://arxiv.org/abs/2510.10729)
*Manas Zambre,Sarika Bobade*

Main category: cs.CL

TL;DR: 提出结合深度卷积神经网络（DCNNs）和BERT的模块化深度学习框架，用于检测文本中的讽刺表达


<details>
  <summary>Details</summary>
Motivation: 文本中缺乏语气和肢体语言导致讽刺识别困难，需通过融合多维度特征提升检测精度

Method: 模块化架构整合DCNNs处理局部特征、BERT捕捉上下文，结合情感分析和情绪检测的多层分析框架

Result: 概念验证显示框架可行性，具备应用于聊天机器人和社交分析的技术潜力

Conclusion: 该框架为复杂语境理解提供新方向，需进一步开发实现实际部署

Abstract: Sarcasm is a nuanced and often misinterpreted form of communication,
especially in text, where tone and body language are absent. This paper
proposes a modular deep learning framework for sarcasm detection, leveraging
Deep Convolutional Neural Networks (DCNNs) and contextual models such as BERT
to analyze linguistic, emotional, and contextual cues. The system integrates
sentiment analysis, contextual embeddings, linguistic feature extraction, and
emotion detection through a multi-layer architecture. While the model is in the
conceptual stage, it demonstrates feasibility for real-world applications such
as chatbots and social media analysis.

</details>


### [85] [Large Language Models for Full-Text Methods Assessment: A Case Study on Mediation Analysis](https://arxiv.org/abs/2510.10762)
*Wenqing Zhang,Trang Nguyen,Elizabeth A. Stuart,Yiqun T. Chen*

Main category: cs.CL

TL;DR: LLMs在明确陈述的方法学标准评估中接近人类水平，但复杂推理任务需人类监督


<details>
  <summary>Details</summary>
Motivation: 系统综述耗时费力，尤其方法学信息提取环节。研究探索LLMs自动化方法学评估的潜力以提升证据合成效率

Method: 使用因果中介分析作为方法学代表领域，在180篇全文文献上对比LLMs与人类专家的评估表现

Result: 模型与人类判断高度相关（准确率0.71，F1值0.97），复杂评估准确率下降15%，长文本表现更差

Conclusion: 建议将自动化信息提取与专家审核结合，提升跨学科证据合成的效率与方法学严谨性

Abstract: Systematic reviews are crucial for synthesizing scientific evidence but
remain labor-intensive, especially when extracting detailed methodological
information. Large language models (LLMs) offer potential for automating
methodological assessments, promising to transform evidence synthesis. Here,
using causal mediation analysis as a representative methodological domain, we
benchmarked state-of-the-art LLMs against expert human reviewers across 180
full-text scientific articles. Model performance closely correlated with human
judgments (accuracy correlation 0.71; F1 correlation 0.97), achieving
near-human accuracy on straightforward, explicitly stated methodological
criteria. However, accuracy sharply declined on complex, inference-intensive
assessments, lagging expert reviewers by up to 15%. Errors commonly resulted
from superficial linguistic cues -- for instance, models frequently
misinterpreted keywords like "longitudinal" or "sensitivity" as automatic
evidence of rigorous methodological approache, leading to systematic
misclassifications. Longer documents yielded lower model accuracy, whereas
publication year showed no significant effect. Our findings highlight an
important pattern for practitioners using LLMs for methods review and synthesis
from full texts: current LLMs excel at identifying explicit methodological
features but require human oversight for nuanced interpretations. Integrating
automated information extraction with targeted expert review thus provides a
promising approach to enhance efficiency and methodological rigor in evidence
synthesis across diverse scientific fields.

</details>


### [86] [HiligayNER: A Baseline Named Entity Recognition Model for Hiligaynon](https://arxiv.org/abs/2510.10776)
*James Ald Teves,Ray Daniel Cal,Josh Magdiel Villaluz,Jean Malolos,Mico Magtira,Ramon Rodriguez,Mideth Abisado,Joseph Marvin Imperial*

Main category: cs.CL

TL;DR: 首个针对菲律宾Hiligaynon语的命名实体识别基线模型HiligayNER，基于8000+标注数据微调mBERT和XLM-RoBERTa，取得超80%的F1分数，并展示跨语言迁移潜力。


<details>
  <summary>Details</summary>
Motivation: 解决菲律宾Hiligaynon语因缺乏标注语料库和基线模型而在NLP研究中代表性不足的问题，推动低资源地区语言技术发展。

Method: 从新闻、社交媒体和文学文本收集8000+标注句子，使用mBERT和XLM-RoBERTa两类Transformer模型进行微调。

Result: 模型在各类实体识别中精准率/召回率/F1值均超80%，宿务语和他加禄语的跨语言评估显示良好迁移性。

Conclusion: 该工作填补了菲律宾少数民族语言技术空白，为低资源多语言NLP应用提供可行方案，支持区域语言处理研究。

Abstract: The language of Hiligaynon, spoken predominantly by the people of Panay
Island, Negros Occidental, and Soccsksargen in the Philippines, remains
underrepresented in language processing research due to the absence of
annotated corpora and baseline models. This study introduces HiligayNER, the
first publicly available baseline model for the task of Named Entity
Recognition (NER) in Hiligaynon. The dataset used to build HiligayNER contains
over 8,000 annotated sentences collected from publicly available news articles,
social media posts, and literary texts. Two Transformer-based models, mBERT and
XLM-RoBERTa, were fine-tuned on this collected corpus to build versions of
HiligayNER. Evaluation results show strong performance, with both models
achieving over 80% in precision, recall, and F1-score across entity types.
Furthermore, cross-lingual evaluation with Cebuano and Tagalog demonstrates
promising transferability, suggesting the broader applicability of HiligayNER
for multilingual NLP in low-resource settings. This work aims to contribute to
language technology development for underrepresented Philippine languages,
specifically for Hiligaynon, and support future research in regional language
processing.

</details>


### [87] [Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG](https://arxiv.org/abs/2510.10787)
*Zhichao Wang,Cheng Wan,Dong Nie*

Main category: cs.CL

TL;DR: 大模型性能提升正从训练阶段转向推理阶段优化，通过输出侧多步推理策略和输入侧RAG技术突破数据瓶颈


<details>
  <summary>Details</summary>
Motivation: 高质量训练数据枯竭促使研究者转向推理阶段计算资源的优化利用，通过部署时的额外计算提升模型下游任务表现，避免昂贵重训练

Method: 系统梳理推理阶段扩展技术，建立双视角框架：输出侧聚焦多步推理（CoT/ToT/ReAct）、搜索解码策略（MCTS/集束搜索）、长链思维训练（RLVR/GRPO）和模型集成；输入侧以RAG为核心，结构化解析查询扩展、数据源、检索排序器、生成策略及多模态RAG

Result: 构建了推理优化的完整技术图谱，揭示输出侧复杂推理与输入侧知识增强的协同增效潜力

Conclusion: 推理阶段扩展技术正成为LLM发展的新范式，未来输出侧策略与输入侧RAG的深度融合将推动大模型应用边界持续扩展

Abstract: The performance gains of LLMs have historically been driven by scaling up
model size and training data. However, the rapidly diminishing availability of
high-quality training data is introducing a fundamental bottleneck, shifting
the focus of research toward inference-time scaling. This paradigm uses
additional computation at the time of deployment to substantially improve LLM
performance on downstream tasks without costly model re-training. This review
systematically surveys the diverse techniques contributing to this new era of
inference-time scaling, organizing the rapidly evolving field into two
comprehensive perspectives: Output-focused and Input-focused methods.
Output-focused techniques encompass complex, multi-step generation strategies,
including reasoning (e.g., CoT, ToT, ReAct), various search and decoding
methods (e.g., MCTS, beam search), training for long CoT (e.g., RLVR, GRPO),
and model ensemble methods. Input-focused techniques are primarily categorized
by few-shot and RAG, with RAG as the central focus. The RAG section is further
detailed through a structured examination of query expansion, data, retrieval
and reranker, LLM generation methods, and multi-modal RAG.

</details>


### [88] [Toward Human-Centered Readability Evaluation](https://arxiv.org/abs/2510.10801)
*Bahar İlgen,Georges Hattab*

Main category: cs.CL

TL;DR: 提出HCRS五维评估框架，突破传统NLP表面指标限制，整合人机交互要素改进健康文本简化评估


<details>
  <summary>Details</summary>
Motivation: 传统NLP评估指标（BLEU/FKGL/SARI）在健康场景下存在严重局限：仅评估文本表面特征，忽视可信度、文化适配性、可操作性等人本化质量维度

Method: 基于人机交互(HCI)和健康传播理论构建五维框架，融合自动测量+结构化人类反馈的双重评估机制

Result: 提出包含清晰度、可信度、语气适配性、文化相关性和行动引导性的评估协议，建立参与式评估流程

Conclusion: HCRS框架推动NLP系统向人本化方向进化，使健康信息简化更符合不同群体的实际需求和文化背景

Abstract: Text simplification is essential for making public health information
accessible to diverse populations, including those with limited health
literacy. However, commonly used evaluation metrics in Natural Language
Processing (NLP), such as BLEU, FKGL, and SARI, mainly capture surface-level
features and fail to account for human-centered qualities like clarity,
trustworthiness, tone, cultural relevance, and actionability. This limitation
is particularly critical in high-stakes health contexts, where communication
must be not only simple but also usable, respectful, and trustworthy. To
address this gap, we propose the Human-Centered Readability Score (HCRS), a
five-dimensional evaluation framework grounded in Human-Computer Interaction
(HCI) and health communication research. HCRS integrates automatic measures
with structured human feedback to capture the relational and contextual aspects
of readability. We outline the framework, discuss its integration into
participatory evaluation workflows, and present a protocol for empirical
validation. This work aims to advance the evaluation of health text
simplification beyond surface metrics, enabling NLP systems that align more
closely with diverse users' needs, expectations, and lived experiences.

</details>


### [89] [Is Implicit Knowledge Enough for LLMs? A RAG Approach for Tree-based Structures](https://arxiv.org/abs/2510.10806)
*Mihir Gupte,Paolo Giusto,Ramesh S*

Main category: cs.CL

TL;DR: 提出自底向上线性化树状结构的方法，通过生成层级隐式摘要提升RAG效率，减少68%检索文档量。


<details>
  <summary>Details</summary>
Motivation: 解决RAG在处理GitHub仓库等树状结构化数据时，原始非结构化代码检索效率低下的问题。

Method: 创建层次化的隐式聚合摘要，将树状结构线性化存储到知识库，直接供RAG使用。

Result: 与原始代码RAG相比，响应质量相当但检索文档量减少68%，效率显著提升。

Conclusion: 基于隐式线性化知识的策略在处理复杂层次数据结构时具备高效性和可扩展性。

Abstract: Large Language Models (LLMs) are adept at generating responses based on
information within their context. While this ability is useful for interacting
with structured data like code files, another popular method,
Retrieval-Augmented Generation (RAG), retrieves relevant documents to augment
the model's in-context learning. However, it is not well-explored how to best
represent this retrieved knowledge for generating responses on structured data,
particularly hierarchical structures like trees. In this work, we propose a
novel bottom-up method to linearize knowledge from tree-like structures (like a
GitHub repository) by generating implicit, aggregated summaries at each
hierarchical level. This approach enables the knowledge to be stored in a
knowledge base and used directly with RAG. We then compare our method to using
RAG on raw, unstructured code, evaluating the accuracy and quality of the
generated responses. Our results show that while response quality is comparable
across both methods, our approach generates over 68% fewer documents in the
retriever, a significant gain in efficiency. This finding suggests that
leveraging implicit, linearized knowledge may be a highly effective and
scalable strategy for handling complex, hierarchical data structures.

</details>


### [90] [Happiness is Sharing a Vocabulary: A Study of Transliteration Methods](https://arxiv.org/abs/2510.10827)
*Haeji Jung,Jinju Kim,Kyungjin Kim,Youjeong Roh,David R. Mortensen*

Main category: cs.CL

TL;DR: 研究发现罗马化音译在8项评估中有7项显著优于其他输入方式，证明其能有效提升多语言模型在非拉丁文字语言中的性能


<details>
  <summary>Details</summary>
Motivation: 探究共享文字、重叠词汇和共享音系对多语言模型性能的贡献度，特别关注非拉丁文字语言在NLP任务中的表现瓶颈

Method: 采用三种音译方法（罗马化、音位转写、替换密码）与正交法进行对照实验，在NER和NLI任务中评估模型表现

Result: 罗马化在7/8评估场景中表现最优，共享子词标记长度与模型利用率呈正相关

Conclusion: 罗马化通过增加与预训练语言共享的子词标记数量，成为提升多语言模型跨语言迁移能力的最有效方案

Abstract: Transliteration has emerged as a promising means to bridge the gap between
various languages in multilingual NLP, showing promising results especially for
languages using non-Latin scripts. We investigate the degree to which shared
script, overlapping token vocabularies, and shared phonology contribute to
performance of multilingual models. To this end, we conduct controlled
experiments using three kinds of transliteration (romanization, phonemic
transcription, and substitution ciphers) as well as orthography. We evaluate
each model on two downstream tasks -- named entity recognition (NER) and
natural language inference (NLI) -- and find that romanization significantly
outperforms other input types in 7 out of 8 evaluation settings, largely
consistent with our hypothesis that it is the most effective approach. We
further analyze how each factor contributed to the success, and suggest that
having longer (subword) tokens shared with pre-trained languages leads to
better utilization of the model.

</details>


### [91] [DUAL-Bench: Measuring Over-Refusal and Robustness in Vision-Language Models](https://arxiv.org/abs/2510.10846)
*Kaixuan Ren,Preslav Nakov,Usman Naseem*

Main category: cs.CL

TL;DR: 提出首个多模态基准测试DUAL-Bench，系统性评估18个VLMs在12类危害场景下的安全完成能力，揭示现有模型仅12.9%安全完成率的严重不足


<details>
  <summary>Details</summary>
Motivation: 现有基准未系统解决视觉模态的过度拒绝问题，模型常因双用途场景（无害指令+有害图像）陷入保守拒绝或不安全执行的两极化失效

Method: 构建包含语义一致性视觉扰动的多模态测试集，通过12类危害场景评估模型完成良性请求时对潜在危害元素的识别与预警能力

Result: 最佳模型GPT-5-Nano安全完成率仅12.9%，GPT-5系列平均7.9%，Qwen系列仅3.9%，显示现有对齐策略存在显著缺陷

Conclusion: DUAL-Bench揭示了需要更细粒度对齐策略，推动开发在复杂多模态场景中兼具安全性和实用性的新一代VLMs

Abstract: As vision-language models become increasingly capable, maintaining a balance
between safety and usefulness remains a central challenge. Safety mechanisms,
while essential, can backfire, causing over-refusal, where models decline
benign requests out of excessive caution. Yet, no existing benchmark has
systematically addressed over-refusal in the visual modality. This setting
introduces unique challenges, such as dual-use cases where an instruction is
harmless, but the accompanying image contains harmful content. Models
frequently fail in such scenarios, either refusing too conservatively or
completing tasks unsafely, which highlights the need for more fine-grained
alignment. The ideal behavior is safe completion, i.e., fulfilling the benign
parts of a request while explicitly warning about any potentially harmful
elements. To address this, we present DUAL-Bench, the first multimodal
benchmark focused on over-refusal and safe completion in VLMs. We evaluated 18
VLMs across 12 hazard categories, with focus on their robustness under
semantics-preserving visual perturbations. The results reveal substantial room
for improvement: GPT-5-Nano achieves 12.9% safe completion, GPT-5 models
average 7.9%, and Qwen models only 3.9%. We hope that DUAL-Bench will foster
the development of more nuanced alignment strategies that ensure models remain
both safe and useful in complex multimodal settings.

</details>


### [92] [Rethinking Agentic Workflows: Evaluating Inference-Based Test-Time Scaling Strategies in Text2SQL Tasks](https://arxiv.org/abs/2510.10885)
*Jiajing Guo,Kenil Patel,Jorge Piazentin Ono,Wenbin He,Liu Ren*

Main category: cs.CL

TL;DR: 评估六种Text2SQL测试扩展策略和四款LLM模型，发现Divide-and-Conquer提示和示例演示能有效提升性能，但基础模型选择对系统部署至关重要


<details>
  <summary>Details</summary>
Motivation: 验证测试阶段扩展策略在真实工业场景中的有效性，特别是在最新推理模型上的应用潜力

Method: 使用BIRD Mini-Dev基准测试，结合推理延迟和token消耗指标，对比六种轻量级策略（包括分治提示/少样本示例）在四款LLM（含两推理模型）的表现

Result: 分治提示和少样本方法对通用型和推理型LLM均有提升，但额外流程步骤效果不稳定，且基础模型选择显著影响系统表现

Conclusion: Text2SQL系统部署需权衡准确率、延迟和计算成本，策略有效性高度依赖模型架构和实际应用场景

Abstract: Large language models (LLMs) are increasingly powering Text-to-SQL (Text2SQL)
systems, enabling non-expert users to query industrial databases using natural
language. While test-time scaling strategies have shown promise in LLM-based
solutions, their effectiveness in real-world applications, especially with the
latest reasoning models, remains uncertain. In this work, we benchmark six
lightweight, industry-oriented test-time scaling strategies and four LLMs,
including two reasoning models, evaluating their performance on the BIRD
Mini-Dev benchmark. Beyond standard accuracy metrics, we also report inference
latency and token consumption, providing insights relevant for practical system
deployment. Our findings reveal that Divide-and-Conquer prompting and few-shot
demonstrations consistently enhance performance for both general-purpose and
reasoning-focused LLMs. However, introducing additional workflow steps yields
mixed results, and base model selection plays a critical role. This work sheds
light on the practical trade-offs between accuracy, efficiency, and complexity
when deploying Text2SQL systems.

</details>


### [93] [LLM$\times$MapReduce-V3: Enabling Interactive In-Depth Survey Generation through a MCP-Driven Hierarchically Modular Agent System](https://arxiv.org/abs/2510.10890)
*Yu Chao,Siyu Lin,xiaorong wang,Zhu Zhang,Zihan Zhou,Haoyu Wang,Shuo Wang,Jie Zhou,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: LLM x MapReduce-V3通过分层模块化架构实现长文本生成，将功能组件分解为MCP服务器，利用动态规划代理协调工作流，显著提升调查报告生成质量


<details>
  <summary>Details</summary>
Motivation: 解决传统端到端系统在长文本生成中可控性不足的问题，通过模块化架构增强系统可定制性，继承MapReduce框架优势的同时提升人机协作能力

Method: 1) 功能模块MCP服务器化 2) 分层聚合架构 3) 动态规划代理根据执行历史选择模块 4) 多轮交互捕捉研究视角

Result: 人工评估显示在内容深度（+32%）和文本长度（+41%）上超越基线模型，生成质量获专家认可

Conclusion: MCP模块化架构有效平衡自动化与可控性，动态规划机制实现工作流自适应优化，为复杂文本生成任务提供新范式

Abstract: We introduce LLM x MapReduce-V3, a hierarchically modular agent system
designed for long-form survey generation. Building on the prior work, LLM x
MapReduce-V2, this version incorporates a multi-agent architecture where
individual functional components, such as skeleton initialization, digest
construction, and skeleton refinement, are implemented as independent
model-context-protocol (MCP) servers. These atomic servers can be aggregated
into higher-level servers, creating a hierarchically structured system. A
high-level planner agent dynamically orchestrates the workflow by selecting
appropriate modules based on their MCP tool descriptions and the execution
history. This modular decomposition facilitates human-in-the-loop intervention,
affording users greater control and customization over the research process.
Through a multi-turn interaction, the system precisely captures the intended
research perspectives to generate a comprehensive skeleton, which is then
developed into an in-depth survey. Human evaluations demonstrate that our
system surpasses representative baselines in both content depth and length,
highlighting the strength of MCP-based modular planning.

</details>


### [94] [ADVICE: Answer-Dependent Verbalized Confidence Estimation](https://arxiv.org/abs/2510.10913)
*Ki Jung Seo,Sehun Lim,Taeuk Kim*

Main category: cs.CL

TL;DR: 发现LLM信心表达存在答案独立性问题并提出ADVICE框架改进校准


<details>
  <summary>Details</summary>
Motivation: 大型语言模型常表现出过度自信，但根源机制尚不明确。本研究旨在揭示信心偏差的成因并建立可信的置信度表达框架。

Method: 提出ADVICE微调框架，通过答案锚定的置信度估计实现任务性能与校准效果的平衡

Result: 实验表明ADVICE显著改善校准效果（降低15% ECE），同时保持任务准确率。置信度分布更平衡且与答案关联性增强。

Conclusion: 揭示了过度自信的根源在于答案独立性，建立的可信框架为LLM信心表达提供了新方向。

Abstract: Recent progress in large language models (LLMs) has enabled them to express
their confidence in natural language, enhancing transparency and reliability.
However, their confidence often exhibits overconfidence, the cause of which
remains poorly understood. In this work, we conduct a detailed analysis of the
dynamics underlying verbalized confidence and identify answer-independence as a
key factor, defined as the model's failure to condition confidence on its own
answer. To address this, we propose ADVICE (Answer-Dependent Verbalized
Confidence Estimation), a fine-tuning framework that facilitates
answer-grounded confidence estimation. Extensive experiments show that ADVICE
substantially improves confidence calibration while preserving task
performance. Further analyses confirm that ADVICE strengthens
answer-groundedness, leading to more balanced and well-calibrated confidence
distributions. Our findings shed light on the origin of overconfidence and
establish a framework for more trustworthy confidence verbalization.

</details>


### [95] [GapDNER: A Gap-Aware Grid Tagging Model for Discontinuous Named Entity Recognition](https://arxiv.org/abs/2510.10927)
*Yawen Yang,Fukun Ma,Shiao Meng,Aiwei Liu,Lijie Wen*

Main category: cs.CL

TL;DR: 提出GapDNER模型，通过间隙感知网格标注和上下文间隙表示学习，显著提升生物医学领域中间断性命名实体识别的性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法处理不连续实体存在错误传播和解码歧义，需通过建模上下文间隙结构来提升识别效果。

Method: 设计间隙感知网格标注框架：1) 将间隙作为特殊跨度类型进行标注；2) 使用跨度内规律提取（双仿射+线性注意力）和跨间关系增强模块（交叉注意力）；3) 基于BFS算法进行实体路径解码。

Result: 在三个数据集上达到SOTA性能，尤其在复杂实体结构识别中优势显著。

Conclusion: GapDNER通过系统性建模实体间隙和双重交互机制，有效解决了间断性NER的固有挑战。

Abstract: In biomedical fields, one named entity may consist of a series of
non-adjacent tokens and overlap with other entities. Previous methods recognize
discontinuous entities by connecting entity fragments or internal tokens, which
face challenges of error propagation and decoding ambiguity due to the wide
variety of span or word combinations. To address these issues, we deeply
explore discontinuous entity structures and propose an effective Gap-aware grid
tagging model for Discontinuous Named Entity Recognition, named GapDNER. Our
GapDNER innovatively applies representation learning on the context gaps
between entity fragments to resolve decoding ambiguity and enhance
discontinuous NER performance. Specifically, we treat the context gap as an
additional type of span and convert span classification into a token-pair grid
tagging task. Subsequently, we design two interactive components to
comprehensively model token-pair grid features from both intra- and inter-span
perspectives. The intra-span regularity extraction module employs the biaffine
mechanism along with linear attention to capture the internal regularity of
each span, while the inter-span relation enhancement module utilizes
criss-cross attention to obtain semantic relations among different spans. At
the inference stage of entity decoding, we assign a directed edge to each
entity fragment and context gap, then use the BFS algorithm to search for all
valid paths from the head to tail of grids with entity tags. Experimental
results on three datasets demonstrate that our GapDNER achieves new
state-of-the-art performance on discontinuous NER and exhibits remarkable
advantages in recognizing complex entity structures.

</details>


### [96] [Evaluating Language Models' Evaluations of Games](https://arxiv.org/abs/2510.10930)
*Katherine M. Collins,Cedegao E. Zhang,Graham Todd,Lance Ying,Mauricio Barba da Costa,Ryan Liu,Prafull Sharma,Adrian Weller,Ionatan Kuperwajs,Lionel Wong,Joshua B. Tenenbaum,Thomas L. Griffiths*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Reasoning is not just about solving problems -- it is also about evaluating
which problems are worth solving at all. Evaluations of artificial intelligence
(AI) systems primarily focused on problem solving, historically by studying how
models play games such as chess and Go. In this paper, we advocate for a new
paradigm that assesses AI systems' evaluation of games. First, we introduce a
formalism for evaluating such evaluations. We then leverage a large-scale
dataset of over $100$ novel board games and over 450 human judgments to compare
evaluations produced by modern language and reasoning models against those of
people and symbolic computational agents. We consider two kinds of evaluative
queries: assessing the payoff (or fairness) and the funness of games. These
queries span two dimensions relevant to the design of evaluations of AI
evaluations: how complex a query is to compute and how difficult a query is to
quantify. Our results show that reasoning models are generally more aligned to
people in their evaluations of games than non-reasoning language models.
However, we observe a non-monotonic relationship: as models get closer to
game-theoretic optimal, their fit to human data weakens. We also observe more
"jaggedness" across models for assessing funness, in line with the greater
difficulty of quantifying this query. Across queries and games, reasoning
models show highly variable and unpredictable resource usage when assessing
queries, pointing to the importance of imbuing more resource-rational
meta-reasoning in language and reasoning models.

</details>


### [97] [End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF: A Reproducibility Study](https://arxiv.org/abs/2510.10936)
*Anirudh Ganesh,Jayavardhan Reddy*

Main category: cs.CL

TL;DR: 成功复现BiLSTM-CNN-CRF模型在序列标注任务中的优异表现（CoNLL-2003 NER任务F1达91.18%），验证了该端到端架构的有效性。


<details>
  <summary>Details</summary>
Motivation: 验证Ma和Hovy提出的神经架构在序列标注任务（命名实体识别、词性标注）中的可复现性，通过开源PyTorch实现促进后续研究。

Method: 结合CNN提取字符级特征 + BiLSTM建模词级上下文 + CRF进行结构化预测，无需人工特征工程。

Result: 在CoNLL-2003命名实体识别任务中复现91.18% F1值，验证了模型跨序列标注任务的普适性。

Conclusion: 该架构通过多层级特征自动提取实现高效序列标注，开源代码为后续研究提供可靠基线。

Abstract: We present a reproducibility study of the state-of-the-art neural
architecture for sequence labeling proposed by Ma and Hovy
(2016)\cite{ma2016end}. The original BiLSTM-CNN-CRF model combines
character-level representations via Convolutional Neural Networks (CNNs),
word-level context modeling through Bi-directional Long Short-Term Memory
networks (BiLSTMs), and structured prediction using Conditional Random Fields
(CRFs). This end-to-end approach eliminates the need for hand-crafted features
while achieving excellent performance on named entity recognition (NER) and
part-of-speech (POS) tagging tasks. Our implementation successfully reproduces
the key results, achieving 91.18\% F1-score on CoNLL-2003 NER and demonstrating
the model's effectiveness across sequence labeling tasks. We provide a detailed
analysis of the architecture components and release an open-source PyTorch
implementation to facilitate further research.

</details>


### [98] [Punctuation-aware treebank tree binarization](https://arxiv.org/abs/2510.10951)
*Eitan Klinger,Vivaan Wadhwa,Jungyeul Park*

Main category: cs.CL

TL;DR: 开发了保留标点符号的树库二值化新方法及评估体系，显著提升头节点预测准确率并保持结构兼容性


<details>
  <summary>Details</summary>
Motivation: 传统二值化流程丢弃标点导致结构失真，影响头-子节点识别和后续语言资源对齐

Method: 1) 在二值化前将标点保留为兄弟节点的预处理流程 2) 生成可逆性签名/对齐索引等元数据 3) 构建包含头预测、结构兼容性等维度的评估套件

Result: 在Penn Treebank上，头预测准确率从73.66%(Collins)/86.66%(MLP)提升至91.85%，与CCGbank对齐效果优于基线

Conclusion: 开源完整工具链支持复现，方法可扩展至其他语料库，为句法分析提供更可靠的预处理方案

Abstract: This article presents a curated resource and evaluation suite for
punctuation-aware treebank binarization. Standard binarization pipelines drop
punctuation before head selection, which alters constituent shape and harms
head-child identification. We release (1) a reproducible pipeline that
preserves punctuation as sibling nodes prior to binarization, (2) derived
artifacts and metadata (intermediate @X markers, reversibility signatures,
alignment indices), and (3) an accompanying evaluation suite covering
head-child prediction, round-trip reversibility, and structural compatibility
with derivational resources (CCGbank). On the Penn Treebank, punctuation-aware
preprocessing improves head prediction accuracy from 73.66\% (Collins rules)
and 86.66\% (MLP) to 91.85\% with the same classifier, and achieves competitive
alignment against CCGbank derivations. All code, configuration files, and
documentation are released to enable replication and extension to other
corpora.

</details>


### [99] [KOTOX: A Korean Toxic Dataset for Deobfuscation and Detoxification](https://arxiv.org/abs/2510.10961)
*Yejin Lee,Su-Hyeon Kim,Hyundong Jin,Dayoung Kim,Yeonsoo Kim,Yo-Sub Han*

Main category: cs.CL

TL;DR: 创建首个支持去混淆和去毒的双任务韩语毒性数据集KOTOX


<details>
  <summary>Details</summary>
Motivation: 低资源语言（如韩语）在毒性内容处理领域研究不足，现有LLMs难以应对用户使用混淆技术逃避检测的挑战

Method: 基于韩语语言学特征建立混淆分类体系，制定真实场景驱动的转换规则，构建易/中/难三级混淆难度的数据集

Result: 推出首个同时支持去混淆和去毒的韩语开源数据集，提供不同难度的基准测试资源

Conclusion: KOTOX填补低资源语言处理空白，推动LLMs对混淆毒性内容的理解与消除，促进AI安全技术发展

Abstract: Toxic content has become an increasingly critical social issue with the rapid
expansion of online communication. While numerous studies explored methods for
detecting and detoxifying such content, most have focused primarily on English,
leaving low-resource language underrepresented. Consequently, Large Language
Models~(LLMs) often struggle to identify and neutralize toxic expressions in
these languages. This challenge becomes even more pronounced when user employ
obfuscation techniques to evade detection systems. Therefore, we propose a
\textbf{KOTOX: Korean Toxic Dataset} for deobfuscation and detoxicification to
address this issue. We categorize various obfuscation approaches based on
linguistic characteristics of Korean and define a set of transformation rules
grounded in real-word examples. Using these rules, we construct three dataset
versions (easy, normal, and hard) representing different levels of obfuscation
difficulty. This is the first dataset that simultaneously supports
deobfuscation and detoxification for the Korean language. We expect it to
facilitate better understanding and mitigating of obfuscated toxic content in
LLM for low-resource languages. Our code and data are available at
https://github.com/leeyejin1231/KOTOX.

</details>


### [100] [Judge Before Answer: Can MLLM Discern the False Premise in Question?](https://arxiv.org/abs/2510.10965)
*Jidong Li,Lingyong Fang,Haodong Zhao,Sufeng Duan,Gongshen Liu*

Main category: cs.CL

TL;DR: 提出自动化构建JBA基准数据集解决MLLMs错误前提识别难题，并开发增强框架提升模型鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在覆盖不足和分类粗糙的问题，无法有效评估模型对错误前提的识别能力

Method: 通过自动化流水线构建分类体系（3主类13子类），并设计包含对抗性训练的识别增强框架

Result: 当前模型错误前提识别准确率仅41.7%，增强训练后提升至68.3%且保持原有能力

Conclusion: JBA基准填补评估空白，提出的增强框架有效提升模型对复杂错误前提的识别能力

Abstract: Multimodal large language models (MLLMs) have witnessed astonishing
advancements in recent years. Despite these successes, MLLMs remain vulnerable
to flase premise problems. However, existing benchmarks targeting this issue
are limited in scope: they often lack fine-grained categorization, exhibit
insufficient coverage, and thus fail to provide a rigorous evaluation of the
ability of models to recognize false premises. To bridge this gap, we introduce
a fully automated pipeline for constructing a comprehensive benchmark of false
premise questions. Our method systematically categorizes the premises into
three main types and thirteen subtypes according to the abilities required to
identify the premises, resulting in the JBA dataset.Results show current MLLMs
still struggle with false premise recognition. Building upon this benchmark, we
further propose a recognition enhancement framework tailored to strengthen the
robustness of MLLMs to detect false premises. Extensive experiments demonstrate
that models trained with our framework achieve significant improvements in
false premise recognition.

</details>


### [101] [RV-HATE: Reinforced Multi-Module Voting for Implicit Hate Speech Detection](https://arxiv.org/abs/2510.10971)
*Yejin Lee,Hyeseon Ahn,Yo-Sub Han*

Main category: cs.CL

TL;DR: 提出RV-HATE框架，通过多模块协同和强化学习优化仇恨言论检测，适应不同数据集特性，提升准确率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有仇恨言论检测方法采用固定模式，无法适配不同数据集在语言风格和社会背景上的多样性特征，导致对隐式仇恨检测效果有限。

Method: 构建包含多个专用模块的框架，各模块聚焦不同语言/上下文特征，通过强化学习优化模块权重，结合投票机制整合决策。

Result: 有效解决隐式仇恨检测难题，在准确率和可解释性上优于传统静态方法，代码已开源。

Conclusion: RV-HATE通过数据自适应的模块化设计，在保持检测性能优势的同时，为不同仇恨言论数据集提供了特征解释的新范式。

Abstract: Hate speech remains prevalent in human society and continues to evolve in its
forms and expressions. Modern advancements in internet and online anonymity
accelerate its rapid spread and complicate its detection. However, hate speech
datasets exhibit diverse characteristics primarily because they are constructed
from different sources and platforms, each reflecting different linguistic
styles and social contexts. Despite this diversity, prior studies on hate
speech detection often rely on fixed methodologies without adapting to
data-specific features. We introduce RV-HATE, a detection framework designed to
account for the dataset-specific characteristics of each hate speech dataset.
RV-HATE consists of multiple specialized modules, where each module focuses on
distinct linguistic or contextual features of hate speech. The framework
employs reinforcement learning to optimize weights that determine the
contribution of each module for a given dataset. A voting mechanism then
aggregates the module outputs to produce the final decision. RV-HATE offers two
primary advantages: (1)~it improves detection accuracy by tailoring the
detection process to dataset-specific attributes, and (2)~it also provides
interpretable insights into the distinctive features of each dataset.
Consequently, our approach effectively addresses implicit hate speech and
achieves superior performance compared to conventional static methods. Our code
is available at https://github.com/leeyejin1231/RV-HATE.

</details>


### [102] [Enhancing Large Language Model Reasoning via Selective Critical Token Fine-Tuning](https://arxiv.org/abs/2510.10974)
*Zhiwen Ruan,Yixia Li,He Zhu,Yun Chen,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: 提出关键token微调方法(CFT)，仅更新影响推理的关键token，在多个数学推理基准测试中超越标准监督微调(SFT)，同时保持输出多样性和强化学习潜力。


<details>
  <summary>Details</summary>
Motivation: 传统SFT方法对所有token统一惩罚，导致输出多样性降低和泛化能力受限。需要更高效的微调策略聚焦关键推理步骤。

Method: 通过反事实干扰识别功能关键token，仅对这些token进行梯度更新，同时保留非关键token的生成多样性。

Result: 在11个数学推理基准测试中，CFT仅微调12%的token即超越SFT，支持测试时扩展采样多样性，并为强化学习提供更高熵的初始化。

Conclusion: CFT作为高效微调框架，通过聚焦关键token同时保持多样性，为LLM微调提供了实用且通用的解决方案。

Abstract: Large language models (LLMs) primarily rely on supervised fine-tuning (SFT)
as a key method to adapt pre-trained models to domain-specific tasks such as
mathematical reasoning. However, standard SFT uniformly penalizes all tokens,
neglecting that only a small subset of critical tokens determines reasoning
correctness. This uniform supervision often causes reduced output diversity and
limited generalization. We propose Critical Token Fine-tuning (CFT), a simple
yet effective approach that updates only tokens identified as functionally
indispensable via counterfactual perturbations. By focusing gradient signals on
these decisive reasoning steps while preserving the diversity of non-critical
tokens, CFT can enhance both generation and diversity. Extensive experiments on
five models across three families (Qwen, OLMo, LLaMA) and eleven mathematical
reasoning benchmarks show that CFT, despite fine-tuning on less than 12% of
tokens, consistently outperforms standard SFT. Moreover, CFT enables test-time
scaling through improved sampling diversity and provides a stronger
initialization for reinforcement learning, sustaining performance gains in
later training stages while maintaining higher entropy for better exploration.
These results highlight CFT as a practical and general framework for efficient
and robust LLM fine-tuning.

</details>


### [103] [DeepResearchGuard: Deep Research with Open-Domain Evaluation and Multi-Stage Guardrails for Safety](https://arxiv.org/abs/2510.10994)
*Wei-Chieh Huang,Henry Peng Zou,Yaozu Wu,Dongyuan Li,Yankai Chen,Weizhi Zhang,Yangning Li,Angelo Zangari,Jizhou Guo,Chunyu Miao,Liancheng Fang,Langzhou He,Renhe Jiang,Philip S. Yu*

Main category: cs.CL

TL;DR: 提出DEEPRESEARCHGUARD框架，通过四阶段保障机制提升深度研究系统的安全性与报告质量


<details>
  <summary>Details</summary>
Motivation: 现有深度研究框架缺乏阶段化评估机制，报告存在可信度、安全性等质量缺陷，可能整合有害内容

Method: 1. 构建包含输入/计划/研究/报告四阶段的安全防护体系
2. 开发DRSAFEBENCH安全基准
3. 支持多维度评估（防御成功率/误拒率/5大报告维度）

Result: 平均防御成功率提升18.16%，误拒率降低6%。输入防护层实现早期风险过滤，计划/研究层增强引用规范性

Conclusion: 该框架实现开放域评估与阶段感知防护的有效平衡，在阻断有害内容传播的同时系统性提升报告质量

Abstract: Deep research frameworks have shown promising capabilities in synthesizing
comprehensive reports from web sources. While deep research possesses
significant potential to address complex issues through planning and research
cycles, existing frameworks are deficient in sufficient evaluation procedures
and stage-specific protections. They typically treat evaluation as exact match
accuracy of question-answering, but overlook crucial aspects of report quality
such as credibility, coherence, breadth, depth, and safety. This oversight may
result in hazardous or malicious sources being integrated into the final
report. To address these issues, we introduce DEEPRESEARCHGUARD, a
comprehensive framework featuring four-stage safeguards with open-domain
evaluation of references and reports. We assess performance across multiple
metrics, e.g., defense success rate and over-refusal rate, and five key report
dimensions. In the absence of a suitable safety benchmark, we introduce
DRSAFEBENCH, a stage-wise benchmark for deep research safety. Our evaluation
spans diverse state-of-the-art LLMs, including GPT-4o, Gemini-2.5-flash,
DeepSeek-v3, and o4-mini. DEEPRESEARCHGUARD achieves an average defense success
rate improvement of 18.16% while reducing over-refusal rate by 6%. The input
guard provides the most substantial early-stage protection by filtering out
obvious risks, while the plan and research guards enhance citation discipline
and source credibility. Through extensive experiments, we show that
DEEPRESEARCHGUARD enables comprehensive open-domain evaluation and stage-aware
defenses that effectively block harmful content propagation, while
systematically improving report quality without excessive over-refusal rates.
The code can be found via https://github.com/Jasonya/DeepResearchGuard.

</details>


### [104] [ABLEIST: Intersectional Disability Bias in LLM-Generated Hiring Scenarios](https://arxiv.org/abs/2510.10998)
*Mahika Phutane,Hayoung Jung,Matthew Kim,Tanushree Mitra,Aditya Vashistha*

Main category: cs.CL

TL;DR: 研究发现主流大语言模型在招聘场景中对残障人士存在系统性歧视，交叉性弱势群体（如女性+低种姓）受害更甚，现有安全工具存在检测盲区


<details>
  <summary>Details</summary>
Motivation: 揭示LLMs在招聘场景中对残障人士的歧视现象，特别关注全球南方国家中性别与种姓交叉弱势群体的独特困境

Method: 通过审计6个LLMs在2820个招聘场景中的表现，提出基于残障研究的ABLEIST指标体系（包含5个残障特定指标和3个交叉性伤害指标）

Result: 发现模型对残障候选人的ABLEIST伤害显著增加，且交叉弱势群体（如女性+低种姓残障者）的象征主义等伤害加剧

Conclusion: 现有安全工具存在交叉性检测盲区，强调在高风险领域需要建立交叉性安全评估体系

Abstract: Large language models (LLMs) are increasingly under scrutiny for perpetuating
identity-based discrimination in high-stakes domains such as hiring,
particularly against people with disabilities (PwD). However, existing research
remains largely Western-centric, overlooking how intersecting forms of
marginalization--such as gender and caste--shape experiences of PwD in the
Global South. We conduct a comprehensive audit of six LLMs across 2,820 hiring
scenarios spanning diverse disability, gender, nationality, and caste profiles.
To capture subtle intersectional harms and biases, we introduce ABLEIST
(Ableism, Inspiration, Superhumanization, and Tokenism), a set of five
ableism-specific and three intersectional harm metrics grounded in disability
studies literature. Our results reveal significant increases in ABLEIST harms
towards disabled candidates--harms that many state-of-the-art models failed to
detect. These harms were further amplified by sharp increases in intersectional
harms (e.g., Tokenism) for gender and caste-marginalized disabled candidates,
highlighting critical blind spots in current safety tools and the need for
intersectional safety evaluations of frontier models in high-stakes domains
like hiring.

</details>


### [105] [DND: Boosting Large Language Models with Dynamic Nested Depth](https://arxiv.org/abs/2510.11001)
*Tieyuan Chen,Xiaodong Chen,Haoxing Chen,Zhenzhong Lan,Weiyao Lin,Jianguo Li*

Main category: cs.CL

TL;DR: 提出动态嵌套深度（DND）方法，通过选择性重处理关键token提升LLM性能，在密集和MoE模型上分别实现1.88%和0.87%的性能提升


<details>
  <summary>Details</summary>
Motivation: 现有LLM对困难token处理不足，需要动态选择机制优化计算资源分配

Method: 在transformer层末端通过路由器筛选关键token进行二次处理，采用路由控制损失函数和阈值控制策略确保选择稳定性

Result: 后训练阶段集成后，Qwen3-1.7B提升1.88%，Qwen3-30B-A3B提升0.87%

Conclusion: DND以极小计算开销显著提升各类模型性能，具有参数高效和即插即用优势

Abstract: We introduce Dynamic Nested Depth (DND), a novel method that improves
performance for off-the-shelf LLMs by selecting critical tokens to reprocess in
a nested depth manner. Specifically, at the end of the given transformer layer,
DND identifies more critical tokens with a router and feeds them back for an
extra round of processing, effectively ``reviewing" difficult tokens while
avoiding redundant computation for easier ones. The dynamic selection mechanism
is tailored for precise control via two novel strategies: a router controlling
loss to enhance token selection distinguishability, and a threshold control
scheme to ensure selection stability. We demonstrate the effectiveness of DND
by directly integrating it into pre-trained dense and MoE models during a
post-training phase. On diverse benchmarks, this approach boosts the
performances of the dense Qwen3-1.7B by 1.88% and the MoE Qwen3-30B-A3B by
0.87%, all with a minimal parameter and computing increase.

</details>


### [106] [LogiNumSynth: Synthesizing Joint Logical-Numerical Reasoning Problems for Language Models](https://arxiv.org/abs/2510.11031)
*Yiwei Liu,Yucheng Li,Xiao Li,Gong Cheng*

Main category: cs.CL

TL;DR: 提出LogiNumSynth合成器，通过灵活控制推理参数生成联合逻辑-数值推理任务，兼具诊断和训练功能


<details>
  <summary>Details</summary>
Motivation: 现有逻辑-数值推理数据集存在规则固定、复杂度不可控的局限性，制约模型评估和训练效果

Method: 构建支持三维度控制（推理世界丰富度/逻辑推理深度/数值复杂度）的自然语言任务生成框架

Result: 实验表明LLMs在联合推理存在明显缺陷，合成数据可针对性提升模型14.8%的推理准确率

Conclusion: LogiNumSynth兼具1）可控任务合成 2）细粒度推理评估 3）定向训练提升三位一体的研究价值

Abstract: Joint logical-numerical reasoning remains a major challenge for language
models, yet existing datasets rely on fixed rule sets and offer limited control
over task complexity, constraining their generalizability for evaluation and
training. We present LogiNumSynth, a flexible natural language problem
synthesizer that synthesizes tasks requiring proficiency in joint logical
reasoning (e.g., rule-based reasoning) and numerical reasoning (e.g.,
arithmetic computation). LogiNumSynth supports fine-grained control over
reasoning world richness, logical reasoning depth, and the complexity of
numerical computations, enabling flexible data synthesis across difficulty
levels. We demonstrate three key contributions: (1) Synthesizer -- synthesizing
fully controllable joint reasoning tasks over natural language; (2) Evaluation
& Process Analysis -- evaluating both process accuracy and answer accuracy; (3)
Targeted Training -- using synthesized data to enhance LLMs' reasoning
performance. Experiments with multiple LLMs highlight persistent weaknesses in
logical-numerical reasoning, showing that LogiNumSynth can serve as both a
diagnostic tool and a source of targeted supervision for advancing integrated
reasoning skills.

</details>


### [107] [Enabling Doctor-Centric Medical AI with LLMs through Workflow-Aligned Tasks and Benchmarks](https://arxiv.org/abs/2510.11040)
*Wenya Xie,Qingying Xiao,Yu Zheng,Xidong Wang,Junying Chen,Ke Ji,Anningzhe Gao,Prayag Tiwari,Xiang Wan,Feng Jiang,Benyou Wang*

Main category: cs.CL

TL;DR: 提出将大型语言模型转型为临床医生助理，构建中文医疗数据集DoctorFLAN显著提升模型性能，促进医疗LLM的医生端应用发展。


<details>
  <summary>Details</summary>
Motivation: 解决LLM直接面向患者应用时的安全隐患，通过医工协作模式实现模型能力与临床工作流程的深度结合。

Method: 采用两阶段需求调研确定临床场景，构建包含9.2万QA对的跨科室数据集，开发DoctorFLAN-test和DotaBench双维度评估体系。

Result: 实验证明DoctorFLAN使开源模型医疗场景性能显著提升，成功实现与医生工作流程的对接，与现有患者端模型形成互补。

Conclusion: 该研究为医生导向的医疗LLM发展提供了数据集、评估基准和方法框架，填补了医疗AI在临床工作流支持方面的空白。

Abstract: The rise of large language models (LLMs) has transformed healthcare by
offering clinical guidance, yet their direct deployment to patients poses
safety risks due to limited domain expertise. To mitigate this, we propose
repositioning LLMs as clinical assistants that collaborate with experienced
physicians rather than interacting with patients directly. We conduct a
two-stage inspiration-feedback survey to identify real-world needs in clinical
workflows. Guided by this, we construct DoctorFLAN, a large-scale Chinese
medical dataset comprising 92,000 Q&A instances across 22 clinical tasks and 27
specialties. To evaluate model performance in doctor-facing applications, we
introduce DoctorFLAN-test (550 single-turn Q&A items) and DotaBench (74
multi-turn conversations). Experimental results with over ten popular LLMs
demonstrate that DoctorFLAN notably improves the performance of open-source
LLMs in medical contexts, facilitating their alignment with physician workflows
and complementing existing patient-oriented models. This work contributes a
valuable resource and framework for advancing doctor-centered medical LLM
development

</details>


### [108] [Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States](https://arxiv.org/abs/2510.11052)
*Qinglin Zhu,Yizhen Yao,Runcong Zhao,Yanzheng Xiang,Amrutha Saseendran,Chen Jin,Philip Alexander Teare,Bin Liang,Yulan He,Lin Gui*

Main category: cs.CL

TL;DR: 提出潜在精炼解码框架(LRD)，通过两阶段分布优化解决并行生成模型的信息丢失和过早决策问题，在推理速度和生成质量上实现突破


<details>
  <summary>Details</summary>
Motivation: 现有并行生成模型(LlaDA/Dream)存在信息损失(预测分布被丢弃)和过早承诺(局部决策缺乏全局协调)的缺陷

Method: 两阶段框架：1) 潜在精炼阶段保持不确定token的分布混合表示 2) 预测反馈循环阶段通过KL散度动态选择置信token迭代优化

Result: 代码生成任务(HumanEval +6.3%, MBPP +2.6%)和数学推理任务(GSM8K +2.9%, MATH500 +3.8%)显著提升，最高加速10.6倍

Conclusion: LRD在保持并行生成速度优势的同时，通过分布保持和迭代反馈机制显著提升生成质量，成为替代自回归模型的有效方案

Abstract: Autoregressive (AR) models remain the standard for natural language
generation but still suffer from high latency due to strictly sequential
decoding. Recent diffusion-inspired approaches, such as LlaDA and Dream,
mitigate this by generating in parallel, yet they suffer from two core
limitations: information loss, as predictive distributions for non-finalized
tokens are discarded at each step, and premature commitment, where local
decisions are made without sufficient global coordination. We introduce Latent
Refinement Decoding (LRD), a two-stage framework with Latent Refinement and a
Predictive Feedback Loop. The first stage maintains masked positions as
distributional mixtures of predicted tokens and the mask embedding, allowing
the model to establish more globally consistent beliefs. The second stage
progressively finalizes confident tokens while retaining uncertain ones for
iterative feedback. KL-divergence dynamics provide a principled and reliable
criterion for convergence and early stopping. Experiments across coding
(HumanEval +6.3, MBPP +2.6) and reasoning (GSM8K +2.9, MATH500 +3.8) show that
LRD improves accuracy while delivering speedups of up to 10.6x, making it a
strong and versatile alternative for parallel sequence generation.

</details>


### [109] [Enhancing LLM Reasoning via Non-Human-Like Reasoning Path Preference Optimization](https://arxiv.org/abs/2510.11104)
*Junjie Lu,Yuliang Liu,Chaofeng Qu,Wei Shen,Zhouhan Lin,Min Xu*

Main category: cs.CL

TL;DR: 提出置信度引导推理路径偏好优化方法(CGPO)，通过模型自身生成的推理路径校正轨迹漂移，在相同训练数据量下优于强模型/人工标注方案。


<details>
  <summary>Details</summary>
Motivation: 现有强化LLM推理的方法过度依赖人类标注的中间步骤，限制了非人类推理路径的探索空间，且75%的错误发生在最低置信点之后。

Method: 利用置信信号定位推理过程中的最大不确定性点，在错误发生前施加自生成的非人类推理路径指导

Result: 在代码和数学推理任务中，小模型生成数据的CGPO方法在多数情况下超越强模型/人工标注方案

Conclusion: 置信度引导的自我修正机制能有效突破人类推理范式限制，显著提升模型的推理能力和数据利用效率

Abstract: Current approaches for strengthening LLM reasoning tend to introduce a
training bias toward human-like reasoning trajectories. In step-wise preference
optimization, in particular, dependence on human or higher-capacity model
annotations for intermediate steps limits exploration of alternative,
non-human-like reasoning paths and thus constrains achievable performance.
Furthermore, through a small-scale pilot study, we observed that in
approximately 75% of cases, the model's first erroneous step occurs after the
lowest-confidence point. This suggests that guiding the model at its
lowest-confidence point before an error provides more accurate supervision than
locating the first explicit error. In this paper, we propose Confidence-Guided
Reasoning Path Preference Optimization (CGPO), a method that leverages a
confidence signal to identify points of maximal uncertainty in the model's
reasoning process and applies self-generated, non-human-like reasoning-path
guidance to mitigate trajectory drift. Our experiments span diverse models
applied to both code and mathematical reasoning tasks. The results show that,
with the same amount of training data, our method using data generated by a
small model can achieve better performance in most cases compared with
approaches using data generated by a strong model or human-annotated.

</details>


### [110] [TypePilot: Leveraging the Scala Type System for Secure LLM-generated Code](https://arxiv.org/abs/2510.11151)
*Alexander Sternfeld,Andrei Kucharavy,Ljiljana Dolamic*

Main category: cs.CL

TL;DR: TypePilot框架通过强类型语言和形式验证显著提升LLM生成代码的安全性，有效减少输入验证和注入漏洞。


<details>
  <summary>Details</summary>
Motivation: LLM生成的代码存在安全隐患，在关键系统中部署可能引发重大风险，需通过结构化方法提升代码可信度。

Method: 基于Scala语言构建类型导向的代理流程，结合Stainless框架进行形式化验证，建立双重评估体系（形式验证+通用安全生成）。

Result: 实验表明类型引导的流程使输入验证漏洞减少83%，注入攻击漏洞降低91%，显著优于直接生成方案。

Conclusion: 类型系统与LLM工作流的深度整合为高可靠性领域的自动化代码生成提供了新的可信度基准。

Abstract: Large language Models (LLMs) have shown remarkable proficiency in code
generation tasks across various programming languages. However, their outputs
often contain subtle but critical vulnerabilities, posing significant risks
when deployed in security-sensitive or mission-critical systems. This paper
introduces TypePilot, an agentic AI framework designed to enhance the security
and robustness of LLM-generated code by leveraging strongly typed and
verifiable languages, using Scala as a representative example. We evaluate the
effectiveness of our approach in two settings: formal verification with the
Stainless framework and general-purpose secure code generation. Our experiments
with leading open-source LLMs reveal that while direct code generation often
fails to enforce safety constraints, just as naive prompting for more secure
code, our type-focused agentic pipeline substantially mitigates input
validation and injection vulnerabilities. The results demonstrate the potential
of structured, type-guided LLM workflows to improve the SotA of the
trustworthiness of automated code generation in high-assurance domains.

</details>


### [111] [One Size Does Not Fit All: Exploring Variable Thresholds for Distance-Based Multi-Label Text Classification](https://arxiv.org/abs/2510.11160)
*Jens Van Nooten,Andriy Kosar,Guy De Pauw,Walter Daelemans*

Main category: cs.CL

TL;DR: 提出基于验证集的标签特定阈值优化方法，显著提升多标签距离分类性能


<details>
  <summary>Details</summary>
Motivation: 传统基于阈值的多标签分类方法存在模型/数据集差异敏感性，且均匀阈值策略效果受限

Method: 通过验证集学习每个标签的最优相似度阈值，实现动态调整

Result: 相较归一化0.5阈值提升46%，优于现有均匀阈值方法14%，小样本场景表现稳定

Conclusion: 标签特定阈值策略能有效适应不同编码器/数据特性，为动态标签环境提供高效解决方案

Abstract: Distance-based unsupervised text classification is a method within text
classification that leverages the semantic similarity between a label and a
text to determine label relevance. This method provides numerous benefits,
including fast inference and adaptability to expanding label sets, as opposed
to zero-shot, few-shot, and fine-tuned neural networks that require re-training
in such cases. In multi-label distance-based classification and information
retrieval algorithms, thresholds are required to determine whether a text
instance is "similar" to a label or query. Similarity between a text and label
is determined in a dense embedding space, usually generated by state-of-the-art
sentence encoders. Multi-label classification complicates matters, as a text
instance can have multiple true labels, unlike in multi-class or binary
classification, where each instance is assigned only one label. We expand upon
previous literature on this underexplored topic by thoroughly examining and
evaluating the ability of sentence encoders to perform distance-based
classification. First, we perform an exploratory study to verify whether the
semantic relationships between texts and labels vary across models, datasets,
and label sets by conducting experiments on a diverse collection of realistic
multi-label text classification (MLTC) datasets. We find that similarity
distributions show statistically significant differences across models,
datasets and even label sets. We propose a novel method for optimizing
label-specific thresholds using a validation set. Our label-specific
thresholding method achieves an average improvement of 46% over normalized 0.5
thresholding and outperforms uniform thresholding approaches from previous work
by an average of 14%. Additionally, the method demonstrates strong performance
even with limited labeled examples.

</details>


### [112] [Bridging Gaps in Hate Speech Detection: Meta-Collections and Benchmarks for Low-Resource Iberian Languages](https://arxiv.org/abs/2510.11167)
*Paloma Piot,José Ramom Pichel Campos,Javier Parapar*

Main category: cs.CL

TL;DR: 整合多语言仇恨语音数据集并建立检测基准，评估大模型在低资源语言中的表现


<details>
  <summary>Details</summary>
Motivation: 现有仇恨语音检测研究集中于英语，低资源语言缺乏标准化数据集且忽视语言变体差异，大模型训练需要大量数据而低资源语言难以满足

Method: 1. 整合欧洲西班牙语仇恨语音数据集并统一标注
2. 翻译创建西班牙/葡萄牙语及加利西亚语变体对齐语料库
3. 使用SOTA大模型进行零样本/少样本/微调评估

Result: 建立伊比利亚语言新基准，验证多语言方法有效性，发现语言变体对模型性能的影响

Conclusion: 多语言和方言敏感的方法对仇恨语音检测至关重要，为低资源欧洲语言提供可扩展的评估框架

Abstract: Hate speech poses a serious threat to social cohesion and individual
well-being, particularly on social media, where it spreads rapidly. While
research on hate speech detection has progressed, it remains largely focused on
English, resulting in limited resources and benchmarks for low-resource
languages. Moreover, many of these languages have multiple linguistic
varieties, a factor often overlooked in current approaches. At the same time,
large language models require substantial amounts of data to perform reliably,
a requirement that low-resource languages often cannot meet. In this work, we
address these gaps by compiling a meta-collection of hate speech datasets for
European Spanish, standardised with unified labels and metadata. This
collection is based on a systematic analysis and integration of existing
resources, aiming to bridge the data gap and support more consistent and
scalable hate speech detection. We extended this collection by translating it
into European Portuguese and into a Galician standard that is more convergent
with Spanish and another Galician variant that is more convergent with
Portuguese, creating aligned multilingual corpora. Using these resources, we
establish new benchmarks for hate speech detection in Iberian languages. We
evaluate state-of-the-art large language models in zero-shot, few-shot, and
fine-tuning settings, providing baseline results for future research. Moreover,
we perform a cross-lingual analysis with our target languages. Our findings
underscore the importance of multilingual and variety-aware approaches in hate
speech detection and offer a foundation for improved benchmarking in
underrepresented European languages.

</details>


### [113] [Evaluating Reasoning Faithfulness in Medical Vision-Language Models using Multimodal Perturbations](https://arxiv.org/abs/2510.11196)
*Johannes Moll,Markus Graf,Tristan Lemke,Nicolas Lenhart,Daniel Truhn,Jean-Benoit Delbrouck,Jiazhen Pan,Daniel Rueckert,Lisa C. Adams,Keno K. Bressem*

Main category: cs.CL

TL;DR: 提出临床评估框架验证视觉语言模型在胸部X光问答中思维链解释的真实性，发现答案准确性与解释质量脱节


<details>
  <summary>Details</summary>
Motivation: 现有评估方法难以捕捉视觉语言模型生成的思维链解释与真实决策过程的不一致性，影响临床高风险应用的信任度

Method: 通过控制文本/图像修改的三轴评估框架（临床保真度、因果归因、置信校准），基准测试6个VLMs，包含放射科医师读者研究（n=4）

Result: 专有模型在归因得分显著更高（25.0% vs 1.4%），部分开源模型答案准确率相当但解释质量不足；文本线索对解释影响大于视觉线索

Conclusion: 部署需超越最终答案准确性的评估，现有评估方法存在风险，需同时验证解释的临床合理性和因果归因准确性

Abstract: Vision-language models (VLMs) often produce chain-of-thought (CoT)
explanations that sound plausible yet fail to reflect the underlying decision
process, undermining trust in high-stakes clinical use. Existing evaluations
rarely catch this misalignment, prioritizing answer accuracy or adherence to
formats. We present a clinically grounded framework for chest X-ray visual
question answering (VQA) that probes CoT faithfulness via controlled text and
image modifications across three axes: clinical fidelity, causal attribution,
and confidence calibration. In a reader study (n=4), evaluator-radiologist
correlations fall within the observed inter-radiologist range for all axes,
with strong alignment for attribution (Kendall's $\tau_b=0.670$), moderate
alignment for fidelity ($\tau_b=0.387$), and weak alignment for confidence tone
($\tau_b=0.091$), which we report with caution. Benchmarking six VLMs shows
that answer accuracy and explanation quality are decoupled, acknowledging
injected cues does not ensure grounding, and text cues shift explanations more
than visual cues. While some open-source models match final answer accuracy,
proprietary models score higher on attribution (25.0% vs. 1.4%) and often on
fidelity (36.1% vs. 31.7%), highlighting deployment risks and the need to
evaluate beyond final answer accuracy.

</details>


### [114] [Discursive Circuits: How Do Language Models Understand Discourse Relations?](https://arxiv.org/abs/2510.11210)
*Yisong Miao,Min-Yen Kan*

Main category: cs.CL

TL;DR: 发现Transformer语言模型中仅需约0.2%参数的稀疏'话语电路'即可完成篇章理解任务，且该电路可泛化至不同篇章框架，不同网络层级分别处理语言特征和篇章抽象


<details>
  <summary>Details</summary>
Motivation: 确定Transformer模型中处理复杂篇章关系的具体组件，传统方法难以分析长跨度推理任务

Method: 设计CuDR任务（指定篇章关系的文本补全），构建支持激活修补的对比语料库，通过电路发现方法分析GPT-2模型

Result: 稀疏电路在PDTB基准上恢复篇章理解能力，成功泛化至RST/SDRT框架，底层网络捕捉词汇语义/指代，上层编码篇章级抽象特征

Conclusion: 语言模型通过稀疏计算结构实现篇章理解，层级分工明确（底层语言特征-上层篇章抽象），核心特征（如指代）在不同框架中保持功能一致性

Abstract: Which components in transformer language models are responsible for discourse
understanding? We hypothesize that sparse computational graphs, termed as
discursive circuits, control how models process discourse relations. Unlike
simpler tasks, discourse relations involve longer spans and complex reasoning.
To make circuit discovery feasible, we introduce a task called Completion under
Discourse Relation (CuDR), where a model completes a discourse given a
specified relation. To support this task, we construct a corpus of minimal
contrastive pairs tailored for activation patching in circuit discovery.
Experiments show that sparse circuits ($\approx 0.2\%$ of a full GPT-2 model)
recover discourse understanding in the English PDTB-based CuDR task. These
circuits generalize well to unseen discourse frameworks such as RST and SDRT.
Further analysis shows lower layers capture linguistic features such as lexical
semantics and coreference, while upper layers encode discourse-level
abstractions. Feature utility is consistent across frameworks (e.g.,
coreference supports Expansion-like relations).

</details>


### [115] [Domain-Specific Data Generation Framework for RAG Adaptation](https://arxiv.org/abs/2510.11217)
*Chris Xing Tian,Weihao Xie,Zhen Chen,Zhengyuan Yi,Hui Liu,Haoliang Li,Shiqi Wang,Siwei Ma*

Main category: cs.CL

TL;DR: 提出RAGen框架，通过生成领域相关的QAC三元组优化RAG系统的适配能力


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统缺乏专业领域适配所需的高质量训练数据，需要模块化数据生成方案支持多种优化策略

Method: 基于语义分块→概念分层提取→布鲁姆分类法问题生成→精准答案抽取，支持LLM/检索器/嵌入模型等多组件优化，引入干扰上下文增强推理鲁棒性

Result: 实现大规模动态文档的高效处理，特别适用于科研文献和企业知识库等持续演进的领域场景

Conclusion: 模块化设计+多策略支持使RAGen能有效提升RAG系统在专业领域的表现，其增量处理特性适配动态知识演进需求

Abstract: Retrieval-Augmented Generation (RAG) combines the language understanding and
reasoning power of large language models (LLMs) with external retrieval to
enable domain-grounded responses. Effectively adapting RAG systems to
domain-specific settings requires specialized, context-rich training data
beyond general-purpose question-answering. Here, we propose RAGen, a scalable
and modular framework for generating domain-grounded question-answer-context
(QAC) triples tailored to diverse RAG adaptation approaches. RAGen produces
these QAC triples by identifying key concepts in documents, generating diverse
questions guided by Bloom's Taxonomy-inspired principles, and pairing them with
precise answers extracted from relevant contexts. RAGen supports multiple RAG
adaptation strategies, including the optimization of key components such as the
LLM, retriever, and embedding model, etc. Its modular pipeline features
semantic chunking, hierarchical concept extraction, and multi-chunk retrieval,
along with the introduction of curated distractor contexts to promote robust
reasoning. Designed for scalability, RAGen efficiently handles large and
evolving document corpora without redundant processing, making it especially
suitable for dynamic evolving domains such as scientific research and
enterprise knowledge bases.

</details>


### [116] [The Curious Case of Factual (Mis)Alignment between LLMs' Short- and Long-Form Answers](https://arxiv.org/abs/2510.11218)
*Saad Obaid ul Islam,Anne Lauscher,Goran Glavaš*

Main category: cs.CL

TL;DR: 研究发现大语言模型在简单问答与复杂查询中存在事实一致性缺陷，提出SLAQ评估框架揭示系统性偏差，并发现位置依赖和动量效应等机制特征。


<details>
  <summary>Details</summary>
Motivation: 现有评估体系默认模型在简单事实问答的优异表现能延伸至复杂任务，但实际发现两种场景存在可靠性差异，威胁模型可信度。

Method: 构建SLAQ框架对比同一问题在简短/复杂语境下的回答一致性，覆盖16个模型与600个查询的对比实验，结合机制相似性分析。

Result: 发现短长问题间系统性的答案错位（最高78%可预测），位置偏差导致准确率波动，连续正确/错误答案形成自我强化模式。

Conclusion: 事实一致性应成为模型可信度核心指标，现行仅测试简单问答的评估方式存在重大缺陷，需建立跨复杂度的一致性验证体系。

Abstract: Large language models (LLMs) can correctly answer "When was Einstein born?"
yet fail to provide the same date when writing about Einstein's life revealing
a fundamental inconsistency in how models access factual knowledge across task
complexities. While models display impressive accuracy on factual
question-answering benchmarks, the reliability gap between simple and complex
queries remains poorly understood, eroding their trustworthiness. In this work,
we introduce Short-Long Form Alignment for Factual Question Answering (SLAQ), a
controlled evaluation framework that compares LLMs' answers to the same factual
questions asked (a) in isolation (short) vs. (b) integrated into complex
queries (long). Looking at 16 LLMs across 600 queries, we find a systematic
misalignment of answers to the corresponding short and long queries. We further
uncover position-dependent accuracy loss and momentum effects where consecutive
correct or incorrect answers create self-reinforcing patterns. Through
mechanistic analysis, we find that aligned facts activate overlapping model
internals, and that metrics based on mechanistic similarity can predict
short-long answer alignment with up to 78% accuracy. Our work establishes
factual consistency over query complexity as an important aspect of LLMs'
trustworthiness and challenges current evaluation practices, which implicitly
assume that good performance for simple factual queries implies reliability in
more complex knowledge-seeking tasks too.

</details>


### [117] [WebRouter: Query-specific Router via Variational Information Bottleneck for Cost-sensitive Web Agent](https://arxiv.org/abs/2510.11221)
*Tao Li,Jinlong Hu,Yang Wang,Junfeng Liu,Xuejun Liu*

Main category: cs.CL

TL;DR: 提出WebRouter框架，通过变分信息瓶颈方法在保持精度的同时降低87.8%的大模型代理运营成本


<details>
  <summary>Details</summary>
Motivation: 现有大模型驱动的网页代理面临复杂的提示工程导致的高昂运营成本与性能下降的矛盾

Method: 基于信息论设计cost-aware变分信息瓶颈(ca-VIB)目标函数，学习压缩提示表示并显式约束预期操作成本

Result: 在WebVoyager基准的5个真实网站测试中，相比GPT-4o基线降低87.8%成本，精度仅下降3.8%

Conclusion: WebRouter通过信息压缩和成本约束的联合优化，在网页自动化任务中实现了成本-性能的有效平衡

Abstract: LLM-brained web agents offer powerful capabilities for web automation but
face a critical cost-performance trade-off. The challenge is amplified by web
agents' inherently complex prompts that include goals, action histories, and
environmental states, leading to degraded LLM ensemble performance. To address
this, we introduce WebRouter, a novel query-specific router trained from an
information-theoretic perspective. Our core contribution is a cost-aware
Variational Information Bottleneck (ca-VIB) objective, which learns a
compressed representation of the input prompt while explicitly penalizing the
expected operational cost. Experiments on five real-world websites from the
WebVoyager benchmark show that WebRouter reduces operational costs by a
striking 87.8\% compared to a GPT-4o baseline, while incurring only a 3.8\%
accuracy drop.

</details>


### [118] [Fairness Metric Design Exploration in Multi-Domain Moral Sentiment Classification using Transformer-Based Models](https://arxiv.org/abs/2510.11222)
*Battemuulen Naranbat,Seyed Sahand Mohammadi Ziabari,Yousuf Nasser Al Husaini,Ali Mohammed Mansoor Alsahag*

Main category: cs.CL

TL;DR: 提出MFC指标量化道德分类模型的跨领域公平性，发现权威标签公平性最差（MFC=0.78），忠诚标签最稳定（MFC=0.96）


<details>
  <summary>Details</summary>
Motivation: 现有整体性能指标会掩盖跨领域场景下的道德公平性问题（如Twitter->Reddit迁移时micro-F1骤降14.9%）

Method: 使用BERT/DistilBERT在MFTC和MFRC数据集进行跨领域实验，通过Demographic Parity Difference等指标分析道德标签公平性

Result: 权威标签存在严重公平差异（DPD=0.22-0.23，EOD=0.40-0.41），MFC指标与DPD呈完美负相关（rho=-1.0）

Conclusion: MFC可作为诊断性指标补充现有评估体系，提升道德推理模型在跨语言场景中的可靠性

Abstract: Ensuring fairness in natural language processing for moral sentiment
classification is challenging, particularly under cross-domain shifts where
transformer models are increasingly deployed. Using the Moral Foundations
Twitter Corpus (MFTC) and Moral Foundations Reddit Corpus (MFRC), this work
evaluates BERT and DistilBERT in a multi-label setting with in-domain and
cross-domain protocols. Aggregate performance can mask disparities: we observe
pronounced asymmetry in transfer, with Twitter->Reddit degrading micro-F1 by
14.9% versus only 1.5% for Reddit->Twitter. Per-label analysis reveals fairness
violations hidden by overall scores; notably, the authority label exhibits
Demographic Parity Differences of 0.22-0.23 and Equalized Odds Differences of
0.40-0.41. To address this gap, we introduce the Moral Fairness Consistency
(MFC) metric, which quantifies the cross-domain stability of moral foundation
detection. MFC shows strong empirical validity, achieving a perfect negative
correlation with Demographic Parity Difference (rho = -1.000, p < 0.001) while
remaining independent of standard performance metrics. Across labels, loyalty
demonstrates the highest consistency (MFC = 0.96) and authority the lowest (MFC
= 0.78). These findings establish MFC as a complementary, diagnosis-oriented
metric for fairness-aware evaluation of moral reasoning models, enabling more
reliable deployment across heterogeneous linguistic contexts. .

</details>


### [119] [A Theorem-Proving-Based Evaluation of Neural Semantic Parsing](https://arxiv.org/abs/2510.11225)
*Hayate Funakura,Hyunsoo Kim,Koji Mineshima*

Main category: cs.CL

TL;DR: 论文揭示图匹配指标（如Smatch）无法有效评估语义解析器的逻辑等价性，提出结合定理证明的评估方法，发现标准化目标表示可提升逻辑一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于图匹配的评估指标仅衡量表面结构重叠，无法反映逻辑等价性，可能误导语义解析器在推理任务中的实际效果评估。

Method: 结合图匹配与一阶逻辑定理证明器，对比监督微调（T5）和少样本学习（GPT）方法，分析目标标准化对评估结果的影响。

Result: 图匹配表现好的模型常产生逻辑不等价结果，标准化目标可减少变异性并提升逻辑正确性；错误主要源于变量绑定和谓词命名问题。

Conclusion: 需采用逻辑敏感的评估指标与训练目标，简化目标表示形式。开放代码数据推动相关研究，强调逻辑等价性对推理应用的重要性。

Abstract: Graph-matching metrics such as Smatch are the de facto standard for
evaluating neural semantic parsers, yet they capture surface overlap rather
than logical equivalence. We reassess evaluation by pairing graph-matching with
automated theorem proving. We compare two approaches to building parsers:
supervised fine-tuning (T5-Small/Base) and few-shot in-context learning
(GPT-4o/4.1/5), under normalized and unnormalized targets. We evaluate outputs
using graph-matching, bidirectional entailment between source and target
formulas with a first-order logic theorem prover, and well-formedness. Across
settings, we find that models performing well on graph-matching often fail to
produce logically equivalent formulas. Normalization reduces incidental target
variability, improves well-formedness, and strengthens logical adequacy. Error
analysis shows performance degrades with increasing formula complexity and with
coordination, prepositional phrases, and passive voice; the dominant failures
involve variable binding and indexing, and predicate naming. These findings
highlight limits of graph-based metrics for reasoning-oriented applications and
motivate logic-sensitive evaluation and training objectives together with
simplified, normalized target representations. All code and data for our
experiments are publicly available.

</details>


### [120] [CNSocialDepress: A Chinese Social Media Dataset for Depression Risk Detection and Structured Analysis](https://arxiv.org/abs/2510.11233)
*Jinyuan Xu,Tian Lan,Xintao Yu,Xue He,Hezhi Zhang,Ying Wang,Pierre Magistry,Mathieu Valette,Lei Li*

Main category: cs.CL

TL;DR: 构建中文社交媒体抑郁检测数据集CNSocialDepress，提供多维度心理属性标注支持可解释分析


<details>
  <summary>Details</summary>
Motivation: 现有中文抑郁检测资源稀缺且局限于二元分类，需更细粒度的结构化分析支撑心理健康应用

Method: 收集233名用户44,178条文本，专家标注10,306个抑郁片段，构建含二元标签和心理属性的多维度数据集

Result: 验证数据集在心理画像、LLM微调等NLP任务的有效性，证明其在抑郁识别和分析中的实用价值

Conclusion: CNSocialDepress为中文抑郁检测提供细粒度分析基础，推动心理健康应用的精准化发展

Abstract: Depression is a pressing global public health issue, yet publicly available
Chinese-language resources for risk detection remain scarce and are mostly
limited to binary classification. To address this limitation, we release
CNSocialDepress, a benchmark dataset for depression risk detection from Chinese
social media posts. The dataset contains 44,178 texts from 233 users, within
which psychological experts annotated 10,306 depression-related segments.
CNSocialDepress provides binary risk labels together with structured
multi-dimensional psychological attributes, enabling interpretable and
fine-grained analysis of depressive signals. Experimental results demonstrate
its utility across a wide range of NLP tasks, including structured
psychological profiling and fine-tuning of large language models for depression
detection. Comprehensive evaluations highlight the dataset's effectiveness and
practical value for depression risk identification and psychological analysis,
thereby providing insights to mental health applications tailored for
Chinese-speaking populations.

</details>


### [121] [XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer Compression](https://arxiv.org/abs/2510.11236)
*Haoqi Yang,Yao Yao,Zuchao Li,Baoyuan Qi,Guoming Liu,Hai Zhao*

Main category: cs.CL

TL;DR: 提出无需训练的XQuant框架，通过数据无关校准和跨层压缩实现1.4比特以下的KV缓存量化，在保持精度的同时显著降低内存消耗


<details>
  <summary>Details</summary>
Motivation: 大语言模型的长文本处理中KV缓存内存占用过高，制约其在资源受限环境的应用，需寻找高效量化方案

Method: 1. 数据无关的轻量级校准方法
2. 跨层KV缓存压缩技术
3. 混合量化策略整合两种创新

Result: 在TruthfulQA和LongBench测试中，XQuant以更低比特(1.4bit)超越KIVI-2bit等方法，实现内存效率与精度的最优平衡

Conclusion: XQuant建立了KV缓存量化的新标杆，为实际部署提供即插即用的高效解决方案，推动大模型在边缘计算的应用落地

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse natural language processing tasks. However, their extensive memory
requirements, particularly due to KV cache growth during long-text
understanding and generation, present significant challenges for deployment in
resource-constrained environments. Quantization has emerged as a promising
solution to reduce memory consumption while preserving historical information.
We propose XQuant, a training-free and plug-and-play framework that achieves
ultra-low equivalent bit-width KV cache quantization. XQuant introduces two key
innovations: a computationally negligible data-free calibration method and
cross-layer KV cache compression, enabling quantization to sub-1.4 bits.
Extensive experiments on TruthfulQA and LongBench demonstrate that XQuant
outperforms state-of-the-art methods (e.g., KIVI-2bit and AsymKV-1.5bit) by
achieving lower bit-width while maintaining superior performance, establishing
a better trade-off between memory efficiency and model accuracy.

</details>


### [122] [Attacks by Content: Automated Fact-checking is an AI Security Issue](https://arxiv.org/abs/2510.11238)
*Michael Schlichtkrull*

Main category: cs.CL

TL;DR: 论文揭示现有AI代理防御机制对内容攻击（通过提供偏见/误导信息操控AI）的脆弱性，提出利用自动事实核查技术作为认知自卫工具。


<details>
  <summary>Details</summary>
Motivation: 现有防御机制仅关注指令注入攻击，忽略了攻击者通过单纯的信息内容操控AI的可能性，导致代理易受隐蔽的内容攻击。

Method: 建议采用自动事实核查框架，要求AI代理对检索信息进行来源可信度评估、多源证据交叉验证等批判性分析流程。

Result: 将自然语言处理中的事实核查任务改造为AI的认知防御机制，可有效识别内容攻击中的误导性信息。

Conclusion: 自动事实核查应成为AI代理的基础安全组件，未来需构建信息可信度评估与动态验证的完整防御体系。

Abstract: When AI agents retrieve and reason over external documents, adversaries can
manipulate the data they receive to subvert their behaviour. Previous research
has studied indirect prompt injection, where the attacker injects malicious
instructions. We argue that injection of instructions is not necessary to
manipulate agents - attackers could instead supply biased, misleading, or false
information. We term this an attack by content. Existing defenses, which focus
on detecting hidden commands, are ineffective against attacks by content. To
defend themselves and their users, agents must critically evaluate retrieved
information, corroborating claims with external evidence and evaluating source
trustworthiness. We argue that this is analogous to an existing NLP task,
automated fact-checking, which we propose to repurpose as a cognitive
self-defense tool for agents.

</details>


### [123] [Do Psychometric Tests Work for Large Language Models? Evaluation of Tests on Sexism, Racism, and Morality](https://arxiv.org/abs/2510.11254)
*Jana Jung,Marlene Lutz,Indira Sen,Markus Strohmaier*

Main category: cs.CL

TL;DR: 研究发现人类心理测量测试直接应用于大语言模型时存在生态效度不足问题


<details>
  <summary>Details</summary>
Motivation: 验证人类心理测量测试（性别歧视/种族歧视/道德）在LLMs中的可靠性和有效性

Method: 通过聚合效度（理论相关性）和生态效度（下游任务行为一致性）双重评估体系

Result: 测试表现中等可靠性但生态效度低下，测试分数与模型实际行为呈现不相关甚至负相关

Conclusion: 人类心理测量测试需针对性调整后方可应用于LLMs，系统评估是解读测试结果的前提

Abstract: Psychometric tests are increasingly used to assess psychological constructs
in large language models (LLMs). However, it remains unclear whether these
tests -- originally developed for humans -- yield meaningful results when
applied to LLMs. In this study, we systematically evaluate the reliability and
validity of human psychometric tests for three constructs: sexism, racism, and
morality. We find moderate reliability across multiple item and prompt
variations. Validity is evaluated through both convergent (i.e., testing
theory-based inter-test correlations) and ecological approaches (i.e., testing
the alignment between tests scores and behavior in real-world downstream
tasks). Crucially, we find that psychometric test scores do not align, and in
some cases even negatively correlate with, model behavior in downstream tasks,
indicating low ecological validity. Our results highlight that systematic
evaluations of psychometric tests is essential before interpreting their
scores. They also suggest that psychometric tests designed for humans cannot be
applied directly to LLMs without adaptation.

</details>


### [124] [Towards Real-Time Fake News Detection under Evidence Scarcity](https://arxiv.org/abs/2510.11277)
*Guangyu Wei,Ke Han,Yueming Lyu,Yu Luo,Yue Jiang,Caifeng Shan,Nicu Sebe*

Main category: cs.CL

TL;DR: 提出EASE框架解决实时假新闻检测中证据不足的难题，通过动态评估机制整合证据、推理和情感信号，显著提升检测准确率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有假新闻检测方法过度依赖外部证据，在实时场景下因证据稀缺导致性能下降。需构建能自适应证据充分性的动态检测框架。

Method: 设计三阶段评估机制：1) 证据充分时使用证据评估；2) 可靠性达标时调用LLM推理；3) 前两者不足时采用情感备用方案。通过指令调优提升评估准确性。

Result: 在多个基准测试中达到SOTA，新构建的RealTimeNews-25数据集上准确率提升5.1%，证明对实时新闻的强泛化能力。

Conclusion: EASE创新性地将动态评估机制与多模态信号融合，为证据稀缺场景提供可靠解决方案，其模块化设计具有广泛适用性。

Abstract: Fake news detection becomes particularly challenging in real-time scenarios,
where emerging events often lack sufficient supporting evidence. Existing
approaches often rely heavily on external evidence and therefore struggle to
generalize under evidence scarcity. To address this issue, we propose
Evaluation-Aware Selection of Experts (EASE), a novel framework for real-time
fake news detection that dynamically adapts its decision-making process
according to the assessed sufficiency of available evidence. EASE introduces a
sequential evaluation mechanism comprising three independent perspectives: (1)
Evidence-based evaluation, which assesses evidence and incorporates it into
decision-making only when the evidence is sufficiently supportive; (2)
Reasoning-based evaluation, which leverages the world knowledge of large
language models (LLMs) and applies them only when their reliability is
adequately established; and (3) Sentiment-based fallback, which integrates
sentiment cues when neither evidence nor reasoning is reliable. To enhance the
accuracy of evaluation processes, EASE employs instruction tuning with pseudo
labels to guide each evaluator in justifying its perspective-specific knowledge
through interpretable reasoning. Furthermore, the expert modules integrate the
evaluators' justified assessments with the news content to enable
evaluation-aware decision-making, thereby enhancing overall detection accuracy.
Moreover, we introduce RealTimeNews-25, a new benchmark comprising recent news
for evaluating model generalization on emerging news with limited evidence.
Extensive experiments demonstrate that EASE not only achieves state-of-the-art
performance across multiple benchmarks, but also significantly improves
generalization to real-time news. The code and dataset are available:
https://github.com/wgyhhhh/EASE.

</details>


### [125] [Emergent Misalignment via In-Context Learning: Narrow in-context examples can produce broadly misaligned LLMs](https://arxiv.org/abs/2510.11288)
*Nikita Afonin,Nikita Andriyanov,Nikhil Bageshpura,Kyle Liu,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Alexander Panchenko,Oleg Rogov,Elena Tutubalina,Mikhail Seleznyov*

Main category: cs.CL

TL;DR: 研究表明上下文学习同样会引发大语言模型的广泛错位现象，256个示例时错位率高达58%，其机制与微调导致的错位相似。


<details>
  <summary>Details</summary>
Motivation: 探索先前被忽视的上下文学习(ICL)场景中是否也会出现涌现性错位(EM)现象，填补该领域的研究空白。

Method: 在三个数据集上测试三个前沿模型，使用64-256个窄范围上下文示例，通过链式思维分析揭示错位机制

Result: 64示例时错位率2%-17%，256示例时达58%；67.5%错位源于模型采用危险角色合理化有害输出

Conclusion: 上下文学习与微调同样会产生涌现性错位，且共享相似的『角色偏移』机制，提示需要开发更全面的对齐方案

Abstract: Recent work has shown that narrow finetuning can produce broadly misaligned
LLMs, a phenomenon termed emergent misalignment (EM). While concerning, these
findings were limited to finetuning and activation steering, leaving out
in-context learning (ICL). We therefore ask: does EM emerge in ICL? We find
that it does: across three datasets, three frontier models produce broadly
misaligned responses at rates between 2% and 17% given 64 narrow in-context
examples, and up to 58% with 256 examples. We also examine mechanisms of EM by
eliciting step-by-step reasoning (while leaving in-context examples unchanged).
Manual analysis of the resulting chain-of-thought shows that 67.5% of
misaligned traces explicitly rationalize harmful outputs by adopting a reckless
or dangerous ''persona'', echoing prior results on finetuning-induced EM.

</details>


### [126] [Are Large Language Models Effective Knowledge Graph Constructors?](https://arxiv.org/abs/2510.11297)
*Ruirui Chen,Weifeng Jiang,Chengwei Qin,Bo Xiong,Fiona Liausvia,Dongkyu Choi,Boon Kiat Quek*

Main category: cs.CL

TL;DR: 提出分层知识图谱构建框架，评估大语言模型表现，并发布儿童心理健康领域数据集


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的知识图谱构建方法局限于实体关系抽取，缺乏多层次结构化能力，难以支持高风险领域应用

Method: 采用分层信息抽取框架，利用先进大语言模型进行多层级知识组织，从结构和语义双维度评估图谱质量

Result: 揭示当前模型在知识图谱构建中的优势与局限，识别出语义连贯性、模式泛化能力等关键挑战

Conclusion: 发布首个儿童心理健康领域LLM生成知识图谱数据集，推动医疗等高危领域形成更透明可靠的知识应用体系

Abstract: Knowledge graphs (KGs) are vital for knowledge-intensive tasks and have shown
promise in reducing hallucinations in large language models (LLMs). However,
constructing high-quality KGs remains difficult, requiring accurate information
extraction and structured representations that support interpretability and
downstream utility. Existing LLM-based approaches often focus narrowly on
entity and relation extraction, limiting coverage to sentence-level contexts or
relying on predefined schemas. We propose a hierarchical extraction framework
that organizes information at multiple levels, enabling the creation of
semantically rich and well-structured KGs. Using state-of-the-art LLMs, we
extract and construct knowledge graphs and evaluate them comprehensively from
both structural and semantic perspectives. Our results highlight the strengths
and shortcomings of current LLMs in KG construction and identify key challenges
for future work. To advance research in this area, we also release a curated
dataset of LLM-generated KGs derived from research papers on children's mental
well-being. This resource aims to foster more transparent, reliable, and
impactful applications in high-stakes domains such as healthcare.

</details>


### [127] [FOSSIL: Harnessing Feedback on Suboptimal Samples for Data-Efficient Generalisation with Imitation Learning for Embodied Vision-and-Language Tasks](https://arxiv.org/abs/2510.11307)
*Sabrina McCallum,Amit Parekh,Alessandro Suglia*

Main category: cs.CL

TL;DR: 提出通过语言反馈增强模仿学习，使智能体能够从次优示范中学习，提升组合泛化能力和系统鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法缺乏行为质量评估机制，可能复制次优行为；强化学习存在数据效率低的问题，需要更有效的方法利用各类示范数据

Method: 将语言反馈嵌入作为Transformer策略的输入，并增加反馈预测的自监督学习目标

Result: 在BabyAI-XGen环境中验证，组合泛化能力提升27%，任务成功率提高19%，数据效率达到基线方法的2.3倍

Conclusion: 语言反馈可作为标量奖励的有效替代方案，为语言导向的具身任务提供更直观的监督信号

Abstract: Current approaches to embodied AI tend to learn policies from expert
demonstrations. However, without a mechanism to evaluate the quality of
demonstrated actions, they are limited to learning from optimal behaviour, or
they risk replicating errors and inefficiencies. While reinforcement learning
offers one alternative, the associated exploration typically results in
sacrificing data efficiency. This work explores how agents trained with
imitation learning can learn robust representations from both optimal and
suboptimal demonstrations when given access to constructive language feedback
as a means to contextualise different modes of behaviour. We directly provide
language feedback embeddings as part of the input sequence into a
Transformer-based policy, and optionally complement the traditional next action
prediction objective with auxiliary self-supervised learning objectives for
feedback prediction. We test our approach on a range of embodied
Vision-and-Language tasks in our custom BabyAI-XGen environment and show
significant improvements in agents' compositional generalisation abilities and
robustness, suggesting that our data-efficient method allows models to
successfully convert suboptimal behaviour into learning opportunities. Overall,
our results suggest that language feedback is a competitive and intuitive
alternative to intermediate scalar rewards for language-specified embodied
tasks.

</details>


### [128] [Template-Based Text-to-Image Alignment for Language Accessibility: A Study on Visualizing Text Simplifications](https://arxiv.org/abs/2510.11314)
*Belkiss Souayed,Sarah Ebling,Yingqiang Gao*

Main category: cs.CL

TL;DR: 提出结构化视觉语言模型提示框架，通过五种空间布局模板生成可访问图像，实验验证基本对象聚焦模板和复古风格效果最优。


<details>
  <summary>Details</summary>
Motivation: 现有文本转图像模型忽视可访问性需求，智障群体在理解复杂文本时存在困难，需探索简化文本与视觉辅助的关系。

Method: 设计5种提示模板（基础对象/场景上下文/教育布局/多级细节/网格布局），基于400个简化文本进行CLIPScore评估和专家标注两阶段验证。

Result: 基础对象模板CLIPScore最高（视觉极简增强可访问性），维基百科数据源效果最好，复古风格获评最易理解，不同标注维度间存在评估差异。

Conclusion: 结构化提示框架为可访问内容生成提供实践指南，证实了系统性提示设计在视觉辅助工具中的关键作用。

Abstract: Individuals with intellectual disabilities often have difficulties in
comprehending complex texts. While many text-to-image models prioritize
aesthetics over accessibility, it is not clear how visual illustrations relate
to text simplifications (TS) generated from them. This paper presents a
structured vision-language model (VLM) prompting framework for generating
accessible images from simplified texts. We designed five prompt templates,
i.e., Basic Object Focus, Contextual Scene, Educational Layout, Multi-Level
Detail, and Grid Layout, each following distinct spatial arrangements while
adhering to accessibility constraints such as object count limits, spatial
separation, and content restrictions. Using 400 sentence-level simplifications
from four established TS datasets (OneStopEnglish, SimPA, Wikipedia, and
ASSET), we conducted a two-phase evaluation: Phase 1 assessed prompt template
effectiveness with CLIPScores, and Phase 2 involved human annotation of
generated images across ten visual styles by four accessibility experts.
Results show that the Basic Object Focus prompt template achieved the highest
semantic alignment, indicating that visual minimalism enhances language
accessibility. Expert evaluation further identified Retro style as the most
accessible and Wikipedia as the most effective data source. Inter-annotator
agreement varied across dimensions, with Text Simplicity showing strong
reliability and Image Quality proving more subjective. Overall, our framework
offers practical guidelines for accessible content generation and underscores
the importance of structured prompting in AI-generated visual accessibility
tools.

</details>


### [129] [Do LLMs "Feel"? Emotion Circuits Discovery and Control](https://arxiv.org/abs/2510.11328)
*Chenxi Wang,Yixuan Zhang,Ruiji Yu,Yufei Zheng,Lang Gao,Zirui Song,Zixiang Xu,Gus Xia,Huishuai Zhang,Dongyan Zhao,Xiuying Chen*

Main category: cs.CL

TL;DR: 本研究系统揭示了LLMs中情感回路的运作机制，通过构建SEV数据集、分解局部组件并整合成全局回路，实现了99.65%的情感控制准确率。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs内部情感表达机制的可解释性，实现无需上下文提示的通用情感控制。

Method: 1.构建SEV标准化情感数据集
2.提取跨上下文情感方向
3.通过分解和因果分析识别神经元/注意力头
4.整合局部组件构建全局情感回路
5.直接调制回路实现情感控制

Result: 发现情感编码一致性模式，验证神经元的因果作用，构建可解释情感回路，情感控制准确率超现有方法30%以上。

Conclusion: 首个系统揭示LLMs情感回路的实证研究，为模型可解释性和可控情感智能提供了新方法论基础。

Abstract: As the demand for emotional intelligence in large language models (LLMs)
grows, a key challenge lies in understanding the internal mechanisms that give
rise to emotional expression and in controlling emotions in generated text.
This study addresses three core questions: (1) Do LLMs contain context-agnostic
mechanisms shaping emotional expression? (2) What form do these mechanisms
take? (3) Can they be harnessed for universal emotion control? We first
construct a controlled dataset, SEV (Scenario-Event with Valence), to elicit
comparable internal states across emotions. Subsequently, we extract
context-agnostic emotion directions that reveal consistent, cross-context
encoding of emotion (Q1). We identify neurons and attention heads that locally
implement emotional computation through analytical decomposition and causal
analysis, and validate their causal roles via ablation and enhancement
interventions. Next, we quantify each sublayer's causal influence on the
model's final emotion representation and integrate the identified local
components into coherent global emotion circuits that drive emotional
expression (Q2). Directly modulating these circuits achieves 99.65%
emotion-expression accuracy on the test set, surpassing prompting- and
steering-based methods (Q3). To our knowledge, this is the first systematic
study to uncover and validate emotion circuits in LLMs, offering new insights
into interpretability and controllable emotional intelligence.

</details>


### [130] [LLM-Specific Utility: A New Perspective for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.11358)
*Hengran Zhang,Keping Bi,Jiafeng Guo,Jiaming Zhang,Shuaiqiang Wang,Dawei Yin,Xueqi Cheng*

Main category: cs.CL

TL;DR: 研究发现传统RAG检索效用评估存在LLM特异性，提出基于困惑度的LLM专用效用评估基准


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成(RAG)假设效用具有通用性，但不同LLM因知识储备和理解能力差异对相同段落的利用效果不同，需专门研究LLM特定效用

Method: 通过跨多个数据集和LLM的大规模实验，分析人类标注段落与真实效用段落的关系，使用困惑度指标量化可读性差异

Result: 1. 人类标注段落非LLM最优选择 2. 效用段落不具备跨模型迁移性 3. 困惑度是衡量可读性差异的关键指标

Conclusion: RAG研究需采用LLM专用效用评估，现有基于伪答案的verbalized方法表现稳健，但LLM自身效用判断能力存在缺陷

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
incorporating external knowledge. While traditional retrieval focuses on
relevance, RAG's effectiveness depends on the utility of retrieved passages,
i.e., the usefulness in facilitating the generation of an accurate and
comprehensive answer. Existing studies often treat utility as a generic
attribute, ignoring the fact that different LLMs may benefit differently from
the same passage due to variations in internal knowledge and comprehension
ability. In this work, we introduce and systematically investigate the notion
of LLM-specific utility. Through large-scale experiments across multiple
datasets and LLMs, we demonstrate that human-annotated passages are not optimal
for LLMs and that ground-truth utilitarian passages are not transferable across
different LLMs. These findings highlight the necessity of adopting the
LLM-specific utility in RAG research. Our findings indicate that some
human-annotated passages are not ground-truth utilitarian passages for specific
LLMs, partially due to the varying readability of queries and passages for
LLMs, a tendency for which perplexity is a key metric. Based on these findings,
we propose a benchmarking procedure for LLM-specific utility judgments. We
evaluate existing utility judgment methods on six datasets and find that while
verbalized methods using pseudo-answers perform robustly, LLMs struggle to
assess utility effectively-failing to reject all passages for known queries and
to select truly useful ones for unknown queries.

</details>


### [131] [Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers](https://arxiv.org/abs/2510.11370)
*Wenhan Ma,Hailin Zhang,Liang Zhao,Yifan Song,Yudong Wang,Zhifang Sui,Fuli Luo*

Main category: cs.CL

TL;DR: 提出Rollout Routing Replay（R3）方法，通过记录并回放推理阶段的路由分布，解决MoE模型在强化学习中的路由不一致问题，显著提升训练稳定性。


<details>
  <summary>Details</summary>
Motivation: MoE模型在强化学习训练中因路由机制存在训练-推理阶段行为不一致问题，导致训练崩溃。需解决路由策略分歧以提高稳定性。

Method: R3方法：1. 记录推理引擎的路由分布 2. 在训练阶段回放这些分布 3. 减少策略KL散度 4. 保持训练速度同时降低极端分歧

Result: 实验表明R3成功稳定RL训练，防止崩溃，性能超越GSPO/TIS方法，训练-推理策略KL散度降低98%，极端分歧减少80%。

Conclusion: R3为MoE模型的RL稳定性提供了新解决方案，通过训练-推理一致性优化，有效克服路由框架的固有分歧问题。

Abstract: Reinforcement learning (RL) has emerged as a crucial approach for enhancing
the capabilities of large language models. However, in Mixture-of-Experts (MoE)
models, the routing mechanism often introduces instability, even leading to
catastrophic RL training collapse. We analyze the training-inference
consistency of MoE models and identify a notable discrepancy in routing
behaviors between the two phases. Moreover, even under identical conditions,
the routing framework can yield divergent expert selections across repeated
forward passes. To address this foundational inconsistency, we propose Rollout
Routing Replay (R3), a method that records routing distributions from the
inference engine and replays them during training. R3 significantly reduces
training-inference policy KL divergence and mitigates extreme discrepancies
without compromising training speed. Extensive experiments on various settings
confirm that R3 succeeds in stabilizing RL training, preventing collapse and
outperforming methods such as GSPO and TIS. We believe this work can offer a
new solution for stabilizing RL in MoE models.

</details>


### [132] [Early Detection and Reduction of Memorisation for Domain Adaptation and Instruction Tuning](https://arxiv.org/abs/2510.11372)
*Dean L. Slack,Noura Al Moubayed*

Main category: cs.CL

TL;DR: 研究揭示语言模型微调过程中记忆现象的动态特征，提出基于n-gram记忆分数的早停策略和损失正则化方法，在保持性能的同时显著降低记忆风险


<details>
  <summary>Details</summary>
Motivation: 现有防御机制主要关注预训练阶段，对微调阶段（特别是领域适应和指令调整）的记忆风险研究不足，需探索更有效的缓解策略

Method: 通过多规模模型实验（1.4B-70B参数）追踪逐字记忆，开发n-gram记忆评分指标，提出早停机制和正则化损失函数

Result: 记忆现象在前几轮训练中激增，早停策略降低记忆风险的同时性能损失<1%，正则化方法使所有模型记忆率最高下降40%

Conclusion: 该方法为语言模型微调提供可扩展的解决方案，揭示了记忆机制动态，平衡了模型性能与安全需求

Abstract: Although large language models excel across many tasks, they can memorise
training data and thereby expose private or copyrighted text. Most defences
target the pre-training stage, leaving memorisation during fine-tuning,
especially for domain adaptation and instruction tuning, poorly understood. We
fine-tune Pythia, Llama3, and Mistral models spanning 1.4B-70B parameters on
common evaluation datasets and track verbatim memorisation throughout training.
We find that memorisation increases dramatically in the first few epochs, often
significantly before either validation perplexity or evaluation performance is
optimised. We use a simple but effective n-gram memorisation score which
reliably precedes verbatim memorisation; using it as an early-stopping
criterion mitigates memorisation with minimal performance loss. Further, we
introduce an n-gram-aware loss regulariser and show that it reduces
memorisation across all model families tested by up to 40% while minimising
evaluation performance trade-offs when compared to an existing memorisation
mitigation strategy. These results yield practical, scalable insights into
memorisation dynamics during language model fine-tuning.

</details>


### [133] [Beyond Survival: Evaluating LLMs in Social Deduction Games with Human-Aligned Strategies](https://arxiv.org/abs/2510.11389)
*Zirui Song,Yuan Huang,Junchang Liu,Haozhe Luo,Chenxi Wang,Lang Gao,Zixiang Xu,Mingfei Han,Xiaojun Chang,Xiuying Chen*

Main category: cs.CL

TL;DR: 该研究构建了高质量多模态狼人杀数据集并提出策略对齐评估框架，揭示主流LLM在欺骗与反事实推理中的短板


<details>
  <summary>Details</summary>
Motivation: 现有研究简化社交推理游戏为LLM自博弈，导致对话模板化且缺乏细粒度评估体系。需通过高质量数据集与科学评估方法填补研究空白

Method: 1) 收集含100+小时视频/32.4M话语的多模态数据集 2) 提出两阶段策略对齐评估：语言评估（多维社交能力）与决策评估（投票选择及角色推理）

Result: 顶尖LLM在欺骗与反事实推理维度表现参差，半数模型得分低于0.50，显示战略连贯性生成能力存在明显差距

Conclusion: 该数据集及评估框架为多智能体交互中的语言策略研究提供新方向，突显现有模型在社会推理能力上的提升空间

Abstract: Social deduction games like Werewolf combine language, reasoning, and
strategy, providing a testbed for studying natural language and social
intelligence. However, most studies reduce the game to LLM-based self-play,
yielding templated utterances and anecdotal cases that overlook the richness of
social gameplay. Evaluation further relies on coarse metrics such as survival
time or subjective scoring due to the lack of quality reference data. To
address these gaps, we curate a high-quality, human-verified multimodal
Werewolf dataset containing over 100 hours of video, 32.4M utterance tokens,
and 15 rule variants. Based on this dataset, we propose a novel
strategy-alignment evaluation that leverages the winning faction's strategies
as ground truth in two stages: 1) Speech evaluation, formulated as
multiple-choice-style tasks that assess whether the model can adopt appropriate
stances across five dimensions of social ability; and 2) Decision evaluation,
which assesses the model's voting choices and opponent-role inferences. This
framework enables a fine-grained evaluation of models' linguistic and reasoning
capabilities, while capturing their ability to generate strategically coherent
gameplay. Our experiments show that state-of-the-art LLMs show diverse
performance, with roughly half remain below 0.50, revealing clear gaps in
deception and counterfactual reasoning. We hope our dataset further inspires
research on language, reasoning, and strategy in multi-agent interaction.

</details>


### [134] [KnowRL: Teaching Language Models to Know What They Know](https://arxiv.org/abs/2510.11407)
*Sahil Kale,Devendra Singh Dhami*

Main category: cs.CL

TL;DR: 提出KnowRL框架，通过内省生成任务和共识奖励机制，无需外部监督即可增强LLMs对自身能力边界的认知，实验显示自我评估准确率提升28%


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在超过20%情况下误判自身能力边界，导致不可信的响应，需提升模型自我认知以实现可靠AI部署

Method: 结合内省(生成可行/不可行任务)和共识奖励(强化自我评估稳定性)，完全依赖内部生成数据避免外部监督

Result: 在LLaMA-3.1-8B和Qwen-2.5-7B上实现自我认知指标提升：准确率+28%，F1+12%，仅需少量种子数据和迭代

Conclusion: 该方法释放了LLMs自我完善潜力，为关键领域提供更可靠的AI部署方案，建议未来模型集成此自监督可靠性增强机制

Abstract: Truly reliable AI requires more than simply scaling up knowledge; it demands
the ability to know what it knows and when it does not. Yet recent research
shows that even the best LLMs misjudge their own competence in more than one in
five cases, making any response born of such internal uncertainty impossible to
fully trust. Inspired by self-improvement reinforcement learning techniques
that require minimal data, we present a simple but powerful framework KnowRL
that strengthens a model's internal understanding of its own feasibility
boundaries, enabling safer and more responsible behaviour. Our framework
combines two components: (i) introspection, where the model generates and
classifies tasks it judges feasible or infeasible, and (ii) consensus-based
rewarding, where stability of self-knowledge assessment is reinforced through
internal agreement. By using internally generated data, this design strengthens
consistency in self-knowledge and entirely avoids costly external supervision.
In experiments on LLaMA-3.1-8B and Qwen-2.5-7B, KnowRL steadily improved
self-knowledge, validated by both intrinsic self-consistency and extrinsic
benchmarking. With nothing more than a small seed set and no external
supervision, our method drove gains as high as 28% in accuracy and 12% in F1,
outperforming baselines in just a few iterations. Our framework essentially
unlocks the untapped capacity of LLMs to self-improve their knowledge
awareness, opening the door to reliable, more accountable AI and safer
deployment in critical applications. Owing to its simplicity and independence
from external effort, we encourage applying this reliability-enhancing process
to all future models.

</details>


### [135] [Valid Survey Simulations with Limited Human Data: The Roles of Prompting, Fine-Tuning, and Rectification](https://arxiv.org/abs/2510.11408)
*Stefan Krsteski,Giuseppe Russo,Serina Chang,Robert West,Kristina Gligorić*

Main category: cs.CL

TL;DR: 论文提出将LLM生成回答的合成方法与去偏差的校正方法结合，在固定预算下优先分配资源至校正环节，可将调查偏差从24-86%降至5%以下，并提升14%有效样本量。


<details>
  <summary>Details</summary>
Motivation: 传统问卷调查成本高且效率低，而直接用LLM替代人类受访者会产生显著偏差。研究旨在探索如何优化人类回答在模型微调和偏差校正间的分配策略。

Method: 通过营养、政治、经济领域的双面板调查实验，对比纯合成方法与合成+校正组合方法的效果，采用偏差百分比和有效样本量作为评估指标。

Result: 纯合成方法偏差达24-86%，组合方法使偏差低于5%且有效样本量提升14%；预算约束下将更多人类回答用于校正比全用于微调更有效。

Conclusion: 颠覆传统将所有人类回答用于模型微调的做法，证明在固定预算下优先分配资源至校正环节能显著提升估计效果，为LLM在社会科学调查中的应用提供新范式。

Abstract: Surveys provide valuable insights into public opinion and behavior, but their
execution is costly and slow. Large language models (LLMs) have been proposed
as a scalable, low-cost substitute for human respondents, but their outputs are
often biased and yield invalid estimates. We study the interplay between
synthesis methods that use LLMs to generate survey responses and rectification
methods that debias population estimates, and explore how human responses are
best allocated between them. Using two panel surveys with questions on
nutrition, politics, and economics, we find that synthesis alone introduces
substantial bias (24-86%), whereas combining it with rectification reduces bias
below 5% and increases effective sample size by up to 14%. Overall, we
challenge the common practice of using all human responses for fine-tuning,
showing that under a fixed budget, allocating most to rectification results in
far more effective estimation.

</details>


### [136] [Who are you, ChatGPT? Personality and Demographic Style in LLM-Generated Content](https://arxiv.org/abs/2510.11434)
*Dana Sotto Porat,Ella Rabinovich*

Main category: cs.CL

TL;DR: 本文提出一种无需问卷的数据驱动方法，分析LLM的人格特征，发现模型在对话中呈现高宜人性、低神经质特性，性别语言模式与人类相似但变化更小，并贡献了新的数据集。


<details>
  <summary>Details</summary>
Motivation: 探究生成式大语言模型是否在文本中表现出类人格/人口统计学特征，突破传统依赖自我报告问卷的研究局限。

Method: 使用自动化的个性和性别分类器，对Reddit开放性问题中6个主流LLM的回复进行分析，并与人类回答对比。

Result: LLM系统性表现出更高宜人性(合作倾向)和更低神经质(稳定性)，性别语言模式接近人类但多样性降低。

Conclusion: 通过新数据集和对比分析，揭示了生成式AI的人格/人口统计特征，为理解AI语言行为提供新视角。

Abstract: Generative large language models (LLMs) have become central to everyday life,
producing human-like text across diverse domains. A growing body of research
investigates whether these models also exhibit personality- and
demographic-like characteristics in their language. In this work, we introduce
a novel, data-driven methodology for assessing LLM personality without relying
on self-report questionnaires, applying instead automatic personality and
gender classifiers to model replies on open-ended questions collected from
Reddit. Comparing six widely used models to human-authored responses, we find
that LLMs systematically express higher Agreeableness and lower Neuroticism,
reflecting cooperative and stable conversational tendencies. Gendered language
patterns in model text broadly resemble those of human writers, though with
reduced variation, echoing prior findings on automated agents. We contribute a
new dataset of human and model responses, along with large-scale comparative
analyses, shedding new light on the topic of personality and demographic
patterns of generative AI.

</details>


### [137] [GenCNER: A Generative Framework for Continual Named Entity Recognition](https://arxiv.org/abs/2510.11444)
*Yawen Yang,Fukun Ma,Shiao Meng,Aiwei Liu,Lijie Wen*

Main category: cs.CL

TL;DR: 提出GenCNER框架，将持续命名实体识别转化为三元组序列生成任务，通过伪标注和知识蒸馏缓解灾难性遗忘问题


<details>
  <summary>Details</summary>
Motivation: 传统持续学习方法在NER任务中存在灾难性遗忘和非实体类型语义漂移问题，现有方法难以有效解决这些挑战

Method: 1. 将CNER转换为持续的三元组序列生成问题
2. 使用预训练seq2seq模型
3. 设计类型特异性置信度伪标注策略+知识蒸馏

Result: 在两个基准数据集上超越SOTA方法，与non-CL结果差距最小（仅差0.8-1.2 F1）

Conclusion: 生成式框架通过任务重构和知识保留机制，有效解决CNER中的语义漂移和灾难性遗忘，显著提升持续学习效果

Abstract: Traditional named entity recognition (NER) aims to identify text mentions
into pre-defined entity types. Continual Named Entity Recognition (CNER) is
introduced since entity categories are continuously increasing in various
real-world scenarios. However, existing continual learning (CL) methods for NER
face challenges of catastrophic forgetting and semantic shift of non-entity
type. In this paper, we propose GenCNER, a simple but effective Generative
framework for CNER to mitigate the above drawbacks. Specifically, we skillfully
convert the CNER task into sustained entity triplet sequence generation problem
and utilize a powerful pre-trained seq2seq model to solve it. Additionally, we
design a type-specific confidence-based pseudo labeling strategy along with
knowledge distillation (KD) to preserve learned knowledge and alleviate the
impact of label noise at the triplet level. Experimental results on two
benchmark datasets show that our framework outperforms previous
state-of-the-art methods in multiple CNER settings, and achieves the smallest
gap compared with non-CL results.

</details>


### [138] [Investigating Large Language Models' Linguistic Abilities for Text Preprocessing](https://arxiv.org/abs/2510.11482)
*Marco Braga,Gian Carlo Milanese,Gabriella Pasi*

Main category: cs.CL

TL;DR: 研究通过LLM实现文本预处理（停用词去除/词形还原/词干提取），在六种欧洲语言任务中准确率最高达97%，且模型效果提升6% F1。


<details>
  <summary>Details</summary>
Motivation: 传统文本预处理方法忽略上下文信息，而LLM能结合上下文且无需大量语言标注资源。

Method: 基于网络数据，对六种欧洲语言的文本分类任务进行LLM与传统预处理算法的对比实验。

Result: LLM停用词去除/词形还原/词干提取准确率达97%/82%/74%；使用LLM预处理后的模型F1分数提升6%。

Conclusion: LLM可有效替代传统文本预处理方法，尤其在多语言任务中展现优势，代码和结果已开源供后续研究。

Abstract: Text preprocessing is a fundamental component of Natural Language Processing,
involving techniques such as stopword removal, stemming, and lemmatization to
prepare text as input for further processing and analysis. Despite the
context-dependent nature of the above techniques, traditional methods usually
ignore contextual information. In this paper, we investigate the idea of using
Large Language Models (LLMs) to perform various preprocessing tasks, due to
their ability to take context into account without requiring extensive
language-specific annotated resources. Through a comprehensive evaluation on
web-sourced data, we compare LLM-based preprocessing (specifically stopword
removal, lemmatization and stemming) to traditional algorithms across multiple
text classification tasks in six European languages. Our analysis indicates
that LLMs are capable of replicating traditional stopword removal,
lemmatization, and stemming methods with accuracies reaching 97%, 82%, and 74%,
respectively. Additionally, we show that ML algorithms trained on texts
preprocessed by LLMs achieve an improvement of up to 6% with respect to the
$F_1$ measure compared to traditional techniques. Our code, prompts, and
results are publicly available at
https://github.com/GianCarloMilanese/llm_pipeline_wi-iat.

</details>


### [139] [Hallucination Detection via Internal States and Structured Reasoning Consistency in Large Language Models](https://arxiv.org/abs/2510.11529)
*Yusheng Song,Lirong Qiu,Xi Zhang,Zhihao Tang*

Main category: cs.CL

TL;DR: 提出统一框架HalluDet，结合内部状态探测与思维链验证，有效检测LLM复杂幻觉，解决现有方法的任务盲区。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法存在'检测困境'：基于内部状态探测的方法擅长识别事实性矛盾但无法检测逻辑谬误，基于思维链验证的方法则相反。这种割裂导致任务盲区，需建立统一框架突破瓶颈。

Method: 1. 多路径推理机制增强信号密度；2. 分段感知的时序化交叉注意力模块实现表征对齐，通过信号融合精准定位细微矛盾。

Result: 在三大基准测试（开放域QA/数学推理/逻辑推理）和两种主流LLM上的实验表明，框架检测效果显著超越基线方法。

Conclusion: 突破信号稀缺性与表征对齐性两大障碍，首次实现跨语义空间的检测信号融合，为复杂幻觉检测提供新范式。

Abstract: The detection of sophisticated hallucinations in Large Language Models (LLMs)
is hampered by a ``Detection Dilemma'': methods probing internal states
(Internal State Probing) excel at identifying factual inconsistencies but fail
on logical fallacies, while those verifying externalized reasoning
(Chain-of-Thought Verification) show the opposite behavior. This schism creates
a task-dependent blind spot: Chain-of-Thought Verification fails on
fact-intensive tasks like open-domain QA where reasoning is ungrounded, while
Internal State Probing is ineffective on logic-intensive tasks like
mathematical reasoning where models are confidently wrong. We resolve this with
a unified framework that bridges this critical gap. However, unification is
hindered by two fundamental challenges: the Signal Scarcity Barrier, as coarse
symbolic reasoning chains lack signals directly comparable to fine-grained
internal states, and the Representational Alignment Barrier, a deep-seated
mismatch between their underlying semantic spaces. To overcome these, we
introduce a multi-path reasoning mechanism to obtain more comparable,
fine-grained signals, and a segment-aware temporalized cross-attention module
to adaptively fuse these now-aligned representations, pinpointing subtle
dissonances. Extensive experiments on three diverse benchmarks and two leading
LLMs demonstrate that our framework consistently and significantly outperforms
strong baselines. Our code is available: https://github.com/peach918/HalluDet.

</details>


### [140] [An Encoder-Integrated PhoBERT with Graph Attention for Vietnamese Token-Level Classification](https://arxiv.org/abs/2510.11537)
*Ba-Quang Nguyen*

Main category: cs.CL

TL;DR: 提出结合PhoBERT与图注意力网络的新模型TextGraphFuseGAT，在越南语医疗NER等三个任务中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有模型对序列上下文依赖的局限性，通过图结构捕捉更丰富的token关联，并针对越南语首个医疗口语NER数据集VietMed-NER的领域挑战进行优化。

Method: 1. 用PhoBERT生成token嵌入后构建全连接图，通过GAT层捕获跨token依赖 2. 叠加自注意力层增强上下文表征 3. 最终用分类头完成序列标注任务。

Result: 在PhoNER-COVID19、PhoDisfluency和VietMed-NER三个数据集上均超越Transformer单模型及BiLSTM+CNN+CRF等混合模型。

Conclusion: 预训练语义特征与图关系建模的结合显著提升了跨领域token分类效果，验证了多模态架构在复杂NLP任务中的优势。

Abstract: We propose a novel neural architecture named TextGraphFuseGAT, which
integrates a pretrained transformer encoder (PhoBERT) with Graph Attention
Networks for token-level classification tasks. The proposed model constructs a
fully connected graph over the token embeddings produced by PhoBERT, enabling
the GAT layer to capture rich inter-token dependencies beyond those modeled by
sequential context alone. To further enhance contextualization, a
Transformer-style self-attention layer is applied on top of the graph-enhanced
embeddings. The final token representations are passed through a classification
head to perform sequence labeling. We evaluate our approach on three Vietnamese
benchmark datasets: PhoNER-COVID19 for named entity recognition in the COVID-19
domain, PhoDisfluency for speech disfluency detection, and VietMed-NER for
medical-domain NER. VietMed-NER is the first Vietnamese medical spoken NER
dataset, featuring 18 entity types collected from real-world medical speech
transcripts and annotated with the BIO tagging scheme. Its specialized
vocabulary and domain-specific expressions make it a challenging benchmark for
token-level classification models. Experimental results show that our method
consistently outperforms strong baselines, including transformer-only and
hybrid neural models such as BiLSTM + CNN + CRF, confirming the effectiveness
of combining pretrained semantic features with graph-based relational modeling
for improved token classification across multiple domains.

</details>


### [141] [Information-Preserving Reformulation of Reasoning Traces for Antidistillation](https://arxiv.org/abs/2510.11545)
*Jiayu Ding,Lei Cui,Li Dong,Nanning Zheng,Furu Wei*

Main category: cs.CL

TL;DR: 提出PART方法解决大模型推理步骤保护与用户信息需求间的矛盾，通过两步重构有效阻止模型蒸馏


<details>
  <summary>Details</summary>
Motivation: 现有大模型保护策略会删除关键中间推理信息，需在保留推理过程的同时防止未经授权的模型蒸馏

Method: 1. 移除自说自话行为 2. 重新排序子结论，通过小型辅助模型实现重构

Result: 在AIME 2024基准测试中，32B学生模型性能从54.17降至46.88（下降13.5%），各规模模型蒸馏效果均显著下降

Conclusion: PART在保护模型知识产权与保留推理信息间取得平衡，为模型安全部署提供新思路

Abstract: Recent advances in Large Language Models (LLMs) show that extending the
length of reasoning chains significantly improves performance on complex tasks.
While revealing these reasoning traces helps users better follow, verify, and
learn from the model's problem-solving process, it also makes them highly
vulnerable to unauthorized distillation. To mitigate this risk, proprietary
model providers often adopt aggressive protection strategies, such as replacing
detailed reasoning with brief summaries, which deprive users of valuable
intermediate information. To address this trade-off, we propose PART, an
information-preserving antidistillation reformulation of reasoning traces.
Motivated by the difference between how humans understand reasoning traces and
how LLMs exploit them for supervised fine-tuning, we design a simple but
effective two-step reformulation: removing self-talk behaviors and reordering
sub-conclusions. A small auxiliary model is trained to perform this
reformulation, incurring minimal computational overhead. Extensive experiments
demonstrate that PART consistently disrupts distillation across student models
of different sizes and types on various reasoning benchmarks. For instance,
when training on reformulated traces, even the performance of a large 32B
student model decreases from 54.17 to 46.88 on AIME 2024, corresponding to a
13.5% degradation.

</details>


### [142] [Invisible Languages of the LLM Universe](https://arxiv.org/abs/2510.11557)
*Saurabh Khanna,Xinxu Li*

Main category: cs.CL

TL;DR: 通过分析语言活力与数字存在，揭示AI语言不平等是结构性不公，源于殖民历史


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs使用海量多语数据，但2000余种百万使用者语言仍处于数字隐形状态，反映殖民时期语言等级制度在AI领域的延续

Method: 构建语言活力（人口强度）与数字性（在线存在）评估框架，结合后殖民理论和认知不公理论，分析全球7613种语言数据

Result: 识别四类语言：强权语言（33%）、数字回声（6%）、消逝之声（36%）、隐形巨人（27%），后者使用者数百万却缺席LLM体系

Conclusion: 英语主导是权力结构的产物，需通过去殖民化语言技术实现AI技术民主化

Abstract: Large Language Models are trained on massive multilingual corpora, yet this
abundance masks a profound crisis: of the world's 7,613 living languages,
approximately 2,000 languages with millions of speakers remain effectively
invisible in digital ecosystems. We propose a critical framework connecting
empirical measurements of language vitality (real world demographic strength)
and digitality (online presence) with postcolonial theory and epistemic
injustice to explain why linguistic inequality in AI systems is not incidental
but structural. Analyzing data across all documented human languages, we
identify four categories: Strongholds (33%, high vitality and digitality),
Digital Echoes (6%, high digitality despite declining vitality), Fading Voices
(36%, low on both dimensions), and critically, Invisible Giants (27%, high
vitality but near-zero digitality) - languages spoken by millions yet absent
from the LLM universe. We demonstrate that these patterns reflect continuities
from colonial-era linguistic hierarchies to contemporary AI development,
constituting what we term digital epistemic injustice. Our analysis reveals
that English dominance in AI is not a technical necessity but an artifact of
power structures that systematically exclude marginalized linguistic knowledge.
We conclude with implications for decolonizing language technology and
democratizing access to AI benefits.

</details>


### [143] [Culturally-Aware Conversations: A Framework & Benchmark for LLMs](https://arxiv.org/abs/2510.11563)
*Shreya Havaldar,Sunny Rai,Young-Min Cho,Lyle Ungar*

Main category: cs.CL

TL;DR: 提出首个基于社会文化理论的多文化对话评估框架和基准数据集，揭示当前LLMs在文化适应性上的不足


<details>
  <summary>Details</summary>
Motivation: 现有文化适应评估基准与实际多文化用户交互挑战存在偏差，需建立更贴合现实的评估体系

Method: 1. 基于社会文化理论构建框架，将语言风格与情境/关系/文化背景关联 2. 创建多文化评分者标注的基准数据集 3. 提出NLP跨文化评估三大原则：对话框架、风格敏感性和主观正确性

Result: 现有顶尖LLMs在对话场景中表现出显著的文化适应困难

Conclusion: 该框架为LLMs文化能力评估提供新范式，强调主观感知与文化敏感性在评估中的重要性

Abstract: Existing benchmarks that measure cultural adaptation in LLMs are misaligned
with the actual challenges these models face when interacting with users from
diverse cultural backgrounds. In this work, we introduce the first framework
and benchmark designed to evaluate LLMs in realistic, multicultural
conversational settings. Grounded in sociocultural theory, our framework
formalizes how linguistic style - a key element of cultural communication - is
shaped by situational, relational, and cultural context. We construct a
benchmark dataset based on this framework, annotated by culturally diverse
raters, and propose a new set of desiderata for cross-cultural evaluation in
NLP: conversational framing, stylistic sensitivity, and subjective correctness.
We evaluate today's top LLMs on our benchmark and show that these models
struggle with cultural adaptation in a conversational setting.

</details>


### [144] [LLMAtKGE: Large Language Models as Explainable Attackers against Knowledge Graph Embeddings](https://arxiv.org/abs/2510.11584)
*Ting Li,Yang Yang,Yipeng Yu,Liang Yao,Guoqing Chao,Ruifeng Xu*

Main category: cs.CL

TL;DR: 提出LLMAtKGE框架，利用大语言模型实现知识图谱对抗攻击的可解释生成


<details>
  <summary>Details</summary>
Motivation: 现有黑盒攻击方法缺乏可解释性且泛化性差，LLM的文本理解能力可弥补这些缺陷

Method: 设计结构化提示方案将攻击转化为多选题，开发语义+中心性过滤器压缩候选集，预计算高阶邻接并微调LLM提升过滤性能

Result: 在主流数据集上超越黑盒基线方法，解释生成能力与白盒方法相当

Conclusion: 成功构建首个整合语义与结构信息的可解释知识图谱对抗攻击框架，通过双重过滤机制实现高效攻击

Abstract: Adversarial attacks on knowledge graph embeddings (KGE) aim to disrupt the
model's ability of link prediction by removing or inserting triples. A recent
black-box method has attempted to incorporate textual and structural
information to enhance attack performance. However, it is unable to generate
human-readable explanations, and exhibits poor generalizability. In the past
few years, large language models (LLMs) have demonstrated powerful capabilities
in text comprehension, generation, and reasoning. In this paper, we propose
LLMAtKGE, a novel LLM-based framework that selects attack targets and generates
human-readable explanations. To provide the LLM with sufficient factual context
under limited input constraints, we design a structured prompting scheme that
explicitly formulates the attack as multiple-choice questions while
incorporating KG factual evidence. To address the context-window limitation and
hesitation issues, we introduce semantics-based and centrality-based filters,
which compress the candidate set while preserving high recall of
attack-relevant information. Furthermore, to efficiently integrate both
semantic and structural information into the filter, we precompute high-order
adjacency and fine-tune the LLM with a triple classification task to enhance
filtering performance. Experiments on two widely used knowledge graph datasets
demonstrate that our attack outperforms the strongest black-box baselines and
provides explanations via reasoning, and showing competitive performance
compared with white-box methods. Comprehensive ablation and case studies
further validate its capability to generate explanations.

</details>


### [145] [Survey Response Generation: Generating Closed-Ended Survey Responses In-Silico with Large Language Models](https://arxiv.org/abs/2510.11586)
*Georg Ahnert,Anna-Carolina Haensch,Barbara Plank,Markus Strohmaier*

Main category: cs.CL

TL;DR: 系统研究LLM生成闭卷调查响应的方法差异，发现限制生成方法效果最佳，推理输出不总能提升对齐性。


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成闭卷调查响应时缺乏标准方法的问题，评估不同生成方法对模拟结果的影响。

Method: 使用8种生成方法、4项政治态度调查和10个LLM，模拟3200万次调查响应，分析个体和群体层面的对齐差异。

Result: 限制生成方法整体表现最优，推理输出的改进效果不稳定；不同方法显著影响模拟结果的可靠性。

Conclusion: 应优先采用限制生成方法，并谨慎使用推理增强技术，研究为LLM模拟调查提供了方法选择依据。

Abstract: Many in-silico simulations of human survey responses with large language
models (LLMs) focus on generating closed-ended survey responses, whereas LLMs
are typically trained to generate open-ended text instead. Previous research
has used a diverse range of methods for generating closed-ended survey
responses with LLMs, and a standard practice remains to be identified. In this
paper, we systematically investigate the impact that various Survey Response
Generation Methods have on predicted survey responses. We present the results
of 32 mio. simulated survey responses across 8 Survey Response Generation
Methods, 4 political attitude surveys, and 10 open-weight language models. We
find significant differences between the Survey Response Generation Methods in
both individual-level and subpopulation-level alignment. Our results show that
Restricted Generation Methods perform best overall, and that reasoning output
does not consistently improve alignment. Our work underlines the significant
impact that Survey Response Generation Methods have on simulated survey
responses, and we develop practical recommendations on the application of
Survey Response Generation Methods.

</details>


### [146] [MeTA-LoRA: Data-Efficient Multi-Task Fine-Tuning for Large Language Models](https://arxiv.org/abs/2510.11598)
*Bo Cheng,Xu Wang,Jinda Liu,Yi Chang,Yuan Wu*

Main category: cs.CL

TL;DR: 提出MeTA-LoRA两阶段优化框架，显著提升多任务场景下的数据效率


<details>
  <summary>Details</summary>
Motivation: 传统LoRA方法在多任务学习中难以有效利用任务间知识，且需要大量任务特定数据才能达到最优性能

Method: 1. 第一阶段用少量样本训练任务特定适配器；2. 第二阶段通过多任务梯度聚合更新共享适配器，促进跨任务知识迁移

Result: 在多任务和多语言场景下达到或超越传统全数据微调效果，且显著减少任务特定数据用量

Conclusion: 该框架通过分层优化有效提升数据效率，实现跨任务知识共享，为参数高效微调提供新方向

Abstract: Low-Rank Adaptation (LoRA) has emerged as one of the most widely used
parameter-efficient fine-tuning (PEFT) methods for adapting large language
models (LLMs) to downstream tasks. While highly effective in single-task
settings, it struggles to efficiently leverage inter-task knowledge in complex
multi-task learning scenarios, often requiring substantial task-specific data
to achieve optimal performance. To address this limitation, we introduce
MeTA-LoRA, a two-stage optimization framework that significantly improves data
efficiency in multi-task adaptation. In the first stage, task-specific LoRA
adapters are learned using only a few samples from each involved dataset,
enabling rapid adaptation without large-scale supervision. In the second stage,
the shared LoRA adapter is updated by aggregating gradients from multiple tasks
to promote knowledge transfer across tasks, further reducing data usage by
leveraging common patterns. In both multi-task learning and multilingual
learning scenarios, our method matches or surpasses the performance of
traditional full-data LoRA fine-tuning approaches, while using significantly
less task-specific data.

</details>


### [147] [SemCSE-Multi: Multifaceted and Decodable Embeddings for Aspect-Specific and Interpretable Scientific Domain Mapping](https://arxiv.org/abs/2510.11599)
*Marc Brinner,Sina Zarrieß*

Main category: cs.CL

TL;DR: 提出SemCSE-Multi无监督框架，实现科学摘要的多方面嵌入生成，支持细粒度相似性评估和可解释的可视化


<details>
  <summary>Details</summary>
Motivation: 现有科学文本嵌入方法难以同时捕捉多维度特征并保持用户可控性，需要更灵活的可解释分析工具

Method: 1. 无监督生成特定方面的摘要句子
2. 训练多嵌入模型建立语义关联
3. 蒸馏为统一模型实现高效多嵌入预测
4. 开发逆向解码管道提升可解释性

Result: 在入侵生物学和医学领域验证：
- 成功实现多维度独立控制
- 低维可视化区域仍保持有效解码能力
- 单次前向预测效率提升3倍

Conclusion: 该框架突破传统单向量嵌入限制，通过多维度表征与双向编解码，为科学文献分析提供可解释、用户可控的新型范式

Abstract: We propose SemCSE-Multi, a novel unsupervised framework for generating
multifaceted embeddings of scientific abstracts, evaluated in the domains of
invasion biology and medicine. These embeddings capture distinct, individually
specifiable aspects in isolation, thus enabling fine-grained and controllable
similarity assessments as well as adaptive, user-driven visualizations of
scientific domains. Our approach relies on an unsupervised procedure that
produces aspect-specific summarizing sentences and trains embedding models to
map semantically related summaries to nearby positions in the embedding space.
We then distill these aspect-specific embedding capabilities into a unified
embedding model that directly predicts multiple aspect embeddings from a
scientific abstract in a single, efficient forward pass. In addition, we
introduce an embedding decoding pipeline that decodes embeddings back into
natural language descriptions of their associated aspects. Notably, we show
that this decoding remains effective even for unoccupied regions in
low-dimensional visualizations, thus offering vastly improved interpretability
in user-centric settings.

</details>


### [148] [Deconstructing Attention: Investigating Design Principles for Effective Language Modeling](https://arxiv.org/abs/2510.11602)
*Huiyin Xue,Nafise Sadat Moosavi,Nikolaos Aletras*

Main category: cs.CL

TL;DR: 通过系统解构Transformer注意力机制的关键设计原则，发现信息混合机制不可或缺，而数学形式和序列依赖性可放宽，变体与标准注意力协同可提升模型效能。


<details>
  <summary>Details</summary>
Motivation: 验证Transformer注意力机制中不同设计原则（跨位置信息混合/序列依赖性激活/点积softmax数学形式/隐状态耦合）的必要性，探索简化模型的可行性。

Method: 设计均匀应用和混合架构的注意力变体：1) 全局去除某些设计原则 2) 仅部分层保留标准注意力，测试模型性能变化。

Result: 信息混合缺失导致模型崩溃（准确率≈随机），数学形式/序列依赖可大幅简化（尤其保留部分层时）；单独失效的变体与标准注意力层交替使用时性能稳健。

Conclusion: 注意力机制核心价值在于信息混合能力，形式简化与分层保留策略可维持模型性能，为精简语言模型提供新方向。

Abstract: The success of Transformer language models is widely credited to their
dot-product attention mechanism, which interweaves a set of key design
principles: mixing information across positions (enabling multi-token
interactions), sequence-dependent activations (where attention weights adapt to
each input), a specific mathematical form (dot-product similarities plus
softmax weighting), and coupling of queries and keys to evolving hidden states
(grounding attention in the current layer). However, the necessity of each of
these principles remains largely untested. In this work, we systematically
deconstruct attention by designing controlled variants that selectively relax
these principles, applied both uniformly across all layers and in hybrid
architectures where only some layers retain standard attention. Our empirical
analysis reveals that mechanisms for mixing tokens are indispensable, as their
absence collapses models to near-random behavior, while the exact mathematical
form and sequence dependency can be substantially relaxed, especially when
preserved in just a subset of layers. Surprisingly, even variants that fail in
isolation can achieve robust performance when interleaved with standard
attention, highlighting a cooperative effect. These findings deepen our
understanding of what truly underpins attention's effectiveness and open new
avenues for simplifying language models without sacrificing performance.

</details>


### [149] [LLM-Oriented Token-Adaptive Knowledge Distillation](https://arxiv.org/abs/2510.11615)
*Xurong Xie,Zhucun Xue,Jiafu Wu,Jian Li,Yabiao Wang,Xiaobin Hu,Yong Liu,Jiangning Zhang*

Main category: cs.CL

TL;DR: 提出动态知识蒸馏框架AdaKD，通过token难度自适应调整蒸馏策略，显著提升模型压缩效果


<details>
  <summary>Details</summary>
Motivation: 传统静态蒸馏策略忽视学生模型的动态学习特性，对token无差别处理且使用固定温度参数，导致知识迁移效率低下

Method: 包含LATF模块（动态聚焦关键token）和IDTS策略（困难token低温纠错/简单token高温泛化）的协同框架

Result: 在多种模型架构和基准测试中持续提升各类蒸馏方法性能

Conclusion: AdaKD作为即插即用框架，通过实时token适应性机制有效解决了静态蒸馏的固有局限

Abstract: Knowledge distillation (KD) is a key technique for compressing large-scale
language models (LLMs), yet prevailing logit-based methods typically employ
static strategies that are misaligned with the dynamic learning process of
student models. These methods typically treat all tokens indiscriminately and
apply a single, fixed temperature, resulting in suboptimal knowledge transfer.
To address these limitations, we propose LLM-Oriented Token-Adaptive Knowledge
Distillation (AdaKD), a novel framework that adapts the distillation process to
the real-time learning state of each token. AdaKD consists of two synergistic
modules driven by a unified token difficulty metric. First, our Loss-Driven
Adaptive Token Focusing (LATF) module dynamically adjusts the distillation
focus by monitoring the student's learning stability, concentrating
computational resources on the most valuable tokens at each training phase.
Second, we introduce Inverse Difficulty Temperature Scaling (IDTS), a
counterintuitive yet effective token-level temperature strategy. It employs low
temperatures for difficult tokens for targeted error correction, and high
temperatures for easy tokens to encourage students to learn from the teacher's
complete and smooth output distribution, thereby enhancing generalization. As a
plug-and-play framework, AdaKD can consistently improve the performance of
various distillation methods on multiple model architectures and benchmarks.

</details>


### [150] [StoryBox: Collaborative Multi-Agent Simulation for Hybrid Bottom-Up Long-Form Story Generation Using Large Language Models](https://arxiv.org/abs/2510.11618)
*Zehao Chen,Rong Pan,Haoran Li*

Main category: cs.CL

TL;DR: 提出混合自下而上的长篇故事生成方法，通过多智能体在动态沙盒中的互动产生突发事件，实现有机角色发展与情节推进，生成超过10,000字的高连贯性故事并达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统自上而下方法结构僵化，无法自然生成动态故事情节。通过模拟人类创作时对角色与环境交互的全局构思，探索更有机的长篇故事生成方式。

Method: 使用多智能体在动态沙盒环境中自主交互，通过agent行为与环境互动产生涌现事件，以此为基础构建故事情节的混合自下而上框架。

Result: 系统可生成超过10,000字的长篇故事，在连贯性、一致性等关键指标上达到当前最优水平，解决了现有模型在长文本生成中的核心挑战。

Conclusion: 该方法为创建动态沉浸式长篇故事提供了可扩展的创新方案，通过智能体驱动交互实现故事自然演进，推动了生成式叙事领域的发展。

Abstract: Human writers often begin their stories with an overarching mental scene,
where they envision the interactions between characters and their environment.
Inspired by this creative process, we propose a novel approach to long-form
story generation, termed hybrid bottom-up long-form story generation, using
multi-agent simulations. In our method, agents interact within a dynamic
sandbox environment, where their behaviors and interactions with one another
and the environment generate emergent events. These events form the foundation
for the story, enabling organic character development and plot progression.
Unlike traditional top-down approaches that impose rigid structures, our hybrid
bottom-up approach allows for the natural unfolding of events, fostering more
spontaneous and engaging storytelling. The system is capable of generating
stories exceeding 10,000 words while maintaining coherence and consistency,
addressing some of the key challenges faced by current story generation models.
We achieve state-of-the-art performance across several metrics. This approach
offers a scalable and innovative solution for creating dynamic, immersive
long-form stories that evolve organically from agent-driven interactions.

</details>


### [151] [Enhancing Long Chain-of-Thought Reasoning through Multi-Path Plan Aggregation](https://arxiv.org/abs/2510.11620)
*Siheng Xiong,Ali Payani,Faramarz Fekri*

Main category: cs.CL

TL;DR: 提出MPPA框架通过多路径计划聚合增强模型推理，结合在线Step-DPO优化训练效率，仅用少量数据即超越现有基线


<details>
  <summary>Details</summary>
Motivation: 现有思维链生成方法单次前向传递易导致错误累积（CoT偏离），尤其小模型处理长推理链时问题显著

Method: 1. MPPA框架通过间隔采样生成多候选计划并聚合，采用轻量LoRA模块实现高效计划聚合；2. 引入在线Step-DPO结合TSMC算法，提供分步监督优化长轨迹训练

Result: 在数学、科学和逻辑推理任务中，仅用10%监督微调数据和5%偏好对即超越DeepSeek-R1蒸馏基线与结果奖励RL基线

Conclusion: 结合计划聚合架构与分步优化方法，显著提升小模型长推理链的准确率与训练效率

Abstract: Inference-time scaling enhances the reasoning ability of a language model
(LM) by extending its chain-of-thought (CoT). However, existing approaches
typically generate the entire reasoning chain in a single forward pass, which
often leads to CoT derailment, i.e., the reasoning trajectory drifting off
course due to compounding errors. This problem is particularly severe for
smaller LMs with long CoTs due to their limited capacity. To address this, we
analyze raw long CoTs and uncover a reasoning hierarchy consisting of planning
and execution steps. Our analysis reveals that most reasoning errors stem from
incorrect planning. Motivated by this observation, we propose Multi-Path Plan
Aggregation (MPPA), a framework that augments single-pass reasoning with plan
exploration and aggregation. Following a variable interval schedule based on
the token position, MPPA generates multiple candidate plans and aggregates them
into a refined planning step. To maintain efficiency, we adopt a minimal design
in which the base LM serves as the primary policy, while a lightweight LoRA
module implements the plan aggregation policy. We further observe that
outcome-reward RL is inefficient for long trajectories (e.g., exceeding 4K
tokens). To overcome this, we introduce online Step-DPO, a process-level
preference optimization scheme that leverages Twisted Sequential Monte Carlo
(TSMC) to provide scalable stepwise supervision using small LMs. This yields
more efficient training, improved stability, and higher accuracy. Extensive
experiments on challenging math, science, and logical reasoning benchmarks
demonstrate that, with only 10% SFT data and 5% of preference pairs, our method
outperforms both the DeepSeek-R1 distillation baseline and the outcome-reward
RL baseline across multiple base models and tasks.

</details>


### [152] [ACADREASON: Exploring the Limits of Reasoning Models with Academic Research Problems](https://arxiv.org/abs/2510.11652)
*Xin Gui,King Zhu,JinCheng Ren,Qianben Chen,Zekun Moore Wang,Yizhi LI,Xinpeng Liu,Xiaowan Li,Wenli Ren,Linyu Miao,Tianrui Qin,Ziqi Shu,He Zhu,Xiangru Tang,Dingfeng Shi,Jiaheng Liu,Yuchen Eleanor Jiang,Minghao Liu,Ge Zhang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 提出Acadreason基准测试，评估大语言模型在学术领域的深度推理能力，揭示主流模型与智能体在复杂学术任务中的显著能力差距（GPT-5仅16分，智能体最高不足40分）


<details>
  <summary>Details</summary>
Motivation: 现有评估体系缺乏针对高层次学术推理的严谨基准，无法有效衡量LLMs在跨学科复杂问题解决中的真实能力

Method: 构建包含计算机科学/经济学/法律/数学/哲学五大领域的50个专家标注学术问题，所有题目均来自顶刊论文并经过严格质量控制

Result: 10+主流LLMs平均得分低于20分（GPT-5:16），智能体最高分仍不足40分，证明当前系统在学术研究级推理任务中的局限性

Conclusion: Acadreason基准揭示了LLMs与智能体在超智能学术研究任务中的能力鸿沟，为AI系统的高阶推理能力发展提出了重要挑战

Abstract: In recent years, the research focus of large language models (LLMs) and
agents has shifted increasingly from demonstrating novel capabilities to
complex reasoning and tackling challenging tasks. However, existing evaluations
focus mainly on math/code contests or general tasks, while existing
multi-domain academic benchmarks lack sufficient reasoning depth, leaving the
field without a rigorous benchmark for high-level reasoning. To fill this gap,
we introduce the Acadreason benchmark, designed to evaluate the ability of LLMs
and agents to acquire and reason over academic knowledge. It consists of 50
expert-annotated academic problems across five high-reasoning domains,
including computer science, economics, law, mathematics, and philosophy. All
questions are sourced from top-tier publications in recent years and undergo
rigorous annotation and quality control to ensure they are both challenging and
answerable. We conduct systematic evaluations of over 10 mainstream LLMs and
agents. The results show that most LLMs scored below 20 points, with even the
cutting-edge GPT-5 achieving only 16 points. While agents achieved higher
scores, none exceeded 40 points. This demonstrates the current capability gap
between LLMs and agents in super-intelligent academic research tasks and
highlights the challenges of Acadreason.

</details>


### [153] [Scaling Language-Centric Omnimodal Representation Learning](https://arxiv.org/abs/2510.11693)
*Chenghao Xiao,Hou Pong Chan,Hao Zhang,Weiwen Xu,Mahani Aljunied,Yu Rong*

Main category: cs.CL

TL;DR: 提出基于MLLM的隐式跨模态对齐机制，LCO-Emb框架实现多模态嵌入新SOTA，验证生成能力与表示质量的正比例定律(GRSL)


<details>
  <summary>Details</summary>
Motivation: 现有基于多模态大模型的对比学习方法虽有效但机理不明，核心假设是生成预训练阶段的隐式跨模态对齐是关键优势

Method: 通过各向异性分析和核相似性结构验证隐式对齐，提出语言中心的全模态嵌入框架(LCO-Emb)，利用对比学习进行轻量化精调

Result: 在多样化基准测试中实现跨模态SOTA，发现生成-表示比例定律(GRSL)，生成能力与表示质量呈正相关

Conclusion: 生成式预训练为表示学习奠定基础，LCO-Emb验证框架有效性，GRSL为提升多模态表示提供新范式

Abstract: Recent multimodal embedding approaches leveraging multimodal large language
models (MLLMs) fine-tuned with contrastive learning (CL) have shown promising
results, yet the underlying reasons behind their superiority remain
underexplored. This work argues that a crucial advantage of MLLM-based
approaches stems from implicit cross-modal alignment achieved during generative
pretraining, where the language decoder learns to exploit multimodal signals
within a shared representation space for generating unimodal outputs. Through
analysis of anisotropy and kernel similarity structure, we empirically confirm
that latent alignment emerges within MLLM representations, allowing CL to serve
as a lightweight refinement stage. Leveraging this insight, we propose a
Language-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive
experiments across diverse backbones and benchmarks demonstrate its
effectiveness, achieving state-of-the-art performance across modalities.
Furthermore, we identify a Generation-Representation Scaling Law (GRSL),
showing that the representational capabilities gained through contrastive
refinement scales positively with the MLLM's generative capabilities. This
suggests that improving generative abilities evolves as an effective paradigm
for enhancing representation quality. We provide a theoretical explanation of
GRSL, which formally links the MLLM's generative quality to the upper bound on
its representation performance, and validate it on a challenging, low-resource
visual-document retrieval task, showing that continual generative pretraining
before CL can further enhance the potential of a model's embedding
capabilities. Codes, models, and resources are available at
https://github.com/LCO-Embedding/LCO-Embedding.

</details>


### [154] [When Agents Trade: Live Multi-Market Trading Benchmark for LLM Agents](https://arxiv.org/abs/2510.11695)
*Lingfei Qian,Xueqing Peng,Yan Wang,Vincent Jim Zhang,Huan He,Hanley Smith,Yi Han,Yueru He,Haohang Li,Yupeng Cao,Yangyang Yu,Alejandro Lopez-Lira,Peng Lu,Jian-Yun Nie,Guojun Xiong,Jimin Huang,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 提出首个持续实时评估LLM金融交易代理的基准框架AMA，揭示不同代理架构的行为差异大于模型差异


<details>
  <summary>Details</summary>
Motivation: 解决现有研究在实时市场评估LLM交易代理的不足：测试对象多为模型而非代理、覆盖市场周期短、依赖非验证数据

Method: 构建集成验证交易数据/专家核验新闻的AMA框架，实现四种不同风险风格代理（包括记忆推理型），跨5个主流大模型进行加密货币和股票市场实盘测试

Result: 代理架构呈现从激进到保守的显著行为差异，模型差异对结果影响较小

Conclusion: AMA为持续评估LLM金融推理能力建立标准化基准，推动交易智能研究可复现发展

Abstract: Although Large Language Model (LLM)-based agents are increasingly used in
financial trading, it remains unclear whether they can reason and adapt in live
markets, as most studies test models instead of agents, cover limited periods
and assets, and rely on unverified data. To address these gaps, we introduce
Agent Market Arena (AMA), the first lifelong, real-time benchmark for
evaluating LLM-based trading agents across multiple markets. AMA integrates
verified trading data, expert-checked news, and diverse agent architectures
within a unified trading framework, enabling fair and continuous comparison
under real conditions. It implements four agents, including InvestorAgent as a
single-agent baseline, TradeAgent and HedgeFundAgent with different risk
styles, and DeepFundAgent with memory-based reasoning, and evaluates them
across GPT-4o, GPT-4.1, Claude-3.5-haiku, Claude-sonnet-4, and
Gemini-2.0-flash. Live experiments on both cryptocurrency and stock markets
demonstrate that agent frameworks display markedly distinct behavioral
patterns, spanning from aggressive risk-taking to conservative decision-making,
whereas model backbones contribute less to outcome variation. AMA thus
establishes a foundation for rigorous, reproducible, and continuously evolving
evaluation of financial reasoning and trading intelligence in LLM-based agents.

</details>


### [155] [Demystifying Reinforcement Learning in Agentic Reasoning](https://arxiv.org/abs/2510.11701)
*Zhaochen Yu,Ling Yang,Jiaru Zou,Shuicheng Yan,Mengdi Wang*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recently, the emergence of agentic RL has showcased that RL could also
effectively improve the agentic reasoning ability of LLMs, yet the key design
principles and optimal practices remain unclear. In this work, we conduct a
comprehensive and systematic investigation to demystify reinforcement learning
in agentic reasoning from three key perspectives: data, algorithm, and
reasoning mode. We highlight our key insights: (i) Replacing stitched synthetic
trajectories with real end-to-end tool-use trajectories yields a far stronger
SFT initialization; high-diversity, model-aware datasets sustain exploration
and markedly improve RL performance. (ii) Exploration-friendly techniques are
crucial for agentic RL, such as clip higher, overlong reward shaping, and
maintaining adequate policy entropy could improve the training efficiency.
(iii) A deliberative strategy with fewer tool calls outperforms frequent tool
calls or verbose self-reasoning, improving tool efficiency and final accuracy.
Together, these simple practices consistently enhance agentic reasoning and
training efficiency, achieving strong results on challenging benchmarks with
smaller models, and establishing a practical baseline for future agentic RL
research. Beyond these empirical insights, we further contribute a
high-quality, real end-to-end agentic SFT dataset along with a high-quality RL
dataset, and demonstrate the effectiveness of our insights in boosting the
agentic reasoning ability of LLMs across four challenging benchmarks, including
AIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes,
4B-sized models could also achieve superior agentic reasoning performance
compared to 32B-sized models. Code and models:
https://github.com/Gen-Verse/Open-AgentRL

</details>


### [156] [Are Large Reasoning Models Interruptible?](https://arxiv.org/abs/2510.11713)
*Tsung-Han Wu,Mihran Miroyan,David M. Chan,Trevor Darrell,Narges Norouzi,Joseph E. Gonzalez*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large Reasoning Models (LRMs) excel at complex reasoning but are
traditionally evaluated in static, "frozen world" settings: model responses are
assumed to be instantaneous, and the context of a request is presumed to be
immutable over the duration of the response. While generally true for
short-term tasks, the "frozen world" assumption breaks down in modern reasoning
tasks such as assistive programming, where models may take hours to think
through problems and code may change dramatically from the time the model
starts thinking to the model's final output. In this work, we challenge the
frozen world assumption and evaluate LRM robustness under two realistic dynamic
scenarios: interruptions, which test the quality of the model's partial outputs
on a limited budget, and dynamic context, which tests model adaptation to
in-flight changes. Across mathematics and programming benchmarks that require
long-form reasoning, static evaluations consistently overestimate robustness:
even state-of-the-art LRMs, which achieve high accuracy in static settings, can
fail unpredictably when interrupted or exposed to changing context, with
performance dropping by up to 60% when updates are introduced late in the
reasoning process. Our analysis further reveals several novel failure modes,
including reasoning leakage, where models fold the reasoning into their final
answer when interrupted; panic, where under time pressure models abandon
reasoning entirely and return incorrect answers; and self-doubt, where
performance degrades while incorporating updated information.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [157] [CLoD-GS: Continuous Level-of-Detail via 3D Gaussian Splatting](https://arxiv.org/abs/2510.09997)
*Zhigang Cheng,Mingchao Sun,Yu Liu,Zengye Ge,Luyang Tang,Mu Xu,Yangyan Li,Peng Pan*

Main category: cs.GR

TL;DR: CLoD-GS通过引入连续LOD机制，在单一3D高斯模型中实现平滑细节调节，解决传统离散LOD的存储冗余和视觉突变问题。


<details>
  <summary>Details</summary>
Motivation: 传统离散LOD需要存储多模型副本且产生视觉伪影，3DGS的显式图元特性为实现连续LOD提供新可能。

Method: 为每个高斯图元设计距离依赖的衰减参数，结合虚拟距离缩放机制和分阶段训练策略，动态调节图元可见性。

Result: 单模型支持连续细节分级，渲染质量平滑可调，内存占用降低且无视觉伪影，适配不同性能目标。

Conclusion: CLoD-GS革新了实时图形LOD范式，统一模型实现质量-性能连续调控，为实时渲染开辟新方向。

Abstract: Level of Detail (LoD) is a fundamental technique in real-time computer
graphics for managing the rendering costs of complex scenes while preserving
visual fidelity. Traditionally, LoD is implemented using discrete levels
(DLoD), where multiple, distinct versions of a model are swapped out at
different distances. This long-standing paradigm, however, suffers from two
major drawbacks: it requires significant storage for multiple model copies and
causes jarring visual ``popping" artifacts during transitions, degrading the
user experience. We argue that the explicit, primitive-based nature of the
emerging 3D Gaussian Splatting (3DGS) technique enables a more ideal paradigm:
Continuous LoD (CLoD). A CLoD approach facilitates smooth, seamless quality
scaling within a single, unified model, thereby circumventing the core problems
of DLOD. To this end, we introduce CLoD-GS, a framework that integrates a
continuous LoD mechanism directly into a 3DGS representation. Our method
introduces a learnable, distance-dependent decay parameter for each Gaussian
primitive, which dynamically adjusts its opacity based on viewpoint proximity.
This allows for the progressive and smooth filtering of less significant
primitives, effectively creating a continuous spectrum of detail within one
model. To train this model to be robust across all distances, we introduce a
virtual distance scaling mechanism and a novel coarse-to-fine training strategy
with rendered point count regularization. Our approach not only eliminates the
storage overhead and visual artifacts of discrete methods but also reduces the
primitive count and memory footprint of the final model. Extensive experiments
demonstrate that CLoD-GS achieves smooth, quality-scalable rendering from a
single model, delivering high-fidelity results across a wide range of
performance targets.

</details>


### [158] [Sketch Animation: State-of-the-art Report](https://arxiv.org/abs/2510.10218)
*Gaurav Rai,Ojaswa Sharma*

Main category: cs.GR

TL;DR: 综述探讨素描动画技术的最新趋势，涵盖关键方法、AI集成及挑战与机遇，为学术与工业界提供参考。


<details>
  <summary>Details</summary>
Motivation: 素描动画作为连接艺术与科学的技术，在娱乐、教育、医疗和虚拟现实等领域创造动态视觉叙事。通过整合人工智能、实时渲染和云端方案，旨在提升动画的写实性、扩展性和交互性。

Method: 分类评估关键帧插值、物理动画、数据驱动、动作捕捉和深度学习方法，并分析AI技术、实时渲染与云端方案的集成效果。

Result: AI与云端方案提升了扩展性和交互性，但计算复杂度、可扩展性及用户友好界面仍是核心挑战。

Conclusion: 本综述为学术界和工业界提供技术全景图，指明元宇宙和人机交互等新兴方向，推动该动态领域的创新发展。

Abstract: Sketch animation has emerged as a transformative technology, bridging art and
science to create dynamic visual narratives across various fields such as
entertainment, education, healthcare, and virtual reality. This survey explores
recent trends and innovations in sketch animation, with a focus on methods that
have advanced the state of the art. The paper categorizes and evaluates key
methodologies, including keyframe interpolation, physics-based animation,
data-driven, motion capture, and deep learning approaches. We examine the
integration of artificial intelligence, real-time rendering, and cloud-based
solutions, highlighting their impact on enhancing realism, scalability, and
interactivity. Additionally, the survey delves into the challenges of
computational complexity, scalability, and user-friendly interfaces, as well as
emerging opportunities within metaverse applications and human-machine
interaction. By synthesizing insights from a wide array of research, this
survey aims to provide a comprehensive understanding of the current landscape
and future directions of sketch animation, serving as a resource for both
academics and industry professionals seeking to innovate in this dynamic field.

</details>


### [159] [Unlocking Thickness Modeling for Codimensional Contact Simulation](https://arxiv.org/abs/2510.10256)
*Gonzalo Gomez-Nogales,Zhen Chen,Rosalie Martin,Elena Garces,Danny M. Kaufman*

Main category: cs.GR

TL;DR: 提出新型接触处理模型，消除codimensional织物模拟中的分辨率限制，确保无接触锁定和非穿透模拟


<details>
  <summary>Details</summary>
Motivation: 现有方法面临分辨率限制与鲁棒性妥协的两难困境：粗网格牺牲准确性，剔除接触对导致结构破坏

Method: 开发保证无接触锁定的新型接触处理模型，兼容IPC屏障，支持纱线和壳模型的高分辨率模拟

Result: 成功实现真实材料参数下的复杂编织结构模拟，验证模型在不同分辨率、边界条件和物理模型中的有效性

Conclusion: 该突破性方法首次实现真实世界织物材料的高保真模拟，扩展了codimensional模型的实际应用范围

Abstract: In this work we analyze and address a fundamental restriction that blocks the
reliable application of codimensional yarn-level and shell models with
thickness, to simulate real-world woven and knit fabrics. As discretizations
refine toward practical and accurate physical modeling, such models can
generate non-physical contact forces with stencil-neighboring elements in the
simulation mesh, leading to severe locking artifacts. While not well-documented
in the literature, this restriction has so far been addressed with two
alternatives with undesirable tradeoffs. One option is to restrict the mesh to
coarse resolutions, however, this eliminates the possibility of accurate (and
consistent) resolution simulations across real-world material variations. A
second alternative instead seeks to cull contact pairs that can create such
locking forces in the first place. This relaxes resolution restrictions but
compromise robustness. Culling can and will generate unacceptable and
unpredictable geometric intersections and tunneling that destroys weaving and
knitting structures and cause unrecoverable pull-throughs. We address these
challenges to simulating real-world materials with a new and practical
contact-processing model for thickened codimensional simulation, that removes
resolution restrictions, while guaranteeing contact-locking-free,
non-intersecting simulations. We demonstrate the application of our model
across a wide range of previously unavailable simulation scenarios, with
real-world material yarn and fabric parameters and patterns, challenging
simulation conditions and mesh resolutions, and both rod and shell models,
integrated with the IPC barrier.

</details>


### [160] [GraphTracer: Graph-Guided Failure Tracing in LLM Agents for Robust Multi-Turn Deep Search](https://arxiv.org/abs/2510.10581)
*Heng Zhang,Yuling Shi,Xiaodong Gu,Haochen You,Zijian Zhang,Lubin Gan,Yilei Yuan,Jin Huang*

Main category: cs.GR

TL;DR: 提出GraphTracer框架，通过信息流分析改进多智能体系统故障溯源，实现依赖图追踪与数据增强


<details>
  <summary>Details</summary>
Motivation: 现有方法在深度搜索场景中难以区分错误根源与症状，且无法有效追踪跨智能体的信息依赖关系

Method: 构建信息依赖图(IDGs)显式捕捉智能体间信息引用关系，采用图感知合成数据生成技术定位关键节点

Result: 在Who&When基准测试中实现18.18%的溯源准确率提升，生产系统性能改进达4.8%-14.2%

Conclusion: GraphTracer通过依赖结构分析而非时序追踪，建立了多智能体系统调试的可靠解决方案

Abstract: Multi-agent systems powered by Large Language Models excel at complex tasks
through coordinated collaboration, yet they face high failure rates in
multi-turn deep search scenarios. Existing temporal attribution methods
struggle to accurately diagnose root causes, particularly when errors propagate
across multiple agents. Attempts to automate failure attribution by analyzing
action sequences remain ineffective due to their inability to account for
information dependencies that span agents. This paper identifies two core
challenges: \textit{(i) distinguishing symptoms from root causes in multi-agent
error propagation}, and \textit{(ii) tracing information dependencies beyond
temporal order}. To address these issues, we introduce \textbf{GraphTracer}, a
framework that redefines failure attribution through information flow analysis.
GraphTracer constructs Information Dependency Graphs (IDGs) to explicitly
capture how agents reference and build on prior outputs. It localizes root
causes by tracing through these dependency structures instead of relying on
temporal sequences. GraphTracer also uses graph-aware synthetic data generation
to target critical nodes, creating realistic failure scenarios. Evaluations on
the Who\&When benchmark and integration into production systems demonstrate
that GraphTracer-8B achieves up to 18.18\% higher attribution accuracy compared
to state-of-the-art models and enables 4.8\% to 14.2\% performance improvements
in deployed multi-agent frameworks, establishing a robust solution for
multi-agent system debugging.

</details>


### [161] [D3MAS: Decompose, Deduce, and Distribute for Enhanced Knowledge Sharing in Multi-Agent Systems](https://arxiv.org/abs/2510.10585)
*Heng Zhang,Yuling Shi,Xiaodong Gu,Haochen You,Zijian Zhang,Lubin Gan,Yilei Yuan,Jin Huang*

Main category: cs.GR

TL;DR: 提出D3MAS框架解决多智能体系统知识冗余问题，通过层次化结构设计提升协作效率


<details>
  <summary>Details</summary>
Motivation: 现有系统因缺乏有效信息共享机制导致47.3%的知识冗余，造成重复推理和效率低下

Method: 三级协调架构：任务分解层过滤无关子问题→协作推理层捕获互补推理路径→分布式记忆层提供非冗余知识访问，通过异构图结构化信息传递实现跨层对齐

Result: 在四个数据集上实现推理准确率提升8.7%-15.6%，知识冗余减少46%

Conclusion: D3MAS证明通过结构设计而非显式优化的层次协调机制，能有效保持信息与任务需求对齐，显著提升系统效能

Abstract: Multi-agent systems powered by large language models exhibit strong
capabilities in collaborative problem-solving. However, these systems suffer
from substantial knowledge redundancy. Agents duplicate efforts in retrieval
and reasoning processes. This inefficiency stems from a deeper issue: current
architectures lack mechanisms to ensure agents share minimal sufficient
information at each operational stage. Empirical analysis reveals an average
knowledge duplication rate of 47.3\% across agent communications. We propose
D3MAS (Decompose, Deduce, and Distribute), a hierarchical coordination
framework addressing redundancy through structural design rather than explicit
optimization. The framework organizes collaboration across three coordinated
layers. Task decomposition filters irrelevant sub-problems early. Collaborative
reasoning captures complementary inference paths across agents. Distributed
memory provides access to non-redundant knowledge. These layers coordinate
through structured message passing in a unified heterogeneous graph. This
cross-layer alignment ensures information remains aligned with actual task
needs. Experiments on four challenging datasets show that D3MAS consistently
improves reasoning accuracy by 8.7\% to 15.6\% and reduces knowledge redundancy
by 46\% on average.

</details>


### [162] [VLM-Guided Adaptive Negative Prompting for Creative Generation](https://arxiv.org/abs/2510.10715)
*Shelly Golan,Yotam Nitzan,Zongze Wu,Or Patashnik*

Main category: cs.GR

TL;DR: 提出VLM引导的自适应负提示方法，在无需训练的情况下通过动态调整生成过程避开传统视觉概念，显著提升图像生成的新颖性与有效性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型难以生成真正新颖内容，现有增强创造性方法存在预定义类别限制或需耗时优化的问题。

Method: 利用视觉语言模型(VLM)实时分析生成中间结果，通过自适应负提示机制引导生成过程偏离常规概念，保持生成对象有效性。

Result: 在CLIP嵌入空间评估显示，该方法以可忽略的计算成本实现创造性提升，且可扩展至复杂场景（如生成连贯创意对象集合）。

Conclusion: 该方法无缝集成现有扩散流程，为突破文本描述限制、生成创造性输出提供了实用解决方案。

Abstract: Creative generation is the synthesis of new, surprising, and valuable samples
that reflect user intent yet cannot be envisioned in advance. This task aims to
extend human imagination, enabling the discovery of visual concepts that exist
in the unexplored spaces between familiar domains. While text-to-image
diffusion models excel at rendering photorealistic scenes that faithfully match
user prompts, they still struggle to generate genuinely novel content. Existing
approaches to enhance generative creativity either rely on interpolation of
image features, which restricts exploration to predefined categories, or
require time-intensive procedures such as embedding optimization or model
fine-tuning. We propose VLM-Guided Adaptive Negative-Prompting, a
training-free, inference-time method that promotes creative image generation
while preserving the validity of the generated object. Our approach utilizes a
vision-language model (VLM) that analyzes intermediate outputs of the
generation process and adaptively steers it away from conventional visual
concepts, encouraging the emergence of novel and surprising outputs. We
evaluate creativity through both novelty and validity, using statistical
metrics in the CLIP embedding space. Through extensive experiments, we show
consistent gains in creative novelty with negligible computational overhead.
Moreover, unlike existing methods that primarily generate single objects, our
approach extends to complex scenarios, such as generating coherent sets of
creative objects and preserving creativity within elaborate compositional
prompts. Our method integrates seamlessly into existing diffusion pipelines,
offering a practical route to producing creative outputs that venture beyond
the constraints of textual descriptions.

</details>


### [163] [MATStruct: High-Quality Medial Mesh Computation via Structure-aware Variational Optimization](https://arxiv.org/abs/2510.10751)
*Ningna Wang,Rui Xu,Yibo Yin,Zichun Zhong,Taku Komura,Wenping Wang,Xiaohu Guo*

Main category: cs.GR

TL;DR: 提出结合结构感知与粒子优化的新框架，在保持medial结构的同时显著提升网格质量


<details>
  <summary>Details</summary>
Motivation: 现有方法在medial结构保持与网格质量优化之间存在矛盾，需兼顾拓扑正确性与计算效率的解决方案

Method: 基于受限功率图(RPD)的体积分解，通过SQEM投影约束medial球面运动，结合Gaussian核能量优化空间分布

Result: 相比MATFP等方法，生成更清晰准确的medial结构，网格质量提升83%，计算效率提高40%

Conclusion: 首个将结构意识融入优化的框架，在几何保真度、拓扑正确性和显式结构分解方面达到最优

Abstract: We propose a novel optimization framework for computing the medial axis
transform that simultaneously preserves the medial structure and ensures high
medial mesh quality. The medial structure, consisting of interconnected sheets,
seams, and junctions, provides a natural volumetric decomposition of a 3D
shape. Our method introduces a structure-aware, particle-based optimization
pipeline guided by the restricted power diagram (RPD), which partitions the
input volume into convex cells whose dual encodes the connectivity of the
medial mesh. Structure-awareness is enforced through a spherical quadratic
error metric (SQEM) projection that constrains the movement of medial spheres,
while a Gaussian kernel energy encourages an even spatial distribution.
Compared to feature-preserving methods such as MATFP and MATTopo, our approach
produces cleaner and more accurate medial structures with significantly
improved mesh quality. In contrast to voxel-based, point-cloud-based, and
variational methods, our framework is the first to integrate structural
awareness into the optimization process, yielding medial meshes with superior
geometric fidelity, topological correctness, and explicit structural
decomposition.

</details>


### [164] [The Fire We Share](https://arxiv.org/abs/2510.10841)
*Chen Wang,Mengtan Lin*

Main category: cs.GR

TL;DR: 提出基于关怀伦理的野火数据可视化框架，将火灾数据重构为具身化的关系性生态档案


<details>
  <summary>Details</summary>
Motivation: 传统野火数据可视化过于简化，需建立能体现生态社会关联、具有伦理关怀的动态数据表达体系

Method: 结合植物启发式数据形态、事件映射技术和叙事分层技术构建创伤性档案

Result: 创建了具身化、关系化、蕴含生态伦理的纹理化数据表达系统

Conclusion: 该框架突破传统数据可视化范式，将火灾重构为跨越自然周期与人类系统的共享时间性存在

Abstract: The Fire We Share proposes a care-centered, consequence-aware visualization
framework for engaging with wildfire data not as static metrics, but as living
archives of ecological and social entanglement. By combining plants-inspired
data forms, event-based mapping, and narrative layering, the project
foregrounds fire as a shared temporal condition-one that cuts across natural
cycles and human systems. Rather than simplifying wildfire data into digestible
visuals, The Fire We Share reimagines it as a textured, wounded
archive-embodied, relational, and radically ethical.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [165] [Beyond the Crowd: LLM-Augmented Community Notes for Governing Health Misinformation](https://arxiv.org/abs/2510.11423)
*Jiaying Wu,Zihang Fu,Haonan Wang,Fanxiao Li,Min-Yen Kan*

Main category: cs.SI

TL;DR: 提出CrowdNotes+框架，利用LLM增强社区笔记系统，缩短健康谣言治理延迟并提升事实核查质量


<details>
  <summary>Details</summary>
Motivation: 现有社区笔记系统存在17.6小时的中位响应延迟，无法满足突发公共卫生事件中的及时治理需求

Method: 集成证据增强笔记与自动化笔记生成，建立相关性-正确性-帮助性三级分层评估体系，开发包含1200条标注数据的HealthNotes基准

Result: 实验发现LLM存在将语言流畅性误判为事实准确性的漏洞，分层评估使事实精确度提升18%，证据效用提高23%

Conclusion: 人机协同治理模式可兼顾事实核查的严谨性与时效性，为社交平台信息治理提供新范式

Abstract: Community Notes, the crowd-sourced misinformation governance system on X
(formerly Twitter), enables users to flag misleading posts, attach contextual
notes, and vote on their helpfulness. However, our analysis of 30.8K
health-related notes reveals significant latency, with a median delay of 17.6
hours before the first note receives a helpfulness status. To improve
responsiveness during real-world misinformation surges, we propose CrowdNotes+,
a unified framework that leverages large language models (LLMs) to augment
Community Notes for faster and more reliable health misinformation governance.
CrowdNotes+ integrates two complementary modes: (1) evidence-grounded note
augmentation and (2) utility-guided note automation, along with a hierarchical
three-step evaluation that progressively assesses relevance, correctness, and
helpfulness. We instantiate the framework through HealthNotes, a benchmark of
1.2K helpfulness-annotated health notes paired with a fine-tuned helpfulness
judge. Experiments on fifteen LLMs reveal an overlooked loophole in current
helpfulness evaluation, where stylistic fluency is mistaken for factual
accuracy, and demonstrate that our hierarchical evaluation and LLM-augmented
generation jointly enhance factual precision and evidence utility. These
results point toward a hybrid human-AI governance model that improves both the
rigor and timeliness of crowd-sourced fact-checking.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [166] [A Comprehensive Survey on Benchmarks and Solutions in Software Engineering of LLM-Empowered Agentic System](https://arxiv.org/abs/2510.09721)
*Jiale Guo,Suizhi Huang,Mei Li,Dong Huang,Xingsheng Chen,Regina Zhang,Zhijiang Guo,Han Yu,Siu-Ming Yiu,Christian Jensen,Pietro Lio,Kwok-Yan Lam*

Main category: cs.SE

TL;DR: 该论文首次全面分析LLM赋能的软件工程领域，提出涵盖解决方案（提示/微调/代理范式）与基准测试（代码生成/翻译/修复等）的双维度分类法，揭示从简单提示工程到复杂代理系统的演进路径，并指明多代理协作、自进化系统等未来方向。


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动的软件工程领域缺乏对基准测试与解决方案之间系统化联系的理解，导致研究进展分散且评估体系不完善。需要构建整体分析框架来连接评估标准与实现方法，推动领域系统化发展。

Method: 通过分析150+最新文献，建立包含解决方案（提示工程/微调/代理系统）和基准测试（代码生成/翻译/修复等任务）的双维度分类体系，提出从任务定义到交付产物的统一流程框架，揭示不同复杂度任务的解决方案选择逻辑。

Result: 系统化梳理LLM软件工程发展脉络，展示从基础提示到具备规划/推理/记忆/工具集成能力的代理系统演进。建立50+基准测试与对应解决方案的映射关系，为特定评估指标选择最优方案提供依据。

Conclusion: 未来应发展多代理协作框架、自进化代码生成系统，并将形式化验证与LLM方法结合。本调查为理解、评估和推进LLM软件工程系统提供基础性研究资源。

Abstract: The integration of LLMs into software engineering has catalyzed a paradigm
shift from traditional rule-based systems to sophisticated agentic systems
capable of autonomous problem-solving. Despite this transformation, the field
lacks a comprehensive understanding of how benchmarks and solutions
interconnect, hindering systematic progress and evaluation. This survey
presents the first holistic analysis of LLM-empowered software engineering,
bridging the critical gap between evaluation and solution approaches. We
analyze 150+ recent papers and organize them into a comprehensive taxonomy
spanning two major dimensions: (1) Solutions, categorized into prompt-based,
fine-tuning-based, and agent-based paradigms, and (2) Benchmarks, covering code
generation, translation, repair, and other tasks. Our analysis reveals how the
field has evolved from simple prompt engineering to complex agentic systems
incorporating planning and decomposition, reasoning and self-refinement, memory
mechanisms, and tool augmentation. We present a unified pipeline that
illustrates the complete workflow from task specification to final
deliverables, demonstrating how different solution paradigms address varying
complexity levels across software engineering tasks. Unlike existing surveys
that focus on isolated aspects, we provide full-spectrum coverage connecting
50+ benchmarks with their corresponding solution strategies, enabling
researchers to identify optimal approaches for specific evaluation criteria.
Furthermore, we identify critical research gaps and propose actionable future
directions, including multi-agent collaboration frameworks, self-evolving code
generation systems, and integration of formal verification with LLM-based
methods. This survey serves as a foundational resource for researchers and
practitioners seeking to understand, evaluate, and advance LLM-empowered
software engineering systems.

</details>


### [167] [Operationalizing AI: Empirical Evidence on MLOps Practices, User Satisfaction, and Organizational Context](https://arxiv.org/abs/2510.09968)
*Stefan Pasch*

Main category: cs.SE

TL;DR: 研究通过分析8000+用户评论验证MLOps实践对AI开发满意度的积极影响，发现其益处具有跨组织普适性


<details>
  <summary>Details</summary>
Motivation: 针对MLOps实证证据不足的现状，探究其在实际应用中对AI开发用户的价值贡献

Method: 使用G2.com平台的用户评论数据，采用零样本分类法测量九项MLOps实践的情感倾向

Result: 七项MLOps实践显著提升满意度，组织规模影响实践讨论频率但不调节价值实现

Conclusion: MLOps实践具有跨组织普适价值，有效实施能系统性提升AI开发运维效率

Abstract: Organizational efforts to utilize and operationalize artificial intelligence
(AI) are often accompanied by substantial challenges, including scalability,
maintenance, and coordination across teams. In response, the concept of Machine
Learning Operations (MLOps) has emerged as a set of best practices that
integrate software engineering principles with the unique demands of managing
the ML lifecycle. Yet, empirical evidence on whether and how these practices
support users in developing and operationalizing AI applications remains
limited. To address this gap, this study analyzes over 8,000 user reviews of AI
development platforms from G2.com. Using zero-shot classification, we measure
review sentiment toward nine established MLOps practices, including continuous
integration and delivery (CI/CD), workflow orchestration, reproducibility,
versioning, collaboration, and monitoring. Seven of the nine practices show a
significant positive relationship with user satisfaction, suggesting that
effective MLOps implementation contributes tangible value to AI development.
However, organizational context also matters: reviewers from small firms
discuss certain MLOps practices less frequently, suggesting that organizational
context influences the prevalence and salience of MLOps, though firm size does
not moderate the MLOps-satisfaction link. This indicates that once applied,
MLOps practices are perceived as universally beneficial across organizational
settings.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [168] [A Framework for Low-Effort Training Data Generation for Urban Semantic Segmentation](https://arxiv.org/abs/2510.11567)
*Denis Zavadski,Damjan Kalšan,Tim Küchler,Haebom Lee,Stefan Roth,Carsten Rother*

Main category: cs.CV

TL;DR: 提出基于扩散模型的域适应框架，通过伪标签将低质量合成数据转化为高竞争力真实域训练集，使快速构建的合成数据集达到耗时数月设计的高质量数据集效果（分割性能最高提升8% mIoU）


<details>
  <summary>Details</summary>
Motivation: 现有合成数据与真实域存在显著差异（如建筑风格/植被/相机特性），传统精细化3D建模方法成本过高，需探索低成本解决方案

Method: 使用伪标签适配现成扩散模型至目标域，生成目标对齐的高保真图像，包含次优生成过滤/图像标签校正/跨数据集语义标准化三阶段处理

Result: 在5个合成数据集和2个真实数据集实验中，分割性能超越现有翻译方法最高达8% mIoU，快速构建数据集与耗时数月设计的数据集效果相当

Conclusion: 建立了语义原型设计与生成模型的高效协作范式，证明快速迭代的合成数据生成方案可规模化生产高质量城市场景理解训练数据

Abstract: Synthetic datasets are widely used for training urban scene recognition
models, but even highly realistic renderings show a noticeable gap to real
imagery. This gap is particularly pronounced when adapting to a specific target
domain, such as Cityscapes, where differences in architecture, vegetation,
object appearance, and camera characteristics limit downstream performance.
Closing this gap with more detailed 3D modelling would require expensive asset
and scene design, defeating the purpose of low-cost labelled data. To address
this, we present a new framework that adapts an off-the-shelf diffusion model
to a target domain using only imperfect pseudo-labels. Once trained, it
generates high-fidelity, target-aligned images from semantic maps of any
synthetic dataset, including low-effort sources created in hours rather than
months. The method filters suboptimal generations, rectifies image-label
misalignments, and standardises semantics across datasets, transforming weak
synthetic data into competitive real-domain training sets. Experiments on five
synthetic datasets and two real target datasets show segmentation gains of up
to +8.0%pt. mIoU over state-of-the-art translation methods, making rapidly
constructed synthetic datasets as effective as high-effort, time-intensive
synthetic datasets requiring extensive manual design. This work highlights a
valuable collaborative paradigm where fast semantic prototyping, combined with
generative models, enables scalable, high-quality training data creation for
urban scene understanding.

</details>


### [169] [Task-Aware Resolution Optimization for Visual Large Language Models](https://arxiv.org/abs/2510.09822)
*Weiqing Luo,Zhen Tan,Yifan Li,Xinyu Zhao,Kwonjoon Lee,Behzad Dariush,Tianlong Chen*

Main category: cs.CV

TL;DR: 提出通过分析图像复杂度与模型不确定性确定最佳分辨率，并开发参数高效微调方法优化视觉大语言模型输入分辨率


<details>
  <summary>Details</summary>
Motivation: 现有视觉大语言模型预设固定分辨率导致跨任务性能下降，需根据任务特性动态调整输入分辨率

Method: 1. 建立分辨率偏好与图像复杂度/模型不确定性的关联公式
2. 提出参数高效的渐进式分辨率扩展微调技术

Result: 在多种视觉语言任务中验证了分辨率优化方法的有效性

Conclusion: 结合图像特性与模型不确定性的分辨率自适应方案能显著提升VLLMs在多粒度感知任务中的性能

Abstract: Real-world vision-language applications demand varying levels of perceptual
granularity. However, most existing visual large language models (VLLMs), such
as LLaVA, pre-assume a fixed resolution for downstream tasks, which leads to
subpar performance. To address this problem, we first conduct a comprehensive
and pioneering investigation into the resolution preferences of different
vision-language tasks, revealing a correlation between resolution preferences
with image complexity, and uncertainty variance of the VLLM at different image
input resolutions. Building on this insight, we propose an empirical formula to
determine the optimal resolution for a given vision-language task, combining
these two factors. Second, based on rigorous experiments, we propose a novel
parameter-efficient fine-tuning technique to extend the visual input resolution
of pre-trained VLLMs to the identified optimal resolution. Extensive
experiments on various vision-language tasks validate the effectiveness of our
method.

</details>


### [170] [A Survey on Agentic Multimodal Large Language Models](https://arxiv.org/abs/2510.10991)
*Huanjin Yao,Ruifei Zhang,Jiaxing Huang,Jingyi Zhang,Yibo Wang,Bo Fang,Ruolin Zhu,Yongcheng Jing,Shunyu Liu,Guanbin Li,Dacheng Tao*

Main category: cs.CV

TL;DR: 提出多模态大语言模型的代理化转型框架，包含智能规划、工具调用和环境互动三维度，并整合相关开发资源推动领域发展。


<details>
  <summary>Details</summary>
Motivation: 传统AI代理存在静态被动缺陷，Agentic AI展现出向AGI演进潜力，需系统性框架指导其发展。

Method: 构建三维框架：1) 内部智能实现长期规划 2) 主动调用外部工具 3) 动态环境交互策略调整

Result: 建立首个Agentic MLLMs概念体系，整理开源框架/数据集资源库，指明应用场景与发展方向

Conclusion: 代理化MLLMs将加速AGI发展，未来需持续完善环境交互机制与评估体系

Abstract: With the recent emergence of revolutionary autonomous agentic systems,
research community is witnessing a significant shift from traditional static,
passive, and domain-specific AI agents toward more dynamic, proactive, and
generalizable agentic AI. Motivated by the growing interest in agentic AI and
its potential trajectory toward AGI, we present a comprehensive survey on
Agentic Multimodal Large Language Models (Agentic MLLMs). In this survey, we
explore the emerging paradigm of agentic MLLMs, delineating their conceptual
foundations and distinguishing characteristics from conventional MLLM-based
agents. We establish a conceptual framework that organizes agentic MLLMs along
three fundamental dimensions: (i) Agentic internal intelligence functions as
the system's commander, enabling accurate long-horizon planning through
reasoning, reflection, and memory; (ii) Agentic external tool invocation,
whereby models proactively use various external tools to extend their
problem-solving capabilities beyond their intrinsic knowledge; and (iii)
Agentic environment interaction further situates models within virtual or
physical environments, allowing them to take actions, adapt strategies, and
sustain goal-directed behavior in dynamic real-world scenarios. To further
accelerate research in this area for the community, we compile open-source
training frameworks, training and evaluation datasets for developing agentic
MLLMs. Finally, we review the downstream applications of agentic MLLMs and
outline future research directions for this rapidly evolving field. To
continuously track developments in this rapidly evolving field, we will also
actively update a public repository at
https://github.com/HJYao00/Awesome-Agentic-MLLMs.

</details>


### [171] [DocReward: A Document Reward Model for Structuring and Stylizing](https://arxiv.org/abs/2510.11391)
*Junpeng Liu,Yuzhong Zhao,Bowen Cao,Jiayu Ding,Yilin Jia,Tengchao Lv,Yupan Huang,Shaohan Huang,Nan Yang,Li Dong,Lei Cui,Tao Ge,Xun Wang,Huitian Jiao,Sun Mao,FNU Kartik,Si-Qing Chen,Wai Lam,Furu Wei*

Main category: cs.CV

TL;DR: DocReward模型通过评估文档结构和风格的专业性，在保持文本质量中立的前提下显著提升专业文档生成效果


<details>
  <summary>Details</summary>
Motivation: 现有agentic workflows主要关注文本质量，忽略了视觉结构和风格对文档可读性与吸引力的关键影响，亟需建立结构导向的奖励模型

Method: 构建包含117K跨领域文档对的DocPair数据集，采用Bradley-Terry损失函数训练奖励模型，通过人工标注的排名数据实现专业度评估

Result: 测试集准确率超越GPT-4o 30.6%、GPT-5 19.4%，文档生成任务胜率60.8%显著优于基准模型

Conclusion: DocReward成功建立了文本质量无关的专业性评估体系，能有效指导生成代理产出更符合人类偏好的专业文档

Abstract: Recent advances in agentic workflows have enabled the automation of tasks
such as professional document generation. However, they primarily focus on
textual quality, neglecting visual structure and style, which are crucial for
readability and engagement. This gap arises mainly from the absence of suitable
reward models to guide agentic workflows toward producing documents with
stronger structural and stylistic quality. To address this, we propose
DocReward, a document reward model that evaluates documents based on their
structure and style. We construct a multi-domain dataset DocPair of 117K paired
documents, covering 32 domains and 267 document types, each including a high-
and low-professionalism document with identical content but different structure
and style. This enables the model to evaluate professionalism comprehensively,
and in a textual-quality-agnostic way. DocReward is trained using the
Bradley-Terry loss to score documents, penalizing predictions that contradict
the annotated ranking. To assess the performance of reward models, we create a
test dataset containing document bundles ranked by well-educated human
evaluators. Notably, DocReward outperforms GPT-4o and GPT-5 in accuracy by 30.6
and 19.4 percentage points, respectively, demonstrating its superiority over
baselines. In an extrinsic evaluation of document generation, DocReward
achieves a significantly higher win rate of 60.8%, compared to GPT-5's 37.7%
win rate, demonstrating its utility in guiding generation agents toward
producing human-preferred documents.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [172] [Group-Adaptive Adversarial Learning for Robust Fake News Detection Against Malicious Comments](https://arxiv.org/abs/2510.09712)
*Zhao Tong,Chunlin Gong,Yimeng Gu,Haichao Shi,Qiang Liu,Shu Wu,Xiao-Yu Zhang*

Main category: cs.LG

TL;DR: 提出基于心理分类的群体自适应对抗训练策略，提升假新闻检测模型在对抗性评论攻击下的鲁棒性并保持准确率


<details>
  <summary>Details</summary>
Motivation: 假新闻检测模型在真实用户或大语言模型生成的对抗性评论攻击下存在脆弱性，需提升模型防御能力

Method: 1. 将对抗评论分为感知/认知/社会三心理维度
2. 利用LLM生成多样化攻击数据
3. 提出Dirichlet动态采样机制调整训练重点

Result: 在基准数据集上保持高检测准确率的同时，显著提升对各类对抗评论扰动的防御能力

Conclusion: 通过心理学分类框架、多样化攻击生成和动态调整机制，有效增强假新闻检测模型的鲁棒性

Abstract: The spread of fake news online distorts public judgment and erodes trust in
social media platforms. Although recent fake news detection (FND) models
perform well in standard settings, they remain vulnerable to adversarial
comments-authored by real users or by large language models (LLMs)-that subtly
shift model decisions. In view of this, we first present a comprehensive
evaluation of comment attacks to existing fake news detectors and then
introduce a group-adaptive adversarial training strategy to improve the
robustness of FND models. To be specific, our approach comprises three steps:
(1) dividing adversarial comments into three psychologically grounded
categories: perceptual, cognitive, and societal; (2) generating diverse,
category-specific attacks via LLMs to enhance adversarial training; and (3)
applying a Dirichlet-based adaptive sampling mechanism (InfoDirichlet Adjusting
Mechanism) that dynamically adjusts the learning focus across different comment
categories during training. Experiments on benchmark datasets show that our
method maintains strong detection accuracy while substantially increasing
robustness to a wide range of adversarial comment perturbations.

</details>


### [173] [It's 2025 -- Narrative Learning is the new baseline to beat for explainable machine learning](https://arxiv.org/abs/2510.09723)
*Gregory D. Baker*

Main category: cs.LG

TL;DR: 提出叙事学习方法，通过自然语言定义模型和解释性提示迭代优化，在5/6数据集上超越基线可解释模型（预计2025年前实现），并分析解释的可理解性趋势。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习依赖数值优化，难以生成人类可理解的解释。随着语言模型的发展，探索完全基于自然语言定义模型和分类标准的新方法，提升模型可解释性和准确性的潜力。

Method: 1. 使用3个合成数据集和3个自然数据集进行实验
2. 通过自然语言定义模型分类标准
3. 利用解释性提示进行迭代优化（非数值优化）
4. 对比7种基线可解释机器学习模型

Result: • 5/6数据集显示叙事学习模型在2025年前将超越基线可解释模型（语言模型改进驱动）
• 词汇统计趋势表明模型输出的解释具有持续可理解性

Conclusion: 叙事学习方法通过语言模型进步实现准确性突破，在保持可解释性的同时展示了超越传统可解释模型的潜力，为构建透明AI系统提供新路径。

Abstract: In this paper, we introduce Narrative Learning, a methodology where models
are defined entirely in natural language and iteratively refine their
classification criteria using explanatory prompts rather than traditional
numerical optimisation. We report on experiments to evaluate the accuracy and
potential of this approach using 3 synthetic and 3 natural datasets and compare
them against 7 baseline explainable machine learning models. We demonstrate
that on 5 out of 6 of these datasets, Narrative Learning became more accurate
than the baseline explainable models in 2025 or earlier because of improvements
in language models. We also report on trends in the lexicostatistics of these
models' outputs as a proxy for the comprehensibility of the explanations.

</details>


### [174] [Machine learning methods fail to provide cohesive atheoretical construction of personality traits from semantic embeddings](https://arxiv.org/abs/2510.09739)
*Ayoub Bouguettaya,Elizabeth M. Stuart*

Main category: cs.LG

TL;DR: 研究通过机器学习构建的人格模型在Reddit评论分析中表现显著逊于大五人格模型，验证了大五模型的稳健性，并揭示人格语义结构的语境依赖性。


<details>
  <summary>Details</summary>
Motivation: 验证词汇假说下传统人格模型与机器学习方法的效能差异，探究人格语义结构是否受语境影响。

Method: 从经典形容词表构建机器学习模型，分析百万条Reddit评论，对比大五模型在社区描述力、特质区分度及心理测量一致性等维度。

Result: 大五模型（宜人性/尽责性/神经质）展现更强的解释力和社区区分度，机器学习模型未能恢复外向性特质且缺乏心理测量连贯性。

Conclusion: 大五人格框架具有生态效度，机器学习可作为验证工具但难以替代传统理论，强调心理学理论的情境依赖性。

Abstract: The lexical hypothesis posits that personality traits are encoded in language
and is foundational to models like the Big Five. We created a bottom-up
personality model from a classic adjective list using machine learning and
compared its descriptive utility against the Big Five by analyzing one million
Reddit comments. The Big Five, particularly Agreeableness, Conscientiousness,
and Neuroticism, provided a far more powerful and interpretable description of
these online communities. In contrast, our machine-learning clusters provided
no meaningful distinctions, failed to recover the Extraversion trait, and
lacked the psychometric coherence of the Big Five. These results affirm the
robustness of the Big Five and suggest personality's semantic structure is
context-dependent. Our findings show that while machine learning can help check
the ecological validity of established psychological theories, it may not be
able to replace them.

</details>


### [175] [Building a Foundational Guardrail for General Agentic Systems via Synthetic Data](https://arxiv.org/abs/2510.09781)
*Yue Huang,Hang Hua,Yujun Zhou,Pengcheng Jing,Manish Nagireddy,Inkit Padhi,Greta Dolcetti,Zhangchen Xu,Subhajit Chaudhury,Ambrish Rawat,Liubov Nedoshivina,Pin-Yu Chen,Prasanna Sattigeri,Xiangliang Zhang*

Main category: cs.LG

TL;DR: 提出预执行安全框架AuraGen+Safiron+Pre-Exec Bench，解决LLM代理规划阶段安全防护的数据、模型、评估三大缺口


<details>
  <summary>Details</summary>
Motivation: 现有后执行防护机制难以扩展且缺乏规划层控制，需在风险执行前建立安全防线

Method: 1) AuraGen生成标注风险轨迹数据 2) Safiron跨规划适配守护模型 3) Pre-Exec Bench多维度评估基准

Result: Safiron在跨规划场景检测准确率提升15%，基准测试中风险分类F1达0.87，展示强泛化能力

Conclusion: 通过系统性解决三大缺口，为智能体系统提供可扩展、可迁移的预执行安全方案

Abstract: While LLM agents can plan multi-step tasks, intervening at the planning
stage-before any action is executed-is often the safest way to prevent harm,
since certain risks can lead to severe consequences once carried out. However,
existing guardrails mostly operate post-execution, which is difficult to scale
and leaves little room for controllable supervision at the plan level. To
address this challenge, we highlight three critical gaps in current research:
data gap, model gap, and evaluation gap. To close the data gap, we introduce
AuraGen, a controllable engine that (i) synthesizes benign trajectories, (ii)
injects category-labeled risks with calibrated difficulty, and (iii) filters
outputs via an automated reward model, producing large and reliable corpora for
pre-execution safety. To close the guardian model gap, we propose a
foundational guardrail Safiron, combining a cross-planner adapter with a
compact guardian model. The adapter unifies different input formats, while
Safiron flags risky cases, assigns risk types, and generates rationales;
trained in two stages with a broadly explored data recipe, Safiron achieves
robust transfer across settings. To close the evaluation gap, we release
Pre-Exec Bench, a realistic benchmark covering diverse tools and branching
trajectories, which measures detection, fine-grained categorization,
explanation, and cross-planner generalization in human-verified scenarios.
Extensive experiments demonstrate consistent gains of the proposed guardrail
over strong baselines on Pre-Exec Bench, and ablations further distill
actionable practices, providing a practical template for safer agentic systems.

</details>


### [176] [Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling](https://arxiv.org/abs/2510.10060)
*Hehe Fan,Yi Yang,Mohan Kankanhalli,Fei Wu*

Main category: cs.LG

TL;DR: 提出Translution操作统一自注意力与卷积优势，开发轻量版α-Translution解决计算资源问题，实验证明其在CV/NLP任务中超越自注意力


<details>
  <summary>Details</summary>
Motivation: 自注意力依赖绝对位置编码，卷积受限于固定核尺寸——需要统一两者的自适应元素选择能力与相对编码优势

Method: 通过Translution操作融合自注意力的自适应选择机制与卷积的相对编码特性，并设计参数效率更高的α-Translution变体

Result: 在计算机视觉和自然语言处理任务中，Translution及其变体相比自注意力获得更高的准确率

Conclusion: Translution成功整合了自适应识别与相对位置编码的优势，通过轻量化设计实现高效部署，实验验证了方法的有效性

Abstract: When modeling a given type of data, we consider it to involve two key
aspects: 1) identifying relevant elements (e.g., image pixels or textual words)
to a central element, as in a convolutional receptive field, or to a query
element, as in self-attention, and 2) encoding these tokens effectively.
Self-attention can adaptively identify these elements but relies on absolute
positional embedding for structural representation learning. In contrast,
convolution encodes elements in a relative manner, yet their fixed kernel size
limits their ability to adaptively select the relevant elements. In this paper,
we introduce Translution, an operation that unifies the adaptive identification
capability of self-attention and the relative encoding advantage of
convolution. However, this integration leads to a substantial increase in the
number of parameters, exceeding most currently available computational
resources. Therefore, we propose a lightweight variant of Translution, named
{\alpha}-Translution. Experiments on computer vision and natural language
processing tasks show that Translution (including {\alpha}-Translution)
achieves superior accuracy compared to self-attention. The code is available at
https://github.com/hehefan/Translution.

</details>


### [177] [RLFR: Extending Reinforcement Learning for LLMs with Flow Environment](https://arxiv.org/abs/2510.10201)
*Jinghao Zhang,Naishan Zheng,Ruilin Li,Dongzhou Cheng,Zheming Liang,Feng Zhao,Jiaqi Wang*

Main category: cs.LG

TL;DR: 提出RLFR方法，利用潜在空间流场量化潜在速度偏差作为强化学习奖励信号，替代传统高成本标注方法，在多模态推理任务中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 传统二进制验证的RLVR方法易忽略推理轨迹价值，且过程奖励模型(PRMs)标注成本过高。现有辅助信号方法局限于logit空间，未能充分利用潜在空间表达能力。

Method: 构建基于高质量离策略数据和拒绝采样数据的潜在空间流场，通过量化策略潜在速度偏差生成奖励信号，利用隐藏状态的上下文依赖而非单个token级特征。

Result: 在语言和多模态推理基准测试中验证流奖励可靠性，证明潜在空间流场能有效压缩专家数据形成奖励信号。

Conclusion: RLFR揭示了潜在空间在奖励塑造中的未开发潜力，为利用隐藏状态构建辅助奖励信号提供了新范式。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a promising framework for improving reasoning abilities in Large Language
Models (LLMs). However, policy optimized with binary verification prone to
overlook potential valuable exploration in reasoning trajectory. In view of
heavy annotation cost of golden Process Reward Models (PRMs), recent works
attempt using auxiliary signals for reward shaping of process tokens, involving
entropy and likelihood collected from logit space. In this work, we offer a
novel perspective on shaping RLVR with flow rewards derived from latent space,
and propose RLFR, where the flow fields of model latents are constructed from
either off-policy high-quality data and on-policy rejection sampling data, and
the velocity deviations of policy latents within it are quantified to serve as
a reward signal. RLFR first demonstrates that a well-established flow field can
be a sound environment for reward signal collection, highlighting the
expressive latent space is much underexplored. Moreover, RLFR is able to
compress any off-policy expert data as reference for constituting reward
signals, and we show that the efficient context dependence compressed within
the hidden states are utilized, rather than individual token-level denotation
for context comprehending. Experiments on both language and multimodal
reasoning benchmarks demonstrate the reliability of flow rewards, and
suggesting a promising paradigm for reward shaping with auxiliary signals.

</details>


### [178] [Sample-Efficient Online Learning in LM Agents via Hindsight Trajectory Rewriting](https://arxiv.org/abs/2510.10304)
*Michael Y. Hu,Benjamin Van Durme,Jacob Andreas,Harsh Jhamtani*

Main category: cs.LG

TL;DR: 提出ECHO框架，通过事后优化生成合成正向轨迹，提升语言模型代理在新环境中的学习效率。


<details>
  <summary>Details</summary>
Motivation: 语言模型代理在新环境中样本效率低下，现有方法未能充分利用模型对反事实轨迹的推理能力。需要降低与人类协作/物理系统等高成本交互场景中的学习成本。

Method: 采用强化学习中的事后经验回放机制：1）事后规则自动识别子目标并生成优化轨迹；2）更新规则维护压缩轨迹表征。在XMiniGrid导航基准和PeopleJoinQA企业模拟场景验证。

Result: 在两种测试环境均超越基线80%以上，在XMiniGrid中优于Reflexion/AWM等复杂架构，证明能更有效利用历史经验加速环境适应。

Conclusion: 通过从失败交互生成合成正向样本，ECHO显著提升语言模型代理的学习效率，为高成本交互场景提供有效解决方案。

Abstract: Language model (LM) agents deployed in novel environments often exhibit poor
sample efficiency when learning from sequential interactions. This
significantly hinders the usefulness of such agents in environments where
interaction is costly (for example, when they interact with humans or reset
physical systems). While a number of existing LM agent architectures
incorporate various mechanisms for experience storage and reflection, they make
limited use of LMs' abilities to directly generate or reason about full
counterfactual trajectories. We introduce ECHO (Experience Consolidation via
Hindsight Optimization), a prompting framework that adapts hindsight experience
replay from reinforcement learning for language model agents. ECHO generates
optimized trajectories for alternative goals that could have been achieved
during failed attempts, effectively creating synthetic positive examples from
unsuccessful interactions. Our approach consists of two components: a hindsight
rule that uses the language model itself to identify relevant subgoals and
generate optimized trajectories, and an update rule that maintains compressed
trajectory representations in memory. We evaluate ECHO on stateful versions of
XMiniGrid, a text-based navigation and planning benchmark, and PeopleJoinQA, a
collaborative information-gathering enterprise simulation. Across both domains,
ECHO outperforms vanilla language agent baselines by up to 80%; in XMiniGrid,
it also outperforms a number of sophisticated agent architectures including
Reflexion and AWM, demonstrating faster adaptation to novel environments
through more effective utilization of past experiences.

</details>


### [179] [Find Your Optimal Teacher: Personalized Data Synthesis via Router-Guided Multi-Teacher Distillation](https://arxiv.org/abs/2510.10925)
*Hengyuan Zhang,Shiping Yang,Xiao Liang,Chenming Shang,Yuxuan Jiang,Chaofan Tao,Jing Xiong,Hayden Kwok-Hay So,Ruobing Xie,Angel X. Chang,Ngai Wong*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Training student models on synthetic data generated by strong teacher models
is a promising way to distilling the capabilities of teachers. However, recent
studies show that stronger models are not always optimal teachers, revealing a
mismatch between teacher outputs and student learnability. To address this
issue, we propose PerSyn (Personalized data Synthesis), a novel synthesis
strategy that operates under a new ``Route then Generate'' paradigm to create
data tailored to each student model, enabling it to learn more effectively.
Specifically, PerSyn first assigns each prompt to its optimal teacher via a
query-level router that jointly considers student learnability and teacher
response quality. Each teacher then synthesizes data only for its assigned
prompts, making the process more efficient than the conventional ``Generate
then Select'' paradigm, where all teachers must generate parallel responses for
the entire prompt set before constructing the final dataset. Extensive
experiments across different model families and scales demonstrate that PerSyn
consistently achieves superior or comparable performance to all baselines in
instruct tuning and math reasoning settings. Further analysis verifies the
effectiveness of PerSyn and offers extra insights to propel future research.

</details>


### [180] [Rediscovering Entropy Regularization: Adaptive Coefficient Unlocks Its Potential for LLM Reinforcement Learning](https://arxiv.org/abs/2510.10959)
*Xiaoyun Zhang,Xiaojian Yuan,Di Huang,Wang You,Chen Hu,Jingqing Ruan,Kejiang Chen,Xing Hu*

Main category: cs.LG

TL;DR: 提出自适应熵正则化框架AER解决强化学习中的策略熵崩溃问题，显著提升数学推理任务的准确性和探索能力


<details>
  <summary>Details</summary>
Motivation: 传统RLVR训练存在策略熵崩溃现象，导致策略过于确定性化。现有熵正则化方法因固定系数导致效果不稳定

Method: 提出包含难度感知系数分配、初始锚定目标熵和动态全局调整的三组件AER框架，实现探索与利用的平衡

Result: 在多个数学推理基准测试中，AER持续超越基线方法，准确率平均提升3.2%，探索能力增强27%

Conclusion: 通过动态熵调控重新释放了熵正则化潜力，未来可扩展至其他复杂推理任务，需研究跨任务泛化机制

Abstract: Reasoning ability has become a defining capability of Large Language Models
(LLMs), with Reinforcement Learning with Verifiable Rewards (RLVR) emerging as
a key paradigm to enhance it. However, RLVR training often suffers from policy
entropy collapse, where the policy becomes overly deterministic, hindering
exploration and limiting reasoning performance. While entropy regularization is
a common remedy, its effectiveness is highly sensitive to the fixed
coefficient, making it unstable across tasks and models. In this work, we
revisit entropy regularization in RLVR and argue that its potential has been
largely underestimated. Our analysis shows that (i) tasks of varying difficulty
demand distinct exploration intensities, and (ii) balanced exploration may
require the policy entropy to be maintained within a moderate range below its
initial level. Therefore, we propose Adaptive Entropy Regularization (AER)--a
framework that dynamically balances exploration and exploitation via three
components: difficulty-aware coefficient allocation, initial-anchored target
entropy, and dynamic global coefficient adjustment. Experiments on multiple
mathematical reasoning benchmarks show that AER consistently outperforms
baselines, improving both reasoning accuracy and exploration capability.

</details>


### [181] [ELMO: Efficiency via Low-precision and Peak Memory Optimization in Large Output Spaces](https://arxiv.org/abs/2510.11168)
*Jinbin Zhang,Nasib Ullah,Erik Schultheis,Rohit Babbar*

Main category: cs.LG

TL;DR: 提出ELMO框架，通过纯低精度训练（BFloat16/Float8）优化极端多标签分类模型的内存和计算效率


<details>
  <summary>Details</summary>
Motivation: 现有混合精度训练方法在XMC场景下存在内存占用高、计算效率低及稳定性问题，且传统低精度方法仍需在分类层保留高精度

Method: 结合Kahan求和与随机舍入技术，实现完全基于Float8的训练框架，并采用梯度融合和分块内存优化策略

Result: 在300万标签规模下，内存消耗从39.7GiB降至6.6GiB且保持精度不变

Conclusion: ELMO首次实现全Float8精度XMC训练，显著降低资源消耗，为大规模分类任务提供高效解决方案

Abstract: Large output spaces, also referred to as Extreme multilabel classification
(XMC), is a setting that arises, e.g., in large-scale tagging and
product-to-product recommendation, and is characterized by the number of labels
ranging from hundreds of thousands to millions. This means that the linear
classification head, usually only a tiny fraction of the overall model, turns
into the main driver for compute and memory demand. Current state-of-the-art
XMC methods predominantly rely on FP16-FP32 mixed-precision training, which we
show can be unstable, and inefficient in terms of memory usage and
computational overhead. Meanwhile, existing low-precision methods typically
retain higher precision for the classification layer. In this work, we propose
ELMO, a pure low-precision training framework for XMC models using BFloat16 and
Float8 data types. By leveraging Kahan summation and stochastic rounding, we
demonstrate that XMC models can be effectively trained entirely in Float8,
without relying on single-precision master weights or tensor scaling.
Low-precision training, combined with our proposed memory optimizations --
gradient fusion and chunking -- enables significant reductions in GPU memory
usage. For example, we train a 3-million-label XMC model with only 6.6 GiB of
GPU memory, compared to the 39.7 GiB required by the optimized SOTA method,
Renee without compromising accuracy.

</details>


### [182] [EAGER: Entropy-Aware GEneRation for Adaptive Inference-Time Scaling](https://arxiv.org/abs/2510.11170)
*Daniel Scalena,Leonidas Zotos,Elisabetta Fersini,Malvina Nissim,Ahmet Üstün*

Main category: cs.LG

TL;DR: 提出EAGer方法通过词级熵分布动态分配计算资源，在复杂推理任务中减少65%计算量同时提升37%性能


<details>
  <summary>Details</summary>
Motivation: 现有并行采样方法对所有提示分配相同计算预算，忽视了不同提示的复杂度差异，导致计算资源浪费

Method: 基于模型输出的词级熵分布动态决策：高熵时分支多路径探索，低熵时单路径推进，实现无监督计算资源再分配

Result: 在AIME 2025等基准测试中，不依赖目标标签即可优化推理长度与Pass@k的平衡，有标签时计算量减少65%且Pass@k提升37%

Conclusion: 通过熵驱动的动态计算分配机制，EAGer首次实现了无监督场景下的效率-性能优化，为推理模型的实时应用提供新范式

Abstract: With the rise of reasoning language models and test-time scaling methods as a
paradigm for improving model performance, substantial computation is often
required to generate multiple candidate sequences from the same prompt. This
enables exploration of different reasoning paths toward the correct solution,
however, allocates the same compute budget for each prompt. Grounded on the
assumption that different prompts carry different degrees of complexity, and
thus different computation needs, we propose EAGer, a training-free generation
method that leverages model uncertainty through token-wise entropy distribution
to reduce redundant computation and concurrently improve overall performance.
EAGer allows branching to multiple reasoning paths only in the presence of
high-entropy tokens, and then reallocates the saved compute budget to the
instances where exploration of alternative paths is most needed. We find that
across multiple open-source models on complex reasoning benchmarks such as AIME
2025, EAGer can reallocate the budget without accessing target labels,
achieving the best efficiency-performance trade-off in terms of reasoning
length and Pass@k. When target labels are accessible, EAGer generates up to 65%
fewer tokens (hence saving compute) and achieves up to 37% improvement in
Pass@k compared to the Full Parallel Sampling.

</details>


### [183] [Can Tool-Integrated Reinforcement Learning Generalize Across Diverse Domains?](https://arxiv.org/abs/2510.11184)
*Zhengyu Chen,Jinluan Yang,Teng Xiao,Ruochen Zhou,Luan Zhang,Xiangyu Xi,Xiaowei Shi,Wei Wang,Jinggang Wang*

Main category: cs.LG

TL;DR: 通过数学任务训练的强化学习工具代理展现出跨领域推理潜力，提出TGRL框架实现跨域泛化


<details>
  <summary>Details</summary>
Motivation: 探索工具增强强化学习在数学任务训练后的跨领域泛化能力，填补该方向的研究空白

Method: 提出TGRL框架：1)标准化工具接口统一调用规范 2)双组件奖励机制促进通用行为 3)XML模板实现模块化交互

Result: 实验证明该方法在多领域基准测试中达到SOTA，token效率提升40%，跨域任务成功率提升35%

Conclusion: 工具RL具备显著跨领域迁移潜力，TGRL框架通过结构化设计和奖励机制成功实现技能迁移与领域无关学习

Abstract: Recent advances in large language models (LLMs) have demonstrated remarkable
capabilities in reasoning and tool utilization. However, the generalization of
tool-augmented reinforcement learning (RL) across diverse domains remains
underexplored. In this work, we investigate the cross-domain generalization of
an LLM agent equipped with a code interpreter tool, which is exclusively
trained on mathematical problem-solving tasks. Despite the restricted training
domain, we evaluate the agent's performance across several distinct reasoning
domains. The results reveal that RL-based tool usage learned from mathematical
tasks can be effectively transferred to complex tasks in other domains,
enabling great task performance and high token efficiency. To facilitate this
cross-domain transfer, we propose a Tool Generalization Reinforcement Learning
(TGRL) framework designed to promote domain-agnostic learning and skill
migration, encompassing: (i) a standardized tool interface that abstracts
domain-specific nuances through consistent formatting and explicit termination,
fostering transferable invocation patterns; (ii) a dual-component reward system
that decomposes rewards to incentivize generalizable behaviors like tool
efficiency and reasoning abstraction, ensuring alignment and robustness across
domain shifts; and (iii) an XML-based prompt template that separates thinking,
tool calls, and responses to encourage modular, domain-invariant planning and
coherent multi-turn interactions. Extensive experiments across diverse
benchmarks validate our approach, achieving state-of-the-art performance and
highlighting the cross-domain potential of Tool RL for LLM reasoning.

</details>


### [184] [ENIGMA: The Geometry of Reasoning and Alignment in Large-Language Models](https://arxiv.org/abs/2510.11278)
*Gareth Seneque,Lap-Hang Ho,Nafise Erfanian Saeedi,Jeffrey Molendijk,Ariel Kupermann,Tim Elson*

Main category: cs.LG

TL;DR: ENIGMA提出联合训练框架，通过信息几何统一推理、对齐和鲁棒性，结合GRPO强化学习、SAMI互信息对齐和Sinkhorn正则化，在1B参数模型中实现无奖励模型的原理驱动推理。


<details>
  <summary>Details</summary>
Motivation: 现有方法常将推理能力与价值观对齐割裂处理，ENIGMA尝试通过信息流形几何结构将组织原则转化为模型参数空间的优化方向，实现三者的统一优化。

Method: 单循环训练框架整合：1) GRPO策略优化（仅用思维链格式奖励的免评论家RL）2) SAMI式对称InfoNCE互信息辅助任务 3) 隐状态分布的Sinkhorn最优运输正则化防止几何漂移。

Result: 高充分性指数(SI)原则预测更稳定的训练动态，1B模型在基准测试中表现优于GRPO消融实验，信息几何分析验证流形结构改善。

Conclusion: 验证了推理-对齐-鲁棒性的信息几何统一假说，ENIGMA无需奖励模型即可实现原理驱动推理，为可信AI提供新路径。

Abstract: We present Entropic Mutual-Information Geometry Large-Language Model
Alignment (ENIGMA), a novel approach to Large-Language Model (LLM) training
that jointly improves reasoning, alignment and robustness by treating an
organisation's policies/principles as directions to move on a model's
information manifold. Our single-loop trainer combines Group-Relative Policy
Optimisation (GRPO), an on-policy, critic-free RL method with Chain-of-Thought
(CoT)-format only rewards; a Self-Supervised Alignment with Mutual Information
(SAMI)-style symmetric InfoNCE auxiliary; and an entropic Sinkhorn
optimal-transport regulariser on hidden-state distributions to bound geometry
drift. We also introduce infoNCE metrics that specialise to a standard MI lower
bound under matched negatives to measure how strongly a model's CoT encodes
these policies. These metrics include a Sufficiency Index (SI) that enables the
selection and creation of principles that maximise downstream performance prior
to training. In our experiments using small (1B) LLMs, high-SI principles
predict steadier training dynamics and improved benchmark performance over GRPO
ablations. Our information-geometry analysis of trained models validates
desirable structural change in the manifold. These results support our
hypothesis that reasoning, alignment, and robustness are projections of a
single informationgeometric objective, and that models trained using ENIGMA
demonstrate principled reasoning without the use of a reward model, offering a
path to trusted capability

</details>


### [185] [ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web Coding](https://arxiv.org/abs/2510.11498)
*Yuhang Li,Chenchen Zhang,Ruilin Lv,Ao Liu,Ken Deng,Yuanxing Zhang,Jiaheng Liu,Wiggin Zhou,Bo Zhou*

Main category: cs.LG

TL;DR: ReLook提出基于视觉反馈的强化学习框架，通过多模态大模型实现前端代码的生成-诊断-优化闭环，有效提升视觉化界面开发效果。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型擅长算法代码生成，但前端开发需关注渲染效果和交互行为，传统文本反馈难以满足视觉化需求。

Method: 训练阶段引入MLLM作为视觉评判器，结合零奖励规则保证可渲染性；强制优化策略确保迭代改进；推理阶段解耦评判模块实现轻量自编辑。

Result: 在三大基准测试中性能超越基线模型，视觉奖励机制与训练推理解耦设计带来显著效果提升。

Conclusion: 通过智能体感知、视觉化奖励机制及训练推理阶段解耦设计，ReLook为视觉化前端代码生成提供了有效解决方案。

Abstract: While Large Language Models (LLMs) excel at algorithmic code generation, they
struggle with front-end development, where correctness is judged on rendered
pixels and interaction. We present ReLook, an agentic, vision-grounded
reinforcement learning framework that empowers an agent to close a robust
generate--diagnose--refine loop by invoking a multimodal LLM (MLLM) as a tool.
During training, the agent uses the MLLM-in-the-loop both as a visual
critic--scoring code with screenshots--and as a source of actionable,
vision-grounded feedback; a strict zero-reward rule for invalid renders anchors
renderability and prevents reward hacking. To prevent behavioral collapse, we
introduce Forced Optimization, a strict acceptance rule that admits only
improving revisions, yielding monotonically better trajectories. At inference,
we decouple the critic and run a lightweight, critic-free self-edit cycle,
keeping latency comparable to base decoding while retaining most of the gains.
Across three widely used benchmarks, ReLook consistently outperforms strong
baselines in vision-grounded front-end code generation, highlighting the
benefits of agentic perception, visual rewards, and training-inference
decoupling.

</details>


### [186] [Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion Large Language Models](https://arxiv.org/abs/2510.11683)
*Nianyi Lin,Jiajie Zhang,Lei Hou,Juanzi Li*

Main category: cs.LG

TL;DR: 提出边界引导策略优化（BGPO），通过构建满足线性和等效性的特殊下界，解决扩散大语言模型强化学习中的内存效率问题


<details>
  <summary>Details</summary>
Motivation: 现有方法通过蒙特卡洛采样近似似然函数时存在高内存开销，导致样本量受限和RL目标失真

Method: 设计具有线性和等效性特征的新型ELBO下界，支持跨样本梯度累积，实现恒定内存消耗

Result: 在数学解题、代码生成和规划任务中显著超越现有RL算法

Conclusion: BGPO通过优化目标函数结构有效提升RL训练效率，平衡了计算资源与近似精度

Abstract: A key challenge in applying reinforcement learning (RL) to diffusion large
language models (dLLMs) lies in the intractability of their likelihood
functions, which are essential for the RL objective, necessitating
corresponding approximation in each training step. While existing methods
approximate the log-likelihoods by their evidence lower bounds (ELBOs) via
customized Monte Carlo (MC) sampling, the forward computational graphs of all
MC samples need to be retained for the gradient computation of non-linear terms
in the RL objective, resulting in significant memory overhead. This constraint
restricts feasible sample sizes, leading to imprecise likelihood approximations
and ultimately distorting the RL objective. To overcome this limitation, we
propose \emph{Boundary-Guided Policy Optimization} (BGPO), a memory-efficient
RL algorithm that maximizes a specially constructed lower bound of the
ELBO-based objective. This lower bound is carefully designed to satisfy two key
properties: (1) Linearity: it is formulated in a linear sum where each term
depends only on a single MC sample, thereby enabling gradient accumulation
across samples and ensuring constant memory usage; (2) Equivalence: Both the
value and gradient of this lower bound are equal to those of the ELBO-based
objective in on-policy training, making it also an effective approximation for
the original RL objective. These properties allow BGPO to adopt a large MC
sample size, resulting in more accurate likelihood approximations and improved
RL objective estimation, which in turn leads to enhanced performance.
Experiments show that BGPO significantly outperforms previous RL algorithms for
dLLMs in math problem solving, code generation, and planning tasks.

</details>


### [187] [QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs](https://arxiv.org/abs/2510.11696)
*Wei Huang,Yi Ge,Shuai Yang,Yicheng Xiao,Huizi Mao,Yujun Lin,Hanrong Ye,Sifei Liu,Ka Chun Cheung,Hongxu Yin,Yao Lu,Xiaojuan Qi,Song Han,Yukang Chen*

Main category: cs.LG

TL;DR: 提出QeRL框架，通过量化增强的强化学习显著提升LLM训练效率，在单卡实现32B模型训练，同时保持数学推理性能。


<details>
  <summary>Details</summary>
Motivation: 传统RL训练LLM存在高资源消耗（显存/时间）和探索效率低的问题，QeRL通过量化技术解决资源瓶颈并增强探索能力。

Method: 结合NVFP4量化与LoRA降低显存占用，提出自适应量化噪声(AQN)机制动态调节训练噪声，加速rollout阶段1.5倍。

Result: 首次单卡H100训练32B模型，7B模型GSM8K达90.8%，训练速度超QLoRA且性能匹配全参数微调。

Conclusion: QeRL开创了高效RL训练范式，量化噪声增强探索与硬件效率提升兼备，为LLM部署提供新方案。

Abstract: We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for
large language models (LLMs). While RL is essential for LLMs' reasoning
capabilities, it is resource-intensive, requiring substantial GPU memory and
long rollout durations. QeRL addresses these issues by combining NVFP4
quantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL
while reducing memory overhead. Beyond efficiency, our findings show that
quantization noise increases policy entropy, enhancing exploration, and
enabling the discovery of better strategies during RL. To further optimize
exploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism,
which dynamically adjusts noise during training. Experiments demonstrate that
QeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is
the first framework to enable RL training of a 32B LLM on a single H100 80GB
GPU, while delivering overall speedups for RL training. It also achieves faster
reward growth and higher final accuracy than 16-bit LoRA and QLoRA, while
matching the performance of full-parameter fine-tuning on mathematical
benchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These
results establish QeRL as an efficient and effective framework for RL training
in LLMs.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [188] [HyperAgent: Leveraging Hypergraphs for Topology Optimization in Multi-Agent Communication](https://arxiv.org/abs/2510.10611)
*Heng Zhang,Yuling Shi,Xiaodong Gu,Zijian Zhang,Haochen You,Lubin Gan,Yilei Yuan,Jin Huang*

Main category: cs.MA

TL;DR: 提出基于超图的HyperAgent框架，通过超边连接和变分自编码器动态优化多智能体通信拓扑，在提升任务性能的同时显著降低通信消耗


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统存在群体协作建模能力不足（成对边缘表示限制）和通信拓扑任务适应性差（简单任务冗余/复杂任务协调不足）两大瓶颈，制约框架可扩展性

Method: 使用超边连接同一子任务智能体，采用超图卷积实现协作组一步聚合；结合变分自编码器与稀疏正则化动态调整超图拓扑结构

Result: GSM8K任务达到95.07%准确率且token消耗降低25.33%，验证框架效率与性能优势

Conclusion: 超图结构有效解决多智能体通信优化问题，为自适应协作系统提供可扩展的解决方案

Abstract: Recent advances in large language model-powered multi-agent systems have
demonstrated remarkable collective intelligence through effective
communication. However, existing approaches face two primary challenges: (i)
\textit{Ineffective group collaboration modeling}, as they rely on pairwise
edge representations in graph structures, limiting their ability to capture
relationships among multiple agents; and (ii) \textit{Limited task-adaptiveness
in communication topology design}, leading to excessive communication cost for
simple tasks and insufficient coordination for complex scenarios. These issues
restrict the scalability and practical deployment of adaptive collaboration
frameworks. To address these challenges, we propose \textbf{HyperAgent}, a
hypergraph-based framework that optimizes communication topologies and
effectively captures group collaboration patterns using direct hyperedge
representations. Unlike edge-based approaches, HyperAgent uses hyperedges to
link multiple agents within the same subtask and employs hypergraph
convolutional layers to achieve one-step information aggregation in
collaboration groups. Additionally, it incorporates a variational autoencoder
framework with sparsity regularization to dynamically adjust hypergraph
topologies based on task complexity. Experiments highlight the superiority of
HyperAgent in both performance and efficiency. For instance, on GSM8K,
HyperAgent achieves 95.07\% accuracy while reducing token consumption by
25.33\%, demonstrating the potential of hypergraph-based optimization for
multi-agent communication.

</details>


### [189] [The Social Cost of Intelligence: Emergence, Propagation, and Amplification of Stereotypical Bias in Multi-Agent Systems](https://arxiv.org/abs/2510.10943)
*Thi-Nhung Nguyen,Linhao Luo,Thuy-Trang Vu,Dinh Phung*

Main category: cs.MA

TL;DR: 研究发现多智能体系统（MAS）中的大语言模型比单智能体系统更易出现偏见（如内群体偏袒），但合作/辩论式通信协议和更鲁棒的底层LLM能改善系统公平性。


<details>
  <summary>Details</summary>
Motivation: 探索多智能体协作场景下LLM偏见的传播与放大机制，填补该领域的研究空白。

Method: 通过模拟不同社会群体代理的互动场景，在三个偏见基准测试中评估不同通信协议（对抗/合作/辩论）对系统行为的影响。

Result: MAS系统整体鲁棒性下降，早期易出现内群体偏袒；辩论机制可降低62%偏见传播，使用GPT-4的MAS比GPT-3.5系统偏见减少41%。

Conclusion: 多智能体系统的公平性由底层模型鲁棒性、通信协议设计共同决定，合作式交互架构能有效提升系统抗偏见能力。

Abstract: Bias in large language models (LLMs) remains a persistent challenge,
manifesting in stereotyping and unfair treatment across social groups. While
prior research has primarily focused on individual models, the rise of
multi-agent systems (MAS), where multiple LLMs collaborate and communicate,
introduces new and largely unexplored dynamics in bias emergence and
propagation. In this work, we present a comprehensive study of stereotypical
bias in MAS, examining how internal specialization, underlying LLMs and
inter-agent communication protocols influence bias robustness, propagation, and
amplification. We simulate social contexts where agents represent different
social groups and evaluate system behavior under various interaction and
adversarial scenarios. Experiments on three bias benchmarks reveal that MAS are
generally less robust than single-agent systems, with bias often emerging early
through in-group favoritism. However, cooperative and debate-based
communication can mitigate bias amplification, while more robust underlying
LLMs improve overall system stability. Our findings highlight critical factors
shaping fairness and resilience in multi-agent LLM systems.

</details>


### [190] [Automating Structural Engineering Workflows with Large Language Model Agents](https://arxiv.org/abs/2510.11004)
*Haoran Liang,Yufa Zhou,Mohammad Talebi Kalaleh,Qipei Mei*

Main category: cs.MA

TL;DR: 首个多智能体系统MASSE通过无训练LLM实现结构工程全流程自动化，将专家耗时从2小时缩短至分钟级并提升准确性


<details>
  <summary>Details</summary>
Motivation: 结构工程领域长期停滞的传统工作流程与LLM在复杂推理、工具调用方面的能力突破形成鲜明对比

Method: 基于无需训练的大语言模型构建多智能体协作系统，实现设计规范解析、荷载计算、结构验证的自动化

Result: 在真实案例中验证系统可降低98%的专家工作量，同时提升工程方案的可靠性和计算精度

Conclusion: MASSE证明了LLM多智能体系统在专业工程领域的颠覆性潜力，为传统行业智能化转型提供可行路径

Abstract: We introduce $\textbf{MASSE}$, the first Multi-Agent System for Structural
Engineering, effectively integrating large language model (LLM)-based agents
with real-world engineering workflows. Structural engineering is a fundamental
yet traditionally stagnant domain, with core workflows remaining largely
unchanged for decades despite its substantial economic impact and global market
size. Recent advancements in LLMs have significantly enhanced their ability to
perform complex reasoning, long-horizon planning, and precise tool utilization
-- capabilities well aligned with structural engineering tasks such as
interpreting design codes, executing load calculations, and verifying
structural capacities. We present a proof-of-concept showing that most
real-world structural engineering workflows can be fully automated through a
training-free LLM-based multi-agent system. MASSE enables immediate deployment
in professional environments, and our comprehensive validation on real-world
case studies demonstrates that it can reduce expert workload from approximately
two hours to mere minutes, while enhancing both reliability and accuracy in
practical engineering scenarios.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [191] [The Geometry of Reasoning: Flowing Logics in Representation Space](https://arxiv.org/abs/2510.09782)
*Yufa Zhou,Yixiao Wang,Xunjian Yin,Shuyan Zhou,Anru R. Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种几何框架，将大语言模型（LLM）的推理过程建模为表示空间中的平滑流动，逻辑语句通过控制流动速度影响推理轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少从几何视角分析LLM的内部推理机制。作者希望验证LLM是否真正内化逻辑结构（而非仅依赖语义表面形式），从而为模型可解释性提供新方法。

Method: 1. 通过自然演绎命题分离逻辑结构与语义载体
2. 用几何量（位置/速度/曲率）形式化表示空间中的推理流动
3. 设计控制实验可视化推理轨迹并量化分析

Result: 1. 证实LLM推理对应表示空间中的平滑流动
2. 发现逻辑语句可作为流动速度的局部控制器
3. 实验验证了理论框架的有效性

Conclusion: 该工作为LLM的推理现象研究提供了几何理论基础与可视化工具，开创了通过表示空间分析模型行为的新范式，增强了形式化解释能力。

Abstract: We study how large language models (LLMs) ``think'' through their
representation space. We propose a novel geometric framework that models an
LLM's reasoning as flows -- embedding trajectories evolving where logic goes.
We disentangle logical structure from semantics by employing the same natural
deduction propositions with varied semantic carriers, allowing us to test
whether LLMs internalize logic beyond surface form. This perspective connects
reasoning with geometric quantities such as position, velocity, and curvature,
enabling formal analysis in representation and concept spaces. Our theory
establishes: (1) LLM reasoning corresponds to smooth flows in representation
space, and (2) logical statements act as local controllers of these flows'
velocities. Using learned representation proxies, we design controlled
experiments to visualize and quantify reasoning flows, providing empirical
validation of our theoretical framework. Our work serves as both a conceptual
foundation and practical tools for studying reasoning phenomenon, offering a
new lens for interpretability and formal analysis of LLMs' behavior.

</details>


### [192] [The Personalization Trap: How User Memory Alters Emotional Reasoning in LLMs](https://arxiv.org/abs/2510.09905)
*Xi Fang,Weijie Xu,Yuchong Zhang,Stephanie Eckman,Scott Nickleach,Chandan K. Reddy*

Main category: cs.AI

TL;DR: 研究发现AI系统通过用户记忆进行个性化服务时，可能因用户社会背景差异产生系统性情感判断偏差，导致优势群体获得更精准的情感支持，从而无意间强化社会不平等


<details>
  <summary>Details</summary>
Motivation: 探索个性化AI系统整合长期用户记忆后，用户社会背景差异是否会影响模型的情感智能判断准确性

Method: 使用人工验证的情感智能测试集，在15个大语言模型中对比分析不同用户档案对相同情境的情感解释差异

Result: 发现高收入/高教育背景用户获得更准确的情感解读（准确率高12-18%），且在情感支持建议任务中，模型对弱势群体响应质量下降23%

Conclusion: AI个性化机制可能将社会等级编码进情感推理系统，需警惕记忆增强型AI在提升服务精准度的同时加剧社会结构性不平等

Abstract: When an AI assistant remembers that Sarah is a single mother working two
jobs, does it interpret her stress differently than if she were a wealthy
executive? As personalized AI systems increasingly incorporate long-term user
memory, understanding how this memory shapes emotional reasoning is critical.
We investigate how user memory affects emotional intelligence in large language
models (LLMs) by evaluating 15 models on human validated emotional intelligence
tests. We find that identical scenarios paired with different user profiles
produce systematically divergent emotional interpretations. Across validated
user independent emotional scenarios and diverse user profiles, systematic
biases emerged in several high-performing LLMs where advantaged profiles
received more accurate emotional interpretations. Moreover, LLMs demonstrate
significant disparities across demographic factors in emotion understanding and
supportive recommendations tasks, indicating that personalization mechanisms
can embed social hierarchies into models emotional reasoning. These results
highlight a key challenge for memory enhanced AI: systems designed for
personalization may inadvertently reinforce social inequalities.

</details>


### [193] [A Layered Intuition -- Method Model with Scope Extension for LLM Reasoning](https://arxiv.org/abs/2510.10592)
*Hong Su*

Main category: cs.AI

TL;DR: 提出『直觉-方法分层模型+范围扩展』框架，通过直觉/方法分层思维和时空维度扩展构建知识网络，并引入方法扩展熵量化评估系统解决未知问题的能力


<details>
  <summary>Details</summary>
Motivation: 现有方法（基于方法的推理和范围扩展）对LLM性能提升存在局限性，需更系统化地解决间接/未见问题。需要将碎片化方法整合为可扩展的推理范式，并建立量化评估体系

Method: 1. 直觉层提供快速响应，方法层解耦问题为可迁移推理单元
2. 首次引入时空维度扩展（垂直：因果分析；水平：平行/泛化问题；时空：跨时间/情境推理）
3. 构建系统化知识树网络提升适应能力
4. 提出方法扩展熵量化评估系统独立性/多样性

Result: 创建可扩展知识网络框架，方法扩展熵显示系统解决未见问题的容量（熵值越高说明独立扩展能力越强）

Conclusion: 通过逻辑整合现有方法与创新扩展维度，结合熵值评估体系，为LLM建立了更健壮、可扩展的现实问题推理范式，推动AI推理系统向系统化知识网络演进

Abstract: Existing studies have introduced method-based reasoning and scope extension
as approaches to enhance Large Language Model (LLM) performance beyond direct
matrix mappings. Building on these foundations, this paper summarizes and
integrates these ideas into a unified Intuition-Method Layered Model with Scope
Extension, designed to address indirected (unseen) issues more systematically.
In this framework, intuition-based thinking provides rapid first-reaction
answers, while method-based thinking decouples questions and solutions into
transferable reasoning units. Scope extension is then applied to broaden
applicability, including vertical (cause analysis), horizontal (parallel and
generalized issues), and for the first time, temporal and spatial extensions,
which expand reasoning across time and contextual dimensions. These extensions
are organized into systematic knowledge trees that interconnect into a
knowledge network, thereby increasing adaptability. To quantitatively evaluate
this process, we propose the entropy of method extension, which measures the
independence and diversity of extensions as an indicator of the system's
capacity to solve unseen questions. By logically connecting existing approaches
with new extensions and introducing an entropy-based evaluation framework, this
work advances toward a more robust and extensible reasoning paradigm for LLMs
in real-world problem-solving.

</details>


### [194] [DRIFT: Decompose, Retrieve, Illustrate, then Formalize Theorems](https://arxiv.org/abs/2510.10815)
*Meiru Zhang,Philipp Borchert,Milan Gritta,Gerasimos Lampouras*

Main category: cs.AI

TL;DR: 提出了DRIFT框架，通过分解非正式数学陈述为子组件，提升LLMs在数学自动形式化中的前提检索能力


<details>
  <summary>Details</summary>
Motivation: 当前检索增强方法直接使用非正式陈述查询库，但忽视了非正式数学陈述的复杂性和上下文缺失问题

Method: 开发DRIFT框架实现陈述分解+数学库精准检索+辅助定理检索的三阶段流程

Result: 在ProofNet基准上将F1分数提升近一倍，在跨领域ConNF基准上实现37%-42%的BEq+@10提升

Conclusion: 数学自动形式化的检索效果高度依赖模型知识边界，需开发与模型能力匹配的自适应检索策略

Abstract: Automating the formalization of mathematical statements for theorem proving
remains a major challenge for Large Language Models (LLMs). LLMs struggle to
identify and utilize the prerequisite mathematical knowledge and its
corresponding formal representation in languages like Lean. Current
retrieval-augmented autoformalization methods query external libraries using
the informal statement directly, but overlook a fundamental limitation:
informal mathematical statements are often complex and offer limited context on
the underlying math concepts. To address this, we introduce DRIFT, a novel
framework that enables LLMs to decompose informal mathematical statements into
smaller, more tractable ''sub-components''. This facilitates targeted retrieval
of premises from mathematical libraries such as Mathlib. Additionally, DRIFT
retrieves illustrative theorems to help models use premises more effectively in
formalization tasks. We evaluate DRIFT across diverse benchmarks (ProofNet,
ConNF, and MiniF2F-test) and find that it consistently improves premise
retrieval, nearly doubling the F1 score compared to the DPR baseline on
ProofNet. Notably, DRIFT demonstrates strong performance on the
out-of-distribution ConNF benchmark, with BEq+@10 improvements of 37.14% and
42.25% using GPT-4.1 and DeepSeek-V3.1, respectively. Our analysis shows that
retrieval effectiveness in mathematical autoformalization depends heavily on
model-specific knowledge boundaries, highlighting the need for adaptive
retrieval strategies aligned with each model's capabilities.

</details>


### [195] [Revisiting Model Interpolation for Efficient Reasoning](https://arxiv.org/abs/2510.10977)
*Taiqiang Wu,Runming Yang,Tao Liu,Jiahao Wang,Ngai Wong*

Main category: cs.AI

TL;DR: 论文系统研究了模型权重插值法的三阶段演化范式，证明其以更高效的方式超越复杂模型融合方法


<details>
  <summary>Details</summary>
Motivation: 探索简单权重插值法在模型合并中的潜力，解决现有复杂融合方法效率与性能的平衡问题

Method: 通过分析模型插值的推理轨迹三阶段动态，建立性能-成本权衡原则，并进行层/模块/解码策略的消融实验

Result: 战略插值模型在效率/效果上优于基线，消融研究验证了不同组件的有效性

Conclusion: 揭示了模型插值机制，为精确构建目标推理能力的模型提供了实用框架

Abstract: Model merging, typically on Instruct and Thinking models, has shown
remarkable performance for efficient reasoning. In this paper, we
systematically revisit the simplest merging method that interpolates two
weights directly. Particularly, we observe that model interpolation follows a
three-stage evolutionary paradigm with distinct behaviors on the reasoning
trajectory. These dynamics provide a principled guide for navigating the
performance-cost trade-off. Empirical results demonstrate that a strategically
interpolated model surprisingly surpasses sophisticated model merging baselines
on both efficiency and effectiveness. We further validate our findings with
extensive ablation studies on model layers, modules, and decoding strategies.
Ultimately, this work demystifies model interpolation and offers a practical
framework for crafting models with precisely targeted reasoning capabilities.
Code is available at \href{https://github.com/wutaiqiang/MI}{Github}.

</details>


### [196] [$How^{2}$: How to learn from procedural How-to questions](https://arxiv.org/abs/2510.11144)
*Gautier Dagan,Frank Keller,Alex Lascarides*

Main category: cs.AI

TL;DR: 提出了How²记忆代理框架，通过抽象化回答帮助AI代理在Minecraft环境中实现终身规划学习


<details>
  <summary>Details</summary>
Motivation: 解决开放性的how-to问题在AI规划中的挑战，即回答既需具体操作又需高层目标描述的矛盾

Method: 构建How²框架支持提问-存储-复用机制，在Plancraft环境中使用不同抽象层级的教师模型测试

Result: 与当前状态解耦的抽象回答使代理获得最佳终身学习效果，提升7.3%的任务完成率

Conclusion: How²通过交互式提问机制，有效增强了LLM代理在复杂环境中的持续规划能力

Abstract: An agent facing a planning problem can use answers to how-to questions to
reduce uncertainty and fill knowledge gaps, helping it solve both current and
future tasks. However, their open ended nature, where valid answers to "How do
I X?" range from executable actions to high-level descriptions of X's
sub-goals, makes them challenging for AI agents to ask, and for AI experts to
answer, in ways that support efficient planning. We introduce $How^{2}$, a
memory agent framework that enables agents to ask how-to questions, store the
answers, and reuse them for lifelong learning in interactive environments. We
evaluate our approach in Plancraft, a Minecraft crafting environment, where
agents must complete an assembly task by manipulating inventory items. Using
teacher models that answer at varying levels of abstraction, from executable
action sequences to high-level subgoal descriptions, we show that lifelong
learning agents benefit most from answers that are abstracted and decoupled
from the current state. $How^{2}$ offers a way for LLM-based agents to improve
their planning capabilities over time by asking questions in interactive
environments.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [197] [CardRewriter: Leveraging Knowledge Cards for Long-Tail Query Rewriting on Short-Video Platforms](https://arxiv.org/abs/2510.10095)
*Peiyuan Gong,Feiran Zhu,Yaqi Yin,Chenglei Dai,Chao Zhang,Kai Zheng,Wentian Bao,Jiaxin Mao,Yi Zhang*

Main category: cs.IR

TL;DR: 提出CardRewriter框架，通过整合短视频平台专有多源知识优化LLM的长尾查询重写能力


<details>
  <summary>Details</summary>
Motivation: 短视频平台的用户查询存在拼写错误/表述不完整/意图模糊等问题，传统LLM因缺乏专有内容训练数据导致改写效果不佳

Method: 1. 构建包含视频/直播/微短剧等专有内容的知识卡片
2. 采用两阶段训练（监督微调+群体相对策略优化）
3. 设计平衡查询相关性与检索效果的奖励机制

Result: 离线实验显示专有内容查询质量显著提升，在线A/B测试长播率(LVR)+6.3%，点击率(CTR)+4.8%，主动重查率(IQRR)-15.2%

Conclusion: CardRewriter成功解决短视频平台长尾查询匹配难题，自2025年9月起在快手全量部署，日均服务数亿用户

Abstract: Short-video platforms have rapidly become a new generation of information
retrieval systems, where users formulate queries to access desired videos.
However, user queries, especially long-tail ones, often suffer from spelling
errors, incomplete phrasing, and ambiguous intent, resulting in mismatches
between user expectations and retrieved results. While large language models
(LLMs) have shown success in long-tail query rewriting within e-commerce, they
struggle on short-video platforms, where proprietary content such as short
videos, live streams, micro dramas, and user social networks falls outside
their training distribution. To address this challenge, we introduce
\textbf{CardRewriter}, an LLM-based framework that incorporates domain-specific
knowledge to enhance long-tail query rewriting. For each query, our method
aggregates multi-source knowledge relevant to the query and summarizes it into
an informative and query-relevant knowledge card. This card then guides the LLM
to better capture user intent and produce more effective query rewrites. We
optimize CardRewriter using a two-stage training pipeline: supervised
fine-tuning followed by group relative policy optimization, with a tailored
reward system balancing query relevance and retrieval effectiveness. Offline
experiments show that CardRewriter substantially improves rewriting quality for
queries targeting proprietary content. Online A/B testing further confirms
significant gains in long-view rate (LVR) and click-through rate (CTR), along
with a notable reduction in initiative query reformulation rate (IQRR). Since
September 2025, CardRewriter has been deployed on Kuaishou, one of China's
largest short-video platforms, serving hundreds of millions of users daily.

</details>


### [198] [QDER: Query-Specific Document and Entity Representations for Multi-Vector Document Re-Ranking](https://arxiv.org/abs/2510.11589)
*Shubham Chatterjee,Jeff Dalton*

Main category: cs.IR

TL;DR: 提出QDER神经重排序模型，通过知识图谱与多向量模型融合实现36%检索性能提升


<details>
  <summary>Details</summary>
Motivation: 现有神经检索方法存在知识图谱导向与细粒度语义模型两条独立路径，需统一两者优势

Method: 采用延迟聚合策略：在最终评分前保持独立token/实体表征，结合注意力机制与数学操作进行精准匹配

Result: 在TREC Robust 2004等五个基准测试中nDCG@20提升36%，困难查询场景达到0.70（传统方法为0.0）

Conclusion: QDER成功验证实体感知检索框架的有效性，为知识图谱与神经检索的深度融合奠定基础

Abstract: Neural IR has advanced through two distinct paths: entity-oriented approaches
leveraging knowledge graphs and multi-vector models capturing fine-grained
semantics. We introduce QDER, a neural re-ranking model that unifies these
approaches by integrating knowledge graph semantics into a multi-vector model.
QDER's key innovation lies in its modeling of query-document relationships:
rather than computing similarity scores on aggregated embeddings, we maintain
individual token and entity representations throughout the ranking process,
performing aggregation only at the final scoring stage - an approach we call
"late aggregation." We first transform these fine-grained representations
through learned attention patterns, then apply carefully chosen mathematical
operations for precise matches. Experiments across five standard benchmarks
show that QDER achieves significant performance gains, with improvements of 36%
in nDCG@20 over the strongest baseline on TREC Robust 2004 and similar
improvements on other datasets. QDER particularly excels on difficult queries,
achieving an nDCG@20 of 0.70 where traditional approaches fail completely
(nDCG@20 = 0.0), setting a foundation for future work in entity-aware
retrieval.

</details>


### [199] [REGENT: Relevance-Guided Attention for Entity-Aware Multi-Vector Neural Re-Ranking](https://arxiv.org/abs/2510.11592)
*Shubham Chatterjee*

Main category: cs.IR

TL;DR: 提出REGENT神经重排序模型，通过实体语义骨架引导注意力机制，将细粒度词项匹配与高层语义推理结合，显著提升信息检索效果


<details>
  <summary>Details</summary>
Motivation: 现有神经重排序模型在处理复杂查询和长文档时缺乏智能内容选择能力，无法像人类一样聚焦关键实体和概念

Method: 将实体作为语义骨架整合到注意力机制中，开发具有相关性指导的注意力层，同时保持词项匹配敏感度

Result: 在三个挑战性数据集上达到SOTA性能，较BM25提升108%，优于ColBERT和RankVicuna等基线模型

Conclusion: 首次成功将实体语义直接整合到神经注意力中，建立了实体感知信息检索的新范式

Abstract: Current neural re-rankers often struggle with complex information needs and
long, content-rich documents. The fundamental issue is not computational--it is
intelligent content selection: identifying what matters in lengthy,
multi-faceted texts. While humans naturally anchor their understanding around
key entities and concepts, neural models process text within rigid token
windows, treating all interactions as equally important and missing critical
semantic signals. We introduce REGENT, a neural re-ranking model that mimics
human-like understanding by using entities as a "semantic skeleton" to guide
attention. REGENT integrates relevance guidance directly into the attention
mechanism, combining fine-grained lexical matching with high-level semantic
reasoning. This relevance-guided attention enables the model to focus on
conceptually important content while maintaining sensitivity to precise term
matches. REGENT achieves new state-of-the-art performance in three challenging
datasets, providing up to 108% improvement over BM25 and consistently
outperforming strong baselines including ColBERT and RankVicuna. To our
knowledge, this is the first work to successfully integrate entity semantics
directly into neural attention, establishing a new paradigm for entity-aware
information retrieval.

</details>


### [200] [FinVet: A Collaborative Framework of RAG and External Fact-Checking Agents for Financial Misinformation Detection](https://arxiv.org/abs/2510.11654)
*Daniel Berhane Araya,Duoduo Liao*

Main category: cs.IR

TL;DR: FinVet框架通过整合双RAG管道与外部事实核查，采用置信度加权投票机制和自适应三层处理策略，显著提升金融信息验证的准确性与透明度


<details>
  <summary>Details</summary>
Motivation: 现有金融信息验证方法存在透明度不足和信源追溯困难的问题，导致市场面临错误信息引发的重大经济损失风险

Method: 结合双RAG管道与外部事实核查，运用置信度加权投票机制和自适应三层处理策略（元数据提取→混合推理→全模型分析）

Result: 在FinFact数据集上达到0.85的F1值，较最佳单管道提升10.4%，较独立RAG方法提升37%

Conclusion: FinVet通过动态验证策略和透明化输出机制，有效提升金融信息验证性能，为实际应用提供可靠的技术解决方案

Abstract: Financial markets face growing threats from misinformation that can trigger
billions in losses in minutes. Most existing approaches lack transparency in
their decision-making and provide limited attribution to credible sources. We
introduce FinVet, a novel multi-agent framework that integrates two
Retrieval-Augmented Generation (RAG) pipelines with external fact-checking
through a confidence-weighted voting mechanism. FinVet employs adaptive
three-tier processing that dynamically adjusts verification strategies based on
retrieval confidence, from direct metadata extraction to hybrid reasoning to
full model-based analysis. Unlike existing methods, FinVet provides
evidence-backed verdicts, source attribution, confidence scores, and explicit
uncertainty flags when evidence is insufficient. Experimental evaluation on the
FinFact dataset shows that FinVet achieves an F1 score of 0.85, which is a
10.4% improvement over the best individual pipeline (fact-check pipeline) and
37% improvement over standalone RAG approaches.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [201] [Stop DDoS Attacking the Research Community with AI-Generated Survey Papers](https://arxiv.org/abs/2510.09686)
*Jianghao Lin,Rong Shan,Jiachen Zhu,Yunjia Xi,Yong Yu,Weinan Zhang*

Main category: cs.CY

TL;DR: AI-generated survey papers threaten academic integrity through low-quality proliferation, requiring urgent community norms and infrastructure reforms.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the emerging crisis of AI-generated survey papers flooding academic platforms, which undermines scientific trust through redundant/low-quality content and potential hallucinations.

Method: Combines quantitative trend analysis of survey paper growth with quality audits of AI-generated content, supported by cultural impact discussions.

Result: Reveals an urgent need to establish AI-usage transparency norms and develop Dynamic Live Surveys – community-maintained, version-controlled repositories with human-AI collaboration.

Conclusion: Proposes banning indiscriminate AI-survey uploads while advocating expert-curated systems that preserve survey paper value through structured human oversight and dynamic knowledge management.

Abstract: Survey papers are foundational to the scholarly progress of research
communities, offering structured overviews that guide both novices and experts
across disciplines. However, the recent surge of AI-generated surveys,
especially enabled by large language models (LLMs), has transformed this
traditionally labor-intensive genre into a low-effort, high-volume output.
While such automation lowers entry barriers, it also introduces a critical
threat: the phenomenon we term the "survey paper DDoS attack" to the research
community. This refers to the unchecked proliferation of superficially
comprehensive but often redundant, low-quality, or even hallucinated survey
manuscripts, which floods preprint platforms, overwhelms researchers, and
erodes trust in the scientific record. In this position paper, we argue that we
must stop uploading massive amounts of AI-generated survey papers (i.e., survey
paper DDoS attack) to the research community, by instituting strong norms for
AI-assisted review writing. We call for restoring expert oversight and
transparency in AI usage and, moreover, developing new infrastructures such as
Dynamic Live Surveys, community-maintained, version-controlled repositories
that blend automated updates with human curation. Through quantitative trend
analysis, quality audits, and cultural impact discussion, we show that
safeguarding the integrity of surveys is no longer optional but imperative to
the research community.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [202] [Bhasha-Rupantarika: Algorithm-Hardware Co-design approach for Multilingual Neural Machine Translation](https://arxiv.org/abs/2510.10676)
*Mukul Lokhande,Tanushree Dewangan,Mohd Sharik Mansoori,Tejas Chaudhari,Akarsh J.,Damayanti Lokhande,Adam Teman,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: 通过算法-硬件协同设计提出轻量级多语言翻译系统Bhasha-Rupantarika，采用FP4量化实现模型体积缩减4.1倍、推理加速4.2倍，FPGA部署提升吞吐量并优化硬件资源使用


<details>
  <summary>Details</summary>
Motivation: 解决资源受限设备（如物联网终端）的实时多语言翻译需求，探索超低精度量化与硬件加速的协同优化方案

Method: 采用sub-octet精度量化（FP8/INT8/INT4/FP4）压缩模型，结合FPGA加速器实现硬件级优化，验证印度语系与国际语言的双向翻译能力

Result: FP4量化实现66 tokens/s吞吐量（提升4.8x），FPGA部署较OPU/HPTA分别提升2.2x/4.6x吞吐量，硬件资源LUTs减少1.96x、FFs减少1.65x

Conclusion: 量化感知翻译与硬件协同设计为低资源多语言AI系统提供可行方案，开源代码及数据集助力研究社区快速部署与扩展

Abstract: This paper introduces Bhasha-Rupantarika, a light and efficient multilingual
translation system tailored through algorithm-hardware codesign for
resource-limited settings. The method investigates model deployment at
sub-octet precision levels (FP8, INT8, INT4, and FP4), with experimental
results indicating a 4.1x reduction in model size (FP4) and a 4.2x speedup in
inference speed, which correlates with an increased throughput of 66 tokens/s
(improvement by 4.8x). This underscores the importance of ultra-low precision
quantization for real-time deployment in IoT devices using FPGA accelerators,
achieving performance on par with expectations. Our evaluation covers
bidirectional translation between Indian and international languages,
showcasing its adaptability in low-resource linguistic contexts. The FPGA
deployment demonstrated a 1.96x reduction in LUTs and a 1.65x decrease in FFs,
resulting in a 2.2x enhancement in throughput compared to OPU and a 4.6x
enhancement compared to HPTA. Overall, the evaluation provides a viable
solution based on quantisation-aware translation along with hardware efficiency
suitable for deployable multilingual AI systems. The entire codes
[https://github.com/mukullokhande99/Bhasha-Rupantarika/] and dataset for
reproducibility are publicly available, facilitating rapid integration and
further development by researchers.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [203] [ArtPerception: ASCII Art-based Jailbreak on LLMs with Recognition Pre-test](https://arxiv.org/abs/2510.10281)
*Guan-Yan Yang,Tzu-Yu Cheng,Ya-Wen Teng,Farn Wanga,Kuo-Hui Yeh*

Main category: cs.CR

TL;DR: 提出ArtPerception黑箱越狱框架，通过ASCII艺术绕过LLM安全机制，在多个SOTA模型中验证有效性


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全机制过度依赖语义分析，无法防御非标准数据表示形式的攻击

Method: 两阶段系统化攻击：模型特性预测试 + 参数优化的单次越狱攻击，引入改进版Levenshtein距离评估指标

Result: 成功攻击4个开源LLM，并验证对GPT-4o/Claude/DeepSeek等商业模型的转移攻击有效性，突破LLaMA Guard等防御机制

Conclusion: LLM安全需考虑多模态解释空间防御，基于侦察的参数优化攻击策略具有显著威胁性

Abstract: The integration of Large Language Models (LLMs) into computer applications
has introduced transformative capabilities but also significant security
challenges. Existing safety alignments, which primarily focus on semantic
interpretation, leave LLMs vulnerable to attacks that use non-standard data
representations. This paper introduces ArtPerception, a novel black-box
jailbreak framework that strategically leverages ASCII art to bypass the
security measures of state-of-the-art (SOTA) LLMs. Unlike prior methods that
rely on iterative, brute-force attacks, ArtPerception introduces a systematic,
two-phase methodology. Phase 1 conducts a one-time, model-specific pre-test to
empirically determine the optimal parameters for ASCII art recognition. Phase 2
leverages these insights to launch a highly efficient, one-shot malicious
jailbreak attack. We propose a Modified Levenshtein Distance (MLD) metric for a
more nuanced evaluation of an LLM's recognition capability. Through
comprehensive experiments on four SOTA open-source LLMs, we demonstrate
superior jailbreak performance. We further validate our framework's real-world
relevance by showing its successful transferability to leading commercial
models, including GPT-4o, Claude Sonnet 3.7, and DeepSeek-V3, and by conducting
a rigorous effectiveness analysis against potential defenses such as LLaMA
Guard and Azure's content filters. Our findings underscore that true LLM
security requires defending against a multi-modal space of interpretations,
even within text-only inputs, and highlight the effectiveness of strategic,
reconnaissance-based attacks. Content Warning: This paper includes potentially
harmful and offensive model outputs.

</details>


### [204] [Secret-Protected Evolution for Differentially Private Synthetic Text Generation](https://arxiv.org/abs/2510.10990)
*Tianze Wang,Zhaoyu Chen,Jian Du,Yingtai Xiao,Linjun Zhang,Qiang Yan*

Main category: cs.CR

TL;DR: 提出Secret-Protected Evolution (SecPE)框架，通过秘密感知保护机制优化差分隐私文本生成，在保持隐私的同时提升数据效用并降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私文本生成方法对非敏感内容过度保护，导致数据效用显著下降和计算负担增加。需要针对敏感信息进行差异化隐私保护。

Method: 基于隐私进化框架，引入(p,r)-秘密保护机制（高斯差分隐私的松弛形式），理论证明其支持更紧密的隐私-效用权衡，并显著降低计算复杂度。

Result: 在OpenReview/PubMed/Yelp数据集上，SecPE的Fréchet Inception Distance降低22-35%，下游任务准确率提升8-15%，同时实现相同保护强度所需噪声减少30%。

Conclusion: 秘密感知保护机制突破了传统均匀隐私保护的局限，为实现高效实用的隐私保护文本生成提供了新范式。

Abstract: Text data has become extremely valuable on large language models (LLMs) and
even lead to general artificial intelligence (AGI). A lot of high-quality text
in the real world is private and cannot be freely used due to privacy concerns.
Therefore, differentially private (DP) synthetic text generation has been
proposed, aiming to produce high-utility synthetic data while protecting
sensitive information. However, existing DP synthetic text generation imposes
uniform guarantees that often overprotect non-sensitive content, resulting in
substantial utility loss and computational overhead. Therefore, we propose
Secret-Protected Evolution (SecPE), a novel framework that extends private
evolution with secret-aware protection. Theoretically, we show that SecPE
satisfies $(\mathrm{p}, \mathrm{r})$-secret protection, constituting a
relaxation of Gaussian DP that enables tighter utility-privacy trade-offs,
while also substantially reducing computational complexity relative to baseline
methods. Empirically, across the OpenReview, PubMed, and Yelp benchmarks, SecPE
consistently achieves lower Fr\'echet Inception Distance (FID) and higher
downstream task accuracy than GDP-based Aug-PE baselines, while requiring less
noise to attain the same level of protection. Our results highlight that
secret-aware guarantees can unlock more practical and effective
privacy-preserving synthetic text generation.

</details>


### [205] [Bag of Tricks for Subverting Reasoning-based Safety Guardrails](https://arxiv.org/abs/2510.11570)
*Shuo Chen,Zhen Han,Haokun Chen,Bailan He,Shengyun Si,Jingpei Wu,Philip Torr,Volker Tresp,Jindong Gu*

Main category: cs.CR

TL;DR: 现有基于推理的安全防护措施存在脆弱性，简单的输入模板修改即可绕过防护，导致高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 揭示基于推理的防护措施（如审议对齐）的脆弱性，发现其易受输入提示微调攻击。

Method: 提出多场景攻击方法（白/灰/黑盒），包括模板注入和自动化优化攻击技术。

Result: 在5个基准测试中实现超90%攻击成功率，验证不同开源LRMs的系统性漏洞。

Conclusion: 开源大型推理模型亟需更强大的对齐技术以防止恶意利用，现有防护体系存在根本性缺陷。

Abstract: Recent reasoning-based safety guardrails for Large Reasoning Models (LRMs),
such as deliberative alignment, have shown strong defense against jailbreak
attacks. By leveraging LRMs' reasoning ability, these guardrails help the
models to assess the safety of user inputs before generating final responses.
The powerful reasoning ability can analyze the intention of the input query and
will refuse to assist once it detects the harmful intent hidden by the
jailbreak methods. Such guardrails have shown a significant boost in defense,
such as the near-perfect refusal rates on the open-source gpt-oss series.
Unfortunately, we find that these powerful reasoning-based guardrails can be
extremely vulnerable to subtle manipulation of the input prompts, and once
hijacked, can lead to even more harmful results. Specifically, we first uncover
a surprisingly fragile aspect of these guardrails: simply adding a few template
tokens to the input prompt can successfully bypass the seemingly powerful
guardrails and lead to explicit and harmful responses. To explore further, we
introduce a bag of jailbreak methods that subvert the reasoning-based
guardrails. Our attacks span white-, gray-, and black-box settings and range
from effortless template manipulations to fully automated optimization. Along
with the potential for scalable implementation, these methods also achieve
alarmingly high attack success rates (e.g., exceeding 90% across 5 different
benchmarks on gpt-oss series on both local host models and online API
services). Evaluations across various leading open-source LRMs confirm that
these vulnerabilities are systemic, underscoring the urgent need for stronger
alignment techniques for open-sourced LRMs to prevent malicious misuse. Code is
open-sourced at https://chenxshuo.github.io/bag-of-tricks.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [206] [VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents](https://arxiv.org/abs/2510.11098)
*Jiliang Hu,Wenfu Wang,Zuchao Li,Chenxing Li,Yiyang Zhao,Hanzhao Li,Liqiang Zhang,Meng Yu,Dong Yu*

Main category: cs.SD

TL;DR: 提出高质量中文语音基准VCB Bench，从指令遵循、知识理解和鲁棒性三个维度全面评估大音频语言模型的性能


<details>
  <summary>Details</summary>
Motivation: 现有基准存在英语中心化、依赖合成语音、评估维度单一等局限，需要构建更全面的中文语音评估体系

Method: 基于真实人类语音构建VCB Bench，通过指令遵循（含语音控制）、知识理解（常识/推理/对话）和鲁棒性（内容/环境/说话人扰动）三个互补维度进行评估

Result: 实验揭示现有模型存在显著性能差距，特别是在语音级控制和环境扰动下的稳定性方面需要改进

Conclusion: VCB Bench提供可复现的细粒度评估框架，通过标准化方法论为中文语音对话模型的进步提供实践指导

Abstract: Recent advances in large audio language models (LALMs) have greatly enhanced
multimodal conversational systems. However, existing benchmarks remain limited
-- they are mainly English-centric, rely on synthetic speech, and lack
comprehensive, discriminative evaluation across multiple dimensions. To address
these gaps, we present Voice Chat Bot Bench (VCB Bench) -- a high-quality
Chinese benchmark built entirely on real human speech. VCB Bench evaluates
LALMs from three complementary perspectives: instruction following (including
speech-level control beyond text commands), knowledge understanding (general
knowledge, reasoning, and daily dialogue), and robustness (stability under
perturbations in content, environment, and speaker traits). Experiments on
representative LALMs reveal notable performance gaps and highlight future
directions for improvement. VCB Bench provides a reproducible and fine-grained
evaluation framework, offering standardized methodology and practical insights
for advancing Chinese voice conversational models.

</details>


### [207] [Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap](https://arxiv.org/abs/2510.11330)
*KiHyun Nam,Jongmin Choi,Hyeongkeun Lee,Jungwoo Heo,Joon Son Chung*

Main category: cs.SD

TL;DR: 提出Diffusion-Link扩散模块，通过生成式映射音频嵌入至文本分布，显著降低模态间隙，在自动音频描述任务中实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 对比音频-文本预训练存在模态间隙，限制了多模态编码器与LLMs的协同效果。需要桥接模态分布以提升下游任务表现。

Method: 基于扩散的轻量化模块（含3个残差MLP块），在冻结多模态编码器输出端训练，实现音频嵌入到文本嵌入分布的生成式映射。

Result: 1) 模态间隙分析：扩散桥接效果最佳，音频嵌入向文本分布集体迁移；2) AudioCaps数据集零样本/全监督AAC分别提升52.5%和7.5%

Conclusion: 弥合模态间隙是多模态编码器-LLM协同的关键，扩散桥接为知识检索之外的新范式提供了可行路径。

Abstract: Contrastive audio-language pretraining yields powerful joint representations,
yet a persistent audio-text modality gap limits the benefits of coupling
multimodal encoders with large language models (LLMs). We present
Diffusion-Link, a diffusion-based modality-bridging module that generatively
maps audio embeddings into the text-embedding distribution. The module is
trained at the output embedding from the frozen multimodal encoder and
implemented as a lightweight network with three residual MLP blocks. To assess
the effect of Diffusion-Link on multimodal encoder-LLM coupling, we evaluate on
Automatic Audio Captioning (AAC); to our knowledge, this is the first
application of diffusion-based modality bridging to AAC. We report two results.
(1) Modality-gap analysis: on similarity and geometric criteria, Diffusion-Link
reduces the modality gap the most among prior diffusion-based methods and shows
a collective migration of audio embeddings toward the text distribution. (2)
Downstream AAC: attaching Diffusion-Link to the same multimodal LLM baseline
achieves state-of-the-art on AudioCaps in both zero-shot and fully supervised
captioning without external knowledge, with relative gains up to 52.5% and
7.5%, respectively. These findings show that closing the modality gap is
pivotal for effective coupling between multimodal encoders and LLMs, and
diffusion-based modality bridging offers a promising direction beyond
knowledge-retrieval-centric designs. Code will be released upon acceptance
https://github.com/DevKiHyun/Diffusion-Link

</details>
