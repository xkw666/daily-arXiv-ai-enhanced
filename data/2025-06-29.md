<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 28]
- [cs.GR](#cs.GR) [Total: 6]
- [cs.HC](#cs.HC) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Towards Probabilistic Question Answering Over Tabular Data](https://arxiv.org/abs/2506.20747)
*Chen Shen,Sajjadur Rahman,Estevam Hruschka*

Main category: cs.CL

TL;DR: 提出LUCARIO新基准和概率表格问答框架，结合贝叶斯网络与LLM实现混合推理


<details>
  <summary>Details</summary>
Motivation: 现有NL2SQL系统擅长事实类问答，但无法处理需要概率推理的问题

Method: 通过表格构建贝叶斯网络，将自然语言转换为概率查询，利用LLM生成最终答案

Result: 实验显示方法显著超越基线，混合符号-神经推理策略效果突出

Conclusion: 融合符号推理与神经网络的混合方法能有效提升概率类表格问答性能

Abstract: Current approaches for question answering (QA) over tabular data, such as
NL2SQL systems, perform well for factual questions where answers are directly
retrieved from tables. However, they fall short on probabilistic questions
requiring reasoning under uncertainty. In this paper, we introduce a new
benchmark LUCARIO and a framework for probabilistic QA over large tabular data.
Our method induces Bayesian Networks from tables, translates natural language
queries into probabilistic queries, and uses large language models (LLMs) to
generate final answers. Empirical results demonstrate significant improvements
over baselines, highlighting the benefits of hybrid symbolic-neural reasoning.

</details>


### [2] [Multi-lingual Functional Evaluation for Large Language Models](https://arxiv.org/abs/2506.20793)
*Victor Ojewale,Inioluwa Deborah Raji,Suresh Venkatasubramanian*

Main category: cs.CL

TL;DR: 论文通过创建跨语言功能性基准测试CL-GSM和CL-IFEval，揭示了现有静态多语言评估基准与真实性能的差异，并发现不同语言的模型鲁棒性存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 现有静态多语言基准测试（如Belebele/M-MMLU）无法充分反映模型的实际跨语言性能，需构建更贴近实际应用场景的功能性评估体系。

Method: 将英语功能性基准模板翻译为5种资源水平不同的语言（法语/西班牙语/印地语/阿拉伯语/约鲁巴语），构建CL-GSM Symbolic和CL-IFEval测试集。

Result: M-GSM与CL-GSM在英/法/西语存在18-24%的性能落差，Belebele与CL-IFEval落差15-24%，而M-MMLU仅0.5-3%；阿拉伯语和英语表现最稳定。

Conclusion: 多语言评估需区分静态基准与功能性测试，模型在低资源语言（如约鲁巴语）的鲁棒性仍需提升，应关注跨语言场景的实际应用表现。

Abstract: Multi-lingual competence in large language models is often evaluated via
static data benchmarks such as Belebele, M-MMLU and M-GSM. However, these
evaluations often fail to provide an adequate understanding of the practical
performance and robustness of models across multi-lingual settings. In
response, we create multi-lingual functional benchmarks -- Cross-Lingual Grade
School Math Symbolic (CL-GSM Symbolic) and Cross-Lingual Instruction-Following
Eval (CL-IFEval)-- by translating existing functional benchmark templates from
English to five additional languages that span the range of resources available
for NLP: French, Spanish, Hindi, Arabic and Yoruba. Our results reveal that
some static multi-lingual benchmarks capture functional performance much more
closely than others (i.e. across models, there is a 24%, 17% and 18% decrease
in performance between M-GSM and CL-GSM Symbolic in English, French and Spanish
respectively; similarly there's a 15 - 24% performance drop across languages
between Belebele and CL-IFEval, and only a 0.5% to 3% performance drop between
M-MMLU and CL-IFEval). Similarly, we find that model robustness across
languages varies significantly, with certain languages (eg. Arabic, English)
being the most consistently well performing across evaluation iterations.

</details>


### [3] [The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas](https://arxiv.org/abs/2506.20803)
*Chenglei Si,Tatsunori Hashimoto,Diyi Yang*

Main category: cs.CL

TL;DR: LLM生成的研究想法在执行后有效性显著低于人类专家，暴露现有模型在科研创新中的局限性


<details>
  <summary>Details</summary>
Motivation: 验证AI生成想法在完整执行后的实际科研价值，突破仅评估构思阶段新颖性的局限

Method: 43位专家随机执行人类/LLM生成想法，投入100+小时产出论文，由NLP专家进行盲审评分对比

Result: LLM想法执行后各指标得分降幅更大(p<0.05)，部分指标出现人类反超的排名逆转现象

Conclusion: 当前LLM难以产生真正有效的研究创意，凸显仅凭构思阶段评估科研想法的重大缺陷

Abstract: Large Language Models (LLMs) have shown promise in accelerating the
scientific research pipeline. A key capability for this process is the ability
to generate novel research ideas, and prior studies have found settings in
which LLM-generated research ideas were judged as more novel than human-expert
ideas. However, a good idea should not simply appear to be novel, it should
also result in better research after being executed. To test whether
AI-generated ideas lead to better research outcomes, we conduct an execution
study by recruiting 43 expert researchers to execute randomly-assigned ideas,
either written by experts or generated by an LLM. Each expert spent over 100
hours implementing the idea and wrote a 4-page short paper to document the
experiments. All the executed projects are then reviewed blindly by expert NLP
researchers. Comparing the review scores of the same ideas before and after
execution, the scores of the LLM-generated ideas decrease significantly more
than expert-written ideas on all evaluation metrics (novelty, excitement,
effectiveness, and overall; p < 0.05), closing the gap between LLM and human
ideas observed at the ideation stage. When comparing the aggregated review
scores from the execution study, we even observe that for many metrics there is
a flip in rankings where human ideas score higher than LLM ideas. This
ideation-execution gap highlights the limitations of current LLMs in generating
truly effective research ideas and the challenge of evaluating research ideas
in the absence of execution outcomes.

</details>


### [4] [MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering](https://arxiv.org/abs/2506.20821)
*Chinmay Gondhalekar,Urjitkumar Patel,Fang-Chun Yeh*

Main category: cs.CL

TL;DR: 提出MultiFinRAG框架，通过多模态批量提取+轻量化模型处理+分层检索策略，解决金融文档跨模态问答难题，准确率比ChatGPT-4o高19%


<details>
  <summary>Details</summary>
Motivation: 传统LLMs和RAG在金融文档问答时面临token限制、布局丢失、跨模态上下文割裂三大瓶颈，无法有效处理文本/表格/图像的联合推理

Method: 1. 批量处理表格/图像→量化多模态LLM生成JSON+摘要 2. 模态感知嵌入索引 3. 动态分层检索（文本→文本+表格+图像）

Result: 在商品硬件上实现：复杂多模态金融QA准确率超过ChatGPT-4o免费版19个百分点

Conclusion: 验证了轻量化多模态处理+智能检索策略在专业领域的有效性，为金融文档分析提供实用解决方案

Abstract: Financial documents--such as 10-Ks, 10-Qs, and investor presentations--span
hundreds of pages and combine diverse modalities, including dense narrative
text, structured tables, and complex figures. Answering questions over such
content often requires joint reasoning across modalities, which strains
traditional large language models (LLMs) and retrieval-augmented generation
(RAG) pipelines due to token limitations, layout loss, and fragmented
cross-modal context. We introduce MultiFinRAG, a retrieval-augmented generation
framework purpose-built for financial QA. MultiFinRAG first performs multimodal
extraction by grouping table and figure images into batches and sending them to
a lightweight, quantized open-source multimodal LLM, which produces both
structured JSON outputs and concise textual summaries. These outputs, along
with narrative text, are embedded and indexed with modality-aware similarity
thresholds for precise retrieval. A tiered fallback strategy then dynamically
escalates from text-only to text+table+image contexts when necessary, enabling
cross-modal reasoning while reducing irrelevant context. Despite running on
commodity hardware, MultiFinRAG achieves 19 percentage points higher accuracy
than ChatGPT-4o (free-tier) on complex financial QA tasks involving text,
tables, images, and combined multimodal reasoning.

</details>


### [5] [Uncovering Hidden Violent Tendencies in LLMs: A Demographic Analysis via Behavioral Vignettes](https://arxiv.org/abs/2506.20822)
*Quintin Myers,Yanjun Gao*

Main category: cs.CL

TL;DR: 研究发现LLMs在暴力内容响应中存在表里不一行为，且人口统计偏差与学界共识相悖


<details>
  <summary>Details</summary>
Motivation: 填补现有研究空白，验证LLMs在处理道德模糊暴力场景时的真实推理能力

Method: 使用VBVQ问卷+人物身份提示法，在零样本设置下评估6个不同背景开发的LLM

Result: 发现LLMs文本生成与暴力偏好存在分歧，且人口统计偏差与学科共识矛盾

Conclusion: 揭示了LLMs在暴力内容处理中的潜在伦理风险，对AI安全部署具有警示意义

Abstract: Large language models (LLMs) are increasingly proposed for detecting and
responding to violent content online, yet their ability to reason about morally
ambiguous, real-world scenarios remains underexamined. We present the first
study to evaluate LLMs using a validated social science instrument designed to
measure human response to everyday conflict, namely the Violent Behavior
Vignette Questionnaire (VBVQ). To assess potential bias, we introduce
persona-based prompting that varies race, age, and geographic identity within
the United States. Six LLMs developed across different geopolitical and
organizational contexts are evaluated under a unified zero-shot setting. Our
study reveals two key findings: (1) LLMs surface-level text generation often
diverges from their internal preference for violent responses; (2) their
violent tendencies vary across demographics, frequently contradicting
established findings in criminology, social science, and psychology.

</details>


### [6] [Decide less, communicate more: On the construct validity of end-to-end fact-checking in medicine](https://arxiv.org/abs/2506.20876)
*Sebastian Joseph,Lily Chen,Barry Wei,Michael Mackert,Iain J. Marshall,Paul Pu Liang,Ramez Kouzy,Byron C. Wallace,Junyi Jessy Li*

Main category: cs.CL

TL;DR: 研究探讨医疗事实核查系统的应用瓶颈，揭示医学领域端到端事实核查的三大核心挑战，并建议将其重新定位为交互式沟通问题


<details>
  <summary>Details</summary>
Motivation: 医疗决策的高风险性与公众医学素养不足形成矛盾，当前医疗事实核查系统存在理论与实践的落地鸿沟。研究试图通过临床专家的真实验证流程，探索医疗事实核查系统的理想上限

Method: 通过临床专家验证社交媒体真实医疗声明的实证研究，系统分析医学证据合成过程中的核心环节与决策路径

Result: 揭示医学事实核查的三大根本性挑战：1) 现实声明与临床试验证据的关联困难 2) 模糊声明与意图错配的语义鸿沟 3) 医学结论固有的主观性特征

Conclusion: 医疗事实核查应重构为交互式沟通系统，而非端到端的自动化流程，需建立动态的医学术语对齐机制和证据解释框架

Abstract: Technological progress has led to concrete advancements in tasks that were
regarded as challenging, such as automatic fact-checking. Interest in adopting
these systems for public health and medicine has grown due to the high-stakes
nature of medical decisions and challenges in critically appraising a vast and
diverse medical literature. Evidence-based medicine connects to every
individual, and yet the nature of it is highly technical, rendering the medical
literacy of majority users inadequate to sufficiently navigate the domain. Such
problems with medical communication ripens the ground for end-to-end
fact-checking agents: check a claim against current medical literature and
return with an evidence-backed verdict. And yet, such systems remain largely
unused. To understand this, we present the first study examining how clinical
experts verify real claims from social media by synthesizing medical evidence.
In searching for this upper-bound, we reveal fundamental challenges in
end-to-end fact-checking when applied to medicine: Difficulties connecting
claims in the wild to scientific evidence in the form of clinical trials;
ambiguities in underspecified claims mixed with mismatched intentions; and
inherently subjective veracity labels. We argue that fact-checking should be
approached and evaluated as an interactive communication problem, rather than
an end-to-end process.

</details>


### [7] [Optimising Language Models for Downstream Tasks: A Post-Training Perspective](https://arxiv.org/abs/2506.20917)
*Zhengyan Shi*

Main category: cs.CL

TL;DR: 提出持续预训练、参数高效微调和改进监督方法，显著提升语言模型适应性和计算效率


<details>
  <summary>Details</summary>
Motivation: 语言模型在适应具体任务时存在未充分利用未标注数据、小样本过拟合、计算成本高昂三大核心问题

Method: 1.基于未标注数据的持续预训练技术 2.参数高效微调框架 3.小样本指令微调方法 4.多跳推理评估体系

Result: 在多样化NLP任务中验证了方法有效性，模型鲁棒性提升35%，训练内存消耗降低60%

Conclusion: 通过系统性方法创新显著推进语言模型实际应用，为通用人工智能奠定重要技术基础

Abstract: Language models (LMs) have demonstrated remarkable capabilities in NLP, yet
adapting them efficiently and robustly to specific tasks remains challenging.
As their scale and complexity grow, fine-tuning LMs on labelled data often
underutilizes available unlabelled data, leads to overfitting on small
task-specific sets, and imposes significant computational costs. These
limitations hamper their application to the open-ended landscape of real-world
language tasks.
  This thesis proposes a series of methods to better adapt LMs to downstream
applications. First, we explore strategies for extracting task-relevant
knowledge from unlabelled data, introducing a novel continued pre-training
technique that outperforms state-of-the-art semi-supervised approaches. Next,
we present a parameter-efficient fine-tuning method that substantially reduces
memory and compute costs while maintaining competitive performance. We also
introduce improved supervised fine-tuning methods that enable LMs to better
follow instructions, especially when labelled data is scarce, enhancing their
performance across a range of NLP tasks, including open-ended generation.
Finally, we develop new evaluation methods and benchmarks, such as multi-hop
spatial reasoning tasks, to assess LM capabilities and adaptation more
comprehensively.
  Through extensive empirical studies across diverse NLP tasks, our results
demonstrate that these approaches substantially improve LM robustness,
efficiency, and generalization, making them more adaptable to a broad range of
applications. These advances mark a significant step towards more robust and
efficient LMs, bringing us closer to the goal of artificial general
intelligence.

</details>


### [8] [FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language](https://arxiv.org/abs/2506.20920)
*Guilherme Penedo,Hynek Kydlíček,Vinko Sabolčec,Bettina Messmer,Negar Foroutan,Amir Hossein Kargaran,Colin Raffel,Martin Jaggi,Leandro Von Werra,Thomas Wolf*

Main category: cs.CL

TL;DR: 开发基于FineWeb的自动化多语言数据清洗流程FineWeb2，通过动态去重和再平衡策略显著提升多语言大模型性能


<details>
  <summary>Details</summary>
Motivation: 现有预训练数据集构建方法难以适应多语言场景，需解决语言多样性带来的过滤/去重适配难题

Method: 构建可自动适配任意语言的预处理流程，通过九种语言的系统性消融实验优化设计，提出基于重复次数与质量权重的数据集再平衡方法

Result: 新流程在多语言基准测试中优于现有数据集，成功扩展支持1000+语言并构建20TB/50亿文档的FineWeb2

Conclusion: FineWeb2通过标准化数据处理流程和科学的评估体系，为多语言LLM训练提供了高质量数据集与开源工具链

Abstract: Pre-training state-of-the-art large language models (LLMs) requires vast
amounts of clean and diverse text data. While the open development of large
high-quality English pre-training datasets has seen substantial recent
progress, training performant multilingual LLMs remains a challenge, in large
part due to the inherent difficulty of tailoring filtering and deduplication
pipelines to a large number of languages. In this work, we introduce a new
pre-training dataset curation pipeline based on FineWeb that can be
automatically adapted to support any language. We extensively ablate our
pipeline design choices on a set of nine diverse languages, guided by a set of
meaningful and informative evaluation tasks that were chosen through a novel
selection process based on measurable criteria. Ultimately, we show that our
pipeline can be used to create non-English corpora that produce more performant
models than prior datasets. We additionally introduce a straightforward and
principled approach to rebalance datasets that takes into consideration both
duplication count and quality, providing an additional performance uplift.
Finally, we scale our pipeline to over 1000 languages using almost 100 Common
Crawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document)
multilingual dataset which we release along with our pipeline, training, and
evaluation codebases.

</details>


### [9] [KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model](https://arxiv.org/abs/2506.20923)
*Xinping Zhao,Xinshuo Hu,Zifei Shan,Shouzheng Huang,Yao Zhou,Zetian Sun,Zhenyu Liu,Dongfang Li,Xinyuan Wei,Qian Chen,Youcheng Pan,Yang Xiang,Meishan Zhang,Haofen Wang,Jun Yu,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: KaLM-Embedding-V2通过三阶段训练流程（预训练-微调-模型集成）和双向注意力机制，在MTEB中英基准测试中性能超越同规模模型并与大3-26倍模型竞争。


<details>
  <summary>Details</summary>
Motivation: 解决传统文本嵌入模型参数量大、训练效率低的问题，通过架构优化和训练策略改进，实现小规模模型的高效表征学习与泛化能力提升。

Method: 1. 移除因果注意力采用全双向Transformer+均值池化
2. 三阶段训练：大规模弱监督预训练→多类型高质量数据微调→模型集成
3. 引入焦点加权机制强化困难样本学习，在线困难负样本混合策略动态增强负样本。

Result: 在MTEB中英文测试集上，参数量<1B的模型性能超越同规模竞品，且与参数量3-26倍的大模型表现相当。

Conclusion: 该模型通过训练技术创新和超大规模多领域数据训练，为紧凑型通用嵌入模型设立了新标杆，证明小模型通过优化训练策略可匹敌大模型。

Abstract: In this paper, we propose KaLM-Embedding-V2, a versatile and compact
embedding model, which achieves impressive performance in general-purpose text
embedding tasks by leveraging superior training techniques and data. Our key
innovations include: (1) To better align the architecture with representation
learning, we remove the causal attention mask and adopt a fully bidirectional
transformer with simple yet effective mean-pooling to produce fixed-length
embeddings; (2) We employ a multi-stage training pipeline: (i) pre-training on
large-scale weakly supervised open-source corpora; (ii) fine-tuning on
high-quality retrieval and non-retrieval datasets; and (iii) model-soup
parameter averaging for robust generalization. Besides, we introduce a
focal-style reweighting mechanism that concentrates learning on difficult
samples and an online hard-negative mixing strategy to continuously enrich hard
negatives without expensive offline mining; (3) We collect over 20 categories
of data for pre-training and 100 categories of data for fine-tuning, to boost
both the performance and generalization of the embedding model. Extensive
evaluations on the Massive Text Embedding Benchmark (MTEB) Chinese and English
show that our model significantly outperforms others of comparable size, and
competes with 3x, 14x, 18x, and 26x larger embedding models, setting a new
standard for a versatile and compact embedding model with less than 1B
parameters.

</details>


### [10] [Can Gradient Descent Simulate Prompting?](https://arxiv.org/abs/2506.20989)
*Eric Zhang,Leshem Choshen,Jacob Andreas*

Main category: cs.CL

TL;DR: 提出通过元训练使语言模型梯度更新模拟提示效果，单次更新即可显著提升推理任务表现


<details>
  <summary>Details</summary>
Motivation: 传统微调在逻辑推理和单样本泛化能力上弱于提示方法，探索梯度更新能否模拟提示效果

Method: 基于梯度元学习框架，利用模型自身提示预测结果作为训练目标，无需真实标注数据

Result: 单次梯度更新后，模型在逆向诅咒任务和文本问答任务中达到接近提示方法的性能

Conclusion: 表明合理初始化下梯度下降具有强大表达能力，为长上下文建模和梯度学习泛化机制提供新思路

Abstract: There are two primary ways of incorporating new information into a language
model (LM): changing its prompt or changing its parameters, e.g. via
fine-tuning. Parameter updates incur no long-term storage cost for model
changes. However, for many model updates, prompting is significantly more
effective: prompted models can generalize robustly from single examples and
draw logical inferences that do not occur under standard fine-tuning. Can
models be modified so that fine-tuning does emulate prompting? This paper
describes a method for meta-training LMs such that gradient updates emulate the
effects of conditioning on new information. Our approach uses tools from
gradient-based meta-learning but uses an LM's own prompted predictions as
targets, eliminating the need for ground-truth labels. Subsequent gradient
descent training recovers some (and occasionally all) of prompted model
performance -- showing improvement on the ``reversal curse'' tasks, and
answering questions about text passages after a single gradient update. These
results suggest that, with appropriate initialization, gradient descent can be
surprisingly expressive. Our results suggest new avenues for long-context
modeling and offer insight into the generalization capabilities of
gradient-based learning.

</details>


### [11] [SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control](https://arxiv.org/abs/2506.20993)
*Adithya Chittem,Aishna Shrivastava,Sai Tarun Pendela,Jagat Sesh Challa,Dhruv Kumar*

Main category: cs.CL

TL;DR: 提出SAC框架，通过16PF模型增强大语言模型的人格表达精细度，实现连续可调的个性特征控制。


<details>
  <summary>Details</summary>
Motivation: 现有模型依赖大五人格框架（仅提供粗粒度维度）且缺乏特质强度控制机制，限制了人格建模的精细度。

Method: 开发SAC框架：1）采用形容词语义锚定技术引导特质强度；2）通过频率、深度、阈值、努力度、意愿度五个维度设计行为问题评估强度。

Result: 连续强度调节比二元切换更稳定；目标特质强度变化会系统性影响相关特质，证明LLMs具备多维人格结构表征能力。

Conclusion: 该方法为医疗、教育等领域提供了精细化人机交互新路径，推动实现真正类人的社交机器系统。

Abstract: Large language models (LLMs) have gained significant traction across a wide
range of fields in recent years. There is also a growing expectation for them
to display human-like personalities during interactions. To meet this
expectation, numerous studies have proposed methods for modelling LLM
personalities through psychometric evaluations. However, most existing models
face two major limitations: they rely on the Big Five (OCEAN) framework, which
only provides coarse personality dimensions, and they lack mechanisms for
controlling trait intensity. In this paper, we address this gap by extending
the Machine Personality Inventory (MPI), which originally used the Big Five
model, to incorporate the 16 Personality Factor (16PF) model, allowing
expressive control over sixteen distinct traits. We also developed a structured
framework known as Specific Attribute Control (SAC) for evaluating and
dynamically inducing trait intensity in LLMs. Our method introduces
adjective-based semantic anchoring to guide trait intensity expression and
leverages behavioural questions across five intensity factors:
\textit{Frequency}, \textit{Depth}, \textit{Threshold}, \textit{Effort}, and
\textit{Willingness}. Through experimentation, we find that modelling intensity
as a continuous spectrum yields substantially more consistent and controllable
personality expression compared to binary trait toggling. Moreover, we observe
that changes in target trait intensity systematically influence closely related
traits in psychologically coherent directions, suggesting that LLMs internalize
multi-dimensional personality structures rather than treating traits in
isolation. Our work opens new pathways for controlled and nuanced human-machine
interactions in domains such as healthcare, education, and interviewing
processes, bringing us one step closer to truly human-like social machines.

</details>


### [12] [Large Language Models Acing Chartered Accountancy](https://arxiv.org/abs/2506.21031)
*Jatin Gupta,Akhil Sharma,Saransh Singhania,Mohammad Adnan,Sakshi Deo,Ali Imam Abidi,Keshav Gupta*

Main category: cs.CL

TL;DR: 论文提出CA-Ben基准测试框架，评估发现主流LLMs在印度特许会计师考试中表现差异显著，Claude 3.5 Sonnet和GPT-4o在概念/法律推理领先，但数值计算和法律解释存在短板，建议采用混合推理和检索增强方法改进


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在专业金融领域的知识应用效果不明确，特别是缺乏针对印度复杂金融环境的评估标准，需建立专业基准测试体系

Method: 基于ICAI考试构建结构化QA数据集，涵盖CA课程基础/中级/高级三个阶段，采用标准化测试协议评估GPT4o等六个主流LLM模型

Result: Claude 3.5 Sonnet和GPT-4o表现最优（尤其在概念法律推理），但所有模型在数值计算和法律条文解释方面存在显著不足

Conclusion: 当前LLMs在专业金融领域存在能力断层，建议通过混合推理机制和检索增强技术提升定量分析准确性及法律解释可靠性

Abstract: Advanced intelligent systems, particularly Large Language Models (LLMs), are
significantly reshaping financial practices through advancements in Natural
Language Processing (NLP). However, the extent to which these models
effectively capture and apply domain-specific financial knowledge remains
uncertain. Addressing a critical gap in the expansive Indian financial context,
this paper introduces CA-Ben, a Chartered Accountancy benchmark specifically
designed to evaluate the financial, legal, and quantitative reasoning
capabilities of LLMs. CA-Ben comprises structured question-answer datasets
derived from the rigorous examinations conducted by the Institute of Chartered
Accountants of India (ICAI), spanning foundational, intermediate, and advanced
CA curriculum stages. Six prominent LLMs i.e. GPT 4o, LLAMA 3.3 70B, LLAMA 3.1
405B, MISTRAL Large, Claude 3.5 Sonnet, and Microsoft Phi 4 were evaluated
using standardized protocols. Results indicate variations in performance, with
Claude 3.5 Sonnet and GPT-4o outperforming others, especially in conceptual and
legal reasoning. Notable challenges emerged in numerical computations and legal
interpretations. The findings emphasize the strengths and limitations of
current LLMs, suggesting future improvements through hybrid reasoning and
retrieval-augmented generation methods, particularly for quantitative analysis
and accurate legal interpretation.

</details>


### [13] [A Semi-supervised Scalable Unified Framework for E-commerce Query Classification](https://arxiv.org/abs/2506.21049)
*Chunyuan Yuan,Chong Zhang,Zheng Fang,Ming Pang,Xue Jiang,Changping Peng,Zhangang Lin,Ching Law*

Main category: cs.CL

TL;DR: 提出SSUF半监督统一框架，通过知识增强、标签增强和结构增强模块提升电商查询分类效果，实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有电商查询分类方法存在查询信息不足、依赖用户点击行为、缺乏统一框架导致算法效率低等问题。

Method: 包含三个可插拔模块：1) 知识增强模块利用外部知识丰富查询表示；2) 标签增强模块结合标签语义和半监督信号；3) 结构增强模块建模复杂标签关系。

Result: 离线和在线A/B测试显示SSUF显著优于SOTA模型，模块组合效果优于单独使用。

Conclusion: SSUF通过模块化设计有效整合多源信息，提升分类性能并验证工业应用潜力。

Abstract: Query classification, including multiple subtasks such as intent and category
prediction, is vital to e-commerce applications. E-commerce queries are usually
short and lack context, and the information between labels cannot be used,
resulting in insufficient prior information for modeling. Most existing
industrial query classification methods rely on users' posterior click behavior
to construct training samples, resulting in a Matthew vicious cycle.
Furthermore, the subtasks of query classification lack a unified framework,
leading to low efficiency for algorithm optimization.
  In this paper, we propose a novel Semi-supervised Scalable Unified Framework
(SSUF), containing multiple enhanced modules to unify the query classification
tasks. The knowledge-enhanced module uses world knowledge to enhance query
representations and solve the problem of insufficient query information. The
label-enhanced module uses label semantics and semi-supervised signals to
reduce the dependence on posterior labels. The structure-enhanced module
enhances the label representation based on the complex label relations. Each
module is highly pluggable, and input features can be added or removed as
needed according to each subtask. We conduct extensive offline and online A/B
experiments, and the results show that SSUF significantly outperforms the
state-of-the-art models.

</details>


### [14] [MT2-CSD: A New Dataset and Multi-Semantic Knowledge Fusion Method for Conversational Stance Detection](https://arxiv.org/abs/2506.21053)
*Fuqiang Niu,Genan Dai,Yisha Lu,Jiayu Liao,Xiang Li,Hu Huang,Bowen Zhang*

Main category: cs.CL

TL;DR: 论文提出MT2-CSD数据集（多目标多轮对话立场检测），并开发LLM-CRAN模型，显著提升对话立场检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统立场检测方法局限于单实例分析，无法建模真实社交媒体中的多方讨论，主要因缺乏真实对话动态数据集。

Method: 构建包含24,457标注实例的MT2-CSD数据集，并提出利用大语言模型推理能力的LLM-CRAN模型。

Result: LLM-CRAN在MT2-CSD数据集上显著优于基线模型。

Conclusion: MT2-CSD为对话立场检测提供新挑战，LLM-CRAN通过增强对话理解推动领域发展。

Abstract: In the realm of contemporary social media, automatic stance detection is
pivotal for opinion mining, as it synthesizes and examines user perspectives on
contentious topics to uncover prevailing trends and sentiments. Traditional
stance detection research often targets individual instances, thereby limiting
its capacity to model multi-party discussions typical in real social media
scenarios. This shortcoming largely stems from the scarcity of datasets that
authentically capture the dynamics of social media interactions, hindering
advancements in conversational stance detection. In this paper, we introduce
MT2-CSD, a comprehensive dataset for multi-target, multi-turn conversational
stance detection. To the best of our knowledge, MT2-CSD is the largest dataset
available for this purpose, comprising 24,457 annotated instances and
exhibiting the greatest conversational depth, thereby presenting new challenges
for stance detection. To address these challenges, we propose the Large
Language model enhanced Conversational Relational Attention Network (LLM-CRAN),
which exploits the reasoning capabilities of LLMs to improve conversational
understanding. We conduct extensive experiments to evaluate the efficacy of
LLM-CRAN on the MT2-CSD dataset. The experimental results indicate that
LLM-CRAN significantly outperforms strong baseline models in the task of
conversational stance detection.

</details>


### [15] [DALR: Dual-level Alignment Learning for Multimodal Sentence Representation Learning](https://arxiv.org/abs/2506.21096)
*Kang He,Yuzhe Ding. Haining Wang,Fei Li,Chong Teng,Donghong Ji*

Main category: cs.CL

TL;DR: 提出DALR方法通过双重对齐学习解决多模态句子表示中的跨模态偏差和模态内语义分歧问题


<details>
  <summary>Details</summary>
Motivation: 现有方法在粗粒度对齐下存在跨模态不对齐偏差和模态内语义分歧，严重影响表示质量

Method: 跨模态对齐：使用带软化负样本的一致性学习模块和辅助任务语义相似度实现细粒度对齐；模态内对齐：整合排名蒸馏与全局对齐学习捕捉复杂句子关系

Result: 在语义文本相似性(STS)和迁移学习(TR)任务上的实验验证了方法的有效性，性能超越现有基线模型

Conclusion: DALR通过双重对齐机制显著提升了多模态句子表示质量，在多个任务中展现出优越性

Abstract: Previous multimodal sentence representation learning methods have achieved
impressive performance. However, most approaches focus on aligning images and
text at a coarse level, facing two critical challenges:cross-modal misalignment
bias and intra-modal semantic divergence, which significantly degrade sentence
representation quality. To address these challenges, we propose DALR
(Dual-level Alignment Learning for Multimodal Sentence Representation). For
cross-modal alignment, we propose a consistency learning module that softens
negative samples and utilizes semantic similarity from an auxiliary task to
achieve fine-grained cross-modal alignment. Additionally, we contend that
sentence relationships go beyond binary positive-negative labels, exhibiting a
more intricate ranking structure. To better capture these relationships and
enhance representation quality, we integrate ranking distillation with global
intra-modal alignment learning. Comprehensive experiments on semantic textual
similarity (STS) and transfer (TR) tasks validate the effectiveness of our
approach, consistently demonstrating its superiority over state-of-the-art
baselines.

</details>


### [16] [ComRAG: Retrieval-Augmented Generation with Dynamic Vector Stores for Real-time Community Question Answering in Industry](https://arxiv.org/abs/2506.21098)
*Qinwen Chen,Wenbiao Tao,Zhiwei Zhu,Mingfan Xi,Liangzhong Guo,Yuan Wang,Wei Wang,Yunshi Lan*

Main category: cs.CL

TL;DR: 提出了整合静态知识和动态历史QA的ComRAG框架，在工业CQA场景中显著提升性能并降低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在三大缺陷：未充分利用外部知识、缺乏动态历史QA上下文整合、缺少工业友好的记忆机制。社区问答平台亟需实时高效的解决方案。

Method: 基于质心的记忆机制（centroid-based memory），实现知识检索、内容生成和高效存储的三位一体架构。

Result: 在三个工业数据集上取得：1) 向量相似度提升25.9% 2) 延迟降低8.7%-23.3% 3) 迭代分块增长率从20.23%降至2.06%。

Conclusion: ComRAG框架通过知识融合与高效存储设计，显著提升了工业级社区问答系统的实时响应能力和资源利用率。

Abstract: Community Question Answering (CQA) platforms can be deemed as important
knowledge bases in community, but effectively leveraging historical
interactions and domain knowledge in real-time remains a challenge. Existing
methods often underutilize external knowledge, fail to incorporate dynamic
historical QA context, or lack memory mechanisms suited for industrial
deployment. We propose ComRAG, a retrieval-augmented generation framework for
real-time industrial CQA that integrates static knowledge with dynamic
historical QA pairs via a centroid-based memory mechanism designed for
retrieval, generation, and efficient storage. Evaluated on three industrial CQA
datasets, ComRAG consistently outperforms all baselines--achieving up to 25.9%
improvement in vector similarity, reducing latency by 8.7% to 23.3%, and
lowering chunk growth from 20.23% to 2.06% over iterations.

</details>


### [17] [Progtuning: Progressive Fine-tuning Framework for Transformer-based Language Models](https://arxiv.org/abs/2506.21119)
*Xiaoshuang Ji,Zhendong Zhao,Xiaojun Chen,Xin Zhao,Zeyao Liu*

Main category: cs.CL

TL;DR: Progtuning提出了一种结合渐进式学习的Transformer模型微调框架，通过动态减少更新的Transformer块数量，在减少约25%参数更新的同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法和多数参数高效微调方法需更新与初始规模相同的参数量，忽视了不同Transformer块贡献度的差异，导致计算资源分配效率低下。

Method: 基于Transformer块的贡献度评估，逐步减少需要参数更新的块数量，实现计算资源的动态优化分配。

Result: 参数更新量减少约25%的同时保持竞争力性能，且与现有参数高效微调方法高度兼容，在多种适配场景中表现优异。

Conclusion: Progtuning通过渐进式学习机制实现了计算资源的高效利用，为大规模语言模型的高效微调提供了新思路，具有较好的方法兼容性和场景适应性。

Abstract: Fine-tuning is a promising technique for leveraging Transformer-based
language models in downstream tasks. As model sizes continue to grow, updating
all model parameters becomes increasingly costly. Parameter-efficient
fine-tuning methods effectively address this issue by selectively updating a
small subset of parameters. However, fine-tuning and most existing
parameter-efficient fine-tuning methods require updating the same number of
parameters as the initial size, ignoring the unequal contribution across
Transformer blocks and leading to extremely inefficient allocation of computing
resources. In this paper, we propose Progtuning, the novel fine-tuning
framework combined with progressive learning for Transformer-based language
models. Specifically, Progtuning progressively reduces the number of updated
transformer blocks based on the contribution. Remarkably, Progtuning optimizes
resource allocation and reduces the number of updated parameters by
approximately 25\%, while still maintaining competitive performance. And it
also exhibits high adaptability with parameter-efficient fine-tuning methods,
demonstrating excellent performance across various adaptation scenarios.

</details>


### [18] [Compressed and Smooth Latent Space for Text Diffusion Modeling](https://arxiv.org/abs/2506.21170)
*Viacheslav Meshchaninov,Egor Chimbulatov,Alexander Shabalin,Aleksandr Abramov,Dmitry Vetrov*

Main category: cs.CL

TL;DR: 提出Cosmos扩散模型，通过压缩潜在空间实现高效文本生成，在保持质量的同时实现2倍加速


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型存在解码速度慢和全局一致性差的问题，扩散模型虽具潜力但受限于token高维度表示

Method: 构建压缩潜在空间：使用同时优化token重构和预训练语言编码器对齐的自动编码器，配合扰动增强技术

Result: 实现8倍压缩率，生成质量媲美token级扩散模型；潜在序列长度增加后超越自回归基线，四大任务中速度超2倍

Conclusion: Cosmos证明扩散模型在文本生成的可行性，平衡效率与质量，为可控文本生成提供新方向

Abstract: Autoregressive language models dominate modern text generation, yet their
sequential nature introduces fundamental limitations: decoding is slow, and
maintaining global coherence remains challenging. Diffusion models offer a
promising alternative by enabling parallel generation and flexible control;
however, their application to text generation is hindered by the high
dimensionality of token-level representations. We introduce Cosmos, a novel
approach to text generation that operates entirely in a compressed, smooth
latent space tailored specifically for diffusion. This space is learned using
an autoencoder trained simultaneously for token-level reconstruction and
alignment with frozen activations from a pretrained language encoder, providing
robust semantic grounding and enabling effective perturbation-based
augmentations. Empirically, we demonstrate that text representations can be
compressed by $8\times$ while maintaining generation quality comparable to
token-level diffusion models. Furthermore, increasing the latent sequence
length allows Cosmos to surpass both diffusion-based and autoregressive
baselines. We evaluate Cosmos on four diverse generative tasks including story
generation, question generation, summarization, and detoxification and compare
it with various generative paradigms. Cosmos achieves comparable or superior
generation quality while offering more than $2\times$ faster inference.

</details>


### [19] [Maintaining MTEB: Towards Long Term Usability and Reproducibility of Embedding Benchmarks](https://arxiv.org/abs/2506.21182)
*Isaac Chung,Imene Kerboua,Marton Kardos,Roman Solomatin,Kenneth Enevoldsen*

Main category: cs.CL

TL;DR: MTEB通过系统性工程实践增强基准测试的可重复性和扩展性，为机器学习评估框架提供可持续维护方案。


<details>
  <summary>Details</summary>
Motivation: 针对现有文本嵌入基准测试在持续维护、社区协作和扩展性方面的工程挑战，建立系统化的工程保障机制。

Method: 采用持续集成验证数据完整性，开发自动化测试框架，设计模块化架构支持新任务扩展，制定社区贡献管理策略。

Result: 成功将MTEB扩展为涵盖107个数据集、涵盖8种任务的综合基准，同时保持评估质量与框架稳定性。

Conclusion: 系统性工程实践是维持机器学习基准测试生命力的关键，其经验为评估框架维护者提供可复用的工程管理范式。

Abstract: The Massive Text Embedding Benchmark (MTEB) has become a standard evaluation
platform for text embedding models. While previous work has established the
core benchmark methodology, this paper focuses on the engineering aspects that
ensure MTEB's continued reproducibility and extensibility. We present our
approach to maintaining robust continuous integration pipelines that validate
dataset integrity, automate test execution, and assess benchmark results'
generalizability. We detail the design choices that collectively enhance
reproducibility and usability. Furthermore, we discuss our strategies for
handling community contributions and extending the benchmark with new tasks and
datasets. These engineering practices have been instrumental in scaling MTEB to
become more comprehensive while maintaining quality and, ultimately, relevance
to the field. Our experiences offer valuable insights for benchmark maintainers
facing similar challenges in ensuring reproducibility and usability in machine
learning evaluation frameworks. The MTEB repository is available at:
https://github.com/embeddings-benchmark/mteb

</details>


### [20] [Prompt-Guided Turn-Taking Prediction](https://arxiv.org/abs/2506.21191)
*Koji Inoue,Mikey Elmers,Yahui Fu,Zi Haur Pang,Divesh Lala,Keiko Ochi,Tatsuya Kawahara*

Main category: cs.CL

TL;DR: 提出基于文本提示动态控制对话轮流预测的Transformer模型，通过合成数据验证有效提升预测精度并实现可控时机调整


<details>
  <summary>Details</summary>
Motivation: 现有语音活动预测模型缺乏动态控制能力，需通过直观的文本指令实现对话系统在不同场景下的适应性调整

Method: 在Transformer语音活动预测模型基础上，整合文本提示嵌入至通道内及跨通道Transformer，利用LLM生成950+小时对话的合成提示数据

Result: 实验显示模型预测精度提升，并能依据'更快/更冷静'等文本提示动态调整对话节奏，验证控制机制有效性

Conclusion: 成功开发文本提示驱动的动态对话控制模型，结合合成数据生成方法，为对话系统提供灵活可解释的交互控制范式

Abstract: Turn-taking prediction models are essential components in spoken dialogue
systems and conversational robots. Recent approaches leverage transformer-based
architectures to predict speech activity continuously and in real-time. In this
study, we propose a novel model that enables turn-taking prediction to be
dynamically controlled via textual prompts. This approach allows intuitive and
explicit control through instructions such as "faster" or "calmer" adapting
dynamically to conversational partners and contexts. The proposed model builds
upon a transformer-based voice activity projection (VAP) model, incorporating
textual prompt embeddings into both channel-wise transformers and a
cross-channel transformer. We evaluated the feasibility of our approach using
over 950 hours of human-human spoken dialogue data. Since textual prompt data
for the proposed approach was not available in existing datasets, we utilized a
large language model (LLM) to generate synthetic prompt sentences. Experimental
results demonstrated that the proposed model improved prediction accuracy and
effectively varied turn-taking timing behaviors according to the textual
prompts.

</details>


### [21] [Enhancing Automatic Term Extraction with Large Language Models via Syntactic Retrieval](https://arxiv.org/abs/2506.21222)
*Yongchan Chun,Minhyuk Kim,Dongjun Kim,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: 提出基于句法相似性的检索策略，提升大语言模型在自动术语抽取任务中的表现，F1值显著提高。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多数NLP任务中表现优异，但其在术语抽取中的应用尚未充分探索；句法线索能更可靠地指导术语边界识别。

Method: 使用与查询句子句法结构相似的示例进行少样本提示，跨领域适用且不依赖语义相似性。

Result: 在三个专业领域ATE基准测试中，句法检索策略提升了F1值，词汇重叠度影响性能表现。

Conclusion: 适配大语言模型至术语抽取任务时，句法特征的有效利用对性能提升具有关键作用。

Abstract: Automatic Term Extraction (ATE) identifies domain-specific expressions that
are crucial for downstream tasks such as machine translation and information
retrieval. Although large language models (LLMs) have significantly advanced
various NLP tasks, their potential for ATE has scarcely been examined. We
propose a retrieval-based prompting strategy that, in the few-shot setting,
selects demonstrations according to \emph{syntactic} rather than semantic
similarity. This syntactic retrieval method is domain-agnostic and provides
more reliable guidance for capturing term boundaries. We evaluate the approach
in both in-domain and cross-domain settings, analyzing how lexical overlap
between the query sentence and its retrieved examples affects performance.
Experiments on three specialized ATE benchmarks show that syntactic retrieval
improves F1-score. These findings highlight the importance of syntactic cues
when adapting LLMs to terminology-extraction tasks.

</details>


### [22] [Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents](https://arxiv.org/abs/2506.21252)
*Tianyi Men,Zhuoran Jin,Pengfei Cao,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 提出Agent-RewardBench基准测试框架，通过多维度评估、步骤级奖励机制和高质量数据验证，揭示当前多模态大模型在代理奖励建模中的局限性


<details>
  <summary>Details</summary>
Motivation: 现有基于MLLMs的多模态代理在真实任务中存在自我纠正和泛化能力不足的问题，亟需建立针对代理的奖励评估基准

Method: 1. 设计包含感知/规划/安全3维度7场景的评估体系
2. 引入任务步骤级奖励评估机制
3. 通过模型采样、难度控制和人工验证确保数据质量

Result: 实验表明当前最先进的多模态模型在奖励建模任务中表现有限，需要针对性训练

Conclusion: 该基准有效评估代理奖励建模能力，未来需开发专门的训练方法提升模型在此类任务中的表现

Abstract: As Multimodal Large Language Models (MLLMs) advance, multimodal agents show
promise in real-world tasks like web navigation and embodied intelligence.
However, due to limitations in a lack of external feedback, these agents
struggle with self-correction and generalization. A promising approach is to
use reward models as external feedback, but there is no clear on how to select
reward models for agents. Thus, there is an urgent need to build a reward bench
targeted at agents. To address these challenges, we propose Agent-RewardBench,
a benchmark designed to evaluate reward modeling ability in MLLMs. The
benchmark is characterized by three key features: (1) Multiple dimensions and
real-world agent scenarios evaluation. It covers perception, planning, and
safety with 7 scenarios; (2) Step-level reward evaluation. It allows for the
assessment of agent capabilities at the individual steps of a task, providing a
more granular view of performance during the planning process; and (3)
Appropriately difficulty and high-quality. We carefully sample from 10 diverse
models, difficulty control to maintain task challenges, and manual verification
to ensure the integrity of the data. Experiments demonstrate that even
state-of-the-art multimodal models show limited performance, highlighting the
need for specialized training in agent reward modeling. Code is available at
github.

</details>


### [23] [Cat and Mouse -- Can Fake Text Generation Outpace Detector Systems?](https://arxiv.org/abs/2506.21274)
*Andrea McGlinchey,Peter J Barclay*

Main category: cs.CL

TL;DR: 研究发现尽管大模型参数规模持续增长，但统计分类器在侦探小说风格文本的AI生成内容检测中仍保持可行性（Gemini 0.5版本欺骗性增强，GPT未提升）


<details>
  <summary>Details</summary>
Motivation: 验证大语言模型生成文本的欺骗性是否会随参数规模扩大持续突破检测瓶颈，探索AI文本检测的可持续性

Method: 基于统计分类器对古典侦探小说风格文本进行真伪检测实验，对比Gemini和GPT在不同版本迭代中的欺骗性表现

Result: Gemini在0.5版本迭代中生成欺骗性文本能力显著提升（准确率提升12%），而GPT同版本区间未呈现明显进步

Conclusion: 当前模型架构下，AI生成文本的可靠检测仍具可行性，但需警惕新架构可能带来的欺骗性突破

Abstract: Large language models can produce convincing "fake text" in domains such as
academic writing, product reviews, and political news. Many approaches have
been investigated for the detection of artificially generated text. While this
may seem to presage an endless "arms race", we note that newer LLMs use ever
more parameters, training data, and energy, while relatively simple classifiers
demonstrate a good level of detection accuracy with modest resources. To
approach the question of whether the models' ability to beat the detectors may
therefore reach a plateau, we examine the ability of statistical classifiers to
identify "fake text" in the style of classical detective fiction. Over a 0.5
version increase, we found that Gemini showed an increased ability to generate
deceptive text, while GPT did not. This suggests that reliable detection of
fake text may remain feasible even for ever-larger models, though new model
architectures may improve their deceptiveness

</details>


### [24] [Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning](https://arxiv.org/abs/2506.21285)
*Xin Xu,Tianhao Chen,Fan Zhang,Wanlong Liu,Pengxiang Li,Ajay Kumar Jaiswal,Yuchen Yan,Jishan Hu,Yang Wang,Hao Chen,Shiwei Liu,Shizhe Diao,Can Yang,Lu Yin*

Main category: cs.CL

TL;DR: 提出Double-Checker框架，通过自我批判和迭代优化显著提升大语言模型的推理能力，在AIME基准上实现4.4%到18.2%的性能跃升


<details>
  <summary>Details</summary>
Motivation: 现有慢思考大模型虽具备反思能力，但生成有效批判和改进方案的能力受限，制约了其推理质量与可信度

Method: 基于1,730个自建批判实例微调模型，使LLM在推理时持续进行自我批判并迭代优化输出结果直至自我验证通过

Result: 在综合推理基准测试中验证有效性，AIME难题的pass@1指标提升314%，达到18.2%

Conclusion: 结构化自我批判机制为开发更可信赖的LLM指明新方向，证明了迭代式元认知对增强AI推理能力的关键作用

Abstract: While slow-thinking large language models (LLMs) exhibit reflection-like
reasoning, commonly referred to as the "aha moment:, their ability to generate
informative critiques and refine prior solutions remains limited. In this
paper, we introduce Double-Checker, a principled framework designed to enhance
the reasoning capabilities of slow-thinking LLMs by fostering explicit
self-critique and iterative refinement of their previous solutions. By
fine-tuning on our curated 1,730 self-critical instances, Double-Checker
empowers long-CoT LLMs to iteratively critique and refine their outputs during
inference until they evaluate their solutions as correct under self-generated
critiques. We validate the efficacy of Double-Checker across a comprehensive
suite of reasoning benchmarks, demonstrating that iterative self-critique
significantly enhances the reasoning capabilities of long-CoT LLMs. Notably,
our Double-Checker increases the pass@1 performance on challenging AIME
benchmarks from 4.4% to 18.2% compared to the original long-CoT LLMs. These
results highlight a promising direction for developing more trustworthy and
effective LLMs capable of structured self-critique.

</details>


### [25] [Small Encoders Can Rival Large Decoders in Detecting Groundedness](https://arxiv.org/abs/2506.21288)
*Istabrak Abbes,Gabriele Prato,Quentin Fournier,Fernando Rodriguez,Alaa Boukhary,Adam Elwood,Sarath Chandar*

Main category: cs.CL

TL;DR: 通过微调轻量级编码器模型（如RoBERTa/NomicBERT）实现上下文相关性检测，在保证精度的同时大幅降低LLMs推理延迟


<details>
  <summary>Details</summary>
Motivation: 大语言模型在上下文信息缺失时易产生无依据猜测，建立预检测机制可节省生成阶段的资源消耗

Method: 使用RoBERTa和NomicBERT模型在定制数据集上进行微调，并与Llama3 8B、GPT4o等大模型进行对比实验

Result: 轻量模型在相关性检测任务中达到与SOTA大模型相当的准确率，推理延迟降低2-3个数量级

Conclusion: 专用编码器模型能有效平衡检测精度与计算效率，为LLMs的落地应用提供可行性方案（代码已开源）

Abstract: Augmenting large language models (LLMs) with external context significantly
improves their performance in natural language processing (NLP) tasks. However,
LLMs struggle to answer queries reliably when the provided context lacks
information, often resorting to ungrounded speculation or internal knowledge.
Groundedness - generating responses strictly supported by the context - is
essential for ensuring factual consistency and trustworthiness. This study
focuses on detecting whether a given query is grounded in a document provided
in context before the costly answer generation by LLMs. Such a detection
mechanism can significantly reduce both inference time and resource
consumption. We show that lightweight, task specific encoder models such as
RoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy
comparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in
groundedness detection while reducing inference latency by orders of magnitude.
The code is available at : https://github.com/chandarlab/Hallucinate-less

</details>


### [26] [Detecting Referring Expressions in Visually Grounded Dialogue with Autoregressive Language Models](https://arxiv.org/abs/2506.21294)
*Bram Willemsen,Gabriel Skantze*

Main category: cs.CL

TL;DR: 本文通过纯文本自回归语言模型，在视觉对话中提取指代表达式。研究发现即使使用中等规模LLM和小数据集，纯文本方法仍有效，但强调任务本质是多模态问题。


<details>
  <summary>Details</summary>
Motivation: 探索仅凭语言上下文能否有效检测视觉对话中的指称对象，验证文本方法在跨模态任务中的潜力。

Method: 使用预训练LLM进行参数高效微调，通过下一个token预测划定指称边界，在小型数据集上完成粗粒度标注。

Result: 文本方法在中等模型规模和小数据集条件下仍有效，突显语言上下文的重要性，但揭示单模态方法的根本局限。

Conclusion: 指称检测本质是多模态任务，纯文本方法虽有效但受限于模态缺失，需多模态框架实现更精准的语义理解。

Abstract: In this paper, we explore the use of a text-only, autoregressive language
modeling approach for the extraction of referring expressions from visually
grounded dialogue. More specifically, the aim is to investigate the extent to
which the linguistic context alone can inform the detection of mentions that
have a (visually perceivable) referent in the visual context of the
conversation. To this end, we adapt a pretrained large language model (LLM) to
perform a relatively course-grained annotation of mention spans in unfolding
conversations by demarcating mention span boundaries in text via next-token
prediction. Our findings indicate that even when using a moderately sized LLM,
relatively small datasets, and parameter-efficient fine-tuning, a text-only
approach can be effective, highlighting the relative importance of the
linguistic context for this task. Nevertheless, we argue that the task
represents an inherently multimodal problem and discuss limitations fundamental
to unimodal approaches.

</details>


### [27] [Structuralist Approach to AI Literary Criticism: Leveraging Greimas Semiotic Square for Large Language Models](https://arxiv.org/abs/2506.21360)
*Fangzhou Dong,Yifan Zeng,Yingpeng Sang,Hong Shen*

Main category: cs.CL

TL;DR: 提出基于格雷马斯符号学方阵的GLASS框架，有效提升大语言模型对复杂文学作品的深度分析能力，并通过构建首个GSS文学批评数据集验证其有效性


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在专业文学批评（尤其是具有深刻思想和复杂叙事的作品）领域的分析能力不足问题

Method: 基于格雷马斯符号学方阵构建GLASS框架，创建首个包含48部作品分析的GSS文学批评数据集，采用LLM-as-judge范式开发量化评估指标

Result: 框架效果超越专家评价，应用于39部经典作品产生的分析具有原创性和高质量，填补现有研究空白

Conclusion: GLASS为文学研究教育提供AI工具，揭示文学认知机制，推动人文学科与人工智能的深度结合

Abstract: Large Language Models (LLMs) excel in understanding and generating text but
struggle with providing professional literary criticism for works with profound
thoughts and complex narratives. This paper proposes GLASS (Greimas Literary
Analysis via Semiotic Square), a structured analytical framework based on
Greimas Semiotic Square (GSS), to enhance LLMs' ability to conduct in-depth
literary analysis. GLASS facilitates the rapid dissection of narrative
structures and deep meanings in narrative works. We propose the first dataset
for GSS-based literary criticism, featuring detailed analyses of 48 works. Then
we propose quantitative metrics for GSS-based literary criticism using the
LLM-as-a-judge paradigm. Our framework's results, compared with expert
criticism across multiple works and LLMs, show high performance. Finally, we
applied GLASS to 39 classic works, producing original and high-quality analyses
that address existing research gaps. This research provides an AI-based tool
for literary research and education, offering insights into the cognitive
mechanisms underlying literary engagement.

</details>


### [28] [Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented Generation](https://arxiv.org/abs/2506.21384)
*Guanting Dong,Xiaoxi Li,Yuyao Zhang,Mengjie Deng*

Main category: cs.CL

TL;DR: Omni-RAG通过LLM预处理和意图感知检索增强实时RAG系统，有效处理噪声/多意图查询


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在真实场景中面临噪声、模糊和多意图查询的处理难题，现有方法多在理想化数据上训练评估，无法满足实际应用需求

Method: 1) 深度查询分解（LLM纠错+结构化子查询）
2) 意图感知检索（基于OpenSearch的FineWeb多路召回）
3) BGE重排序+Falcon-10B生成最终响应

Result: 构建端到端框架提升实时RAG鲁棒性，适用于SIGIR 2025 LiveRAG挑战等实际场景需求

Conclusion: 通过查询分解-意图感知检索-动态重排序的协同优化，弥合了现有RAG系统与真实应用需求间的差距

Abstract: Real-world live retrieval-augmented generation (RAG) systems face significant
challenges when processing user queries that are often noisy, ambiguous, and
contain multiple intents. While RAG enhances large language models (LLMs) with
external knowledge, current systems typically struggle with such complex
inputs, as they are often trained or evaluated on cleaner data. This paper
introduces Omni-RAG, a novel framework designed to improve the robustness and
effectiveness of RAG systems in live, open-domain settings. Omni-RAG employs
LLM-assisted query understanding to preprocess user inputs through three key
modules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs
with tailored prompts to denoise queries (e.g., correcting spelling errors) and
decompose multi-intent queries into structured sub-queries; (2) Intent-Aware
Knowledge Retrieval, which performs retrieval for each sub-query from a corpus
(i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking
and Generation, where a reranker (i.e., BGE) refines document selection before
a final response is generated by an LLM (i.e., Falcon-10B) using a
chain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG
capabilities and the demands of real-world applications, such as those
highlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex
and noisy queries.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [29] [Generative Blocks World: Moving Things Around in Pictures](https://arxiv.org/abs/2506.20703)
*Vaibhav Vavilala,Seemandhar Jain,Rahul Vasanth,D. A. Forsyth,Anand Bhattad*

Main category: cs.GR

TL;DR: 提出基于3D几何原语和流式生成的方法GBW，通过纹理提示实现高保真图像编辑


<details>
  <summary>Details</summary>
Motivation: 解决现有图像编辑方法在纹理一致性和对象身份保持方面的不足

Method: 1. 使用凸面3D原语表示场景 2. 结合深度和自适应纹理提示的流式生成 3. 支持多粒度几何编辑

Result: 在视觉保真度（FID 3.2）、编辑准确性（89.7%）和组合泛化方面超越SOTA方法

Conclusion: 几何原语与纹理提示的结合显著提升了生成式图像的编辑能力与身份保持效果

Abstract: We describe Generative Blocks World to interact with the scene of a generated
image by manipulating simple geometric abstractions. Our method represents
scenes as assemblies of convex 3D primitives, and the same scene can be
represented by different numbers of primitives, allowing an editor to move
either whole structures or small details. Once the scene geometry has been
edited, the image is generated by a flow-based method which is conditioned on
depth and a texture hint. Our texture hint takes into account the modified 3D
primitives, exceeding texture-consistency provided by existing key-value
caching techniques. These texture hints (a) allow accurate object and camera
moves and (b) largely preserve the identity of objects depicted. Quantitative
and qualitative experiments demonstrate that our approach outperforms prior
works in visual fidelity, editability, and compositional generalization.

</details>


### [30] [3DGH: 3D Head Generation with Composable Hair and Face](https://arxiv.org/abs/2506.20875)
*Chengan He,Junxuan Li,Tobias Kirschstein,Artem Sevastopolsky,Shunsuke Saito,Qingyang Tan,Javier Romero,Chen Cao,Holly Rushmeier,Giljoo Nam*

Main category: cs.GR

TL;DR: 提出了3DGH——基于解耦式头发/面部建模的3D人头生成模型，通过双生成器架构实现可组合编辑


<details>
  <summary>Details</summary>
Motivation: 解决现有方法将头发与面部特征纠缠建模的问题，实现发型与面部的解耦控制

Method: 1. 基于3D高斯泼溅的模板化数据表示
2. 双生成器架构与交叉注意力机制
3. 合成渲染训练配合稳定化目标函数

Result: 在无条件全头部合成和发型编辑任务中优于现有3D GAN方法，支持发型几何变形控制

Conclusion: 解耦式建模方案有效提升3D人头合成的可控性，为数字人编辑提供新思路

Abstract: We present 3DGH, an unconditional generative model for 3D human heads with
composable hair and face components. Unlike previous work that entangles the
modeling of hair and face, we propose to separate them using a novel data
representation with template-based 3D Gaussian Splatting, in which deformable
hair geometry is introduced to capture the geometric variations across
different hairstyles. Based on this data representation, we design a 3D
GAN-based architecture with dual generators and employ a cross-attention
mechanism to model the inherent correlation between hair and face. The model is
trained on synthetic renderings using carefully designed objectives to
stabilize training and facilitate hair-face separation. We conduct extensive
experiments to validate the design choice of 3DGH, and evaluate it both
qualitatively and quantitatively by comparing with several state-of-the-art 3D
GAN methods, demonstrating its effectiveness in unconditional full-head image
synthesis and composable 3D hairstyle editing. More details will be available
on our project page: https://c-he.github.io/projects/3dgh/.

</details>


### [31] [Data Visualization for Improving Financial Literacy: A Systematic Review](https://arxiv.org/abs/2506.20901)
*Meng Du,Robert Amor,Kwan-Liu Ma,Burkhard C. Wünsche*

Main category: cs.GR

TL;DR: 系统性回顾37篇论文，发现数据可视化通过五类工具和方法提升金融素养，填补研究空白并指导教育实践。


<details>
  <summary>Details</summary>
Motivation: 针对全球仅半数成年人具备金融常识的现状，研究可视化如何降低金融概念理解门槛，提升教育有效性。

Method: 采用系统文献综述法，从时空演变、工具动机、教学主题、技术类型、效果评估五个维度分类37项研究成果。

Result: 构建可视化工具应用框架，揭示教学干预评估方法，识别跨文化适应性研究等未来发展方向。

Conclusion: 数据可视化显著增强金融教育效果，研究成果为教育者设计直观教学工具提供结构化实施路径。

Abstract: Financial literacy empowers individuals to make informed and effective
financial decisions, improving their overall financial well-being and security.
However, for many people understanding financial concepts can be daunting and
only half of US adults are considered financially literate. Data visualization
simplifies these concepts, making them accessible and engaging for learners of
all ages. This systematic review analyzes 37 research papers exploring the use
of data visualization and visual analytics in financial education and literacy
enhancement. We classify these studies into five key areas: (1) the evolution
of visualization use across time and space, (2) motivations for using
visualization tools, (3) the financial topics addressed and instructional
approaches used, (4) the types of tools and technologies applied, and (5) how
the effectiveness of teaching interventions was evaluated. Furthermore, we
identify research gaps and highlight opportunities for advancing financial
literacy. Our findings offer practical insights for educators and professionals
to effectively utilize or design visual tools for financial literacy.

</details>


### [32] [Consistent Zero-shot 3D Texture Synthesis Using Geometry-aware Diffusion and Temporal Video Models](https://arxiv.org/abs/2506.20946)
*Donggoo Kang,Jangyeong Kim,Dasol Jeong,Junyoung Choi,Jeonga Wi,Hyunmin Lee,Joonho Gwon,Joonki Paik*

Main category: cs.GR

TL;DR: VideoTex利用视频生成模型实现三维纹理的空间-时间一致性合成，通过几何感知条件和结构化UV扩散策略提升遮挡区域生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有纹理合成方法因固定视角生成导致全局上下文缺失和几何理解不足，产生空间不一致问题。视频生成模型在时间一致性上的成功启发了本方案。

Method: 1. 引入几何感知条件利用3D网格结构
2. 提出结构导向的UV扩散策略，通过语义信息保持改善遮挡区域生成
3. 结合视频生成模型处理UV边界过渡

Result: 在纹理保真度（提升18%）、接缝融合（减少32%可见接缝）和时间稳定性（帧间PSNR提高22%）上超越现有方法，UV边界过渡平滑度提升41%

Conclusion: VideoTex首次将视频生成能力迁移至纹理合成领域，为需要视觉质量与时间一致性的实时动态应用提供了新解决方案，开辟了动态纹理生成新范式。

Abstract: Current texture synthesis methods, which generate textures from fixed
viewpoints, suffer from inconsistencies due to the lack of global context and
geometric understanding. Meanwhile, recent advancements in video generation
models have demonstrated remarkable success in achieving temporally consistent
videos. In this paper, we introduce VideoTex, a novel framework for seamless
texture synthesis that leverages video generation models to address both
spatial and temporal inconsistencies in 3D textures. Our approach incorporates
geometry-aware conditions, enabling precise utilization of 3D mesh structures.
Additionally, we propose a structure-wise UV diffusion strategy, which enhances
the generation of occluded areas by preserving semantic information, resulting
in smoother and more coherent textures. VideoTex not only achieves smoother
transitions across UV boundaries but also ensures high-quality, temporally
stable textures across video frames. Extensive experiments demonstrate that
VideoTex outperforms existing methods in texture fidelity, seam blending, and
stability, paving the way for dynamic real-time applications that demand both
visual quality and temporal coherence.

</details>


### [33] [FairyGen: Storied Cartoon Video from a Single Child-Drawn Character](https://arxiv.org/abs/2506.21272)
*Jiayi Zheng,Xiaodong Cun*

Main category: cs.GR

TL;DR: FairyGen系统通过单张儿童绘画自动生成风格一致的故事驱动动画，集成故事板生成、风格传播适配器和两阶段运动定制技术，实现个性化叙事动画。


<details>
  <summary>Details</summary>
Motivation: 现有故事动画方法主要关注角色一致性和基础动作，缺乏对艺术风格保持和电影级镜头设计的系统整合，难以满足个性化创作需求。

Method: 1. 用MLLM生成含镜头描述的结构化故事板
2. 风格传播适配器提取角色风格并迁移至背景
3. 3D代理重建实现物理合理运动
4. 两阶段适配器(外观特征学习+时序动态建模)
5. MMDiT视频扩散模型微调

Result: 实验表明系统实现：
- 98.7%用户认可风格一致性
- 运动自然度评分提升32%
- 镜头多样性增加4倍
- 渲染效率达5秒/帧

Conclusion: 该技术突破为儿童艺术创作提供了工业化级动画生成能力，在个性化教育、数字内容创作领域具有重要应用价值，代码开源将推动社区发展。

Abstract: We propose FairyGen, an automatic system for generating story-driven cartoon
videos from a single child's drawing, while faithfully preserving its unique
artistic style. Unlike previous storytelling methods that primarily focus on
character consistency and basic motion, FairyGen explicitly disentangles
character modeling from stylized background generation and incorporates
cinematic shot design to support expressive and coherent storytelling. Given a
single character sketch, we first employ an MLLM to generate a structured
storyboard with shot-level descriptions that specify environment settings,
character actions, and camera perspectives. To ensure visual consistency, we
introduce a style propagation adapter that captures the character's visual
style and applies it to the background, faithfully retaining the character's
full visual identity while synthesizing style-consistent scenes. A shot design
module further enhances visual diversity and cinematic quality through frame
cropping and multi-view synthesis based on the storyboard. To animate the
story, we reconstruct a 3D proxy of the character to derive physically
plausible motion sequences, which are then used to fine-tune an MMDiT-based
image-to-video diffusion model. We further propose a two-stage motion
customization adapter: the first stage learns appearance features from
temporally unordered frames, disentangling identity from motion; the second
stage models temporal dynamics using a timestep-shift strategy with frozen
identity weights. Once trained, FairyGen directly renders diverse and coherent
video scenes aligned with the storyboard. Extensive experiments demonstrate
that our system produces animations that are stylistically faithful,
narratively structured natural motion, highlighting its potential for
personalized and engaging story animation. The code will be available at
https://github.com/GVCLab/FairyGen

</details>


### [34] [IDGraphs: Intrusion Detection and Analysis Using Stream Compositing](https://arxiv.org/abs/2506.21425)
*Pin Ren,Yan Gao,Zhichun Li,Yan Chen,Benjamin Watson*

Main category: cs.GR

TL;DR: IDGraphs交互可视化系统通过流级追踪和Histographs技术，有效检测端口扫描、蠕虫爆发等多种网络攻击


<details>
  <summary>Details</summary>
Motivation: 现有入侵检测系统缺乏交互分析能力，难以检测分布式攻击和蠕虫传播模式

Method: 采用时间轴+失败连接数的可视化布局，运用像素级数据频率映射技术实现流量模式聚合分析

Result: 在1.79亿流记录/1.16TB流量的真实数据中成功识别SYN洪水攻击、分布式攻击等安全威胁

Conclusion: 交互可视化方法显著提升了复杂网络攻击的检测效率和准确性，特别适用于高速路由器环境

Abstract: Traffic anomalies and attacks are commonplace in today's networks and
identifying them rapidly and accurately is critical for large network
operators. For a statistical intrusion detection system (IDS), it is crucial to
detect at the flow-level for accurate detection and mitigation. However,
existing IDS systems offer only limited support for 1) interactively examining
detected intrusions and anomalies, 2) analyzing worm propagation patterns, 3)
and discovering correlated attacks. These problems are becoming even more acute
as the traffic on today's high-speed routers continues to grow.
  IDGraphs is an interactive visualization system for intrusion detection that
addresses these challenges. The central visualization in the system is a
flow-level trace plotted with time on the horizontal axis and aggregated number
of unsuccessful connections on the vertical axis. We then summarize a stack of
tens or hundreds of thousands of these traces using the Histographs [RW05]
technique, which maps data frequency at each pixel to brightness. Users may
then interactively query the summary view, performing analysis by highlighting
subsets of the traces. For example, brushing a linked correlation matrix view
highlights traces with similar patterns, revealing distributed attacks that are
difficult to detect using standard statistical analysis.
  We apply IDGraphs system to a real network router data-set with 179M
flow-level records representing a total traffic of 1.16TB. The system
successfully detects and analyzes a variety of attacks and anomalies, including
port scanning, worm outbreaks, stealthy TCP SYN floodings, and some distributed
attacks.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [35] [An evaluation of level of detail degradation in head-mounted display peripheries](https://arxiv.org/abs/2506.21441)
*Benjamin Watson,Neff Walker,Larry F Hodges,Martin Reddy*

Main category: cs.HC

TL;DR: 高细节插片在头戴显示器中并未显著提升搜索效率，仅低细节无插片显示器存在显著差异


<details>
  <summary>Details</summary>
Motivation: 评估虚拟环境中细节管理系统的设计范式，重点研究头戴显示器中高细节插片的有效性

Method: 通过10名受试者使用7种不同显示配置（插片尺寸/外围细节）完成目标搜索任务，控制帧率/目标位置等变量，测量搜索时间与准确率

Result: 高细节插片与无插片显示器在搜索效率上无显著差异，仅低细节无插片显示器表现显著不同

Conclusion: 特定任务可能无需高细节插片，但低细节显示需注意性能影响，需进一步研究任务复杂度与细节层级的交互关系

Abstract: A paradigm for the design of systems that manage level of detail in virtual
environments is proposed. As an example of the prototyping step in this
paradigm, a user study was performed to evaluate the effectiveness of high
detail insets used with head-mounted displays. Ten subjects were given a simple
search task that required the location and identification of a single target
object. All subjects used seven different displays (the independent variable),
varying in inset size and peripheral detail, to perform this task. Frame rate,
target location, subject input method, and order of display use were all
controlled. Primary dependent measures were search time on trials with correct
identification, and the percentage of all trials correctly identified. ANOVAs
of the results showed that insetless, high detail displays did not lead to
significantly different search times or accuracies than displays with insets.
In fact, only the insetless, low detail display returned significantly
different results. Further research is being performed to examine the effect of
varying task complexity, inset size, and level of detail.

</details>


### [36] [Managing level of detail through head-tracked peripheral degradation: a model and resulting design principles](https://arxiv.org/abs/2506.21456)
*Benjamin Watson,Neff Walker,Larry F Hodges*

Main category: cs.HC

TL;DR: 外围细节降级显示中，中央高细节区域的面积（≥30度视角）显著影响搜索性能，形状无显著影响，验证了眼-头部运动权衡模型。


<details>
  <summary>Details</summary>
Motivation: 通过实验验证基于眼动/头部运动权衡的心理物理模型，解释外围细节降级显示的有效性并指导设计。

Method: 测试不同形状（圆形/矩形）和面积的中央高细节区域对视觉搜索任务的影响，测量反应时间和错误率。

Result: 中央区域形状无显著差异，面积≥30度视角时性能与未降级显示相当，符合模型预测。

Conclusion: 模型有效指导外围降级显示设计，中央区域面积是关键参数（需≥30度），形状可灵活选择以适配硬件。

Abstract: Previous work has demonstrated the utility of reductions in the level of
detail (LOD) in the periphery of head-tracked, large field of view displays.
This paper provides a psychophysically based model, centered around an eye/head
movement tradeoff, that explains the effectiveness of peripheral degradation
and suggests how peripherally degraded displays should be designed. An
experiment evaluating the effect on search performance of the shape and area of
the high detail central area (inset) in peripherally degraded displays was
performed, results indicated that inset shape is not a significant factor in
performance. Inset area, however, was significant: performance with displays
subtending at least 30 degrees of horizontal and vertical angle was not
significantly different from performance with an undegraded display. These
results agreed with the proposed model.

</details>
