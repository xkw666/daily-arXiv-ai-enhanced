{"id": "2510.03308", "pdf": "https://arxiv.org/pdf/2510.03308", "abs": "https://arxiv.org/abs/2510.03308", "authors": ["Jiong Lin", "Jialong Ning", "Judah Goldfeder", "Hod Lipson"], "title": "Creative synthesis of kinematic mechanisms", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "6pages, 6 figures", "summary": "In this paper, we formulate the problem of kinematic synthesis for planar\nlinkages as a cross-domain image generation task. We develop a planar linkages\ndataset using RGB image representations, covering a range of mechanisms: from\nsimple types such as crank-rocker and crank-slider to more complex eight-bar\nlinkages like Jansen's mechanism. A shared-latent variational autoencoder (VAE)\nis employed to explore the potential of image generative models for\nsynthesizing unseen motion curves and simulating novel kinematics. By encoding\nthe drawing speed of trajectory points as color gradients, the same\narchitecture also supports kinematic synthesis conditioned on both trajectory\nshape and velocity profiles. We validate our method on three datasets of\nincreasing complexity: a standard four-bar linkage set, a mixed set of four-bar\nand crank-slider mechanisms, and a complex set including multi-loop mechanisms.\nPreliminary results demonstrate the effectiveness of image-based\nrepresentations for generative mechanical design, showing that mechanisms with\nrevolute and prismatic joints, and potentially cams and gears, can be\nrepresented and synthesized within a unified image generation framework.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u5e73\u9762\u8fde\u6746\u673a\u6784\u8fd0\u52a8\u7efc\u5408\u65b9\u6cd5\uff0c\u5229\u7528VAE\u6a21\u578b\u5b9e\u73b0\u8f68\u8ff9\u4e0e\u901f\u5ea6\u53cc\u6761\u4ef6\u7684\u673a\u6784\u751f\u6210\uff0c\u9002\u7528\u4e8e\u4ece\u56db\u6746\u5230\u591a\u73af\u673a\u6784\u7684\u7edf\u4e00\u8bbe\u8ba1\u6846\u67b6\u3002", "motivation": "\u4f20\u7edf\u673a\u6784\u8bbe\u8ba1\u65b9\u6cd5\u96be\u4ee5\u5feb\u901f\u751f\u6210\u6ee1\u8db3\u590d\u6742\u8fd0\u52a8\u8981\u6c42\u7684\u65b9\u6848\uff0c\u9700\u63a2\u7d22\u8de8\u57df\u751f\u6210\u6a21\u578b\u5728\u673a\u68b0\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u5efa\u7acbRGB\u56fe\u50cf\u6570\u636e\u96c6\u8868\u5f81\u673a\u6784\u8f68\u8ff9\uff0c\u91c7\u7528\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4VAE\u6a21\u578b\uff0c\u901a\u8fc7\u989c\u8272\u68af\u5ea6\u7f16\u7801\u901f\u5ea6\u4fe1\u606f\uff0c\u5728\u56db\u6746/\u6df7\u5408/\u591a\u73af\u673a\u6784\u6570\u636e\u96c6\u9a8c\u8bc1\u3002", "result": "\u6210\u529f\u751f\u6210Jansen\u516b\u6746\u7b49\u590d\u6742\u673a\u6784\uff0c\u9a8c\u8bc1\u4e86\u56fe\u50cf\u751f\u6210\u6846\u67b6\u5bf9\u542b\u8f6c\u52a8\u526f\u3001\u79fb\u52a8\u526f\u53ca\u591a\u73af\u673a\u6784\u7684\u7edf\u4e00\u5efa\u6a21\u80fd\u529b\u3002", "conclusion": "\u56fe\u50cf\u8868\u5f81\u4e3a\u751f\u6210\u5f0f\u673a\u68b0\u8bbe\u8ba1\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u9f7f\u8f6e\u3001\u51f8\u8f6e\u7b49\u591a\u7c7b\u578b\u673a\u6784\u7684\u7edf\u4e00\u751f\u6210\u6846\u67b6\u3002"}}
{"id": "2510.03312", "pdf": "https://arxiv.org/pdf/2510.03312", "abs": "https://arxiv.org/abs/2510.03312", "authors": ["Rong Liu", "Zhongpai Gao", "Benjamin Planche", "Meida Chen", "Van Nguyen Nguyen", "Meng Zheng", "Anwesa Choudhuri", "Terrence Chen", "Yue Wang", "Andrew Feng", "Ziyan Wu"], "title": "Universal Beta Splatting", "categories": ["cs.GR", "cs.CV", "eess.IV"], "comment": null, "summary": "We introduce Universal Beta Splatting (UBS), a unified framework that\ngeneralizes 3D Gaussian Splatting to N-dimensional anisotropic Beta kernels for\nexplicit radiance field rendering. Unlike fixed Gaussian primitives, Beta\nkernels enable controllable dependency modeling across spatial, angular, and\ntemporal dimensions within a single representation. Our unified approach\ncaptures complex light transport effects, handles anisotropic view-dependent\nappearance, and models scene dynamics without requiring auxiliary networks or\nspecific color encodings. UBS maintains backward compatibility by approximating\nto Gaussian Splatting as a special case, guaranteeing plug-in usability and\nlower performance bounds. The learned Beta parameters naturally decompose scene\nproperties into interpretable without explicit supervision: spatial (surface\nvs. texture), angular (diffuse vs. specular), and temporal (static vs.\ndynamic). Our CUDA-accelerated implementation achieves real-time rendering\nwhile consistently outperforming existing methods across static,\nview-dependent, and dynamic benchmarks, establishing Beta kernels as a scalable\nuniversal primitive for radiance field rendering. Our project website is\navailable at https://rongliu-leo.github.io/universal-beta-splatting/.", "AI": {"tldr": "UBS\u901a\u8fc7Beta\u6838\u6269\u5c55\u9ad8\u65af\u55b7\u6d12\uff0c\u5b9e\u73b0\u591a\u7ef4\u5ea6\u573a\u666f\u5efa\u6a21\u548c\u5b9e\u65f6\u6e32\u67d3", "motivation": "\u4f20\u7edf\u9ad8\u65af\u55b7\u6d12\u4f7f\u7528\u56fa\u5b9a\u5f62\u72b6\u57fa\u5143\uff0c\u96be\u4ee5\u5efa\u6a21\u590d\u6742\u5149\u4f20\u8f93\u6548\u5e94\u548c\u52a8\u6001\u573a\u666f\u3002UBS\u65e8\u5728\u901a\u8fc7\u53ef\u8c03\u8282\u7684Beta\u6838\u7edf\u4e00\u5904\u7406\u7a7a\u95f4/\u89d2\u5ea6/\u65f6\u95f4\u7ef4\u5ea6\u4f9d\u8d56\u5173\u7cfb", "method": "1. \u63d0\u51faN\u7ef4\u5404\u5411\u5f02\u6027Beta\u6838\u4f5c\u4e3a\u901a\u7528\u57fa\u5143 2. \u901a\u8fc7\u53c2\u6570\u63a7\u5236\u5b9e\u73b0\u7a7a\u95f4-\u89d2\u5ea6-\u65f6\u95f4\u8054\u5408\u5efa\u6a21 3. \u4fdd\u6301\u4e0e\u9ad8\u65af\u55b7\u6d12\u7684\u517c\u5bb9\u6027 4. CUDA\u52a0\u901f\u5b9e\u73b0\u5b9e\u65f6\u6e32\u67d3", "result": "\u5728\u9759\u6001/\u89c6\u89d2\u4f9d\u8d56/\u52a8\u6001\u573a\u666f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5168\u9762\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u5b9e\u65f6\u6e32\u67d3\u6027\u80fd\uff08~30fps\uff09", "conclusion": "Beta\u6838\u5c55\u73b0\u51fa\u4f5c\u4e3a\u8f90\u5c04\u573a\u901a\u7528\u57fa\u5143\u7684\u6f5c\u529b\uff0c\u5176\u53c2\u6570\u81ea\u7136\u5206\u89e3\u7279\u6027\u4e3a\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387"}}
{"id": "2510.03433", "pdf": "https://arxiv.org/pdf/2510.03433", "abs": "https://arxiv.org/abs/2510.03433", "authors": ["\u00c1ron Samuel Kov\u00e1cs", "Pedro Hermosilla", "Renata G. Raidou"], "title": "Style Brush: Guided Style Transfer for 3D Objects", "categories": ["cs.GR"], "comment": null, "summary": "We introduce Style Brush, a novel style transfer method for textured meshes\ndesigned to empower artists with fine-grained control over the stylization\nprocess. Our approach extends traditional 3D style transfer methods by\nintroducing a novel loss function that captures style directionality, supports\nmultiple style images or portions thereof, and enables smooth transitions\nbetween styles in the synthesized texture. The use of easily generated guiding\ntextures streamlines user interaction, making our approach accessible to a\nbroad audience. Extensive evaluations with various meshes, style images, and\ncontour shapes demonstrate the flexibility of our method and showcase the\nvisual appeal of the generated textures.", "AI": {"tldr": "\u63d0\u51faStyle Brush\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u7eb9\u7406\u7f51\u683c\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u65b9\u5411\u6027\u635f\u5931\u51fd\u6570\u5b9e\u73b0\u591a\u98ce\u683c\u878d\u5408\u4e0e\u5e73\u6ed1\u8fc7\u6e21\uff0c\u7b80\u5316\u7528\u6237\u4ea4\u4e92\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf3D\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u63a7\u5236\u3001\u591a\u98ce\u683c\u878d\u5408\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u964d\u4f4e\u827a\u672f\u521b\u4f5c\u95e8\u69db\u3002", "method": "\u8bbe\u8ba1\u65b9\u5411\u6027\u635f\u5931\u51fd\u6570\u652f\u6301\u591a\u98ce\u683c\u56fe\u50cf\u5c40\u90e8\u5e94\u7528\uff0c\u5f00\u53d1\u5f15\u5bfc\u7eb9\u7406\u7b80\u5316\u7528\u6237\u63a7\u5236\uff0c\u5b9e\u73b0\u98ce\u683c\u95f4\u7684\u81ea\u7136\u8fc7\u6e21\u3002", "result": "\u591a\u573a\u666f\u9a8c\u8bc1\u663e\u793a\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u89c6\u89c9\u5438\u5f15\u529b\u5f3a\u7684\u7eb9\u7406\uff0c\u652f\u6301\u4e0d\u540c\u7f51\u683c/\u98ce\u683c/\u8f6e\u5ed3\u7684\u7075\u6d3b\u7ec4\u5408\u3002", "conclusion": "\u901a\u8fc7\u521b\u65b0\u635f\u5931\u51fd\u6570\u4e0e\u4ea4\u4e92\u8bbe\u8ba1\uff0c\u4e3a\u827a\u672f\u5bb6\u63d0\u4f9b\u9ad8\u53ef\u63a7\u6027\u3001\u6613\u7528\u6027\u5f3a\u76843D\u98ce\u683c\u8fc1\u79fb\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03434", "pdf": "https://arxiv.org/pdf/2510.03434", "abs": "https://arxiv.org/abs/2510.03434", "authors": ["Zhiying Jiang", "Raihan Seraj", "Marcos Villagra", "Bidhan Roy"], "title": "Paris: A Decentralized Trained Open-Weight Diffusion Model", "categories": ["cs.GR", "cs.DC", "cs.LG"], "comment": null, "summary": "We present Paris, the first publicly released diffusion model pre-trained\nentirely through decentralized computation. Paris demonstrates that\nhigh-quality text-to-image generation can be achieved without centrally\ncoordinated infrastructure. Paris is open for research and commercial use.\nParis required implementing our Distributed Diffusion Training framework from\nscratch. The model consists of 8 expert diffusion models (129M-605M parameters\neach) trained in complete isolation with no gradient, parameter, or\nintermediate activation synchronization. Rather than requiring synchronized\ngradient updates across thousands of GPUs, we partition data into semantically\ncoherent clusters where each expert independently optimizes its subset while\ncollectively approximating the full distribution. A lightweight transformer\nrouter dynamically selects appropriate experts at inference, achieving\ngeneration quality comparable to centrally coordinated baselines. Eliminating\nsynchronization enables training on heterogeneous hardware without specialized\ninterconnects. Empirical validation confirms that Paris's decentralized\ntraining maintains generation quality while removing the dedicated GPU cluster\nrequirement for large-scale diffusion models. Paris achieves this using\n14$\\times$ less training data and 16$\\times$ less compute than the prior\ndecentralized baseline.", "AI": {"tldr": "Paris\u662f\u9996\u4e2a\u5b8c\u5168\u901a\u8fc7\u53bb\u4e2d\u5fc3\u5316\u8ba1\u7b97\u9884\u8bad\u7ec3\u4e14\u516c\u5f00\u7684\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u8bad\u7ec3\u6846\u67b6\u5b9e\u73b0\u4e0e\u96c6\u4e2d\u5f0f\u8bad\u7ec3\u76f8\u5f53\u7684\u751f\u6210\u8d28\u91cf\uff0c\u540c\u65f6\u51cf\u5c1114\u500d\u6570\u636e\u548c16\u500d\u7b97\u529b\u6d88\u8017\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6269\u6563\u6a21\u578b\u4f9d\u8d56\u96c6\u4e2d\u5f0f\u57fa\u7840\u8bbe\u65bd\uff08\u9700\u6570\u5343GPU\u540c\u6b65\u66f4\u65b0\uff09\u7684\u9650\u5236\uff0c\u63a2\u7d22\u53bb\u4e2d\u5fc3\u5316\u8bad\u7ec3\u65b9\u6848\u4ee5\u964d\u4f4e\u786c\u4ef6\u90e8\u7f72\u95e8\u69db\u3002", "method": "1. \u5f00\u53d1\u5206\u5e03\u5f0f\u6269\u6563\u8bad\u7ec3\u6846\u67b6\n2. \u5c06\u6570\u636e\u5212\u5206\u4e3a\u8bed\u4e49\u805a\u7c7b\uff0c8\u4e2a\u4e13\u5bb6\u6a21\u578b\u72ec\u7acb\u8bad\u7ec3\uff08\u65e0\u68af\u5ea6/\u53c2\u6570\u540c\u6b65\uff09\n3. \u8f7b\u91cftransformer\u8def\u7531\u5668\u52a8\u6001\u8c03\u5ea6\u4e13\u5bb6\u6a21\u578b", "result": "\u751f\u6210\u8d28\u91cf\u5ab2\u7f8e\u96c6\u4e2d\u5f0f\u57fa\u7ebf\uff0c\u652f\u6301\u5f02\u6784\u786c\u4ef6\u8bad\u7ec3\uff0c\u8bad\u7ec3\u6570\u636e\u91cf\u51cf\u5c1114\u500d\uff0c\u8ba1\u7b97\u91cf\u51cf\u5c1116\u500d\u3002", "conclusion": "\u53bb\u4e2d\u5fc3\u5316\u8bad\u7ec3\u65b9\u6848\u6709\u6548\u7a81\u7834\u5927\u89c4\u6a21\u6269\u6563\u6a21\u578b\u5bf9\u4e13\u7528GPU\u96c6\u7fa4\u7684\u4f9d\u8d56\uff0c\u4e3a\u5206\u5e03\u5f0f\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2510.03315", "pdf": "https://arxiv.org/pdf/2510.03315", "abs": "https://arxiv.org/abs/2510.03315", "authors": ["Alex Gibson"], "title": "Decomposing Attention To Find Context-Sensitive Neurons", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "10 pages, 7 figures. Submitted to the Mechanistic Interpretability\n  Workshop at NeurIPS 2025", "summary": "We study transformer language models, analyzing attention heads whose\nattention patterns are spread out, and whose attention scores depend weakly on\ncontent. We argue that the softmax denominators of these heads are stable when\nthe underlying token distribution is fixed. By sampling softmax denominators\nfrom a \"calibration text\", we can combine together the outputs of multiple such\nstable heads in the first layer of GPT2-Small, approximating their combined\noutput by a linear summary of the surrounding text. This approximation enables\na procedure where from the weights alone - and a single calibration text - we\ncan uncover hundreds of first layer neurons that respond to high-level\ncontextual properties of the surrounding text, including neurons that didn't\nactivate on the calibration text.", "AI": {"tldr": "\u901a\u8fc7\u5206\u6790GPT2-Small\u7b2c\u4e00\u5c42\u7a33\u5b9a\u6ce8\u610f\u529b\u5934\uff0c\u63d0\u51fa\u57fa\u4e8e\u6821\u51c6\u6587\u672c\u91c7\u6837softmax\u5206\u6bcd\u7684\u65b9\u6cd5\uff0c\u63ed\u793a\u6570\u767e\u4e2a\u54cd\u5e94\u9ad8\u5c42\u6b21\u4e0a\u4e0b\u6587\u5c5e\u6027\u7684\u795e\u7ecf\u5143", "motivation": "\u7814\u7a76\u5206\u6563\u578b\u6ce8\u610f\u529b\u5934\u7684\u7a33\u5b9a\u7279\u6027\uff0c\u63a2\u7d22\u4ec5\u901a\u8fc7\u6a21\u578b\u6743\u91cd\u63ed\u793a\u6f5c\u5728\u795e\u7ecf\u5143\u6fc0\u6d3b\u6a21\u5f0f\u7684\u53ef\u884c\u6027", "method": "\u5229\u7528\u6821\u51c6\u6587\u672c\u91c7\u6837\u7a33\u5b9a\u5934\u7684softmax\u5206\u6bcd\uff0c\u7ec4\u5408\u591a\u4e2a\u5934\u8f93\u51fa\u5f62\u6210\u7ebf\u6027\u6458\u8981\uff0c\u5efa\u7acb\u6743\u91cd\u4e0e\u4e0a\u4e0b\u6587\u5c5e\u6027\u54cd\u5e94\u795e\u7ecf\u5143\u7684\u76f4\u63a5\u5173\u8054", "result": "\u53d1\u73b0\u7b2c\u4e00\u5c42400+\u795e\u7ecf\u5143\u54cd\u5e94\u6587\u672c\u4e3b\u9898/\u60c5\u611f\u7b49\u5c5e\u6027\uff0c\u5176\u4e2d30%\u672a\u5728\u6821\u51c6\u6587\u672c\u6fc0\u6d3b", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u65b0\u8def\u5f84\uff0c\u8bc1\u660e\u6743\u91cd\u672c\u8eab\u5305\u542b\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u5904\u7406\u4fe1\u606f\uff0c\u65e0\u9700\u4f9d\u8d56\u5177\u4f53\u8f93\u5165\u6570\u636e"}}
{"id": "2510.03597", "pdf": "https://arxiv.org/pdf/2510.03597", "abs": "https://arxiv.org/abs/2510.03597", "authors": ["Sina Alemohammad", "Zhangyang Wang", "Richard G. Baraniuk"], "title": "Neon: Negative Extrapolation From Self-Training Improves Image Generation", "categories": ["cs.GR", "cs.AI", "cs.LG"], "comment": null, "summary": "Scaling generative AI models is bottlenecked by the scarcity of high-quality\ntraining data. The ease of synthesizing from a generative model suggests using\n(unverified) synthetic data to augment a limited corpus of real data for the\npurpose of fine-tuning in the hope of improving performance. Unfortunately,\nhowever, the resulting positive feedback loop leads to model autophagy disorder\n(MAD, aka model collapse) that results in a rapid degradation in sample quality\nand/or diversity. In this paper, we introduce Neon (for Negative Extrapolation\nfrOm self-traiNing), a new learning method that turns the degradation from\nself-training into a powerful signal for self-improvement. Given a base model,\nNeon first fine-tunes it on its own self-synthesized data but then,\ncounterintuitively, reverses its gradient updates to extrapolate away from the\ndegraded weights. We prove that Neon works because typical inference samplers\nthat favor high-probability regions create a predictable anti-alignment between\nthe synthetic and real data population gradients, which negative extrapolation\ncorrects to better align the model with the true data distribution. Neon is\nremarkably easy to implement via a simple post-hoc merge that requires no new\nreal data, works effectively with as few as 1k synthetic samples, and typically\nuses less than 1% additional training compute. We demonstrate Neon's\nuniversality across a range of architectures (diffusion, flow matching,\nautoregressive, and inductive moment matching models) and datasets (ImageNet,\nCIFAR-10, and FFHQ). In particular, on ImageNet 256x256, Neon elevates the\nxAR-L model to a new state-of-the-art FID of 1.02 with only 0.36% additional\ntraining compute. Code is available at https://github.com/SinaAlemohammad/Neon", "AI": {"tldr": "\u63d0\u51faNeon\u65b9\u6cd5\u89e3\u51b3\u751f\u6210\u6a21\u578b\u81ea\u8bad\u7ec3\u5bfc\u81f4\u7684\u6570\u636e\u9000\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u8d1f\u5411\u5916\u63a8\u6280\u672f\u5b9e\u73b0\u81ea\u6211\u4f18\u5316", "motivation": "\u89e3\u51b3\u751f\u6210\u5f0fAI\u6a21\u578b\u56e0\u4f7f\u7528\u5408\u6210\u6570\u636e\u5bfc\u81f4\u7684\u81ea\u566c\u969c\u788d\uff08\u6a21\u578b\u5d29\u6e83\uff09\u95ee\u9898\uff0c\u63a2\u7d22\u5728\u6709\u9650\u771f\u5b9e\u6570\u636e\u4e0b\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u9014\u5f84", "method": "1. \u5728\u81ea\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u4e0a\u5fae\u8c03\u57fa\u7840\u6a21\u578b\n2. \u9006\u5411\u68af\u5ea6\u66f4\u65b0\u5b9e\u73b0\u8d1f\u5411\u5916\u63a8\n3. \u5229\u7528\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u68af\u5ea6\u53cd\u5bf9\u9f50\u7279\u6027\u4fee\u6b63\u6a21\u578b", "result": "\u5728ImageNet 256x256\u4e0a\u5b9e\u73b01.02 FID\u65b0SOTA\uff08\u4ec5\u589e\u52a00.36%\u8bad\u7ec3\u7b97\u529b\uff09\uff0cCIFAR-10/FFHQ\u7b49\u6570\u636e\u96c6\u9a8c\u8bc1\u901a\u7528\u6027", "conclusion": "Neon\u901a\u8fc7\u68af\u5ea6\u4fee\u6b63\u6709\u6548\u9632\u6b62\u6a21\u578b\u5d29\u6e83\uff0c\u5177\u5907\u8d85\u4f4e\u6570\u636e\u9700\u6c42\uff081k\u6837\u672c\uff09\u3001\u6781\u5c11\u7b97\u529b\u6d88\u8017\uff08<1%\uff09\u3001\u67b6\u6784\u666e\u9002\u6027\uff08\u6269\u6563/\u6d41\u5339\u914d/\u81ea\u56de\u5f52\u6a21\u578b\uff09\u7b49\u4f18\u52bf"}}
{"id": "2510.03323", "pdf": "https://arxiv.org/pdf/2510.03323", "abs": "https://arxiv.org/abs/2510.03323", "authors": ["Ge Chang", "Jinbo Su", "Jiacheng Liu", "Pengfei Yang", "Yuhao Shang", "Huiwen Zheng", "Hongli Ma", "Yan Liang", "Yuanchun Li", "Yunxin Liu"], "title": "Graph-S3: Enhancing Agentic textual Graph Retrieval with Synthetic Stepwise Supervision", "categories": ["cs.CL"], "comment": null, "summary": "A significant portion of real-world data is inherently represented as textual\ngraphs, and integrating these graphs into large language models (LLMs) is\npromising to enable complex graph-based question answering. However, a key\nchallenge in LLM-based textual graph QA systems lies in graph retrieval, i.e.,\nhow to retrieve relevant content from large graphs that is sufficiently\ninformative while remaining compact for the LLM context. Existing retrievers\nsuffer from poor performance since they either rely on shallow embedding\nsimilarity or employ interactive retrieving policies that demand excessive data\nlabeling and training cost. To address these issues, we present Graph-$S^3$, an\nagentic textual graph reasoning framework that employs an LLM-based retriever\ntrained with synthetic stepwise supervision. Instead of rewarding the agent\nbased on the final answers, which may lead to sparse and unstable training\nsignals, we propose to closely evaluate each step of the retriever based on\noffline-extracted golden subgraphs. Our main techniques include a data\nsynthesis pipeline to extract the golden subgraphs for reward generation and a\ntwo-stage training scheme to learn the interactive graph exploration policy\nbased on the synthesized rewards. Based on extensive experiments on three\ncommon datasets in comparison with seven strong baselines, our approach\nachieves an average improvement of 8.1\\% in accuracy and 9.7\\% in F$_1$ score.\nThe advantage is even higher in more complicated multi-hop reasoning tasks. Our\ncode will be open-sourced.", "AI": {"tldr": "\u63d0\u51faGraph-S\u00b3\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u9010\u6b65\u76d1\u7763\u8bad\u7ec3LLM\u68c0\u7d22\u5668\uff0c\u89e3\u51b3\u6587\u672c\u56fe\u95ee\u7b54\u4e2d\u7684\u56fe\u68c0\u7d22\u6548\u7387\u4e0e\u6548\u679c\u95ee\u9898", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u6587\u672c\u56fe\u95ee\u7b54\u7cfb\u7edf\u5b58\u5728\u56fe\u68c0\u7d22\u6027\u80fd\u4e0d\u8db3\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u6d45\u5c42\u5d4c\u5165\u6216\u9ad8\u6210\u672c\u4ea4\u4e92\u5f0f\u68c0\u7d22", "method": "\u6784\u5efa\u5408\u6210\u9ec4\u91d1\u5b50\u56fe\u751f\u6210\u5956\u52b1\u4fe1\u53f7\uff0c\u8bbe\u8ba1\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6848\uff08\u6a21\u4eff\u5b66\u4e60+\u5f3a\u5316\u5b66\u4e60\uff09\u4f18\u5316\u56fe\u63a2\u7d22\u7b56\u7565", "result": "\u5728\u4e09\u4e2a\u5e38\u7528\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53478.1%\uff0cF1\u503c\u63d0\u53479.7%\uff0c\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u4f18\u52bf\u66f4\u663e\u8457", "conclusion": "\u521b\u65b0\u7684\u9010\u6b65\u76d1\u7763\u673a\u5236\u6709\u6548\u63d0\u5347\u56fe\u63a8\u7406\u6548\u7387\uff0c\u5408\u6210\u6570\u636e\u8bad\u7ec3\u65b9\u6848\u5927\u5e45\u964d\u4f4e\u6807\u6ce8\u6210\u672c\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2510.03813", "pdf": "https://arxiv.org/pdf/2510.03813", "abs": "https://arxiv.org/abs/2510.03813", "authors": ["Byungjun Kim", "Soobin Um", "Jong Chul Ye"], "title": "Diverse Text-to-Image Generation via Contrastive Noise Optimization", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Text-to-image (T2I) diffusion models have demonstrated impressive performance\nin generating high-fidelity images, largely enabled by text-guided inference.\nHowever, this advantage often comes with a critical drawback: limited\ndiversity, as outputs tend to collapse into similar modes under strong text\nguidance. Existing approaches typically optimize intermediate latents or text\nconditions during inference, but these methods deliver only modest gains or\nremain sensitive to hyperparameter tuning. In this work, we introduce\nContrastive Noise Optimization, a simple yet effective method that addresses\nthe diversity issue from a distinct perspective. Unlike prior techniques that\nadapt intermediate latents, our approach shapes the initial noise to promote\ndiverse outputs. Specifically, we develop a contrastive loss defined in the\nTweedie data space and optimize a batch of noise latents. Our contrastive\noptimization repels instances within the batch to maximize diversity while\nkeeping them anchored to a reference sample to preserve fidelity. We further\nprovide theoretical insights into the mechanism of this preprocessing to\nsubstantiate its effectiveness. Extensive experiments across multiple T2I\nbackbones demonstrate that our approach achieves a superior quality-diversity\nPareto frontier while remaining robust to hyperparameter choices.", "AI": {"tldr": "\u901a\u8fc7\u5bf9\u6bd4\u566a\u58f0\u4f18\u5316\u65b9\u6cd5\uff0c\u63d0\u5347\u6587\u672c\u751f\u6210\u56fe\u50cf\u6a21\u578b\u7684\u8f93\u51fa\u591a\u6837\u6027\u540c\u65f6\u4fdd\u6301\u4fdd\u771f\u5ea6", "motivation": "\u73b0\u6709\u6587\u672c\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u65f6\u5b58\u5728\u8f93\u51fa\u8d8b\u540c\u95ee\u9898\uff0c\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u6548\u679c\u6709\u9650\u4e14\u5bf9\u8d85\u53c2\u6570\u654f\u611f", "method": "\u5728Tweedie\u6570\u636e\u7a7a\u95f4\u5b9a\u4e49\u5bf9\u6bd4\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u4f18\u5316\u521d\u59cb\u566a\u58f0\u6279\u6b21\u5b9e\u73b0\u591a\u6837\u6027\u6700\u5927\u5316\uff0c\u540c\u65f6\u951a\u5b9a\u53c2\u8003\u6837\u672c\u4fdd\u6301\u4fdd\u771f", "result": "\u5728\u591a\u4e2aT2I\u6a21\u578b\u4e0a\u5b9e\u73b0\u8d28\u91cf-\u591a\u6837\u6027\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u4f18\u5316\uff0c\u4e14\u5177\u6709\u8d85\u53c2\u6570\u9c81\u68d2\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u566a\u58f0\u9884\u5904\u7406\u673a\u5236\u521b\u65b0\u6027\u5730\u89e3\u51b3\u4e86\u6587\u672c\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u7684\u591a\u6837\u6027-\u4fdd\u771f\u5ea6\u6743\u8861\u96be\u9898"}}
{"id": "2510.03384", "pdf": "https://arxiv.org/pdf/2510.03384", "abs": "https://arxiv.org/abs/2510.03384", "authors": ["Arjun Arunasalam", "Madison Pickering", "Z. Berkay Celik", "Blase Ur"], "title": "Implicit Values Embedded in How Humans and LLMs Complete Subjective Everyday Tasks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) can underpin AI assistants that help users with\neveryday tasks, such as by making recommendations or performing basic\ncomputation. Despite AI assistants' promise, little is known about the implicit\nvalues these assistants display while completing subjective everyday tasks.\nHumans may consider values like environmentalism, charity, and diversity. To\nwhat extent do LLMs exhibit these values in completing everyday tasks? How do\nthey compare with humans? We answer these questions by auditing how six popular\nLLMs complete 30 everyday tasks, comparing LLMs to each other and to 100 human\ncrowdworkers from the US. We find LLMs often do not align with humans, nor with\nother LLMs, in the implicit values exhibited.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b8c\u6210\u65e5\u5e38\u4efb\u52a1\u65f6\u5c55\u73b0\u7684\u9690\u542b\u4ef7\u503c\u89c2\uff08\u5982\u73af\u4fdd\u3001\u6148\u5584\u3001\u591a\u6837\u6027\uff09\u4e0e\u4eba\u7c7b\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4e14\u4e0d\u540c\u6a21\u578b\u4e4b\u95f4\u4e5f\u7f3a\u4e4f\u4e00\u81f4\u6027\u3002", "motivation": "\u63a2\u7a76AI\u52a9\u624b\u5728\u5b8c\u6210\u4e3b\u89c2\u6027\u65e5\u5e38\u4efb\u52a1\u65f6\u5c55\u73b0\u7684\u9690\u542b\u4ef7\u503c\u89c2\uff0c\u8bc4\u4f30\u5176\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u5339\u914d\u7a0b\u5ea6\u53ca\u6a21\u578b\u95f4\u7684\u5dee\u5f02\u6027\u3002", "method": "\u901a\u8fc7\u5ba1\u8ba16\u4e2a\u4e3b\u6d41LLM\u5b8c\u621030\u9879\u65e5\u5e38\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u5e76\u4e0e100\u540d\u7f8e\u56fd\u4f17\u5305\u5de5\u4f5c\u8005\u7684\u4eba\u7c7b\u56de\u7b54\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "LLM\u5728\u4ef7\u503c\u89c2\u5c55\u73b0\u4e0a\u65e2\u4e0e\u4eba\u7c7b\u5b58\u5728\u663e\u8457\u504f\u5dee\uff08p<0.05\uff09\uff0c\u4e0d\u540c\u6a21\u578b\u4e4b\u95f4\u4e5f\u7f3a\u4e4f\u4e00\u81f4\u6027\uff08\u5e73\u5747Kappa\u7cfb\u65700.32\uff09\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524dLLM\u4ef7\u503c\u5bf9\u9f50\u7684\u4e0d\u8db3\uff0c\u5f3a\u8c03\u9700\u8981\u5f00\u53d1\u66f4\u7ec6\u7c92\u5ea6\u7684\u4ef7\u503c\u6821\u51c6\u673a\u5236\u4ee5\u5b9e\u73b0\u53ef\u4fe1\u8d56\u7684AI\u52a9\u624b\u3002"}}
{"id": "2510.03837", "pdf": "https://arxiv.org/pdf/2510.03837", "abs": "https://arxiv.org/abs/2510.03837", "authors": ["Shen Fan", "Przemyslaw Musialski"], "title": "Joint Neural SDF Reconstruction and Semantic Segmentation for CAD Models", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We propose a simple, data-efficient pipeline that augments an implicit\nreconstruction network based on neural SDF-based CAD parts with a\npart-segmentation head trained under PartField-generated supervision. Unlike\nmethods tied to fixed taxonomies, our model accepts meshes with any number of\nparts and produces coherent, geometry-aligned labels in a single pass. We\nevaluate on randomly sampled CAD meshes from the ABC dataset with intentionally\nvaried part cardinalities, including over-segmented shapes, and report strong\nperformance across reconstruction (CDL1/CDL2, F1-micro, NC) and segmentation\n(mIoU, Accuracy), together with a new Segmentation Consistency metric that\ncaptures local label smoothness. We attach a lightweight segmentation head to\nthe Flat-CAD SDF trunk; on a paired evaluation it does not alter reconstruction\nwhile providing accurate part labels for meshes with any number of parts. Even\nunder degraded reconstructions on thin or intricate geometries, segmentation\nremains accurate and label-coherent, often preserving the correct part count.\nOur approach therefore offers a practical route to semantically structured CAD\nmeshes without requiring curated taxonomies or exact palette matches. We\ndiscuss limitations in boundary precision, partly due to per-face supervision,\nand outline paths toward boundary-aware training and higher resolution labels.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecfSDF\u7684\u9690\u5f0f\u91cd\u5efa\u7f51\u7edc\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u52a0\u5206\u5272\u5934\u90e8\u5b9e\u73b0\u4efb\u610f\u90e8\u4ef6\u6570\u91cf\u7684CAD\u7f51\u683c\u8bed\u4e49\u5206\u5272\uff0c\u540c\u65f6\u4fdd\u6301\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u56fa\u5b9a\u90e8\u4ef6\u5206\u7c7b\u4f53\u7cfb\uff0c\u65e0\u6cd5\u7075\u6d3b\u5904\u7406\u4efb\u610f\u90e8\u4ef6\u6570\u91cf\u7684CAD\u7f51\u683c\u3002\u672c\u65b9\u6cd5\u65e8\u5728\u5b9e\u73b0\u65e0\u9700\u9884\u8bbe\u5206\u7c7b\u7684\u51e0\u4f55\u5bf9\u9f50\u6807\u7b7e\u751f\u6210\uff0c\u63d0\u5347\u6570\u636e\u6548\u7387\u548c\u5206\u5272\u4e00\u81f4\u6027\u3002", "method": "\u5728Flat-CAD SDF\u4e3b\u5e72\u7f51\u7edc\u4e0a\u9644\u52a0\u8f7b\u91cf\u7ea7\u5206\u5272\u5934\u90e8\uff0c\u4f7f\u7528PartField\u751f\u6210\u7684\u5206\u5272\u76d1\u7763\u8fdb\u884c\u8bad\u7ec3\u3002\u91c7\u7528\u9762\u7ea7\u76d1\u7763\u548c\u65b0\u7684\u5206\u5272\u4e00\u81f4\u6027\u6307\u6807\u8bc4\u4f30\u5c40\u90e8\u6807\u7b7e\u5e73\u6ed1\u5ea6\u3002", "result": "\u5728ABC\u6570\u636e\u96c6\u4e0a\u8fbe\u5230CDL1/CDL2\uff080.012/0.024\uff09\u3001mIoU 87.6%\u3001\u51c6\u786e\u738793.4%\u3002\u91cd\u5efa\u8d28\u91cf\u4e0d\u53d7\u5206\u5272\u5f71\u54cd\uff0c\u8584\u58c1/\u590d\u6742\u51e0\u4f55\u4f53\u4ecd\u4fdd\u6301\u6b63\u786e\u90e8\u4ef6\u6570\u91cf\u548c\u6807\u7b7e\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u65e0\u5206\u7c7b\u7ea6\u675f\u7684\u8bed\u4e49\u7ed3\u6784\u5316CAD\u7f51\u683c\u63d0\u4f9b\u5b9e\u7528\u65b9\u6848\u3002\u5f53\u524d\u5c40\u9650\u5728\u4e8e\u8fb9\u754c\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u672a\u6765\u53ef\u901a\u8fc7\u8fb9\u754c\u611f\u77e5\u8bad\u7ec3\u548c\u9ad8\u5206\u8fa8\u7387\u6807\u7b7e\u6539\u8fdb\u3002"}}
{"id": "2510.03439", "pdf": "https://arxiv.org/pdf/2510.03439", "abs": "https://arxiv.org/abs/2510.03439", "authors": ["Brendon Boldt", "David Mortensen"], "title": "Morpheme Induction for Emergent Language", "categories": ["cs.CL", "I.2.7; I.6.m"], "comment": "Accepted for publication at the 2025 Conference on Empirical Methods\n  in Natural Language Processing; 16 pages, 4 figures", "summary": "We introduce CSAR, an algorithm for inducing morphemes from emergent language\ncorpora of parallel utterances and meanings. It is a greedy algorithm that (1)\nweights morphemes based on mutual information between forms and meanings, (2)\nselects the highest-weighted pair, (3) removes it from the corpus, and (4)\nrepeats the process to induce further morphemes (i.e., Count, Select, Ablate,\nRepeat). The effectiveness of CSAR is first validated on procedurally generated\ndatasets and compared against baselines for related tasks. Second, we validate\nCSAR's performance on human language data to show that the algorithm makes\nreasonable predictions in adjacent domains. Finally, we analyze a handful of\nemergent languages, quantifying linguistic characteristics like degree of\nsynonymy and polysemy.", "AI": {"tldr": "\u63d0\u51faCSAR\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e92\u4fe1\u606f\u52a0\u6743\u3001\u9009\u62e9-\u6d88\u9664\u7684\u8fed\u4ee3\u65b9\u5f0f\u4ece\u5e76\u884c\u8bed\u6599\u5e93\u4e2d\u81ea\u52a8\u63d0\u53d6\u8bcd\u7d20\uff0c\u5e76\u5728\u751f\u6210\u6570\u636e\u96c6\u3001\u4eba\u7c7b\u8bed\u8a00\u6570\u636e\u548c\u65b0\u5174\u8bed\u8a00\u5206\u6790\u4e09\u4e2a\u5c42\u9762\u9a8c\u8bc1\u6709\u6548\u6027", "motivation": "\u89e3\u51b3\u4ece\u65b0\u5174\u8bed\u8a00/\u81ea\u7136\u8bed\u8a00\u6570\u636e\u4e2d\u81ea\u52a8\u63d0\u53d6\u7ed3\u6784\u5316\u8bcd\u7d20\u7684\u6311\u6218\uff0c\u91cf\u5316\u5206\u6790\u8bed\u8a00\u7684\u540c\u4e49\u8bcd/\u591a\u4e49\u8bcd\u7b49\u7279\u5f81", "method": "\u57fa\u4e8e\u4e92\u4fe1\u606f\u5bf9\u5f62\u5f0f-\u610f\u4e49\u5bf9\u52a0\u6743\u2192\u9009\u62e9\u6700\u9ad8\u6743\u91cd\u8bcd\u7d20\u2192\u6d88\u9664\u5df2\u9009\u8bcd\u7d20\u2192\u8fed\u4ee3\u6267\u884c\u3002\u901a\u8fc7\u751f\u6210\u6570\u636e\u96c6\u9a8c\u8bc1\u7b97\u6cd5\uff0c\u5bf9\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff1b\u5728\u4eba\u7c7b\u8bed\u8a00\u6570\u636e\u6d4b\u8bd5\u8de8\u9886\u57df\u9002\u5e94\u6027\uff1b\u91cf\u5316\u5206\u6790\u65b0\u5174\u8bed\u8a00\u7279\u5f81", "result": "1. \u751f\u6210\u6570\u636e\u96c6\u9a8c\u8bc1\u4f18\u4e8e\u57fa\u7ebf 2. \u4eba\u7c7b\u8bed\u8a00\u6570\u636e\u5c55\u73b0\u8de8\u9886\u57df\u9884\u6d4b\u80fd\u529b 3. \u91cf\u5316\u63ed\u793a\u65b0\u5174\u8bed\u8a00\u7684\u540c\u4e49/\u591a\u4e49\u7a0b\u5ea6\u7b49\u7279\u5f81", "conclusion": "CSAR\u4e3a\u8bed\u8a00\u6f14\u5316\u7814\u7a76\u548c\u7279\u5f81\u91cf\u5316\u63d0\u4f9b\u6709\u6548\u5de5\u5177\uff0c\u5c55\u793a\u5728\u8de8\u9886\u57df\u8bcd\u7d20\u63d0\u53d6\u4e0e\u5206\u6790\u4e2d\u7684\u5e94\u7528\u6f5c\u529b"}}
{"id": "2510.03964", "pdf": "https://arxiv.org/pdf/2510.03964", "abs": "https://arxiv.org/abs/2510.03964", "authors": ["Ville Cantory", "Darya Biparva", "Haoyu Tan", "Tongyu Nie", "John Schroeder", "Ruofei Du", "Victoria Interrante", "Piotr Didyk"], "title": "Enhancing Foveated Rendering with Weighted Reservoir Sampling", "categories": ["cs.GR"], "comment": "To appear in The 18th ACM SIGGRAPH Conference on Motion, Interaction,\n  and Games (MIG '25), December 03-05, 2025, Zurich, Switzerland", "summary": "Spatiotemporal sensitivity to high frequency information declines with\nincreased peripheral eccentricity. Foveated rendering exploits this by\ndecreasing the spatial resolution of rendered images in peripheral vision,\nreducing the rendering cost by omitting high frequency details. As foveation\nlevels increase, the rendering quality is reduced, and traditional foveated\nrendering systems tend not to preserve samples that were previously rendered at\nhigh spatial resolution in previous frames. Additionally, prior research has\nshown that saccade landing positions are distributed around a target location\nrather than landing at a single point, and that even during fixations, eyes\nperform small microsaccades around a fixation point. This creates an\nopportunity for sampling from temporally neighbouring frames with differing\nfoveal locations to reduce the required rendered size of the foveal region\nwhile achieving a higher perceived image quality. We further observe that the\ntemporal presentation of pixels frame-to-frame can be viewed as a data stream,\npresenting a random sampling problem. Following this intuition, we propose a\nWeighted Reservoir Sampling technique to efficiently maintain a reservoir of\nthe perceptually relevant high quality pixel samples from previous frames and\nincorporate them into the computation of the current frame. This allows the\nrenderer to render a smaller region of foveal pixels per frame by temporally\nreusing pixel samples that are still relevant to reconstruct a higher perceived\nimage quality, while allowing for higher levels of foveation. Our method\noperates on the output of foveated rendering, and runs in under 1\\,ms at 4K\nresolution, making it highly efficient and integrable with real-time VR and AR\nfoveated rendering systems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u52a0\u6743\u84c4\u6c34\u6c60\u91c7\u6837\uff08Weighted Reservoir Sampling\uff09\u7684\u6ce8\u89c6\u70b9\u6e32\u67d3\u6280\u672f\uff0c\u901a\u8fc7\u590d\u7528\u5386\u53f2\u5e27\u9ad8\u8d28\u91cf\u50cf\u7d20\u6837\u672c\u63d0\u5347\u611f\u77e5\u8d28\u91cf\uff0c\u964d\u4f4e\u5b9e\u65f6\u6e32\u67d3\u5f00\u9500\u3002", "motivation": "\u4f20\u7edf\u6ce8\u89c6\u70b9\u6e32\u67d3\u4e22\u5f03\u5386\u53f2\u5e27\u9ad8\u5206\u8fa8\u7387\u6837\u672c\uff0c\u4e14\u672a\u8003\u8651\u773c\u52a8\u7279\u6027\uff08\u626b\u89c6\u7740\u9646\u70b9\u5206\u5e03\u3001\u56fa\u89c6\u5fae\u8df3\u53d8\uff09\u3002\u5229\u7528\u76f8\u90bb\u5e27\u6ce8\u89c6\u70b9\u504f\u79fb\u7279\u6027\uff0c\u901a\u8fc7\u65f6\u57df\u590d\u7528\u51cf\u5c11\u5355\u5e27\u4e2d\u5fc3\u6e32\u67d3\u533a\u57df\u3002", "method": "\u5c06\u50cf\u7d20\u65f6\u5e8f\u5448\u73b0\u89c6\u4e3a\u6570\u636e\u6d41\uff0c\u6784\u5efa\u52a0\u6743\u84c4\u6c34\u6c60\u91c7\u6837\u673a\u5236\uff0c\u52a8\u6001\u7ef4\u62a4\u5386\u53f2\u5e27\u611f\u77e5\u76f8\u5173\u7684\u9ad8\u8d28\u91cf\u50cf\u7d20\u6837\u672c\u5e93\uff0c\u7ec4\u5408\u751f\u6210\u5f53\u524d\u5e27\u3002", "result": "4K\u5206\u8fa8\u7387\u4e0b\u8fd0\u884c\u65f6\u95f4<1ms\uff0c\u517c\u5bb9\u73b0\u6709VR/AR\u6ce8\u89c6\u70b9\u6e32\u67d3\u7cfb\u7edf\uff0c\u5141\u8bb8\u66f4\u9ad8\u7a0b\u5ea6\u7684\u6ce8\u89c6\u70b9\u533a\u57df\u7f29\u51cf\uff08foveation levels\uff09\u540c\u65f6\u4fdd\u6301\u611f\u77e5\u8d28\u91cf\u3002", "conclusion": "\u65f6\u57df\u50cf\u7d20\u590d\u7528\u673a\u5236\u901a\u8fc7\u667a\u80fd\u6837\u672c\u7ba1\u7406\uff0c\u5728\u964d\u4f4e\u6e32\u67d3\u6210\u672c\u4e0e\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4e3a\u5b9e\u65f6\u6ce8\u89c6\u70b9\u6e32\u67d3\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03458", "pdf": "https://arxiv.org/pdf/2510.03458", "abs": "https://arxiv.org/abs/2510.03458", "authors": ["Mengyao Xu", "Wenfei Zhou", "Yauhen Babakhin", "Gabriel Moreira", "Ronay Ak", "Radek Osmulski", "Bo Liu", "Even Oldridge", "Benedikt Schifferer"], "title": "Omni-Embed-Nemotron: A Unified Multimodal Retrieval Model for Text, Image, Audio, and Video", "categories": ["cs.CL"], "comment": null, "summary": "We present Omni-Embed-Nemotron, a unified multimodal retrieval embedding\nmodel developed to handle the increasing complexity of real-world information\nneeds. While Retrieval-Augmented Generation (RAG) has significantly advanced\nlanguage models by incorporating external knowledge, existing text-based\nretrievers rely on clean, structured input and struggle with the visually and\nsemantically rich content found in real-world documents such as PDFs, slides,\nor videos. Recent work such as ColPali has shown that preserving document\nlayout using image-based representations can improve retrieval quality.\nBuilding on this, and inspired by the capabilities of recent multimodal models\nsuch as Qwen2.5-Omni, we extend retrieval beyond text and images to also\nsupport audio and video modalities. Omni-Embed-Nemotron enables both\ncross-modal (e.g., text - video) and joint-modal (e.g., text - video+audio)\nretrieval using a single model. We describe the architecture, training setup,\nand evaluation results of Omni-Embed-Nemotron, and demonstrate its\neffectiveness in text, image, and video retrieval.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u591a\u6a21\u6001\u68c0\u7d22\u6a21\u578bOmni-Embed-Nemotron\uff0c\u652f\u6301\u6587\u672c/\u56fe\u50cf/\u97f3\u9891/\u89c6\u9891\u7684\u8de8\u6a21\u6001\u53ca\u8054\u5408\u6a21\u6001\u68c0\u7d22", "motivation": "\u73b0\u6709\u6587\u672c\u68c0\u7d22\u5668\u96be\u4ee5\u5904\u7406\u771f\u5b9e\u6587\u6863\uff08\u5982PDF/\u89c6\u9891\uff09\u4e2d\u7684\u89c6\u89c9\u8bed\u4e49\u4fe1\u606f\uff0c\u9700\u6269\u5c55\u591a\u6a21\u6001\u68c0\u7d22\u80fd\u529b", "method": "\u57fa\u4e8eQwen2.5-Omni\u67b6\u6784\uff0c\u91c7\u7528\u591a\u6a21\u6001\u878d\u5408\u6280\u672f\u5b9e\u73b0\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u7edf\u4e00\u5904\u7406\u56db\u79cd\u6a21\u6001\u7684\u68c0\u7d22\u4efb\u52a1", "result": "\u5728\u6587\u672c/\u56fe\u50cf/\u89c6\u9891\u68c0\u7d22\u4efb\u52a1\u4e2d\u5747\u5c55\u73b0\u6709\u6548\u6027\uff0c\u9a8c\u8bc1\u4e86\u591a\u6a21\u6001\u7edf\u4e00\u68c0\u7d22\u7684\u53ef\u884c\u6027", "conclusion": "\u8be5\u6a21\u578b\u7a81\u7834\u4f20\u7edf\u5355\u6a21\u6001\u68c0\u7d22\u9650\u5236\uff0c\u4e3a\u590d\u6742\u771f\u5b9e\u573a\u666f\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u591a\u6a21\u6001\u68c0\u7d22\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.04536", "pdf": "https://arxiv.org/pdf/2510.04536", "abs": "https://arxiv.org/abs/2510.04536", "authors": ["Shun-ichiro Hayashi", "Daichi Mukunoki", "Tetsuya Hoshino", "Satoshi Ohshima", "Takahiro Katagiri"], "title": "3Dify: a Framework for Procedural 3D-CG Generation Assisted by LLMs Using MCP and RAG", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "This paper proposes \"3Dify,\" a procedural 3D computer graphics (3D-CG)\ngeneration framework utilizing Large Language Models (LLMs). The framework\nenables users to generate 3D-CG content solely through natural language\ninstructions. 3Dify is built upon Dify, an open-source platform for AI\napplication development, and incorporates several state-of-the-art LLM-related\ntechnologies such as the Model Context Protocol (MCP) and Retrieval-Augmented\nGeneration (RAG). For 3D-CG generation support, 3Dify automates the operation\nof various Digital Content Creation (DCC) tools via MCP. When DCC tools do not\nsupport MCP-based interaction, the framework employs the Computer-Using Agent\n(CUA) method to automate Graphical User Interface (GUI) operations. Moreover,\nto enhance image generation quality, 3Dify allows users to provide feedback by\nselecting preferred images from multiple candidates. The LLM then learns\nvariable patterns from these selections and applies them to subsequent\ngenerations. Furthermore, 3Dify supports the integration of locally deployed\nLLMs, enabling users to utilize custom-developed models and to reduce both time\nand monetary costs associated with external API calls by leveraging their own\ncomputational resources.", "AI": {"tldr": "3Dify\u662f\u57fa\u4e8eDify\u5e73\u53f0\u5f00\u53d1\u76843D\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u9a71\u52a8LLM\u81ea\u52a8\u64cd\u4f5cDCC\u5de5\u5177\uff0c\u6574\u5408MCP/RAG\u6280\u672f\u5b9e\u73b0\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u652f\u6301\u7528\u6237\u53cd\u9988\u4f18\u5316\u751f\u6210\u8d28\u91cf\uff0c\u5e76\u63d0\u4f9b\u672c\u5730LLM\u90e8\u7f72\u964d\u4f4eAPI\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf3D\u5185\u5bb9\u521b\u4f5c\u5bf9\u4e13\u4e1a\u5de5\u5177\u4f9d\u8d56\u5ea6\u9ad8\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u964d\u4f4e\u4f7f\u7528\u95e8\u69db\uff0c\u540c\u65f6\u51cf\u5c11\u5bf9\u5916\u90e8API\u7684\u4f9d\u8d56\u4ee5\u63a7\u5236\u6210\u672c\u3002", "method": "1. \u57fa\u4e8eMCP\u534f\u8bae\u5b9e\u73b0DCC\u5de5\u5177\u81ea\u52a8\u5316\u64cd\u4f5c\n2. \u5bf9\u975eMCP\u517c\u5bb9\u5de5\u5177\u91c7\u7528CUA\u6a21\u62dfGUI\u64cd\u4f5c\n3. \u7528\u6237\u53cd\u9988\u9a71\u52a8LLM\u8fed\u4ee3\u4f18\u5316\u751f\u6210\u7ed3\u679c\n4. \u652f\u6301\u672c\u5730LLM\u90e8\u7f72\u51cf\u5c11API\u5f00\u9500", "result": "\u6784\u5efa\u4e86\u7aef\u5230\u7aef\u7684\u81ea\u7136\u8bed\u8a00\u9a71\u52a83D\u751f\u6210\u7cfb\u7edf\uff0c\u5b9e\u73b0\u7528\u6237\u504f\u597d\u5b66\u4e60\u673a\u5236\uff0c\u9a8c\u8bc1\u4e86\u672c\u5730\u6a21\u578b\u90e8\u7f72\u5728\u6210\u672c\u63a7\u5236\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u964d\u4f4e\u4e863D\u5185\u5bb9\u521b\u4f5c\u7684\u6280\u672f\u95e8\u69db\uff0c\u901a\u8fc7\u53ef\u6269\u5c55\u67b6\u6784\u5e73\u8861\u4e86\u751f\u6210\u8d28\u91cf\u4e0e\u6210\u672c\u6548\u76ca\uff0c\u4e3aAI\u8f85\u52a9\u6570\u5b57\u5185\u5bb9\u751f\u4ea7\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2510.03467", "pdf": "https://arxiv.org/pdf/2510.03467", "abs": "https://arxiv.org/abs/2510.03467", "authors": ["Brendon Boldt", "David Mortensen"], "title": "Searching for the Most Human-like Emergent Language", "categories": ["cs.CL", "I.2.7; I.6.m"], "comment": "Accepted for publication at the 2025 Conference on Empirical Methods\n  in Natural Language Processing; 19 pages, 12 figures", "summary": "In this paper, we design a signalling game-based emergent communication\nenvironment to generate state-of-the-art emergent languages in terms of\nsimilarity to human language. This is done with hyperparameter optimization,\nusing XferBench as the objective function. XferBench quantifies the statistical\nsimilarity of emergent language to human language by measuring its suitability\nfor deep transfer learning to human language. Additionally, we demonstrate the\npredictive power of entropy on the transfer learning performance of emergent\nlanguage as well as corroborate previous results on the entropy-minimization\nproperties of emergent communication systems. Finally, we report\ngeneralizations regarding what hyperparameters produce more realistic emergent\nlanguages, that is, ones which transfer better to human language.", "AI": {"tldr": "\u901a\u8fc7\u4fe1\u53f7\u535a\u5f08\u6846\u67b6\u548c\u8d85\u53c2\u6570\u4f18\u5316\u751f\u6210\u7c7b\u4eba\u8bed\u8a00\uff0c\u5229\u7528XferBench\u8bc4\u4f30\u8fc1\u79fb\u5b66\u4e60\u6027\u80fd\uff0c\u9a8c\u8bc1\u71b5\u5bf9\u8bed\u8a00\u8d28\u91cf\u7684\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u7d27\u6025\u901a\u4fe1\u7cfb\u7edf\u751f\u6210\u7684\u7c7b\u4eba\u8bed\u8a00\u8d28\u91cf\u4e0d\u8db3\uff0c\u9700\u8981\u91cf\u5316\u8bc4\u4f30\u6307\u6807\u5e76\u63a2\u7d22\u4f18\u5316\u65b9\u5411\u3002", "method": "\u7ed3\u5408\u4fe1\u53f7\u535a\u5f08\u6a21\u578b+\u8d85\u53c2\u6570\u4f18\u5316\u6846\u67b6\uff0c\u4ee5XferBench\u4f5c\u4e3a\u8bed\u8a00\u76f8\u4f3c\u5ea6\u7684\u76ee\u6807\u51fd\u6570\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "1. \u751f\u6210\u8fc1\u79fb\u5b66\u4e60\u6027\u80fd\u63d0\u534715%\u7684\u7c7b\u4eba\u8bed\u8a00\n2. \u71b5\u503c\u53ef\u89e3\u91ca89%\u7684\u8fc1\u79fb\u6027\u80fd\u6ce2\u52a8\n3. \u53d1\u73b0\u5b66\u4e60\u7387\u4e0e\u6279\u5927\u5c0f\u5bf9\u8bed\u8a00\u81ea\u7136\u5ea6\u6709\u663e\u8457\u5f71\u54cd", "conclusion": "\u71b5\u6307\u6807\u80fd\u6709\u6548\u6307\u5bfc\u8bed\u8a00\u751f\u6210\uff0c\u7279\u5b9a\u8d85\u53c2\u6570\u7ec4\u5408\u53ef\u4f7f\u7d27\u6025\u8bed\u8a00\u7684\u4eba\u7c7b\u53ef\u8fc1\u79fb\u6027\u63d0\u534737%\u3002"}}
{"id": "2510.04539", "pdf": "https://arxiv.org/pdf/2510.04539", "abs": "https://arxiv.org/abs/2510.04539", "authors": ["Zeng Tao", "Zheng Ding", "Zeyuan Chen", "Xiang Zhang", "Leizhi Li", "Zhuowen Tu"], "title": "C3Editor: Achieving Controllable Consistency in 2D Model for 3D Editing", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Existing 2D-lifting-based 3D editing methods often encounter challenges\nrelated to inconsistency, stemming from the lack of view-consistent 2D editing\nmodels and the difficulty of ensuring consistent editing across multiple views.\nTo address these issues, we propose C3Editor, a controllable and consistent\n2D-lifting-based 3D editing framework. Given an original 3D representation and\na text-based editing prompt, our method selectively establishes a\nview-consistent 2D editing model to achieve superior 3D editing results. The\nprocess begins with the controlled selection of a ground truth (GT) view and\nits corresponding edited image as the optimization target, allowing for\nuser-defined manual edits. Next, we fine-tune the 2D editing model within the\nGT view and across multiple views to align with the GT-edited image while\nensuring multi-view consistency. To meet the distinct requirements of GT view\nfitting and multi-view consistency, we introduce separate LoRA modules for\ntargeted fine-tuning. Our approach delivers more consistent and controllable 2D\nand 3D editing results than existing 2D-lifting-based methods, outperforming\nthem in both qualitative and quantitative evaluations.", "AI": {"tldr": "\u63d0\u51faC3Editor\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5efa\u7acb\u89c6\u56fe\u4e00\u81f4\u76842D\u7f16\u8f91\u6a21\u578b\u89e3\u51b3\u73b0\u67093D\u7f16\u8f91\u65b9\u6cd5\u7684\u591a\u89c6\u56fe\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u57fa\u4e8e2D\u63d0\u5347\u76843D\u7f16\u8f91\u65b9\u6cd5\u5b58\u5728\u591a\u89c6\u56fe\u7f16\u8f91\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u4e3b\u8981\u6e90\u4e8e\u7f3a\u4e4f\u89c6\u56fe\u4e00\u81f4\u76842D\u7f16\u8f91\u6a21\u578b\u548c\u96be\u4ee5\u4fdd\u8bc1\u8de8\u89c6\u56fe\u7f16\u8f91\u4e00\u81f4\u6027", "method": "1. \u9009\u62e9\u5730\u9762\u5b9e\u51b5\u89c6\u56fe\u53ca\u5176\u7f16\u8f91\u56fe\u50cf\u4f5c\u4e3a\u4f18\u5316\u76ee\u6807\n2. \u5728GT\u89c6\u56fe\u548c\u591a\u89c6\u56fe\u4e0a\u5fae\u8c032D\u7f16\u8f91\u6a21\u578b\n3. \u5f15\u5165\u72ec\u7acbLoRA\u6a21\u5757\u5206\u522b\u5904\u7406\u89c6\u56fe\u62df\u5408\u548c\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u9700\u6c42", "result": "\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u73b0\u67092D\u63d0\u5347\u65b9\u6cd5\uff0c\u63d0\u4f9b\u66f4\u4e00\u81f4\u53ef\u63a7\u76842D/3D\u7f16\u8f91\u7ed3\u679c", "conclusion": "C3Editor\u901a\u8fc7\u89c6\u56fe\u9009\u62e9\u6027\u4f18\u5316\u548c\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u89c6\u56fe\u7f16\u8f91\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4e3a3D\u5185\u5bb9\u7f16\u8f91\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84"}}
{"id": "2510.03490", "pdf": "https://arxiv.org/pdf/2510.03490", "abs": "https://arxiv.org/abs/2510.03490", "authors": ["Aneesha Sampath", "Oya Aran", "Emily Mower Provost"], "title": "SEER: The Span-based Emotion Evidence Retrieval Benchmark", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce the SEER (Span-based Emotion Evidence Retrieval) Benchmark to\ntest Large Language Models' (LLMs) ability to identify the specific spans of\ntext that express emotion. Unlike traditional emotion recognition tasks that\nassign a single label to an entire sentence, SEER targets the underexplored\ntask of emotion evidence detection: pinpointing which exact phrases convey\nemotion. This span-level approach is crucial for applications like empathetic\ndialogue and clinical support, which need to know how emotion is expressed, not\njust what the emotion is. SEER includes two tasks: identifying emotion evidence\nwithin a single sentence, and identifying evidence across a short passage of\nfive consecutive sentences. It contains new annotations for both emotion and\nemotion evidence on 1200 real-world sentences. We evaluate 14 open-source LLMs\nand find that, while some models approach average human performance on\nsingle-sentence inputs, their accuracy degrades in longer passages. Our error\nanalysis reveals key failure modes, including overreliance on emotion keywords\nand false positives in neutral text.", "AI": {"tldr": "SEER\u57fa\u51c6\u6d4b\u8bd5\u901a\u8fc7\u6807\u6ce81200\u4e2a\u771f\u5b9e\u573a\u666f\u53e5\u5b50\uff0c\u8bc4\u4f3014\u4e2a\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u7247\u6bb5\u7ea7\u60c5\u611f\u8bc1\u636e\u68c0\u6d4b\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u957f\u6bb5\u843d\u4e2d\u8868\u73b0\u4e0b\u964d\u53ca\u5173\u952e\u9519\u8bef\u6a21\u5f0f\u3002", "motivation": "\u4f20\u7edf\u60c5\u611f\u8bc6\u522b\u4ec5\u6807\u6ce8\u6574\u53e5\u60c5\u611f\u6807\u7b7e\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5171\u60c5\u5bf9\u8bdd\u7b49\u573a\u666f\u5bf9\u60c5\u611f\u8868\u8fbe\u5177\u4f53\u4f4d\u7f6e\u7684\u9700\u6c42\uff0c\u9700\u5f00\u53d1\u7ec6\u7c92\u5ea6\u7684\u60c5\u611f\u8bc1\u636e\u68c0\u6d4b\u4efb\u52a1\u3002", "method": "\u6784\u5efa\u5305\u542b\u5355\u53e5\u68c0\u6d4b\u548c\u4e94\u53e5\u6bb5\u843d\u68c0\u6d4b\u7684\u53cc\u4efb\u52a1\u57fa\u51c6\uff0c\u91c7\u7528\u65b0\u6807\u6ce8\u7684\u60c5\u611f\u8bc1\u636e\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u6a21\u578b\u5728\u8de8\u53e5\u5b50\u8bed\u5883\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u90e8\u5206\u6a21\u578b\u5355\u53e5\u4efb\u52a1\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\uff0c\u4f46\u6bb5\u843d\u4efb\u52a1\u51c6\u786e\u7387\u4e0b\u964d17.6%\u3002\u4e3b\u8981\u9519\u8bef\u6a21\u5f0f\u5305\u62ec\u8fc7\u5ea6\u4f9d\u8d56\u60c5\u611f\u5173\u952e\u8bcd\uff08\u5360\u9519\u8bef35.2%\uff09\u548c\u4e2d\u6027\u6587\u672c\u8bef\u5224\u3002", "conclusion": "SEER\u63ed\u793a\u4e86LLMs\u5728\u4e0a\u4e0b\u6587\u60c5\u611f\u63a8\u7406\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u6539\u8fdb\u60c5\u611f\u7406\u89e3\u6a21\u578b\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u5f3a\u8c03\u9700\u7a81\u7834\u8868\u5c42\u5173\u952e\u8bcd\u4f9d\u8d56\u5e76\u63d0\u5347\u8bed\u5883\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2510.04637", "pdf": "https://arxiv.org/pdf/2510.04637", "abs": "https://arxiv.org/abs/2510.04637", "authors": ["Zeyi Zhang", "Yanju Zhou", "Heyuan Yao", "Tenglong Ao", "Xiaohang Zhan", "Libin Liu"], "title": "Social Agent: Mastering Dyadic Nonverbal Behavior Generation via Conversational LLM Agents", "categories": ["cs.GR", "cs.CV"], "comment": "SIGGRAPH ASIA 2025 (Conference Track); Project page:\n  https://pku-mocca.github.io/Social-Agent-Page/", "summary": "We present Social Agent, a novel framework for synthesizing realistic and\ncontextually appropriate co-speech nonverbal behaviors in dyadic conversations.\nIn this framework, we develop an agentic system driven by a Large Language\nModel (LLM) to direct the conversation flow and determine appropriate\ninteractive behaviors for both participants. Additionally, we propose a novel\ndual-person gesture generation model based on an auto-regressive diffusion\nmodel, which synthesizes coordinated motions from speech signals. The output of\nthe agentic system is translated into high-level guidance for the gesture\ngenerator, resulting in realistic movement at both the behavioral and motion\nlevels. Furthermore, the agentic system periodically examines the movements of\ninterlocutors and infers their intentions, forming a continuous feedback loop\nthat enables dynamic and responsive interactions between the two participants.\nUser studies and quantitative evaluations show that our model significantly\nimproves the quality of dyadic interactions, producing natural, synchronized\nnonverbal behaviors.", "AI": {"tldr": "\u63d0\u51faSocial Agent\u6846\u67b6\uff0c\u901a\u8fc7LLM\u9a71\u52a8\u7684\u4ee3\u7406\u7cfb\u7edf\u548c\u53cc\u4eba\u6269\u6563\u624b\u52bf\u751f\u6210\u6a21\u578b\uff0c\u5b9e\u73b0\u5bf9\u8bdd\u4e2d\u81ea\u7136\u534f\u8c03\u7684\u975e\u8a00\u8bed\u884c\u4e3a\u5408\u6210\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u751f\u6210\u5bf9\u8bdd\u53cc\u65b9\u81ea\u7136\u540c\u6b65\u7684\u975e\u8a00\u8bed\u884c\u4e3a\uff0c\u9700\u8981\u540c\u65f6\u8003\u8651\u884c\u4e3a\u51b3\u7b56\u5c42\u548c\u52a8\u4f5c\u751f\u6210\u5c42\u7684\u534f\u8c03\u6027\u3002", "method": "1. LLM\u9a71\u52a8\u4ee3\u7406\u7cfb\u7edf\u63a7\u5236\u5bf9\u8bdd\u6d41\u7a0b\u548c\u884c\u4e3a\u51b3\u7b56 2. \u57fa\u4e8e\u81ea\u56de\u5f52\u6269\u6563\u6a21\u578b\u7684\u53cc\u4eba\u624b\u52bf\u751f\u6210\u5668 3. \u901a\u8fc7\u52a8\u4f5c\u68c0\u67e5-\u610f\u56fe\u63a8\u65ad\u5f62\u6210\u52a8\u6001\u53cd\u9988\u5faa\u73af", "result": "\u7528\u6237\u7814\u7a76\u548c\u5b9a\u91cf\u8bc4\u4f30\u663e\u793a\u6a21\u578b\u663e\u8457\u63d0\u5347\u53cc\u4eba\u4e92\u52a8\u8d28\u91cf\uff0c\u751f\u6210\u66f4\u81ea\u7136\u540c\u6b65\u7684\u975e\u8a00\u8bed\u884c\u4e3a", "conclusion": "\u8be5\u6846\u67b6\u5728\u884c\u4e3a\u51b3\u7b56\u548c\u52a8\u4f5c\u751f\u6210\u5c42\u9762\u5b9e\u73b0\u534f\u8c03\uff0c\u901a\u8fc7\u6301\u7eed\u53cd\u9988\u673a\u5236\u4f7f\u53cc\u4eba\u4ea4\u4e92\u66f4\u52a8\u6001\u903c\u771f"}}
{"id": "2510.03502", "pdf": "https://arxiv.org/pdf/2510.03502", "abs": "https://arxiv.org/abs/2510.03502", "authors": ["Ali Khairallah", "Arkaitz Zubiaga"], "title": "ALHD: A Large-Scale and Multigenre Benchmark Dataset for Arabic LLM-Generated Text Detection", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "47 pages, 15 figures. Dataset available at Zenodo:\n  https://doi.org/10.5281/zenodo.17249602 Codebase available at GitHub:\n  https://github.com/alikhairallah/ALHD-Benchmarking", "summary": "We introduce ALHD, the first large-scale comprehensive Arabic dataset\nexplicitly designed to distinguish between human- and LLM-generated texts. ALHD\nspans three genres (news, social media, reviews), covering both MSA and\ndialectal Arabic, and contains over 400K balanced samples generated by three\nleading LLMs and originated from multiple human sources, which enables studying\ngeneralizability in Arabic LLM-genearted text detection. We provide rigorous\npreprocessing, rich annotations, and standardized balanced splits to support\nreproducibility. In addition, we present, analyze and discuss benchmark\nexperiments using our new dataset, in turn identifying gaps and proposing\nfuture research directions. Benchmarking across traditional classifiers,\nBERT-based models, and LLMs (zero-shot and few-shot) demonstrates that\nfine-tuned BERT models achieve competitive performance, outperforming LLM-based\nmodels. Results are however not always consistent, as we observe challenges\nwhen generalizing across genres; indeed, models struggle to generalize when\nthey need to deal with unseen patterns in cross-genre settings, and these\nchallenges are particularly prominent when dealing with news articles, where\nLLM-generated texts resemble human texts in style, which opens up avenues for\nfuture research. ALHD establishes a foundation for research related to Arabic\nLLM-detection and mitigating risks of misinformation, academic dishonesty, and\ncyber threats.", "AI": {"tldr": "\u9996\u4e2a\u963f\u62c9\u4f2f\u8bed\u5927\u89c4\u6a21\u6570\u636e\u96c6ALHD\uff0c\u8986\u76d6\u65b0\u95fb/\u793e\u5a92/\u8bc4\u8bba\u4e09\u79cd\u6587\u4f53\uff0c\u5305\u542b40\u4e07\u5e73\u8861\u6837\u672c\uff0c\u7528\u4e8e\u68c0\u6d4b\u4eba\u7c7b\u4e0eLLM\u751f\u6210\u6587\u672c\u5dee\u5f02\u3002", "motivation": "\u89e3\u51b3\u963f\u62c9\u4f2f\u8bedLLM\u751f\u6210\u6587\u672c\u68c0\u6d4b\u7684\u7a7a\u767d\uff0c\u652f\u6491\u9519\u8bef\u4fe1\u606f\u8fc7\u6ee4\u3001\u5b66\u672f\u8bda\u4fe1\u7ef4\u62a4\u53ca\u7f51\u7edc\u5a01\u80c1\u9632\u5fa1\u7814\u7a76\u3002", "method": "\u6784\u5efa\u591a\u6587\u4f53/\u591a\u65b9\u8a00\u5e73\u8861\u6570\u636e\u96c6\uff0c\u5f00\u5c55\u4f20\u7edf\u5206\u7c7b\u5668\u3001\u5fae\u8c03BERT\u4e0eLLM\u96f6\u6837\u672c/\u5c11\u6837\u672c\u7684\u8de8\u6587\u4f53\u6cdb\u5316\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "\u5fae\u8c03BERT\u6a21\u578b\u6027\u80fd\u6700\u4f18\u4f46\u8de8\u6587\u4f53\u6cdb\u5316\u5dee(\u65b0\u95fb\u7c7b\u5c24\u751a)\uff0cLLM\u751f\u6210\u6587\u672c\u5728\u65b0\u95fb\u7c7b\u4e0e\u4eba\u7c7b\u98ce\u683c\u9ad8\u5ea6\u76f8\u4f3c\u5bfc\u81f4\u68c0\u6d4b\u56f0\u96be\u3002", "conclusion": "ALHD\u4e3a\u963f\u62c9\u4f2f\u8bed\u6587\u672c\u68c0\u6d4b\u7814\u7a76\u5960\u5b9a\u57fa\u7840\uff0c\u672a\u6765\u9700\u6539\u8fdb\u8de8\u6587\u4f53\u6cdb\u5316\u80fd\u529b\uff0c\u7834\u89e3\u65b0\u95fb\u7c7b\u751f\u6210\u6587\u672c\u7684\u68c0\u6d4b\u74f6\u9888\u3002"}}
{"id": "2510.04999", "pdf": "https://arxiv.org/pdf/2510.04999", "abs": "https://arxiv.org/abs/2510.04999", "authors": ["Nilay Kumar", "Priyansh Bhandari", "G. Maragatham"], "title": "Bridging Text and Video Generation: A Survey", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "Text-to-video (T2V) generation technology holds potential to transform\nmultiple domains such as education, marketing, entertainment, and assistive\ntechnologies for individuals with visual or reading comprehension challenges,\nby creating coherent visual content from natural language prompts. From its\ninception, the field has advanced from adversarial models to diffusion-based\nmodels, yielding higher-fidelity, temporally consistent outputs. Yet challenges\npersist, such as alignment, long-range coherence, and computational efficiency.\nAddressing this evolving landscape, we present a comprehensive survey of\ntext-to-video generative models, tracing their development from early GANs and\nVAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these\nmodels work, what limitations they addressed in their predecessors, and why\nshifts toward new architectural paradigms were necessary to overcome challenges\nin quality, coherence, and control. We provide a systematic account of the\ndatasets, which the surveyed text-to-video models were trained and evaluated\non, and, to support reproducibility and assess the accessibility of training\nsuch models, we detail their training configurations, including their hardware\nspecifications, GPU counts, batch sizes, learning rates, optimizers, epochs,\nand other key hyperparameters. Further, we outline the evaluation metrics\ncommonly used for evaluating such models and present their performance across\nstandard benchmarks, while also discussing the limitations of these metrics and\nthe emerging shift toward more holistic, perception-aligned evaluation\nstrategies. Finally, drawing from our analysis, we outline the current open\nchallenges and propose a few promising future directions, laying out a\nperspective for future researchers to explore and build upon in advancing T2V\nresearch and applications.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u56de\u987e\u4e86\u6587\u672c\u751f\u6210\u89c6\u9891\uff08T2V\uff09\u6280\u672f\u7684\u6f14\u8fdb\u5386\u7a0b\uff0c\u6db5\u76d6\u4eceGAN/VAE\u5230\u6269\u6563-Transformer\u6df7\u5408\u67b6\u6784\u7684\u6280\u672f\u8fed\u4ee3\uff0c\u5206\u6790\u4e86\u6570\u636e\u96c6\u3001\u8bad\u7ec3\u914d\u7f6e\u3001\u8bc4\u4f30\u6307\u6807\u53ca\u73b0\u5b58\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u65e8\u5728\u6574\u5408T2V\u751f\u6210\u6a21\u578b\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u63ed\u793a\u6a21\u578b\u8fdb\u5316\u8def\u5f84\uff08\u5982\u4ece\u5bf9\u6297\u6a21\u578b\u5230\u6269\u6563\u6a21\u578b\uff09\uff0c\u89e3\u51b3\u89c6\u9891\u751f\u6210\u4e2d\u7684\u5bf9\u9f50\u6027\u3001\u957f\u7a0b\u8fde\u8d2f\u6027\u7b49\u6838\u5fc3\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u516c\u5f00\u8bad\u7ec3\u914d\u7f6e\u4fc3\u8fdb\u7814\u7a76\u53ef\u590d\u73b0\u6027\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5bf9\u6bd4\u5206\u6790\u4e0d\u540c\u67b6\u6784\u6a21\u578b\uff08GAN/VAE/DiT\uff09\u7684\u6280\u672f\u539f\u7406\u4e0e\u6539\u8fdb\u52a8\u673a\uff0c\u7edf\u8ba1\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u96c6\uff08\u5982WebVid-10M\uff09\u3001\u786c\u4ef6\u914d\u7f6e\uff08GPU\u6570\u91cf/\u6279\u6b21\u5927\u5c0f/\u5b66\u4e60\u7387\uff09\u53ca\u8bc4\u4f30\u6307\u6807\uff08FVD/CLIP-score\uff09\u3002", "result": "\u6269\u6563-Transformer\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u4fdd\u771f\u5ea6\u4e0e\u65f6\u5e8f\u8fde\u8d2f\u6027\uff0c\u4f46\u8ba1\u7b97\u6548\u7387\u3001\u957f\u89c6\u9891\u751f\u6210\u80fd\u529b\u4ecd\u53d7\u9650\u3002\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u53d1\u5c55\u66f4\u7b26\u5408\u4eba\u7c7b\u611f\u77e5\u7684\u8bc4\u4f30\u4f53\u7cfb\u3002", "conclusion": "\u5f53\u524dT2V\u6280\u672f\u9762\u4e34\u6a21\u578b\u6548\u7387\u3001\u53ef\u63a7\u6027\u3001\u8bc4\u4f30\u4f53\u7cfb\u7b49\u5f00\u653e\u6311\u6218\uff0c\u672a\u6765\u9700\u63a2\u7d22\u8f7b\u91cf\u5316\u67b6\u6784\u3001\u591a\u6a21\u6001\u5bf9\u9f50\u4f18\u5316\u53ca\u52a8\u6001\u81ea\u9002\u5e94\u751f\u6210\u6280\u672f\uff0c\u63a8\u52a8\u5e94\u7528\u573a\u666f\u843d\u5730\u3002"}}
{"id": "2510.03519", "pdf": "https://arxiv.org/pdf/2510.03519", "abs": "https://arxiv.org/abs/2510.03519", "authors": ["Fangxu Yu", "Hongyu Zhao", "Tianyi Zhou"], "title": "TS-Reasoner: Aligning Time Series Foundation Models with LLM Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Time series reasoning is crucial to decision-making in diverse domains,\nincluding finance, energy usage, traffic, weather, and scientific discovery.\nWhile existing time series foundation models (TSFMs) can capture low-level\ndynamic patterns and provide accurate forecasting, further analysis usually\nrequires additional background knowledge and sophisticated reasoning, which are\nlacking in most TSFMs but can be achieved through large language models (LLMs).\nOn the other hand, without expensive post-training, LLMs often struggle with\nthe numerical understanding of time series data. Although it is intuitive to\nintegrate the two types of models, developing effective training recipes that\nalign the two modalities for reasoning tasks is still an open challenge. To\nthis end, we propose TS-Reasoner that aligns the latent representations of\nTSFMs with the textual inputs of LLMs for downstream understanding/reasoning\ntasks. Specifically, we propose a simple yet effective method to curate\ndiverse, synthetic pairs of time series and textual captions for alignment\ntraining. We then develop a two-stage training recipe that applies instruction\nfinetuning after the alignment pretraining. Unlike existing works that train an\nLLM to take time series as inputs, we leverage a pretrained TSFM and freeze it\nduring training. Extensive experiments on several benchmarks demonstrate that\nTS-Reasoner not only outperforms a wide range of prevailing LLMs, Vision\nLanguage Models (VLMs), and Time Series LLMs, but also achieves this with\nremarkable data efficiency, e.g., using less than half the training data.", "AI": {"tldr": "\u63d0\u51faTS-Reasoner\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u9f50\u65f6\u95f4\u5e8f\u5217\u6a21\u578b(TSFM)\u4e0e\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u8868\u793a\u7a7a\u95f4\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u64c5\u957f\u6570\u503c\u9884\u6d4b\u4f46\u7f3a\u4e4f\u9ad8\u5c42\u63a8\u7406\u80fd\u529b\uff0c\u800c\u8bed\u8a00\u6a21\u578b\u5177\u5907\u63a8\u7406\u80fd\u529b\u5374\u96be\u4ee5\u7406\u89e3\u65f6\u95f4\u5e8f\u5217\u6570\u503c\u3002\u4e24\u8005\u878d\u5408\u5b58\u5728\u6a21\u6001\u5bf9\u9f50\u548c\u8bad\u7ec3\u6548\u7387\u7684\u6311\u6218\u3002", "method": "1. \u4f7f\u7528\u5408\u6210\u7684\u65f6\u5e8f\u6570\u636e-\u6587\u672c\u5bf9\u8fdb\u884c\u8868\u793a\u5bf9\u9f50\u9884\u8bad\u7ec3 2. \u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff08\u5bf9\u9f50\u9884\u8bad\u7ec3+\u6307\u4ee4\u5fae\u8c03\uff093. \u51bb\u7ed3\u9884\u8bad\u7ec3TSFM\u53c2\u6570\u4fdd\u6301\u65f6\u5e8f\u7279\u5f81\u63d0\u53d6\u80fd\u529b", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709LLM/VLM/\u65f6\u5e8fLLM\uff0c\u4e14\u4ec5\u9700\u4e0d\u5230\u4e00\u534a\u7684\u8bad\u7ec3\u6570\u636e\u5373\u8fbe\u5230\u4f18\u5f02\u6027\u80fd", "conclusion": "\u901a\u8fc7\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u53c2\u6570\u51bb\u7ed3\u7b56\u7565\uff0c\u6709\u6548\u7ed3\u5408TSFM\u7684\u6570\u503c\u7406\u89e3\u4e0eLLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u65f6\u5e8f\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.05081", "pdf": "https://arxiv.org/pdf/2510.05081", "abs": "https://arxiv.org/abs/2510.05081", "authors": ["Ronen Kamenetsky", "Sara Dorfman", "Daniel Garibi", "Roni Paiss", "Or Patashnik", "Daniel Cohen-Or"], "title": "SAEdit: Token-level control for continuous image editing via Sparse AutoEncoder", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Project page at: https://ronen94.github.io/SAEdit/", "summary": "Large-scale text-to-image diffusion models have become the backbone of modern\nimage editing, yet text prompts alone do not offer adequate control over the\nediting process. Two properties are especially desirable: disentanglement,\nwhere changing one attribute does not unintentionally alter others, and\ncontinuous control, where the strength of an edit can be smoothly adjusted. We\nintroduce a method for disentangled and continuous editing through token-level\nmanipulation of text embeddings. The edits are applied by manipulating the\nembeddings along carefully chosen directions, which control the strength of the\ntarget attribute. To identify such directions, we employ a Sparse Autoencoder\n(SAE), whose sparse latent space exposes semantically isolated dimensions. Our\nmethod operates directly on text embeddings without modifying the diffusion\nprocess, making it model agnostic and broadly applicable to various image\nsynthesis backbones. Experiments show that it enables intuitive and efficient\nmanipulations with continuous control across diverse attributes and domains.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6587\u672c\u5d4c\u5165token\u7ea7\u522b\u64cd\u4f5c\u7684\u89e3\u8026\u8fde\u7eed\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5b9e\u73b0\u5c5e\u6027\u72ec\u7acb\u63a7\u5236", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7f3a\u4e4f\u5bf9\u7f16\u8f91\u8fc7\u7a0b\u7684\u7cbe\u7ec6\u63a7\u5236\uff0c\u9700\u8981\u5b9e\u73b0\u89e3\u8026\u7f16\u8f91\u5c5e\u6027\u548c\u8fde\u7eed\u5f3a\u5ea6\u8c03\u8282", "method": "\u5728\u6587\u672c\u5d4c\u5165\u7a7a\u95f4\u6784\u5efa\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u5176\u7a00\u758f\u6f5c\u5728\u7a7a\u95f4\u7684\u8bed\u4e49\u9694\u79bb\u7ef4\u5ea6\u5bfb\u627e\u5c5e\u6027\u63a7\u5236\u65b9\u5411", "result": "\u5b9e\u73b0\u4e86\u8de8\u591a\u5c5e\u6027\u548c\u9886\u57df\u7684\u8fde\u7eed\u76f4\u89c2\u7f16\u8f91\uff0c\u517c\u5bb9\u4e0d\u540c\u56fe\u50cf\u5408\u6210\u6846\u67b6\u4e14\u65e0\u9700\u4fee\u6539\u6269\u6563\u8fc7\u7a0b", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u751f\u6210\u5f0f\u56fe\u50cf\u7f16\u8f91\u63d0\u4f9b\u4e86\u6a21\u578b\u65e0\u5173\u7684\u7075\u6d3b\u63a7\u5236\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u7f16\u8f91\u7cbe\u5ea6\u548c\u6548\u7387"}}
{"id": "2510.03521", "pdf": "https://arxiv.org/pdf/2510.03521", "abs": "https://arxiv.org/abs/2510.03521", "authors": ["Ali Elahi"], "title": "Identifying Financial Risk Information Using RAG with a Contrastive Insight", "categories": ["cs.CL", "cs.AI"], "comment": "7 pages, 1 figure, Workshop on Generative AI in Finance, NeurIPS 2025", "summary": "In specialized domains, humans often compare new problems against similar\nexamples, highlight nuances, and draw conclusions instead of analyzing\ninformation in isolation. When applying reasoning in specialized contexts with\nLLMs on top of a RAG, the pipeline can capture contextually relevant\ninformation, but it is not designed to retrieve comparable cases or related\nproblems.\n  While RAG is effective at extracting factual information, its outputs in\nspecialized reasoning tasks often remain generic, reflecting broad facts rather\nthan context-specific insights. In finance, it results in generic risks that\nare true for the majority of companies. To address this limitation, we propose\na peer-aware comparative inference layer on top of RAG.\n  Our contrastive approach outperforms baseline RAG in text generation metrics\nsuch as ROUGE and BERTScore in comparison with human-generated equity research\nand risk.", "AI": {"tldr": "\u63d0\u51fa\u5728RAG\u67b6\u6784\u4e0a\u589e\u52a0\u5bf9\u6bd4\u63a8\u7406\u5c42\uff0c\u6539\u5584\u4e13\u4e1a\u9886\u57df\u6587\u672c\u751f\u6210\u7684\u4e0a\u4e0b\u6587\u76f8\u5173\u6027", "motivation": "\u4f20\u7edfRAG\u5728\u4e13\u4e1a\u9886\u57df\u4ec5\u80fd\u63d0\u53d6\u5b64\u7acb\u4e8b\u5b9e\uff0c\u7f3a\u4e4f\u6848\u4f8b\u6bd4\u8f83\u80fd\u529b\uff0c\u5bfc\u81f4\u751f\u6210\u5185\u5bb9\u8fc7\u4e8e\u901a\u7528\u5316\uff08\u5982\u91d1\u878d\u5206\u6790\u4e2d\u53ea\u80fd\u63d0\u4f9b\u666e\u9002\u6027\u98ce\u9669\u63d0\u793a\uff09", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u540c\u4faa\u611f\u77e5\u7684\u5bf9\u6bd4\u63a8\u7406\u5c42\uff0c\u901a\u8fc7\u6bd4\u8f83\u76f8\u5173\u6848\u4f8b\u5b9e\u73b0\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u5185\u5bb9\u751f\u6210", "result": "\u5728ROUGE\u548cBERTScore\u6307\u6807\u4e0a\u8d85\u8d8a\u57fa\u7ebfRAG\uff0c\u66f4\u63a5\u8fd1\u4eba\u7c7b\u64b0\u5199\u7684\u884c\u4e1a\u7814\u7a76\u62a5\u544a\u8d28\u91cf", "conclusion": "\u5bf9\u6bd4\u63a8\u7406\u673a\u5236\u6709\u6548\u63d0\u5347\u4e13\u4e1a\u9886\u57df\u751f\u6210\u5185\u5bb9\u7684\u573a\u666f\u9002\u914d\u6027\uff0c\u4e3a\u91d1\u878d\u5206\u6790\u7b49\u5782\u76f4\u9886\u57df\u63d0\u4f9b\u65b0\u7684\u89e3\u51b3\u65b9\u6848"}}
