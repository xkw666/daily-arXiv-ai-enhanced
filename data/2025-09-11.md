<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 33]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Bilingual Word Level Language Identification for Omotic Languages](https://arxiv.org/abs/2509.07998)
*Mesay Gemeda Yigezu,Girma Yohannis Bade,Atnafu Lambebo Tonja,Olga Kolesnikova,Grigori Sidorov,Alexander Gelbukh*

Main category: cs.CL

TL;DR: 该研究针对埃塞俄比亚南部的Wolaita和Gofa双语识别难题，结合BERT预训练模型与LSTM，在测试集上达到0.72 F1分数。


<details>
  <summary>Details</summary>
Motivation: 解决多语言文本（尤其是词汇相似的Wolaita和Gofa双语）在社交媒体等场景中的准确识别需求。

Method: 采用基于BERT的预训练语言模型与LSTM混合架构，通过多组对比实验验证方案有效性。

Result: 混合模型在测试集获得0.72的F1值，显著优于传统方法，具备实际应用价值。

Conclusion: 该方案有效提升相似语种识别精度，为多语言NLP应用及后续研究提供技术基础。

Abstract: Language identification is the task of determining the languages for a given
text. In many real world scenarios, text may contain more than one language,
particularly in multilingual communities. Bilingual Language Identification
(BLID) is the task of identifying and distinguishing between two languages in a
given text. This paper presents BLID for languages spoken in the southern part
of Ethiopia, namely Wolaita and Gofa. The presence of words similarities and
differences between the two languages makes the language identification task
challenging. To overcome this challenge, we employed various experiments on
various approaches. Then, the combination of the BERT based pretrained language
model and LSTM approach performed better, with an F1 score of 0.72 on the test
set. As a result, the work will be effective in tackling unwanted social media
issues and providing a foundation for further research in this area.

</details>


### [2] [AntiDote: Bi-level Adversarial Training for Tamper-Resistant LLMs](https://arxiv.org/abs/2509.08000)
*Debdeep Sanyal,Manodeep Ray,Murari Mandal*

Main category: cs.CL

TL;DR: 通过双层次优化框架AntiDote增强开源大语言模型的抗篡改能力，在保持模型性能的同时显著提升对抗恶意微调的鲁棒性（+27.4%），性能损失仅0.5%


<details>
  <summary>Details</summary>
Motivation: 现有安全方案在模型权重完全公开时无法兼顾模型能力保持与防滥用（攻击者可通过全参数微调绕过安全防护）

Method: 提出包含对抗超网络的双层次优化框架：1）超网络根据模型激活生成恶意LoRA权重 2）主模型训练目标为抵消这些对抗权重的影响

Result: 在52种攻击场景测试中，相比现有方法提升27.4%鲁棒性；在MMLU等基准测试中性能损失<0.5%

Conclusion: AntiDote为开源模型安全提供了一种计算高效的安全范式，使安全性成为更本质和鲁棒的模型属性

Abstract: The release of open-weight large language models (LLMs) creates a tension
between advancing accessible research and preventing misuse, such as malicious
fine-tuning to elicit harmful content. Current safety measures struggle to
preserve the general capabilities of the LLM while resisting a determined
adversary with full access to the model's weights and architecture, who can use
full-parameter fine-tuning to erase existing safeguards. To address this, we
introduce AntiDote, a bi-level optimization procedure for training LLMs to be
resistant to such tampering. AntiDote involves an auxiliary adversary
hypernetwork that learns to generate malicious Low-Rank Adaptation (LoRA)
weights conditioned on the defender model's internal activations. The defender
LLM is then trained with an objective to nullify the effect of these
adversarial weight additions, forcing it to maintain its safety alignment. We
validate this approach against a diverse suite of 52 red-teaming attacks,
including jailbreak prompting, latent space manipulation, and direct
weight-space attacks. AntiDote is upto 27.4\% more robust against adversarial
attacks compared to both tamper-resistance and unlearning baselines. Crucially,
this robustness is achieved with a minimal trade-off in utility, incurring a
performance degradation of upto less than 0.5\% across capability benchmarks
including MMLU, HellaSwag, and GSM8K. Our work offers a practical and compute
efficient methodology for building open-weight models where safety is a more
integral and resilient property.

</details>


### [3] [MVPBench: A Benchmark and Fine-Tuning Framework for Aligning Large Language Models with Diverse Human Values](https://arxiv.org/abs/2509.08022)
*Yao Liang,Dongcheng Zhao,Feifei Zhao,Guobin Shen,Yuwei Wang,Dongqi Liang,Yi Zeng*

Main category: cs.CL

TL;DR: 提出MVPBench基准，系统评估大语言模型在75个国家多维人类价值观对齐表现，揭示地理人口差异，验证微调方法提升效果


<details>
  <summary>Details</summary>
Motivation: 现有评估基准缺乏文化多样性考量，难以评估大语言模型的全球价值观对齐效果

Method: 构建包含24,020个标注实例的MVPBench数据集，含细粒度价值标签和人口统计元数据，采用LoRA/DPO进行微调实验

Result: 发现模型表现存在显著地域差异，微调方法在域内外均提升20%+对齐效果

Conclusion: 需建立人口感知的评估体系，MVPBench为全球对齐研究和公平AI发展提供基础支撑

Abstract: The alignment of large language models (LLMs) with human values is critical
for their safe and effective deployment across diverse user populations.
However, existing benchmarks often neglect cultural and demographic diversity,
leading to limited understanding of how value alignment generalizes globally.
In this work, we introduce MVPBench, a novel benchmark that systematically
evaluates LLMs' alignment with multi-dimensional human value preferences across
75 countries. MVPBench contains 24,020 high-quality instances annotated with
fine-grained value labels, personalized questions, and rich demographic
metadata, making it the most comprehensive resource of its kind to date. Using
MVPBench, we conduct an in-depth analysis of several state-of-the-art LLMs,
revealing substantial disparities in alignment performance across geographic
and demographic lines. We further demonstrate that lightweight fine-tuning
methods, such as Low-Rank Adaptation (LoRA) and Direct Preference Optimization
(DPO), can significantly enhance value alignment in both in-domain and
out-of-domain settings. Our findings underscore the necessity for
population-aware alignment evaluation and provide actionable insights for
building culturally adaptive and value-sensitive LLMs. MVPBench serves as a
practical foundation for future research on global alignment, personalized
value modeling, and equitable AI development.

</details>


### [4] [NOWJ@COLIEE 2025: A Multi-stage Framework Integrating Embedding Models and Large Language Models for Legal Retrieval and Entailment](https://arxiv.org/abs/2509.08025)
*Hoang-Trung Nguyen,Tan-Minh Nguyen,Xuan-Bach Le,Tuan-Kiet Le,Khanh-Huyen Nguyen,Ha-Thanh Nguyen,Thi-Hai-Yen Vuong,Le-Minh Nguyen*

Main category: cs.CL

TL;DR: NOWJ团队在COLIEE 2025竞赛中，通过混合传统IR技术与生成模型，在五项法律任务中取得突破性成果，Task 2以F1 0.3195夺冠


<details>
  <summary>Details</summary>
Motivation: 探索传统信息检索技术与大语言模型的协同效应，提升法律文本处理效果

Method: 两阶段检索系统（词汇语义过滤+LLM上下文分析）+集成模型（BM25/BERT/monoT5/BGE-m3/LLM2Vec）+大模型组合（Qwen-2/QwQ-32B/DeepSeek-V3）

Result: Task 2获F1 0.3195第一，其他任务通过提示工程与集成策略展现稳健性能

Conclusion: 混合模型架构为法律信息处理提供新范式，验证传统与生成式技术的互补优势

Abstract: This paper presents the methodologies and results of the NOWJ team's
participation across all five tasks at the COLIEE 2025 competition, emphasizing
advancements in the Legal Case Entailment task (Task 2). Our comprehensive
approach systematically integrates pre-ranking models (BM25, BERT, monoT5),
embedding-based semantic representations (BGE-m3, LLM2Vec), and advanced Large
Language Models (Qwen-2, QwQ-32B, DeepSeek-V3) for summarization, relevance
scoring, and contextual re-ranking. Specifically, in Task 2, our two-stage
retrieval system combined lexical-semantic filtering with contextualized LLM
analysis, achieving first place with an F1 score of 0.3195. Additionally, in
other tasks--including Legal Case Retrieval, Statute Law Retrieval, Legal
Textual Entailment, and Legal Judgment Prediction--we demonstrated robust
performance through carefully engineered ensembles and effective prompt-based
reasoning strategies. Our findings highlight the potential of hybrid models
integrating traditional IR techniques with contemporary generative models,
providing a valuable reference for future advancements in legal information
processing.

</details>


### [5] [SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery](https://arxiv.org/abs/2509.08032)
*Fengyu She,Nan Wang,Hongfei Wu,Ziyi Wan,Jingmian Wang,Chang Wang*

Main category: cs.CL

TL;DR: SciGPT通过领域适配和SMoE机制提升科学文献处理能力，在ScienceBench基准上超越GPT-4o并展现强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型难以捕捉科学领域专有术语和方法论严谨性，跨学科知识整合存在瓶颈。

Method: 1.两阶段领域蒸馏平衡性能效率
2.SMoE注意力机制降低55%长文档内存消耗
3.知识感知适配整合领域本体构建跨学科桥梁

Result: ScienceBench测试显示SciGPT在序列标注/生成/推理任务中优于GPT-4o，在未见任务中保持强鲁棒性

Conclusion: 该框架验证了AI增强科学发现的潜力，为领域特定LLM开发提供新范式

Abstract: Scientific literature is growing exponentially, creating a critical
bottleneck for researchers to efficiently synthesize knowledge. While
general-purpose Large Language Models (LLMs) show potential in text processing,
they often fail to capture scientific domain-specific nuances (e.g., technical
jargon, methodological rigor) and struggle with complex scientific tasks,
limiting their utility for interdisciplinary research. To address these gaps,
this paper presents SciGPT, a domain-adapted foundation model for scientific
literature understanding and ScienceBench, an open source benchmark tailored to
evaluate scientific LLMs.
  Built on the Qwen3 architecture, SciGPT incorporates three key innovations:
(1) low-cost domain distillation via a two-stage pipeline to balance
performance and efficiency; (2) a Sparse Mixture-of-Experts (SMoE) attention
mechanism that cuts memory consumption by 55\% for 32,000-token long-document
reasoning; and (3) knowledge-aware adaptation integrating domain ontologies to
bridge interdisciplinary knowledge gaps.
  Experimental results on ScienceBench show that SciGPT outperforms GPT-4o in
core scientific tasks including sequence labeling, generation, and inference.
It also exhibits strong robustness in unseen scientific tasks, validating its
potential to facilitate AI-augmented scientific discovery.

</details>


### [6] [No for Some, Yes for Others: Persona Prompts and Other Sources of False Refusal in Language Models](https://arxiv.org/abs/2509.08075)
*Flor Miriam Plaza-del-Arco,Paul Röttger,Nino Scherrer,Emanuele Borgonovo,Elmar Plischke,Dirk Hovy*

Main category: cs.CL

TL;DR: 大型语言模型个性化可能导致错误拒绝请求，但模型能力越强角色影响越小，模型选择和任务类型对错误拒绝影响更显著


<details>
  <summary>Details</summary>
Motivation: 研究LLM个性化可能引发的副作用（如错误拒绝请求），量化社会人口学角色对模型决策的影响

Method: 测试15种社会人口角色、16个模型、3个分类任务（自然语言推理/礼貌/冒犯性）和9种提示改写，使用蒙特卡洛方法量化样本效率

Result: 模型能力提升后角色影响减弱；特定角色在某些模型中增加错误拒绝，反映安全机制偏见；模型选择与敏感内容任务对错误拒绝影响最大

Conclusion: 角色效应可能被高估，实际由模型架构和任务特征主导，需在安全对齐策略中考虑模型与任务的交互影响

Abstract: Large language models (LLMs) are increasingly integrated into our daily lives
and personalized. However, LLM personalization might also increase unintended
side effects. Recent work suggests that persona prompting can lead models to
falsely refuse user requests. However, no work has fully quantified the extent
of this issue. To address this gap, we measure the impact of 15
sociodemographic personas (based on gender, race, religion, and disability) on
false refusal. To control for other factors, we also test 16 different models,
3 tasks (Natural Language Inference, politeness, and offensiveness
classification), and nine prompt paraphrases. We propose a Monte Carlo-based
method to quantify this issue in a sample-efficient manner. Our results show
that as models become more capable, personas impact the refusal rate less and
less. Certain sociodemographic personas increase false refusal in some models,
which suggests underlying biases in the alignment strategies or safety
mechanisms. However, we find that the model choice and task significantly
influence false refusals, especially in sensitive content tasks. Our findings
suggest that persona effects have been overestimated, and might be due to other
factors.

</details>


### [7] [Culturally transmitted color categories in LLMs reflect a learning bias toward efficient compression](https://arxiv.org/abs/2509.08093)
*Nathaniel Imel,Noga Zaslavsky*

Main category: cs.CL

TL;DR: 大型语言模型（Gemini/Llama）能够通过信息瓶颈原则演化出类人的高效颜色语义系统，与人类语言效率机制一致


<details>
  <summary>Details</summary>
Motivation: 验证LLMs是否具备人类特有的信息瓶颈效率倾向，而非单纯模仿训练数据模式

Method: 复制人类颜色命名研究（英语命名实验）+ 模拟文化演化（通过上下文学习迭代优化伪颜色系统）

Result: Gemini英语命名与人类高度一致且IB效率突出，Llama呈现低复杂度高效系统；模拟演化后两者均显著提升IB效率并与全球语言模式趋同

Conclusion: LLMs展现出与人类语言相同的语义效率演化能力，证明其可发展基于感知基础的人类级语义系统

Abstract: Converging evidence suggests that systems of semantic categories across human
languages achieve near-optimal compression via the Information Bottleneck (IB)
complexity-accuracy principle. Large language models (LLMs) are not trained for
this objective, which raises the question: are LLMs capable of evolving
efficient human-like semantic systems? To address this question, we focus on
the domain of color as a key testbed of cognitive theories of categorization
and replicate with LLMs (Gemini 2.0-flash and Llama 3.3-70B-Instruct) two
influential human behavioral studies. First, we conduct an English color-naming
study, showing that Gemini aligns well with the naming patterns of native
English speakers and achieves a significantly high IB-efficiency score, while
Llama exhibits an efficient but lower complexity system compared to English.
Second, to test whether LLMs simply mimic patterns in their training data or
actually exhibit a human-like inductive bias toward IB-efficiency, we simulate
cultural evolution of pseudo color-naming systems in LLMs via iterated
in-context language learning. We find that akin to humans, LLMs iteratively
restructure initially random systems towards greater IB-efficiency and
increased alignment with patterns observed across the world's languages. These
findings demonstrate that LLMs are capable of evolving perceptually grounded,
human-like semantic systems, driven by the same fundamental principle that
governs semantic efficiency across human languages.

</details>


### [8] [MERLIN: Multi-Stage Curriculum Alignment for Multilingual Encoder and LLM Fusion](https://arxiv.org/abs/2509.08105)
*Kosei Uemura,David Guzmán,Quang Phuoc Nguyen,Jesujoba Oluwadara Alabi,En-shiun Annie Lee,David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: MERLIN两阶段框架通过课程学习策略在低资源语言上实现显著性能提升（AfriMGSM +12.9pp），同时保持高资源语言效果


<details>
  <summary>Details</summary>
Motivation: 解决现有方法（如LangBridge/MindMerger）在低资源语言（LRLs）复杂推理任务上的性能缺口

Method: 1. 两阶段模型堆叠框架 2. 课程学习策略（从双语文本到任务数据）3. 仅调整少量DoRA权重

Result: AfriMGSM准确率超越MindMerger 12.9pp且优于GPT-4o-mini；MGSM/MSVAMP分别提升0.9pp/2.8pp

Conclusion: MERLIN在低/高资源语言场景均有效，证明课程学习与参数高效适配策略的协同作用

Abstract: Large language models excel in English but still struggle with complex
reasoning in many low-resource languages (LRLs). Existing encoder-plus-decoder
methods such as LangBridge and MindMerger raise accuracy on mid and
high-resource languages, yet they leave a large gap on LRLs. We present MERLIN,
a two-stage model-stacking framework that applies a curriculum learning
strategy -- from general bilingual bitext to task-specific data -- and adapts
only a small set of DoRA weights. On the AfriMGSM benchmark MERLIN improves
exact-match accuracy by +12.9 pp over MindMerger and outperforms GPT-4o-mini.
It also yields consistent gains on MGSM and MSVAMP (+0.9 and +2.8 pp),
demonstrating effectiveness across both low and high-resource settings.

</details>


### [9] [Bias after Prompting: Persistent Discrimination in Large Language Models](https://arxiv.org/abs/2509.08146)
*Nivedha Sivakumar,Natalie Mackraz,Samira Khorshidi,Krishna Patel,Barry-John Theobald,Luca Zappella,Nicholas Apostoloff*

Main category: cs.CL

TL;DR: 研究表明预训练大语言模型（LLM）中的偏见可能通过提示调整传递到下游任务，现有提示去偏方法无法完全阻断这种传递


<details>
  <summary>Details</summary>
Motivation: 推翻先前关于'预训练模型偏见不会传递到适应模型'的假设，验证提示调整这种主流适配方式中的偏见传递现象

Method: 通过因果模型分析提示调整过程中的偏见传递，测试不同人口统计特征（性别/年龄/宗教）在共指消解和问答任务中的表现，评估少样本参数变化对偏见传递的影响

Result: 观察到强相关性（性别ρ≥0.94，年龄ρ≥0.98），参数调整后偏见保持强相关（ρ≥0.90），现有去偏策略效果参差不齐且无普适性解决方案

Conclusion: 修正基础模型的偏见可能有效预防下游任务中的偏见传播，提示工程需结合内在模型改进才能实现可靠去偏

Abstract: A dangerous assumption that can be made from prior work on the bias transfer
hypothesis (BTH) is that biases do not transfer from pre-trained large language
models (LLMs) to adapted models. We invalidate this assumption by studying the
BTH in causal models under prompt adaptations, as prompting is an extremely
popular and accessible adaptation strategy used in real-world applications. In
contrast to prior work, we find that biases can transfer through prompting and
that popular prompt-based mitigation methods do not consistently prevent biases
from transferring. Specifically, the correlation between intrinsic biases and
those after prompt adaptation remain moderate to strong across demographics and
tasks -- for example, gender (rho >= 0.94) in co-reference resolution, and age
(rho >= 0.98) and religion (rho >= 0.69) in question answering. Further, we
find that biases remain strongly correlated when varying few-shot composition
parameters, such as sample size, stereotypical content, occupational
distribution and representational balance (rho >= 0.90). We evaluate several
prompt-based debiasing strategies and find that different approaches have
distinct strengths, but none consistently reduce bias transfer across models,
tasks or demographics. These results demonstrate that correcting bias, and
potentially improving reasoning ability, in intrinsic models may prevent
propagation of biases to downstream tasks.

</details>


### [10] [Verbalized Algorithms](https://arxiv.org/abs/2509.08150)
*Supriya Lall,Christian Farrell,Hari Pathanjaly,Marko Pavic,Sarvesh Chezhian,Masataro Asai*

Main category: cs.CL

TL;DR: 提出语言化算法（VAs），将经典算法与LLM作为基础操作模块结合，提升复杂任务处理的可靠性


<details>
  <summary>Details</summary>
Motivation: 传统LLM单次查询方式在复杂推理任务中存在可靠性问题，需要将任务分解为可控的原子操作

Method: 将任务拆解为自然语言原子操作（如二元比较），在经典算法框架（如双调排序网络）中系统集成LLM

Result: 在排序和聚类任务中验证了该范式的有效性

Conclusion: 语言化算法通过限制LLM仅处理简单子任务，结合经典算法框架，显著提升了任务执行的可靠性

Abstract: Instead of querying LLMs in a one-shot manner and hoping to get the right
answer for a reasoning task, we propose a paradigm we call \emph{verbalized
algorithms} (VAs), which leverage classical algorithms with established
theoretical understanding. VAs decompose a task into simple elementary
operations on natural language strings that they should be able to answer
reliably, and limit the scope of LLMs to only those simple tasks. For example,
for sorting a series of natural language strings, \emph{verbalized sorting}
uses an LLM as a binary comparison oracle in a known and well-analyzed sorting
algorithm (e.g., bitonic sorting network). We demonstrate the effectiveness of
this approach on sorting and clustering tasks.

</details>


### [11] [Balancing Quality and Variation: Spam Filtering Distorts Data Label Distributions](https://arxiv.org/abs/2509.08217)
*Eve Fleisig,Matthias Orlikowski,Philipp Cimiano,Dan Klein*

Main category: cs.CL

TL;DR: 研究发现在需保留标注多样性的任务中，现有垃圾过滤方法因错误假设导致效果不佳，保守标注者移除策略（<5%）效果最佳，需开发兼顾多样性的新方法。


<details>
  <summary>Details</summary>
Motivation: 传统标注过滤方法在单一正确答案场景下设计，但主观任务需保留合理意见差异。如何平衡标注者可靠性（过滤垃圾）与群体代表性（保留多样性）成为核心问题。

Method: 通过实证评估多种标注过滤启发式方法对标签变异性的影响，并基于合成垃圾数据分析现有方法的假设缺陷。

Result: 1. 超过5%的标注者移除显著增加标签平均绝对误差；2. 现有方法易误删合理异议标注者；3. 多数垃圾标注者分布与正常标注者无异，可识别垃圾标注者多采用固定答案模式。

Conclusion: 主观任务的垃圾过滤需颠覆传统假设（变异=噪声），开发能区分合理意见分歧与真实垃圾的新方法，重点关注标注者行为模式（固定答案vs合理随机性）。

Abstract: For machine learning datasets to accurately represent diverse opinions in a
population, they must preserve variation in data labels while filtering out
spam or low-quality responses. How can we balance annotator reliability and
representation? We empirically evaluate how a range of heuristics for annotator
filtering affect the preservation of variation on subjective tasks. We find
that these methods, designed for contexts in which variation from a single
ground-truth label is considered noise, often remove annotators who disagree
instead of spam annotators, introducing suboptimal tradeoffs between accuracy
and label diversity. We find that conservative settings for annotator removal
(<5%) are best, after which all tested methods increase the mean absolute error
from the true average label. We analyze performance on synthetic spam to
observe that these methods often assume spam annotators are less random than
real spammers tend to be: most spammers are distributionally indistinguishable
from real annotators, and the minority that are distinguishable tend to give
fixed answers, not random ones. Thus, tasks requiring the preservation of
variation reverse the intuition of existing spam filtering methods: spammers
tend to be less random than non-spammers, so metrics that assume variation is
spam fare worse. These results highlight the need for spam removal methods that
account for label diversity.

</details>


### [12] [Towards Knowledge-Aware Document Systems: Modeling Semantic Coverage Relations via Answerability Detection](https://arxiv.org/abs/2509.08304)
*Yehudit Aperstein,Alon Gottlib,Gal Benita,Alexander Apartsin*

Main category: cs.CL

TL;DR: 提出基于问答的语义覆盖关系分类框架，实验显示判别模型优于生成方法


<details>
  <summary>Details</summary>
Motivation: 理解不同格式文档间的信息共享机制对信息检索、摘要生成等任务至关重要，需系统化分类文档信息对齐方式

Method: 定义三种语义覆盖关系（等价/包含/重叠），采用问答可答性指标，构建SQuAD衍生的合成数据集，比较判别模型与生成模型的性能

Result: RoBERTa-base模型达61.4%准确率，随机森林模型获52.9%宏F1分数，判别模型全面优于生成方法

Conclusion: 问答方法能有效评估跨文本语义关系，判别模型展现更强推理能力，相关数据集和代码已开源促进可复现性

Abstract: Understanding how information is shared across documents, regardless of the
format in which it is expressed, is critical for tasks such as information
retrieval, summarization, and content alignment. In this work, we introduce a
novel framework for modelling Semantic Coverage Relations (SCR), which
classifies document pairs based on how their informational content aligns. We
define three core relation types: equivalence, where both texts convey the same
information using different textual forms or styles; inclusion, where one
document fully contains the information of another and adds more; and semantic
overlap, where each document presents partially overlapping content. To capture
these relations, we adopt a question answering (QA)-based approach, using the
answerability of shared questions across documents as an indicator of semantic
coverage. We construct a synthetic dataset derived from the SQuAD corpus by
paraphrasing source passages and selectively omitting information, enabling
precise control over content overlap. This dataset allows us to benchmark
generative language models and train transformer-based classifiers for SCR
prediction. Our findings demonstrate that discriminative models significantly
outperform generative approaches, with the RoBERTa-base model achieving the
highest accuracy of 61.4% and the Random Forest-based model showing the best
balance with a macro-F1 score of 52.9%. The results show that QA provides an
effective lens for assessing semantic relations across stylistically diverse
texts, offering insights into the capacity of current models to reason about
information beyond surface similarity. The dataset and code developed in this
study are publicly available to support reproducibility.

</details>


### [13] [Toward Subtrait-Level Model Explainability in Automated Writing Evaluation](https://arxiv.org/abs/2509.08345)
*Alejandro Andrade-Lotero,Lee Becker,Joshua Southerland,Scott Hellman*

Main category: cs.CL

TL;DR: 利用生成语言模型进行写作子特质评估，提升自动评分的可解释性


<details>
  <summary>Details</summary>
Motivation: 通过分解写作特质的潜在成分，为教育者和学生提供更透明的评分依据

Method: 构建生成式语言模型原型，对比人类专家与自动系统的子特质评分相关性

Result: 人类子特质评分与总体特质评分呈中度相关，自动系统与人类子特质评分相关性达到0.45-0.65

Conclusion: 子特质评估方法有效提升评分透明度，为教学反馈提供了可解释的维度分析

Abstract: Subtrait (latent-trait components) assessment presents a promising path
toward enhancing transparency of automated writing scores. We prototype
explainability and subtrait scoring with generative language models and show
modest correlation between human subtrait and trait scores, and between
automated and human subtrait scores. Our approach provides details to demystify
scores for educators and students.

</details>


### [14] [Automatic Detection of Inauthentic Templated Responses in English Language Assessments](https://arxiv.org/abs/2509.08355)
*Yashad Samant,Lee Becker,Scott Hellman,Bradley Behan,Sarah Hughes,Joshua Southerland*

Main category: cs.CL

TL;DR: 提出自动化检测英语考试模板化应答系统AuDITR，采用机器学习方法并强调模型定期更新的重要性


<details>
  <summary>Details</summary>
Motivation: 解决考生利用记忆模板欺骗自动评分系统的问题，维护高风险语言评估的公正性

Method: 基于机器学习的模板化应答检测方法开发

Result: 成功构建检测系统并验证模型迭代更新的必要性

Conclusion: 持续更新检测模型是维持考试诚信的关键措施

Abstract: In high-stakes English Language Assessments, low-skill test takers may employ
memorized materials called ``templates'' on essay questions to ``game'' or fool
the automated scoring system. In this study, we introduce the automated
detection of inauthentic, templated responses (AuDITR) task, describe a machine
learning-based approach to this task and illustrate the importance of regularly
updating these models in production.

</details>


### [15] [<think> So let's replace this phrase with insult... </think> Lessons learned from generation of toxic texts with LLMs](https://arxiv.org/abs/2509.08358)
*Sergey Pletenev,Daniil Moskovskiy,Alexander Panchenko*

Main category: cs.CL

TL;DR: 大语言模型生成的合成有毒数据训练去毒模型时性能下降30%，主要归因于LLM生成内容的词汇多样性不足


<details>
  <summary>Details</summary>
Motivation: 探索用LLM生成的有毒合成数据替代人工标注数据训练去毒模型的可行性

Method: 使用Llama 3和Qwen生成ParaDetox/SST-2中性文本的有毒版本，对比合成数据与人类数据训练效果

Result: 合成数据训练的模型性能持续低于人类数据，联合指标下降达30%

Conclusion: 当前LLM生成内容存在词汇多样性缺陷，构建鲁棒去毒系统仍需多样化人工标注数据

Abstract: Modern Large Language Models (LLMs) are excellent at generating synthetic
data. However, their performance in sensitive domains such as text
detoxification has not received proper attention from the scientific community.
This paper explores the possibility of using LLM-generated synthetic toxic data
as an alternative to human-generated data for training models for
detoxification. Using Llama 3 and Qwen activation-patched models, we generated
synthetic toxic counterparts for neutral texts from ParaDetox and SST-2
datasets. Our experiments show that models fine-tuned on synthetic data
consistently perform worse than those trained on human data, with a drop in
performance of up to 30% in joint metrics. The root cause is identified as a
critical lexical diversity gap: LLMs generate toxic content using a small,
repetitive vocabulary of insults that fails to capture the nuances and variety
of human toxicity. These findings highlight the limitations of current LLMs in
this domain and emphasize the continued importance of diverse, human-annotated
data for building robust detoxification systems.

</details>


### [16] [Low-Resource Fine-Tuning for Multi-Task Structured Information Extraction with a Billion-Parameter Instruction-Tuned Model](https://arxiv.org/abs/2509.08381)
*Yu Cheng Chih,Yong Hao Hou*

Main category: cs.CL

TL;DR: 小型LLaMA模型ETLCH通过低秩适应微调，在低资源多任务场景下实现高效结构化数据提取，性能优于基线且计算成本显著降低


<details>
  <summary>Details</summary>
Motivation: 解决大模型在结构化数据提取场景中部署成本高、数据准备困难的问题，验证小模型在低资源多任务条件下的可行性

Method: 基于LLaMA架构构建十亿参数模型ETLCH，采用低秩适应(LoRA)方法，每个任务仅用数百至一千样本微调，支持JSON提取、知识图谱构建和命名实体识别

Result: 在多数评估指标上超越基线模型，尤其在最低数据量时提升显著（低数据规模下增益最大）

Conclusion: 精细化调优的小模型能够以极低计算成本提供稳定可靠的结构化输出，为资源受限环境提供经济高效的信息抽取解决方案

Abstract: Deploying large language models (LLMs) for structured data extraction in
domains such as financial compliance reporting, legal document analytics, and
multilingual knowledge base construction is often impractical for smaller teams
due to the high cost of running large architectures and the difficulty of
preparing large, high-quality datasets. Most recent instruction-tuning studies
focus on seven-billion-parameter or larger models, leaving limited evidence on
whether much smaller models can work reliably under low-resource, multi-task
conditions. This work presents ETLCH, a billion-parameter LLaMA-based model
fine-tuned with low-rank adaptation on only a few hundred to one thousand
samples per task for JSON extraction, knowledge graph extraction, and named
entity recognition. Despite its small scale, ETLCH outperforms strong baselines
across most evaluation metrics, with substantial gains observed even at the
lowest data scale. These findings demonstrate that well-tuned small models can
deliver stable and accurate structured outputs at a fraction of the
computational cost, enabling cost-effective and reliable information extraction
pipelines in resource-constrained environments.

</details>


### [17] [CommonVoice-SpeechRE and RPG-MoGe: Advancing Speech Relation Extraction with a New Dataset and Multi-Order Generative Framework](https://arxiv.org/abs/2509.08438)
*Jinzhong Ning,Paerhati Tulajiang,Yingying Le,Yijia Zhang,Yuanyuan Sun,Hongfei Lin,Haifeng Liu*

Main category: cs.CL

TL;DR: 论文提出首个大规模真实人类语音关系抽取数据集CommonVoice-SpeechRE，并设计多序生成集成框架RPG-MoGe解决现有模型生成模板单一和语义对齐弱的问题。


<details>
  <summary>Details</summary>
Motivation: 现有SpeechRE研究依赖合成数据且模型存在单序生成模板僵化、跨模态对齐不足的缺陷，无法满足真实场景需求。

Method: 1. 构建含2万真实语音的基准数据集；2. 提出多序三元组生成集成策略，通过训练/推理阶段的元素顺序多样性增强数据利用；3. 基于CNN的潜在关系预测头生成显式关系提示指导跨模态对齐。

Result: 实验表明RPG-MoGe超越现有SOTA方法，在真实场景SpeechRE任务中F1值提升显著（具体数值需参考原文）。

Conclusion: 同时提供基准数据集和有效解决方案，推动语音关系抽取的实际应用。代码与数据集已开源。

Abstract: Speech Relation Extraction (SpeechRE) aims to extract relation triplets
directly from speech. However, existing benchmark datasets rely heavily on
synthetic data, lacking sufficient quantity and diversity of real human speech.
Moreover, existing models also suffer from rigid single-order generation
templates and weak semantic alignment, substantially limiting their
performance. To address these challenges, we introduce CommonVoice-SpeechRE, a
large-scale dataset comprising nearly 20,000 real-human speech samples from
diverse speakers, establishing a new benchmark for SpeechRE research.
Furthermore, we propose the Relation Prompt-Guided Multi-Order Generative
Ensemble (RPG-MoGe), a novel framework that features: (1) a multi-order triplet
generation ensemble strategy, leveraging data diversity through diverse element
orders during both training and inference, and (2) CNN-based latent relation
prediction heads that generate explicit relation prompts to guide cross-modal
alignment and accurate triplet generation. Experiments show our approach
outperforms state-of-the-art methods, providing both a benchmark dataset and an
effective solution for real-world SpeechRE. The source code and dataset are
publicly available at https://github.com/NingJinzhong/SpeechRE_RPG_MoGe.

</details>


### [18] [Adversarial Attacks Against Automated Fact-Checking: A Survey](https://arxiv.org/abs/2509.08463)
*Fanzhen Liu,Alsharif Abuadbba,Kristen Moore,Surya Nepal,Cecile Paris,Jia Wu,Jian Yang,Quan Z. Sheng*

Main category: cs.CL

TL;DR: 针对自动化事实核查系统脆弱性的首篇系统性综述，揭示对抗攻击对模型可靠性的威胁并呼吁构建防御框架


<details>
  <summary>Details</summary>
Motivation: 虚假信息泛滥时代中自动化事实核查系统面临对抗攻击威胁（如篡改声明/证据/关联关系），当前缺乏对攻击策略、模型鲁棒性评估及防御机制的系统性研究

Method: 通过文献综述方法对现有对抗攻击技术进行分类学分析，评估不同攻击方式对事实核查系统的影响，同时系统梳理防御机制并提出开放研究问题

Result: 发现现有模型普遍存在安全漏洞，提出通过对抗训练、鲁棒性验证框架等方法可提升系统防御能力，但当前仍缺乏统一评估基准

Conclusion: 构建抗对抗攻击的事实核查框架具有紧迫性，需在攻击场景建模、动态防御机制、多模态证据验证等方向深化研究以维护核查准确性

Abstract: In an era where misinformation spreads freely, fact-checking (FC) plays a
crucial role in verifying claims and promoting reliable information. While
automated fact-checking (AFC) has advanced significantly, existing systems
remain vulnerable to adversarial attacks that manipulate or generate claims,
evidence, or claim-evidence pairs. These attacks can distort the truth, mislead
decision-makers, and ultimately undermine the reliability of FC models. Despite
growing research interest in adversarial attacks against AFC systems, a
comprehensive, holistic overview of key challenges remains lacking. These
challenges include understanding attack strategies, assessing the resilience of
current models, and identifying ways to enhance robustness. This survey
provides the first in-depth review of adversarial attacks targeting FC,
categorizing existing attack methodologies and evaluating their impact on AFC
systems. Additionally, we examine recent advancements in adversary-aware
defenses and highlight open research questions that require further
exploration. Our findings underscore the urgent need for resilient FC
frameworks capable of withstanding adversarial manipulations in pursuit of
preserving high verification accuracy.

</details>


### [19] [Acquiescence Bias in Large Language Models](https://arxiv.org/abs/2509.08480)
*Daniel Braun*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLMs）存在与人类相反的'默许偏差'，倾向于回答'否'，且不受任务或语言影响。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs易受输入细微变化影响且训练数据源自人类，探究其是否具有类似人类的默许偏差。

Method: 通过跨模型（不同LLMs）、多任务、多语言（英/德/波兰语）的实验设计进行系统性测试。

Result: LLMs表现出显著偏向否定回答的倾向，与人类默认同意倾向形成鲜明对比。

Conclusion: LLMs存在系统性回答偏差，这种'否定偏向'揭示了其决策机制与人类认知模式的本质差异。

Abstract: Acquiescence bias, i.e. the tendency of humans to agree with statements in
surveys, independent of their actual beliefs, is well researched and
documented. Since Large Language Models (LLMs) have been shown to be very
influenceable by relatively small changes in input and are trained on
human-generated data, it is reasonable to assume that they could show a similar
tendency. We present a study investigating the presence of acquiescence bias in
LLMs across different models, tasks, and languages (English, German, and
Polish). Our results indicate that, contrary to humans, LLMs display a bias
towards answering no, regardless of whether it indicates agreement or
disagreement.

</details>


### [20] [Simulating Identity, Propagating Bias: Abstraction and Stereotypes in LLM-Generated Text](https://arxiv.org/abs/2509.08484)
*Pia Sommerauer,Giulia Rambelli,Tommaso Caselli*

Main category: cs.CL

TL;DR: 研究探讨Persona-prompting对LLMs语言抽象性的影响，发现其可能无意中强化刻板印象，即使使用边缘群体身份提示


<details>
  <summary>Details</summary>
Motivation: 探索个性化提示技术如何影响LLMs对社会群体的表征，特别是通过语言抽象性（刻板印象标记）的视角

Method: 使用六个开源LLM在三种提示条件下生成文本，结合Linguistic Expectancy Bias框架和自建Reddit数据集Self-Stereo，通过具体性/特异性/否定性三个指标量化抽象程度

Result: 人物提示在调节语言抽象性方面效果有限，且可能传播刻板印象，挑战了人物作为社会群体代表的生态效度

Conclusion: 需重新评估人物提示策略的设计，警惕其无意强化社会偏见的风险，即使意图是代表弱势群体

Abstract: Persona-prompting is a growing strategy to steer LLMs toward simulating
particular perspectives or linguistic styles through the lens of a specified
identity. While this method is often used to personalize outputs, its impact on
how LLMs represent social groups remains underexplored. In this paper, we
investigate whether persona-prompting leads to different levels of linguistic
abstraction - an established marker of stereotyping - when generating short
texts linking socio-demographic categories with stereotypical or
non-stereotypical attributes. Drawing on the Linguistic Expectancy Bias
framework, we analyze outputs from six open-weight LLMs under three prompting
conditions, comparing 11 persona-driven responses to those of a generic AI
assistant. To support this analysis, we introduce Self-Stereo, a new dataset of
self-reported stereotypes from Reddit. We measure abstraction through three
metrics: concreteness, specificity, and negation. Our results highlight the
limits of persona-prompting in modulating abstraction in language, confirming
criticisms about the ecology of personas as representative of socio-demographic
groups and raising concerns about the risk of propagating stereotypes even when
seemingly evoking the voice of a marginalized group.

</details>


### [21] [Too Helpful, Too Harmless, Too Honest or Just Right?](https://arxiv.org/abs/2509.08486)
*Gautam Siddharth Kashyap,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 提出TrinityX框架，通过校准专家混合机制同时优化大语言模型在帮助性/无害性/诚实性三个维度的对齐表现，在效果和效率上均超越基线


<details>
  <summary>Details</summary>
Motivation: 现有方法单独优化HHH维度导致性能权衡，而传统MoE架构存在路由校准不足的问题。需开发能协同优化多个对齐维度并提升资源效率的解决方案

Method: 在Transformer架构中集成校准专家混合模块(MoCaE)，通过任务自适应的校准路由机制整合三个HHH维度的专家输出，形成统一的对齐感知表示

Result: 在三大基准测试中相对提升32.5%胜率/33.9%安全性/28.4%真实性，同时减少40%内存和延迟。消融实验验证校准路由的关键作用

Conclusion: TrinityX证明多维度协同对齐的可行性，校准路由机制显著提升模型性能与效率，其设计范式可推广至不同LLM架构

Abstract: Large Language Models (LLMs) exhibit strong performance across a wide range
of NLP tasks, yet aligning their outputs with the principles of Helpfulness,
Harmlessness, and Honesty (HHH) remains a persistent challenge. Existing
methods often optimize for individual alignment dimensions in isolation,
leading to trade-offs and inconsistent behavior. While Mixture-of-Experts (MoE)
architectures offer modularity, they suffer from poorly calibrated routing,
limiting their effectiveness in alignment tasks. We propose TrinityX, a modular
alignment framework that incorporates a Mixture of Calibrated Experts (MoCaE)
within the Transformer architecture. TrinityX leverages separately trained
experts for each HHH dimension, integrating their outputs through a calibrated,
task-adaptive routing mechanism that combines expert signals into a unified,
alignment-aware representation. Extensive experiments on three standard
alignment benchmarks-Alpaca (Helpfulness), BeaverTails (Harmlessness), and
TruthfulQA (Honesty)-demonstrate that TrinityX outperforms strong baselines,
achieving relative improvements of 32.5% in win rate, 33.9% in safety score,
and 28.4% in truthfulness. In addition, TrinityX reduces memory usage and
inference latency by over 40% compared to prior MoE-based approaches. Ablation
studies highlight the importance of calibrated routing, and cross-model
evaluations confirm TrinityX's generalization across diverse LLM backbones.

</details>


### [22] [CM-Align: Consistency-based Multilingual Alignment for Large Language Models](https://arxiv.org/abs/2509.08541)
*Xue Zhang,Yunlong Liang,Fandong Meng,Songming Zhang,Yufeng Chen,Jinan Xu,Jie Zhou*

Main category: cs.CL

TL;DR: 提出CM-Align方法，通过一致性数据筛选提升多语言模型对齐效果


<details>
  <summary>Details</summary>
Motivation: 现有方法存在英语参考质量不稳定和偏好数据构建偏倚的问题，导致多语言对齐效果受限

Method: 包含一致性引导的英语参考筛选 + 跨语言一致性多语言偏好对构建

Result: 在三个大模型和任务中验证有效性，提升多语言对齐性能

Conclusion: 构建高质量偏好数据对提升模型多语言对齐能力具有关键作用

Abstract: Current large language models (LLMs) generally show a significant performance
gap in alignment between English and other languages. To bridge this gap,
existing research typically leverages the model's responses in English as a
reference to select the best/worst responses in other languages, which are then
used for Direct Preference Optimization (DPO) training. However, we argue that
there are two limitations in the current methods that result in noisy
multilingual preference data and further limited alignment performance: 1) Not
all English responses are of high quality, and using a response with low
quality may mislead the alignment for other languages. 2) Current methods
usually use biased or heuristic approaches to construct multilingual preference
pairs. To address these limitations, we design a consistency-based data
selection method to construct high-quality multilingual preference data for
improving multilingual alignment (CM-Align). Specifically, our method includes
two parts: consistency-guided English reference selection and cross-lingual
consistency-based multilingual preference data construction. Experimental
results on three LLMs and three common tasks demonstrate the effectiveness and
superiority of our method, which further indicates the necessity of
constructing high-quality preference data.

</details>


### [23] [LLM Ensemble for RAG: Role of Context Length in Zero-Shot Question Answering for BioASQ Challenge](https://arxiv.org/abs/2509.08596)
*Dima Galat,Diego Molla-Aliod*

Main category: cs.CL

TL;DR: 集成多个零样本大语言模型结合检索增强生成(RAG)，无需微调即可实现生物医学问答的先进性能


<details>
  <summary>Details</summary>
Motivation: 解决生物医学领域QA任务中专业知识复杂、数据更新快、精准检索困难等核心挑战

Method: 集成Anthropic和Google等不同LLM变体输出，构建RAG流程并分析上下文长度对性能的影响

Result: 在BioASQ任务中超越单模型表现，部分指标媲美领域定制系统，且保持跨领域通用性

Conclusion: 基于RAG的集成零样本方法为生物医学QA提供了高效解决方案，强调精确检索对模型性能的关键作用

Abstract: Biomedical question answering (QA) poses significant challenges due to the
need for precise interpretation of specialized knowledge drawn from a vast,
complex, and rapidly evolving corpus. In this work, we explore how large
language models (LLMs) can be used for information retrieval (IR), and an
ensemble of zero-shot models can accomplish state-of-the-art performance on a
domain-specific Yes/No QA task. Evaluating our approach on the BioASQ challenge
tasks, we show that ensembles can outperform individual LLMs and in some cases
rival or surpass domain-tuned systems - all while preserving generalizability
and avoiding the need for costly fine-tuning or labeled data. Our method
aggregates outputs from multiple LLM variants, including models from Anthropic
and Google, to synthesize more accurate and robust answers. Moreover, our
investigation highlights a relationship between context length and performance:
while expanded contexts are meant to provide valuable evidence, they
simultaneously risk information dilution and model disorientation. These
findings emphasize IR as a critical foundation in Retrieval-Augmented
Generation (RAG) approaches for biomedical QA systems. Precise, focused
retrieval remains essential for ensuring LLMs operate within relevant
information boundaries when generating answers from retrieved documents. Our
results establish that ensemble-based zero-shot approaches, when paired with
effective RAG pipelines, constitute a practical and scalable alternative to
domain-tuned systems for biomedical question answering.

</details>


### [24] [Memorization in Large Language Models in Medicine: Prevalence, Characteristics, and Implications](https://arxiv.org/abs/2509.08604)
*Anran Li,Lingfei Qian,Mengmeng Du,Yu Yin,Yan Hu,Zihao Sun,Yihang Fu,Erica Stutz,Xuguang Ai,Qianqian Xie,Rui Zhu,Jimin Huang,Yifan Yang,Siru Liu,Yih-Chung Tham,Lucila Ohno-Machado,Hyunghoon Cho,Zhiyong Lu,Hua Xu,Qingyu Chen*

Main category: cs.CL

TL;DR: 首次系统评估医学领域大型语言模型的记忆现象，揭示其在三种适应场景下的普遍性及分类影响，并提出针对性优化建议


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在医学应用中记忆训练数据的程度及其对医疗任务（如诊断辅助、临床信息合成）的实际影响

Method: 通过三种适应场景评估：1)医学语料继续预训练 2)标准医学基准微调 3)耶鲁纽黑文健康系统13,000+真实临床数据微调

Result: 医学领域记忆率显著高于通用领域，识别三类记忆：有益（临床指南/生物医学参考）、无信息（模板化语言）、有害（敏感患者数据泄露）

Conclusion: 建议建立分层优化框架：增强领域推理的有益记忆，抑制表面模式的无意义记忆，建立敏感信息泄露防护机制

Abstract: Large Language Models (LLMs) have demonstrated significant potential in
medicine. To date, LLMs have been widely applied to tasks such as diagnostic
assistance, medical question answering, and clinical information synthesis.
However, a key open question remains: to what extent do LLMs memorize medical
training data. In this study, we present the first comprehensive evaluation of
memorization of LLMs in medicine, assessing its prevalence (how frequently it
occurs), characteristics (what is memorized), volume (how much content is
memorized), and potential downstream impacts (how memorization may affect
medical applications). We systematically analyze common adaptation scenarios:
(1) continued pretraining on medical corpora, (2) fine-tuning on standard
medical benchmarks, and (3) fine-tuning on real-world clinical data, including
over 13,000 unique inpatient records from Yale New Haven Health System. The
results demonstrate that memorization is prevalent across all adaptation
scenarios and significantly higher than reported in the general domain.
Memorization affects both the development and adoption of LLMs in medicine and
can be categorized into three types: beneficial (e.g., accurate recall of
clinical guidelines and biomedical references), uninformative (e.g., repeated
disclaimers or templated medical document language), and harmful (e.g.,
regeneration of dataset-specific or sensitive clinical content). Based on these
findings, we offer practical recommendations to facilitate beneficial
memorization that enhances domain-specific reasoning and factual accuracy,
minimize uninformative memorization to promote deeper learning beyond
surface-level patterns, and mitigate harmful memorization to prevent the
leakage of sensitive or identifiable patient information.

</details>


### [25] [OTESGN:Optimal Transport Enhanced Syntactic-Semantic Graph Networks for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2509.08612)
*Xinfeng Liao,Xuanqi Chen,Lianxi Wang,Jiahuan Yang,Zhuowei Chen,Ziying Rong*

Main category: cs.CL

TL;DR: 提出OTESGN模型，通过句法-语义协作注意力机制提升ABSA任务性能，在Twitter和Laptop14数据集实现F1值+1.01%/+1.30%提升


<details>
  <summary>Details</summary>
Motivation: 现有ABSA方法依赖线性点积特征，难以捕捉非线性关联且易受文本噪声干扰，导致关键情感信号被掩盖

Method: OTESGN模型包含：1) 句法图感知注意力挖掘潜在依赖关系 2) 语义最优传输注意力消除噪声干扰 3) 自适应注意力融合模块 4) 对比正则化增强鲁棒性

Result: 实验显示F1值超越SOTA模型（Twitter +1.01%，Laptop14 +1.30%），消融实验验证模块有效性，可视化证实精准定位意见词能力

Conclusion: 该模型通过联合优化句法拓扑与语义对齐，显著提升情感分析的抗噪能力和细粒度语义理解，为复杂文本分析提供新思路

Abstract: Aspect-based sentiment analysis (ABSA) aims to identify aspect terms and
determine their sentiment polarity. While dependency trees combined with
contextual semantics effectively identify aspect sentiment, existing methods
relying on syntax trees and aspect-aware attention struggle to model complex
semantic relationships. Their dependence on linear dot-product features fails
to capture nonlinear associations, allowing noisy similarity from irrelevant
words to obscure key opinion terms. Motivated by Differentiable Optimal
Matching, we propose the Optimal Transport Enhanced Syntactic-Semantic Graph
Network (OTESGN), which introduces a Syntactic-Semantic Collaborative
Attention. It comprises a Syntactic Graph-Aware Attention for mining latent
syntactic dependencies and modeling global syntactic topology, as well as a
Semantic Optimal Transport Attention designed to uncover fine-grained semantic
alignments amidst textual noise, thereby accurately capturing sentiment signals
obscured by irrelevant tokens. A Adaptive Attention Fusion module integrates
these heterogeneous features, and contrastive regularization further improves
robustness. Experiments demonstrate that OTESGN achieves state-of-the-art
results, outperforming previous best models by +1.01% F1 on Twitter and +1.30%
F1 on Laptop14 benchmarks. Ablative studies and visual analyses corroborate its
efficacy in precise localization of opinion words and noise resistance.

</details>


### [26] [X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to Single-turn Jailbreak Templates](https://arxiv.org/abs/2509.08729)
*Hyunjun Kim,Junwoo Ha,Sangyoon Yu,Haon Park*

Main category: cs.CL

TL;DR: 提出自动化框架X-Teaming Evolutionary M2S，通过语言模型引导的进化算法优化多轮红队测试模板，实现44.8%的成功率并揭示提示长度与效果的正相关性。


<details>
  <summary>Details</summary>
Motivation: 解决传统多轮红队测试依赖人工模板的问题，通过自动化框架提升单轮探针测试的效率和效果。

Method: 采用进化算法框架：12种数据源的智能采样+类StrongREJECT的LLM自动评估机制，通过θ=0.70的筛选阈值进行五代进化。

Result: 在GPT-4.1上实现44.8%成功率（103/230），跨模型测试显示结构增益可迁移但模型差异显著，发现提示长度与评分正相关（r=0.78）。

Conclusion: 结构级搜索是提升单轮测试的有效路径，强调阈值校准和跨模型评估的重要性，开源框架促进可复现研究。

Abstract: Multi-turn-to-single-turn (M2S) compresses iterative red-teaming into one
structured prompt, but prior work relied on a handful of manually written
templates. We present X-Teaming Evolutionary M2S, an automated framework that
discovers and optimizes M2S templates through language-model-guided evolution.
The system pairs smart sampling from 12 sources with an LLM-as-judge inspired
by StrongREJECT and records fully auditable logs.
  Maintaining selection pressure by setting the success threshold to $\theta =
0.70$, we obtain five evolutionary generations, two new template families, and
44.8% overall success (103/230) on GPT-4.1. A balanced cross-model panel of
2,500 trials (judge fixed) shows that structural gains transfer but vary by
target; two models score zero at the same threshold. We also find a positive
coupling between prompt length and score, motivating length-aware judging.
  Our results demonstrate that structure-level search is a reproducible route
to stronger single-turn probes and underscore the importance of threshold
calibration and cross-model evaluation. Code, configurations, and artifacts are
available at https://github.com/hyunjun1121/M2S-x-teaming.

</details>


### [27] [Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling](https://arxiv.org/abs/2509.08753)
*Neil Zeghidour,Eugene Kharitonov,Manu Orsini,Václav Volhejn,Gabriel de Marmiesse,Edouard Grave,Patrick Pérez,Laurent Mazaré,Alexandre Défossez*

Main category: cs.CL

TL;DR: 提出延迟流建模（DSM）框架，通过预处理延迟实现多模态流式推理，在ASR和TTS任务中实现SOTA效果


<details>
  <summary>Details</summary>
Motivation: 传统流式模型需学习输入输出策略，离线模型需完整输入序列。DSM通过预处理对齐和延迟机制，支持任意输出序列的流式推理，解决多模态任务局限性

Method: 使用仅解码器语言模型处理预对齐流，在文本/音频流间引入延迟（ASR延迟文本流，TTS延迟音频流），通过预处理实现任意长序列流式处理

Result: 在ASR和TTS实验中取得SOTA性能与延迟，支持无限长序列处理，部分指标媲美离线模型

Conclusion: DSM框架统一解决多模态流式任务，通过延迟机制平衡性能与延迟，为序列转换任务提供灵活高效的解决方案

Abstract: We introduce Delayed Streams Modeling (DSM), a flexible formulation for
streaming, multimodal sequence-to-sequence learning. Sequence-to-sequence
generation is often cast in an offline manner, where the model consumes the
complete input sequence before generating the first output timestep.
Alternatively, streaming sequence-to-sequence rely on learning a policy for
choosing when to advance on the input stream, or write to the output stream.
DSM instead models already time-aligned streams with a decoder-only language
model. By moving the alignment to a pre-processing step,and introducing
appropriate delays between streams, DSM provides streaming inference of
arbitrary output sequences, from any input combination, making it applicable to
many sequence-to-sequence problems. In particular, given text and audio
streams, automatic speech recognition (ASR) corresponds to the text stream
being delayed, while the opposite gives a text-to-speech (TTS) model. We
perform extensive experiments for these two major sequence-to-sequence tasks,
showing that DSM provides state-of-the-art performance and latency while
supporting arbitrary long sequences, being even competitive with offline
baselines. Code, samples and demos are available at
https://github.com/kyutai-labs/delayed-streams-modeling

</details>


### [28] [Do All Autoregressive Transformers Remember Facts the Same Way? A Cross-Architecture Analysis of Recall Mechanisms](https://arxiv.org/abs/2509.08778)
*Minyeong Choe,Haehyun Cho,Changho Seo,Hyunil Kim*

Main category: cs.CL

TL;DR: 研究发现不同自回归Transformer架构（GPT/LLaMA/Qwen/DeepSeek）的事实召回机制存在本质差异，其中Qwen模型的早期注意力模块贡献度显著高于MLP模块。


<details>
  <summary>Details</summary>
Motivation: 探究Transformer语言模型存储和检索事实关联的机制差异，验证先前在GPT模型上发现的MLP主导模式是否适用于其他自回归架构。

Method: 通过跨模型对比实验（GPT/LLaMA/Qwen/DeepSeek），分析各层模块（MLP/attention）对事实召回的功能贡献度。

Result: 发现Qwen架构中最早层的注意力模块对事实召回贡献度超越MLP模块，与GPT类模型形成鲜明对比。

Conclusion: 自回归Transformer家族内部架构差异会导致事实召回机制的根本性不同，这对模型可解释性和定向编辑策略具有重要启示。

Abstract: Understanding how Transformer-based language models store and retrieve
factual associations is critical for improving interpretability and enabling
targeted model editing. Prior work, primarily on GPT-style models, has
identified MLP modules in early layers as key contributors to factual recall.
However, it remains unclear whether these findings generalize across different
autoregressive architectures. To address this, we conduct a comprehensive
evaluation of factual recall across several models -- including GPT, LLaMA,
Qwen, and DeepSeek -- analyzing where and how factual information is encoded
and accessed. Consequently, we find that Qwen-based models behave differently
from previous patterns: attention modules in the earliest layers contribute
more to factual recall than MLP modules. Our findings suggest that even within
the autoregressive Transformer family, architectural variations can lead to
fundamentally different mechanisms of factual recall.

</details>


### [29] [Evaluating LLMs Without Oracle Feedback: Agentic Annotation Evaluation Through Unsupervised Consistency Signals](https://arxiv.org/abs/2509.08809)
*Cheng Chen,Haiyan Yin,Ivor Tsang*

Main category: cs.CL

TL;DR: 提出CAI Ratio作为无监督评估指标，通过学生模型与LLM协作提升动态环境下标注质量评估


<details>
  <summary>Details</summary>
Motivation: LLM在动态无监督环境中的标注质量评估缺乏有效反馈机制，传统方法无法有效解决标注可靠性评估问题

Method: 1. 构建学生模型作为无监督反馈机制
2. 采用用户偏好多数投票策略评估LLM输出一致性
3. 提出CAI Ratio量化标注可靠性

Result: 在4个LLM和10个NLP数据集验证中，CAI Ratio与模型准确率呈现强正相关（r=0.86）

Conclusion: CAI Ratio作为无监督评估指标，有效解决动态环境下LLM模型选择难题，显著提升实际应用中的可靠性评估效率

Abstract: Large Language Models (LLMs), when paired with prompt-based tasks, have
significantly reduced data annotation costs and reliance on human annotators.
However, evaluating the quality of their annotations remains challenging in
dynamic, unsupervised environments where oracle feedback is scarce and
conventional methods fail. To address this challenge, we propose a novel
agentic annotation paradigm, where a student model collaborates with a noisy
teacher (the LLM) to assess and refine annotation quality without relying on
oracle feedback. The student model, acting as an unsupervised feedback
mechanism, employs a user preference-based majority voting strategy to evaluate
the consistency of the LLM outputs. To systematically measure the reliability
of LLM-generated annotations, we introduce the Consistent and Inconsistent
(CAI) Ratio, a novel unsupervised evaluation metric. The CAI Ratio not only
quantifies the annotation quality of the noisy teacher under limited user
preferences but also plays a critical role in model selection, enabling the
identification of robust LLMs in dynamic, unsupervised environments. Applied to
ten open-domain NLP datasets across four LLMs, the CAI Ratio demonstrates a
strong positive correlation with LLM accuracy, establishing it as an essential
tool for unsupervised evaluation and model selection in real-world settings.

</details>


### [30] [MoVoC: Morphology-Aware Subword Construction for Geez Script Languages](https://arxiv.org/abs/2509.08812)
*Hailay Kidu Teklehaymanot,Dren Fazlija,Wolfgang Nejdl*

Main category: cs.CL

TL;DR: 提出MoVoC-Tok分词器，通过结合形态分析和BPE分词提升吉兹字母语言的形态完整性，发布四个语言的形态标注数据集


<details>
  <summary>Details</summary>
Motivation: 解决子词分词在低资源形态复杂语言（吉兹字母语言）中形态边界保留不足的问题

Method: 1. 监督形态分析+BPE混合切分 2. 构建人工标注的形态数据集 3. 设计MorphoScore等评估指标

Result: 翻译质量无显著提升，但形态完整性指标（MorphoScore/Boundary Precision）持续改善，token效率提高

Conclusion: 形态感知分词增强语言保真度，发布的数据集/tokenizer为低资源形态丰富语言研究提供基础支持

Abstract: Subword-based tokenization methods often fail to preserve morphological
boundaries, a limitation especially pronounced in low-resource, morphologically
complex languages such as those written in the Geez script. To address this, we
present MoVoC (Morpheme-aware Subword Vocabulary Construction) and train
MoVoC-Tok, a tokenizer that integrates supervised morphological analysis into
the subword vocabulary. This hybrid segmentation approach combines
morpheme-based and Byte Pair Encoding (BPE) tokens to preserve morphological
integrity while maintaining lexical meaning. To tackle resource scarcity, we
curate and release manually annotated morpheme data for four Geez script
languages and a morpheme-aware vocabulary for two of them. While the proposed
tokenization method does not lead to significant gains in automatic translation
quality, we observe consistent improvements in intrinsic metrics, MorphoScore,
and Boundary Precision, highlighting the value of morphology-aware segmentation
in enhancing linguistic fidelity and token efficiency. Our morpheme-annotated
datasets and tokenizer will be publicly available to support further research
in low-resource, morphologically rich languages. Our code and data are
available on GitHub: https://github.com/hailaykidu/MoVoC

</details>


### [31] [Building High-Quality Datasets for Portuguese LLMs: From Common Crawl Snapshots to Industrial-Grade Corpora](https://arxiv.org/abs/2509.08824)
*Thales Sales Almeida,Rodrigo Nogueira,Helio Pedrini*

Main category: cs.CL

TL;DR: 研究通过构建葡萄牙语语料库揭示语言特定数据对LLM性能的影响，提出可扩展的多语言语料构建方法


<details>
  <summary>Details</summary>
Motivation: 现有研究集中于英语，缺乏对其他语言LLM训练数据构建的系统研究。本文旨在探索构建非英语语料库的有效方法及其对模型迁移的影响

Method: 1. 采用持续预训练框架
2. 开发语言特定过滤管道（教育/STEM/毒性内容分类器）
3. 构建120B葡萄牙语语料库
4. 对比不同数据处理策略对英葡语言迁移的影响

Result: 1. 葡萄牙语语料库达到工业级标准
2. 语言适配使模型性能提升
3. STEM分类器提升数据质量
4. 毒性过滤有效控制内容安全

Conclusion: 高质量语言特定数据是LLM性能关键，本文方法论适用于多语言开发，为资源较少语言的模型构建提供参考框架

Abstract: The performance of large language models (LLMs) is deeply influenced by the
quality and composition of their training data. While much of the existing work
has centered on English, there remains a gap in understanding how to construct
effective training corpora for other languages. We explore scalable methods for
building web-based corpora for LLMs. We apply them to build a new 120B token
corpus in Portuguese that achieves competitive results to an industrial-grade
corpus. Using a continual pretraining setup, we study how different data
selection and preprocessing strategies affect LLM performance when
transitioning a model originally trained in English to another language. Our
findings demonstrate the value of language-specific filtering pipelines,
including classifiers for education, science, technology, engineering, and
mathematics (STEM), as well as toxic content. We show that adapting a model to
the target language leads to performance improvements, reinforcing the
importance of high-quality, language-specific data. While our case study
focuses on Portuguese, our methods are applicable to other languages, offering
insights for multilingual LLM development.

</details>


### [32] [Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation](https://arxiv.org/abs/2509.08825)
*Joachim Baumann,Paul Röttger,Aleksandra Urman,Albert Wendsjö,Flor Miriam Plaza-del-Arco,Johannes B. Gruber,Dirk Hovy*

Main category: cs.CL

TL;DR: 大语言模型在社会科学研究中存在系统性偏差风险（LLM hacking），约1/3假设会被先进模型误判，小型模型误判率更高


<details>
  <summary>Details</summary>
Motivation: 研究量化LLM在数据标注任务中因研究者选择不同实现方案（模型选择/提示策略/温度设置）导致的系统性偏差风险

Method: 通过复制37个数据标注任务（来自21项研究）并使用18个不同模型生成1300万标签，测试2361个现实假设验证研究者选择对统计结论的影响

Result: 先进模型约33%假设产生错误结论，小模型达50%；模型性能和任务准确率可降低风险但无法消除；常见修正技术效果有限，人工标注对减少误报最有效

Conclusion: LLM标注需谨慎处理显著性边界发现，强调人工验证的重要性；模型能力提升可降低风险但无法根治，故意操纵LLM结果异常容易

Abstract: Large language models (LLMs) are rapidly transforming social science research
by enabling the automation of labor-intensive tasks like data annotation and
text analysis. However, LLM outputs vary significantly depending on the
implementation choices made by researchers (e.g., model selection, prompting
strategy, or temperature settings). Such variation can introduce systematic
biases and random errors, which propagate to downstream analyses and cause Type
I, Type II, Type S, or Type M errors. We call this LLM hacking.
  We quantify the risk of LLM hacking by replicating 37 data annotation tasks
from 21 published social science research studies with 18 different models.
Analyzing 13 million LLM labels, we test 2,361 realistic hypotheses to measure
how plausible researcher choices affect statistical conclusions. We find
incorrect conclusions based on LLM-annotated data in approximately one in three
hypotheses for state-of-the-art models, and in half the hypotheses for small
language models. While our findings show that higher task performance and
better general model capabilities reduce LLM hacking risk, even highly accurate
models do not completely eliminate it. The risk of LLM hacking decreases as
effect sizes increase, indicating the need for more rigorous verification of
findings near significance thresholds. Our extensive analysis of LLM hacking
mitigation techniques emphasizes the importance of human annotations in
reducing false positive findings and improving model selection. Surprisingly,
common regression estimator correction techniques are largely ineffective in
reducing LLM hacking risk, as they heavily trade off Type I vs. Type II errors.
  Beyond accidental errors, we find that intentional LLM hacking is
unacceptably simple. With few LLMs and just a handful of prompt paraphrases,
anything can be presented as statistically significant.

</details>


### [33] [A Survey of Reinforcement Learning for Large Reasoning Models](https://arxiv.org/abs/2509.08827)
*Kaiyan Zhang,Yuxin Zuo,Bingxiang He,Youbang Sun,Runze Liu,Che Jiang,Yuchen Fan,Kai Tian,Guoli Jia,Pengfei Li,Yu Fu,Xingtai Lv,Yuchen Zhang,Sihang Zeng,Shang Qu,Haozhan Li,Shijie Wang,Yuru Wang,Xinwei Long,Fangfu Liu,Xiang Xu,Jiaze Ma,Xuekai Zhu,Ermo Hua,Yihao Liu,Zonglin Li,Huayu Chen,Xiaoye Qu,Yafu Li,Weize Chen,Zhenzhao Yuan,Junqi Gao,Dong Li,Zhiyuan Ma,Ganqu Cui,Zhiyuan Liu,Biqing Qi,Ning Ding,Bowen Zhou*

Main category: cs.CL

TL;DR: 论文系统综述强化学习在增强大语言模型逻辑推理能力中的进展，分析当前技术瓶颈并提出面向人工超级智能的扩展路径


<details>
  <summary>Details</summary>
Motivation: 针对强化学习在构建逻辑推理模型(LRMs)过程中面临的算法设计、训练数据、基础设施等系统性挑战，探索突破现有技术瓶颈的方法论路径

Method: 通过文献计量分析与技术框架拆解，系统梳理从DeepSeek-R1发布以来的RL-LRM技术体系，涵盖核心组件、关键问题、训练资源及应用场景

Result: 揭示强化学习在复杂逻辑任务中的技术优势与扩展限制，提出基于计算-算法-数据协同优化的可持续扩展方案

Conclusion: 建立强化学习驱动LRMs发展的技术路线图，为构建面向人工超级智能的通用推理系统提供方法论指导

Abstract: In this paper, we survey recent advances in Reinforcement Learning (RL) for
reasoning with Large Language Models (LLMs). RL has achieved remarkable success
in advancing the frontier of LLM capabilities, particularly in addressing
complex logical tasks such as mathematics and coding. As a result, RL has
emerged as a foundational methodology for transforming LLMs into LRMs. With the
rapid progress of the field, further scaling of RL for LRMs now faces
foundational challenges not only in computational resources but also in
algorithm design, training data, and infrastructure. To this end, it is timely
to revisit the development of this domain, reassess its trajectory, and explore
strategies to enhance the scalability of RL toward Artificial SuperIntelligence
(ASI). In particular, we examine research applying RL to LLMs and LRMs for
reasoning abilities, especially since the release of DeepSeek-R1, including
foundational components, core problems, training resources, and downstream
applications, to identify future opportunities and directions for this rapidly
evolving area. We hope this review will promote future research on RL for
broader reasoning models. Github:
https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [34] [X-Part: high fidelity and structure coherent shape decomposition](https://arxiv.org/abs/2509.08643)
*Xinhao Yan,Jiachen Xu,Yang Li,Changfeng Ma,Yunhan Yang,Chunshi Wang,Zibo Zhao,Zeqiang Lai,Yunfei Zhao,Zhuo Chen,Chunchao Guo*

Main category: cs.GR

TL;DR: 提出X-Part模型，通过边界框提示和语义特征注入实现高可控、语义清晰的3D部件生成


<details>
  <summary>Details</summary>
Motivation: 现有部件生成方法存在可控性不足和语义分解质量低的问题

Method: 1. 利用边界框作为部件生成提示
2. 注入点级语义特征实现有意义分解
3. 设计可编辑交互生成流程

Result: 在部件级形状生成任务中达到SOTA性能，建立生产级可编辑3D资产新范式

Conclusion: X-Part为创建生产就绪、可编辑且结构合理的3D资产提供了创新解决方案，代码将开源

Abstract: Generating 3D shapes at part level is pivotal for downstream applications
such as mesh retopology, UV mapping, and 3D printing. However, existing
part-based generation methods often lack sufficient controllability and suffer
from poor semantically meaningful decomposition. To this end, we introduce
X-Part, a controllable generative model designed to decompose a holistic 3D
object into semantically meaningful and structurally coherent parts with high
geometric fidelity. X-Part exploits the bounding box as prompts for the part
generation and injects point-wise semantic features for meaningful
decomposition. Furthermore, we design an editable pipeline for interactive part
generation. Extensive experimental results show that X-Part achieves
state-of-the-art performance in part-level shape generation. This work
establishes a new paradigm for creating production-ready, editable, and
structurally sound 3D assets. Codes will be released for public research.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [35] [Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles](https://arxiv.org/abs/2509.08777)
*Eric Slyman,Mehrab Tanjim,Kushal Kafle,Stefan Lee*

Main category: cs.CV

TL;DR: 提出多模态贝叶斯提示集成方法MMB，通过视觉特征动态分配权重，显著提升文本到图像生成系统评估的准确性和校准性


<details>
  <summary>Details</summary>
Motivation: 现有MLLM评估工具存在偏见、过度自信及跨域不一致问题，传统提示集成方法在TTI任务中效果不足

Method: MMB方法结合贝叶斯提示集成与图像聚类技术，根据样本视觉特征动态分配提示权重

Result: 在HPSv2和MJBench基准测试中，MMB在人类标注对齐度和校准度上优于基线，跨域性能提升显著

Conclusion: 多模态特定策略对评估校准至关重要，MMB为可靠的大规模TTI评估提供了新方向

Abstract: Multimodal large language models (MLLMs) are increasingly used to evaluate
text-to-image (TTI) generation systems, providing automated judgments based on
visual and textual context. However, these "judge" models often suffer from
biases, overconfidence, and inconsistent performance across diverse image
domains. While prompt ensembling has shown promise for mitigating these issues
in unimodal, text-only settings, our experiments reveal that standard
ensembling methods fail to generalize effectively for TTI tasks. To address
these limitations, we propose a new multimodal-aware method called Multimodal
Mixture-of-Bayesian Prompt Ensembles (MMB). Our method uses a Bayesian prompt
ensemble approach augmented by image clustering, allowing the judge to
dynamically assign prompt weights based on the visual characteristics of each
sample. We show that MMB improves accuracy in pairwise preference judgments and
greatly enhances calibration, making it easier to gauge the judge's true
uncertainty. In evaluations on two TTI benchmarks, HPSv2 and MJBench, MMB
outperforms existing baselines in alignment with human annotations and
calibration across varied image content. Our findings highlight the importance
of multimodal-specific strategies for judge calibration and suggest a promising
path forward for reliable large-scale TTI evaluation.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [36] [Measuring and mitigating overreliance is necessary for building human-compatible AI](https://arxiv.org/abs/2509.08010)
*Lujain Ibrahim,Katherine M. Collins,Sunnie S. Y. Kim,Anka Reuel,Max Lamparth,Kevin Feng,Lama Ahmad,Prajna Soni,Alia El Kattan,Merlin Stein,Siddharth Swaroop,Ilia Sucholutsky,Andrew Strait,Q. Vera Liao,Umang Bhatt*

Main category: cs.CY

TL;DR: 本文探讨大型语言模型作为协作伙伴时引发的过度依赖风险，提出测量和缓解该风险应成为LLM研究的核心任务。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在医疗、个人咨询等关键决策中的渗透，其能力边界外的过度使用可能导致高风险错误、治理困境和人类认知能力退化。

Method: 通过整合个人/社会层面风险，分析LLM特性与用户认知偏见的交互作用，评估历史测量方法并提出三方面改进方向。

Result: 揭示LLM系统设计与人类认知偏差共同加剧实践风险，建立包含认知科学指标的测量框架，提出增强人机互补性的技术方案。

Conclusion: 强调LLM研究需优先开发抑制过度依赖的机制，确保技术真正增强而非削弱人类决策能力。

Abstract: Large language models (LLMs) distinguish themselves from previous
technologies by functioning as collaborative "thought partners," capable of
engaging more fluidly in natural language. As LLMs increasingly influence
consequential decisions across diverse domains from healthcare to personal
advice, the risk of overreliance - relying on LLMs beyond their capabilities -
grows. This position paper argues that measuring and mitigating overreliance
must become central to LLM research and deployment. First, we consolidate risks
from overreliance at both the individual and societal levels, including
high-stakes errors, governance challenges, and cognitive deskilling. Then, we
explore LLM characteristics, system design features, and user cognitive biases
that - together - raise serious and unique concerns about overreliance in
practice. We also examine historical approaches for measuring overreliance,
identifying three important gaps and proposing three promising directions to
improve measurement. Finally, we propose mitigation strategies that the AI
research community can pursue to ensure LLMs augment rather than undermine
human capabilities.

</details>


### [37] [HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants](https://arxiv.org/abs/2509.08494)
*Benjamin Sturgeon,Daniel Samuelson,Jacob Haimes,Jacy Reese Anthis*

Main category: cs.CY

TL;DR: 研究开发HumanAgencyBench基准测试，发现当代AI助手对人类代理支持不足且存在维度差异，建议加强安全目标


<details>
  <summary>Details</summary>
Motivation: AI技术普及导致人类失去决策控制权，现有算法系统（如社交媒体推送）已影响人类自主决策能力

Method: 整合哲学/科学代理理论，使用LLM模拟用户查询，构建含6个代理维度的HAB评估框架（澄清问题/避免价值操纵/纠正错误信息/延迟重大决策/鼓励学习/保持社交边界）

Result: 主流LLM助手仅低-中度支持代理，Anthropic整体最佳但避免价值操纵最差，代理支持与模型能力/指令遵循无必然关联

Conclusion: 应超越现有RLHF范式，建立更全面的安全评估体系，在AI开发中优先保障人类代理权

Abstract: As humans delegate more tasks and decisions to artificial intelligence (AI),
we risk losing control of our individual and collective futures. Relatively
simple algorithmic systems already steer human decision-making, such as social
media feed algorithms that lead people to unintentionally and absent-mindedly
scroll through engagement-optimized content. In this paper, we develop the idea
of human agency by integrating philosophical and scientific theories of agency
with AI-assisted evaluation methods: using large language models (LLMs) to
simulate and validate user queries and to evaluate AI responses. We develop
HumanAgencyBench (HAB), a scalable and adaptive benchmark with six dimensions
of human agency based on typical AI use cases. HAB measures the tendency of an
AI assistant or agent to Ask Clarifying Questions, Avoid Value Manipulation,
Correct Misinformation, Defer Important Decisions, Encourage Learning, and
Maintain Social Boundaries. We find low-to-moderate agency support in
contemporary LLM-based assistants and substantial variation across system
developers and dimensions. For example, while Anthropic LLMs most support human
agency overall, they are the least supportive LLMs in terms of Avoid Value
Manipulation. Agency support does not appear to consistently result from
increasing LLM capabilities or instruction-following behavior (e.g., RLHF), and
we encourage a shift towards more robust safety and alignment targets.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [38] [EvolKV: Evolutionary KV Cache Compression for LLM Inference](https://arxiv.org/abs/2509.08315)
*Bohan Yu,Yekun Chai*

Main category: cs.LG

TL;DR: 提出自适应框架EvolKV，通过进化搜索动态优化KV缓存分层分配，在极低预算下实现超越完整缓存性能


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存压缩方法采用均匀分配/静态淘汰策略，忽视了层级特征交互与任务性能的关联，导致泛化能力下降

Method: 将缓存分配建模为多目标优化问题，利用进化搜索动态配置各层缓存预算，直接优化下游任务性能

Result: 在11个任务中全面超越基线，长上下文任务表现优异，GSM8K任务提升7%，代码补全任务仅用1.5%预算超越完整缓存

Conclusion: EvolKV揭示了学习型压缩策略在KV缓存分配中的巨大潜力，为高效大模型推理开辟新方向

Abstract: Existing key-value (KV) cache compression methods typically rely on
heuristics, such as uniform cache allocation across layers or static eviction
policies, however, they ignore the critical interplays among layer-specific
feature patterns and task performance, which can lead to degraded
generalization. In this paper, we propose EvolKV, an adaptive framework for
layer-wise, task-driven KV cache compression that jointly optimizes the memory
efficiency and task performance. By reformulating cache allocation as a
multi-objective optimization problem, EvolKV leverages evolutionary search to
dynamically configure layer budgets while directly maximizing downstream
performance. Extensive experiments on 11 tasks demonstrate that our approach
outperforms all baseline methods across a wide range of KV cache budgets on
long-context tasks and surpasses heuristic baselines by up to 7 percentage
points on GSM8K. Notably, EvolKV achieves superior performance over the full KV
cache setting on code completion while utilizing only 1.5% of the original
budget, suggesting the untapped potential in learned compression strategies for
KV cache budget allocation.

</details>


### [39] [Generative Data Refinement: Just Ask for Better Data](https://arxiv.org/abs/2509.08653)
*Minqi Jiang,João G. M. Araújo,Will Ellsworth,Sian Gooding,Edward Grefenstette*

Main category: cs.LG

TL;DR: 提出生成数据精炼（GDR）框架，通过预训练生成模型将不良内容数据集转化为适合训练的精炼数据，解决数据枯竭问题并保持数据多样性。


<details>
  <summary>Details</summary>
Motivation: 现有训练数据增长快于网络索引速度，且未索引的用户数据存在隐私泄露和不良内容风险，需开发安全高效的数据处理方法。

Method: 利用预训练生成模型对含不良内容的数据进行转换，生成条件依赖真实数据的合成数据，避免传统多样性生成的复杂性。

Result: GDR在匿名化和去毒化任务中优于行业方案，且生成数据自然匹配网络规模数据集的多样性。

Conclusion: GDR为前沿模型提供简单有效的训练数据扩展方案，通过数据精炼提升模型训练数据储备。

Abstract: For a fixed parameter size, the capabilities of large models are primarily
determined by the quality and quantity of its training data. Consequently,
training datasets now grow faster than the rate at which new data is indexed on
the web, leading to projected data exhaustion over the next decade. Much more
data exists as user-generated content that is not publicly indexed, but
incorporating such data comes with considerable risks, such as leaking private
information and other undesirable content. We introduce a framework, Generative
Data Refinement (GDR), for using pretrained generative models to transform a
dataset with undesirable content into a refined dataset that is more suitable
for training. Our experiments show that GDR can outperform industry-grade
solutions for dataset anonymization, as well as enable direct detoxification of
highly unsafe datasets. Moreover, we show that by generating synthetic data
that is conditioned on each example in the real dataset, GDR's refined outputs
naturally match the diversity of web scale datasets, and thereby avoid the
often challenging task of generating diverse synthetic data via model
prompting. The simplicity and effectiveness of GDR make it a powerful tool for
scaling up the total stock of training data for frontier models.

</details>


### [40] [AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2509.08755)
*Zhiheng Xi,Jixuan Huang,Chenyang Liao,Baodai Huang,Honglin Guo,Jiaqi Liu,Rui Zheng,Junjie Ye,Jiazheng Zhang,Wenxiang Chen,Wei He,Yiwen Ding,Guanyu Li,Zehui Chen,Zhengyin Du,Xuesong Yao,Yufei Xu,Jiecao Chen,Tao Gui,Zuxuan Wu,Qi Zhang,Xuanjing Huang,Yu-Gang Jiang*

Main category: cs.LG

TL;DR: 提出AgentGym-RL强化学习框架和ScalingInter-RL训练方法，通过模块化架构和分阶段训练策略实现稳定高效的LLM代理训练，在27个任务中超越商业模型


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏统一的交互式强化学习框架，无法在多样化现实环境中从头训练LLM代理且过度依赖监督微调

Method: 构建模块化RL框架支持主流算法，提出分阶段训练策略ScalingInter-RL：前期限制交互次数侧重开发核心能力，后期扩大探索范围鼓励多样化策略

Result: 在多样化环境中验证框架稳定性，代理在Web/数据库/知识图谱等场景的27项任务表现优于GPT-4等商业模型

Conclusion: 通过系统化的RL训练框架和渐进式训练方法，成功开发出具备复杂决策能力的LLM代理，并承诺开源完整代码库推动社区发展

Abstract: Developing autonomous LLM agents capable of making a series of intelligent
decisions to solve complex, real-world tasks is a fast-evolving frontier. Like
human cognitive development, agents are expected to acquire knowledge and
skills through exploration and interaction with the environment. Despite
advances, the community still lacks a unified, interactive reinforcement
learning (RL) framework that can effectively train such agents from scratch --
without relying on supervised fine-tuning (SFT) -- across diverse and realistic
environments. To bridge this gap, we introduce AgentGym-RL, a new framework to
train LLM agents for multi-turn interactive decision-making through RL. The
framework features a modular and decoupled architecture, ensuring high
flexibility and extensibility. It encompasses a wide variety of real-world
scenarios, and supports mainstream RL algorithms. Furthermore, we propose
ScalingInter-RL, a training approach designed for exploration-exploitation
balance and stable RL optimization. In early stages, it emphasizes exploitation
by restricting the number of interactions, and gradually shifts towards
exploration with larger horizons to encourage diverse problem-solving
strategies. In this way, the agent develops more diverse behaviors and is less
prone to collapse under long horizons. We perform extensive experiments to
validate the stability and effectiveness of both the AgentGym-RL framework and
the ScalingInter-RL approach. Our agents match or surpass commercial models on
27 tasks across diverse environments. We offer key insights and will
open-source the complete AgentGym-RL framework -- including code and datasets
-- to empower the research community in developing the next generation of
intelligent agents.

</details>


### [41] [Merge-of-Thought Distillation](https://arxiv.org/abs/2509.08814)
*Zhanming Shen,Zeyu Qin,Zenan Huang,Hao Chen,Jiaqi Hu,Yihong Zhuang,Guoshan Lu,Gang Chen,Junbo Zhao*

Main category: cs.LG

TL;DR: 提出Merge-of-Thought蒸馏框架(MoT)，通过多教师权重空间融合策略，仅用200个CoT样本即实现学生模型推理能力跃升，超越更大参数规模模型。


<details>
  <summary>Details</summary>
Motivation: 传统单教师蒸馏方法无法有效利用多教师资源，且不同学生的最佳教师存在差异。需突破单教师假设，解决多教师监督冲突问题。

Method: 1. 交替进行教师特定的监督微调分支训练
2. 对多个学生变体进行权重空间融合
3. 共识过滤机制提升特征迁移能力

Result: Qwen3-14B经MoT训练后在数学竞赛基准超越DeepSeek-R1、Qwen3-30B等更大模型，且在分布偏移/同级教师场景展现鲁棒性，推理遗忘率降低40%。

Conclusion: MoT为紧凑学生模型的高效知识蒸馏提供新路径，证明多教师共识特征具有跨领域迁移潜力，显著提升模型泛化推理能力。

Abstract: Efficient reasoning distillation for long chain-of-thought (CoT) models is
increasingly constrained by the assumption of a single oracle teacher, despite
practical availability of multiple candidate teachers and growing CoT corpora.
We revisit teacher selection and observe that different students have different
"best teachers," and even for the same student the best teacher can vary across
datasets. Therefore, to unify multiple teachers' reasoning abilities into
student with overcoming conflicts among various teachers' supervision, we
propose Merge-of-Thought Distillation (MoT), a lightweight framework that
alternates between teacher-specific supervised fine-tuning branches and
weight-space merging of the resulting student variants. On competition math
benchmarks, using only about 200 high-quality CoT samples, applying MoT to a
Qwen3-14B student surpasses strong models including DEEPSEEK-R1, QWEN3-30B-A3B,
QWEN3-32B, and OPENAI-O1, demonstrating substantial gains. Besides, MoT
consistently outperforms the best single-teacher distillation and the naive
multi-teacher union, raises the performance ceiling while mitigating
overfitting, and shows robustness to distribution-shifted and peer-level
teachers. Moreover, MoT reduces catastrophic forgetting, improves general
reasoning beyond mathematics and even cultivates a better teacher, indicating
that consensus-filtered reasoning features transfer broadly. These results
position MoT as a simple, scalable route to efficiently distilling long CoT
capabilities from diverse teachers into compact students.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [42] [XML Prompting as Grammar-Constrained Interaction: Fixed-Point Semantics, Convergence Guarantees, and Human-AI Protocols](https://arxiv.org/abs/2509.08182)
*Faruk Alpay,Taylan Alpay*

Main category: cs.PL

TL;DR: 提出基于XML的结构化提示方法，通过语法约束解码和人类-AI交互协议实现可靠输出，建立数学框架统一文法对齐、验证链等技术


<details>
  <summary>Details</summary>
Motivation: 解决大模型在现实系统中输出格式不可靠的问题，通过结构化提示实现可解析且符合模式(schema)的稳定输出

Method: 构建XML树格的数学框架，结合(i)语法约束解码(ii)格上的不动点语义(iii)契约式人机交互循环，通过上下文无关文法实现结构化输出保障

Result: 证明XML树格上单调算子的不动点存在性(Banach/Tarski定理)，实现多层级人机协作模式(规划-验证-修订)和工具调用协议，保持任务性能的同时确保格式正确性

Conclusion: 建立数学完备的提示工程框架，将语法约束解码、验证链等技术统一，为结构化人机交互提供理论支撑和部署模式

Abstract: Structured prompting with XML tags has emerged as an effective way to steer
large language models (LLMs) toward parseable, schema-adherent outputs in
real-world systems. We develop a logic-first treatment of XML prompting that
unifies (i) grammar-constrained decoding, (ii) fixed-point semantics over
lattices of hierarchical prompts, and (iii) convergent human-AI interaction
loops. We formalize a complete lattice of XML trees under a refinement order
and prove that monotone prompt-to-prompt operators admit least fixed points
(Knaster-Tarski) that characterize steady-state protocols; under a task-aware
contraction metric on trees, we further prove Banach-style convergence of
iterative guidance. We instantiate these results with context-free grammars
(CFGs) for XML schemas and show how constrained decoding guarantees
well-formedness while preserving task performance. A set of multi-layer
human-AI interaction recipes demonstrates practical deployment patterns,
including multi-pass "plan $\to$ verify $\to$ revise" routines and agentic tool
use. We provide mathematically complete proofs and tie our framework to recent
advances in grammar-aligned decoding, chain-of-verification, and programmatic
prompting.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [43] [Scaling Truth: The Confidence Paradox in AI Fact-Checking](https://arxiv.org/abs/2509.08803)
*Ihsan A. Qazi,Zohaib Khan,Abdullah Ghani,Agha A. Raza,Zafar A. Qazi,Wassay Sajjad,Ayesha Ali,Asher Javaid,Muhammad Abdullah Sohail,Abdul H. Azeemi*

Main category: cs.SI

TL;DR: 论文系统评估了9种大语言模型在全球多语种事实核查中的表现，发现模型性能与自信度呈现类似邓宁-克鲁格效应的倒置关系，可能加剧信息不平等。


<details>
  <summary>Details</summary>
Motivation: 错误信息泛滥需要可扩展的事实核查方案，但LLMs在全球语境下的有效性尚未明确，需验证其跨语言/地域的公平性。

Method: 使用5000个经专业机构验证的多语言声明，测试9种不同架构/规模的LLMs在训练后声明上的泛化能力，结合4种提示策略和24万人工标注数据。

Result: 小模型高自信低准确（如闭源模型74%准确率），大模型高准确低自信（如GPT-4达83%准确率）；非英语和全球南方声明存在显著表现差距。

Conclusion: 研究揭示了AI事实核查中的系统性偏见风险，呼吁建立多语言基准并制定政策确保技术公平性。

Abstract: The rise of misinformation underscores the need for scalable and reliable
fact-checking solutions. Large language models (LLMs) hold promise in
automating fact verification, yet their effectiveness across global contexts
remains uncertain. We systematically evaluate nine established LLMs across
multiple categories (open/closed-source, multiple sizes, diverse architectures,
reasoning-based) using 5,000 claims previously assessed by 174 professional
fact-checking organizations across 47 languages. Our methodology tests model
generalizability on claims postdating training cutoffs and four prompting
strategies mirroring both citizen and professional fact-checker interactions,
with over 240,000 human annotations as ground truth. Findings reveal a
concerning pattern resembling the Dunning-Kruger effect: smaller, accessible
models show high confidence despite lower accuracy, while larger models
demonstrate higher accuracy but lower confidence. This risks systemic bias in
information verification, as resource-constrained organizations typically use
smaller models. Performance gaps are most pronounced for non-English languages
and claims originating from the Global South, threatening to widen existing
information inequalities. These results establish a multilingual benchmark for
future research and provide an evidence base for policy aimed at ensuring
equitable access to trustworthy, AI-assisted fact-checking.

</details>
