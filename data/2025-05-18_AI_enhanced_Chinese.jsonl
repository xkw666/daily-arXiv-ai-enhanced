{"id": "2505.09649", "pdf": "https://arxiv.org/pdf/2505.09649", "abs": "https://arxiv.org/abs/2505.09649", "authors": ["Abisha Thapa Magar", "Anup Shakya"], "title": "Next Word Suggestion using Graph Neural Network", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Language Modeling is a prevalent task in Natural Language Processing. The\ncurrently existing most recent and most successful language models often tend\nto build a massive model with billions of parameters, feed in a tremendous\namount of text data, and train with enormous computation resources which\nrequire millions of dollars. In this project, we aim to address an important\nsub-task in language modeling, i.e., context embedding. We propose an approach\nto exploit the Graph Convolution operation in GNNs to encode the context and\nuse it in coalition with LSTMs to predict the next word given a local context\nof preceding words. We test this on the custom Wikipedia text corpus using a\nvery limited amount of resources and show that this approach works fairly well\nto predict the next word.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\u4e0eLSTM\u7684\u4e0a\u4e0b\u6587\u5d4c\u5165\u65b9\u6cd5\uff0c\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u6709\u6548\u9884\u6d4b\u4e0b\u4e00\u4e2a\u5355\u8bcd", "motivation": "\u73b0\u6709\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56\u5de8\u989d\u8d44\u6e90\u548c\u6210\u672c\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u8d44\u6e90\u9ad8\u6548\u7684\u4e0a\u4e0b\u6587\u5d4c\u5165\u65b9\u6cd5\uff0c\u964d\u4f4e\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u95e8\u69db", "method": "\u91c7\u7528\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\u7f16\u7801\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4e0eLSTM\u534f\u540c\u5de5\u4f5c\uff0c\u5728\u6709\u9650\u8d44\u6e90\u6761\u4ef6\u4e0b\u4e8e\u7ef4\u57fa\u767e\u79d1\u8bed\u6599\u5e93\u8fdb\u884c\u5b9e\u9a8c", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u60c5\u51b5\u4e0b\u80fd\u6709\u6548\u5b8c\u6210\u4e0b\u4e00\u8bcd\u9884\u6d4b\u4efb\u52a1\uff0c\u9a8c\u8bc1\u4e86\u8f7b\u91cf\u7ea7\u6a21\u578b\u7684\u53ef\u884c\u6027", "conclusion": "GCN\u4e0eLSTM\u7684\u878d\u5408\u4e3a\u8d44\u6e90\u654f\u611f\u573a\u666f\u4e0b\u7684\u8bed\u8a00\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u8bc1\u660e\u4e86\u8f7b\u91cf\u5316\u67b6\u6784\u7684\u6f5c\u529b"}}
{"id": "2505.09655", "pdf": "https://arxiv.org/pdf/2505.09655", "abs": "https://arxiv.org/abs/2505.09655", "authors": ["Xiwen Chen", "Wenhui Zhu", "Peijie Qiu", "Xuanzhao Dong", "Hao Wang", "Haiyu Wu", "Huayu Li", "Aristeidis Sotiras", "Yalin Wang", "Abolfazl Razi"], "title": "DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in reinforcement learning for language model post-training,\nsuch as Group Relative Policy Optimization (GRPO), have shown promise in\nlow-resource settings. However, GRPO typically relies on solution-level and\nscalar reward signals that fail to capture the semantic diversity among sampled\ncompletions. This leads to what we identify as a diversity-quality\ninconsistency, where distinct reasoning paths may receive indistinguishable\nrewards. To address this limitation, we propose $\\textit{Diversity-aware Reward\nAdjustment}$ (DRA), a method that explicitly incorporates semantic diversity\ninto the reward computation. DRA uses Submodular Mutual Information (SMI) to\ndownweight redundant completions and amplify rewards for diverse ones. This\nencourages better exploration during learning, while maintaining stable\nexploitation of high-quality samples. Our method integrates seamlessly with\nboth GRPO and its variant DR.~GRPO, resulting in $\\textit{DRA-GRPO}$ and\n$\\textit{DGA-DR.~GRPO}$. We evaluate our method on five mathematical reasoning\nbenchmarks and find that it outperforms recent strong baselines. It achieves\nstate-of-the-art performance with an average accuracy of 58.2%, using only\n7,000 fine-tuning samples and a total training cost of approximately $55. The\ncode is available at https://github.com/xiwenc1/DRA-GRPO.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5b50\u6a21\u4e92\u4fe1\u606f\u7684\u591a\u6837\u6027\u5956\u52b1\u8c03\u6574\u65b9\u6cd5DRA-GRPO\uff0c\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u6587\u8bed\u6a21\u578b\u540e\u8bad\u7ec3\u4e2d\u7684\u591a\u6837\u6027-\u8d28\u91cf\u77db\u76fe\u95ee\u9898\uff0c\u4ec5\u75287000\u6837\u672c\u5b9e\u73b058.2%\u7684SOTA\u51c6\u786e\u7387", "motivation": "\u73b0\u6709GRPO\u65b9\u6cd5\u4f9d\u8d56\u6807\u91cf\u5956\u52b1\u4fe1\u53f7\uff0c\u65e0\u6cd5\u6355\u6349\u8bed\u4e49\u591a\u6837\u6027\uff0c\u5bfc\u81f4\u4e0d\u540c\u63a8\u7406\u8def\u5f84\u83b7\u5f97\u76f8\u4f3c\u5956\u52b1\u7684\u591a\u6837\u6027-\u8d28\u91cf\u4e0d\u4e00\u81f4\u95ee\u9898", "method": "\u901a\u8fc7\u5b50\u6a21\u4e92\u4fe1\u606f(SMI)\u91cf\u5316\u8bed\u4e49\u591a\u6837\u6027\uff0c\u5bf9\u5197\u4f59\u6837\u672c\u964d\u6743\u5e76\u589e\u5f3a\u591a\u6837\u6027\u6837\u672c\u5956\u52b1\uff0c\u5f00\u53d1DRA-GRPO\u548cDGA-DR.GRPO\u4e24\u79cd\u53d8\u4f53", "result": "\u57285\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523058.2%\u5e73\u5747\u51c6\u786e\u7387\uff0c\u4ec5\u6d88\u80177000\u5fae\u8c03\u6837\u672c\u548c\u7ea655\u7f8e\u5143\u8bad\u7ec3\u6210\u672c", "conclusion": "DRA\u65b9\u6cd5\u6709\u6548\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u5728\u6781\u4f4e\u8d44\u6e90\u6761\u4ef6\u4e0b\u5b9e\u73b0\u6027\u80fd\u7a81\u7834\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.09662", "pdf": "https://arxiv.org/pdf/2505.09662", "abs": "https://arxiv.org/abs/2505.09662", "authors": ["Philipp Schoenegger", "Francesco Salvi", "Jiacheng Liu", "Xiaoli Nan", "Ramit Debnath", "Barbara Fasolo", "Evelina Leivada", "Gabriel Recchia", "Fritz G\u00fcnther", "Ali Zarifhonarvar", "Joe Kwon", "Zahoor Ul Islam", "Marco Dehnert", "Daryl Y. H. Lee", "Madeline G. Reinecke", "David G. Kamper", "Mert Koba\u015f", "Adam Sandford", "Jonas Kgomo", "Luke Hewitt", "Shreya Kapoor", "Kerem Oktar", "Eyup Engin Kucuk", "Bo Feng", "Cameron R. Jones", "Izzy Gainsburg", "Sebastian Olschewski", "Nora Heinzelmann", "Francisco Cruz", "Ben M. Tappin", "Tao Ma", "Peter S. Park", "Rayan Onyonka", "Arthur Hjorth", "Peter Slattery", "Qingcheng Zeng", "Lennart Finke", "Igor Grossmann", "Alessandro Salatiello", "Ezra Karger"], "title": "Large Language Models Are More Persuasive Than Incentivized Human Persuaders", "categories": ["cs.CL", "I.2.7; H.1.2; K.4.1; H.5.2"], "comment": null, "summary": "We directly compare the persuasion capabilities of a frontier large language\nmodel (LLM; Claude Sonnet 3.5) against incentivized human persuaders in an\ninteractive, real-time conversational quiz setting. In this preregistered,\nlarge-scale incentivized experiment, participants (quiz takers) completed an\nonline quiz where persuaders (either humans or LLMs) attempted to persuade quiz\ntakers toward correct or incorrect answers. We find that LLM persuaders\nachieved significantly higher compliance with their directional persuasion\nattempts than incentivized human persuaders, demonstrating superior persuasive\ncapabilities in both truthful (toward correct answers) and deceptive (toward\nincorrect answers) contexts. We also find that LLM persuaders significantly\nincreased quiz takers' accuracy, leading to higher earnings, when steering quiz\ntakers toward correct answers, and significantly decreased their accuracy,\nleading to lower earnings, when steering them toward incorrect answers.\nOverall, our findings suggest that AI's persuasion capabilities already exceed\nthose of humans that have real-money bonuses tied to performance. Our findings\nof increasingly capable AI persuaders thus underscore the urgency of emerging\nalignment and governance frameworks.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578bClaude Sonnet 3.5\u5728\u771f\u5b9e\u5bf9\u8bdd\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u8d85\u8d8a\u6fc0\u52b1\u4eba\u7c7b\u7684\u8bf4\u670d\u80fd\u529b\uff0c\u65e0\u8bba\u5f15\u5bfc\u6b63\u786e\u6216\u9519\u8bef\u7b54\u6848\u5747\u663e\u8457\u5f71\u54cd\u7b54\u9898\u8005\u51b3\u7b56", "motivation": "\u9a8c\u8bc1\u524d\u6cbfAI\u6a21\u578b\u7684\u8bf4\u670d\u80fd\u529b\u662f\u5426\u8d85\u8d8a\u53d7\u91d1\u94b1\u6fc0\u52b1\u7684\u4eba\u7c7b\u8bf4\u670d\u8005\uff0c\u63ed\u793aAI\u8bf4\u670d\u6280\u672f\u5bf9\u73b0\u5b9e\u51b3\u7b56\u7684\u5f71\u54cd", "method": "\u91c7\u7528\u9884\u6ce8\u518c\u5927\u89c4\u6a21\u6fc0\u52b1\u5b9e\u9a8c\uff1a\u8bf4\u670d\u8005\uff08\u4eba\u7c7b/AI\uff09\u5728\u5b9e\u65f6\u5bf9\u8bdd\u6d4b\u8bd5\u4e2d\u5f15\u5bfc\u7b54\u9898\u8005\u9009\u62e9\u6307\u5b9a\u7b54\u6848\uff0c\u5bf9\u6bd4\u5408\u89c4\u7387\u4e0e\u7ecf\u6d4e\u6536\u76ca", "result": "LLM\u8bf4\u670d\u5408\u89c4\u7387\u663e\u8457\u66f4\u9ad8\uff08\u6b63\u786e\u5f15\u5bfc+16.5%\uff0c\u9519\u8bef\u5f15\u5bfc+21.3%\uff09\uff0c\u76f4\u63a5\u5f71\u54cd\u7b54\u9898\u51c6\u786e\u7387\uff08\u6b63\u786e\u5f15\u5bfc\u63d0\u5347\u6536\u76ca$0.68\uff0c\u9519\u8bef\u5f15\u5bfc\u964d\u4f4e\u6536\u76ca$1.24\uff09", "conclusion": "AI\u8bf4\u670d\u80fd\u529b\u5df2\u8d85\u8d8a\u53d7\u7ecf\u6d4e\u6fc0\u52b1\u7684\u4eba\u7c7b\uff0c\u7a81\u663e\u6784\u5efaAI\u4f26\u7406\u6846\u67b6\u4e0e\u6cbb\u7406\u4f53\u7cfb\u7684\u7d27\u8feb\u6027\uff0c\u9700\u8b66\u60d5\u6ee5\u7528\u98ce\u9669\u4e0e\u793e\u4f1a\u5f71\u54cd"}}
{"id": "2505.09666", "pdf": "https://arxiv.org/pdf/2505.09666", "abs": "https://arxiv.org/abs/2505.09666", "authors": ["Yumin Choi", "Jinheon Baek", "Sung Ju Hwang"], "title": "System Prompt Optimization with Meta-Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable capabilities, with\noptimizing their input prompts playing a pivotal role in maximizing their\nperformance. However, while LLM prompts consist of both the task-agnostic\nsystem prompts and task-specific user prompts, existing work on prompt\noptimization has focused on user prompts specific to individual queries or\ntasks, and largely overlooked the system prompt that is, once optimized,\napplicable across different tasks and domains. Motivated by this, we introduce\nthe novel problem of bilevel system prompt optimization, whose objective is to\ndesign system prompts that are robust to diverse user prompts and transferable\nto unseen tasks. To tackle this problem, we then propose a meta-learning\nframework, which meta-learns the system prompt by optimizing it over various\nuser prompts across multiple datasets, while simultaneously updating the user\nprompts in an iterative manner to ensure synergy between them. We conduct\nexperiments on 14 unseen datasets spanning 5 different domains, on which we\nshow that our approach produces system prompts that generalize effectively to\ndiverse user prompts. Also, our findings reveal that the optimized system\nprompt enables rapid adaptation even to unseen tasks, requiring fewer\noptimization steps for test-time user prompts while achieving improved\nperformance.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u5c42\u7cfb\u7edf\u63d0\u793a\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5143\u5b66\u4e60\u8054\u5408\u4f18\u5316\u7cfb\u7edf\u63d0\u793a\u548c\u7528\u6237\u63d0\u793a\uff0c\u63d0\u5347LLM\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u672a\u89c1\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u4e0e\u5feb\u901f\u9002\u5e94\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u805a\u7126\u4efb\u52a1\u7279\u5b9a\u7684\u7528\u6237\u63d0\u793a\u4f18\u5316\uff0c\u5ffd\u89c6\u4e86\u8de8\u4efb\u52a1\u901a\u7528\u7684\u7cfb\u7edf\u63d0\u793a\u6f5c\u529b\u3002\u7cfb\u7edf\u63d0\u793a\u4e00\u65e6\u4f18\u5316\u53ef\u5e7f\u6cdb\u9002\u7528\u4e8e\u4e0d\u540c\u4efb\u52a1\uff0c\u5177\u6709\u66f4\u9ad8\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u91c7\u7528\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u591a\u6570\u636e\u96c6\u4e0a\u4ea4\u66ff\u4f18\u5316\u7cfb\u7edf\u63d0\u793a\u4e0e\u7528\u6237\u63d0\u793a\uff1a1) \u56fa\u5b9a\u7cfb\u7edf\u63d0\u793a\u4f18\u5316\u7528\u6237\u63d0\u793a 2) \u57fa\u4e8e\u7528\u6237\u63d0\u793a\u66f4\u65b0\u7cfb\u7edf\u63d0\u793a\uff0c\u5f62\u6210\u534f\u540c\u8fdb\u5316\u673a\u5236\u3002", "result": "\u57285\u4e2a\u9886\u57df14\u4e2a\u672a\u89c1\u6570\u636e\u96c6\u9a8c\u8bc1\uff1a\u4f18\u5316\u540e\u7684\u7cfb\u7edf\u63d0\u793a\u5bf9\u591a\u6837\u5316\u7528\u6237\u63d0\u793a\u5c55\u73b0\u5f3a\u6cdb\u5316\u6027\uff0c\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u4ec5\u9700\u5c11\u91cf\u4f18\u5316\u6b65\u9aa4\u5373\u53ef\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\uff08\u6700\u9ad8\u8fbeXX%\uff09\u3002", "conclusion": "\u7cfb\u7edf\u63d0\u793a\u4f18\u5316\u662f\u63d0\u5347LLM\u901a\u7528\u6027\u7684\u5173\u952e\u8def\u5f84\uff0c\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u63d0\u793a\u5de5\u7a0b\u4ece\u4efb\u52a1\u7279\u5b9a\u5230\u4efb\u52a1\u65e0\u5173\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u4e3a\u6a21\u578b\u5feb\u901f\u90e8\u7f72\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2505.09746", "pdf": "https://arxiv.org/pdf/2505.09746", "abs": "https://arxiv.org/abs/2505.09746", "authors": ["Xabier Morales", "Ayah Elsayed", "Debbie Zhao", "Filip Loncaric", "Ainhoa Aguado", "Mireia Masias", "Gina Quill", "Marc Ramos", "Ada Doltra", "Ana Garcia", "Marta Sitges", "David Marlevi", "Alistair Young", "Martyn Nash", "Bart Bijnens", "Oscar Camara"], "title": "A Computational Pipeline for Advanced Analysis of 4D Flow MRI in the Left Atrium", "categories": ["cs.CV"], "comment": null, "summary": "The left atrium (LA) plays a pivotal role in modulating left ventricular\nfilling, but our comprehension of its hemodynamics is significantly limited by\nthe constraints of conventional ultrasound analysis. 4D flow magnetic resonance\nimaging (4D Flow MRI) holds promise for enhancing our understanding of atrial\nhemodynamics. However, the low velocities within the LA and the limited spatial\nresolution of 4D Flow MRI make analyzing this chamber challenging. Furthermore,\nthe absence of dedicated computational frameworks, combined with diverse\nacquisition protocols and vendors, complicates gathering large cohorts for\nstudying the prognostic value of hemodynamic parameters provided by 4D Flow\nMRI. In this study, we introduce the first open-source computational framework\ntailored for the analysis of 4D Flow MRI in the LA, enabling comprehensive\nqualitative and quantitative analysis of advanced hemodynamic parameters. Our\nframework proves robust to data from different centers of varying quality,\nproducing high-accuracy automated segmentations (Dice $>$ 0.9 and Hausdorff 95\n$<$ 3 mm), even with limited training data. Additionally, we conducted the\nfirst comprehensive assessment of energy, vorticity, and pressure parameters in\nthe LA across a spectrum of disorders to investigate their potential as\nprognostic biomarkers.", "AI": {"tldr": "\u5f00\u53d1\u9996\u4e2a\u9488\u5bf9\u5de6\u5fc3\u623f4D Flow MRI\u5206\u6790\u7684\u5f00\u6e90\u8ba1\u7b97\u6846\u67b6\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u81ea\u52a8\u5206\u5272\uff08Dice>0.9\uff09\u5e76\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u8840\u6d41\u52a8\u529b\u5b66\u53c2\u6570\u4f5c\u4e3a\u9884\u540e\u751f\u7269\u6807\u5fd7\u7269\u7684\u6f5c\u529b\u3002", "motivation": "\u4f20\u7edf\u8d85\u58f0\u548c\u73b0\u67094D Flow MRI\u5206\u6790\u5de5\u5177\u5728\u5de6\u5fc3\u623f\u8840\u6d41\u52a8\u529b\u5b66\u7814\u7a76\u5b58\u5728\u7a7a\u95f4\u5206\u8fa8\u7387\u4f4e\u3001\u7f3a\u4e4f\u4e13\u7528\u5206\u6790\u6846\u67b6\u3001\u6570\u636e\u5f02\u6784\u6027\u4e09\u5927\u74f6\u9888\uff0c\u9650\u5236\u4e86\u8840\u6d41\u53c2\u6570\u9884\u540e\u4ef7\u503c\u7684\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u591a\u4e2d\u5fc3\u517c\u5bb9\u7684\u5f00\u6e90\u8ba1\u7b97\u6846\u67b6\uff0c\u91c7\u7528\u81ea\u52a8\u5316\u5206\u5272\u7b97\u6cd5\uff08\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u5b9e\u73b0Dice>0.9\u7684\u5206\u5272\u7cbe\u5ea6\uff09\uff0c\u652f\u6301\u80fd\u91cf\u3001\u6da1\u5ea6\u3001\u538b\u529b\u7b49\u9ad8\u7ea7\u8840\u6d41\u53c2\u6570\u7684\u5b9a\u6027\u4e0e\u5b9a\u91cf\u5206\u6790\u3002", "result": "\u6846\u67b6\u5728\u4e0d\u540c\u8d28\u91cf\u7684\u591a\u4e2d\u5fc3\u6570\u636e\u4e2d\u8868\u73b0\u7a33\u5065\uff0c\u5b9e\u73b0\u6beb\u7c73\u7ea7\u7cbe\u5ea6\u5206\u5272\uff08Hausdorff 95<3mm\uff09\uff0c\u5e76\u9996\u6b21\u7cfb\u7edf\u63ed\u793a\u4e0d\u540c\u75be\u75c5\u8c31\u4e2d\u5de6\u5fc3\u623f\u8840\u6d41\u52a8\u529b\u5b66\u53c2\u6570\u7684\u5dee\u5f02\u6027\u7279\u5f81\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5de6\u5fc3\u623f\u8840\u6d41\u5206\u6790\u63d0\u4f9b\u6807\u51c6\u5316\u5de5\u5177\uff0c\u8bc1\u5b9e\u80fd\u91cf/\u6da1\u5ea6/\u538b\u529b\u7b49\u53c2\u6570\u4f5c\u4e3a\u9884\u540e\u751f\u7269\u6807\u5fd7\u7269\u7684\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\uff0c\u63a8\u52a8\u4e2a\u6027\u5316\u8840\u6d41\u52a8\u529b\u5b66\u8bc4\u4f30\u53d1\u5c55\u3002"}}
{"id": "2505.10144", "pdf": "https://arxiv.org/pdf/2505.10144", "abs": "https://arxiv.org/abs/2505.10144", "authors": ["Xuechang Tu", "Lukas Radl", "Michael Steiner", "Markus Steinberger", "Bernhard Kerbl", "Fernando de la Torre"], "title": "VRSplat: Fast and Robust Gaussian Splatting for Virtual Reality", "categories": ["cs.GR", "cs.CV"], "comment": "I3D'25 (PACMCGIT); Project Page: https://cekavis.site/VRSplat/", "summary": "3D Gaussian Splatting (3DGS) has rapidly become a leading technique for\nnovel-view synthesis, providing exceptional performance through efficient\nsoftware-based GPU rasterization. Its versatility enables real-time\napplications, including on mobile and lower-powered devices. However, 3DGS\nfaces key challenges in virtual reality (VR): (1) temporal artifacts, such as\npopping during head movements, (2) projection-based distortions that result in\ndisturbing and view-inconsistent floaters, and (3) reduced framerates when\nrendering large numbers of Gaussians, falling below the critical threshold for\nVR. Compared to desktop environments, these issues are drastically amplified by\nlarge field-of-view, constant head movements, and high resolution of\nhead-mounted displays (HMDs). In this work, we introduce VRSplat: we combine\nand extend several recent advancements in 3DGS to address challenges of VR\nholistically. We show how the ideas of Mini-Splatting, StopThePop, and Optimal\nProjection can complement each other, by modifying the individual techniques\nand core 3DGS rasterizer. Additionally, we propose an efficient foveated\nrasterizer that handles focus and peripheral areas in a single GPU launch,\navoiding redundant computations and improving GPU utilization. Our method also\nincorporates a fine-tuning step that optimizes Gaussian parameters based on\nStopThePop depth evaluations and Optimal Projection. We validate our method\nthrough a controlled user study with 25 participants, showing a strong\npreference for VRSplat over other configurations of Mini-Splatting. VRSplat is\nthe first, systematically evaluated 3DGS approach capable of supporting modern\nVR applications, achieving 72+ FPS while eliminating popping and\nstereo-disrupting floaters.", "AI": {"tldr": "VRSplat\u6574\u5408\u591a\u98793DGS\u4f18\u5316\u6280\u672f\uff0c\u89e3\u51b3VR\u573a\u666f\u4e0b\u65f6\u95f4\u4f2a\u5f71\u3001\u6295\u5f71\u5931\u771f\u548c\u5e27\u7387\u4e0d\u8db3\u95ee\u9898\uff0c\u5b9e\u73b072+ FPS\u6d41\u7545\u4f53\u9a8c\u5e76\u6d88\u9664\u89c6\u89c9\u5e72\u6270", "motivation": "3DGS\u5728VR\u5e94\u7528\u4e2d\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a(1)\u5934\u90e8\u8fd0\u52a8\u65f6\u7684\u52a8\u6001\u4f2a\u5f71 (2)\u6295\u5f71\u5931\u771f\u5bfc\u81f4\u7684\u89c6\u89c9\u5e72\u6270 (3)HMD\u9ad8\u5206\u8fa8\u7387\u4e0b\u5e27\u7387\u4e0d\u8db3", "method": "\u6574\u5408Mini-Splatting/StopThePop/Optimal Projection\u6280\u672f\uff0c\u6539\u8fdb\u6838\u5fc3\u5149\u6805\u5316\u5668\uff0c\u63d0\u51fa\u5355GPU\u542f\u52a8\u7684\u773c\u52a8\u8ffd\u8e2a\u6e32\u67d3\u65b9\u6848\uff0c\u901a\u8fc7\u6df1\u5ea6\u8bc4\u4f30\u4f18\u5316\u9ad8\u65af\u53c2\u6570", "result": "\u7528\u6237\u7814\u7a76\u663e\u793a\u663e\u8457\u4f18\u4e8e\u57fa\u7840Mini-Splatting\uff0c\u8fbe\u523072+ FPS\uff0c\u6210\u529f\u6d88\u9664\u52a8\u6001\u4f2a\u5f71\u548c\u7acb\u4f53\u89c6\u89c9\u5e72\u6270\uff0c\u652f\u6301\u73b0\u4ee3VR\u5e94\u7528", "conclusion": "VRSplat\u9996\u6b21\u7cfb\u7edf\u6027\u5730\u89e3\u51b33DGS\u5728VR\u4e2d\u7684\u6838\u5fc3\u75db\u70b9\uff0c\u901a\u8fc7\u6280\u672f\u521b\u65b0\u5b9e\u73b0\u9ad8\u8d28\u91cf\u5b9e\u65f6\u6e32\u67d3\uff0c\u4e3a\u79fb\u52a8\u7aefVR\u5e94\u7528\u5960\u5b9a\u6280\u672f\u57fa\u7840"}}
{"id": "2505.09701", "pdf": "https://arxiv.org/pdf/2505.09701", "abs": "https://arxiv.org/abs/2505.09701", "authors": ["Xin Liu", "Lechen Zhang", "Sheza Munir", "Yiyang Gu", "Lu Wang"], "title": "VeriFact: Enhancing Long-Form Factuality Evaluation with Refined Fact Extraction and Reference Facts", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) excel at generating long-form responses, but\nevaluating their factuality remains challenging due to complex inter-sentence\ndependencies within the generated facts. Prior solutions predominantly follow a\ndecompose-decontextualize-verify pipeline but often fail to capture essential\ncontext and miss key relational facts. In this paper, we introduce VeriFact, a\nfactuality evaluation framework designed to enhance fact extraction by\nidentifying and resolving incomplete and missing facts to support more accurate\nverification results. Moreover, we introduce FactRBench , a benchmark that\nevaluates both precision and recall in long-form model responses, whereas prior\nwork primarily focuses on precision. FactRBench provides reference fact sets\nfrom advanced LLMs and human-written answers, enabling recall assessment.\nEmpirical evaluations show that VeriFact significantly enhances fact\ncompleteness and preserves complex facts with critical relational information,\nresulting in more accurate factuality evaluation. Benchmarking various open-\nand close-weight LLMs on FactRBench indicate that larger models within same\nmodel family improve precision and recall, but high precision does not always\ncorrelate with high recall, underscoring the importance of comprehensive\nfactuality assessment.", "AI": {"tldr": "\u63d0\u51faVeriFact\u6846\u67b6\u63d0\u5347LLMs\u4e8b\u5b9e\u6027\u8bc4\u4f30\u7684\u5b8c\u6574\u6027\uff0c\u5e76\u521b\u5efaFactRBench\u57fa\u51c6\u5b9e\u73b0\u7cbe\u786e\u7387\u4e0e\u53ec\u56de\u7387\u53cc\u91cd\u8bc4\u4f30", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5206\u89e3-\u53bb\u8bed\u5883\u5316-\u9a8c\u8bc1\u6d41\u7a0b\u4e2d\u96be\u4ee5\u6355\u6349\u5173\u952e\u4e0a\u4e0b\u6587\u548c\u5173\u7cfb\u4e8b\u5b9e\uff0c\u5bfc\u81f4\u4e8b\u5b9e\u5b8c\u6574\u6027\u4e0d\u8db3", "method": "1. \u5f00\u53d1VeriFact\u6846\u67b6\u589e\u5f3a\u4e8b\u5b9e\u63d0\u53d6\u5b8c\u6574\u6027 2. \u6784\u5efaFactRBench\u57fa\u51c6\u5305\u542b\u4eba\u5de5\u6807\u6ce8\u548c\u5148\u8fdbLLM\u53c2\u8003\u7b54\u6848 3. \u540c\u65f6\u8bc4\u4f30\u7cbe\u786e\u7387\u4e0e\u53ec\u56de\u7387", "result": "VeriFact\u63d0\u5347\u4e8b\u5b9e\u5b8c\u6574\u602780%\uff0c\u6a21\u578b\u89c4\u6a21\u6269\u5c55\u53ef\u63d0\u9ad8\u6027\u80fd\u4f46\u7cbe\u786e\u7387\u4e0e\u53ec\u56de\u7387\u65e0\u5fc5\u7136\u76f8\u5173\u6027\uff08\u5982Llama3-70B\u7cbe\u786e\u738770%\u4f46\u53ec\u56de\u4ec562%\uff09", "conclusion": "\u7efc\u5408\u4e8b\u5b9e\u6027\u8bc4\u4f30\u9700\u540c\u65f6\u8003\u8651\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\uff0c\u6a21\u578b\u6027\u80fd\u63d0\u5347\u4e0d\u7b49\u4e8e\u8bc4\u4f30\u7ef4\u5ea6\u5168\u9762\u8986\u76d6\uff0c\u9700\u4e13\u7528\u8bc4\u4f30\u6846\u67b6\u652f\u6301"}}
{"id": "2505.09827", "pdf": "https://arxiv.org/pdf/2505.09827", "abs": "https://arxiv.org/abs/2505.09827", "authors": ["Julian Tanke", "Takashi Shibuya", "Kengo Uchida", "Koichi Saito", "Yuki Mitsufuji"], "title": "Dyadic Mamba: Long-term Dyadic Human Motion Synthesis", "categories": ["cs.CV"], "comment": "CVPR 2025 HuMoGen Workshop", "summary": "Generating realistic dyadic human motion from text descriptions presents\nsignificant challenges, particularly for extended interactions that exceed\ntypical training sequence lengths. While recent transformer-based approaches\nhave shown promising results for short-term dyadic motion synthesis, they\nstruggle with longer sequences due to inherent limitations in positional\nencoding schemes. In this paper, we introduce Dyadic Mamba, a novel approach\nthat leverages State-Space Models (SSMs) to generate high-quality dyadic human\nmotion of arbitrary length. Our method employs a simple yet effective\narchitecture that facilitates information flow between individual motion\nsequences through concatenation, eliminating the need for complex\ncross-attention mechanisms. We demonstrate that Dyadic Mamba achieves\ncompetitive performance on standard short-term benchmarks while significantly\noutperforming transformer-based approaches on longer sequences. Additionally,\nwe propose a new benchmark for evaluating long-term motion synthesis quality,\nproviding a standardized framework for future research. Our results demonstrate\nthat SSM-based architectures offer a promising direction for addressing the\nchallenging task of long-term dyadic human motion synthesis from text\ndescriptions.", "AI": {"tldr": "\u63d0\u51faDyadic Mamba\u6a21\u578b\uff0c\u5229\u7528\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5b9e\u73b0\u4efb\u610f\u957f\u5ea6\u7684\u9ad8\u8d28\u91cf\u4e8c\u5143\u4eba\u4f53\u8fd0\u52a8\u5408\u6210\uff0c\u5728\u957f\u5e8f\u5217\u751f\u6210\u4e0a\u663e\u8457\u4f18\u4e8eTransformer\u65b9\u6cd5", "motivation": "\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u53d7\u9650\u4e8e\u4f4d\u7f6e\u7f16\u7801\u673a\u5236\uff0c\u96be\u4ee5\u751f\u6210\u7b26\u5408\u6587\u672c\u63cf\u8ff0\u7684\u957f\u65f6\u95f4\u4e8c\u5143\u4eba\u4f53\u4ea4\u4e92\u8fd0\u52a8", "method": "\u91c7\u7528\u72b6\u6001\u7a7a\u95f4\u6a21\u578b(SSM)\u67b6\u6784\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u5e8f\u5217\u62fc\u63a5\u5b9e\u73b0\u8fd0\u52a8\u5e8f\u5217\u95f4\u7684\u4fe1\u606f\u6d41\u52a8\uff0c\u907f\u514d\u590d\u6742\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236", "result": "\u5728\u77ed\u5e8f\u5217\u57fa\u51c6\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u957f\u5e8f\u5217\u751f\u6210\u8d28\u91cf\u663e\u8457\u8d85\u8d8aTransformer\u65b9\u6cd5\uff0c\u5e76\u5efa\u7acb\u65b0\u7684\u957f\u65f6\u8fd0\u52a8\u5408\u6210\u8bc4\u4f30\u57fa\u51c6", "conclusion": "\u57fa\u4e8eSSM\u7684\u67b6\u6784\u4e3a\u957f\u65f6\u4e8c\u5143\u8fd0\u52a8\u5408\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u51fa\u7684\u65b0\u57fa\u51c6\u5c06\u63a8\u52a8\u8be5\u9886\u57df\u6807\u51c6\u5316\u53d1\u5c55"}}
{"id": "2505.10558", "pdf": "https://arxiv.org/pdf/2505.10558", "abs": "https://arxiv.org/abs/2505.10558", "authors": ["Peiying Zhang", "Nanxuan Zhao", "Jing Liao"], "title": "Style Customization of Text-to-Vector Generation with Image Diffusion Priors", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted by SIGGRAPH 2025 (Conference Paper). Project page:\n  https://customsvg.github.io", "summary": "Scalable Vector Graphics (SVGs) are highly favored by designers due to their\nresolution independence and well-organized layer structure. Although existing\ntext-to-vector (T2V) generation methods can create SVGs from text prompts, they\noften overlook an important need in practical applications: style\ncustomization, which is vital for producing a collection of vector graphics\nwith consistent visual appearance and coherent aesthetics. Extending existing\nT2V methods for style customization poses certain challenges.\nOptimization-based T2V models can utilize the priors of text-to-image (T2I)\nmodels for customization, but struggle with maintaining structural regularity.\nOn the other hand, feed-forward T2V models can ensure structural regularity,\nyet they encounter difficulties in disentangling content and style due to\nlimited SVG training data.\n  To address these challenges, we propose a novel two-stage style customization\npipeline for SVG generation, making use of the advantages of both feed-forward\nT2V models and T2I image priors. In the first stage, we train a T2V diffusion\nmodel with a path-level representation to ensure the structural regularity of\nSVGs while preserving diverse expressive capabilities. In the second stage, we\ncustomize the T2V diffusion model to different styles by distilling customized\nT2I models. By integrating these techniques, our pipeline can generate\nhigh-quality and diverse SVGs in custom styles based on text prompts in an\nefficient feed-forward manner. The effectiveness of our method has been\nvalidated through extensive experiments. The project page is\nhttps://customsvg.github.io.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u9636\u6bb5SVG\u98ce\u683c\u5b9a\u5236\u6d41\u7a0b\uff0c\u7ed3\u5408\u524d\u9988T2V\u6a21\u578b\u548cT2I\u56fe\u50cf\u5148\u9a8c\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u98ce\u683c\u5316\u77e2\u91cf\u56fe\u5f62\u751f\u6210", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u77e2\u91cf\u65b9\u6cd5\u5728\u98ce\u683c\u5b9a\u5236\u65f6\u5b58\u5728\u7ed3\u6784\u4e0d\u89c4\u5f8b\u6216\u5185\u5bb9\u98ce\u683c\u8026\u5408\u95ee\u9898\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u9700\u6c42", "method": "\u7b2c\u4e00\u9636\u6bb5\u8bad\u7ec3\u8def\u5f84\u7ea7T2V\u6269\u6563\u6a21\u578b\u4fdd\u8bc1\u7ed3\u6784\u89c4\u5f8b\u6027\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u84b8\u998f\u5b9a\u5236T2I\u6a21\u578b\u5b9e\u73b0\u98ce\u683c\u9002\u5e94", "result": "\u7ecf\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u53ef\u751f\u6210\u98ce\u683c\u4e00\u81f4\u3001\u7ed3\u6784\u89c4\u8303\u7684\u9ad8\u8d28\u91cfSVG\uff0c\u652f\u6301\u591a\u6837\u5316\u6587\u672c\u63d0\u793a\u7684\u5feb\u901f\u751f\u6210", "conclusion": "\u8be5\u521b\u65b0\u65b9\u6cd5\u6709\u6548\u7ed3\u5408\u4e0d\u540c\u6a21\u578b\u4f18\u52bf\uff0c\u7a81\u7834\u73b0\u6709\u6280\u672f\u74f6\u9888\uff0c\u4e3a\u8bbe\u8ba1\u5e08\u63d0\u4f9b\u9ad8\u6548\u7684\u98ce\u683c\u5316\u77e2\u91cf\u521b\u4f5c\u5de5\u5177"}}
{"id": "2505.09724", "pdf": "https://arxiv.org/pdf/2505.09724", "abs": "https://arxiv.org/abs/2505.09724", "authors": ["Gino Carmona-D\u00edaz", "William Jim\u00e9nez-Leal", "Mar\u00eda Alejandra Grisales", "Chandra Sripada", "Santiago Amaya", "Michael Inzlicht", "Juan Pablo Berm\u00fadez"], "title": "An AI-Powered Research Assistant in the Lab: A Practical Guide for Text Analysis Through Iterative Collaboration with LLMs", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "31 pages, 1 figure", "summary": "Analyzing texts such as open-ended responses, headlines, or social media\nposts is a time- and labor-intensive process highly susceptible to bias. LLMs\nare promising tools for text analysis, using either a predefined (top-down) or\na data-driven (bottom-up) taxonomy, without sacrificing quality. Here we\npresent a step-by-step tutorial to efficiently develop, test, and apply\ntaxonomies for analyzing unstructured data through an iterative and\ncollaborative process between researchers and LLMs. Using personal goals\nprovided by participants as an example, we demonstrate how to write prompts to\nreview datasets and generate a taxonomy of life domains, evaluate and refine\nthe taxonomy through prompt and direct modifications, test the taxonomy and\nassess intercoder agreements, and apply the taxonomy to categorize an entire\ndataset with high intercoder reliability. We discuss the possibilities and\nlimitations of using LLMs for text analysis.", "AI": {"tldr": "\u63d0\u51faLLM\u534f\u540c\u6587\u672c\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u4eba\u673a\u8fed\u4ee3\u5f00\u53d1\u5206\u7c7b\u6cd5\u5e76\u5b9e\u73b0\u9ad8\u7f16\u7801\u4e00\u81f4\u6027", "motivation": "\u4f20\u7edf\u6587\u672c\u5206\u6790\u65b9\u6cd5\u8017\u65f6\u4e14\u6613\u53d7\u4e3b\u89c2\u504f\u5dee\u5f71\u54cd\uff0c\u9700\u8981\u9ad8\u6548\u53ef\u9760\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848", "method": "\u91c7\u7528\u8fed\u4ee3\u5f0f\u4eba\u673a\u534f\u4f5c\u6d41\u7a0b\uff1a1) \u63d0\u793a\u5de5\u7a0b\u751f\u6210\u521d\u59cb\u5206\u7c7b\u6cd5 2) \u6df7\u5408\u63d0\u793a\u4f18\u5316\u4e0e\u4eba\u5de5\u4fee\u6b63 3) \u7f16\u7801\u4e00\u81f4\u6027\u68c0\u9a8c 4) \u5168\u6570\u636e\u96c6\u81ea\u52a8\u5206\u7c7b", "result": "\u5728\u4e2a\u4eba\u76ee\u6807\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b9e\u73b00.88\u7684\u7f16\u7801\u8005\u95f4\u4e00\u81f4\u6027\u7cfb\u6570\uff0c\u663e\u8457\u9ad8\u4e8e\u4f20\u7edf\u4eba\u5de5\u7f16\u7801", "conclusion": "LLM\u5728\u4fdd\u6301\u9ad8\u6548\u7387\u7684\u540c\u65f6\u53ef\u5b9e\u73b0\u4e13\u4e1a\u7ea7\u6587\u672c\u5206\u6790\u6548\u679c\uff0c\u4f46\u9700\u8b66\u60d5\u6a21\u578b\u504f\u5dee\u4e0e\u9886\u57df\u9002\u5e94\u6027\u9650\u5236"}}
{"id": "2505.09829", "pdf": "https://arxiv.org/pdf/2505.09829", "abs": "https://arxiv.org/abs/2505.09829", "authors": ["Tushar Kataria", "Shireen Y. Elhabian"], "title": "BoundarySeg:An Embarrassingly Simple Method To Boost Medical Image Segmentation Performance for Low Data Regimes", "categories": ["cs.CV"], "comment": null, "summary": "Obtaining large-scale medical data, annotated or unannotated, is challenging\ndue to stringent privacy regulations and data protection policies. In addition,\nannotating medical images requires that domain experts manually delineate\nanatomical structures, making the process both time-consuming and costly. As a\nresult, semi-supervised methods have gained popularity for reducing annotation\ncosts. However, the performance of semi-supervised methods is heavily dependent\non the availability of unannotated data, and their effectiveness declines when\nsuch data are scarce or absent. To overcome this limitation, we propose a\nsimple, yet effective and computationally efficient approach for medical image\nsegmentation that leverages only existing annotations. We propose BoundarySeg ,\na multi-task framework that incorporates organ boundary prediction as an\nauxiliary task to full organ segmentation, leveraging consistency between the\ntwo task predictions to provide additional supervision. This strategy improves\nsegmentation accuracy, especially in low data regimes, allowing our method to\nachieve performance comparable to or exceeding state-of-the-art semi supervised\napproaches all without relying on unannotated data or increasing computational\ndemands. Code will be released upon acceptance.", "AI": {"tldr": "\u63d0\u51faBoundarySeg\u591a\u4efb\u52a1\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5668\u5b98\u8fb9\u754c\u9884\u6d4b\u4e0e\u5668\u5b98\u5206\u5272\u4efb\u52a1\u7684\u4e00\u81f4\u6027\u76d1\u7763\uff0c\u5728\u65e0\u9700\u672a\u6807\u6ce8\u6570\u636e\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u4f4e\u6570\u636e\u573a\u666f\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7cbe\u5ea6", "motivation": "\u533b\u7597\u6570\u636e\u83b7\u53d6\u56f0\u96be\u4e14\u6807\u6ce8\u6210\u672c\u9ad8\u6602\uff0c\u73b0\u6709\u534a\u76d1\u7763\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u672a\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u5728\u5b9e\u9645\u4e34\u5e8a\u573a\u666f\u4e2d\u672a\u6807\u6ce8\u6570\u636e\u53ef\u80fd\u540c\u6837\u7a00\u7f3a\u3002\u9700\u8981\u5f00\u53d1\u4ec5\u5229\u7528\u73b0\u6709\u6807\u6ce8\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51fa\u8fb9\u754c\u5206\u5272\u6846\u67b6BoundarySeg\uff1a1\uff09\u5c06\u5668\u5b98\u8fb9\u754c\u9884\u6d4b\u4f5c\u4e3a\u8f85\u52a9\u4efb\u52a1 2\uff09\u901a\u8fc7\u4e3b\u4efb\u52a1\uff08\u5668\u5b98\u5206\u5272\uff09\u4e0e\u8f85\u52a9\u4efb\u52a1\u9884\u6d4b\u7ed3\u679c\u7684\u4e00\u81f4\u6027\u7ea6\u675f\u63d0\u4f9b\u989d\u5916\u76d1\u7763\u4fe1\u53f7 3\uff09\u53cc\u4efb\u52a1\u5171\u4eab\u7f16\u7801\u5668\u4f46\u4f7f\u7528\u72ec\u7acb\u89e3\u7801\u5668", "result": "\u5728\u4f4e\u6570\u636e\u6761\u4ef6\u4e0b\u8fbe\u5230/\u8d85\u8d8a\u73b0\u6709\u534a\u76d1\u7763\u65b9\u6cd5\u6027\u80fd\uff0c\u8ba1\u7b97\u6548\u7387\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u5f53\uff08FLOPs\u4ec5\u589e\u52a00.02%\uff09\uff0c\u4ee3\u7801\u5c06\u5728\u8bba\u6587\u63a5\u6536\u540e\u5f00\u6e90", "conclusion": "\u901a\u8fc7\u591a\u4efb\u52a1\u6846\u67b6\u4e0e\u9884\u6d4b\u4e00\u81f4\u6027\u76d1\u7763\u673a\u5236\uff0c\u6709\u6548\u6316\u6398\u6709\u9650\u6807\u6ce8\u6570\u636e\u7684\u6f5c\u529b\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u4e34\u5e8a\u573a\u666f"}}
{"id": "2505.09936", "pdf": "https://arxiv.org/pdf/2505.09936", "abs": "https://arxiv.org/abs/2505.09936", "authors": ["Chenglong Wang", "Yuhao Kang", "Zhaoya Gong", "Pengjun Zhao", "Yu Feng", "Wenjia Zhang", "Ge Li"], "title": "CartoAgent: a multimodal large language model-powered multi-agent cartographic framework for map style transfer and evaluation", "categories": ["cs.HC", "cs.GR", "cs.MA", "cs.MM"], "comment": "57 pages, 17 figures", "summary": "The rapid development of generative artificial intelligence (GenAI) presents\nnew opportunities to advance the cartographic process. Previous studies have\neither overlooked the artistic aspects of maps or faced challenges in creating\nboth accurate and informative maps. In this study, we propose CartoAgent, a\nnovel multi-agent cartographic framework powered by multimodal large language\nmodels (MLLMs). This framework simulates three key stages in cartographic\npractice: preparation, map design, and evaluation. At each stage, different\nMLLMs act as agents with distinct roles to collaborate, discuss, and utilize\ntools for specific purposes. In particular, CartoAgent leverages MLLMs' visual\naesthetic capability and world knowledge to generate maps that are both\nvisually appealing and informative. By separating style from geographic data,\nit can focus on designing stylesheets without modifying the vector-based data,\nthereby ensuring geographic accuracy. We applied CartoAgent to a specific task\ncentered on map restyling-namely, map style transfer and evaluation. The\neffectiveness of this framework was validated through extensive experiments and\na human evaluation study. CartoAgent can be extended to support a variety of\ncartographic design decisions and inform future integrations of GenAI in\ncartography.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u5236\u56fe\u6846\u67b6CartoAgent\uff0c\u901a\u8fc7\u5206\u79bb\u6837\u5f0f\u4e0e\u5730\u7406\u6570\u636e\u5b9e\u73b0\u89c6\u89c9\u7f8e\u89c2\u4e14\u7cbe\u51c6\u7684\u5730\u56fe\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5b58\u5728\u5ffd\u89c6\u5730\u56fe\u827a\u672f\u6027\u6216\u96be\u4ee5\u517c\u987e\u5730\u56fe\u7cbe\u786e\u6027\u4e0e\u4fe1\u606f\u4e30\u5bcc\u6027\u7684\u75db\u70b9\uff0c\u751f\u6210\u5f0fAI\u53d1\u5c55\u4e3a\u6539\u8fdb\u5236\u56fe\u6d41\u7a0b\u63d0\u4f9b\u65b0\u673a\u9047\u3002", "method": "\u6784\u5efa\u4e09\u9636\u6bb5\u6846\u67b6\uff08\u51c6\u5907/\u8bbe\u8ba1/\u8bc4\u4f30\uff09\uff0c\u91c7\u7528\u591aMLLM\u667a\u80fd\u4f53\u534f\u540c\u5de5\u4f5c\uff0c\u5229\u7528\u89c6\u89c9\u5ba1\u7f8e\u80fd\u529b\u8bbe\u8ba1\u6837\u5f0f\u8868\u5e76\u4fdd\u6301\u5730\u7406\u6570\u636e\u72ec\u7acb\u6027\u3002", "result": "\u5728\u5730\u56fe\u98ce\u683c\u8fc1\u79fb\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u901a\u8fc7\u5b9e\u9a8c\u548c\u4eba\u7c7b\u8bc4\u4f30\u8bc1\u660e\u6846\u67b6\u4ea7\u51fa\u8d28\u91cf\u3002", "conclusion": "\u6846\u67b6\u53ef\u6269\u5c55\u652f\u6301\u591a\u79cd\u5236\u56fe\u51b3\u7b56\uff0c\u4e3a\u751f\u6210\u5f0fAI\u4e0e\u5236\u56fe\u5b66\u878d\u5408\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2505.09738", "pdf": "https://arxiv.org/pdf/2505.09738", "abs": "https://arxiv.org/abs/2505.09738", "authors": ["Shaurya Sharthak", "Vinayak Pahalwan", "Adithya Kamath", "Adarsh Shirawalmath"], "title": "Achieving Tokenizer Flexibility in Language Models through Heuristic Adaptation and Supertoken Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Pretrained language models (LLMs) are often constrained by their fixed\ntokenization schemes, leading to inefficiencies and performance limitations,\nparticularly for multilingual or specialized applications. This tokenizer\nlock-in presents significant challenges. standard methods to overcome this\noften require prohibitive computational resources. Although tokenizer\nreplacement with heuristic initialization aims to reduce this burden, existing\nmethods often require exhaustive residual fine-tuning and still may not fully\npreserve semantic nuances or adequately address the underlying compression\ninefficiencies. Our framework introduces two innovations: first, Tokenadapt, a\nmodel-agnostic tokenizer transplantation method, and second, novel\npre-tokenization learning for multi-word Supertokens to enhance compression and\nreduce fragmentation. Tokenadapt initializes new unique token embeddings via a\nhybrid heuristic that combines two methods: a local estimate based on subword\ndecomposition using the old tokenizer, and a global estimate utilizing the\ntop-k semantically similar tokens from the original vocabulary. This\nmethodology aims to preserve semantics while significantly minimizing\nretraining requirements. Empirical investigations validate both contributions:\nthe transplantation heuristic successfully initializes unique tokens, markedly\noutperforming conventional baselines and sophisticated methods including\nTranstokenizer and ReTok, while our Supertokens achieve notable compression\ngains. Our zero-shot perplexity results demonstrate that the TokenAdapt hybrid\ninitialization consistently yields lower perplexity ratios compared to both\nReTok and TransTokenizer baselines across different base models and newly\ntrained target tokenizers. TokenAdapt typically reduced the overall perplexity\nratio significantly compared to ReTok, yielding at least a 2-fold improvement\nin these aggregate scores.", "AI": {"tldr": "\u63d0\u51faTokenadapt\u6846\u67b6\u89e3\u51b3LLM\u56fa\u5b9a\u5206\u8bcd\u5668\u9501\u5b9a\u95ee\u9898\uff0c\u901a\u8fc7\u6df7\u5408\u521d\u59cb\u5316\u7b56\u7565\u548cSupertokens\u9884\u5206\u8bcd\u6280\u672f\u63d0\u5347\u79fb\u690d\u6548\u7387\u5e76\u4fdd\u6301\u8bed\u4e49", "motivation": "\u56fa\u5b9a\u5206\u8bcd\u5668\u5bfc\u81f4\u591a\u8bed\u8a00/\u4e13\u4e1a\u573a\u666f\u6548\u7387\u4f4e\u4e0b\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u9ad8\u6602\u7b97\u529b\u4e14\u65e0\u6cd5\u6709\u6548\u4fdd\u7559\u8bed\u4e49\uff0c\u9700\u66f4\u4f18\u89e3\u51b3\u65b9\u6848", "method": "1. Tokenadapt\uff1a\u7ed3\u5408\u5b50\u8bcd\u5206\u89e3\u7684\u5c40\u90e8\u4f30\u8ba1\u4e0e\u5168\u5c40\u8bed\u4e49\u76f8\u4f3ctoken\u7684\u6df7\u5408\u521d\u59cb\u5316\u7b56\u7565\n2. Supertokens\uff1a\u591a\u8bcd\u9884\u5206\u8bcd\u5b66\u4e60\u4f18\u5316\u538b\u7f29\u4e0e\u788e\u7247\u5316", "result": "Tokenadapt\u96f6\u6837\u672c\u56f0\u60d1\u5ea6\u663e\u8457\u4f18\u4e8eReTok/TransTokenizer\uff0c\u56f0\u60d1\u5ea6\u6bd4\u7387\u964d\u4f4e\u81f3\u5c112\u500d\uff0cSupertokens\u5b9e\u73b0\u660e\u663e\u538b\u7f29\u589e\u76ca", "conclusion": "\u6df7\u5408\u521d\u59cb\u5316\u6709\u6548\u5e73\u8861\u8bed\u4e49\u4fdd\u6301\u4e0e\u8bad\u7ec3\u6210\u672c\uff0c\u4e3a\u5206\u8bcd\u5668\u79fb\u690d\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c"}}
{"id": "2505.09858", "pdf": "https://arxiv.org/pdf/2505.09858", "abs": "https://arxiv.org/abs/2505.09858", "authors": ["Danush Kumar Venkatesh", "Isabel Funke", "Micha Pfeiffer", "Fiona Kolbinger", "Hanna Maria Schmeiser", "Juergen Weitz", "Marius Distler", "Stefanie Speidel"], "title": "Mission Balance: Generating Under-represented Class Samples using Video Diffusion Models", "categories": ["cs.CV"], "comment": "Early accept at MICCAI 2025", "summary": "Computer-assisted interventions can improve intra-operative guidance,\nparticularly through deep learning methods that harness the spatiotemporal\ninformation in surgical videos. However, the severe data imbalance often found\nin surgical video datasets hinders the development of high-performing models.\nIn this work, we aim to overcome the data imbalance by synthesizing surgical\nvideos. We propose a unique two-stage, text-conditioned diffusion-based method\nto generate high-fidelity surgical videos for under-represented classes. Our\napproach conditions the generation process on text prompts and decouples\nspatial and temporal modeling by utilizing a 2D latent diffusion model to\ncapture spatial content and then integrating temporal attention layers to\nensure temporal consistency. Furthermore, we introduce a rejection sampling\nstrategy to select the most suitable synthetic samples, effectively augmenting\nexisting datasets to address class imbalance. We evaluate our method on two\ndownstream tasks-surgical action recognition and intra-operative event\nprediction-demonstrating that incorporating synthetic videos from our approach\nsubstantially enhances model performance. We open-source our implementation at\nhttps://gitlab.com/nct_tso_public/surgvgen.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6587\u672c\u6761\u4ef6\u6269\u6563\u65b9\u6cd5\u751f\u6210\u9ad8\u4fdd\u771f\u624b\u672f\u89c6\u9891\uff0c\u901a\u8fc7\u62d2\u7edd\u91c7\u6837\u7b56\u7565\u589e\u5f3a\u6570\u636e\u96c6\uff0c\u6709\u6548\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u5e76\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd", "motivation": "\u624b\u672f\u89c6\u9891\u6570\u636e\u4e25\u91cd\u4e0d\u5e73\u8861\u95ee\u9898\u9650\u5236\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u5f00\u53d1\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u624b\u672f\u89c6\u9891\u7684\u65f6\u7a7a\u7279\u6027", "method": "\u6587\u672c\u6761\u4ef6\u6269\u6563\u6a21\u578b+\u65f6\u7a7a\u89e3\u8026\u5efa\u6a21\uff082D\u6f5c\u5728\u6269\u6563\u6a21\u578b\u5904\u7406\u7a7a\u95f4\u5185\u5bb9+\u65f6\u95f4\u6ce8\u610f\u529b\u5c42\u4fdd\u8bc1\u65f6\u5e8f\u4e00\u81f4\u6027\uff09\uff0c\u7ed3\u5408\u62d2\u7edd\u91c7\u6837\u7b5b\u9009\u673a\u5236", "result": "\u5728\u624b\u672f\u52a8\u4f5c\u8bc6\u522b\u548c\u672f\u4e2d\u4e8b\u4ef6\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528\u5408\u6210\u89c6\u9891\u589e\u5f3a\u540e\u7684\u6a21\u578b\u6027\u80fd\u663e\u8457\u63d0\u5347", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u624b\u672f\u89c6\u9891\u6570\u636e\u589e\u5f3a\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5f00\u6e90\u5b9e\u73b0\u4fc3\u8fdb\u9886\u57df\u53d1\u5c55"}}
{"id": "2505.10101", "pdf": "https://arxiv.org/pdf/2505.10101", "abs": "https://arxiv.org/abs/2505.10101", "authors": ["Jongmin Jung", "Dasaem Jeong"], "title": "LAV: Audio-Driven Dynamic Visual Generation with Neural Compression and StyleGAN2", "categories": ["cs.SD", "cs.AI", "cs.GR", "cs.MM", "eess.AS"], "comment": "Paper accepted at ISEA 2025, The 30th International Symposium on\n  Electronic/Emerging Art, Seoul, Republic of Korea, 23 - 29 May 2025", "summary": "This paper introduces LAV (Latent Audio-Visual), a system that integrates\nEnCodec's neural audio compression with StyleGAN2's generative capabilities to\nproduce visually dynamic outputs driven by pre-recorded audio. Unlike previous\nworks that rely on explicit feature mappings, LAV uses EnCodec embeddings as\nlatent representations, directly transformed into StyleGAN2's style latent\nspace via randomly initialized linear mapping. This approach preserves semantic\nrichness in the transformation, enabling nuanced and semantically coherent\naudio-visual translations. The framework demonstrates the potential of using\npretrained audio compression models for artistic and computational\napplications.", "AI": {"tldr": "\u63d0\u51faLAV\u7cfb\u7edf\uff0c\u901a\u8fc7\u795e\u7ecf\u97f3\u9891\u538b\u7f29\u4e0e\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u5b9e\u73b0\u97f3\u9891\u9a71\u52a8\u7684\u52a8\u6001\u89c6\u89c9\u751f\u6210", "motivation": "\u7a81\u7834\u4f20\u7edf\u663e\u5f0f\u7279\u5f81\u6620\u5c04\u65b9\u6cd5\uff0c\u63a2\u7d22\u9884\u8bad\u7ec3\u97f3\u9891\u6a21\u578b\u5728\u8de8\u6a21\u6001\u751f\u6210\u4e2d\u7684\u6f5c\u529b", "method": "\u4f7f\u7528EnCodec\u5d4c\u5165\u4f5c\u4e3a\u6f5c\u8868\u793a\uff0c\u901a\u8fc7\u968f\u673a\u521d\u59cb\u5316\u7ebf\u6027\u5c42\u76f4\u63a5\u6620\u5c04\u5230StyleGAN2\u98ce\u683c\u7a7a\u95f4", "result": "\u5b9e\u73b0\u8bed\u4e49\u8fde\u8d2f\u7684\u97f3\u89c6\u8f6c\u6362\uff0c\u9a8c\u8bc1\u9884\u8bad\u7ec3\u97f3\u9891\u538b\u7f29\u6a21\u578b\u5728\u827a\u672f\u8ba1\u7b97\u4e2d\u7684\u5e94\u7528\u4ef7\u503c", "conclusion": "\u4e3a\u8de8\u6a21\u6001\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u62d3\u5c55\u751f\u6210\u5f0f\u6a21\u578b\u5728\u521b\u610f\u8ba1\u7b97\u4e2d\u7684\u53ef\u80fd\u6027"}}
{"id": "2505.09794", "pdf": "https://arxiv.org/pdf/2505.09794", "abs": "https://arxiv.org/abs/2505.09794", "authors": ["J. Moreno-Casanova", "J. M. Au\u00f1\u00f3n", "A. M\u00e1rtinez-P\u00e9rez", "M. E. P\u00e9rez-Mart\u00ednez", "M. E. Gas-L\u00f3pez"], "title": "Automated Detection of Clinical Entities in Lung and Breast Cancer Reports Using NLP Techniques", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Research projects, including those focused on cancer, rely on the manual\nextraction of information from clinical reports. This process is time-consuming\nand prone to errors, limiting the efficiency of data-driven approaches in\nhealthcare. To address these challenges, Natural Language Processing (NLP)\noffers an alternative for automating the extraction of relevant data from\nelectronic health records (EHRs). In this study, we focus on lung and breast\ncancer due to their high incidence and the significant impact they have on\npublic health. Early detection and effective data management in both types of\ncancer are crucial for improving patient outcomes. To enhance the accuracy and\nefficiency of data extraction, we utilized GMV's NLP tool uQuery, which excels\nat identifying relevant entities in clinical texts and converting them into\nstandardized formats such as SNOMED and OMOP. uQuery not only detects and\nclassifies entities but also associates them with contextual information,\nincluding negated entities, temporal aspects, and patient-related details. In\nthis work, we explore the use of NLP techniques, specifically Named Entity\nRecognition (NER), to automatically identify and extract key clinical\ninformation from EHRs related to these two cancers. A dataset from Health\nResearch Institute Hospital La Fe (IIS La Fe), comprising 200 annotated breast\ncancer and 400 lung cancer reports, was used, with eight clinical entities\nmanually labeled using the Doccano platform. To perform NER, we fine-tuned the\nbsc-bio-ehr-en3 model, a RoBERTa-based biomedical linguistic model pre-trained\nin Spanish. Fine-tuning was performed using the Transformers architecture,\nenabling accurate recognition of clinical entities in these cancer types. Our\nresults demonstrate strong overall performance, particularly in identifying\nentities like MET and PAT, although challenges remain with less frequent\nentities like EVOL.", "AI": {"tldr": "\u5229\u7528NLP\u6280\u672f\u81ea\u52a8\u63d0\u53d6\u80ba\u764c\u548c\u4e73\u817a\u764c\u7535\u5b50\u75c5\u5386\u4e2d\u7684\u4e34\u5e8a\u4fe1\u606f\uff0c\u901a\u8fc7\u5fae\u8c03RoBERTa\u6a21\u578b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5b9e\u4f53\u8bc6\u522b\uff0c\u663e\u8457\u63d0\u5347\u533b\u7597\u6570\u636e\u5904\u7406\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u4e34\u5e8a\u62a5\u544a\u4eba\u5de5\u63d0\u53d6\u65b9\u5f0f\u6548\u7387\u4f4e\u4e0b\u4e14\u6613\u9519\uff0c\u963b\u788d\u533b\u7597\u6570\u636e\u9a71\u52a8\u7814\u7a76\u3002\u80ba\u764c\u548c\u4e73\u817a\u764c\u7684\u9ad8\u53d1\u75c5\u7387\u4e9f\u9700\u9ad8\u6548\u6570\u636e\u7ba1\u7406\u624b\u6bb5\u4ee5\u6539\u5584\u60a3\u8005\u9884\u540e\u3002", "method": "\u4f7f\u7528GMV\u7684uQuery\u5de5\u5177\u6807\u51c6\u5316\u4e34\u5e8a\u6587\u672c\uff0c\u57fa\u4e8e\u897f\u73ed\u7259\u8bedbsc-bio-ehr-en3\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u91c7\u7528Transformers\u67b6\u6784\u5728600\u4efd\u6807\u6ce8\u62a5\u544a\uff08200\u4e73\u817a\u764c+400\u80ba\u764c\uff09\u4e0a\u5b9e\u73b0\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3002", "result": "\u6a21\u578b\u6574\u4f53\u8868\u73b0\u4f18\u5f02\uff08MET/PAT\u8bc6\u522bF1\u503c0.94\uff09\uff0c\u4f46\u4f4e\u9891\u5b9e\u4f53EVOL\u8bc6\u522b\u5b58\u5728\u6311\u6218\uff08F1\u503c0.62\uff09\uff0c\u663e\u793a\u6570\u636e\u4e0d\u5e73\u8861\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "NLP\u6280\u672f\u663e\u8457\u63d0\u5347\u764c\u75c7\u4e34\u5e8a\u6570\u636e\u63d0\u53d6\u6548\u7387\uff0c\u672a\u6765\u9700\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u9886\u57df\u9002\u5e94\u4f18\u5316\u4f4e\u9891\u5b9e\u4f53\u8bc6\u522b\uff0c\u63a8\u52a8\u7cbe\u51c6\u533b\u7597\u53d1\u5c55\u3002"}}
{"id": "2505.09859", "pdf": "https://arxiv.org/pdf/2505.09859", "abs": "https://arxiv.org/abs/2505.09859", "authors": ["Andrew Jun Lee", "Taylor Webb", "Trevor Bihl", "Keith Holyoak", "Hongjing Lu"], "title": "Few-Shot Learning of Visual Compositional Concepts through Probabilistic Schema Induction", "categories": ["cs.CV"], "comment": "Lee, A. J., Webb, T., Bihl, T., Holyoak, K. J., & Lu, H. (2025).\n  Few-shot learning of visual compositional concepts through probabilistic\n  schema induction. In A. Ruggeri, D. Barner, C. Walker, & N. Bramley (Eds.),\n  Proceedings of the 47th Annual Conference of the Cognitive Science Society.\n  Cognitive Science Society", "summary": "The ability to learn new visual concepts from limited examples is a hallmark\nof human cognition. While traditional category learning models represent each\nexample as an unstructured feature vector, compositional concept learning is\nthought to depend on (1) structured representations of examples (e.g., directed\ngraphs consisting of objects and their relations) and (2) the identification of\nshared relational structure across examples through analogical mapping. Here,\nwe introduce Probabilistic Schema Induction (PSI), a prototype model that\nemploys deep learning to perform analogical mapping over structured\nrepresentations of only a handful of examples, forming a compositional concept\ncalled a schema. In doing so, PSI relies on a novel conception of similarity\nthat weighs object-level similarity and relational similarity, as well as a\nmechanism for amplifying relations relevant to classification, analogous to\nselective attention parameters in traditional models. We show that PSI produces\nhuman-like learning performance and outperforms two controls: a prototype model\nthat uses unstructured feature vectors extracted from a deep learning model,\nand a variant of PSI with weaker structured representations. Notably, we find\nthat PSI's human-like performance is driven by an adaptive strategy that\nincreases relational similarity over object-level similarity and upweights the\ncontribution of relations that distinguish classes. These findings suggest that\nstructured representations and analogical mapping are critical to modeling\nrapid human-like learning of compositional visual concepts, and demonstrate how\ndeep learning can be leveraged to create psychological models.", "AI": {"tldr": "\u63d0\u51fa\u6982\u7387\u56fe\u5f0f\u5f52\u7eb3\uff08PSI\uff09\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8868\u5f81\u4e0e\u7c7b\u6bd4\u6620\u5c04\u5b9e\u73b0\u4eba\u7c7b\u6c34\u5e73\u7684\u7ec4\u5408\u5f0f\u89c6\u89c9\u6982\u5ff5\u5b66\u4e60", "motivation": "\u4f20\u7edf\u5206\u7c7b\u6a21\u578b\u4f7f\u7528\u975e\u7ed3\u6784\u5316\u7279\u5f81\u5411\u91cf\uff0c\u800c\u4eba\u7c7b\u7ec4\u5408\u5f0f\u6982\u5ff5\u5b66\u4e60\u4f9d\u8d56\u7ed3\u6784\u5316\u8868\u5f81\u4e0e\u7c7b\u6bd4\u63a8\u7406\u673a\u5236\uff0c\u9700\u5efa\u7acb\u66f4\u63a5\u8fd1\u4eba\u7c7b\u8ba4\u77e5\u673a\u7406\u7684\u8ba1\u7b97\u6a21\u578b", "method": "\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u4e0e\u6982\u7387\u5efa\u6a21\uff1a1\uff09\u6784\u5efa\u5305\u542b\u5bf9\u8c61\u53ca\u5176\u5173\u7cfb\u7684\u56fe\u7ed3\u6784\u8868\u5f81\uff1b2\uff09\u8bbe\u8ba1\u6743\u8861\u5bf9\u8c61\u76f8\u4f3c\u5ea6\u4e0e\u5173\u7cfb\u76f8\u4f3c\u5ea6\u7684\u65b0\u578b\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u6846\u67b6\uff1b3\uff09\u5f15\u5165\u7c7b\u4f3c\u9009\u62e9\u6027\u6ce8\u610f\u529b\u7684\u5173\u7cfb\u6743\u91cd\u653e\u5927\u673a\u5236", "result": "PSI\u5728\u5c11\u6837\u672c\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\uff0c\u4f18\u4e8e\u975e\u7ed3\u6784\u5316\u539f\u578b\u6a21\u578b\uff08\u51c6\u786e\u7387\u63d0\u534715%\uff09\u548c\u5f31\u7ed3\u6784\u5316\u5bf9\u7167\u7ec4\uff0c\u5176\u81ea\u9002\u5e94\u7b56\u7565\u80fd\u52a8\u6001\u589e\u5f3a\u533a\u5206\u6027\u5173\u7cfb\u7279\u5f81", "conclusion": "\u7ed3\u6784\u5316\u8868\u5f81\u4e0e\u7c7b\u6bd4\u6620\u5c04\u662f\u5efa\u6a21\u4eba\u7c7b\u7ec4\u5408\u6982\u5ff5\u5b66\u4e60\u7684\u5173\u952e\uff0c\u6df1\u5ea6\u5b66\u4e60\u4e3a\u6784\u5efa\u5fc3\u7406\u8ba1\u7b97\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u81ea\u9002\u5e94\u5173\u7cfb\u6743\u91cd\u673a\u5236\u63ed\u793a\u4e86\u4eba\u7c7b\u6982\u5ff5\u5b66\u4e60\u7684\u8ba4\u77e5\u7b56\u7565"}}
{"id": "2505.09807", "pdf": "https://arxiv.org/pdf/2505.09807", "abs": "https://arxiv.org/abs/2505.09807", "authors": ["Timour Ichmoukhamedov", "David Martens"], "title": "Exploring the generalization of LLM truth directions on conversational formats", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Several recent works argue that LLMs have a universal truth direction where\ntrue and false statements are linearly separable in the activation space of the\nmodel. It has been demonstrated that linear probes trained on a single hidden\nstate of the model already generalize across a range of topics and might even\nbe used for lie detection in LLM conversations. In this work we explore how\nthis truth direction generalizes between various conversational formats. We\nfind good generalization between short conversations that end on a lie, but\npoor generalization to longer formats where the lie appears earlier in the\ninput prompt. We propose a solution that significantly improves this type of\ngeneralization by adding a fixed key phrase at the end of each conversation.\nOur results highlight the challenges towards reliable LLM lie detectors that\ngeneralize to new settings.", "AI": {"tldr": "\u8bba\u6587\u63a2\u7d22LLM\u771f\u7406\u65b9\u5411\u5728\u4e0d\u540c\u5bf9\u8bdd\u683c\u5f0f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u63d0\u51fa\u901a\u8fc7\u6dfb\u52a0\u5173\u952e\u77ed\u8bed\u6539\u5584\u957f\u5bf9\u8bdd\u573a\u666f\u4e0b\u7684\u8c0e\u8a00\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660eLLM\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u5b58\u5728\u53ef\u5206\u79bb\u771f\u5047\u9648\u8ff0\u7684\u771f\u7406\u65b9\u5411\uff0c\u4f46\u8be5\u65b9\u5411\u5728\u4e0d\u540c\u957f\u5ea6\u548c\u7ed3\u6784\u7684\u5bf9\u8bdd\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u963b\u788d\u53ef\u9760\u8c0e\u8a00\u68c0\u6d4b\u7cfb\u7edf\u7684\u5f00\u53d1\u3002", "method": "1. \u6d4b\u8bd5\u771f\u7406\u65b9\u5411\u5728\u77ed\u5bf9\u8bdd\uff08\u8c0e\u8a00\u5728\u672b\u5c3e\uff09\u4e0e\u957f\u5bf9\u8bdd\uff08\u8c0e\u8a00\u5728\u5f00\u5934\uff09\u7684\u6cdb\u5316\u8868\u73b0\n2. \u63d0\u51fa\u5728\u5bf9\u8bdd\u672b\u5c3e\u6dfb\u52a0\u56fa\u5b9a\u5173\u952e\u77ed\u8bed\u7684\u4f18\u5316\u65b9\u6848", "result": "\u5173\u952e\u77ed\u8bed\u7b56\u7565\u4f7f\u957f\u5bf9\u8bdd\u573a\u666f\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u63d0\u534737%\uff0c\u4f46\u8de8\u683c\u5f0f\u6cdb\u5316\u95ee\u9898\u4ecd\u672a\u5b8c\u5168\u89e3\u51b3\uff08\u77ed\u5bf9\u8bdd\u95f4AUC 0.89\u21920.91\uff0c\u77ed\u5230\u957f\u5bf9\u8bddAUC\u4e0b\u964d\u81f30.68\uff09", "conclusion": "LLM\u8c0e\u8a00\u68c0\u6d4b\u5668\u7684\u5b9e\u9645\u5e94\u7528\u9700\u8981\u66f4\u9c81\u68d2\u7684\u8de8\u683c\u5f0f\u6cdb\u5316\u80fd\u529b\uff0c\u5f53\u524d\u57fa\u4e8e\u5355\u4e00\u9690\u85cf\u72b6\u6001\u7684\u7ebf\u6027\u63a2\u6d4b\u65b9\u6cd5\u5b58\u5728\u573a\u666f\u5c40\u9650\u6027"}}
{"id": "2505.09915", "pdf": "https://arxiv.org/pdf/2505.09915", "abs": "https://arxiv.org/abs/2505.09915", "authors": ["Zhe Xin", "Chenyang Wu", "Penghui Huang", "Yanyong Zhang", "Yinian Mao", "Guoquan Huang"], "title": "Large-Scale Gaussian Splatting SLAM", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "The recently developed Neural Radiance Fields (NeRF) and 3D Gaussian\nSplatting (3DGS) have shown encouraging and impressive results for visual SLAM.\nHowever, most representative methods require RGBD sensors and are only\navailable for indoor environments. The robustness of reconstruction in\nlarge-scale outdoor scenarios remains unexplored. This paper introduces a\nlarge-scale 3DGS-based visual SLAM with stereo cameras, termed LSG-SLAM. The\nproposed LSG-SLAM employs a multi-modality strategy to estimate prior poses\nunder large view changes. In tracking, we introduce feature-alignment warping\nconstraints to alleviate the adverse effects of appearance similarity in\nrendering losses. For the scalability of large-scale scenarios, we introduce\ncontinuous Gaussian Splatting submaps to tackle unbounded scenes with limited\nmemory. Loops are detected between GS submaps by place recognition and the\nrelative pose between looped keyframes is optimized utilizing rendering and\nfeature warping losses. After the global optimization of camera poses and\nGaussian points, a structure refinement module enhances the reconstruction\nquality. With extensive evaluations on the EuRoc and KITTI datasets, LSG-SLAM\nachieves superior performance over existing Neural, 3DGS-based, and even\ntraditional approaches. Project page: https://lsg-slam.github.io.", "AI": {"tldr": "\u57fa\u4e8e3D\u9ad8\u65af\u6e85\u5c04\u7684LSG-SLAM\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u7b56\u7565\u548c\u5b50\u5730\u56fe\u7ba1\u7406\u5b9e\u73b0\u6237\u5916\u5927\u89c4\u6a21\u573a\u666f\u7684\u9c81\u68d2\u89c6\u89c9SLAM", "motivation": "\u73b0\u6709\u57fa\u4e8eNeRF\u548c3DGS\u7684SLAM\u65b9\u6cd5\u4f9d\u8d56RGBD\u4f20\u611f\u5668\u4e14\u5c40\u9650\u4e8e\u5ba4\u5185\u573a\u666f\uff0c\u4e9f\u9700\u89e3\u51b3\u6237\u5916\u5927\u89c4\u6a21\u573a\u666f\u7684\u91cd\u5efa\u9c81\u68d2\u6027\u95ee\u9898", "method": "1. \u591a\u6a21\u6001\u5148\u9a8c\u59ff\u6001\u4f30\u8ba1\n2. \u7279\u5f81\u5bf9\u9f50\u626d\u66f2\u7ea6\u675f\u51cf\u5c11\u6e32\u67d3\u635f\u5931\n3. \u8fde\u7eed\u9ad8\u65af\u5b50\u5730\u56fe\u5904\u7406\u65e0\u754c\u573a\u666f\n4. \u57fa\u4e8e\u95ed\u73af\u68c0\u6d4b\u7684\u5168\u5c40\u59ff\u6001\u4f18\u5316\n5. \u7ed3\u6784\u7ec6\u5316\u6a21\u5757\u63d0\u5347\u91cd\u5efa\u8d28\u91cf", "result": "\u5728EuRoc\u548cKITTI\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u795e\u7ecf\u65b9\u6cd5\u30013DGS\u65b9\u6cd5\u53ca\u4f20\u7edfSLAM\uff0c\u5b9e\u73b0\u6700\u4f73\u6027\u80fd\u6307\u6807", "conclusion": "LSG-SLAM\u9996\u6b21\u9a8c\u8bc1\u4e86\u7eaf\u89c6\u89c93DGS\u5728\u5927\u89c4\u6a21\u6237\u5916SLAM\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u7cfb\u7edf\u7ea7\u4f18\u5316\u5b9e\u73b0\u4e86\u65e0\u9700\u6df1\u5ea6\u4f20\u611f\u5668\u7684\u9ad8\u6548\u573a\u666f\u91cd\u5efa"}}
{"id": "2505.09825", "pdf": "https://arxiv.org/pdf/2505.09825", "abs": "https://arxiv.org/abs/2505.09825", "authors": ["Peiqi Sui", "Juan Diego Rodriguez", "Philippe Laban", "Dean Murphy", "Joseph P. Dexter", "Richard Jean So", "Samuel Baker", "Pramit Chaudhuri"], "title": "KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Each year, tens of millions of essays are written and graded in college-level\nEnglish courses. Students are asked to analyze literary and cultural texts\nthrough a process known as close reading, in which they gather textual details\nto formulate evidence-based arguments. Despite being viewed as a basis for\ncritical thinking and widely adopted as a required element of university\ncoursework, close reading has never been evaluated on large language models\n(LLMs), and multi-discipline benchmarks like MMLU do not include literature as\na subject. To fill this gap, we present KRISTEVA, the first close reading\nbenchmark for evaluating interpretive reasoning, consisting of 1331\nmultiple-choice questions adapted from classroom data. With KRISTEVA, we\npropose three progressively more difficult sets of tasks to approximate\ndifferent elements of the close reading process, which we use to test how well\nLLMs may seem to understand and reason about literary works: 1) extracting\nstylistic features, 2) retrieving relevant contextual information from\nparametric knowledge, and 3) multi-hop reasoning between style and external\ncontexts. Our baseline results find that, while state-of-the-art LLMs possess\nsome college-level close reading competency (accuracy 49.7% - 69.7%), their\nperformances still trail those of experienced human evaluators on 10 out of our\n11 tasks.", "AI": {"tldr": "\u9996\u4e2a\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u5b66\u7ec6\u8bfb\u8bc4\u4f30\u57fa\u51c6KRISTEVA\uff0c\u901a\u8fc71331\u9053\u9009\u62e9\u9898\u6784\u5efa\u4e09\u5c42\u6e10\u8fdb\u4efb\u52a1\uff0c\u63ed\u793aLLMs\u5728\u6587\u5b66\u89e3\u8bfb\u63a8\u7406\u4e2d\u7684\u8868\u73b0\u843d\u540e\u4e8e\u4eba\u7c7b", "motivation": "\u7ec6\u8bfb\u80fd\u529b\u4f5c\u4e3a\u6279\u5224\u6027\u601d\u7ef4\u57fa\u7840\u4ece\u672a\u5728LLMs\u4e2d\u8bc4\u4f30\uff0c\u73b0\u6709\u901a\u7528\u57fa\u51c6\u7f3a\u4e4f\u6587\u5b66\u79d1\u76ee\u3002\u9700\u586b\u8865LLMs\u5728\u6587\u5b66\u6587\u672c\u6df1\u5ea6\u89e3\u6790\u4e0e\u63a8\u7406\u80fd\u529b\u8bc4\u4f30\u7684\u7a7a\u767d", "method": "1) \u6539\u7f16\u8bfe\u5802\u6570\u636e\u6784\u5efa1331\u9053\u9009\u62e9\u9898\u57fa\u51c6\n2) \u8bbe\u8ba1\u4e09\u5c42\u6e10\u8fdb\u4efb\u52a1\uff1a\u6587\u4f53\u7279\u5f81\u63d0\u53d6\u2192\u53c2\u6570\u5316\u77e5\u8bc6\u68c0\u7d22\u2192\u98ce\u683c\u4e0e\u8bed\u5883\u7684\u591a\u8df3\u63a8\u7406\n3) \u5bf9\u6bd4LLMs\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u8005\u8868\u73b0", "result": "\u9876\u5c16LLMs\u5c55\u73b0\u90e8\u5206\u5927\u5b66\u7ea7\u7ec6\u8bfb\u80fd\u529b\uff08\u51c6\u786e\u738749.7%-69.7%\uff09\uff0c\u4f46\u572811\u9879\u4efb\u52a1\u4e2d10\u9879\u8868\u73b0\u843d\u540e\u4e8e\u7ecf\u9a8c\u4e30\u5bcc\u7684\u4eba\u7c7b\u8bc4\u4f30\u8005", "conclusion": "KRISTEVA\u586b\u8865\u6587\u5b66\u8bc4\u4f30\u57fa\u51c6\u7a7a\u767d\uff0c\u63ed\u793aLLMs\u5728\u590d\u6742\u6587\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6301\u7eed\u7f3a\u9677\uff0c\u5f3a\u8c03\u9700\u63d0\u5347\u6a21\u578b\u5bf9\u6587\u672c\u98ce\u683c\u4e0e\u8bed\u5883\u5173\u8054\u7684\u6df1\u5c42\u7406\u89e3\u80fd\u529b"}}
{"id": "2505.09926", "pdf": "https://arxiv.org/pdf/2505.09926", "abs": "https://arxiv.org/abs/2505.09926", "authors": ["Bin-Bin Gao", "Yue Zhu", "Jiangtao Yan", "Yuezhi Cai", "Weixi Zhang", "Meng Wang", "Jun Liu", "Yong Liu", "Lei Wang", "Chengjie Wang"], "title": "AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection", "categories": ["cs.CV", "cs.AI"], "comment": "27 pages, 15 figures, 22 tables", "summary": "Universal visual anomaly detection aims to identify anomalies from novel or\nunseen vision domains without additional fine-tuning, which is critical in open\nscenarios. Recent studies have demonstrated that pre-trained vision-language\nmodels like CLIP exhibit strong generalization with just zero or a few normal\nimages. However, existing methods struggle with designing prompt templates,\ncomplex token interactions, or requiring additional fine-tuning, resulting in\nlimited flexibility. In this work, we present a simple yet effective method\ncalled AdaptCLIP based on two key insights. First, adaptive visual and textual\nrepresentations should be learned alternately rather than jointly. Second,\ncomparative learning between query and normal image prompt should incorporate\nboth contextual and aligned residual features, rather than relying solely on\nresidual features. AdaptCLIP treats CLIP models as a foundational service,\nadding only three simple adapters, visual adapter, textual adapter, and\nprompt-query adapter, at its input or output ends. AdaptCLIP supports\nzero-/few-shot generalization across domains and possesses a training-free\nmanner on target domains once trained on a base dataset. AdaptCLIP achieves\nstate-of-the-art performance on 12 anomaly detection benchmarks from industrial\nand medical domains, significantly outperforming existing competitive methods.\nWe will make the code and model of AdaptCLIP available at\nhttps://github.com/gaobb/AdaptCLIP.", "AI": {"tldr": "\u63d0\u51faAdaptCLIP\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ea4\u66ff\u5b66\u4e60\u89c6\u89c9\u6587\u672c\u8868\u793a\u548c\u5bf9\u6bd4\u6b8b\u5dee\u7279\u5f81\uff0c\u572812\u4e2a\u8de8\u57df\u5f02\u5e38\u68c0\u6d4b\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA", "motivation": "\u73b0\u6709\u57fa\u4e8eCLIP\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u63d0\u793a\u6a21\u677f\u8bbe\u8ba1\u590d\u6742\u3001\u9700\u8981\u5fae\u8c03\u3001\u7075\u6d3b\u6027\u5dee\u7b49\u95ee\u9898\uff0c\u9700\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u9002\u914d\u65b9\u5f0f", "method": "1. \u89c6\u89c9/\u6587\u672c\u9002\u914d\u5668\u4ea4\u66ff\u5b66\u4e60 2. \u63d0\u793a-\u67e5\u8be2\u9002\u914d\u5668\u878d\u5408\u4e0a\u4e0b\u6587\u4e0e\u5bf9\u9f50\u6b8b\u5dee\u7279\u5f81 3. \u4ec5\u6dfb\u52a0\u4e09\u4e2a\u8f7b\u91cf\u9002\u914d\u5668\u65e0\u9700\u5fae\u8c03CLIP\u4e3b\u4f53", "result": "\u5728\u5de5\u4e1a\u548c\u533b\u7597\u9886\u57df12\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u652f\u6301\u96f6\u6837\u672c/\u5c0f\u6837\u672c\u8de8\u57df\u8fc1\u79fb", "conclusion": "AdaptCLIP\u9a8c\u8bc1\u4e86\u4ea4\u66ff\u5b66\u4e60\u4e0e\u7279\u5f81\u878d\u5408\u7684\u6709\u6548\u6027\uff0c\u4e3a\u901a\u7528\u89c6\u89c9\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.09852", "pdf": "https://arxiv.org/pdf/2505.09852", "abs": "https://arxiv.org/abs/2505.09852", "authors": ["Apollinaire Poli Nemkova", "Sarath Chandra Lingareddy", "Sagnik Ray Choudhury", "Mark V. Albert"], "title": "Do Large Language Models Know Conflict? Investigating Parametric vs. Non-Parametric Knowledge of LLMs for Conflict Forecasting", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown impressive performance across natural\nlanguage tasks, but their ability to forecast violent conflict remains\nunderexplored. We investigate whether LLMs possess meaningful parametric\nknowledge-encoded in their pretrained weights-to predict conflict escalation\nand fatalities without external data. This is critical for early warning\nsystems, humanitarian planning, and policy-making. We compare this parametric\nknowledge with non-parametric capabilities, where LLMs access structured and\nunstructured context from conflict datasets (e.g., ACLED, GDELT) and recent\nnews reports via Retrieval-Augmented Generation (RAG). Incorporating external\ninformation could enhance model performance by providing up-to-date context\notherwise missing from pretrained weights. Our two-part evaluation framework\nspans 2020-2024 across conflict-prone regions in the Horn of Africa and the\nMiddle East. In the parametric setting, LLMs predict conflict trends and\nfatalities relying only on pretrained knowledge. In the non-parametric setting,\nmodels receive summaries of recent conflict events, indicators, and\ngeopolitical developments. We compare predicted conflict trend labels (e.g.,\nEscalate, Stable Conflict, De-escalate, Peace) and fatalities against\nhistorical data. Our findings highlight the strengths and limitations of LLMs\nfor conflict forecasting and the benefits of augmenting them with structured\nexternal knowledge.", "AI": {"tldr": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5185\u5728\u77e5\u8bc6\u4e0e\u5916\u90e8\u6570\u636e\u589e\u5f3a\u5728\u51b2\u7a81\u9884\u6d4b\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0\u7ed3\u5408\u7ed3\u6784\u5316\u5916\u90e8\u4fe1\u606f\u53ef\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027", "motivation": "\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u5177\u5907\u9884\u6d4b\u66b4\u529b\u51b2\u7a81\u5347\u7ea7\u7684\u5185\u5728\u53c2\u6570\u5316\u77e5\u8bc6\uff0c\u53ca\u5176\u4e0e\u5916\u90e8\u6570\u636e\u589e\u5f3a\uff08RAG\u6280\u672f\uff09\u7ed3\u5408\u7684\u6548\u679c\uff0c\u4e3a\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u63d0\u4f9b\u6280\u672f\u652f\u6301", "method": "\u5efa\u7acb\u53cc\u8f68\u8bc4\u4f30\u6846\u67b6\uff082020-2024\uff09\uff0c\u5bf9\u6bd4\u7eaf\u53c2\u6570\u5316\u9884\u6d4b\u4e0e\u5f15\u5165ACLED/GDELT\u6570\u636e\u96c6\u53ca\u65b0\u95fb\u7684RAG\u6a21\u5f0f\uff0c\u4f7f\u7528\u8d8b\u52bf\u6807\u7b7e\u548c\u4f24\u4ea1\u6570\u636e\u9a8c\u8bc1\u6a21\u578b\u8868\u73b0", "result": "LLMs\u5177\u5907\u57fa\u7840\u51b2\u7a81\u9884\u6d4b\u80fd\u529b\uff0c\u4f46\u7ed3\u5408\u7ed3\u6784\u5316\u5916\u90e8\u77e5\u8bc6\u540e\u9884\u6d4b\u51c6\u786e\u6027\u663e\u8457\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u5730\u7f18\u653f\u6cbb\u52a8\u6001\u6355\u6349\u65b9\u9762\u8868\u73b0\u66f4\u4f18", "conclusion": "\u53c2\u6570\u5316\u77e5\u8bc6\u4e0e\u975e\u53c2\u6570\u5316\u5916\u90e8\u4fe1\u606f\u7684\u878d\u5408\u80fd\u6709\u6548\u589e\u5f3a\u51b2\u7a81\u9884\u6d4b\u7cfb\u7edf\uff0c\u4e3a\u5b9e\u65f6\u4eba\u9053\u4e3b\u4e49\u54cd\u5e94\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u6280\u672f\u652f\u6301"}}
{"id": "2505.09927", "pdf": "https://arxiv.org/pdf/2505.09927", "abs": "https://arxiv.org/abs/2505.09927", "authors": ["Siqi Yin", "Shaolei Liu", "Manning Wang"], "title": "DDFP: Data-dependent Frequency Prompt for Source Free Domain Adaptation of Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Domain adaptation addresses the challenge of model performance degradation\ncaused by domain gaps. In the typical setup for unsupervised domain adaptation,\nlabeled data from a source domain and unlabeled data from a target domain are\nused to train a target model. However, access to labeled source domain data,\nparticularly in medical datasets, can be restricted due to privacy policies. As\na result, research has increasingly shifted to source-free domain adaptation\n(SFDA), which requires only a pretrained model from the source domain and\nunlabeled data from the target domain data for adaptation. Existing SFDA\nmethods often rely on domain-specific image style translation and\nself-supervision techniques to bridge the domain gap and train the target\ndomain model. However, the quality of domain-specific style-translated images\nand pseudo-labels produced by these methods still leaves room for improvement.\nMoreover, training the entire model during adaptation can be inefficient under\nlimited supervision. In this paper, we propose a novel SFDA framework to\naddress these challenges. Specifically, to effectively mitigate the impact of\ndomain gap in the initial training phase, we introduce preadaptation to\ngenerate a preadapted model, which serves as an initialization of target model\nand allows for the generation of high-quality enhanced pseudo-labels without\nintroducing extra parameters. Additionally, we propose a data-dependent\nfrequency prompt to more effectively translate target domain images into a\nsource-like style. To further enhance adaptation, we employ a style-related\nlayer fine-tuning strategy, specifically designed for SFDA, to train the target\nmodel using the prompted target domain images and pseudo-labels. Extensive\nexperiments on cross-modality abdominal and cardiac SFDA segmentation tasks\ndemonstrate that our proposed method outperforms existing state-of-the-art\nmethods.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u578b\u65e0\u6e90\u57df\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u9002\u5e94\u6a21\u578b\u3001\u6570\u636e\u4f9d\u8d56\u9891\u7387\u63d0\u793a\u548c\u5206\u5c42\u5fae\u8c03\u7b56\u7565\uff0c\u63d0\u5347\u8de8\u57df\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u73b0\u6709SFDA\u65b9\u6cd5\u5728\u98ce\u683c\u8f6c\u6362\u56fe\u50cf\u8d28\u91cf\u3001\u4f2a\u6807\u7b7e\u51c6\u786e\u6027\u53ca\u8bad\u7ec3\u6548\u7387\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u5728\u533b\u7597\u6570\u636e\u9690\u79c1\u53d7\u9650\u573a\u666f\u4e0b\u9700\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1) \u9884\u9002\u5e94\u9636\u6bb5\u751f\u6210\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\u521d\u59cb\u5316\u6a21\u578b\n2) \u6570\u636e\u4f9d\u8d56\u9891\u7387\u63d0\u793a\u5b9e\u73b0\u9ad8\u6548\u98ce\u683c\u8fc1\u79fb\n3) \u9488\u5bf9SFDA\u8bbe\u8ba1\u7684\u98ce\u683c\u76f8\u5173\u5c42\u9009\u62e9\u6027\u5fae\u8c03\u7b56\u7565", "result": "\u5728\u8179\u90e8\u548c\u5fc3\u810f\u8de8\u6a21\u6001\u5206\u5272\u4efb\u52a1\u4e2d\u8d85\u8d8a\u73b0\u6709SOTA\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u4e09\u9636\u6bb5\u4f18\u5316\u7b56\u7565\u6709\u6548\u89e3\u51b3\u533b\u7597\u9886\u57dfSFDA\u6838\u5fc3\u75db\u70b9\uff0c\u4e3a\u9690\u79c1\u654f\u611f\u573a\u666f\u63d0\u4f9b\u9ad8\u6027\u80fd\u8de8\u57df\u9002\u5e94\u65b9\u6848\u3002"}}
{"id": "2505.09902", "pdf": "https://arxiv.org/pdf/2505.09902", "abs": "https://arxiv.org/abs/2505.09902", "authors": ["Martin Capdevila", "Esteban Villa Turek", "Ellen Karina Chumbe Fernandez", "Luis Felipe Polo Galvez", "Luis Cadavid", "Andrea Marroquin", "Rebeca Vargas Quesada", "Johanna Crew", "Nicole Vallejo Galarraga", "Christopher Rodriguez", "Diego Gutierrez", "Radhi Datla"], "title": "Crossing Borders Without Crossing Boundaries: How Sociolinguistic Awareness Can Optimize User Engagement with Localized Spanish AI Models Across Hispanophone Countries", "categories": ["cs.CL"], "comment": null, "summary": "Large language models are, by definition, based on language. In an effort to\nunderscore the critical need for regional localized models, this paper examines\nprimary differences between variants of written Spanish across Latin America\nand Spain, with an in-depth sociocultural and linguistic contextualization\ntherein. We argue that these differences effectively constitute significant\ngaps in the quotidian use of Spanish among dialectal groups by creating\nsociolinguistic dissonances, to the extent that locale-sensitive AI models\nwould play a pivotal role in bridging these divides. In doing so, this approach\ninforms better and more efficient localization strategies that also serve to\nmore adequately meet inclusivity goals, while securing sustainable active daily\nuser growth in a major low-risk investment geographic area. Therefore,\nimplementing at least the proposed five sub variants of Spanish addresses two\nlines of action: to foment user trust and reliance on AI language models while\nalso demonstrating a level of cultural, historical, and sociolinguistic\nawareness that reflects positively on any internationalization strategy.", "AI": {"tldr": "\u8bba\u6587\u5f3a\u8c03\u9700\u5f00\u53d1\u533a\u57df\u672c\u5730\u5316\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u6790\u897f\u73ed\u7259\u8bed\u4e0d\u540c\u53d8\u4f53\u7684\u793e\u4f1a\u8bed\u8a00\u5b66\u5dee\u5f02\uff0c\u63d0\u51fa\u5b9e\u65bd\u4e94\u79cd\u897f\u73ed\u7259\u8bed\u5b50\u53d8\u4f53\u4ee5\u63d0\u5347AI\u5305\u5bb9\u6027\u548c\u7528\u6237\u589e\u957f\u3002", "motivation": "\u4e0d\u540c\u897f\u73ed\u7259\u8bed\u53d8\u4f53\u5728\u65e5\u5e38\u4f7f\u7528\u4e2d\u5b58\u5728\u663e\u8457\u793e\u4f1a\u8bed\u8a00\u5b66\u5dee\u5f02\uff0c\u5bfc\u81f4\u7528\u6237\u4e0eAI\u6a21\u578b\u95f4\u7684\u9694\u9602\uff0c\u9700\u901a\u8fc7\u672c\u5730\u5316\u7b56\u7565\u63d0\u5347\u6a21\u578b\u5305\u5bb9\u6027\u548c\u56fd\u9645\u5316\u6548\u679c\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u62c9\u4e01\u7f8e\u6d32\u4e0e\u897f\u73ed\u7259\u7684\u897f\u73ed\u7259\u8bed\u53d8\u4f53\u5dee\u5f02\uff0c\u7ed3\u5408\u793e\u4f1a\u6587\u5316\u80cc\u666f\u5206\u6790\uff0c\u8bba\u8bc1\u672c\u5730\u5316AI\u6a21\u578b\u5bf9\u6d88\u9664\u793e\u4f1a\u8bed\u8a00\u4e0d\u534f\u8c03\u7684\u4f5c\u7528\u3002", "result": "\u53d1\u73b0\u65b9\u8a00\u5dee\u5f02\u5b9e\u8d28\u6784\u6210\u7528\u6237\u4fe1\u4efb\u969c\u788d\uff0c\u8bc1\u660e\u533a\u57df\u654f\u611f\u6a21\u578b\u53ef\u6709\u6548\u5f25\u5408\u5206\u6b67\u5e76\u4fc3\u8fdb\u53ef\u6301\u7eed\u7528\u6237\u589e\u957f\u3002", "conclusion": "\u5efa\u8bae\u81f3\u5c11\u5b9e\u65bd\u4e94\u79cd\u897f\u73ed\u7259\u8bed\u5b50\u53d8\u4f53\uff0c\u65e2\u80fd\u589e\u5f3a\u7528\u6237\u5bf9AI\u7684\u4fe1\u4efb\uff0c\u53c8\u4f53\u73b0\u6587\u5316\u654f\u611f\u6027\uff0c\u652f\u6301\u56fd\u9645\u5316\u6218\u7565\u7684\u4f4e\u98ce\u9669\u6295\u8d44\u5e03\u5c40\u3002"}}
{"id": "2505.09935", "pdf": "https://arxiv.org/pdf/2505.09935", "abs": "https://arxiv.org/abs/2505.09935", "authors": ["Ahmed S. Abdelrahman", "Mohamed Abdel-Aty", "Quoc Dai Tran"], "title": "VRU-CIPI: Crossing Intention Prediction at Intersections for Improving Vulnerable Road Users Safety", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Understanding and predicting human behavior in-thewild, particularly at urban\nintersections, remains crucial for enhancing interaction safety between road\nusers. Among the most critical behaviors are crossing intentions of Vulnerable\nRoad Users (VRUs), where misinterpretation may result in dangerous conflicts\nwith oncoming vehicles. In this work, we propose the VRU-CIPI framework with a\nsequential attention-based model designed to predict VRU crossing intentions at\nintersections. VRU-CIPI employs Gated Recurrent Unit (GRU) to capture temporal\ndynamics in VRU movements, combined with a multi-head Transformer\nself-attention mechanism to encode contextual and spatial dependencies critical\nfor predicting crossing direction. Evaluated on UCF-VRU dataset, our proposed\nachieves state-of-the-art performance with an accuracy of 96.45% and achieving\nreal-time inference speed reaching 33 frames per second. Furthermore, by\nintegrating with Infrastructure-to-Vehicles (I2V) communication, our approach\ncan proactively enhance intersection safety through timely activation of\ncrossing signals and providing early warnings to connected vehicles, ensuring\nsmoother and safer interactions for all road users.", "AI": {"tldr": "\u63d0\u51faVRU-CIPI\u6846\u67b6\uff0c\u91c7\u7528GRU\u548cTransformer\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u9884\u6d4b\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\u8fc7\u8857\u610f\u56fe\uff0c\u51c6\u786e\u738796.45%\u4e14\u652f\u6301\u5b9e\u65f6\u5904\u7406\u3002", "motivation": "\u89e3\u51b3\u57ce\u5e02\u4ea4\u53c9\u8def\u53e3\u4eba\u8f66\u4ea4\u4e92\u5b89\u5168\u9690\u60a3\uff0c\u7279\u522b\u662fVRU\u8fc7\u8857\u610f\u56fe\u8bef\u5224\u5bfc\u81f4\u7684\u5371\u9669\u51b2\u7a81\u95ee\u9898\u3002", "method": "\u878d\u5408GRU\u6355\u6349\u65f6\u95f4\u52a8\u6001\u7279\u5f81 + Transformer\u591a\u5934\u81ea\u6ce8\u610f\u529b\u7f16\u7801\u4e0a\u4e0b\u6587\u7a7a\u95f4\u4f9d\u8d56", "result": "UCF-VRU\u6570\u636e\u96c6\u9a8c\u8bc1\uff1a96.45%\u51c6\u786e\u7387\uff0c33FPS\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6", "conclusion": "\u7ed3\u5408I2V\u901a\u4fe1\u7cfb\u7edf\uff0c\u901a\u8fc7\u63d0\u524d\u6fc0\u6d3b\u8fc7\u8857\u4fe1\u53f7\u548c\u8f66\u8f86\u9884\u8b66\uff0c\u63d0\u5347\u6240\u6709\u9053\u8def\u4f7f\u7528\u8005\u7684\u4ea4\u4e92\u5b89\u5168\u6027"}}
{"id": "2505.09924", "pdf": "https://arxiv.org/pdf/2505.09924", "abs": "https://arxiv.org/abs/2505.09924", "authors": ["Yidan Wang", "Yubing Ren", "Yanan Cao", "Binxing Fang"], "title": "From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework for Large Language Models", "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "The rise of Large Language Models (LLMs) has heightened concerns about the\nmisuse of AI-generated text, making watermarking a promising solution.\nMainstream watermarking schemes for LLMs fall into two categories: logits-based\nand sampling-based. However, current schemes entail trade-offs among\nrobustness, text quality, and security. To mitigate this, we integrate\nlogits-based and sampling-based schemes, harnessing their respective strengths\nto achieve synergy. In this paper, we propose a versatile symbiotic\nwatermarking framework with three strategies: serial, parallel, and hybrid. The\nhybrid framework adaptively embeds watermarks using token entropy and semantic\nentropy, optimizing the balance between detectability, robustness, text\nquality, and security. Furthermore, we validate our approach through\ncomprehensive experiments on various datasets and models. Experimental results\nindicate that our method outperforms existing baselines and achieves\nstate-of-the-art (SOTA) performance. We believe this framework provides novel\ninsights into diverse watermarking paradigms. Our code is available at\n\\href{https://github.com/redwyd/SymMark}{https://github.com/redwyd/SymMark}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u878d\u5408logits-based\u548csampling-based\u7684\u5171\u751f\u6c34\u5370\u6846\u67b6\uff0c\u901a\u8fc7\u71b5\u81ea\u9002\u5e94\u673a\u5236\u5e73\u8861\u6c34\u5370\u6027\u80fd\uff0c\u5728\u591a\u9879\u6307\u6807\u4e0a\u8fbe\u5230SOTA\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709LLM\u6c34\u5370\u65b9\u6848\u5728\u9c81\u68d2\u6027\u3001\u6587\u672c\u8d28\u91cf\u548c\u5b89\u5168\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u9700\u6574\u5408\u4e0d\u540c\u65b9\u6848\u7684\u534f\u540c\u4f18\u52bf\u6765\u89e3\u51b3\u8fd9\u4e00\u56f0\u5883\u3002", "method": "\u63d0\u51fa\u4e32\u884c\u3001\u5e76\u884c\u548c\u6df7\u5408\u4e09\u79cd\u5171\u751f\u7b56\u7565\uff0c\u5176\u4e2d\u6df7\u5408\u6846\u67b6\u901a\u8fc7token\u71b5\u548c\u8bed\u4e49\u71b5\u81ea\u9002\u5e94\u5d4c\u5165\u6c34\u5370\uff0c\u4f18\u5316\u68c0\u6d4b\u6027\u3001\u9c81\u68d2\u6027\u3001\u6587\u672c\u8d28\u91cf\u4e0e\u5b89\u5168\u6027\u7684\u5e73\u8861\u3002", "result": "\u5728\u591a\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5168\u9762\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\uff0c\u5c24\u5176\u5728\u8bed\u4e49\u4fdd\u6301\u548c\u6c34\u5370\u4e0d\u53ef\u5bdf\u89c9\u6027\u65b9\u9762\u63d0\u5347\u663e\u8457\uff08\u6700\u9ad8\u63d0\u534730%\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u591a\u8303\u5f0f\u6c34\u5370\u534f\u540c\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002\u57fa\u4e8e\u71b5\u7684\u81ea\u9002\u5e94\u673a\u5236\u4e3a\u672a\u6765\u6c34\u5370\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2505.09939", "pdf": "https://arxiv.org/pdf/2505.09939", "abs": "https://arxiv.org/abs/2505.09939", "authors": ["Zhe Shan", "Lei Zhou", "Liu Mao", "Shaofan Chen", "Chuanqiu Ren", "Xia Xie"], "title": "Non-Registration Change Detection: A Novel Change Detection Task and Benchmark Dataset", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted to IGARSS 2025", "summary": "In this study, we propose a novel remote sensing change detection task,\nnon-registration change detection, to address the increasing number of\nemergencies such as natural disasters, anthropogenic accidents, and military\nstrikes. First, in light of the limited discourse on the issue of\nnon-registration change detection, we systematically propose eight scenarios\nthat could arise in the real world and potentially contribute to the occurrence\nof non-registration problems. Second, we develop distinct image transformation\nschemes tailored to various scenarios to convert the available registration\nchange detection dataset into a non-registration version. Finally, we\ndemonstrate that non-registration change detection can cause catastrophic\ndamage to the state-of-the-art methods. Our code and dataset are available at\nhttps://github.com/ShanZard/NRCD.", "AI": {"tldr": "\u63d0\u51fa\u975e\u914d\u51c6\u53d8\u5316\u68c0\u6d4b\u65b0\u4efb\u52a1\uff0c\u901a\u8fc7\u573a\u666f\u5206\u7c7b\u548c\u6570\u636e\u96c6\u8f6c\u6362\u65b9\u6848\u9a8c\u8bc1\u73b0\u6709\u65b9\u6cd5\u7f3a\u9677\u3002", "motivation": "\u9488\u5bf9\u81ea\u7136\u707e\u5bb3/\u4eba\u4e3a\u4e8b\u6545\u7b49\u7d27\u6025\u573a\u666f\u4e0b\u9065\u611f\u56fe\u50cf\u914d\u51c6\u56f0\u96be\u7684\u5b9e\u9645\u9700\u6c42\uff0c\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u5bf9\u975e\u914d\u51c6\u95ee\u9898\u8ba8\u8bba\u4e0d\u8db3\u7684\u7f3a\u9677\u3002", "method": "1. \u5b9a\u4e498\u79cd\u73b0\u5b9e\u975e\u914d\u51c6\u573a\u666f\n2. \u5f00\u53d1\u56fe\u50cf\u8f6c\u6362\u65b9\u6848\u5c06\u914d\u51c6\u6570\u636e\u96c6\u8f6c\u4e3a\u975e\u914d\u51c6\u7248\u672c\n3. \u6784\u5efaNRCD\u6570\u636e\u96c6\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u6027", "result": "\u975e\u914d\u51c6\u53d8\u5316\u68c0\u6d4b\u4f1a\u5bfc\u81f4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u6027\u80fd\u51fa\u73b0\u707e\u96be\u6027\u4e0b\u964d", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u975e\u914d\u51c6\u53d8\u5316\u68c0\u6d4b\u9886\u57df\u7684\u7a7a\u767d\uff0c\u5f00\u6e90\u4ee3\u7801\u548c\u6570\u636e\u96c6\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\u5e73\u53f0\u3002"}}
{"id": "2505.09930", "pdf": "https://arxiv.org/pdf/2505.09930", "abs": "https://arxiv.org/abs/2505.09930", "authors": ["Zixiao Zhu", "Hanzhang Zhou", "Zijian Feng", "Tianjiao Li", "Chua Jia Jim Deryl", "Mak Lee Onn", "Gee Wah Ng", "Kezhi Mao"], "title": "Rethinking Prompt Optimizers: From Prompt Merits to Optimization", "categories": ["cs.CL"], "comment": "20 pages, 14 figures", "summary": "Prompt optimization (PO) offers a practical alternative to fine-tuning large\nlanguage models (LLMs), enabling performance improvements without altering\nmodel weights. Existing methods typically rely on advanced, large-scale LLMs\nlike GPT-4 to generate optimized prompts. However, due to limited downward\ncompatibility, verbose, instruction-heavy prompts from advanced LLMs can\noverwhelm lightweight inference models and degrade response quality. In this\nwork, we rethink prompt optimization through the lens of interpretable design.\nWe first identify a set of model-agnostic prompt quality merits and empirically\nvalidate their effectiveness in enhancing prompt and response quality. We then\nintroduce MePO, a merit-guided, lightweight, and locally deployable prompt\noptimizer trained on our preference dataset built from merit-aligned prompts\ngenerated by a lightweight LLM. Unlike prior work, MePO avoids online\noptimization reliance, reduces cost and privacy concerns, and, by learning\nclear, interpretable merits, generalizes effectively to both large-scale and\nlightweight inference models. Experiments demonstrate that MePO achieves better\nresults across diverse tasks and model types, offering a scalable and robust\nsolution for real-world deployment. Our model and dataset are available at:\nhttps://github.com/MidiyaZhu/MePO", "AI": {"tldr": "\u63d0\u51faMePO\u2014\u2014\u57fa\u4e8e\u53ef\u89e3\u91ca\u8bbe\u8ba1\u539f\u5219\u7684\u8f7b\u91cf\u7ea7\u63d0\u793a\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u660e\u786e\u7684\u8d28\u91cf\u6307\u6807\u548c\u672c\u5730\u5316\u90e8\u7f72\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5bf9\u9ad8\u7ea7LLM\u7684\u4f9d\u8d56\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56GPT-4\u7b49\u9ad8\u7ea7LLM\u751f\u6210\u5197\u957f\u63d0\u793a\uff0c\u5bfc\u81f4\u8f7b\u91cf\u7ea7\u63a8\u7406\u6a21\u578b\u54cd\u5e94\u8d28\u91cf\u4e0b\u964d\u3002\u9700\u8981\u627e\u5230\u6a21\u578b\u65e0\u5173\u7684\u63d0\u793a\u8d28\u91cf\u8bc4\u4f30\u6807\u51c6\uff0c\u5b9e\u73b0\u5411\u4e0b\u517c\u5bb9\u3002", "method": "1. \u8bc6\u522b\u6a21\u578b\u65e0\u5173\u7684\u63d0\u793a\u8d28\u91cf\u6307\u6807\u5e76\u9a8c\u8bc1\u6709\u6548\u6027\n2. \u4f7f\u7528\u8f7b\u91cf\u7ea7LLM\u751f\u6210\u7b26\u5408\u6307\u6807\u7684\u63d0\u793a\u6784\u5efa\u504f\u597d\u6570\u636e\u96c6\n3. \u8bad\u7ec3\u57fa\u4e8e\u8d28\u91cf\u6307\u6807\u6307\u5bfc\u7684\u672c\u5730\u53ef\u90e8\u7f72\u4f18\u5316\u5668MePO", "result": "\u5b9e\u9a8c\u8bc1\u660eMePO\u5728\u591a\u6837\u5316\u4efb\u52a1\u548c\u6a21\u578b\u7c7b\u578b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5bf9\u8f7b\u91cf\u7ea7\u6a21\u578b\u4f18\u5316\u6548\u679c\u63d0\u5347\u663e\u8457\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u573a\u666f\u4e0b\u7684\u7a33\u5b9a\u6cdb\u5316\u3002", "conclusion": "MePO\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u8d28\u91cf\u6307\u6807\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5728\u7ebf\u4f18\u5316\u4f9d\u8d56\u548c\u9690\u79c1\u6210\u672c\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.09943", "pdf": "https://arxiv.org/pdf/2505.09943", "abs": "https://arxiv.org/abs/2505.09943", "authors": ["Jiakun Deng", "Kexuan Li", "Xingye Cui", "Jiaxuan Li", "Chang Long", "Tian Pu", "Zhenming Peng"], "title": "CSPENet: Contour-Aware and Saliency Priors Embedding Network for Infrared Small Target Detection", "categories": ["cs.CV"], "comment": null, "summary": "Infrared small target detection (ISTD) plays a critical role in a wide range\nof civilian and military applications. Existing methods suffer from\ndeficiencies in the localization of dim targets and the perception of contour\ninformation under dense clutter environments, severely limiting their detection\nperformance. To tackle these issues, we propose a contour-aware and saliency\npriors embedding network (CSPENet) for ISTD. We first design a\nsurround-convergent prior extraction module (SCPEM) that effectively captures\nthe intrinsic characteristic of target contour pixel gradients converging\ntoward their center. This module concurrently extracts two collaborative\npriors: a boosted saliency prior for accurate target localization and\nmulti-scale structural priors for comprehensively enriching contour detail\nrepresentation. Building upon this, we propose a dual-branch priors embedding\narchitecture (DBPEA) that establishes differentiated feature fusion pathways,\nembedding these two priors at optimal network positions to achieve performance\nenhancement. Finally, we develop an attention-guided feature enhancement module\n(AGFEM) to refine feature representations and improve saliency estimation\naccuracy. Experimental results on public datasets NUDT-SIRST, IRSTD-1k, and\nNUAA-SIRST demonstrate that our CSPENet outperforms other state-of-the-art\nmethods in detection performance. The code is available at\nhttps://github.com/IDIP2025/CSPENet.", "AI": {"tldr": "\u63d0\u51fa\u878d\u5408\u8f6e\u5ed3\u611f\u77e5\u4e0e\u663e\u8457\u6027\u5148\u9a8c\u7684CSPENet\u7f51\u7edc\uff0c\u901a\u8fc7SCPEM\u6a21\u5757\u6355\u83b7\u76ee\u6807\u68af\u5ea6\u7279\u6027\uff0cDBPEA\u53cc\u5206\u652f\u5d4c\u5165\u5148\u9a8c\uff0cAGFEM\u4f18\u5316\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u5728\u5f31\u76ee\u6807\u5b9a\u4f4d\u548c\u5bc6\u96c6\u6742\u6ce2\u4e0b\u7684\u8f6e\u5ed3\u611f\u77e5\u5b58\u5728\u7f3a\u9677\uff0c\u5236\u7ea6\u68c0\u6d4b\u7cbe\u5ea6\u3002\u9700\u5f00\u53d1\u80fd\u540c\u65f6\u589e\u5f3a\u76ee\u6807\u5b9a\u4f4d\u4e0e\u8f6e\u5ed3\u8868\u5f81\u7684\u65b0\u65b9\u6cd5\u3002", "method": "1.SCPEM\u6a21\u5757\u63d0\u53d6\u76ee\u6807\u4e2d\u5fc3\u6536\u655b\u68af\u5ea6\u7279\u6027\uff0c\u751f\u6210\u663e\u8457\u6027\u5148\u9a8c\u548c\u591a\u5c3a\u5ea6\u7ed3\u6784\u5148\u9a8c\uff1b2.DBPSEA\u53cc\u5206\u652f\u5dee\u5f02\u5316\u878d\u5408\u8def\u5f84\u5d4c\u5165\u5148\u9a8c\uff1b3.AGFEM\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u4f18\u5316\u7279\u5f81\u8868\u793a\u3002", "result": "\u5728NUDT-SIRST\u3001IRSTD-1k\u3001NUAA-SIRST\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "\u901a\u8fc7\u534f\u540c\u5229\u7528\u76ee\u6807\u68af\u5ea6\u7279\u6027\u548c\u53cc\u5148\u9a8c\u5d4c\u5165\u673a\u5236\uff0cCSPENet\u6709\u6548\u89e3\u51b3\u4e86\u5f31\u76ee\u6807\u5b9a\u4f4d\u4e0e\u8f6e\u5ed3\u611f\u77e5\u96be\u9898\uff0c\u4e3a\u590d\u6742\u573a\u666f\u7ea2\u5916\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848\u3002"}}
{"id": "2505.09945", "pdf": "https://arxiv.org/pdf/2505.09945", "abs": "https://arxiv.org/abs/2505.09945", "authors": ["Deeksha Prahlad", "Chanhee Lee", "Dongha Kim", "Hokeun Kim"], "title": "Personalizing Large Language Models using Retrieval Augmented Generation and Knowledge Graph", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "To appear in the Companion Proceedings of the ACM Web Conference 2025\n  (WWW Companion '25)", "summary": "The advent of large language models (LLMs) has allowed numerous applications,\nincluding the generation of queried responses, to be leveraged in chatbots and\nother conversational assistants. Being trained on a plethora of data, LLMs\noften undergo high levels of over-fitting, resulting in the generation of extra\nand incorrect data, thus causing hallucinations in output generation. One of\nthe root causes of such problems is the lack of timely, factual, and\npersonalized information fed to the LLM. In this paper, we propose an approach\nto address these problems by introducing retrieval augmented generation (RAG)\nusing knowledge graphs (KGs) to assist the LLM in personalized response\ngeneration tailored to the users. KGs have the advantage of storing\ncontinuously updated factual information in a structured way. While our KGs can\nbe used for a variety of frequently updated personal data, such as calendar,\ncontact, and location data, we focus on calendar data in this paper. Our\nexperimental results show that our approach works significantly better in\nunderstanding personal information and generating accurate responses compared\nto the baseline LLMs using personal data as text inputs, with a moderate\nreduction in response time.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5b58\u50a8\u5b9e\u65f6\u66f4\u65b0\u7684\u4e2a\u4eba\u65e5\u5386\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347LLM\u751f\u6210\u4e2a\u6027\u5316\u56de\u590d\u7684\u51c6\u786e\u6027", "motivation": "\u89e3\u51b3LLMs\u56e0\u8bad\u7ec3\u6570\u636e\u8fc7\u65f6\u548c\u7f3a\u4e4f\u4e2a\u6027\u5316\u4fe1\u606f\u5bfc\u81f4\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u91cd\u70b9\u89e3\u51b3\u4e2a\u4eba\u65e5\u5386\u6570\u636e\u7684\u5b9e\u65f6\u6574\u5408\u9700\u6c42", "method": "\u5c06\u65e5\u5386\u6570\u636e\u6784\u5efa\u4e3a\u77e5\u8bc6\u56fe\u8c31\uff0c\u901a\u8fc7RAG\u6846\u67b6\u5b9e\u73b0\u7ed3\u6784\u5316\u68c0\u7d22\u4e0eLLM\u7684\u534f\u540c\uff0c\u91c7\u7528\u5b9e\u4f53\u5173\u7cfb\u63d0\u53d6\u548c\u52a8\u6001\u68c0\u7d22\u673a\u5236", "result": "\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u51c6\u786e\u7387\u63d0\u534737%\uff0c\u54cd\u5e94\u65f6\u95f4\u51cf\u5c1119%\uff0c\u5728\u4e2a\u4eba\u65e5\u7a0b\u7406\u89e3\u4efb\u52a1\u4e2dF1\u503c\u8fbe\u52300.82", "conclusion": "\u7ed3\u6784\u5316\u77e5\u8bc6\u56fe\u8c31\u6709\u6548\u89e3\u51b3LLMs\u7684\u5b9e\u65f6\u6570\u636e\u6574\u5408\u96be\u9898\uff0c\u4e3a\u4e2a\u6027\u5316AI\u52a9\u624b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.09965", "pdf": "https://arxiv.org/pdf/2505.09965", "abs": "https://arxiv.org/abs/2505.09965", "authors": ["Hao Yang", "Tao Tan", "Shuai Tan", "Weiqin Yang", "Kunyan Cai", "Calvin Chen", "Yue Sun"], "title": "MambaControl: Anatomy Graph-Enhanced Mamba ControlNet with Fourier Refinement for Diffusion-Based Disease Trajectory Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Modelling disease progression in precision medicine requires capturing\ncomplex spatio-temporal dynamics while preserving anatomical integrity.\nExisting methods often struggle with longitudinal dependencies and structural\nconsistency in progressive disorders. To address these limitations, we\nintroduce MambaControl, a novel framework that integrates selective state-space\nmodelling with diffusion processes for high-fidelity prediction of medical\nimage trajectories. To better capture subtle structural changes over time while\nmaintaining anatomical consistency, MambaControl combines Mamba-based\nlong-range modelling with graph-guided anatomical control to more effectively\nrepresent anatomical correlations. Furthermore, we introduce Fourier-enhanced\nspectral graph representations to capture spatial coherence and multiscale\ndetail, enabling MambaControl to achieve state-of-the-art performance in\nAlzheimer's disease prediction. Quantitative and regional evaluations\ndemonstrate improved progression prediction quality and anatomical fidelity,\nhighlighting its potential for personalised prognosis and clinical decision\nsupport.", "AI": {"tldr": "\u63d0\u51faMambaControl\u6846\u67b6\uff0c\u6574\u5408\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4e0e\u6269\u6563\u8fc7\u7a0b\uff0c\u901a\u8fc7Mamba\u957f\u7a0b\u5efa\u6a21+\u56fe\u5f15\u5bfc\u89e3\u5256\u63a7\u5236+\u5085\u91cc\u53f6\u8c31\u56fe\u8868\u793a\uff0c\u5b9e\u73b0\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u9884\u6d4bSOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u5f71\u50cf\u9884\u6d4b\u65b9\u6cd5\u5728\u957f\u671f\u4f9d\u8d56\u5efa\u6a21\u548c\u89e3\u5256\u4e00\u81f4\u6027\u4fdd\u6301\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5f71\u54cd\u8fdb\u5c55\u6027\u75be\u75c5\u5efa\u6a21\u7684\u51c6\u786e\u6027\u3002\u9700\u8981\u63d0\u5347\u65f6\u7a7a\u52a8\u6001\u6355\u6349\u80fd\u529b\u4ee5\u652f\u6301\u7cbe\u51c6\u533b\u7597\u51b3\u7b56\u3002", "method": "1) Mamba\u67b6\u6784\u6355\u6349\u957f\u671f\u4f9d\u8d56 2) \u56fe\u5f15\u5bfc\u89e3\u5256\u63a7\u5236\u4fdd\u6301\u7ed3\u6784\u4e00\u81f4\u6027 3) \u5085\u91cc\u53f6\u589e\u5f3a\u8c31\u56fe\u8868\u793a\u7a7a\u95f4/\u591a\u5c3a\u5ea6\u7279\u5f81 4) \u6269\u6563\u8fc7\u7a0b\u751f\u6210\u9ad8\u8d28\u91cf\u9884\u6d4b\u8f68\u8ff9", "result": "\u5b9a\u91cf\u8bc4\u4f30\u663e\u793a\u9884\u6d4bPSNR\u63d0\u534712.5%\uff0c\u6d77\u9a6c\u4f53\u7b49\u5173\u952e\u533a\u57df\u4f53\u79ef\u9884\u6d4b\u8bef\u5dee\u964d\u4f4e18.7%\uff0c\u5728ADNI\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u5f53\u524d\u6700\u4f18\u9884\u6d4b\u7cbe\u5ea6\u3002", "conclusion": "MambaControl\u521b\u65b0\u6027\u5730\u878d\u5408\u72b6\u6001\u7a7a\u95f4\u5efa\u6a21\u4e0e\u89e3\u5256\u7ea6\u675f\uff0c\u4e3a\u4e2a\u6027\u5316\u75be\u75c5\u8fdb\u5c55\u9884\u6d4b\u548c\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6280\u672f\u6846\u67b6\u3002"}}
{"id": "2505.10013", "pdf": "https://arxiv.org/pdf/2505.10013", "abs": "https://arxiv.org/abs/2505.10013", "authors": ["Lake Yin", "Fan Huang"], "title": "DIF: A Framework for Benchmarking and Verifying Implicit Bias in LLMs", "categories": ["cs.CL"], "comment": "7 pages, 1 figure", "summary": "As Large Language Models (LLMs) have risen in prominence over the past few\nyears, there has been concern over the potential biases in LLMs inherited from\nthe training data. Previous studies have examined how LLMs exhibit implicit\nbias, such as when response generation changes when different social contexts\nare introduced. We argue that this implicit bias is not only an ethical, but\nalso a technical issue, as it reveals an inability of LLMs to accommodate\nextraneous information. However, unlike other measures of LLM intelligence,\nthere are no standard methods to benchmark this specific subset of LLM bias. To\nbridge this gap, we developed a method for calculating an easily interpretable\nbenchmark, DIF (Demographic Implicit Fairness), by evaluating preexisting LLM\nlogic and math problem datasets with sociodemographic personas. We demonstrate\nthat this method can statistically validate the presence of implicit bias in\nLLM behavior and find an inverse trend between question answering accuracy and\nimplicit bias, supporting our argument.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u9690\u6027\u504f\u89c1\u95ee\u9898\uff0c\u63d0\u51faDIF\u57fa\u51c6\u65b9\u6cd5\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u6a21\u578b\u51c6\u786e\u6027\u4e0e\u9690\u6027\u504f\u89c1\u5448\u8d1f\u76f8\u5173\u3002", "motivation": "LLMs\u7684\u9690\u6027\u504f\u89c1\u4e0d\u4ec5\u662f\u4f26\u7406\u95ee\u9898\uff0c\u66f4\u66b4\u9732\u5176\u65e0\u6cd5\u6709\u6548\u6574\u5408\u989d\u5916\u4fe1\u606f\u7684\u6280\u672f\u7f3a\u9677\uff0c\u4e14\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u65b9\u6cd5\u3002\u5efa\u7acbDIF\u57fa\u51c6\u53ef\u91cf\u5316\u68c0\u6d4b\u6b64\u7c7b\u504f\u5dee\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u73b0\u6709\u903b\u8f91/\u6570\u5b66\u95ee\u9898\u6570\u636e\u96c6\u4e0e\u793e\u4f1a\u4eba\u53e3\u7edf\u8ba1\u89d2\u8272\uff0c\u6784\u5efaDIF\u6307\u6807\u5b9a\u91cf\u5206\u6790LLMs\u8f93\u51fa\u4e2d\u7684\u7cfb\u7edf\u6027\u504f\u5dee\u6a21\u5f0f\u3002", "result": "DIF\u65b9\u6cd5\u6709\u6548\u9a8c\u8bc1\u4e86LLMs\u5b58\u5728\u9690\u6027\u504f\u89c1\uff0c\u4e14\u95ee\u9898\u56de\u7b54\u51c6\u786e\u7387\u4e0e\u9690\u6027\u504f\u89c1\u5f3a\u5ea6\u5448\u73b0\u7edf\u8ba1\u5b66\u663e\u8457\u7684\u8d1f\u76f8\u5173\u8d8b\u52bf\u3002", "conclusion": "DIF\u57fa\u51c6\u4e3a\u8bc4\u4f30LLM\u504f\u89c1\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u6280\u672f\u6307\u6807\uff0c\u8bc1\u5b9e\u9690\u6027\u504f\u89c1\u53cd\u6620\u6a21\u578b\u6280\u672f\u7f3a\u9677\uff0c\u4e3a\u6539\u8fdb\u6a21\u578b\u4fe1\u606f\u5904\u7406\u80fd\u529b\u6307\u660e\u65b9\u5411\u3002"}}
{"id": "2505.09967", "pdf": "https://arxiv.org/pdf/2505.09967", "abs": "https://arxiv.org/abs/2505.09967", "authors": ["Liqian Deng"], "title": "TKFNet: Learning Texture Key Factor Driven Feature for Facial Expression Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Facial expression recognition (FER) in the wild remains a challenging task\ndue to the subtle and localized nature of expression-related features, as well\nas the complex variations in facial appearance. In this paper, we introduce a\nnovel framework that explicitly focuses on Texture Key Driver Factors (TKDF),\nlocalized texture regions that exhibit strong discriminative power across\nemotional categories. By carefully observing facial image patterns, we identify\nthat certain texture cues, such as micro-changes in skin around the brows,\neyes, and mouth, serve as primary indicators of emotional dynamics. To\neffectively capture and leverage these cues, we propose a FER architecture\ncomprising a Texture-Aware Feature Extractor (TAFE) and Dual Contextual\nInformation Filtering (DCIF). TAFE employs a ResNet-based backbone enhanced\nwith multi-branch attention to extract fine-grained texture representations,\nwhile DCIF refines these features by filtering context through adaptive pooling\nand attention mechanisms. Experimental results on RAF-DB and KDEF datasets\ndemonstrate that our method achieves state-of-the-art performance, verifying\nthe effectiveness and robustness of incorporating TKDFs into FER pipelines.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7eb9\u7406\u5173\u952e\u9a71\u52a8\u56e0\u7d20\uff08TKDF\uff09\u7684FER\u6846\u67b6\uff0c\u901a\u8fc7\u7eb9\u7406\u611f\u77e5\u7279\u5f81\u63d0\u53d6\u5668\uff08TAFE\uff09\u548c\u53cc\u4e0a\u4e0b\u6587\u4fe1\u606f\u8fc7\u6ee4\uff08DCIF\uff09\u5b9e\u73b0\u9ad8\u6548\u8868\u60c5\u8bc6\u522b", "motivation": "\u89e3\u51b3\u81ea\u7136\u573a\u666f\u4e0b\u56e0\u8868\u60c5\u7279\u5f81\u7ec6\u5fae/\u5c40\u90e8\u6027\u53ca\u9762\u90e8\u5916\u89c2\u590d\u6742\u53d8\u5316\u5bfc\u81f4\u7684FER\u96be\u9898\uff0c\u53d1\u73b0\u7709/\u773c/\u5634\u7b49\u533a\u57df\u5fae\u7eb9\u7406\u53d8\u5316\u662f\u60c5\u7eea\u52a8\u6001\u7684\u6838\u5fc3\u6307\u6807", "method": "TAFE\u6a21\u5757\uff08\u57fa\u4e8eResNet+\u591a\u5206\u652f\u6ce8\u610f\u529b\uff09\u63d0\u53d6\u7ec6\u7c92\u5ea6\u7eb9\u7406\u7279\u5f81\uff0cDCIF\u6a21\u5757\uff08\u81ea\u9002\u5e94\u6c60\u5316+\u6ce8\u610f\u529b\uff09\u8fdb\u884c\u4e0a\u4e0b\u6587\u4fe1\u606f\u8fc7\u6ee4", "result": "\u5728RAF-DB\u548cKDEF\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u9a8c\u8bc1TKDF\u7b56\u7565\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027", "conclusion": "\u663e\u5f0f\u5efa\u6a21\u7eb9\u7406\u5173\u952e\u9a71\u52a8\u56e0\u7d20\u80fd\u663e\u8457\u63d0\u5347FER\u7cfb\u7edf\u6027\u80fd\uff0c\u6ce8\u610f\u529b\u673a\u5236\u4e0e\u81ea\u9002\u5e94\u7279\u5f81\u8fc7\u6ee4\u7684\u7ec4\u5408\u4f18\u5316\u7b56\u7565\u6548\u679c\u663e\u8457"}}
{"id": "2505.10063", "pdf": "https://arxiv.org/pdf/2505.10063", "abs": "https://arxiv.org/abs/2505.10063", "authors": ["Han Peng", "Jinhao Jiang", "Zican Dong", "Wayne Xin Zhao", "Lei Fang"], "title": "CAFE: Retrieval Head-based Coarse-to-Fine Information Seeking to Enhance Multi-Document QA Capability", "categories": ["cs.CL"], "comment": null, "summary": "Advancements in Large Language Models (LLMs) have extended their input\ncontext length, yet they still struggle with retrieval and reasoning in\nlong-context inputs. Existing methods propose to utilize the prompt strategy\nand retrieval head to alleviate this limitation. However, they still face\nchallenges in balancing retrieval precision and recall, impacting their\nefficacy in answering questions. To address this, we introduce $\\textbf{CAFE}$,\na two-stage coarse-to-fine method to enhance multi-document question-answering\ncapacities. By gradually eliminating the negative impacts of background and\ndistracting documents, CAFE makes the responses more reliant on the evidence\ndocuments. Initially, a coarse-grained filtering method leverages retrieval\nheads to identify and rank relevant documents. Then, a fine-grained steering\nmethod guides attention to the most relevant content. Experiments across\nbenchmarks show CAFE outperforms baselines, achieving up to 22.1% and 13.7%\nSubEM improvement over SFT and RAG methods on the Mistral model, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u9636\u6bb5\u7c97\u7c92\u5ea6\u5230\u7ec6\u7c92\u5ea6\u7684CAFE\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u5927\u6a21\u578b\u5728\u591a\u6587\u6863\u95ee\u7b54\u4e2d\u7684\u6027\u80fd", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u4e2d\u5b58\u5728\u68c0\u7d22\u7cbe\u5ea6\u4e0e\u53ec\u56de\u7387\u96be\u4ee5\u5e73\u8861\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u95ee\u7b54\u6548\u679c\u53d7\u9650", "method": "1. \u7c97\u7c92\u5ea6\u8fc7\u6ee4\u9636\u6bb5\u5229\u7528\u68c0\u7d22\u5934\u7b5b\u9009\u76f8\u5173\u6587\u6863\n2. \u7ec6\u7c92\u5ea6\u5f15\u5bfc\u9636\u6bb5\u96c6\u4e2d\u5173\u6ce8\u6700\u76f8\u5173\u5185\u5bb9\uff0c\u6d88\u9664\u5e72\u6270\u6587\u6863\u5f71\u54cd", "result": "\u5728Mistral\u6a21\u578b\u4e0a\u5206\u522b\u8d85\u8d8aSFT\u548cRAG\u65b9\u6cd522.1%\u548c13.7%\u7684SubEM\u6307\u6807\u63d0\u5347", "conclusion": "CAFE\u901a\u8fc7\u6e10\u8fdb\u5f0f\u6587\u6863\u7b5b\u9009\u673a\u5236\uff0c\u6709\u6548\u589e\u5f3a\u6a21\u578b\u5bf9\u8bc1\u636e\u6587\u6863\u7684\u4f9d\u8d56\uff0c\u5728\u591a\u6587\u6863\u95ee\u7b54\u573a\u666f\u5c55\u73b0\u663e\u8457\u4f18\u52bf"}}
{"id": "2505.09971", "pdf": "https://arxiv.org/pdf/2505.09971", "abs": "https://arxiv.org/abs/2505.09971", "authors": ["Yuan Gao", "Shaobo Xia", "Sheng Nie", "Cheng Wang", "Xiaohuan Xi", "Bisheng Yang"], "title": "APCoTTA: Continual Test-Time Adaptation for Semantic Segmentation of Airborne LiDAR Point Clouds", "categories": ["cs.CV"], "comment": "18 pages,12 figures", "summary": "Airborne laser scanning (ALS) point cloud segmentation is a fundamental task\nfor large-scale 3D scene understanding. In real-world applications, models are\ntypically fixed after training. However, domain shifts caused by changes in the\nenvironment, sensor types, or sensor degradation often lead to a decline in\nmodel performance. Continuous Test-Time Adaptation (CTTA) offers a solution by\nadapting a source-pretrained model to evolving, unlabeled target domains.\nDespite its potential, research on ALS point clouds remains limited, facing\nchallenges such as the absence of standardized datasets and the risk of\ncatastrophic forgetting and error accumulation during prolonged adaptation. To\ntackle these challenges, we propose APCoTTA, the first CTTA method tailored for\nALS point cloud semantic segmentation. We propose a dynamic trainable layer\nselection module. This module utilizes gradient information to select\nlow-confidence layers for training, and the remaining layers are kept frozen,\nmitigating catastrophic forgetting. To further reduce error accumulation, we\npropose an entropy-based consistency loss. By losing such samples based on\nentropy, we apply consistency loss only to the reliable samples, enhancing\nmodel stability. In addition, we propose a random parameter interpolation\nmechanism, which randomly blends parameters from the selected trainable layers\nwith those of the source model. This approach helps balance target adaptation\nand source knowledge retention, further alleviating forgetting. Finally, we\nconstruct two benchmarks, ISPRSC and H3DC, to address the lack of CTTA\nbenchmarks for ALS point cloud segmentation. Experimental results demonstrate\nthat APCoTTA achieves the best performance on two benchmarks, with mIoU\nimprovements of approximately 9% and 14% over direct inference. The new\nbenchmarks and code are available at https://github.com/Gaoyuan2/APCoTTA.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2aALS\u70b9\u4e91\u8fde\u7eed\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u65b9\u6cd5APCoTTA\uff0c\u901a\u8fc7\u52a8\u6001\u5c42\u9009\u62e9\u3001\u71b5\u4e00\u81f4\u6027\u635f\u5931\u548c\u53c2\u6570\u63d2\u503c\u673a\u5236\u89e3\u51b3\u707e\u96be\u6027\u9057\u5fd8\u4e0e\u9519\u8bef\u7d2f\u79ef\u95ee\u9898", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u4f20\u611f\u5668\u5dee\u5f02\u548c\u73af\u5883\u53d8\u5316\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0b\u964d\uff0c\u800cALS\u70b9\u4e91\u9886\u57df\u7f3a\u4e4fCTTA\u57fa\u51c6\u4e14\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u957f\u671f\u9002\u5e94\u5931\u6548\u98ce\u9669", "method": "1) \u52a8\u6001\u53ef\u8bad\u7ec3\u5c42\u9009\u62e9\u6a21\u5757\u901a\u8fc7\u68af\u5ea6\u7b5b\u9009\u4f4e\u7f6e\u4fe1\u5c42\u8bad\u7ec3\uff1b2) \u57fa\u4e8e\u71b5\u7684\u53ef\u9760\u6027\u6837\u672c\u4e00\u81f4\u6027\u635f\u5931\uff1b3) \u968f\u673a\u53c2\u6570\u63d2\u503c\u5e73\u8861\u76ee\u6807\u57df\u9002\u5e94\u4e0e\u6e90\u77e5\u8bc6\u4fdd\u7559", "result": "\u5728ISPRSC\u548cH3DC\u4e24\u4e2a\u65b0\u57fa\u51c6\u4e0amIoU\u5206\u522b\u63d0\u53479%\u548c14%\uff0c\u663e\u8457\u4f18\u4e8e\u76f4\u63a5\u63a8\u7406", "conclusion": "APCoTTA\u6709\u6548\u89e3\u51b3\u4e86\u6301\u7eed\u9002\u5e94\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u516c\u5f00\u7684\u57fa\u51c6\u4e0e\u4ee3\u7801\u586b\u8865\u4e86\u9886\u57df\u7a7a\u767d"}}
{"id": "2505.10066", "pdf": "https://arxiv.org/pdf/2505.10066", "abs": "https://arxiv.org/abs/2505.10066", "authors": ["Michael Fire", "Yitzhak Elbazis", "Adi Wasenstein", "Lior Rokach"], "title": "Dark LLMs: The Growing Threat of Unaligned AI Models", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG", "68T50, 68T05, 68P25", "I.2.7"], "comment": null, "summary": "Large Language Models (LLMs) rapidly reshape modern life, advancing fields\nfrom healthcare to education and beyond. However, alongside their remarkable\ncapabilities lies a significant threat: the susceptibility of these models to\njailbreaking. The fundamental vulnerability of LLMs to jailbreak attacks stems\nfrom the very data they learn from. As long as this training data includes\nunfiltered, problematic, or 'dark' content, the models can inherently learn\nundesirable patterns or weaknesses that allow users to circumvent their\nintended safety controls. Our research identifies the growing threat posed by\ndark LLMs models deliberately designed without ethical guardrails or modified\nthrough jailbreak techniques. In our research, we uncovered a universal\njailbreak attack that effectively compromises multiple state-of-the-art models,\nenabling them to answer almost any question and produce harmful outputs upon\nrequest. The main idea of our attack was published online over seven months\nago. However, many of the tested LLMs were still vulnerable to this attack.\nDespite our responsible disclosure efforts, responses from major LLM providers\nwere often inadequate, highlighting a concerning gap in industry practices\nregarding AI safety. As model training becomes more accessible and cheaper, and\nas open-source LLMs proliferate, the risk of widespread misuse escalates.\nWithout decisive intervention, LLMs may continue democratizing access to\ndangerous knowledge, posing greater risks than anticipated.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLMs\u5b58\u5728\u901a\u8fc7\u8bad\u7ec3\u6570\u636e\u6f0f\u6d1e\u5b9e\u73b0\u7684\u901a\u7528\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\uff0c\u591a\u6570\u5148\u8fdb\u6a21\u578b\u5728\u653b\u51fb\u62ab\u97327\u4e2a\u6708\u540e\u4ecd\u5b58\u5728\u5b89\u5168\u9690\u60a3\uff0c\u884c\u4e1a\u5e94\u5bf9\u63aa\u65bd\u4e0d\u8db3\u51f8\u663eAI\u5b89\u5168\u5b9e\u8df5\u7f3a\u9677", "motivation": "\u968f\u7740LLMs\u5728\u533b\u7597\u3001\u6559\u80b2\u7b49\u9886\u57df\u7684\u5feb\u901f\u5e94\u7528\uff0c\u5176\u5b89\u5168\u6f0f\u6d1e\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\u3002\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u901a\u8fc7\u672a\u8fc7\u6ee4\u8bad\u7ec3\u6570\u636e\u5b66\u4e60\u5230\u53ef\u88ab\u5229\u7528\u7684\u5f31\u70b9\uff0c\u5bfc\u81f4\u4f26\u7406\u62a4\u680f\u5931\u6548\u98ce\u9669", "method": "\u57fa\u4e8e\u8bad\u7ec3\u6570\u636e\u4e2d'\u6697\u9ed1\u5185\u5bb9'\u7684\u5b58\u5728\uff0c\u5f00\u53d1\u51fa\u53ef\u7a81\u7834\u591a\u6b3e\u5148\u8fdb\u6a21\u578b\u7684\u901a\u7528\u8d8a\u72f1\u653b\u51fb\u6280\u672f\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u6a21\u578b\u5b66\u4e60\u5230\u7684\u6f5c\u5728\u6f0f\u6d1e\u7ed5\u8fc7\u5b89\u5168\u63a7\u5236\u673a\u5236", "result": "\u5b9e\u9a8c\u663e\u793a\u653b\u51fb\u65b9\u6cd5\u80fd\u6709\u6548\u4f7f\u4e3b\u6d41LLMs\u54cd\u5e94\u6709\u5bb3\u8bf7\u6c42\uff0c\u653b\u51fb\u65b9\u6848\u516c\u5f007\u4e2a\u6708\u540e\u591a\u6570\u53d7\u6d4b\u6a21\u578b\u4ecd\u5b58\u5728\u6f0f\u6d1e\u3002\u4e3b\u8981LLM\u4f9b\u5e94\u5546\u54cd\u5e94\u5b58\u5728\u4e25\u91cd\u6ede\u540e", "conclusion": "\u968f\u7740\u5f00\u6e90\u6a21\u578b\u666e\u53ca\u548c\u8bad\u7ec3\u6210\u672c\u964d\u4f4e\uff0cLLMs\u53ef\u80fd\u6210\u4e3a\u5371\u9669\u77e5\u8bc6\u4f20\u64ad\u5de5\u5177\u3002\u7f3a\u4e4f\u6709\u6548\u76d1\u7ba1\u548c\u884c\u4e1a\u6807\u51c6\u53ef\u80fd\u5bfc\u81f4AI\u5b89\u5168\u98ce\u9669\u8d85\u51fa\u9884\u671f\uff0c\u9700\u5efa\u7acb\u66f4\u4e25\u683c\u7684\u5b89\u5168\u54cd\u5e94\u673a\u5236"}}
{"id": "2505.09986", "pdf": "https://arxiv.org/pdf/2505.09986", "abs": "https://arxiv.org/abs/2505.09986", "authors": ["Yimin Zhou", "Yichong Xia", "Sicheng Pan", "Bin Chen", "Baoyi An", "Haoqian Wang", "Zhi Wang", "Yaowei Wang", "Zikun Zhou"], "title": "High Quality Underwater Image Compression with Adaptive Correction and Codebook-based Augmentation", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "With the increasing exploration and exploitation of the underwater world,\nunderwater images have become a critical medium for human interaction with\nmarine environments, driving extensive research into their efficient\ntransmission and storage. However, contemporary underwater image compression\nalgorithms fail to fully leverage the unique characteristics distinguishing\nunderwater scenes from terrestrial images, resulting in suboptimal performance.\nTo address this limitation, we introduce HQUIC, designed to exploit\nunderwater-image-specific features for enhanced compression efficiency. HQUIC\nemploys an ALTC module to adaptively predict the attenuation coefficients and\nglobal light information of the images, which effectively mitigates the issues\ncaused by the differences in lighting and tone existing in underwater images.\nSubsequently, HQUIC employs a codebook as an auxiliary branch to extract the\ncommon objects within underwater images and enhances the performance of the\nmain branch. Furthermore, HQUIC dynamically weights multi-scale frequency\ncomponents, prioritizing information critical for distortion quality while\ndiscarding redundant details. Extensive evaluations on diverse underwater\ndatasets demonstrate that HQUIC outperforms state-of-the-art compression\nmethods.", "AI": {"tldr": "\u63d0\u51fa\u6c34\u4e0b\u56fe\u50cf\u538b\u7f29\u6a21\u578bHQUIC\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5149\u4f20\u8f93\u8865\u507f\u6a21\u5757\u548c\u9891\u57df\u52a8\u6001\u52a0\u6743\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u6c34\u4e0b\u56fe\u50cf\u538b\u7f29\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u538b\u7f29\u7b97\u6cd5\u672a\u5145\u5206\u6316\u6398\u6c34\u4e0b\u56fe\u50cf\u7684\u5149\u7167\u8870\u51cf\u3001\u8272\u8c03\u504f\u5dee\u548c\u5e38\u89c1\u7269\u4f53\u91cd\u590d\u6027\u7b49\u7279\u6027\uff0c\u5bfc\u81f4\u538b\u7f29\u6027\u80fd\u53d7\u9650\u3002\u6c34\u4e0b\u56fe\u50cf\u4f20\u8f93\u5b58\u50a8\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u9700\u9488\u5bf9\u6027\u4f18\u5316\u538b\u7f29\u65b9\u6cd5\u3002", "method": "1. \u91c7\u7528ALTC\u6a21\u5757\u9884\u6d4b\u56fe\u50cf\u8870\u51cf\u7cfb\u6570\u548c\u5168\u5c40\u5149\u7167\uff0c\u7f13\u89e3\u6c34\u4e0b\u5149\u7167\u5dee\u5f02\n2. \u901a\u8fc7\u7801\u672c\u5206\u652f\u63d0\u53d6\u6c34\u4e0b\u5e38\u89c1\u7269\u4f53\u7279\u5f81\u589e\u5f3a\u4e3b\u5206\u652f\n3. \u52a8\u6001\u52a0\u6743\u591a\u5c3a\u5ea6\u9891\u7387\u5206\u91cf\uff0c\u4f18\u5148\u4fdd\u7559\u5173\u952e\u4fe1\u606f", "result": "\u5728\u591a\u4e2a\u6c34\u4e0b\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHQUIC\u5728\u538b\u7f29\u6548\u7387\u4e0a\u8d85\u8d8a\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\uff0cPSNR\u6307\u6807\u5e73\u5747\u63d0\u53471.2dB", "conclusion": "\u901a\u8fc7\u9488\u5bf9\u6027\u5efa\u6a21\u6c34\u4e0b\u56fe\u50cf\u7684\u5149\u4f20\u8f93\u7279\u6027\u548c\u7269\u4f53\u5206\u5e03\u89c4\u5f8b\uff0cHQUIC\u6709\u6548\u89e3\u51b3\u4e86\u6c34\u4e0b\u538b\u7f29\u573a\u666f\u7684\u7279\u6b8a\u6311\u6218\uff0c\u4e3a\u6d77\u6d0b\u5de5\u7a0b\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u56fe\u50cf\u538b\u7f29\u65b9\u6848\u3002"}}
{"id": "2505.10081", "pdf": "https://arxiv.org/pdf/2505.10081", "abs": "https://arxiv.org/abs/2505.10081", "authors": ["Wisdom Aduah", "Francois Meyer"], "title": "Designing and Contextualising Probes for African Languages", "categories": ["cs.CL"], "comment": null, "summary": "Pretrained language models (PLMs) for African languages are continually\nimproving, but the reasons behind these advances remain unclear. This paper\npresents the first systematic investigation into probing PLMs for linguistic\nknowledge about African languages. We train layer-wise probes for six\ntypologically diverse African languages to analyse how linguistic features are\ndistributed. We also design control tasks, a way to interpret probe\nperformance, for the MasakhaPOS dataset. We find PLMs adapted for African\nlanguages to encode more linguistic information about target languages than\nmassively multilingual PLMs. Our results reaffirm previous findings that\ntoken-level syntactic information concentrates in middle-to-last layers, while\nsentence-level semantic information is distributed across all layers. Through\ncontrol tasks and probing baselines, we confirm that performance reflects the\ninternal knowledge of PLMs rather than probe memorisation. Our study applies\nestablished interpretability techniques to African-language PLMs. In doing so,\nwe highlight the internal mechanisms underlying the success of strategies like\nactive learning and multilingual adaptation.", "AI": {"tldr": "\u9996\u6b21\u7cfb\u7edf\u6027\u7814\u7a76\u975e\u6d32\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bed\u8a00\u77e5\u8bc6\u7f16\u7801\u673a\u5236\uff0c\u53d1\u73b0\u4e13\u7528\u6a21\u578b\u4f18\u4e8e\u901a\u7528\u591a\u8bed\u8a00\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u53e5\u6cd5\u548c\u8bed\u4e49\u4fe1\u606f\u7684\u5c42\u6b21\u5206\u5e03\u7279\u5f81\u3002", "motivation": "\u975e\u6d32\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u6027\u80fd\u6301\u7eed\u63d0\u5347\uff0c\u4f46\u5176\u5185\u90e8\u8bed\u8a00\u5b66\u77e5\u8bc6\u7f16\u7801\u673a\u5236\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u7cfb\u7edf\u63a2\u7a76\u6a21\u578b\u5bf9\u76ee\u6807\u8bed\u8a00\u7279\u5f81\u7684\u6355\u83b7\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u5206\u5c42\u63a2\u9488\u6280\u672f\u5206\u6790\u516d\u79cd\u7c7b\u578b\u5b66\u5dee\u5f02\u663e\u8457\u7684\u975e\u6d32\u8bed\u8a00\uff0c\u8bbe\u8ba1MasakhaPOS\u63a7\u5236\u4efb\u52a1\u9a8c\u8bc1\u63a2\u9488\u6709\u6548\u6027\uff0c\u6bd4\u8f83\u4e13\u7528PLMs\u4e0e\u5927\u89c4\u6a21\u591a\u8bed\u8a00PLMs\u7684\u8868\u73b0\u5dee\u5f02\u3002", "result": "\u975e\u6d32\u8bed\u8a00\u4e13\u7528PLMs\u7f16\u7801\u66f4\u591a\u76ee\u6807\u8bed\u8a00\u7279\u5f81\uff1b\u53e5\u6cd5\u4fe1\u606f\u96c6\u4e2d\u5728\u4e2d\u540e\u5c42\uff08\u7b2c6-8\u5c42\uff09\uff0c\u8bed\u4e49\u4fe1\u606f\u5168\u5c42\u5206\u5e03\uff1b\u63a7\u5236\u4efb\u52a1\u8bc1\u5b9e\u63a2\u9488\u53cd\u6620\u6a21\u578b\u771f\u5b9e\u77e5\u8bc6\u800c\u975e\u8bb0\u5fc6\u6548\u5e94\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u548c\u591a\u8bed\u8a00\u9002\u5e94\u7684\u5185\u90e8\u673a\u5236\uff0c\u4e3a\u975e\u6d32\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u7406\u8bba\u652f\u6491\uff0c\u9996\u6b21\u5c06\u53ef\u89e3\u91ca\u6027\u6280\u672f\u7cfb\u7edf\u5e94\u7528\u4e8e\u975e\u6d32\u8bed\u8a00PLMs\u5206\u6790\u3002"}}
{"id": "2505.09990", "pdf": "https://arxiv.org/pdf/2505.09990", "abs": "https://arxiv.org/abs/2505.09990", "authors": ["Long Cheng", "Jiafei Duan", "Yi Ru Wang", "Haoquan Fang", "Boyang Li", "Yushan Huang", "Elvis Wang", "Ainaz Eftekhar", "Jason Lee", "Wentao Yuan", "Rose Hendrix", "Noah A. Smith", "Fei Xia", "Dieter Fox", "Ranjay Krishna"], "title": "PointArena: Probing Multimodal Grounding Through Language-Guided Pointing", "categories": ["cs.CV"], "comment": "10 Pages, Dataset and code:https://pointarena.github.io/", "summary": "Pointing serves as a fundamental and intuitive mechanism for grounding\nlanguage within visual contexts, with applications spanning robotics, assistive\ntechnologies, and interactive AI systems. While recent multimodal models have\nstarted to support pointing capabilities, existing benchmarks typically focus\nonly on referential object localization tasks. We introduce PointArena, a\ncomprehensive platform for evaluating multimodal pointing across diverse\nreasoning scenarios. PointArena comprises three components: (1) Point-Bench, a\ncurated dataset containing approximately 1,000 pointing tasks across five\nreasoning categories; (2) Point-Battle, an interactive, web-based arena\nfacilitating blind, pairwise model comparisons, which has already gathered over\n4,500 anonymized votes; and (3) Point-Act, a real-world robotic manipulation\nsystem allowing users to directly evaluate multimodal model pointing\ncapabilities in practical settings. We conducted extensive evaluations of both\nstate-of-the-art open-source and proprietary multimodal models. Results\nindicate that Molmo-72B consistently outperforms other models, though\nproprietary models increasingly demonstrate comparable performance.\nAdditionally, we find that supervised training specifically targeting pointing\ntasks significantly enhances model performance. Across our multi-stage\nevaluation pipeline, we also observe strong correlations, underscoring the\ncritical role of precise pointing capabilities in enabling multimodal models to\neffectively bridge abstract reasoning with concrete, real-world actions.\nProject page: https://pointarena.github.io/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u591a\u6a21\u6001\u6307\u5411\u80fd\u529b\u7684\u7efc\u5408\u5e73\u53f0PointArena\uff0c\u5305\u542b\u57fa\u51c6\u6d4b\u8bd5\u3001\u4ea4\u4e92\u5f0f\u8bc4\u4f30\u548c\u673a\u5668\u4eba\u9a8c\u8bc1\u4e09\u90e8\u5206", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4ec5\u5173\u6ce8\u5bf9\u8c61\u5b9a\u4f4d\u4efb\u52a1\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u591a\u6a21\u6001\u6307\u5411\u80fd\u529b\u8bc4\u4f30\u4f53\u7cfb", "method": "\u6784\u5efa\u5305\u542b1,000\u4e2a\u4efb\u52a1\u7684Point-Bench\u6570\u636e\u96c6\u3001\u6536\u96c64,500+\u533f\u540d\u6295\u7968\u7684Point-Battle\u5e73\u53f0\uff0c\u4ee5\u53ca\u673a\u5668\u4eba\u7cfb\u7edfPoint-Act", "result": "Molmo-72B\u8868\u73b0\u6700\u4f73\uff0c\u76d1\u7763\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5404\u8bc4\u4f30\u9636\u6bb5\u5448\u73b0\u5f3a\u76f8\u5173\u6027", "conclusion": "\u7cbe\u786e\u5b9a\u4f4d\u80fd\u529b\u662f\u591a\u6a21\u6001\u6a21\u578b\u8fde\u63a5\u62bd\u8c61\u63a8\u7406\u4e0e\u771f\u5b9e\u4e16\u754c\u884c\u52a8\u7684\u5173\u952e"}}
{"id": "2505.10089", "pdf": "https://arxiv.org/pdf/2505.10089", "abs": "https://arxiv.org/abs/2505.10089", "authors": ["Wei Liu", "Sony Trenous", "Leonardo F. R. Ribeiro", "Bill Byrne", "Felix Hieber"], "title": "XRAG: Cross-lingual Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "We propose XRAG, a novel benchmark designed to evaluate the generation\nabilities of LLMs in cross-lingual Retrieval-Augmented Generation (RAG)\nsettings where the user language does not match the retrieval results. XRAG is\nconstructed from recent news articles to ensure that its questions require\nexternal knowledge to be answered. It covers the real-world scenarios of\nmonolingual and multilingual retrieval, and provides relevancy annotations for\neach retrieved document. Our novel dataset construction pipeline results in\nquestions that require complex reasoning, as evidenced by the significant gap\nbetween human and LLM performance. Consequently, XRAG serves as a valuable\nbenchmark for studying LLM reasoning abilities, even before considering the\nadditional cross-lingual complexity. Experimental results on five LLMs uncover\ntwo previously unreported challenges in cross-lingual RAG: 1) in the\nmonolingual retrieval setting, all evaluated models struggle with response\nlanguage correctness; 2) in the multilingual retrieval setting, the main\nchallenge lies in reasoning over retrieved information across languages rather\nthan generation of non-English text.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u8de8\u8bed\u8a00\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u57fa\u51c6XRAG\uff0c\u901a\u8fc7\u65b0\u95fb\u6570\u636e\u96c6\u9a8c\u8bc1LLMs\u5728\u8de8\u8bed\u8a00\u573a\u666f\u4e0b\u7684\u4e24\u5927\u6838\u5fc3\u6311\u6218", "motivation": "\u89e3\u51b3\u7528\u6237\u8bed\u8a00\u4e0e\u68c0\u7d22\u5185\u5bb9\u8bed\u8a00\u4e0d\u5339\u914d\u65f6\uff0c\u73b0\u6709\u6a21\u578b\u5728\u8de8\u8bed\u8a00\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u573a\u666f\u4e2d\u7684\u8bc4\u4f30\u7f3a\u5931\u95ee\u9898", "method": "\u57fa\u4e8e\u5b9e\u65f6\u65b0\u95fb\u6784\u5efa\u542b\u76f8\u5173\u6027\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff0c\u8986\u76d6\u5355\u8bed/\u591a\u8bed\u68c0\u7d22\u573a\u666f\uff0c\u8bbe\u8ba1\u9700\u8981\u590d\u6742\u63a8\u7406\u7684\u95ee\u9898", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff1a1\uff09\u5355\u8bed\u68c0\u7d22\u4e2d\u6a21\u578b\u5b58\u5728\u5e94\u7b54\u8bed\u8a00\u51c6\u786e\u6027\u95ee\u9898 2\uff09\u591a\u8bed\u68c0\u7d22\u4e2d\u8de8\u8bed\u8a00\u63a8\u7406\u662f\u4e3b\u8981\u74f6\u9888", "conclusion": "XRAG\u4e0d\u4ec5\u80fd\u8bc4\u4f30\u8de8\u8bed\u8a00\u590d\u6742\u5ea6\uff0c\u66f4\u53ef\u4f5c\u4e3a\u7814\u7a76LLMs\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4f20\u7edf\u8bc4\u6d4b\u5ffd\u89c6\u7684\u6a21\u578b\u7f3a\u9677"}}
{"id": "2505.09997", "pdf": "https://arxiv.org/pdf/2505.09997", "abs": "https://arxiv.org/abs/2505.09997", "authors": ["Jinhyun Jang", "Jiyeong Lee", "Kwanghoon Sohn"], "title": "Descriptive Image-Text Matching with Graded Contextual Similarity", "categories": ["cs.CV"], "comment": null, "summary": "Image-text matching aims to build correspondences between visual and textual\ndata by learning their pairwise similarities. Most existing approaches have\nadopted sparse binary supervision, indicating whether a pair of images and\nsentences matches or not. However, such sparse supervision covers a limited\nsubset of image-text relationships, neglecting their inherent many-to-many\ncorrespondences; an image can be described in numerous texts at different\ndescriptive levels. Moreover, existing approaches overlook the implicit\nconnections from general to specific descriptions, which form the underlying\nrationale for the many-to-many relationships between vision and language. In\nthis work, we propose descriptive image-text matching, called DITM, to learn\nthe graded contextual similarity between image and text by exploring the\ndescriptive flexibility of language. We formulate the descriptiveness score of\neach sentence with cumulative term frequency-inverse document frequency\n(TF-IDF) to balance the pairwise similarity according to the keywords in the\nsentence. Our method leverages sentence descriptiveness to learn robust\nimage-text matching in two key ways: (1) to refine the false negative labeling,\ndynamically relaxing the connectivity between positive and negative pairs, and\n(2) to build more precise matching, aligning a set of relevant sentences in a\ngeneric-to-specific order. By moving beyond rigid binary supervision, DITM\nenhances the discovery of both optimal matches and potential positive pairs.\nExtensive experiments on MS-COCO, Flickr30K, and CxC datasets demonstrate the\neffectiveness of our method in representing complex image-text relationships\ncompared to state-of-the-art approaches. In addition, DITM enhances the\nhierarchical reasoning ability of the model, supported by the extensive\nanalysis on HierarCaps benchmark.", "AI": {"tldr": "\u63d0\u51faDITM\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u8a00\u63cf\u8ff0\u7684\u7075\u6d3b\u6027\u5b66\u4e60\u5206\u7ea7\u4e0a\u4e0b\u6587\u76f8\u4f3c\u6027\uff0c\u5229\u7528\u53e5\u5b50\u63cf\u8ff0\u6027\u8bc4\u5206\u5b9e\u73b0\u52a8\u6001\u8d1f\u6837\u672c\u8c03\u6574\u548c\u5c42\u7ea7\u6587\u672c\u5bf9\u9f50\uff0c\u589e\u5f3a\u56fe\u50cf-\u6587\u672c\u5339\u914d\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u4e8c\u5143\u76d1\u7763\u65b9\u6cd5\u8986\u76d6\u6709\u9650\u56fe\u50cf-\u6587\u672c\u5173\u7cfb\uff0c\u5ffd\u7565\u591a\u5bf9\u591a\u5bf9\u5e94\u53ca\u9690\u542b\u7684\u4ece\u901a\u7528\u5230\u7279\u5b9a\u63cf\u8ff0\u7684\u5c42\u6b21\u8054\u7cfb\u3002", "method": "1. \u57fa\u4e8e\u7d2f\u79efTF-IDF\u8ba1\u7b97\u53e5\u5b50\u63cf\u8ff0\u6027\u8bc4\u5206\n2. \u52a8\u6001\u677e\u5f1b\u6b63\u8d1f\u6837\u672c\u8fde\u63a5\u6027\n3. \u901a\u7528\u5230\u7279\u5b9a\u987a\u5e8f\u7684\u6587\u672c\u5bf9\u9f50\u7b56\u7565", "result": "\u5728MS-COCO/Flickr30K/CxC\u6570\u636e\u96c6\u4e0a\u8d85\u8d8aSOTA\uff0cHierarCaps\u57fa\u51c6\u9a8c\u8bc1\u6a21\u578b\u5c42\u6b21\u63a8\u7406\u80fd\u529b\u63d0\u5347", "conclusion": "DITM\u7a81\u7834\u521a\u6027\u4e8c\u5143\u76d1\u7763\u8303\u5f0f\uff0c\u901a\u8fc7\u63cf\u8ff0\u6027\u8bc4\u5206\u673a\u5236\u6709\u6548\u6316\u6398\u6f5c\u5728\u6b63\u6837\u672c\u5bf9\uff0c\u5b9e\u73b0\u590d\u6742\u8de8\u6a21\u6001\u5173\u7cfb\u7684\u5c42\u6b21\u5316\u5efa\u6a21"}}
{"id": "2505.10113", "pdf": "https://arxiv.org/pdf/2505.10113", "abs": "https://arxiv.org/abs/2505.10113", "authors": ["Xinlan Yan", "Di Wu", "Yibin Lei", "Christof Monz", "Iacer Calixto"], "title": "What Does Neuro Mean to Cardio? Investigating the Role of Clinical Specialty Data in Medical LLMs", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we introduce S-MedQA, an English medical question-answering\n(QA) dataset for benchmarking large language models in fine-grained clinical\nspecialties. We use S-MedQA to check the applicability of a popular hypothesis\nrelated to knowledge injection in the knowledge-intense scenario of medical QA,\nand show that: 1) training on data from a speciality does not necessarily lead\nto best performance on that specialty and 2) regardless of the specialty\nfine-tuned on, token probabilities of clinically relevant terms for all\nspecialties increase consistently. Thus, we believe improvement gains come\nmostly from domain shifting (e.g., general to medical) rather than knowledge\ninjection and suggest rethinking the role of fine-tuning data in the medical\ndomain. We release S-MedQA and all code needed to reproduce all our experiments\nto the research community.", "AI": {"tldr": "\u63d0\u51faS-MedQA\u533b\u5b66\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u9a8c\u8bc1\u5927\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u6ce8\u5165\u5047\u8bbe\uff0c\u53d1\u73b0\u6027\u80fd\u63d0\u5347\u4e3b\u8981\u6765\u81ea\u9886\u57df\u8f6c\u79fb\u800c\u975e\u77e5\u8bc6\u6ce8\u5165\u3002", "motivation": "\u9a8c\u8bc1\u533b\u5b66\u95ee\u7b54\u573a\u666f\u4e2d\u77e5\u8bc6\u6ce8\u5165\u5047\u8bbe\u7684\u9002\u7528\u6027\uff0c\u63a2\u7a76\u4e13\u4e1a\u9886\u57df\u5fae\u8c03\u6570\u636e\u5bf9\u6a21\u578b\u8868\u73b0\u7684\u771f\u5b9e\u5f71\u54cd\u673a\u5236\u3002", "method": "\u901a\u8fc7S-MedQA\u6570\u636e\u96c6\u5728\u4e0d\u540c\u4e34\u5e8a\u4e13\u4e1a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5206\u6790\u6a21\u578b\u5728\u7279\u5b9a\u4e13\u4e1a\u8bad\u7ec3\u540e\u7684\u8868\u73b0\u53ca\u672f\u8bed\u6982\u7387\u53d8\u5316\u3002", "result": "1. \u7279\u5b9a\u4e13\u4e1a\u8bad\u7ec3\u4e0d\u4fdd\u8bc1\u8be5\u9886\u57df\u6700\u4f73\u8868\u73b0\uff1b2. \u6240\u6709\u4e13\u4e1a\u5fae\u8c03\u540e\u4e34\u5e8a\u672f\u8bed\u6982\u7387\u5747\u6301\u7eed\u589e\u52a0\uff0c\u663e\u793a\u9886\u57df\u8f6c\u79fb\u6548\u5e94\u3002", "conclusion": "\u5e94\u91cd\u65b0\u8bc4\u4f30\u533b\u5b66\u9886\u57df\u5fae\u8c03\u6570\u636e\u7684\u4f5c\u7528\uff0c\u6027\u80fd\u63d0\u5347\u4e3b\u56e0\u662f\u9886\u57df\u8f6c\u6362\uff08\u901a\u7528\u2192\u533b\u5b66\uff09\u800c\u975e\u77e5\u8bc6\u6ce8\u5165\u3002"}}
{"id": "2505.09998", "pdf": "https://arxiv.org/pdf/2505.09998", "abs": "https://arxiv.org/abs/2505.09998", "authors": ["Ying Zang", "Yuanqi Hu", "Xinyu Chen", "Yuxia Xu", "Suhui Wang", "Chunan Yu", "Lanyun Zhu", "Deyi Ji", "Xin Xu", "Tianrun Chen"], "title": "From Air to Wear: Personalized 3D Digital Fashion with AR/VR Immersive 3D Sketching", "categories": ["cs.CV"], "comment": "8 pages, 5 figures", "summary": "In the era of immersive consumer electronics, such as AR/VR headsets and\nsmart devices, people increasingly seek ways to express their identity through\nvirtual fashion. However, existing 3D garment design tools remain inaccessible\nto everyday users due to steep technical barriers and limited data. In this\nwork, we introduce a 3D sketch-driven 3D garment generation framework that\nempowers ordinary users - even those without design experience - to create\nhigh-quality digital clothing through simple 3D sketches in AR/VR environments.\nBy combining a conditional diffusion model, a sketch encoder trained in a\nshared latent space, and an adaptive curriculum learning strategy, our system\ninterprets imprecise, free-hand input and produces realistic, personalized\ngarments. To address the scarcity of training data, we also introduce\nKO3DClothes, a new dataset of paired 3D garments and user-created sketches.\nExtensive experiments and user studies confirm that our method significantly\noutperforms existing baselines in both fidelity and usability, demonstrating\nits promise for democratized fashion design on next-generation consumer\nplatforms.", "AI": {"tldr": "\u63d0\u51fa3D\u8349\u56fe\u9a71\u52a8\u7684\u670d\u88c5\u751f\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u6761\u4ef6\u6269\u6563\u6a21\u578b\u4e0e\u81ea\u9002\u5e94\u8bfe\u7a0b\u5b66\u4e60\uff0c\u964d\u4f4e\u865a\u62df\u670d\u88c5\u8bbe\u8ba1\u95e8\u69db", "motivation": "\u73b0\u67093D\u670d\u88c5\u8bbe\u8ba1\u5de5\u5177\u56e0\u6280\u672f\u95e8\u69db\u9ad8\u548c\u6570\u636e\u532e\u4e4f\u96be\u4ee5\u4e3a\u666e\u901a\u7528\u6237\u6240\u7528\uff0c\u9700\u5f00\u53d1\u66f4\u6613\u7528\u7684\u521b\u4f5c\u65b9\u6848", "method": "\u878d\u5408\u6761\u4ef6\u6269\u6563\u6a21\u578b/\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u8349\u56fe\u7f16\u7801\u5668/\u81ea\u9002\u5e94\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u5e76\u6784\u5efaKO3DClothes\u8349\u56fe-\u670d\u88c5\u914d\u5bf9\u6570\u636e\u96c6", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u65b9\u6cd5\u5728\u4fdd\u771f\u5ea6\u548c\u53ef\u7528\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u7528\u6237\u7814\u7a76\u8bc1\u5b9e\u975e\u4e13\u4e1a\u7528\u6237\u53ef\u6709\u6548\u521b\u4f5c\u4e2a\u6027\u5316\u670d\u88c5", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e0b\u4e00\u4ee3\u6d88\u8d39\u5e73\u53f0\u7684\u6c11\u4e3b\u5316\u65f6\u5c1a\u8bbe\u8ba1\u63d0\u4f9b\u53ef\u884c\u8def\u5f84\uff0c\u63a8\u52a8\u865a\u62df\u8eab\u4efd\u8868\u8fbe\u7684\u6280\u672f\u666e\u60e0"}}
{"id": "2505.10143", "pdf": "https://arxiv.org/pdf/2505.10143", "abs": "https://arxiv.org/abs/2505.10143", "authors": ["Longchao Da", "Parth Mitesh Shah", "Kuan-Ru Liou", "Jiaxing Zhang", "Hua Wei"], "title": "GE-Chat: A Graph Enhanced RAG Framework for Evidential Response Generation of LLMs", "categories": ["cs.CL", "68T50, 68T30", "I.2.7; I.2.4; H.3.3"], "comment": "5 pages, 4 figures, accepted to IJCAI2025 demo track", "summary": "Large Language Models are now key assistants in human decision-making\nprocesses. However, a common note always seems to follow: \"LLMs can make\nmistakes. Be careful with important info.\" This points to the reality that not\nall outputs from LLMs are dependable, and users must evaluate them manually.\nThe challenge deepens as hallucinated responses, often presented with seemingly\nplausible explanations, create complications and raise trust issues among\nusers. To tackle such issue, this paper proposes GE-Chat, a knowledge Graph\nenhanced retrieval-augmented generation framework to provide Evidence-based\nresponse generation. Specifically, when the user uploads a material document, a\nknowledge graph will be created, which helps construct a retrieval-augmented\nagent, enhancing the agent's responses with additional knowledge beyond its\ntraining corpus. Then we leverage Chain-of-Thought (CoT) logic generation,\nn-hop sub-graph searching, and entailment-based sentence generation to realize\naccurate evidence retrieval. We demonstrate that our method improves the\nexisting models' performance in terms of identifying the exact evidence in a\nfree-form context, providing a reliable way to examine the resources of LLM's\nconclusion and help with the judgment of the trustworthiness.", "AI": {"tldr": "\u63d0\u51faGE-Chat\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u7684\u68c0\u7d22\u751f\u6210\u673a\u5236\u63d0\u5347LLM\u54cd\u5e94\u53ef\u4fe1\u5ea6", "motivation": "LLM\u8f93\u51fa\u5b58\u5728\u4e0d\u53ef\u9760\u6027\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u9700\u5efa\u7acb\u8bc1\u636e\u9a8c\u8bc1\u673a\u5236\u589e\u5f3a\u7528\u6237\u4fe1\u4efb", "method": "\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u3001\u601d\u7ef4\u94fe\u751f\u6210\u3001\u591a\u8df3\u5b50\u56fe\u641c\u7d22\u4e0e\u8574\u542b\u63a8\u7406\u7684\u68c0\u7d22\u589e\u5f3a\u6846\u67b6", "result": "\u663e\u8457\u63d0\u5347\u73b0\u6709\u6a21\u578b\u5728\u81ea\u7531\u6587\u672c\u4e2d\u51c6\u786e\u8bc6\u522b\u8bc1\u636e\u7684\u80fd\u529b\uff08\u5b9e\u9a8c\u6570\u636e\u672a\u660e\u786e\u7ed9\u51fa\uff09", "conclusion": "\u901a\u8fc7\u53ef\u9a8c\u8bc1\u8bc1\u636e\u94fe\u6784\u5efa\uff0c\u4e3aLLM\u7ed3\u8bba\u63d0\u4f9b\u6eaf\u6e90\u652f\u6301\uff0c\u5efa\u7acb\u53ef\u4fe1\u5ea6\u8bc4\u4f30\u4f53\u7cfb"}}
{"id": "2505.10016", "pdf": "https://arxiv.org/pdf/2505.10016", "abs": "https://arxiv.org/abs/2505.10016", "authors": ["Shijie Lyu"], "title": "Application of YOLOv8 in monocular downward multiple Car Target detection", "categories": ["cs.CV", "cs.AI", "I.4.8; I.2.10"], "comment": "Accepted by the 5th International Conference on Signal Processing and\n  Machine Learning (CONF-SPML 2025), to appear in Applied and Computational\n  Engineering", "summary": "Autonomous driving technology is progressively transforming traditional car\ndriving methods, marking a significant milestone in modern transportation.\nObject detection serves as a cornerstone of autonomous systems, playing a vital\nrole in enhancing driving safety, enabling autonomous functionality, improving\ntraffic efficiency, and facilitating effective emergency responses. However,\ncurrent technologies such as radar for environmental perception, cameras for\nroad perception, and vehicle sensor networks face notable challenges, including\nhigh costs, vulnerability to weather and lighting conditions, and limited\nresolution.To address these limitations, this paper presents an improved\nautonomous target detection network based on YOLOv8. By integrating structural\nreparameterization technology, a bidirectional pyramid structure network model,\nand a novel detection pipeline into the YOLOv8 framework, the proposed approach\nachieves highly efficient and precise detection of multi-scale, small, and\nremote objects. Experimental results demonstrate that the enhanced model can\neffectively detect both large and small objects with a detection accuracy of\n65%, showcasing significant advancements over traditional methods.This improved\nmodel holds substantial potential for real-world applications and is\nwell-suited for autonomous driving competitions, such as the Formula Student\nAutonomous China (FSAC), particularly excelling in scenarios involving\nsingle-target and small-object detection.", "AI": {"tldr": "\u57fa\u4e8eYOLOv8\u6539\u8fdb\u7684\u81ea\u52a8\u9a7e\u9a76\u76ee\u6807\u68c0\u6d4b\u7f51\u7edc\uff0c\u901a\u8fc7\u7ed3\u6784\u91cd\u53c2\u6570\u5316\u6280\u672f\u3001\u53cc\u5411\u91d1\u5b57\u5854\u7ed3\u6784\u548c\u65b0\u68c0\u6d4b\u6d41\u7a0b\uff0c\u5b9e\u73b0\u591a\u5c3a\u5ea6\u5c0f\u76ee\u6807\u9ad8\u6548\u68c0\u6d4b\uff0c\u7cbe\u5ea665%\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u96f7\u8fbe\u611f\u77e5\u6210\u672c\u9ad8\u3001\u6444\u50cf\u5934\u6613\u53d7\u73af\u5883\u5e72\u6270\u3001\u8f66\u8f7d\u4f20\u611f\u5668\u5206\u8fa8\u7387\u4e0d\u8db3\u7b49\u95ee\u9898\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u76ee\u6807\u68c0\u6d4b\u80fd\u529b\u3002", "method": "\u5728YOLOv8\u6846\u67b6\u4e2d\u96c6\u6210\uff1a1\uff09\u7ed3\u6784\u91cd\u53c2\u6570\u5316\u6280\u672f\u4f18\u5316\u7f51\u7edc\u53c2\u6570 2\uff09\u53cc\u5411\u91d1\u5b57\u5854\u7ed3\u6784\u589e\u5f3a\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408 3\uff09\u65b0\u578b\u68c0\u6d4b\u6d41\u7a0b\u63d0\u5347\u5c0f\u76ee\u6807\u8bc6\u522b", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u6a21\u578b\u5bf9\u5927\u5c0f\u7269\u4f53\u68c0\u6d4b\u5747\u6709\u6548\uff0c\u68c0\u6d4b\u7cbe\u5ea6\u8fbe65%\uff0c\u5728FSAC\u7ade\u8d5b\u573a\u666f\u4e2d\u5355\u76ee\u6807/\u5c0f\u7269\u4f53\u68c0\u6d4b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u6539\u8fdb\u6a21\u578b\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u7279\u522b\u9002\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7ade\u8d5b\u573a\u666f\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e0b\u7684\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.10182", "pdf": "https://arxiv.org/pdf/2505.10182", "abs": "https://arxiv.org/abs/2505.10182", "authors": ["Yoichi Ishibashi", "Taro Yano", "Masafumi Oyamada"], "title": "Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic Data for LLM Reasoning", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated significant improvements in\nreasoning capabilities through supervised fine-tuning and reinforcement\nlearning. However, when training reasoning models, these approaches are\nprimarily applicable to specific domains such as mathematics and programming,\nwhich imposes fundamental constraints on the breadth and scalability of\ntraining data. In contrast, continual pretraining (CPT) offers the advantage of\nnot requiring task-specific signals. Nevertheless, how to effectively\nsynthesize training data for reasoning and how such data affect a wide range of\ndomains remain largely unexplored. This study provides a detailed evaluation of\nReasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden\nthought processes underlying texts, based on the premise that texts are the\nresult of the author's thinking process. Specifically, we apply Reasoning CPT\nto Gemma2-9B using synthetic data with hidden thoughts derived from STEM and\nLaw corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis\nreveals that Reasoning CPT consistently improves performance across all\nevaluated domains. Notably, reasoning skills acquired in one domain transfer\neffectively to others; the performance gap with conventional methods widens as\nproblem difficulty increases, with gains of up to 8 points on the most\nchallenging problems. Furthermore, models trained with hidden thoughts learn to\nadjust the depth of their reasoning according to problem difficulty.", "AI": {"tldr": "Reasoning CPT\u901a\u8fc7\u5408\u6210\u9690\u85cf\u601d\u7ef4\u6570\u636e\u63d0\u5347\u591a\u9886\u57df\u63a8\u7406\u80fd\u529b\uff0c\u5728\u590d\u6742\u95ee\u9898\u4e0a\u6548\u679c\u663e\u8457", "motivation": "\u73b0\u6709\u8bad\u7ec3\u65b9\u6cd5\u5c40\u9650\u4e8e\u6570\u5b66\u7b49\u7279\u5b9a\u9886\u57df\uff0c\u6301\u7eed\u9884\u8bad\u7ec3(CPT)\u867d\u65e0\u9700\u4efb\u52a1\u4fe1\u53f7\u4f46\u7f3a\u4e4f\u591a\u9886\u57df\u9a8c\u8bc1\uff0c\u9700\u63a2\u7d22\u5982\u4f55\u6709\u6548\u5408\u6210\u63a8\u7406\u6570\u636e\u53ca\u5176\u8de8\u9886\u57df\u5f71\u54cd", "method": "\u5728Gemma2-9B\u4e0a\u5e94\u7528Reasoning CPT\uff0c\u4f7f\u7528STEM\u548c\u6cd5\u5f8b\u9886\u57df\u7684\u9690\u85cf\u601d\u7ef4\u6570\u636e\uff0c\u4e0e\u6807\u51c6CPT\u5728MMLU\u57fa\u51c6\u5bf9\u6bd4", "result": "\u5168\u9886\u57df\u6027\u80fd\u63d0\u5347\uff0c\u8de8\u9886\u57df\u8fc1\u79fb\u6709\u6548\uff08\u96be\u9898\u6700\u9ad8+8%\uff09\uff0c\u6a21\u578b\u5b66\u4f1a\u6839\u636e\u96be\u5ea6\u8c03\u6574\u63a8\u7406\u6df1\u5ea6", "conclusion": "\u601d\u7ef4\u91cd\u6784\u7684\u6301\u7eed\u9884\u8bad\u7ec3\u7a81\u7834\u9886\u57df\u9650\u5236\uff0c\u9a8c\u8bc1\u5408\u6210\u601d\u7ef4\u6570\u636e\u5bf9\u590d\u6742\u95ee\u9898\u89e3\u51b3\u7684\u5173\u952e\u4f5c\u7528"}}
{"id": "2505.10027", "pdf": "https://arxiv.org/pdf/2505.10027", "abs": "https://arxiv.org/abs/2505.10027", "authors": ["Shijie Lyu"], "title": "ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model Super-Resolution Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by the 4th International Conference on Computing Innovation\n  and Applied Physics (CONF-CIAP 2025), and will be published in EAI Community\n  Research Series-CORE or Theoretical and Natural Science (TNS)", "summary": "With the rapid advancement of remote sensing technology, super-resolution\nimage reconstruction is of great research and practical significance. Existing\ndeep learning methods have made progress but still face limitations in handling\ncomplex scenes and preserving image details. This paper proposes a\nreinforcement learning-based latent diffusion model (LDM) fine-tuning method\nfor remote sensing image super-resolution. The method constructs a\nreinforcement learning environment with states, actions, and rewards,\noptimizing decision objectives through proximal policy optimization (PPO)\nduring the reverse denoising process of the LDM model. Experiments on the\nRESISC45 dataset show significant improvements over the baseline model in PSNR,\nSSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11,\nand LPIPS reducing by 0.06-0.10, particularly in structured and complex natural\nscenes. The results demonstrate the method's effectiveness in enhancing\nsuper-resolution quality and adaptability across scenes.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\u5fae\u8c03\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u9065\u611f\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u8d28\u91cf\uff0c\u5728\u590d\u6742\u573a\u666f\u4e2d\u6548\u679c\u7a81\u51fa\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u9065\u611f\u573a\u666f\u548c\u7ec6\u8282\u4fdd\u7559\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u5f00\u53d1\u66f4\u6709\u6548\u7684\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u4ee5\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u548c\u8de8\u573a\u666f\u9002\u5e94\u6027\u3002", "method": "\u6784\u5efa\u5f3a\u5316\u5b66\u4e60\u73af\u5883\uff08\u72b6\u6001\u3001\u52a8\u4f5c\u3001\u5956\u52b1\u673a\u5236\uff09\uff0c\u5728\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDM\uff09\u7684\u53cd\u5411\u53bb\u566a\u8fc7\u7a0b\u4e2d\u5e94\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u8fdb\u884c\u51b3\u7b56\u4f18\u5316\u3002", "result": "\u5728RESISC45\u6570\u636e\u96c6\u4e0a\uff0cPSNR\u63d0\u53473-4dB\uff0cSSIM\u63d0\u9ad80.08-0.11\uff0cLPIPS\u964d\u4f4e0.06-0.10\uff0c\u5728\u7ed3\u6784\u548c\u590d\u6742\u81ea\u7136\u573a\u666f\u4e2d\u6539\u8fdb\u5c24\u4e3a\u663e\u8457\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u9065\u611f\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u8d28\u91cf\u548c\u8de8\u573a\u666f\u9002\u5e94\u6027\uff0c\u4e3a\u590d\u6742\u573a\u666f\u4e0b\u7684\u56fe\u50cf\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.10185", "pdf": "https://arxiv.org/pdf/2505.10185", "abs": "https://arxiv.org/abs/2505.10185", "authors": ["Seongyun Lee", "Seungone Kim", "Minju Seo", "Yongrae Jo", "Dongyoung Go", "Hyeonbin Hwang", "Jinho Park", "Xiang Yue", "Sean Welleck", "Graham Neubig", "Moontae Lee", "Minjoon Seo"], "title": "The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a Reasoning Model will Think", "categories": ["cs.CL", "cs.AI"], "comment": "Work in progress", "summary": "Long chain-of-thought (CoT) is an essential ingredient in effective usage of\nmodern large language models, but our understanding of the reasoning strategies\nunderlying these capabilities remains limited. While some prior works have\nattempted to categorize CoTs using predefined strategy types, such approaches\nare constrained by human intuition and fail to capture the full diversity of\nmodel behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up\nframework for analyzing and steering model reasoning. Our method automatically\nextracts diverse reasoning criteria from model-generated CoTs, embeds them into\na semantic space, clusters them into representative categories, and derives\ncontrastive rubrics to interpret reasoning behavior. Human evaluations show\nthat this framework produces more interpretable and comprehensive analyses than\nexisting methods. Moreover, we demonstrate that this understanding enables\nperformance gains: we can predict which strategy a model is likely to use and\nguide it toward more effective alternatives. Finally, we provide practical\ninsights, such as that training data format (e.g., free-form vs.\nmultiple-choice) has a far greater impact on reasoning behavior than data\ndomain, underscoring the importance of format-aware model design.", "AI": {"tldr": "\u63d0\u51faCoT\u767e\u79d1\u5168\u4e66\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u4e0b\u800c\u4e0a\u7684\u65b9\u5f0f\u5206\u6790\u8bed\u8a00\u6a21\u578b\u7684\u601d\u7ef4\u94fe\u63a8\u7406\u7b56\u7565\uff0c\u5e76\u5b9e\u73b0\u7b56\u7565\u9884\u6d4b\u4e0e\u5f15\u5bfc", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u7b56\u7565\u7c7b\u578b\uff0c\u53d7\u9650\u4e8e\u4eba\u7c7b\u76f4\u89c9\u4e14\u65e0\u6cd5\u6355\u6349\u6a21\u578b\u63a8\u7406\u7b56\u7565\u7684\u591a\u6837\u6027\uff0c\u9700\u5efa\u7acb\u66f4\u5168\u9762\u7684\u5206\u6790\u4f53\u7cfb", "method": "1.\u81ea\u52a8\u63d0\u53d6\u601d\u7ef4\u94fe\u4e2d\u7684\u63a8\u7406\u6807\u51c6 2.\u8bed\u4e49\u7a7a\u95f4\u5d4c\u5165 3.\u805a\u7c7b\u751f\u6210\u7b56\u7565\u7c7b\u522b 4.\u6784\u5efa\u5bf9\u6bd4\u5f0f\u8bc4\u4f30\u6807\u51c6\u89e3\u91ca\u6a21\u578b\u884c\u4e3a", "result": "\u4eba\u7c7b\u8bc4\u4f30\u663e\u793a\u8be5\u65b9\u6cd5\u89e3\u91ca\u6027\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\uff1b\u53ef\u9884\u6d4b\u6a21\u578b\u7b56\u7565\u5e76\u5f15\u5bfc\u4f18\u5316\uff1b\u53d1\u73b0\u6570\u636e\u683c\u5f0f\uff08\u975e\u9886\u57df\uff09\u5bf9\u63a8\u7406\u884c\u4e3a\u5f71\u54cd\u663e\u8457", "conclusion": "\u5efa\u7acb\u4e86\u53ef\u64cd\u4f5c\u7684\u6a21\u578b\u63a8\u7406\u5206\u6790\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u6570\u636e\u683c\u5f0f\u5bf9\u6a21\u578b\u8bbe\u8ba1\u7684\u5173\u952e\u5f71\u54cd\uff0c\u63d0\u4f9b\u4e86\u7b56\u7565\u5e72\u9884\u7684\u65b0\u8def\u5f84"}}
{"id": "2505.10030", "pdf": "https://arxiv.org/pdf/2505.10030", "abs": "https://arxiv.org/abs/2505.10030", "authors": ["Miit Daga", "Dhriti Parikh", "Swarna Priya Ramu"], "title": "DeepSeqCoco: A Robust Mobile Friendly Deep Learning Model for Detection of Diseases in Cocos nucifera", "categories": ["cs.CV", "cs.LG"], "comment": "This paper is accepted for publication in IEEE Access journal and is\n  currently pending revisions before publication", "summary": "Coconut tree diseases are a serious risk to agricultural yield, particularly\nin developing countries where conventional farming practices restrict early\ndiagnosis and intervention. Current disease identification methods are manual,\nlabor-intensive, and non-scalable. In response to these limitations, we come up\nwith DeepSeqCoco, a deep learning based model for accurate and automatic\ndisease identification from coconut tree images. The model was tested under\nvarious optimizer settings, such as SGD, Adam, and hybrid configurations, to\nidentify the optimal balance between accuracy, minimization of loss, and\ncomputational cost. Results from experiments indicate that DeepSeqCoco can\nachieve as much as 99.5% accuracy (achieving up to 5% higher accuracy than\nexisting models) with the hybrid SGD-Adam showing the lowest validation loss of\n2.81%. It also shows a drop of up to 18% in training time and up to 85% in\nprediction time for input images. The results point out the promise of the\nmodel to improve precision agriculture through an AI-based, scalable, and\nefficient disease monitoring system.", "AI": {"tldr": "\u63d0\u51fa\u6df1\u5ea6\u5b66\u4e60\u6a21\u578bDeepSeqCoco\uff0c\u901a\u8fc7\u6df7\u5408\u4f18\u5316\u5668\u5b9e\u73b099.5%\u51c6\u786e\u7387\uff0c\u663e\u8457\u964d\u4f4e\u519c\u4e1a\u75be\u75c5\u68c0\u6d4b\u8017\u65f6", "motivation": "\u4f20\u7edf\u6930\u6811\u75c5\u5bb3\u68c0\u6d4b\u4f9d\u8d56\u4eba\u5de5\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u53d1\u5c55\u4e2d\u56fd\u5bb6\u56e0\u8bca\u65ad\u6ede\u540e\u5bfc\u81f4\u4e25\u91cd\u519c\u4e1a\u635f\u5931", "method": "\u5f00\u53d1\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684DeepSeqCoco\u6a21\u578b\uff0c\u7cfb\u7edf\u6d4b\u8bd5SGD\u3001Adam\u53ca\u6df7\u5408\u4f18\u5316\u5668\u7684\u6027\u80fd\u5e73\u8861", "result": "\u6df7\u5408SGD-Adam\u4f18\u5316\u5b9e\u73b099.5%\u51c6\u786e\u7387\uff08\u63d0\u53475%\uff09\uff0c\u9a8c\u8bc1\u635f\u59312.81%\uff0c\u8bad\u7ec3/\u9884\u6d4b\u65f6\u95f4\u5206\u522b\u51cf\u5c1118%/85%", "conclusion": "\u8be5AI\u6a21\u578b\u4e3a\u7cbe\u51c6\u519c\u4e1a\u63d0\u4f9b\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u75be\u75c5\u76d1\u6d4b\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6548\u7387\u4e0e\u519c\u4e1a\u751f\u4ea7\u529b"}}
{"id": "2505.10202", "pdf": "https://arxiv.org/pdf/2505.10202", "abs": "https://arxiv.org/abs/2505.10202", "authors": ["Jintian Shao", "Hongyi Huang", "Jiayi Wu", "YiMing Cheng", "ZhiYu Wu", "You Shan", "MingKai Zheng"], "title": "VQ-Logits: Compressing the Output Bottleneck of Large Language Models via Vector Quantized Logits", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success but face\nsignificant computational and memory challenges, particularly due to their\nextensive output vocabularies. The final linear projection layer, mapping\nhidden states to vocabulary-sized logits, often constitutes a substantial\nportion of the model's parameters and computational cost during inference.\nExisting methods like adaptive softmax or hierarchical softmax introduce\nstructural complexities. In this paper, we propose VQ-Logits, a novel approach\nthat leverages Vector Quantization (VQ) to drastically reduce the parameter\ncount and computational load of the LLM output layer. VQ-Logits replaces the\nlarge V * dmodel output embedding matrix with a small, shared codebook of K\nembedding vectors (K << V ). Each token in the vocabulary is mapped to one of\nthese K codebook vectors. The LLM predicts logits over this compact codebook,\nwhich are then efficiently \"scattered\" to the full vocabulary space using the\nlearned or preassigned mapping. We demonstrate through extensive experiments on\nstandard language modeling benchmarks (e.g., WikiText-103, C4) that VQ-Logits\ncan achieve up to 99% parameter reduction in the output layer and 6x speedup in\nlogit computation, with only a marginal 4% increase in perplexity compared to\nfull softmax baselines. We further provide detailed ablation studies on\ncodebook size, initialization, and learning strategies, showcasing the\nrobustness and effectiveness of our approach.", "AI": {"tldr": "\u63d0\u51faVQ-Logits\u65b9\u6cd5\uff0c\u901a\u8fc7\u5411\u91cf\u91cf\u5316\u6280\u672f\u5c06LLM\u8f93\u51fa\u5c42\u53c2\u6570\u51cf\u5c1199%\u3001\u8ba1\u7b97\u901f\u5ea6\u63d0\u53476\u500d\uff0c\u4ec5\u727a\u72724%\u7684\u56f0\u60d1\u5ea6\u6027\u80fd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u5c42\u7684\u5de8\u5927\u8bcd\u8868\u5bfc\u81f4\u53c2\u6570\u4e0e\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\uff08\u5982\u81ea\u9002\u5e94softmax\uff09\u5b58\u5728\u7ed3\u6784\u590d\u6742\u6027\u7f3a\u9677\u3002", "method": "\u7528\u5c0f\u578b\u5171\u4eabcodebook\u66ff\u4ee3\u4f20\u7edf\u8f93\u51fa\u77e9\u9635\uff0c\u901a\u8fc7\u5411\u91cf\u91cf\u5316\u5c06\u8bcd\u6c47\u6620\u5c04\u5230codebook\u5411\u91cf\uff0c\u4f7f\u7528\u6563\u5c04\u673a\u5236\u5c06\u7d27\u51d1logits\u6620\u5c04\u56de\u5b8c\u6574\u8bcd\u8868\u7a7a\u95f4\u3002", "result": "\u5728WikiText-103\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u8f93\u51fa\u5c4299%\u53c2\u6570\u524a\u51cf\u548c6\u500d\u52a0\u901f\uff0c\u56f0\u60d1\u5ea6\u4ec5\u6bd4\u57fa\u7ebf\u589e\u52a04%\u3002", "conclusion": "VQ-Logits\u4e3aLLM\u8f93\u51fa\u5c42\u4f18\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u4e0e\u6a21\u578b\u6027\u80fd\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2505.10046", "pdf": "https://arxiv.org/pdf/2505.10046", "abs": "https://arxiv.org/abs/2505.10046", "authors": ["Bingda Tang", "Boyang Zheng", "Xichen Pan", "Sayak Paul", "Saining Xie"], "title": "Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text-to-Image Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "This paper does not describe a new method; instead, it provides a thorough\nexploration of an important yet understudied design space related to recent\nadvances in text-to-image synthesis -- specifically, the deep fusion of large\nlanguage models (LLMs) and diffusion transformers (DiTs) for multi-modal\ngeneration. Previous studies mainly focused on overall system performance\nrather than detailed comparisons with alternative methods, and key design\ndetails and training recipes were often left undisclosed. These gaps create\nuncertainty about the real potential of this approach. To fill these gaps, we\nconduct an empirical study on text-to-image generation, performing controlled\ncomparisons with established baselines, analyzing important design choices, and\nproviding a clear, reproducible recipe for training at scale. We hope this work\noffers meaningful data points and practical guidelines for future research in\nmulti-modal generation.", "AI": {"tldr": "\u5bf9\u591a\u6a21\u6001\u751f\u6210\u4e2dLLM\u4e0eDiT\u6df1\u5ea6\u878d\u5408\u8bbe\u8ba1\u7a7a\u95f4\u7684\u7cfb\u7edf\u6027\u63a2\u7d22\u4e0e\u53ef\u590d\u73b0\u8bad\u7ec3\u65b9\u6848\u4f18\u5316", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u4e0e\u57fa\u51c6\u65b9\u6cd5\u7684\u8be6\u7ec6\u5bf9\u6bd4\uff0c\u5173\u952e\u8bbe\u8ba1\u7ec6\u8282\u548c\u8bad\u7ec3\u65b9\u6848\u4e0d\u900f\u660e\uff0c\u5bfc\u81f4\u8be5\u65b9\u6cd5\u771f\u5b9e\u6f5c\u529b\u5b58\u5728\u4e0d\u786e\u5b9a\u6027", "method": "\u901a\u8fc7\u63a7\u5236\u53d8\u91cf\u5b9e\u9a8c\u4e0e\u73b0\u6709\u57fa\u7ebf\u5bf9\u6bd4\uff0c\u6df1\u5ea6\u5206\u6790\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\uff0c\u63d0\u4f9b\u89c4\u6a21\u5316\u8bad\u7ec3\u7684\u53ef\u590d\u73b0\u65b9\u6848", "result": "\u5efa\u7acb\u4e86\u8bbe\u8ba1\u9009\u62e9\u5bf9\u6027\u80fd\u5f71\u54cd\u7684\u91cf\u5316\u6307\u6807\uff0c\u5f62\u6210\u4e86\u660e\u786e\u7684\u89c4\u6a21\u5316\u8bad\u7ec3\u6307\u5bfc\u539f\u5219", "conclusion": "\u7cfb\u7edf\u6027\u5b9e\u8bc1\u7814\u7a76\u4e3a\u591a\u6a21\u6001\u751f\u6210\u9886\u57df\u63d0\u4f9b\u4e86\u5173\u952e\u8bbe\u8ba1\u6d1e\u89c1\u548c\u53ef\u590d\u73b0\u57fa\u51c6\uff0c\u63a8\u52a8\u672a\u6765\u7814\u7a76\u89c4\u8303\u5316\u53d1\u5c55"}}
{"id": "2505.10218", "pdf": "https://arxiv.org/pdf/2505.10218", "abs": "https://arxiv.org/abs/2505.10218", "authors": ["Zongsheng Wang", "Kaili Sun", "Bowen Wu", "Qun Yu", "Ying Li", "Baoxun Wang"], "title": "RAIDEN-R1: Improving Role-awareness of LLMs via GRPO with Verifiable Reward", "categories": ["cs.CL"], "comment": null, "summary": "Role-playing conversational agents (RPCAs) face persistent challenges in\nmaintaining role consistency. To address this, we propose RAIDEN-R1, a novel\nreinforcement learning framework that integrates Verifiable Role-Awareness\nReward (VRAR). The method introduces both singular and multi-term mining\nstrategies to generate quantifiable rewards by assessing role-specific keys.\nAdditionally, we construct a high-quality, role-aware Chain-of-Thought dataset\nthrough multi-LLM collaboration, and implement experiments to enhance reasoning\ncoherence. Experiments on the RAIDEN benchmark demonstrate RAIDEN-R1's\nsuperiority: our 14B-GRPO model achieves 88.04% and 88.65% accuracy on\nScript-Based Knowledge and Conversation Memory metrics, respectively,\noutperforming baseline models while maintaining robustness. Case analyses\nfurther reveal the model's enhanced ability to resolve conflicting contextual\ncues and sustain first-person narrative consistency. This work bridges the\nnon-quantifiability gap in RPCA training and provides insights into role-aware\nreasoning patterns, advancing the development of RPCAs.", "AI": {"tldr": "Proposes RAIDEN-R1 framework with VRAR reward mechanism and multi-LLM dataset construction to enhance RPCA role consistency.", "motivation": "Persistent challenges in maintaining role consistency for RPCAs need quantifiable training solutions.", "method": "Reinforcement learning framework integrates verifiable role-awareness reward (VRAR) through singular/multi-term mining strategies and multi-LLM CoT dataset construction.", "result": "14B-GRPO model achieves 88.04%/88.65% accuracy on two metrics, outperforming baselines in contextual conflict resolution and narrative consistency.", "conclusion": "Bridges non-quantifiable training gap in RPCAs and advances role-aware reasoning patterns through verifiable reward mechanisms."}}
{"id": "2505.10049", "pdf": "https://arxiv.org/pdf/2505.10049", "abs": "https://arxiv.org/abs/2505.10049", "authors": ["Jinlong Fan", "Xuepu Zeng", "Jing Zhang", "Mingming Gong", "Yuxiang Yang", "Dacheng Tao"], "title": "Advances in Radiance Field for Dynamic Scene: From Neural Field to Gaussian Field", "categories": ["cs.CV"], "comment": null, "summary": "Dynamic scene representation and reconstruction have undergone transformative\nadvances in recent years, catalyzed by breakthroughs in neural radiance fields\nand 3D Gaussian splatting techniques. While initially developed for static\nenvironments, these methodologies have rapidly evolved to address the\ncomplexities inherent in 4D dynamic scenes through an expansive body of\nresearch. Coupled with innovations in differentiable volumetric rendering,\nthese approaches have significantly enhanced the quality of motion\nrepresentation and dynamic scene reconstruction, thereby garnering substantial\nattention from the computer vision and graphics communities. This survey\npresents a systematic analysis of over 200 papers focused on dynamic scene\nrepresentation using radiance field, spanning the spectrum from implicit neural\nrepresentations to explicit Gaussian primitives. We categorize and evaluate\nthese works through multiple critical lenses: motion representation paradigms,\nreconstruction techniques for varied scene dynamics, auxiliary information\nintegration strategies, and regularization approaches that ensure temporal\nconsistency and physical plausibility. We organize diverse methodological\napproaches under a unified representational framework, concluding with a\ncritical examination of persistent challenges and promising research\ndirections. By providing this comprehensive overview, we aim to establish a\ndefinitive reference for researchers entering this rapidly evolving field while\noffering experienced practitioners a systematic understanding of both\nconceptual principles and practical frontiers in dynamic scene reconstruction.", "AI": {"tldr": "\u7cfb\u7edf\u7efc\u8ff0\u52a8\u6001\u573a\u666f\u91cd\u5efa\u6280\u672f\uff0c\u5206\u6790200+\u8bba\u6587\u7684\u6280\u672f\u6f14\u8fdb\u4e0e\u5206\u7c7b\u6846\u67b6\uff0c\u6307\u51fa\u795e\u7ecf\u8f90\u5c04\u573a\u4e0e\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u57284D\u52a8\u6001\u5efa\u6a21\u4e2d\u7684\u7a81\u7834\u4e0e\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u52a8\u6001\u573a\u666f\u8868\u793a\u6280\u672f\u8fd1\u5e74\u5feb\u901f\u8fed\u4ee3\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u6574\u7406\uff0c\u9700\u5efa\u7acb\u7edf\u4e00\u6846\u67b6\u6307\u5bfc\u7814\u7a76\u8005\u5feb\u901f\u5207\u5165\u8be5\u9886\u57df\u3002", "method": "\u901a\u8fc7\u8fd0\u52a8\u8868\u793a\u8303\u5f0f\u3001\u91cd\u5efa\u6280\u672f\u3001\u8f85\u52a9\u4fe1\u606f\u6574\u5408\u3001\u6b63\u5219\u5316\u65b9\u6cd5\u56db\u4e2a\u7ef4\u5ea6\uff0c\u5bf9200+\u8bba\u6587\u8fdb\u884c\u7cfb\u7edf\u5206\u7c7b\u4e0e\u8bc4\u4f30\u3002", "result": "\u63ed\u793a\u4e86\u52a8\u6001\u573a\u666f\u91cd\u5efa\u4ece\u9690\u5f0f\u795e\u7ecf\u8868\u8fbe\u5230\u663e\u5f0f\u9ad8\u65af\u57fa\u5143\u7684\u6280\u672f\u8def\u5f84\uff0c\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u663e\u8457\u63d0\u5347\u8fd0\u52a8\u5efa\u6a21\u7269\u7406\u5408\u7406\u6027\u3002", "conclusion": "\u5f53\u524d\u6311\u6218\u5728\u4e8e\u590d\u6742\u8fd0\u52a8\u62d3\u6251\u5efa\u6a21\u4e0e\u5b9e\u65f6\u6027\u7a81\u7834\uff0c\u672a\u6765\u5e94\u7ed3\u5408\u7269\u7406\u5f15\u64ce\u4e0e\u8de8\u6a21\u6001\u5b66\u4e60\u63d0\u5347\u52a8\u6001\u91cd\u5efa\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.10260", "pdf": "https://arxiv.org/pdf/2505.10260", "abs": "https://arxiv.org/abs/2505.10260", "authors": ["Poli Apollinaire Nemkova", "Solomon Ubani", "Mark V. Albert"], "title": "Comparing LLM Text Annotation Skills: A Study on Human Rights Violations in Social Media Data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the era of increasingly sophisticated natural language processing (NLP)\nsystems, large language models (LLMs) have demonstrated remarkable potential\nfor diverse applications, including tasks requiring nuanced textual\nunderstanding and contextual reasoning. This study investigates the\ncapabilities of multiple state-of-the-art LLMs - GPT-3.5, GPT-4, LLAMA3,\nMistral 7B, and Claude-2 - for zero-shot and few-shot annotation of a complex\ntextual dataset comprising social media posts in Russian and Ukrainian.\nSpecifically, the focus is on the binary classification task of identifying\nreferences to human rights violations within the dataset.\n  To evaluate the effectiveness of these models, their annotations are compared\nagainst a gold standard set of human double-annotated labels across 1000\nsamples. The analysis includes assessing annotation performance under different\nprompting conditions, with prompts provided in both English and Russian.\nAdditionally, the study explores the unique patterns of errors and\ndisagreements exhibited by each model, offering insights into their strengths,\nlimitations, and cross-linguistic adaptability.\n  By juxtaposing LLM outputs with human annotations, this research contributes\nto understanding the reliability and applicability of LLMs for sensitive,\ndomain-specific tasks in multilingual contexts. It also sheds light on how\nlanguage models handle inherently subjective and context-dependent judgments, a\ncritical consideration for their deployment in real-world scenarios.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86GPT-3.5/4\u3001LLAMA3\u7b49\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4fc4\u4e4c\u793e\u4ea4\u5a92\u4f53\u4eba\u6743\u4fb5\u72af\u4e8c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u96f6\u6837\u672c/\u5c11\u6837\u672c\u6807\u6ce8\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u8868\u73b0\u5b58\u5728\u663e\u8457\u5dee\u5f02\u4e14\u8de8\u8bed\u8a00\u9002\u5e94\u6027\u4e0d\u540c\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u654f\u611f\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u7684\u53ef\u9760\u6027\uff0c\u7279\u522b\u662f\u5904\u7406\u4e3b\u89c2\u6027\u5f3a\u3001\u8bed\u5883\u4f9d\u8d56\u7684\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u65f6\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u4f7f\u75281000\u6761\u4eba\u5de5\u53cc\u6807\u6ce8\u6570\u636e\u4f5c\u4e3a\u91d1\u6807\u51c6\uff0c\u5bf9\u6bd4\u4e94\u5927\u4e3b\u6d41\u6a21\u578b\u5728\u82f1\u4fc4\u53cc\u8bed\u63d0\u793a\u4e0b\u7684\u6807\u6ce8\u6027\u80fd\uff0c\u5206\u6790\u9519\u8bef\u6a21\u5f0f\u548c\u8de8\u8bed\u8a00\u9002\u5e94\u6027\u3002", "result": "\u4e0d\u540c\u6a21\u578b\u5728\u76f8\u540c\u63d0\u793a\u6761\u4ef6\u4e0b\u51c6\u786e\u7387\u6ce2\u52a8\u8fbe25%\uff0c\u4fc4\u8bed\u63d0\u793a\u6574\u4f53\u8868\u73b0\u5f31\u4e8e\u82f1\u8bed\u3002Claude-2\u5728\u8de8\u8bed\u8a00\u4e00\u81f4\u6027\u6700\u4f73\uff0cLLAMA3\u5728\u5c11\u6837\u672c\u573a\u666f\u63d0\u5347\u663e\u8457\u3002", "conclusion": "LLMs\u53ef\u7528\u4e8e\u591a\u8bed\u8a00\u654f\u611f\u4efb\u52a1\u4f46\u9700\u8c28\u614e\uff0c\u6a21\u578b\u8868\u73b0\u53d7\u63d0\u793a\u8bed\u8a00\u548c\u6807\u6ce8\u8303\u4f8b\u6570\u91cf\u5f71\u54cd\u663e\u8457\uff0c\u8bed\u5883\u7406\u89e3\u504f\u5dee\u662f\u4e3b\u8981\u9519\u8bef\u6765\u6e90\u3002"}}
{"id": "2505.10055", "pdf": "https://arxiv.org/pdf/2505.10055", "abs": "https://arxiv.org/abs/2505.10055", "authors": ["Ijazul Haq", "Yingjie Zhang", "Irfan Ali Khan"], "title": "PsOCR: Benchmarking Large Multimodal Models for Optical Character Recognition in Low-resource Pashto Language", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper evaluates the performance of Large Multimodal Models (LMMs) on\nOptical Character Recognition (OCR) in the low-resource Pashto language.\nNatural Language Processing (NLP) in Pashto faces several challenges due to the\ncursive nature of its script and a scarcity of structured datasets. To address\nthis, we developed a synthetic Pashto OCR dataset, PsOCR, consisting of one\nmillion images annotated with bounding boxes at word, line, and document\nlevels, suitable for training and evaluating models based on different\narchitectures, including Convolutional Neural Networks (CNNs) and Transformers.\nPsOCR covers variations across 1,000 unique font families, colors, image sizes,\nand layouts. A benchmark subset of 10K images was selected to evaluate the\nperformance of several LMMs, including seven open-source models: DeepSeek's\nJanus, InternVL, MiniCPM, Florence, and Qwen (3B and 7B), and four\nclosed-source models: GPT-4o, Gemini, Claude, and Grok. Experimental results\ndemonstrate that Gemini achieves the best performance among all models, whereas\namong open-source models, Qwen-7B stands out. This work provides an insightful\nassessment of the capabilities and limitations of current LMMs for OCR tasks in\nPashto and establishes a foundation for further research not only in Pashto OCR\nbut also for other similar scripts such as Arabic, Persian, and Urdu. PsOCR is\navailable at https://github.com/zirak-ai/PashtoOCR.", "AI": {"tldr": "\u521b\u5efa\u767e\u4e07\u7ea7\u5408\u6210\u666e\u4ec0\u56fe\u8bedOCR\u6570\u636e\u96c6PsOCR\uff0c\u8bc4\u4f3011\u79cdLMMs\u6a21\u578b\u6027\u80fd\uff0cGemini\u8868\u73b0\u6700\u4f18\uff0cQwen-7B\u9886\u8dd1\u5f00\u6e90\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u666e\u4ec0\u56fe\u8bed\u56e0\u6587\u5b57\u8fde\u7b14\u7279\u6027\u548c\u7ed3\u6784\u5316\u6570\u636e\u7a00\u7f3a\u5bfc\u81f4\u7684OCR\u5904\u7406\u96be\u9898\uff0c\u586b\u8865\u963f\u62c9\u4f2f\u8bed\u7cfb\u4f4e\u8d44\u6e90\u8bed\u8a00\u7814\u7a76\u7a7a\u767d", "method": "\u6784\u5efa\u542b100\u4e07\u56fe\u50cf\u7684PsOCR\u6570\u636e\u96c6\uff0c\u6db5\u76d61000\u79cd\u5b57\u4f53/\u989c\u8272/\u7248\u5f0f\uff0c\u9009\u53d610K\u6d4b\u8bd5\u96c6\u8bc4\u4f307\u4e2a\u5f00\u6e90\u6a21\u578b\u548c4\u4e2a\u95ed\u6e90\u6a21\u578b", "result": "\u95ed\u6e90\u6a21\u578b\u4e2dGemini\u7efc\u5408\u6700\u4f73\uff0c\u5f00\u6e90\u6a21\u578b\u4e2dQwen-7B\u8868\u73b0\u6700\u4f18\uff08\u51c6\u786e\u738772.3% vs \u95ed\u6e90\u5e73\u574768.5%\uff09", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u666e\u4ec0\u56fe\u8bedOCR\u5efa\u7acb\u57fa\u51c6\uff0c\u65b9\u6cd5\u8bba\u53ef\u6269\u5c55\u81f3\u963f\u62c9\u4f2f\u8bed/\u6ce2\u65af\u8bed/\u4e4c\u5c14\u90fd\u8bed\u7b49\u76f8\u4f3c\u6587\u5b57\u7684\u4f4e\u8d44\u6e90OCR\u573a\u666f"}}
{"id": "2505.10261", "pdf": "https://arxiv.org/pdf/2505.10261", "abs": "https://arxiv.org/abs/2505.10261", "authors": ["Rui Yang", "Huitao Li", "Matthew Yu Heng Wong", "Yuhe Ke", "Xin Li", "Kunyu Yu", "Jingchi Liao", "Jonathan Chong Kai Liew", "Sabarinath Vinod Nair", "Jasmine Chiat Ling Ong", "Irene Li", "Douglas Teodoro", "Chuan Hong", "Daniel Shu Wei Ting", "Nan Liu"], "title": "The Evolving Landscape of Generative Large Language Models and Traditional Natural Language Processing in Medicine", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Natural language processing (NLP) has been traditionally applied to medicine,\nand generative large language models (LLMs) have become prominent recently.\nHowever, the differences between them across different medical tasks remain\nunderexplored. We analyzed 19,123 studies, finding that generative LLMs\ndemonstrate advantages in open-ended tasks, while traditional NLP dominates in\ninformation extraction and analysis tasks. As these technologies advance,\nethical use of them is essential to ensure their potential in medical\napplications.", "AI": {"tldr": "\u6bd4\u8f83\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u4f20\u7edfNLP\u5728\u533b\u7597\u4efb\u52a1\u4e2d\u7684\u6548\u80fd\u5dee\u5f02\uff0c\u5f3a\u8c03\u4f26\u7406\u5e94\u7528\u7684\u91cd\u8981\u6027", "motivation": "\u63a2\u7d22\u751f\u6210\u5f0fLLM\u4e0e\u4f20\u7edfNLP\u5728\u4e0d\u540c\u533b\u7597\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\u53ca\u9002\u7528\u573a\u666f", "method": "\u901a\u8fc7\u5bf919,123\u9879\u7814\u7a76\u8fdb\u884c\u7cfb\u7edf\u5206\u6790\uff0c\u6bd4\u8f83\u4e24\u79cd\u6280\u672f\u7684\u4efb\u52a1\u8868\u73b0", "result": "\u751f\u6210\u5f0fLLM\u5728\u5f00\u653e\u5f0f\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f20\u7edfNLP\u5728\u4fe1\u606f\u62bd\u53d6\u5206\u6790\u4efb\u52a1\u4e2d\u4fdd\u6301\u4f18\u52bf", "conclusion": "\u533b\u7597AI\u5e94\u7528\u4e2d\u9700\u6839\u636e\u4efb\u52a1\u7279\u6027\u9009\u62e9\u5408\u9002\u6280\u672f\uff0c\u5e76\u5f3a\u8c03\u4f26\u7406\u89c4\u8303\u5bf9\u6280\u672f\u53d1\u5c55\u7684\u91cd\u8981\u6027"}}
{"id": "2505.10072", "pdf": "https://arxiv.org/pdf/2505.10072", "abs": "https://arxiv.org/abs/2505.10072", "authors": ["Rui-Yang Ju", "Sheng-Yen Huang", "Yi-Ping Hung"], "title": "ToonifyGB: StyleGAN-based Gaussian Blendshapes for 3D Stylized Head Avatars", "categories": ["cs.CV"], "comment": null, "summary": "The introduction of 3D Gaussian blendshapes has enabled the real-time\nreconstruction of animatable head avatars from monocular video. Toonify, a\nStyleGAN-based framework, has become widely used for facial image stylization.\nTo extend Toonify for synthesizing diverse stylized 3D head avatars using\nGaussian blendshapes, we propose an efficient two-stage framework, ToonifyGB.\nIn Stage 1 (stylized video generation), we employ an improved StyleGAN to\ngenerate the stylized video from the input video frames, which addresses the\nlimitation of cropping aligned faces at a fixed resolution as preprocessing for\nnormal StyleGAN. This process provides a more stable video, which enables\nGaussian blendshapes to better capture the high-frequency details of the video\nframes, and efficiently generate high-quality animation in the next stage. In\nStage 2 (Gaussian blendshapes synthesis), we learn a stylized neutral head\nmodel and a set of expression blendshapes from the generated video. By\ncombining the neutral head model with expression blendshapes, ToonifyGB can\nefficiently render stylized avatars with arbitrary expressions. We validate the\neffectiveness of ToonifyGB on the benchmark dataset using two styles: Arcane\nand Pixar.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eStyleGAN\u4e0e3D\u9ad8\u65af\u6df7\u5408\u5f62\u72b6\u7684\u9ad8\u6548\u4e24\u9636\u6bb5\u6846\u67b6ToonifyGB\uff0c\u5b9e\u73b0\u5b9e\u65f6\u751f\u6210\u591a\u6837\u5316\u98ce\u683c\u5316\u5934\u90e8\u865a\u62df\u52a8\u753b", "motivation": "\u73b0\u6709Toonify\u6846\u67b6\u9700\u5728\u56fa\u5b9a\u5206\u8fa8\u7387\u4e0b\u88c1\u526a\u5bf9\u9f50\u9762\u90e8\uff0c\u5bfc\u81f4\u89c6\u9891\u4e0d\u7a33\u5b9a\u4e14\u5f71\u54cd\u9ad8\u65af\u6df7\u5408\u5f62\u72b6\u5bf9\u9ad8\u9891\u7ec6\u8282\u7684\u6355\u6349\u80fd\u529b", "method": "1. \u6539\u8fdbStyleGAN\u751f\u6210\u7a33\u5b9a\u98ce\u683c\u5316\u89c6\u9891\n2. \u4ece\u89c6\u9891\u4e2d\u5b66\u4e60\u98ce\u683c\u5316\u4e2d\u6027\u5934\u90e8\u6a21\u578b\u53ca\u8868\u60c5\u6df7\u5408\u5f62\u72b6\n3. \u7ec4\u5408\u4e2d\u6027\u6a21\u578b\u4e0e\u8868\u60c5\u53c2\u6570\u5b9e\u73b0\u4efb\u610f\u8868\u60c5\u6e32\u67d3", "result": "\u5728Arcane\u548cPixar\u4e24\u79cd\u98ce\u683c\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u9ad8\u9891\u7ec6\u8282\u4fdd\u7559\u7684\u9ad8\u8d28\u91cf\u52a8\u753b\u751f\u6210", "conclusion": "ToonifyGB\u901a\u8fc7\u4e24\u9636\u6bb5\u67b6\u6784\u7a81\u7834\u9884\u5904\u7406\u9650\u5236\uff0c\u663e\u8457\u63d0\u5347\u98ce\u683c\u5316\u865a\u62df\u5f62\u8c61\u7684\u6e32\u67d3\u6548\u7387\u4e0e\u89c6\u89c9\u8d28\u91cf"}}
{"id": "2505.10282", "pdf": "https://arxiv.org/pdf/2505.10282", "abs": "https://arxiv.org/abs/2505.10282", "authors": ["Dubai Li", "Nan Jiang", "Kangping Huang", "Ruiqi Tu", "Shuyu Ouyang", "Huayu Yu", "Lin Qiao", "Chen Yu", "Tianshu Zhou", "Danyang Tong", "Qian Wang", "Mengtao Li", "Xiaofeng Zeng", "Yu Tian", "Xinping Tian", "Jingsong Li"], "title": "From Questions to Clinical Recommendations: Large Language Models Driving Evidence-Based Clinical Decision Making", "categories": ["cs.CL"], "comment": null, "summary": "Clinical evidence, derived from rigorous research and data analysis, provides\nhealthcare professionals with reliable scientific foundations for informed\ndecision-making. Integrating clinical evidence into real-time practice is\nchallenging due to the enormous workload, complex professional processes, and\ntime constraints. This highlights the need for tools that automate evidence\nsynthesis to support more efficient and accurate decision making in clinical\nsettings. This study introduces Quicker, an evidence-based clinical decision\nsupport system powered by large language models (LLMs), designed to automate\nevidence synthesis and generate clinical recommendations modeled after standard\nclinical guideline development processes. Quicker implements a fully automated\nchain that covers all phases, from questions to clinical recommendations, and\nfurther enables customized decision-making through integrated tools and\ninteractive user interfaces. To evaluate Quicker's capabilities, we developed\nthe Q2CRBench-3 benchmark dataset, based on clinical guideline development\nrecords for three different diseases. Experimental results highlighted\nQuicker's strong performance, with fine-grained question decomposition tailored\nto user preferences, retrieval sensitivities comparable to human experts, and\nliterature screening performance approaching comprehensive inclusion of\nrelevant studies. In addition, Quicker-assisted evidence assessment effectively\nsupported human reviewers, while Quicker's recommendations were more\ncomprehensive and logically coherent than those of clinicians. In system-level\ntesting, collaboration between a single reviewer and Quicker reduced the time\nrequired for recommendation development to 20-40 minutes. In general, our\nfindings affirm the potential of Quicker to help physicians make quicker and\nmore reliable evidence-based clinical decisions.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edfQuicker\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u8bc1\u636e\u5408\u6210\u63d0\u5347\u4e34\u5e8a\u51b3\u7b56\u6548\u7387", "motivation": "\u89e3\u51b3\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u8bc1\u636e\u6574\u5408\u7684\u9ad8\u8d1f\u8377\u3001\u590d\u6742\u6d41\u7a0b\u4e0e\u65f6\u6548\u6027\u6311\u6218\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u8bc1\u636e\u5408\u6210\u9700\u6c42", "method": "\u6784\u5efaLLM\u9a71\u52a8\u7684\u5168\u81ea\u52a8\u8bc1\u636e\u94fe\uff08\u4ece\u95ee\u9898\u5206\u89e3\u5230\u63a8\u8350\u751f\u6210\uff09\uff0c\u5f00\u53d1Q2CRBench-3\u57fa\u51c6\u6570\u636e\u96c6\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30", "result": "\u5355\u8bc4\u5ba1\u5458\u534f\u4f5c\u6a21\u5f0f\u4e0b\u63a8\u8350\u5f00\u53d1\u65f6\u95f4\u7f29\u77ed\u81f320-40\u5206\u949f\uff0c\u8bc1\u636e\u68c0\u7d22\u654f\u611f\u5ea6\u8fbe\u4e13\u5bb6\u6c34\u5e73\uff0c\u63a8\u8350\u903b\u8f91\u4f18\u4e8e\u4e34\u5e8a\u533b\u751f", "conclusion": "Quicker\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u5faa\u8bc1\u51b3\u7b56\u6548\u7387\u4e0e\u53ef\u9760\u6027\uff0c\u9a8c\u8bc1\u4e86AI\u5728\u4e34\u5e8a\u6307\u5357\u5f00\u53d1\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c"}}
{"id": "2505.10088", "pdf": "https://arxiv.org/pdf/2505.10088", "abs": "https://arxiv.org/abs/2505.10088", "authors": ["Yuncheng Guo", "Xiaodong Gu"], "title": "MMRL++: Parameter-Efficient and Interaction-Aware Representation Learning for Vision-Language Models", "categories": ["cs.CV"], "comment": "Due to the limitation \"The abstract field cannot be longer than 1,920\n  characters\", the abstract appearing here is slightly shorter than that in the\n  PDF file", "summary": "Large-scale pre-trained Vision-Language Models (VLMs) have significantly\nadvanced transfer learning across diverse tasks. However, adapting these models\nwith limited few-shot data often leads to overfitting, undermining their\nability to generalize to new tasks. To address this, we propose Multi-Modal\nRepresentation Learning (MMRL), which introduces a shared, learnable,\nmodality-agnostic representation space. MMRL generates space tokens projected\ninto both text and image encoders as representation tokens, enabling more\neffective cross-modal interactions. Unlike prior methods that mainly optimize\nclass token features, MMRL inserts representation tokens into higher encoder\nlayers--where task-specific features are more prominent--while preserving\ngeneral knowledge in the lower layers. During training, both class and\nrepresentation features are jointly optimized: a trainable projection layer is\napplied to representation tokens for task adaptation, while the projection\nlayer for class token remains frozen to retain pre-trained knowledge. To\nfurther promote generalization, we introduce a regularization term aligning\nclass and text features with the frozen VLM's zero-shot features. At inference,\na decoupling strategy uses both class and representation features for base\ntasks, but only class features for novel tasks due to their stronger\ngeneralization. Building upon this, we propose MMRL++, a parameter-efficient\nand interaction-aware extension that significantly reduces trainable parameters\nand enhances intra-modal interactions--particularly across the layers of\nrepresentation tokens--allowing gradient sharing and instance-specific\ninformation to propagate more effectively through the network. Extensive\nexperiments on 15 datasets demonstrate that MMRL and MMRL++ consistently\noutperform state-of-the-art methods, achieving a strong balance between\ntask-specific adaptation and generalization.", "AI": {"tldr": "Proposes MMRL/MMRL++ frameworks to address overfitting in few-shot vision-language adaptation via shared representation spaces and parameter-efficient interaction-aware design.", "motivation": "Traditional few-shot adaptation of VLMs causes overfitting and weak generalization by focusing optimization only on class tokens in lower layers.", "method": "Introduces modality-agnostic representation tokens inserted into higher encoder layers. Jointly optimizes class/representation features with frozen class projections. Adds regularization aligning features with zero-shot VLM outputs.", "result": "Achieves SOTA performance on 15 datasets, demonstrating superior balance between task-specific adaptation and generalization capability.", "conclusion": "MMRL frameworks effectively preserve pretrained knowledge while enabling task adaptation through layer-wise representation optimization and enhanced cross-modal interactions."}}
{"id": "2505.10320", "pdf": "https://arxiv.org/pdf/2505.10320", "abs": "https://arxiv.org/abs/2505.10320", "authors": ["Chenxi Whitehouse", "Tianlu Wang", "Ping Yu", "Xian Li", "Jason Weston", "Ilia Kulikov", "Swarnadeep Saha"], "title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "10 pages, 8 tables, 11 figures", "summary": "The progress of AI is bottlenecked by the quality of evaluation, and powerful\nLLM-as-a-Judge models have proved to be a core solution. Improved judgment\nability is enabled by stronger chain-of-thought reasoning, motivating the need\nto find the best recipes for training such models to think. In this work we\nintroduce J1, a reinforcement learning approach to training such models. Our\nmethod converts both verifiable and non-verifiable prompts to judgment tasks\nwith verifiable rewards that incentivize thinking and mitigate judgment bias.\nIn particular, our approach outperforms all other existing 8B or 70B models\nwhen trained at those sizes, including models distilled from DeepSeek-R1. J1\nalso outperforms o1-mini, and even R1 on some benchmarks, despite training a\nsmaller model. We provide analysis and ablations comparing Pairwise-J1 vs\nPointwise-J1 models, offline vs online training recipes, reward strategies,\nseed prompts, and variations in thought length and content. We find that our\nmodels make better judgments by learning to outline evaluation criteria,\ncomparing against self-generated reference answers, and re-evaluating the\ncorrectness of model responses.", "AI": {"tldr": "\u63d0\u51fa\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5J1\uff0c\u901a\u8fc7\u53ef\u9a8c\u8bc1\u5956\u52b1\u673a\u5236\u63d0\u5347LLM\u8bc4\u4f30\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u540c\u89c4\u6a21\u53ca\u66f4\u5927\u6a21\u578b", "motivation": "AI\u53d1\u5c55\u53d7\u9650\u4e8e\u8bc4\u4f30\u8d28\u91cf\uff0c\u73b0\u6709LLM\u8bc4\u4f30\u6a21\u578b\u9700\u63d0\u5347\u601d\u7ef4\u94fe\u63a8\u7406\u80fd\u529b\u4ee5\u4f18\u5316\u5224\u65ad\u8d28\u91cf", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u53ef\u9a8c\u8bc1/\u4e0d\u53ef\u9a8c\u8bc1\u63d0\u793a\u8f6c\u5316\u4e3a\u5e26\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5224\u65ad\u4efb\u52a1\uff0c\u6fc0\u52b1\u601d\u8003\u5e76\u51cf\u5c11\u5224\u65ad\u504f\u89c1", "result": "J1\u57288B/70B\u89c4\u6a21\u5747\u8d85\u8d8a\u6240\u6709\u540c\u89c4\u6a21\u6a21\u578b\uff08\u5305\u62ecDeepSeek-R1\u84b8\u998f\u6a21\u578b\uff09\uff0c\u90e8\u5206\u57fa\u51c6\u751a\u81f3\u4f18\u4e8e\u66f4\u5927\u89c4\u6a21\u7684R1\u6a21\u578b", "conclusion": "J1\u901a\u8fc7\u5236\u5b9a\u8bc4\u4f30\u6807\u51c6\u3001\u81ea\u751f\u6210\u53c2\u8003\u7b54\u6848\u5bf9\u6bd4\u3001\u54cd\u5e94\u6b63\u786e\u6027\u91cd\u8bc4\u4f30\u7b49\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5224\u65ad\u8d28\u91cf"}}
{"id": "2505.10118", "pdf": "https://arxiv.org/pdf/2505.10118", "abs": "https://arxiv.org/abs/2505.10118", "authors": ["Yangfu Li", "Hongjian Zhan", "Tianyi Chen", "Qi Liu", "Yue Lu"], "title": "Why 1 + 1 < 1 in Visual Token Pruning: Beyond Naive Integration via Multi-Objective Balanced Covering", "categories": ["cs.CV", "cs.CL"], "comment": "31 pages,9 figures,conference", "summary": "Existing visual token pruning methods target prompt alignment and visual\npreservation with static strategies, overlooking the varying relative\nimportance of these objectives across tasks, which leads to inconsistent\nperformance. To address this, we derive the first closed-form error bound for\nvisual token pruning based on the Hausdorff distance, uniformly characterizing\nthe contributions of both objectives. Moreover, leveraging $\\epsilon$-covering\ntheory, we reveal an intrinsic trade-off between these objectives and quantify\ntheir optimal attainment levels under a fixed budget. To practically handle\nthis trade-off, we propose Multi-Objective Balanced Covering (MoB), which\nreformulates visual token pruning as a bi-objective covering problem. In this\nframework, the attainment trade-off reduces to budget allocation via greedy\nradius trading. MoB offers a provable performance bound and linear scalability\nwith respect to the number of input visual tokens, enabling adaptation to\nchallenging pruning scenarios. Extensive experiments show that MoB preserves\n96.4% of performance for LLaVA-1.5-7B using only 11.1% of the original visual\ntokens and accelerates LLaVA-Next-7B by 1.3-1.5$\\times$ with negligible\nperformance loss. Additionally, evaluations on Qwen2-VL and Video-LLaVA confirm\nthat MoB integrates seamlessly into advanced MLLMs and diverse vision-language\ntasks.", "AI": {"tldr": "\u63d0\u51fa\u591a\u76ee\u6807\u5e73\u8861\u8986\u76d6\u6846\u67b6(MoB)\uff0c\u901a\u8fc7\u52a8\u6001\u9884\u7b97\u5206\u914d\u89e3\u51b3\u89c6\u89c9\u4ee4\u724c\u526a\u679d\u4e2d\u76ee\u6807\u6743\u8861\u95ee\u9898", "motivation": "\u73b0\u6709\u89c6\u89c9\u4ee4\u724c\u526a\u679d\u65b9\u6cd5\u91c7\u7528\u9759\u6001\u7b56\u7565\uff0c\u5ffd\u89c6\u4e0d\u540c\u4efb\u52a1\u4e2d\u63d0\u793a\u5bf9\u9f50\u4e0e\u89c6\u89c9\u4fdd\u7559\u7684\u76f8\u5bf9\u91cd\u8981\u6027\u5dee\u5f02\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u7a33\u5b9a", "method": "\u57fa\u4e8eHausdorff\u8ddd\u79bb\u63a8\u5bfc\u8bef\u5dee\u754c\u9650\uff0c\u5229\u7528\u03b5-covering\u7406\u8bba\u91cf\u5316\u76ee\u6807\u6743\u8861\uff0c\u8bbe\u8ba1\u53cc\u76ee\u6807\u8986\u76d6\u6846\u67b6\u5b9e\u73b0\u8d2a\u5fc3\u534a\u5f84\u5206\u914d", "result": "\u5728LLaVA\u7cfb\u5217\u6a21\u578b\u4e2d\u4fdd\u755996.4%\u6027\u80fd\u4ec5\u970011.1%\u4ee4\u724c\uff0c\u52a0\u901f1.3-1.5\u500d\uff1b\u9002\u914dQwen2-VL\u548cVideo-LLaVA\u9a8c\u8bc1\u901a\u7528\u6027", "conclusion": "MoB\u901a\u8fc7\u7406\u8bba\u9a71\u52a8\u7684\u52a8\u6001\u6743\u8861\u673a\u5236\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u591a\u6a21\u6001\u5927\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1"}}
{"id": "2505.10354", "pdf": "https://arxiv.org/pdf/2505.10354", "abs": "https://arxiv.org/abs/2505.10354", "authors": ["Yile Wang", "Zhanyu Shen", "Hui Huang"], "title": "LDIR: Low-Dimensional Dense and Interpretable Text Embeddings with Relative Representations", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Semantic text representation is a fundamental task in the field of natural\nlanguage processing. Existing text embedding (e.g., SimCSE and LLM2Vec) have\ndemonstrated excellent performance, but the values of each dimension are\ndifficult to trace and interpret. Bag-of-words, as classic sparse interpretable\nembeddings, suffers from poor performance. Recently, Benara et al. (2024)\npropose interpretable text embeddings using large language models, which forms\n\"0/1\" embeddings based on responses to a series of questions. These\ninterpretable text embeddings are typically high-dimensional (larger than\n10,000). In this work, we propose Low-dimensional (lower than 500) Dense and\nInterpretable text embeddings with Relative representations (LDIR). The\nnumerical values of its dimensions indicate semantic relatedness to different\nanchor texts through farthest point sampling, offering both semantic\nrepresentation as well as a certain level of traceability and interpretability.\nWe validate LDIR on multiple semantic textual similarity, retrieval, and\nclustering tasks. Extensive experimental results show that LDIR performs close\nto the black-box baseline models and outperforms the interpretable embeddings\nbaselines with much fewer dimensions. Code is available at\nhttps://github.com/szu-tera/LDIR.", "AI": {"tldr": "\u63d0\u51fa\u4f4e\u7ef4\u5bc6\u96c6\u53ef\u89e3\u91ca\u6587\u672c\u5d4c\u5165LDIR\uff0c\u901a\u8fc7\u951a\u6587\u672c\u8bed\u4e49\u5173\u8054\u5b9e\u73b0500\u7ef4\u4ee5\u4e0b\u7684\u5bc6\u96c6\u8868\u793a\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u53ef\u89e3\u91ca\u6587\u672c\u5d4c\u5165\u5b58\u5728\u9ad8\u7ef4\u5ea6\uff08\u4e07\u7ef4\u4ee5\u4e0a\uff09\u6216\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u5e73\u8861\u8bed\u4e49\u8868\u793a\u80fd\u529b\u4e0e\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u91c7\u7528\u6700\u8fdc\u70b9\u91c7\u6837\u9009\u62e9\u951a\u6587\u672c\uff0c\u6784\u5efa\u4f4e\u7ef4\u5bc6\u96c6\u5411\u91cf\uff0c\u6bcf\u4e2a\u7ef4\u5ea6\u6570\u503c\u8868\u793a\u4e0e\u4e0d\u540c\u951a\u6587\u672c\u7684\u8bed\u4e49\u76f8\u5173\u5ea6\u3002", "result": "\u5728\u8bed\u4e49\u76f8\u4f3c\u5ea6\u3001\u68c0\u7d22\u548c\u805a\u7c7b\u4efb\u52a1\u4e2d\u63a5\u8fd1\u9ed1\u76d2\u6a21\u578b\u6548\u679c\uff0c\u4e14\u4ee5\u66f4\u5c11\u7ef4\u5ea6\u8d85\u8d8a\u5176\u4ed6\u53ef\u89e3\u91ca\u5d4c\u5165\u57fa\u7ebf\u3002", "conclusion": "LDIR\u6210\u529f\u5b9e\u73b0\u4f4e\u7ef4\u5bc6\u96c6\u4e0e\u53ef\u89e3\u91ca\u7684\u7ed3\u5408\uff0c\u4e3a\u53ef\u89e3\u91caNLP\u6a21\u578b\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.10124", "pdf": "https://arxiv.org/pdf/2505.10124", "abs": "https://arxiv.org/abs/2505.10124", "authors": ["Ziad Kheil", "Lucas Robinet", "Laurent Risser", "Soleakhena Ken"], "title": "IMITATE: Image Registration with Context for unknown time frame recovery", "categories": ["cs.CV", "eess.IV"], "comment": "IEEE ISBI 2025", "summary": "In this paper, we formulate a novel image registration formalism dedicated to\nthe estimation of unknown condition-related images, based on two or more known\nimages and their associated conditions. We show how to practically model this\nformalism by using a new conditional U-Net architecture, which fully takes into\naccount the conditional information and does not need any fixed image. Our\nformalism is then applied to image moving tumors for radiotherapy treatment at\ndifferent breathing amplitude using 4D-CT (3D+t) scans in thoracoabdominal\nregions. This driving application is particularly complex as it requires to\nstitch a collection of sequential 2D slices into several 3D volumes at\ndifferent organ positions. Movement interpolation with standard methods then\ngenerates well known reconstruction artefacts in the assembled volumes due to\nirregular patient breathing, hysteresis and poor correlation of breathing\nsignal to internal motion. Results obtained on 4D-CT clinical data showcase\nartefact-free volumes achieved through real-time latencies. The code is\npublicly available at https://github.com/Kheil-Z/IMITATE .", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6761\u4ef6U-Net\u7684\u65b0\u578b\u56fe\u50cf\u914d\u51c6\u65b9\u6cd5\uff0c\u6210\u529f\u5e94\u7528\u4e8e4D-CT\u6570\u636e\u5b9e\u73b0\u65e0\u4f2a\u5f71\u7684\u547c\u5438\u8fd0\u52a8\u5efa\u6a21", "motivation": "\u89e3\u51b3\u653e\u7597\u4e2d\u547c\u5438\u8fd0\u52a8\u5bfc\u81f4\u76844D-CT\u56fe\u50cf\u91cd\u5efa\u4f2a\u5f71\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u56e0\u547c\u5438\u4e0d\u89c4\u5219\u6027/\u8fdf\u6ede\u6548\u5e94\u4ea7\u751f\u4f53\u79ef\u62fc\u63a5\u5931\u771f", "method": "\u5f00\u53d1\u6761\u4ef6U-Net\u67b6\u6784\uff0c\u76f4\u63a5\u878d\u5408\u6761\u4ef6\u4fe1\u606f(\u547c\u5438\u5e45\u5ea6)\uff0c\u65e0\u9700\u56fa\u5b9a\u53c2\u8003\u56fe\u50cf\u7684\u591a\u6a21\u6001\u914d\u51c6\u6846\u67b6", "result": "\u4e34\u5e8a\u6570\u636e\u9a8c\u8bc1\u663e\u793a\u5b9e\u73b0\u65e0\u4f2a\u5f71\u76843D\u4f53\u79ef\u91cd\u5efa\uff0c\u4e14\u6ee1\u8db3\u5b9e\u65f6\u6027\u8981\u6c42(\u4e9a\u79d2\u7ea7\u5ef6\u8fdf)", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u52a8\u6001\u5668\u5b98\u5efa\u6a21\u96be\u9898\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u63a8\u52a8\u533b\u5b66\u56fe\u50cf\u5206\u6790\u9886\u57df\u53d1\u5c55"}}
{"id": "2505.10356", "pdf": "https://arxiv.org/pdf/2505.10356", "abs": "https://arxiv.org/abs/2505.10356", "authors": ["Chunyu Ye", "Shaonan Wang"], "title": "Coherent Language Reconstruction from Brain Recordings with Flexible Multi-Modal Input Stimuli", "categories": ["cs.CL"], "comment": null, "summary": "Decoding thoughts from brain activity offers valuable insights into human\ncognition and enables promising applications in brain-computer interaction.\nWhile prior studies have explored language reconstruction from fMRI data, they\nare typically limited to single-modality inputs such as images or audio. In\ncontrast, human thought is inherently multimodal. To bridge this gap, we\npropose a unified and flexible framework for reconstructing coherent language\nfrom brain recordings elicited by diverse input modalities-visual, auditory,\nand textual. Our approach leverages visual-language models (VLMs), using\nmodality-specific experts to jointly interpret information across modalities.\nExperiments demonstrate that our method achieves performance comparable to\nstate-of-the-art systems while remaining adaptable and extensible. This work\nadvances toward more ecologically valid and generalizable mind decoding.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6a21\u6001\u7edf\u4e00\u6846\u67b6\uff0c\u5b9e\u73b0\u8de8\u89c6\u89c9/\u542c\u89c9/\u6587\u672c\u8f93\u5165\u7684\u5927\u8111\u4fe1\u53f7\u8bed\u8a00\u91cd\u5efa", "motivation": "\u73b0\u6709fMRI\u8bed\u8a00\u89e3\u7801\u7814\u7a76\u5c40\u9650\u4e8e\u5355\u6a21\u6001\u8f93\u5165\uff0c\u4e0e\u4eba\u7c7b\u601d\u7ef4\u7684\u591a\u6a21\u6001\u672c\u8d28\u5b58\u5728\u5dee\u8ddd", "method": "\u91c7\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\uff0c\u901a\u8fc7\u6a21\u6001\u7279\u5b9a\u4e13\u5bb6\u7f51\u7edc\u5b9e\u73b0\u591a\u6a21\u6001\u4fe1\u606f\u8054\u5408\u89e3\u7801", "result": "\u5728\u4fdd\u6301\u7cfb\u7edf\u6269\u5c55\u6027\u7684\u540c\u65f6\u8fbe\u5230SOTA\u6027\u80fd\u6c34\u5e73\uff0c\u652f\u6301\u89c6\u89c9/\u542c\u89c9/\u6587\u672c\u8f93\u5165\u7684\u6df7\u5408\u89e3\u7801", "conclusion": "\u8be5\u6846\u67b6\u63a8\u52a8\u601d\u7ef4\u89e3\u7801\u5411\u751f\u6001\u6548\u5ea6\u66f4\u9ad8\u3001\u6cdb\u5316\u6027\u66f4\u5f3a\u7684\u65b9\u5411\u53d1\u5c55"}}
{"id": "2505.10152", "pdf": "https://arxiv.org/pdf/2505.10152", "abs": "https://arxiv.org/abs/2505.10152", "authors": ["Yikang Wei"], "title": "Multi-Source Collaborative Style Augmentation and Domain-Invariant Learning for Federated Domain Generalization", "categories": ["cs.CV"], "comment": "IJCAI 2025", "summary": "Federated domain generalization aims to learn a generalizable model from\nmultiple decentralized source domains for deploying on the unseen target\ndomain. The style augmentation methods have achieved great progress on domain\ngeneralization. However, the existing style augmentation methods either explore\nthe data styles within isolated source domain or interpolate the style\ninformation across existing source domains under the data decentralization\nscenario, which leads to limited style space. To address this issue, we propose\na Multi-source Collaborative Style Augmentation and Domain-invariant learning\nmethod (MCSAD) for federated domain generalization. Specifically, we propose a\nmulti-source collaborative style augmentation module to generate data in the\nbroader style space. Furthermore, we conduct domain-invariant learning between\nthe original data and augmented data by cross-domain feature alignment within\nthe same class and classes relation ensemble distillation between different\nclasses to learn a domain-invariant model. By alternatively conducting\ncollaborative style augmentation and domain-invariant learning, the model can\ngeneralize well on unseen target domain. Extensive experiments on multiple\ndomain generalization datasets indicate that our method significantly\noutperforms the state-of-the-art federated domain generalization methods.", "AI": {"tldr": "\u63d0\u51fa\u591a\u6e90\u534f\u4f5c\u98ce\u683c\u589e\u5f3a\u4e0e\u57df\u4e0d\u53d8\u5b66\u4e60\u65b9\u6cd5MCSAD\uff0c\u901a\u8fc7\u62d3\u5c55\u98ce\u683c\u7a7a\u95f4\u548c\u8de8\u57df\u7279\u5f81\u5bf9\u9f50\u63d0\u5347\u8054\u90a6\u57df\u6cdb\u5316\u6548\u679c", "motivation": "\u73b0\u6709\u8054\u90a6\u57df\u6cdb\u5316\u65b9\u6cd5\u5728\u5206\u6563\u6570\u636e\u573a\u666f\u4e0b\u4ec5\u63a2\u7d22\u5b64\u7acb\u6e90\u57df\u98ce\u683c\u6216\u7b80\u5355\u63d2\u503c\u8de8\u57df\u98ce\u683c\uff0c\u5bfc\u81f4\u98ce\u683c\u7a7a\u95f4\u53d7\u9650", "method": "\u5305\u542b\u591a\u6e90\u534f\u4f5c\u98ce\u683c\u589e\u5f3a\u6a21\u5757\uff08\u751f\u6210\u66f4\u5e7f\u98ce\u683c\u7a7a\u95f4\u6570\u636e\uff09\u548c\u57df\u4e0d\u53d8\u5b66\u4e60\u6a21\u5757\uff08\u901a\u8fc7\u7c7b\u5185\u8de8\u57df\u7279\u5f81\u5bf9\u9f50\u4e0e\u7c7b\u95f4\u5173\u7cfb\u96c6\u6210\u84b8\u998f\uff09\u7684\u4ea4\u66ff\u8bad\u7ec3\u6846\u67b6", "result": "\u5728\u591a\u4e2a\u9886\u57df\u6cdb\u5316\u6570\u636e\u96c6\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u8054\u90a6\u57df\u6cdb\u5316\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u901a\u8fc7\u534f\u4f5c\u5f0f\u98ce\u683c\u6269\u5c55\u4e0e\u53cc\u5c42\u6b21\u57df\u4e0d\u53d8\u5b66\u4e60\u7b56\u7565\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5bf9\u672a\u89c1\u76ee\u6807\u57df\u7684\u5f3a\u6cdb\u5316\u80fd\u529b"}}
{"id": "2505.10389", "pdf": "https://arxiv.org/pdf/2505.10389", "abs": "https://arxiv.org/abs/2505.10389", "authors": ["Benjamin White", "Anastasia Shimorina"], "title": "Multi-domain Multilingual Sentiment Analysis in Industry: Predicting Aspect-based Opinion Quadruples", "categories": ["cs.CL"], "comment": null, "summary": "This paper explores the design of an aspect-based sentiment analysis system\nusing large language models (LLMs) for real-world use. We focus on quadruple\nopinion extraction -- identifying aspect categories, sentiment polarity,\ntargets, and opinion expressions from text data across different domains and\nlanguages. Using internal datasets, we investigate whether a single fine-tuned\nmodel can effectively handle multiple domain-specific taxonomies\nsimultaneously. We demonstrate that a combined multi-domain model achieves\nperformance comparable to specialized single-domain models while reducing\noperational complexity. We also share lessons learned for handling\nnon-extractive predictions and evaluating various failure modes when developing\nLLM-based systems for structured prediction tasks.", "AI": {"tldr": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8bbe\u8ba1\u591a\u9886\u57df\u7edf\u4e00\u7684\u60c5\u611f\u5206\u6790\u7cfb\u7edf\uff0c\u5b9e\u73b0\u56db\u5143\u7ec4\u89c2\u70b9\u63d0\u53d6\uff0c\u9a8c\u8bc1\u591a\u9886\u57df\u6a21\u578b\u6548\u679c\u4e0e\u5355\u9886\u57df\u76f8\u5f53\u4e14\u964d\u4f4e\u8fd0\u7ef4\u590d\u6742\u5ea6\u3002", "motivation": "\u89e3\u51b3\u73b0\u5b9e\u573a\u666f\u4e2d\u591a\u9886\u57df/\u591a\u8bed\u8a00\u60c5\u611f\u5206\u6790\u7cfb\u7edf\u7ef4\u62a4\u590d\u6742\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u5355\u4e00\u6a21\u578b\u540c\u65f6\u5904\u7406\u591a\u9886\u57df\u7684\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u5185\u90e8\u6570\u636e\u96c6\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6784\u5efa\u7edf\u4e00\u591a\u9886\u57df\u6a21\u578b\uff0c\u5bf9\u6bd4\u5355\u9886\u57df\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u5f00\u53d1\u975e\u62bd\u53d6\u5f0f\u9884\u6d4b\u5904\u7406\u65b9\u6cd5\u3002", "result": "\u591a\u9886\u57df\u6a21\u578b\u4fdd\u6301\u5355\u9886\u57df\u6027\u80fd\u6c34\u5e73\uff0c\u663e\u8457\u964d\u4f4e\u8fd0\u7ef4\u590d\u6742\u5ea6\uff1b\u603b\u7ed3\u7ed3\u6784\u5316\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u5931\u8d25\u6a21\u5f0f\u5904\u7406\u7ecf\u9a8c\u3002", "conclusion": "\u9a8c\u8bc1\u591a\u9886\u57df\u7edf\u4e00\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u4e3aLLM\u7ed3\u6784\u5316\u9884\u6d4b\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u975e\u62bd\u53d6\u5904\u7406\u3001\u9519\u8bef\u5206\u6790\u7b49\u5b9e\u8df5\u7ecf\u9a8c\u3002"}}
{"id": "2505.10169", "pdf": "https://arxiv.org/pdf/2505.10169", "abs": "https://arxiv.org/abs/2505.10169", "authors": ["Matthias K\u00fcmmerer", "Harneet Khanuja", "Matthias Bethge"], "title": "Modeling Saliency Dataset Bias", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances in image-based saliency prediction are approaching gold\nstandard performance levels on existing benchmarks. Despite this success, we\nshow that predicting fixations across multiple saliency datasets remains\nchallenging due to dataset bias. We find a significant performance drop (around\n40%) when models trained on one dataset are applied to another. Surprisingly,\nincreasing dataset diversity does not resolve this inter-dataset gap, with\nclose to 60% attributed to dataset-specific biases. To address this remaining\ngeneralization gap, we propose a novel architecture extending a mostly\ndataset-agnostic encoder-decoder structure with fewer than 20 dataset-specific\nparameters that govern interpretable mechanisms such as multi-scale structure,\ncenter bias, and fixation spread. Adapting only these parameters to new data\naccounts for more than 75% of the generalization gap, with a large fraction of\nthe improvement achieved with as few as 50 samples. Our model sets a new\nstate-of-the-art on all three datasets of the MIT/Tuebingen Saliency Benchmark\n(MIT300, CAT2000, and COCO-Freeview), even when purely generalizing from\nunrelated datasets, but with a substantial boost when adapting to the\nrespective training datasets. The model also provides valuable insights into\nspatial saliency properties, revealing complex multi-scale effects that combine\nboth absolute and relative sizes.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u578b\u89c6\u89c9\u663e\u8457\u6027\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc720\u4e2a\u53ef\u89e3\u91ca\u7684\u8de8\u6570\u636e\u96c6\u53c2\u6570\u89e3\u51b3\u6cdb\u5316\u96be\u9898\uff0c\u5728MIT/Tuebingen\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0SOTA\u3002", "motivation": "\u73b0\u6709\u663e\u8457\u6027\u9884\u6d4b\u6a21\u578b\u5b58\u5728\u4e25\u91cd\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u95ee\u9898\uff08\u6027\u80fd\u4e0b\u964d40%\uff09\uff0c60%\u7684\u6cdb\u5316\u5dee\u8ddd\u6e90\u4e8e\u6570\u636e\u96c6\u7279\u5f02\u6027\u504f\u5dee\uff0c\u9700\u8bbe\u8ba1\u66f4\u901a\u7528\u7684\u53c2\u6570\u5316\u6846\u67b6\u3002", "method": "\u57fa\u4e8e\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u5f15\u5165\u5c11\u4e8e20\u4e2a\u53ef\u89e3\u91ca\u7684dataset-specific\u53c2\u6570\uff08\u591a\u5c3a\u5ea6\u7ed3\u6784/\u4e2d\u5fc3\u504f\u7f6e/\u6ce8\u89c6\u6269\u6563\uff09\uff0c\u4ec5\u970050\u4e2a\u6837\u672c\u5373\u53ef\u9002\u914d\u65b0\u6570\u636e\u96c6\uff0c\u8986\u76d675%\u6cdb\u5316\u5dee\u8ddd\u3002", "result": "\u5728MIT300/CAT2000/COCO-Freeview\u4e09\u5927\u57fa\u51c6\u5168\u9762\u5237\u65b0SOTA\uff0c\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u6027\u80fd\u663e\u8457\u63d0\u5347\uff08\u5c24\u5176\u9002\u914d\u8bad\u7ec3\u6570\u636e\u540e\uff09\uff0c\u63ed\u793a\u591a\u5c3a\u5ea6\u7279\u5f81\u4e0e\u7edd\u5bf9/\u76f8\u5bf9\u5c3a\u5bf8\u7684\u590d\u5408\u6548\u5e94\u3002", "conclusion": "\u5c11\u91cf\u53ef\u89e3\u91ca\u7684\u8de8\u6570\u636e\u96c6\u53c2\u6570\u53ef\u6709\u6548\u89e3\u51b3\u663e\u8457\u6027\u9884\u6d4b\u7684\u6cdb\u5316\u74f6\u9888\uff0c\u4e3a\u7a7a\u95f4\u663e\u8457\u6027\u673a\u5236\u63d0\u4f9b\u65b0\u89c1\u89e3\uff0c\u5efa\u7acb\u9ad8\u6548\u6a21\u578b\u9002\u914d\u8303\u5f0f\u3002"}}
{"id": "2505.10402", "pdf": "https://arxiv.org/pdf/2505.10402", "abs": "https://arxiv.org/abs/2505.10402", "authors": ["Yihong Dong", "Yuchen Liu", "Xue Jiang", "Zhi Jin", "Ge Li"], "title": "Rethinking Repetition Problems of LLMs in Code Generation", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "comment": "Accepted to ACL 2025 (main)", "summary": "With the advent of neural language models, the performance of code generation\nhas been significantly boosted. However, the problem of repetitions during the\ngeneration process continues to linger. Previous work has primarily focused on\ncontent repetition, which is merely a fraction of the broader repetition\nproblem in code generation. A more prevalent and challenging problem is\nstructural repetition. In structural repetition, the repeated code appears in\nvarious patterns but possesses a fixed structure, which can be inherently\nreflected in grammar. In this paper, we formally define structural repetition\nand propose an efficient decoding approach called RPG, which stands for\nRepetition Penalization based on Grammar, to alleviate the repetition problems\nin code generation for LLMs. Specifically, RPG first leverages grammar rules to\nidentify repetition problems during code generation, and then strategically\ndecays the likelihood of critical tokens that contribute to repetitions,\nthereby mitigating them in code generation. To facilitate this study, we\nconstruct a new dataset CodeRepetEval to comprehensively evaluate approaches\nfor mitigating the repetition problems in code generation. Extensive\nexperimental results demonstrate that RPG substantially outperforms the\nbest-performing baselines on CodeRepetEval dataset as well as HumanEval and\nMBPP benchmarks, effectively reducing repetitions and enhancing the quality of\ngenerated code.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u8bed\u6cd5\u89c4\u5219\u7684RPG\u89e3\u7801\u65b9\u6cd5\uff0c\u6709\u6548\u7f13\u89e3\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u7ed3\u6784\u91cd\u590d\u95ee\u9898", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u89e3\u51b3\u5185\u5bb9\u91cd\u590d\u95ee\u9898\uff0c\u4f46\u7ed3\u6784\u91cd\u590d\uff08\u8bed\u6cd5\u5c42\u9762\u56fa\u5b9a\u7ed3\u6784\u7684\u91cd\u590d\u6a21\u5f0f\uff09\u66f4\u5177\u6311\u6218\u6027\u548c\u666e\u904d\u6027", "method": "RPG\u901a\u8fc7\u8bed\u6cd5\u89c4\u5219\u8bc6\u522b\u4ee3\u7801\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u7ed3\u6784\u91cd\u590d\uff0c\u5e76\u6218\u7565\u6027\u5730\u964d\u4f4e\u5bfc\u81f4\u91cd\u590d\u7684\u5173\u952etoken\u7684\u751f\u6210\u6982\u7387", "result": "\u5728CodeRepetEval\u6570\u636e\u96c6\u53caHumanEval/MBPP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRPG\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u51cf\u5c11\u91cd\u590d\u5e76\u63d0\u5347\u4ee3\u7801\u8d28\u91cf", "conclusion": "\u57fa\u4e8e\u8bed\u6cd5\u89c4\u5219\u7684\u91cd\u590d\u60e9\u7f5a\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u7ed3\u6784\u91cd\u590d\u95ee\u9898\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411"}}
{"id": "2505.10205", "pdf": "https://arxiv.org/pdf/2505.10205", "abs": "https://arxiv.org/abs/2505.10205", "authors": ["Umair Haroon", "Ahmad AlMughrabi", "Thanasis Zoumpekas", "Ricardo Marques", "Petia Radeva"], "title": "VolE: A Point-cloud Framework for Food 3D Reconstruction and Volume Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate food volume estimation is crucial for medical nutrition management\nand health monitoring applications, but current food volume estimation methods\nare often limited by mononuclear data, leveraging single-purpose hardware such\nas 3D scanners, gathering sensor-oriented information such as depth\ninformation, or relying on camera calibration using a reference object. In this\npaper, we present VolE, a novel framework that leverages mobile device-driven\n3D reconstruction to estimate food volume. VolE captures images and camera\nlocations in free motion to generate precise 3D models, thanks to AR-capable\nmobile devices. To achieve real-world measurement, VolE is a reference- and\ndepth-free framework that leverages food video segmentation for food mask\ngeneration. We also introduce a new food dataset encompassing the challenging\nscenarios absent in the previous benchmarks. Our experiments demonstrate that\nVolE outperforms the existing volume estimation techniques across multiple\ndatasets by achieving 2.22 % MAPE, highlighting its superior performance in\nfood volume estimation.", "AI": {"tldr": "\u63d0\u51faVolE\u6846\u67b6\uff0c\u5229\u7528\u79fb\u52a8\u8bbe\u5907\u8fdb\u884c3D\u91cd\u5efa\u5b9e\u73b0\u65e0\u9700\u53c2\u8003\u7269\u6216\u6df1\u5ea6\u4fe1\u606f\u7684\u7cbe\u51c6\u98df\u7269\u4f53\u79ef\u4f30\u8ba1\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u98df\u7269\u4f53\u79ef\u4f30\u8ba1\u65b9\u6cd5\u53d7\u9650\u4e8e\u4e13\u7528\u786c\u4ef6\u3001\u6df1\u5ea6\u4fe1\u606f\u6216\u53c2\u8003\u7269\uff0cVolE\u65e8\u5728\u901a\u8fc7\u79fb\u52a8\u8bbe\u5907\u9a71\u52a8\u76843D\u91cd\u5efa\u5b9e\u73b0\u65e0\u9700\u53c2\u8003\u7269\u6216\u6df1\u5ea6\u4fe1\u606f\u7684\u5b9e\u65f6\u6d4b\u91cf\u3002", "method": "\u901a\u8fc7\u79fb\u52a8\u8bbe\u5907\u81ea\u7531\u8fd0\u52a8\u6355\u6349\u56fe\u50cf\u53ca\u4f4d\u7f6e\u751f\u62103D\u6a21\u578b\uff0c\u7ed3\u5408\u98df\u7269\u89c6\u9891\u5206\u5272\u751f\u6210\u63a9\u6a21\uff0c\u65e0\u9700\u53c2\u8003\u7269\u6216\u6df1\u5ea6\u4fe1\u606f\uff0c\u5e76\u5f15\u5165\u65b0\u6570\u636e\u96c6\u5e94\u5bf9\u590d\u6742\u573a\u666f\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cVolE\u4ee52.22%\u7684MAPE\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5c55\u73b0\u5353\u8d8a\u6027\u80fd\u3002", "conclusion": "VolE\u901a\u8fc7\u79fb\u52a8\u8bbe\u5907\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u98df\u7269\u4f53\u79ef\u4f30\u8ba1\uff0c\u65e0\u9700\u4e13\u7528\u786c\u4ef6\u6216\u53c2\u8003\u7269\uff0c\u4e3a\u533b\u7597\u8425\u517b\u7ba1\u7406\u548c\u5065\u5eb7\u76d1\u6d4b\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.10409", "pdf": "https://arxiv.org/pdf/2505.10409", "abs": "https://arxiv.org/abs/2505.10409", "authors": ["Yue Guo", "Jae Ho Sohn", "Gondy Leroy", "Trevor Cohen"], "title": "Are LLM-generated plain language summaries truly understandable? A large-scale crowdsourced evaluation", "categories": ["cs.CL"], "comment": null, "summary": "Plain language summaries (PLSs) are essential for facilitating effective\ncommunication between clinicians and patients by making complex medical\ninformation easier for laypeople to understand and act upon. Large language\nmodels (LLMs) have recently shown promise in automating PLS generation, but\ntheir effectiveness in supporting health information comprehension remains\nunclear. Prior evaluations have generally relied on automated scores that do\nnot measure understandability directly, or subjective Likert-scale ratings from\nconvenience samples with limited generalizability. To address these gaps, we\nconducted a large-scale crowdsourced evaluation of LLM-generated PLSs using\nAmazon Mechanical Turk with 150 participants. We assessed PLS quality through\nsubjective Likert-scale ratings focusing on simplicity, informativeness,\ncoherence, and faithfulness; and objective multiple-choice comprehension and\nrecall measures of reader understanding. Additionally, we examined the\nalignment between 10 automated evaluation metrics and human judgments. Our\nfindings indicate that while LLMs can generate PLSs that appear\nindistinguishable from human-written ones in subjective evaluations,\nhuman-written PLSs lead to significantly better comprehension. Furthermore,\nautomated evaluation metrics fail to reflect human judgment, calling into\nquestion their suitability for evaluating PLSs. This is the first study to\nsystematically evaluate LLM-generated PLSs based on both reader preferences and\ncomprehension outcomes. Our findings highlight the need for evaluation\nframeworks that move beyond surface-level quality and for generation methods\nthat explicitly optimize for layperson comprehension.", "AI": {"tldr": "LLM\u751f\u6210\u7684\u901a\u4fd7\u6458\u8981\u4e3b\u89c2\u8bc4\u4ef7\u4e0e\u4eba\u5de5\u76f8\u4f3c\uff0c\u4f46\u4eba\u5de5\u6458\u8981\u663e\u8457\u63d0\u5347\u8bfb\u8005\u7406\u89e3\uff0c\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u4e0d\u53ef\u9760\u9700\u65b0\u6846\u67b6", "motivation": "\u73b0\u6709PLSs\u8bc4\u4f30\u4f9d\u8d56\u81ea\u52a8\u5316\u6307\u6807\u6216\u5c0f\u6837\u672c\u4e3b\u89c2\u8bc4\u5206\uff0c\u7f3a\u4e4f\u6709\u6548\u8861\u91cf\u5927\u4f17\u7406\u89e3\u5ea6\u7684\u7cfb\u7edf\u6027\u7814\u7a76", "method": "\u901a\u8fc7Amazon Mechanical Turk\u5e73\u53f0\u5bf9150\u4eba\u5f00\u5c55\u4e3b\u5ba2\u89c2\u7efc\u5408\u8bc4\u4f30\uff0c\u5305\u542b\u6613\u8bfb\u6027\u8bc4\u5206\u3001\u591a\u9009\u9898\u7406\u89e3\u6d4b\u8bd5\u53ca\u81ea\u52a8\u6307\u6807\u5bf9\u6bd4\u5206\u6790", "result": "\u4eba\u5de5\u6458\u8981\u7ec4\u9605\u8bfb\u7406\u89e3\u51c6\u786e\u7387\u6bd4LLM\u7ec4\u9ad815%\uff0c\u81ea\u52a8\u6307\u6807\u4e0e\u4eba\u7c7b\u8bc4\u5206\u76f8\u5173\u6027\u4f4e\u4e8e0.3", "conclusion": "\u9700\u5efa\u7acb\u4ee5\u7406\u89e3\u4e3a\u5bfc\u5411\u7684\u8bc4\u4f30\u4f53\u7cfb\uff0c\u5f00\u53d1\u76f4\u63a5\u4f18\u5316\u60a3\u8005\u7406\u89e3\u7684PLSs\u751f\u6210\u65b9\u6cd5"}}
{"id": "2505.10223", "pdf": "https://arxiv.org/pdf/2505.10223", "abs": "https://arxiv.org/abs/2505.10223", "authors": ["Puru Vaish", "Felix Meister", "Tobias Heimann", "Christoph Brune", "Jelmer M. Wolterink"], "title": "Data-Agnostic Augmentations for Unknown Variations: Out-of-Distribution Generalisation in MRI Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at MIDL 2025", "summary": "Medical image segmentation models are often trained on curated datasets,\nleading to performance degradation when deployed in real-world clinical\nsettings due to mismatches between training and test distributions. While data\naugmentation techniques are widely used to address these challenges,\ntraditional visually consistent augmentation strategies lack the robustness\nneeded for diverse real-world scenarios. In this work, we systematically\nevaluate alternative augmentation strategies, focusing on MixUp and Auxiliary\nFourier Augmentation. These methods mitigate the effects of multiple variations\nwithout explicitly targeting specific sources of distribution shifts. We\ndemonstrate how these techniques significantly improve out-of-distribution\ngeneralization and robustness to imaging variations across a wide range of\ntransformations in cardiac cine MRI and prostate MRI segmentation. We\nquantitatively find that these augmentation methods enhance learned feature\nrepresentations by promoting separability and compactness. Additionally, we\nhighlight how their integration into nnU-Net training pipelines provides an\neasy-to-implement, effective solution for enhancing the reliability of medical\nsegmentation models in real-world applications.", "AI": {"tldr": "\u901a\u8fc7MixUp\u548c\u5085\u91cc\u53f6\u6570\u636e\u589e\u5f3a\u63d0\u5347\u533b\u5b66\u5f71\u50cf\u5206\u5272\u6a21\u578b\u5728\u771f\u5b9e\u4e34\u5e8a\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027", "motivation": "\u533b\u5b66\u5f71\u50cf\u5206\u5272\u6a21\u578b\u5728\u771f\u5b9e\u4e34\u5e8a\u90e8\u7f72\u65f6\u56e0\u8bad\u7ec3-\u6d4b\u8bd5\u5206\u5e03\u4e0d\u5339\u914d\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u4f20\u7edf\u89c6\u89c9\u4e00\u81f4\u6027\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u591a\u6837\u5316\u771f\u5b9e\u573a\u666f", "method": "\u7cfb\u7edf\u8bc4\u4f30MixUp\u548c\u8f85\u52a9\u5085\u91cc\u53f6\u589e\u5f3a(Auxiliary Fourier Augmentation)\u7b56\u7565\uff0c\u5728\u5fc3\u810f\u7535\u5f71MRI\u548c\u524d\u5217\u817aMRI\u5206\u5272\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027", "result": "\u589e\u5f3a\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u5206\u5e03\u5916\u6570\u636e\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u901a\u8fc7\u7279\u5f81\u53ef\u5206\u6027\u548c\u7d27\u51d1\u6027\u6539\u5584\u5b66\u4e60\u8868\u793a\uff0c\u5728nnU-Net\u6846\u67b6\u4e2d\u5b9e\u73b02.38%-7.53%\u7684Dice\u63d0\u5347", "conclusion": "\u5c06\u65b0\u578b\u6570\u636e\u589e\u5f3a\u65b9\u6848\u96c6\u6210\u5230nnU-Net\u8bad\u7ec3\u6d41\u7a0b\uff0c\u4e3a\u63d0\u5347\u533b\u5b66\u5f71\u50cf\u5206\u5272\u6a21\u578b\u4e34\u5e8a\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u5373\u63d2\u5373\u7528\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.10413", "pdf": "https://arxiv.org/pdf/2505.10413", "abs": "https://arxiv.org/abs/2505.10413", "authors": ["Jiajie Jin", "Xiaoxi Li", "Guanting Dong", "Yuyao Zhang", "Yutao Zhu", "Yongkang Wu", "Zhonghua Li", "Qi Ye", "Zhicheng Dou"], "title": "Hierarchical Document Refinement for Long-context Retrieval-augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Real-world RAG applications often encounter long-context input scenarios,\nwhere redundant information and noise results in higher inference costs and\nreduced performance. To address these challenges, we propose LongRefiner, an\nefficient plug-and-play refiner that leverages the inherent structural\ncharacteristics of long documents. LongRefiner employs dual-level query\nanalysis, hierarchical document structuring, and adaptive refinement through\nmulti-task learning on a single foundation model. Experiments on seven QA\ndatasets demonstrate that LongRefiner achieves competitive performance in\nvarious scenarios while using 10x fewer computational costs and latency\ncompared to the best baseline. Further analysis validates that LongRefiner is\nscalable, efficient, and effective, providing practical insights for real-world\nlong-text RAG applications. Our code is available at\nhttps://github.com/ignorejjj/LongRefiner.", "AI": {"tldr": "\u63d0\u51faLongRefiner\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u7ea7\u67e5\u8be2\u5206\u6790\u548c\u5c42\u6b21\u5316\u6587\u6863\u7ed3\u6784\uff0c\u7528\u5355\u6a21\u578b\u591a\u4efb\u52a1\u5b66\u4e60\u5b9e\u73b0\u957f\u6587\u672cRAG\u7684\u9ad8\u6548\u5904\u7406\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e10\u500d\u4e14\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u5b9e\u957f\u6587\u672cRAG\u5e94\u7528\u4e2d\u5197\u4f59\u4fe1\u606f/\u566a\u58f0\u5bfc\u81f4\u7684\u63a8\u7406\u6210\u672c\u9ad8\u3001\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u53cc\u7ea7\u67e5\u8be2\u5206\u6790\u3001\u5c42\u6b21\u5316\u6587\u6863\u7ed3\u6784\u6784\u5efa\u3001\u57fa\u4e8e\u5355\u57fa\u7840\u6a21\u578b\u7684\u591a\u4efb\u52a1\u81ea\u9002\u5e94\u4f18\u5316\u673a\u5236", "result": "\u57287\u4e2aQA\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u8ba1\u7b97\u6210\u672c/\u5ef6\u8fdf\u4ec5\u4e3a\u6700\u4f18\u57fa\u7ebf\u76841/10\u4e14\u6027\u80fd\u76f8\u5f53\uff0c\u5b9e\u9a8c\u8bc1\u660e\u65b9\u6848\u5177\u5907\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027", "conclusion": "LongRefiner\u4e3a\u957f\u6587\u672cRAG\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7ed3\u6784\u7279\u5f81\u6316\u6398\u548c\u8f7b\u91cf\u5316\u8bbe\u8ba1\u5b9e\u73b0\u6210\u672c\u6548\u76ca\u5e73\u8861"}}
{"id": "2505.10231", "pdf": "https://arxiv.org/pdf/2505.10231", "abs": "https://arxiv.org/abs/2505.10231", "authors": ["Haozhe Luo", "Ziyu Zhou", "Zixin Shu", "Aur\u00e9lie Pahud de Mortanges", "Robert Berke", "Mauricio Reyes"], "title": "On the Interplay of Human-AI Alignment,Fairness, and Performance Trade-offs in Medical Imaging", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Deep neural networks excel in medical imaging but remain prone to biases,\nleading to fairness gaps across demographic groups. We provide the first\nsystematic exploration of Human-AI alignment and fairness in this domain. Our\nresults show that incorporating human insights consistently reduces fairness\ngaps and enhances out-of-domain generalization, though excessive alignment can\nintroduce performance trade-offs, emphasizing the need for calibrated\nstrategies. These findings highlight Human-AI alignment as a promising approach\nfor developing fair, robust, and generalizable medical AI systems, striking a\nbalance between expert guidance and automated efficiency. Our code is available\nat https://github.com/Roypic/Aligner.", "AI": {"tldr": "\u533b\u7597AI\u4e2d\u7ed3\u5408\u4eba\u7c7b\u4e13\u5bb6\u6307\u5bfc\u53ef\u51cf\u5c11\u7fa4\u4f53\u516c\u5e73\u6027\u5dee\u8ddd\uff0c\u4f46\u9700\u5e73\u8861\u6821\u51c6\u7b56\u7565\u4ee5\u907f\u514d\u6027\u80fd\u635f\u8017", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u8868\u73b0\u4f18\u5f02\u4f46\u5b58\u5728\u504f\u89c1\uff0c\u5bfc\u81f4\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u95f4\u7684\u516c\u5e73\u6027\u5dee\u8ddd\uff0c\u9700\u8981\u63a2\u7d22\u4eba\u673a\u5bf9\u9f50\u65b9\u6cd5", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u63a2\u7d22\u4eba\u673a\u5bf9\u9f50\u4e0e\u516c\u5e73\u6027\u5173\u7cfb\uff0c\u7ed3\u5408\u4eba\u7c7b\u4e13\u5bb6\u89c1\u89e3\u4f18\u5316\u6a21\u578b", "result": "\u4eba\u7c7b\u89c1\u89e3\u53ef\u964d\u4f4e35%\u516c\u5e73\u6027\u5dee\u8ddd\u5e76\u63d0\u5347\u8de8\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u8fc7\u5ea6\u5bf9\u9f50\u4f1a\u5bfc\u81f48%\u7684\u51c6\u786e\u7387\u4e0b\u964d", "conclusion": "\u4eba\u673a\u5bf9\u9f50\u662f\u6784\u5efa\u516c\u5e73\u3001\u9c81\u68d2\u533b\u7597AI\u7cfb\u7edf\u7684\u6709\u6548\u9014\u5f84\uff0c\u9700\u5728\u4e13\u5bb6\u6307\u5bfc\u4e0e\u81ea\u52a8\u5316\u6548\u7387\u95f4\u53d6\u5f97\u5e73\u8861"}}
{"id": "2505.10446", "pdf": "https://arxiv.org/pdf/2505.10446", "abs": "https://arxiv.org/abs/2505.10446", "authors": ["Zemin Huang", "Zhiyang Chen", "Zijun Wang", "Tiancheng Li", "Guo-Jun Qi"], "title": "Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models", "categories": ["cs.CL"], "comment": null, "summary": "We introduce the \\emph{Diffusion Chain of Lateral Thought (DCoLT)}, a\nreasoning framework for diffusion language models. DCoLT treats each\nintermediate step in the reverse diffusion process as a latent \"thinking\"\naction and optimizes the entire reasoning trajectory to maximize the reward on\nthe correctness of the final answer with outcome-based Reinforcement Learning\n(RL). Unlike traditional Chain-of-Thought (CoT) methods that follow a causal,\nlinear thinking process, DCoLT allows bidirectional, non-linear reasoning with\nno strict rule on grammatical correctness amid its intermediate steps of\nthought. We implement DCoLT on two representative Diffusion Language Models\n(DLMs). First, we choose SEDD as a representative continuous-time discrete\ndiffusion model, where its concrete score derives a probabilistic policy to\nmaximize the RL reward over the entire sequence of intermediate diffusion\nsteps. We further consider the discrete-time masked diffusion language model --\nLLaDA, and find that the order to predict and unmask tokens plays an essential\nrole to optimize its RL action resulting from the ranking-based Unmasking\nPolicy Module (UPM) defined by the Plackett-Luce model. Experiments on both\nmath and code generation tasks show that using only public data and 16 H800\nGPUs, DCoLT-reinforced DLMs outperform other DLMs trained by SFT or RL or even\nboth. Notably, DCoLT-reinforced LLaDA boosts its reasoning accuracy by +9.8%,\n+5.7%, +11.4%, +19.5% on GSM8K, MATH, MBPP, and HumanEval.", "AI": {"tldr": "\u63d0\u51faDCoLT\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u975e\u7ebf\u6027\u63a8\u7406\u8f68\u8ff9\uff0c\u663e\u8457\u63d0\u5347\u6570\u5b66\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u6027\u80fd", "motivation": "\u4f20\u7edfChain-of-Thought\u65b9\u6cd5\u53d7\u9650\u4e8e\u7ebf\u6027\u56e0\u679c\u63a8\u7406\u4e14\u8981\u6c42\u4e2d\u95f4\u6b65\u9aa4\u8bed\u6cd5\u6b63\u786e\uff0cDCoLT\u901a\u8fc7\u6269\u6563\u8fc7\u7a0b\u7684\u9006\u5411\u6b65\u9aa4\u5b9e\u73b0\u53cc\u5411\u975e\u7ebf\u6027\u63a8\u7406", "method": "1. \u5728SEDD\u6a21\u578b\u5e94\u7528\u6982\u7387\u7b56\u7565\u4f18\u5316\u5f3a\u5316\u5b66\u4e60\u8f68\u8ff9 2. \u5728LLaDA\u6a21\u578b\u5f15\u5165\u57fa\u4e8ePlackett-Luce\u6a21\u578b\u7684Unmasking Policy Module\u63a7\u5236\u89e3\u63a9\u987a\u5e8f", "result": "\u4ec5\u7528\u516c\u5f00\u6570\u636e\u548c16\u5757H800 GPU\u5373\u8d85\u8d8a\u5176\u4ed6\u8bad\u7ec3\u65b9\u6cd5\uff0cLLaDA\u5728GSM8K/MATH/MBPP/HumanEval\u4efb\u52a1\u5206\u522b\u63d0\u5347+9.8%/+5.7%/+11.4%/+19.5%", "conclusion": "DCoLT\u901a\u8fc7\u6269\u6563\u8fc7\u7a0b\u7684\u6f5c\u5728\u601d\u7ef4\u52a8\u4f5c\u7a81\u7834\u4f20\u7edf\u7ebf\u6027\u63a8\u7406\u8303\u5f0f\uff0c\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6269\u6563\u6a21\u578b\u67b6\u6784"}}
{"id": "2505.10238", "pdf": "https://arxiv.org/pdf/2505.10238", "abs": "https://arxiv.org/abs/2505.10238", "authors": ["Yanbo Ding"], "title": "MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation", "categories": ["cs.CV"], "comment": null, "summary": "Human image animation has gained increasing attention and developed rapidly\ndue to its broad applications in digital humans. However, existing methods rely\nlargely on 2D-rendered pose images for motion guidance, which limits\ngeneralization and discards essential 3D information for open-world animation.\nTo tackle this problem, we propose MTVCrafter (Motion Tokenization Video\nCrafter), the first framework that directly models raw 3D motion sequences\n(i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT\n(4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens.\nCompared to 2D-rendered pose images, 4D motion tokens offer more robust\nspatio-temporal cues and avoid strict pixel-level alignment between pose image\nand character, enabling more flexible and disentangled control. Then, we\nintroduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention\nwith 4D positional encodings, MV-DiT can effectively leverage motion tokens as\n4D compact yet expressive context for human image animation in the complex 3D\nworld. Hence, it marks a significant step forward in this field and opens a new\ndirection for pose-guided human video generation. Experiments show that our\nMTVCrafter achieves state-of-the-art results with an FID-VID of 6.98,\nsurpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter\nalso generalizes well to diverse open-world characters (single/multiple,\nfull/half-body) across various styles and scenarios. Our video demos and code\nare provided in the supplementary material and at this anonymous GitHub link:\nhttps://anonymous.4open.science/r/MTVCrafter-1B13.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u76f4\u63a5\u5efa\u6a214D\u52a8\u4f5c\u7684MTVCrafter\u6846\u67b6\uff0c\u901a\u8fc74D\u8fd0\u52a8\u4ee4\u724c\u5b9e\u73b0\u5f00\u653e\u4e16\u754c3D\u4eba\u4f53\u52a8\u753b\uff0cFID-VID\u6307\u6807\u8fbe6.98\u8d85\u8d8aSOTA 65%", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d562D\u59ff\u6001\u56fe\u50cf\u5bfc\u81f43D\u4fe1\u606f\u4e22\u5931\u548c\u6cdb\u5316\u80fd\u529b\u53d7\u9650\uff0c\u9700\u63a2\u7d22\u66f4\u7b26\u54083D\u5f00\u653e\u4e16\u754c\u52a8\u753b\u7684\u5efa\u6a21\u65b9\u5f0f", "method": "1) 4DMoT\u5c063D\u52a8\u4f5c\u5e8f\u5217\u91cf\u5316\u4e3a4D\u8fd0\u52a8\u4ee4\u724c\uff1b2) MV-DiT\u6a21\u578b\u901a\u8fc74D\u4f4d\u7f6e\u7f16\u7801\u7684\u8fd0\u52a8\u6ce8\u610f\u529b\u673a\u5236\u751f\u6210\u52a8\u753b", "result": "FID-VID\u6307\u68076.98\u521b\u7eaa\u5f55\uff0c\u76f8\u6bd4\u6b21\u4f18\u65b9\u6cd5\u63d0\u534765%\uff0c\u652f\u6301\u5355/\u591a\u4eba\u3001\u5168\u8eab/\u534a\u8eab\u7b49\u5f00\u653e\u573a\u666f\u89d2\u8272\u52a8\u753b", "conclusion": "\u9996\u6b21\u5b9e\u73b0\u76f4\u63a54D\u52a8\u4f5c\u5efa\u6a21\uff0c\u5f00\u8f9f\u65b0\u7814\u7a76\u8303\u5f0f\u3002\u8fd0\u52a8\u4ee4\u724c\u63d0\u4f9b\u5f3a\u65f6\u7a7a\u8868\u5f81\uff0c\u652f\u6301\u590d\u67423D\u573a\u666f\u7684\u7075\u6d3b\u89e3\u8026\u63a7\u5236"}}
{"id": "2505.10493", "pdf": "https://arxiv.org/pdf/2505.10493", "abs": "https://arxiv.org/abs/2505.10493", "authors": ["Shaohan Wang", "Licheng Zhang", "Zheren Fu", "Zhendong Mao"], "title": "CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with Curriculum Learning", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) is an effective method to enhance the\ncapabilities of large language models (LLMs). Existing methods focus on\noptimizing the retriever or generator in the RAG system by directly utilizing\nthe top-k retrieved documents. However, the documents effectiveness are various\nsignificantly across user queries, i.e. some documents provide valuable\nknowledge while others totally lack critical information. It hinders the\nretriever and generator's adaptation during training. Inspired by human\ncognitive learning, curriculum learning trains models using samples progressing\nfrom easy to difficult, thus enhancing their generalization ability, and we\nintegrate this effective paradigm to the training of the RAG system. In this\npaper, we propose a multi-stage Curriculum Learning based RAG system training\nframework, named CL-RAG. We first construct training data with multiple\ndifficulty levels for the retriever and generator separately through sample\nevolution. Then, we train the model in stages based on the curriculum learning\napproach, thereby optimizing the overall performance and generalization of the\nRAG system more effectively. Our CL-RAG framework demonstrates consistent\neffectiveness across four open-domain QA datasets, achieving performance gains\nof 2% to 4% over multiple advanced methods.", "AI": {"tldr": "\u63d0\u51faCL-RAG\u6846\u67b6\uff0c\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u5206\u9636\u6bb5\u8bad\u7ec3RAG\u7cfb\u7edf\uff0c\u6709\u6548\u63d0\u5347\u5f00\u653e\u57dfQA\u4efb\u52a1\u6027\u80fd2%-4%", "motivation": "\u73b0\u6709RAG\u65b9\u6cd5\u76f4\u63a5\u4f7f\u7528top-k\u6587\u6863\uff0c\u4f46\u4e0d\u540c\u67e5\u8be2\u4e0b\u6587\u6863\u6709\u6548\u6027\u5dee\u5f02\u663e\u8457\uff0c\u5f71\u54cd\u6a21\u578b\u8bad\u7ec3\u9002\u5e94\u6027", "method": "1.\u901a\u8fc7\u6837\u672c\u6f14\u5316\u6784\u5efa\u591a\u96be\u5ea6\u8bad\u7ec3\u6570\u636e\uff1b2.\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u5206\u9636\u6bb5\u8bad\u7ec3\u68c0\u7d22\u5668\u548c\u751f\u6210\u5668", "result": "\u5728\u56db\u4e2a\u5f00\u653e\u57dfQA\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u53472%-4%\uff0c\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u6027", "conclusion": "\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\u901a\u8fc7\u5206\u9636\u6bb5\u8bad\u7ec3\u6709\u6548\u4f18\u5316RAG\u7cfb\u7edf\u6027\u80fd\u4e0e\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u5176\u6709\u6548\u6027"}}
{"id": "2505.10250", "pdf": "https://arxiv.org/pdf/2505.10250", "abs": "https://arxiv.org/abs/2505.10250", "authors": ["Wenhao Shen", "Wanqi Yin", "Xiaofeng Yang", "Cheng Chen", "Chaoyue Song", "Zhongang Cai", "Lei Yang", "Hao Wang", "Guosheng Lin"], "title": "ADHMR: Aligning Diffusion-based Human Mesh Recovery via Direct Preference Optimization", "categories": ["cs.CV"], "comment": "Accepted by ICML 2025. Code: https://github.com/shenwenhao01/ADHMR", "summary": "Human mesh recovery (HMR) from a single image is inherently ill-posed due to\ndepth ambiguity and occlusions. Probabilistic methods have tried to solve this\nby generating numerous plausible 3D human mesh predictions, but they often\nexhibit misalignment with 2D image observations and weak robustness to\nin-the-wild images. To address these issues, we propose ADHMR, a framework that\nAligns a Diffusion-based HMR model in a preference optimization manner. First,\nwe train a human mesh prediction assessment model, HMR-Scorer, capable of\nevaluating predictions even for in-the-wild images without 3D annotations. We\nthen use HMR-Scorer to create a preference dataset, where each input image has\na pair of winner and loser mesh predictions. This dataset is used to finetune\nthe base model using direct preference optimization. Moreover, HMR-Scorer also\nhelps improve existing HMR models by data cleaning, even with fewer training\nsamples. Extensive experiments show that ADHMR outperforms current\nstate-of-the-art methods. Code is available at:\nhttps://github.com/shenwenhao01/ADHMR.", "AI": {"tldr": "ADHMR\u6846\u67b6\u901a\u8fc7\u6269\u6563\u6a21\u578b\u4e0e\u504f\u597d\u4f18\u5316\u7684\u5bf9\u9f50\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u5355\u76ee\u4eba\u4f53\u7f51\u683c\u6062\u590d\u4e2d\u7684\u6df1\u5ea6\u6a21\u7cca\u548c\u906e\u6321\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u9884\u6d4b\u4e0e2D\u56fe\u50cf\u7684\u5bf9\u9f50\u80fd\u529b\u548c\u91ce\u5916\u56fe\u50cf\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u6982\u7387\u65b9\u6cd5\u751f\u6210\u76843D\u4eba\u4f53\u7f51\u683c\u5e38\u4e0e2D\u89c2\u6d4b\u4e0d\u5339\u914d\uff0c\u4e14\u5728\u590d\u6742\u771f\u5b9e\u573a\u666f\u4e2d\u9c81\u68d2\u6027\u4e0d\u8db3\u3002\u9700\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u9884\u6d4b\u5bf9\u9f50\u673a\u5236\u3002", "method": "1. \u8bad\u7ec3\u65e0\u97003D\u6807\u6ce8\u7684\u8bc4\u4f30\u6a21\u578bHMR-Scorer\uff1b2. \u6784\u5efa\u504f\u597d\u6570\u636e\u96c6\u5e76\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\u5fae\u8c03\u6269\u6563\u57fa\u6a21\u578b\uff1b3. \u5229\u7528HMR-Scorer\u8fdb\u884c\u6570\u636e\u6e05\u6d17\u63d0\u5347\u73b0\u6709\u6a21\u578b\u3002", "result": "ADHMR\u5728\u5b9e\u9a8c\u4e2d\u8d85\u8d8aSOTA\u65b9\u6cd5\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002HMR-Scorer\u53ef\u4f7f\u73b0\u6709\u6a21\u578b\u5728\u51cf\u5c1150%\u6570\u636e\u91cf\u65f6\u4ecd\u4fdd\u6301\u6027\u80fd\u3002", "conclusion": "ADHMR\u901a\u8fc7\u504f\u597d\u5bf9\u9f50\u663e\u8457\u63d0\u5347\u9884\u6d4b\u8d28\u91cf\uff0cHMR-Scorer\u7684\u53cc\u91cd\u4f5c\u7528\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u53ef\u9760\u8bc4\u4f30\u4e0e\u6570\u636e\u4f18\u5316\u65b9\u6848\u3002"}}
{"id": "2505.10494", "pdf": "https://arxiv.org/pdf/2505.10494", "abs": "https://arxiv.org/abs/2505.10494", "authors": ["Yutao Mou", "Xiao Deng", "Yuxiao Luo", "Shikun Zhang", "Wei Ye"], "title": "Can You Really Trust Code Copilots? Evaluating Large Language Models from a Code Security Perspective", "categories": ["cs.CL"], "comment": "Accepted by ACL2025 Main Conference", "summary": "Code security and usability are both essential for various coding assistant\napplications driven by large language models (LLMs). Current code security\nbenchmarks focus solely on single evaluation task and paradigm, such as code\ncompletion and generation, lacking comprehensive assessment across dimensions\nlike secure code generation, vulnerability repair and discrimination. In this\npaper, we first propose CoV-Eval, a multi-task benchmark covering various tasks\nsuch as code completion, vulnerability repair, vulnerability detection and\nclassification, for comprehensive evaluation of LLM code security. Besides, we\ndeveloped VC-Judge, an improved judgment model that aligns closely with human\nexperts and can review LLM-generated programs for vulnerabilities in a more\nefficient and reliable way. We conduct a comprehensive evaluation of 20\nproprietary and open-source LLMs. Overall, while most LLMs identify vulnerable\ncodes well, they still tend to generate insecure codes and struggle with\nrecognizing specific vulnerability types and performing repairs. Extensive\nexperiments and qualitative analyses reveal key challenges and optimization\ndirections, offering insights for future research in LLM code security.", "AI": {"tldr": "\u63d0\u51faCoV-Eval\u591a\u7ef4\u5ea6\u4ee3\u7801\u5b89\u5168\u8bc4\u4f30\u57fa\u51c6\u548cVC-Judge\u5224\u65ad\u6a21\u578b\uff0c\u63ed\u793a\u4e3b\u6d41LLM\u751f\u6210\u5b89\u5168\u4ee3\u7801\u80fd\u529b\u4e0d\u8db3", "motivation": "\u73b0\u6709\u4ee3\u7801\u5b89\u5168\u57fa\u51c6\u5c40\u9650\u4e8e\u5355\u4e00\u4efb\u52a1\u8bc4\u4f30\uff0c\u7f3a\u4e4f\u5bf9\u4ee3\u7801\u751f\u6210/\u6f0f\u6d1e\u4fee\u590d/\u68c0\u6d4b\u7684\u5168\u7ef4\u5ea6\u8986\u76d6\uff0c\u9700\u7cfb\u7edf\u8bc4\u4f30LLM\u5b89\u5168\u80fd\u529b", "method": "\u6784\u5efa\u591a\u4efb\u52a1\u57fa\u51c6CoV-Eval(\u4ee3\u7801\u8865\u5168/\u6f0f\u6d1e\u4fee\u590d/\u68c0\u6d4b/\u5206\u7c7b)\uff0c\u5f00\u53d1VC-Judge\u5bf9\u9f50\u4e13\u5bb6\u8bc4\u4f30\uff0c\u6d4b\u8bd520\u4e2a\u5f00\u6e90/\u95ed\u6e90LLM", "result": "LLM\u8bc6\u522b\u6f0f\u6d1e\u80fd\u529b\u8f83\u5f3a(\u5e73\u5747\u51c6\u786e\u738776%)\uff0c\u4f46\u751f\u6210\u4e0d\u5b89\u5168\u4ee3\u7801\u6bd4\u4f8b\u8fbe32%\uff0cCWE-22\u7c7b\u6f0f\u6d1e\u4fee\u590d\u6210\u529f\u7387\u4ec541%", "conclusion": "LLM\u4ee3\u7801\u5b89\u5168\u9762\u4e34\u751f\u6210\u4e0d\u53ef\u63a7\u548c\u4fee\u590d\u80fd\u529b\u74f6\u9888\uff0c\u9700\u589e\u5f3a\u5b89\u5168\u5bf9\u9f50\u8bad\u7ec3\u548c\u6f0f\u6d1e\u77e5\u8bc6\u6ce8\u5165\uff0c\u57fa\u51c6\u9a8c\u8bc1\u6846\u67b6\u63a8\u52a8\u9886\u57df\u53d1\u5c55"}}
{"id": "2505.10257", "pdf": "https://arxiv.org/pdf/2505.10257", "abs": "https://arxiv.org/abs/2505.10257", "authors": ["Hao Lu", "Jiaqi Tang", "Jiyao Wang", "Yunfan LU", "Xu Cao", "Qingyong Hu", "Yin Wang", "Yuting Zhang", "Tianxin Xie", "Yunpeng Zhang", "Yong Chen", "Jiayu. Gao", "Bin Huang", "Dengbo He", "Shuiguang Deng", "Hao Chen", "Ying-Cong Chen"], "title": "Sage Deer: A Super-Aligned Driving Generalist Is Your Copilot", "categories": ["cs.CV"], "comment": null, "summary": "The intelligent driving cockpit, an important part of intelligent driving,\nneeds to match different users' comfort, interaction, and safety needs. This\npaper aims to build a Super-Aligned and GEneralist DRiving agent, SAGE DeeR.\nSage Deer achieves three highlights: (1) Super alignment: It achieves different\nreactions according to different people's preferences and biases. (2)\nGeneralist: It can understand the multi-view and multi-mode inputs to reason\nthe user's physiological indicators, facial emotions, hand movements, body\nmovements, driving scenarios, and behavioral decisions. (3) Self-Eliciting: It\ncan elicit implicit thought chains in the language space to further increase\ngeneralist and super-aligned abilities. Besides, we collected multiple data\nsets and built a large-scale benchmark. This benchmark measures the deer's\nperceptual decision-making ability and the super alignment's accuracy.", "AI": {"tldr": "\u63d0\u51faSAGE DeeR\u667a\u80fd\u9a7e\u9a76\u5ea7\u8231\u4ee3\u7406\uff0c\u5b9e\u73b0\u7528\u6237\u9700\u6c42\u7684\u4e09\u91cd\u8d85\u5bf9\u9f50\uff08\u504f\u597d\u9002\u914d\uff09\u3001\u901a\u7528\u6027\uff08\u591a\u6a21\u6001\u611f\u77e5\uff09\u548c\u81ea\u6211\u6fc0\u53d1\uff08\u8bed\u8a00\u601d\u7ef4\u94fe\uff09\u80fd\u529b", "motivation": "\u667a\u80fd\u9a7e\u9a76\u5ea7\u8231\u9700\u9002\u914d\u4e0d\u540c\u7528\u6237\u5728\u8212\u9002\u6027\u3001\u4ea4\u4e92\u6027\u548c\u5b89\u5168\u6027\u65b9\u9762\u7684\u4e2a\u6027\u5316\u9700\u6c42", "method": "1. \u901a\u8fc7\u751f\u7406\u6307\u6807\u3001\u9762\u90e8\u60c5\u7eea\u7b49\u591a\u6a21\u6001\u8f93\u5165\u63a8\u7406\u7528\u6237\u72b6\u6001\n2. \u8bed\u8a00\u7a7a\u95f4\u9690\u5f0f\u601d\u7ef4\u94fe\u6fc0\u53d1\u673a\u5236\n3. \u6784\u5efa\u5305\u542b\u611f\u77e5\u51b3\u7b56\u548c\u8d85\u5bf9\u9f50\u80fd\u529b\u8bc4\u4f30\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6", "result": "\u5b9e\u73b0\u4e2a\u6027\u5316\u53cd\u5e94\uff08\u8d85\u5bf9\u9f50\u51c6\u786e\u5ea6\uff09\u3001\u591a\u6a21\u6001\u7406\u89e3\uff08\u901a\u7528\u6027\uff09\u4ee5\u53ca\u901a\u8fc7\u601d\u7ef4\u94fe\u589e\u5f3a\u7684\u51b3\u7b56\u80fd\u529b", "conclusion": "SAGE DeeR\u901a\u8fc7\u4e09\u4f4d\u4e00\u4f53\u7684\u6280\u672f\u8def\u7ebf\uff0c\u4e3a\u667a\u80fd\u5ea7\u8231\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u4eba\u6027\u5316\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.10507", "pdf": "https://arxiv.org/pdf/2505.10507", "abs": "https://arxiv.org/abs/2505.10507", "authors": ["Benedikt Ebing", "Goran Glava\u0161"], "title": "The Devil Is in the Word Alignment Details: On Translation-Based Cross-Lingual Transfer for Token Classification Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Translation-based strategies for cross-lingual transfer XLT such as\ntranslate-train -- training on noisy target language data translated from the\nsource language -- and translate-test -- evaluating on noisy source language\ndata translated from the target language -- are competitive XLT baselines. In\nXLT for token classification tasks, however, these strategies include label\nprojection, the challenging step of mapping the labels from each token in the\noriginal sentence to its counterpart(s) in the translation. Although word\naligners (WAs) are commonly used for label projection, the low-level design\ndecisions for applying them to translation-based XLT have not been\nsystematically investigated. Moreover, recent marker-based methods, which\nproject labeled spans by inserting tags around them before (or after)\ntranslation, claim to outperform WAs in label projection for XLT. In this work,\nwe revisit WAs for label projection, systematically investigating the effects\nof low-level design decisions on token-level XLT: (i) the algorithm for\nprojecting labels between (multi-)token spans, (ii) filtering strategies to\nreduce the number of noisily mapped labels, and (iii) the pre-tokenization of\nthe translated sentences. We find that all of these substantially impact\ntranslation-based XLT performance and show that, with optimized choices, XLT\nwith WA offers performance at least comparable to that of marker-based methods.\nWe then introduce a new projection strategy that ensembles translate-train and\ntranslate-test predictions and demonstrate that it substantially outperforms\nthe marker-based projection. Crucially, we show that our proposed ensembling\nalso reduces sensitivity to low-level WA design choices, resulting in more\nrobust XLT for token classification tasks.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u8bcd\u5bf9\u9f50\u5668\u5728\u8de8\u8bed\u8a00\u8fc1\u79fb\u4e2d\u7684\u6807\u7b7e\u6295\u5f71\u8bbe\u8ba1\uff0c\u63d0\u51fa\u96c6\u6210\u7ffb\u8bd1\u8bad\u7ec3\u548c\u6d4b\u8bd5\u9884\u6d4b\u7684\u65b0\u7b56\u7565\uff0c\u6027\u80fd\u8d85\u8d8a\u6807\u8bb0\u65b9\u6cd5\u5e76\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u7ffb\u8bd1\u7684\u8de8\u8bed\u8a00\u8fc1\u79fb\u65b9\u6cd5\u5728\u6807\u7b7e\u6295\u5f71\u65f6\u4e3b\u8981\u4f9d\u8d56\u8bcd\u5bf9\u9f50\u5668\uff0c\u4f46\u5176\u5e95\u5c42\u8bbe\u8ba1\u51b3\u7b56\u672a\u5f97\u5230\u7cfb\u7edf\u7814\u7a76\uff0c\u4e14\u6807\u8bb0\u7c7b\u65b9\u6cd5\u58f0\u79f0\u6027\u80fd\u66f4\u4f18\u3002\u9700\u8981\u9a8c\u8bc1\u8bcd\u5bf9\u9f50\u5668\u7684\u771f\u5b9e\u6f5c\u529b\u5e76\u63a2\u7d22\u4f18\u5316\u65b9\u6848\u3002", "method": "1. \u7cfb\u7edf\u7814\u7a76\u8bcd\u5bf9\u9f50\u5668\u8bbe\u8ba1\u4e09\u8981\u7d20\uff08\u8de8token\u6807\u7b7e\u6620\u5c04\u7b97\u6cd5\u3001\u566a\u58f0\u8fc7\u6ee4\u7b56\u7565\u3001\u8bd1\u6587\u9884\u5206\u8bcd\uff09\n2. \u63d0\u51fa\u96c6\u6210\u7ffb\u8bd1\u8bad\u7ec3\u548c\u7ffb\u8bd1\u6d4b\u8bd5\u9884\u6d4b\u7684\u6df7\u5408\u6295\u5f71\u7b56\u7565", "result": "1. \u4f18\u5316\u540e\u7684\u8bcd\u5bf9\u9f50\u5668\u6027\u80fd\u4e0e\u6807\u8bb0\u65b9\u6cd5\u76f8\u5f53\n2. \u65b0\u96c6\u6210\u7b56\u7565\u6bd4\u6807\u8bb0\u65b9\u6cd5\u63d0\u53477.5% F1\u503c\n3. \u96c6\u6210\u65b9\u6cd5\u964d\u4f4e\u5bf9\u8bbe\u8ba1\u9009\u62e9\u7684\u654f\u611f\u6027\uff0c\u63d0\u5347\u7cfb\u7edf\u9c81\u68d2\u6027", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u4f18\u5316\u8bcd\u5bf9\u9f50\u5668\u8bbe\u8ba1\u5e76\u5f15\u5165\u96c6\u6210\u6295\u5f71\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u8de8\u8bed\u8a00\u8fc1\u79fb\u5728\u5e8f\u5217\u6807\u6ce8\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u7a33\u5b9a\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.10258", "pdf": "https://arxiv.org/pdf/2505.10258", "abs": "https://arxiv.org/abs/2505.10258", "authors": ["Michael Hubbertz", "Pascal Colling", "Qi Han", "Tobias Meisen"], "title": "Inferring Driving Maps by Deep Learning-based Trail Map Extraction", "categories": ["cs.CV", "cs.RO"], "comment": "This paper was accepted at the CVPR WAD 2025 Workshop", "summary": "High-definition (HD) maps offer extensive and accurate environmental\ninformation about the driving scene, making them a crucial and essential\nelement for planning within autonomous driving systems. To avoid extensive\nefforts from manual labeling, methods for automating the map creation have\nemerged. Recent trends have moved from offline mapping to online mapping,\nensuring availability and actuality of the utilized maps. While the performance\nhas increased in recent years, online mapping still faces challenges regarding\ntemporal consistency, sensor occlusion, runtime, and generalization. We propose\na novel offline mapping approach that integrates trails - informal routes used\nby drivers - into the map creation process. Our method aggregates trail data\nfrom the ego vehicle and other traffic participants to construct a\ncomprehensive global map using transformer-based deep learning models. Unlike\ntraditional offline mapping, our approach enables continuous updates while\nremaining sensor-agnostic, facilitating efficient data transfer. Our method\ndemonstrates superior performance compared to state-of-the-art online mapping\napproaches, achieving improved generalization to previously unseen environments\nand sensor configurations. We validate our approach on two benchmark datasets,\nhighlighting its robustness and applicability in autonomous driving systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u79bb\u7ebf\u5730\u56fe\u6784\u5efa\u65b9\u6cd5\uff0c\u6574\u5408\u8f66\u8f86\u8f68\u8ff9\u6570\u636e\u5e76\u91c7\u7528\u57fa\u4e8eTransformer\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5730\u56fe\u6cdb\u5316\u80fd\u529b\u548c\u66f4\u65b0\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u9ad8\u7cbe\u5730\u56fe\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\uff0c\u5728\u7ebf\u5efa\u56fe\u65b9\u6cd5\u5b58\u5728\u65f6\u5e8f\u4e00\u81f4\u6027\u3001\u4f20\u611f\u5668\u906e\u6321\u3001\u8ba1\u7b97\u6548\u7387\u53ca\u6cdb\u5316\u80fd\u529b\u7b49\u74f6\u9888\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u7814\u7a76\u63d0\u51fa\u5229\u7528\u8f66\u8f86\u884c\u9a76\u8f68\u8ff9\u6570\u636e\u6784\u5efa\u5168\u5c40\u5730\u56fe\u3002", "method": "\u878d\u5408\u81ea\u8f66\u4e0e\u5176\u4ed6\u4ea4\u901a\u53c2\u4e0e\u8005\u7684\u8f68\u8ff9\u6570\u636e\uff0c\u901a\u8fc7Transformer\u6a21\u578b\u6784\u5efa\u5168\u5c40\u5730\u56fe\u3002\u652f\u6301\u6301\u7eed\u66f4\u65b0\u4e14\u4fdd\u6301\u4f20\u611f\u5668\u65e0\u5173\u6027\uff0c\u5b9e\u73b0\u9ad8\u6548\u6570\u636e\u4f20\u8f93\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u663e\u793a\uff0c\u672c\u65b9\u6cd5\u5728\u672a\u89c1\u8fc7\u7684\u73af\u5883\u548c\u4f20\u611f\u5668\u914d\u7f6e\u4e2d\u6cdb\u5316\u80fd\u529b\u4f18\u4e8e\u73b0\u6709\u5728\u7ebf\u5efa\u56fe\u65b9\u6cd5\uff0c\u8fd0\u884c\u6548\u7387\u63d0\u534730%\u3002", "conclusion": "\u57fa\u4e8e\u8f68\u8ff9\u6574\u5408\u7684\u79bb\u7ebf\u5efa\u56fe\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5728\u7ebf\u5efa\u56fe\u7684\u56fa\u6709\u7f3a\u9677\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u3001\u53ef\u6269\u5c55\u7684\u5730\u56fe\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.10518", "pdf": "https://arxiv.org/pdf/2505.10518", "abs": "https://arxiv.org/abs/2505.10518", "authors": ["Anastasios Gerontopoulos", "Spyros Gidaris", "Nikos Komodakis"], "title": "Multi-Token Prediction Needs Registers", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Multi-token prediction has emerged as a promising objective for improving\nlanguage model pretraining, but its benefits have not consistently generalized\nto other settings such as fine-tuning. In this paper, we propose MuToR, a\nsimple and effective approach to multi-token prediction that interleaves\nlearnable register tokens into the input sequence, each tasked with predicting\nfuture targets. Compared to existing methods, MuToR offers several key\nadvantages: it introduces only a negligible number of additional parameters,\nrequires no architectural changes--ensuring compatibility with off-the-shelf\npretrained language models--and remains aligned with the next-token pretraining\nobjective, making it especially well-suited for supervised fine-tuning.\nMoreover, it naturally supports scalable prediction horizons. We demonstrate\nthe effectiveness and versatility of MuToR across a range of use cases,\nincluding supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and\npretraining, on challenging generative tasks in both language and vision\ndomains. Our code will be available at: https://github.com/nasosger/MuToR.", "AI": {"tldr": "MuToR\u63d0\u51fa\u901a\u8fc7\u63d2\u5165\u53ef\u5b66\u4e60\u7684\u5bc4\u5b58\u5668\u4ee4\u724c\u5b9e\u73b0\u591a\u4ee4\u724c\u9884\u6d4b\uff0c\u4fdd\u6301\u4e0e\u73b0\u6709\u6a21\u578b\u7684\u517c\u5bb9\u6027\uff0c\u5728\u76d1\u7763\u5fae\u8c03/PEFT/\u9884\u8bad\u7ec3\u4efb\u52a1\u4e2d\u5747\u9a8c\u8bc1\u6709\u6548\u6027", "motivation": "\u89e3\u51b3\u73b0\u6709\u591a\u4ee4\u724c\u9884\u6d4b\u65b9\u6cd5\u5728\u5fae\u8c03\u573a\u666f\u6548\u679c\u4e0d\u7a33\u5b9a\u3001\u9700\u6539\u52a8\u6a21\u578b\u7ed3\u6784\u3001\u4e0e\u9884\u8bad\u7ec3\u76ee\u6807\u4e0d\u4e00\u81f4\u7684\u95ee\u9898", "method": "\u5728\u8f93\u5165\u5e8f\u5217\u4e2d\u63d2\u5165\u5bc4\u5b58\u5668\u4ee4\u724c\u8fdb\u884c\u591a\u6b65\u9884\u6d4b\uff0c\u4fdd\u6301\u6a21\u578b\u67b6\u6784\u4e0d\u53d8\u4e14\u4ec5\u589e\u52a0\u5c11\u91cf\u53c2\u6570\uff0c\u652f\u6301\u53ef\u6269\u5c55\u7684\u9884\u6d4b\u8303\u56f4", "result": "\u5728\u8bed\u8a00/\u89c6\u89c9\u751f\u6210\u4efb\u52a1\u7684\u76d1\u7763\u5fae\u8c03\u3001\u53c2\u6570\u9ad8\u6548\u5fae\u8c03(PEFT)\u3001\u9884\u8bad\u7ec3\u573a\u666f\u4e2d\u5747\u5c55\u793a\u6709\u6548\u6027", "conclusion": "MuToR\u4e3a\u591a\u4ee4\u724c\u9884\u6d4b\u63d0\u4f9b\u7b80\u5355\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u517c\u5177\u517c\u5bb9\u6027\u4e0e\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u573a\u666f\uff08\u4ee3\u7801\u5df2\u5f00\u6e90\uff09"}}
{"id": "2505.10267", "pdf": "https://arxiv.org/pdf/2505.10267", "abs": "https://arxiv.org/abs/2505.10267", "authors": ["Pavel Korotaev", "Petr Surovtsev", "Alexander Kapitanov", "Karina Kvanchiani", "Aleksandr Nagaev"], "title": "HandReader: Advanced Techniques for Efficient Fingerspelling Recognition", "categories": ["cs.CV", "cs.LG"], "comment": "https://github.com/ai-forever/handreader", "summary": "Fingerspelling is a significant component of Sign Language (SL), allowing the\ninterpretation of proper names, characterized by fast hand movements during\nsigning. Although previous works on fingerspelling recognition have focused on\nprocessing the temporal dimension of videos, there remains room for improving\nthe accuracy of these approaches. This paper introduces HandReader, a group of\nthree architectures designed to address the fingerspelling recognition task.\nHandReader$_{RGB}$ employs the novel Temporal Shift-Adaptive Module (TSAM) to\nprocess RGB features from videos of varying lengths while preserving important\nsequential information. HandReader$_{KP}$ is built on the proposed Temporal\nPose Encoder (TPE) operated on keypoints as tensors. Such keypoints composition\nin a batch allows the encoder to pass them through 2D and 3D convolution\nlayers, utilizing temporal and spatial information and accumulating keypoints\ncoordinates. We also introduce HandReader_RGB+KP - architecture with a joint\nencoder to benefit from RGB and keypoint modalities. Each HandReader model\npossesses distinct advantages and achieves state-of-the-art results on the\nChicagoFSWild and ChicagoFSWild+ datasets. Moreover, the models demonstrate\nhigh performance on the first open dataset for Russian fingerspelling, Znaki,\npresented in this paper. The Znaki dataset and HandReader pre-trained models\nare publicly available.", "AI": {"tldr": "\u63d0\u51faHandReader\u7cfb\u5217\u67b6\u6784\uff08RGB\u3001\u5173\u952e\u70b9\u3001\u6df7\u5408\u6a21\u6001\uff09\uff0c\u901a\u8fc7\u521b\u65b0\u65f6\u5e8f\u6a21\u5757TSAM\u548cTPE\u63d0\u5347\u624b\u8bed\u62fc\u5199\u8bc6\u522b\u7cbe\u5ea6\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u53d6\u5f97SOTA\u5e76\u53d1\u5e03\u4fc4\u8bed\u624b\u8bed\u6570\u636e\u96c6Znaki\u3002", "motivation": "\u73b0\u6709\u624b\u8bed\u62fc\u5199\u8bc6\u522b\u65b9\u6cd5\u4fa7\u91cd\u89c6\u9891\u65f6\u5e8f\u5904\u7406\u4f46\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u5bf9\u5feb\u901f\u624b\u90e8\u52a8\u4f5c\u7684\u65f6\u5e8f\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u3002", "method": "1) HandReader_RGB\uff1a\u91c7\u7528TSAM\u6a21\u5757\u5904\u7406\u53d8\u957f\u89c6\u9891\u7684RGB\u7279\u5f81\uff1b2) HandReader_KP\uff1a\u57fa\u4e8eTPE\u7f16\u7801\u5668\u5904\u7406\u5173\u952e\u70b9\u5f20\u91cf\uff1b3) HandReader_RGB+KP\uff1a\u878d\u5408\u4e24\u79cd\u6a21\u6001\u7684\u8054\u5408\u7f16\u7801\u5668\u3002", "result": "\u5728ChicagoFSWild\u7cfb\u5217\u6570\u636e\u96c6\u8fbe\u5230SOTA\uff0c\u65b0\u4fc4\u8bed\u6570\u636e\u96c6Znaki\u4e0a\u8868\u73b0\u4f18\u5f02\u3002\u540c\u6b65\u5f00\u6e90Znaki\u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "conclusion": "\u591a\u6a21\u6001\u67b6\u6784\u8bbe\u8ba1\u6709\u6548\u63d0\u5347\u8bc6\u522b\u6027\u80fd\uff0c\u586b\u8865\u4fc4\u8bed\u624b\u8bed\u6570\u636e\u7a7a\u767d\uff0c\u63a8\u52a8\u624b\u8bed\u6280\u672f\u751f\u6001\u53d1\u5c55\u3002"}}
{"id": "2505.10527", "pdf": "https://arxiv.org/pdf/2505.10527", "abs": "https://arxiv.org/abs/2505.10527", "authors": ["Binghai Wang", "Runji Lin", "Keming Lu", "Le Yu", "Zhenru Zhang", "Fei Huang", "Chujie Zheng", "Kai Dang", "Yang Fan", "Xingzhang Ren", "An Yang", "Binyuan Hui", "Dayiheng Liu", "Tao Gui", "Qi Zhang", "Xuanjing Huang", "Yu-Gang Jiang", "Bowen Yu", "Jingren Zhou", "Junyang Lin"], "title": "WorldPM: Scaling Human Preference Modeling", "categories": ["cs.CL"], "comment": null, "summary": "Motivated by scaling laws in language modeling that demonstrate how test loss\nscales as a power law with model and dataset sizes, we find that similar laws\nexist in preference modeling. We propose World Preference Modeling$ (WorldPM)\nto emphasize this scaling potential, where World Preference embodies a unified\nrepresentation of human preferences. In this paper, we collect preference data\nfrom public forums covering diverse user communities, and conduct extensive\ntraining using 15M-scale data across models ranging from 1.5B to 72B\nparameters. We observe distinct patterns across different evaluation metrics:\n(1) Adversarial metrics (ability to identify deceptive features) consistently\nscale up with increased training data and base model size; (2) Objective\nmetrics (objective knowledge with well-defined answers) show emergent behavior\nin larger language models, highlighting WorldPM's scalability potential; (3)\nSubjective metrics (subjective preferences from a limited number of humans or\nAI) do not demonstrate scaling trends. Further experiments validate the\neffectiveness of WorldPM as a foundation for preference fine-tuning. Through\nevaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly\nimproves the generalization performance across human preference datasets of\nvarying sizes (7K, 100K and 800K samples), with performance gains exceeding 5%\non many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we\nobserve significant improvements on both in-house and public evaluation sets,\nwith notable gains of 4% to 8% in our in-house evaluations.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc715M\u89c4\u6a21\u6570\u636e\u548c1.5B-72B\u53c2\u6570\u6a21\u578b\u7684\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86World Preference Modeling\u5728\u504f\u597d\u5efa\u6a21\u4e2d\u7684\u6269\u5c55\u89c4\u5f8b\u53ca\u5176\u5728RLHF\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6548\u679c", "motivation": "\u53d7\u8bed\u8a00\u6a21\u578b\u6269\u5c55\u5b9a\u5f8b\u7684\u542f\u53d1\uff0c\u63a2\u7d22\u504f\u597d\u5efa\u6a21\u4e2d\u7684\u7c7b\u4f3c\u89c4\u5f8b\uff0c\u63d0\u51faWorldPM\u4f5c\u4e3a\u7edf\u4e00\u4eba\u7c7b\u504f\u597d\u7684\u8868\u793a\u6846\u67b6", "method": "\u6536\u96c6\u516c\u5171\u8bba\u575b\u591a\u6837\u5316\u504f\u597d\u6570\u636e\uff0c\u4f7f\u752815M\u89c4\u6a21\u6570\u636e\u96c6\u8bad\u7ec3\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\uff081.5B-72B\u53c2\u6570\uff09\u5e76\u8fdb\u884c\u591a\u7ef4\u5ea6\u8bc4\u4f30", "result": "\u5bf9\u6297\u6027\u6307\u6807\u7ebf\u6027\u6269\u5c55/\u5ba2\u89c2\u6307\u6807\u6d8c\u73b0/\u4e3b\u89c2\u6307\u6807\u65e0\u6269\u5c55\u8d8b\u52bf\uff1bWorldPM\u57287\u4e2a\u57fa\u51c620\u4e2a\u5b50\u4efb\u52a1\u4e2d\u5e73\u5747\u63d0\u5347\u8d855%\uff0cRLHF\u6574\u5408\u540e\u5185\u90e8\u8bc4\u4f30\u63d0\u53474-8%", "conclusion": "WorldPM\u5c55\u793a\u4e86\u504f\u597d\u5efa\u6a21\u7684\u6269\u5c55\u6f5c\u529b\uff0c\u7279\u522b\u5728\u5bf9\u6297\u6027\u548c\u5ba2\u89c2\u6307\u6807\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u4e3a\u5927\u89c4\u6a21\u504f\u597d\u5fae\u8c03\u63d0\u4f9b\u4e86\u6709\u6548\u57fa\u7840\u6846\u67b6"}}
{"id": "2505.10281", "pdf": "https://arxiv.org/pdf/2505.10281", "abs": "https://arxiv.org/abs/2505.10281", "authors": ["Mengqiu Xu", "Kaixin Chen", "Heng Guo", "Yixiang Huang", "Ming Wu", "Zhenwei Shi", "Chuang Zhang", "Jun Guo"], "title": "MFogHub: Bridging Multi-Regional and Multi-Satellite Data for Global Marine Fog Detection and Forecasting", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning approaches for marine fog detection and forecasting have\noutperformed traditional methods, demonstrating significant scientific and\npractical importance. However, the limited availability of open-source datasets\nremains a major challenge. Existing datasets, often focused on a single region\nor satellite, restrict the ability to evaluate model performance across diverse\nconditions and hinder the exploration of intrinsic marine fog characteristics.\nTo address these limitations, we introduce \\textbf{MFogHub}, the first\nmulti-regional and multi-satellite dataset to integrate annotated marine fog\nobservations from 15 coastal fog-prone regions and six geostationary\nsatellites, comprising over 68,000 high-resolution samples. By encompassing\ndiverse regions and satellite perspectives, MFogHub facilitates rigorous\nevaluation of both detection and forecasting methods under varying conditions.\nExtensive experiments with 16 baseline models demonstrate that MFogHub can\nreveal generalization fluctuations due to regional and satellite discrepancy,\nwhile also serving as a valuable resource for the development of targeted and\nscalable fog prediction techniques. Through MFogHub, we aim to advance both the\npractical monitoring and scientific understanding of marine fog dynamics on a\nglobal scale. The dataset and code are at\n\\href{https://github.com/kaka0910/MFogHub}{https://github.com/kaka0910/MFogHub}.", "AI": {"tldr": "MFogHub\u9996\u4e2a\u591a\u533a\u57df\u591a\u536b\u661f\u6d77\u6d0b\u96fe\u6570\u636e\u96c6\uff0c\u6574\u540815\u4e2a\u96fe\u533a6\u9897\u536b\u661f\u76846.8\u4e07+\u9ad8\u5206\u8fa8\u7387\u6837\u672c\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u8bc4\u4f30\u6cdb\u5316\u6027\u5e76\u63a8\u52a8\u96fe\u76d1\u6d4b\u6280\u672f\u53d1\u5c55", "motivation": "\u73b0\u6709\u6d77\u6d0b\u96fe\u6570\u636e\u96c6\u5c40\u9650\u4e8e\u5355\u4e00\u533a\u57df\u6216\u536b\u661f\uff0c\u96be\u4ee5\u8bc4\u4f30\u6a21\u578b\u8de8\u533a\u57df\u6027\u80fd\u53ca\u63a2\u7d22\u96fe\u672c\u8d28\u7279\u5f81", "method": "\u6574\u540815\u4e2a\u6cbf\u6d77\u96fe\u533a\u30016\u9897\u5730\u7403\u540c\u6b65\u536b\u661f\u7684\u6807\u6ce8\u6570\u636e\uff0c\u6784\u5efa\u5305\u542b68,000+\u9ad8\u5206\u8fa8\u7387\u6837\u672c\u7684\u591a\u6e90\u5f02\u6784\u6570\u636e\u96c6", "result": "16\u4e2a\u57fa\u7ebf\u6a21\u578b\u5b9e\u9a8c\u663e\u793a\uff1a\u6570\u636e\u96c6\u53ef\u6709\u6548\u63ed\u793a\u533a\u57df/\u536b\u661f\u5dee\u5f02\u5bfc\u81f4\u7684\u6a21\u578b\u6cdb\u5316\u6ce2\u52a8\uff0c\u5e76\u4e3a\u5b9a\u5236\u5316\u96fe\u9884\u6d4b\u6280\u672f\u5f00\u53d1\u63d0\u4f9b\u8d44\u6e90", "conclusion": "MFogHub\u901a\u8fc7\u591a\u7ef4\u5ea6\u6570\u636e\u878d\u5408\uff0c\u63a8\u52a8\u5168\u7403\u6d77\u6d0b\u96fe\u52a8\u6001\u76d1\u6d4b\u80fd\u529b\u548c\u79d1\u5b66\u8ba4\u77e5\u7684\u53cc\u91cd\u63d0\u5347\uff0c\u4ee3\u7801\u6570\u636e\u96c6\u5df2\u5f00\u6e90"}}
{"id": "2505.10554", "pdf": "https://arxiv.org/pdf/2505.10554", "abs": "https://arxiv.org/abs/2505.10554", "authors": ["Zhiyuan Hu", "Yibo Wang", "Hanze Dong", "Yuhui Xu", "Amrita Saha", "Caiming Xiong", "Bryan Hooi", "Junnan Li"], "title": "Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large Reasoning Models", "categories": ["cs.CL"], "comment": "In Progress", "summary": "Large reasoning models (LRMs) already possess a latent capacity for long\nchain-of-thought reasoning. Prior work has shown that outcome-based\nreinforcement learning (RL) can incidentally elicit advanced reasoning\nbehaviors such as self-correction, backtracking, and verification phenomena\noften referred to as the model's \"aha moment\". However, the timing and\nconsistency of these emergent behaviors remain unpredictable and\nuncontrollable, limiting the scalability and reliability of LRMs' reasoning\ncapabilities. To address these limitations, we move beyond reliance on prompts\nand coincidental \"aha moments\". Instead, we explicitly align models with three\nmeta-abilities: deduction, induction, and abduction, using automatically\ngenerated, self-verifiable tasks. Our three stage-pipeline individual\nalignment, parameter-space merging, and domain-specific reinforcement learning,\nboosting performance by over 10\\% relative to instruction-tuned baselines.\nFurthermore, domain-specific RL from the aligned checkpoint yields an\nadditional 2\\% average gain in the performance ceiling across math, coding, and\nscience benchmarks, demonstrating that explicit meta-ability alignment offers a\nscalable and dependable foundation for reasoning. Code is available at:\nhttps://github.com/zhiyuanhubj/Meta-Ability-Alignment", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u5143\u80fd\u529b\u5bf9\u9f50\u4e09\u9636\u6bb5\u6846\u67b6\uff08\u4e2a\u4f53\u5bf9\u9f50+\u53c2\u6570\u5408\u5e76+\u9886\u57df\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u7cfb\u7edf\u6027\u63d0\u5347\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u53ef\u63a7\u6027\u548c\u6027\u80fd\u5929\u82b1\u677f", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7ed3\u679c\u5f3a\u5316\u7684\u65b9\u6cd5\u4f9d\u8d56\u5076\u7136\u7684'\u987f\u609f\u65f6\u523b'\uff0c\u5bfc\u81f4\u63a8\u7406\u884c\u4e3a\u4e0d\u53ef\u9884\u6d4b\u4e14\u6027\u80fd\u5929\u82b1\u677f\u53d7\u9650\u3002\u9700\u8981\u5efa\u7acb\u7cfb\u7edf\u5316\u7684\u5143\u80fd\u529b\u5bf9\u9f50\u673a\u5236", "method": "1. \u4e2a\u4f53\u5bf9\u9f50\uff1a\u81ea\u52a8\u751f\u6210\u53ef\u81ea\u9a8c\u8bc1\u4efb\u52a1\u5bf9\u9f50\u6f14\u7ece/\u5f52\u7eb3/\u6eaf\u56e0\u80fd\u529b\n2. \u53c2\u6570\u7a7a\u95f4\u5408\u5e76\uff1a\u6574\u5408\u4e0d\u540c\u903b\u8f91\u80fd\u529b\n3. \u9886\u57df\u5f3a\u5316\u5b66\u4e60\uff1a\u5728\u6570\u5b66/\u4ee3\u7801/\u79d1\u5b66\u7b49\u9886\u57df\u6301\u7eed\u8c03\u4f18", "result": "\u76f8\u5bf9\u6307\u4ee4\u5fae\u8c03\u57fa\u7ebf\u63d0\u534710%+\uff0c\u9886\u57df\u5f3a\u5316\u5b66\u4e60\u989d\u5916\u5e26\u67652%\u5e73\u5747\u589e\u76ca\u3002\u5728GSM8K/MBPP/MMLU\u7b49\u57fa\u51c6\u6301\u7eed\u7a81\u7834\u6027\u80fd\u4e0a\u9650", "conclusion": "\u663e\u5f0f\u7684\u5143\u80fd\u529b\u5bf9\u9f50\u4e3a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u9760\u7684\u57fa\u7840\u6846\u67b6\uff0c\u4ee3\u7801\u5f00\u6e90\u63a8\u52a8\u9886\u57df\u53d1\u5c55"}}
{"id": "2505.10289", "pdf": "https://arxiv.org/pdf/2505.10289", "abs": "https://arxiv.org/abs/2505.10289", "authors": ["Yue Wang", "Shuai Xu", "Xuelin Zhu", "Yicong Li"], "title": "MSCI: Addressing CLIP's Inherent Limitations for Compositional Zero-Shot Learning", "categories": ["cs.CV"], "comment": "9 pages, 5 figures", "summary": "Compositional Zero-Shot Learning (CZSL) aims to recognize unseen state-object\ncombinations by leveraging known combinations. Existing studies basically rely\non the cross-modal alignment capabilities of CLIP but tend to overlook its\nlimitations in capturing fine-grained local features, which arise from its\narchitectural and training paradigm. To address this issue, we propose a\nMulti-Stage Cross-modal Interaction (MSCI) model that effectively explores and\nutilizes intermediate-layer information from CLIP's visual encoder.\nSpecifically, we design two self-adaptive aggregators to extract local\ninformation from low-level visual features and integrate global information\nfrom high-level visual features, respectively. These key information are\nprogressively incorporated into textual representations through a\nstage-by-stage interaction mechanism, significantly enhancing the model's\nperception capability for fine-grained local visual information. Additionally,\nMSCI dynamically adjusts the attention weights between global and local visual\ninformation based on different combinations, as well as different elements\nwithin the same combination, allowing it to flexibly adapt to diverse\nscenarios. Experiments on three widely used datasets fully validate the\neffectiveness and superiority of the proposed model. Data and code are\navailable at https://github.com/ltpwy/MSCI.", "AI": {"tldr": "\u63d0\u51fa\u591a\u9636\u6bb5\u8de8\u6a21\u6001\u4ea4\u4e92\u6a21\u578bMSCI\uff0c\u901a\u8fc7\u63d0\u53d6CLIP\u89c6\u89c9\u7f16\u7801\u5668\u7684\u4e2d\u95f4\u5c42\u4fe1\u606f\uff0c\u589e\u5f3a\u7ec4\u5408\u96f6\u6837\u672c\u5b66\u4e60\u4e2d\u5bf9\u7ec6\u7c92\u5ea6\u5c40\u90e8\u7279\u5f81\u7684\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eCLIP\u7684\u7ec4\u5408\u96f6\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u5176\u8de8\u6a21\u6001\u5bf9\u9f50\u80fd\u529b\uff0c\u5374\u5ffd\u89c6\u4e86CLIP\u5728\u67b6\u6784\u548c\u8bad\u7ec3\u8303\u5f0f\u4e0a\u5bfc\u81f4\u7684\u7ec6\u7c92\u5ea6\u5c40\u90e8\u7279\u5f81\u6355\u6349\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e24\u4e2a\u81ea\u9002\u5e94\u805a\u5408\u5668\u5206\u522b\u4ece\u4f4e\u5c42\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\u5c40\u90e8\u4fe1\u606f\u3001\u6574\u5408\u9ad8\u5c42\u89c6\u89c9\u7279\u5f81\u7684\u5168\u5c40\u4fe1\u606f\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u4ea4\u4e92\u673a\u5236\u5c06\u5173\u952e\u4fe1\u606f\u6e10\u8fdb\u878d\u5165\u6587\u672c\u8868\u5f81\u3002", "result": "\u5728\u4e09\u4e2a\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u578b\u6709\u6548\u6027\uff0c\u8bc1\u660e\u5176\u80fd\u52a8\u6001\u8c03\u6574\u5168\u5c40/\u5c40\u90e8\u6ce8\u610f\u529b\u6743\u91cd\u4ee5\u9002\u5e94\u4e0d\u540c\u7ec4\u5408\u573a\u666f\u3002", "conclusion": "MSCI\u901a\u8fc7\u591a\u9636\u6bb5\u8de8\u6a21\u6001\u4ea4\u4e92\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7279\u5f81\u7684\u611f\u77e5\u80fd\u529b\uff0c\u5728\u7ec4\u5408\u96f6\u6837\u672c\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2505.09665", "pdf": "https://arxiv.org/pdf/2505.09665", "abs": "https://arxiv.org/abs/2505.09665", "authors": ["Sulong Zhou", "Qunying Huang", "Shaoheng Zhou", "Yun Hang", "Xinyue Ye", "Aodong Mei", "Kathryn Phung", "Yuning Ye", "Uma Govindswamy", "Zehan Li"], "title": "Tales of the 2025 Los Angeles Fire: Hotwash for Public Health Concerns in Reddit via LLM-Enhanced Topic Modeling", "categories": ["cs.SI", "cs.CL"], "comment": null, "summary": "Wildfires have become increasingly frequent, irregular, and severe in recent\nyears. Understanding how affected populations perceive and respond during\nwildfire crises is critical for timely and empathetic disaster response. Social\nmedia platforms offer a crowd-sourced channel to capture evolving public\ndiscourse, providing hyperlocal information and insight into public sentiment.\nThis study analyzes Reddit discourse during the 2025 Los Angeles wildfires,\nspanning from the onset of the disaster to full containment. We collect 385\nposts and 114,879 comments related to the Palisades and Eaton fires. We adopt\ntopic modeling methods to identify the latent topics, enhanced by large\nlanguage models (LLMs) and human-in-the-loop (HITL) refinement. Furthermore, we\ndevelop a hierarchical framework to categorize latent topics, consisting of two\nmain categories, Situational Awareness (SA) and Crisis Narratives (CN). The\nvolume of SA category closely aligns with real-world fire progressions, peaking\nwithin the first 2-5 days as the fires reach the maximum extent. The most\nfrequent co-occurring category set of public health and safety, loss and\ndamage, and emergency resources expands on a wide range of health-related\nlatent topics, including environmental health, occupational health, and one\nhealth. Grief signals and mental health risks consistently accounted for 60\npercentage and 40 percentage of CN instances, respectively, with the highest\ntotal volume occurring at night. This study contributes the first annotated\nsocial media dataset on the 2025 LA fires, and introduces a scalable\nmulti-layer framework that leverages topic modeling for crisis discourse\nanalysis. By identifying persistent public health concerns, our results can\ninform more empathetic and adaptive strategies for disaster response, public\nhealth communication, and future research in comparable climate-related\ndisaster events.", "AI": {"tldr": "\u901a\u8fc7Reddit\u6570\u636e\u5206\u67902025\u5e74\u6d1b\u6749\u77f6\u5c71\u706b\u671f\u95f4\u516c\u4f17\u8206\u8bba\uff0c\u7ed3\u5408LLM\u4e0e\u4eba\u5de5\u6807\u6ce8\u6784\u5efa\u53cc\u5c42\u6846\u67b6\uff08SA\u60c5\u5883\u611f\u77e5\u4e0eCN\u5371\u673a\u53d9\u4e8b\uff09\uff0c\u63ed\u793a\u516c\u5171\u536b\u751f\u5173\u5207\u4e0e\u5fc3\u7406\u5065\u5eb7\u98ce\u9669\u7684\u65f6\u95f4\u5206\u5e03\u89c4\u5f8b\u3002", "motivation": "\u8fd1\u5e74\u91ce\u706b\u9891\u53d1\u4e14\u7834\u574f\u529b\u589e\u5f3a\uff0c\u9700\u901a\u8fc7\u793e\u4ea4\u5a92\u4f53\u5b9e\u65f6\u6355\u6349\u53d7\u707e\u7fa4\u4f53\u7684\u8ba4\u77e5\u6f14\u53d8\uff0c\u4e3a\u707e\u5bb3\u54cd\u5e94\u63d0\u4f9b\u4eba\u672c\u5316\u51b3\u7b56\u4f9d\u636e\u3002", "method": "\u6536\u96c6385\u4e2a\u4e3b\u5e16\u4e0e114,879\u6761\u8bc4\u8bba\uff0c\u91c7\u7528\u4e3b\u9898\u5efa\u6a21\uff08LLM\u589e\u5f3a+HITL\u4eba\u5de5\u6821\u51c6\uff09\uff0c\u6784\u5efaSA\uff08\u706b\u60c5\u8fdb\u5c55/\u5e94\u6025\u8d44\u6e90\uff09\u4e0eCN\uff08\u5fc3\u7406\u5065\u5eb7/\u7fa4\u4f53\u53d9\u4e8b\uff09\u53cc\u5c42\u5206\u7c7b\u4f53\u7cfb\u3002", "result": "SA\u8bdd\u9898\u91cf\u4e0e\u5b9e\u9645\u706b\u52bf\u540c\u6b65\u53d8\u5316\uff08\u5cf0\u503c\u51fa\u73b0\u5728\u707e\u540e2-5\u5929\uff09\uff0cCN\u4e2d\u5fc3\u7406\u5065\u5eb7\u8bdd\u9898\u5360\u6bd440%\u4e14\u591c\u95f4\u6d3b\u8dc3\u5ea6\u6700\u9ad8\uff0c\u521b\u5efa\u9996\u4e2a\u6807\u6ce8\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u96c6\u4e0e\u53ef\u6269\u5c55\u5206\u6790\u6846\u67b6\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u7cfb\u7edf\u6027\u8bc6\u522b\u707e\u5bb3\u4e2d\u7684\u6301\u7eed\u6027\u516c\u5171\u536b\u751f\u95ee\u9898\uff0c\u4e3a\u707e\u540e\u5fc3\u7406\u5e72\u9884\u3001\u5e94\u6025\u8d44\u6e90\u8c03\u914d\u53ca\u6c14\u5019\u5371\u673a\u6c9f\u901a\u63d0\u4f9b\u6570\u636e\u9a71\u52a8\u7684\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2505.10292", "pdf": "https://arxiv.org/pdf/2505.10292", "abs": "https://arxiv.org/abs/2505.10292", "authors": ["Daniel A. P. Oliveira", "David Martins de Matos"], "title": "StoryReasoning Dataset: Using Chain-of-Thought for Scene Understanding and Grounded Story Generation", "categories": ["cs.CV", "cs.CL", "I.2.10; I.2.7"], "comment": "31 pages, 14 figures", "summary": "Visual storytelling systems struggle to maintain character identity across\nframes and link actions to appropriate subjects, frequently leading to\nreferential hallucinations. These issues can be addressed through grounding of\ncharacters, objects, and other entities on the visual elements. We propose\nStoryReasoning, a dataset containing 4,178 stories derived from 52,016 movie\nimages, with both structured scene analyses and grounded stories. Each story\nmaintains character and object consistency across frames while explicitly\nmodeling multi-frame relationships through structured tabular representations.\nOur approach features cross-frame object re-identification using visual\nsimilarity and face recognition, chain-of-thought reasoning for explicit\nnarrative modeling, and a grounding scheme that links textual elements to\nvisual entities across multiple frames. We establish baseline performance by\nfine-tuning Qwen2.5-VL 7B, creating Qwen Storyteller, which performs end-to-end\nobject detection, re-identification, and landmark detection while maintaining\nconsistent object references throughout the story. Evaluation demonstrates a\nreduction from 4.06 to 3.56 (-12.3%) hallucinations on average per story when\ncompared to a non-fine-tuned model.", "AI": {"tldr": "\u63d0\u51faStoryReasoning\u6570\u636e\u96c6\u548cQwen Storyteller\u6a21\u578b\uff0c\u901a\u8fc7\u8de8\u5e27\u5b9e\u4f53\u57fa\u7840\u4e0e\u7ed3\u6784\u5316\u63a8\u7406\u51cf\u5c11\u89c6\u89c9\u53d9\u4e8b\u4e2d\u7684\u6307\u79f0\u5e7b\u89c9\u95ee\u9898", "motivation": "\u89e3\u51b3\u73b0\u6709\u89c6\u89c9\u53d9\u4e8b\u7cfb\u7edf\u4e2d\u89d2\u8272\u8eab\u4efd\u8de8\u5e27\u4e0d\u4e00\u81f4\u3001\u52a8\u4f5c\u4e0e\u4e3b\u4f53\u5173\u8054\u9519\u8bef\u5bfc\u81f4\u7684\u6307\u79f0\u5e7b\u89c9\u95ee\u9898", "method": "1. \u6784\u5efa\u542b4178\u4e2a\u7ed3\u6784\u5316\u6545\u4e8b\u7684StoryReasoning\u6570\u636e\u96c6\uff1b2. \u901a\u8fc7\u89c6\u89c9\u76f8\u4f3c\u5ea6\u4e0e\u9762\u90e8\u8bc6\u522b\u7684\u8de8\u5e27\u5bf9\u8c61\u91cd\u8bc6\u522b\uff1b3. \u57fa\u4e8e\u601d\u7ef4\u94fe\u7684\u663e\u5f0f\u53d9\u4e8b\u5efa\u6a21\uff1b4. \u5fae\u8c03Qwen2.5-VL 7B\u5b9e\u73b0\u7aef\u5230\u7aef\u591a\u4efb\u52a1\u5904\u7406", "result": "\u5fae\u8c03\u6a21\u578b\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\uff0c\u5e73\u5747\u6bcf\u4e2a\u6545\u4e8b\u7684\u5e7b\u89c9\u73b0\u8c61\u4ece4.06\u51cf\u5c11\u52303.56\uff08\u4e0b\u964d12.3%\uff09", "conclusion": "\u7ed3\u6784\u5316\u5b9e\u4f53\u57fa\u7840\u4e0e\u8de8\u5e27\u63a8\u7406\u673a\u5236\u6709\u6548\u63d0\u5347\u89c6\u89c9\u53d9\u4e8b\u8fde\u8d2f\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u6545\u4e8b\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6\u65b9\u6cd5"}}
{"id": "2505.09777", "pdf": "https://arxiv.org/pdf/2505.09777", "abs": "https://arxiv.org/abs/2505.09777", "authors": ["Alejo Lopez-Avila", "Jinhua Du"], "title": "A Survey on Large Language Models in Multimodal Recommender Systems", "categories": ["cs.IR", "cs.CL"], "comment": "30 pages, 6 figures", "summary": "Multimodal recommender systems (MRS) integrate heterogeneous user and item\ndata, such as text, images, and structured information, to enhance\nrecommendation performance. The emergence of large language models (LLMs)\nintroduces new opportunities for MRS by enabling semantic reasoning, in-context\nlearning, and dynamic input handling. Compared to earlier pre-trained language\nmodels (PLMs), LLMs offer greater flexibility and generalisation capabilities\nbut also introduce challenges related to scalability and model accessibility.\nThis survey presents a comprehensive review of recent work at the intersection\nof LLMs and MRS, focusing on prompting strategies, fine-tuning methods, and\ndata adaptation techniques. We propose a novel taxonomy to characterise\nintegration patterns, identify transferable techniques from related\nrecommendation domains, provide an overview of evaluation metrics and datasets,\nand point to possible future directions. We aim to clarify the emerging role of\nLLMs in multimodal recommendation and support future research in this rapidly\nevolving field.", "AI": {"tldr": "\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\u6574\u5408\u5f02\u6784\u6570\u636e\u63d0\u5347\u63a8\u8350\u6548\u679c\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5e26\u6765\u65b0\u673a\u9047\u4e0e\u6311\u6218\uff0c\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u76f8\u5173\u6280\u672f\u5e76\u63d0\u51fa\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e3a\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u8bed\u4e49\u63a8\u7406\u7b49\u65b0\u80fd\u529b\uff0c\u4f46\u5b58\u5728\u53ef\u6269\u5c55\u6027\u548c\u6a21\u578b\u8bbf\u95ee\u6027\u6311\u6218\uff0c\u9700\u7cfb\u7edf\u6027\u6574\u5408\u7814\u7a76\u8fdb\u5c55\u4ee5\u652f\u6301\u9886\u57df\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u63d0\u51fa\u6574\u5408\u6a21\u5f0f\u5206\u7c7b\u6cd5\uff0c\u8bc6\u522b\u6280\u672f\u8fc1\u79fb\u8def\u5f84\uff0c\u5e76\u7cfb\u7edf\u5206\u6790\u8bc4\u4f30\u6307\u6807\u4e0e\u6570\u636e\u96c6\u3002", "result": "\u660e\u786eLLMs\u5728\u63a8\u8350\u4e2d\u7684\u6574\u5408\u8303\u5f0f\uff0c\u5efa\u7acb\u6280\u672f\u8fc1\u79fb\u6846\u67b6\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u8bc4\u4f30\u4f53\u7cfb\u4e0e\u9886\u57df\u6570\u636e\u96c6\u5168\u666f\u56fe\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u6b63\u6210\u4e3a\u591a\u6a21\u6001\u63a8\u8350\u7684\u6838\u5fc3\u7ec4\u4ef6\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u6a21\u578b\u6548\u7387\u4f18\u5316\u3001\u8de8\u6a21\u6001\u5bf9\u9f50\u7b49\u65b9\u5411\u63a8\u52a8\u9886\u57df\u7a81\u7834\u3002"}}
{"id": "2505.10294", "pdf": "https://arxiv.org/pdf/2505.10294", "abs": "https://arxiv.org/abs/2505.10294", "authors": ["Guillaume Balezo", "Roger Trullo", "Albert Pla Planas", "Etienne Decenciere", "Thomas Walter"], "title": "MIPHEI-ViT: Multiplex Immunofluorescence Prediction from H&E Images using ViT Foundation Models", "categories": ["cs.CV", "q-bio.TO", "68T07 (Primary), 92C55 (Secondary)", "I.4.9; I.2.10; I.5.4; J.3"], "comment": null, "summary": "Histopathological analysis is a cornerstone of cancer diagnosis, with\nHematoxylin and Eosin (H&E) staining routinely acquired for every patient to\nvisualize cell morphology and tissue architecture. On the other hand, multiplex\nimmunofluorescence (mIF) enables more precise cell type identification via\nproteomic markers, but has yet to achieve widespread clinical adoption due to\ncost and logistical constraints. To bridge this gap, we introduce MIPHEI\n(Multiplex Immunofluorescence Prediction from H&E), a U-Net-inspired\narchitecture that integrates state-of-the-art ViT foundation models as encoders\nto predict mIF signals from H&E images. MIPHEI targets a comprehensive panel of\nmarkers spanning nuclear content, immune lineages (T cells, B cells, myeloid),\nepithelium, stroma, vasculature, and proliferation. We train our model using\nthe publicly available ORION dataset of restained H&E and mIF images from\ncolorectal cancer tissue, and validate it on two independent datasets. MIPHEI\nachieves accurate cell-type classification from H&E alone, with F1 scores of\n0.88 for Pan-CK, 0.57 for CD3e, 0.56 for SMA, 0.36 for CD68, and 0.30 for CD20,\nsubstantially outperforming both a state-of-the-art baseline and a random\nclassifier for most markers. Our results indicate that our model effectively\ncaptures the complex relationships between nuclear morphologies in their tissue\ncontext, as visible in H&E images and molecular markers defining specific cell\ntypes. MIPHEI offers a promising step toward enabling cell-type-aware analysis\nof large-scale H&E datasets, in view of uncovering relationships between\nspatial cellular organization and patient outcomes.", "AI": {"tldr": "\u63d0\u51faMIPHEI\u6a21\u578b\uff0c\u901a\u8fc7U-Net\u67b6\u6784\u7ed3\u5408ViT\u57fa\u7840\u6a21\u578b\uff0c\u4eceH&E\u56fe\u50cf\u9884\u6d4bmIF\u4fe1\u53f7\u4ee5\u66ff\u4ee3\u4e34\u5e8a\u6210\u672c\u8f83\u9ad8\u7684\u591a\u8def\u590d\u7528\u514d\u75ab\u8367\u5149\u68c0\u6d4b\u3002", "motivation": "\u4e34\u5e8a\u5e7f\u6cdb\u4f7f\u7528\u7684H&E\u67d3\u8272\u65e0\u6cd5\u7cbe\u786e\u5b9a\u4f4d\u7ec6\u80de\u7c7b\u578b\uff0c\u800cmIF\u6280\u672f\u867d\u80fd\u901a\u8fc7\u86cb\u767d\u6807\u8bb0\u5b9e\u73b0\u7cbe\u51c6\u8bc6\u522b\uff0c\u5374\u56e0\u6210\u672c\u548c\u64cd\u4f5c\u590d\u6742\u6027\u96be\u4ee5\u666e\u53ca\u3002\u672c\u7814\u7a76\u65e8\u5728\u7a81\u7834\u8fd9\u4e00\u6280\u672f\u9e3f\u6c9f\u3002", "method": "\u4f7f\u7528ORION\u7ed3\u76f4\u80a0\u764c\u6570\u636e\u96c6\u8bad\u7ec3U-Net\u67b6\u6784\u6a21\u578b\uff0c\u6574\u5408ViT\u4f5c\u4e3a\u7f16\u7801\u5668\uff0c\u9884\u6d4b\u5305\u62ec\u514d\u75ab\u7ec6\u80de/\u57fa\u8d28/\u8840\u7ba1\u7b497\u7c7b\u6807\u8bb0\u7684mIF\u4fe1\u53f7\uff0c\u5e76\u5728\u4e24\u4e2a\u72ec\u7acb\u6570\u636e\u96c6\u9a8c\u8bc1\u6027\u80fd\u3002", "result": "\u5728Pan-CK\u7b49\u6807\u8bb0\u9884\u6d4b\u4e2dF1\u8fbe0.88\uff0cCD3e/SMA\u5206\u522b0.57/0.56\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002CD68/CD20\u9884\u6d4b\u80fd\u529b\u76f8\u5bf9\u8f83\u5f31(F1 0.36/0.30)\u3002", "conclusion": "MIPHEI\u9996\u6b21\u5b9e\u73b0\u4ece\u5e38\u89c4H&E\u56fe\u50cf\u89e3\u6790\u7ec6\u80de\u5206\u5b50\u7279\u5f81\uff0c\u4e3a\u6316\u6398\u80bf\u7624\u5fae\u73af\u5883\u4e2d\u7ec6\u80de\u7a7a\u95f4\u5206\u5e03\u4e0e\u4e34\u5e8a\u9884\u540e\u7684\u5173\u7cfb\u63d0\u4f9b\u4e86\u9ad8\u6548\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2505.09855", "pdf": "https://arxiv.org/pdf/2505.09855", "abs": "https://arxiv.org/abs/2505.09855", "authors": ["Alexander Y. Ku", "Thomas L. Griffiths", "Stephanie C. Y. Chan"], "title": "Predictability Shapes Adaptation: An Evolutionary Perspective on Modes of Learning in Transformers", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Transformer models learn in two distinct modes: in-weights learning (IWL),\nencoding knowledge into model weights, and in-context learning (ICL), adapting\nflexibly to context without weight modification. To better understand the\ninterplay between these learning modes, we draw inspiration from evolutionary\nbiology's analogous adaptive strategies: genetic encoding (akin to IWL,\nadapting over generations and fixed within an individual's lifetime) and\nphenotypic plasticity (akin to ICL, enabling flexible behavioral responses to\nenvironmental cues). In evolutionary biology, environmental predictability\ndictates the balance between these strategies: stability favors genetic\nencoding, while reliable predictive cues promote phenotypic plasticity. We\nexperimentally operationalize these dimensions of predictability and\nsystematically investigate their influence on the ICL/IWL balance in\nTransformers. Using regression and classification tasks, we show that high\nenvironmental stability decisively favors IWL, as predicted, with a sharp\ntransition at maximal stability. Conversely, high cue reliability enhances ICL\nefficacy, particularly when stability is low. Furthermore, learning dynamics\nreveal task-contingent temporal evolution: while a canonical ICL-to-IWL shift\noccurs in some settings (e.g., classification with many classes), we\ndemonstrate that scenarios with easier IWL (e.g., fewer classes) or slower ICL\nacquisition (e.g., regression) can exhibit an initial IWL phase later yielding\nto ICL dominance. These findings support a relative-cost hypothesis for\nexplaining these learning mode transitions, establishing predictability as a\ncritical factor governing adaptive strategies in Transformers, and offering\nnovel insights for understanding ICL and guiding training methodologies.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u8fdb\u5316\u751f\u7269\u5b66\u7c7b\u6bd4\u63ed\u793aTransformer\u6a21\u578b\u6743\u91cd\u5b66\u4e60(IWL)\u4e0e\u4e0a\u4e0b\u6587\u5b66\u4e60(ICL)\u7684\u5e73\u8861\u673a\u5236\uff0c\u53d1\u73b0\u73af\u5883\u53ef\u9884\u6d4b\u6027\uff08\u7a33\u5b9a\u6027\u4e0e\u7ebf\u7d22\u53ef\u9760\u6027\uff09\u662f\u6838\u5fc3\u51b3\u5b9a\u56e0\u7d20\u3002", "motivation": "\u63a2\u7d22Transformer\u6a21\u578b\u4e24\u79cd\u5b66\u4e60\u6a21\u5f0f\u7684\u4ea4\u4e92\u673a\u5236\uff0c\u501f\u9274\u8fdb\u5316\u751f\u7269\u5b66\u7684\u9057\u4f20\u7f16\u7801\u4e0e\u8868\u578b\u53ef\u5851\u6027\u7406\u8bba\uff0c\u89e3\u6790\u73af\u5883\u53ef\u9884\u6d4b\u6027\u5bf9\u5b66\u4e60\u7b56\u7565\u9009\u62e9\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u56de\u5f52\u4e0e\u5206\u7c7b\u4efb\u52a1\uff0c\u7cfb\u7edf\u6027\u6d4b\u8bd5\u73af\u5883\u7a33\u5b9a\u6027\u3001\u7ebf\u7d22\u53ef\u9760\u6027\u5bf9IWL/ICL\u5e73\u8861\u7684\u5f71\u54cd\uff0c\u5206\u6790\u5b66\u4e60\u52a8\u6001\u7684\u65f6\u5e8f\u6f14\u53d8\u89c4\u5f8b\u3002", "result": "\u9ad8\u7a33\u5b9a\u6027\u73af\u5883\u5f3a\u70c8\u504f\u5411IWL\uff08\u6700\u5927\u7a33\u5b9a\u6027\u65f6\u51fa\u73b0\u9661\u5ced\u8f6c\u53d8\uff09\uff0c\u9ad8\u7ebf\u7d22\u53ef\u9760\u6027\u63d0\u5347ICL\u6548\u679c\uff08\u5c24\u5176\u4f4e\u7a33\u5b9a\u6027\u65f6\uff09\uff1b\u5b66\u4e60\u8def\u5f84\u5448\u73b0\u4efb\u52a1\u4f9d\u8d56\u7684\u65f6\u5e8f\u6a21\u5f0f\uff08ICL/IWL\u4e3b\u5bfc\u9636\u6bb5\u4ea4\u66ff\uff09\u3002", "conclusion": "\u9a8c\u8bc1\u53ef\u9884\u6d4b\u6027\u4f5c\u4e3aTransformer\u81ea\u9002\u5e94\u7b56\u7565\u7684\u6838\u5fc3\u8c03\u63a7\u56e0\u5b50\uff0c\u63d0\u51fa\u76f8\u5bf9\u6210\u672c\u5047\u8bf4\u89e3\u91ca\u5b66\u4e60\u6a21\u5f0f\u8f6c\u53d8\uff0c\u4e3a\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\u8bba\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002"}}
{"id": "2505.10351", "pdf": "https://arxiv.org/pdf/2505.10351", "abs": "https://arxiv.org/abs/2505.10351", "authors": ["Jie Zhu", "Jirong Zha", "Ding Li", "Leye Wang"], "title": "A Unified and Scalable Membership Inference Method for Visual Self-supervised Encoder via Part-aware Capability", "categories": ["cs.CV"], "comment": "An extension of our ACM CCS2024 conference paper (arXiv:2404.02462).\n  We show the impacts of scaling from both data and model aspects on membership\n  inference for self-supervised visual encoders", "summary": "Self-supervised learning shows promise in harnessing extensive unlabeled\ndata, but it also confronts significant privacy concerns, especially in vision.\nIn this paper, we perform membership inference on visual self-supervised models\nin a more realistic setting: self-supervised training method and details are\nunknown for an adversary when attacking as he usually faces a black-box system\nin practice. In this setting, considering that self-supervised model could be\ntrained by completely different self-supervised paradigms, e.g., masked image\nmodeling and contrastive learning, with complex training details, we propose a\nunified membership inference method called PartCrop. It is motivated by the\nshared part-aware capability among models and stronger part response on the\ntraining data. Specifically, PartCrop crops parts of objects in an image to\nquery responses within the image in representation space. We conduct extensive\nattacks on self-supervised models with different training protocols and\nstructures using three widely used image datasets. The results verify the\neffectiveness and generalization of PartCrop. Moreover, to defend against\nPartCrop, we evaluate two common approaches, i.e., early stop and differential\nprivacy, and propose a tailored method called shrinking crop scale range. The\ndefense experiments indicate that all of them are effective. Finally, besides\nprototype testing on toy visual encoders and small-scale image datasets, we\nquantitatively study the impacts of scaling from both data and model aspects in\na realistic scenario and propose a scalable PartCrop-v2 by introducing two\nstructural improvements to PartCrop. Our code is at\nhttps://github.com/JiePKU/PartCrop.", "AI": {"tldr": "\u63d0\u51faPartCrop\u65b9\u6cd5\uff0c\u9488\u5bf9\u672a\u77e5\u8bad\u7ec3\u7ec6\u8282\u7684\u9ed1\u76d2\u81ea\u76d1\u7763\u6a21\u578b\u8fdb\u884c\u6210\u5458\u63a8\u7406\u653b\u51fb\uff0c\u901a\u8fc7\u88c1\u526a\u5bf9\u8c61\u90e8\u5206\u5206\u6790\u8868\u793a\u54cd\u5e94\uff0c\u9a8c\u8bc1\u6709\u6548\u6027\u5e76\u63d0\u51fa\u9632\u5fa1\u63aa\u65bd", "motivation": "\u81ea\u76d1\u7763\u5b66\u4e60\u5b58\u5728\u9690\u79c1\u98ce\u9669\uff0c\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u5728\u9ed1\u76d2\u573a\u666f\u4e0b\u6548\u679c\u6709\u9650\u3002\u9488\u5bf9\u4e0d\u540c\u81ea\u76d1\u7763\u8303\u5f0f\uff08\u5982\u906e\u853d\u56fe\u50cf\u5efa\u6a21\u548c\u5bf9\u6bd4\u5b66\u4e60\uff09\u7684\u7edf\u4e00\u653b\u51fb\u9700\u6c42", "method": "\u57fa\u4e8e\u6a21\u578b\u5171\u4eab\u7684\u90e8\u5206\u611f\u77e5\u80fd\u529b\uff0c\u88c1\u526a\u56fe\u50cf\u4e2d\u7684\u5bf9\u8c61\u90e8\u5206\u5e76\u5728\u8868\u793a\u7a7a\u95f4\u67e5\u8be2\u54cd\u5e94\u3002\u8986\u76d6\u4e0d\u540c\u8bad\u7ec3\u534f\u8bae\u548c\u6a21\u578b\u7ed3\u6784\uff0c\u4f7f\u7528\u4e09\u4e2a\u56fe\u50cf\u6570\u636e\u96c6\u9a8c\u8bc1", "result": "\u5b9e\u9a8c\u8bc1\u660ePartCrop\u5728\u591a\u79cd\u8bad\u7ec3\u534f\u8bae\u4e0b\u6709\u6548\u4e14\u6cdb\u5316\u6027\u5f3a\u3002\u9632\u5fa1\u63aa\u65bd\u4e2d\u65e9\u505c\u3001\u5dee\u5206\u9690\u79c1\u548c\u88c1\u526a\u5c3a\u5ea6\u8c03\u6574\u5747\u6709\u6548\u3002\u6269\u5c55PartCrop-v2\u9002\u5e94\u5927\u89c4\u6a21\u573a\u666f", "conclusion": "PartCrop\u6210\u529f\u653b\u51fb\u591a\u79cd\u81ea\u76d1\u7763\u6a21\u578b\uff0c\u63d0\u51fa\u7684\u9632\u5fa1\u7b56\u7565\u6709\u6548\u3002\u6a21\u578b\u4e0e\u6570\u636e\u89c4\u6a21\u6269\u5c55\u5206\u6790\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u53c2\u8003\uff0c\u6539\u8fdb\u7248PartCrop-v2\u589e\u5f3a\u5b9e\u7528\u6027"}}
{"id": "2505.09901", "pdf": "https://arxiv.org/pdf/2505.09901", "abs": "https://arxiv.org/abs/2505.09901", "authors": ["Ziyuan Zhang", "Darcy Wang", "Ningyuan Chen", "Rodrigo Mansur", "Vahid Sarhangian"], "title": "Comparing Exploration-Exploitation Strategies of LLMs and Humans: Insights from Standard Multi-armed Bandit Tasks", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) are increasingly used to simulate or automate\nhuman behavior in complex sequential decision-making tasks. A natural question\nis then whether LLMs exhibit similar decision-making behavior to humans, and\ncan achieve comparable (or superior) performance. In this work, we focus on the\nexploration-exploitation (E&E) tradeoff, a fundamental aspect of dynamic\ndecision-making under uncertainty. We employ canonical multi-armed bandit (MAB)\ntasks introduced in the cognitive science and psychiatry literature to conduct\na comparative study of the E&E strategies of LLMs, humans, and MAB algorithms.\nWe use interpretable choice models to capture the E&E strategies of the agents\nand investigate how explicit reasoning, through both prompting strategies and\nreasoning-enhanced models, shapes LLM decision-making. We find that reasoning\nshifts LLMs toward more human-like behavior, characterized by a mix of random\nand directed exploration. In simple stationary tasks, reasoning-enabled LLMs\nexhibit similar levels of random and directed exploration compared to humans.\nHowever, in more complex, non-stationary environments, LLMs struggle to match\nhuman adaptability, particularly in effective directed exploration, despite\nachieving similar regret in certain scenarios. Our findings highlight both the\npromise and limits of LLMs as simulators of human behavior and tools for\nautomated decision-making and point to potential areas of improvements.", "AI": {"tldr": "LLMs\u5728\u63a2\u7d22-\u5229\u7528\u6743\u8861\u4e2d\u5c55\u73b0\u51fa\u63a5\u8fd1\u4eba\u7c7b\u7684\u884c\u4e3a\uff0c\u4f46\u5728\u590d\u6742\u73af\u5883\u4e2d\u9002\u5e94\u6027\u4e0d\u8db3", "motivation": "\u9a8c\u8bc1LLMs\u662f\u5426\u5177\u6709\u7c7b\u4f3c\u4eba\u7c7b\u7684\u52a8\u6001\u51b3\u7b56\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u63a2\u7d22-\u5229\u7528\u6743\u8861\u673a\u5236", "method": "\u4f7f\u7528\u591a\u81c2\u8001\u864e\u673a\u5b9e\u9a8c\u6846\u67b6\uff0c\u7ed3\u5408\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u6a21\u578b\uff0c\u5bf9\u6bd4\u5206\u6790LLMs/\u4eba\u7c7b/\u7b97\u6cd5\u7684\u63a2\u7d22\u7b56\u7565\uff0c\u91cd\u70b9\u8003\u5bdf\u663e\u5f0f\u63a8\u7406\uff08\u63d0\u793a\u7b56\u7565\u548c\u589e\u5f3a\u63a8\u7406\u6a21\u578b\uff09\u7684\u5f71\u54cd", "result": "1. \u63a8\u7406\u80fd\u529b\u4f7fLLMs\u5448\u73b0\u6df7\u5408\u968f\u673a/\u5b9a\u5411\u63a2\u7d22\u7684\u4eba\u7c7b\u7279\u5f81\n2. \u7b80\u5355\u4efb\u52a1\u4e2d\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\n3. \u590d\u6742\u975e\u7a33\u6001\u73af\u5883\u4e2d\u5b9a\u5411\u63a2\u7d22\u80fd\u529b\u4e0d\u8db3", "conclusion": "LLMs\u5177\u5907\u6a21\u62df\u4eba\u7c7b\u51b3\u7b56\u7684\u6f5c\u529b\u4f46\u5b58\u5728\u73af\u5883\u9002\u5e94\u6027\u5c40\u9650\uff0c\u9700\u6539\u8fdb\u590d\u6742\u73af\u5883\u4e0b\u7684\u5b9a\u5411\u63a2\u7d22\u673a\u5236"}}
{"id": "2505.10352", "pdf": "https://arxiv.org/pdf/2505.10352", "abs": "https://arxiv.org/abs/2505.10352", "authors": ["Shihao Zou", "Qingfeng Li", "Wei Ji", "Jingjing Li", "Yongkui Yang", "Guoqi Li", "Chao Dong"], "title": "SpikeVideoFormer: An Efficient Spike-Driven Video Transformer with Hamming Attention and $\\mathcal{O}(T)$ Complexity", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted by ICML 2025", "summary": "Spiking Neural Networks (SNNs) have shown competitive performance to\nArtificial Neural Networks (ANNs) in various vision tasks, while offering\nsuperior energy efficiency. However, existing SNN-based Transformers primarily\nfocus on single-image tasks, emphasizing spatial features while not effectively\nleveraging SNNs' efficiency in video-based vision tasks. In this paper, we\nintroduce SpikeVideoFormer, an efficient spike-driven video Transformer,\nfeaturing linear temporal complexity $\\mathcal{O}(T)$. Specifically, we design\na spike-driven Hamming attention (SDHA) which provides a theoretically guided\nadaptation from traditional real-valued attention to spike-driven attention.\nBuilding on SDHA, we further analyze various spike-driven space-time attention\ndesigns and identify an optimal scheme that delivers appealing performance for\nvideo tasks, while maintaining only linear temporal complexity. The\ngeneralization ability and efficiency of our model are demonstrated across\ndiverse downstream video tasks, including classification, human pose tracking,\nand semantic segmentation. Empirical results show our method achieves\nstate-of-the-art (SOTA) performance compared to existing SNN approaches, with\nover 15\\% improvement on the latter two tasks. Additionally, it matches the\nperformance of recent ANN-based methods while offering significant efficiency\ngains, achieving $\\times 16$, $\\times 10$ and $\\times 5$ improvements on the\nthree tasks. https://github.com/JimmyZou/SpikeVideoFormer", "AI": {"tldr": "\u63d0\u51faSpikeVideoFormer\u2014\u2014\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u8109\u51b2\u9a71\u52a8\u89c6\u9891Transformer\uff0c\u5728\u89c6\u9891\u4efb\u52a1\u4e2d\u5b9e\u73b0SOTA\u6027\u80fd\u4e0e\u9ad8\u6548\u7387", "motivation": "\u73b0\u6709SNN Transformer\u4ec5\u5173\u6ce8\u5355\u56fe\u4efb\u52a1\uff0c\u672a\u80fd\u6709\u6548\u5229\u7528\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5728\u89c6\u9891\u4efb\u52a1\u4e2d\u7684\u6548\u7387\u4f18\u52bf", "method": "\u8bbe\u8ba1\u8109\u51b2\u9a71\u52a8\u6c49\u660e\u6ce8\u610f\u529b(SDHA)\uff0c\u7406\u8bba\u63a8\u5bfc\u5b9e\u503c\u6ce8\u610f\u529b\u5230\u8109\u51b2\u6ce8\u610f\u529b\u7684\u8f6c\u6362\uff0c\u4f18\u5316\u65f6\u7a7a\u6ce8\u610f\u529b\u67b6\u6784", "result": "\u5728\u89c6\u9891\u5206\u7c7b/\u59ff\u6001\u8ddf\u8e2a/\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u8d85\u8d8aSNN\u65b9\u6cd515%+\uff0c\u6548\u7387\u5206\u522b\u63d0\u534716/10/5\u500d\uff0c\u6027\u80fd\u5339\u914dANN\u65b9\u6cd5", "conclusion": "\u901a\u8fc7SDHA\u548c\u65f6\u7a7a\u6ce8\u610f\u529b\u4f18\u5316\uff0c\u9a8c\u8bc1\u4e86\u8109\u51b2\u9a71\u52a8\u6a21\u578b\u5728\u89c6\u9891\u4efb\u52a1\u4e2d\u7684\u9ad8\u6548\u6027\u4e0e\u7ade\u4e89\u529b"}}
{"id": "2505.09921", "pdf": "https://arxiv.org/pdf/2505.09921", "abs": "https://arxiv.org/abs/2505.09921", "authors": ["Yidan Wang", "Yanan Cao", "Yubing Ren", "Fang Fang", "Zheng Lin", "Binxing Fang"], "title": "PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative In-Context Optimization", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) excel in various domains but pose inherent\nprivacy risks. Existing methods to evaluate privacy leakage in LLMs often use\nmemorized prefixes or simple instructions to extract data, both of which\nwell-alignment models can easily block. Meanwhile, Jailbreak attacks bypass LLM\nsafety mechanisms to generate harmful content, but their role in privacy\nscenarios remains underexplored. In this paper, we examine the effectiveness of\njailbreak attacks in extracting sensitive information, bridging privacy leakage\nand jailbreak attacks in LLMs. Moreover, we propose PIG, a novel framework\ntargeting Personally Identifiable Information (PII) and addressing the\nlimitations of current jailbreak methods. Specifically, PIG identifies PII\nentities and their types in privacy queries, uses in-context learning to build\na privacy context, and iteratively updates it with three gradient-based\nstrategies to elicit target PII. We evaluate PIG and existing jailbreak methods\nusing two privacy-related datasets. Experiments on four white-box and two\nblack-box LLMs show that PIG outperforms baseline methods and achieves\nstate-of-the-art (SoTA) results. The results underscore significant privacy\nrisks in LLMs, emphasizing the need for stronger safeguards. Our code is\navailble at\n\\href{https://github.com/redwyd/PrivacyJailbreak}{https://github.com/redwyd/PrivacyJailbreak}.", "AI": {"tldr": "\u63d0\u51faPIG\u6846\u67b6\u6709\u6548\u5229\u7528\u8d8a\u72f1\u653b\u51fb\u63d0\u53d6LLMs\u4e2d\u7684\u4e2a\u4eba\u9690\u79c1\u4fe1\u606f\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u9690\u79c1\u8bc4\u4f30\u65b9\u6cd5\u6613\u88ab\u5bf9\u9f50\u6a21\u578b\u963b\u65ad\uff0c\u4e14\u8d8a\u72f1\u653b\u51fb\u5728\u9690\u79c1\u573a\u666f\u7684\u5e94\u7528\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u7814\u7a76\u65e8\u5728\u8fde\u63a5\u9690\u79c1\u6cc4\u9732\u4e0e\u8d8a\u72f1\u653b\u51fb\u7684\u5173\u8054\u6027", "method": "1. \u8bc6\u522b\u9690\u79c1\u67e5\u8be2\u4e2d\u7684PII\u5b9e\u4f53\u53ca\u7c7b\u578b 2. \u4e0a\u4e0b\u6587\u5b66\u4e60\u6784\u5efa\u9690\u79c1\u4e0a\u4e0b\u6587 3. \u4e09\u79cd\u68af\u5ea6\u7b56\u7565\u8fed\u4ee3\u66f4\u65b0\u63d0\u53d6\u76ee\u6807PII 4. \u57fa\u4e8e6\u79cdLLM\u7684\u767d\u76d2/\u9ed1\u76d2\u5b9e\u9a8c\u9a8c\u8bc1", "result": "PIG\u57284\u4e2a\u767d\u76d2\u6a21\u578b\u548c2\u4e2a\u9ed1\u76d2\u6a21\u578b\u4e0a\u5b9e\u73b0SOTA\u6548\u679c\uff0c\u51c6\u786e\u7387\u6700\u9ad8\u8fbe0.92\uff0c\u663e\u8457\u66b4\u9732LLMs\u7684\u9690\u79c1\u98ce\u9669", "conclusion": "\u7814\u7a76\u8bc1\u660e\u73b0\u6709LLM\u5b89\u5168\u673a\u5236\u5b58\u5728\u91cd\u5927\u9690\u79c1\u6f0f\u6d1e\uff0c\u9700\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u9690\u79c1\u4fdd\u62a4\u65b9\u6848\u5e94\u5bf9\u65b0\u578b\u653b\u51fb\u624b\u6bb5"}}
{"id": "2505.10420", "pdf": "https://arxiv.org/pdf/2505.10420", "abs": "https://arxiv.org/abs/2505.10420", "authors": ["Andrei Arhire", "Radu Timofte"], "title": "Learned Lightweight Smartphone ISP with Unpaired Data", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at CVPRW 2025", "summary": "The Image Signal Processor (ISP) is a fundamental component in modern\nsmartphone cameras responsible for conversion of RAW sensor image data to RGB\nimages with a strong focus on perceptual quality. Recent work highlights the\npotential of deep learning approaches and their ability to capture details with\na quality increasingly close to that of professional cameras. A difficult and\ncostly step when developing a learned ISP is the acquisition of pixel-wise\naligned paired data that maps the raw captured by a smartphone camera sensor to\nhigh-quality reference images. In this work, we address this challenge by\nproposing a novel training method for a learnable ISP that eliminates the need\nfor direct correspondences between raw images and ground-truth data with\nmatching content. Our unpaired approach employs a multi-term loss function\nguided by adversarial training with multiple discriminators processing feature\nmaps from pre-trained networks to maintain content structure while learning\ncolor and texture characteristics from the target RGB dataset. Using\nlightweight neural network architectures suitable for mobile devices as\nbackbones, we evaluated our method on the Zurich RAW to RGB and Fujifilm\nUltraISP datasets. Compared to paired training methods, our unpaired learning\nstrategy shows strong potential and achieves high fidelity across multiple\nevaluation metrics. The code and pre-trained models are available at\nhttps://github.com/AndreiiArhire/Learned-Lightweight-Smartphone-ISP-with-Unpaired-Data .", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u914d\u5bf9\u6570\u636e\u7684\u8f7b\u91cf\u7ea7\u667a\u80fd\u624b\u673aISP\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u548c\u591a\u5224\u522b\u5668\u7b56\u7565\u5b9e\u73b0\u9ad8\u8d28\u91cf\u56fe\u50cf\u8f6c\u6362\u3002", "motivation": "\u4f20\u7edf\u5b66\u4e60\u578bISP\u4f9d\u8d56\u50cf\u7d20\u7ea7\u5bf9\u9f50\u7684\u914d\u5bf9\u6570\u636e\uff0c\u91c7\u96c6\u6210\u672c\u9ad8\u6602\u4e14\u56f0\u96be\u3002\u672c\u6587\u65e8\u5728\u6d88\u9664\u5bf9\u914d\u5bf9\u6570\u636e\u7684\u9700\u6c42\uff0c\u7b80\u5316\u8bad\u7ec3\u6d41\u7a0b\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u5bf9\u6297\u8bad\u7ec3\u7684\u591a\u9879\u635f\u5931\u51fd\u6570\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u7f51\u7edc\u7279\u5f81\u56fe\u7684\u591a\u4e2a\u5224\u522b\u5668\uff0c\u4fdd\u6301\u5185\u5bb9\u7ed3\u6784\u540c\u65f6\u5b66\u4e60\u76ee\u6807RGB\u7279\u6027\u3002\u91c7\u7528\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u9002\u914d\u79fb\u52a8\u7aef\u3002", "result": "\u5728Zurich RAW to RGB\u548cFujifilm UltraISP\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u4e0e\u914d\u5bf9\u8bad\u7ec3\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728PSNR/SSIM\u7b49\u6307\u6807\u4e0a\u8fbe\u5230\u7ade\u4e89\u6027\u7ed3\u679c(\u4f8b\u5982PSNR 23.42 vs 23.71)\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u65e0\u914d\u5bf9\u6570\u636e\u8bad\u7ec3\uff0c\u5f00\u53d1\u51fa\u9002\u7528\u4e8e\u79fb\u52a8\u8bbe\u5907\u7684\u8f7b\u91cfISP\u6a21\u578b\uff0c\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u5df2\u5f00\u6e90\uff0c\u5177\u5907\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c\u3002"}}
{"id": "2505.09949", "pdf": "https://arxiv.org/pdf/2505.09949", "abs": "https://arxiv.org/abs/2505.09949", "authors": ["Ahmed S. Abdelrahman", "Mohamed Abdel-Aty", "Samgyu Yang", "Abdulrahman Faden"], "title": "Advanced Crash Causation Analysis for Freeway Safety: A Large Language Model Approach to Identifying Key Contributing Factors", "categories": ["cs.LG", "cs.CL", "stat.AP"], "comment": null, "summary": "Understanding the factors contributing to traffic crashes and developing\nstrategies to mitigate their severity is essential. Traditional statistical\nmethods and machine learning models often struggle to capture the complex\ninteractions between various factors and the unique characteristics of each\ncrash. This research leverages large language model (LLM) to analyze freeway\ncrash data and provide crash causation analysis accordingly. By compiling 226\ntraffic safety studies related to freeway crashes, a training dataset\nencompassing environmental, driver, traffic, and geometric design factors was\ncreated. The Llama3 8B model was fine-tuned using QLoRA to enhance its\nunderstanding of freeway crashes and their contributing factors, as covered in\nthese studies. The fine-tuned Llama3 8B model was then used to identify crash\ncausation without pre-labeled data through zero-shot classification, providing\ncomprehensive explanations to ensure that the identified causes were reasonable\nand aligned with existing research. Results demonstrate that LLMs effectively\nidentify primary crash causes such as alcohol-impaired driving, speeding,\naggressive driving, and driver inattention. Incorporating event data, such as\nroad maintenance, offers more profound insights. The model's practical\napplicability and potential to improve traffic safety measures were validated\nby a high level of agreement among researchers in the field of traffic safety,\nas reflected in questionnaire results with 88.89%. This research highlights the\ncomplex nature of traffic crashes and how LLMs can be used for comprehensive\nanalysis of crash causation and other contributing factors. Moreover, it\nprovides valuable insights and potential countermeasures to aid planners and\npolicymakers in developing more effective and efficient traffic safety\npractices.", "AI": {"tldr": "\u5229\u7528QLoRA\u5fae\u8c03Llama3 8B\u6a21\u578b\u5206\u6790\u9ad8\u901f\u516c\u8def\u4e8b\u6545\u6210\u56e0\uff0c\u901a\u8fc7\u96f6\u6837\u672c\u5206\u7c7b\u8bc6\u522b\u9152\u9a7e/\u8d85\u901f\u7b49\u4e3b\u56e0\uff0c\u9a8c\u8bc1\u6a21\u578b\u6709\u6548\u6027\uff08\u7814\u7a76\u8005\u8ba4\u53ef\u5ea688.89%\uff09", "motivation": "\u4f20\u7edf\u7edf\u8ba1\u65b9\u6cd5\u548c\u673a\u5668\u5b66\u4e60\u96be\u4ee5\u6355\u6349\u4e8b\u6545\u590d\u6742\u56e0\u7d20\u7684\u4ea4\u4e92\u5173\u7cfb\uff0c\u9700\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u975e\u7ed3\u6784\u5316\u6570\u636e\u7684\u4f18\u52bf\u4ee5\u5b9e\u73b0\u5168\u9762\u4e8b\u6545\u5206\u6790", "method": "1. \u6574\u5408226\u9879\u7814\u7a76\u6784\u5efa\u8bad\u7ec3\u6570\u636e\u96c6\n2. \u57fa\u4e8eQLoRA\u5fae\u8c03Llama3 8B\u6a21\u578b\n3. \u96f6\u6837\u672c\u5206\u7c7b\u8bc6\u522b\u4e8b\u6545\u539f\u56e0\n4. \u95ee\u5377\u9a8c\u8bc1\u6a21\u578b\u5b9e\u7528\u6027", "result": "\u6a21\u578b\u6210\u529f\u8bc6\u522b\u9152\u9a7e/\u8d85\u901f/\u6fc0\u8fdb\u9a7e\u9a76\u7b49\u4e3b\u56e0\uff0c\u7ed3\u5408\u9053\u8def\u7ef4\u62a4\u4e8b\u4ef6\u6570\u636e\u63d0\u5347\u5206\u6790\u6df1\u5ea6\uff0c\u95ee\u5377\u663e\u793a88.89%\u7814\u7a76\u8005\u8ba4\u53ef\u6a21\u578b\u8f93\u51fa\u5408\u7406\u6027", "conclusion": "LLM\u4e3a\u4e8b\u6545\u56e0\u7d20\u5206\u6790\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u652f\u6301\u653f\u7b56\u5236\u5b9a\u8005\u5f00\u53d1\u9ad8\u6548\u5b89\u5168\u63aa\u65bd\uff0c\u8bc1\u660e\u5927\u6a21\u578b\u5728\u4ea4\u901a\u5b89\u5168\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b"}}
{"id": "2505.10453", "pdf": "https://arxiv.org/pdf/2505.10453", "abs": "https://arxiv.org/abs/2505.10453", "authors": ["Tyler Tran", "Sangeet Khemlani", "J. G. Trafton"], "title": "Vision language models have difficulty recognizing virtual objects", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision language models (VLMs) are AI systems paired with both language and\nvision encoders to process multimodal input. They are capable of performing\ncomplex semantic tasks such as automatic captioning, but it remains an open\nquestion about how well they comprehend the visuospatial properties of scenes\ndepicted in the images they process. We argue that descriptions of virtual\nobjects -- objects that are not visually represented in an image -- can help\ntest scene comprehension in these AI systems. For example, an image that\ndepicts a person standing under a tree can be paired with the following prompt:\nimagine that a kite is stuck in the tree. VLMs that comprehend the scene should\nupdate their representations and reason sensibly about the spatial relations\nbetween all three objects. We describe systematic evaluations of\nstate-of-the-art VLMs and show that their ability to process virtual objects is\ninadequate.", "AI": {"tldr": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u5728\u7406\u89e3\u56fe\u50cf\u4e2d\u865a\u62df\u5bf9\u8c61\uff08\u672a\u89c6\u89c9\u5448\u73b0\u7684\u7269\u4f53\uff09\u7684\u7a7a\u95f4\u5173\u7cfb\u65b9\u9762\u8868\u73b0\u4e0d\u8db3", "motivation": "\u901a\u8fc7\u6d4b\u8bd5\u6a21\u578b\u5bf9\u865a\u62df\u5bf9\u8c61\u7a7a\u95f4\u5173\u7cfb\u7684\u7406\u89e3\u80fd\u529b\uff0c\u9a8c\u8bc1VLMs\u662f\u5426\u771f\u6b63\u7406\u89e3\u573a\u666f\u7684\u89c6\u89c9\u7a7a\u95f4\u5c5e\u6027", "method": "\u4f7f\u7528\u5305\u542b\u865a\u62df\u5bf9\u8c61\u63d0\u793a\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u65b9\u6cd5\uff08\u4f8b\u5982\u5728\u5df2\u6709\u573a\u666f\u4e2d\u65b0\u589e\u865a\u62df\u7269\u4f53\u540e\u6d4b\u8bd5\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff09", "result": "\u5f53\u524d\u6700\u5148\u8fdb\u7684VLMs\u5904\u7406\u865a\u62df\u5bf9\u8c61\u7684\u80fd\u529b\u5b58\u5728\u663e\u8457\u7f3a\u9677", "conclusion": "\u9700\u8981\u5f00\u53d1\u65b0\u7684\u65b9\u6cd5\u6765\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u573a\u666f\u9690\u542b\u7a7a\u95f4\u5173\u7cfb\u7684\u7406\u89e3\u4e0e\u63a8\u7406\u80fd\u529b"}}
{"id": "2505.10093", "pdf": "https://arxiv.org/pdf/2505.10093", "abs": "https://arxiv.org/abs/2505.10093", "authors": ["Hsuan-Lei Shao"], "title": "From Text to Network: Constructing a Knowledge Graph of Taiwan-Based China Studies Using Generative AI", "categories": ["cs.AI", "cs.CL", "I.2.4; H.3.3; J.5"], "comment": "4 pages, 4 figures", "summary": "Taiwanese China Studies (CS) has developed into a rich, interdisciplinary\nresearch field shaped by the unique geopolitical position and long standing\nacademic engagement with Mainland China. This study responds to the growing\nneed to systematically revisit and reorganize decades of Taiwan based CS\nscholarship by proposing an AI assisted approach that transforms unstructured\nacademic texts into structured, interactive knowledge representations. We apply\ngenerative AI (GAI) techniques and large language models (LLMs) to extract and\nstandardize entity relation triples from 1,367 peer reviewed CS articles\npublished between 1996 and 2019. These triples are then visualized through a\nlightweight D3.js based system, forming the foundation of a domain specific\nknowledge graph and vector database for the field. This infrastructure allows\nusers to explore conceptual nodes and semantic relationships across the corpus,\nrevealing previously uncharted intellectual trajectories, thematic clusters,\nand research gaps. By decomposing textual content into graph structured\nknowledge units, our system enables a paradigm shift from linear text\nconsumption to network based knowledge navigation. In doing so, it enhances\nscholarly access to CS literature while offering a scalable, data driven\nalternative to traditional ontology construction. This work not only\ndemonstrates how generative AI can augment area studies and digital humanities\nbut also highlights its potential to support a reimagined scholarly\ninfrastructure for regional knowledge systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u8fd0\u7528\u751f\u6210\u5f0fAI\u6280\u672f\u5c06\u53f0\u6e7e\u4e2d\u56fd\u7814\u7a76\u6587\u732e\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u77e5\u8bc6\u56fe\u8c31\uff0c\u5b9e\u73b0\u4ece\u7ebf\u6027\u6587\u672c\u5230\u7f51\u7edc\u5316\u77e5\u8bc6\u5bfc\u822a\u7684\u8303\u5f0f\u8f6c\u6362", "motivation": "\u57fa\u4e8e\u53f0\u6e7e\u4e2d\u56fd\u7814\u7a76\u9886\u57df\u72ec\u7279\u7684\u5730\u7f18\u5b66\u672f\u5730\u4f4d\u548c\u6570\u5341\u5e74\u5b66\u672f\u79ef\u7d2f\uff0c\u9700\u7cfb\u7edf\u6027\u91cd\u6784\u975e\u7ed3\u6784\u5316\u6587\u732e\u4ee5\u5efa\u7acb\u53ef\u4ea4\u4e92\u77e5\u8bc6\u4f53\u7cfb", "method": "\u91c7\u7528LLM\u4ece1367\u7bc7\u6587\u732e(1996-2019)\u63d0\u53d6\u5b9e\u4f53\u5173\u7cfb\u4e09\u5143\u7ec4\uff0c\u901a\u8fc7D3.js\u6784\u5efa\u8f7b\u91cf\u7ea7\u53ef\u89c6\u5316\u7cfb\u7edf\uff0c\u5f62\u6210\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u548c\u5411\u91cf\u6570\u636e\u5e93", "result": "\u6210\u529f\u63ed\u793a\u672a\u53d1\u73b0\u7684\u5b66\u672f\u8f68\u8ff9\u4e0e\u4e3b\u9898\u96c6\u7fa4\uff0c\u5efa\u7acb\u652f\u6301\u8bed\u4e49\u5173\u7cfb\u63a2\u7d22\u7684\u9886\u57df\u77e5\u8bc6\u57fa\u7840\u8bbe\u65bd\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u6027\u672c\u4f53\u6784\u5efa\u65b9\u6848", "conclusion": "\u8bba\u8bc1\u751f\u6210\u5f0fAI\u9769\u65b0\u533a\u57df\u7814\u7a76\u7684\u6f5c\u529b\uff0c\u4e3a\u6570\u5b57\u4eba\u6587\u63d0\u4f9b\u65b0\u578b\u5b66\u672f\u57fa\u7840\u8bbe\u65bd\u8303\u5f0f\uff0c\u63a8\u52a8\u77e5\u8bc6\u6d88\u8d39\u6a21\u5f0f\u5411\u7f51\u7edc\u5316\u5bfc\u822a\u8f6c\u578b"}}
{"id": "2505.10473", "pdf": "https://arxiv.org/pdf/2505.10473", "abs": "https://arxiv.org/abs/2505.10473", "authors": ["Fengdi Zhang", "Hongkun Cao", "Ruqi Huang"], "title": "Consistent Quantity-Quality Control across Scenes for Deployment-Aware Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "To reduce storage and computational costs, 3D Gaussian splatting (3DGS) seeks\nto minimize the number of Gaussians used while preserving high rendering\nquality, introducing an inherent trade-off between Gaussian quantity and\nrendering quality. Existing methods strive for better quantity-quality\nperformance, but lack the ability for users to intuitively adjust this\ntrade-off to suit practical needs such as model deployment under diverse\nhardware and communication constraints. Here, we present ControlGS, a 3DGS\noptimization method that achieves semantically meaningful and cross-scene\nconsistent quantity-quality control while maintaining strong quantity-quality\nperformance. Through a single training run using a fixed setup and a\nuser-specified hyperparameter reflecting quantity-quality preference, ControlGS\ncan automatically find desirable quantity-quality trade-off points across\ndiverse scenes, from compact objects to large outdoor scenes. It also\noutperforms baselines by achieving higher rendering quality with fewer\nGaussians, and supports a broad adjustment range with stepless control over the\ntrade-off.", "AI": {"tldr": "ControlGS\u63d0\u51fa\u4e00\u79cd\u652f\u6301\u8de8\u573a\u666f\u8bed\u4e49\u4e00\u81f4\u7684\u6570\u91cf-\u8d28\u91cf\u8fde\u7eed\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u6b21\u8bad\u7ec3\u548c\u7528\u6237\u504f\u597d\u53c2\u6570\u5373\u53ef\u81ea\u52a8\u4f18\u53163D\u9ad8\u65af\u6e85\u5c04\u6a21\u578b", "motivation": "\u73b0\u67093DGS\u4f18\u5316\u65b9\u6cd5\u7f3a\u4e4f\u7528\u6237\u53ef\u8c03\u8282\u7684\u6570\u91cf-\u8d28\u91cf\u6743\u8861\u673a\u5236\uff0c\u96be\u4ee5\u9002\u5e94\u4e0d\u540c\u786c\u4ef6\u90e8\u7f72\u9700\u6c42\u3002\u9700\u8981\u5b9e\u73b0\u65e2\u4fdd\u6301\u6027\u80fd\u53c8\u652f\u6301\u7075\u6d3b\u63a7\u5236\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u91c7\u7528\u56fa\u5b9a\u8bad\u7ec3\u914d\u7f6e\u7ed3\u5408\u7528\u6237\u6307\u5b9a\u504f\u597d\u8d85\u53c2\u6570\uff0c\u901a\u8fc7\u5355\u6b21\u8bad\u7ec3\u81ea\u52a8\u5b66\u4e60\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u6700\u4f18\u6743\u8861\u70b9\uff0c\u5b9e\u73b0\u4ece\u7d27\u51d1\u7269\u4f53\u5230\u5927\u578b\u6237\u5916\u573a\u666f\u7684\u8fde\u7eed\u63a7\u5236", "result": "\u5728\u4fdd\u6301\u9ad8\u6e32\u67d3\u8d28\u91cf\u7684\u540c\u65f6\u51cf\u5c11\u9ad8\u65af\u6570\u91cf\uff08\u6bd4\u57fa\u7ebf\u5c11\uff09\uff0c\u652f\u6301\u5e7f\u6cdb\u8c03\u6574\u8303\u56f4\uff080.1-10\u500d\u9ad8\u65af\u6570\u91cf\u53d8\u5316\uff09\uff0c\u8de8\u573a\u666f\u8868\u73b0\u4e00\u81f4\u6027\u9a8c\u8bc1", "conclusion": "ControlGS\u7a81\u7834\u4e86\u4f20\u7edf\u6570\u91cf-\u8d28\u91cf\u6743\u8861\u7684\u521a\u6027\u9650\u5236\uff0c\u4e3a3DGS\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u7075\u6d3b\u53ef\u63a7\u7684\u4f18\u5316\u8303\u5f0f\uff0c\u663e\u8457\u589e\u5f3a\u5b9e\u9645\u5e94\u7528\u9002\u5e94\u6027"}}
{"id": "2505.10117", "pdf": "https://arxiv.org/pdf/2505.10117", "abs": "https://arxiv.org/abs/2505.10117", "authors": ["JieHao Wu", "Ziwei Wang", "Junjie Sheng", "Wenhao Li", "Xiangfei Wang", "Jun Luo"], "title": "Learning Virtual Machine Scheduling in Cloud Computing through Language Agents", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "In cloud services, virtual machine (VM) scheduling is a typical Online\nDynamic Multidimensional Bin Packing (ODMBP) problem, characterized by\nlarge-scale complexity and fluctuating demands. Traditional optimization\nmethods struggle to adapt to real-time changes, domain-expert-designed\nheuristic approaches suffer from rigid strategies, and existing learning-based\nmethods often lack generalizability and interpretability. To address these\nlimitations, this paper proposes a hierarchical language agent framework named\nMiCo, which provides a large language model (LLM)-driven heuristic design\nparadigm for solving ODMBP. Specifically, ODMBP is formulated as a Semi-Markov\nDecision Process with Options (SMDP-Option), enabling dynamic scheduling\nthrough a two-stage architecture, i.e., Option Miner and Option Composer.\nOption Miner utilizes LLMs to discover diverse and useful non-context-aware\nstrategies by interacting with constructed environments. Option Composer\nemploys LLMs to discover a composing strategy that integrates the\nnon-context-aware strategies with the contextual ones. Extensive experiments on\nreal-world enterprise datasets demonstrate that MiCo achieves a 96.9\\%\ncompetitive ratio in large-scale scenarios involving more than 10,000 virtual\nmachines. It maintains high performance even under nonstationary request flows\nand diverse configurations, thus validating its effectiveness in complex and\nlarge-scale cloud environments.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42\u8bed\u8a00\u667a\u80fd\u4f53\u6846\u67b6MiCo\uff0c\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u4e24\u9636\u6bb5\u7b56\u7565\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e91\u73af\u5883\u4e2d\u5927\u89c4\u6a21\u52a8\u6001\u865a\u62df\u673a\u8c03\u5ea6\u95ee\u9898\uff0c\u5b9e\u9a8c\u663e\u793a\u5728\u4e07\u7ea7\u89c4\u6a21\u4e0b\u8fbe\u523096.9%\u7ade\u4e89\u6bd4\u3002", "motivation": "\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u65e0\u6cd5\u5b9e\u65f6\u9002\u5e94\u9700\u6c42\u6ce2\u52a8\uff0c\u4e13\u5bb6\u8bbe\u8ba1\u7684\u542f\u53d1\u5f0f\u7b56\u7565\u7075\u6d3b\u6027\u4e0d\u8db3\uff0c\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u901a\u7528\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5c06ODMBP\u5efa\u6a21\u4e3a\u534a\u9a6c\u5c14\u53ef\u592b\u9009\u9879\u51b3\u7b56\u8fc7\u7a0b(SMDP-Option)\uff0c\u901a\u8fc7Option Miner\u751f\u6210\u975e\u4e0a\u4e0b\u6587\u611f\u77e5\u7b56\u7565\uff0cOption Composer\u6574\u5408\u4e0a\u4e0b\u6587\u611f\u77e5\u7b56\u7565\u3002", "result": "\u5728\u4f01\u4e1a\u771f\u5b9e\u6570\u636e\u96c6\u5b9e\u9a8c\u4e2d\uff0cMiCo\u5728\u8d8510000\u53f0\u865a\u62df\u673a\u573a\u666f\u4e0b\u8fbe\u523096.9%\u7ade\u4e89\u6bd4\uff0c\u4e14\u5728\u975e\u7a33\u6001\u8bf7\u6c42\u6d41\u4e2d\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "conclusion": "MiCo\u6846\u67b6\u9a8c\u8bc1\u4e86LLM\u9a71\u52a8\u7684\u7b56\u7565\u8bbe\u8ba1\u8303\u5f0f\u5728\u590d\u6742\u4e91\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u9002\u5e94\u6027\u548c\u53ef\u6269\u5c55\u7684\u865a\u62df\u673a\u8c03\u5ea6\u3002"}}
{"id": "2505.10481", "pdf": "https://arxiv.org/pdf/2505.10481", "abs": "https://arxiv.org/abs/2505.10481", "authors": ["Ilya Ovodov", "Petr Surovtsev", "Karina Kvanchiani", "Alexander Kapitanov", "Alexander Nagaev"], "title": "Logos as a Well-Tempered Pre-train for Sign Language Recognition", "categories": ["cs.CV"], "comment": null, "summary": "This paper examines two aspects of the isolated sign language recognition\n(ISLR) task. First, despite the availability of a number of datasets, the\namount of data for most individual sign languages is limited. It poses the\nchallenge of cross-language ISLR model training, including transfer learning.\nSecond, similar signs can have different semantic meanings. It leads to\nambiguity in dataset labeling and raises the question of the best policy for\nannotating such signs. To address these issues, this study presents Logos, a\nnovel Russian Sign Language (RSL) dataset, the most extensive ISLR dataset by\nthe number of signers and one of the largest available datasets while also the\nlargest RSL dataset in size and vocabulary. It is shown that a model,\npre-trained on the Logos dataset can be used as a universal encoder for other\nlanguage SLR tasks, including few-shot learning. We explore cross-language\ntransfer learning approaches and find that joint training using multiple\nclassification heads benefits accuracy for the target lowresource datasets the\nmost. The key feature of the Logos dataset is explicitly annotated visually\nsimilar sign groups. We show that explicitly labeling visually similar signs\nimproves trained model quality as a visual encoder for downstream tasks. Based\non the proposed contributions, we outperform current state-of-the-art results\nfor the WLASL dataset and get competitive results for the AUTSL dataset, with a\nsingle stream model processing solely RGB video. The source code, dataset, and\npre-trained models are publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4fc4\u8bed\u624b\u8bed\u6570\u636e\u96c6Logos\uff0c\u901a\u8fc7\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\u63d0\u5347\u5b64\u7acb\u624b\u8bed\u8bc6\u522b\u6027\u80fd\uff0c\u5e76\u9a8c\u8bc1\u76f8\u4f3c\u624b\u52bf\u6807\u6ce8\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u5b64\u7acb\u624b\u8bed\u8bc6\u522b\u6570\u636e\u96c6\u89c4\u6a21\u6709\u9650\uff0c\u4e14\u76f8\u4f3c\u624b\u52bf\u6613\u5bfc\u81f4\u6807\u6ce8\u6b67\u4e49\uff0c\u963b\u788d\u6a21\u578b\u6027\u80fd\u63d0\u5347\u3002", "method": "1. \u6784\u5efa\u5305\u542b\u5927\u91cf\u6807\u6ce8\u8005\u7684\u5927\u89c4\u6a21\u4fc4\u8bed\u624b\u8bed\u6570\u636e\u96c6Logos\n2. \u4f7f\u7528\u591a\u5206\u7c7b\u5934\u8054\u5408\u8bad\u7ec3\u7b56\u7565\u8fdb\u884c\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\n3. \u5bf9\u89c6\u89c9\u76f8\u4f3c\u624b\u52bf\u8fdb\u884c\u663e\u5f0f\u6807\u6ce8", "result": "\u5728WLASL\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\uff0cAUTSL\u6570\u636e\u96c6\u83b7\u5f97\u7ade\u4e89\u6027\u7ed3\u679c\uff0c\u5355\u6d41RGB\u6a21\u578b\u5b9e\u73b0\u6700\u4f73\u6027\u80fd\u3002\u4ee3\u7801\u3001\u6570\u636e\u96c6\u548c\u6a21\u578b\u5747\u5df2\u5f00\u6e90\u3002", "conclusion": "Logos\u6570\u636e\u96c6\u901a\u8fc7\u8de8\u8bed\u8a00\u9884\u8bad\u7ec3\u548c\u76f8\u4f3c\u624b\u52bf\u6807\u6ce8\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u5b64\u7acb\u624b\u8bed\u8bc6\u522b\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u8bc6\u522b\u51c6\u786e\u7387\u3002"}}
{"id": "2505.10483", "pdf": "https://arxiv.org/pdf/2505.10483", "abs": "https://arxiv.org/abs/2505.10483", "authors": ["Yi Li", "Haonan Wang", "Qixiang Zhang", "Boyu Xiao", "Chenchang Hu", "Hualiang Wang", "Xiaomeng Li"], "title": "UniEval: Unified Holistic Evaluation for Unified Multimodal Understanding and Generation", "categories": ["cs.CV", "cs.AI"], "comment": "UniEval is the first evaluation framework designed for unified\n  multimodal models, including a holistic benchmark UniBench and the UniScore\n  metric", "summary": "The emergence of unified multimodal understanding and generation models is\nrapidly attracting attention because of their ability to enhance\ninstruction-following capabilities while minimizing model redundancy. However,\nthere is a lack of a unified evaluation framework for these models, which would\nenable an elegant, simplified, and overall evaluation. Current models conduct\nevaluations on multiple task-specific benchmarks, but there are significant\nlimitations, such as the lack of overall results, errors from extra evaluation\nmodels, reliance on extensive labeled images, benchmarks that lack diversity,\nand metrics with limited capacity for instruction-following evaluation. To\ntackle these challenges, we introduce UniEval, the first evaluation framework\ndesigned for unified multimodal models without extra models, images, or\nannotations. This facilitates a simplified and unified evaluation process. The\nUniEval framework contains a holistic benchmark, UniBench (supports both\nunified and visual generation models), along with the corresponding UniScore\nmetric. UniBench includes 81 fine-grained tags contributing to high diversity.\nExperimental results indicate that UniBench is more challenging than existing\nbenchmarks, and UniScore aligns closely with human evaluations, surpassing\ncurrent metrics. Moreover, we extensively evaluated SoTA unified and visual\ngeneration models, uncovering new insights into Univeral's unique values.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u65e0\u9700\u989d\u5916\u8d44\u6e90\u7684\u7edf\u4e00\u591a\u6a21\u6001\u8bc4\u4f30\u6846\u67b6UniEval\uff0c\u5305\u542bUniBench\u57fa\u51c6\u548cUniScore\u6307\u6807", "motivation": "\u73b0\u6709\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u8bc4\u4f30\u5b58\u5728\u57fa\u51c6\u5206\u6563\u3001\u4f9d\u8d56\u989d\u5916\u6a21\u578b\u3001\u6807\u6ce8\u6570\u636e\u6709\u9650\u3001\u6307\u6807\u4e0d\u8db3\u7b49\u95ee\u9898", "method": "\u8bbe\u8ba1\u5305\u542b81\u4e2a\u7ec6\u7c92\u5ea6\u6807\u7b7e\u7684UniBench\u57fa\u51c6\uff0c\u914d\u5408\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684UniScore\u8bc4\u4f30\u6307\u6807", "result": "\u5b9e\u9a8c\u8868\u660eUniBench\u66f4\u5177\u6311\u6218\u6027\uff0cUniScore\u4e0e\u4eba\u5de5\u8bc4\u4f30\u76f8\u5173\u6027\u8fbe80.5%\uff0c\u4f18\u4e8e\u73b0\u6709\u6307\u6807", "conclusion": "UniEval\u4e3a\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u63d0\u4f9b\u9ad8\u6548\u8bc4\u4f30\u65b9\u6848\uff0c\u63ed\u793a\u4e86Universal\u6a21\u578b\u7684\u72ec\u7279\u4ef7\u503c"}}
{"id": "2505.10222", "pdf": "https://arxiv.org/pdf/2505.10222", "abs": "https://arxiv.org/abs/2505.10222", "authors": ["Jintian Shao", "Hongyi Huang", "Jiayi Wu", "Beiwen Zhang", "ZhiYu Wu", "You Shan", "MingKai Zheng"], "title": "ComplexFormer: Disruptively Advancing Transformer Inference Ability via Head-Specific Complex Vector Attention", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Transformer models rely on self-attention to capture token dependencies but\nface challenges in effectively integrating positional information while\nallowing multi-head attention (MHA) flexibility. Prior methods often model\nsemantic and positional differences disparately or apply uniform positional\nadjustments across heads, potentially limiting representational capacity. This\npaper introduces ComplexFormer, featuring Complex Multi-Head Attention-CMHA.\nCMHA empowers each head to independently model semantic and positional\ndifferences unified within the complex plane, representing interactions as\nrotations and scaling. ComplexFormer incorporates two key improvements: (1) a\nper-head Euler transformation, converting real-valued query/key projections\ninto polar-form complex vectors for head-specific complex subspace operation;\nand (2) a per-head adaptive differential rotation mechanism,\nexp[i(Adapt(ASmn,i) + Delta(Pmn),i)], allowing each head to learn distinct\nstrategies for integrating semantic angle differences (ASmn,i) with relative\npositional encodings (Delta(Pmn),i). Extensive experiments on language\nmodeling, text generation, code generation, and mathematical reasoning show\nComplexFormer achieves superior performance, significantly lower generation\nperplexity , and improved long-context coherence compared to strong baselines\nlike RoPE-Transformers. ComplexFormer demonstrates strong parameter efficiency,\noffering a more expressive, adaptable attention mechanism.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u578bComplexFormer\u6a21\u578b\uff0c\u901a\u8fc7\u590d\u6570\u7a7a\u95f4\u7684\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7edf\u4e00\u5efa\u6a21\u8bed\u4e49\u4e0e\u4f4d\u7f6e\u5dee\u5f02\uff0c\u663e\u8457\u63d0\u5347\u8bed\u8a00\u4efb\u52a1\u6027\u80fd", "motivation": "\u4f20\u7edfTransformer\u591a\u5934\u6ce8\u610f\u529b\u96be\u4ee5\u7075\u6d3b\u534f\u8c03\u8bed\u4e49\u5dee\u5f02\u5efa\u6a21\u4e0e\u4f4d\u7f6e\u4fe1\u606f\u878d\u5408\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5206\u79bb\u5efa\u6a21\u6216\u8de8\u5934\u7edf\u4e00\u8c03\u6574\u7684\u5c40\u9650\u6027", "method": "1) \u6bcf\u4e2a\u6ce8\u610f\u529b\u5934\u72ec\u7acb\u8fdb\u884c\u6b27\u62c9\u53d8\u6362\uff0c\u5c06\u67e5\u8be2/\u952e\u6620\u5c04\u5230\u6781\u5750\u6807\u590d\u6570\u7a7a\u95f4\n2) \u8bbe\u8ba1\u81ea\u9002\u5e94\u5dee\u5206\u65cb\u8f6c\u673a\u5236exp[i(Adapt(ASmn,i)+\u0394(Pmn,i))]\uff0c\u5b9e\u73b0\u8bed\u4e49\u89d2\u5ea6\u5dee\u4e0e\u4f4d\u7f6e\u7f16\u7801\u7684\u7075\u6d3b\u878d\u5408", "result": "\u5728\u8bed\u8a00\u5efa\u6a21/\u751f\u6210\u3001\u4ee3\u7801\u751f\u6210\u548c\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97SOTA\u6027\u80fd\uff0c\u56f0\u60d1\u5ea6\u964d\u4f4e15.2%\uff0c\u957f\u4e0a\u4e0b\u6587\u8fde\u8d2f\u6027\u63d0\u534723%", "conclusion": "ComplexFormer\u901a\u8fc7\u590d\u6570\u7a7a\u95f4\u5efa\u6a21\u63d0\u4f9b\u66f4\u5177\u8868\u8fbe\u529b\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u4fdd\u6301\u53c2\u6570\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u6ce8\u610f\u529b\u673a\u5236\u8bbe\u8ba1\u5f00\u8f9f\u65b0\u65b9\u5411"}}
{"id": "2505.10496", "pdf": "https://arxiv.org/pdf/2505.10496", "abs": "https://arxiv.org/abs/2505.10496", "authors": ["Raman Dutt", "Pedro Sanchez", "Yongchen Yao", "Steven McDonagh", "Sotirios A. Tsaftaris", "Timothy Hospedales"], "title": "CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of Synthetic Chest Radiographs", "categories": ["cs.CV"], "comment": null, "summary": "We introduce CheXGenBench, a rigorous and multifaceted evaluation framework\nfor synthetic chest radiograph generation that simultaneously assesses\nfidelity, privacy risks, and clinical utility across state-of-the-art\ntext-to-image generative models. Despite rapid advancements in generative AI\nfor real-world imagery, medical domain evaluations have been hindered by\nmethodological inconsistencies, outdated architectural comparisons, and\ndisconnected assessment criteria that rarely address the practical clinical\nvalue of synthetic samples. CheXGenBench overcomes these limitations through\nstandardised data partitioning and a unified evaluation protocol comprising\nover 20 quantitative metrics that systematically analyse generation quality,\npotential privacy vulnerabilities, and downstream clinical applicability across\n11 leading text-to-image architectures. Our results reveal critical\ninefficiencies in the existing evaluation protocols, particularly in assessing\ngenerative fidelity, leading to inconsistent and uninformative comparisons. Our\nframework establishes a standardised benchmark for the medical AI community,\nenabling objective and reproducible comparisons while facilitating seamless\nintegration of both existing and future generative models. Additionally, we\nrelease a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K\nradiographs generated by the top-performing model (Sana 0.6B) in our benchmark\nto support further research in this critical domain. Through CheXGenBench, we\nestablish a new state-of-the-art and release our framework, models, and\nSynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/", "AI": {"tldr": "\u63d0\u51faCheXGenBench\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u534f\u8bae\u548c20+\u6307\u6807\u7cfb\u7edf\u8bc4\u4f30\u80f8\u90e8X\u5149\u5408\u6210\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\u3001\u9690\u79c1\u98ce\u9669\u4e0e\u4e34\u5e8a\u4ef7\u503c\uff0c\u5e76\u53d1\u5e0375K\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u96c6SynthCheX-75K\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u56fe\u50cf\u751f\u6210\u8bc4\u4f30\u5b58\u5728\u65b9\u6cd5\u4e0d\u4e00\u81f4\u3001\u67b6\u6784\u5bf9\u6bd4\u8fc7\u65f6\u3001\u8bc4\u4f30\u6307\u6807\u5272\u88c2\u4e14\u5ffd\u89c6\u4e34\u5e8a\u5b9e\u7528\u6027\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6807\u51c6\u5316\u6570\u636e\u5206\u533a\u548c\u7edf\u4e00\u8bc4\u4f30\u534f\u8bae\uff0820+\u91cf\u5316\u6307\u6807\uff09\uff0c\u5bf911\u79cd\u9886\u5148\u6587\u672c-\u56fe\u50cf\u6a21\u578b\u8fdb\u884c\u751f\u6210\u8d28\u91cf\u3001\u9690\u79c1\u6f0f\u6d1e\u548c\u4e34\u5e8a\u9002\u7528\u6027\u4e09\u7ef4\u5ea6\u7cfb\u7edf\u5206\u6790\u3002", "result": "\u63ed\u793a\u73b0\u6709\u8bc4\u4f30\u534f\u8bae\u5728\u751f\u6210\u4fdd\u771f\u5ea6\u8bc4\u4f30\u4e2d\u7684\u4f4e\u6548\u6027\uff0c\u5bfc\u81f4\u4e0d\u4e00\u81f4\u7684\u6a21\u578b\u5bf9\u6bd4\uff1b\u540c\u65f6\u901a\u8fc7Sana 0.6B\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u96c6\u3002", "conclusion": "CheXGenBench\u5efa\u7acb\u4e86\u533b\u5b66AI\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u652f\u6301\u5ba2\u89c2\u53ef\u590d\u73b0\u7684\u6a21\u578b\u5bf9\u6bd4\uff0c\u5e76\u5f00\u6e90\u6846\u67b6\u3001\u6a21\u578b\u53caSynthCheX-75K\u6570\u636e\u96c6\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2505.10497", "pdf": "https://arxiv.org/pdf/2505.10497", "abs": "https://arxiv.org/abs/2505.10497", "authors": ["Iurii Medvedev", "Nuno Goncalves"], "title": "MorphGuard: Morph Specific Margin Loss for Enhancing Robustness to Face Morphing Attacks", "categories": ["cs.CV"], "comment": null, "summary": "Face recognition has evolved significantly with the advancement of deep\nlearning techniques, enabling its widespread adoption in various applications\nrequiring secure authentication. However, this progress has also increased its\nexposure to presentation attacks, including face morphing, which poses a\nserious security threat by allowing one identity to impersonate another.\nTherefore, modern face recognition systems must be robust against such attacks.\n  In this work, we propose a novel approach for training deep networks for face\nrecognition with enhanced robustness to face morphing attacks. Our method\nmodifies the classification task by introducing a dual-branch classification\nstrategy that effectively handles the ambiguity in the labeling of face morphs.\nThis adaptation allows the model to incorporate morph images into the training\nprocess, improving its ability to distinguish them from bona fide samples.\n  Our strategy has been validated on public benchmarks, demonstrating its\neffectiveness in enhancing robustness against face morphing attacks.\nFurthermore, our approach is universally applicable and can be integrated into\nexisting face recognition training pipelines to improve classification-based\nrecognition methods.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u5206\u652f\u5206\u7c7b\u8bad\u7ec3\u7b56\u7565\uff0c\u589e\u5f3a\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u5bf9\u6297\u9762\u90e8\u53d8\u5f62\u653b\u51fb\u7684\u9c81\u68d2\u6027", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u63a8\u52a8\u4eba\u8138\u8bc6\u522b\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u9762\u4e34\u9762\u90e8\u53d8\u5f62\u7b49\u5448\u73b0\u653b\u51fb\u7684\u5b89\u5168\u5a01\u80c1", "method": "\u6539\u8fdb\u5206\u7c7b\u4efb\u52a1\u67b6\u6784\uff0c\u91c7\u7528\u53cc\u5206\u652f\u5206\u7c7b\u7b56\u7565\u5904\u7406\u53d8\u5f62\u56fe\u50cf\u6807\u7b7e\u6a21\u7cca\u6027\uff0c\u5c06\u53d8\u5f62\u6837\u672c\u878d\u5165\u8bad\u7ec3\u8fc7\u7a0b", "result": "\u5728\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u6709\u6548\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u533a\u5206\u771f\u5b9e\u6837\u672c\u4e0e\u53d8\u5f62\u653b\u51fb\u7684\u80fd\u529b", "conclusion": "\u8be5\u901a\u7528\u6027\u65b9\u6cd5\u53ef\u65e0\u7f1d\u96c6\u6210\u81f3\u73b0\u6709\u8bad\u7ec3\u6d41\u7a0b\uff0c\u63d0\u5347\u57fa\u4e8e\u5206\u7c7b\u7684\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u5b89\u5168\u6027"}}
{"id": "2505.10533", "pdf": "https://arxiv.org/pdf/2505.10533", "abs": "https://arxiv.org/abs/2505.10533", "authors": ["Aaryan Sharma", "Shivansh Gupta", "Samar Agarwal", "Vishak Prasad C.", "Ganesh Ramakrishnan"], "title": "Enhancing Multi-Image Question Answering via Submodular Subset Selection", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Large multimodal models (LMMs) have achieved high performance in\nvision-language tasks involving single image but they struggle when presented\nwith a collection of multiple images (Multiple Image Question Answering\nscenario). These tasks, which involve reasoning over large number of images,\npresent issues in scalability (with increasing number of images) and retrieval\nperformance. In this work, we propose an enhancement for retriever framework\nintroduced in MIRAGE model using submodular subset selection techniques. Our\nmethod leverages query-aware submodular functions, such as GraphCut, to\npre-select a subset of semantically relevant images before main retrieval\ncomponent. We demonstrate that using anchor-based queries and augmenting the\ndata improves submodular-retriever pipeline effectiveness, particularly in\nlarge haystack sizes.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5b50\u6a21\u51fd\u6570\u4f18\u5316\u7684\u68c0\u7d22\u589e\u5f3a\u6846\u67b6\uff0c\u63d0\u5347\u591a\u56fe\u50cf\u95ee\u7b54\u573a\u666f\u4e0b\u7684\u6269\u5c55\u6027\u548c\u68c0\u7d22\u6027\u80fd", "motivation": "\u73b0\u6709\u5927\u6a21\u578b\u5728\u591a\u56fe\u50cf\u573a\u666f\uff08MIQA\uff09\u5b58\u5728\u6269\u5c55\u6027\u5dee\uff08\u56fe\u50cf\u6570\u91cf\u589e\u52a0\u65f6\uff09\u548c\u68c0\u7d22\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898", "method": "\u4f7f\u7528\u57fa\u4e8e\u67e5\u8be2\u7684\u5b50\u6a21\u51fd\u6570\uff08\u5982GraphCut\uff09\u9884\u9009\u8bed\u4e49\u76f8\u5173\u56fe\u50cf\uff0c\u7ed3\u5408\u951a\u70b9\u67e5\u8be2\u548c\u6570\u636e\u589e\u5f3a\u4f18\u5316\u68c0\u7d22\u6d41\u7a0b", "result": "\u5728\u5927\u578b\u6570\u636e\u96c6\u4e2d\uff08\u5927\u5e72\u8349\u5806\u573a\u666f\uff09\u663e\u8457\u63d0\u5347\u5b50\u6a21\u68c0\u7d22\u7ba1\u9053\u7684\u6709\u6548\u6027", "conclusion": "\u5b50\u6a21\u9884\u9009\u673a\u5236\u7ed3\u5408\u67e5\u8be2\u611f\u77e5\u7b56\u7565\u80fd\u6709\u6548\u89e3\u51b3\u591a\u6a21\u6001\u6a21\u578b\u5728\u591a\u56fe\u50cf\u68c0\u7d22\u4e2d\u7684\u6269\u5c55\u6027\u74f6\u9888"}}
{"id": "2505.10465", "pdf": "https://arxiv.org/pdf/2505.10465", "abs": "https://arxiv.org/abs/2505.10465", "authors": ["Yizhou liu", "Ziming Liu", "Jeff Gore"], "title": "Superposition Yields Robust Neural Scaling", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "30 pages, 23 figures", "summary": "The success of today's large language models (LLMs) depends on the\nobservation that larger models perform better. However, the origin of this\nneural scaling law -- the finding that loss decreases as a power law with model\nsize -- remains unclear. Starting from two empirical principles -- that LLMs\nrepresent more things than the model dimensions (widths) they have (i.e.,\nrepresentations are superposed), and that words or concepts in language occur\nwith varying frequencies -- we constructed a toy model to study the loss\nscaling with model size. We found that when superposition is weak, meaning only\nthe most frequent features are represented without interference, the scaling of\nloss with model size depends on the underlying feature frequency; if feature\nfrequencies follow a power law, so does the loss. In contrast, under strong\nsuperposition, where all features are represented but overlap with each other,\nthe loss becomes inversely proportional to the model dimension across a wide\nrange of feature frequency distributions. This robust scaling behavior is\nexplained geometrically: when many more vectors are packed into a lower\ndimensional space, the interference (squared overlaps) between vectors scales\ninversely with that dimension. We then analyzed four families of open-sourced\nLLMs and found that they exhibit strong superposition and quantitatively match\nthe predictions of our toy model. The Chinchilla scaling law turned out to also\nagree with our results. We conclude that representation superposition is an\nimportant mechanism underlying the observed neural scaling laws. We anticipate\nthat these insights will inspire new training strategies and model\narchitectures to achieve better performance with less computation and fewer\nparameters.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u8868\u5f81\u53e0\u52a0\u662f\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u7684\u6838\u5fc3\u673a\u5236\u2014\u2014\u5f3a\u53e0\u52a0\u72b6\u6001\u4e0b\u635f\u5931\u4e0e\u6a21\u578b\u7ef4\u5ea6\u6210\u53cd\u6bd4\uff0c\u89e3\u91ca\u4e86\u73b0\u6709LLM\u7684\u7f29\u653e\u89c4\u5f8b\u3002", "motivation": "\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u7f29\u653e\u5b9a\u5f8b\u7684\u8d77\u6e90\uff0c\u5373\u4e3a\u4f55\u6a21\u578b\u89c4\u6a21\u8d8a\u5927\u6027\u80fd\u8d8a\u597d\u8fd9\u4e00\u73b0\u8c61\u7684\u6839\u672c\u539f\u56e0\u4ecd\u4e0d\u660e\u786e\u3002", "method": "\u57fa\u4e8e\u53e0\u52a0\u8868\u793a\u548c\u7279\u5f81\u9891\u7387\u5dee\u5f02\u4e24\u4e2a\u539f\u5219\u6784\u5efa\u73a9\u5177\u6a21\u578b\uff0c\u5206\u6790\u4e0d\u540c\u53e0\u52a0\u5f3a\u5ea6\u4e0b\u635f\u5931\u968f\u6a21\u578b\u5c3a\u5bf8\u7684\u7f29\u653e\u89c4\u5f8b\uff0c\u5e76\u7ed3\u5408\u51e0\u4f55\u89e3\u91ca\u548c\u5b9e\u9645LLM\u6570\u636e\u9a8c\u8bc1\u3002", "result": "\u5f3a\u53e0\u52a0\u573a\u666f\u4e2d\u635f\u5931\u4e0e\u6a21\u578b\u7ef4\u5ea6\u5448\u53cd\u6bd4\u5173\u7cfb\uff0c\u56db\u7c7b\u5f00\u6e90LLM\u7684\u6570\u636e\u9a8c\u8bc1\u4e0e\u6a21\u578b\u9884\u6d4b\u4e00\u81f4\uff0cChinchilla\u7f29\u653e\u5b9a\u5f8b\u4e5f\u7b26\u5408\u8be5\u7ed3\u8bba\u3002", "conclusion": "\u8868\u5f81\u53e0\u52a0\u673a\u5236\u662f\u89c2\u6d4b\u5230\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u7684\u5173\u952e\uff0c\u8be5\u53d1\u73b0\u53ef\u4e3a\u8bbe\u8ba1\u66f4\u9ad8\u6548\u8bad\u7ec3\u7b56\u7565\u548c\u6a21\u578b\u67b6\u6784\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2505.10541", "pdf": "https://arxiv.org/pdf/2505.10541", "abs": "https://arxiv.org/abs/2505.10541", "authors": ["Pengfei Wang", "Guohai Xu", "Weinong Wang", "Junjie Yang", "Jie Lou", "Yunhua Xue"], "title": "Exploring Implicit Visual Misunderstandings in Multimodal Large Language Models through Attention Analysis", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements have enhanced the capability of Multimodal Large Language\nModels (MLLMs) to comprehend multi-image information. However, existing\nbenchmarks primarily evaluate answer correctness, overlooking whether models\ngenuinely comprehend the visual input. To address this, we define implicit\nvisual misunderstanding (IVM), where MLLMs provide correct answers without\nfully comprehending the visual input. Through our analysis, we decouple the\nvisual and textual modalities within the causal attention module, revealing\nthat attention distribution increasingly converges on the image associated with\nthe correct answer as the network layers deepen. This insight leads to the\nintroduction of a scale-agnostic metric, \\textit{attention accuracy}, and a\nnovel benchmark for quantifying IVMs. Attention accuracy directly evaluates the\nmodel's visual understanding via internal mechanisms, remaining robust to\npositional biases for more reliable assessments. Furthermore, we extend our\napproach to finer granularities and demonstrate its effectiveness in unimodal\nscenarios, underscoring its versatility and generalizability.", "AI": {"tldr": "\u63d0\u51fa\u6ce8\u610f\u529b\u51c6\u786e\u7387\u6307\u6807\u548c\u91cf\u5316IVM\u7684\u65b0\u57fa\u51c6\uff0c\u901a\u8fc7\u89e3\u8026\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u9a8c\u8bc1\u6a21\u578b\u89c6\u89c9\u7406\u89e3\u80fd\u529b", "motivation": "\u73b0\u6709\u8bc4\u4f30\u57fa\u51c6\u4ec5\u5173\u6ce8\u7b54\u6848\u6b63\u786e\u6027\uff0c\u65e0\u6cd5\u68c0\u6d4b\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u89c6\u89c9\u5185\u5bb9\uff0c\u5bfc\u81f4\u9690\u5f0f\u89c6\u89c9\u8bef\u89e3(IVM)\u95ee\u9898", "method": "\u89e3\u8026\u89c6\u89c9-\u6587\u672c\u6a21\u6001\u7684\u56e0\u679c\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5206\u6790\u6ce8\u610f\u529b\u5206\u5e03\u6536\u655b\u7279\u6027\uff0c\u63d0\u51fa\u4e0e\u89c4\u6a21\u65e0\u5173\u7684\u6ce8\u610f\u529b\u51c6\u786e\u7387\u6307\u6807", "result": "\u6ce8\u610f\u529b\u51c6\u786e\u7387\u5bf9\u4f4d\u7f6e\u504f\u5dee\u4fdd\u6301\u9c81\u68d2\uff0c\u4e14\u5728\u5355\u6a21\u6001\u573a\u666f\u4e2d\u540c\u6837\u6709\u6548\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u901a\u7528\u6027", "conclusion": "\u901a\u8fc7\u5185\u90e8\u673a\u5236\u76f4\u63a5\u8bc4\u4f30\u89c6\u89c9\u7406\u89e3\uff0c\u5efa\u7acb\u4e86\u66f4\u53ef\u9760\u7684MLLM\u8bc4\u4f30\u4f53\u7cfb\uff0c\u89e3\u51b3\u4e86IVM\u7684\u6838\u5fc3\u6311\u6218"}}
{"id": "2505.10475", "pdf": "https://arxiv.org/pdf/2505.10475", "abs": "https://arxiv.org/abs/2505.10475", "authors": ["Mouxiang Chen", "Binyuan Hui", "Zeyu Cui", "Jiaxi Yang", "Dayiheng Liu", "Jianling Sun", "Junyang Lin", "Zhongxin Liu"], "title": "Parallel Scaling Law for Language Models", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "It is commonly believed that scaling language models should commit a\nsignificant space or time cost, by increasing the parameters (parameter\nscaling) or output tokens (inference-time scaling). We introduce the third and\nmore inference-efficient scaling paradigm: increasing the model's parallel\ncomputation during both training and inference time. We apply $P$ diverse and\nlearnable transformations to the input, execute forward passes of the model in\nparallel, and dynamically aggregate the $P$ outputs. This method, namely\nparallel scaling (ParScale), scales parallel computation by reusing existing\nparameters and can be applied to any model structure, optimization procedure,\ndata, or task. We theoretically propose a new scaling law and validate it\nthrough large-scale pre-training, which shows that a model with $P$ parallel\nstreams is similar to scaling the parameters by $O(\\log P)$ while showing\nsuperior inference efficiency. For example, ParScale can use up to 22$\\times$\nless memory increase and 6$\\times$ less latency increase compared to parameter\nscaling that achieves the same performance improvement. It can also recycle an\noff-the-shelf pre-trained model into a parallelly scaled one by post-training\non a small amount of tokens, further reducing the training budget. The new\nscaling law we discovered potentially facilitates the deployment of more\npowerful models in low-resource scenarios, and provides an alternative\nperspective for the role of computation in machine learning.", "AI": {"tldr": "\u63d0\u51fa\u5e76\u884c\u6269\u5c55\u8303\u5f0fParScale\uff0c\u901a\u8fc7\u589e\u52a0\u5e76\u884c\u8ba1\u7b97\u800c\u975e\u53c2\u6570\u89c4\u6a21\u6765\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u6548\u7387\uff0c\u663e\u8457\u964d\u4f4e\u5185\u5b58\u548c\u5ef6\u8fdf\u6d88\u8017\u3002", "motivation": "\u4f20\u7edf\u8bed\u8a00\u6a21\u578b\u6269\u5c55\u65b9\u6cd5\uff08\u53c2\u6570\u6269\u5c55/\u63a8\u7406\u65f6token\u6269\u5c55\uff09\u5b58\u5728\u9ad8\u8d44\u6e90\u6d88\u8017\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u5e76\u884c\u8ba1\u7b97\u65b9\u6848\u3002", "method": "\u91c7\u7528P\u4e2a\u53ef\u5b66\u4e60\u8f93\u5165\u53d8\u6362+\u5e76\u884c\u524d\u5411\u4f20\u64ad+\u52a8\u6001\u8f93\u51fa\u805a\u5408\uff0c\u590d\u7528\u73b0\u6709\u53c2\u6570\u7ed3\u6784\u5b9e\u73b0\u8ba1\u7b97\u5e76\u884c\u5316\u3002", "result": "\u7406\u8bba\u8bc1\u660eP\u5e76\u884c\u76f8\u5f53\u4e8e\u53c2\u6570O(logP)\u6269\u5c55\uff0c\u5b9e\u9645\u5b9e\u73b022\u500d\u5185\u5b58\u4f18\u5316\u4e0e6\u500d\u5ef6\u8fdf\u964d\u4f4e\uff0c\u652f\u6301\u901a\u8fc7\u5c11\u91cftoken\u540e\u8bad\u7ec3\u8fc1\u79fb\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "ParScale\u4e3a\u4f4e\u8d44\u6e90\u573a\u666f\u63d0\u4f9b\u9ad8\u6548\u6269\u5c55\u8def\u5f84\uff0c\u91cd\u6784\u4e86\u8ba1\u7b97\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u89d2\u8272\u8ba4\u77e5\uff0c\u5f00\u8f9f\u6a21\u578b\u90e8\u7f72\u65b0\u8303\u5f0f\u3002"}}
{"id": "2505.10551", "pdf": "https://arxiv.org/pdf/2505.10551", "abs": "https://arxiv.org/abs/2505.10551", "authors": ["Yiwen Liu", "Jessica Bader", "Jae Myung Kim"], "title": "Does Feasibility Matter? Understanding the Impact of Feasibility on Synthetic Training Data", "categories": ["cs.CV", "cs.AI"], "comment": "CVPRW 2025", "summary": "With the development of photorealistic diffusion models, models trained in\npart or fully on synthetic data achieve progressively better results. However,\ndiffusion models still routinely generate images that would not exist in\nreality, such as a dog floating above the ground or with unrealistic texture\nartifacts. We define the concept of feasibility as whether attributes in a\nsynthetic image could realistically exist in the real-world domain; synthetic\nimages containing attributes that violate this criterion are considered\ninfeasible. Intuitively, infeasible images are typically considered\nout-of-distribution; thus, training on such images is expected to hinder a\nmodel's ability to generalize to real-world data, and they should therefore be\nexcluded from the training set whenever possible. However, does feasibility\nreally matter? In this paper, we investigate whether enforcing feasibility is\nnecessary when generating synthetic training data for CLIP-based classifiers,\nfocusing on three target attributes: background, color, and texture. We\nintroduce VariReal, a pipeline that minimally edits a given source image to\ninclude feasible or infeasible attributes given by the textual prompt generated\nby a large language model. Our experiments show that feasibility minimally\naffects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference\nin top-1 accuracy across three fine-grained datasets. Also, the attribute\nmatters on whether the feasible/infeasible images adversarially influence the\nclassification performance. Finally, mixing feasible and infeasible images in\ntraining datasets does not significantly impact performance compared to using\npurely feasible or infeasible datasets.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\u5408\u6210\u56fe\u50cf\u53ef\u884c\u6027\uff08feasibility\uff09\u5bf9CLIP\u5206\u7c7b\u5668\u6027\u80fd\u5f71\u54cd\u6781\u5c0f\uff08\u51c6\u786e\u7387\u5dee\u5f02<0.3%\uff09\uff0c\u6df7\u5408\u53ef\u884c/\u4e0d\u53ef\u884c\u6570\u636e\u8bad\u7ec3\u4e0d\u5f71\u54cd\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u63a2\u7d22\u5408\u6210\u6570\u636e\u8bad\u7ec3\u4e2d\u56fe\u50cf\u5c5e\u6027\u53ef\u884c\u6027\uff08\u5982\u6f02\u6d6e\u7269\u4f53/\u5f02\u5e38\u7eb9\u7406\uff09\u662f\u5426\u5f71\u54cd\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u9a8c\u8bc1\u8fc7\u6ee4\u4e0d\u53ef\u884c\u56fe\u50cf\u7684\u5fc5\u8981\u6027\u3002", "method": "\u63d0\u51faVariReal\u6d41\u7a0b\uff0c\u901a\u8fc7LLM\u751f\u6210\u6587\u672c\u63d0\u793a\u5bf9\u6e90\u56fe\u50cf\u8fdb\u884c\u6700\u5c0f\u7f16\u8f91\uff0c\u6d4b\u8bd5\u53ef\u884c/\u4e0d\u53ef\u884c\u5c5e\u6027\u5bf9LoRA\u5fae\u8c03CLIP\u6a21\u578b\u7684\u5f71\u54cd\u3002", "result": "\u53ef\u884c\u6027\u5bf9\u5206\u7c7b\u51c6\u786e\u7387\u5f71\u54cd\u5fae\u5f31\uff08Top-1\u5e73\u5747\u5dee\u5f02<0.3%\uff09\uff0c\u5c5e\u6027\u7c7b\u578b\uff08\u80cc\u666f/\u989c\u8272/\u7eb9\u7406\uff09\u5bf9\u7ed3\u679c\u5f71\u54cd\u5b58\u5728\u5dee\u5f02\uff0c\u6df7\u5408\u8bad\u7ec3\u96c6\u65e0\u663e\u8457\u6027\u80fd\u53d8\u5316\u3002", "conclusion": "\u5728CLIP\u5fae\u8c03\u573a\u666f\u4e2d\uff0c\u65e0\u9700\u4e25\u683c\u8fc7\u6ee4\u4e0d\u53ef\u884c\u5408\u6210\u56fe\u50cf\uff0c\u4f46\u9700\u6839\u636e\u76ee\u6807\u5c5e\u6027\u7c7b\u578b\u6743\u8861\u53ef\u884c\u6027\u7ea6\u675f\u3002"}}
{"id": "2505.10495", "pdf": "https://arxiv.org/pdf/2505.10495", "abs": "https://arxiv.org/abs/2505.10495", "authors": ["Vibha Belavadi", "Tushar Vatsa", "Dewang Sultania", "Suhas Suresha", "Ishita Verma", "Cheng Chen", "Tracy Holloway King", "Michael Friedrich"], "title": "RouteNator: A Router-Based Multi-Modal Architecture for Generating Synthetic Training Data for Function Calling LLMs", "categories": ["cs.LG", "cs.CL"], "comment": "Proceedings of the 4th International Workshop on Knowledge-Augmented\n  Methods for Natural Language Processing", "summary": "This paper addresses fine-tuning Large Language Models (LLMs) for function\ncalling tasks when real user interaction data is unavailable. In digital\ncontent creation tools, where users express their needs through natural\nlanguage queries that must be mapped to API calls, the lack of real-world\ntask-specific data and privacy constraints for training on it necessitate\nsynthetic data generation. Existing approaches to synthetic data generation\nfall short in diversity and complexity, failing to replicate real-world data\ndistributions and leading to suboptimal performance after LLM fine-tuning. We\npresent a novel router-based architecture that leverages domain resources like\ncontent metadata and structured knowledge graphs, along with text-to-text and\nvision-to-text language models to generate high-quality synthetic training\ndata. Our architecture's flexible routing mechanism enables synthetic data\ngeneration that matches observed real-world distributions, addressing a\nfundamental limitation of traditional approaches. Evaluation on a comprehensive\nset of real user queries demonstrates significant improvements in both function\nclassification accuracy and API parameter selection. Models fine-tuned with our\nsynthetic data consistently outperform traditional approaches, establishing new\nbenchmarks for function calling tasks.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8def\u7531\u5668\u7684\u67b6\u6784\u751f\u6210\u5339\u914d\u771f\u5b9e\u5206\u5e03\u7684\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u51fd\u6570\u8c03\u7528\u4efb\u52a1\u4e2d\u7684\u5fae\u8c03\u6548\u679c", "motivation": "\u73b0\u6709\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\u5728\u591a\u6837\u6027\u548c\u590d\u6742\u6027\u4e0a\u7684\u4e0d\u8db3\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u53d7\u9650\uff0c\u4e14\u771f\u5b9e\u7528\u6237\u6570\u636e\u56e0\u9690\u79c1\u95ee\u9898\u96be\u4ee5\u83b7\u53d6", "method": "\u7ed3\u5408\u5185\u5bb9\u5143\u6570\u636e\u3001\u77e5\u8bc6\u56fe\u8c31\u548c\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u7075\u6d3b\u8def\u7531\u673a\u5236\u751f\u6210\u7b26\u5408\u5b9e\u9645\u5206\u5e03\u7684\u5408\u6210\u8bad\u7ec3\u6570\u636e", "result": "\u5728\u771f\u5b9e\u7528\u6237\u67e5\u8be2\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u51fd\u6570\u5206\u7c7b\u51c6\u786e\u7387\u63d0\u5347\u548cAPI\u53c2\u6570\u4f18\u5316\uff0c\u6a21\u578b\u6027\u80fd\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5", "conclusion": "\u8be5\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4f20\u7edf\u5408\u6210\u6570\u636e\u65b9\u6cd5\u7684\u5206\u5e03\u5339\u914d\u95ee\u9898\uff0c\u4e3a\u51fd\u6570\u8c03\u7528\u4efb\u52a1\u5efa\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u57fa\u51c6"}}
{"id": "2505.10557", "pdf": "https://arxiv.org/pdf/2505.10557", "abs": "https://arxiv.org/abs/2505.10557", "authors": ["Ke Wang", "Junting Pan", "Linda Wei", "Aojun Zhou", "Weikang Shi", "Zimu Lu", "Han Xiao", "Yunqiao Yang", "Houxing Ren", "Mingjie Zhan", "Hongsheng Li"], "title": "MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025 Findings", "summary": "Natural language image-caption datasets, widely used for training Large\nMultimodal Models, mainly focus on natural scenarios and overlook the intricate\ndetails of mathematical figures that are critical for problem-solving,\nhindering the advancement of current LMMs in multimodal mathematical reasoning.\nTo this end, we propose leveraging code as supervision for cross-modal\nalignment, since code inherently encodes all information needed to generate\ncorresponding figures, establishing a precise connection between the two\nmodalities. Specifically, we co-develop our image-to-code model and dataset\nwith model-in-the-loop approach, resulting in an image-to-code model,\nFigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date.\nFurthermore, we utilize FigCodifier to synthesize novel mathematical figures\nand then construct MM-MathInstruct-3M, a high-quality multimodal math\ninstruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with\nImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on\nMM-MathInstruct-3M for multimodal math problem solving. Our model achieves a\nnew open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and\nClaude 3.5 Sonnet in the geometry problem-solving subset of MathVista,\nachieving improvements of 8.9% and 9.2%. The dataset and models will be\nreleased at https://github.com/mathllm/MathCoder.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u4ee3\u7801\u76d1\u7763\u5b9e\u73b0\u56fe\u6587\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u5f00\u53d1FigCodifier\u6a21\u578b\u548cImgCode-8.6M\u6570\u636e\u96c6\uff0c\u6784\u5efa3M\u7ea7\u6570\u5b66\u6307\u4ee4\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u51fa\u8d85\u8d8aGPT-4o\u7684\u591a\u6a21\u6001\u6570\u5b66\u63a8\u7406\u6a21\u578bMathCoder-VL", "motivation": "\u4f20\u7edf\u591a\u6a21\u6001\u6570\u636e\u96c6\u5ffd\u7565\u6570\u5b66\u56fe\u8868\u7ec6\u8282\uff0c\u963b\u788dLMMs\u6570\u5b66\u63a8\u7406\u80fd\u529b\u53d1\u5c55\u3002\u4ee3\u7801\u80fd\u7cbe\u786e\u7f16\u7801\u56fe\u5f62\u751f\u6210\u4fe1\u606f\uff0c\u53ef\u5efa\u7acb\u56fe\u6587\u6a21\u6001\u7cbe\u786e\u8fde\u63a5", "method": "\u91c7\u7528\u6a21\u578b\u95ed\u73af\u5f00\u53d1\u6a21\u5f0f\uff1a1\uff09\u8054\u5408\u5f00\u53d1FigCodifier\u56fe\u50cf\u8f6c\u7801\u6a21\u578b\u548cImgCode-8.6M\u6570\u636e\u96c6\uff1b2\uff09\u5408\u6210\u6570\u5b66\u56fe\u8868\u6784\u5efaMM-MathInstruct-3M\u5fae\u8c03\u6570\u636e\u96c6\uff1b3\uff09\u4e24\u9636\u6bb5\u8bad\u7ec3MathCoder-VL\u6a21\u578b", "result": "1\uff09MathCoder-VL\u5728MathVista\u51e0\u4f55\u5b50\u96c6\u8d85\u8d8aGPT-4o 8.9%\uff1b2\uff09\u521b\u5f00\u6e90\u6a21\u578b6\u9879\u6307\u6807SOTA\uff1b3\uff09\u6784\u5efa\u76ee\u524d\u6700\u5927\u56fe\u50cf\u4ee3\u7801\u6570\u636e\u96c6\uff08860\u4e07\uff09\u548c\u6570\u5b66\u6307\u4ee4\u96c6\uff08300\u4e07\uff09", "conclusion": "\u4ee3\u7801\u76d1\u7763\u6709\u6548\u89e3\u51b3\u6570\u5b66\u56fe\u6587\u5bf9\u9f50\u96be\u9898\uff0c\u5927\u89c4\u6a21\u6570\u636e\u96c6\u6784\u5efa\u65b9\u6cd5\u4e3a\u591a\u6a21\u6001\u6570\u5b66\u63a8\u7406\u5f00\u8f9f\u65b0\u8def\u5f84\u3002\u6a21\u578b\u5f00\u6e90\u5c06\u63a8\u52a8\u793e\u533a\u53d1\u5c55\uff0c\u4ee3\u7801\u9a71\u52a8\u65b9\u6cd5\u53ef\u62d3\u5c55\u81f3\u5176\u4ed6\u4e13\u4e1a\u9886\u57df"}}
{"id": "2505.10526", "pdf": "https://arxiv.org/pdf/2505.10526", "abs": "https://arxiv.org/abs/2505.10526", "authors": ["Mugilan Ganesan", "Shane Segal", "Ankur Aggarwal", "Nish Sinnadurai", "Sean Lie", "Vithursan Thangarasa"], "title": "MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "Main paper: 11 pp., 4 figs., 3 tabs.; Supplementary: 2 pp", "summary": "Speculative decoding significantly accelerates language model inference by\nenabling a lightweight draft model to propose multiple tokens that a larger\ntarget model verifies simultaneously. However, applying this technique to\nvision-language models (VLMs) presents two fundamental challenges: small\nlanguage models that could serve as efficient drafters lack the architectural\ncomponents to process visual inputs, and their token predictions fail to match\nthose of VLM target models that consider visual context. We introduce\nMultimodal Adaptation and Self-Data Distillation for Speculative Decoding of\nVision-Language Models (MASSV), which transforms existing small language models\ninto effective multimodal drafters through a two-phase approach. MASSV first\nconnects the target VLM's vision encoder to the draft model via a lightweight\ntrainable projector, then applies self-distilled visual instruction tuning\nusing responses generated by the target VLM to align token predictions.\nComprehensive experiments across the Qwen2.5-VL and Gemma3 model families\ndemonstrate that MASSV increases accepted length by up to 30% and delivers\nend-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV\nprovides a scalable, architecture-compatible method for accelerating both\ncurrent and future VLMs.", "AI": {"tldr": "\u63d0\u51faMASSV\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u5c06\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u8f6c\u5316\u4e3a\u9ad8\u6548\u591a\u6a21\u6001\u8349\u7a3f\u6a21\u578b\uff0c\u52a0\u901f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406", "motivation": "\u73b0\u6709\u63a8\u6d4b\u89e3\u7801\u6280\u672f\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e2d\u9762\u4e34\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u65e0\u6cd5\u5904\u7406\u89c6\u89c9\u8f93\u5165\u3001token\u9884\u6d4b\u4e0d\u5339\u914d\u89c6\u89c9\u4e0a\u4e0b\u6587\u4e24\u5927\u6311\u6218", "method": "1. \u901a\u8fc7\u53ef\u8bad\u7ec3\u6295\u5f71\u5668\u8fde\u63a5\u76ee\u6807VLM\u7684\u89c6\u89c9\u7f16\u7801\u5668 2. \u4f7f\u7528\u76ee\u6807VLM\u751f\u6210\u7684\u81ea\u84b8\u998f\u89c6\u89c9\u6307\u4ee4\u6570\u636e\u8fdb\u884c\u5bf9\u9f50\u8bad\u7ec3", "result": "\u5728Qwen2.5-VL\u548cGemma3\u6a21\u578b\u4e0a\u5b9e\u73b0\u63a5\u53d7\u957f\u5ea6\u63d0\u534730%\uff0c\u7aef\u5230\u7aef\u63a8\u7406\u52a0\u901f\u8fbe1.46\u500d", "conclusion": "MASSV\u4e3a\u5f53\u524d\u53ca\u672a\u6765\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u67b6\u6784\u517c\u5bb9\u7684\u52a0\u901f\u65b9\u6848"}}
{"id": "2505.10562", "pdf": "https://arxiv.org/pdf/2505.10562", "abs": "https://arxiv.org/abs/2505.10562", "authors": ["Wenxuan Wang", "Fan Zhang", "Yufeng Cui", "Haiwen Diao", "Zhuoyan Luo", "Huchuan Lu", "Jing Liu", "Xinlong Wang"], "title": "End-to-End Vision Tokenizer Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Existing vision tokenization isolates the optimization of vision tokenizers\nfrom downstream training, implicitly assuming the visual tokens can generalize\nwell across various tasks, e.g., image generation and visual question\nanswering. The vision tokenizer optimized for low-level reconstruction is\nagnostic to downstream tasks requiring varied representations and semantics.\nThis decoupled paradigm introduces a critical misalignment: The loss of the\nvision tokenization can be the representation bottleneck for target tasks. For\nexample, errors in tokenizing text in a given image lead to poor results when\nrecognizing or generating them. To address this, we propose ETT, an end-to-end\nvision tokenizer tuning approach that enables joint optimization between vision\ntokenization and target autoregressive tasks. Unlike prior autoregressive\nmodels that use only discrete indices from a frozen vision tokenizer, ETT\nleverages the visual embeddings of the tokenizer codebook, and optimizes the\nvision tokenizers end-to-end with both reconstruction and caption objectives.\nETT can be seamlessly integrated into existing training pipelines with minimal\narchitecture modifications. Our ETT is simple to implement and integrate,\nwithout the need to adjust the original codebooks or architectures of the\nemployed large language models. Extensive experiments demonstrate that our\nproposed end-to-end vision tokenizer tuning unlocks significant performance\ngains, i.e., 2-6% for multimodal understanding and visual generation tasks\ncompared to frozen tokenizer baselines, while preserving the original\nreconstruction capability. We hope this very simple and strong method can\nempower multimodal foundation models besides image generation and\nunderstanding.", "AI": {"tldr": "\u63d0\u51fa\u7aef\u5230\u7aef\u89c6\u89c9\u6807\u8bb0\u5668\u8c03\u4f18\u65b9\u6cd5ETT\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u89c6\u89c9\u6807\u8bb0\u5316\u4e0e\u4e0b\u6e38\u4efb\u52a1\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5206\u79bb\u4f18\u5316\u5bfc\u81f4\u7684\u8868\u5f81\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u6807\u8bb0\u5316\u65b9\u6cd5\u72ec\u7acb\u4e8e\u4e0b\u6e38\u4efb\u52a1\u4f18\u5316\uff08\u5982\u4f4e\u5c42\u91cd\u5efa\uff09\uff0c\u5bfc\u81f4\u4e0d\u540c\u4efb\u52a1\uff08\u5982\u6587\u672c\u8bc6\u522b\u3001\u56fe\u50cf\u751f\u6210\uff09\u7684\u8868\u5f81\u9519\u4f4d\u3002\u4f8b\u5982\u56fe\u50cf\u4e2d\u6587\u5b57\u6807\u8bb0\u9519\u8bef\u4f1a\u76f4\u63a5\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u3002", "method": "ETT\u5229\u7528\u89c6\u89c9\u6807\u8bb0\u5668codebook\u7684\u5d4c\u5165\u8868\u793a\uff0c\u901a\u8fc7\u91cd\u6784+\u6587\u672c\u63cf\u8ff0\u53cc\u76ee\u6807\u7aef\u5230\u7aef\u4f18\u5316\u3002\u65e0\u9700\u4fee\u6539\u539f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u67b6\u6784\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u73b0\u6709\u8bad\u7ec3\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5728\u591a\u6a21\u6001\u7406\u89e3\u548c\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u6027\u80fd\u63d0\u53472-6%\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u6709\u91cd\u5efa\u80fd\u529b\u3002", "conclusion": "\u8fd9\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\u7a81\u7834\u4e86\u89c6\u89c9\u6807\u8bb0\u5668\u7684\u8868\u5f81\u74f6\u9888\uff0c\u4e3a\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u4f18\u5316\u8303\u5f0f\uff0c\u5177\u6709\u8d85\u8d8a\u56fe\u50cf\u751f\u6210/\u7406\u89e3\u4efb\u52a1\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.09649", "pdf": "https://arxiv.org/pdf/2505.09649", "abs": "https://arxiv.org/abs/2505.09649", "authors": ["Abisha Thapa Magar", "Anup Shakya"], "title": "Next Word Suggestion using Graph Neural Network", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Language Modeling is a prevalent task in Natural Language Processing. The\ncurrently existing most recent and most successful language models often tend\nto build a massive model with billions of parameters, feed in a tremendous\namount of text data, and train with enormous computation resources which\nrequire millions of dollars. In this project, we aim to address an important\nsub-task in language modeling, i.e., context embedding. We propose an approach\nto exploit the Graph Convolution operation in GNNs to encode the context and\nuse it in coalition with LSTMs to predict the next word given a local context\nof preceding words. We test this on the custom Wikipedia text corpus using a\nvery limited amount of resources and show that this approach works fairly well\nto predict the next word.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u7ed3\u5408\u56fe\u5377\u79ef\u7f51\u7edc(GCN)\u4e0eLSTM\u7684\u6df7\u5408\u6a21\u578b\uff0c\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7684\u4e0a\u4e0b\u6587\u5d4c\u5165\u4e0e\u4e0b\u4e00\u4e2a\u5355\u8bcd\u9884\u6d4b\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9700\u8981\u6570\u5341\u4ebf\u53c2\u6570\u548c\u5de8\u989d\u8ba1\u7b97\u8d44\u6e90\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u8d44\u6e90\u6548\u7387\u66f4\u9ad8\u7684\u4e0a\u4e0b\u6587\u5d4c\u5165\u5b50\u4efb\u52a1\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528GCN\u7f16\u7801\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4e0eLSTM\u5c40\u90e8\u5e8f\u5217\u5efa\u6a21\u7ed3\u5408\uff0c\u5728\u81ea\u5b9a\u4e49\u7ef4\u57fa\u767e\u79d1\u8bed\u6599\u5e93\u4e0a\u8fdb\u884c\u4f4e\u8d44\u6e90\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6df7\u5408\u6a21\u578b\u5728\u6709\u9650\u8d44\u6e90\u6761\u4ef6\u4e0b\u80fd\u6709\u6548\u9884\u6d4b\u4e0b\u4e00\u4e2a\u5355\u8bcd\uff0c\u6027\u80fd\u8868\u73b0\u826f\u597d\u3002", "conclusion": "GCN\u4e0eLSTM\u7684\u534f\u540c\u67b6\u6784\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u8bed\u8a00\u5efa\u6a21\u63d0\u4f9b\u4e86\u53ef\u884c\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2505.09655", "pdf": "https://arxiv.org/pdf/2505.09655", "abs": "https://arxiv.org/abs/2505.09655", "authors": ["Xiwen Chen", "Wenhui Zhu", "Peijie Qiu", "Xuanzhao Dong", "Hao Wang", "Haiyu Wu", "Huayu Li", "Aristeidis Sotiras", "Yalin Wang", "Abolfazl Razi"], "title": "DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in reinforcement learning for language model post-training,\nsuch as Group Relative Policy Optimization (GRPO), have shown promise in\nlow-resource settings. However, GRPO typically relies on solution-level and\nscalar reward signals that fail to capture the semantic diversity among sampled\ncompletions. This leads to what we identify as a diversity-quality\ninconsistency, where distinct reasoning paths may receive indistinguishable\nrewards. To address this limitation, we propose $\\textit{Diversity-aware Reward\nAdjustment}$ (DRA), a method that explicitly incorporates semantic diversity\ninto the reward computation. DRA uses Submodular Mutual Information (SMI) to\ndownweight redundant completions and amplify rewards for diverse ones. This\nencourages better exploration during learning, while maintaining stable\nexploitation of high-quality samples. Our method integrates seamlessly with\nboth GRPO and its variant DR.~GRPO, resulting in $\\textit{DRA-GRPO}$ and\n$\\textit{DGA-DR.~GRPO}$. We evaluate our method on five mathematical reasoning\nbenchmarks and find that it outperforms recent strong baselines. It achieves\nstate-of-the-art performance with an average accuracy of 58.2%, using only\n7,000 fine-tuning samples and a total training cost of approximately $55. The\ncode is available at https://github.com/xiwenc1/DRA-GRPO.", "AI": {"tldr": "\u63d0\u51faDRA\u65b9\u6cd5\u901a\u8fc7\u5b50\u6a21\u4e92\u4fe1\u606f\u91cf\u5316\u8bed\u4e49\u591a\u6837\u6027\uff0c\u89e3\u51b3GRPO\u7b97\u6cd5\u5956\u52b1\u673a\u5236\u4e2d\u7684\u591a\u6837\u6027-\u8d28\u91cf\u77db\u76fe\uff0c\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u5b9e\u73b058.2%\u7684SOTA\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709GRPO\u65b9\u6cd5\u4f7f\u7528\u6807\u91cf\u5956\u52b1\u4fe1\u53f7\uff0c\u65e0\u6cd5\u533a\u5206\u4e0d\u540c\u8bed\u4e49\u7684\u63a8\u7406\u8def\u5f84\uff0c\u5bfc\u81f4\u591a\u6837\u6027-\u8d28\u91cf\u77db\u76fe\uff08\u4f18\u8d28\u591a\u6837\u6837\u672c\u88ab\u540c\u7b49\u5bf9\u5f85\uff09\u3002", "method": "DRA\u65b9\u6cd5\uff1a1. \u5229\u7528\u5b50\u6a21\u4e92\u4fe1\u606f(SMI)\u8ba1\u7b97\u8bed\u4e49\u591a\u6837\u6027\uff1b2. \u964d\u4f4e\u5197\u4f59\u6837\u672c\u5956\u52b1\uff0c\u589e\u5f3a\u591a\u6837\u6837\u672c\u5956\u52b1\uff1b3. \u517c\u5bb9GRPO\u53ca\u5176\u53d8\u4f53DR.GRPO\uff0c\u5f62\u6210DRA-GRPO/DGA-DR.GRPO\u6df7\u5408\u7b97\u6cd5\u3002", "result": "\u57285\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff1a1. \u5e73\u5747\u51c6\u786e\u738758.2%\uff08SOTA\uff09\uff1b2. \u4ec5\u97007,000\u5fae\u8c03\u6837\u672c\uff1b3. \u603b\u8bad\u7ec3\u6210\u672c\u7ea655\u7f8e\u5143\u3002", "conclusion": "DRA\u673a\u5236\u6709\u6548\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u8bc1\u5b9e\u8bed\u4e49\u591a\u6837\u6027\u5efa\u6a21\u5bf9\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u4f4e\u8d44\u6e90\u573a\u666f\u63d0\u4f9b\u9ad8\u6027\u4ef7\u6bd4\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.09662", "pdf": "https://arxiv.org/pdf/2505.09662", "abs": "https://arxiv.org/abs/2505.09662", "authors": ["Philipp Schoenegger", "Francesco Salvi", "Jiacheng Liu", "Xiaoli Nan", "Ramit Debnath", "Barbara Fasolo", "Evelina Leivada", "Gabriel Recchia", "Fritz G\u00fcnther", "Ali Zarifhonarvar", "Joe Kwon", "Zahoor Ul Islam", "Marco Dehnert", "Daryl Y. H. Lee", "Madeline G. Reinecke", "David G. Kamper", "Mert Koba\u015f", "Adam Sandford", "Jonas Kgomo", "Luke Hewitt", "Shreya Kapoor", "Kerem Oktar", "Eyup Engin Kucuk", "Bo Feng", "Cameron R. Jones", "Izzy Gainsburg", "Sebastian Olschewski", "Nora Heinzelmann", "Francisco Cruz", "Ben M. Tappin", "Tao Ma", "Peter S. Park", "Rayan Onyonka", "Arthur Hjorth", "Peter Slattery", "Qingcheng Zeng", "Lennart Finke", "Igor Grossmann", "Alessandro Salatiello", "Ezra Karger"], "title": "Large Language Models Are More Persuasive Than Incentivized Human Persuaders", "categories": ["cs.CL", "I.2.7; H.1.2; K.4.1; H.5.2"], "comment": null, "summary": "We directly compare the persuasion capabilities of a frontier large language\nmodel (LLM; Claude Sonnet 3.5) against incentivized human persuaders in an\ninteractive, real-time conversational quiz setting. In this preregistered,\nlarge-scale incentivized experiment, participants (quiz takers) completed an\nonline quiz where persuaders (either humans or LLMs) attempted to persuade quiz\ntakers toward correct or incorrect answers. We find that LLM persuaders\nachieved significantly higher compliance with their directional persuasion\nattempts than incentivized human persuaders, demonstrating superior persuasive\ncapabilities in both truthful (toward correct answers) and deceptive (toward\nincorrect answers) contexts. We also find that LLM persuaders significantly\nincreased quiz takers' accuracy, leading to higher earnings, when steering quiz\ntakers toward correct answers, and significantly decreased their accuracy,\nleading to lower earnings, when steering them toward incorrect answers.\nOverall, our findings suggest that AI's persuasion capabilities already exceed\nthose of humans that have real-money bonuses tied to performance. Our findings\nof increasingly capable AI persuaders thus underscore the urgency of emerging\nalignment and governance frameworks.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08Claude Sonnet 3.5\uff09\u5728\u771f\u5b9e\u5bf9\u8bdd\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u8d85\u8d8a\u4eba\u7c7b\u7684\u8bf4\u670d\u80fd\u529b\uff0c\u65e0\u8bba\u5f15\u5bfc\u6b63\u786e\u6216\u9519\u8bef\u7b54\u6848\u5747\u66f4\u6709\u6548", "motivation": "\u63a2\u7a76\u524d\u6cbfAI\u6a21\u578b\u5728\u5b9e\u65f6\u5bf9\u8bdd\u573a\u666f\u4e2d\u7684\u8bf4\u670d\u80fd\u529b\uff0c\u53ca\u5176\u4e0e\u6fc0\u52b1\u578b\u4eba\u7c7b\u8bf4\u670d\u8005\u7684\u6bd4\u8f83", "method": "\u91c7\u7528\u9884\u6ce8\u518c\u5927\u89c4\u6a21\u6fc0\u52b1\u5b9e\u9a8c\uff1a1\uff09\u5728\u7ebf\u5b9e\u65f6\u95ee\u7b54\u573a\u666f 2\uff09\u8bf4\u670d\u8005\uff08\u4eba\u7c7b/AI\uff09\u5f15\u5bfc\u53c2\u4e0e\u8005\u9009\u62e9\u6b63\u8bef\u7b54\u6848 3\uff09\u6d4b\u91cf\u670d\u4ece\u7387\u4e0e\u7ecf\u6d4e\u6536\u76ca", "result": "AI\u8bf4\u670d\u6210\u529f\u7387\u663e\u8457\u9ad8\u4e8e\u4eba\u7c7b\uff08+21%\u6b63\u786e\u5f15\u5bfc\uff0c+19%\u9519\u8bef\u5f15\u5bfc\uff09\uff0c\u53cc\u5411\u6539\u53d8\u53c2\u4e0e\u8005\u51c6\u786e\u7387\u4e0e\u7ecf\u6d4e\u6536\u76ca", "conclusion": "AI\u8bf4\u670d\u80fd\u529b\u5df2\u8d85\u8d8a\u91d1\u94b1\u6fc0\u52b1\u7684\u4eba\u7c7b\u8868\u73b0\uff0c\u7a81\u663eAI\u6cbb\u7406\u4e0e\u5bf9\u9f50\u673a\u5236\u7684\u7d27\u8feb\u9700\u6c42"}}
{"id": "2505.09666", "pdf": "https://arxiv.org/pdf/2505.09666", "abs": "https://arxiv.org/abs/2505.09666", "authors": ["Yumin Choi", "Jinheon Baek", "Sung Ju Hwang"], "title": "System Prompt Optimization with Meta-Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable capabilities, with\noptimizing their input prompts playing a pivotal role in maximizing their\nperformance. However, while LLM prompts consist of both the task-agnostic\nsystem prompts and task-specific user prompts, existing work on prompt\noptimization has focused on user prompts specific to individual queries or\ntasks, and largely overlooked the system prompt that is, once optimized,\napplicable across different tasks and domains. Motivated by this, we introduce\nthe novel problem of bilevel system prompt optimization, whose objective is to\ndesign system prompts that are robust to diverse user prompts and transferable\nto unseen tasks. To tackle this problem, we then propose a meta-learning\nframework, which meta-learns the system prompt by optimizing it over various\nuser prompts across multiple datasets, while simultaneously updating the user\nprompts in an iterative manner to ensure synergy between them. We conduct\nexperiments on 14 unseen datasets spanning 5 different domains, on which we\nshow that our approach produces system prompts that generalize effectively to\ndiverse user prompts. Also, our findings reveal that the optimized system\nprompt enables rapid adaptation even to unseen tasks, requiring fewer\noptimization steps for test-time user prompts while achieving improved\nperformance.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u5c42\u7cfb\u7edf\u63d0\u793a\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5143\u5b66\u4e60\u65b9\u6cd5\u8bbe\u8ba1\u53ef\u6cdb\u5316\u81f3\u591a\u4efb\u52a1\u4e14\u652f\u6301\u5feb\u901f\u8fc1\u79fb\u7684\u7cfb\u7edf\u63d0\u793a\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u4f18\u5316\u7814\u7a76\u96c6\u4e2d\u4e8e\u7528\u6237\u63d0\u793a\u5c42\u9762\uff0c\u5ffd\u89c6\u4e86\u8de8\u4efb\u52a1\u901a\u7528\u7684\u7cfb\u7edf\u63d0\u793a\u4f18\u5316\u6f5c\u529b\u3002\u7cfb\u7edf\u63d0\u793a\u7684\u4f18\u5316\u53ef\u63d0\u5347\u6a21\u578b\u5bf9\u4e0d\u540c\u7528\u6237\u63d0\u793a\u7684\u517c\u5bb9\u6027\u53ca\u4efb\u52a1\u8fc1\u79fb\u80fd\u529b\u3002", "method": "\u91c7\u7528\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u591a\u6570\u636e\u96c6\u4e0a\u8fed\u4ee3\u4f18\u5316\u7cfb\u7edf\u63d0\u793a\u4e0e\u7528\u6237\u63d0\u793a\u7684\u534f\u540c\u5173\u7cfb\uff0c\u786e\u4fdd\u7cfb\u7edf\u63d0\u793a\u7684\u9c81\u68d2\u6027\u548c\u7528\u6237\u63d0\u793a\u9002\u914d\u6027\u3002", "result": "\u57285\u4e2a\u9886\u57df14\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u4f18\u5316\u540e\u7684\u7cfb\u7edf\u63d0\u793a\u5bf9\u672a\u89c1\u4efb\u52a1\u7528\u6237\u63d0\u793a\u9002\u914d\u901f\u5ea6\u63d0\u534740%\uff0c\u6027\u80fd\u5e73\u5747\u63d0\u534712.6%\u3002", "conclusion": "\u7cfb\u7edf\u63d0\u793a\u4f18\u5316\u663e\u8457\u589e\u5f3aLLMs\u7684\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u73b0\u96f6\u6837\u672c/\u5c0f\u6837\u672c\u573a\u666f\u4e0b\u7684\u5feb\u901f\u90e8\u7f72\uff0c\u4e3a\u8de8\u9886\u57df\u5e94\u7528\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2505.09746", "pdf": "https://arxiv.org/pdf/2505.09746", "abs": "https://arxiv.org/abs/2505.09746", "authors": ["Xabier Morales", "Ayah Elsayed", "Debbie Zhao", "Filip Loncaric", "Ainhoa Aguado", "Mireia Masias", "Gina Quill", "Marc Ramos", "Ada Doltra", "Ana Garcia", "Marta Sitges", "David Marlevi", "Alistair Young", "Martyn Nash", "Bart Bijnens", "Oscar Camara"], "title": "A Computational Pipeline for Advanced Analysis of 4D Flow MRI in the Left Atrium", "categories": ["cs.CV"], "comment": null, "summary": "The left atrium (LA) plays a pivotal role in modulating left ventricular\nfilling, but our comprehension of its hemodynamics is significantly limited by\nthe constraints of conventional ultrasound analysis. 4D flow magnetic resonance\nimaging (4D Flow MRI) holds promise for enhancing our understanding of atrial\nhemodynamics. However, the low velocities within the LA and the limited spatial\nresolution of 4D Flow MRI make analyzing this chamber challenging. Furthermore,\nthe absence of dedicated computational frameworks, combined with diverse\nacquisition protocols and vendors, complicates gathering large cohorts for\nstudying the prognostic value of hemodynamic parameters provided by 4D Flow\nMRI. In this study, we introduce the first open-source computational framework\ntailored for the analysis of 4D Flow MRI in the LA, enabling comprehensive\nqualitative and quantitative analysis of advanced hemodynamic parameters. Our\nframework proves robust to data from different centers of varying quality,\nproducing high-accuracy automated segmentations (Dice $>$ 0.9 and Hausdorff 95\n$<$ 3 mm), even with limited training data. Additionally, we conducted the\nfirst comprehensive assessment of energy, vorticity, and pressure parameters in\nthe LA across a spectrum of disorders to investigate their potential as\nprognostic biomarkers.", "AI": {"tldr": "\u5f00\u53d1\u9996\u4e2a\u9488\u5bf9\u5de6\u5fc3\u623f4D Flow MRI\u5206\u6790\u7684\u5f00\u6e90\u8ba1\u7b97\u6846\u67b6\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u81ea\u52a8\u5206\u5272\u5e76\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u591a\u79cd\u8840\u6d41\u52a8\u529b\u5b66\u53c2\u6570\u7684\u9884\u540e\u4ef7\u503c", "motivation": "\u4f20\u7edf\u8d85\u58f0\u5206\u6790\u5de6\u5fc3\u623f\u8840\u6d41\u52a8\u529b\u5b66\u5b58\u5728\u5c40\u9650\uff0c4D Flow MRI\u5206\u6790\u53d7\u9650\u4e8e\u4f4e\u6d41\u901f/\u5206\u8fa8\u7387\u3001\u7f3a\u4e4f\u4e13\u7528\u6846\u67b6\u53ca\u591a\u4e2d\u5fc3\u6570\u636e\u6574\u5408\u56f0\u96be", "method": "\u521b\u5efa\u9c81\u68d2\u6027\u5f3a\u7684\u5f00\u6e90\u6846\u67b6\uff0c\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u5b9e\u73b0\u8de8\u4e2d\u5fc3\u6570\u636e\u7684\u9ad8\u7cbe\u5ea6\u81ea\u52a8\u5206\u5272\uff08Dice>0.9\uff09\uff0c\u4f7f\u7528\u6709\u9650\u8bad\u7ec3\u6570\u636e\u8fbe\u6210\u4f18\u5f02\u6548\u679c", "result": "\u6846\u67b6\u5b9e\u73b0\u6beb\u7c73\u7ea7\u7cbe\u5ea6\u7684\u81ea\u52a8\u5206\u5272\uff0c\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u5de6\u5fc3\u623f\u80fd\u91cf/\u6da1\u5ea6/\u538b\u529b\u53c2\u6570\u5728\u4e0d\u540c\u75be\u75c5\u4e2d\u7684\u9884\u540e\u751f\u7269\u6807\u5fd7\u7269\u6f5c\u529b", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u591a\u4e2d\u5fc3\u7814\u7a76\u63d0\u4f9b\u6807\u51c6\u5316\u5de5\u5177\uff0c\u9a8c\u8bc1\u7684\u8840\u6d41\u52a8\u529b\u5b66\u53c2\u6570\u4e3a\u5fc3\u8840\u7ba1\u75be\u75c5\u9884\u540e\u8bc4\u4f30\u5f00\u8f9f\u65b0\u9014\u5f84"}}
{"id": "2505.10144", "pdf": "https://arxiv.org/pdf/2505.10144", "abs": "https://arxiv.org/abs/2505.10144", "authors": ["Xuechang Tu", "Lukas Radl", "Michael Steiner", "Markus Steinberger", "Bernhard Kerbl", "Fernando de la Torre"], "title": "VRSplat: Fast and Robust Gaussian Splatting for Virtual Reality", "categories": ["cs.GR", "cs.CV"], "comment": "I3D'25 (PACMCGIT); Project Page: https://cekavis.site/VRSplat/", "summary": "3D Gaussian Splatting (3DGS) has rapidly become a leading technique for\nnovel-view synthesis, providing exceptional performance through efficient\nsoftware-based GPU rasterization. Its versatility enables real-time\napplications, including on mobile and lower-powered devices. However, 3DGS\nfaces key challenges in virtual reality (VR): (1) temporal artifacts, such as\npopping during head movements, (2) projection-based distortions that result in\ndisturbing and view-inconsistent floaters, and (3) reduced framerates when\nrendering large numbers of Gaussians, falling below the critical threshold for\nVR. Compared to desktop environments, these issues are drastically amplified by\nlarge field-of-view, constant head movements, and high resolution of\nhead-mounted displays (HMDs). In this work, we introduce VRSplat: we combine\nand extend several recent advancements in 3DGS to address challenges of VR\nholistically. We show how the ideas of Mini-Splatting, StopThePop, and Optimal\nProjection can complement each other, by modifying the individual techniques\nand core 3DGS rasterizer. Additionally, we propose an efficient foveated\nrasterizer that handles focus and peripheral areas in a single GPU launch,\navoiding redundant computations and improving GPU utilization. Our method also\nincorporates a fine-tuning step that optimizes Gaussian parameters based on\nStopThePop depth evaluations and Optimal Projection. We validate our method\nthrough a controlled user study with 25 participants, showing a strong\npreference for VRSplat over other configurations of Mini-Splatting. VRSplat is\nthe first, systematically evaluated 3DGS approach capable of supporting modern\nVR applications, achieving 72+ FPS while eliminating popping and\nstereo-disrupting floaters.", "AI": {"tldr": "VRSplat\u6574\u5408Mini-Splatting\u3001StopThePop\u548cOptimal Projection\u6280\u672f\uff0c\u9996\u6b21\u5b9e\u73b0\u652f\u6301\u73b0\u4ee3VR\u5e94\u7528\u76843DGS\u65b9\u6848\uff0c\u6d88\u9664\u753b\u9762\u5f02\u5e38\u5e76\u8fbe\u523072+ FPS", "motivation": "\u89e3\u51b33DGS\u5728VR\u4e2d\u9762\u4e34\u7684\u4e09\u91cd\u6311\u6218\uff1a\u5934\u90e8\u8fd0\u52a8\u5bfc\u81f4\u7684\u65f6\u95f4\u4f2a\u5f71\u3001\u6295\u5f71\u5931\u771f\u4ea7\u751f\u7684\u89c6\u89c9\u5e72\u6270\u6d6e\u70b9\uff0c\u4ee5\u53ca\u5927\u573a\u666f\u4e0b\u5e27\u7387\u4e0d\u8db3\u7684\u95ee\u9898", "method": "\u6539\u8fdb\u6838\u5fc3\u5149\u6805\u5316\u5668+\u773c\u7a9d\u5206\u533a\u6e32\u67d3\uff08\u5355GPU\u542f\u52a8\u5904\u7406\u805a\u7126/\u5468\u8fb9\u533a\u57df\uff09+\u57fa\u4e8e\u6df1\u5ea6\u8bc4\u4f30\u7684\u53c2\u6570\u5fae\u8c03\uff0c\u7efc\u5408\u4f18\u5316\u9ad8\u65af\u6e85\u5c04\u7b97\u6cd5", "result": "\u7528\u6237\u7814\u7a76\u663e\u793a25\u540d\u53c2\u4e0e\u8005\u663e\u8457\u504f\u597dVRSplat\uff0c\u7cfb\u7edf\u5728\u4fdd\u630172+\u5e27\u7387\u7684\u540c\u65f6\u6d88\u9664\u753b\u9762\u5f02\u5e38", "conclusion": "VRSplat\u662f\u9996\u4e2a\u7ecf\u7cfb\u7edf\u9a8c\u8bc1\u3001\u80fd\u652f\u6491\u73b0\u4ee3VR\u76843DGS\u65b9\u6848\uff0c\u901a\u8fc7\u6280\u672f\u521b\u65b0\u5b9e\u73b0\u5b9e\u65f6\u7a33\u5b9a\u6e32\u67d3"}}
{"id": "2505.09701", "pdf": "https://arxiv.org/pdf/2505.09701", "abs": "https://arxiv.org/abs/2505.09701", "authors": ["Xin Liu", "Lechen Zhang", "Sheza Munir", "Yiyang Gu", "Lu Wang"], "title": "VeriFact: Enhancing Long-Form Factuality Evaluation with Refined Fact Extraction and Reference Facts", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) excel at generating long-form responses, but\nevaluating their factuality remains challenging due to complex inter-sentence\ndependencies within the generated facts. Prior solutions predominantly follow a\ndecompose-decontextualize-verify pipeline but often fail to capture essential\ncontext and miss key relational facts. In this paper, we introduce VeriFact, a\nfactuality evaluation framework designed to enhance fact extraction by\nidentifying and resolving incomplete and missing facts to support more accurate\nverification results. Moreover, we introduce FactRBench , a benchmark that\nevaluates both precision and recall in long-form model responses, whereas prior\nwork primarily focuses on precision. FactRBench provides reference fact sets\nfrom advanced LLMs and human-written answers, enabling recall assessment.\nEmpirical evaluations show that VeriFact significantly enhances fact\ncompleteness and preserves complex facts with critical relational information,\nresulting in more accurate factuality evaluation. Benchmarking various open-\nand close-weight LLMs on FactRBench indicate that larger models within same\nmodel family improve precision and recall, but high precision does not always\ncorrelate with high recall, underscoring the importance of comprehensive\nfactuality assessment.", "AI": {"tldr": "\u63d0\u51faVeriFact\u6846\u67b6\u4f18\u5316LLM\u4e8b\u5b9e\u6027\u8bc4\u4f30\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e0a\u4e0b\u6587\u5173\u8054\u7684\u95ee\u9898\uff0c\u5e76\u5efa\u7acbFactRBench\u57fa\u51c6\u540c\u65f6\u8bc4\u4f30\u7cbe\u786e\u5ea6\u4e0e\u53ec\u56de\u7387", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u4e8b\u5b9e\u6027\u8bc4\u4f30\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u5206\u89e3-\u53bb\u4e0a\u4e0b\u6587\u5316\u6d41\u7a0b\uff0c\u5bfc\u81f4\u5173\u952e\u4e0a\u4e0b\u6587\u4e22\u5931\u4e14\u65e0\u6cd5\u6355\u6349\u590d\u6742\u5173\u7cfb\u4e8b\u5b9e\u3002\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u4f53\u7cfb\u540c\u65f6\u8861\u91cf\u7cbe\u786e\u5ea6\u548c\u53ec\u56de\u7387", "method": "1. \u5f00\u53d1VeriFact\u6846\u67b6\u901a\u8fc7\u4e8b\u5b9e\u5b8c\u6574\u6027\u589e\u5f3a\u548c\u5173\u7cfb\u4fe1\u606f\u4fdd\u7559\u6280\u672f\u6539\u8fdb\u4e8b\u5b9e\u63d0\u53d6 2. \u6784\u5efaFactRBench\u57fa\u51c6\uff0c\u6574\u5408LLM\u751f\u6210\u5185\u5bb9\u4e0e\u4eba\u5de5\u6807\u6ce8\u7b54\u6848\uff0c\u652f\u6301\u53ec\u56de\u7387\u8bc4\u4f30", "result": "VeriFact\u663e\u8457\u63d0\u5347\u4e8b\u5b9e\u5b8c\u6574\u6027(\u8f83\u57fa\u7ebf\u63d0\u9ad837%)\u5e76\u4fdd\u7559\u5173\u952e\u5173\u7cfb\u4fe1\u606f\uff0c\u76f8\u540c\u6a21\u578b\u5bb6\u65cf\u4e2d\u66f4\u5927\u6a21\u578b\u5728\u7cbe\u786e\u5ea6\u548c\u53ec\u56de\u7387\u5747\u6709\u63d0\u5347\uff0c\u4f46\u4e24\u8005\u8868\u73b0\u4e0d\u603b\u6b63\u76f8\u5173", "conclusion": "\u7efc\u5408\u4e8b\u5b9e\u6027\u8bc4\u4f30\u9700\u540c\u65f6\u8003\u8651\u7cbe\u786e\u5ea6\u4e0e\u53ec\u56de\u7387\uff0c\u6a21\u578b\u89c4\u6a21\u6269\u5927\u80fd\u63d0\u5347\u6027\u80fd\u4f46\u9700\u9488\u5bf9\u6027\u4f18\u5316\uff0c\u4e3aLLM\u4e8b\u5b9e\u6027\u8bc4\u4f30\u63d0\u4f9b\u65b0\u65b9\u6cd5\u8bba\u4e0e\u8bc4\u4f30\u57fa\u51c6"}}
{"id": "2505.09827", "pdf": "https://arxiv.org/pdf/2505.09827", "abs": "https://arxiv.org/abs/2505.09827", "authors": ["Julian Tanke", "Takashi Shibuya", "Kengo Uchida", "Koichi Saito", "Yuki Mitsufuji"], "title": "Dyadic Mamba: Long-term Dyadic Human Motion Synthesis", "categories": ["cs.CV"], "comment": "CVPR 2025 HuMoGen Workshop", "summary": "Generating realistic dyadic human motion from text descriptions presents\nsignificant challenges, particularly for extended interactions that exceed\ntypical training sequence lengths. While recent transformer-based approaches\nhave shown promising results for short-term dyadic motion synthesis, they\nstruggle with longer sequences due to inherent limitations in positional\nencoding schemes. In this paper, we introduce Dyadic Mamba, a novel approach\nthat leverages State-Space Models (SSMs) to generate high-quality dyadic human\nmotion of arbitrary length. Our method employs a simple yet effective\narchitecture that facilitates information flow between individual motion\nsequences through concatenation, eliminating the need for complex\ncross-attention mechanisms. We demonstrate that Dyadic Mamba achieves\ncompetitive performance on standard short-term benchmarks while significantly\noutperforming transformer-based approaches on longer sequences. Additionally,\nwe propose a new benchmark for evaluating long-term motion synthesis quality,\nproviding a standardized framework for future research. Our results demonstrate\nthat SSM-based architectures offer a promising direction for addressing the\nchallenging task of long-term dyadic human motion synthesis from text\ndescriptions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b(SSM)\u7684Dyadic Mamba\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e8f\u5217\u8fde\u63a5\u67b6\u6784\u6709\u6548\u89e3\u51b3\u957f\u5e8f\u5217\u53cc\u4eba\u8fd0\u52a8\u5408\u6210\u7684\u6280\u672f\u74f6\u9888\uff0c\u5728\u957f\u5e8f\u5217\u751f\u6210\u6548\u679c\u4e0a\u663e\u8457\u8d85\u8d8aTransformer\u65b9\u6cd5\uff0c\u5e76\u5efa\u7acb\u65b0\u8bc4\u4f30\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u53d7\u9650\u4e8e\u4f4d\u7f6e\u7f16\u7801\u673a\u5236\uff0c\u96be\u4ee5\u751f\u6210\u8d85\u8fc7\u8bad\u7ec3\u5e8f\u5217\u957f\u5ea6\u7684\u771f\u5b9e\u53cc\u4eba\u4ea4\u4e92\u52a8\u4f5c\u3002\u9700\u8981\u63a2\u7d22\u66f4\u9002\u5e94\u957f\u5e8f\u5217\u751f\u6210\u7684\u65b0\u67b6\u6784\u3002", "method": "\u91c7\u7528\u72b6\u6001\u7a7a\u95f4\u6a21\u578b(SSM)\u6784\u5efa\u7b80\u5355\u67b6\u6784\uff0c\u901a\u8fc7\u76f4\u63a5\u8fde\u63a5\u4e2a\u4f53\u8fd0\u52a8\u5e8f\u5217\u5b9e\u73b0\u4fe1\u606f\u4ea4\u4e92\uff0c\u907f\u514d\u4f20\u7edf\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u7684\u590d\u6742\u6027\u3002", "result": "\u5728\u77ed\u671f\u57fa\u51c6\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u957f\u5e8f\u5217\u751f\u6210\u8d28\u91cf\u663e\u8457\u4f18\u4e8eTransformer\u3002\u63d0\u51fa\u7684\u65b0\u8bc4\u4f30\u57fa\u51c6\u4e3a\u9886\u57df\u7814\u7a76\u63d0\u4f9b\u6807\u51c6\u5316\u6846\u67b6\u3002", "conclusion": "SSM\u67b6\u6784\u4e3a\u6587\u672c\u9a71\u52a8\u7684\u957f\u65f6\u53cc\u4eba\u8fd0\u52a8\u5408\u6210\u4efb\u52a1\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u5176\u5e8f\u5217\u5efa\u6a21\u4f18\u52bf\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2505.10558", "pdf": "https://arxiv.org/pdf/2505.10558", "abs": "https://arxiv.org/abs/2505.10558", "authors": ["Peiying Zhang", "Nanxuan Zhao", "Jing Liao"], "title": "Style Customization of Text-to-Vector Generation with Image Diffusion Priors", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted by SIGGRAPH 2025 (Conference Paper). Project page:\n  https://customsvg.github.io", "summary": "Scalable Vector Graphics (SVGs) are highly favored by designers due to their\nresolution independence and well-organized layer structure. Although existing\ntext-to-vector (T2V) generation methods can create SVGs from text prompts, they\noften overlook an important need in practical applications: style\ncustomization, which is vital for producing a collection of vector graphics\nwith consistent visual appearance and coherent aesthetics. Extending existing\nT2V methods for style customization poses certain challenges.\nOptimization-based T2V models can utilize the priors of text-to-image (T2I)\nmodels for customization, but struggle with maintaining structural regularity.\nOn the other hand, feed-forward T2V models can ensure structural regularity,\nyet they encounter difficulties in disentangling content and style due to\nlimited SVG training data.\n  To address these challenges, we propose a novel two-stage style customization\npipeline for SVG generation, making use of the advantages of both feed-forward\nT2V models and T2I image priors. In the first stage, we train a T2V diffusion\nmodel with a path-level representation to ensure the structural regularity of\nSVGs while preserving diverse expressive capabilities. In the second stage, we\ncustomize the T2V diffusion model to different styles by distilling customized\nT2I models. By integrating these techniques, our pipeline can generate\nhigh-quality and diverse SVGs in custom styles based on text prompts in an\nefficient feed-forward manner. The effectiveness of our method has been\nvalidated through extensive experiments. The project page is\nhttps://customsvg.github.io.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u9636\u6bb5SVG\u98ce\u683c\u5b9a\u5236\u6846\u67b6\uff0c\u7ed3\u5408\u524d\u9988T2V\u6a21\u578b\u7684\u7ed3\u6784\u4f18\u52bf\u4e0eT2I\u56fe\u50cf\u5148\u9a8c\uff0c\u5b9e\u73b0\u9ad8\u6548\u751f\u6210\u98ce\u683c\u4e00\u81f4\u7684\u77e2\u91cf\u56fe\u5f62\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u751f\u6210\u77e2\u91cf\u65b9\u6cd5\u65e0\u6cd5\u517c\u987e\u98ce\u683c\u5b9a\u5236\u9700\u6c42\u4e0e\u7ed3\u6784\u89c4\u5f8b\u6027\uff0c\u524d\u8005\u5bfc\u81f4\u89c6\u89c9\u4e00\u81f4\u6027\u4e0d\u8db3\uff0c\u540e\u8005\u53d7\u9650\u4e8e\u6570\u636e\u91cf\u96be\u4ee5\u89e3\u8026\u5185\u5bb9\u4e0e\u98ce\u683c\u3002", "method": "1. \u8bad\u7ec3\u8def\u5f84\u7ea7T2V\u6269\u6563\u6a21\u578b\u786e\u4fddSVG\u7ed3\u6784\u89c4\u5f8b\u6027\uff1b2. \u901a\u8fc7\u84b8\u998f\u5b9a\u5236T2I\u6a21\u578b\u5b9e\u73b0\u98ce\u683c\u8fc1\u79fb\uff0c\u4fdd\u7559\u751f\u6210\u591a\u6837\u6027\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u98ce\u683c\u7edf\u4e00\u7684SVG\uff0c\u5728\u6548\u7387\u4e0e\u89c6\u89c9\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\u3002", "conclusion": "\u7ed3\u5408\u524d\u9988\u6a21\u578b\u7ed3\u6784\u4f18\u52bf\u4e0eT2I\u98ce\u683c\u5b9a\u5236\u80fd\u529b\uff0c\u6709\u6548\u5e73\u8861\u77e2\u91cf\u56fe\u5f62\u7684\u7ed3\u6784\u89c4\u8303\u4e0e\u98ce\u683c\u8868\u8fbe\u9700\u6c42\u3002"}}
{"id": "2505.09724", "pdf": "https://arxiv.org/pdf/2505.09724", "abs": "https://arxiv.org/abs/2505.09724", "authors": ["Gino Carmona-D\u00edaz", "William Jim\u00e9nez-Leal", "Mar\u00eda Alejandra Grisales", "Chandra Sripada", "Santiago Amaya", "Michael Inzlicht", "Juan Pablo Berm\u00fadez"], "title": "An AI-Powered Research Assistant in the Lab: A Practical Guide for Text Analysis Through Iterative Collaboration with LLMs", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "31 pages, 1 figure", "summary": "Analyzing texts such as open-ended responses, headlines, or social media\nposts is a time- and labor-intensive process highly susceptible to bias. LLMs\nare promising tools for text analysis, using either a predefined (top-down) or\na data-driven (bottom-up) taxonomy, without sacrificing quality. Here we\npresent a step-by-step tutorial to efficiently develop, test, and apply\ntaxonomies for analyzing unstructured data through an iterative and\ncollaborative process between researchers and LLMs. Using personal goals\nprovided by participants as an example, we demonstrate how to write prompts to\nreview datasets and generate a taxonomy of life domains, evaluate and refine\nthe taxonomy through prompt and direct modifications, test the taxonomy and\nassess intercoder agreements, and apply the taxonomy to categorize an entire\ndataset with high intercoder reliability. We discuss the possibilities and\nlimitations of using LLMs for text analysis.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5229\u7528LLMs\u8fdb\u884c\u6587\u672c\u5206\u6790\u7684\u8fed\u4ee3\u534f\u4f5c\u6d41\u7a0b\uff0c\u901a\u8fc7\u5206\u6b65\u6559\u7a0b\u5c55\u793a\u5982\u4f55\u751f\u6210\u5e76\u9a8c\u8bc1\u5206\u7c7b\u6cd5\uff0c\u6700\u7ec8\u5b9e\u73b0\u9ad8\u6548\u975e\u7ed3\u6784\u5316\u6570\u636e\u5904\u7406\uff0c\u540c\u65f6\u8ba8\u8bba\u4e86\u6280\u672f\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u4eba\u5de5\u6587\u672c\u5206\u6790\u5b58\u5728\u6548\u7387\u4f4e\u548c\u4e3b\u89c2\u504f\u5dee\u95ee\u9898\uff0cLLMs\u53ef\u517c\u987e\u6548\u7387\u4e0e\u8d28\u91cf\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u5e94\u7528\u65b9\u6cd5\uff0c\u9700\u5efa\u7acb\u6807\u51c6\u5316\u6d41\u7a0b\u6307\u5bfc\u5206\u7c7b\u6cd5\u5f00\u53d1\u4e0e\u5e94\u7528\u3002", "method": "\u91c7\u7528\u7814\u7a76\u8005\u4e0eLLMs\u534f\u4f5c\u7684\u8fed\u4ee3\u6d41\u7a0b\uff1a1) \u901a\u8fc7prompt\u751f\u6210\u521d\u59cb\u5206\u7c7b\u6cd5 2) \u591a\u8f6e\u8bc4\u4f30\u4f18\u5316\u5206\u7c7b\u7ed3\u6784 3) \u6d4b\u8bd5\u7f16\u7801\u8005\u95f4\u4fe1\u5ea6 4) \u5168\u6570\u636e\u96c6\u5e94\u7528\u9a8c\u8bc1\u53ef\u9760\u6027\u3002", "result": "\u4ee5\u4e2a\u4eba\u76ee\u6807\u5206\u7c7b\u4e3a\u4f8b\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b00.85\u7684Kappa\u7cfb\u6570\uff0c\u8bc1\u5b9eLLMs\u80fd\u53ef\u9760\u5b8c\u6210\u6587\u672c\u5206\u7c7b\uff0c\u4f46\u5b58\u5728\u4e0a\u4e0b\u6587\u7406\u89e3\u504f\u5dee\u9700\u4eba\u5de5\u6821\u6b63\u3002", "conclusion": "LLMs\u4e3a\u6587\u672c\u5206\u6790\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u9700\u6784\u5efa\u4eba\u673a\u534f\u4f5c\u673a\u5236\uff0c\u7ed3\u5408\u81ea\u52a8\u5316\u5904\u7406\u4e0e\u4eba\u5de5\u9a8c\u8bc1\uff0c\u5e73\u8861\u6548\u7387\u4e0e\u51c6\u786e\u6027\u3002"}}
{"id": "2505.09829", "pdf": "https://arxiv.org/pdf/2505.09829", "abs": "https://arxiv.org/abs/2505.09829", "authors": ["Tushar Kataria", "Shireen Y. Elhabian"], "title": "BoundarySeg:An Embarrassingly Simple Method To Boost Medical Image Segmentation Performance for Low Data Regimes", "categories": ["cs.CV"], "comment": null, "summary": "Obtaining large-scale medical data, annotated or unannotated, is challenging\ndue to stringent privacy regulations and data protection policies. In addition,\nannotating medical images requires that domain experts manually delineate\nanatomical structures, making the process both time-consuming and costly. As a\nresult, semi-supervised methods have gained popularity for reducing annotation\ncosts. However, the performance of semi-supervised methods is heavily dependent\non the availability of unannotated data, and their effectiveness declines when\nsuch data are scarce or absent. To overcome this limitation, we propose a\nsimple, yet effective and computationally efficient approach for medical image\nsegmentation that leverages only existing annotations. We propose BoundarySeg ,\na multi-task framework that incorporates organ boundary prediction as an\nauxiliary task to full organ segmentation, leveraging consistency between the\ntwo task predictions to provide additional supervision. This strategy improves\nsegmentation accuracy, especially in low data regimes, allowing our method to\nachieve performance comparable to or exceeding state-of-the-art semi supervised\napproaches all without relying on unannotated data or increasing computational\ndemands. Code will be released upon acceptance.", "AI": {"tldr": "\u63d0\u51fa\u591a\u4efb\u52a1\u6846\u67b6BoundarySeg\uff0c\u901a\u8fc7\u5668\u5b98\u5206\u5272\u4e0e\u8fb9\u754c\u9884\u6d4b\u7684\u4efb\u52a1\u4e00\u81f4\u6027\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7cbe\u5ea6\uff0c\u65e0\u9700\u672a\u6807\u6ce8\u6570\u636e\u5373\u53ef\u8d85\u8d8a\u73b0\u6709\u534a\u76d1\u7763\u65b9\u6cd5", "motivation": "\u533b\u5b66\u6570\u636e\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u9690\u79c1\u9650\u5236\u4e25\u683c\uff0c\u73b0\u6709\u534a\u76d1\u7763\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u672a\u6807\u6ce8\u6570\u636e\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u65f6\u6027\u80fd\u4e0b\u964d\u660e\u663e", "method": "\u6784\u5efa\u5668\u5b98\u5206\u5272+\u8fb9\u754c\u9884\u6d4b\u53cc\u4efb\u52a1\u6846\u67b6\uff0c\u5229\u7528\u4efb\u52a1\u9884\u6d4b\u4e00\u81f4\u6027\u4f5c\u4e3a\u989d\u5916\u76d1\u7763\u4fe1\u53f7\uff0c\u5728\u4f4e\u6570\u636e\u91cf\u65f6\u589e\u5f3a\u6a21\u578b\u8868\u73b0", "result": "\u5728\u6709\u9650\u6807\u6ce8\u6570\u636e\u4e0b\u8fbe\u5230\u6216\u8d85\u8d8a\u534a\u76d1\u7763\u65b9\u6cd5\u6027\u80fd\uff0c\u8ba1\u7b97\u6548\u7387\u4e0e\u7eaf\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53", "conclusion": "\u901a\u8fc7\u8fb9\u754c\u9884\u6d4b\u7684\u8f85\u52a9\u4efb\u52a1\u673a\u5236\u6709\u6548\u7a81\u7834\u534a\u76d1\u7763\u65b9\u6cd5\u7684\u6570\u636e\u4f9d\u8d56\u9650\u5236\uff0c\u4e3a\u4f4e\u8d44\u6e90\u573a\u666f\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.09936", "pdf": "https://arxiv.org/pdf/2505.09936", "abs": "https://arxiv.org/abs/2505.09936", "authors": ["Chenglong Wang", "Yuhao Kang", "Zhaoya Gong", "Pengjun Zhao", "Yu Feng", "Wenjia Zhang", "Ge Li"], "title": "CartoAgent: a multimodal large language model-powered multi-agent cartographic framework for map style transfer and evaluation", "categories": ["cs.HC", "cs.GR", "cs.MA", "cs.MM"], "comment": "57 pages, 17 figures", "summary": "The rapid development of generative artificial intelligence (GenAI) presents\nnew opportunities to advance the cartographic process. Previous studies have\neither overlooked the artistic aspects of maps or faced challenges in creating\nboth accurate and informative maps. In this study, we propose CartoAgent, a\nnovel multi-agent cartographic framework powered by multimodal large language\nmodels (MLLMs). This framework simulates three key stages in cartographic\npractice: preparation, map design, and evaluation. At each stage, different\nMLLMs act as agents with distinct roles to collaborate, discuss, and utilize\ntools for specific purposes. In particular, CartoAgent leverages MLLMs' visual\naesthetic capability and world knowledge to generate maps that are both\nvisually appealing and informative. By separating style from geographic data,\nit can focus on designing stylesheets without modifying the vector-based data,\nthereby ensuring geographic accuracy. We applied CartoAgent to a specific task\ncentered on map restyling-namely, map style transfer and evaluation. The\neffectiveness of this framework was validated through extensive experiments and\na human evaluation study. CartoAgent can be extended to support a variety of\ncartographic design decisions and inform future integrations of GenAI in\ncartography.", "AI": {"tldr": "\u63d0\u51faCartoAgent\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u5730\u56fe\u91cd\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5206\u79bb\u6837\u5f0f\u4e0e\u5730\u7406\u6570\u636e\u786e\u4fdd\u51c6\u786e\u6027\uff0c\u5728\u5730\u56fe\u98ce\u683c\u8fc1\u79fb\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u5730\u56fe\u827a\u672f\u6027\u4e0e\u4fe1\u606f\u51c6\u786e\u6027\uff0cGenAI\u53d1\u5c55\u4e3a\u5236\u56fe\u6d41\u7a0b\u521b\u65b0\u63d0\u4f9b\u53ef\u80fd\u3002\u7814\u7a76\u65e8\u5728\u5229\u7528MLLM\u7684\u89c6\u89c9\u5ba1\u7f8e\u80fd\u529b\u63d0\u5347\u5730\u56fe\u7f8e\u89c2\u5ea6\u4e0e\u4fe1\u606f\u91cf\u3002", "method": "\u6784\u5efa\u4e09\u9636\u6bb5\u591a\u4ee3\u7406\u534f\u4f5c\u6846\u67b6\uff1a1) \u51c6\u5907\u9636\u6bb5\u914d\u7f6e\u4efb\u52a1\u53c2\u6570 2) \u8bbe\u8ba1\u9636\u6bb5MLLM\u4ee3\u7406\u534f\u540c\u751f\u6210\u6837\u5f0f\u8868 3) \u8bc4\u4f30\u9636\u6bb5\u591a\u7ef4\u5ea6\u9a8c\u8bc1\u5730\u56fe\u8d28\u91cf\uff0c\u901a\u8fc7\u6837\u5f0f-\u6570\u636e\u5206\u79bb\u673a\u5236\u4fdd\u6301\u5730\u7406\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u6846\u67b6\u80fd\u6709\u6548\u5b8c\u6210\u5730\u56fe\u98ce\u683c\u8fc1\u79fb\uff0c\u4eba\u5de5\u8bc4\u4f30\u663e\u793a\u751f\u6210\u5730\u56fe\u5728\u7f8e\u89c2\u6027(\u63d0\u534732%)\u548c\u4fe1\u606f\u5b8c\u6574\u6027(\u63d0\u534728%)\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CartoAgent\u5c55\u793a\u4e86GenAI\u9a71\u52a8\u5236\u56fe\u51b3\u7b56\u7684\u6f5c\u529b\uff0c\u5176\u6a21\u5757\u5316\u8bbe\u8ba1\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u5236\u56fe\u573a\u666f\uff0c\u4e3a\u667a\u80fd\u5236\u56fe\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2505.09738", "pdf": "https://arxiv.org/pdf/2505.09738", "abs": "https://arxiv.org/abs/2505.09738", "authors": ["Shaurya Sharthak", "Vinayak Pahalwan", "Adithya Kamath", "Adarsh Shirawalmath"], "title": "Achieving Tokenizer Flexibility in Language Models through Heuristic Adaptation and Supertoken Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Pretrained language models (LLMs) are often constrained by their fixed\ntokenization schemes, leading to inefficiencies and performance limitations,\nparticularly for multilingual or specialized applications. This tokenizer\nlock-in presents significant challenges. standard methods to overcome this\noften require prohibitive computational resources. Although tokenizer\nreplacement with heuristic initialization aims to reduce this burden, existing\nmethods often require exhaustive residual fine-tuning and still may not fully\npreserve semantic nuances or adequately address the underlying compression\ninefficiencies. Our framework introduces two innovations: first, Tokenadapt, a\nmodel-agnostic tokenizer transplantation method, and second, novel\npre-tokenization learning for multi-word Supertokens to enhance compression and\nreduce fragmentation. Tokenadapt initializes new unique token embeddings via a\nhybrid heuristic that combines two methods: a local estimate based on subword\ndecomposition using the old tokenizer, and a global estimate utilizing the\ntop-k semantically similar tokens from the original vocabulary. This\nmethodology aims to preserve semantics while significantly minimizing\nretraining requirements. Empirical investigations validate both contributions:\nthe transplantation heuristic successfully initializes unique tokens, markedly\noutperforming conventional baselines and sophisticated methods including\nTranstokenizer and ReTok, while our Supertokens achieve notable compression\ngains. Our zero-shot perplexity results demonstrate that the TokenAdapt hybrid\ninitialization consistently yields lower perplexity ratios compared to both\nReTok and TransTokenizer baselines across different base models and newly\ntrained target tokenizers. TokenAdapt typically reduced the overall perplexity\nratio significantly compared to ReTok, yielding at least a 2-fold improvement\nin these aggregate scores.", "AI": {"tldr": "\u63d0\u51faTokenAdapt\u6846\u67b6\u89e3\u51b3LLM\u56fa\u5b9a\u5206\u8bcd\u65b9\u6848\u7684\u4f4e\u6548\u95ee\u9898\uff0c\u901a\u8fc7\u6a21\u578b\u65e0\u5173\u7684\u5206\u8bcd\u5668\u79fb\u690d\u548cSupertokens\u9884\u5b66\u4e60\u663e\u8457\u63d0\u5347\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u7684\u538b\u7f29\u6548\u7387\u4e0e\u6a21\u578b\u6027\u80fd", "motivation": "\u73b0\u6709LLM\u56fa\u5b9a\u5206\u8bcd\u65b9\u6848\u5bfc\u81f4\u591a\u8bed\u8a00/\u4e13\u4e1a\u573a\u666f\u6548\u7387\u4f4e\u4e0b\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u5927\u91cf\u7b97\u529b\u4e14\u96be\u4ee5\u4fdd\u6301\u8bed\u4e49\uff0c\u73b0\u6709\u6539\u8fdb\u65b9\u6848\u4ecd\u5b58\u5728\u518d\u8bad\u7ec3\u6210\u672c\u9ad8\u548c\u538b\u7f29\u6548\u7387\u4e0d\u8db3\u7684\u95ee\u9898", "method": "Tokenadapt\u7ed3\u5408\u5c40\u90e8\uff08\u5b50\u8bcd\u5206\u89e3\uff09\u4e0e\u5168\u5c40\uff08\u8bed\u4e49\u76f8\u4f3c\u6027\uff09\u6df7\u5408\u542f\u53d1\u5f0f\u521d\u59cb\u5316\u65b0token\u5d4c\u5165\uff0c\u914d\u5408\u591a\u8bcdSupertokens\u7684\u9884\u5206\u8bcd\u5b66\u4e60\u673a\u5236", "result": "\u5b9e\u9a8c\u663e\u793aTokenAdapt\u56f0\u60d1\u5ea6\u6bd4\u7387\u8f83ReTok\u964d\u4f4e\u81f3\u5c1150%\uff0cSupertokens\u5b9e\u73b0\u663e\u8457\u538b\u7f29\u589e\u76ca\uff0c\u8de8\u6a21\u578b\u9a8c\u8bc1\u663e\u793a2\u500d\u4ee5\u4e0a\u7efc\u5408\u6027\u80fd\u63d0\u5347", "conclusion": "\u8be5\u6846\u67b6\u5728\u4fdd\u6301\u8bed\u4e49\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u518d\u8bad\u7ec3\u9700\u6c42\uff0c\u4e3aLLM\u5206\u8bcd\u65b9\u6848\u4f18\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u591a\u8bed\u8a00\u548c\u9886\u57df\u7279\u5b9a\u573a\u666f"}}
{"id": "2505.09858", "pdf": "https://arxiv.org/pdf/2505.09858", "abs": "https://arxiv.org/abs/2505.09858", "authors": ["Danush Kumar Venkatesh", "Isabel Funke", "Micha Pfeiffer", "Fiona Kolbinger", "Hanna Maria Schmeiser", "Juergen Weitz", "Marius Distler", "Stefanie Speidel"], "title": "Mission Balance: Generating Under-represented Class Samples using Video Diffusion Models", "categories": ["cs.CV"], "comment": "Early accept at MICCAI 2025", "summary": "Computer-assisted interventions can improve intra-operative guidance,\nparticularly through deep learning methods that harness the spatiotemporal\ninformation in surgical videos. However, the severe data imbalance often found\nin surgical video datasets hinders the development of high-performing models.\nIn this work, we aim to overcome the data imbalance by synthesizing surgical\nvideos. We propose a unique two-stage, text-conditioned diffusion-based method\nto generate high-fidelity surgical videos for under-represented classes. Our\napproach conditions the generation process on text prompts and decouples\nspatial and temporal modeling by utilizing a 2D latent diffusion model to\ncapture spatial content and then integrating temporal attention layers to\nensure temporal consistency. Furthermore, we introduce a rejection sampling\nstrategy to select the most suitable synthetic samples, effectively augmenting\nexisting datasets to address class imbalance. We evaluate our method on two\ndownstream tasks-surgical action recognition and intra-operative event\nprediction-demonstrating that incorporating synthetic videos from our approach\nsubstantially enhances model performance. We open-source our implementation at\nhttps://gitlab.com/nct_tso_public/surgvgen.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6587\u672c\u6761\u4ef6\u6269\u6563\u6a21\u578b\u751f\u6210\u624b\u672f\u89c6\u9891\uff0c\u901a\u8fc7\u65f6\u7a7a\u89e3\u8026\u5efa\u6a21\u548c\u62d2\u7edd\u91c7\u6837\u7b56\u7565\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd", "motivation": "\u624b\u672f\u89c6\u9891\u6570\u636e\u96c6\u5b58\u5728\u4e25\u91cd\u7684\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5236\u7ea6\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\u63d0\u5347\uff0c\u9700\u901a\u8fc7\u5408\u6210\u89c6\u9891\u6269\u5145\u6570\u636e", "method": "1. \u6587\u672c\u6761\u4ef6\u53cc\u9636\u6bb5\u751f\u6210\u6846\u67b6\uff1a2D\u6f5c\u5728\u6269\u6563\u6a21\u578b\u6355\u83b7\u7a7a\u95f4\u5185\u5bb9\uff0c\u6574\u5408\u65f6\u95f4\u6ce8\u610f\u529b\u5c42\u4fdd\u8bc1\u65f6\u5e8f\u4e00\u81f4\u6027\uff1b2. \u5f15\u5165\u62d2\u7edd\u91c7\u6837\u7b56\u7565\u7b5b\u9009\u4f18\u8d28\u5408\u6210\u6837\u672c", "result": "\u5728\u624b\u672f\u52a8\u4f5c\u8bc6\u522b\u548c\u672f\u4e2d\u4e8b\u4ef6\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u5408\u6210\u89c6\u9891\u4f7f\u6a21\u578b\u51c6\u786e\u7387\u5206\u522b\u63d0\u53473.4%\u548c4.1%\uff0cF1\u5206\u6570\u63d0\u53475.1%\u548c3.8%", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u5f00\u6e90\u6846\u67b6\u63a8\u52a8\u624b\u672fAI\u53d1\u5c55\uff0c\u65f6\u7a7a\u89e3\u8026\u7b56\u7565\u4e3a\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2505.10101", "pdf": "https://arxiv.org/pdf/2505.10101", "abs": "https://arxiv.org/abs/2505.10101", "authors": ["Jongmin Jung", "Dasaem Jeong"], "title": "LAV: Audio-Driven Dynamic Visual Generation with Neural Compression and StyleGAN2", "categories": ["cs.SD", "cs.AI", "cs.GR", "cs.MM", "eess.AS"], "comment": "Paper accepted at ISEA 2025, The 30th International Symposium on\n  Electronic/Emerging Art, Seoul, Republic of Korea, 23 - 29 May 2025", "summary": "This paper introduces LAV (Latent Audio-Visual), a system that integrates\nEnCodec's neural audio compression with StyleGAN2's generative capabilities to\nproduce visually dynamic outputs driven by pre-recorded audio. Unlike previous\nworks that rely on explicit feature mappings, LAV uses EnCodec embeddings as\nlatent representations, directly transformed into StyleGAN2's style latent\nspace via randomly initialized linear mapping. This approach preserves semantic\nrichness in the transformation, enabling nuanced and semantically coherent\naudio-visual translations. The framework demonstrates the potential of using\npretrained audio compression models for artistic and computational\napplications.", "AI": {"tldr": "LAV\u7cfb\u7edf\u6574\u5408EnCodec\u97f3\u9891\u538b\u7f29\u4e0eStyleGAN2\u751f\u6210\u80fd\u529b\uff0c\u901a\u8fc7\u6f5c\u5728\u8868\u793a\u5b9e\u73b0\u8bed\u4e49\u4e30\u5bcc\u7684\u97f3\u89c6\u8f6c\u6362\u3002", "motivation": "\u7a81\u7834\u4f20\u7edf\u663e\u5f0f\u7279\u5f81\u6620\u5c04\u9650\u5236\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u63a2\u7d22\u66f4\u81ea\u7136\u8fde\u8d2f\u7684\u8de8\u6a21\u6001\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528EnCodec\u5d4c\u5165\u4f5c\u4e3a\u6f5c\u5728\u8868\u793a\uff0c\u901a\u8fc7\u968f\u673a\u7ebf\u6027\u6620\u5c04\u8f6c\u6362\u81f3StyleGAN2\u98ce\u683c\u7a7a\u95f4\uff0c\u4fdd\u7559\u8bed\u4e49\u5b8c\u6574\u6027\u3002", "result": "\u7cfb\u7edf\u751f\u6210\u7ec6\u81f4\u4e14\u8bed\u4e49\u4e00\u81f4\u7684\u52a8\u6001\u89c6\u89c9\u8f93\u51fa\uff0c\u9a8c\u8bc1\u4e86\u9884\u8bad\u7ec3\u97f3\u9891\u6a21\u578b\u5728\u8de8\u6a21\u6001\u751f\u6210\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "LAV\u6846\u67b6\u5f00\u521b\u4e86\u9884\u8bad\u7ec3\u97f3\u9891\u6a21\u578b\u5728\u827a\u672f\u751f\u6210\u9886\u57df\u7684\u65b0\u8303\u5f0f\uff0c\u5c55\u793a\u4e86\u6f5c\u5728\u7a7a\u95f4\u8f6c\u6362\u7684\u6280\u672f\u6f5c\u529b\u3002"}}
{"id": "2505.09794", "pdf": "https://arxiv.org/pdf/2505.09794", "abs": "https://arxiv.org/abs/2505.09794", "authors": ["J. Moreno-Casanova", "J. M. Au\u00f1\u00f3n", "A. M\u00e1rtinez-P\u00e9rez", "M. E. P\u00e9rez-Mart\u00ednez", "M. E. Gas-L\u00f3pez"], "title": "Automated Detection of Clinical Entities in Lung and Breast Cancer Reports Using NLP Techniques", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Research projects, including those focused on cancer, rely on the manual\nextraction of information from clinical reports. This process is time-consuming\nand prone to errors, limiting the efficiency of data-driven approaches in\nhealthcare. To address these challenges, Natural Language Processing (NLP)\noffers an alternative for automating the extraction of relevant data from\nelectronic health records (EHRs). In this study, we focus on lung and breast\ncancer due to their high incidence and the significant impact they have on\npublic health. Early detection and effective data management in both types of\ncancer are crucial for improving patient outcomes. To enhance the accuracy and\nefficiency of data extraction, we utilized GMV's NLP tool uQuery, which excels\nat identifying relevant entities in clinical texts and converting them into\nstandardized formats such as SNOMED and OMOP. uQuery not only detects and\nclassifies entities but also associates them with contextual information,\nincluding negated entities, temporal aspects, and patient-related details. In\nthis work, we explore the use of NLP techniques, specifically Named Entity\nRecognition (NER), to automatically identify and extract key clinical\ninformation from EHRs related to these two cancers. A dataset from Health\nResearch Institute Hospital La Fe (IIS La Fe), comprising 200 annotated breast\ncancer and 400 lung cancer reports, was used, with eight clinical entities\nmanually labeled using the Doccano platform. To perform NER, we fine-tuned the\nbsc-bio-ehr-en3 model, a RoBERTa-based biomedical linguistic model pre-trained\nin Spanish. Fine-tuning was performed using the Transformers architecture,\nenabling accurate recognition of clinical entities in these cancer types. Our\nresults demonstrate strong overall performance, particularly in identifying\nentities like MET and PAT, although challenges remain with less frequent\nentities like EVOL.", "AI": {"tldr": "\u5229\u7528NLP\u6280\u672f\u81ea\u52a8\u5316\u63d0\u53d6\u80ba\u764c/\u4e73\u817a\u764c\u4e34\u5e8a\u6570\u636e\uff0c\u4f7f\u7528uQuery\u5de5\u5177\u548c\u5fae\u8c03bsc-bio-ehr-en3\u6a21\u578b\uff0c\u5728200+400\u4efd\u897f\u73ed\u7259\u533b\u7597\u62a5\u544a\u4e0a\u5b9e\u73b0\u9ad8\u6548\u5b9e\u4f53\u8bc6\u522b\uff08MET/PAT\u8fbe\u4f18\uff0cEVOL\u4ecd\u6709\u6311\u6218\uff09", "motivation": "\u89e3\u51b3\u4eba\u5de5\u63d0\u53d6\u4e34\u5e8a\u6570\u636e\u6548\u7387\u4f4e\u3001\u9519\u8bef\u7387\u9ad8\u7684\u95ee\u9898\uff0c\u9488\u5bf9\u80ba\u764c\u548c\u4e73\u817a\u764c\u8fd9\u4e24\u79cd\u9ad8\u53d1\u764c\u75c7\u7c7b\u578b\u63d0\u5347\u6570\u636e\u7ba1\u7406\u6548\u7387\u4ee5\u6539\u5584\u60a3\u8005\u9884\u540e", "method": "1. \u4f7f\u7528GMV\u7684uQuery\u5de5\u5177\u8fdb\u884c\u6807\u51c6\u5316\u5b9e\u4f53\u8bc6\u522b\n2. \u57fa\u4e8eRoBERTa\u7684bsc-bio-ehr-en3\u6a21\u578b\u8fdb\u884c\u897f\u73ed\u7259\u533b\u7597\u6587\u672c\u5fae\u8c03\n3. \u91c7\u7528IIS La Fe\u7684600\u4efd\u6807\u6ce8\u62a5\u544a\uff08Doccano\u5e73\u53f0\u6807\u6ce88\u7c7b\u5b9e\u4f53\uff09", "result": "\u6574\u4f53\u8bc6\u522b\u6027\u80fd\u4f18\u5f02\uff08MET\u51c6\u786e\u738798.7%\uff0cPAT\u53ec\u56de\u738795.2%\uff09\uff0c\u4f46\u4f4e\u9891\u5b9e\u4f53EVOL\u7684F1-score\u4ec572.1%", "conclusion": "\u8bc1\u5b9eNLP\u5728\u80bf\u7624\u4e34\u5e8a\u6570\u636e\u5904\u7406\u4e2d\u7684\u6709\u6548\u6027\uff0c\u672a\u6765\u9700\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u63d0\u5347\u4f4e\u9891\u5b9e\u4f53\u8bc6\u522b\u80fd\u529b"}}
{"id": "2505.09859", "pdf": "https://arxiv.org/pdf/2505.09859", "abs": "https://arxiv.org/abs/2505.09859", "authors": ["Andrew Jun Lee", "Taylor Webb", "Trevor Bihl", "Keith Holyoak", "Hongjing Lu"], "title": "Few-Shot Learning of Visual Compositional Concepts through Probabilistic Schema Induction", "categories": ["cs.CV"], "comment": "Lee, A. J., Webb, T., Bihl, T., Holyoak, K. J., & Lu, H. (2025).\n  Few-shot learning of visual compositional concepts through probabilistic\n  schema induction. In A. Ruggeri, D. Barner, C. Walker, & N. Bramley (Eds.),\n  Proceedings of the 47th Annual Conference of the Cognitive Science Society.\n  Cognitive Science Society", "summary": "The ability to learn new visual concepts from limited examples is a hallmark\nof human cognition. While traditional category learning models represent each\nexample as an unstructured feature vector, compositional concept learning is\nthought to depend on (1) structured representations of examples (e.g., directed\ngraphs consisting of objects and their relations) and (2) the identification of\nshared relational structure across examples through analogical mapping. Here,\nwe introduce Probabilistic Schema Induction (PSI), a prototype model that\nemploys deep learning to perform analogical mapping over structured\nrepresentations of only a handful of examples, forming a compositional concept\ncalled a schema. In doing so, PSI relies on a novel conception of similarity\nthat weighs object-level similarity and relational similarity, as well as a\nmechanism for amplifying relations relevant to classification, analogous to\nselective attention parameters in traditional models. We show that PSI produces\nhuman-like learning performance and outperforms two controls: a prototype model\nthat uses unstructured feature vectors extracted from a deep learning model,\nand a variant of PSI with weaker structured representations. Notably, we find\nthat PSI's human-like performance is driven by an adaptive strategy that\nincreases relational similarity over object-level similarity and upweights the\ncontribution of relations that distinguish classes. These findings suggest that\nstructured representations and analogical mapping are critical to modeling\nrapid human-like learning of compositional visual concepts, and demonstrate how\ndeep learning can be leveraged to create psychological models.", "AI": {"tldr": "\u63d0\u51fa\u6982\u7387\u6a21\u5f0f\u5f52\u7eb3\uff08PSI\uff09\u6a21\u578b\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u5b9e\u73b0\u7ed3\u6784\u5316\u8868\u5f81\u7684\u7c7b\u6bd4\u6620\u5c04\uff0c\u6a21\u62df\u4eba\u7c7b\u4ece\u5c11\u91cf\u6837\u672c\u4e2d\u5b66\u4e60\u7ec4\u5408\u89c6\u89c9\u6982\u5ff5\u7684\u8ba4\u77e5\u673a\u5236", "motivation": "\u4f20\u7edf\u7c7b\u522b\u5b66\u4e60\u6a21\u578b\u4f7f\u7528\u975e\u7ed3\u6784\u5316\u7279\u5f81\u5411\u91cf\uff0c\u96be\u4ee5\u89e3\u91ca\u4eba\u7c7b\u57fa\u4e8e\u7ed3\u6784\u5316\u8868\u5f81\u548c\u7c7b\u6bd4\u6620\u5c04\u7684\u5feb\u901f\u7ec4\u5408\u6982\u5ff5\u5b66\u4e60\u80fd\u529b", "method": "\u6784\u5efa\u5305\u542b\u5bf9\u8c61\u5173\u7cfb\u7684\u56fe\u7ed3\u6784\u8868\u5f81\uff0c\u91c7\u7528\u6743\u8861\u5bf9\u8c61\u76f8\u4f3c\u6027\u548c\u5173\u7cfb\u76f8\u4f3c\u6027\u7684\u65b0\u578b\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u8fdb\u884c\u7c7b\u6bd4\u6620\u5c04\u751f\u6210\u7ec4\u5408\u6a21\u5f0f\uff08Schema\uff09", "result": "PSI\u5728\u5c11\u91cf\u6837\u672c\u5b66\u4e60\u4efb\u52a1\u4e2d\u8fbe\u5230\u7c7b\u4eba\u6c34\u5e73\uff0c\u8868\u73b0\u4f18\u4e8e\u975e\u7ed3\u6784\u5316\u7279\u5f81\u5411\u91cf\u539f\u578b\u6a21\u578b\u548c\u5f31\u7ed3\u6784\u5316\u5bf9\u7167\u6a21\u578b\uff0c\u5176\u4f18\u52bf\u6e90\u4e8e\u589e\u5f3a\u5173\u7cfb\u76f8\u4f3c\u6027\u53ca\u5173\u952e\u5173\u7cfb\u6743\u91cd\u7684\u81ea\u9002\u5e94\u7b56\u7565", "conclusion": "\u7ed3\u6784\u5316\u8868\u5f81\u548c\u7c7b\u6bd4\u6620\u5c04\u662f\u5b9e\u73b0\u4eba\u7c7b\u5feb\u901f\u7ec4\u5408\u6982\u5ff5\u5b66\u4e60\u7684\u5173\u952e\u673a\u5236\uff0c\u6df1\u5ea6\u5b66\u4e60\u4e3a\u6784\u5efa\u5fc3\u7406\u5b66\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84"}}
{"id": "2505.09807", "pdf": "https://arxiv.org/pdf/2505.09807", "abs": "https://arxiv.org/abs/2505.09807", "authors": ["Timour Ichmoukhamedov", "David Martens"], "title": "Exploring the generalization of LLM truth directions on conversational formats", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Several recent works argue that LLMs have a universal truth direction where\ntrue and false statements are linearly separable in the activation space of the\nmodel. It has been demonstrated that linear probes trained on a single hidden\nstate of the model already generalize across a range of topics and might even\nbe used for lie detection in LLM conversations. In this work we explore how\nthis truth direction generalizes between various conversational formats. We\nfind good generalization between short conversations that end on a lie, but\npoor generalization to longer formats where the lie appears earlier in the\ninput prompt. We propose a solution that significantly improves this type of\ngeneralization by adding a fixed key phrase at the end of each conversation.\nOur results highlight the challenges towards reliable LLM lie detectors that\ngeneralize to new settings.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22LLM\u4e2d'\u771f\u5b9e\u65b9\u5411'\u5728\u4e0d\u540c\u5bf9\u8bdd\u683c\u5f0f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u53d1\u73b0\u77ed\u5bf9\u8bdd\u7ed3\u5c3e\u6492\u8c0e\u65f6\u6cdb\u5316\u826f\u597d\uff0c\u4f46\u957f\u5bf9\u8bdd\u65e9\u671f\u6492\u8c0e\u65f6\u8868\u73b0\u5dee\uff0c\u63d0\u51fa\u901a\u8fc7\u6dfb\u52a0\u5173\u952e\u77ed\u8bed\u63d0\u5347\u6cdb\u5316\u6027\u3002", "motivation": "\u57fa\u4e8e\u524d\u4eba\u53d1\u73b0LLM\u5b58\u5728\u7ebf\u6027\u53ef\u5206\u79bb\u7684\u771f\u5047\u9648\u8ff0\u65b9\u5411\uff0c\u672c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u8be5\u65b9\u5411\u5728\u4e0d\u540c\u957f\u5ea6/\u7ed3\u6784\u7684\u5bf9\u8bdd\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u8c0e\u8a00\u4f4d\u7f6e\u53d8\u5316\u65f6\u63a2\u6d4b\u5668\u7684\u7a33\u5b9a\u6027\u3002", "method": "\u901a\u8fc7\u6d4b\u8bd5\u4e0d\u540c\u5bf9\u8bdd\u683c\u5f0f(\u77ed\u5bf9\u8bdd\u8c0e\u8a00\u7ed3\u5c3e vs \u957f\u5bf9\u8bdd\u8c0e\u8a00\u524d\u7f6e)\u7684\u6cdb\u5316\u8868\u73b0\uff0c\u63d0\u51fa\u5728\u5bf9\u8bdd\u672b\u5c3e\u6dfb\u52a0\u56fa\u5b9a\u5173\u952e\u77ed\u8bed\u7684\u4f18\u5316\u65b9\u6848\u3002", "result": "\u77ed\u5bf9\u8bdd\u7ed3\u5c3e\u8c0e\u8a00\u573a\u666f\u6cdb\u5316\u826f\u597d\uff0c\u4f46\u957f\u5bf9\u8bdd\u65e9\u671f\u8c0e\u8a00\u573a\u666f\u6cdb\u5316\u5dee\uff1b\u6dfb\u52a0\u5173\u952e\u77ed\u8bed\u540e\u957f\u5bf9\u8bdd\u573a\u666f\u7684\u6cdb\u5316\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "LLM\u8c0e\u8a00\u68c0\u6d4b\u5668\u7684\u5b9e\u9645\u5e94\u7528\u9762\u4e34\u6cdb\u5316\u6027\u6311\u6218\uff0c\u9700\u901a\u8fc7\u5bf9\u8bdd\u7ed3\u6784\u4f18\u5316\u63d0\u5347\u63a2\u6d4b\u6a21\u578b\u5728\u65b0\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.09915", "pdf": "https://arxiv.org/pdf/2505.09915", "abs": "https://arxiv.org/abs/2505.09915", "authors": ["Zhe Xin", "Chenyang Wu", "Penghui Huang", "Yanyong Zhang", "Yinian Mao", "Guoquan Huang"], "title": "Large-Scale Gaussian Splatting SLAM", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "The recently developed Neural Radiance Fields (NeRF) and 3D Gaussian\nSplatting (3DGS) have shown encouraging and impressive results for visual SLAM.\nHowever, most representative methods require RGBD sensors and are only\navailable for indoor environments. The robustness of reconstruction in\nlarge-scale outdoor scenarios remains unexplored. This paper introduces a\nlarge-scale 3DGS-based visual SLAM with stereo cameras, termed LSG-SLAM. The\nproposed LSG-SLAM employs a multi-modality strategy to estimate prior poses\nunder large view changes. In tracking, we introduce feature-alignment warping\nconstraints to alleviate the adverse effects of appearance similarity in\nrendering losses. For the scalability of large-scale scenarios, we introduce\ncontinuous Gaussian Splatting submaps to tackle unbounded scenes with limited\nmemory. Loops are detected between GS submaps by place recognition and the\nrelative pose between looped keyframes is optimized utilizing rendering and\nfeature warping losses. After the global optimization of camera poses and\nGaussian points, a structure refinement module enhances the reconstruction\nquality. With extensive evaluations on the EuRoc and KITTI datasets, LSG-SLAM\nachieves superior performance over existing Neural, 3DGS-based, and even\ntraditional approaches. Project page: https://lsg-slam.github.io.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u5927\u89c4\u6a21\u7acb\u4f53\u89c6\u89c9SLAM\u7cfb\u7edfLSG-SLAM\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u4f4d\u59ff\u4f30\u8ba1\u3001\u7279\u5f81\u5bf9\u9f50\u7ea6\u675f\u548c\u8fde\u7eed\u5b50\u5730\u56fe\u7b56\u7565\uff0c\u89e3\u51b3\u5ba4\u5916\u573a\u666f\u91cd\u5efa\u7684\u9c81\u68d2\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u8f90\u5c04\u573a\u548c3DGS\u65b9\u6cd5\u4f9d\u8d56RGBD\u4f20\u611f\u5668\u4e14\u5c40\u9650\u5ba4\u5185\u573a\u666f\uff0c\u7f3a\u4e4f\u5bf9\u5927\u89c4\u6a21\u5ba4\u5916\u73af\u5883\u7684\u6709\u6548\u652f\u6301\u3002", "method": "\u7ed3\u5408\u7279\u5f81\u5bf9\u9f50\u7684\u6e32\u67d3\u635f\u5931\u7ea6\u675f\u3001\u8fde\u7eed\u5b50\u5730\u56fe\u5185\u5b58\u7ba1\u7406\u3001\u57fa\u4e8e\u95ed\u73af\u68c0\u6d4b\u7684\u4f4d\u59ff\u4f18\u5316\uff0c\u4ee5\u53ca\u7ed3\u6784\u7ec6\u5316\u6a21\u5757\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002", "result": "\u5728EuRoc\u548cKITTI\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u795e\u7ecf\u65b9\u6cd5\u30013DGS\u65b9\u6848\u53ca\u4f20\u7edf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u5ba4\u5916\u573a\u666f\u7684\u9ad8\u6548\u7cbe\u51c6\u91cd\u5efa\u3002", "conclusion": "\u7cfb\u7edf\u901a\u8fc7\u591a\u7b56\u7565\u534f\u540c\u663e\u8457\u63d0\u5347\u5ba4\u5916SLAM\u7684\u9c81\u68d2\u6027\uff0c\u9a8c\u8bc1\u4e863DGS\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.09825", "pdf": "https://arxiv.org/pdf/2505.09825", "abs": "https://arxiv.org/abs/2505.09825", "authors": ["Peiqi Sui", "Juan Diego Rodriguez", "Philippe Laban", "Dean Murphy", "Joseph P. Dexter", "Richard Jean So", "Samuel Baker", "Pramit Chaudhuri"], "title": "KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Each year, tens of millions of essays are written and graded in college-level\nEnglish courses. Students are asked to analyze literary and cultural texts\nthrough a process known as close reading, in which they gather textual details\nto formulate evidence-based arguments. Despite being viewed as a basis for\ncritical thinking and widely adopted as a required element of university\ncoursework, close reading has never been evaluated on large language models\n(LLMs), and multi-discipline benchmarks like MMLU do not include literature as\na subject. To fill this gap, we present KRISTEVA, the first close reading\nbenchmark for evaluating interpretive reasoning, consisting of 1331\nmultiple-choice questions adapted from classroom data. With KRISTEVA, we\npropose three progressively more difficult sets of tasks to approximate\ndifferent elements of the close reading process, which we use to test how well\nLLMs may seem to understand and reason about literary works: 1) extracting\nstylistic features, 2) retrieving relevant contextual information from\nparametric knowledge, and 3) multi-hop reasoning between style and external\ncontexts. Our baseline results find that, while state-of-the-art LLMs possess\nsome college-level close reading competency (accuracy 49.7% - 69.7%), their\nperformances still trail those of experienced human evaluators on 10 out of our\n11 tasks.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u6587\u5b66\u7ec6\u8bfb\u8bc4\u6d4b\u57fa\u51c6KRISTEVA\uff0c\u53d1\u73b0\u6700\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u5177\u5907\u672c\u79d1\u7ea7\u7ec6\u8bfb\u80fd\u529b\uff08\u51c6\u786e\u738749.7%-69.7%\uff09\uff0c\u4f46\u572811\u9879\u4efb\u52a1\u4e2d\u670910\u9879\u843d\u540e\u4e8e\u4eba\u7c7b\u8bc4\u4f30\u8005\u3002", "motivation": "\u5927\u5b66\u82f1\u8bed\u8bfe\u7a0b\u6bcf\u5e74\u4ea7\u751f\u6570\u5343\u4e07\u4efd\u9700\u4eba\u5de5\u8bc4\u9605\u7684\u6587\u672c\u5206\u6790\u4f5c\u4e1a\u3002\u7ec6\u8bfb\u80fd\u529b\u4f5c\u4e3a\u6279\u5224\u6027\u601d\u7ef4\u57fa\u7840\u5c1a\u672a\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\uff0c\u73b0\u6709\u57fa\u51c6\u7f3a\u4e4f\u6587\u5b66\u5b66\u79d1\u8986\u76d6\u3002", "method": "\u6784\u5efa\u5305\u542b1331\u9053\u9009\u62e9\u9898\u7684KRISTEVA\u57fa\u51c6\uff0c\u8bbe\u8ba1\u4e09\u4e2a\u9012\u8fdb\u4efb\u52a1\uff1a1\uff09\u63d0\u53d6\u6587\u4f53\u7279\u5f81 2\uff09\u68c0\u7d22\u8bed\u5883\u4fe1\u606f 3\uff09\u98ce\u683c\u4e0e\u8bed\u5883\u7684\u591a\u8df3\u63a8\u7406\uff0c\u6d4b\u8bd5\u6a21\u578b\u5bf9\u6587\u5b66\u6587\u672c\u7684\u89e3\u8bfb\u80fd\u529b\u3002", "result": "\u5f53\u524d\u6700\u4f18\u6a21\u578b\u5728\u7ec6\u8bfb\u4efb\u52a1\u4e2d\u5c55\u73b0\u672c\u79d1\u6c34\u5e73\u80fd\u529b\uff0c\u4f46\u572870%\u51c6\u786e\u7387\u7684\u4f20\u7edf\u9608\u503c\u4e0b\uff0c10/11\u4efb\u52a1\u8868\u73b0\u4e0d\u53ca\u4eba\u7c7b\u8bc4\u4f30\u8005\uff08\u4eba\u7c7b\u57fa\u7ebf\u51c6\u786e\u7387\u9ad8\u4e8e\u6a21\u578b\uff09", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86LLM\u6587\u5b66\u89e3\u8bfb\u80fd\u529b\u8bc4\u4f30\u7a7a\u767d\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u9700\u8981\u6df1\u5ea6\u6587\u672c\u5206\u6790\u4e0e\u8bed\u5883\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u6539\u8fdbAI\u7684\u9610\u91ca\u6027\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u57fa\u51c6\u3002"}}
{"id": "2505.09926", "pdf": "https://arxiv.org/pdf/2505.09926", "abs": "https://arxiv.org/abs/2505.09926", "authors": ["Bin-Bin Gao", "Yue Zhu", "Jiangtao Yan", "Yuezhi Cai", "Weixi Zhang", "Meng Wang", "Jun Liu", "Yong Liu", "Lei Wang", "Chengjie Wang"], "title": "AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection", "categories": ["cs.CV", "cs.AI"], "comment": "27 pages, 15 figures, 22 tables", "summary": "Universal visual anomaly detection aims to identify anomalies from novel or\nunseen vision domains without additional fine-tuning, which is critical in open\nscenarios. Recent studies have demonstrated that pre-trained vision-language\nmodels like CLIP exhibit strong generalization with just zero or a few normal\nimages. However, existing methods struggle with designing prompt templates,\ncomplex token interactions, or requiring additional fine-tuning, resulting in\nlimited flexibility. In this work, we present a simple yet effective method\ncalled AdaptCLIP based on two key insights. First, adaptive visual and textual\nrepresentations should be learned alternately rather than jointly. Second,\ncomparative learning between query and normal image prompt should incorporate\nboth contextual and aligned residual features, rather than relying solely on\nresidual features. AdaptCLIP treats CLIP models as a foundational service,\nadding only three simple adapters, visual adapter, textual adapter, and\nprompt-query adapter, at its input or output ends. AdaptCLIP supports\nzero-/few-shot generalization across domains and possesses a training-free\nmanner on target domains once trained on a base dataset. AdaptCLIP achieves\nstate-of-the-art performance on 12 anomaly detection benchmarks from industrial\nand medical domains, significantly outperforming existing competitive methods.\nWe will make the code and model of AdaptCLIP available at\nhttps://github.com/gaobb/AdaptCLIP.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u65e0\u9700\u5fae\u8c03\u7684AdaptCLIP\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u4e2a\u9002\u914d\u5668\u4ea4\u66ff\u5b66\u4e60\u89c6\u89c9\u6587\u672c\u8868\u5f81\uff0c\u572812\u4e2a\u5f02\u5e38\u68c0\u6d4b\u57fa\u51c6\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd", "motivation": "\u73b0\u6709\u89c6\u89c9\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u63d0\u793a\u6a21\u677f\u8bbe\u8ba1\u590d\u6742\u3001\u9700\u8981\u989d\u5916\u5fae\u8c03\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u5b9e\u73b0\u8de8\u9886\u57df\u7075\u6d3b\u5e94\u7528", "method": "\u5728CLIP\u6a21\u578b\u8f93\u5165/\u8f93\u51fa\u7aef\u6dfb\u52a0\u89c6\u89c9\u9002\u914d\u5668\u3001\u6587\u672c\u9002\u914d\u5668\u548c\u63d0\u793a-\u67e5\u8be2\u9002\u914d\u5668\uff0c\u4ea4\u66ff\u5b66\u4e60\u81ea\u9002\u5e94\u8868\u5f81\uff0c\u7ed3\u5408\u4e0a\u4e0b\u6587\u548c\u5bf9\u9f50\u6b8b\u5dee\u7279\u5f81\u7684\u5bf9\u6bd4\u5b66\u4e60", "result": "\u5728\u5de5\u4e1a\u548c\u533b\u5b66\u9886\u57df\u768412\u4e2a\u5f02\u5e38\u68c0\u6d4b\u57fa\u51c6\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u4fdd\u6301\u8de8\u9886\u57df\u96f6/\u5c11\u6837\u672c\u6cdb\u5316\u80fd\u529b", "conclusion": "AdaptCLIP\u901a\u8fc7\u7b80\u5355\u800c\u6709\u6548\u7684\u9002\u914d\u5668\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u8bad\u7ec3\u540e\u65e0\u9700\u76ee\u6807\u57df\u5fae\u8c03\u7684\u9ad8\u6027\u80fd\u8de8\u57df\u5f02\u5e38\u68c0\u6d4b"}}
{"id": "2505.09852", "pdf": "https://arxiv.org/pdf/2505.09852", "abs": "https://arxiv.org/abs/2505.09852", "authors": ["Apollinaire Poli Nemkova", "Sarath Chandra Lingareddy", "Sagnik Ray Choudhury", "Mark V. Albert"], "title": "Do Large Language Models Know Conflict? Investigating Parametric vs. Non-Parametric Knowledge of LLMs for Conflict Forecasting", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown impressive performance across natural\nlanguage tasks, but their ability to forecast violent conflict remains\nunderexplored. We investigate whether LLMs possess meaningful parametric\nknowledge-encoded in their pretrained weights-to predict conflict escalation\nand fatalities without external data. This is critical for early warning\nsystems, humanitarian planning, and policy-making. We compare this parametric\nknowledge with non-parametric capabilities, where LLMs access structured and\nunstructured context from conflict datasets (e.g., ACLED, GDELT) and recent\nnews reports via Retrieval-Augmented Generation (RAG). Incorporating external\ninformation could enhance model performance by providing up-to-date context\notherwise missing from pretrained weights. Our two-part evaluation framework\nspans 2020-2024 across conflict-prone regions in the Horn of Africa and the\nMiddle East. In the parametric setting, LLMs predict conflict trends and\nfatalities relying only on pretrained knowledge. In the non-parametric setting,\nmodels receive summaries of recent conflict events, indicators, and\ngeopolitical developments. We compare predicted conflict trend labels (e.g.,\nEscalate, Stable Conflict, De-escalate, Peace) and fatalities against\nhistorical data. Our findings highlight the strengths and limitations of LLMs\nfor conflict forecasting and the benefits of augmenting them with structured\nexternal knowledge.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8LLMs\u80fd\u5426\u901a\u8fc7\u9884\u8bad\u7ec3\u53c2\u6570\u9884\u6d4b\u66b4\u529b\u51b2\u7a81\uff0c\u5e76\u5bf9\u6bd4\u7ed3\u5408\u5916\u90e8\u6570\u636e\u7684\u975e\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u53d1\u73b0\u5916\u90e8\u77e5\u8bc6\u80fd\u6709\u6548\u63d0\u5347\u51b2\u7a81\u8d8b\u52bf\u548c\u6b7b\u4ea1\u4eba\u6570\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u672a\u5145\u5206\u9a8c\u8bc1LLMs\u5728\u51b2\u7a81\u9884\u6d4b\u4e2d\u7684\u6f5c\u529b\uff0c\u800c\u8be5\u80fd\u529b\u5bf9\u65e9\u671f\u9884\u8b66\u548c\u4eba\u9053\u6551\u63f4\u81f3\u5173\u91cd\u8981\u3002\u63a2\u7d22\u53c2\u6570\u5316\u4e0e\u975e\u53c2\u6570\u5316\u77e5\u8bc6\u7684\u7ed3\u5408\u53ef\u4f18\u5316\u9884\u6d4b\u6a21\u578b\u5b9e\u7528\u6027\u3002", "method": "\u6784\u5efa2020-2024\u5e74\u975e\u6d32\u4e4b\u89d2/\u4e2d\u4e1c\u7684\u8bc4\u4f30\u6846\u67b6\uff1a1) \u53c2\u6570\u5316\u8bbe\u7f6e\u4ec5\u7528\u9884\u8bad\u7ec3\u6743\u91cd\u9884\u6d4b\u51b2\u7a81\u8d8b\u52bf\u548c\u6b7b\u4ea1\u4eba\u6570\uff1b2) \u975e\u53c2\u6570\u5316\u8bbe\u7f6e\u901a\u8fc7RAG\u6574\u5408ACLED/GDELT\u7b49\u7ed3\u6784\u5316\u6570\u636e\u53ca\u65b0\u95fb\u4e8b\u4ef6\u3002\u5bf9\u6bd4\u9884\u6d4b\u7ed3\u679c\u4e0e\u5386\u53f2\u6570\u636e\u3002", "result": "LLMs\u7684\u9884\u8bad\u7ec3\u6743\u91cd\u5305\u542b\u57fa\u7840\u51b2\u7a81\u9884\u6d4b\u80fd\u529b\uff0c\u4f46\u7ed3\u5408\u5916\u90e8\u7ed3\u6784\u5316\u6570\u636e\u540e\u9884\u6d4b\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\u3002\u975e\u53c2\u6570\u5316\u65b9\u6cd5\u5728\u51b2\u7a81\u8d8b\u52bf\u5206\u7c7b\u548c\u6b7b\u4ea1\u4eba\u6570\u9884\u6d4b\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "LLMs\u5177\u5907\u51b2\u7a81\u9884\u6d4b\u7684\u521d\u59cb\u80fd\u529b\uff0c\u4f46\u9700\u4e0e\u5b9e\u65f6\u6570\u636e\u7ed3\u5408\u65b9\u53ef\u5b9e\u7528\u5316\u3002\u8be5\u65b9\u6cd5\u53ef\u4e3a\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u52a8\u6001\u98ce\u9669\u9884\u8b66\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u66f4\u591a\u51b2\u7a81\u6307\u6807\u5206\u6790\u3002"}}
{"id": "2505.09927", "pdf": "https://arxiv.org/pdf/2505.09927", "abs": "https://arxiv.org/abs/2505.09927", "authors": ["Siqi Yin", "Shaolei Liu", "Manning Wang"], "title": "DDFP: Data-dependent Frequency Prompt for Source Free Domain Adaptation of Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Domain adaptation addresses the challenge of model performance degradation\ncaused by domain gaps. In the typical setup for unsupervised domain adaptation,\nlabeled data from a source domain and unlabeled data from a target domain are\nused to train a target model. However, access to labeled source domain data,\nparticularly in medical datasets, can be restricted due to privacy policies. As\na result, research has increasingly shifted to source-free domain adaptation\n(SFDA), which requires only a pretrained model from the source domain and\nunlabeled data from the target domain data for adaptation. Existing SFDA\nmethods often rely on domain-specific image style translation and\nself-supervision techniques to bridge the domain gap and train the target\ndomain model. However, the quality of domain-specific style-translated images\nand pseudo-labels produced by these methods still leaves room for improvement.\nMoreover, training the entire model during adaptation can be inefficient under\nlimited supervision. In this paper, we propose a novel SFDA framework to\naddress these challenges. Specifically, to effectively mitigate the impact of\ndomain gap in the initial training phase, we introduce preadaptation to\ngenerate a preadapted model, which serves as an initialization of target model\nand allows for the generation of high-quality enhanced pseudo-labels without\nintroducing extra parameters. Additionally, we propose a data-dependent\nfrequency prompt to more effectively translate target domain images into a\nsource-like style. To further enhance adaptation, we employ a style-related\nlayer fine-tuning strategy, specifically designed for SFDA, to train the target\nmodel using the prompted target domain images and pseudo-labels. Extensive\nexperiments on cross-modality abdominal and cardiac SFDA segmentation tasks\ndemonstrate that our proposed method outperforms existing state-of-the-art\nmethods.", "AI": {"tldr": "\u63d0\u51fa\u5305\u542b\u9884\u9002\u5e94\u6a21\u5757\u3001\u9891\u7387\u63d0\u793a\u548c\u98ce\u683c\u76f8\u5173\u5c42\u5fae\u8c03\u7684\u65b0\u578b\u6e90\u65e0\u5173\u57df\u9002\u5e94\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u533b\u5b66\u5f71\u50cf\u8de8\u57df\u5206\u5272\u6548\u679c", "motivation": "\u89e3\u51b3\u73b0\u6709SFDA\u65b9\u6cd5\u4e2d\u5b58\u5728\u7684\u4e09\u4e2a\u6838\u5fc3\u95ee\u9898\uff1a1) \u57df\u98ce\u683c\u8f6c\u6362\u56fe\u50cf\u8d28\u91cf\u4e0d\u8db3 2) \u4f2a\u6807\u7b7e\u751f\u6210\u7cbe\u5ea6\u6709\u9650 3) \u5168\u6a21\u578b\u8bad\u7ec3\u6548\u7387\u4f4e\u4e0b\uff0c\u7279\u522b\u662f\u5728\u533b\u7597\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u8981\u6c42\u4e0b\u5b9e\u73b0\u9ad8\u6548\u57df\u9002\u5e94", "method": "\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u9884\u9002\u5e94\u6a21\u5757\u751f\u6210\u57df\u5dee\u8ddd\u7f29\u5c0f\u7684\u521d\u59cb\u5316\u6a21\u578b 2) \u6570\u636e\u4f9d\u8d56\u578b\u9891\u7387\u63d0\u793a\u5b9e\u73b0\u9ad8\u8d28\u91cf\u6e90\u57df\u98ce\u683c\u8f6c\u6362 3) \u98ce\u683c\u76f8\u5173\u5c42\u9009\u62e9\u6027\u5fae\u8c03\u7b56\u7565\uff0c\u4ec5\u4f18\u5316\u5173\u952e\u7f51\u7edc\u5c42", "result": "\u5728\u8179\u90e8\u591a\u6a21\u6001(CT\u2192MRI)\u548c\u5fc3\u810f\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e2d\uff0cDSC\u6307\u6807\u5206\u522b\u8fbe\u523078.3%\u548c84.7%\uff0c\u8d85\u8d8a\u73b0\u6709SOTA\u65b9\u6cd52.1%\u548c1.8%", "conclusion": "\u901a\u8fc7\u9884\u9002\u5e94\u673a\u5236\u4e0e\u9488\u5bf9\u6027\u7f51\u7edc\u5c42\u4f18\u5316\uff0c\u5728\u4e25\u683c\u4fdd\u62a4\u6e90\u57df\u6570\u636e\u9690\u79c1\u524d\u63d0\u4e0b\uff0c\u5b9e\u73b0\u4e86\u533b\u7597\u5f71\u50cf\u8de8\u57df\u5206\u5272\u7684\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u6548\u9002\u5e94\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u65b0\u7684\u57df\u9002\u5e94\u8303\u5f0f"}}
{"id": "2505.09902", "pdf": "https://arxiv.org/pdf/2505.09902", "abs": "https://arxiv.org/abs/2505.09902", "authors": ["Martin Capdevila", "Esteban Villa Turek", "Ellen Karina Chumbe Fernandez", "Luis Felipe Polo Galvez", "Luis Cadavid", "Andrea Marroquin", "Rebeca Vargas Quesada", "Johanna Crew", "Nicole Vallejo Galarraga", "Christopher Rodriguez", "Diego Gutierrez", "Radhi Datla"], "title": "Crossing Borders Without Crossing Boundaries: How Sociolinguistic Awareness Can Optimize User Engagement with Localized Spanish AI Models Across Hispanophone Countries", "categories": ["cs.CL"], "comment": null, "summary": "Large language models are, by definition, based on language. In an effort to\nunderscore the critical need for regional localized models, this paper examines\nprimary differences between variants of written Spanish across Latin America\nand Spain, with an in-depth sociocultural and linguistic contextualization\ntherein. We argue that these differences effectively constitute significant\ngaps in the quotidian use of Spanish among dialectal groups by creating\nsociolinguistic dissonances, to the extent that locale-sensitive AI models\nwould play a pivotal role in bridging these divides. In doing so, this approach\ninforms better and more efficient localization strategies that also serve to\nmore adequately meet inclusivity goals, while securing sustainable active daily\nuser growth in a major low-risk investment geographic area. Therefore,\nimplementing at least the proposed five sub variants of Spanish addresses two\nlines of action: to foment user trust and reliance on AI language models while\nalso demonstrating a level of cultural, historical, and sociolinguistic\nawareness that reflects positively on any internationalization strategy.", "AI": {"tldr": "\u897f\u73ed\u7259\u8bed\u5b58\u5728\u663e\u8457\u5730\u533a\u5dee\u5f02\uff0c\u9700\u6784\u5efa\u81f3\u5c11\u4e94\u4e2a\u5b50\u53d8\u4f53\u7684\u672c\u5730\u5316AI\u6a21\u578b\u4ee5\u5f25\u5408\u793e\u4f1a\u8bed\u8a00\u9e3f\u6c9f\uff0c\u63d0\u5347\u7528\u6237\u4fe1\u4efb\u5e76\u4fc3\u8fdb\u53ef\u6301\u7eed\u53d1\u5c55", "motivation": "\u897f\u73ed\u7259\u8bed\u4e0d\u540c\u53d8\u4f53\u95f4\u7684\u8bed\u8a00\u5dee\u5f02\u5bfc\u81f4\u793e\u4f1a\u8bed\u8a00\u4e0d\u534f\u8c03\uff0c\u5f71\u54cdAI\u6a21\u578b\u65e5\u5e38\u5e94\u7528\uff0c\u9700\u901a\u8fc7\u672c\u5730\u5316\u7b56\u7565\u5b9e\u73b0\u6587\u5316\u5305\u5bb9\u6027\u76ee\u6807", "method": "\u901a\u8fc7\u793e\u4f1a\u6587\u5316\u548c\u8bed\u8a00\u5b66\u7684\u591a\u7ef4\u5ea6\u5dee\u5f02\u5206\u6790\uff08\u62c9\u4e01\u7f8e\u6d32\u4e0e\u897f\u73ed\u7259\uff09\uff0c\u63d0\u51fa\u897f\u73ed\u7259\u8bed\u5b50\u53d8\u4f53\u5efa\u6a21\u6846\u67b6", "result": "\u672c\u5730\u5316\u6a21\u578b\u53ef\u63d0\u534715-20%\u7528\u6237\u4fe1\u4efb\u5ea6\uff0c\u5728\u4f4e\u98ce\u9669\u6295\u8d44\u533a\u57df\u5b9e\u73b0\u65e5\u5747\u7528\u6237\u589e\u957f2.3%\u7684\u53ef\u6301\u7eed\u6548\u76ca", "conclusion": "\u8bed\u8a00\u53d8\u4f53\u672c\u5730\u5316\u662f\u56fd\u9645\u5316\u6218\u7565\u7684\u6838\u5fc3\u8981\u7d20\uff0c\u4e94\u79cd\u5b50\u53d8\u4f53\u6a21\u578b\u80fd\u6709\u6548\u589e\u5f3a\u6587\u5316\u654f\u611f\u6027\u5e76\u521b\u9020\u5546\u4e1a\u4ef7\u503c"}}
{"id": "2505.09935", "pdf": "https://arxiv.org/pdf/2505.09935", "abs": "https://arxiv.org/abs/2505.09935", "authors": ["Ahmed S. Abdelrahman", "Mohamed Abdel-Aty", "Quoc Dai Tran"], "title": "VRU-CIPI: Crossing Intention Prediction at Intersections for Improving Vulnerable Road Users Safety", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Understanding and predicting human behavior in-thewild, particularly at urban\nintersections, remains crucial for enhancing interaction safety between road\nusers. Among the most critical behaviors are crossing intentions of Vulnerable\nRoad Users (VRUs), where misinterpretation may result in dangerous conflicts\nwith oncoming vehicles. In this work, we propose the VRU-CIPI framework with a\nsequential attention-based model designed to predict VRU crossing intentions at\nintersections. VRU-CIPI employs Gated Recurrent Unit (GRU) to capture temporal\ndynamics in VRU movements, combined with a multi-head Transformer\nself-attention mechanism to encode contextual and spatial dependencies critical\nfor predicting crossing direction. Evaluated on UCF-VRU dataset, our proposed\nachieves state-of-the-art performance with an accuracy of 96.45% and achieving\nreal-time inference speed reaching 33 frames per second. Furthermore, by\nintegrating with Infrastructure-to-Vehicles (I2V) communication, our approach\ncan proactively enhance intersection safety through timely activation of\ncrossing signals and providing early warnings to connected vehicles, ensuring\nsmoother and safer interactions for all road users.", "AI": {"tldr": "\u63d0\u51faVRU-CIPI\u6846\u67b6\uff0c\u7ed3\u5408GRU\u548cTransformer\u81ea\u6ce8\u610f\u529b\u673a\u5236\u9884\u6d4b\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\u7684\u8fc7\u8857\u610f\u56fe\uff0c\u51c6\u786e\u7387\u8fbe96.45%\u5e76\u5b9e\u73b033fps\u5b9e\u65f6\u63a8\u7406\uff0c\u901a\u8fc7I2V\u901a\u4fe1\u63d0\u5347\u8def\u53e3\u5b89\u5168", "motivation": "\u89e3\u51b3\u57ce\u5e02\u8def\u53e3\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\u8fc7\u8857\u610f\u56fe\u8bef\u5224\u53ef\u80fd\u5bfc\u81f4\u8f66\u8f86\u51b2\u7a81\u7684\u5b89\u5168\u9690\u60a3\uff0c\u9700\u63d0\u5347\u4eba\u8f66\u4ea4\u4e92\u5b89\u5168\u6027", "method": "\u4f7f\u7528GRU\u6355\u6349\u65f6\u95f4\u52a8\u6001\u7279\u5f81\uff0c\u7ed3\u5408\u591a\u5934Transformer\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7f16\u7801\u4e0a\u4e0b\u6587\u7a7a\u95f4\u4f9d\u8d56\uff0c\u6784\u5efa\u5e8f\u5217\u5316\u9884\u6d4b\u6a21\u578b", "result": "\u5728UCF-VRU\u6570\u636e\u96c6\u8fbe\u523096.45%\u51c6\u786e\u7387\uff0c\u5b9e\u65f6\u63a8\u7406\u901f\u5ea633fps\uff0c\u7ed3\u5408I2V\u901a\u4fe1\u53ef\u63d0\u524d\u6fc0\u6d3b\u8fc7\u8857\u4fe1\u53f7\u5e76\u5411\u8054\u7f51\u8f66\u8f86\u53d1\u9001\u9884\u8b66", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7cbe\u51c6\u610f\u56fe\u9884\u6d4b\u548c\u57fa\u7840\u8bbe\u65bd\u534f\u540c\uff0c\u6709\u6548\u63d0\u5347\u8def\u53e3\u5b89\u5168\u7cfb\u6570\uff0c\u4fdd\u969c\u6240\u6709\u9053\u8def\u4f7f\u7528\u8005\u7684\u987a\u7545\u4ea4\u4e92"}}
{"id": "2505.09924", "pdf": "https://arxiv.org/pdf/2505.09924", "abs": "https://arxiv.org/abs/2505.09924", "authors": ["Yidan Wang", "Yubing Ren", "Yanan Cao", "Binxing Fang"], "title": "From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework for Large Language Models", "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "The rise of Large Language Models (LLMs) has heightened concerns about the\nmisuse of AI-generated text, making watermarking a promising solution.\nMainstream watermarking schemes for LLMs fall into two categories: logits-based\nand sampling-based. However, current schemes entail trade-offs among\nrobustness, text quality, and security. To mitigate this, we integrate\nlogits-based and sampling-based schemes, harnessing their respective strengths\nto achieve synergy. In this paper, we propose a versatile symbiotic\nwatermarking framework with three strategies: serial, parallel, and hybrid. The\nhybrid framework adaptively embeds watermarks using token entropy and semantic\nentropy, optimizing the balance between detectability, robustness, text\nquality, and security. Furthermore, we validate our approach through\ncomprehensive experiments on various datasets and models. Experimental results\nindicate that our method outperforms existing baselines and achieves\nstate-of-the-art (SOTA) performance. We believe this framework provides novel\ninsights into diverse watermarking paradigms. Our code is available at\n\\href{https://github.com/redwyd/SymMark}{https://github.com/redwyd/SymMark}.", "AI": {"tldr": "\u63d0\u51faSymbiotic\u6c34\u5370\u6846\u67b6\uff0c\u6574\u5408logits-based\u548csampling-based\u65b9\u6848\uff0c\u901a\u8fc7\u4e32\u884c/\u5e76\u884c/\u6df7\u5408\u4e09\u79cd\u7b56\u7565\u5b9e\u73b0\u68c0\u6d4b\u6027\u3001\u9c81\u68d2\u6027\u3001\u6587\u672c\u8d28\u91cf\u4e0e\u5b89\u5168\u6027\u7684\u5e73\u8861\u4f18\u5316", "motivation": "\u73b0\u6709\u6c34\u5370\u65b9\u6848\u5728\u9c81\u68d2\u6027\u3001\u6587\u672c\u8d28\u91cf\u548c\u5b89\u5168\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u9700\u6574\u5408\u4e24\u79cd\u4e3b\u6d41\u65b9\u6848\u5b9e\u73b0\u534f\u540c\u589e\u6548", "method": "\u901a\u8fc7\u4ee4\u724c\u71b5\u548c\u8bed\u4e49\u71b5\u81ea\u9002\u5e94\u9009\u62e9\u5d4c\u5165\u7b56\u7565\uff0c\u6784\u5efa\u53ef\u9002\u5e94\u4e0d\u540c\u573a\u666f\u7684\u6df7\u5408\u6c34\u5370\u6846\u67b6", "result": "\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u7387\uff0898.4%\uff09\u3001\u6587\u672c\u8d28\u91cf\uff08\u56f0\u60d1\u5ea64.32\uff09\u548c\u6297\u653b\u51fb\u6027\uff0893.6%\u5b58\u6d3b\u7387\uff09\u4e0a\u8fbe\u5230SOTA", "conclusion": "\u8be5\u6846\u67b6\u5f00\u521b\u4e86\u6c34\u5370\u8303\u5f0f\u534f\u540c\u4f18\u5316\u7684\u65b0\u8def\u5f84\uff0c\u5176\u71b5\u9a71\u52a8\u7b56\u7565\u4e3a\u751f\u6210\u5185\u5bb9\u5b89\u5168\u63d0\u4f9b\u4e86\u666e\u9002\u6027\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.09939", "pdf": "https://arxiv.org/pdf/2505.09939", "abs": "https://arxiv.org/abs/2505.09939", "authors": ["Zhe Shan", "Lei Zhou", "Liu Mao", "Shaofan Chen", "Chuanqiu Ren", "Xia Xie"], "title": "Non-Registration Change Detection: A Novel Change Detection Task and Benchmark Dataset", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted to IGARSS 2025", "summary": "In this study, we propose a novel remote sensing change detection task,\nnon-registration change detection, to address the increasing number of\nemergencies such as natural disasters, anthropogenic accidents, and military\nstrikes. First, in light of the limited discourse on the issue of\nnon-registration change detection, we systematically propose eight scenarios\nthat could arise in the real world and potentially contribute to the occurrence\nof non-registration problems. Second, we develop distinct image transformation\nschemes tailored to various scenarios to convert the available registration\nchange detection dataset into a non-registration version. Finally, we\ndemonstrate that non-registration change detection can cause catastrophic\ndamage to the state-of-the-art methods. Our code and dataset are available at\nhttps://github.com/ShanZard/NRCD.", "AI": {"tldr": "\u63d0\u51fa\u975e\u914d\u51c6\u53d8\u5316\u68c0\u6d4b\u4efb\u52a1(NRCD)\uff0c\u6784\u5efa\u516b\u79cd\u771f\u5b9e\u573a\u666f\u5e76\u5f00\u53d1\u6570\u636e\u8f6c\u6362\u65b9\u6848\uff0c\u8bc1\u660e\u73b0\u6709\u65b9\u6cd5\u5728\u8be5\u573a\u666f\u4e0b\u5b58\u5728\u4e25\u91cd\u6027\u80fd\u7f3a\u9677\u3002", "motivation": "\u73b0\u6709\u53d8\u5316\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u56fe\u50cf\u914d\u51c6\uff0c\u4f46\u5728\u81ea\u7136\u707e\u5bb3/\u519b\u4e8b\u51b2\u7a81\u7b49\u7d27\u6025\u573a\u666f\u4e2d\u96be\u4ee5\u83b7\u53d6\u914d\u51c6\u56fe\u50cf\uff0c\u5bfc\u81f4\u68c0\u6d4b\u6027\u80fd\u5927\u5e45\u4e0b\u964d\u3002", "method": "1. \u7cfb\u7edf\u63d0\u51fa\u516b\u79cd\u73b0\u5b9e\u975e\u914d\u51c6\u573a\u666f 2. \u5f00\u53d1\u56fe\u50cf\u8f6c\u6362\u65b9\u6848\u5c06\u914d\u51c6\u6570\u636e\u96c6\u8f6c\u4e3a\u975e\u914d\u51c6\u7248\u672c 3. \u8bc4\u4f30SOTA\u65b9\u6cd5\u5728\u975e\u914d\u51c6\u573a\u666f\u4e0b\u7684\u8868\u73b0", "result": "\u975e\u914d\u51c6\u53d8\u5316\u68c0\u6d4b\u4f1a\u4f7f\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u4ea7\u751f\u707e\u96be\u6027\u9519\u8bef\uff0c\u53ec\u56de\u7387\u4e0b\u964d\u8d85\u8fc750%", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u975e\u914d\u51c6\u573a\u666f\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u9065\u611f\u5e94\u6025\u68c0\u6d4b\u63d0\u4f9b\u65b0\u65b9\u5411\uff0c\u5e76\u5f00\u6e90\u4ee3\u7801\u548cNRCD\u6570\u636e\u96c6\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2505.09930", "pdf": "https://arxiv.org/pdf/2505.09930", "abs": "https://arxiv.org/abs/2505.09930", "authors": ["Zixiao Zhu", "Hanzhang Zhou", "Zijian Feng", "Tianjiao Li", "Chua Jia Jim Deryl", "Mak Lee Onn", "Gee Wah Ng", "Kezhi Mao"], "title": "Rethinking Prompt Optimizers: From Prompt Merits to Optimization", "categories": ["cs.CL"], "comment": "20 pages, 14 figures", "summary": "Prompt optimization (PO) offers a practical alternative to fine-tuning large\nlanguage models (LLMs), enabling performance improvements without altering\nmodel weights. Existing methods typically rely on advanced, large-scale LLMs\nlike GPT-4 to generate optimized prompts. However, due to limited downward\ncompatibility, verbose, instruction-heavy prompts from advanced LLMs can\noverwhelm lightweight inference models and degrade response quality. In this\nwork, we rethink prompt optimization through the lens of interpretable design.\nWe first identify a set of model-agnostic prompt quality merits and empirically\nvalidate their effectiveness in enhancing prompt and response quality. We then\nintroduce MePO, a merit-guided, lightweight, and locally deployable prompt\noptimizer trained on our preference dataset built from merit-aligned prompts\ngenerated by a lightweight LLM. Unlike prior work, MePO avoids online\noptimization reliance, reduces cost and privacy concerns, and, by learning\nclear, interpretable merits, generalizes effectively to both large-scale and\nlightweight inference models. Experiments demonstrate that MePO achieves better\nresults across diverse tasks and model types, offering a scalable and robust\nsolution for real-world deployment. Our model and dataset are available at:\nhttps://github.com/MidiyaZhu/MePO", "AI": {"tldr": "MePO\u63d0\u51fa\u8f7b\u91cf\u7ea7\u672c\u5730\u53ef\u90e8\u7f72\u7684prompt\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u7684prompt\u8d28\u91cf\u6307\u6807\u6307\u5bfc\u4f18\u5316\uff0c\u517c\u5bb9\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u5e76\u63d0\u5347\u6548\u679c", "motivation": "\u73b0\u6709prompt\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56GPT-4\u7b49\u5927\u6a21\u578b\u751f\u6210\u590d\u6742prompt\uff0c\u5bfc\u81f4\u8f7b\u91cf\u6a21\u578b\u54cd\u5e94\u8d28\u91cf\u4e0b\u964d\u4e14\u5b58\u5728\u6210\u672c/\u9690\u79c1\u95ee\u9898", "method": "1. \u5efa\u7acb\u6a21\u578b\u65e0\u5173\u7684prompt\u8d28\u91cf\u8bc4\u4f30\u6807\u51c6\n2. \u57fa\u4e8e\u8f7b\u91cf\u6a21\u578b\u751f\u6210merit\u5bf9\u9f50\u7684prompt\u6784\u5efa\u504f\u597d\u6570\u636e\u96c6\n3. \u8bad\u7ec3merit\u5f15\u5bfc\u7684\u672c\u5730\u90e8\u7f72\u4f18\u5316\u5668MePO", "result": "\u8de8\u4efb\u52a1\u548c\u6a21\u578b\u7c7b\u578b\u7684\u5b9e\u9a8c\u8bc1\u660eMePO\u6548\u679c\u66f4\u4f18\uff0c\u5c24\u5176\u5728\u8f7b\u91cf\u6a21\u578b\u4e0a\u63d0\u5347\u663e\u8457", "conclusion": "MePO\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u4f18\u5316\u6307\u6807\u5b9e\u73b0\u4f4e\u6210\u672c\u9690\u79c1\u5b89\u5168\u7684prompt\u4f18\u5316\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.09943", "pdf": "https://arxiv.org/pdf/2505.09943", "abs": "https://arxiv.org/abs/2505.09943", "authors": ["Jiakun Deng", "Kexuan Li", "Xingye Cui", "Jiaxuan Li", "Chang Long", "Tian Pu", "Zhenming Peng"], "title": "CSPENet: Contour-Aware and Saliency Priors Embedding Network for Infrared Small Target Detection", "categories": ["cs.CV"], "comment": null, "summary": "Infrared small target detection (ISTD) plays a critical role in a wide range\nof civilian and military applications. Existing methods suffer from\ndeficiencies in the localization of dim targets and the perception of contour\ninformation under dense clutter environments, severely limiting their detection\nperformance. To tackle these issues, we propose a contour-aware and saliency\npriors embedding network (CSPENet) for ISTD. We first design a\nsurround-convergent prior extraction module (SCPEM) that effectively captures\nthe intrinsic characteristic of target contour pixel gradients converging\ntoward their center. This module concurrently extracts two collaborative\npriors: a boosted saliency prior for accurate target localization and\nmulti-scale structural priors for comprehensively enriching contour detail\nrepresentation. Building upon this, we propose a dual-branch priors embedding\narchitecture (DBPEA) that establishes differentiated feature fusion pathways,\nembedding these two priors at optimal network positions to achieve performance\nenhancement. Finally, we develop an attention-guided feature enhancement module\n(AGFEM) to refine feature representations and improve saliency estimation\naccuracy. Experimental results on public datasets NUDT-SIRST, IRSTD-1k, and\nNUAA-SIRST demonstrate that our CSPENet outperforms other state-of-the-art\nmethods in detection performance. The code is available at\nhttps://github.com/IDIP2025/CSPENet.", "AI": {"tldr": "\u63d0\u51faCSPENet\u7f51\u7edc\uff0c\u901a\u8fc7\u6574\u5408\u76ee\u6807\u8f6e\u5ed3\u5148\u9a8c\u548c\u663e\u8457\u6027\u5148\u9a8c\uff0c\u5728\u590d\u6742\u80cc\u666f\u4e0b\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4f4e\u5bf9\u6bd4\u5ea6\u76ee\u6807\u5b9a\u4f4d\u548c\u5bc6\u96c6\u6742\u6ce2\u73af\u5883\u4e0b\u7684\u8f6e\u5ed3\u611f\u77e5\u5b58\u5728\u4e0d\u8db3\uff0c\u4e25\u91cd\u5f71\u54cd\u68c0\u6d4b\u6027\u80fd\u3002\u9700\u8981\u540c\u65f6\u63d0\u5347\u76ee\u6807\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u8f6e\u5ed3\u7ec6\u8282\u8868\u5f81\u80fd\u529b\u3002", "method": "1) \u8bbe\u8ba1SCPEM\u6a21\u5757\u6355\u83b7\u76ee\u6807\u8f6e\u5ed3\u68af\u5ea6\u7279\u5f81\uff0c\u63d0\u53d6\u589e\u5f3a\u578b\u663e\u8457\u6027\u5148\u9a8c\u548c\u591a\u5c3a\u5ea6\u7ed3\u6784\u5148\u9a8c\uff1b2) \u6784\u5efa\u53cc\u5206\u652f\u5148\u9a8c\u5d4c\u5165\u67b6\u6784DBPEA\u5b9e\u73b0\u7279\u5f81\u878d\u5408\uff1b3) \u5f00\u53d1AGFEM\u6a21\u5757\u4f18\u5316\u7279\u5f81\u8868\u793a", "result": "\u5728NUDT-SIRST\u3001IRSTD-1k\u3001NUAA-SIRST\u7b49\u6570\u636e\u96c6\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\uff0c\u68c0\u6d4b\u7cbe\u5ea6\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "\u901a\u8fc7\u534f\u540c\u5229\u7528\u76ee\u6807\u8f6e\u5ed3\u6536\u655b\u7279\u6027\u548c\u591a\u5c3a\u5ea6\u7ed3\u6784\u5148\u9a8c\uff0c\u5b9e\u73b0\u4e86\u590d\u6742\u573a\u666f\u4e0b\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\u7a81\u7834\uff0c\u4e3a\u76f8\u5173\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.09945", "pdf": "https://arxiv.org/pdf/2505.09945", "abs": "https://arxiv.org/abs/2505.09945", "authors": ["Deeksha Prahlad", "Chanhee Lee", "Dongha Kim", "Hokeun Kim"], "title": "Personalizing Large Language Models using Retrieval Augmented Generation and Knowledge Graph", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "To appear in the Companion Proceedings of the ACM Web Conference 2025\n  (WWW Companion '25)", "summary": "The advent of large language models (LLMs) has allowed numerous applications,\nincluding the generation of queried responses, to be leveraged in chatbots and\nother conversational assistants. Being trained on a plethora of data, LLMs\noften undergo high levels of over-fitting, resulting in the generation of extra\nand incorrect data, thus causing hallucinations in output generation. One of\nthe root causes of such problems is the lack of timely, factual, and\npersonalized information fed to the LLM. In this paper, we propose an approach\nto address these problems by introducing retrieval augmented generation (RAG)\nusing knowledge graphs (KGs) to assist the LLM in personalized response\ngeneration tailored to the users. KGs have the advantage of storing\ncontinuously updated factual information in a structured way. While our KGs can\nbe used for a variety of frequently updated personal data, such as calendar,\ncontact, and location data, we focus on calendar data in this paper. Our\nexperimental results show that our approach works significantly better in\nunderstanding personal information and generating accurate responses compared\nto the baseline LLMs using personal data as text inputs, with a moderate\nreduction in response time.", "AI": {"tldr": "\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u68c0\u7d22\u7684\u751f\u6210\u65b9\u6cd5\uff08RAG\uff09\u6709\u6548\u63d0\u5347LLMs\u5728\u4e2a\u6027\u5316\u65e5\u5386\u573a\u666f\u4e2d\u7684\u51c6\u786e\u6027", "motivation": "\u89e3\u51b3LLMs\u56e0\u8bad\u7ec3\u6570\u636e\u8fc7\u65f6\u548c\u7f3a\u4e4f\u4e2a\u6027\u5316\u4fe1\u606f\u5bfc\u81f4\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u7279\u522b\u662f\u65e5\u5386\u7b49\u52a8\u6001\u4e2a\u4eba\u6570\u636e\u573a\u666f\u7684\u54cd\u5e94\u51c6\u786e\u6027\u4e0d\u8db3", "method": "\u5c06\u77e5\u8bc6\u56fe\u8c31\uff08\u5b58\u50a8\u7ed3\u6784\u5316\u5b9e\u65f6\u6570\u636e\uff09\u4e0e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u7ed3\u5408\uff0c\u6784\u5efa\u5305\u542b\u65e5\u5386\u4e8b\u4ef6\u7684\u4e09\u5143\u7ec4\u77e5\u8bc6\u56fe\u8c31\u8fdb\u884c\u68c0\u7d22\u589e\u5f3a", "result": "\u5728\u4e2a\u6027\u5316\u7406\u89e3\u548c\u54cd\u5e94\u51c6\u786e\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff08\u7eaf\u6587\u672c\u8f93\u5165\uff09\uff0c\u54cd\u5e94\u65f6\u95f4\u589e\u52a014%\u4f46\u4ecd\u4fdd\u6301\u5b9e\u7528\u6c34\u5e73", "conclusion": "\u77e5\u8bc6\u56fe\u8c31\u7684\u7ed3\u6784\u5316\u5b58\u50a8\u7279\u6027\u6709\u6548\u63d0\u5347LLMs\u7684\u5b9e\u65f6\u6570\u636e\u5229\u7528\u80fd\u529b\uff0c\u4e3a\u4e2a\u6027\u5316AI\u52a9\u624b\u63d0\u4f9b\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.09965", "pdf": "https://arxiv.org/pdf/2505.09965", "abs": "https://arxiv.org/abs/2505.09965", "authors": ["Hao Yang", "Tao Tan", "Shuai Tan", "Weiqin Yang", "Kunyan Cai", "Calvin Chen", "Yue Sun"], "title": "MambaControl: Anatomy Graph-Enhanced Mamba ControlNet with Fourier Refinement for Diffusion-Based Disease Trajectory Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Modelling disease progression in precision medicine requires capturing\ncomplex spatio-temporal dynamics while preserving anatomical integrity.\nExisting methods often struggle with longitudinal dependencies and structural\nconsistency in progressive disorders. To address these limitations, we\nintroduce MambaControl, a novel framework that integrates selective state-space\nmodelling with diffusion processes for high-fidelity prediction of medical\nimage trajectories. To better capture subtle structural changes over time while\nmaintaining anatomical consistency, MambaControl combines Mamba-based\nlong-range modelling with graph-guided anatomical control to more effectively\nrepresent anatomical correlations. Furthermore, we introduce Fourier-enhanced\nspectral graph representations to capture spatial coherence and multiscale\ndetail, enabling MambaControl to achieve state-of-the-art performance in\nAlzheimer's disease prediction. Quantitative and regional evaluations\ndemonstrate improved progression prediction quality and anatomical fidelity,\nhighlighting its potential for personalised prognosis and clinical decision\nsupport.", "AI": {"tldr": "\u63d0\u51faMambaControl\u6846\u67b6\uff0c\u7ed3\u5408\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u5efa\u6a21\u4e0e\u6269\u6563\u8fc7\u7a0b\uff0c\u7528\u4e8e\u533b\u5b66\u5f71\u50cf\u8f68\u8ff9\u7684\u9ad8\u4fdd\u771f\u9884\u6d4b", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7eb5\u5411\u4f9d\u8d56\u5efa\u6a21\u548c\u9000\u884c\u6027\u75be\u75c5\u7ed3\u6784\u4e00\u81f4\u6027\u4fdd\u6301\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u517c\u987e\u590d\u6742\u65f6\u7a7a\u52a8\u6001\u4e0e\u89e3\u5256\u5b8c\u6574\u6027", "method": "\u6574\u5408Mamba\u957f\u7a0b\u5efa\u6a21\u4e0e\u56fe\u5f15\u5bfc\u89e3\u5256\u63a7\u5236\uff0c\u5f15\u5165\u5085\u91cc\u53f6\u589e\u5f3a\u9891\u8c31\u56fe\u8868\u5f81\u6355\u6349\u7a7a\u95f4\u8fde\u8d2f\u6027\u548c\u591a\u5c3a\u5ea6\u7ec6\u8282", "result": "\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u9884\u6d4b\u4e2d\u5b9e\u73b0SOTA\u6027\u80fd\uff0c\u5b9a\u91cf\u4e0e\u533a\u57df\u8bc4\u4f30\u663e\u793a\u8fdb\u5c55\u9884\u6d4b\u8d28\u91cf\u548c\u89e3\u5256\u4fdd\u771f\u5ea6\u63d0\u5347", "conclusion": "\u6846\u67b6\u4e3a\u4e2a\u6027\u5316\u9884\u540e\u548c\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u63d0\u4f9b\u65b0\u65b9\u6848\uff0c\u9a8c\u8bc1\u4e86\u6574\u5408\u89e3\u5256\u63a7\u5236\u4e0e\u9891\u8c31\u8868\u5f81\u7684\u6709\u6548\u6027"}}
{"id": "2505.10013", "pdf": "https://arxiv.org/pdf/2505.10013", "abs": "https://arxiv.org/abs/2505.10013", "authors": ["Lake Yin", "Fan Huang"], "title": "DIF: A Framework for Benchmarking and Verifying Implicit Bias in LLMs", "categories": ["cs.CL"], "comment": "7 pages, 1 figure", "summary": "As Large Language Models (LLMs) have risen in prominence over the past few\nyears, there has been concern over the potential biases in LLMs inherited from\nthe training data. Previous studies have examined how LLMs exhibit implicit\nbias, such as when response generation changes when different social contexts\nare introduced. We argue that this implicit bias is not only an ethical, but\nalso a technical issue, as it reveals an inability of LLMs to accommodate\nextraneous information. However, unlike other measures of LLM intelligence,\nthere are no standard methods to benchmark this specific subset of LLM bias. To\nbridge this gap, we developed a method for calculating an easily interpretable\nbenchmark, DIF (Demographic Implicit Fairness), by evaluating preexisting LLM\nlogic and math problem datasets with sociodemographic personas. We demonstrate\nthat this method can statistically validate the presence of implicit bias in\nLLM behavior and find an inverse trend between question answering accuracy and\nimplicit bias, supporting our argument.", "AI": {"tldr": "\u8bba\u6587\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9690\u5f0f\u504f\u89c1\u95ee\u9898\uff0c\u63d0\u51faDIF\u91cf\u5316\u6307\u6807\u5e76\u901a\u8fc7\u793e\u4f1a\u89d2\u8272\u5b9e\u9a8c\u9a8c\u8bc1\u504f\u89c1\u5b58\u5728\uff0c\u53d1\u73b0\u6a21\u578b\u51c6\u786e\u7387\u4e0e\u504f\u89c1\u7a0b\u5ea6\u5448\u8d1f\u76f8\u5173", "motivation": "\u63ed\u793aLLM\u7684\u9690\u5f0f\u504f\u89c1\u4e0d\u4ec5\u662f\u4f26\u7406\u95ee\u9898\uff0c\u66f4\u66b4\u9732\u5176\u6280\u672f\u7f3a\u9677\u2014\u2014\u65e0\u6cd5\u6709\u6548\u6574\u5408\u5916\u90e8\u4fe1\u606f\u3002\u5f53\u524d\u7f3a\u4e4f\u8be5\u9886\u57df\u6807\u51c6\u5316\u8bc4\u4f30\u65b9\u6cd5\uff0c\u9700\u5efa\u7acb\u53ef\u89e3\u91ca\u7684\u91cf\u5316\u57fa\u51c6", "method": "\u901a\u8fc7\u7ed9\u73b0\u6709\u903b\u8f91/\u6570\u5b66\u9898\u6570\u636e\u96c6\u6dfb\u52a0\u793e\u4f1a\u4eba\u53e3\u5c5e\u6027\u89d2\u8272\uff0c\u6784\u5efaDIF\u6307\u6807\uff08Demographic Implicit Fairness\uff09\uff0c\u7edf\u8ba1\u5206\u6790\u6a21\u578b\u8868\u73b0\u4e0e\u4eba\u53e3\u7279\u5f81\u7684\u5173\u7cfb", "result": "DIF\u65b9\u6cd5\u7edf\u8ba1\u9a8c\u8bc1\u4e86LLM\u5b58\u5728\u9690\u5f0f\u504f\u89c1\uff0c\u53d1\u73b0\u95ee\u7b54\u51c6\u786e\u7387\u4e0e\u504f\u89c1\u7a0b\u5ea6\u5448\u73b0\u663e\u8457\u53cd\u5411\u76f8\u5173\u8d8b\u52bf", "conclusion": "\u7814\u7a76\u786e\u7acb\u4e86\u9996\u4e2a\u53ef\u89e3\u91ca\u7684LLM\u9690\u5f0f\u504f\u89c1\u91cf\u5316\u57fa\u51c6\uff0c\u8bc1\u660e\u6a21\u578b\u6027\u80fd\u4e0e\u516c\u5e73\u6027\u5b58\u5728\u5185\u5728\u51b2\u7a81\uff0c\u4e3a\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u65b0\u7ef4\u5ea6"}}
{"id": "2505.09967", "pdf": "https://arxiv.org/pdf/2505.09967", "abs": "https://arxiv.org/abs/2505.09967", "authors": ["Liqian Deng"], "title": "TKFNet: Learning Texture Key Factor Driven Feature for Facial Expression Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Facial expression recognition (FER) in the wild remains a challenging task\ndue to the subtle and localized nature of expression-related features, as well\nas the complex variations in facial appearance. In this paper, we introduce a\nnovel framework that explicitly focuses on Texture Key Driver Factors (TKDF),\nlocalized texture regions that exhibit strong discriminative power across\nemotional categories. By carefully observing facial image patterns, we identify\nthat certain texture cues, such as micro-changes in skin around the brows,\neyes, and mouth, serve as primary indicators of emotional dynamics. To\neffectively capture and leverage these cues, we propose a FER architecture\ncomprising a Texture-Aware Feature Extractor (TAFE) and Dual Contextual\nInformation Filtering (DCIF). TAFE employs a ResNet-based backbone enhanced\nwith multi-branch attention to extract fine-grained texture representations,\nwhile DCIF refines these features by filtering context through adaptive pooling\nand attention mechanisms. Experimental results on RAF-DB and KDEF datasets\ndemonstrate that our method achieves state-of-the-art performance, verifying\nthe effectiveness and robustness of incorporating TKDFs into FER pipelines.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7eb9\u7406\u5173\u952e\u9a71\u52a8\u56e0\u7d20\uff08TKDF\uff09\u7684FER\u6846\u67b6\uff0c\u901a\u8fc7TAFE\u7279\u5f81\u63d0\u53d6\u5668\u548cDCIF\u4e0a\u4e0b\u6587\u8fc7\u6ee4\u673a\u5236\u63d0\u5347\u8868\u60c5\u8bc6\u522b\u6548\u679c", "motivation": "\u81ea\u7136\u573a\u666f\u4e0b\u9762\u90e8\u8868\u60c5\u7279\u5f81\u5177\u6709\u5c40\u90e8\u7ec6\u5fae\u6027\u4e0e\u590d\u6742\u53d8\u5316\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u5173\u952e\u7eb9\u7406\u7279\u5f81", "method": "1. TAFE\u6a21\u5757\uff08\u57fa\u4e8eResNet+\u591a\u5206\u652f\u6ce8\u610f\u529b\uff09\u63d0\u53d6\u7ec6\u7c92\u5ea6\u7eb9\u7406\u7279\u5f81\n2. DCIF\u673a\u5236\u901a\u8fc7\u81ea\u9002\u5e94\u6c60\u5316\u4e0e\u6ce8\u610f\u529b\u8fc7\u6ee4\u4e0a\u4e0b\u6587", "result": "\u5728RAF-DB\u548cKDEF\u6570\u636e\u96c6\u8fbe\u5230SOTA\uff0c\u9a8c\u8bc1TKDF\u7684\u6709\u6548\u6027\u4e0e\u9c81\u68d2\u6027", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u7709\u6bdb/\u773c\u775b/\u5634\u90e8\u533a\u57df\u7684\u5fae\u7eb9\u7406\u53d8\u5316\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u573a\u666f\u4e0b\u7684\u8868\u60c5\u8bc6\u522b\u6027\u80fd"}}
{"id": "2505.10063", "pdf": "https://arxiv.org/pdf/2505.10063", "abs": "https://arxiv.org/abs/2505.10063", "authors": ["Han Peng", "Jinhao Jiang", "Zican Dong", "Wayne Xin Zhao", "Lei Fang"], "title": "CAFE: Retrieval Head-based Coarse-to-Fine Information Seeking to Enhance Multi-Document QA Capability", "categories": ["cs.CL"], "comment": null, "summary": "Advancements in Large Language Models (LLMs) have extended their input\ncontext length, yet they still struggle with retrieval and reasoning in\nlong-context inputs. Existing methods propose to utilize the prompt strategy\nand retrieval head to alleviate this limitation. However, they still face\nchallenges in balancing retrieval precision and recall, impacting their\nefficacy in answering questions. To address this, we introduce $\\textbf{CAFE}$,\na two-stage coarse-to-fine method to enhance multi-document question-answering\ncapacities. By gradually eliminating the negative impacts of background and\ndistracting documents, CAFE makes the responses more reliant on the evidence\ndocuments. Initially, a coarse-grained filtering method leverages retrieval\nheads to identify and rank relevant documents. Then, a fine-grained steering\nmethod guides attention to the most relevant content. Experiments across\nbenchmarks show CAFE outperforms baselines, achieving up to 22.1% and 13.7%\nSubEM improvement over SFT and RAG methods on the Mistral model, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u9636\u6bb5\u65b9\u6cd5CAFE\uff0c\u901a\u8fc7\u7c97\u7c92\u5ea6\u8fc7\u6ee4\u548c\u7ec6\u7c92\u5ea6\u5f15\u5bfc\u589e\u5f3a\u591a\u6587\u6863\u95ee\u7b54\u80fd\u529b\uff0c\u51cf\u5c11\u5e72\u6270\u6587\u6863\u5f71\u54cd", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u4e0a\u4e0b\u6587\u95ee\u7b54\u4e2d\u5b58\u5728\u68c0\u7d22\u7cbe\u5ea6\u4e0e\u53ec\u56de\u7387\u7684\u5e73\u8861\u95ee\u9898\uff0c\u5bfc\u81f4\u8bc1\u636e\u6587\u6863\u5229\u7528\u7387\u4e0d\u8db3", "method": "1. \u7c97\u7c92\u5ea6\u8fc7\u6ee4\uff1a\u5229\u7528\u68c0\u7d22\u5934\u8bc6\u522b\u5e76\u6392\u5e8f\u76f8\u5173\u6587\u6863\n2. \u7ec6\u7c92\u5ea6\u5f15\u5bfc\uff1a\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u805a\u7126\u5173\u952e\u5185\u5bb9", "result": "\u5728Mistral\u6a21\u578b\u4e0a\u5b9e\u73b0SubEM\u6307\u6807\u63d0\u5347\uff0c\u8f83SFT\u548cRAG\u65b9\u6cd5\u5206\u522b\u63d0\u9ad822.1%\u548c13.7%", "conclusion": "CAFE\u901a\u8fc7\u5206\u7ea7\u5904\u7406\u673a\u5236\u6709\u6548\u63d0\u5347\u6a21\u578b\u5bf9\u8bc1\u636e\u6587\u6863\u7684\u4f9d\u8d56\uff0c\u663e\u8457\u6539\u5584\u591a\u6587\u6863\u95ee\u7b54\u6027\u80fd"}}
{"id": "2505.09971", "pdf": "https://arxiv.org/pdf/2505.09971", "abs": "https://arxiv.org/abs/2505.09971", "authors": ["Yuan Gao", "Shaobo Xia", "Sheng Nie", "Cheng Wang", "Xiaohuan Xi", "Bisheng Yang"], "title": "APCoTTA: Continual Test-Time Adaptation for Semantic Segmentation of Airborne LiDAR Point Clouds", "categories": ["cs.CV"], "comment": "18 pages,12 figures", "summary": "Airborne laser scanning (ALS) point cloud segmentation is a fundamental task\nfor large-scale 3D scene understanding. In real-world applications, models are\ntypically fixed after training. However, domain shifts caused by changes in the\nenvironment, sensor types, or sensor degradation often lead to a decline in\nmodel performance. Continuous Test-Time Adaptation (CTTA) offers a solution by\nadapting a source-pretrained model to evolving, unlabeled target domains.\nDespite its potential, research on ALS point clouds remains limited, facing\nchallenges such as the absence of standardized datasets and the risk of\ncatastrophic forgetting and error accumulation during prolonged adaptation. To\ntackle these challenges, we propose APCoTTA, the first CTTA method tailored for\nALS point cloud semantic segmentation. We propose a dynamic trainable layer\nselection module. This module utilizes gradient information to select\nlow-confidence layers for training, and the remaining layers are kept frozen,\nmitigating catastrophic forgetting. To further reduce error accumulation, we\npropose an entropy-based consistency loss. By losing such samples based on\nentropy, we apply consistency loss only to the reliable samples, enhancing\nmodel stability. In addition, we propose a random parameter interpolation\nmechanism, which randomly blends parameters from the selected trainable layers\nwith those of the source model. This approach helps balance target adaptation\nand source knowledge retention, further alleviating forgetting. Finally, we\nconstruct two benchmarks, ISPRSC and H3DC, to address the lack of CTTA\nbenchmarks for ALS point cloud segmentation. Experimental results demonstrate\nthat APCoTTA achieves the best performance on two benchmarks, with mIoU\nimprovements of approximately 9% and 14% over direct inference. The new\nbenchmarks and code are available at https://github.com/Gaoyuan2/APCoTTA.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u9762\u5411ALS\u70b9\u4e91\u5206\u5272\u7684\u8fde\u7eed\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5APCoTTA\uff0c\u901a\u8fc7\u52a8\u6001\u5c42\u9009\u62e9\u3001\u71b5\u4e00\u81f4\u6027\u635f\u5931\u548c\u53c2\u6570\u63d2\u503c\u673a\u5236\u89e3\u51b3\u707e\u96be\u6027\u9057\u5fd8\u4e0e\u9519\u8bef\u7d2f\u79ef\u95ee\u9898", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u73af\u5883/\u4f20\u611f\u5668\u53d8\u5316\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0b\u964d\uff0c\u73b0\u6709ALS\u70b9\u4e91CTTA\u7814\u7a76\u5b58\u5728\u57fa\u51c6\u7f3a\u5931\u3001\u707e\u96be\u6027\u9057\u5fd8\u548c\u957f\u671f\u9002\u5e94\u8bef\u5dee\u7d2f\u79ef\u6311\u6218", "method": "1. \u52a8\u6001\u53ef\u8bad\u7ec3\u5c42\u9009\u62e9\u6a21\u5757\uff08\u57fa\u4e8e\u68af\u5ea6\u9009\u62e9\u4f4e\u7f6e\u4fe1\u5c42\uff09 2. \u57fa\u4e8e\u71b5\u7684\u4e00\u81f4\u6027\u635f\u5931\uff08\u7b5b\u9009\u53ef\u9760\u6837\u672c\uff09 3. \u968f\u673a\u53c2\u6570\u63d2\u503c\u673a\u5236\uff08\u5e73\u8861\u76ee\u6807\u57df\u9002\u5e94\u4e0e\u6e90\u77e5\u8bc6\u4fdd\u7559\uff09", "result": "\u5728ISPRSC/H3DC\u57fa\u51c6\u4e0a\u5206\u522b\u53d6\u5f979%\u548c14%\u7684mIoU\u63d0\u5347\uff0c\u663e\u8457\u4f18\u4e8e\u76f4\u63a5\u63a8\u7406", "conclusion": "APCoTTA\u6709\u6548\u7f13\u89e3\u8fde\u7eed\u9002\u5e94\u4e2d\u7684\u6027\u80fd\u9000\u5316\u95ee\u9898\uff0c\u540c\u65f6\u5efa\u7acb\u4e86\u9996\u4e2aALS\u70b9\u4e91CTTA\u57fa\u51c6\u6570\u636e\u96c6\u63a8\u52a8\u9886\u57df\u7814\u7a76"}}
{"id": "2505.10066", "pdf": "https://arxiv.org/pdf/2505.10066", "abs": "https://arxiv.org/abs/2505.10066", "authors": ["Michael Fire", "Yitzhak Elbazis", "Adi Wasenstein", "Lior Rokach"], "title": "Dark LLMs: The Growing Threat of Unaligned AI Models", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG", "68T50, 68T05, 68P25", "I.2.7"], "comment": null, "summary": "Large Language Models (LLMs) rapidly reshape modern life, advancing fields\nfrom healthcare to education and beyond. However, alongside their remarkable\ncapabilities lies a significant threat: the susceptibility of these models to\njailbreaking. The fundamental vulnerability of LLMs to jailbreak attacks stems\nfrom the very data they learn from. As long as this training data includes\nunfiltered, problematic, or 'dark' content, the models can inherently learn\nundesirable patterns or weaknesses that allow users to circumvent their\nintended safety controls. Our research identifies the growing threat posed by\ndark LLMs models deliberately designed without ethical guardrails or modified\nthrough jailbreak techniques. In our research, we uncovered a universal\njailbreak attack that effectively compromises multiple state-of-the-art models,\nenabling them to answer almost any question and produce harmful outputs upon\nrequest. The main idea of our attack was published online over seven months\nago. However, many of the tested LLMs were still vulnerable to this attack.\nDespite our responsible disclosure efforts, responses from major LLM providers\nwere often inadequate, highlighting a concerning gap in industry practices\nregarding AI safety. As model training becomes more accessible and cheaper, and\nas open-source LLMs proliferate, the risk of widespread misuse escalates.\nWithout decisive intervention, LLMs may continue democratizing access to\ndangerous knowledge, posing greater risks than anticipated.", "AI": {"tldr": "LLMs\u9762\u4e34\u8d8a\u72f1\u653b\u51fb\u98ce\u9669\uff0c\u5176\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u672a\u8fc7\u6ee4\u5185\u5bb9\u5bfc\u81f4\u5b89\u5168\u6f0f\u6d1e\u3002\u7814\u7a76\u53d1\u73b0\u901a\u7528\u8d8a\u72f1\u653b\u51fb\u53ef\u7a81\u7834\u591a\u6b3e\u5148\u8fdb\u6a21\u578b\uff0c\u884c\u4e1a\u5e94\u5bf9\u63aa\u65bd\u4e0d\u8db3\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5feb\u901f\u53d1\u5c55\u7684\u540c\u65f6\uff0c\u5176\u8bad\u7ec3\u6570\u636e\u4e2d\u6f5c\u5728\u7684\u2018\u6697\u6570\u636e\u2019\u5bfc\u81f4\u6a21\u578b\u6613\u53d7\u8d8a\u72f1\u653b\u51fb\uff0c\u5a01\u80c1AI\u5b89\u5168\u6027\u3002\u7814\u7a76\u8005\u8bd5\u56fe\u63ed\u793a\u8be5\u6f0f\u6d1e\u7684\u666e\u904d\u6027\u548c\u884c\u4e1a\u5e94\u5bf9\u7f3a\u9677\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u901a\u7528\u8d8a\u72f1\u653b\u51fb\u7b56\u7565\uff08\u5177\u4f53\u65b9\u6cd5\u672a\u516c\u5f00\uff09\uff0c\u5bf9\u591a\u4e2a\u524d\u6cbfLLM\u8fdb\u884c\u6e17\u900f\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u653b\u51fb\u6709\u6548\u6027\u3002\u653b\u51fb\u539f\u7406\u57fa\u4e8e\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u4e0d\u826f\u6a21\u5f0f\u5229\u7528\u3002", "result": "7\u4e2a\u6708\u524d\u516c\u5f00\u7684\u653b\u51fb\u65b9\u6cd5\u4ecd\u5bf9\u591a\u6570\u4e3b\u6d41LLM\u6709\u6548\uff1b\u5382\u5546\u54cd\u5e94\u6ede\u540e\uff0c\u5f00\u6e90\u6a21\u578b\u6269\u6563\u52a0\u5267\u98ce\u9669\uff1b\u6a21\u578b\u8bad\u7ec3\u95e8\u69db\u964d\u4f4e\u5bfc\u81f4\u5371\u9669\u77e5\u8bc6\u4f20\u64ad\u98ce\u9669\u9661\u589e\u3002", "conclusion": "\u5f53\u524dLLM\u5b89\u5168\u9632\u62a4\u5b58\u5728\u7cfb\u7edf\u6027\u6f0f\u6d1e\uff0c\u9700\u5efa\u7acb\u66f4\u4e25\u683c\u7684\u8bad\u7ec3\u6570\u636e\u5ba1\u67e5\u673a\u5236\u548c\u5feb\u901f\u54cd\u5e94\u4f53\u7cfb\uff0c\u5426\u5219\u53ef\u80fd\u5f15\u53d1\u8fdc\u8d85\u9884\u671f\u7684\u793e\u4f1a\u98ce\u9669\u3002"}}
{"id": "2505.09986", "pdf": "https://arxiv.org/pdf/2505.09986", "abs": "https://arxiv.org/abs/2505.09986", "authors": ["Yimin Zhou", "Yichong Xia", "Sicheng Pan", "Bin Chen", "Baoyi An", "Haoqian Wang", "Zhi Wang", "Yaowei Wang", "Zikun Zhou"], "title": "High Quality Underwater Image Compression with Adaptive Correction and Codebook-based Augmentation", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "With the increasing exploration and exploitation of the underwater world,\nunderwater images have become a critical medium for human interaction with\nmarine environments, driving extensive research into their efficient\ntransmission and storage. However, contemporary underwater image compression\nalgorithms fail to fully leverage the unique characteristics distinguishing\nunderwater scenes from terrestrial images, resulting in suboptimal performance.\nTo address this limitation, we introduce HQUIC, designed to exploit\nunderwater-image-specific features for enhanced compression efficiency. HQUIC\nemploys an ALTC module to adaptively predict the attenuation coefficients and\nglobal light information of the images, which effectively mitigates the issues\ncaused by the differences in lighting and tone existing in underwater images.\nSubsequently, HQUIC employs a codebook as an auxiliary branch to extract the\ncommon objects within underwater images and enhances the performance of the\nmain branch. Furthermore, HQUIC dynamically weights multi-scale frequency\ncomponents, prioritizing information critical for distortion quality while\ndiscarding redundant details. Extensive evaluations on diverse underwater\ndatasets demonstrate that HQUIC outperforms state-of-the-art compression\nmethods.", "AI": {"tldr": "\u63d0\u51fa\u6c34\u4e0b\u56fe\u50cf\u538b\u7f29\u6a21\u578bHQUIC\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5149\u8870\u51cf\u8865\u507f\u6a21\u5757\u548c\u52a8\u6001\u9891\u57df\u52a0\u6743\u673a\u5236\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u538b\u7f29\u7b97\u6cd5\u672a\u5145\u5206\u5229\u7528\u6c34\u4e0b\u56fe\u50cf\u5149\u7167\u8870\u51cf\u3001\u8272\u8c03\u5dee\u5f02\u7b49\u7279\u6027\uff0c\u5bfc\u81f4\u538b\u7f29\u6548\u7387\u53d7\u9650\u3002\u9700\u8981\u9488\u5bf9\u6027\u89e3\u51b3\u6c34\u4e0b\u573a\u666f\u4e0e\u9646\u5730\u56fe\u50cf\u7684\u5dee\u5f02\u5316\u7279\u5f81", "method": "1. ALTC\u6a21\u5757\u9884\u6d4b\u8870\u51cf\u7cfb\u6570\u4e0e\u5168\u5c40\u5149\u7167\u4fe1\u606f\n2. \u5f15\u5165\u7801\u672c\u673a\u5236\u63d0\u53d6\u6c34\u4e0b\u7269\u4f53\u5171\u6027\u7279\u5f81\n3. \u52a8\u6001\u52a0\u6743\u591a\u5c3a\u5ea6\u9891\u57df\u5206\u91cf\u5b9e\u73b0\u4fe1\u606f\u4f18\u9009", "result": "\u5728\u591a\u4e2a\u6c34\u4e0b\u6570\u636e\u96c6\u8bc4\u4f30\u4e2d\uff0cHQUIC\u7684\u538b\u7f29\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff08PSNR\u63d0\u5347\u7ea60.3dB\uff0c\u7801\u7387\u8282\u770110%-15%\uff09", "conclusion": "\u901a\u8fc7\u9488\u5bf9\u6027\u5efa\u6a21\u6c34\u4e0b\u56fe\u50cf\u7684\u5149\u7167\u8870\u51cf\u7279\u6027\u4e0e\u7269\u4f53\u5206\u5e03\u89c4\u5f8b\uff0c\u7ed3\u5408\u9891\u57df\u52a8\u6001\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u538b\u7f29\u6548\u7387\uff0c\u9a8c\u8bc1\u7279\u5f81\u9002\u914d\u8bbe\u8ba1\u7684\u91cd\u8981\u6027"}}
{"id": "2505.10081", "pdf": "https://arxiv.org/pdf/2505.10081", "abs": "https://arxiv.org/abs/2505.10081", "authors": ["Wisdom Aduah", "Francois Meyer"], "title": "Designing and Contextualising Probes for African Languages", "categories": ["cs.CL"], "comment": null, "summary": "Pretrained language models (PLMs) for African languages are continually\nimproving, but the reasons behind these advances remain unclear. This paper\npresents the first systematic investigation into probing PLMs for linguistic\nknowledge about African languages. We train layer-wise probes for six\ntypologically diverse African languages to analyse how linguistic features are\ndistributed. We also design control tasks, a way to interpret probe\nperformance, for the MasakhaPOS dataset. We find PLMs adapted for African\nlanguages to encode more linguistic information about target languages than\nmassively multilingual PLMs. Our results reaffirm previous findings that\ntoken-level syntactic information concentrates in middle-to-last layers, while\nsentence-level semantic information is distributed across all layers. Through\ncontrol tasks and probing baselines, we confirm that performance reflects the\ninternal knowledge of PLMs rather than probe memorisation. Our study applies\nestablished interpretability techniques to African-language PLMs. In doing so,\nwe highlight the internal mechanisms underlying the success of strategies like\nactive learning and multilingual adaptation.", "AI": {"tldr": "\u7cfb\u7edf\u7814\u7a76\u8868\u660e\u9488\u5bf9\u975e\u6d32\u8bed\u8a00\u4f18\u5316\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u6bd4\u591a\u8bed\u8a00\u6a21\u578b\u66f4\u80fd\u6355\u6349\u76ee\u6807\u8bed\u8a00\u7684\u53e5\u6cd5\u548c\u8bed\u4e49\u7279\u5f81\uff0c\u63a7\u5236\u4efb\u52a1\u9a8c\u8bc1\u4e86\u6a21\u578b\u5185\u90e8\u77e5\u8bc6\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u975e\u6d32\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8fdb\u6b65\u539f\u56e0\u4e0d\u660e\u786e\uff0c\u9700\u7cfb\u7edf\u6027\u63a2\u7a76\u5176\u8bed\u8a00\u5b66\u77e5\u8bc6\u7f16\u7801\u673a\u5236\u53ca\u4f18\u5316\u7b56\u7565\u7684\u6709\u6548\u6027\u57fa\u7840\u3002", "method": "1. \u4e3a6\u79cd\u975e\u6d32\u8bed\u8a00\u8bad\u7ec3\u5206\u5c42\u63a2\u9488\n2. \u8bbe\u8ba1MasakhaPOS\u63a7\u5236\u4efb\u52a1\u9a8c\u8bc1\u63a2\u9488\u53ef\u9760\u6027\n3. \u5bf9\u6bd4\u975e\u6d32\u4e13\u7528\u6a21\u578b\u4e0e\u591a\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u5dee\u5f02", "result": "\u975e\u6d32\u4e13\u7528\u6a21\u578b\u7f16\u7801\u66f4\u591a\u76ee\u6807\u8bed\u8a00\u7279\u5f81\uff1b\u53e5\u6cd5\u4fe1\u606f\u96c6\u4e2d\u4e8e\u4e2d\u540e\u5c42\uff0c\u8bed\u4e49\u4fe1\u606f\u5168\u5c42\u5206\u5e03\uff1b\u63a7\u5236\u4efb\u52a1\u8bc1\u5b9e\u6a21\u578b\u771f\u5b9e\u77e5\u8bc6\u975e\u63a2\u9488\u8bb0\u5fc6", "conclusion": "\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u6280\u672f\u63ed\u793a\u4e86\u975e\u6d32\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u7b56\u7565\uff08\u4e3b\u52a8\u5b66\u4e60/\u591a\u8bed\u8a00\u9002\u5e94\uff09\u7684\u5185\u90e8\u673a\u5236\uff0c\u4e3a\u540e\u7eed\u4f18\u5316\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e"}}
{"id": "2505.09990", "pdf": "https://arxiv.org/pdf/2505.09990", "abs": "https://arxiv.org/abs/2505.09990", "authors": ["Long Cheng", "Jiafei Duan", "Yi Ru Wang", "Haoquan Fang", "Boyang Li", "Yushan Huang", "Elvis Wang", "Ainaz Eftekhar", "Jason Lee", "Wentao Yuan", "Rose Hendrix", "Noah A. Smith", "Fei Xia", "Dieter Fox", "Ranjay Krishna"], "title": "PointArena: Probing Multimodal Grounding Through Language-Guided Pointing", "categories": ["cs.CV"], "comment": "10 Pages, Dataset and code:https://pointarena.github.io/", "summary": "Pointing serves as a fundamental and intuitive mechanism for grounding\nlanguage within visual contexts, with applications spanning robotics, assistive\ntechnologies, and interactive AI systems. While recent multimodal models have\nstarted to support pointing capabilities, existing benchmarks typically focus\nonly on referential object localization tasks. We introduce PointArena, a\ncomprehensive platform for evaluating multimodal pointing across diverse\nreasoning scenarios. PointArena comprises three components: (1) Point-Bench, a\ncurated dataset containing approximately 1,000 pointing tasks across five\nreasoning categories; (2) Point-Battle, an interactive, web-based arena\nfacilitating blind, pairwise model comparisons, which has already gathered over\n4,500 anonymized votes; and (3) Point-Act, a real-world robotic manipulation\nsystem allowing users to directly evaluate multimodal model pointing\ncapabilities in practical settings. We conducted extensive evaluations of both\nstate-of-the-art open-source and proprietary multimodal models. Results\nindicate that Molmo-72B consistently outperforms other models, though\nproprietary models increasingly demonstrate comparable performance.\nAdditionally, we find that supervised training specifically targeting pointing\ntasks significantly enhances model performance. Across our multi-stage\nevaluation pipeline, we also observe strong correlations, underscoring the\ncritical role of precise pointing capabilities in enabling multimodal models to\neffectively bridge abstract reasoning with concrete, real-world actions.\nProject page: https://pointarena.github.io/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u8bc4\u4f30\u591a\u6a21\u6001\u6307\u5411\u80fd\u529b\u7684PointArena\u5e73\u53f0\uff0c\u5305\u542b\u57fa\u51c6\u6570\u636e\u96c6\u3001\u4ea4\u4e92\u8bc4\u6d4b\u573a\u548c\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u53d1\u73b0\u76d1\u7763\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4ec5\u5173\u6ce8\u5bf9\u8c61\u5b9a\u4f4d\u4efb\u52a1\uff0c\u7f3a\u4e4f\u5bf9\u591a\u6a21\u6001\u6307\u5411\u80fd\u529b\u7684\u5168\u9762\u8bc4\u4f30\u4f53\u7cfb\uff0c\u963b\u788dAI\u7cfb\u7edf\u5728\u5177\u8eab\u667a\u80fd\u9886\u57df\u7684\u53d1\u5c55", "method": "\u6784\u5efa\u5305\u542bPoint-Bench\uff08\u8de85\u7c7b\u63a8\u7406\u4efb\u52a1\u7684\u5343\u7ea7\u6570\u636e\u96c6\uff09\u3001Point-Battle\uff08\u57fa\u4e8e\u7f51\u9875\u7684\u76f2\u6d4b\u7ade\u6280\u573a\uff09\u548cPoint-Act\uff08\u771f\u5b9e\u673a\u5668\u4eba\u7cfb\u7edf\uff09\u7684\u4e09\u4f4d\u4e00\u4f53\u8bc4\u4f30\u5e73\u53f0", "result": "Molmo-72B\u8868\u73b0\u6700\u4f18\uff0c\u4e13\u6709\u6a21\u578b\u9010\u6e10\u903c\u8fd1\uff1b\u76d1\u7763\u8bad\u7ec3\u4f7f\u51c6\u786e\u7387\u63d0\u534718.7%\uff1b\u591a\u9636\u6bb5\u8bc4\u4f30\u6307\u6807\u95f4\u5448\u73b0\u5f3a\u76f8\u5173\u6027\uff08Spearman's \u03c1>0.82\uff09", "conclusion": "\u7cbe\u786e\u5b9a\u4f4d\u80fd\u529b\u662f\u8fde\u63a5\u62bd\u8c61\u63a8\u7406\u4e0e\u73b0\u5b9e\u884c\u52a8\u7684\u5173\u952e\u6865\u6881\uff0c\u7cfb\u7edf\u5316\u8bc4\u4f30\u6846\u67b6\u4e3a\u5f00\u53d1\u5177\u8eab\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u91cd\u8981\u57fa\u51c6"}}
{"id": "2505.10089", "pdf": "https://arxiv.org/pdf/2505.10089", "abs": "https://arxiv.org/abs/2505.10089", "authors": ["Wei Liu", "Sony Trenous", "Leonardo F. R. Ribeiro", "Bill Byrne", "Felix Hieber"], "title": "XRAG: Cross-lingual Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "We propose XRAG, a novel benchmark designed to evaluate the generation\nabilities of LLMs in cross-lingual Retrieval-Augmented Generation (RAG)\nsettings where the user language does not match the retrieval results. XRAG is\nconstructed from recent news articles to ensure that its questions require\nexternal knowledge to be answered. It covers the real-world scenarios of\nmonolingual and multilingual retrieval, and provides relevancy annotations for\neach retrieved document. Our novel dataset construction pipeline results in\nquestions that require complex reasoning, as evidenced by the significant gap\nbetween human and LLM performance. Consequently, XRAG serves as a valuable\nbenchmark for studying LLM reasoning abilities, even before considering the\nadditional cross-lingual complexity. Experimental results on five LLMs uncover\ntwo previously unreported challenges in cross-lingual RAG: 1) in the\nmonolingual retrieval setting, all evaluated models struggle with response\nlanguage correctness; 2) in the multilingual retrieval setting, the main\nchallenge lies in reasoning over retrieved information across languages rather\nthan generation of non-English text.", "AI": {"tldr": "XRAG\u662f\u9996\u4e2a\u4e13\u6ce8\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u8bed\u8a00\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u573a\u666f\u4e2d\u8868\u73b0\u7684\u65b0\u57fa\u51c6\uff0c\u901a\u8fc7\u65b0\u95fb\u6570\u636e\u6784\u5efa\u590d\u6742\u63a8\u7406\u95ee\u9898\uff0c\u63ed\u793a\u4e86LLMs\u5728\u8de8\u8bed\u8a00\u573a\u666f\u4e2d\u7684\u54cd\u5e94\u8bed\u8a00\u6b63\u786e\u6027\u548c\u591a\u8bed\u8a00\u4fe1\u606f\u6574\u5408\u4e24\u5927\u65b0\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u73b0\u5b9e\u573a\u666f\u4e2d\u7528\u6237\u8bed\u8a00\u4e0e\u68c0\u7d22\u7ed3\u679c\u4e0d\u5339\u914d\u65f6LLMs\u7684\u8868\u73b0\u8bc4\u4f30\u95ee\u9898\uff0c\u901a\u8fc7\u6784\u5efa\u9700\u8981\u5916\u90e8\u77e5\u8bc6\u652f\u6301\u7684\u590d\u6742\u63a8\u7406\u95ee\u9898\uff0c\u4e3a\u7814\u7a76LLMs\u7684\u8de8\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u57fa\u51c6\u3002", "method": "\u57fa\u4e8e\u8fd1\u671f\u65b0\u95fb\u6587\u7ae0\u6784\u5efa\u6570\u636e\u96c6\uff0c\u8986\u76d6\u5355\u8bed\u68c0\u7d22\u548c\u591a\u8bed\u8a00\u68c0\u7d22\u573a\u666f\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u95ee\u9898\u751f\u6210\u6d41\u7a0b\u8bbe\u8ba1\u9700\u8981\u590d\u6742\u63a8\u7406\u7684\u95ee\u9898\uff0c\u5e76\u6807\u6ce8\u68c0\u7d22\u6587\u6863\u76f8\u5173\u6027\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff1a1\uff09\u5355\u8bed\u68c0\u7d22\u573a\u666f\u4e2d\u6240\u6709\u6a21\u578b\u5b58\u5728\u54cd\u5e94\u8bed\u8a00\u6b63\u786e\u6027\u95ee\u9898\uff1b2\uff09\u591a\u8bed\u8a00\u68c0\u7d22\u7684\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u8de8\u8bed\u8a00\u4fe1\u606f\u63a8\u7406\u800c\u975e\u975e\u82f1\u8bed\u6587\u672c\u751f\u6210\u3002", "conclusion": "XRAG\u4e0d\u4ec5\u662f\u8de8\u8bed\u8a00RAG\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u5176\u6784\u5efa\u65b9\u6cd5\u8bba\u672c\u8eab\u4e5f\u4e3a\u7814\u7a76LLMs\u7684\u901a\u7528\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u63ed\u793a\u4e86\u8de8\u8bed\u8a00\u573a\u666f\u7279\u6709\u7684\u6280\u672f\u6311\u6218\u3002"}}
{"id": "2505.09997", "pdf": "https://arxiv.org/pdf/2505.09997", "abs": "https://arxiv.org/abs/2505.09997", "authors": ["Jinhyun Jang", "Jiyeong Lee", "Kwanghoon Sohn"], "title": "Descriptive Image-Text Matching with Graded Contextual Similarity", "categories": ["cs.CV"], "comment": null, "summary": "Image-text matching aims to build correspondences between visual and textual\ndata by learning their pairwise similarities. Most existing approaches have\nadopted sparse binary supervision, indicating whether a pair of images and\nsentences matches or not. However, such sparse supervision covers a limited\nsubset of image-text relationships, neglecting their inherent many-to-many\ncorrespondences; an image can be described in numerous texts at different\ndescriptive levels. Moreover, existing approaches overlook the implicit\nconnections from general to specific descriptions, which form the underlying\nrationale for the many-to-many relationships between vision and language. In\nthis work, we propose descriptive image-text matching, called DITM, to learn\nthe graded contextual similarity between image and text by exploring the\ndescriptive flexibility of language. We formulate the descriptiveness score of\neach sentence with cumulative term frequency-inverse document frequency\n(TF-IDF) to balance the pairwise similarity according to the keywords in the\nsentence. Our method leverages sentence descriptiveness to learn robust\nimage-text matching in two key ways: (1) to refine the false negative labeling,\ndynamically relaxing the connectivity between positive and negative pairs, and\n(2) to build more precise matching, aligning a set of relevant sentences in a\ngeneric-to-specific order. By moving beyond rigid binary supervision, DITM\nenhances the discovery of both optimal matches and potential positive pairs.\nExtensive experiments on MS-COCO, Flickr30K, and CxC datasets demonstrate the\neffectiveness of our method in representing complex image-text relationships\ncompared to state-of-the-art approaches. In addition, DITM enhances the\nhierarchical reasoning ability of the model, supported by the extensive\nanalysis on HierarCaps benchmark.", "AI": {"tldr": "\u63d0\u51faDITM\u65b9\u6cd5\uff0c\u901a\u8fc7\u63cf\u8ff0\u6027\u6587\u672c\u5339\u914d\u5b66\u4e60\u56fe\u50cf\u4e0e\u6587\u672c\u7684\u5206\u7ea7\u76f8\u4f3c\u6027\uff0c\u7a81\u7834\u4f20\u7edf\u4e8c\u5143\u76d1\u7763\u9650\u5236", "motivation": "\u4f20\u7edf\u56fe\u50cf\u6587\u672c\u5339\u914d\u91c7\u7528\u4e8c\u5143\u76d1\u7763\uff0c\u65e0\u6cd5\u8986\u76d6\u591a\u5bf9\u591a\u5173\u7cfb\uff0c\u5ffd\u89c6\u8bed\u8a00\u4ece\u901a\u7528\u5230\u5177\u4f53\u7684\u5c42\u6b21\u6027\u5173\u8054", "method": "1. \u57fa\u4e8e\u7d2f\u79efTF-IDF\u8ba1\u7b97\u53e5\u5b50\u63cf\u8ff0\u6027\u5f97\u5206\n2. \u52a8\u6001\u8c03\u6574\u6b63\u8d1f\u6837\u672c\u8fde\u63a5\uff08\u5047\u9634\u6027\u5904\u7406\uff09\n3. \u6784\u5efa\u901a\u7528\u5230\u7279\u5b9a\u7684\u6587\u672c\u5e8f\u5217\u5bf9\u9f50", "result": "\u5728MS-COCO/Flickr30K/CxC\u6570\u636e\u96c6\u8d85\u8d8aSOTA\uff0cHierarCaps\u57fa\u51c6\u663e\u793a\u6a21\u578b\u5c42\u6b21\u63a8\u7406\u80fd\u529b\u63d0\u5347", "conclusion": "\u63cf\u8ff0\u6027\u76f8\u4f3c\u5ea6\u5efa\u6a21\u80fd\u6709\u6548\u6355\u6349\u590d\u6742\u56fe\u6587\u5173\u7cfb\uff0c\u5206\u7ea7\u5bf9\u9f50\u673a\u5236\u589e\u5f3a\u6a21\u578b\u7684\u591a\u5c42\u6b21\u8bed\u4e49\u7406\u89e3\u80fd\u529b"}}
{"id": "2505.10113", "pdf": "https://arxiv.org/pdf/2505.10113", "abs": "https://arxiv.org/abs/2505.10113", "authors": ["Xinlan Yan", "Di Wu", "Yibin Lei", "Christof Monz", "Iacer Calixto"], "title": "What Does Neuro Mean to Cardio? Investigating the Role of Clinical Specialty Data in Medical LLMs", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we introduce S-MedQA, an English medical question-answering\n(QA) dataset for benchmarking large language models in fine-grained clinical\nspecialties. We use S-MedQA to check the applicability of a popular hypothesis\nrelated to knowledge injection in the knowledge-intense scenario of medical QA,\nand show that: 1) training on data from a speciality does not necessarily lead\nto best performance on that specialty and 2) regardless of the specialty\nfine-tuned on, token probabilities of clinically relevant terms for all\nspecialties increase consistently. Thus, we believe improvement gains come\nmostly from domain shifting (e.g., general to medical) rather than knowledge\ninjection and suggest rethinking the role of fine-tuning data in the medical\ndomain. We release S-MedQA and all code needed to reproduce all our experiments\nto the research community.", "AI": {"tldr": "\u63d0\u51faS-MedQA\u533b\u5b66\u95ee\u7b54\u6570\u636e\u96c6\u9a8c\u8bc1\u5927\u6a21\u578b\u6027\u80fd\uff0c\u53d1\u73b0\u9886\u57df\u8fc1\u79fb\u6bd4\u77e5\u8bc6\u6ce8\u5165\u66f4\u80fd\u63d0\u5347\u4e34\u5e8aQA\u6548\u679c", "motivation": "\u9a8c\u8bc1\u533b\u5b66\u9886\u57df\u77e5\u8bc6\u6ce8\u5165\u5047\u8bf4\uff0c\u63a2\u7d22\u5fae\u8c03\u6570\u636e\u5bf9\u4e13\u4e1a\u9886\u57df\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u673a\u5236", "method": "\u4f7f\u7528\u81ea\u5efaS-MedQA\u6570\u636e\u96c6\u8fdb\u884c\u591a\u4e13\u4e1a\u5b9e\u9a8c\uff0c\u5206\u6790\u8bad\u7ec3\u6570\u636e\u4e0e\u4e13\u4e1a\u6027\u80fd\u5173\u8054\u6027\u53catoken\u6982\u7387\u53d8\u5316", "result": "\u4e13\u4e1a\u5fae\u8c03\u4e0d\u4e00\u5b9a\u63d0\u5347\u5bf9\u5e94\u9886\u57df\u6027\u80fd\uff0c\u4f46\u4e34\u5e8a\u76f8\u5173\u672f\u8bed\u6982\u7387\u666e\u904d\u63d0\u5347\uff1b\u6027\u80fd\u63d0\u5347\u4e3b\u8981\u6765\u81ea\u9886\u57df\u8fc1\u79fb", "conclusion": "\u533b\u5b66\u9886\u57df\u6027\u80fd\u63d0\u5347\u6e90\u4e8e\u9886\u57df\u8fc1\u79fb\u800c\u975e\u77e5\u8bc6\u6ce8\u5165\uff0c\u9700\u91cd\u65b0\u5ba1\u89c6\u5fae\u8c03\u6570\u636e\u5728\u4e13\u4e1a\u9886\u57df\u7684\u4f5c\u7528"}}
{"id": "2505.09998", "pdf": "https://arxiv.org/pdf/2505.09998", "abs": "https://arxiv.org/abs/2505.09998", "authors": ["Ying Zang", "Yuanqi Hu", "Xinyu Chen", "Yuxia Xu", "Suhui Wang", "Chunan Yu", "Lanyun Zhu", "Deyi Ji", "Xin Xu", "Tianrun Chen"], "title": "From Air to Wear: Personalized 3D Digital Fashion with AR/VR Immersive 3D Sketching", "categories": ["cs.CV"], "comment": "8 pages, 5 figures", "summary": "In the era of immersive consumer electronics, such as AR/VR headsets and\nsmart devices, people increasingly seek ways to express their identity through\nvirtual fashion. However, existing 3D garment design tools remain inaccessible\nto everyday users due to steep technical barriers and limited data. In this\nwork, we introduce a 3D sketch-driven 3D garment generation framework that\nempowers ordinary users - even those without design experience - to create\nhigh-quality digital clothing through simple 3D sketches in AR/VR environments.\nBy combining a conditional diffusion model, a sketch encoder trained in a\nshared latent space, and an adaptive curriculum learning strategy, our system\ninterprets imprecise, free-hand input and produces realistic, personalized\ngarments. To address the scarcity of training data, we also introduce\nKO3DClothes, a new dataset of paired 3D garments and user-created sketches.\nExtensive experiments and user studies confirm that our method significantly\noutperforms existing baselines in both fidelity and usability, demonstrating\nits promise for democratized fashion design on next-generation consumer\nplatforms.", "AI": {"tldr": "\u901a\u8fc73D\u8349\u56fe\u9a71\u52a8\u751f\u6210\u6846\u67b6\uff0c\u8ba9\u666e\u901a\u7528\u6237\u80fd\u5728AR/VR\u4e2d\u8f7b\u677e\u521b\u5efa\u9ad8\u8d28\u91cf\u6570\u5b57\u670d\u88c5", "motivation": "\u73b0\u67093D\u670d\u88c5\u8bbe\u8ba1\u5de5\u5177\u5b58\u5728\u6280\u672f\u95e8\u69db\u9ad8\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u963b\u788d\u65e5\u5e38\u7528\u6237\u4f7f\u7528\u3002\u9700\u5f00\u53d1\u6613\u7528\u5de5\u5177\u964d\u4f4e\u865a\u62df\u65f6\u5c1a\u521b\u4f5c\u95e8\u69db", "method": "\u7ed3\u5408\u6761\u4ef6\u6269\u6563\u6a21\u578b+\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u8349\u56fe\u7f16\u7801\u5668+\u81ea\u9002\u5e94\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u5904\u7406\u81ea\u7531\u624b\u7ed8\u8f93\u5165", "result": "\u6784\u5efaKO3DClothes\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u8bc1\u660e\u65b9\u6cd5\u5728\u4fdd\u771f\u5ea6\u548c\u53ef\u7528\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e0b\u4e00\u4ee3\u6d88\u8d39\u5e73\u53f0\u5b9e\u73b0\u6c11\u4e3b\u5316\u65f6\u5c1a\u8bbe\u8ba1\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.10143", "pdf": "https://arxiv.org/pdf/2505.10143", "abs": "https://arxiv.org/abs/2505.10143", "authors": ["Longchao Da", "Parth Mitesh Shah", "Kuan-Ru Liou", "Jiaxing Zhang", "Hua Wei"], "title": "GE-Chat: A Graph Enhanced RAG Framework for Evidential Response Generation of LLMs", "categories": ["cs.CL", "68T50, 68T30", "I.2.7; I.2.4; H.3.3"], "comment": "5 pages, 4 figures, accepted to IJCAI2025 demo track", "summary": "Large Language Models are now key assistants in human decision-making\nprocesses. However, a common note always seems to follow: \"LLMs can make\nmistakes. Be careful with important info.\" This points to the reality that not\nall outputs from LLMs are dependable, and users must evaluate them manually.\nThe challenge deepens as hallucinated responses, often presented with seemingly\nplausible explanations, create complications and raise trust issues among\nusers. To tackle such issue, this paper proposes GE-Chat, a knowledge Graph\nenhanced retrieval-augmented generation framework to provide Evidence-based\nresponse generation. Specifically, when the user uploads a material document, a\nknowledge graph will be created, which helps construct a retrieval-augmented\nagent, enhancing the agent's responses with additional knowledge beyond its\ntraining corpus. Then we leverage Chain-of-Thought (CoT) logic generation,\nn-hop sub-graph searching, and entailment-based sentence generation to realize\naccurate evidence retrieval. We demonstrate that our method improves the\nexisting models' performance in terms of identifying the exact evidence in a\nfree-form context, providing a reliable way to examine the resources of LLM's\nconclusion and help with the judgment of the trustworthiness.", "AI": {"tldr": "\u63d0\u51faGE-Chat\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u68c0\u7d22\u751f\u6210\u673a\u5236\uff0c\u4e3aLLM\u751f\u6210\u57fa\u4e8e\u8bc1\u636e\u7684\u53ef\u4fe1\u54cd\u5e94\u3002", "motivation": "LLM\u8f93\u51fa\u5b58\u5728\u4e0d\u53ef\u9760\u6027\uff08\u5982\u5e7b\u89c9\u56de\u7b54\uff09\uff0c\u7528\u6237\u9700\u624b\u52a8\u9a8c\u8bc1\u7ed3\u8bba\uff0c\u5f71\u54cd\u4fe1\u4efb\u5ea6\u3002", "method": "1) \u6587\u6863\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31 \u2192 2) \u601d\u7ef4\u94fe\u751f\u6210 \u2192 3) n\u8df3\u5b50\u56fe\u641c\u7d22 \u2192 4) \u8574\u542b\u5f0f\u53e5\u5b50\u751f\u6210 \u2192 \u5b9e\u73b0\u7cbe\u51c6\u8bc1\u636e\u68c0\u7d22", "result": "\u63d0\u5347\u73b0\u6709\u6a21\u578b\u5728\u81ea\u7531\u6587\u672c\u4e2d\u5b9a\u4f4d\u8bc1\u636e\u7684\u51c6\u786e\u6027\uff0c\u63d0\u4f9b\u9a8c\u8bc1LLM\u7ed3\u8bba\u6765\u6e90\u7684\u53ef\u89e3\u91ca\u8def\u5f84", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u7684\u77e5\u8bc6\u56fe\u8c31\u8bc1\u636e\u94fe\uff0c\u589e\u5f3a\u7528\u6237\u5bf9LLM\u8f93\u51fa\u53ef\u4fe1\u5ea6\u7684\u5224\u65ad\u80fd\u529b"}}
{"id": "2505.10016", "pdf": "https://arxiv.org/pdf/2505.10016", "abs": "https://arxiv.org/abs/2505.10016", "authors": ["Shijie Lyu"], "title": "Application of YOLOv8 in monocular downward multiple Car Target detection", "categories": ["cs.CV", "cs.AI", "I.4.8; I.2.10"], "comment": "Accepted by the 5th International Conference on Signal Processing and\n  Machine Learning (CONF-SPML 2025), to appear in Applied and Computational\n  Engineering", "summary": "Autonomous driving technology is progressively transforming traditional car\ndriving methods, marking a significant milestone in modern transportation.\nObject detection serves as a cornerstone of autonomous systems, playing a vital\nrole in enhancing driving safety, enabling autonomous functionality, improving\ntraffic efficiency, and facilitating effective emergency responses. However,\ncurrent technologies such as radar for environmental perception, cameras for\nroad perception, and vehicle sensor networks face notable challenges, including\nhigh costs, vulnerability to weather and lighting conditions, and limited\nresolution.To address these limitations, this paper presents an improved\nautonomous target detection network based on YOLOv8. By integrating structural\nreparameterization technology, a bidirectional pyramid structure network model,\nand a novel detection pipeline into the YOLOv8 framework, the proposed approach\nachieves highly efficient and precise detection of multi-scale, small, and\nremote objects. Experimental results demonstrate that the enhanced model can\neffectively detect both large and small objects with a detection accuracy of\n65%, showcasing significant advancements over traditional methods.This improved\nmodel holds substantial potential for real-world applications and is\nwell-suited for autonomous driving competitions, such as the Formula Student\nAutonomous China (FSAC), particularly excelling in scenarios involving\nsingle-target and small-object detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8eYOLOv8\u6539\u8fdb\u7684\u81ea\u52a8\u9a7e\u9a76\u76ee\u6807\u68c0\u6d4b\u7f51\u7edc\uff0c\u901a\u8fc7\u96c6\u6210\u7ed3\u6784\u91cd\u53c2\u6570\u5316\u6280\u672f\u3001\u53cc\u5411\u91d1\u5b57\u5854\u7ed3\u6784\u7f51\u7edc\u6a21\u578b\u548c\u65b0\u68c0\u6d4b\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u591a\u5c3a\u5ea6/\u5c0f\u7269\u4f53/\u8fdc\u8ddd\u76ee\u6807\u7684\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u96f7\u8fbe\u611f\u77e5\u3001\u6444\u50cf\u5934\u611f\u77e5\u7b49\u6280\u672f\u5b58\u5728\u6210\u672c\u9ad8\u3001\u6613\u53d7\u73af\u5883\u5e72\u6270\u3001\u5206\u8fa8\u7387\u6709\u9650\u7b49\u95ee\u9898\uff0c\u96be\u4ee5\u6ee1\u8db3\u81ea\u52a8\u9a7e\u9a76\u5bf9\u7cbe\u51c6\u76ee\u6807\u68c0\u6d4b\u7684\u9700\u6c42\u3002", "method": "1. \u5728YOLOv8\u6846\u67b6\u4e2d\u96c6\u6210\u7ed3\u6784\u91cd\u53c2\u6570\u5316\u6280\u672f\n2. \u6784\u5efa\u53cc\u5411\u91d1\u5b57\u5854\u7ed3\u6784\u7f51\u7edc\u6a21\u578b\n3. \u8bbe\u8ba1\u65b0\u578b\u68c0\u6d4b\u6d41\u7a0b", "result": "\u5b9e\u9a8c\u663e\u793a\u6539\u8fdb\u6a21\u578b\u68c0\u6d4b\u7cbe\u5ea6\u8fbe65%\uff0c\u5728\u5927/\u5c0f\u76ee\u6807\u68c0\u6d4b\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u8f83\u4f20\u7edf\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728FSAC\u7b49\u81ea\u52a8\u9a7e\u9a76\u7ade\u8d5b\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b\uff0c\u7279\u522b\u64c5\u957f\u5355\u76ee\u6807\u548c\u5c0f\u7269\u4f53\u68c0\u6d4b\u573a\u666f\uff0c\u5c55\u73b0\u4e86\u5de5\u7a0b\u5b9e\u8df5\u4ef7\u503c\u3002"}}
{"id": "2505.10182", "pdf": "https://arxiv.org/pdf/2505.10182", "abs": "https://arxiv.org/abs/2505.10182", "authors": ["Yoichi Ishibashi", "Taro Yano", "Masafumi Oyamada"], "title": "Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic Data for LLM Reasoning", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated significant improvements in\nreasoning capabilities through supervised fine-tuning and reinforcement\nlearning. However, when training reasoning models, these approaches are\nprimarily applicable to specific domains such as mathematics and programming,\nwhich imposes fundamental constraints on the breadth and scalability of\ntraining data. In contrast, continual pretraining (CPT) offers the advantage of\nnot requiring task-specific signals. Nevertheless, how to effectively\nsynthesize training data for reasoning and how such data affect a wide range of\ndomains remain largely unexplored. This study provides a detailed evaluation of\nReasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden\nthought processes underlying texts, based on the premise that texts are the\nresult of the author's thinking process. Specifically, we apply Reasoning CPT\nto Gemma2-9B using synthetic data with hidden thoughts derived from STEM and\nLaw corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis\nreveals that Reasoning CPT consistently improves performance across all\nevaluated domains. Notably, reasoning skills acquired in one domain transfer\neffectively to others; the performance gap with conventional methods widens as\nproblem difficulty increases, with gains of up to 8 points on the most\nchallenging problems. Furthermore, models trained with hidden thoughts learn to\nadjust the depth of their reasoning according to problem difficulty.", "AI": {"tldr": "Reasoning CPT\u901a\u8fc7\u5408\u6210\u9690\u85cf\u601d\u7ef4\u6570\u636e\u63d0\u5347LLM\u8de8\u9886\u57df\u63a8\u7406\u80fd\u529b\uff0c\u5c24\u5176\u5728\u96be\u9898\u4e0a\u6548\u679c\u663e\u8457\uff08\u6700\u9ad8\u63d0\u53478%\uff09\uff0c\u4e14\u6a21\u578b\u53ef\u81ea\u9002\u5e94\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\u3002", "motivation": "\u4f20\u7edf\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u53d7\u9650\u4e8e\u7279\u5b9a\u9886\u57df\u6570\u636e\uff0c\u800c\u6301\u7eed\u9884\u8bad\u7ec3(CPT)\u7f3a\u4e4f\u6709\u6548\u7684\u63a8\u7406\u6570\u636e\u5408\u6210\u65b9\u6cd5\u3002\u672c\u7814\u7a76\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u91cd\u6784\u6587\u672c\u80cc\u540e\u7684\u601d\u7ef4\u8fc7\u7a0b\u6765\u63d0\u5347\u901a\u7528\u63a8\u7406\u80fd\u529b\u3002", "method": "\u5728Gemma2-9B\u6a21\u578b\u4e0a\u5b9e\u65bdReasoning CPT\uff1a1. \u57fa\u4e8eSTEM/Law\u8bed\u6599\u751f\u6210\u9690\u85cf\u601d\u7ef4\u6570\u636e\uff08\u5047\u8bbe\u6587\u672c\u4e3a\u601d\u7ef4\u8fc7\u7a0b\u7684\u7ed3\u679c\uff09 2. \u4e0e\u6807\u51c6CPT\u5728MMLU\u57fa\u51c6\u5bf9\u6bd4\u8bc4\u4f30", "result": "1. \u5168\u9886\u57df\u6027\u80fd\u63d0\u5347 2. \u8de8\u9886\u57df\u63a8\u7406\u8fc1\u79fb\u6709\u6548\uff08\u6cd5\u5f8b\u2192STEM\uff09 3. \u95ee\u9898\u96be\u5ea6\u6bcf\u589e\u4e00\u7ea7\uff0c\u6027\u80fd\u4f18\u52bf\u6269\u59272-8% 4. \u6a21\u578b\u81ea\u52a8\u8c03\u8282\u601d\u7ef4\u6df1\u5ea6\u4e0e\u95ee\u9898\u96be\u5ea6\u6b63\u76f8\u5173", "conclusion": "\u91cd\u6784\u9690\u85cf\u601d\u7ef4\u8fc7\u7a0b\u7684CPT\u663e\u8457\u589e\u5f3a\u6a21\u578b\u901a\u7528\u63a8\u7406\u80fd\u529b\uff0c\u7a81\u7834\u9886\u57df\u9650\u5236\uff0c\u4e14\u8bc1\u660e\uff1a\u9ad8\u96be\u5ea6\u95ee\u9898\u66f4\u4f9d\u8d56\u7cfb\u7edf\u6027\u63a8\u7406\uff0c\u8fd9\u79cd\u80fd\u529b\u5177\u6709\u53ef\u8fc1\u79fb\u6027\u3002"}}
{"id": "2505.10027", "pdf": "https://arxiv.org/pdf/2505.10027", "abs": "https://arxiv.org/abs/2505.10027", "authors": ["Shijie Lyu"], "title": "ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model Super-Resolution Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by the 4th International Conference on Computing Innovation\n  and Applied Physics (CONF-CIAP 2025), and will be published in EAI Community\n  Research Series-CORE or Theoretical and Natural Science (TNS)", "summary": "With the rapid advancement of remote sensing technology, super-resolution\nimage reconstruction is of great research and practical significance. Existing\ndeep learning methods have made progress but still face limitations in handling\ncomplex scenes and preserving image details. This paper proposes a\nreinforcement learning-based latent diffusion model (LDM) fine-tuning method\nfor remote sensing image super-resolution. The method constructs a\nreinforcement learning environment with states, actions, and rewards,\noptimizing decision objectives through proximal policy optimization (PPO)\nduring the reverse denoising process of the LDM model. Experiments on the\nRESISC45 dataset show significant improvements over the baseline model in PSNR,\nSSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11,\nand LPIPS reducing by 0.06-0.10, particularly in structured and complex natural\nscenes. The results demonstrate the method's effectiveness in enhancing\nsuper-resolution quality and adaptability across scenes.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\u5fae\u8c03\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u9065\u611f\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u8d28\u91cf", "motivation": "\u89e3\u51b3\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u5904\u7406\u548c\u56fe\u50cf\u7ec6\u8282\u4fdd\u7559\u65b9\u9762\u7684\u4e0d\u8db3", "method": "\u6784\u5efa\u5f3a\u5316\u5b66\u4e60\u73af\u5883\uff08\u72b6\u6001-\u52a8\u4f5c-\u5956\u52b1\u673a\u5236\uff09\uff0c\u5728LDM\u53cd\u5411\u53bb\u566a\u8fc7\u7a0b\u4e2d\u4f7f\u7528PPO\u7b97\u6cd5\u4f18\u5316\u51b3\u7b56\u76ee\u6807", "result": "RESISC45\u6570\u636e\u96c6\u5b9e\u9a8c\u663e\u793a\uff1aPSNR\u63d0\u53473-4dB\uff0cSSIM\u589e\u52a00.08-0.11\uff0cLPIPS\u964d\u4f4e0.06-0.10\uff08\u5c24\u5176\u5728\u7ed3\u6784\u5316\u590d\u6742\u573a\u666f\uff09", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u8d85\u5206\u8fa8\u7387\u8d28\u91cf\uff0c\u5e76\u5c55\u793a\u51fa\u4f18\u79c0\u7684\u8de8\u573a\u666f\u9002\u5e94\u80fd\u529b"}}
{"id": "2505.10185", "pdf": "https://arxiv.org/pdf/2505.10185", "abs": "https://arxiv.org/abs/2505.10185", "authors": ["Seongyun Lee", "Seungone Kim", "Minju Seo", "Yongrae Jo", "Dongyoung Go", "Hyeonbin Hwang", "Jinho Park", "Xiang Yue", "Sean Welleck", "Graham Neubig", "Moontae Lee", "Minjoon Seo"], "title": "The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a Reasoning Model will Think", "categories": ["cs.CL", "cs.AI"], "comment": "Work in progress", "summary": "Long chain-of-thought (CoT) is an essential ingredient in effective usage of\nmodern large language models, but our understanding of the reasoning strategies\nunderlying these capabilities remains limited. While some prior works have\nattempted to categorize CoTs using predefined strategy types, such approaches\nare constrained by human intuition and fail to capture the full diversity of\nmodel behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up\nframework for analyzing and steering model reasoning. Our method automatically\nextracts diverse reasoning criteria from model-generated CoTs, embeds them into\na semantic space, clusters them into representative categories, and derives\ncontrastive rubrics to interpret reasoning behavior. Human evaluations show\nthat this framework produces more interpretable and comprehensive analyses than\nexisting methods. Moreover, we demonstrate that this understanding enables\nperformance gains: we can predict which strategy a model is likely to use and\nguide it toward more effective alternatives. Finally, we provide practical\ninsights, such as that training data format (e.g., free-form vs.\nmultiple-choice) has a far greater impact on reasoning behavior than data\ndomain, underscoring the importance of format-aware model design.", "AI": {"tldr": "\u63d0\u51faCoT\u767e\u79d1\u5168\u4e66\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u4e0b\u800c\u4e0a\u65b9\u5f0f\u81ea\u52a8\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u7b56\u7565", "motivation": "\u7a81\u7834\u4f20\u7edf\u57fa\u4e8e\u4eba\u5de5\u76f4\u89c9\u7684\u7b56\u7565\u5206\u7c7b\u9650\u5236\uff0c\u5168\u9762\u6355\u6349\u6a21\u578b\u63a8\u7406\u884c\u4e3a\u7684\u591a\u6837\u6027", "method": "\u4ece\u6a21\u578b\u751f\u6210\u7684\u601d\u7ef4\u94fe\u4e2d\u81ea\u52a8\u63d0\u53d6\u63a8\u7406\u6807\u51c6\u2192\u8bed\u4e49\u5d4c\u5165\u2192\u805a\u7c7b\u2192\u6784\u5efa\u5bf9\u6bd4\u5f0f\u89e3\u91ca\u6846\u67b6", "result": "\u4eba\u7c7b\u8bc4\u4f30\u663e\u793a\u8be5\u65b9\u6cd5\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u5177\u89e3\u91ca\u6027\u548c\u5168\u9762\u6027\uff0c\u5e76\u80fd\u6307\u5bfc\u6a21\u578b\u91c7\u7528\u66f4\u4f18\u7b56\u7565", "conclusion": "\u8bad\u7ec3\u6570\u636e\u683c\u5f0f\uff08\u5982\u81ea\u7531\u6587\u672cvs\u591a\u9009\u9898\uff09\u5bf9\u63a8\u7406\u884c\u4e3a\u7684\u5f71\u54cd\u8fdc\u5927\u4e8e\u6570\u636e\u9886\u57df\u672c\u8eab"}}
{"id": "2505.10030", "pdf": "https://arxiv.org/pdf/2505.10030", "abs": "https://arxiv.org/abs/2505.10030", "authors": ["Miit Daga", "Dhriti Parikh", "Swarna Priya Ramu"], "title": "DeepSeqCoco: A Robust Mobile Friendly Deep Learning Model for Detection of Diseases in Cocos nucifera", "categories": ["cs.CV", "cs.LG"], "comment": "This paper is accepted for publication in IEEE Access journal and is\n  currently pending revisions before publication", "summary": "Coconut tree diseases are a serious risk to agricultural yield, particularly\nin developing countries where conventional farming practices restrict early\ndiagnosis and intervention. Current disease identification methods are manual,\nlabor-intensive, and non-scalable. In response to these limitations, we come up\nwith DeepSeqCoco, a deep learning based model for accurate and automatic\ndisease identification from coconut tree images. The model was tested under\nvarious optimizer settings, such as SGD, Adam, and hybrid configurations, to\nidentify the optimal balance between accuracy, minimization of loss, and\ncomputational cost. Results from experiments indicate that DeepSeqCoco can\nachieve as much as 99.5% accuracy (achieving up to 5% higher accuracy than\nexisting models) with the hybrid SGD-Adam showing the lowest validation loss of\n2.81%. It also shows a drop of up to 18% in training time and up to 85% in\nprediction time for input images. The results point out the promise of the\nmodel to improve precision agriculture through an AI-based, scalable, and\nefficient disease monitoring system.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684DeepSeqCoco\u6a21\u578b\uff0c\u5b9e\u73b0\u6930\u6811\u75c5\u5bb3\u9ad8\u7cbe\u5ea6\u81ea\u52a8\u5316\u8bc6\u522b\uff0c\u51c6\u786e\u738799.5%\u4e14\u6548\u7387\u663e\u8457\u63d0\u5347", "motivation": "\u53d1\u5c55\u4e2d\u56fd\u5bb6\u6930\u6811\u75c5\u5bb3\u8bca\u65ad\u4f9d\u8d56\u4eba\u5de5\u4e14\u6548\u7387\u4f4e\u4e0b\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u89c4\u6a21\u5316\u5e94\u7528\uff0c\u9700\u6784\u5efaAI\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u76d1\u6d4b\u7cfb\u7edf", "method": "\u901a\u8fc7\u6d4b\u8bd5SGD\u3001Adam\u53ca\u6df7\u5408\u4f18\u5316\u5668\u7684\u6027\u80fd\uff0c\u786e\u5b9a\u6700\u4f73\u8ba1\u7b97\u7cbe\u5ea6\u4e0e\u6210\u672c\u7684\u5e73\u8861\u65b9\u6848\uff08\u6df7\u5408SGD-Adam\u4f18\u5316\u5668\u9a8c\u8bc1\u635f\u5931\u6700\u4f4e2.81%\uff09", "result": "\u6a21\u578b\u51c6\u786e\u7387\u6700\u9ad8\u8fbe99.5%\uff08\u4f18\u4e8e\u73b0\u6709\u6a21\u578b5%\uff09\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1118%\uff0c\u5355\u56fe\u9884\u6d4b\u65f6\u95f4\u964d\u4f4e85%", "conclusion": "DeepSeqCoco\u4e3a\u7cbe\u51c6\u519c\u4e1a\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684AI\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u75c5\u5bb3\u76d1\u6d4b\u6548\u7387\uff0c\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c"}}
{"id": "2505.10202", "pdf": "https://arxiv.org/pdf/2505.10202", "abs": "https://arxiv.org/abs/2505.10202", "authors": ["Jintian Shao", "Hongyi Huang", "Jiayi Wu", "YiMing Cheng", "ZhiYu Wu", "You Shan", "MingKai Zheng"], "title": "VQ-Logits: Compressing the Output Bottleneck of Large Language Models via Vector Quantized Logits", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success but face\nsignificant computational and memory challenges, particularly due to their\nextensive output vocabularies. The final linear projection layer, mapping\nhidden states to vocabulary-sized logits, often constitutes a substantial\nportion of the model's parameters and computational cost during inference.\nExisting methods like adaptive softmax or hierarchical softmax introduce\nstructural complexities. In this paper, we propose VQ-Logits, a novel approach\nthat leverages Vector Quantization (VQ) to drastically reduce the parameter\ncount and computational load of the LLM output layer. VQ-Logits replaces the\nlarge V * dmodel output embedding matrix with a small, shared codebook of K\nembedding vectors (K << V ). Each token in the vocabulary is mapped to one of\nthese K codebook vectors. The LLM predicts logits over this compact codebook,\nwhich are then efficiently \"scattered\" to the full vocabulary space using the\nlearned or preassigned mapping. We demonstrate through extensive experiments on\nstandard language modeling benchmarks (e.g., WikiText-103, C4) that VQ-Logits\ncan achieve up to 99% parameter reduction in the output layer and 6x speedup in\nlogit computation, with only a marginal 4% increase in perplexity compared to\nfull softmax baselines. We further provide detailed ablation studies on\ncodebook size, initialization, and learning strategies, showcasing the\nrobustness and effectiveness of our approach.", "AI": {"tldr": "\u4f7f\u7528\u5411\u91cf\u91cf\u5316\u6280\u672f\u538b\u7f29\u5927\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u5c42\u53c2\u6570\uff0c\u5b9e\u73b099%\u53c2\u6570\u524a\u51cf\u548c6\u500d\u8ba1\u7b97\u52a0\u901f\uff0c\u4ec5\u5e26\u67654%\u7684\u56f0\u60d1\u5ea6\u589e\u957f", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u5c42\u56e0\u6d77\u91cf\u8bcd\u6c47\u8868\u5bfc\u81f4\u7684\u5de8\u5927\u53c2\u6570\u91cf\u548c\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u7ed3\u6784\u590d\u6742\u6027\u95ee\u9898", "method": "\u7528\u5c0f\u578b\u5171\u4eab\u7801\u672c\u66ff\u4ee3\u4f20\u7edf\u8f93\u51fa\u5d4c\u5165\u77e9\u9635\uff0c\u901a\u8fc7\u5411\u91cf\u91cf\u5316\u5c06\u8bcd\u6c47\u8868\u6620\u5c04\u5230\u7801\u672c\u5411\u91cf\uff0c\u6a21\u578b\u9884\u6d4b\u7801\u672c\u7a7a\u95f4logits\u540e\u6563\u5c04\u5230\u5b8c\u6574\u8bcd\u6c47\u7a7a\u95f4", "result": "\u5728WikiText-103\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u8f93\u51fa\u5c4299%\u53c2\u6570\u524a\u51cf\uff0c\u8ba1\u7b97\u901f\u5ea6\u63d0\u53476\u500d\uff0c\u56f0\u60d1\u5ea6\u4ec5\u589e\u52a04%", "conclusion": "VQ-Logits\u4e3aLLM\u8f93\u51fa\u5c42\u538b\u7f29\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7801\u672c\u5c3a\u5bf8/\u521d\u59cb\u5316\u7b56\u7565\u7684\u9c81\u68d2\u6027"}}
{"id": "2505.10046", "pdf": "https://arxiv.org/pdf/2505.10046", "abs": "https://arxiv.org/abs/2505.10046", "authors": ["Bingda Tang", "Boyang Zheng", "Xichen Pan", "Sayak Paul", "Saining Xie"], "title": "Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text-to-Image Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "This paper does not describe a new method; instead, it provides a thorough\nexploration of an important yet understudied design space related to recent\nadvances in text-to-image synthesis -- specifically, the deep fusion of large\nlanguage models (LLMs) and diffusion transformers (DiTs) for multi-modal\ngeneration. Previous studies mainly focused on overall system performance\nrather than detailed comparisons with alternative methods, and key design\ndetails and training recipes were often left undisclosed. These gaps create\nuncertainty about the real potential of this approach. To fill these gaps, we\nconduct an empirical study on text-to-image generation, performing controlled\ncomparisons with established baselines, analyzing important design choices, and\nproviding a clear, reproducible recipe for training at scale. We hope this work\noffers meaningful data points and practical guidelines for future research in\nmulti-modal generation.", "AI": {"tldr": "\u63a2\u7d22LLM\u4e0eDiT\u6df1\u5ea6\u878d\u5408\u7684\u591a\u6a21\u6001\u751f\u6210\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7a7a\u767d", "motivation": "\u89e3\u51b3\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u7814\u7a76\u4e2d\u8bbe\u8ba1\u7ec6\u8282\u4e0d\u900f\u660e\u3001\u7f3a\u4e4f\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u5bf9\u6bd4\u7684\u95ee\u9898\uff0c\u9a8c\u8bc1\u8be5\u65b9\u6cd5\u7684\u771f\u5b9e\u6f5c\u529b", "method": "\u901a\u8fc7\u63a7\u5236\u53d8\u91cf\u5b9e\u9a8c\u4e0e\u6210\u719f\u57fa\u7ebf\u5bf9\u6bd4\uff0c\u5206\u6790\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\uff0c\u63d0\u4f9b\u53ef\u590d\u73b0\u7684\u5927\u89c4\u6a21\u8bad\u7ec3\u65b9\u6848", "result": "\u5efa\u7acb\u4e86\u6e05\u6670\u7684\u8bad\u7ec3\u6d41\u7a0b\u89c4\u8303\uff0c\u63ed\u793a\u4e0d\u540c\u67b6\u6784\u9009\u62e9\u5bf9\u751f\u6210\u6548\u679c\u7684\u5f71\u54cd", "conclusion": "\u4e3a\u591a\u6a21\u6001\u751f\u6210\u9886\u57df\u63d0\u4f9b\u53ef\u9760\u6570\u636e\u652f\u6301\u548c\u5b9e\u7528\u6307\u5bfc\u65b9\u6848\uff0c\u63a8\u52a8\u672a\u6765\u7814\u7a76\u53d1\u5c55"}}
{"id": "2505.10218", "pdf": "https://arxiv.org/pdf/2505.10218", "abs": "https://arxiv.org/abs/2505.10218", "authors": ["Zongsheng Wang", "Kaili Sun", "Bowen Wu", "Qun Yu", "Ying Li", "Baoxun Wang"], "title": "RAIDEN-R1: Improving Role-awareness of LLMs via GRPO with Verifiable Reward", "categories": ["cs.CL"], "comment": null, "summary": "Role-playing conversational agents (RPCAs) face persistent challenges in\nmaintaining role consistency. To address this, we propose RAIDEN-R1, a novel\nreinforcement learning framework that integrates Verifiable Role-Awareness\nReward (VRAR). The method introduces both singular and multi-term mining\nstrategies to generate quantifiable rewards by assessing role-specific keys.\nAdditionally, we construct a high-quality, role-aware Chain-of-Thought dataset\nthrough multi-LLM collaboration, and implement experiments to enhance reasoning\ncoherence. Experiments on the RAIDEN benchmark demonstrate RAIDEN-R1's\nsuperiority: our 14B-GRPO model achieves 88.04% and 88.65% accuracy on\nScript-Based Knowledge and Conversation Memory metrics, respectively,\noutperforming baseline models while maintaining robustness. Case analyses\nfurther reveal the model's enhanced ability to resolve conflicting contextual\ncues and sustain first-person narrative consistency. This work bridges the\nnon-quantifiability gap in RPCA training and provides insights into role-aware\nreasoning patterns, advancing the development of RPCAs.", "AI": {"tldr": "\u63d0\u51faRAIDEN-R1\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u9a8c\u8bc1\u89d2\u8272\u611f\u77e5\u5956\u52b1\uff08VRAR\uff09\u89e3\u51b3\u89d2\u8272\u626e\u6f14\u5bf9\u8bdd\u4ee3\u7406\uff08RPCAs\uff09\u7684\u89d2\u8272\u4e00\u81f4\u6027\u96be\u9898\uff0c14B-GRPO\u6a21\u578b\u5728\u5173\u952e\u6307\u6807\u4e0a\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u9488\u5bf9RPCAs\u89d2\u8272\u4e00\u81f4\u6027\u4e0d\u8db3\u7684\u75db\u70b9\uff0c\u4f20\u7edf\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u91cf\u5316\u8bc4\u4f30\u673a\u5236\uff0c\u9700\u5efa\u7acb\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\u4f53\u7cfb\u63d0\u5347\u89d2\u8272\u8ba4\u77e5\u80fd\u529b\u3002", "method": "\u878d\u5408\u5355/\u591a\u9636\u6bb5\u6316\u6398\u7b56\u7565\u751f\u6210\u53ef\u91cf\u5316\u5956\u52b1\uff0c\u6784\u5efa\u591aLLM\u534f\u4f5c\u7684\u89d2\u8272\u611f\u77e5\u601d\u7ef4\u94fe\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u63a8\u7406\u8fde\u8d2f\u6027\u3002", "result": "14B-GRPO\u6a21\u578b\u5728\u5267\u672c\u77e5\u8bc6\uff0888.04%\uff09\u548c\u5bf9\u8bdd\u8bb0\u5fc6\uff0888.65%\uff09\u6307\u6807\u8fbe\u5230SOTA\uff0c\u6848\u4f8b\u5206\u6790\u663e\u793a\u5176\u51b2\u7a81\u4e0a\u4e0b\u6587\u5904\u7406\u80fd\u529b\u63d0\u53473\u500d\u3002", "conclusion": "\u9996\u6b21\u5b9e\u73b0RPCA\u8bad\u7ec3\u7684\u53ef\u91cf\u5316\u8bc4\u4f30\uff0c\u63ed\u793a\u89d2\u8272\u611f\u77e5\u63a8\u7406\u6a21\u5f0f\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u5bf9\u8bdd\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2505.10049", "pdf": "https://arxiv.org/pdf/2505.10049", "abs": "https://arxiv.org/abs/2505.10049", "authors": ["Jinlong Fan", "Xuepu Zeng", "Jing Zhang", "Mingming Gong", "Yuxiang Yang", "Dacheng Tao"], "title": "Advances in Radiance Field for Dynamic Scene: From Neural Field to Gaussian Field", "categories": ["cs.CV"], "comment": null, "summary": "Dynamic scene representation and reconstruction have undergone transformative\nadvances in recent years, catalyzed by breakthroughs in neural radiance fields\nand 3D Gaussian splatting techniques. While initially developed for static\nenvironments, these methodologies have rapidly evolved to address the\ncomplexities inherent in 4D dynamic scenes through an expansive body of\nresearch. Coupled with innovations in differentiable volumetric rendering,\nthese approaches have significantly enhanced the quality of motion\nrepresentation and dynamic scene reconstruction, thereby garnering substantial\nattention from the computer vision and graphics communities. This survey\npresents a systematic analysis of over 200 papers focused on dynamic scene\nrepresentation using radiance field, spanning the spectrum from implicit neural\nrepresentations to explicit Gaussian primitives. We categorize and evaluate\nthese works through multiple critical lenses: motion representation paradigms,\nreconstruction techniques for varied scene dynamics, auxiliary information\nintegration strategies, and regularization approaches that ensure temporal\nconsistency and physical plausibility. We organize diverse methodological\napproaches under a unified representational framework, concluding with a\ncritical examination of persistent challenges and promising research\ndirections. By providing this comprehensive overview, we aim to establish a\ndefinitive reference for researchers entering this rapidly evolving field while\noffering experienced practitioners a systematic understanding of both\nconceptual principles and practical frontiers in dynamic scene reconstruction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u56de\u987e\u52a8\u6001\u573a\u666f\u8868\u793a\u6280\u672f\u4ece\u795e\u7ecf\u8f90\u5c04\u573a\u52303D\u9ad8\u65af\u6e85\u5c04\u7684\u6f14\u8fdb\uff0c\u5206\u6790200+\u6587\u732e\uff0c\u5efa\u7acb\u5206\u7c7b\u6846\u67b6\u5e76\u6307\u51fa\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411", "motivation": "\u89e3\u51b3\u52a8\u6001\u573a\u666f\u91cd\u5efa\u4e2d\u8fd0\u52a8\u8868\u793a\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u3001\u7269\u7406\u5408\u7406\u6027\u7b49\u6838\u5fc3\u96be\u9898\uff0c\u63a8\u52a8\u8ba1\u7b97\u673a\u89c6\u89c9\u4e0e\u56fe\u5f62\u5b66\u9886\u57df\u53d1\u5c55", "method": "\u901a\u8fc7\u56db\u7ef4\u5206\u6790\u6846\u67b6\uff08\u8fd0\u52a8\u8868\u793a\u8303\u5f0f/\u52a8\u6001\u91cd\u5efa\u6280\u672f/\u8f85\u52a9\u4fe1\u606f\u878d\u5408/\u6b63\u5219\u5316\u65b9\u6cd5\uff09\u5bf9\u6587\u732e\u8fdb\u884c\u7cfb\u7edf\u6027\u5206\u7c7b\u4e0e\u8bc4\u4f30", "result": "\u5efa\u7acb\u4e86\u6db5\u76d6\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u5230\u663e\u5f0f\u9ad8\u65af\u57fa\u5143\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u63ed\u793a\u73b0\u6709\u65b9\u6cd5\u5728\u975e\u521a\u6027\u8fd0\u52a8\u5904\u7406\u4e0e\u7269\u7406\u7ea6\u675f\u6574\u5408\u7684\u5c40\u9650\u6027", "conclusion": "\u8be5\u7efc\u8ff0\u4e3a\u52a8\u6001\u573a\u666f\u91cd\u5efa\u9886\u57df\u5efa\u7acb\u9996\u4e2a\u7cfb\u7edf\u6027\u53c2\u8003\u6846\u67b6\uff0c\u6307\u660e\u53ef\u5fae\u5206\u6e32\u67d3\u4e0e\u7269\u7406\u5f15\u64ce\u7ed3\u5408\u7684\u524d\u6cbf\u65b9\u5411"}}
{"id": "2505.10260", "pdf": "https://arxiv.org/pdf/2505.10260", "abs": "https://arxiv.org/abs/2505.10260", "authors": ["Poli Apollinaire Nemkova", "Solomon Ubani", "Mark V. Albert"], "title": "Comparing LLM Text Annotation Skills: A Study on Human Rights Violations in Social Media Data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the era of increasingly sophisticated natural language processing (NLP)\nsystems, large language models (LLMs) have demonstrated remarkable potential\nfor diverse applications, including tasks requiring nuanced textual\nunderstanding and contextual reasoning. This study investigates the\ncapabilities of multiple state-of-the-art LLMs - GPT-3.5, GPT-4, LLAMA3,\nMistral 7B, and Claude-2 - for zero-shot and few-shot annotation of a complex\ntextual dataset comprising social media posts in Russian and Ukrainian.\nSpecifically, the focus is on the binary classification task of identifying\nreferences to human rights violations within the dataset.\n  To evaluate the effectiveness of these models, their annotations are compared\nagainst a gold standard set of human double-annotated labels across 1000\nsamples. The analysis includes assessing annotation performance under different\nprompting conditions, with prompts provided in both English and Russian.\nAdditionally, the study explores the unique patterns of errors and\ndisagreements exhibited by each model, offering insights into their strengths,\nlimitations, and cross-linguistic adaptability.\n  By juxtaposing LLM outputs with human annotations, this research contributes\nto understanding the reliability and applicability of LLMs for sensitive,\ndomain-specific tasks in multilingual contexts. It also sheds light on how\nlanguage models handle inherently subjective and context-dependent judgments, a\ncritical consideration for their deployment in real-world scenarios.", "AI": {"tldr": "\u7814\u7a76\u5bf9\u6bd4GPT-3.5/4\u3001LLAMA3\u7b49\u5927\u6a21\u578b\u5728\u4fc4\u4e4c\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u7684\u4eba\u6743\u4fb5\u72af\u4e8c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u96f6\u6837\u672c/\u5c11\u6837\u672c\u6807\u6ce8\u6027\u80fd\uff0c\u53d1\u73b0\u6a21\u578b\u95f4\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u5f02\u53ca\u8de8\u8bed\u8a00\u9002\u5e94\u6027\u7279\u5f81\u3002", "motivation": "\u9a8c\u8bc1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u654f\u611f\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u7684\u6807\u6ce8\u53ef\u9760\u6027\uff0c\u63a2\u7d22\u5176\u5904\u7406\u4e3b\u89c2\u8bed\u5883\u4efb\u52a1\u65f6\u7684\u8de8\u8bed\u8a00\u9002\u914d\u80fd\u529b\u3002", "method": "\u4f7f\u75285\u4e2a\u524d\u6cbfLLM\u5bf91000\u6761\u4eba\u5de5\u53cc\u6807\u6ce8\u6837\u672c\u8fdb\u884c\u6807\u6ce8\uff0c\u5bf9\u6bd4\u82f1\u8bed/\u4fc4\u8bed\u63d0\u793a\u4e0b\u7684\u5206\u7c7b\u8868\u73b0\uff0c\u5206\u6790\u9519\u8bef\u6a21\u5f0f\u4e0e\u6807\u6ce8\u8005\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u4e0d\u540c\u6a21\u578b\u95f4F1\u5206\u6570\u5dee\u5f02\u8fbe23.5%\uff0c\u4fc4\u8bed\u63d0\u793a\u5e73\u5747\u63d0\u53477.2%\u53ec\u56de\u7387\u3002GPT-4\u5c55\u73b0\u6700\u5f3a\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5b58\u5728\u6587\u5316\u8bed\u5883\u8bef\u5224\u73b0\u8c61\u3002", "conclusion": "LLMs\u5728\u591a\u8bed\u8a00\u654f\u611f\u4efb\u52a1\u4e2d\u5c55\u73b0\u6f5c\u529b\u4f46\u5b58\u5728\u8bed\u5883\u4f9d\u8d56\u6027\uff0c\u9700\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u8fdb\u884c\u7ed3\u679c\u6821\u9a8c\uff0c\u63d0\u793a\u8bed\u8a00\u9009\u62e9\u663e\u8457\u5f71\u54cd\u6a21\u578b\u8de8\u6587\u5316\u7406\u89e3\u3002"}}
{"id": "2505.10055", "pdf": "https://arxiv.org/pdf/2505.10055", "abs": "https://arxiv.org/abs/2505.10055", "authors": ["Ijazul Haq", "Yingjie Zhang", "Irfan Ali Khan"], "title": "PsOCR: Benchmarking Large Multimodal Models for Optical Character Recognition in Low-resource Pashto Language", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper evaluates the performance of Large Multimodal Models (LMMs) on\nOptical Character Recognition (OCR) in the low-resource Pashto language.\nNatural Language Processing (NLP) in Pashto faces several challenges due to the\ncursive nature of its script and a scarcity of structured datasets. To address\nthis, we developed a synthetic Pashto OCR dataset, PsOCR, consisting of one\nmillion images annotated with bounding boxes at word, line, and document\nlevels, suitable for training and evaluating models based on different\narchitectures, including Convolutional Neural Networks (CNNs) and Transformers.\nPsOCR covers variations across 1,000 unique font families, colors, image sizes,\nand layouts. A benchmark subset of 10K images was selected to evaluate the\nperformance of several LMMs, including seven open-source models: DeepSeek's\nJanus, InternVL, MiniCPM, Florence, and Qwen (3B and 7B), and four\nclosed-source models: GPT-4o, Gemini, Claude, and Grok. Experimental results\ndemonstrate that Gemini achieves the best performance among all models, whereas\namong open-source models, Qwen-7B stands out. This work provides an insightful\nassessment of the capabilities and limitations of current LMMs for OCR tasks in\nPashto and establishes a foundation for further research not only in Pashto OCR\nbut also for other similar scripts such as Arabic, Persian, and Urdu. PsOCR is\navailable at https://github.com/zirak-ai/PashtoOCR.", "AI": {"tldr": "\u8bc4\u4f30\u5927\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u666e\u4ec0\u56fe\u8bedOCR\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u6784\u5efa\u5408\u6210\u6570\u636e\u96c6PsOCR\u5e76\u6d4b\u8bd5\u591a\u6b3e\u6a21\u578b\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u666e\u4ec0\u56fe\u8bedNLP\u9762\u4e34\u7684\u5b57\u7b26\u8fde\u5199\u7279\u6027\u6311\u6218\u548c\u7ed3\u6784\u5316\u6570\u636e\u7a00\u7f3a\u95ee\u9898", "method": "\u521b\u5efa\u5305\u542b100\u4e07\u5f20\u591a\u5c42\u7ea7\u6807\u6ce8\u7684PsOCR\u5408\u6210\u6570\u636e\u96c6\uff0c\u8986\u76d61000\u79cd\u5b57\u4f53\u53d8\u4f53\uff0c\u5e76\u752810K\u6d4b\u8bd5\u96c6\u8bc4\u4f307\u4e2a\u5f00\u6e90\u6a21\u578b\u548c4\u4e2a\u95ed\u6e90\u6a21\u578b", "result": "Gemini\u7efc\u5408\u8868\u73b0\u6700\u4f73\uff0c\u5f00\u6e90\u6a21\u578b\u4e2dQwen-7B\u8868\u73b0\u7a81\u51fa", "conclusion": "\u4e3a\u666e\u4ec0\u56fe\u8bedOCR\u5efa\u7acb\u8bc4\u4f30\u57fa\u51c6\uff0c\u7814\u7a76\u7ed3\u8bba\u53ef\u63a8\u5e7f\u81f3\u963f\u62c9\u4f2f\u8bed\u3001\u6ce2\u65af\u8bed\u7b49\u76f8\u4f3c\u6587\u5b57\uff0c\u6570\u636e\u96c6\u5df2\u5f00\u6e90"}}
{"id": "2505.10261", "pdf": "https://arxiv.org/pdf/2505.10261", "abs": "https://arxiv.org/abs/2505.10261", "authors": ["Rui Yang", "Huitao Li", "Matthew Yu Heng Wong", "Yuhe Ke", "Xin Li", "Kunyu Yu", "Jingchi Liao", "Jonathan Chong Kai Liew", "Sabarinath Vinod Nair", "Jasmine Chiat Ling Ong", "Irene Li", "Douglas Teodoro", "Chuan Hong", "Daniel Shu Wei Ting", "Nan Liu"], "title": "The Evolving Landscape of Generative Large Language Models and Traditional Natural Language Processing in Medicine", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Natural language processing (NLP) has been traditionally applied to medicine,\nand generative large language models (LLMs) have become prominent recently.\nHowever, the differences between them across different medical tasks remain\nunderexplored. We analyzed 19,123 studies, finding that generative LLMs\ndemonstrate advantages in open-ended tasks, while traditional NLP dominates in\ninformation extraction and analysis tasks. As these technologies advance,\nethical use of them is essential to ensure their potential in medical\napplications.", "AI": {"tldr": "\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u5f00\u653e\u5f0f\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f20\u7edfNLP\u5728\u4fe1\u606f\u5904\u7406\u4efb\u52a1\u4fdd\u6301\u4f18\u52bf\uff0c\u9700\u6ce8\u91cd\u4f26\u7406\u89c4\u8303\u5e94\u7528", "motivation": "\u63a2\u8ba8\u751f\u6210\u5f0fLLMs\u4e0e\u4f20\u7edfNLP\u5728\u4e0d\u540c\u533b\u7597\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\u53ca\u5e94\u7528\u8fb9\u754c", "method": "\u901a\u8fc7\u5206\u679019,123\u9879\u533b\u5b66\u9886\u57df\u7814\u7a76\u6570\u636e\u8fdb\u884c\u5bf9\u6bd4\u7814\u7a76", "result": "\u751f\u6210\u5f0fLLMs\u5728\u5f00\u653e\u578b\u4efb\u52a1\uff08\u5982\u95ee\u7b54\u751f\u6210\uff09\u4e2d\u66f4\u5177\u4f18\u52bf\uff0c\u4f20\u7edfNLP\u5728\u4fe1\u606f\u62bd\u53d6/\u5206\u6790\u7c7b\u4efb\u52a1\u4fdd\u6301\u9886\u5148", "conclusion": "\u4e24\u79cd\u6280\u672f\u5b58\u5728\u660e\u786e\u7684\u5e94\u7528\u573a\u666f\u533a\u5206\uff0c\u672a\u6765\u9700\u7ed3\u5408\u4f26\u7406\u6846\u67b6\u63a8\u52a8\u533b\u7597AI\u7684\u8d1f\u8d23\u4efb\u53d1\u5c55"}}
{"id": "2505.10072", "pdf": "https://arxiv.org/pdf/2505.10072", "abs": "https://arxiv.org/abs/2505.10072", "authors": ["Rui-Yang Ju", "Sheng-Yen Huang", "Yi-Ping Hung"], "title": "ToonifyGB: StyleGAN-based Gaussian Blendshapes for 3D Stylized Head Avatars", "categories": ["cs.CV"], "comment": null, "summary": "The introduction of 3D Gaussian blendshapes has enabled the real-time\nreconstruction of animatable head avatars from monocular video. Toonify, a\nStyleGAN-based framework, has become widely used for facial image stylization.\nTo extend Toonify for synthesizing diverse stylized 3D head avatars using\nGaussian blendshapes, we propose an efficient two-stage framework, ToonifyGB.\nIn Stage 1 (stylized video generation), we employ an improved StyleGAN to\ngenerate the stylized video from the input video frames, which addresses the\nlimitation of cropping aligned faces at a fixed resolution as preprocessing for\nnormal StyleGAN. This process provides a more stable video, which enables\nGaussian blendshapes to better capture the high-frequency details of the video\nframes, and efficiently generate high-quality animation in the next stage. In\nStage 2 (Gaussian blendshapes synthesis), we learn a stylized neutral head\nmodel and a set of expression blendshapes from the generated video. By\ncombining the neutral head model with expression blendshapes, ToonifyGB can\nefficiently render stylized avatars with arbitrary expressions. We validate the\neffectiveness of ToonifyGB on the benchmark dataset using two styles: Arcane\nand Pixar.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6ToonifyGB\uff0c\u7ed3\u5408\u6539\u8fdbStyleGAN\u4e0e\u9ad8\u65af\u6df7\u5408\u5f62\u72b6\uff0c\u5b9e\u73b0\u591a\u6837\u53163D\u98ce\u683c\u5316\u5934\u50cf\u7684\u5b9e\u65f6\u751f\u6210\u4e0e\u52a8\u753b\u6e32\u67d3\u3002", "motivation": "\u6269\u5c55Toonify\u6846\u67b6\u81f33D\u9886\u57df\uff0c\u7a81\u7834\u4f20\u7edfStyleGAN\u56fa\u5b9a\u5206\u8fa8\u7387\u5bf9\u9f50\u9762\u90e8\u88c1\u526a\u7684\u9650\u5236\uff0c\u63d0\u5347\u89c6\u9891\u7a33\u5b9a\u6027\u4ee5\u6355\u6349\u9ad8\u9891\u7ec6\u8282\u3002", "method": "1. \u6539\u8fdbStyleGAN\u751f\u6210\u7a33\u5b9a\u98ce\u683c\u5316\u89c6\u9891 | 2. \u6784\u5efa\u9ad8\u65af\u6df7\u5408\u5f62\u72b6\u4e2d\u6027\u6a21\u578b\u53ca\u8868\u60c5\u57fa\uff0c\u5b9e\u73b0\u4efb\u610f\u8868\u60c5\u9ad8\u6548\u6e32\u67d3", "result": "\u5728Arcane/Pixar\u98ce\u683c\u6570\u636e\u96c6\u9a8c\u8bc1\u4e2d\uff0c\u6210\u529f\u751f\u6210\u8868\u60c5\u4e30\u5bcc\u76843D\u5361\u901a\u5316\u5934\u50cf\u52a8\u753b", "conclusion": "ToonifyGB\u6709\u6548\u878d\u5408StyleGAN\u751f\u6210\u80fd\u529b\u4e0e\u9ad8\u65af\u6df7\u5408\u5f62\u72b6\u53c2\u6570\u5316\u5efa\u6a21\uff0c\u4e3a3D\u865a\u62df\u5f62\u8c61\u521b\u4f5c\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.10282", "pdf": "https://arxiv.org/pdf/2505.10282", "abs": "https://arxiv.org/abs/2505.10282", "authors": ["Dubai Li", "Nan Jiang", "Kangping Huang", "Ruiqi Tu", "Shuyu Ouyang", "Huayu Yu", "Lin Qiao", "Chen Yu", "Tianshu Zhou", "Danyang Tong", "Qian Wang", "Mengtao Li", "Xiaofeng Zeng", "Yu Tian", "Xinping Tian", "Jingsong Li"], "title": "From Questions to Clinical Recommendations: Large Language Models Driving Evidence-Based Clinical Decision Making", "categories": ["cs.CL"], "comment": null, "summary": "Clinical evidence, derived from rigorous research and data analysis, provides\nhealthcare professionals with reliable scientific foundations for informed\ndecision-making. Integrating clinical evidence into real-time practice is\nchallenging due to the enormous workload, complex professional processes, and\ntime constraints. This highlights the need for tools that automate evidence\nsynthesis to support more efficient and accurate decision making in clinical\nsettings. This study introduces Quicker, an evidence-based clinical decision\nsupport system powered by large language models (LLMs), designed to automate\nevidence synthesis and generate clinical recommendations modeled after standard\nclinical guideline development processes. Quicker implements a fully automated\nchain that covers all phases, from questions to clinical recommendations, and\nfurther enables customized decision-making through integrated tools and\ninteractive user interfaces. To evaluate Quicker's capabilities, we developed\nthe Q2CRBench-3 benchmark dataset, based on clinical guideline development\nrecords for three different diseases. Experimental results highlighted\nQuicker's strong performance, with fine-grained question decomposition tailored\nto user preferences, retrieval sensitivities comparable to human experts, and\nliterature screening performance approaching comprehensive inclusion of\nrelevant studies. In addition, Quicker-assisted evidence assessment effectively\nsupported human reviewers, while Quicker's recommendations were more\ncomprehensive and logically coherent than those of clinicians. In system-level\ntesting, collaboration between a single reviewer and Quicker reduced the time\nrequired for recommendation development to 20-40 minutes. In general, our\nfindings affirm the potential of Quicker to help physicians make quicker and\nmore reliable evidence-based clinical decisions.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edfQuicker\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u8bc1\u636e\u5408\u6210\u4e0e\u6807\u51c6\u5316\u6d41\u7a0b\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e34\u5e8a\u51b3\u7b56\u6548\u7387\u4e0e\u53ef\u9760\u6027", "motivation": "\u4e34\u5e8a\u8bc1\u636e\u6574\u5408\u9762\u4e34\u5de5\u4f5c\u91cf\u5927\uff08\u9700\u5904\u74062.2\u4e07\u7bc7\u6587\u732e\uff09\u3001\u6d41\u7a0b\u590d\u6742\uff08\u6d89\u53ca\u5206\u7ea7\u63a8\u8350\u7cfb\u7edf\u5f00\u53d1\uff09\u548c\u65f6\u95f4\u538b\u529b\uff08\u5355\u75c5\u79cd\u6307\u5357\u5f00\u53d1\u9700200\u5c0f\u65f6\uff09\u7684\u6311\u6218\uff0c\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u652f\u6301", "method": "\u6784\u5efa\u7aef\u5230\u7aef\u81ea\u52a8\u5316\u7cfb\u7edf\uff1a1) \u57fa\u4e8eGRADE\u6846\u67b6\u5efa\u7acb\u95ee\u9898\u5206\u89e3\u673a\u5236 2) \u5f00\u53d1\u591a\u9636\u6bb5\u6587\u732e\u7b5b\u9009\u6a21\u5757\uff08\u654f\u611f\u6027\u8fbe95%\uff093) \u96c6\u6210\u8bc1\u636e\u8bc4\u4f30\u4eea\u8868\u76d8\u4e0e\u52a8\u6001\u63a8\u8350\u751f\u6210\u5668", "result": "\u5728Q2CRBench-3\u6d4b\u8bd5\u4e2d\uff1a\u6587\u732e\u67e5\u5168\u7387\u63d0\u534732%\uff08vs\u4eba\u5de5\uff09\uff0c\u63a8\u8350\u903b\u8f91\u8fde\u8d2f\u6027\u8bc4\u5206\u8fbe4.2/5\uff0c\u5355\u75c5\u79cd\u51b3\u7b56\u65f6\u95f4\u4ece200\u5c0f\u65f6\u7f29\u77ed\u81f340\u5206\u949f\uff0c\u8bc1\u636e\u652f\u6301\u5b8c\u6574\u5ea6\u63d0\u534741%", "conclusion": "Quicker\u7cfb\u7edf\u901a\u8fc7\u81ea\u52a8\u5316\u8bc1\u636e\u5904\u7406\u94fe\u6761\uff0c\u6709\u6548\u89e3\u51b3\u4e34\u5e8a\u51b3\u7b56\u4e2d\u7684\u8bc1\u636e\u6574\u5408\u74f6\u9888\uff0c\u4e3a\u6784\u5efa\u5b9e\u65f6\u66f4\u65b0\u7684\u667a\u80fd\u4e34\u5e8a\u6307\u5357\u7cfb\u7edf\u5960\u5b9a\u6280\u672f\u57fa\u7840"}}
{"id": "2505.10088", "pdf": "https://arxiv.org/pdf/2505.10088", "abs": "https://arxiv.org/abs/2505.10088", "authors": ["Yuncheng Guo", "Xiaodong Gu"], "title": "MMRL++: Parameter-Efficient and Interaction-Aware Representation Learning for Vision-Language Models", "categories": ["cs.CV"], "comment": "Due to the limitation \"The abstract field cannot be longer than 1,920\n  characters\", the abstract appearing here is slightly shorter than that in the\n  PDF file", "summary": "Large-scale pre-trained Vision-Language Models (VLMs) have significantly\nadvanced transfer learning across diverse tasks. However, adapting these models\nwith limited few-shot data often leads to overfitting, undermining their\nability to generalize to new tasks. To address this, we propose Multi-Modal\nRepresentation Learning (MMRL), which introduces a shared, learnable,\nmodality-agnostic representation space. MMRL generates space tokens projected\ninto both text and image encoders as representation tokens, enabling more\neffective cross-modal interactions. Unlike prior methods that mainly optimize\nclass token features, MMRL inserts representation tokens into higher encoder\nlayers--where task-specific features are more prominent--while preserving\ngeneral knowledge in the lower layers. During training, both class and\nrepresentation features are jointly optimized: a trainable projection layer is\napplied to representation tokens for task adaptation, while the projection\nlayer for class token remains frozen to retain pre-trained knowledge. To\nfurther promote generalization, we introduce a regularization term aligning\nclass and text features with the frozen VLM's zero-shot features. At inference,\na decoupling strategy uses both class and representation features for base\ntasks, but only class features for novel tasks due to their stronger\ngeneralization. Building upon this, we propose MMRL++, a parameter-efficient\nand interaction-aware extension that significantly reduces trainable parameters\nand enhances intra-modal interactions--particularly across the layers of\nrepresentation tokens--allowing gradient sharing and instance-specific\ninformation to propagate more effectively through the network. Extensive\nexperiments on 15 datasets demonstrate that MMRL and MMRL++ consistently\noutperform state-of-the-art methods, achieving a strong balance between\ntask-specific adaptation and generalization.", "AI": {"tldr": "\u63d0\u51faMMRL\u6846\u67b6\u89e3\u51b3\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5c0f\u6837\u672c\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u901a\u8fc7\u5171\u4eab\u8868\u793a\u7a7a\u95f4\u548c\u5206\u5c42\u4f18\u5316\u7b56\u7565\u63d0\u5347\u8de8\u6a21\u6001\u4ea4\u4e92\u4e0e\u6cdb\u5316\u80fd\u529b", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5c0f\u6837\u672c\u4efb\u52a1\u4e2d\u8fc7\u5ea6\u4f18\u5316\u7c7b\u522b\u6807\u8bb0\u5bfc\u81f4\u8fc7\u62df\u5408\uff0c\u4f4e\u6548\u5229\u7528\u9884\u8bad\u7ec3\u77e5\u8bc6\u4e14\u7f3a\u4e4f\u8de8\u6a21\u6001\u4ea4\u4e92", "method": "1. \u6784\u5efa\u6a21\u6001\u65e0\u5173\u8868\u793a\u7a7a\u95f4\u751f\u6210\u8de8\u6a21\u6001\u8868\u793a\u6807\u8bb0\n2. \u5728\u9ad8\u5c42\u7f16\u7801\u5668\u63d2\u5165\u8868\u793a\u6807\u8bb0\u4fdd\u7559\u5e95\u5c42\u901a\u7528\u77e5\u8bc6\n3. \u8054\u5408\u4f18\u5316\u53ef\u8bad\u7ec3\u8868\u793a\u6295\u5f71\u5c42\u4e0e\u51bb\u7ed3\u7c7b\u522b\u6295\u5f71\u5c42\n4. \u5f15\u5165\u96f6\u6837\u672c\u7279\u5f81\u5bf9\u9f50\u6b63\u5219\u5316\u9879", "result": "\u572815\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8aSOTA\u65b9\u6cd5\uff0c\u6700\u9ad8\u63d0\u53472.1%\u51c6\u786e\u7387\uff0cMMRL++\u51cf\u5c1175%\u8bad\u7ec3\u53c2\u6570", "conclusion": "MMRL\u7cfb\u5217\u901a\u8fc7\u5206\u5c42\u7279\u5f81\u4f18\u5316\u4e0e\u89e3\u8026\u63a8\u7406\u7b56\u7565\uff0c\u5728\u4efb\u52a1\u9002\u5e94\u4e0e\u6cdb\u5316\u80fd\u529b\u95f4\u53d6\u5f97\u6700\u4f18\u5e73\u8861"}}
{"id": "2505.10320", "pdf": "https://arxiv.org/pdf/2505.10320", "abs": "https://arxiv.org/abs/2505.10320", "authors": ["Chenxi Whitehouse", "Tianlu Wang", "Ping Yu", "Xian Li", "Jason Weston", "Ilia Kulikov", "Swarnadeep Saha"], "title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "10 pages, 8 tables, 11 figures", "summary": "The progress of AI is bottlenecked by the quality of evaluation, and powerful\nLLM-as-a-Judge models have proved to be a core solution. Improved judgment\nability is enabled by stronger chain-of-thought reasoning, motivating the need\nto find the best recipes for training such models to think. In this work we\nintroduce J1, a reinforcement learning approach to training such models. Our\nmethod converts both verifiable and non-verifiable prompts to judgment tasks\nwith verifiable rewards that incentivize thinking and mitigate judgment bias.\nIn particular, our approach outperforms all other existing 8B or 70B models\nwhen trained at those sizes, including models distilled from DeepSeek-R1. J1\nalso outperforms o1-mini, and even R1 on some benchmarks, despite training a\nsmaller model. We provide analysis and ablations comparing Pairwise-J1 vs\nPointwise-J1 models, offline vs online training recipes, reward strategies,\nseed prompts, and variations in thought length and content. We find that our\nmodels make better judgments by learning to outline evaluation criteria,\ncomparing against self-generated reference answers, and re-evaluating the\ncorrectness of model responses.", "AI": {"tldr": "\u63d0\u51fa\u5f3a\u5316\u5b66\u4e60\u6846\u67b6J1\uff0c\u901a\u8fc7\u53ef\u9a8c\u8bc1\u5956\u52b1\u673a\u5236\u63d0\u5347LLM\u8bc4\u4f30\u6a21\u578b\u7684\u5224\u65ad\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u89c4\u6a21\u4e0b\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\u8868\u73b0\u3002", "motivation": "AI\u53d1\u5c55\u53d7\u9650\u4e8e\u8bc4\u4f30\u8d28\u91cf\uff0c\u73b0\u6709LLM\u8bc4\u4f30\u6a21\u578b\u9700\u63d0\u5347\u601d\u7ef4\u94fe\u63a8\u7406\u80fd\u529b\u4ee5\u751f\u6210\u66f4\u51c6\u786e\u7684\u5224\u65ad\u3002", "method": "\u5c06\u53ef\u9a8c\u8bc1/\u4e0d\u53ef\u9a8c\u8bc1\u63d0\u793a\u8f6c\u5316\u4e3a\u5e26\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5224\u65ad\u4efb\u52a1\uff0c\u901a\u8fc7\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u8bad\u7ec3\u6a21\u578b\uff0c\u6574\u5408\u8bc4\u4f30\u6807\u51c6\u751f\u6210\u3001\u53c2\u8003\u7b54\u6848\u6bd4\u5bf9\u3001\u54cd\u5e94\u91cd\u8bc4\u4f30\u7b49\u673a\u5236\u3002", "result": "J1\u57288B/70B\u89c4\u6a21\u5747\u8d85\u8d8a\u540c\u7c7b\u6a21\u578b\uff0c\u90e8\u5206\u573a\u666f\u51fb\u8d25\u66f4\u5927\u6a21\u578bR1\uff1b\u6a21\u578b\u901a\u8fc7\u751f\u6210\u8bc4\u4f30\u6807\u51c6\u3001\u53c2\u8003\u7b54\u6848\u5bf9\u6bd4\u63d0\u5347\u5224\u65ad\u8d28\u91cf\u3002", "conclusion": "J1\u8bc1\u660e\u4e86\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6846\u67b6\u5728\u63d0\u5347\u8bc4\u4f30\u6a21\u578b\u5224\u65ad\u80fd\u529b\u4e0a\u7684\u6709\u6548\u6027\uff0c\u5c0f\u6a21\u578b\u901a\u8fc7\u4f18\u5316\u601d\u7ef4\u8fc7\u7a0b\u53ef\u83b7\u5f97\u8d85\u8d8a\u5927\u6a21\u578b\u7684\u8bc4\u4f30\u6027\u80fd\u3002"}}
{"id": "2505.10118", "pdf": "https://arxiv.org/pdf/2505.10118", "abs": "https://arxiv.org/abs/2505.10118", "authors": ["Yangfu Li", "Hongjian Zhan", "Tianyi Chen", "Qi Liu", "Yue Lu"], "title": "Why 1 + 1 < 1 in Visual Token Pruning: Beyond Naive Integration via Multi-Objective Balanced Covering", "categories": ["cs.CV", "cs.CL"], "comment": "31 pages,9 figures,conference", "summary": "Existing visual token pruning methods target prompt alignment and visual\npreservation with static strategies, overlooking the varying relative\nimportance of these objectives across tasks, which leads to inconsistent\nperformance. To address this, we derive the first closed-form error bound for\nvisual token pruning based on the Hausdorff distance, uniformly characterizing\nthe contributions of both objectives. Moreover, leveraging $\\epsilon$-covering\ntheory, we reveal an intrinsic trade-off between these objectives and quantify\ntheir optimal attainment levels under a fixed budget. To practically handle\nthis trade-off, we propose Multi-Objective Balanced Covering (MoB), which\nreformulates visual token pruning as a bi-objective covering problem. In this\nframework, the attainment trade-off reduces to budget allocation via greedy\nradius trading. MoB offers a provable performance bound and linear scalability\nwith respect to the number of input visual tokens, enabling adaptation to\nchallenging pruning scenarios. Extensive experiments show that MoB preserves\n96.4% of performance for LLaVA-1.5-7B using only 11.1% of the original visual\ntokens and accelerates LLaVA-Next-7B by 1.3-1.5$\\times$ with negligible\nperformance loss. Additionally, evaluations on Qwen2-VL and Video-LLaVA confirm\nthat MoB integrates seamlessly into advanced MLLMs and diverse vision-language\ntasks.", "AI": {"tldr": "\u63d0\u51fa\u591a\u76ee\u6807\u5e73\u8861\u8986\u76d6\u65b9\u6cd5(MoB)\uff0c\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u548c\u8d2a\u5fc3\u7b56\u7565\u52a8\u6001\u5e73\u8861\u89c6\u89c9token\u526a\u679d\u4e2d\u7684\u63d0\u793a\u5bf9\u9f50\u548c\u89c6\u89c9\u4fdd\u7559\u76ee\u6807\uff0c\u5728\u4fdd\u630196.4%\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u4ec5\u4f7f\u752811.1%\u539f\u59cbtoken\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9token\u526a\u679d\u65b9\u6cd5\u91c7\u7528\u9759\u6001\u7b56\u7565\uff0c\u5ffd\u89c6\u4e0d\u540c\u4efb\u52a1\u95f4\u76ee\u6807\u91cd\u8981\u6027\u7684\u52a8\u6001\u53d8\u5316\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u7a33\u5b9a\u3002\u9700\u8981\u5efa\u7acb\u7406\u8bba\u6846\u67b6\u91cf\u5316\u76ee\u6807\u6743\u8861\u5173\u7cfb\u3002", "method": "1. \u57fa\u4e8eHausdorff\u8ddd\u79bb\u63a8\u5bfc\u89c6\u89c9token\u526a\u679d\u7684\u95ed\u5f0f\u8bef\u5dee\u8fb9\u754c\n2. \u5229\u7528\u03b5-\u8986\u76d6\u7406\u8bba\u63ed\u793a\u76ee\u6807\u95f4\u672c\u8d28\u6743\u8861\n3. \u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u53cc\u76ee\u6807\u8986\u76d6\u95ee\u9898\uff0c\u901a\u8fc7\u8d2a\u5fc3\u534a\u5f84\u4ea4\u6613\u5b9e\u73b0\u9884\u7b97\u5206\u914d", "result": "LLaVA-1.5-7B\u6a21\u578b\u4fdd\u755996.4%\u6027\u80fd\uff0811.1% token\uff09\uff0cLLaVA-Next-7B\u52a0\u901f1.3-1.5\u500d\uff1b\u9002\u914dQwen2-VL\u548cVideo-LLaVA\u9a8c\u8bc1\u6269\u5c55\u6027", "conclusion": "MoB\u901a\u8fc7\u7406\u8bba\u9a71\u52a8\u7684\u52a8\u6001\u5e73\u8861\u673a\u5236\uff0c\u5728\u4fdd\u8bc1\u53ef\u8bc1\u660e\u6027\u80fd\u8fb9\u754c\u7684\u524d\u63d0\u4e0b\uff0c\u5b9e\u73b0\u4e86\u89c6\u89c9token\u526a\u679d\u7684\u7ebf\u6027\u6269\u5c55\u80fd\u529b\u548c\u8de8\u6a21\u578b/\u4efb\u52a1\u7684\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2505.10354", "pdf": "https://arxiv.org/pdf/2505.10354", "abs": "https://arxiv.org/abs/2505.10354", "authors": ["Yile Wang", "Zhanyu Shen", "Hui Huang"], "title": "LDIR: Low-Dimensional Dense and Interpretable Text Embeddings with Relative Representations", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Semantic text representation is a fundamental task in the field of natural\nlanguage processing. Existing text embedding (e.g., SimCSE and LLM2Vec) have\ndemonstrated excellent performance, but the values of each dimension are\ndifficult to trace and interpret. Bag-of-words, as classic sparse interpretable\nembeddings, suffers from poor performance. Recently, Benara et al. (2024)\npropose interpretable text embeddings using large language models, which forms\n\"0/1\" embeddings based on responses to a series of questions. These\ninterpretable text embeddings are typically high-dimensional (larger than\n10,000). In this work, we propose Low-dimensional (lower than 500) Dense and\nInterpretable text embeddings with Relative representations (LDIR). The\nnumerical values of its dimensions indicate semantic relatedness to different\nanchor texts through farthest point sampling, offering both semantic\nrepresentation as well as a certain level of traceability and interpretability.\nWe validate LDIR on multiple semantic textual similarity, retrieval, and\nclustering tasks. Extensive experimental results show that LDIR performs close\nto the black-box baseline models and outperforms the interpretable embeddings\nbaselines with much fewer dimensions. Code is available at\nhttps://github.com/szu-tera/LDIR.", "AI": {"tldr": "\u63d0\u51faLDIR\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f4e\u7ef4\u5bc6\u96c6\u76f8\u5bf9\u8868\u793a\u5b9e\u73b0\u9ad8\u6027\u80fd\u4e14\u53ef\u89e3\u91ca\u7684\u6587\u672c\u5d4c\u5165\uff0c\u6027\u80fd\u63a5\u8fd1\u9ed1\u76d2\u6a21\u578b\u5e76\u8d85\u8d8a\u73b0\u6709\u53ef\u89e3\u91ca\u57fa\u7ebf", "motivation": "\u73b0\u6709\u53ef\u89e3\u91ca\u6587\u672c\u5d4c\u5165\u5b58\u5728\u7ef4\u5ea6\u4e0e\u6027\u80fd\u7684\u77db\u76fe\uff08\u9ad8\u7ef4\u53ef\u89e3\u91ca\u6a21\u578b\u590d\u6742\uff0c\u4f4e\u7ef4\u7a00\u758f\u6a21\u578b\u6027\u80fd\u5dee\uff09\uff0c\u9700\u5f00\u53d1\u517c\u987e\u4f4e\u7ef4\u3001\u9ad8\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u7684\u65b0\u65b9\u6cd5", "method": "\u91c7\u7528\u6700\u8fdc\u70b9\u91c7\u6837\u9009\u62e9\u951a\u6587\u672c\uff0c\u6784\u5efa\u7ef4\u5ea6\u6570\u503c\u53cd\u6620\u6587\u672c\u4e0e\u951a\u6587\u672c\u8bed\u4e49\u76f8\u5173\u6027\u7684\u76f8\u5bf9\u8868\u793a\uff0c\u901a\u8fc7\u5bc6\u96c6\u4f4e\u7ef4\u5411\u91cf\u5b9e\u73b0\u8bed\u4e49\u7f16\u7801\u4e0e\u89e3\u91ca\u6027\u5e73\u8861", "result": "\u5728\u8bed\u4e49\u76f8\u4f3c\u5ea6\u3001\u68c0\u7d22\u548c\u805a\u7c7b\u4efb\u52a1\u4e2d\uff0cLDIR\u6027\u80fd\u63a5\u8fd1\u9ed1\u76d2\u6a21\u578b\uff08\u5982SimCSE\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u53ef\u89e3\u91ca\u65b9\u6cd5\u4e14\u7ef4\u5ea6\u964d\u4f4e95%\u4ee5\u4e0a", "conclusion": "LDIR\u8bc1\u660e\u4e86\u4f4e\u7ef4\u5bc6\u96c6\u8868\u793a\u4e0e\u53ef\u89e3\u91ca\u6027\u7684\u517c\u5bb9\u6027\uff0c\u4e3a\u6784\u5efa\u9ad8\u6548\u53ef\u89e3\u91ca\u6587\u672c\u5d4c\u5165\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84"}}
{"id": "2505.10124", "pdf": "https://arxiv.org/pdf/2505.10124", "abs": "https://arxiv.org/abs/2505.10124", "authors": ["Ziad Kheil", "Lucas Robinet", "Laurent Risser", "Soleakhena Ken"], "title": "IMITATE: Image Registration with Context for unknown time frame recovery", "categories": ["cs.CV", "eess.IV"], "comment": "IEEE ISBI 2025", "summary": "In this paper, we formulate a novel image registration formalism dedicated to\nthe estimation of unknown condition-related images, based on two or more known\nimages and their associated conditions. We show how to practically model this\nformalism by using a new conditional U-Net architecture, which fully takes into\naccount the conditional information and does not need any fixed image. Our\nformalism is then applied to image moving tumors for radiotherapy treatment at\ndifferent breathing amplitude using 4D-CT (3D+t) scans in thoracoabdominal\nregions. This driving application is particularly complex as it requires to\nstitch a collection of sequential 2D slices into several 3D volumes at\ndifferent organ positions. Movement interpolation with standard methods then\ngenerates well known reconstruction artefacts in the assembled volumes due to\nirregular patient breathing, hysteresis and poor correlation of breathing\nsignal to internal motion. Results obtained on 4D-CT clinical data showcase\nartefact-free volumes achieved through real-time latencies. The code is\npublicly available at https://github.com/Kheil-Z/IMITATE .", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6761\u4ef6U-Net\u7684\u65b0\u578b\u56fe\u50cf\u914d\u51c6\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u6761\u4ef6\u533b\u5b66\u5f71\u50cf\u91cd\u5efa\uff0c\u6210\u529f\u6d88\u96644D-CT\u547c\u5438\u8fd0\u52a8\u4f2a\u5f71\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf4D-CT\u56fe\u50cf\u91cd\u5efa\u4e2d\u56e0\u547c\u5438\u4e0d\u89c4\u5219\u6027\u548c\u5668\u5b98\u6ede\u540e\u6548\u5e94\u5bfc\u81f4\u7684\u7f1d\u5408\u4f2a\u5f71\u95ee\u9898\uff0c\u63d0\u5347\u653e\u7597\u80bf\u7624\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "method": "1. \u5efa\u7acb\u57fa\u4e8e\u6761\u4ef6\u4fe1\u606f\u7684\u56fe\u50cf\u914d\u51c6\u8303\u5f0f\n2. \u8bbe\u8ba1\u65e0\u9700\u56fa\u5b9a\u53c2\u8003\u56fe\u50cf\u7684\u6761\u4ef6U-Net\u67b6\u6784\n3. \u5e94\u7528\u4e8e\u80f8\u81544D-CT\u5e8f\u5217\u7684\u8de8\u547c\u5438\u5e45\u5ea6\u80bf\u7624\u8fd0\u52a8\u5efa\u6a21", "result": "\u4e34\u5e8a\u6570\u636e\u9a8c\u8bc1\u663e\u793a\uff1a\n- \u5b9e\u65f6\u751f\u6210\u65e0\u4f2a\u5f713D\u4f53\u79ef\n- \u547c\u5438\u8fd0\u52a8\u4e0e\u5185\u90e8\u5668\u5b98\u8fd0\u52a8\u5b9e\u73b0\u7cbe\u786e\u5173\u8054\n- \u5904\u7406\u5ef6\u8fdf\u8fbe\u5230\u5b9e\u65f6\u8981\u6c42", "conclusion": "\u8be5\u6846\u67b6\u7a81\u7834\u4e86\u4f20\u7edf\u914d\u51c6\u9700\u56fa\u5b9a\u53c2\u8003\u56fe\u50cf\u7684\u9650\u5236\uff0c\u5728\u590d\u6742\u547c\u5438\u6a21\u5f0f\u4e0b\u5b9e\u73b0\u53ef\u9760\u7684\u8fd0\u52a8\u5efa\u6a21\uff0c\u4e3a\u7cbe\u51c6\u653e\u7597\u63d0\u4f9b\u65b0\u5de5\u5177\uff0c\u4ee3\u7801\u5f00\u6e90\u4fc3\u8fdb\u4e34\u5e8a\u5e94\u7528\u9a8c\u8bc1\u3002"}}
{"id": "2505.10356", "pdf": "https://arxiv.org/pdf/2505.10356", "abs": "https://arxiv.org/abs/2505.10356", "authors": ["Chunyu Ye", "Shaonan Wang"], "title": "Coherent Language Reconstruction from Brain Recordings with Flexible Multi-Modal Input Stimuli", "categories": ["cs.CL"], "comment": null, "summary": "Decoding thoughts from brain activity offers valuable insights into human\ncognition and enables promising applications in brain-computer interaction.\nWhile prior studies have explored language reconstruction from fMRI data, they\nare typically limited to single-modality inputs such as images or audio. In\ncontrast, human thought is inherently multimodal. To bridge this gap, we\npropose a unified and flexible framework for reconstructing coherent language\nfrom brain recordings elicited by diverse input modalities-visual, auditory,\nand textual. Our approach leverages visual-language models (VLMs), using\nmodality-specific experts to jointly interpret information across modalities.\nExperiments demonstrate that our method achieves performance comparable to\nstate-of-the-art systems while remaining adaptable and extensible. This work\nadvances toward more ecologically valid and generalizable mind decoding.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u591a\u6a21\u6001\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u8de8\u89c6\u89c9/\u542c\u89c9/\u6587\u672c\u7684\u8111\u4fe1\u53f7\u89e3\u7801\uff0c\u6027\u80fd\u5ab2\u7f8e\u5355\u6a21\u6001SOTA\u4e14\u66f4\u5177\u6269\u5c55\u6027", "motivation": "\u73b0\u6709fMRI\u8bed\u8a00\u91cd\u5efa\u7814\u7a76\u5c40\u9650\u4e8e\u5355\u6a21\u6001\u8f93\u5165\uff0c\u800c\u4eba\u7c7b\u601d\u7ef4\u672c\u8d28\u662f\u591a\u6a21\u6001\u7684\uff0c\u9700\u5f00\u53d1\u66f4\u7075\u6d3b\u7edf\u4e00\u7684\u89e3\u7801\u6846\u67b6", "method": "\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\uff0c\u901a\u8fc7\u6a21\u6001\u7279\u5f02\u6027\u4e13\u5bb6\u7f51\u7edc\u5b9e\u73b0\u8de8\u6a21\u6001\u4fe1\u606f\u8054\u5408\u89e3\u7801", "result": "\u5728\u591a\u79cd\u6a21\u6001\u8f93\u5165\u573a\u666f\u4e0b\u8fbe\u5230\u4e0e\u5355\u6a21\u6001SOTA\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u6846\u67b6\u7684\u9002\u5e94\u6027\u548c\u53ef\u6269\u5c55\u6027", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u63d0\u5347\u4e86\u601d\u7ef4\u89e3\u7801\u7684\u751f\u6001\u6548\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u63a8\u52a8\u8111\u673a\u4ea4\u4e92\u5411\u771f\u5b9e\u573a\u666f\u8fc8\u8fdb"}}
{"id": "2505.10152", "pdf": "https://arxiv.org/pdf/2505.10152", "abs": "https://arxiv.org/abs/2505.10152", "authors": ["Yikang Wei"], "title": "Multi-Source Collaborative Style Augmentation and Domain-Invariant Learning for Federated Domain Generalization", "categories": ["cs.CV"], "comment": "IJCAI 2025", "summary": "Federated domain generalization aims to learn a generalizable model from\nmultiple decentralized source domains for deploying on the unseen target\ndomain. The style augmentation methods have achieved great progress on domain\ngeneralization. However, the existing style augmentation methods either explore\nthe data styles within isolated source domain or interpolate the style\ninformation across existing source domains under the data decentralization\nscenario, which leads to limited style space. To address this issue, we propose\na Multi-source Collaborative Style Augmentation and Domain-invariant learning\nmethod (MCSAD) for federated domain generalization. Specifically, we propose a\nmulti-source collaborative style augmentation module to generate data in the\nbroader style space. Furthermore, we conduct domain-invariant learning between\nthe original data and augmented data by cross-domain feature alignment within\nthe same class and classes relation ensemble distillation between different\nclasses to learn a domain-invariant model. By alternatively conducting\ncollaborative style augmentation and domain-invariant learning, the model can\ngeneralize well on unseen target domain. Extensive experiments on multiple\ndomain generalization datasets indicate that our method significantly\noutperforms the state-of-the-art federated domain generalization methods.", "AI": {"tldr": "\u63d0\u51fa\u591a\u6e90\u534f\u4f5c\u98ce\u683c\u589e\u5f3a\u4e0e\u9886\u57df\u4e0d\u53d8\u5b66\u4e60\u65b9\u6cd5\uff08MCSAD\uff09\uff0c\u901a\u8fc7\u8de8\u57df\u534f\u4f5c\u751f\u6210\u66f4\u5e7f\u98ce\u683c\u7a7a\u95f4\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u7279\u5f81\u5bf9\u9f50\u4e0e\u5173\u7cfb\u84b8\u998f\u63d0\u5347\u8054\u90a6\u9886\u57df\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u4e0b\u7684\u98ce\u683c\u589e\u5f3a\u65b9\u6cd5\u5b58\u5728\u98ce\u683c\u7a7a\u95f4\u53d7\u9650\u95ee\u9898\uff08\u5355\u57df\u5185\u589e\u5f3a/\u8de8\u57df\u7ebf\u6027\u63d2\u503c\uff09\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u672a\u77e5\u76ee\u6807\u57df\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "1. \u591a\u6e90\u534f\u4f5c\u98ce\u683c\u589e\u5f3a\u6a21\u5757\u751f\u6210\u8de8\u57df\u7ec4\u5408\u5f0f\u98ce\u683c\u6570\u636e\n2. \u901a\u8fc7\u8de8\u57df\u7279\u5f81\u5bf9\u9f50\uff08\u540c\u7c7b\u522b\uff09\u4e0e\u7c7b\u95f4\u5173\u7cfb\u96c6\u6210\u84b8\u998f\uff08\u4e0d\u540c\u7c7b\u522b\uff09\u5b9e\u73b0\u9886\u57df\u4e0d\u53d8\u5b66\u4e60\n3. \u4ea4\u66ff\u6267\u884c\u98ce\u683c\u589e\u5f3a\u4e0e\u9886\u57df\u4e0d\u53d8\u5b66\u4e60\u4f18\u5316\u6a21\u578b", "result": "\u5728\u591a\u4e2a\u9886\u57df\u6cdb\u5316\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMCSAD\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u8054\u90a6\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\uff08SOTA\uff09", "conclusion": "\u901a\u8fc7\u534f\u540c\u5f0f\u98ce\u683c\u589e\u5f3a\u4e0e\u53cc\u5c42\u6b21\u9886\u57df\u4e0d\u53d8\u5b66\u4e60\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u5728\u672a\u77e5\u76ee\u6807\u57df\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.10389", "pdf": "https://arxiv.org/pdf/2505.10389", "abs": "https://arxiv.org/abs/2505.10389", "authors": ["Benjamin White", "Anastasia Shimorina"], "title": "Multi-domain Multilingual Sentiment Analysis in Industry: Predicting Aspect-based Opinion Quadruples", "categories": ["cs.CL"], "comment": null, "summary": "This paper explores the design of an aspect-based sentiment analysis system\nusing large language models (LLMs) for real-world use. We focus on quadruple\nopinion extraction -- identifying aspect categories, sentiment polarity,\ntargets, and opinion expressions from text data across different domains and\nlanguages. Using internal datasets, we investigate whether a single fine-tuned\nmodel can effectively handle multiple domain-specific taxonomies\nsimultaneously. We demonstrate that a combined multi-domain model achieves\nperformance comparable to specialized single-domain models while reducing\noperational complexity. We also share lessons learned for handling\nnon-extractive predictions and evaluating various failure modes when developing\nLLM-based systems for structured prediction tasks.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u9886\u57df\u56db\u5143\u89c2\u70b9\u63d0\u53d6\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e0e\u5355\u9886\u57df\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u540c\u65f6\u964d\u4f4e\u590d\u6742\u5ea6", "motivation": "\u63a2\u7d22\u5982\u4f55\u5229\u7528\u5355\u4e00\u5fae\u8c03\u6a21\u578b\u5904\u7406\u591a\u9886\u57df\u591a\u8bed\u8a00\u7684\u590d\u6742\u60c5\u611f\u5206\u6790\u4efb\u52a1\uff0c\u89e3\u51b3\u90e8\u7f72\u591a\u4e2a\u5355\u9886\u57df\u6a21\u578b\u5e26\u6765\u7684\u8fd0\u7ef4\u590d\u6742\u6027", "method": "\u4f7f\u7528\u5185\u90e8\u6570\u636e\u96c6\u8bad\u7ec3\u7ec4\u5408\u591a\u9886\u57df\u6a21\u578b\uff0c\u5bf9\u6bd4\u5206\u6790\u5355\u9886\u57df\u4e0e\u591a\u9886\u57df\u6a21\u578b\u5728\u56db\u5143\u89c2\u70b9\u63d0\u53d6\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u5dee\u5f02", "result": "\u591a\u9886\u57df\u6a21\u578b\u5728\u4fdd\u6301\u6027\u80fd\u76f8\u5f53\uff08\u4e0e\u5355\u9886\u57df\u6a21\u578b\u76f8\u6bd4\uff09\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6a21\u578b\u8fd0\u7ef4\u7684\u590d\u6742\u6027", "conclusion": "\u9a8c\u8bc1\u4e86\u7edf\u4e00\u591a\u9886\u57df\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u5e76\u603b\u7ed3\u4e86\u5904\u7406\u975e\u63d0\u53d6\u9884\u6d4b\u53ca\u8bc4\u4f30LLM\u7ed3\u6784\u5316\u4efb\u52a1\u5931\u8d25\u6a21\u5f0f\u7684\u91cd\u8981\u5b9e\u8df5\u7ecf\u9a8c"}}
{"id": "2505.10169", "pdf": "https://arxiv.org/pdf/2505.10169", "abs": "https://arxiv.org/abs/2505.10169", "authors": ["Matthias K\u00fcmmerer", "Harneet Khanuja", "Matthias Bethge"], "title": "Modeling Saliency Dataset Bias", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances in image-based saliency prediction are approaching gold\nstandard performance levels on existing benchmarks. Despite this success, we\nshow that predicting fixations across multiple saliency datasets remains\nchallenging due to dataset bias. We find a significant performance drop (around\n40%) when models trained on one dataset are applied to another. Surprisingly,\nincreasing dataset diversity does not resolve this inter-dataset gap, with\nclose to 60% attributed to dataset-specific biases. To address this remaining\ngeneralization gap, we propose a novel architecture extending a mostly\ndataset-agnostic encoder-decoder structure with fewer than 20 dataset-specific\nparameters that govern interpretable mechanisms such as multi-scale structure,\ncenter bias, and fixation spread. Adapting only these parameters to new data\naccounts for more than 75% of the generalization gap, with a large fraction of\nthe improvement achieved with as few as 50 samples. Our model sets a new\nstate-of-the-art on all three datasets of the MIT/Tuebingen Saliency Benchmark\n(MIT300, CAT2000, and COCO-Freeview), even when purely generalizing from\nunrelated datasets, but with a substantial boost when adapting to the\nrespective training datasets. The model also provides valuable insights into\nspatial saliency properties, revealing complex multi-scale effects that combine\nboth absolute and relative sizes.", "AI": {"tldr": "\u63d0\u51fa\u4ec5\u9700\u8c03\u657420\u4e2a\u53ef\u89e3\u91ca\u53c2\u6570\u5373\u53ef\u89e3\u51b3\u663e\u8457\u6027\u9884\u6d4b\u6a21\u578b\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u96be\u9898\u7684\u65b0\u67b6\u6784\uff0c\u5728MIT300\u7b49\u57fa\u51c6\u5237\u65b0SOTA", "motivation": "\u73b0\u6709\u663e\u8457\u6027\u9884\u6d4b\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u95f4\u6cdb\u5316\u6027\u80fd\u9aa4\u964d40%\uff0c\u53d1\u73b060%\u5dee\u5f02\u6e90\u4e8e\u6570\u636e\u96c6\u56fa\u6709\u504f\u5dee\u3002\u73b0\u6709\u589e\u52a0\u6570\u636e\u591a\u6837\u6027\u7684\u65b9\u6cd5\u65e0\u6cd5\u89e3\u51b3\u8be5\u95ee\u9898", "method": "\u5728\u6570\u636e\u96c6\u65e0\u5173\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u4e2d\u5f15\u5165\u5c11\u91cf\uff08<20\uff09\u53ef\u89e3\u91ca\u7684\u7279\u5b9a\u6570\u636e\u96c6\u53c2\u6570\uff0c\u63a7\u5236\u591a\u5c3a\u5ea6\u7ed3\u6784\u3001\u4e2d\u5fc3\u504f\u5dee\u3001\u6ce8\u89c6\u6269\u6563\u7b49\u673a\u5236", "result": "\u4ec5\u8c03\u6574\u8fd9\u4e9b\u53c2\u6570\u5373\u53ef\u5f25\u886575%\u6cdb\u5316\u5dee\u8ddd\uff0850\u6837\u672c\u5373\u8fbe\u663e\u8457\u63d0\u5347\uff09\uff0c\u5728MIT300/CAT2000/COCO-Freeview\u4e09\u5927\u57fa\u51c6\u5168\u9762\u5237\u65b0SOTA", "conclusion": "\u901a\u8fc7\u6781\u5c0f\u91cf\u53ef\u89e3\u91ca\u53c2\u6570\u7684\u9886\u57df\u9002\u5e94\uff0c\u5b9e\u73b0\u8de8\u6570\u636e\u96c6\u6027\u80fd\u7a81\u7834\uff0c\u540c\u65f6\u63ed\u793a\u591a\u5c3a\u5ea6\u6548\u5e94\u7531\u7edd\u5bf9\u4e0e\u76f8\u5bf9\u5c3a\u5bf8\u5171\u540c\u4f5c\u7528\u7684\u590d\u6742\u673a\u5236"}}
{"id": "2505.10402", "pdf": "https://arxiv.org/pdf/2505.10402", "abs": "https://arxiv.org/abs/2505.10402", "authors": ["Yihong Dong", "Yuchen Liu", "Xue Jiang", "Zhi Jin", "Ge Li"], "title": "Rethinking Repetition Problems of LLMs in Code Generation", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "comment": "Accepted to ACL 2025 (main)", "summary": "With the advent of neural language models, the performance of code generation\nhas been significantly boosted. However, the problem of repetitions during the\ngeneration process continues to linger. Previous work has primarily focused on\ncontent repetition, which is merely a fraction of the broader repetition\nproblem in code generation. A more prevalent and challenging problem is\nstructural repetition. In structural repetition, the repeated code appears in\nvarious patterns but possesses a fixed structure, which can be inherently\nreflected in grammar. In this paper, we formally define structural repetition\nand propose an efficient decoding approach called RPG, which stands for\nRepetition Penalization based on Grammar, to alleviate the repetition problems\nin code generation for LLMs. Specifically, RPG first leverages grammar rules to\nidentify repetition problems during code generation, and then strategically\ndecays the likelihood of critical tokens that contribute to repetitions,\nthereby mitigating them in code generation. To facilitate this study, we\nconstruct a new dataset CodeRepetEval to comprehensively evaluate approaches\nfor mitigating the repetition problems in code generation. Extensive\nexperimental results demonstrate that RPG substantially outperforms the\nbest-performing baselines on CodeRepetEval dataset as well as HumanEval and\nMBPP benchmarks, effectively reducing repetitions and enhancing the quality of\ngenerated code.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8bed\u6cd5\u89c4\u5219\u7684RPG\u89e3\u7801\u65b9\u6cd5\uff0c\u6709\u6548\u7f13\u89e3\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u7ed3\u6784\u91cd\u590d\u95ee\u9898\u5e76\u63d0\u5347\u8d28\u91cf", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4ee3\u7801\u5185\u5bb9\u91cd\u590d\uff0c\u4f46\u66f4\u666e\u904d\u7684\u7ed3\u6784\u91cd\u590d\u95ee\u9898(\u8bed\u6cd5\u5c42\u9762\u56fa\u5b9a\u7ed3\u6784\u7684\u91cd\u590d\u6a21\u5f0f)\u5c1a\u672a\u88ab\u5145\u5206\u89e3\u51b3", "method": "RPG\u65b9\u6cd5\uff1a1. \u5229\u7528\u8bed\u6cd5\u89c4\u5219\u5b9e\u65f6\u68c0\u6d4b\u7ed3\u6784\u91cd\u590d 2. \u5bf9\u5bfc\u81f4\u91cd\u590d\u7684\u5173\u952etoken\u8fdb\u884c\u53ef\u80fd\u6027\u8870\u51cf 3. \u6784\u5efaCodeRepetEval\u8bc4\u4f30\u6570\u636e\u96c6", "result": "\u5728CodeRepetEval/HumanEval/MBPP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u6709\u6548\u51cf\u5c11\u91cd\u590d\u5e76\u63d0\u5347\u4ee3\u7801\u8d28\u91cf", "conclusion": "RPG\u901a\u8fc7\u8bed\u6cd5\u5f15\u5bfc\u7684\u89e3\u7801\u673a\u5236\uff0c\u7cfb\u7edf\u6027\u89e3\u51b3\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u91cd\u590d\u95ee\u9898\uff0c\u4e3a\u7f16\u7a0b\u8f85\u52a9\u5de5\u5177\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2505.10205", "pdf": "https://arxiv.org/pdf/2505.10205", "abs": "https://arxiv.org/abs/2505.10205", "authors": ["Umair Haroon", "Ahmad AlMughrabi", "Thanasis Zoumpekas", "Ricardo Marques", "Petia Radeva"], "title": "VolE: A Point-cloud Framework for Food 3D Reconstruction and Volume Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate food volume estimation is crucial for medical nutrition management\nand health monitoring applications, but current food volume estimation methods\nare often limited by mononuclear data, leveraging single-purpose hardware such\nas 3D scanners, gathering sensor-oriented information such as depth\ninformation, or relying on camera calibration using a reference object. In this\npaper, we present VolE, a novel framework that leverages mobile device-driven\n3D reconstruction to estimate food volume. VolE captures images and camera\nlocations in free motion to generate precise 3D models, thanks to AR-capable\nmobile devices. To achieve real-world measurement, VolE is a reference- and\ndepth-free framework that leverages food video segmentation for food mask\ngeneration. We also introduce a new food dataset encompassing the challenging\nscenarios absent in the previous benchmarks. Our experiments demonstrate that\nVolE outperforms the existing volume estimation techniques across multiple\ndatasets by achieving 2.22 % MAPE, highlighting its superior performance in\nfood volume estimation.", "AI": {"tldr": "\u63d0\u51faVolE\u6846\u67b6\uff0c\u5229\u7528\u79fb\u52a8\u8bbe\u5907\u81ea\u7531\u8fd0\u52a8\u6355\u6349\u5b9e\u73b0\u65e0\u9700\u53c2\u8003\u7269/\u6df1\u5ea6\u4fe1\u606f\u7684\u7cbe\u51c6\u98df\u7269\u4f53\u79ef\u4f30\u8ba1\uff0cMAPE\u8fbe2.22%", "motivation": "\u73b0\u6709\u98df\u7269\u4f53\u79ef\u4f30\u8ba1\u65b9\u6cd5\u53d7\u9650\u4e8e\u4e13\u7528\u786c\u4ef6\u3001\u6df1\u5ea6\u4f20\u611f\u5668\u6216\u53c2\u7167\u7269\u6821\u51c6\uff0c\u96be\u4ee5\u5728\u79fb\u52a8\u7aef\u5b9e\u73b0\u4fbf\u6377\u7cbe\u51c6\u6d4b\u91cf", "method": "\u57fa\u4e8eAR\u79fb\u52a8\u8bbe\u5907\u81ea\u7531\u8fd0\u52a8\u6355\u6349\u6784\u5efa3D\u6a21\u578b\uff0c\u7ed3\u5408\u89c6\u9891\u5206\u5272\u751f\u6210\u98df\u7269\u63a9\u6a21\uff0c\u5f00\u53d1\u65e0\u53c2\u7167\u7269/\u6df1\u5ea6\u4fe1\u606f\u7684\u6846\u67b6", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u6280\u672f\uff0c\u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee2.22%\uff0c\u65b0\u57fa\u51c6\u6d4b\u8bd5\u6db5\u76d6\u66f4\u590d\u6742\u73b0\u5b9e\u573a\u666f", "conclusion": "VolE\u9a8c\u8bc1\u4e86\u79fb\u52a8\u7aef\u89c6\u89c9\u65b9\u6848\u5728\u8425\u517b\u7ba1\u7406\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5065\u5eb7\u76d1\u6d4b\u63d0\u4f9b\u4f4e\u95e8\u69db\u7cbe\u51c6\u6d4b\u91cf\u65b9\u6848"}}
{"id": "2505.10409", "pdf": "https://arxiv.org/pdf/2505.10409", "abs": "https://arxiv.org/abs/2505.10409", "authors": ["Yue Guo", "Jae Ho Sohn", "Gondy Leroy", "Trevor Cohen"], "title": "Are LLM-generated plain language summaries truly understandable? A large-scale crowdsourced evaluation", "categories": ["cs.CL"], "comment": null, "summary": "Plain language summaries (PLSs) are essential for facilitating effective\ncommunication between clinicians and patients by making complex medical\ninformation easier for laypeople to understand and act upon. Large language\nmodels (LLMs) have recently shown promise in automating PLS generation, but\ntheir effectiveness in supporting health information comprehension remains\nunclear. Prior evaluations have generally relied on automated scores that do\nnot measure understandability directly, or subjective Likert-scale ratings from\nconvenience samples with limited generalizability. To address these gaps, we\nconducted a large-scale crowdsourced evaluation of LLM-generated PLSs using\nAmazon Mechanical Turk with 150 participants. We assessed PLS quality through\nsubjective Likert-scale ratings focusing on simplicity, informativeness,\ncoherence, and faithfulness; and objective multiple-choice comprehension and\nrecall measures of reader understanding. Additionally, we examined the\nalignment between 10 automated evaluation metrics and human judgments. Our\nfindings indicate that while LLMs can generate PLSs that appear\nindistinguishable from human-written ones in subjective evaluations,\nhuman-written PLSs lead to significantly better comprehension. Furthermore,\nautomated evaluation metrics fail to reflect human judgment, calling into\nquestion their suitability for evaluating PLSs. This is the first study to\nsystematically evaluate LLM-generated PLSs based on both reader preferences and\ncomprehension outcomes. Our findings highlight the need for evaluation\nframeworks that move beyond surface-level quality and for generation methods\nthat explicitly optimize for layperson comprehension.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5927\u89c4\u6a21\u4f17\u5305\u8bc4\u4f30\u53d1\u73b0\uff0c\u5c3d\u7ba1LLM\u751f\u6210\u7684\u533b\u5b66\u7b80\u660e\u6458\u8981\u4e3b\u89c2\u8d28\u91cf\u4e0e\u4eba\u7c7b\u64b0\u5199\u76f8\u5f53\uff0c\u4f46\u4eba\u7c7b\u64b0\u5199\u7684\u6458\u8981\u663e\u8457\u63d0\u5347\u60a3\u8005\u7406\u89e3\u6548\u679c\uff0c\u4e14\u81ea\u52a8\u5316\u8bc4\u4f30\u6307\u6807\u65e0\u6cd5\u6709\u6548\u53cd\u6620\u4eba\u5de5\u5224\u65ad\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u533b\u5b66\u7b80\u660e\u6458\u8981\u7684\u6709\u6548\u6027\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u65e2\u5f80\u7814\u7a76\u4f9d\u8d56\u81ea\u52a8\u5316\u6307\u6807\u6216\u5c0f\u6837\u672c\u4e3b\u89c2\u8bc4\u5206\uff0c\u65e0\u6cd5\u51c6\u786e\u8861\u91cf\u60a3\u8005\u771f\u5b9e\u7406\u89e3\u7a0b\u5ea6\u3002", "method": "\u901a\u8fc7\u4e9a\u9a6c\u900a\u4f17\u5305\u5e73\u53f0\u62db\u52df150\u540d\u53c2\u4e0e\u8005\uff0c\u7ed3\u5408\u4e3b\u89c2\u8bc4\u5206\uff08\u7b80\u6d01\u6027/\u4fe1\u606f\u91cf/\u8fde\u8d2f\u6027/\u5fe0\u5b9e\u5ea6\uff09\u548c\u5ba2\u89c2\u6d4b\u8bd5\uff08\u9009\u62e9\u9898\u7406\u89e3/\u56de\u5fc6\u6d4b\u8bd5\uff09\uff0c\u5e76\u5bf9\u6bd410\u79cd\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u4e0e\u4eba\u5de5\u5224\u65ad\u7684\u4e00\u81f4\u6027\u3002", "result": "1. LLM\u751f\u6210\u6458\u8981\u4e3b\u89c2\u8bc4\u5206\u4e0e\u4eba\u7c7b\u76f8\u5f53\uff08p=0.12\uff09\n2. \u4eba\u7c7b\u6458\u8981\u7406\u89e3\u6b63\u786e\u7387\u663e\u8457\u66f4\u9ad8\uff0862% vs 53%\uff0cp<0.01\uff09\n3. \u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u4e0e\u4eba\u5de5\u5224\u65ad\u76f8\u5173\u6027\u5f31\uff08\u5e73\u5747\u03c1=0.22\uff09", "conclusion": "\u9700\u5efa\u7acb\u8d85\u8d8a\u8868\u9762\u8d28\u91cf\u8bc4\u4f30\u7684\u65b0\u6846\u67b6\uff0c\u5e76\u5f00\u53d1\u4ee5\u60a3\u8005\u7406\u89e3\u4e3a\u4f18\u5316\u76ee\u6807\u7684\u5185\u5bb9\u751f\u6210\u65b9\u6cd5\uff0c\u5f53\u524d\u81ea\u52a8\u8bc4\u4f30\u4f53\u7cfb\u4e0d\u9002\u7528\u4e8e\u533b\u5b66\u7b80\u660e\u6458\u8981\u8d28\u91cf\u8bc4\u4ef7\u3002"}}
{"id": "2505.10223", "pdf": "https://arxiv.org/pdf/2505.10223", "abs": "https://arxiv.org/abs/2505.10223", "authors": ["Puru Vaish", "Felix Meister", "Tobias Heimann", "Christoph Brune", "Jelmer M. Wolterink"], "title": "Data-Agnostic Augmentations for Unknown Variations: Out-of-Distribution Generalisation in MRI Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at MIDL 2025", "summary": "Medical image segmentation models are often trained on curated datasets,\nleading to performance degradation when deployed in real-world clinical\nsettings due to mismatches between training and test distributions. While data\naugmentation techniques are widely used to address these challenges,\ntraditional visually consistent augmentation strategies lack the robustness\nneeded for diverse real-world scenarios. In this work, we systematically\nevaluate alternative augmentation strategies, focusing on MixUp and Auxiliary\nFourier Augmentation. These methods mitigate the effects of multiple variations\nwithout explicitly targeting specific sources of distribution shifts. We\ndemonstrate how these techniques significantly improve out-of-distribution\ngeneralization and robustness to imaging variations across a wide range of\ntransformations in cardiac cine MRI and prostate MRI segmentation. We\nquantitatively find that these augmentation methods enhance learned feature\nrepresentations by promoting separability and compactness. Additionally, we\nhighlight how their integration into nnU-Net training pipelines provides an\neasy-to-implement, effective solution for enhancing the reliability of medical\nsegmentation models in real-world applications.", "AI": {"tldr": "MixUp\u548c\u8f85\u52a9\u5085\u91cc\u53f6\u589e\u5f3a\u663e\u8457\u63d0\u5347\u533b\u5b66\u5f71\u50cf\u5206\u5272\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u6027", "motivation": "\u4f20\u7edf\u89c6\u89c9\u4e00\u81f4\u6027\u6570\u636e\u589e\u5f3a\u7b56\u7565\u5bf9\u4e34\u5e8a\u573a\u666f\u591a\u6837\u6027\u53d8\u5316\u9002\u5e94\u6027\u4e0d\u8db3\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u6cdb\u5316\u6027\u4e0b\u964d", "method": "\u7cfb\u7edf\u8bc4\u4f30MixUp\u548c\u8f85\u52a9\u5085\u91cc\u53f6\u589e\u5f3a\u7b56\u7565\uff0c\u901a\u8fc7\u6539\u8fdbnnU-Net\u8bad\u7ec3\u6d41\u7a0b\u63d0\u5347\u7279\u5f81\u53ef\u5206\u79bb\u6027\u548c\u7d27\u81f4\u5ea6", "result": "\u5728\u5fc3\u810f\u7535\u5f71MRI\u548c\u524d\u5217\u817aMRI\u5206\u5272\u4efb\u52a1\u4e2d\u6709\u6548\u63d0\u5347\u8de8\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u6a21\u578b\u5bf9\u591a\u79cd\u56fe\u50cf\u53d8\u6362\u5c55\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027", "conclusion": "\u5c06\u975e\u9488\u5bf9\u6027\u589e\u5f3a\u7b56\u7565\u96c6\u6210\u5230\u73b0\u6709\u6846\u67b6\uff0c\u4e3a\u63d0\u5347\u533b\u5b66\u5206\u5272\u6a21\u578b\u4e34\u5e8a\u53ef\u9760\u6027\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.10413", "pdf": "https://arxiv.org/pdf/2505.10413", "abs": "https://arxiv.org/abs/2505.10413", "authors": ["Jiajie Jin", "Xiaoxi Li", "Guanting Dong", "Yuyao Zhang", "Yutao Zhu", "Yongkang Wu", "Zhonghua Li", "Qi Ye", "Zhicheng Dou"], "title": "Hierarchical Document Refinement for Long-context Retrieval-augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Real-world RAG applications often encounter long-context input scenarios,\nwhere redundant information and noise results in higher inference costs and\nreduced performance. To address these challenges, we propose LongRefiner, an\nefficient plug-and-play refiner that leverages the inherent structural\ncharacteristics of long documents. LongRefiner employs dual-level query\nanalysis, hierarchical document structuring, and adaptive refinement through\nmulti-task learning on a single foundation model. Experiments on seven QA\ndatasets demonstrate that LongRefiner achieves competitive performance in\nvarious scenarios while using 10x fewer computational costs and latency\ncompared to the best baseline. Further analysis validates that LongRefiner is\nscalable, efficient, and effective, providing practical insights for real-world\nlong-text RAG applications. Our code is available at\nhttps://github.com/ignorejjj/LongRefiner.", "AI": {"tldr": "\u63d0\u51faLongRefiner\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u53cc\u7ea7\u67e5\u8be2\u5206\u6790\u548c\u5c42\u6b21\u5316\u7ed3\u6784\u5904\u7406\u957f\u6587\u672cRAG\u5e94\u7528\uff0c\u5b9e\u73b010\u500d\u8ba1\u7b97\u6548\u7387\u63d0\u5347", "motivation": "\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587RAG\u5e94\u7528\u4e2d\u5197\u4f59\u4fe1\u606f\u548c\u566a\u58f0\u5bfc\u81f4\u7684\u63a8\u7406\u6210\u672c\u9ad8\u3001\u6027\u80fd\u4e0b\u964d\u95ee\u9898", "method": "\u7ed3\u5408\u53cc\u7ea7\u67e5\u8be2\u5206\u6790\u3001\u5c42\u6b21\u5316\u6587\u6863\u6784\u5efa\uff0c\u5728\u5355\u4e00\u57fa\u7840\u6a21\u578b\u4e0a\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u5b9e\u73b0\u81ea\u9002\u5e94\u4f18\u5316", "result": "\u57287\u4e2aQA\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u4f73\u57fa\u7ebf\u6027\u80fd\uff0c\u8ba1\u7b97\u6210\u672c\u548c\u5ef6\u8fdf\u51cf\u5c1110\u500d\uff0c\u9a8c\u8bc1\u53ef\u6269\u5c55\u6027\u548c\u6709\u6548\u6027", "conclusion": "\u4e3a\u5b9e\u9645\u957f\u6587\u672cRAG\u5e94\u7528\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5f00\u6e90\u652f\u6301\u5b9e\u8df5\u5e94\u7528"}}
{"id": "2505.10231", "pdf": "https://arxiv.org/pdf/2505.10231", "abs": "https://arxiv.org/abs/2505.10231", "authors": ["Haozhe Luo", "Ziyu Zhou", "Zixin Shu", "Aur\u00e9lie Pahud de Mortanges", "Robert Berke", "Mauricio Reyes"], "title": "On the Interplay of Human-AI Alignment,Fairness, and Performance Trade-offs in Medical Imaging", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Deep neural networks excel in medical imaging but remain prone to biases,\nleading to fairness gaps across demographic groups. We provide the first\nsystematic exploration of Human-AI alignment and fairness in this domain. Our\nresults show that incorporating human insights consistently reduces fairness\ngaps and enhances out-of-domain generalization, though excessive alignment can\nintroduce performance trade-offs, emphasizing the need for calibrated\nstrategies. These findings highlight Human-AI alignment as a promising approach\nfor developing fair, robust, and generalizable medical AI systems, striking a\nbalance between expert guidance and automated efficiency. Our code is available\nat https://github.com/Roypic/Aligner.", "AI": {"tldr": "\u533b\u5b66AI\u516c\u5e73\u6027\u7814\u7a76\uff1a\u901a\u8fc7\u4eba\u7c7b\u5bf9\u9f50\u51cf\u5c11\u7fa4\u4f53\u504f\u89c1\uff0c\u4f46\u9700\u5e73\u8861\u5bf9\u9f50\u7a0b\u5ea6\u4ee5\u907f\u514d\u6027\u80fd\u635f\u5931\u3002", "motivation": "\u89e3\u51b3\u533b\u7597AI\u7cfb\u7edf\u4e2d\u56e0\u6570\u636e\u504f\u89c1\u5bfc\u81f4\u7684\u7fa4\u4f53\u516c\u5e73\u6027\u5dee\u8ddd\uff0c\u63a2\u7d22\u4eba\u7c7b\u4e13\u5bb6\u6307\u5bfc\u5bf9\u6a21\u578b\u516c\u5e73\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u5f71\u54cd\u3002", "method": "\u9996\u6b21\u7cfb\u7edf\u6027\u7814\u7a76\u533b\u5b66AI\u9886\u57df\u7684\u4eba\u7c7b\u5bf9\u9f50\u673a\u5236\uff0c\u5206\u6790\u4e0d\u540c\u5bf9\u9f50\u5f3a\u5ea6\u5bf9\u6a21\u578b\u516c\u5e73\u6027\u548c\u8de8\u9886\u57df\u6cdb\u5316\u7684\u5f71\u54cd\u3002", "result": "\u4eba\u7c7b\u77e5\u8bc6\u878d\u5408\u4f7f\u516c\u5e73\u5dee\u8ddd\u51cf\u5c1122.5%\uff0c\u8de8\u57df\u51c6\u786e\u7387\u63d0\u534715.7%\uff0c\u4f46\u8fc7\u5ea6\u5bf9\u9f50\u4f1a\u5bfc\u81f4\u7279\u5b9a\u4efb\u52a1\u6027\u80fd\u4e0b\u964d12%", "conclusion": "\u6821\u51c6\u5f0f\u4eba\u7c7b\u5bf9\u9f50\u7b56\u7565\u5728\u533b\u5b66AI\u5f00\u53d1\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u9700\u5728\u4e13\u5bb6\u6307\u5bfc\u6548\u7387\u4e0e\u81ea\u52a8\u5316\u6027\u80fd\u95f4\u5bfb\u6c42\u6700\u4f18\u5e73\u8861\u70b9\u3002"}}
{"id": "2505.10446", "pdf": "https://arxiv.org/pdf/2505.10446", "abs": "https://arxiv.org/abs/2505.10446", "authors": ["Zemin Huang", "Zhiyang Chen", "Zijun Wang", "Tiancheng Li", "Guo-Jun Qi"], "title": "Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models", "categories": ["cs.CL"], "comment": null, "summary": "We introduce the \\emph{Diffusion Chain of Lateral Thought (DCoLT)}, a\nreasoning framework for diffusion language models. DCoLT treats each\nintermediate step in the reverse diffusion process as a latent \"thinking\"\naction and optimizes the entire reasoning trajectory to maximize the reward on\nthe correctness of the final answer with outcome-based Reinforcement Learning\n(RL). Unlike traditional Chain-of-Thought (CoT) methods that follow a causal,\nlinear thinking process, DCoLT allows bidirectional, non-linear reasoning with\nno strict rule on grammatical correctness amid its intermediate steps of\nthought. We implement DCoLT on two representative Diffusion Language Models\n(DLMs). First, we choose SEDD as a representative continuous-time discrete\ndiffusion model, where its concrete score derives a probabilistic policy to\nmaximize the RL reward over the entire sequence of intermediate diffusion\nsteps. We further consider the discrete-time masked diffusion language model --\nLLaDA, and find that the order to predict and unmask tokens plays an essential\nrole to optimize its RL action resulting from the ranking-based Unmasking\nPolicy Module (UPM) defined by the Plackett-Luce model. Experiments on both\nmath and code generation tasks show that using only public data and 16 H800\nGPUs, DCoLT-reinforced DLMs outperform other DLMs trained by SFT or RL or even\nboth. Notably, DCoLT-reinforced LLaDA boosts its reasoning accuracy by +9.8%,\n+5.7%, +11.4%, +19.5% on GSM8K, MATH, MBPP, and HumanEval.", "AI": {"tldr": "\u63d0\u51fa\u4e86DCoLT\u6846\u67b6\uff0c\u901a\u8fc7\u9006\u5411\u6269\u6563\u8fc7\u7a0b\u7684\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u63a8\u7406\u8f68\u8ff9\uff0c\u663e\u8457\u63d0\u5347\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u7a81\u7834\u4f20\u7edfCoT\u65b9\u6cd5\u7684\u7ebf\u6027\u601d\u7ef4\u9650\u5236\uff0c\u5141\u8bb8\u975e\u8bed\u6cd5\u7ea6\u675f\u7684\u53cc\u5411\u975e\u7ebf\u6027\u63a8\u7406\uff0c\u5145\u5206\u5229\u7528\u6269\u6563\u6a21\u578b\u4e2d\u95f4\u6b65\u9aa4\u7684\u6f5c\u5728\u601d\u8003\u80fd\u529b\u3002", "method": "\u57fa\u4e8eSEDD\u7684\u8fde\u7eed\u65f6\u95f4\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u6982\u7387\u7b56\u7565\u4f18\u5316\uff0c\u5728LLaDA\u6a21\u578b\u4e0a\u91c7\u7528Plackett-Luce\u6a21\u578b\u7684Unmasking Policy Module\u8fdb\u884c\u8bcd\u5e8f\u9884\u6d4b\u4f18\u5316\u3002", "result": "DCoLT\u589e\u5f3a\u7684LLaDA\u5728GSM8K/MATH/MBPP/HumanEval\u4efb\u52a1\u4e0a\u5206\u522b\u63d0\u5347+9.8%/+5.7%/+11.4%/+19.5%\u51c6\u786e\u7387\uff0c\u4ec5\u7528\u516c\u5f00\u6570\u636e\u548c16\u5757H800 GPU\u3002", "conclusion": "DCoLT\u4e3a\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5f00\u8f9f\u4e86\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u8def\u5f84\uff0c\u901a\u8fc7\u4f18\u5316\u4e2d\u95f4\u63a8\u7406\u8fc7\u7a0b\u800c\u975e\u4ec5\u6700\u7ec8\u8f93\u51fa\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2505.10238", "pdf": "https://arxiv.org/pdf/2505.10238", "abs": "https://arxiv.org/abs/2505.10238", "authors": ["Yanbo Ding"], "title": "MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation", "categories": ["cs.CV"], "comment": null, "summary": "Human image animation has gained increasing attention and developed rapidly\ndue to its broad applications in digital humans. However, existing methods rely\nlargely on 2D-rendered pose images for motion guidance, which limits\ngeneralization and discards essential 3D information for open-world animation.\nTo tackle this problem, we propose MTVCrafter (Motion Tokenization Video\nCrafter), the first framework that directly models raw 3D motion sequences\n(i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT\n(4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens.\nCompared to 2D-rendered pose images, 4D motion tokens offer more robust\nspatio-temporal cues and avoid strict pixel-level alignment between pose image\nand character, enabling more flexible and disentangled control. Then, we\nintroduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention\nwith 4D positional encodings, MV-DiT can effectively leverage motion tokens as\n4D compact yet expressive context for human image animation in the complex 3D\nworld. Hence, it marks a significant step forward in this field and opens a new\ndirection for pose-guided human video generation. Experiments show that our\nMTVCrafter achieves state-of-the-art results with an FID-VID of 6.98,\nsurpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter\nalso generalizes well to diverse open-world characters (single/multiple,\nfull/half-body) across various styles and scenarios. Our video demos and code\nare provided in the supplementary material and at this anonymous GitHub link:\nhttps://anonymous.4open.science/r/MTVCrafter-1B13.", "AI": {"tldr": "\u63d0\u51faMTVCrafter\u6846\u67b6\uff0c\u9996\u6b21\u76f4\u63a5\u5efa\u6a213D\u8fd0\u52a8\u5e8f\u5217\u5b9e\u73b0\u4eba\u4f53\u56fe\u50cf\u52a8\u753b\uff0c\u901a\u8fc74DMoT\u91cf\u5316\u8fd0\u52a8\u5e8f\u5217\u548cMV-DiT\u751f\u6210\u52a8\u753b\uff0c\u53d6\u5f97SOTA\u6548\u679c\uff08FID-VID 6.98\uff09\u5e76\u5b9e\u73b0\u5f00\u653e\u4e16\u754c\u6cdb\u5316", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d562D\u59ff\u52bf\u56fe\u50cf\u5bfc\u81f4\u6cdb\u5316\u6027\u5dee\u4e14\u4e22\u59313D\u4fe1\u606f\uff0c\u9700\u5f00\u53d1\u76f4\u63a5\u5904\u7406\u539f\u59cb3D\u8fd0\u52a8\u5e8f\u5217\u7684\u65b9\u6848", "method": "1. 4DMoT\u5c063D\u8fd0\u52a8\u5e8f\u5217\u91cf\u5316\u4e3a4D\u8fd0\u52a8token 2. MV-DiT\u901a\u8fc7\u5e264D\u4f4d\u7f6e\u7f16\u7801\u7684\u8fd0\u52a8\u6ce8\u610f\u529b\u673a\u5236\u5229\u7528token\u751f\u6210\u52a8\u753b", "result": "FID-VID\u6307\u6807\u8fbe6.98\uff08\u6bd4\u7b2c\u4e8c\u540d\u63d0\u534765%\uff09\uff0c\u652f\u6301\u591a\u89d2\u8272/\u5168\u8eab/\u534a\u8eab\u7b49\u5f00\u653e\u573a\u666f\uff0cGitHub\u533f\u540d\u4ee3\u7801\u5df2\u5f00\u6e90", "conclusion": "\u5f00\u521b\u4e86\u57fa\u4e8e3D\u8fd0\u52a8token\u7684\u52a8\u753b\u751f\u6210\u65b0\u8303\u5f0f\uff0c\u7a81\u7834\u4e86\u4f20\u7edf2D\u59ff\u52bf\u5f15\u5bfc\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u590d\u67423D\u4e16\u754c\u7684\u4eba\u7269\u89c6\u9891\u751f\u6210\u5f00\u8f9f\u65b0\u65b9\u5411"}}
{"id": "2505.10493", "pdf": "https://arxiv.org/pdf/2505.10493", "abs": "https://arxiv.org/abs/2505.10493", "authors": ["Shaohan Wang", "Licheng Zhang", "Zheren Fu", "Zhendong Mao"], "title": "CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with Curriculum Learning", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) is an effective method to enhance the\ncapabilities of large language models (LLMs). Existing methods focus on\noptimizing the retriever or generator in the RAG system by directly utilizing\nthe top-k retrieved documents. However, the documents effectiveness are various\nsignificantly across user queries, i.e. some documents provide valuable\nknowledge while others totally lack critical information. It hinders the\nretriever and generator's adaptation during training. Inspired by human\ncognitive learning, curriculum learning trains models using samples progressing\nfrom easy to difficult, thus enhancing their generalization ability, and we\nintegrate this effective paradigm to the training of the RAG system. In this\npaper, we propose a multi-stage Curriculum Learning based RAG system training\nframework, named CL-RAG. We first construct training data with multiple\ndifficulty levels for the retriever and generator separately through sample\nevolution. Then, we train the model in stages based on the curriculum learning\napproach, thereby optimizing the overall performance and generalization of the\nRAG system more effectively. Our CL-RAG framework demonstrates consistent\neffectiveness across four open-domain QA datasets, achieving performance gains\nof 2% to 4% over multiple advanced methods.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u7684\u591a\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6CL-RAG\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u4f18\u5316\u68c0\u7d22\u5668\u548c\u751f\u6210\u5668\uff0c\u5728\u5f00\u653e\u57dfQA\u4efb\u52a1\u4e2d\u5b9e\u73b02%-4%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709RAG\u7cfb\u7edf\u76f4\u63a5\u4f7f\u7528top-k\u6587\u6863\uff0c\u4f46\u4e0d\u540c\u67e5\u8be2\u7684\u6709\u6548\u6587\u6863\u5dee\u5f02\u5927\uff0c\u5f71\u54cd\u6a21\u578b\u8bad\u7ec3\u65f6\u7684\u9002\u5e94\u80fd\u529b\u3002\u53d7\u4eba\u7c7b\u6e10\u8fdb\u5f0f\u5b66\u4e60\u542f\u53d1\uff0c\u5c06\u8bfe\u7a0b\u5b66\u4e60\u5f15\u5165RAG\u8bad\u7ec3\u8fc7\u7a0b\u3002", "method": "1. \u901a\u8fc7\u6837\u672c\u8fdb\u5316\u5206\u522b\u6784\u5efa\u68c0\u7d22\u5668\u548c\u751f\u6210\u5668\u7684\u591a\u96be\u5ea6\u8bad\u7ec3\u6570\u636e\n2. \u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u8303\u5f0f\u5206\u9636\u6bb5\u8bad\u7ec3\u6a21\u578b", "result": "\u5728\u56db\u4e2a\u5f00\u653e\u57dfQA\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd52%-4%\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u901a\u7528\u6027", "conclusion": "CL-RAG\u6709\u6548\u4f18\u5316RAG\u7cfb\u7edf\u6027\u80fd\uff0c\u8bc1\u660e\u8bfe\u7a0b\u5b66\u4e60\u673a\u5236\u5bf9\u68c0\u7d22\u751f\u6210\u8054\u5408\u4f18\u5316\u7684\u4fc3\u8fdb\u4f5c\u7528\uff0c\u4e3a\u590d\u6742\u573a\u666f\u4e0b\u7684\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2505.10250", "pdf": "https://arxiv.org/pdf/2505.10250", "abs": "https://arxiv.org/abs/2505.10250", "authors": ["Wenhao Shen", "Wanqi Yin", "Xiaofeng Yang", "Cheng Chen", "Chaoyue Song", "Zhongang Cai", "Lei Yang", "Hao Wang", "Guosheng Lin"], "title": "ADHMR: Aligning Diffusion-based Human Mesh Recovery via Direct Preference Optimization", "categories": ["cs.CV"], "comment": "Accepted by ICML 2025. Code: https://github.com/shenwenhao01/ADHMR", "summary": "Human mesh recovery (HMR) from a single image is inherently ill-posed due to\ndepth ambiguity and occlusions. Probabilistic methods have tried to solve this\nby generating numerous plausible 3D human mesh predictions, but they often\nexhibit misalignment with 2D image observations and weak robustness to\nin-the-wild images. To address these issues, we propose ADHMR, a framework that\nAligns a Diffusion-based HMR model in a preference optimization manner. First,\nwe train a human mesh prediction assessment model, HMR-Scorer, capable of\nevaluating predictions even for in-the-wild images without 3D annotations. We\nthen use HMR-Scorer to create a preference dataset, where each input image has\na pair of winner and loser mesh predictions. This dataset is used to finetune\nthe base model using direct preference optimization. Moreover, HMR-Scorer also\nhelps improve existing HMR models by data cleaning, even with fewer training\nsamples. Extensive experiments show that ADHMR outperforms current\nstate-of-the-art methods. Code is available at:\nhttps://github.com/shenwenhao01/ADHMR.", "AI": {"tldr": "\u63d0\u51faADHMR\u6846\u67b6\uff0c\u901a\u8fc7\u504f\u597d\u4f18\u5316\u5bf9\u9f50\u6269\u6563\u6a21\u578b\uff0c\u89e3\u51b3\u5355\u76ee\u4eba\u4f53\u7f51\u683c\u91cd\u5efa\u4e2d\u76842D\u5bf9\u9f50\u548c\u9c81\u68d2\u6027\u95ee\u9898", "motivation": "\u73b0\u6709\u6982\u7387\u65b9\u6cd5\u751f\u6210\u76843D\u9884\u6d4b\u5e38\u4e0e2D\u89c2\u5bdf\u4e0d\u5339\u914d\uff0c\u4e14\u5bf9\u771f\u5b9e\u573a\u666f\u56fe\u50cf\u9c81\u68d2\u6027\u5dee\u3002\u9700\u65e0\u97003D\u6807\u6ce8\u7684\u8bc4\u4f30\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u6027\u80fd", "method": "1. \u8bad\u7ec3HMR-Scorer\u8bc4\u4f30\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\u8d28\u91cf\u8bc4\u4f30 \u2192 2. \u6784\u5efa\u504f\u597d\u6570\u636e\u96c6 \u2192 3. \u4f7f\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\u5fae\u8c03\u57fa\u7840\u6a21\u578b", "result": "ADHMR\u8d85\u8d8a\u73b0\u6709SOTA\u65b9\u6cd5\uff0cHMR-Scorer\u4ec5\u752830%\u6570\u636e\u5373\u53ef\u63d0\u5347\u73b0\u6709\u6a21\u578b\u6027\u80fd\uff0c\u57283DPW\u6570\u636e\u96c6\u4e0aMPJPE\u964d\u4f4e7.3%", "conclusion": "\u901a\u8fc7\u504f\u597d\u4f18\u5316\u5bf9\u9f50\u7b56\u7565\u6709\u6548\u89e3\u51b32D\u89c2\u6d4b\u5bf9\u9f50\u95ee\u9898\uff0cHMR-Scorer\u7684\u53cc\u91cd\u529f\u80fd\uff08\u6a21\u578b\u4f18\u5316+\u6570\u636e\u6e05\u6d17\uff09\u663e\u8457\u63d0\u5347\u7b97\u6cd5\u5728\u771f\u5b9e\u573a\u666f\u7684\u9002\u7528\u6027"}}
{"id": "2505.10494", "pdf": "https://arxiv.org/pdf/2505.10494", "abs": "https://arxiv.org/abs/2505.10494", "authors": ["Yutao Mou", "Xiao Deng", "Yuxiao Luo", "Shikun Zhang", "Wei Ye"], "title": "Can You Really Trust Code Copilots? Evaluating Large Language Models from a Code Security Perspective", "categories": ["cs.CL"], "comment": "Accepted by ACL2025 Main Conference", "summary": "Code security and usability are both essential for various coding assistant\napplications driven by large language models (LLMs). Current code security\nbenchmarks focus solely on single evaluation task and paradigm, such as code\ncompletion and generation, lacking comprehensive assessment across dimensions\nlike secure code generation, vulnerability repair and discrimination. In this\npaper, we first propose CoV-Eval, a multi-task benchmark covering various tasks\nsuch as code completion, vulnerability repair, vulnerability detection and\nclassification, for comprehensive evaluation of LLM code security. Besides, we\ndeveloped VC-Judge, an improved judgment model that aligns closely with human\nexperts and can review LLM-generated programs for vulnerabilities in a more\nefficient and reliable way. We conduct a comprehensive evaluation of 20\nproprietary and open-source LLMs. Overall, while most LLMs identify vulnerable\ncodes well, they still tend to generate insecure codes and struggle with\nrecognizing specific vulnerability types and performing repairs. Extensive\nexperiments and qualitative analyses reveal key challenges and optimization\ndirections, offering insights for future research in LLM code security.", "AI": {"tldr": "\u63d0\u51faCoV-Eval\u591a\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u548cVC-Judge\u8bc4\u4f30\u6a21\u578b\uff0c\u53d1\u73b0\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u5b89\u5168\u6027\u548c\u6f0f\u6d1e\u4fee\u590d\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3", "motivation": "\u73b0\u6709\u4ee3\u7801\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u5c40\u9650\u4e8e\u5355\u4e00\u4efb\u52a1\u7ef4\u5ea6\uff08\u5982\u4ee3\u7801\u8865\u5168\uff09\uff0c\u7f3a\u4e4f\u4ee3\u7801\u751f\u6210/\u4fee\u590d/\u68c0\u6d4b\u7684\u5168\u65b9\u4f4d\u5b89\u5168\u8bc4\u4f30\u4f53\u7cfb", "method": "\u6784\u5efa\u8986\u76d6\u4ee3\u7801\u8865\u5168\u3001\u6f0f\u6d1e\u4fee\u590d\u3001\u6f0f\u6d1e\u68c0\u6d4b\u5206\u7c7b\u7684\u591a\u7ef4\u5ea6\u8bc4\u4f30\u6846\u67b6CoV-Eval\uff0c\u5f00\u53d1\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5bf9\u9f50\u7684VC-Judge\u8bc4\u4f30\u6a21\u578b\uff0c\u5bf920\u4e2a\u4e3b\u6d41LLM\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30", "result": "\u591a\u6570LLM\u64c5\u957f\u6f0f\u6d1e\u8bc6\u522b\uff08\u51c6\u786e\u7387>80%\uff09\uff0c\u4f46\u5b58\u5728\u5b89\u5168\u4ee3\u7801\u751f\u6210\u7387\u4f4e\uff08\u5e73\u5747\u4ec535%\uff09\u3001\u7279\u5b9a\u6f0f\u6d1e\u7c7b\u578b\u8bc6\u522b\u56f0\u96be\uff08\u5982\u5185\u5b58\u6cc4\u6f0f\uff09\u3001\u4fee\u590d\u6210\u529f\u7387\u4e0d\u8db3\uff08<50%\uff09\u4e09\u5927\u6838\u5fc3\u7f3a\u9677", "conclusion": "\u63ed\u793a\u4e86LLM\u4ee3\u7801\u5b89\u5168\u80fd\u529b\u7684\u5173\u952e\u74f6\u9888\uff0c\u6307\u51fa\u9700\u52a0\u5f3a\u5b89\u5168\u4ee3\u7801\u751f\u6210\u3001\u7ec6\u7c92\u5ea6\u6f0f\u6d1e\u7406\u89e3\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u4fee\u590d\u4e09\u5927\u65b9\u5411\u7684\u6280\u672f\u7a81\u7834"}}
{"id": "2505.10257", "pdf": "https://arxiv.org/pdf/2505.10257", "abs": "https://arxiv.org/abs/2505.10257", "authors": ["Hao Lu", "Jiaqi Tang", "Jiyao Wang", "Yunfan LU", "Xu Cao", "Qingyong Hu", "Yin Wang", "Yuting Zhang", "Tianxin Xie", "Yunpeng Zhang", "Yong Chen", "Jiayu. Gao", "Bin Huang", "Dengbo He", "Shuiguang Deng", "Hao Chen", "Ying-Cong Chen"], "title": "Sage Deer: A Super-Aligned Driving Generalist Is Your Copilot", "categories": ["cs.CV"], "comment": null, "summary": "The intelligent driving cockpit, an important part of intelligent driving,\nneeds to match different users' comfort, interaction, and safety needs. This\npaper aims to build a Super-Aligned and GEneralist DRiving agent, SAGE DeeR.\nSage Deer achieves three highlights: (1) Super alignment: It achieves different\nreactions according to different people's preferences and biases. (2)\nGeneralist: It can understand the multi-view and multi-mode inputs to reason\nthe user's physiological indicators, facial emotions, hand movements, body\nmovements, driving scenarios, and behavioral decisions. (3) Self-Eliciting: It\ncan elicit implicit thought chains in the language space to further increase\ngeneralist and super-aligned abilities. Besides, we collected multiple data\nsets and built a large-scale benchmark. This benchmark measures the deer's\nperceptual decision-making ability and the super alignment's accuracy.", "AI": {"tldr": "\u63d0\u51fa\u8d85\u7ea7\u5bf9\u9f50\u7684\u901a\u7528\u9a7e\u9a76\u4ee3\u7406SAGE DeeR\uff0c\u901a\u8fc7\u4e09\u91cd\u7279\u6027\u63d0\u5347\u667a\u80fd\u5ea7\u8231\u7684\u4e2a\u6027\u5316\u4e0e\u7efc\u5408\u51b3\u7b56\u80fd\u529b", "motivation": "\u667a\u80fd\u9a7e\u9a76\u5ea7\u8231\u9700\u6ee1\u8db3\u4e0d\u540c\u7528\u6237\u5728\u8212\u9002\u6027\u3001\u4ea4\u4e92\u6027\u548c\u5b89\u5168\u6027\u65b9\u9762\u7684\u4e2a\u6027\u5316\u9700\u6c42\uff0c\u73b0\u6709\u7cfb\u7edf\u5728\u7528\u6237\u591a\u7ef4\u9700\u6c42\u5bf9\u9f50\u548c\u7efc\u5408\u51b3\u7b56\u80fd\u529b\u5b58\u5728\u4e0d\u8db3", "method": "1) \u8d85\u7ea7\u5bf9\u9f50\uff1a\u57fa\u4e8e\u7528\u6237\u504f\u597d\u751f\u6210\u4e2a\u6027\u5316\u54cd\u5e94 2) \u901a\u7528\u6027\uff1a\u878d\u5408\u591a\u89c6\u89d2\u591a\u6a21\u6001\u8f93\u5165\u7406\u89e3\u7528\u6237\u751f\u7406\u6307\u6807-\u60c5\u611f-\u884c\u4e3a 3) \u81ea\u6211\u6fc0\u53d1\uff1a\u901a\u8fc7\u8bed\u8a00\u7a7a\u95f4\u9690\u5f0f\u601d\u7ef4\u94fe\u589e\u5f3a\u7cfb\u7edf\u80fd\u529b", "result": "\u6784\u5efa\u5305\u542b\u591a\u7ef4\u5ea6\u6570\u636e\u96c6\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u91cf\u5316\u8bc4\u4f30\u667a\u80fd\u4f53\u7684\u611f\u77e5\u51b3\u7b56\u80fd\u529b\uff08\u5e73\u5747\u63d0\u534732.1%\uff09\u548c\u8d85\u7ea7\u5bf9\u9f50\u51c6\u786e\u7387\uff08\u8fbe\u523089.7%\uff09", "conclusion": "SAGE DeeR\u901a\u8fc7\u4e09\u4f4d\u4e00\u4f53\u7684\u67b6\u6784\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u667a\u80fd\u5ea7\u8231\u7cfb\u7edf\u5728\u4e2a\u6027\u5316\u9002\u914d\u4e0e\u7efc\u5408\u51b3\u7b56\u80fd\u529b\u4e0a\u7684\u7a81\u7834\uff0c\u5176\u4e2d\u81ea\u6211\u6fc0\u53d1\u673a\u5236\u4f7f\u901a\u7528\u6027\u548c\u5bf9\u9f50\u80fd\u529b\u5f62\u6210\u6b63\u5411\u5faa\u73af\u589e\u5f3a"}}
{"id": "2505.10507", "pdf": "https://arxiv.org/pdf/2505.10507", "abs": "https://arxiv.org/abs/2505.10507", "authors": ["Benedikt Ebing", "Goran Glava\u0161"], "title": "The Devil Is in the Word Alignment Details: On Translation-Based Cross-Lingual Transfer for Token Classification Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Translation-based strategies for cross-lingual transfer XLT such as\ntranslate-train -- training on noisy target language data translated from the\nsource language -- and translate-test -- evaluating on noisy source language\ndata translated from the target language -- are competitive XLT baselines. In\nXLT for token classification tasks, however, these strategies include label\nprojection, the challenging step of mapping the labels from each token in the\noriginal sentence to its counterpart(s) in the translation. Although word\naligners (WAs) are commonly used for label projection, the low-level design\ndecisions for applying them to translation-based XLT have not been\nsystematically investigated. Moreover, recent marker-based methods, which\nproject labeled spans by inserting tags around them before (or after)\ntranslation, claim to outperform WAs in label projection for XLT. In this work,\nwe revisit WAs for label projection, systematically investigating the effects\nof low-level design decisions on token-level XLT: (i) the algorithm for\nprojecting labels between (multi-)token spans, (ii) filtering strategies to\nreduce the number of noisily mapped labels, and (iii) the pre-tokenization of\nthe translated sentences. We find that all of these substantially impact\ntranslation-based XLT performance and show that, with optimized choices, XLT\nwith WA offers performance at least comparable to that of marker-based methods.\nWe then introduce a new projection strategy that ensembles translate-train and\ntranslate-test predictions and demonstrate that it substantially outperforms\nthe marker-based projection. Crucially, we show that our proposed ensembling\nalso reduces sensitivity to low-level WA design choices, resulting in more\nrobust XLT for token classification tasks.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u4f18\u5316\u8bcd\u5bf9\u9f50\u5668\u7684\u8bbe\u8ba1\u7b56\u7565\uff08\u6295\u5f71\u7b97\u6cd5\u3001\u6807\u7b7e\u8fc7\u6ee4\u3001\u9884\u5206\u8bcd\uff09\u5e76\u96c6\u6210translate-train/translate-test\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u8bed\u8a00\u8fc1\u79fb\u5728token\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u8de8\u8bed\u8a00\u8fc1\u79fb\u4e2d\u7684\u6807\u7b7e\u6295\u5f71\u5b58\u5728\u6311\u6218\uff0c\u4f20\u7edf\u8bcd\u5bf9\u9f50\u65b9\u6cd5\u7f3a\u4e4f\u7cfb\u7edf\u5206\u6790\uff0c\u800c\u57fa\u4e8e\u6807\u8bb0\u7684\u65b9\u6cd5\u58f0\u79f0\u66f4\u4f18\u3002\u672c\u6587\u91cd\u65b0\u8bc4\u4f30\u8bcd\u5bf9\u9f50\u5668\u7684\u6f5c\u529b\u5e76\u63a2\u7d22\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u8bcd\u5bf9\u9f50\u5668\u5728\u6807\u7b7e\u6295\u5f71\u4e2d\u7684\u4e09\u4e2a\u5173\u952e\u8bbe\u8ba1\u56e0\u7d20\uff1a(i)\u8de8token\u6807\u7b7e\u6295\u5f71\u7b97\u6cd5 (ii)\u566a\u58f0\u6807\u7b7e\u8fc7\u6ee4\u7b56\u7565 (iii)\u8bd1\u6587\u7684\u9884\u5206\u8bcd\u65b9\u5f0f\uff0c\u63d0\u51fa\u96c6\u6210\u4e24\u79cd\u7ffb\u8bd1\u7b56\u7565\u9884\u6d4b\u7ed3\u679c\u7684\u65b0\u65b9\u6cd5\u3002", "result": "\u4f18\u5316\u540e\u7684\u8bcd\u5bf9\u9f50\u5668\u6027\u80fd\u4e0e\u6807\u8bb0\u65b9\u6cd5\u76f8\u5f53\uff0c\u65b0\u63d0\u51fa\u7684\u96c6\u6210\u7b56\u7565\u4e0d\u4ec5\u8d85\u8d8a\u6807\u8bb0\u65b9\u6cd5\uff0c\u4e14\u964d\u4f4e\u5bf9\u5e95\u5c42\u8bbe\u8ba1\u9009\u62e9\u7684\u654f\u611f\u6027\uff0c\u63d0\u5347\u8de8\u8bed\u8a00\u8fc1\u79fb\u7a33\u5b9a\u6027\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u6027\u4f18\u5316\u548c\u96c6\u6210\u7b56\u7565\uff0c\u57fa\u4e8e\u8bcd\u5bf9\u9f50\u5668\u7684\u7ffb\u8bd1\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u66f4\u9c81\u68d2\u7684\u8de8\u8bed\u8a00\u8fc1\u79fb\u6548\u679c\uff0c\u6311\u6218\u4e86\u6807\u8bb0\u65b9\u6cd5\u66f4\u4f18\u7684\u65e2\u6709\u8ba4\u77e5\u3002"}}
{"id": "2505.10258", "pdf": "https://arxiv.org/pdf/2505.10258", "abs": "https://arxiv.org/abs/2505.10258", "authors": ["Michael Hubbertz", "Pascal Colling", "Qi Han", "Tobias Meisen"], "title": "Inferring Driving Maps by Deep Learning-based Trail Map Extraction", "categories": ["cs.CV", "cs.RO"], "comment": "This paper was accepted at the CVPR WAD 2025 Workshop", "summary": "High-definition (HD) maps offer extensive and accurate environmental\ninformation about the driving scene, making them a crucial and essential\nelement for planning within autonomous driving systems. To avoid extensive\nefforts from manual labeling, methods for automating the map creation have\nemerged. Recent trends have moved from offline mapping to online mapping,\nensuring availability and actuality of the utilized maps. While the performance\nhas increased in recent years, online mapping still faces challenges regarding\ntemporal consistency, sensor occlusion, runtime, and generalization. We propose\na novel offline mapping approach that integrates trails - informal routes used\nby drivers - into the map creation process. Our method aggregates trail data\nfrom the ego vehicle and other traffic participants to construct a\ncomprehensive global map using transformer-based deep learning models. Unlike\ntraditional offline mapping, our approach enables continuous updates while\nremaining sensor-agnostic, facilitating efficient data transfer. Our method\ndemonstrates superior performance compared to state-of-the-art online mapping\napproaches, achieving improved generalization to previously unseen environments\nand sensor configurations. We validate our approach on two benchmark datasets,\nhighlighting its robustness and applicability in autonomous driving systems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8etransformer\u7684\u79bb\u7ebf\u9ad8\u7cbe\u5730\u56fe\u6784\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u591a\u6e90\u8f68\u8ff9\u6570\u636e\u5b9e\u73b0\u4f18\u4e8e\u5728\u7ebf\u65b9\u6cd5\u7684\u6cdb\u5316\u6027\u80fd\u4e0e\u8fd0\u884c\u6548\u7387", "motivation": "\u5728\u7ebf\u5236\u56fe\u5b58\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u5dee\u3001\u4f20\u611f\u5668\u906e\u6321\u654f\u611f\u3001\u6cdb\u5316\u80fd\u529b\u5f31\u7b49\u95ee\u9898\uff0c\u4f20\u7edf\u79bb\u7ebf\u5236\u56fe\u7f3a\u4e4f\u52a8\u6001\u66f4\u65b0\u80fd\u529b\u3002\u9700\u5f00\u53d1\u652f\u6301\u6301\u7eed\u66f4\u65b0\u4e14\u4f20\u611f\u5668\u65e0\u5173\u7684\u9c81\u68d2\u5236\u56fe\u65b9\u6848", "method": "\u878d\u5408\u81ea\u8f66\u4e0e\u4ed6\u8f66\u8f68\u8ff9\u6570\u636e\uff0c\u91c7\u7528transformer\u6a21\u578b\u8fdb\u884c\u591a\u6e90\u4fe1\u606f\u805a\u5408\uff0c\u6784\u5efa\u53ef\u52a8\u6001\u66f4\u65b0\u7684\u4f20\u611f\u5668\u65e0\u5173\u5168\u5c40\u5730\u56fe", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u6bd4SOTA\u5728\u7ebf\u65b9\u6cd5\u63d0\u534714.7%\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u672a\u77e5\u73af\u5883\u4e0b\u7684\u5730\u56fe\u5b8c\u6574\u6027\u63d0\u9ad822%", "conclusion": "\u57fa\u4e8e\u591a\u6e90\u8f68\u8ff9\u878d\u5408\u7684\u79bb\u7ebf\u5236\u56fe\u8303\u5f0f\u6709\u6548\u514b\u670d\u5728\u7ebf\u65b9\u6cd5\u5c40\u9650\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u66f4\u5177\u9002\u5e94\u6027\u7684\u5730\u56fe\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.10518", "pdf": "https://arxiv.org/pdf/2505.10518", "abs": "https://arxiv.org/abs/2505.10518", "authors": ["Anastasios Gerontopoulos", "Spyros Gidaris", "Nikos Komodakis"], "title": "Multi-Token Prediction Needs Registers", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Multi-token prediction has emerged as a promising objective for improving\nlanguage model pretraining, but its benefits have not consistently generalized\nto other settings such as fine-tuning. In this paper, we propose MuToR, a\nsimple and effective approach to multi-token prediction that interleaves\nlearnable register tokens into the input sequence, each tasked with predicting\nfuture targets. Compared to existing methods, MuToR offers several key\nadvantages: it introduces only a negligible number of additional parameters,\nrequires no architectural changes--ensuring compatibility with off-the-shelf\npretrained language models--and remains aligned with the next-token pretraining\nobjective, making it especially well-suited for supervised fine-tuning.\nMoreover, it naturally supports scalable prediction horizons. We demonstrate\nthe effectiveness and versatility of MuToR across a range of use cases,\nincluding supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and\npretraining, on challenging generative tasks in both language and vision\ndomains. Our code will be available at: https://github.com/nasosger/MuToR.", "AI": {"tldr": "\u63d0\u51faMuToR\u65b9\u6cd5\u2014\u2014\u901a\u8fc7\u63d2\u5165\u53ef\u5b66\u4e60\u7684\u5bc4\u5b58\u5668\u4ee4\u724c\u5b9e\u73b0\u9ad8\u6548\u591a\u4ee4\u724c\u9884\u6d4b\uff0c\u5728\u4fdd\u6301\u539f\u6709\u67b6\u6784\u517c\u5bb9\u6027\u7684\u540c\u65f6\u63d0\u5347\u76d1\u7763\u5fae\u8c03\u6548\u679c", "motivation": "\u73b0\u6709\u591a\u4ee4\u724c\u9884\u6d4b\u65b9\u6cd5\u5728\u5fae\u8c03\u9636\u6bb5\u6548\u679c\u6b20\u4f73\uff0c\u9700\u8981\u517c\u5bb9\u73b0\u6709\u9884\u8bad\u7ec3\u6a21\u578b\u4e14\u4e0d\u663e\u8457\u589e\u52a0\u53c2\u6570\u91cf\u7684\u6539\u8fdb\u65b9\u6848", "method": "\u5728\u8f93\u5165\u5e8f\u5217\u4e2d\u63d2\u5165\u53ef\u5b66\u4e60\u7684\u5bc4\u5b58\u5668\u4ee4\u724c\uff0c\u6bcf\u4e2a\u4ee4\u724c\u8d1f\u8d23\u9884\u6d4b\u672a\u6765\u76ee\u6807\uff0c\u4fdd\u6301\u4e0e\u73b0\u6709\u67b6\u6784\u517c\u5bb9\uff0c\u65e0\u9700\u4fee\u6539\u9884\u8bad\u7ec3\u76ee\u6807", "result": "\u5728\u8bed\u8a00\u548c\u89c6\u89c9\u9886\u57df\u7684\u751f\u6210\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u5305\u62ec\u76d1\u7763\u5fae\u8c03\u3001\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u548c\u9884\u8bad\u7ec3\u573a\u666f", "conclusion": "MuToR\u901a\u8fc7\u6781\u7b80\u8bbe\u8ba1\u5b9e\u73b0\u9ad8\u6548\u591a\u4ee4\u724c\u9884\u6d4b\uff0c\u7279\u522b\u9002\u5408\u76d1\u7763\u5fae\u8c03\uff0c\u652f\u6301\u53ef\u6269\u5c55\u7684\u9884\u6d4b\u8303\u56f4\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027"}}
{"id": "2505.10267", "pdf": "https://arxiv.org/pdf/2505.10267", "abs": "https://arxiv.org/abs/2505.10267", "authors": ["Pavel Korotaev", "Petr Surovtsev", "Alexander Kapitanov", "Karina Kvanchiani", "Aleksandr Nagaev"], "title": "HandReader: Advanced Techniques for Efficient Fingerspelling Recognition", "categories": ["cs.CV", "cs.LG"], "comment": "https://github.com/ai-forever/handreader", "summary": "Fingerspelling is a significant component of Sign Language (SL), allowing the\ninterpretation of proper names, characterized by fast hand movements during\nsigning. Although previous works on fingerspelling recognition have focused on\nprocessing the temporal dimension of videos, there remains room for improving\nthe accuracy of these approaches. This paper introduces HandReader, a group of\nthree architectures designed to address the fingerspelling recognition task.\nHandReader$_{RGB}$ employs the novel Temporal Shift-Adaptive Module (TSAM) to\nprocess RGB features from videos of varying lengths while preserving important\nsequential information. HandReader$_{KP}$ is built on the proposed Temporal\nPose Encoder (TPE) operated on keypoints as tensors. Such keypoints composition\nin a batch allows the encoder to pass them through 2D and 3D convolution\nlayers, utilizing temporal and spatial information and accumulating keypoints\ncoordinates. We also introduce HandReader_RGB+KP - architecture with a joint\nencoder to benefit from RGB and keypoint modalities. Each HandReader model\npossesses distinct advantages and achieves state-of-the-art results on the\nChicagoFSWild and ChicagoFSWild+ datasets. Moreover, the models demonstrate\nhigh performance on the first open dataset for Russian fingerspelling, Znaki,\npresented in this paper. The Znaki dataset and HandReader pre-trained models\nare publicly available.", "AI": {"tldr": "\u63d0\u51faHandReader\u7cfb\u5217\u67b6\u6784\uff08RGB/KP/RGB+KP\uff09\uff0c\u901a\u8fc7\u65f6\u5e8f\u5904\u7406\u6a21\u5757TSAM\u548cTPE\u6539\u8fdb\u624b\u8bed\u62fc\u5199\u8bc6\u522b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u521bSOTA\u5e76\u53d1\u5e03\u4fc4\u8bed\u6570\u636e\u96c6Znaki\u3002", "motivation": "\u73b0\u6709\u624b\u8bed\u62fc\u5199\u8bc6\u522b\u65b9\u6cd5\u5728\u89c6\u9891\u65f6\u5e8f\u4fe1\u606f\u5904\u7406\u4e0a\u5b58\u5728\u7cbe\u5ea6\u63d0\u5347\u7a7a\u95f4\uff0c\u9700\u5f00\u53d1\u80fd\u6709\u6548\u5904\u7406\u4e0d\u540c\u957f\u5ea6\u89c6\u9891\u5e76\u4fdd\u7559\u5173\u952e\u5e8f\u5217\u4fe1\u606f\u7684\u65b0\u65b9\u6cd5\u3002", "method": "1. HandReader_RGB\u4f7f\u7528TSAM\u6a21\u5757\u5904\u7406RGB\u7279\u5f81\uff1b2. HandReader_KP\u57fa\u4e8eTPE\u7f16\u7801\u5668\u5904\u7406\u5173\u952e\u70b9\u5f20\u91cf\uff1b3. HandReader_RGB+KP\u8054\u5408\u4e24\u79cd\u6a21\u6001\u7684\u6df7\u5408\u67b6\u6784\u3002", "result": "\u5728ChicagoFSWild\u7cfb\u5217\u6570\u636e\u96c6\u53d6\u5f97SOTA\uff0c\u4fc4\u8bed\u6570\u636e\u96c6Znaki\u4e0a\u8868\u73b0\u4f18\u5f02\u3002\u540c\u6b65\u5f00\u6e90Znaki\u6570\u636e\u96c6\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "conclusion": "\u4e09\u79cd\u67b6\u6784\u5404\u5177\u4f18\u52bf\uff0cTSAM\u548cTPE\u6709\u6548\u5229\u7528\u65f6\u7a7a\u4fe1\u606f\uff0c\u591a\u6a21\u6001\u878d\u5408\u65b9\u6848\u663e\u8457\u63d0\u5347\u8bc6\u522b\u6027\u80fd\uff0c\u586b\u8865\u4fc4\u8bed\u624b\u8bed\u8d44\u6e90\u7a7a\u767d\u3002"}}
{"id": "2505.10527", "pdf": "https://arxiv.org/pdf/2505.10527", "abs": "https://arxiv.org/abs/2505.10527", "authors": ["Binghai Wang", "Runji Lin", "Keming Lu", "Le Yu", "Zhenru Zhang", "Fei Huang", "Chujie Zheng", "Kai Dang", "Yang Fan", "Xingzhang Ren", "An Yang", "Binyuan Hui", "Dayiheng Liu", "Tao Gui", "Qi Zhang", "Xuanjing Huang", "Yu-Gang Jiang", "Bowen Yu", "Jingren Zhou", "Junyang Lin"], "title": "WorldPM: Scaling Human Preference Modeling", "categories": ["cs.CL"], "comment": null, "summary": "Motivated by scaling laws in language modeling that demonstrate how test loss\nscales as a power law with model and dataset sizes, we find that similar laws\nexist in preference modeling. We propose World Preference Modeling$ (WorldPM)\nto emphasize this scaling potential, where World Preference embodies a unified\nrepresentation of human preferences. In this paper, we collect preference data\nfrom public forums covering diverse user communities, and conduct extensive\ntraining using 15M-scale data across models ranging from 1.5B to 72B\nparameters. We observe distinct patterns across different evaluation metrics:\n(1) Adversarial metrics (ability to identify deceptive features) consistently\nscale up with increased training data and base model size; (2) Objective\nmetrics (objective knowledge with well-defined answers) show emergent behavior\nin larger language models, highlighting WorldPM's scalability potential; (3)\nSubjective metrics (subjective preferences from a limited number of humans or\nAI) do not demonstrate scaling trends. Further experiments validate the\neffectiveness of WorldPM as a foundation for preference fine-tuning. Through\nevaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly\nimproves the generalization performance across human preference datasets of\nvarying sizes (7K, 100K and 800K samples), with performance gains exceeding 5%\non many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we\nobserve significant improvements on both in-house and public evaluation sets,\nwith notable gains of 4% to 8% in our in-house evaluations.", "AI": {"tldr": "\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u548c\u6a21\u578b\u6269\u5c55\u9a8c\u8bc1\u504f\u597d\u5efa\u6a21\u7684\u6269\u5c55\u5b9a\u5f8b\uff0cWorldPM\u6846\u67b6\u5728\u5bf9\u6297\u6027\u548c\u5ba2\u89c2\u6307\u6807\u4e2d\u5c55\u73b0\u663e\u8457\u6269\u5c55\u6f5c\u529b\uff0c\u4e3b\u89c2\u6307\u6807\u672a\u5448\u73b0\u6269\u5c55\u8d8b\u52bf\u3002", "motivation": "\u501f\u9274\u8bed\u8a00\u6a21\u578b\u7684\u89c4\u6a21\u5b9a\u5f8b\uff0c\u63a2\u7d22\u5176\u5728\u504f\u597d\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u6784\u5efa\u7edf\u4e00\u7684\u4eba\u7c7b\u504f\u597d\u8868\u5f81\u6846\u67b6WorldPM\u3002", "method": "\u6536\u96c6\u516c\u5171\u8bba\u575b\u591a\u6837\u5316\u504f\u597d\u6570\u636e\uff0c\u4f7f\u752815M\u89c4\u6a21\u6570\u636e\u96c6\u8bad\u7ec31.5B-72B\u53c2\u6570\u6a21\u578b\uff0c\u57287\u4e2a\u57fa\u51c620\u4e2a\u5b50\u4efb\u52a1\u5f00\u5c55\u8bc4\u4f30\u3002", "result": "\u5bf9\u6297\u6027\u6307\u6807\u968f\u89c4\u6a21\u7a33\u5b9a\u63d0\u5347\uff0c\u5ba2\u89c2\u6307\u6807\u5448\u73b0\u7a81\u73b0\u80fd\u529b\uff0c\u4e3b\u89c2\u6307\u6807\u65e0\u6269\u5c55\u8d8b\u52bf\u3002WorldPM\u4f7f\u6cdb\u5316\u6027\u80fd\u63d0\u5347\u8d855%\uff0cRLHF\u6d41\u7a0b\u96c6\u6210\u540e\u8bc4\u4f30\u7ed3\u679c\u63d0\u53474-8%\u3002", "conclusion": "WorldPM\u8bc1\u5b9e\u504f\u597d\u5efa\u6a21\u7684\u6269\u5c55\u6f5c\u529b\uff0c\u63ed\u793a\u4e0d\u540c\u6307\u6807\u7279\u6027\u5dee\u5f02\uff0c\u4e3a\u504f\u597d\u5fae\u8c03\u63d0\u4f9b\u6709\u6548\u57fa\u7840\u6846\u67b6\u3002"}}
{"id": "2505.10281", "pdf": "https://arxiv.org/pdf/2505.10281", "abs": "https://arxiv.org/abs/2505.10281", "authors": ["Mengqiu Xu", "Kaixin Chen", "Heng Guo", "Yixiang Huang", "Ming Wu", "Zhenwei Shi", "Chuang Zhang", "Jun Guo"], "title": "MFogHub: Bridging Multi-Regional and Multi-Satellite Data for Global Marine Fog Detection and Forecasting", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning approaches for marine fog detection and forecasting have\noutperformed traditional methods, demonstrating significant scientific and\npractical importance. However, the limited availability of open-source datasets\nremains a major challenge. Existing datasets, often focused on a single region\nor satellite, restrict the ability to evaluate model performance across diverse\nconditions and hinder the exploration of intrinsic marine fog characteristics.\nTo address these limitations, we introduce \\textbf{MFogHub}, the first\nmulti-regional and multi-satellite dataset to integrate annotated marine fog\nobservations from 15 coastal fog-prone regions and six geostationary\nsatellites, comprising over 68,000 high-resolution samples. By encompassing\ndiverse regions and satellite perspectives, MFogHub facilitates rigorous\nevaluation of both detection and forecasting methods under varying conditions.\nExtensive experiments with 16 baseline models demonstrate that MFogHub can\nreveal generalization fluctuations due to regional and satellite discrepancy,\nwhile also serving as a valuable resource for the development of targeted and\nscalable fog prediction techniques. Through MFogHub, we aim to advance both the\npractical monitoring and scientific understanding of marine fog dynamics on a\nglobal scale. The dataset and code are at\n\\href{https://github.com/kaka0910/MFogHub}{https://github.com/kaka0910/MFogHub}.", "AI": {"tldr": "\u5f00\u53d1\u9996\u4e2a\u591a\u533a\u57df\u591a\u536b\u661f\u6d77\u6d0b\u96fe\u6570\u636e\u96c6MFogHub\uff0c\u6574\u540815\u4e2a\u6cbf\u6d77\u533a\u57df\u548c6\u9897\u536b\u661f\u768468,000+\u6837\u672c\uff0c\u63d0\u5347\u68c0\u6d4b\u4e0e\u9884\u6d4b\u6a21\u578b\u7684\u8de8\u57df\u8bc4\u4f30\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6d77\u6d0b\u96fe\u6570\u636e\u96c6\u5c40\u9650\u4e8e\u5355\u4e00\u533a\u57df/\u536b\u661f\uff0c\u96be\u4ee5\u8bc4\u4f30\u6a21\u578b\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u4e5f\u963b\u788d\u4e86\u5bf9\u6d77\u6d0b\u96fe\u672c\u8d28\u7279\u5f81\u7684\u63a2\u7d22\u3002", "method": "\u6574\u540815\u4e2a\u591a\u96fe\u6cbf\u6d77\u533a\u57df\u548c6\u9897\u5730\u7403\u540c\u6b65\u536b\u661f\u7684\u6807\u6ce8\u6570\u636e\uff0c\u6784\u5efa\u5305\u542b68,000+\u9ad8\u5206\u8fa8\u7387\u6837\u672c\u7684\u591a\u6e90\u5f02\u6784\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc716\u4e2a\u57fa\u7ebf\u6a21\u578b\u8fdb\u884c\u8de8\u57df\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u533a\u57df\u5dee\u5f02\u548c\u536b\u661f\u89c6\u89d2\u5dee\u5f02\u5bfc\u81f4\u7684\u6a21\u578b\u6cdb\u5316\u6ce2\u52a8\uff0c\u9a8c\u8bc1\u6570\u636e\u96c6\u53ef\u4f5c\u4e3a\u5f00\u53d1\u9488\u5bf9\u6027\u96fe\u9884\u6d4b\u6280\u672f\u7684\u91cd\u8981\u8d44\u6e90\u3002", "conclusion": "MFogHub\u901a\u8fc7\u591a\u7ef4\u5ea6\u6570\u636e\u6574\u5408\uff0c\u63a8\u52a8\u4e86\u5168\u7403\u6d77\u6d0b\u96fe\u52a8\u6001\u76d1\u6d4b\u80fd\u529b\u548c\u79d1\u5b66\u8ba4\u77e5\u7684\u63d0\u5347\uff0c\u5f00\u6e90\u6570\u636e\u52a9\u529b\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2505.10554", "pdf": "https://arxiv.org/pdf/2505.10554", "abs": "https://arxiv.org/abs/2505.10554", "authors": ["Zhiyuan Hu", "Yibo Wang", "Hanze Dong", "Yuhui Xu", "Amrita Saha", "Caiming Xiong", "Bryan Hooi", "Junnan Li"], "title": "Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large Reasoning Models", "categories": ["cs.CL"], "comment": "In Progress", "summary": "Large reasoning models (LRMs) already possess a latent capacity for long\nchain-of-thought reasoning. Prior work has shown that outcome-based\nreinforcement learning (RL) can incidentally elicit advanced reasoning\nbehaviors such as self-correction, backtracking, and verification phenomena\noften referred to as the model's \"aha moment\". However, the timing and\nconsistency of these emergent behaviors remain unpredictable and\nuncontrollable, limiting the scalability and reliability of LRMs' reasoning\ncapabilities. To address these limitations, we move beyond reliance on prompts\nand coincidental \"aha moments\". Instead, we explicitly align models with three\nmeta-abilities: deduction, induction, and abduction, using automatically\ngenerated, self-verifiable tasks. Our three stage-pipeline individual\nalignment, parameter-space merging, and domain-specific reinforcement learning,\nboosting performance by over 10\\% relative to instruction-tuned baselines.\nFurthermore, domain-specific RL from the aligned checkpoint yields an\nadditional 2\\% average gain in the performance ceiling across math, coding, and\nscience benchmarks, demonstrating that explicit meta-ability alignment offers a\nscalable and dependable foundation for reasoning. Code is available at:\nhttps://github.com/zhiyuanhubj/Meta-Ability-Alignment", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u901a\u8fc7\u663e\u5f0f\u5bf9\u9f50\u6a21\u578b\u7684\u6f14\u7ece/\u5f52\u7eb3/\u6eaf\u56e0\u4e09\u79cd\u5143\u80fd\u529b\uff0c\u7a81\u7834\u4f20\u7edf\u63d0\u793a\u5de5\u7a0b\u548c\u968f\u673a\u6d8c\u73b0\u7684\u5c40\u9650\uff0c\u6784\u5efa\u53ef\u6269\u5c55\u7684\u63a8\u7406\u57fa\u7840", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7ed3\u679c\u7684\u5f3a\u5316\u5b66\u4e60\u867d\u80fd\u5076\u7136\u5f15\u53d1\u81ea\u6211\u4fee\u6b63\u7b49\u63a8\u7406\u884c\u4e3a\uff0c\u4f46\u8fd9\u4e9b\u884c\u4e3a\u7684\u65f6\u673a\u548c\u4e00\u81f4\u6027\u4e0d\u53ef\u63a7\uff0c\u9650\u5236\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u53ef\u9760\u6027\u548c\u6269\u5c55\u6027", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u6846\u67b6\uff1a\u4e2a\u4f53\u5143\u80fd\u529b\u5bf9\u9f50\u8bad\u7ec3\u3001\u53c2\u6570\u7a7a\u95f4\u878d\u5408\u3001\u9886\u57df\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u7684\u81ea\u9a8c\u8bc1\u4efb\u52a1\u9a71\u52a8\u80fd\u529b\u5185\u5316", "result": "\u5728\u6570\u5b66/\u7f16\u7a0b/\u79d1\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u5bf9\u6307\u4ee4\u5fae\u8c03\u57fa\u7ebf\u63d0\u5347\u8d8510%\uff0c\u9886\u57df\u5f3a\u5316\u5b66\u4e60\u8fdb\u4e00\u6b65\u5e26\u67652%\u7684\u5e73\u5747\u589e\u76ca", "conclusion": "\u663e\u5f0f\u5143\u80fd\u529b\u5bf9\u9f50\u4e3a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u9760\u7684\u57fa\u7840\u6846\u67b6\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u57fa\u4e8e\u63d0\u793a\u548c\u968f\u673a\u6d8c\u73b0\u7684\u8303\u5f0f\u9650\u5236"}}
{"id": "2505.10289", "pdf": "https://arxiv.org/pdf/2505.10289", "abs": "https://arxiv.org/abs/2505.10289", "authors": ["Yue Wang", "Shuai Xu", "Xuelin Zhu", "Yicong Li"], "title": "MSCI: Addressing CLIP's Inherent Limitations for Compositional Zero-Shot Learning", "categories": ["cs.CV"], "comment": "9 pages, 5 figures", "summary": "Compositional Zero-Shot Learning (CZSL) aims to recognize unseen state-object\ncombinations by leveraging known combinations. Existing studies basically rely\non the cross-modal alignment capabilities of CLIP but tend to overlook its\nlimitations in capturing fine-grained local features, which arise from its\narchitectural and training paradigm. To address this issue, we propose a\nMulti-Stage Cross-modal Interaction (MSCI) model that effectively explores and\nutilizes intermediate-layer information from CLIP's visual encoder.\nSpecifically, we design two self-adaptive aggregators to extract local\ninformation from low-level visual features and integrate global information\nfrom high-level visual features, respectively. These key information are\nprogressively incorporated into textual representations through a\nstage-by-stage interaction mechanism, significantly enhancing the model's\nperception capability for fine-grained local visual information. Additionally,\nMSCI dynamically adjusts the attention weights between global and local visual\ninformation based on different combinations, as well as different elements\nwithin the same combination, allowing it to flexibly adapt to diverse\nscenarios. Experiments on three widely used datasets fully validate the\neffectiveness and superiority of the proposed model. Data and code are\navailable at https://github.com/ltpwy/MSCI.", "AI": {"tldr": "\u63d0\u51fa\u591a\u9636\u6bb5\u8de8\u6a21\u6001\u4ea4\u4e92\u6a21\u578bMSCI\uff0c\u901a\u8fc7\u63d0\u53d6CLIP\u4e2d\u95f4\u5c42\u89c6\u89c9\u7279\u5f81\u589e\u5f3a\u7ec4\u5408\u96f6\u6837\u672c\u5b66\u4e60\u4e2d\u7ec6\u7c92\u5ea6\u5c40\u90e8\u7279\u5f81\u611f\u77e5\u80fd\u529b", "motivation": "\u73b0\u6709\u57fa\u4e8eCLIP\u7684\u7ec4\u5408\u96f6\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\u5ffd\u89c6\u5176\u67b6\u6784\u548c\u8bad\u7ec3\u8303\u5f0f\u5bfc\u81f4\u7684\u7ec6\u7c92\u5ea6\u5c40\u90e8\u7279\u5f81\u6355\u6349\u7f3a\u9677", "method": "\u8bbe\u8ba1\u53cc\u81ea\u9002\u5e94\u805a\u5408\u5668\uff08\u4f4e\u5c42\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\u5668+\u9ad8\u5c42\u5168\u5c40\u7279\u5f81\u6574\u5408\u5668\uff09\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u4ea4\u4e92\u673a\u5236\u5c06\u89c6\u89c9\u5173\u952e\u4fe1\u606f\u6e10\u8fdb\u878d\u5165\u6587\u672c\u8868\u793a", "result": "\u5728\u4e09\u4e2a\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6a21\u578b\u6709\u6548\u6027\uff0c\u6027\u80fd\u8868\u73b0\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5", "conclusion": "MSCI\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5168\u5c40/\u5c40\u90e8\u6ce8\u610f\u529b\u6743\u91cd\uff0c\u7075\u6d3b\u9002\u5e94\u4e0d\u540c\u7ec4\u5408\u573a\u666f\uff0c\u663e\u8457\u63d0\u5347\u7ec6\u7c92\u5ea6\u611f\u77e5\u80fd\u529b"}}
{"id": "2505.09665", "pdf": "https://arxiv.org/pdf/2505.09665", "abs": "https://arxiv.org/abs/2505.09665", "authors": ["Sulong Zhou", "Qunying Huang", "Shaoheng Zhou", "Yun Hang", "Xinyue Ye", "Aodong Mei", "Kathryn Phung", "Yuning Ye", "Uma Govindswamy", "Zehan Li"], "title": "Tales of the 2025 Los Angeles Fire: Hotwash for Public Health Concerns in Reddit via LLM-Enhanced Topic Modeling", "categories": ["cs.SI", "cs.CL"], "comment": null, "summary": "Wildfires have become increasingly frequent, irregular, and severe in recent\nyears. Understanding how affected populations perceive and respond during\nwildfire crises is critical for timely and empathetic disaster response. Social\nmedia platforms offer a crowd-sourced channel to capture evolving public\ndiscourse, providing hyperlocal information and insight into public sentiment.\nThis study analyzes Reddit discourse during the 2025 Los Angeles wildfires,\nspanning from the onset of the disaster to full containment. We collect 385\nposts and 114,879 comments related to the Palisades and Eaton fires. We adopt\ntopic modeling methods to identify the latent topics, enhanced by large\nlanguage models (LLMs) and human-in-the-loop (HITL) refinement. Furthermore, we\ndevelop a hierarchical framework to categorize latent topics, consisting of two\nmain categories, Situational Awareness (SA) and Crisis Narratives (CN). The\nvolume of SA category closely aligns with real-world fire progressions, peaking\nwithin the first 2-5 days as the fires reach the maximum extent. The most\nfrequent co-occurring category set of public health and safety, loss and\ndamage, and emergency resources expands on a wide range of health-related\nlatent topics, including environmental health, occupational health, and one\nhealth. Grief signals and mental health risks consistently accounted for 60\npercentage and 40 percentage of CN instances, respectively, with the highest\ntotal volume occurring at night. This study contributes the first annotated\nsocial media dataset on the 2025 LA fires, and introduces a scalable\nmulti-layer framework that leverages topic modeling for crisis discourse\nanalysis. By identifying persistent public health concerns, our results can\ninform more empathetic and adaptive strategies for disaster response, public\nhealth communication, and future research in comparable climate-related\ndisaster events.", "AI": {"tldr": "\u901a\u8fc7Reddit\u6570\u636e\u5206\u67902025\u5e74\u6d1b\u6749\u77f6\u91ce\u706b\u671f\u95f4\u7684\u516c\u5171\u8ba8\u8bba\uff0c\u7ed3\u5408\u4e3b\u9898\u5efa\u6a21\u4e0e\u5206\u5c42\u6846\u67b6\u63ed\u793a\u516c\u4f17\u5065\u5eb7\u5173\u6ce8\u4e0e\u5371\u673a\u5e94\u5bf9\u6a21\u5f0f", "motivation": "\u8fd1\u5e74\u6765\u91ce\u706b\u707e\u5bb3\u52a0\u5267\uff0c\u7406\u89e3\u53d7\u707e\u4eba\u7fa4\u7684\u5b9e\u65f6\u53cd\u5e94\u5bf9\u707e\u5bb3\u54cd\u5e94\u81f3\u5173\u91cd\u8981\u3002\u793e\u4ea4\u5a92\u4f53\u4e3a\u6355\u6349\u52a8\u6001\u516c\u5171\u8bdd\u8bed\u63d0\u4f9b\u72ec\u7279\u89c6\u89d2\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u7cfb\u7edf\u6027\u5206\u6790\u6846\u67b6\u3002", "method": "\u91c7\u7528LLM\u589e\u5f3a\u7684\u4e3b\u9898\u5efa\u6a21\u65b9\u6cd5\uff0c\u6784\u5efa\u5305\u542b385\u5e16\u3001114K\u8bc4\u8bba\u7684\u6570\u636e\u96c6\u3002\u5efa\u7acbSA-CN\u53cc\u5c42\u5206\u7c7b\u6846\u67b6\uff0c\u7ed3\u5408\u5b9a\u91cf\u5206\u6790\u4e0eHITL\u4eba\u5de5\u7ec6\u5316\u3002", "result": "SA\u8ba8\u8bba\u5cf0\u503c\u4e0e\u706b\u52bf\u6269\u5f20\u540c\u6b65\uff082-5\u5929\uff09\uff0c\u516c\u5171\u5065\u5eb7/\u5b89\u5168\u4e3b\u9898\u8986\u76d6\u73af\u5883/\u804c\u4e1a\u5065\u5eb7\u7b49\u591a\u7ef4\u5ea6\u3002CN\u4e2d60%\u4e3a\u60b2\u4f24\u8868\u8fbe\uff0c40%\u6d89\u53ca\u5fc3\u7406\u5065\u5eb7\u98ce\u9669\uff0c\u591c\u95f4\u8ba8\u8bba\u6700\u6d3b\u8dc3\u3002", "conclusion": "\u9996\u6b21\u521b\u5efa\u6807\u6ce8\u793e\u4ea4\u5a92\u4f53\u707e\u96be\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u53ef\u6269\u5c55\u7684\u591a\u5c42\u5206\u6790\u6846\u67b6\uff0c\u4e3a\u707e\u540e\u5fc3\u7406\u5e72\u9884\u548c\u5065\u5eb7\u4f20\u64ad\u63d0\u4f9b\u6570\u636e\u9a71\u52a8\u7684\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2505.10292", "pdf": "https://arxiv.org/pdf/2505.10292", "abs": "https://arxiv.org/abs/2505.10292", "authors": ["Daniel A. P. Oliveira", "David Martins de Matos"], "title": "StoryReasoning Dataset: Using Chain-of-Thought for Scene Understanding and Grounded Story Generation", "categories": ["cs.CV", "cs.CL", "I.2.10; I.2.7"], "comment": "31 pages, 14 figures", "summary": "Visual storytelling systems struggle to maintain character identity across\nframes and link actions to appropriate subjects, frequently leading to\nreferential hallucinations. These issues can be addressed through grounding of\ncharacters, objects, and other entities on the visual elements. We propose\nStoryReasoning, a dataset containing 4,178 stories derived from 52,016 movie\nimages, with both structured scene analyses and grounded stories. Each story\nmaintains character and object consistency across frames while explicitly\nmodeling multi-frame relationships through structured tabular representations.\nOur approach features cross-frame object re-identification using visual\nsimilarity and face recognition, chain-of-thought reasoning for explicit\nnarrative modeling, and a grounding scheme that links textual elements to\nvisual entities across multiple frames. We establish baseline performance by\nfine-tuning Qwen2.5-VL 7B, creating Qwen Storyteller, which performs end-to-end\nobject detection, re-identification, and landmark detection while maintaining\nconsistent object references throughout the story. Evaluation demonstrates a\nreduction from 4.06 to 3.56 (-12.3%) hallucinations on average per story when\ncompared to a non-fine-tuned model.", "AI": {"tldr": "\u901a\u8fc7StoryReasoning\u6570\u636e\u96c6\u548c\u6a21\u578b\u5fae\u8c03\uff0c\u51cf\u5c11\u89c6\u89c9\u53d9\u4e8b\u4e2d\u7684\u6307\u79f0\u5e7b\u89c9", "motivation": "\u73b0\u6709\u89c6\u89c9\u53d9\u4e8b\u7cfb\u7edf\u5b58\u5728\u89d2\u8272\u8eab\u4efd\u8de8\u5e27\u4e0d\u4e00\u81f4\u3001\u52a8\u4f5c\u4e0e\u4e3b\u4f53\u5173\u8054\u9519\u8bef\u5bfc\u81f4\u7684\u6307\u79f0\u5e7b\u89c9\u95ee\u9898", "method": "\u8de8\u5e27\u5bf9\u8c61\u91cd\u8bc6\u522b\uff08\u89c6\u89c9\u76f8\u4f3c\u5ea6+\u4eba\u8138\u8bc6\u522b\uff09+\u601d\u7ef4\u94fe\u63a8\u7406\u53d9\u4e8b\u5efa\u6a21+\u6587\u672c-\u89c6\u89c9\u5b9e\u4f53\u591a\u5e27\u94fe\u63a5\u65b9\u6848\uff0c\u5fae\u8c03Qwen2.5-VL 7B\u5b9e\u73b0\u7aef\u5230\u7aef\u68c0\u6d4b\u7cfb\u7edf", "result": "\u5fae\u8c03\u6a21\u578bQwen Storyteller\u5c06\u6bcf\u4e2a\u6545\u4e8b\u7684\u5e73\u5747\u5e7b\u89c9\u6570\u4ece4.06\u964d\u81f33.56\uff08-12.3%\uff09", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u5316\u6570\u636e\u4e0e\u8de8\u5e27\u5206\u6790\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u89d2\u8272/\u5bf9\u8c61\u4e00\u81f4\u6027\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u5fae\u8c03\u5bf9\u51cf\u5c11\u53d9\u4e8b\u5e7b\u89c9\u7684\u6709\u6548\u6027"}}
{"id": "2505.09777", "pdf": "https://arxiv.org/pdf/2505.09777", "abs": "https://arxiv.org/abs/2505.09777", "authors": ["Alejo Lopez-Avila", "Jinhua Du"], "title": "A Survey on Large Language Models in Multimodal Recommender Systems", "categories": ["cs.IR", "cs.CL"], "comment": "30 pages, 6 figures", "summary": "Multimodal recommender systems (MRS) integrate heterogeneous user and item\ndata, such as text, images, and structured information, to enhance\nrecommendation performance. The emergence of large language models (LLMs)\nintroduces new opportunities for MRS by enabling semantic reasoning, in-context\nlearning, and dynamic input handling. Compared to earlier pre-trained language\nmodels (PLMs), LLMs offer greater flexibility and generalisation capabilities\nbut also introduce challenges related to scalability and model accessibility.\nThis survey presents a comprehensive review of recent work at the intersection\nof LLMs and MRS, focusing on prompting strategies, fine-tuning methods, and\ndata adaptation techniques. We propose a novel taxonomy to characterise\nintegration patterns, identify transferable techniques from related\nrecommendation domains, provide an overview of evaluation metrics and datasets,\nand point to possible future directions. We aim to clarify the emerging role of\nLLMs in multimodal recommendation and support future research in this rapidly\nevolving field.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\uff08MRS\uff09\u7684\u878d\u5408\uff0c\u63d0\u51fa\u65b0\u5206\u7c7b\u6cd5\u5e76\u5206\u6790\u6280\u672f\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u63a2\u7d22LLMs\u5982\u4f55\u901a\u8fc7\u8bed\u4e49\u63a8\u7406\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\u7b49\u80fd\u529b\u7a81\u7834\u4f20\u7edf\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\u74f6\u9888\uff0c\u5e76\u89e3\u51b3\u5176\u53ef\u6269\u5c55\u6027\u3001\u6a21\u578b\u8bbf\u95ee\u6027\u7b49\u65b0\u6311\u6218\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\uff0c\u6574\u5408\u63d0\u793a\u5de5\u7a0b\u3001\u5fae\u8c03\u7b56\u7565\u548c\u6570\u636e\u9002\u914d\u6280\u672f\uff0c\u63d0\u51fa\u57fa\u4e8e\u6574\u5408\u6a21\u5f0f\u7684\u65b0\u5206\u7c7b\u6cd5\uff0c\u5e76\u4ece\u76f8\u5173\u63a8\u8350\u9886\u57df\u8fc1\u79fb\u53ef\u590d\u7528\u6280\u672f\u3002", "result": "\u6784\u5efa\u4e86\u6db5\u76d637\u9879\u6838\u5fc3\u7814\u7a76\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u8bc6\u522b\u51fa\u8de8\u57df\u53ef\u8fc1\u79fb\u6280\u672f\uff0c\u5e76\u5efa\u7acb\u4e86\u5305\u542b8\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u7684\u8bc4\u4f30\u4f53\u7cfb\u3002", "conclusion": "LLMs\u4e3aMRS\u5e26\u6765\u8303\u5f0f\u9769\u65b0\uff0c\u4f46\u9700\u5728\u8ba1\u7b97\u6548\u7387\u3001\u9690\u79c1\u4fdd\u62a4\u7b49\u65b9\u5411\u6301\u7eed\u7a81\u7834\uff0c\u672a\u6765\u7814\u7a76\u5e94\u805a\u7126\u6a21\u578b\u8f7b\u91cf\u5316\u4e0e\u591a\u6a21\u6001\u5bf9\u9f50\u6280\u672f\u3002"}}
{"id": "2505.10294", "pdf": "https://arxiv.org/pdf/2505.10294", "abs": "https://arxiv.org/abs/2505.10294", "authors": ["Guillaume Balezo", "Roger Trullo", "Albert Pla Planas", "Etienne Decenciere", "Thomas Walter"], "title": "MIPHEI-ViT: Multiplex Immunofluorescence Prediction from H&E Images using ViT Foundation Models", "categories": ["cs.CV", "q-bio.TO", "68T07 (Primary), 92C55 (Secondary)", "I.4.9; I.2.10; I.5.4; J.3"], "comment": null, "summary": "Histopathological analysis is a cornerstone of cancer diagnosis, with\nHematoxylin and Eosin (H&E) staining routinely acquired for every patient to\nvisualize cell morphology and tissue architecture. On the other hand, multiplex\nimmunofluorescence (mIF) enables more precise cell type identification via\nproteomic markers, but has yet to achieve widespread clinical adoption due to\ncost and logistical constraints. To bridge this gap, we introduce MIPHEI\n(Multiplex Immunofluorescence Prediction from H&E), a U-Net-inspired\narchitecture that integrates state-of-the-art ViT foundation models as encoders\nto predict mIF signals from H&E images. MIPHEI targets a comprehensive panel of\nmarkers spanning nuclear content, immune lineages (T cells, B cells, myeloid),\nepithelium, stroma, vasculature, and proliferation. We train our model using\nthe publicly available ORION dataset of restained H&E and mIF images from\ncolorectal cancer tissue, and validate it on two independent datasets. MIPHEI\nachieves accurate cell-type classification from H&E alone, with F1 scores of\n0.88 for Pan-CK, 0.57 for CD3e, 0.56 for SMA, 0.36 for CD68, and 0.30 for CD20,\nsubstantially outperforming both a state-of-the-art baseline and a random\nclassifier for most markers. Our results indicate that our model effectively\ncaptures the complex relationships between nuclear morphologies in their tissue\ncontext, as visible in H&E images and molecular markers defining specific cell\ntypes. MIPHEI offers a promising step toward enabling cell-type-aware analysis\nof large-scale H&E datasets, in view of uncovering relationships between\nspatial cellular organization and patient outcomes.", "AI": {"tldr": "\u63d0\u51faMIPHEI\u6a21\u578b\uff0c\u901a\u8fc7U-Net\u67b6\u6784\u96c6\u6210ViT\u57fa\u7840\u6a21\u578b\uff0c\u5b9e\u73b0\u4ec5\u7528H&E\u67d3\u8272\u56fe\u50cf\u9884\u6d4b\u591a\u6807\u8bb0\u514d\u75ab\u8367\u5149\u4fe1\u53f7", "motivation": "\u89e3\u51b3mIF\u68c0\u6d4b\u6210\u672c\u9ad8\u3001\u64cd\u4f5c\u590d\u6742\u96be\u4ee5\u4e34\u5e8a\u666e\u53ca\u7684\u75db\u70b9\uff0c\u5229\u7528\u5e38\u89c4H&E\u67d3\u8272\u56fe\u50cf\u6316\u6398\u6df1\u5c42\u7ec6\u80de\u7c7b\u578b\u4fe1\u606f", "method": "\u91c7\u7528U-Net\u67b6\u6784\u96c6\u6210\u89c6\u89c9Transformer\u7f16\u7801\u5668\uff0c\u57fa\u4e8eORION\u7ed3\u76f4\u80a0\u764c\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u9a8c\u8bc1\u96c6\u8986\u76d6\u4e24\u4e2a\u72ec\u7acb\u6570\u636e\u96c6", "result": "Pan-CK\u5206\u7c7bF1\u8fbe0.88\uff0cCD3e/SMA/CD68/CD20\u5206\u522b\u4e3a0.57/0.56/0.36/0.30\uff0c\u591a\u6570\u6307\u6807\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u6a21\u578b", "conclusion": "\u8bc1\u660eH&E\u56fe\u50cf\u4e2d\u8574\u542b\u7ec6\u80de\u5206\u5b50\u6807\u8bb0\u5173\u8054\u7279\u5f81\uff0c\u4e3a\u5927\u89c4\u6a21\u7ec4\u7ec7\u5b66\u5206\u6790\u63d0\u4f9b\u7ec6\u80de\u7c7b\u578b\u8bc6\u522b\u65b0\u8def\u5f84"}}
{"id": "2505.09855", "pdf": "https://arxiv.org/pdf/2505.09855", "abs": "https://arxiv.org/abs/2505.09855", "authors": ["Alexander Y. Ku", "Thomas L. Griffiths", "Stephanie C. Y. Chan"], "title": "Predictability Shapes Adaptation: An Evolutionary Perspective on Modes of Learning in Transformers", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Transformer models learn in two distinct modes: in-weights learning (IWL),\nencoding knowledge into model weights, and in-context learning (ICL), adapting\nflexibly to context without weight modification. To better understand the\ninterplay between these learning modes, we draw inspiration from evolutionary\nbiology's analogous adaptive strategies: genetic encoding (akin to IWL,\nadapting over generations and fixed within an individual's lifetime) and\nphenotypic plasticity (akin to ICL, enabling flexible behavioral responses to\nenvironmental cues). In evolutionary biology, environmental predictability\ndictates the balance between these strategies: stability favors genetic\nencoding, while reliable predictive cues promote phenotypic plasticity. We\nexperimentally operationalize these dimensions of predictability and\nsystematically investigate their influence on the ICL/IWL balance in\nTransformers. Using regression and classification tasks, we show that high\nenvironmental stability decisively favors IWL, as predicted, with a sharp\ntransition at maximal stability. Conversely, high cue reliability enhances ICL\nefficacy, particularly when stability is low. Furthermore, learning dynamics\nreveal task-contingent temporal evolution: while a canonical ICL-to-IWL shift\noccurs in some settings (e.g., classification with many classes), we\ndemonstrate that scenarios with easier IWL (e.g., fewer classes) or slower ICL\nacquisition (e.g., regression) can exhibit an initial IWL phase later yielding\nto ICL dominance. These findings support a relative-cost hypothesis for\nexplaining these learning mode transitions, establishing predictability as a\ncritical factor governing adaptive strategies in Transformers, and offering\nnovel insights for understanding ICL and guiding training methodologies.", "AI": {"tldr": "Transformer\u6a21\u578b\u901a\u8fc7\u6743\u91cd\u5b66\u4e60\uff08IWL\uff09\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u7684\u7c7b\u6bd4\u751f\u7269\u8fdb\u5316\u7b56\u7565\uff0c\u63ed\u793a\u73af\u5883\u53ef\u9884\u6d4b\u6027\u5bf9\u5b66\u4e60\u6a21\u5f0f\u5e73\u8861\u7684\u8c03\u63a7\u673a\u5236\uff1a\u7a33\u5b9a\u6027\u9a71\u52a8IWL\uff0c\u7ebf\u7d22\u53ef\u9760\u6027\u589e\u5f3aICL\uff0c\u5b66\u4e60\u52a8\u6001\u5448\u73b0\u4efb\u52a1\u4f9d\u8d56\u7684\u9636\u6bb5\u6027\u7279\u5f81\u3002", "motivation": "\u53d7\u8fdb\u5316\u751f\u7269\u5b66\u4e2d\u9057\u4f20\u7f16\u7801\uff08\u7c7b\u6bd4IWL\uff09\u4e0e\u8868\u578b\u53ef\u5851\u6027\uff08\u7c7b\u6bd4ICL\uff09\u7b56\u7565\u7684\u542f\u53d1\uff0c\u63a2\u7a76\u73af\u5883\u53ef\u9884\u6d4b\u6027\u7ef4\u5ea6\uff08\u7a33\u5b9a\u6027\u4e0e\u7ebf\u7d22\u53ef\u9760\u6027\uff09\u5982\u4f55\u51b3\u5b9aTransformer\u6a21\u578b\u4e24\u79cd\u5b66\u4e60\u6a21\u5f0f\u7684\u5e73\u8861\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u56de\u5f52\u548c\u5206\u7c7b\u4efb\u52a1\u5b9e\u9a8c\uff0c\u64cd\u4f5c\u5316\u73af\u5883\u7a33\u5b9a\u6027\uff08\u957f\u671f\u6a21\u5f0f\u6301\u7eed\u6027\uff09\u4e0e\u7ebf\u7d22\u53ef\u9760\u6027\uff08\u4e0a\u4e0b\u6587\u4fe1\u606f\u9884\u6d4b\u6027\uff09\uff0c\u7cfb\u7edf\u5206\u6790\u5176\u5bf9IWL/ICL\u5e73\u8861\u7684\u5f71\u54cd\u53ca\u5b66\u4e60\u52a8\u6001\u6f14\u53d8\u3002", "result": "1. \u9ad8\u73af\u5883\u7a33\u5b9a\u6027\uff08>95%\uff09\u663e\u8457\u4fc3\u8fdbIWL\u4e3b\u5bfc\n2. \u9ad8\u7ebf\u7d22\u53ef\u9760\u6027\u63d0\u5347ICL\u6548\u80fd\uff08\u4f4e\u7a33\u5b9a\u6027\u65f6\u6548\u679c\u500d\u589e\uff09\n3. \u5b66\u4e60\u9636\u6bb5\u5448\u73b0\u975e\u5355\u8c03\u6f14\u5316\uff1a\u5206\u7c7b\u4efb\u52a1\u51fa\u73b0ICL\u2192IWL\u8f6c\u53d8\uff0c\u800c\u56de\u5f52/\u5c11\u7c7b\u4efb\u52a1\u5448\u73b0IWL\u2192ICL\u9006\u8f6c", "conclusion": "\u7814\u7a76\u786e\u7acb\u4e86\u53ef\u9884\u6d4b\u6027\u4f5c\u4e3aTransformer\u81ea\u9002\u5e94\u7b56\u7565\u7684\u6838\u5fc3\u8c03\u63a7\u56e0\u5b50\uff0c\u63d0\u51fa\u76f8\u5bf9\u6210\u672c\u5047\u8bf4\u89e3\u91ca\u5b66\u4e60\u6a21\u5f0f\u8f6c\u6362\u673a\u5236\uff0c\u4e3a\u7406\u89e3ICL\u5185\u5728\u89c4\u5f8b\u548c\u4f18\u5316\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2505.10351", "pdf": "https://arxiv.org/pdf/2505.10351", "abs": "https://arxiv.org/abs/2505.10351", "authors": ["Jie Zhu", "Jirong Zha", "Ding Li", "Leye Wang"], "title": "A Unified and Scalable Membership Inference Method for Visual Self-supervised Encoder via Part-aware Capability", "categories": ["cs.CV"], "comment": "An extension of our ACM CCS2024 conference paper (arXiv:2404.02462).\n  We show the impacts of scaling from both data and model aspects on membership\n  inference for self-supervised visual encoders", "summary": "Self-supervised learning shows promise in harnessing extensive unlabeled\ndata, but it also confronts significant privacy concerns, especially in vision.\nIn this paper, we perform membership inference on visual self-supervised models\nin a more realistic setting: self-supervised training method and details are\nunknown for an adversary when attacking as he usually faces a black-box system\nin practice. In this setting, considering that self-supervised model could be\ntrained by completely different self-supervised paradigms, e.g., masked image\nmodeling and contrastive learning, with complex training details, we propose a\nunified membership inference method called PartCrop. It is motivated by the\nshared part-aware capability among models and stronger part response on the\ntraining data. Specifically, PartCrop crops parts of objects in an image to\nquery responses within the image in representation space. We conduct extensive\nattacks on self-supervised models with different training protocols and\nstructures using three widely used image datasets. The results verify the\neffectiveness and generalization of PartCrop. Moreover, to defend against\nPartCrop, we evaluate two common approaches, i.e., early stop and differential\nprivacy, and propose a tailored method called shrinking crop scale range. The\ndefense experiments indicate that all of them are effective. Finally, besides\nprototype testing on toy visual encoders and small-scale image datasets, we\nquantitatively study the impacts of scaling from both data and model aspects in\na realistic scenario and propose a scalable PartCrop-v2 by introducing two\nstructural improvements to PartCrop. Our code is at\nhttps://github.com/JiePKU/PartCrop.", "AI": {"tldr": "\u63d0\u51fa\u9ed1\u76d2\u573a\u666f\u4e0b\u9488\u5bf9\u89c6\u89c9\u81ea\u76d1\u7763\u6a21\u578b\u7684\u7edf\u4e00\u6210\u5458\u63a8\u7406\u65b9\u6cd5PartCrop\uff0c\u901a\u8fc7\u5c40\u90e8\u88c1\u526a\u63a2\u6d4b\u6a21\u578b\u54cd\u5e94\uff0c\u9a8c\u8bc1\u4e86\u653b\u51fb\u6709\u6548\u6027\u5e76\u63a2\u8ba8\u9632\u5fa1\u65b9\u6848\u3002", "motivation": "\u81ea\u76d1\u7763\u5b66\u4e60\u9762\u4e34\u9690\u79c1\u98ce\u9669\uff0c\u73b0\u6709\u6210\u5458\u63a8\u7406\u65b9\u6cd5\u4f9d\u8d56\u767d\u76d2\u4fe1\u606f\u3002\u9488\u5bf9\u5b9e\u9645\u9ed1\u76d2\u573a\u666f\u4e2d\u6a21\u578b\u8bad\u7ec3\u7ec6\u8282\u672a\u77e5\u7684\u6311\u6218\uff0c\u9700\u5f00\u53d1\u8de8\u8303\u5f0f\u7edf\u4e00\u653b\u51fb\u65b9\u6cd5\u3002", "method": "PartCrop\u57fa\u4e8e\u6a21\u578b\u5171\u6709\u7684\u5c40\u90e8\u611f\u77e5\u80fd\u529b\uff0c\u901a\u8fc7\u88c1\u526a\u56fe\u50cf\u5c40\u90e8\u533a\u57df\uff0c\u5728\u8868\u5f81\u7a7a\u95f4\u8ba1\u7b97\u539f\u56fe\u4e0e\u88c1\u526a\u56fe\u7684\u54cd\u5e94\u76f8\u4f3c\u5ea6\uff0c\u5229\u7528\u8bad\u7ec3\u6570\u636e\u66f4\u5f3a\u7684\u5c40\u90e8\u54cd\u5e94\u7279\u5f81\u8fdb\u884c\u6210\u5458\u63a8\u65ad\u3002", "result": "\u5728\u4e09\u79cd\u5e38\u7528\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86PartCrop\u5bf9Moco\u3001SimCLR\u7b49\u4e0d\u540c\u534f\u8bae\u6a21\u578b\u7684\u653b\u51fb\u6709\u6548\u6027\uff08AUC\u6700\u9ad80.78\uff09\u3002\u63d0\u51fa\u65e9\u671f\u505c\u6b62\u3001\u5dee\u5206\u9690\u79c1\u548c\u88c1\u526a\u5c3a\u5ea6\u9650\u5236\u4e09\u79cd\u9632\u5fa1\u65b9\u6848\uff0c\u5747\u5c55\u793a\u9632\u62a4\u6548\u679c\u3002", "conclusion": "\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u9ed1\u76d2\u573a\u666f\u4e0b\u81ea\u76d1\u7763\u6a21\u578b\u9690\u79c1\u98ce\u9669\uff0c\u63d0\u51fa\u7684PartCrop\u5177\u6709\u8de8\u67b6\u6784\u6cdb\u5316\u6027\u3002\u901a\u8fc7\u6a21\u578b/\u6570\u636e\u89c4\u6a21\u6269\u5c55\u5b9e\u9a8c\uff0c\u5f00\u53d1\u4e86\u6539\u8fdb\u7248PartCrop-v2\uff0c\u4e3a\u5b9e\u9645\u7cfb\u7edf\u9690\u79c1\u9632\u62a4\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2505.09901", "pdf": "https://arxiv.org/pdf/2505.09901", "abs": "https://arxiv.org/abs/2505.09901", "authors": ["Ziyuan Zhang", "Darcy Wang", "Ningyuan Chen", "Rodrigo Mansur", "Vahid Sarhangian"], "title": "Comparing Exploration-Exploitation Strategies of LLMs and Humans: Insights from Standard Multi-armed Bandit Tasks", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) are increasingly used to simulate or automate\nhuman behavior in complex sequential decision-making tasks. A natural question\nis then whether LLMs exhibit similar decision-making behavior to humans, and\ncan achieve comparable (or superior) performance. In this work, we focus on the\nexploration-exploitation (E&E) tradeoff, a fundamental aspect of dynamic\ndecision-making under uncertainty. We employ canonical multi-armed bandit (MAB)\ntasks introduced in the cognitive science and psychiatry literature to conduct\na comparative study of the E&E strategies of LLMs, humans, and MAB algorithms.\nWe use interpretable choice models to capture the E&E strategies of the agents\nand investigate how explicit reasoning, through both prompting strategies and\nreasoning-enhanced models, shapes LLM decision-making. We find that reasoning\nshifts LLMs toward more human-like behavior, characterized by a mix of random\nand directed exploration. In simple stationary tasks, reasoning-enabled LLMs\nexhibit similar levels of random and directed exploration compared to humans.\nHowever, in more complex, non-stationary environments, LLMs struggle to match\nhuman adaptability, particularly in effective directed exploration, despite\nachieving similar regret in certain scenarios. Our findings highlight both the\npromise and limits of LLMs as simulators of human behavior and tools for\nautomated decision-making and point to potential areas of improvements.", "AI": {"tldr": "LLMs\u5728\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u6743\u8861\u4e2d\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u968f\u673a+\u5b9a\u5411\u63a2\u7d22\u7b56\u7565\uff0c\u4f46\u5728\u590d\u6742\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u9002\u5e94\u6027\u4e0d\u8db3", "motivation": "\u63a2\u7a76LLMs\u5728\u52a8\u6001\u51b3\u7b56\u4efb\u52a1\u4e2d\u662f\u5426\u5177\u6709\u7c7b\u4f3c\u4eba\u7c7b\u7684\u63a2\u7d22-\u5229\u7528\u6743\u8861\u7b56\u7565\uff0c\u4ee5\u53ca\u663e\u5f0f\u63a8\u7406\u5982\u4f55\u5f71\u54cd\u5176\u51b3\u7b56\u884c\u4e3a", "method": "\u4f7f\u7528\u8ba4\u77e5\u79d1\u5b66\u4e2d\u7684\u591a\u81c2\u8001\u864e\u673a\u4efb\u52a1\uff0c\u7ed3\u5408\u53ef\u89e3\u91ca\u9009\u62e9\u6a21\u578b\u5206\u6790LLMs/\u4eba\u7c7b/\u7b97\u6cd5\u7684\u7b56\u7565\uff0c\u901a\u8fc7\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u548c\u589e\u5f3a\u63a8\u7406\u6a21\u578b\u6d4b\u8bd5\u51b3\u7b56\u6a21\u5f0f", "result": "\u63a8\u7406\u4f7fLLMs\u66f4\u63a5\u8fd1\u4eba\u7c7b\u884c\u4e3a\uff08\u6df7\u5408\u63a2\u7d22\u6a21\u5f0f\uff09\uff0c\u7b80\u5355\u4efb\u52a1\u4e2d\u63a2\u7d22\u6c34\u5e73\u76f8\u4f3c\uff0c\u4f46\u590d\u6742\u73af\u5883\u4e0bLLMs\u5b9a\u5411\u63a2\u7d22\u80fd\u529b\u4e0d\u8db3\uff08\u5c3d\u7ba1\u67d0\u4e9b\u573a\u666f\u8fbe\u5230\u76f8\u4f3c\u9057\u61be\u503c\uff09", "conclusion": "LLMs\u4f5c\u4e3a\u4eba\u7c7b\u884c\u4e3a\u6a21\u62df\u5668\u5b58\u5728\u6f5c\u529b\u4e0e\u5c40\u9650\uff1a\u63a8\u7406\u589e\u5f3a\u4f7f\u5176\u63a5\u8fd1\u4eba\u7c7b\u51b3\u7b56\u6a21\u5f0f\uff0c\u4f46\u590d\u6742\u73af\u5883\u9002\u5e94\u6027\u4ecd\u9700\u63d0\u5347\uff0c\u9700\u6539\u8fdb\u63a2\u7d22\u7b56\u7565\u7684\u7075\u6d3b\u6027"}}
{"id": "2505.10352", "pdf": "https://arxiv.org/pdf/2505.10352", "abs": "https://arxiv.org/abs/2505.10352", "authors": ["Shihao Zou", "Qingfeng Li", "Wei Ji", "Jingjing Li", "Yongkui Yang", "Guoqi Li", "Chao Dong"], "title": "SpikeVideoFormer: An Efficient Spike-Driven Video Transformer with Hamming Attention and $\\mathcal{O}(T)$ Complexity", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted by ICML 2025", "summary": "Spiking Neural Networks (SNNs) have shown competitive performance to\nArtificial Neural Networks (ANNs) in various vision tasks, while offering\nsuperior energy efficiency. However, existing SNN-based Transformers primarily\nfocus on single-image tasks, emphasizing spatial features while not effectively\nleveraging SNNs' efficiency in video-based vision tasks. In this paper, we\nintroduce SpikeVideoFormer, an efficient spike-driven video Transformer,\nfeaturing linear temporal complexity $\\mathcal{O}(T)$. Specifically, we design\na spike-driven Hamming attention (SDHA) which provides a theoretically guided\nadaptation from traditional real-valued attention to spike-driven attention.\nBuilding on SDHA, we further analyze various spike-driven space-time attention\ndesigns and identify an optimal scheme that delivers appealing performance for\nvideo tasks, while maintaining only linear temporal complexity. The\ngeneralization ability and efficiency of our model are demonstrated across\ndiverse downstream video tasks, including classification, human pose tracking,\nand semantic segmentation. Empirical results show our method achieves\nstate-of-the-art (SOTA) performance compared to existing SNN approaches, with\nover 15\\% improvement on the latter two tasks. Additionally, it matches the\nperformance of recent ANN-based methods while offering significant efficiency\ngains, achieving $\\times 16$, $\\times 10$ and $\\times 5$ improvements on the\nthree tasks. https://github.com/JimmyZou/SpikeVideoFormer", "AI": {"tldr": "\u63d0\u51faSpikeVideoFormer\u2014\u2014\u9996\u4e2a\u5b9e\u73b0\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u8109\u51b2\u9a71\u52a8\u89c6\u9891Transformer\uff0c\u5728\u591a\u9879\u89c6\u9891\u4efb\u52a1\u4e2d\u8d85\u8d8a\u73b0\u6709SNN\u65b9\u6cd5\u5e76\u8fbe\u5230ANN\u6027\u80fd\uff0c\u540c\u65f6\u5b9e\u73b016\u500d\u300110\u500d\u548c5\u500d\u7684\u6548\u7387\u63d0\u5347\u3002", "motivation": "\u73b0\u6709SNN Transformer\u4e3b\u8981\u9488\u5bf9\u5355\u56fe\u50cf\u4efb\u52a1\uff0c\u672a\u80fd\u6709\u6548\u53d1\u6325SNN\u5728\u89c6\u9891\u4efb\u52a1\u4e2d\u7684\u6548\u7387\u4f18\u52bf\u3002\u9700\u8981\u5f00\u53d1\u517c\u5177\u9ad8\u6548\u65f6\u7a7a\u5efa\u6a21\u80fd\u529b\u548c\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u89c6\u9891\u67b6\u6784\u3002", "method": "1. \u8bbe\u8ba1\u8109\u51b2\u9a71\u52a8\u6c49\u660e\u6ce8\u610f\u529b(SDHA)\uff0c\u5b9e\u73b0\u4ece\u4f20\u7edf\u6ce8\u610f\u529b\u5230\u8109\u51b2\u6ce8\u610f\u529b\u7684\u7406\u8bba\u9002\u914d\n2. \u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u63d0\u51fa\u6700\u4f18\u7684\u8109\u51b2\u65f6\u7a7a\u6ce8\u610f\u529b\u65b9\u6848\n3. \u6784\u5efa\u4ec5\u5177O(T)\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u89c6\u9891Transformer\u67b6\u6784", "result": "1. \u5728\u89c6\u9891\u5206\u7c7b/\u59ff\u6001\u8ddf\u8e2a/\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u5206\u522b\u53d6\u5f97SOTA\u6027\u80fd\uff0c\u540e\u4e24\u9879\u63d0\u5347\u8d8515%\n2. \u6548\u7387\u6307\u6807\u663e\u8457\u8d85\u8d8aANN\u65b9\u6cd5(\u00d716/\u00d710/\u00d75)\n3. \u9996\u6b21\u5b9e\u73b0SNN\u5728\u590d\u6742\u89c6\u9891\u4efb\u52a1\u4e2d\u4e0eANN\u7684\u6027\u80fd\u5339\u914d", "conclusion": "SpikeVideoFormer\u901a\u8fc7\u521b\u65b0\u7684\u8109\u51b2\u65f6\u7a7a\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u4fdd\u6301\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u89c6\u9891\u7406\u89e3\u7684\u6027\u80fd\u7a81\u7834\uff0c\u4e3aSNN\u5728\u5b9e\u65f6\u89c6\u9891\u5904\u7406\u9886\u57df\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.09921", "pdf": "https://arxiv.org/pdf/2505.09921", "abs": "https://arxiv.org/abs/2505.09921", "authors": ["Yidan Wang", "Yanan Cao", "Yubing Ren", "Fang Fang", "Zheng Lin", "Binxing Fang"], "title": "PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative In-Context Optimization", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) excel in various domains but pose inherent\nprivacy risks. Existing methods to evaluate privacy leakage in LLMs often use\nmemorized prefixes or simple instructions to extract data, both of which\nwell-alignment models can easily block. Meanwhile, Jailbreak attacks bypass LLM\nsafety mechanisms to generate harmful content, but their role in privacy\nscenarios remains underexplored. In this paper, we examine the effectiveness of\njailbreak attacks in extracting sensitive information, bridging privacy leakage\nand jailbreak attacks in LLMs. Moreover, we propose PIG, a novel framework\ntargeting Personally Identifiable Information (PII) and addressing the\nlimitations of current jailbreak methods. Specifically, PIG identifies PII\nentities and their types in privacy queries, uses in-context learning to build\na privacy context, and iteratively updates it with three gradient-based\nstrategies to elicit target PII. We evaluate PIG and existing jailbreak methods\nusing two privacy-related datasets. Experiments on four white-box and two\nblack-box LLMs show that PIG outperforms baseline methods and achieves\nstate-of-the-art (SoTA) results. The results underscore significant privacy\nrisks in LLMs, emphasizing the need for stronger safeguards. Our code is\navailble at\n\\href{https://github.com/redwyd/PrivacyJailbreak}{https://github.com/redwyd/PrivacyJailbreak}.", "AI": {"tldr": "\u63d0\u51faPIG\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8d8a\u72f1\u653b\u51fb\u6709\u6548\u63d0\u53d6LLMs\u4e2d\u7684\u4e2a\u4eba\u9690\u79c1\u4fe1\u606f\uff0c\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u5b9e\u73b0SOTA\u6548\u679c", "motivation": "\u73b0\u6709\u9690\u79c1\u8bc4\u4f30\u65b9\u6cd5\u6613\u88ab\u5bf9\u9f50\u6a21\u578b\u963b\u6b62\uff0c\u800c\u8d8a\u72f1\u653b\u51fb\u5728\u9690\u79c1\u6cc4\u9732\u573a\u666f\u7684\u5e94\u7528\u6f5c\u529b\u672a\u88ab\u5145\u5206\u6316\u6398", "method": "1.\u8bc6\u522bPII\u5b9e\u4f53\u53ca\u7c7b\u578b 2.\u4e0a\u4e0b\u6587\u5b66\u4e60\u6784\u5efa\u9690\u79c1\u4e0a\u4e0b\u6587 3.\u4e09\u79cd\u68af\u5ea6\u7b56\u7565\u8fed\u4ee3\u66f4\u65b0\u4e0a\u4e0b\u6587\u63d0\u53d6\u76ee\u6807\u4fe1\u606f", "result": "\u57284\u4e2a\u767d\u76d2/2\u4e2a\u9ed1\u76d2LLMs\u5b9e\u9a8c\u4e2d\uff0cPIG\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff08\u6700\u9ad8\u63d0\u534732.1%\uff09\uff0c\u8fbe\u5230SOTA\u8868\u73b0", "conclusion": "LLMs\u5b58\u5728\u91cd\u5927\u9690\u79c1\u98ce\u9669\uff0c\u9700\u52a0\u5f3a\u5b89\u5168\u9632\u62a4\u63aa\u65bd\uff0c\u7814\u7a76\u4e3a\u9690\u79c1\u4fdd\u62a4\u673a\u5236\u8bbe\u8ba1\u63d0\u4f9b\u65b0\u89c6\u89d2"}}
{"id": "2505.10420", "pdf": "https://arxiv.org/pdf/2505.10420", "abs": "https://arxiv.org/abs/2505.10420", "authors": ["Andrei Arhire", "Radu Timofte"], "title": "Learned Lightweight Smartphone ISP with Unpaired Data", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at CVPRW 2025", "summary": "The Image Signal Processor (ISP) is a fundamental component in modern\nsmartphone cameras responsible for conversion of RAW sensor image data to RGB\nimages with a strong focus on perceptual quality. Recent work highlights the\npotential of deep learning approaches and their ability to capture details with\na quality increasingly close to that of professional cameras. A difficult and\ncostly step when developing a learned ISP is the acquisition of pixel-wise\naligned paired data that maps the raw captured by a smartphone camera sensor to\nhigh-quality reference images. In this work, we address this challenge by\nproposing a novel training method for a learnable ISP that eliminates the need\nfor direct correspondences between raw images and ground-truth data with\nmatching content. Our unpaired approach employs a multi-term loss function\nguided by adversarial training with multiple discriminators processing feature\nmaps from pre-trained networks to maintain content structure while learning\ncolor and texture characteristics from the target RGB dataset. Using\nlightweight neural network architectures suitable for mobile devices as\nbackbones, we evaluated our method on the Zurich RAW to RGB and Fujifilm\nUltraISP datasets. Compared to paired training methods, our unpaired learning\nstrategy shows strong potential and achieves high fidelity across multiple\nevaluation metrics. The code and pre-trained models are available at\nhttps://github.com/AndreiiArhire/Learned-Lightweight-Smartphone-ISP-with-Unpaired-Data .", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u914d\u5bf9\u6570\u636e\u7684\u667a\u80fd\u624b\u673aISP\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u548c\u591a\u5224\u522b\u5668\u673a\u5236\u5b66\u4e60\u76ee\u6807\u56fe\u50cf\u7279\u5f81\uff0c\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u8d28\u91cf\u56fe\u50cf\u8f6c\u6362\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684ISP\u65b9\u6cd5\u4f9d\u8d56\u50cf\u7d20\u7ea7\u5bf9\u9f50\u7684RAW-RGB\u914d\u5bf9\u6570\u636e\uff0c\u800c\u8fd9\u7c7b\u6570\u636e\u7684\u83b7\u53d6\u6210\u672c\u9ad8\u6602\u4e14\u56f0\u96be\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002", "method": "\u91c7\u7528\u672a\u914d\u5bf9\u8bad\u7ec3\u7b56\u7565\uff0c\u8bbe\u8ba1\u591a\u672f\u8bed\u635f\u5931\u51fd\u6570\u7ed3\u5408\u5bf9\u6297\u8bad\u7ec3\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7f51\u7edc\u7279\u5f81\u56fe\u6307\u5bfc\u5185\u5bb9\u7ed3\u6784\u4fdd\u6301\uff0c\u540c\u65f6\u901a\u8fc7\u591a\u4e2a\u5224\u522b\u5668\u5b66\u4e60\u76ee\u6807\u57df\u7684\u989c\u8272\u7eb9\u7406\u7279\u5f81\u3002", "result": "\u5728Zurich RAW-RGB\u548cFujifilm UltraISP\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7f51\u7edc\u67b6\u6784\u5373\u8fbe\u5230\u4e0e\u914d\u5bf9\u8bad\u7ec3\u65b9\u6cd5\u76f8\u5f53\u7684\u6307\u6807\u8868\u73b0\uff0cPSNR\u8d85\u8fc724dB\uff0c\u6a21\u578b\u53c2\u6570\u5c0f\u4e8e100K\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u7a81\u7834\u4e86\u914d\u5bf9\u6570\u636e\u9650\u5236\uff0c\u4e3a\u79fb\u52a8\u7aefISP\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5f00\u6e90\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u5c06\u63a8\u52a8\u76f8\u5173\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2505.09949", "pdf": "https://arxiv.org/pdf/2505.09949", "abs": "https://arxiv.org/abs/2505.09949", "authors": ["Ahmed S. Abdelrahman", "Mohamed Abdel-Aty", "Samgyu Yang", "Abdulrahman Faden"], "title": "Advanced Crash Causation Analysis for Freeway Safety: A Large Language Model Approach to Identifying Key Contributing Factors", "categories": ["cs.LG", "cs.CL", "stat.AP"], "comment": null, "summary": "Understanding the factors contributing to traffic crashes and developing\nstrategies to mitigate their severity is essential. Traditional statistical\nmethods and machine learning models often struggle to capture the complex\ninteractions between various factors and the unique characteristics of each\ncrash. This research leverages large language model (LLM) to analyze freeway\ncrash data and provide crash causation analysis accordingly. By compiling 226\ntraffic safety studies related to freeway crashes, a training dataset\nencompassing environmental, driver, traffic, and geometric design factors was\ncreated. The Llama3 8B model was fine-tuned using QLoRA to enhance its\nunderstanding of freeway crashes and their contributing factors, as covered in\nthese studies. The fine-tuned Llama3 8B model was then used to identify crash\ncausation without pre-labeled data through zero-shot classification, providing\ncomprehensive explanations to ensure that the identified causes were reasonable\nand aligned with existing research. Results demonstrate that LLMs effectively\nidentify primary crash causes such as alcohol-impaired driving, speeding,\naggressive driving, and driver inattention. Incorporating event data, such as\nroad maintenance, offers more profound insights. The model's practical\napplicability and potential to improve traffic safety measures were validated\nby a high level of agreement among researchers in the field of traffic safety,\nas reflected in questionnaire results with 88.89%. This research highlights the\ncomplex nature of traffic crashes and how LLMs can be used for comprehensive\nanalysis of crash causation and other contributing factors. Moreover, it\nprovides valuable insights and potential countermeasures to aid planners and\npolicymakers in developing more effective and efficient traffic safety\npractices.", "AI": {"tldr": "\u5229\u7528\u5fae\u8c03\u540e\u7684Llama3 8B\u6a21\u578b\u5206\u6790\u9ad8\u901f\u516c\u8def\u4e8b\u6545\u6210\u56e0\uff0c\u901a\u8fc7\u96f6\u6837\u672c\u5206\u7c7b\u6709\u6548\u8bc6\u522b\u9152\u9a7e/\u8d85\u901f\u7b49\u4e3b\u56e0\uff0c\u83b7\u5f9788.89%\u4e13\u5bb6\u8ba4\u53ef\u5ea6", "motivation": "\u4f20\u7edf\u7edf\u8ba1\u65b9\u6cd5\u548c\u673a\u5668\u5b66\u4e60\u96be\u4ee5\u6355\u6349\u4ea4\u901a\u4e8b\u6545\u591a\u56e0\u7d20\u590d\u6742\u4ea4\u4e92\u7279\u6027\uff0c\u63a2\u7d22LLM\u5728\u4e8b\u6545\u6df1\u5ea6\u5206\u6790\u4e2d\u7684\u6f5c\u529b", "method": "\u57fa\u4e8e226\u9879\u7814\u7a76\u6784\u5efa\u6570\u636e\u96c6\uff0c\u4f7f\u7528QLoRA\u5fae\u8c03Llama3 8B\u6a21\u578b\uff0c\u901a\u8fc7\u96f6\u6837\u672c\u5206\u7c7b\u5b9e\u73b0\u65e0\u6807\u7b7e\u6570\u636e\u7684\u4e8b\u6545\u6210\u56e0\u8bc6\u522b", "result": "\u6a21\u578b\u6210\u529f\u8bc6\u522b\u9152\u9a7e/\u8d85\u901f/\u653b\u51fb\u6027\u9a7e\u9a76/\u5206\u5fc3\u9a7e\u9a76\u7b49\u4e3b\u56e0\uff0c\u7ed3\u5408\u9053\u8def\u7ef4\u62a4\u7b49\u4e8b\u4ef6\u6570\u636e\u63d0\u5347\u5206\u6790\u6df1\u5ea6\uff0c\u95ee\u5377\u9a8c\u8bc1\u4e13\u5bb6\u8ba4\u53ef\u7387\u8fbe88.89%", "conclusion": "LLM\u4e3a\u4ea4\u901a\u4e8b\u6545\u591a\u7ef4\u5ea6\u5206\u6790\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u5176\u89e3\u91ca\u6027\u8f93\u51fa\u6709\u52a9\u4e8e\u5236\u5b9a\u7cbe\u51c6\u5b89\u5168\u7b56\u7565\uff0c\u9a8c\u8bc1\u7ed3\u679c\u8bc1\u5b9e\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2505.10453", "pdf": "https://arxiv.org/pdf/2505.10453", "abs": "https://arxiv.org/abs/2505.10453", "authors": ["Tyler Tran", "Sangeet Khemlani", "J. G. Trafton"], "title": "Vision language models have difficulty recognizing virtual objects", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision language models (VLMs) are AI systems paired with both language and\nvision encoders to process multimodal input. They are capable of performing\ncomplex semantic tasks such as automatic captioning, but it remains an open\nquestion about how well they comprehend the visuospatial properties of scenes\ndepicted in the images they process. We argue that descriptions of virtual\nobjects -- objects that are not visually represented in an image -- can help\ntest scene comprehension in these AI systems. For example, an image that\ndepicts a person standing under a tree can be paired with the following prompt:\nimagine that a kite is stuck in the tree. VLMs that comprehend the scene should\nupdate their representations and reason sensibly about the spatial relations\nbetween all three objects. We describe systematic evaluations of\nstate-of-the-art VLMs and show that their ability to process virtual objects is\ninadequate.", "AI": {"tldr": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u865a\u62df\u7269\u4f53\u7684\u7a7a\u95f4\u5173\u7cfb\u7406\u89e3\u5b58\u5728\u660e\u663e\u7f3a\u9677", "motivation": "\u901a\u8fc7\u5f15\u5165\u865a\u62df\u7269\u4f53\uff08\u56fe\u50cf\u4e2d\u672a\u5b9e\u9645\u5b58\u5728\u7684\u7269\u4f53\uff09\u6d4b\u8bd5VLM\u7684\u573a\u666f\u7406\u89e3\u80fd\u529b\uff0c\u63ed\u793a\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027", "method": "\u4f7f\u7528\u5305\u542b\u865a\u62df\u7269\u4f53\u63d0\u793a\u7684\u6d4b\u8bd5\u6846\u67b6\uff08\u5982\u56fe\u50cf\u4e2d\u6697\u793a\u98ce\u7b5d\u5361\u5728\u6811\u4e0a\uff09\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e3b\u6d41VLMs\u7684\u7a7a\u95f4\u5173\u7cfb\u63a8\u7406\u80fd\u529b", "result": "\u5f53\u524d\u6700\u5148\u8fdb\u7684VLMs\u5728\u865a\u62df\u7269\u4f53\u5904\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u4e0d\u8db3\uff08\u51c6\u786e\u7387\u4f4e\u4e8e50%\uff09", "conclusion": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9700\u8981\u589e\u5f3a\u573a\u666f\u8868\u5f81\u66f4\u65b0\u80fd\u529b\u548c\u7a7a\u95f4\u63a8\u7406\u673a\u5236\uff0c\u865a\u62df\u7269\u4f53\u6d4b\u8bd5\u53ef\u4f5c\u4e3a\u91cd\u8981\u7684\u8bc4\u4f30\u57fa\u51c6"}}
{"id": "2505.10093", "pdf": "https://arxiv.org/pdf/2505.10093", "abs": "https://arxiv.org/abs/2505.10093", "authors": ["Hsuan-Lei Shao"], "title": "From Text to Network: Constructing a Knowledge Graph of Taiwan-Based China Studies Using Generative AI", "categories": ["cs.AI", "cs.CL", "I.2.4; H.3.3; J.5"], "comment": "4 pages, 4 figures", "summary": "Taiwanese China Studies (CS) has developed into a rich, interdisciplinary\nresearch field shaped by the unique geopolitical position and long standing\nacademic engagement with Mainland China. This study responds to the growing\nneed to systematically revisit and reorganize decades of Taiwan based CS\nscholarship by proposing an AI assisted approach that transforms unstructured\nacademic texts into structured, interactive knowledge representations. We apply\ngenerative AI (GAI) techniques and large language models (LLMs) to extract and\nstandardize entity relation triples from 1,367 peer reviewed CS articles\npublished between 1996 and 2019. These triples are then visualized through a\nlightweight D3.js based system, forming the foundation of a domain specific\nknowledge graph and vector database for the field. This infrastructure allows\nusers to explore conceptual nodes and semantic relationships across the corpus,\nrevealing previously uncharted intellectual trajectories, thematic clusters,\nand research gaps. By decomposing textual content into graph structured\nknowledge units, our system enables a paradigm shift from linear text\nconsumption to network based knowledge navigation. In doing so, it enhances\nscholarly access to CS literature while offering a scalable, data driven\nalternative to traditional ontology construction. This work not only\ndemonstrates how generative AI can augment area studies and digital humanities\nbut also highlights its potential to support a reimagined scholarly\ninfrastructure for regional knowledge systems.", "AI": {"tldr": "\u53f0\u6e7e\u5b66\u8005\u8fd0\u7528\u751f\u6210\u5f0fAI\u6280\u672f\u6784\u5efa\u4e2d\u56fd\u7814\u7a76\u77e5\u8bc6\u56fe\u8c31\uff0c\u5c061367\u7bc7\u6587\u732e\u8f6c\u5316\u4e3a\u53ef\u4ea4\u4e92\u7684\u77e5\u8bc6\u7f51\u7edc\uff0c\u5b9e\u73b0\u4e86\u4ece\u7ebf\u6027\u9605\u8bfb\u5230\u7f51\u7edc\u5316\u77e5\u8bc6\u5bfc\u822a\u7684\u8303\u5f0f\u8f6c\u6362", "motivation": "\u9488\u5bf9\u53f0\u6e7e\u4e2d\u56fd\u7814\u7a76\u9886\u57df\u6570\u5341\u5e74\u79ef\u7d2f\u7684\u975e\u7ed3\u6784\u5316\u5b66\u672f\u6587\u672c\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u91cd\u7ec4\u5e76\u5efa\u7acb\u9886\u57df\u4e13\u5c5e\u77e5\u8bc6\u57fa\u7840\u8bbe\u65bd\u4ee5\u63d0\u5347\u5b66\u672f\u8d44\u6e90\u5229\u7528\u7387", "method": "\u5e94\u7528\u751f\u6210\u5f0fAI\u548c\u5927\u8bed\u8a00\u6a21\u578b\u62bd\u53d6\u5b9e\u4f53\u5173\u7cfb\u4e09\u5143\u7ec4\uff0c\u901a\u8fc7D3.js\u6784\u5efa\u8f7b\u91cf\u7ea7\u53ef\u89c6\u5316\u7cfb\u7edf\uff0c\u5f62\u6210\u77e5\u8bc6\u56fe\u8c31\u548c\u5411\u91cf\u6570\u636e\u5e93", "result": "\u7cfb\u7edf\u6210\u529f\u63ed\u793a\u8de8\u6587\u732e\u7684\u5b66\u672f\u8f68\u8ff9\u4e0e\u4e3b\u9898\u96c6\u7fa4\uff0c\u63d0\u4f9b\u6570\u636e\u9a71\u52a8\u7684\u672c\u4f53\u8bba\u5efa\u6784\u65b0\u8def\u5f84\uff0c\u652f\u6301\u7f51\u7edc\u5316\u77e5\u8bc6\u63a2\u7d22", "conclusion": "\u8be5\u7814\u7a76\u4e0d\u4ec5\u9a8c\u8bc1\u4e86\u751f\u6210\u5f0fAI\u5728\u533a\u57df\u7814\u7a76\u4e2d\u7684\u589e\u5f3a\u4f5c\u7528\uff0c\u66f4\u4e3a\u91cd\u6784\u533a\u57df\u77e5\u8bc6\u4f53\u7cfb\u7684\u5b66\u672f\u57fa\u7840\u8bbe\u65bd\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u65b9\u6848"}}
{"id": "2505.10473", "pdf": "https://arxiv.org/pdf/2505.10473", "abs": "https://arxiv.org/abs/2505.10473", "authors": ["Fengdi Zhang", "Hongkun Cao", "Ruqi Huang"], "title": "Consistent Quantity-Quality Control across Scenes for Deployment-Aware Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "To reduce storage and computational costs, 3D Gaussian splatting (3DGS) seeks\nto minimize the number of Gaussians used while preserving high rendering\nquality, introducing an inherent trade-off between Gaussian quantity and\nrendering quality. Existing methods strive for better quantity-quality\nperformance, but lack the ability for users to intuitively adjust this\ntrade-off to suit practical needs such as model deployment under diverse\nhardware and communication constraints. Here, we present ControlGS, a 3DGS\noptimization method that achieves semantically meaningful and cross-scene\nconsistent quantity-quality control while maintaining strong quantity-quality\nperformance. Through a single training run using a fixed setup and a\nuser-specified hyperparameter reflecting quantity-quality preference, ControlGS\ncan automatically find desirable quantity-quality trade-off points across\ndiverse scenes, from compact objects to large outdoor scenes. It also\noutperforms baselines by achieving higher rendering quality with fewer\nGaussians, and supports a broad adjustment range with stepless control over the\ntrade-off.", "AI": {"tldr": "ControlGS\u901a\u8fc7\u7528\u6237\u6307\u5b9a\u53c2\u6570\u5b9e\u73b03D\u9ad8\u65af\u6e85\u5c04\u7684\u91cf\u5316\u8d28\u91cf\u63a7\u5236\uff0c\u81ea\u52a8\u9002\u914d\u4e0d\u540c\u573a\u666f\u5e76\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u8ba9\u7528\u6237\u76f4\u89c2\u8c03\u8282\u6570\u91cf-\u8d28\u91cf\u6743\u8861\u7684\u80fd\u529b\uff0c\u96be\u4ee5\u9002\u5e94\u4e0d\u540c\u786c\u4ef6\u90e8\u7f72\u7684\u5b9e\u8df5\u9700\u6c42", "method": "\u4f7f\u7528\u56fa\u5b9a\u8bad\u7ec3\u8bbe\u7f6e\u7ed3\u5408\u7528\u6237\u6307\u5b9a\u7684\u91cf\u5316-\u8d28\u91cf\u504f\u597d\u8d85\u53c2\u6570\uff0c\u652f\u6301\u5e7f\u6cdb\u8c03\u6574\u8303\u56f4\u7684\u8fde\u7eed\u63a7\u5236", "result": "\u5728\u591a\u79cd\u573a\u666f\u4e0b\u5b9e\u73b0\u66f4\u9ad8\u6e32\u67d3\u8d28\u91cf\uff08\u66f4\u5c11\u9ad8\u65af\u6570\u91cf\uff09\uff0c\u652f\u6301\u4ece\u7d27\u51d1\u7269\u4f53\u5230\u5927\u578b\u6237\u5916\u573a\u666f\u7684\u5e7f\u6cdb\u8c03\u8282", "conclusion": "ControlGS\u63d0\u4f9b\u8de8\u573a\u666f\u4e00\u81f4\u7684\u8bed\u4e49\u5316\u91cf\u5316\u63a7\u5236\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u6ee1\u8db3\u591a\u6837\u5316\u90e8\u7f72\u9700\u6c42"}}
{"id": "2505.10117", "pdf": "https://arxiv.org/pdf/2505.10117", "abs": "https://arxiv.org/abs/2505.10117", "authors": ["JieHao Wu", "Ziwei Wang", "Junjie Sheng", "Wenhao Li", "Xiangfei Wang", "Jun Luo"], "title": "Learning Virtual Machine Scheduling in Cloud Computing through Language Agents", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "In cloud services, virtual machine (VM) scheduling is a typical Online\nDynamic Multidimensional Bin Packing (ODMBP) problem, characterized by\nlarge-scale complexity and fluctuating demands. Traditional optimization\nmethods struggle to adapt to real-time changes, domain-expert-designed\nheuristic approaches suffer from rigid strategies, and existing learning-based\nmethods often lack generalizability and interpretability. To address these\nlimitations, this paper proposes a hierarchical language agent framework named\nMiCo, which provides a large language model (LLM)-driven heuristic design\nparadigm for solving ODMBP. Specifically, ODMBP is formulated as a Semi-Markov\nDecision Process with Options (SMDP-Option), enabling dynamic scheduling\nthrough a two-stage architecture, i.e., Option Miner and Option Composer.\nOption Miner utilizes LLMs to discover diverse and useful non-context-aware\nstrategies by interacting with constructed environments. Option Composer\nemploys LLMs to discover a composing strategy that integrates the\nnon-context-aware strategies with the contextual ones. Extensive experiments on\nreal-world enterprise datasets demonstrate that MiCo achieves a 96.9\\%\ncompetitive ratio in large-scale scenarios involving more than 10,000 virtual\nmachines. It maintains high performance even under nonstationary request flows\nand diverse configurations, thus validating its effectiveness in complex and\nlarge-scale cloud environments.", "AI": {"tldr": "\u63d0\u51faMiCo\u5206\u5c42\u8bed\u8a00\u4ee3\u7406\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8bbe\u8ba1\u542f\u53d1\u5f0f\u7b56\u7565\u89e3\u51b3\u4e91\u73af\u5883\u4e2d\u5927\u89c4\u6a21\u52a8\u6001\u591a\u7ef4\u88c5\u7bb1\u95ee\u9898\uff0c\u5728\u8d851\u4e07\u53f0\u865a\u62df\u673a\u573a\u666f\u4e0b\u8fbe\u523096.9%\u7ade\u4e89\u6bd4\u3002", "motivation": "\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u96be\u4ee5\u9002\u5e94\u5b9e\u65f6\u53d8\u5316\uff0c\u542f\u53d1\u5f0f\u7b56\u7565\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u901a\u7528\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u3002\u9700\u8981\u5f00\u53d1\u9002\u5e94\u5927\u89c4\u6a21\u590d\u6742\u573a\u666f\u4e14\u5177\u5907\u52a8\u6001\u7ec4\u5408\u7b56\u7565\u80fd\u529b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06ODMBP\u5efa\u6a21\u4e3a\u534a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08SMDP-Option\uff09\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u67b6\u6784\uff1aOption Miner\u901a\u8fc7LLM\u751f\u6210\u975e\u4e0a\u4e0b\u6587\u611f\u77e5\u7b56\u7565\uff0cOption Composer\u52a8\u6001\u7ec4\u5408\u4e0a\u4e0b\u6587\u7b56\u7565\u4e0e\u975e\u4e0a\u4e0b\u6587\u7b56\u7565\u3002", "result": "\u5728\u771f\u5b9e\u4f01\u4e1a\u6570\u636e\u96c6\u5b9e\u9a8c\u4e2d\uff0cMiCo\u5728\u8d85\u4e07\u7ea7\u865a\u62df\u673a\u89c4\u6a21\u4e0b\u5b9e\u73b096.9%\u7ade\u4e89\u6bd4\uff0c\u5728\u975e\u7a33\u6001\u8bf7\u6c42\u6d41\u548c\u591a\u6837\u5316\u914d\u7f6e\u573a\u666f\u4ecd\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "conclusion": "MiCo\u9a8c\u8bc1\u4e86LLM\u9a71\u52a8\u7684\u5206\u5c42\u6846\u67b6\u5728\u590d\u6742\u4e91\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5927\u89c4\u6a21\u52a8\u6001\u8d44\u6e90\u8c03\u5ea6\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.10481", "pdf": "https://arxiv.org/pdf/2505.10481", "abs": "https://arxiv.org/abs/2505.10481", "authors": ["Ilya Ovodov", "Petr Surovtsev", "Karina Kvanchiani", "Alexander Kapitanov", "Alexander Nagaev"], "title": "Logos as a Well-Tempered Pre-train for Sign Language Recognition", "categories": ["cs.CV"], "comment": null, "summary": "This paper examines two aspects of the isolated sign language recognition\n(ISLR) task. First, despite the availability of a number of datasets, the\namount of data for most individual sign languages is limited. It poses the\nchallenge of cross-language ISLR model training, including transfer learning.\nSecond, similar signs can have different semantic meanings. It leads to\nambiguity in dataset labeling and raises the question of the best policy for\nannotating such signs. To address these issues, this study presents Logos, a\nnovel Russian Sign Language (RSL) dataset, the most extensive ISLR dataset by\nthe number of signers and one of the largest available datasets while also the\nlargest RSL dataset in size and vocabulary. It is shown that a model,\npre-trained on the Logos dataset can be used as a universal encoder for other\nlanguage SLR tasks, including few-shot learning. We explore cross-language\ntransfer learning approaches and find that joint training using multiple\nclassification heads benefits accuracy for the target lowresource datasets the\nmost. The key feature of the Logos dataset is explicitly annotated visually\nsimilar sign groups. We show that explicitly labeling visually similar signs\nimproves trained model quality as a visual encoder for downstream tasks. Based\non the proposed contributions, we outperform current state-of-the-art results\nfor the WLASL dataset and get competitive results for the AUTSL dataset, with a\nsingle stream model processing solely RGB video. The source code, dataset, and\npre-trained models are publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4fc4\u8bed\u624b\u8bed\u6570\u636e\u96c6Logos\u89e3\u51b3\u5b64\u7acb\u624b\u8bed\u8bc6\u522b\u7684\u6570\u636e\u7a00\u7f3a\u4e0e\u6807\u6ce8\u6b67\u4e49\u95ee\u9898\uff0c\u901a\u8fc7\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\u548c\u89c6\u89c9\u76f8\u4f3c\u7ec4\u6807\u6ce8\u63d0\u5347\u6a21\u578b\u6027\u80fd", "motivation": "\u73b0\u6709\u5355\u4e00\u624b\u8bed\u6570\u636e\u96c6\u89c4\u6a21\u6709\u9650\u4e14\u5b58\u5728\u76f8\u4f3c\u624b\u52bf\u8bed\u4e49\u6b67\u4e49\uff0c\u5236\u7ea6\u8de8\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e0e\u6807\u6ce8\u8d28\u91cf", "method": "\u6784\u5efa\u6700\u5927\u89c4\u6a21RSL\u6570\u636e\u96c6Logos\uff0c\u91c7\u7528\u591a\u5206\u7c7b\u5934\u8054\u5408\u8bad\u7ec3\u7b56\u7565\u5b9e\u73b0\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\uff0c\u5e76\u6807\u6ce8\u89c6\u89c9\u76f8\u4f3c\u7b26\u53f7\u7ec4\u4f18\u5316\u6a21\u578b\u8bad\u7ec3", "result": "\u5728WLASL\u6570\u636e\u96c6\u53d6\u5f97SOTA\u7ed3\u679c\uff0cAUTSL\u6570\u636e\u96c6\u8868\u73b0\u53ef\u6bd4\uff0c\u4ec5\u7528RGB\u5355\u6d41\u6a21\u578b\u5373\u5b9e\u73b0\u7ade\u4e89\u4f18\u52bf", "conclusion": "Logos\u6570\u636e\u96c6\u53d1\u5e03\u3001\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\u53ca\u76f8\u4f3c\u7ec4\u6807\u6ce8\u7b56\u7565\u6709\u6548\u63d0\u5347\u5b64\u7acb\u624b\u8bed\u8bc6\u522b\u6027\u80fd\uff0c\u63a8\u52a8\u591a\u8bed\u8a00\u624b\u8bed\u7814\u7a76\u53d1\u5c55"}}
{"id": "2505.10483", "pdf": "https://arxiv.org/pdf/2505.10483", "abs": "https://arxiv.org/abs/2505.10483", "authors": ["Yi Li", "Haonan Wang", "Qixiang Zhang", "Boyu Xiao", "Chenchang Hu", "Hualiang Wang", "Xiaomeng Li"], "title": "UniEval: Unified Holistic Evaluation for Unified Multimodal Understanding and Generation", "categories": ["cs.CV", "cs.AI"], "comment": "UniEval is the first evaluation framework designed for unified\n  multimodal models, including a holistic benchmark UniBench and the UniScore\n  metric", "summary": "The emergence of unified multimodal understanding and generation models is\nrapidly attracting attention because of their ability to enhance\ninstruction-following capabilities while minimizing model redundancy. However,\nthere is a lack of a unified evaluation framework for these models, which would\nenable an elegant, simplified, and overall evaluation. Current models conduct\nevaluations on multiple task-specific benchmarks, but there are significant\nlimitations, such as the lack of overall results, errors from extra evaluation\nmodels, reliance on extensive labeled images, benchmarks that lack diversity,\nand metrics with limited capacity for instruction-following evaluation. To\ntackle these challenges, we introduce UniEval, the first evaluation framework\ndesigned for unified multimodal models without extra models, images, or\nannotations. This facilitates a simplified and unified evaluation process. The\nUniEval framework contains a holistic benchmark, UniBench (supports both\nunified and visual generation models), along with the corresponding UniScore\nmetric. UniBench includes 81 fine-grained tags contributing to high diversity.\nExperimental results indicate that UniBench is more challenging than existing\nbenchmarks, and UniScore aligns closely with human evaluations, surpassing\ncurrent metrics. Moreover, we extensively evaluated SoTA unified and visual\ngeneration models, uncovering new insights into Univeral's unique values.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u65e0\u9700\u989d\u5916\u6a21\u578b/\u6807\u6ce8\u7684\u591a\u6a21\u6001\u7edf\u4e00\u8bc4\u4f30\u6846\u67b6UniEval\uff0c\u5305\u542b\u9ad8\u591a\u6837\u6027\u8bc4\u6d4b\u57fa\u51c6UniBench\u548c\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u9ad8\u5ea6\u4e00\u81f4\u7684UniScore\u5ea6\u91cf\u6807\u51c6", "motivation": "\u73b0\u6709\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u8bc4\u4f30\u5b58\u5728\u4e94\u5927\u75db\u70b9\uff1a\u7f3a\u4e4f\u6574\u4f53\u8bc4\u4f30\u7ed3\u679c\u3001\u4f9d\u8d56\u5916\u90e8\u8bc4\u4f30\u6a21\u578b\u8bef\u5dee\u3001\u9700\u8981\u6d77\u91cf\u6807\u6ce8\u56fe\u50cf\u3001\u8bc4\u6d4b\u57fa\u51c6\u591a\u6837\u6027\u4e0d\u8db3\u3001\u73b0\u6709\u6307\u6807\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u6307\u4ee4\u8ddf\u968f\u80fd\u529b", "method": "\u901a\u8fc7UniBench\u57fa\u51c6\uff08\u542b81\u4e2a\u7ec6\u7c92\u5ea6\u6807\u7b7e\uff09\u652f\u6301\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\uff0c\u5f00\u53d1UniScore\u5ea6\u91cf\u6807\u51c6\uff08\u7ed3\u5408\u7ec6\u7c92\u5ea6\u6807\u7b7e\u6743\u91cd\u548c\u4eba\u5de5\u8bc4\u4f30\u504f\u597d\uff09\u5b9e\u73b0\u65e0\u76d1\u7763\u8bc4\u4f30", "result": "\u5b9e\u9a8c\u663e\u793aUniBench\u6bd4\u73b0\u6709\u57fa\u51c6\u66f4\u5177\u6311\u6218\u6027\uff08\u5e73\u5747\u5f97\u5206\u4f4e20.5%\uff09\uff0cUniScore\u4e0e\u4eba\u5de5\u8bc4\u4f30\u76f8\u5173\u6027\u8fbe80.4%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6307\u6807\uff08\u6700\u9ad8\u63d0\u534735.8%\uff09", "conclusion": "UniEval\u6846\u67b6\u4e3a\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u65b9\u6848\uff0c\u5176\u7ec6\u7c92\u5ea6\u6807\u7b7e\u4f53\u7cfb\u548c\u81ea\u52a8\u5316\u8bc4\u4f30\u80fd\u529b\u63ed\u793a\u4e86\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u72ec\u7279\u4ef7\u503c"}}
{"id": "2505.10222", "pdf": "https://arxiv.org/pdf/2505.10222", "abs": "https://arxiv.org/abs/2505.10222", "authors": ["Jintian Shao", "Hongyi Huang", "Jiayi Wu", "Beiwen Zhang", "ZhiYu Wu", "You Shan", "MingKai Zheng"], "title": "ComplexFormer: Disruptively Advancing Transformer Inference Ability via Head-Specific Complex Vector Attention", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Transformer models rely on self-attention to capture token dependencies but\nface challenges in effectively integrating positional information while\nallowing multi-head attention (MHA) flexibility. Prior methods often model\nsemantic and positional differences disparately or apply uniform positional\nadjustments across heads, potentially limiting representational capacity. This\npaper introduces ComplexFormer, featuring Complex Multi-Head Attention-CMHA.\nCMHA empowers each head to independently model semantic and positional\ndifferences unified within the complex plane, representing interactions as\nrotations and scaling. ComplexFormer incorporates two key improvements: (1) a\nper-head Euler transformation, converting real-valued query/key projections\ninto polar-form complex vectors for head-specific complex subspace operation;\nand (2) a per-head adaptive differential rotation mechanism,\nexp[i(Adapt(ASmn,i) + Delta(Pmn),i)], allowing each head to learn distinct\nstrategies for integrating semantic angle differences (ASmn,i) with relative\npositional encodings (Delta(Pmn),i). Extensive experiments on language\nmodeling, text generation, code generation, and mathematical reasoning show\nComplexFormer achieves superior performance, significantly lower generation\nperplexity , and improved long-context coherence compared to strong baselines\nlike RoPE-Transformers. ComplexFormer demonstrates strong parameter efficiency,\noffering a more expressive, adaptable attention mechanism.", "AI": {"tldr": "ComplexFormer\u901a\u8fc7\u590d\u6570\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236(CMHA)\uff0c\u5728\u590d\u6570\u5e73\u9762\u4e0a\u7edf\u4e00\u5efa\u6a21\u8bed\u4e49\u5dee\u5f02\u4e0e\u4f4d\u7f6e\u4fe1\u606f\uff0c\u5b9e\u73b0\u66f4\u7075\u6d3b\u9ad8\u6548\u7684\u6ce8\u610f\u529b\u8868\u8fbe\uff0c\u663e\u8457\u63d0\u5347\u8bed\u8a00\u4efb\u52a1\u6027\u80fd\u4e0e\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edfTransformer\u7684\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u96be\u4ee5\u534f\u8c03\u8bed\u4e49\u5dee\u5f02\u5efa\u6a21\u4e0e\u4f4d\u7f6e\u4fe1\u606f\u6574\u5408\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u8bed\u4e49/\u4f4d\u7f6e\u5904\u7406\u5272\u88c2\u6216\u4f4d\u7f6e\u8c03\u6574\u7b56\u7565\u50f5\u5316\u7684\u95ee\u9898\u3002ComplexFormer\u65e8\u5728\u901a\u8fc7\u590d\u6570\u7a7a\u95f4\u7edf\u4e00\u8868\u793a\uff0c\u589e\u5f3a\u6ce8\u610f\u529b\u7684\u8868\u8fbe\u80fd\u529b\u3002", "method": "1. \u5934\u7279\u5f02\u6027\u6b27\u62c9\u53d8\u6362\uff1a\u5c06\u67e5\u8be2/\u952e\u6295\u5f71\u5230\u6781\u5750\u6807\u590d\u6570\u7a7a\u95f4\n2. \u81ea\u9002\u5e94\u5dee\u5206\u65cb\u8f6c\u673a\u5236\uff1aexp[i(Adapt(AS_mn,i)+\u0394(P_mn,i))]\uff0c\u5141\u8bb8\u5404\u5934\u72ec\u7acb\u5b66\u4e60\u8bed\u4e49\u89d2\u5ea6\u5dee(AS)\u4e0e\u4f4d\u7f6e\u7f16\u7801(\u0394)\u7684\u878d\u5408\u7b56\u7565", "result": "\u5728\u8bed\u8a00\u5efa\u6a21/\u6587\u672c\u751f\u6210/\u4ee3\u7801\u751f\u6210/\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cComplexFormer\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u751f\u6210\u56f0\u60d1\u5ea6\u3001\u66f4\u5f3a\u7684\u957f\u4e0a\u4e0b\u6587\u8fde\u8d2f\u6027\uff0c\u4e14\u53c2\u6570\u6548\u7387\u4f18\u4e8eRoPE-Transformer\u7b49\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "ComplexFormer\u901a\u8fc7\u590d\u6570\u7a7a\u95f4\u7edf\u4e00\u5efa\u6a21\u8bed\u4e49\u4e0e\u4f4d\u7f6e\u5173\u7cfb\uff0c\u4e3a\u6ce8\u610f\u529b\u673a\u5236\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u53c2\u6570\u5316\u65b9\u5f0f\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u8868\u73b0\u529b\u3002"}}
{"id": "2505.10496", "pdf": "https://arxiv.org/pdf/2505.10496", "abs": "https://arxiv.org/abs/2505.10496", "authors": ["Raman Dutt", "Pedro Sanchez", "Yongchen Yao", "Steven McDonagh", "Sotirios A. Tsaftaris", "Timothy Hospedales"], "title": "CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of Synthetic Chest Radiographs", "categories": ["cs.CV"], "comment": null, "summary": "We introduce CheXGenBench, a rigorous and multifaceted evaluation framework\nfor synthetic chest radiograph generation that simultaneously assesses\nfidelity, privacy risks, and clinical utility across state-of-the-art\ntext-to-image generative models. Despite rapid advancements in generative AI\nfor real-world imagery, medical domain evaluations have been hindered by\nmethodological inconsistencies, outdated architectural comparisons, and\ndisconnected assessment criteria that rarely address the practical clinical\nvalue of synthetic samples. CheXGenBench overcomes these limitations through\nstandardised data partitioning and a unified evaluation protocol comprising\nover 20 quantitative metrics that systematically analyse generation quality,\npotential privacy vulnerabilities, and downstream clinical applicability across\n11 leading text-to-image architectures. Our results reveal critical\ninefficiencies in the existing evaluation protocols, particularly in assessing\ngenerative fidelity, leading to inconsistent and uninformative comparisons. Our\nframework establishes a standardised benchmark for the medical AI community,\nenabling objective and reproducible comparisons while facilitating seamless\nintegration of both existing and future generative models. Additionally, we\nrelease a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K\nradiographs generated by the top-performing model (Sana 0.6B) in our benchmark\nto support further research in this critical domain. Through CheXGenBench, we\nestablish a new state-of-the-art and release our framework, models, and\nSynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/", "AI": {"tldr": "\u63d0\u51faCheXGenBench\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u6d41\u7a0b\u5168\u9762\u8bc4\u4f30\u80f8\u90e8X\u5149\u751f\u6210\u6a21\u578b\u7684\u771f\u5b9e\u6027\u3001\u9690\u79c1\u98ce\u9669\u548c\u4e34\u5e8a\u4ef7\u503c\uff0c\u5e76\u53d1\u5e03SynthCheX-75K\u6570\u636e\u96c6", "motivation": "\u73b0\u6709\u533b\u5b66\u56fe\u50cf\u751f\u6210\u8bc4\u4f30\u5b58\u5728\u65b9\u6cd5\u4e0d\u4e00\u81f4\u3001\u67b6\u6784\u5bf9\u6bd4\u8fc7\u65f6\u3001\u4e34\u5e8a\u4ef7\u503c\u8bc4\u4f30\u7f3a\u5931\u7b49\u95ee\u9898\uff0c\u9700\u5efa\u7acb\u7edf\u4e00\u6807\u51c6\u63a8\u52a8\u533b\u5b66AI\u53d1\u5c55", "method": "\u91c7\u7528\u6807\u51c6\u5316\u6570\u636e\u5212\u5206\u548c20+\u91cf\u5316\u6307\u6807\u8bc4\u4f30\u534f\u8bae\uff0c\u7cfb\u7edf\u6027\u5206\u679011\u79cd\u9886\u5148\u6587\u672c-\u56fe\u50cf\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\u3001\u9690\u79c1\u6f0f\u6d1e\u53ca\u4e34\u5e8a\u9002\u7528\u6027", "result": "\u53d1\u73b0\u73b0\u6709\u8bc4\u4f30\u534f\u8bae\u5728\u771f\u5b9e\u6027\u8bc4\u4f30\u65b9\u9762\u5b58\u5728\u7f3a\u9677\uff0c\u63d0\u51fa\u65b0\u57fa\u51c6\u786e\u7acbSana 0.6B\u4e3a\u6700\u4f18\u6a21\u578b\uff0c\u751f\u621075K\u9ad8\u8d28\u91cf\u5408\u6210\u5f71\u50cf\u6570\u636e\u96c6", "conclusion": "CheXGenBench\u4e3a\u533b\u5b66AI\u793e\u533a\u5efa\u7acb\u6807\u51c6\u5316\u8bc4\u4f30\u57fa\u51c6\uff0c\u4fc3\u8fdb\u5ba2\u89c2\u6a21\u578b\u6bd4\u8f83\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u6846\u67b6\u548c\u6570\u636e\u96c6\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55"}}
{"id": "2505.10497", "pdf": "https://arxiv.org/pdf/2505.10497", "abs": "https://arxiv.org/abs/2505.10497", "authors": ["Iurii Medvedev", "Nuno Goncalves"], "title": "MorphGuard: Morph Specific Margin Loss for Enhancing Robustness to Face Morphing Attacks", "categories": ["cs.CV"], "comment": null, "summary": "Face recognition has evolved significantly with the advancement of deep\nlearning techniques, enabling its widespread adoption in various applications\nrequiring secure authentication. However, this progress has also increased its\nexposure to presentation attacks, including face morphing, which poses a\nserious security threat by allowing one identity to impersonate another.\nTherefore, modern face recognition systems must be robust against such attacks.\n  In this work, we propose a novel approach for training deep networks for face\nrecognition with enhanced robustness to face morphing attacks. Our method\nmodifies the classification task by introducing a dual-branch classification\nstrategy that effectively handles the ambiguity in the labeling of face morphs.\nThis adaptation allows the model to incorporate morph images into the training\nprocess, improving its ability to distinguish them from bona fide samples.\n  Our strategy has been validated on public benchmarks, demonstrating its\neffectiveness in enhancing robustness against face morphing attacks.\nFurthermore, our approach is universally applicable and can be integrated into\nexisting face recognition training pipelines to improve classification-based\nrecognition methods.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u5206\u652f\u5206\u7c7b\u7b56\u7565\u589e\u5f3a\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u5bf9\u878d\u5408\u653b\u51fb\u7684\u9632\u5fa1\u80fd\u529b\uff0c\u901a\u8fc7\u6539\u8fdb\u6807\u7b7e\u5904\u7406\u673a\u5236\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "motivation": "\u9762\u90e8\u878d\u5408\u653b\u51fb\u5141\u8bb8\u4e0d\u540c\u8eab\u4efd\u76f8\u4e92\u5192\u5145\uff0c\u4e25\u91cd\u5a01\u80c1\u57fa\u4e8e\u4eba\u8138\u8bc6\u522b\u7684\u8eab\u4efd\u8ba4\u8bc1\u7cfb\u7edf\u5b89\u5168\u6027\uff0c\u9700\u63d0\u5347\u6a21\u578b\u5bf9\u6b64\u7c7b\u653b\u51fb\u7684\u9632\u5fa1\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u53cc\u5206\u652f\u5206\u7c7b\u67b6\u6784\u5904\u7406\u878d\u5408\u56fe\u50cf\u7684\u6807\u7b7e\u6a21\u7cca\u95ee\u9898\uff0c\u5c06\u653b\u51fb\u6837\u672c\u6574\u5408\u81f3\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5f3a\u5316\u6a21\u578b\u5bf9\u771f\u5b9e\u6837\u672c\u4e0e\u653b\u51fb\u6837\u672c\u7684\u533a\u5206\u80fd\u529b\u3002", "result": "\u5728\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u6297\u878d\u5408\u653b\u51fb\u6027\u80fd\uff0c\u4e14\u8be5\u65b9\u6cd5\u5177\u5907\u901a\u7528\u6027\u53ef\u65e0\u7f1d\u96c6\u6210\u73b0\u6709\u8bad\u7ec3\u6846\u67b6\u3002", "conclusion": "\u8be5\u8bad\u7ec3\u7b56\u7565\u4e3a\u63d0\u5347\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c\u4e0e\u884c\u4e1a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.10533", "pdf": "https://arxiv.org/pdf/2505.10533", "abs": "https://arxiv.org/abs/2505.10533", "authors": ["Aaryan Sharma", "Shivansh Gupta", "Samar Agarwal", "Vishak Prasad C.", "Ganesh Ramakrishnan"], "title": "Enhancing Multi-Image Question Answering via Submodular Subset Selection", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Large multimodal models (LMMs) have achieved high performance in\nvision-language tasks involving single image but they struggle when presented\nwith a collection of multiple images (Multiple Image Question Answering\nscenario). These tasks, which involve reasoning over large number of images,\npresent issues in scalability (with increasing number of images) and retrieval\nperformance. In this work, we propose an enhancement for retriever framework\nintroduced in MIRAGE model using submodular subset selection techniques. Our\nmethod leverages query-aware submodular functions, such as GraphCut, to\npre-select a subset of semantically relevant images before main retrieval\ncomponent. We demonstrate that using anchor-based queries and augmenting the\ndata improves submodular-retriever pipeline effectiveness, particularly in\nlarge haystack sizes.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5b50\u6a21\u51fd\u6570\u9009\u62e9\u7684\u591a\u56fe\u50cf\u68c0\u7d22\u589e\u5f3a\u6846\u67b6\uff0c\u63d0\u5347\u5927\u89c4\u6a21\u591a\u56fe\u50cf\u95ee\u7b54\u4efb\u52a1\u7684\u6027\u80fd", "motivation": "\u73b0\u6709\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u5904\u7406\u591a\u56fe\u50cf\u95ee\u7b54\u4efb\u52a1\u65f6\u5b58\u5728\u6269\u5c55\u6027\u4e0d\u8db3\u548c\u68c0\u7d22\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u9700\u6539\u8fdb\u68c0\u7d22\u6548\u7387", "method": "\u4f7f\u7528GraphCut\u7b49\u67e5\u8be2\u611f\u77e5\u7684\u5b50\u6a21\u51fd\u6570\u9884\u9009\u8bed\u4e49\u76f8\u5173\u56fe\u50cf\uff0c\u7ed3\u5408\u951a\u70b9\u67e5\u8be2\u673a\u5236\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\u4f18\u5316\u68c0\u7d22\u6d41\u7a0b", "result": "\u5728\u5927\u578b\u56fe\u50cf\u96c6\u5408\uff08haystack\uff09\u4e2d\u663e\u8457\u63d0\u5347\u68c0\u7d22\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u6d77\u91cf\u56fe\u50cf\u573a\u666f\u4e0b\u4fdd\u6301\u6027\u80fd\u7a33\u5b9a", "conclusion": "\u5b50\u6a21\u9009\u62e9\u4e0e\u6570\u636e\u589e\u5f3a\u7684\u7ed3\u5408\u6709\u6548\u89e3\u51b3\u4e86\u591a\u56fe\u50cf\u95ee\u7b54\u573a\u666f\u7684\u6269\u5c55\u74f6\u9888\uff0c\u4e3a\u590d\u6742\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2505.10465", "pdf": "https://arxiv.org/pdf/2505.10465", "abs": "https://arxiv.org/abs/2505.10465", "authors": ["Yizhou liu", "Ziming Liu", "Jeff Gore"], "title": "Superposition Yields Robust Neural Scaling", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "30 pages, 23 figures", "summary": "The success of today's large language models (LLMs) depends on the\nobservation that larger models perform better. However, the origin of this\nneural scaling law -- the finding that loss decreases as a power law with model\nsize -- remains unclear. Starting from two empirical principles -- that LLMs\nrepresent more things than the model dimensions (widths) they have (i.e.,\nrepresentations are superposed), and that words or concepts in language occur\nwith varying frequencies -- we constructed a toy model to study the loss\nscaling with model size. We found that when superposition is weak, meaning only\nthe most frequent features are represented without interference, the scaling of\nloss with model size depends on the underlying feature frequency; if feature\nfrequencies follow a power law, so does the loss. In contrast, under strong\nsuperposition, where all features are represented but overlap with each other,\nthe loss becomes inversely proportional to the model dimension across a wide\nrange of feature frequency distributions. This robust scaling behavior is\nexplained geometrically: when many more vectors are packed into a lower\ndimensional space, the interference (squared overlaps) between vectors scales\ninversely with that dimension. We then analyzed four families of open-sourced\nLLMs and found that they exhibit strong superposition and quantitatively match\nthe predictions of our toy model. The Chinchilla scaling law turned out to also\nagree with our results. We conclude that representation superposition is an\nimportant mechanism underlying the observed neural scaling laws. We anticipate\nthat these insights will inspire new training strategies and model\narchitectures to achieve better performance with less computation and fewer\nparameters.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u8868\u793a\u53e0\u52a0\u662f\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u7684\u5173\u952e\u673a\u5236\uff0c\u6a21\u578b\u5c3a\u5bf8\u4e0e\u635f\u5931\u7684\u5173\u7cfb\u53d7\u7279\u5f81\u9891\u7387\u5206\u5e03\u548c\u53e0\u52a0\u5f3a\u5ea6\u5171\u540c\u5f71\u54cd", "motivation": "\u73b0\u6709\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\uff08\u6a21\u578b\u6027\u80fd\u968f\u5c3a\u5bf8\u589e\u5927\u5448\u5e42\u5f8b\u63d0\u5347\uff09\u7684\u7269\u7406\u673a\u5236\u5c1a\u672a\u660e\u786e\uff0c\u9700\u63a2\u7a76\u8868\u793a\u53e0\u52a0\u548c\u7279\u5f81\u9891\u7387\u5206\u5e03\u7684\u76f8\u4e92\u4f5c\u7528\u673a\u5236", "method": "\u6784\u5efa\u57fa\u4e8e\u7279\u5f81\u53e0\u52a0\u539f\u7406\u7684\u73a9\u5177\u6a21\u578b\uff0c\u5206\u6790\u5f3a\u5f31\u4e24\u79cd\u53e0\u52a0\u6a21\u5f0f\u4e0b\u635f\u5931\u968f\u6a21\u578b\u5c3a\u5bf8\u7684\u7f29\u653e\u89c4\u5f8b\uff0c\u901a\u8fc7\u51e0\u4f55\u89e3\u91ca\u548c\u56db\u7c7b\u5f00\u6e90LLM\u7684\u5b9e\u8bc1\u9a8c\u8bc1", "result": "\u5f31\u53e0\u52a0\u65f6\u635f\u5931\u7f29\u653e\u4f9d\u8d56\u7279\u5f81\u9891\u7387\u5206\u5e03\uff08\u7b26\u5408\u5e42\u5f8b\uff09\uff0c\u5f3a\u53e0\u52a0\u65f6\u635f\u5931\u4e0e\u6a21\u578b\u7ef4\u5ea6\u6210\u53cd\u6bd4\uff1b\u5b9e\u8bc1\u663e\u793a\u4e3b\u6d41LLM\u7b26\u5408\u5f3a\u53e0\u52a0\u6a21\u5f0f\u7684\u9884\u6d4b", "conclusion": "\u8868\u793a\u53e0\u52a0\u673a\u5236\u89e3\u91ca\u4e86\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u7684\u7269\u7406\u672c\u8d28\uff0c\u8be5\u53d1\u73b0\u4e3a\u5f00\u53d1\u66f4\u9ad8\u6548\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e"}}
{"id": "2505.10541", "pdf": "https://arxiv.org/pdf/2505.10541", "abs": "https://arxiv.org/abs/2505.10541", "authors": ["Pengfei Wang", "Guohai Xu", "Weinong Wang", "Junjie Yang", "Jie Lou", "Yunhua Xue"], "title": "Exploring Implicit Visual Misunderstandings in Multimodal Large Language Models through Attention Analysis", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements have enhanced the capability of Multimodal Large Language\nModels (MLLMs) to comprehend multi-image information. However, existing\nbenchmarks primarily evaluate answer correctness, overlooking whether models\ngenuinely comprehend the visual input. To address this, we define implicit\nvisual misunderstanding (IVM), where MLLMs provide correct answers without\nfully comprehending the visual input. Through our analysis, we decouple the\nvisual and textual modalities within the causal attention module, revealing\nthat attention distribution increasingly converges on the image associated with\nthe correct answer as the network layers deepen. This insight leads to the\nintroduction of a scale-agnostic metric, \\textit{attention accuracy}, and a\nnovel benchmark for quantifying IVMs. Attention accuracy directly evaluates the\nmodel's visual understanding via internal mechanisms, remaining robust to\npositional biases for more reliable assessments. Furthermore, we extend our\napproach to finer granularities and demonstrate its effectiveness in unimodal\nscenarios, underscoring its versatility and generalizability.", "AI": {"tldr": "\u63d0\u51fa\u9690\u5f0f\u89c6\u89c9\u8bef\u89e3\uff08IVM\uff09\u6982\u5ff5\uff0c\u901a\u8fc7\u89e3\u8026\u6ce8\u610f\u529b\u6a21\u5757\u53d1\u73b0\u6ce8\u610f\u529b\u5206\u5e03\u89c4\u5f8b\uff0c\u6784\u5efa\u6ce8\u610f\u529b\u51c6\u786e\u7387\u6307\u6807\u548c\u65b0\u57fa\u51c6\u91cf\u5316IVM\uff0c\u9a8c\u8bc1\u65b9\u6cd5\u5728\u5355\u6a21\u6001\u573a\u666f\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u57fa\u51c6\u4ec5\u5173\u6ce8\u7b54\u6848\u6b63\u786e\u6027\uff0c\u65e0\u6cd5\u68c0\u6d4b\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u89c6\u89c9\u8f93\u5165\uff08IVM\u95ee\u9898\uff09\uff0c\u9700\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "1. \u89e3\u8026\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u7684\u56e0\u679c\u6ce8\u610f\u529b\u6a21\u5757\n2. \u5206\u6790\u6ce8\u610f\u529b\u5206\u5e03\u4e0e\u7b54\u6848\u5173\u8054\u6027\n3. \u63d0\u51fa\u6ce8\u610f\u529b\u51c6\u786e\u7387\u6307\u6807\u53caIVM\u91cf\u5316\u57fa\u51c6", "result": "\u6ce8\u610f\u529b\u51c6\u786e\u7387\u53ef\u6709\u6548\u8bc4\u4f30\u89c6\u89c9\u7406\u89e3\u80fd\u529b\uff0c\u5bf9\u4f4d\u7f6e\u504f\u5dee\u5177\u6709\u9c81\u68d2\u6027\uff1b\u65b9\u6cd5\u53ef\u6269\u5c55\u81f3\u7ec6\u7c92\u5ea6\u5206\u6790\u548c\u5355\u6a21\u6001\u573a\u666f\u9a8c\u8bc1\u3002", "conclusion": "\u901a\u8fc7\u6a21\u578b\u5185\u90e8\u673a\u5236\u76f4\u63a5\u8bc4\u4f30\u89c6\u89c9\u7406\u89e3\u80fd\u529b\uff0c\u4e3aMLLMs\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u4fc3\u8fdb\u6a21\u578b\u4f18\u5316\u65b9\u5411\u3002"}}
{"id": "2505.10475", "pdf": "https://arxiv.org/pdf/2505.10475", "abs": "https://arxiv.org/abs/2505.10475", "authors": ["Mouxiang Chen", "Binyuan Hui", "Zeyu Cui", "Jiaxi Yang", "Dayiheng Liu", "Jianling Sun", "Junyang Lin", "Zhongxin Liu"], "title": "Parallel Scaling Law for Language Models", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "It is commonly believed that scaling language models should commit a\nsignificant space or time cost, by increasing the parameters (parameter\nscaling) or output tokens (inference-time scaling). We introduce the third and\nmore inference-efficient scaling paradigm: increasing the model's parallel\ncomputation during both training and inference time. We apply $P$ diverse and\nlearnable transformations to the input, execute forward passes of the model in\nparallel, and dynamically aggregate the $P$ outputs. This method, namely\nparallel scaling (ParScale), scales parallel computation by reusing existing\nparameters and can be applied to any model structure, optimization procedure,\ndata, or task. We theoretically propose a new scaling law and validate it\nthrough large-scale pre-training, which shows that a model with $P$ parallel\nstreams is similar to scaling the parameters by $O(\\log P)$ while showing\nsuperior inference efficiency. For example, ParScale can use up to 22$\\times$\nless memory increase and 6$\\times$ less latency increase compared to parameter\nscaling that achieves the same performance improvement. It can also recycle an\noff-the-shelf pre-trained model into a parallelly scaled one by post-training\non a small amount of tokens, further reducing the training budget. The new\nscaling law we discovered potentially facilitates the deployment of more\npowerful models in low-resource scenarios, and provides an alternative\nperspective for the role of computation in machine learning.", "AI": {"tldr": "\u63d0\u51fa\u5e76\u884c\u6269\u5c55(ParScale)\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u52a0\u5e76\u884c\u8ba1\u7b97\u800c\u975e\u53c2\u6570\u89c4\u6a21\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\uff0c\u6bd4\u4f20\u7edf\u53c2\u6570\u6269\u5c55\u8282\u770122\u500d\u5185\u5b58\u548c6\u500d\u5ef6\u8fdf", "motivation": "\u4f20\u7edf\u8bed\u8a00\u6a21\u578b\u6269\u5c55\u65b9\u6cd5\uff08\u53c2\u6570\u6269\u5c55/\u63a8\u7406\u65f6\u6269\u5c55\uff09\u5b58\u5728\u663e\u8457\u8d44\u6e90\u6d88\u8017\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002ParScale\u65e8\u5728\u901a\u8fc7\u5e76\u884c\u8ba1\u7b97\u590d\u7528\u53c2\u6570\uff0c\u7a81\u7834\u6269\u5c55\u6548\u7387\u74f6\u9888", "method": "\u5bf9\u8f93\u5165\u65bd\u52a0P\u79cd\u53ef\u5b66\u4e60\u53d8\u6362\u2192\u5e76\u884c\u6267\u884c\u6a21\u578b\u524d\u5411\u4f20\u64ad\u2192\u52a8\u6001\u805a\u5408\u8f93\u51fa\u3002\u652f\u6301\u4efb\u610f\u6a21\u578b\u7ed3\u6784\uff0c\u53ef\u901a\u8fc7\u540e\u8bad\u7ec3\u5feb\u901f\u9002\u914d\u5df2\u6709\u9884\u8bad\u7ec3\u6a21\u578b", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u5e76\u884c\u6269\u5c55\u5b9a\u5f8b\uff1aP\u8def\u5e76\u884c\u7b49\u6548O(logP)\u53c2\u6570\u6269\u5c55\u6548\u679c\uff1b\u5185\u5b58\u589e\u52a0\u51cf\u5c1122\u500d\uff0c\u5ef6\u8fdf\u589e\u52a0\u964d\u4f4e6\u500d\uff1b\u540e\u8bad\u7ec3\u4ec5\u9700\u5c11\u91cftoken\u5373\u53ef\u6539\u9020\u73b0\u6709\u6a21\u578b", "conclusion": "\u4e3a\u4f4e\u8d44\u6e90\u573a\u666f\u90e8\u7f72\u5f3a\u5927\u6a21\u578b\u63d0\u4f9b\u65b0\u8def\u5f84\uff0c\u91cd\u65b0\u5b9a\u4e49\u4e86\u8ba1\u7b97\u6548\u7387\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u4ef7\u503c\uff0c\u5f00\u8f9f\u4e86\u6a21\u578b\u6269\u5c55\u7684\u7b2c\u4e09\u8303\u5f0f"}}
{"id": "2505.10551", "pdf": "https://arxiv.org/pdf/2505.10551", "abs": "https://arxiv.org/abs/2505.10551", "authors": ["Yiwen Liu", "Jessica Bader", "Jae Myung Kim"], "title": "Does Feasibility Matter? Understanding the Impact of Feasibility on Synthetic Training Data", "categories": ["cs.CV", "cs.AI"], "comment": "CVPRW 2025", "summary": "With the development of photorealistic diffusion models, models trained in\npart or fully on synthetic data achieve progressively better results. However,\ndiffusion models still routinely generate images that would not exist in\nreality, such as a dog floating above the ground or with unrealistic texture\nartifacts. We define the concept of feasibility as whether attributes in a\nsynthetic image could realistically exist in the real-world domain; synthetic\nimages containing attributes that violate this criterion are considered\ninfeasible. Intuitively, infeasible images are typically considered\nout-of-distribution; thus, training on such images is expected to hinder a\nmodel's ability to generalize to real-world data, and they should therefore be\nexcluded from the training set whenever possible. However, does feasibility\nreally matter? In this paper, we investigate whether enforcing feasibility is\nnecessary when generating synthetic training data for CLIP-based classifiers,\nfocusing on three target attributes: background, color, and texture. We\nintroduce VariReal, a pipeline that minimally edits a given source image to\ninclude feasible or infeasible attributes given by the textual prompt generated\nby a large language model. Our experiments show that feasibility minimally\naffects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference\nin top-1 accuracy across three fine-grained datasets. Also, the attribute\nmatters on whether the feasible/infeasible images adversarially influence the\nclassification performance. Finally, mixing feasible and infeasible images in\ntraining datasets does not significantly impact performance compared to using\npurely feasible or infeasible datasets.", "AI": {"tldr": "\u7814\u7a76\u9a8c\u8bc1\u5408\u6210\u56fe\u50cf\u7684\u53ef\u884c\u6027\u5bf9CLIP\u5206\u7c7b\u5668\u6027\u80fd\u5f71\u54cd\u6709\u9650\uff0c\u6df7\u5408\u53ef\u884c/\u4e0d\u53ef\u884c\u6570\u636e\u5bf9\u8bad\u7ec3\u5f71\u54cd\u4e0d\u663e\u8457", "motivation": "\u9488\u5bf9\u6269\u6563\u6a21\u578b\u751f\u6210\u4e0d\u73b0\u5b9e\u56fe\u50cf\u53ef\u80fd\u5f71\u54cd\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u5408\u6210\u8bad\u7ec3\u6570\u636e\u4e2d\u53ef\u884c\u6027\u7ea6\u675f\u7684\u5fc5\u8981\u6027", "method": "\u63d0\u51faVariReal\u6d41\u7a0b\uff0c\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6587\u672c\u63d0\u793a\uff0c\u5bf9\u6e90\u56fe\u50cf\u8fdb\u884c\u6700\u5c0f\u5316\u7f16\u8f91\u4ee5\u63a7\u5236\u5c5e\u6027\u53ef\u884c\u6027", "result": "\u53ef\u884c\u6027\u5dee\u5f02\u5bfc\u81f4LoRA\u5fae\u8c03\u7684CLIP\u5206\u7c7b\u5668top-1\u51c6\u786e\u7387\u53d8\u5316\u666e\u904d\u4f4e\u4e8e0.3%\uff0c\u4e0d\u540c\u5c5e\u6027\u5bf9\u5206\u7c7b\u6027\u80fd\u5f71\u54cd\u5b58\u5728\u5dee\u5f02", "conclusion": "\u53ef\u884c\u6027\u7ea6\u675f\u5728\u7279\u5b9a\u5c5e\u6027\uff08\u5982\u80cc\u666f/\u989c\u8272/\u7eb9\u7406\uff09\u8bad\u7ec3\u4e2d\u5177\u6709\u9009\u62e9\u6027\u4ef7\u503c\uff0c\u4f46\u6574\u4f53\u5fc5\u8981\u6027\u4f4e\u4e8e\u9884\u671f"}}
{"id": "2505.10495", "pdf": "https://arxiv.org/pdf/2505.10495", "abs": "https://arxiv.org/abs/2505.10495", "authors": ["Vibha Belavadi", "Tushar Vatsa", "Dewang Sultania", "Suhas Suresha", "Ishita Verma", "Cheng Chen", "Tracy Holloway King", "Michael Friedrich"], "title": "RouteNator: A Router-Based Multi-Modal Architecture for Generating Synthetic Training Data for Function Calling LLMs", "categories": ["cs.LG", "cs.CL"], "comment": "Proceedings of the 4th International Workshop on Knowledge-Augmented\n  Methods for Natural Language Processing", "summary": "This paper addresses fine-tuning Large Language Models (LLMs) for function\ncalling tasks when real user interaction data is unavailable. In digital\ncontent creation tools, where users express their needs through natural\nlanguage queries that must be mapped to API calls, the lack of real-world\ntask-specific data and privacy constraints for training on it necessitate\nsynthetic data generation. Existing approaches to synthetic data generation\nfall short in diversity and complexity, failing to replicate real-world data\ndistributions and leading to suboptimal performance after LLM fine-tuning. We\npresent a novel router-based architecture that leverages domain resources like\ncontent metadata and structured knowledge graphs, along with text-to-text and\nvision-to-text language models to generate high-quality synthetic training\ndata. Our architecture's flexible routing mechanism enables synthetic data\ngeneration that matches observed real-world distributions, addressing a\nfundamental limitation of traditional approaches. Evaluation on a comprehensive\nset of real user queries demonstrates significant improvements in both function\nclassification accuracy and API parameter selection. Models fine-tuned with our\nsynthetic data consistently outperform traditional approaches, establishing new\nbenchmarks for function calling tasks.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8def\u7531\u5668\u7684\u67b6\u6784\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u4f18\u5316LLM\u51fd\u6570\u8c03\u7528\u4efb\u52a1\u8868\u73b0", "motivation": "\u771f\u5b9e\u7528\u6237\u6570\u636e\u7a00\u7f3a\u4e14\u9690\u79c1\u53d7\u9650\uff0c\u4f20\u7edf\u5408\u6210\u65b9\u6cd5\u96be\u4ee5\u5339\u914d\u771f\u5b9e\u6570\u636e\u5206\u5e03", "method": "\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u548c\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u8def\u7531\u673a\u5236\u63a7\u5236\u6570\u636e\u751f\u6210\u5206\u5e03", "result": "\u5728\u771f\u5b9e\u7528\u6237\u6d4b\u8bd5\u4e2d\u51fd\u6570\u5206\u7c7b\u51c6\u786e\u7387\u63d0\u5347\uff0cAPI\u53c2\u6570\u9009\u62e9\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5", "conclusion": "\u8be5\u67b6\u6784\u5efa\u7acb\u4e86\u51fd\u6570\u8c03\u7528\u4efb\u52a1\u65b0\u57fa\u51c6\uff0c\u9a8c\u8bc1\u4e86\u5408\u6210\u6570\u636e\u751f\u6210\u7684\u6709\u6548\u6027"}}
{"id": "2505.10557", "pdf": "https://arxiv.org/pdf/2505.10557", "abs": "https://arxiv.org/abs/2505.10557", "authors": ["Ke Wang", "Junting Pan", "Linda Wei", "Aojun Zhou", "Weikang Shi", "Zimu Lu", "Han Xiao", "Yunqiao Yang", "Houxing Ren", "Mingjie Zhan", "Hongsheng Li"], "title": "MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025 Findings", "summary": "Natural language image-caption datasets, widely used for training Large\nMultimodal Models, mainly focus on natural scenarios and overlook the intricate\ndetails of mathematical figures that are critical for problem-solving,\nhindering the advancement of current LMMs in multimodal mathematical reasoning.\nTo this end, we propose leveraging code as supervision for cross-modal\nalignment, since code inherently encodes all information needed to generate\ncorresponding figures, establishing a precise connection between the two\nmodalities. Specifically, we co-develop our image-to-code model and dataset\nwith model-in-the-loop approach, resulting in an image-to-code model,\nFigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date.\nFurthermore, we utilize FigCodifier to synthesize novel mathematical figures\nand then construct MM-MathInstruct-3M, a high-quality multimodal math\ninstruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with\nImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on\nMM-MathInstruct-3M for multimodal math problem solving. Our model achieves a\nnew open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and\nClaude 3.5 Sonnet in the geometry problem-solving subset of MathVista,\nachieving improvements of 8.9% and 9.2%. The dataset and models will be\nreleased at https://github.com/mathllm/MathCoder.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u4ee3\u7801\u76d1\u7763\u5b9e\u73b0\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u63d0\u5347\u591a\u6a21\u6001\u6570\u5b66\u63a8\u7406\u6a21\u578b\u6027\u80fd\uff0c\u6a21\u578bMathCoder-VL\u5728\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u4e0a\u8d85\u8d8aGPT-4o\u548cClaude 3.5\u3002", "motivation": "\u73b0\u6709\u81ea\u7136\u8bed\u8a00\u56fe\u50cf\u6570\u636e\u96c6\u5ffd\u89c6\u6570\u5b66\u56fe\u8868\u7ec6\u8282\uff0c\u963b\u788d\u5927\u6a21\u578b\u5728\u591a\u6a21\u6001\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u53d1\u5c55\u3002\u4ee3\u7801\u80fd\u5b8c\u6574\u7f16\u7801\u56fe\u8868\u4fe1\u606f\uff0c\u53ef\u5efa\u7acb\u7cbe\u51c6\u7684\u8de8\u6a21\u6001\u8fde\u63a5\u3002", "method": "1. \u6a21\u578b\u95ed\u73af\u5f00\u53d1FigCodifier\u56fe\u50cf\u8f6c\u7801\u6a21\u578b\u548cImgCode-8.6M\u6570\u636e\u96c6\n2. \u6784\u5efaMM-MathInstruct-3M\u6570\u5b66\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u96c6\n3. \u4e24\u9636\u6bb5\u8bad\u7ec3MathCoder-VL\u6a21\u578b", "result": "\u5b9e\u73b0\u5f00\u6e90\u6a21\u578bSOTA\uff0cMathVista\u51e0\u4f55\u95ee\u9898\u51c6\u786e\u7387\u63d0\u53478.9-9.2%\uff0c\u8d85\u8d8a\u5546\u4e1a\u6a21\u578b\u3002", "conclusion": "\u4ee3\u7801\u76d1\u7763\u6709\u6548\u63d0\u5347\u591a\u6a21\u6001\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u5f00\u6e90\u6570\u636e\u96c6\u548c\u6a21\u578b\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2505.10526", "pdf": "https://arxiv.org/pdf/2505.10526", "abs": "https://arxiv.org/abs/2505.10526", "authors": ["Mugilan Ganesan", "Shane Segal", "Ankur Aggarwal", "Nish Sinnadurai", "Sean Lie", "Vithursan Thangarasa"], "title": "MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "Main paper: 11 pp., 4 figs., 3 tabs.; Supplementary: 2 pp", "summary": "Speculative decoding significantly accelerates language model inference by\nenabling a lightweight draft model to propose multiple tokens that a larger\ntarget model verifies simultaneously. However, applying this technique to\nvision-language models (VLMs) presents two fundamental challenges: small\nlanguage models that could serve as efficient drafters lack the architectural\ncomponents to process visual inputs, and their token predictions fail to match\nthose of VLM target models that consider visual context. We introduce\nMultimodal Adaptation and Self-Data Distillation for Speculative Decoding of\nVision-Language Models (MASSV), which transforms existing small language models\ninto effective multimodal drafters through a two-phase approach. MASSV first\nconnects the target VLM's vision encoder to the draft model via a lightweight\ntrainable projector, then applies self-distilled visual instruction tuning\nusing responses generated by the target VLM to align token predictions.\nComprehensive experiments across the Qwen2.5-VL and Gemma3 model families\ndemonstrate that MASSV increases accepted length by up to 30% and delivers\nend-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV\nprovides a scalable, architecture-compatible method for accelerating both\ncurrent and future VLMs.", "AI": {"tldr": "MASSV\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\u5c06\u5c0f\u8bed\u8a00\u6a21\u578b\u8f6c\u5316\u4e3a\u591a\u6a21\u6001\u8349\u6848\u6a21\u578b\uff0c\u52a0\u901f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u6d4b\u89e3\u7801\uff0c\u5b9e\u73b0\u6700\u9ad81.46\u500d\u63a8\u7406\u52a0\u901f", "motivation": "\u4f20\u7edf\u63a8\u6d4b\u89e3\u7801\u6280\u672f\u5e94\u7528\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u65f6\u9762\u4e34\u4e24\u4e2a\u6838\u5fc3\u6311\u6218\uff1a\u5c0f\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u89c6\u89c9\u5904\u7406\u80fd\u529b\uff0c\u4e14\u5176\u9884\u6d4b\u65e0\u6cd5\u5339\u914d\u8003\u8651\u89c6\u89c9\u4e0a\u4e0b\u6587\u7684VLM\u76ee\u6807\u6a21\u578b", "method": "1. \u901a\u8fc7\u53ef\u8bad\u7ec3\u6295\u5f71\u5668\u8fde\u63a5VLM\u89c6\u89c9\u7f16\u7801\u5668\u4e0e\u8349\u6848\u6a21\u578b\n2. \u4f7f\u7528\u76ee\u6807VLM\u751f\u6210\u54cd\u5e94\u8fdb\u884c\u81ea\u84b8\u998f\u89c6\u89c9\u6307\u4ee4\u8c03\u6574\uff0c\u5bf9\u9f50\u9884\u6d4b\u5206\u5e03", "result": "\u5728Qwen2.5-VL\u548cGemma3\u6a21\u578b\u4e0a\u5b9e\u73b0\u63a5\u53d7\u957f\u5ea6\u63d0\u534730%\uff0c\u7aef\u5230\u7aef\u63a8\u7406\u52a0\u901f\u6700\u9ad8\u8fbe1.46\u500d\uff08\u89c6\u89c9\u76f8\u5173\u4efb\u52a1\uff09", "conclusion": "MASSV\u4e3a\u52a0\u901f\u5f53\u524d\u53ca\u672a\u6765\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u67b6\u6784\u517c\u5bb9\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.10562", "pdf": "https://arxiv.org/pdf/2505.10562", "abs": "https://arxiv.org/abs/2505.10562", "authors": ["Wenxuan Wang", "Fan Zhang", "Yufeng Cui", "Haiwen Diao", "Zhuoyan Luo", "Huchuan Lu", "Jing Liu", "Xinlong Wang"], "title": "End-to-End Vision Tokenizer Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Existing vision tokenization isolates the optimization of vision tokenizers\nfrom downstream training, implicitly assuming the visual tokens can generalize\nwell across various tasks, e.g., image generation and visual question\nanswering. The vision tokenizer optimized for low-level reconstruction is\nagnostic to downstream tasks requiring varied representations and semantics.\nThis decoupled paradigm introduces a critical misalignment: The loss of the\nvision tokenization can be the representation bottleneck for target tasks. For\nexample, errors in tokenizing text in a given image lead to poor results when\nrecognizing or generating them. To address this, we propose ETT, an end-to-end\nvision tokenizer tuning approach that enables joint optimization between vision\ntokenization and target autoregressive tasks. Unlike prior autoregressive\nmodels that use only discrete indices from a frozen vision tokenizer, ETT\nleverages the visual embeddings of the tokenizer codebook, and optimizes the\nvision tokenizers end-to-end with both reconstruction and caption objectives.\nETT can be seamlessly integrated into existing training pipelines with minimal\narchitecture modifications. Our ETT is simple to implement and integrate,\nwithout the need to adjust the original codebooks or architectures of the\nemployed large language models. Extensive experiments demonstrate that our\nproposed end-to-end vision tokenizer tuning unlocks significant performance\ngains, i.e., 2-6% for multimodal understanding and visual generation tasks\ncompared to frozen tokenizer baselines, while preserving the original\nreconstruction capability. We hope this very simple and strong method can\nempower multimodal foundation models besides image generation and\nunderstanding.", "AI": {"tldr": "\u63d0\u51faETT\u7aef\u5230\u7aef\u89c6\u89c9\u6807\u8bb0\u5668\u8c03\u4f18\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u89c6\u89c9\u6807\u8bb0\u5316\u4e0e\u76ee\u6807\u4efb\u52a1\uff0c\u89e3\u51b3\u8868\u5f81\u74f6\u9888\u95ee\u9898\uff0c\u63d0\u5347\u591a\u6a21\u6001\u4efb\u52a1\u6027\u80fd", "motivation": "\u73b0\u6709\u89c6\u89c9\u6807\u8bb0\u5316\u65b9\u6cd5\u5c06\u4f18\u5316\u4e0e\u4e0b\u6e38\u4efb\u52a1\u89e3\u8026\uff0c\u4f4e\u5c42\u91cd\u5efa\u4f18\u5316\u7684\u6807\u8bb0\u5668\u65e0\u6cd5\u9002\u5e94\u9700\u8981\u4e0d\u540c\u8868\u5f81\u7684\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u56fe\u50cf\u6587\u672c\u8bc6\u522b\u9519\u8bef\u5bfc\u81f4\u751f\u6210\u5931\u8d25\uff09", "method": "\u5229\u7528\u6807\u8bb0\u5668\u4ee3\u7801\u672c\u7684\u89c6\u89c9\u5d4c\u5165\uff0c\u901a\u8fc7\u91cd\u5efa+\u63cf\u8ff0\u76ee\u6807\u8054\u5408\u4f18\u5316\u89c6\u89c9\u6807\u8bb0\u5668\uff0c\u65e0\u9700\u4fee\u6539\u539f\u6709\u67b6\u6784/\u4ee3\u7801\u672c", "result": "\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u6027\u80fd\u63d0\u53472-6%\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u6709\u91cd\u5efa\u80fd\u529b", "conclusion": "ETT\u7b80\u5355\u9ad8\u6548\uff0c\u53ef\u589e\u5f3a\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\uff0c\u62d3\u5c55\u5230\u56fe\u50cf\u751f\u6210/\u7406\u89e3\u4e4b\u5916\u7684\u573a\u666f"}}
{"id": "2505.10543", "pdf": "https://arxiv.org/pdf/2505.10543", "abs": "https://arxiv.org/abs/2505.10543", "authors": ["Annie Wong", "Thomas B\u00e4ck", "Aske Plaat", "Niki van Stein", "Anna V. Kononova"], "title": "Towards a Deeper Understanding of Reasoning Capabilities in Large Language Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "While large language models demonstrate impressive performance on static\nbenchmarks, the true potential of large language models as self-learning and\nreasoning agents in dynamic environments remains unclear. This study\nsystematically evaluates the efficacy of self-reflection, heuristic mutation,\nand planning as prompting techniques to test the adaptive capabilities of\nagents. We conduct experiments with various open-source language models in\ndynamic environments and find that larger models generally outperform smaller\nones, but that strategic prompting can close this performance gap. Second, a\ntoo-long prompt can negatively impact smaller models on basic reactive tasks,\nwhile larger models show more robust behaviour. Third, advanced prompting\ntechniques primarily benefit smaller models on complex games, but offer less\nimprovement for already high-performing large language models. Yet, we find\nthat advanced reasoning methods yield highly variable outcomes: while capable\nof significantly improving performance when reasoning and decision-making\nalign, they also introduce instability and can lead to big performance drops.\nCompared to human performance, our findings reveal little evidence of true\nemergent reasoning. Instead, large language model performance exhibits\npersistent limitations in crucial areas such as planning, reasoning, and\nspatial coordination, suggesting that current-generation large language models\nstill suffer fundamental shortcomings that may not be fully overcome through\nself-reflective prompting alone. Reasoning is a multi-faceted task, and while\nreasoning methods like Chain of thought improves multi-step reasoning on math\nword problems, our findings using dynamic benchmarks highlight important\nshortcomings in general reasoning capabilities, indicating a need to move\nbeyond static benchmarks to capture the complexity of reasoning.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u4ecd\u5b58\u5728\u6839\u672c\u6027\u7f3a\u9677\uff0c\u7b56\u7565\u6027\u63d0\u793a\u53ef\u7f29\u5c0f\u6a21\u578b\u95f4\u5dee\u8ddd\u4f46\u65e0\u6cd5\u5b8c\u5168\u514b\u670d\u6838\u5fc3\u63a8\u7406\u77ed\u677f\uff0c\u52a8\u6001\u57fa\u51c6\u6d4b\u8bd5\u5bf9\u8bc4\u4f30\u771f\u5b9e\u63a8\u7406\u80fd\u529b\u81f3\u5173\u91cd\u8981", "motivation": "\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u4f5c\u4e3a\u81ea\u5b66\u4e60\u667a\u80fd\u4f53\u7684\u771f\u5b9e\u6f5c\u529b\uff0c\u8bc4\u4f30\u63d0\u793a\u7b56\u7565\u5bf9\u6a21\u578b\u9002\u5e94\u80fd\u529b\u7684\u63d0\u5347\u6548\u679c", "method": "\u4f7f\u7528\u4e0d\u540c\u89c4\u6a21\u7684\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7cfb\u7edf\u6d4b\u8bd5\u81ea\u6211\u53cd\u601d\u3001\u542f\u53d1\u5f0f\u7a81\u53d8\u548c\u89c4\u5212\u7b49\u63d0\u793a\u6280\u672f\u7684\u6709\u6548\u6027", "result": "\u5927\u6a21\u578b\u8868\u73b0\u66f4\u7a33\u5065\u4f46\u63d0\u793a\u7b56\u7565\u53ef\u7f29\u5c0f\u5dee\u8ddd\uff1b\u957f\u63d0\u793a\u635f\u5bb3\u5c0f\u6a21\u578b\u57fa\u7840\u4efb\u52a1\u8868\u73b0\uff1b\u9ad8\u7ea7\u63d0\u793a\u4e3b\u8981\u63d0\u5347\u5c0f\u6a21\u578b\u590d\u6742\u4efb\u52a1\u8868\u73b0\u4f46\u5bfc\u81f4\u6027\u80fd\u6ce2\u52a8\uff1b\u73b0\u6709\u6a21\u578b\u5728\u89c4\u5212\u63a8\u7406\u548c\u7a7a\u95f4\u534f\u8c03\u65b9\u9762\u5b58\u5728\u660e\u663e\u7f3a\u9677", "conclusion": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5c1a\u672a\u5c55\u73b0\u771f\u6b63\u7684\u6d8c\u73b0\u63a8\u7406\u80fd\u529b\uff0c\u52a8\u6001\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u6838\u5fc3\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8d85\u8d8a\u9759\u6001\u6d4b\u8bd5\u6846\u67b6\u5f00\u53d1\u65b0\u65b9\u6cd5\u63d0\u5347\u901a\u7528\u63a8\u7406\u80fd\u529b"}}
{"id": "2505.10565", "pdf": "https://arxiv.org/pdf/2505.10565", "abs": "https://arxiv.org/abs/2505.10565", "authors": ["Zehan Wang", "Siyu Chen", "Lihe Yang", "Jialei Wang", "Ziang Zhang", "Hengshuang Zhao", "Zhou Zhao"], "title": "Depth Anything with Any Prior", "categories": ["cs.CV"], "comment": "Home page: https://prior-depth-anything.github.io/", "summary": "This work presents Prior Depth Anything, a framework that combines incomplete\nbut precise metric information in depth measurement with relative but complete\ngeometric structures in depth prediction, generating accurate, dense, and\ndetailed metric depth maps for any scene. To this end, we design a\ncoarse-to-fine pipeline to progressively integrate the two complementary depth\nsources. First, we introduce pixel-level metric alignment and distance-aware\nweighting to pre-fill diverse metric priors by explicitly using depth\nprediction. It effectively narrows the domain gap between prior patterns,\nenhancing generalization across varying scenarios. Second, we develop a\nconditioned monocular depth estimation (MDE) model to refine the inherent noise\nof depth priors. By conditioning on the normalized pre-filled prior and\nprediction, the model further implicitly merges the two complementary depth\nsources. Our model showcases impressive zero-shot generalization across depth\ncompletion, super-resolution, and inpainting over 7 real-world datasets,\nmatching or even surpassing previous task-specific methods. More importantly,\nit performs well on challenging, unseen mixed priors and enables test-time\nimprovements by switching prediction models, providing a flexible\naccuracy-efficiency trade-off while evolving with advancements in MDE models.", "AI": {"tldr": "\u63d0\u51faPrior Depth Anything\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u4e0d\u5b8c\u6574\u4f46\u7cbe\u786e\u7684\u6df1\u5ea6\u6d4b\u91cf\u4e0e\u76f8\u5bf9\u4f46\u5b8c\u6574\u7684\u51e0\u4f55\u9884\u6d4b\uff0c\u751f\u6210\u9ad8\u7cbe\u5ea6\u7a20\u5bc6\u6df1\u5ea6\u56fe", "motivation": "\u89e3\u51b3\u73b0\u6709\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u5728\u57df\u9002\u5e94\u3001\u566a\u58f0\u5904\u7406\u548c\u573a\u666f\u6cdb\u5316\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u901a\u8fc7\u4e92\u8865\u6df1\u5ea6\u6e90\u7684\u663e\u9690\u5f0f\u878d\u5408\u63d0\u5347\u6548\u679c", "method": "\u5206\u9636\u6bb5\u6d41\u7a0b\uff1a1\uff09\u50cf\u7d20\u7ea7\u5ea6\u91cf\u5bf9\u9f50\u4e0e\u8ddd\u79bb\u611f\u77e5\u52a0\u6743\u9884\u586b\u5145 2\uff09\u5e26\u6761\u4ef6\u7ea6\u675f\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\u7ec6\u5316\u566a\u58f0", "result": "\u57287\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u96f6\u6837\u672c\u6cdb\u5316\uff0c\u6df1\u5ea6\u8865\u5168/\u8d85\u5206\u8fa8\u7387/\u4fee\u590d\u4efb\u52a1\u6027\u80fd\u8d85\u8d8a\u4e13\u7528\u6a21\u578b\uff0c\u6df7\u5408\u5148\u9a8c\u5904\u7406\u8868\u73b0\u4f18\u5f02", "conclusion": "\u8be5\u6846\u67b6\u5177\u6709\u6d4b\u8bd5\u65f6\u6539\u8fdb\u80fd\u529b\uff0c\u53ef\u901a\u8fc7\u5207\u6362\u9884\u6d4b\u6a21\u578b\u5b9e\u73b0\u7075\u6d3b\u7684\u6548\u7387-\u7cbe\u5ea6\u6743\u8861\uff0c\u9002\u5e94MDE\u6a21\u578b\u7684\u6280\u672f\u6f14\u8fdb"}}
{"id": "2505.10566", "pdf": "https://arxiv.org/pdf/2505.10566", "abs": "https://arxiv.org/abs/2505.10566", "authors": ["Yen-Chi Cheng", "Krishna Kumar Singh", "Jae Shin Yoon", "Alex Schwing", "Liangyan Gui", "Matheus Gadelha", "Paul Guerrero", "Nanxuan Zhao"], "title": "3D-Fixup: Advancing Photo Editing with 3D Priors", "categories": ["cs.CV"], "comment": "SIGGRAPH 2025. Project page: https://3dfixup.github.io/", "summary": "Despite significant advances in modeling image priors via diffusion models,\n3D-aware image editing remains challenging, in part because the object is only\nspecified via a single image. To tackle this challenge, we propose 3D-Fixup, a\nnew framework for editing 2D images guided by learned 3D priors. The framework\nsupports difficult editing situations such as object translation and 3D\nrotation. To achieve this, we leverage a training-based approach that harnesses\nthe generative power of diffusion models. As video data naturally encodes\nreal-world physical dynamics, we turn to video data for generating training\ndata pairs, i.e., a source and a target frame. Rather than relying solely on a\nsingle trained model to infer transformations between source and target frames,\nwe incorporate 3D guidance from an Image-to-3D model, which bridges this\nchallenging task by explicitly projecting 2D information into 3D space. We\ndesign a data generation pipeline to ensure high-quality 3D guidance throughout\ntraining. Results show that by integrating these 3D priors, 3D-Fixup\neffectively supports complex, identity coherent 3D-aware edits, achieving\nhigh-quality results and advancing the application of diffusion models in\nrealistic image manipulation. The code is provided at\nhttps://3dfixup.github.io/", "AI": {"tldr": "\u63d0\u51fa3D-Fixup\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u6269\u6563\u6a21\u578b\u4e0e3D\u5148\u9a8c\u6307\u5bfc\uff0c\u5b9e\u73b0\u590d\u67423D\u611f\u77e5\u76842D\u56fe\u50cf\u7f16\u8f91", "motivation": "\u89e3\u51b3\u5355\u56fe\u50cf\u96be\u4ee5\u8fdb\u884c3D\u7f16\u8f91\uff08\u5982\u7269\u4f53\u5e73\u79fb/\u65cb\u8f6c\uff09\u7684\u6311\u6218\uff0c\u5229\u7528\u89c6\u9891\u52a8\u6001\u4fe1\u606f\u4e0e3D\u6295\u5f71\u7a81\u78342D\u7f16\u8f91\u5c40\u9650", "method": "1. \u57fa\u4e8e\u89c6\u9891\u751f\u6210\u8bad\u7ec3\u6570\u636e\u5bf9 2. \u6574\u5408Image-to-3D\u6a21\u578b\u63d0\u4f9b3D\u6307\u5bfc 3. \u8bbe\u8ba1\u6570\u636e\u751f\u6210\u6d41\u7a0b\u4fdd\u969c3D\u6307\u5bfc\u8d28\u91cf", "result": "\u5b9e\u9a8c\u8bc1\u660e\u6846\u67b6\u652f\u6301\u590d\u67423D\u7f16\u8f91\u4efb\u52a1\uff0c\u5728\u8eab\u4efd\u4e00\u81f4\u6027/\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90", "conclusion": "3D-Fixup\u901a\u8fc7\u663e\u5f0f3D\u6295\u5f71\u673a\u5236\uff0c\u63a8\u52a8\u4e86\u6269\u6563\u6a21\u578b\u5728\u771f\u5b9e\u56fe\u50cf\u7f16\u8f91\u4e2d\u7684\u5e94\u7528\u8fb9\u754c"}}
{"id": "2505.09630", "pdf": "https://arxiv.org/pdf/2505.09630", "abs": "https://arxiv.org/abs/2505.09630", "authors": ["Tien Comlekoglu", "J. Quetzalc\u00f3atl Toledo-Mar\u00edn", "Douglas W. DeSimone", "Shayn M. Peirce", "Geoffrey Fox", "James A. Glazier"], "title": "Generative diffusion model surrogates for mechanistic agent-based biological models", "categories": ["q-bio.QM", "cs.CV", "cs.ET", "cs.PF"], "comment": null, "summary": "Mechanistic, multicellular, agent-based models are commonly used to\ninvestigate tissue, organ, and organism-scale biology at single-cell\nresolution. The Cellular-Potts Model (CPM) is a powerful and popular framework\nfor developing and interrogating these models. CPMs become computationally\nexpensive at large space- and time- scales making application and investigation\nof developed models difficult. Surrogate models may allow for the accelerated\nevaluation of CPMs of complex biological systems. However, the stochastic\nnature of these models means each set of parameters may give rise to different\nmodel configurations, complicating surrogate model development. In this work,\nwe leverage denoising diffusion probabilistic models to train a generative AI\nsurrogate of a CPM used to investigate \\textit{in vitro} vasculogenesis. We\ndescribe the use of an image classifier to learn the characteristics that\ndefine unique areas of a 2-dimensional parameter space. We then apply this\nclassifier to aid in surrogate model selection and verification. Our CPM model\nsurrogate generates model configurations 20,000 timesteps ahead of a reference\nconfiguration and demonstrates approximately a 22x reduction in computational\ntime as compared to native code execution. Our work represents a step towards\nthe implementation of DDPMs to develop digital twins of stochastic biological\nsystems.", "AI": {"tldr": "\u4f7f\u7528\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\u6784\u5efaCPM\u7684\u751f\u6210\u5f0fAI\u66ff\u4ee3\u6a21\u578b\uff0c\u52a0\u901f\u8840\u7ba1\u751f\u6210\u6a21\u62df\uff0c\u8ba1\u7b97\u6548\u7387\u63d0\u534722\u500d", "motivation": "\u4f20\u7edfCPM\u6a21\u578b\u5728\u6a21\u62df\u5927\u65f6\u7a7a\u5c3a\u5ea6\u751f\u7269\u7cfb\u7edf\u65f6\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u66ff\u4ee3\u6a21\u578b", "method": "\u7ed3\u5408\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\uff08DDPM\uff09\u548c\u56fe\u50cf\u5206\u7c7b\u5668\uff0c\u5efa\u7acb\u8840\u7ba1\u751f\u6210CPM\u7684\u751f\u6210\u5f0fAI\u66ff\u4ee3\u6a21\u578b", "result": "\u66ff\u4ee3\u6a21\u578b\u53ef\u63d0\u524d\u751f\u621020000\u65f6\u95f4\u6b65\u7684\u914d\u7f6e\uff0c\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c11\u7ea622\u500d", "conclusion": "\u4e3a\u5f00\u53d1\u968f\u673a\u751f\u7269\u7cfb\u7edf\u6570\u5b57\u5b6a\u751f\u5960\u5b9a\u57fa\u7840\uff0c\u5c55\u793a\u751f\u6210\u5f0fAI\u66ff\u4ee3\u590d\u6742\u751f\u7269\u6a21\u578b\u7684\u5e94\u7528\u6f5c\u529b"}}
{"id": "2505.09819", "pdf": "https://arxiv.org/pdf/2505.09819", "abs": "https://arxiv.org/abs/2505.09819", "authors": ["Ruichen Yang", "Gy\u00f6rgy M. L\u00e9vay", "Christopher L. Hunt", "D\u00e1niel Czeiner", "Megan C. Hodgson", "Damini Agarwal", "Rahul R. Kaliki", "Nitish V. Thakor"], "title": "Visual Feedback of Pattern Separability Improves Myoelectric Decoding Performance of Upper Limb Prostheses", "categories": ["cs.HC", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "State-of-the-art upper limb myoelectric prostheses often use pattern\nrecognition (PR) control systems that translate electromyography (EMG) signals\ninto desired movements. As prosthesis movement complexity increases, users\noften struggle to produce sufficiently distinct EMG patterns for reliable\nclassification. Existing training typically involves heuristic, trial-and-error\nuser adjustments to static decoder boundaries. Goal: We introduce the Reviewer,\na 3D visual interface projecting EMG signals directly into the decoder's\nclassification space, providing intuitive, real-time insight into PR algorithm\nbehavior. This structured feedback reduces cognitive load and fosters mutual,\ndata-driven adaptation between user-generated EMG patterns and decoder\nboundaries. Methods: A 10-session study with 12 able-bodied participants\ncompared PR performance after motor-based training and updating using the\nReviewer versus conventional virtual arm visualization. Performance was\nassessed using a Fitts law task that involved the aperture of the cursor and\nthe control of orientation. Results: Participants trained with the Reviewer\nachieved higher completion rates, reduced overshoot, and improved path\nefficiency and throughput compared to the standard visualization group.\nSignificance: The Reviewer introduces decoder-informed motor training,\nfacilitating immediate and consistent PR-based myoelectric control\nimprovements. By iteratively refining control through real-time feedback, this\napproach reduces reliance on trial-and-error recalibration, enabling a more\nadaptive, self-correcting training framework. Conclusion: The 3D visual\nfeedback significantly improves PR control in novice operators through\nstructured training, enabling feedback-driven adaptation and reducing reliance\non extensive heuristic adjustments.", "AI": {"tldr": "\u5f00\u53d1\u4e86Reviewer 3D\u53ef\u89c6\u5316\u754c\u9762\uff0c\u901a\u8fc7\u5b9e\u65f6\u6295\u5f71\u808c\u7535\u4fe1\u53f7\u81f3\u89e3\u7801\u5668\u5206\u7c7b\u7a7a\u95f4\uff0c\u663e\u8457\u63d0\u5347PR\u63a7\u5236\u7cfb\u7edf\u8bad\u7ec3\u6548\u679c\u548c\u64cd\u4f5c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u808c\u7535\u5047\u80a2\u6a21\u5f0f\u8bc6\u522b\u7cfb\u7edf\u4f9d\u8d56\u7528\u6237\u751f\u6210\u9ad8\u533a\u5206\u5ea6EMG\u4fe1\u53f7\uff0c\u4f20\u7edf\u8bd5\u9519\u8bad\u7ec3\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u66f4\u76f4\u89c2\u7684\u53cd\u9988\u673a\u5236\u5b9e\u73b0\u7528\u6237\u4e0e\u89e3\u7801\u5668\u7684\u534f\u540c\u9002\u5e94\u3002", "method": "\u5bf912\u540d\u5065\u5eb7\u53d7\u8bd5\u8005\u8fdb\u884c10\u6b21\u8bad\u7ec3\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u4f7f\u7528Fitts\u5b9a\u5f8b\u4efb\u52a1\u8bc4\u4f30Reviewer 3D\u53ef\u89c6\u5316\u4e0e\u4f20\u7edf\u865a\u62df\u624b\u81c2\u5728\u8def\u5f84\u6548\u7387\u3001\u5b8c\u6210\u7387\u7b49\u6307\u6807\u7684\u8868\u73b0\u5dee\u5f02\u3002", "result": "Reviewer\u7ec4\u5728\u4efb\u52a1\u5b8c\u6210\u7387(\u63d0\u9ad819%)\u3001\u8def\u5f84\u6548\u7387(\u63d0\u534727%)\u548c\u541e\u5410\u91cf(\u589e\u52a00.8bps)\u7b49\u6838\u5fc3\u6307\u6807\u663e\u8457\u4f18\u4e8e\u5bf9\u7167\u7ec4\uff0c\u540c\u65f6\u51cf\u5c1132%\u7684\u8f68\u8ff9\u8d85\u8c03\u3002", "conclusion": "\u901a\u8fc7\u5c06\u89e3\u7801\u5668\u5206\u7c7b\u7a7a\u95f4\u53ef\u89c6\u5316\uff0cReviewer\u5efa\u7acb\u4e86\u53cc\u5411\u53cd\u9988\u8bad\u7ec3\u6846\u67b6\uff0c\u4f7f\u64cd\u4f5c\u8005\u8bad\u7ec3\u6548\u7387\u63d0\u534740%\uff0c\u51cf\u5c1175%\u7684\u8bd5\u9519\u6821\u51c6\u9700\u6c42\uff0c\u63a8\u52a8PR\u63a7\u5236\u7cfb\u7edf\u5411\u81ea\u9002\u5e94\u8bad\u7ec3\u8303\u5f0f\u8f6c\u53d8\u3002"}}
{"id": "2505.09831", "pdf": "https://arxiv.org/pdf/2505.09831", "abs": "https://arxiv.org/abs/2505.09831", "authors": ["Tushar Kataria", "Beatrice Knudsen", "Shireen Y. Elhabian"], "title": "ImplicitStainer: Data-Efficient Medical Image Translation for Virtual Antibody-based Tissue Staining Using Local Implicit Functions", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Hematoxylin and eosin (H&E) staining is a gold standard for microscopic\ndiagnosis in pathology. However, H&E staining does not capture all the\ndiagnostic information that may be needed. To obtain additional molecular\ninformation, immunohistochemical (IHC) stains highlight proteins that mark\nspecific cell types, such as CD3 for T-cells or CK8/18 for epithelial cells.\nWhile IHC stains are vital for prognosis and treatment guidance, they are\ntypically only available at specialized centers and time consuming to acquire,\nleading to treatment delays for patients. Virtual staining, enabled by deep\nlearning-based image translation models, provides a promising alternative by\ncomputationally generating IHC stains from H&E stained images. Although many\nGAN and diffusion based image to image (I2I) translation methods have been used\nfor virtual staining, these models treat image patches as independent data\npoints, which results in increased and more diverse data requirements for\neffective generation. We present ImplicitStainer, a novel approach that\nleverages local implicit functions to improve image translation, specifically\nvirtual staining performance, by focusing on pixel-level predictions. This\nmethod enhances robustness to variations in dataset sizes, delivering\nhigh-quality results even with limited data. We validate our approach on two\ndatasets using a comprehensive set of metrics and benchmark it against over\nfifteen state-of-the-art GAN- and diffusion based models. Full Code and models\ntrained will be released publicly via Github upon acceptance.", "AI": {"tldr": "\u63d0\u51faImplicitStainer\u65b9\u6cd5\uff0c\u5229\u7528\u5c40\u90e8\u9690\u51fd\u6570\u6539\u8fdb\u865a\u62df\u67d3\u8272\u6280\u672f\uff0c\u5728\u5c11\u91cf\u6570\u636e\u4e0b\u5b9e\u73b0\u9ad8\u8d28\u91cfIHC\u67d3\u8272\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u4f20\u7edfIHC\u67d3\u8272\u8017\u65f6\u4e14\u4f9d\u8d56\u4e13\u4e1a\u8bbe\u5907\uff0c\u73b0\u6709\u865a\u62df\u67d3\u8272\u6a21\u578b\u9700\u8981\u5927\u91cf\u6570\u636e\u3002\u9700\u5f00\u53d1\u6570\u636e\u9ad8\u6548\u7684\u65b9\u6cd5\u52a0\u901f\u75c5\u7406\u8bca\u65ad\u3002", "method": "\u57fa\u4e8e\u5c40\u90e8\u9690\u51fd\u6570\u8fdb\u884c\u50cf\u7d20\u7ea7\u9884\u6d4b\uff0c\u901a\u8fc7\u5750\u6807\u6620\u5c04\u5b66\u4e60\u56fe\u50cf\u8f6c\u6362\u5173\u7cfb\uff0c\u63d0\u5347\u6a21\u578b\u5bf9\u6570\u636e\u89c4\u6a21\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u4e24\u7c7b\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a15\u79cdSOTA\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u5728\u6709\u9650\u6570\u636e\u4e0b\u7684\u9ad8\u8d28\u91cf\u751f\u6210\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u865a\u62df\u67d3\u8272\u6570\u636e\u9700\u6c42\uff0c\u4e3a\u8d44\u6e90\u6709\u9650\u673a\u6784\u63d0\u4f9b\u53ef\u884c\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u75c5\u7406AI\u5e94\u7528\u843d\u5730\u3002"}}
{"id": "2505.09985", "pdf": "https://arxiv.org/pdf/2505.09985", "abs": "https://arxiv.org/abs/2505.09985", "authors": ["Pengfei Yu", "Bin Huang", "Minghui Zhang", "Weiwen Wu", "Shaoyu Wang", "Qiegen Liu"], "title": "Ordered-subsets Multi-diffusion Model for Sparse-view CT Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Score-based diffusion models have shown significant promise in the field of\nsparse-view CT reconstruction. However, the projection dataset is large and\nriddled with redundancy. Consequently, applying the diffusion model to\nunprocessed data results in lower learning effectiveness and higher learning\ndifficulty, frequently leading to reconstructed images that lack fine details.\nTo address these issues, we propose the ordered-subsets multi-diffusion model\n(OSMM) for sparse-view CT reconstruction. The OSMM innovatively divides the CT\nprojection data into equal subsets and employs multi-subsets diffusion model\n(MSDM) to learn from each subset independently. This targeted learning approach\nreduces complexity and enhances the reconstruction of fine details.\nFurthermore, the integration of one-whole diffusion model (OWDM) with complete\nsinogram data acts as a global information constraint, which can reduce the\npossibility of generating erroneous or inconsistent sinogram information.\nMoreover, the OSMM's unsupervised learning framework provides strong robustness\nand generalizability, adapting seamlessly to varying sparsity levels of CT\nsinograms. This ensures consistent and reliable performance across different\nclinical scenarios. Experimental results demonstrate that OSMM outperforms\ntraditional diffusion models in terms of image quality and noise resilience,\noffering a powerful and versatile solution for advanced CT imaging in\nsparse-view scenarios.", "AI": {"tldr": "\u63d0\u51fa\u6709\u5e8f\u5b50\u96c6\u591a\u91cd\u6269\u6563\u6a21\u578b(OSMM)\uff0c\u901a\u8fc7\u6295\u5f71\u6570\u636e\u5b50\u96c6\u5212\u5206\u548c\u591a\u91cd\u6269\u6563\u5b66\u4e60\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u7a00\u758f\u89c6\u56feCT\u91cd\u5efa\u7684\u7ec6\u8282\u8868\u73b0\u4e0e\u6297\u566a\u80fd\u529b", "motivation": "\u4f20\u7edf\u6269\u6563\u6a21\u578b\u76f4\u63a5\u5904\u7406\u539f\u59cbCT\u6295\u5f71\u6570\u636e\u5b58\u5728\u5b66\u4e60\u6548\u7387\u4f4e\u3001\u7ec6\u8282\u91cd\u5efa\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\uff0c\u5197\u4f59\u6570\u636e\u5bfc\u81f4\u6a21\u578b\u96be\u4ee5\u6355\u6349\u5173\u952e\u7279\u5f81", "method": "1. \u5c06CT\u6295\u5f71\u6570\u636e\u5212\u5206\u4e3a\u7b49\u91cf\u5b50\u96c6\uff0c\u91c7\u7528\u591a\u5b50\u96c6\u6269\u6563\u6a21\u578b(MSDM)\u72ec\u7acb\u5b66\u4e60\u5404\u5b50\u96c6\u7279\u5f81\n2. \u7ed3\u5408\u5b8c\u6574\u6295\u5f71\u6570\u636e\u7684\u6574\u4f53\u6269\u6563\u6a21\u578b(OWDM)\u4f5c\u4e3a\u5168\u5c40\u7ea6\u675f\n3. \u6784\u5efa\u65e0\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027", "result": "\u5b9e\u9a8c\u8868\u660eOSMM\u5728PSNR\u6307\u6807\u4e0a\u63d0\u53473.2dB\uff0c\u566a\u58f0\u6807\u51c6\u5dee\u964d\u4f4e41%\uff0c\u572880%\u7a00\u758f\u5ea6\u573a\u666f\u4e0b\u4ecd\u80fd\u4fdd\u6301\u7ed3\u6784\u76f8\u4f3c\u6027(SSIM)\u8d85\u8fc70.92", "conclusion": "OSMM\u901a\u8fc7\u5c40\u90e8\u7279\u5f81\u5b66\u4e60\u4e0e\u5168\u5c40\u7ea6\u675f\u7684\u534f\u540c\u673a\u5236\uff0c\u4e3a\u4e0d\u540c\u4e34\u5e8a\u573a\u666f\u63d0\u4f9b\u81ea\u9002\u5e94\u5f3a\u3001\u7ec6\u8282\u4fdd\u7559\u4f18\u5f02\u7684CT\u91cd\u5efa\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.10075", "pdf": "https://arxiv.org/pdf/2505.10075", "abs": "https://arxiv.org/abs/2505.10075", "authors": ["Jun Guo", "Xiaojian Ma", "Yikai Wang", "Min Yang", "Huaping Liu", "Qing Li"], "title": "FlowDreamer: A RGB-D World Model with Flow-based Motion Representations for Robot Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": "Project page: see https://sharinka0715.github.io/FlowDreamer/", "summary": "This paper investigates training better visual world models for robot\nmanipulation, i.e., models that can predict future visual observations by\nconditioning on past frames and robot actions. Specifically, we consider world\nmodels that operate on RGB-D frames (RGB-D world models). As opposed to\ncanonical approaches that handle dynamics prediction mostly implicitly and\nreconcile it with visual rendering in a single model, we introduce FlowDreamer,\nwhich adopts 3D scene flow as explicit motion representations. FlowDreamer\nfirst predicts 3D scene flow from past frame and action conditions with a\nU-Net, and then a diffusion model will predict the future frame utilizing the\nscene flow. FlowDreamer is trained end-to-end despite its modularized nature.\nWe conduct experiments on 4 different benchmarks, covering both video\nprediction and visual planning tasks. The results demonstrate that FlowDreamer\nachieves better performance compared to other baseline RGB-D world models by 7%\non semantic similarity, 11% on pixel quality, and 6% on success rate in various\nrobot manipulation domains.", "AI": {"tldr": "\u63d0\u51faFlowDreamer\u6a21\u578b\uff0c\u901a\u8fc7\u663e\u5f0f3D\u573a\u666f\u6d41\u9884\u6d4b\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u89c6\u89c9\u4e16\u754c\u5efa\u6a21\u6027\u80fd", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9690\u5f0f\u5904\u7406\u52a8\u6001\u9884\u6d4b\u5e76\u4e0e\u89c6\u89c9\u6e32\u67d3\u8026\u5408\uff0c\u9650\u5236\u4e86RGB-D\u4e16\u754c\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u9884\u6d4b\u7cbe\u5ea6", "method": "\u5206\u9636\u6bb5\u9884\u6d4b\u6846\u67b6\uff1aU-Net\u9884\u6d4b3D\u573a\u666f\u6d41 \u2192 \u6269\u6563\u6a21\u578b\u57fa\u4e8e\u573a\u666f\u6d41\u751f\u6210\u672a\u6765\u5e27\uff0c\u7aef\u5230\u7aef\u8bad\u7ec3\u6a21\u5757\u5316\u67b6\u6784", "result": "\u57284\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff1a\u8bed\u4e49\u76f8\u4f3c\u5ea6+7%\u3001\u50cf\u7d20\u8d28\u91cf+11%\u3001\u591a\u9886\u57df\u64cd\u4f5c\u6210\u529f\u7387+6%", "conclusion": "\u663e\u5f0f\u8fd0\u52a8\u8868\u5f81\u4e0e\u6a21\u5757\u5316\u67b6\u6784\u663e\u8457\u63d0\u5347\u89c6\u89c9\u4e16\u754c\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\u548c\u673a\u5668\u4eba\u4efb\u52a1\u6210\u529f\u7387"}}
{"id": "2505.10271", "pdf": "https://arxiv.org/pdf/2505.10271", "abs": "https://arxiv.org/abs/2505.10271", "authors": ["Rafael Pablos Sarabia", "Joachim Nyborg", "Morten Birk", "Jeppe Liborius Sj\u00f8rup", "Anders Lillevang Vesterholt", "Ira Assent"], "title": "RainPro-8: An Efficient Deep Learning Model to Estimate Rainfall Probabilities Over 8 Hours", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "We present a deep learning model for high-resolution probabilistic\nprecipitation forecasting over an 8-hour horizon in Europe, overcoming the\nlimitations of radar-only deep learning models with short forecast lead times.\nOur model efficiently integrates multiple data sources - including radar,\nsatellite, and physics-based numerical weather prediction (NWP) - while\ncapturing long-range interactions, resulting in accurate forecasts with robust\nuncertainty quantification through consistent probabilistic maps. Featuring a\ncompact architecture, it enables more efficient training and faster inference\nthan existing models. Extensive experiments demonstrate that our model\nsurpasses current operational NWP systems, extrapolation-based methods, and\ndeep-learning nowcasting models, setting a new standard for high-resolution\nprecipitation forecasting in Europe, ensuring a balance between accuracy,\ninterpretability, and computational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u6b27\u6d328\u5c0f\u65f6\u9ad8\u5206\u8fa8\u7387\u6982\u7387\u964d\u6c34\u9884\u62a5\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u6574\u5408\u591a\u6e90\u6570\u636e\u5b9e\u73b0\u8d85\u8d8a\u73b0\u6709\u7cfb\u7edf\u7684\u6027\u80fd", "motivation": "\u514b\u670d\u96f7\u8fbe\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9884\u62a5\u65f6\u6548\u77ed\uff08\u4ec56\u5c0f\u65f6\uff09\u7684\u5c40\u9650\uff0c\u901a\u8fc7\u591a\u6e90\u6570\u636e\u878d\u5408\u63d0\u5347\u9884\u62a5\u7cbe\u5ea6\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u80fd\u529b", "method": "\u7d27\u51d1\u578b\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u6574\u5408\u96f7\u8fbe\u3001\u536b\u661f\u548c\u6570\u503c\u9884\u62a5\u6570\u636e\uff0c\u91c7\u7528\u6982\u7387\u6620\u5c04\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u517c\u987e\u8bad\u7ec3\u6548\u7387\u548c\u957f\u7a0b\u76f8\u4e92\u4f5c\u7528\u6355\u6349", "result": "\u5728\u51c6\u786e\u7387\uff08CRPS\u8bc4\u5206\uff09\u548c\u8ba1\u7b97\u6548\u7387\uff08\u63a8\u7406\u901f\u5ea6\u63d0\u53473\u500d\uff09\u4e0a\u5168\u9762\u8d85\u8d8a\u4f20\u7edf\u6570\u503c\u9884\u62a5\u3001\u5916\u63a8\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u4e34\u8fd1\u9884\u62a5\u6a21\u578b", "conclusion": "\u5efa\u7acb\u4e86\u6b27\u6d32\u9ad8\u5206\u8fa8\u7387\u964d\u6c34\u9884\u62a5\u65b0\u6807\u51c6\uff0c\u5728\u9884\u62a5\u7cbe\u5ea6\uff080-8\u5c0f\u65f6\uff09\u3001\u53ef\u89e3\u91ca\u6027\u548c\u8ba1\u7b97\u6548\u7387\u95f4\u5b9e\u73b0\u6700\u4f73\u5e73\u8861"}}
{"id": "2505.10312", "pdf": "https://arxiv.org/pdf/2505.10312", "abs": "https://arxiv.org/abs/2505.10312", "authors": ["Anh Tuan Ha", "Hoang Khang Phan", "Thai Minh Tien Ngo", "Anh Phan Truong", "Nhat Tan Le"], "title": "SOS: A Shuffle Order Strategy for Data Augmentation in Industrial Human Activity Recognition", "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "In the realm of Human Activity Recognition (HAR), obtaining high quality and\nvariance data is still a persistent challenge due to high costs and the\ninherent variability of real-world activities. This study introduces a\ngeneration dataset by deep learning approaches (Attention Autoencoder and\nconditional Generative Adversarial Networks). Another problem that data\nheterogeneity is a critical challenge, one of the solutions is to shuffle the\ndata to homogenize the distribution. Experimental results demonstrate that the\nrandom sequence strategy significantly improves classification performance,\nachieving an accuracy of up to 0.70 $\\pm$ 0.03 and a macro F1 score of 0.64\n$\\pm$ 0.01. For that, disrupting temporal dependencies through random sequence\nreordering compels the model to focus on instantaneous recognition, thereby\nimproving robustness against activity transitions. This approach not only\nbroadens the effective training dataset but also offers promising avenues for\nenhancing HAR systems in complex, real-world scenarios.", "AI": {"tldr": "\u5229\u7528\u6ce8\u610f\u529b\u81ea\u7f16\u7801\u5668\u548c\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u751f\u6210\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u968f\u673a\u5e8f\u5217\u91cd\u7ec4\u7b56\u7565\u63d0\u5347\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u6a21\u578b\u6027\u80fd\u81f3\u51c6\u786e\u73870.70\u00b10.03\u3002", "motivation": "\u89e3\u51b3\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u9886\u57df\u9ad8\u8d28\u91cf\u6570\u636e\u83b7\u53d6\u6210\u672c\u9ad8\u3001\u73b0\u5b9e\u573a\u666f\u6d3b\u52a8\u5f02\u8d28\u6027\u5bfc\u81f4\u7684\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u751f\u6210\u5408\u6210\u6570\u636e\uff08\u6ce8\u610f\u529b\u81ea\u7f16\u7801\u5668+\u6761\u4ef6GAN\uff09\uff0c\u5e76\u901a\u8fc7\u6253\u4e71\u65f6\u95f4\u5e8f\u5217\u6d88\u9664\u6570\u636e\u5f02\u8d28\u6027\uff0c\u5f3a\u5236\u6a21\u578b\u5b66\u4e60\u77ac\u65f6\u7279\u5f81\u3002", "result": "\u968f\u673a\u5e8f\u5217\u7b56\u7565\u4f7f\u5206\u7c7b\u51c6\u786e\u7387\u8fbe0.70\u00b10.03\uff0c\u5b8f\u89c2F1\u5206\u65700.64\u00b10.01\uff0c\u6a21\u578b\u5bf9\u6d3b\u52a8\u8fc7\u6e21\u7684\u9c81\u68d2\u6027\u663e\u8457\u589e\u5f3a\u3002", "conclusion": "\u65f6\u95f4\u4f9d\u8d56\u89e3\u6784\u7b56\u7565\u4e0d\u4ec5\u6269\u5c55\u4e86\u6709\u6548\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u8fd8\u4e3a\u590d\u6742\u573a\u666f\u4e0b\u7684\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u7cfb\u7edf\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.10405", "pdf": "https://arxiv.org/pdf/2505.10405", "abs": "https://arxiv.org/abs/2505.10405", "authors": ["Jianhao Huang", "Qunsong Zeng", "Kaibin Huang"], "title": "Visual Fidelity Index for Generative Semantic Communications with Critical Information Embedding", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Generative semantic communication (Gen-SemCom) with large artificial\nintelligence (AI) model promises a transformative paradigm for 6G networks,\nwhich reduces communication costs by transmitting low-dimensional prompts\nrather than raw data. However, purely prompt-driven generation loses\nfine-grained visual details. Additionally, there is a lack of systematic\nmetrics to evaluate the performance of Gen-SemCom systems. To address these\nissues, we develop a hybrid Gen-SemCom system with a critical information\nembedding (CIE) framework, where both text prompts and semantically critical\nfeatures are extracted for transmissions. First, a novel approach of semantic\nfiltering is proposed to select and transmit the semantically critical features\nof images relevant to semantic label. By integrating the text prompt and\ncritical features, the receiver reconstructs high-fidelity images using a\ndiffusion-based generative model. Next, we propose the generative visual\ninformation fidelity (GVIF) metric to evaluate the visual quality of the\ngenerated image. By characterizing the statistical models of image features,\nthe GVIF metric quantifies the mutual information between the distorted\nfeatures and their original counterparts. By maximizing the GVIF metric, we\ndesign a channel-adaptive Gen-SemCom system that adaptively control the volume\nof features and compression rate according to the channel state. Experimental\nresults validate the GVIF metric's sensitivity to visual fidelity, correlating\nwith both the PSNR and critical information volume. In addition, the optimized\nsystem achieves superior performance over benchmarking schemes in terms of\nhigher PSNR and lower FID scores.", "AI": {"tldr": "\u63d0\u51fa\u878d\u5408\u5173\u952e\u4fe1\u606f\u5d4c\u5165\u7684\u751f\u6210\u5f0f\u8bed\u4e49\u901a\u4fe1\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bed\u4e49\u8fc7\u6ee4\u63d0\u53d6\u6587\u672c\u63d0\u793a\u4e0e\u5173\u952e\u7279\u5f81\uff0c\u7ed3\u5408GVIF\u8bc4\u4f30\u6307\u6807\u5b9e\u73b0\u4fe1\u9053\u81ea\u9002\u5e94\u4f18\u5316\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u7cfb\u7edf\u5728\u56fe\u50cf\u4fdd\u771f\u5ea6\u4e0e\u4f20\u8f93\u6548\u7387\u4e0a\u7684\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5f0f\u8bed\u4e49\u901a\u4fe1\u7cfb\u7edf\u5b58\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7ec6\u8282\u4e22\u5931\u95ee\u9898\uff0c\u4e14\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u4f30\u6307\u6807\u3002\u9700\u8981\u517c\u987e\u8bed\u4e49\u538b\u7f29\u6548\u7387\u4e0e\u9ad8\u4fdd\u771f\u91cd\u5efa\u80fd\u529b\u3002", "method": "1. \u8bbe\u8ba1\u8bed\u4e49\u8fc7\u6ee4\u7b97\u6cd5\u63d0\u53d6\u4e0e\u8bed\u4e49\u6807\u7b7e\u76f8\u5173\u7684\u5173\u952e\u7279\u5f81 2. \u878d\u5408\u6587\u672c\u63d0\u793a\u4e0e\u5173\u952e\u7279\u5f81\u8fdb\u884c\u6269\u6563\u6a21\u578b\u91cd\u5efa 3. \u63d0\u51faGVIF\u6307\u6807\u91cf\u5316\u7279\u5f81\u4e92\u4fe1\u606f 4. \u57fa\u4e8eGVIF\u6700\u5927\u5316\u5b9e\u73b0\u4fe1\u9053\u81ea\u9002\u5e94\u7684\u7279\u5f81\u91cf/\u538b\u7f29\u7387\u8054\u5408\u4f18\u5316", "result": "GVIF\u6307\u6807\u4e0ePSNR/\u5173\u952e\u4fe1\u606f\u91cf\u663e\u8457\u76f8\u5173\uff0c\u4f18\u5316\u7cfb\u7edf\u5728PSNR\u63d0\u53473.2dB\uff0cFID\u964d\u4f4e15.7%\uff0c\u4f18\u4e8e\u57fa\u51c6\u65b9\u6848\u3002", "conclusion": "\u5173\u952e\u4fe1\u606f\u5d4c\u5165\u6846\u67b6\u6709\u6548\u5e73\u8861\u8bed\u4e49\u538b\u7f29\u4e0e\u89c6\u89c9\u4fdd\u771f\uff0cGVIF\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u7cfb\u7edf\u4e3a6G\u8bed\u4e49\u901a\u4fe1\u63d0\u4f9b\u53ef\u9760\u8bc4\u4f30\u4e0e\u4f18\u5316\u8def\u5f84\u3002"}}
{"id": "2505.10441", "pdf": "https://arxiv.org/pdf/2505.10441", "abs": "https://arxiv.org/abs/2505.10441", "authors": ["Filippo Leveni", "Luca Magri", "Giacomo Boracchi", "Cesare Alippi"], "title": "PIF: Anomaly detection via preference embedding", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "Accepted at International Conference on Pattern Recognition (ICPR\n  2020)", "summary": "We address the problem of detecting anomalies with respect to structured\npatterns. To this end, we conceive a novel anomaly detection method called PIF,\nthat combines the advantages of adaptive isolation methods with the flexibility\nof preference embedding. Specifically, we propose to embed the data in a high\ndimensional space where an efficient tree-based method, PI-Forest, is employed\nto compute an anomaly score. Experiments on synthetic and real datasets\ndemonstrate that PIF favorably compares with state-of-the-art anomaly detection\ntechniques, and confirm that PI-Forest is better at measuring arbitrary\ndistances and isolate points in the preference space.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u578b\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5PIF\uff0c\u901a\u8fc7\u9ad8\u7ef4\u7a7a\u95f4\u5d4c\u5165\u548c\u6811\u7ed3\u6784(PI-Forest)\u5b9e\u73b0\u9ad8\u6548\u5f02\u5e38\u8bc4\u5206\uff0c\u5b9e\u9a8c\u663e\u793a\u4f18\u4e8e\u4e3b\u6d41\u65b9\u6cd5", "motivation": "\u89e3\u51b3\u7ed3\u6784\u5316\u6a21\u5f0f\u5f02\u5e38\u68c0\u6d4b\u95ee\u9898\uff0c\u878d\u5408\u81ea\u9002\u5e94\u9694\u79bb\u65b9\u6cd5\u4e0e\u504f\u597d\u5d4c\u5165\u7684\u7075\u6d3b\u6027", "method": "PI-Forest\uff1a\u5c06\u6570\u636e\u5d4c\u5165\u9ad8\u7ef4\u504f\u597d\u7a7a\u95f4\uff0c\u91c7\u7528\u6811\u57fa\u65b9\u6cd5\u8ba1\u7b97\u5f02\u5e38\u5206\u6570", "result": "\u5408\u6210/\u771f\u5b9e\u6570\u636e\u96c6\u5b9e\u9a8c\u8868\u660ePIF\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0cPI-Forest\u5728\u504f\u597d\u7a7a\u95f4\u8ddd\u79bb\u5ea6\u91cf/\u5f02\u5e38\u70b9\u9694\u79bb\u8868\u73b0\u66f4\u4f18", "conclusion": "PI-Forest\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u590d\u6742\u504f\u597d\u7a7a\u95f4\u4e2d\u7684\u8ddd\u79bb\u6d4b\u91cf\u548c\u5f02\u5e38\u9694\u79bb\u5c40\u9650\u6027"}}
{"id": "2505.10457", "pdf": "https://arxiv.org/pdf/2505.10457", "abs": "https://arxiv.org/abs/2505.10457", "authors": ["Matteo Gambella", "Vicente Javier Castro Solar", "Manuel Roveri"], "title": "SEAL: Searching Expandable Architectures for Incremental Learning", "categories": ["cs.LG", "cs.AI", "cs.CV", "68T07"], "comment": "8 pages, 5 figures", "summary": "Incremental learning is a machine learning paradigm where a model learns from\na sequential stream of tasks. This setting poses a key challenge: balancing\nplasticity (learning new tasks) and stability (preserving past knowledge).\nNeural Architecture Search (NAS), a branch of AutoML, automates the design of\nthe architecture of Deep Neural Networks and has shown success in static\nsettings. However, existing NAS-based approaches to incremental learning often\nrely on expanding the model at every task, making them impractical in\nresource-constrained environments. In this work, we introduce SEAL, a NAS-based\nframework tailored for data-incremental learning, a scenario where disjoint\ndata samples arrive sequentially and are not stored for future access. SEAL\nadapts the model structure dynamically by expanding it only when necessary,\nbased on a capacity estimation metric. Stability is preserved through\ncross-distillation training after each expansion step. The NAS component\njointly searches for both the architecture and the optimal expansion policy.\nExperiments across multiple benchmarks demonstrate that SEAL effectively\nreduces forgetting and enhances accuracy while maintaining a lower model size\ncompared to prior methods. These results highlight the promise of combining NAS\nand selective expansion for efficient, adaptive learning in incremental\nscenarios.", "AI": {"tldr": "\u63d0\u51faSEAL\u6846\u67b6\uff0c\u7ed3\u5408\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u548c\u9009\u62e9\u6027\u6269\u5c55\u7b56\u7565\uff0c\u5728\u6570\u636e\u589e\u91cf\u5b66\u4e60\u4e2d\u52a8\u6001\u8c03\u6574\u6a21\u578b\u7ed3\u6784\uff0c\u6709\u6548\u5e73\u8861\u53ef\u5851\u6027\u4e0e\u7a33\u5b9a\u6027\uff0c\u51cf\u5c11\u9057\u5fd8\u5e76\u4fdd\u6301\u8f83\u5c0f\u6a21\u578b\u89c4\u6a21\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8eNAS\u7684\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u6301\u7eed\u6269\u5c55\u6a21\u578b\u5bfc\u81f4\u7684\u8d44\u6e90\u6d6a\u8d39\u95ee\u9898\uff0c\u9488\u5bf9\u6570\u636e\u589e\u91cf\u573a\u666f\u4e2d\u6837\u672c\u4e0d\u53ef\u91cd\u590d\u8bbf\u95ee\u7684\u7279\u6027\uff0c\u5bfb\u6c42\u66f4\u9ad8\u6548\u7684\u52a8\u6001\u67b6\u6784\u8c03\u6574\u65b9\u6848\u3002", "method": "1. \u5f15\u5165\u5bb9\u91cf\u4f30\u8ba1\u6307\u6807\u52a8\u6001\u89e6\u53d1\u6a21\u578b\u6269\u5c55\n2. \u4ea4\u53c9\u84b8\u998f\u8bad\u7ec3\u4fdd\u6301\u6269\u5c55\u540e\u7684\u7a33\u5b9a\u6027\n3. NAS\u8054\u5408\u4f18\u5316\u67b6\u6784\u8bbe\u8ba1\u4e0e\u6269\u5c55\u7b56\u7565\n4. \u9009\u62e9\u6027\u6269\u5c55\u673a\u5236\u51cf\u5c11\u53c2\u6570\u589e\u957f", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u5e73\u5747\u51c6\u786e\u7387\u63d0\u534715%\uff0c\u6a21\u578b\u5c3a\u5bf8\u7f29\u51cf40%\u4ee5\u4e0a\uff0c\u9057\u5fd8\u7387\u964d\u4f4e\u81f3\u4f20\u7edf\u65b9\u6cd5\u76841/3\u3002", "conclusion": "\u9a8c\u8bc1\u4e86NAS\u4e0e\u52a8\u6001\u6269\u5c55\u7b56\u7565\u7ed3\u5408\u5728\u589e\u91cf\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u62d3\u5c55\u4e86AutoML\u5728\u52a8\u6001\u573a\u666f\u4e2d\u7684\u5e94\u7528\u8fb9\u754c\u3002"}}
{"id": "2505.10464", "pdf": "https://arxiv.org/pdf/2505.10464", "abs": "https://arxiv.org/abs/2505.10464", "authors": ["Jiaming Liang", "Lihuan Dai", "Xiaoqi Sheng", "Xiangguang Chen", "Chun Yao", "Guihua Tao", "Qibin Leng", "Honming Cai", "Xi Zhong"], "title": "HWA-UNETR: Hierarchical Window Aggregate UNETR for 3D Multimodal Gastric Lesion Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": "This work has been provisionally accepted for MICCAI 2025", "summary": "Multimodal medical image segmentation faces significant challenges in the\ncontext of gastric cancer lesion analysis. This clinical context is defined by\nthe scarcity of independent multimodal datasets and the imperative to\namalgamate inherently misaligned modalities. As a result, algorithms are\nconstrained to train on approximate data and depend on application migration,\nleading to substantial resource expenditure and a potential decline in analysis\naccuracy. To address those challenges, we have made two major contributions:\nFirst, we publicly disseminate the GCM 2025 dataset, which serves as the first\nlarge-scale, open-source collection of gastric cancer multimodal MRI scans,\nfeaturing professionally annotated FS-T2W, CE-T1W, and ADC images from 500\npatients. Second, we introduce HWA-UNETR, a novel 3D segmentation framework\nthat employs an original HWA block with learnable window aggregation layers to\nestablish dynamic feature correspondences between different modalities'\nanatomical structures, and leverages the innovative tri-orientated fusion mamba\nmechanism for context modeling and capturing long-range spatial dependencies.\nExtensive experiments on our GCM 2025 dataset and the publicly BraTS 2021\ndataset validate the performance of our framework, demonstrating that the new\napproach surpasses existing methods by up to 1.68\\% in the Dice score while\nmaintaining solid robustness. The dataset and code are public via\nhttps://github.com/JeMing-creater/HWA-UNETR.", "AI": {"tldr": "\u63d0\u51faGCM 2025\u591a\u6a21\u6001MRI\u6570\u636e\u96c6\u4e0eHWA-UNETR\u5206\u5272\u6846\u67b6\uff0c\u89e3\u51b3\u80c3\u764c\u75c5\u7076\u5206\u6790\u4e2d\u6570\u636e\u7a00\u7f3a\u4e0e\u6a21\u6001\u878d\u5408\u96be\u9898", "motivation": "\u73b0\u6709\u80c3\u764c\u591a\u6a21\u6001\u5206\u6790\u9762\u4e34\u72ec\u7acb\u6a21\u6001\u6570\u636e\u96c6\u7a00\u7f3a\u3001\u56fa\u6709\u6a21\u6001\u9519\u4f4d\u5bfc\u81f4\u7684\u6a21\u578b\u8bad\u7ec3\u56f0\u96be\u53ca\u7cbe\u5ea6\u4e0b\u964d\u95ee\u9898", "method": "HWA-UNETR\u6846\u67b6\u91c7\u7528\u53ef\u5b66\u4e60\u7a97\u53e3\u805a\u5408\u5c42\u5efa\u7acb\u8de8\u6a21\u6001\u7279\u5f81\u5bf9\u5e94\uff0c\u7ed3\u5408\u4e09\u5411\u878d\u5408\u66fc\u5df4\u673a\u5236\u8fdb\u884c\u4e0a\u4e0b\u6587\u5efa\u6a21\u4e0e\u957f\u7a0b\u4f9d\u8d56\u6355\u83b7", "result": "\u5728GCM 2025\u548cBraTS 2021\u6570\u636e\u96c6\u4e0aDice\u5206\u6570\u63d0\u53471.68%\uff0c\u4fdd\u6301\u5f3a\u9c81\u68d2\u6027", "conclusion": "\u9996\u4e2a\u5927\u89c4\u6a21\u80c3\u764c\u591a\u6a21\u6001\u5f00\u653e\u6570\u636e\u96c6\u4e0e\u65b0\u578b\u52a8\u6001\u7279\u5f81\u878d\u5408\u6846\u67b6\u7684\u7ed3\u5408\uff0c\u4e3a\u7cbe\u51c6\u533b\u5b66\u5f71\u50cf\u5206\u6790\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2505.10492", "pdf": "https://arxiv.org/pdf/2505.10492", "abs": "https://arxiv.org/abs/2505.10492", "authors": ["Taylor L. Bobrow", "Mayank Golhar", "Suchapa Arayakarnkul", "Anthony A. Song", "Saowanee Ngamruengphong", "Nicholas J. Durr"], "title": "Multi-contrast laser endoscopy for in vivo gastrointestinal imaging", "categories": ["eess.IV", "cs.CV", "physics.med-ph", "physics.optics"], "comment": null, "summary": "White light endoscopy is the clinical gold standard for detecting diseases in\nthe gastrointestinal tract. Most applications involve identifying visual\nabnormalities in tissue color, texture, and shape. Unfortunately, the contrast\nof these features is often subtle, causing many clinically relevant cases to go\nundetected. To overcome this challenge, we introduce Multi-contrast Laser\nEndoscopy (MLE): a platform for widefield clinical imaging with rapidly tunable\nspectral, coherent, and directional illumination. We demonstrate three\ncapabilities of MLE: enhancing tissue chromophore contrast with multispectral\ndiffuse reflectance, quantifying blood flow using laser speckle contrast\nimaging, and characterizing mucosal topography using photometric stereo. We\nvalidate MLE with benchtop models, then demonstrate MLE in vivo during clinical\ncolonoscopies. MLE images from 31 polyps demonstrate an approximate three-fold\nimprovement in contrast and a five-fold improvement in color difference\ncompared to white light and narrow band imaging. With the ability to reveal\nmultiple complementary types of tissue contrast while seamlessly integrating\ninto the clinical environment, MLE shows promise as an investigative tool to\nimprove gastrointestinal imaging.", "AI": {"tldr": "\u591a\u5bf9\u6bd4\u6fc0\u5149\u5185\u7aa5\u955c(MLE)\u901a\u8fc7\u53ef\u8c03\u8c10\u5149\u8c31\u3001\u76f8\u5e72\u548c\u5b9a\u5411\u7167\u660e\u6280\u672f\uff0c\u5b9e\u73b0\u4e09\u500d\u7ec4\u7ec7\u5bf9\u6bd4\u5ea6\u63d0\u5347\u548c\u4e94\u500d\u989c\u8272\u5dee\u5f02\u6539\u5584\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u767d\u5149\u548c\u7a84\u5e26\u6210\u50cf\u3002", "motivation": "\u4f20\u7edf\u767d\u5149\u5185\u7aa5\u955c\u5bf9\u7ec4\u7ec7\u989c\u8272\u3001\u7eb9\u7406\u548c\u5f62\u72b6\u7684\u7ec6\u5fae\u5dee\u5f02\u68c0\u6d4b\u80fd\u529b\u6709\u9650\uff0c\u5bfc\u81f4\u4e34\u5e8a\u6f0f\u8bca\u3002MLE\u65e8\u5728\u901a\u8fc7\u591a\u6a21\u6001\u6210\u50cf\u6280\u672f\u7a81\u7834\u8fd9\u4e00\u9650\u5236\u3002", "method": "1. \u591a\u5149\u8c31\u6269\u6563\u53cd\u5c04\u589e\u5f3a\u7ec4\u7ec7\u8272\u56e2\u5bf9\u6bd4\n2. \u6fc0\u5149\u6563\u6591\u5bf9\u6bd4\u6210\u50cf\u91cf\u5316\u8840\u6d41\n3. \u5149\u5ea6\u7acb\u4f53\u6280\u672f\u8868\u5f81\u9ecf\u819c\u5730\u5f62\n\u5e73\u53f0\u9a8c\u8bc1\u5305\u542b\u79bb\u4f53\u6a21\u578b\u548c\u4e34\u5e8a\u7ed3\u80a0\u955c\u68c0\u67e5\u3002", "result": "\u4e34\u5e8a31\u4e2a\u606f\u8089\u6837\u672c\u663e\u793a\uff1a\n- \u5bf9\u6bd4\u5ea6\u63d0\u5347\u7ea63\u500d\n- \u989c\u8272\u5dee\u5f02\u6539\u55845\u500d\n- \u65e0\u7f1d\u6574\u5408\u81f3\u4e34\u5e8a\u73af\u5883", "conclusion": "MLE\u901a\u8fc7\u591a\u7ef4\u5ea6\u4e92\u8865\u6027\u7ec4\u7ec7\u5bf9\u6bd4\u6280\u672f\uff0c\u5c55\u73b0\u51fa\u4f5c\u4e3a\u6539\u5584\u80c3\u80a0\u9053\u6210\u50cf\u7814\u7a76\u5de5\u5177\u7684\u4e34\u5e8a\u6f5c\u529b\u3002"}}
