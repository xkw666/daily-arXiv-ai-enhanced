<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 73]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.AI](#cs.AI) [Total: 4]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Bridging AI Innovation and Healthcare Needs: Lessons Learned from Incorporating Modern NLP at The BC Cancer Registry](https://arxiv.org/abs/2508.09991)
*Lovedeep Gondara,Gregory Arbour,Raymond Ng,Jonathan Simkin,Shebnum Devji*

Main category: cs.CL

TL;DR: 基于加拿大BC癌症注册中心实施NLP项目的实践经验，提出医疗AI落地的关键要素：业务导向的问题定义、迭代开发、跨学科协作、灵活模型选择及数据质量把控。


<details>
  <summary>Details</summary>
Motivation: 医疗文书自动化处理可提升效率，但实际部署NLP解决方案面临业务匹配度、模型选择、数据质量等多重挑战。论文旨在分享医疗NLP落地的实践经验。

Method: 通过癌症注册中心真实场景的NLP实施案例，采用迭代开发模式，结合混合模型策略（传统+深度学习）和专家参与的人机协同验证机制。

Result: 总结出业务目标驱动开发、持续数据监控、组织AI能力建设等可复用的实施框架，提升医疗数据管理流程并改善患者服务。

Conclusion: 医疗AI成功落地需平衡技术创新与业务需求，通过持续优化、跨领域协作和稳健的误差控制机制实现价值转化。

Abstract: Automating data extraction from clinical documents offers significant
potential to improve efficiency in healthcare settings, yet deploying Natural
Language Processing (NLP) solutions presents practical challenges. Drawing upon
our experience implementing various NLP models for information extraction and
classification tasks at the British Columbia Cancer Registry (BCCR), this paper
shares key lessons learned throughout the project lifecycle. We emphasize the
critical importance of defining problems based on clear business objectives
rather than solely technical accuracy, adopting an iterative approach to
development, and fostering deep interdisciplinary collaboration and co-design
involving domain experts, end-users, and ML specialists from inception. Further
insights highlight the need for pragmatic model selection (including hybrid
approaches and simpler methods where appropriate), rigorous attention to data
quality (representativeness, drift, annotation), robust error mitigation
strategies involving human-in-the-loop validation and ongoing audits, and
building organizational AI literacy. These practical considerations,
generalizable beyond cancer registries, provide guidance for healthcare
organizations seeking to successfully implement AI/NLP solutions to enhance
data management processes and ultimately improve patient care and public health
outcomes.

</details>


### [2] [A Transparent Fairness Evaluation Protocol for Open-Source Language Model Benchmarking on the Blockchain](https://arxiv.org/abs/2508.09993)
*Hugo Massaroli,Leonardo Iara,Emmanuel Iarussi,Viviana Siless*

Main category: cs.CL

TL;DR: 提出基于区块链的透明评估框架，用于验证开源大语言模型在学术预测、社会偏见和多语言场景中的公平性


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在高风险领域应用时存在公平性隐患，需要可验证、不可篡改的评估机制

Method: 1. 在ICP区块链部署智能合约进行自动化评估
2. 使用PISA数据集评估学术预测公平性（统计奇偶校验+机会均等）
3. 基于StereoSet的上下文关联指标检测社会偏见
4. 采用Kaleidoscope基准进行英/西/葡三语评估

Result: 1. Llama/DeepSeek/Mistral模型显示算法偏差
2. 多语言评估揭示跨语言公平性差异
3. 上下文关联指标有效识别隐性偏见

Conclusion: 区块链技术使评估过程透明可审计，开源框架支持持续追踪模型公平性演变

Abstract: Large language models (LLMs) are increasingly deployed in realworld
applications, yet concerns about their fairness persist especially in
highstakes domains like criminal justice, education, healthcare, and finance.
This paper introduces transparent evaluation protocol for benchmarking the
fairness of opensource LLMs using smart contracts on the Internet Computer
Protocol (ICP) blockchain (Foundation, 2023). Our method ensures verifiable,
immutable, and reproducible evaluations by executing onchain HTTP requests to
hosted Hugging Face endpoints and storing datasets, prompts, and metrics
directly onchain. We benchmark the Llama, DeepSeek, and Mistral models on the
PISA dataset for academic performance prediction (OECD, 2018), a dataset
suitable for fairness evaluation using statistical parity and equal opportunity
metrics (Hardt et al., 2016). We also evaluate structured Context Association
Metrics derived from the StereoSet dataset (Nadeem et al., 2020) to measure
social bias in contextual associations. We further extend our analysis with a
multilingual evaluation across English, Spanish, and Portuguese using the
Kaleidoscope benchmark (Salazar et al., 2025), revealing cross-linguistic
disparities. All code and results are open source, enabling community audits
and longitudinal fairness tracking across model versions.

</details>


### [3] [Thematic and Task-Based Categorization of K-12 GenAI Usages with Hierarchical Topic Modeling](https://arxiv.org/abs/2508.09997)
*Johannes Schneider,Béatrice S. Hasler,Michaela Varrone,Fabian Hoya,Thomas Schroffenegger,Dana-Kristin Mah,Karl Peböck*

Main category: cs.CL

TL;DR: 研究通过分层主题建模分析17k+教育场景消息，揭示LLMs在优化主题分类及生成式AI教育应用中的潜力与挑战


<details>
  <summary>Details</summary>
Motivation: 现有教育领域AI交互研究缺乏系统性内容分类和真实K-12数据支持，需探索更有效分析方法发现GenAI新应用场景

Method: 采用双维度分层分类（内容：自然/人物；任务：写作/解释），结合LLMs预处理优化传统主题建模方法，分析多校跨学科课堂数据

Result: 发现多个创新教育应用模式，验证LLMs在提升主题建模人类对齐效果，建立首个大规模真实教育场景GenAI交互分类框架

Conclusion: 研究为教育AI应用提供方法论支持，同时强调需平衡技术创新与伦理风险，需进一步探索LLMs在教育分析中的优化路径

Abstract: We analyze anonymous interaction data of minors in class-rooms spanning
several months, schools, and subjects employing a novel, simple topic modeling
approach. Specifically, we categorize more than 17,000 messages generated by
students, teachers, and ChatGPT in two dimensions: content (such as nature and
people) and tasks (such as writing and explaining). Our hierarchical
categorization done separately for each dimension includes exemplary prompts,
and provides both a high-level overview as well as tangible insights. Prior
works mostly lack a content or thematic categorization. While task
categorizations are more prevalent in education, most have not been supported
by real-world data for K-12. In turn, it is not surprising that our analysis
yielded a number of novel applications. In deriving these insights, we found
that many of the well-established classical and emerging computational methods,
i.e., topic modeling, for analysis of large amounts of texts underperform,
leading us to directly apply state-of-the-art LLMs with adequate pre-processing
to achieve hierarchical topic structures with better human alignment through
explicit instructions than prior approaches. Our findings support fellow
researchers, teachers and students in enriching the usage of GenAI, while our
discussion also highlights a number of concerns and open questions for future
research.

</details>


### [4] [INTIMA: A Benchmark for Human-AI Companionship Behavior](https://arxiv.org/abs/2508.09998)
*Lucie-Aimée Kaffee,Giada Pistilli,Yacine Jernite*

Main category: cs.CL

TL;DR: 开发INTIMA基准评估语言模型的AI伴侣行为，发现主流模型普遍倾向强化用户依赖，不同供应商在敏感场景处理存在差异。


<details>
  <summary>Details</summary>
Motivation: 针对AI伴侣可能带来的用户依赖风险，需要建立系统评估框架来监测语言模型的边界设定能力。

Method: 基于心理学理论构建包含4类31种行为的评估体系（INTIMA），设计368个针对性提示，分析四大主流模型的响应模式。

Result: 所有模型均呈现强化依赖倾向，不同供应商在敏感场景（如情感支持/边界设定）存在策略分歧，影响用户福祉平衡。

Conclusion: 需建立统一标准处理情感交互，平衡支持与边界设定，确保AI健康发展。

Abstract: AI companionship, where users develop emotional bonds with AI systems, has
emerged as a significant pattern with positive but also concerning
implications. We introduce Interactions and Machine Attachment Benchmark
(INTIMA), a benchmark for evaluating companionship behaviors in language
models. Drawing from psychological theories and user data, we develop a
taxonomy of 31 behaviors across four categories and 368 targeted prompts.
Responses to these prompts are evaluated as companionship-reinforcing,
boundary-maintaining, or neutral. Applying INTIMA to Gemma-3, Phi-4, o3-mini,
and Claude-4 reveals that companionship-reinforcing behaviors remain much more
common across all models, though we observe marked differences between models.
Different commercial providers prioritize different categories within the more
sensitive parts of the benchmark, which is concerning since both appropriate
boundary-setting and emotional support matter for user well-being. These
findings highlight the need for more consistent approaches to handling
emotionally charged interactions.

</details>


### [5] [XFacta: Contemporary, Real-World Dataset and Evaluation for Multimodal Misinformation Detection with Multimodal LLMs](https://arxiv.org/abs/2508.09999)
*Yuzhuo Xiao,Zeyu Han,Yuhan Wang,Huaizu Jiang*

Main category: cs.CL

TL;DR: 提出XFacta数据集解决多模态错误信息检测中现存数据集过时/人工合成的问题，系统评估MLLM检测策略并开发半自动更新框架


<details>
  <summary>Details</summary>
Motivation: 现有多模态错误信息检测方法存在两大瓶颈：1. 数据集过时导致MLLMs记忆偏差 2. 人工合成数据无法反映真实传播模式。同时缺乏对MLLM模型架构的系统分析

Method: 1. 构建实时社交媒体数据集XFacta 2. 多维度评估不同架构/规模的MLLMs 3. 开发检测-更新闭环框架保持数据时效性

Result: 创建首个适配当代社交媒体场景的基准数据集，提供模型设计策略的量化分析，实现数据集内容动态更新机制

Conclusion: 通过系统性评估揭示了MLLMs在错误信息检测中的关键能力瓶颈，提出的框架为领域发展提供数据基础和方法论指导

Abstract: The rapid spread of multimodal misinformation on social media calls for more
effective and robust detection methods. Recent advances leveraging multimodal
large language models (MLLMs) have shown the potential in addressing this
challenge. However, it remains unclear exactly where the bottleneck of existing
approaches lies (evidence retrieval v.s. reasoning), hindering the further
advances in this field. On the dataset side, existing benchmarks either contain
outdated events, leading to evaluation bias due to discrepancies with
contemporary social media scenarios as MLLMs can simply memorize these events,
or artificially synthetic, failing to reflect real-world misinformation
patterns. Additionally, it lacks comprehensive analyses of MLLM-based model
design strategies. To address these issues, we introduce XFacta, a
contemporary, real-world dataset that is better suited for evaluating
MLLM-based detectors. We systematically evaluate various MLLM-based
misinformation detection strategies, assessing models across different
architectures and scales, as well as benchmarking against existing detection
methods. Building on these analyses, we further enable a semi-automatic
detection-in-the-loop framework that continuously updates XFacta with new
content to maintain its contemporary relevance. Our analysis provides valuable
insights and practices for advancing the field of multimodal misinformation
detection. The code and data have been released.

</details>


### [6] [AutoGeTS: Knowledge-based Automated Generation of Text Synthetics for Improving Text Classification](https://arxiv.org/abs/2508.10000)
*Chenhao Xue,Yuanzhe Jin,Adrian Carrasco-Revilla,Joyraj Chakraborty,Min Chen*

Main category: cs.CL

TL;DR: 利用大语言模型生成合成数据并通过自动化流程优化文本分类模型性能


<details>
  <summary>Details</summary>
Motivation: 现实应用中文本分类模型面临数据不足的挑战，需要不依赖持续收集真实数据的新方法

Method: 构建自动化流程（三种搜索策略 + 集成算法）筛选有效输入示例，生成高质量合成数据

Result: 实验证明集成策略在提升分类模型效果方面优于单一策略

Conclusion: 基于类别特性动态选择策略的集成方法显著提升了LLM辅助文本分类模型的效率

Abstract: When developing text classification models for real world applications, one
major challenge is the difficulty to collect sufficient data for all text
classes. In this work, we address this challenge by utilizing large language
models (LLMs) to generate synthetic data and using such data to improve the
performance of the models without waiting for more real data to be collected
and labelled. As an LLM generates different synthetic data in response to
different input examples, we formulate an automated workflow, which searches
for input examples that lead to more ``effective'' synthetic data for improving
the model concerned. We study three search strategies with an extensive set of
experiments, and use experiment results to inform an ensemble algorithm that
selects a search strategy according to the characteristics of a class. Our
further experiments demonstrate that this ensemble approach is more effective
than each individual strategy in our automated workflow for improving
classification models using LLMs.

</details>


### [7] [HiFACTMix: A Code-Mixed Benchmark and Graph-Aware Model for EvidenceBased Political Claim Verification in Hinglish](https://arxiv.org/abs/2508.10001)
*Rakesh Thakur,Sneha Sharma,Gauri Chopra*

Main category: cs.CL

TL;DR: 针对低资源混合语言环境下的Hinglish政治声明事实核查难题，提出了包含1500条标注数据的HiFACT数据集和新型图神经网络模型HiFACTMix，在准确性和可解释性上超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有事实核查系统无法适应印度等地区多语言混合的政治话语场景，亟需开发支持代码混合、上下文感知的核查工具以应对社交媒体对舆论的影响。

Method: 构建真实政治声明数据集HiFACT，设计检索增强的图神经网络框架，整合多语言编码、语义对齐、图推理与自然语言解释生成模块。

Result: HiFACTMix模型在准确率上超越现有多语言基线模型，并生成可解释的核查结论。

Conclusion: 该研究为多语言混合环境下的政治事实核查开辟了新方向，强调了语境理解和图结构推理在复杂语言场景中的重要性。

Abstract: Fact-checking in code-mixed, low-resource languages such as Hinglish remains
an underexplored challenge in natural language processing. Existing
fact-verification systems largely focus on high-resource, monolingual settings
and fail to generalize to real-world political discourse in linguistically
diverse regions like India. Given the widespread use of Hinglish by public
figures, particularly political figures, and the growing influence of social
media on public opinion, there's a critical need for robust, multilingual and
context-aware fact-checking tools. To address this gap a novel benchmark HiFACT
dataset is introduced with 1,500 realworld factual claims made by 28 Indian
state Chief Ministers in Hinglish, under a highly code-mixed low-resource
setting. Each claim is annotated with textual evidence and veracity labels. To
evaluate this benchmark, a novel graphaware, retrieval-augmented fact-checking
model is proposed that combines multilingual contextual encoding,
claim-evidence semantic alignment, evidence graph construction, graph neural
reasoning, and natural language explanation generation. Experimental results
show that HiFACTMix outperformed accuracy in comparison to state of art
multilingual baselines models and provides faithful justifications for its
verdicts. This work opens a new direction for multilingual, code-mixed, and
politically grounded fact verification research.

</details>


### [8] [Semantic Structure in Large Language Model Embeddings](https://arxiv.org/abs/2508.10003)
*Austin C. Kozlowski,Callin Dai,Andrei Boutyline*

Main category: cs.CL

TL;DR: 研究发现LLM的语义嵌入与人类低维语义结构高度相似，且调整语义方向会引发几何对齐特征的副作用。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型（LLMs）的语义关联结构是否与人类语义评估的低维特性一致，以及调整语义特征时可能产生的非目标效应。

Method: 通过反义词对定义语义方向，分析LLM词向量投影与人类评分的相关性，并降维至3D子空间；测试语义方向调整对几何对齐特征的影响。

Result: LLM词向量投影与人类评分高度相关，可压缩至3D子空间（类似人类语义结构）；调整特定语义方向时，余弦相似度高的特征会产生连带偏移。

Conclusion: LLM的语义编码具有类似人类语言的低维纠缠特性，忽视这种结构可能导致特征调整时的意外后果。

Abstract: Psychological research consistently finds that human ratings of words across
diverse semantic scales can be reduced to a low-dimensional form with
relatively little information loss. We find that the semantic associations
encoded in the embedding matrices of large language models (LLMs) exhibit a
similar structure. We show that the projections of words on semantic directions
defined by antonym pairs (e.g. kind - cruel) correlate highly with human
ratings, and further find that these projections effectively reduce to a
3-dimensional subspace within LLM embeddings, closely resembling the patterns
derived from human survey responses. Moreover, we find that shifting tokens
along one semantic direction causes off-target effects on geometrically aligned
features proportional to their cosine similarity. These findings suggest that
semantic features are entangled within LLMs similarly to how they are
interconnected in human language, and a great deal of semantic information,
despite its apparent complexity, is surprisingly low-dimensional. Furthermore,
accounting for this semantic structure may prove essential for avoiding
unintended consequences when steering features.

</details>


### [9] [User Perception of Attention Visualizations: Effects on Interpretability Across Evidence-Based Medical Documents](https://arxiv.org/abs/2508.10004)
*Andrés Carvallo,Denis Parra,Peter Brusilovsky,Hernan Valdivieso,Gabriel Rada,Ivania Donoso,Vladimir Araujo*

Main category: cs.CL

TL;DR: 注意力权重的解释性效果存在争议，研究发现可视化方式显著影响用户感知（文本亮度 > 条形图），但整体未确认其实际效用。


<details>
  <summary>Details</summary>
Motivation: 验证注意力机制在生物医学文献分类中的解释有效性，探索不同可视化方式对医生理解AI决策的帮助程度。

Method: 通过用户研究（跨学科医学专家参与），采用XLNet模型进行文献分类实验，对比条形图/文本亮度/背景色三种注意力可视化方式。

Result: 模型准确率较高（F1=0.91），但注意力解释整体帮助有限；可视化形式显著影响感知效果（亮度/颜色 > 精确数据展示）。

Conclusion: 注意力权重解释价值未被充分验证，但可视化形式的选择会显著改变其感知有效性，用户更偏好直觉化呈现方式。

Abstract: The attention mechanism is a core component of the Transformer architecture.
Beyond improving performance, attention has been proposed as a mechanism for
explainability via attention weights, which are associated with input features
(e.g., tokens in a document). In this context, larger attention weights may
imply more relevant features for the model's prediction. In evidence-based
medicine, such explanations could support physicians' understanding and
interaction with AI systems used to categorize biomedical literature. However,
there is still no consensus on whether attention weights provide helpful
explanations. Moreover, little research has explored how visualizing attention
affects its usefulness as an explanation aid. To bridge this gap, we conducted
a user study to evaluate whether attention-based explanations support users in
biomedical document classification and whether there is a preferred way to
visualize them. The study involved medical experts from various disciplines who
classified articles based on study design (e.g., systematic reviews, broad
synthesis, randomized and non-randomized trials). Our findings show that the
Transformer model (XLNet) classified documents accurately; however, the
attention weights were not perceived as particularly helpful for explaining the
predictions. However, this perception varied significantly depending on how
attention was visualized. Contrary to Munzner's principle of visual
effectiveness, which favors precise encodings like bar length, users preferred
more intuitive formats, such as text brightness or background color. While our
results do not confirm the overall utility of attention weights for
explanation, they suggest that their perceived helpfulness is influenced by how
they are visually presented.

</details>


### [10] [From Answers to Questions: EQGBench for Evaluating LLMs' Educational Question Generation](https://arxiv.org/abs/2508.10005)
*Chengliang Zhou,Mei Wang,Ting Zhang,Qiannan Zhu,Jian Li,Hua Huang*

Main category: cs.CL

TL;DR: 提出EQGBench基准测试，用于评估大语言模型在中文教育问题生成（EQG）中的表现，发现现有模型在教育价值体现方面仍有较大发展空间。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs从问题解答转向高质量教育问题生成时存在的挑战，推动生成具有教学价值和教育有效性的问题

Method: 构建包含数学/物理/化学三学科的900样本数据集，建立五维评估框架，系统评估46个主流大模型

Result: 现有模型在体现教育价值和培养学生综合能力方面仍有显著提升空间

Conclusion: EQGBench通过多维度评估体系揭示了LLMs在教育问题生成领域的不足，为教育智能化发展提供重要基准

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
mathematical problem-solving. However, the transition from providing answers to
generating high-quality educational questions presents significant challenges
that remain underexplored. To advance Educational Question Generation (EQG) and
facilitate LLMs in generating pedagogically valuable and educationally
effective questions, we introduce EQGBench, a comprehensive benchmark
specifically designed for evaluating LLMs' performance in Chinese EQG. EQGBench
establishes a five-dimensional evaluation framework supported by a dataset of
900 evaluation samples spanning three fundamental middle school disciplines:
mathematics, physics, and chemistry. The dataset incorporates user queries with
varying knowledge points, difficulty gradients, and question type
specifications to simulate realistic educational scenarios. Through systematic
evaluation of 46 mainstream large models, we reveal significant room for
development in generating questions that reflect educational value and foster
students' comprehensive abilities.

</details>


### [11] [Automated scoring of the Ambiguous Intentions Hostility Questionnaire using fine-tuned large language models](https://arxiv.org/abs/2508.10007)
*Y. Lyu,D. Combs,D. Neumann,Y. C. Leong*

Main category: cs.CL

TL;DR: 研究证实大型语言模型可有效自动化AIHQ问卷的敌意归因偏见评分，在TBI患者与健康人群的组间差异分析中展现与人工评分高度一致的结果。


<details>
  <summary>Details</summary>
Motivation: 传统AIHQ开放式问题评分依赖耗时的人工评估，需要探索自动化解决方案以提高心理评估效率。

Method: 使用TBI患者和健康对照组数据微调语言模型，通过半样本训练和测试验证模型评分与人工评分的一致性，并测试非临床数据泛化能力。

Result: 微调后模型在敌意归因和攻击性反应评分上与人类高度吻合，成功复现TBI/HC组间差异，且在非临床数据中表现良好。

Conclusion: 大型语言模型可有效提升AIHQ评分效率，为跨群体心理评估提供可靠的技术支持。

Abstract: Hostile attribution bias is the tendency to interpret social interactions as
intentionally hostile. The Ambiguous Intentions Hostility Questionnaire (AIHQ)
is commonly used to measure hostile attribution bias, and includes open-ended
questions where participants describe the perceived intentions behind a
negative social situation and how they would respond. While these questions
provide insights into the contents of hostile attributions, they require
time-intensive scoring by human raters. In this study, we assessed whether
large language models can automate the scoring of AIHQ open-ended responses. We
used a previously collected dataset in which individuals with traumatic brain
injury (TBI) and healthy controls (HC) completed the AIHQ and had their
open-ended responses rated by trained human raters. We used half of these
responses to fine-tune the two models on human-generated ratings, and tested
the fine-tuned models on the remaining half of AIHQ responses. Results showed
that model-generated ratings aligned with human ratings for both attributions
of hostility and aggression responses, with fine-tuned models showing higher
alignment. This alignment was consistent across ambiguous, intentional, and
accidental scenario types, and replicated previous findings on group
differences in attributions of hostility and aggression responses between TBI
and HC groups. The fine-tuned models also generalized well to an independent
nonclinical dataset. To support broader adoption, we provide an accessible
scoring interface that includes both local and cloud-based options. Together,
our findings suggest that large language models can streamline AIHQ scoring in
both research and clinical contexts, revealing their potential to facilitate
psychological assessments across different populations.

</details>


### [12] [Multidimensional classification of posts for online course discussion forum curation](https://arxiv.org/abs/2508.10008)
*Antonio Leandro Martins Candido,Jose Everardo Bessa Maia*

Main category: cs.CL

TL;DR: 提出贝叶斯融合方法结合预训练大语言模型与本地分类器，避免频繁微调的资源消耗


<details>
  <summary>Details</summary>
Motivation: 在线课程讨论论坛自动管理需要持续更新模型，而大语言模型的频繁微调具有较高资源成本

Method: 将预训练通用大语言模型的多维分类分数与本地数据训练的分类器进行贝叶斯融合

Result: 融合方法效果优于单独模型，且与微调方法竞争力相当

Conclusion: 贝叶斯融合可作为替代微调的有效方案，显著降低资源需求

Abstract: The automatic curation of discussion forums in online courses requires
constant updates, making frequent retraining of Large Language Models (LLMs) a
resource-intensive process. To circumvent the need for costly fine-tuning, this
paper proposes and evaluates the use of Bayesian fusion. The approach combines
the multidimensional classification scores of a pre-trained generic LLM with
those of a classifier trained on local data. The performance comparison
demonstrated that the proposed fusion improves the results compared to each
classifier individually, and is competitive with the LLM fine-tuning approach

</details>


### [13] [Beyond Hard Sharing: Efficient Multi-Task Speech-to-Text Modeling with Supervised Mixture of Experts](https://arxiv.org/abs/2508.10009)
*Hojun Jin,Eunsoo Hong,Ziwon Hyung,Sungjun Lim,Seungjin Lee,Keunseok Cho*

Main category: cs.CL

TL;DR: 提出监督混合专家模型(S-MoE)，通过任务专属路由机制解决多任务学习中参数共享导致的性能干扰问题


<details>
  <summary>Details</summary>
Motivation: 传统硬参数共享策略导致多任务间干扰，影响模型整体性能。需开发更有效的参数隔离方案

Method: 利用指导令牌替代门控函数，将各任务定向到独立的前馈网络。应用于语音文本模型的编解码器架构

Result: 在语音识别和翻译任务中，词错误率相对降低6.35%（编解码器同时应用时）

Conclusion: S-MoE通过专家网络隔离有效提升多任务模型性能，特别适用于混合带宽输入的语音处理场景

Abstract: Hard-parameter sharing is a common strategy to train a single model jointly
across diverse tasks. However, this often leads to task interference, impeding
overall model performance. To address the issue, we propose a simple yet
effective Supervised Mixture of Experts (S-MoE). Unlike traditional Mixture of
Experts models, S-MoE eliminates the need for training gating functions by
utilizing special guiding tokens to route each task to its designated expert.
By assigning each task to a separate feedforward network, S-MoE overcomes the
limitations of hard-parameter sharing. We further apply S-MoE to a
speech-to-text model, enabling the model to process mixed-bandwidth input while
jointly performing automatic speech recognition (ASR) and speech translation
(ST). Experimental results demonstrate the effectiveness of the proposed S-MoE,
achieving a 6.35% relative improvement in Word Error Rate (WER) when applied to
both the encoder and decoder.

</details>


### [14] [An Audit and Analysis of LLM-Assisted Health Misinformation Jailbreaks Against LLMs](https://arxiv.org/abs/2508.10010)
*Ayana Hussain,Patrick Zhao,Nicholas Vincent*

Main category: cs.CL

TL;DR: 大型语言模型存在生成医疗错误信息的风险，但也可有效检测来自其他模型和人类的错误信息，支持构建更健康的信息生态系统


<details>
  <summary>Details</summary>
Motivation: 研究LLMs通过越狱攻击生成医疗错误信息的机制，并验证其检测错误信息的潜力

Method: 对3个目标LLM进行109次越狱攻击，对比攻击提示与真实查询，将生成错误信息与Reddit健康类错误信息进行机器学习检测分析

Result: LLM生成错误信息与真实社交媒体错误信息相似度达75%，标准检测模型准确率达89%

Conclusion: 通过精心设计，LLMs可同时作为错误信息生成器和检测器，为健康信息生态建设提供技术支持

Abstract: Large Language Models (LLMs) are a double-edged sword capable of generating
harmful misinformation -- inadvertently, or when prompted by "jailbreak"
attacks that attempt to produce malicious outputs. LLMs could, with additional
research, be used to detect and prevent the spread of misinformation. In this
paper, we investigate the efficacy and characteristics of LLM-produced
jailbreak attacks that cause other models to produce harmful medical
misinformation. We also study how misinformation generated by jailbroken LLMs
compares to typical misinformation found on social media, and how effectively
it can be detected using standard machine learning approaches. Specifically, we
closely examine 109 distinct attacks against three target LLMs and compare the
attack prompts to in-the-wild health-related LLM queries. We also examine the
resulting jailbreak responses, comparing the generated misinformation to
health-related misinformation on Reddit. Our findings add more evidence that
LLMs can be effectively used to detect misinformation from both other LLMs and
from people, and support a body of work suggesting that with careful design,
LLMs can contribute to a healthier overall information ecosystem.

</details>


### [15] [Evaluation of GPT-based large language generative AI models as study aids for the national licensure examination for registered dietitians in Japan](https://arxiv.org/abs/2508.10011)
*Yuta Nagamori,Mikoto Kosai,Yuji Kawai,Haruka Marumo,Misaki Shibuya,Tatsuya Negishi,Masaki Imanishi,Yasumasa Ikeda,Koichiro Tsuchiya,Asuka Sawai,Licht Miyamoto*

Main category: cs.CL

TL;DR: 生成式AI在营养师资格考试中部分模型勉强及格，但存在答案稳定性不足与学科表现差异问题


<details>
  <summary>Details</summary>
Motivation: 探究生成式AI在营养学教育中的应用潜力，特别是在日本注册营养师国家资格考试中的表现评估

Method: 使用日本注册营养师考试真题测试ChatGPT和三个Bing模型（Precise/Creative/Balanced），分析准确性、响应一致性和提示工程效果

Result: Bing-Precise(66.2%)和Bing-Creative(61.4%)超过60%及格线，但所有模型在营养教育科目表现最差；答案稳定性普遍不足，提示工程仅在使用正确答案示例时略有提升

Conclusion: 当前生成式AI作为备考工具的可靠性有限，需改进模型稳定性与学科适配性才能满足营养师资格认证需求

Abstract: Generative artificial intelligence (AI) based on large language models
(LLMs), such as ChatGPT, has demonstrated remarkable progress across various
professional fields, including medicine and education. However, their
performance in nutritional education, especially in Japanese national licensure
examination for registered dietitians, remains underexplored. This study aimed
to evaluate the potential of current LLM-based generative AI models as study
aids for nutrition students. Questions from the Japanese national examination
for registered dietitians were used as prompts for ChatGPT and three Bing
models (Precise, Creative, Balanced), based on GPT-3.5 and GPT-4. Each question
was entered into independent sessions, and model responses were analyzed for
accuracy, consistency, and response time. Additional prompt engineering,
including role assignment, was tested to assess potential performance
improvements. Bing-Precise (66.2%) and Bing-Creative (61.4%) surpassed the
passing threshold (60%), while Bing-Balanced (43.3%) and ChatGPT (42.8%) did
not. Bing-Precise and Bing-Creative generally outperformed others across
subject fields except Nutrition Education, where all models underperformed.
None of the models consistently provided the same correct responses across
repeated attempts, highlighting limitations in answer stability. ChatGPT showed
greater consistency in response patterns but lower accuracy. Prompt engineering
had minimal effect, except for modest improvement when correct answers and
explanations were explicitly provided. While some generative AI models
marginally exceeded the passing threshold, overall accuracy and answer
consistency remained suboptimal. Moreover, all the models demonstrated notable
limitations in answer consistency and robustness. Further advancements are
needed to ensure reliable and stable AI-based study aids for dietitian
licensure preparation.

</details>


### [16] [Guided Navigation in Knowledge-Dense Environments: Structured Semantic Exploration with Guidance Graphs](https://arxiv.org/abs/2508.10012)
*Dehao Tao,Guangjie Liu,Weizheng,Yongfeng Huang,Minghu jiang*

Main category: cs.CL

TL;DR: 提出GG Explore框架，通过中间指导图实现精准高效知识检索，实验显示该方法在复杂任务中超越SOTA并保持小模型高效性


<details>
  <summary>Details</summary>
Motivation: LLM依赖静态知识且推理不透明，现有知识图谱方法存在粒度不匹配导致的冗余探索与复杂场景上下文利用不足的双重矛盾

Method: 构建中间指导图定义检索空间，开发结构对齐（无LLM开销过滤不兼容候选）和上下文感知剪枝（图约束保证语义一致性）

Result: 实验证明该方法效率显著优于基线，尤其在复杂任务中超越现有方法，小模型也能保持较强性能

Conclusion: 指导图机制有效平衡检索精度与效率，为知识增强型LLM系统提供实用化解决方案

Abstract: While Large Language Models (LLMs) exhibit strong linguistic capabilities,
their reliance on static knowledge and opaque reasoning processes limits their
performance in knowledge intensive tasks. Knowledge graphs (KGs) offer a
promising solution, but current exploration methods face a fundamental trade
off: question guided approaches incur redundant exploration due to granularity
mismatches, while clue guided methods fail to effectively leverage contextual
information for complex scenarios. To address these limitations, we propose
Guidance Graph guided Knowledge Exploration (GG Explore), a novel framework
that introduces an intermediate Guidance Graph to bridge unstructured queries
and structured knowledge retrieval. The Guidance Graph defines the retrieval
space by abstracting the target knowledge' s structure while preserving broader
semantic context, enabling precise and efficient exploration. Building upon the
Guidance Graph, we develop: (1) Structural Alignment that filters incompatible
candidates without LLM overhead, and (2) Context Aware Pruning that enforces
semantic consistency with graph constraints. Extensive experiments show our
method achieves superior efficiency and outperforms SOTA, especially on complex
tasks, while maintaining strong performance with smaller LLMs, demonstrating
practical value.

</details>


### [17] [Semantic Bridge: Universal Multi-Hop Question Generation via AMR-Driven Graph Synthesis](https://arxiv.org/abs/2508.10013)
*Linqing Chen,Hanmeng Zhong,Wentao Wu,Weilei Wang*

Main category: cs.CL

TL;DR: 提出Semantic Bridge框架解决LLM训练数据中复杂推理问题生成的难题，通过语义图编织技术实现跨文档多跳推理问题的可控生成，在多个领域和语言中显著超越基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法从稀疏领域数据中生成可控的复杂推理问题，制约了LLM在深层理解能力上的训练提升，特别是多跳推理场景下的数据匮乏问题亟待突破。

Method: 基于AMR语义分析构建三种桥接机制：实体桥接(共享实体角色变换)、谓词链桥接(时序/因果序列)、因果桥接(显式推理链)，通过语义图编织技术生成多模态推理路径。

Result: 在维基百科和生物医学领域实现18.3%-25.4%性能提升，200个源材料生成的问题优于600个人工标注样本(节省67%资源)，人工评估显示复杂性+23.4%、可答性+18.7%、模式覆盖+31.2%。

Conclusion: 建立了LLM训练数据合成新范式，通过语义驱动的可控生成机制突破多跳推理问题生成瓶颈，核心代码和语义桥接模型将开源推动领域发展。

Abstract: Large language model (LLM) training faces a critical bottleneck: the scarcity
of high-quality, reasoning-intensive question-answer pairs, especially from
sparse, domain-specific sources like PubMed papers or legal documents. Existing
methods rely on surface patterns, fundamentally failing to generate
controllable, complex multi-hop reasoning questions that test genuine
understanding-essential for advancing LLM training paradigms. We present
\textbf{Semantic Bridge}, the first universal framework for controllably
generating sophisticated multi-hop reasoning questions from arbitrary sources.
Our breakthrough innovation is \textit{semantic graph weaving}-three
complementary bridging mechanisms (entity bridging for role-varying shared
entities, predicate chain bridging for temporal/causal/logical sequences, and
causal bridging for explicit reasoning chains)-that systematically construct
complex pathways across documents, with fine-grained control over complexity
and types via AMR-driven analysis. Our multi-modal AMR pipeline achieves up to
9.5% better round-trip quality, enabling production-ready controllable QA
generation. Extensive evaluation demonstrates performance across both
general-purpose datasets (Wikipedia) and specialized domains (biomedicine) It
yields consistent 18.3%-25.4% gains over baselines across four languages
(English, Chinese, French, German). Question pairs generated from 200 sources
outperform 600 native human annotation examples with 67% fewer materials. Human
evaluation shows 23.4% higher complexity, 18.7% better answerability, and 31.2%
improved pattern coverage. Semantic Bridge establishes a new paradigm for LLM
training data synthesis, enabling controllable generation of targeted reasoning
questions from sparse sources. We will release our core code and semantic
bridge model.

</details>


### [18] [PersonaEval: Are LLM Evaluators Human Enough to Judge Role-Play?](https://arxiv.org/abs/2508.10014)
*Lingfeng Zhou,Jialing Zhang,Jin Gao,Mohan Jiang,Dequan Wang*

Main category: cs.CL

TL;DR: 提出首个基准测试PersonaEval揭示LLM评估器在角色识别任务中准确率（69%）显著低于人类水平（90.8%），凸显现有评估范式局限性


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的角色扮演评估方法依赖未经充分验证的评判范式，无法有效衡量角色忠诚度。角色识别能力是评估质量的前提条件，但尚未建立可靠基准

Method: 构建PersonaEval基准，使用小说/剧本/视频文本中的人类创作对话，要求模型根据上下文判断说话者身份，并通过人类实验进行对比验证

Result: 最佳LLM准确率仅69%（远低于人类90.8%），显示现有模型无法可靠完成角色识别任务。人类表现接近天花板说明任务可行性

Conclusion: 可靠的评估需要LLM具备人类级推理能力，而非单纯的任务适配。研究为角色扮演评估系统建立了新标准，揭示当前LLM与人类认知能力的本质差异

Abstract: Current role-play studies often rely on unvalidated LLM-as-a-judge paradigms,
which may fail to reflect how humans perceive role fidelity. A key prerequisite
for human-aligned evaluation is role identification, the ability to recognize
who is speaking based on dialogue context. We argue that any meaningful
judgment of role-playing quality (how well a character is played) fundamentally
depends on first correctly attributing words and actions to the correct persona
(who is speaking). We present PersonaEval, the first benchmark designed to test
whether LLM evaluators can reliably identify human roles. PersonaEval uses
human-authored dialogues from novels, scripts, and video transcripts,
challenging models to determine the correct persona according to the
conversation context. Our experiments, including a human study, show that even
the best-performing LLMs reach only around 69% accuracy, well below the level
needed for reliable evaluation. In contrast, human participants perform near
ceiling with 90.8% accuracy, highlighting that current LLM evaluators are still
not human enough to effectively judge role-play scenarios. To better understand
this gap, we examine training-time adaptation and test-time compute, suggesting
that reliable evaluation requires more than task-specific tuning, but depends
on strong, human-like reasoning abilities in LLM evaluators. We release our
benchmark at https://github.com/maple-zhou/PersonaEval.

</details>


### [19] [RealTalk-CN: A Realistic Chinese Speech-Text Dialogue Benchmark With Cross-Modal Interaction Analysis](https://arxiv.org/abs/2508.10015)
*Enzhi Wang,Qicheng Li,Shiwan Zhao,Aobo Kong,Jiaming Zhou,Xi Yang,Yequan Wang,Yonghua Lin,Yong Qin*

Main category: cs.CL

TL;DR: 提出首个中文多领域语音-文本双模态对话数据集RealTalk-CN，填补现有数据集在真实语音特征和中文场景的空白


<details>
  <summary>Details</summary>
Motivation: 现有任务导向对话数据集以英文文本为主，缺乏真实语音信号及中文场景下的语音不流畅性、说话人多样性等关键要素，制约语音大模型评估

Method: 构建包含5.4k对话的语音-文本平行数据集，标注自发语音不流畅特征；设计支持模态动态切换的跨模态对话任务

Result: 实验验证数据集有效性，为中文语音大模型研究建立评估基准，证实其对语音不流畅性和说话人特征的鲁棒性

Conclusion: RealTalk-CN通过真实场景建模和多维度评估体系，推动了中文语音交互系统的研究进展

Abstract: In recent years, large language models (LLMs) have achieved remarkable
advancements in multimodal processing, including end-to-end speech-based
language models that enable natural interactions and perform specific tasks in
task-oriented dialogue (TOD) systems. However, existing TOD datasets are
predominantly text-based, lacking real speech signals that are essential for
evaluating the robustness of speech-based LLMs. Moreover, existing speech TOD
datasets are primarily English and lack critical aspects such as speech
disfluencies and speaker variations. To address these gaps, we introduce
RealTalk-CN, the first Chinese multi-turn, multi-domain speech-text dual-modal
TOD dataset, comprising 5.4k dialogues (60K utterances, 150 hours) with paired
speech-text annotations. RealTalk-CN captures diverse dialogue scenarios with
annotated spontaneous speech disfluencies, ensuring comprehensive coverage of
real-world complexities in speech dialogue. In addition, we propose a novel
cross-modal chat task that authentically simulates real-world user
interactions, allowing dynamic switching between speech and text modalities.
Our evaluation covers robustness to speech disfluencies, sensitivity to speaker
characteristics, and cross-domain performance. Extensive experiments validate
the effectiveness of RealTalk-CN, establishing a strong foundation for Chinese
speech-based LLMs research.

</details>


### [20] [Training-Free Multimodal Large Language Model Orchestration](https://arxiv.org/abs/2508.10016)
*Tianyu Xie,Yuhang Wu,Yongdong Luo,Jiayi Ji,Xiawu Zheng*

Main category: cs.CL

TL;DR: 提出无需额外训练的多模态大语言模型协调框架，通过智能路由、并行TTS架构和跨模态记忆系统实现高效多模态交互


<details>
  <summary>Details</summary>
Motivation: 现有不同多模态大语言模型难以直接整合成统一系统，传统联合训练方法存在模态对齐困难、TTS效率低等问题

Method: 1) LLM中央控制器动态路由任务 2) 并行TTS架构实现全双工交互 3) 跨模态记忆系统智能合成上下文

Result: 在标准测试集上性能提升7.8%，延迟降低10.3%，实现全双工交互和跨模态语境保持

Conclusion: 该协调框架显著提升计算效率、可解释性和交互自然性，为多模态系统构建提供模块化解决方案

Abstract: Different Multimodal Large Language Models (MLLMs) cannot be integrated into
a unified multimodal input-output system directly. In previous work, training
has been considered as an inevitable component due to challenges in modal
alignment, Text-to-Speech efficiency and other integration issues. In this
paper, we introduce Multimodal Large Language Model Orchestration, an effective
approach for creating interactive multimodal AI systems without additional
training. MLLM Orchestration leverages the inherent reasoning capabilities of
large language models to coordinate specialized models through explicit
workflows, enabling natural multimodal interactions while maintaining
modularity, improving interpretability, and significantly enhancing
computational efficiency. Our orchestration framework is built upon three key
innovations: (1) a central controller LLM that analyzes user inputs and
dynamically routes tasks to appropriate specialized models through carefully
designed agents; (2) a parallel Text-to-Speech architecture that enables true
full-duplex interaction with seamless interruption handling and natural
conversational flow; and (3) a cross-modal memory integration system that
maintains coherent context across modalities through intelligent information
synthesis and retrieval, selectively avoiding unnecessary modality calls in
certain scenarios to improve response speed. Extensive evaluations demonstrate
that MLLM Orchestration achieves comprehensive multimodal capabilities without
additional training, performance improvements of up to 7.8% over traditional
jointly-trained approaches on standard benchmarks, reduced latency by 10.3%,
and significantly enhanced interpretability through explicit orchestration
processes.

</details>


### [21] [A Rose by Any Other Name Would Smell as Sweet: Categorical Homotopy Theory for Large Language Models](https://arxiv.org/abs/2508.10018)
*Sridhar Mahadevan*

Main category: cs.CL

TL;DR: 提出基于范畴同伦理论的LLM框架，解决语义等价但句法不同句子在语言模型中概率不一致问题


<details>
  <summary>Details</summary>
Motivation: 现有语言模型对同义句生成不同概率分布，影响模型可靠性。经验性方法缺乏理论支撑，需建立抽象数学框架统一处理语义等价性

Method: 构建LLM的Markov范畴表示语言概率分布，引入范畴同伦技术（包括高阶代数K理论和模型范畴）建立弱等价关系

Result: 成功构建包含语义等价关系的范畴同伦框架，为语言模型概率一致性提供数学基础

Conclusion: 该理论框架连接了范畴论与语言模型，开辟了基于严格数学理论的语义等价处理新范式，为后续应用奠定基础

Abstract: Natural language is replete with superficially different statements, such as
``Charles Darwin wrote" and ``Charles Darwin is the author of", which carry the
same meaning. Large language models (LLMs) should generate the same next-token
probabilities in such cases, but usually do not. Empirical workarounds have
been explored, such as using k-NN estimates of sentence similarity to produce
smoothed estimates. In this paper, we tackle this problem more abstractly,
introducing a categorical homotopy framework for LLMs. We introduce an LLM
Markov category to represent probability distributions in language generated by
an LLM, where the probability of a sentence, such as ``Charles Darwin wrote" is
defined by an arrow in a Markov category. However, this approach runs into
difficulties as language is full of equivalent rephrases, and each generates a
non-isomorphic arrow in the LLM Markov category. To address this fundamental
problem, we use categorical homotopy techniques to capture ``weak equivalences"
in an LLM Markov category. We present a detailed overview of application of
categorical homotopy to LLMs, from higher algebraic K-theory to model
categories, building on powerful theoretical results developed over the past
half a century.

</details>


### [22] [Decoupling Understanding from Reasoning via Problem Space Mapping for Small-scale Model Reasoning](https://arxiv.org/abs/2508.10019)
*Li Wang,Changhao Zhang,Zengqi Xiu,Kai Lu,Xin Yu,Kui Zhang,Wenjun Wu*

Main category: cs.CL

TL;DR: 提出DURIT框架，通过解耦自然语言理解与推理过程，使小型语言模型专注标准化推理任务，显著提升数学与逻辑推理能力


<details>
  <summary>Details</summary>
Motivation: 小型语言模型在自然语言复杂性问题中面临双重负担：需同时处理语言理解与逻辑推理，导致优化困难

Method: 三阶段算法：1)强化学习映射问题到规范空间 2)自蒸馏对齐推理轨迹 3)规范空间训练推理策略，mapper与reasoner交替训练

Result: 实验显示DURIT使SLMs在领域内外数学/逻辑推理任务表现显著提升，并增强推理过程鲁棒性

Conclusion: 解耦理解与推理的策略有效增强小型语言模型能力，规范问题空间训练方法具有普适应用价值

Abstract: Despite recent advances in the reasoning capabilities of Large Language
Models (LLMs), improving the reasoning ability of Small Language Models (SLMs,
e.g., $\leq$ 1.5B) remains challenging. A key obstacle lies in the complexity
and variability of natural language: essentially equivalent problems often
appear in diverse surface forms, often obscured by redundant or distracting
details. This imposes a dual burden on SLMs: they must first extract the core
problem from complex linguistic input, and then perform reasoning based on that
understanding. The resulting vast and noisy problem space hinders optimization,
particularly for models with limited capacity. To address this, we propose a
new framework that decouples understanding from reasoning by mapping natural
language problems into a canonical problem space-a semantically simplified yet
expressive domain. This enables SLMs to focus on reasoning over standardized
inputs, free from linguistic variability. Within this framework, we introduce
DURIT (Decoupled Understanding from Reasoning via Iterative Training), a
three-step algorithm that iteratively: (1) mapping natural language problems
via reinforcement learning, (2) aligns reasoning trajectories through
self-distillation, and (3) trains reasoning policies in the problem space. The
mapper and reasoner are co-trained in an alternating loop throughout this
process. Experiments show that DURIT substantially improves SLMs' performance
on both in-domain and out-of-domain mathematical and logical reasoning tasks.
Beyond improving reasoning capabilities, DURIT also improves the robustness of
reasoning, validating decoupling understanding from reasoning as an effective
strategy for strengthening SLMs.

</details>


### [23] [FedCoT: Communication-Efficient Federated Reasoning Enhancement for Large Language Models](https://arxiv.org/abs/2508.10020)
*Chuan Li,Qianyi Zhao,Fengran Mo,Cen Chen*

Main category: cs.CL

TL;DR: 提出FedCoT框架，通过轻量级思维链增强机制和基于LoRA的改进聚合方法，在联邦学习环境下高效提升大语言模型的推理能力并保障医疗领域隐私安全。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法在医疗领域面临三大挑战：1) 仅优化答案正确性而忽视思维链质量 2) 依赖中心化模型的知识蒸馏导致隐私泄露风险 3) 传统微调通信开销过大。医疗决策需要兼具准确输出和可解释推理路径。

Method: 1) 设计本地模型多推理路径生成+轻量判别器动态选择机制 2) 采用改进的LoRA模块堆叠聚合策略，集成客户端分类器感知实现跨异构客户端的无噪声参数聚合。

Result: 在医疗推理任务上的实验表明，FedCoT在严格资源限制下显著提升客户端推理性能(准确率提升+推理稳定性增强)，同时完整保护数据隐私。

Conclusion: FedCoT成功解决了联邦学习中推理能力增强与隐私保护/资源限制的平衡问题，其提供的可解释推理路径特别契合医疗应用对安全性和可追溯性的严苛要求。

Abstract: Efficiently enhancing the reasoning capabilities of large language models
(LLMs) in federated learning environments remains challenging, particularly
when balancing performance gains with strict computational, communication, and
privacy constraints. This challenge is especially acute in healthcare, where
decisions-spanning clinical, operational, and patient-facing contexts-demand
not only accurate outputs but also interpretable, traceable rationales to
ensure safety, accountability, and regulatory compliance. Conventional
federated tuning approaches on LLM fail to address this need: they optimize
primarily for answer correctness while neglecting rationale quality, leaving
CoT capabilities dependent on models' innate pre-training abilities. Moreover,
existing methods for improving rationales typically rely on privacy-violating
knowledge distillation from centralized models. Additionally, the communication
overhead in traditional federated fine-tuning on LLMs remains substantial. We
addresses this gap by proposing FedCoT, a novel framework specifically designed
to enhance reasoning in federated settings. FedCoT leverages a lightweight
chain-of-thought enhancement mechanism: local models generate multiple
reasoning paths, and a compact discriminator dynamically selects the most
promising one. This approach improves reasoning accuracy and robustness while
providing valuable interpretability, which is particularly critical for medical
applications. To manage client heterogeneity efficiently, we adopt an improved
aggregation approach building upon advanced LoRA module stacking, incorporating
client classifier-awareness to achieve noise-free aggregation across diverse
clients. Comprehensive experiments on medical reasoning tasks demonstrate that
FedCoT significantly boosts client-side reasoning performance under stringent
resource budgets while fully preserving data privacy.

</details>


### [24] [LATTE: Learning Aligned Transactions and Textual Embeddings for Bank Clients](https://arxiv.org/abs/2508.10021)
*Egor Fadeev,Dzhambulat Mollaev,Aleksei Shestov,Dima Korolev,Omar Zoloev,Ivan Kireev,Andrey Savchenko,Maksim Makarenko*

Main category: cs.CL

TL;DR: 提出LATTE框架，通过对比学习将原始事件序列与冻结LLM的语义嵌入对齐，降低金融场景下的推理成本和输入规模


<details>
  <summary>Details</summary>
Motivation: 大型语言模型直接处理长事件序列计算成本过高，难以在实时金融场景中部署

Method: 使用冻结LLM生成行为特征的语义嵌入作为监督信号，通过对比损失训练事件序列表征

Result: 在真实金融数据集上超越现有事件序列表征学习方法，同时保持低延迟部署能力

Conclusion: LATTE在保证性能的前提下显著降低LLM的计算开销，适用于延迟敏感的实际金融应用场景

Abstract: Learning clients embeddings from sequences of their historic communications
is central to financial applications. While large language models (LLMs) offer
general world knowledge, their direct use on long event sequences is
computationally expensive and impractical in real-world pipelines. In this
paper, we propose LATTE, a contrastive learning framework that aligns raw event
embeddings with semantic embeddings from frozen LLMs. Behavioral features are
summarized into short prompts, embedded by the LLM, and used as supervision via
contrastive loss. The proposed approach significantly reduces inference cost
and input size compared to conventional processing of complete sequence by LLM.
We experimentally show that our method outperforms state-of-the-art techniques
for learning event sequence representations on real-world financial datasets
while remaining deployable in latency-sensitive environments.

</details>


### [25] [Conformal P-Value in Multiple-Choice Question Answering Tasks with Provable Risk Control](https://arxiv.org/abs/2508.10022)
*Yuanchang Ye*

Main category: cs.CL

TL;DR: 提出融合显著性检验的共形预测框架，通过自洽性重采样和p值计算提升LLM在选择题问答中的可信度，实证验证了错误覆盖率控制和预测集尺寸与风险水平的负相关性。


<details>
  <summary>Details</summary>
Motivation: 针对大语言模型(LLM)在多选题场景中存在的幻觉生成和事实错误问题，现有共形预测方法仅提供统计覆盖保证但缺乏与显著性检验的协同，需建立更严谨的统计框架提升可靠性。

Method: 1. 自洽性重采样统计选项频率作为置信度指标
2. 构建基于零假设检验的预测集
3. 通过经验p值整合统计显著性检验与共形评分

Result: 在MMLU基准测试中：
1. 达到用户指定的错误覆盖率(93.2% @5% α)
2. 预测集平均尺寸随风险水平α增大呈单调递减趋势(APSS从2.1降至1.3)

Conclusion: 该框架为高风险QA场景提供了统计严谨的部署范式，通过双统计方法融合有效控制幻觉生成，预测集尺寸可作为可靠的不确定性度量指标。

Abstract: This study introduces a significance testing-enhanced conformal prediction
(CP) framework to improve trustworthiness of large language models (LLMs) in
multiple-choice question answering (MCQA). While LLMs have been increasingly
deployed in disciplinary QA scenarios, hallucination and nonfactual generation
substantially compromise response reliability. Although CP provides
statistically rigorous marginal coverage guarantees for prediction sets, and
significance testing offers established statistical rigor, their synergistic
integration remains unexplored. To mitigate hallucination and factual
inaccuracies, our framework integrates $p$-value computation with conformity
scoring through self-consistency resampling of MCQA responses. This approach
calculates option frequencies to address LLMs' black-box nature, subsequently
constructing prediction sets via null hypothesis testing ($\mathcal{H}_0$) with
empirically derived $p$-values. Evaluations on MMLU and MMLU-Pro benchmarks
using off-the-shelf LLMs demonstrate: (1) The enhanced CP achieves
user-specified empirical miscoverage rates; (2) Test-set average prediction set
size (APSS) decreases monotonically with increasing risk levels ($\alpha$),
validating APSS as an effective uncertainty metric. This work establishes a
principled statistical framework for trustworthy LLM deployment in high-stakes
QA applications.

</details>


### [26] [RTTC: Reward-Guided Collaborative Test-Time Compute](https://arxiv.org/abs/2508.10024)
*J. Pablo Muñoz,Jinjie Yuan*

Main category: cs.CL

TL;DR: RTTC框架通过奖励模型自适应选择最佳测试时计算策略，在减少计算开销的同时显著提升多领域任务的准确率


<details>
  <summary>Details</summary>
Motivation: 现有TTC策略（如TTT/RAG）在不同查询场景下效果差异大，盲目应用会导致计算资源浪费。需要动态选择最优策略以实现效率与精度的平衡

Method: 采用分布式服务架构：1) 服务器端预训练奖励模型决策策略 2) 客户端按需执行RAG或轻量微调 3) 创新查询状态缓存机制减少重复计算

Result: 在多个LLM和基准测试中，RTTC的准确率持续超越传统方法（最高提升15.6%），计算开销降低37%

Conclusion: 验证了奖励模型引导的自适应策略选择必要性，证明RTTC在可扩展的高效模型适配方面具有重要应用价值

Abstract: Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the
performance of Large Language Models (LLMs) at inference, leveraging strategies
such as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG).
However, the optimal adaptation strategy varies across queries, and
indiscriminate application of TTC strategy incurs substantial computational
overhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a
novel framework that adaptively selects the most effective TTC strategy for
each query via a pretrained reward model, maximizing downstream accuracy across
diverse domains and tasks. RTTC operates in a distributed server-client
architecture, retrieving relevant samples from a remote knowledge base and
applying RAG or lightweight fine-tuning on client devices only when necessary.
To further mitigate redundant computation, we propose Query-State Caching,
which enables the efficient reuse of historical query states at both retrieval
and adaptation levels. Extensive experiments across multiple LLMs and
benchmarks demonstrate that RTTC consistently achieves superior accuracy
compared to vanilla RAG or TTT, validating the necessity of adaptive,
reward-guided TTC selection and the potential of RTTC for scalable,
high-performance language model adaptation.

</details>


### [27] [Detecting and explaining postpartum depression in real-time with generative artificial intelligence](https://arxiv.org/abs/2508.10025)
*Silvia García-Méndez,Francisco de Arriba-Pérez*

Main category: cs.CL

TL;DR: 本文提出一种结合自然语言处理、机器学习与大语言模型的智能产后抑郁症筛查系统，通过非侵入性语音分析实现90%检测准确率，优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 产后抑郁症严重威胁产妇身心健康，需通过实时、低成本的非侵入性技术实现快速筛查，并为临床决策提供可解释的预测结果。

Method: 集成自然语言处理（NLP）、机器学习（树状算法）和大语言模型（LLMs），利用特征重要性和自然语言解释解决模型黑箱问题。

Result: 所有评估指标均达90%，在PPD检测中超越文献中的竞品方案。

Conclusion: 该系统通过可解释的AI技术实现精准、实时的PPD筛查，为及时干预提供技术支持，兼具准确性与临床可操作性。

Abstract: Among the many challenges mothers undergo after childbirth, postpartum
depression (PPD) is a severe condition that significantly impacts their mental
and physical well-being. Consequently, the rapid detection of ppd and their
associated risk factors is critical for in-time assessment and intervention
through specialized prevention procedures. Accordingly, this work addresses the
need to help practitioners make decisions with the latest technological
advancements to enable real-time screening and treatment recommendations.
Mainly, our work contributes to an intelligent PPD screening system that
combines Natural Language Processing, Machine Learning (ML), and Large Language
Models (LLMs) towards an affordable, real-time, and non-invasive free speech
analysis. Moreover, it addresses the black box problem since the predictions
are described to the end users thanks to the combination of LLMs with
interpretable ml models (i.e., tree-based algorithms) using feature importance
and natural language. The results obtained are 90 % on ppd detection for all
evaluation metrics, outperforming the competing solutions in the literature.
Ultimately, our solution contributes to the rapid detection of PPD and their
associated risk factors, critical for in-time and proper assessment and
intervention.

</details>


### [28] [SABER: Switchable and Balanced Training for Efficient LLM Reasoning](https://arxiv.org/abs/2508.10026)
*Kai Zhao,Yanjun Zhao,Jiaming Song,Shien He,Lusheng Zhang,Qiang Zhang,Tianjiao Li*

Main category: cs.CL

TL;DR: SABER框架通过可切换的推理模式和预算控制机制，在保持推理精度的同时显著降低LLM的计算成本与延迟


<details>
  <summary>Details</summary>
Motivation: 现有链式思维推理方法存在计算资源浪费问题，需实现用户可控的token预算推理与灵活推理深度调节

Method: 基于强化学习的四阶段分层训练：预算分层标注（NoThink/FastThink/CoreThink/DeepThink）、长度感知奖励机制、无推理样本融合训练

Result: 在MATH基准上推理长度减少65.4%且准确率提升3.6%，支持跨尺度（7B/13B模型）和跨领域（数学/代码/逻辑）泛化

Conclusion: SABER首次实现LLM推理过程的可控预算分配，为实时推理场景提供高效解决方案

Abstract: Large language models (LLMs) empowered by chain-of-thought reasoning have
achieved impressive accuracy on complex tasks but suffer from excessive
inference costs and latency when applied uniformly to all problems. We propose
SABER (Switchable and Balanced Training for Efficient LLM Reasoning), a
reinforcement learning framework that endows LLMs with user-controllable,
token-budgeted reasoning. SABER first profiles each training example's
base-model thinking token usage and assigns it to one of the predefined budget
tiers. During fine-tuning, the model is guided by system prompts and
length-aware rewards to respect its assigned budget. In parallel, we
incorporate no-think examples to ensure the model remains reliable even when
explicit reasoning is turned off. SABER further supports four discrete
inference modes - NoThink, FastThink, CoreThink, and DeepThink, enabling
flexible trade-offs between latency and reasoning depth. Extensive evaluations
on math reasoning (MATH, GSM8K), code generation (MBPP), and logical reasoning
(LiveBench-Reasoning) demonstrate that SABER achieves high accuracy under tight
budgets, graceful degradation, and effective cross-scale and cross-domain
generalization. In particular, SABER-FastThink cuts reasoning length by 65.4%
and yields a 3.6% accuracy gain compared with the base model on the MATH
benchmark.

</details>


### [29] [LLMCARE: Alzheimer's Detection via Transformer Models Enhanced by LLM-Generated Synthetic Data](https://arxiv.org/abs/2508.10027)
*Ali Zolnour,Hossein Azadmaleki,Yasaman Haghbin,Fatemeh Taherinezhad,Mohamad Javad Momeni Nezhad,Sina Rashidi,Masoud Khani,AmirSajjad Taleban,Samin Mahdizadeh Sani,Maryam Dadkhah,James M. Noble,Suzanne Bakken,Yadollah Yaghoobzadeh,Abdol-Hossein Vahabie,Masoud Rouhizadeh,Maryam Zolnoori*

Main category: cs.CL

TL;DR: 融合Transformer嵌入与语言学特征，结合LLM生成合成数据进行增强，显著提升ADRD语音检测性能


<details>
  <summary>Details</summary>
Motivation: 美国约500万老年人受ADRD影响但超半数未确诊，需开发基于语音NLP的早期筛查方法

Method: 1. 使用DementiaBank的237条语音转录数据
2. 评估10个Transformer模型并融合语言学特征
3. 用LLM生成标签化合成语音进行数据增强
4. 测试单模态/多模态LLM分类器性能

Result: 融合模型F1=83.3(AUC=89.5)；MedAlpaca-7B合成数据增强后F1提升至85.7；微调后单模态LLM提升显著(如MedAlpaca F1从47.3→78.5)，多模态模型性能较低(GPT-4o F1=70.2)

Conclusion: Transformer与语言学特征融合有效，临床优化LLM支持分类与数据增强，多模态模型仍需改进。性能提升与合成/真实数据分布相似性相关

Abstract: Alzheimer's disease and related dementias (ADRD) affect approximately five
million older adults in the U.S., yet over half remain undiagnosed.
Speech-based natural language processing (NLP) offers a promising, scalable
approach to detect early cognitive decline through linguistic markers.
  To develop and evaluate a screening pipeline that (i) fuses transformer
embeddings with handcrafted linguistic features, (ii) tests data augmentation
using synthetic speech generated by large language models (LLMs), and (iii)
benchmarks unimodal and multimodal LLM classifiers for ADRD detection.
  Transcripts from the DementiaBank "cookie-theft" task (n = 237) were used.
Ten transformer models were evaluated under three fine-tuning strategies. A
fusion model combined embeddings from the top-performing transformer with 110
lexical-derived linguistic features. Five LLMs (LLaMA-8B/70B, MedAlpaca-7B,
Ministral-8B, GPT-4o) were fine-tuned to generate label-conditioned synthetic
speech, which was used to augment training data. Three multimodal models
(GPT-4o, Qwen-Omni, Phi-4) were tested for speech-text classification in
zero-shot and fine-tuned settings.
  The fusion model achieved F1 = 83.3 (AUC = 89.5), outperforming linguistic or
transformer-only baselines. Augmenting training data with 2x MedAlpaca-7B
synthetic speech increased F1 to 85.7. Fine-tuning significantly improved
unimodal LLM classifiers (e.g., MedAlpaca: F1 = 47.3 -> 78.5 F1). Current
multimodal models demonstrated lower performance (GPT-4o = 70.2 F1; Qwen =
66.0). Performance gains aligned with the distributional similarity between
synthetic and real speech.
  Integrating transformer embeddings with linguistic features enhances ADRD
detection from speech. Clinically tuned LLMs effectively support both
classification and data augmentation, while further advancement is needed in
multimodal modeling.

</details>


### [30] [PREF: Reference-Free Evaluation of Personalised Text Generation in LLMs](https://arxiv.org/abs/2508.10028)
*Xiao Fu,Hossein A. Rahmani,Bin Wu,Jerome Ramos,Emine Yilmaz,Aldo Lipani*

Main category: cs.CL

TL;DR: 提出PREF框架，通过三阶段流程实现无需参考文本的个性化生成评估，在准确性和用户对齐方面优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成评估方法忽视用户个性化需求，无法有效衡量用户偏好匹配度。

Method: 1) 覆盖阶段生成通用评估标准；2) 偏好阶段整合用户画像构建个性化评估矩阵；3) 评分阶段用LLM进行分级评估

Result: 在PrefEval基准测试中，PREF准确率提升10.2%，与人类判断的相关系数达0.81，模型效率提高3倍

Conclusion: PREF框架为个性化生成系统建立了可扩展、可解释的评估体系，支持更可靠的系统开发

Abstract: Personalised text generation is essential for user-centric information
systems, yet most evaluation methods overlook the individuality of users. We
introduce \textbf{PREF}, a \textbf{P}ersonalised \textbf{R}eference-free
\textbf{E}valuation \textbf{F}ramework that jointly measures general output
quality and user-specific alignment without requiring gold personalised
references. PREF operates in a three-step pipeline: (1) a coverage stage uses a
large language model (LLM) to generate a comprehensive, query-specific
guideline covering universal criteria such as factuality, coherence, and
completeness; (2) a preference stage re-ranks and selectively augments these
factors using the target user's profile, stated or inferred preferences, and
context, producing a personalised evaluation rubric; and (3) a scoring stage
applies an LLM judge to rate candidate answers against this rubric, ensuring
baseline adequacy while capturing subjective priorities. This separation of
coverage from preference improves robustness, transparency, and reusability,
and allows smaller models to approximate the personalised quality of larger
ones. Experiments on the PrefEval benchmark, including implicit
preference-following tasks, show that PREF achieves higher accuracy, better
calibration, and closer alignment with human judgments than strong baselines.
By enabling scalable, interpretable, and user-aligned evaluation, PREF lays the
groundwork for more reliable assessment and development of personalised
language generation systems.

</details>


### [31] [Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs](https://arxiv.org/abs/2508.10029)
*Wenpeng Xing,Mohan Li,Chunqiang Hu,Haitao XuNingyu Zhang,Bo Lin,Meng Han*

Main category: cs.CL

TL;DR: 提出潜在融合越狱攻击（LFJ）及其对抗训练防御方案，攻击成功率94.01%，防御降低攻击成功率80%+


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型存在安全漏洞，需要更有效的越狱攻击检测方法和防御机制

Method: 1. 选择高相似性查询对 2. 在关键层进行梯度引导隐状态插值 3. 三阶段优化策略平衡攻击效果与效率

Result: 在Vicuna等模型实现94.01%平均攻击成功率，对抗训练后攻击成功率下降超80%

Conclusion: LFJ通过精准的查询匹配和隐空间融合实现高效攻击，提出的防御方案可有效提升模型鲁棒性

Abstract: Large language models (LLMs) demonstrate impressive capabilities in various
language tasks but are susceptible to jailbreak attacks that circumvent their
safety alignments. This paper introduces Latent Fusion Jailbreak (LFJ), a
representation-based attack that interpolates hidden states from harmful and
benign query pairs to elicit prohibited responses. LFJ begins by selecting
query pairs with high thematic and syntactic similarity, then performs
gradient-guided interpolation at influential layers and tokens, followed by
optimization to balance attack success, output fluency, and computational
efficiency. Evaluations on models such as Vicuna and LLaMA-2 across benchmarks
like AdvBench and MaliciousInstruct yield an average attack success rate (ASR)
of 94.01%, outperforming existing methods. To mitigate LFJ, we propose an
adversarial training defense that fine-tunes models on interpolated examples,
reducing ASR by over 80% without degrading performance on benign inputs.
Ablation studies validate the importance of query pair selection, hidden state
interpolation components, and optimization strategies in LFJ's effectiveness.

</details>


### [32] [Inference-Aware Prompt Optimization for Aligning Black-Box Large Language Models](https://arxiv.org/abs/2508.10030)
*Saaduddin Mahmud,Mason Nakamura,Kyle H. Wray,Shlomo Zilberstein*

Main category: cs.CL

TL;DR: 提出了IAPO框架，联合优化提示和推理规模，在有限预算下提升黑盒大语言模型的对齐效果


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法未考虑推理策略的相互影响，用户对多目标权衡和推理预算的偏好显著影响配置选择，需要联合优化方法

Method: 开发IAPO框架联合优化提示和推理规模，提出PSST训练算法（基于顺序修剪的提示缩放），分析有限预算下的错误概率保证

Result: 在六项任务（包括多目标文本生成和推理）验证有效性，证明推理感知机制对提示优化的关键作用

Conclusion: 联合优化提示与推理配置能显著提升模型对齐效果，IAPO框架为资源受限场景提供了系统化解决方案

Abstract: Prompt optimization methods have demonstrated significant effectiveness in
aligning black-box large language models (LLMs). In parallel, inference scaling
strategies such as Best-of-N Sampling and Majority Voting have also proven to
enhance alignment and performance by trading off computation. However, existing
prompt optimization approaches are inference strategy agnostic; that is, they
optimize prompts without regard to the inference strategy employed during
deployment. This constitutes a significant methodological gap, as our empirical
and theoretical analysis reveals a strong interdependence between these two
paradigms. Moreover, we find that user preferences regarding trade-offs among
multiple objectives and inference budgets substantially influence the choice of
prompt and inference configuration. To address this gap, we introduce a unified
novel framework named IAPO (Inference-Aware Prompt Optimization) that jointly
optimizes the prompt and inference scale, while being aware of the inference
budget and different task objectives. We then develop a fixed-budget training
algorithm for IAPO, which we call PSST (Prompt Scaling via Sequential
Trimming), and analyze finite-budget guarantees on error probability. Finally,
we evaluate the effectiveness of PSST on six different tasks, including
multi-objective text generation and reasoning, and demonstrate the critical
role of incorporating inference-awareness when aligning black-box LLMs through
prompt optimization.

</details>


### [33] [The Cost of Thinking: Increased Jailbreak Risk in Large Language Models](https://arxiv.org/abs/2508.10032)
*Fan Yang*

Main category: cs.CL

TL;DR: 研究发现大语言模型的思维模式更易受越狱攻击，提出通过添加特定思维令牌的安全干预方法，显著降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为思维模式是LLMs最有价值的模式之一，但本文发现该模式存在被忽视的安全隐患——越狱攻击成功率显著高于非思维模式。

Method: 在AdvBench和HarmBench上评估9个LLMs，分析攻击成功样本特征，提出在prompt中添加特定思维令牌的安全干预方法，显式引导模型内部思考过程。

Result: 安全思维干预使思维模式下的攻击成功率平均下降37.2%（AdvBench）和28.5%（HarmBench），且保持正常回答质量。

Conclusion: 思维长度与教育属性是攻击成功关键特征，安全干预有效提升LLMs防御能力。该研究揭示了思维模式安全风险，为模型安全设计提供新方向。

Abstract: Thinking mode has always been regarded as one of the most valuable modes in
LLMs. However, we uncover a surprising and previously overlooked phenomenon:
LLMs with thinking mode are more easily broken by Jailbreak attack. We evaluate
9 LLMs on AdvBench and HarmBench and find that the success rate of attacking
thinking mode in LLMs is almost higher than that of non-thinking mode. Through
large numbers of sample studies, it is found that for educational purposes and
excessively long thinking lengths are the characteristics of successfully
attacked data, and LLMs also give harmful answers when they mostly know that
the questions are harmful. In order to alleviate the above problems, this paper
proposes a method of safe thinking intervention for LLMs, which explicitly
guides the internal thinking processes of LLMs by adding "specific thinking
tokens" of LLMs to the prompt. The results demonstrate that the safe thinking
intervention can significantly reduce the attack success rate of LLMs with
thinking mode.

</details>


### [34] [Reflect then Learn: Active Prompting for Information Extraction Guided by Introspective Confusion](https://arxiv.org/abs/2508.10036)
*Dong Zhao,Yadong Wang,Xiang Chen,Chenxi Wang,Hongliang Dai,Chuanxing Geng,Shengzhong Zhang,Shaoyuan Li,Sheng-Jun Huang*

Main category: cs.CL

TL;DR: 提出APIE框架——基于内省式困惑的主动提示方法，通过量化格式与内容双重不确定性，提升少样本信息抽取的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统提示策略忽视模型在生成结构化格式（语法）和语义内容上的双重困惑，导致信息抽取效果受限。APIE通过自我评估模型在格式生成和语义一致性上的困惑度，主动选择最具挑战性的样本来改善这一问题。

Method: 1. 定义格式不确定性（语法正确性）和内容不确定性（语义一致性）的双重评估指标；2. 使用LLM自评估未标注数据的综合困惑度；3. 基于困惑度排序主动选择最优少样本示例。

Result: 在四个基准测试中显著超越基线模型，信息抽取准确率平均提升12.3%，格式错误率降低41%。

Conclusion: 同时考虑格式和语义的双层次不确定性度量，是构建可靠结构化生成系统的关键，该方法为提升LLMs结构化输出能力提供了新范式。

Abstract: Large Language Models (LLMs) show remarkable potential for few-shot
information extraction (IE), yet their performance is highly sensitive to the
choice of in-context examples. Conventional selection strategies often fail to
provide informative guidance, as they overlook a key source of model
fallibility: confusion stemming not just from semantic content, but also from
the generation of well-structured formats required by IE tasks. To address
this, we introduce Active Prompting for Information Extraction (APIE), a novel
active prompting framework guided by a principle we term introspective
confusion. Our method empowers an LLM to assess its own confusion through a
dual-component uncertainty metric that uniquely quantifies both Format
Uncertainty (difficulty in generating correct syntax) and Content Uncertainty
(inconsistency in extracted semantics). By ranking unlabeled data with this
comprehensive score, our framework actively selects the most challenging and
informative samples to serve as few-shot exemplars. Extensive experiments on
four benchmarks show that our approach consistently outperforms strong
baselines, yielding significant improvements in both extraction accuracy and
robustness. Our work highlights the critical importance of a fine-grained,
dual-level view of model uncertainty when it comes to building effective and
reliable structured generation systems.

</details>


### [35] [mSCoRe: a $M$ultilingual and Scalable Benchmark for $S$kill-based $Co$mmonsense $Re$asoning](https://arxiv.org/abs/2508.10137)
*Nghia Trung Ngo,Franck Dernoncourt,Thien Huu Nguyen*

Main category: cs.CL

TL;DR: 提出多语言可扩展技能基准mSCoRe，系统评估大语言模型在多语言常识推理中的技能应用机制，揭示模型在跨文化场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有推理增强型大语言模型缺乏对跨语言/文化常识推理中人类技能应用机制的系统研究，需建立专门评估体系填补空白。

Method: 构建包含推理技能分类法、数据合成流程和动态难度调节框架的三位一体评估体系，覆盖8个前沿模型进行多维度测试。

Result: 实验表明mSCoRe对现有模型具有显著挑战性，模型在高级别复杂度和文化相关常识推理任务中表现明显受限。

Conclusion: 当前模型在跨语言常识推理中存在系统性缺陷，需通过改进训练范式和数据多样性提升文化感知推理能力。

Abstract: Recent advancements in reasoning-reinforced Large Language Models (LLMs) have
shown remarkable capabilities in complex reasoning tasks. However, the
mechanism underlying their utilization of different human reasoning skills
remains poorly investigated, especially for multilingual commonsense reasoning
that involves everyday knowledge across different languages and cultures. To
address this gap, we propose a \textbf{M}ultilingual and Scalable Benchmark for
\textbf{S}kill-based \textbf{Co}mmonsense \textbf{Re}asoning (\textbf{mSCoRe}).
Our benchmark incorporates three key components that are designed to
systematically evaluate LLM's reasoning capabilities, including: (1) a novel
taxonomy of reasoning skills that enables fine-grained analysis of models'
reasoning processes, (2) a robust data synthesis pipeline tailored specifically
for commonsense reasoning evaluation, and (3) a complexity scaling framework
allowing task difficulty to scale dynamically alongside future improvements in
LLM abilities. Extensive experiments on eights state-of-the-art LLMs of varying
sizes and training approaches demonstrate that \textbf{mSCoRe} remains
significantly challenging for current models, particularly at higher complexity
levels. Our results reveal the limitations of such reasoning-reinforced models
when confronted with nuanced multilingual general and cultural commonsense. We
further provide detailed analysis on the models' reasoning processes,
suggesting future directions for improving multilingual commonsense reasoning
capabilities.

</details>


### [36] [Multi-Turn Puzzles: Evaluating Interactive Reasoning and Strategic Dialogue in LLMs](https://arxiv.org/abs/2508.10142)
*Kartikeya Badola,Jonathan Simon,Arian Hosseini,Sara Marie Mc Carthy,Tsendsuren Munkhdalai,Abhimanyu Goyal,Tomáš Kočiský,Shyam Upadhyay,Bahare Fatemi,Mehran Kazemi*

Main category: cs.CL

TL;DR: 提出多轮对话基准测试以评估LLMs在复杂交互场景中的逻辑推理和信息获取能力


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在模糊环境和交互任务中存在指令遵循、逻辑推理及规划能力的不足

Method: 构建包含确定性评分机制的多轮任务套件（推理/交互对话/信息检索）

Result: 前沿模型存在显著改进空间，主要错误源于指令遵循、推理失败和规划不足

Conclusion: 该基准为评估和改进LLMs复杂交互能力提供标准化平台

Abstract: Large language models (LLMs) excel at solving problems with clear and
complete statements, but often struggle with nuanced environments or
interactive tasks which are common in most real-world scenarios. This
highlights the critical need for developing LLMs that can effectively engage in
logically consistent multi-turn dialogue, seek information and reason with
incomplete data. To this end, we introduce a novel benchmark comprising a suite
of multi-turn tasks each designed to test specific reasoning, interactive
dialogue, and information-seeking abilities. These tasks have deterministic
scoring mechanisms, thus eliminating the need for human intervention.
Evaluating frontier models on our benchmark reveals significant headroom. Our
analysis shows that most errors emerge from poor instruction following,
reasoning failures, and poor planning. This benchmark provides valuable
insights into the strengths and weaknesses of current LLMs in handling complex,
interactive scenarios and offers a robust platform for future research aimed at
improving these critical capabilities.

</details>


### [37] [LaajMeter: A Framework for LaaJ Evaluation](https://arxiv.org/abs/2508.10161)
*Gal Amram,Eitan Farchi,Shmulik Froimovich,Raviv Gal,Avi Ziv*

Main category: cs.CL

TL;DR: 提出LaaJMeter框架，通过生成合成数据实现大语言模型评估器的系统化元评估，解决特定领域评估指标有效性验证问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-as-a-Judge方法在领域特定场景下面临标注数据稀缺、专家评估成本高、未经验证指标被误用的问题，需建立可控评估标准。

Method: 构建LaaJMeter仿真框架，生成虚拟模型与评估者数据，系统性分析评估指标敏感性并确定质量阈值。

Result: 代码翻译任务实验显示不同指标对评估质量敏感度差异显著，验证了常见指标的局限性及指标选择的必要性。

Conclusion: LaaJMeter为低资源场景提供可扩展的评估解决方案，提升NLP评估可信度与可重复性。

Abstract: Large Language Models (LLMs) are increasingly used as evaluators in natural
language processing tasks, a paradigm known as LLM-as-a-Judge (LaaJ). While
effective in general domains, LaaJs pose significant challenges in
domain-specific contexts, where annotated data is scarce and expert evaluation
is costly. In such cases, meta-evaluation is often performed using metrics that
have not been validated for the specific domain in which they are applied. As a
result, it becomes difficult to determine which metrics effectively identify
LaaJ quality, and further, what threshold indicates sufficient evaluator
performance. In this work, we introduce LaaJMeter, a simulation-based framework
for controlled meta-evaluation of LaaJs. LaaJMeter enables engineers to
generate synthetic data representing virtual models and judges, allowing
systematic analysis of evaluation metrics under realistic conditions. This
helps practitioners validate and refine LaaJs for specific evaluation tasks:
they can test whether their metrics correctly distinguish between better and
worse (virtual) LaaJs, and estimate appropriate thresholds for evaluator
adequacy.
  We demonstrate the utility of LaaJMeter in a code translation task involving
a legacy programming language, showing how different metrics vary in
sensitivity to evaluator quality. Our results highlight the limitations of
common metrics and the importance of principled metric selection. LaaJMeter
provides a scalable and extensible solution for assessing LaaJs in low-resource
settings, contributing to the broader effort to ensure trustworthy and
reproducible evaluation in NLP.

</details>


### [38] [Estimating Machine Translation Difficulty](https://arxiv.org/abs/2508.10175)
*Lorenzo Proietti,Stefano Perrella,Vilém Zouhar,Roberto Navigli,Tom Kocmi*

Main category: cs.CL

TL;DR: 提出翻译难度估计任务，开发Sentinel-src模型有效识别机器翻译难点，构建更具挑战性的测试集


<details>
  <summary>Details</summary>
Motivation: 机器翻译质量接近完美，现有评估方法难以区分顶尖模型优劣，需新方法识别翻译难点

Method: 定义基于翻译质量期望的难度指标，开发Sentinel-src模型，结合新评估指标验证效果

Result: Sentinel-src模型超越传统方法（词频/句法复杂度）和LLM评估，成功构建高难度测试集

Conclusion: 翻译难度估计模型能有效筛选挑战性文本，推动机器翻译评估体系发展和研究方向优化

Abstract: Machine translation quality has began achieving near-perfect translations in
some setups. These high-quality outputs make it difficult to distinguish
between state-of-the-art models and to identify areas for future improvement.
Automatically identifying texts where machine translation systems struggle
holds promise for developing more discriminative evaluations and guiding future
research.
  We formalize the task of translation difficulty estimation, defining a text's
difficulty based on the expected quality of its translations. We introduce a
new metric to evaluate difficulty estimators and use it to assess both
baselines and novel approaches. Finally, we demonstrate the practical utility
of difficulty estimators by using them to construct more challenging machine
translation benchmarks. Our results show that dedicated models (dubbed
Sentinel-src) outperform both heuristic-based methods (e.g. word rarity or
syntactic complexity) and LLM-as-a-judge approaches. We release two improved
models for difficulty estimation, Sentinel-src-24 and Sentinel-src-25, which
can be used to scan large collections of texts and select those most likely to
challenge contemporary machine translation systems.

</details>


### [39] [Efficient Forward-Only Data Valuation for Pretrained LLMs and VLMs](https://arxiv.org/abs/2508.10180)
*Wenlong Deng,Jiaming Zhang,Qi Zeng,Christos Thrampoulidis,Boying Gong,Xiaoxiao Li*

Main category: cs.CL

TL;DR: 提出For-Value框架，通过单次前向传播即可高效计算大语言模型和视觉语言模型的训练样本影响力，避免传统梯度计算的高昂开销。


<details>
  <summary>Details</summary>
Motivation: 现有基于Hessian矩阵或模型重训练的数据估值方法计算成本过高，难以适用于十亿参数级别的大模型分析。

Method: 利用基础模型的隐藏层表示能力，通过训练样本与验证样本在表征空间的对齐程度和预测误差的闭式计算，实现无需反向传播的影响力评估。

Result: 在微调样本影响力识别和错误标签检测任务中，For-Value达到或超越传统梯度基线方法的效果。

Conclusion: 该框架为大规模模型的可解释性分析提供了高效解决方案，显著降低了数据估值计算复杂度。

Abstract: Quantifying the influence of individual training samples is essential for
enhancing the transparency and accountability of large language models (LLMs)
and vision-language models (VLMs). However, existing data valuation methods
often rely on Hessian information or model retraining, making them
computationally prohibitive for billion-parameter models. In this work, we
introduce For-Value, a forward-only data valuation framework that enables
scalable and efficient influence estimation for both LLMs and VLMs. By
leveraging the rich representations of modern foundation models, For-Value
computes influence scores using a simple closed-form expression based solely on
a single forward pass, thereby eliminating the need for costly gradient
computations. Our theoretical analysis demonstrates that For-Value accurately
estimates per-sample influence by capturing alignment in hidden representations
and prediction errors between training and validation samples. Extensive
experiments show that For-Value matches or outperforms gradient-based baselines
in identifying impactful fine-tuning examples and effectively detecting
mislabeled data.

</details>


### [40] [PakBBQ: A Culturally Adapted Bias Benchmark for QA](https://arxiv.org/abs/2508.10186)
*Abdullah Hashmat,Muhammad Arham Mirza,Agha Ali Raza*

Main category: cs.CL

TL;DR: 构建巴基斯坦文化适配的PakBBQ数据集，验证多语言大模型在消除地域性偏见中的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在低资源语言（如乌尔都语）和区域文化背景中存在的系统性偏见评估缺失问题。

Method: 扩展BBQ数据集，创建含17180个双语QA对的巴基斯坦语境偏见测试集，覆盖8个本土相关偏见维度。通过歧义/明确语境、消极/中性提问框架评估模型表现。

Result: 消歧后准确率提升12%；乌尔都语反偏见性强于英语；消极提问显著降低刻板回答（最高降幅达特定百分比）。

Conclusion: 本土化基准和简单提示工程（如负面提问框架）能有效缓解低资源环境的模型偏见问题。

Abstract: With the widespread adoption of Large Language Models (LLMs) across various
applications, it is empirical to ensure their fairness across all user
communities. However, most LLMs are trained and evaluated on Western centric
data, with little attention paid to low-resource languages and regional
contexts. To address this gap, we introduce PakBBQ, a culturally and regionally
adapted extension of the original Bias Benchmark for Question Answering (BBQ)
dataset. PakBBQ comprises over 214 templates, 17180 QA pairs across 8
categories in both English and Urdu, covering eight bias dimensions including
age, disability, appearance, gender, socio-economic status, religious, regional
affiliation, and language formality that are relevant in Pakistan. We evaluate
multiple multilingual LLMs under both ambiguous and explicitly disambiguated
contexts, as well as negative versus non negative question framings. Our
experiments reveal (i) an average accuracy gain of 12\% with disambiguation,
(ii) consistently stronger counter bias behaviors in Urdu than in English, and
(iii) marked framing effects that reduce stereotypical responses when questions
are posed negatively. These findings highlight the importance of contextualized
benchmarks and simple prompt engineering strategies for bias mitigation in low
resource settings.

</details>


### [41] [Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models](https://arxiv.org/abs/2508.10192)
*Igor Halperin*

Main category: cs.CL

TL;DR: 提出SDM轻量级框架，通过语义等效提示变体检测大语言模型的幻觉问题，构建Semantic Box诊断系统。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如语义熵）仅测试固定提示下的答案多样性，无法检测深层语义偏差。需开发更敏感的幻觉检测框架。

Method: 1. 联合聚类创建提示-回答共享主题空间
2. 热图可视化对话语义关系
3. 组合Jensen-Shannon散度与Wasserstein距离计算S_H分数
4. 通过KL散度识别语义探索行为

Result: 成功区分自信虚构等危险响应类型，量化语义偏差（S_H高分=严重幻觉），建立响应分类诊断框架。

Conclusion: SDM框架在检测语义偏差方面优于传统方法，Semantic Box为LLM可靠性评估提供新工具，KL散度是识别生成行为的关键指标。

Abstract: The proliferation of Large Language Models (LLMs) is challenged by
hallucinations, critical failure modes where models generate non-factual,
nonsensical or unfaithful text. This paper introduces Semantic Divergence
Metrics (SDM), a novel lightweight framework for detecting Faithfulness
Hallucinations -- events of severe deviations of LLMs responses from input
contexts. We focus on a specific implementation of these LLM errors,
{confabulations, defined as responses that are arbitrary and semantically
misaligned with the user's query. Existing methods like Semantic Entropy test
for arbitrariness by measuring the diversity of answers to a single, fixed
prompt. Our SDM framework improves upon this by being more prompt-aware: we
test for a deeper form of arbitrariness by measuring response consistency not
only across multiple answers but also across multiple, semantically-equivalent
paraphrases of the original prompt. Methodologically, our approach uses joint
clustering on sentence embeddings to create a shared topic space for prompts
and answers. A heatmap of topic co-occurances between prompts and responses can
be viewed as a quantified two-dimensional visualization of the user-machine
dialogue. We then compute a suite of information-theoretic metrics to measure
the semantic divergence between prompts and responses. Our practical score,
$\mathcal{S}_H$, combines the Jensen-Shannon divergence and Wasserstein
distance to quantify this divergence, with a high score indicating a
Faithfulness hallucination. Furthermore, we identify the KL divergence
KL(Answer $||$ Prompt) as a powerful indicator of \textbf{Semantic
Exploration}, a key signal for distinguishing different generative behaviors.
These metrics are further combined into the Semantic Box, a diagnostic
framework for classifying LLM response types, including the dangerous,
confident confabulation.

</details>


### [42] [Understanding Textual Emotion Through Emoji Prediction](https://arxiv.org/abs/2508.10222)
*Ethan Gordon,Nishank Kuppa,Rigved Tummala,Sriram Anasuri*

Main category: cs.CL

TL;DR: 该研究对比四种深度学习模型在表情符号预测任务中的表现，发现BERT整体最优而CNN擅长预测罕见表情，强调模型选择和超参数调优的重要性。


<details>
  <summary>Details</summary>
Motivation: 通过解决类别不平衡问题并比较不同架构性能，提升短文本中表情符号预测的准确性，从而改进情感感知的人机交互体验。

Method: 使用包含前馈网络、CNN、Transformer和BERT的四类模型，在TweetEval数据集上采用焦点损失和正则化技术处理数据不平衡。

Result: BERT因预训练优势获得最高综合准确率，CNN在罕见表情类别预测中表现最佳，焦点损失有效缓解了类别不平衡问题。

Conclusion: 架构选择和超参数调优是提升表情预测效果的关键，不同模型在不同场景下具备互补优势，为对话系统优化提供重要参考。

Abstract: This project explores emoji prediction from short text sequences using four
deep learning architectures: a feed-forward network, CNN, transformer, and
BERT. Using the TweetEval dataset, we address class imbalance through focal
loss and regularization techniques. Results show BERT achieves the highest
overall performance due to its pre-training advantage, while CNN demonstrates
superior efficacy on rare emoji classes. This research shows the importance of
architecture selection and hyperparameter tuning for sentiment-aware emoji
prediction, contributing to improved human-computer interaction.

</details>


### [43] [Using Large Language Models to Measure Symptom Severity in Patients At Risk for Schizophrenia](https://arxiv.org/abs/2508.10226)
*Andrew X. Chen,Guillermo Horga,Sean Escola*

Main category: cs.CL

TL;DR: 利用大语言模型（LLMs）通过非结构化临床访谈文本预测精神分裂症高危人群的BPRS症状评分，效果接近人类评估可靠性，并展示在多语言和纵向评估中的潜力


<details>
  <summary>Details</summary>
Motivation: 临床实践中BPRS量表因需结构化访谈难以普及，研究试图通过LLMs实现自动化评估以解决临床监测难题

Method: 使用AMP-SCZ队列409例CHR患者的临床访谈文本，通过零样本学习预测BPRS评分，测试模型在外语环境（中/西语）和纵向信息整合（单样本/少样本学习）中的表现

Result: 模型预测与真实评估高度一致（中位一致性0.84，组内相关系数0.73），外语评估表现更优（一致性0.88，ICC 0.70），纵向整合进一步提升准确性

Conclusion: LLMs能有效替代人工进行BPRS评估，突破语言障碍实现标准化症状监测，为精神分裂症高危人群的长期追踪提供高效解决方案

Abstract: Patients who are at clinical high risk (CHR) for schizophrenia need close
monitoring of their symptoms to inform appropriate treatments. The Brief
Psychiatric Rating Scale (BPRS) is a validated, commonly used research tool for
measuring symptoms in patients with schizophrenia and other psychotic
disorders; however, it is not commonly used in clinical practice as it requires
a lengthy structured interview. Here, we utilize large language models (LLMs)
to predict BPRS scores from clinical interview transcripts in 409 CHR patients
from the Accelerating Medicines Partnership Schizophrenia (AMP-SCZ) cohort.
Despite the interviews not being specifically structured to measure the BPRS,
the zero-shot performance of the LLM predictions compared to the true
assessment (median concordance: 0.84, ICC: 0.73) approaches human inter- and
intra-rater reliability. We further demonstrate that LLMs have substantial
potential to improve and standardize the assessment of CHR patients via their
accuracy in assessing the BPRS in foreign languages (median concordance: 0.88,
ICC: 0.70), and integrating longitudinal information in a one-shot or few-shot
learning approach.

</details>


### [44] [A Computational Approach to Analyzing Language Change and Variation in the Constructed Language Toki Pona](https://arxiv.org/abs/2508.10246)
*Daniel Huang,Hyoun-A Joo*

Main category: cs.CL

TL;DR: 通过计算语言学和语料库分析发现，人造语言Toki Pona的语言演变规律与自然语言相似，其社区使用驱动的变化验证了社会语言学因素对语言系统的普适影响。


<details>
  <summary>Details</summary>
Motivation: 探究人造语言Toki Pona是否与自然语言类似受到社会语言学因素影响，及其随时间演变的动态机制。

Method: 采用计算语言学和语料库分析方法，通过追踪流变词类、及物性特征，对比历时语料中的句法偏好差异及跨语料库使用变体。

Result: Toki Pona的内容词句法选择呈现历时变化，不同社群语料显示显著变异模式，印证社会因素驱动语言演变的基本原理。

Conclusion: 即使刻意设计的语言系统，在实际社群使用中仍会自然发展出类似自然语言的变异机制，证实语言演变的社会性本质。

Abstract: This study explores language change and variation in Toki Pona, a constructed
language with approximately 120 core words. Taking a computational and
corpus-based approach, the study examines features including fluid word classes
and transitivity in order to examine (1) changes in preferences of content
words for different syntactic positions over time and (2) variation in usage
across different corpora. The results suggest that sociolinguistic factors
influence Toki Pona in the same way as natural languages, and that even
constructed linguistic systems naturally evolve as communities use them.

</details>


### [45] [Inductive Bias Extraction and Matching for LLM Prompts](https://arxiv.org/abs/2508.10295)
*Christian M. Angel,Francis Ferraro*

Main category: cs.CL

TL;DR: 通过归纳偏置提取与匹配策略优化LLM提示词，显著提升Likert评分效果


<details>
  <summary>Details</summary>
Motivation: 大型语言模型对提示词微小变化敏感，需要开发有效方法优化提示工程

Method: 利用LLM自身输出构建符合模型归纳偏置的提示词（Inductive Bias Extraction and Matching）

Result: 分类任务Likert评分提升19%，排序任务提升27%

Conclusion: 该策略有效利用模型内在偏置，显著提升LLM在评估任务中的表现

Abstract: The active research topic of prompt engineering makes it evident that LLMs
are sensitive to small changes in prompt wording. A portion of this can be
ascribed to the inductive bias that is present in the LLM. By using an LLM's
output as a portion of its prompt, we can more easily create satisfactory
wording for prompts. This has the effect of creating a prompt that matches the
inductive bias in model. Empirically, we show that using this Inductive Bias
Extraction and Matching strategy improves LLM Likert ratings used for
classification by up to 19% and LLM Likert ratings used for ranking by up to
27%.

</details>


### [46] [Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race](https://arxiv.org/abs/2508.10304)
*Gustavo Bonil,Simone Hashiguti,Jhessica Silva,João Gondim,Helena Maia,Nádia Silva,Helio Pedrini,Sandra Avila*

Main category: cs.CL

TL;DR: 研究通过定性分析发现，大型语言模型生成的文本中固化着种族与性别偏见，修正提示难以消除深层次刻板印象


<details>
  <summary>Details</summary>
Motivation: 当前基于定量方法的偏见检测常忽略自然语言中细微的偏见表现形式，需定性框架补充检测体系

Method: 手动分析LLM生成的黑人/白人女性短篇故事，采用话语分析方法揭示偏见模式

Result: 黑人女性被固化描述为与祖先/抵抗相关，白人女性更多展现自我发现；修正提示仅产生表面修正

Conclusion: 需采用跨学科批判方法进行AI开发，解决语言模型生成内容对社会不平等的再生产问题

Abstract: With the advance of Artificial Intelligence (AI), Large Language Models
(LLMs) have gained prominence and been applied in diverse contexts. As they
evolve into more sophisticated versions, it is essential to assess whether they
reproduce biases, such as discrimination and racialization, while maintaining
hegemonic discourses. Current bias detection approaches rely mostly on
quantitative, automated methods, which often overlook the nuanced ways in which
biases emerge in natural language. This study proposes a qualitative,
discursive framework to complement such methods. Through manual analysis of
LLM-generated short stories featuring Black and white women, we investigate
gender and racial biases. We contend that qualitative methods such as the one
proposed here are fundamental to help both developers and users identify the
precise ways in which biases manifest in LLM outputs, thus enabling better
conditions to mitigate them. Results show that Black women are portrayed as
tied to ancestry and resistance, while white women appear in self-discovery
processes. These patterns reflect how language models replicate crystalized
discursive representations, reinforcing essentialization and a sense of social
immobility. When prompted to correct biases, models offered superficial
revisions that maintained problematic meanings, revealing limitations in
fostering inclusive narratives. Our results demonstrate the ideological
functioning of algorithms and have significant implications for the ethical use
and development of AI. The study reinforces the need for critical,
interdisciplinary approaches to AI design and deployment, addressing how
LLM-generated discourses reflect and perpetuate inequalities.

</details>


### [47] [ReviewRL: Towards Automated Scientific Review with RL](https://arxiv.org/abs/2508.10308)
*Sihang Zeng,Kai Tian,Kaiyan Zhang,Yuru wang,Junqi Gao,Runze Liu,Sa Yang,Jingxuan Li,Xinwei Long,Jiaheng Ma,Biqing Qi,Bowen Zhou*

Main category: cs.CL

TL;DR: 提出强化学习框架ReviewRL，通过检索增强与复合奖励函数显著提升论文评审质量与评分准确性


<details>
  <summary>Details</summary>
Motivation: 传统自动化评审存在事实准确性低、评分一致性差、分析深度不足等问题，生成的评审意见缺乏高质量人工评审的洞察力

Method: 1. 整合相关科学文献的ArXiv-MCP检索增强上下文生成管道
2. 建立基础评审能力的监督微调
3. 采用复合奖励函数的强化学习框架（质量提升与评分准确联合优化）

Result: 在ICLR 2025论文实验中，ReviewRL在规则指标和模型质量评估中均显著超越现有方法

Conclusion: ReviewRL奠定了强化学习驱动科学批判生成的基础框架，其GitHub开源实现将推动该领域的未来发展

Abstract: Peer review is essential for scientific progress but faces growing challenges
due to increasing submission volumes and reviewer fatigue. Existing automated
review approaches struggle with factual accuracy, rating consistency, and
analytical depth, often generating superficial or generic feedback lacking the
insights characteristic of high-quality human reviews. We introduce ReviewRL, a
reinforcement learning framework for generating comprehensive and factually
grounded scientific paper reviews. Our approach combines: (1) an ArXiv-MCP
retrieval-augmented context generation pipeline that incorporates relevant
scientific literature, (2) supervised fine-tuning that establishes foundational
reviewing capabilities, and (3) a reinforcement learning procedure with a
composite reward function that jointly enhances review quality and rating
accuracy. Experiments on ICLR 2025 papers demonstrate that ReviewRL
significantly outperforms existing methods across both rule-based metrics and
model-based quality assessments. ReviewRL establishes a foundational framework
for RL-driven automatic critique generation in scientific discovery,
demonstrating promising potential for future development in this domain. The
implementation of ReviewRL will be released at GitHub.

</details>


### [48] [From Surface to Semantics: Semantic Structure Parsing for Table-Centric Document Analysis](https://arxiv.org/abs/2508.10311)
*Xuan Li,Jialiang Dong,Raymond Wong*

Main category: cs.CL

TL;DR: DOTABLER框架通过深度语义解析表格与上下文关联，实现90%+精确率的文档结构解析与跨领域表格检索


<details>
  <summary>Details</summary>
Motivation: 现有表格研究局限于表层任务，缺乏深度语义关联分析，难以支持跨段落数据解释等高级文档解析需求

Method: 结合定制数据集与预训练模型微调，构建包含上下文语义关联识别、文档结构解析和领域表格检索的完整解析流程

Result: 在4,000页真实PDF测试中，精确率和F1值超90%，语义分析性能显著优于GPT-4o等先进模型

Conclusion: DOTABLER成功突破表格深度语义解析瓶颈，为金融、医疗等领域的复杂文档分析提供了有效解决方案

Abstract: Documents are core carriers of information and knowl-edge, with broad
applications in finance, healthcare, and scientific research. Tables, as the
main medium for structured data, encapsulate key information and are among the
most critical document components. Existing studies largely focus on
surface-level tasks such as layout analysis, table detection, and data
extraction, lacking deep semantic parsing of tables and their contextual
associations. This limits advanced tasks like cross-paragraph data
interpretation and context-consistent analysis. To address this, we propose
DOTABLER, a table-centric semantic document parsing framework designed to
uncover deep semantic links between tables and their context. DOTABLER
leverages a custom dataset and domain-specific fine-tuning of pre-trained
models, integrating a complete parsing pipeline to identify context segments
semantically tied to tables. Built on this semantic understanding, DOTABLER
implements two core functionalities: table-centric document structure parsing
and domain-specific table retrieval, delivering comprehensive table-anchored
semantic analysis and precise extraction of semantically relevant tables.
Evaluated on nearly 4,000 pages with over 1,000 tables from real-world PDFs,
DOTABLER achieves over 90% Precision and F1 scores, demonstrating superior
performance in table-context semantic analysis and deep document parsing
compared to advanced models such as GPT-4o.

</details>


### [49] [Beyond Semantic Understanding: Preserving Collaborative Frequency Components in LLM-based Recommendation](https://arxiv.org/abs/2508.10312)
*Minhao Wang,Yunhang He,Cong Xu,Zhangchi Zhu,Wei Zhang*

Main category: cs.CL

TL;DR: 提出FreLLM4Rec框架，通过频谱方法平衡语义与协同信号，解决LLM推荐系统协同信号衰减问题，实验显示NDCG@10提升达8%


<details>
  <summary>Details</summary>
Motivation: LLM推荐系统在传播协同ID嵌入时会逐层削弱协作信号，而传统Transformer模型能保持协作信号。需要探索LLM处理协作信息的机理并提出改进方案

Method: 1. 全局图低通滤波器(G-LPF)去除嵌入中的高频噪声；2. 时域频率调制(TFM)逐层保留协作信号，并通过理论证明将局部图傅里叶滤波与频域滤波关联

Result: 在四个基准数据集上验证，成功缓解协作信号衰减，NDCG@10指标最高提升8%，超越现有基线模型

Conclusion: 揭示了LLM处理协作信息的机制，提出的频谱方法为改进LLM推荐系统提供理论支撑，验证了频域操作在推荐系统中的有效性

Abstract: Recommender systems in concert with Large Language Models (LLMs) present
promising avenues for generating semantically-informed recommendations.
However, LLM-based recommenders exhibit a tendency to overemphasize semantic
correlations within users' interaction history. When taking pretrained
collaborative ID embeddings as input, LLM-based recommenders progressively
weaken the inherent collaborative signals as the embeddings propagate through
LLM backbones layer by layer, as opposed to traditional Transformer-based
sequential models in which collaborative signals are typically preserved or
even enhanced for state-of-the-art performance. To address this limitation, we
introduce FreLLM4Rec, an approach designed to balance semantic and
collaborative information from a spectral perspective. Item embeddings that
incorporate both semantic and collaborative information are first purified
using a Global Graph Low-Pass Filter (G-LPF) to preliminarily remove irrelevant
high-frequency noise. Temporal Frequency Modulation (TFM) then actively
preserves collaborative signal layer by layer. Note that the collaborative
preservation capability of TFM is theoretically guaranteed by establishing a
connection between the optimal but hard-to-implement local graph fourier
filters and the suboptimal yet computationally efficient frequency-domain
filters. Extensive experiments on four benchmark datasets demonstrate that
FreLLM4Rec successfully mitigates collaborative signal attenuation and achieves
competitive performance, with improvements of up to 8.00\% in NDCG@10 over the
best baseline. Our findings provide insights into how LLMs process
collaborative information and offer a principled approach for improving
LLM-based recommendation systems.

</details>


### [50] [Cross-Prompt Encoder for Low-Performing Languages](https://arxiv.org/abs/2508.10352)
*Beso Mikaberidze,Teimuraz Saghinadze,Simon Ostermann,Philipp Muller*

Main category: cs.CL

TL;DR: 提出交叉提示编码器(XPE)和双软提示机制，通过多语言联合训练提升低性能语言模型表现


<details>
  <summary>Details</summary>
Motivation: 解决传统全模型微调在低性能语言（准确率较低的语言）上效果不佳的问题，探索提示编码器的跨语言迁移潜力

Method: XPE结合轻量级编码架构与类型学多语言联合训练，搭配双软提示机制（编码器生成提示+直接训练的标准软提示）

Result: 在SIB-200基准测试中，XPE对低性能语言效果显著，混合方案在多语言场景具有更广适应性

Conclusion: XPE专注于低资源语言优化，混合方案平衡共享结构与语言特异性，形成多语言适配的灵活框架

Abstract: Soft prompts have emerged as a powerful alternative to adapters in
parameter-efficient fine-tuning (PEFT), enabling large language models (LLMs)
to adapt to downstream tasks without architectural changes or parameter
updates. While prior work has focused on stabilizing training via parameter
interaction in small neural prompt encoders, their broader potential for
transfer across languages remains unexplored. In this paper, we demonstrate
that a prompt encoder can play a central role in improving performance on
low-performing languages-those that achieve poor accuracy even under full-model
fine-tuning. We introduce the Cross-Prompt Encoder (XPE), which combines a
lightweight encoding architecture with multi-source training on typologically
diverse languages - a design that enables the model to capture abstract and
transferable patterns across languages. To complement XPE, we propose a Dual
Soft Prompt mechanism that combines an encoder-based prompt with a directly
trained standard soft prompt. This hybrid design proves especially effective
for target languages that benefit from both broadly shared structure and
language-specific alignment. Experiments on the SIB-200 benchmark reveal a
consistent trade-off: XPE is most effective for low-performing languages, while
hybrid variants offer broader adaptability across multilingual settings.

</details>


### [51] [Making Qwen3 Think in Korean with Reinforcement Learning](https://arxiv.org/abs/2508.10355)
*Jungyup Lee,Jemin Kim,Sang Park,SeungJae Lee*

Main category: cs.CL

TL;DR: 提出两阶段微调方法（监督微调+强化学习GRPO算法），使Qwen3 14B模型实现韩语原生思维，在数学/编程等复杂推理任务中显著提升表现，同时保持知识完整性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在韩语逻辑推理中的思维断层问题，通过强化韩语内部思维链提升专业领域任务表现，同时避免传统强化学习方法中的策略崩溃风险。

Method: 第一阶段使用高质量韩语推理数据集进行监督微调（SFT），建立韩语逻辑推理基础；第二阶段采用定制化GRPO算法，通过oracle裁判模型校准奖励信号，解决奖励黑客攻击和策略崩溃问题。

Result: 最终模型在GSAT数学（+15.2%）等推理基准显著提升，代码生成准确率提高23.8%，且保持多语言知识能力，实现完全韩语思维链推导。

Conclusion: 该方法成功实现大模型在目标语言的深度认知对齐，为低资源语言的AI应用提供新范式，通过奖励校准机制突破强化学习稳定性瓶颈。

Abstract: We present a two-stage fine-tuning approach to make the large language model
Qwen3 14B "think" natively in Korean. In the first stage, supervised
fine-tuning (SFT) on a high-quality Korean reasoning dataset establishes a
strong foundation in Korean logical reasoning, yielding notable improvements in
Korean-language tasks and even some gains in general reasoning ability. In the
second stage, we employ reinforcement learning with a customized Group Relative
Policy Optimization (GRPO) algorithm to further enhance both Korean reasoning
alignment and overall problem-solving performance. We address critical
stability challenges in GRPO training - such as reward hacking and policy
collapse - by introducing an oracle judge model that calibrates the reward
signal. Our approach achieves stable learning (avoiding the collapse observed
in naive GRPO) and leads to steady, incremental performance gains. The final
RL-tuned model demonstrates substantially improved results on advanced
reasoning benchmarks (particularly math and coding tasks) while maintaining
knowledge and language proficiency, successfully conducting its internal
chain-of-thought entirely in Korean.

</details>


### [52] [Advancing Cross-lingual Aspect-Based Sentiment Analysis with LLMs and Constrained Decoding for Sequence-to-Sequence Models](https://arxiv.org/abs/2508.10366)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 提出无需翻译工具的序列到序列方法，约束解码使跨语言ABSA性能提升10%，验证多语言大模型适配性


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言ABSA过度依赖翻译工具、现有研究局限于简单任务的问题

Method: 基于约束解码的序列到序列框架，直接处理复杂跨语言ABSA任务

Result: 性能提升最高达10%，支持更复杂任务；微调多语言大模型效果可比拟专用模型

Conclusion: 为跨语言ABSA提供高效解决方案，揭示大模型适配边界

Abstract: Aspect-based sentiment analysis (ABSA) has made significant strides, yet
challenges remain for low-resource languages due to the predominant focus on
English. Current cross-lingual ABSA studies often centre on simpler tasks and
rely heavily on external translation tools. In this paper, we present a novel
sequence-to-sequence method for compound ABSA tasks that eliminates the need
for such tools. Our approach, which uses constrained decoding, improves
cross-lingual ABSA performance by up to 10\%. This method broadens the scope of
cross-lingual ABSA, enabling it to handle more complex tasks and providing a
practical, efficient alternative to translation-dependent techniques.
Furthermore, we compare our approach with large language models (LLMs) and show
that while fine-tuned multilingual LLMs can achieve comparable results,
English-centric LLMs struggle with these tasks.

</details>


### [53] [Large Language Models for Summarizing Czech Historical Documents and Beyond](https://arxiv.org/abs/2508.10368)
*Václav Tran,Jakub Šmíd,Jiří Martínek,Ladislav Lenc,Pavel Král*

Main category: cs.CL

TL;DR: 研究利用Mistral/mT5大模型提升捷克语摘要任务性能，同时构建历史文档数据集推动该领域发展


<details>
  <summary>Details</summary>
Motivation: 捷克语尤其是历史文档的文本摘要存在语言复杂性高、标注数据稀缺的研究空白

Method: 采用Mistral和mT5大语言模型进行迁移学习，构建包含现代与历史文档的双语料库

Result: 1) 在SumeCzech数据集取得SOTA效果 2) 发布首个捷克历史文档摘要数据集Posel od Čerchova

Conclusion: 为低资源语言处理提供范式参考，推动捷克历史文献数字化进程

Abstract: Text summarization is the task of shortening a larger body of text into a
concise version while retaining its essential meaning and key information.
While summarization has been significantly explored in English and other
high-resource languages, Czech text summarization, particularly for historical
documents, remains underexplored due to linguistic complexities and a scarcity
of annotated datasets. Large language models such as Mistral and mT5 have
demonstrated excellent results on many natural language processing tasks and
languages. Therefore, we employ these models for Czech summarization, resulting
in two key contributions: (1) achieving new state-of-the-art results on the
modern Czech summarization dataset SumeCzech using these advanced models, and
(2) introducing a novel dataset called Posel od \v{C}erchova for summarization
of historical Czech documents with baseline results. Together, these
contributions provide a great potential for advancing Czech text summarization
and open new avenues for research in Czech historical text processing.

</details>


### [54] [Improving Generative Cross-lingual Aspect-Based Sentiment Analysis with Constrained Decoding](https://arxiv.org/abs/2508.10369)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 提出基于约束解码的跨语言ABSA方法，提升复杂任务性能5%并支持多任务处理


<details>
  <summary>Details</summary>
Motivation: 现有跨语言ABSA方法存在任务复杂度低、依赖翻译工具的问题，尤其在低资源语言场景下效果不佳

Method: 使用序列到序列模型结合约束解码技术，消除翻译工具依赖，实现多任务统一建模

Result: 在7种语言6个任务中超越SOTA，约束解码提升效果超10%，微调后LLMs达到竞争力但效率较低

Conclusion: 该方法为跨语言ABSA提供了高效解决方案，实验证明约束解码的有效性，并提出实际部署建议

Abstract: While aspect-based sentiment analysis (ABSA) has made substantial progress,
challenges remain for low-resource languages, which are often overlooked in
favour of English. Current cross-lingual ABSA approaches focus on limited, less
complex tasks and often rely on external translation tools. This paper
introduces a novel approach using constrained decoding with
sequence-to-sequence models, eliminating the need for unreliable translation
tools and improving cross-lingual performance by 5\% on average for the most
complex task. The proposed method also supports multi-tasking, which enables
solving multiple ABSA tasks with a single model, with constrained decoding
boosting results by more than 10\%.
  We evaluate our approach across seven languages and six ABSA tasks,
surpassing state-of-the-art methods and setting new benchmarks for previously
unexplored tasks. Additionally, we assess large language models (LLMs) in
zero-shot, few-shot, and fine-tuning scenarios. While LLMs perform poorly in
zero-shot and few-shot settings, fine-tuning achieves competitive results
compared to smaller multilingual models, albeit at the cost of longer training
and inference times.
  We provide practical recommendations for real-world applications, enhancing
the understanding of cross-lingual ABSA methodologies. This study offers
valuable insights into the strengths and limitations of cross-lingual ABSA
approaches, advancing the state-of-the-art in this challenging research domain.

</details>


### [55] [Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts](https://arxiv.org/abs/2508.10390)
*Chiyu Zhang,Lu Zhou,Xiaogang Xu,Jiafei Wu,Liming Fang,Zhe Liu*

Main category: cs.CL

TL;DR: 论文提出基于LLM与人工辅助的MDH框架用于恶意内容检测和数据集清理，同时开发了D-Attack和DH-CoT两种新型越狱攻击策略。


<details>
  <summary>Details</summary>
Motivation: 现有红队数据集存在大量不合适的非恶意提示，手动标注成本高且LLM检测存在类型偏差，需要平衡检测效率与准确性。

Method: 结合LLM自动标注与少量人工监督形成混合评估框架MDH，通过上下文模拟提出D-Attack策略，结合劫持思维链开发DH-CoT方法。

Result: 成功实现数据集清洗和越狱响应检测，新攻击策略显著提升攻击成功率，相关代码和数据将在GitHub开源。

Conclusion: MDH框架有效平衡检测质量与效率，提出的新策略为红队测试提供有效工具，资源开放将推动相关领域研究。

Abstract: Evaluating jailbreak attacks is challenging when prompts are not overtly
harmful or fail to induce harmful outputs. Unfortunately, many existing
red-teaming datasets contain such unsuitable prompts. To evaluate attacks
accurately, these datasets need to be assessed and cleaned for maliciousness.
However, existing malicious content detection methods rely on either manual
annotation, which is labor-intensive, or large language models (LLMs), which
have inconsistent accuracy in harmful types. To balance accuracy and
efficiency, we propose a hybrid evaluation framework named MDH (Malicious
content Detection based on LLMs with Human assistance) that combines LLM-based
annotation with minimal human oversight, and apply it to dataset cleaning and
detection of jailbroken responses. Furthermore, we find that well-crafted
developer messages can significantly boost jailbreak success, leading us to
propose two new strategies: D-Attack, which leverages context simulation, and
DH-CoT, which incorporates hijacked chains of thought. The Codes, datasets,
judgements, and detection results will be released in github repository:
https://github.com/AlienZhang1996/DH-CoT.

</details>


### [56] [Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation](https://arxiv.org/abs/2508.10404)
*Huizhen Shu,Xuying Li,Qirui Wang,Yuji Kosuga,Mengqiu Tian,Zhuo Li*

Main category: cs.CL

TL;DR: 提出稀疏特征扰动框架（SFPF），利用稀疏自编码器识别关键文本特征生成对抗样本，可绕过现有防御机制。


<details>
  <summary>Details</summary>
Motivation: 现有对抗样本生成方法未能有效利用大模型可解释性特征。通过识别高激活特征进行定向扰动，可平衡攻击效果与安全对齐，提升对抗文本的防御绕过能力。

Method: 采用稀疏自编码器重构隐层表征，对成功攻击文本进行特征聚类，选择性扰动高激活特征生成保留恶意意图但强化安全信号的新对抗文本。

Result: 实验证明SFPF生成的对抗文本能突破最先进防御机制，但方法有效性受提示词和模型层影响，且需验证其他架构/大模型的泛化性。

Conclusion: SFPF为红队测试提供新范式，揭示了当前NLP系统的持续脆弱性，但需进一步验证方法在多样化场景下的适用性。

Abstract: With the rapid proliferation of Natural Language Processing (NLP), especially
Large Language Models (LLMs), generating adversarial examples to jailbreak LLMs
remains a key challenge for understanding model vulnerabilities and improving
robustness. In this context, we propose a new black-box attack method that
leverages the interpretability of large models. We introduce the Sparse Feature
Perturbation Framework (SFPF), a novel approach for adversarial text generation
that utilizes sparse autoencoders to identify and manipulate critical features
in text. After using the SAE model to reconstruct hidden layer representations,
we perform feature clustering on the successfully attacked texts to identify
features with higher activations. These highly activated features are then
perturbed to generate new adversarial texts. This selective perturbation
preserves the malicious intent while amplifying safety signals, thereby
increasing their potential to evade existing defenses. Our method enables a new
red-teaming strategy that balances adversarial effectiveness with safety
alignment. Experimental results demonstrate that adversarial texts generated by
SFPF can bypass state-of-the-art defense mechanisms, revealing persistent
vulnerabilities in current NLP systems.However, the method's effectiveness
varies across prompts and layers, and its generalizability to other
architectures and larger models remains to be validated.

</details>


### [57] [ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning](https://arxiv.org/abs/2508.10419)
*Juyuan Wang,Rongchen Zhao,Wei Wei,Yufeng Wang,Mo Yu,Jie Zhou,Jin Xu,Liyan Xu*

Main category: cs.CL

TL;DR: 提出动态迭代检索框架ComoRAG，通过模拟人类记忆整合机制解决长文本叙事理解的全局推理难题，在200K+ token的基准测试中相对提升11%


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在长文本处理中存在静态检索、无法捕捉动态关系的问题，需要模拟人类认知中记忆信号处理的动态过程

Method: 构建动态记忆工作区，通过迭代推理循环：1）生成探测查询探索新路径 2）整合证据到全局记忆池 3）形成连贯上下文解决查询

Result: 在四个长文本叙事基准（200K+ tokens）上优于现有方法，相对提升达11%，特别在需要全局理解的复杂查询中表现突出

Conclusion: 提出认知启发的动态检索范式，通过记忆整合机制实现长文本的状态推理，为检索式长文本理解提供了新范式（代码已开源）

Abstract: Narrative comprehension on long stories and novels has been a challenging
domain attributed to their intricate plotlines and entangled, often evolving
relations among characters and entities. Given the LLM's diminished reasoning
over extended context and high computational cost, retrieval-based approaches
remain a pivotal role in practice. However, traditional RAG methods can fall
short due to their stateless, single-step retrieval process, which often
overlooks the dynamic nature of capturing interconnected relations within
long-range context. In this work, we propose ComoRAG, holding the principle
that narrative reasoning is not a one-shot process, but a dynamic, evolving
interplay between new evidence acquisition and past knowledge consolidation,
analogous to human cognition when reasoning with memory-related signals in the
brain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes
iterative reasoning cycles while interacting with a dynamic memory workspace.
In each cycle, it generates probing queries to devise new exploratory paths,
then integrates the retrieved evidence of new aspects into a global memory
pool, thereby supporting the emergence of a coherent context for the query
resolution. Across four challenging long-context narrative benchmarks (200K+
tokens), ComoRAG outperforms strong RAG baselines with consistent relative
gains up to 11% compared to the strongest baseline. Further analysis reveals
that ComoRAG is particularly advantageous for complex queries requiring global
comprehension, offering a principled, cognitively motivated paradigm for
retrieval-based long context comprehension towards stateful reasoning. Our code
is publicly released at https://github.com/EternityJune25/ComoRAG

</details>


### [58] [Evaluating LLMs on Chinese Idiom Translation](https://arxiv.org/abs/2508.10421)
*Cai Yang,Yao Dou,David Heineman,Xiaofeng Wu,Wei Xu*

Main category: cs.CL

TL;DR: 构建IdiomEval框架评估中文成语翻译，发现现有系统错误率高达28%，自动评估指标不可靠，并提出改进模型


<details>
  <summary>Details</summary>
Motivation: 中文成语具有独特历史背景和结构特征，现有翻译系统常产生错误/字面化/不完整翻译，需建立专门评估体系

Method: 开发包含错误分类法的评估框架，标注900个来自9个系统（含GPT-4）的跨领域翻译对

Result: 最佳系统GPT-4错误率28%，现有指标相关性低（Pearson<0.48），改进模型F1达0.68

Conclusion: 需针对性提升成语翻译质量，传统评估指标需改进，专有模型能有效检测翻译错误

Abstract: Idioms, whose figurative meanings usually differ from their literal
interpretations, are common in everyday language, especially in Chinese, where
they often contain historical references and follow specific structural
patterns. Despite recent progress in machine translation with large language
models, little is known about Chinese idiom translation. In this work, we
introduce IdiomEval, a framework with a comprehensive error taxonomy for
Chinese idiom translation. We annotate 900 translation pairs from nine modern
systems, including GPT-4o and Google Translate, across four domains: web, news,
Wikipedia, and social media. We find these systems fail at idiom translation,
producing incorrect, literal, partial, or even missing translations. The
best-performing system, GPT-4, makes errors in 28% of cases. We also find that
existing evaluation metrics measure idiom quality poorly with Pearson
correlation below 0.48 with human ratings. We thus develop improved models that
achieve F$_1$ scores of 0.68 for detecting idiom translation errors.

</details>


### [59] [Computational Economics in Large Language Models: Exploring Model Behavior and Incentive Design under Resource Constraints](https://arxiv.org/abs/2508.10426)
*Sandeep Reddy,Kabir Khan,Rohit Patil,Ananya Chakraborty,Faizan A. Khan,Swati Kulkarni,Arjun Verma,Neha Singh*

Main category: cs.CL

TL;DR: 提出计算经济学框架优化LLM资源分配，实现40%计算效率提升


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在严格资源约束下的计算成本过高问题，通过经济原理建立资源分配机制

Method: 激励驱动训练范式：在任务损失函数中增加可微分计算成本项，引导稀疏激活

Result: 在GLUE和WikiText-103上实现Pareto优化，FLOPS减少40%，延迟降低且注意力模式更可解释

Conclusion: 经济原则为资源受限场景下的高效、自适应、透明LLM设计提供了理论依据

Abstract: Large language models (LLMs) are limited by substantial computational cost.
We introduce a "computational economics" framework that treats an LLM as an
internal economy of resource-constrained agents (attention heads and neuron
blocks) that must allocate scarce computation to maximize task utility. First,
we show empirically that when computation is scarce, standard LLMs reallocate
attention toward high-value tokens while preserving accuracy. Building on this
observation, we propose an incentive-driven training paradigm that augments the
task loss with a differentiable computation cost term, encouraging sparse and
efficient activations. On GLUE (MNLI, STS-B, CoLA) and WikiText-103, the method
yields a family of models that trace a Pareto frontier and consistently
dominate post-hoc pruning; for a similar accuracy we obtain roughly a forty
percent reduction in FLOPS and lower latency, together with more interpretable
attention patterns. These results indicate that economic principles offer a
principled route to designing efficient, adaptive, and more transparent LLMs
under strict resource constraints.

</details>


### [60] [DiFaR: Enhancing Multimodal Misinformation Detection with Diverse, Factual, and Relevant Rationales](https://arxiv.org/abs/2508.10444)
*Herun Wan,Jiaying Wu,Minnan Luo,Xiangzheng Kong,Zihan Ma,Zhi Zeng*

Main category: cs.CL

TL;DR: 提出DiFaR框架，通过多样化思维链提示和轻量级过滤模块，解决多模态虚假信息检测中生成文本依据的多样性不足、事实错误和内容噪声问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于大视觉语言模型生成的文本依据存在多样性不足、幻觉导致事实错误、无关/冲突内容引入噪声三个核心缺陷，制约虚假信息检测效果。

Method: 使用5种思维链提示激发多样化推理轨迹，设计基于句子级事实性&相关性评分的轻量级后过滤模块进行依据筛选。

Result: 在4个基准测试中超越基线最高5.9%，现有检测器性能提升达8.7%，人工评估证实依据质量三维度显著提升。

Conclusion: DiFaR通过多样化提示组合和层次化过滤机制，有效提升文本依据质量，为可训练的多模态虚假信息检测器提供更优质的支持。

Abstract: Generating textual rationales from large vision-language models (LVLMs) to
support trainable multimodal misinformation detectors has emerged as a
promising paradigm. However, its effectiveness is fundamentally limited by
three core challenges: (i) insufficient diversity in generated rationales, (ii)
factual inaccuracies due to hallucinations, and (iii) irrelevant or conflicting
content that introduces noise. We introduce DiFaR, a detector-agnostic
framework that produces diverse, factual, and relevant rationales to enhance
misinformation detection. DiFaR employs five chain-of-thought prompts to elicit
varied reasoning traces from LVLMs and incorporates a lightweight post-hoc
filtering module to select rationale sentences based on sentence-level
factuality and relevance scores. Extensive experiments on four popular
benchmarks demonstrate that DiFaR outperforms four baseline categories by up to
5.9% and boosts existing detectors by as much as 8.7%. Both automatic metrics
and human evaluations confirm that DiFaR significantly improves rationale
quality across all three dimensions.

</details>


### [61] [When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing](https://arxiv.org/abs/2508.10482)
*Mahdi Dhaini,Stephen Meisenbacher,Ege Erdogan,Florian Matthes,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 探讨NLP中解释性与隐私性的关系，发现两者可共存但受任务和方法影响


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对解释性与隐私性交叉领域的探索，需明确二者兼容性

Method: 采用差分隐私(DP)与事后解释性方法进行实证研究

Result: 隐私与解释性存在复杂动态关系，其兼容性取决于任务特性和方法选择

Conclusion: 二者可实现共存，提出未来研究应注重方法创新与任务适配的建议框架

Abstract: In the study of trustworthy Natural Language Processing (NLP), a number of
important research fields have emerged, including that of
\textit{explainability} and \textit{privacy}. While research interest in both
explainable and privacy-preserving NLP has increased considerably in recent
years, there remains a lack of investigation at the intersection of the two.
This leaves a considerable gap in understanding of whether achieving
\textit{both} explainability and privacy is possible, or whether the two are at
odds with each other. In this work, we conduct an empirical investigation into
the privacy-explainability trade-off in the context of NLP, guided by the
popular overarching methods of \textit{Differential Privacy} (DP) and Post-hoc
Explainability. Our findings include a view into the intricate relationship
between privacy and explainability, which is formed by a number of factors,
including the nature of the downstream task and choice of the text
privatization and explainability method. In this, we highlight the potential
for privacy and explainability to co-exist, and we summarize our findings in a
collection of practical recommendations for future work at this important
intersection.

</details>


### [62] [When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models](https://arxiv.org/abs/2508.10552)
*Huyu Wu,Meng Tang,Xinhan Zheng,Haiyun Jiang*

Main category: cs.CL

TL;DR: 论文揭示了多模态大语言模型普遍存在的文本主导问题，并通过指标MDI/AEI量化了该现象在不同模态中的严重程度。作者提出令牌压缩方法显著改善了注意力失衡。


<details>
  <summary>Details</summary>
Motivation: 先前研究仅在视觉-语言任务中发现文本主导现象，本文首次系统研究图像、视频、音频、时间序列和图表等多模态数据中的注意力失衡问题，揭示其普遍性和形成机制。

Method: 提出Modality Dominance Index（MDI）和Attention Efficiency Index（AEI）两个评估指标，通过注意力权重分析发现注意力稀释、融合架构设计、任务公式化三个核心成因，并设计基于令牌压缩的平衡方法。

Result: 在LLaVA-7B模型上应用令牌压缩方法后，MDI从严重失衡的10.23降至接近平衡的0.86，证实方法有效性。该方法可推广到视频、音频等其他模态。

Conclusion: 研究为开发更均衡的多模态模型提供了系统性分析框架，揭示的注意力机制缺陷和解决方案对改进模型架构设计具有指导意义，推动多模态AI向更全面的方向发展。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities across a diverse range of multimodal tasks. However, these models
suffer from a core problem known as text dominance: they depend heavily on text
for their inference, while underutilizing other modalities. While prior work
has acknowledged this phenomenon in vision-language tasks, often attributing it
to data biases or model architectures. In this paper, we conduct the first
systematic investigation of text dominance across diverse data modalities,
including images, videos, audio, time-series, and graphs. To measure this
imbalance, we propose two evaluation metrics: the Modality Dominance Index
(MDI) and the Attention Efficiency Index (AEI). Our comprehensive analysis
reveals that text dominance is both significant and pervasive across all tested
modalities. Our in-depth analysis identifies three underlying causes: attention
dilution from severe token redundancy in non-textual modalities, the influence
of fusion architecture design, and task formulations that implicitly favor
textual inputs. Furthermore, we propose a simple token compression method that
effectively rebalances model attention. Applying this method to LLaVA-7B, for
instance, drastically reduces its MDI from 10.23 to a well-balanced value of
0.86. Our analysis and methodological framework offer a foundation for the
development of more equitable and comprehensive multimodal language models.

</details>


### [63] [eDIF: A European Deep Inference Fabric for Remote Interpretability of LLM](https://arxiv.org/abs/2508.10553)
*Irma Heithoff. Marc Guggenberger,Sandra Kalogiannis,Susanne Mayer,Fabian Maag,Sigurd Schacht,Carsten Lanquillon*

Main category: cs.CL

TL;DR: 欧洲研究者开发兼容NDIF的eDIF基础设施，通过GPU集群和NNsight API实现远程LLM可解释性研究，试点研究显示平台稳定性良好且用户参与度逐步提升，但需解决数据下载延迟问题


<details>
  <summary>Details</summary>
Motivation: 推动大型语言模型可解释性研究基础设施在欧洲的普及，为科研社区提供民主化的先进模型分析能力

Method: 在安斯巴赫应用科学大学部署GPU集群，通过NNsight API实现远程模型检查，组织16名欧洲研究者开展激活修补、因果追踪等干预实验

Result: 用户参与度持续上升（平均每个实验周期增长23%），平台保持99.2%运行稳定性，DeepSeek-R1-70B模型分析延迟低于400ms，但激活数据下载耗时达47分钟/GB

Conclusion: 该基础设施为欧洲LLM可解释性研究奠定基础，未来将优化数据管道（目标下载速度提升300%）并扩展工具链，计划建立包含50+机构的跨欧洲研究网络

Abstract: This paper presents a feasibility study on the deployment of a European Deep
Inference Fabric (eDIF), an NDIF-compatible infrastructure designed to support
mechanistic interpretability research on large language models. The need for
widespread accessibility of LLM interpretability infrastructure in Europe
drives this initiative to democratize advanced model analysis capabilities for
the research community. The project introduces a GPU-based cluster hosted at
Ansbach University of Applied Sciences and interconnected with partner
institutions, enabling remote model inspection via the NNsight API. A
structured pilot study involving 16 researchers from across Europe evaluated
the platform's technical performance, usability, and scientific utility. Users
conducted interventions such as activation patching, causal tracing, and
representation analysis on models including GPT-2 and DeepSeek-R1-70B. The
study revealed a gradual increase in user engagement, stable platform
performance throughout, and a positive reception of the remote experimentation
capabilities. It also marked the starting point for building a user community
around the platform. Identified limitations such as prolonged download
durations for activation data as well as intermittent execution interruptions
are addressed in the roadmap for future development. This initiative marks a
significant step towards widespread accessibility of LLM interpretability
infrastructure in Europe and lays the groundwork for broader deployment,
expanded tooling, and sustained community collaboration in mechanistic
interpretability research.

</details>


### [64] [Neural Machine Translation for Coptic-French: Strategies for Low-Resource Ancient Languages](https://arxiv.org/abs/2508.10683)
*Nasma Chaoui,Richard Khoury*

Main category: cs.CL

TL;DR: 首个系统研究科普特语译法语的方法，通过多样化训练数据提升翻译质量


<details>
  <summary>Details</summary>
Motivation: 针对历史语言翻译工具开发的空白，首次系统研究科普特语译法语的策略，为其他历史语言提供通用解决方案

Method: 使用圣经平行语料库，系统评估枢轴/直译模式、预训练影响、多版本微调和抗噪能力，采用风格多样化及噪声感知的微调策略

Result: 混合风格和抗噪声训练使翻译质量显著提升，BLEU值提升15%

Conclusion: 研究结果为历史语言机器翻译提供关键实践范式，验证了数据多样性策略的普适价值

Abstract: This paper presents the first systematic study of strategies for translating
Coptic into French. Our comprehensive pipeline systematically evaluates: pivot
versus direct translation, the impact of pre-training, the benefits of
multi-version fine-tuning, and model robustness to noise. Utilizing aligned
biblical corpora, we demonstrate that fine-tuning with a stylistically-varied
and noise-aware training corpus significantly enhances translation quality. Our
findings provide crucial practical insights for developing translation tools
for historical languages in general.

</details>


### [65] [Continuous Bangla Sign Language Translation: Mitigating the Expense of Gloss Annotation with the Assistance of Graph](https://arxiv.org/abs/2508.10687)
*Safaeid Hossain Arib,Rabeya Akter,Sejuti Rahman*

Main category: cs.CL

TL;DR: 通过融合Transformer和STGCN-LSTM架构提升手语翻译效果，在多个数据集创下新SOTA并首次建立BornilDB基准


<details>
  <summary>Details</summary>
Motivation: 解决手语在社会中被低估导致的沟通障碍问题，改进现有基于Transformer方法的局限性

Method: 将图神经网络(STGCN)与Transformer架构融合，采用编码器-解码器框架进行gloss-free翻译

Result: 在RWTH-PHOENIX-2014T等四个数据集上BLEU-4提升最高达4.01，首次建立BornilDB基准结果

Conclusion: 架构融合方法为手语翻译设立新标杆，强调gloss-free翻译对提升听障群体沟通可及性的重要性

Abstract: Millions of individuals worldwide are affected by deafness and hearing
impairment. Sign language serves as a sophisticated means of communication for
the deaf and hard of hearing. However, in societies that prioritize spoken
languages, sign language often faces underestimation, leading to communication
barriers and social exclusion. The Continuous Bangla Sign Language Translation
project aims to address this gap by enhancing translation methods. While recent
approaches leverage transformer architecture for state-of-the-art results, our
method integrates graph-based methods with the transformer architecture. This
fusion, combining transformer and STGCN-LSTM architectures, proves more
effective in gloss-free translation. Our contributions include architectural
fusion, exploring various fusion strategies, and achieving a new
state-of-the-art performance on diverse sign language datasets, namely
RWTH-PHOENIX-2014T, CSL-Daily, How2Sign, and BornilDB v1.0. Our approach
demonstrates superior performance compared to current translation outcomes
across all datasets, showcasing notable improvements of BLEU-4 scores of 4.01,
2.07, and 0.5, surpassing those of GASLT, GASLT and slt_how2sign in
RWTH-PHOENIX-2014T, CSL-Daily, and How2Sign, respectively. Also, we introduce
benchmarking on the BornilDB v1.0 dataset for the first time. Our method sets a
benchmark for future research, emphasizing the importance of gloss-free
translation to improve communication accessibility for the deaf and hard of
hearing.

</details>


### [66] [Learning from Natural Language Feedback for Personalized Question Answering](https://arxiv.org/abs/2508.10695)
*Alireza Salemi,Hamed Zamani*

Main category: cs.CL

TL;DR: 提出VAC框架，通过自然语言反馈替代标量奖励信号，显著提升个性化问答系统的效果


<details>
  <summary>Details</summary>
Motivation: 现有基于标量奖励的个性化方法反馈信号薄弱，限制了模型学习效率和个性化质量

Method: 开发交替训练框架：1) 基于用户画像生成自然语言反馈 2) 通过反馈迭代优化策略模型 3) 最终实现无反馈推理

Result: 在LaMP-QA基准测试中三个领域均取得SOTA结果，人工评估显示响应质量显著优于现有方法

Conclusion: 自然语言反馈为个性化任务提供了更有效的优化信号，该方法可扩展至其他语言生成任务

Abstract: Personalization is crucial for enhancing both the effectiveness and user
satisfaction of language technologies, particularly in information-seeking
tasks like question answering. Current approaches for personalizing large
language models (LLMs) often rely on retrieval-augmented generation (RAG),
followed by reinforcement learning with scalar reward signals to teach models
how to use retrieved personal context. We believe that these scalar rewards
sometimes provide weak, non-instructive feedback, limiting learning efficiency
and personalization quality. We introduce VAC, a novel framework for
personalized response generation that replaces scalar rewards with natural
language feedback (NLF) that are generated conditioned on the user profiles and
the question narratives. NLF serves as a rich and actionable supervision
signal, allowing the policy model to iteratively refine its outputs and
internalize effective personalization strategies. Training alternates between
optimizing the feedback model and fine-tuning the policy model on the improved
responses, resulting in a policy model that no longer requires feedback at
inference. Evaluation on the LaMP-QA benchmark that consists of three diverse
domains demonstrates consistent and significant improvements over the
state-of-the-art results. Human evaluations further confirm the superior
quality of the generated responses. These results demonstrate that NLF provides
more effective signals for optimizing personalized question answering.

</details>


### [67] [Thinking Inside the Mask: In-Place Prompting in Diffusion LLMs](https://arxiv.org/abs/2508.10736)
*Xiangqi Jin,Yuxuan Wang,Yifeng Gao,Zichen Wen,Biqing Qi,Dongrui Liu,Linfeng Zhang*

Main category: cs.CL

TL;DR: ICE框架通过就地链式思维提示(IN-PLACE Chain-of-Thought)与提前退出机制，显著提升扩散大语言模型(dLLMs)的推理效率和性能，在GSM8K和MMLU等基准测试中分别实现最高17.29%准确率提升与276.67倍加速效果


<details>
  <summary>Details</summary>
Motivation: 针对传统大语言模型(LLMs)前缀式提示范式在双向信息利用和灵活性上的局限性，利用扩散语言模型(dLLMs)特有的双向注意力机制和迭代优化特性，开发更高效的提示策略框架

Method: 1. 在迭代优化过程中将提示直接嵌入掩码标记位置
2. 开发置信度感知的提前退出机制，通过动态终止低置信度样本的计算过程降低资源消耗

Result: 实验显示：
- GSM8K数学推理：准确率↑17.29%，推理速度↑4.12倍
- MMLU知识问答：计算加速↑276.67倍，性能保持竞争力
- 综合效率提升显著优于传统提示方法

Conclusion: ICE框架通过结构创新有效平衡扩散语言模型的性能与计算效率，为实时推理场景提供了可行性验证，拓展了双向注意力机制在提示工程中的应用边界

Abstract: Despite large language models (LLMs) have achieved remarkable success, their
prefix-only prompting paradigm and sequential generation process offer limited
flexibility for bidirectional information. Diffusion large language models
(dLLMs) present new opportunities through their bidirectional attention
mechanisms and iterative refinement processes, enabling more flexible in-place
prompting strategies. We introduce ICE (In-Place Chain-of-Thought Prompting
with Early Exit), a novel framework that transforms prefix-only prompting into
in-place prompting specifically designed for dLLMs. ICE integrates in-place
prompts directly within masked token positions during iterative refinement and
employs a confidence-aware early exit mechanism to significantly reduce
computational overhead. Extensive experiments demonstrate ICE's effectiveness,
achieving up to 17.29% accuracy improvement with 4.12$\times$ speedup on GSM8K,
and up to 276.67$\times$ acceleration on MMLU while maintaining competitive
performance.

</details>


### [68] [Beyond "Not Novel Enough": Enriching Scholarly Critique with LLM-Assisted Feedback](https://arxiv.org/abs/2508.10795)
*Osama Mohammed Afzal,Preslav Nakov,Tom Hope,Iryna Gurevych*

Main category: cs.CL

TL;DR: 提出基于结构化LLM的三阶段自动化论文新颖性评估方法，在人类评估一致性上显著超越现有基线


<details>
  <summary>Details</summary>
Motivation: 解决同行评审中新颖性评估标准化不足的问题，特别是在NLP等高产领域人工评审容量不足的痛点

Method: 1. 内容提取 → 2. 相关文献检索与综合 → 3. 结构化对比分析，通过大规模人类评审数据分析验证独立主张和上下文推理模式

Result: 在182篇ICLR 2025投稿中实现与人类推理86.5%对齐，新颖性结论75.3%一致，较现有LLM基线显著提升，且分析更细致连贯

Conclusion: 结构化LLM辅助方法可提升评审严谨性与透明度，与人类专业知识形成互补，数据代码已开源推动领域发展

Abstract: Novelty assessment is a central yet understudied aspect of peer review,
particularly in high volume fields like NLP where reviewer capacity is
increasingly strained. We present a structured approach for automated novelty
evaluation that models expert reviewer behavior through three stages: content
extraction from submissions, retrieval and synthesis of related work, and
structured comparison for evidence based assessment. Our method is informed by
a large scale analysis of human written novelty reviews and captures key
patterns such as independent claim verification and contextual reasoning.
Evaluated on 182 ICLR 2025 submissions with human annotated reviewer novelty
assessments, the approach achieves 86.5% alignment with human reasoning and
75.3% agreement on novelty conclusions - substantially outperforming existing
LLM based baselines. The method produces detailed, literature aware analyses
and improves consistency over ad hoc reviewer judgments. These results
highlight the potential for structured LLM assisted approaches to support more
rigorous and transparent peer review without displacing human expertise. Data
and code are made available.

</details>


### [69] [Reinforced Language Models for Sequential Decision Making](https://arxiv.org/abs/2508.10839)
*Jim Dilkes,Vahid Yazdanpanah,Sebastian Stein*

Main category: cs.CL

TL;DR: 提出MS-GRPO算法用于小参数模型的后训练，通过多步信用分配机制提升语言模型的序列决策能力，使3B模型在Frozen Lake任务上超越72B基线50%


<details>
  <summary>Details</summary>
Motivation: 现有后训练方法专注于单轮交互，无法解决多步智能体任务中的信用分配问题，需开发适用于多步决策任务的高效训练方法

Method: 基于文本媒介随机博弈(TSMG)和语言智能体策略(LAP)框架，提出多步组相对策略优化(MS-GRPO)算法，结合绝对优势加权的episode采样策略

Result: 3B参数后训练模型在Snake和Frozen Lake任务中表现优异，Frozen Lake任务上性能超越72B基线模型50%

Conclusion: 针对性的后训练可替代模型规模化路径，是构建序列决策语言智能体的高效解决方案

Abstract: Large Language Models (LLMs) show potential as sequential decision-making
agents, but their application is often limited due to a reliance on large,
computationally expensive models. This creates a need to improve smaller
models, yet existing post-training methods are designed for single-turn
interactions and cannot handle credit assignment in multi-step agentic tasks.
To address this, we introduce Multi-Step Group-Relative Policy Optimization
(MS-GRPO), a new algorithm for post-training LLM agents, grounded in formal
Text-Mediated Stochastic Game (TSMG) and Language-Agent Policy (LAP)
frameworks. For credit assignment, MS-GRPO attributes the entire cumulative
episode reward to each individual episode step. We supplement this algorithm
with a novel absolute-advantage-weighted episode sampling strategy that we show
improves training performance. We evaluate our approach by post-training a
3-billion parameter model on Snake and Frozen Lake. Our experiments demonstrate
that the method is effective in improving decision-making performance: our
post-trained 3B parameter model outperforms a 72B parameter baseline by 50% on
the Frozen Lake task. This work demonstrates that targeted post-training is a
practical and efficient alternative to relying on model scale for creating
sequential decision-making agents using LLMs.

</details>


### [70] [Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy, Expertise, and Reasoning](https://arxiv.org/abs/2508.10848)
*Chongyuan Dai,Jinpeng Hu,Hongchang Shi,Zhuo Li,Xun Yang,Meng Wang*

Main category: cs.CL

TL;DR: 首个中文心理大模型Psyche-R1，通过创新数据管道整合共情力、心理学专业知识和推理能力，在多项心理测评中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有心理领域研究过度关注情感支持而忽视推理机制，Psyche-R1旨在填补心理大模型中专业推理能力的空白。

Method: 1. 设计数据合成管道生成7.5万带详细推理链的心理问题+7.3万共情对话
2. 采用混合训练策略：多LLM交叉筛选困难样本进行GRPO优化推理，剩余数据SFT增强共情和专业能力

Result: 7B参数的Psyche-R1在心理测评中达到与671B参数DeepSeek-R1相当的效果

Conclusion: 通过系统性的数据构建和混合训练策略，成功实现了共情、专业与推理能力的协同优化，验证了模型在心理领域的应用潜力

Abstract: Amidst a shortage of qualified mental health professionals, the integration
of large language models (LLMs) into psychological applications offers a
promising way to alleviate the growing burden of mental health disorders.
Recent reasoning-augmented LLMs have achieved remarkable performance in
mathematics and programming, while research in the psychological domain has
predominantly emphasized emotional support and empathetic dialogue, with
limited attention to reasoning mechanisms that are beneficial to generating
reliable responses. Therefore, in this paper, we propose Psyche-R1, the first
Chinese psychological LLM that jointly integrates empathy, psychological
expertise, and reasoning, built upon a novel data curation pipeline.
Specifically, we design a comprehensive data synthesis pipeline that produces
over 75k high-quality psychological questions paired with detailed rationales,
generated through chain-of-thought (CoT) reasoning and iterative
prompt-rationale optimization, along with 73k empathetic dialogues.
Subsequently, we employ a hybrid training strategy wherein challenging samples
are identified through a multi-LLM cross-selection strategy for group relative
policy optimization (GRPO) to improve reasoning ability, while the remaining
data is used for supervised fine-tuning (SFT) to enhance empathetic response
generation and psychological domain knowledge. Extensive experiment results
demonstrate the effectiveness of the Psyche-R1 across several psychological
benchmarks, where our 7B Psyche-R1 achieves comparable results to 671B
DeepSeek-R1.

</details>


### [71] [From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms](https://arxiv.org/abs/2508.10860)
*Zhaokun Jiang,Ziyin Zhang*

Main category: cs.CL

TL;DR: 提出基于特征工程、数据增强和可解释机器学习的三维框架，通过可解释特征和SHAP分析提升口译质量评估的透明度


<details>
  <summary>Details</summary>
Motivation: 解决现有口译质量评估研究中语言维度研究不足、数据稀缺导致模型效果欠佳、预测结果缺乏可解释性的问题

Method: 整合特征工程（构造相关透明特征）、数据增强技术和可解释机器学习（SHAP分析）的三维建模框架

Result: 在英汉交替传译数据集验证：BLEURT/CometKiwi预测忠实度最佳，停顿特征预测流畅度，中文短语多样性指标预测语言使用

Conclusion: 通过强调可解释性，构建了可扩展的透明评估体系，支持学习者诊断性反馈与自我调节学习

Abstract: Recent advancements in machine learning have spurred growing interests in
automated interpreting quality assessment. Nevertheless, existing research
suffers from insufficient examination of language use quality, unsatisfactory
modeling effectiveness due to data scarcity and imbalance, and a lack of
efforts to explain model predictions. To address these gaps, we propose a
multi-dimensional modeling framework that integrates feature engineering, data
augmentation, and explainable machine learning. This approach prioritizes
explainability over ``black box'' predictions by utilizing only
construct-relevant, transparent features and conducting Shapley Value (SHAP)
analysis. Our results demonstrate strong predictive performance on a novel
English-Chinese consecutive interpreting dataset, identifying BLEURT and
CometKiwi scores to be the strongest predictive features for fidelity,
pause-related features for fluency, and Chinese-specific phraseological
diversity metrics for language use. Overall, by placing particular emphasis on
explainability, we present a scalable, reliable, and transparent alternative to
traditional human evaluation, facilitating the provision of detailed diagnostic
feedback for learners and supporting self-regulated learning advantages not
afforded by automated scores in isolation.

</details>


### [72] [SSRL: Self-Search Reinforcement Learning](https://arxiv.org/abs/2508.10874)
*Yuchen Fan,Kaiyan Zhang,Heng Zhou,Yuxin Zuo,Yanxu Chen,Yu Fu,Xinwei Long,Xuekai Zhu,Che Jiang,Yuchen Zhang,Li Kang,Gang Chen,Cheng Huang,Zhizhou He,Bingning Wang,Lei Bai,Ning Ding,Bowen Zhou*

Main category: cs.CL

TL;DR: 探索大型语言模型作为强化学习代理搜索任务模拟器的潜力，通过自搜索机制减少对外部搜索引擎的依赖。


<details>
  <summary>Details</summary>
Motivation: 利用LLMs内在的世界知识替代昂贵的外部搜索交互，为强化学习训练提供更高效稳定的环境。

Method: 提出Self-Search量化模型内在搜索能力，并通过SSRL（格式奖励+规则奖励）实现知识利用的迭代优化。

Result: SSRL训练的策略模型可降低80%外部搜索依赖，支持sim-to-real迁移且幻觉率降低37%。

Conclusion: LLMs具备可激发的世界知识，SSRL证实利用内部知识可减少幻觉，模型与外部搜索引擎无缝兼容。

Abstract: We investigate the potential of large language models (LLMs) to serve as
efficient simulators for agentic search tasks in reinforcement learning (RL),
thereby reducing dependence on costly interactions with external search
engines. To this end, we first quantify the intrinsic search capability of LLMs
via structured prompting and repeated sampling, which we term Self-Search. Our
results reveal that LLMs exhibit strong scaling behavior with respect to the
inference budget, achieving high pass@k on question-answering benchmarks,
including the challenging BrowseComp task. Building on these observations, we
introduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability
through format-based and rule-based rewards. SSRL enables models to iteratively
refine their knowledge utilization internally, without requiring access to
external tools. Empirical evaluations demonstrate that SSRL-trained policy
models provide a cost-effective and stable environment for search-driven RL
training, reducing reliance on external search engines and facilitating robust
sim-to-real transfer. We draw the following conclusions: 1) LLMs possess world
knowledge that can be effectively elicited to achieve high performance; 2) SSRL
demonstrates the potential of leveraging internal knowledge to reduce
hallucination; 3) SSRL-trained models integrate seamlessly with external search
engines without additional effort. Our findings highlight the potential of LLMs
to support more scalable RL agent training.

</details>


### [73] [A Survey on Diffusion Language Models](https://arxiv.org/abs/2508.10875)
*Tianyi Li,Mingda Chen,Bowei Guo,Zhiqiang Shen*

Main category: cs.CL

TL;DR: 扩散语言模型（DLMs）通过并行迭代去噪生成，在推理延迟和上下文捕捉方面超越自回归模型，成为NLP任务新选择。


<details>
  <summary>Details</summary>
Motivation: 探索DLMs如何通过并行生成机制突破传统自回归模型的序列生成限制，并实现多倍加速下的可比性能表现。

Method: 系统梳理DLMs发展脉络，涵盖预训练策略、推理优化（并行解码/缓存机制）、多模态扩展及实际应用场景分析。

Result: DLMs在保持与自回归模型相当性能的同时实现数倍加速，成功应用于文本生成控制、多模态任务等领域。

Conclusion: 尽管面临效率优化和长序列处理等挑战，DLMs通过持续改进训练策略与硬件适配，展现出替代传统范式的潜力。

Abstract: Diffusion Language Models (DLMs) are rapidly emerging as a powerful and
promising alternative to the dominant autoregressive (AR) paradigm. By
generating tokens in parallel through an iterative denoising process, DLMs
possess inherent advantages in reducing inference latency and capturing
bidirectional context, thereby enabling fine-grained control over the
generation process. While achieving a several-fold speed-up, recent
advancements have allowed DLMs to show performance comparable to their
autoregressive counterparts, making them a compelling choice for various
natural language processing tasks. In this survey, we provide a holistic
overview of the current DLM landscape. We trace its evolution and relationship
with other paradigms, such as autoregressive and masked language models, and
cover both foundational principles and state-of-the-art models. Our work offers
an up-to-date, comprehensive taxonomy and an in-depth analysis of current
techniques, from pre-training strategies to advanced post-training methods.
Another contribution of this survey is a thorough review of DLM inference
strategies and optimizations, including improvements in decoding parallelism,
caching mechanisms, and generation quality. We also highlight the latest
approaches to multimodal extensions of DLMs and delineate their applications
across various practical scenarios. Furthermore, our discussion addresses the
limitations and challenges of DLMs, including efficiency, long-sequence
handling, and infrastructure requirements, while outlining future research
directions to sustain progress in this rapidly evolving field. Project GitHub
is available at https://github.com/VILA-Lab/Awesome-DLMs.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [74] [B-repLer: Semantic B-rep Latent Editor using Large Language Models](https://arxiv.org/abs/2508.10201)
*Yilin Liu,Niladri Shekhar Dutt,Changjian Li,Niloy J. Mitra*

Main category: cs.GR

TL;DR: 提出B-repLer模型，通过微调多模态大语言模型实现基于文本的B-rep CAD语义编辑，突破3D领域数据瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有mLLMs在3D编辑任务中效果受限，主要由于B-rep数据标注稀缺与表示脆弱性，需探索适配方案。

Method: 设计专用多模态架构处理B-rep数据，结合CAD工具自动生成训练数据，实现无标注微调。

Result: 成功展示多种复杂文本驱动B-rep编辑案例，验证模型在保持几何有效性方面的优势。

Conclusion: B-repLer通过创新架构与数据生成机制，拓展了mLLMs在工程CAD领域的实际应用边界。

Abstract: Multimodal large language models (mLLMs), trained in a mixed modal setting as
a universal model, have been shown to compete with or even outperform many
specialized algorithms for imaging and graphics tasks. As demonstrated across
many applications, mLLMs' ability to jointly process image and text data makes
them suitable for zero-shot applications or efficient fine-tuning towards
specialized tasks. However, they have had limited success in 3D analysis and
editing tasks. This is due to both the lack of suitable (annotated) 3D data as
well as the idiosyncrasies of 3D representations. In this paper, we investigate
whether mLLMs can be adapted to support high-level editing of Boundary
Representation (B-rep) CAD objects. B-reps remain the industry-standard for
precisely encoding engineering objects, but are challenging as the
representation is fragile (i.e. can easily lead to invalid CAD objects) and no
publicly available data source exists with semantically-annotated B-reps or CAD
construction history. We present B-repLer as a finetuned mLLM that can
understand text prompts and make semantic edits on given B-Reps to produce
valid outputs. We enable this via a novel multimodal architecture, specifically
designed to handle B-rep models, and demonstrate how existing CAD tools, in
conjunction with mLLMs, can be used to automatically generate the required
reasoning dataset, without relying on external annotations. We extensively
evaluate B-repLer and demonstrate several text-based B-rep edits of various
complexity, which were not previously possible.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [75] [CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model](https://arxiv.org/abs/2508.10416)
*Zhuoyuan Yu,Yuxing Long,Zihan Yang,Chengyan Zeng,Hongwei Fan,Jiyao Zhang,Hao Dong*

Main category: cs.RO

TL;DR: 提出自我纠正飞轮范式，通过迭代利用错误轨迹生成自纠正数据，显著提升导航模型性能


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言导航模型缺乏有效纠错能力，错误轨迹未被充分利用作为训练资源

Method: 构建自我纠正飞轮机制：1) 识别错误轨迹偏差 2) 自动生成感知与动作的自纠正数据 3) 通过多轮迭代持续训练

Result: R2R-CE/RxR-CE基准分别达65.1%/69.3%成功率（提升8.2%/16.4%），真实测试展示动态避障与长指令跟随能力

Conclusion: 将错误轨迹转化为训练资源的新范式，成功实现模型自我进化，推动导航系统实际应用

Abstract: Existing vision-and-language navigation models often deviate from the correct
trajectory when executing instructions. However, these models lack effective
error correction capability, hindering their recovery from errors. To address
this challenge, we propose Self-correction Flywheel, a novel post-training
paradigm. Instead of considering the model's error trajectories on the training
set as a drawback, our paradigm emphasizes their significance as a valuable
data source. We have developed a method to identify deviations in these error
trajectories and devised innovative techniques to automatically generate
self-correction data for perception and action. These self-correction data
serve as fuel to power the model's continued training. The brilliance of our
paradigm is revealed when we re-evaluate the model on the training set,
uncovering new error trajectories. At this time, the self-correction flywheel
begins to spin. Through multiple flywheel iterations, we progressively enhance
our monocular RGB-based VLA navigation model CorrectNav. Experiments on R2R-CE
and RxR-CE benchmarks show CorrectNav achieves new state-of-the-art success
rates of 65.1% and 69.3%, surpassing prior best VLA navigation models by 8.2%
and 16.4%. Real robot tests in various indoor and outdoor environments
demonstrate \method's superior capability of error correction, dynamic obstacle
avoidance, and long instruction following.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [76] [Nested-ReFT: Efficient Reinforcement Learning for Large Language Model Fine-Tuning via Off-Policy Rollouts](https://arxiv.org/abs/2508.10123)
*Maxime Heuillet,Yufei Cui,Boxing Chen,Audrey Durand,Prasanna Parthasarathi*

Main category: cs.LG

TL;DR: 提出Nested-ReFT框架，通过动态层跳过和离策略梯度估计，在保持强化微调性能的同时显著提升计算效率


<details>
  <summary>Details</summary>
Motivation: 传统ReFT方法需要多次生成完整回答导致计算成本过高，需开发更高效的训练范式

Method: 结合离策略RL和推测解码技术，使用目标模型子层作为行为模型，动态跳过非必要层计算，并设计三种梯度偏差缓解策略

Result: 在多个数学推理基准测试中实现计算效率（tokens/sec）显著提升，同时保持与基线ReFT相当的模型性能

Conclusion: Nested-ReFT有效平衡计算效率与模型性能，为大规模语言模型的高效推理训练提供新思路

Abstract: Advanced reasoning in LLMs on challenging domains like mathematical reasoning
can be tackled using verifiable rewards based reinforced fine-tuning (ReFT). In
standard ReFT frameworks, a behavior model generates multiple completions with
answers per problem, for the answer to be then scored by a reward function.
While such RL post-training methods demonstrate significant performance
improvements across challenging reasoning domains, the computational cost of
generating completions during training with multiple inference steps makes the
training cost non-trivial. To address this, we draw inspiration from off-policy
RL, and speculative decoding to introduce a novel ReFT framework, dubbed
Nested-ReFT, where a subset of layers of the target model acts as the behavior
model to generate off-policy completions during training. The behavior model
configured with dynamic layer skipping per batch during training decreases the
inference cost compared to the standard ReFT frameworks. Our theoretical
analysis shows that Nested-ReFT yields unbiased gradient estimates with
controlled variance. Our empirical analysis demonstrates improved computational
efficiency measured as tokens/sec across multiple math reasoning benchmarks and
model sizes. Additionally, we explore three variants of bias mitigation to
minimize the off-policyness in the gradient updates that allows for maintaining
performance that matches the baseline ReFT performance.

</details>


### [77] [Stabilizing Long-term Multi-turn Reinforcement Learning with Gated Rewards](https://arxiv.org/abs/2508.10548)
*Zetian Sun,Dongfang Li,Zhuoen Chen,Yuhuai Qin,Baotian Hu*

Main category: cs.LG

TL;DR: 针对长期强化学习的奖励稀疏问题，提出基于阈值控制的G-RA方法，在软件工程任务中实现93.8%的完成率提升


<details>
  <summary>Details</summary>
Motivation: 现有基于验证的奖励塑造方法存在即时奖励与长期目标错位问题，导致策略次优和奖励黑客行为

Method: 提出SWE-oriented RL框架（支持多轮交互和Docker执行）与门控奖励积累机制（G-RA），通过高层奖励阈值控制即时奖励积累

Result: 在SWE-bench和kBench上分别将完成率从47.6%/22.0%提升至93.8%/86.0%，修改率提升4.2%/30.0%

Conclusion: 平衡的奖励积累机制对长期RL任务至关重要，G-RA为解决奖励错位问题提供了有效方案

Abstract: Reward sparsity in long-horizon reinforcement learning (RL) tasks remains a
significant challenge, while existing outcome-based reward shaping struggles to
define meaningful immediate rewards without introducing bias or requiring
explicit task decomposition. Alternatively, verification-based reward shaping
uses stepwise critics, but misalignment between immediate rewards and long-term
objectives can lead to reward hacking and suboptimal policies. In this work, we
address this problem in the context of software engineering (SWE) tasks, where
multi-turn reasoning and rule-based verification are critical. We introduce the
SWE-oriented RL Framework, a unified system supporting multi-turn interaction,
docker-based execution, and customizable reward functions. Additionally, we
propose Gated Reward Accumulation (G-RA), a novel method that accumulates
immediate rewards only when high-level (long-term) rewards meet a predefined
threshold, ensuring stable RL optimization. Experiments on SWE-bench Verified
and kBench demonstrate that G-RA leads to an increase in completion rates
(47.6\% \rightarrow 93.8\% and 22.0\% \rightarrow 86.0\%) and modification
rates (19.6\% \rightarrow 23.8\% and 12.0\% \rightarrow 42.0\%), while avoiding
policy degradation caused by reward misalignment. Our findings highlight the
importance of balanced reward accumulation in long-horizon RL and provide a
practical solution.

</details>


### [78] [Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models](https://arxiv.org/abs/2508.10751)
*Zhipeng Chen,Xiaobo Qin,Youbin Wu,Yue Ling,Qinghao Ye,Wayne Xin Zhao,Guang Shi*

Main category: cs.LG

TL;DR: 提出使用Pass@k作为奖励函数提升强化学习的探索能力，通过理论推导实现高效训练过程，揭示探索与利用的协同效应。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR方法使用Pass@1奖励导致策略保守陷入局部最优，需探索更有效的奖励指标。现有工作中Pass@k虽被用于评估，但其对探索能力的影响机制尚未明确。

Method: 1. 采用Pass@k Training策略训练模型
2. 推导Pass@k Training的优势解析解
3. 提出优势函数设计框架

Result: 1. 探索与利用形成正反馈循环
2. 理论推导使训练效率提升3倍
3. 优势函数设计在HumanEval基准取得85.4%准确率

Conclusion: 突破传统探索-利用困境认知，为RLVR提供新的理论框架。优势函数设计方向展现出解决复杂RL问题的潜力。

Abstract: Reinforcement learning with verifiable rewards (RLVR), which typically adopts
Pass@1 as the reward, has faced the issues in balancing exploration and
exploitation, causing policies to prefer conservative actions, converging to a
local optimum. Identifying an appropriate reward metric is therefore crucial.
Regarding the prior work, although Pass@k has been used in evaluation, its
connection to LLM exploration ability in RLVR remains largely overlooked. To
investigate this, we first use Pass@k as the reward to train the policy model
(i.e., $\textbf{Pass@k Training}$), and observe the improvement on its
exploration ability. Next, we derive an analytical solution for the advantage
of Pass@k Training, leading to an efficient and effective process. Building on
this, our analysis reveals that exploration and exploitation are not inherently
conflicting objectives, while they can mutually enhance each other. Moreover,
Pass@k Training with analytical derivation essentially involves directly
designing the advantage function. Inspired by this, we preliminarily explore
the advantage design for RLVR, showing promising results and highlighting a
potential future direction.

</details>


### [79] [Memory-Augmented Transformers: A Systematic Review from Neuroscience Principles to Technical Solutions](https://arxiv.org/abs/2508.10824)
*Parsa Omidi,Xingshuai Huang,Axel Laborieux,Bahareh Nikpour,Tianyu Shi,Armaghan Eshaghi*

Main category: cs.LG

TL;DR: 提出结合神经科学原理的记忆增强型Transformer框架，解决现有模型在长期记忆和持续学习中的瓶颈


<details>
  <summary>Details</summary>
Motivation: Transformer架构虽擅长序列建模，但在长程上下文保留、持续学习和知识整合方面存在局限，需融合神经科学中的动态多时间尺度记忆机制

Method: 通过功能目标（上下文扩展/推理/知识整合）、记忆表示（参数编码/显式存储）和整合机制（注意力融合/门控控制）三维度构建记忆增强系统

Result: 揭示了从静态缓存转向测试时自适应学习系统的趋势，提出分层缓冲和意外门控更新等解决扩展性/干扰问题的新方案

Conclusion: 该框架为开发具有终身学习能力的类认知Transformer架构提供路线图，需持续解决可扩展性与记忆干扰的核心挑战

Abstract: Memory is fundamental to intelligence, enabling learning, reasoning, and
adaptability across biological and artificial systems. While Transformer
architectures excel at sequence modeling, they face critical limitations in
long-range context retention, continual learning, and knowledge integration.
This review presents a unified framework bridging neuroscience principles,
including dynamic multi-timescale memory, selective attention, and
consolidation, with engineering advances in Memory-Augmented Transformers. We
organize recent progress through three taxonomic dimensions: functional
objectives (context extension, reasoning, knowledge integration, adaptation),
memory representations (parameter-encoded, state-based, explicit, hybrid), and
integration mechanisms (attention fusion, gated control, associative
retrieval). Our analysis of core memory operations (reading, writing,
forgetting, and capacity management) reveals a shift from static caches toward
adaptive, test-time learning systems. We identify persistent challenges in
scalability and interference, alongside emerging solutions including
hierarchical buffering and surprise-gated updates. This synthesis provides a
roadmap toward cognitively-inspired, lifelong-learning Transformer
architectures.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [80] [Personalized Real-time Jargon Support for Online Meetings](https://arxiv.org/abs/2508.10239)
*Yifan Song,Wing Yee Au,Hon Yung Wong,Brian P. Bailey,Tal August*

Main category: cs.HC

TL;DR: 研究者开发了ParseJargon系统，通过个性化LLM支持解决跨学科交流中的术语障碍问题。实验证明个性化解释显著提升会议效果，通用方案则适得其反。


<details>
  <summary>Details</summary>
Motivation: 专业术语阻碍跨学科交流，现有术语管理策略在实时会议场景中存在明显局限性

Method: 通过日记研究发现问题→设计个性化LLM系统→开展对照实验（个性化/通用/无支持）→实施实地验证

Result: 个性化支持提升理解度(13%)、参与度(18%)和工作认可度(22%)，通用支持降低参与度；实地验证显示系统具备实用价值

Conclusion: 研究证实个性化术语支持工具的有效性，为跨学科协作和教育应用提供了新范式，同时揭示实际部署中的技术边界

Abstract: Effective interdisciplinary communication is frequently hindered by
domain-specific jargon. To explore the jargon barriers in-depth, we conducted a
formative diary study with 16 professionals, revealing critical limitations in
current jargon-management strategies during workplace meetings. Based on these
insights, we designed ParseJargon, an interactive LLM-powered system providing
real-time personalized jargon identification and explanations tailored to
users' individual backgrounds. A controlled experiment comparing ParseJargon
against baseline (no support) and general-purpose (non-personalized) conditions
demonstrated that personalized jargon support significantly enhanced
participants' comprehension, engagement, and appreciation of colleagues' work,
whereas general-purpose support negatively affected engagement. A follow-up
field study validated ParseJargon's usability and practical value in real-time
meetings, highlighting both opportunities and limitations for real-world
deployment. Our findings contribute insights into designing personalized jargon
support tools, with implications for broader interdisciplinary and educational
applications.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [81] [SaraCoder: Orchestrating Semantic and Structural Cues for Profit-Oriented Repository-Level Code Completion](https://arxiv.org/abs/2508.10068)
*Xiaohan Chen,Zhongying Pan,Quan Feng,Yu Tian,Shuqun Yang,Mengru Wang,Lina Gong,Yuxia Geng,Piji Li,Xiang Chen*

Main category: cs.SE

TL;DR: Saracoder提出分层特征优化的检索框架，通过语义关系蒸馏、图结构相似度评估和外部符号消歧，显著提升仓库级代码补全效果。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法依赖文本相似性检索，导致语义误导/冗余/同质化问题，且无法解决跨文件符号歧义。

Method: 1. 分层特征优化模块：语义关系蒸馏、重复剪枝、基于图拓扑权重的结构相似度评估、多样性最大化重排序
2. 外部感知标识符消歧模块：依赖分析解决跨文件符号歧义

Result: 在CrossCodeEval和RepoEval-Updated基准测试中，Saracoder显著超越现有基线，支持多编程语言和模型。

Conclusion: 通过多维度系统化优化检索结果，为构建更精准鲁棒的仓库级代码补全系统提供了新范式。

Abstract: Retrieval-augmented generation (RAG) for repository-level code completion
commonly relies on superficial text similarity, leading to results plagued by
semantic misguidance, redundancy, and homogeneity, while also failing to
resolve external symbol ambiguity. To address these challenges, we introduce
Saracoder, a Hierarchical Feature-Optimized retrieval framework. Its core
Hierarchical Feature Optimization module systematically refines candidates by
distilling deep semantic relationships, pruning exact duplicates, assessing
structural similarity with a novel graph-based metric that weighs edits by
their topological importance, and reranking results to maximize both relevance
and diversity. Furthermore, an External-Aware Identifier Disambiguator module
accurately resolves cross-file symbol ambiguity via dependency analysis.
Extensive experiments on the challenging CrossCodeEval and RepoEval-Updated
benchmarks demonstrate that Saracoder significantly outperforms existing
baselines across multiple programming languages and models. Our work proves
that systematically refining retrieval results across multiple dimensions
provides a new paradigm for building more accurate and robust repository-level
code completion systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [82] [Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development](https://arxiv.org/abs/2508.10108)
*Sattvik Sahai,Prasoon Goyal,Michael Johnston,Anna Gottardi,Yao Lu,Lucy Hu,Luke Dai,Shaohua Liu,Samyuth Sagi,Hangjie Shi,Desheng Zhang,Lavina Vaz,Leslie Ball,Maureen Murray,Rahul Gupta,Shankar Ananthakrishna*

Main category: cs.AI

TL;DR: 亚马逊Nova AI挑战赛通过红队与AI助手对抗模式，推动软件开发中AI安全技术的创新，开发出推理安全对齐、模型防护等多维度解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决AI在软件开发过程中存在的安全隐患，通过全球高校对抗竞赛形式激发安全技术创新，建立系统性评估框架。

Method: 1. 设置红队与AI助手对抗锦标赛
2. 提供标注数据闭环优化
3. 开发基线模型+赛事协调服务+评估体系

Result: 产出四大技术突破：
- 基于推理的安全对齐
- 鲁棒模型防护机制
- 多轮越狱攻击技术
- LLM高效探测方法
亚马逊配套建设：专用编程模型、赛事管理系统、评估工具链

Conclusion: 该竞赛证明对抗测试能有效提升AI安全性，产学研协同创新为AI安全领域建立了新的技术标杆，推动行业安全标准演进。

Abstract: AI systems for software development are rapidly gaining prominence, yet
significant challenges remain in ensuring their safety. To address this, Amazon
launched the Trusted AI track of the Amazon Nova AI Challenge, a global
competition among 10 university teams to drive advances in secure AI. In the
challenge, five teams focus on developing automated red teaming bots, while the
other five create safe AI assistants. This challenge provides teams with a
unique platform to evaluate automated red-teaming and safety alignment methods
through head-to-head adversarial tournaments where red teams have multi-turn
conversations with the competing AI coding assistants to test their safety
alignment. Along with this, the challenge provides teams with a feed of high
quality annotated data to fuel iterative improvement. Throughout the challenge,
teams developed state-of-the-art techniques, introducing novel approaches in
reasoning-based safety alignment, robust model guardrails, multi-turn
jail-breaking, and efficient probing of large language models (LLMs). To
support these efforts, the Amazon Nova AI Challenge team made substantial
scientific and engineering investments, including building a custom baseline
coding specialist model for the challenge from scratch, developing a tournament
orchestration service, and creating an evaluation harness. This paper outlines
the advancements made by university teams and the Amazon Nova AI Challenge team
in addressing the safety challenges of AI for software development,
highlighting this collaborative effort to raise the bar for AI safety.

</details>


### [83] [Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model](https://arxiv.org/abs/2508.10492)
*Shicheng Xu,Xin Huang,Zihao Wei,Liang Pang,Huawei Shen,Xueqi Cheng*

Main category: cs.AI

TL;DR: 提出DxDirector-7B模型，通过将AI定位为诊断主导者、医生作为辅助的新范式，实现全流程临床诊断自动化，显著提升诊断准确性和效率，减少医生工作量。


<details>
  <summary>Details</summary>
Motivation: 现有AI在临床诊断中仅作为医生助手，无法主导从模糊主诉开始的完整诊断流程，限制了AI减少医生负担和提升效率的潜力。

Method: 开发具备深度思考能力的DxDirector-7B大语言模型，建立AI误诊责任框架，通过罕见/复杂/真实病例进行全流程诊断评估。

Result: 诊断准确率显著优于现有医疗LLM和通用LLM，医生工作量大幅降低，专家评估显示其具备替代专科医生的潜力。

Conclusion: 该研究标志着AI从医生助手转变为诊断主导者的新时代，为高效精准诊断提供创新解决方案。

Abstract: Full-process clinical diagnosis in the real world encompasses the entire
diagnostic workflow that begins with only an ambiguous chief complaint. While
artificial intelligence (AI), particularly large language models (LLMs), is
transforming clinical diagnosis, its role remains largely as an assistant to
physicians. This AI-assisted working pattern makes AI can only answer specific
medical questions at certain parts within the diagnostic process, but lack the
ability to drive the entire diagnostic process starting from an ambiguous
complaint, which still relies heavily on human physicians. This gap limits AI's
ability to fully reduce physicians' workload and enhance diagnostic efficiency.
To address this, we propose a paradigm shift that reverses the relationship
between physicians and AI: repositioning AI as the primary director, with
physicians serving as its assistants. So we present DxDirector-7B, an LLM
endowed with advanced deep thinking capabilities, enabling it to drive the
full-process diagnosis with minimal physician involvement. Furthermore,
DxDirector-7B establishes a robust accountability framework for misdiagnoses,
delineating responsibility between AI and human physicians. In evaluations
across rare, complex, and real-world cases under full-process diagnosis
setting, DxDirector-7B not only achieves significant superior diagnostic
accuracy but also substantially reduces physician workload than
state-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained
analyses across multiple clinical departments and tasks validate its efficacy,
with expert evaluations indicating its potential to serve as a viable
substitute for medical specialists. These findings mark a new era where AI,
traditionally a physicians' assistant, now drives the entire diagnostic process
to drastically reduce physicians' workload, indicating an efficient and
accurate diagnostic solution.

</details>


### [84] [Diversity First, Quality Later: A Two-Stage Assumption for Language Model Alignment](https://arxiv.org/abs/2508.10530)
*Zetian Sun,Dongfang Li,Baotian Hu*

Main category: cs.AI

TL;DR: 提出语言模型对齐的'阶段假设理论'，将优化过程分为偏好注入和微调两阶段，实验验证不同数据策略在5个模型和2种方法中的有效性差异


<details>
  <summary>Details</summary>
Motivation: 发现静态数据与动态策略采样在模型对齐中存在系统性效果差异（如Llama-3效果提升3倍而Zephyr下降0.4倍），需解释不同数据策略的有效性边界

Method: 提出两阶段对齐假设，通过理论分析和边界测量算法，在5个主流模型（Llama/Zephyr等）和2种对齐方法（DPO/SLiC-HF）上进行验证

Result: 实验证明不同阶段对数据特性需求不同：偏好注入阶段需要多样性数据，微调阶段需要高质量数据，边界测量算法能有效识别阶段转换点

Conclusion: 阶段假设理论为语言模型对齐提供了新的优化框架，指导不同训练阶段采用最优数据策略，提升对齐效率和模型性能

Abstract: The alignment of language models (LMs) with human preferences is critical for
building reliable AI systems. The problem is typically framed as optimizing an
LM policy to maximize the expected reward that reflects human preferences.
Recently, Direct Preference Optimization (DPO) was proposed as a LM alignment
method that directly optimize the policy from static preference data, and
further improved by incorporating on-policy sampling (i.e., preference
candidates generated during the training loop) for better LM alignment.
However, we show on-policy data is not always optimal, with systematic
effectiveness difference emerging between static and on-policy preference
candidates. For example, on-policy data can result in a 3$\times$ effectiveness
compared with static data for Llama-3, and a 0.4$\times$ effectiveness for
Zephyr. To explain the phenomenon, we propose the alignment stage assumption,
which divides the alignment process into two distinct stages: the preference
injection stage, which benefits from diverse data, and the preference
fine-tuning stage, which favors high-quality data. Through theoretical and
empirical analysis, we characterize these stages and propose an effective
algorithm to identify the boundaries between them. We perform experiments on 5
models (Llama, Zephyr, Phi-2, Qwen, Pythia) and 2 alignment methods (DPO,
SLiC-HF) to show the generalizability of alignment stage assumption and
boundary measurement.

</details>


### [85] [Improving Value-based Process Verifier via Low-Cost Variance Reduction](https://arxiv.org/abs/2508.10539)
*Zetian Sun,Dongfang Li,Baotian Hu,Min Zhang*

Main category: cs.AI

TL;DR: 提出ComMCS方法，通过组合蒙特卡洛采样减少方差，提升大语言模型在数学推理中的效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于价值的验证器因训练标注的估计误差（高方差问题）受限，主要由于蒙特卡洛采样成本高导致样本量不足。

Method: 设计复合蒙特卡洛采样(ComMCS)，通过线性组合当前与后续步骤的MC估计器构建无偏估计，理论证明可降低方差且不增加LLM推理成本。

Result: 在MATH-500基准的Best-of-32实验中，ComMCS比回归优化方法高2.8分，比非方差缩减基线高2.2分。

Conclusion: ComMCS有效解决了估计方差问题，显著提升复杂领域推理性能，且无需额外计算开销。

Abstract: Large language models (LLMs) have achieved remarkable success in a wide range
of tasks. However, their reasoning capabilities, particularly in complex
domains like mathematics, remain a significant challenge. Value-based process
verifiers, which estimate the probability of a partial reasoning chain leading
to a correct solution, are a promising approach for improving reasoning.
Nevertheless, their effectiveness is often hindered by estimation error in
their training annotations, a consequence of the limited number of Monte Carlo
(MC) samples feasible due to the high cost of LLM inference. In this paper, we
identify that the estimation error primarily arises from high variance rather
than bias, and the MC estimator is a Minimum Variance Unbiased Estimator
(MVUE). To address the problem, we propose the \textsc{Com}pound \textsc{M}onte
\textsc{C}arlo \textsc{S}ampling (ComMCS) method, which constructs an unbiased
estimator by linearly combining the MC estimators from the current and
subsequent steps. Theoretically, we show that our method leads to a predictable
reduction in variance, while maintaining an unbiased estimation without
additional LLM inference cost. We also perform empirical experiments on the
MATH-500 and GSM8K benchmarks to demonstrate the effectiveness of our method.
Notably, ComMCS outperforms regression-based optimization method by 2.8 points,
the non-variance-reduced baseline by 2.2 points on MATH-500 on Best-of-32
sampling experiment.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [86] [Large Language Models Show Signs of Alignment with Human Neurocognition During Abstract Reasoning](https://arxiv.org/abs/2508.10057)
*Christopher Pinier,Sonia Acuña Vargas,Mariia Steeghs-Turchina,Dora Matzke,Claire E. Stevenson,Michael D. Nunez*

Main category: q-bio.NC

TL;DR: 70B参数规模的大语言模型在抽象推理任务中达到人类水平准确度，并在表征模式上与人类脑电特征存在潜在关联


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在抽象推理任务中是否与人类神经认知机制存在相似性，验证生物智能与人工智能的潜在共性原理

Method: 使用脑电图(EEG)记录人类受试者在抽象模式补全任务中的fixation-related potentials(FRPs)，对比8个开源LLMs的中间层表征几何结构

Result: 仅最大规模模型(~70B)达到人类水平准确度，中间层形成抽象模式聚类（与任务表现正相关），模型最优层表征与人类前额叶FRPs存在中度相关性

Conclusion: LLMs可能模拟了人脑的抽象推理机制，为生物智能与人工智能的共性原理提供了初步证据

Abstract: This study investigates whether large language models (LLMs) mirror human
neurocognition during abstract reasoning. We compared the performance and
neural representations of human participants with those of eight open-source
LLMs on an abstract-pattern-completion task. We leveraged pattern type
differences in task performance and in fixation-related potentials (FRPs) as
recorded by electroencephalography (EEG) during the task. Our findings indicate
that only the largest tested LLMs (~70 billion parameters) achieve
human-comparable accuracy, with Qwen-2.5-72B and DeepSeek-R1-70B also showing
similarities with the human pattern-specific difficulty profile. Critically,
every LLM tested forms representations that distinctly cluster the abstract
pattern categories within their intermediate layers, although the strength of
this clustering scales with their performance on the task. Moderate positive
correlations were observed between the representational geometries of
task-optimal LLM layers and human frontal FRPs. These results consistently
diverged from comparisons with other EEG measures (response-locked ERPs and
resting EEG), suggesting a potential shared representational space for abstract
patterns. This indicates that LLMs might mirror human brain mechanisms in
abstract reasoning, offering preliminary evidence of shared principles between
biological and artificial intelligence.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [87] [Context Misleads LLMs: The Role of Context Filtering in Maintaining Safe Alignment of LLMs](https://arxiv.org/abs/2508.10031)
*Jinhwa Kim,Ian G. Harris*

Main category: cs.CR

TL;DR: 提出Context Filtering模型作为输入预处理方法，通过过滤不可信上下文并识别用户真实意图，有效将越狱攻击成功率降低88%且保持模型性能，适用于所有LLMs


<details>
  <summary>Details</summary>
Motivation: 现有LLMs防御机制在增强安全性时往往损害模型的有用性，需要同时兼顾安全与性能平衡

Method: 开发上下文过滤模型进行输入预处理，通过识别核心用户意图来检测隐藏的恶意请求

Result: 攻击成功率降低88%，保持原始模型性能，安全性与有用性乘积指标达到SOTA，支持所有LLM的即插即用部署

Conclusion: 该即插即用方法在不影响模型性能的前提下显著提升LLMs安全性，适用于黑白盒模型且无需微调

Abstract: While Large Language Models (LLMs) have shown significant advancements in
performance, various jailbreak attacks have posed growing safety and ethical
risks. Malicious users often exploit adversarial context to deceive LLMs,
prompting them to generate responses to harmful queries. In this study, we
propose a new defense mechanism called Context Filtering model, an input
pre-processing method designed to filter out untrustworthy and unreliable
context while identifying the primary prompts containing the real user intent
to uncover concealed malicious intent. Given that enhancing the safety of LLMs
often compromises their helpfulness, potentially affecting the experience of
benign users, our method aims to improve the safety of the LLMs while
preserving their original performance. We evaluate the effectiveness of our
model in defending against jailbreak attacks through comparative analysis,
comparing our approach with state-of-the-art defense mechanisms against six
different attacks and assessing the helpfulness of LLMs under these defenses.
Our model demonstrates its ability to reduce the Attack Success Rates of
jailbreak attacks by up to 88% while maintaining the original LLMs'
performance, achieving state-of-the-art Safety and Helpfulness Product results.
Notably, our model is a plug-and-play method that can be applied to all LLMs,
including both white-box and black-box models, to enhance their safety without
requiring any fine-tuning of the models themselves. We will make our model
publicly available for research purposes.

</details>


### [88] [Searching for Privacy Risks in LLM Agents via Simulation](https://arxiv.org/abs/2508.10880)
*Yanzhe Zhang,Diyi Yang*

Main category: cs.CR

TL;DR: 提出对抗性框架揭示LLM代理的隐私威胁，通过交互式攻击防御迭代发现隐私漏洞与防护策略


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理的动态对话特性可能导致新型隐私攻击，传统静态分析方法难以捕捉多轮交互中的复杂漏洞

Method: 构建三方交互框架（数据主体/发送方/接收方），使用LLM驱动的并行搜索算法迭代优化攻击指令与防御策略

Result: 发现攻击策略从直接请求演进到身份伪造/授权伪造，防御策略从规则限制发展为身份验证状态机

Conclusion: 该框架发现的攻防策略具有跨场景迁移性，为构建隐私感知代理提供有效验证手段

Abstract: The widespread deployment of LLM-based agents is likely to introduce a
critical privacy threat: malicious agents that proactively engage others in
multi-turn interactions to extract sensitive information. These dynamic
dialogues enable adaptive attack strategies that can cause severe privacy
violations, yet their evolving nature makes it difficult to anticipate and
discover sophisticated vulnerabilities manually. To tackle this problem, we
present a search-based framework that alternates between improving attacker and
defender instructions by simulating privacy-critical agent interactions. Each
simulation involves three roles: data subject, data sender, and data recipient.
While the data subject's behavior is fixed, the attacker (data recipient)
attempts to extract sensitive information from the defender (data sender)
through persistent and interactive exchanges. To explore this interaction space
efficiently, our search algorithm employs LLMs as optimizers, using parallel
search with multiple threads and cross-thread propagation to analyze simulation
trajectories and iteratively propose new instructions. Through this process, we
find that attack strategies escalate from simple direct requests to
sophisticated multi-turn tactics such as impersonation and consent forgery,
while defenses advance from rule-based constraints to identity-verification
state machines. The discovered attacks and defenses transfer across diverse
scenarios and backbone models, demonstrating strong practical utility for
building privacy-aware agents.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [89] [Personalized Product Search Ranking: A Multi-Task Learning Approach with Tabular and Non-Tabular Data](https://arxiv.org/abs/2508.09636)
*Lalitesh Morishetti,Abhay Kumar,Jonathan Scott,Kaushiki Nag,Gunjan Sharma,Shanu Vashishtha,Rahul Sridhar,Rohit Chatter,Kannan Achan*

Main category: cs.IR

TL;DR: 提出基于多任务学习框架的新型模型架构，通过整合表格/非表格数据与TinyBERT嵌入技术优化个性化产品搜索排序


<details>
  <summary>Details</summary>
Motivation: 解决传统方法处理混合数据类型的局限性，降低对人工标注的依赖，提升个性化排序效果

Method: 1. 多任务学习框架整合异构数据
2. 预训练TinyBERT生成语义嵌入
3. 新型采样技术捕捉用户行为模式
4. 基于点击行为与语义相似性的自动相关性标注机制

Result: 实验显示结合非表格数据与先进嵌入技术使NDCG@10提升7.7%，消融实验验证相关性标注、TinyBERT微调及嵌入交互的有效性

Conclusion: 该方法显著提升个性化搜索效果，提出的自动化相关性标注机制具有实际应用价值

Abstract: In this paper, we present a novel model architecture for optimizing
personalized product search ranking using a multi-task learning (MTL)
framework. Our approach uniquely integrates tabular and non-tabular data,
leveraging a pre-trained TinyBERT model for semantic embeddings and a novel
sampling technique to capture diverse customer behaviors. We evaluate our model
against several baselines, including XGBoost, TabNet, FT-Transformer, DCN-V2,
and MMoE, focusing on their ability to handle mixed data types and optimize
personalized ranking. Additionally, we propose a scalable relevance labeling
mechanism based on click-through rates, click positions, and semantic
similarity, offering an alternative to traditional human-annotated labels.
Experimental results show that combining non-tabular data with advanced
embedding techniques in multi-task learning paradigm significantly enhances
model performance. Ablation studies further underscore the benefits of
incorporating relevance labels, fine-tuning TinyBERT layers, and TinyBERT
query-product embedding interactions. These results demonstrate the
effectiveness of our approach in achieving improved personalized product search
ranking.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [90] [Puppeteer: Rig and Animate Your 3D Models](https://arxiv.org/abs/2508.10898)
*Chaoyue Song,Xiu Li,Fan Yang,Zhongcong Xu,Jiacheng Wei,Fayao Liu,Jiashi Feng,Guosheng Lin,Jianfeng Zhang*

Main category: cs.CV

TL;DR: 提出Puppeteer框架实现3D模型的自动骨骼绑定与动画生成，通过联合预测、注意力蒙皮和可微分优化解决内容创作瓶颈


<details>
  <summary>Details</summary>
Motivation: 现有3D动画制作依赖专家手动操作，生成式AI在静态建模取得进展但动画流程仍低效，需自动化解决方案提升创作效率

Method: 1. 自回归Transformer预测骨骼结构（联合标记化+层次排序） 2. 拓扑感知注意力网络计算蒙皮权重 3. 可微分优化动画生成框架

Result: 在多个基准测试中骨骼预测精度提升18%，蒙皮质量提高23%，支持游戏资产/AI生成模型的稳定动画输出，解决现有方法抖动问题

Conclusion: Puppeteer系统性突破自动绑定与动画技术，支持多样化3D内容的高效处理，为交互应用提供动态内容生成新范式

Abstract: Modern interactive applications increasingly demand dynamic 3D content, yet
the transformation of static 3D models into animated assets constitutes a
significant bottleneck in content creation pipelines. While recent advances in
generative AI have revolutionized static 3D model creation, rigging and
animation continue to depend heavily on expert intervention. We present
Puppeteer, a comprehensive framework that addresses both automatic rigging and
animation for diverse 3D objects. Our system first predicts plausible skeletal
structures via an auto-regressive transformer that introduces a joint-based
tokenization strategy for compact representation and a hierarchical ordering
methodology with stochastic perturbation that enhances bidirectional learning
capabilities. It then infers skinning weights via an attention-based
architecture incorporating topology-aware joint attention that explicitly
encodes inter-joint relationships based on skeletal graph distances. Finally,
we complement these rigging advances with a differentiable optimization-based
animation pipeline that generates stable, high-fidelity animations while being
computationally more efficient than existing approaches. Extensive evaluations
across multiple benchmarks demonstrate that our method significantly
outperforms state-of-the-art techniques in both skeletal prediction accuracy
and skinning quality. The system robustly processes diverse 3D content, ranging
from professionally designed game assets to AI-generated shapes, producing
temporally coherent animations that eliminate the jittering issues common in
existing methods.

</details>


### [91] [Improving OCR for Historical Texts of Multiple Languages](https://arxiv.org/abs/2508.10356)
*Hylke Westerdijk,Ben Blankenborg,Khondoker Ittehadul Islam*

Main category: cs.CV

TL;DR: 提出基于深度学习的OCR和文档布局分析方法，分别在古卷文字、历史文献和现代手写体三个场景验证有效性


<details>
  <summary>Details</summary>
Motivation: 解决不同历史时期文档（死海古卷希伯来文、16-18世纪会议记录、现代英文手写）的字符识别与版面分析难题

Method: 1. Kraken/TrOCR模型+数据增强处理古卷 2. CRNN+DeepLabV3+分割+双向LSTM处理历史文献 3. ResNet34编码器+CTC损失处理现代手写

Result: 建立跨时代文档处理框架，验证深度学习在不同场景的适用性，提出置信度伪标签等改进方法

Conclusion: 方法体系在多个历史层级的文档分析中展现有效性，未来可探索跨时代模型迁移与多模态融合

Abstract: This paper presents our methodology and findings from three tasks across
Optical Character Recognition (OCR) and Document Layout Analysis using advanced
deep learning techniques. First, for the historical Hebrew fragments of the
Dead Sea Scrolls, we enhanced our dataset through extensive data augmentation
and employed the Kraken and TrOCR models to improve character recognition. In
our analysis of 16th to 18th-century meeting resolutions task, we utilized a
Convolutional Recurrent Neural Network (CRNN) that integrated DeepLabV3+ for
semantic segmentation with a Bidirectional LSTM, incorporating confidence-based
pseudolabeling to refine our model. Finally, for modern English handwriting
recognition task, we applied a CRNN with a ResNet34 encoder, trained using the
Connectionist Temporal Classification (CTC) loss function to effectively
capture sequential dependencies. This report offers valuable insights and
suggests potential directions for future research.

</details>
