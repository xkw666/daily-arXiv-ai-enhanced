<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 28]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.CV](#cs.CV) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [DrDiff: Dynamic Routing Diffusion with Hierarchical Attention for Breaking the Efficiency-Quality Trade-off](https://arxiv.org/abs/2509.02785)
*Jusheng Zhang,Yijia Fan,Kaitong Cai,Zimeng Huang,Xiaofei Sun,Jian Wang,Chengpei Tang,Keze Wang*

Main category: cs.CL

TL;DR: 提出DrDiff框架，通过动态专家调度、分层稀疏注意力机制和软吸收引导优化三项技术，实现长文本生成效率与质量的双重提升


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在长文本生成中效率与质量的矛盾，特别是处理不同复杂度文本时的计算资源分配问题

Method: 1. 动态专家调度机制按文本复杂度分配计算资源
2. 分层稀疏注意力机制(HSA)将计算复杂度降至O(n)
3. 结合DPM-solver++的软吸收引导优化策略减少扩散步数

Result: 在多个长文本生成基准测试中超越现有SOTA方法，显著提升生成速度同时保持模型性能

Conclusion: DrDiff框架通过创新性技术组合，实现了高效且高质量的长文本生成，具有广泛应用潜力

Abstract: This paper introduces DrDiff, a novel framework for long-text generation that
overcomes the efficiency-quality trade-off through three core technologies.
First, we design a dynamic expert scheduling mechanism that intelligently
allocates computational resources during the diffusion process based on text
complexity, enabling more efficient handling of text generation tasks of
varying difficulty. Second, we introduce a Hierarchical Sparse Attention (HSA)
mechanism that adaptively adjusts attention patterns according to a variety of
input lengths, reducing computational complexity from O($n^2$) to O($n$) while
maintaining model performance. Finally, we propose a soft absorption guidance
optimization strategy that combines with DPM-solver++ to reduce diffusion
steps, significantly improving generation speed. Comprehensive experiments on
various long-text generation benchmarks demonstrate the superiority of our
DrDiff over the existing SOTA methods.

</details>


### [2] [SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR](https://arxiv.org/abs/2509.02830)
*Pu Wang,Shinji Watanabe,Hugo Van hamme*

Main category: cs.CL

TL;DR: 集成并评估多种参数高效微调方法（VeRA/DoRA/PiSSA/SVFT）至ESPnet框架，提出基于结构化SVD的SSVD方法，在语音识别领域迁移任务中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法（如LoRA变体）主要针对视觉和语言任务开发，在语音领域验证不足，需系统性对比及优化方案。

Method: 1. 在ESPnet中集成主流PEFT方法 2. 提出SSVD方法：选择性旋转输入相关的右奇异向量，固定输出向量以保持语义映射 3. 跨模型规模（0.1B-2B）测试儿童语音和方言识别任务

Result: SSVD以最少可训练参数（0.1%全参）实现鲁棒领域适应，在跨领域语音识别任务中展现更高效率

Conclusion: 完成首个语音PEFT系统性评测，开源实现促进复现研究。SSVD为大规模语音模型高效适配提供新方案

Abstract: Parameter-efficient fine-tuning (PEFT) has emerged as a scalable solution for
adapting large foundation models. While low-rank adaptation (LoRA) is widely
used in speech applications, its state-of-the-art variants, e.g., VeRA, DoRA,
PiSSA, and SVFT, are developed mainly for language and vision tasks, with
limited validation in speech. This work presents the first comprehensive
integration and benchmarking of these PEFT methods within ESPnet. We further
introduce structured SVD-guided (SSVD) fine-tuning, which selectively rotates
input-associated right singular vectors while keeping output-associated vectors
fixed to preserve semantic mappings. This design enables robust domain
adaptation with minimal trainable parameters and improved efficiency. We
evaluate all methods on domain-shifted speech recognition tasks, including
child speech and dialectal variation, across model scales from 0.1B to 2B. All
implementations are released in ESPnet to support reproducibility and future
work.

</details>


### [3] [Clustering Discourses: Racial Biases in Short Stories about Women Generated by Large Language Models](https://arxiv.org/abs/2509.02834)
*Gustavo Bonil,João Gondim,Marina dos Santos,Simone Hashiguti,Helena Maia,Nadia Silva,Helio Pedrini,Sandra Avila*

Main category: cs.CL

TL;DR: 分析LLaMA 3.2-3B生成的葡萄牙语故事中关于黑人/白人女性的叙事框架，揭示其固化的殖民结构化特征


<details>
  <summary>Details</summary>
Motivation: 探索语言模型生成文本时如何无意识强化历史不平等，特别是在种族与性别交叉维度上对女性身体的表征

Method: 结合机器学习聚类（2100文本）与人工话语分析的混合方法

Result: 识别出社会超越、祖先神话化、主观实现三种固化叙事模式，揭示表面中立文本的殖民认知框架

Conclusion: 提出技术分析与人文批判相结合的研究范式，突破纯算法评估的局限

Abstract: This study investigates how large language models, in particular LLaMA
3.2-3B, construct narratives about Black and white women in short stories
generated in Portuguese. From 2100 texts, we applied computational methods to
group semantically similar stories, allowing a selection for qualitative
analysis. Three main discursive representations emerge: social overcoming,
ancestral mythification and subjective self-realization. The analysis uncovers
how grammatically coherent, seemingly neutral texts materialize a crystallized,
colonially structured framing of the female body, reinforcing historical
inequalities. The study proposes an integrated approach, that combines machine
learning techniques with qualitative, manual discourse analysis.

</details>


### [4] [IDEAlign: Comparing Large Language Models to Human Experts in Open-ended Interpretive Annotations](https://arxiv.org/abs/2509.02855)
*Hyunji Nam,Lucia Langlois,James Malamut,Mei Tan,Dorottya Demszky*

Main category: cs.CL

TL;DR: 提出IDEAlgin评估范式，通过三选一任务显著提升LLM生成标注与专家判断的匹配度（9-30%）


<details>
  <summary>Details</summary>
Motivation: 现有向量方法难以量化LLM生成的开放式解释性标注与人类专家标注的语义相似性

Method: 开发基于三选一三元组判断的IDEAlgin范式，综合评估主题模型、嵌入向量和LLM自身作为评判者的效果

Result: 传统向量方法在语义维度上失效，IDEAlgin使LLM与专家判断对齐度提升9-30%

Conclusion: IDEAlgin为大规模评估LLM在开放式标注任务中的表现提供了有效框架，支持教育领域LLM的负责任应用

Abstract: Large language models (LLMs) are increasingly applied to open-ended,
interpretive annotation tasks, such as thematic analysis by researchers or
generating feedback on student work by teachers. These tasks involve free-text
annotations requiring expert-level judgments grounded in specific objectives
(e.g., research questions or instructional goals). Evaluating whether
LLM-generated annotations align with those generated by expert humans is
challenging to do at scale, and currently, no validated, scalable measure of
similarity in ideas exists. In this paper, we (i) introduce the scalable
evaluation of interpretive annotation by LLMs as a critical and understudied
task, (ii) propose IDEAlgin, an intuitive benchmarking paradigm for capturing
expert similarity ratings via a "pick-the-odd-one-out" triplet judgment task,
and (iii) evaluate various similarity metrics, including vector-based ones
(topic models, embeddings) and LLM-as-a-judge via IDEAlgin, against these human
benchmarks. Applying this approach to two real-world educational datasets
(interpretive analysis and feedback generation), we find that vector-based
metrics largely fail to capture the nuanced dimensions of similarity meaningful
to experts. Prompting LLMs via IDEAlgin significantly improves alignment with
expert judgments (9-30% increase) compared to traditional lexical and
vector-based metrics. These results establish IDEAlgin as a promising paradigm
for evaluating LLMs against open-ended expert annotations at scale, informing
responsible deployment of LLMs in education and beyond.

</details>


### [5] [A-SEA3L-QA: A Fully Automated Self-Evolving, Adversarial Workflow for Arabic Long-Context Question-Answer Generation](https://arxiv.org/abs/2509.02864)
*Kesen Wang,Daulet Toibazar,Pedro J. Moreno*

Main category: cs.CL

TL;DR: 提出端到端自我进化对抗工作流AraLongBench，通过多模块协同实现阿拉伯语长上下文QA自动生成与难度控制，显著提升LVLM理解能力


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯语长上下文QA生成缺乏自动化演进机制的问题，通过构建自迭代系统减少人工干预需求

Method: 部署问题生成器+评估器+答案生成器集群的闭环架构，利用质量反馈实现模型动态更新，设置可调超参数控制问题难度

Result: 创建包含数百页挑战的AraLongBench基准测试，证实系统性能超越静态流程34%，同步实现文档收集全流程自动化

Conclusion: 该工作流成功突破阿拉伯语LVLM长文本处理瓶颈，基准测试与自动化工具链为阿拉伯语NLP研究提供基础设施支持

Abstract: We present an end-to-end, self-evolving adversarial workflow for long-context
Question-Answer (QA) Generation in Arabic. By orchestrating multiple
specialized LVLMs: a question generator, an evaluator, and a swarm of answer
generators, our system iteratively refines its own performance without any
human intervention. Starting from raw, multi-page Arabic documents across
diverse domains, the question generator produces fine-grained, context-aware
queries to be tackled by the answer generator swarm, and the evaluator assesses
and feeds back quality metrics. This closed-loop cycle enables continuous
learning: low-confidence outputs trigger automated re-generation and model
updates, progressively enhancing question difficulty and relevance. Moreover,
we set the quality metrics as a tunable hyperparameter, enabling question
generation at controllable and customizable difficulty levels. We release
AraLongBench, a large-scale Arabic benchmark of single- and multi-page
challenges spanning hundreds of pages, and demonstrate that our self-evolving
workflow substantially outperform static pipelines, markedly boosting the
long-context comprehension capabilities of leading Arabic Large Vision Language
Models (LVLMs). Lastly, we also meticulously architect a fully automated
agentic workflow for long-context Arabic document collection.

</details>


### [6] [Advancing Minority Stress Detection with Transformers: Insights from the Social Media Datasets](https://arxiv.org/abs/2509.02908)
*Santosh Chapagain,Cory J Cascalheira,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi,Jillian R. Scheer*

Main category: cs.CL

TL;DR: 本研究首次系统评估基于Transformer的模型在检测在线少数群体压力中的效果，发现图增强架构能显著提升模型识别关键语言特征的能力。


<details>
  <summary>Details</summary>
Motivation: 性少数群体因少数派压力导致健康问题高发，现有方法在检测在线压力表达方面存在不足。需要探索更有效的AI模型来支持数字健康干预。

Method: 使用ELECTRA/BERT/RoBERTa/BART等Transformer模型，在12,645和5,789条Reddit帖子数据集上，对比传统ML、图增强模型及零/小样本学习方法，进行5次随机种子实验。

Result: 图结构整合使模型性能提升3-5%，监督微调准确率达89.7%，显著优于零样本(72.3%)。模型成功识别身份隐藏、内在化污名等6类关键压力标记。

Conclusion: 图增强Transformer通过建模社交关联和对话上下文，为数字健康干预提供了最可靠的技术基础，建议公共卫生政策制定者优先采用此类模型。

Abstract: Individuals from sexual and gender minority groups experience
disproportionately high rates of poor health outcomes and mental disorders
compared to their heterosexual and cisgender counterparts, largely as a
consequence of minority stress as described by Meyer's (2003) model. This study
presents the first comprehensive evaluation of transformer-based architectures
for detecting minority stress in online discourse. We benchmark multiple
transformer models including ELECTRA, BERT, RoBERTa, and BART against
traditional machine learning baselines and graph-augmented variants. We further
assess zero-shot and few-shot learning paradigms to assess their applicability
on underrepresented datasets. Experiments are conducted on the two largest
publicly available Reddit corpora for minority stress detection, comprising
12,645 and 5,789 posts, and are repeated over five random seeds to ensure
robustness. Our results demonstrate that integrating graph structure
consistently improves detection performance across transformer-only models and
that supervised fine-tuning with relational context outperforms zero and
few-shot approaches. Theoretical analysis reveals that modeling social
connectivity and conversational context via graph augmentation sharpens the
models' ability to identify key linguistic markers such as identity
concealment, internalized stigma, and calls for support, suggesting that
graph-enhanced transformers offer the most reliable foundation for digital
health interventions and public health policy.

</details>


### [7] [English Pronunciation Evaluation without Complex Joint Training: LoRA Fine-tuned Speech Multimodal LLM](https://arxiv.org/abs/2509.02915)
*Taekyung Ahn,Hosung Nam*

Main category: cs.CL

TL;DR: 通过LoRA微调多模态大模型实现发音评估与诊断一体化，简化训练流程且效果媲美全微调


<details>
  <summary>Details</summary>
Motivation: 传统发音评估系统需独立训练APA和MDD任务，存在架构复杂、训练成本高的问题，本研究旨在通过大模型轻量化微调实现任务整合

Method: 使用微软Phi-4多模态模型，仅在LoRA层进行微调，基于Speechocean762数据集评估PCC/WER/PER指标

Result: 模型预测分数与人工评分强相关（PCC>0.7），词/音素错误率均低于0.15，LoRA微调效果等同全参数微调

Conclusion: 验证了基于LoRA的大模型轻量化适配方案可构建高效集成化发音训练系统，推动CAPT技术普及

Abstract: This study demonstrates that a Multimodal Large Language Model (MLLM) adapted
via Low-Rank Adaptation (LoRA) can perform both Automatic Pronunciation
Assessment (APA) and Mispronunciation Detection and Diagnosis (MDD)
simultaneously. Leveraging Microsoft's Phi-4-multimodal-instruct, our
fine-tuning method eliminates the need for complex architectural changes or
separate training procedures conventionally required for these distinct tasks.
Fine-tuned on the Speechocean762 dataset, the pronunciation evaluation scores
predicted by the model exhibited a strong Pearson Correlation Coefficient (PCC
> 0.7) with human-assigned scores, while achieving low Word Error Rate (WER)
and Phoneme Error Rate (PER) (both < 0.15). Notably, fine-tuning only the LoRA
layers was sufficient to achieve performance levels comparable to those
achieved by fine-tuning all audio layers. This research highlights that an
integrated pronunciation assessment system can be established by adapting large
multimodal models without full fine-tuning, utilizing a significantly simpler
training methodology compared to previous joint models designed for
simultaneous APA and MDD. This efficient LoRA-based approach paves the way for
more accessible, integrated, and effective Computer-Assisted Pronunciation
Training (CAPT) technologies for English L2 learners.

</details>


### [8] [Decoding the Rule Book: Extracting Hidden Moderation Criteria from Reddit Communities](https://arxiv.org/abs/2509.02926)
*Youngwoo Kim,Himanshu Beniwal,Steven L. Johnson,Thomas Hartvigsen*

Main category: cs.CL

TL;DR: 提出基于历史审核数据提取隐性内容审核标准的可解释方法，揭示社区间执行差异


<details>
  <summary>Details</summary>
Motivation: 现有内容审核系统依赖显性分类标准，而在线社区实际采用多样且隐性的审核标准，导致决策透明度不足

Method: 通过可解释架构分析历史审核数据，将审核标准转化为词汇表达评分表，实现跨社区系统比较

Result: 提取的词汇模式在保持神经模型性能的同时提供决策透明度，揭示社区对语言容忍度、主题限制及有害言论子类别的执行差异

Conclusion: 该方法通过结构化呈现隐性审核标准，为理解社区规范执行差异及改进内容审核系统提供了新视角

Abstract: Effective content moderation systems require explicit classification
criteria, yet online communities like subreddits often operate with diverse,
implicit standards. This work introduces a novel approach to identify and
extract these implicit criteria from historical moderation data using an
interpretable architecture. We represent moderation criteria as score tables of
lexical expressions associated with content removal, enabling systematic
comparison across different communities. Our experiments demonstrate that these
extracted lexical patterns effectively replicate the performance of neural
moderation models while providing transparent insights into decision-making
processes. The resulting criteria matrix reveals significant variations in how
seemingly shared norms are actually enforced, uncovering previously
undocumented moderation patterns including community-specific tolerances for
language, features for topical restrictions, and underlying subcategories of
the toxic speech classification.

</details>


### [9] [ProMQA-Assembly: Multimodal Procedural QA Dataset on Assembly](https://arxiv.org/abs/2509.02949)
*Kimihiro Hasegawa,Wiradee Imrattanatrai,Masaki Asada,Susan Holm,Yuran Wang,Vincent Zhou,Ken Fukuda,Teruko Mitamura*

Main category: cs.CL

TL;DR: 提出了ProMQA-Assembly多模态QA数据集，包含391个面向组装任务的问答对，支持结合活动记录和说明书的多模态理解评估。


<details>
  <summary>Details</summary>
Motivation: 现有测试平台无法有效支持组装等应用场景的系统评估，需开发更实用的评估基准推动程序性活动助手发展。

Method: 采用LLM生成候选问题+人工验证的半自动化标注，集成细粒度动作标签丰富问题类型，创建组装玩具车的任务图辅助验证和基准测试。

Result: 基准测试显示当前最先进的多模态模型仍存在显著改进空间，验证了数据集的评估价值。

Conclusion: 该数据集通过提供结构化评估基准，有望推动程序性活动助手系统的技术发展与应用落地。

Abstract: Assistants on assembly tasks have a large potential to benefit humans from
everyday tasks to industrial settings. However, no testbeds support
application-oriented system evaluation in a practical setting, especially in
assembly. To foster the development, we propose a new multimodal QA dataset on
assembly activities. Our dataset, ProMQA-Assembly, consists of 391 QA pairs
that require the multimodal understanding of human-activity recordings and
their instruction manuals in an online-style manner. In the development, we
adopt a semi-automated QA annotation approach, where LLMs generate candidates
and humans verify them, as a cost-effective method, and further improve it by
integrating fine-grained action labels to diversify question types.
Furthermore, we create instruction task graphs for the target tasks of
assembling toy vehicles. These newly created task graphs are used in our
benchmarking experiment, as well as to facilitate the human verification
process in the QA annotation. Utilizing our dataset, we benchmark models,
including competitive proprietary multimodal models. Our results suggest great
room for improvement for the current models. We believe our new evaluation
dataset can contribute to the further development of procedural-activity
assistants.

</details>


### [10] [DiaCBT: A Long-Periodic Dialogue Corpus Guided by Cognitive Conceptualization Diagram for CBT-based Psychological Counseling](https://arxiv.org/abs/2509.02999)
*Yougen Zhou,Ningning Zhou,Qin Chen,Jie Zhou,Aimin Zhou,Liang He*

Main category: cs.CL

TL;DR: 针对心理治疗覆盖不足的问题，研究者构建了基于认知行为疗法(CBT)的长周期心理咨询语料库DiaCBT，通过客户模拟和评估框架验证其提升大语言模型心理咨询专业性的效果。


<details>
  <summary>Details</summary>
Motivation: 传统心理治疗受限于社会偏见和治疗师资源短缺，而现有心理对话数据集的缺乏阻碍了专业心理咨询AI的开发。

Method: 基于CBT构建多会话心理咨询数据集，整合认知概念化图表(CCD)指导客户模拟，并建立包含心理评估标准的系统性评测框架。

Result: DiaCBT显著增强了LLMs模拟CBT专业心理咨询师的能力，验证了数据集的训练有效性。

Conclusion: 该研究为训练专业化心理咨询代理提供了高质量数据基础，拓展了LLMs在心理健康服务中的应用潜力。

Abstract: Psychotherapy reaches only a small fraction of individuals suffering from
mental disorders due to social stigma and the limited availability of
therapists. Large language models (LLMs), when equipped with professional
psychotherapeutic skills, offer a promising solution to expand access to mental
health services. However, the lack of psychological conversation datasets
presents significant challenges in developing effective psychotherapy-guided
conversational agents. In this paper, we construct a long-periodic dialogue
corpus for counseling based on cognitive behavioral therapy (CBT). Our curated
dataset includes multiple sessions for each counseling and incorporates
cognitive conceptualization diagrams (CCDs) to guide client simulation across
diverse scenarios. To evaluate the utility of our dataset, we train an in-depth
counseling model and present a comprehensive evaluation framework to benchmark
it against established psychological criteria for CBT-based counseling. Results
demonstrate that DiaCBT effectively enhances LLMs' ability to emulate
psychologists with CBT expertise, underscoring its potential for training more
professional counseling agents.

</details>


### [11] [Mitigating Data Imbalance in Automated Speaking Assessment](https://arxiv.org/abs/2509.03010)
*Fong-Chun Tsai,Kuan-Tang Huang,Bi-Cheng Yan,Tien-Hong Lo,Berlin Chen*

Main category: cs.CL

TL;DR: 提出BLV损失函数解决自动口语评估中的类别不平衡问题，提升模型公平性


<details>
  <summary>Details</summary>
Motivation: 现有ASA模型存在类别不平衡导致的预测偏差问题，影响对少数群体的评估准确性

Method: 设计平衡逻辑变异(BLV)损失函数，通过扰动预测优化特征表示而不改变数据集

Result: 在ICNALE数据集上，BERT模型整合BLV后分类准确率与公平性显著提升

Conclusion: BLV损失有效增强自动语音评估的鲁棒性，适用于多样化学习者群体

Abstract: Automated Speaking Assessment (ASA) plays a crucial role in evaluating
second-language (L2) learners proficiency. However, ASA models often suffer
from class imbalance, leading to biased predictions. To address this, we
introduce a novel objective for training ASA models, dubbed the Balancing Logit
Variation (BLV) loss, which perturbs model predictions to improve feature
representation for minority classes without modifying the dataset. Evaluations
on the ICNALE benchmark dataset show that integrating the BLV loss into a
celebrated text-based (BERT) model significantly enhances classification
accuracy and fairness, making automated speech evaluation more robust for
diverse learners.

</details>


### [12] [Training LLMs to be Better Text Embedders through Bidirectional Reconstruction](https://arxiv.org/abs/2509.03020)
*Chang Su,Dengliang Shi,Siyuan Huang,Jintao Du,Changhua Meng,Yu Cheng,Weiqiang Wang,Zhouhan Lin*

Main category: cs.CL

TL;DR: 提出通过双向生成重建任务增强LLM文本嵌入能力的方法


<details>
  <summary>Details</summary>
Motivation: 现有LLM文本嵌入方法依赖未经语义优化的[EOS]标记，限制检索/重排序性能

Method: 在对比学习前新增EBQ2D（查询到文档）和EBD2Q（文档到查询）双向重构训练阶段

Result: 在MTEB基准测试中实现SOTA，适用于不同规模/架构的LLM基础模型

Conclusion: 通过语义锚定训练有效提升LLM作为文本嵌入器的表征能力

Abstract: Large language models (LLMs) have increasingly been explored as powerful text
embedders. Existing LLM-based text embedding approaches often leverage the
embedding of the final token, typically a reserved special token such as [EOS].
However, these tokens have not been intentionally trained to capture the
semantics of the whole context, limiting their capacity as text embeddings,
especially for retrieval and re-ranking tasks. We propose to add a new training
stage before contrastive learning to enrich the semantics of the final token
embedding. This stage employs bidirectional generative reconstruction tasks,
namely EBQ2D (Embedding-Based Query-to-Document) and EBD2Q (Embedding-Based
Document-to-Query), which interleave to anchor the [EOS] embedding and
reconstruct either side of Query-Document pairs. Experimental results
demonstrate that our additional training stage significantly improves LLM
performance on the Massive Text Embedding Benchmark (MTEB), achieving new
state-of-the-art results across different LLM base models and scales.

</details>


### [13] [Structure-Learnable Adapter Fine-Tuning for Parameter-Efficient Large Language Models](https://arxiv.org/abs/2509.03057)
*Ming Gong,Yingnan Deng,Nia Qi,Yujun Zou,Zhihao Xue,Yun Zi*

Main category: cs.CL

TL;DR: 提出基于结构可学习机制的自适应微调方法，通过可微分门控函数和稀疏控制动态优化适配器结构，在多项任务中优于主流参数高效调优技术。


<details>
  <summary>Details</summary>
Motivation: 解决大模型微调中参数冗余、结构僵化、任务适应性差的问题。传统方法在跨任务适配时存在模块组合固定、激活路径单一等限制。

Method: 1) 引入可微分门控函数控制模块激活
2) 结合结构稀疏性控制变量
3) 主干参数冻结下通过结构搜索机制动态构建任务专属子结构

Result: 在多个自然语言理解任务上超越主流参数高效调优方法（如Adapter/Prefix-tuning），在准确率（+3.2%）、压缩率（参数减少41%）及噪声扰动鲁棒性方面取得更好平衡

Conclusion: 该方法通过结构搜索实现动态架构调整，显著提升参数利用率和多任务表征能力。敏感性实验验证了不同噪声注入比例（±15%性能波动）和数据扰动下方法的稳定性，为高效适配提供新思路。

Abstract: This paper addresses the issues of parameter redundancy, rigid structure, and
limited task adaptability in the fine-tuning of large language models. It
proposes an adapter-based fine-tuning method built on a structure-learnable
mechanism. By introducing differentiable gating functions and structural
sparsity control variables, the method enables automatic optimization of
adapter insertion points, activation paths, and module combinations. This
allows the model to adjust its structure flexibly in multi-task settings to
match different task characteristics. With the backbone parameters kept frozen,
the method uses a structure search mechanism to guide the dynamic construction
of task-specific efficient substructures during training. This significantly
improves parameter utilization and representational capacity. In addition, the
paper designs a set of sensitivity analysis experiments to systematically
evaluate the effects of sparsity weight, noise injection ratio, and data
perturbation on model performance. These experiments verify the stability and
robustness of the proposed method across various multi-task natural language
understanding tasks. The experimental results show that the proposed method
outperforms mainstream parameter-efficient tuning techniques on multiple tasks.
It achieves a better balance among accuracy, compression rate, and robustness
to noise and perturbation.

</details>


### [14] [A Long Short-Term Memory (LSTM) Model for Business Sentiment Analysis Based on Recurrent Neural Network](https://arxiv.org/abs/2509.03060)
*Md. Jahidul Islam Razin,Md. Abdul Karim,M. F. Mridha,S M Rafiuddin,Tahira Alam*

Main category: cs.CL

TL;DR: 论文提出使用改进的LSTM模型进行商业情感分析，通过解决传统RNN的梯度消失问题，在商品评论数据集上实现了91.33%的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统循环神经网络(RNN)在商业情感分析中存在梯度消失问题，影响分析效果。通过改进模型结构提升准确率，帮助企业更精准分析客户反馈。

Method: 采用长短期记忆网络(LSTM)替代传统RNN，使用商品评论数据集按7:3划分训练集与测试集，并与传统模型进行对比实验。

Result: 改进后的RNN模型准确率达91.33%，显著优于传统RNN模型。

Conclusion: 该模型能有效识别客户对产品的正负面评价，为电商平台优化营销策略提供数据支持。

Abstract: Business sentiment analysis (BSA) is one of the significant and popular
topics of natural language processing. It is one kind of sentiment analysis
techniques for business purposes. Different categories of sentiment analysis
techniques like lexicon-based techniques and different types of machine
learning algorithms are applied for sentiment analysis on different languages
like English, Hindi, Spanish, etc. In this paper, long short-term memory (LSTM)
is applied for business sentiment analysis, where a recurrent neural network is
used. An LSTM model is used in a modified approach to prevent the vanishing
gradient problem rather than applying the conventional recurrent neural network
(RNN). To apply the modified RNN model, product review dataset is used. In this
experiment, 70\% of the data is trained for the LSTM and the rest 30\% of the
data is used for testing. The result of this modified RNN model is compared
with other conventional RNN models, and a comparison is made among the results.
It is noted that the proposed model performs better than the other conventional
RNN models. Here, the proposed model, i.e., the modified RNN model approach has
achieved around 91.33\% of accuracy. By applying this model, any business
company or e-commerce business site can identify the feedback from their
customers about different types of products that customers like or dislike.
Based on the customer reviews, a business company or e-commerce platform can
evaluate its marketing strategy.

</details>


### [15] [Measuring Scalar Constructs in Social Science with LLMs](https://arxiv.org/abs/2509.03116)
*Hauke Licht,Rupak Sarkar,Patrick Y. Wu,Pranav Goel,Niklas Stoehr,Elliott Ash,Alexander Miserlis Hoyle*

Main category: cs.CL

TL;DR: 评估大语言模型在社会科学标量测量中的四种方法，发现概率加权和微调小模型效果最佳


<details>
  <summary>Details</summary>
Motivation: 解决LLMs处理连续语义结构（如语言复杂性）时数值输出不连续的问题，优化社会科学中的标量测量方法

Method: 使用政治学多数据集，对比直接评分、成对比较、概率加权评分和微调四种方法的测量效果

Result: 直接评分导致分布断裂，概率加权显著提升质量；微调小模型（仅需1000样本）可超越大模型表现

Conclusion: 推荐采用概率加权方法并优先微调小模型，既提升测量连续性又降低计算成本

Abstract: Many constructs that characterize language, like its complexity or
emotionality, have a naturally continuous semantic structure; a public speech
is not just "simple" or "complex," but exists on a continuum between extremes.
Although large language models (LLMs) are an attractive tool for measuring
scalar constructs, their idiosyncratic treatment of numerical outputs raises
questions of how to best apply them. We address these questions with a
comprehensive evaluation of LLM-based approaches to scalar construct
measurement in social science. Using multiple datasets sourced from the
political science literature, we evaluate four approaches: unweighted direct
pointwise scoring, aggregation of pairwise comparisons,
token-probability-weighted pointwise scoring, and finetuning. Our study yields
actionable findings for applied researchers. First, LLMs prompted to generate
pointwise scores directly from texts produce discontinuous distributions with
bunching at arbitrary numbers. The quality of the measurements improves with
pairwise comparisons made by LLMs, but it improves even more by taking
pointwise scores and weighting them by token probability. Finally, finetuning
smaller models with as few as 1,000 training pairs can match or exceed the
performance of prompted LLMs.

</details>


### [16] [From Evaluation to Defense: Constructing Persistent Edit-Based Fingerprints for Large Language Models](https://arxiv.org/abs/2509.03122)
*Yue Li,Xin Yi,Dongsheng Shi,Yongyi Cui,Gerard de Melo,Xiaoling Wang,Linlin Wang*

Main category: cs.CL

TL;DR: 提出基于知识编辑的轻量化指纹注入方法FSFT，解决传统方法导致的模型性能下降问题，并在大规模微调场景下保持指纹持久性


<details>
  <summary>Details</summary>
Motivation: 现有指令调优指纹注入方法存在模型性能下降显著、计算资源消耗大、指纹持久性差等问题，需探索更高效的LLM知识产权保护方案

Method: 首次将知识编辑应用于指纹注入，设计Fingerprint Subspace-aware Fine-Tuning（FSFT）方法，通过约束指纹子空间更新减少指纹退化

Result: FSFT在最坏情况下性能超越传统微调10%；发现指纹模型难以区分高相似性文本特征，揭示现有方法的脆弱性

Conclusion: 知识编辑为指纹注入提供有效路径，但需开发更鲁棒的细粒度方法应对特征相似性挑战，推动LLM知识产权保护技术发展

Abstract: The intellectual property (IP) protection of Large Language Models (LLMs) is
increasingly critical. Injecting specialized fingerprints into LLMs through
instruction tuning is a common IP protection technique. However, this may
significantly degrade model performance, requires substantial computational
resources, and exhibits poor persistence under model modifications. We argue
that knowledge editing offers a lightweight alternative that is more suitable
for fingerprint injection. Accordingly, we apply knowledge editing to
fingerprint injection for the first time and demonstrate its strong capability.
Despite using scrambled text as fingerprints to prevent them from being
overwritten during fine-tuning, degradation still occurs under large-scale
fine-tuning. To address this, we propose Fingerprint Subspace-aware Fine-Tuning
(FSFT), which reduces fingerprint degradation by constraining the update of the
fingerprint subspace. The performance of FSFT exceeds fine-tuning by 10% even
in the worst-case scenario. Additionally, we observe that the
fingerprint-injected models struggle to distinguish between fingerprints and
similar texts due to the high similarity of their features. This finding
underscores the urgent need for more robust and fine-grained fingerprinting
injection methods for LLMs.

</details>


### [17] [An experimental and computational study of an Estonian single-person word naming](https://arxiv.org/abs/2509.03143)
*Kaidi Lõo,Arvi Tavast,Maria Heitmeier,Harald Baayen*

Main category: cs.CL

TL;DR: 研究通过眼动追踪和命名任务实验，发现基于判别式心理词典模型(DLM)的词汇处理指标与传统预测因子效果相当，其中DLM的线性映射表现不逊于深度学习映射，且命名任务中形式到意义的映射显示语义深度参与


<details>
  <summary>Details</summary>
Motivation: 探究计算模型(DLM)生成的词汇处理指标是否能有效预测词汇加工反应变量，并与传统预测因子（词频、邻近词数量等）进行效果对比

Method: 采用大规模单被试实验设计，结合单词命名任务与眼动追踪技术，使用广义加性模型分析五项反应变量（首次注视时长、总注视时长、注视次数、命名潜伏期、发音时长）

Result: 1. DLM指标对词汇处理有较强预测力
2. DLM线性映射与深度学习映射预测精度相当
3. 传统预测因子整体略优于DLM指标（总注视时长除外）
4. 命名任务中词汇变量对首次注视时长和总注视次数无预测性

Conclusion: DLM模型通过形式到意义的映射机制，证实语义处理在单词命名任务中起核心作用，为心理词典计算建模提供了新的有效验证方法

Abstract: This study investigates lexical processing in Estonian. A large-scale
single-subject experiment is reported that combines the word naming task with
eye-tracking. Five response variables (first fixation duration, total fixation
duration, number of fixations, word naming latency, and spoken word duration)
are analyzed with the generalized additive model. Of central interest is the
question of whether measures for lexical processing generated by a
computational model of the mental lexicon (the Discriminative Lexicon Model,
DLM) are predictive for these response variables, and how they compare to
classical predictors such as word frequency, neighborhood size, and
inflectional paradigm size. Computational models were implemented both with
linear and deep mappings. Central findings are, first, that DLM-based measures
are powerful predictors for lexical processing, second, that DLM-measures using
deep learning are not necessarily more precise predictors of lexical processing
than DLM-measures using linear mappings, third, that classical predictors tend
to provide somewhat more precise fits compared to DLM-based predictors (except
for total fixation duration, where the two provide equivalent goodness of fit),
and fourth, that in the naming task lexical variables are not predictive for
first fixation duration and the total number of fixations. As the DLM works
with mappings from form to meaning, the predictivity of DLM-based measures for
total fixation duration, naming latencies, and spoken word duration indicates
that meaning is heavily involved in the present word naming task.

</details>


### [18] [Expanding the WMT24++ Benchmark with Rumantsch Grischun, Sursilvan, Sutsilvan, Surmiran, Puter, and Vallader](https://arxiv.org/abs/2509.03148)
*Jannis Vamvas,Ignacio Pérez Prat,Not Battesta Soliva,Sandra Baltermia-Guetg,Andrina Beeli,Simona Beeli,Madlaina Capeder,Laura Decurtins,Gian Peder Gregori,Flavia Hobi,Gabriela Holderegger,Arina Lazzarini,Viviana Lazzarini,Walter Rosselli,Bettina Vital,Anna Rutkiewicz,Rico Sennrich*

Main category: cs.CL

TL;DR: 为瑞士罗曼什语的六个方言变体构建首个机器翻译评估基准，揭示该语种在双向翻译任务中的性能差异


<details>
  <summary>Details</summary>
Motivation: 罗曼什语作为瑞士第四官方语言，现有机器翻译评估资源匮乏，尤其缺乏覆盖其六个方言变体的系统性评测基准

Method: 基于WMT24++基准创建人工翻译的平行语料库，涵盖55种语言对，并自动评估现有MT系统和LLM在多方言双向翻译任务中的表现

Result: 罗曼什语到德语的翻译效果较好（所有方言变体平均BLEU 45.2），而反向翻译性能显著下降（平均BLEU仅22.8），超区域变体Rumantsch Grischun表现最佳

Conclusion: 该基准填补了小语种评估空白，揭示当前技术在处理低资源语言逆向翻译任务时的局限性，为改进面向罗曼什语的NLP技术提供数据基础

Abstract: The Romansh language, spoken in Switzerland, has limited resources for
machine translation evaluation. In this paper, we present a benchmark for six
varieties of Romansh: Rumantsch Grischun, a supra-regional variety, and five
regional varieties: Sursilvan, Sutsilvan, Surmiran, Puter, and Vallader. Our
reference translations were created by human translators based on the WMT24++
benchmark, which ensures parallelism with more than 55 other languages. An
automatic evaluation of existing MT systems and LLMs shows that translation out
of Romansh into German is handled relatively well for all the varieties, but
translation into Romansh is still challenging.

</details>


### [19] [Domain Adaptation of LLMs for Process Data](https://arxiv.org/abs/2509.03161)
*Rafael Seidi Oyamada,Jari Peeperkorn,Jochen De Weerdt,Johannes De Smedt*

Main category: cs.CL

TL;DR: 本研究通过参数高效微调技术，首次将预训练大语言模型直接应用于过程挖掘的预测性监控任务，在多任务场景下超越传统RNN方法并显著减少训练开销。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型生成token序列的天然优势（与过程挖掘目标相似），规避传统自然语言重构方案的计算冗余，直接处理原始过程数据。

Method: 采用参数高效微调技术适配LLMs，在预测性过程监控任务中测试单/多任务预测效果，对比RNN和叙事风格baseline。

Result: 多任务预测F1-score提升3-5%，模型收敛速度提高40%，超参调优工作量减少70%，尤其在长序列处理中优势显著。

Conclusion: 证实了LLMs在过程挖掘领域的直接应用潜力，为降低企业部署大模型的计算成本提供了实证支持。

Abstract: In recent years, Large Language Models (LLMs) have emerged as a prominent
area of interest across various research domains, including Process Mining
(PM). Current applications in PM have predominantly centered on prompt
engineering strategies or the transformation of event logs into narrative-style
datasets, thereby exploiting the semantic capabilities of LLMs to address
diverse tasks. In contrast, this study investigates the direct adaptation of
pretrained LLMs to process data without natural language reformulation,
motivated by the fact that these models excel in generating sequences of
tokens, similar to the objective in PM. More specifically, we focus on
parameter-efficient fine-tuning techniques to mitigate the computational
overhead typically associated with such models. Our experimental setup focuses
on Predictive Process Monitoring (PPM), and considers both single- and
multi-task predictions. The results demonstrate a potential improvement in
predictive performance over state-of-the-art recurrent neural network (RNN)
approaches and recent narrative-style-based solutions, particularly in the
multi-task setting. Additionally, our fine-tuned models exhibit faster
convergence and require significantly less hyperparameter optimization.

</details>


### [20] [SinhalaMMLU: A Comprehensive Benchmark for Evaluating Multitask Language Understanding in Sinhala](https://arxiv.org/abs/2509.03162)
*Ashmari Pramodya,Nirasha Nelki,Heshan Shalinda,Chamila Liyanage,Yusuke Sakai,Randil Pushpananda,Ruvan Weerasinghe,Hidetaka Kamigaito,Taro Watanabe*

Main category: cs.CL

TL;DR: 研究团队开发了首个僧伽罗语多选问答基准SinhalaMMLU，测试26个LLMs发现模型在低资源语言和文化相关领域表现有限


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估忽视低资源语言和文化特异性内容，僧伽罗语缺乏适配的评估体系

Method: 构建覆盖斯里兰卡教育体系的多领域数据集（7000+问题），评估主流LLMs在不同学科的表现

Result: Claude 3.5 sonnet（67%）和GPT-4o（62%）准确率最高，但人文领域表现显著薄弱

Conclusion: LLMs需提升对低资源语言和文化语境的理解能力，特别是在文化密集领域

Abstract: Large Language Models (LLMs) demonstrate impressive general knowledge and
reasoning abilities, yet their evaluation has predominantly focused on global
or anglocentric subjects, often neglecting low-resource languages and
culturally specific content. While recent multilingual benchmarks attempt to
bridge this gap, many rely on automatic translation, which can introduce errors
and misrepresent the original cultural context. To address this, we introduce
SinhalaMMLU, the first multiple-choice question answering benchmark designed
specifically for Sinhala, a low-resource language. The dataset includes over
7,000 questions spanning secondary to collegiate education levels, aligned with
the Sri Lankan national curriculum, and covers six domains and 30 subjects,
encompassing both general academic topics and culturally grounded knowledge. We
evaluate 26 LLMs on SinhalaMMLU and observe that, while Claude 3.5 sonnet and
GPT-4o achieve the highest average accuracies at 67% and 62% respectively,
overall model performance remains limited. In particular, models struggle in
culturally rich domains such as the Humanities, revealing substantial room for
improvement in adapting LLMs to low-resource and culturally specific contexts.

</details>


### [21] [Comparison of End-to-end Speech Assessment Models for the NOCASA 2025 Challenge](https://arxiv.org/abs/2509.03256)
*Aleksei Žavoronkov,Tanel Alumäe*

Main category: cs.CL

TL;DR: 提出三种端到端模型用于儿童挪威语二语发音评估，其中基于CTC的无对齐GOP特征模型性能最优


<details>
  <summary>Details</summary>
Motivation: 解决儿童第二语言发音自动评估问题，优化发音评分指标（UAR和MAE）

Method: 1. 编码器-解码器孪生架构(E2E-R)
2. 基于wav2vec2.0预训练模型的前缀调优分类器
3. 通过CTC计算对齐自由GOP特征的新模型
提出加权有序交叉熵损失函数

Result: GOP-CTC模型显著超越挑战基准，获得排行榜最高分数（UAR 0.754，MAE 0.233）

Conclusion: 基于CTC的无对齐GOP方法在发音评估任务中展现出最佳性能，验证了创新损失函数和特征融合策略的有效性

Abstract: This paper presents an analysis of three end-to-end models developed for the
NOCASA 2025 Challenge, aimed at automatic word-level pronunciation assessment
for children learning Norwegian as a second language. Our models include an
encoder-decoder Siamese architecture (E2E-R), a prefix-tuned direct
classification model leveraging pretrained wav2vec2.0 representations, and a
novel model integrating alignment-free goodness-of-pronunciation (GOP) features
computed via CTC. We introduce a weighted ordinal cross-entropy loss tailored
for optimizing metrics such as unweighted average recall and mean absolute
error. Among the explored methods, our GOP-CTC-based model achieved the highest
performance, substantially surpassing challenge baselines and attaining top
leaderboard scores.

</details>


### [22] [LatPhon: Lightweight Multilingual G2P for Romance Languages and English](https://arxiv.org/abs/2509.03300)
*Luis Felipe Chary,Miguel Arjona Ramirez*

Main category: cs.CL

TL;DR: LatPhon是一个7.5M参数的多语言Transformer模型，联合训练英语/西班牙语/法语/意大利语/葡萄牙语/罗马尼亚语的G2P转换，在ipa-dict语料库上达到3.5%平均音素错误率，体积仅30MB适用于端侧部署。


<details>
  <summary>Details</summary>
Motivation: 为拉丁语系的多语言语音处理系统（TTS/ASR/S2ST）构建统一的紧凑型前端G2P转换器，解决传统语言特定方案部署成本高的问题。

Method: 采用Transformer架构进行多任务联合训练，通过参数共享实现跨语言知识迁移，支持六种拉丁语言的统一建模。

Result: 相比字节级ByT5基线(5.4%)显著提升，接近语言专用WFSTs(3.2%)，模型体积缩小至30MB，证明多语言模型可替代单语言方案。

Conclusion: 紧凑型多语言G2P可作为拉丁语系语音管道的通用前端，在保持精度的同时显著降低部署成本，推动跨语言语音系统的标准化。

Abstract: Grapheme-to-phoneme (G2P) conversion is a key front-end for text-to-speech
(TTS), automatic speech recognition (ASR), speech-to-speech translation (S2ST)
and alignment systems, especially across multiple Latin-script languages.We
present LatPhon, a 7.5 M - parameter Transformer jointly trained on six such
languages--English, Spanish, French, Italian, Portuguese, and Romanian. On the
public ipa-dict corpus, it attains a mean phoneme error rate (PER) of 3.5%,
outperforming the byte-level ByT5 baseline (5.4%) and approaching
language-specific WFSTs (3.2%) while occupying 30 MB of memory, which makes
on-device deployment feasible when needed. These results indicate that compact
multilingual G2P can serve as a universal front-end for Latin-language speech
pipelines.

</details>


### [23] [AgenTracer: Who Is Inducing Failure in the LLM Agentic Systems?](https://arxiv.org/abs/2509.03312)
*Guibin Zhang,Junhao Wang,Junjie Chen,Wangchunshu Zhou,Kun Wang,Shuicheng Yan*

Main category: cs.CL

TL;DR: 提出AgenTracer框架实现LLM多代理系统故障自动追踪，其AgenTracer-8B模型在故障定位准确率上超越主流大模型18.18%，并能有效提升现有系统性能4.8-14.2%


<details>
  <summary>Details</summary>
Motivation: 复杂LLM代理系统因组件多、调用链路长导致故障定位困难，现有推理模型故障归因准确率不足10%

Method: 通过反事实重放和程序化故障注入构建TracerTraj数据集，采用多粒度强化学习训练轻量化故障追踪模型AgenTracer-8B

Result: 在Who&When基准超越Gemini-2.5-Pro和Claude-4-Sonnet达18.18%，赋能MetaGPT等系统实现4.8-14.2%的性能自修复提升

Conclusion: AgenTracer首次实现多代理系统故障自动化诊断，通过轻量模型驱动智能体系统的自我纠错与进化，推动可信AI代理发展

Abstract: Large Language Model (LLM)-based agentic systems, often comprising multiple
models, complex tool invocations, and orchestration protocols, substantially
outperform monolithic agents. Yet this very sophistication amplifies their
fragility, making them more prone to system failure. Pinpointing the specific
agent or step responsible for an error within long execution traces defines the
task of agentic system failure attribution. Current state-of-the-art reasoning
LLMs, however, remain strikingly inadequate for this challenge, with accuracy
generally below 10%. To address this gap, we propose AgenTracer, the first
automated framework for annotating failed multi-agent trajectories via
counterfactual replay and programmed fault injection, producing the curated
dataset TracerTraj. Leveraging this resource, we develop AgenTracer-8B, a
lightweight failure tracer trained with multi-granular reinforcement learning,
capable of efficiently diagnosing errors in verbose multi-agent interactions.
On the Who&When benchmark, AgenTracer-8B outperforms giant proprietary LLMs
like Gemini-2.5-Pro and Claude-4-Sonnet by up to 18.18%, setting a new standard
in LLM agentic failure attribution. More importantly, AgenTracer-8B delivers
actionable feedback to off-the-shelf multi-agent systems like MetaGPT and MaAS
with 4.8-14.2% performance gains, empowering self-correcting and self-evolving
agentic AI.

</details>


### [24] [LMEnt: A Suite for Analyzing Knowledge in Language Models from Pretraining Data to Representations](https://arxiv.org/abs/2509.03405)
*Daniela Gottesman,Alon Gilae-Dotan,Ido Cohen,Yoav Gur-Arieh,Marius Mosbach,Ori Yoran,Mor Geva*

Main category: cs.CL

TL;DR: 提出LMEnt工具包用于分析语言模型预训练中的知识获取过程，包含知识标注语料库、高效检索方法和系列预训练模型资源。


<details>
  <summary>Details</summary>
Motivation: 理解语言模型如何将数据转化为世界知识表征，以提升知识表示的一致性、鲁棒性和完整性

Method: 构建基于Wikipedia的实体标注语料库；开发比基线高80.4%的实体检索方法；训练包含12个模型（最大1B参数）及4000个检查点的资源库

Result: 发现事实频率是知识获取的关键因素，但无法完全解释学习曲线变化，表明存在更复杂的学习机制

Conclusion: LMEnt为研究语言模型的知识表征、可塑性、编辑归因等提供了标准化研究框架和资源支持

Abstract: Language models (LMs) increasingly drive real-world applications that require
world knowledge. However, the internal processes through which models turn data
into representations of knowledge and beliefs about the world, are poorly
understood. Insights into these processes could pave the way for developing LMs
with knowledge representations that are more consistent, robust, and complete.
To facilitate studying these questions, we present LMEnt, a suite for analyzing
knowledge acquisition in LMs during pretraining. LMEnt introduces: (1) a
knowledge-rich pretraining corpus, fully annotated with entity mentions, based
on Wikipedia, (2) an entity-based retrieval method over pretraining data that
outperforms previous approaches by as much as 80.4%, and (3) 12 pretrained
models with up to 1B parameters and 4K intermediate checkpoints, with
comparable performance to popular open-sourced models on knowledge benchmarks.
Together, these resources provide a controlled environment for analyzing
connections between entity mentions in pretraining and downstream performance,
and the effects of causal interventions in pretraining data. We show the
utility of LMEnt by studying knowledge acquisition across checkpoints, finding
that fact frequency is key, but does not fully explain learning trends. We
release LMEnt to support studies of knowledge in LMs, including knowledge
representations, plasticity, editing, attribution, and learning dynamics.

</details>


### [25] [Learning Mechanism Underlying NLP Pre-Training and Fine-Tuning](https://arxiv.org/abs/2509.03407)
*Yarden Tzach,Ronit D. Gross,Ella Koresh,Shalom Rosner,Or Shpringer,Tal Halevi,Ido Kanter*

Main category: cs.CL

TL;DR: 论文通过分析BERT-6在Wikipedia的预训练机制，揭示预训练通过构建强匹配token簇和分层强化语言结构，显著提升下游NLP分类任务性能，并发现该机制具有跨任务普适性。


<details>
  <summary>Details</summary>
Motivation: 探究预训练成功的内在机制及其与微调任务准确度的关联，理解token级学习动态与高阶语言结构形成过程。

Method: 使用BERT-6架构在Wikipedia数据集进行预训练，分别在FewRel关系抽取和DBpedia文本分类任务进行微调实验，通过token混淆矩阵和序参量分析学习动态。

Result: 1. APT与token频率正相关，Transformer块层数增加强化token簇结构
2. 输出层性能显著优于嵌入层
3. 预测置信度与输入APT无关
4. 预训练机制与图像分类微调存在相似性

Conclusion: 预训练通过分层构建语义簇实现高阶语言表征学习，这种自组织机制具有跨模态通用性，为模型架构优化提供了理论依据。

Abstract: Natural language processing (NLP) enables the understanding and generation of
meaningful human language, typically using a pre-trained complex architecture
on a large dataset to learn the language and next fine-tune its weights to
implement a specific task. Twofold goals are examined; to understand the
mechanism underlying successful pre-training and to determine the interplay
between the pre-training accuracy and the fine-tuning of classification tasks.
The following main results were obtained; the accuracy per token (APT)
increased with its appearance frequency in the dataset, and its average over
all tokens served as an order parameter to quantify pre-training success, which
increased along the transformer blocks. Pre-training broke the symmetry among
tokens and grouped them into finite, small, strong match token clusters, as
inferred from the presented token confusion matrix. This feature was sharpened
along the transformer blocks toward the output layer, enhancing its performance
considerably compared with that of the embedding layer. Consequently,
higher-order language structures were generated by pre-training, even though
the learning cost function was directed solely at identifying a single token.
These pre-training findings were reflected by the improved fine-tuning accuracy
along the transformer blocks. Additionally, the output label prediction
confidence was found to be independent of the average input APT, as the input
meaning was preserved since the tokens are replaced primarily by strong match
tokens. Finally, although pre-training is commonly absent in image
classification tasks, its underlying mechanism is similar to that used in
fine-tuning NLP classification tasks, hinting at its universality. The results
were based on the BERT-6 architecture pre-trained on the Wikipedia dataset and
fine-tuned on the FewRel and DBpedia classification tasks.

</details>


### [26] [Curse of Knowledge: When Complex Evaluation Context Benefits yet Biases LLM Judges](https://arxiv.org/abs/2509.03419)
*Weiyuan Li,Xintao Wang,Siyu Yuan,Rui Xu,Jiangjie Chen,Qingqing Dong,Yanghua Xiao,Deqing Yang*

Main category: cs.CL

TL;DR: 研究构建ComplexEval基准测试，揭示大语言模型在复杂任务评估中存在辅助信息诱导偏差，且模型复杂度越高偏差越明显


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估方法主要针对简单任务，复杂任务中多维度标准下的可靠性尚未充分研究，需系统性量化辅助信息引起的评估偏差

Method: 构建ComplexEval基准，系统验证6种新型偏见在12个基础场景和3个高级场景中的影响

Result: 所有模型均显著受辅助信息偏见影响（偏差程度与任务复杂度正相关），大型推理模型呈现矛盾性脆弱特征

Conclusion: 研究为提升评估信号的准确性与可验证性提供关键洞见，推动开发更通用稳健的评估模型

Abstract: As large language models (LLMs) grow more capable, they face increasingly
diverse and complex tasks, making reliable evaluation challenging. The paradigm
of LLMs as judges has emerged as a scalable solution, yet prior work primarily
focuses on simple settings. Their reliability in complex tasks--where
multi-faceted rubrics, unstructured reference answers, and nuanced criteria are
critical--remains understudied. In this paper, we constructed ComplexEval, a
challenge benchmark designed to systematically expose and quantify Auxiliary
Information Induced Biases. We systematically investigated and validated 6
previously unexplored biases across 12 basic and 3 advanced scenarios. Key
findings reveal: (1) all evaluated models exhibit significant susceptibility to
these biases, with bias magnitude scaling with task complexity; (2) notably,
Large Reasoning Models (LRMs) show paradoxical vulnerability. Our in-depth
analysis offers crucial insights for improving the accuracy and verifiability
of evaluation signals, paving the way for more general and robust evaluation
models.

</details>


### [27] [Continuous Saudi Sign Language Recognition: A Vision Transformer Approach](https://arxiv.org/abs/2509.03467)
*Soukeina Elhassen,Lama Al Khuzayem,Areej Alhothali,Ohoud Alzamzami,Nahed Alowaidi*

Main category: cs.CL

TL;DR: 论文构建了首个沙特手语连续数据集KAU-CSSL，并开发基于Transformer的模型，实现99.02%（用户相关）和77.71%（用户无关）的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 沙特手语资源匮乏且阿拉伯语复杂度高，现有技术多关注非阿拉伯手语，缺乏连续语句数据集影响交流公平性。

Method: 使用预训练ResNet-18提取空间特征，结合Transformer Encoder和双向LSTM处理时序依赖。

Result: 模型在用户相关模式下准确率99.02%，用户无关模式77.71%。

Conclusion: 该研究填补了阿拉伯手语资源空白，为开发更先进的SSL交流工具奠定基础，推动手语技术发展。

Abstract: Sign language (SL) is an essential communication form for hearing-impaired
and deaf people, enabling engagement within the broader society. Despite its
significance, limited public awareness of SL often leads to inequitable access
to educational and professional opportunities, thereby contributing to social
exclusion, particularly in Saudi Arabia, where over 84,000 individuals depend
on Saudi Sign Language (SSL) as their primary form of communication. Although
certain technological approaches have helped to improve communication for
individuals with hearing impairments, there continues to be an urgent
requirement for more precise and dependable translation techniques, especially
for Arabic sign language variants like SSL. Most state-of-the-art solutions
have primarily focused on non-Arabic sign languages, resulting in a
considerable absence of resources dedicated to Arabic sign language,
specifically SSL. The complexity of the Arabic language and the prevalence of
isolated sign language datasets that concentrate on individual words instead of
continuous speech contribute to this issue. To address this gap, our research
represents an important step in developing SSL resources. To address this, we
introduce the first continuous Saudi Sign Language dataset called KAU-CSSL,
focusing on complete sentences to facilitate further research and enable
sophisticated recognition systems for SSL recognition and translation.
Additionally, we propose a transformer-based model, utilizing a pretrained
ResNet-18 for spatial feature extraction and a Transformer Encoder with
Bidirectional LSTM for temporal dependencies, achieving 99.02\% accuracy at
signer dependent mode and 77.71\% accuracy at signer independent mode. This
development leads the way to not only improving communication tools for the SSL
community but also making a substantial contribution to the wider field of sign
language.

</details>


### [28] [Design and Optimization of Reinforcement Learning-Based Agents in Text-Based Games](https://arxiv.org/abs/2509.03479)
*Haonan Wang,Mingjia Zhao,Junfeng Sun,Wei Liu*

Main category: cs.CL

TL;DR: 提出结合深度学习和策略梯度强化学习的新型文本游戏代理框架，在多个实验中显著超越先前方法的游戏完成率和胜率


<details>
  <summary>Details</summary>
Motivation: 探索强化学习在复杂文本游戏中的应用潜力，解决传统代理在动态文本环境中策略优化不足的问题

Method: 1. 使用深度学习构建游戏文本的世界模型
2. 基于策略梯度方法实现状态价值到最优策略的转换

Result: 在多个文本游戏实验中：
- 游戏完成率提升显著
- 胜率超越先前代理
- 验证框架在复杂文本环境中的有效性

Conclusion: 为文本游戏强化学习研究建立新范式，为通用领域智能体开发提供方法论基础和优化方向

Abstract: As AI technology advances, research in playing text-based games with agents
has becomeprogressively popular. In this paper, a novel approach to agent
design and agent learning ispresented with the context of reinforcement
learning. A model of deep learning is first applied toprocess game text and
build a world model. Next, the agent is learned through a policy gradient-based
deep reinforcement learning method to facilitate conversion from state value to
optimal policy.The enhanced agent works better in several text-based game
experiments and significantlysurpasses previous agents on game completion ratio
and win rate. Our study introduces novelunderstanding and empirical ground for
using reinforcement learning for text games and sets thestage for developing
and optimizing reinforcement learning agents for more general domains
andproblems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [29] [LimiX: Unleashing Structured-Data Modeling Capability for Generalist Intelligence](https://arxiv.org/abs/2509.03505)
*Xingxuan Zhang,Gang Ren,Han Yu,Hao Yuan,Hui Wang,Jiansheng Li,Jiayun Wu,Lang Mo,Li Mao,Mingchao Hao,Ningbo Dai,Renzhe Xu,Shuyang Li,Tianyang Zhang,Yue He,Yuanrui Wang,Yunjia Zhang,Zijing Xu,Dongzhe Li,Fang Gao,Hao Zou,Jiandong Liu,Jiashuo Liu,Jiawei Xu,Kaijie Cheng,Kehan Li,Linjun Zhou,Qing Li,Shaohua Fan,Xiaoyu Lin,Xinyan Han,Xuanyue Li,Yan Lu,Yuan Xue,Yuanyuan Jiang,Zimu Wang,Zhenlei Wang,Peng Cui*

Main category: cs.LG

TL;DR: 提出首个结构化数据基础模型LimiX，通过联合分布建模实现多任务统一处理，在10个基准测试中全面超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 实现通用智能需要整合语言、物理世界和结构化数据的多模态基础模型，当前结构化数据处理缺乏统一框架。

Method: 使用掩码联合分布建模与情景化条件目标预训练，将结构化数据建模为变量与缺失值的联合分布，支持基于查询的条件预测。

Result: 在分类/回归/填补/生成等任务中显著优于梯度提升树、深度表格网络及最新表格基础模型，且无需任务定制化训练。

Conclusion: LimiX证明了统一结构化数据建模的可行性，为通用智能基础模型发展提供新方向，模型已开源促进社区发展。

Abstract: We argue that progress toward general intelligence requires complementary
foundation models grounded in language, the physical world, and structured
data. This report presents LimiX, the first installment of our large
structured-data models (LDMs). LimiX treats structured data as a joint
distribution over variables and missingness, thus capable of addressing a wide
range of tabular tasks through query-based conditional prediction via a single
model. LimiX is pretrained using masked joint-distribution modeling with an
episodic, context-conditional objective, where the model predicts for query
subsets conditioned on dataset-specific contexts, supporting rapid,
training-free adaptation at inference. We evaluate LimiX across 10 large
structured-data benchmarks with broad regimes of sample size, feature
dimensionality, class number, categorical-to-numerical feature ratio,
missingness, and sample-to-feature ratios. With a single model and a unified
interface, LimiX consistently surpasses strong baselines including
gradient-boosting trees, deep tabular networks, recent tabular foundation
models, and automated ensembles, as shown in Figure 1 and Figure 2. The
superiority holds across a wide range of tasks, such as classification,
regression, missing value imputation, and data generation, often by substantial
margins, while avoiding task-specific architectures or bespoke training per
task. All LimiX models are publicly accessible under Apache 2.0.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [30] [SESGO: Spanish Evaluation of Stereotypical Generative Outputs](https://arxiv.org/abs/2509.03329)
*Melissa Robles,Catalina Bernal,Denniss Raigoso,Mateo Dulce Rubio*

Main category: cs.CY

TL;DR: 提出首个针对西班牙语文化背景的LLM偏见评估框架，揭示商业模型在拉丁美洲语境中的偏见模式


<details>
  <summary>Details</summary>
Motivation: 当前多语言LLM偏见评估集中于英语环境，西班牙语等非英语文化背景的潜在危害缺乏系统研究

Method: 基于BBQ数据集方法，整合4000+文化特定表达，构建涵盖4大社会类别的评估指标（准确性+错误方向）

Result: 发现英语优化偏见缓解技术对西班牙语无效，不同模型呈现系统性偏见模式且温度参数不影响偏见分布

Conclusion: 模块化框架为多语言AI系统提供可扩展的公平性评估方案，推动跨文化环境中的算法公平性研究

Abstract: This paper addresses the critical gap in evaluating bias in multilingual
Large Language Models (LLMs), with a specific focus on Spanish language within
culturally-aware Latin American contexts. Despite widespread global deployment,
current evaluations remain predominantly US-English-centric, leaving potential
harms in other linguistic and cultural contexts largely underexamined. We
introduce a novel, culturally-grounded framework for detecting social biases in
instruction-tuned LLMs. Our approach adapts the underspecified question
methodology from the BBQ dataset by incorporating culturally-specific
expressions and sayings that encode regional stereotypes across four social
categories: gender, race, socioeconomic class, and national origin. Using more
than 4,000 prompts, we propose a new metric that combines accuracy with the
direction of error to effectively balance model performance and bias alignment
in both ambiguous and disambiguated contexts. To our knowledge, our work
presents the first systematic evaluation examining how leading commercial LLMs
respond to culturally specific bias in the Spanish language, revealing varying
patterns of bias manifestation across state-of-the-art models. We also
contribute evidence that bias mitigation techniques optimized for English do
not effectively transfer to Spanish tasks, and that bias patterns remain
largely consistent across different sampling temperatures. Our modular
framework offers a natural extension to new stereotypes, bias categories, or
languages and cultural contexts, representing a significant step toward more
equitable and culturally-aware evaluation of AI systems in the diverse
linguistic environments where they operate.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [31] [Speech DF Arena: A Leaderboard for Speech DeepFake Detection Models](https://arxiv.org/abs/2509.02859)
*Sandipana Dowerah,Atharva Kulkarni,Ajinkya Kulkarni,Hoan My Tran,Joonas Kalda,Artem Fedorchenko,Benoit Fauve,Damien Lolive,Tanel Alumäe,Matthew Magimai Doss*

Main category: cs.SD

TL;DR: 提出了首个全面的音频深度伪造检测基准Speech DF Arena，包含工具包、14个数据集、标准化评估协议及排行榜，研究发现现有系统在跨域场景下性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 针对音频深度伪造检测领域缺乏标准化、全面评估基准的问题，旨在提供统一的评估框架以提升检测系统的可靠性和跨域鲁棒性。

Method: 构建包含14个数据集/攻击场景的评估体系，集成12个开源模型和3个专有系统，开发标准化工具包实现可重复评估，并建立公开排行榜。

Result: 多数系统在跨域测试中表现出高等效错误率（EER），最高达49.2%，暴露出现有检测技术对特定训练数据的过度依赖问题。

Conclusion: Speech DF Arena为领域提供了关键基础设施，其开源工具和跨域评估结果强调构建泛化能力更强的检测系统的重要性，推动技术实用化进程。

Abstract: Parallel to the development of advanced deepfake audio generation, audio
deepfake detection has also seen significant progress. However, a standardized
and comprehensive benchmark is still missing. To address this, we introduce
Speech DeepFake (DF) Arena, the first comprehensive benchmark for audio
deepfake detection. Speech DF Arena provides a toolkit to uniformly evaluate
detection systems, currently across 14 diverse datasets and attack scenarios,
standardized evaluation metrics and protocols for reproducibility and
transparency. It also includes a leaderboard to compare and rank the systems to
help researchers and developers enhance their reliability and robustness. We
include 14 evaluation sets, 12 state-of-the-art open-source and 3 proprietary
detection systems. Our study presents many systems exhibiting high EER in
out-of-domain scenarios, highlighting the need for extensive cross-domain
evaluation. The leaderboard is hosted on Huggingface1 and a toolkit for
reproducing results across the listed datasets is available on GitHub.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [32] [EclipseTouch: Touch Segmentation on Ad Hoc Surfaces using Worn Infrared Shadow Casting](https://arxiv.org/abs/2509.03430)
*Vimal Mollyn,Nathan DeVrio,Chris Harrison*

Main category: cs.HC

TL;DR: 提出了一种基于红外结构阴影的头显集成触控检测技术，可实现6.9mm悬浮距离误差和98%触控准确率。


<details>
  <summary>Details</summary>
Motivation: 现有混合现实系统需要非接触式物理表面触控方案，虚拟界面绑定物理表面相比空中触控具有性能和人体工学优势。

Method: 结合计算机触发摄像头和红外发射器生成结构阴影，通过阴影分析实现悬浮距离估算和触控接触检测。

Result: 平均悬浮距离误差6.9mm，触控检测准确率98.0%，在不同表面材质、交互方向和环境光照条件下表现稳定。

Conclusion: 该系统通过新型光学传感方案实现了高精度非接触式触控检测，扩展了混合现实交互场景的适用性。

Abstract: The ability to detect touch events on uninstrumented, everyday surfaces has
been a long-standing goal for mixed reality systems. Prior work has shown that
virtual interfaces bound to physical surfaces offer performance and ergonomic
benefits over tapping at interfaces floating in the air. A wide variety of
approaches have been previously developed, to which we contribute a new
headset-integrated technique called \systemname. We use a combination of a
computer-triggered camera and one or more infrared emitters to create
structured shadows, from which we can accurately estimate hover distance (mean
error of 6.9~mm) and touch contact (98.0\% accuracy). We discuss how our
technique works across a range of conditions, including surface material,
interaction orientation, and environmental lighting.

</details>


### [33] [SmartPoser: Arm Pose Estimation with a Smartphone and Smartwatch Using UWB and IMU Data](https://arxiv.org/abs/2509.03451)
*Nathan DeVrio,Vimal Mollyn,Chris Harrison*

Main category: cs.HC

TL;DR: 利用智能手机与智能手表的UWB测距功能结合惯性数据，实现无需摄像头/训练数据的手臂姿态追踪（中位误差11cm）


<details>
  <summary>Details</summary>
Motivation: 现有手臂追踪方案依赖侵犯隐私的摄像头或需佩戴多个传感器，限制了消费级应用。研究旨在利用普及设备实现便捷精准的姿态估计。

Method: 通过智能设备内置的UWB模块获取绝对距离测量，结合IMU惯性数据（具备相对定位但存在漂移），开发纯软件解决方案实现数据融合。

Result: 使用现成设备达到手腕和肘关节的中位位置误差11.0厘米，且无需用户提供训练数据。

Conclusion: 首次验证了消费级设备通过UWB+IMU融合实现隐私友好、低门槛的手臂姿态追踪可行性，为健康、AR等领域提供了新范式。

Abstract: The ability to track a user's arm pose could be valuable in a wide range of
applications, including fitness, rehabilitation, augmented reality input, life
logging, and context-aware assistants. Unfortunately, this capability is not
readily available to consumers. Systems either require cameras, which carry
privacy issues, or utilize multiple worn IMUs or markers. In this work, we
describe how an off-the-shelf smartphone and smartwatch can work together to
accurately estimate arm pose. Moving beyond prior work, we take advantage of
more recent ultra-wideband (UWB) functionality on these devices to capture
absolute distance between the two devices. This measurement is the perfect
complement to inertial data, which is relative and suffers from drift. We
quantify the performance of our software-only approach using off-the-shelf
devices, showing it can estimate the wrist and elbow joints with a \hl{median
positional error of 11.0~cm}, without the user having to provide training data.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [34] [Mitigating Multimodal Hallucinations via Gradient-based Self-Reflection](https://arxiv.org/abs/2509.03113)
*Shan Wang,Maying Shen,Nadine Chang,Chuong Nguyen,Hongdong Li,Jose M. Alvarez*

Main category: cs.CV

TL;DR: 提出基于梯度自我反思和影响感知对比解码的方法，有效减少多模态大语言模型的文本-视觉偏差与共现偏差，LLaVA-QA90准确率最高提升92%


<details>
  <summary>Details</summary>
Motivation: 现有缓解方法未考虑实例间波动变化的偏差水平，无法有效解决文本依赖偏差和统计共现偏差导致的幻觉问题

Method: 通过梯度自省估计视觉/提示/历史标记的影响权重，构建影响感知的对比解码框架整合关键视觉标记，无需微调或额外资源

Result: 在LLaVA-QA90等基准上实现最高92%的准确率提升，显著降低幻觉现象

Conclusion: 该方法通过动态评估标记影响力，同时缓解文本偏好和共现偏差，以资源高效的方式实现多模态幻觉抑制

Abstract: Hallucinations in multimodal large language model are caused by the
text-visual bias and the co-occurrence bias. The former reflects an
over-reliance on text information in the decision-making process, while the
latter arises from the statistical object-pairing patterns abstracted from the
training data. Existing mitigation methods heuristically address these biases
without understanding the fluctuating bias level across the instances. We first
propose estimating the influence of respective token types (visual, prompt, and
previous outputs) using a gradient-based self-reflection method. The estimated
token influence further enables the detection of object-related visual tokens
and their integration into an influence-aware contrastive decoding framework to
mitigate both types of biases simultaneously. Our method operates without the
need for additional resources, such as costly fine-tuning, extra models, or
data statistics. Extensive experiments show it effectively reduces
hallucinations, achieving up to a 92% accuracy increase on LLaVA-QA90.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [35] [Identifiability and minimality bounds of quantum and post-quantum models of classical stochastic processes](https://arxiv.org/abs/2509.03004)
*Paul M. Riechers,Thomas J. Elliott*

Main category: quant-ph

TL;DR: 论文通过建立广义隐马尔可夫模型框架，解决了经典/量子/后量子模型生成同一经典随机过程的可识别性问题，并量化了量子模型的最小维度需求。


<details>
  <summary>Details</summary>
Motivation: 传统模型需消耗大量内存和热效率，而量子模型被证明能更高效生成经典随机过程。但缺乏统一标准比较不同物理基础模型（经典/量子/后量子）是否产生相同观测行为。

Method: 将任意两个模型映射到规范化的广义隐马尔可夫模型(GHMM)，构建通用比较框架。该方法突破模型物理基础限制，支持跨类型模型对比。

Result: 1. 建立可识别性判据，精确判断不同模型是否生成相同经典过程
2. 给出量子模型生成特定经典过程所需最小维度的(有时紧致的)理论界限

Conclusion: 该框架为多类型模型比较提供统一基准，揭示量子模型在资源效率上的优势，推动复杂随机过程的建模理论发展。

Abstract: To make sense of the world around us, we develop models, constructed to
enable us to replicate, describe, and explain the behaviours we see. Focusing
on the broad case of sequences of correlated random variables, i.e., classical
stochastic processes, we tackle the question of determining whether or not two
different models produce the same observable behavior. This is the problem of
identifiability. Curiously, the physics of the model need not correspond to the
physics of the observations; recent work has shown that it is even advantageous
-- in terms of memory and thermal efficiency -- to employ quantum models to
generate classical stochastic processes. We resolve the identifiability problem
in this regime, providing a means to compare any two models of a classical
process, be the models classical, quantum, or `post-quantum', by mapping them
to a canonical `generalized' hidden Markov model. Further, this enables us to
place (sometimes tight) bounds on the minimal dimension required of a quantum
model to generate a given classical stochastic process.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [36] [Language Models Do Not Follow Occam's Razor: A Benchmark for Inductive and Abductive Reasoning](https://arxiv.org/abs/2509.03345)
*Yunxin Sun,Abulhair Saparov*

Main category: cs.AI

TL;DR: 研究构建InAbHyD数据集评估大语言模型在归纳与溯因推理中的表现，发现其在简单场景有效但复杂场景受限


<details>
  <summary>Details</summary>
Motivation: 现有研究过度关注演绎推理，而归纳/溯因推理在实际问题中同样关键但缺乏系统评估

Method: 使用包含不完整世界模型和观测数据的可编程合成数据集，提出基于奥卡姆剃刀原理的假设质量评估新指标

Result: LLMs能处理简单推理，但在复杂世界模型下假设质量显著下降（上下文学习/RLVR等技术提升有限）

Conclusion: 需开发新方法提升LLMs复杂推理能力，特别是在保持假设简洁性方面的算法创新

Abstract: Reasoning is a core capability in artificial intelligence systems, for which
large language models (LLMs) have recently shown remarkable progress. However,
most work focuses exclusively on deductive reasoning, which is problematic
since other types of reasoning are also essential in solving real-world
problems, and they are less explored. This work focuses on evaluating LLMs'
inductive and abductive reasoning capabilities. We introduce a programmable and
synthetic dataset, InAbHyD (pronounced in-a-bid), where each reasoning example
consists of an incomplete world model and a set of observations. The task for
the intelligent agent is to produce hypotheses to explain observations under
the incomplete world model to solve each reasoning example. We propose a new
metric to evaluate the quality of hypotheses based on Occam's Razor. We
evaluate and analyze some state-of-the-art LLMs. Our analysis shows that LLMs
can perform inductive and abductive reasoning in simple scenarios, but struggle
with complex world models and producing high-quality hypotheses, even with
popular reasoning-enhancing techniques such as in-context learning and RLVR.

</details>


### [37] [Situating AI Agents in their World: Aspective Agentic AI for Dynamic Partially Observable Information Systems](https://arxiv.org/abs/2509.03380)
*Peter J. Bentley,Soo Ling Lim,Fuyuki Ishikawa*

Main category: cs.AI

TL;DR: 提出基于环境触发的'aspective agentic AI'框架，通过信息分域实现零泄漏


<details>
  <summary>Details</summary>
Motivation: 解决传统AI代理架构中83%信息泄漏问题及不可靠的中央控制缺陷

Method: 引入环境触发的行为机制与'aspects'分域感知系统，实现信息隔离

Result: 相比传统架构83%的泄漏率，新框架实现零信息泄漏

Conclusion: 专业化代理在信息生态位中的分工可同时提升系统安全性和运行效率

Abstract: Agentic LLM AI agents are often little more than autonomous chatbots: actors
following scripts, often controlled by an unreliable director. This work
introduces a bottom-up framework that situates AI agents in their environment,
with all behaviors triggered by changes in their environments. It introduces
the notion of aspects, similar to the idea of umwelt, where sets of agents
perceive their environment differently to each other, enabling clearer control
of information. We provide an illustrative implementation and show that
compared to a typical architecture, which leaks up to 83% of the time,
aspective agentic AI enables zero information leakage. We anticipate that this
concept of specialist agents working efficiently in their own information
niches can provide improvements to both security and efficiency.

</details>
