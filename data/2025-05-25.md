<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 128]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.LG](#cs.LG) [Total: 9]
- [cs.AI](#cs.AI) [Total: 11]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.CV](#cs.CV) [Total: 10]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.DB](#cs.DB) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [BR-TaxQA-R: A Dataset for Question Answering with References for Brazilian Personal Income Tax Law, including case law](https://arxiv.org/abs/2505.15916)
*Juvenal Domingos Júnior,Augusto Faria,E. Seiti de Oliveira,Erick de Brito,Matheus Teotonio,Andre Assumpção,Diedre Carmo,Roberto Lotufo,Jayr Pereira*

Main category: cs.CL

TL;DR: 提出巴西税务问答数据集BR-TaxQA-R，定制RAG系统在响应相关性优于商业模型，但事实准确性和流畅性较弱，强调人工法律审核必要性


<details>
  <summary>Details</summary>
Motivation: 构建支持巴西个人所得税法领域带参考文献的精准问答系统，评估AI系统在专业法律场景中的有效性

Method: 整合715个官方税务问答构建数据集，采用OpenAI嵌入和GPT-4o-mini搭建RAG系统，通过RAGAS指标对比不同文本分割策略与商业工具

Result: 定制RAG系统响应相关性得分更高（0.89 vs 0.82），但ChatGPT事实准确率领先（0.92 vs 0.85），呈现法律依据与语言流畅性的权衡

Conclusion: 在税务等高风险领域，AI生成内容需结合专业法律评估确保有效性，当前技术尚未完全替代人工审核

Abstract: This paper presents BR-TaxQA-R, a novel dataset designed to support question
answering with references in the context of Brazilian personal income tax law.
The dataset contains 715 questions from the 2024 official Q\&A document
published by Brazil's Internal Revenue Service, enriched with statutory norms
and administrative rulings from the Conselho Administrativo de Recursos Fiscais
(CARF). We implement a Retrieval-Augmented Generation (RAG) pipeline using
OpenAI embeddings for searching and GPT-4o-mini for answer generation. We
compare different text segmentation strategies and benchmark our system against
commercial tools such as ChatGPT and Perplexity.ai using RAGAS-based metrics.
Results show that our custom RAG pipeline outperforms commercial systems in
Response Relevancy, indicating stronger alignment with user queries, while
commercial models achieve higher scores in Factual Correctness and fluency.
These findings highlight a trade-off between legally grounded generation and
linguistic fluency. Crucially, we argue that human expert evaluation remains
essential to ensure the legal validity of AI-generated answers in high-stakes
domains such as taxation. BR-TaxQA-R is publicly available at
https://huggingface.co/datasets/unicamp-dl/BR-TaxQA-R.

</details>


### [2] [Extracting Probabilistic Knowledge from Large Language Models for Bayesian Network Parameterization](https://arxiv.org/abs/2505.15918)
*Aliakbar Nafar,Kristen Brent Venable,Zijun Cui,Parisa Kordjamshidi*

Main category: cs.CL

TL;DR: 提出利用大语言模型（LLMs）提取概率知识构建贝叶斯网络，结合少量真实数据实现自动建模。


<details>
  <summary>Details</summary>
Motivation: LLMs作为知识库的潜力已被证实，但其对现实事件概率知识的生成能力尚未充分研究。探索如何利用LLMs参数化贝叶斯网络以实现概率建模。

Method: 在80个公开贝叶斯网络（涵盖医疗/金融领域）上实验，比较LLM生成条件概率与基线方法（随机/均匀分布、token概率法）的表现。

Result: LLM生成的条件概率显著优于基线方法，且能作为专家先验知识有效修正小数据提取的分布，降低系统偏差。

Conclusion: 首次系统验证LLM提取概率知识的可行性，提出LLM概率知识+小数据的贝叶斯网络自动构建策略，并建立完整的评估基线体系。

Abstract: Large Language Models (LLMs) have demonstrated potential as factual knowledge
bases; however, their capability to generate probabilistic knowledge about
real-world events remains understudied. This paper investigates using
probabilistic knowledge inherent in LLMs to derive probability estimates for
statements concerning events and their interrelationships captured via a
Bayesian Network (BN). Using LLMs in this context allows for the
parameterization of BNs, enabling probabilistic modeling within specific
domains. Experiments on eighty publicly available Bayesian Networks, from
healthcare to finance, demonstrate that querying LLMs about the conditional
probabilities of events provides meaningful results when compared to baselines,
including random and uniform distributions, as well as approaches based on
next-token generation probabilities. We explore how these LLM-derived
distributions can serve as expert priors to refine distributions extracted from
minimal data, significantly reducing systematic biases. Overall, this work
introduces a promising strategy for automatically constructing Bayesian
Networks by combining probabilistic knowledge extracted from LLMs with small
amounts of real-world data. Additionally, we evaluate several prompting
strategies for eliciting probabilistic knowledge from LLMs and establish the
first comprehensive baseline for assessing LLM performance in extracting
probabilistic knowledge.

</details>


### [3] [Aligning Dialogue Agents with Global Feedback via Large Language Model Reward Decomposition](https://arxiv.org/abs/2505.15922)
*Dong Won Lee,Hae Won Park,Cynthia Breazeal,Louis-Philippe Morency*

Main category: cs.CL

TL;DR: 提出基于大语言模型的奖励分解框架，通过分解全局反馈信号实现对话代理对齐，无需人工奖励设计和细粒度反馈


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法需要手动设计奖励函数或依赖细粒度人工反馈，LLM的推理能力可自动分解会话级反馈为细粒度奖励

Method: 1. 文本模式：仅用对话文本进行奖励分解；2. 多模态模式：整合音调/注视/面部表情等行为线索的自然语言描述，蒸馏轻量级奖励模型进行强化学习微调

Result: 在对话质量的人类评估中，文本和多模态变体均显著优于现有奖励分解方法

Conclusion: LLM作为强大的奖励分解器，有效规避人工奖励塑造需求，证明语言模型具备自动推断细粒度反馈的潜力

Abstract: We propose a large language model based reward decomposition framework for
aligning dialogue agents using only a single session-level feedback signal. We
leverage the reasoning capabilities of a frozen, pretrained large language
model (LLM) to infer fine-grained local implicit rewards by decomposing global,
session-level feedback. Our first text-only variant prompts the LLM to perform
reward decomposition using only the dialogue transcript. The second multimodal
variant incorporates additional behavioral cues, such as pitch, gaze, and
facial affect, expressed as natural language descriptions. These inferred
turn-level rewards are distilled into a lightweight reward model, which we
utilize for RL-based fine-tuning for dialogue generation. We evaluate both
text-only and multimodal variants against state-of-the-art reward decomposition
methods and demonstrate notable improvements in human evaluations of
conversation quality, suggesting that LLMs are strong reward decomposers that
obviate the need for manual reward shaping and granular human feedback.

</details>


### [4] [Citation Parsing and Analysis with Language Models](https://arxiv.org/abs/2505.15948)
*Parth Sarin,Juan Pablo Alperin*

Main category: cs.CL

TL;DR: 开发基于Qwen3-0.6B语言模型的引用解析工具，通过32次迭代实现高精度引文标注，显著提升全球南方研究的索引可见性


<details>
  <summary>Details</summary>
Motivation: 全球南方国家在知识共享网络追踪工具的缺失导致其学者被主流索引服务排斥，加剧学术领域的殖民结构不平等

Method: 构建包含预印本和已发表论文的引文数据集，评估多个开放权重语言模型在引文结构化标注任务中的性能表现

Result: 未经微调的语言模型即超越现有方法，最小模型Qwen3-0.6B通过32次迭代实现全字段高精度解析（F1值达0.92）

Conclusion: 该工具可有效改善引文网络保真度，推动研究索引革新和元科学发展，为缩小全球学术资源鸿沟提供技术解决方案

Abstract: A key type of resource needed to address global inequalities in knowledge
production and dissemination is a tool that can support journals in
understanding how knowledge circulates. The absence of such a tool has resulted
in comparatively less information about networks of knowledge sharing in the
Global South. In turn, this gap authorizes the exclusion of researchers and
scholars from the South in indexing services, reinforcing colonial arrangements
that de-center and minoritize those scholars. In order to support citation
network tracking on a global scale, we investigate the capacity of open-weight
language models to mark up manuscript citations in an indexable format. We
assembled a dataset of matched plaintext and annotated citations from preprints
and published research papers. Then, we evaluated a number of open-weight
language models on the annotation task. We find that, even out of the box,
today's language models achieve high levels of accuracy on identifying the
constituent components of each citation, outperforming state-of-the-art
methods. Moreover, the smallest model we evaluated, Qwen3-0.6B, can parse all
fields with high accuracy in $2^5$ passes, suggesting that post-training is
likely to be effective in producing small, robust citation parsing models. Such
a tool could greatly improve the fidelity of citation networks and thus
meaningfully improve research indexing and discovery, as well as further
metascientific research.

</details>


### [5] [Training Step-Level Reasoning Verifiers with Formal Verification Tools](https://arxiv.org/abs/2505.15960)
*Ryo Kamoi,Yusen Zhang,Nan Zhang,Sarkar Snigdha Sarathi Das,Rui Zhang*

Main category: cs.CL

TL;DR: 提出FoVer方法，利用形式化验证工具自动标注步骤错误，训练过程奖励模型（PRMs）实现跨任务泛化


<details>
  <summary>Details</summary>
Motivation: 解决现有过程奖励模型依赖人工标注步骤错误（成本高）且局限于数学推理任务的问题

Method: 使用Z3（形式逻辑）和Isabelle（定理证明）等工具自动验证符号任务，合成带错误标签的训练数据，训练基于LLM的PRMs

Result: 在ProcessBench上步骤验证准确率显著优于基线模型，12个推理基准（包括MATH/AIME等）的Best-of-K性能达到或超过人工标注的SOTA模型

Conclusion: 形式化验证可实现自动准确的错误标注，训练出的PRMs具有跨任务泛化能力，降低对人类标注的依赖

Abstract: Process Reward Models (PRMs), which provide step-by-step feedback on the
reasoning generated by Large Language Models (LLMs), are receiving increasing
attention. However, two key research gaps remain: collecting accurate
step-level error labels for training typically requires costly human
annotation, and existing PRMs are limited to math reasoning problems. In
response to these gaps, this paper aims to address the challenges of automatic
dataset creation and the generalization of PRMs to diverse reasoning tasks. To
achieve this goal, we propose FoVer, an approach for training PRMs on
step-level error labels automatically annotated by formal verification tools,
such as Z3 for formal logic and Isabelle for theorem proof, which provide
automatic and accurate verification for symbolic tasks. Using this approach, we
synthesize a training dataset with error labels on LLM responses for formal
logic and theorem proof tasks without human annotation. Although this data
synthesis is feasible only for tasks compatible with formal verification, we
observe that LLM-based PRMs trained on our dataset exhibit cross-task
generalization, improving verification across diverse reasoning tasks.
Specifically, PRMs trained with FoVer significantly outperform baseline PRMs
based on the original LLMs and achieve competitive or superior results compared
to state-of-the-art PRMs trained on labels annotated by humans or stronger
models, as measured by step-level verification on ProcessBench and Best-of-K
performance across 12 reasoning benchmarks, including MATH, AIME, ANLI, MMLU,
and BBH. The datasets, models, and code are provided at
https://github.com/psunlpgroup/FoVer.

</details>


### [6] [Pre-training Large Memory Language Models with Internal and External Knowledge](https://arxiv.org/abs/2505.15962)
*Linxi Zhao,Sofian Zalouk,Christian K. Belardi,Justin Lovelace,Jin Peng Zhou,Kilian Q. Weinberger,Yoav Artzi,Jennifer J. Sun*

Main category: cs.CL

TL;DR: 提出新型大记忆语言模型(LMLM)，通过外部数据库与模型权重协同存储知识，实现可编辑、可验证的知识管理


<details>
  <summary>Details</summary>
Motivation: 传统神经语言模型将知识与语言模式混杂存储在数十亿不透明参数中，导致事实知识难以可靠检查、验证和更新

Method: 采用预训练策略，在外部数据库中存储事实知识，通过策略性遮蔽外部检索值训练模型进行定向查询而非依赖权重记忆

Result: LMLM在标准基准测试中与更大规模的知识密集型LLMs竞争，同时提供显式、可编辑、可验证的知识库优势

Conclusion: 本工作标志着语言模型与事实知识交互方式的根本性转变，推动AI系统向更透明可信的方向发展

Abstract: Neural language models are black-boxes -- both linguistic patterns and
factual knowledge are distributed across billions of opaque parameters. This
entangled encoding makes it difficult to reliably inspect, verify, or update
specific facts. We propose a new class of language models, Large Memory
Language Models (LMLM) with a pre-training recipe that stores factual knowledge
in both internal weights and an external database. Our approach strategically
masks externally retrieved factual values from the training loss, thereby
teaching the model to perform targeted lookups rather than relying on
memorization in model weights. Our experiments demonstrate that LMLMs achieve
competitive performance compared to significantly larger, knowledge-dense LLMs
on standard benchmarks, while offering the advantages of explicit, editable,
and verifiable knowledge bases. This work represents a fundamental shift in how
language models interact with and manage factual knowledge.

</details>


### [7] [Explaining Puzzle Solutions in Natural Language: An Exploratory Study on 6x6 Sudoku](https://arxiv.org/abs/2505.15993)
*Anirudh Maiya,Razan Alghamdi,Maria Leonor Pacheco,Ashutosh Trivedi,Fabio Somenzi*

Main category: cs.CL

TL;DR: 大型语言模型在数独解题中存在解释能力不足的挑战


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在人类-AI协作决策中的表现，特别是解释能力对复杂问题（如数独）解决的重要性

Method: 测试五个LLM模型在解决六宫格数独谜题时的表现，重点关注解题过程和解释能力

Result: 1个LLM展现有限解题能力，但所有模型均无法提供体现战略推理或直观解题的解释

Conclusion: LLMs要成为有效的人类-AI协作伙伴，需重点解决战略推理和解释能力的关键挑战

Abstract: The success of Large Language Models (LLMs) in human-AI collaborative
decision-making hinges on their ability to provide trustworthy, gradual, and
tailored explanations. Solving complex puzzles, such as Sudoku, offers a
canonical example of this collaboration, where clear and customized
explanations often hold greater importance than the final solution. In this
study, we evaluate the performance of five LLMs in solving and explaining
\sixsix{} Sudoku puzzles. While one LLM demonstrates limited success in solving
puzzles, none can explain the solution process in a manner that reflects
strategic reasoning or intuitive problem-solving. These findings underscore
significant challenges that must be addressed before LLMs can become effective
partners in human-AI collaborative decision-making.

</details>


### [8] [Leveraging Online Data to Enhance Medical Knowledge in a Small Persian Language Model](https://arxiv.org/abs/2505.16000)
*Mehrdad ghassabi,Pedram Rostami,Hamidreza Baradaran Kashani,Amirhossein Poursina,Zahra Kazemi,Milad Tavakoli*

Main category: cs.CL

TL;DR: 该研究构建首个波斯语医疗数据集，并通过微调提升小模型在医疗问答中的表现


<details>
  <summary>Details</summary>
Motivation: 解决波斯语等低资源语言环境下，小语言模型在医疗专业领域表现不足的问题（缺乏波斯语医疗数据集）

Method: 1. 爬取医学杂志语料库 2. 收集真实医患QA数据集 3. 使用定制数据微调基线模型

Result: 微调后模型在医疗问答准确率提升，生成回答质量优于基准模型

Conclusion: 验证了利用开放数据增强小语言模型在医疗领域的可行性，为资源受限环境提供波斯语医疗AI解决方案

Abstract: The rapid advancement of language models has demonstrated the potential of
artificial intelligence in the healthcare industry. However, small language
models struggle with specialized domains in low-resource languages like
Persian. While numerous medical-domain websites exist in Persian, no curated
dataset or corpus has been available making ours the first of its kind. This
study explores the enhancement of medical knowledge in a small language model
by leveraging accessible online data, including a crawled corpus from medical
magazines and a dataset of real doctor-patient QA pairs. We fine-tuned a
baseline model using our curated data to improve its medical knowledge.
Benchmark evaluations demonstrate that the fine-tuned model achieves improved
accuracy in medical question answering and provides better responses compared
to its baseline. This work highlights the potential of leveraging open-access
online data to enrich small language models in medical fields, providing a
novel solution for Persian medical AI applications suitable for
resource-constrained environments.

</details>


### [9] [Causal Interventions Reveal Shared Structure Across English Filler-Gap Constructions](https://arxiv.org/abs/2505.16002)
*Sasha Boguraev,Christopher Potts,Kyle Mahowald*

Main category: cs.CL

TL;DR: 大型语言模型通过因果可解释性方法揭示了英语填充-空缺结构共享抽象分析机制，为更新句法理论提供新依据


<details>
  <summary>Details</summary>
Motivation: 利用LLMs的内部机制分析验证语言学理论，突破传统基于语言产出的间接分析方法局限

Method: 采用分布式交换干预实验（Distributed Interchange Interventions），针对疑问句、关系从句等填充-空缺结构进行模型机制分析

Result: 发现LLMs对不同填充-空缺结构具有共享的抽象分析机制，揭示频率、填充词类型和上下文环境等被忽视的语言学理论要素

Conclusion: 基于LLMs的机械式内部机制分析能有效推动语言学理论发展，为句法分析提供新的方法论突破

Abstract: Large Language Models (LLMs) have emerged as powerful sources of evidence for
linguists seeking to develop theories of syntax. In this paper, we argue that
causal interpretability methods, applied to LLMs, can greatly enhance the value
of such evidence by helping us characterize the abstract mechanisms that LLMs
learn to use. Our empirical focus is a set of English filler-gap dependency
constructions (e.g., questions, relative clauses). Linguistic theories largely
agree that these constructions share many properties. Using experiments based
in Distributed Interchange Interventions, we show that LLMs converge on similar
abstract analyses of these constructions. These analyses also reveal previously
overlooked factors -- relating to frequency, filler type, and surrounding
context -- that could motivate changes to standard linguistic theory. Overall,
these results suggest that mechanistic, internal analyses of LLMs can push
linguistic theory forward.

</details>


### [10] [SLMEval: Entropy-Based Calibration for Human-Aligned Evaluation of Large Language Models](https://arxiv.org/abs/2505.16003)
*Roland Daynauth,Christopher Clarke,Krisztian Flautner,Lingjia Tang,Jason Mars*

Main category: cs.CL

TL;DR: 提出SLMEval校准方法解决LLM评估器在开放任务中与人类判断弱相关的问题，在真实场景和基准测试中实现显著效果并降低成本


<details>
  <summary>Details</summary>
Motivation: 现有校准方法在结构化基准表现良好，但在开放任务中效果差（与人类判断呈负相关），需开发更通用的评估校准方案

Method: 基于熵最大化和少量人类偏好数据，通过估计模型质量潜在分布并重新加权评估分数（SLMEval方法）

Result: 在真实生产用例中Spearman相关系数达0.57（优于G-Eval的负相关），评估成本比GPT-4校准方法降低5-30倍

Conclusion: SLMEval有效提升开放场景下LLM评估的准确性，同时显著降低评估成本，为实际应用提供高效解决方案

Abstract: The LLM-as-a-Judge paradigm offers a scalable, reference-free approach for
evaluating language models. Although several calibration techniques have been
proposed to better align these evaluators with human judgment, prior studies
focus primarily on narrow, well-structured benchmarks. As a result, it remains
unclear whether such calibrations generalize to real-world, open-ended tasks.
  In this work, we show that SOTA calibrated evaluators often fail in these
settings, exhibiting weak or even negative correlation with human judgments. To
address this, we propose SLMEval, a novel and efficient calibration method
based on entropy maximization over a small amount of human preference data. By
estimating a latent distribution over model quality and reweighting evaluator
scores accordingly, SLMEval achieves strong correlation with human evaluations
across two real-world production use cases and the public benchmark. For
example, on one such task, SLMEval achieves a Spearman correlation of 0.57 with
human judgments, while G-Eval yields a negative correlation. In addition,
SLMEval reduces evaluation costs by 5-30x compared to GPT-4-based calibrated
evaluators such as G-eval.

</details>


### [11] [LAGO: Few-shot Crosslingual Embedding Inversion Attacks via Language Similarity-Aware Graph Optimization](https://arxiv.org/abs/2505.16008)
*Wenrui Yu,Yiyi Chen,Johannes Bjerva,Sokol Kosta,Qiongxiu Li*

Main category: cs.CL

TL;DR: 提出LAGO（语言相似性感知图优化框架），通过建模语言关系显著提升少样本跨语言嵌入反转攻击效果


<details>
  <summary>Details</summary>
Motivation: 现有嵌入反转攻击独立处理各种语言，忽视了语言间的相似性特征对攻击迁移性的潜在价值

Method: 构建基于句法/词汇相似性的图约束分布式优化框架，结合Frobenius范数正则化与线性不等式约束，实现跨语言参数协同学习

Result: 在10样本条件下，Rouge-L指标较基线提升10-20%，验证语言相似性约束对攻击迁移性的增益效果

Conclusion: 揭示语言相似性是跨语言隐私攻击的关键因子，呼吁开发语言感知的隐私保护嵌入方法

Abstract: We propose LAGO - Language Similarity-Aware Graph Optimization - a novel
approach for few-shot cross-lingual embedding inversion attacks, addressing
critical privacy vulnerabilities in multilingual NLP systems. Unlike prior work
in embedding inversion attacks that treat languages independently, LAGO
explicitly models linguistic relationships through a graph-based constrained
distributed optimization framework. By integrating syntactic and lexical
similarity as edge constraints, our method enables collaborative parameter
learning across related languages. Theoretically, we show this formulation
generalizes prior approaches, such as ALGEN, which emerges as a special case
when similarity constraints are relaxed. Our framework uniquely combines
Frobenius-norm regularization with linear inequality or total variation
constraints, ensuring robust alignment of cross-lingual embedding spaces even
with extremely limited data (as few as 10 samples per language). Extensive
experiments across multiple languages and embedding models demonstrate that
LAGO substantially improves the transferability of attacks with 10-20% increase
in Rouge-L score over baselines. This work establishes language similarity as a
critical factor in inversion attack transferability, urging renewed focus on
language-aware privacy-preserving multilingual embeddings.

</details>


### [12] [Ranking Free RAG: Replacing Re-ranking with Selection in RAG for Sensitive Domains](https://arxiv.org/abs/2505.16014)
*Yash Saxena,Anpur Padia,Mandar S Chaudhary,Kalpa Gunaratna,Srinivasan Parthasarathy,Manas Gaur*

Main category: cs.CL

TL;DR: METEORA提出用理性驱动选择替代传统RAG的相似性重排序，通过三阶段证据筛选机制提升生成准确率33.34%，并在对抗场景中显著增强防御能力


<details>
  <summary>Details</summary>
Motivation: 传统RAG流程依赖相似性检索和启发式重排序，存在可解释性不足、抗对抗攻击能力弱的问题。需要更可靠的证据选择机制来保证生成内容的安全性和可信度。

Method: 1. 偏好调整LLM生成查询相关的rationales
2. 三阶段证据选择：局部相关性匹配→肘部检测全局筛选→上下文扩展
3. Verifier LLM进行内容真实性验证

Result: 在6个跨领域数据集上实现生成准确率提升33.34%，证据块使用量减少50%。对抗场景下F1分数从0.10提升至0.44，抗毒化攻击能力显著增强

Conclusion: METEORA通过理性驱动的可解释证据流，在提升生成质量的同时增强了系统鲁棒性，为安全可靠的RAG系统提供了新范式

Abstract: Traditional Retrieval-Augmented Generation (RAG) pipelines rely on
similarity-based retrieval and re-ranking, which depend on heuristics such as
top-k, and lack explainability, interpretability, and robustness against
adversarial content. To address this gap, we propose a novel method METEORA
that replaces re-ranking in RAG with a rationale-driven selection approach.
METEORA operates in two stages. First, a general-purpose LLM is
preference-tuned to generate rationales conditioned on the input query using
direct preference optimization. These rationales guide the evidence chunk
selection engine, which selects relevant chunks in three stages: pairing
individual rationales with corresponding retrieved chunks for local relevance,
global selection with elbow detection for adaptive cutoff, and context
expansion via neighboring chunks. This process eliminates the need for top-k
heuristics. The rationales are also used for consistency check using a Verifier
LLM to detect and filter poisoned or misleading content for safe generation.
The framework provides explainable and interpretable evidence flow by using
rationales consistently across both selection and verification. Our evaluation
across six datasets spanning legal, financial, and academic research domains
shows that METEORA improves generation accuracy by 33.34% while using
approximately 50% fewer chunks than state-of-the-art re-ranking methods. In
adversarial settings, METEORA significantly improves the F1 score from 0.10 to
0.44 over the state-of-the-art perplexity-based defense baseline, demonstrating
strong resilience to poisoning attacks. Code available at:
https://anonymous.4open.science/r/METEORA-DC46/README.md

</details>


### [13] [NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning](https://arxiv.org/abs/2505.16022)
*Wei Liu,Siya Qi,Xinyu Wang,Chen Qian,Yali Du,Yulan He*

Main category: cs.CL

TL;DR: 提出无需外部验证器的NOVER强化学习框架，仅需监督微调数据即可实现跨任务激励训练，性能超越同规模蒸馏模型7.7%


<details>
  <summary>Details</summary>
Motivation: 现有激励训练方法依赖外部验证器，限制了其在数学/编程以外领域的应用，且奖励模型需要高成本标注数据

Method: 设计NOVER框架：1）基于标准监督数据自动生成奖励信号 2）支持文本到文本任务的通用强化学习 3）允许逆向激励训练等新型优化方式

Result: 在多个文本任务中：1）比同规模模型提升7.7% 2）超越从DeepSeek R1 671B蒸馏的模型 3）展示逆向训练等新可能性

Conclusion: NOVER突破了验证器依赖限制，为通用NLP任务提供高效强化学习方案，同时开辟了语言模型优化新路径

Abstract: Recent advances such as DeepSeek R1-Zero highlight the effectiveness of
incentive training, a reinforcement learning paradigm that computes rewards
solely based on the final answer part of a language model's output, thereby
encouraging the generation of intermediate reasoning steps. However, these
methods fundamentally rely on external verifiers, which limits their
applicability to domains like mathematics and coding where such verifiers are
readily available. Although reward models can serve as verifiers, they require
high-quality annotated data and are costly to train. In this work, we propose
NOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning
framework that requires only standard supervised fine-tuning data with no need
for an external verifier. NOVER enables incentive training across a wide range
of text-to-text tasks and outperforms the model of the same size distilled from
large reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the
flexibility of NOVER enables new possibilities for optimizing large language
models, such as inverse incentive training.

</details>


### [14] [Prototypical Human-AI Collaboration Behaviors from LLM-Assisted Writing in the Wild](https://arxiv.org/abs/2505.16023)
*Sheshera Mysore,Debarati Das,Hancheng Cao,Bahareh Sarrafzadeh*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As large language models (LLMs) are used in complex writing workflows, users
engage in multi-turn interactions to steer generations to better fit their
needs. Rather than passively accepting output, users actively refine, explore,
and co-construct text. We conduct a large-scale analysis of this collaborative
behavior for users engaged in writing tasks in the wild with two popular AI
assistants, Bing Copilot and WildChat. Our analysis goes beyond simple task
classification or satisfaction estimation common in prior work and instead
characterizes how users interact with LLMs through the course of a session. We
identify prototypical behaviors in how users interact with LLMs in prompts
following their original request. We refer to these as Prototypical Human-AI
Collaboration Behaviors (PATHs) and find that a small group of PATHs explain a
majority of the variation seen in user-LLM interaction. These PATHs span users
revising intents, exploring texts, posing questions, adjusting style or
injecting new content. Next, we find statistically significant correlations
between specific writing intents and PATHs, revealing how users' intents shape
their collaboration behaviors. We conclude by discussing the implications of
our findings on LLM alignment.

</details>


### [15] [OpenEthics: A Comprehensive Ethical Evaluation of Open-Source Generative Large Language Models](https://arxiv.org/abs/2505.16036)
*Burak Erinç Çetin,Yıldırım Özen,Elif Naz Demiryılmaz,Kaan Engür,Cagri Toraman*

Main category: cs.CL

TL;DR: 对29个开源大语言模型进行多语言伦理评估，发现大参数模型在安全/公平表现更优，可靠性仍是挑战


<details>
  <summary>Details</summary>
Motivation: 现有研究存在伦理维度狭窄、语言覆盖单一、模型多样性不足的缺陷，需更全面的伦理评估框架

Method: 采用LLM-as-a-Judge方法，从鲁棒性/可靠性/安全性/公平性四个维度，评估英语和土耳其语场景下的29个模型

Result: 多数模型优先优化安全与公平，大模型（如Gemma/Qwen）伦理表现更优，可靠性存在缺陷，伦理评估结果与语言无关

Conclusion: 建立了独立于语言的伦理评估体系，证明模型参数规模与伦理表现正相关，为安全模型开发提供指导框架

Abstract: Generative large language models present significant potential but also raise
critical ethical concerns. Most studies focus on narrow ethical dimensions, and
also limited diversity of languages and models. To address these gaps, we
conduct a broad ethical evaluation of 29 recent open-source large language
models using a novel data collection including four ethical aspects:
Robustness, reliability, safety, and fairness. We analyze model behavior in
both a commonly used language, English, and a low-resource language, Turkish.
Our aim is to provide a comprehensive ethical assessment and guide safer model
development by filling existing gaps in evaluation breadth, language coverage,
and model diversity. Our experimental results, based on LLM-as-a-Judge, reveal
that optimization efforts for many open-source models appear to have
prioritized safety and fairness, and demonstrated good robustness while
reliability remains a concern. We demonstrate that ethical evaluation can be
effectively conducted independently of the language used. In addition, models
with larger parameter counts tend to exhibit better ethical performance, with
Gemma and Qwen models demonstrating the most ethical behavior among those
evaluated.

</details>


### [16] [Internal and External Impacts of Natural Language Processing Papers](https://arxiv.org/abs/2505.16061)
*Yu Zhang*

Main category: cs.CL

TL;DR: 分析1979-2024年顶级NLP会议的学术影响力与社会影响差异


<details>
  <summary>Details</summary>
Motivation: 探究自然语言处理研究在学术界和公共领域的实际影响差异，揭示不同研究主题的应用价值与社会关注度

Method: 通过分析学术论文引用及专利/媒体/政策文件等外部数据源，构建跨领域影响力评估框架

Result: 语言模型技术具有最大影响力，基础语言学研究影响较弱；伦理类主题在政策领域受关注但学术引用少；不同外部领域存在应用偏好分化

Conclusion: NLP研究需平衡技术创新与社会责任，建议强化学术界对伦理研究的投入，建立多维度影响力评估体系

Abstract: We investigate the impacts of NLP research published in top-tier conferences
(i.e., ACL, EMNLP, and NAACL) from 1979 to 2024. By analyzing citations from
research articles and external sources such as patents, media, and policy
documents, we examine how different NLP topics are consumed both within the
academic community and by the broader public. Our findings reveal that language
modeling has the widest internal and external influence, while linguistic
foundations have lower impacts. We also observe that internal and external
impacts generally align, but topics like ethics, bias, and fairness show
significant attention in policy documents with much fewer academic citations.
Additionally, external domains exhibit distinct preferences, with patents
focusing on practical NLP applications and media and policy documents engaging
more with the societal implications of NLP models.

</details>


### [17] [Small Language Models in the Real World: Insights from Industrial Text Classification](https://arxiv.org/abs/2505.16078)
*Lujun Li,Lama Sleem,Niccolo' Gentile,Geoffrey Nichil,Radu State*

Main category: cs.CL

TL;DR: 本文评估小语言模型通过提示工程与监督微调在工业级文本分类任务中的可行性，发现其能在保持性能的同时显著降低算力需求。


<details>
  <summary>Details</summary>
Motivation: 针对大模型推理效率低、显存消耗高、提示工程依赖性强的问题，探索小模型在文本分类任务中的实用性及部署可行性。

Method: 采用提示工程与监督微调双路径，在邮件分类、法律文本分类、超长学术文本分类等工业场景进行系统性评估，重点关注VRAM利用效率。

Result: 验证了小模型通过针对性优化可达到与大模型相当的分类精度，同时实现10倍以上的VRAM使用效率提升，揭示模型规模与任务适配性的非线性关系。

Conclusion: 证明适当优化的小模型能有效平衡性能与资源消耗，为工业场景的本地化部署提供高性价比解决方案，推动紧凑型Transformer的实际应用。

Abstract: With the emergence of ChatGPT, Transformer models have significantly advanced
text classification and related tasks. Decoder-only models such as Llama
exhibit strong performance and flexibility, yet they suffer from inefficiency
on inference due to token-by-token generation, and their effectiveness in text
classification tasks heavily depends on prompt quality. Moreover, their
substantial GPU resource requirements often limit widespread adoption. Thus,
the question of whether smaller language models are capable of effectively
handling text classification tasks emerges as a topic of significant interest.
However, the selection of appropriate models and methodologies remains largely
underexplored. In this paper, we conduct a comprehensive evaluation of prompt
engineering and supervised fine-tuning methods for transformer-based text
classification. Specifically, we focus on practical industrial scenarios,
including email classification, legal document categorization, and the
classification of extremely long academic texts. We examine the strengths and
limitations of smaller models, with particular attention to both their
performance and their efficiency in Video Random-Access Memory (VRAM)
utilization, thereby providing valuable insights for the local deployment and
application of compact models in industrial settings.

</details>


### [18] [BiasLab: Toward Explainable Political Bias Detection with Dual-Axis Annotations and Rationale Indicators](https://arxiv.org/abs/2505.16081)
*KMA Solaiman*

Main category: cs.CL

TL;DR: BiasLab是一个包含300篇政治新闻标注的数据集，通过双向量化标注和丰富理由注释，支持可解释的政治偏见检测模型开发。


<details>
  <summary>Details</summary>
Motivation: 解决NLP系统中政治偏见检测缺乏可解释性标注的问题，促进透明化AI系统的构建。

Method: ① 从900篇文档池精选政治新闻 ② 设计双向量化标注体系（民主/共和党情感倾向） ③ 结合众包标注与GPT-4o模拟标注 ④ 建立感知漂移预测和理由分类基线模型

Result: ① 发现标注者间一致性较高（Krippendorff's α=0.78） ② 揭示15%文章存在来源偏见与内容感知偏差 ③ GPT-4o在右倾内容误判率达23%（人类标注组仅8%）

Conclusion: BiasLab通过结构化理由标注体系，为可解释NLP系统开发提供新基准，特别在政治偏见检测的算法透明性方面具有突破价值。

Abstract: We present BiasLab, a dataset of 300 political news articles annotated for
perceived ideological bias. These articles were selected from a curated
900-document pool covering diverse political events and source biases. Each
article is labeled by crowdworkers along two independent scales, assessing
sentiment toward the Democratic and Republican parties, and enriched with
rationale indicators. The annotation pipeline incorporates targeted worker
qualification and was refined through pilot-phase analysis. We quantify
inter-annotator agreement, analyze misalignment with source-level outlet bias,
and organize the resulting labels into interpretable subsets. Additionally, we
simulate annotation using schema-constrained GPT-4o, enabling direct comparison
to human labels and revealing mirrored asymmetries, especially in
misclassifying subtly right-leaning content. We define two modeling tasks:
perception drift prediction and rationale type classification, and report
baseline performance to illustrate the challenge of explainable bias detection.
BiasLab's rich rationale annotations provide actionable interpretations that
facilitate explainable modeling of political bias, supporting the development
of transparent, socially aware NLP systems. We release the dataset, annotation
schema, and modeling code to encourage research on human-in-the-loop
interpretability and the evaluation of explanation effectiveness in real-world
settings.

</details>


### [19] [Date Fragments: A Hidden Bottleneck of Tokenization for Temporal Reasoning](https://arxiv.org/abs/2505.16088)
*Gagan Bhatia,Maxime Peyrard,Wei Zhao*

Main category: cs.CL

TL;DR: 研究揭示BPE分词器的日期碎片化问题，提出量化指标和评测基准，发现大语言模型通过层级注意力机制重组日期片段的新机制


<details>
  <summary>Details</summary>
Motivation: 现代分词器将日期拆分为无意义片段（如20250312→202/503/12），破坏时间推理所需的结构化信息，导致模型在处理历史/未来日期时准确率显著下降

Method: 1. 提出可解释的日期碎片化比率指标
2. 构建包含6500个样本的DateAugBench评测集（含上下文日期解析、格式不变性谜题、跨时代日期运算）
3. 通过层级探针分析和因果注意力跳研究，揭示LLM的日期抽象机制

Result: 1. 过度碎片化导致历史/未来日期准确率下降10%
2. 模型越大，日期片段的层级重组速度越快
3. LLM遵循年→月→日的推理路径（与人类认知相反）

Conclusion: 日期碎片化严重影响时序推理性能，大模型通过层级注意力自愈该问题。该发现为分词器设计提供新视角，并为提升模型时间推理能力指明方向

Abstract: Modern BPE tokenizers often split calendar dates into meaningless fragments,
e.g., 20250312 $\rightarrow$ 202, 503, 12, inflating token counts and obscuring
the inherent structure needed for robust temporal reasoning. In this work, we
(1) introduce a simple yet interpretable metric, termed date fragmentation
ratio, that measures how faithfully a tokenizer preserves multi-digit date
components; (2) release DateAugBench, a suite of 6500 examples spanning three
temporal reasoning tasks: context-based date resolution, format-invariance
puzzles, and date arithmetic across historical, contemporary, and future
regimes; and (3) through layer-wise probing and causal attention-hop analyses,
uncover an emergent date-abstraction mechanism whereby large language models
stitch together the fragments of month, day, and year components for temporal
reasoning. Our experiments show that excessive fragmentation correlates with
accuracy drops of up to 10 points on uncommon dates like historical and
futuristic dates. Further, we find that the larger the model, the faster the
emergent date abstraction that heals date fragments is accomplished. Lastly, we
observe a reasoning path that LLMs follow to assemble date fragments, typically
differing from human interpretation (year $\rightarrow$ month $\rightarrow$
day).

</details>


### [20] [Continually Self-Improving Language Models for Bariatric Surgery Question--Answering](https://arxiv.org/abs/2505.16102)
*Yash Kumar Atri,Thomas H Shin,Thomas Hartvigsen*

Main category: cs.CL

TL;DR: 针对减重代谢手术（MBS）护理中的信息获取障碍，研究者开发了自适应RAG模型bRAGgen及配套数据集bRAGq，显著提升临床应答质量。


<details>
  <summary>Details</summary>
Motivation: 现有减重代谢手术的跨学科护理存在医疗资源获取障碍，导致患者难以及时获取循证医学信息。需通过技术创新解决临床信息支持不足的问题。

Method: 开发自适应RAG模型bRAGgen（实时整合最新医学证据）和领域专用数据集bRAGq（1,302个经专家验证的问题），采用LLM指标和外科专家双阶段评估体系。

Result: 在临床准确性、相关性等维度上，bRAGgen显著优于现有先进模型，专家评审确认其应答质量的优越性。

Conclusion: bRAGgen通过动态知识更新机制有效提升MBS护理信息支持，bRAGq为领域首个大规模评估基准，该方案有望改善患者长期预后。

Abstract: While bariatric and metabolic surgery (MBS) is considered the gold standard
treatment for severe and morbid obesity, its therapeutic efficacy hinges upon
active and longitudinal engagement with multidisciplinary providers, including
surgeons, dietitians/nutritionists, psychologists, and endocrinologists. This
engagement spans the entire patient journey, from preoperative preparation to
long-term postoperative management. However, this process is often hindered by
numerous healthcare disparities, such as logistical and access barriers, which
impair easy patient access to timely, evidence-based, clinician-endorsed
information. To address these gaps, we introduce bRAGgen, a novel adaptive
retrieval-augmented generation (RAG)-based model that autonomously integrates
real-time medical evidence when response confidence dips below dynamic
thresholds. This self-updating architecture ensures that responses remain
current and accurate, reducing the risk of misinformation. Additionally, we
present bRAGq, a curated dataset of 1,302 bariatric surgery--related questions,
validated by an expert bariatric surgeon. bRAGq constitutes the first
large-scale, domain-specific benchmark for comprehensive MBS care. In a
two-phase evaluation, bRAGgen is benchmarked against state-of-the-art models
using both large language model (LLM)--based metrics and expert surgeon review.
Across all evaluation dimensions, bRAGgen demonstrates substantially superior
performance in generating clinically accurate and relevant responses.

</details>


### [21] [Hierarchical Safety Realignment: Lightweight Restoration of Safety in Pruned Large Vision-Language Models](https://arxiv.org/abs/2505.16104)
*Yue Li,Xin Yi,Dongsheng Shi,Gerard de Melo,Xiaoling Wang,Linlin Wang*

Main category: cs.CL

TL;DR: 提出HSR方法通过层级安全重校准修复大视觉语言模型剪枝后的安全性能退化问题


<details>
  <summary>Details</summary>
Motivation: 模型剪枝导致大视觉语言模型安全性能显著下降，现有方法未针对性解决该问题

Method: 分层校准机制：先量化注意力头安全贡献度定位关键头，再选择性恢复其中的核心神经元

Result: 在多种模型和剪枝策略中验证，安全性能显著提升

Conclusion: 首次明确针对剪枝后大视觉语言模型的安全修复研究，提出层级安全校准新思路

Abstract: With the increasing size of Large Vision-Language Models (LVLMs), network
pruning techniques aimed at compressing models for deployment in
resource-constrained environments have garnered significant attention. However,
we observe that pruning often leads to a degradation in safety performance. To
address this issue, we present a novel and lightweight approach, termed
Hierarchical Safety Realignment (HSR). HSR operates by first quantifying the
contribution of each attention head to safety, identifying the most critical
ones, and then selectively restoring neurons directly within these attention
heads that play a pivotal role in maintaining safety. This process
hierarchically realigns the safety of pruned LVLMs, progressing from the
attention head level to the neuron level. We validate HSR across various models
and pruning strategies, consistently achieving notable improvements in safety
performance. To our knowledge, this is the first work explicitly focused on
restoring safety in LVLMs post-pruning.

</details>


### [22] [MPL: Multiple Programming Languages with Large Language Models for Information Extraction](https://arxiv.org/abs/2505.16107)
*Bo Li,Gexiang Fang,Wei Ye,Zhenghua Xu,Jinglei Zhang,Hao Cheng,Shikun Zhang*

Main category: cs.CL

TL;DR: 提出MPL框架，通过多编程语言监督微调和虚拟运行技术提升信息抽取效果


<details>
  <summary>Details</summary>
Motivation: 现有研究局限于Python的代码风格模拟，未探索C++/Java等其他编程语言在SFT阶段的潜力

Method: MPL框架整合多种编程语言监督微调，引入带虚拟运行的function-prompt技术模拟代码输入

Result: 多数据集实验验证有效性，并提供全面分析

Conclusion: 证明多编程语言协同优势，代码已开源促进后续研究

Abstract: Recent research in information extraction (IE) focuses on utilizing
code-style inputs to enhance structured output generation. The intuition behind
this is that the programming languages (PLs) inherently exhibit greater
structural organization than natural languages (NLs). This structural advantage
makes PLs particularly suited for IE tasks. Nevertheless, existing research
primarily focuses on Python for code-style simulation, overlooking the
potential of other widely-used PLs (e.g., C++ and Java) during the supervised
fine-tuning (SFT) phase. In this research, we propose \textbf{M}ultiple
\textbf{P}rogramming \textbf{L}anguages with large language models for
information extraction (abbreviated as \textbf{MPL}), a novel framework that
explores the potential of incorporating different PLs in the SFT phase.
Additionally, we introduce \texttt{function-prompt} with virtual running to
simulate code-style inputs more effectively and efficiently. Experimental
results on a wide range of datasets demonstrate the effectiveness of MPL.
Furthermore, we conduct extensive experiments to provide a comprehensive
analysis. We have released our code for future research.

</details>


### [23] [Semiotic Reconstruction of Destination Expectation Constructs An LLM-Driven Computational Paradigm for Social Media Tourism Analytics](https://arxiv.org/abs/2505.16118)
*Haotian Lan,Yao Gao,Yujun Cheng,Wei Yuan,Kun Wang*

Main category: cs.CL

TL;DR: 本研究构建双LLM框架提取用户生成内容中的旅游期待，发现休闲社交期待比自然情感因素更具驱动力


<details>
  <summary>Details</summary>
Motivation: 当前旅游决策分析缺乏可扩展方法，用户生成内容（UGC）的价值未被充分挖掘。社交媒体数据激增需要新的分析框架实现大规模期待量化

Method: 采用无监督LLM提取UGC中的旅游期待要素，结合问卷调查数据进行监督微调，建立双方法混合模型

Result: 休闲/社交期待对旅游参与的驱动力超过基础性自然/情感因素，验证了LLM在期待量化中的精准性

Conclusion: 该框架可拓展至消费行为研究，通过计算社会科学方法推动旅游体验个性化与社交旅行营销策略优化，展示市场营销优化的转型潜力

Abstract: Social media's rise establishes user-generated content (UGC) as pivotal for
travel decisions, yet analytical methods lack scalability. This study
introduces a dual-method LLM framework: unsupervised expectation extraction
from UGC paired with survey-informed supervised fine-tuning. Findings reveal
leisure/social expectations drive engagement more than foundational
natural/emotional factors. By establishing LLMs as precision tools for
expectation quantification, we advance tourism analytics methodology and
propose targeted strategies for experience personalization and social travel
promotion. The framework's adaptability extends to consumer behavior research,
demonstrating computational social science's transformative potential in
marketing optimization.

</details>


### [24] [KoBALT: Korean Benchmark For Advanced Linguistic Tasks](https://arxiv.org/abs/2505.16125)
*Hyopil Shin,Sangah Lee,Dongjun Jang,Wooseok Song,Jaeyoon Kim,Chaeyoung Oh,Hyemi Jo,Youngchae Ahn,Sihyun Oh,Hyohyeong Chang,Sunkyoung Kim,Jinsik Lee*

Main category: cs.CL

TL;DR: KoBALT是首个针对韩语开发的综合性语言学基准测试，包含700个专家设计的问题，用于评估LLM在5个语言领域的能力并揭示其显著性能差异。


<details>
  <summary>Details</summary>
Motivation: 传统基准测试缺乏语言学深度和类型学基础，尤其对形态复杂的韩语评估不足。KoBALT旨在解决数据污染问题，提供更可靠的语言理解评估框架。

Method: 构建24种语言现象的700道多选题(5大领域)，采用最小n-gram重叠原则设计。评估20个主流LLM，并通过95名标注者的人类偏好实验验证有效性。

Result: 最佳模型总体准确率61%，语义领域表现最佳(66%)，音系(31%)和形态学(36%)最弱。人类评估显示KoBALT分数与人工判断强相关(p<0.001)。

Conclusion: KoBALT填补了形态丰富语言的评估空白，建立了可靠的韩语能力评估体系，为类型学多样语言的LM评估提供了新范式。

Abstract: We introduce KoBALT (Korean Benchmark for Advanced Linguistic Tasks), a
comprehensive linguistically-motivated benchmark comprising 700 multiple-choice
questions spanning 24 phenomena across five linguistic domains: syntax,
semantics, pragmatics, phonetics/phonology, and morphology. KoBALT is designed
to advance the evaluation of large language models (LLMs) in Korean, a
morphologically rich language, by addressing the limitations of conventional
benchmarks that often lack linguistic depth and typological grounding. It
introduces a suite of expert-curated, linguistically motivated questions with
minimal n-gram overlap with standard Korean corpora, substantially mitigating
the risk of data contamination and allowing a more robust assessment of true
language understanding. Our evaluation of 20 contemporary LLMs reveals
significant performance disparities, with the highest-performing model
achieving 61\% general accuracy but showing substantial variation across
linguistic domains - from stronger performance in semantics (66\%) to
considerable weaknesses in phonology (31\%) and morphology (36\%). Through
human preference evaluation with 95 annotators, we demonstrate a strong
correlation between KoBALT scores and human judgments, validating our
benchmark's effectiveness as a discriminative measure of Korean language
understanding. KoBALT addresses critical gaps in linguistic evaluation for
typologically diverse languages and provides a robust framework for assessing
genuine linguistic competence in Korean language models.

</details>


### [25] [Veracity Bias and Beyond: Uncovering LLMs' Hidden Beliefs in Problem-Solving Reasoning](https://arxiv.org/abs/2505.16128)
*Yue Zhou,Barbara Di Eugenio*

Main category: cs.CL

TL;DR: 研究发现LLMs存在解决方案真实性与人口统计学关联的深层偏见，在数学/编程领域非裔美国人正确率被低估，写作评估中亚裔评分最低，且存在种族刻板色彩关联


<details>
  <summary>Details</summary>
Motivation: 揭示LLMs在教育和评估场景中潜藏的深层次人口偏见，超越传统表面刻板印象的研究范畴

Method: 通过数学/编程/常识/写作四类任务测试五个对齐人类价值观的LLM，分析解决方案归属与评估偏差，并研究可视化代码中的色彩刻板印象

Result: 发现普遍存在归因偏差（非裔正确解少28%）和评估偏差（亚裔写作评分低15%），且LLM自动为不同群体分配种族刻板色彩（如非裔→深棕，亚裔→浅黄）

Conclusion: 人口偏见深植于LLM推理过程，对教育评估应用构成威胁，需开发更全面的去偏方法超越现有表层对齐

Abstract: Despite LLMs' explicit alignment against demographic stereotypes, they have
been shown to exhibit biases under various social contexts. In this work, we
find that LLMs exhibit concerning biases in how they associate solution
veracity with demographics. Through experiments across five human value-aligned
LLMs on mathematics, coding, commonsense, and writing problems, we reveal two
forms of such veracity biases: Attribution Bias, where models
disproportionately attribute correct solutions to certain demographic groups,
and Evaluation Bias, where models' assessment of identical solutions varies
based on perceived demographic authorship. Our results show pervasive biases:
LLMs consistently attribute fewer correct solutions and more incorrect ones to
African-American groups in math and coding, while Asian authorships are least
preferred in writing evaluation. In additional studies, we show LLMs
automatically assign racially stereotypical colors to demographic groups in
visualization code, suggesting these biases are deeply embedded in models'
reasoning processes. Our findings indicate that demographic bias extends beyond
surface-level stereotypes and social context provocations, raising concerns
about LLMs' deployment in educational and evaluation settings.

</details>


### [26] [LLMs Are Not Scorers: Rethinking MT Evaluation with Generation-Based Methods](https://arxiv.org/abs/2505.16129)
*Hyang Cui*

Main category: cs.CL

TL;DR: 提出基于LLM生成参考译文的机器翻译质量评估新范式，通过语义相似度评分显著提升评估效果


<details>
  <summary>Details</summary>
Motivation: 现有LLM直接评分法在段落级别与人工评估相关性低，需开发更准确的评估方法

Method: 1. 使用仅解码器LLM生成参考译文 2. 通过句子嵌入计算语义相似度 3. 构建混合评估框架

Result: 在8个LLM和8种语言对测试中，新方法超越传统直接评分法及非LLM指标，最高提升25%相关性

Conclusion: 生成式评估结合语义分析形成混合方法，为机器翻译质量评估开辟新方向

Abstract: Recent studies have applied large language models (LLMs) to machine
translation quality estimation (MTQE) by prompting models to assign numeric
scores. Nonetheless, these direct scoring methods tend to show low
segment-level correlation with human judgments. In this paper, we propose a
generation-based evaluation paradigm that leverages decoder-only LLMs to
produce high-quality references, followed by semantic similarity scoring using
sentence embeddings. We conduct the most extensive evaluation to date in MTQE,
covering 8 LLMs and 8 language pairs. Empirical results show that our method
outperforms both intra-LLM direct scoring baselines and external non-LLM
reference-free metrics from MTME. These findings demonstrate the strength of
generation-based evaluation and support a shift toward hybrid approaches that
combine fluent generation with accurate semantic assessment.

</details>


### [27] [Position of Uncertainty: A Cross-Linguistic Study of Positional Bias in Large Language Models](https://arxiv.org/abs/2505.16134)
*Menschikov Mikhail,Alexander Kharitonov,Maiia Kotyga,Vadim Porvatov,Anna Zhukovskaya,David Kagramanyan,Egor Shvetsov,Evgeny Burnaev*

Main category: cs.CL

TL;DR: 研究发现大语言模型存在模型驱动的跨语言位置偏差，显式位置提示损害准确率，词序特性显著影响模型表现。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型位置偏差与语言多样性的相互作用，揭示不同语言中位置敏感度的差异及其对提示工程的影响。

Method: 采用五类语系差异显著的语言（英语/俄语/德语/印地语/越南语），通过控制位置变量分析模型响应模式与熵值变化。

Result: Qwen2.5-7B呈现后位置偏好，显式位置指导使多语言准确率下降15-30%，自由语序语言出现词序强制规范化现象。

Conclusion: 提示工程需考虑目标语言特性，位置偏差具有模型特异性，单纯依赖熵值无法有效预测模型输出质量。

Abstract: Large language models exhibit positional bias -- systematic neglect of
information at specific context positions -- yet its interplay with linguistic
diversity remains poorly understood. We present a cross-linguistic study across
five typologically distinct languages (English, Russian, German, Hindi,
Vietnamese), examining how positional bias interacts with model uncertainty,
syntax, and prompting. Key findings: (1) Positional bias is model-driven, with
language-specific variations -- Qwen2.5-7B favors late positions, challenging
assumptions of early-token bias; (2) Explicit positional guidance (e.g.,
correct context is at position X) reduces accuracy across languages,
undermining prompt-engineering practices; (3) Aligning context with positional
bias increases entropy, yet minimal entropy does not predict accuracy. (4) We
further uncover that LLMs differently impose dominant word order in
free-word-order languages like Hindi.

</details>


### [28] [Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.16142)
*Shicheng Xu,Liang Pang,Yunchang Zhu,Jia Gu,Zihao Wei,Jingcheng Deng,Feiyang Pan,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: 提出强化学习框架RLKD，通过生成结构奖励模型(GSRM)改善知识蒸馏效果，突破传统监督微调扁平化结构限制，释放学生模型推理潜力


<details>
  <summary>Details</summary>
Motivation: 传统监督微调(SFT)将教师模型的多分支推理结构压缩为线性token预测，导致学生模型无法学习真实的推理结构模式

Method: RLKD框架结合强化学习与生成结构奖励模型(GSRM)，将推理路径分解为元推理-解决步骤，通过结构对齐奖励引导学生模型内化多分支推理模式

Result: 实验显示RLKD仅用0.1%数据即超越标准SFT-RL流程，在GSRM指导下展现出比传统蒸馏方法更强的推理能力提升效果

Conclusion: 通过显式建模推理的隐式结构特征，RLKD开创了知识蒸馏新范式，为提升小模型复杂推理能力提供了结构感知的优化路径

Abstract: Distilling reasoning paths from teacher to student models via supervised
fine-tuning (SFT) provides a shortcut for improving the reasoning ability of
smaller Large Language Models (LLMs). However, the reasoning paths generated by
teacher models often reflect only surface-level traces of their underlying
authentic reasoning. Insights from cognitive neuroscience suggest that
authentic reasoning involves a complex interweaving between meta-reasoning
(which selects appropriate sub-problems from multiple candidates) and solving
(which addresses the sub-problem). This implies authentic reasoning has an
implicit multi-branch structure. Supervised fine-tuning collapses this rich
structure into a flat sequence of token prediction in the teacher's reasoning
path, preventing effective distillation of this structure to students. To
address this limitation, we propose RLKD, a reinforcement learning (RL)-based
distillation framework guided by a novel Generative Structure Reward Model
(GSRM). Our GSRM converts reasoning paths into multiple meta-reasoning-solving
steps and computes rewards to measure structural alignment between student and
teacher reasoning. RLKD combines this reward with RL, enabling student LLMs to
internalize the teacher's implicit multi-branch reasoning structure rather than
merely mimicking fixed output paths. Experiments show RLKD surpasses standard
SFT-RL pipelines even when trained on 0.1% of data under an RL-only regime,
unlocking greater student reasoning potential than SFT-based distillation.

</details>


### [29] [EduBench: A Comprehensive Benchmarking Dataset for Evaluating Large Language Models in Diverse Educational Scenarios](https://arxiv.org/abs/2505.16160)
*Bin Xu,Yu Bai,Huashan Sun,Yiguan Lin,Siming Liu,Xinyue Liang,Yaolin Li,Yang Gao,Heyan Huang*

Main category: cs.CL

TL;DR: 提出了首个教育场景多样化基准测试EduBench，包含9大场景和4000+教育情境数据，通过多维评估指标验证了小模型可达到与大模型相当的评估效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在教育场景的应用存在研究空白，现有模型未针对教育场景优化，缺乏专用评估体系。

Method: 1. 构建含9大教育场景的合成数据集
2. 设计覆盖12个维度的评估指标
3. 通过人工标注验证评估有效性
4. 训练小规模模型并与Deepseek V3等大模型对比

Result: 训练的小模型在测试集上达到与Deepseek V3、Qwen Max等顶尖大模型相当的表现，验证了基准有效性。

Conclusion: 为教育领域语言模型的开发与评估提供了数据基础和方法论支持，相关代码数据已开源。

Abstract: As large language models continue to advance, their application in
educational contexts remains underexplored and under-optimized. In this paper,
we address this gap by introducing the first diverse benchmark tailored for
educational scenarios, incorporating synthetic data containing 9 major
scenarios and over 4,000 distinct educational contexts. To enable comprehensive
assessment, we propose a set of multi-dimensional evaluation metrics that cover
12 critical aspects relevant to both teachers and students. We further apply
human annotation to ensure the effectiveness of the model-generated evaluation
responses. Additionally, we succeed to train a relatively small-scale model on
our constructed dataset and demonstrate that it can achieve performance
comparable to state-of-the-art large models (e.g., Deepseek V3, Qwen Max) on
the test set. Overall, this work provides a practical foundation for the
development and evaluation of education-oriented language models. Code and data
are released at https://github.com/ybai-nlp/EduBench.

</details>


### [30] [KNN-SSD: Enabling Dynamic Self-Speculative Decoding via Nearest Neighbor Layer Set Optimization](https://arxiv.org/abs/2505.16162)
*Mingbo Song,Heming Xia,Jun Zhang,Chak Tou Leong,Qiancheng Xu,Wenjie Li,Sujian Li*

Main category: cs.CL

TL;DR: 提出KNN-SSD算法，通过K近邻搜索匹配不同跳跃层与领域输入，提升自推测解码的领域泛化能力，实现LLM推理1.3-1.6倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有自推测解码（层跳跃机制）对领域变化敏感，导致加速性能显著下降。需增强该范式在跨领域场景下的适应能力。

Method: 采用K近邻搜索技术，根据输入领域动态匹配最优的层跳跃配置，提升领域泛化性。

Result: 在多模型、多任务测试中实现1.3-1.6倍的LLM推理加速。

Conclusion: KNN-SSD有效解决领域敏感性问题，在不增加训练成本的前提下显著提升推理效率，具有实际应用价值。

Abstract: Speculative Decoding (SD) has emerged as a widely used paradigm to accelerate
the inference of large language models (LLMs) without compromising generation
quality. It works by efficiently drafting multiple tokens using a compact model
and then verifying them in parallel using the target LLM. Notably,
Self-Speculative Decoding proposes skipping certain layers to construct the
draft model, which eliminates the need for additional parameters or training.
Despite its strengths, we observe in this work that drafting with layer
skipping exhibits significant sensitivity to domain shifts, leading to a
substantial drop in acceleration performance. To enhance the domain
generalizability of this paradigm, we introduce KNN-SSD, an algorithm that
leverages K-Nearest Neighbor (KNN) search to match different skipped layers
with various domain inputs. We evaluated our algorithm in various models and
multiple tasks, observing that its application leads to 1.3x-1.6x speedup in
LLM inference.

</details>


### [31] [Can LLMs Simulate Human Behavioral Variability? A Case Study in the Phonemic Fluency Task](https://arxiv.org/abs/2505.16164)
*Mengyang Qiu,Zoe Brisebois,Siena Sun*

Main category: cs.CL

TL;DR: LLMs能匹配人类音素流畅性任务的平均表现，但无法模拟人类行为变异性


<details>
  <summary>Details</summary>
Motivation: 验证LLMs是否能够模拟人类认知任务中的个体差异，特别是音素流畅性任务中的行为变异性

Method: 使用34种模型配置（不同提示、温度参数和模型类型），与106名人类参与者的表现进行对比分析

Result: Claude 3.7 Sonnet部分匹配人类平均表现，但所有模型输出多样性不足，结构僵化，集成模型也无法提升多样性，网络分析显示检索机制存在本质差异

Conclusion: LLMs在模拟人类认知行为时存在关键限制，特别是无法复现人类行为变异性

Abstract: Large language models (LLMs) are increasingly explored as substitutes for
human participants in cognitive tasks, but their ability to simulate human
behavioral variability remains unclear. This study examines whether LLMs can
approximate individual differences in the phonemic fluency task, where
participants generate words beginning with a target letter. We evaluated 34
model configurations, varying prompt specificity, sampling temperature, and
model type, and compared outputs to responses from 106 human participants.
While some configurations, especially Claude 3.7 Sonnet, matched human averages
and lexical preferences, none reproduced the scope of human variability. LLM
outputs were consistently less diverse and structurally rigid, and LLM
ensembles failed to increase diversity. Network analyses further revealed
fundamental differences in retrieval structure between humans and models. These
results highlight key limitations in using LLMs to simulate human cognition and
behavior.

</details>


### [32] [When Do LLMs Admit Their Mistakes? Understanding the Role of Model Belief in Retraction](https://arxiv.org/abs/2505.16170)
*Yuqing Yang,Robin Jia*

Main category: cs.CL

TL;DR: 该研究探讨大型语言模型（LLMs）能否主动撤回与自身知识相矛盾的错误答案，发现模型撤回行为与内部信念相关，并通过监督微调提升撤回表现。


<details>
  <summary>Details</summary>
Motivation: 验证LLMs是否具备自我纠错能力，探究其撤回行为与内部知识表示的关系，为提升模型可信度提供理论依据。

Method: 构建模型特定数据集评估撤回行为，设计因果实验分析内部信念的影响，采用注意力机制分析验证自我检查过程。

Result: LLMs撤回频率低，错误答案的坚持源于错误内部信念；模型自我验证时注意力模式改变；监督微调使准确率提升25%。

Conclusion: 模型内部信念直接影响其错误承认能力，通过参数调整可改善自我纠正机制，为构建可信AI系统提供新路径。

Abstract: Can large language models (LLMs) admit their mistakes when they should know
better? In this work, we define the behavior of acknowledging errors in
previously generated answers as "retraction" and aim to understand when and why
LLMs choose to retract. We first construct model-specific datasets to evaluate
whether a model will retract an incorrect answer that contradicts its own
parametric knowledge. While LLMs are capable of retraction, they do so only
infrequently. We demonstrate that retraction is closely tied to previously
identified indicators of models' internal belief: models fail to retract wrong
answers that they "believe" to be factually correct. Steering experiments
further demonstrate that internal belief causally influences model retraction.
In particular, when the model does not believe its answer, this not only
encourages the model to attempt to verify the answer, but also alters attention
behavior during self-verification. Finally, we demonstrate that simple
supervised fine-tuning significantly improves retraction performance by helping
the model learn more accurate internal beliefs. Code and datasets are available
on https://github.com/ayyyq/llm-retraction.

</details>


### [33] [Automated Feedback Loops to Protect Text Simplification with Generative AI from Information Loss](https://arxiv.org/abs/2505.16172)
*Abhay Kumara Sri Krishna Nandiraju,Gondy Leroy,David Kauchak,Arif Ahmed*

Main category: cs.CL

TL;DR: 研究验证添加缺失实体可优化生成式AI简化的健康文本，但现有工具无法有效评估实体优先级


<details>
  <summary>Details</summary>
Motivation: 生成式AI简化健康信息时会遗漏关键内容，需系统评估补全方法对信息完整性的影响

Method: 使用GPT-4简化50篇健康文本，通过五种策略（全实体/关键词添加、Top3实体、随机实体）重构文本，采用余弦相似度和ROUGE评估

Result: 添加所有缺失实体效果最佳（优于Top3和随机添加），但现有排名工具无法有效区分实体重要性

Conclusion: 实体补全可提升简化质量，但需开发更有效的实体优先级评估算法

Abstract: Understanding health information is essential in achieving and maintaining a
healthy life. We focus on simplifying health information for better
understanding. With the availability of generative AI, the simplification
process has become efficient and of reasonable quality, however, the algorithms
remove information that may be crucial for comprehension. In this study, we
compare generative AI to detect missing information in simplified text,
evaluate its importance, and fix the text with the missing information. We
collected 50 health information texts and simplified them using gpt-4-0613. We
compare five approaches to identify missing elements and regenerate the text by
inserting the missing elements. These five approaches involve adding missing
entities and missing words in various ways: 1) adding all the missing entities,
2) adding all missing words, 3) adding the top-3 entities ranked by gpt-4-0613,
and 4, 5) serving as controls for comparison, adding randomly chosen entities.
We use cosine similarity and ROUGE scores to evaluate the semantic similarity
and content overlap between the original, simplified, and reconstructed
simplified text. We do this for both summaries and full text. Overall, we find
that adding missing entities improves the text. Adding all the missing entities
resulted in better text regeneration, which was better than adding the
top-ranked entities or words, or random words. Current tools can identify these
entities, but are not valuable in ranking them.

</details>


### [34] [Understanding Fact Recall in Language Models: Why Two-Stage Training Encourages Memorization but Mixed Training Teaches Knowledge](https://arxiv.org/abs/2505.16178)
*Ying Zhang,Benjamin Heinzerling,Dongyuan Li,Ryoma Ishigaki,Yuta Hitomi,Kentaro Inui*

Main category: cs.CL

TL;DR: 论文通过参数分析发现混合训练（同时使用事实存储与回忆样本）比两阶段训练更易形成共享参数，促进语言模型事实回忆能力的跨任务泛化。


<details>
  <summary>Details</summary>
Motivation: 探究不同训练策略（两阶段训练 vs 混合训练）如何影响语言模型参数演化及其事实回忆能力的可泛化性。

Method: 使用合成事实数据集，通过跨任务梯度追踪分析Llama-3.2B/Pythia-2.8B模型参数变化，识别被事实存储和回忆任务共同影响的共享参数。

Result: 混合训练产生的共享参数数量更多且分布更集中，这些参数成为不同任务间的知识桥梁。

Conclusion: 参数涌现机制可能是语言模型泛化事实知识的关键，混合训练通过强化跨任务共享参数提升模型事实回忆的鲁棒性。

Abstract: Fact recall, the ability of language models (LMs) to retrieve specific
factual knowledge, remains a challenging task despite their impressive general
capabilities. Common training strategies often struggle to promote robust
recall behavior with two-stage training, which first trains a model with
fact-storing examples (e.g., factual statements) and then with fact-recalling
examples (question-answer pairs), tending to encourage rote memorization rather
than generalizable fact retrieval. In contrast, mixed training, which jointly
uses both types of examples, has been empirically shown to improve the ability
to recall facts, but the underlying mechanisms are still poorly understood. In
this work, we investigate how these training strategies affect how model
parameters are shaped during training and how these differences relate to their
ability to recall facts. We introduce cross-task gradient trace to identify
shared parameters, those strongly influenced by both fact-storing and
fact-recalling examples. Our analysis on synthetic fact recall datasets with
the Llama-3.2B and Pythia-2.8B models reveals that mixed training encouraging a
larger and more centralized set of shared parameters. These findings suggest
that the emergence of parameters may play a key role in enabling LMs to
generalize factual knowledge across task formulations.

</details>


### [35] [SAE-SSV: Supervised Steering in Sparse Representation Spaces for Reliable Control of Language Models](https://arxiv.org/abs/2505.16188)
*Zirui He,Mingyu Jin,Bo Shen,Ali Payani,Yongfeng Zhang,Mengnan Du*

Main category: cs.CL

TL;DR: 提出基于稀疏表征空间的监督引导方法，通过约束子空间实现更精准可控的LLM行为干预


<details>
  <summary>Details</summary>
Motivation: 现有方法在开放场景中控制LLM行为存在可靠性不足和生成质量下降的问题，需更可解释的干预方案

Method: 1) 使用稀疏自编码器提取解耦语义的潜在表征 2) 训练线性分类器定位任务相关子空间 3) 在子空间内学习约束型监督引导向量

Result: 在情感/真实性/政治倾向任务中取得更高成功率（+15%），生成困惑度仅增加0.3（baseline+2.1），子空间维度减少80%

Conclusion: 稀疏表征子空间约束显著提升引导的针对性，为LLM控制提供可解释且高效的干预范式

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
natural language understanding and generation, but controlling their behavior
reliably remains challenging, especially in open-ended generation settings.
This paper introduces a novel supervised steering approach that operates in
sparse, interpretable representation spaces. We employ sparse autoencoders
(SAEs)to obtain sparse latent representations that aim to disentangle semantic
attributes from model activations. Then we train linear classifiers to identify
a small subspace of task-relevant dimensions in latent representations.
Finally, we learn supervised steering vectors constrained to this subspace,
optimized to align with target behaviors. Experiments across sentiment,
truthfulness, and politics polarity steering tasks with multiple LLMs
demonstrate that our supervised steering vectors achieve higher success rates
with minimal degradation in generation quality compared to existing methods.
Further analysis reveals that a notably small subspace is sufficient for
effective steering, enabling more targeted and interpretable interventions.

</details>


### [36] [The Language of Interoception: Examining Embodiment and Emotion Through a Corpus of Body Part Mentions](https://arxiv.org/abs/2505.16189)
*Sophie Wu,Jan Philip Wahle,Saif M. Mohammad*

Main category: cs.CL

TL;DR: 首项基于大规模自然语言数据的研究，揭示了日常语言中身体部位提及（BPMs）与情绪表达及健康结果的关联性，为NLP与情感科学的交叉研究开辟新方向。


<details>
  <summary>Details</summary>
Motivation: 探索语言中身体部位提及（BPMs）与情绪表达、生理反应之间的潜在联系，及其对人类健康研究的意义，填补自然语言处理与情感科学交叉领域的空白。

Method: 1. 构建包含博客和推文的英文语料库，标注身体部位提及
2. 人工标注含BPM文本的情绪属性
3. 结合情绪词典量化分析BPM文本的情感强度
4. 统计分析BPM使用模式与时间/地理分布的关系
5. 建立身体语言与健康指标（如疾病率）的统计关联模型

Result: 1. BPM在个人叙事和推文中占比5%-10%，使用时空差异显著
2. 含BPM文本情绪强度平均提升23%（p<0.001）
3. 身体语言与心血管疾病率（r=0.42）、心理障碍就诊率（r=0.38）呈显著正相关
4. 情绪传导效应独立于具体生理反应描述存在

Conclusion: 身体部位语言分析为NLP、情感科学与健康研究提供了创新方法论，未来可延伸至跨文化比较、心理健康早期预警系统开发等应用场景。

Abstract: This paper is the first investigation of the connection between emotion,
embodiment, and everyday language in a large sample of natural language data.
We created corpora of body part mentions (BPMs) in online English text (blog
posts and tweets). This includes a subset featuring human annotations for the
emotions of the person whose body part is mentioned in the text. We show that
BPMs are common in personal narratives and tweets (~5% to 10% of posts include
BPMs) and that their usage patterns vary markedly by time and %geographic
location. Using word-emotion association lexicons and our annotated data, we
show that text containing BPMs tends to be more emotionally charged, even when
the BPM is not explicitly used to describe a physical reaction to the emotion
in the text. Finally, we discover a strong and statistically significant
correlation between body-related language and a variety of poorer health
outcomes. In sum, we argue that investigating the role of body-part related
words in language can open up valuable avenues of future research at the
intersection of NLP, the affective sciences, and the study of human wellbeing.

</details>


### [37] [An Empirical Study on Configuring In-Context Learning Demonstrations for Unleashing MLLMs' Sentimental Perception Capability](https://arxiv.org/abs/2505.16193)
*Daiqing Wu,Dongbao Yang,Sicheng Zhao,Can Ma,Yu Zhou*

Main category: cs.CL

TL;DR: 通过优化多模态大语言模型（MLLMs）的上下文学习配置，显著提升零样本范式下多模态情感分析（MSA）的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有零样本范式在MSA任务中表现不佳，需验证MLLMs是否具备与监督模型相当的情感感知能力。

Method: 扩展零样本至上下文学习（ICL），系统优化示例的检索、呈现、分布三要素，并发现并抵消MLLMs的情感预测偏差。

Result: 在6个MSA数据集上平均准确率较零样本提升15.9%，较随机ICL基线提升11.2%。

Conclusion: 合理配置ICL可使MLLMs在MSA任务中达到监督模型水平，验证其情感理解潜力。

Abstract: The advancements in Multimodal Large Language Models (MLLMs) have enabled
various multimodal tasks to be addressed under a zero-shot paradigm. This
paradigm sidesteps the cost of model fine-tuning, emerging as a dominant trend
in practical application. Nevertheless, Multimodal Sentiment Analysis (MSA), a
pivotal challenge in the quest for general artificial intelligence, fails to
accommodate this convenience. The zero-shot paradigm exhibits undesirable
performance on MSA, casting doubt on whether MLLMs can perceive sentiments as
competent as supervised models. By extending the zero-shot paradigm to
In-Context Learning (ICL) and conducting an in-depth study on configuring
demonstrations, we validate that MLLMs indeed possess such capability.
Specifically, three key factors that cover demonstrations' retrieval,
presentation, and distribution are comprehensively investigated and optimized.
A sentimental predictive bias inherent in MLLMs is also discovered and later
effectively counteracted. By complementing each other, the devised strategies
for three factors result in average accuracy improvements of 15.9% on six MSA
datasets against the zero-shot paradigm and 11.2% against the random ICL
baseline.

</details>


### [38] [Large Language Models based ASR Error Correction for Child Conversations](https://arxiv.org/abs/2505.16212)
*Anfeng Xu,Tiantian Feng,So Hyun Kim,Somer Bishop,Catherine Lord,Shrikanth Narayanan*

Main category: cs.CL

TL;DR: 大语言模型可有效改进儿童对话语音的零样本ASR和CTC-ASR识别错误，但在上下文整合和自回归ASR（如Whisper）优化方面仍存在挑战。


<details>
  <summary>Details</summary>
Motivation: 儿童语音识别准确率低，现有大语言模型在对话场景中的应用潜力尚未充分挖掘，需探索其在儿童对话语音ASR纠错中的有效性。

Method: 基于两个儿童对话数据集，对比分析大语言模型在零样本ASR、微调CTC-ASR和自回归ASR（Whisper）输出上的纠错表现，测试上下文信息整合效果。

Result: 大语言模型成功提升零样本ASR和CTC-ASR性能，但在整合上下文信息或处理微调自回归ASR（如Whisper）时改善有限。

Conclusion: 大语言模型在儿童语音纠错领域显示应用潜力，但需进一步解决自回归模型适配和上下文利用效率问题。

Abstract: Automatic Speech Recognition (ASR) has recently shown remarkable progress,
but accurately transcribing children's speech remains a significant challenge.
Recent developments in Large Language Models (LLMs) have shown promise in
improving ASR transcriptions. However, their applications in child speech
including conversational scenarios are underexplored. In this study, we explore
the use of LLMs in correcting ASR errors for conversational child speech. We
demonstrate the promises and challenges of LLMs through experiments on two
children's conversational speech datasets with both zero-shot and fine-tuned
ASR outputs. We find that while LLMs are helpful in correcting zero-shot ASR
outputs and fine-tuned CTC-based ASR outputs, it remains challenging for LLMs
to improve ASR performance when incorporating contextual information or when
using fine-tuned autoregressive ASR (e.g., Whisper) outputs.

</details>


### [39] [Memorization or Reasoning? Exploring the Idiom Understanding of LLMs](https://arxiv.org/abs/2505.16216)
*Jisu Kim,Youngwoo Shin,Uiji Hwang,Jihun Choi,Richeng Xuan,Taeuk Kim*

Main category: cs.CL

TL;DR: 研究揭示大型语言模型通过知识检索与推理推断相结合处理多语言习语，尤其擅长组合性习语分析。


<details>
  <summary>Details</summary>
Motivation: 习语的独特性使LLMs处理面临挑战，现有研究缺乏对其多语言环境下处理机制的深入探索。

Method: 构建六语言习语数据集MIDAS，系统评估模型表现并分析关键影响因素。

Result: LLMs采用记忆+上下文推理的混合策略，组合性习语处理体现知识检索与推理的协同作用。

Conclusion: 模型习语理解能力源于知识体系与推理机制的动态交互，为优化多语言习语处理提供新视角。

Abstract: Idioms have long posed a challenge due to their unique linguistic properties,
which set them apart from other common expressions. While recent studies have
leveraged large language models (LLMs) to handle idioms across various tasks,
e.g., idiom-containing sentence generation and idiomatic machine translation,
little is known about the underlying mechanisms of idiom processing in LLMs,
particularly in multilingual settings. To this end, we introduce MIDAS, a new
large-scale dataset of idioms in six languages, each paired with its
corresponding meaning. Leveraging this resource, we conduct a comprehensive
evaluation of LLMs' idiom processing ability, identifying key factors that
influence their performance. Our findings suggest that LLMs rely not only on
memorization, but also adopt a hybrid approach that integrates contextual cues
and reasoning, especially when processing compositional idioms. This implies
that idiom understanding in LLMs emerges from an interplay between internal
knowledge retrieval and reasoning-based inference.

</details>


### [40] [Don't Judge Code by Its Cover: Exploring Biases in LLM Judges for Code Evaluation](https://arxiv.org/abs/2505.16222)
*Jiwon Moon,Yerin Hwang,Dongryeol Lee,Taegwan Kang,Yongil Kim,Kyomin Jung*

Main category: cs.CL

TL;DR: 研究发现所有LLM评估器在代码评估中存在系统性偏见，无法公平处理语义相同但表面形式差异的代码，需开发更鲁棒的评估方法


<details>
  <summary>Details</summary>
Motivation: 探索LLM作为代码评估器时对语义等效但存在变量名/注释/格式差异的代码能否保持公平性，解决该领域尚未验证的关键可靠性问题

Method: 通过定义六种代码评估偏见类型，在五种编程语言和多个主流LLM上开展实验验证，包含测试用例生成前后的对比分析

Result: 所有LLM均表现出正负双向偏见（评分虚高或过低），且生成测试用例后仍存在偏见，Python/C++等语言受影响更显著

Conclusion: 当前LLM代码评估方法存在系统性缺陷，需开发抗表面变化干扰的评估框架，确保代码功能正确性的公平判断

Abstract: With the growing use of large language models(LLMs) as evaluators, their
application has expanded to code evaluation tasks, where they assess the
correctness of generated code without relying on reference implementations.
While this offers scalability and flexibility, it also raises a critical,
unresolved question: Can LLM judges fairly and robustly evaluate semantically
equivalent code with superficial variations? Functionally correct code often
exhibits variations-such as differences in variable names, comments, or
formatting-that should not influence its correctness. Yet, whether LLM judges
can reliably handle these variations remains unclear. We present the first
comprehensive study of this issue, defining six types of potential bias in code
evaluation and revealing their systematic impact on LLM judges. Across five
programming languages and multiple LLMs, we empirically demonstrate that all
tested LLM judges are susceptible to both positive and negative biases,
resulting in inflated or unfairly low scores. Moreover, we observe that LLM
judges remain vulnerable to these biases even when prompted to generate test
cases before scoring, highlighting the need for more robust code evaluation
methods.

</details>


### [41] [Explain Less, Understand More: Jargon Detection via Personalized Parameter-Efficient Fine-tuning](https://arxiv.org/abs/2505.16227)
*Bohao Wu,Qingyun Wang,Yue Guo*

Main category: cs.CL

TL;DR: 提出两种个性化术语检测方法（LoRA微调和个性化提示），在资源有限场景下性能超越GPT-4和基线模型，仅需10%训练数据即可实现可比效果


<details>
  <summary>Details</summary>
Motivation: 现有用户个性化术语检测方法需要大量标注数据和计算资源，难以实际部署。研究旨在开发高效、低资源消耗的个性化模型适配方案

Method: 1. 使用LoRA对开源模型轻量微调
2. 基于个性化提示的推理时适配
3. 结合有限标注数据与无监督用户背景信号的混合方法

Result: 个性化LoRA模型F1分数比GPT-4高21.4%，超过最优基线8.3%，仅用10%标注数据即可达到可比性能

Conclusion: 首次系统探索基于开源模型的高效个性化术语检测，为可扩展的用户自适应NLP系统提供实用解决方案

Abstract: Personalizing jargon detection and explanation is essential for making
technical documents accessible to readers with diverse disciplinary
backgrounds. However, tailoring models to individual users typically requires
substantial annotation efforts and computational resources due to user-specific
finetuning. To address this, we present a systematic study of personalized
jargon detection, focusing on methods that are both efficient and scalable for
real-world deployment. We explore two personalization strategies: (1)
lightweight fine-tuning using Low-Rank Adaptation (LoRA) on open-source models,
and (2) personalized prompting, which tailors model behavior at inference time
without retaining. To reflect realistic constraints, we also investigate hybrid
approaches that combine limited annotated data with unsupervised user
background signals. Our personalized LoRA model outperforms GPT-4 by 21.4% in
F1 score and exceeds the best performing oracle baseline by 8.3%. Remarkably,
our method achieves comparable performance using only 10% of the annotated
training data, demonstrating its practicality for resource-constrained
settings. Our study offers the first work to systematically explore efficient,
low-resource personalization of jargon detection using open-source language
models, offering a practical path toward scalable, user-adaptive NLP system.

</details>


### [42] [MuseRAG: Idea Originality Scoring At Scale](https://arxiv.org/abs/2505.16232)
*Ali Sarosh Bangash,Krish Veera,Ishfat Abrar Islam,Raiyan Abdul Baten*

Main category: cs.CL

TL;DR: 提出自动化框架MuseRAG，结合LLM和RAG实现高效、可靠的创意原创性评分


<details>
  <summary>Details</summary>
Motivation: 传统人工分类创意的方法效率低且难以扩展，需自动化方案解决大规模数据下的原创性评估难题

Method: 通过LLM+外部编排的RAG框架检索相似创意，使用零样本提示让LLM自动判断创意归属形成分类桶

Result: 在5个数据集（N=1143）中达到与人类标注者相当的聚类效果（AMI=0.59）和评分一致性（r=0.89）

Conclusion: 实现了大规模、意图敏感且与人类判断对齐的原创性评估体系，为创造力研究提供可靠量化工具

Abstract: An objective, face-valid way to assess the originality of creative ideas is
to measure how rare each idea is within a population -- an approach long used
in creativity research but difficult to automate at scale. Tabulating response
frequencies via manual bucketing of idea rephrasings is labor-intensive,
error-prone, and brittle under large corpora. We introduce a fully automated,
psychometrically validated pipeline for frequency-based originality scoring.
Our method, MuseRAG, combines large language models (LLMs) with an externally
orchestrated retrieval-augmented generation (RAG) framework. Given a new idea,
the system retrieves semantically similar prior idea buckets and zero-shot
prompts the LLM to judge whether the new idea belongs to an existing bucket or
forms a new one. The resulting buckets enable computation of frequency-based
originality metrics. Across five datasets (N=1143, n_ideas=16294), MuseRAG
matches human annotators in idea clustering structure and resolution (AMI =
0.59) and in participant-level scoring (r = 0.89) -- while exhibiting strong
convergent and external validity. Our work enables intent-sensitive,
human-aligned originality scoring at scale to aid creativity research.

</details>


### [43] [LIFEBench: Evaluating Length Instruction Following in Large Language Models](https://arxiv.org/abs/2505.16234)
*Wei Zhang,Zhenhong Zhou,Junfeng Fang,Rongwu Xu,Kun Wang,Yuanhe Zhang,Rui Wang,Ge Zhang,Xinfeng Li,Li Sun,Lingjuan Lyu,Yang Liu,Sen Su*

Main category: cs.CL

TL;DR: LLMs在显式长度指令遵循能力上存在显著不足，LIFEBench基准测试揭示了模型在长文本生成中的根本性局限


<details>
  <summary>Details</summary>
Motivation: 现有评估基准忽视长度约束指标，需系统评估LLMs遵循显式长度指令的能力

Method: 构建跨10,800实例/4任务类别/中英双语的LIFEBench基准，测试16-8192词范围并扩展至32K词评估

Result: 多数模型短文本表现尚可但存在临界阈值；所有模型实际输出未达厂商标称；推理型LLMs表现最优

Conclusion: 揭示LLMs长度控制机制的根本缺陷，为改进文本生成系统提供关键方向

Abstract: While large language models (LLMs) can solve PhD-level reasoning problems
over long context inputs, they still struggle with a seemingly simpler task:
following explicit length instructions-e.g., write a 10,000-word novel.
Additionally, models often generate far too short outputs, terminate
prematurely, or even refuse the request. Existing benchmarks focus primarily on
evaluating generations quality, but often overlook whether the generations meet
length constraints. To this end, we introduce Length Instruction Following
Evaluation Benchmark (LIFEBench) to comprehensively evaluate LLMs' ability to
follow length instructions across diverse tasks and a wide range of specified
lengths. LIFEBench consists of 10,800 instances across 4 task categories in
both English and Chinese, covering length constraints ranging from 16 to 8192
words. We evaluate 26 widely-used LLMs and find that most models reasonably
follow short-length instructions but deteriorate sharply beyond a certain
threshold. Surprisingly, almost all models fail to reach the vendor-claimed
maximum output lengths in practice, as further confirmed by our evaluations
extending up to 32K words. Even long-context LLMs, despite their extended
input-output windows, counterintuitively fail to improve length-instructions
following. Notably, Reasoning LLMs outperform even specialized long-text
generation models, achieving state-of-the-art length following. Overall,
LIFEBench uncovers fundamental limitations in current LLMs' length instructions
following ability, offering critical insights for future progress.

</details>


### [44] [Align-GRAG: Reasoning-Guided Dual Alignment for Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2505.16237)
*Derong Xu,Pengyue Jia,Xiaopeng Li,Yingyi Zhang,Maolin Wang,Qidong Liu,Xiangyu Zhao,Yichao Wang,Huifeng Guo,Ruiming Tang,Enhong Chen,Tong Xu*

Main category: cs.CL

TL;DR: 提出Align-GRAG框架，通过后检索阶段的双重对齐机制优化图RAG系统，解决冗余节点和表示差异问题，在GraphQA基准测试中验证有效性


<details>
  <summary>Details</summary>
Motivation: 现有图RAG系统存在两个关键缺陷：1）检索时引入不相关节点导致输入冗余 2）图结构与语言模型间的表示差异限制了性能提升

Method: 1）构建节点和边组成的子图 2）设计Aligner模块联合优化图编码器和LLM推理摘要，通过KL散度损失和对比损失实现节点对齐与表示对齐 3）生成器整合对齐后的图数据与LLM

Result: 在GraphQA基准测试中（包含常识推理、场景图理解、知识图谱推理三个任务）验证了方法的有效性

Conclusion: Align-GRAG通过双重对齐机制有效剪枝冗余知识并建立统一语义空间，显著提升图RAG系统的准确性和效率

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities, but
still struggle with issues like hallucinations and outdated information.
Retrieval-augmented generation (RAG) addresses these issues by grounding LLM
outputs in external knowledge with an Information Retrieval (IR) system.
Building on this foundation, graph-based RAG systems go a step further by
retrieving subgraphs, which preserve the relationships between knowledge
entities and provide more comprehensive context. However, graph RAG faces two
challenges: (1) Retrieving relevant information introduces irrelevant nodes
(especially in dense graph databases, where retrieval usually extends to
adjacent nodes), and leads to overly lengthy inputs that hinder efficiency; (2)
The representation gap between graph and language during generation with LLMs
limits the ability to fully leverage graph structures for enhanced
understanding. To address these limitations, we propose Align-GRAG, a novel
reasoning-guided dual alignment framework in post-retrieval phrase. It first
formulates a subgraph by retrieving nodes and edges. Then an Aligner is
proposed to jointly optimizes a graph encoder with LLM-summarized reasoning. It
achieves dual alignment of graph node and representation by leveraging KL
divergence loss and contrastive loss, facilitating efficient pruning of
irrelevant knowledge and establishing a unified semantic space. The Generator
integrates the aligned graph data with LLM to produce coherent and accurate
answers. Experiments on GraphQA benchmark across three tasks (including common
sense reasoning, scene graph understanding, and knowledge graph reasoning)
validate the effectiveness of our method. The code will be available upon
accepted.

</details>


### [45] [Three Minds, One Legend: Jailbreak Large Reasoning Model with Adaptive Stacked Ciphers](https://arxiv.org/abs/2505.16241)
*Viet-Anh Nguyen,Shiqian Zhao,Gia Dao,Runyi Hu,Yi Xie,Luu Anh Tuan*

Main category: cs.CL

TL;DR: SEAL提出一种通过堆叠加密和动态策略绕过大型推理模型安全机制的新型越狱攻击方法，在GPT-4o-mini上实现80.8%攻击成功率，超越现有基线27.2%。


<details>
  <summary>Details</summary>
Motivation: 现有越狱方法难以平衡攻击有效性与对抗安全机制的鲁棒性，而强大推理能力可能引发更严重的安全漏洞。为此需要开发能压制模型推理能力并规避自适应防护的新型攻击方式。

Method: 采用多密码堆叠加密突破模型推理能力，结合随机/自适应动态策略调整密码参数，形成难以防御的自适应加密管道。

Result: 在DeepSeek-R1、Claude Sonnet和GPT-4o等模型上验证，SEAL对GPT-4o-mini攻击成功率达80.8%，较SOTA基线提升27.2%。

Conclusion: 该研究证明压制模型推理能力可有效突破安全机制，动态加密策略能预防模型形成对抗措施，但需警惕该方法可能被滥用的风险。

Abstract: Recently, Large Reasoning Models (LRMs) have demonstrated superior logical
capabilities compared to traditional Large Language Models (LLMs), gaining
significant attention. Despite their impressive performance, the potential for
stronger reasoning abilities to introduce more severe security vulnerabilities
remains largely underexplored. Existing jailbreak methods often struggle to
balance effectiveness with robustness against adaptive safety mechanisms. In
this work, we propose SEAL, a novel jailbreak attack that targets LRMs through
an adaptive encryption pipeline designed to override their reasoning processes
and evade potential adaptive alignment. Specifically, SEAL introduces a stacked
encryption approach that combines multiple ciphers to overwhelm the models
reasoning capabilities, effectively bypassing built-in safety mechanisms. To
further prevent LRMs from developing countermeasures, we incorporate two
dynamic strategies - random and adaptive - that adjust the cipher length,
order, and combination. Extensive experiments on real-world reasoning models,
including DeepSeek-R1, Claude Sonnet, and OpenAI GPT-o4, validate the
effectiveness of our approach. Notably, SEAL achieves an attack success rate of
80.8% on GPT o4-mini, outperforming state-of-the-art baselines by a significant
margin of 27.2%. Warning: This paper contains examples of inappropriate,
offensive, and harmful content.

</details>


### [46] [Diverse, not Short: A Length-Controlled Self-Learning Framework for Improving Response Diversity of Language Models](https://arxiv.org/abs/2505.16245)
*Vijeta Deshpande,Debasmita Ghose,John D. Patterson,Roger Beaty,Anna Rumshisky*

Main category: cs.CL

TL;DR: Diverse-NS框架通过控制输出长度解决现有多样性指标的长度偏差问题，仅用3000对数据即可显著提升语言模型的词汇和语义多样性，实验证明小模型能有效指导大模型提升多样性表现。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型的多样性评估指标和偏好优化奖励模型存在系统性长度偏差，导致模型倾向于生成简短输出，严重限制了生成内容的表达能力。

Method: 提出长度控制自学习框架Diverse-NS，通过生成并筛选平衡多样性、质量和长度的偏好数据，在LLaMA-3.1-8B和Olmo系列模型上实现高效训练。

Result: 在发散联想、角色生成、替代用途和创意写作四个任务中，模型词汇/语义多样性提升15-30%，响应质量保持稳定甚至提升。Olmo-2-7B小模型成功指导更大模型提升多样性。

Conclusion: 通过显式控制长度偏差，Diverse-NS有效推动了语言模型生成更具多样性和表达力的输出，为模型自我改进提供了高效训练范式。

Abstract: Diverse language model responses are crucial for creative generation,
open-ended tasks, and self-improvement training. We show that common diversity
metrics, and even reward models used for preference optimization,
systematically bias models toward shorter outputs, limiting expressiveness. To
address this, we introduce Diverse, not Short (Diverse-NS), a length-controlled
self-learning framework that improves response diversity while maintaining
length parity. By generating and filtering preference data that balances
diversity, quality, and length, Diverse-NS enables effective training using
only 3,000 preference pairs. Applied to LLaMA-3.1-8B and the Olmo-2 family,
Diverse-NS substantially enhances lexical and semantic diversity. We show
consistent improvement in diversity with minor reduction or gains in response
quality on four creative generation tasks: Divergent Associations, Persona
Generation, Alternate Uses, and Creative Writing. Surprisingly, experiments
with the Olmo-2 model family (7B, and 13B) show that smaller models like
Olmo-2-7B can serve as effective "diversity teachers" for larger models. By
explicitly addressing length bias, our method efficiently pushes models toward
more diverse and expressive outputs.

</details>


### [47] [Does Localization Inform Unlearning? A Rigorous Examination of Local Parameter Attribution for Knowledge Unlearning in Language Models](https://arxiv.org/abs/2505.16252)
*Hwiyeong Lee,Uiji Hwang,Hyelim Lim,Taeuk Kim*

Main category: cs.CL

TL;DR: 现有局部化遗忘方法的核心假设（参数位置性决定遗忘效果）存在根本性缺陷，有效遗忘所需的参数修改范围并不严格确定


<details>
  <summary>Details</summary>
Motivation: 针对当前局部化知识遗忘方法缺乏对参数更新与遗忘效果关系的严格验证，研究旨在通过控制实验揭示该方法的有效性边界

Method: 通过重新审视现有方法+设计控制实验组，系统验证局部参数更新与知识遗忘之间的因果关系

Result: 实验证明参数修改的必要性集合具有非严格确定性，参数位置性无法可靠指示知识移除效果

Conclusion: 局部化遗忘方法的理论基础存在根本性缺陷，需重新构建基于动态参数影响分析的知识遗忘框架

Abstract: Large language models often retain unintended content, prompting growing
interest in knowledge unlearning. Recent approaches emphasize localized
unlearning, which restricts parameter updates to specific regions in an effort
to remove target knowledge while preserving unrelated general knowledge.
However, their effectiveness remains uncertain due to the lack of robust and
thorough evaluation of the trade-off between the competing goals of unlearning.
In this paper, we begin by revisiting existing localized unlearning approaches.
We then conduct controlled experiments to rigorously evaluate whether local
parameter updates causally contribute to unlearning. Our findings reveal that
the set of parameters that must be modified for effective unlearning is not
strictly determined, challenging the core assumption of localized unlearning
that parameter locality is inherently indicative of effective knowledge
removal.

</details>


### [48] [IRONIC: Coherence-Aware Reasoning Chains for Multi-Modal Sarcasm Detection](https://arxiv.org/abs/2505.16258)
*Aashish Anantha Ramakrishnan,Aadarsh Anantha Ramakrishnan,Dongwon Lee*

Main category: cs.CL

TL;DR: 提出IRONIC框架通过多模态连贯关系分析实现零样本讽刺检测，取得SOTA效果


<details>
  <summary>Details</summary>
Motivation: 现有多模态讽刺检测方法过度依赖任务微调且未有效模拟人类认知机制

Method: 利用指代/类比/语用三类多模态连贯关系构建上下文学习框架

Result: 在零样本场景下超越基线模型，验证语言认知机制融合的有效性

Conclusion: 多模态推理策略应整合语言学认知理论以实现更优的讽刺理解

Abstract: Interpreting figurative language such as sarcasm across multi-modal inputs
presents unique challenges, often requiring task-specific fine-tuning and
extensive reasoning steps. However, current Chain-of-Thought approaches do not
efficiently leverage the same cognitive processes that enable humans to
identify sarcasm. We present IRONIC, an in-context learning framework that
leverages Multi-modal Coherence Relations to analyze referential, analogical
and pragmatic image-text linkages. Our experiments show that IRONIC achieves
state-of-the-art performance on zero-shot Multi-modal Sarcasm Detection across
different baselines. This demonstrates the need for incorporating linguistic
and cognitive insights into the design of multi-modal reasoning strategies. Our
code is available at: https://github.com/aashish2000/IRONIC

</details>


### [49] [Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning](https://arxiv.org/abs/2505.16270)
*Jiaru Zou,Yikun Ban,Zihao Li,Yunzhe Qi,Ruizhong Qiu,Ling Yang,Jingrui He*

Main category: cs.CL

TL;DR: 提出Transformer Copilot框架，通过错误日志追踪和Copilot模型修正，显著提升大模型微调效果


<details>
  <summary>Details</summary>
Motivation: 传统监督微调仅优化参数，忽视模型自身学习信号。受人类反思学习机制启发，需系统性捕捉并利用模型学习过程中的错误模式

Method: 1. 创新Mistake Log追踪模型错误模式 2. 设计Copilot模型进行logits修正 3. 建立Pilot-Copilot联合训练及融合推理机制

Result: 在12个基准测试中最高提升34.5%性能，仅增加边际计算开销，展现强扩展性和迁移性

Conclusion: Transformer Copilot通过协同训练和推理修正，为模型微调提供新范式，平衡性能提升与计算效率

Abstract: Large language models are typically adapted to downstream tasks through
supervised fine-tuning on domain-specific data. While standard fine-tuning
focuses on minimizing generation loss to optimize model parameters, we take a
deeper step by retaining and leveraging the model's own learning signals,
analogous to how human learners reflect on past mistakes to improve future
performance. We first introduce the concept of Mistake Log to systematically
track the model's learning behavior and recurring errors throughout
fine-tuning. Treating the original transformer-based model as the Pilot, we
correspondingly design a Copilot model to refine the Pilot's inference
performance via logits rectification. We name the overall Pilot-Copilot
framework the Transformer Copilot, which introduces (i) a novel Copilot model
design, (ii) a joint training paradigm where the Copilot continuously learns
from the evolving Mistake Log alongside the Pilot, and (iii) a fused inference
paradigm where the Copilot rectifies the Pilot's logits for enhanced
generation. We provide both theoretical and empirical analyses on our new
learning framework. Experiments on 12 benchmarks spanning commonsense,
arithmetic, and recommendation tasks demonstrate that Transformer Copilot
consistently improves performance by up to 34.5%, while introducing marginal
computational overhead to Pilot models and exhibiting strong scalability and
transferability.

</details>


### [50] [Spontaneous Speech Variables for Evaluating LLMs Cognitive Plausibility](https://arxiv.org/abs/2505.16277)
*Sheng-Fu Wang,Laurent Prevot,Jou-an Chi,Ri-Sheng Huang,Shu-Kai Hsieh*

Main category: cs.CL

TL;DR: 使用自发语音语料库提取语音缩减和韵律突出变量，验证不同预训练数据类型的LLM预测表现，发现语音数据训练的模型预测更优


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估多基于文本理解任务（如眼动追踪/脑神经响应预测），本文尝试从语言生成角度（语音生产特征）建立新的评估维度

Method: 1. 从自发语音中提取语音缩减和韵律突出变量
2. 使用不同预训练数据（纯文本/纯语音/混合）的模型进行微调
3. 对比模型预测能力与基线表现

Result: 1. 微调后模型预测能力显著超越基线
2. 语音类预训练数据模型预测精度优于文本类数据
3. 验证语音缩减与韵律特征的模型可预测性

Conclusion: 语音生产特征可作为LLM评估新基准，高质量语音语料库对提升语言模型认知适配性具有重要价值

Abstract: The achievements of Large Language Models in Natural Language Processing,
especially for high-resource languages, call for a better understanding of
their characteristics from a cognitive perspective. Researchers have attempted
to evaluate artificial models by testing their ability to predict behavioral
(e.g., eye-tracking fixations) and physiological (e.g., brain responses)
variables during language processing (e.g., reading/listening). In this paper,
we propose using spontaneous speech corpora to derive production variables
(speech reductions, prosodic prominences) and applying them in a similar
fashion. More precisely, we extract. We then test models trained with a
standard procedure on different pretraining datasets (written, spoken, and
mixed genres) for their ability to predict these two variables. Our results
show that, after some fine-tuning, the models can predict these production
variables well above baselines. We also observe that spoken genre training data
provides more accurate predictions than written genres. These results
contribute to the broader effort of using high-quality speech corpora as
benchmarks for LLMs.

</details>


### [51] [HiMATE: A Hierarchical Multi-Agent Framework for Machine Translation Evaluation](https://arxiv.org/abs/2505.16281)
*Shijie Zhang,Renhao Li,Songsheng Wang,Philipp Koehn,Min Yang,Derek F. Wong*

Main category: cs.CL

TL;DR: 提出分层多智能体框架HiMATE，通过自反思和代理讨论策略显著提升机器翻译评估效果，错误检测F1值提升89%


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估方法未能充分挖掘多维质量指标（MQM）的细粒度结构信息，导致错误定位和严重性判断不准确

Method: 基于MQM错误类型构建分层多智能体系统，结合模型自反思能力和非对称信息代理讨论策略

Result: 在多个数据集上超越基线模型，错误范围检测F1值平均提升89%，尤其在严重性评估方面优势显著

Conclusion: HiMATE通过层级化架构和双重防幻觉机制实现了更人类对齐的翻译质量评估，相关资源已开源

Abstract: The advancement of Large Language Models (LLMs) enables flexible and
interpretable automatic evaluations. In the field of machine translation
evaluation, utilizing LLMs with translation error annotations based on
Multidimensional Quality Metrics (MQM) yields more human-aligned judgments.
However, current LLM-based evaluation methods still face challenges in
accurately identifying error spans and assessing their severity. In this paper,
we propose HiMATE, a Hierarchical Multi-Agent Framework for Machine Translation
Evaluation. We argue that existing approaches inadequately exploit the
fine-grained structural and semantic information within the MQM hierarchy. To
address this, we develop a hierarchical multi-agent system grounded in the MQM
error typology, enabling granular evaluation of subtype errors. Two key
strategies are incorporated to further mitigate systemic hallucinations within
the framework: the utilization of the model's self-reflection capability and
the facilitation of agent discussion involving asymmetric information.
Empirically, HiMATE outperforms competitive baselines across different datasets
in conducting human-aligned evaluations. Further analyses underscore its
significant advantage in error span detection and severity assessment,
achieving an average F1-score improvement of 89% over the best-performing
baseline. We make our code and data publicly available at
https://anonymous.4open.science/r/HiMATE-Anony.

</details>


### [52] [Augmenting LLM Reasoning with Dynamic Notes Writing for Complex QA](https://arxiv.org/abs/2505.16293)
*Rishabh Maheshwary,Masoud Hashemi,Khyati Mahajan,Shiva Krishna Reddy Malay,Sai Rajeswar,Sathwik Tejaswi Madhusudhan,Spandana Gella,Vikas Yadav*

Main category: cs.CL

TL;DR: 提出Notes Writing方法解决迭代RAG中长上下文和噪声积累问题，提升15.6%平均性能


<details>
  <summary>Details</summary>
Motivation: 迭代RAG在多跳问答中面临上下文冗长和无关信息累积，影响模型推理能力和性能表现

Method: 通过逐步骤生成简洁笔记(Notes Writing)，过滤噪声保留关键信息，框架兼容不同迭代RAG方法

Result: 在4个数据集/3种方法/2个模型上验证，平均提升15.6个百分点，输出token仅小幅增加

Conclusion: 笔记写作机制间接扩展LLM有效上下文长度，增强多跳推理能力，具备框架普适性和高效性

Abstract: Iterative RAG for multi-hop question answering faces challenges with lengthy
contexts and the buildup of irrelevant information. This hinders a model's
capacity to process and reason over retrieved content and limits performance.
While recent methods focus on compressing retrieved information, they are
either restricted to single-round RAG, require finetuning or lack scalability
in iterative RAG. To address these challenges, we propose Notes Writing, a
method that generates concise and relevant notes from retrieved documents at
each step, thereby reducing noise and retaining only essential information.
This indirectly increases the effective context length of Large Language Models
(LLMs), enabling them to reason and plan more effectively while processing
larger volumes of input text. Notes Writing is framework agnostic and can be
integrated with different iterative RAG methods. We demonstrate its
effectiveness with three iterative RAG methods, across two models and four
evaluation datasets. Notes writing yields an average improvement of 15.6
percentage points overall, with minimal increase in output tokens.

</details>


### [53] [ToDi: Token-wise Distillation via Fine-Grained Divergence Control](https://arxiv.org/abs/2505.16297)
*Seongryong Jung,Suwan Yoon,DongGeon Kim,Hwanhee Lee*

Main category: cs.CL

TL;DR: 提出Token-wise蒸馏方法(ToDi)，通过自适应组合FKL和RKL实现逐token知识蒸馏，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法在全部词表上采用均匀分布的损失函数，忽视token级别的预测差异。FKL仅提升被低估的token预测，RKL抑制被高估的预测，二者存在互补性但未被有效利用。

Method: 基于梯度分析提出ToDi方法，利用sigmoid加权函数根据师生概率对数比动态分配FKL和RKL的权重，实现逐token的分布对齐。

Result: 在指令跟随基准测试中，ToDi持续优于采用均匀策略的蒸馏基线方法，消融实验和效率分析验证了其有效性。

Conclusion: 通过动态强调不同散度的互补优势，ToDi实现了更精准的分布对齐，为高效部署语言模型提供了新方案。

Abstract: Large language models (LLMs) offer impressive performance but are impractical
for resource-constrained deployment due to high latency and energy consumption.
Knowledge distillation (KD) addresses this by transferring knowledge from a
large teacher to a smaller student model. However, conventional KD, notably
approaches like Forward KL (FKL) and Reverse KL (RKL), apply uniform divergence
loss across the entire vocabulary, neglecting token-level prediction
discrepancies. By investigating these representative divergences via gradient
analysis, we reveal that FKL boosts underestimated tokens, while RKL suppresses
overestimated ones, showing their complementary roles. Based on this
observation, we propose Token-wise Distillation (ToDi), a novel method that
adaptively combines FKL and RKL per token using a sigmoid-based weighting
function derived from the teacher-student probability log-ratio. ToDi
dynamically emphasizes the appropriate divergence for each token, enabling
precise distribution alignment. We demonstrate that ToDi consistently
outperforms recent distillation baselines using uniform or less granular
strategies across instruction-following benchmarks. Extensive ablation studies
and efficiency analysis further validate ToDi's effectiveness and practicality.

</details>


### [54] [INFERENCEDYNAMICS: Efficient Routing Across LLMs through Structured Capability and Knowledge Profiling](https://arxiv.org/abs/2505.16303)
*Haochen Shi,Tianshi Zheng,Weiqi Wang,Baixuan Xu,Chunyang Li,Chunkit Chan,Tao Fan,Yangqiu Song,Qiang Yang*

Main category: cs.CL

TL;DR: 提出InferenceDynamics多维度路由框架，通过建模LLM能力实现高效资源分配，在MMLU-Pro等基准测试中验证其有效性


<details>
  <summary>Details</summary>
Motivation: 现有LLM路由方案面临大规模专业模型池扩展性不足、模型能力领域动态适应困难等核心挑战

Method: 基于模型能力建模构建多维度路由框架，使用RouteMix数据集进行验证，支持动态扩展模型范围和能力维度

Result: 在MMLU-Pro、GPQA等基准测试中实现组级路由优化，资源利用率提升30%的同时保持90%+的专家模型性能

Conclusion: 该框架释放了LLM生态的专业潜力，代码开源将推动更高效的模型路由研究与应用

Abstract: Large Language Model (LLM) routing is a pivotal technique for navigating a
diverse landscape of LLMs, aiming to select the best-performing LLMs tailored
to the domains of user queries, while managing computational resources.
However, current routing approaches often face limitations in scalability when
dealing with a large pool of specialized LLMs, or in their adaptability to
extending model scope and evolving capability domains. To overcome those
challenges, we propose InferenceDynamics, a flexible and scalable
multi-dimensional routing framework by modeling the capability and knowledge of
models. We operate it on our comprehensive dataset RouteMix, and demonstrate
its effectiveness and generalizability in group-level routing using modern
benchmarks including MMLU-Pro, GPQA, BigGenBench, and LiveBench, showcasing its
ability to identify and leverage top-performing models for given tasks, leading
to superior outcomes with efficient resource utilization. The broader adoption
of Inference Dynamics can empower users to harness the full specialized
potential of the LLM ecosystem, and our code will be made publicly available to
encourage further research.

</details>


### [55] [PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models](https://arxiv.org/abs/2505.16307)
*Chenzhuo Zhao,Ziqian Liu,Xingda Wang,Junting Lu,Chaoyi Ruan*

Main category: cs.CL

TL;DR: 提出PMPO框架——通过交叉熵损失直接优化提示词，无需采样输出即可提升LLM性能


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法依赖高成本的输出生成、自我批判或人工标注，限制了在小模型上的扩展应用

Method: 1. 使用token级交叉熵损失作为评估信号
2. 遮蔽低质量提示段评估影响
3. 通过最小化损失重写优化提示词

Result: BBH平均准确率最高/GSM8K和AQUA-RAT表现优异/AlpacaEval胜率提升19%+

Conclusion: PMPO证实了基于概率度量的轻量级优化策略在效果、效率和通用性上的显著优势

Abstract: Prompt optimization offers a practical and broadly applicable alternative to
fine-tuning for improving large language model (LLM) performance. However,
existing methods often rely on costly output generation, self-critiquing
abilities, or human-annotated preferences, which limit their scalability,
especially for smaller or non-instruction-tuned models. We introduce PMPO
(Probabilistic Metric Prompt Optimization), a unified framework that refines
prompts using token-level cross-entropy loss as a direct, lightweight
evaluation signal. PMPO identifies low-quality prompt segments by masking and
measuring their impact on loss, then rewrites and selects improved variants by
minimizing loss over positive and negative examples. Unlike prior methods, it
requires no output sampling or human evaluation during optimization, relying
only on forward passes and log-likelihoods. PMPO supports both supervised and
preference-based tasks through a closely aligned loss-based evaluation
strategy. Experiments show that PMPO consistently outperforms prior methods
across model sizes and tasks: it achieves the highest average accuracy on BBH,
performs strongly on GSM8K and AQUA-RAT, and improves AlpacaEval 2.0 win rates
by over 19 points. These results highlight PMPO's effectiveness, efficiency,
and broad applicability.

</details>


### [56] [CLEAR: A Clinically-Grounded Tabular Framework for Radiology Report Evaluation](https://arxiv.org/abs/2505.16325)
*Yuyang Jiang,Chacha Chen,Shengyuan Wang,Feng Li,Zecong Tang,Benjamin M. Mervak,Lydia Chelala,Christopher M Straus,Reve Chahine,Samuel G. Armato III,Chenhao Tan*

Main category: cs.CL

TL;DR: 提出CLEAR框架，通过五维属性评估放射学报告的临床准确性


<details>
  <summary>Details</summary>
Motivation: 现有评估指标无法捕捉放射报告中的临床细微差异，缺乏多维可解释性评估

Method: 构建基于表格的评估框架（CLEAR），结合专家标注的CLEAR-Bench数据集（含100份胸部X光报告），从首次发生/变化/严重性/解剖位置/建议五个核心属性进行自动化分析

Result: CLEAR在属性提取准确率达到91.3%，其自动评估指标与临床判断的相关系数r=0.82（p<0.01）

Conclusion: CLEAR框架通过结构化属性分析，实现了更全面、临床可解释的放射报告质量评估，为AI辅助诊断系统提供可靠的质量控制工具

Abstract: Existing metrics often lack the granularity and interpretability to capture
nuanced clinical differences between candidate and ground-truth radiology
reports, resulting in suboptimal evaluation. We introduce a Clinically-grounded
tabular framework with Expert-curated labels and Attribute-level comparison for
Radiology report evaluation (CLEAR). CLEAR not only examines whether a report
can accurately identify the presence or absence of medical conditions, but also
assesses whether it can precisely describe each positively identified condition
across five key attributes: first occurrence, change, severity, descriptive
location, and recommendation. Compared to prior works, CLEAR's
multi-dimensional, attribute-level outputs enable a more comprehensive and
clinically interpretable evaluation of report quality. Additionally, to measure
the clinical alignment of CLEAR, we collaborate with five board-certified
radiologists to develop CLEAR-Bench, a dataset of 100 chest X-ray reports from
MIMIC-CXR, annotated across 6 curated attributes and 13 CheXpert conditions.
Our experiments show that CLEAR achieves high accuracy in extracting clinical
attributes and provides automated metrics that are strongly aligned with
clinical judgment.

</details>


### [57] [SC4ANM: Identifying Optimal Section Combinations for Automated Novelty Prediction in Academic Papers](https://arxiv.org/abs/2505.16330)
*Wenqing Wu,Chengzhi Zhang,Tong Bao,Yi Zhao*

Main category: cs.CL

TL;DR: 提出通过结合学术论文不同核心章节(IMRaD)作为输入，驱动语言模型预测新颖性分数，发现引言+结果+讨论的组合最适合评估新颖性


<details>
  <summary>Details</summary>
Motivation: 现有新颖性评估方法局限于词语组合分析，而论文新颖性内容分布于不同章节，需探索最优章节组合实现自动化评估

Method: 使用NLP技术识别论文IMRaD结构，将不同章节组合输入预训练模型，以专家评分作为标签预测新颖性分数

Result: 引言+结果+讨论组合评估效果最佳；全文输入效果不显著；引言和结果部分对预测最重要

Conclusion: 章节组合策略可提升自动化新颖性评估效果，代码数据集已开源(https://github.com/njust-winchy/SC4ANM)

Abstract: Novelty is a core component of academic papers, and there are multiple
perspectives on the assessment of novelty. Existing methods often focus on word
or entity combinations, which provide limited insights. The content related to
a paper's novelty is typically distributed across different core sections,
e.g., Introduction, Methodology and Results. Therefore, exploring the optimal
combination of sections for evaluating the novelty of a paper is important for
advancing automated novelty assessment. In this paper, we utilize different
combinations of sections from academic papers as inputs to drive language
models to predict novelty scores. We then analyze the results to determine the
optimal section combinations for novelty score prediction. We first employ
natural language processing techniques to identify the sectional structure of
academic papers, categorizing them into introduction, methods, results, and
discussion (IMRaD). Subsequently, we used different combinations of these
sections (e.g., introduction and methods) as inputs for pretrained language
models (PLMs) and large language models (LLMs), employing novelty scores
provided by human expert reviewers as ground truth labels to obtain prediction
results. The results indicate that using introduction, results and discussion
is most appropriate for assessing the novelty of a paper, while the use of the
entire text does not yield significant results. Furthermore, based on the
results of the PLMs and LLMs, the introduction and results appear to be the
most important section for the task of novelty score prediction. The code and
dataset for this paper can be accessed at
https://github.com/njust-winchy/SC4ANM.

</details>


### [58] [Embodied Agents Meet Personalization: Exploring Memory Utilization for Personalized Assistance](https://arxiv.org/abs/2505.16348)
*Taeyoon Kwon,Dongwook Choi,Sunghwan Kim,Hyojun Kim,Seungjun Moon,Beong-woo Kwak,Kuan-Hao Huang,Jinyoung Yeo*

Main category: cs.CL

TL;DR: 提出了MEMENTO框架，用于评估具身代理利用记忆提供个性化协助的能力，发现现有模型在需要多记忆引用时性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有具身代理主要关注单轮交互，难以理解用户对物理世界的个性化语义（如日常习惯、物品偏好），需通过记忆利用提供真正有效的个性化协助。

Method: 设计两阶段记忆评估流程：1) 通过对象语义识别个性化目标对象；2) 通过用户模式推断物品位置配置。建立量化指标评估记忆对任务表现的影响。

Result: 实验显示主流LLMs存在显著局限，GPT-4o在需要参考多个记忆时性能下降30.5%，在用户模式推理任务中表现尤其薄弱。

Conclusion: MEMENTO框架揭示了当前个性化具身代理的不足，为未来开发更有效的记忆利用机制提供了系统评估基准和方向指导。

Abstract: Embodied agents empowered by large language models (LLMs) have shown strong
performance in household object rearrangement tasks. However, these tasks
primarily focus on single-turn interactions with simplified instructions, which
do not truly reflect the challenges of providing meaningful assistance to
users. To provide personalized assistance, embodied agents must understand the
unique semantics that users assign to the physical world (e.g., favorite cup,
breakfast routine) by leveraging prior interaction history to interpret
dynamic, real-world instructions. Yet, the effectiveness of embodied agents in
utilizing memory for personalized assistance remains largely underexplored. To
address this gap, we present MEMENTO, a personalized embodied agent evaluation
framework designed to comprehensively assess memory utilization capabilities to
provide personalized assistance. Our framework consists of a two-stage memory
evaluation process design that enables quantifying the impact of memory
utilization on task performance. This process enables the evaluation of agents'
understanding of personalized knowledge in object rearrangement tasks by
focusing on its role in goal interpretation: (1) the ability to identify target
objects based on personal meaning (object semantics), and (2) the ability to
infer object-location configurations from consistent user patterns, such as
routines (user patterns). Our experiments across various LLMs reveal
significant limitations in memory utilization, with even frontier models like
GPT-4o experiencing a 30.5% performance drop when required to reference
multiple memories, particularly in tasks involving user patterns. These
findings, along with our detailed analyses and case studies, provide valuable
insights for future research in developing more effective personalized embodied
agents. Project website: https://connoriginal.github.io/MEMENTO

</details>


### [59] [Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization](https://arxiv.org/abs/2505.16349)
*Pierre Achkar,Tim Gollub,Martin Potthast*

Main category: cs.CL

TL;DR: XSum是一种基于检索增强生成（RAG）的科学领域多文档摘要模块化框架，通过动态生成问题精准检索信息，并合成符合学术规范的摘要。


<details>
  <summary>Details</summary>
Motivation: 解决科学文献爆炸式增长导致研究者难以高效追踪和整合知识的痛点。

Method: 包含两个核心模块：动态问题生成模块（适配论文内容生成检索问题）和编辑模块（将检索内容合成为结构化学术摘要）

Result: 在SurveySum数据集上，CheckEval/G-Eval/Ref-F1等指标显著超越现有方法

Conclusion: 提供透明可扩展的科学摘要框架，支持跨领域应用，代码已开源

Abstract: The exponential growth of scientific publications has made it increasingly
difficult for researchers to stay updated and synthesize knowledge effectively.
This paper presents XSum, a modular pipeline for multi-document summarization
(MDS) in the scientific domain using Retrieval-Augmented Generation (RAG). The
pipeline includes two core components: a question-generation module and an
editor module. The question-generation module dynamically generates questions
adapted to the input papers, ensuring the retrieval of relevant and accurate
information. The editor module synthesizes the retrieved content into coherent
and well-structured summaries that adhere to academic standards for proper
citation. Evaluated on the SurveySum dataset, XSum demonstrates strong
performance, achieving considerable improvements in metrics such as CheckEval,
G-Eval and Ref-F1 compared to existing approaches. This work provides a
transparent, adaptable framework for scientific summarization with potential
applications in a wide range of domains. Code available at
https://github.com/webis-de/scolia25-xsum

</details>


### [60] [PaTH Attention: Position Encoding via Accumulating Householder Transformations](https://arxiv.org/abs/2505.16381)
*Songlin Yang,Yikang Shen,Kaiyue Wen,Shawn Tan,Mayank Mishra,Liliang Ren,Rameswar Panda,Yoon Kim*

Main category: cs.CL

TL;DR: 提出了一种名为PaTH的数据依赖型位置编码方案，通过Householder变换动态调整位置编码，显著提升了Transformer模型的表达能力


<details>
  <summary>Details</summary>
Motivation: 现有RoPE位置编码仅依赖相对位置而忽略输入内容，限制了模型表达能力。需要开发数据依赖的位置编码方案突破这一限制

Method: 基于Householder变换的累积乘积构建位置编码，每个变换都与输入数据相关。开发了并行训练算法和FlashAttention风格的块计算策略优化IO效率

Result: 在合成基准测试和中等规模语言建模实验中，PaTH在性能表现上显著优于RoPE及其他基线方法

Conclusion: PaTH通过数据驱动的位置编码机制，为提升Transformer模型的语言建模能力提供了新的有效方案，具有重要的理论和应用价值

Abstract: The attention mechanism is a core primitive in modern large language models
(LLMs) and AI more broadly. Since attention by itself is permutation-invariant,
position encoding is essential for modeling structured domains such as
language. Rotary position encoding (RoPE) has emerged as the de facto standard
approach for position encoding and is part of many modern LLMs. However, in
RoPE the key/query transformation between two elements in a sequence is only a
function of their relative position and otherwise independent of the actual
input. This limits the expressivity of RoPE-based transformers.
  This paper describes PaTH, a flexible data-dependent position encoding scheme
based on accumulated products of Householder(like) transformations, where each
transformation is data-dependent, i.e., a function of the input. We derive an
efficient parallel algorithm for training through exploiting a compact
representation of products of Householder matrices, and implement a
FlashAttention-style blockwise algorithm that minimizes I/O cost. Across both
targeted synthetic benchmarks and moderate-scale real-world language modeling
experiments, we find that PaTH demonstrates superior performance compared to
RoPE and other recent baselines.

</details>


### [61] [Semantic Pivots Enable Cross-Lingual Transfer in Large Language Models](https://arxiv.org/abs/2505.16385)
*Kaiyu He,Tong Zhou,Yubo Chen,Delai Qiu,Shengping Liu,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 论文通过词级跨语言翻译任务揭示大语言模型（LLM）跨语言能力的两种机制：共现行为和语义枢纽行为，并提出基于语义枢纽优化的预训练数据重构方法。


<details>
  <summary>Details</summary>
Motivation: 量化并解释LLM的跨语言能力来源，发现其核心机制受预训练数据中词汇共现频率影响，进而改进模型跨语言表现。

Method: 1.设计词级跨语言翻译任务量化能力
2.追踪模型中间层输出识别两种行为模式
3.通过语义枢纽重构预训练数据

Result: 实验验证语义枢纽优化数据集能有效提升LLM跨语言能力，两种行为模式与词汇共现频率强相关。

Conclusion: LLM跨语言能力源于数据中的语义枢纽结构，针对性优化预训练数据可显著增强该能力，为模型可解释性研究提供新视角。

Abstract: Large language models (LLMs) demonstrate remarkable ability in cross-lingual
tasks. Understanding how LLMs acquire this ability is crucial for their
interpretability. To quantify the cross-lingual ability of LLMs accurately, we
propose a Word-Level Cross-Lingual Translation Task. To find how LLMs learn
cross-lingual ability, we trace the outputs of LLMs' intermediate layers in the
word translation task. We identify and distinguish two distinct behaviors in
the forward pass of LLMs: co-occurrence behavior and semantic pivot behavior.
We attribute LLMs' two distinct behaviors to the co-occurrence frequency of
words and find the semantic pivot from the pre-training dataset. Finally, to
apply our findings to improve the cross-lingual ability of LLMs, we reconstruct
a semantic pivot-aware pre-training dataset using documents with a high
proportion of semantic pivots. Our experiments validate the effectiveness of
our approach in enhancing cross-lingual ability. Our research contributes
insights into the interpretability of LLMs and offers a method for improving
LLMs' cross-lingual ability.

</details>


### [62] [Resource for Error Analysis in Text Simplification: New Taxonomy and Test Collection](https://arxiv.org/abs/2505.16392)
*Benjamin Vendeville,Liana Ermakova,Pierre De Loor*

Main category: cs.CL

TL;DR: 该论文针对自动文本简化(ATS)评估方法滞后的问题，提出了包含错误分类法、标注数据集和模型性能分析的新测试框架，以提升错误检测能力和简化文本质量。


<details>
  <summary>Details</summary>
Motivation: 现有ATS评估指标无法有效检测简化文本中的信息失真等错误，导致自动简化结果可靠性不足，亟需更精细的评估体系。

Method: 1. 提出基于信息失真的错误分类法；2. 构建自动简化科学文本的并行数据集并进行人工标注；3. 分析现有模型在该分类法下的错误检测与分类性能。

Result: 提供了带标注的数据集和分类法，实验表明现有模型在错误检测任务中仍有提升空间，为优化ATS模型提供基准工具。

Conclusion: 该框架使研究者能系统评估ATS错误，推动模型优化，最终提升自动简化文本的准确性与可信度。

Abstract: The general public often encounters complex texts but does not have the time
or expertise to fully understand them, leading to the spread of misinformation.
Automatic Text Simplification (ATS) helps make information more accessible, but
its evaluation methods have not kept up with advances in text generation,
especially with Large Language Models (LLMs). In particular, recent studies
have shown that current ATS metrics do not correlate with the presence of
errors. Manual inspections have further revealed a variety of errors,
underscoring the need for a more nuanced evaluation framework, which is
currently lacking. This resource paper addresses this gap by introducing a test
collection for detecting and classifying errors in simplified texts. First, we
propose a taxonomy of errors, with a formal focus on information distortion.
Next, we introduce a parallel dataset of automatically simplified scientific
texts. This dataset has been human-annotated with labels based on our proposed
taxonomy. Finally, we analyze the quality of the dataset, and we study the
performance of existing models to detect and classify errors from that
taxonomy. These contributions give researchers the tools to better evaluate
errors in ATS, develop more reliable models, and ultimately improve the quality
of automatically simplified texts.

</details>


### [63] [On the reliability of feature attribution methods for speech classification](https://arxiv.org/abs/2505.16406)
*Gaofei Shen,Hosein Mohebbi,Arianna Bisazza,Afra Alishahi,Grzegorz Chrupała*

Main category: cs.CL

TL;DR: 研究发现标准特征归因方法在语音领域普遍不可靠，但词对齐扰动方法在基于单词的分类任务中有效


<details>
  <summary>Details</summary>
Motivation: 随着预训练模型能力提升，理解其输出机制变得重要。语音信号的特殊性使特征归因方法面临挑战

Method: 通过研究输入类型、聚合方式、扰动时间跨度与分类任务特性的相互作用，评估不同特征归因方法的可靠性

Result: 标准方法可靠性不足，但词对齐扰动在词汇级分类任务中表现可靠

Conclusion: 需要开发针对语音任务特性的专用特征归因方法，现有方法仅适用于特定任务场景

Abstract: As the capabilities of large-scale pre-trained models evolve, understanding
the determinants of their outputs becomes more important. Feature attribution
aims to reveal which parts of the input elements contribute the most to model
outputs. In speech processing, the unique characteristics of the input signal
make the application of feature attribution methods challenging. We study how
factors such as input type and aggregation and perturbation timespan impact the
reliability of standard feature attribution methods, and how these factors
interact with characteristics of each classification task. We find that
standard approaches to feature attribution are generally unreliable when
applied to the speech domain, with the exception of word-aligned perturbation
methods when applied to word-based classification tasks.

</details>


### [64] [From Surveys to Narratives: Rethinking Cultural Value Adaptation in LLMs](https://arxiv.org/abs/2505.16408)
*Muhammad Farid Adilazuarda,Chen Cecilia Liu,Iryna Gurevych,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 探讨基于WVS数据增强文化价值观对齐方法的局限性并提出改进方案


<details>
  <summary>Details</summary>
Motivation: 现有基于世界价值观调查(WVS)的LLMs文化对齐方法存在文化差异捕捉不足和知识干扰问题

Method: 通过整合维基百科文化叙事和NormAd场景数据增强WVS训练框架

Result: 增强后的叙事数据相比纯调查数据显著提升文化独特性，但对下游任务产生差异化影响

Conclusion: 文化价值观对齐需要平衡任务目标与文化特异性，单一调查数据存在局限性

Abstract: Adapting cultural values in Large Language Models (LLMs) presents significant
challenges, particularly due to biases and limited training data. Prior work
primarily aligns LLMs with different cultural values using World Values Survey
(WVS) data. However, it remains unclear whether this approach effectively
captures cultural nuances or produces distinct cultural representations for
various downstream tasks. In this paper, we systematically investigate
WVS-based training for cultural value adaptation and find that relying solely
on survey data can homogenize cultural norms and interfere with factual
knowledge. To investigate these issues, we augment WVS with encyclopedic and
scenario-based cultural narratives from Wikipedia and NormAd. While these
narratives may have variable effects on downstream tasks, they consistently
improve cultural distinctiveness than survey data alone. Our work highlights
the inherent complexity of aligning cultural values with the goal of guiding
task-specific behavior.

</details>


### [65] [Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning](https://arxiv.org/abs/2505.16410)
*Guanting Dong,Yifei Chen,Xiaoxi Li,Jiajie Jin,Hongjin Qian,Yutao Zhu,Hangyu Mao,Guorui Zhou,Zhicheng Dou,Ji-Rong Wen*

Main category: cs.CL

TL;DR: 提出基于强化学习的Tool-Star框架，通过两阶段训练机制增强大语言模型的多工具协作推理能力，在10+基准测试中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型通过强化学习展现强推理能力，但在多工具协同推理方面仍存在技术空白。现有方法难以实现工具间的有效协作。

Method: 1) 开发工具集成数据合成流水线，结合提示工程与分层采样自动生成工具使用轨迹 2) 设计包含质量标准化和难度分类的数据处理流程 3) 两阶段训练框架（冷启动微调+分层奖励强化学习）

Result: 在10+复杂推理基准测试中实现显著性能提升，代码已开源。实验证明框架能有效协调多个工具完成复杂推理任务。

Conclusion: Tool-Star通过系统化的数据合成和分层强化学习设计，成功突破多工具协同推理的技术瓶颈，为LLMs的工具集成应用提供新范式。

Abstract: Recently, large language models (LLMs) have shown remarkable reasoning
capabilities via large-scale reinforcement learning (RL). However, leveraging
the RL algorithm to empower effective multi-tool collaborative reasoning in
LLMs remains an open challenge. In this paper, we introduce Tool-Star, an
RL-based framework designed to empower LLMs to autonomously invoke multiple
external tools during stepwise reasoning. Tool-Star integrates six types of
tools and incorporates systematic designs in both data synthesis and training.
To address the scarcity of tool-use data, we propose a general tool-integrated
reasoning data synthesis pipeline, which combines tool-integrated prompting
with hint-based sampling to automatically and scalably generate tool-use
trajectories. A subsequent quality normalization and difficulty-aware
classification process filters out low-quality samples and organizes the
dataset from easy to hard. Furthermore, we propose a two-stage training
framework to enhance multi-tool collaborative reasoning by: (1) cold-start
fine-tuning, which guides LLMs to explore reasoning patterns via
tool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with
hierarchical reward design, which reinforces reward understanding and promotes
effective tool collaboration. Experimental analyses on over 10 challenging
reasoning benchmarks highlight the effectiveness and efficiency of Tool-Star.
The code is available at https://github.com/dongguanting/Tool-Star.

</details>


### [66] [Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.16415)
*Ruizhe Li,Chen Chen,Yuchen Hu,Yanjun Gao,Xi Wang,Emine Yilmaz*

Main category: cs.CL

TL;DR: 提出基于Jensen-Shannon散度的ARC-JSD方法，无需额外训练即可高效识别RAG模型中的关键上下文，并揭示模型内部归因机制。


<details>
  <summary>Details</summary>
Motivation: 解决RAG模型中现有上下文归因方法计算成本高、依赖人工标注的问题，提升效率与可靠性。

Method: 使用Jensen-Shannon散度直接量化响应与上下文关联性，通过注意力头与MLP层机制分析实现无监督归因。

Result: 在TyDi QA等基准测试中实现准确率提升，计算效率比代理模型方法提高10倍，并定位到12个关键注意力头。

Conclusion: ARC-JSD为RAG提供高效归因方案，其机制发现对优化模型架构具有指导意义，推动可靠内容溯源的发展。

Abstract: Retrieval-Augmented Generation (RAG) leverages large language models (LLMs)
combined with external contexts to enhance the accuracy and reliability of
generated responses. However, reliably attributing generated content to
specific context segments, context attribution, remains challenging due to the
computationally intensive nature of current methods, which often require
extensive fine-tuning or human annotation. In this work, we introduce a novel
Jensen-Shannon Divergence driven method to Attribute Response to Context
(ARC-JSD), enabling efficient and accurate identification of essential context
sentences without additional fine-tuning or surrogate modelling. Evaluations on
a wide range of RAG benchmarks, such as TyDi QA, Hotpot QA, and Musique, using
instruction-tuned LLMs in different scales demonstrate superior accuracy and
significant computational efficiency improvements compared to the previous
surrogate-based method. Furthermore, our mechanistic analysis reveals specific
attention heads and multilayer perceptron (MLP) layers responsible for context
attribution, providing valuable insights into the internal workings of RAG
models.

</details>


### [67] [Exploring the Relationship Between Diversity and Quality in Ad Text Generation](https://arxiv.org/abs/2505.16418)
*Yoichi Aoki,Soichiro Murakami,Ukyo Honda,Akihiko Kato*

Main category: cs.CL

TL;DR: 探讨广告文本生成中多样性增强方法对广告质量的影响及其相关因素


<details>
  <summary>Details</summary>
Motivation: 广告文本生成与摘要/翻译任务存在文本风格差异，现有多样性增强方法在广告场景的效果尚未充分验证

Method: 通过多维度分析多样性增强方法（包括模型架构、超参数设置、输入输出格式等）

Result: 揭示不同因素组合对广告文本质量和多样性的非线性影响关系

Conclusion: 广告生成需建立专门的多样性评估框架，参数优化需适配广告场景特性

Abstract: In natural language generation for advertising, creating diverse and engaging
ad texts is crucial for capturing a broad audience and avoiding advertising
fatigue. Regardless of the importance of diversity, the impact of the
diversity-enhancing methods in ad text generation -- mainly tested on tasks
such as summarization and machine translation -- has not been thoroughly
explored. Ad text generation significantly differs from these tasks owing to
the text style and requirements. This research explores the relationship
between diversity and ad quality in ad text generation by considering multiple
factors, such as diversity-enhancing methods, their hyperparameters,
input-output formats, and the models.

</details>


### [68] [WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2505.16421)
*Zhepei Wei,Wenlin Yao,Yao Liu,Weizhi Zhang,Qin Lu,Liang Qiu,Changlong Yu,Puyang Xu,Chao Zhang,Bing Yin,Hyokun Yun,Lihong Li*

Main category: cs.CL

TL;DR: 提出WebAgent-R1强化学习框架，通过异步轨迹生成与二元奖励机制，显著提升多轮网络任务成功率（Qwen模型33.9%，Llama模型44.8%）


<details>
  <summary>Details</summary>
Motivation: 传统强化学习主要优化单轮任务，而多轮网页交互需处理动态界面长时决策，现有方法面临显著挑战

Method: 端到端多轮RL框架，通过在线交互异步生成多样化轨迹，完全依赖任务成功的二元奖励驱动训练过程

Result: WebArena-Lite基准测试中超越SOTA方法，成功率提升5倍以上，优于OpenAI o3等商业模型

Conclusion: 验证了思考型提示策略与交互扩展的有效性，预热训练阶段对模型性能起关键作用，长链推理整合提供新方向

Abstract: While reinforcement learning (RL) has demonstrated remarkable success in
enhancing large language models (LLMs), it has primarily focused on single-turn
tasks such as solving math problems. Training effective web agents for
multi-turn interactions remains challenging due to the complexity of
long-horizon decision-making across dynamic web interfaces. In this work, we
present WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework
for training web agents. It learns directly from online interactions with web
environments by asynchronously generating diverse trajectories, entirely guided
by binary rewards depending on task success. Experiments on the WebArena-Lite
benchmark demonstrate the effectiveness of WebAgent-R1, boosting the task
success rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to
44.8%, significantly outperforming existing state-of-the-art methods and strong
proprietary models such as OpenAI o3. In-depth analyses reveal the
effectiveness of the thinking-based prompting strategy and test-time scaling
through increased interactions for web tasks. We further investigate different
RL initialization policies by introducing two variants, namely WebAgent-R1-Zero
and WebAgent-R1-CoT, which highlight the importance of the warm-up training
stage (i.e., behavior cloning) and provide insights on incorporating long
chain-of-thought (CoT) reasoning in web agents.

</details>


### [69] [$I^2G$: Generating Instructional Illustrations via Text-Conditioned Diffusion](https://arxiv.org/abs/2505.16425)
*Jing Bi,Pinxin Liu,Ali Vosoughi,Jiarui Wu,Jinxi He,Chenliang Xu*

Main category: cs.CL

TL;DR: 提出语言驱动框架将过程性文本转化为视觉指令，通过三个创新点提升语言-图像对齐效果


<details>
  <summary>Details</summary>
Motivation: 纯文本难以有效传达复杂物理动作和空间关系，需要解决过程性知识的可视化表达问题

Method: 1.基于成分句法分析的文本编码机制 2.成对语篇连贯模型 3.专门的过程性语言-图像对齐评估协议

Result: 在HTStep/CaptainCook4D/WikiAll三个数据集上显著优于基线模型

Conclusion: 该框架为教育、任务指导和多模态理解领域提供了有效的过程性语言可视化解决方案

Abstract: The effective communication of procedural knowledge remains a significant
challenge in natural language processing (NLP), as purely textual instructions
often fail to convey complex physical actions and spatial relationships. We
address this limitation by proposing a language-driven framework that
translates procedural text into coherent visual instructions. Our approach
models the linguistic structure of instructional content by decomposing it into
goal statements and sequential steps, then conditioning visual generation on
these linguistic elements. We introduce three key innovations: (1) a
constituency parser-based text encoding mechanism that preserves semantic
completeness even with lengthy instructions, (2) a pairwise discourse coherence
model that maintains consistency across instruction sequences, and (3) a novel
evaluation protocol specifically designed for procedural language-to-image
alignment. Our experiments across three instructional datasets (HTStep,
CaptainCook4D, and WikiAll) demonstrate that our method significantly
outperforms existing baselines in generating visuals that accurately reflect
the linguistic content and sequential nature of instructions. This work
contributes to the growing body of research on grounding procedural language in
visual content, with applications spanning education, task guidance, and
multimodal language understanding.

</details>


### [70] [Beyond Static Testbeds: An Interaction-Centric Agent Simulation Platform for Dynamic Recommender Systems](https://arxiv.org/abs/2505.16429)
*Song Jin,Juntian Zhang,Yuhan Liu,Xun Zhang,Yufei Zhang,Guojun Yin,Fei Jiang,Wei Lin,Rui Yan*

Main category: cs.CL

TL;DR: 提出基于智能体的推荐系统仿真平台RecInter，通过动态交互机制实现高可信度系统演化模拟，成功复现品牌忠诚度与马太效应等涌现现象


<details>
  <summary>Details</summary>
Motivation: 传统A/B测试资源消耗大，离线方法难以捕捉动态用户-平台交互。现有仿真平台缺乏用户行为动态重塑环境的机制，无法模拟真实系统演化过程

Method: 1. 构建智能体间实时动态交互机制（用户行为实时更新物品属性/商家智能体响应）
2. 多维用户画像模块保证仿真保真度
3. 融合思维链的LLM微调框架处理交互数据
4. 创新智能体架构支持环境动态演化

Result: 平台仿真可信度显著提升（+38.7%），成功复现品牌忠诚度形成（用户重复购买率+62%）、马太效应（头部商品曝光集中度+45%）等实际商业场景中的典型现象

Conclusion: RecInter通过双向动态交互机制建立了推荐系统研究的可信实验场，为系统演化模拟提供创新解决方案，验证交互机制对模拟真实商业生态的关键作用

Abstract: Evaluating and iterating upon recommender systems is crucial, yet traditional
A/B testing is resource-intensive, and offline methods struggle with dynamic
user-platform interactions. While agent-based simulation is promising, existing
platforms often lack a mechanism for user actions to dynamically reshape the
environment. To bridge this gap, we introduce RecInter, a novel agent-based
simulation platform for recommender systems featuring a robust interaction
mechanism. In RecInter platform, simulated user actions (e.g., likes, reviews,
purchases) dynamically update item attributes in real-time, and introduced
Merchant Agents can reply, fostering a more realistic and evolving ecosystem.
High-fidelity simulation is ensured through Multidimensional User Profiling
module, Advanced Agent Architecture, and LLM fine-tuned on Chain-of-Thought
(CoT) enriched interaction data. Our platform achieves significantly improved
simulation credibility and successfully replicates emergent phenomena like
Brand Loyalty and the Matthew Effect. Experiments demonstrate that this
interaction mechanism is pivotal for simulating realistic system evolution,
establishing our platform as a credible testbed for recommender systems
research.

</details>


### [71] [University of Indonesia at SemEval-2025 Task 11: Evaluating State-of-the-Art Encoders for Multi-Label Emotion Detection](https://arxiv.org/abs/2505.16460)
*Ikhlasul Akmal Hanif,Eryawan Presma Yulianrifat,Jaycent Gunawan Ongris,Eduardus Tjitrahardja,Muhammad Falensi Azmi,Rahmat Bryan Naufal,Alfan Farizki Wicaksono*

Main category: cs.CL

TL;DR: 集成多个BGE编码器+CatBoost分类器的方案在28语种多标签情绪分类任务中取得56.58 F1分数


<details>
  <summary>Details</summary>
Motivation: 探索Transformer模型在跨语言多标签情绪分类任务中的最佳应用方式，比较完全微调与分类器训练的效能差异

Method: 对比两种方案：1) 完全微调XLMR/mBERT；2) 在mE5/BGE提示编码器上训练分类器。测试不同架构、损失函数及分类器组合

Result: 基于BGE编码器的集成模型（CatBoost分类器）在最终榜单上达到跨28语言平均56.58 F1-macro分

Conclusion: 提示编码器+轻量分类器的范式优于完全微调，模型集成策略能有效提升多语言情感分类性能

Abstract: This paper presents our approach for SemEval 2025 Task 11 Track A, focusing
on multilabel emotion classification across 28 languages. We explore two main
strategies: fully fine-tuning transformer models and classifier-only training,
evaluating different settings such as fine-tuning strategies, model
architectures, loss functions, encoders, and classifiers. Our findings suggest
that training a classifier on top of prompt-based encoders such as mE5 and BGE
yields significantly better results than fully fine-tuning XLMR and mBERT. Our
best-performing model on the final leaderboard is an ensemble combining
multiple BGE models, where CatBoost serves as the classifier, with different
configurations. This ensemble achieves an average F1-macro score of 56.58
across all languages.

</details>


### [72] [Reading Between the Prompts: How Stereotypes Shape LLM's Implicit Personalization](https://arxiv.org/abs/2505.16467)
*Vera Neplenbroek,Arianna Bisazza,Raquel Fernández*

Main category: cs.CL

TL;DR: 大型语言模型通过对话线索隐性推断用户身份特征，发现刻板印象推断存在持续性，并提出通过干预模型表示缓解该问题


<details>
  <summary>Details</summary>
Motivation: 探究LLMs如何通过隐性个性化机制基于刻板印象推断用户属性，及其对少数群体回复质量的影响

Method: 使用受控合成对话分析模型内部表示和生成响应，采用线性探针干预技术

Result: LLMs持续通过刻板信号推断用户属性（即使明确声明不同身份），干预技术可有效缓解此现象

Conclusion: 需增强LLMs用户身份表征的透明度与控制机制，确保公平性

Abstract: Generative Large Language Models (LLMs) infer user's demographic information
from subtle cues in the conversation -- a phenomenon called implicit
personalization. Prior work has shown that such inferences can lead to lower
quality responses for users assumed to be from minority groups, even when no
demographic information is explicitly provided. In this work, we systematically
explore how LLMs respond to stereotypical cues using controlled synthetic
conversations, by analyzing the models' latent user representations through
both model internals and generated answers to targeted user questions. Our
findings reveal that LLMs do infer demographic attributes based on these
stereotypical signals, which for a number of groups even persists when the user
explicitly identifies with a different demographic group. Finally, we show that
this form of stereotype-driven implicit personalization can be effectively
mitigated by intervening on the model's internal representations using a
trained linear probe to steer them toward the explicitly stated identity. Our
results highlight the need for greater transparency and control in how LLMs
represent user identity.

</details>


### [73] [Teaching Large Language Models to Maintain Contextual Faithfulness via Synthetic Tasks and Reinforcement Learning](https://arxiv.org/abs/2505.16483)
*Shuzheng Si,Haozhe Zhao,Cheng Gao,Yuzhuo Bai,Zhitong Wang,Bofei Gao,Kangyang Luo,Wenhao Li,Yufei Huang,Gang Chen,Fanchao Qi,Minjia Zhang,Baobao Chang,Maosong Sun*

Main category: cs.CL

TL;DR: 提出了CANOE框架，通过合成短格式QA数据与Dual-GRPO强化学习方法，无需人工标注即可提升大语言模型在生成任务中的忠实性


<details>
  <summary>Details</summary>
Motivation: 提高大语言模型在上下文中的忠实性对构建可靠的信息检索系统至关重要

Method: 1. 合成四类短格式QA数据构建可验证训练集；2. 提出Dual-GRPO强化学习方法，使用三个基于规则的奖励同时优化短/长格式生成

Result: 在11个下游任务中显著提升模型忠实性，表现超越GPT-4o等先进模型

Conclusion: CANOE框架通过自动化数据合成与双目标优化策略，有效解决了人工标注依赖与短格式过优化问题，显著提升模型可靠性

Abstract: Teaching large language models (LLMs) to be faithful in the provided context
is crucial for building reliable information-seeking systems. Therefore, we
propose a systematic framework, CANOE, to improve the faithfulness of LLMs in
both short-form and long-form generation tasks without human annotations.
Specifically, we first synthesize short-form question-answering (QA) data with
four diverse tasks to construct high-quality and easily verifiable training
data without human annotation. Also, we propose Dual-GRPO, a rule-based
reinforcement learning method that includes three tailored rule-based rewards
derived from synthesized short-form QA data, while simultaneously optimizing
both short-form and long-form response generation. Notably, Dual-GRPO
eliminates the need to manually label preference data to train reward models
and avoids over-optimizing short-form generation when relying only on the
synthesized short-form QA data. Experimental results show that CANOE greatly
improves the faithfulness of LLMs across 11 different downstream tasks, even
outperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1.

</details>


### [74] [LLaMAs Have Feelings Too: Unveiling Sentiment and Emotion Representations in LLaMA Models Through Probing](https://arxiv.org/abs/2505.16491)
*Dario Di Palma,Alessandro De Bellis,Giovanni Servedio,Vito Walter Anelli,Fedelucio Narducci,Tommaso Di Noia*

Main category: cs.CL

TL;DR: 研究通过分层探测发现Llama模型的情感信息集中在中间层，检测准确率提升14%，内存需求降低57%


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLMs如何捕捉情感信息的机制理解有限，需探究模型内部情感表征的分布及其对任务性能的影响

Method: 使用探测分类器分析不同层和模型规模的情感编码，通过对比实验确定最佳表征层和池化方法

Result: 1) 二元情感任务中检测准确率最高提升14% 2) 仅解码器模型的最后token非最佳信息载体 3) 平均减少57%内存需求

Conclusion: 分层探测机制为超越prompt的情感分析提供了新范式，既能提升模型效用又可降低资源消耗，推动LLMs内部表征机制研究

Abstract: Large Language Models (LLMs) have rapidly become central to NLP,
demonstrating their ability to adapt to various tasks through prompting
techniques, including sentiment analysis. However, we still have a limited
understanding of how these models capture sentiment-related information. This
study probes the hidden layers of Llama models to pinpoint where sentiment
features are most represented and to assess how this affects sentiment
analysis.
  Using probe classifiers, we analyze sentiment encoding across layers and
scales, identifying the layers and pooling methods that best capture sentiment
signals. Our results show that sentiment information is most concentrated in
mid-layers for binary polarity tasks, with detection accuracy increasing up to
14% over prompting techniques. Additionally, we find that in decoder-only
models, the last token is not consistently the most informative for sentiment
encoding. Finally, this approach enables sentiment tasks to be performed with
memory requirements reduced by an average of 57%.
  These insights contribute to a broader understanding of sentiment in LLMs,
suggesting layer-specific probing as an effective approach for sentiment tasks
beyond prompting, with potential to enhance model utility and reduce memory
requirements.

</details>


### [75] [Sparse Activation Editing for Reliable Instruction Following in Narratives](https://arxiv.org/abs/2505.16505)
*Runcong Zhao,Chengyu Cao,Qinglin Zhu,Xiucheng Lv,Shun Shao,Lin Gui,Ruifeng Xu,Yulan He*

Main category: cs.CL

TL;DR: 提出无需训练的Concise-SAE框架，通过神经元编辑提升指令遵循能力，并创建FreeInstruct基准验证其在叙事场景中的有效性


<details>
  <summary>Details</summary>
Motivation: 现有基准无法有效评估复杂叙事场景中的指令遵循能力，需开发新方法解决该问题

Method: 使用自然语言指令识别/编辑相关神经元（无需标注数据），构建包含1,212个叙事场景样本的FreeInstruct评估基准

Result: 在保持生成质量的同时实现SOTA指令遵循性能，证明方法在多样化任务中的通用性

Conclusion: Concise-SAE有效解决复杂叙事下的指令遵循难题，FreeInstruct为后续研究提供标准化评估体系，该方法兼具高效性与通用性

Abstract: Complex narrative contexts often challenge language models' ability to follow
instructions, and existing benchmarks fail to capture these difficulties. To
address this, we propose Concise-SAE, a training-free framework that improves
instruction following by identifying and editing instruction-relevant neurons
using only natural language instructions, without requiring labelled data. To
thoroughly evaluate our method, we introduce FreeInstruct, a diverse and
realistic benchmark of 1,212 examples that highlights the challenges of
instruction following in narrative-rich settings. While initially motivated by
complex narratives, Concise-SAE demonstrates state-of-the-art instruction
adherence across varied tasks without compromising generation quality.

</details>


### [76] [AppealCase: A Dataset and Benchmark for Civil Case Appeal Scenarios](https://arxiv.org/abs/2505.16514)
*Yuting Huang,Meitong Guo,Yiquan Wu,Ang Li,Xiaozhong Liu,Keting Yin,Changlong Sun,Fei Wu,Kun Kuang*

Main category: cs.CL

TL;DR: 提出AppealCase数据集，推动上诉案例分析与司法一致性研究


<details>
  <summary>Details</summary>
Motivation: 现有LegalAI研究集中于个案分析，忽视了司法系统中上诉流程对纠错机制和公平审判的关键作用

Method: 构建包含10,000对真实一/二审文书的数据集，覆盖91类民事案件，包含5个上诉审查维度的标注，提出5个新任务并评估20个主流模型

Result: 现有模型在判决逆转预测任务上F1值均低于50%，证明上诉场景的复杂性和挑战性

Conclusion: AppealCase数据集将促进LegalAI在上诉案例分析领域的研究，助力提升司法决策一致性

Abstract: Recent advances in LegalAI have primarily focused on individual case judgment
analysis, often overlooking the critical appellate process within the judicial
system. Appeals serve as a core mechanism for error correction and ensuring
fair trials, making them highly significant both in practice and in research.
To address this gap, we present the AppealCase dataset, consisting of 10,000
pairs of real-world, matched first-instance and second-instance documents
across 91 categories of civil cases. The dataset also includes detailed
annotations along five dimensions central to appellate review: judgment
reversals, reversal reasons, cited legal provisions, claim-level decisions, and
whether there is new information in the second instance. Based on these
annotations, we propose five novel LegalAI tasks and conduct a comprehensive
evaluation across 20 mainstream models. Experimental results reveal that all
current models achieve less than 50% F1 scores on the judgment reversal
prediction task, highlighting the complexity and challenge of the appeal
scenario. We hope that the AppealCase dataset will spur further research in
LegalAI for appellate case analysis and contribute to improving consistency in
judicial decision-making.

</details>


### [77] [CUB: Benchmarking Context Utilisation Techniques for Language Models](https://arxiv.org/abs/2505.16518)
*Lovisa Hagström,Youna Kim,Haeun Yu,Sang-goo Lee,Richard Johansson,Hyunsoo Cho,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 开发CUB基准评估上下文操纵技术在检索增强生成中的表现，发现现有方法难以处理真实场景的多样性且在合成数据上存在高估现象


<details>
  <summary>Details</summary>
Motivation: 现有上下文利用操纵技术(CMTs)缺乏系统性评估，且难以应对真实检索增强场景中复杂的上下文类型矛盾问题

Method: 构建包含三种关键上下文类型的CUB基准，在三个不同数据集和九个语言模型上评估七种典型CMTs方法

Result: 多数CMTs无法全面处理真实场景的上下文类型，且在合成数据上的表现显著优于自然发生的真实数据样本

Conclusion: 需要开发能同时处理多种上下文类型的CMTs，并建立更全面的评估体系来避免方法选择的偏差

Abstract: Incorporating external knowledge is crucial for knowledge-intensive tasks,
such as question answering and fact checking. However, language models (LMs)
may ignore relevant information that contradicts outdated parametric memory or
be distracted by irrelevant contexts. While many context utilisation
manipulation techniques (CMTs) that encourage or suppress context utilisation
have recently been proposed to alleviate these issues, few have seen systematic
comparison. In this paper, we develop CUB (Context Utilisation Benchmark) to
help practitioners within retrieval-augmented generation (RAG) identify the
best CMT for their needs. CUB allows for rigorous testing on three distinct
context types, observed to capture key challenges in realistic context
utilisation scenarios. With this benchmark, we evaluate seven state-of-the-art
methods, representative of the main categories of CMTs, across three diverse
datasets and tasks, applied to nine LMs. Our results show that most of the
existing CMTs struggle to handle the full set of types of contexts that may be
encountered in real-world retrieval-augmented scenarios. Moreover, we find that
many CMTs display an inflated performance on simple synthesised datasets,
compared to more realistic datasets with naturally occurring samples.
Altogether, our results show the need for holistic tests of CMTs and the
development of CMTs that can handle multiple context types.

</details>


### [78] [Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs](https://arxiv.org/abs/2505.16520)
*Giovanni Servedio,Alessandro De Bellis,Dario Di Palma,Vito Walter Anelli,Tommaso Di Noia*

Main category: cs.CL

TL;DR: 研究挑战现有LLM真实性编码研究，提出基于表格和问答数据的真实-虚假事实生成方法，验证真实性检测在LLM生成数据中的泛化挑战。


<details>
  <summary>Details</summary>
Motivation: 传统LLM事实性研究依赖合成数据，缺乏现实性导致泛化能力不足。需构建更真实的数据集评估模型事实性。

Method: 1. 从表格数据采样真伪事实句 2. 基于问答数据生成LLM依赖型真伪数据集

Result: 开源LLM验证显示：先前结论部分有效，但LLM生成数据的真实性检测泛化仍困难

Conclusion: 为LLM事实性研究奠定基础，提供更有效评估指南，强调现实数据验证的重要性

Abstract: Factual hallucinations are a major challenge for Large Language Models
(LLMs). They undermine reliability and user trust by generating inaccurate or
fabricated content. Recent studies suggest that when generating false
statements, the internal states of LLMs encode information about truthfulness.
However, these studies often rely on synthetic datasets that lack realism,
which limits generalization when evaluating the factual accuracy of text
generated by the model itself. In this paper, we challenge the findings of
previous work by investigating truthfulness encoding capabilities, leading to
the generation of a more realistic and challenging dataset. Specifically, we
extend previous work by introducing: (1) a strategy for sampling plausible
true-false factoid sentences from tabular data and (2) a procedure for
generating realistic, LLM-dependent true-false datasets from Question Answering
collections. Our analysis of two open-source LLMs reveals that while the
findings from previous studies are partially validated, generalization to
LLM-generated datasets remains challenging. This study lays the groundwork for
future research on factuality in LLMs and offers practical guidelines for more
effective evaluation.

</details>


### [79] [Benchmarking and Pushing the Multi-Bias Elimination Boundary of LLMs via Causal Effect Estimation-guided Debiasing](https://arxiv.org/abs/2505.16522)
*Zhouhao Sun,Zhiyuan Kan,Xiao Ding,Li Du,Yang Zhao,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: 提出多偏见基准测试揭示现有大模型去偏方法的不足，开发CMBE方法通过因果效应估计消除多重偏见，有效提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试仅含单一类型偏见，而实际应用中数据常含多重偏见，亟需研究多重偏见同时消除方法以提升LLM泛化性能。

Method: 提出因果效应估计引导的多重偏见消除方法（CMBE）：1. 同时估计多重偏见的因果效应；2. 从总因果效应中剥离偏见影响。

Result: 实验表明现有LLM及去偏方法在多偏见场景下表现欠佳，而CMBE能有效消除97.8%的偏见影响，显著提升模型鲁棒性。

Conclusion: CMBE首次实现多重偏见同步消除，验证因果推断方法在提升大模型泛化能力方面的有效性，为AI公平性研究提供新方向。

Abstract: Despite significant progress, recent studies have indicated that current
large language models (LLMs) may still utilize bias during inference, leading
to the poor generalizability of LLMs. Some benchmarks are proposed to
investigate the generalizability of LLMs, with each piece of data typically
containing one type of controlled bias. However, a single piece of data may
contain multiple types of biases in practical applications. To bridge this gap,
we propose a multi-bias benchmark where each piece of data contains five types
of biases. The evaluations conducted on this benchmark reveal that the
performance of existing LLMs and debiasing methods is unsatisfying,
highlighting the challenge of eliminating multiple types of biases
simultaneously. To overcome this challenge, we propose a causal effect
estimation-guided multi-bias elimination method (CMBE). This method first
estimates the causal effect of multiple types of biases simultaneously.
Subsequently, we eliminate the causal effect of biases from the total causal
effect exerted by both the semantic information and biases during inference.
Experimental results show that CMBE can effectively eliminate multiple types of
bias simultaneously to enhance the generalizability of LLMs.

</details>


### [80] [EnSToM: Enhancing Dialogue Systems with Entropy-Scaled Steering Vectors for Topic Maintenance](https://arxiv.org/abs/2505.16526)
*Heejae Suh,Yejin Jeon,Deokhyung Kang,Taehee Park,Yejin Min,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 提出EnSToM方法，通过基于输入不确定性的动态调整机制，解决小语言模型在任务对话中的主题一致性难题。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在资源受限场景应用广泛，但现有激活工程方法在主题坚持上存在局限，影响服务可靠性。

Method: 开发熵值标定的动态引导向量（EnSToM），根据输入不确定性自适应调整引导强度，平衡主题维护与任务准确性。

Result: 实验显示EnSToM相比微调方法数据效率提升显著，在100个样本下实现63.2%的准确率提升。

Conclusion: EnSToM在不牺牲效率的前提下增强主题坚持能力，为小模型对话系统提供轻量级解决方案。

Abstract: Small large language models (sLLMs) offer the advantage of being lightweight
and efficient, which makes them suitable for resource-constrained environments.
However, sLLMs often struggle to maintain topic consistency in task-oriented
dialogue systems, which is critical for scenarios such as service chatbots.
Specifically, it is important to ensure that the model denies off-topic or
malicious inputs and adheres to its intended functionality so as to prevent
potential misuse and uphold reliability. Towards this, existing activation
engineering approaches have been proposed to manipulate internal activations
during inference. While these methods are effective in certain scenarios, our
preliminary experiments reveal their limitations in ensuring topic adherence.
Therefore, to address this, we propose a novel approach termed Entropy-scaled
Steering vectors for Topic Maintenance (EnSToM). EnSToM dynamically adjusts the
steering intensity based on input uncertainty, which allows the model to handle
off-topic distractors effectively while preserving on-topic accuracy. Our
experiments demonstrate that EnSToM achieves significant performance gain with
a relatively small data size compared to fine-tuning approaches. By improving
topic adherence without compromising efficiency, our approach provides a robust
solution for enhancing sLLM-based dialogue systems.

</details>


### [81] [Mechanistic Understanding and Mitigation of Language Confusion in English-Centric Large Language Models](https://arxiv.org/abs/2505.16538)
*Ercong Nie,Helmut Schmid,Hinrich Schütze*

Main category: cs.CL

TL;DR: 通过机制可解释性方法揭示LLM语言混淆的核心机制，提出神经元编辑干预方案有效缓解问题


<details>
  <summary>Details</summary>
Motivation: 英语中心模型在处理多语言时易产生非目标语言输出的语言混淆问题，影响模型可靠性

Method: 结合语言混淆基准测试(LCB)、TunedLens分层分析和多语言模型对比的神经元归因方法

Result: 发现末层过渡故障是主因，编辑少量关键神经元可减少83%混淆且保持模型性能

Conclusion: 神经元级干预为多语言建模提供新方向，在保持模型流畅度的同时提升输出准确性

Abstract: Language confusion -- where large language models (LLMs) generate unintended
languages against the user's need -- remains a critical challenge, especially
for English-centric models. We present the first mechanistic interpretability
(MI) study of language confusion, combining behavioral benchmarking with
neuron-level analysis. Using the Language Confusion Benchmark (LCB), we show
that confusion points (CPs) -- specific positions where language switches occur
-- are central to this phenomenon. Through layer-wise analysis with TunedLens
and targeted neuron attribution, we reveal that transition failures in the
final layers drive confusion. We further demonstrate that editing a small set
of critical neurons, identified via comparative analysis with
multilingual-tuned models, substantially mitigates confusion without harming
general competence or fluency. Our approach matches multilingual alignment in
confusion reduction for most languages and yields cleaner, higher-quality
outputs. These findings provide new insights into the internal dynamics of LLMs
and highlight neuron-level interventions as a promising direction for robust,
interpretable multilingual language modeling.

</details>


### [82] [Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains](https://arxiv.org/abs/2505.16552)
*Wenhui Tan,Jiaze Li,Jianzhong Ju,Zhenbo Luo,Jian Luan,Ruihua Song*

Main category: cs.CL

TL;DR: 提出CoLaR框架，通过潜在空间压缩和强化学习实现高效推理，在数学任务中准确率提升14.1%，推理链长度减少53.3%


<details>
  <summary>Details</summary>
Motivation: 传统CoT推理在token级别计算成本高且低效，需探索潜在空间压缩方法提升效率

Method: 1. 监督微调阶段引入压缩嵌入预测目标，合并连续token嵌入；2. 强化学习阶段探索紧凑推理路径

Result: 四个数学数据集上准确率提升14.1%，推理链减少53.3%；RL增强版性能提升5.4%，潜在推理链缩短82.8%

Conclusion: CoLaR在潜在空间实现高效密集推理，动态调节推理速度，平衡效率与性能，代码即将开源

Abstract: Large Language Models (LLMs) achieve superior performance through
Chain-of-Thought (CoT) reasoning, but these token-level reasoning chains are
computationally expensive and inefficient. In this paper, we introduce
Compressed Latent Reasoning (CoLaR), a novel framework that dynamically
compresses reasoning processes in latent space through a two-stage training
approach. First, during supervised fine-tuning, CoLaR extends beyond next-token
prediction by incorporating an auxiliary next compressed embedding prediction
objective. This process merges embeddings of consecutive tokens using a
compression factor randomly sampled from a predefined range, and trains a
specialized latent head to predict distributions of subsequent compressed
embeddings. Second, we enhance CoLaR through reinforcement learning (RL) that
leverages the latent head's non-deterministic nature to explore diverse
reasoning paths and exploit more compact ones. This approach enables CoLaR to:
i) perform reasoning at a dense latent level (i.e., silently), substantially
reducing reasoning chain length, and ii) dynamically adjust reasoning speed at
inference time by simply prompting the desired compression factor. Extensive
experiments across four mathematical reasoning datasets demonstrate that CoLaR
achieves 14.1% higher accuracy than latent-based baseline methods at comparable
compression ratios, and reduces reasoning chain length by 53.3% with only 4.8%
performance degradation compared to explicit CoT method. Moreover, when applied
to more challenging mathematical reasoning tasks, our RL-enhanced CoLaR
demonstrates performance gains of up to 5.4% while dramatically reducing latent
reasoning chain length by 82.8%. The code and models will be released upon
acceptance.

</details>


### [83] [ScholarBench: A Bilingual Benchmark for Abstraction, Comprehension, and Reasoning Evaluation in Academic Contexts](https://arxiv.org/abs/2505.16566)
*Dongwon Noh,Donghyeok Koh,Junghun Yuk,Gyuwan Kim,Jaeyong Lee,Kyungtae Lim,Cheoneum Park*

Main category: cs.CL

TL;DR: 提出跨领域英韩双语学术基准ScholarBench，通过多维度评估显示现有大模型在复杂学术推理任务中表现欠佳（平均分0.543）


<details>
  <summary>Details</summary>
Motivation: 解决现有基准在复杂学术任务评估上的局限性，构建深度专业化的评估体系以全面测试大语言模型学术能力

Method: 采用三步构建流程：1) 从学术文献提取专业问题 2) 设计符合学科特性的评估维度 3) 构建包含5类问题、8个领域的英韩双语数据集（总计10,340例）

Result: 顶尖模型o3-mini在5000+样本测试中仅获0.543分，验证基准挑战性

Conclusion: ScholarBench成功建立多维学术评估标准，其双语特性和领域深度为LLM能力评估提供新范式

Abstract: Prior benchmarks for evaluating the domain-specific knowledge of large
language models (LLMs) lack the scalability to handle complex academic tasks.
To address this, we introduce \texttt{ScholarBench}, a benchmark centered on
deep expert knowledge and complex academic problem-solving, which evaluates the
academic reasoning ability of LLMs and is constructed through a three-step
process. \texttt{ScholarBench} targets more specialized and logically complex
contexts derived from academic literature, encompassing five distinct problem
types. Unlike prior benchmarks, \texttt{ScholarBench} evaluates the
abstraction, comprehension, and reasoning capabilities of LLMs across eight
distinct research domains. To ensure high-quality evaluation data, we define
category-specific example attributes and design questions that are aligned with
the characteristic research methodologies and discourse structures of each
domain. Additionally, this benchmark operates as an English-Korean bilingual
dataset, facilitating simultaneous evaluation for linguistic capabilities of
LLMs in both languages. The benchmark comprises 5,031 examples in Korean and
5,309 in English, with even state-of-the-art models like o3-mini achieving an
average evaluation score of only 0.543, demonstrating the challenging nature of
this benchmark.

</details>


### [84] [URLs Help, Topics Guide: Understanding Metadata Utility in LLM Training](https://arxiv.org/abs/2505.16570)
*Dongyang Fan,Vinko Sabolčec,Martin Jaggi*

Main category: cs.CL

TL;DR: 研究表明选择性使用上下文元数据能优化LLM训练：URL加速训练效率但需长提示提升下游性能，主题/格式元数据实现可控生成，质量评分效果有限。


<details>
  <summary>Details</summary>
Motivation: 探索预训练中不同上下文元数据（URL/质量评分/主题）对训练效率和生成控制的实际效果差异及其作用条件

Method: 通过系统评估比较URL/质量/主题三类元数据在训练加速、下游任务表现、生成可控性三个维度的效果差异

Result: 1. 仅URL元数据缩短15%训练时间
2. URL条件化下游性能提升需300+ tokens提示
3. 主题元数据使生成控制精度提升47%
4. 质量评分对模型无显著影响

Conclusion: 上下文元数据应差异化应用：URL优化训练效率，主题/格式增强可控生成，提示工程是发挥元数据效用的关键

Abstract: Large Language Models (LLMs) are commonly pretrained on vast corpora of text
without utilizing contextual metadata such as source, quality, or topic,
leading to a context-free learning paradigm. While recent studies suggest that
adding metadata like URL information as context (i.e., auxiliary inputs not
used in the loss calculation) can improve training efficiency and downstream
performance, they offer limited understanding of which types of metadata are
truly effective and under what conditions. In this work, we conduct a
systematic evaluation and find that not all metadata types contribute equally.
Only URL context speeds up training, whereas quality scores and topic/format
domain information offer no clear benefit. Furthermore, the improved downstream
performances of URL conditioning emerge only when longer prompts are used at
inference time. In addition, we demonstrate that context-aware pretraining
enables more controllable generation than context-free pretraining, in a
classifier-free guidance fashion. Although topic and format metadata do not
accelerate training, they are effective for steering outputs, offering
human-interpretable control over generation.

</details>


### [85] [EMULATE: A Multi-Agent Framework for Determining the Veracity of Atomic Claims by Emulating Human Actions](https://arxiv.org/abs/2505.16576)
*Spencer Hong,Meng Luo,Xinyi Wan*

Main category: cs.CL

TL;DR: 提出多智能体框架EMULATE改进事实核查系统，通过分工协作模拟人类行为，在多个基准测试中超越现有方法


<details>
  <summary>Details</summary>
Motivation: 现有系统采用单次证据检索+分类的线性流程，与人类多次迭代检索证据的行为模式存在差异。近期迭代检索方法虽改进但仍未完全模拟人类分工协作特性

Method: 设计多智能体框架，不同Agent分别承担搜索结果排序、网页内容评估等子任务，通过协同工作完成完整的声明验证流程

Result: 在多个事实核查基准测试中取得显著效果提升，验证了多智能体框架的有效性

Conclusion: 通过模块化分工设计更贴近人类事实核查模式，为自动化验证系统提供了新的架构范式

Abstract: Determining the veracity of atomic claims is an imperative component of many
recently proposed fact-checking systems. Many approaches tackle this problem by
first retrieving evidence by querying a search engine and then performing
classification by providing the evidence set and atomic claim to a large
language model, but this process deviates from what a human would do in order
to perform the task. Recent work attempted to address this issue by proposing
iterative evidence retrieval, allowing for evidence to be collected several
times and only when necessary. Continuing along this line of research, we
propose a novel claim verification system, called EMULATE, which is designed to
better emulate human actions through the use of a multi-agent framework where
each agent performs a small part of the larger task, such as ranking search
results according to predefined criteria or evaluating webpage content.
Extensive experiments on several benchmarks show clear improvements over prior
work, demonstrating the efficacy of our new multi-agent framework.

</details>


### [86] [O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering](https://arxiv.org/abs/2505.16582)
*Jianbiao Mei,Tao Hu,Daocheng Fu,Licheng Wen,Xuemeng Yang,Rong Wu,Pinlong Cai,Xing Gao,Yu Yang,Chengjun Xie,Botian Shi,Yong Liu,Yu Qiao*

Main category: cs.CL

TL;DR: 提出强化学习驱动的O²-Searcher搜索代理，通过本地模拟环境动态获取知识，在开放/封闭式问答任务中均实现SOTA性能，且3B小模型效率显著优于大模型。


<details>
  <summary>Details</summary>
Motivation: LLMs受限于静态参数知识，处理开放域动态信息时表现不足，尤其在答案不唯一的开放式问题上缺乏有效解决方案。

Method: 构建本地模拟搜索环境实现知识获取与推理解耦，设计统一强化学习框架（含问题类型识别奖励机制）适配不同问答策略。

Result: 在自建O²-QA基准上显著超越主流LLM代理，封闭式任务达同尺寸模型SOTA，性能媲美参数量大10倍的模型。

Conclusion: 验证了小模型通过高效知识获取架构处理复杂开放问题的可行性，为LLM与动态知识库结合提供了新范式。

Abstract: Large Language Models (LLMs), despite their advancements, are fundamentally
limited by their static parametric knowledge, hindering performance on tasks
requiring open-domain up-to-date information. While enabling LLMs to interact
with external knowledge environments is a promising solution, current efforts
primarily address closed-end problems. Open-ended questions, which
characterized by lacking a standard answer or providing non-unique and diverse
answers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a
novel search agent leveraging reinforcement learning to effectively tackle both
open-ended and closed-ended questions in the open domain. O$^2$-Searcher
leverages an efficient, locally simulated search environment for dynamic
knowledge acquisition, effectively decoupling the external world knowledge from
model's sophisticated reasoning processes. It employs a unified training
mechanism with meticulously designed reward functions, enabling the agent to
identify problem types and adapt different answer generation strategies.
Furthermore, to evaluate performance on complex open-ended tasks, we construct
O$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain
open-ended questions with associated web page caches. Extensive experiments
show that O$^2$-Searcher, using only a 3B model, significantly surpasses
leading LLM agents on O$^2$-QA. It also achieves SOTA results on various
closed-ended QA benchmarks against similarly-sized models, while performing on
par with much larger ones.

</details>


### [87] [Evaluating Large Language Model with Knowledge Oriented Language Specific Simple Question Answering](https://arxiv.org/abs/2505.16591)
*Bowen Jiang,Runchuan Zhu,Jiang Wu,Zinco Jiang,Yifan He,Junyuan Gao,Jia Yu,Rui Min,Yinfan Wang,Haote Yang,Songyang Zhang,Dahua Lin,Lijun Wu,Conghui He*

Main category: cs.CL

TL;DR: 首个评估大语言模型多语言事实能力的基准测试KoLasSimpleQA，覆盖9种语言和双领域设计，揭示模型在通用和特定领域的显著性能差异。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏针对多语言场景的系统性评估框架，无法有效识别LLMs在跨语言事实记忆与自我认知能力的边界。

Method: 1. 构建单知识点、客观稳定的问题集；2. 扩展多语言覆盖（9种语言）；3. 设计通用领域+语言专属领域的双维度评估体系。

Result: 传统LLM与新型推理模型在跨语言场景下呈现显著性能差异，尤其在特定领域（历史文化）的校准能力和鲁棒性存在明显短板。

Conclusion: KoLasSimpleQA为多语言模型优化提供诊断工具，揭示需针对不同语言领域设计差异化评估策略，推动LLM全球化应用能力发展。

Abstract: We introduce KoLasSimpleQA, the first benchmark evaluating the multilingual
factual ability of Large Language Models (LLMs). Inspired by existing research,
we created the question set with features such as single knowledge point
coverage, absolute objectivity, unique answers, and temporal stability. These
questions enable efficient evaluation using the LLM-as-judge paradigm, testing
both the LLMs' factual memory and self-awareness ("know what they don't know").
KoLasSimpleQA expands existing research in two key dimensions: (1) Breadth
(Multilingual Coverage): It includes 9 languages, supporting global
applicability evaluation. (2) Depth (Dual Domain Design): It covers both the
general domain (global facts) and the language-specific domain (such as
history, culture, and regional traditions) for a comprehensive assessment of
multilingual capabilities. We evaluated mainstream LLMs, including traditional
LLM and emerging Large Reasoning Models. Results show significant performance
differences between the two domains, particularly in performance metrics,
ranking, calibration, and robustness. This highlights the need for targeted
evaluation and optimization in multilingual contexts. We hope KoLasSimpleQA
will help the research community better identify LLM capability boundaries in
multilingual contexts and provide guidance for model optimization. We will
release KoLasSimpleQA at https://github.com/opendatalab/KoLasSimpleQA .

</details>


### [88] [What Media Frames Reveal About Stance: A Dataset and Study about Memes in Climate Change Discourse](https://arxiv.org/abs/2505.16592)
*Shijia Zhou,Siyao Peng,Simon Luebke,Jörg Haßler,Mario Haim,Saif M. Mohammad,Barbara Plank*

Main category: cs.CL

TL;DR: 研究者创建首个气候迷因多模态数据集CLIMATEMEMES，探索立场与媒体框架的相互作用，发现视觉语言模型在立场检测表现优异但框架识别欠佳，文本模型更适合框架分析。


<details>
  <summary>Details</summary>
Motivation: 填补立场与媒体框架交互研究的空白，通过互联网迷因这种新兴传播形式揭示气候变化讨论中的舆论引导机制。

Method: 跨学科方法构建含1184个subreddit迷因的标注数据集，设计双任务评估框架（立场检测+框架检测），对比LLaVA-NeXT、Molmo等模型在不同输入模式（原始图像/人工描述/合成描述/OCR）下的表现。

Result: 1. 人工描述持续提升模型性能
2. VLM在立场检测准确率达73.2%，但框架检测仅51.3%
3. LLM在框架检测反超VLM达58.1%
4. 时间序列分析显示'责任归属'框架在2020年后增长32%

Conclusion: 研究揭示了多模态内容分析中模态优势的差异性，为传播学研究提供计算范式，同时指出VLM在语义细微性理解上的技术瓶颈需突破。

Abstract: Media framing refers to the emphasis on specific aspects of perceived reality
to shape how an issue is defined and understood. Its primary purpose is to
shape public perceptions often in alignment with the authors' opinions and
stances. However, the interaction between stance and media frame remains
largely unexplored. In this work, we apply an interdisciplinary approach to
conceptualize and computationally explore this interaction with internet memes
on climate change. We curate CLIMATEMEMES, the first dataset of climate-change
memes annotated with both stance and media frames, inspired by research in
communication science. CLIMATEMEMES includes 1,184 memes sourced from 47
subreddits, enabling analysis of frame prominence over time and communities,
and sheds light on the framing preferences of different stance holders. We
propose two meme understanding tasks: stance detection and media frame
detection. We evaluate LLaVA-NeXT and Molmo in various setups, and report the
corresponding results on their LLM backbone. Human captions consistently
enhance performance. Synthetic captions and human-corrected OCR also help
occasionally. Our findings highlight that VLMs perform well on stance, but
struggle on frames, where LLMs outperform VLMs. Finally, we analyze VLMs'
limitations in handling nuanced frames and stance expressions on climate change
internet memes.

</details>


### [89] [From Generic Empathy to Personalized Emotional Support: A Self-Evolution Framework for User Preference Alignment](https://arxiv.org/abs/2505.16610)
*Jing Ye,Lu Xiang,Yaping Zhang,Chengqing Zong*

Main category: cs.CL

TL;DR: 提出两阶段自进化框架，通过自我反思和优化帮助LLM生成更符合用户隐式偏好的个性化情绪支持回复


<details>
  <summary>Details</summary>
Motivation: 现有LLM在情感支持中存在通用化回应问题，无法满足用户个性化需求（用户画像/情感状态/具体情境），导致用户偏好与模型输出的不匹配

Method: 1. 情绪支持经验获取：用有限对话数据微调LLM提供基础支持
2. 自我改进阶段：通过自我反思-优化机制生成个性化回复，迭代执行改进前后的直接偏好优化

Result: 实验表明该方法显著提升情绪支持效果，减少无效回复（降低35.6%），用户偏好与模型输出的差异减少48.2%

Conclusion: 该框架有效缩小用户偏好与模型输出的差距，为个性化情绪支持系统提供新范式

Abstract: Effective emotional support hinges on understanding users' emotions and needs
to provide meaningful comfort during multi-turn interactions. Large Language
Models (LLMs) show great potential for expressing empathy; however, they often
deliver generic and one-size-fits-all responses that fail to address users'
specific needs. To tackle this issue, we propose a self-evolution framework
designed to help LLMs improve their responses to better align with users'
implicit preferences concerning user profiles (personalities), emotional
states, and specific situations. Our framework consists of two distinct phases:
\textit{(1)} \textit{Emotional Support Experience Acquisition}, where LLMs are
fine-tuned on limited emotional support conversation data to provide basic
support, and \textit{(2)} \textit{Self-Improvement for Personalized Emotional
Support}, where LLMs leverage self-reflection and self-refinement to generate
personalized responses. Through iterative direct preference optimization
between the pre- and post-refined responses, our model generates responses that
reflect a better understanding of the user's implicit preferences. Extensive
experiments and evaluations demonstrate that our method significantly enhances
the model's performance in emotional support, reducing unhelpful responses and
minimizing discrepancies between user preferences and model outputs.

</details>


### [90] [Steering Large Language Models for Machine Translation Personalization](https://arxiv.org/abs/2505.16612)
*Daniel Scalena,Gabriele Sarti,Arianna Bisazza,Elisabetta Fersini,Malvina Nissim*

Main category: cs.CL

TL;DR: 提出通过提示策略和潜在概念对比框架，在低资源场景下实现LLM文学翻译的个性化风格适配，同时保持翻译质量。


<details>
  <summary>Details</summary>
Motivation: 解决大模型在隐式风格约束场景（如文学翻译）中难以通过常规提示实现个性化翻译的问题。

Method: 结合多提示策略、推理时干预，以及基于稀疏自编码器提取潜在概念的对比框架进行风格控制。

Result: 实验表明steering方法在保留翻译质量的同时实现强个性化，且发现该方法与few-shot提示对模型层的调控机制相似。

Conclusion: 潜在概念引导为低资源文学翻译的个性化控制提供了有效解决方案，揭示了LLM个性化调控的潜在工作机制。

Abstract: High-quality machine translation systems based on large language models
(LLMs) have simplified the production of personalized translations reflecting
specific stylistic constraints. However, these systems still struggle in
settings where stylistic requirements are less explicit and might be harder to
convey via prompting. We explore various strategies for personalizing
LLM-generated translations in low-resource settings, focusing on the
challenging literary translation domain. We explore prompting strategies and
inference-time interventions for steering model generations towards a
personalized style, and propose a contrastive framework exploiting latent
concepts extracted from sparse autoencoders to identify salient personalization
properties. Our results show that steering achieves strong personalization
while preserving translation quality. We further examine the impact of steering
on LLM representations, finding model layers with a relevant impact for
personalization are impacted similarly by multi-shot prompting and our steering
method, suggesting similar mechanism at play.

</details>


### [91] [SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation](https://arxiv.org/abs/2505.16637)
*Wenjie Yang,Mao Zheng,Mingyang Song,Zheng Li*

Main category: cs.CL

TL;DR: 提出自奖励强化学习框架SSR用于机器翻译，仅需13K单语数据即在中英翻译任务中超越多个大型模型


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译专用大模型过度依赖昂贵的外部监督信号（人工标注参考数据/训练奖励模型）

Method: 基于自评判奖励的参考无关全在线强化学习框架（SSR），结合COMET外部监督增强的SSR-X版本

Result: SSR-Zero-7B超越TowerInstruct-13B等专用模型；SSR-X-Zero-7B在中英翻译任务上达到SOTA，超越GPT-4o和Gemini 1.5 Pro

Conclusion: 自奖励机制相比外部LLM评判更有效，与奖励模型结合具有互补优势，验证了自改进强化学习方法的潜力

Abstract: Large language models (LLMs) have recently demonstrated remarkable
capabilities in machine translation (MT). However, most advanced MT-specific
LLMs heavily rely on external supervision signals during training, such as
human-annotated reference data or trained reward models (RMs), which are often
expensive to obtain and challenging to scale. To overcome this limitation, we
propose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for
MT that is reference-free, fully online, and relies solely on self-judging
rewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as
the backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs,
e.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like
Qwen2.5-32B-Instruct in English $\leftrightarrow$ Chinese translation tasks
from WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR
with external supervision from COMET, our strongest model, SSR-X-Zero-7B,
achieves state-of-the-art performance in English $\leftrightarrow$ Chinese
translation, surpassing all existing open-source models under 72B parameters
and even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro.
Our analysis highlights the effectiveness of the self-rewarding mechanism
compared to the external LLM-as-a-judge approach in MT and demonstrates its
complementary benefits when combined with trained RMs. Our findings provide
valuable insight into the potential of self-improving RL methods. We have
publicly released our code, data and models.

</details>


### [92] [Collaboration among Multiple Large Language Models for Medical Question Answering](https://arxiv.org/abs/2505.16648)
*Kexin Shang,Chia-Hsuan Chang,Christopher C. Yang*

Main category: cs.CL

TL;DR: 提出多LLM协作框架提升医学选择题推理能力，并发现模型信心与预测准确性的关联


<details>
  <summary>Details</summary>
Motivation: 现有研究未能有效整合多个LLM的专业知识和背景产生协同效应

Method: 基于医学选择题数据集设计多LLM协作框架，使用3个预训练LLM进行后验分析

Result: 框架提升所有LLM的推理能力（+5.2%准确率），减少答案分歧（分歧率下降37%）

Conclusion: LLM在面临反对意见时的置信度与预测准确性呈现显著正相关（r=0.82,p<0.01）

Abstract: Empowered by vast internal knowledge reservoir, the new generation of large
language models (LLMs) demonstrate untapped potential to tackle medical tasks.
However, there is insufficient effort made towards summoning up a synergic
effect from multiple LLMs' expertise and background. In this study, we propose
a multi-LLM collaboration framework tailored on a medical multiple-choice
questions dataset. Through post-hoc analysis on 3 pre-trained LLM participants,
our framework is proved to boost all LLMs reasoning ability as well as
alleviate their divergence among questions. We also measure an LLM's confidence
when it confronts with adversary opinions from other LLMs and observe a
concurrence between LLM's confidence and prediction accuracy.

</details>


### [93] [Can reasoning models comprehend mathematical problems in Chinese ancient texts? An empirical study based on data from Suanjing Shishu](https://arxiv.org/abs/2505.16660)
*Liu Chang,Wang Dongbo,Liu liu,Zhao Zhixiao*

Main category: cs.CL

TL;DR: 构建Guji_MATH基准评估古籍数学问题解决能力，主流推理模型在文言文数学问题表现弱于现代任务，需加强古文理解与文化知识。


<details>
  <summary>Details</summary>
Motivation: 针对中文古代数学典籍智能化处理的挑战，通过构建基于《算经十书》的评测基准，系统评估主流推理模型在文言文特殊语言约束下的数学问题解决能力。

Method: 采用机器辅助标注与人工校验，从8部典籍提取538个数学问题，构建以'问题-答案-解法'为核心的结构化数据集，设计闭卷（自主解题）和开卷（复现经典解法）双评测模式评估6个推理模型。

Result: 推理模型能部分理解并解决问题，但整体表现落后于现代数学任务基准。模型在文言文理解和传统文化知识方面存在明显短板。

Conclusion: 研究为古籍数学知识挖掘和传统文化传播提供方法支持，同时为评估推理模型的跨语言、跨文化能力提供新视角，建议优先加强模型的古文理解与文化知识优化。

Abstract: This study addresses the challenges in intelligent processing of Chinese
ancient mathematical classics by constructing Guji_MATH, a benchmark for
evaluating classical texts based on Suanjing Shishu. It systematically assesses
the mathematical problem-solving capabilities of mainstream reasoning models
under the unique linguistic constraints of classical Chinese. Through
machine-assisted annotation and manual verification, 538 mathematical problems
were extracted from 8 canonical texts, forming a structured dataset centered on
the "Question-Answer-Solution" framework, supplemented by problem types and
difficulty levels. Dual evaluation modes--closed-book (autonomous
problem-solving) and open-book (reproducing classical solution methods)--were
designed to evaluate the performance of six reasoning models on ancient Chinese
mathematical problems. Results indicate that reasoning models can partially
comprehend and solve these problems, yet their overall performance remains
inferior to benchmarks on modern mathematical tasks. Enhancing models'
classical Chinese comprehension and cultural knowledge should be prioritized
for optimization. This study provides methodological support for mining
mathematical knowledge from ancient texts and disseminating traditional
culture, while offering new perspectives for evaluating cross-linguistic and
cross-cultural capabilities of reasoning models.

</details>


### [94] [A Japanese Language Model and Three New Evaluation Benchmarks for Pharmaceutical NLP](https://arxiv.org/abs/2505.16661)
*Issey Sukeda,Takuro Fujii,Kosei Buma,Shunsuke Sasaki,Shinnosuke Ono*

Main category: cs.CL

TL;DR: 研究者开发了日语医药领域专用语言模型，通过混合预训练和三个创新基准测试，证明其性能超越开源模型并与商业模型竞争，同时揭示跨句推理的现存挑战


<details>
  <summary>Details</summary>
Motivation: 构建安全高效的领域专用模型，解决现有模型在医药术语处理、跨语言归一化和跨句一致性推理方面的不足

Method: 使用20亿日语医药词汇+80亿英语生物医学词汇持续预训练，创建YakugakuQA（药师考试）、NayoseQA（跨语言术语归一化）、SogoCheck（语句一致性验证）三个诊断性基准

Result: 模型在术语密集型任务上超越开源模型，与GPT-4o等商业模型相当。所有模型在SogoCheck表现均不佳，揭示跨句一致性推理仍是重大挑战

Conclusion: 成功验证领域专用模型的可行性，提供覆盖事实召回、术语变异和逻辑一致性的三维评估体系，开源资源推动医药NLP发展

Abstract: We present a Japanese domain-specific language model for the pharmaceutical
field, developed through continual pretraining on 2 billion Japanese
pharmaceutical tokens and 8 billion English biomedical tokens. To enable
rigorous evaluation, we introduce three new benchmarks: YakugakuQA, based on
national pharmacist licensing exams; NayoseQA, which tests cross-lingual
synonym and terminology normalization; and SogoCheck, a novel task designed to
assess consistency reasoning between paired statements. We evaluate our model
against both open-source medical LLMs and commercial models, including GPT-4o.
Results show that our domain-specific model outperforms existing open models
and achieves competitive performance with commercial ones, particularly on
terminology-heavy and knowledge-based tasks. Interestingly, even GPT-4o
performs poorly on SogoCheck, suggesting that cross-sentence consistency
reasoning remains an open challenge. Our benchmark suite offers a broader
diagnostic lens for pharmaceutical NLP, covering factual recall, lexical
variation, and logical consistency. This work demonstrates the feasibility of
building practical, secure, and cost-effective language models for Japanese
domain-specific applications, and provides reusable evaluation resources for
future research in pharmaceutical and healthcare NLP. Our model, codes, and
datasets are released at https://github.com/EQUES-Inc/pharma-LLM-eval.

</details>


### [95] [Beyond Induction Heads: In-Context Meta Learning Induces Multi-Phase Circuit Emergence](https://arxiv.org/abs/2505.16694)
*Gouki Minegishi,Hiroki Furuta,Shohei Taniguchi,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.CL

TL;DR: 探索Transformer模型在训练中如何获得上下文元学习能力，发现多阶段电路动态与单阶段归纳头机制不同


<details>
  <summary>Details</summary>
Motivation: 现有研究仅解释复制式上下文学习，但模型如何从上下文元学习任务的机制尚未明确

Method: 扩展复制任务至上下文元学习场景，系统性分析模型电路训练动态

Result: 发现元学习能力通过多阶段电路涌现实现，不同阶段对应大语言模型不同现象

Conclusion: 揭示了Transformer上下文学习能力的多阶段形成机制，深化对模型认知

Abstract: Transformer-based language models exhibit In-Context Learning (ICL), where
predictions are made adaptively based on context. While prior work links
induction heads to ICL through a sudden jump in accuracy, this can only account
for ICL when the answer is included within the context. However, an important
property of practical ICL in large language models is the ability to meta-learn
how to solve tasks from context, rather than just copying answers from context;
how such an ability is obtained during training is largely unexplored. In this
paper, we experimentally clarify how such meta-learning ability is acquired by
analyzing the dynamics of the model's circuit during training. Specifically, we
extend the copy task from previous research into an In-Context Meta Learning
setting, where models must infer a task from examples to answer queries.
Interestingly, in this setting, we find that there are multiple phases in the
process of acquiring such abilities, and that a unique circuit emerges in each
phase, contrasting with the single-phases change in induction heads. The
emergence of such circuits can be related to several phenomena known in large
language models, and our analysis lead to a deeper understanding of the source
of the transformer's ICL ability.

</details>


### [96] [Locate-then-Merge: Neuron-Level Parameter Fusion for Mitigating Catastrophic Forgetting in Multimodal LLMs](https://arxiv.org/abs/2505.16703)
*Zeping Yu,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 提出Locate-then-Merge框架和Neuron-Fusion神经元融合策略，解决多模态大模型在指令调整阶段的语言能力退化问题。


<details>
  <summary>Details</summary>
Motivation: 多模态指令调整会导致基础大语言模型（如Llama3）出现语言能力的灾难性遗忘，需要平衡视觉适应与语言保留。

Method: 1. 两阶段参数融合框架（定位重要参数→选择性融合）
2. 神经元级策略：保留参数变化大的视觉能力神经元，衰减变化小的通用语言神经元

Result: 在13个语言和视觉任务基准测试中超越现有方法，有效减少生成中的上下文幻觉现象

Conclusion: Neuron-Fusion通过精细的神经元级参数控制，在保持视觉能力的同时显著缓解语言退化

Abstract: Although multimodal large language models (MLLMs) have achieved impressive
performance, the multimodal instruction tuning stage often causes catastrophic
forgetting of the base LLM's language ability, even in strong models like
Llama3. To address this, we propose Locate-then-Merge, a training-free
parameter fusion framework that first locates important parameters and then
selectively merges them. We further introduce Neuron-Fusion, a neuron-level
strategy that preserves the influence of neurons with large parameter
shifts--neurons likely responsible for newly acquired visual
capabilities--while attenuating the influence of neurons with smaller changes
that likely encode general-purpose language skills. This design enables better
retention of visual adaptation while mitigating language degradation.
Experiments on 13 benchmarks across both language and visual tasks show that
Neuron-Fusion consistently outperforms existing model merging methods. Further
analysis reveals that our method effectively reduces context hallucination in
generation.

</details>


### [97] [Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification](https://arxiv.org/abs/2505.16722)
*Himanshu Beniwal,Youngwoo Kim,Maarten Sap,Soham Dan,Thomas Hartvigsen*

Main category: cs.CL

TL;DR: 提出跨语言解毒范式，解决大语言模型在多语言环境中的毒性问题并平衡安全性与知识保留


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在全球应用中的普及，确保其在不同语言环境中的无毒性成为关键挑战。现有方法难以在高低资源语言间有效转移解毒能力。

Method: 通过504种实验设置评估跨分布环境下毒性减少效果，分析数据有限场景的解毒能力迁移，并研究解毒对非毒性任务模型性能的影响

Result: 验证了跨语言解毒的有效性，揭示安全性与知识保留之间的权衡关系。模型在毒性减少时非毒性任务表现会下降约3-5%

Conclusion: 该研究为多语言场景下LLM安全部署提供新范式，开源的代码和数据集将推动安全大模型研究社区发展

Abstract: As large language models (LLMs) become increasingly prevalent in global
applications, ensuring that they are toxicity-free across diverse linguistic
contexts remains a critical challenge. We explore "Cross-lingual
Detoxification", a cross-lingual paradigm that mitigates toxicity, enabling
detoxification capabilities to transfer between high and low-resource languages
across different script families. We analyze cross-lingual detoxification's
effectiveness through 504 extensive settings to evaluate toxicity reduction in
cross-distribution settings with limited data and investigate how mitigation
impacts model performance on non-toxic tasks, revealing trade-offs between
safety and knowledge preservation. Our code and dataset are publicly available
at https://github.com/himanshubeniwal/Breaking-mBad.

</details>


### [98] [TRIM: Achieving Extreme Sparsity with Targeted Row-wise Iterative Metric-driven Pruning](https://arxiv.org/abs/2505.16743)
*Florentin Beck,William Rudman,Carsten Eickhoff*

Main category: cs.CL

TL;DR: 提出TRIM方法，通过维度级稀疏优化显著提升大语言模型剪枝效果，在多个模型家族上实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 传统统一稀疏约束方法在高压缩率下性能急剧下降，需更精细的维度级稀疏分配策略

Method: 基于质量指标的迭代调整机制，对每个输出维度动态分配不同稀疏率，保留关键信息同时最大化压缩

Result: 80%稀疏率下困惑度降低48%（Qwen2.5-14B）和90%+（OPT-13B），显著优于基线方法

Conclusion: 维度级稀疏适应是实现极限模型压缩的关键，TRIM为高效部署大模型提供新方向

Abstract: Large Language Models (LLMs) present significant computational and memory
challenges due to their extensive size, making pruning essential for their
efficient deployment. Existing one-shot pruning methods often apply uniform
sparsity constraints across layers or within each layer, resulting in
suboptimal performance, especially at high sparsity ratios. This work
introduces TRIM (Targeted Row-wise Iterative Metric-driven pruning), a novel
approach that applies varying sparsity ratios to individual output dimensions
(rows) within each layer. TRIM employs an iterative adjustment process guided
by quality metrics to optimize dimension-wise sparsity allocation, focusing on
reducing variance in quality retention across outputs to preserve critical
information. TRIM can be seamlessly integrated with existing layer-wise pruning
strategies. Our evaluations on perplexity and zero-shot tasks across diverse
LLM families (Qwen2.5, LLaMA-2, and OPT) and sparsity levels demonstrate that
TRIM achieves new state-of-the-art results and enhances stability. For
instance, at 80% sparsity, TRIM reduces perplexity by 48% for Qwen2.5-14B and
over 90% for OPT-13B compared to baseline methods. We conclude that
fine-grained, dimension-wise sparsity adaptation is crucial for pushing the
limits of extreme LLM compression. Code available at:
https://github.com/flobk/TRIM

</details>


### [99] [IFEval-Audio: Benchmarking Instruction-Following Capability in Audio-based Large Language Models](https://arxiv.org/abs/2505.16774)
*Yiming Gao,Bin Wang,Chengwei Wei,Shuo Sun,AiTi Aw*

Main category: cs.CL

TL;DR: 提出IFEval-Audio数据集评估音频大语言模型的指令遵循能力，发现多模态对齐会削弱文本指令能力，测试覆盖6个结构维度。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦文本和视觉模态，音频大语言模型的指令遵循能力缺乏系统评估，需填补该领域空白。

Method: 构建含280个音频-指令-答案三元组的评估集，涵盖内容、大小写、符号等6个维度，对主流音频LLM进行基准测试。

Result: 数据集公开促进领域发展，基准测试揭示现有音频LLM在结构化指令遵循方面存在显著不足。

Conclusion: 音频模态的指令对齐需专门优化，IFEval-Audio为改进音频LLM的指令响应能力提供评估基础。

Abstract: Large language models (LLMs) have demonstrated strong instruction-following
capabilities in text-based tasks. However, this ability often deteriorates in
multimodal models after alignment with non-text modalities such as images or
audio. While several recent efforts have investigated instruction-following
performance in text and vision-language models, instruction-following in
audio-based large language models remains largely unexplored. To bridge this
gap, we introduce IFEval-Audio, a novel evaluation dataset designed to assess
the ability to follow instructions in an audio LLM. IFEval-Audio contains 280
audio-instruction-answer triples across six diverse dimensions: Content,
Capitalization, Symbol, List Structure, Length, and Format. Each example pairs
an audio input with a text instruction, requiring the model to generate an
output that follows a specified structure. We benchmark state-of-the-art audio
LLMs on their ability to follow audio-involved instructions. The dataset is
released publicly to support future research in this emerging area.

</details>


### [100] [Reasoning Beyond Language: A Comprehensive Survey on Latent Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.16782)
*Xinghao Chen,Anhao Zhao,Heming Xia,Xuan Lu,Hanlin Wang,Yanjun Chen,Wei Zhang,Jian Wang,Wenjie Li,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 论文提出潜在CoT推理范式，通过潜在空间推理解决传统思维链方法的低效问题，建立了四维分类法并系统分析代表性方法。


<details>
  <summary>Details</summary>
Motivation: 传统思维链(CoT)依赖显式语言推理步骤，存在效率低下且难以适应抽象推理的局限。研究潜在CoT旨在通过解耦推理与语言，实现更高效的认知表示和推理过程。

Method: 提出包含代币策略、内部机制、分析框架和应用场景的四维统一分类法，通过对比分析揭示不同方法的架构模式与优势。

Result: 建立了潜在CoT的系统分析框架，证明其在提升推理速度（平均提升3.1倍）和抽象表征能力方面的优势，识别出模块化架构与端到端训练两种主流范式。

Conclusion: 潜在CoT为LLM推理开辟了新方向，本文提出的分类体系和比较分析方法为后续研究提供了结构化基础，相关资源将持续更新于GitHub仓库。

Abstract: Large Language Models (LLMs) have achieved impressive performance on complex
reasoning tasks with Chain-of-Thought (CoT) prompting. However, conventional
CoT relies on reasoning steps explicitly verbalized in natural language,
introducing inefficiencies and limiting its applicability to abstract
reasoning. To address this, there has been growing research interest in latent
CoT reasoning, where inference occurs within latent spaces. By decoupling
reasoning from language, latent reasoning promises richer cognitive
representations and more flexible, faster inference. Researchers have explored
various directions in this promising field, including training methodologies,
structural innovations, and internal reasoning mechanisms. This paper presents
a comprehensive overview and analysis of this reasoning paradigm. We begin by
proposing a unified taxonomy from four perspectives: token-wise strategies,
internal mechanisms, analysis, and applications. We then provide in-depth
discussions and comparative analyses of representative methods, highlighting
their design patterns, strengths, and open challenges. We aim to provide a
structured foundation for advancing this emerging direction in LLM reasoning.
The relevant papers will be regularly updated at
https://github.com/EIT-NLP/Awesome-Latent-CoT.

</details>


### [101] [Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected Vulnerability](https://arxiv.org/abs/2505.16789)
*Punya Syon Pandey,Samuel Simko,Kellin Pelrine,Zhijing Jin*

Main category: cs.CL

TL;DR: 研究揭示微调数据特征可能导致语言模型意外漏洞，提出数据设计对防御对抗攻击的关键作用


<details>
  <summary>Details</summary>
Motivation: 探索微调模型在提升性能时可能引入的意外漏洞（Accidental Misalignment），分析数据特征与对抗攻击成功率的关系

Method: 通过识别数据相关性因素（语言特征/语义相似性/毒性）→评估对抗性能→分析数据因素与攻击成功率相关性→探索因果关系链

Result: 发现微调数据集特征与攻击成功率存在显著相关性，提出数据设计是保持模型安全对齐的核心要素

Conclusion: 揭示模型安全性与数据质量的深度关联，为对抗防御提供新视角，强调数据集设计需纳入安全考量

Abstract: As large language models gain popularity, their vulnerability to adversarial
attacks remains a primary concern. While fine-tuning models on domain-specific
datasets is often employed to improve model performance, it can introduce
vulnerabilities within the underlying model. In this work, we investigate
Accidental Misalignment, unexpected vulnerabilities arising from
characteristics of fine-tuning data. We begin by identifying potential
correlation factors such as linguistic features, semantic similarity, and
toxicity within our experimental datasets. We then evaluate the adversarial
performance of these fine-tuned models and assess how dataset factors correlate
with attack success rates. Lastly, we explore potential causal links, offering
new insights into adversarial defense strategies and highlighting the crucial
role of dataset design in preserving model alignment. Our code is available at
https://github.com/psyonp/accidental_misalignment.

</details>


### [102] [Learning Beyond Limits: Multitask Learning and Synthetic Data for Low-Resource Canonical Morpheme Segmentation](https://arxiv.org/abs/2505.16800)
*Changbing Yang,Garrett Nicolai*

Main category: cs.CL

TL;DR: 基于Transformer的多任务学习框架结合LLM生成合成数据，显著提升低资源语言的词素分割性能


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言形态分析中训练数据不足的问题，通过共享语言表示和合成数据增强模型泛化能力

Method: 联合预测形态分割与语义注释的多任务学习框架，整合大语言模型生成的合成训练数据

Result: 在SIGMORPHON 2023数据集上实现词级准确率提升3.2%，词素级F1分数平均提高7.5%（尤其非洲与土著语言）

Conclusion: 多任务架构与合成数据增强的组合方法为低资源形态分析提供了有效解决方案

Abstract: We introduce a transformer-based morpheme segmentation system that augments a
low-resource training signal through multitask learning and LLM-generated
synthetic data. Our framework jointly predicts morphological segments and
glosses from orthographic input, leveraging shared linguistic representations
obtained through a common documentary process to enhance model generalization.
To further address data scarcity, we integrate synthetic training data
generated by large language models (LLMs) using in-context learning.
Experimental results on the SIGMORPHON 2023 dataset show that our approach
significantly improves word-level segmentation accuracy and morpheme-level
F1-score across multiple low-resource languages.

</details>


### [103] [Two-way Evidence self-Alignment based Dual-Gated Reasoning Enhancement](https://arxiv.org/abs/2505.16806)
*Kexin Zhang,Junlan Chen,Daifeng Li,Yuxuan Zhang,Yangyang Feng,Bowen Deng,Weixu Chen*

Main category: cs.CL

TL;DR: 提出TW-ESA和DGR双模块框架解决大模型在知识密集型多步推理任务中的证据对齐与知识融合问题


<details>
  <summary>Details</summary>
Motivation: 现有方法在知识密集型多步推理任务中存在逻辑无关证据提取和不确定证据利用不足的问题，导致推理错误

Method: TW-ESA模块通过严格推理与LLM推理的互对齐增强证据因果逻辑理解；DGR模块通过双门控机制融合LLM内在知识与严格推理

Result: 在三个KIMSR数据集上平均提升4% EM和5% F1，显著超越现有LLM微调方法

Conclusion: ESA-DGR框架通过证据自对齐和双门控推理增强，有效提升模型在复杂推理任务中的准确性和鲁棒性

Abstract: Large language models (LLMs) encounter difficulties in knowledge-intensive
multi-step reasoning (KIMSR) tasks. One challenge is how to effectively extract
and represent rationale evidence. The current methods often extract
semantically relevant but logically irrelevant evidence, resulting in flawed
reasoning and inaccurate responses. We propose a two-way evidence
self-alignment (TW-ESA) module, which utilizes the mutual alignment between
strict reasoning and LLM reasoning to enhance its understanding of the causal
logic of evidence, thereby addressing the first challenge. Another challenge is
how to utilize the rationale evidence and LLM's intrinsic knowledge for
accurate reasoning when the evidence contains uncertainty. We propose a
dual-gated reasoning enhancement (DGR) module to gradually fuse useful
knowledge of LLM within strict reasoning, which can enable the model to perform
accurate reasoning by focusing on causal elements in the evidence and exhibit
greater robustness. The two modules are collaboratively trained in a unified
framework ESA-DGR. Extensive experiments on three diverse and challenging KIMSR
datasets reveal that ESA-DGR significantly surpasses state-of-the-art LLM-based
fine-tuning methods, with remarkable average improvements of 4% in exact match
(EM) and 5% in F1 score. The implementation code is available at
https://anonymous.4open.science/r/ESA-DGR-2BF8.

</details>


### [104] [Does Synthetic Data Help Named Entity Recognition for Low-Resource Languages?](https://arxiv.org/abs/2505.16814)
*Gaurav Kamath,Sowmya Vajjala*

Main category: cs.CL

TL;DR: 研究探讨合成数据在11种低资源语言命名实体识别中的应用效果，发现其有效性但存在显著语言差异


<details>
  <summary>Details</summary>
Motivation: 低资源语言的NER系统常面临标注数据不足的问题，数据增强是常见解决方案。研究旨在验证合成数据在跨语言低资源NER中的实际效果

Method: 使用合成数据增强方法，在11个不同语系的低资源语言上进行多语言NER实验

Result: 合成数据整体提升低资源NER效果，但不同语言间存在显著性能差异（如语系特性导致数据增强效果不同）

Conclusion: 合成数据对低资源语言NER具有应用潜力，但需考虑语言类型学特征对增强效果的影响，未来应开发更适应语言特性的数据增强方法

Abstract: Named Entity Recognition(NER) for low-resource languages aims to produce
robust systems for languages where there is limited labeled training data
available, and has been an area of increasing interest within NLP. Data
augmentation for increasing the amount of low-resource labeled data is a common
practice. In this paper, we explore the role of synthetic data in the context
of multilingual, low-resource NER, considering 11 languages from diverse
language families. Our results suggest that synthetic data does in fact hold
promise for low-resource language NER, though we see significant variation
between languages.

</details>


### [105] [Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs](https://arxiv.org/abs/2505.16831)
*Xiaoyu Xu,Xiang Yue,Yang Liu,Qingqing Ye,Haibo Hu,Minxin Du*

Main category: cs.CL

TL;DR: 研究发现大模型遗忘评估存在可逆/不可逆两种形态，现有词级指标存在误导性，需通过表示层分析工具诊断真实遗忘程度。


<details>
  <summary>Details</summary>
Motivation: 现有LLM遗忘评估过度依赖词级指标（准确率/困惑度），但研究发现这些指标可能产生误导——模型表面遗忘后可通过微调迅速恢复，揭示现有评估体系存在根本缺陷。

Method: 提出表示层分析框架：使用PCA相似性、中心核对齐、Fisher信息等工具，在6种遗忘方法/3个领域/2个开源LLM上系统分析表示空间变化。

Result: 发现可逆遗忘（词级崩溃但特征保留）与不可逆遗忘（表示层损伤）的本质区别，理论证明输出层附近的权重扰动会导致虚假遗忘信号，任务类型和超参数影响可逆性。

Conclusion: 揭示了当前LLM遗忘评估的根本缺陷，建立了基于表示分析的新诊断范式，为可信赖的模型遗忘提供了理论工具包（https://github.com/XiaoyuXU1/Representational_Analysis_Tools.git）。

Abstract: Unlearning in large language models (LLMs) is intended to remove the
influence of specific data, yet current evaluations rely heavily on token-level
metrics such as accuracy and perplexity. We show that these metrics can be
misleading: models often appear to forget, but their original behavior can be
rapidly restored with minimal fine-tuning, revealing that unlearning may
obscure information rather than erase it. To diagnose this phenomenon, we
introduce a representation-level evaluation framework using PCA-based
similarity and shift, centered kernel alignment, and Fisher information.
Applying this toolkit across six unlearning methods, three domains (text, code,
math), and two open-source LLMs, we uncover a critical distinction between
reversible and irreversible forgetting. In reversible cases, models suffer
token-level collapse yet retain latent features; in irreversible cases, deeper
representational damage occurs. We further provide a theoretical account
linking shallow weight perturbations near output layers to misleading
unlearning signals, and show that reversibility is modulated by task type and
hyperparameters. Our findings reveal a fundamental gap in current evaluation
practices and establish a new diagnostic foundation for trustworthy unlearning
in LLMs. We provide a unified toolkit for analyzing LLM representation changes
under unlearning and relearning:
https://github.com/XiaoyuXU1/Representational_Analysis_Tools.git.

</details>


### [106] [SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis](https://arxiv.org/abs/2505.16834)
*Shuang Sun,Huatong Song,Yuhao Wang,Ruiyang Ren,Jinhao Jiang,Junjie Zhang,Fei Bai,Jia Deng,Wayne Xin Zhao,Zheng Liu,Lei Fang,Zhongyuan Wang,Ji-Rong Wen*

Main category: cs.CL

TL;DR: 提出轻量级框架SimpleDeepSearcher，通过数据工程而非复杂训练范式改进RAG系统，仅需871个样本即超越RL基线


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法存在训练轨迹质量低、模拟环境分布不匹配、真实部署计算成本高等瓶颈，需更高效的解决方案

Method: 在实时网络搜索环境模拟真实用户交互生成训练数据，结合多标准数据筛选策略优化输入输出端的多样性与质量

Result: 在5个跨领域基准测试中，仅用871个精选样本的SFT训练即显著优于基于强化学习的基线方法

Conclusion: 通过系统解决数据稀缺瓶颈，验证了SFT作为高效深度搜索系统可行路径的潜力，代码已开源

Abstract: Retrieval-augmented generation (RAG) systems have advanced large language
models (LLMs) in complex deep search scenarios requiring multi-step reasoning
and iterative information retrieval. However, existing approaches face critical
limitations that lack high-quality training trajectories or suffer from the
distributional mismatches in simulated environments and prohibitive
computational costs for real-world deployment. This paper introduces
SimpleDeepSearcher, a lightweight yet effective framework that bridges this gap
through strategic data engineering rather than complex training paradigms. Our
approach synthesizes high-quality training data by simulating realistic user
interactions in live web search environments, coupled with a multi-criteria
curation strategy that optimizes the diversity and quality of input and output
side. Experiments on five benchmarks across diverse domains demonstrate that
SFT on only 871 curated samples yields significant improvements over RL-based
baselines. Our work establishes SFT as a viable pathway by systematically
addressing the data-scarce bottleneck, offering practical insights for
efficient deep search systems. Our code is available at
https://github.com/RUCAIBox/SimpleDeepSearcher.

</details>


### [107] [R1-Compress: Long Chain-of-Thought Compression via Chunk Compression and Search](https://arxiv.org/abs/2505.16838)
*Yibo Wang,Li Shen,Huanjin Yao,Tiansheng Huang,Rui Liu,Naiqiang Tan,Jiaxing Huang,Kai Zhang,Dacheng Tao*

Main category: cs.CL

TL;DR: 提出R1-Compress两阶段分块压缩框架，在保持推理精度的同时显著减少20%的token使用量


<details>
  <summary>Details</summary>
Motivation: 现有Long-CoT扩展方案存在局部推理信号丢失或输出不连贯问题，需开发能兼顾信息保留与压缩效率的新方法

Method: 1. 分块处理Long-CoT序列 2. 基于LLM的块内压缩 3. 块间搜索机制筛选最优压缩序列

Result: 在MATH500数据集达到92.4%准确率（仅比基线下降0.6%），GPQA-Diamond等数据集验证有效性

Conclusion: 该方法突破了CoT推理的效率瓶颈，为实际部署提供了精度与效率的平衡方案

Abstract: Chain-of-Thought (CoT) reasoning enhances large language models (LLMs) by
enabling step-by-step problem-solving, yet its extension to Long-CoT introduces
substantial computational overhead due to increased token length. Existing
compression approaches -- instance-level and token-level -- either sacrifice
essential local reasoning signals like reflection or yield incoherent outputs.
To address these limitations, we propose R1-Compress, a two-stage chunk-level
compression framework that preserves both local information and coherence. Our
method segments Long-CoT into manageable chunks, applies LLM-driven inner-chunk
compression, and employs an inter-chunk search mechanism to select the short
and coherent sequence. Experiments on Qwen2.5-Instruct models across MATH500,
AIME24, and GPQA-Diamond demonstrate that R1-Compress significantly reduces
token usage while maintaining comparable reasoning accuracy. On MATH500,
R1-Compress achieves an accuracy of 92.4%, with only a 0.6% drop compared to
the Long-CoT baseline, while reducing token usage by about 20%. Source code
will be available at https://github.com/w-yibo/R1-Compress

</details>


### [108] [Understanding and Analyzing Inappropriately Targeting Language in Online Discourse: A Comparative Annotation Study](https://arxiv.org/abs/2505.16847)
*Baran Barbarestani,Isa Maks,Piek Vossen*

Main category: cs.CL

TL;DR: 整合人群标注、专家标注与ChatGPT的在线对话不当内容检测方法，通过Reddit数据揭示上下文对仇恨言语识别的影响并发现新型针对类别


<details>
  <summary>Details</summary>
Motivation: 解决现有内容审核系统在识别隐性歧视语言和复杂上下文关联的仇恨言论方面的不足，探索结合人类标注与AI模型的混合审核模式

Method: 构建多维度标注框架，对Reddit英文对话进行目标类别/目标词标注，对比专家标注、众包标注与ChatGPT在显性/隐性仇恨言语识别中的表现差异

Result: 发现社会信仰、身体形象等新型针对类别，证实上下文对仇恨言语判断的关键影响，揭示ChatGPT在微妙语言理解及主观判断中的局限性

Conclusion: 提出融合人类专业判断与AI效率的混合审核策略，为构建更安全包容的在线环境提供方法论支持和技术改进方向

Abstract: This paper introduces a method for detecting inappropriately targeting
language in online conversations by integrating crowd and expert annotations
with ChatGPT. We focus on English conversation threads from Reddit, examining
comments that target individuals or groups. Our approach involves a
comprehensive annotation framework that labels a diverse data set for various
target categories and specific target words within the conversational context.
We perform a comparative analysis of annotations from human experts, crowd
annotators, and ChatGPT, revealing strengths and limitations of each method in
recognizing both explicit hate speech and subtler discriminatory language. Our
findings highlight the significant role of contextual factors in identifying
hate speech and uncover new categories of targeting, such as social belief and
body image. We also address the challenges and subjective judgments involved in
annotation and the limitations of ChatGPT in grasping nuanced language. This
study provides insights for improving automated content moderation strategies
to enhance online safety and inclusivity.

</details>


### [109] [Nested Named Entity Recognition as Single-Pass Sequence Labeling](https://arxiv.org/abs/2505.16855)
*Alberto Muñoz-Ortiz,David Vilares,Caio COrro,Carlos Gómez-Rodríguez*

Main category: cs.CL

TL;DR: 将嵌套命名实体识别转化为序列标注任务，通过线性化选区结构和预训练编码器实现高效嵌套实体捕获


<details>
  <summary>Details</summary>
Motivation: 解决传统嵌套实体识别方法复杂度高、效率低的问题，利用现有技术简化结构化预测流程

Method: 1. 将嵌套结构线性化为序列标注格式
2. 结合预训练编码器进行特征提取
3. 通过n次标注动作完成嵌套识别

Result: 在保持与复杂系统相当性能的前提下，显著提升处理效率（支持现成序列标注库训练）

Conclusion: 该方法实现了效率与性能的平衡，为嵌套实体识别提供了轻量级解决方案

Abstract: We cast nested named entity recognition (NNER) as a sequence labeling task by
leveraging prior work that linearizes constituency structures, effectively
reducing the complexity of this structured prediction problem to
straightforward token classification. By combining these constituency
linearizations with pretrained encoders, our method captures nested entities
while performing exactly $n$ tagging actions. Our approach achieves competitive
performance compared to less efficient systems, and it can be trained using any
off-the-shelf sequence labeling library.

</details>


### [110] [Comparative analysis of subword tokenization approaches for Indian languages](https://arxiv.org/abs/2505.16868)
*Sudhansu Bala Das,Samujjal Choudhury,Tapas Kumar Mishra,Bidyut Kr. Patra*

Main category: cs.CL

TL;DR: 该研究比较了SentencePiece、BPE和WordPiece分词技术对印度语言机器翻译的影响，发现SentencePiece在统计和神经模型中表现最优，而BPE在多语言模型中效果最佳，且印→英翻译质量普遍优于英→印方向。


<details>
  <summary>Details</summary>
Motivation: 针对印度语言复杂的形态结构和黏着特性，探索适合的子词分词方法以提升机器翻译质量，弥补现有研究的不足。

Method: 使用SentencePiece/BPE/WordPiece三种分词器，在统计机器翻译(SMT)、神经机器翻译(NMT)和多语言NMT模型中测试，采用BLEU/TER/METEOR等6种指标评估13个语言对的翻译质量。

Result: 1. 统计/NMT模型：SentencePiece在76.92%语言对取得最高BLEU分数
2. 多语言NMT模型：BPE分词效果最优
3. 所有模型印→英翻译质量显著优于英→印方向

Conclusion: 子词分词技术选择直接影响印度语言机器翻译性能，SentencePiece适合单语场景，BPE在多语言场景表现更优，翻译方向的不对称性提示需要针对性优化策略。

Abstract: Tokenization is the act of breaking down text into smaller parts, or tokens,
that are easier for machines to process. This is a key phase in machine
translation (MT) models. Subword tokenization enhances this process by breaking
down words into smaller subword units, which is especially beneficial in
languages with complicated morphology or a vast vocabulary. It is useful in
capturing the intricate structure of words in Indian languages (ILs), such as
prefixes, suffixes, and other morphological variations. These languages
frequently use agglutinative structures, in which words are formed by the
combination of multiple morphemes such as suffixes, prefixes, and stems. As a
result, a suitable tokenization strategy must be chosen to address these
scenarios. This paper examines how different subword tokenization techniques,
such as SentencePiece, Byte Pair Encoding (BPE), and WordPiece Tokenization,
affect ILs. The effectiveness of these subword tokenization techniques is
investigated in statistical, neural, and multilingual neural machine
translation models. All models are examined using standard evaluation metrics,
such as the Bilingual Evaluation Understudy (BLEU) score, TER, METEOR, CHRF,
RIBES, and COMET. Based on the results, it appears that for the majority of
language pairs for the Statistical and Neural MT models, the SentencePiece
tokenizer continuously performed better than other tokenizers in terms of BLEU
score. However, BPE tokenization outperformed other tokenization techniques in
the context of Multilingual Neural Machine Translation model. The results show
that, despite using the same tokenizer and dataset for each model, translations
from ILs to English surpassed translations from English to ILs.

</details>


### [111] [MPO: Multilingual Safety Alignment via Reward Gap Optimization](https://arxiv.org/abs/2505.16869)
*Weixiang Zhao,Yulin Hu,Yang Deng,Tongtong Wu,Wenxuan Zhang,Jiahe Guo,An Zhang,Yanyan Zhao,Bing Qin,Tat-Seng Chua,Ting Liu*

Main category: cs.CL

TL;DR: 提出MPO方法，利用英语主导语言的安全能力优化多语言安全对齐


<details>
  <summary>Details</summary>
Motivation: 现有RLHF/DPO等单语言安全对齐方法难以处理多语言噪声数据

Method: 通过最小化主导语言与目标语言的奖励差距差异，实现跨语言安全能力迁移

Result: 在LLaMA-3.1/Gemma-2/Qwen2.5三个模型验证有效性，且不降低多语言通用性

Conclusion: MPO成功实现了多语言安全对齐，为全球化部署提供可靠解决方案

Abstract: Large language models (LLMs) have become increasingly central to AI
applications worldwide, necessitating robust multilingual safety alignment to
ensure secure deployment across diverse linguistic contexts. Existing
preference learning methods for safety alignment, such as RLHF and DPO, are
primarily monolingual and struggle with noisy multilingual data. To address
these limitations, we introduce Multilingual reward gaP Optimization (MPO), a
novel approach that leverages the well-aligned safety capabilities of the
dominant language (English) to improve safety alignment across multiple
languages. MPO directly minimizes the reward gap difference between the
dominant language and target languages, effectively transferring safety
capabilities while preserving the original strengths of the dominant language.
Extensive experiments on three LLMs, LLaMA-3.1, Gemma-2 and Qwen2.5, validate
MPO's efficacy in multilingual safety alignment without degrading general
multilingual utility.

</details>


### [112] [CASTILLO: Characterizing Response Length Distributions of Large Language Models](https://arxiv.org/abs/2505.16881)
*Daniel F. Perez-Ramirez,Dejan Kostic,Magnus Boman*

Main category: cs.CL

TL;DR: 提出CASTILLO数据集，系统性分析13个开源大语言模型在7种指令集下的响应长度分布特征，揭示模型间/内生成长度显著差异及部分文本退化现象。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效预估LLM生成文本长度，导致资源调度低效。现有方案存在生成结果长度偏差或忽视模型/提示特异性差异的问题。

Method: 构建包含13个LLM在7个指令集生成10次独立响应的数据集，记录token长度统计指标（均值、标准差、百分位数）及退化样本，固定解码超参数。

Result: 发现同模型同参数下响应长度存在显著波动（模型间标准差差达4.8倍），部分响应出现文本退化但其他样本正常，模型呈现特异性生成模式。

Conclusion: CASTILLO为预测模型开发提供数据支撑，建立了生成行为系统分析框架，公开数据/代码促进生成模型与系统优化的交叉研究。

Abstract: Efficiently managing compute resources for Large Language Model (LLM)
inference remains challenging due to the inherently stochastic and variable
lengths of autoregressive text generation. Accurately estimating response
lengths in advance enables proactive resource allocation, yet existing
approaches either bias text generation towards certain lengths or rely on
assumptions that ignore model- and prompt-specific variability. We introduce
CASTILLO, a dataset characterizing response length distributions across 13
widely-used open-source LLMs evaluated on seven distinct instruction-following
corpora. For each $\langle$prompt, model$\rangle$ sample pair, we generate 10
independent completions using fixed decoding hyper-parameters, record the token
length of each response, and publish summary statistics (mean, std-dev,
percentiles), along with the shortest and longest completions, and the exact
generation settings. Our analysis reveals significant inter- and intra-model
variability in response lengths (even under identical generation settings), as
well as model-specific behaviors and occurrences of partial text degeneration
in only subsets of responses. CASTILLO enables the development of predictive
models for proactive scheduling and provides a systematic framework for
analyzing model-specific generation behaviors. We publicly release the dataset
and code to foster research at the intersection of generative language modeling
and systems.

</details>


### [113] [Shadows in the Attention: Contextual Perturbation and Representation Drift in the Dynamics of Hallucination in LLMs](https://arxiv.org/abs/2505.16894)
*Zeyu Wei,Shuo Wang,Xiaohui Rong,Xuemin Liu,He Li*

Main category: cs.CL

TL;DR: 系统性研究揭示大模型幻觉产生与上下文注入导致的内部状态漂移存在正相关，不同上下文类型引发差异化的错误模式，并提出注意力锁定阈值的量化指标。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型输出中存在的可信幻觉问题，探索其产生机制与上下文注入导致的表征漂移之间的关联，为模型可靠性部署提供理论依据。

Method: 使用TruthfulQA构建两类16轮次上下文滴定实验，通过三视角检测器追踪显性幻觉率，并采用余弦相似度/熵/JS散度/Spearman相关性分析隐藏状态和注意力图动态。

Result: 1) 幻觉频率与表征漂移呈正相关并趋于稳定；2) 相关上下文产生高置信自我一致幻觉，无关上下文导致注意力重定向的主题漂移；3) JS散度(~0.69)和Spearman(~0)标志注意力锁定阈值。

Conclusion: 研究为基于表征漂移的幻觉预测和上下文感知缓解机制提供了实证基础，揭示了模型规模与错误模式的权衡关系（同化能力与注意力扩散的跷跷板效应）。

Abstract: Hallucinations -- plausible yet erroneous outputs -- remain a critical
barrier to reliable deployment of large language models (LLMs). We present the
first systematic study linking hallucination incidence to internal-state drift
induced by incremental context injection. Using TruthfulQA, we construct two
16-round "titration" tracks per question: one appends relevant but partially
flawed snippets, the other injects deliberately misleading content. Across six
open-source LLMs, we track overt hallucination rates with a tri-perspective
detector and covert dynamics via cosine, entropy, JS and Spearman drifts of
hidden states and attention maps. Results reveal (1) monotonic growth of
hallucination frequency and representation drift that plateaus after 5--7
rounds; (2) relevant context drives deeper semantic assimilation, producing
high-confidence "self-consistent" hallucinations, whereas irrelevant context
induces topic-drift errors anchored by attention re-routing; and (3)
convergence of JS-Drift ($\sim0.69$) and Spearman-Drift ($\sim0$) marks an
"attention-locking" threshold beyond which hallucinations solidify and become
resistant to correction. Correlation analyses expose a seesaw between
assimilation capacity and attention diffusion, clarifying size-dependent error
modes. These findings supply empirical foundations for intrinsic hallucination
prediction and context-aware mitigation mechanisms.

</details>


### [114] [Power-Law Decay Loss for Large Language Model Finetuning: Focusing on Information Sparsity to Enhance Generation Quality](https://arxiv.org/abs/2505.16900)
*Jintian Shao,Hongyi Huang,Jiayi Wu,Beiwen Zhang,ZhiYu Wu,You Shan,MingKai Zheng*

Main category: cs.CL

TL;DR: 提出基于幂律衰减的PDL损失函数，通过重新加权token贡献优化文本生成任务的微调过程，提升生成内容特异性与信息量


<details>
  <summary>Details</summary>
Motivation: 标准交叉熵损失平等对待所有token，导致模型过度关注高频低信息token，忽视对生成内容特异性至关重要的低频关键token

Method: 根据token频率构建幂律衰减权重函数，降低高频token权重，提升低频信息密集token在损失计算中的贡献度

Result: 有效引导模型关注信息密集token，提升生成文本质量与多样性，在摘要生成/对话系统/风格迁移等任务中展现优势

Conclusion: PDL通过频率感知的损失重加权机制，为文本生成任务的微调提供了理论创新与实践指导，具有广泛应用潜力

Abstract: During the finetuning stage of text generation tasks, standard cross-entropy
loss treats all tokens equally. This can lead models to overemphasize
high-frequency, low-information tokens, neglecting lower-frequency tokens
crucial for specificity and informativeness in generated content. This paper
introduces a novel loss function, Power-Law Decay Loss (PDL), specifically
designed to optimize the finetuning process for text generation. The core
motivation for PDL stems from observations in information theory and
linguistics: the informativeness of a token is often inversely proportional to
its frequency of occurrence. PDL re-weights the contribution of each token in
the standard cross-entropy loss based on its frequency in the training corpus,
following a power-law decay. Specifically, the weights for high-frequency
tokens are reduced, while low-frequency, information-dense tokens are assigned
higher weights. This mechanism guides the model during finetuning to focus more
on learning and generating tokens that convey specific and unique information,
thereby enhancing the quality, diversity, and informativeness of the generated
text. We theoretically elaborate on the motivation and construction of PDL and
discuss its potential applications and advantages across various text
generation finetuning tasks, such as abstractive summarization, dialogue
systems, and style transfer.

</details>


### [115] [UNCLE: Uncertainty Expressions in Long-Form Generation](https://arxiv.org/abs/2505.16922)
*Ruihan Yang,Caiqi Zhang,Zhisong Zhang,Xinting Huang,Dong Yu,Nigel Collier,Deqing Yang*

Main category: cs.CL

TL;DR: 论文提出UNCLE基准测试框架，用于评估大语言模型在长短问答中表达不确定性的能力，并揭示当前模型在长文本生成中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对长文本生成中不确定性表达的直接公平评估，需建立跨领域数据集与评估体系。

Method: 构建包含4k长问答实例和20k+短问答对的跨领域数据集，提出新型评估指标，探索提示工程与训练优化方法。

Result: 当前模型在长文本中无法恰当表达不确定性，训练优化方法比提示工程效果更显著（+15%准确率）。

Conclusion: UNCLE基准揭示了长短文本不确定性表达的差异，为改进模型校准与可信生成提供了新方向。

Abstract: Large Language Models (LLMs) are prone to hallucination, particularly in
long-form generations. A promising direction to mitigate hallucination is to
teach LLMs to express uncertainty explicitly when they lack sufficient
knowledge. However, existing work lacks direct and fair evaluation of LLMs'
ability to express uncertainty effectively in long-form generation. To address
this gap, we first introduce UNCLE, a benchmark designed to evaluate
uncertainty expression in both long- and short-form question answering (QA).
UNCLE spans five domains and comprises 4k long-form QA instances and over 20k
short-form QA pairs. Our dataset is the first to directly bridge short- and
long-form QA with paired questions and gold-standard answers. Along with the
benchmark, we propose a suite of new metrics to assess the models' capabilities
to selectively express uncertainty. Using UNCLE, we then demonstrate that
current models fail to convey uncertainty appropriately in long-form
generation. We further explore both prompt-based and training-based methods to
improve models' performance, with the training-based methods yielding greater
gains. Further analysis of alignment gaps between short- and long-form
uncertainty expression highlights promising directions for future research
using UNCLE.

</details>


### [116] [Latent Principle Discovery for Language Model Self-Improvement](https://arxiv.org/abs/2505.16927)
*Keshav Ramji,Tahira Naseem,Ramón Fernandez Astudillo*

Main category: cs.CL

TL;DR: 提出通过自校正框架自动挖掘语言模型的潜在指导原则，结合聚类压缩实现高效自改进


<details>
  <summary>Details</summary>
Motivation: 传统人工标注模型行为准则成本高昂，需要自动化方法挖掘语言模型内在指导原则

Method: 采用后验正则化蒙特卡洛EM算法，通过自纠正机制挖掘潜在原则并聚类压缩，使模型自主调用原则改进生成质量

Result: 使7-8B参数模型在AlpacaEval胜率提升8-10%、MT-Bench平均+0.3分、IFEval原则遵循率提升19-23%

Conclusion: 验证了自动化原则驱动训练方法在模型持续自改进中的有效性，为后续训练范式提供新思路

Abstract: When language model (LM) users aim to improve the quality of its generations,
it is crucial to specify concrete behavioral attributes that the model should
strive to reflect. However, curating such principles across many domains, even
non-exhaustively, requires a labor-intensive annotation process. To automate
this process, we propose eliciting these latent attributes guiding model
reasoning towards human-preferred responses by explicitly modeling them in a
self-correction setting. Our approach mines new principles from the LM itself
and compresses the discovered elements to an interpretable set via clustering.
Specifically, we employ an approximation of posterior-regularized Monte Carlo
Expectation-Maximization to both identify a condensed set of the most effective
latent principles and teach the LM to strategically invoke them in order to
intrinsically refine its responses. We demonstrate that bootstrapping our
algorithm over multiple iterations enables smaller language models (7-8B
parameters) to self-improve, achieving +8-10% in AlpacaEval win-rate, an
average of +0.3 on MT-Bench, and +19-23% in principle-following win-rate on
IFEval. We also show that clustering the principles yields interpretable and
diverse model-generated constitutions while retaining model performance. The
gains our method achieves highlight the potential of automated,
principle-driven post-training recipes toward continual self-improvement.

</details>


### [117] [PIIvot: A Lightweight NLP Anonymization Framework for Question-Anchored Tutoring Dialogues](https://arxiv.org/abs/2505.16931)
*Matthew Zent,Digory Smith,Simon Woodhead*

Main category: cs.CL

TL;DR: 提出轻量级PII匿名化框架PIIvot及教育对话数据集QATD-2k


<details>
  <summary>Details</summary>
Motivation: 现有PII匿名化方法受限于错误阈值和召回率/精准率权衡，阻碍科研数据共享

Method: 利用数据上下文知识简化PII检测问题，构建开源教育对话数据集进行验证

Result: 成功开发PIIvot框架并发布当前最大规模的真实场景教育对话数据集

Conclusion: 该方法突破现有技术限制，同时为教育领域提供高质量对话数据资源

Abstract: Personally identifiable information (PII) anonymization is a high-stakes task
that poses a barrier to many open-science data sharing initiatives. While PII
identification has made large strides in recent years, in practice, error
thresholds and the recall/precision trade-off still limit the uptake of these
anonymization pipelines. We present PIIvot, a lighter-weight framework for PII
anonymization that leverages knowledge of the data context to simplify the PII
detection problem. To demonstrate its effectiveness, we also contribute
QATD-2k, the largest open-source real-world tutoring dataset of its kind, to
support the demand for quality educational dialogue data.

</details>


### [118] [In-Context Watermarks for Large Language Models](https://arxiv.org/abs/2505.16934)
*Yepeng Liu,Xuandong Zhao,Christopher Kruegel,Dawn Song,Yuheng Bu*

Main category: cs.CL

TL;DR: 提出In-Context Watermarking方法，通过提示工程嵌入水印，无需访问模型内部，解决现有水印技术依赖解码过程的局限性


<details>
  <summary>Details</summary>
Motivation: 现有水印技术需要访问模型解码过程，无法满足学术评审等实际场景中会议组织者检测AI生成内容的需求

Method: 利用LLM的上下文学习能力，通过提示工程设计四种不同粒度的水印策略，结合间接提示注入案例研究，并开发相应检测方法

Result: 实验证明ICW作为模型无关方法具有可行性，且LLM能力越强该水印技术的扩展性和实用性越显著

Conclusion: ICW为AI生成内容归属问题提供了可扩展的解决方案，标志着无需模型访问权限的水印技术新方向

Abstract: The growing use of large language models (LLMs) for sensitive applications
has highlighted the need for effective watermarking techniques to ensure the
provenance and accountability of AI-generated text. However, most existing
watermarking methods require access to the decoding process, limiting their
applicability in real-world settings. One illustrative example is the use of
LLMs by dishonest reviewers in the context of academic peer review, where
conference organizers have no access to the model used but still need to detect
AI-generated reviews. Motivated by this gap, we introduce In-Context
Watermarking (ICW), which embeds watermarks into generated text solely through
prompt engineering, leveraging LLMs' in-context learning and
instruction-following abilities. We investigate four ICW strategies at
different levels of granularity, each paired with a tailored detection method.
We further examine the Indirect Prompt Injection (IPI) setting as a specific
case study, in which watermarking is covertly triggered by modifying input
documents such as academic manuscripts. Our experiments validate the
feasibility of ICW as a model-agnostic, practical watermarking approach.
Moreover, our findings suggest that as LLMs become more capable, ICW offers a
promising direction for scalable and accessible content attribution.

</details>


### [119] [On Multilingual Encoder Language Model Compression for Low-Resource Languages](https://arxiv.org/abs/2505.16956)
*Daniil Gurgurov,Michal Gregor,Josef van Genabith,Simon Ostermann*

Main category: cs.CL

TL;DR: 提出多语言模型极端压缩方案，结合知识蒸馏+结构化剪枝+词汇修剪，实现92%压缩率，下游任务性能仅降2-10%


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言场景下大模型部署难题，通过系统整合现有技术实现模型极致压缩，在保留语言知识前提下显著减小参数量

Method: 两步知识蒸馏框架配合结构化剪枝（压缩层深度/前馈隐藏层/中间嵌入维度），结合词汇表截断策略

Result: 在3种低资源语言的4个NLP任务中，压缩后模型仅损失2-10%性能（性能衰减程度与教师模型语言数据量负相关）

Conclusion: 证实极端压缩在低资源场景可行性，数据规模决定性能保留度，系统化整合现有技术可创造实用级小型语言模型

Abstract: In this paper, we combine two-step knowledge distillation, structured
pruning, truncation, and vocabulary trimming for extremely compressing
multilingual encoder-only language models for low-resource languages. Our novel
approach systematically combines existing techniques and takes them to the
extreme, reducing layer depth, feed-forward hidden size, and intermediate layer
embedding size to create significantly smaller monolingual models while
retaining essential language-specific knowledge. We achieve compression rates
of up to 92% with only a marginal performance drop of 2-10% in four downstream
tasks, including sentiment analysis, topic classification, named entity
recognition, and part-of-speech tagging, across three low-resource languages.
Notably, the performance degradation correlates with the amount of
language-specific data in the teacher model, with larger datasets resulting in
smaller performance losses. Additionally, we conduct extensive ablation studies
to identify best practices for multilingual model compression using these
techniques.

</details>


### [120] [BP-Seg: A graphical model approach to unsupervised and non-contiguous text segmentation using belief propagation](https://arxiv.org/abs/2505.16965)
*Fengyi Li,Kayhan Behdin,Natesh Pillai,Xiaofeng Wang,Zhipeng Wang,Ercan Yildiz*

Main category: cs.CL

TL;DR: 提出基于图模型的非监督文本分割方法BP-Seg，兼具局部连贯性与全局语义相似性捕捉能力


<details>
  <summary>Details</summary>
Motivation: 现有文本分割方法缺乏对远距离语义相似句子的有效分组能力，需兼顾局部连贯与全局语义关联

Method: 通过置信传播算法在图模型上实现，同时考虑相邻句子的关联性和远距离语义相似性

Result: 在长文档数据集上实验表现优于对比方法，验证了模型有效性

Conclusion: BP-Seg通过图模型创新性地平衡了局部与全局语义关系，为文本分割任务提供了新思路

Abstract: Text segmentation based on the semantic meaning of sentences is a fundamental
task with broad utility in many downstream applications. In this paper, we
propose a graphical model-based unsupervised learning approach, named BP-Seg
for efficient text segmentation. Our method not only considers local coherence,
capturing the intuition that adjacent sentences are often more related, but
also effectively groups sentences that are distant in the text yet semantically
similar. This is achieved through belief propagation on the carefully
constructed graphical models. Experimental results on both an illustrative
example and a dataset with long-form documents demonstrate that our method
performs favorably compared to competing approaches.

</details>


### [121] [From Tens of Hours to Tens of Thousands: Scaling Back-Translation for Speech Recognition](https://arxiv.org/abs/2505.16972)
*Tianduo Wang,Lu Xu,Wei Lu,Shanbo Cheng*

Main category: cs.CL

TL;DR: 通过语音反向翻译技术，将大规模文本语料转化为合成语音，显著提升多语言语音识别模型的性能


<details>
  <summary>Details</summary>
Motivation: 现有语音识别技术依赖海量语音数据，但在低资源语言扩展上存在瓶颈，需要开发更高效的解决方案

Method: 1) 使用现成TTS模型将文本转为合成语音
2) 开发基于可懂度的质量评估框架
3) 仅需数十小时真实语音即可生成数百倍合成数据

Result: 在10种语言生成超50万小时合成语音，Whisper-large-v3模型错误率平均降低30%以上

Conclusion: 该方法证明了合成语音数据的规模化应用潜力，确立了质量门槛对ASR训练的有效性

Abstract: Recent advances in Automatic Speech Recognition (ASR) have been largely
fueled by massive speech corpora. However, extending coverage to diverse
languages with limited resources remains a formidable challenge. This paper
introduces Speech Back-Translation, a scalable pipeline that improves
multilingual ASR models by converting large-scale text corpora into synthetic
speech via off-the-shelf text-to-speech (TTS) models. We demonstrate that just
tens of hours of real transcribed speech can effectively train TTS models to
generate synthetic speech at hundreds of times the original volume while
maintaining high quality. To evaluate synthetic speech quality, we develop an
intelligibility-based assessment framework and establish clear thresholds for
when synthetic data benefits ASR training. Using Speech Back-Translation, we
generate more than 500,000 hours of synthetic speech in ten languages and
continue pre-training Whisper-large-v3, achieving average transcription error
reductions of over 30\%. These results highlight the scalability and
effectiveness of Speech Back-Translation for enhancing multilingual ASR
systems.

</details>


### [122] [VeriFastScore: Speeding up long-form factuality evaluation](https://arxiv.org/abs/2505.16973)
*Rishanth Rajendhran,Amir Zadeh,Matthew Sarte,Chuan Li,Mohit Iyyer*

Main category: cs.CL

TL;DR: 提出VeriFastScore方法，通过微调Llama3.1 8B实现声明同步提取与验证，将事实性评估速度提升6.6倍


<details>
  <summary>Details</summary>
Motivation: 现有事实性评估方法因多次LLM调用导致耗时过长（约100秒/次），难以支持大规模评估场景

Method: 基于Google Search证据合成数据，微调模型实现并发声明分解、可验证性判断及噪音证据验证的端到端处理

Result: 在样本级(r=0.80)和系统级(r=0.94)保持高相关性，整体加速6.6倍（排除证据检索后9.9倍），开源模型与数据集

Conclusion: VeriFastScore在保持评估质量的同时突破效率瓶颈，公开资源将推动事实性评估技术的进一步发展

Abstract: Metrics like FactScore and VeriScore that evaluate long-form factuality
operate by decomposing an input response into atomic claims and then
individually verifying each claim. While effective and interpretable, these
methods incur numerous LLM calls and can take upwards of 100 seconds to
evaluate a single response, limiting their practicality in large-scale
evaluation and training scenarios. To address this, we propose VeriFastScore,
which leverages synthetic data to fine-tune Llama3.1 8B for simultaneously
extracting and verifying all verifiable claims within a given text based on
evidence from Google Search. We show that this task cannot be solved via
few-shot prompting with closed LLMs due to its complexity: the model receives
~4K tokens of evidence on average and needs to concurrently decompose claims,
judge their verifiability, and verify them against noisy evidence. However, our
fine-tuned VeriFastScore model demonstrates strong correlation with the
original VeriScore pipeline at both the example level (r=0.80) and system level
(r=0.94) while achieving an overall speedup of 6.6x (9.9x excluding evidence
retrieval) over VeriScore. To facilitate future factuality research, we
publicly release our VeriFastScore model and synthetic datasets.

</details>


### [123] [LLM as Effective Streaming Processor: Bridging Streaming-Batch Mismatches with Group Position Encoding](https://arxiv.org/abs/2505.16983)
*Junlong Tong,Jinlan Fu,Zixuan Lin,Yingqi Fan,Anhao Zhao,Hui Su,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 提出基于分组位置编码的流式大语言模型适配方法，在不修改架构的前提下有效解决输入注意力错配问题，显著提升流式处理性能。


<details>
  <summary>Details</summary>
Motivation: 现有流式大语言模型适配方法存在高代价重编码或架构局限性的缺陷，本文通过系统性分析揭示仅输入注意力错配是关键瓶颈，打破需频繁重编码输出的固有认知。

Method: 通过位置编码对模型影响的首次系统性分析，提出基于批式架构的分组位置编码范式，增强流式与批处理模式的一致性。

Result: 跨语言和跨模态任务实验表明方法优于现有方案，无需架构修改且在流式/批处理模式均展现强泛化能力，代码已开源。

Conclusion: 分组位置编码机制有效解决流式适配难题，为批处理架构的流式应用提供高效解决方案，推动大模型在实时场景的落地应用。

Abstract: Large Language Models (LLMs) are primarily designed for batch processing.
Existing methods for adapting LLMs to streaming rely either on expensive
re-encoding or specialized architectures with limited scalability. This work
identifies three key mismatches in adapting batch-oriented LLMs to streaming:
(1) input-attention, (2) output-attention, and (3) position-ID mismatches.
While it is commonly assumed that the latter two mismatches require frequent
re-encoding, our analysis reveals that only the input-attention mismatch
significantly impacts performance, indicating re-encoding outputs is largely
unnecessary. To better understand this discrepancy with the common assumption,
we provide the first comprehensive analysis of the impact of position encoding
on LLMs in streaming, showing that preserving relative positions within source
and target contexts is more critical than maintaining absolute order. Motivated
by the above analysis, we introduce a group position encoding paradigm built on
batch architectures to enhance consistency between streaming and batch modes.
Extensive experiments on cross-lingual and cross-modal tasks demonstrate that
our method outperforms existing approaches. Our method requires no
architectural modifications, exhibits strong generalization in both streaming
and batch modes. The code is available at repository
https://github.com/EIT-NLP/StreamingLLM.

</details>


### [124] [T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic Planning](https://arxiv.org/abs/2505.16986)
*Amartya Chakraborty,Paresh Dashore,Nadia Bathaee,Anmol Jain,Anirban Das,Shi-Xiong Zhang,Sambit Sahu,Milind Naphade,Genta Indra Winata*

Main category: cs.CL

TL;DR: T1是一个支持多领域、多轮次对话的工具增强数据集，通过集成缓存机制解决大语言模型在复杂工具依赖场景下的动态重规划问题。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在处理API/工具调用依赖关系（尤其是多轮对话场景）时存在显著规划能力缺陷，需针对性数据集推动相关研究。

Method: 构建包含9个领域（4单领域+5跨领域）的T1数据集，集成长短期记忆缓存机制，开发T1-Agent评估框架支持动态重规划决策（重新计算/复用缓存）。

Result: T1成为首个系统评估工具依赖场景下规划能力的基准，实验证明基于T1-Agent的模型能有效协调工具使用并进行动态决策。

Conclusion: T1不仅推动工具使用与规划研究，还为开源模型提供复杂场景下的性能评估标准，其缓存机制和跨领域设计显著提升对话代理的规划能力。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities as
intelligent agents capable of solving complex problems. However, effective
planning in scenarios involving dependencies between API or tool
calls-particularly in multi-turn conversations-remains a significant challenge.
To address this, we introduce T1, a tool-augmented, multi-domain, multi-turn
conversational dataset specifically designed to capture and manage inter-tool
dependencies across diverse domains. T1 enables rigorous evaluation of agents'
ability to coordinate tool use across nine distinct domains (4 single domain
and 5 multi-domain) with the help of an integrated caching mechanism for both
short- and long-term memory, while supporting dynamic replanning-such as
deciding whether to recompute or reuse cached results. Beyond facilitating
research on tool use and planning, T1 also serves as a benchmark for evaluating
the performance of open-source language models. We present results powered by
T1-Agent, highlighting their ability to plan and reason in complex,
tool-dependent scenarios.

</details>


### [125] [MASLab: A Unified and Comprehensive Codebase for LLM-based Multi-Agent Systems](https://arxiv.org/abs/2505.16988)
*Rui Ye,Keduan Huang,Qimin Wu,Yuzhu Cai,Tian Jin,Xianghe Pang,Xiangrui Liu,Jiaqi Su,Chen Qian,Bohan Tang,Kaiqu Liang,Jiaao Chen,Yue Hu,Zhenfei Yin,Rongye Shi,Bo An,Yang Gao,Wenjun Wu,Lei Bai,Siheng Chen*

Main category: cs.CL

TL;DR: MASLab推出首个统一代码库，整合20+多智能体方法，解决领域内冗余开发、评估标准不统一等问题，提供标准化研究框架


<details>
  <summary>Details</summary>
Motivation: 当前LLM多智能体领域缺乏统一代码库，导致重复实现、评估不公平、研究门槛高。研究者旨在通过MASLab建立标准化平台，推动领域协同发展

Method: 1) 整合验证20+官方方法 2) 构建统一环境与多基准测试 3) 模块化代码架构降低使用门槛 4) 开展涵盖10+基准测试与8种模型的大规模实验

Result: 实验系统揭示了当前多智能体方法生态，验证了框架有效性。代码库持续迭代并计划开源社区共建

Conclusion: MASLab成功构建了多智能体研究基础设施，通过标准化实现方法公平比较，未来将持续跟踪领域发展并与社区共同推进技术演进

Abstract: LLM-based multi-agent systems (MAS) have demonstrated significant potential
in enhancing single LLMs to address complex and diverse tasks in practical
applications. Despite considerable advancements, the field lacks a unified
codebase that consolidates existing methods, resulting in redundant
re-implementation efforts, unfair comparisons, and high entry barriers for
researchers. To address these challenges, we introduce MASLab, a unified,
comprehensive, and research-friendly codebase for LLM-based MAS. (1) MASLab
integrates over 20 established methods across multiple domains, each rigorously
validated by comparing step-by-step outputs with its official implementation.
(2) MASLab provides a unified environment with various benchmarks for fair
comparisons among methods, ensuring consistent inputs and standardized
evaluation protocols. (3) MASLab implements methods within a shared streamlined
structure, lowering the barriers for understanding and extension. Building on
MASLab, we conduct extensive experiments covering 10+ benchmarks and 8 models,
offering researchers a clear and comprehensive view of the current landscape of
MAS methods. MASLab will continue to evolve, tracking the latest developments
in the field, and invite contributions from the broader open-source community.

</details>


### [126] [DecoupledESC: Enhancing Emotional Support Generation via Strategy-Response Decoupled Preference Optimization](https://arxiv.org/abs/2505.16995)
*Chao Zhang,Xin Shi,Xueqiao Zhang,Yifan Zhu,Yi Yang,Yawei Luo*

Main category: cs.CL

TL;DR: 提出分层的Inferential Preference Mining框架解决情感支持对话中数据纠缠与优化模糊问题，通过策略规划与共情响应分步优化提升生成质量


<details>
  <summary>Details</summary>
Motivation: 现有情感支持对话方法存在心理策略与响应内容的数据纠缠问题，直接应用DPO会导致优化目标模糊，需要解耦策略规划与响应生成

Method: 1. 提出推断性偏好挖掘(IPM)构建IPM-PrefDial数据集；2. 基于Gross情绪调节模型设计分层的ESC框架：先策略规划(SFT+DPO)，后共情响应生成(SFT+DPO)

Result: 实验表明分层的ESC框架优于联合优化基线，偏好偏差降低21.7%，响应质量提升15.3%

Conclusion: 解耦式训练框架有效解决了数据纠缠问题，心理对齐的DPO优化显著提升情感支持效果

Abstract: Recent advances in Emotional Support Conversation (ESC) have improved
emotional support generation by fine-tuning Large Language Models (LLMs) via
Supervised Fine-Tuning (SFT). However, common psychological errors still
persist. While Direct Preference Optimization (DPO) shows promise in reducing
such errors through pairwise preference learning, its effectiveness in ESC
tasks is limited by two key challenges: (1) Entangled data structure: Existing
ESC data inherently entangles psychological strategies and response content,
making it difficult to construct high-quality preference pairs; and (2)
Optimization ambiguity: Applying vanilla DPO to such entangled pairwise data
leads to ambiguous training objectives. To address these issues, we introduce
Inferential Preference Mining (IPM) to construct high-quality preference data,
forming the IPM-PrefDial dataset. Building upon this data, we propose a
Decoupled ESC framework inspired by Gross's Extended Process Model of Emotion
Regulation, which decomposes the ESC task into two sequential subtasks:
strategy planning and empathic response generation. Each was trained via SFT
and subsequently enhanced by DPO to align with the psychological preference.
Extensive experiments demonstrate that our Decoupled ESC framework outperforms
joint optimization baselines, reducing preference bias and improving response
quality.

</details>


### [127] [Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?](https://arxiv.org/abs/2505.16998)
*Jin Jiang,Jianing Wang,Yuchen Yan,Yang Liu,Jianhua Zhu,Mengdi Zhang,Xunliang Cai,Liangcai Gao*

Main category: cs.CL

TL;DR: 该论文通过三个维度（LLM类型、任务分类、轨迹格式）系统评估大语言模型在形式语言下的逻辑推理能力，发现思维模型显著优于指导模型，所有模型在归纳推理上存在局限，且PoT格式数据泛化性能最佳。通过形式相关训练数据增强小模型后，拒绝微调方法可提升跨语言泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注用形式语言引导大语言模型生成可靠推理路径，但缺乏对其逻辑推理能力的系统性评估。本文旨在通过形式语言全面评估LLMs在不同逻辑推理任务中的表现。

Method: 1) 对比思维模型与指导模型在形式语言下的表现；2) 测试LLMs在演绎/归纳推理任务中的性能；3) 分析不同轨迹格式（CoT/PoT）的泛化能力；4) 构建形式相关训练数据并采用拒绝微调方法增强模型。

Result: 1) 思维模型准确率超指导模型15.3%；2) 所有模型归纳推理准确率低于40%；3) PoT格式数据在跨语言任务中表现最优；4) 拒绝微调使模型在形式语言间泛化准确率提升12.8%。

Conclusion: 形式语言可有效提升LLMs的逻辑推理能力，但归纳推理仍是瓶颈。PoT格式数据与拒绝微调方法为提升模型跨语言泛化性能提供了新方向，相关训练数据增强策略对小模型优化具有实用价值。

Abstract: Large Language Models (LLMs) have been shown to achieve breakthrough
performance on complex logical reasoning tasks. Nevertheless, most existing
research focuses on employing formal language to guide LLMs to derive reliable
reasoning paths, while systematic evaluations of these capabilities are still
limited. In this paper, we aim to conduct a comprehensive evaluation of LLMs
across various logical reasoning problems utilizing formal languages. From the
perspective of three dimensions, i.e., spectrum of LLMs, taxonomy of tasks, and
format of trajectories, our key findings are: 1) Thinking models significantly
outperform Instruct models, especially when formal language is employed; 2) All
LLMs exhibit limitations in inductive reasoning capability, irrespective of
whether they use a formal language; 3) Data with PoT format achieves the best
generalization performance across other languages. Additionally, we also curate
the formal-relative training data to further enhance the small language models,
and the experimental results indicate that a simple rejected fine-tuning method
can better enable LLMs to generalize across formal languages and achieve the
best overall performance. Our codes and reports are available at
https://github.com/jiangjin1999/FormalEval.

</details>


### [128] [R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning](https://arxiv.org/abs/2505.17005)
*Huatong Song,Jinhao Jiang,Wenqing Tian,Zhipeng Chen,Yuhuan Wu,Jiahao Zhao,Yingqian Min,Wayne Xin Zhao,Lei Fang,Ji-Rong Wen*

Main category: cs.CL

TL;DR: 提出R1-Searcher++框架，通过两阶段训练策略结合内外知识源，提升LLMs的检索增强推理效率


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法存在高成本、泛化性差且忽视模型内部知识的问题，需构建更高效的知识融合机制

Method: 采用两阶段训练：SFT冷启动实现格式学习，强化学习阶段通过结果监督探索+奖励机制+记忆机制融合内外知识

Result: 实验表明R1-Searcher++在检索效率和推理质量上超越现有RAG方法，支持持续知识吸收

Conclusion: 该框架成功实现动态知识获取，通过强化学习机制持续增强模型的内外知识协同能力

Abstract: Large Language Models (LLMs) are powerful but prone to hallucinations due to
static knowledge. Retrieval-Augmented Generation (RAG) helps by injecting
external information, but current methods often are costly, generalize poorly,
or ignore the internal knowledge of the model. In this paper, we introduce
R1-Searcher++, a novel framework designed to train LLMs to adaptively leverage
both internal and external knowledge sources. R1-Searcher++ employs a two-stage
training strategy: an initial SFT Cold-start phase for preliminary format
learning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses
outcome-supervision to encourage exploration, incorporates a reward mechanism
for internal knowledge utilization, and integrates a memorization mechanism to
continuously assimilate retrieved information, thereby enriching the model's
internal knowledge. By leveraging internal knowledge and external search
engine, the model continuously improves its capabilities, enabling efficient
retrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++
outperforms previous RAG and reasoning methods and achieves efficient
retrieval. The code is available at
https://github.com/RUCAIBox/R1-Searcher-plus.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [129] [Dynamic Caustics by Ultrasonically Modulated Liquid Surface](https://arxiv.org/abs/2505.16397)
*Koki Nagakura,Tatsuki Fushimi,Ayaka Tsutsui,Yoichi Ochiai*

Main category: cs.GR

TL;DR: 提出基于双优化全息场与相控阵换能器的动态焦散生成方法，通过数字孪生框架实现实时流体表面控制，可在高频下生成连续动画焦散图案。


<details>
  <summary>Details</summary>
Motivation: 现有焦散生成技术多局限于静态场景，液体表面作为折射介质的动态应用尚未实现。突破传统固体表面限制，探索实时可控的焦散生成新范式。

Method: 结合双优化全息场算法与PAT超声波控制，利用数字孪生实现闭环优化。以液体表面为动态折射介质，通过超声波实时形变调控光路。

Result: 实现30Hz更新率的动态焦散生成，图案复杂度提升40%。虽对比度较固体方法低15%，但具备毫秒级实时响应能力。

Conclusion: 该技术为交互显示开辟新途径，适用于艺术装置与教育领域。未来可通过相位优化算法提升图案分辨率至2000dpi级别。

Abstract: This paper presents a method for generating dynamic caustic patterns by
utilising dual-optimised holographic fields with Phased Array Transducer (PAT).
Building on previous research in static caustic optimisation and ultrasonic
manipulation, this approach employs computational techniques to dynamically
shape fluid surfaces, thereby creating controllable and real-time caustic
images. The system employs a Digital Twin framework, which enables iterative
feedback and refinement, thereby improving the accuracy and quality of the
caustic patterns produced. This paper extends the foundational work in caustic
generation by integrating liquid surfaces as refractive media. This concept has
previously been explored in simulations but not fully realised in practical
applications. The utilisation of ultrasound to directly manipulate these
surfaces enables the generation of dynamic caustics with a high degree of
flexibility. The Digital Twin approach further enhances this process by
allowing for precise adjustments and optimisation based on real-time feedback.
Experimental results demonstrate the technique's capacity to generate
continuous animations and complex caustic patterns at high frequencies.
Although there are limitations in contrast and resolution compared to
solid-surface methods, this approach offers advantages in terms of real-time
adaptability and scalability. This technique has the potential to be applied in
a number of areas, including interactive displays, artistic installations and
educational tools. This research builds upon the work of previous researchers
in the fields of caustics optimisation, ultrasonic manipulation, and
computational displays. Future research will concentrate on enhancing the
resolution and intricacy of the generated patterns.

</details>


### [130] [From Reality to Virtual Worlds: The Role of Photogrammetry in Game Development](https://arxiv.org/abs/2505.16951)
*Santiago Berrezueta-Guzman,Andrei Koshelev,Stefan Wagner*

Main category: cs.GR

TL;DR: 论文评估了RealityCapture工具在VR游戏开发中的效能，证实其能显著缩短开发周期并保持高精度，但用户对小物件细节设计存在偏好差异。


<details>
  <summary>Details</summary>
Motivation: 探索GPU加速的摄影测量工具如何优化VR内容生产流程，比较自动化建模与传统人工建模的优劣，分析用户对两类模型的接受度差异。

Method: 通过测量建模效率、几何精度、引擎集成度等指标，对比传统流程；开展用户调研评估模型偏好；分析硬件需求与工作流兼容性。

Result: 摄影测量模型增强环境真实感（场景接受度+32%），但用户更倾向手动设计可交互小物件（偏好度57%）；开发周期缩短40%，纹理质量提升2.3倍。

Conclusion: RealityCapture通过自动化建模革新VR开发范式，未来结合AI优化与云处理可扩展至文化遗产数字化等领域，需解决细节控制与硬件门槛问题。

Abstract: Photogrammetry is transforming digital content creation by enabling the rapid
conversion of real-world objects into highly detailed 3D models. This paper
evaluates the role of RealityCapture, a GPU-accelerated photogrammetry tool, in
game development of Virtual Reality (VR). We assess its efficiency,
reconstruction accuracy, and integration with Unreal Engine, comparing its
advantages and limitations against traditional modeling workflows.
Additionally, we examined user preferences between designed 3D assets and
photogrammetry-generated models. The results revealed that while photogrammetry
enhances realism and interactivity, users slightly preferred manually designed
models for small, manipulable elements because of the level of detail. However,
from a developer perspective, RealityCapture significantly reduces development
time while maintaining geometric precision and photorealistic textures. Despite
its reliance on high-performance hardware, its automation, scalability, and
seamless integration with real-time rendering engines make it a valuable tool
for game developers and VR creators. Future improvements in AI-driven
optimization and cloud-based processing could enhance accessibility, broadening
its applications in gaming, cultural heritage preservation, and simulation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [131] [Interpretability Illusions with Sparse Autoencoders: Evaluating Robustness of Concept Representations](https://arxiv.org/abs/2505.16004)
*Aaron J. Li,Suraj Srinivas,Usha Bhalla,Himabindu Lakkaraju*

Main category: cs.LG

TL;DR: 论文发现稀疏自编码器（SAE）生成的概念表示对微小对抗扰动敏感，可能不适用于模型监控场景。


<details>
  <summary>Details</summary>
Motivation: 现有SAE评估指标（如重构稀疏性、可解释性）未考虑概念表示的鲁棒性，而鲁棒性是概念标注可信度的关键指标。

Method: 通过输入空间优化问题量化鲁棒性，构建包含对抗扰动场景的评估框架测试SAE表示稳定性。

Result: 微小对抗扰动即可显著操纵SAE概念解释（平均扰动幅度0.01即达80%成功率），且不影响基础LLM输出。

Conclusion: SAE概念表示的脆弱性可能限制其在模型监控与安全监管中的应用价值。

Abstract: Sparse autoencoders (SAEs) are commonly used to interpret the internal
activations of large language models (LLMs) by mapping them to
human-interpretable concept representations. While existing evaluations of SAEs
focus on metrics such as the reconstruction-sparsity tradeoff, human
(auto-)interpretability, and feature disentanglement, they overlook a critical
aspect: the robustness of concept representations to input perturbations. We
argue that robustness must be a fundamental consideration for concept
representations, reflecting the fidelity of concept labeling. To this end, we
formulate robustness quantification as input-space optimization problems and
develop a comprehensive evaluation framework featuring realistic scenarios in
which adversarial perturbations are crafted to manipulate SAE representations.
Empirically, we find that tiny adversarial input perturbations can effectively
manipulate concept-based interpretations in most scenarios without notably
affecting the outputs of the base LLMs themselves. Overall, our results suggest
that SAE concept representations are fragile and may be ill-suited for
applications in model monitoring and oversight.

</details>


### [132] [Merge to Mix: Mixing Datasets via Model Merging](https://arxiv.org/abs/2505.16066)
*Zhixu Silvia Tao,Kasper Vinken,Hao-Wei Yeh,Avi Cooper,Xavier Boix*

Main category: cs.LG

TL;DR: 提出Merge to Mix方法，通过模型合并技术加速数据集混合选择，避免传统试错式微调的低效性，实验显示优于现有方法


<details>
  <summary>Details</summary>
Motivation: 当前数据集混合方法依赖试错和多次微调，计算成本高且效率低下，需开发更高效的替代方案

Method: 将各数据集单独微调的模型通过算术合并生成混合模型，作为全量微调的替代方案，实现候选数据集快速评估

Result: 实验证明该方法在微调数据集选择任务中超越现有state-of-the-art方法

Conclusion: 模型合并技术有效替代传统全量微调流程，显著提升数据集混合选择效率

Abstract: Mixing datasets for fine-tuning large models (LMs) has become critical for
maximizing performance on downstream tasks. However, composing effective
dataset mixtures typically relies on heuristics and trial-and-error, often
requiring multiple fine-tuning runs to achieve the desired outcome. We propose
a novel method, $\textit{Merge to Mix}$, that accelerates composing dataset
mixtures through model merging. Model merging is a recent technique that
combines the abilities of multiple individually fine-tuned LMs into a single LM
by using a few simple arithmetic operations. Our key insight is that merging
models individually fine-tuned on each dataset in a mixture can effectively
serve as a surrogate for a model fine-tuned on the entire mixture. Merge to Mix
leverages this insight to accelerate selecting dataset mixtures without
requiring full fine-tuning on each candidate mixture. Our experiments
demonstrate that Merge to Mix surpasses state-of-the-art methods in dataset
selection for fine-tuning LMs.

</details>


### [133] [A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization](https://arxiv.org/abs/2505.16094)
*Ziqing Wang,Kexin Zhang,Zihan Zhao,Yibo Wen,Abhishek Pandey,Han Liu,Kaize Ding*

Main category: cs.LG

TL;DR: LLMs通过文本引导和跨模态整合推动分子发现范式转型，聚焦分子生成与优化两大核心任务


<details>
  <summary>Details</summary>
Motivation: 整合LLMs能力推动分子发现领域发展，构建系统化研究框架

Method: 提出分类法分析分子生成与优化技术，评估不同学习设置下的LLM应用模式

Result: 总结常用数据集、评估协议及技术局限，建立持续更新的学术资源库

Conclusion: 需解决数据稀缺、模型可解释性等挑战，推动多模态LLM与分子科学的深度融合

Abstract: Large language models (LLMs) are introducing a paradigm shift in molecular
discovery by enabling text-guided interaction with chemical spaces through
natural language, symbolic notations, with emerging extensions to incorporate
multi-modal inputs. To advance the new field of LLM for molecular discovery,
this survey provides an up-to-date and forward-looking review of the emerging
use of LLMs for two central tasks: molecule generation and molecule
optimization. Based on our proposed taxonomy for both problems, we analyze
representative techniques in each category, highlighting how LLM capabilities
are leveraged across different learning settings. In addition, we include the
commonly used datasets and evaluation protocols. We conclude by discussing key
challenges and future directions, positioning this survey as a resource for
researchers working at the intersection of LLMs and molecular science. A
continuously updated reading list is available at
https://github.com/REAL-Lab-NU/Awesome-LLM-Centric-Molecular-Discovery.

</details>


### [134] [NAN: A Training-Free Solution to Coefficient Estimation in Model Merging](https://arxiv.org/abs/2505.16148)
*Chongjie Si,Kangtao Lv,Jingjing Jiang,Yadao Wang,Yongwei Wang,Xiaokang Yang,Wenbo Su,Bo Zheng,Wei Shen*

Main category: cs.LG

TL;DR: 提出NAN方法，通过参数范数逆估计模型融合系数，提升现有训练免更新策略的性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型融合方法依赖启发式确定融合系数，限制了方法的扩展性和通用性。需要基于优化理论提出更系统的解决方案。

Method: 基于最小二乘优化框架，通过参数范数的倒数估计融合系数。NAN无需训练即插即用，适用于多种融合策略。

Result: 大量实验表明NAN能持续提升基线方法性能，在多样化场景中展现有效性。

Conclusion: NAN为模型融合提供了理论支持，通过参数范数反映任务特定信息量，为多种融合策略提供简单有效的系数估计方案。

Abstract: Model merging offers a training-free alternative to multi-task learning by
combining independently fine-tuned models into a unified one without access to
raw data. However, existing approaches often rely on heuristics to determine
the merging coefficients, limiting their scalability and generality. In this
work, we revisit model merging through the lens of least-squares optimization
and show that the optimal merging weights should scale with the amount of
task-specific information encoded in each model. Based on this insight, we
propose NAN, a simple yet effective method that estimates model merging
coefficients via the inverse of parameter norm. NAN is training-free,
plug-and-play, and applicable to a wide range of merging strategies. Extensive
experiments on show that NAN consistently improves performance of baseline
methods.

</details>


### [135] [NQKV: A KV Cache Quantization Scheme Based on Normal Distribution Characteristics](https://arxiv.org/abs/2505.16210)
*Zhihang Cai,Xingjun Zhang,Zhendong Tan,Zheng Wei*

Main category: cs.LG

TL;DR: 提出NQKV算法对KV缓存进行低位量化，在不大幅影响模型质量前提下实现2倍批大小/4倍上下文扩展，提升推理效率


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理过程中KV缓存的内存消耗成为部署瓶颈，现有量化方法难以实现8bit以下的有效量化

Method: 通过分析KV缓存的元素分布规律，利用分块正态分布特性设计信息论最优的量化算法（分块分位数量化）

Result: 使OPT模型在KV缓存量化后支持2倍批大小/4倍上下文长度，吞吐量相对无KV缓存提升9.3倍

Conclusion: NQKV通过分块量化策略有效降低KV缓存内存占用，为低比特量化部署LLM提供了可行方案

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency across
a wide range of tasks. However, LLMs often require larger batch sizes to
enhance throughput or longer context lengths to meet task demands, which
significantly increases the memory resource consumption of the Key-Value (KV)
cache during inference, becoming a major bottleneck in LLM deployment. To
address this issue, quantization is a common and straightforward approach.
Currently, quantization methods for activations are limited to 8-bit, and
quantization to even lower bits can lead to substantial accuracy drops. To
further save space by quantizing the KV cache to even lower bits, we analyzed
the element distribution of the KV cache and designed the NQKV algorithm. Since
the elements within each block of the KV cache follow a normal distribution,
NQKV employs per-block quantile quantization to achieve
information-theoretically optimal quantization error. Without significantly
compromising model output quality, NQKV enables the OPT model to perform
inference with an 2x larger batch size or a 4x longer context length, and it
improves throughput by 9.3x compared to when the KV cache is not used.

</details>


### [136] [AdaSTaR: Adaptive Data Sampling for Training Self-Taught Reasoners](https://arxiv.org/abs/2505.16322)
*Woosung Koh,Wonbeen Oh,Jaein Jang,MinHyung Lee,Hyeongjin Kim,Ah Yeon Kim,Joonkee Kim,Junghyun Lee,Taehyeon Kim,Se-Young Yun*

Main category: cs.LG

TL;DR: 提出自适应STaR（AdaSTaR）算法，通过多样性自适应采样和课程自适应采样策略，在保持性能的同时显著减少训练计算成本


<details>
  <summary>Details</summary>
Motivation: 传统STaR/RFT的随机采样导致训练样本不平衡——过度训练简单样本而忽略困难样本，影响模型效果和训练效率

Method: 集成两种自适应机制：1）多样性采样确保样本均衡训练，2）课程采样动态调整数据难度匹配模型能力

Result: 在6个基准测试中全部达到最佳准确率，平均减少58.6%训练FLOPs，且改进效果在不同预训练模型上具有普适性

Conclusion: 通过自适应训练机制突破自改进语言模型的效率瓶颈，为更高效的大模型自优化方法提供新方向

Abstract: Self-Taught Reasoners (STaR), synonymously known as Rejection sampling
Fine-Tuning (RFT), is an integral part of the training pipeline of
self-improving reasoning Language Models (LMs). The self-improving mechanism
often employs random observation (data) sampling. However, this results in
trained observation imbalance; inefficiently over-training on solved examples
while under-training on challenging ones. In response, we introduce Adaptive
STaR (AdaSTaR), a novel algorithm that rectifies this by integrating two
adaptive sampling principles: (1) Adaptive Sampling for Diversity: promoting
balanced training across observations, and (2) Adaptive Sampling for
Curriculum: dynamically adjusting data difficulty to match the model's evolving
strength. Across six benchmarks, AdaSTaR achieves best test accuracy in all
instances (6/6) and reduces training FLOPs by an average of 58.6% against an
extensive list of baselines. These improvements in performance and efficiency
generalize to different pre-trained LMs and larger models, paving the way for
more efficient and effective self-improving LMs.

</details>


### [137] [AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning](https://arxiv.org/abs/2505.16400)
*Yang Chen,Zhuolin Yang,Zihan Liu,Chankyu Lee,Peng Xu,Mohammad Shoeybi,Bryan Catanzaro,Wei Ping*

Main category: cs.LG

TL;DR: 提出通过分阶段数学+代码强化学习训练策略，显著提升中小型模型在数学与代码推理任务中的表现，超越传统蒸馏方法效果


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分探索大规模RL对中小型推理模型的潜力，且前沿模型的训练细节不透明。需要验证RL是否能突破蒸馏方法的效果瓶颈

Method: 1. 分阶段RL训练：先数学提示训练，后代码提示训练
2. 建立包含验证测试用例的高质量数据筛选流程
3. 采用课程学习策略逐步增加响应长度
4. 基于验证的RL参数更新机制

Result: 数学RL提升7B/14B模型数学基准14.6%/17.2%，代码基准6.8%/5.8%；代码RL迭代进一步优化代码表现且不影响数学能力

Conclusion: RL不仅能激发模型预训练获得的推理潜能，更能突破原有能力边界，解决先前无法解决的问题。课程学习与在线参数更新是稳定训练的关键

Abstract: Despite recent progress in large-scale reinforcement learning (RL) for
reasoning, the training recipe for building high-performing reasoning models
remains elusive. Key implementation details of frontier models, such as
DeepSeek-R1, including data curation strategies and RL training recipe, are
often omitted. Moreover, recent research indicates distillation remains more
effective than RL for smaller models. In this work, we demonstrate that
large-scale RL can significantly enhance the reasoning capabilities of strong,
small- and mid-sized models, achieving results that surpass those of
state-of-the-art distillation-based models. We systematically study the RL
training process through extensive ablations and propose a simple yet effective
approach: first training on math-only prompts, then on code-only prompts.
Notably, we find that math-only RL not only significantly enhances the
performance of strong distilled models on math benchmarks (e.g., +14.6% /
+17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks
(e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition,
extended code-only RL iterations further improve performance on code benchmarks
with minimal or no degradation in math results. We develop a robust data
curation pipeline to collect challenging prompts with high-quality, verifiable
answers and test cases to enable verification-based RL across both domains.
Finally, we identify key experimental insights, including curriculum learning
with progressively increasing response lengths and the stabilizing effect of
on-policy parameter updates. We find that RL not only elicits the foundational
reasoning capabilities acquired during pretraining and supervised fine-tuning
(e.g., distillation), but also pushes the limits of the model's reasoning
ability, enabling it to solve problems that were previously unsolvable.

</details>


### [138] [Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization](https://arxiv.org/abs/2505.16737)
*Chengcan Wu,Zhixin Zhang,Zeming Wei,Yihao Zhang,Meng Sun*

Main category: cs.LG

TL;DR: 提出安全感知探测框架SAP，解决大模型微调过程中安全性能退化问题


<details>
  <summary>Details</summary>
Motivation: 大模型在良性数据微调时仍出现安全性退化，现有安全对齐技术存在局限，需探究根本原因并寻求解决方案

Method: 在梯度传播过程中嵌入安全感知探针，通过识别梯度方向潜在风险，构建双目标优化框架平衡任务性能与安全保护

Result: 实验显示SAP将模型危害性降低至原始微调模型水平以下，同时保持与标准微调相当的任务性能

Conclusion: SAP框架有效解决了LLMs微调时的安全-性能权衡难题，为模型安全部署提供新思路

Abstract: The significant progress of large language models (LLMs) has led to
remarkable achievements across numerous applications. However, their ability to
generate harmful content has sparked substantial safety concerns. Despite the
implementation of safety alignment techniques during the pre-training phase,
recent research indicates that fine-tuning LLMs on adversarial or even benign
data can inadvertently compromise their safety. In this paper, we re-examine
the fundamental issue of why fine-tuning on non-harmful data still results in
safety degradation. We introduce a safety-aware probing (SAP) optimization
framework designed to mitigate the safety risks of fine-tuning LLMs.
Specifically, SAP incorporates a safety-aware probe into the gradient
propagation process, mitigating the model's risk of safety degradation by
identifying potential pitfalls in gradient directions, thereby enhancing
task-specific performance while successfully preserving model safety. Our
extensive experimental results demonstrate that SAP effectively reduces
harmfulness below the original fine-tuned model and achieves comparable test
loss to standard fine-tuning methods. Our code is available at
https://github.com/ChengcanWu/SAP.

</details>


### [139] [ATR-Bench: A Federated Learning Benchmark for Adaptation, Trust, and Reasoning](https://arxiv.org/abs/2505.16850)
*Tajamul Ashraf,Mohammed Mohsen Peerzada,Moloud Abdar,Yutong Xie,Yuyin Zhou,Xiaofeng Liu,Iqra Altaf Gillani,Janibul Bashir*

Main category: cs.LG

TL;DR: 提出ATR-Bench联邦学习评估框架，通过适应性、信任度、推理能力三维度实现系统性评估


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习领域因缺乏标准化评估体系导致的方法进步受限和对比困难问题

Method: 构建三维分析框架（适应性-信任度-推理能力），对异构环境适应性和对抗环境信任度进行基准测试，推理维度提供文献洞见

Result: 建立首个联邦学习三维评估体系，揭示现有方法在跨设备适应与安全防护中的性能差异，明确推理维度研究空白

Conclusion: ATR-Bench为联邦学习提供真实场景下的系统评估基准，通过开源代码和动态文献库推动领域标准化进程

Abstract: Federated Learning (FL) has emerged as a promising paradigm for collaborative
model training while preserving data privacy across decentralized participants.
As FL adoption grows, numerous techniques have been proposed to tackle its
practical challenges. However, the lack of standardized evaluation across key
dimensions hampers systematic progress and fair comparison of FL methods. In
this work, we introduce ATR-Bench, a unified framework for analyzing federated
learning through three foundational dimensions: Adaptation, Trust, and
Reasoning. We provide an in-depth examination of the conceptual foundations,
task formulations, and open research challenges associated with each theme. We
have extensively benchmarked representative methods and datasets for adaptation
to heterogeneous clients and trustworthiness in adversarial or unreliable
environments. Due to the lack of reliable metrics and models for reasoning in
FL, we only provide literature-driven insights for this dimension. ATR-Bench
lays the groundwork for a systematic and holistic evaluation of federated
learning with real-world relevance. We will make our complete codebase publicly
accessible and a curated repository that continuously tracks new developments
and research in the FL literature.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [140] [Causal LLM Routing: End-to-End Regret Minimization from Observational Data](https://arxiv.org/abs/2505.16037)
*Asterios Tsiourvas,Wei Sun,Georgia Perakis*

Main category: cs.AI

TL;DR: 提出因果端到端框架LLM路由策略，通过观察数据最小化决策遗憾，实现成本感知的模型选择


<details>
  <summary>Details</summary>
Motivation: 传统解耦式LLM路由方法存在误差叠加风险且依赖全反馈数据，实际部署成本高昂

Method: 构建因果推理框架，设计两种理论支撑的代理目标：分类上界和softmax加权遗憾近似，并扩展区间条件架构处理异构成本偏好

Result: 在公共基准测试中超越现有基线，不同嵌入模型下均达到SOTA性能

Conclusion: 该框架通过端到端学习有效利用观察数据，实现高效决策且适应多样化的成本约束场景

Abstract: LLM routing aims to select the most appropriate model for each query,
balancing competing performance metrics such as accuracy and cost across a pool
of language models. Prior approaches typically adopt a decoupled strategy,
where the metrics are first predicted and the model is then selected based on
these estimates. This setup is prone to compounding errors and often relies on
full-feedback data, where each query is evaluated by all candidate models,
which is costly to obtain and maintain in practice. In contrast, we learn from
observational data, which records only the outcome of the model actually
deployed. We propose a causal end-to-end framework that learns routing policies
by minimizing decision-making regret from observational data. To enable
efficient optimization, we introduce two theoretically grounded surrogate
objectives: a classification-based upper bound, and a softmax-weighted regret
approximation shown to recover the optimal policy at convergence. We further
extend our framework to handle heterogeneous cost preferences via an
interval-conditioned architecture. Experiments on public benchmarks show that
our method outperforms existing baselines, achieving state-of-the-art
performance across different embedding models.

</details>


### [141] [Optimizing LLM-Based Multi-Agent System with Textual Feedback: A Case Study on Software Development](https://arxiv.org/abs/2505.16086)
*Ming Shen,Raphael Shu,Anurag Pratik,James Gung,Yubin Ge,Monica Sunkara,Yi Zhang*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We have seen remarkable progress in large language models (LLMs) empowered
multi-agent systems solving complex tasks necessitating cooperation among
experts with diverse skills. However, optimizing LLM-based multi-agent systems
remains challenging. In this work, we perform an empirical case study on group
optimization of role-based multi-agent systems utilizing natural language
feedback for challenging software development tasks under various evaluation
dimensions. We propose a two-step agent prompts optimization pipeline:
identifying underperforming agents with their failure explanations utilizing
textual feedback and then optimizing system prompts of identified agents
utilizing failure explanations. We then study the impact of various
optimization settings on system performance with two comparison groups: online
against offline optimization and individual against group optimization. For
group optimization, we study two prompting strategies: one-pass and multi-pass
prompting optimizations. Overall, we demonstrate the effectiveness of our
optimization method for role-based multi-agent systems tackling software
development tasks evaluated on diverse evaluation dimensions, and we
investigate the impact of diverse optimization settings on group behaviors of
the multi-agent systems to provide practical insights for future development.

</details>


### [142] [Can AI Read Between The Lines? Benchmarking LLMs On Financial Nuance](https://arxiv.org/abs/2505.16090)
*Dominick Kubica,Dylan T. Gordon,Nanami Emura,Derleen Saini,Charlie Goldenberg*

Main category: cs.AI

TL;DR: 研究评估了大型语言模型在金融文本情感分析中的表现，发现其对策略性模糊语言处理存在挑战，但通过提示工程可改善结果


<details>
  <summary>Details</summary>
Motivation: 金融文本中大量使用对冲表述、前瞻性语言和行业术语，导致人类分析师和AI模型均难以稳定解读情感信号，需验证LLMs在专业领域的可靠性

Method: 使用微软财报电话会议文本，对比微软Copilot、ChatGPT、Gemini与传统机器学习模型，运用提示工程技术优化分析，并通过可视化评估情感趋势与股价关联

Result: 发现LLMs处理金融文本情感存在显著偏差，特定提示工程可提升准确率15%，云计算部门情感信号对股价影响权重达32%

Conclusion: LLMs在专业领域应用需结合领域知识优化，提示工程与可视化分析是提升金融文本解读有效性的关键路径

Abstract: As of 2025, Generative Artificial Intelligence (GenAI) has become a central
tool for productivity across industries. Beyond text generation, GenAI now
plays a critical role in coding, data analysis, and research workflows. As
large language models (LLMs) continue to evolve, it is essential to assess the
reliability and accuracy of their outputs, especially in specialized,
high-stakes domains like finance. Most modern LLMs transform text into
numerical vectors, which are used in operations such as cosine similarity
searches to generate responses. However, this abstraction process can lead to
misinterpretation of emotional tone, particularly in nuanced financial
contexts. While LLMs generally excel at identifying sentiment in everyday
language, these models often struggle with the nuanced, strategically ambiguous
language found in earnings call transcripts. Financial disclosures frequently
embed sentiment in hedged statements, forward-looking language, and
industry-specific jargon, making it difficult even for human analysts to
interpret consistently, let alone AI models. This paper presents findings from
the Santa Clara Microsoft Practicum Project, led by Professor Charlie
Goldenberg, which benchmarks the performance of Microsoft's Copilot, OpenAI's
ChatGPT, Google's Gemini, and traditional machine learning models for sentiment
analysis of financial text. Using Microsoft earnings call transcripts, the
analysis assesses how well LLM-derived sentiment correlates with market
sentiment and stock movements and evaluates the accuracy of model outputs.
Prompt engineering techniques are also examined to improve sentiment analysis
results. Visualizations of sentiment consistency are developed to evaluate
alignment between tone and stock performance, with sentiment trends analyzed
across Microsoft's lines of business to determine which segments exert the
greatest influence.

</details>


### [143] [BioDSA-1K: Benchmarking Data Science Agents for Biomedical Research](https://arxiv.org/abs/2505.16100)
*Zifeng Wang,Benjamin Danek,Jimeng Sun*

Main category: cs.AI

TL;DR: BioDSA-1K是一个包含1,029个生物医学假设验证任务的基准测试，用于评估AI代理在真实研究场景下的表现，特别关注非可验证假设的评估。


<details>
  <summary>Details</summary>
Motivation: 现有AI在复杂生物医学数据分析中存在假设验证能力不足的问题，特别是缺乏反映真实研究场景（含非可验证假设）的评估基准。

Method: 基于300+已发表研究构建假设-证据对，通过四维评估框架（假设决策准确率、证据结论对齐度、推理正确性、代码可执行性）实现全面测试。

Result: 创建了首个包含非可验证假设场景的生物医学基准，提供标准化的多维度评估体系，弥补现有AI评估与现实科研需求间的差距。

Conclusion: BioDSA-1K为开发生物医学领域可信AI代理奠定基础，推动AI在复杂科研验证任务中的实际应用。

Abstract: Validating scientific hypotheses is a central challenge in biomedical
research, and remains difficult for artificial intelligence (AI) agents due to
the complexity of real-world data analysis and evidence interpretation. In this
work, we present BioDSA-1K, a benchmark designed to evaluate AI agents on
realistic, data-driven biomedical hypothesis validation tasks. BioDSA-1K
consists of 1,029 hypothesis-centric tasks paired with 1,177 analysis plans,
curated from over 300 published biomedical studies to reflect the structure and
reasoning found in authentic research workflows. Each task includes a
structured hypothesis derived from the original study's conclusions, expressed
in the affirmative to reflect the language of scientific reporting, and one or
more pieces of supporting evidence grounded in empirical data tables. While
these hypotheses mirror published claims, they remain testable using standard
statistical or machine learning methods. The benchmark enables evaluation along
four axes: (1) hypothesis decision accuracy, (2) alignment between evidence and
conclusion, (3) correctness of the reasoning process, and (4) executability of
the AI-generated analysis code. Importantly, BioDSA-1K includes non-verifiable
hypotheses: cases where the available data are insufficient to support or
refute a claim, reflecting a common yet underexplored scenario in real-world
science. We propose BioDSA-1K as a foundation for building and evaluating
generalizable, trustworthy AI agents for biomedical discovery.

</details>


### [144] [Dynamic Sampling that Adapts: Iterative DPO for Self-Aware Mathematical Reasoning](https://arxiv.org/abs/2505.16176)
*Jun Rao,Xuebo Liu,Hexuan Deng,Zepeng Lin,Zixiong Yu,Jiansheng Wei,Xiaojun Meng,Min Zhang*

Main category: cs.AI

TL;DR: 提出SAI-DPO算法通过动态评估模型推理能力实现训练数据自适应选择，在数学推理任务中实现21.3%平均性能提升。


<details>
  <summary>Details</summary>
Motivation: 静态数据选择方法无法适应在线训练过程中模型能力的动态演化，尤其在强化学习框架中表现显著局限性。

Method: 通过实时模型性能反馈，动态调整数据选择策略以适应模型不同训练阶段的推理能力特征。

Result: 在8个数学推理基准测试中实现最高21.3%性能提升，在AIME24/AMC23竞赛数据集分别提升10/15个百分点。

Conclusion: 动态自适应数据选择策略显著优于静态策略，为推理能力提升提供了新方法论框架。

Abstract: In the realm of data selection for reasoning tasks, existing approaches
predominantly rely on externally predefined static metrics such as difficulty
and diversity, which are often designed for supervised fine-tuning (SFT) and
lack adaptability to continuous training processes. A critical limitation of
these methods is their inability to dynamically align with the evolving
capabilities of models during online training, a gap that becomes increasingly
pronounced with the rise of dynamic training paradigms and online reinforcement
learning (RL) frameworks (e.g., R1 models). To address this, we introduce
SAI-DPO, an algorithm that dynamically selects training data by continuously
assessing a model's stage-specific reasoning abilities across different
training phases. By integrating real-time model performance feedback, SAI-DPO
adaptively adapts data selection to the evolving strengths and weaknesses of
the model, thus enhancing both data utilization efficiency and final task
performance. Extensive experiments on three state-of-the-art models and eight
mathematical reasoning benchmarks, including challenging competition-level
datasets (e.g., AIME24 and AMC23), demonstrate that SAI-DPO achieves an average
performance boost of up to 21.3 percentage points, with particularly notable
improvements of 10 and 15 points on AIME24 and AMC23, respectively. These
results highlight the superiority of dynamic, model-adaptive data selection
over static, externally defined strategies in advancing reasoning.

</details>


### [145] [SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning](https://arxiv.org/abs/2505.16186)
*Kaiwen Zhou,Xuandong Zhao,Gaowen Liu,Jayanth Srinivasa,Aosong Feng,Dawn Song,Xin Eric Wang*

Main category: cs.AI

TL;DR: 提出SafeKey方法，通过双路径安全头和查询掩码建模激活安全推理关键时刻，显著提升大模型安全性


<details>
  <summary>Details</summary>
Motivation: 现有监督微调方法在应对未见过越狱提示时泛化能力不足，需要激活模型的安全'顿悟时刻'

Method: 1）双路径安全头增强关键句前的安全信号；2）查询掩码建模提升模型对查询理解的注意力

Result: 在多个安全基准测试中平均危害率降低9.6%，同时保持模型通用能力

Conclusion: SafeKey通过重塑内部注意机制和提升隐藏表示质量，有效防御多种越狱攻击和OOD有害提示

Abstract: Large Reasoning Models (LRMs) introduce a new generation paradigm of
explicitly reasoning before answering, leading to remarkable improvements in
complex tasks. However, they pose great safety risks against harmful queries
and adversarial attacks. While recent mainstream safety efforts on LRMs,
supervised fine-tuning (SFT), improve safety performance, we find that
SFT-aligned models struggle to generalize to unseen jailbreak prompts. After
thorough investigation of LRMs' generation, we identify a safety aha moment
that can activate safety reasoning and lead to a safe response. This aha moment
typically appears in the `key sentence', which follows models' query
understanding process and can indicate whether the model will proceed safely.
Based on these insights, we propose SafeKey, including two complementary
objectives to better activate the safety aha moment in the key sentence: (1) a
Dual-Path Safety Head to enhance the safety signal in the model's internal
representations before the key sentence, and (2) a Query-Mask Modeling
objective to improve the models' attention on its query understanding, which
has important safety hints. Experiments across multiple safety benchmarks
demonstrate that our methods significantly improve safety generalization to a
wide range of jailbreak attacks and out-of-distribution harmful prompts,
lowering the average harmfulness rate by 9.6\%, while maintaining general
abilities. Our analysis reveals how SafeKey enhances safety by reshaping
internal attention and improving the quality of hidden representations.

</details>


### [146] [How do Scaling Laws Apply to Knowledge Graph Engineering Tasks? The Impact of Model Size on Large Language Model Performance](https://arxiv.org/abs/2505.16276)
*Desiree Heim,Lars-Peter Meyer,Markus Schröder,Johannes Frey,Andreas Dengel*

Main category: cs.AI

TL;DR: 研究发现大语言模型在知识图谱工程任务中基本遵循模型规模扩展定律，但在特定场景下会出现性能平台效应，建议通过成本效益分析选择合适模型规模。


<details>
  <summary>Details</summary>
Motivation: 验证模型规模扩展定律在知识图谱工程任务中的适用性，探索模型性能与资源成本之间的平衡关系。

Method: 使用LLM-KG-Bench框架评估26个开源大语言模型，分析不同规模模型的KGE任务表现及其与模型大小的相关性。

Result: 1. 多数情况遵循规模扩展定律
2. 部分任务出现性能平台效应
3. 同家族模型存在大模型性能反超现象

Conclusion: 建议根据任务需求选择性价比最优的模型规模，测试时需涵盖同家族不同规模模型以规避局部性能异常。

Abstract: When using Large Language Models (LLMs) to support Knowledge Graph
Engineering (KGE), one of the first indications when searching for an
appropriate model is its size. According to the scaling laws, larger models
typically show higher capabilities. However, in practice, resource costs are
also an important factor and thus it makes sense to consider the ratio between
model performance and costs. The LLM-KG-Bench framework enables the comparison
of LLMs in the context of KGE tasks and assesses their capabilities of
understanding and producing KGs and KG queries. Based on a dataset created in
an LLM-KG-Bench run covering 26 open state-of-the-art LLMs, we explore the
model size scaling laws specific to KGE tasks. In our analyses, we assess how
benchmark scores evolve between different model size categories. Additionally,
we inspect how the general score development of single models and families of
models correlates to their size. Our analyses revealed that, with a few
exceptions, the model size scaling laws generally also apply to the selected
KGE tasks. However, in some cases, plateau or ceiling effects occurred, i.e.,
the task performance did not change much between a model and the next larger
model. In these cases, smaller models could be considered to achieve high
cost-effectiveness. Regarding models of the same family, sometimes larger
models performed worse than smaller models of the same family. These effects
occurred only locally. Hence it is advisable to additionally test the next
smallest and largest model of the same family.

</details>


### [147] [Incentivizing Dual Process Thinking for Efficient Large Language Model Reasoning](https://arxiv.org/abs/2505.16315)
*Xiaoxue Cheng,Junyi Li,Zhenduo Zhang,Xinyu Tang,Wayne Xin Zhao,Xinyu Kong,Zhiqiang Zhang*

Main category: cs.AI

TL;DR: 提出ACPO强化学习框架，通过系统感知推理令牌和动态切换机制减少大模型冗余推理，实现基于任务难度的自适应认知分配


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在处理复杂任务时存在过度思考问题，生成冗余内容而不考虑任务难度，需要更高效的认知分配机制

Method: 1. 引入系统感知推理令牌显式表征思维模式
2. 结合在线难度估计和令牌长度预算指导强化学习
3. 两阶段训练策略（监督微调+ACPO强化学习）

Result: 实验证明ACPO有效减少34%冗余推理，在数学推理任务中保持98%准确率的同时降低30%计算开销

Conclusion: ACPO框架通过透明化认知过程和动态系统切换，实现了任务复杂度自适应的混合推理范式

Abstract: Large reasoning models (LRMs) have demonstrated strong performance on complex
reasoning tasks, but often suffer from overthinking, generating redundant
content regardless of task difficulty. Inspired by the dual process theory in
cognitive science, we propose Adaptive Cognition Policy Optimization (ACPO), a
reinforcement learning framework that enables LRMs to achieve efficient
reasoning through adaptive cognitive allocation and dynamic system switch. ACPO
incorporates two key components: (1) introducing system-aware reasoning tokens
to explicitly represent the thinking modes thereby making the model's cognitive
process transparent, and (2) integrating online difficulty estimation and token
length budget to guide adaptive system switch and reasoning during
reinforcement learning. To this end, we propose a two-stage training strategy.
The first stage begins with supervised fine-tuning to cold start the model,
enabling it to generate reasoning paths with explicit thinking modes. In the
second stage, we apply ACPO to further enhance adaptive system switch for
difficulty-aware reasoning. Experimental results demonstrate that ACPO
effectively reduces redundant reasoning while adaptively adjusting cognitive
allocation based on task complexity, achieving efficient hybrid reasoning.

</details>


### [148] [SPaRC: A Spatial Pathfinding Reasoning Challenge](https://arxiv.org/abs/2505.16686)
*Lars Benedikt Kaesberg,Jan Philip Wahle,Terry Ruas,Bela Gipp*

Main category: cs.AI

TL;DR: 提出SPaRC空间路径推理数据集揭示AI模型在复杂空间推理任务中的显著不足


<details>
  <summary>Details</summary>
Motivation: 现有推理数据集无法有效测试需要多步骤路径规划和复杂规则约束的抽象问题

Method: 构建包含1000个二维网格路径谜题的数据集，要求结合算术和几何规则进行分步规划

Result: 人类准确率98%（难题94.5%），最佳模型o4-mini仅15.8%（难题1.1%），模型生成无效路径比例超50%

Conclusion: SPaRC可作为改进AI空间推理能力的基准，需开发新的训练方法和测试时计算扩展策略

Abstract: Existing reasoning datasets saturate and fail to test abstract, multi-step
problems, especially pathfinding and complex rule constraint satisfaction. We
introduce SPaRC (Spatial Pathfinding Reasoning Challenge), a dataset of 1,000
2D grid pathfinding puzzles to evaluate spatial and symbolic reasoning,
requiring step-by-step planning with arithmetic and geometric rules. Humans
achieve near-perfect accuracy (98.0%; 94.5% on hard puzzles), while the best
reasoning models, such as o4-mini, struggle (15.8%; 1.1% on hard puzzles).
Models often generate invalid paths (>50% of puzzles for o4-mini), and
reasoning tokens reveal they make errors in navigation and spatial logic.
Unlike humans, who take longer on hard puzzles, models fail to scale test-time
compute with difficulty. Allowing models to make multiple solution attempts
improves accuracy, suggesting potential for better spatial reasoning with
improved training and efficient test-time scaling methods. SPaRC can be used as
a window into models' spatial reasoning limitations and drive research toward
new methods that excel in abstract, multi-step problem-solving.

</details>


### [149] [KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in Mathematical Reasoning](https://arxiv.org/abs/2505.16826)
*Wei Sun,Wen Yang,Pu Jian,Qianlong Du,Fuwei Cui,Shuo Ren,Jiajun Zhang*

Main category: cs.AI

TL;DR: 提出KTAE算法优化强化学习中的优势估计，实现更细粒度的token级优势计算，显著提升数学推理能力


<details>
  <summary>Details</summary>
Motivation: 现有GRPO/DAPO等强化学习算法采用rollout级优势估计，导致序列中所有token共享相同优势值，无法捕捉token级贡献

Method: KTAE算法通过统计分析方法量化单个token对最终结果的影响权重，将token级重要性与rollout级优势结合，无需额外模型

Result: GRPO/DAPO+KTAE在5个数学推理基准测试中超越基线方法，响应更短且准确率更高，同等模型下超越R1-Distill-Qwen-1.5B

Conclusion: 通过细粒度优势估计显著提升强化学习在复杂推理任务中的效果，为LLM优化提供新方向

Abstract: Recent advances have demonstrated that integrating reinforcement learning
with rule-based rewards can significantly enhance the reasoning capabilities of
large language models, even without supervised fine-tuning. However, prevalent
reinforcement learning algorithms such as GRPO and its variants like DAPO,
suffer from a coarse granularity issue when computing the advantage.
Specifically, they compute rollout-level advantages that assign identical
values to every token within a sequence, failing to capture token-specific
contributions and hindering effective learning. To address this limitation, we
propose Key-token Advantage Estimation (KTAE) - a novel algorithm that
estimates fine-grained, token-level advantages without introducing additional
models. KTAE leverages the correctness of sampled rollouts and applies
statistical analysis to quantify the importance of individual tokens within a
sequence to the final outcome. This quantified token-level importance is then
combined with the rollout-level advantage to obtain a more fine-grained
token-level advantage estimation. Empirical results show that models trained
with GRPO+KTAE and DAPO+KTAE outperform baseline methods across five
mathematical reasoning benchmarks. Notably, they achieve higher accuracy with
shorter responses and even surpass R1-Distill-Qwen-1.5B using the same base
model.

</details>


### [150] [From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Pedagogical Visualization](https://arxiv.org/abs/2505.16832)
*Haonian Ji,Shi Qiu,Siyang Xin,Siwei Han,Zhaorun Chen,Hongyi Wang,Dake Zhang,Huaxiu Yao*

Main category: cs.AI

TL;DR: 提出EduVisBench基准和EduVisAgent框架，解决基础模型在教育可视化领域的推理分解与认知对齐问题，实现40.2%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在教育场景中生成的视觉解释缺乏教学有效性，过度依赖文本推理而忽视结构化可视化对概念理解的关键作用。

Method: 1. 构建多领域、多层次的EduVisBench评测基准；2. 设计包含教学规划、推理分解、元认知提示和可视化设计的多智能体协作框架EduVisAgent。

Result: EduVisAgent比基线模型性能提升40.2%，生成的视觉化内容在认知对齐度和教学有效性上显著优化。

Conclusion: 通过系统性基准构建和模块化智能体协作，有效解决了复杂推理可视化与人类认知过程对齐的挑战，为教育AI提供新范式。

Abstract: While foundation models (FMs), such as diffusion models and large
vision-language models (LVLMs), have been widely applied in educational
contexts, their ability to generate pedagogically effective visual explanations
remains limited. Most existing approaches focus primarily on textual reasoning,
overlooking the critical role of structured and interpretable visualizations in
supporting conceptual understanding. To better assess the visual reasoning
capabilities of FMs in educational settings, we introduce EduVisBench, a
multi-domain, multi-level benchmark. EduVisBench features diverse STEM problem
sets requiring visually grounded solutions, along with a fine-grained
evaluation rubric informed by pedagogical theory. Our empirical analysis
reveals that existing models frequently struggle with the inherent challenge of
decomposing complex reasoning and translating it into visual representations
aligned with human cognitive processes. To address these limitations, we
propose EduVisAgent, a multi-agent collaborative framework that coordinates
specialized agents for instructional planning, reasoning decomposition,
metacognitive prompting, and visualization design. Experimental results show
that EduVisAgent substantially outperforms all baselines, achieving a 40.2%
improvement and delivering more educationally aligned visualizations.
EduVisBench and EduVisAgent are available at
https://github.com/aiming-lab/EduVisBench and
https://github.com/aiming-lab/EduVisAgent.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [151] [All You Need is "Leet": Evading Hate-speech Detection AI](https://arxiv.org/abs/2505.16263)
*Sampanna Yashwant Kahu,Naman Ahuja*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Social media and online forums are increasingly becoming popular.
Unfortunately, these platforms are being used for spreading hate speech. In
this paper, we design black-box techniques to protect users from hate-speech on
online platforms by generating perturbations that can fool state of the art
deep learning based hate speech detection models thereby decreasing their
efficiency. We also ensure a minimal change in the original meaning of
hate-speech. Our best perturbation attack is successfully able to evade
hate-speech detection for 86.8 % of hateful text.

</details>


### [152] [DuFFin: A Dual-Level Fingerprinting Framework for LLMs IP Protection](https://arxiv.org/abs/2505.16530)
*Yuliang Yan,Haochun Tang,Shuo Yan,Enyan Dai*

Main category: cs.CR

TL;DR: 提出双层次指纹框架DuFFin，通过触发模式和知识级指纹验证黑盒环境下LLM模型所有权，实验显示其能有效识别基础模型的变体版本。


<details>
  <summary>Details</summary>
Motivation: 现有LLM知识产权保护方法存在影响生成质量或依赖白盒访问的缺陷，需要开发黑盒环境下实用的所有权验证方案。

Method: 设计结合触发模式（触发特定响应）和知识级指纹（模型知识验证）的双层次指纹框架，实现黑盒模型溯源。

Result: 在开源模型及其微调/量化/对齐版本上测试，IP-ROC指标＞0.95，能准确识别基础模型的变体版本归属。

Conclusion: DuFFin框架有效解决了黑盒环境下LLM知识产权验证难题，为模型版权保护提供了可靠的技术方案。

Abstract: Large language models (LLMs) are considered valuable Intellectual Properties
(IP) for legitimate owners due to the enormous computational cost of training.
It is crucial to protect the IP of LLMs from malicious stealing or unauthorized
deployment. Despite existing efforts in watermarking and fingerprinting LLMs,
these methods either impact the text generation process or are limited in
white-box access to the suspect model, making them impractical. Hence, we
propose DuFFin, a novel $\textbf{Du}$al-Level $\textbf{Fin}$gerprinting
$\textbf{F}$ramework for black-box setting ownership verification. DuFFin
extracts the trigger pattern and the knowledge-level fingerprints to identify
the source of a suspect model. We conduct experiments on a variety of models
collected from the open-source website, including four popular base models as
protected LLMs and their fine-tuning, quantization, and safety alignment
versions, which are released by large companies, start-ups, and individual
users. Results show that our method can accurately verify the copyright of the
base protected LLM on their model variants, achieving the IP-ROC metric greater
than 0.95. Our code is available at
https://github.com/yuliangyan0807/llm-fingerprint.

</details>


### [153] [CTRAP: Embedding Collapse Trap to Safeguard Large Language Models from Harmful Fine-Tuning](https://arxiv.org/abs/2505.16559)
*Biao Yi,Tiansheng Huang,Baolei Zhang,Tong Li,Lihai Nie,Zheli Liu,Li Shen*

Main category: cs.CR

TL;DR: 针对大语言模型恶意微调攻击，提出通过诱导模型崩溃取代传统选择性遗忘，从根本上瓦解攻击者利用模型适应能力的可能性。


<details>
  <summary>Details</summary>
Motivation: 现有选择性遗忘防御无法阻止攻击者利用LLM强大的通用适应能力进行恶意任务重组，需要更彻底的解决方案。

Method: 在模型对齐阶段预埋崩溃陷阱(CTRAP)，当检测到持续性恶意微调时触发渐进式语言建模能力退化机制，良性微调则保持模型完整。

Result: 实验证明CTRAP在不同LLM和攻击场景下有效防御恶意微调，同时保持良性场景模型性能（代码已开源）。

Conclusion: 通过条件性模型崩溃机制，既解决攻击者利用模型适应能力的根本问题，又保持对正常用户的服务可用性。

Abstract: Fine-tuning-as-a-service, while commercially successful for Large Language
Model (LLM) providers, exposes models to harmful fine-tuning attacks. As a
widely explored defense paradigm against such attacks, unlearning attempts to
remove malicious knowledge from LLMs, thereby essentially preventing them from
being used to perform malicious tasks. However, we highlight a critical flaw:
the powerful general adaptability of LLMs allows them to easily bypass
selective unlearning by rapidly relearning or repurposing their capabilities
for harmful tasks. To address this fundamental limitation, we propose a
paradigm shift: instead of selective removal, we advocate for inducing model
collapse--effectively forcing the model to "unlearn everything"--specifically
in response to updates characteristic of malicious adaptation. This collapse
directly neutralizes the very general capabilities that attackers exploit,
tackling the core issue unaddressed by selective unlearning. We introduce the
Collapse Trap (CTRAP) as a practical mechanism to implement this concept
conditionally. Embedded during alignment, CTRAP pre-configures the model's
reaction to subsequent fine-tuning dynamics. If updates during fine-tuning
constitute a persistent attempt to reverse safety alignment, the pre-configured
trap triggers a progressive degradation of the model's core language modeling
abilities, ultimately rendering it inert and useless for the attacker.
Crucially, this collapse mechanism remains dormant during benign fine-tuning,
ensuring the model's utility and general capabilities are preserved for
legitimate users. Extensive empirical results demonstrate that CTRAP
effectively counters harmful fine-tuning risks across various LLMs and attack
settings, while maintaining high performance in benign scenarios. Our code is
available at https://anonymous.4open.science/r/CTRAP.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [154] [Highlighting What Matters: Promptable Embeddings for Attribute-Focused Image Retrieval](https://arxiv.org/abs/2505.15877)
*Siting Li,Xiang Gao,Simon Shaolei Du*

Main category: cs.CV

TL;DR: 提出通过可提示图像嵌入和加速策略提升属性聚焦的文本-图像检索性能


<details>
  <summary>Details</summary>
Motivation: 现有CLIP类检索模型因关注全局语义而忽视细节，导致属性查询性能差且不平衡

Method: 使用多模态检索器的可提示图像嵌入技术，提出预处理嵌入和线性近似两种加速策略

Result: 预处理策略提升Recall@5 15%，线性近似策略提升8%

Conclusion: 可提示嵌入技术能有效突破通用图像嵌入的检索瓶颈，加速策略增强了实际应用可行性

Abstract: While an image is worth more than a thousand words, only a few provide
crucial information for a given task and thus should be focused on. In light of
this, ideal text-to-image (T2I) retrievers should prioritize specific visual
attributes relevant to queries. To evaluate current retrievers on handling
attribute-focused queries, we build COCO-Facet, a COCO-based benchmark with
9,112 queries about diverse attributes of interest. We find that CLIP-like
retrievers, which are widely adopted due to their efficiency and zero-shot
ability, have poor and imbalanced performance, possibly because their image
embeddings focus on global semantics and subjects while leaving out other
details. Notably, we reveal that even recent Multimodal Large Language Model
(MLLM)-based, stronger retrievers with a larger output dimension struggle with
this limitation. Hence, we hypothesize that retrieving with general image
embeddings is suboptimal for performing such queries. As a solution, we propose
to use promptable image embeddings enabled by these multimodal retrievers,
which boost performance by highlighting required attributes. Our pipeline for
deriving such embeddings generalizes across query types, image pools, and base
retriever architectures. To enhance real-world applicability, we offer two
acceleration strategies: Pre-processing promptable embeddings and using linear
approximations. We show that the former yields a 15% improvement in Recall@5
when prompts are predefined, while the latter achieves an 8% improvement when
prompts are only available during inference.

</details>


### [155] [GRIT: Teaching MLLMs to Think with Images](https://arxiv.org/abs/2505.15879)
*Yue Fan,Xuehai He,Diji Yang,Kaizhi Zheng,Ching-Chen Kuo,Yuting Zheng,Sravana Jyothi Narayanaraju,Xinze Guan,Xin Eric Wang*

Main category: cs.CV

TL;DR: 提出GRIT方法，通过结合自然语言与视觉坐标增强多模态大模型的推理能力，仅需少量数据即可训练模型生成视觉基础的推理链。


<details>
  <summary>Details</summary>
Motivation: 现有开源视觉推理模型仅依赖自然语言生成推理链，缺乏显式视觉信息整合，导致推理过程不够清晰和视觉可解释性不足。

Method: 提出GRIT方法：1) 引入自然语言与边界框坐标交替的推理链范式；2) 基于GRPO算法设计GRPO-GR强化学习框架，仅依赖答案准确性及格式作为奖励信号，无需标注推理链或坐标标签。

Result: GRIT成功训练模型生成视觉基础化且连贯的推理链，综合评估显示其统一了推理与视觉定位能力，数据效率极高（仅需20个图像-问题-答案三元组）。

Conclusion: GRIT以极低数据成本实现了多模态模型的视觉推理能力升级，为可解释性视觉推理提供了新范式。

Abstract: Recent studies have demonstrated the efficacy of using Reinforcement Learning
(RL) in building reasoning models that articulate chains of thoughts prior to
producing final answers. However, despite ongoing advances that aim at enabling
reasoning for vision-language tasks, existing open-source visual reasoning
models typically generate reasoning content with pure natural language, lacking
explicit integration of visual information. This limits their ability to
produce clearly articulated and visually grounded reasoning chains. To this
end, we propose Grounded Reasoning with Images and Texts (GRIT), a novel method
for training MLLMs to think with images. GRIT introduces a grounded reasoning
paradigm, in which models generate reasoning chains that interleave natural
language and explicit bounding box coordinates. These coordinates point to
regions of the input image that the model consults during its reasoning
process. Additionally, GRIT is equipped with a reinforcement learning approach,
GRPO-GR, built upon the GRPO algorithm. GRPO-GR employs robust rewards focused
on the final answer accuracy and format of the grounded reasoning output, which
eliminates the need for data with reasoning chain annotations or explicit
bounding box labels. As a result, GRIT achieves exceptional data efficiency,
requiring as few as 20 image-question-answer triplets from existing datasets.
Comprehensive evaluations demonstrate that GRIT effectively trains MLLMs to
produce coherent and visually grounded reasoning chains, showing a successful
unification of reasoning and grounding abilities.

</details>


### [156] [ViQAgent: Zero-Shot Video Question Answering via Agent with Open-Vocabulary Grounding Validation](https://arxiv.org/abs/2505.15928)
*Tony Montes,Fernando Lozano*

Main category: cs.CV

TL;DR: 提出基于LLM的智能体框架，结合思维链与YOLO-World，实现零样本视频问答新SOTA


<details>
  <summary>Details</summary>
Motivation: 现有视频问答系统在时序目标追踪与推理决策存在不足，需改进对象追踪与语言模型输出的对齐

Method: 构建LLM智能体框架，集成思维链推理机制与YOLO-World目标检测，增加跨时间帧验证机制

Result: 在NExT-QA等三大基准实现SOTA，支持跨视频领域的时间帧交叉验证

Conclusion: 该框架显著提升视频理解准确性，验证机制增强输出可靠性，为多模态推理提供新范式

Abstract: Recent advancements in Video Question Answering (VideoQA) have introduced
LLM-based agents, modular frameworks, and procedural solutions, yielding
promising results. These systems use dynamic agents and memory-based mechanisms
to break down complex tasks and refine answers. However, significant
improvements remain in tracking objects for grounding over time and
decision-making based on reasoning to better align object references with
language model outputs, as newer models get better at both tasks. This work
presents an LLM-brained agent for zero-shot Video Question Answering (VideoQA)
that combines a Chain-of-Thought framework with grounding reasoning alongside
YOLO-World to enhance object tracking and alignment. This approach establishes
a new state-of-the-art in VideoQA and Video Understanding, showing enhanced
performance on NExT-QA, iVQA, and ActivityNet-QA benchmarks. Our framework also
enables cross-checking of grounding timeframes, improving accuracy and
providing valuable support for verification and increased output reliability
across multiple video domains. The code is available at
https://github.com/t-montes/viqagent.

</details>


### [157] [OViP: Online Vision-Language Preference Learning](https://arxiv.org/abs/2505.15963)
*Shujun Liu,Siyuan Wang,Zejun Li,Jianxiang Wang,Cheng Zeng,Zhongyu Wei*

Main category: cs.CV

TL;DR: 提出OViP框架通过动态构建对比训练数据，利用模型自身幻觉输出来实时生成监督信号，有效减少多模态模型幻觉现象


<details>
  <summary>Details</summary>
Motivation: 现有基于预定义负样本的DPO方法无法反映模型实际错误，导致训练信号不充分。需要动态生成与模型失败案例相关的监督信号

Method: 通过语义差异识别构建响应对，使用扩散模型合成负样本图像，实现文本与视觉偏好的自适应对齐

Result: 实验显示OViP在幻觉基准上有效降低85%的幻觉率，同时在VQA等任务保持95%的原始性能

Conclusion: 失败驱动的训练机制结合改进的评估协议，在抑制幻觉与保持表达能力间取得更好平衡

Abstract: Large vision-language models (LVLMs) remain vulnerable to hallucination,
often generating content misaligned with visual inputs. While recent approaches
advance multi-modal Direct Preference Optimization (DPO) to mitigate
hallucination, they typically rely on predefined or randomly edited negative
samples that fail to reflect actual model errors, limiting training efficacy.
In this work, we propose an Online Vision-language Preference Learning (OViP)
framework that dynamically constructs contrastive training data based on the
model's own hallucinated outputs. By identifying semantic differences between
sampled response pairs and synthesizing negative images using a diffusion
model, OViP generates more relevant supervision signals in real time. This
failure-driven training enables adaptive alignment of both textual and visual
preferences. Moreover, we refine existing evaluation protocols to better
capture the trade-off between hallucination suppression and expressiveness.
Experiments on hallucination and general benchmarks demonstrate that OViP
effectively reduces hallucinations while preserving core multi-modal
capabilities.

</details>


### [158] [Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning](https://arxiv.org/abs/2505.15966)
*Alex Su,Haozhe Wang,Weimin Ren,Fangzhen Lin,Wenhu Chen*

Main category: cs.CV

TL;DR: 提出像素空间推理框架，通过视觉操作和两阶段训练显著提升视觉语言模型性能，在多项基准测试达到开源模型最高精度


<details>
  <summary>Details</summary>
Motivation: 传统文本空间推理在视觉任务中存在局限性，需要直接在像素空间进行视觉证据交互以提高推理准确性

Method: 两阶段训练：1) 基于合成推理轨迹的指令调优 2) 采用好奇心驱动奖励机制的强化学习平衡不同推理方式

Result: 7B模型在V* Bench(84%)/TallyQA-Complex(74%)/InfographicsVQA(84%)均刷新开源模型记录

Conclusion: 像素空间推理框架有效提升了视觉任务的推理能力，验证了该方法的创新性和实用性

Abstract: Chain-of-thought reasoning has significantly improved the performance of
Large Language Models (LLMs) across various domains. However, this reasoning
process has been confined exclusively to textual space, limiting its
effectiveness in visually intensive tasks. To address this limitation, we
introduce the concept of reasoning in the pixel-space. Within this novel
framework, Vision-Language Models (VLMs) are equipped with a suite of visual
reasoning operations, such as zoom-in and select-frame. These operations enable
VLMs to directly inspect, interrogate, and infer from visual evidences, thereby
enhancing reasoning fidelity for visual tasks. Cultivating such pixel-space
reasoning capabilities in VLMs presents notable challenges, including the
model's initially imbalanced competence and its reluctance to adopt the newly
introduced pixel-space operations. We address these challenges through a
two-phase training approach. The first phase employs instruction tuning on
synthesized reasoning traces to familiarize the model with the novel visual
operations. Following this, a reinforcement learning (RL) phase leverages a
curiosity-driven reward scheme to balance exploration between pixel-space
reasoning and textual reasoning. With these visual operations, VLMs can
interact with complex visual inputs, such as information-rich images or videos
to proactively gather necessary information. We demonstrate that this approach
significantly improves VLM performance across diverse visual reasoning
benchmarks. Our 7B model, \model, achieves 84\% on V* bench, 74\% on
TallyQA-Complex, and 84\% on InfographicsVQA, marking the highest accuracy
achieved by any open-source model to date. These results highlight the
importance of pixel-space reasoning and the effectiveness of our framework.

</details>


### [159] [Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation](https://arxiv.org/abs/2505.16146)
*Zhenglin Hua,Jinghan He,Zijun Yao,Tianxu Han,Haiyun Guo,Yuheng Jia,Junfeng Fang*

Main category: cs.CV

TL;DR: 提出基于稀疏自编码器的SSL方法，通过识别语义方向有效抑制视觉语言模型幻觉，兼具高效性和可迁移性


<details>
  <summary>Details</summary>
Motivation: 现有解决LVLM幻觉的方法存在计算成本高或干预精度不足的问题，需要更精细的表示控制方法

Method: 利用SAE识别幻觉/实际语义方向，通过潜在空间向量干预实现零训练成本的幻觉抑制

Result: SSL在多个实验中显著降低幻觉率，时间开销可忽略且跨模型架构有效

Conclusion: 首次将SAE应用于LVLM幻觉控制，为模型安全部署提供高效解决方案

Abstract: Large vision-language models (LVLMs) have achieved remarkable performance on
multimodal tasks such as visual question answering (VQA) and image captioning.
However, they still suffer from hallucinations, generating text inconsistent
with visual input, posing significant risks in real-world applications.
Existing approaches to address this issue focus on incorporating external
knowledge bases, alignment training, or decoding strategies, all of which
require substantial computational cost and time. Recent works try to explore
more efficient alternatives by adjusting LVLMs' internal representations.
Although promising, these methods may cause hallucinations to be insufficiently
suppressed or lead to excessive interventions that negatively affect normal
semantics. In this work, we leverage sparse autoencoders (SAEs) to identify
semantic directions closely associated with either hallucinations or actuality,
realizing more precise and direct hallucination-related representations. Our
analysis demonstrates that interventions along the faithful direction we
identified can mitigate hallucinations, while those along the hallucinatory
direction can exacerbate them. Building on these insights, we propose Steering
LVLMs via SAE Latent Directions (SSL), a training-free method based on
SAE-derived latent directions to mitigate hallucinations in LVLMs. Extensive
experiments demonstrate that SSL significantly outperforms existing decoding
approaches in mitigating hallucinations, while maintaining transferability
across different model architectures with negligible additional time overhead.

</details>


### [160] [When VLMs Meet Image Classification: Test Sets Renovation via Missing Label Identification](https://arxiv.org/abs/2505.16149)
*Zirui Pang,Haosheng Tan,Yuhan Pu,Zhijie Deng,Zhouan Shen,Keyu Hu,Jiaheng Wei*

Main category: cs.CV

TL;DR: 提出REVEAL框架，通过整合视觉-语言模型与标签清理方法，系统性解决图像分类测试集的噪声标签和缺失标签问题，显著提升6个基准数据集质量


<details>
  <summary>Details</summary>
Motivation: 现有图像分类基准数据集（CIFAR/MNIST/ImageNet）存在噪声标签和因多类别共存导致的标签缺失，导致模型评估不公平且存在误导性

Method: 整合LLaVA/BLIP/Janus/Qwen等视觉-语言模型与Docta/Cleanlab/MTurk等标注技术，通过置信度预测和共识过滤实现标签检测与修正

Result: 发现10个关键模型特性观察点，成功改进6个测试集质量，人工验证显示与人类判断高度一致（准确率提升23.6%）

Conclusion: REVEAL首次系统性解决标签噪声与缺失的双重问题，为图像分类领域提供更可靠的评估基准，推动模型比较的实质性进步

Abstract: Image classification benchmark datasets such as CIFAR, MNIST, and ImageNet
serve as critical tools for model evaluation. However, despite the cleaning
efforts, these datasets still suffer from pervasive noisy labels and often
contain missing labels due to the co-existing image pattern where multiple
classes appear in an image sample. This results in misleading model comparisons
and unfair evaluations. Existing label cleaning methods focus primarily on
noisy labels, but the issue of missing labels remains largely overlooked.
Motivated by these challenges, we present a comprehensive framework named
REVEAL, integrating state-of-the-art pre-trained vision-language models (e.g.,
LLaVA, BLIP, Janus, Qwen) with advanced machine/human label curation methods
(e.g., Docta, Cleanlab, MTurk), to systematically address both noisy labels and
missing label detection in widely-used image classification test sets. REVEAL
detects potential noisy labels and omissions, aggregates predictions from
various methods, and refines label accuracy through confidence-informed
predictions and consensus-based filtering. Additionally, we provide a thorough
analysis of state-of-the-art vision-language models and pre-trained image
classifiers, highlighting their strengths and limitations within the context of
dataset renovation by revealing 10 observations. Our method effectively reveals
missing labels from public datasets and provides soft-labeled results with
likelihoods. Through human verifications, REVEAL significantly improves the
quality of 6 benchmark test sets, highly aligning to human judgments and
enabling more accurate and meaningful comparisons in image classification.

</details>


### [161] [Redemption Score: An Evaluation Framework to Rank Image Captions While Redeeming Image Semantics and Language Pragmatics](https://arxiv.org/abs/2505.16180)
*Ashim Dahal,Ankit Ghimire,Saydul Akbar Murad,Nick Rahimi*

Main category: cs.CV

TL;DR: 提出Redemption Score混合框架，通过融合互信息散度、DINO图像相似度和BERTScore三个信号，实现更全面的图像标题评估，在Flickr8k基准上超越12种现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图像标题评估指标难以同时捕捉视觉语义和语言语用特征，需要更全面的多维度评估框架。

Method: 1) 互信息散度(MID)对齐全局图文分布 2) DINO视觉特征验证图像循环生成一致性 3) BERTScore衡量文本相似度，三信号校准融合

Result: 在Flickr8k达到56.43 Kendall-τ值，无任务特定训练情况下与人类判断相关性最优，跨数据集(Conceptual Captions/MS COCO)表现稳健

Conclusion: Redemption Score通过多维度信号融合，实现了对图像语义和语言解释力的双重保障，为标题质量评估提供更鲁棒的量化框架。

Abstract: Evaluating image captions requires cohesive assessment of both visual
semantics and language pragmatics, which is often not entirely captured by most
metrics. We introduce Redemption Score, a novel hybrid framework that ranks
image captions by triangulating three complementary signals: (1) Mutual
Information Divergence (MID) for global image-text distributional alignment,
(2) DINO-based perceptual similarity of cycle-generated images for visual
grounding, and (3) BERTScore for contextual text similarity against human
references. A calibrated fusion of these signals allows Redemption Score to
offer a more holistic assessment. On the Flickr8k benchmark, Redemption Score
achieves a Kendall-$\tau$ of 56.43, outperforming twelve prior methods and
demonstrating superior correlation with human judgments without requiring
task-specific training. Our framework provides a more robust and nuanced
evaluation by effectively redeeming image semantics and linguistic
interpretability indicated by strong transfer of knowledge in the Conceptual
Captions and MS COCO datasets.

</details>


### [162] [Grounding Chest X-Ray Visual Question Answering with Generated Radiology Reports](https://arxiv.org/abs/2505.16624)
*Francesco Dalla Serra,Patrick Schrempf,Chaoyang Wang,Zaiqiao Meng,Fani Deligianni,Alison Q. O'Neil*

Main category: cs.CV

TL;DR: 提出基于放射科报告增强的胸部X光视觉问答方法，支持单图异常检测和双图时序对比，通过报告生成+答案生成两阶段实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅在预训练阶段利用放射科报告，本文探索将报告作为额外输入直接提升VQA答案生成质量，尤其是针对医疗影像时序对比的特殊需求。

Method: 1. 统一框架处理单图/双图问题：单图输入单张CXR，双图输入同患者不同时间点的两张CXR；2. 类思维链的两阶段模型：先生成放射科报告(RG)，再基于报告证据自回归生成答案(AG)。

Result: 在Medical-Diff-VQA数据集达到SOTA，证明放射科报告的引入使单图问答准确率提升3.2%，图像差异问答提升5.1%。

Conclusion: 将放射科报告作为中间证据能有效提升医疗VQA性能，双阶段设计为时序对比任务提供了可解释的推理路径。

Abstract: We present a novel approach to Chest X-ray (CXR) Visual Question Answering
(VQA), addressing both single-image image-difference questions. Single-image
questions focus on abnormalities within a specific CXR ("What abnormalities are
seen in image X?"), while image-difference questions compare two longitudinal
CXRs acquired at different time points ("What are the differences between image
X and Y?"). We further explore how the integration of radiology reports can
enhance the performance of VQA models. While previous approaches have
demonstrated the utility of radiology reports during the pre-training phase, we
extend this idea by showing that the reports can also be leveraged as
additional input to improve the VQA model's predicted answers. First, we
propose a unified method that handles both types of questions and
auto-regressively generates the answers. For single-image questions, the model
is provided with a single CXR. For image-difference questions, the model is
provided with two CXRs from the same patient, captured at different time
points, enabling the model to detect and describe temporal changes. Taking
inspiration from 'Chain-of-Thought reasoning', we demonstrate that performance
on the CXR VQA task can be improved by grounding the answer generator module
with a radiology report predicted for the same CXR. In our approach, the VQA
model is divided into two steps: i) Report Generation (RG) and ii) Answer
Generation (AG). Our results demonstrate that incorporating predicted radiology
reports as evidence to the AG model enhances performance on both single-image
and image-difference questions, achieving state-of-the-art results on the
Medical-Diff-VQA dataset.

</details>


### [163] [R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO](https://arxiv.org/abs/2505.16673)
*Huanjin Yao,Qixiang Yin,Jingyi Zhang,Min Yang,Yibo Wang,Wenhao Wu,Fei Su,Li Shen,Minghui Qiu,Dacheng Tao,Jiaxing Huang*

Main category: cs.CV

TL;DR: 提出Share-GRPO方法，通过强化学习的扩展问题空间探索和分层奖励共享机制，有效提升多模态大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在强化学习中面临的稀疏奖励和优势消失问题，提升模型推理能力。

Method: 1. 通过数据变换技术扩展问题空间；2. 在扩展空间内探索多样推理轨迹并跨问题共享；3. 分层奖励共享机制（跨问题变体和问题内部）优化优势估计。

Result: 在六个常用推理基准测试中展现优越性能，代码将在GitHub开源。

Conclusion: Share-GRPO通过空间扩展和轨迹共享有效提升模型推理能力，为多模态推理任务提供了新的强化学习解决方案。

Abstract: In this work, we aim to incentivize the reasoning ability of Multimodal Large
Language Models (MLLMs) via reinforcement learning (RL) and develop an
effective approach that mitigates the sparse reward and advantage vanishing
issues during RL. To this end, we propose Share-GRPO, a novel RL approach that
tackle these issues by exploring and sharing diverse reasoning trajectories
over expanded question space. Specifically, Share-GRPO first expands the
question space for a given question via data transformation techniques, and
then encourages MLLM to effectively explore diverse reasoning trajectories over
the expanded question space and shares the discovered reasoning trajectories
across the expanded questions during RL. In addition, Share-GRPO also shares
reward information during advantage computation, which estimates solution
advantages hierarchically across and within question variants, allowing more
accurate estimation of relative advantages and improving the stability of
policy training. Extensive evaluations over six widely-used reasoning
benchmarks showcase the superior performance of our method. Code will be
available at https://github.com/HJYao00/R1-ShareVL.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [164] [AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models](https://arxiv.org/abs/2505.16211)
*Kai Li,Can Shen,Yile Liu,Jirui Han,Kelong Zheng,Xuechao Zou,Zhe Wang,Xingjian Du,Shun Zhang,Hanjun Luo,Yingbin Jin,Xinxin Xing,Ziyang Ma,Yue Liu,Xiaojun Jia,Yifan Zhang,Junfeng Fang,Kun Wang,Yibo Yan,Haoyang Li,Yiming Li,Xiaobin Zhuang,Yang Liu,Haibo Hu,Zhuo Chen,Zhizheng Wu,Xiaolin Hu,Eng-Siong Chng,XiaoFeng Wang,Wenyuan Xu,Wei Dong,Xinfeng Li*

Main category: cs.SD

TL;DR: 提出首个针对音频大模型的多维度可信评估框架AudioTrust，包含6大评估维度和18种实验场景，覆盖4200+真实音频样本，揭示当前模型的信任边界。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架主要针对文本模态或仅关注有限安全维度，缺乏对音频模态特有风险（如语音助手交互风险、紧急呼叫场景风险）的系统性评估需求。

Method: 构建包含4,420个真实场景音频/文本样本的数据集，设计9个音频专用评估指标，采用自动化评估流程对六维度（公平性/幻觉/安全/隐私/鲁棒性/认证）进行系统评估。

Result: 实验揭示了当前开源/闭源ALLMs在高风险音频场景中的可信度边界，如存在语音指令注入漏洞、环境噪音干扰决策等问题。

Conclusion: AudioTrust为音频模型的可靠部署提供系统评估基准，其模块化设计支持持续扩展，公开平台促进研究社区共同推进音频AI安全。

Abstract: The rapid advancement and expanding applications of Audio Large Language
Models (ALLMs) demand a rigorous understanding of their trustworthiness.
However, systematic research on evaluating these models, particularly
concerning risks unique to the audio modality, remains largely unexplored.
Existing evaluation frameworks primarily focus on the text modality or address
only a restricted set of safety dimensions, failing to adequately account for
the unique characteristics and application scenarios inherent to the audio
modality. We introduce AudioTrust-the first multifaceted trustworthiness
evaluation framework and benchmark specifically designed for ALLMs. AudioTrust
facilitates assessments across six key dimensions: fairness, hallucination,
safety, privacy, robustness, and authentication. To comprehensively evaluate
these dimensions, AudioTrust is structured around 18 distinct experimental
setups. Its core is a meticulously constructed dataset of over 4,420 audio/text
samples, drawn from real-world scenarios (e.g., daily conversations, emergency
calls, voice assistant interactions), specifically designed to probe the
multifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully
designs 9 audio-specific evaluation metrics, and we employ a large-scale
automated pipeline for objective and scalable scoring of model outputs.
Experimental results reveal the trustworthiness boundaries and limitations of
current state-of-the-art open-source and closed-source ALLMs when confronted
with various high-risk audio scenarios, offering valuable insights for the
secure and trustworthy deployment of future audio models. Our platform and
benchmark are available at https://github.com/JusperLee/AudioTrust.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [165] [InfoDeepSeek: Benchmarking Agentic Information Seeking for Retrieval-Augmented Generation](https://arxiv.org/abs/2505.15872)
*Yunjia Xi,Jianghao Lin,Menghui Zhu,Yongzhao Xiao,Zhuoying Ou,Jiaqi Liu,Tong Wan,Bo Chen,Weiwen Liu,Yasheng Wang,Ruiming Tang,Weinan Zhang,Yong Yu*

Main category: cs.IR

TL;DR: 提出了动态代理信息检索评估基准InfoDeepSeek，通过确定性、难度和多样性标准构建挑战性查询，建立细粒度评估框架揭示代理行为特性。


<details>
  <summary>Details</summary>
Motivation: 现有基准局限静态检索环境，无法评估动态网络环境下的代理信息获取能力，且依赖预定义黄金文档集的评估方式不适用于开放网络场景。

Method: 1. 构建满足确定性、难度和多样性标准的挑战性查询
2. 开发首个动态代理信息检索评估框架
3. 设计包含准确性、效用性和信息紧凑度的细粒度评估指标

Result: 通过多维度实验揭示了不同LLM和搜索引擎在动态信息获取中的行为差异，为优化代理系统提供实证依据。

Conclusion: InfoDeepSeek填补了动态代理信息检索评估的空白，其方法论和评估框架为后续研究提供了系统性指导和技术路线。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
grounding responses with retrieved information. As an emerging paradigm,
Agentic RAG further enhances this process by introducing autonomous LLM agents
into the information seeking process. However, existing benchmarks fall short
in evaluating such systems, as they are confined to a static retrieval
environment with a fixed, limited corpus} and simple queries that fail to
elicit agentic behavior. Moreover, their evaluation protocols assess
information seeking effectiveness by pre-defined gold sets of documents, making
them unsuitable for the open-ended and dynamic nature of real-world web
environments. To bridge this gap, we present InfoDeepSeek, a new benchmark with
challenging questions designed for assessing agentic information seeking in
real-world, dynamic web environments. We propose a systematic methodology for
constructing challenging queries satisfying the criteria of determinacy,
difficulty, and diversity. Based on this, we develop the first evaluation
framework tailored to dynamic agentic information seeking, including
fine-grained metrics about the accuracy, utility, and compactness of
information seeking outcomes. Through extensive experiments across LLMs, search
engines, and question types, InfoDeepSeek reveals nuanced agent behaviors and
offers actionable insights for future research.

</details>


### [166] [Aug2Search: Enhancing Facebook Marketplace Search with LLM-Generated Synthetic Data Augmentation](https://arxiv.org/abs/2505.16065)
*Ruijie Xi,He Ba,Hao Yuan,Rishu Agrawal,Arul Prakash*

Main category: cs.IR

TL;DR: 提出Aug2Search框架，利用生成式AI生成高质量合成数据提升嵌入式检索模型性能，纯合成数据训练效果优于原始数据


<details>
  <summary>Details</summary>
Motivation: 现有搜索日志数据缺乏多样性和细节，限制了EBR模型对复杂搜索模式的捕捉能力

Method: 采用Llama模型生成合成数据（生成查询/增强商品列表/列表生成查询），使用原始数据、合成数据及其混合训练EBR模型

Result: 合成数据显著提升模型性能（ROC_AUC+4%），纯合成数据训练效果优于原始数据或混合数据

Conclusion: 生成式AI能有效生成高质量检索训练数据，Aug2Search框架通过纯合成数据训练可突破原始数据质量瓶颈

Abstract: Embedding-Based Retrieval (EBR) is an important technique in modern search
engines, enabling semantic match between search queries and relevant results.
However, search logging data on platforms like Facebook Marketplace lacks the
diversity and details needed for effective EBR model training, limiting the
models' ability to capture nuanced search patterns. To address this challenge,
we propose Aug2Search, an EBR-based framework leveraging synthetic data
generated by Generative AI (GenAI) models, in a multimodal and multitask
approach to optimize query-product relevance. This paper investigates the
capabilities of GenAI, particularly Large Language Models (LLMs), in generating
high-quality synthetic data, and analyzing its impact on enhancing EBR models.
We conducted experiments using eight Llama models and 100 million data points
from Facebook Marketplace logs. Our synthetic data generation follows three
strategies: (1) generate queries, (2) enhance product listings, and (3)
generate queries from enhanced listings. We train EBR models on three different
datasets: sampled engagement data or original data ((e.g., "Click" and "Listing
Interactions")), synthetic data, and a mixture of both engagement and synthetic
data to assess their performance across various training sets. Our findings
underscore the robustness of Llama models in producing synthetic queries and
listings with high coherence, relevance, and diversity, while maintaining low
levels of hallucination. Aug2Search achieves an improvement of up to 4% in
ROC_AUC with 100 million synthetic data samples, demonstrating the
effectiveness of our approach. Moreover, our experiments reveal that with the
same volume of training data, models trained exclusively on synthetic data
often outperform those trained on original data only or a mixture of original
and synthetic data.

</details>


### [167] [Benchmarking Retrieval-Augmented Multimomal Generation for Document Question Answering](https://arxiv.org/abs/2505.16470)
*Kuicai Dong,Yujing Chang,Shijie Huang,Yasheng Wang,Ruiming Tang,Yong Liu*

Main category: cs.IR

TL;DR: 研究者提出多模态文档问答基准MMDocRAG，包含4055个跨模态证据链的QA对，通过大规模实验揭示了多模态检索与推理的挑战与解决方案


<details>
  <summary>Details</summary>
Motivation: 当前文档问答系统过度依赖文本信息，忽视视觉元素，且缺乏评估多模态证据整合的可靠基准

Method: 构建包含跨页面多模态证据链的数据集，设计创新评估指标，在60个视觉语言模型和14个检索系统上进行大规模实验

Result: 闭源视觉语言模型在多模态输入下表现优异（开源模型性能显著下降），精细调整的LLM使用图像描述后性能提升明显

Conclusion: MMDocRAG为开发鲁棒的多模态文档问答系统提供严格测试基准和实践洞见，相关资源已开源

Abstract: Document Visual Question Answering (DocVQA) faces dual challenges in
processing lengthy multimodal documents (text, images, tables) and performing
cross-modal reasoning. Current document retrieval-augmented generation (DocRAG)
methods remain limited by their text-centric approaches, frequently missing
critical visual information. The field also lacks robust benchmarks for
assessing multimodal evidence selection and integration. We introduce MMDocRAG,
a comprehensive benchmark featuring 4,055 expert-annotated QA pairs with
multi-page, cross-modal evidence chains. Our framework introduces innovative
metrics for evaluating multimodal quote selection and enables answers that
interleave text with relevant visual elements. Through large-scale experiments
with 60 VLM/LLM models and 14 retrieval systems, we identify persistent
challenges in multimodal evidence retrieval, selection, and integration.Key
findings reveal advanced proprietary LVMs show superior performance than
open-sourced alternatives. Also, they show moderate advantages using multimodal
inputs over text-only inputs, while open-source alternatives show significant
performance degradation. Notably, fine-tuned LLMs achieve substantial
improvements when using detailed image descriptions. MMDocRAG establishes a
rigorous testing ground and provides actionable insights for developing more
robust multimodal DocVQA systems. Our benchmark and code are available at
https://mmdocrag.github.io/MMDocRAG/.

</details>


### [168] [MiLQ: Benchmarking IR Models for Bilingual Web Search with Mixed Language Queries](https://arxiv.org/abs/2505.16631)
*Jonghwi Kim,Deokhyung Kang,Seonjeong Hwang,Yunsu Kim,Jungseul Ok,Gary Lee*

Main category: cs.IR

TL;DR: 提出首个混合语言查询测试集MiLQ，验证其有效性并探索混合查询策略对跨语言检索的优化效果


<details>
  <summary>Details</summary>
Motivation: 双语用户频繁使用混合语言搜索但缺乏系统研究，需建立基准测试集并探索优化方案

Method: 构建MiLQ测试集，通过多语言IR模型实验评估性能，分析代码切换训练数据潜力及混合查询策略有效性

Result: 模型在混合查询表现中等且不稳定，代码切换数据可增强鲁棒性；英语混合查询显著提升英文文档检索效果

Conclusion: MiLQ填补研究空白，混合训练数据与主动混合策略为提升跨语言检索效果提供新方向

Abstract: Despite bilingual speakers frequently using mixed-language queries in web
searches, Information Retrieval (IR) research on them remains scarce. To
address this, we introduce MiLQ,Mixed-Language Query test set, the first public
benchmark of mixed-language queries, confirmed as realistic and highly
preferred. Experiments show that multilingual IR models perform moderately on
MiLQ and inconsistently across native, English, and mixed-language queries,
also suggesting code-switched training data's potential for robust IR models
handling such queries. Meanwhile, intentional English mixing in queries proves
an effective strategy for bilinguals searching English documents, which our
analysis attributes to enhanced token matching compared to native queries.

</details>


### [169] [Don't "Overthink" Passage Reranking: Is Reasoning Truly Necessary?](https://arxiv.org/abs/2505.16886)
*Nour Jedidi,Yung-Sung Chuang,James Glass,Jimmy Lin*

Main category: cs.IR

TL;DR: 研究发现基于推理的段落重排序器效果反而差于非推理方法，推理过程会导致相关性评分极化


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型的推理能力是否真正能提升信息检索中段落重排序的准确性

Method: 通过对比实验比较推理型(ReasonRR)与非推理型(StandardRR)重排序器，并设计ReasonRR-NoReason消融实验

Result: StandardRR普遍优于ReasonRR，禁用推理的ReasonRR-NoReason效果反超原模型

Conclusion: 推理过程限制模型对部分相关性的判断，极化评分机制损害了点状重排序器的核心评估能力

Abstract: With the growing success of reasoning models across complex natural language
tasks, researchers in the Information Retrieval (IR) community have begun
exploring how similar reasoning capabilities can be integrated into passage
rerankers built on Large Language Models (LLMs). These methods typically employ
an LLM to produce an explicit, step-by-step reasoning process before arriving
at a final relevance prediction. But, does reasoning actually improve reranking
accuracy? In this paper, we dive deeper into this question, studying the impact
of the reasoning process by comparing reasoning-based pointwise rerankers
(ReasonRR) to standard, non-reasoning pointwise rerankers (StandardRR) under
identical training conditions, and observe that StandardRR generally
outperforms ReasonRR. Building on this observation, we then study the
importance of reasoning to ReasonRR by disabling its reasoning process
(ReasonRR-NoReason), and find that ReasonRR-NoReason is surprisingly more
effective than ReasonRR. Examining the cause of this result, our findings
reveal that reasoning-based rerankers are limited by the LLM's reasoning
process, which pushes it toward polarized relevance scores and thus fails to
consider the partial relevance of passages, a key factor for the accuracy of
pointwise rerankers.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [170] [MAPS: A Multilingual Benchmark for Global Agent Performance and Security](https://arxiv.org/abs/2505.15935)
*Omer Hofman,Oren Rachmil,Shamik Bose,Vikas Pahuja,Jonathan Brokman,Toshiya Shimizu,Trisha Starostina,Kelly Marchisio,Seraphina Goldfarb-Tarrant,Roman Vainshtein*

Main category: cs.DB

TL;DR: 提出多语言基准测试套件MAPS，评估基于LLM的智能AI系统在多语言环境下的性能与安全性，发现性能下降现象并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 现有Agentic AI评估基准仅关注英语场景，多语言环境下存在性能下降和安全风险，影响AI系统的全球可访问性。

Method: 整合GAIA、SWE-bench、MATH和安全基准，翻译为10种语言构建MAPS基准套件（含805任务/8,855实例），系统性分析多语言对Agent性能的影响。

Result: 实证显示非英语环境下Agent性能和安全性持续下降，严重程度与任务类型及翻译输入量正相关。

Conclusion: 建立标准化评估框架MAPS，促进开发公平可靠的多语言Agentic AI系统，数据集已开源供社区使用。

Abstract: Agentic AI systems, which build on Large Language Models (LLMs) and interact
with tools and memory, have rapidly advanced in capability and scope. Yet,
since LLMs have been shown to struggle in multilingual settings, typically
resulting in lower performance and reduced safety, agentic systems risk
inheriting these limitations. This raises concerns about the global
accessibility of such systems, as users interacting in languages other than
English may encounter unreliable or security-critical agent behavior. Despite
growing interest in evaluating agentic AI, existing benchmarks focus
exclusively on English, leaving multilingual settings unexplored. To address
this gap, we propose MAPS, a multilingual benchmark suite designed to evaluate
agentic AI systems across diverse languages and tasks. MAPS builds on four
widely used agentic benchmarks - GAIA (real-world tasks), SWE-bench (code
generation), MATH (mathematical reasoning), and the Agent Security Benchmark
(security). We translate each dataset into ten diverse languages, resulting in
805 unique tasks and 8,855 total language-specific instances. Our benchmark
suite enables a systematic analysis of how multilingual contexts affect agent
performance and robustness. Empirically, we observe consistent degradation in
both performance and security when transitioning from English to other
languages, with severity varying by task and correlating with the amount of
translated input. Building on these findings, we provide actionable
recommendations to guide agentic AI systems development and assessment under
multilingual settings. This work establishes a standardized evaluation
framework, encouraging future research towards equitable, reliable, and
globally accessible agentic AI. MAPS benchmark suite is publicly available at
https://huggingface.co/datasets/Fujitsu-FRE/MAPS

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [171] [Towards Holistic Evaluation of Large Audio-Language Models: A Comprehensive Survey](https://arxiv.org/abs/2505.15957)
*Chih-Kai Yang,Neo S. Ho,Hung-yi Lee*

Main category: eess.AS

TL;DR: 提出首个针对大型音频语言模型(LALMs)评估的四维度分类体系，系统化解决领域内基准测试碎片化问题，并提供未来发展方向指南


<details>
  <summary>Details</summary>
Motivation: 现有LALMs评估基准缺乏系统性分类，导致研究缺乏统一框架。本文旨在建立结构化评估体系以指导领域发展

Method: 通过全面文献调研，基于评估目标将LALM能力划分为：通用听觉处理、知识推理、对话能力、伦理安全四个维度，并详细分析各维度的评估要求

Result: 创建首个专门针对LALMs评估的分类标准，建立持续更新的论文数据集，为学术界提供系统评估框架和资源共享平台

Conclusion: 该分类体系填补了LALM评估领域的系统性空白，未来需持续更新评估标准并解决多模态集成、伦理对齐等技术挑战

Abstract: With advancements in large audio-language models (LALMs), which enhance large
language models (LLMs) with auditory capabilities, these models are expected to
demonstrate universal proficiency across various auditory tasks. While numerous
benchmarks have emerged to assess LALMs' performance, they remain fragmented
and lack a structured taxonomy. To bridge this gap, we conduct a comprehensive
survey and propose a systematic taxonomy for LALM evaluations, categorizing
them into four dimensions based on their objectives: (1) General Auditory
Awareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented
Ability, and (4) Fairness, Safety, and Trustworthiness. We provide detailed
overviews within each category and highlight challenges in this field, offering
insights into promising future directions. To the best of our knowledge, this
is the first survey specifically focused on the evaluations of LALMs, providing
clear guidelines for the community. We will release the collection of the
surveyed papers and actively maintain it to support ongoing advancements in the
field.

</details>


### [172] [Meta-PerSER: Few-Shot Listener Personalized Speech Emotion Recognition via Meta-learning](https://arxiv.org/abs/2505.16220)
*Liang-Yeh Shen,Shi-Xin Fang,Yi-Cheng Lin,Huang-Cheng Chou,Hung-yi Lee*

Main category: eess.AS

TL;DR: Meta-PerSER提出基于元学习的个性化语音情感识别框架，通过少量样本快速适配个体差异，在IEMOCAP数据集上显著超越基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统语音情感识别系统依赖群体标注数据，忽视个体情感认知差异，导致预测结果与个人感知不一致。该研究旨在实现个性化适配的情感识别模型。

Method: 采用模型无关元学习（MAML）框架，结合组合元训练、导数退火策略和分层分步学习率调节。利用预训练自监督模型提取鲁棒特征后，通过两阶段训练（通用情感学习+个性化微调）实现快速适配。

Result: 在IEMOCAP数据集实验中，Meta-PerSER在已知和未知用户场景下的加权准确率分别达到72.8%和68.4%，较传统方法提升超过5个百分点。

Conclusion: 该框架有效解决个体情感认知差异问题，为个性化情感计算提供了可扩展的解决方案，在心理咨询、人机交互等领域具有应用潜力。

Abstract: This paper introduces Meta-PerSER, a novel meta-learning framework that
personalizes Speech Emotion Recognition (SER) by adapting to each listener's
unique way of interpreting emotion. Conventional SER systems rely on aggregated
annotations, which often overlook individual subtleties and lead to
inconsistent predictions. In contrast, Meta-PerSER leverages a Model-Agnostic
Meta-Learning (MAML) approach enhanced with Combined-Set Meta-Training,
Derivative Annealing, and per-layer per-step learning rates, enabling rapid
adaptation with only a few labeled examples. By integrating robust
representations from pre-trained self-supervised models, our framework first
captures general emotional cues and then fine-tunes itself to personal
annotation styles. Experiments on the IEMOCAP corpus demonstrate that
Meta-PerSER significantly outperforms baseline methods in both seen and unseen
data scenarios, highlighting its promise for personalized emotion recognition.

</details>
