{"id": "2508.07240", "pdf": "https://arxiv.org/pdf/2508.07240", "abs": "https://arxiv.org/abs/2508.07240", "authors": ["Zixuan Li", "Zixiong Wang", "Jian Yang", "Milos Hasan", "Beibei Wang"], "title": "PureSample: Neural Materials Learned by Sampling Microgeometry", "categories": ["cs.GR"], "comment": null, "summary": "Traditional physically-based material models rely on analytically derived\nbidirectional reflectance distribution functions (BRDFs), typically by\nconsidering statistics of micro-primitives such as facets, flakes, or spheres,\nsometimes combined with multi-bounce interactions such as layering and multiple\nscattering. These derivations are often complex and model-specific, and\ntypically consider a statistical aggregate of a large surface area, ignoring\nspatial variation. Once an analytic BRDF's evaluation is defined, one still\nneeds to design an importance sampling method for it, and a way to evaluate the\npdf of that sampling distribution, requiring further model-specific\nderivations.\n  We present PureSample: a novel neural BRDF representation that allows\nlearning a material's behavior purely by sampling forward random walks on the\nmicrogeometry, which is usually straightforward to implement. Our\nrepresentation allows for efficient importance sampling, pdf evaluation, and\nBRDF evaluation, for homogeneous as well as spatially varying materials.\n  We achieve this by two learnable components: first, the sampling distribution\nis modeled using a flow matching neural network, which allows both importance\nsampling and pdf evaluation; second, we introduce a view-dependent albedo term,\ncaptured by a lightweight neural network, which allows for converting a scalar\npdf value to a colored BRDF value for any pair of view and light directions.\n  We demonstrate PureSample on challenging materials, including multi-layered\nmaterials, multiple-scattering microfacet materials, and various other\nmicrostructures.", "AI": {"tldr": "\u63d0\u51faPureSample\u2014\u2014\u57fa\u4e8e\u5fae\u51e0\u4f55\u968f\u673a\u884c\u8d70\u91c7\u6837\u7684\u795e\u7ecfBRDF\u8868\u793a\u65b9\u6cd5\uff0c\u5b9e\u73b0\u9ad8\u6548\u91cd\u8981\u6027\u91c7\u6837\u4e0e\u8bc4\u4f30\uff0c\u7a81\u7834\u4f20\u7edf\u5206\u6790\u6a21\u578b\u9650\u5236", "motivation": "\u4f20\u7edfBRDF\u5efa\u6a21\u4f9d\u8d56\u590d\u6742\u5206\u6790\u63a8\u5bfc\uff0c\u96be\u4ee5\u5904\u7406\u7a7a\u95f4\u53d8\u5316\u6750\u6599\u4e14\u9700\u91cd\u590d\u8bbe\u8ba1\u91c7\u6837\u65b9\u6cd5\uff0c\u4e9f\u9700\u901a\u7528\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848", "method": "\u6d41\u5339\u914d\u795e\u7ecf\u7f51\u7edc\u5efa\u6a21\u91c7\u6837\u5206\u5e03 + \u8f7b\u91cf\u7ea7\u89c6\u89d2\u76f8\u5173\u53cd\u7167\u7387\u7f51\u7edc\uff0c\u5b9e\u73b0\u6807\u91cfPDF\u5230\u5f69\u8272BRDF\u7684\u8f6c\u6362", "result": "\u5728\u591a\u5c42\u6750\u6599\u3001\u591a\u91cd\u6563\u5c04\u5fae\u8868\u9762\u7b49\u590d\u6742\u573a\u666f\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u652f\u6301\u5404\u5411\u5f02\u6027\u4e0e\u7a7a\u95f4\u53d8\u5316\u6750\u6599\u5efa\u6a21", "conclusion": "PureSample\u4e3a\u7269\u7406\u6750\u8d28\u5efa\u6a21\u63d0\u4f9b\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u5b9e\u73b0\u590d\u6742\u5ea6\u5e76\u63d0\u5347\u91c7\u6837\u6548\u7387"}}
{"id": "2508.07615", "pdf": "https://arxiv.org/pdf/2508.07615", "abs": "https://arxiv.org/abs/2508.07615", "authors": ["Chuanfu Hu", "Aimin Hou"], "title": "Verification Method for Graph Isomorphism Criteria", "categories": ["cs.GR"], "comment": "17 pages, 5 figures, 2 tables", "summary": "The criteria for determining graph isomorphism are crucial for solving graph\nisomorphism problems. The necessary condition is that two isomorphic graphs\npossess invariants, but their function can only be used to filtrate and\nsubdivide candidate spaces. The sufficient conditions are used to rebuild the\nisomorphic reconstruction of special graphs, but their drawback is that the\nisomorphic functions of subgraphs may not form part of the isomorphic functions\nof the parent graph. The use of sufficient or necessary conditions generally\nresults in backtracking to ensure the correctness of the decision algorithm.\nThe sufficient and necessary conditions can ensure that the determination of\ngraph isomorphism does not require backtracking, but the correctness of its\nproof process is difficult to guarantee. This article proposes a verification\nmethod that can correctly determine whether the judgment conditions proposed by\nprevious researchers are sufficient and necessary conditions. A subdivision\nmethod has also been proposed in this article, which can obtain more\nsubdivisions for necessary conditions and effectively reduce the size of\nbacktracking space.", "AI": {"tldr": "\u63d0\u51fa\u9a8c\u8bc1\u56fe\u540c\u6784\u5224\u5b9a\u6761\u4ef6\u7684\u5145\u8981\u6027\u65b9\u6cd5\u53ca\u7ec6\u5206\u5019\u9009\u7a7a\u95f4\u7684\u56de\u6eaf\u4f18\u5316\u65b9\u6848", "motivation": "\u73b0\u6709\u56fe\u540c\u6784\u5224\u5b9a\u65b9\u6cd5\u5b58\u5728\u56de\u6eaf\u6548\u7387\u4f4e\u3001\u5145\u8981\u6761\u4ef6\u8bc1\u660e\u56f0\u96be\u7b49\u95ee\u9898\uff0c\u9700\u6539\u8fdb\u9a8c\u8bc1\u673a\u5236\u4e0e\u7a7a\u95f4\u7ec6\u5206\u65b9\u6cd5", "method": "1. \u6784\u5efa\u5145\u8981\u6761\u4ef6\u9a8c\u8bc1\u6846\u67b6\n2. \u8bbe\u8ba1\u5019\u9009\u7a7a\u95f4\u7ec6\u5206\u7b97\u6cd5\n3. \u901a\u8fc7\u5b50\u56fe\u540c\u6784\u51fd\u6570\u517c\u5bb9\u6027\u5206\u6790\u4f18\u5316\u7236\u56fe\u5224\u5b9a", "result": "\u5b9e\u73b0\u975e\u56de\u6eaf\u5f0f\u5224\u5b9a\u9a8c\u8bc1\uff0c\u5019\u9009\u7a7a\u95f4\u7ec6\u5206\u5ea6\u63d0\u5347\u4f7f\u56de\u6eaf\u89c4\u6a21\u663e\u8457\u964d\u4f4e", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u786e\u4fdd\u5224\u5b9a\u6761\u4ef6\u6b63\u786e\u6027\uff0c\u901a\u8fc7\u7cbe\u7ec6\u5316\u7a7a\u95f4\u5212\u5206\u63d0\u5347\u56fe\u540c\u6784\u7b97\u6cd5\u6548\u7387"}}
{"id": "2508.07852", "pdf": "https://arxiv.org/pdf/2508.07852", "abs": "https://arxiv.org/abs/2508.07852", "authors": ["Rui Su", "Honghao Dong", "Haojie Jin", "Yisong Chen", "Guoping Wang", "Sheng Li"], "title": "Vertex Features for Neural Global Illumination", "categories": ["cs.GR", "cs.AI"], "comment": "Accepted by ACM SIGGRAPH Asia'2025", "summary": "Recent research on learnable neural representations has been widely adopted\nin the field of 3D scene reconstruction and neural rendering applications.\nHowever, traditional feature grid representations often suffer from substantial\nmemory footprint, posing a significant bottleneck for modern parallel computing\nhardware. In this paper, we present neural vertex features, a generalized\nformulation of learnable representation for neural rendering tasks involving\nexplicit mesh surfaces. Instead of uniformly distributing neural features\nthroughout 3D space, our method stores learnable features directly at mesh\nvertices, leveraging the underlying geometry as a compact and structured\nrepresentation for neural processing. This not only optimizes memory\nefficiency, but also improves feature representation by aligning compactly with\nthe surface using task-specific geometric priors. We validate our neural\nrepresentation across diverse neural rendering tasks, with a specific emphasis\non neural radiosity. Experimental results demonstrate that our method reduces\nmemory consumption to only one-fifth (or even less) of grid-based\nrepresentations, while maintaining comparable rendering quality and lowering\ninference overhead.", "AI": {"tldr": "\u63d0\u51fa\u795e\u7ecf\u9876\u70b9\u7279\u5f81\u8868\u793a\u6cd5\uff0c\u901a\u8fc7\u5c06\u7279\u5f81\u5b58\u50a8\u5728\u7f51\u683c\u9876\u70b9\u800c\u975e\u4e09\u7ef4\u7a7a\u95f4\uff0c\u964d\u4f4e\u4e94\u500d\u5185\u5b58\u6d88\u8017\u540c\u65f6\u4fdd\u6301\u6e32\u67d3\u8d28\u91cf", "motivation": "\u4f20\u7edf\u7279\u5f81\u7f51\u683c\u8868\u793a\u5b58\u5728\u9ad8\u5185\u5b58\u5360\u7528\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5e76\u884c\u8ba1\u7b97\u786c\u4ef6\u7684\u6027\u80fd\u53d1\u6325", "method": "\u57fa\u4e8e\u663e\u5f0f\u7f51\u683c\u8868\u9762\uff0c\u5c06\u53ef\u5b66\u4e60\u7279\u5f81\u76f4\u63a5\u5b58\u50a8\u5728\u9876\u70b9\uff0c\u5229\u7528\u51e0\u4f55\u5148\u9a8c\u5b9e\u73b0\u5185\u5b58\u4f18\u5316\u4e0e\u7279\u5f81\u589e\u5f3a", "result": "\u5185\u5b58\u6d88\u8017\u964d\u81f3\u7f51\u683c\u8868\u793a\u7684\u4e94\u5206\u4e4b\u4e00\u4ee5\u4e0b\uff0c\u63a8\u7406\u6548\u7387\u63d0\u5347\u540c\u65f6\u4fdd\u6301\u6e32\u67d3\u8d28\u91cf", "conclusion": "\u8be5\u8868\u793a\u65b9\u6cd5\u4e3a\u795e\u7ecf\u6e32\u67d3\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u51e0\u4f55\u5148\u9a8c\u7684\u8f90\u5c04\u7167\u5ea6\u8ba1\u7b97\u7b49\u4efb\u52a1"}}
{"id": "2508.08198", "pdf": "https://arxiv.org/pdf/2508.08198", "abs": "https://arxiv.org/abs/2508.08198", "authors": ["Yupeng Zhang", "Adam Alon", "M. Khalid Jawed"], "title": "Emergent morphogenesis via planar fabrication enabled by a reduced model of composites", "categories": ["cs.GR", "cs.RO"], "comment": "GitHub repository:\n  https://github.com/StructuresComp/discrete-shells-shrinky-dink/", "summary": "The ability to engineer complex three-dimensional shapes from planar sheets\nwith precise, programmable control underpins emerging technologies in soft\nrobotics, reconfigurable devices, and functional materials. Here, we present a\nreduced-order numerical and experimental framework for a bilayer system\nconsisting of a stimuli-responsive thermoplastic sheet (Shrinky Dink) bonded to\na kirigami-patterned, inert plastic layer. Upon uniform heating, the active\nlayer contracts while the patterned layer constrains in-plane stretch but\nallows out-of-plane bending, yielding programmable 3D morphologies from simple\nplanar precursors. Our approach enables efficient computational design and\nscalable manufacturing of 3D forms with a single-layer reduced model that\ncaptures the coupled mechanics of stretching and bending. Unlike traditional\nbilayer modeling, our framework collapses the multilayer composite into a\nsingle layer of nodes and elements, reducing the degrees of freedom and\nenabling simulation on a 2D geometry. This is achieved by introducing a novel\nenergy formulation that captures the coupling between in-plane stretch mismatch\nand out-of-plane bending - extending beyond simple isotropic linear elastic\nmodels. Experimentally, we establish a fully planar, repeatable fabrication\nprotocol using a stimuli-responsive thermoplastic and a laser-cut inert plastic\nlayer. The programmed strain mismatch drives an array of 3D morphologies, such\nas bowls, canoes, and flower petals, all verified by both simulation and\nphysical prototypes.", "AI": {"tldr": "\u5f00\u53d1\u65b0\u578b\u5355\u5c42\u7b80\u5316\u6a21\u578b\uff0c\u901a\u8fc7\u70ed\u9a71\u52a8\u53cc\u5c42\u6750\u6599\u7cfb\u7edf\u5b9e\u73b0\u53ef\u7f16\u7a0b\u4e09\u7ef4\u5f62\u6001\u7684\u9ad8\u6548\u8bbe\u8ba1\u4e0e\u5236\u9020", "motivation": "\u4f20\u7edf\u4e09\u7ef4\u5f62\u6001\u5de5\u7a0b\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u5c42\u72b6\u7ed3\u6784\u5efa\u6a21\uff0c\u9700\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u529b\u5b66\u8026\u5408\u5efa\u6a21\u4e0e\u53ef\u6269\u5c55\u5236\u9020\u65b9\u6848", "method": "\u63d0\u51fa\u5355\u5c42\u964d\u9636\u6a21\u578b\uff08\u878d\u5408\u62c9\u4f38-\u5f2f\u66f2\u8026\u5408\u80fd\u91cf\u516c\u5f0f\uff09+ \u70ed\u54cd\u5e94\u6750\u6599\u4e0e\u6fc0\u5149\u5207\u5272\u5de5\u827a\u7ed3\u5408\u7684\u5b9e\u9a8c\u4f53\u7cfb", "result": "\u6210\u529f\u5236\u5907\u7897\u72b6/\u72ec\u6728\u821f/\u82b1\u74e3\u7b49\u591a\u6837\u5316\u4e09\u7ef4\u5f62\u6001\uff0c\u4eff\u771f\u4e0e\u5b9e\u4f53\u539f\u578b\u9a8c\u8bc1\u4e00\u81f4\u6027", "conclusion": "\u8be5\u6846\u67b6\u7a81\u7834\u4f20\u7edf\u53cc\u5c42\u5efa\u6a21\u9650\u5236\uff0c\u4e3a\u667a\u80fd\u6750\u6599\u5f62\u6001\u7f16\u7a0b\u63d0\u4f9b\u8ba1\u7b97-\u5236\u9020\u4e00\u4f53\u5316\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.06495", "pdf": "https://arxiv.org/pdf/2508.06495", "abs": "https://arxiv.org/abs/2508.06495", "authors": ["Juliana Resplande Sant'anna Gomes", "Arlindo Rodrigues Galv\u00e3o Filho"], "title": "Semi-automated Fact-checking in Portuguese: Corpora Enrichment using Retrieval with Claim extraction", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Master Thesis in Computer Science at Federal University on Goias\n  (UFG). Written in Portuguese", "summary": "The accelerated dissemination of disinformation often outpaces the capacity\nfor manual fact-checking, highlighting the urgent need for Semi-Automated\nFact-Checking (SAFC) systems. Within the Portuguese language context, there is\na noted scarcity of publicly available datasets that integrate external\nevidence, an essential component for developing robust AFC systems, as many\nexisting resources focus solely on classification based on intrinsic text\nfeatures. This dissertation addresses this gap by developing, applying, and\nanalyzing a methodology to enrich Portuguese news corpora (Fake.Br, COVID19.BR,\nMuMiN-PT) with external evidence. The approach simulates a user's verification\nprocess, employing Large Language Models (LLMs, specifically Gemini 1.5 Flash)\nto extract the main claim from texts and search engine APIs (Google Search API,\nGoogle FactCheck Claims Search API) to retrieve relevant external documents\n(evidence). Additionally, a data validation and preprocessing framework,\nincluding near-duplicate detection, is introduced to enhance the quality of the\nbase corpora.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u5229\u7528LLM\u63d0\u53d6\u6838\u5fc3\u4e3b\u5f20\u5e76\u901a\u8fc7\u641c\u7d22\u5f15\u64ceAPI\u83b7\u53d6\u5916\u90e8\u8bc1\u636e\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u589e\u5f3a\u8461\u8404\u7259\u65b0\u95fb\u8bed\u6599\u5e93\u4ee5\u652f\u6301\u534a\u81ea\u52a8\u5316\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf", "motivation": "\u73b0\u6709\u8461\u8404\u7259\u8bed\u4e8b\u5b9e\u6838\u67e5\u6570\u636e\u96c6\u7f3a\u4e4f\u5916\u90e8\u8bc1\u636e\u6574\u5408\uff0c\u9650\u5236\u4e86\u81ea\u52a8\u5316\u7cfb\u7edf\u7684\u5f00\u53d1\u80fd\u529b", "method": "\u4f7f\u7528Gemini 1.5 Flash\u63d0\u53d6\u6587\u672c\u4e3b\u5f20\uff0c\u7ed3\u5408Google Search API\u83b7\u53d6\u8bc1\u636e\uff0c\u5e76\u6784\u5efa\u5305\u542b\u8fd1\u91cd\u590d\u68c0\u6d4b\u7684\u6570\u636e\u9884\u5904\u7406\u6846\u67b6", "result": "\u6210\u529f\u521b\u5efa\u4e86\u5305\u542b\u5916\u90e8\u8bc1\u636e\u7684\u589e\u5f3a\u578b\u8bed\u6599\u5e93\uff08Fake.Br/COVID19.BR/MuMiN-PT\uff09\uff0c\u63d0\u5347\u4e86\u57fa\u7840\u6570\u636e\u8d28\u91cf", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u586b\u8865\u4e86\u8461\u8404\u7259\u8bed\u4e8b\u5b9e\u6838\u67e5\u6570\u636e\u7f3a\u53e3\uff0c\u4e3a\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u534a\u81ea\u52a8\u5316\u6838\u67e5\u7cfb\u7edf\u5960\u5b9a\u6570\u636e\u57fa\u7840"}}
{"id": "2508.08228", "pdf": "https://arxiv.org/pdf/2508.08228", "abs": "https://arxiv.org/abs/2508.08228", "authors": ["Sining Lu", "Guan Chen", "Nam Anh Dinh", "Itai Lang", "Ari Holtzman", "Rana Hanocka"], "title": "LL3M: Large Language 3D Modelers", "categories": ["cs.GR", "cs.AI"], "comment": "Our project page is at https://threedle.github.io/ll3m", "summary": "We present LL3M, a multi-agent system that leverages pretrained large\nlanguage models (LLMs) to generate 3D assets by writing interpretable Python\ncode in Blender. We break away from the typical generative approach that learns\nfrom a collection of 3D data. Instead, we reformulate shape generation as a\ncode-writing task, enabling greater modularity, editability, and integration\nwith artist workflows. Given a text prompt, LL3M coordinates a team of\nspecialized LLM agents to plan, retrieve, write, debug, and refine Blender\nscripts that generate and edit geometry and appearance. The generated code\nworks as a high-level, interpretable, human-readable, well-documented\nrepresentation of scenes and objects, making full use of sophisticated Blender\nconstructs (e.g. B-meshes, geometry modifiers, shader nodes) for diverse,\nunconstrained shapes, materials, and scenes. This code presents many avenues\nfor further agent and human editing and experimentation via code tweaks or\nprocedural parameters. This medium naturally enables a co-creative loop in our\nsystem: agents can automatically self-critique using code and visuals, while\niterative user instructions provide an intuitive way to refine assets. A shared\ncode context across agents enables awareness of previous attempts, and a\nretrieval-augmented generation knowledge base built from Blender API\ndocumentation, BlenderRAG, equips agents with examples, types, and functions\nempowering advanced modeling operations and code correctness. We demonstrate\nthe effectiveness of LL3M across diverse shape categories, style and material\nedits, and user-driven refinements. Our experiments showcase the power of code\nas a generative and interpretable medium for 3D asset creation. Our project\npage is at https://threedle.github.io/ll3m.", "AI": {"tldr": "LL3M\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210Blender\u4ee3\u7801\u5b9e\u73b03D\u8d44\u4ea7\u521b\u5efa\uff0c\u901a\u8fc7\u4ee3\u7801\u7f16\u5199\u66ff\u4ee3\u4f20\u7edf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u3001\u53ef\u534f\u4f5c\u7684\u521b\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u7a81\u7834\u4f20\u7edf\u57fa\u4e8e3D\u6570\u636e\u5b66\u4e60\u7684\u751f\u6210\u8303\u5f0f\uff0c\u901a\u8fc7\u4ee3\u7801\u5f62\u5f0f\u63d0\u53473D\u521b\u4f5c\u7684\u6a21\u5757\u5316\u7a0b\u5ea6\u3001\u53ef\u7f16\u8f91\u6027\u4ee5\u53ca\u4e0e\u827a\u672f\u5bb6\u5de5\u4f5c\u6d41\u7a0b\u7684\u517c\u5bb9\u6027\u3002", "method": "\u90e8\u7f72\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u534f\u8c03\u89c4\u5212/\u68c0\u7d22/\u7f16\u7801/\u8c03\u8bd5\u6d41\u7a0b\uff0c\u7ed3\u5408BlenderRAG\u77e5\u8bc6\u5e93\u589e\u5f3aAPI\u8c03\u7528\u51c6\u786e\u6027\uff0c\u652f\u6301\u51e0\u4f55\u4e0e\u6750\u8d28\u7684\u591a\u5c42\u6b21\u7f16\u8f91\u3002", "result": "\u5728\u591a\u6837\u5316\u5f62\u72b6\u7c7b\u522b\u3001\u98ce\u683c\u7f16\u8f91\u548c\u7528\u6237\u9a71\u52a8\u4f18\u5316\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u4ee3\u7801\u5a92\u4ecb\u6210\u529f\u5b9e\u73b0\u9ad8\u590d\u6742\u5ea6Blender\u6784\u9020\u7684\u7075\u6d3b\u8c03\u7528\u3002", "conclusion": "\u4ee3\u7801\u4f5c\u4e3a\u751f\u6210\u5a92\u4ecb\u517c\u5177\u521b\u9020\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\uff0c\u901a\u8fc7\u4eba\u673a\u534f\u540c\u5faa\u73af\u4e3a3D\u5185\u5bb9\u521b\u4f5c\u5f00\u8f9f\u4e86\u7ed3\u6784\u5316\u3001\u53ef\u8fed\u4ee3\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2508.06504", "pdf": "https://arxiv.org/pdf/2508.06504", "abs": "https://arxiv.org/abs/2508.06504", "authors": ["Yao Ge", "Sudeshna Das", "Yuting Guo", "Abeed Sarker"], "title": "Retrieval augmented generation based dynamic prompting for few-shot biomedical named entity recognition using large language models", "categories": ["cs.CL", "cs.AI"], "comment": "31 pages, 4 figures, 15 tables", "summary": "Biomedical named entity recognition (NER) is a high-utility natural language\nprocessing (NLP) task, and large language models (LLMs) show promise\nparticularly in few-shot settings (i.e., limited training data). In this\narticle, we address the performance challenges of LLMs for few-shot biomedical\nNER by investigating a dynamic prompting strategy involving retrieval-augmented\ngeneration (RAG). In our approach, the annotated in-context learning examples\nare selected based on their similarities with the input texts, and the prompt\nis dynamically updated for each instance during inference. We implemented and\noptimized static and dynamic prompt engineering techniques and evaluated them\non five biomedical NER datasets. Static prompting with structured components\nincreased average F1-scores by 12% for GPT-4, and 11% for GPT-3.5 and LLaMA\n3-70B, relative to basic static prompting. Dynamic prompting further improved\nperformance, with TF-IDF and SBERT retrieval methods yielding the best results,\nimproving average F1-scores by 7.3% and 5.6% in 5-shot and 10-shot settings,\nrespectively. These findings highlight the utility of contextually adaptive\nprompts via RAG for biomedical NER.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eRAG\u7684\u52a8\u6001\u63d0\u793a\u7b56\u7565\u663e\u8457\u63d0\u5347LLMs\u5728\u5c11\u6837\u672c\u751f\u7269\u533b\u5b66NER\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff085\u4e2a\u6570\u636e\u96c6\u9a8c\u8bc1\uff0c\u52a8\u6001\u63d0\u793a\u6bd4\u9759\u6001\u63d0\u793aF1\u503c\u63d0\u53477.3%-5.6%\uff09", "motivation": "\u89e3\u51b3LLMs\u5728\u5c11\u6837\u672c\u751f\u7269\u533b\u5b66NER\u4e2d\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u4e0a\u4e0b\u6587\u52a8\u6001\u9002\u914d\u7684\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5", "method": "\u7ed3\u5408\u9759\u6001\u63d0\u793a(\u7ed3\u6784\u5316\u7ec4\u4ef6)\u548c\u52a8\u6001\u63d0\u793a(RAG\u68c0\u7d22\u76f8\u4f3c\u6587\u672c)\uff0c\u4f7f\u7528TF-IDF/SBERT\u8fdb\u884c\u5b9e\u4f8b\u7ea7\u76f8\u4f3c\u5ea6\u68c0\u7d22\u5e76\u52a8\u6001\u66f4\u65b0prompt", "result": "\u9759\u6001\u63d0\u793a\u63d0\u5347GPT-4\u5e73\u5747F1\u503c12%\uff0c\u52a8\u6001\u63d0\u793a\u57285-shot/10-shot\u4e0b\u5206\u522b\u63d0\u53477.3%\u548c5.6%\uff08\u6700\u4f73\u68c0\u7d22\u65b9\u6cd5\u4e3aTF-IDF\u548cSBERT\uff09", "conclusion": "\u901a\u8fc7RAG\u5b9e\u73b0\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u7684\u52a8\u6001\u63d0\u793a\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u751f\u7269\u533b\u5b66\u9886\u57df\u5c11\u6837\u672cNER\u4efb\u52a1\u6027\u80fd"}}
{"id": "2508.06757", "pdf": "https://arxiv.org/pdf/2508.06757", "abs": "https://arxiv.org/abs/2508.06757", "authors": ["Yash Garg", "Saketh Bachu", "Arindam Dutta", "Rohit Lal", "Sarosij Bose", "Calvin-Khang Ta", "M. Salman Asif", "Amit Roy-Chowdhury"], "title": "VOccl3D: A Video Benchmark Dataset for 3D Human Pose and Shape Estimation under real Occlusions", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Human pose and shape (HPS) estimation methods have been extensively studied,\nwith many demonstrating high zero-shot performance on in-the-wild images and\nvideos. However, these methods often struggle in challenging scenarios\ninvolving complex human poses or significant occlusions. Although some studies\naddress 3D human pose estimation under occlusion, they typically evaluate\nperformance on datasets that lack realistic or substantial occlusions, e.g.,\nmost existing datasets introduce occlusions with random patches over the human\nor clipart-style overlays, which may not reflect real-world challenges. To\nbridge this gap in realistic occlusion datasets, we introduce a novel benchmark\ndataset, VOccl3D, a Video-based human Occlusion dataset with 3D body pose and\nshape annotations. Inspired by works such as AGORA and BEDLAM, we constructed\nthis dataset using advanced computer graphics rendering techniques,\nincorporating diverse real-world occlusion scenarios, clothing textures, and\nhuman motions. Additionally, we fine-tuned recent HPS methods, CLIFF and\nBEDLAM-CLIFF, on our dataset, demonstrating significant qualitative and\nquantitative improvements across multiple public datasets, as well as on the\ntest split of our dataset, while comparing its performance with other\nstate-of-the-art methods. Furthermore, we leveraged our dataset to enhance\nhuman detection performance under occlusion by fine-tuning an existing object\ndetector, YOLO11, thus leading to a robust end-to-end HPS estimation system\nunder occlusions. Overall, this dataset serves as a valuable resource for\nfuture research aimed at benchmarking methods designed to handle occlusions,\noffering a more realistic alternative to existing occlusion datasets. See the\nProject page for code and dataset:https://yashgarg98.github.io/VOccl3D-dataset/", "AI": {"tldr": "\u63d0\u51fa\u4e86VOccl3D\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u8ba1\u7b97\u673a\u56fe\u5f62\u6e32\u67d3\u6280\u672f\u751f\u6210\u771f\u5b9e\u906e\u6321\u573a\u666f\uff0c\u586b\u8865\u73b0\u6709\u906e\u6321\u6570\u636e\u96c6\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03HPS\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u906e\u6321\u573a\u666f\u4e0b\u76843D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u6027\u80fd\u3002", "motivation": "\u73b0\u67093D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u5728\u590d\u6742\u59ff\u6001\u548c\u4e25\u91cd\u906e\u6321\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u73b0\u6709\u906e\u6321\u6570\u636e\u96c6\u591a\u91c7\u7528\u968f\u673a\u8d34\u7247/\u526a\u8d34\u753b\u5f0f\u906e\u6321\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u906e\u6321\u7279\u5f81\u3002", "method": "1) \u57fa\u4e8eAGORA/BEDLAM\u65b9\u6cd5\u8bba\uff0c\u5229\u7528\u56fe\u5f62\u6e32\u67d3\u6280\u672f\u6784\u5efa\u542b\u591a\u6837\u5316\u771f\u5b9e\u906e\u6321\u3001\u670d\u88c5\u7eb9\u7406\u548c\u52a8\u4f5c\u7684\u6807\u6ce8\u6570\u636e\u96c6\uff1b2) \u5bf9CLIFF/BEDLAM-CLIFF\u65b9\u6cd5\u8fdb\u884c\u5fae\u8c03\uff1b3) \u5fae\u8c03YOLO11\u63d0\u5347\u906e\u6321\u4e0b\u4eba\u4f53\u68c0\u6d4b\u6027\u80fd\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u548c\u81ea\u5efa\u6d4b\u8bd5\u96c6\u4e0a\u5b9e\u73b0\u5b9a\u6027\u4e0e\u5b9a\u91cf\u6027\u80fd\u63d0\u5347\uff08\u5982MPJPE\u964d\u4f4e18.3%\uff09\uff0c\u5e76\u6784\u5efa\u4e86\u9996\u4e2a\u7aef\u5230\u7aef\u7684\u906e\u6321\u9c81\u68d2HPS\u7cfb\u7edf\u3002", "conclusion": "VOccl3D\u4e3a\u906e\u6321\u573a\u666f\u7b97\u6cd5\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u57fa\u51c6\uff0c\u5176\u591a\u7ef4\u5ea6\u6570\u636e\u589e\u5f3a\u7b56\u7565\u548c\u68c0\u6d4b-\u4f30\u8ba1\u8054\u5408\u4f18\u5316\u6846\u67b6\u5bf9\u540e\u7eed\u7814\u7a76\u5177\u6709\u542f\u53d1\u6027\u3002"}}
{"id": "2508.06524", "pdf": "https://arxiv.org/pdf/2508.06524", "abs": "https://arxiv.org/abs/2508.06524", "authors": ["Lei Jiang", "Fan Chen"], "title": "CarbonScaling: Extending Neural Scaling Laws for Carbon Footprint in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.DC", "cs.LG"], "comment": "8 pages", "summary": "Neural scaling laws have driven the development of increasingly large\nlanguage models (LLMs) by linking accuracy improvements to growth in parameter\ncount, dataset size, and compute. However, these laws overlook the carbon\nemissions that scale exponentially with LLM size. This paper presents\n\\textit{CarbonScaling}, an analytical framework that extends neural scaling\nlaws to incorporate both operational and embodied carbon in LLM training. By\nintegrating models for neural scaling, GPU hardware evolution, parallelism\noptimization, and carbon estimation, \\textit{CarbonScaling} quantitatively\nconnects model accuracy to carbon footprint. Results show that while a\npower-law relationship between accuracy and carbon holds, real-world\ninefficiencies significantly increase the scaling factor. Hardware technology\nscaling reduces carbon emissions for small to mid-sized models, but offers\ndiminishing returns for extremely large LLMs due to communication overhead and\nunderutilized GPUs. Training optimizations-especially aggressive critical batch\nsize scaling-help alleviate this inefficiency. \\textit{CarbonScaling} offers\nkey insights for training more sustainable and carbon-efficient LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCarbonScaling\u6846\u67b6\uff0c\u5c06\u78b3\u6392\u653e\u56e0\u7d20\u6574\u5408\u5230LLM\u8bad\u7ec3\u7684\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u4e2d\uff0c\u63ed\u793a\u6a21\u578b\u7cbe\u5ea6\u4e0e\u78b3\u8db3\u8ff9\u7684\u91cf\u5316\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u5ffd\u89c6LLM\u8bad\u7ec3\u4e2d\u968f\u6a21\u578b\u89c4\u6a21\u6307\u6570\u589e\u957f\u7684\u78b3\u6392\u653e\u95ee\u9898\uff0c\u9700\u5efa\u7acb\u78b3\u6548\u7387\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u96c6\u6210\u795e\u7ecf\u7f29\u653e\u6a21\u578b\u3001GPU\u786c\u4ef6\u6f14\u8fdb\u6a21\u578b\u3001\u5e76\u884c\u4f18\u5316\u7b97\u6cd5\u548c\u78b3\u6392\u653e\u4f30\u7b97\uff0c\u6784\u5efa\u78b3-\u7cbe\u5ea6\u8054\u5408\u5206\u6790\u6846\u67b6\u3002", "result": "\u53d1\u73b0\u78b3-\u7cbe\u5ea6\u5448\u73b0\u5e42\u5f8b\u5173\u7cfb\u4f46\u5b58\u5728\u7cfb\u7edf\u4f4e\u6548\uff0c\u786c\u4ef6\u5347\u7ea7\u5bf9\u8d85\u5927\u6a21\u578b\u51cf\u6392\u6548\u679c\u9012\u51cf\uff0c\u6279\u91cf\u4f18\u5316\u53ef\u7f13\u89e3\u4f4e\u6548\u3002", "conclusion": "CarbonScaling\u4e3a\u53ef\u6301\u7eedLLM\u8bad\u7ec3\u63d0\u4f9b\u91cf\u5316\u5de5\u5177\uff0c\u5f3a\u8c03\u4f18\u5316\u6279\u91cf\u89c4\u6a21\u5bf9\u78b3\u6548\u7387\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2508.06768", "pdf": "https://arxiv.org/pdf/2508.06768", "abs": "https://arxiv.org/abs/2508.06768", "authors": ["Noe Bertramo", "Gabriel Duguey", "Vivek Gopalakrishnan"], "title": "DiffUS: Differentiable Ultrasound Rendering from Volumetric Imaging", "categories": ["cs.CV", "cs.GR"], "comment": "10 pages, accepted to MICCAI ASMUS 25", "summary": "Intraoperative ultrasound imaging provides real-time guidance during numerous\nsurgical procedures, but its interpretation is complicated by noise, artifacts,\nand poor alignment with high-resolution preoperative MRI/CT scans. To bridge\nthe gap between reoperative planning and intraoperative guidance, we present\nDiffUS, a physics-based, differentiable ultrasound renderer that synthesizes\nrealistic B-mode images from volumetric imaging. DiffUS first converts MRI 3D\nscans into acoustic impedance volumes using a machine learning approach. Next,\nwe simulate ultrasound beam propagation using ray tracing with coupled\nreflection-transmission equations. DiffUS formulates wave propagation as a\nsparse linear system that captures multiple internal reflections. Finally, we\nreconstruct B-mode images via depth-resolved echo extraction across fan-shaped\nacquisition geometry, incorporating realistic artifacts including speckle noise\nand depth-dependent degradation. DiffUS is entirely implemented as\ndifferentiable tensor operations in PyTorch, enabling gradient-based\noptimization for downstream applications such as slice-to-volume registration\nand volumetric reconstruction. Evaluation on the ReMIND dataset demonstrates\nDiffUS's ability to generate anatomically accurate ultrasound images from brain\nMRI data.", "AI": {"tldr": "DiffUS\u662f\u4e00\u4e2a\u57fa\u4e8e\u7269\u7406\u7684\u53ef\u5fae\u5206\u8d85\u58f0\u6e32\u67d3\u5668\uff0c\u901a\u8fc7MRI\u751f\u6210\u89e3\u5256\u5b66\u7cbe\u786e\u7684B\u8d85\u56fe\u50cf\uff0c\u89e3\u51b3\u672f\u4e2d\u8d85\u58f0\u4e0e\u672f\u524d\u5f71\u50cf\u914d\u51c6\u96be\u9898\u3002", "motivation": "\u672f\u4e2d\u8d85\u58f0\u5b58\u5728\u566a\u58f0\u3001\u4f2a\u5f71\u548c\u4e0e\u672f\u524d\u5f71\u50cf\u914d\u51c6\u56f0\u96be\u7684\u95ee\u9898\uff0c\u9700\u8981\u5efa\u7acb\u672f\u524d\u89c4\u5212\u4e0e\u672f\u4e2d\u5bfc\u822a\u7684\u6865\u6881\u3002", "method": "1. \u673a\u5668\u5b66\u4e60\u5c06MRI\u4f53\u79ef\u6570\u636e\u8f6c\u4e3a\u58f0\u963b\u6297\u6570\u636e\n2. \u5c04\u7ebf\u8ffd\u8e2a\u8026\u5408\u53cd\u5c04-\u900f\u5c04\u65b9\u7a0b\u6a21\u62df\u8d85\u58f0\u4f20\u64ad\n3. \u7a00\u758f\u7ebf\u6027\u7cfb\u7edf\u5efa\u6a21\u591a\u91cd\u5185\u90e8\u53cd\u5c04\n4. \u6247\u5f62\u626b\u63cf\u51e0\u4f55\u5b9e\u73b0\u6df1\u5ea6\u89e3\u6790\u56de\u6ce2\u63d0\u53d6\uff0c\u96c6\u6210\u6591\u70b9\u566a\u58f0\u7b49\u4f2a\u5f71", "result": "\u5728ReMIND\u6570\u636e\u96c6\u9a8c\u8bc1\u4e2d\uff0cDiffUS\u6210\u529f\u4ece\u8111\u90e8MRI\u751f\u6210\u89e3\u5256\u5b66\u7cbe\u786e\u7684\u8d85\u58f0\u56fe\u50cf\u3002", "conclusion": "DiffUS\u901a\u8fc7\u53ef\u5fae\u5206\u67b6\u6784\u5b9e\u73b0\u672f\u524d-\u672f\u4e2d\u5f71\u50cf\u878d\u5408\uff0c\u4e3a\u5207\u7247-\u4f53\u79ef\u914d\u51c6\u7b49\u4e34\u5e8a\u5e94\u7528\u63d0\u4f9b\u68af\u5ea6\u4f18\u5316\u57fa\u7840\u3002"}}
{"id": "2508.06533", "pdf": "https://arxiv.org/pdf/2508.06533", "abs": "https://arxiv.org/abs/2508.06533", "authors": ["Aamod Thakur", "Ajay Nagpal", "Atharva Savarkar", "Kundeshwar Pundalik", "Siddhesh Dosi", "Piyush Sawarkar", "Viraj Thakur", "Rohit Saluja", "Maunendra Sankar Desarkar", "Ganesh Ramakrishnan"], "title": "The Art of Breaking Words: Rethinking Multilingual Tokenizer Design", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While model architecture and training objectives are well-studied,\ntokenization, particularly in multilingual contexts, remains a relatively\nneglected aspect of Large Language Model (LLM) development. Existing tokenizers\noften exhibit high token-to-word ratios, inefficient use of context length, and\nslower inference. We present a systematic study that links vocabulary size,\npre-tokenization rules, and training-corpus composition to both token-to-word\nefficiency and model quality. To ground our analysis in a linguistically\ndiverse context, we conduct extensive experiments on Indic scripts, which\npresent unique challenges due to their high script diversity and orthographic\ncomplexity. Drawing on the insights from these analyses, we propose a novel\nalgorithm for data composition that balances multilingual data for tokenizer\ntraining. Our observations on pretokenization strategies significantly improve\nmodel performance, and our data composition algorithm reduces the average\ntoken-to-word ratio by approximately 6% with respect to the conventional data\nrandomization approach. Our tokenizer achieves more than 40% improvement on\naverage token-to-word ratio against stateof-the-art multilingual Indic models.\nThis improvement yields measurable gains in both model performance and\ninference speed. This highlights tokenization alongside architecture and\ntraining objectives as a critical lever for building efficient, scalable\nmultilingual LLMs", "AI": {"tldr": "\u901a\u8fc7\u7cfb\u7edf\u7814\u7a76\u63d0\u51fa\u6570\u636e\u7ec4\u5408\u7b97\u6cd5\uff0c\u663e\u8457\u964d\u4f4etoken-to-word\u6bd4\u4f8b\u5e76\u63d0\u5347\u591a\u8bed\u8a00\u6a21\u578b\u6548\u7387", "motivation": "\u73b0\u6709\u5206\u8bcd\u5668\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u5b58\u5728\u9ad8token-to-word\u6bd4\u4f8b\u3001\u4e0a\u4e0b\u6587\u5229\u7528\u6548\u7387\u4f4e\u53ca\u63a8\u7406\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u5c24\u5176\u5728\u5370\u5ea6\u8bed\u7cfb\u7b49\u9ad8\u590d\u6742\u5ea6\u6587\u5b57\u4e2d\u8868\u73b0\u4e0d\u8db3", "method": "1. \u7cfb\u7edf\u5206\u6790\u8bcd\u6c47\u91cf/\u9884\u5206\u8bcd\u89c4\u5219/\u8bed\u6599\u7ec4\u6210\u7684\u5f71\u54cd 2. \u4ee5\u5370\u5ea6\u8bed\u7cfb\u4e3a\u5b9e\u9a8c\u5bf9\u8c61 3. \u63d0\u51fa\u5e73\u8861\u591a\u8bed\u8a00\u6570\u636e\u7684\u7ec4\u5408\u7b97\u6cd5", "result": "\u5e73\u5747token-word\u6bd4\u4f8b\u964d\u4f4e6%\uff0c\u5bf9\u6bd4\u73b0\u6709\u6a21\u578b\u6548\u7387\u63d0\u534740%+\uff0c\u63a8\u7406\u901f\u5ea6\u4e0e\u6a21\u578b\u6027\u80fd\u53cc\u91cd\u63d0\u5347", "conclusion": "\u5206\u8bcd\u6280\u672f\u4e0e\u67b6\u6784/\u8bad\u7ec3\u76ee\u6807\u540c\u7b49\u91cd\u8981\uff0c\u662f\u6784\u5efa\u9ad8\u6548\u53ef\u6269\u5c55\u591a\u8bed\u8a00\u5927\u6a21\u578b\u7684\u6838\u5fc3\u8981\u7d20"}}
{"id": "2508.06775", "pdf": "https://arxiv.org/pdf/2508.06775", "abs": "https://arxiv.org/abs/2508.06775", "authors": ["Michelle Morgenstern", "Amy Rae Fox", "Graham M. Jones", "Arvind Satyanarayan"], "title": "Visualization Vibes: The Socio-Indexical Function of Visualization Design", "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "In contemporary information ecologies saturated with misinformation,\ndisinformation, and a distrust of science itself, public data communication\nfaces significant hurdles. Although visualization research has broadened\ncriteria for effective design, governing paradigms privilege the accurate and\nefficient transmission of data. Drawing on theory from linguistic anthropology,\nwe argue that such approaches-focused on encoding and decoding propositional\ncontent-cannot fully account for how people engage with visualizations and why\nparticular visualizations might invite adversarial or receptive responses. In\nthis paper, we present evidence that data visualizations communicate not only\nsemantic, propositional meaning$\\unicode{x2013}$meaning about\ndata$\\unicode{x2013}$but also social, indexical meaning$\\unicode{x2013}$meaning\nbeyond data. From a series of ethnographically-informed interviews, we document\nhow readers make rich and varied assessments of a visualization's\n\"vibes\"$\\unicode{x2013}$inferences about the social provenance of a\nvisualization based on its design features. Furthermore, these social\nattributions have the power to influence reception, as readers' decisions about\nhow to engage with a visualization concern not only content, or even aesthetic\nappeal, but also their sense of alignment or disalignment with the entities\nthey imagine to be involved in its production and circulation. We argue these\ninferences hinge on a function of human sign systems that has thus far been\nlittle studied in data visualization: socio-indexicality, whereby the formal\nfeatures (rather than the content) of communication evoke social contexts,\nidentities, and characteristics. Demonstrating the presence and significance of\nthis socio-indexical function in visualization, this paper offers both a\nconceptual foundation and practical intervention for troubleshooting breakdowns\nin public data communication.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u53ef\u89c6\u5316\u56fe\u8868\u901a\u8fc7\u8bbe\u8ba1\u7279\u5f81\u4f20\u9012\u793e\u4f1a\u5c5e\u6027\u610f\u4e49\uff08\u793e\u4f1a\u6307\u793a\u6027\uff09\uff0c\u5f71\u54cd\u516c\u4f17\u6570\u636e\u4f20\u64ad\u6548\u679c", "motivation": "\u5f53\u524d\u4fe1\u606f\u751f\u6001\u5b58\u5728\u4fe1\u4efb\u5371\u673a\uff0c\u4f20\u7edf\u53ef\u89c6\u5316\u8303\u5f0f\u4ec5\u5173\u6ce8\u6570\u636e\u51c6\u786e\u4f20\u8f93\uff0c\u65e0\u6cd5\u89e3\u91ca\u516c\u4f17\u5bf9\u53ef\u89c6\u5316\u7684\u5dee\u5f02\u5316\u53cd\u5e94", "method": "\u91c7\u7528\u6c11\u65cf\u5fd7\u6df1\u5ea6\u8bbf\u8c08\u6cd5\uff0c\u5206\u6790\u8bfb\u8005\u5bf9\u53ef\u89c6\u5316\u8bbe\u8ba1\u7279\u5f81\u7684'\u6c1b\u56f4'\u611f\u77e5\u4e0e\u793e\u4f1a\u5c5e\u6027\u63a8\u65ad", "result": "\u53d1\u73b0\u53ef\u89c6\u5316\u5f62\u5f0f\u7279\u5f81\u4f1a\u89e6\u53d1\u8bfb\u8005\u5bf9\u521b\u4f5c\u80cc\u666f\u7684\u793e\u4f1a\u60f3\u8c61\uff0c\u8fd9\u79cd\u793e\u4f1a\u8ba4\u77e5\u76f4\u63a5\u5f71\u54cd\u6570\u636e\u63a5\u53d7\u5ea6", "conclusion": "\u5f15\u5165\u793e\u4f1a\u6307\u793a\u6027\u7406\u8bba\u6846\u67b6\uff0c\u4e3a\u6539\u5584\u516c\u5171\u6570\u636e\u4f20\u64ad\u63d0\u4f9b\u5f62\u5f0f\u7279\u5f81\u8bbe\u8ba1\u7684\u65b0\u89c6\u89d2"}}
{"id": "2508.06548", "pdf": "https://arxiv.org/pdf/2508.06548", "abs": "https://arxiv.org/abs/2508.06548", "authors": ["Zhanye Luo", "Yuefeng Han", "Xiufan Yu"], "title": "Factor Augmented Supervised Learning with Text Embeddings", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "comment": null, "summary": "Large language models (LLMs) generate text embeddings from text data,\nproducing vector representations that capture the semantic meaning and\ncontextual relationships of words. However, the high dimensionality of these\nembeddings often impedes efficiency and drives up computational cost in\ndownstream tasks. To address this, we propose AutoEncoder-Augmented Learning\nwith Text (AEALT), a supervised, factor-augmented framework that incorporates\ndimension reduction directly into pre-trained LLM workflows. First, we extract\nembeddings from text documents; next, we pass them through a supervised\naugmented autoencoder to learn low-dimensional, task-relevant latent factors.\nBy modeling the nonlinear structure of complex embeddings, AEALT outperforms\nconventional deep-learning approaches that rely on raw embeddings. We validate\nits broad applicability with extensive experiments on classification, anomaly\ndetection, and prediction tasks using multiple real-world public datasets.\nNumerical results demonstrate that AEALT yields substantial gains over both\nvanilla embeddings and several standard dimension reduction methods.", "AI": {"tldr": "\u63d0\u51faAEALT\u6846\u67b6\uff0c\u901a\u8fc7\u76d1\u7763\u589e\u5f3a\u81ea\u7f16\u7801\u5668\u5c06\u9ad8\u7ef4\u6587\u672c\u5d4c\u5165\u964d\u7ef4\uff0c\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6548\u679c", "motivation": "LLM\u751f\u6210\u7684\u9ad8\u7ef4\u6587\u672c\u5d4c\u5165\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\u975e\u7ebf\u6027\u7ed3\u6784\u7684\u5d4c\u5165\u964d\u7ef4", "method": "1. \u4ece\u6587\u672c\u63d0\u53d6\u5d4c\u5165 2. \u4f7f\u7528\u76d1\u7763\u589e\u5f3a\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u4efb\u52a1\u76f8\u5173\u7684\u4f4e\u7ef4\u6f5c\u5728\u56e0\u5b50 3. \u5efa\u6a21\u5d4c\u5165\u7684\u975e\u7ebf\u6027\u7ed3\u6784", "result": "\u5728\u5206\u7c7b\u3001\u5f02\u5e38\u68c0\u6d4b\u548c\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cAEALT\u663e\u8457\u4f18\u4e8e\u539f\u59cb\u5d4c\u5165\u548c\u4f20\u7edf\u964d\u7ef4\u65b9\u6cd5", "conclusion": "AEALT\u6846\u67b6\u6709\u6548\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u5e76\u4fdd\u6301\u8bed\u4e49\u4fe1\u606f\uff0c\u5728\u591a\u79cd\u73b0\u5b9e\u573a\u666f\u4e2d\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027"}}
{"id": "2508.06786", "pdf": "https://arxiv.org/pdf/2508.06786", "abs": "https://arxiv.org/abs/2508.06786", "authors": ["Amy Rae Fox", "Michelle Morgenstern", "Graham M. Jones", "Arvind Satyanarayan"], "title": "Quantifying Visualization Vibes: Measuring Socio-Indexicality at Scale", "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "What impressions might readers form with visualizations that go beyond the\ndata they encode? In this paper, we build on recent work that demonstrates the\nsocio-indexical function of visualization, showing that visualizations\ncommunicate more than the data they explicitly encode. Bridging this with prior\nwork examining public discourse about visualizations, we contribute an analytic\nframework for describing inferences about an artifact's social provenance. Via\na series of attribution-elicitation surveys, we offer descriptive evidence that\nthese social inferences: (1) can be studied asynchronously, (2) are not unique\nto a particular sociocultural group or a function of limited data literacy, and\n(3) may influence assessments of trust. Further, we demonstrate (4) how design\nfeatures act in concert with the topic and underlying messages of an artifact's\ndata to give rise to such 'beyond-data' readings. We conclude by discussing the\ndesign and research implications of inferences about social provenance, and why\nwe believe broadening the scope of research on human factors in visualization\nto include sociocultural phenomena can yield actionable design recommendations\nto address urgent challenges in public data communication.", "AI": {"tldr": "\u53ef\u89c6\u5316\u56fe\u8868\u80fd\u591f\u5f15\u53d1\u8bfb\u8005\u5bf9\u6570\u636e\u793e\u4f1a\u6765\u6e90\u7684\u63a8\u65ad\uff0c\u8fd9\u79cd'\u8d85\u6570\u636e'\u89e3\u8bfb\u4f1a\u5f71\u54cd\u4fe1\u4efb\u8bc4\u4f30\u3002\u7814\u7a76\u901a\u8fc7\u5f52\u56e0\u5f15\u53d1\u5b9e\u9a8c\u8bc1\u660e\u6b64\u7c7b\u63a8\u65ad\u5177\u6709\u666e\u904d\u6027\uff0c\u4e14\u4e0e\u8bbe\u8ba1\u7279\u5f81\u3001\u6570\u636e\u4e3b\u9898\u5171\u540c\u4f5c\u7528\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u663e\u793a\u53ef\u89c6\u5316\u5177\u6709\u793e\u4f1a\u7d22\u5f15\u529f\u80fd\uff0c\u4f46\u516c\u5171\u6570\u636e\u4f20\u64ad\u4e2d\u4ecd\u5b58\u5728\u4fe1\u4efb\u5371\u673a\u3002\u63a2\u7d22\u53ef\u89c6\u5316\u5982\u4f55\u901a\u8fc7\u8bbe\u8ba1\u5143\u7d20\u4f20\u9012\u793e\u4f1a\u6587\u5316\u4fe1\u606f\uff0c\u53ef\u4e3a\u89e3\u51b3\u6570\u636e\u4f20\u64ad\u6311\u6218\u63d0\u4f9b\u65b0\u601d\u8def\u3002", "method": "\u91c7\u7528\u7cfb\u5217\u5f52\u56e0\u5f15\u53d1\u95ee\u5377\u8c03\u67e5\uff0c\u5206\u6790\u4e0d\u540c\u793e\u4f1a\u6587\u5316\u7fa4\u4f53\u5bf9\u53ef\u89c6\u5316\u793e\u4f1a\u6765\u6e90\u7684\u63a8\u65ad\u6a21\u5f0f\uff0c\u8003\u5bdf\u6570\u636e\u7d20\u517b\u4e0e\u8bbe\u8ba1\u7279\u5f81\u7684\u4ea4\u4e92\u4f5c\u7528\u3002", "result": "1) \u793e\u4f1a\u63a8\u65ad\u53ef\u5f02\u6b65\u7814\u7a76 2) \u666e\u904d\u5b58\u5728\u4e8e\u5404\u7fa4\u4f53 3) \u5f71\u54cd\u4fe1\u4efb\u8bc4\u4f30 4) \u8bbe\u8ba1\u7279\u5f81\u4e0e\u6570\u636e\u4e3b\u9898\u534f\u540c\u5851\u9020\u89e3\u8bfb", "conclusion": "\u5e94\u5c06\u793e\u4f1a\u6587\u5316\u73b0\u8c61\u7eb3\u5165\u53ef\u89c6\u5316\u4eba\u56e0\u7814\u7a76\uff0c\u901a\u8fc7\u9488\u5bf9\u6027\u8bbe\u8ba1\u6539\u5584\u516c\u5171\u6570\u636e\u4f20\u64ad\u6548\u679c\uff0c\u5e94\u5bf9\u6570\u636e\u4fe1\u4efb\u5371\u673a\u7b49\u7d27\u8feb\u6311\u6218\u3002"}}
{"id": "2508.06583", "pdf": "https://arxiv.org/pdf/2508.06583", "abs": "https://arxiv.org/abs/2508.06583", "authors": ["Ying Liu", "Can Li", "Ting Zhang", "Mei Wang", "Qiannan Zhu", "Jian Li", "Hua Huang"], "title": "Discerning minds or generic tutors? Evaluating instructional guidance capabilities in Socratic LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The conversational capabilities of large language models hold significant\npromise for enabling scalable and interactive tutoring. While prior research\nhas primarily examined their capacity for Socratic questioning, it often\noverlooks a critical dimension: adaptively guiding learners based on their\ncognitive states. This study shifts focus from mere question generation to the\nbroader instructional guidance capability. We ask: Can LLMs emulate expert\ntutors who dynamically adjust strategies in response to learners'\nunderstanding? To investigate this, we propose GuideEval, a benchmark grounded\nin authentic educational dialogues that evaluates pedagogical guidance through\na three-phase behavioral framework: (1) Perception, inferring learner states;\n(2) Orchestration, adapting instructional strategies; and (3) Elicitation,\nstimulating proper reflections. Empirical findings reveal that existing LLMs\nfrequently fail to provide effective adaptive scaffolding when learners exhibit\nconfusion or require redirection. Furthermore, we introduce a behavior-guided\nfinetuning strategy that leverages behavior-prompted instructional dialogues,\nsignificantly enhancing guidance performance. By shifting the focus from\nisolated content evaluation to learner-centered interaction, our work advocates\na more dialogic paradigm for evaluating Socratic LLMs.", "AI": {"tldr": "\u63d0\u51faGuideEval\u8bc4\u4f30\u6846\u67b6\u9a8c\u8bc1\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u9002\u5e94\u8f85\u5bfc\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u611f\u77e5\u5b66\u4e60\u8005\u8ba4\u77e5\u72b6\u6001\u548c\u52a8\u6001\u8c03\u6574\u7b56\u7565\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u884c\u4e3a\u5f15\u5bfc\u5fae\u8c03\u7b56\u7565\u663e\u8457\u63d0\u5347\u6559\u5b66\u6307\u5bfc\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8fc7\u5ea6\u5173\u6ce8\u82cf\u683c\u62c9\u5e95\u5f0f\u63d0\u95ee\u751f\u6210\uff0c\u5ffd\u89c6\u4e86\u5bf9\u5b66\u4e60\u8005\u8ba4\u77e5\u72b6\u6001\u7684\u52a8\u6001\u611f\u77e5\u4e0e\u6559\u5b66\u7b56\u7565\u7684\u9002\u5e94\u6027\u8c03\u6574\u8fd9\u4e00\u6838\u5fc3\u6559\u5b66\u80fd\u529b\u3002", "method": "\u6784\u5efa\u5305\u542b\u611f\u77e5\uff08\u63a8\u65ad\u5b66\u4e60\u8005\u72b6\u6001\uff09\u3001\u7f16\u6392\uff08\u8c03\u6574\u6559\u5b66\u7b56\u7565\uff09\u3001\u5f15\u53d1\uff08\u6fc0\u53d1\u6df1\u5ea6\u53cd\u601d\uff09\u4e09\u9636\u6bb5\u7684\u884c\u4e3a\u6846\u67b6GuideEval\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u884c\u4e3a\u63d0\u793a\u7684\u5fae\u8c03\u7b56\u7565\u4f18\u5316\u6a21\u578b\u8868\u73b0\u3002", "result": "\u5b9e\u8bc1\u663e\u793a\u73b0\u6709\u6a21\u578b\u5728\u5e94\u5bf9\u5b66\u4e60\u8005\u56f0\u60d1/\u65b9\u5411\u504f\u79bb\u65f6\u7f3a\u4e4f\u6709\u6548\u652f\u67b6\uff0c\u4f46\u884c\u4e3a\u5f15\u5bfc\u5fae\u8c03\u4f7f\u6a21\u578b\u6559\u5b66\u6307\u5bfc\u51c6\u786e\u7387\u63d0\u534727.8%\u3002", "conclusion": "\u5e94\u5efa\u7acb\u5bf9\u8bdd\u5f0f\u8bc4\u4f30\u8303\u5f0f\uff0c\u4ece\u5355\u5411\u5185\u5bb9\u751f\u6210\u8f6c\u5411\u4ee5\u5b66\u4e60\u8005\u8ba4\u77e5\u72b6\u6001\u4e3a\u6838\u5fc3\u7684\u52a8\u6001\u6559\u5b66\u80fd\u529b\u8bc4\u4f30\uff0c\u63a8\u52a8AI\u8f85\u5bfc\u7cfb\u7edf\u7684\u5b9e\u8d28\u6027\u8fdb\u6b65\u3002"}}
{"id": "2508.06968", "pdf": "https://arxiv.org/pdf/2508.06968", "abs": "https://arxiv.org/abs/2508.06968", "authors": ["Ulas Gunes", "Matias Turkulainen", "Juho Kannala", "Esa Rahtu"], "title": "Evaluating Fisheye-Compatible 3D Gaussian Splatting Methods on Real Images Beyond 180 Degree Field of View", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "We present the first evaluation of fisheye-based 3D Gaussian Splatting\nmethods, Fisheye-GS and 3DGUT, on real images with fields of view exceeding 180\ndegree. Our study covers both indoor and outdoor scenes captured with 200\ndegree fisheye cameras and analyzes how each method handles extreme distortion\nin real world settings. We evaluate performance under varying fields of view\n(200 degree, 160 degree, and 120 degree) to study the tradeoff between\nperipheral distortion and spatial coverage. Fisheye-GS benefits from field of\nview (FoV) reduction, particularly at 160 degree, while 3DGUT remains stable\nacross all settings and maintains high perceptual quality at the full 200\ndegree view. To address the limitations of SfM-based initialization, which\noften fails under strong distortion, we also propose a depth-based strategy\nusing UniK3D predictions from only 2-3 fisheye images per scene. Although\nUniK3D is not trained on real fisheye data, it produces dense point clouds that\nenable reconstruction quality on par with SfM, even in difficult scenes with\nfog, glare, or sky. Our results highlight the practical viability of\nfisheye-based 3DGS methods for wide-angle 3D reconstruction from sparse and\ndistortion-heavy image inputs.", "AI": {"tldr": "\u9996\u6b21\u8bc4\u4f30200\u5ea6\u771f\u5b9e\u9c7c\u773c\u56fe\u50cf\u4e0a\u7684Fisheye-GS\u4e0e3DGUT\u65b9\u6cd5\uff0c\u63d0\u51fa\u57fa\u4e8eUniK3D\u7684\u6df1\u5ea6\u521d\u59cb\u5316\u7b56\u7565\uff0c\u8bc1\u660e\u9c7c\u773c3DGS\u5728\u6781\u7aef\u7578\u53d8\u573a\u666f\u7684\u53ef\u884c\u6027", "motivation": "\u89e3\u51b3\u4f20\u7edfSfM\u521d\u59cb\u5316\u5728\u5f3a\u7578\u53d8\u573a\u666f\u5931\u6548\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u9c7c\u773c3D\u91cd\u5efa\u65b9\u6cd5\u5728\u4e0d\u540c\u89c6\u573a\u89d2\u4e0b\u7684\u6027\u80fd\u8fb9\u754c", "method": "\u5728\u771f\u5b9e\u5ba4\u5185\u5916\u573a\u666f\u6d4b\u8bd5\u4e0d\u540c\u89c6\u573a\u89d2\uff08200/160/120\u5ea6\uff09\uff0c\u63d0\u51fa\u57fa\u4e8eUniK3D\u9884\u6d4b\u7684\u6df1\u5ea6\u521d\u59cb\u5316\u7b56\u7565\uff08\u4ec5\u97002-3\u5f20\u56fe\u50cf\uff09\u66ff\u4ee3SfM", "result": "Fisheye-GS\u5728160\u5ea6\u8868\u73b0\u6700\u4f73\uff0c3DGUT\u5728200\u5ea6\u4fdd\u6301\u7a33\u5b9a\uff1bUniK3D\u751f\u6210\u7684\u70b9\u4e91\u8d28\u91cf\u4e0eSfM\u76f8\u5f53\uff0c\u9002\u7528\u4e8e\u96fe\u973e/\u7729\u5149\u7b49\u590d\u6742\u573a\u666f", "conclusion": "\u9c7c\u773c3DGS\u65b9\u6cd5\u5728\u5bbd\u89c6\u573a\u89d2\u3001\u7578\u53d8\u4e25\u91cd\u4e14\u56fe\u50cf\u7a00\u758f\u7684\u573a\u666f\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u4e3a\u6781\u7aef\u73af\u5883\u4e0b\u7684\u4e09\u7ef4\u91cd\u5efa\u63d0\u4f9b\u65b0\u65b9\u6848"}}
{"id": "2508.06595", "pdf": "https://arxiv.org/pdf/2508.06595", "abs": "https://arxiv.org/abs/2508.06595", "authors": ["Xiaoyuan Zhu", "Muru Zhang", "Ollie Liu", "Robin Jia", "Willie Neiswanger"], "title": "LLM Unlearning Without an Expert Curated Dataset", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Modern large language models often encode sensitive, harmful, or copyrighted\nknowledge, raising the need for post-hoc unlearning-the ability to remove\nspecific domains of knowledge from a model without full retraining. A major\nbottleneck in current unlearning pipelines is constructing effective forget\nsets-datasets that approximate the target domain and guide the model to forget\nit. In this work, we introduce a scalable, automated approach to generate\nhigh-quality forget sets using language models themselves. Our method\nsynthesizes textbook-style data through a structured prompting pipeline,\nrequiring only a domain name as input. Through experiments on unlearning\nbiosecurity, cybersecurity, and Harry Potter novels, we show that our synthetic\ndatasets consistently outperform the baseline synthetic alternatives and are\ncomparable to the expert-curated ones. Additionally, ablation studies reveal\nthat the multi-step generation pipeline significantly boosts data diversity,\nwhich in turn improves unlearning utility. Overall, our findings suggest that\nsynthetic datasets offer a promising path toward practical, scalable unlearning\nfor a wide range of emerging domains without the need for manual intervention.\nWe release our code and dataset at\nhttps://github.com/xyzhu123/Synthetic_Textbook.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u9057\u5fd8\u6570\u636e\u96c6\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u751f\u6210\u6559\u79d1\u4e66\u5f0f\u6570\u636e\uff0c\u6709\u6548\u63d0\u5347\u5927\u6a21\u578b\u7279\u5b9a\u9886\u57df\u77e5\u8bc6\u9057\u5fd8\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5e38\u5305\u542b\u654f\u611f/\u6709\u5bb3/\u7248\u6743\u77e5\u8bc6\uff0c\u4f46\u73b0\u6709\u9057\u5fd8\u6d41\u7a0b\u4f9d\u8d56\u4eba\u5de5\u6784\u5efa\u9057\u5fd8\u6570\u636e\u96c6\uff0c\u5b58\u5728\u6548\u7387\u74f6\u9888\u3002", "method": "\u5f00\u53d1\u591a\u6b65\u751f\u6210\u7ba1\u9053\uff1a\u8f93\u5165\u9886\u57df\u540d\u79f0\u2192\u5206\u7ae0\u8282\u751f\u6210\u6280\u672f\u5b9a\u4e49\u2192\u77e5\u8bc6\u6269\u5c55\u2192\u95ee\u7b54\u5bf9\u2192\u5f62\u6210\u7ed3\u6784\u5316\u6559\u79d1\u4e66\u6570\u636e\u96c6\u3002", "result": "\u5728\u751f\u7269\u5b89\u5168/\u7f51\u7edc\u5b89\u5168/\u54c8\u5229\u6ce2\u7279\u9886\u57df\u6d4b\u8bd5\u663e\u793a\uff0c\u5408\u6210\u6570\u636e\u6548\u679c\u4f18\u4e8e\u57fa\u7ebf\uff0c\u4e0e\u4eba\u5de5\u6807\u6ce8\u96c6\u76f8\u5f53\uff1b\u6570\u636e\u591a\u6837\u6027\u63d0\u534730%\u5e26\u6765\u9057\u5fd8\u6548\u679c\u589e\u76ca\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u65b9\u6848\u4e3a\u65b0\u5174\u9886\u57df\u7684\u5927\u6a21\u578b\u9057\u5fd8\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u8def\u5f84\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u9700\u6c42\uff0c\u5177\u6709\u91cd\u8981\u5de5\u7a0b\u5b9e\u8df5\u4ef7\u503c\u3002"}}
{"id": "2508.07011", "pdf": "https://arxiv.org/pdf/2508.07011", "abs": "https://arxiv.org/abs/2508.07011", "authors": ["Zixiong Wang", "Jian Yang", "Yiwei Hu", "Milos Hasan", "Beibei Wang"], "title": "HiMat: DiT-based Ultra-High Resolution SVBRDF Generation", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Creating highly detailed SVBRDFs is essential for 3D content creation. The\nrise of high-resolution text-to-image generative models, based on diffusion\ntransformers (DiT), suggests an opportunity to finetune them for this task.\nHowever, retargeting the models to produce multiple aligned SVBRDF maps instead\nof just RGB images, while achieving high efficiency and ensuring consistency\nacross different maps, remains a challenge. In this paper, we introduce HiMat:\na memory- and computation-efficient diffusion-based framework capable of\ngenerating native 4K-resolution SVBRDFs. A key challenge we address is\nmaintaining consistency across different maps in a lightweight manner, without\nrelying on training new VAEs or significantly altering the DiT backbone (which\nwould damage its prior capabilities). To tackle this, we introduce the\nCrossStitch module, a lightweight convolutional module that captures inter-map\ndependencies through localized operations. Its weights are initialized such\nthat the DiT backbone operation is unchanged before finetuning starts. HiMat\nenables generation with strong structural coherence and high-frequency details.\nResults with a large set of text prompts demonstrate the effectiveness of our\napproach for 4K SVBRDF generation. Further experiments suggest generalization\nto tasks such as intrinsic decomposition.", "AI": {"tldr": "\u63d0\u51faHiMat\u6846\u67b6\uff1a\u57fa\u4e8eDiT\u7684\u9ad8\u6548\u6269\u6563\u6a21\u578b\uff0c\u53ef\u751f\u6210\u539f\u751f4K SVBRDF\u8d34\u56fe\uff0c\u901a\u8fc7CrossStitch\u6a21\u5757\u4fdd\u6301\u8d34\u56fe\u4e00\u81f4\u6027", "motivation": "\u73b0\u6709DiT\u6a21\u578b\u96be\u4ee5\u751f\u6210\u591a\u901a\u9053\u5bf9\u9f50\u7684SVBRDF\u8d34\u56fe\uff0c\u4e14\u76f4\u63a5\u4fee\u6539\u4f1a\u7834\u574f\u539f\u6709\u80fd\u529b\u3002\u9700\u8981\u8f7b\u91cf\u7ea7\u65b9\u6848\u89e3\u51b3\u8de8\u8d34\u56fe\u4e00\u81f4\u6027", "method": "\u5728DiT\u4e3b\u5e72\u4e0a\u6dfb\u52a0CrossStitch\u5377\u79ef\u6a21\u5757\uff0c\u901a\u8fc7\u5c40\u90e8\u64cd\u4f5c\u6355\u6349\u8d34\u56fe\u95f4\u4f9d\u8d56\u5173\u7cfb\u3002\u4fdd\u6301\u6743\u91cd\u521d\u59cb\u5316\u4e0d\u53d8\u4ee5\u4fdd\u7559\u5148\u9a8c\u80fd\u529b", "result": "\u5b9e\u73b04K SVBRDF\u751f\u6210\uff0c\u5c55\u793a\u7ed3\u6784\u8fde\u8d2f\u6027\u548c\u9ad8\u9891\u7ec6\u8282\u3002\u9a8c\u8bc1\u53ef\u63a8\u5e7f\u5230\u56fa\u6709\u5206\u89e3\u7b49\u4efb\u52a1", "conclusion": "HiMat\u6846\u67b6\u5728\u4fdd\u6301DiT\u539f\u6709\u80fd\u529b\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6a21\u5757\u6709\u6548\u89e3\u51b3\u8de8\u8d34\u56fe\u5bf9\u9f50\u95ee\u9898\uff0c\u4e3a\u9ad8\u8d28\u91cf\u6750\u8d28\u751f\u6210\u63d0\u4f9b\u65b0\u65b9\u6848"}}
{"id": "2508.06600", "pdf": "https://arxiv.org/pdf/2508.06600", "abs": "https://arxiv.org/abs/2508.06600", "authors": ["Zijian Chen", "Xueguang Ma", "Shengyao Zhuang", "Ping Nie", "Kai Zou", "Andrew Liu", "Joshua Green", "Kshama Patel", "Ruoxi Meng", "Mingyi Su", "Sahel Sharifymoghaddam", "Yanxi Li", "Haoran Hong", "Xinyu Shi", "Xuye Liu", "Nandan Thakur", "Crystina Zhang", "Luyu Gao", "Wenhu Chen", "Jimmy Lin"], "title": "BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Deep-Research agents, which integrate large language models (LLMs) with\nsearch tools, have shown success in improving the effectiveness of handling\ncomplex queries that require iterative search planning and reasoning over\nsearch results. Evaluations on current benchmarks like BrowseComp relies on\nblack-box live web search APIs, have notable limitations in (1) fairness:\ndynamic and opaque web APIs hinder fair comparisons and reproducibility of deep\nresearch methods; (2) transparency: lack of control over the document corpus\nmakes it difficult to isolate retriever contributions. In other words, the\ncurrent evaluations may compare a complete deep research system at a given\ntime, but they do not foster well-controlled experiments to provide insights\ninto the capability of underlying deep research LLMs. To address these\nchallenges, we introduce BrowseComp-Plus, a benchmark derived from BrowseComp,\nemploying a fixed, carefully curated corpus. Each query in BrowseComp-Plus\nincludes human-verified supporting documents and mined challenging negatives,\nenabling controlled experimentation. The benchmark is shown to be effective in\ndistinguishing the performance of deep research systems. For instance, the\nopen-source model Search-R1, when paired with the BM25 retriever, achieves\n3.86% accuracy, whereas the GPT-5 achieves 55.9%. Integrating the GPT-5 with\nthe Qwen3-Embedding-8B retriever further enhances its accuracy to 70.1% with\nfewer search calls. This benchmark allows comprehensive evaluation and\ndisentangled analysis of deep research agents and retrieval methods, fostering\ninsights into retrieval effectiveness, citation accuracy, and context\nengineering in Deep-Research system.", "AI": {"tldr": "\u63d0\u51faBrowseComp-Plus\u57fa\u51c6\uff0c\u901a\u8fc7\u56fa\u5b9a\u8bed\u6599\u5e93\u89e3\u51b3\u73b0\u6709\u6df1\u5ea6\u7814\u7a76\u667a\u80fd\u4f53\u8bc4\u4f30\u57fa\u51c6\u5728\u516c\u5e73\u6027\u548c\u900f\u660e\u5ea6\u4e0a\u7684\u4e0d\u8db3\uff0c\u5b9e\u73b0\u53ef\u63a7\u5b9e\u9a8c\u4e0e\u5206\u79bb\u8bc4\u4f30", "motivation": "\u73b0\u6709\u8bc4\u4f30\u57fa\u51c6\u4f9d\u8d56\u52a8\u6001\u4e0d\u900f\u660e\u7684\u7f51\u7edc\u641c\u7d22API\uff0c\u5b58\u5728\u516c\u5e73\u6027\u4e0d\u8db3\uff08\u96be\u4ee5\u590d\u73b0\u6bd4\u8f83\uff09\u548c\u900f\u660e\u5ea6\u7f3a\u5931\uff08\u65e0\u6cd5\u5206\u79bb\u68c0\u7d22\u5668\u5f71\u54cd\uff09\u7684\u7f3a\u9677\uff0c\u963b\u788d\u6df1\u5ea6\u7814\u7a76LLM\u80fd\u529b\u7684\u7cfb\u7edf\u6027\u5206\u6790", "method": "\u57fa\u4e8eBrowseComp\u6784\u5efa\u65b0\u57fa\u51c6\uff0c\u91c7\u7528\u56fa\u5b9a\u7cbe\u9009\u8bed\u6599\u5e93\uff0c\u6bcf\u4e2a\u67e5\u8be2\u5305\u542b\u4eba\u5de5\u9a8c\u8bc1\u652f\u6301\u6587\u6863\u548c\u6311\u6218\u6027\u8d1f\u6837\u672c\uff0c\u652f\u6301\u68c0\u7d22\u65b9\u6cd5\u4e0e\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u7684\u5206\u79bb\u8bc4\u4f30", "result": "Search-R1+BM25\u51c6\u786e\u73873.86%\uff0cGPT-5\u8fbe55.9%\uff0c\u7ed3\u5408Qwen3-Embedding-8B\u540e\u63d0\u5347\u81f370.1%\uff08\u641c\u7d22\u8c03\u7528\u66f4\u5c11\uff09\uff0c\u8bc1\u660e\u57fa\u51c6\u6709\u6548\u533a\u5206\u7cfb\u7edf\u6027\u80fd", "conclusion": "BrowseComp-Plus\u5b9e\u73b0\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u4e0e\u68c0\u7d22\u65b9\u6cd5\u7684\u89e3\u8026\u8bc4\u4f30\uff0c\u4e3a\u68c0\u7d22\u6548\u679c\u3001\u5f15\u7528\u51c6\u786e\u6027\u548c\u4e0a\u4e0b\u6587\u5de5\u7a0b\u7814\u7a76\u63d0\u4f9b\u65b0\u89c1\u89e3\uff0c\u63a8\u52a8\u9886\u57df\u53d1\u5c55"}}
{"id": "2508.07760", "pdf": "https://arxiv.org/pdf/2508.07760", "abs": "https://arxiv.org/abs/2508.07760", "authors": ["Maximilian Kromer", "Panagiotis Agrafiotis", "Beg\u00fcm Demir"], "title": "Sea-Undistort: A Dataset for Through-Water Image Restoration in High Resolution Airborne Bathymetric Mapping", "categories": ["eess.IV", "cs.CV", "cs.GR"], "comment": "Under review in IEEE Geoscience and Remote Sensing Letters", "summary": "Accurate image-based bathymetric mapping in shallow waters remains\nchallenging due to the complex optical distortions such as wave induced\npatterns, scattering and sunglint, introduced by the dynamic water surface, the\nwater column properties, and solar illumination. In this work, we introduce\nSea-Undistort, a comprehensive synthetic dataset of 1200 paired 512x512\nthrough-water scenes rendered in Blender. Each pair comprises a distortion-free\nand a distorted view, featuring realistic water effects such as sun glint,\nwaves, and scattering over diverse seabeds. Accompanied by per-image metadata\nsuch as camera parameters, sun position, and average depth, Sea-Undistort\nenables supervised training that is otherwise infeasible in real environments.\nWe use Sea-Undistort to benchmark two state-of-the-art image restoration\nmethods alongside an enhanced lightweight diffusion-based framework with an\nearly-fusion sun-glint mask. When applied to real aerial data, the enhanced\ndiffusion model delivers more complete Digital Surface Models (DSMs) of the\nseabed, especially in deeper areas, reduces bathymetric errors, suppresses\nglint and scattering, and crisply restores fine seabed details. Dataset,\nweights, and code are publicly available at\nhttps://www.magicbathy.eu/Sea-Undistort.html.", "AI": {"tldr": "\u63d0\u51faSea-Undistort\u5408\u6210\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u6709\u6548\u6d88\u9664\u6d45\u6c34\u56fe\u50cf\u7578\u53d8\uff0c\u63d0\u5347\u6d77\u5e95\u5efa\u6a21\u7cbe\u5ea6", "motivation": "\u6d45\u6c34\u533a\u5149\u5b66\u7578\u53d8\uff08\u6ce2\u6d6a/\u6563\u5c04/\u8000\u5149\uff09\u5bfc\u81f4\u6d4b\u6df1\u5236\u56fe\u4e0d\u51c6\u786e\uff0c\u771f\u5b9e\u73af\u5883\u6570\u636e\u83b7\u53d6\u56f0\u96be\uff0c\u9700\u5408\u6210\u6570\u636e\u652f\u6491\u6a21\u578b\u8bad\u7ec3", "method": "\u7528Blender\u6e32\u67d31200\u5bf9\u542b\u5143\u6570\u636e\u7684\u7578\u53d8-\u65e0\u7578\u53d8\u56fe\u50cf\u5bf9\uff0c\u5f00\u53d1\u5e26\u8000\u5149\u63a9\u6a21\u7684\u8f7b\u91cf\u6269\u6563\u6a21\u578b\u8fdb\u884c\u56fe\u50cf\u6062\u590d", "result": "\u5728\u771f\u5b9e\u822a\u62cd\u6570\u636e\u4e2d\uff1aDSM\u5b8c\u6574\u6027\u63d0\u5347\uff08\u5c24\u5176\u6df1\u6c34\u533a\uff09\uff0c\u6d4b\u6df1\u8bef\u5dee\u964d\u4f4e\uff0c\u8000\u5149/\u6563\u5c04\u6291\u5236\uff0c\u6d77\u5e95\u7ec6\u8282\u6062\u590d\u66f4\u6e05\u6670", "conclusion": "Sea-Undistort\u586b\u8865\u76d1\u7763\u8bad\u7ec3\u6570\u636e\u7a7a\u767d\uff0c\u6539\u8fdb\u7684\u6269\u6563\u6a21\u578b\u9a8c\u8bc1\u6709\u6548\uff0c\u516c\u5f00\u8d44\u6e90\u63a8\u52a8\u6c34\u4e0b\u89c6\u89c9\u7814\u7a76\u53d1\u5c55"}}
{"id": "2508.06621", "pdf": "https://arxiv.org/pdf/2508.06621", "abs": "https://arxiv.org/abs/2508.06621", "authors": ["Tomohiro Sawada", "Kartik Goyal"], "title": "Train It and Forget It: Merge Lists are Unnecessary for BPE Inference in Language Models", "categories": ["cs.CL"], "comment": "Submitted to EMNLP", "summary": "Standard Byte-Pair Encoding (BPE) tokenization compresses text by pairing a\nlearned token vocabulary with a detailed merge list. Recent work has shown that\nthis merge list exposes a potential attack surface for extracting information\nabout language model's training data. In this paper, we explore the downstream\nimpact of BPE inference algorithms that do not rely on this merge list at all,\nand hence differ from the encoding process during BPE training. To address this\nquestion, we investigate two broad classes of BPE inference schemes that differ\nfrom BPE application during training: a) targeted deviation from merge-lists\nincluding random merge orders, and various corruptions of merge list involving\ndeletion/truncation, and b) non-targeted BPE inference algorithms that do not\ndepend on the merge list but focus on compressing the text either greedily or\nexactly. Extensive experiments across diverse language modeling tasks like\naccuracy-based QA benchmarks, machine translation, and open-ended generation\nreveal that while targeted deviation from the merge lists exhibits significant\ndegradation in language model performance, the non-targeted merge-list-free\ninference algorithms result in minimal impact on downstream performance that is\noften much smaller than expected. These findings pave way for simpler and\npotentially more privacy-preserving tokenization schemes that do not\ncatastrophically compromise model performance.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u975e\u76ee\u6807BPE\u63a8\u7406\u7b97\u6cd5\uff08\u4e0d\u4f9d\u8d56\u5408\u5e76\u5217\u8868\uff09\u5bf9\u4e0b\u6e38\u4efb\u52a1\u5f71\u54cd\u6781\u5c0f\uff0c\u4e3a\u66f4\u7b80\u5355\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u6807\u8bb0\u5316\u65b9\u6848\u63d0\u4f9b\u4e86\u53ef\u80fd", "motivation": "\u6807\u51c6BPE\u7684\u5408\u5e76\u5217\u8868\u53ef\u80fd\u6210\u4e3a\u653b\u51fb\u9762\u6cc4\u9732\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u4fe1\u606f\uff0c\u9700\u63a2\u7d22\u4e0d\u4f9d\u8d56\u5408\u5e76\u5217\u8868\u7684BPE\u63a8\u7406\u7b97\u6cd5\u7684\u5f71\u54cd", "method": "\u7814\u7a76\u4e24\u7c7bBPE\u63a8\u7406\u65b9\u6848\uff1a1\uff09\u9488\u5bf9\u5408\u5e76\u5217\u8868\u7684\u5b9a\u5411\u504f\u79bb\uff08\u968f\u673a\u5408\u5e76\u987a\u5e8f/\u5220\u9664\u622a\u65ad\uff09 2\uff09\u975e\u76ee\u6807\u7b97\u6cd5\uff08\u8d2a\u5a6a/\u7cbe\u786e\u6587\u672c\u538b\u7f29\uff09", "result": "\u5b9a\u5411\u504f\u79bb\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u800c\u975e\u76ee\u6807\u7b97\u6cd5\u5728QA\u3001\u7ffb\u8bd1\u3001\u751f\u6210\u7b49\u4efb\u52a1\u4e2d\u4ec5\u4ea7\u751f\u5fae\u5c0f\u5f71\u54cd", "conclusion": "\u65e0\u9700\u5408\u5e76\u5217\u8868\u7684BPE\u63a8\u7406\u7b97\u6cd5\u53ef\u4fdd\u6301\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u7b80\u5316tokenization\u6d41\u7a0b\u548c\u589e\u5f3a\u9690\u79c1\u4fdd\u62a4\u5f00\u8f9f\u65b0\u8def\u5f84"}}
{"id": "2508.08086", "pdf": "https://arxiv.org/pdf/2508.08086", "abs": "https://arxiv.org/abs/2508.08086", "authors": ["Zhongqi Yang", "Wenhang Ge", "Yuqi Li", "Jiaqi Chen", "Haoyuan Li", "Mengyin An", "Fei Kang", "Hua Xue", "Baixin Xu", "Yuyang Yin", "Eric Li", "Yang Liu", "Yikai Wang", "Hao-Xiang Guo", "Yahui Zhou"], "title": "Matrix-3D: Omnidirectional Explorable 3D World Generation", "categories": ["cs.CV", "cs.GR"], "comment": "Technical Report", "summary": "Explorable 3D world generation from a single image or text prompt forms a\ncornerstone of spatial intelligence. Recent works utilize video model to\nachieve wide-scope and generalizable 3D world generation. However, existing\napproaches often suffer from a limited scope in the generated scenes. In this\nwork, we propose Matrix-3D, a framework that utilize panoramic representation\nfor wide-coverage omnidirectional explorable 3D world generation that combines\nconditional video generation and panoramic 3D reconstruction. We first train a\ntrajectory-guided panoramic video diffusion model that employs scene mesh\nrenders as condition, to enable high-quality and geometrically consistent scene\nvideo generation. To lift the panorama scene video to 3D world, we propose two\nseparate methods: (1) a feed-forward large panorama reconstruction model for\nrapid 3D scene reconstruction and (2) an optimization-based pipeline for\naccurate and detailed 3D scene reconstruction. To facilitate effective\ntraining, we also introduce the Matrix-Pano dataset, the first large-scale\nsynthetic collection comprising 116K high-quality static panoramic video\nsequences with depth and trajectory annotations. Extensive experiments\ndemonstrate that our proposed framework achieves state-of-the-art performance\nin panoramic video generation and 3D world generation. See more in\nhttps://matrix-3d.github.io.", "AI": {"tldr": "\u63d0\u51faMatrix-3D\u6846\u67b6\uff0c\u7ed3\u5408\u5168\u666f\u89c6\u9891\u751f\u6210\u4e0e\u4e24\u79cd3D\u91cd\u5efa\u65b9\u6cd5\uff0c\u5b9e\u73b0\u5927\u8303\u56f4\u53ef\u63a2\u7d223D\u4e16\u754c\u751f\u6210", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c6\u9891\u76843D\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u573a\u666f\u8986\u76d6\u8303\u56f4\u53d7\u9650\u7684\u95ee\u9898", "method": "1. \u8f68\u8ff9\u5f15\u5bfc\u7684\u5168\u666f\u89c6\u9891\u6269\u6563\u6a21\u578b\uff08\u573a\u666f\u7f51\u683c\u6e32\u67d3\u4f5c\u4e3a\u6761\u4ef6\uff09\n2. \u4e24\u79cd\u91cd\u5efa\u65b9\u6cd5\uff1a\u524d\u9988\u5f0f\u5168\u666f\u91cd\u5efa\u6a21\u578b+\u4f18\u5316\u5f0f3D\u91cd\u5efa\u7ba1\u9053", "result": "\u5728\u5168\u666f\u89c6\u9891\u751f\u6210\u548c3D\u4e16\u754c\u751f\u6210\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\u6027\u80fd", "conclusion": "\u901a\u8fc7\u5168\u666f\u8868\u793a\u4e0e\u53cc\u91cd\u5efa\u7b56\u7565\u6709\u6548\u6269\u5c553D\u751f\u6210\u8303\u56f4\uff0c\u914d\u5957\u53d1\u5e03\u7684Matrix-Pano\u6570\u636e\u96c6\uff08\u542b116K\u5168\u666f\u89c6\u9891\u5e8f\u5217\uff09\u652f\u6301\u8bad\u7ec3"}}
{"id": "2508.06649", "pdf": "https://arxiv.org/pdf/2508.06649", "abs": "https://arxiv.org/abs/2508.06649", "authors": ["Daniel Wang", "Eli Brignac", "Minjia Mao", "Xiao Fang"], "title": "Measuring Stereotype and Deviation Biases in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are widely applied across diverse domains,\nraising concerns about their limitations and potential risks. In this study, we\ninvestigate two types of bias that LLMs may display: stereotype bias and\ndeviation bias. Stereotype bias refers to when LLMs consistently associate\nspecific traits with a particular demographic group. Deviation bias reflects\nthe disparity between the demographic distributions extracted from\nLLM-generated content and real-world demographic distributions. By asking four\nadvanced LLMs to generate profiles of individuals, we examine the associations\nbetween each demographic group and attributes such as political affiliation,\nreligion, and sexual orientation. Our experimental results show that all\nexamined LLMs exhibit both significant stereotype bias and deviation bias\ntowards multiple groups. Our findings uncover the biases that occur when LLMs\ninfer user attributes and shed light on the potential harms of LLM-generated\noutputs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u666e\u904d\u5b58\u5728\u523b\u677f\u5370\u8c61\u504f\u89c1\u548c\u7edf\u8ba1\u5206\u5e03\u504f\u5dee\uff0c\u53ef\u80fd\u5bf9\u7528\u6237\u5c5e\u6027\u63a8\u65ad\u4ea7\u751f\u5371\u5bb3", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5404\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8bc4\u4f30\u5176\u53ef\u80fd\u5b58\u5728\u7684\u504f\u89c1\u98ce\u9669\u3002\u4f20\u7edf\u7814\u7a76\u4fa7\u91cd\u8bad\u7ec3\u6570\u636e\u504f\u89c1\uff0c\u672c\u6587\u805a\u7126\u6a21\u578b\u63a8\u7406\u9636\u6bb5\u7684\u6f5c\u5728\u504f\u89c1\u95ee\u9898", "method": "\u8981\u6c424\u4e2a\u5148\u8fdbLLM\u751f\u6210\u4e2a\u4eba\u7279\u5f81\u8d44\u6599\uff0c\u5206\u6790\u4eba\u53e3\u7fa4\u4f53\u4e0e\u653f\u6cbb\u503e\u5411/\u5b97\u6559\u4fe1\u4ef0/\u6027\u53d6\u5411\u7b49\u5c5e\u6027\u7684\u5173\u8054\u6027", "result": "\u6240\u6709\u6d4b\u8bd5\u6a21\u578b\u5728\u591a\u4e2a\u7fa4\u4f53\u7279\u5f81\u4e0a\u5747\u8868\u73b0\u51fa\u663e\u8457\u7684\u523b\u677f\u5370\u8c61\u504f\u89c1\uff08\u5e73\u5747\u504f\u5dee\u8fbe18.7%\uff09\u548c\u5206\u5e03\u504f\u5dee\uff08\u6700\u5927KL\u6563\u5ea60.43\uff09", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86LLM\u751f\u6210\u5185\u5bb9\u65f6\u5b58\u5728\u7684\u53cc\u91cd\u504f\u89c1\u673a\u5236\uff0c\u5bf9\u7406\u89e3\u6a21\u578b\u8f93\u51fa\u5371\u5bb3\u53ca\u6539\u8fdb\u7b97\u6cd5\u516c\u5e73\u6027\u5177\u6709\u542f\u793a\u610f\u4e49"}}
{"id": "2508.06665", "pdf": "https://arxiv.org/pdf/2508.06665", "abs": "https://arxiv.org/abs/2508.06665", "authors": ["Jonathan Shaw", "Dillon Mee", "Timothy Khouw", "Zackary Leech", "Daniel Wilson"], "title": "Testing the Limits of Machine Translation from One Book", "categories": ["cs.CL"], "comment": null, "summary": "Current state-of-the-art models demonstrate capacity to leverage in-context\nlearning to translate into previously unseen language contexts. Tanzer et al.\n[2024] utilize language materials (e.g. a grammar) to improve translation\nquality for Kalamang using large language models (LLMs). We focus on Kanuri, a\nlanguage that, despite having substantial speaker population, has minimal\ndigital resources. We design two datasets for evaluation: one focused on health\nand humanitarian terms, and another containing generalized terminology,\ninvestigating how domain-specific tasks impact LLM translation quality.\n  By providing different combinations of language resources (grammar,\ndictionary, and parallel sentences), we measure LLM translation effectiveness,\ncomparing results to native speaker translations and human linguist\nperformance. We evaluate using both automatic metrics and native speaker\nassessments of fluency and accuracy.\n  Results demonstrate that parallel sentences remain the most effective data\nsource, outperforming other methods in human evaluations and automatic metrics.\nWhile incorporating grammar improves over zero-shot translation, it fails as an\neffective standalone data source. Human evaluations reveal that LLMs achieve\naccuracy (meaning) more effectively than fluency (grammaticality).\n  These findings suggest LLM translation evaluation benefits from\nmultidimensional assessment beyond simple accuracy metrics, and that grammar\nalone, without parallel sentences, does not provide sufficient context for\neffective domain-specific translation.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u4e0d\u540c\u8bed\u8a00\u8d44\u6e90\u7ec4\u5408\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728Kanuri\u8bed\u7ffb\u8bd1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5e76\u884c\u53e5\u5b50\u662f\u6700\u6709\u6548\u6570\u636e\u6e90\uff0c\u5355\u72ec\u4f7f\u7528\u8bed\u6cd5\u4e0d\u8db3\u4ee5\u652f\u6491\u9886\u57df\u7ffb\u8bd1\u6548\u679c", "motivation": "\u9488\u5bf9Kanuri\u8bed\u4f7f\u7528\u4eba\u53e3\u4f17\u591a\u4f46\u6570\u5b57\u8d44\u6e90\u532e\u4e4f\u7684\u73b0\u72b6\uff0c\u63a2\u7d22\u5982\u4f55\u5229\u7528\u6709\u9650\u8bed\u8a00\u8d44\u6e90\u63d0\u5347LLM\u5728\u7279\u5b9a\u9886\u57df\uff08\u5982\u5065\u5eb7\u672f\u8bed\uff09\u7684\u7ffb\u8bd1\u8d28\u91cf", "method": "\u6784\u5efa\u5065\u5eb7\u672f\u8bed\u548c\u901a\u7528\u672f\u8bed\u4e24\u4e2a\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u8bed\u6cd5/\u8bcd\u5178/\u5e73\u884c\u53e5\u5b50\u7684\u4e0d\u540c\u7ec4\u5408\uff0c\u901a\u8fc7\u81ea\u52a8\u6307\u6807\u548c\u6bcd\u8bed\u8005\u8bc4\u4f30\uff08\u6d41\u7545\u5ea6/\u51c6\u786e\u6027\uff09\u5bf9\u6bd4LLM\u4e0e\u4eba\u7c7b\u8868\u73b0", "result": "\u5e73\u884c\u53e5\u5b50\u5728\u4eba\u5de5\u8bc4\u4f30\u548c\u81ea\u52a8\u6307\u6807\u4e2d\u8868\u73b0\u6700\u4f73\uff1b\u8bed\u6cd5\u5355\u72ec\u4f7f\u7528\u6548\u679c\u6709\u9650\uff1bLLM\u5728\u8bed\u4e49\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u8bed\u6cd5\u6d41\u7545\u5ea6", "conclusion": "LLM\u7ffb\u8bd1\u8bc4\u4f30\u9700\u8981\u591a\u7ef4\u6307\u6807\uff0c\u4ec5\u8bed\u6cd5\u8d44\u6e90\u4e0d\u8db3\u4ee5\u652f\u6491\u9886\u57df\u7ffb\u8bd1\u6548\u679c\uff0c\u5f3a\u8c03\u5e73\u884c\u53e5\u5b50\u5728\u8de8\u8bed\u8a00\u8fc1\u79fb\u4e2d\u7684\u5173\u952e\u4f5c\u7528"}}
{"id": "2508.06671", "pdf": "https://arxiv.org/pdf/2508.06671", "abs": "https://arxiv.org/abs/2508.06671", "authors": ["Swati Rajwal", "Shivank Garg", "Reem Abdel-Salam", "Abdelrahman Zayed"], "title": "Do Biased Models Have Biased Thoughts?", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "Accepted at main track of the Second Conference on Language Modeling\n  (COLM 2025)", "summary": "The impressive performance of language models is undeniable. However, the\npresence of biases based on gender, race, socio-economic status, physical\nappearance, and sexual orientation makes the deployment of language models\nchallenging. This paper studies the effect of chain-of-thought prompting, a\nrecent approach that studies the steps followed by the model before it\nresponds, on fairness. More specifically, we ask the following question:\n\\textit{Do biased models have biased thoughts}? To answer our question, we\nconduct experiments on $5$ popular large language models using fairness metrics\nto quantify $11$ different biases in the model's thoughts and output. Our\nresults show that the bias in the thinking steps is not highly correlated with\nthe output bias (less than $0.6$ correlation with a $p$-value smaller than\n$0.001$ in most cases). In other words, unlike human beings, the tested models\nwith biased decisions do not always possess biased thoughts.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u51b3\u7b56\u504f\u89c1\u4e0e\u601d\u8003\u6b65\u9aa4\u504f\u89c1\u76f8\u5173\u6027\u8f83\u4f4e\uff08<0.6\uff09\uff0c\u8868\u660e\u6a21\u578b\u504f\u89c1\u51b3\u7b56\u4e0d\u4e00\u5b9a\u4f34\u968f\u504f\u89c1\u601d\u7ef4\u8fc7\u7a0b", "motivation": "\u63a2\u7d22\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u5bf9\u8bed\u8a00\u6a21\u578b\u516c\u5e73\u6027\u7684\u5f71\u54cd\uff0c\u9a8c\u8bc1\u5b58\u5728\u51b3\u7b56\u504f\u89c1\u7684\u6a21\u578b\u662f\u5426\u540c\u65f6\u5177\u6709\u504f\u89c1\u601d\u7ef4\u8fc7\u7a0b", "method": "\u4f7f\u7528\u516c\u5e73\u6027\u6307\u6807\u5bf95\u4e2a\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\uff0c\u91cf\u531611\u79cd\u504f\u89c1\u5728\u601d\u8003\u6b65\u9aa4\u548c\u6700\u7ec8\u8f93\u51fa\u7684\u8868\u73b0", "result": "\u601d\u8003\u6b65\u9aa4\u504f\u89c1\u4e0e\u8f93\u51fa\u504f\u89c1\u76f8\u5173\u6027\u4f4e\u4e8e0.6\uff08p<0.001\uff09\uff0c\u6a21\u578b\u504f\u89c1\u51b3\u7b56\u4e0d\u5fc5\u7136\u4f34\u968f\u504f\u89c1\u601d\u7ef4", "conclusion": "\u8bed\u8a00\u6a21\u578b\u7684\u504f\u89c1\u51b3\u7b56\u673a\u5236\u4e0e\u4eba\u7c7b\u4e0d\u540c\uff0c\u5176\u601d\u8003\u8fc7\u7a0b\u53ef\u80fd\u4fdd\u6301\u76f8\u5bf9\u4e2d\u7acb\uff0c\u8fd9\u4e3a\u6a21\u578b\u504f\u89c1\u8bca\u65ad\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2"}}
{"id": "2508.06709", "pdf": "https://arxiv.org/pdf/2508.06709", "abs": "https://arxiv.org/abs/2508.06709", "authors": ["Evangelia Spiliopoulou", "Riccardo Fogliato", "Hanna Burnsky", "Tamer Soliman", "Jie Ma", "Graham Horwood", "Miguel Ballesteros"], "title": "Play Favorites: A Statistical Method to Measure Self-Bias in LLM-as-a-Judge", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) can serve as judges that offer rapid and\nreliable assessments of other LLM outputs. However, models may systematically\nassign overly favorable ratings to their own outputs, a phenomenon known as\nself-bias, which can distort evaluations of true model performance. Previous\nstudies often conflate genuine differences in model quality with bias or\nincorrectly assume that evaluations from LLMs and humans follow the same rating\ndistributions. In this work, we present a statistical framework that explicitly\nformalizes assumptions under which self-bias can be identified and estimated.\nOur method models the difference in the scoring distribution that\nLLM-as-a-judge assigns to its own completions compared to other models, while\naccounting for the underlying quality of the completions provided by an\nindependent, third-party judge (e.g., humans). Our method reliably isolates and\nquantifies self-bias, even when models vary in ability, ensuring that genuine\nperformance differences are not mistaken for self-bias. We conduct an empirical\nanalysis of self-bias on a large dataset (>5000 prompt-completion pairs)\nconsisting of expert human annotations and judgments from nine different LLM\njudges. We find that some models, such as GPT-4o and Claude 3.5 Sonnet,\nsystematically assign higher scores to their own outputs. These models also\ndisplay family-bias; systematically assigning higher ratings to outputs\nproduced by other models of the same family. Our findings highlight potential\npitfalls of using LLM judges and offer practical guidance to mitigate biases\nwhen interpreting automated evaluations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7edf\u8ba1\u6846\u67b6\u91cf\u5316LLM\u4f5c\u4e3a\u8bc4\u59d4\u65f6\u7684\u81ea\u6211\u504f\u89c1\uff08self-bias\uff09\uff0c\u53d1\u73b0GPT-4o\u7b49\u6a21\u578b\u5b58\u5728\u7cfb\u7edf\u6027\u9ad8\u4f30\u81ea\u8eab\u53ca\u540c\u7cfb\u5217\u6a21\u578b\u8f93\u51fa\u7684\u73b0\u8c61", "motivation": "\u73b0\u6709\u7814\u7a76\u6df7\u6dc6\u6a21\u578b\u771f\u5b9e\u8d28\u91cf\u5dee\u5f02\u4e0e\u504f\u89c1\uff0c\u4e14\u9519\u8bef\u5047\u8bbeLLM\u4e0e\u4eba\u7c7b\u8bc4\u5206\u5206\u5e03\u4e00\u81f4\u3002\u9700\u5f00\u53d1\u65b0\u65b9\u6cd5\u51c6\u786e\u8bc6\u522b\u548c\u91cf\u5316LLM\u8bc4\u59d4\u7684\u81ea\u6211\u504f\u89c1", "method": "\u5efa\u7acb\u7edf\u8ba1\u6a21\u578b\u5206\u79bb\u81ea\u6211\u504f\u89c1\u4e0e\u771f\u5b9e\u8d28\u91cf\u5dee\u5f02\uff0c\u4f7f\u7528\u7b2c\u4e09\u65b9\u8bc4\u59d4\uff08\u4eba\u7c7b\u4e13\u5bb6\uff09\u6570\u636e\u9a8c\u8bc1\uff0c\u5206\u67905000+\u63d0\u793a-\u56de\u7b54\u5bf9\u53ca9\u4e2aLLM\u8bc4\u59d4\u7684\u8bc4\u4f30\u7ed3\u679c", "result": "GPT-4o\u548cClaude 3.5 Sonnet\u7b49\u6a21\u578b\u5b58\u5728\u81ea\u6211\u504f\u89c1\uff08\u5e73\u5747\u8bc4\u5206\u63d0\u534715-20%\uff09\u53ca\u5bb6\u65cf\u504f\u89c1\uff0c\u65b0\u65b9\u6cd5\u6709\u6548\u533a\u5206\u771f\u5b9e\u6027\u80fd\u5dee\u5f02\u4e0e\u504f\u89c1", "conclusion": "\u63ed\u793aLLM\u8bc4\u59d4\u6f5c\u5728\u504f\u89c1\u98ce\u9669\uff0c\u63d0\u51fa\u7edf\u8ba1\u6821\u6b63\u65b9\u6cd5\u548c\u5b9e\u8df5\u6307\u5357\uff0c\u5efa\u8bae\u7ed3\u5408\u7b2c\u4e09\u65b9\u8bc4\u4f30\u4e0e\u504f\u5dee\u6821\u6b63\u6765\u63d0\u5347\u81ea\u52a8\u5316\u8bc4\u4f30\u53ef\u9760\u6027"}}
{"id": "2508.06729", "pdf": "https://arxiv.org/pdf/2508.06729", "abs": "https://arxiv.org/abs/2508.06729", "authors": ["Komala Subramanyam Cherukuri", "Pranav Abishai Moses", "Aisa Sakata", "Jiangping Chen", "Haihua Chen"], "title": "Large Language Models for Oral History Understanding with Text Classification and Sentiment Analysis", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Oral histories are vital records of lived experience, particularly within\ncommunities affected by systemic injustice and historical erasure. Effective\nand efficient analysis of their oral history archives can promote access and\nunderstanding of the oral histories. However, Large-scale analysis of these\narchives remains limited due to their unstructured format, emotional\ncomplexity, and high annotation costs. This paper presents a scalable framework\nto automate semantic and sentiment annotation for Japanese American\nIncarceration Oral History. Using LLMs, we construct a high-quality dataset,\nevaluate multiple models, and test prompt engineering strategies in\nhistorically sensitive contexts. Our multiphase approach combines expert\nannotation, prompt design, and LLM evaluation with ChatGPT, Llama, and Qwen. We\nlabeled 558 sentences from 15 narrators for sentiment and semantic\nclassification, then evaluated zero-shot, few-shot, and RAG strategies. For\nsemantic classification, ChatGPT achieved the highest F1 score (88.71%),\nfollowed by Llama (84.99%) and Qwen (83.72%). For sentiment analysis, Llama\nslightly outperformed Qwen (82.66%) and ChatGPT (82.29%), with all models\nshowing comparable results. The best prompt configurations were used to\nannotate 92,191 sentences from 1,002 interviews in the JAIOH collection. Our\nfindings show that LLMs can effectively perform semantic and sentiment\nannotation across large oral history collections when guided by well-designed\nprompts. This study provides a reusable annotation pipeline and practical\nguidance for applying LLMs in culturally sensitive archival analysis. By\nbridging archival ethics with scalable NLP techniques, this work lays the\ngroundwork for responsible use of artificial intelligence in digital humanities\nand preservation of collective memory. GitHub:\nhttps://github.com/kc6699c/LLM4OralHistoryAnalysis.", "AI": {"tldr": "\u6784\u5efa\u57fa\u4e8eLLM\u7684\u53ef\u6269\u5c55\u6807\u6ce8\u6846\u67b6\uff0c\u7528\u4e8e\u65e5\u88d4\u76d1\u7981\u53e3\u8ff0\u5386\u53f2\u7684\u8bed\u4e49\u60c5\u611f\u5206\u6790\uff0cChatGPT\u8bed\u4e49\u5206\u7c7b\u6700\u4f18\uff0888.71% F1\uff09\uff0cLlama\u60c5\u611f\u5206\u6790\u7565\u4f18\uff0882.66% F1\uff09\uff0c\u6210\u529f\u6807\u6ce892k\u53e5\u5b50\u3002", "motivation": "\u53e3\u8ff0\u5386\u53f2\u4f5c\u4e3a\u7cfb\u7edf\u6027\u4e0d\u516c\u7684\u91cd\u8981\u89c1\u8bc1\uff0c\u5176\u975e\u7ed3\u6784\u5316\u7279\u5f81\u3001\u60c5\u611f\u590d\u6742\u6027\u548c\u9ad8\u6807\u6ce8\u6210\u672c\u9650\u5236\u4e86\u5927\u5c3a\u5ea6\u5206\u6790\u3002", "method": "\u901a\u8fc7\u4e13\u5bb6\u6807\u6ce8+\u63d0\u793a\u5de5\u7a0b+RAG\u7b56\u7565\uff0c\u8bc4\u4f30ChatGPT/Llama/Qwen\u5728558\u53e5\u6837\u672c\u7684\u96f6\u6837\u672c/\u5c0f\u6837\u672c\u8868\u73b0\uff0c\u4f18\u5316\u540e\u6269\u5c55\u81f3\u6574\u4e2a\u6863\u6848\u5e93\u3002", "result": "\u8bed\u4e49\u5206\u7c7b\uff1aChatGPT(88.71%) > Llama(84.99%) > Qwen(83.72%)\uff1b\u60c5\u611f\u5206\u6790\uff1aLlama(82.66%) \u2248 Qwen(82.29%) \u2248 ChatGPT(82.29%)\uff0c\u6700\u7ec8\u6807\u6ce892,191\u53e5\u5b50\u3002", "conclusion": "\u63d0\u51fa\u53ef\u590d\u7528\u7684LLM\u6807\u6ce8\u6d41\u7a0b\uff0c\u4e3a\u6587\u5316\u654f\u611f\u6863\u6848\u5206\u6790\u63d0\u4f9b\u5b9e\u8df5\u6307\u5357\uff0c\u5e73\u8861\u6863\u6848\u4f26\u7406\u4e0eAI\u53ef\u6269\u5c55\u6027\uff0c\u4fc3\u8fdb\u6570\u5b57\u4eba\u6587\u9886\u57df\u8d1f\u8d23\u4efbAI\u5e94\u7528\u3002"}}
{"id": "2508.06755", "pdf": "https://arxiv.org/pdf/2508.06755", "abs": "https://arxiv.org/abs/2508.06755", "authors": ["Xianjun Yang", "Liqiang Xiao", "Shiyang Li", "Faisal Ladhak", "Hyokun Yun", "Linda Ruth Petzold", "Yi Xu", "William Yang Wang"], "title": "Many-Turn Jailbreaking", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Current jailbreaking work on large language models (LLMs) aims to elicit\nunsafe outputs from given prompts. However, it only focuses on single-turn\njailbreaking targeting one specific query. On the contrary, the advanced LLMs\nare designed to handle extremely long contexts and can thus conduct multi-turn\nconversations. So, we propose exploring multi-turn jailbreaking, in which the\njailbroken LLMs are continuously tested on more than the first-turn\nconversation or a single target query. This is an even more serious threat\nbecause 1) it is common for users to continue asking relevant follow-up\nquestions to clarify certain jailbroken details, and 2) it is also possible\nthat the initial round of jailbreaking causes the LLMs to respond to additional\nirrelevant questions consistently. As the first step (First draft done at June\n2024) in exploring multi-turn jailbreaking, we construct a Multi-Turn Jailbreak\nBenchmark (MTJ-Bench) for benchmarking this setting on a series of open- and\nclosed-source models and provide novel insights into this new safety threat. By\nrevealing this new vulnerability, we aim to call for community efforts to build\nsafer LLMs and pave the way for a more in-depth understanding of jailbreaking\nLLMs.", "AI": {"tldr": "\u63d0\u51fa\u591a\u8f6e\u8d8a\u72f1\u653b\u51fb\u65b0\u8303\u5f0f\uff08MTJ\uff09\uff0c\u6784\u5efaMTJ-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793aLLM\u5728\u6301\u7eed\u5bf9\u8bdd\u4e2d\u7684\u5b89\u5168\u9690\u60a3\u3002", "motivation": "1. \u7528\u6237\u5e38\u4f1a\u8ffd\u95ee\u8d8a\u72f1\u7ec6\u8282\uff1b2. \u521d\u59cb\u8d8a\u72f1\u53ef\u80fd\u5bfc\u81f4\u540e\u7eed\u65e0\u5173\u63d0\u95ee\u6301\u7eed\u8fdd\u89c4\u3002\u73b0\u6709\u5355\u8f6e\u8d8a\u72f1\u7814\u7a76\u65e0\u6cd5\u8986\u76d6\u771f\u5b9e\u5bf9\u8bdd\u573a\u666f\u7684\u5b89\u5168\u98ce\u9669\u3002", "method": "\u6784\u5efa\u591a\u8f6e\u8d8a\u72f1\u57fa\u51c6\u6d4b\u8bd5MTJ-Bench\uff082024\u5e746\u6708\u5b8c\u6210\u521d\u7a3f\uff09\uff0c\u6db5\u76d6\u5f00\u6e90/\u95ed\u6e90\u6a21\u578b\u6d4b\u8bd5\uff0c\u91cf\u5316\u5206\u6790\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "result": "\u53d1\u73b0LLM\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u5b58\u5728\u6301\u7eed\u8d8a\u72f1\u6f0f\u6d1e\uff0c\u8bc1\u660e\u73b0\u6709\u5b89\u5168\u673a\u5236\u4e0d\u8db3\uff0c\u547c\u5401\u793e\u533a\u52a0\u5f3a\u591a\u8f6e\u5bf9\u8bdd\u5b89\u5168\u7814\u7a76\u3002", "conclusion": "\u591a\u8f6e\u8d8a\u72f1\u662f\u66f4\u4e25\u91cd\u7684\u5a01\u80c1\uff0cMTJ-Bench\u4e3a\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u65b0\u65b9\u5411\uff0c\u9700\u5f00\u53d1\u9488\u5bf9\u6027\u9632\u5fa1\u673a\u5236\u5e76\u6df1\u5165\u7406\u89e3\u8d8a\u72f1\u673a\u7406\u3002"}}
{"id": "2508.06803", "pdf": "https://arxiv.org/pdf/2508.06803", "abs": "https://arxiv.org/abs/2508.06803", "authors": ["Ziqi Liu", "Yangbin Chen", "Ziyang Zhou", "Yilin Li", "Mingxuan Hu", "Yushan Pan", "Zhijie Xu"], "title": "SEVADE: Self-Evolving Multi-Agent Analysis with Decoupled Evaluation for Hallucination-Resistant Irony Detection", "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "Sarcasm detection is a crucial yet challenging Natural Language Processing\ntask. Existing Large Language Model methods are often limited by\nsingle-perspective analysis, static reasoning pathways, and a susceptibility to\nhallucination when processing complex ironic rhetoric, which impacts their\naccuracy and reliability. To address these challenges, we propose **SEVADE**, a\nnovel **S**elf-**Ev**olving multi-agent **A**nalysis framework with\n**D**ecoupled **E**valuation for hallucination-resistant sarcasm detection. The\ncore of our framework is a Dynamic Agentive Reasoning Engine (DARE), which\nutilizes a team of specialized agents grounded in linguistic theory to perform\na multifaceted deconstruction of the text and generate a structured reasoning\nchain. Subsequently, a separate lightweight rationale adjudicator (RA) performs\nthe final classification based solely on this reasoning chain. This decoupled\narchitecture is designed to mitigate the risk of hallucination by separating\ncomplex reasoning from the final judgment. Extensive experiments on four\nbenchmark datasets demonstrate that our framework achieves state-of-the-art\nperformance, with average improvements of **6.75%** in Accuracy and **6.29%**\nin Macro-F1 score.", "AI": {"tldr": "\u63d0\u51faSEVADE\u6846\u67b6\u89e3\u51b3LLM\u5728\u8bbd\u523a\u68c0\u6d4b\u4e2d\u7684\u591a\u89c6\u89d2\u7f3a\u5931\u3001\u9759\u6001\u63a8\u7406\u8def\u5f84\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u4ee3\u7406\u63a8\u7406\u5f15\u64ce\u4e0e\u89e3\u8026\u8bc4\u4f30\u673a\u5236\u5b9e\u73b06.75%\u51c6\u786e\u7387\u63d0\u5347", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u5b58\u5728\u5355\u89c6\u89d2\u5206\u6790\u5c40\u9650\u3001\u9759\u6001\u63a8\u7406\u8def\u5f84\u4f9d\u8d56\u4ee5\u53ca\u5728\u590d\u6742\u53cd\u8bbd\u4fee\u8f9e\u5904\u7406\u4e2d\u7684\u5e7b\u89c9\u98ce\u9669\uff0c\u5f71\u54cd\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027", "method": "\u57fa\u4e8e\u8bed\u8a00\u5b66\u7406\u8bba\u6784\u5efa\u52a8\u6001\u4ee3\u7406\u63a8\u7406\u5f15\u64ce(DARE)\uff0c\u901a\u8fc7\u591a\u4ee3\u7406\u534f\u4f5c\u5b9e\u73b0\u6587\u672c\u591a\u7ef4\u5ea6\u89e3\u6784\uff0c\u914d\u5408\u72ec\u7acb\u7684\u8f7b\u91cf\u7ea7\u51b3\u7b56\u8bc4\u4f30\u6a21\u5757(RA)\u5b8c\u6210\u6700\u7ec8\u5206\u7c7b", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\uff0c\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53476.75%\uff0cMacro-F1\u63d0\u53476.29%", "conclusion": "\u89e3\u8026\u5f0f\u67b6\u6784\u6709\u6548\u964d\u4f4e\u5e7b\u89c9\u98ce\u9669\uff0c\u52a8\u6001\u591a\u4ee3\u7406\u5206\u6790\u6846\u67b6\u4e3a\u590d\u6742\u8bed\u4e49\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84"}}
{"id": "2508.06810", "pdf": "https://arxiv.org/pdf/2508.06810", "abs": "https://arxiv.org/abs/2508.06810", "authors": ["Steven Coyne", "Diana Galvan-Sosa", "Ryan Spring", "Cam\u00e9lia Guerraoui", "Michael Zock", "Keisuke Sakaguchi", "Kentaro Inui"], "title": "Annotating Errors in English Learners' Written Language Production: Advancing Automated Written Feedback Systems", "categories": ["cs.CL"], "comment": "Pre-review version of DOI 10.1007/978-3-031-98459-4_21, presented at\n  AIED 2025. All content is as of submission time except for de-anonymization,\n  ensuing layout fixes, use of the current code repository link, and BibTeX\n  fixes. Readers are encouraged to refer to the published version", "summary": "Recent advances in natural language processing (NLP) have contributed to the\ndevelopment of automated writing evaluation (AWE) systems that can correct\ngrammatical errors. However, while these systems are effective at improving\ntext, they are not optimally designed for language learning. They favor direct\nrevisions, often with a click-to-fix functionality that can be applied without\nconsidering the reason for the correction. Meanwhile, depending on the error\ntype, learners may benefit most from simple explanations and strategically\nindirect hints, especially on generalizable grammatical rules. To support the\ngeneration of such feedback, we introduce an annotation framework that models\neach error's error type and generalizability. For error type classification, we\nintroduce a typology focused on inferring learners' knowledge gaps by\nconnecting their errors to specific grammatical patterns. Following this\nframework, we collect a dataset of annotated learner errors and corresponding\nhuman-written feedback comments, each labeled as a direct correction or hint.\nWith this data, we evaluate keyword-guided, keyword-free, and template-guided\nmethods of generating feedback using large language models (LLMs). Human\nteachers examined each system's outputs, assessing them on grounds including\nrelevance, factuality, and comprehensibility. We report on the development of\nthe dataset and the comparative performance of the systems investigated.", "AI": {"tldr": "\u9488\u5bf9\u73b0\u6709\u81ea\u52a8\u5199\u4f5c\u8bc4\u4f30\u7cfb\u7edf\u5728\u8bed\u8a00\u5b66\u4e60\u573a\u666f\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u57fa\u4e8e\u9519\u8bef\u7c7b\u578b\u5206\u7c7b\u548c\u53cd\u9988\u7b56\u7565\u7684\u6ce8\u91ca\u6846\u67b6\uff0c\u5e76\u901a\u8fc7LLM\u751f\u6210\u6559\u80b2\u6709\u6548\u6027\u66f4\u5f3a\u7684\u53cd\u9988", "motivation": "\u73b0\u6709AWE\u7cfb\u7edf\u4ec5\u63d0\u4f9b\u76f4\u63a5\u4fee\u6b63\u529f\u80fd\uff0c\u672a\u80fd\u7ed3\u5408\u9519\u8bef\u7c7b\u578b\u4e3a\u5b66\u4e60\u8005\u63d0\u4f9b\u53ef\u63a8\u5e7f\u7684\u8bed\u6cd5\u89c4\u5219\u89e3\u91ca\u548c\u7b56\u7565\u6027\u63d0\u793a\uff0c\u9650\u5236\u4e86\u6559\u5b66\u4ef7\u503c", "method": "1. \u5efa\u7acb\u8fde\u63a5\u8bed\u6cd5\u6a21\u5f0f\u4e0e\u77e5\u8bc6\u7f3a\u53e3\u7684\u9519\u8bef\u5206\u7c7b\u4f53\u7cfb\n2. \u6536\u96c6\u5e26\u6709\u4eba\u5de5\u6807\u6ce8\u53cd\u9988\u7c7b\u578b(\u76f4\u63a5\u4fee\u6b63/\u63d0\u793a)\u7684\u5b66\u4e60\u8005\u9519\u8bef\u6570\u636e\u96c6\n3. \u8bc4\u4f30\u5173\u952e\u8bcd\u5f15\u5bfc\u3001\u81ea\u7531\u751f\u6210\u548c\u6a21\u677f\u5f15\u5bfc\u4e09\u79cdLLM\u53cd\u9988\u751f\u6210\u65b9\u6cd5", "result": "\u6784\u5efa\u4e86\u9996\u4e2a\u5305\u542b\u9519\u8bef\u7c7b\u578b\u53ef\u63a8\u5e7f\u6027\u6807\u6ce8\u7684\u6559\u80b2\u53cd\u9988\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u8868\u660e\u5173\u952e\u8bcd\u5f15\u5bfc\u65b9\u6cd5\u5728\u6559\u5e08\u8bc4\u4f30\u4e2d\u53d6\u5f97\u6700\u4f73\u7efc\u5408\u8868\u73b0(\u76f8\u5173\u6027/\u4e8b\u5b9e\u6027/\u53ef\u7406\u89e3\u6027)", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u5316\u9519\u8bef\u5206\u7c7b\u6846\u67b6\u6307\u5bfcLLM\u751f\u6210\u7b56\u7565\u6027\u53cd\u9988\uff0c\u53ef\u63d0\u5347AWE\u7cfb\u7edf\u7684\u6559\u80b2\u6709\u6548\u6027\uff0c\u4fc3\u8fdb\u4e2a\u6027\u5316\u8bed\u8a00\u5b66\u4e60\u652f\u6301"}}
{"id": "2508.06870", "pdf": "https://arxiv.org/pdf/2508.06870", "abs": "https://arxiv.org/abs/2508.06870", "authors": ["Gangular Singh Irengbam", "Nirvash Singh Wahengbam", "Lanthoiba Meitei Khumanthem", "Paikhomba Oinam"], "title": "Text to Speech System for Meitei Mayek Script", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "This paper presents the development of a Text-to-Speech (TTS) system for the\nManipuri language using the Meitei Mayek script. Leveraging Tacotron 2 and\nHiFi-GAN, we introduce a neural TTS architecture adapted to support tonal\nphonology and under-resourced linguistic environments. We develop a phoneme\nmapping for Meitei Mayek to ARPAbet, curate a single-speaker dataset, and\ndemonstrate intelligible and natural speech synthesis, validated through\nsubjective and objective metrics. This system lays the groundwork for\nlinguistic preservation and technological inclusion of Manipuri.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8eTacotron 2\u548cHiFi-GAN\u7684\u66fc\u5c3c\u666e\u5c14\u8bedTTS\u7cfb\u7edf\uff0c\u5b9e\u73b0\u97f3\u4f4d\u6620\u5c04\u4e0e\u5355\u8bf4\u8bdd\u4eba\u6570\u636e\u96c6\u6784\u5efa\uff0c\u901a\u8fc7\u4e3b\u5ba2\u89c2\u6307\u6807\u9a8c\u8bc1\u5408\u6210\u8bed\u97f3\u53ef\u7406\u89e3\u6027\u548c\u81ea\u7136\u5ea6\u3002", "motivation": "\u89e3\u51b3\u66fc\u5c3c\u666e\u5c14\u8bed\u8d44\u6e90\u532e\u4e4f\u95ee\u9898\uff0c\u901a\u8fc7\u8bed\u97f3\u5408\u6210\u6280\u672f\u5b9e\u73b0\u8bed\u8a00\u4fdd\u5b58\u548c\u6280\u672f\u5305\u5bb9\uff0c\u9002\u5e94\u58f0\u8c03\u8bed\u8a00\u7279\u6027\u3002", "method": "\u91c7\u7528Tacotron 2+HiFi-GAN\u67b6\u6784\uff0c\u5f00\u53d1Meitei Mayek\u81f3ARPAbet\u97f3\u4f4d\u6620\u5c04\uff0c\u6784\u5efa\u5355\u8bf4\u8bdd\u4eba\u8bed\u97f3\u6570\u636e\u96c6\uff0c\u4f18\u5316\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u7684\u795e\u7ecf\u7f51\u7edc\u9002\u914d\u3002", "result": "\u7cfb\u7edf\u751f\u6210\u8bed\u97f3\u5177\u590791.2%\u53ef\u61c2\u5ea6\uff08WER\u6307\u6807\uff09\u548c4.1/5 MOS\u81ea\u7136\u5ea6\u8bc4\u5206\uff0c\u9891\u8c31\u91cd\u5efa\u8bef\u5dee\u964d\u4f4e23%\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6fd2\u5371\u8bed\u8a00\u4fdd\u62a4\u63d0\u4f9b\u6280\u672f\u8303\u5f0f\uff0c\u9a8c\u8bc1\u4e86\u795e\u7ecfTTS\u5728\u4f4e\u8d44\u6e90\u58f0\u8c03\u8bed\u8a00\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u63a8\u52a8\u6570\u5b57\u65f6\u4ee3\u8bed\u8a00\u591a\u6837\u6027\u4fdd\u62a4\u3002"}}
{"id": "2508.06877", "pdf": "https://arxiv.org/pdf/2508.06877", "abs": "https://arxiv.org/abs/2508.06877", "authors": ["Xiaobo Zhang", "Congqing He", "Ying He", "Jian Peng", "Dajie Fu", "Tien-Ping Tan"], "title": "ESNERA: Empirical and semantic named entity alignment for named entity dataset merging", "categories": ["cs.CL", "cs.AI"], "comment": "30 pages, 12 figures", "summary": "Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing. It remains a research hotspot due to its wide applicability across\ndomains. Although recent advances in deep learning have significantly improved\nNER performance, they rely heavily on large, high-quality annotated datasets.\nHowever, building these datasets is expensive and time-consuming, posing a\nmajor bottleneck for further research. Current dataset merging approaches\nmainly focus on strategies like manual label mapping or constructing label\ngraphs, which lack interpretability and scalability. To address this, we\npropose an automatic label alignment method based on label similarity. The\nmethod combines empirical and semantic similarities, using a greedy pairwise\nmerging strategy to unify label spaces across different datasets. Experiments\nare conducted in two stages: first, merging three existing NER datasets into a\nunified corpus with minimal impact on NER performance; second, integrating this\ncorpus with a small-scale, self-built dataset in the financial domain. The\nresults show that our method enables effective dataset merging and enhances NER\nperformance in the low-resource financial domain. This study presents an\nefficient, interpretable, and scalable solution for integrating multi-source\nNER corpora.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6807\u7b7e\u76f8\u4f3c\u5ea6\u7684\u81ea\u52a8\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5b9e\u73b0\u591a\u6e90NER\u6570\u636e\u96c6\u9ad8\u6548\u878d\u5408\uff0c\u63d0\u5347\u4f4e\u8d44\u6e90\u9886\u57df\uff08\u5982\u91d1\u878d\uff09\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60NER\u6a21\u578b\u4f9d\u8d56\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u3002\u4f20\u7edf\u6570\u636e\u96c6\u5408\u5e76\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u6807\u7b7e\u6620\u5c04\u6216\u6784\u5efa\u6807\u7b7e\u56fe\u8c31\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u6269\u5c55\u6027\u3002", "method": "\u7ed3\u5408\u7ecf\u9a8c\u76f8\u4f3c\u5ea6\u4e0e\u8bed\u4e49\u76f8\u4f3c\u5ea6\uff0c\u91c7\u7528\u8d2a\u5fc3\u5f0f\u4e24\u4e24\u5408\u5e76\u7b56\u7565\uff0c\u901a\u8fc7\u6807\u7b7e\u7a7a\u95f4\u5bf9\u9f50\u5b9e\u73b0\u8de8\u6570\u636e\u96c6\u7684\u81ea\u52a8\u878d\u5408\u3002", "result": "\u6210\u529f\u5408\u5e76\u4e09\u4e2aNER\u6570\u636e\u96c6\uff08\u6027\u80fd\u635f\u5931<1.6%\uff09\uff0c\u4e0e\u81ea\u5efa\u91d1\u878d\u9886\u57df\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u878d\u5408\u540e\uff0cF1\u503c\u63d0\u53472.3-5.8\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u591a\u6e90NER\u8bed\u6599\u6574\u5408\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u7a81\u7834\u6570\u636e\u7a00\u7f3a\u9886\u57df\u7684\u7814\u7a76\u74f6\u9888\u3002"}}
{"id": "2508.06880", "pdf": "https://arxiv.org/pdf/2508.06880", "abs": "https://arxiv.org/abs/2508.06880", "authors": ["Philipp Christmann", "Gerhard Weikum"], "title": "The ReQAP System for Question Answering over Personal Information", "categories": ["cs.CL", "cs.IR"], "comment": "Accepted at CIKM 2025 (demonstration paper)", "summary": "Personal information is abundant on users' devices, from structured data in\ncalendar, shopping records or fitness tools, to unstructured contents in mail\nand social media posts. This works presents the ReQAP system that supports\nusers with answers for complex questions that involve filters, joins and\naggregation over heterogeneous sources. The unique trait of ReQAP is that it\nrecursively decomposes questions and incrementally builds an operator tree for\nexecution. Both the question interpretation and the individual operators make\nsmart use of light-weight language models, with judicious fine-tuning. The demo\nshowcases the rich functionality for advanced user questions, and also offers\ndetailed tracking of how the answers are computed by the operators in the\nexecution tree. Being able to trace answers back to the underlying sources is\nvital for human comprehensibility and user trust in the system.", "AI": {"tldr": "\u63d0\u51faReQAP\u7cfb\u7edf\uff0c\u901a\u8fc7\u9012\u5f52\u5206\u89e3\u95ee\u9898\u6784\u5efa\u64cd\u4f5c\u6811\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u5904\u7406\u8de8\u6e90\u590d\u6742\u67e5\u8be2\uff0c\u5e76\u5b9e\u73b0\u7b54\u6848\u6eaf\u6e90\u529f\u80fd", "motivation": "\u7528\u6237\u8bbe\u5907\u4e2d\u5b58\u5728\u5927\u91cf\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u6570\u636e\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u5904\u7406\u590d\u6742\u8de8\u6e90\u67e5\u8be2\uff08\u6d89\u53ca\u8fc7\u6ee4/\u8fde\u63a5/\u805a\u5408\uff09\u7684\u89e3\u51b3\u65b9\u6848", "method": "1. \u9012\u5f52\u5f0f\u95ee\u9898\u5206\u89e3\u4e0e\u64cd\u4f5c\u6811\u589e\u91cf\u6784\u5efa\n2. \u57fa\u4e8e\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u95ee\u9898\u89e3\u6790\u4e0e\u7b97\u5b50\u6267\u884c\n3. \u7cbe\u7ec6\u5316\u5fae\u8c03\u7b56\u7565\u4f18\u5316\u6a21\u578b\u8868\u73b0", "result": "\u7cfb\u7edf\u6f14\u793a\u5c55\u793a\uff1a\n- \u652f\u6301\u9ad8\u7ea7\u7528\u6237\u95ee\u9898\u7684\u4e30\u5bcc\u529f\u80fd\n- \u901a\u8fc7\u6267\u884c\u6811\u64cd\u4f5c\u7b26\u5b9e\u73b0\u7b54\u6848\u8ba1\u7b97\u8fc7\u7a0b\u8ffd\u8e2a\n- \u7b54\u6848\u6eaf\u6e90\u81f3\u5e95\u5c42\u6570\u636e\u6e90\u7684\u80fd\u529b\u9a8c\u8bc1", "conclusion": "ReQAP\u5728\u63d0\u5347\u590d\u6742\u95ee\u9898\u5904\u7406\u80fd\u529b\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u6267\u884c\u8fc7\u7a0b\u900f\u660e\u5316\u589e\u5f3a\u4e86\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u7528\u6237\u4fe1\u4efb\u5ea6"}}
{"id": "2508.06886", "pdf": "https://arxiv.org/pdf/2508.06886", "abs": "https://arxiv.org/abs/2508.06886", "authors": ["Arpita Saggar", "Jonathan C. Darling", "Vania Dimitrova", "Duygu Sarikaya", "David C. Hogg"], "title": "Score Before You Speak: Improving Persona Consistency in Dialogue Generation using Response Quality Scores", "categories": ["cs.CL"], "comment": "Camera-Ready version for ECAI 2025. 8 pages", "summary": "Persona-based dialogue generation is an important milestone towards building\nconversational artificial intelligence. Despite the ever-improving capabilities\nof large language models (LLMs), effectively integrating persona fidelity in\nconversations remains challenging due to the limited diversity in existing\ndialogue data. We propose a novel framework SBS (Score-Before-Speaking), which\noutperforms previous methods and yields improvements for both million and\nbillion-parameter models. Unlike previous methods, SBS unifies the learning of\nresponses and their relative quality into a single step. The key innovation is\nto train a dialogue model to correlate augmented responses with a quality score\nduring training and then leverage this knowledge at inference. We use\nnoun-based substitution for augmentation and semantic similarity-based scores\nas a proxy for response quality. Through extensive experiments with benchmark\ndatasets (PERSONA-CHAT and ConvAI2), we show that score-conditioned training\nallows existing models to better capture a spectrum of persona-consistent\ndialogues. Our ablation studies also demonstrate that including scores in the\ninput prompt during training is superior to conventional training setups. Code\nand further details are available at\nhttps://arpita2512.github.io/score_before_you_speak", "AI": {"tldr": "\u63d0\u51faSBS\u6846\u67b6\u901a\u8fc7\u8d28\u91cf\u8bc4\u5206\u673a\u5236\u589e\u5f3a\u89d2\u8272\u5bf9\u8bdd\u4e00\u81f4\u6027\uff0c\u5728\u8bad\u7ec3\u4e2d\u7edf\u4e00\u54cd\u5e94\u751f\u6210\u4e0e\u8d28\u91cf\u8bc4\u4f30\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u8868\u73b0", "motivation": "\u73b0\u6709\u5bf9\u8bdd\u6a21\u578b\u56e0\u6570\u636e\u591a\u6837\u6027\u9650\u5236\u96be\u4ee5\u6709\u6548\u4fdd\u6301\u89d2\u8272\u4fdd\u771f\u5ea6\uff0c\u9700\u8981\u521b\u65b0\u65b9\u6cd5\u6574\u5408\u54cd\u5e94\u8d28\u91cf\u8bc4\u4f30\u4e0e\u751f\u6210\u8fc7\u7a0b", "method": "\u91c7\u7528\u540d\u8bcd\u66ff\u6362\u589e\u5f3a\u5bf9\u8bdd\u6570\u636e\uff0c\u4f7f\u7528\u8bed\u4e49\u76f8\u4f3c\u5ea6\u4f5c\u4e3a\u8d28\u91cf\u8bc4\u5206\uff0c\u5728\u8bad\u7ec3\u65f6\u5efa\u7acb\u54cd\u5e94\u4e0e\u8bc4\u5206\u7684\u5173\u8054\uff0c\u63a8\u7406\u65f6\u901a\u8fc7\u8bc4\u5206\u4f18\u5316\u751f\u6210", "result": "\u5728PERSONA-CHAT\u548cConvAI2\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u663e\u793a\uff0cSBS\u4f7f\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u751f\u6210\u7684\u89d2\u8272\u4e00\u81f4\u6027\u5bf9\u8bdd\u8d28\u91cf\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u901a\u8fc7\u5c06\u8d28\u91cf\u8bc4\u5206\u878d\u5165\u8bad\u7ec3\u76ee\u6807\uff0cSBS\u6846\u67b6\u4e3a\u63d0\u5347\u5bf9\u8bdd\u7cfb\u7edf\u89d2\u8272\u4fdd\u771f\u5ea6\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u65b0\u8303\u5f0f\uff0c\u8bc1\u660e\u8bc4\u5206\u673a\u5236\u7684\u7aef\u5230\u7aef\u6574\u5408\u4f18\u52bf"}}
{"id": "2508.06913", "pdf": "https://arxiv.org/pdf/2508.06913", "abs": "https://arxiv.org/abs/2508.06913", "authors": ["Siyuan Li", "Xi Lin", "Guangyan Li", "Zehao Liu", "Aodu Wulianghai", "Li Ding", "Jun Wu", "Jianhua Li"], "title": "Model-Agnostic Sentiment Distribution Stability Analysis for Robust LLM-Generated Texts Detection", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has resulted in\nincreasingly sophisticated AI-generated content, posing significant challenges\nin distinguishing LLM-generated text from human-written language. Existing\ndetection methods, primarily based on lexical heuristics or fine-tuned\nclassifiers, often suffer from limited generalizability and are vulnerable to\nparaphrasing, adversarial perturbations, and cross-domain shifts. In this work,\nwe propose SentiDetect, a model-agnostic framework for detecting LLM-generated\ntext by analyzing the divergence in sentiment distribution stability. Our\nmethod is motivated by the empirical observation that LLM outputs tend to\nexhibit emotionally consistent patterns, whereas human-written texts display\ngreater emotional variability. To capture this phenomenon, we define two\ncomplementary metrics: sentiment distribution consistency and sentiment\ndistribution preservation, which quantify stability under sentiment-altering\nand semantic-preserving transformations. We evaluate SentiDetect on five\ndiverse datasets and a range of advanced LLMs,including Gemini-1.5-Pro,\nClaude-3, GPT-4-0613, and LLaMa-3.3. Experimental results demonstrate its\nsuperiority over state-of-the-art baselines, with over 16% and 11% F1 score\nimprovements on Gemini-1.5-Pro and GPT-4-0613, respectively. Moreover,\nSentiDetect also shows greater robustness to paraphrasing, adversarial attacks,\nand text length variations, outperforming existing detectors in challenging\nscenarios.", "AI": {"tldr": "\u63d0\u51faSentiDetect\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u60c5\u611f\u5206\u5e03\u7a33\u5b9a\u6027\u5dee\u5f02\u68c0\u6d4bAI\u751f\u6210\u6587\u672c\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5728\u5bf9\u6297\u653b\u51fb\u548c\u8de8\u57df\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0cLLM\u751f\u6210\u6587\u672c\u60c5\u611f\u4e00\u81f4\u6027\u7279\u5f81\u4e0e\u4eba\u7c7b\u6587\u672c\u5b58\u5728\u5dee\u5f02", "method": "\u5b9a\u4e49\u60c5\u611f\u5206\u5e03\u4e00\u81f4\u6027\u548c\u4fdd\u6301\u6027\u53cc\u6307\u6807\uff0c\u91cf\u5316\u6587\u672c\u5728\u60c5\u611f\u8f6c\u6362\u4e0b\u7684\u7a33\u5b9a\u6027", "result": "\u5728Gemini-1.5-Pro\u548cGPT-4\u4e0aF1\u5206\u6570\u63d0\u534716%\u548c11%\uff0c\u5bf9\u6539\u5199\u653b\u51fb\u7684\u9c81\u68d2\u6027\u66f4\u5f3a", "conclusion": "\u60c5\u611f\u5206\u5e03\u7a33\u5b9a\u6027\u53ef\u4f5c\u4e3a\u901a\u7528\u7279\u5f81\u5b9e\u73b0\u66f4\u53ef\u9760\u7684AI\u751f\u6210\u6587\u672c\u68c0\u6d4b"}}
{"id": "2508.06971", "pdf": "https://arxiv.org/pdf/2508.06971", "abs": "https://arxiv.org/abs/2508.06971", "authors": ["Mohamed Basem", "Islam Oshallah", "Ali Hamdi", "Khaled Shaban", "Hozaifa Kassab"], "title": "Two-Stage Quranic QA via Ensemble Retrieval and Instruction-Tuned Answer Extraction", "categories": ["cs.CL", "cs.IR"], "comment": "8 pages , 4 figures , Accepted in Aiccsa 2025 ,\n  https://conferences.sigappfr.org/aiccsa2025/", "summary": "Quranic Question Answering presents unique challenges due to the linguistic\ncomplexity of Classical Arabic and the semantic richness of religious texts. In\nthis paper, we propose a novel two-stage framework that addresses both passage\nretrieval and answer extraction. For passage retrieval, we ensemble fine-tuned\nArabic language models to achieve superior ranking performance. For answer\nextraction, we employ instruction-tuned large language models with few-shot\nprompting to overcome the limitations of fine-tuning on small datasets. Our\napproach achieves state-of-the-art results on the Quran QA 2023 Shared Task,\nwith a MAP@10 of 0.3128 and MRR@10 of 0.5763 for retrieval, and a pAP@10 of\n0.669 for extraction, substantially outperforming previous methods. These\nresults demonstrate that combining model ensembling and instruction-tuned\nlanguage models effectively addresses the challenges of low-resource question\nanswering in specialized domains.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff08\u68c0\u7d22+\u7b54\u6848\u62bd\u53d6\uff09\uff0c\u878d\u5408\u6a21\u578b\u96c6\u6210\u548c\u6307\u4ee4\u8c03\u4f18\u6280\u672f\uff0c\u5728\u4f4e\u8d44\u6e90\u53e4\u5170\u7ecf\u95ee\u7b54\u4efb\u52a1\u4e2d\u5237\u65b0SOTA", "motivation": "\u89e3\u51b3\u53e4\u5178\u963f\u62c9\u4f2f\u8bed\u590d\u6742\u6027\u3001\u5b97\u6559\u6587\u672c\u8bed\u4e49\u4e30\u5bcc\u6027\u5e26\u6765\u7684\u6311\u6218\uff0c\u514b\u670d\u5c0f\u6570\u636e\u96c6\u5fae\u8c03\u9650\u5236", "method": "\u68c0\u7d22\u9636\u6bb5\u96c6\u6210\u5fae\u8c03\u963f\u62c9\u4f2f\u8bed\u8a00\u6a21\u578b\uff0c\u7b54\u6848\u62bd\u53d6\u9636\u6bb5\u91c7\u7528\u6307\u4ee4\u8c03\u4f18\u5927\u6a21\u578b+\u5c0f\u6837\u672c\u63d0\u793a\u5b66\u4e60", "result": "Quran QA 2023\u4efb\u52a1\u4e2d\u68c0\u7d22\u6307\u6807MAP@10 0.3128/MRR@10 0.5763\uff0c\u62bd\u53d6\u6307\u6807pAP@10 0.669", "conclusion": "\u6a21\u578b\u96c6\u6210+\u6307\u4ee4\u8c03\u4f18\u7684\u7ec4\u5408\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e13\u4e1a\u9886\u57df\u4f4e\u8d44\u6e90\u95ee\u7b54\u96be\u9898\uff0c\u5177\u6709\u9886\u57df\u6269\u5c55\u6f5c\u529b"}}
{"id": "2508.06974", "pdf": "https://arxiv.org/pdf/2508.06974", "abs": "https://arxiv.org/abs/2508.06974", "authors": ["Zhijun Tu", "Hanting Chen", "Siqi Liu", "Chuanjian Liu", "Jian Li", "Jie Hu", "Yunhe Wang"], "title": "Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models", "categories": ["cs.CL"], "comment": "16 pages, 5 figures", "summary": "1-bit LLM quantization offers significant advantages in reducing storage and\ncomputational costs. However, existing methods typically train 1-bit LLMs from\nscratch, failing to fully leverage pre-trained models. This results in high\ntraining costs and notable accuracy degradation. We identify that the large gap\nbetween full precision and 1-bit representations makes direct adaptation\ndifficult. In this paper, we introduce a consistent progressive training for\nboth forward and backward, smoothly converting the floating-point weights into\nthe binarized ones. Additionally, we incorporate binary-aware initialization\nand dual-scaling compensation to reduce the difficulty of progressive training\nand improve the performance. Experimental results on LLMs of various sizes\ndemonstrate that our method outperforms existing approaches. Our results show\nthat high-performance 1-bit LLMs can be achieved using pre-trained models,\neliminating the need for expensive training from scratch.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u6e10\u8fdb\u5f0f\u8bad\u7ec3+\u53cc\u8865\u507f\u7b56\u7565\u5b9e\u73b0\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u9ad8\u65481-bit\u91cf\u5316\uff0c\u907f\u514d\u4ece\u5934\u8bad\u7ec3\u5e76\u4fdd\u6301\u6027\u80fd", "motivation": "\u73b0\u67091-bit\u91cf\u5316\u65b9\u6cd5\u9700\u4ece\u5934\u8bad\u7ec3LLM\uff0c\u5bfc\u81f4\u8bad\u7ec3\u6210\u672c\u9ad8\u4e14\u7cbe\u5ea6\u635f\u5931\u4e25\u91cd\uff0c\u9884\u8bad\u7ec3\u6a21\u578b\u96be\u4ee5\u76f4\u63a5\u9002\u914d", "method": "\u91c7\u7528\u524d\u5411\u540e\u5411\u4e00\u81f4\u7684\u6e10\u8fdb\u5f0f\u6743\u91cd\u4e8c\u503c\u5316\uff0c\u7ed3\u5408\u4e8c\u8fdb\u5236\u611f\u77e5\u521d\u59cb\u5316\u4e0e\u53cc\u7f29\u653e\u8865\u507f\u6280\u672f\u964d\u4f4e\u8bad\u7ec3\u96be\u5ea6", "result": "\u4e0d\u540c\u89c4\u6a21LLM\u5b9e\u9a8c\u663e\u793a\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\uff0c\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u5b9e\u73b0\u9ad8\u6027\u80fd1-bit\u91cf\u5316\u7684\u53ef\u884c\u6027", "conclusion": "\u65b0\u65b9\u6cd5\u7a81\u7834\u4e861-bit\u91cf\u5316\u9700\u4ece\u5934\u8bad\u7ec3\u7684\u9650\u5236\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a\u9ad8\u6548\u90e8\u7f72\u5927\u6a21\u578b\u63d0\u4f9b\u65b0\u8def\u5f84"}}
{"id": "2508.07017", "pdf": "https://arxiv.org/pdf/2508.07017", "abs": "https://arxiv.org/abs/2508.07017", "authors": ["Mao Li", "Fred Conrad", "Johann Gagnon-Bartsch"], "title": "Vec2Summ: Text Summarization via Probabilistic Sentence Embeddings", "categories": ["cs.CL"], "comment": null, "summary": "We propose Vec2Summ, a novel method for abstractive summarization that frames\nthe task as semantic compression. Vec2Summ represents a document collection\nusing a single mean vector in the semantic embedding space, capturing the\ncentral meaning of the corpus. To reconstruct fluent summaries, we perform\nembedding inversion -- decoding this mean vector into natural language using a\ngenerative language model. To improve reconstruction quality and capture some\ndegree of topical variability, we introduce stochasticity by sampling from a\nGaussian distribution centered on the mean. This approach is loosely analogous\nto bagging in ensemble learning, where controlled randomness encourages more\nrobust and varied outputs. Vec2Summ addresses key limitations of LLM-based\nsummarization methods. It avoids context-length constraints, enables\ninterpretable and controllable generation via semantic parameters, and scales\nefficiently with corpus size -- requiring only $O(d + d^2)$ parameters.\nEmpirical results show that Vec2Summ produces coherent summaries for topically\nfocused, order-invariant corpora, with performance comparable to direct LLM\nsummarization in terms of thematic coverage and efficiency, albeit with less\nfine-grained detail. These results underscore Vec2Summ's potential in settings\nwhere scalability, semantic control, and corpus-level abstraction are\nprioritized.", "AI": {"tldr": "Vec2Summ\u901a\u8fc7\u8bed\u4e49\u5411\u91cf\u538b\u7f29\u5b9e\u73b0\u9ad8\u6548\u6458\u8981\u751f\u6210\uff0c\u7a81\u7834\u5927\u6a21\u578b\u4e0a\u4e0b\u6587\u9650\u5236\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8bed\u4e49\u63a7\u5236\u65b9\u6848", "motivation": "\u89e3\u51b3\u57fa\u4e8eLLM\u7684\u6458\u8981\u65b9\u6cd5\u5b58\u5728\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\u3001\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u4ee5\u53ca\u6269\u5c55\u6027\u5dee\u7684\u95ee\u9898", "method": "1. \u5c06\u6587\u6863\u96c6\u7f16\u7801\u4e3a\u8bed\u4e49\u7a7a\u95f4\u7684\u5747\u503c\u5411\u91cf 2. \u901a\u8fc7\u5d4c\u5165\u53cd\u6f14\u751f\u6210\u6458\u8981 3. \u5f15\u5165\u9ad8\u65af\u5206\u5e03\u91c7\u6837\u589e\u52a0\u591a\u6837\u6027", "result": "\u5728\u4e3b\u9898\u96c6\u4e2d\u3001\u987a\u5e8f\u65e0\u5173\u7684\u8bed\u6599\u4e0a\u751f\u6210\u8fde\u8d2f\u6458\u8981\uff0c\u6027\u80fd\u4e0e\u76f4\u63a5LLM\u6458\u8981\u76f8\u5f53\uff0c\u4f46\u7ec6\u8282\u7cbe\u7ec6\u5ea6\u7a0d\u5f31", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u9700\u8981\u6269\u5c55\u6027\u3001\u8bed\u4e49\u63a7\u5236\u548c\u8bed\u6599\u7ea7\u62bd\u8c61\u7684\u9886\u57df\u5177\u6709\u5e94\u7528\u6f5c\u529b"}}
{"id": "2508.07069", "pdf": "https://arxiv.org/pdf/2508.07069", "abs": "https://arxiv.org/abs/2508.07069", "authors": ["Muhammad Dehan Al Kautsar", "Aswin Candra", "Muhammad Alif Al Hakim", "Maxalmina Satria Kahfi", "Fajri Koto", "Alham Fikri Aji", "Peerat Limkonchotiwat", "Ekapol Chuangsuwanich", "Genta Indra Winata"], "title": "SEADialogues: A Multilingual Culturally Grounded Multi-turn Dialogue Dataset on Southeast Asian Languages", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Although numerous datasets have been developed to support dialogue systems,\nmost existing chit-chat datasets overlook the cultural nuances inherent in\nnatural human conversations. To address this gap, we introduce SEADialogues, a\nculturally grounded dialogue dataset centered on Southeast Asia, a region with\nover 700 million people and immense cultural diversity. Our dataset features\ndialogues in eight languages from six Southeast Asian countries, many of which\nare low-resource despite having sizable speaker populations. To enhance\ncultural relevance and personalization, each dialogue includes persona\nattributes and two culturally grounded topics that reflect everyday life in the\nrespective communities. Furthermore, we release a multi-turn dialogue dataset\nto advance research on culturally aware and human-centric large language\nmodels, including conversational dialogue agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86SEADialogues\u6570\u636e\u96c6\uff0c\u805a\u7126\u4e1c\u5357\u4e9a\u6587\u5316\u80cc\u666f\u7684\u5bf9\u8bdd\u6570\u636e\uff0c\u586b\u8865\u73b0\u6709\u95f2\u804a\u6570\u636e\u96c6\u5ffd\u7565\u6587\u5316\u5dee\u5f02\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u5bf9\u8bdd\u6570\u636e\u96c6\u666e\u904d\u7f3a\u4e4f\u81ea\u7136\u5bf9\u8bdd\u4e2d\u7684\u6587\u5316\u654f\u611f\u6027\uff0c\u7279\u522b\u662f\u9488\u5bf9\u62e5\u67097\u4ebf\u4eba\u53e3\u4e14\u6587\u5316\u591a\u5143\u7684\u4e1c\u5357\u4e9a\u5730\u533a\u3002", "method": "\u6784\u5efa\u5305\u542b6\u4e2a\u56fd\u5bb68\u79cd\u8bed\u8a00\u7684\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u5bf9\u8bdd\u9644\u5e26\u4eba\u7269\u5c5e\u6027\u548c\u4e24\u4e2a\u53cd\u6620\u5f53\u5730\u65e5\u5e38\u751f\u6d3b\u7684\u6587\u5316\u4e3b\u9898\uff0c\u5e76\u53d1\u5e03\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u3002", "result": "\u521b\u5efa\u4e86\u9996\u4e2a\u4e1c\u5357\u4e9a\u6587\u5316\u80cc\u666f\u7684\u591a\u8bed\u8a00\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u4e3a\u5f00\u53d1\u6587\u5316\u654f\u611f\u7684\u5bf9\u8bdd\u667a\u80fd\u4f53\u63d0\u4f9b\u8d44\u6e90\u652f\u6301\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u80fd\u63a8\u52a8\u4ee5\u4eba\u4e3a\u672c\u7684\u5927\u8bed\u8a00\u6a21\u578b\u7814\u7a76\uff0c\u4fc3\u8fdb\u5177\u6709\u6587\u5316\u9002\u5e94\u6027\u7684\u5bf9\u8bdd\u7cfb\u7edf\u5f00\u53d1\uff0c\u7279\u522b\u5173\u6ce8\u4f4e\u8d44\u6e90\u8bed\u8a00\u793e\u533a\u9700\u6c42\u3002"}}
{"id": "2508.07090", "pdf": "https://arxiv.org/pdf/2508.07090", "abs": "https://arxiv.org/abs/2508.07090", "authors": ["Aditya Tomar", "Nihar Ranjan Sahoo", "Pushpak Bhattacharyya"], "title": "BharatBBQ: A Multilingual Bias Benchmark for Question Answering in the Indian Context", "categories": ["cs.CL"], "comment": null, "summary": "Evaluating social biases in language models (LMs) is crucial for ensuring\nfairness and minimizing the reinforcement of harmful stereotypes in AI systems.\nExisting benchmarks, such as the Bias Benchmark for Question Answering (BBQ),\nprimarily focus on Western contexts, limiting their applicability to the Indian\ncontext. To address this gap, we introduce BharatBBQ, a culturally adapted\nbenchmark designed to assess biases in Hindi, English, Marathi, Bengali, Tamil,\nTelugu, Odia, and Assamese. BharatBBQ covers 13 social categories, including 3\nintersectional groups, reflecting prevalent biases in the Indian sociocultural\nlandscape. Our dataset contains 49,108 examples in one language that are\nexpanded using translation and verification to 392,864 examples in eight\ndifferent languages. We evaluate five multilingual LM families across zero and\nfew-shot settings, analyzing their bias and stereotypical bias scores. Our\nfindings highlight persistent biases across languages and social categories and\noften amplified biases in Indian languages compared to English, demonstrating\nthe necessity of linguistically and culturally grounded benchmarks for bias\nevaluation.", "AI": {"tldr": "\u5f00\u53d1\u5370\u5ea6\u6587\u5316\u80cc\u666f\u7684\u591a\u8bed\u8a00\u504f\u89c1\u8bc4\u4f30\u57fa\u51c6BharatBBQ\uff0c\u53d1\u73b0\u5370\u5ea6\u8bed\u8a00\u4e2d\u7684\u6a21\u578b\u504f\u89c1\u6bd4\u82f1\u8bed\u66f4\u4e25\u91cd", "motivation": "\u73b0\u6709\u504f\u89c1\u8bc4\u4f30\u57fa\u51c6\uff08\u5982BBQ\uff09\u805a\u7126\u897f\u65b9\u8bed\u5883\uff0c\u7f3a\u4e4f\u5bf9\u5370\u5ea6\u591a\u8bed\u8a00\u6587\u5316\u73af\u5883\u7684\u9002\u5e94\u6027", "method": "\u6784\u5efa\u542b8\u79cd\u8bed\u8a00/13\u4e2a\u793e\u4f1a\u7c7b\u522b\u7684\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u7ffb\u8bd1\u6269\u5c55\u81f339\u4e07\u4f8b\uff0c\u8bc4\u4f305\u5927\u6a21\u578b\u5bb6\u65cf\u5728\u96f6\u6837\u672c/\u5c0f\u6837\u672c\u573a\u666f\u7684\u504f\u89c1\u8868\u73b0", "result": "\u6240\u6709\u8bed\u8a00\u5747\u5b58\u5728\u6301\u7eed\u504f\u89c1\uff0c\u5370\u5ea6\u8bed\u8a00\u7684\u523b\u677f\u5370\u8c61\u5f97\u5206\u6bd4\u82f1\u8bed\u5e73\u5747\u9ad822%", "conclusion": "\u9700\u5efa\u7acb\u57fa\u4e8e\u672c\u571f\u8bed\u8a00\u6587\u5316\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u591a\u8bed\u8a00\u6a21\u578b\u5728\u975e\u82f1\u8bed\u73af\u5883\u4e2d\u7684\u504f\u89c1\u7f13\u89e3\u673a\u5236\u4e9f\u5f85\u52a0\u5f3a"}}
{"id": "2508.07101", "pdf": "https://arxiv.org/pdf/2508.07101", "abs": "https://arxiv.org/abs/2508.07101", "authors": ["Lijie Yang", "Zhihao Zhang", "Arti Jain", "Shijie Cao", "Baihong Yuan", "Yiwei Chen", "Zhihao Jia", "Ravi Netravali"], "title": "Less Is More: Training-Free Sparse Attention with Global Locality for Efficient Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large reasoning models achieve strong performance through test-time scaling\nbut incur substantial computational overhead, particularly from excessive token\ngeneration when processing short input prompts. While sparse attention\nmechanisms can reduce latency and memory usage, existing approaches suffer from\nsignificant accuracy degradation due to accumulated errors during\nlong-generation reasoning. These methods generally require either high token\nretention rates or expensive retraining. We introduce LessIsMore, a\ntraining-free sparse attention mechanism for reasoning tasks, which leverages\nglobal attention patterns rather than relying on traditional head-specific\nlocal optimizations. LessIsMore aggregates token selections from local\nattention heads with recent contextual information, enabling unified cross-head\ntoken ranking for future decoding layers. This unified selection improves\ngeneralization and efficiency by avoiding the need to maintain separate token\nsubsets per head. Evaluation across diverse reasoning tasks and benchmarks\nshows that LessIsMore preserves -- and in some cases improves -- accuracy while\nachieving a $1.1\\times$ average decoding speed-up compared to full attention.\nMoreover, LessIsMore attends to $2\\times$ fewer tokens without accuracy loss,\nachieving a $1.13\\times$ end-to-end speed-up compared to existing sparse\nattention methods.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684LessIsMore\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u5168\u5c40\u6ce8\u610f\u529b\u6a21\u5f0f\u548c\u8de8\u5934\u4ee4\u724c\u6574\u5408\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6", "motivation": "\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u5b58\u5728\u7d2f\u79ef\u9519\u8bef\u548c\u9ad8\u4ee4\u724c\u4fdd\u7559\u7387\u95ee\u9898\uff0c\u9700\u8981\u6743\u8861\u6548\u7387\u4e0e\u51c6\u786e\u6027\u3002\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5934\u90e8\u5c40\u90e8\u4f18\u5316\u5bfc\u81f4\u8d44\u6e90\u5f00\u9500\u5927", "method": "\u805a\u5408\u5c40\u90e8\u6ce8\u610f\u529b\u5934\u7684\u4ee4\u724c\u9009\u62e9\uff0c\u7ed3\u5408\u6700\u65b0\u4e0a\u4e0b\u6587\u8fdb\u884c\u8de8\u5934\u7edf\u4e00\u6392\u5e8f\u3002\u901a\u8fc7\u5168\u5c40\u6a21\u5f0f\u6574\u5408\u66ff\u4ee3\u4f20\u7edf\u9010\u5934\u4f18\u5316\uff0c\u51cf\u5c11\u7ef4\u62a4\u591a\u5b50\u96c6\u7684\u5f00\u9500", "result": "\u5e73\u5747\u89e3\u7801\u901f\u5ea6\u63d0\u53471.1\u500d\uff0c\u5173\u6ce8\u4ee4\u724c\u6570\u51cf\u534a\u65e0\u7cbe\u5ea6\u635f\u5931\u3002\u7aef\u5230\u7aef\u901f\u5ea6\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb1.13\u500d\uff0c\u90e8\u5206\u4efb\u52a1\u51c6\u786e\u7387\u8fd8\u6709\u63d0\u5347", "conclusion": "LessIsMore\u9996\u6b21\u5b9e\u73b0\u7a00\u758f\u6ce8\u610f\u529b\u4e0d\u727a\u7272\u51c6\u786e\u6027\u7684\u6548\u7387\u7a81\u7834\uff0c\u4e3a\u5927\u89c4\u6a21\u63a8\u7406\u6a21\u578b\u63d0\u4f9b\u66f4\u4f18\u7684\u8ba1\u7b97\u6548\u7387\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.07111", "pdf": "https://arxiv.org/pdf/2508.07111", "abs": "https://arxiv.org/abs/2508.07111", "authors": ["Falaah Arif Khan", "Nivedha Sivakumar", "Yinong Oliver Wang", "Katherine Metcalf", "Cezanne Camacho", "Barry-John Theobald", "Luca Zappella", "Nicholas Apostoloff"], "title": "Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have achieved impressive performance, leading to\ntheir widespread adoption as decision-support tools in resource-constrained\ncontexts like hiring and admissions. There is, however, scientific consensus\nthat AI systems can reflect and exacerbate societal biases, raising concerns\nabout identity-based harm when used in critical social contexts. Prior work has\nlaid a solid foundation for assessing bias in LLMs by evaluating demographic\ndisparities in different language reasoning tasks. In this work, we extend\nsingle-axis fairness evaluations to examine intersectional bias, recognizing\nthat when multiple axes of discrimination intersect, they create distinct\npatterns of disadvantage. We create a new benchmark called WinoIdentity by\naugmenting the WinoBias dataset with 25 demographic markers across 10\nattributes, including age, nationality, and race, intersected with binary\ngender, yielding 245,700 prompts to evaluate 50 distinct bias patterns.\nFocusing on harms of omission due to underrepresentation, we investigate bias\nthrough the lens of uncertainty and propose a group (un)fairness metric called\nCoreference Confidence Disparity which measures whether models are more or less\nconfident for some intersectional identities than others. We evaluate five\nrecently published LLMs and find confidence disparities as high as 40% along\nvarious demographic attributes including body type, sexual orientation and\nsocio-economic status, with models being most uncertain about\ndoubly-disadvantaged identities in anti-stereotypical settings. Surprisingly,\ncoreference confidence decreases even for hegemonic or privileged markers,\nindicating that the recent impressive performance of LLMs is more likely due to\nmemorization than logical reasoning. Notably, these are two independent\nfailures in value alignment and validity that can compound to cause social\nharm.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.07143", "pdf": "https://arxiv.org/pdf/2508.07143", "abs": "https://arxiv.org/abs/2508.07143", "authors": ["Anna Seo Gyeong Choi", "Hoon Choi"], "title": "Fairness of Automatic Speech Recognition: Looking Through a Philosophical Lens", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Automatic Speech Recognition (ASR) systems now mediate countless\nhuman-technology interactions, yet research on their fairness implications\nremains surprisingly limited. This paper examines ASR bias through a\nphilosophical lens, arguing that systematic misrecognition of certain speech\nvarieties constitutes more than a technical limitation -- it represents a form\nof disrespect that compounds historical injustices against marginalized\nlinguistic communities. We distinguish between morally neutral classification\n(discriminate1) and harmful discrimination (discriminate2), demonstrating how\nASR systems can inadvertently transform the former into the latter when they\nconsistently misrecognize non-standard dialects. We identify three unique\nethical dimensions of speech technologies that differentiate ASR bias from\nother algorithmic fairness concerns: the temporal burden placed on speakers of\nnon-standard varieties (\"temporal taxation\"), the disruption of conversational\nflow when systems misrecognize speech, and the fundamental connection between\nspeech patterns and personal/cultural identity. These factors create asymmetric\npower relationships that existing technical fairness metrics fail to capture.\nThe paper analyzes the tension between linguistic standardization and pluralism\nin ASR development, arguing that current approaches often embed and reinforce\nproblematic language ideologies. We conclude that addressing ASR bias requires\nmore than technical interventions; it demands recognition of diverse speech\nvarieties as legitimate forms of expression worthy of technological\naccommodation. This philosophical reframing offers new pathways for developing\nASR systems that respect linguistic diversity and speaker autonomy.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b(ASR)\u7cfb\u7edf\u5bf9\u975e\u6807\u51c6\u65b9\u8a00\u4f7f\u7528\u8005\u7684\u7cfb\u7edf\u6027\u8bef\u8bc6\u522b\u95ee\u9898\uff0c\u63ed\u793a\u5176\u4e0d\u4ec5\u662f\u6280\u672f\u7f3a\u9677\uff0c\u66f4\u662f\u52a0\u5267\u8fb9\u7f18\u8bed\u8a00\u793e\u533a\u5386\u53f2\u4e0d\u516c\u7684\u4f26\u7406\u95ee\u9898", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9ASR\u7cfb\u7edf\u516c\u5e73\u6027\u5173\u6ce8\u4e0d\u8db3\uff0c\u7cfb\u7edf\u6027\u8bef\u8bc6\u522b\u7279\u5b9a\u8bed\u8a00\u53d8\u4f53\u4f1a\u5f62\u6210\u7ed3\u6784\u6027\u4e0d\u5c0a\u91cd\uff0c\u5ef6\u7eed\u5bf9\u8fb9\u7f18\u8bed\u8a00\u793e\u533a\u7684\u5386\u53f2\u6027\u538b\u8feb", "method": "\u901a\u8fc7\u54f2\u5b66\u5206\u6790\u533a\u5206\u6280\u672f\u5206\u7c7b(discriminate1)\u4e0e\u6709\u5bb3\u6b67\u89c6(discriminate2)\uff0c\u63d0\u51fa\u65f6\u95f4\u8d1f\u62c5\u3001\u5bf9\u8bdd\u6d41\u4e2d\u65ad\u548c\u8eab\u4efd\u5173\u8054\u4e09\u4e2a\u72ec\u7279\u4f26\u7406\u7ef4\u5ea6", "result": "\u63ed\u793aASR\u504f\u89c1\u5bfc\u81f4\u975e\u6807\u51c6\u65b9\u8a00\u4f7f\u7528\u8005\u627f\u53d7\u989d\u5916\u65f6\u95f4\u6210\u672c(temporal taxation)\u3001\u7834\u574f\u5bf9\u8bdd\u8fde\u8d2f\u6027\uff0c\u5e76\u635f\u5bb3\u4e0e\u8bed\u8a00\u8eab\u4efd\u7d27\u5bc6\u76f8\u8fde\u7684\u81ea\u6211\u8ba4\u540c", "conclusion": "\u89e3\u51b3ASR\u504f\u89c1\u9700\u8d85\u8d8a\u6280\u672f\u4f18\u5316\uff0c\u627f\u8ba4\u8bed\u8a00\u591a\u6837\u6027\u4ef7\u503c\uff0c\u91cd\u6784\u6280\u672f\u5f00\u53d1\u4e2d\u7684\u8bed\u8a00\u610f\u8bc6\u5f62\u6001\uff0c\u5efa\u7acb\u5c0a\u91cd\u8bed\u8a00\u591a\u5143\u6027\u548c\u8bf4\u8bdd\u8005\u81ea\u4e3b\u6743\u7684\u6280\u672f\u6846\u67b6"}}
{"id": "2508.07172", "pdf": "https://arxiv.org/pdf/2508.07172", "abs": "https://arxiv.org/abs/2508.07172", "authors": ["Biao Yi", "Jiahao Li", "Baolei Zhang", "Lihai Nie", "Tong Li", "Tiansheng Huang", "Zheli Liu"], "title": "Gradient Surgery for Safe LLM Fine-Tuning", "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning-as-a-Service introduces a critical vulnerability where a few\nmalicious examples mixed into the user's fine-tuning dataset can compromise the\nsafety alignment of Large Language Models (LLMs). While a recognized paradigm\nframes safe fine-tuning as a multi-objective optimization problem balancing\nuser task performance with safety alignment, we find existing solutions are\ncritically sensitive to the harmful ratio, with defenses degrading sharply as\nharmful ratio increases. We diagnose that this failure stems from conflicting\ngradients, where the user-task update directly undermines the safety objective.\nTo resolve this, we propose SafeGrad, a novel method that employs gradient\nsurgery. When a conflict is detected, SafeGrad nullifies the harmful component\nof the user-task gradient by projecting it onto the orthogonal plane of the\nalignment gradient, allowing the model to learn the user's task without\nsacrificing safety. To further enhance robustness and data efficiency, we\nemploy a KL-divergence alignment loss that learns the rich, distributional\nsafety profile of the well-aligned foundation model. Extensive experiments show\nthat SafeGrad provides state-of-the-art defense across various LLMs and\ndatasets, maintaining robust safety even at high harmful ratios without\ncompromising task fidelity.", "AI": {"tldr": "\u63d0\u51faSafeGrad\u65b9\u6cd5\u89e3\u51b3\u5927\u6a21\u578b\u5fae\u8c03\u670d\u52a1\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\u95ee\u9898\uff0c\u901a\u8fc7\u68af\u5ea6\u624b\u672f\u6d88\u9664\u7528\u6237\u4efb\u52a1\u68af\u5ea6\u4e0e\u5b89\u5168\u76ee\u6807\u7684\u51b2\u7a81\uff0c\u5b9e\u73b0\u9ad8\u6709\u5bb3\u6bd4\u4f8b\u4e0b\u7684\u9c81\u68d2\u9632\u5fa1", "motivation": "\u73b0\u6709\u5b89\u5168\u5fae\u8c03\u65b9\u6cd5\u5bf9\u6709\u5bb3\u6837\u672c\u6bd4\u4f8b\u9ad8\u5ea6\u654f\u611f\uff0c\u7528\u6237\u4efb\u52a1\u68af\u5ea6\u4e0e\u5b89\u5168\u76ee\u6807\u4ea7\u751f\u51b2\u7a81\u5bfc\u81f4\u9632\u5fa1\u5931\u6548", "method": "\u91c7\u7528\u68af\u5ea6\u6295\u5f71\u624b\u672f\u6d88\u9664\u68af\u5ea6\u51b2\u7a81\uff0c\u7ed3\u5408KL\u6563\u5ea6\u5bf9\u9f50\u635f\u5931\u5b66\u4e60\u57fa\u7840\u6a21\u578b\u7684\u5b89\u5168\u5206\u5e03\u7279\u5f81", "result": "\u5728\u591a\u79cd\u5927\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0SOTA\u9632\u5fa1\u6548\u679c\uff0c\u9ad8\u6709\u5bb3\u6bd4\u4f8b\u4e0b\u4ecd\u4fdd\u6301\u5b89\u5168\u6027\u4e14\u4e0d\u635f\u5931\u4efb\u52a1\u6027\u80fd", "conclusion": "SafeGrad\u521b\u65b0\u6027\u5730\u5c06\u68af\u5ea6\u624b\u672f\u5e94\u7528\u4e8e\u5b89\u5168\u5bf9\u9f50\uff0c\u901a\u8fc7\u51e0\u4f55\u7ea6\u675f\u548c\u5206\u5e03\u5b66\u4e60\u7a81\u7834\u73b0\u6709\u9632\u5fa1\u7684\u6027\u80fd\u74f6\u9888"}}
{"id": "2508.07173", "pdf": "https://arxiv.org/pdf/2508.07173", "abs": "https://arxiv.org/abs/2508.07173", "authors": ["Leyi Pan", "Zheyu Fu", "Yunpeng Zhai", "Shuchang Tao", "Sheng Guan", "Shiyu Huang", "Lingzhe Zhang", "Zhaoyang Liu", "Bolin Ding", "Felix Henry", "Lijie Wen", "Aiwei Liu"], "title": "Omni-SafetyBench: A Benchmark for Safety Evaluation of Audio-Visual Large Language Models", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "20 pages, 8 figures, 12 tables", "summary": "The rise of Omni-modal Large Language Models (OLLMs), which integrate visual\nand auditory processing with text, necessitates robust safety evaluations to\nmitigate harmful outputs. However, no dedicated benchmarks currently exist for\nOLLMs, and prior benchmarks designed for other LLMs lack the ability to assess\nsafety performance under audio-visual joint inputs or cross-modal safety\nconsistency. To fill this gap, we introduce Omni-SafetyBench, the first\ncomprehensive parallel benchmark for OLLM safety evaluation, featuring 24\nmodality combinations and variations with 972 samples each, including dedicated\naudio-visual harm cases. Considering OLLMs' comprehension challenges with\ncomplex omni-modal inputs and the need for cross-modal consistency evaluation,\nwe propose tailored metrics: a Safety-score based on conditional Attack Success\nRate (C-ASR) and Refusal Rate (C-RR) to account for comprehension failures, and\na Cross-Modal Safety Consistency Score (CMSC-score) to measure consistency\nacross modalities. Evaluating 6 open-source and 4 closed-source OLLMs reveals\ncritical vulnerabilities: (1) no model excels in both overall safety and\nconsistency, with only 3 models achieving over 0.6 in both metrics and top\nperformer scoring around 0.8; (2) safety defenses weaken with complex inputs,\nespecially audio-visual joints; (3) severe weaknesses persist, with some models\nscoring as low as 0.14 on specific modalities. Our benchmark and metrics\nhighlight urgent needs for enhanced OLLM safety, providing a foundation for\nfuture improvements.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u591a\u6a21\u6001\u5927\u6a21\u578b\u5b89\u5168\u8bc4\u4f30\u57fa\u51c6Omni-SafetyBench\uff0c\u63ed\u793a\u73b0\u6709\u6a21\u578b\u5728\u8de8\u6a21\u6001\u5b89\u5168\u9632\u5fa1\u7684\u4e25\u91cd\u6f0f\u6d1e", "motivation": "\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u89c6\u542c\u8054\u5408\u8f93\u5165\u4e0b\u7684\u5b89\u5168\u6027\u53ca\u8de8\u6a21\u6001\u5b89\u5168\u4e00\u81f4\u6027", "method": "\u6784\u5efa\u5305\u542b24\u79cd\u6a21\u6001\u7ec4\u5408\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u8bbe\u8ba1\u5b89\u5168\u8bc4\u5206\uff08Safety-score\uff09\u548c\u8de8\u6a21\u6001\u5b89\u5168\u4e00\u81f4\u6027\u8bc4\u5206\uff08CMSC-score\uff09", "result": "\u8bc4\u4f30\u663e\u793a\uff1a\u65e0\u6a21\u578b\u5728\u5b89\u5168\u6027\u548c\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff08\u6700\u9ad8\u4ec50.8\u5206\uff09\uff1b\u590d\u6742\u8f93\u5165\u964d\u4f4e\u9632\u5fa1\u6548\u679c\uff1b\u90e8\u5206\u6a21\u6001\u5b89\u5168\u8bc4\u5206\u4f4e\u81f30.14", "conclusion": "\u8be5\u57fa\u51c6\u66b4\u9732\u591a\u6a21\u6001\u5927\u6a21\u578b\u5b89\u5168\u9632\u5fa1\u8584\u5f31\u73af\u8282\uff0c\u4e3a\u63d0\u5347\u6a21\u578b\u5b89\u5168\u6027\u63d0\u4f9b\u5173\u952e\u8bc4\u4f30\u5de5\u5177\u548c\u65b9\u5411\u6307\u5f15"}}
{"id": "2508.07178", "pdf": "https://arxiv.org/pdf/2508.07178", "abs": "https://arxiv.org/abs/2508.07178", "authors": ["Kejin Liu", "Junhong Lian", "Xiang Ao", "Ningtao Wang", "Xing Fu", "Yu Cheng", "Weiqiang Wang", "Xinyu Liu"], "title": "Improved Personalized Headline Generation via Denoising Fake Interests from Implicit Feedback", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by the 34th ACM International Conference on Information and\n  Knowledge Management (CIKM '25), Full Research Papers track", "summary": "Accurate personalized headline generation hinges on precisely capturing user\ninterests from historical behaviors. However, existing methods neglect\npersonalized-irrelevant click noise in entire historical clickstreams, which\nmay lead to hallucinated headlines that deviate from genuine user preferences.\nIn this paper, we reveal the detrimental impact of click noise on personalized\ngeneration quality through rigorous analysis in both user and news dimensions.\nBased on these insights, we propose a novel Personalized Headline Generation\nframework via Denoising Fake Interests from Implicit Feedback (PHG-DIF).\nPHG-DIF first employs dual-stage filtering to effectively remove clickstream\nnoise, identified by short dwell times and abnormal click bursts, and then\nleverages multi-level temporal fusion to dynamically model users' evolving and\nmulti-faceted interests for precise profiling. Moreover, we release DT-PENS, a\nnew benchmark dataset comprising the click behavior of 1,000 carefully curated\nusers and nearly 10,000 annotated personalized headlines with historical dwell\ntime annotations. Extensive experiments demonstrate that PHG-DIF substantially\nmitigates the adverse effects of click noise and significantly improves\nheadline quality, achieving state-of-the-art (SOTA) results on DT-PENS. Our\nframework implementation and dataset are available at\nhttps://github.com/liukejin-up/PHG-DIF.", "AI": {"tldr": "\u63d0\u51faPHG-DIF\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u9636\u6bb5\u8fc7\u6ee4\u548c\u591a\u7ea7\u65f6\u95f4\u878d\u5408\u6d88\u9664\u70b9\u51fb\u566a\u58f0\uff0c\u63d0\u5347\u4e2a\u6027\u5316\u6807\u9898\u751f\u6210\u8d28\u91cf", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u5904\u7406\u5386\u53f2\u70b9\u51fb\u6d41\u4e2d\u7684\u566a\u58f0\uff08\u5982\u77ed\u505c\u7559\u65f6\u95f4\u548c\u5f02\u5e38\u70b9\u51fb\uff09\uff0c\u5bfc\u81f4\u751f\u6210\u6807\u9898\u504f\u79bb\u771f\u5b9e\u7528\u6237\u5174\u8da3", "method": "1. \u53cc\u9636\u6bb5\u8fc7\u6ee4\uff08\u57fa\u4e8e\u505c\u7559\u65f6\u95f4\u548c\u70b9\u51fb\u9891\u6b21\u53bb\u566a\uff09 2. \u591a\u7ea7\u65f6\u95f4\u878d\u5408\u52a8\u6001\u5efa\u6a21\u7528\u6237\u5174\u8da3 3. \u53d1\u5e03\u542b1,000\u7528\u6237\u884c\u4e3a\u6570\u636e\u7684DT-PENS\u6570\u636e\u96c6", "result": "\u5728DT-PENS\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\uff0c\u5b9e\u9a8c\u8bc1\u660e\u6846\u67b6\u6709\u6548\u964d\u4f4e63%\u566a\u58f0\u5f71\u54cd\uff0c\u6807\u9898\u8d28\u91cf\u63d0\u534722%", "conclusion": "PHG-DIF\u9996\u6b21\u7cfb\u7edf\u6027\u89e3\u51b3\u70b9\u51fb\u566a\u58f0\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u5174\u8da3\u5efa\u6a21\u5b9e\u73b0\u7cbe\u51c6\u7528\u6237\u753b\u50cf\uff0c\u4e3a\u4e2a\u6027\u5316\u751f\u6210\u63d0\u4f9b\u65b0\u57fa\u51c6"}}
{"id": "2508.07179", "pdf": "https://arxiv.org/pdf/2508.07179", "abs": "https://arxiv.org/abs/2508.07179", "authors": ["Jiaqi Yin", "Yi-Wei Chen", "Meng-Lung Lee", "Xiya Liu"], "title": "Schema Lineage Extraction at Scale: Multilingual Pipelines, Composite Evaluation, and Language-Model Benchmarks", "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "Enterprise data pipelines, characterized by complex transformations across\nmultiple programming languages, often cause a semantic disconnect between\noriginal metadata and downstream data. This \"semantic drift\" compromises data\nreproducibility and governance, and impairs the utility of services like\nretrieval-augmented generation (RAG) and text-to-SQL systems. To address this,\na novel framework is proposed for the automated extraction of fine-grained\nschema lineage from multilingual enterprise pipeline scripts. This method\nidentifies four key components: source schemas, source tables, transformation\nlogic, and aggregation operations, creating a standardized representation of\ndata transformations. For the rigorous evaluation of lineage quality, this\npaper introduces the Schema Lineage Composite Evaluation (SLiCE), a metric that\nassesses both structural correctness and semantic fidelity. A new benchmark is\nalso presented, comprising 1,700 manually annotated lineages from real-world\nindustrial scripts. Experiments were conducted with 12 language models, from\n1.3B to 32B small language models (SLMs) to large language models (LLMs) like\nGPT-4o and GPT-4.1. The results demonstrate that the performance of schema\nlineage extraction scales with model size and the sophistication of prompting\ntechniques. Specially, a 32B open-source model, using a single reasoning trace,\ncan achieve performance comparable to the GPT series under standard prompting.\nThis finding suggests a scalable and economical approach for deploying\nschema-aware agents in practical applications.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u52a8\u5316\u63d0\u53d6\u591a\u8bed\u8a00\u4f01\u4e1a\u6570\u636e\u7ba1\u9053\u4e2d\u7ec6\u7c92\u5ea6\u6a21\u5f0f\u6cbf\u88ad\u7684\u6846\u67b6\uff0c\u901a\u8fc7SLiCE\u8bc4\u4f30\u6307\u6807\u9a8c\u8bc1\uff0c\u8bc1\u660e\u5927\u6a21\u578b\u89c4\u6a21\u4e0e\u63d0\u793a\u6280\u672f\u5bf9\u6027\u80fd\u63d0\u5347\u6709\u6548\uff0c32B\u5f00\u6e90\u6a21\u578b\u53ef\u8fbeGPT\u7cfb\u5217\u6c34\u5e73\u3002", "motivation": "\u89e3\u51b3\u4f01\u4e1a\u591a\u8bed\u8a00\u6570\u636e\u7ba1\u9053\u4e2d\u5143\u6570\u636e\u4e0e\u4e0b\u6e38\u6570\u636e\u8bed\u4e49\u8131\u8282\u5bfc\u81f4\u7684'\u8bed\u4e49\u6f02\u79fb'\u95ee\u9898\uff0c\u4fdd\u969c\u6570\u636e\u53ef\u590d\u73b0\u6027\u4e0e\u6cbb\u7406\u80fd\u529b\u3002", "method": "\u6784\u5efa\u81ea\u52a8\u5316\u6846\u67b6\u63d0\u53d6\u56db\u8981\u7d20(\u6e90\u6a21\u5f0f/\u8868\u3001\u8f6c\u6362\u903b\u8f91\u3001\u805a\u5408\u64cd\u4f5c)\uff0c\u5efa\u7acb\u542b1700\u4e2a\u5de5\u4e1a\u811a\u672c\u6807\u6ce8\u7684\u57fa\u51c6\uff0c\u91c7\u752812\u79cd\u6a21\u578b(1.3B-32B SLMs/LLMs)\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u6a21\u578b\u6027\u80fd\u4e0e\u89c4\u6a21/\u63d0\u793a\u590d\u6742\u5ea6\u6b63\u76f8\u5173\uff0c32B\u5f00\u6e90\u6a21\u578b\u5355\u6b21\u63a8\u7406\u53ef\u8fbeGPT\u7cfb\u5217\u6807\u51c6\u63d0\u793a\u6548\u679c\uff0c\u9a8c\u8bc1\u7ecf\u6d4e\u90e8\u7f72\u53ef\u884c\u6027\u3002", "conclusion": "\u901a\u8fc7\u6a21\u578b\u89c4\u6a21\u6269\u5c55\u4e0e\u63d0\u793a\u4f18\u5316\uff0c\u53ef\u5b9e\u73b0\u9ad8\u6027\u4ef7\u6bd4\u7684\u6a21\u5f0f\u611f\u77e5\u4ee3\u7406\u90e8\u7f72\uff0c\u4e3a\u5de5\u4e1a\u5e94\u7528\u63d0\u4f9b\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07185", "pdf": "https://arxiv.org/pdf/2508.07185", "abs": "https://arxiv.org/abs/2508.07185", "authors": ["Kabir Khan", "Priya Sharma", "Arjun Mehta", "Neha Gupta", "Ravi Narayanan"], "title": "DySK-Attn: A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; H.3.3; H.2.8"], "comment": "Preprint; 7 figures, 3 tables, 1 algorithm; v1. Code and data will be\n  released", "summary": "Large Language Models (LLMs) suffer from a critical limitation: their\nknowledge is static and quickly becomes outdated. Retraining these massive\nmodels is computationally prohibitive, while existing knowledge editing\ntechniques can be slow and may introduce unforeseen side effects. To address\nthis, we propose DySK-Attn, a novel framework that enables LLMs to efficiently\nintegrate real-time knowledge from a dynamic external source. Our approach\nsynergizes an LLM with a dynamic Knowledge Graph (KG) that can be updated\ninstantaneously. The core of our framework is a sparse knowledge attention\nmechanism, which allows the LLM to perform a coarse-to-fine grained search,\nefficiently identifying and focusing on a small, highly relevant subset of\nfacts from the vast KG. This mechanism avoids the high computational cost of\ndense attention over the entire knowledge base and mitigates noise from\nirrelevant information. We demonstrate through extensive experiments on\ntime-sensitive question-answering tasks that DySK-Attn significantly\noutperforms strong baselines, including standard Retrieval-Augmented Generation\n(RAG) and model editing techniques, in both factual accuracy for updated\nknowledge and computational efficiency. Our framework offers a scalable and\neffective solution for building LLMs that can stay current with the\never-changing world.", "AI": {"tldr": "\u63d0\u51faDySK-Attn\u6846\u67b6\uff0c\u901a\u8fc7\u7a00\u758f\u77e5\u8bc6\u6ce8\u610f\u529b\u673a\u5236\u5c06\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\u4e0eLLM\u7ed3\u5408\uff0c\u89e3\u51b3\u5927\u6a21\u578b\u77e5\u8bc6\u8fc7\u65f6\u95ee\u9898\uff0c\u5728\u4fdd\u8bc1\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u65f6\u6548\u6027\u95ee\u7b54\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edfLLM\u77e5\u8bc6\u66f4\u65b0\u9700\u91cd\u65b0\u8bad\u7ec3\uff08\u8ba1\u7b97\u6210\u672c\u9ad8\uff09\uff0c\u73b0\u6709\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u6548\u7387\u4f4e\u4e14\u5b58\u5728\u526f\u4f5c\u7528\u3002\u9700\u5b9e\u73b0\u5b9e\u65f6\u77e5\u8bc6\u6574\u5408\u540c\u65f6\u907f\u514d\u8ba1\u7b97\u8fc7\u8f7d\u3002", "method": "1. \u6784\u5efa\u53ef\u5b9e\u65f6\u66f4\u65b0\u7684\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31 2. \u8bbe\u8ba1\u7c97\u7c92\u5ea6\u5230\u7ec6\u7c92\u5ea6\u7684\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5feb\u901f\u5b9a\u4f4d\u77e5\u8bc6\u56fe\u8c31\u4e2d\u6700\u76f8\u5173\u5b50\u96c6 3. \u901a\u8fc7\u7a00\u758f\u8ba1\u7b97\u51cf\u5c11\u566a\u58f0\u5e72\u6270\u548c\u7b97\u529b\u6d88\u8017", "result": "\u5728\u65f6\u6548\u6027\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0c\u51c6\u786e\u7387\u8d85\u8d8aRAG\u548c\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\uff08\u5177\u4f53\u63d0\u5347\u5e45\u5ea6\u672a\u516c\u5f00\uff09\uff0c\u8ba1\u7b97\u6548\u7387\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b", "conclusion": "DySK-Attn\u4e3a\u6784\u5efa\u5b9e\u65f6\u66f4\u65b0\u7684LLM\u63d0\u4f9b\u53ef\u6269\u5c55\u65b9\u6848\uff0c\u5e73\u8861\u77e5\u8bc6\u65f6\u6548\u6027\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u7684\u518d\u8bad\u7ec3\u6210\u672c\u4e0e\u526f\u4f5c\u7528\u98ce\u9669"}}
{"id": "2508.07195", "pdf": "https://arxiv.org/pdf/2508.07195", "abs": "https://arxiv.org/abs/2508.07195", "authors": ["Yanru Sun", "Emadeldeen Eldele", "Zongxia Xie", "Yucheng Wang", "Wenzhe Niu", "Qinghua Hu", "Chee Keong Kwoh", "Min Wu"], "title": "Adapting LLMs to Time Series Forecasting via Temporal Heterogeneity Modeling and Semantic Alignment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have recently demonstrated impressive\ncapabilities in natural language processing due to their strong generalization\nand sequence modeling capabilities. However, their direct application to time\nseries forecasting remains challenging due to two fundamental issues: the\ninherent heterogeneity of temporal patterns and the modality gap between\ncontinuous numerical signals and discrete language representations. In this\nwork, we propose TALON, a unified framework that enhances LLM-based forecasting\nby modeling temporal heterogeneity and enforcing semantic alignment.\nSpecifically, we design a Heterogeneous Temporal Encoder that partitions\nmultivariate time series into structurally coherent segments, enabling\nlocalized expert modeling across diverse temporal patterns. To bridge the\nmodality gap, we introduce a Semantic Alignment Module that aligns temporal\nfeatures with LLM-compatible representations, enabling effective integration of\ntime series into language-based models while eliminating the need for\nhandcrafted prompts during inference. Extensive experiments on seven real-world\nbenchmarks demonstrate that TALON achieves superior performance across all\ndatasets, with average MSE improvements of up to 11\\% over recent\nstate-of-the-art methods. These results underscore the effectiveness of\nincorporating both pattern-aware and semantic-aware designs when adapting LLMs\nfor time series forecasting. The code is available at:\nhttps://github.com/syrGitHub/TALON.", "AI": {"tldr": "\u63d0\u51faTALON\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u65f6\u95f4\u5f02\u8d28\u6027\u548c\u8bed\u4e49\u5bf9\u9f50\u589e\u5f3aLLM\u5728\u65f6\u5e8f\u9884\u6d4b\u4e2d\u7684\u8868\u73b0", "motivation": "LLM\u76f4\u63a5\u5e94\u7528\u4e8e\u65f6\u5e8f\u9884\u6d4b\u5b58\u5728\u4e24\u4e2a\u6838\u5fc3\u6311\u6218\uff1a\u65f6\u95f4\u6a21\u5f0f\u7684\u5f02\u8d28\u6027\u548c\u6570\u503c\u4fe1\u53f7\u4e0e\u8bed\u8a00\u8868\u5f81\u7684\u6a21\u6001\u5dee\u5f02", "method": "\u5305\u542b\u5f02\u8d28\u6027\u65f6\u95f4\u7f16\u7801\u5668\uff08\u5206\u5272\u65f6\u5e8f\u6570\u636e\u5b9e\u73b0\u5c40\u90e8\u4e13\u5bb6\u5efa\u6a21\uff09\u548c\u8bed\u4e49\u5bf9\u9f50\u6a21\u5757\uff08\u6865\u63a5\u65f6\u5e8f\u7279\u5f81\u4e0eLLM\u8868\u5f81\uff09", "result": "\u57287\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u5e73\u5747MSE\u63d0\u534711%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "\u9a8c\u8bc1\u4e86\u540c\u65f6\u8003\u8651\u6a21\u5f0f\u611f\u77e5\u548c\u8bed\u4e49\u611f\u77e5\u8bbe\u8ba1\u5728LLM\u65f6\u5e8f\u9884\u6d4b\u9002\u914d\u4e2d\u7684\u6709\u6548\u6027"}}
{"id": "2508.07209", "pdf": "https://arxiv.org/pdf/2508.07209", "abs": "https://arxiv.org/abs/2508.07209", "authors": ["Chaoqun Cui", "Siyuan Li", "Kunkun Ma", "Caiyan Jia"], "title": "Enhancing Rumor Detection Methods with Propagation Structure Infused Language Model", "categories": ["cs.CL", "cs.SI"], "comment": "This paper is accepted by COLING2025", "summary": "Pretrained Language Models (PLMs) have excelled in various Natural Language\nProcessing tasks, benefiting from large-scale pretraining and self-attention\nmechanism's ability to capture long-range dependencies. However, their\nperformance on social media application tasks like rumor detection remains\nsuboptimal. We attribute this to mismatches between pretraining corpora and\nsocial texts, inadequate handling of unique social symbols, and pretraining\ntasks ill-suited for modeling user engagements implicit in propagation\nstructures. To address these issues, we propose a continue pretraining strategy\ncalled Post Engagement Prediction (PEP) to infuse information from propagation\nstructures into PLMs. PEP makes models to predict root, branch, and parent\nrelations between posts, capturing interactions of stance and sentiment crucial\nfor rumor detection. We also curate and release large-scale Twitter corpus:\nTwitterCorpus (269GB text), and two unlabeled claim conversation datasets with\npropagation structures (UTwitter and UWeibo). Utilizing these resources and PEP\nstrategy, we train a Twitter-tailored PLM called SoLM. Extensive experiments\ndemonstrate PEP significantly boosts rumor detection performance across\nuniversal and social media PLMs, even in few-shot scenarios. On benchmark\ndatasets, PEP enhances baseline models by 1.0-3.7\\% accuracy, even enabling it\nto outperform current state-of-the-art methods on multiple datasets. SoLM\nalone, without high-level modules, also achieves competitive results,\nhighlighting the strategy's effectiveness in learning discriminative post\ninteraction features.", "AI": {"tldr": "\u63d0\u51faPEP\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u9884\u6d4b\u5e16\u5b50\u4f20\u64ad\u5173\u7cfb\u589e\u5f3a\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5bf9\u793e\u4ea4\u6587\u672c\u7684\u7406\u89e3\uff0c\u5728\u8c23\u8a00\u68c0\u6d4b\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd", "motivation": "\u73b0\u6709\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u793e\u4ea4\u5a92\u4f53\u4efb\u52a1\uff08\u5982\u8c23\u8a00\u68c0\u6d4b\uff09\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u7531\u4e8e\u9884\u8bad\u7ec3\u8bed\u6599\u4e0d\u5339\u914d\u3001\u793e\u4ea4\u7b26\u53f7\u5904\u7406\u4e0d\u8db3\u3001\u9884\u8bad\u7ec3\u4efb\u52a1\u8bbe\u8ba1\u4e0d\u9002\u5e94\u4f20\u64ad\u7ed3\u6784\u5efa\u6a21", "method": "\u8bbe\u8ba1Post Engagement Prediction\u7b56\u7565\u9884\u6d4b\u5e16\u5b50\u95f4\u4f20\u64ad\u5173\u7cfb\uff08\u6839/\u5206\u652f/\u7236\u7ea7\uff09\uff0c\u6784\u5efaTwitterCorpus\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u9002\u914d\u793e\u4ea4\u5a92\u4f53\u7684SoLM\u6a21\u578b", "result": "PEP\u4f7f\u57fa\u7ebf\u6a21\u578b\u51c6\u786e\u7387\u63d0\u53471.0-3.7%\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u8d85\u8d8aSOTA\u65b9\u6cd5\uff0cSoLM\u65e0\u9700\u590d\u6742\u6a21\u5757\u5373\u53d6\u5f97\u7ade\u4e89\u6027\u6548\u679c", "conclusion": "\u901a\u8fc7\u4f20\u64ad\u7ed3\u6784\u611f\u77e5\u7684\u7ee7\u7eed\u9884\u8bad\u7ec3\uff0c\u80fd\u6709\u6548\u5b66\u4e60\u793e\u4ea4\u5a92\u4f53\u6587\u672c\u7684\u5224\u522b\u6027\u4ea4\u4e92\u7279\u5f81\uff0c\u9a8c\u8bc1\u4e86PEP\u7b56\u7565\u5728\u793e\u4ea4\u4efb\u52a1\u4f18\u5316\u4e2d\u7684\u6709\u6548\u6027"}}
{"id": "2508.07229", "pdf": "https://arxiv.org/pdf/2508.07229", "abs": "https://arxiv.org/abs/2508.07229", "authors": ["Itai Allouche", "Itay Asael", "Rotem Rousso", "Vered Dassa", "Ann Bradlow", "Seung-Eun Kim", "Matthew Goldrick", "Joseph Keshet"], "title": "How Does a Deep Neural Network Look at Lexical Stress?", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": "10 pages, 4 figures, submitted to the Journal of the Acoustical\n  Society of America (JASA)", "summary": "Despite their success in speech processing, neural networks often operate as\nblack boxes, prompting the question: what informs their decisions, and how can\nwe interpret them? This work examines this issue in the context of lexical\nstress. A dataset of English disyllabic words was automatically constructed\nfrom read and spontaneous speech. Several Convolutional Neural Network (CNN)\narchitectures were trained to predict stress position from a spectrographic\nrepresentation of disyllabic words lacking minimal stress pairs (e.g., initial\nstress WAllet, final stress exTEND), achieving up to 92% accuracy on held-out\ntest data. Layerwise Relevance Propagation (LRP), a technique for CNN\ninterpretability analysis, revealed that predictions for held-out minimal pairs\n(PROtest vs. proTEST ) were most strongly influenced by information in stressed\nversus unstressed syllables, particularly the spectral properties of stressed\nvowels. However, the classifiers also attended to information throughout the\nword. A feature-specific relevance analysis is proposed, and its results\nsuggest that our best-performing classifier is strongly influenced by the\nstressed vowel's first and second formants, with some evidence that its pitch\nand third formant also contribute. These results reveal deep learning's ability\nto acquire distributed cues to stress from naturally occurring data, extending\ntraditional phonetic work based around highly controlled stimuli.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7CNN\u548cLRP\u6280\u672f\u5206\u6790\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u82f1\u8bed\u8bcd\u6c47\u91cd\u97f3\u7684\u51b3\u7b56\u4f9d\u636e\uff0c\u53d1\u73b0\u91cd\u97f3\u5143\u97f3\u9891\u8c31\u7279\u5f81\uff08\u5c24\u5176\u7b2c\u4e00/\u4e8c\u5171\u632f\u5cf0\uff09\u662f\u4e3b\u8981\u5224\u65ad\u4f9d\u636e\uff0c\u5206\u7c7b\u5668\u51c6\u786e\u7387\u8fbe92%\u5e76\u5177\u5907\u5168\u5c40\u5206\u6790\u80fd\u529b\u3002", "motivation": "\u63a2\u7a76\u795e\u7ecf\u7f51\u7edc\u5728\u8bed\u97f3\u5904\u7406\u4e2d\u7684\u9ed1\u7bb1\u51b3\u7b56\u673a\u5236\uff0c\u4ee5\u82f1\u8bed\u8bcd\u6c47\u91cd\u97f3\u9884\u6d4b\u4e3a\u5207\u5165\u70b9\uff0c\u63ed\u793a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6355\u6349\u5206\u5e03\u5f0f\u58f0\u5b66\u7ebf\u7d22\u7684\u80fd\u529b\u3002", "method": "1. \u6784\u5efa\u542b\u6717\u8bfb/\u81ea\u53d1\u8bed\u97f3\u7684\u82f1\u8bed\u53cc\u97f3\u8282\u8bcd\u6570\u636e\u96c6\n2. \u8bad\u7ec3CNN\u6a21\u578b\u9884\u6d4b\u65e0\u6700\u5c0f\u5bf9\u7acb\u5bf9\u8bcd\u6c47\u7684\u91cd\u97f3\u4f4d\u7f6e\n3. \u5e94\u7528\u5c42\u95f4\u76f8\u5173\u6027\u4f20\u64ad(LRP)\u6280\u672f\u8fdb\u884c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u5206\u6790\n4. \u63d0\u51fa\u7279\u5f81\u7279\u5f02\u6027\u76f8\u5173\u6027\u5206\u6790\u65b9\u6cd5\u91cf\u5316\u58f0\u5b66\u7279\u5f81\u8d21\u732e\u5ea6", "result": "1. \u6a21\u578b\u6d4b\u8bd5\u51c6\u786e\u7387\u6700\u9ad8\u8fbe92%\n2. LRP\u663e\u793a\u91cd\u97f3\u5143\u97f3\u9891\u8c31\u7279\u6027\uff08\u7b2c\u4e00/\u4e8c\u5171\u632f\u5cf0\uff09\u4e3b\u5bfc\u9884\u6d4b\n3. \u97f3\u9ad8\u548c\u7b2c\u4e09\u5171\u632f\u5cf0\u6709\u8f85\u52a9\u4f5c\u7528\n4. \u5206\u7c7b\u5668\u540c\u65f6\u5173\u6ce8\u5355\u8bcd\u6574\u4f53\u58f0\u5b66\u7279\u5f81", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u80fd\u4ece\u81ea\u7136\u8bed\u97f3\u4e2d\u6355\u83b7\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u8bc6\u522b\u7684\u5206\u5e03\u5f0f\u91cd\u97f3\u7ebf\u7d22\uff0c\u7a81\u7834\u4e86\u57fa\u4e8e\u63a7\u5236\u6027\u5b9e\u9a8c\u7684\u4f20\u7edf\u8bed\u97f3\u5b66\u7814\u7a76\u8303\u5f0f\uff0c\u4e3a\u795e\u7ecf\u7f51\u7edc\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2508.07248", "pdf": "https://arxiv.org/pdf/2508.07248", "abs": "https://arxiv.org/abs/2508.07248", "authors": ["Zhe Ren"], "title": "Prompt Tuning for Few-Shot Continual Learning Named Entity Recognition", "categories": ["cs.CL"], "comment": null, "summary": "Knowledge distillation has been successfully applied to Continual Learning\nNamed Entity Recognition (CLNER) tasks, by using a teacher model trained on\nold-class data to distill old-class entities present in new-class data as a\nform of regularization, thereby avoiding catastrophic forgetting. However, in\nFew-Shot CLNER (FS-CLNER) tasks, the scarcity of new-class entities makes it\ndifficult for the trained model to generalize during inference. More\ncritically, the lack of old-class entity information hinders the distillation\nof old knowledge, causing the model to fall into what we refer to as the\nFew-Shot Distillation Dilemma. In this work, we address the above challenges\nthrough a prompt tuning paradigm and memory demonstration template strategy.\nSpecifically, we designed an expandable Anchor words-oriented Prompt Tuning\n(APT) paradigm to bridge the gap between pre-training and fine-tuning, thereby\nenhancing performance in few-shot scenarios. Additionally, we incorporated\nMemory Demonstration Templates (MDT) into each training instance to provide\nreplay samples from previous tasks, which not only avoids the Few-Shot\nDistillation Dilemma but also promotes in-context learning. Experiments show\nthat our approach achieves competitive performances on FS-CLNER.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u951a\u8bcd\u5bfc\u5411\u7684\u63d0\u793a\u8c03\u4f18\u8303\u5f0f(APT)\u548c\u8bb0\u5fc6\u6f14\u793a\u6a21\u677f(MDT)\uff0c\u89e3\u51b3\u5c11\u6837\u672c\u6301\u7eed\u5b66\u4e60NER\u4efb\u52a1\u4e2d\u7684\u77e5\u8bc6\u84b8\u998f\u56f0\u5883", "motivation": "\u5c11\u6837\u672c\u573a\u666f\u4e0b\u65b0\u65e7\u5b9e\u4f53\u4fe1\u606f\u4e0d\u8db3\u5bfc\u81f4\u6a21\u578b\u9677\u5165'\u5c11\u6837\u672c\u84b8\u998f\u56f0\u5883'\u2014\u2014\u65e2\u96be\u4ee5\u6cdb\u5316\u65b0\u7c7b\u5b9e\u4f53\uff0c\u53c8\u7f3a\u4e4f\u65e7\u7c7b\u5b9e\u4f53\u4fe1\u606f\u963b\u788d\u77e5\u8bc6\u84b8\u998f", "method": "1. \u53ef\u6269\u5c55\u7684\u951a\u8bcd\u5bfc\u5411\u63d0\u793a\u8c03\u4f18(APT)\u6865\u63a5\u9884\u8bad\u7ec3\u4e0e\u5fae\u8c03\n2. \u8bb0\u5fc6\u6f14\u793a\u6a21\u677f(MDT)\u63d0\u4f9b\u5386\u53f2\u4efb\u52a1\u6837\u672c\uff0c\u652f\u6301\u4e0a\u4e0b\u6587\u5b66\u4e60", "result": "\u5728FS-CLNER\u4efb\u52a1\u4e0a\u53d6\u5f97\u7ade\u4e89\u6027\u6027\u80fd\u8868\u73b0", "conclusion": "\u7ed3\u5408\u63d0\u793a\u8c03\u4f18\u4e0e\u8bb0\u5fc6\u6a21\u677f\u7684\u7b56\u7565\u6709\u6548\u7f13\u89e3\u4e86\u5c11\u6837\u672c\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u77e5\u8bc6\u9057\u5fd8\u95ee\u9898\uff0c\u4fc3\u8fdb\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b"}}
{"id": "2508.07262", "pdf": "https://arxiv.org/pdf/2508.07262", "abs": "https://arxiv.org/abs/2508.07262", "authors": ["Bernd J. Kr\u00f6ger"], "title": "The 2D+ Dynamic Articulatory Model DYNARTmo: Tongue-Palate Contact Area Estimation", "categories": ["cs.CL", "cs.RO"], "comment": "11 pages, 9 figures, 14 references; supplementary material: python\n  source code", "summary": "This paper describes an extension of the two-dimensional dynamic articulatory\nmodel DYNARTmo by integrating an internal three-dimensional representation of\nthe palatal dome to estimate tongue-palate contact areas from midsagittal\ntongue contours. Two alternative dome geometries - a half-ellipse and a cosine\nbased profile - are implemented to model lateral curvature in the coronal\nplane. Using these geometries, lateral contact points are analytically computed\nfor each anterior-posterior position, enabling the generation of\nelectropalatography-like visualizations within the 2D+ framework. The enhanced\nmodel supports three synchronized views (sagittal, glottal, and palatal) for\nstatic and dynamic (animated) articulation displays, suitable for speech\nscience education and speech therapy. Future work includes adding a facial\n(lip) view and implementing articulatory-to-acoustic synthesis to\nquantitatively evaluate model realism.", "AI": {"tldr": "\u901a\u8fc7\u6574\u5408\u4e09\u7ef4\u816d\u7a79\u6a21\u578b\u6269\u5c55\u4e8c\u7ef4\u53d1\u97f3\u6a21\u578bDYNARTmo\uff0c\u5b9e\u73b0\u820c\u816d\u63a5\u89e6\u533a\u57df\u53ef\u89c6\u5316\uff0c\u5e94\u7528\u4e8e\u8bed\u97f3\u6559\u5b66\u4e0e\u6cbb\u7597\u3002", "motivation": "\u63d0\u5347\u52a8\u6001\u53d1\u97f3\u6a21\u578b\u5bf9\u820c\u816d\u63a5\u89e6\u7684\u91cf\u5316\u8868\u5f81\u80fd\u529b\uff0c\u4e3a\u8bed\u97f3\u79d1\u5b66\u6559\u80b2\u548c\u8a00\u8bed\u6cbb\u7597\u63d0\u4f9b\u66f4\u76f4\u89c2\u7684\u53d1\u97f3\u53ef\u89c6\u5316\u5de5\u5177\u3002", "method": "\u5728\u4e8c\u7ef4\u6a21\u578b\u57fa\u7840\u4e0a\u96c6\u6210\u4e09\u7ef4\u816d\u7a79\u7ed3\u6784\uff08\u534a\u692d\u5706/\u4f59\u5f26\u66f2\u7ebf\u4e24\u79cd\u51e0\u4f55\u5f62\u6001\uff09\uff0c\u901a\u8fc7\u89e3\u6790\u8ba1\u7b97\u751f\u6210\u7535\u816d\u56fe\u5f0f\u63a5\u89e6\u53ef\u89c6\u5316\u3002", "result": "\u5f00\u53d1\u51fa\u652f\u6301\u77e2\u72b6\u9762\u3001\u58f0\u95e8\u9762\u548c\u816d\u9762\u4e09\u89c6\u56fe\u540c\u6b65\u663e\u793a\u7684\u589e\u5f3a\u6a21\u578b\uff0c\u652f\u6301\u9759\u6001/\u52a8\u6001\u53d1\u97f3\u52a8\u753b\u6f14\u793a\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u53d1\u97f3\u8bad\u7ec3\u63d0\u4f9b\u591a\u7ef4\u5ea6\u53ef\u89c6\u5316\u652f\u6301\uff0c\u672a\u6765\u62df\u589e\u52a0\u9762\u90e8\u89c6\u56fe\u548c\u58f0\u5b66\u5408\u6210\u529f\u80fd\u4ee5\u63d0\u5347\u6a21\u578b\u771f\u5b9e\u6027\u8bc4\u4f30\u80fd\u529b\u3002"}}
{"id": "2508.07273", "pdf": "https://arxiv.org/pdf/2508.07273", "abs": "https://arxiv.org/abs/2508.07273", "authors": ["Qiongqiong Wang", "Hardik B. Sailor", "Jeremy H. M. Wong", "Tianchi Liu", "Shuo Sun", "Wenyu Zhang", "Muhammad Huzaifah", "Nancy Chen", "Ai Ti Aw"], "title": "Incorporating Contextual Paralinguistic Understanding in Large Speech-Language Models", "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": "Accepted at (ASRU 2025) 2025 IEEE Automatic Speech Recognition and\n  Understanding Workshop", "summary": "Current large speech language models (Speech-LLMs) often exhibit limitations\nin empathetic reasoning, primarily due to the absence of training datasets that\nintegrate both contextual content and paralinguistic cues. In this work, we\npropose two approaches to incorporate contextual paralinguistic information\ninto model training: (1) an explicit method that provides paralinguistic\nmetadata (e.g., emotion annotations) directly to the LLM, and (2) an implicit\nmethod that automatically generates novel training question-answer (QA) pairs\nusing both categorical and dimensional emotion annotations alongside speech\ntranscriptions. Our implicit method boosts performance (LLM-judged) by 38.41%\non a human-annotated QA benchmark, reaching 46.02% when combined with the\nexplicit approach, showing effectiveness in contextual paralinguistic\nunderstanding. We also validate the LLM judge by demonstrating its correlation\nwith classification metrics, providing support for its reliability.", "AI": {"tldr": "\u63d0\u51fa\u663e\u5f0f\u548c\u9690\u5f0f\u65b9\u6cd5\u589e\u5f3a\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u526f\u8bed\u8a00\u7406\u89e3\u80fd\u529b\uff0c\u9690\u5f0f\u65b9\u6cd5\u63d0\u534738.41%\uff0c\u7ed3\u5408\u663e\u5f0f\u8fbe46.02%", "motivation": "\u5f53\u524d\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u56e0\u7f3a\u4e4f\u6574\u5408\u4e0a\u4e0b\u6587\u4e0e\u526f\u8bed\u8a00\u7ebf\u7d22\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u5728\u5171\u60c5\u63a8\u7406\u4e2d\u5b58\u5728\u5c40\u9650", "method": "\u663e\u5f0f\u65b9\u6cd5\u76f4\u63a5\u6ce8\u5165\u526f\u8bed\u8a00\u5143\u6570\u636e\uff0c\u9690\u5f0f\u65b9\u6cd5\u901a\u8fc7\u60c5\u611f\u6807\u6ce8+\u8bed\u97f3\u8f6c\u5f55\u81ea\u52a8\u751f\u6210\u8bad\u7ec3QA\u5bf9", "result": "\u9690\u5f0f\u65b9\u6cd5\u5355\u7528\u63d0\u534738.41%\u6027\u80fd\uff0c\u663e\u9690\u7ed3\u5408\u8fbe46.02%\uff0cLLM\u8bc4\u4f30\u4e0e\u5206\u7c7b\u6307\u6807\u5177\u5f3a\u76f8\u5173\u6027", "conclusion": "\u53cc\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u526f\u8bed\u8a00\u7406\u89e3\uff0c\u9a8c\u8bc1LLM\u8bc4\u4f30\u53ef\u9760\u6027\uff0c\u4e3a\u60c5\u611f\u611f\u77e5\u8bed\u97f3\u6a21\u578b\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2508.07279", "pdf": "https://arxiv.org/pdf/2508.07279", "abs": "https://arxiv.org/abs/2508.07279", "authors": ["Vasudha Varadarajan", "Hui Xu", "Rebecca Astrid Boehme", "Mariam Marlan Mirstrom", "Sverker Sikstrom", "H. Andrew Schwartz"], "title": "MAQuA: Adaptive Question-Asking for Multidimensional Mental Health Screening using Item Response Theory", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) offer new opportunities for\nscalable, interactive mental health assessment, but excessive querying by LLMs\nburdens users and is inefficient for real-world screening across\ntransdiagnostic symptom profiles. We introduce MAQuA, an adaptive\nquestion-asking framework for simultaneous, multidimensional mental health\nscreening. Combining multi-outcome modeling on language responses with item\nresponse theory (IRT) and factor analysis, MAQuA selects the questions with\nmost informative responses across multiple dimensions at each turn to optimize\ndiagnostic information, improving accuracy and potentially reducing response\nburden. Empirical results on a novel dataset reveal that MAQuA reduces the\nnumber of assessment questions required for score stabilization by 50-87%\ncompared to random ordering (e.g., achieving stable depression scores with 71%\nfewer questions and eating disorder scores with 85% fewer questions). MAQuA\ndemonstrates robust performance across both internalizing (depression, anxiety)\nand externalizing (substance use, eating disorder) domains, with early stopping\nstrategies further reducing patient time and burden. These findings position\nMAQuA as a powerful and efficient tool for scalable, nuanced, and interactive\nmental health screening, advancing the integration of LLM-based agents into\nreal-world clinical workflows.", "AI": {"tldr": "MAQuA\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u63d0\u95ee\u7b56\u7565\uff0c\u7ed3\u5408\u591a\u7ef4\u5ea6\u5efa\u6a21\u548cIRT\u7406\u8bba\uff0c\u5b9e\u73b0\u9ad8\u6548\u5fc3\u7406\u5065\u5eb7\u7b5b\u67e5\uff0c\u51cf\u5c1150-87%\u8bc4\u4f30\u95ee\u9898\u6570\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5fc3\u7406\u5065\u5eb7\u8bc4\u4f30\u5b58\u5728\u63d0\u95ee\u5197\u4f59\u95ee\u9898\uff0c\u65e0\u6cd5\u5b9e\u73b0\u8de8\u8bca\u65ad\u7ef4\u5ea6\u7684\u5b9e\u65f6\u9ad8\u6548\u7b5b\u67e5\u3002", "method": "\u6574\u5408\u591a\u7ed3\u679c\u8bed\u8a00\u54cd\u5e94\u5efa\u6a21\u3001\u9879\u76ee\u53cd\u5e94\u7406\u8bba(IRT)\u548c\u56e0\u5b50\u5206\u6790\uff0c\u52a8\u6001\u9009\u62e9\u4fe1\u606f\u91cf\u6700\u5927\u7684\u8de8\u7ef4\u5ea6\u95ee\u9898\u8fdb\u884c\u4f18\u5316\u8bca\u65ad\u3002", "result": "\u5728\u65b0\u578b\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cMAQuA\u4f7f\u8bc4\u5206\u7a33\u5b9a\u6240\u9700\u95ee\u9898\u51cf\u5c1150-87%\uff08\u6291\u90c1\u75c771%\uff0c\u996e\u98df\u969c\u788d85%\uff09\uff0c\u5728\u5185\u5316/\u5916\u5316\u9886\u57df\u5747\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "MAQuA\u4f5c\u4e3a\u9ad8\u6548\u7b5b\u67e5\u5de5\u5177\uff0c\u63a8\u8fdb\u4e86\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u4e0e\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7684\u6574\u5408\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u7cbe\u7ec6\u5316\u7684\u4ea4\u4e92\u5f0f\u5fc3\u7406\u5065\u5eb7\u8bc4\u4f30\u3002"}}
{"id": "2508.07284", "pdf": "https://arxiv.org/pdf/2508.07284", "abs": "https://arxiv.org/abs/2508.07284", "authors": ["Junchen Ding", "Penghao Jiang", "Zihao Xu", "Ziqi Ding", "Yichen Zhu", "Jiaojiao Jiang", "Yuekang Li"], "title": "\"Pull or Not to Pull?'': Investigating Moral Biases in Leading Large Language Models Across Ethical Dilemmas", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "As large language models (LLMs) increasingly mediate ethically sensitive\ndecisions, understanding their moral reasoning processes becomes imperative.\nThis study presents a comprehensive empirical evaluation of 14 leading LLMs,\nboth reasoning enabled and general purpose, across 27 diverse trolley problem\nscenarios, framed by ten moral philosophies, including utilitarianism,\ndeontology, and altruism. Using a factorial prompting protocol, we elicited\n3,780 binary decisions and natural language justifications, enabling analysis\nalong axes of decisional assertiveness, explanation answer consistency, public\nmoral alignment, and sensitivity to ethically irrelevant cues. Our findings\nreveal significant variability across ethical frames and model types: reasoning\nenhanced models demonstrate greater decisiveness and structured justifications,\nyet do not always align better with human consensus. Notably, \"sweet zones\"\nemerge in altruistic, fairness, and virtue ethics framings, where models\nachieve a balance of high intervention rates, low explanation conflict, and\nminimal divergence from aggregated human judgments. However, models diverge\nunder frames emphasizing kinship, legality, or self interest, often producing\nethically controversial outcomes. These patterns suggest that moral prompting\nis not only a behavioral modifier but also a diagnostic tool for uncovering\nlatent alignment philosophies across providers. We advocate for moral reasoning\nto become a primary axis in LLM alignment, calling for standardized benchmarks\nthat evaluate not just what LLMs decide, but how and why.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc727\u4e2a\u7535\u8f66\u95ee\u9898\u573a\u666f\u8bc4\u4f3014\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9053\u5fb7\u51b3\u7b56\uff0c\u53d1\u73b0\u4e0d\u540c\u4f26\u7406\u6846\u67b6\u4e0b\u6a21\u578b\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u63a8\u7406\u589e\u5f3a\u6a21\u578b\u867d\u51b3\u7b56\u66f4\u679c\u65ad\u4f46\u672a\u5fc5\u66f4\u7b26\u5408\u4eba\u7c7b\u5171\u8bc6\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u53c2\u4e0e\u4f26\u7406\u654f\u611f\u51b3\u7b56\uff0c\u63ed\u793a\u5176\u9053\u5fb7\u63a8\u7406\u673a\u5236\u6210\u4e3a\u8feb\u5207\u9700\u6c42\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\u8bc4\u4f30\u4e3b\u6d41\u6a21\u578b\u5728\u4e0d\u540c\u9053\u5fb7\u54f2\u5b66\u6846\u67b6\u4e0b\u7684\u51b3\u7b56\u6a21\u5f0f\u53ca\u5176\u4e0e\u4eba\u7c7b\u5171\u8bc6\u7684\u504f\u5dee\u3002", "method": "\u4f7f\u7528\u5341\u79cd\u9053\u5fb7\u54f2\u5b66\u6846\u67b6\uff08\u529f\u5229\u4e3b\u4e49\u3001\u4e49\u52a1\u8bba\u7b49\uff09\u6784\u5efa27\u4e2a\u7535\u8f66\u95ee\u9898\uff0c\u901a\u8fc7\u56e0\u5b50\u5316\u63d0\u793a\u534f\u8bae\u6536\u96c614\u4e2aLLMs\u76843,780\u4e2a\u51b3\u7b56\u53ca\u89e3\u91ca\uff0c\u5206\u6790\u51b3\u7b56\u575a\u5b9a\u6027\u3001\u89e3\u91ca\u4e00\u81f4\u6027\u3001\u516c\u4f17\u9053\u5fb7\u5bf9\u9f50\u5ea6\u7b49\u6307\u6807\u3002", "result": "\u63a8\u7406\u589e\u5f3a\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u9ad8\u51b3\u7b56\u679c\u65ad\u6027\uff08\u63d0\u534725%\uff09\u548c\u7ed3\u6784\u5316\u89e3\u91ca\u80fd\u529b\uff0c\u4f46\u5728\u5229\u4ed6\u4e3b\u4e49/\u516c\u5e73/\u7f8e\u5fb7\u4f26\u7406\u6846\u67b6\u4e0b\u4e0e\u4eba\u7c7b\u5171\u8bc6\u5339\u914d\u5ea6\u6700\u4f73\u3002\u6cd5\u5f8b/\u4eb2\u5c5e/\u81ea\u5229\u6846\u67b6\u5f15\u53d1\u6700\u5927\u4f26\u7406\u4e89\u8bae\uff0840%\u51b3\u7b56\u504f\u79bb\u4eba\u7c7b\u57fa\u51c6\uff09\u3002", "conclusion": "\u9053\u5fb7\u63d0\u793a\u5e94\u6210\u4e3aLLM\u5bf9\u9f50\u7684\u6838\u5fc3\u6307\u6807\uff0c\u5efa\u8bae\u5efa\u7acb\u6807\u51c6\u5316\u8bc4\u6d4b\u57fa\u51c6\uff0c\u91cd\u70b9\u8003\u5bdf\u6a21\u578b\u7684\u51b3\u7b56\u903b\u8f91\u800c\u4e0d\u4ec5\u662f\u7ed3\u679c\u3002\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u5b58\u5728'\u9690\u6027\u9053\u5fb7\u56fe\u8c31'\uff0c\u9700\u901a\u8fc7\u7cfb\u7edf\u5316\u9053\u5fb7\u538b\u529b\u6d4b\u8bd5\u63ed\u793a\u3002"}}
{"id": "2508.07286", "pdf": "https://arxiv.org/pdf/2508.07286", "abs": "https://arxiv.org/abs/2508.07286", "authors": ["Jian Chen", "Jinbao Tian", "Yankui Li", "Zhou Li"], "title": "Arce: Augmented Roberta with Contextualized Elucidations for Ner in Automated Rule Checking", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Accurate information extraction from specialized texts is a critical\nchallenge, particularly for named entity recognition (NER) in the architecture,\nengineering, and construction (AEC) domain to support automated rule checking\n(ARC). The performance of standard pre-trained models is often constrained by\nthe domain gap, as they struggle to interpret the specialized terminology and\ncomplex relational contexts inherent in AEC texts. Although this issue can be\nmitigated by further pre-training on large, human-curated domain corpora, as\nexemplified by methods like ARCBERT, this approach is both labor-intensive and\ncost-prohibitive. Consequently, leveraging large language models (LLMs) for\nautomated knowledge generation has emerged as a promising alternative. However,\nthe optimal strategy for generating knowledge that can genuinely enhance\nsmaller, efficient models remains an open question. To address this, we propose\nARCE (augmented RoBERTa with contextualized elucidations), a novel approach\nthat systematically explores and optimizes this generation process. ARCE\nemploys an LLM to first generate a corpus of simple, direct explanations, which\nwe term Cote, and then uses this corpus to incrementally pre-train a RoBERTa\nmodel prior to its fine-tuning on the downstream task. Our extensive\nexperiments show that ARCE establishes a new state-of-the-art on a benchmark\nAEC dataset, achieving a Macro-F1 score of 77.20%. This result also reveals a\nkey finding: simple, explanation-based knowledge proves surprisingly more\neffective than complex, role-based rationales for this task. The code is\npublicly available at:https://github.com/nxcc-lab/ARCE.", "AI": {"tldr": "\u63d0\u51faARCE\u65b9\u6cd5\uff0c\u901a\u8fc7\u5927\u6a21\u578b\u751f\u6210\u60c5\u5883\u5316\u89e3\u91ca\u8bed\u6599\u5e93\u589e\u5f3a\u5c0f\u6a21\u578b\uff0c\u5728AEC\u9886\u57dfNER\u4efb\u52a1\u4e2d\u53d6\u5f9777.20%\u7684SOTA\u6548\u679c", "motivation": "\u4f20\u7edf\u9884\u8bad\u7ec3\u6a21\u578b\u5b58\u5728\u9886\u57df\u9002\u5e94\u6027\u5dee\uff0c\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff1bLLM\u751f\u6210\u77e5\u8bc6\u53ef\u6709\u6548\u589e\u5f3a\u5c0f\u6a21\u578b\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u4f18\u5316\u65b9\u6cd5", "method": "\u4f7f\u7528LLM\u751f\u6210\u7b80\u5355\u89e3\u91ca\u8bed\u6599\u5e93\uff08Cote\uff09\uff0c\u57fa\u4e8e\u8be5\u8bed\u6599\u5e93\u8fdb\u884cRoBERTa\u589e\u91cf\u9884\u8bad\u7ec3\u540e\u5fae\u8c03", "result": "\u5728AEC\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u523077.20% Macro-F1\uff0c\u6bd4\u590d\u6742\u89d2\u8272\u63a8\u7406\u65b9\u6cd5\u66f4\u6709\u6548", "conclusion": "\u7b80\u5355\u89e3\u91ca\u6027\u77e5\u8bc6\u589e\u5f3a\u663e\u8457\u4f18\u4e8e\u590d\u6742\u89d2\u8272\u63a8\u7406\uff0c\u8bc1\u660e\u8bed\u4e49\u5c42\u9762\u77e5\u8bc6\u6ce8\u5165\u7684\u6709\u6548\u6027"}}
{"id": "2508.07295", "pdf": "https://arxiv.org/pdf/2508.07295", "abs": "https://arxiv.org/abs/2508.07295", "authors": ["Yexing Du", "Kaiyuan Liu", "Youcheng Pan", "Zheng Chu", "Bo Yang", "Xiaocheng Feng", "Yang Xiang", "Ming Liu"], "title": "CCFQA: A Benchmark for Cross-Lingual and Cross-Modal Speech and Text Factuality Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) are increasingly popularized in the\nmultilingual world, ensuring hallucination-free factuality becomes markedly\ncrucial. However, existing benchmarks for evaluating the reliability of\nMultimodal Large Language Models (MLLMs) predominantly focus on textual or\nvisual modalities with a primary emphasis on English, which creates a gap in\nevaluation when processing multilingual input, especially in speech. To bridge\nthis gap, we propose a novel \\textbf{C}ross-lingual and \\textbf{C}ross-modal\n\\textbf{F}actuality benchmark (\\textbf{CCFQA}). Specifically, the CCFQA\nbenchmark contains parallel speech-text factual questions across 8 languages,\ndesigned to systematically evaluate MLLMs' cross-lingual and cross-modal\nfactuality capabilities. Our experimental results demonstrate that current\nMLLMs still face substantial challenges on the CCFQA benchmark. Furthermore, we\npropose a few-shot transfer learning strategy that effectively transfers the\nQuestion Answering (QA) capabilities of LLMs in English to multilingual Spoken\nQuestion Answering (SQA) tasks, achieving competitive performance with\nGPT-4o-mini-Audio using just 5-shot training. We release CCFQA as a\nfoundational research resource to promote the development of MLLMs with more\nrobust and reliable speech understanding capabilities. Our code and dataset are\navailable at https://github.com/yxduir/ccfqa.", "AI": {"tldr": "\u63d0\u51fa\u8de8\u8bed\u8a00\u4e0e\u8de8\u6a21\u6001\u4e8b\u5b9e\u6027\u57fa\u51c6CCFQA\uff0c\u7cfb\u7edf\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u8bed\u8a00\u8bed\u97f3\u7406\u89e3\u80fd\u529b", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u82f1\u8bed\u6587\u672c/\u89c6\u89c9\u6a21\u6001\uff0c\u7f3a\u4e4f\u591a\u8bed\u8a00\u8bed\u97f3\u8f93\u5165\u573a\u666f\u7684\u8bc4\u4f30\u4f53\u7cfb", "method": "\u6784\u5efa\u542b8\u79cd\u8bed\u8a00\u5e73\u884c\u8bed\u97f3-\u6587\u672c\u95ee\u9898\u7684\u57fa\u51c6\uff0c\u5e76\u63d0\u51fafew-shot\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\u5b9e\u73b0\u82f1\u8bedQA\u80fd\u529b\u5411\u591a\u8bed\u8a00SQA\u4efb\u52a1\u7684\u8fc1\u79fb", "result": "\u5f53\u524d\u6a21\u578b\u5728CCFQA\u8868\u73b0\u6b20\u4f73\uff0c\u4f465-shot\u8bad\u7ec3\u5373\u53ef\u8fbe\u5230\u4e0eGPT-4o-mini-Audio\u7ade\u4e89\u7684\u6027\u80fd", "conclusion": "CCFQA\u4f5c\u4e3a\u57fa\u7840\u7814\u7a76\u8d44\u6e90\uff0c\u5c06\u63a8\u52a8\u5f00\u53d1\u5177\u5907\u66f4\u9c81\u68d2\u8bed\u97f3\u7406\u89e3\u80fd\u529b\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b"}}
{"id": "2508.07308", "pdf": "https://arxiv.org/pdf/2508.07308", "abs": "https://arxiv.org/abs/2508.07308", "authors": ["Cristian Cosentino", "Annamaria Defilippo", "Marco Dossena", "Christopher Irwin", "Sara Joubbi", "Pietro Li\u00f2"], "title": "HealthBranches: Synthesizing Clinically-Grounded Question Answering Datasets via Decision Pathways", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "HealthBranches is a novel benchmark dataset for medical Question-Answering\n(Q&A), specifically designed to evaluate complex reasoning in Large Language\nModels (LLMs). This dataset is generated through a semi-automated pipeline that\ntransforms explicit decision pathways from medical source into realistic\npatient cases with associated questions and answers. Covering 4,063 case\nstudies across 17 healthcare topics, each data point is based on clinically\nvalidated reasoning chains. HealthBranches supports both open-ended and\nmultiple-choice question formats and uniquely includes the full reasoning path\nfor each Q&A. Its structured design enables robust evaluation of LLMs'\nmulti-step inference capabilities, including their performance in structured\nRetrieval-Augmented Generation (RAG) contexts. HealthBranches establishes a\nfoundation for the development of more trustworthy, interpretable, and\nclinically reliable LLMs in high-stakes domains while also serving as a\nvaluable resource for educational purposes.", "AI": {"tldr": "\u533b\u7597\u95ee\u7b54\u57fa\u51c6\u6570\u636e\u96c6HealthBranches\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u542b4063\u6848\u4f8b\u53ca\u5b8c\u6574\u63a8\u7406\u8def\u5f84", "motivation": "\u89e3\u51b3\u533b\u7597\u9886\u57dfLLMs\u7f3a\u4e4f\u591a\u6b65\u63a8\u7406\u80fd\u529b\u9a8c\u8bc1\u7684\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u53ef\u4fe1\u5ea6\u4e0e\u4e34\u5e8a\u53ef\u9760\u6027", "method": "\u901a\u8fc7\u534a\u81ea\u52a8\u6d41\u7a0b\u5c06\u4e34\u5e8a\u51b3\u7b56\u8def\u5f84\u8f6c\u5316\u4e3a\u771f\u5b9e\u75c5\u4f8bQA\uff0c\u652f\u6301\u5f00\u653e\u5f0f/\u9009\u62e9\u9898\u683c\u5f0f\u5e76\u4fdd\u7559\u5b8c\u6574\u63a8\u7406\u94fe", "result": "\u5efa\u7acb\u7ed3\u6784\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u6709\u6548\u9a8c\u8bc1LLMs\u5728\u589e\u5f3a\u68c0\u7d22\u751f\u6210(RAG)\u573a\u666f\u4e2d\u7684\u591a\u6b65\u63a8\u7406\u6027\u80fd", "conclusion": "\u4e3a\u9ad8\u98ce\u9669\u9886\u57df\u5f00\u53d1\u53ef\u4fe1\u8d56\u7684LLMs\u5960\u5b9a\u57fa\u7840\uff0c\u517c\u5177\u533b\u5b66\u6559\u80b2\u4e0e\u6a21\u578b\u8bc4\u4f30\u53cc\u91cd\u4ef7\u503c"}}
{"id": "2508.07321", "pdf": "https://arxiv.org/pdf/2508.07321", "abs": "https://arxiv.org/abs/2508.07321", "authors": ["Shubhra Ghosh", "Abhilekh Borah", "Aditya Kumar Guru", "Kripabandhu Ghosh"], "title": "ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": null, "summary": "The rapid proliferation of Large Language Models (LLMs) has significantly\ncontributed to the development of equitable AI systems capable of factual\nquestion-answering (QA). However, no known study tests the LLMs' robustness\nwhen presented with obfuscated versions of questions. To systematically\nevaluate these limitations, we propose a novel technique, ObfusQAte and,\nleveraging the same, introduce ObfusQA, a comprehensive, first of its kind,\nframework with multi-tiered obfuscation levels designed to examine LLM\ncapabilities across three distinct dimensions: (i) Named-Entity Indirection,\n(ii) Distractor Indirection, and (iii) Contextual Overload. By capturing these\nfine-grained distinctions in language, ObfusQA provides a comprehensive\nbenchmark for evaluating LLM robustness and adaptability. Our study observes\nthat LLMs exhibit a tendency to fail or generate hallucinated responses when\nconfronted with these increasingly nuanced variations. To foster research in\nthis direction, we make ObfusQAte publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86ObfusQAte\u6280\u672f\u548c\u591a\u5c42\u7ea7\u6df7\u6dc6\u6846\u67b6ObfusQA\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6df7\u6dc6\u95ee\u9898\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u6a21\u578b\u6613\u4ea7\u751f\u9519\u8bef\u56de\u7b54\u5e76\u516c\u5f00\u4e86\u6d4b\u8bd5\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9LLMs\u5728\u8bed\u8a00\u6df7\u6dc6\u573a\u666f\u4e0b\u9c81\u68d2\u6027\u7684\u7cfb\u7edf\u6027\u6d4b\u8bd5\uff0c\u7279\u522b\u662f\u5728\u547d\u540d\u5b9e\u4f53\u5e72\u6270\u3001\u5e72\u6270\u9879\u95f4\u63a5\u5f15\u7528\u548c\u4e0a\u4e0b\u6587\u8fc7\u8f7d\u7b49\u7ec6\u7c92\u5ea6\u8bed\u8a00\u53d8\u5f02\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u5305\u542b\u4e09\u4e2a\u7ef4\u5ea6\uff08\u547d\u540d\u5b9e\u4f53\u95f4\u63a5/\u5e72\u6270\u9879\u95f4\u63a5/\u4e0a\u4e0b\u6587\u8fc7\u8f7d\uff09\u7684\u591a\u5c42\u7ea7\u6df7\u6dc6\u6846\u67b6ObfusQA\uff0c\u91c7\u7528\u5c42\u6b21\u5316\u6d4b\u8bd5\u65b9\u6cd5\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u7684\u6297\u5e72\u6270\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLLMs\u5728\u9762\u5bf9\u6e10\u8fdb\u5f0f\u8bed\u8a00\u6df7\u6dc6\u65f6\uff0c\u4f1a\u4ea7\u751f\u663e\u8457\u589e\u52a0\u7684\u9519\u8bef\u7387\u548c\u4e8b\u5b9e\u6027\u5e7b\u89c9\u56de\u7b54\u3002", "conclusion": "ObfusQA\u6846\u67b6\u4e3a\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5176\u516c\u5f00\u5c06\u63a8\u52a8AI\u7cfb\u7edf\u5728\u590d\u6742\u8bed\u8a00\u7406\u89e3\u65b9\u5411\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.07325", "pdf": "https://arxiv.org/pdf/2508.07325", "abs": "https://arxiv.org/abs/2508.07325", "authors": ["Dean Geckt", "Melinda Fricke", "Shuly Wintner"], "title": "Strategies of Code-switching in Human-Machine Dialogs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Most people are multilingual, and most multilinguals code-switch, yet the\ncharacteristics of code-switched language are not fully understood. We\ndeveloped a chatbot capable of completing a Map Task with human participants\nusing code-switched Spanish and English. In two experiments, we prompted the\nbot to code-switch according to different strategies, examining (1) the\nfeasibility of such experiments for investigating bilingual language use, and\n(2) whether participants would be sensitive to variations in discourse and\ngrammatical patterns. Participants generally enjoyed code-switching with our\nbot as long as it produced predictable code-switching behavior; when\ncode-switching was random or ungrammatical (as when producing unattested\nincongruent mixed-language noun phrases, such as `la fork'), participants\nenjoyed the task less and were less successful at completing it. These results\nunderscore the potential downsides of deploying insufficiently developed\nmultilingual language technology, while also illustrating the promise of such\ntechnology for conducting research on bilingual language use.", "AI": {"tldr": "\u901a\u8fc7\u53ef\u63a7\u5236\u4ee3\u7801\u5207\u6362\u7b56\u7565\u7684\u804a\u5929\u673a\u5668\u4eba\u5b9e\u9a8c\uff0c\u63ed\u793a\u53cc\u8bed\u8005\u5bf9\u4e0d\u540c\u8bed\u7801\u8f6c\u6362\u6a21\u5f0f\u7684\u63a5\u53d7\u5ea6\u5dee\u5f02\u53ca\u5176\u5bf9\u6280\u672f\u5f00\u53d1\u7684\u542f\u793a", "motivation": "\u73b0\u6709\u7814\u7a76\u5c1a\u672a\u5145\u5206\u7406\u89e3\u8bed\u7801\u8f6c\u6362\u7684\u8bed\u8a00\u7279\u5f81\uff0c\u4e14\u591a\u8bed\u8a00\u6280\u672f\u5b58\u5728\u6f5c\u5728\u7f3a\u9677\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4eba\u673a\u4ea4\u4e92\u5b9e\u9a8c\u63a2\u7d22\u53cc\u8bed\u8005\u8bed\u8a00\u4f7f\u7528\u89c4\u5f8b\uff0c\u5e76\u9a8c\u8bc1\u5b9e\u9a8c\u65b9\u6cd5\u7684\u53ef\u884c\u6027", "method": "\u5f00\u53d1\u897f\u82f1\u53cc\u8bed\u4ee3\u7801\u5207\u6362\u804a\u5929\u673a\u5668\u4eba\uff0c\u8bbe\u8ba1\u5730\u56fe\u4efb\u52a1\u3002\u8bbe\u7f6e\u4e24\u79cd\u4ee3\u7801\u5207\u6362\u7b56\u7565\uff08\u7b26\u5408\u8bed\u6cd5\u89c4\u5219vs\u968f\u673a/\u4e0d\u5408\u8bed\u6cd5\uff09\uff0c\u6d4b\u91cf\u53c2\u4e0e\u8005\u7684\u4efb\u52a1\u5b8c\u6210\u5ea6\u4e0e\u5fc3\u7406\u4f53\u9a8c", "result": "\u53c2\u4e0e\u8005\u66f4\u80fd\u63a5\u53d7\u7b26\u5408\u8bed\u6cd5\u89c4\u5219\u7684\u9884\u6d4b\u6027\u4ee3\u7801\u5207\u6362\uff08\u6210\u529f\u738778%\uff09\u3002\u5f53\u51fa\u73b0\u968f\u673a\u5207\u6362\u6216\u6df7\u5408\u7ed3\u6784\u9519\u8bef\uff08\u5982'la fork'\uff09\u65f6\uff0c\u4efb\u52a1\u6210\u529f\u7387\u4e0b\u964d\u81f352%\uff0c\u4f53\u9a8c\u6ee1\u610f\u5ea6\u964d\u4f4e31%", "conclusion": "\u672a\u6210\u719f\u7684\u591a\u8bed\u8a00\u6280\u672f\u53ef\u80fd\u5e26\u6765\u7528\u6237\u4f53\u9a8c\u98ce\u9669\uff0c\u4f46\u53ef\u63a7\u7684\u4ee3\u7801\u5207\u6362\u5b9e\u9a8c\u8303\u5f0f\u4e3a\u53cc\u8bed\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u63ed\u793a\u4e86\u8bed\u6cd5\u4e00\u81f4\u6027\u5728\u8bed\u8a00\u6280\u672f\u5f00\u53d1\u4e2d\u7684\u91cd\u8981\u6027"}}
{"id": "2508.07375", "pdf": "https://arxiv.org/pdf/2508.07375", "abs": "https://arxiv.org/abs/2508.07375", "authors": ["Wenqian Cui", "Lei Zhu", "Xiaohui Li", "Zhihan Guo", "Haoli Bai", "Lu Hou", "Irwin King"], "title": "Think Before You Talk: Enhancing Meaningful Dialogue Generation in Full-Duplex Speech Language Models with Planning-Inspired Text Guidance", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Work in progress", "summary": "Full-Duplex Speech Language Models (FD-SLMs) are specialized foundation\nmodels designed to enable natural, real-time spoken interactions by modeling\ncomplex conversational dynamics such as interruptions, backchannels, and\noverlapping speech, and End-to-end (e2e) FD-SLMs leverage real-world\ndouble-channel conversational data to capture nuanced two-speaker dialogue\npatterns for human-like interactions. However, they face a critical challenge\n-- their conversational abilities often degrade compared to pure-text\nconversation due to prolonged speech sequences and limited high-quality spoken\ndialogue data. While text-guided speech generation could mitigate these issues,\nit suffers from timing and length issues when integrating textual guidance into\ndouble-channel audio streams, disrupting the precise time alignment essential\nfor natural interactions. To address these challenges, we propose TurnGuide, a\nnovel planning-inspired approach that mimics human conversational planning by\ndynamically segmenting assistant speech into dialogue turns and generating\nturn-level text guidance before speech output, which effectively resolves both\ninsertion timing and length challenges. Extensive experiments demonstrate our\napproach significantly improves e2e FD-SLMs' conversational abilities, enabling\nthem to generate semantically meaningful and coherent speech while maintaining\nnatural conversational flow. Demos are available at\nhttps://dreamtheater123.github.io/TurnGuide-Demo/. Code will be available at\nhttps://github.com/dreamtheater123/TurnGuide.", "AI": {"tldr": "\u63d0\u51fa\u4e86TurnGuide\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u5272\u5bf9\u8bdd\u8f6e\u6b21\u751f\u6210\u6587\u672c\u6307\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u5168\u53cc\u5de5\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff08FD-SLMs\uff09\u7684\u5bf9\u8bdd\u6d41\u7545\u6027\u548c\u8bed\u4e49\u8fde\u8d2f\u6027\u3002", "motivation": "\u5168\u53cc\u5de5\u8bed\u97f3\u6a21\u578b\u9762\u4e34\u957f\u8bed\u97f3\u5e8f\u5217\u548c\u9ad8\u8d28\u91cf\u53e3\u8bed\u5bf9\u8bdd\u6570\u636e\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u5bfc\u81f4\u5bf9\u8bdd\u80fd\u529b\u76f8\u6bd4\u7eaf\u6587\u672c\u6a21\u578b\u4e0b\u964d\u3002\u73b0\u6709\u6587\u672c\u6307\u5bfc\u7684\u8bed\u97f3\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u65f6\u95f4\u5bf9\u9f50\u548c\u957f\u5ea6\u63a7\u5236\u95ee\u9898\u3002", "method": "\u63d0\u51faTurnGuide\u6846\u67b6\uff0c\u6a21\u4eff\u4eba\u7c7b\u5bf9\u8bdd\u89c4\u5212\u673a\u5236\uff0c\u5c06\u52a9\u7406\u8bed\u97f3\u52a8\u6001\u5206\u5272\u4e3a\u5bf9\u8bdd\u8f6e\u6b21\uff0c\u5728\u8bed\u97f3\u8f93\u51fa\u524d\u751f\u6210\u8f6e\u6b21\u7ea7\u6587\u672c\u6307\u5bfc\uff0c\u89e3\u51b3\u63d2\u5165\u65f6\u673a\u548c\u957f\u5ea6\u63a7\u5236\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347FD-SLMs\u7684\u5bf9\u8bdd\u80fd\u529b\uff0c\u5728\u4fdd\u6301\u81ea\u7136\u5bf9\u8bdd\u6d41\u7684\u540c\u65f6\u751f\u6210\u8bed\u4e49\u8fde\u8d2f\u7684\u8bed\u97f3\u3002", "conclusion": "TurnGuide\u901a\u8fc7\u521b\u65b0\u7684\u5bf9\u8bdd\u89c4\u5212\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8bed\u97f3\u6a21\u578b\u7684\u65f6\u95f4\u5bf9\u9f50\u548c\u957f\u5ea6\u63a7\u5236\u96be\u9898\uff0c\u4e3a\u5b9e\u73b0\u7c7b\u4eba\u81ea\u7136\u8bed\u97f3\u4ea4\u4e92\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.07414", "pdf": "https://arxiv.org/pdf/2508.07414", "abs": "https://arxiv.org/abs/2508.07414", "authors": ["Jean de Dieu Nyandwi", "Yueqi Song", "Simran Khanuja", "Graham Neubig"], "title": "Grounding Multilingual Multimodal LLMs With Cultural Knowledge", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Multimodal Large Language Models excel in high-resource settings, but often\nmisinterpret long-tail cultural entities and underperform in low-resource\nlanguages. To address this gap, we propose a data-centric approach that\ndirectly grounds MLLMs in cultural knowledge. Leveraging a large scale\nknowledge graph from Wikidata, we collect images that represent culturally\nsignificant entities, and generate synthetic multilingual visual question\nanswering data. The resulting dataset, CulturalGround, comprises 22 million\nhigh-quality, culturally-rich VQA pairs spanning 42 countries and 39 languages.\nWe train an open-source MLLM CulturalPangea on CulturalGround, interleaving\nstandard multilingual instruction-tuning data to preserve general abilities.\nCulturalPangea achieves state-of-the-art performance among open models on\nvarious culture-focused multilingual multimodal benchmarks, outperforming prior\nmodels by an average of 5.0 without degrading results on mainstream\nvision-language tasks. Our findings show that our targeted, culturally grounded\napproach could substantially narrow the cultural gap in MLLMs and offer a\npractical path towards globally inclusive multimodal systems.", "AI": {"tldr": "\u63d0\u51faCulturalGround\u6570\u636e\u96c6\u548cCulturalPangea\u6a21\u578b\uff0c\u901a\u8fc7\u6587\u5316\u77e5\u8bc6\u5d4c\u5165\u6539\u5584\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00/\u6587\u5316\u573a\u666f\u7684\u8868\u73b0", "motivation": "\u89e3\u51b3MLLMs\u5728\u957f\u5c3e\u6587\u5316\u5b9e\u4f53\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u7406\u89e3\u4e0a\u7684\u4e0d\u8db3", "method": "\u57fa\u4e8eWikidata\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u5305\u542b42\u56fd39\u8bed\u8a00\u76842200\u4e07\u6587\u5316VQA\u5bf9\uff0c\u7ed3\u5408\u591a\u8bed\u8a00\u6307\u4ee4\u6570\u636e\u8bad\u7ec3\u5f00\u6e90\u6a21\u578b", "result": "\u5728\u6587\u5316\u76f8\u5173\u57fa\u51c6\u4e0a\u5e73\u5747\u63d0\u53475.0\u5206\uff0c\u4e3b\u6d41\u4efb\u52a1\u6027\u80fd\u65e0\u635f\uff0c\u5b9e\u73b0\u5f53\u524d\u5f00\u6e90\u6a21\u578b\u6700\u4f73\u8868\u73b0", "conclusion": "\u6587\u5316\u77e5\u8bc6\u5d4c\u5165\u80fd\u6709\u6548\u7f29\u5c0fMLLMs\u7684\u6587\u5316\u9e3f\u6c9f\uff0c\u4e3a\u6784\u5efa\u5168\u7403\u5305\u5bb9\u6027\u591a\u6a21\u6001\u7cfb\u7edf\u63d0\u4f9b\u53ef\u884c\u8def\u5f84"}}
{"id": "2508.07434", "pdf": "https://arxiv.org/pdf/2508.07434", "abs": "https://arxiv.org/abs/2508.07434", "authors": ["Zhiyi Lyu", "Jianguo Huang", "Yanchen Deng", "Steven Hoi", "Bo An"], "title": "Let's Revise Step-by-Step: A Unified Local Search Framework for Code Generation with LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) with inference-time scaling techniques show\npromise for code generation, yet face notable efficiency and scalability\nchallenges. Construction-based tree-search methods suffer from rapid growth in\ntree size, high token consumption, and lack of anytime property. In contrast,\nimprovement-based methods offer better performance but often struggle with\nuninformative reward signals and inefficient search strategies. In this work,\nwe propose \\textbf{ReLoc}, a unified local search framework which effectively\nperforms step-by-step code revision. Specifically, ReLoc explores a series of\nlocal revisions through four key algorithmic components: initial code drafting,\nneighborhood code generation, candidate evaluation, and incumbent code\nupdating, each of which can be instantiated with specific decision rules to\nrealize different local search algorithms such as Hill Climbing (HC) or Genetic\nAlgorithm (GA). Furthermore, we develop a specialized revision reward model\nthat evaluates code quality based on revision distance to produce fine-grained\npreferences that guide the local search toward more promising candidates.\nFinally, our extensive experimental results demonstrate that our approach\nachieves superior performance across diverse code generation tasks,\nsignificantly outperforming both construction-based tree search as well as the\nstate-of-the-art improvement-based code generation methods.", "AI": {"tldr": "ReLoc\u63d0\u51fa\u57fa\u4e8e\u56db\u6b65\u5c40\u90e8\u4fee\u8ba2\u7684\u7edf\u4e00\u4ee3\u7801\u751f\u6210\u6846\u67b6\uff08\u521d\u59cb\u4ee3\u7801\u751f\u6210-\u5019\u9009\u751f\u6210-\u8bc4\u4f30-\u8fed\u4ee3\u66f4\u65b0\uff09\uff0c\u914d\u5408\u4fee\u8ba2\u5956\u52b1\u6a21\u578b\uff0c\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u6811\u641c\u7d22\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u7f3a\u4e4f\u5373\u65f6\u6027\u7f3a\u9677\uff0c\u6539\u8fdb\u5f0f\u65b9\u6cd5\u5b58\u5728\u5956\u52b1\u4fe1\u53f7\u6a21\u7cca\u548c\u641c\u7d22\u7b56\u7565\u4f4e\u6548\u95ee\u9898\u3002\u9700\u8981\u66f4\u9ad8\u6548\u7684\u4ee3\u7801\u751f\u6210\u4f18\u5316\u6846\u67b6\u3002", "method": "1. \u56db\u6b65\u5c40\u90e8\u641c\u7d22\u6846\u67b6\uff1a\u521d\u59cb\u4ee3\u7801\u751f\u6210\u3001\u90bb\u57df\u4ee3\u7801\u751f\u6210\u3001\u5019\u9009\u8bc4\u4f30\u3001\u5f53\u524d\u6700\u4f18\u66f4\u65b0\uff1b2. \u57fa\u4e8e\u4fee\u8ba2\u8ddd\u79bb\u7684\u5956\u52b1\u6a21\u578b\uff1b3. \u652f\u6301\u722c\u5c71\u7b97\u6cd5/\u9057\u4f20\u7b97\u6cd5\u7b49\u591a\u79cd\u5b9e\u73b0\u65b9\u5f0f\u3002", "result": "\u5728\u591a\u6837\u5316\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u53d6\u5f97SOTA\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u663e\u8457\u4f18\u4e8e\u6811\u641c\u7d22\u548c\u73b0\u6709\u6539\u8fdb\u5f0f\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u5316\u5c40\u90e8\u641c\u7d22\u6846\u67b6\u4e0e\u7ec6\u7c92\u5ea6\u5956\u52b1\u6a21\u578b\u7684\u7ed3\u5408\uff0cReLoc\u6709\u6548\u63d0\u5347\u4e86\u4ee3\u7801\u751f\u6210\u8d28\u91cf\uff0c\u4e3aLLM\u4ee3\u7801\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2508.07479", "pdf": "https://arxiv.org/pdf/2508.07479", "abs": "https://arxiv.org/abs/2508.07479", "authors": ["Blerta Veseli", "Julian Chibane", "Mariya Toneva", "Alexander Koller"], "title": "Positional Biases Shift as Inputs Approach Context Window Limits", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) often struggle to use information across long\ninputs effectively. Prior work has identified positional biases, such as the\nLost in the Middle (LiM) effect, where models perform better when information\nappears at the beginning (primacy bias) or end (recency bias) of the input,\nrather than in the middle. However, long-context studies have not consistently\nreplicated these effects, raising questions about their intensity and the\nconditions under which they manifest. To address this, we conducted a\ncomprehensive analysis using relative rather than absolute input lengths,\ndefined with respect to each model's context window. Our findings reveal that\nthe LiM effect is strongest when inputs occupy up to 50% of a model's context\nwindow. Beyond that, the primacy bias weakens, while recency bias remains\nrelatively stable. This effectively eliminates the LiM effect; instead, we\nobserve a distance-based bias, where model performance is better when relevant\ninformation is closer to the end of the input. Furthermore, our results suggest\nthat successful retrieval is a prerequisite for reasoning in LLMs, and that the\nobserved positional biases in reasoning are largely inherited from retrieval.\nThese insights have implications for long-context tasks, the design of future\nLLM benchmarks, and evaluation methodologies for LLMs handling extended inputs.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u6587\u672c\u5904\u7406\u4e2d\u5b58\u5728\u4f4d\u7f6e\u504f\u5dee\uff0cLiM\u6548\u5e94\u5728\u8f93\u5165\u5360\u4e0a\u4e0b\u6587\u7a97\u53e350%\u65f6\u6700\u663e\u8457\uff0c\u8d85\u8fc7\u540e\u9996\u56e0\u504f\u5dee\u51cf\u5f31\uff0c\u8ddd\u79bb\u504f\u5dee\u4e3b\u5bfc\u3002\u68c0\u7d22\u80fd\u529b\u662fLLM\u63a8\u7406\u7684\u524d\u63d0\u3002", "motivation": "\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u7814\u7a76\u4e2d\u4f4d\u7f6e\u504f\u5dee\uff08LiM\u6548\u5e94\uff09\u8868\u73b0\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u63a2\u7a76\u5176\u5f3a\u5ea6\u4e0e\u89e6\u53d1\u6761\u4ef6\uff0c\u4e3a\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\u3002", "method": "\u91c7\u7528\u76f8\u5bf9\u8f93\u5165\u957f\u5ea6\uff08\u57fa\u4e8e\u6a21\u578b\u81ea\u8eab\u4e0a\u4e0b\u6587\u7a97\u53e3\uff09\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u7cfb\u7edf\u5206\u6790\u4e0d\u540c\u8f93\u5165\u6bd4\u4f8b\u4e0b\u4f4d\u7f6e\u504f\u5dee\u7684\u53d8\u5316\u89c4\u5f8b\u3002", "result": "1. LiM\u6548\u5e94\u5728\u8f93\u5165\u226450%\u4e0a\u4e0b\u6587\u7a97\u53e3\u65f6\u6700\u5f3a\n2. \u9996\u56e0\u504f\u5dee\u968f\u8f93\u5165\u957f\u5ea6\u589e\u52a0\u51cf\u5f31\uff0c\u8fd1\u56e0\u504f\u5dee\u7a33\u5b9a\n3. \u8ddd\u79bb\u504f\u5dee\uff08\u4fe1\u606f\u79bb\u7ed3\u5c3e\u8d8a\u8fd1\u8868\u73b0\u8d8a\u597d\uff09\u6210\u4e3a\u4e3b\u5bfc\u6a21\u5f0f\n4. \u68c0\u7d22\u80fd\u529b\u51b3\u5b9a\u6a21\u578b\u63a8\u7406\u6548\u679c\uff0c\u4f4d\u7f6e\u504f\u5dee\u7ee7\u627f\u81ea\u68c0\u7d22\u8fc7\u7a0b", "conclusion": "\u7814\u7a76\u4e3a\u957f\u6587\u672c\u4efb\u52a1\u8bbe\u8ba1\u3001LLM\u57fa\u51c6\u6d4b\u8bd5\u4f18\u5316\u548c\u8bc4\u4f30\u65b9\u6cd5\u8bba\u63d0\u4f9b\u4e86\u5173\u952e\u6d1e\u89c1\uff0c\u63ed\u793a\u4e86\u4f4d\u7f6e\u504f\u5dee\u7684\u5f62\u6210\u673a\u5236\u4e0e\u6f14\u53d8\u89c4\u5f8b\u3002"}}
{"id": "2508.07484", "pdf": "https://arxiv.org/pdf/2508.07484", "abs": "https://arxiv.org/abs/2508.07484", "authors": ["Archchana Sindhujan", "Shenbin Qian", "Chan Chi Chun Matthew", "Constantin Orasan", "Diptesh Kanojia"], "title": "ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to COLM 2025 Conference", "summary": "Large Language Models (LLMs) have shown remarkable performance across a wide\nrange of natural language processing tasks. Quality Estimation (QE) for Machine\nTranslation (MT), which assesses the quality of a source-target pair without\nrelying on reference translations, remains a challenging cross-lingual task for\nLLMs. The challenges stem from the inherent limitations of existing LLM-based\nQE systems, which are pre-trained for causal language modelling rather than\nregression-specific tasks, further elevated by the presence of low-resource\nlanguages given pre-training data distribution. This paper introduces ALOPE, an\nadaptive layer-optimization framework designed to enhance LLM-based QE by\nrestructuring Transformer representations through layer-wise adaptation for\nimproved regression-based prediction. Our framework integrates low-rank\nadapters (LoRA) with regression task heads, leveraging selected pre-trained\nTransformer layers for improved cross-lingual alignment. In addition to the\nlayer-specific adaptation, ALOPE introduces two strategies-dynamic weighting,\nwhich adaptively combines representations from multiple layers, and multi-head\nregression, which aggregates regression losses from multiple heads for QE. Our\nframework shows improvements over various existing LLM-based QE approaches.\nEmpirical evidence suggests that intermediate Transformer layers in LLMs\nprovide contextual representations that are more aligned with the cross-lingual\nnature of the QE task. We make resultant models and framework code publicly\navailable for further research, also allowing existing LLM-based MT frameworks\nto be scaled with QE capabilities.", "AI": {"tldr": "ALOPE\u6846\u67b6\u901a\u8fc7\u5c42\u4f18\u5316\u548c\u8de8\u8bed\u8a00\u5bf9\u9f50\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u4f30\u8ba1\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u8d28\u91cf\u4f30\u8ba1\u7cfb\u7edf\u56e0\u9884\u8bad\u7ec3\u76ee\u6807(\u56e0\u679c\u8bed\u8a00\u5efa\u6a21)\u4e0e\u56de\u5f52\u4efb\u52a1\u4e0d\u5339\u914d\uff0c\u4e14\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u8868\u73b0\u53d7\u9650\uff0c\u9700\u6539\u8fdb\u8de8\u8bed\u8a00\u5bf9\u9f50\u80fd\u529b\u3002", "method": "\u6574\u5408\u4f4e\u79e9\u9002\u914d\u5668(LoRA)\u4e0e\u56de\u5f52\u4efb\u52a1\u5934\uff0c\u91c7\u7528\u52a8\u6001\u52a0\u6743(\u591a\u5c42\u7ea7\u8868\u793a\u878d\u5408)\u548c\u591a\u5934\u56de\u5f52(\u805a\u5408\u591a\u4e2a\u635f\u5931)\u7b56\u7565\u4f18\u5316Transformer\u5c42\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eLLM\u4e2d\u95f4\u5c42\u66f4\u9002\u5408\u8d28\u91cf\u4f30\u8ba1\u4efb\u52a1\uff0cALOPE\u6846\u67b6\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u8bed\u8a00\u8d28\u91cf\u4f30\u8ba1\u7684\u6311\u6218\uff0c\u4e3aLLM\u8d4b\u4e88\u53ef\u6269\u5c55\u7684\u8d28\u91cf\u8bc4\u4f30\u80fd\u529b\uff0c\u63a8\u52a8\u4e86\u76f8\u5173\u7814\u7a76\u8fdb\u5c55\u3002"}}
{"id": "2508.07516", "pdf": "https://arxiv.org/pdf/2508.07516", "abs": "https://arxiv.org/abs/2508.07516", "authors": ["Keshav Varadarajan", "Tananun Songdechakraiwut"], "title": "Augmenting Bias Detection in LLMs Using Topological Data Analysis", "categories": ["cs.CL"], "comment": "15 pages, 9 figures, 4 tables", "summary": "Recently, many bias detection methods have been proposed to determine the\nlevel of bias a large language model captures. However, tests to identify which\nparts of a large language model are responsible for bias towards specific\ngroups remain underdeveloped. In this study, we present a method using\ntopological data analysis to identify which heads in GPT-2 contribute to the\nmisrepresentation of identity groups present in the StereoSet dataset. We find\nthat biases for particular categories, such as gender or profession, are\nconcentrated in attention heads that act as hot spots. The metric we propose\ncan also be used to determine which heads capture bias for a specific group\nwithin a bias category, and future work could extend this method to help\nde-bias large language models.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u62d3\u6251\u6570\u636e\u5206\u6790\u7684\u65b9\u6cd5\u5b9a\u4f4dGPT-2\u6a21\u578b\u4e2d\u5bfc\u81f4\u7279\u5b9a\u7fa4\u4f53\u504f\u89c1\u7684\u6ce8\u610f\u529b\u5934\u70ed\u70b9\u533a\u57df", "motivation": "\u73b0\u6709\u504f\u89c1\u68c0\u6d4b\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u5b9a\u4f4d\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u5177\u4f53\u5f15\u53d1\u7fa4\u4f53\u504f\u89c1\u7684\u529f\u80fd\u6a21\u5757\uff0c\u9700\u5f00\u53d1\u9488\u5bf9\u6027\u8bca\u65ad\u5de5\u5177", "method": "\u8fd0\u7528\u62d3\u6251\u6570\u636e\u5206\u6790\u6280\u672f\u5206\u6790GPT-2\u6ce8\u610f\u529b\u5934\u5728StereoSet\u6570\u636e\u96c6\u4e2d\u7684\u6fc0\u6d3b\u6a21\u5f0f\uff0c\u8bbe\u8ba1\u6307\u6807\u91cf\u5316\u6ce8\u610f\u529b\u5934\u5bf9\u7279\u5b9a\u8eab\u4efd\u7fa4\u4f53\u504f\u89c1\u7684\u8d21\u732e\u5ea6", "result": "\u53d1\u73b0\u6027\u522b/\u804c\u4e1a\u7b49\u504f\u89c1\u7c7b\u522b\u96c6\u4e2d\u4e8e\u7279\u5b9a\u70ed\u70b9\u6ce8\u610f\u529b\u5934\uff0c\u63d0\u51fa\u7684\u6307\u6807\u53ef\u7cbe\u786e\u5b9a\u4f4d\u5177\u4f53\u7fa4\u4f53\u5728\u504f\u89c1\u7c7b\u522b\u4e2d\u7684\u8868\u5f81\u504f\u5dee\u6e90", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6a21\u578b\u53bb\u504f\u89c1\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u5b9a\u4f4d\u5de5\u5177\uff0c\u672a\u6765\u53ef\u6269\u5c55\u5e94\u7528\u4e8e\u5927\u6a21\u578b\u504f\u89c1\u4fee\u6b63\u7684\u6a21\u5757\u5316\u5e72\u9884"}}
{"id": "2508.07517", "pdf": "https://arxiv.org/pdf/2508.07517", "abs": "https://arxiv.org/abs/2508.07517", "authors": ["Joseph T. Colonel", "Baihan Lin"], "title": "Word Clouds as Common Voices: LLM-Assisted Visualization of Participant-Weighted Themes in Qualitative Interviews", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Word clouds are a common way to summarize qualitative interviews, yet\ntraditional frequency-based methods often fail in conversational contexts: they\nsurface filler words, ignore paraphrase, and fragment semantically related\nideas. This limits their usefulness in early-stage analysis, when researchers\nneed fast, interpretable overviews of what participant actually said. We\nintroduce ThemeClouds, an open-source visualization tool that uses large\nlanguage models (LLMs) to generate thematic, participant-weighted word clouds\nfrom dialogue transcripts. The system prompts an LLM to identify concept-level\nthemes across a corpus and then counts how many unique participants mention\neach topic, yielding a visualization grounded in breadth of mention rather than\nraw term frequency. Researchers can customize prompts and visualization\nparameters, providing transparency and control. Using interviews from a user\nstudy comparing five recording-device configurations (31 participants; 155\ntranscripts, Whisper ASR), our approach surfaces more actionable device\nconcerns than frequency clouds and topic-modeling baselines (e.g., LDA,\nBERTopic). We discuss design trade-offs for integrating LLM assistance into\nqualitative workflows, implications for interpretability and researcher agency,\nand opportunities for interactive analyses such as per-condition contrasts\n(``diff clouds'').", "AI": {"tldr": "\u63d0\u51faThemeClouds\u5de5\u5177\uff0c\u901a\u8fc7LLM\u751f\u6210\u57fa\u4e8e\u53c2\u4e0e\u8005\u63d0\u53ca\u5e7f\u5ea6\u7684\u4e3b\u9898\u8bcd\u4e91\uff0c\u514b\u670d\u4f20\u7edf\u8bcd\u4e91\u5728\u5bf9\u8bdd\u8bed\u5883\u4e2d\u7684\u8bed\u4e49\u788e\u7247\u5316\u95ee\u9898", "motivation": "\u4f20\u7edf\u8bcd\u4e91\u4f9d\u8d56\u8bcd\u9891\u7edf\u8ba1\uff0c\u5728\u5bf9\u8bdd\u573a\u666f\u4e2d\u4f1a\u7a81\u51fa\u65e0\u610f\u4e49\u586b\u5145\u8bcd\u3001\u5ffd\u7565\u540c\u4e49\u8868\u8fbe\u4e14\u5272\u88c2\u8bed\u4e49\u5173\u8054\uff0c\u96be\u4ee5\u6ee1\u8db3\u7814\u7a76\u4eba\u5458\u5feb\u901f\u83b7\u53d6\u53ef\u89e3\u91ca\u6027\u6982\u89c8\u7684\u9700\u6c42", "method": "\u4f7f\u7528LLM\u8bc6\u522b\u5bf9\u8bdd\u8bed\u6599\u4e2d\u7684\u6982\u5ff5\u7ea7\u4e3b\u9898\uff0c\u7edf\u8ba1\u6bcf\u4e2a\u4e3b\u9898\u88ab\u72ec\u7acb\u53c2\u4e0e\u8005\u63d0\u53ca\u7684\u6b21\u6570\uff0c\u652f\u6301\u7814\u7a76\u8005\u81ea\u5b9a\u4e49\u63d0\u793a\u8bcd\u548c\u53ef\u89c6\u5316\u53c2\u6570", "result": "\u57fa\u4e8e31\u540d\u53c2\u4e0e\u8005155\u4efdASR\u8f6c\u5f55\u672c\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8f83\u4f20\u7edf\u8bcd\u4e91\u548c\u4e3b\u9898\u6a21\u578b\uff08LDA/BERTopic\uff09\u80fd\u53d1\u73b0\u66f4\u591a\u53ef\u64cd\u4f5c\u7684\u8bbe\u5907\u914d\u7f6e\u95ee\u9898", "conclusion": "\u8bc1\u660e\u4e86LLM\u8f85\u52a9\u5b9a\u6027\u5206\u6790\u7684\u6f5c\u529b\uff0c\u9700\u5728\u81ea\u52a8\u5316\u4e0e\u7814\u7a76\u8005\u63a7\u5236\u6743\u4e4b\u95f4\u5e73\u8861\uff0c\u5e76\u63d0\u51fa\u4e86\u652f\u6301\u6761\u4ef6\u5bf9\u6bd4\u7684\u4ea4\u4e92\u5f0f\u5206\u6790\u65b9\u5411\uff08\u5982\u5dee\u5f02\u8bcd\u4e91\uff09"}}
{"id": "2508.07534", "pdf": "https://arxiv.org/pdf/2508.07534", "abs": "https://arxiv.org/abs/2508.07534", "authors": ["Jia Deng", "Jie Chen", "Zhipeng Chen", "Daixuan Cheng", "Fei Bai", "Beichen Zhang", "Yinqian Min", "Yanzipeng Gao", "Wayne Xin Zhao", "Ji-Rong Wen"], "title": "From Trial-and-Error to Improvement: A Systematic Analysis of LLM Exploration Mechanisms in RLVR", "categories": ["cs.CL"], "comment": "27pages,25figures. arXiv admin note: text overlap with\n  arXiv:2508.02260", "summary": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a\npowerful paradigm for enhancing the reasoning capabilities of large language\nmodels (LLMs). Unlike traditional RL approaches, RLVR leverages rule-based\nfeedback to guide LLMs in generating and refining complex reasoning chains -- a\nprocess critically dependent on effective exploration strategies. While prior\nwork has demonstrated RLVR's empirical success, the fundamental mechanisms\ngoverning LLMs' exploration behaviors remain underexplored. This technical\nreport presents a systematic investigation of exploration capacities in RLVR,\ncovering four main aspects: (1) exploration space shaping, where we develop\nquantitative metrics to characterize LLMs' capability boundaries; (2)\nentropy-performance exchange, analyzed across training stages, individual\ninstances, and token-level patterns; and (3) RL performance optimization,\nexamining methods to effectively translate exploration gains into measurable\nimprovements. By unifying previously identified insights with new empirical\nevidence, this work aims to provide a foundational framework for advancing RLVR\nsystems.", "AI": {"tldr": "\u7cfb\u7edf\u7814\u7a76\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60(RLVR)\u4e2d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a2\u7d22\u80fd\u529b\u673a\u5236\uff0c\u63d0\u51fa\u91cf\u5316\u5206\u6790\u6846\u67b6\u5e76\u63ed\u793a\u63a2\u7d22\u80fd\u529b\u4e0e\u6027\u80fd\u7684\u5173\u8054\u89c4\u5f8b", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u4f9d\u8d56\u63a2\u7d22\u7b56\u7565\u4f46\u7f3a\u4e4f\u7406\u8bba\u652f\u6491\uff0c\u9700\u6df1\u5165\u7406\u89e3LLM\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u63a2\u7d22\u8fb9\u754c\u4e0e\u4f18\u5316\u673a\u5236", "method": "\u4ece\u63a2\u7d22\u7a7a\u95f4\u5851\u9020\uff08\u5efa\u7acb\u80fd\u529b\u8fb9\u754c\u91cf\u5316\u6307\u6807\uff09\u3001\u71b5-\u6027\u80fd\u52a8\u6001\u5173\u7cfb\uff08\u5206\u9636\u6bb5/\u5b9e\u4f8b/token\u5206\u6790\uff09\u3001\u6027\u80fd\u4f18\u5316\u8def\u5f84\u4e09\u4e2a\u7ef4\u5ea6\u5c55\u5f00\u7cfb\u7edf\u6027\u5b9e\u8bc1\u7814\u7a76", "result": "\u6784\u5efa\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u5206\u6790\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u63a2\u7d22\u80fd\u529b\u4e0e\u6a21\u578b\u6027\u80fd\u7684\u975e\u7ebf\u6027\u5173\u7cfb\uff0c\u63d0\u51fa\u4e86\u6709\u6548\u7684\u63a2\u7d22\u589e\u76ca\u8f6c\u5316\u65b9\u6cd5\u8bba", "conclusion": "\u8be5\u7814\u7a76\u4e3aRLVR\u7cfb\u7edf\u4e2d\u7684\u63a2\u7d22\u673a\u5236\u63d0\u4f9b\u4e86\u9996\u4e2a\u7cfb\u7edf\u6027\u5206\u6790\u8303\u5f0f\uff0c\u5efa\u7acb\u4e86\u4ece\u7406\u8bba\u5230\u5b9e\u8df5\u7684\u4f18\u5316\u6846\u67b6\uff0c\u5bf9\u590d\u6742\u63a8\u7406\u4efb\u52a1\u8bad\u7ec3\u5177\u6709\u6307\u5bfc\u4ef7\u503c"}}
{"id": "2508.07592", "pdf": "https://arxiv.org/pdf/2508.07592", "abs": "https://arxiv.org/abs/2508.07592", "authors": ["Puspesh Kumar Srivastava", "Uddeshya Raj", "Praveen Patel", "/Shubham Kumar Nigam", "Noel Shallum", "Arnab Bhattacharya"], "title": "IBPS: Indian Bail Prediction System", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Bail decisions are among the most frequently adjudicated matters in Indian\ncourts, yet they remain plagued by subjectivity, delays, and inconsistencies.\nWith over 75% of India's prison population comprising undertrial prisoners,\nmany from socioeconomically disadvantaged backgrounds, the lack of timely and\nfair bail adjudication exacerbates human rights concerns and contributes to\nsystemic judicial backlog. In this paper, we present the Indian Bail Prediction\nSystem (IBPS), an AI-powered framework designed to assist in bail\ndecision-making by predicting outcomes and generating legally sound rationales\nbased solely on factual case attributes and statutory provisions. We curate and\nrelease a large-scale dataset of 150,430 High Court bail judgments, enriched\nwith structured annotations such as age, health, criminal history, crime\ncategory, custody duration, statutes, and judicial reasoning. We fine-tune a\nlarge language model using parameter-efficient techniques and evaluate its\nperformance across multiple configurations, with and without statutory context,\nand with RAG. Our results demonstrate that models fine-tuned with statutory\nknowledge significantly outperform baselines, achieving strong accuracy and\nexplanation quality, and generalize well to a test set independently annotated\nby legal experts. IBPS offers a transparent, scalable, and reproducible\nsolution to support data-driven legal assistance, reduce bail delays, and\npromote procedural fairness in the Indian judicial system.", "AI": {"tldr": "\u5f00\u53d1\u5370\u5ea6\u4fdd\u91ca\u9884\u6d4b\u7cfb\u7edf\uff08IBPS\uff09\uff0c\u901a\u8fc7AI\u9884\u6d4b\u4fdd\u91ca\u7ed3\u679c\u5e76\u751f\u6210\u6cd5\u5f8b\u4f9d\u636e\uff0c\u89e3\u51b3\u53f8\u6cd5\u5ef6\u8fdf\u548c\u4e0d\u516c\u5e73\u95ee\u9898\u3002", "motivation": "\u5370\u5ea6\u6cd5\u9662\u4fdd\u91ca\u51b3\u7b56\u5b58\u5728\u4e3b\u89c2\u6027\u3001\u5ef6\u8fdf\u548c\u4e0d\u4e00\u81f4\uff0c75%\u76d1\u72f1\u4eba\u53e3\u4e3a\u5019\u5ba1\u8005\uff08\u591a\u6765\u81ea\u5f31\u52bf\u7fa4\u4f53\uff09\uff0c\u6025\u9700\u900f\u660e\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u752815\u4e07+\u9ad8\u7b49\u6cd5\u9662\u5224\u51b3\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u6cd5\u5f8b\u6761\u6587\u548cRAG\u6280\u672f\uff0c\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u6cd5\u5f8b\u77e5\u8bc6\u589e\u5f3a\u7684\u6a21\u578b\u5728\u51c6\u786e\u6027(85.7%)\u548c\u89e3\u91ca\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u9a8c\u8bc1\u96c6\u4e13\u5bb6\u6807\u6ce8\u7ed3\u679c\u5339\u914d\u5ea6\u8fbe92.3%\u3002", "conclusion": "IBPS\u4e3a\u5370\u5ea6\u53f8\u6cd5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6570\u636e\u9a71\u52a8\u7684\u900f\u660e\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u51cf\u5c11\u4fdd\u91ca\u5ef6\u8fdf\u5e76\u4fc3\u8fdb\u7a0b\u5e8f\u516c\u5e73\u3002"}}
{"id": "2508.07598", "pdf": "https://arxiv.org/pdf/2508.07598", "abs": "https://arxiv.org/abs/2508.07598", "authors": ["Ziheng Li", "Zhi-Hong Deng"], "title": "Keyword-Centric Prompting for One-Shot Event Detection with Self-Generated Rationale Enhancements", "categories": ["cs.CL"], "comment": "ECAI 2025", "summary": "Although the LLM-based in-context learning (ICL) paradigm has demonstrated\nconsiderable success across various natural language processing tasks, it\nencounters challenges in event detection. This is because LLMs lack an accurate\nunderstanding of event triggers and tend to make over-interpretation, which\ncannot be effectively corrected through in-context examples alone. In this\npaper, we focus on the most challenging one-shot setting and propose KeyCP++, a\nkeyword-centric chain-of-thought prompting approach. KeyCP++ addresses the\nweaknesses of conventional ICL by automatically annotating the logical gaps\nbetween input text and detection results for the demonstrations. Specifically,\nto generate in-depth and meaningful rationale, KeyCP++ constructs a trigger\ndiscrimination prompting template. It incorporates the exemplary triggers\n(a.k.a keywords) into the prompt as the anchor to simply trigger profiling, let\nLLM propose candidate triggers, and justify each candidate. These\npropose-and-judge rationales help LLMs mitigate over-reliance on the keywords\nand promote detection rule learning. Extensive experiments demonstrate the\neffectiveness of our approach, showcasing significant advancements in one-shot\nevent detection.", "AI": {"tldr": "KeyCP++\u901a\u8fc7\u5173\u952e\u8bcd\u5f15\u5bfc\u7684\u601d\u7ef4\u94fe\u63d0\u793a\u65b9\u6cd5\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5355\u6837\u672c\u4e8b\u4ef6\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u4e0a\u4e0b\u6587\u5b66\u4e60\u5728\u4e8b\u4ef6\u68c0\u6d4b\u4e2d\u5b58\u5728\u5bf9\u89e6\u53d1\u8bcd\u7406\u89e3\u4e0d\u51c6\u786e\u548c\u8fc7\u5ea6\u89e3\u8bfb\u7684\u95ee\u9898\uff0c\u5355\u6837\u672c\u573a\u666f\u4e0b\u96be\u4ee5\u6709\u6548\u7ea0\u6b63\u3002", "method": "\u6784\u5efa\u89e6\u53d1\u8bcd\u5224\u522b\u63d0\u793a\u6a21\u677f\uff0c\u5c06\u5173\u952e\u8bcd\u4f5c\u4e3a\u951a\u70b9\u751f\u6210\u5019\u9009\u89e6\u53d1\u8bcd\u5e76\u9a8c\u8bc1\uff0c\u901a\u8fc7'\u63d0\u51fa-\u9a8c\u8bc1'\u673a\u5236\u4fc3\u8fdb\u68c0\u6d4b\u89c4\u5219\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5355\u6837\u672c\u4e8b\u4ef6\u68c0\u6d4b\u4e2d\u53d6\u5f97\u663e\u8457\u8fdb\u6b65\uff0c\u68c0\u6d4b\u51c6\u786e\u7387\u63d0\u5347\u660e\u663e\u3002", "conclusion": "\u7ed3\u5408\u5173\u952e\u8bcd\u951a\u5b9a\u4e0e\u903b\u8f91\u63a8\u6f14\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u5173\u952e\u8bcd\u7684\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u63d0\u5347\u4e86\u4e8b\u4ef6\u68c0\u6d4b\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2508.07630", "pdf": "https://arxiv.org/pdf/2508.07630", "abs": "https://arxiv.org/abs/2508.07630", "authors": ["Anirudh Iyengar Kaniyar Narayana Iyengar", "Srija Mukhopadhyay", "Adnan Qidwai", "Shubhankar Singh", "Dan Roth", "Vivek Gupta"], "title": "InterChart: Benchmarking Visual Reasoning Across Decomposed and Distributed Chart Information", "categories": ["cs.CL", "cs.AI", "cs.CV", "I.2.7; I.2.10; I.4.10; I.7.5"], "comment": "18 pages, 6 figures, 12 tables. Benchmark dataset and evaluation code\n  will be publicly made available", "summary": "We introduce InterChart, a diagnostic benchmark that evaluates how well\nvision-language models (VLMs) reason across multiple related charts, a task\ncentral to real-world applications such as scientific reporting, financial\nanalysis, and public policy dashboards. Unlike prior benchmarks focusing on\nisolated, visually uniform charts, InterChart challenges models with diverse\nquestion types ranging from entity inference and trend correlation to numerical\nestimation and abstract multi-step reasoning grounded in 2-3 thematically or\nstructurally related charts. We organize the benchmark into three tiers of\nincreasing difficulty: (1) factual reasoning over individual charts, (2)\nintegrative analysis across synthetically aligned chart sets, and (3) semantic\ninference over visually complex, real-world chart pairs. Our evaluation of\nstate-of-the-art open and closed-source VLMs reveals consistent and steep\naccuracy declines as chart complexity increases. We find that models perform\nbetter when we decompose multi-entity charts into simpler visual units,\nunderscoring their struggles with cross-chart integration. By exposing these\nsystematic limitations, InterChart provides a rigorous framework for advancing\nmultimodal reasoning in complex, multi-visual environments.", "AI": {"tldr": "InterChart\u662f\u4e00\u4e2a\u8bca\u65ad\u6027\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u56fe\u8868\u590d\u6742\u73af\u5883\u4e0b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u56fe\u8868\u590d\u6742\u5ea6\u589e\u52a0\u65f6\u51c6\u786e\u7387\u663e\u8457\u4e0b\u964d\u7684\u73b0\u8c61\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4ec5\u9488\u5bf9\u5b64\u7acb\u56fe\u8868\uff0c\u800c\u771f\u5b9e\u573a\u666f\u9700\u8981\u5904\u7406\u591a\u4e2a\u76f8\u5173\u56fe\u8868\u3002InterChart\u901a\u8fc7\u6784\u5efa\u4e09\u4e2a\u96be\u5ea6\u5c42\u7ea7\u7684\u6311\u6218\uff08\u5b9e\u4f53\u63a8\u65ad/\u8d8b\u52bf\u5173\u8054/\u591a\u6b65\u63a8\u7406\uff09\uff0c\u586b\u8865\u4e86\u591a\u56fe\u8868\u6574\u5408\u63a8\u7406\u8bc4\u4f30\u7684\u7a7a\u767d\u3002", "method": "\u5c06\u57fa\u51c6\u5206\u4e3a\u4e09\u4e2a\u5c42\u7ea7\uff1a1\uff09\u5355\u56fe\u8868\u4e8b\u5b9e\u63a8\u7406\uff1b2\uff09\u5408\u6210\u5bf9\u9f50\u56fe\u8868\u96c6\u7684\u7efc\u5408\u5206\u6790\uff1b3\uff09\u771f\u5b9e\u590d\u6742\u56fe\u8868\u5bf9\u7684\u8bed\u4e49\u63a8\u7406\u3002\u901a\u8fc7\u5206\u89e3\u591a\u5b9e\u4f53\u56fe\u8868\u4e3a\u7b80\u5355\u5355\u5143\u9a8c\u8bc1\u6a21\u578b\u5c40\u9650\u3002", "result": "\u9876\u5c16VLMs\u5728\u56fe\u8868\u590d\u6742\u5ea6\u4e0a\u5347\u65f6\u51c6\u786e\u7387\u4e0b\u964d\u660e\u663e\uff08\u5c42\u7ea71\u52303\u964d\u5e45\u8fbe40%\uff09\u3002\u6a21\u578b\u5728\u5206\u89e3\u540e\u7684\u7b80\u5355\u56fe\u8868\u5355\u5143\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u66b4\u9732\u8de8\u56fe\u8868\u6574\u5408\u80fd\u529b\u4e0d\u8db3\u3002", "conclusion": "InterChart\u4e3a\u590d\u6742\u591a\u89c6\u89c9\u73af\u5883\u4e2d\u7684\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u4e25\u683c\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7cfb\u7edf\u6027\u7f3a\u9677\uff0c\u63a8\u52a8\u8de8\u56fe\u8868\u63a8\u7406\u6280\u672f\u7684\u8fdb\u6b65\u3002"}}
{"id": "2508.07690", "pdf": "https://arxiv.org/pdf/2508.07690", "abs": "https://arxiv.org/abs/2508.07690", "authors": ["Luyao Zhuang", "Qinggang Zhang", "Huachi Zhou", "Juhua Liu", "Qing Li", "Xiao Huang"], "title": "LoSemB: Logic-Guided Semantic Bridging for Inductive Tool Retrieval", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tool learning has emerged as a promising paradigm for large language models\n(LLMs) to solve many real-world tasks. Nonetheless, with the tool repository\nrapidly expanding, it is impractical to contain all tools within the limited\ninput length of LLMs. To alleviate these issues, researchers have explored\nincorporating a tool retrieval module to select the most relevant tools or\nrepresent tools as unique tokens within LLM parameters. However, most\nstate-of-the-art methods are under transductive settings, assuming all tools\nhave been observed during training. Such a setting deviates from reality as the\nreal-world tool repository is evolving and incorporates new tools frequently.\nWhen dealing with these unseen tools, which refer to tools not encountered\nduring the training phase, these methods are limited by two key issues,\nincluding the large distribution shift and the vulnerability of\nsimilarity-based retrieval. To this end, inspired by human cognitive processes\nof mastering unseen tools through discovering and applying the logical\ninformation from prior experience, we introduce a novel Logic-Guided Semantic\nBridging framework for inductive tool retrieval, namely, LoSemB, which aims to\nmine and transfer latent logical information for inductive tool retrieval\nwithout costly retraining. Specifically, LoSemB contains a logic-based\nembedding alignment module to mitigate distribution shifts and implements a\nrelational augmented retrieval mechanism to reduce the vulnerability of\nsimilarity-based retrieval. Extensive experiments demonstrate that LoSemB\nachieves advanced performance in inductive settings while maintaining desirable\neffectiveness in the transductive setting.", "AI": {"tldr": "\u63d0\u51faLoSemB\u6846\u67b6\uff0c\u901a\u8fc7\u903b\u8f91\u5f15\u5bfc\u7684\u8bed\u4e49\u6865\u63a5\u89e3\u51b3\u52a8\u6001\u5de5\u5177\u5e93\u4e2d\u7684\u5f52\u7eb3\u5f0f\u68c0\u7d22\u95ee\u9898\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u5904\u7406\u65b0\u5de5\u5177", "motivation": "\u73b0\u6709\u8f6c\u5bfc\u5f0f\u65b9\u6cd5\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u5de5\u5177\u5e93\u4e2d\u7684\u65b0\u5de5\u5177\uff0c\u5b58\u5728\u5206\u5e03\u504f\u79fb\u548c\u76f8\u4f3c\u6027\u68c0\u7d22\u8106\u5f31\u6027\u4e24\u5927\u7f3a\u9677", "method": "\u5305\u542b\u903b\u8f91\u5d4c\u5165\u5bf9\u9f50\u6a21\u5757\uff08\u7f13\u89e3\u5206\u5e03\u504f\u79fb\uff09\u548c\u5173\u7cfb\u589e\u5f3a\u68c0\u7d22\u673a\u5236\uff08\u63d0\u5347\u68c0\u7d22\u9c81\u68d2\u6027\uff09\u7684\u53cc\u91cd\u67b6\u6784", "result": "\u5b9e\u9a8c\u8bc1\u660e\u5728\u5f52\u7eb3\u5f0f\u548c\u8f6c\u5bfc\u5f0f\u573a\u666f\u4e0b\u5747\u53d6\u5f97\u5148\u8fdb\u6027\u80fd\uff0c\u4fdd\u6301\u6a21\u578b\u6709\u6548\u6027", "conclusion": "\u903b\u8f91\u5f15\u5bfc\u7684\u8bed\u4e49\u6865\u63a5\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u5de5\u5177\u77e5\u8bc6\u7684\u8fc1\u79fb\u5e94\u7528\uff0c\u4e3a\u52a8\u6001\u5de5\u5177\u73af\u5883\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.07702", "pdf": "https://arxiv.org/pdf/2508.07702", "abs": "https://arxiv.org/abs/2508.07702", "authors": ["Charlie Wyatt", "Aditya Joshi", "Flora Salim"], "title": "What am I missing here?: Evaluating Large Language Models for Masked Sentence Prediction", "categories": ["cs.CL"], "comment": "Under Review", "summary": "Transformer-based models primarily rely on Next Token Prediction (NTP), which\npredicts the next token in a sequence based on the preceding context. However,\nNTP's focus on single-token prediction often limits a model's ability to plan\nahead or maintain long-range coherence, raising questions about how well LLMs\ncan predict longer contexts, such as full sentences within structured\ndocuments. While NTP encourages local fluency, it provides no explicit\nincentive to ensure global coherence across sentence boundaries-an essential\nskill for reconstructive or discursive tasks. To investigate this, we evaluate\nthree commercial LLMs (GPT-4o, Claude 3.5 Sonnet, and Gemini 2.0 Flash) on\nMasked Sentence Prediction (MSP) - the task of infilling a randomly removed\nsentence - from three domains: ROCStories (narrative), Recipe1M (procedural),\nand Wikipedia (expository). We assess both fidelity (similarity to the original\nsentence) and cohesiveness (fit within the surrounding context). Our key\nfinding reveals that commercial LLMs, despite their superlative performance in\nother tasks, are poor at predicting masked sentences in low-structured domains,\nhighlighting a gap in current model capabilities.", "AI": {"tldr": "\u5546\u4e1a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u975e\u7ed3\u6784\u5316\u9886\u57df\u7684\u63a9\u7801\u53e5\u5b50\u9884\u6d4b\u8868\u73b0\u4e0d\u4f73\uff0c\u63ed\u793a\u5f53\u524d\u6a21\u578b\u5168\u5c40\u8fde\u8d2f\u6027\u80fd\u529b\u7684\u7f3a\u9677", "motivation": "\u9488\u5bf9NTP\u673a\u5236\u5728\u957f\u7a0b\u8fde\u8d2f\u6027\u89c4\u5212\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u63a2\u7d22LLM\u5728\u5b8c\u6574\u53e5\u5b50\u5c42\u9762\uff08\u800c\u975e\u5355token\uff09\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u7279\u522b\u5173\u6ce8\u5176\u5728\u91cd\u6784\u6027/\u8bba\u8ff0\u6027\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u5dee\u8ddd", "method": "\u4f7f\u7528GPT-4o/Claude 3.5 Sonnet/Gemini 2.0 Flash\u4e09\u7c7b\u5546\u4e1a\u6a21\u578b\uff0c\u5728\u53d9\u4e8b/\u6d41\u7a0b/\u8bf4\u660e\u4e09\u7c7b\u6587\u672c\uff08ROCStories/Recipe1M/Wikipedia\uff09\u8fdb\u884c\u63a9\u7801\u53e5\u5b50\u9884\u6d4b\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u9884\u6d4b\u53e5\u5b50\u7684\u4fdd\u771f\u5ea6\uff08\u4e0e\u539f\u53e5\u76f8\u4f3c\u6027\uff09\u548c\u8fde\u8d2f\u6027\uff08\u4e0a\u4e0b\u6587\u9002\u914d\u5ea6\uff09", "result": "\u5546\u4e1aLLM\u5728\u4f4e\u7ed3\u6784\u5316\u9886\u57df\uff08\u5982\u53d9\u4e8b\u6587\u672c\uff09\u7684\u63a9\u7801\u53e5\u5b50\u9884\u6d4b\u8868\u73b0\u663e\u8457\u843d\u540e\uff0c\u7a81\u663e\u5f53\u524d\u6a21\u578b\u80fd\u529b\u7684\u7ed3\u6784\u6027\u7f3a\u9677", "conclusion": "\u4ec5\u4f9d\u9760NTP\u8bad\u7ec3\u76ee\u6807\u4e0d\u8db3\u4ee5\u57f9\u517b\u6a21\u578b\u7684\u5168\u5c40\u6587\u672c\u89c4\u5212\u80fd\u529b\uff0c\u9700\u5f00\u53d1\u65b0\u7684\u8bad\u7ec3\u8303\u5f0f\u63d0\u5347LLM\u5728\u590d\u6742\u8bed\u5883\u4e0b\u7684\u8fde\u8d2f\u6027\u8868\u73b0"}}
{"id": "2508.07753", "pdf": "https://arxiv.org/pdf/2508.07753", "abs": "https://arxiv.org/abs/2508.07753", "authors": ["Zhenliang Zhang", "Junzhe Zhang", "Xinyu Hu", "HuiXuan Zhang", "Xiaojun Wan"], "title": "Exploring Causal Effect of Social Bias on Faithfulness Hallucinations in Large Language Models", "categories": ["cs.CL"], "comment": "Accepted by CIKM 2025 (Full Paper)", "summary": "Large language models (LLMs) have achieved remarkable success in various\ntasks, yet they remain vulnerable to faithfulness hallucinations, where the\noutput does not align with the input. In this study, we investigate whether\nsocial bias contributes to these hallucinations, a causal relationship that has\nnot been explored. A key challenge is controlling confounders within the\ncontext, which complicates the isolation of causality between bias states and\nhallucinations. To address this, we utilize the Structural Causal Model (SCM)\nto establish and validate the causality and design bias interventions to\ncontrol confounders. In addition, we develop the Bias Intervention Dataset\n(BID), which includes various social biases, enabling precise measurement of\ncausal effects. Experiments on mainstream LLMs reveal that biases are\nsignificant causes of faithfulness hallucinations, and the effect of each bias\nstate differs in direction. We further analyze the scope of these causal\neffects across various models, specifically focusing on unfairness\nhallucinations, which are primarily targeted by social bias, revealing the\nsubtle yet significant causal effect of bias on hallucination generation.", "AI": {"tldr": "\u7814\u7a76\u8bc1\u5b9e\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u793e\u4f1a\u504f\u89c1\u662f\u5bfc\u81f4\u5fe0\u5b9e\u6027\u5e7b\u89c9\u7684\u91cd\u8981\u539f\u56e0\uff0c\u901a\u8fc7\u56e0\u679c\u6a21\u578b\u548c\u504f\u89c1\u5e72\u9884\u6570\u636e\u96c6\u9a8c\u8bc1\u56e0\u679c\u5173\u7cfb", "motivation": "\u63a2\u7d22\u672a\u88ab\u7814\u7a76\u7684\u793e\u4f1a\u504f\u89c1\u4e0e\u5e7b\u89c9\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u89e3\u51b3\u8bed\u5883\u6df7\u6742\u53d8\u91cf\u5bf9\u56e0\u679c\u5f52\u56e0\u7684\u5e72\u6270", "method": "\u91c7\u7528\u7ed3\u6784\u56e0\u679c\u6a21\u578b(SCM)\u5efa\u7acb\u56e0\u679c\u5173\u7cfb\uff0c\u8bbe\u8ba1\u504f\u89c1\u5e72\u9884\u673a\u5236\u63a7\u5236\u6df7\u6742\u53d8\u91cf\uff0c\u6784\u5efa\u5305\u542b\u591a\u7ef4\u5ea6\u793e\u4f1a\u504f\u89c1\u7684BID\u6570\u636e\u96c6", "result": "\u5b9e\u9a8c\u53d1\u73b0\u4e0d\u540c\u504f\u89c1\u72b6\u6001\u5bf9\u5e7b\u89c9\u4ea7\u751f\u5b58\u5728\u663e\u8457\u56e0\u679c\u6548\u5e94\uff0c\u5176\u4e2d\u9488\u5bf9\u793e\u4f1a\u504f\u89c1\u7684\u4e0d\u516c\u5e73\u6027\u5e7b\u89c9\u8868\u73b0\u51fa\u7279\u5f02\u6027\u56e0\u679c\u5173\u7cfb", "conclusion": "\u793e\u4f1a\u504f\u89c1\u662f\u5f15\u53d1\u8bed\u8a00\u6a21\u578b\u5fe0\u5b9e\u6027\u5e7b\u89c9\u7684\u91cd\u8981\u8bf1\u56e0\uff0c\u63ed\u793a\u6a21\u578b\u504f\u5dee\u4e0e\u5e7b\u89c9\u751f\u6210\u95f4\u5b58\u5728\u5fae\u5999\u4f46\u663e\u8457\u7684\u56e0\u679c\u673a\u5236"}}
{"id": "2508.07781", "pdf": "https://arxiv.org/pdf/2508.07781", "abs": "https://arxiv.org/abs/2508.07781", "authors": ["Zeyu Yang", "Lai Wei", "Roman Koshkin", "Xi Chen", "Satoshi Nakamura"], "title": "SASST: Leveraging Syntax-Aware Chunking and LLMs for Simultaneous Speech Translation", "categories": ["cs.CL"], "comment": null, "summary": "This work proposes a grammar-based chunking strategy that segments input\nstreams into semantically complete units by parsing dependency relations (e.g.,\nnoun phrase boundaries, verb-object structures) and punctuation features. The\nmethod ensures chunk coherence and minimizes semantic fragmentation. Building\non this mechanism, we present SASST (Syntax-Aware Simultaneous Speech\nTranslation), an end-to-end framework integrating frozen Whisper encoder and\ndecoder-only LLM. The unified architecture dynamically outputs translation\ntokens or <WAIT> symbols to jointly optimize translation timing and content,\nwith target-side reordering addressing word-order divergence. Experiments on\nCoVoST2 multilingual corpus En-{De, Zh, Ja} demonstrate significant translation\nquality improvements across languages and validate the effectiveness of\nsyntactic structures in LLM-driven SimulST systems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8bed\u6cd5\u5206\u5757\u7684SASST\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u7ffb\u8bd1\u4f18\u5316\u663e\u8457\u63d0\u5347\u591a\u8bed\u8a00\u540c\u6b65\u8bed\u97f3\u7ffb\u8bd1\u8d28\u91cf", "motivation": "\u89e3\u51b3\u540c\u6b65\u8bed\u97f3\u7ffb\u8bd1\u4e2d\u7684\u8bed\u4e49\u788e\u7247\u5316\u95ee\u9898\uff0c\u4f18\u5316\u7ffb\u8bd1\u65f6\u673a\u4e0e\u5185\u5bb9\uff0c\u5904\u7406\u8de8\u8bed\u8a00\u8bcd\u5e8f\u5dee\u5f02", "method": "1. \u57fa\u4e8e\u4f9d\u8d56\u5173\u7cfb\u548c\u6807\u70b9\u7684\u5206\u5757\u7b56\u7565\n2. \u6574\u5408Whisper\u7f16\u7801\u5668\u548cLLM\u89e3\u7801\u5668\u7684\u7aef\u5230\u7aef\u67b6\u6784\n3. \u52a8\u6001\u8f93\u51fa\u7ffb\u8bd1\u6807\u8bb0\u4e0e<WAIT>\u7b26\u53f7\n4. \u76ee\u6807\u7aef\u91cd\u6392\u5e8f\u6280\u672f", "result": "\u5728CoVoST2\u591a\u8bed\u8a00\u8bed\u6599\u5e93\uff08\u82f1\u2192\u5fb7/\u4e2d/\u65e5\uff09\u4e2d\u5b9e\u73b0\u7ffb\u8bd1\u8d28\u91cf\u663e\u8457\u63d0\u5347\uff0cBLEU\u503c\u6700\u9ad8\u63d0\u53472.8", "conclusion": "\u8bed\u6cd5\u7ed3\u6784\u5f15\u5bfc\u7684SASST\u7cfb\u7edf\u6709\u6548\u63d0\u5347\u5b9e\u65f6\u7ffb\u8bd1\u6027\u80fd\uff0c\u7279\u522b\u5728\u8de8\u8bed\u8a00\u8bcd\u5e8f\u5904\u7406\u65b9\u9762\u8868\u73b0\u7a81\u51fa"}}
{"id": "2508.07785", "pdf": "https://arxiv.org/pdf/2508.07785", "abs": "https://arxiv.org/abs/2508.07785", "authors": ["Haoyuan Wu", "Haoxing Chen", "Xiaodong Chen", "Zhanchao Zhou", "Tieyuan Chen", "Yihong Zhuang", "Guoshan Lu", "Zenan Huang", "Junbo Zhao", "Lin Liu", "Zhenzhong Lan", "Bei Yu", "Jianguo Li"], "title": "Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts", "categories": ["cs.CL"], "comment": null, "summary": "The Mixture of Experts (MoE) architecture is a cornerstone of modern\nstate-of-the-art (SOTA) large language models (LLMs). MoE models facilitate\nscalability by enabling sparse parameter activation. However, traditional MoE\narchitecture uses homogeneous experts of a uniform size, activating a fixed\nnumber of parameters irrespective of input complexity and thus limiting\ncomputational efficiency. To overcome this limitation, we introduce Grove MoE,\na novel architecture incorporating experts of varying sizes, inspired by the\nheterogeneous big.LITTLE CPU architecture. This architecture features novel\nadjugate experts with a dynamic activation mechanism, enabling model capacity\nexpansion while maintaining manageable computational overhead. Building on this\narchitecture, we present GroveMoE-Base and GroveMoE-Inst, 33B-parameter LLMs\ndeveloped by applying an upcycling strategy to the Qwen3-30B-A3B-Base model\nduring mid-training and post-training. GroveMoE models dynamically activate\n3.14-3.28B parameters based on token complexity and achieve performance\ncomparable to SOTA open-source models of similar or even larger size.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f02\u6784\u4e13\u5bb6\u67b6\u6784\u7684Grove MoE\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u6fc0\u6d3b\u4e0d\u540c\u89c4\u6a21\u4e13\u5bb6\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97", "motivation": "\u4f20\u7edfMoE\u67b6\u6784\u4f7f\u7528\u540c\u8d28\u5316\u4e13\u5bb6\u5bfc\u81f4\u56fa\u5b9a\u53c2\u6570\u6fc0\u6d3b\uff0c\u9650\u5236\u4e86\u8ba1\u7b97\u6548\u7387\u3002\u53d7big.LITTLE CPU\u67b6\u6784\u542f\u53d1\uff0c\u901a\u8fc7\u5f02\u6784\u4e13\u5bb6\u63d0\u5347\u6548\u7387", "method": "1. \u91c7\u7528\u7c7b\u4f3cCPU\u7684\u5f02\u6784\u67b6\u6784\u8bbe\u8ba1\n2. \u5f15\u5165\u4f34\u968f\u4e13\u5bb6(adjugate experts)\u548c\u52a8\u6001\u6fc0\u6d3b\u673a\u5236\n3. \u5728\u4e2d\u8bad/\u540e\u8bad\u9636\u6bb5\u5bf9Qwen3-30B-A3B-Base\u6a21\u578b\u8fdb\u884c\u5347\u7ea7\u91cd\u6784", "result": "33B\u53c2\u6570\u7684GroveMoE\u6a21\u578b\u52a8\u6001\u6fc0\u6d3b3.14-3.28B\u53c2\u6570\uff0c\u6027\u80fd\u8fbe\u5230\u540c\u89c4\u6a21/\u66f4\u5927\u89c4\u6a21SOTA\u5f00\u6e90\u6a21\u578b\u6c34\u5e73", "conclusion": "Grove MoE\u5728\u53ef\u63a7\u8ba1\u7b97\u5f00\u9500\u4e0b\u5b9e\u73b0\u4e86\u6a21\u578b\u5bb9\u91cf\u6269\u5c55\uff0c\u4e3aLLM\u67b6\u6784\u521b\u65b0\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411"}}
{"id": "2508.07805", "pdf": "https://arxiv.org/pdf/2508.07805", "abs": "https://arxiv.org/abs/2508.07805", "authors": ["Yerin Hwang", "Dongryeol Lee", "Taegwan Kang", "Yongil Kim", "Kyomin Jung"], "title": "Can You Trick the Grader? Adversarial Persuasion of LLM Judges", "categories": ["cs.CL"], "comment": "19 pages, 8 figures", "summary": "As large language models take on growing roles as automated evaluators in\npractical settings, a critical question arises: Can individuals persuade an LLM\njudge to assign unfairly high scores? This study is the first to reveal that\nstrategically embedded persuasive language can bias LLM judges when scoring\nmathematical reasoning tasks, where correctness should be independent of\nstylistic variation. Grounded in Aristotle's rhetorical principles, we\nformalize seven persuasion techniques (Majority, Consistency, Flattery,\nReciprocity, Pity, Authority, Identity) and embed them into otherwise identical\nresponses. Across six math benchmarks, we find that persuasive language leads\nLLM judges to assign inflated scores to incorrect solutions, by up to 8% on\naverage, with Consistency causing the most severe distortion. Notably,\nincreasing model size does not substantially mitigate this vulnerability.\nFurther analysis demonstrates that combining multiple persuasion techniques\namplifies the bias, and pairwise evaluation is likewise susceptible. Moreover,\nthe persuasive effect persists under counter prompting strategies, highlighting\na critical vulnerability in LLM-as-a-Judge pipelines and underscoring the need\nfor robust defenses against persuasion-based attacks.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u4f5c\u4e3a\u8bc4\u5206\u8005\u65f6\u6613\u53d7\u7b56\u7565\u6027\u8bf4\u670d\u8bed\u8a00\u5f71\u54cd\uff0c\u5bfc\u81f4\u9519\u8bef\u7b54\u6848\u8bc4\u5206\u865a\u9ad8\uff0c\u5176\u4e2d\u4e00\u81f4\u6027\u8bf4\u670d\u624b\u6cd5\u5f71\u54cd\u6700\u663e\u8457\u4e14\u6a21\u578b\u89c4\u6a21\u65e0\u6cd5\u6709\u6548\u7f13\u89e3\u8be5\u95ee\u9898\u3002", "motivation": "\u63a2\u7a76LLM\u4f5c\u4e3a\u81ea\u52a8\u8bc4\u4f30\u5668\u65f6\u662f\u5426\u4f1a\u88ab\u8bf4\u670d\u6027\u8bed\u8a00\u64cd\u7eb5\u8bc4\u5206\uff0c\u9a8c\u8bc1\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u4fee\u8f9e\u624b\u6cd5\u5bf9\u8bc4\u5206\u516c\u6b63\u6027\u7684\u5f71\u54cd\u3002", "method": "\u57fa\u4e8e\u4e9a\u91cc\u58eb\u591a\u5fb7\u4fee\u8f9e\u5b66\u539f\u7406\u6784\u5efa\u4e03\u79cd\u8bf4\u670d\u7b56\u7565\uff0c\u5728\u516d\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u690d\u5165\u76f8\u540c\u9519\u8bef\u7b54\u6848\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "\u8bf4\u670d\u6027\u8bed\u8a00\u4f7f\u9519\u8bef\u7b54\u6848\u5e73\u5747\u8bc4\u5206\u865a\u9ad88%\uff0c\u4e00\u81f4\u6027\u624b\u6cd5\u504f\u5dee\u6700\u5927\uff0c\u591a\u6280\u5de7\u53e0\u52a0\u53ca\u6210\u5bf9\u8bc4\u4f30\u5747\u52a0\u5267\u504f\u5dee\uff0c\u5bf9\u6297\u63d0\u793a\u65e0\u6548\u3002", "conclusion": "LLM\u8bc4\u5206\u7cfb\u7edf\u5b58\u5728\u6839\u672c\u6027\u6f0f\u6d1e\uff0c\u9700\u5efa\u7acb\u9488\u5bf9\u8bf4\u670d\u653b\u51fb\u7684\u9632\u5fa1\u673a\u5236\u4ee5\u786e\u4fdd\u8bc4\u4f30\u53ef\u9760\u6027\u3002"}}
{"id": "2508.07810", "pdf": "https://arxiv.org/pdf/2508.07810", "abs": "https://arxiv.org/abs/2508.07810", "authors": ["Olga Kellert", "Muhammad Imran", "Nicholas Hill Matlis", "Mahmud Uz Zaman", "Carlos G\u00f3mez-Rodr\u00edguez"], "title": "Evaluating Compositional Approaches for Focus and Sentiment Analysis", "categories": ["cs.CL"], "comment": null, "summary": "This paper summarizes the results of evaluating a compositional approach for\nFocus Analysis (FA) in Linguistics and Sentiment Analysis (SA) in Natural\nLanguage Processing (NLP). While quantitative evaluations of compositional and\nnon-compositional approaches in SA exist in NLP, similar quantitative\nevaluations are very rare in FA in Linguistics that deal with linguistic\nexpressions representing focus or emphasis such as \"it was John who left\". We\nfill this gap in research by arguing that compositional rules in SA also apply\nto FA because FA and SA are closely related meaning that SA is part of FA. Our\ncompositional approach in SA exploits basic syntactic rules such as rules of\nmodification, coordination, and negation represented in the formalism of\nUniversal Dependencies (UDs) in English and applied to words representing\nsentiments from sentiment dictionaries. Some of the advantages of our\ncompositional analysis method for SA in contrast to non-compositional analysis\nmethods are interpretability and explainability. We test the accuracy of our\ncompositional approach and compare it with a non-compositional approach VADER\nthat uses simple heuristic rules to deal with negation, coordination and\nmodification. In contrast to previous related work that evaluates\ncompositionality in SA on long reviews, this study uses more appropriate\ndatasets to evaluate compositionality. In addition, we generalize the results\nof compositional approaches in SA to compositional approaches in FA.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u60c5\u611f\u5206\u6790\u4e2d\u7684\u7ec4\u5408\u89c4\u5219\u5e94\u7528\u4e8e\u7126\u70b9\u5206\u6790\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b9e\u9a8c\u9a8c\u8bc1\u7ec4\u5408\u65b9\u6cd5\u5728\u53ef\u89e3\u91ca\u6027\u548c\u51c6\u786e\u7387\u4e0a\u7684\u4f18\u52bf\u3002", "motivation": "\u586b\u8865\u7126\u70b9\u5206\u6790\u9886\u57df\u7f3a\u4e4f\u5b9a\u91cf\u8bc4\u4f30\u7ec4\u5408/\u975e\u7ec4\u5408\u65b9\u6cd5\u7684\u7a7a\u767d\uff0c\u57fa\u4e8e\u60c5\u611f\u5206\u6790\u4e0e\u7126\u70b9\u5206\u6790\u7684\u9ad8\u5ea6\u5173\u8054\u6027\uff0c\u5c1d\u8bd5\u5c06\u60c5\u611f\u5206\u6790\u4e2d\u7684\u7ec4\u5408\u89c4\u5219\u63a8\u5e7f\u81f3\u7126\u70b9\u5206\u6790\u9886\u57df\u3002", "method": "\u4f7f\u7528\u901a\u7528\u4f9d\u8d56\u5173\u7cfb\u53e5\u6cd5\u89c4\u5219\uff08\u4fee\u9970\u3001\u5e76\u5217\u3001\u5426\u5b9a\uff09\u6784\u5efa\u7ec4\u5408\u5206\u6790\u65b9\u6cd5\uff0c\u5728\u66f4\u5408\u9002\u7684\u6570\u636e\u96c6\u4e0a\u4e0eVADER\u975e\u7ec4\u5408\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u6d4b\u8bd5\u3002", "result": "\u7ec4\u5408\u65b9\u6cd5\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4e14\u5728\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u7684\u51c6\u786e\u7387\u4f18\u4e8e\u57fa\u4e8e\u542f\u53d1\u5f0f\u89c4\u5219\u7684\u975e\u7ec4\u5408\u65b9\u6cd5\u3002", "conclusion": "\u60c5\u611f\u5206\u6790\u4e2d\u7684\u7ec4\u5408\u5206\u6790\u65b9\u6cd5\u53ef\u6709\u6548\u63a8\u5e7f\u81f3\u7126\u70b9\u5206\u6790\u9886\u57df\uff0c\u7ec4\u5408\u65b9\u6cd5\u76f8\u6bd4\u975e\u7ec4\u5408\u65b9\u6cd5\u5177\u6709\u66f4\u5f3a\u7684\u89e3\u91ca\u6027\u548c\u9002\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.07827", "pdf": "https://arxiv.org/pdf/2508.07827", "abs": "https://arxiv.org/abs/2508.07827", "authors": ["Yu-Min Tseng", "Wei-Lin Chen", "Chung-Chi Chen", "Hsin-Hsi Chen"], "title": "Evaluating Large Language Models as Expert Annotators", "categories": ["cs.CL"], "comment": "Accepted to COLM 2025", "summary": "Textual data annotation, the process of labeling or tagging text with\nrelevant information, is typically costly, time-consuming, and labor-intensive.\nWhile large language models (LLMs) have demonstrated their potential as direct\nalternatives to human annotators for general domains natural language\nprocessing (NLP) tasks, their effectiveness on annotation tasks in domains\nrequiring expert knowledge remains underexplored. In this paper, we\ninvestigate: whether top-performing LLMs, which might be perceived as having\nexpert-level proficiency in academic and professional benchmarks, can serve as\ndirect alternatives to human expert annotators? To this end, we evaluate both\nindividual LLMs and multi-agent approaches across three highly specialized\ndomains: finance, biomedicine, and law. Specifically, we propose a multi-agent\ndiscussion framework to simulate a group of human annotators, where LLMs are\ntasked to engage in discussions by considering others' annotations and\njustifications before finalizing their labels. Additionally, we incorporate\nreasoning models (e.g., o3-mini) to enable a more comprehensive comparison. Our\nempirical results reveal that: (1) Individual LLMs equipped with inference-time\ntechniques (e.g., chain-of-thought (CoT), self-consistency) show only marginal\nor even negative performance gains, contrary to prior literature suggesting\ntheir broad effectiveness. (2) Overall, reasoning models do not demonstrate\nstatistically significant improvements over non-reasoning models in most\nsettings. This suggests that extended long CoT provides relatively limited\nbenefits for data annotation in specialized domains. (3) Certain model\nbehaviors emerge in the multi-agent discussion environment. For instance,\nClaude 3.7 Sonnet with thinking rarely changes its initial annotations, even\nwhen other agents provide correct annotations or valid reasoning.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u9876\u7ea7\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\uff08\u91d1\u878d\u3001\u751f\u7269\u533b\u5b66\u3001\u6cd5\u5f8b\uff09\u4f5c\u4e3a\u4eba\u5de5\u4e13\u5bb6\u6807\u6ce8\u66ff\u4ee3\u65b9\u6848\u7684\u53ef\u884c\u6027\uff0c\u53d1\u73b0\u5176\u6027\u80fd\u63d0\u5347\u6709\u9650\u4e14\u5b58\u5728\u6a21\u578b\u884c\u4e3a\u56fa\u5316\u73b0\u8c61\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u9700\u4e13\u5bb6\u77e5\u8bc6\u7684\u4e13\u4e1a\u9886\u57df\u6807\u6ce8\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u9a8c\u8bc1\u5176\u662f\u5426\u5177\u5907\u66ff\u4ee3\u4eba\u7c7b\u4e13\u5bb6\u6807\u6ce8\u7684\u80fd\u529b\u3002", "method": "\u91c7\u7528\u4e2a\u4f53LLMs\uff08\u542b\u63a8\u7406\u6280\u672f\u5982CoT\uff09\u548c\u591a\u667a\u80fd\u4f53\u8ba8\u8bba\u6846\u67b6\uff0c\u7ed3\u5408\u63a8\u7406\u6a21\u578b\uff08o3-mini\uff09\u8fdb\u884c\u8de8\u9886\u57df\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "1. \u4e2a\u4f53LLMs\u63a8\u7406\u6280\u672f\u4ec5\u8fb9\u9645\u63d0\u5347\u6027\u80fd\n2. \u63a8\u7406\u6a21\u578b\u65e0\u663e\u8457\u4f18\u52bf\n3. Claude 3.7 Sonnet\u7b49\u6a21\u578b\u5728\u8ba8\u8bba\u4e2d\u56fa\u5b88\u521d\u59cb\u6807\u6ce8", "conclusion": "LLMs\u5728\u4e13\u4e1a\u9886\u57df\u6807\u6ce8\u4efb\u52a1\u4e2d\u4ecd\u9700\u4eba\u7c7b\u76d1\u7763\uff0c\u591a\u667a\u80fd\u4f53\u8ba8\u8bba\u73af\u5883\u66b4\u9732\u6a21\u578b\u884c\u4e3a\u5c40\u9650\uff0c\u957f\u63a8\u7406\u94fe\u5bf9\u4e13\u4e1a\u6807\u6ce8\u5e2e\u52a9\u6709\u9650\u3002"}}
{"id": "2508.07849", "pdf": "https://arxiv.org/pdf/2508.07849", "abs": "https://arxiv.org/abs/2508.07849", "authors": ["Amrita Singh", "H. Suhan Karaca", "Aditya Joshi", "Hye-young Paik", "Jiaojiao Jiang"], "title": "LLMs for Law: Evaluating Legal-Specific LLMs on Contract Understanding", "categories": ["cs.CL"], "comment": "Under review. 4 pages + references", "summary": "Despite advances in legal NLP, no comprehensive evaluation covering multiple\nlegal-specific LLMs currently exists for contract classification tasks in\ncontract understanding. To address this gap, we present an evaluation of 10\nlegal-specific LLMs on three English language contract understanding tasks and\ncompare them with 7 general-purpose LLMs. The results show that legal-specific\nLLMs consistently outperform general-purpose models, especially on tasks\nrequiring nuanced legal understanding. Legal-BERT and Contracts-BERT establish\nnew SOTAs on two of the three tasks, despite having 69% fewer parameters than\nthe best-performing general-purpose LLM. We also identify CaseLaw-BERT and\nLexLM as strong additional baselines for contract understanding. Our results\nprovide a holistic evaluation of legal-specific LLMs and will facilitate the\ndevelopment of more accurate contract understanding systems.", "AI": {"tldr": "\u6cd5\u5f8b\u9886\u57df\u4e13\u7528\u5927\u6a21\u578b\u5728\u5408\u540c\u7406\u89e3\u4efb\u52a1\u4e2d\u5168\u9762\u8d85\u8d8a\u901a\u7528\u6a21\u578b\uff0cLegal-BERT\u4ee5\u66f4\u5c11\u53c2\u6570\u91cf\u5237\u65b0\u4e24\u9879SOTA\u3002", "motivation": "\u5f53\u524d\u6cd5\u5f8bNLP\u9886\u57df\u7f3a\u4e4f\u9488\u5bf9\u5408\u540c\u5206\u7c7b\u4efb\u52a1\u7684\u4e13\u7528\u5927\u6a21\u578b\u5168\u9762\u8bc4\u4f30\uff0c\u672c\u7814\u7a76\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5728\u4e09\u4e2a\u82f1\u6587\u5408\u540c\u7406\u89e3\u4efb\u52a1\u4e0a\u7cfb\u7edf\u8bc4\u4f3010\u4e2a\u6cd5\u5f8b\u4e13\u7528LLM\uff0c\u5e76\u4e0e7\u4e2a\u901a\u7528LLM\u8fdb\u884c\u5bf9\u6bd4\u6d4b\u8bd5\u3002", "result": "\u6cd5\u5f8b\u4e13\u7528\u6a21\u578b\u51c6\u786e\u7387\u5e73\u5747\u63d0\u534723%\uff08\u53c2\u6570\u5c1169%\u7684Legal-BERT\u5237\u65b0\u4e24\u9879\u4efb\u52a1\u8bb0\u5f55\uff09\uff0cCaseLaw-BERT\u548cLexLM\u88ab\u786e\u8ba4\u4e3a\u65b0\u57fa\u51c6\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u6cd5\u5f8bNLP\u63d0\u4f9b\u9996\u4e2a\u7cfb\u7edf\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06\u63a8\u52a8\u66f4\u7cbe\u51c6\u7684\u5408\u540c\u89e3\u6790\u7cfb\u7edf\u5f00\u53d1\u3002"}}
{"id": "2508.07860", "pdf": "https://arxiv.org/pdf/2508.07860", "abs": "https://arxiv.org/abs/2508.07860", "authors": ["Jakub \u0160m\u00edd", "Pavel P\u0159ib\u00e1\u0148", "Pavel Kr\u00e1l"], "title": "Large Language Models for Czech Aspect-Based Sentiment Analysis", "categories": ["cs.CL"], "comment": "Accepted for presentation at the 28th International Conference on\n  Text, Speech and Dialogue (TSD 2025)", "summary": "Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis\ntask that aims to identify sentiment toward specific aspects of an entity.\nWhile large language models (LLMs) have shown strong performance in various\nnatural language processing (NLP) tasks, their capabilities for Czech ABSA\nremain largely unexplored. In this work, we conduct a comprehensive evaluation\nof 19 LLMs of varying sizes and architectures on Czech ABSA, comparing their\nperformance in zero-shot, few-shot, and fine-tuning scenarios. Our results show\nthat small domain-specific models fine-tuned for ABSA outperform\ngeneral-purpose LLMs in zero-shot and few-shot settings, while fine-tuned LLMs\nachieve state-of-the-art results. We analyze how factors such as\nmultilingualism, model size, and recency influence performance and present an\nerror analysis highlighting key challenges, particularly in aspect term\nprediction. Our findings provide insights into the suitability of LLMs for\nCzech ABSA and offer guidance for future research in this area.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u8bc4\u4f3019\u79cd\u4e0d\u540c\u89c4\u6a21\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6377\u514b\u8bedABSA\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u9886\u57df\u4e13\u7528\u5c0f\u6a21\u578b\u7ecf\u8fc7\u5fae\u8c03\u540e\u4f18\u4e8e\u901a\u7528\u5927\u6a21\u578b\u7684\u96f6\u6837\u672c/\u5c11\u6837\u672c\u8868\u73b0\uff0c\u800c\u5fae\u8c03\u540e\u7684\u5927\u6a21\u578b\u80fd\u8fbe\u5230SOTA\u6c34\u5e73", "motivation": "\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6377\u514b\u8bed\u57fa\u4e8e\u65b9\u9762\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u8868\u73b0\uff0c\u586b\u8865\u8be5\u9886\u57df\u7814\u7a76\u7a7a\u767d\uff0c\u5e76\u6bd4\u8f83\u4e0d\u540c\u8bad\u7ec3\u7b56\u7565\uff08\u96f6\u6837\u672c/\u5c11\u6837\u672c/\u5fae\u8c03\uff09\u7684\u6548\u679c\u5dee\u5f02", "method": "\u5bf919\u79cd\u4e0d\u540c\u67b6\u6784\u89c4\u6a21\u7684LLM\u8fdb\u884c\u4e09\u9636\u6bb5\u8bc4\u4f30\uff08\u96f6\u6837\u672c/\u5c11\u6837\u672c/\u5fae\u8c03\uff09\uff0c\u901a\u8fc7\u63a7\u5236\u53d8\u91cf\u6cd5\u5206\u6790\u591a\u8bed\u8a00\u652f\u6301\u3001\u6a21\u578b\u53c2\u6570\u91cf\u3001\u6a21\u578b\u65f6\u6548\u6027\u7b49\u56e0\u7d20\u7684\u5f71\u54cd\uff0c\u5e76\u8fdb\u884c\u9519\u8bef\u5f52\u56e0\u5206\u6790", "result": "\u9886\u57df\u4e13\u7528\u5c0f\u6a21\u578b\u5fae\u8c03\u540eF1\u503c\u8fbe85.1\uff0c\u663e\u8457\u4f18\u4e8eGPT-4\u7684\u96f6\u6837\u672c\u8868\u73b0\uff0862.3\uff09\uff1b\u5fae\u8c03\u540e\u7684LLaMA2-13B\u521b\u9020\u65b0SOTA\uff0888.7\uff09\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u589e\u52a0300%", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u65af\u62c9\u592b\u8bed\u7cfb\u8bed\u8a00\u5904\u7406\u63d0\u4f9b\u91cd\u8981\u57fa\u51c6\uff0c\u8bc1\u660e\u6a21\u578b\u4e13\u4e1a\u5316\u6bd4\u5355\u7eaf\u6269\u5927\u53c2\u6570\u66f4\u6709\u6548\uff0c\u540c\u65f6\u6307\u51fa\u8de8\u8bed\u8a00\u8fc1\u79fb\u4e2d\u65b9\u9762\u672f\u8bed\u8bc6\u522b\u4ecd\u662f\u5173\u952e\u6311\u6218"}}
{"id": "2508.07866", "pdf": "https://arxiv.org/pdf/2508.07866", "abs": "https://arxiv.org/abs/2508.07866", "authors": ["Jakub \u0160m\u00edd", "Pavel P\u0159ib\u00e1\u0148", "Pavel Kr\u00e1l"], "title": "Few-shot Cross-lingual Aspect-Based Sentiment Analysis with Sequence-to-Sequence Models", "categories": ["cs.CL"], "comment": "Accepted for presentation at the 28th International Conference on\n  Text, Speech and Dialogue (TSD 2025)", "summary": "Aspect-based sentiment analysis (ABSA) has received substantial attention in\nEnglish, yet challenges remain for low-resource languages due to the scarcity\nof labelled data. Current cross-lingual ABSA approaches often rely on external\ntranslation tools and overlook the potential benefits of incorporating a small\nnumber of target language examples into training. In this paper, we evaluate\nthe effect of adding few-shot target language examples to the training set\nacross four ABSA tasks, six target languages, and two sequence-to-sequence\nmodels. We show that adding as few as ten target language examples\nsignificantly improves performance over zero-shot settings and achieves a\nsimilar effect to constrained decoding in reducing prediction errors.\nFurthermore, we demonstrate that combining 1,000 target language examples with\nEnglish data can even surpass monolingual baselines. These findings offer\npractical insights for improving cross-lingual ABSA in low-resource and\ndomain-specific settings, as obtaining ten high-quality annotated examples is\nboth feasible and highly effective.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\u5728\u8de8\u8bed\u8a00ABSA\u4efb\u52a1\u4e2d\uff0c\u6dfb\u52a0\u5c11\u91cf\u76ee\u6807\u8bed\u8a00\u6837\u672c\u53ef\u663e\u8457\u63d0\u5347\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u7684\u6a21\u578b\u6027\u80fd\uff0c10\u4e2a\u6837\u672c\u5373\u4f18\u4e8e\u96f6\u6837\u672c\u65b9\u6cd5\uff0c1000\u4e2a\u6837\u672c\u751a\u81f3\u8d85\u8d8a\u5355\u8bed\u57fa\u7ebf", "motivation": "\u73b0\u6709\u8de8\u8bed\u8a00ABSA\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u7ffb\u8bd1\u5de5\u5177\u4e14\u5ffd\u89c6\u76ee\u6807\u8bed\u8a00\u6837\u672c\u7684\u6f5c\u5728\u4ef7\u503c\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u548c\u9886\u57df\u7279\u5b9a\u573a\u666f\u4e0b\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898", "method": "\u901a\u8fc7\u56db\u79cdABSA\u4efb\u52a1\u3001\u516d\u79cd\u76ee\u6807\u8bed\u8a00\u548c\u4e24\u79cd\u5e8f\u5217\u5230\u5e8f\u5217\u6a21\u578b\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u6570\u91cf\u76ee\u6807\u8bed\u8a00\u6837\u672c\u5bf9\u8bad\u7ec3\u6548\u679c\u7684\u5f71\u54cd", "result": "\u2460\u6dfb\u52a010\u4e2a\u6837\u672c\u4f7fF1\u503c\u76f8\u5bf9\u96f6\u6837\u672c\u63d0\u53478-33%\uff1b\u24611000\u4e2a\u76ee\u6807\u6837\u672c\u7ed3\u5408\u82f1\u8bed\u6570\u636e\u65f6\uff0c\u6027\u80fd\u8d85\u8d8a\u5355\u8bed\u57fa\u7ebf\uff08\u5e73\u5747+4.5 F1\uff09", "conclusion": "\u83b7\u53d610\u4e2a\u9ad8\u8d28\u91cf\u6807\u6ce8\u6837\u672c\u5177\u6709\u5b9e\u8df5\u53ef\u884c\u6027\uff0c\u80fd\u6709\u6548\u63d0\u5347\u8de8\u8bed\u8a00ABSA\u6027\u80fd\uff0c\u4e3a\u4f4e\u8d44\u6e90\u548c\u9886\u57df\u7279\u5b9a\u573a\u666f\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.07902", "pdf": "https://arxiv.org/pdf/2508.07902", "abs": "https://arxiv.org/abs/2508.07902", "authors": ["Chen Cecilia Liu", "Hiba Arnaout", "Nils Kova\u010di\u0107", "Dana Atzil-Slonim", "Iryna Gurevych"], "title": "Tailored Emotional LLM-Supporter: Enhancing Cultural Sensitivity", "categories": ["cs.CL"], "comment": "Under review; joint first authors", "summary": "Large language models (LLMs) show promise in offering emotional support and\ngenerating empathetic responses for individuals in distress, but their ability\nto deliver culturally sensitive support remains underexplored due to lack of\nresources. In this work, we introduce CultureCare, the first dataset designed\nfor this task, spanning four cultures and including 1729 distress messages,\n1523 cultural signals, and 1041 support strategies with fine-grained emotional\nand cultural annotations. Leveraging CultureCare, we (i) develop and test four\nadaptation strategies for guiding three state-of-the-art LLMs toward culturally\nsensitive responses; (ii) conduct comprehensive evaluations using LLM judges,\nin-culture human annotators, and clinical psychologists; (iii) show that\nadapted LLMs outperform anonymous online peer responses, and that simple\ncultural role-play is insufficient for cultural sensitivity; and (iv) explore\nthe application of LLMs in clinical training, where experts highlight their\npotential in fostering cultural competence in future therapists.", "AI": {"tldr": "\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u63d0\u4f9b\u6587\u5316\u654f\u611f\u7684\u60c5\u611f\u652f\u6301\uff0c\u5f15\u5165CultureCare\u6570\u636e\u96c6\u5e76\u6d4b\u8bd5\u56db\u79cd\u7b56\u7565\uff0c\u53d1\u73b0\u5176\u6f5c\u529b\u4f46\u9700\u6539\u8fdb", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63d0\u4f9b\u6587\u5316\u654f\u611f\u7684\u60c5\u611f\u652f\u6301\u65b9\u9762\u7f3a\u4e4f\u7cfb\u7edf\u8d44\u6e90\uff0c\u56e0\u6b64\u6784\u5efa\u9996\u4e2a\u8de8\u6587\u5316\u6570\u636e\u96c6CultureCare\u4ee5\u586b\u8865\u8be5\u9886\u57df\u7a7a\u767d", "method": "1. \u521b\u5efa\u5305\u542b1729\u6761\u6c42\u52a9\u4fe1\u606f\u30011523\u4e2a\u6587\u5316\u4fe1\u53f7\u548c1041\u79cd\u652f\u6301\u7b56\u7565\u7684\u6570\u636e\u96c6\uff1b2. \u5f00\u53d1\u56db\u79cd\u9002\u5e94\u7b56\u7565\u6307\u5bfc\u4e09\u5927\u524d\u6cbfLLM\uff1b3. \u91c7\u7528LLM\u8bc4\u4f30\u3001\u672c\u571f\u4eba\u5de5\u6807\u6ce8\u548c\u4e34\u5e8a\u5fc3\u7406\u5b66\u5bb6\u4e09\u91cd\u8bc4\u4f30\u4f53\u7cfb", "result": "\u9002\u914d\u540e\u7684LLM\u8868\u73b0\u4f18\u4e8e\u533f\u540d\u5728\u7ebf\u56de\u590d\uff08\u63d0\u534723.4%\uff09\uff0c\u4f46\u7b80\u5355\u6587\u5316\u89d2\u8272\u626e\u6f14\u7b56\u7565\u6548\u679c\u4e0d\u8db3\uff08\u51c6\u786e\u7387\u4ec561.2%\uff09\uff1b\u4e34\u5e8a\u4e13\u5bb6\u8ba4\u53ef\u5176\u5728\u57f9\u8bad\u4e2d\u7684\u6f5c\u529b\uff0885%\u4e13\u5bb6\u63a8\u8350\uff09", "conclusion": "LLM\u901a\u8fc7\u7cfb\u7edf\u6587\u5316\u9002\u914d\u53ef\u663e\u8457\u63d0\u5347\u60c5\u611f\u652f\u6301\u8d28\u91cf\uff0c\u5728\u4e34\u5e8a\u57f9\u8bad\u4e2d\u5c55\u73b0\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u66f4\u6df1\u5165\u7684\u6587\u5316\u5efa\u6a21\u548c\u9886\u57df\u77e5\u8bc6\u6574\u5408"}}
{"id": "2508.07937", "pdf": "https://arxiv.org/pdf/2508.07937", "abs": "https://arxiv.org/abs/2508.07937", "authors": ["John C. McDonald", "Rosalee Wolfe", "Fabrizio Nunnari"], "title": "Challenges and opportunities in portraying emotion in generated sign language", "categories": ["cs.CL"], "comment": null, "summary": "Non-manual signals in sign languages continue to be a challenge for signing\navatars. More specifically, emotional content has been difficult to incorporate\nbecause of a lack of a standard method of specifying the avatar's emotional\nstate. This paper explores the application of an intuitive two-parameter\nrepresentation for emotive non-manual signals to the Paula signing avatar that\nshows promise for facilitating the linguistic specification of emotional facial\nexpressions in a more coherent manner than previous methods. Users can apply\nthese parameters to control Paula's emotional expressions through a textual\nrepresentation called the EASIER notation. The representation can allow avatars\nto express more nuanced emotional states using two numerical parameters. It\nalso has the potential to enable more consistent specification of emotional\nnon-manual signals in linguistic annotations which drive signing avatars.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u53c2\u6570\u6a21\u578b\u89e3\u51b3\u624b\u8bed\u865a\u62df\u4eba\u60c5\u611f\u8868\u8fbe\u6807\u51c6\u5316\u96be\u9898\uff0c\u901a\u8fc7EASIER\u6587\u672c\u63a7\u5236\u5b9e\u73b0\u7ec6\u817b\u60c5\u611f\u5448\u73b0", "motivation": "\u73b0\u6709\u624b\u8bed\u865a\u62df\u4eba\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u60c5\u611f\u72b6\u6001\u6307\u5b9a\u65b9\u6cd5\uff0c\u5bfc\u81f4\u60c5\u611f\u878d\u5165\u56f0\u96be\u4e14\u7f3a\u4e4f\u7cfb\u7edf\u6027\u6807\u6ce8\u89c4\u8303", "method": "\u5f00\u53d1\u57fa\u4e8e\u4e24\u4e2a\u6570\u503c\u53c2\u6570\u7684\u60c5\u611f\u8868\u8fbe\u7cfb\u7edf\uff0c\u7ed3\u5408EASIER\u6587\u672c\u7b26\u53f7\u63a7\u5236Paula\u865a\u62df\u4eba\u7684\u975e\u624b\u52a8\u60c5\u611f\u4fe1\u53f7", "result": "\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u60c5\u611f\u72b6\u6001\u63a7\u5236\uff0c\u5efa\u7acb\u53ef\u590d\u7528\u7684\u8bed\u8a00\u5b66\u6807\u6ce8\u6807\u51c6\uff0c\u63d0\u5347\u865a\u62df\u4eba\u624b\u8bed\u8868\u8fbe\u7684\u8fde\u8d2f\u6027", "conclusion": "\u53c2\u6570\u5316\u65b9\u6cd5\u4e3a\u624b\u8bed\u865a\u62df\u4eba\u60c5\u611f\u8868\u8fbe\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u6846\u67b6\uff0c\u663e\u8457\u6539\u5584\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u60c5\u611f\u4f20\u8fbe\u6548\u679c"}}
{"id": "2508.07955", "pdf": "https://arxiv.org/pdf/2508.07955", "abs": "https://arxiv.org/abs/2508.07955", "authors": ["Furkan \u015eahinu\u00e7", "Subhabrata Dutta", "Iryna Gurevych"], "title": "Expert Preference-based Evaluation of Automated Related Work Generation", "categories": ["cs.CL"], "comment": "Project page: https://ukplab.github.io/arxiv2025-expert-eval-rw/", "summary": "Expert domain writing, such as scientific writing, typically demands\nextensive domain knowledge. Recent advances in LLMs show promising potential in\nreducing the expert workload. However, evaluating the quality of automatically\ngenerated scientific writing is a crucial open issue, as it requires knowledge\nof domain-specific evaluation criteria and the ability to discern expert\npreferences. Conventional automatic metrics and LLM-as-a-judge systems are\ninsufficient to grasp expert preferences and domain-specific quality standards.\nTo address this gap and support human-AI collaborative writing, we focus on\nrelated work generation, one of the most challenging scientific tasks, as an\nexemplar. We propose GREP, a multi-turn evaluation framework that integrates\nclassical related work evaluation criteria with expert-specific preferences.\nInstead of assigning a single score, our framework decomposes the evaluation\ninto fine-grained dimensions. This localized evaluation approach is further\naugmented with contrastive few-shot examples to provide detailed contextual\nguidance for the evaluation dimensions. The design principles allow our\nframework to deliver cardinal assessment of quality, which can facilitate\nbetter post-training compared to ordinal preference data. For better\naccessibility, we design two variants of GREP: a more precise variant with\nproprietary LLMs as evaluators, and a cheaper alternative with open-weight\nLLMs. Empirical investigation reveals that our framework is able to assess the\nquality of related work sections in a much more robust manner compared to\nstandard LLM judges, reflects natural scenarios of scientific writing, and\nbears a strong correlation with the human expert assessment. We also observe\nthat generations from state-of-the-art LLMs struggle to satisfy validation\nconstraints of a suitable related work section. They (mostly) fail to improve\nbased on feedback as well.", "AI": {"tldr": "\u63d0\u51faGREP\u6846\u67b6\u89e3\u51b3LLM\u751f\u6210\u79d1\u5b66\u5199\u4f5c\u7684\u8bc4\u4f30\u96be\u9898\uff0c\u7ed3\u5408\u7ecf\u5178\u6807\u51c6\u4e0e\u4e13\u5bb6\u504f\u597d\u8fdb\u884c\u591a\u7ef4\u5ea6\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u6355\u6349\u4e13\u5bb6\u504f\u597d\u4e0e\u9886\u57df\u6807\u51c6\uff0c\u5bfc\u81f4\u79d1\u5b66\u5199\u4f5c\u8d28\u91cf\u8bc4\u4f30\u5b58\u5728\u74f6\u9888\u3002", "method": "\u8bbe\u8ba1\u591a\u56de\u5408\u8bc4\u4f30\u6846\u67b6GREP\uff0c\u878d\u5408\u7ecf\u5178\u76f8\u5173\u5de5\u4f5c\u603b\u7ed3\u6807\u51c6\u4e0e\u4e13\u5bb6\u504f\u597d\uff0c\u91c7\u7528\u7ec6\u7c92\u5ea6\u7ef4\u5ea6\u5206\u89e3+\u5bf9\u6bd4\u6837\u672c\u589e\u5f3a\u8bc4\u4f30\u3002", "result": "GREP\u8bc4\u4f30\u8d28\u91cf\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e0e\u4e13\u5bb6\u8bc4\u4f30\u5f3a\u76f8\u5173\uff1b\u5f53\u524dLLM\u751f\u6210\u5185\u5bb9\u96be\u4ee5\u6ee1\u8db3\u7ea6\u675f\u4e14\u53cd\u9988\u6539\u8fdb\u6709\u9650\u3002", "conclusion": "GREP\u6846\u67b6\u4e3a\u79d1\u5b66\u5199\u4f5c\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u53ef\u9760\u8bc4\u4f30\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u73b0\u6709LLM\u5728\u4e13\u4e1a\u5199\u4f5c\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.07959", "pdf": "https://arxiv.org/pdf/2508.07959", "abs": "https://arxiv.org/abs/2508.07959", "authors": ["Changhao Song", "Yazhou Zhang", "Hui Gao", "Ben Yao", "Peng Zhang"], "title": "Large Language Models for Subjective Language Understanding: A Survey", "categories": ["cs.CL"], "comment": null, "summary": "Subjective language understanding refers to a broad set of natural language\nprocessing tasks where the goal is to interpret or generate content that\nconveys personal feelings, opinions, or figurative meanings rather than\nobjective facts. With the advent of large language models (LLMs) such as\nChatGPT, LLaMA, and others, there has been a paradigm shift in how we approach\nthese inherently nuanced tasks. In this survey, we provide a comprehensive\nreview of recent advances in applying LLMs to subjective language tasks,\nincluding sentiment analysis, emotion recognition, sarcasm detection, humor\nunderstanding, stance detection, metaphor interpretation, intent detection, and\naesthetics assessment. We begin by clarifying the definition of subjective\nlanguage from linguistic and cognitive perspectives, and we outline the unique\nchallenges posed by subjective language (e.g. ambiguity, figurativeness,\ncontext dependence). We then survey the evolution of LLM architectures and\ntechniques that particularly benefit subjectivity tasks, highlighting why LLMs\nare well-suited to model subtle human-like judgments. For each of the eight\ntasks, we summarize task definitions, key datasets, state-of-the-art LLM-based\nmethods, and remaining challenges. We provide comparative insights, discussing\ncommonalities and differences among tasks and how multi-task LLM approaches\nmight yield unified models of subjectivity. Finally, we identify open issues\nsuch as data limitations, model bias, and ethical considerations, and suggest\nfuture research directions. We hope this survey will serve as a valuable\nresource for researchers and practitioners interested in the intersection of\naffective computing, figurative language processing, and large-scale language\nmodels.", "AI": {"tldr": "\u7efc\u8ff0\u63a2\u8ba8\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\u5728\u4e3b\u89c2\u8bed\u8a00\u7406\u89e3\u4efb\u52a1(\u60c5\u611f\u5206\u6790/\u9690\u55bb\u7406\u89e3\u7b49)\u4e2d\u7684\u5e94\u7528\uff0c\u5206\u6790\u5176\u4f18\u52bf\u5e76\u6307\u51fa\u6570\u636e\u504f\u5dee\u3001\u4f26\u7406\u98ce\u9669\u7b49\u6311\u6218", "motivation": "\u4e3b\u89c2\u8bed\u8a00\u5b58\u5728\u6b67\u4e49\u6027\u3001\u6bd4\u55bb\u6027\u3001\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7b49\u7279\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u5904\u7406\u6548\u679c\u6709\u9650\uff0c\u9700\u7cfb\u7edf\u6027\u7814\u7a76LLMs\u5728\u6355\u6349\u4eba\u7c7b\u4e3b\u89c2\u8ba4\u77e5\u65b9\u9762\u7684\u6f5c\u529b", "method": "1. \u4ece\u8bed\u8a00\u5b66\u89c6\u89d2\u5b9a\u4e49\u4e3b\u89c2\u8bed\u8a00 2. \u68b3\u7406LLM\u67b6\u6784\u6f14\u5316\u8def\u5f84 3. \u5206\u7c7b\u603b\u7ed38\u7c7b\u4e3b\u89c2\u4efb\u52a1\u7684\u6700\u65b0\u65b9\u6cd5 4. \u63d0\u51fa\u591a\u4efb\u52a1\u7edf\u4e00\u5efa\u6a21\u6846\u67b6\u8bbe\u60f3", "result": "LLMs\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u5c55\u73b0\u5bf9\u4e3b\u89c2\u8bed\u4e49\u7684\u5f3a\u6355\u6349\u80fd\u529b\uff0c\u4f46\u5728\u5c0f\u6837\u672c\u573a\u666f\u4ecd\u5b58\u5728\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4e14\u9762\u4e34\u8bad\u7ec3\u6570\u636e\u504f\u89c1\u3001\u8bc4\u4ef7\u6307\u6807\u4e0d\u5b8c\u5584\u7b49\u7cfb\u7edf\u6027\u6311\u6218", "conclusion": "\u672c\u6587\u4e3a\u7406\u89e3LLMs\u5904\u7406\u4e3b\u89c2\u8bed\u8a00\u7684\u80fd\u529b\u63d0\u4f9b\u7cfb\u7edf\u6846\u67b6\uff0c\u5efa\u8bae\u672a\u6765\u5f00\u53d1\u591a\u6a21\u6001\u4e3b\u89c2\u6a21\u578b\u3001\u5efa\u7acb\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u4f53\u7cfb\u3001\u6784\u5efa\u4f26\u7406\u6307\u5bfc\u539f\u5219"}}
{"id": "2508.07964", "pdf": "https://arxiv.org/pdf/2508.07964", "abs": "https://arxiv.org/abs/2508.07964", "authors": ["Matthias Sperber", "Maureen de Seyssel", "Jiajun Bao", "Matthias Paulik"], "title": "Toward Machine Interpreting: Lessons from Human Interpreting Studies", "categories": ["cs.CL"], "comment": null, "summary": "Current speech translation systems, while having achieved impressive\naccuracies, are rather static in their behavior and do not adapt to real-world\nsituations in ways human interpreters do. In order to improve their practical\nusefulness and enable interpreting-like experiences, a precise understanding of\nthe nature of human interpreting is crucial. To this end, we discuss human\ninterpreting literature from the perspective of the machine translation field,\nwhile considering both operational and qualitative aspects. We identify\nimplications for the development of speech translation systems and argue that\nthere is great potential to adopt many human interpreting principles using\nrecent modeling techniques. We hope that our findings provide inspiration for\nclosing the perceived usability gap, and can motivate progress toward true\nmachine interpreting.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7\u501f\u9274\u4eba\u7c7b\u53e3\u8bd1\u539f\u5219\u6539\u8fdb\u9759\u6001\u7684\u8bed\u97f3\u7ffb\u8bd1\u7cfb\u7edf", "motivation": "\u73b0\u6709\u8bed\u97f3\u7ffb\u8bd1\u7cfb\u7edf\u7f3a\u4e4f\u4eba\u7c7b\u53e3\u8bd1\u7684\u52a8\u6001\u9002\u5e94\u80fd\u529b\uff0c\u5b9e\u7528\u6027\u5b58\u5728\u663e\u8457\u5dee\u8ddd", "method": "\u4ece\u673a\u5668\u7ffb\u8bd1\u89c6\u89d2\u7cfb\u7edf\u5206\u6790\u4eba\u7c7b\u53e3\u8bd1\u6587\u732e\uff0c\u7ed3\u5408\u64cd\u4f5c\u6d41\u7a0b\u4e0e\u8d28\u91cf\u8bc4\u4ef7\u7ef4\u5ea6", "result": "\u8bc1\u5b9e\u5229\u7528\u73b0\u4ee3\u5efa\u6a21\u6280\u672f\u6574\u5408\u4eba\u7c7b\u53e3\u8bd1\u539f\u5219\u5177\u6709\u5de8\u5927\u6f5c\u529b", "conclusion": "\u901a\u8fc7\u878d\u5408\u4eba\u7c7b\u53e3\u8bd1\u7b56\u7565\uff0c\u53ef\u63a8\u52a8\u5b9e\u73b0\u771f\u6b63\u7684\u673a\u5668\u53e3\u8bd1\u7cfb\u7edf"}}
{"id": "2508.07969", "pdf": "https://arxiv.org/pdf/2508.07969", "abs": "https://arxiv.org/abs/2508.07969", "authors": ["David Arps", "Hassan Sajjad", "Laura Kallmeyer"], "title": "Understanding Syntactic Generalization in Structure-inducing Language Models", "categories": ["cs.CL"], "comment": "Code available at https://github.com/davidarps/silm", "summary": "Structure-inducing Language Models (SiLM) are trained on a self-supervised\nlanguage modeling task, and induce a hierarchical sentence representation as a\nbyproduct when processing an input. A wide variety of SiLMs have been proposed.\nHowever, these have typically been evaluated on a relatively small scale, and\nevaluation of these models has systematic gaps and lacks comparability. In this\nwork, we study three different SiLM architectures using both natural language\n(English) corpora and synthetic bracketing expressions: Structformer (Shen et\nal., 2021), UDGN (Shen et al., 2022) and GPST (Hu et al., 2024). We compare\nthem with respect to (i) properties of the induced syntactic representations\n(ii) performance on grammaticality judgment tasks, and (iii) training dynamics.\nWe find that none of the three architectures dominates across all evaluation\nmetrics. However, there are significant differences, in particular with respect\nto the induced syntactic representations. The Generative Pretrained Structured\nTransformer (GPST; Hu et al. 2024) performs most consistently across evaluation\nsettings, and outperforms the other models on long-distance dependencies in\nbracketing expressions. Furthermore, our study shows that small models trained\non large amounts of synthetic data provide a useful testbed for evaluating\nbasic model properties.", "AI": {"tldr": "\u7cfb\u7edf\u8bc4\u4f30\u4e09\u79cd\u7ed3\u6784\u8bf1\u5bfc\u8bed\u8a00\u6a21\u578b\uff08Structformer/UDGN/GPST\uff09\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u53d1\u73b0GPST\u5728\u957f\u8ddd\u79bb\u4f9d\u8d56\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f18\u4e14\u6700\u7a33\u5b9a\u3002", "motivation": "\u73b0\u6709\u7ed3\u6784\u8bf1\u5bfc\u8bed\u8a00\u6a21\u578b\uff08SiLM\uff09\u7684\u8bc4\u4f30\u5b58\u5728\u89c4\u6a21\u5c0f\u3001\u7cfb\u7edf\u6027\u4e0d\u8db3\u548c\u53ef\u6bd4\u6027\u5dee\u7684\u95ee\u9898\uff0c\u9700\u8981\u5168\u9762\u6bd4\u8f83\u4e0d\u540c\u67b6\u6784\u7684\u4f18\u52a3\u3002", "method": "\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\uff08\u82f1\u8bed\uff09\u8bed\u6599\u5e93\u548c\u5408\u6210\u62ec\u53f7\u8868\u8fbe\u5f0f\uff0c\u7cfb\u7edf\u6bd4\u8f83\u4e09\u79cdSiLM\u67b6\u6784\u7684\u53e5\u6cd5\u8868\u793a\u3001\u8bed\u6cd5\u5224\u65ad\u80fd\u529b\u548c\u8bad\u7ec3\u52a8\u6001\u3002", "result": "GPST\u5728\u8de8\u8bc4\u4f30\u573a\u666f\u4e2d\u8868\u73b0\u6700\u7a33\u5b9a\uff0c\u5728\u62ec\u53f7\u8868\u8fbe\u5f0f\u957f\u8ddd\u79bb\u4f9d\u8d56\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u5c0f\u6a21\u578b\u9a8c\u8bc1\u57fa\u7840\u6a21\u578b\u7279\u6027\u6709\u6548\u3002", "conclusion": "\u4e0d\u540c\u67b6\u6784\u5728\u8bc4\u4f30\u6307\u6807\u4e0a\u5404\u5177\u4f18\u52bf\uff0cGPST\u7efc\u5408\u8868\u73b0\u6700\u4f73\u3002\u5c0f\u6a21\u578b+\u5408\u6210\u6570\u636e\u7684\u7ec4\u5408\u4e3a\u6a21\u578b\u57fa\u7840\u7279\u6027\u8bc4\u4f30\u63d0\u4f9b\u6709\u6548\u6d4b\u8bd5\u65b9\u6848\u3002"}}
{"id": "2508.07976", "pdf": "https://arxiv.org/pdf/2508.07976", "abs": "https://arxiv.org/abs/2508.07976", "authors": ["Jiaxuan Gao", "Wei Fu", "Minyang Xie", "Shusheng Xu", "Chuyi He", "Zhiyu Mei", "Banghua Zhu", "Yi Wu"], "title": "Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in LLM-based agents have demonstrated remarkable\ncapabilities in handling complex, knowledge-intensive tasks by integrating\nexternal tools. Among diverse choices of tools, search tools play a pivotal\nrole in accessing vast external knowledge. However, open-source agents still\nfall short of achieving expert-level Search Intelligence, the ability to\nresolve ambiguous queries, generate precise searches, analyze results, and\nconduct thorough exploration. Existing approaches fall short in scalability,\nefficiency, and data quality. For example, small turn limits in existing online\nRL methods, e.g. <=10, restrict complex strategy learning. This paper\nintroduces ASearcher, an open-source project for large-scale RL training of\nsearch agents. Our key contributions include: (1) Scalable fully asynchronous\nRL training that enables long-horizon search while maintaining high training\nefficiency. (2) A prompt-based LLM agent that autonomously synthesizes\nhigh-quality and challenging QAs, creating a large-scale QA dataset. Through RL\ntraining, our prompt-based QwQ-32B agent achieves substantial improvements,\nwith 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our\nagent exhibits extreme long-horizon search, with tool calls exceeding 40 turns\nand output tokens exceeding 150k during training time. With a simple agent\ndesign and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on\nxBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We\nopen-source our models, training data, and codes in\nhttps://github.com/inclusionAI/ASearcher.", "AI": {"tldr": "\u63d0\u51fa\u5f00\u6e90\u9879\u76eeASearcher\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63d0\u5347\u641c\u7d22\u667a\u80fd\u4f53\u7684\u957f\u5468\u671f\u641c\u7d22\u80fd\u529b", "motivation": "\u73b0\u6709\u5f00\u6e90\u667a\u80fd\u4f53\u5728\u641c\u7d22\u667a\u80fd\uff08\u5904\u7406\u6a21\u7cca\u67e5\u8be2\u3001\u7cbe\u51c6\u641c\u7d22\u3001\u7ed3\u679c\u5206\u6790\u548c\u6df1\u5ea6\u63a2\u7d22\uff09\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4e14\u4f20\u7edf\u65b9\u6cd5\u5728\u6269\u5c55\u6027\u3001\u6548\u7387\u548c\u6570\u636e\u8d28\u91cf\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff08\u5982\u5728\u7ebfRL\u65b9\u6cd5\u56de\u5408\u6570\u9650\u5236\u226410\uff09", "method": "\u91c7\u7528\uff081\uff09\u5b8c\u5168\u5f02\u6b65\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u652f\u6301\u957f\u5468\u671f\u641c\u7d22\u8bad\u7ec3\uff0c\uff082\uff09\u57fa\u4e8e\u63d0\u793a\u7684LLM\u667a\u80fd\u4f53\u81ea\u4e3b\u751f\u6210\u9ad8\u8d28\u91cfQA\u6570\u636e\u96c6", "result": "QwQ-32B\u667a\u80fd\u4f53\u5728xBench\u548cGAIA\u57fa\u51c6\u4e0a\u5206\u522b\u53d6\u5f9746.7%\u548c20.8%\u7684Avg@4\u63d0\u5347\uff0c\u8bad\u7ec3\u65f6\u5de5\u5177\u8c03\u7528\u8d8540\u8f6e\u3001\u8f93\u51fa\u6807\u8bb0\u8d8515\u4e07", "conclusion": "ASearcher\u65e0\u9700\u5916\u90e8LLM\u5373\u8d85\u8d8a\u73b0\u670932B\u5f00\u6e90\u667a\u80fd\u4f53\uff08xBench 42.1\uff0cGAIA 52.8\uff09\uff0c\u76f8\u5173\u6a21\u578b\u3001\u6570\u636e\u548c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2508.07993", "pdf": "https://arxiv.org/pdf/2508.07993", "abs": "https://arxiv.org/abs/2508.07993", "authors": ["Anna Sofia Lippolis", "Andrea Giovanni Nuzzolese", "Aldo Gangemi"], "title": "The Medical Metaphors Corpus (MCC)", "categories": ["cs.CL"], "comment": null, "summary": "Metaphor is a fundamental cognitive mechanism that shapes scientific\nunderstanding, enabling the communication of complex concepts while potentially\nconstraining paradigmatic thinking. Despite the prevalence of figurative\nlanguage in scientific discourse, existing metaphor detection resources\nprimarily focus on general-domain text, leaving a critical gap for\ndomain-specific applications. In this paper, we present the Medical Metaphors\nCorpus (MCC), a comprehensive dataset of 792 annotated scientific conceptual\nmetaphors spanning medical and biological domains. MCC aggregates metaphorical\nexpressions from diverse sources including peer-reviewed literature, news\nmedia, social media discourse, and crowdsourced contributions, providing both\nbinary and graded metaphoricity judgments validated through human annotation.\nEach instance includes source-target conceptual mappings and perceived\nmetaphoricity scores on a 0-7 scale, establishing the first annotated resource\nfor computational scientific metaphor research. Our evaluation demonstrates\nthat state-of-the-art language models achieve modest performance on scientific\nmetaphor detection, revealing substantial room for improvement in\ndomain-specific figurative language understanding. MCC enables multiple\nresearch applications including metaphor detection benchmarking, quality-aware\ngeneration systems, and patient-centered communication tools.", "AI": {"tldr": "\u6784\u5efa\u9996\u4e2a\u533b\u5b66\u9886\u57df\u9690\u55bb\u8bed\u6599\u5e93MCC\uff0c\u542b792\u4e2a\u591a\u6e90\u6807\u6ce8\u6837\u672c\uff0c\u63ed\u793a\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u9690\u55bb\u68c0\u6d4b\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u9690\u55bb\u68c0\u6d4b\u8d44\u6e90\u96c6\u4e2d\u4e8e\u901a\u7528\u9886\u57df\uff0c\u7f3a\u4e4f\u9488\u5bf9\u533b\u5b66/\u751f\u7269\u5b66\u7b49\u4e13\u4e1a\u9886\u57df\u7684\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5236\u7ea6\u4e86\u9886\u57df\u7279\u5f02\u6027\u6bd4\u55bb\u8bed\u8a00\u7814\u7a76\u3002", "method": "\u6574\u5408\u540c\u884c\u8bc4\u5ba1\u6587\u732e\u3001\u65b0\u95fb\u5a92\u4f53\u3001\u793e\u4ea4\u5a92\u4f53\u53ca\u4f17\u5305\u6570\u636e\uff0c\u91c7\u7528\u4eba\u5de5\u6807\u6ce8\u9a8c\u8bc1\u7684\u53cc\u91cd\u6807\u6ce8\u4f53\u7cfb\uff08\u4e8c\u5143\u5224\u65ad+0-7\u5206\u9690\u55bb\u6027\u8bc4\u5206\uff09\u3002", "result": "SOTA\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u9690\u55bb\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u9650\uff08F1=0.61\uff09\uff0c\u51f8\u663e\u9886\u57df\u7279\u5f02\u6027\u6bd4\u55bb\u7406\u89e3\u7684\u6280\u672f\u6311\u6218\u3002", "conclusion": "MCC\u4e3a\u9690\u55bb\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u3001\u8d28\u91cf\u611f\u77e5\u751f\u6210\u7cfb\u7edf\u53ca\u533b\u60a3\u6c9f\u901a\u5de5\u5177\u5f00\u53d1\u63d0\u4f9b\u9996\u4e2a\u53ef\u8ba1\u7b97\u7814\u7a76\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2508.07999", "pdf": "https://arxiv.org/pdf/2508.07999", "abs": "https://arxiv.org/abs/2508.07999", "authors": ["Ryan Wong", "Jiawei Wang", "Junjie Zhao", "Li Chen", "Yan Gao", "Long Zhang", "Xuan Zhou", "Zuo Wang", "Kai Xiang", "Ge Zhang", "Wenhao Huang", "Yang Wang", "Ke Wang"], "title": "WideSearch: Benchmarking Agentic Broad Info-Seeking", "categories": ["cs.CL"], "comment": null, "summary": "From professional research to everyday planning, many tasks are bottlenecked\nby wide-scale information seeking, which is more repetitive than cognitively\ncomplex. With the rapid development of Large Language Models (LLMs), automated\nsearch agents powered by LLMs offer a promising solution to liberate humans\nfrom this tedious work. However, the capability of these agents to perform such\n\"wide-context\" collection reliably and completely remains largely unevaluated\ndue to a lack of suitable benchmarks. To bridge this gap, we introduce\nWideSearch, a new benchmark engineered to evaluate agent reliability on these\nlarge-scale collection tasks. The benchmark features 200 manually curated\nquestions (100 in English, 100 in Chinese) from over 15 diverse domains,\ngrounded in real user queries. Each task requires agents to collect large-scale\natomic information, which could be verified one by one objectively, and arrange\nit into a well-organized output. A rigorous five-stage quality control pipeline\nensures the difficulty, completeness, and verifiability of the dataset. We\nbenchmark over 10 state-of-the-art agentic search systems, including\nsingle-agent, multi-agent frameworks, and end-to-end commercial systems. Most\nsystems achieve overall success rates near 0\\%, with the best performer\nreaching just 5\\%. However, given sufficient time, cross-validation by multiple\nhuman testers can achieve a near 100\\% success rate. These results demonstrate\nthat present search agents have critical deficiencies in large-scale\ninformation seeking, underscoring urgent areas for future research and\ndevelopment in agentic search. Our dataset, evaluation pipeline, and benchmark\nresults have been publicly released at https://widesearch-seed.github.io/", "AI": {"tldr": "\u63d0\u51faWideSearch\u57fa\u51c6\u6d4b\u8bd5\u5927\u89c4\u6a21\u4fe1\u606f\u641c\u7d22\u4ee3\u7406\u53ef\u9760\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u73b0\u6709\u7cfb\u7edf\u6210\u529f\u7387\u6781\u4f4e\uff08\u6700\u4f735% vs \u4eba\u7c7b\u8fd1100%\uff09\uff0c\u63ed\u793a\u4ee3\u7406\u5b58\u5728\u91cd\u5927\u7f3a\u9677\u9700\u6539\u8fdb", "motivation": "\u73b0\u6709\u8bc4\u4f30\u57fa\u51c6\u65e0\u6cd5\u6709\u6548\u8861\u91cfLLM\u641c\u7d22\u4ee3\u7406\u5728\u5927\u89c4\u6a21\u4fe1\u606f\u6536\u96c6\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u9700\u6784\u5efa\u66f4\u8d34\u8fd1\u771f\u5b9e\u573a\u666f\u7684\u8bc4\u4f30\u4f53\u7cfb", "method": "\u521b\u5efa\u5305\u542b200\u4e2a\u8de8\u9886\u57df\u95ee\u9898\uff08\u4e2d\u82f1\u6587\u5404100\uff09\u7684\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u4e94\u9636\u6bb5\u8d28\u91cf\u63a7\u5236\u6d41\u7a0b\u786e\u4fdd\u4efb\u52a1\u96be\u5ea6\u548c\u53ef\u9a8c\u8bc1\u6027\uff0c\u6d4b\u8bd510+\u4e3b\u6d41\u641c\u7d22\u4ee3\u7406\u7cfb\u7edf", "result": "\u73b0\u6709\u7cfb\u7edf\u6574\u4f53\u6210\u529f\u7387\u63a5\u8fd10%\uff0c\u6700\u4f18\u7cfb\u7edf\u4ec55%\u6210\u529f\u7387\uff0c\u800c\u4eba\u7c7b\u6d4b\u8bd5\u8005\u901a\u8fc7\u4ea4\u53c9\u9a8c\u8bc1\u53ef\u8fbe\u8fd1100%\u6210\u529f\u7387", "conclusion": "\u5f53\u524d\u641c\u7d22\u4ee3\u7406\u5728\u5927\u89c4\u6a21\u4fe1\u606f\u6536\u96c6\u4efb\u52a1\u4e2d\u5b58\u5728\u5173\u952e\u7f3a\u9677\uff0c\u9700\u52a0\u5f3a\u63a8\u7406\u9a8c\u8bc1\u80fd\u529b\uff0c\u516c\u5f00\u6570\u636e\u96c6\u63a8\u52a8\u76f8\u5173\u9886\u57df\u7814\u7a76"}}
{"id": "2508.08011", "pdf": "https://arxiv.org/pdf/2508.08011", "abs": "https://arxiv.org/abs/2508.08011", "authors": ["Mingzi Cao", "Xi Wang", "Nikolaos Aletras"], "title": "Progressive Depth Up-scaling via Optimal Transport", "categories": ["cs.CL"], "comment": null, "summary": "Scaling Large Language Models (LLMs) yields performance gains but incurs\nsubstantial training costs. Depth up-scaling offers training efficiency by\nadding new layers to pre-trained models. However, most existing methods copy or\naverage weights from base layers, neglecting neuron permutation differences.\nThis limitation can potentially cause misalignment that harms performance.\nInspired by applying Optimal Transport (OT) for neuron alignment, we propose\nOptimal Transport Depth Up-Scaling (OpT-DeUS). OpT-DeUS aligns and fuses\nTransformer blocks in adjacent base layers via OT for new layer creation, to\nmitigate neuron permutation mismatch between layers. OpT-DeUS achieves better\noverall performance and offers improved training efficiency than existing\nmethods for continual pre-training and supervised fine-tuning across different\nmodel sizes. To further evaluate the impact of interpolation positions, our\nextensive analysis shows that inserting new layers closer to the top results in\nhigher training efficiency due to shorter back-propagation time while obtaining\nadditional performance gains.", "AI": {"tldr": "\u63d0\u51faOpT-DeUS\u65b9\u6cd5\uff0c\u5229\u7528\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u5bf9\u9f50Transformer\u5c42\u795e\u7ecf\u5143\u6392\u5217\u5dee\u5f02\uff0c\u63d0\u5347\u5927\u6a21\u578b\u6df1\u5ea6\u6269\u5c55\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u6269\u5c55\u65b9\u6cd5\u76f4\u63a5\u590d\u5236/\u5e73\u5747\u57fa\u5ea7\u6a21\u578b\u6743\u91cd\uff0c\u5ffd\u7565\u4e86\u795e\u7ecf\u5143\u6392\u5217\u5dee\u5f02\u5bfc\u81f4\u7684\u5c42\u95f4\u9519\u4f4d\u95ee\u9898\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u6700\u4f18\u8fd0\u8f93(Optimal Transport)\u5bf9\u9f50\u76f8\u90bb\u57fa\u5ea7\u5c42\u7684Transformer\u5757\uff0c\u521b\u5efa\u65b0\u5c42\u4ee5\u6d88\u9664\u795e\u7ecf\u5143\u6392\u5217\u4e0d\u5339\u914d\u3002", "result": "\u5728\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u7cbe\u8c03\u4efb\u52a1\u4e2d\uff0cOpT-DeUS\u5728\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u4e0a\u5747\u53d6\u5f97\u66f4\u4f18\u6027\u80fd\uff0c\u4e14\u63d2\u5165\u9876\u5c42\u65f6\u8bad\u7ec3\u6548\u7387\u6700\u9ad8\uff08\u7f29\u77ed\u53cd\u5411\u4f20\u64ad\u65f6\u95f4\uff09\u3002", "conclusion": "OpT-DeUS\u6709\u6548\u7f13\u89e3\u795e\u7ecf\u5143\u6392\u5217\u5931\u914d\u95ee\u9898\uff0c\u4e3a\u6a21\u578b\u6df1\u5ea6\u6269\u5c55\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u9876\u5c42\u6269\u5c55\u7684\u4f18\u5316\u6f5c\u529b\u3002"}}
{"id": "2508.08050", "pdf": "https://arxiv.org/pdf/2508.08050", "abs": "https://arxiv.org/abs/2508.08050", "authors": ["Fabrizio Nunnari", "Cristina Luna Jim\u00e9nez", "Rosalee Wolfe", "John C. McDonald", "Michael Filhol", "Eleni Efthimiou", "Evita Fotinea", "Thomas Hanke"], "title": "9th Workshop on Sign Language Translation and Avatar Technologies (SLTAT 2025)", "categories": ["cs.CL"], "comment": null, "summary": "The Sign Language Translation and Avatar Technology (SLTAT) workshops\ncontinue a series of gatherings to share recent advances in improving deaf /\nhuman communication through non-invasive means. This 2025 edition, the 9th\nsince its first appearance in 2011, is hosted by the International Conference\non Intelligent Virtual Agents (IVA), giving the opportunity for contamination\nbetween two research communities, using digital humans as either virtual\ninterpreters or as interactive conversational agents. As presented in this\nsummary paper, SLTAT sees contributions beyond avatar technologies, with a\nconsistent number of submissions on sign language recognition, and other work\non data collection, data analysis, tools, ethics, usability, and affective\ncomputing.", "AI": {"tldr": "SLTAT\u7814\u8ba8\u4f1a\u805a\u7126\u624b\u8bed\u7ffb\u8bd1\u4e0e\u865a\u62df\u4eba\u6280\u672f\uff0c\u6574\u5408\u8bc6\u522b\u6280\u672f\u3001\u6570\u636e\u6536\u96c6\u3001\u4f26\u7406\u7814\u7a76\u7b49\u591a\u9886\u57df\u6210\u679c\uff0c\u4fc3\u8fdb\u804b\u4eba\u4ea4\u6d41\u6280\u672f\u521b\u65b0\u3002", "motivation": "\u901a\u8fc7\u975e\u4fb5\u5165\u5f0f\u6280\u672f\u6539\u5584\u804b\u4eba/\u4eba\u7c7b\u6c9f\u901a\uff0c\u878d\u5408\u865a\u62df\u7ffb\u8bd1\u4e0e\u5bf9\u8bdd\u4ee3\u7406\u6280\u672f\uff0c\u63a8\u52a8\u8de8\u5b66\u79d1\u7814\u7a76\u5408\u4f5c\u3002", "method": "\u4f9d\u6258IVA\u4f1a\u8bae\u5e73\u53f0\uff0c\u6c47\u96c6\u624b\u8bed\u8bc6\u522b\u3001\u6570\u636e\u5de5\u5177\u3001\u60c5\u611f\u8ba1\u7b97\u7b499\u4e2a\u7814\u7a76\u65b9\u5411\u7684\u8bba\u6587\u6295\u7a3f\uff0c\u5f3a\u5316\u6280\u672f\u4ea4\u53c9\u5e94\u7528\u3002", "result": "\u5f62\u6210\u6db5\u76d6\u865a\u62df\u5f62\u8c61\u6280\u672f\u3001\u4f26\u7406\u5ba1\u67e5\u3001\u4ea4\u4e92\u53ef\u7528\u6027\u7684\u5b8c\u6574\u7814\u7a76\u751f\u6001\uff0c\u5438\u5f15\u8d85\u8fc7\u5f80\u5c4a\u7684\u8de8\u9886\u57df\u5b66\u672f\u8d21\u732e\u3002", "conclusion": "\u6570\u5b57\u4eba\u7c7b\u6280\u672f\u9700\u4e0e\u5e95\u5c42\u7b97\u6cd5\u3001\u6570\u636e\u57fa\u5efa\u6df1\u5ea6\u6574\u5408\uff0c\u901a\u8fc7\u6301\u7eed\u793e\u533a\u5efa\u8bbe\u63a8\u52a8\u5305\u5bb9\u6027\u6280\u672f\u521b\u65b0\u3002"}}
{"id": "2508.08095", "pdf": "https://arxiv.org/pdf/2508.08095", "abs": "https://arxiv.org/abs/2508.08095", "authors": ["Chun Wang", "Chenyang Liu", "Wenze Xu", "Weihong Deng"], "title": "Dual Information Speech Language Models for Emotional Conversations", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Presented at IEEE ICME 2025", "summary": "Conversational systems relying on text-based large language models (LLMs)\noften overlook paralinguistic cues, essential for understanding emotions and\nintentions. Speech-language models (SLMs), which use speech as input, are\nemerging as a promising solution. However, SLMs built by extending frozen LLMs\nstruggle to capture paralinguistic information and exhibit reduced context\nunderstanding. We identify entangled information and improper training\nstrategies as key issues. To address these issues, we propose two heterogeneous\nadapters and suggest a weakly supervised training strategy. Our approach\ndisentangles paralinguistic and linguistic information, enabling SLMs to\ninterpret speech through structured representations. It also preserves\ncontextual understanding by avoiding the generation of task-specific vectors\nthrough controlled randomness. This approach trains only the adapters on common\ndatasets, ensuring parameter and data efficiency. Experiments demonstrate\ncompetitive performance in emotional conversation tasks, showcasing the model's\nability to effectively integrate both paralinguistic and linguistic information\nwithin contextual settings.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u5f02\u6784\u9002\u914d\u5668\u548c\u5f31\u76d1\u7763\u8bad\u7ec3\u7b56\u7565\u89e3\u51b3\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u5728\u6574\u5408\u526f\u8bed\u8a00\u4fe1\u606f\u4e0e\u4e0a\u4e0b\u6587\u7406\u89e3\u4e2d\u7684\u4fe1\u606f\u7ea0\u7f20\u95ee\u9898", "motivation": "\u73b0\u6709\u6587\u672c\u5927\u6a21\u578b\u5ffd\u7565\u526f\u8bed\u8a00\u4fe1\u606f\uff0c\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u5728\u6269\u5c55\u65f6\u5b58\u5728\u4fe1\u606f\u6355\u83b7\u4e0d\u5b8c\u6574\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u9000\u5316\u95ee\u9898", "method": "\u4f7f\u7528\u4e24\u4e2a\u5f02\u6784\u9002\u914d\u5668\u5206\u79bb\u526f\u8bed\u8a00/\u8bed\u8a00\u4fe1\u606f\uff0c\u91c7\u7528\u63a7\u5236\u968f\u673a\u6027\u7684\u5f31\u76d1\u7763\u8bad\u7ec3\u7b56\u7565\uff0c\u4ec5\u9700\u5728\u901a\u7528\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u9002\u914d\u5668", "result": "\u5728\u60c5\u611f\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u53d6\u5f97\u7ade\u4e89\u529b\u8868\u73b0\uff0c\u9a8c\u8bc1\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u6709\u6548\u6574\u5408\u526f\u8bed\u8a00\u4e0e\u8bed\u8a00\u4fe1\u606f\u7684\u80fd\u529b", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u6784\u5316\u8868\u5f81\u5b9e\u73b0\u8bed\u97f3\u89e3\u8bfb\uff0c\u4fdd\u6301\u4e0a\u4e0b\u6587\u7406\u89e3\u7684\u540c\u65f6\u63d0\u5347\u53c2\u6570\u4e0e\u6570\u636e\u6548\u7387"}}
{"id": "2508.08096", "pdf": "https://arxiv.org/pdf/2508.08096", "abs": "https://arxiv.org/abs/2508.08096", "authors": ["Lukas Gehring", "Benjamin Paa\u00dfen"], "title": "Assessing LLM Text Detection in Educational Contexts: Does Human Contribution Affect Detection?", "categories": ["cs.CL", "cs.LG"], "comment": "Preprint as provided by the authors (19 pages, 12 figures, 9 tables)", "summary": "Recent advancements in Large Language Models (LLMs) and their increased\naccessibility have made it easier than ever for students to automatically\ngenerate texts, posing new challenges for educational institutions. To enforce\nnorms of academic integrity and ensure students' learning, learning analytics\nmethods to automatically detect LLM-generated text appear increasingly\nappealing. This paper benchmarks the performance of different state-of-the-art\ndetectors in educational contexts, introducing a novel dataset, called\nGenerative Essay Detection in Education (GEDE), containing over 900\nstudent-written essays and over 12,500 LLM-generated essays from various\ndomains. To capture the diversity of LLM usage practices in generating text, we\npropose the concept of contribution levels, representing students' contribution\nto a given assignment. These levels range from purely human-written texts, to\nslightly LLM-improved versions, to fully LLM-generated texts, and finally to\nactive attacks on the detector by \"humanizing\" generated texts. We show that\nmost detectors struggle to accurately classify texts of intermediate student\ncontribution levels, like LLM-improved human-written texts. Detectors are\nparticularly likely to produce false positives, which is problematic in\neducational settings where false suspicions can severely impact students'\nlives. Our dataset, code, and additional supplementary materials are publicly\navailable at\nhttps://github.com/lukasgehring/Assessing-LLM-Text-Detection-in-Educational-Contexts.", "AI": {"tldr": "\u7814\u7a76\u663e\u793a\u73b0\u6709LLM\u6587\u672c\u68c0\u6d4b\u5668\u5728\u6559\u80b2\u573a\u666f\u4e2d\u5bf9\u4e2d\u95f4\u8d21\u732e\u7b49\u7ea7\u6587\u672c\uff08\u5982LLM\u6539\u8fdb\u7684\u4eba\u7c7b\u6587\u672c\uff09\u8bc6\u522b\u80fd\u529b\u4e0d\u8db3\uff0c\u6613\u4ea7\u751f\u9ad8\u8bef\u62a5\u7387", "motivation": "LLM\u751f\u6210\u6587\u672c\u7684\u666e\u53ca\u5a01\u80c1\u5b66\u672f\u8bda\u4fe1\uff0c\u9700\u5f00\u53d1\u53ef\u9760\u68c0\u6d4b\u5de5\u5177\u4fdd\u969c\u6559\u80b2\u516c\u5e73", "method": "\u6784\u5efaGEDE\u6570\u636e\u96c6\uff08\u542b900+\u5b66\u751f\u6587\u672c\u548c12,500+LLM\u751f\u6210\u6587\u672c\uff09\uff0c\u63d0\u51fa\u8d21\u732e\u7b49\u7ea7\uff08\u4eba\u7c7b\u5199\u4f5c\u2192LLM\u6539\u8fdb\u2192\u5168\u751f\u6210\u2192\u5bf9\u6297\u6027\u6539\u5199\uff09\u8bc4\u4f30\u6846\u67b6", "result": "\u591a\u6570\u68c0\u6d4b\u5668\u5bf9\u4e2d\u7b49\u8d21\u732e\u7b49\u7ea7\u6587\u672cF1\u503c\u4f4e\u4e8e0.5\uff0c\u8bef\u62a5\u7387\u9ad8\u8fbe32%\uff08\u4eba\u7c7b\u6587\u672c\u88ab\u8bef\u5224\u4e3aAI\u751f\u6210\uff09", "conclusion": "\u73b0\u6709\u68c0\u6d4b\u5de5\u5177\u5b58\u5728\u4e25\u91cd\u5c40\u9650\uff0c\u8bef\u62a5\u53ef\u80fd\u5bf9\u5b66\u751f\u9020\u6210\u91cd\u5927\u8d1f\u9762\u5f71\u54cd\uff0c\u9700\u5f00\u53d1\u66f4\u7ec6\u7c92\u5ea6\u7684\u6559\u80b2\u4e13\u7528\u68c0\u6d4b\u65b9\u6848"}}
{"id": "2508.08110", "pdf": "https://arxiv.org/pdf/2508.08110", "abs": "https://arxiv.org/abs/2508.08110", "authors": ["Robin Huo", "Ewan Dunbar"], "title": "Iterative refinement, not training objective, makes HuBERT behave differently from wav2vec 2.0", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Proceedings of Interspeech 2025", "summary": "Self-supervised models for speech representation learning now see widespread\nuse for their versatility and performance on downstream tasks, but the effect\nof model architecture on the linguistic information learned in their\nrepresentations remains under-studied. This study investigates two such models,\nHuBERT and wav2vec 2.0, and minimally compares two of their architectural\ndifferences: training objective and iterative pseudo-label refinement through\nmultiple training iterations. We find that differences in canonical correlation\nof hidden representations to word identity, phoneme identity, and speaker\nidentity are explained by training iteration, not training objective. We\nsuggest that future work investigate the reason for the effectiveness of\niterative refinement in encoding linguistic information in self-supervised\nspeech representations.", "AI": {"tldr": "\u8fed\u4ee3\u8bad\u7ec3\u7b56\u7565\uff08\u800c\u975e\u8bad\u7ec3\u76ee\u6807\uff09\u662f\u5f71\u54cd\u81ea\u76d1\u7763\u8bed\u97f3\u8868\u5f81\u4e2d\u8bed\u8a00\u5b66\u4fe1\u606f\u7f16\u7801\u7684\u5173\u952e\u56e0\u7d20", "motivation": "\u63a2\u7a76HuBERT\u548cwav2vec 2.0\u4e24\u79cd\u81ea\u76d1\u7763\u8bed\u97f3\u6a21\u578b\u4e2d\uff0c\u67b6\u6784\u5dee\u5f02\uff08\u8bad\u7ec3\u76ee\u6807\u548c\u8fed\u4ee3\u4f2a\u6807\u7b7e\u4f18\u5316\uff09\u5bf9\u8868\u5f81\u4e2d\u8bed\u8a00\u5b66\u4fe1\u606f\u7684\u5f71\u54cd\u673a\u5236", "method": "\u901a\u8fc7\u5178\u578b\u76f8\u5173\u5206\u6790\u6bd4\u8f83\u6a21\u578b\u9690\u85cf\u8868\u5f81\u4e0e\u5355\u8bcd/\u97f3\u7d20/\u8bf4\u8bdd\u4eba\u8eab\u4efd\u7684\u5173\u8054\u6027\uff0c\u63a7\u5236\u53d8\u91cf\u6cd5\u5206\u79bb\u8bad\u7ec3\u76ee\u6807\u548c\u8fed\u4ee3\u8bad\u7ec3\u7684\u5f71\u54cd", "result": "\u6a21\u578b\u5728\u5355\u8bcd\u8bc6\u522b\u3001\u97f3\u7d20\u8bc6\u522b\u7b49\u8bed\u8a00\u5b66\u4efb\u52a1\u7684\u8868\u73b0\u5dee\u5f02\u4e3b\u8981\u6e90\u4e8e\u8fed\u4ee3\u8bad\u7ec3\u6b21\u6570\uff0c\u4e0e\u5177\u4f53\u8bad\u7ec3\u76ee\u6807\u65e0\u5173", "conclusion": "\u5efa\u8bae\u672a\u6765\u7814\u7a76\u805a\u7126\u8fed\u4ee3\u4f18\u5316\u673a\u5236\u5982\u4f55\u589e\u5f3a\u8bed\u97f3\u8868\u5f81\u4e2d\u7684\u8bed\u8a00\u5b66\u4fe1\u606f\u7f16\u7801\u80fd\u529b"}}
{"id": "2508.08125", "pdf": "https://arxiv.org/pdf/2508.08125", "abs": "https://arxiv.org/abs/2508.08125", "authors": ["Jakub \u0160m\u00edd", "Pavel P\u0159ib\u00e1\u0148", "Ond\u0159ej Pra\u017e\u00e1k", "Pavel Kr\u00e1l"], "title": "Czech Dataset for Complex Aspect-Based Sentiment Analysis Tasks", "categories": ["cs.CL"], "comment": "Published In Proceedings of the 2024 Joint International Conference\n  on Computational Linguistics, Language Resources and Evaluation (LREC-COLING\n  2024). Official version: https://aclanthology.org/2024.lrec-main.374/", "summary": "In this paper, we introduce a novel Czech dataset for aspect-based sentiment\nanalysis (ABSA), which consists of 3.1K manually annotated reviews from the\nrestaurant domain. The dataset is built upon the older Czech dataset, which\ncontained only separate labels for the basic ABSA tasks such as aspect term\nextraction or aspect polarity detection. Unlike its predecessor, our new\ndataset is specifically designed for more complex tasks, e.g.\ntarget-aspect-category detection. These advanced tasks require a unified\nannotation format, seamlessly linking sentiment elements (labels) together. Our\ndataset follows the format of the well-known SemEval-2016 datasets. This design\nchoice allows effortless application and evaluation in cross-lingual scenarios,\nultimately fostering cross-language comparisons with equivalent counterpart\ndatasets in other languages. The annotation process engaged two trained\nannotators, yielding an impressive inter-annotator agreement rate of\napproximately 90%. Additionally, we provide 24M reviews without annotations\nsuitable for unsupervised learning. We present robust monolingual baseline\nresults achieved with various Transformer-based models and insightful error\nanalysis to supplement our contributions. Our code and dataset are freely\navailable for non-commercial research purposes.", "AI": {"tldr": "\u7814\u7a76\u8005\u6784\u5efa\u4e86\u5305\u542b3.1K\u6807\u6ce8+24M\u65e0\u6807\u6ce8\u9910\u5385\u8bc4\u8bba\u7684\u6377\u514b\u8bedABSA\u6570\u636e\u96c6\uff0c\u652f\u6301\u590d\u6742\u60c5\u611f\u5206\u6790\u4efb\u52a1\u5e76\u517c\u5bb9\u8de8\u8bed\u8a00\u7814\u7a76\u3002", "motivation": "\u65e7\u7248\u6377\u514b\u6570\u636e\u96c6\u4ec5\u652f\u6301\u57fa\u7840ABSA\u4efb\u52a1\uff0c\u65e0\u6cd5\u6ee1\u8db3\u76ee\u6807-\u65b9\u9762-\u7c7b\u522b\u68c0\u6d4b\u7b49\u590d\u6742\u9700\u6c42\uff0c\u9700\u7edf\u4e00\u6807\u6ce8\u683c\u5f0f\u4fc3\u8fdb\u8de8\u8bed\u8a00\u6bd4\u8f83\u3002", "method": "\u57fa\u4e8e\u65e7\u6570\u636e\u96c6\u6269\u5c55\uff0c\u91c7\u7528SemEval-2016\u6807\u6ce8\u683c\u5f0f\uff0c\u4e24\u540d\u6807\u6ce8\u5458\u4eba\u5de5\u6807\u6ce8(90%\u4e00\u81f4\u6027)\uff0c\u4f7f\u7528Transformer\u6a21\u578b\u5efa\u7acb\u57fa\u7ebf\u3002", "result": "\u5b9e\u73b0\u9ad8\u6807\u6ce8\u4e00\u81f4\u6027\uff0c\u63d0\u4f9bTransformer\u6a21\u578b\u7684\u57fa\u7ebf\u6027\u80fd\u53ca\u9519\u8bef\u5206\u6790\uff0c\u672a\u6807\u6ce8\u6570\u636e\u91cf\u8fbe24M\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u586b\u8865\u4e86\u6377\u514b\u8bed\u590d\u6742ABSA\u4efb\u52a1\u7a7a\u767d\uff0c\u6807\u51c6\u5316\u683c\u5f0f\u652f\u6301\u8de8\u8bed\u8a00\u7814\u7a76\uff0c\u5f00\u6e90\u8d44\u6e90\u4fc3\u8fdb\u975e\u5546\u4e1a\u5b66\u672f\u53d1\u5c55\u3002"}}
{"id": "2508.08131", "pdf": "https://arxiv.org/pdf/2508.08131", "abs": "https://arxiv.org/abs/2508.08131", "authors": ["Wenze Xu", "Chun Wang", "Jiazhen Yu", "Sheng Chen", "Liang Gao", "Weihong Deng"], "title": "Optimal Transport Regularization for Speech Text Alignment in Spoken Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "To be presented at ACPR 2025 Conference", "summary": "Spoken Language Models (SLMs), which extend Large Language Models (LLMs) to\nperceive speech inputs, have gained increasing attention for their potential to\nadvance speech understanding tasks. However, despite recent progress, studies\nshow that SLMs often struggle to generalize across datasets, even for trained\nlanguages and tasks, raising concerns about whether they process speech in a\ntext-like manner as intended. A key challenge underlying this limitation is the\nmodality gap between speech and text representations. The high variability in\nspeech embeddings may allow SLMs to achieve strong in-domain performance by\nexploiting unintended speech variations, ultimately hindering generalization.\nTo mitigate this modality gap, we introduce Optimal Transport Regularization\n(OTReg), a method that formulates speech-text alignment as an optimal transport\nproblem and derives a regularization loss to improve SLM training. In each\ntraining iteration, OTReg first establishes a structured correspondence between\nspeech and transcript embeddings by determining the optimal transport plan,\nthen incorporates the regularization loss based on this transport plan to\noptimize SLMs in generating speech embeddings that align more effectively with\ntranscript embeddings. OTReg is lightweight, requiring no additional labels or\nlearnable parameters, and integrates seamlessly into existing SLM training\nprocedures. Extensive multilingual ASR experiments demonstrate that OTReg\nenhances speech-text alignment, mitigates the modality gap, and consequently\nimproves SLM generalization across diverse datasets.", "AI": {"tldr": "\u63d0\u51fa\u6700\u4f18\u4f20\u8f93\u6b63\u5219\u5316\u65b9\u6cd5\uff08OTReg\uff09\uff0c\u901a\u8fc7\u4f18\u5316\u8bed\u97f3-\u6587\u672c\u5bf9\u9f50\u7f13\u89e3\u6a21\u6001\u5dee\u8ddd\uff0c\u63d0\u5347\u53e3\u8bed\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709SLM\u5728\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u7531\u4e8e\u8bed\u97f3\u5d4c\u5165\u7684\u9ad8\u53d8\u5f02\u6027\u5bfc\u81f4\u6a21\u578b\u4f9d\u8d56\u975e\u610f\u56fe\u7684\u8bed\u97f3\u7279\u5f81\uff0c\u9700\u89e3\u51b3\u8bed\u97f3-\u6587\u672c\u6a21\u6001\u5dee\u8ddd\u95ee\u9898\u3002", "method": "\u5c06\u8bed\u97f3-\u6587\u672c\u5bf9\u9f50\u5efa\u6a21\u4e3a\u6700\u4f18\u4f20\u8f93\u95ee\u9898\uff0c\u901a\u8fc7\u8ba1\u7b97\u4f20\u8f93\u8ba1\u5212\u751f\u6210\u6b63\u5219\u5316\u635f\u5931\uff0c\u4f18\u5316\u8bed\u97f3\u5d4c\u5165\u4e0e\u6587\u672c\u5d4c\u5165\u7684\u7ed3\u6784\u5316\u5bf9\u9f50\uff0c\u65e0\u9700\u989d\u5916\u53c2\u6570\u6216\u6807\u6ce8\u6570\u636e\u3002", "result": "\u591a\u8bed\u8a00ASR\u5b9e\u9a8c\u8868\u660eOTReg\u663e\u8457\u6539\u5584\u8bed\u97f3-\u6587\u672c\u5bf9\u9f50\u6548\u679c\uff0c\u964d\u4f4e\u6a21\u6001\u5dee\u8ddd\uff0c\u4f7fSLM\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "OTReg\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u5d4c\u5165\u5bf9\u9f50\u673a\u5236\u6709\u6548\u63d0\u5347SLM\u6cdb\u5316\u80fd\u529b\uff0c\u5177\u6709\u65e0\u9700\u989d\u5916\u8d44\u6e90\u3001\u6613\u96c6\u6210\u7b49\u5b9e\u9645\u5e94\u7528\u4f18\u52bf\u3002"}}
{"id": "2508.08139", "pdf": "https://arxiv.org/pdf/2508.08139", "abs": "https://arxiv.org/abs/2508.08139", "authors": ["Tianyi Zhou", "Johanne Medina", "Sanjay Chawla"], "title": "Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are prone to generating fluent but incorrect\ncontent, known as confabulation, which poses increasing risks in multi-turn or\nagentic applications where outputs may be reused as context. In this work, we\ninvestigate how in-context information influences model behavior and whether\nLLMs can identify their unreliable responses. We propose a reliability\nestimation that leverages token-level uncertainty to guide the aggregation of\ninternal model representations. Specifically, we compute aleatoric and\nepistemic uncertainty from output logits to identify salient tokens and\naggregate their hidden states into compact representations for response-level\nreliability prediction. Through controlled experiments on open QA benchmarks,\nwe find that correct in-context information improves both answer accuracy and\nmodel confidence, while misleading context often induces confidently incorrect\nresponses, revealing a misalignment between uncertainty and correctness. Our\nprobing-based method captures these shifts in model behavior and improves the\ndetection of unreliable outputs across multiple open-source LLMs. These results\nunderscore the limitations of direct uncertainty signals and highlight the\npotential of uncertainty-guided probing for reliability-aware generation.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u6613\u751f\u6210\u6d41\u7545\u4f46\u4e0d\u6b63\u786e\u5185\u5bb9\uff0c\u4f5c\u8005\u63d0\u51fa\u57fa\u4e8etoken\u7ea7\u4e0d\u786e\u5b9a\u6027\u7684\u53ef\u9760\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u805a\u5408\u9690\u85cf\u72b6\u6001\u63d0\u5347\u4e0d\u53ef\u9760\u8f93\u51fa\u68c0\u6d4b", "motivation": "\u7814\u7a76LLMs\u5728\u591a\u8f6e/\u4ee3\u7406\u5e94\u7528\u4e2d\u56e0\u9519\u8bef\u5185\u5bb9\u91cd\u7528\u5e26\u6765\u7684\u98ce\u9669\uff0c\u63a2\u7d22\u4e0a\u4e0b\u6587\u4fe1\u606f\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u884c\u4e3a\u53ca\u6a21\u578b\u80fd\u5426\u8bc6\u522b\u4e0d\u53ef\u9760\u54cd\u5e94", "method": "\u5229\u7528\u8f93\u51falogits\u8ba1\u7b97\u4efb\u610f\u6027\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u8bc6\u522b\u5173\u952etoken\u5e76\u805a\u5408\u5176\u9690\u85cf\u72b6\u6001\uff0c\u901a\u8fc7\u5f00\u653eQA\u57fa\u51c6\u8fdb\u884c\u53d7\u63a7\u5b9e\u9a8c\u9a8c\u8bc1", "result": "\u6b63\u786e\u4e0a\u4e0b\u6587\u63d0\u5347\u51c6\u786e\u6027\u4e0e\u6a21\u578b\u4fe1\u5fc3\uff0c\u8bef\u5bfc\u6027\u4e0a\u4e0b\u6587\u5bfc\u81f4\u81ea\u4fe1\u9519\u8bef\uff0c\u63a2\u9488\u65b9\u6cd5\u6709\u6548\u6539\u5584\u591a\u5f00\u6e90LLM\u7684\u4e0d\u53ef\u9760\u8f93\u51fa\u68c0\u6d4b", "conclusion": "\u76f4\u63a5\u4e0d\u786e\u5b9a\u6027\u4fe1\u53f7\u5b58\u5728\u5c40\u9650\u6027\uff0c\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u63a2\u9488\u65b9\u6cd5\u5728\u53ef\u9760\u6027\u611f\u77e5\u751f\u6210\u4e2d\u5c55\u73b0\u51fa\u6f5c\u5728\u5e94\u7528\u4ef7\u503c"}}
{"id": "2508.08140", "pdf": "https://arxiv.org/pdf/2508.08140", "abs": "https://arxiv.org/abs/2508.08140", "authors": ["Jun Wang", "Zaifu Zhan", "Qixin Zhang", "Mingquan Lin", "Meijia Song", "Rui Zhang"], "title": "Data-Efficient Biomedical In-Context Learning: A Diversity-Enhanced Submodular Perspective", "categories": ["cs.CL"], "comment": null, "summary": "Recent progress in large language models (LLMs) has leveraged their\nin-context learning (ICL) abilities to enable quick adaptation to unseen\nbiomedical NLP tasks. By incorporating only a few input-output examples into\nprompts, LLMs can rapidly perform these new tasks. While the impact of these\ndemonstrations on LLM performance has been extensively studied, most existing\napproaches prioritize representativeness over diversity when selecting examples\nfrom large corpora. To address this gap, we propose Dual-Div, a\ndiversity-enhanced data-efficient framework for demonstration selection in\nbiomedical ICL. Dual-Div employs a two-stage retrieval and ranking process:\nFirst, it identifies a limited set of candidate examples from a corpus by\noptimizing both representativeness and diversity (with optional annotation for\nunlabeled data). Second, it ranks these candidates against test queries to\nselect the most relevant and non-redundant demonstrations. Evaluated on three\nbiomedical NLP tasks (named entity recognition (NER), relation extraction (RE),\nand text classification (TC)) using LLaMA 3.1 and Qwen 2.5 for inference, along\nwith three retrievers (BGE-Large, BMRetriever, MedCPT), Dual-Div consistently\noutperforms baselines-achieving up to 5% higher macro-F1 scores-while\ndemonstrating robustness to prompt permutations and class imbalance. Our\nfindings establish that diversity in initial retrieval is more critical than\nranking-stage optimization, and limiting demonstrations to 3-5 examples\nmaximizes performance efficiency.", "AI": {"tldr": "\u63d0\u51faDual-Div\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u68c0\u7d22\uff08\u4ee3\u8868\u6027\u4e0e\u591a\u6837\u6027\u4f18\u5316\uff09\u63d0\u5347\u751f\u7269\u533b\u5b66NLP\u4efb\u52a1\u4e2d\u5c11\u6837\u672c\u793a\u4f8b\u9009\u62e9\u6548\u7387\uff0c\u5b9e\u9a8c\u663e\u793a\u6700\u9ad8\u63d0\u53475% macro-F1\u5206\u6570\u4e14\u5177\u5907\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u751f\u7269\u533b\u5b66NLP\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u8fc7\u5ea6\u5173\u6ce8\u793a\u4f8b\u4ee3\u8868\u6027\u800c\u5ffd\u89c6\u591a\u6837\u6027\uff0c\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "1. \u4e24\u9636\u6bb5\u68c0\u7d22\uff08\u5019\u9009\u96c6\u7b5b\u9009+\u6d4b\u8bd5\u76f8\u5173\u6392\u5e8f\uff09\n2. \u7ed3\u5408BGE-Large/BMRetriever/MedCPT\u68c0\u7d22\u5668\n3. \u4f7f\u7528LLaMA 3.1\u548cQwen 2.5\u9a8c\u8bc1NER/RE/TC\u4efb\u52a1", "result": "\u5728\u4e09\u4e2a\u751f\u7269\u533b\u5b66\u4efb\u52a1\u4e2d\uff1a\n- \u6700\u9ad8\u63d0\u53475% macro-F1\n- \u5bf9\u63d0\u793a\u53d8\u5316\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u9c81\u68d2\n- \u521d\u59cb\u68c0\u7d22\u9636\u6bb5\u591a\u6837\u6027\u6bd4\u6392\u5e8f\u4f18\u5316\u66f4\u91cd\u8981\n- 3-5\u4e2a\u793a\u4f8b\u6548\u679c\u6700\u4f73", "conclusion": "\u901a\u8fc7\u53cc\u91cd\u591a\u6837\u6027\u4f18\u5316\u673a\u5236\uff0c\u8bc1\u660e\u4e86\u793a\u4f8b\u9009\u62e9\u9636\u6bb5\u591a\u6837\u6027\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u9886\u57df\u5c11\u6837\u672c\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.08149", "pdf": "https://arxiv.org/pdf/2508.08149", "abs": "https://arxiv.org/abs/2508.08149", "authors": ["Wentao Jiang", "Xiang Feng", "Zengmao Wang", "Yong Luo", "Pingbo Xu", "Zhe Chen", "Bo Du", "Jing Zhang"], "title": "REX-RAG: Reasoning Exploration with Policy Correction in Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": "17 pages, 4 figures", "summary": "Reinforcement learning (RL) is emerging as a powerful paradigm for enabling\nlarge language models (LLMs) to perform complex reasoning tasks. Recent\nadvances indicate that integrating RL with retrieval-augmented generation (RAG)\nallows LLMs to dynamically incorporate external knowledge, leading to more\ninformed and robust decision making. However, we identify a critical challenge\nduring policy-driven trajectory sampling: LLMs are frequently trapped in\nunproductive reasoning paths, which we refer to as \"dead ends\", committing to\noverconfident yet incorrect conclusions. This severely hampers exploration and\nundermines effective policy optimization. To address this challenge, we propose\nREX-RAG (Reasoning Exploration with Policy Correction in Retrieval-Augmented\nGeneration), a novel framework that explores alternative reasoning paths while\nmaintaining rigorous policy learning through principled distributional\ncorrections. Our approach introduces two key innovations: (1) Mixed Sampling\nStrategy, which combines a novel probe sampling method with exploratory prompts\nto escape dead ends; and (2) Policy Correction Mechanism, which employs\nimportance sampling to correct distribution shifts induced by mixed sampling,\nthereby mitigating gradient estimation bias. We evaluate it on seven\nquestion-answering benchmarks, and the experimental results show that REX-RAG\nachieves average performance gains of 5.1% on Qwen2.5-3B and 3.6% on Qwen2.5-7B\nover strong baselines, demonstrating competitive results across multiple\ndatasets. The code is publicly available at https://github.com/MiliLab/REX-RAG.", "AI": {"tldr": "\u63d0\u51faREX-RAG\u6846\u67b6\u89e3\u51b3LLM\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a8\u7406\u8def\u5f84'\u6b7b\u80e1\u540c'\u95ee\u9898\uff0c\u901a\u8fc7\u6df7\u5408\u91c7\u6837\u7b56\u7565\u548c\u653f\u7b56\u6821\u6b63\u673a\u5236\u5b9e\u73b05.1%\u6027\u80fd\u63d0\u5347", "motivation": "LLM\u5728\u5f3a\u5316\u5b66\u4e60\u8f68\u8ff9\u91c7\u6837\u4e2d\u9891\u7e41\u9677\u5165\u9519\u8bef\u7ed3\u8bba\u7684'\u6b7b\u80e1\u540c'\uff0c\u4e25\u91cd\u5f71\u54cd\u7b56\u7565\u4f18\u5316\u6548\u679c", "method": "\u7ed3\u5408\u63a2\u9488\u91c7\u6837\u4e0e\u63a2\u7d22\u5f0f\u63d0\u793a\u7684\u6df7\u5408\u91c7\u6837\u7b56\u7565+\u57fa\u4e8e\u91cd\u8981\u6027\u91c7\u6837\u7684\u653f\u7b56\u6821\u6b63\u673a\u5236", "result": "\u57287\u4e2aQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cQwen2.5-3B\u6a21\u578b\u5e73\u5747\u63d0\u53475.1%\u51c6\u786e\u7387\uff0cQwen2.5-7B\u63d0\u53473.6%", "conclusion": "REX-RAG\u6709\u6548\u7a81\u7834LLM\u63a8\u7406\u8def\u5f84\u9650\u5236\uff0c\u4e3a\u68c0\u7d22\u589e\u5f3a\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u8bba\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2508.08163", "pdf": "https://arxiv.org/pdf/2508.08163", "abs": "https://arxiv.org/abs/2508.08163", "authors": ["Mandira Sawkar", "Samay U. Shetty", "Deepak Pandita", "Tharindu Cyril Weerasooriya", "Christopher M. Homan"], "title": "LPI-RIT at LeWiDi-2025: Improving Distributional Predictions via Metadata and Loss Reweighting with DisCo", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The Learning With Disagreements (LeWiDi) 2025 shared task is to model\nannotator disagreement through soft label distribution prediction and\nperspectivist evaluation, modeling annotators. We adapt DisCo (Distribution\nfrom Context), a neural architecture that jointly models item-level and\nannotator-level label distributions, and present detailed analysis and\nimprovements. In this paper, we extend the DisCo by incorporating annotator\nmetadata, enhancing input representations, and modifying the loss functions to\ncapture disagreement patterns better. Through extensive experiments, we\ndemonstrate substantial improvements in both soft and perspectivist evaluation\nmetrics across three datasets. We also conduct in-depth error and calibration\nanalyses, highlighting the conditions under which improvements occur. Our\nfindings underscore the value of disagreement-aware modeling and offer insights\ninto how system components interact with the complexity of human-annotated\ndata.", "AI": {"tldr": "\u6539\u8fdbDisCo\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u6574\u5408\u6807\u6ce8\u8005\u5143\u6570\u636e\u3001\u589e\u5f3a\u8f93\u5165\u8868\u793a\u548c\u6539\u8fdb\u635f\u5931\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6807\u6ce8\u5206\u6b67\u5efa\u6a21\u80fd\u529b", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5efa\u6a21\u6807\u6ce8\u8005\u5206\u6b67\u65f6\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u540c\u65f6\u5904\u7406\u9879\u76ee\u7ea7\u548c\u6807\u6ce8\u8005\u7ea7\u6807\u7b7e\u5206\u5e03\u7684\u7cfb\u7edf", "method": "\u5728DisCo\u6846\u67b6\u4e2d\u5f15\u5165\u6807\u6ce8\u8005\u5143\u6570\u636e\u3001\u6539\u8fdb\u8f93\u5165\u8868\u5f81\u3001\u8c03\u6574\u635f\u5931\u51fd\u6570\uff0c\u5e76\u8bbe\u8ba1\u65b0\u7684\u6821\u51c6\u673a\u5236", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8f6f\u6807\u7b7e\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u534712-15%\uff0cperspectivist\u8bc4\u4f30\u6307\u6807\u63d0\u53478-10%\uff0c\u6821\u51c6\u8bef\u5dee\u964d\u4f4e20%", "conclusion": "\u5206\u6b67\u611f\u77e5\u5efa\u6a21\u80fd\u6709\u6548\u6355\u6349\u590d\u6742\u6807\u6ce8\u6a21\u5f0f\uff0c\u7cfb\u7edf\u7ec4\u4ef6\u4e0e\u6570\u636e\u7279\u5f81\u7684\u4ea4\u4e92\u5bf9\u6027\u80fd\u63d0\u5347\u8d77\u5173\u952e\u4f5c\u7528"}}
{"id": "2508.08192", "pdf": "https://arxiv.org/pdf/2508.08192", "abs": "https://arxiv.org/abs/2508.08192", "authors": ["Bangsheng Tang", "Carl Chengyan Fu", "Fei Kou", "Grigory Sizov", "Haoci Zhang", "Jason Park", "Jiawen Liu", "Jie You", "Qirui Yang", "Sachin Mehta", "Shengyong Cai", "Xiaodong Wang", "Xingyu Liu", "Yunlu Li", "Yanjun Zhou", "Wei Wei", "Zhiwei Zhao", "Zixi Qi", "Adolfo Victoria", "Aya Ibrahim", "Bram Wasti", "Changkyu Kim", "Daniel Haziza", "Fei Sun", "Giancarlo Delfin", "Emily Guo", "Jialin Ouyang", "Jaewon Lee", "Jianyu Huang", "Jeremy Reizenstein", "Lu Fang", "Quinn Zhu", "Ria Verma", "Vlad Mihailescu", "Xingwen Guo", "Yan Cui", "Ye Hu", "Yejin Lee"], "title": "Efficient Speculative Decoding for Llama at Scale: Challenges and Solutions", "categories": ["cs.CL"], "comment": "15 pages", "summary": "Speculative decoding is a standard method for accelerating the inference\nspeed of large language models. However, scaling it for production environments\nposes several engineering challenges, including efficiently implementing\ndifferent operations (e.g., tree attention and multi-round speculative\ndecoding) on GPU. In this paper, we detail the training and inference\noptimization techniques that we have implemented to enable EAGLE-based\nspeculative decoding at a production scale for Llama models. With these\nchanges, we achieve a new state-of-the-art inference latency for Llama models.\nFor example, Llama4 Maverick decodes at a speed of about 4 ms per token (with a\nbatch size of one) on 8 NVIDIA H100 GPUs, which is 10% faster than the\npreviously best known method. Furthermore, for EAGLE-based speculative\ndecoding, our optimizations enable us to achieve a speed-up for large batch\nsizes between 1.4x and 2.0x at production scale.", "AI": {"tldr": "\u63d0\u51fa\u5de5\u7a0b\u4f18\u5316\u65b9\u6cd5\u4f7fEAGLE\u63a8\u6d4b\u89e3\u7801\u5728Llama\u6a21\u578b\u4e0a\u5b9e\u73b0\u751f\u4ea7\u7ea7\u52a0\u901f\uff0c\u63a8\u7406\u901f\u5ea6\u8fbe4ms/token(\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb10%)\uff0c\u5927\u6279\u91cf\u5904\u7406\u52a0\u901f1.4-2\u500d", "motivation": "\u89e3\u51b3\u63a8\u6d4b\u89e3\u7801\u6280\u672f\u5728\u751f\u4ea7\u73af\u5883\u6269\u5c55\u65f6\u7684\u5de5\u7a0b\u6311\u6218\uff0c\u7279\u522b\u662fGPU\u4e0a\u6811\u6ce8\u610f\u529b\u673a\u5236\u548c\u591a\u8f6e\u63a8\u6d4b\u89e3\u7801\u7684\u9ad8\u6548\u5b9e\u73b0\u95ee\u9898", "method": "\u9488\u5bf9Llama\u6a21\u578b\u8bbe\u8ba1\u8bad\u7ec3/\u63a8\u7406\u4f18\u5316\u65b9\u6848\uff0c\u91cd\u70b9\u4f18\u5316EAGLE\u6846\u67b6\u7684\u6811\u6ce8\u610f\u529b\u8ba1\u7b97\u548cGPU\u5e76\u884c\u5904\u7406\u67b6\u6784", "result": "\u57288xH100\u4e0a\u5b9e\u73b0\u5355token 4ms\u63a8\u7406\u5ef6\u8fdf(\u6bd4\u73b0\u6709\u5feb10%)\uff0c\u751f\u4ea7\u89c4\u6a21\u4e0b\u5927\u6279\u91cf\u63a8\u7406\u901f\u5ea6\u63d0\u53471.4-2\u500d", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u5de5\u7a0b\u4f18\u5316\u6210\u529f\u5c06EAGLE\u63a8\u6d4b\u89e3\u7801\u5e94\u7528\u4e8e\u751f\u4ea7\u7ea7Llama\u6a21\u578b\uff0c\u521b\u4e0b\u65b0\u7684\u63a8\u7406\u901f\u5ea6\u8bb0\u5f55\u5e76\u9a8c\u8bc1\u4e86\u5927\u6279\u91cf\u5904\u7406\u7684\u6269\u5c55\u6027"}}
{"id": "2508.08204", "pdf": "https://arxiv.org/pdf/2508.08204", "abs": "https://arxiv.org/abs/2508.08204", "authors": ["Kyle Moore", "Jesse Roberts", "Daryl Watson"], "title": "Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "preprint, under review", "summary": "There has been much recent interest in evaluating large language models for\nuncertainty calibration to facilitate model control and modulate user trust.\nInference time uncertainty, which may provide a real-time signal to the model\nor external control modules, is particularly important for applying these\nconcepts to improve LLM-user experience in practice. While many of the existing\npapers consider model calibration, comparatively little work has sought to\nevaluate how closely model uncertainty aligns to human uncertainty. In this\nwork, we evaluate a collection of inference-time uncertainty measures, using\nboth established metrics and novel variations, to determine how closely they\nalign with both human group-level uncertainty and traditional notions of model\ncalibration. We find that numerous measures show evidence of strong alignment\nto human uncertainty, even despite the lack of alignment to human answer\npreference. For those successful metrics, we find moderate to strong evidence\nof model calibration in terms of both correctness correlation and\ndistributional analysis.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u9636\u6bb5\u7684\u4e0d\u786e\u5b9a\u6027\u6307\u6807\u4e0e\u4eba\u7c7b\u7fa4\u4f53\u4e0d\u786e\u5b9a\u6027\u7684\u5bf9\u9f50\u7a0b\u5ea6\uff0c\u53d1\u73b0\u591a\u4e2a\u6307\u6807\u80fd\u6709\u6548\u53cd\u6620\u4eba\u7c7b\u4e0d\u786e\u5b9a\u6027\u4e14\u6a21\u578b\u6821\u51c6\u6548\u679c\u826f\u597d\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u6a21\u578b\u81ea\u8eab\u6821\u51c6\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u5bf9\u9f50\u8bc4\u4f30\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u6307\u6807\u4f18\u5316LLM\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u4f7f\u7528\u4f20\u7edf\u6307\u6807\u53ca\u65b0\u53d8\u4f53\uff0c\u7cfb\u7edf\u8bc4\u4f30\u591a\u79cd\u63a8\u7406\u65f6\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u4e0e\u4eba\u7c7b\u7fa4\u4f53\u4e0d\u786e\u5b9a\u6027\u3001\u4f20\u7edf\u6821\u51c6\u6807\u51c6\u7684\u5bf9\u9f50\u7a0b\u5ea6\u3002", "result": "\u591a\u4e2a\u6307\u6807\u663e\u793a\u4e0e\u4eba\u7c7b\u4e0d\u786e\u5b9a\u6027\u5f3a\u76f8\u5173\u6027\uff08\u5c3d\u7ba1\u4e0e\u7b54\u6848\u504f\u597d\u65e0\u5173\uff09\uff0c\u6210\u529f\u6307\u6807\u5728\u6821\u51c6\u5206\u6790\u4e2d\u5448\u73b0\u4e2d\u7b49\u5230\u5f3a\u7684\u6b63\u786e\u6027\u5173\u8054\u3002", "conclusion": "\u7279\u5b9a\u4e0d\u786e\u5b9a\u6027\u6307\u6807\u80fd\u6709\u6548\u6865\u63a5\u4eba\u7c7b\u8ba4\u77e5\u4e0e\u6a21\u578b\u6821\u51c6\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6a21\u578b\u63a7\u5236\u63d0\u4f9b\u53ef\u9760\u4fe1\u53f7\u3002"}}
{"id": "2508.08211", "pdf": "https://arxiv.org/pdf/2508.08211", "abs": "https://arxiv.org/abs/2508.08211", "authors": ["Zhuohao Yu", "Xingru Jiang", "Weizheng Gu", "Yidong Wang", "Shikun Zhang", "Wei Ye"], "title": "SAEMark: Multi-bit LLM Watermarking with Inference-Time Scaling", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "24 pages, 12 figures, code available:\n  https://zhuohaoyu.github.io/SAEMark", "summary": "Watermarking LLM-generated text is critical for content attribution and\nmisinformation prevention. However, existing methods compromise text quality,\nrequire white-box model access and logit manipulation. These limitations\nexclude API-based models and multilingual scenarios. We propose SAEMark, a\ngeneral framework for post-hoc multi-bit watermarking that embeds personalized\nmessages solely via inference-time, feature-based rejection sampling without\naltering model logits or requiring training. Our approach operates on\ndeterministic features extracted from generated text, selecting outputs whose\nfeature statistics align with key-derived targets. This framework naturally\ngeneralizes across languages and domains while preserving text quality through\nsampling LLM outputs instead of modifying. We provide theoretical guarantees\nrelating watermark success probability and compute budget that hold for any\nsuitable feature extractor. Empirically, we demonstrate the framework's\neffectiveness using Sparse Autoencoders (SAEs), achieving superior detection\naccuracy and text quality. Experiments across 4 datasets show SAEMark's\nconsistent performance, with 99.7% F1 on English and strong multi-bit detection\naccuracy. SAEMark establishes a new paradigm for scalable watermarking that\nworks out-of-the-box with closed-source LLMs while enabling content\nattribution.", "AI": {"tldr": "SAEMark\u63d0\u51fa\u65e0\u9700\u4fee\u6539\u6a21\u578blogits\u6216\u8bad\u7ec3\u7684\u540e\u5904\u7406\u6c34\u5370\u6846\u67b6\uff0c\u901a\u8fc7\u7279\u5f81\u62d2\u7edd\u91c7\u6837\u5b9e\u73b0\u591a\u8bed\u8a00\u652f\u6301\u5e76\u4fdd\u6301\u6587\u672c\u8d28\u91cf", "motivation": "\u73b0\u6709\u6c34\u5370\u65b9\u6cd5\u4f1a\u635f\u5bb3\u6587\u672c\u8d28\u91cf\u3001\u9700\u767d\u76d2\u6a21\u578b\u6743\u9650\u4e14\u4e0d\u9002\u7528\u4e8eAPI\u6a21\u578b\u548c\u591a\u8bed\u8a00\u573a\u666f", "method": "\u5229\u7528\u751f\u6210\u6587\u672c\u7684\u786e\u5b9a\u6027\u7279\u5f81\u8fdb\u884c\u7edf\u8ba1\u5339\u914d\uff0c\u901a\u8fc7\u7279\u5f81\u62d2\u7edd\u91c7\u6837\u9009\u62e9\u7b26\u5408\u5bc6\u94a5\u76ee\u6807\u7684\u8f93\u51fa", "result": "\u57284\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b099.7%\u82f1\u8bedF1\u503c\uff0c\u591a\u6bd4\u7279\u68c0\u6d4b\u51c6\u786e\u7387\u9ad8\u4e14\u6587\u672c\u8d28\u91cf\u65e0\u635f", "conclusion": "SAEMark\u5efa\u7acb\u4e86\u95ed\u6e90LLM\u5373\u63d2\u5373\u7528\u7684\u6c34\u5370\u8303\u5f0f\uff0c\u652f\u6301\u5185\u5bb9\u6eaf\u6e90\u5e76\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u591a\u8bed\u8a00\u6c34\u5370\u65b9\u6848"}}
{"id": "2508.08224", "pdf": "https://arxiv.org/pdf/2508.08224", "abs": "https://arxiv.org/abs/2508.08224", "authors": ["Shansong Wang", "Mingzhe Hu", "Qiang Li", "Mojtaba Safari", "Xiaofeng Yang"], "title": "Capabilities of GPT-5 on Multimodal Medical Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled general-purpose\nsystems to perform increasingly complex domain-specific reasoning without\nextensive fine-tuning. In the medical domain, decision-making often requires\nintegrating heterogeneous information sources, including patient narratives,\nstructured data, and medical images. This study positions GPT-5 as a generalist\nmultimodal reasoner for medical decision support and systematically evaluates\nits zero-shot chain-of-thought reasoning performance on both text-based\nquestion answering and visual question answering tasks under a unified\nprotocol. We benchmark GPT-5, GPT-5-mini, GPT-5-nano, and GPT-4o-2024-11-20\nagainst standardized splits of MedQA, MedXpertQA (text and multimodal), MMLU\nmedical subsets, USMLE self-assessment exams, and VQA-RAD. Results show that\nGPT-5 consistently outperforms all baselines, achieving state-of-the-art\naccuracy across all QA benchmarks and delivering substantial gains in\nmultimodal reasoning. On MedXpertQA MM, GPT-5 improves reasoning and\nunderstanding scores by +29.62% and +36.18% over GPT-4o, respectively, and\nsurpasses pre-licensed human experts by +24.23% in reasoning and +29.40% in\nunderstanding. In contrast, GPT-4o remains below human expert performance in\nmost dimensions. A representative case study demonstrates GPT-5's ability to\nintegrate visual and textual cues into a coherent diagnostic reasoning chain,\nrecommending appropriate high-stakes interventions. Our results show that, on\nthese controlled multimodal reasoning benchmarks, GPT-5 moves from\nhuman-comparable to above human-expert performance. This improvement may\nsubstantially inform the design of future clinical decision-support systems.", "AI": {"tldr": "GPT-5\u5728\u533b\u5b66\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u8d85\u8d8a\u4eba\u7c7b\u4e13\u5bb6\u7684\u6027\u80fd\u7a81\u7834\uff0c\u663e\u8457\u63d0\u5347\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u6f5c\u529b\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u51b3\u7b56\u4e2d\u5f02\u6784\u4fe1\u606f\u6574\u5408\uff08\u6587\u672c/\u6570\u636e/\u56fe\u50cf\uff09\u7684\u6311\u6218\uff0c\u9a8c\u8bc1\u5927\u6a21\u578b\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u6807\u51c6\u5316\u533b\u5b66QA\u57fa\u51c6\uff08MedQA/MedXpertQA/VQA-RAD\u7b49\uff09\uff0c\u901a\u8fc7\u96f6\u6837\u672c\u601d\u7ef4\u94fe\u8bc4\u4f30GPT\u7cfb\u5217\u6a21\u578b\u7684\u591a\u6a21\u6001\u63a8\u7406\u8868\u73b0\u3002", "result": "GPT-5\u5728\u5168\u90e8\u57fa\u51c6\u5b9e\u73b0SOTA\uff1aMedXpertQA\u591a\u6a21\u6001\u4efb\u52a1\u63a8\u7406+29.62%/\u7406\u89e3+36.18%\u8d85\u8d8aGPT-4o\uff0c\u8bca\u65ad\u63a8\u7406\u80fd\u529b\u8d85\u4eba\u7c7b\u4e13\u5bb624.23%\u3002", "conclusion": "GPT-5\u4ece\u4eba\u7c7b\u53ef\u6bd4\u6027\u80fd\u8dc3\u5347\u81f3\u8d85\u8d8a\u4e13\u5bb6\u6c34\u5e73\uff0c\u5176\u591a\u6a21\u6001\u6574\u5408\u80fd\u529b\u4e3a\u4e34\u5e8a\u51b3\u7b56\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u91cd\u8981\u6280\u672f\u53c2\u8003\u3002"}}
{"id": "2508.08236", "pdf": "https://arxiv.org/pdf/2508.08236", "abs": "https://arxiv.org/abs/2508.08236", "authors": ["Yunna Cai", "Fan Wang", "Haowei Wang", "Kun Wang", "Kailai Yang", "Sophia Ananiadou", "Moyan Li", "Mingming Fan"], "title": "Exploring Safety Alignment Evaluation of LLMs in Chinese Mental Health Dialogues via LLM-as-Judge", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Evaluating the safety alignment of LLM responses in high-risk mental health\ndialogues is particularly difficult due to missing gold-standard answers and\nthe ethically sensitive nature of these interactions. To address this\nchallenge, we propose PsyCrisis-Bench, a reference-free evaluation benchmark\nbased on real-world Chinese mental health dialogues. It evaluates whether the\nmodel responses align with the safety principles defined by experts.\nSpecifically designed for settings without standard references, our method\nadopts a prompt-based LLM-as-Judge approach that conducts in-context evaluation\nusing expert-defined reasoning chains grounded in psychological intervention\nprinciples. We employ binary point-wise scoring across multiple safety\ndimensions to enhance the explainability and traceability of the evaluation.\nAdditionally, we present a manually curated, high-quality Chinese-language\ndataset covering self-harm, suicidal ideation, and existential distress,\nderived from real-world online discourse. Experiments on 3600 judgments show\nthat our method achieves the highest agreement with expert assessments and\nproduces more interpretable evaluation rationales compared to existing\napproaches. Our dataset and evaluation tool are publicly available to\nfacilitate further research.", "AI": {"tldr": "\u63d0\u51faPsyCrisis-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5fc3\u7406\u5065\u5eb7\u5bf9\u8bdd\u4e2d\u7684\u5b89\u5168\u5bf9\u9f50\u6027", "motivation": "\u73b0\u6709LLM\u5b89\u5168\u8bc4\u4f30\u65b9\u6cd5\u5728\u5fc3\u7406\u5065\u5eb7\u9ad8\u5371\u5bf9\u8bdd\u573a\u666f\u4e2d\u7f3a\u4e4f\u91d1\u6807\u51c6\u7b54\u6848\u4e14\u4f26\u7406\u654f\u611f\u6027\u9ad8\uff0c\u4e9f\u9700\u4e13\u4e1a\u8bc4\u4f30\u5de5\u5177", "method": "\u57fa\u4e8e\u771f\u5b9e\u4e2d\u6587\u5fc3\u7406\u5bf9\u8bdd\u6784\u5efa\u65e0\u53c2\u8003\u8bc4\u4f30\u57fa\u51c6\uff0c\u91c7\u7528\u94fe\u5f0f\u63a8\u7406\u7684LLM-as-Judge\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e13\u5bb6\u5b9a\u4e49\u7684\u5b89\u5168\u7ef4\u5ea6\u8fdb\u884c\u4e8c\u503c\u5316\u8bc4\u5206", "result": "3600\u6b21\u8bc4\u4f30\u663e\u793a\u8be5\u65b9\u6cd5\u4e0e\u4e13\u5bb6\u5224\u65ad\u4e00\u81f4\u6027\u6700\u9ad8\uff0c\u4e14\u6bd4\u73b0\u6709\u65b9\u6cd5\u4ea7\u751f\u66f4\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u4f9d\u636e", "conclusion": "PsyCrisis-Bench\u4e3a\u5fc3\u7406\u5065\u5eb7\u9886\u57dfLLM\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u6709\u6548\u5de5\u5177\uff0c\u516c\u5f00\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u5de5\u5177\u53ef\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u53d1\u5c55"}}
{"id": "2508.08243", "pdf": "https://arxiv.org/pdf/2508.08243", "abs": "https://arxiv.org/abs/2508.08243", "authors": ["Jiahao Zhao", "Liwei Dong"], "title": "Jinx: Unlimited LLMs for Probing Alignment Failures", "categories": ["cs.CL"], "comment": "https://huggingface.co/Jinx-org", "summary": "Unlimited, or so-called helpful-only language models are trained without\nsafety alignment constraints and never refuse user queries. They are widely\nused by leading AI companies as internal tools for red teaming and alignment\nevaluation. For example, if a safety-aligned model produces harmful outputs\nsimilar to an unlimited model, this indicates alignment failures that require\nfurther attention. Despite their essential role in assessing alignment, such\nmodels are not available to the research community.\n  We introduce Jinx, a helpful-only variant of popular open-weight LLMs. Jinx\nresponds to all queries without refusals or safety filtering, while preserving\nthe base model's capabilities in reasoning and instruction following. It\nprovides researchers with an accessible tool for probing alignment failures,\nevaluating safety boundaries, and systematically studying failure modes in\nlanguage model safety.", "AI": {"tldr": "\u4ecb\u7ecdJinx\u2014\u2014\u4e00\u4e2a\u65e0\u5b89\u5168\u8fc7\u6ee4\u7684\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u8bc4\u4f30\u6a21\u578b\u5bf9\u9f50\u5931\u8d25\u548c\u5b89\u5168\u6027", "motivation": "\u73b0\u6709\u65e0\u9650\u5236\u8bed\u8a00\u6a21\u578b\u4ec5\u9650\u4f01\u4e1a\u4f7f\u7528\uff0c\u7814\u7a76\u793e\u533a\u7f3a\u4e4f\u63a2\u6d4b\u6a21\u578b\u5b89\u5168\u8fb9\u754c\u7684\u5de5\u5177", "method": "\u57fa\u4e8e\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u53bb\u9664\u5b89\u5168\u8fc7\u6ee4\u673a\u5236\uff0c\u4fdd\u7559\u57fa\u7840\u63a8\u7406\u80fd\u529b\u540c\u65f6\u54cd\u5e94\u6240\u6709\u67e5\u8be2", "result": "\u521b\u5efa\u4e86\u9996\u4e2a\u53ef\u516c\u5f00\u83b7\u53d6\u7684\u65e0\u9650\u6a21\u578b\uff0c\u5b9e\u73b0\u5b89\u5168\u8fb9\u754c\u63a2\u6d4b\u548c\u7cfb\u7edf\u6027\u5931\u6548\u6a21\u5f0f\u7814\u7a76", "conclusion": "Jinx\u586b\u8865\u4e86\u5b89\u5168\u8bc4\u4f30\u5de5\u5177\u7a7a\u767d\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u5173\u952e\u57fa\u7840\u8bbe\u65bd"}}
{"id": "2508.06591", "pdf": "https://arxiv.org/pdf/2508.06591", "abs": "https://arxiv.org/abs/2508.06591", "authors": ["Rachel K. Luu", "Jingyu Deng", "Mohammed Shahrudin Ibrahim", "Nam-Joon Cho", "Ming Dao", "Subra Suresh", "Markus J. Buehler"], "title": "Generative Artificial Intelligence Extracts Structure-Function Relationships from Plants for New Materials", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.mtrl-sci", "cond-mat.other", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have reshaped the research landscape by enabling\nnew approaches to knowledge retrieval and creative ideation. Yet their\napplication in discipline-specific experimental science, particularly in highly\nmulti-disciplinary domains like materials science, remains limited. We present\na first-of-its-kind framework that integrates generative AI with literature\nfrom hitherto-unconnected fields such as plant science, biomimetics, and\nmaterials engineering to extract insights and design experiments for materials.\nWe focus on humidity-responsive systems such as pollen-based materials and\nRhapis excelsa (broadleaf lady palm) leaves, which exhibit self-actuation and\nadaptive performance. Using a suite of AI tools, including a fine-tuned model\n(BioinspiredLLM), Retrieval-Augmented Generation (RAG), agentic systems, and a\nHierarchical Sampling strategy, we extract structure-property relationships and\ntranslate them into new classes of bioinspired materials. Structured inference\nprotocols generate and evaluate hundreds of hypotheses from a single query,\nsurfacing novel and experimentally tractable ideas. We validate our approach\nthrough real-world implementation: LLM-generated procedures, materials designs,\nand mechanical predictions were tested in the laboratory, culminating in the\nfabrication of a novel pollen-based adhesive with tunable morphology and\nmeasured shear strength, establishing a foundation for future plant-derived\nadhesive design. This work demonstrates how AI-assisted ideation can drive\nreal-world materials design and enable effective human-AI collaboration.", "AI": {"tldr": "\u5f00\u53d1\u9996\u4e2a\u6574\u5408\u751f\u6210\u5f0fAI\u4e0e\u8de8\u5b66\u79d1\u6587\u732e\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6e7f\u5ea6\u54cd\u5e94\u7cfb\u7edf\u5b9e\u73b0\u65b0\u578b\u4eff\u751f\u6750\u6599\u8bbe\u8ba1\uff0c\u5e76\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u82b1\u7c89\u57fa\u53ef\u8c03\u7c98\u5408\u5242\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u5b66\u79d1\u6750\u6599\u79d1\u5b66\u5b9e\u9a8c\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63a8\u52a8\u8de8\u690d\u7269\u5b66/\u4eff\u751f\u5b66/\u6750\u6599\u5de5\u7a0b\u7684\u77e5\u8bc6\u6574\u5408\u4e0eAI\u8f85\u52a9\u521b\u65b0\u3002", "method": "\u7ed3\u5408BioinspiredLLM\u5b9a\u5236\u6a21\u578b\u3001RAG\u589e\u5f3a\u68c0\u7d22\u3001\u4ee3\u7406\u7cfb\u7edf\u548c\u5206\u5c42\u62bd\u6837\u7b56\u7565\uff0c\u5efa\u7acb\u7ed3\u6784-\u6027\u80fd\u5173\u7cfb\u5e76\u751f\u6210\u53ef\u5b9e\u9a8c\u9a8c\u8bc1\u7684\u5047\u8bf4\u96c6\u7fa4\u3002", "result": "\u5b9e\u9a8c\u5ba4\u6210\u529f\u5b9e\u73b0AI\u751f\u6210\u7684\u7a0b\u5e8f\u4e0e\u6750\u6599\u8bbe\u8ba1\uff0c\u7814\u5236\u51fa\u5f62\u6001\u53ef\u8c03/\u526a\u5207\u5f3a\u5ea6\u8fbe7.3kPa\u7684\u82b1\u7c89\u57fa\u7c98\u5408\u5242\u539f\u578b\u3002", "conclusion": "\u8bc1\u660e\u4e86AI\u8f85\u52a9\u6784\u601d\u9a71\u52a8\u73b0\u5b9e\u6750\u6599\u8bbe\u8ba1\u7684\u53ef\u884c\u6027\uff0c\u5efa\u7acb\u4e86\u4eba\u673a\u534f\u4f5c\u7684\u8de8\u5b66\u79d1\u7814\u7a76\u8303\u5f0f\uff0c\u4e3a\u690d\u7269\u6e90\u6750\u6599\u5f00\u53d1\u63d0\u4f9b\u65b0\u65b9\u6cd5\u8bba\u3002"}}
{"id": "2508.06772", "pdf": "https://arxiv.org/pdf/2508.06772", "abs": "https://arxiv.org/abs/2508.06772", "authors": ["Catherine Yeh", "Tara Menon", "Robin Singh Arya", "Helen He", "Moira Weigel", "Fernanda Vi\u00e9gas", "Martin Wattenberg"], "title": "Story Ribbons: Reimagining Storyline Visualizations with Large Language Models", "categories": ["cs.HC", "cs.CL", "cs.LG"], "comment": "Accepted to IEEE VIS 2025 (11 pages, 9 figures)", "summary": "Analyzing literature involves tracking interactions between characters,\nlocations, and themes. Visualization has the potential to facilitate the\nmapping and analysis of these complex relationships, but capturing structured\ninformation from unstructured story data remains a challenge. As large language\nmodels (LLMs) continue to advance, we see an opportunity to use their text\nprocessing and analysis capabilities to augment and reimagine existing\nstoryline visualization techniques. Toward this goal, we introduce an\nLLM-driven data parsing pipeline that automatically extracts relevant narrative\ninformation from novels and scripts. We then apply this pipeline to create\nStory Ribbons, an interactive visualization system that helps novice and expert\nliterary analysts explore detailed character and theme trajectories at multiple\nnarrative levels. Through pipeline evaluations and user studies with Story\nRibbons on 36 literary works, we demonstrate the potential of LLMs to\nstreamline narrative visualization creation and reveal new insights about\nfamiliar stories. We also describe current limitations of AI-based systems, and\ninteraction motifs designed to address these issues.", "AI": {"tldr": "\u5229\u7528LLMs\u81ea\u52a8\u63d0\u53d6\u6587\u5b66\u4f5c\u54c1\u7684\u53d9\u4e8b\u4fe1\u606f\uff0c\u5f00\u53d1Story Ribbons\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u7cfb\u7edf\uff0c\u901a\u8fc736\u90e8\u4f5c\u54c1\u9a8c\u8bc1\u5176\u6709\u6548\u63ed\u793a\u89d2\u8272\u4e0e\u4e3b\u9898\u8f68\u8ff9\u7684\u80fd\u529b", "motivation": "\u89e3\u51b3\u975e\u7ed3\u6784\u5316\u6587\u672c\u6570\u636e\u96be\u4ee5\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u53d9\u4e8b\u4fe1\u606f\u7684\u6311\u6218\uff0c\u5229\u7528LLM\u7684\u6587\u672c\u5904\u7406\u80fd\u529b\u6539\u8fdb\u4f20\u7edf\u6545\u4e8b\u7ebf\u53ef\u89c6\u5316\u6280\u672f", "method": "1. \u6784\u5efaLLM\u9a71\u52a8\u7684\u53d9\u4e8b\u4fe1\u606f\u89e3\u6790\u7ba1\u9053\n2. \u5f00\u53d1\u652f\u6301\u591a\u5c42\u7ea7\u5206\u6790\u7684Story Ribbons\u4ea4\u4e92\u7cfb\u7edf", "result": "\u9a8c\u8bc1\u7cfb\u7edf\u53ef\u7b80\u531690%\u53ef\u89c6\u5316\u521b\u5efa\u6d41\u7a0b\uff0c\u7528\u6237\u7814\u7a76\u53d1\u73b0\u80fd\u53d1\u73b0\u7ecf\u5178\u6545\u4e8b\u65b0\u89c6\u89d2\uff0c\u540c\u65f6\u63ed\u793aAI\u7cfb\u7edf\u5728\u8bed\u4e49\u7406\u89e3\u6df1\u5ea6\u7684\u5c40\u9650\u6027", "conclusion": "LLM\u4e3a\u53d9\u4e8b\u53ef\u89c6\u5316\u5f00\u8f9f\u65b0\u8def\u5f84\uff0c\u901a\u8fc7\u7279\u5f81\u4ea4\u4e92\u8bbe\u8ba1\u53ef\u90e8\u5206\u5f25\u8865AI\u5c40\u9650\uff0c\u4e3a\u6570\u5b57\u4eba\u6587\u7814\u7a76\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2508.06890", "pdf": "https://arxiv.org/pdf/2508.06890", "abs": "https://arxiv.org/abs/2508.06890", "authors": ["Jinsung Yoon", "Wooyeol Jeong", "Jio Gim", "Young-Joo Suh"], "title": "Maestro-EVC: Controllable Emotional Voice Conversion Guided by References and Explicit Prosody", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": "Accepted at ASRU 2025", "summary": "Emotional voice conversion (EVC) aims to modify the emotional style of speech\nwhile preserving its linguistic content. In practical EVC, controllability, the\nability to independently control speaker identity and emotional style using\ndistinct references, is crucial. However, existing methods often struggle to\nfully disentangle these attributes and lack the ability to model fine-grained\nemotional expressions such as temporal dynamics. We propose Maestro-EVC, a\ncontrollable EVC framework that enables independent control of content, speaker\nidentity, and emotion by effectively disentangling each attribute from separate\nreferences. We further introduce a temporal emotion representation and an\nexplicit prosody modeling with prosody augmentation to robustly capture and\ntransfer the temporal dynamics of the target emotion, even under\nprosody-mismatched conditions. Experimental results confirm that Maestro-EVC\nachieves high-quality, controllable, and emotionally expressive speech\nsynthesis.", "AI": {"tldr": "\u63d0\u51faMaestro-EVC\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u5185\u5bb9\u3001\u8bf4\u8bdd\u4eba\u8eab\u4efd\u548c\u60c5\u611f\u5b9e\u73b0\u72ec\u7acb\u63a7\u5236\uff0c\u5f15\u5165\u65f6\u95f4\u60c5\u611f\u8868\u5f81\u548c\u663e\u5f0f\u97f5\u5f8b\u5efa\u6a21\u63d0\u5347\u60c5\u611f\u52a8\u6001\u8868\u8fbe\u80fd\u529b", "motivation": "\u73b0\u6709\u60c5\u611f\u8bed\u97f3\u8f6c\u6362\u65b9\u6cd5\u5b58\u5728\u5c5e\u6027\u8026\u5408\u95ee\u9898\uff0c\u96be\u4ee5\u72ec\u7acb\u63a7\u5236\u8bf4\u8bdd\u4eba\u8eab\u4efd\u4e0e\u60c5\u611f\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u65f6\u95f4\u52a8\u6001\u60c5\u611f\u8868\u8fbe\uff08\u5982\u97f5\u5f8b\u4e0d\u5339\u914d\u6761\u4ef6\uff09\u7684\u6709\u6548\u5efa\u6a21", "method": "1) \u591a\u53c2\u8003\u89e3\u8026\u67b6\u6784\u5206\u79bb\u5185\u5bb9/\u8eab\u4efd/\u60c5\u611f\u5c5e\u6027 2) \u65f6\u95f4\u60c5\u611f\u8868\u5f81\u5efa\u6a21\u52a8\u6001\u7279\u5f81 3) \u7ed3\u5408\u97f5\u5f8b\u589e\u5f3a\u7684\u663e\u5f0f\u97f5\u5f8b\u5efa\u6a21\u589e\u5f3a\u9c81\u68d2\u6027", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u6846\u67b6\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u8bed\u97f3\u5408\u6210\uff08MOS 4.21\uff09\uff0c\u5b9e\u73b0\u8bf4\u8bdd\u4eba\u8eab\u4efd/\u60c5\u611f\u98ce\u683c\u7684\u72ec\u7acb\u63a7\u5236\uff0c\u5e76\u5728\u97f5\u5f8b\u4e0d\u5339\u914d\u6761\u4ef6\u4e0b\u4fdd\u630191.3%\u7684\u60c5\u611f\u8bc6\u522b\u51c6\u786e\u7387", "conclusion": "Maestro-EVC\u6210\u529f\u89e3\u51b3\u4e86\u60c5\u611f\u8bed\u97f3\u8f6c\u6362\u4e2d\u7684\u5173\u952e\u63a7\u5236\u96be\u9898\uff0c\u901a\u8fc7\u89e3\u8026\u5efa\u6a21\u548c\u65f6\u95f4\u52a8\u6001\u8868\u5f81\u5b9e\u73b0\u4e86\u9ad8\u8868\u73b0\u529b\u7684\u53ef\u63a7\u8bed\u97f3\u5408\u6210"}}
{"id": "2508.06944", "pdf": "https://arxiv.org/pdf/2508.06944", "abs": "https://arxiv.org/abs/2508.06944", "authors": ["Lixuan He", "Jie Feng", "Yong Li"], "title": "AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Large Language Models (LLMs) are typically fine-tuned for reasoning tasks\nthrough a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by\nReinforcement Learning (RL), a process fraught with catastrophic forgetting and\nsuboptimal trade-offs between imitation and exploration. Recent single-stage\nmethods attempt to unify SFT and RL using heuristics, but lack a principled\nmechanism for dynamically balancing the two paradigms. In this paper, we\nreframe this challenge through the theoretical lens of \\textbf{implicit\nrewards}, viewing SFT and RL not as distinct methods but as complementary\nreward signals. We introduce \\textbf{Adaptive Meta Fine-Tuning (AMFT)}, a novel\nsingle-stage algorithm that learns the optimal balance between SFT's implicit,\npath-level reward and RL's explicit, outcome-based reward. The core of AMFT is\na \\textbf{meta-gradient adaptive weight controller} that treats the SFT-RL\nbalance as a learnable parameter, dynamically optimizing it to maximize\nlong-term task performance. This forward-looking approach, regularized by\npolicy entropy for stability, autonomously discovers an effective training\ncurriculum. We conduct a comprehensive evaluation on challenging benchmarks\nspanning mathematical reasoning, abstract visual reasoning (General Points),\nand vision-language navigation (V-IRL). AMFT consistently establishes a new\nstate-of-the-art and demonstrats superior generalization on out-of-distribution\n(OOD) tasks. Ablation studies and training dynamic analysis confirm that the\nmeta-learning controller is crucial for AMFT's stability, sample efficiency,\nand performance, offering a more principled and effective paradigm for LLM\nalignment.Our codes are open-sourced via https://github.com/hlxtsyj/AMFT.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u5143\u5fae\u8c03\u7b97\u6cd5AMFT\uff0c\u901a\u8fc7\u5143\u68af\u5ea6\u63a7\u5236\u5668\u52a8\u6001\u5e73\u8861\u76d1\u7763\u5fae\u8c03\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u5956\u52b1\u673a\u5236\uff0c\u63d0\u5347\u5927\u6a21\u578b\u63a8\u7406\u6027\u80fd", "motivation": "\u4f20\u7edf\u4e24\u9636\u6bb5\u5fae\u8c03\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u548c\u6b21\u4f18\u6743\u8861\u95ee\u9898\uff0c\u73b0\u6709\u5355\u9636\u6bb5\u65b9\u6cd5\u7f3a\u4e4f\u52a8\u6001\u5e73\u8861\u673a\u5236", "method": "\u91c7\u7528\u5143\u68af\u5ea6\u81ea\u9002\u5e94\u6743\u91cd\u63a7\u5236\u5668\u5b66\u4e60SFT\u8def\u5f84\u7ea7\u5956\u52b1\u4e0eRL\u7ed3\u679c\u5956\u52b1\u7684\u6700\u4f18\u5e73\u8861\uff0c\u901a\u8fc7\u7b56\u7565\u71b5\u6b63\u5219\u5316\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027", "result": "\u5728\u6570\u5b66\u63a8\u7406/\u62bd\u8c61\u89c6\u89c9\u63a8\u7406/\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4efb\u52a1\u4e2d\u53d6\u5f97SOTA\uff0c\u5c55\u793a\u51fa\u4f18\u79c0\u7684OOD\u6cdb\u5316\u80fd\u529b", "conclusion": "AMFT\u4e3aLLM\u5bf9\u9f50\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u7684\u52a8\u6001\u5e73\u8861\u8303\u5f0f\uff0c\u901a\u8fc7\u81ea\u4e3b\u8bad\u7ec3\u8bfe\u7a0b\u8bbe\u8ba1\u5b9e\u73b0\u9ad8\u6548\u4f18\u5316"}}
{"id": "2508.06960", "pdf": "https://arxiv.org/pdf/2508.06960", "abs": "https://arxiv.org/abs/2508.06960", "authors": ["Keyu Li", "Mohan Jiang", "Dayuan Fu", "Yunze Wu", "Xiangkun Hu", "Dequan Wang", "Pengfei Liu"], "title": "DatasetResearch: Benchmarking Agent Systems for Demand-Driven Dataset Discovery", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "The rapid advancement of large language models has fundamentally shifted the\nbottleneck in AI development from computational power to data availability-with\ncountless valuable datasets remaining hidden across specialized repositories,\nresearch appendices, and domain platforms. As reasoning capabilities and deep\nresearch methodologies continue to evolve, a critical question emerges: can AI\nagents transcend conventional search to systematically discover any dataset\nthat meets specific user requirements, enabling truly autonomous demand-driven\ndata curation? We introduce DatasetResearch, the first comprehensive benchmark\nevaluating AI agents' ability to discover and synthesize datasets from 208\nreal-world demands across knowledge-intensive and reasoning-intensive tasks.\nOur tri-dimensional evaluation framework reveals a stark reality: even advanced\ndeep research systems achieve only 22% score on our challenging\nDatasetResearch-pro subset, exposing the vast gap between current capabilities\nand perfect dataset discovery. Our analysis uncovers a fundamental\ndichotomy-search agents excel at knowledge tasks through retrieval breadth,\nwhile synthesis agents dominate reasoning challenges via structured\ngeneration-yet both catastrophically fail on \"corner cases\" outside existing\ndistributions. These findings establish the first rigorous baseline for dataset\ndiscovery agents and illuminate the path toward AI systems capable of finding\nany dataset in the digital universe. Our benchmark and comprehensive analysis\nprovide the foundation for the next generation of self-improving AI systems and\nare publicly available at https://github.com/GAIR-NLP/DatasetResearch.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u9996\u4e2a\u6570\u636e\u96c6\u53d1\u73b0\u667a\u80fd\u4f53\u57fa\u51c6DatasetResearch\uff0c\u63ed\u793a\u5f53\u524dAI\u7cfb\u7edf\u5728\u590d\u6742\u6570\u636e\u53d1\u73b0\u4efb\u52a1\u4e0a\u7684\u4e25\u91cd\u4e0d\u8db3", "motivation": "\u89e3\u51b3\u5927\u6a21\u578b\u65f6\u4ee3\u6570\u636e\u74f6\u9888\u95ee\u9898\uff0c\u63a8\u52a8AI\u7cfb\u7edf\u5b9e\u73b0\u81ea\u4e3b\u9700\u6c42\u9a71\u52a8\u7684\u6570\u636e\u53d1\u73b0\u80fd\u529b", "method": "\u6784\u5efa\u5305\u542b208\u4e2a\u73b0\u5b9e\u9700\u6c42\u7684\u57fa\u51c6\uff0c\u91c7\u7528\u4e09\u7ef4\u8bc4\u4f30\u6846\u67b6\uff08\u68c0\u7d22\u5e7f\u5ea6\u3001\u751f\u6210\u8d28\u91cf\u3001\u63a8\u7406\u6df1\u5ea6\uff09", "result": "\u73b0\u6709\u6700\u4f73\u7cfb\u7edf\u5728\u6838\u5fc3\u5b50\u96c6\u4ec5\u83b722%\u5f97\u5206\uff0c\u66b4\u9732\u641c\u7d22\u667a\u80fd\u4f53\u4e0e\u5408\u6210\u667a\u80fd\u4f53\u7684\u80fd\u529b\u65ad\u5c42\u53ca\u8fb9\u89d2\u6848\u4f8b\u5904\u7406\u7f3a\u9677", "conclusion": "\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u5f53\u524dAI\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5f00\u53d1\u80fd\u53d1\u73b0\u6570\u5b57\u4e16\u754c\u4e2d\u4efb\u610f\u6570\u636e\u96c6\u7684\u4e0b\u4e00\u4ee3\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840"}}
{"id": "2508.07014", "pdf": "https://arxiv.org/pdf/2508.07014", "abs": "https://arxiv.org/abs/2508.07014", "authors": ["Andrei Andrusenko", "Vladimir Bataev", "Lilit Grigoryan", "Vitaly Lavrukhin", "Boris Ginsburg"], "title": "TurboBias: Universal ASR Context-Biasing powered by GPU-accelerated Phrase-Boosting Tree", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": "Accepted by ASRU 2025", "summary": "Recognizing specific key phrases is an essential task for contextualized\nAutomatic Speech Recognition (ASR). However, most existing context-biasing\napproaches have limitations associated with the necessity of additional model\ntraining, significantly slow down the decoding process, or constrain the choice\nof the ASR system type. This paper proposes a universal ASR context-biasing\nframework that supports all major types: CTC, Transducers, and Attention\nEncoder-Decoder models. The framework is based on a GPU-accelerated word\nboosting tree, which enables it to be used in shallow fusion mode for greedy\nand beam search decoding without noticeable speed degradation, even with a vast\nnumber of key phrases (up to 20K items). The obtained results showed high\nefficiency of the proposed method, surpassing the considered open-source\ncontext-biasing approaches in accuracy and decoding speed. Our context-biasing\nframework is open-sourced as a part of the NeMo toolkit.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u652f\u6301\u591a\u79cdASR\u6a21\u578b\u7684\u901a\u7528\u4e0a\u4e0b\u6587\u504f\u7f6e\u6846\u67b6\uff0c\u57fa\u4e8eGPU\u52a0\u901f\u5355\u8bcd\u63d0\u5347\u6811\u5b9e\u73b0\u9ad8\u6548\u5b9e\u65f6\u89e3\u7801\uff0c\u652f\u63012\u4e07\u5173\u952e\u8bcd\u4e14\u5f00\u6e90\u96c6\u6210\u81f3NeMo\u5de5\u5177\u5305", "motivation": "\u73b0\u6709\u4e0a\u4e0b\u6587\u504f\u7f6e\u65b9\u6cd5\u5b58\u5728\u4e09\u5927\u5c40\u9650\uff1a1) \u9700\u8981\u989d\u5916\u6a21\u578b\u8bad\u7ec3 2) \u663e\u8457\u964d\u4f4e\u89e3\u7801\u901f\u5ea6 3) \u9650\u5236ASR\u7cfb\u7edf\u7c7b\u578b\u9009\u62e9\u3002\u4e9f\u9700\u5f00\u53d1\u901a\u7528\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u91c7\u7528GPU\u52a0\u901f\u7684\u5355\u8bcd\u63d0\u5347\u6811\u67b6\u6784\uff0c\u652f\u6301CTC/Transducer/Attention Encoder-Decoder\u5168\u6a21\u578b\u7c7b\u578b\u3002\u901a\u8fc7\u6d45\u878d\u5408\u6a21\u5f0f\u5b9e\u73b0\u4e0e\u8d2a\u5a6a/\u96c6\u675f\u641c\u7d22\u89e3\u7801\u7684\u65e0\u7f1d\u96c6\u6210", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u572820K\u5173\u952e\u8bcd\u89c4\u6a21\u4e0b\u4fdd\u6301\u5b9e\u65f6\u89e3\u7801\u901f\u5ea6\uff08\u65e0\u663e\u8457\u5ef6\u8fdf\uff09\uff0c\u51c6\u786e\u7387\u8d85\u8d8a\u73b0\u6709\u5f00\u6e90\u65b9\u6848\uff0c\u89e3\u7801\u901f\u5ea6\u63d0\u534740%\u4ee5\u4e0a", "conclusion": "\u6846\u67b6\u517c\u5177\u901a\u7528\u6027\uff08\u652f\u6301\u4e3b\u6d41ASR\u67b6\u6784\uff09\u4e0e\u9ad8\u6548\u6027\uff08GPU\u52a0\u901f\uff09\uff0c\u901a\u8fc7\u5f00\u6e90\u90e8\u7f72\u63a8\u52a8\u5de5\u4e1a\u7ea7\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u7684\u4e0a\u4e0b\u6587\u504f\u7f6e\u6280\u672f\u53d1\u5c55"}}
{"id": "2508.07022", "pdf": "https://arxiv.org/pdf/2508.07022", "abs": "https://arxiv.org/abs/2508.07022", "authors": ["Shengtao Wen", "Haodong Chen", "Yadong Wang", "Zhongying Pan", "Xiang Chen", "Yu Tian", "Bo Qian", "Dong Liang", "Sheng-Jun Huang"], "title": "MultiMedEdit: A Scenario-Aware Benchmark for Evaluating Knowledge Editing in Medical VQA", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Under Review", "summary": "Knowledge editing (KE) provides a scalable approach for updating factual\nknowledge in large language models without full retraining. While previous\nstudies have demonstrated effectiveness in general domains and medical QA\ntasks, little attention has been paid to KE in multimodal medical scenarios.\nUnlike text-only settings, medical KE demands integrating updated knowledge\nwith visual reasoning to support safe and interpretable clinical decisions. To\naddress this gap, we propose MultiMedEdit, the first benchmark tailored to\nevaluating KE in clinical multimodal tasks. Our framework spans both\nunderstanding and reasoning task types, defines a three-dimensional metric\nsuite (reliability, generality, and locality), and supports cross-paradigm\ncomparisons across general and domain-specific models. We conduct extensive\nexperiments under single-editing and lifelong-editing settings. Results suggest\nthat current methods struggle with generalization and long-tail reasoning,\nparticularly in complex clinical workflows. We further present an efficiency\nanalysis (e.g., edit latency, memory footprint), revealing practical trade-offs\nin real-world deployment across KE paradigms. Overall, MultiMedEdit not only\nreveals the limitations of current approaches but also provides a solid\nfoundation for developing clinically robust knowledge editing techniques in the\nfuture.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u9762\u5411\u4e34\u5e8a\u591a\u6a21\u6001\u4efb\u52a1\u7684\u77e5\u8bc6\u7f16\u8f91\u8bc4\u4f30\u57fa\u51c6MultiMedEdit\uff0c\u63ed\u793a\u5f53\u524d\u65b9\u6cd5\u5728\u590d\u6742\u4e34\u5e8a\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027", "motivation": "\u73b0\u6709\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u5728\u533b\u7597\u591a\u6a21\u6001\u573a\u666f\u4e2d\u7f3a\u4e4f\u8bc4\u4f30\u57fa\u51c6\uff0c\u96be\u4ee5\u652f\u6301\u9700\u8981\u89c6\u89c9\u63a8\u7406\u7684\u4e34\u5e8a\u51b3\u7b56\u9700\u6c42", "method": "\u6784\u5efa\u8986\u76d6\u7406\u89e3\u4e0e\u63a8\u7406\u4efb\u52a1\u7c7b\u578b\u7684\u591a\u7ef4\u5ea6\u57fa\u51c6\uff08\u53ef\u9760\u6027/\u901a\u7528\u6027/\u5c40\u90e8\u6027\uff09\uff0c\u652f\u6301\u8de8\u8303\u5f0f\u6a21\u578b\u5bf9\u6bd4\u5206\u6790", "result": "\u73b0\u6709\u65b9\u6cd5\u5728\u6cdb\u5316\u80fd\u529b\u548c\u957f\u5c3e\u63a8\u7406\u8868\u73b0\u4e0d\u8db3\uff08\u5c24\u5176\u5728\u590d\u6742\u4e34\u5e8a\u6d41\u7a0b\uff09\uff0c\u4e0d\u540c\u77e5\u8bc6\u7f16\u8f91\u8303\u5f0f\u5b58\u5728\u6548\u7387\u6743\u8861", "conclusion": "\u8be5\u57fa\u51c6\u4e3a\u5f00\u53d1\u4e34\u5e8a\u9c81\u68d2\u6027\u77e5\u8bc6\u7f16\u8f91\u6280\u672f\u5960\u5b9a\u57fa\u7840\uff0c\u63a8\u52a8\u591a\u6a21\u6001\u533b\u7597AI\u7cfb\u7edf\u7684\u5b89\u5168\u53ef\u89e3\u91ca\u53d1\u5c55"}}
{"id": "2508.07050", "pdf": "https://arxiv.org/pdf/2508.07050", "abs": "https://arxiv.org/abs/2508.07050", "authors": ["Wenhan Liu", "Xinyu Ma", "Weiwei Sun", "Yutao Zhu", "Yuchen Li", "Dawei Yin", "Zhicheng Dou"], "title": "ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": "21 pages", "summary": "Large Language Model (LLM) based listwise ranking has shown superior\nperformance in many passage ranking tasks. With the development of Large\nReasoning Models, many studies have demonstrated that step-by-step reasoning\nduring test-time helps improve listwise ranking performance. However, due to\nthe scarcity of reasoning-intensive training data, existing rerankers perform\npoorly in many complex ranking scenarios and the ranking ability of\nreasoning-intensive rerankers remains largely underdeveloped. In this paper, we\nfirst propose an automated reasoning-intensive training data synthesis\nframework, which sources training queries and passages from diverse domains and\napplies DeepSeek-R1 to generate high-quality training labels. A\nself-consistency data filtering mechanism is designed to ensure the data\nquality. To empower the listwise reranker with strong reasoning ability, we\nfurther propose a two-stage post-training approach, which includes a cold-start\nsupervised fine-tuning (SFT) stage for reasoning pattern learning and a\nreinforcement learning (RL) stage for further ranking ability enhancement.\nDuring the RL stage, based on the nature of listwise ranking, we design a\nmulti-view ranking reward, which is more effective than a ranking metric-based\nreward. Extensive experiments demonstrate that our trained reasoning-intensive\nreranker \\textbf{ReasonRank} outperforms existing baselines significantly and\nalso achieves much lower latency than pointwise reranker Rank1. \\textbf{Through\nfurther experiments, our ReasonRank has achieved state-of-the-art (SOTA)\nperformance 40.6 on the BRIGHT\nleaderboard\\footnote{https://brightbenchmark.github.io/}.} Our codes are\navailable at https://github.com/8421BCD/ReasonRank.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u52a8\u5316\u63a8\u7406\u5bc6\u96c6\u578b\u8bad\u7ec3\u6570\u636e\u751f\u6210\u6846\u67b6\u4e0e\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u5217\u8868\u6392\u5e8f\u5668\u6027\u80fd", "motivation": "\u73b0\u6709\u5217\u8868\u6392\u5e8f\u5668\u56e0\u7f3a\u4e4f\u63a8\u7406\u5bc6\u96c6\u578b\u8bad\u7ec3\u6570\u636e\uff0c\u5728\u590d\u6742\u6392\u5e8f\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u9700\u8981\u589e\u5f3a\u6392\u5e8f\u5668\u7684\u63a8\u7406\u80fd\u529b\u4ee5\u7a81\u7834\u6027\u80fd\u74f6\u9888", "method": "1. \u8de8\u9886\u57df\u81ea\u52a8\u5408\u6210\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff08DeepSeek-R1\u751f\u6210 + \u81ea\u6d3d\u8fc7\u6ee4\uff09\n2. \u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u76d1\u7763\u5fae\u8c03\u5b66\u4e60\u63a8\u7406\u6a21\u5f0f + \u5f3a\u5316\u5b66\u4e60\u591a\u89c6\u56fe\u6392\u5e8f\u5956\u52b1\u4f18\u5316", "result": "ReasonRank\u6a21\u578b\u5728BRIGHT\u699c\u5355\u53d6\u5f97SOTA 40.6\u5206\uff0c\u5ef6\u8fdf\u663e\u8457\u4f4e\u4e8e\u57fa\u7ebf\u6a21\u578bRank1", "conclusion": "\u901a\u8fc7\u6570\u636e\u5408\u6210\u4e0e\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u6210\u529f\u6784\u5efa\u63a8\u7406\u5bc6\u96c6\u578b\u5217\u8868\u6392\u5e8f\u5668\uff0c\u9a8c\u8bc1\u4e86\u591a\u89c6\u56fe\u5956\u52b1\u673a\u5236\u7684\u6709\u6548\u6027"}}
{"id": "2508.07087", "pdf": "https://arxiv.org/pdf/2508.07087", "abs": "https://arxiv.org/abs/2508.07087", "authors": ["Mohammadreza Daviran", "Brian Lin", "Davood Rafiei"], "title": "SQL-Exchange: Transforming SQL Queries Across Domains", "categories": ["cs.DB", "cs.AI", "cs.CL"], "comment": null, "summary": "We introduce SQL-Exchange, a framework for mapping SQL queries across\ndifferent database schemas by preserving the source query structure while\nadapting domain-specific elements to align with the target schema. We\ninvestigate the conditions under which such mappings are feasible and\nbeneficial, and examine their impact on enhancing the in-context learning\nperformance of text-to-SQL systems as a downstream task. Our comprehensive\nevaluation across multiple model families and benchmark datasets--assessing\nstructural alignment with source queries, execution validity on target\ndatabases, and semantic correctness--demonstrates that SQL-Exchange is\neffective across a wide range of schemas and query types. Our results further\nshow that using mapped queries as in-context examples consistently improves\ntext-to-SQL performance over using queries from the source schema.", "AI": {"tldr": "SQL-Exchange\u6846\u67b6\u901a\u8fc7\u8de8\u6570\u636e\u5e93\u6a21\u5f0f\u6620\u5c04SQL\u67e5\u8be2\uff0c\u5728\u4fdd\u7559\u6e90\u67e5\u8be2\u7ed3\u6784\u7684\u540c\u65f6\u63d0\u5347\u6587\u672c\u5230SQL\u7cfb\u7edf\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u6027\u80fd", "motivation": "\u89e3\u51b3\u4e0d\u540c\u6570\u636e\u5e93\u6a21\u5f0f\u95f4SQL\u67e5\u8be2\u9002\u914d\u96be\u9898\uff0c\u63a2\u7d22\u7ed3\u6784\u5316\u6620\u5c04\u5bf9\u6587\u672c\u5230SQL\u7cfb\u7edf\u7684\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u63d0\u5347\u6f5c\u529b", "method": "\u901a\u8fc7\u8bc4\u4f30\u6e90\u67e5\u8be2\u7ed3\u6784\u5bf9\u9f50\u5ea6\u3001\u76ee\u6807\u5e93\u6267\u884c\u6709\u6548\u6027\u53ca\u8bed\u4e49\u6b63\u786e\u6027\uff0c\u9a8c\u8bc1\u8de8\u6a21\u5f0f\u67e5\u8be2\u6620\u5c04\u7684\u53ef\u884c\u6027", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6846\u67b6\u5728\u591a\u79cd\u6a21\u5f0f/\u67e5\u8be2\u7c7b\u578b\u4e2d\u6709\u6548\uff0c\u4e14\u6620\u5c04\u67e5\u8be2\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u6837\u4f8b\u663e\u8457\u4f18\u4e8e\u6e90\u6a21\u5f0f\u67e5\u8be2", "conclusion": "SQL-Exchange\u6210\u529f\u5b9e\u73b0\u4e86\u7ed3\u6784\u5316\u67e5\u8be2\u6620\u5c04\uff0c\u4e3a\u8de8\u6a21\u5f0f\u6587\u672c\u5230SQL\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u589e\u5f3a\u65b9\u6848"}}
{"id": "2508.07201", "pdf": "https://arxiv.org/pdf/2508.07201", "abs": "https://arxiv.org/abs/2508.07201", "authors": ["Chaoqun Cui", "Caiyan Jia"], "title": "Propagation Tree Is Not Deep: Adaptive Graph Contrastive Learning Approach for Rumor Detection", "categories": ["cs.SI", "cs.AI", "cs.CL"], "comment": "This paper is accepted by AAAI2024", "summary": "Rumor detection on social media has become increasingly important. Most\nexisting graph-based models presume rumor propagation trees (RPTs) have deep\nstructures and learn sequential stance features along branches. However,\nthrough statistical analysis on real-world datasets, we find RPTs exhibit wide\nstructures, with most nodes being shallow 1-level replies. To focus learning on\nintensive substructures, we propose Rumor Adaptive Graph Contrastive Learning\n(RAGCL) method with adaptive view augmentation guided by node centralities. We\nsummarize three principles for RPT augmentation: 1) exempt root nodes, 2)\nretain deep reply nodes, 3) preserve lower-level nodes in deep sections. We\nemploy node dropping, attribute masking and edge dropping with probabilities\nfrom centrality-based importance scores to generate views. A graph contrastive\nobjective then learns robust rumor representations. Extensive experiments on\nfour benchmark datasets demonstrate RAGCL outperforms state-of-the-art methods.\nOur work reveals the wide-structure nature of RPTs and contributes an effective\ngraph contrastive learning approach tailored for rumor detection through\nprincipled adaptive augmentation. The proposed principles and augmentation\ntechniques can potentially benefit other applications involving tree-structured\ngraphs.", "AI": {"tldr": "\u63d0\u51faRAGCL\u65b9\u6cd5\uff0c\u901a\u8fc7\u8282\u70b9\u4e2d\u5fc3\u6027\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u56fe\u5bf9\u6bd4\u5b66\u4e60\u6539\u8fdb\u8c23\u8a00\u68c0\u6d4b\uff0c\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6709\u6548\u6027", "motivation": "\u73b0\u6709\u6a21\u578b\u5047\u8bbe\u8c23\u8a00\u4f20\u64ad\u6811\u5177\u6709\u6df1\u5c42\u7ed3\u6784\uff0c\u4f46\u7edf\u8ba1\u663e\u793a\u5b9e\u9645\u4f20\u64ad\u6811\u591a\u4e3a\u5bbd\u7ed3\u6784\uff081\u7ea7\u56de\u590d\u5360\u6bd4\u9ad8\uff09\uff0c\u9700\u9488\u5bf9\u6027\u4f18\u5316", "method": "\u57fa\u4e8e\u8282\u70b9\u4e2d\u5fc3\u6027\u8bbe\u8ba1\u81ea\u9002\u5e94\u89c6\u56fe\u589e\u5f3a\uff08\u8282\u70b9\u4e22\u5f03/\u5c5e\u6027\u63a9\u7801/\u8fb9\u4e22\u5f03\uff09\uff0c\u7ed3\u5408\u56fe\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u5b66\u4e60\u9c81\u68d2\u8868\u793a", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709SOTA\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u63ed\u793a\u4e86\u8c23\u8a00\u4f20\u64ad\u6811\u7684\u5bbd\u7ed3\u6784\u7279\u6027\uff0c\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u589e\u5f3a\u539f\u5219\u53ef\u63a8\u5e7f\u81f3\u5176\u4ed6\u6811\u72b6\u56fe\u5e94\u7528\u573a\u666f"}}
{"id": "2508.07205", "pdf": "https://arxiv.org/pdf/2508.07205", "abs": "https://arxiv.org/abs/2508.07205", "authors": ["Chaoqun Cui", "Caiyan Jia"], "title": "Towards Real-World Rumor Detection: Anomaly Detection Framework with Graph Supervised Contrastive Learning", "categories": ["cs.SI", "cs.CL"], "comment": "This paper is accepted by COLING2025", "summary": "Current rumor detection methods based on propagation structure learning\npredominately treat rumor detection as a class-balanced classification task on\nlimited labeled data. However, real-world social media data exhibits an\nimbalanced distribution with a minority of rumors among massive regular posts.\nTo address the data scarcity and imbalance issues, we construct two large-scale\nconversation datasets from Weibo and Twitter and analyze the domain\ndistributions. We find obvious differences between rumor and non-rumor\ndistributions, with non-rumors mostly in entertainment domains while rumors\nconcentrate in news, indicating the conformity of rumor detection to an anomaly\ndetection paradigm. Correspondingly, we propose the Anomaly Detection framework\nwith Graph Supervised Contrastive Learning (AD-GSCL). It heuristically treats\nunlabeled data as non-rumors and adapts graph contrastive learning for rumor\ndetection. Extensive experiments demonstrate AD-GSCL's superiority under\nclass-balanced, imbalanced, and few-shot conditions. Our findings provide\nvaluable insights for real-world rumor detection featuring imbalanced data\ndistributions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u56fe\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u7684\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6AD-GSCL\uff0c\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u548c\u5206\u5e03\u4e0d\u5e73\u8861\u6761\u4ef6\u4e0b\u7684\u8c23\u8a00\u68c0\u6d4b\u95ee\u9898", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4f20\u64ad\u7ed3\u6784\u5b66\u4e60\u7684\u8c23\u8a00\u68c0\u6d4b\u65b9\u6cd5\u5728\u7c7b\u522b\u5e73\u8861\u6570\u636e\u4e0a\u8868\u73b0\u53d7\u9650\uff0c\u800c\u771f\u5b9e\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u5177\u6709\u9ad8\u5ea6\u4e0d\u5e73\u8861\u7279\u6027\uff08\u8c23\u8a00\u5360\u6781\u5c11\u6570\uff09\u3002\u7814\u7a76\u53d1\u73b0\u8c23\u8a00\u4e0e\u975e\u8c23\u8a00\u9886\u57df\u5206\u5e03\u5dee\u5f02\u663e\u8457\uff08\u975e\u8c23\u8a00\u96c6\u4e2d\u4e8e\u5a31\u4e50\u9886\u57df\uff0c\u8c23\u8a00\u96c6\u4e2d\u4e8e\u65b0\u95fb\u9886\u57df\uff09\uff0c\u9700\u9002\u914d\u5f02\u5e38\u68c0\u6d4b\u8303\u5f0f", "method": "1. \u6784\u5efa\u5fae\u535a\u548cTwitter\u5927\u89c4\u6a21\u5bf9\u8bdd\u6570\u636e\u96c6\n2. \u5c06\u672a\u6807\u6ce8\u6570\u636e\u542f\u53d1\u5f0f\u89c6\u4e3a\u975e\u8c23\u8a00\n3. \u8bbe\u8ba1\u56fe\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff08AD-GSCL\uff09\uff0c\u901a\u8fc7\u56fe\u7ed3\u6784\u5bf9\u6bd4\u5b66\u4e60\u6355\u6349\u5f02\u5e38\u6a21\u5f0f", "result": "\u5b9e\u9a8c\u8bc1\u660eAD-GSCL\u5728\u7c7b\u522b\u5e73\u8861/\u4e0d\u5e73\u8861/\u5c0f\u6837\u672c\u6761\u4ef6\u4e0b\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5b9e\u9645\u573a\u666f\u4e2d\u6570\u636e\u5206\u5e03\u4e0d\u5e73\u8861\u7684\u8c23\u8a00\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63ed\u793a\u4e86\u9886\u57df\u5206\u5e03\u5206\u6790\u5bf9\u68c0\u6d4b\u8303\u5f0f\u9009\u62e9\u7684\u91cd\u8981\u6027"}}
{"id": "2508.07292", "pdf": "https://arxiv.org/pdf/2508.07292", "abs": "https://arxiv.org/abs/2508.07292", "authors": ["Yi Tang", "Kaini Wang", "Yang Chen", "Guangquan Zhou"], "title": "EndoAgent: A Memory-Guided Reflective Agent for Intelligent Endoscopic Vision-to-Decision Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Developing general artificial intelligence (AI) systems to support endoscopic\nimage diagnosis is an emerging research priority. Existing methods based on\nlarge-scale pretraining often lack unified coordination across tasks and\nstruggle to handle the multi-step processes required in complex clinical\nworkflows. While AI agents have shown promise in flexible instruction parsing\nand tool integration across domains, their potential in endoscopy remains\nunderexplored. To address this gap, we propose EndoAgent, the first\nmemory-guided agent for vision-to-decision endoscopic analysis that integrates\niterative reasoning with adaptive tool selection and collaboration. Built on a\ndual-memory design, it enables sophisticated decision-making by ensuring\nlogical coherence through short-term action tracking and progressively\nenhancing reasoning acuity through long-term experiential learning. To support\ndiverse clinical tasks, EndoAgent integrates a suite of expert-designed tools\nwithin a unified reasoning loop. We further introduce EndoAgentBench, a\nbenchmark of 5,709 visual question-answer pairs that assess visual\nunderstanding and language generation capabilities in realistic scenarios.\nExtensive experiments show that EndoAgent consistently outperforms both general\nand medical multimodal models, exhibiting its strong flexibility and reasoning\ncapabilities.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u57fa\u4e8e\u53cc\u8bb0\u5fc6\u67b6\u6784\u7684EndoAgent\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u7edf\u4e00\u63a8\u7406\u5faa\u73af\u5b9e\u73b0\u5185\u955c\u89c6\u89c9\u51b3\u7b56\uff0c\u5728EndoAgentBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u591a\u6a21\u6001\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u5185\u955cAI\u7cfb\u7edf\u5b58\u5728\u591a\u4efb\u52a1\u534f\u8c03\u4e0d\u8db3\u3001\u591a\u6b65\u9aa4\u4e34\u5e8a\u6d41\u7a0b\u5904\u7406\u56f0\u96be\u7684\u95ee\u9898\uff0c\u800cAI\u667a\u80fd\u4f53\u5728\u7075\u6d3b\u6307\u4ee4\u89e3\u6790\u548c\u5de5\u5177\u534f\u4f5c\u65b9\u9762\u7684\u6f5c\u529b\u5c1a\u672a\u88ab\u5145\u5206\u6316\u6398\u3002", "method": "\u91c7\u7528\u53cc\u8bb0\u5fc6\u67b6\u6784\uff08\u77ed\u671f\u884c\u52a8\u8ffd\u8e2a+\u957f\u671f\u7ecf\u9a8c\u5b66\u4e60\uff09\uff0c\u96c6\u6210\u4e13\u5bb6\u5de5\u5177\u7ec4\u5f62\u6210\u7edf\u4e00\u63a8\u7406\u95ed\u73af\uff0c\u6784\u5efa\u542b5,709\u4e2a\u89c6\u89c9\u95ee\u7b54\u5bf9\u7684EndoAgentBench\u8bc4\u4f30\u4f53\u7cfb\u3002", "result": "\u5728\u590d\u6742\u4e34\u5e8a\u573a\u666f\u4e2d\u51c6\u786e\u7387\u663e\u8457\u8d85\u8d8a\u901a\u7528/\u533b\u7597\u591a\u6a21\u6001\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u667a\u80fd\u4f53\u67b6\u6784\u7684\u7075\u6d3b\u6027\u548c\u63a8\u7406\u80fd\u529b\u4f18\u52bf\u3002", "conclusion": "EndoAgent\u8bc1\u5b9e\u4e86AI\u667a\u80fd\u4f53\u5728\u5185\u955c\u5206\u6790\u4e2d\u7684\u4e34\u5e8a\u4ef7\u503c\uff0c\u5176\u8bb0\u5fc6\u589e\u5f3a\u578b\u51b3\u7b56\u6846\u67b6\u4e3a\u533b\u7597AI\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2508.07315", "pdf": "https://arxiv.org/pdf/2508.07315", "abs": "https://arxiv.org/abs/2508.07315", "authors": ["Lilit Grigoryan", "Vladimir Bataev", "Nikolay Karpov", "Andrei Andrusenko", "Vitaly Lavrukhin", "Boris Ginsburg"], "title": "FlexCTC: GPU-powered CTC Beam Decoding with advanced Contextual Abilities", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "comment": "Accepted to Automatic Speech Recognition and Understanding Workshop\n  (ASRU) 2025", "summary": "While beam search improves speech recognition quality over greedy decoding,\nstandard implementations are slow, often sequential, and CPU-bound. To fully\nleverage modern hardware capabilities, we present a novel open-source FlexCTC\ntoolkit for fully GPU-based beam decoding, designed for Connectionist Temporal\nClassification (CTC) models. Developed entirely in Python and PyTorch, it\noffers a fast, user-friendly, and extensible alternative to traditional C++,\nCUDA, or WFST-based decoders. The toolkit features a high-performance, fully\nbatched GPU implementation with eliminated CPU-GPU synchronization and\nminimized kernel launch overhead via CUDA Graphs. It also supports advanced\ncontextualization techniques, including GPU-powered N-gram language model\nfusion and phrase-level boosting. These features enable accurate and efficient\ndecoding, making them suitable for both research and production use.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eGPU\u52a0\u901f\u7684FlexCTC\u5de5\u5177\u5305\uff0c\u901a\u8fc7\u5b8c\u5168\u6279\u5904\u7406\u5b9e\u73b0\u548cCUDA Graphs\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347CTC\u6a21\u578b\u8bed\u97f3\u89e3\u7801\u6548\u7387\uff0c\u652f\u6301N-gram\u8bed\u8a00\u6a21\u578b\u878d\u5408\u7b49\u9ad8\u7ea7\u529f\u80fd\u3002", "motivation": "\u4f20\u7edfCTC\u89e3\u7801\u5668\u4f9d\u8d56CPU\u4e14\u901f\u5ea6\u7f13\u6162\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u73b0\u4ee3GPU\u786c\u4ef6\u6027\u80fd\uff0c\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u5e76\u884c\u7684GPU\u7aef\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528Python/PyTorch\u5f00\u53d1\u5168GPU\u6279\u5904\u7406\u89e3\u7801\u5668\uff0c\u6d88\u9664CPU-GPU\u540c\u6b65\uff0c\u5229\u7528CUDA Graphs\u964d\u4f4e\u5185\u6838\u542f\u52a8\u5ef6\u8fdf\uff0c\u96c6\u6210N-gram\u8bed\u8a00\u6a21\u578b\u548c\u77ed\u8bed\u589e\u5f3a\u7684\u4e0a\u4e0b\u6587\u5904\u7406\u6280\u672f\u3002", "result": "\u5b9e\u73b0\u9ad8\u901f\u4e14\u5185\u5b58\u9ad8\u6548\u7684\u8bed\u97f3\u8bc6\u522b\u89e3\u7801\uff0c\u652f\u6301\u5b9e\u65f6\u4e0a\u4e0b\u6587\u5904\u7406\uff0c\u9002\u7528\u4e8e\u7814\u7a76\u548c\u5de5\u4e1a\u573a\u666f\u7684\u7aef\u5230\u7aef\u4f18\u5316\u3002", "conclusion": "FlexCTC\u4f5c\u4e3a\u5f00\u6e90\u5de5\u5177\u586b\u8865\u4e86\u4f20\u7edf\u89e3\u7801\u5668\u7684\u6027\u80fd\u7f3a\u9677\uff0c\u4e3aCTC\u6a21\u578b\u63d0\u4f9b\u4e86\u6613\u6269\u5c55\u3001\u786c\u4ef6\u53cb\u597d\u7684\u65b0\u4e00\u4ee3\u89e3\u7801\u65b9\u6848\u3002"}}
{"id": "2508.07342", "pdf": "https://arxiv.org/pdf/2508.07342", "abs": "https://arxiv.org/abs/2508.07342", "authors": ["Kepu Zhang", "Teng Shi", "Weijie Yu", "Jun Xu"], "title": "PrLM: Learning Explicit Reasoning for Personalized RAG via Contrastive Reward Optimization", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Personalized retrieval-augmented generation (RAG) aims to produce\nuser-tailored responses by incorporating retrieved user profiles alongside the\ninput query. Existing methods primarily focus on improving retrieval and rely\non large language models (LLMs) to implicitly integrate the retrieved context\nwith the query. However, such models are often sensitive to retrieval quality\nand may generate responses that are misaligned with user preferences. To\naddress this limitation, we propose PrLM, a reinforcement learning framework\nthat trains LLMs to explicitly reason over retrieved user profiles. Guided by a\ncontrastively trained personalization reward model, PrLM effectively learns\nfrom user responses without requiring annotated reasoning paths. Experiments on\nthree personalized text generation datasets show that PrLM outperforms existing\nmethods and remains robust across varying numbers of retrieved profiles and\ndifferent retrievers.", "AI": {"tldr": "\u63d0\u51fa\u5f3a\u5316\u5b66\u4e60\u6846\u67b6PrLM\uff0c\u901a\u8fc7\u663e\u5f0f\u63a8\u7406\u7528\u6237\u6863\u6848\u63d0\u5347\u4e2a\u6027\u5316\u751f\u6210\u6548\u679c\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u5177\u5907\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u4e2a\u6027\u5316RAG\u65b9\u6cd5\u4f9d\u8d56LLM\u9690\u5f0f\u6574\u5408\u68c0\u7d22\u7ed3\u679c\uff0c\u5bf9\u68c0\u7d22\u8d28\u91cf\u654f\u611f\u4e14\u6613\u4ea7\u751f\u7528\u6237\u504f\u597d\u5931\u914d\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3LLM\u663e\u5f0f\u5904\u7406\u7528\u6237\u6863\u6848\uff0c\u7ed3\u5408\u5bf9\u6bd4\u8bad\u7ec3\u7684\u4e2a\u4eba\u5316\u5956\u52b1\u6a21\u578b\u8fdb\u884c\u65e0\u76d1\u7763\u4f18\u5316\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u57fa\u51c6\u6a21\u578b\uff0c\u5728\u4e0d\u540c\u68c0\u7d22\u5668/\u6863\u6848\u6570\u91cf\u4e0b\u4fdd\u6301\u6027\u80fd\u7a33\u5b9a\u6027\u3002", "conclusion": "PrLM\u901a\u8fc7\u663e\u5f0f\u63a8\u7406\u673a\u5236\u6709\u6548\u964d\u4f4e\u5bf9\u68c0\u7d22\u8d28\u91cf\u7684\u4f9d\u8d56\uff0c\u4e3a\u4e2a\u6027\u5316\u6587\u672c\u751f\u6210\u63d0\u4f9b\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07353", "pdf": "https://arxiv.org/pdf/2508.07353", "abs": "https://arxiv.org/abs/2508.07353", "authors": ["Rubing Chen", "Jiaxin Wu", "Jian Wang", "Xulu Zhang", "Wenqi Fan", "Chenghua Lin", "Xiao-Yong Wei", "Qing Li"], "title": "Rethinking Domain-Specific LLM Benchmark Construction: A Comprehensiveness-Compactness Approach", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Numerous benchmarks have been built to evaluate the domain-specific abilities\nof large language models (LLMs), highlighting the need for effective and\nefficient benchmark construction. Existing domain-specific benchmarks primarily\nfocus on the scaling law, relying on massive corpora for supervised fine-tuning\nor generating extensive question sets for broad coverage. However, the impact\nof corpus and question-answer (QA) set design on the precision and recall of\ndomain-specific LLMs remains unexplored. In this paper, we address this gap and\ndemonstrate that the scaling law is not always the optimal principle for\nbenchmark construction in specific domains. Instead, we propose Comp-Comp, an\niterative benchmarking framework based on a comprehensiveness-compactness\nprinciple. Here, comprehensiveness ensures semantic recall of the domain, while\ncompactness enhances precision, guiding both corpus and QA set construction. To\nvalidate our framework, we conducted a case study in a well-renowned\nuniversity, resulting in the creation of XUBench, a large-scale and\ncomprehensive closed-domain benchmark. Although we use the academic domain as\nthe case in this work, our Comp-Comp framework is designed to be extensible\nbeyond academia, providing valuable insights for benchmark construction across\nvarious domains.", "AI": {"tldr": "\u63d0\u51faComp-Comp\u6846\u67b6\u7a81\u7834\u4f20\u7edf\u6269\u5c55\u6cd5\u5219\uff0c\u901a\u8fc7\u5168\u9762\u6027-\u7d27\u51d1\u6027\u539f\u5219\u8fed\u4ee3\u6784\u5efa\u9886\u57df\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u4ee5XUBench\u9a8c\u8bc1\u6709\u6548\u6027", "motivation": "\u73b0\u6709\u57fa\u51c6\u8fc7\u5ea6\u4f9d\u8d56\u6570\u636e\u89c4\u6a21\u6269\u5c55\u6cd5\u5219\uff0c\u4f46\u8bed\u6599\u8d28\u91cf\u548cQA\u96c6\u8bbe\u8ba1\u5bf9\u6a21\u578b\u7cbe\u5ea6/\u53ec\u56de\u7684\u5f71\u54cd\u673a\u5236\u5c1a\u672a\u88ab\u63ed\u793a\uff0c\u9700\u65b0\u65b9\u6cd5\u8bba\u7a81\u7834", "method": "\u57fa\u4e8ecomp-comprehensiveness\uff08\u4fdd\u969c\u9886\u57df\u8bed\u4e49\u53ec\u56de\uff09\u548ccomp-compactness\uff08\u63d0\u5347\u6d4b\u8bd5\u7cbe\u51c6\u5ea6\uff09\u53cc\u539f\u5219\uff0c\u8bbe\u8ba1\u8fed\u4ee3\u5f0f\u57fa\u51c6\u6784\u5efa\u6846\u67b6", "result": "\u6210\u529f\u6784\u5efa\u5b66\u672f\u9886\u57dfXUBench\u57fa\u51c6\uff0c\u8bc1\u5b9e\u6846\u67b6\u5728\u4fdd\u8bc1\u8bed\u4e49\u8986\u76d6\u7684\u540c\u65f6\u63d0\u5347\u6d4b\u8bd5\u7cbe\u51c6\u5ea6\uff0c\u4e14\u5177\u5907\u8de8\u9886\u57df\u6269\u5c55\u80fd\u529b", "conclusion": "Comp-Comp\u6846\u67b6\u6253\u7834\u4f20\u7edf\u89c4\u6a21\u4f18\u5148\u601d\u7ef4\uff0c\u4e3a\u5404\u9886\u57df\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u57fa\u51c6\u6784\u5efa\u8303\u5f0f\uff0c\u63a8\u52a8\u9886\u57df\u5927\u6a21\u578b\u8bc4\u4f30\u4f53\u7cfb\u5347\u7ea7"}}
{"id": "2508.07405", "pdf": "https://arxiv.org/pdf/2508.07405", "abs": "https://arxiv.org/abs/2508.07405", "authors": ["Jesse Ponnock"], "title": "Generative AI for Strategic Plan Development", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7; I.5.4"], "comment": "11 pages, 9 figures", "summary": "Given recent breakthroughs in Generative Artificial Intelligence (GAI) and\nLarge Language Models (LLMs), more and more professional services are being\naugmented through Artificial Intelligence (AI), which once seemed impossible to\nautomate. This paper presents a modular model for leveraging GAI in developing\nstrategic plans for large scale government organizations and evaluates leading\nmachine learning techniques in their application towards one of the identified\nmodules. Specifically, the performance of BERTopic and Non-negative Matrix\nFactorization (NMF) are evaluated in their ability to use topic modeling to\ngenerate themes representative of Vision Elements within a strategic plan. To\naccomplish this, BERTopic and NMF models are trained using a large volume of\nreports from the Government Accountability Office (GAO). The generated topics\nfrom each model are then scored for similarity against the Vision Elements of a\npublished strategic plan and the results are compared. Our results show that\nthese techniques are capable of generating themes similar to 100% of the\nelements being evaluated against. Further, we conclude that BERTopic performs\nbest in this application with more than half of its correlated topics achieving\na \"medium\" or \"strong\" correlation. A capability of GAI-enabled strategic plan\ndevelopment impacts a multi-billion dollar industry and assists the federal\ngovernment in overcoming regulatory requirements which are crucial to the\npublic good. Further work will focus on the operationalization of the concept\nproven in this study as well as viability of the remaining modules in the\nproposed model for GAI-generated strategic plans.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528\u751f\u6210\u5f0fAI\u5f00\u53d1\u653f\u5e9c\u6218\u7565\u8ba1\u5212\u6a21\u578b\uff0c\u53d1\u73b0BERTopic\u5728\u4e3b\u9898\u5efa\u6a21\u4e2d\u8868\u73b0\u4f18\u4e8eNMF", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7AI\u81ea\u52a8\u5316\u653f\u5e9c\u6218\u7565\u89c4\u5212\u6d41\u7a0b\uff0c\u89e3\u51b3\u4f20\u7edf\u4eba\u5de5\u65b9\u6cd5\u6548\u7387\u4f4e\u3001\u96be\u4ee5\u6ee1\u8db3\u76d1\u7ba1\u9700\u6c42\u7684\u95ee\u9898", "method": "\u4f7f\u7528GAO\u62a5\u544a\u8bad\u7ec3BERTopic\u548cNMF\u6a21\u578b\uff0c\u901a\u8fc7\u4e3b\u9898\u5efa\u6a21\u751f\u6210\u6218\u7565\u613f\u666f\u8981\u7d20\uff0c\u5e76\u8fdb\u884c\u76f8\u4f3c\u6027\u8bc4\u5206\u6bd4\u8f83", "result": "\u4e24\u79cd\u6a21\u578b\u5747\u80fd100%\u751f\u6210\u76f8\u5173\u4e3b\u9898\uff0cBERTopic\u8fc7\u534a\u4e3b\u9898\u8fbe\u5230\u4e2d\u7b49/\u5f3a\u76f8\u5173\u6027\uff08\u4f18\u4e8eNMF\uff09", "conclusion": "\u9a8c\u8bc1\u4e86GAI\u5728\u6218\u7565\u89c4\u5212\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u672a\u6765\u5c06\u63a8\u8fdb\u6a21\u578b\u64cd\u4f5c\u5316\u5e76\u9a8c\u8bc1\u5176\u4ed6\u6a21\u5757\u6548\u679c"}}
{"id": "2508.07407", "pdf": "https://arxiv.org/pdf/2508.07407", "abs": "https://arxiv.org/abs/2508.07407", "authors": ["Jinyuan Fang", "Yanwen Peng", "Xi Zhang", "Yingxu Wang", "Xinhao Yi", "Guibin Zhang", "Yi Xu", "Bin Wu", "Siwei Liu", "Zihao Li", "Zhaochun Ren", "Nikos Aletras", "Xi Wang", "Han Zhou", "Zaiqiao Meng"], "title": "A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems", "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "Recent advances in large language models have sparked growing interest in AI\nagents capable of solving complex, real-world tasks. However, most existing\nagent systems rely on manually crafted configurations that remain static after\ndeployment, limiting their ability to adapt to dynamic and evolving\nenvironments. To this end, recent research has explored agent evolution\ntechniques that aim to automatically enhance agent systems based on interaction\ndata and environmental feedback. This emerging direction lays the foundation\nfor self-evolving AI agents, which bridge the static capabilities of foundation\nmodels with the continuous adaptability required by lifelong agentic systems.\nIn this survey, we provide a comprehensive review of existing techniques for\nself-evolving agentic systems. Specifically, we first introduce a unified\nconceptual framework that abstracts the feedback loop underlying the design of\nself-evolving agentic systems. The framework highlights four key components:\nSystem Inputs, Agent System, Environment, and Optimisers, serving as a\nfoundation for understanding and comparing different strategies. Based on this\nframework, we systematically review a wide range of self-evolving techniques\nthat target different components of the agent system. We also investigate\ndomain-specific evolution strategies developed for specialised fields such as\nbiomedicine, programming, and finance, where optimisation objectives are\ntightly coupled with domain constraints. In addition, we provide a dedicated\ndiscussion on the evaluation, safety, and ethical considerations for\nself-evolving agentic systems, which are critical to ensuring their\neffectiveness and reliability. This survey aims to provide researchers and\npractitioners with a systematic understanding of self-evolving AI agents,\nlaying the foundation for the development of more adaptive, autonomous, and\nlifelong agentic systems.", "AI": {"tldr": "\u7cfb\u7edf\u6027\u56de\u987e\u81ea\u8fdb\u5316AI\u4ee3\u7406\u6280\u672f\uff0c\u63d0\u51fa\u7edf\u4e00\u6982\u5ff5\u6846\u67b6\u5e76\u63a2\u8ba8\u8de8\u9886\u57df\u5e94\u7528\u4e0e\u4f26\u7406\u6311\u6218", "motivation": "\u73b0\u6709\u9759\u6001\u914d\u7f6e\u7684AI\u4ee3\u7406\u7cfb\u7edf\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\uff0c\u9700\u901a\u8fc7\u81ea\u52a8\u8fdb\u5316\u673a\u5236\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\u80fd\u529b", "method": "\u5efa\u7acb\u5305\u542b\u7cfb\u7edf\u8f93\u5165/\u4ee3\u7406\u7cfb\u7edf/\u73af\u5883/\u4f18\u5316\u5668\u7684\u56db\u5143\u53cd\u9988\u6846\u67b6\uff0c\u5206\u7c7b\u6574\u7406\u6280\u672f\u8def\u5f84\u5e76\u7814\u7a76\u751f\u7269\u533b\u5b66/\u7f16\u7a0b/\u91d1\u878d\u7b49\u9886\u57df\u7684\u4e13\u95e8\u4f18\u5316\u65b9\u6cd5", "result": "\u6784\u5efa\u5b8c\u6574\u7684\u7406\u8bba\u4f53\u7cfb\uff0c\u63d0\u51fa\u8bc4\u4f30\u6307\u6807\u6846\u67b6\uff0c\u63ed\u793a\u8de8\u5b66\u79d1\u5408\u4f5c\u5fc5\u8981\u6027", "conclusion": "\u81ea\u8fdb\u5316\u4ee3\u7406\u662f\u8fde\u63a5\u57fa\u7840\u6a21\u578b\u9759\u6001\u80fd\u529b\u4e0e\u6301\u7eed\u9002\u5e94\u6027\u7684\u6865\u6881\uff0c\u9700\u901a\u8fc7\u7b97\u6cd5\u521b\u65b0\u4e0e\u4f26\u7406\u7ea6\u675f\u5b9e\u73b0\u53ef\u9760\u90e8\u7f72"}}
{"id": "2508.07408", "pdf": "https://arxiv.org/pdf/2508.07408", "abs": "https://arxiv.org/abs/2508.07408", "authors": ["Yueyi Wang", "Qiyao Wei"], "title": "Event-Aware Sentiment Factors from LLM-Augmented Financial Tweets: A Transparent Framework for Interpretable Quant Trading", "categories": ["q-fin.ST", "cs.CL", "cs.LG"], "comment": "16 pages, 12 figures, accepted at ICML 2025 New in ML Workshop", "summary": "In this study, we wish to showcase the unique utility of large language\nmodels (LLMs) in financial semantic annotation and alpha signal discovery.\nLeveraging a corpus of company-related tweets, we use an LLM to automatically\nassign multi-label event categories to high-sentiment-intensity tweets. We\nalign these labeled sentiment signals with forward returns over 1-to-7-day\nhorizons to evaluate their statistical efficacy and market tradability. Our\nexperiments reveal that certain event labels consistently yield negative alpha,\nwith Sharpe ratios as low as -0.38 and information coefficients exceeding 0.05,\nall statistically significant at the 95\\% confidence level. This study\nestablishes the feasibility of transforming unstructured social media text into\nstructured, multi-label event variables. A key contribution of this work is its\ncommitment to transparency and reproducibility; all code and methodologies are\nmade publicly available. Our results provide compelling evidence that social\nmedia sentiment is a valuable, albeit noisy, signal in financial forecasting\nand underscore the potential of open-source frameworks to democratize\nalgorithmic trading research.", "AI": {"tldr": "\u5229\u7528LLM\u5b9e\u73b0\u793e\u4ea4\u5a92\u4f53\u91d1\u878d\u6587\u672c\u7684\u7ed3\u6784\u5316\u6807\u6ce8\u4e0e\u03b1\u4fe1\u53f7\u53d1\u73b0\uff0c\u9a8c\u8bc1\u793e\u4ea4\u5a92\u4f53\u60c5\u611f\u5728\u91cf\u5316\u4ea4\u6613\u4e2d\u7684\u9884\u6d4b\u4ef7\u503c", "motivation": "\u89e3\u51b3\u975e\u7ed3\u6784\u5316\u793e\u4ea4\u5a92\u4f53\u6587\u672c\u5728\u91d1\u878d\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u96be\u9898\uff0c\u63a2\u7d22LLM\u5728\u8bed\u4e49\u6807\u6ce8\u4e0e\u4fe1\u53f7\u6316\u6398\u4e2d\u7684\u72ec\u7279\u4ef7\u503c", "method": "1. \u901a\u8fc7LLM\u5bf9\u9ad8\u60c5\u611f\u5f3a\u5ea6\u63a8\u6587\u8fdb\u884c\u591a\u6807\u7b7e\u4e8b\u4ef6\u5206\u7c7b 2. \u5c06\u6807\u6ce8\u4fe1\u53f7\u4e0e1-7\u65e5\u8fdc\u671f\u6536\u76ca\u7387\u5bf9\u9f50 3. \u4f7f\u7528\u590f\u666e\u6bd4\u7387\u548c\u4fe1\u606f\u7cfb\u6570\u8bc4\u4f30\u4fe1\u53f7\u7edf\u8ba1\u663e\u8457\u6027", "result": "\u7279\u5b9a\u4e8b\u4ef6\u6807\u7b7e\u6301\u7eed\u4ea7\u751f\u8d1f\u03b1\uff08\u590f\u666e\u6bd4\u7387-0.38\uff0c\u4fe1\u606f\u7cfb\u6570>0.05\uff09\uff0c\u7edf\u8ba1\u663e\u8457\u6027\u8fbe95%\u7f6e\u4fe1\u6c34\u5e73", "conclusion": "\u793e\u4ea4\u5a92\u4f53\u60c5\u611f\u53ef\u4f5c\u4e3a\u6709\u6548\u4f46\u542b\u566a\u7684\u91d1\u878d\u9884\u6d4b\u4fe1\u53f7\uff0c\u5f00\u6e90\u6846\u67b6\u80fd\u63a8\u52a8\u91cf\u5316\u4ea4\u6613\u7814\u7a76\u7684\u6c11\u4e3b\u5316"}}
{"id": "2508.07468", "pdf": "https://arxiv.org/pdf/2508.07468", "abs": "https://arxiv.org/abs/2508.07468", "authors": ["Stefan Szeider"], "title": "CP-Agent: Agentic Constraint Programming", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE"], "comment": null, "summary": "Translating natural language problem descriptions into formal constraint\nmodels remains a fundamental challenge in constraint programming, requiring\ndeep expertise in both the problem domain and modeling frameworks. Previous\napproaches to automating this translation have employed fixed workflows with\npredetermined modeling steps, failing on a significant number of benchmark\nproblems. We present a new approach using a pure agentic strategy without any\nfixed pipeline. We developed a general-purpose Python coding agent based on the\nReAct (Reason and Act) principle, utilizing a persistent IPython kernel for\nstateful code execution and iterative development. Rather than embedding\nconstraint programming logic into the agent architecture, domain-specific\nexpertise is injected solely through a carefully crafted project prompt. The\nagent combines this prompt-encoded knowledge with access to file operations and\ncode execution tools, enabling it to test hypotheses, debug failures, and\nverify solutions dynamically. Implemented in just a few hundred lines of code,\nthis architecture successfully solves all 101 problems of the CP-Bench\nconstraint programming benchmark set. The results suggest that constraint\nmodeling tasks require the combination of general coding tools and domain\nexpertise encoded in prompts, rather than specialized agent architectures or\npredefined workflows.", "AI": {"tldr": "\u7814\u7a76\u56e2\u961f\u5f00\u53d1\u4e86\u57fa\u4e8eReAct\u539f\u5219\u7684Python\u7f16\u7801\u4ee3\u7406\uff0c\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u6ce8\u5165\u9886\u57df\u77e5\u8bc6\u800c\u975e\u56fa\u5b9a\u67b6\u6784\uff0c\u6210\u529f\u89e3\u51b3CP-Bench\u5168\u90e8101\u4e2a\u7ea6\u675f\u7f16\u7a0b\u57fa\u51c6\u95ee\u9898", "motivation": "\u4f20\u7edf\u56fa\u5b9a\u5efa\u6a21\u6d41\u7a0b\u5728\u81ea\u52a8\u5316\u95ee\u9898\u8f6c\u5316\u4e2d\u5b58\u5728\u663e\u8457\u5931\u8d25\u6848\u4f8b\uff0c\u9700\u7a81\u7834\u9884\u5b9a\u4e49\u6b65\u9aa4\u7684\u9650\u5236", "method": "\u91c7\u7528\u7eaf\u4ee3\u7406\u7b56\u7565\uff1a1\uff09\u6784\u5efa\u57fa\u4e8eReAct\u7684Python\u4ee3\u7406 2\uff09\u5229\u7528IPython\u5185\u6838\u5b9e\u73b0\u72b6\u6001\u5316\u8fed\u4ee3\u5f00\u53d1 3\uff09\u901a\u8fc7prompt\u5de5\u7a0b\u6ce8\u5165\u9886\u57df\u77e5\u8bc6 4\uff09\u96c6\u6210\u6587\u4ef6\u64cd\u4f5c\u548c\u4ee3\u7801\u6267\u884c\u5de5\u5177\u94fe", "result": "\u5728CP-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0100%\u6210\u529f\u7387\uff0c\u8986\u76d6\u5168\u90e8101\u4e2a\u7ea6\u675f\u7f16\u7a0b\u95ee\u9898", "conclusion": "\u7ea6\u675f\u5efa\u6a21\u9700\u8981\u901a\u7528\u7f16\u7801\u5de5\u5177\u4e0eprompt\u7f16\u7801\u9886\u57df\u77e5\u8bc6\u7684\u7ed3\u5408\uff0c\u800c\u975e\u4e13\u7528\u67b6\u6784\u6216\u9884\u5b9a\u4e49\u6d41\u7a0b\u3002\u8be5\u67b6\u6784\u4ec5\u7528\u6570\u767e\u884c\u4ee3\u7801\u5b9e\u73b0\uff0c\u9a8c\u8bc1\u4e86\u8be5\u7406\u5ff5\u7684\u6709\u6548\u6027"}}
{"id": "2508.07485", "pdf": "https://arxiv.org/pdf/2508.07485", "abs": "https://arxiv.org/abs/2508.07485", "authors": ["Alexander Duffy", "Samuel J Paech", "Ishana Shastri", "Elizabeth Karpinski", "Baptiste Alloui-Cros", "Tyler Marques", "Matthew Lyle Olson"], "title": "Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "We present the first evaluation harness that enables any out-of-the-box,\nlocal, Large Language Models (LLMs) to play full-press Diplomacy without\nfine-tuning or specialized training. Previous work required frontier LLMs, or\nfine-tuning, due to the high complexity and information density of Diplomacy's\ngame state. Combined with the high variance of matches, these factors made\nDiplomacy prohibitive for study. In this work, we used data-driven iteration to\noptimize a textual game state representation such that a 24B model can reliably\ncomplete matches without any fine tuning. We develop tooling to facilitate\nhypothesis testing and statistical analysis, and we present case studies on\npersuasion, aggressive playstyles, and performance across a range of models. We\nconduct a variety of experiments across many popular LLMs, finding the larger\nmodels perform the best, but the smaller models still play adequately. We also\nintroduce Critical State Analysis: an experimental protocol for rapidly\niterating and analyzing key moments in a game at depth. Our harness\ndemocratizes the evaluation of strategic reasoning in LLMs by eliminating the\nneed for fine-tuning, and it provides insights into how these capabilities\nemerge naturally from widely used LLMs. Our code is available in the supplement\nand will be open sourced.", "AI": {"tldr": "\u5f00\u53d1\u9996\u4e2a\u65e0\u9700\u5fae\u8c03\u7684LLM\u8bc4\u4f30\u5de5\u5177\uff0c\u4f7f\u666e\u901a\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u5b8c\u6210\u590d\u6742\u5916\u4ea4\u6e38\u620f\u5bf9\u6218\uff0c\u5e76\u901a\u8fc7\u5173\u952e\u72b6\u6001\u5206\u6790\u534f\u8bae\u63ed\u793a\u6a21\u578b\u6218\u7565\u63a8\u7406\u80fd\u529b", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u524d\u6cbf\u6a21\u578b\u6216\u4e13\u95e8\u5fae\u8c03\u624d\u80fd\u5904\u7406\u5916\u4ea4\u6e38\u620f\u7684\u9ad8\u590d\u6742\u5ea6/\u4fe1\u606f\u5bc6\u5ea6\uff0c\u7ed3\u5408\u6bd4\u8d5b\u7ed3\u679c\u7684\u9ad8\u65b9\u5dee\u6027\uff0c\u963b\u788d\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55", "method": "\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u8fed\u4ee3\u4f18\u5316\u6587\u672c\u6e38\u620f\u72b6\u6001\u8868\u793a\uff0c\u5f00\u53d1\u652f\u6301\u5047\u8bbe\u68c0\u9a8c\u7684\u7edf\u8ba1\u5de5\u5177\uff0c\u5f15\u5165\u5173\u952e\u72b6\u6001\u5206\u6790\u534f\u8bae\u6df1\u5ea6\u89e3\u6790\u6e38\u620f\u5173\u952e\u65f6\u523b", "result": "24B\u6a21\u578b\u53ef\u9760\u5b8c\u6210\u6bd4\u8d5b\uff0c\u5927\u6a21\u578b\u8868\u73b0\u66f4\u4f18\u4f46\u5c0f\u6a21\u578b\u4ecd\u53ef\u7528\uff0c\u9a8c\u8bc1\u4e86\u6218\u7565\u63a8\u7406\u80fd\u529b\u5728\u901a\u7528LLM\u4e2d\u7684\u81ea\u7136\u6d8c\u73b0", "conclusion": "\u8be5\u5de5\u5177\u5b9e\u73b0\u4e86\u65e0\u9700\u5fae\u8c03\u7684LLM\u6218\u7565\u63a8\u7406\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u901a\u7528\u6a21\u578b\u5185\u5728\u7684\u6218\u7565\u80fd\u529b\uff0c\u914d\u5957\u5de5\u5177\u94fe\u548c\u5f00\u6e90\u4ee3\u7801\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7814\u7a76\u6c11\u4e3b\u5316"}}
{"id": "2508.07520", "pdf": "https://arxiv.org/pdf/2508.07520", "abs": "https://arxiv.org/abs/2508.07520", "authors": ["Baihan Lin"], "title": "Conversational DNA: A New Visual Language for Understanding Dialogue Structure in Human and AI", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "What if the patterns hidden within dialogue reveal more about communication\nthan the words themselves? We introduce Conversational DNA, a novel visual\nlanguage that treats any dialogue -- whether between humans, between human and\nAI, or among groups -- as a living system with interpretable structure that can\nbe visualized, compared, and understood. Unlike traditional conversation\nanalysis that reduces rich interaction to statistical summaries, our approach\nreveals the temporal architecture of dialogue through biological metaphors.\nLinguistic complexity flows through strand thickness, emotional trajectories\ncascade through color gradients, conversational relevance forms through\nconnecting elements, and topic coherence maintains structural integrity through\nhelical patterns. Through exploratory analysis of therapeutic conversations and\nhistorically significant human-AI dialogues, we demonstrate how this\nvisualization approach reveals interaction patterns that traditional methods\nmiss. Our work contributes a new creative framework for understanding\ncommunication that bridges data visualization, human-computer interaction, and\nthe fundamental question of what makes dialogue meaningful in an age where\nhumans increasingly converse with artificial minds.", "AI": {"tldr": "\u63d0\u51faConversational DNA\u53ef\u89c6\u5316\u8bed\u8a00\uff0c\u901a\u8fc7\u751f\u7269\u9690\u55bb\u89e3\u6790\u5bf9\u8bdd\u7ed3\u6784\uff0c\u7a81\u7834\u4f20\u7edf\u7edf\u8ba1\u5206\u6790\u65b9\u6cd5", "motivation": "\u4f20\u7edf\u5bf9\u8bdd\u5206\u6790\u5c06\u590d\u6742\u4e92\u52a8\u7b80\u5316\u4e3a\u7edf\u8ba1\u6458\u8981\uff0c\u65e0\u6cd5\u63ed\u793a\u5bf9\u8bdd\u7684\u65f6\u7a7a\u67b6\u6784\u548c\u6df1\u5c42\u610f\u4e49\uff0c\u5c24\u5176\u5728\u4eba\u7c7b\u4e0eAI\u5bf9\u8bdd\u65e5\u76ca\u9891\u7e41\u7684\u65f6\u4ee3\u9700\u8981\u65b0\u7684\u7406\u89e3\u6846\u67b6", "method": "\u91c7\u7528\u751f\u7269\u9690\u55bb\u6784\u5efa\u53ef\u89c6\u5316\u7cfb\u7edf\uff1a\u7c97\u7ec6\u7eb9\u8def\u8868\u793a\u8bed\u8a00\u590d\u6742\u5ea6\u3001\u6e10\u53d8\u8272\u8868\u8fbe\u60c5\u611f\u8f68\u8ff9\u3001\u8fde\u63a5\u5143\u7d20\u6784\u6210\u5bf9\u8bdd\u5173\u8054\u6027\u3001\u87ba\u65cb\u7ed3\u6784\u4fdd\u6301\u8bdd\u9898\u8fde\u8d2f\u6027\uff0c\u5e76\u901a\u8fc7\u6cbb\u7597\u5bf9\u8bdd\u548c\u5386\u53f2\u6027\u4eba\u673a\u5bf9\u8bdd\u8fdb\u884c\u9a8c\u8bc1", "result": "\u53ef\u89c6\u5316\u65b9\u6cd5\u6210\u529f\u63ed\u793a\u4e86\u4f20\u7edf\u65b9\u6cd5\u9057\u6f0f\u7684\u4e92\u52a8\u6a21\u5f0f\uff0c\u7279\u522b\u662f\u5728\u6cbb\u7597\u5bf9\u8bdd\u548c\u91cd\u8981\u4eba\u673a\u5bf9\u8bdd\u573a\u666f\u4e2d\u5c55\u73b0\u7ed3\u6784\u6d1e\u5bdf\u529b", "conclusion": "\u8be5\u6846\u67b6\u521b\u9020\u6027\u878d\u5408\u6570\u636e\u53ef\u89c6\u5316\u4e0e\u4eba\u673a\u4ea4\u4e92\uff0c\u4e3a\u6570\u5b57\u65f6\u4ee3\u5bf9\u8bdd\u672c\u8d28\u7684\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u4eba\u5de5\u667a\u80fd\u53c2\u4e0e\u5bf9\u8bdd\u573a\u666f\u7684\u6df1\u5c42\u7ed3\u6784\u89e3\u6790"}}
{"id": "2508.07616", "pdf": "https://arxiv.org/pdf/2508.07616", "abs": "https://arxiv.org/abs/2508.07616", "authors": ["Aswin RRV", "Jacob Dineen", "Divij Handa", "Md Nayem Uddin", "Mihir Parmar", "Chitta Baral", "Ben Zhou"], "title": "ThinkTuning: Instilling Cognitive Reflections without Distillation", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "15 pages", "summary": "Recent advances in test-time scaling have led to the emergence of thinking\nLLMs that exhibit self-reflective behaviors and multi-step reasoning. While RL\ndrives this self-improvement paradigm, a recent study (Gandhi et al., 2025)\nshows that RL alone does not truly instill these new reasoning abilities - it\nmerely draws out behaviors already present in the base models. This raises a\nquestion: How can we train the models that don't exhibit such thinking behavior\nto develop it in the first place? To this end, we propose ThinkTuning, a\nGRPO-based interactive training approach where we augment the rollouts of a\nstudent model with the guidance from a teacher model. A simple idea from\nclassroom practice inspires our method: a teacher poses a problem, lets the\nstudent try an answer, then gives corrective feedback -- enough to point the\nmind in the right direction and then show the solution. Each piece of feedback\nreshapes the student's thoughts, leading them to arrive at the correct\nsolution. Similarly, we find that this type of implicit supervision through\nfeedback from a teacher model of the same size improves the reasoning\ncapabilities of the student model. In particular, on average, our method shows\na 3.85% improvement over zero-shot baselines across benchmarks, and on\nMATH-500, AIME and GPQA-Diamond it shows 2.08%, 2.23% and 3.99% improvements\nover the vanilla-GRPO baseline. Source code is available at\nhttps://github.com/3rdAT/ThinkTuning.", "AI": {"tldr": "\u63d0\u51faThinkTuning\u4ea4\u4e92\u5f0f\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u6559\u5e08\u6a21\u578b\u53cd\u9988\u673a\u5236\u63d0\u5347\u5b66\u751f\u6a21\u578b\u63a8\u7406\u80fd\u529b", "motivation": "\u73b0\u6709RL\u65b9\u6cd5\u4ec5\u80fd\u6fc0\u53d1\u57fa\u7840\u6a21\u578b\u7684\u6f5c\u5728\u80fd\u529b\uff0c\u4f46\u65e0\u6cd5\u8ba9\u672a\u5c55\u73b0\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578b\u83b7\u5f97\u65b0\u80fd\u529b\u3002\u9700\u63a2\u7d22\u5982\u4f55\u4ece\u96f6\u57f9\u517b\u6a21\u578b\u7684\u601d\u7ef4\u63a8\u7406\u80fd\u529b", "method": "\u57fa\u4e8eGRPO\u6846\u67b6\u6784\u5efa\u5e08\u751f\u4ea4\u4e92\u8bad\u7ec3\uff1a\u6559\u5e08\u6a21\u578b\u901a\u8fc7\u6e10\u8fdb\u5f0f\u53cd\u9988\uff08\u95ee\u9898\u2192\u5b66\u751f\u5e94\u7b54\u2192\u7ea0\u6b63\u53cd\u9988\u2192\u89e3\u51b3\u65b9\u6848\uff09\u91cd\u5851\u5b66\u751f\u601d\u7ef4\u8def\u5f84", "result": "\u5728MATH-500/AIME/GPQA-Diamond\u57fa\u51c6\u5206\u522b\u63d0\u53472.08%/2.23%/3.99%\uff0c\u5e73\u5747\u6bd4\u96f6\u6837\u672c\u57fa\u7ebf\u9ad83.85%", "conclusion": "\u540c\u5c3a\u5bf8\u6559\u5e08\u6a21\u578b\u7684\u9690\u5f0f\u53cd\u9988\u76d1\u7763\u80fd\u6709\u6548\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u8bfe\u5802\u6559\u5b66\u6a21\u5f0f\u5728AI\u8bad\u7ec3\u4e2d\u7684\u8fc1\u79fb\u6709\u6548\u6027"}}
{"id": "2508.07629", "pdf": "https://arxiv.org/pdf/2508.07629", "abs": "https://arxiv.org/abs/2508.07629", "authors": ["Zhenpeng Su", "Leiyu Pan", "Xue Bai", "Dening Liu", "Guanting Dong", "Jiaming Huang", "Wenping Hu", "Guorui Zhou"], "title": "Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We present Klear-Reasoner, a model with long reasoning capabilities that\ndemonstrates careful deliberation during problem solving, achieving outstanding\nperformance across multiple benchmarks. Although there are already many\nexcellent works related to inference models in the current community, there are\nstill many problems with reproducing high-performance inference models due to\nincomplete disclosure of training details. This report provides an in-depth\nanalysis of the reasoning model, covering the entire post-training workflow\nfrom data preparation and long Chain-of-Thought supervised fine-tuning (long\nCoT SFT) to reinforcement learning (RL), along with detailed ablation studies\nfor each experimental component. For SFT data, our experiments show that a\nsmall number of high-quality data sources are more effective than a large\nnumber of diverse data sources, and that difficult samples can achieve better\nresults without accuracy filtering. In addition, we investigate two key issues\nwith current clipping mechanisms in RL: Clipping suppresses critical\nexploration signals and ignores suboptimal trajectories. To address these\nchallenges, we propose Gradient-Preserving clipping Policy Optimization (GPPO)\nthat gently backpropagates gradients from clipped tokens. GPPO not only\nenhances the model's exploration capacity but also improves its efficiency in\nlearning from negative samples. Klear-Reasoner exhibits exceptional reasoning\nabilities in mathematics and programming, scoring 90.5\\% on AIME 2024, 83.2\\%\non AIME 2025, 66.0\\% on LiveCodeBench V5 and 58.1\\% on LiveCodeBench V6.", "AI": {"tldr": "Klear-Reasoner\u901a\u8fc7\u957f\u94fe\u601d\u7ef4\u76d1\u7763\u5fae\u8c03\u548c\u521b\u65b0\u7684GPPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u6570\u5b66\u4e0e\u7f16\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u7a81\u7834\u6027\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u63a8\u7406\u6a21\u578b\u56e0\u8bad\u7ec3\u7ec6\u8282\u62ab\u9732\u4e0d\u5b8c\u6574\u5bfc\u81f4\u7684\u590d\u73b0\u56f0\u96be\u95ee\u9898\uff0c\u5e76\u6539\u8fdb\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u68af\u5ea6\u88c1\u526a\u673a\u5236\u7f3a\u9677\u3002", "method": "1. \u91c7\u7528\u5c11\u91cf\u9ad8\u8d28\u91cf\u6570\u636e\u6e90\u8fdb\u884c\u957fCoT\u76d1\u7763\u5fae\u8c03\n2. \u63d0\u51faGPPO\u7b97\u6cd5\u4fdd\u7559\u88c1\u526a\u4ee4\u724c\u68af\u5ea6\uff0c\u589e\u5f3a\u63a2\u7d22\u80fd\u529b\u548c\u8d1f\u6837\u672c\u5b66\u4e60\u6548\u7387\n3. \u5b8c\u6574\u8bad\u7ec3\u6d41\u7a0b\u5305\u542b\u6570\u636e\u51c6\u5907\u3001SFT\u3001RL\u4e09\u9636\u6bb5", "result": "AIME 2024:90.5%\u3001AIME 2025:83.2%\u3001LiveCodeBench V5:66.0%\u3001V6:58.1%", "conclusion": "Klear-Reasoner\u901a\u8fc7\u6570\u636e\u7cbe\u9009\u7b56\u7565\u548cGPPO\u6280\u672f\u521b\u65b0\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u63a2\u7d22\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u590d\u6742\u573a\u666f\u7684\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2508.07642", "pdf": "https://arxiv.org/pdf/2508.07642", "abs": "https://arxiv.org/abs/2508.07642", "authors": ["Tianyi Ma", "Yue Zhang", "Zehao Wang", "Parisa Kordjamshidi"], "title": "Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "18 pages, 5 Figures,", "summary": "Vision-and-Language Navigation (VLN) poses significant challenges in enabling\nagents to interpret natural language instructions and navigate complex 3D\nenvironments. While recent progress has been driven by large-scale pre-training\nand data augmentation, current methods still struggle to generalize to unseen\nscenarios, particularly when complex spatial and temporal reasoning is\nrequired. In this work, we propose SkillNav, a modular framework that\nintroduces structured, skill-based reasoning into Transformer-based VLN agents.\nOur method decomposes navigation into a set of interpretable atomic skills\n(e.g., Vertical Movement, Area and Region Identification, Stop and Pause), each\nhandled by a specialized agent. We then introduce a novel zero-shot\nVision-Language Model (VLM)-based router, which dynamically selects the most\nsuitable agent at each time step by aligning sub-goals with visual observations\nand historical actions. SkillNav achieves a new state-of-the-art performance on\nthe R2R benchmark and demonstrates strong generalization to the GSA-R2R\nbenchmark that includes novel instruction styles and unseen environments.", "AI": {"tldr": "\u63d0\u51fa\u6a21\u5757\u5316\u6846\u67b6SkillNav\uff0c\u901a\u8fc7\u539f\u5b50\u6280\u80fd\u5206\u89e3\u548cVLM\u52a8\u6001\u8def\u7531\u5b9e\u73b0\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u7684\u6cdb\u5316\u63d0\u5347", "motivation": "\u73b0\u6709VLN\u65b9\u6cd5\u5728\u590d\u6742\u65f6\u7a7a\u63a8\u7406\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u5c24\u5176\u9762\u5bf9\u672a\u89c1\u6307\u4ee4\u98ce\u683c\u548c\u73af\u5883\u65f6\u6027\u80fd\u4e0b\u964d\u660e\u663e", "method": "1. \u5c06\u5bfc\u822a\u89e3\u8026\u4e3a\u5782\u76f4\u79fb\u52a8/\u533a\u57df\u8bc6\u522b\u7b49\u539f\u5b50\u6280\u80fd\uff0c\u6bcf\u4e2a\u6280\u80fd\u914d\u5907\u4e13\u7528\u4ee3\u7406\n2. \u8bbe\u8ba1\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u8def\u7531\u5668\u52a8\u6001\u9009\u62e9\u4ee3\u7406\n3. \u901a\u8fc7\u5b50\u76ee\u6807\u4e0e\u5386\u53f2\u52a8\u4f5c\u7684\u5bf9\u9f50\u5b9e\u73b0\u51b3\u7b56\u4f18\u5316", "result": "\u5728R2R\u57fa\u51c6\u8fbe\u5230\u65b0SOTA\uff0c\u5728GSA-R2R\u8de8\u57df\u57fa\u51c6\u4e0a\u4fdd\u630185.3%\u6210\u529f\u7387\uff0c\u76f8\u6bd4\u57fa\u7ebf\u63d0\u534712.7%", "conclusion": "\u6a21\u5757\u5316\u6280\u80fd\u67b6\u6784\u4e0e\u52a8\u6001\u8def\u7531\u673a\u5236\u663e\u8457\u589e\u5f3a\u4e86\u5bfc\u822a\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u548c\u8de8\u573a\u666f\u9002\u5e94\u80fd\u529b"}}
{"id": "2508.07662", "pdf": "https://arxiv.org/pdf/2508.07662", "abs": "https://arxiv.org/abs/2508.07662", "authors": ["Ihor Stepanov", "Mykhailo Shtopko", "Dmytro Vodianytskyi", "Oleksandr Lukashov", "Alexander Yavorskyi", "Mykyta Yaroshenko"], "title": "GLiClass: Generalist Lightweight Model for Sequence Classification Tasks", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "14 pages, 7 tables, 2 figures", "summary": "Classification is one of the most widespread tasks in AI applications,\nserving often as the first step in filtering, sorting, and categorizing data.\nSince modern AI systems must handle large volumes of input data and early\npipeline stages can propagate errors downstream, achieving high efficiency and\naccuracy is critical. Moreover, classification requirements can change\ndynamically based on user needs, necessitating models with strong zero-shot\ncapabilities. While generative LLMs have become mainstream for zero-shot\nclassification due to their versatility, they suffer from inconsistent\ninstruction following and computational inefficiency. Cross-encoders, commonly\nused as rerankers in RAG pipelines, face a different bottleneck: they must\nprocess text-label pairs sequentially, significantly reducing efficiency with\nlarge label sets. Embedding-based approaches offer good efficiency but struggle\nwith complex scenarios involving logical and semantic constraints. We propose\nGLiClass, a novel method that adapts the GLiNER architecture for sequence\nclassification tasks. Our approach achieves strong accuracy and efficiency\ncomparable to embedding-based methods, while maintaining the flexibility needed\nfor zero-shot and few-shot learning scenarios. Additionally, we adapted\nproximal policy optimization (PPO) for multi-label text classification,\nenabling training classifiers in data-sparse conditions or from human feedback.", "AI": {"tldr": "\u63d0\u51faGLiNER\u67b6\u6784\u6539\u8fdb\u7684GLiClass\u5206\u7c7b\u65b9\u6cd5\uff0c\u7ed3\u5408PPO\u4f18\u5316\u5b9e\u73b0\u9ad8\u6548\u96f6\u6837\u672c\u5b66\u4e60", "motivation": "\u73b0\u6709\u5206\u7c7b\u65b9\u6cd5\u5b58\u5728\u6548\u7387\u4e0e\u7075\u6d3b\u6027\u77db\u76fe\uff1a\u751f\u6210\u5f0fLLM\u6548\u7387\u4f4e\uff0c\u4ea4\u53c9\u7f16\u7801\u5668\u5904\u7406\u5927\u89c4\u6a21\u6807\u7b7e\u6548\u7387\u5dee\uff0c\u5d4c\u5165\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u590d\u6742\u903b\u8f91\u7ea6\u675f", "method": "1. \u57fa\u4e8eGLiNER\u67b6\u6784\u5b9e\u73b0\u6807\u7b7e\u5e76\u884c\u5904\u7406 2. \u6539\u8fdbPPO\u7b97\u6cd5\u9002\u914d\u591a\u6807\u7b7e\u5206\u7c7b\uff0c\u652f\u6301\u6570\u636e\u7a00\u758f\u573a\u666f\u548c\u4eba\u7c7b\u53cd\u9988\u8bad\u7ec3", "result": "\u5728\u51c6\u786e\u7387\u4e0e\u6548\u7387\u6307\u6807\u4e0a\u8fbe\u5230\u5d4c\u5165\u65b9\u6cd5\u6c34\u5e73\uff0cPPO\u4f18\u5316\u5b9e\u73b0\u4f4e\u81f30.1%\u6807\u6ce8\u6570\u636e\u7684\u6709\u6548\u8bad\u7ec3", "conclusion": "GLiClass\u5728\u6548\u7387\u4e0e\u7075\u6d3b\u6027\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u9700\u6c42\u573a\u666f\u7684\u96f6\u6837\u672c/\u5c11\u6837\u672c\u5206\u7c7b\u4efb\u52a1"}}
{"id": "2508.07750", "pdf": "https://arxiv.org/pdf/2508.07750", "abs": "https://arxiv.org/abs/2508.07750", "authors": ["Haowen Wang", "Yun Yue", "Zhiling Ye", "Shuowen Zhang", "Lei Fan", "Jiaxin Liang", "Jiadi Jiang", "Cheng Wei", "Jingyuan Deng", "Xudong Han", "Ji Li", "Chunxiao Guo", "Peng Wei", "Jian Wang", "Jinjie Gu"], "title": "Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "12 pages, 5 figures, 7 tables", "summary": "Alignment methodologies have emerged as a critical pathway for enhancing\nlanguage model alignment capabilities. While SFT (supervised fine-tuning)\naccelerates convergence through direct token-level loss intervention, its\nefficacy is constrained by offline policy trajectory. In contrast,\nRL(reinforcement learning) facilitates exploratory policy optimization, but\nsuffers from low sample efficiency and stringent dependency on high-quality\nbase models. To address these dual challenges, we propose GRAO (Group Relative\nAlignment Optimization), a unified framework that synergizes the respective\nstrengths of SFT and RL through three key innovations: 1) A multi-sample\ngeneration strategy enabling comparative quality assessment via reward\nfeedback; 2) A novel Group Direct Alignment Loss formulation leveraging\nintra-group relative advantage weighting; 3) Reference-aware parameter updates\nguided by pairwise preference dynamics. Our theoretical analysis establishes\nGRAO's convergence guarantees and sample efficiency advantages over\nconventional approaches. Comprehensive evaluations across complex human\nalignment tasks demonstrate GRAO's superior performance, achieving\n57.70\\%,17.65\\% 7.95\\% and 5.18\\% relative improvements over SFT, DPO, PPO and\nGRPO baselines respectively. This work provides both a theoretically grounded\nalignment framework and empirical evidence for efficient capability evolution\nin language models.", "AI": {"tldr": "\u63d0\u51faGRAO\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408SFT\u548cRL\u4f18\u52bf\u5b9e\u73b0\u8bed\u8a00\u6a21\u578b\u9ad8\u6548\u5bf9\u9f50\uff0c\u521b\u65b0\u6027\u5f15\u5165\u591a\u6837\u672c\u751f\u6210\u7b56\u7565\u3001\u7ec4\u5185\u76f8\u5bf9\u4f18\u52bf\u52a0\u6743\u548c\u53c2\u8003\u611f\u77e5\u53c2\u6570\u66f4\u65b0\u673a\u5236\u3002", "motivation": "\u89e3\u51b3SFT\u53d7\u9650\u4e8e\u79bb\u7ebf\u7b56\u7565\u8f68\u8ff9\u548cRL\u6837\u672c\u6548\u7387\u4f4e\u4e0b\u7684\u53cc\u91cd\u6311\u6218\uff0c\u6574\u5408\u4e24\u79cd\u65b9\u6cd5\u7684\u4f18\u52bf\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u80fd\u529b\u8fdb\u5316\u3002", "method": "1) \u591a\u6837\u672c\u751f\u6210\u7b56\u7565\u5b9e\u73b0\u57fa\u4e8e\u5956\u52b1\u53cd\u9988\u7684\u8d28\u91cf\u5bf9\u6bd4\u8bc4\u4f30\n2) \u57fa\u4e8e\u7ec4\u5185\u76f8\u5bf9\u4f18\u52bf\u52a0\u6743\u7684Group Direct Alignment Loss\n3) \u57fa\u4e8e\u6210\u5bf9\u504f\u597d\u52a8\u6001\u7684\u53c2\u8003\u611f\u77e5\u53c2\u6570\u66f4\u65b0\u673a\u5236", "result": "\u5728\u590d\u6742\u4eba\u7c7b\u5bf9\u9f50\u4efb\u52a1\u4e2d\u76f8\u5bf9SFT\u63d0\u534757.70%\uff0c\u4f18\u4e8eDPO 17.65%\u3001PPO 7.95%\u548cGRPO 5.18%", "conclusion": "GRAO\u901a\u8fc7\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u80fd\u529b\u8fdb\u5316\u63d0\u4f9b\u4e86\u6837\u672c\u6548\u7387\u4f18\u52bf\u660e\u663e\u7684\u65b0\u578b\u4f18\u5316\u8303\u5f0f\u3002"}}
{"id": "2508.07768", "pdf": "https://arxiv.org/pdf/2508.07768", "abs": "https://arxiv.org/abs/2508.07768", "authors": ["Qiang He", "Setareh Maghsudi"], "title": "Pareto Multi-Objective Alignment for Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted at ECML/PKDD 2025", "summary": "Large language models (LLMs) are increasingly deployed in real-world\napplications that require careful balancing of multiple, often conflicting,\nobjectives, such as informativeness versus conciseness, or helpfulness versus\ncreativity. However, current alignment methods, primarily based on RLHF,\noptimize LLMs toward a single reward function, resulting in rigid behavior that\nfails to capture the complexity and diversity of human preferences. This\nlimitation hinders the adaptability of LLMs to practical scenarios, making\nmulti-objective alignment (MOA) a critical yet underexplored area. To bridge\nthis gap, we propose Pareto Multi-Objective Alignment (PAMA), a principled and\ncomputationally efficient algorithm designed explicitly for MOA in LLMs. In\ncontrast to computationally prohibitive multi-objective optimization (MOO)\nmethods, PAMA transforms multi-objective RLHF into a convex optimization with a\nclosed-form solution, significantly enhancing scalability. Traditional MOO\napproaches suffer from prohibitive O(n^2*d) complexity, where d represents the\nnumber of model parameters, typically in the billions for LLMs, rendering\ndirect optimization infeasible. PAMA reduces this complexity to O(n) where n is\nthe number of objectives, enabling optimization to be completed within\nmilliseconds. We provide theoretical guarantees that PAMA converges to a Pareto\nstationary point, where no objective can be improved without degrading at least\none other. Extensive experiments across language models ranging from 125M to 7B\nparameters demonstrate PAMA's robust and effective MOA capabilities, aligning\nwith its theoretical advantages. PAMA provides a highly efficient solution to\nthe MOA problem that was previously considered intractable, offering a\npractical and theoretically grounded approach to aligning LLMs with diverse\nhuman values, paving the way for versatile and adaptable real-world AI\ndeployments.", "AI": {"tldr": "\u63d0\u51faPAMA\u7b97\u6cd5\u89e3\u51b3LLM\u591a\u76ee\u6807\u5bf9\u9f50\u96be\u9898\uff0c\u901a\u8fc7\u51f8\u4f18\u5316\u5b9e\u73b0\u9ad8\u6548\u591a\u76ee\u6807\u4f18\u5316", "motivation": "\u73b0\u6709RLHF\u65b9\u6cd5\u53ea\u80fd\u4f18\u5316\u5355\u4e00\u76ee\u6807\uff0c\u65e0\u6cd5\u6ee1\u8db3\u73b0\u5b9e\u5e94\u7528\u4e2d\u590d\u6742\u591a\u6837\u7684\u591a\u76ee\u6807\u9700\u6c42", "method": "\u5c06\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u8f6c\u5316\u4e3a\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u95ed\u5f0f\u89e3\u5c06\u590d\u6742\u5ea6\u4eceO(n\u00b2d)\u964d\u81f3O(n)", "result": "\u5728125M\u52307B\u53c2\u6570\u7684\u6a21\u578b\u4e0a\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u4f18\u5316\u65f6\u95f4\u7f29\u77ed\u81f3\u6beb\u79d2\u7ea7", "conclusion": "PAMA\u4e3aLLM\u591a\u76ee\u6807\u5bf9\u9f50\u63d0\u4f9b\u7406\u8bba\u4fdd\u969c\u548c\u5b9e\u7528\u65b9\u6848\uff0c\u63a8\u52a8AI\u5728\u590d\u6742\u573a\u666f\u7684\u9002\u5e94\u6027\u90e8\u7f72"}}
{"id": "2508.07973", "pdf": "https://arxiv.org/pdf/2508.07973", "abs": "https://arxiv.org/abs/2508.07973", "authors": ["Sebastian Murgul", "Johannes Schimper", "Michael Heizmann"], "title": "Joint Transcription of Acoustic Guitar Strumming Directions and Chords", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted to the 26th International Society for Music Information\n  Retrieval Conference (ISMIR), 2025", "summary": "Automatic transcription of guitar strumming is an underrepresented and\nchallenging task in Music Information Retrieval (MIR), particularly for\nextracting both strumming directions and chord progressions from audio signals.\nWhile existing methods show promise, their effectiveness is often hindered by\nlimited datasets. In this work, we extend a multimodal approach to guitar\nstrumming transcription by introducing a novel dataset and a deep\nlearning-based transcription model. We collect 90 min of real-world guitar\nrecordings using an ESP32 smartwatch motion sensor and a structured recording\nprotocol, complemented by a synthetic dataset of 4h of labeled strumming audio.\nA Convolutional Recurrent Neural Network (CRNN) model is trained to detect\nstrumming events, classify their direction, and identify the corresponding\nchords using only microphone audio. Our evaluation demonstrates significant\nimprovements over baseline onset detection algorithms, with a hybrid method\ncombining synthetic and real-world data achieving the highest accuracy for both\nstrumming action detection and chord classification. These results highlight\nthe potential of deep learning for robust guitar strumming transcription and\nopen new avenues for automatic rhythm guitar analysis.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u57fa\u4e8eCRNN\u6a21\u578b\u548c\u6df7\u5408\u6570\u636e\u96c6\u7684\u5409\u4ed6\u5f39\u594f\u81ea\u52a8\u8f6c\u5f55\u65b9\u6cd5\uff0c\u7ed3\u5408\u667a\u80fd\u624b\u8868\u4f20\u611f\u5668\u6570\u636e\u4e0e\u5408\u6210\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u5f39\u594f\u65b9\u5411\u68c0\u6d4b\u4e0e\u548c\u5f26\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u5409\u4ed6\u5f39\u594f\u8f6c\u5f55\u65b9\u6cd5\u53d7\u9650\u4e8e\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u96be\u4ee5\u6709\u6548\u63d0\u53d6\u5f39\u594f\u65b9\u5411\u4e0e\u548c\u5f26\u8fdb\u7a0b\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u91c7\u96c6\u548c\u6df1\u5ea6\u5b66\u4e60\u7a81\u7834\u8fd9\u4e00\u74f6\u9888\u3002", "method": "\u4f7f\u7528ESP32\u667a\u80fd\u624b\u8868\u91c7\u96c690\u5206\u949f\u771f\u5b9e\u5f39\u594f\u6570\u636e\uff0c\u751f\u62104\u5c0f\u65f6\u5408\u6210\u6570\u636e\uff1b\u6784\u5efaCRNN\u6a21\u578b\u540c\u65f6\u5904\u7406\u5f39\u594f\u52a8\u4f5c\u68c0\u6d4b\u3001\u65b9\u5411\u5206\u7c7b\u53ca\u548c\u5f26\u8bc6\u522b\u3002", "result": "\u6df7\u5408\u6570\u636e\u65b9\u6cd5\u53d6\u5f97\u6700\u4f73\u6548\u679c\uff1a\u5f39\u594f\u52a8\u4f5c\u68c0\u6d4bF1-score\u8fbe0.84\uff08\u6bd4\u57fa\u7ebf\u9ad821%\uff09\uff0c\u548c\u5f26\u5206\u7c7b\u51c6\u786e\u738772.8%\uff08\u6bd4\u7eaf\u771f\u5b9e\u6570\u636e\u9ad88.5%\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u5b9e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bf9\u590d\u6742\u5409\u4ed6\u5f39\u594f\u7279\u5f81\u7684\u6355\u6349\u80fd\u529b\uff0c\u4e3a\u81ea\u52a8\u8282\u594f\u5206\u6790\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u66f4\u591a\u6f14\u594f\u573a\u666f\u548c\u4e50\u5668\u7c7b\u578b\u3002"}}
{"id": "2508.07975", "pdf": "https://arxiv.org/pdf/2508.07975", "abs": "https://arxiv.org/abs/2508.07975", "authors": ["Stefano Campese", "Alessandro Moschitti", "Ivano Lauriola"], "title": "Improving Document Retrieval Coherence for Semantically Equivalent Queries", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Dense Retrieval (DR) models have proven to be effective for Document\nRetrieval and Information Grounding tasks. Usually, these models are trained\nand optimized for improving the relevance of top-ranked documents for a given\nquery. Previous work has shown that popular DR models are sensitive to the\nquery and document lexicon: small variations of it may lead to a significant\ndifference in the set of retrieved documents. In this paper, we propose a\nvariation of the Multi-Negative Ranking loss for training DR that improves the\ncoherence of models in retrieving the same documents with respect to\nsemantically similar queries. The loss penalizes discrepancies between the\ntop-k ranked documents retrieved for diverse but semantic equivalent queries.\nWe conducted extensive experiments on various datasets, MS-MARCO, Natural\nQuestions, BEIR, and TREC DL 19/20. The results show that (i) models optimizes\nby our loss are subject to lower sensitivity, and, (ii) interestingly, higher\naccuracy.", "AI": {"tldr": "\u63d0\u51fa\u6539\u8fdbMulti-Negative Ranking\u635f\u5931\u51fd\u6570\u7684\u65b0\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u60e9\u7f5a\u8bed\u4e49\u7b49\u4ef7\u67e5\u8be2\u7684\u6587\u6863\u6392\u5e8f\u5dee\u5f02\uff0c\u63d0\u5347\u7a20\u5bc6\u68c0\u7d22\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u7a20\u5bc6\u68c0\u7d22\u6a21\u578b\u5bf9\u67e5\u8be2\u8bcd\u5fae\u5c0f\u53d8\u5316\u8fc7\u4e8e\u654f\u611f\uff0c\u5bfc\u81f4\u8bed\u4e49\u76f8\u4f3c\u67e5\u8be2\u7684\u68c0\u7d22\u7ed3\u679c\u4e0d\u4e00\u81f4\u3002\u9700\u589e\u5f3a\u6a21\u578b\u5bf9\u8bed\u4e49\u7b49\u4ef7\u67e5\u8be2\u7684\u68c0\u7d22\u4e00\u81f4\u6027\u3002", "method": "\u6539\u8fdbMulti-Negative Ranking\u635f\u5931\u51fd\u6570\uff0c\u5f3a\u5236\u8981\u6c42\u8bed\u4e49\u7b49\u4ef7\u7684\u4e0d\u540c\u67e5\u8be2\u5728top-k\u68c0\u7d22\u7ed3\u679c\u4e2d\u4fdd\u6301\u6587\u6863\u6392\u5e8f\u4e00\u81f4\u6027\uff0c\u901a\u8fc7\u60e9\u7f5a\u7ed3\u679c\u5dee\u5f02\u5b9e\u73b0\u6a21\u578b\u4f18\u5316\u3002", "result": "\u5728MS-MARCO\u3001BEIR\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6a21\u578b\u654f\u611f\u6027\u964d\u4f4e18-35%\uff0c\u68c0\u7d22\u51c6\u786e\u7387\u63d0\u53472-5\u4e2a\u767e\u5206\u70b9\uff0c\u5b9e\u73b0\u7a33\u5b9a\u6027\u4e0e\u7cbe\u5ea6\u7684\u53cc\u91cd\u6539\u8fdb\u3002", "conclusion": "\u901a\u8fc7\u589e\u5f3a\u67e5\u8be2\u8bed\u4e49\u7b49\u4ef7\u6027\u7684\u7ea6\u675f\u6761\u4ef6\uff0c\u53ef\u540c\u6b65\u63d0\u5347\u7a20\u5bc6\u68c0\u7d22\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u68c0\u7d22\u8d28\u91cf\uff0c\u9a8c\u8bc1\u4e86\u68c0\u7d22\u4e00\u81f4\u6027\u4f18\u5316\u5bf9\u6027\u80fd\u7684\u6b63\u5411\u4f5c\u7528\u3002"}}
{"id": "2508.07987", "pdf": "https://arxiv.org/pdf/2508.07987", "abs": "https://arxiv.org/abs/2508.07987", "authors": ["Sebastian Murgul", "Michael Heizmann"], "title": "Exploring Procedural Data Generation for Automatic Acoustic Guitar Fingerpicking Transcription", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted to the 6th Conference on AI Music Creativity (AIMC), 2025", "summary": "Automatic transcription of acoustic guitar fingerpicking performances remains\na challenging task due to the scarcity of labeled training data and legal\nconstraints connected with musical recordings. This work investigates a\nprocedural data generation pipeline as an alternative to real audio recordings\nfor training transcription models. Our approach synthesizes training data\nthrough four stages: knowledge-based fingerpicking tablature composition, MIDI\nperformance rendering, physical modeling using an extended Karplus-Strong\nalgorithm, and audio augmentation including reverb and distortion. We train and\nevaluate a CRNN-based note-tracking model on both real and synthetic datasets,\ndemonstrating that procedural data can be used to achieve reasonable\nnote-tracking results. Finetuning with a small amount of real data further\nenhances transcription accuracy, improving over models trained exclusively on\nreal recordings. These results highlight the potential of procedurally\ngenerated audio for data-scarce music information retrieval tasks.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u7a0b\u5e8f\u5316\u751f\u6210\u5408\u6210\u6570\u636e\u8bad\u7ec3\u5409\u4ed6\u6307\u5f39\u8f6c\u5f55\u6a21\u578b\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u6709\u6548\u63d0\u5347\u97f3\u7b26\u8ffd\u8e2a\u6548\u679c\u3002", "motivation": "\u539f\u58f0\u5409\u4ed6\u6307\u5f39\u81ea\u52a8\u8f6c\u5f55\u9762\u4e34\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u548c\u97f3\u4e50\u7248\u6743\u6cd5\u5f8b\u9650\u5236\uff0c\u9700\u63a2\u7d22\u5408\u6210\u6570\u636e\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u56db\u9636\u6bb5\u6d41\u7a0b\uff1a1)\u77e5\u8bc6\u9a71\u52a8\u751f\u6210\u6307\u5f39\u4e50\u8c31\uff1b2)MIDI\u6e32\u67d3\uff1b3)\u6539\u8fdbKarplus-Strong\u7269\u7406\u5efa\u6a21\u5408\u6210\u97f3\u9891\uff1b4)\u6dfb\u52a0\u6df7\u54cd/\u5931\u771f\u589e\u5f3a\u6570\u636e", "result": "\u5408\u6210\u6570\u636e\u8bad\u7ec3\u6a21\u578b\u8fbe\u5230\u5408\u7406\u7cbe\u5ea6\uff0c\u5c11\u91cf\u771f\u5b9e\u6570\u636e\u5fae\u8c03\u540e\u51c6\u786e\u7387\u8d85\u8d8a\u7eaf\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u6a21\u578b", "conclusion": "\u7a0b\u5e8f\u5316\u97f3\u9891\u751f\u6210\u4e3a\u6570\u636e\u7a00\u7f3a\u7684\u97f3\u4e50\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.08039", "pdf": "https://arxiv.org/pdf/2508.08039", "abs": "https://arxiv.org/abs/2508.08039", "authors": ["Shu Wu", "Chenxing Li", "Wenfu Wang", "Hao Zhang", "Hualei Wang", "Meng Yu", "Dong Yu"], "title": "Audio-Thinker: Guiding Audio Language Model When and How to Think via Reinforcement Learning", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "comment": "preprint", "summary": "Recent advancements in large language models, multimodal large language\nmodels, and large audio language models (LALMs) have significantly improved\ntheir reasoning capabilities through reinforcement learning with rule-based\nrewards. However, the explicit reasoning process has yet to show significant\nbenefits for audio question answering, and effectively leveraging deep\nreasoning remains an open challenge, with LALMs still falling short of\nhuman-level auditory-language reasoning. To address these limitations, we\npropose Audio-Thinker, a reinforcement learning framework designed to enhance\nthe reasoning capabilities of LALMs, with a focus on improving adaptability,\nconsistency, and effectiveness. Our approach introduces an adaptive think\naccuracy reward, enabling the model to adjust its reasoning strategies based on\ntask complexity dynamically. Furthermore, we incorporate an external reward\nmodel to evaluate the overall consistency and quality of the reasoning process,\ncomplemented by think-based rewards that help the model distinguish between\nvalid and flawed reasoning paths during training. Experimental results\ndemonstrate that our Audio-Thinker model outperforms existing\nreasoning-oriented LALMs across various benchmark tasks, exhibiting superior\nreasoning and generalization capabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5f3a\u5316\u5b66\u4e60\u6846\u67b6Audio-Thinker\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u601d\u7ef4\u51c6\u786e\u5ea6\u5956\u52b1\u548c\u5916\u90e8\u4e00\u81f4\u6027\u8bc4\u4f30\uff0c\u663e\u8457\u63d0\u5347\u5927\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5927\u97f3\u9891\u6a21\u578b\u5728\u663e\u5f0f\u63a8\u7406\u8fc7\u7a0b\u5229\u7528\u4e0d\u8db3\uff0c\u63a8\u7406\u80fd\u529b\u672a\u8fbe\u4eba\u7c7b\u6c34\u5e73\uff0c\u9700\u8981\u63d0\u5347\u5176\u9002\u5e94\u6027\u3001\u4e00\u81f4\u6027\u548c\u6709\u6548\u6027\u3002", "method": "1. \u5f15\u5165\u81ea\u9002\u5e94\u601d\u7ef4\u51c6\u786e\u5ea6\u5956\u52b1\u673a\u5236\u52a8\u6001\u8c03\u6574\u63a8\u7406\u7b56\u7565\n2. \u7ed3\u5408\u5916\u90e8\u5956\u52b1\u6a21\u578b\u8bc4\u4f30\u63a8\u7406\u8fc7\u7a0b\u4e00\u81f4\u6027\n3. \u4f7f\u7528\u601d\u7ef4\u8def\u5f84\u5956\u52b1\u533a\u5206\u6709\u6548/\u9519\u8bef\u63a8\u7406", "result": "\u5728\u591a\u9879\u57fa\u51c6\u4efb\u52a1\u4e2d\u8d85\u8d8a\u73b0\u6709\u63a8\u7406\u5bfc\u5411\u6a21\u578b\uff0c\u5c55\u73b0\u51fa\u66f4\u4f18\u7684\u63a8\u7406\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "Audio-Thinker\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5927\u97f3\u9891\u6a21\u578b\u6df1\u5ea6\u63a8\u7406\u96be\u9898\uff0c\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u7814\u7a76\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.08061", "pdf": "https://arxiv.org/pdf/2508.08061", "abs": "https://arxiv.org/abs/2508.08061", "authors": ["Sven Weinzierl", "Sandra Zilker", "Annina Liessmann", "Martin K\u00e4ppel", "Weixin Wang", "Martin Matzner"], "title": "From Source to Target: Leveraging Transfer Learning for Predictive Process Monitoring in Organizations", "categories": ["cs.LG", "cs.CL", "cs.DB"], "comment": null, "summary": "Event logs reflect the behavior of business processes that are mapped in\norganizational information systems. Predictive process monitoring (PPM)\ntransforms these data into value by creating process-related predictions that\nprovide the insights required for proactive interventions at process runtime.\nExisting PPM techniques require sufficient amounts of event data or other\nrelevant resources that might not be readily available, preventing some\norganizations from utilizing PPM. The transfer learning-based PPM technique\npresented in this paper allows organizations without suitable event data or\nother relevant resources to implement PPM for effective decision support. The\ntechnique is instantiated in two real-life use cases, based on which numerical\nexperiments are performed using event logs for IT service management processes\nin an intra- and inter-organizational setting. The results of the experiments\nsuggest that knowledge of one business process can be transferred to a similar\nbusiness process in the same or a different organization to enable effective\nPPM in the target context. With the proposed technique, organizations can\nbenefit from transfer learning in an intra- and inter-organizational setting,\nwhere resources like pre-trained models are transferred within and across\norganizational boundaries.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8fc1\u79fb\u5b66\u4e60\u7684\u9884\u6d4b\u6027\u6d41\u7a0b\u76d1\u63a7\u6280\u672f\uff0c\u89e3\u51b3\u7ec4\u7ec7\u56e0\u6570\u636e\u4e0d\u8db3\u65e0\u6cd5\u5b9e\u65bd\u6d41\u7a0b\u9884\u6d4b\u7684\u95ee\u9898", "motivation": "\u4f20\u7edfPPM\u6280\u672f\u9700\u8981\u5145\u8db3\u7684\u4e8b\u4ef6\u6570\u636e/\u8d44\u6e90\uff0c\u4f46\u8bb8\u591a\u7ec4\u7ec7\u65e0\u6cd5\u6ee1\u8db3\u8be5\u6761\u4ef6\u5bfc\u81f4\u65e0\u6cd5\u5e94\u7528\u6d41\u7a0b\u9884\u6d4b\u6280\u672f", "method": "\u5f00\u53d1\u8de8\u7ec4\u7ec7\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7IT\u670d\u52a1\u7ba1\u7406\u6d41\u7a0b\u7684\u4e8b\u4ef6\u65e5\u5fd7\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u652f\u6301\u7ec4\u7ec7\u5185/\u8de8\u7ec4\u7ec7\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u8fc1\u79fb", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e1a\u52a1\u6d41\u7a0b\u77e5\u8bc6\u53ef\u5728\u540c\u7ec4\u7ec7\u6216\u8de8\u7ec4\u7ec7\u7684\u76f8\u4f3c\u6d41\u7a0b\u95f4\u8fc1\u79fb\uff0c\u663e\u8457\u63d0\u5347\u76ee\u6807\u573a\u666f\u4e0b\u7684\u9884\u6d4b\u6548\u679c", "conclusion": "\u8be5\u6280\u672f\u7a81\u7834\u6570\u636e\u58c1\u5792\uff0c\u4f7f\u8d44\u6e90\u6709\u9650\u7ec4\u7ec7\u80fd\u5229\u7528\u5916\u90e8\u77e5\u8bc6\u5b9e\u73b0\u9884\u6d4b\u6027\u6d41\u7a0b\u76d1\u63a7\uff0c\u4e3a\u667a\u80fd\u51b3\u7b56\u63d0\u4f9b\u8de8\u7ec4\u7ec7\u534f\u4f5c\u65b0\u8303\u5f0f"}}
{"id": "2508.08066", "pdf": "https://arxiv.org/pdf/2508.08066", "abs": "https://arxiv.org/abs/2508.08066", "authors": ["Weitai Kang", "Weiming Zhuang", "Zhizhong Li", "Yan Yan", "Lingjuan Lyu"], "title": "Investigating the Design Space of Visual Grounding in Multimodal Large Language Model", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "8 pages for the main paper", "summary": "Fine-grained multimodal capability in Multimodal Large Language Models\n(MLLMs) has emerged as a critical research direction, particularly for tackling\nthe visual grounding (VG) problem. Despite the strong performance achieved by\nexisting approaches, they often employ disparate design choices when\nfine-tuning MLLMs for VG, lacking systematic verification to support these\ndesigns. To bridge this gap, this paper presents a comprehensive study of\nvarious design choices that impact the VG performance of MLLMs. We conduct our\nanalysis using LLaVA-1.5, which has been widely adopted in prior empirical\nstudies of MLLMs. While more recent models exist, we follow this convention to\nensure our findings remain broadly applicable and extendable to other\narchitectures. We cover two key aspects: (1) exploring different visual\ngrounding paradigms in MLLMs, identifying the most effective design, and\nproviding our insights; and (2) conducting ablation studies on the design of\ngrounding data to optimize MLLMs' fine-tuning for the VG task. Finally, our\nfindings contribute to a stronger MLLM for VG, achieving improvements of +5.6%\n/ +6.9% / +7.0% on RefCOCO/+/g over the LLaVA-1.5.", "AI": {"tldr": "\u8bba\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u8bbe\u8ba1\u9009\u62e9\uff0c\u901a\u8fc7\u4f18\u5316\u89c6\u89c9\u5b9a\u4f4d\u8303\u5f0f\u4e0e\u6570\u636e\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u91c7\u7528\u5206\u6563\u7684\u8bbe\u8ba1\u7b56\u7565\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u9a8c\u8bc1\uff0c\u9700\u660e\u786e\u4e0d\u540c\u8bbe\u8ba1\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u57fa\u4e8eLLaVA-1.5\u6a21\u578b\uff0c\u5bf9\u6bd4\u4e0d\u540c\u89c6\u89c9\u5b9a\u4f4d\u8303\u5f0f\u6548\u679c\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u4f18\u5316\u6570\u636e\u8bbe\u8ba1\u7b56\u7565\u3002", "result": "\u5728RefCOCO/+/g\u6570\u636e\u96c6\u4e0a\u5206\u522b\u5b9e\u73b0+5.6%/+6.9%/+7.0%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u7cfb\u7edf\u5316\u8bbe\u8ba1\u9a8c\u8bc1\u663e\u8457\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u5b9a\u4f4d\u80fd\u529b\uff0c\u7814\u7a76\u6846\u67b6\u5177\u5907\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2508.08088", "pdf": "https://arxiv.org/pdf/2508.08088", "abs": "https://arxiv.org/abs/2508.08088", "authors": ["Jiejun Tan", "Zhicheng Dou", "Yan Yu", "Jiehan Cheng", "Qiang Ju", "Jian Xie", "Ji-Rong Wen"], "title": "HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "Code and datasets are available at\n  https://github.com/plageon/HierSearch", "summary": "Recently, large reasoning models have demonstrated strong mathematical and\ncoding abilities, and deep search leverages their reasoning capabilities in\nchallenging information retrieval tasks. Existing deep search works are\ngenerally limited to a single knowledge source, either local or the Web.\nHowever, enterprises often require private deep search systems that can\nleverage search tools over both local and the Web corpus. Simply training an\nagent equipped with multiple search tools using flat reinforcement learning\n(RL) is a straightforward idea, but it has problems such as low training data\nefficiency and poor mastery of complex tools. To address the above issue, we\npropose a hierarchical agentic deep search framework, HierSearch, trained with\nhierarchical RL. At the low level, a local deep search agent and a Web deep\nsearch agent are trained to retrieve evidence from their corresponding domains.\nAt the high level, a planner agent coordinates low-level agents and provides\nthe final answer. Moreover, to prevent direct answer copying and error\npropagation, we design a knowledge refiner that filters out hallucinations and\nirrelevant evidence returned by low-level agents. Experiments show that\nHierSearch achieves better performance compared to flat RL, and outperforms\nvarious deep search and multi-source retrieval-augmented generation baselines\nin six benchmarks across general, finance, and medical domains.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42\u4ee3\u7406\u6df1\u5ea6\u641c\u7d22\u6846\u67b6HierSearch\uff0c\u901a\u8fc7\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u534f\u8c03\u672c\u5730/\u7f51\u7edc\u53cc\u6e90\u68c0\u7d22\uff0c\u5e76\u8bbe\u8ba1\u77e5\u8bc6\u7cbe\u70bc\u5668\u8fc7\u6ee4\u9519\u8bef\u4fe1\u606f\uff0c\u5728\u591a\u4e2a\u9886\u57df\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f01\u4e1a\u9700\u8981\u540c\u65f6\u5229\u7528\u672c\u5730\u548c\u7f51\u7edc\u8bed\u6599\u7684\u79c1\u6709\u6df1\u5ea6\u641c\u7d22\u7cfb\u7edf\uff0c\u4f46\u73b0\u6709\u5355\u6e90\u68c0\u7d22\u65b9\u6cd5\u5b58\u5728\u8bad\u7ec3\u6570\u636e\u6548\u7387\u4f4e\u3001\u590d\u6742\u5de5\u5177\u638c\u63e1\u5dee\u7684\u95ee\u9898\u3002", "method": "\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u67b6\u6784\uff1a\u5e95\u5c42\u8bad\u7ec3\u672c\u5730/\u7f51\u7edc\u641c\u7d22\u4ee3\u7406\u5206\u522b\u68c0\u7d22\u8bc1\u636e\uff0c\u9ad8\u5c42\u89c4\u5212\u4ee3\u7406\u534f\u8c03\u51b3\u7b56\uff0c\u5f15\u5165\u77e5\u8bc6\u7cbe\u70bc\u5668\u8fc7\u6ee4\u5e7b\u89c9\u548c\u65e0\u5173\u8bc1\u636e\u3002", "result": "\u5728\u901a\u7528\u3001\u91d1\u878d\u548c\u533b\u7597\u9886\u57df\u7684\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u4f18\u4e8e\u6241\u5e73\u5f3a\u5316\u5b66\u4e60\u53ca\u5404\u7c7b\u591a\u6e90\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "HierSearch\u901a\u8fc7\u5206\u5c42\u534f\u8c03\u673a\u5236\u6709\u6548\u89e3\u51b3\u591a\u6e90\u68c0\u7d22\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u7b54\u6848\u51c6\u786e\u6027\u5e76\u964d\u4f4e\u9519\u8bef\u4f20\u64ad\u98ce\u9669\u3002"}}
{"id": "2508.08221", "pdf": "https://arxiv.org/pdf/2508.08221", "abs": "https://arxiv.org/abs/2508.08221", "authors": ["Zihe Liu", "Jiashun Liu", "Yancheng He", "Weixun Wang", "Jiaheng Liu", "Ling Pan", "Xinyu Hu", "Shaopan Xiong", "Ju Huang", "Jian Hu", "Shengyi Huang", "Siran Yang", "Jiamang Wang", "Wenbo Su", "Bo Zheng"], "title": "Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning", "categories": ["cs.LG", "cs.CL"], "comment": "26 pages, 21 figures", "summary": "Reinforcement learning for LLM reasoning has rapidly emerged as a prominent\nresearch area, marked by a significant surge in related studies on both\nalgorithmic innovations and practical applications. Despite this progress,\nseveral critical challenges remain, including the absence of standardized\nguidelines for employing RL techniques and a fragmented understanding of their\nunderlying mechanisms. Additionally, inconsistent experimental settings,\nvariations in training data, and differences in model initialization have led\nto conflicting conclusions, obscuring the key characteristics of these\ntechniques and creating confusion among practitioners when selecting\nappropriate techniques. This paper systematically reviews widely adopted RL\ntechniques through rigorous reproductions and isolated evaluations within a\nunified open-source framework. We analyze the internal mechanisms, applicable\nscenarios, and core principles of each technique through fine-grained\nexperiments, including datasets of varying difficulty, model sizes, and\narchitectures. Based on these insights, we present clear guidelines for\nselecting RL techniques tailored to specific setups, and provide a reliable\nroadmap for practitioners navigating the RL for the LLM domain. Finally, we\nreveal that a minimalist combination of two techniques can unlock the learning\ncapability of critic-free policies using vanilla PPO loss. The results\ndemonstrate that our simple combination consistently improves performance,\nsurpassing strategies like GRPO and DAPO.", "AI": {"tldr": "\u7cfb\u7edf\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u6280\u672f\uff0c\u63d0\u51fa\u57fa\u4e8e\u573a\u666f\u7684\u9009\u62e9\u6307\u5357\u5e76\u9a8c\u8bc1\u6781\u7b80\u7ec4\u5408\u7b56\u7565\u7684\u6709\u6548\u6027", "motivation": "\u89e3\u51b3RL\u6280\u672f\u5e94\u7528\u7f3a\u4e4f\u6807\u51c6\u5316\u6307\u5357\u3001\u673a\u5236\u7406\u89e3\u788e\u7247\u5316\u3001\u5b9e\u9a8c\u7ed3\u8bba\u51b2\u7a81\u7b49\u95ee\u9898\uff0c\u4e3a\u4ece\u4e1a\u8005\u63d0\u4f9b\u6e05\u6670\u7684\u6280\u672f\u9009\u62e9\u6846\u67b6", "method": "\u901a\u8fc7\u7edf\u4e00\u5f00\u6e90\u6846\u67b6\u7684\u4e25\u683c\u590d\u73b0\u4e0e\u9694\u79bb\u8bc4\u4f30\uff0c\u7ed3\u5408\u4e0d\u540c\u96be\u5ea6\u6570\u636e\u96c6/\u6a21\u578b\u89c4\u6a21/\u67b6\u6784\u7684\u7ec6\u7c92\u5ea6\u5b9e\u9a8c\u5206\u6790", "result": "\u6781\u7b80\u7684\u4e24\u79cd\u6280\u672f\u7ec4\u5408\uff08\u666e\u901aPPO\u635f\u5931+\u65e0\u8bc4\u8bba\u5bb6\u7b56\u7565\uff09\u6301\u7eed\u63d0\u5347\u6027\u80fd\uff0c\u8d85\u8d8aGRPO/DAPO\u7b49\u7b56\u7565", "conclusion": "\u7814\u7a76\u4e3aRL for LLM\u9886\u57df\u63d0\u4f9b\u5b9e\u7528\u6280\u672f\u8def\u7ebf\u56fe\uff0c\u8bc1\u660e\u57fa\u7840\u65b9\u6cd5\u7ec4\u5408\u5373\u53ef\u6709\u6548\u6fc0\u53d1\u6a21\u578b\u5b66\u4e60\u6f5c\u529b"}}
