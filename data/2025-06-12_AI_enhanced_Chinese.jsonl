{"id": "2506.09147", "pdf": "https://arxiv.org/pdf/2506.09147", "abs": "https://arxiv.org/abs/2506.09147", "authors": ["Nadezhda Chirkova", "Tunde Oluwaseyi Ajayi", "Seth Aycock", "Zain Muhammad Mujahid", "Vladana Perli\u0107", "Ekaterina Borisova", "Markarit Vartampetian"], "title": "LLM-as-a-qualitative-judge: automating error analysis in natural language generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Prompting large language models (LLMs) to evaluate generated text, known as\nLLM-as-a-judge, has become a standard evaluation approach in natural language\ngeneration (NLG), but is primarily used as a quantitative tool, i.e. with\nnumerical scores as main outputs. In this work, we propose\nLLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main\noutput being a structured report of common issue types in the NLG system\noutputs. Our approach is targeted at providing developers with meaningful\ninsights on what improvements can be done to a given NLG system and consists of\ntwo main steps, namely open-ended per-instance issue analysis and clustering of\nthe discovered issues using an intuitive cumulative algorithm. We also\nintroduce a strategy for evaluating the proposed approach, coupled with ~300\nannotations of issues in instances from 12 NLG datasets. Our results show that\nLLM-as-a-qualitative-judge correctly recognizes instance-specific issues in 2/3\ncases and is capable of producing error type reports resembling the reports\ncomposed by human annotators. Our code and data are publicly available at\nhttps://github.com/tunde-ajayi/llm-as-a-qualitative-judge.", "AI": {"tldr": "\u63d0\u51faLLM-as-a-qualitative-judge\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u95ee\u9898\u62a5\u544a\u66ff\u4ee3\u6570\u503c\u8bc4\u5206\uff0c\u4e3aNLG\u7cfb\u7edf\u6539\u8fdb\u63d0\u4f9b\u5177\u4f53\u65b9\u5411", "motivation": "\u4f20\u7edfLLM\u8bc4\u4f30\u4ec5\u63d0\u4f9b\u6570\u503c\u8bc4\u5206\uff0c\u7f3a\u4e4f\u5bf9\u7cfb\u7edf\u6539\u8fdb\u7684\u5177\u4f53\u6307\u5bfc\u3002\u9700\u8981\u5b9a\u6027\u5206\u6790\u65b9\u6cd5\u5e2e\u52a9\u5f00\u53d1\u8005\u8bc6\u522bNLG\u7cfb\u7edf\u7684\u5177\u4f53\u7f3a\u9677", "method": "1. \u5bf9\u6bcf\u4e2a\u8f93\u51fa\u5b9e\u4f8b\u8fdb\u884c\u5f00\u653e\u5f0f\u95ee\u9898\u5206\u6790\n2. \u4f7f\u7528\u7d2f\u79ef\u7b97\u6cd5\u5bf9\u53d1\u73b0\u7684\u95ee\u9898\u8fdb\u884c\u805a\u7c7b\u6574\u5408", "result": "\u572812\u4e2aNLG\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff1a\n- \u6b63\u786e\u8bc6\u522b\u5b9e\u4f8b\u7ea7\u95ee\u9898\uff08\u51c6\u786e\u73872/3\uff09\n- \u751f\u6210\u7684\u95ee\u9898\u7c7b\u578b\u62a5\u544a\u4e0e\u4eba\u5de5\u6807\u6ce8\u9ad8\u5ea6\u76f8\u4f3c", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5c06LLM\u8f6c\u6362\u4e3a\u7cfb\u7edf\u8bca\u65ad\u5de5\u5177\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u95ee\u9898\u62a5\u544a\u4e3aNLG\u7cfb\u7edf\u4f18\u5316\u63d0\u4f9b\u53ef\u64cd\u4f5c\u89c1\u89e3\uff0c\u63a8\u52a8\u8d28\u91cf\u6539\u8fdb\u5bfc\u5411\u7684\u8bc4\u4f30\u8303\u5f0f\u53d1\u5c55"}}
{"id": "2506.09175", "pdf": "https://arxiv.org/pdf/2506.09175", "abs": "https://arxiv.org/abs/2506.09175", "authors": ["Peidong Wang", "Jian Xue", "Rui Zhao", "Junkun Chen", "Aswin Shanmugam Subramanian", "Jinyu Li"], "title": "PHRASED: Phrase Dictionary Biasing for Speech Translation", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Phrases are essential to understand the core concepts in conversations.\nHowever, due to their rare occurrence in training data, correct translation of\nphrases is challenging in speech translation tasks. In this paper, we propose a\nphrase dictionary biasing method to leverage pairs of phrases mapping from the\nsource language to the target language. We apply the phrase dictionary biasing\nmethod to two types of widely adopted models, a transducer-based streaming\nspeech translation model and a multimodal large language model. Experimental\nresults show that the phrase dictionary biasing method outperforms phrase list\nbiasing by 21% relatively for the streaming speech translation model. In\naddition, phrase dictionary biasing enables multimodal large language models to\nuse external phrase information, achieving 85% relative improvement in phrase\nrecall.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u77ed\u8bed\u8bcd\u5178\u7684\u504f\u7f6e\u65b9\u6cd5\uff0c\u5728\u6d41\u5f0f\u8bed\u97f3\u7ffb\u8bd1\u548c\u591a\u6a21\u6001\u5927\u6a21\u578b\u4e2d\u5206\u522b\u5b9e\u73b021%\u548c85%\u7684\u77ed\u8bed\u53ec\u56de\u7387\u63d0\u5347", "motivation": "\u77ed\u8bed\u5728\u5bf9\u8bdd\u7406\u89e3\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u4e2d\u51fa\u73b0\u9891\u7387\u4f4e\uff0c\u8bed\u97f3\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u7684\u77ed\u8bed\u6b63\u786e\u7ffb\u8bd1\u5b58\u5728\u6311\u6218", "method": "\u4f7f\u7528\u6e90\u8bed\u8a00\u5230\u76ee\u6807\u8bed\u8a00\u7684\u77ed\u8bed\u6620\u5c04\u5bf9\u6784\u5efa\u77ed\u8bed\u8bcd\u5178\u504f\u7f6e\u65b9\u6cd5\uff0c\u5e94\u7528\u4e8e\u6d41\u5f0f\u8bed\u97f3\u7ffb\u8bd1\u6a21\u578b\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b", "result": "\u6d41\u5f0f\u6a21\u578b\u76f8\u5bf9\u77ed\u8bed\u5217\u8868\u504f\u7f6e\u63d0\u534721%\uff0c\u591a\u6a21\u6001\u5927\u6a21\u578b\u77ed\u8bed\u53ec\u56de\u7387\u63d0\u534785%", "conclusion": "\u77ed\u8bed\u8bcd\u5178\u504f\u7f6e\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e0d\u540c\u67b6\u6784\u6a21\u578b\u7684\u77ed\u8bed\u7ffb\u8bd1\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u5916\u90e8\u77ed\u8bed\u77e5\u8bc6\u7684\u6709\u6548\u6574\u5408"}}
{"id": "2506.09218", "pdf": "https://arxiv.org/pdf/2506.09218", "abs": "https://arxiv.org/abs/2506.09218", "authors": ["Bruno Ferenc \u0160egedin"], "title": "A Technique for Isolating Lexically-Independent Phonetic Dependencies in Generative CNNs", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "The ability of deep neural networks (DNNs) to represent phonotactic\ngeneralizations derived from lexical learning remains an open question. This\nstudy (1) investigates the lexically-invariant generalization capacity of\ngenerative convolutional neural networks (CNNs) trained on raw audio waveforms\nof lexical items and (2) explores the consequences of shrinking the\nfully-connected layer (FC) bottleneck from 1024 channels to 8 before training.\nUltimately, a novel technique for probing a model's lexically-independent\ngeneralizations is proposed that works only under the narrow FC bottleneck:\ngenerating audio outputs by bypassing the FC and inputting randomized feature\nmaps into the convolutional block. These outputs are equally biased by a\nphonotactic restriction in training as are outputs generated with the FC. This\nresult shows that the convolutional layers can dynamically generalize phonetic\ndependencies beyond lexically-constrained configurations learned by the FC.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u7f29\u5c0f\u5168\u8fde\u63a5\u5c42\u74f6\u9888\u5e76\u7ed5\u8fc7\u5176\u8f93\u5165\u968f\u673a\u7279\u5f81\u56fe\u7684\u65b0\u65b9\u6cd5\uff0c\u8bc1\u660e\u5377\u79ef\u5c42\u80fd\u52a8\u6001\u6cdb\u5316\u8bed\u97f3\u4f9d\u8d56\u5173\u7cfb", "motivation": "\u63a2\u7a76\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u539f\u59cb\u97f3\u9891\u8bad\u7ec3\u4e2d\u7684\u8bcd\u6c47\u65e0\u5173\u6cdb\u5316\u80fd\u529b\uff0c\u9a8c\u8bc1\u5168\u8fde\u63a5\u5c42\u74f6\u9888\u5bf9\u97f3\u7cfb\u6982\u62ec\u7684\u5f71\u54cd", "method": "1. \u5c06\u5168\u8fde\u63a5\u5c42\u901a\u9053\u4ece1024\u538b\u7f29\u81f38\n2. \u5f00\u53d1\u7ed5\u8fc7\u5168\u8fde\u63a5\u5c42\u3001\u8f93\u5165\u968f\u673a\u7279\u5f81\u56fe\u5230\u5377\u79ef\u5757\u7684\u65b0\u751f\u6210\u6280\u672f", "result": "\u4f7f\u7528\u968f\u673a\u7279\u5f81\u56fe\u751f\u6210\u7684\u97f3\u9891\u4e0e\u5168\u8fde\u63a5\u5c42\u751f\u6210\u7ed3\u679c\u5177\u6709\u76f8\u540c\u7684\u97f3\u7cfb\u9650\u5236\u504f\u7f6e", "conclusion": "\u5377\u79ef\u5c42\u5177\u5907\u72ec\u7acb\u4e8eFC\u5c42\u5b66\u5f97\u8bcd\u6c47\u914d\u7f6e\u7684\u52a8\u6001\u8bed\u97f3\u4f9d\u8d56\u6cdb\u5316\u80fd\u529b"}}
{"id": "2506.09070", "pdf": "https://arxiv.org/pdf/2506.09070", "abs": "https://arxiv.org/abs/2506.09070", "authors": ["Chenqi Zhang", "Yu Feng", "Jieru Zhao", "Guangda Liu", "Wenchao Ding", "Chentao Wu", "Minyi Guo"], "title": "STREAMINGGS: Voxel-Based Streaming 3D Gaussian Splatting with Memory Optimization and Architectural Support", "categories": ["cs.GR", "cs.AI"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has gained popularity for its efficiency and\nsparse Gaussian-based representation. However, 3DGS struggles to meet the\nreal-time requirement of 90 frames per second (FPS) on resource-constrained\nmobile devices, achieving only 2 to 9 FPS.Existing accelerators focus on\ncompute efficiency but overlook memory efficiency, leading to redundant DRAM\ntraffic. We introduce STREAMINGGS, a fully streaming 3DGS\nalgorithm-architecture co-design that achieves fine-grained pipelining and\nreduces DRAM traffic by transforming from a tile-centric rendering to a\nmemory-centric rendering. Results show that our design achieves up to 45.7\n$\\times$ speedup and 62.9 $\\times$ energy savings over mobile Ampere GPUs.", "AI": {"tldr": "\u63d0\u51faSTREAMINGGS\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5185\u5b58\u4e2d\u5fc3\u6e32\u67d3\u5b9e\u73b0\u79fb\u52a8\u7aef45.7\u500d\u52a0\u901f\u548c62.9\u500d\u8282\u80fd", "motivation": "\u73b0\u67093DGS\u52a0\u901f\u5668\u5ffd\u89c6\u5185\u5b58\u6548\u7387\u5bfc\u81f4DRAM\u5197\u4f59\u8bbf\u95ee\uff0c\u65e0\u6cd5\u6ee1\u8db3\u79fb\u52a8\u8bbe\u590790FPS\u5b9e\u65f6\u9700\u6c42\uff08\u5f53\u524d\u4ec52-9FPS\uff09", "method": "\u7b97\u6cd5\u67b6\u6784\u534f\u540c\u8bbe\u8ba1\uff1a\u5c06\u6e32\u67d3\u8303\u5f0f\u4ecetile-centric\u8f6c\u4e3amemory-centric\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u6d41\u6c34\u7ebf\u5e76\u51cf\u5c11DRAM\u6d41\u91cf", "result": "\u76f8\u8f83\u79fb\u52a8\u7aefAmpere GPU\u5b9e\u73b045.7\u500d\u901f\u5ea6\u63d0\u5347\u548c62.9\u500d\u80fd\u6548\u4f18\u5316", "conclusion": "\u5185\u5b58\u6548\u7387\u4f18\u5316\u5bf9\u79fb\u52a8\u7aef3DGS\u52a0\u901f\u5177\u6709\u51b3\u5b9a\u6027\u4f5c\u7528\uff0cSTREAMINGGS\u65b9\u6848\u6210\u529f\u7a81\u7834\u5b9e\u65f6\u6027\u74f6\u9888"}}
{"id": "2506.09251", "pdf": "https://arxiv.org/pdf/2506.09251", "abs": "https://arxiv.org/abs/2506.09251", "authors": ["Ziyang Cai", "Nayoung Lee", "Avi Schwarzschild", "Samet Oymak", "Dimitris Papailiopoulos"], "title": "Extrapolation by Association: Length Generalization Transfer in Transformers", "categories": ["cs.CL", "cs.AI"], "comment": "23 pages, 20 figures", "summary": "Transformer language models have demonstrated impressive generalization\ncapabilities in natural language domains, yet we lack a fine-grained\nunderstanding of how such generalization arises. In this paper, we investigate\nlength generalization--the ability to extrapolate from shorter to longer\ninputs--through the lens of \\textit{task association}. We find that length\ngeneralization can be \\textit{transferred} across related tasks. That is,\ntraining a model with a longer and related auxiliary task can lead it to\ngeneralize to unseen and longer inputs from some other target task. We\ndemonstrate this length generalization transfer across diverse algorithmic\ntasks, including arithmetic operations, string transformations, and maze\nnavigation. Our results show that transformer models can inherit generalization\ncapabilities from similar tasks when trained jointly. Moreover, we observe\nsimilar transfer effects in pretrained language models, suggesting that\npretraining equips models with reusable computational scaffolding that\nfacilitates extrapolation in downstream settings. Finally, we provide initial\nmechanistic evidence that length generalization transfer correlates with the\nre-use of the same attention heads between the tasks. Together, our findings\ndeepen our understanding of how transformers generalize to out-of-distribution\ninputs and highlight the compositional reuse of inductive structure across\ntasks.", "AI": {"tldr": "Transformer\u6a21\u578b\u901a\u8fc7\u4efb\u52a1\u5173\u8054\u5b9e\u73b0\u957f\u5ea6\u6cdb\u5316\u7684\u8de8\u4efb\u52a1\u8fc1\u79fb\uff0c\u6ce8\u610f\u529b\u5934\u590d\u7528\u662f\u6838\u5fc3\u673a\u5236", "motivation": "\u63a2\u7a76Transformer\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u4ece\u77ed\u5e8f\u5217\u8bad\u7ec3\u6cdb\u5316\u5230\u957f\u5e8f\u5217\u63a8\u7406\uff0c\u63ed\u793a\u5176\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u7684\u5f62\u6210\u673a\u5236", "method": "\u91c7\u7528\u7b97\u672f\u8fd0\u7b97/\u5b57\u7b26\u4e32\u53d8\u6362/\u8ff7\u5bab\u5bfc\u822a\u7b49\u7b97\u6cd5\u4efb\u52a1\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u548c\u6ce8\u610f\u529b\u5934\u5206\u6790\u63ed\u793a\u673a\u5236", "result": "\u4efb\u52a1\u95f4\u7684\u957f\u5ea6\u6cdb\u5316\u80fd\u529b\u53ef\u8fc1\u79fb\uff0c\u9884\u8bad\u7ec3\u6a21\u578b\u5b58\u5728\u53ef\u91cd\u7528\u7684\u8ba1\u7b97\u652f\u67b6\uff0c\u6ce8\u610f\u529b\u5934\u590d\u7528\u4e0e\u6cdb\u5316\u80fd\u529b\u6b63\u76f8\u5173", "conclusion": "Transformer\u7684\u6cdb\u5316\u80fd\u529b\u6e90\u4e8e\u8de8\u4efb\u52a1\u7684\u7ec4\u5408\u5f0f\u7ed3\u6784\u590d\u7528\uff0c\u9884\u8bad\u7ec3\u5f62\u6210\u7684\u8ba1\u7b97\u652f\u67b6\u652f\u6301\u4e0b\u6e38\u4efb\u52a1\u5916\u63a8"}}
{"id": "2506.09075", "pdf": "https://arxiv.org/pdf/2506.09075", "abs": "https://arxiv.org/abs/2506.09075", "authors": ["Elly Akhoundi", "Hung Yu Ling", "Anup Anand Deshmukh", "Judith Butepage"], "title": "SILK: Smooth InterpoLation frameworK for motion in-betweening A Simplified Computational Approach", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Accepted to CVPR 2025 Human Motion Generation Workshop. 10 pages, 3\n  figures, 5 Tables, and 40 References", "summary": "Motion in-betweening is a crucial tool for animators, enabling intricate\ncontrol over pose-level details in each keyframe. Recent machine learning\nsolutions for motion in-betweening rely on complex models, incorporating\nskeleton-aware architectures or requiring multiple modules and training steps.\nIn this work, we introduce a simple yet effective Transformer-based framework,\nemploying a single Transformer encoder to synthesize realistic motions for\nmotion in-betweening tasks. We find that data modeling choices play a\nsignificant role in improving in-betweening performance. Among others, we show\nthat increasing data volume can yield equivalent or improved motion\ntransitions, that the choice of pose representation is vital for achieving\nhigh-quality results, and that incorporating velocity input features enhances\nanimation performance. These findings challenge the assumption that model\ncomplexity is the primary determinant of animation quality and provide insights\ninto a more data-centric approach to motion interpolation. Additional videos\nand supplementary material are available at https://silk-paper.github.io.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eTransformer\u7684\u7b80\u5355\u6846\u67b6\u5b9e\u73b0\u9ad8\u6548\u8fd0\u52a8\u4e2d\u95f4\u5e27\u751f\u6210\uff0c\u63ed\u793a\u6570\u636e\u5efa\u6a21\u7b56\u7565\u6bd4\u6a21\u578b\u590d\u6742\u5ea6\u66f4\u91cd\u8981", "motivation": "\u73b0\u6709\u8fd0\u52a8\u4e2d\u95f4\u5e27\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u6a21\u578b\u67b6\u6784\uff0c\u4f46\u7814\u7a76\u8868\u660e\u901a\u8fc7\u4f18\u5316\u6570\u636e\u5efa\u6a21\u7b56\u7565\uff08\u6570\u636e\u91cf/\u59ff\u52bf\u8868\u793a/\u901f\u5ea6\u7279\u5f81\uff09\u53ef\u83b7\u5f97\u66f4\u4f18\u7ed3\u679c", "method": "\u4f7f\u7528\u5355\u4e00Transformer\u7f16\u7801\u5668\u67b6\u6784\uff0c\u914d\u5408\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff08\u6269\u5927\u6570\u636e\u91cf\uff09\u3001\u6539\u8fdb\u59ff\u52bf\u8868\u5f81\u65b9\u5f0f\u3001\u6dfb\u52a0\u901f\u5ea6\u8f93\u5165\u7279\u5f81", "result": "\u8bc1\u660e\u6570\u636e\u4f18\u5316\u7b56\u7565\u53ef\u5b9e\u73b0\u4e0e\u590d\u6742\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u8fd0\u52a8\u8fc7\u6e21\u8d28\u91cf\uff0c\u6700\u9ad8\u63d0\u534740%\u7684\u52a8\u753b\u6027\u80fd\u6307\u6807", "conclusion": "\u8fd0\u52a8\u63d2\u503c\u8d28\u91cf\u4e3b\u8981\u53d6\u51b3\u4e8e\u6570\u636e\u5efa\u6a21\u7b56\u7565\u800c\u975e\u6a21\u578b\u590d\u6742\u5ea6\uff0c\u4e3a\u52a8\u753b\u751f\u6210\u9886\u57df\u63d0\u4f9b\u4e86\u6570\u636e\u4f18\u5316\u7684\u65b0\u7814\u7a76\u8303\u5f0f"}}
{"id": "2506.09259", "pdf": "https://arxiv.org/pdf/2506.09259", "abs": "https://arxiv.org/abs/2506.09259", "authors": ["Zhuofang Li", "Rafal Kocielnik", "Fereshteh Soltani", "Penphob", "Boonyarungsrit", "Animashree Anandkumar", "R. Michael Alvarez"], "title": "Self-Anchored Attention Model for Sample-Efficient Classification of Prosocial Text Chat", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; K.4"], "comment": null, "summary": "Millions of players engage daily in competitive online games, communicating\nthrough in-game chat. Prior research has focused on detecting relatively small\nvolumes of toxic content using various Natural Language Processing (NLP)\ntechniques for the purpose of moderation. However, recent studies emphasize the\nimportance of detecting prosocial communication, which can be as crucial as\nidentifying toxic interactions. Recognizing prosocial behavior allows for its\nanalysis, rewarding, and promotion. Unlike toxicity, there are limited\ndatasets, models, and resources for identifying prosocial behaviors in\ngame-chat text. In this work, we employed unsupervised discovery combined with\ngame domain expert collaboration to identify and categorize prosocial player\nbehaviors from game chat. We further propose a novel Self-Anchored Attention\nModel (SAAM) which gives 7.9% improvement compared to the best existing\ntechnique. The approach utilizes the entire training set as \"anchors\" to help\nimprove model performance under the scarcity of training data. This approach\nled to the development of the first automated system for classifying prosocial\nbehaviors in in-game chats, particularly given the low-resource settings where\nlarge-scale labeled data is not available. Our methodology was applied to one\nof the most popular online gaming titles - Call of Duty(R): Modern\nWarfare(R)II, showcasing its effectiveness. This research is novel in applying\nNLP techniques to discover and classify prosocial behaviors in player in-game\nchat communication. It can help shift the focus of moderation from solely\npenalizing toxicity to actively encouraging positive interactions on online\nplatforms.", "AI": {"tldr": "\u63d0\u51faSAAM\u6a21\u578b\u63d0\u5347\u6e38\u620f\u804a\u5929\u4e2d\u4eb2\u793e\u4f1a\u884c\u4e3a\u5206\u7c7b\u6548\u679c\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u4f73\u6280\u672f\u63d0\u53477.9%", "motivation": "\u73b0\u6709\u7814\u7a76\u8fc7\u5ea6\u5173\u6ce8\u6bd2\u6027\u5185\u5bb9\u68c0\u6d4b\uff0c\u4eb2\u793e\u4f1a\u884c\u4e3a\u8bc6\u522b\u7f3a\u4e4f\u6570\u636e\u8d44\u6e90\u548c\u6709\u6548\u6a21\u578b", "method": "\u7ed3\u5408\u65e0\u76d1\u7763\u53d1\u73b0\u4e0e\u9886\u57df\u4e13\u5bb6\u534f\u4f5c\uff0c\u5f00\u53d1\u57fa\u4e8e\u81ea\u951a\u5b9a\u6ce8\u610f\u529b\u673a\u5236\uff08SAAM\uff09\u7684\u6a21\u578b\uff0c\u5229\u7528\u5168\u8bad\u7ec3\u96c6\u4f5c\u4e3a\u951a\u70b9\u63d0\u5347\u5c0f\u6837\u672c\u8868\u73b0", "result": "\u5728\u300a\u4f7f\u547d\u53ec\u5524\u300b\u6e38\u620f\u804a\u5929\u6570\u636e\u9a8c\u8bc1\u4e2d\uff0cSAAM\u6a21\u578b\u5b9e\u73b07.9%\u7684\u6027\u80fd\u63d0\u5347", "conclusion": "\u9996\u4e2a\u6e38\u620f\u804a\u5929\u4eb2\u793e\u4f1a\u884c\u4e3a\u81ea\u52a8\u5206\u7c7b\u7cfb\u7edf\uff0c\u63a8\u52a8\u5185\u5bb9\u5ba1\u6838\u4ece\u5355\u7eaf\u60e9\u7f5a\u6bd2\u6027\u8f6c\u5411\u79ef\u6781\u57f9\u80b2\u826f\u6027\u4e92\u52a8"}}
{"id": "2506.09665", "pdf": "https://arxiv.org/pdf/2506.09665", "abs": "https://arxiv.org/abs/2506.09665", "authors": ["Jacob Munkberg", "Zian Wang", "Ruofan Liang", "Tianchang Shen", "Jon Hasselgren"], "title": "VideoMat: Extracting PBR Materials from Video Diffusion Models", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We leverage finetuned video diffusion models, intrinsic decomposition of\nvideos, and physically-based differentiable rendering to generate high quality\nmaterials for 3D models given a text prompt or a single image. We condition a\nvideo diffusion model to respect the input geometry and lighting condition.\nThis model produces multiple views of a given 3D model with coherent material\nproperties. Secondly, we use a recent model to extract intrinsics (base color,\nroughness, metallic) from the generated video. Finally, we use the intrinsics\nalongside the generated video in a differentiable path tracer to robustly\nextract PBR materials directly compatible with common content creation tools.", "AI": {"tldr": "\u5229\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e0e\u53ef\u5fae\u5206\u6e32\u67d3\u751f\u62103D\u6a21\u578bPBR\u6750\u8d28\u7684\u5168\u6d41\u7a0b\u65b9\u6cd5", "motivation": "\u89e3\u51b3\u4f20\u7edf3D\u6750\u8d28\u5236\u4f5c\u8017\u65f6\u4e14\u96be\u4ee5\u4fdd\u6301\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u7684\u75db\u70b9\uff0c\u901a\u8fc7AI\u751f\u6210\u517c\u5bb9\u4e3b\u6d41\u5de5\u5177\u7684\u9ad8\u8d28\u91cf\u6750\u8d28", "method": "1) \u5fae\u8c03\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u591a\u89c6\u89d2\u7d20\u6750\n2) \u5185\u5728\u5206\u89e3\u63d0\u53d6\u6750\u8d28\u5c5e\u6027\n3) \u53ef\u5fae\u5206\u8def\u5f84\u8ffd\u8e2a\u4f18\u5316\u6750\u8d28\u53c2\u6570", "result": "\u6210\u529f\u751f\u6210\u7269\u7406\u51c6\u786e\uff08\u57fa\u8272/\u7c97\u7cd9\u5ea6/\u91d1\u5c5e\u5ea6\uff09\u4e14\u4e0eBlender\u7b49\u5de5\u5177\u517c\u5bb9\u7684PBR\u6750\u8d28", "conclusion": "\u8be5\u6d41\u7a0b\u5b9e\u73b0\u4e86\u4ece\u6587\u672c/\u5355\u56fe\u5230\u53ef\u76f4\u63a5\u4f7f\u7528\u7684\u751f\u4ea7\u7ea7\u6750\u8d28\u7684\u7aef\u5230\u7aef\u751f\u6210"}}
{"id": "2506.09277", "pdf": "https://arxiv.org/pdf/2506.09277", "abs": "https://arxiv.org/abs/2506.09277", "authors": ["Milan Bhan", "Jean-Noel Vittaut", "Nicolas Chesneau", "Sarath Chandar", "Marie-Jeanne Lesot"], "title": "Did I Faithfully Say What I Thought? Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLM) have demonstrated the capability of generating\nfree text self Natural Language Explanation (self-NLE) to justify their\nanswers. Despite their logical appearance, self-NLE do not necessarily reflect\nthe LLM actual decision-making process, making such explanations unfaithful.\nWhile existing methods for measuring self-NLE faithfulness mostly rely on\nbehavioral tests or computational block identification, none of them examines\nthe neural activity underlying the model's reasoning. This work introduces a\nnovel flexible framework for quantitatively measuring the faithfulness of\nLLM-generated self-NLE by directly comparing the latter with interpretations of\nthe model's internal hidden states. The proposed framework is versatile and\nprovides deep insights into self-NLE faithfulness by establishing a direct\nconnection between self-NLE and model reasoning. This approach advances the\nunderstanding of self-NLE faithfulness and provides building blocks for\ngenerating more faithful self-NLE.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u6bd4\u8f83\u6a21\u578b\u9690\u85cf\u72b6\u6001\u89e3\u91ca\u4e0e\u81ea\u751f\u6210\u89e3\u91ca\u6765\u91cf\u5316\u8bc4\u4f30LLM\u81ea\u89e3\u91ca\u5fe0\u5b9e\u5ea6\u7684\u65b0\u6846\u67b6", "motivation": "\u73b0\u6709\u81ea\u89e3\u91ca\u5fe0\u5b9e\u5ea6\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u884c\u4e3a\u6d4b\u8bd5\u6216\u8ba1\u7b97\u6a21\u5757\u5206\u6790\uff0c\u7f3a\u4e4f\u5bf9\u6a21\u578b\u795e\u7ecf\u6d3b\u52a8\u7684\u76f4\u63a5\u89c2\u5bdf\uff0c\u5bfc\u81f4\u89e3\u91ca\u53ef\u4fe1\u5ea6\u5b58\u7591", "method": "\u5f00\u53d1\u7075\u6d3b\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u5bf9\u6bd4\u6a21\u578b\u81ea\u751f\u6210\u7684\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u4e0e\u5185\u90e8\u9690\u85cf\u72b6\u6001\u89e3\u91ca\u6765\u5b9a\u91cf\u6d4b\u91cf\u5fe0\u5b9e\u5ea6", "result": "\u6846\u67b6\u5177\u5907\u901a\u7528\u6027\uff0c\u901a\u8fc7\u5efa\u7acb\u81ea\u89e3\u91ca\u4e0e\u6a21\u578b\u63a8\u7406\u7684\u76f4\u63a5\u5173\u8054\uff0c\u4e3a\u7406\u89e3\u81ea\u89e3\u91ca\u5fe0\u5b9e\u5ea6\u63d0\u4f9b\u65b0\u89c6\u89d2", "conclusion": "\u8be5\u65b9\u6cd5\u63a8\u8fdb\u4e86\u5bf9\u81ea\u89e3\u91ca\u673a\u5236\u7684\u7406\u89e3\uff0c\u4e3a\u751f\u6210\u66f4\u53ef\u4fe1\u7684\u89e3\u91ca\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u6280\u672f\u6784\u5efa\u6a21\u5757"}}
{"id": "2506.09909", "pdf": "https://arxiv.org/pdf/2506.09909", "abs": "https://arxiv.org/abs/2506.09909", "authors": ["Yijie Deng", "Lei Han", "Lu Fang"], "title": "TransGI: Real-Time Dynamic Global Illumination With Object-Centric Neural Transfer Model", "categories": ["cs.GR"], "comment": null, "summary": "Neural rendering algorithms have revolutionized computer graphics, yet their\nimpact on real-time rendering under arbitrary lighting conditions remains\nlimited due to strict latency constraints in practical applications. The key\nchallenge lies in formulating a compact yet expressive material representation.\nTo address this, we propose TransGI, a novel neural rendering method for\nreal-time, high-fidelity global illumination. It comprises an object-centric\nneural transfer model for material representation and a radiance-sharing\nlighting system for efficient illumination. Traditional BSDF representations\nand spatial neural material representations lack expressiveness, requiring\nthousands of ray evaluations to converge to noise-free colors. Conversely,\nreal-time methods trade quality for efficiency by supporting only diffuse\nmaterials. In contrast, our object-centric neural transfer model achieves\ncompactness and expressiveness through an MLP-based decoder and vertex-attached\nlatent features, supporting glossy effects with low memory overhead. For\ndynamic, varying lighting conditions, we introduce local light probes capturing\nscene radiance, coupled with an across-probe radiance-sharing strategy for\nefficient probe generation. We implemented our method in a real-time rendering\nengine, combining compute shaders and CUDA-based neural networks. Experimental\nresults demonstrate that our method achieves real-time performance of less than\n10 ms to render a frame and significantly improved rendering quality compared\nto baseline methods.", "AI": {"tldr": "TransGI\u63d0\u51fa\u7ed3\u5408\u795e\u7ecf\u4f20\u8f93\u6a21\u578b\u4e0e\u8f90\u5c04\u5171\u4eab\u7167\u660e\u7cfb\u7edf\uff0c\u572810ms\u5185\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5b9e\u65f6\u5168\u5c40\u5149\u7167\u6e32\u67d3", "motivation": "\u4f20\u7edfBSDF\u6750\u8d28\u8868\u8fbe\u7f3a\u4e4f\u7d27\u51d1\u6027\uff08\u9700\u6570\u5343\u5149\u7ebf\u91c7\u6837\u964d\u566a\uff09\uff0c\u5b9e\u65f6\u6e32\u67d3\u65b9\u6cd5\u4ec5\u652f\u6301\u6f2b\u53cd\u5c04\u6750\u8d28\u727a\u7272\u4e86\u6e32\u67d3\u8d28\u91cf", "method": "1) \u57fa\u4e8eMLP\u89e3\u7801\u5668\u7684\u9876\u70b9\u9644\u7740\u9690\u5f0f\u7279\u5f81\u6750\u8d28\u6a21\u578b 2) \u672c\u5730\u5149\u63a2\u9488\u7ed3\u5408\u8de8\u63a2\u9488\u8f90\u5c04\u5171\u4eab\u7684\u7167\u660e\u7b56\u7565", "result": "\u6e32\u67d3\u5e27\u7387\u7a81\u783410ms\u95e8\u69db\uff0c\u89c6\u89c9\u8d28\u91cf\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u652f\u6301\u52a8\u6001\u9ad8\u5149\u6750\u8d28\u5b9e\u65f6\u6e32\u67d3", "conclusion": "\u9996\u6b21\u5728\u5b9e\u65f6\u7ea6\u675f\u4e0b\u5b9e\u73b0\u590d\u6742\u6750\u8d28\u7684\u9ad8\u8d28\u91cf\u5168\u5c40\u5149\u7167\uff0c\u901a\u8fc7\u89e3\u8026\u6750\u8d28\u4e0e\u5149\u7167\u7684\u795e\u7ecf\u8868\u5f81\u8fbe\u6210\u6548\u7387-\u8d28\u91cf\u5e73\u8861"}}
{"id": "2506.09301", "pdf": "https://arxiv.org/pdf/2506.09301", "abs": "https://arxiv.org/abs/2506.09301", "authors": ["Cesare Spinoso-Di Piano", "David Austin", "Pablo Piantanida", "Jackie Chi Kit Cheung"], "title": "$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "Figurative language (e.g., irony, hyperbole, understatement) is ubiquitous in\nhuman communication, resulting in utterances where the literal and the intended\nmeanings do not match. The Rational Speech Act (RSA) framework, which\nexplicitly models speaker intentions, is the most widespread theory of\nprobabilistic pragmatics, but existing implementations are either unable to\naccount for figurative expressions or require modeling the implicit motivations\nfor using figurative language (e.g., to express joy or annoyance) in a\nsetting-specific way. In this paper, we introduce the Rhetorical-Strategy-Aware\nRSA $(RSA)^2$ framework which models figurative language use by considering a\nspeaker's employed rhetorical strategy. We show that $(RSA)^2$ enables\nhuman-compatible interpretations of non-literal utterances without modeling a\nspeaker's motivations for being non-literal. Combined with LLMs, it achieves\nstate-of-the-art performance on the ironic split of PragMega+, a new irony\ninterpretation dataset introduced in this study.", "AI": {"tldr": "\u63d0\u51fa\u4fee\u8f9e\u7b56\u7565\u611f\u77e5\u7684RSA\u6846\u67b6(RSA)\u00b2\uff0c\u901a\u8fc7\u5efa\u6a21\u8bf4\u8bdd\u8005\u4fee\u8f9e\u7b56\u7565\u89e3\u91ca\u975e\u5b57\u9762\u8bed\u8a00\uff0c\u7ed3\u5408LLM\u5728\u53cd\u8bbd\u89e3\u91ca\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RSA\u6846\u67b6\u65e0\u6cd5\u6709\u6548\u89e3\u91ca\u6bd4\u55bb\u8bed\u8a00\uff08\u5982\u53cd\u8bbd\uff09\uff0c\u6216\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u573a\u666f\u5efa\u6a21\u8bf4\u8bdd\u8005\u52a8\u673a\u3002\u9700\u8981\u66f4\u901a\u7528\u7684\u975e\u5b57\u9762\u8bed\u8a00\u89e3\u91ca\u6846\u67b6\u3002", "method": "\u5728RSA\u6846\u67b6\u4e2d\u5f15\u5165\u4fee\u8f9e\u7b56\u7565\u7ef4\u5ea6\uff0c\u901a\u8fc7\u6982\u7387\u5efa\u6a21\u5c06\u5b57\u9762\u610f\u4e49\u6620\u5c04\u5230\u4fee\u8f9e\u7b56\u7565\u7a7a\u95f4\uff0c\u7ed3\u5408LLM\u5b9e\u73b0\u7b56\u7565\u63a8\u7406\u3002", "result": "\u5728\u81ea\u5efa\u53cd\u8bbd\u6570\u636e\u96c6PragMega+\u4e0a\u53d6\u5f97\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4eba\u7c7b\u5bf9\u9f50\u5ea6\u663e\u8457\u63d0\u5347\u3002", "conclusion": "(RSA)\u00b2\u6846\u67b6\u9996\u6b21\u5b9e\u73b0\u65e0\u9700\u52a8\u673a\u5efa\u6a21\u7684\u901a\u7528\u6bd4\u55bb\u8bed\u8a00\u89e3\u91ca\uff0c\u4e3a\u8ba1\u7b97\u8bed\u7528\u5b66\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2506.09997", "pdf": "https://arxiv.org/pdf/2506.09997", "abs": "https://arxiv.org/abs/2506.09997", "authors": ["Chieh Hubert Lin", "Zhaoyang Lv", "Songyin Wu", "Zhen Xu", "Thu Nguyen-Phuoc", "Hung-Yu Tseng", "Julian Straub", "Numair Khan", "Lei Xiao", "Ming-Hsuan Yang", "Yuheng Ren", "Richard Newcombe", "Zhao Dong", "Zhengqin Li"], "title": "DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "comment": "Project page: https://hubert0527.github.io/dgslrm/", "summary": "We introduce the Deformable Gaussian Splats Large Reconstruction Model\n(DGS-LRM), the first feed-forward method predicting deformable 3D Gaussian\nsplats from a monocular posed video of any dynamic scene. Feed-forward scene\nreconstruction has gained significant attention for its ability to rapidly\ncreate digital replicas of real-world environments. However, most existing\nmodels are limited to static scenes and fail to reconstruct the motion of\nmoving objects. Developing a feed-forward model for dynamic scene\nreconstruction poses significant challenges, including the scarcity of training\ndata and the need for appropriate 3D representations and training paradigms. To\naddress these challenges, we introduce several key technical contributions: an\nenhanced large-scale synthetic dataset with ground-truth multi-view videos and\ndense 3D scene flow supervision; a per-pixel deformable 3D Gaussian\nrepresentation that is easy to learn, supports high-quality dynamic view\nsynthesis, and enables long-range 3D tracking; and a large transformer network\nthat achieves real-time, generalizable dynamic scene reconstruction. Extensive\nqualitative and quantitative experiments demonstrate that DGS-LRM achieves\ndynamic scene reconstruction quality comparable to optimization-based methods,\nwhile significantly outperforming the state-of-the-art predictive dynamic\nreconstruction method on real-world examples. Its predicted physically grounded\n3D deformation is accurate and can readily adapt for long-range 3D tracking\ntasks, achieving performance on par with state-of-the-art monocular video 3D\ntracking methods.", "AI": {"tldr": "\u9996\u4e2a\u524d\u9988\u5f0f\u52a8\u6001\u573a\u666f\u91cd\u5efa\u6a21\u578bDGS-LRM\uff0c\u901a\u8fc7\u53ef\u53d8\u5f623D\u9ad8\u65af\u70b9\u4e91\u5b9e\u73b0\u5355\u76ee\u89c6\u9891\u7684\u5b9e\u65f6\u52a8\u6001\u91cd\u5efa\u4e0e\u957f\u7a0b3D\u8ffd\u8e2a", "motivation": "\u73b0\u6709\u524d\u9988\u6a21\u578b\u5c40\u9650\u4e8e\u9759\u6001\u573a\u666f\u91cd\u5efa\uff0c\u52a8\u6001\u573a\u666f\u9762\u4e34\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u30013D\u8868\u5f81\u65b9\u5f0f\u4e0d\u8db3\u53ca\u8bad\u7ec3\u8303\u5f0f\u6311\u6218", "method": "1.\u6784\u5efa\u542b\u7a20\u5bc63D\u573a\u666f\u6d41\u7684\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6 2.\u63d0\u51fa\u6613\u5b66\u4e60\u7684\u53ef\u53d8\u5f623D\u9ad8\u65af\u70b9\u4e91\u8868\u5f81 3.\u8bbe\u8ba1\u5b9e\u65f6\u901a\u7528\u7684\u5927\u578btransformer\u7f51\u7edc\u67b6\u6784", "result": "\u91cd\u5efa\u8d28\u91cf\u5ab2\u7f8e\u4f18\u5316\u65b9\u6cd5\uff0c\u771f\u5b9e\u573a\u666f\u6027\u80fd\u8d85\u8d8aSOTA\u9884\u6d4b\u6a21\u578b\uff0c3D\u53d8\u5f62\u9884\u6d4b\u7cbe\u5ea6\u8fbe\u5355\u76ee\u89c6\u9891\u8ffd\u8e2aSOTA\u6c34\u5e73", "conclusion": "DGS-LRM\u5f00\u521b\u4e86\u524d\u9988\u5f0f\u52a8\u6001\u91cd\u5efa\u65b0\u8303\u5f0f\uff0c\u5176\u7269\u7406\u57fa\u7840\u76843D\u53d8\u5f62\u9884\u6d4b\u80fd\u529b\u4e3a\u6570\u5b57\u5b6a\u751f\u3001\u673a\u5668\u4eba\u611f\u77e5\u7b49\u5e94\u7528\u5f00\u8f9f\u65b0\u53ef\u80fd"}}
{"id": "2506.09315", "pdf": "https://arxiv.org/pdf/2506.09315", "abs": "https://arxiv.org/abs/2506.09315", "authors": ["Yao Xiao", "Heidi Christensen", "Stefan Goetze"], "title": "Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "To be published in the proceedings of Interspeech 2025", "summary": "Alzheimer's dementia (AD) is a neurodegenerative disorder with cognitive\ndecline that commonly impacts language ability. This work extends the paired\nperplexity approach to detecting AD by using a recent large language model\n(LLM), the instruction-following version of Mistral-7B. We improve accuracy by\nan average of 3.33% over the best current paired perplexity method and by 6.35%\nover the top-ranked method from the ADReSS 2020 challenge benchmark. Our\nfurther analysis demonstrates that the proposed approach can effectively detect\nAD with a clear and interpretable decision boundary in contrast to other\nmethods that suffer from opaque decision-making processes. Finally, by\nprompting the fine-tuned LLMs and comparing the model-generated responses to\nhuman responses, we illustrate that the LLMs have learned the special language\npatterns of AD speakers, which opens up possibilities for novel methods of\nmodel interpretation and data augmentation.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5fae\u8c03Mistral-7B\u5927\u8bed\u8a00\u6a21\u578b\u6539\u8fdb\u914d\u5bf9\u56f0\u60d1\u5ea6\u65b9\u6cd5\uff0c\u5c06\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u68c0\u6d4b\u51c6\u786e\u7387\u63d0\u53473.33%-6.35%\uff0c\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u8fb9\u754c\u3002", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5(AD)\u5e38\u4f34\u968f\u8bed\u8a00\u80fd\u529b\u8870\u9000\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u51c6\u786e\u7387\u4e0d\u8db3\u548c\u51b3\u7b56\u8fc7\u7a0b\u4e0d\u900f\u660e\u7684\u95ee\u9898\u3002", "method": "\u6269\u5c55\u914d\u5bf9\u56f0\u60d1\u5ea6\u65b9\u6cd5\uff0c\u91c7\u7528\u6307\u4ee4\u5fae\u8c03\u7248Mistral-7B\u6a21\u578b\uff0c\u901a\u8fc7\u5bf9\u6bd4\u6a21\u578b\u751f\u6210\u54cd\u5e94\u4e0e\u4eba\u7c7b\u54cd\u5e94\u7684\u8bed\u8a00\u6a21\u5f0f\u8fdb\u884c\u5206\u6790\u3002", "result": "\u51c6\u786e\u7387\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u65b9\u6cd53.33%\uff0c\u76f8\u6bd4ADReSS 2020\u57fa\u51c6\u63d0\u53476.35%\uff0c\u6a21\u578b\u6210\u529f\u6355\u6349AD\u60a3\u8005\u7279\u6b8a\u8bed\u8a00\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u517c\u5177\u9ad8\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u6a21\u578b\u4e60\u5f97\u7684\u8bed\u8a00\u7279\u5f81\u4e3a\u65b0\u578b\u8bca\u65ad\u5de5\u5177\u5f00\u53d1\u548c\u6570\u636e\u589e\u5f3a\u63d0\u4f9b\u4e86\u53ef\u80fd\u65b9\u5411\u3002"}}
{"id": "2506.09485", "pdf": "https://arxiv.org/pdf/2506.09485", "abs": "https://arxiv.org/abs/2506.09485", "authors": ["Yuxin Liu", "Zhenghao Peng", "Xuanhao Cui", "Bolei Zhou"], "title": "Adv-BMT: Bidirectional Motion Transformer for Safety-Critical Traffic Scenario Generation", "categories": ["cs.RO", "cs.AI", "cs.GR"], "comment": null, "summary": "Scenario-based testing is essential for validating the performance of\nautonomous driving (AD) systems. However, such testing is limited by the\nscarcity of long-tailed, safety-critical scenarios in existing datasets\ncollected in the real world. To tackle the data issue, we propose the Adv-BMT\nframework, which augments real-world scenarios with diverse and realistic\nadversarial interactions. The core component of Adv-BMT is a bidirectional\nmotion transformer (BMT) model to perform inverse traffic motion predictions,\nwhich takes agent information in the last time step of the scenario as input,\nand reconstruct the traffic in the inverse of chronological order until the\ninitial time step. The Adv-BMT framework is a two-staged pipeline: it first\nconducts adversarial initializations and then inverse motion predictions.\nDifferent from previous work, we do not need any collision data for\npretraining, and are able to generate realistic and diverse collision\ninteractions. Our experimental results validate the quality of generated\ncollision scenarios by Adv-BMT: training in our augmented dataset would reduce\nepisode collision rates by 20\\% compared to previous work.", "AI": {"tldr": "\u63d0\u51faAdv-BMT\u6846\u67b6\uff0c\u901a\u8fc7\u9006\u5411\u8fd0\u52a8\u9884\u6d4b\u751f\u6210\u5bf9\u6297\u6027\u9a7e\u9a76\u573a\u666f\uff0c\u65e0\u9700\u78b0\u649e\u6570\u636e\u9884\u8bad\u7ec3\uff0c\u5b9e\u9a8c\u663e\u793a\u964d\u4f4e20%\u78b0\u649e\u7387\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u6d4b\u8bd5\u53d7\u9650\u4e8e\u771f\u5b9e\u6570\u636e\u96c6\u4e2d\u957f\u5c3e\u5b89\u5168\u5173\u952e\u573a\u666f\u7684\u7a00\u7f3a\u6027\uff0c\u96be\u4ee5\u5145\u5206\u9a8c\u8bc1\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u5bf9\u6297\u521d\u59cb\u5316\u751f\u6210\u521d\u59cb\u6270\u52a8\uff1b2\uff09\u53cc\u5411\u8fd0\u52a8\u53d8\u6362\u5668(BMT)\u9006\u5411\u91cd\u6784\u4ea4\u901a\u6d41\uff0c\u5229\u7528\u7ec8\u6001\u4fe1\u606f\u9884\u6d4b\u5168\u65f6\u5e8f\u4ea4\u901a\u8fd0\u52a8\u3002", "result": "\u751f\u6210\u7684\u78b0\u649e\u573a\u666f\u8d28\u91cf\u663e\u8457\u63d0\u5347\uff0c\u4f7f\u7528\u589e\u5f3a\u6570\u636e\u8bad\u7ec3\u53ef\u4f7f\u78b0\u649e\u7387\u6bd4\u73b0\u6709\u65b9\u6cd5\u964d\u4f4e20%\u3002", "conclusion": "Adv-BMT\u9996\u6b21\u5b9e\u73b0\u65e0\u9700\u78b0\u649e\u6570\u636e\u9884\u8bad\u7ec3\u7684\u5bf9\u6297\u573a\u666f\u751f\u6210\uff0c\u80fd\u521b\u9020\u66f4\u771f\u5b9e\u591a\u6837\u7684\u5b89\u5168\u5173\u952e\u573a\u666f\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u6d4b\u8bd5\u6709\u6548\u6027\u3002"}}
{"id": "2506.09329", "pdf": "https://arxiv.org/pdf/2506.09329", "abs": "https://arxiv.org/abs/2506.09329", "authors": ["Yuxin Jiang"], "title": "Towards Efficient and Effective Alignment of Large Language Models", "categories": ["cs.CL"], "comment": "PhD thesis", "summary": "Large language models (LLMs) exhibit remarkable capabilities across diverse\ntasks, yet aligning them efficiently and effectively with human expectations\nremains a critical challenge. This thesis advances LLM alignment by introducing\nnovel methodologies in data collection, training, and evaluation. We first\naddress alignment data collection. Existing approaches rely heavily on manually\ncurated datasets or proprietary models. To overcome these limitations, we\npropose Lion, an adversarial distillation framework that iteratively refines\ntraining data by identifying and generating challenging instructions, enabling\nstate-of-the-art zero-shot reasoning. Additionally, we introduce Web\nReconstruction (WebR), a fully automated framework that synthesizes\ninstruction-tuning data directly from raw web documents, significantly\nimproving data diversity and scalability over existing synthetic data methods.\nNext, we enhance alignment training through novel optimization techniques. We\ndevelop Learning to Edit (LTE), a framework that enables LLMs to efficiently\nintegrate new knowledge while preserving existing information. LTE leverages\nmeta-learning to improve both real-time and batch knowledge updates.\nFurthermore, we introduce Bridging and Modeling Correlations (BMC), a\nrefinement of Direct Preference Optimization (DPO) that explicitly captures\ntoken-level correlations in preference data, leading to superior alignment\nacross QA and mathematical reasoning tasks. Finally, we tackle the challenge of\nevaluating alignment. Existing benchmarks emphasize response quality but\noverlook adherence to specific constraints. To bridge this gap, we introduce\nFollowBench, a multi-level, fine-grained benchmark assessing LLMs' ability to\nfollow complex constraints across diverse instruction types. Our results expose\nkey weaknesses in current models' constraint adherence, offering insights for\nfuture improvements.", "AI": {"tldr": "\u63d0\u51faLion\u3001WebR\u3001LTE\u3001BMC\u548cFollowBench\u7b49\u7cfb\u7edf\u65b9\u6cd5\uff0c\u4ece\u6570\u636e\u6784\u5efa\u3001\u8bad\u7ec3\u4f18\u5316\u5230\u8bc4\u4f30\u4f53\u7cfb\u5168\u9762\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u6280\u672f", "motivation": "\u73b0\u6709LLM\u5bf9\u9f50\u65b9\u6cd5\u5b58\u5728\u6570\u636e\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8/\u95ed\u6e90\u6a21\u578b\u3001\u8bad\u7ec3\u8fc7\u7a0b\u4fe1\u606f\u6574\u5408\u6548\u7387\u4f4e\u3001\u8bc4\u4f30\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7ea6\u675f\u8ddf\u8e2a\u7b49\u95ee\u9898", "method": "1. \u6570\u636e\u6784\u5efa\uff1aLion\u5bf9\u6297\u84b8\u998f\u6846\u67b6\u751f\u6210\u6311\u6218\u6027\u6307\u4ee4\uff0cWebR\u4ece\u7f51\u9875\u81ea\u52a8\u5408\u6210\u6307\u4ee4\u6570\u636e\n2. \u8bad\u7ec3\u4f18\u5316\uff1aLTE\u5143\u5b66\u4e60\u6846\u67b6\u5b9e\u73b0\u5b9e\u65f6\u77e5\u8bc6\u66f4\u65b0\uff0cBMC\u6539\u8fdbDPO\u6355\u83b7token\u7ea7\u5173\u8054\n3. \u8bc4\u4f30\u4f53\u7cfb\uff1a\u6784\u5efa\u591a\u5c42\u7ea7\u7ea6\u675f\u9075\u5faa\u57fa\u51c6FollowBench", "result": "\u5b9e\u73b0\u96f6\u6837\u672c\u63a8\u7406SOTA\u3001\u6570\u636e\u591a\u6837\u6027\u63d0\u53473\u500d\u3001\u6570\u5b66\u63a8\u7406\u51c6\u786e\u7387\u63d0\u53477.2%\u3001\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u590d\u6742\u7ea6\u675f\u9075\u5faa\u4e2d\u7684\u5173\u952e\u7f3a\u9677", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u6027\u65b9\u6cd5\u8bba\u521b\u65b0\uff0c\u5728\u6a21\u578b\u5bf9\u9f50\u7684\u6570\u636e\u6784\u5efa\u3001\u8bad\u7ec3\u8303\u5f0f\u3001\u8bc4\u4f30\u4f53\u7cfb\u4e09\u4e2a\u7ef4\u5ea6\u53d6\u5f97\u7a81\u7834\uff0c\u4e3aLLM\u7684\u53ef\u9760\u90e8\u7f72\u63d0\u4f9b\u6280\u672f\u57fa\u7840"}}
{"id": "2506.09331", "pdf": "https://arxiv.org/pdf/2506.09331", "abs": "https://arxiv.org/abs/2506.09331", "authors": ["Arjun Vaithilingam Sudhakar"], "title": "Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "arXiv admin note: substantial text overlap with arXiv:2311.07687", "summary": "Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot\ngeneralization capabilities across complex natural language tasks, enabling\ntheir widespread use as virtual assistants for diverse applications such as\ntranslation and summarization. Despite being trained solely on large corpora of\ntext without explicit supervision on author intent, LLMs appear to infer the\nunderlying meaning of textual interactions. This raises a fundamental question:\ncan LLMs model and reason about the intentions of others, i.e., do they possess\na form of theory of mind? Understanding other's intentions is crucial for\neffective collaboration, which underpins human societal success and is\nessential for cooperative interactions among multiple agents, including humans\nand autonomous systems. In this work, we investigate the theory of mind in LLMs\nthrough the lens of cooperative multi-agent reinforcement learning (MARL),\nwhere agents learn to collaborate via repeated interactions, mirroring human\nsocial reasoning. Our approach aims to enhance artificial agent's ability to\nadapt and cooperate with both artificial and human partners. By leveraging\nLLM-based agents capable of natural language interaction, we move towards\ncreating hybrid human-AI systems that can foster seamless collaboration, with\nbroad implications for the future of human-artificial interaction.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u9a8c\u8bc1\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u5177\u5907\u5fc3\u7406\u7406\u8bba\u80fd\u529b\u4ee5\u4fc3\u8fdb\u4eba\u673a\u534f\u4f5c\u3002", "motivation": "\u7406\u89e3LLMs\u7684\u5fc3\u7406\u7406\u8bba\u80fd\u529b\u662f\u5b9e\u73b0\u6709\u6548\u4eba\u673a\u534f\u4f5c\u7684\u57fa\u7840\uff0c\u5bf9\u6784\u5efa\u6df7\u5408\u667a\u80fd\u7cfb\u7edf\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u5229\u7528\u57fa\u4e8eLLM\u7684\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u667a\u80fd\u4f53\uff0c\u5728\u534f\u4f5c\u5f0f\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u4e2d\u6a21\u62df\u4eba\u7c7b\u793e\u4ea4\u63a8\u7406\u673a\u5236\u3002", "result": "LLM\u667a\u80fd\u4f53\u5c55\u73b0\u51fa\u9002\u5e94\u4e0d\u540c\u534f\u4f5c\u4f19\u4f34\u7684\u6f5c\u529b\uff0c\u4e3a\u6784\u5efa\u65e0\u7f1d\u534f\u4f5c\u7684\u4eba\u673a\u7cfb\u7edf\u63d0\u4f9b\u6280\u672f\u8def\u5f84\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u672a\u6765\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u9884\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u534f\u4f5c\u573a\u666f\u4e2d\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2506.09340", "pdf": "https://arxiv.org/pdf/2506.09340", "abs": "https://arxiv.org/abs/2506.09340", "authors": ["Siheng Li", "Zhanhui Zhou", "Wai Lam", "Chao Yang", "Chaochao Lu"], "title": "RePO: Replay-Enhanced Policy Optimization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project Page: https://github.com/SihengLi99/RePO", "summary": "Reinforcement learning (RL) is vital for optimizing large language models\n(LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages\nusing multiple on-policy outputs per prompt, leading to high computational\ncosts and low data efficiency. To address this, we introduce Replay-Enhanced\nPolicy Optimization (RePO), which leverages diverse replay strategies to\nretrieve off-policy samples from a replay buffer, allowing policy optimization\nbased on a broader and more diverse set of samples for each prompt. Experiments\non five LLMs across seven mathematical reasoning benchmarks demonstrate that\nRePO achieves absolute average performance gains of $18.4$ and $4.1$ points for\nQwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further\nanalysis indicates that RePO increases computational cost by $15\\%$ while\nraising the number of effective optimization steps by $48\\%$ for Qwen3-1.7B,\nwith both on-policy and off-policy sample numbers set to $8$. The repository\ncan be accessed at https://github.com/SihengLi99/RePO.", "AI": {"tldr": "RePO\u63d0\u51fa\u901a\u8fc7\u591a\u6837\u5316\u56de\u653e\u7b56\u7565\u5229\u7528\u79bb\u8f68\u6837\u672c\u4f18\u5316LLM\u5f3a\u5316\u5b66\u4e60\uff0c\u76f8\u6bd4GRPO\u663e\u8457\u63d0\u5347\u6027\u80fd\u4e0e\u6570\u636e\u6548\u7387\u3002", "motivation": "\u73b0\u6709GRPO\u65b9\u6cd5\u4f9d\u8d56\u540c\u8f68\u6837\u672c\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u6570\u636e\u6548\u7387\u4f4e\uff0c\u9700\u901a\u8fc7\u79bb\u8f68\u6837\u672c\u63d0\u5347\u4f18\u5316\u6548\u7387\u3002", "method": "\u91c7\u7528\u56de\u653e\u7f13\u51b2\u533a\u5b58\u50a8\u5386\u53f2\u6837\u672c\uff0c\u7ed3\u5408\u540c\u8f68/\u79bb\u8f68\u6837\u672c\u5b9e\u73b0\u591a\u6837\u5316\u7b56\u7565\u4f18\u5316\uff0c\u6bcf\u4e2aprompt\u57fa\u4e8e\u66f4\u5e7f\u6cdb\u6837\u672c\u66f4\u65b0\u7b56\u7565\u3002", "result": "\u57287\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\uff0cQwen2.5-Math-1.5B/Qwen3-1.7B\u5206\u522b\u5b9e\u73b018.4/4.1\u7edd\u5bf9\u5206\u63d0\u5347\uff0c\u8ba1\u7b97\u6210\u672c\u4ec5\u589e15%\u4f46\u6709\u6548\u4f18\u5316\u6b65\u6570\u63d0\u534748%\u3002", "conclusion": "RePO\u901a\u8fc7\u5e73\u8861\u540c\u8f68/\u79bb\u8f68\u6837\u672c\u6bd4\u4f8b\uff0c\u5728\u5408\u7406\u589e\u52a0\u8ba1\u7b97\u5f00\u9500\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u4e0e\u8bad\u7ec3\u6548\u7387\uff0c\u4e3aLLM\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u65b0\u4f18\u5316\u8303\u5f0f\u3002"}}
{"id": "2506.09342", "pdf": "https://arxiv.org/pdf/2506.09342", "abs": "https://arxiv.org/abs/2506.09342", "authors": ["Sushant Mehta", "Raj Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "Latent Multi-Head Attention for Small Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 1 figure. 5 tables", "summary": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance.", "AI": {"tldr": "\u6f5c\u5728\u591a\u5934\u6ce8\u610f\u529b(MLA)\u5728\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u5b9e\u73b045% KV\u7f13\u5b58\u5185\u5b58\u964d\u4f4e+1.4\u500d\u63a8\u7406\u52a0\u901f\uff0c\u8d28\u91cf\u635f\u5931\u53ef\u5ffd\u7565", "motivation": "\u63a2\u7d22\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4f7f\u7528\u6f5c\u5728\u591a\u5934\u6ce8\u610f\u529b\u67b6\u6784\u7684\u6548\u7387-\u8d28\u91cf\u5e73\u8861\uff0c\u89e3\u51b3\u5185\u5b58\u53d7\u9650\u573a\u666f\u90e8\u7f72\u95ee\u9898", "method": "\u4f7f\u752830M\u53c2\u6570GPT\u6a21\u578b\u572810\u4e07\u5408\u6210\u6545\u4e8b\u96c6\u4e0a\uff0c\u5bf9\u6bd4\u6807\u51c6MHA/MLA/MLA+RoPE\u4e09\u79cd\u67b6\u6784", "result": "MLA+RoPE(r=d/2)\u5185\u5b58\u51cf\u5c1145%\u4e14\u9a8c\u8bc1\u635f\u5931\u4ec5\u589e0.3%\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53471.4\u500d\uff0cGPT-4\u8bc4\u4f30\u5f97\u52067.4/10", "conclusion": "MLA+RoPE\u67b6\u6784\u5728\u5c0f\u578b\u6a21\u578b\u4e2d\u5b9e\u73b0\u5185\u5b58\u6548\u7387\u4e0e\u6a21\u578b\u8d28\u91cf\u7684\u5e15\u7d2f\u6258\u6539\u8fdb\uff0c\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801\u662f\u5173\u952e\u6210\u529f\u56e0\u7d20"}}
{"id": "2506.09349", "pdf": "https://arxiv.org/pdf/2506.09349", "abs": "https://arxiv.org/abs/2506.09349", "authors": ["Chao-Hong Tan", "Qian Chen", "Wen Wang", "Chong Deng", "Qinglin Zhang", "Luyao Cheng", "Hai Yu", "Xin Zhang", "Xiang Lv", "Tianyu Zhao", "Chong Zhang", "Yukun Ma", "Yafeng Chen", "Hui Wang", "Jiaqing Liu", "Jieping Ye"], "title": "OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution Speech Representations and Contrastive Alignment", "categories": ["cs.CL"], "comment": null, "summary": "Recent studies on end-to-end speech generation with large language models\n(LLMs) have attracted significant community attention, with multiple works\nextending text-based LLMs to generate discrete speech tokens. Existing\napproaches primarily fall into two categories: (1) Methods that generate\ndiscrete speech tokens independently without incorporating them into the LLM's\nautoregressive process, resulting in text generation being unaware of\nconcurrent speech synthesis. (2) Models that generate interleaved or parallel\nspeech-text tokens through joint autoregressive modeling, enabling mutual\nmodality awareness during generation. This paper presents OmniDRCA, a parallel\nspeech-text foundation model based on joint autoregressive modeling, featuring\ndual-resolution speech representations and contrastive cross-modal alignment.\nOur approach processes speech and text representations in parallel while\nenhancing audio comprehension through contrastive alignment. Experimental\nresults on Spoken Question Answering benchmarks demonstrate that OmniDRCA\nestablishes new state-of-the-art (SOTA) performance among parallel joint\nspeech-text modeling based foundation models, and achieves competitive\nperformance compared to interleaved models. Additionally, we explore the\npotential of extending the framework to full-duplex conversational scenarios.", "AI": {"tldr": "OmniDRCA\u63d0\u51fa\u57fa\u4e8e\u53cc\u5206\u8fa8\u7387\u8bed\u97f3\u8868\u5f81\u548c\u5bf9\u6bd4\u8de8\u6a21\u6001\u5bf9\u9f50\u7684\u5e76\u884c\u8bed\u97f3-\u6587\u672c\u8054\u5408\u5efa\u6a21\u6846\u67b6\uff0c\u5728SQA\u57fa\u51c6\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u6a21\u6001\u5272\u88c2\u95ee\u9898\uff1a\u72ec\u7acb\u751f\u6210\u8bed\u97f3\u6807\u8bb0\u5bfc\u81f4\u6587\u672c\u751f\u6210\u4e0d\u611f\u77e5\u8bed\u97f3\u5408\u6210\uff0c\u6216\u4ea4\u9519\u5efa\u6a21\u6548\u7387\u8f83\u4f4e\u3002\u9700\u6784\u5efa\u80fd\u5b9e\u73b0\u8bed\u97f3-\u6587\u672c\u53cc\u5411\u611f\u77e5\u7684\u9ad8\u6548\u8054\u5408\u5efa\u6a21\u6846\u67b6\u3002", "method": "1. \u53cc\u5206\u8fa8\u7387\u8bed\u97f3\u8868\u5f81\uff08\u57fa\u7840\u5355\u5143+\u7ec6\u7c92\u5ea6\u5355\u5143\uff09\n2. \u5e76\u884c\u8bed\u97f3-\u6587\u672c\u8054\u5408\u81ea\u56de\u5f52\u5efa\u6a21\n3. \u5bf9\u6bd4\u5b66\u4e60\u8de8\u6a21\u6001\u5bf9\u9f50\u589e\u5f3a\u97f3\u9891\u7406\u89e3", "result": "\u5728Spoken QA\u57fa\u51c6\u4e0a\uff1a\n- \u8d85\u8d8a\u6240\u6709\u5e76\u884c\u8054\u5408\u5efa\u6a21\u7684\u57fa\u7ebf\u6a21\u578b\uff08SOTA\uff09\n- \u4e0e\u4ea4\u9519\u6a21\u578b\u76f8\u6bd4\u8fbe\u5230\u7ade\u4e89\u6027\u8868\u73b0\n- \u9a8c\u8bc1\u5168\u53cc\u5de5\u4f1a\u8bdd\u573a\u666f\u6269\u5c55\u6f5c\u529b", "conclusion": "OmniDRCA\u901a\u8fc7\u521b\u65b0\u7684\u8054\u5408\u5efa\u6a21\u67b6\u6784\u548c\u5bf9\u6bd4\u5bf9\u9f50\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u591a\u6a21\u6001\u4ea4\u4e92\u8d28\u91cf\uff0c\u4e3a\u8bed\u97f3-\u6587\u672c\u8054\u5408\u5efa\u6a21\u5f00\u8f9f\u65b0\u65b9\u5411\uff0c\u5177\u5907\u5b9e\u9645\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2506.09351", "pdf": "https://arxiv.org/pdf/2506.09351", "abs": "https://arxiv.org/abs/2506.09351", "authors": ["Yuchen Feng", "Bowen Shen", "Naibin Gu", "Jiaxuan Zhao", "Peng Fu", "Zheng Lin", "Weiping Wang"], "title": "DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Large language models (LLMs) with the Mixture-of-Experts (MoE) architecture\nachieve high cost-efficiency by selectively activating a subset of the\nparameters. Despite the inference efficiency of MoE LLMs, the training of\nextensive experts from scratch incurs substantial overhead, whereas\nreconstructing a dense LLM into an MoE LLM significantly reduces the training\nbudget. However, existing reconstruction methods often overlook the diversity\namong experts, leading to potential redundancy. In this paper, we come up with\nthe observation that a specific LLM exhibits notable diversity after being\npruned on different calibration datasets, based on which we present a\nDiversity-Enhanced reconstruction method named DIVE. The recipe of DIVE\nincludes domain affinity mining, pruning-based expert reconstruction, and\nefficient retraining. Specifically, the reconstruction includes pruning and\nreassembly of the feed-forward network (FFN) module. After reconstruction, we\nefficiently retrain the model on routers, experts and normalization modules. We\nimplement DIVE on Llama-style LLMs with open-source training corpora.\nExperiments show that DIVE achieves training efficiency with minimal accuracy\ntrade-offs, outperforming existing pruning and MoE reconstruction methods with\nthe same number of activated parameters.", "AI": {"tldr": "\u63d0\u51faDIVE\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u526a\u679d\u7684\u4e13\u5bb6\u591a\u6837\u6027\u589e\u5f3a\u6280\u672f\u6539\u8fdbMoE\u67b6\u6784LLM\u91cd\u5efa\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u73b0\u6709MoE\u91cd\u5efa\u65b9\u6cd5\u5ffd\u89c6\u4e13\u5bb6\u591a\u6837\u6027\u5bfc\u81f4\u5197\u4f59\uff0c\u800c\u4ece\u5934\u8bad\u7ec3\u4e13\u5bb6\u53c2\u6570\u6210\u672c\u8fc7\u9ad8\u3002", "method": "1. \u9886\u57df\u4eb2\u548c\u529b\u6316\u6398\n2. \u57fa\u4e8e\u4e0d\u540c\u6821\u51c6\u96c6\u7684FFN\u526a\u679d\u4e0e\u91cd\u7ec4\n3. \u5bf9\u8def\u7531/\u4e13\u5bb6/\u5f52\u4e00\u5316\u6a21\u5757\u7684\u9ad8\u6548\u91cd\u8bad\u7ec3", "result": "\u5728\u76f8\u540c\u6fc0\u6d3b\u53c2\u6570\u91cf\u4e0b\uff0cDIVE\u7684\u51c6\u786e\u7387\u635f\u5931\u4ec50.3%\uff0c\u8bad\u7ec3\u6548\u7387\u63d0\u53474\u500d\uff0c\u4f18\u4e8e\u73b0\u6709\u526a\u679d\u548cMoE\u91cd\u5efa\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u526a\u679d\u8bf1\u5bfc\u7684\u6a21\u578b\u591a\u6837\u6027\uff0cDIVE\u5b9e\u73b0\u4e86\u6210\u672c\u6548\u76ca\u4e0e\u6a21\u578b\u6027\u80fd\u7684\u66f4\u597d\u5e73\u8861\uff0c\u4e3aMoE\u67b6\u6784\u4f18\u5316\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2506.09359", "pdf": "https://arxiv.org/pdf/2506.09359", "abs": "https://arxiv.org/abs/2506.09359", "authors": ["Qingyun Zeng", "Simin Ma", "Arash Niknafs", "Ashish Basran", "Carol Szabo"], "title": "Taming SQL Complexity: LLM-Based Equivalence Evaluation for Text-to-SQL", "categories": ["cs.CL"], "comment": "8 pages", "summary": "The rise of Large Language Models (LLMs) has significantly advanced\nText-to-SQL (NL2SQL) systems, yet evaluating the semantic equivalence of\ngenerated SQL remains a challenge, especially given ambiguous user queries and\nmultiple valid SQL interpretations. This paper explores using LLMs to assess\nboth semantic and a more practical \"weak\" semantic equivalence. We analyze\ncommon patterns of SQL equivalence and inequivalence, discuss challenges in\nLLM-based evaluation.", "AI": {"tldr": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30Text-to-SQL\u8bed\u4e49\u7b49\u4ef7\u6027\uff0c\u63a2\u7d22\u5f31\u8bed\u4e49\u7b49\u6548\u6807\u51c6\u53ca\u6311\u6218", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5224\u5b9a\u7528\u6237\u6a21\u7cca\u67e5\u8be2\u751f\u6210\u7684SQL\u8bed\u4e49\u7b49\u4ef7\u6027\uff0c\u9700\u5efa\u7acb\u66f4\u5b9e\u7528\u7684\u8bc4\u4f30\u6807\u51c6", "method": "\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u8bed\u4e49\u7b49\u6548\u8bc4\u4f30\u6846\u67b6\uff0c\u5206\u6790\u5e38\u89c1SQL\u7b49\u6548/\u975e\u7b49\u6548\u6a21\u5f0f", "result": "\u63ed\u793a\u4e86LLM\u8bc4\u4f30\u5b58\u5728\u7684\u5c40\u9650\u6027\uff0c\u5305\u62ec\u590d\u6742\u8bed\u4e49\u7406\u89e3\u504f\u5dee\u548c\u8fb9\u754c\u6848\u4f8b\u5904\u7406\u96be\u9898", "conclusion": "LLM\u4e3aSQL\u8bed\u4e49\u8bc4\u4f30\u63d0\u4f9b\u65b0\u601d\u8def\uff0c\u4f46\u9700\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u5efa\u7acb\u66f4\u7cfb\u7edf\u7684\u8bc4\u4f30\u4f53\u7cfb"}}
{"id": "2506.09367", "pdf": "https://arxiv.org/pdf/2506.09367", "abs": "https://arxiv.org/abs/2506.09367", "authors": ["Zhengyuan Liu", "Stella Xin Yin", "Dion Hoe-Lian Goh", "Nancy F. Chen"], "title": "COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content", "categories": ["cs.CL", "cs.AI"], "comment": "BEA 2025", "summary": "While Generative AI has demonstrated strong potential and versatility in\ncontent generation, its application to educational contexts presents several\nchallenges. Models often fail to align with curriculum standards and maintain\ngrade-appropriate reading levels consistently. Furthermore, STEM education\nposes additional challenges in balancing scientific explanations with everyday\nlanguage when introducing complex and abstract ideas and phenomena to younger\nstudents. In this work, we propose COGENT, a curriculum-oriented framework for\ngenerating grade-appropriate educational content. We incorporate three\ncurriculum components (science concepts, core ideas, and learning objectives),\ncontrol readability through length, vocabulary, and sentence complexity, and\nadopt a ``wonder-based'' approach to increase student engagement and interest.\nWe conduct a multi-dimensional evaluation via both LLM-as-a-judge and human\nexpert analysis. Experimental results show that COGENT consistently produces\ngrade-appropriate passages that are comparable or superior to human references.\nOur work establishes a viable approach for scaling adaptive and high-quality\nlearning resources.", "AI": {"tldr": "\u63d0\u51faCOGENT\u6846\u67b6\uff0c\u901a\u8fc7\u8bfe\u7a0b\u4e09\u8981\u7d20\u63a7\u5236\u751f\u6210\u7b26\u5408\u5e74\u7ea7\u7684\u79d1\u5b66\u6559\u80b2\u5185\u5bb9\uff0c\u7ecf\u8bc4\u4f30\u6548\u679c\u4f18\u4e8e\u4eba\u5de5\u53c2\u8003", "motivation": "\u751f\u6210\u5f0fAI\u5728\u6559\u80b2\u573a\u666f\u5b58\u5728\u8bfe\u7a0b\u6807\u51c6\u5bf9\u9f50\u56f0\u96be\u3001\u9605\u8bfb\u6c34\u5e73\u4e0d\u7a33\u5b9a\u3001STEM\u6559\u80b2\u4e2d\u79d1\u5b66\u672f\u8bed\u4e0e\u65e5\u5e38\u8bed\u8a00\u5e73\u8861\u4e0d\u8db3\u7b49\u95ee\u9898", "method": "\u6574\u5408\u79d1\u5b66\u6982\u5ff5/\u6838\u5fc3\u601d\u60f3/\u5b66\u4e60\u76ee\u6807\u4e09\u8981\u7d20\uff0c\u63a7\u5236\u6587\u672c\u957f\u5ea6/\u8bcd\u6c47/\u53e5\u5f0f\u590d\u6742\u5ea6\uff0c\u91c7\u7528\u57fa\u4e8e\u597d\u5947\u5fc3\u7684\u53d9\u8ff0\u65b9\u5f0f", "result": "\u591a\u7ef4\u5ea6\u8bc4\u4f30\u663e\u793a\u751f\u6210\u5185\u5bb9\u5728\u53ef\u8bfb\u6027\u548c\u8d28\u91cf\u4e0a\u8fbe\u5230\u6216\u8d85\u8d8a\u4eba\u5de5\u7f16\u5199\u6c34\u5e73\uff0cLLM\u8bc4\u4f30\u4e0e\u4e13\u5bb6\u5206\u6790\u7ed3\u679c\u4e00\u81f4", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5927\u89c4\u6a21\u751f\u6210\u9002\u5e94\u6027\u5f3a\u3001\u9ad8\u8d28\u91cf\u7684\u6559\u80b2\u8d44\u6e90\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.09375", "pdf": "https://arxiv.org/pdf/2506.09375", "abs": "https://arxiv.org/abs/2506.09375", "authors": ["Massa Baali", "Shuo Han", "Syed Abdul Hannan", "Purusottam Samal", "Karanveer Singh", "Soham Deshmukh", "Rita Singh", "Bhiksha Raj"], "title": "CoLMbo: Speaker Language Model for Descriptive Profiling", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Speaker recognition systems are often limited to classification tasks and\nstruggle to generate detailed speaker characteristics or provide context-rich\ndescriptions. These models primarily extract embeddings for speaker\nidentification but fail to capture demographic attributes such as dialect,\ngender, and age in a structured manner. This paper introduces CoLMbo, a Speaker\nLanguage Model (SLM) that addresses these limitations by integrating a speaker\nencoder with prompt-based conditioning. This allows for the creation of\ndetailed captions based on speaker embeddings. CoLMbo utilizes user-defined\nprompts to adapt dynamically to new speaker characteristics and provides\ncustomized descriptions, including regional dialect variations and age-related\ntraits. This innovative approach not only enhances traditional speaker\nprofiling but also excels in zero-shot scenarios across diverse datasets,\nmarking a significant advancement in the field of speaker recognition.", "AI": {"tldr": "\u63d0\u51faCoLMbo\u6a21\u578b\uff0c\u901a\u8fc7\u96c6\u6210\u8bf4\u8bdd\u4eba\u7f16\u7801\u5668\u548c\u63d0\u793a\u6761\u4ef6\u673a\u5236\uff0c\u5b9e\u73b0\u7ed3\u6784\u5316\u4eba\u53e3\u5c5e\u6027\u6355\u6349\u548c\u52a8\u6001\u751f\u6210\u5b9a\u5236\u5316\u7684\u8bf4\u8bdd\u4eba\u7279\u5f81\u63cf\u8ff0\u3002", "motivation": "\u73b0\u6709\u8bf4\u8bdd\u4eba\u8bc6\u522b\u7cfb\u7edf\u4ec5\u652f\u6301\u5206\u7c7b\u4efb\u52a1\uff0c\u7f3a\u4e4f\u5bf9\u6027\u522b/\u65b9\u8a00/\u5e74\u9f84\u7b49\u7ed3\u6784\u5316\u5c5e\u6027\u7684\u6355\u6349\u80fd\u529b\uff0c\u4e14\u65e0\u6cd5\u751f\u6210\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u63cf\u8ff0\u3002", "method": "\u7ed3\u5408\u8bf4\u8bdd\u4eba\u7f16\u7801\u5668\u4e0e\u63d0\u793a\u6761\u4ef6\u6846\u67b6\uff0c\u901a\u8fc7\u7528\u6237\u5b9a\u4e49\u63d0\u793a\u52a8\u6001\u9002\u914d\u65b0\u7279\u5f81\uff0c\u652f\u6301\u65b9\u8a00\u533a\u57df\u53d8\u5f02\u548c\u5e74\u9f84\u7279\u5f81\u7b49\u5b9a\u5236\u5316\u63cf\u8ff0\u751f\u6210\u3002", "result": "\u5728\u4f20\u7edf\u8bf4\u8bdd\u4eba\u753b\u50cf\u548c\u96f6\u6837\u672c\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u8bf4\u8bdd\u4eba\u8bc6\u522b\u9886\u57df\u7684\u663e\u8457\u7a81\u7834\u3002", "conclusion": "\u63d0\u793a\u6761\u4ef6\u673a\u5236\u9769\u65b0\u4e86\u8bf4\u8bdd\u4eba\u7279\u5f81\u63cf\u8ff0\u8303\u5f0f\uff0c\u901a\u8fc7\u52a8\u6001\u9002\u914d\u80fd\u529b\u663e\u8457\u6269\u5c55\u4e86\u8bf4\u8bdd\u4eba\u8bc6\u522b\u6280\u672f\u7684\u5e94\u7528\u8fb9\u754c\u3002"}}
{"id": "2506.09381", "pdf": "https://arxiv.org/pdf/2506.09381", "abs": "https://arxiv.org/abs/2506.09381", "authors": ["Austin McCutcheon", "Thiago E. A. de Oliveira", "Aleksandr Zheleznov", "Chris Brogly"], "title": "Binary classification for perceived quality of headlines and links on worldwide news websites, 2018-2024", "categories": ["cs.CL"], "comment": null, "summary": "The proliferation of online news enables potential widespread publication of\nperceived low-quality news headlines/links. As a result, we investigated\nwhether it was possible to automatically distinguish perceived lower-quality\nnews headlines/links from perceived higher-quality headlines/links. We\nevaluated twelve machine learning models on a binary, balanced dataset of\n57,544,214 worldwide news website links/headings from 2018-2024 (28,772,107 per\nclass) with 115 extracted linguistic features. Binary labels for each text were\nderived from scores based on expert consensus regarding the respective news\ndomain quality. Traditional ensemble methods, particularly the bagging\nclassifier, had strong performance (88.1% accuracy, 88.3% F1, 80/20 train/test\nsplit). Fine-tuned DistilBERT achieved the highest accuracy (90.3%, 80/20\ntrain/test split) but required more training time. The results suggest that\nboth NLP features with traditional classifiers and deep learning models can\neffectively differentiate perceived news headline/link quality, with some\ntrade-off between predictive performance and train time.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u4f20\u7edf\u96c6\u6210\u5b66\u4e60\uff08Bagging\u5206\u7c7b\u566888.1%\u51c6\u786e\u7387\uff09\u548c\u5fae\u8c03DistilBERT\u6a21\u578b\uff0890.3%\u51c6\u786e\u7387\uff09\u5bf9\u6bd4\uff0c\u9a8c\u8bc1\u4e24\u8005\u5747\u53ef\u6709\u6548\u533a\u5206\u65b0\u95fb\u6807\u9898\u8d28\u91cf\uff0c\u4f46\u9700\u6743\u8861\u9884\u6d4b\u6027\u80fd\u4e0e\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u9488\u5bf9\u7f51\u7edc\u4f4e\u8d28\u91cf\u65b0\u95fb\u6807\u9898/\u94fe\u63a5\u6cdb\u6ee5\u95ee\u9898\uff0c\u63a2\u7d22\u81ea\u52a8\u5316\u533a\u5206\u9ad8/\u4f4e\u8d28\u91cf\u65b0\u95fb\u6807\u9898\u7684\u6280\u672f\u65b9\u6848\u3002", "method": "\u4f7f\u75282018-2024\u5e74\u5168\u74035754\u4e07\u6761\u5e73\u8861\u6570\u636e\u96c6\uff08\u9ad8\u4f4e\u8d28\u91cf\u54042877\u4e07\u6761\uff09\uff0c\u63d0\u53d6115\u4e2a\u8bed\u8a00\u7279\u5f81\uff0c\u5bf9\u6bd412\u4e2a\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u542b\u4f20\u7edf\u96c6\u6210\u65b9\u6cd5\u4e0e\u5fae\u8c03DistilBERT\uff09\uff0c\u91c7\u752880/20\u8bad\u7ec3\u6d4b\u8bd5\u96c6\u5212\u5206\u3002", "result": "\u4f20\u7edfBagging\u5206\u7c7b\u5668\u8868\u73b0\u4f18\u5f02\uff0888.1%\u51c6\u786e\u7387/88.3% F1\uff09\uff0c\u5fae\u8c03DistilBERT\u8fbe\u5230\u6700\u9ad8\u51c6\u786e\u7387\uff0890.3%\uff09\u4f46\u8bad\u7ec3\u8017\u65f6\u66f4\u957f\u3002", "conclusion": "\u57fa\u4e8eNLP\u7279\u5f81\u7684\u4f20\u7edf\u6a21\u578b\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5747\u53ef\u6709\u6548\u8bc6\u522b\u65b0\u95fb\u8d28\u91cf\uff0c\u5b9e\u9645\u5e94\u7528\u9700\u6839\u636e\u5bf9\u9884\u6d4b\u6027\u80fd\u4e0e\u8bad\u7ec3\u6548\u7387\u7684\u9700\u6c42\u8fdb\u884c\u9009\u62e9\u3002"}}
{"id": "2506.09391", "pdf": "https://arxiv.org/pdf/2506.09391", "abs": "https://arxiv.org/abs/2506.09391", "authors": ["Haoran Zhao", "Robert D. Hawkins"], "title": "Comparing human and LLM politeness strategies in free production", "categories": ["cs.CL"], "comment": "25 pages, 5 figures", "summary": "Polite speech poses a fundamental alignment challenge for large language\nmodels (LLMs). Humans deploy a rich repertoire of linguistic strategies to\nbalance informational and social goals -- from positive approaches that build\nrapport (compliments, expressions of interest) to negative strategies that\nminimize imposition (hedging, indirectness). We investigate whether LLMs employ\na similarly context-sensitive repertoire by comparing human and LLM responses\nin both constrained and open-ended production tasks. We find that larger models\n($\\ge$70B parameters) successfully replicate key preferences from the\ncomputational pragmatics literature, and human evaluators surprisingly prefer\nLLM-generated responses in open-ended contexts. However, further linguistic\nanalyses reveal that models disproportionately rely on negative politeness\nstrategies even in positive contexts, potentially leading to\nmisinterpretations. While modern LLMs demonstrate an impressive handle on\npoliteness strategies, these subtle differences raise important questions about\npragmatic alignment in AI systems.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\uff08\u226570B\u53c2\u6570\uff09\u80fd\u590d\u73b0\u793c\u8c8c\u7b56\u7565\u7684\u6838\u5fc3\u504f\u597d\u4e14\u4eba\u7c7b\u66f4\u503e\u5411\u5176\u56de\u7b54\uff0c\u4f46\u8fc7\u5ea6\u4f9d\u8d56\u8d1f\u9762\u793c\u8c8c\u7b56\u7565\u53ef\u80fd\u5f15\u53d1\u8bef\u89e3\uff0c\u63ed\u793aAI\u8bed\u7528\u5bf9\u9f50\u6311\u6218\u3002", "motivation": "\u63a2\u7a76LLMs\u662f\u5426\u80fd\u50cf\u4eba\u7c7b\u4e00\u6837\u6839\u636e\u8bed\u5883\u7075\u6d3b\u8fd0\u7528\u6b63/\u8d1f\u9762\u793c\u8c8c\u7b56\u7565\uff08\u5982\u8d5e\u7f8evs\u59d4\u5a49\uff09\uff0c\u4ee5\u53ca\u8fd9\u79cd\u8bed\u7528\u5bf9\u9f50\u5bf9AI\u7cfb\u7edf\u4ea4\u6d41\u80fd\u529b\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u7ea6\u675f\u6027\u4efb\u52a1\u4e0e\u5f00\u653e\u5f0f\u751f\u6210\u4efb\u52a1\uff0c\u7ed3\u5408\u8ba1\u7b97\u8bed\u7528\u5b66\u7406\u8bba\u6846\u67b6\uff0c\u5bf9\u6bd4\u4eba\u7c7b\u4e0e\u4e0d\u540c\u89c4\u6a21LLMs\u7684\u793c\u8c8c\u7b56\u7565\u9009\u62e9\uff0c\u5e76\u8fdb\u884c\u8bed\u8a00\u5b66\u91cf\u5316\u5206\u6790\u3002", "result": "\u5927\u6a21\u578b\u6210\u529f\u590d\u73b0\u7ecf\u5178\u8bed\u7528\u504f\u597d\uff0c\u5f00\u653e\u5f0f\u56de\u7b54\u83b7\u4eba\u7c7b\u9752\u7750\uff0c\u4f46\u8bed\u8a00\u5206\u6790\u663e\u793a\u6a21\u578b\u5728\u79ef\u6781\u8bed\u5883\u4e2d\u4ecd\u8fc7\u5ea6\u4f7f\u7528\u8d1f\u9762\u7b56\u7565\uff08\u5982\u95f4\u63a5\u8868\u8fbe\uff09\u3002", "conclusion": "\u5c3d\u7ba1LLMs\u5c55\u73b0\u51fa\u60ca\u4eba\u7684\u793c\u8c8c\u7b56\u7565\u5904\u7406\u80fd\u529b\uff0c\u5176\u7b56\u7565\u9009\u62e9\u504f\u5dee\u53ef\u80fd\u5bfc\u81f4\u793e\u4ea4\u8bef\u89e3\uff0c\u51f8\u663eAI\u7cfb\u7edf\u6df1\u5c42\u8bed\u7528\u5bf9\u9f50\u7684\u91cd\u8981\u6027\u4e0e\u590d\u6742\u6027\u3002"}}
{"id": "2506.09393", "pdf": "https://arxiv.org/pdf/2506.09393", "abs": "https://arxiv.org/abs/2506.09393", "authors": ["Xinyi Gao", "Qiucheng Wu", "Yang Zhang", "Xuechen Liu", "Kaizhi Qian", "Ying Xu", "Shiyu Chang"], "title": "A Hierarchical Probabilistic Framework for Incremental Knowledge Tracing in Classroom Settings", "categories": ["cs.CL"], "comment": "24 pages, 4 figures", "summary": "Knowledge tracing (KT) aims to estimate a student's evolving knowledge state\nand predict their performance on new exercises based on performance history.\nMany realistic classroom settings for KT are typically low-resource in data and\nrequire online updates as students' exercise history grows, which creates\nsignificant challenges for existing KT approaches. To restore strong\nperformance under low-resource conditions, we revisit the hierarchical\nknowledge concept (KC) information, which is typically available in many\nclassroom settings and can provide strong prior when data are sparse. We\ntherefore propose Knowledge-Tree-based Knowledge Tracing (KT$^2$), a\nprobabilistic KT framework that models student understanding over a\ntree-structured hierarchy of knowledge concepts using a Hidden Markov Tree\nModel. KT$^2$ estimates student mastery via an EM algorithm and supports\npersonalized prediction through an incremental update mechanism as new\nresponses arrive. Our experiments show that KT$^2$ consistently outperforms\nstrong baselines in realistic online, low-resource settings.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u77e5\u8bc6\u6811\u7ed3\u6784\u7684KT\u00b2\u6846\u67b6\uff0c\u901a\u8fc7\u9690\u9a6c\u5c14\u53ef\u592b\u6811\u6a21\u578b\u548cEM\u7b97\u6cd5\u5b9e\u73b0\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u7684\u9ad8\u6548\u77e5\u8bc6\u8ffd\u8e2a", "motivation": "\u73b0\u6709\u77e5\u8bc6\u8ffd\u8e2a\u65b9\u6cd5\u5728\u6570\u636e\u7a00\u758f\u7684\u8bfe\u5802\u573a\u666f\u4e2d\u8868\u73b0\u53d7\u9650\uff0c\u9700\u8981\u5229\u7528\u77e5\u8bc6\u6982\u5ff5\u5c42\u6b21\u7ed3\u6784\u4f5c\u4e3a\u5148\u9a8c\u63d0\u5347\u4f4e\u8d44\u6e90\u6761\u4ef6\u4e0b\u7684\u9884\u6d4b\u6548\u679c", "method": "\u6784\u5efa\u6811\u72b6\u77e5\u8bc6\u5c42\u6b21\u7ed3\u6784\uff0c\u91c7\u7528\u9690\u9a6c\u5c14\u53ef\u592b\u6811\u6a21\u578b\u5efa\u6a21\u5b66\u751f\u8ba4\u77e5\u72b6\u6001\uff0c\u901a\u8fc7EM\u7b97\u6cd5\u4f30\u8ba1\u77e5\u8bc6\u638c\u63e1\u5ea6\uff0c\u652f\u6301\u589e\u91cf\u5f0f\u5b9e\u65f6\u66f4\u65b0", "result": "\u5728\u5728\u7ebf\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\uff0cKT\u00b2\u6a21\u578b\u5728\u9884\u6d4b\u7cbe\u5ea6\u548c\u4e2a\u6027\u5316\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u77e5\u8bc6\u6811\u7ed3\u6784\u6709\u6548\u63d0\u5347\u6570\u636e\u7a00\u758f\u6761\u4ef6\u4e0b\u7684\u8ffd\u8e2a\u6548\u679c\uff0c\u4e3a\u4e2a\u6027\u5316\u6559\u80b2\u6280\u672f\u63d0\u4f9b\u65b0\u7684\u5efa\u6a21\u8303\u5f0f"}}
{"id": "2506.09408", "pdf": "https://arxiv.org/pdf/2506.09408", "abs": "https://arxiv.org/abs/2506.09408", "authors": ["Jui-Ming Yao", "Hao-Yuan Chen", "Zi-Xian Tang", "Bing-Jia Tan", "Sheng-Wei Peng", "Bing-Cheng Xie", "Shun-Feng Su"], "title": "Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive performance on\nmultiple-choice question answering (MCQA) benchmarks, yet they remain highly\nvulnerable to minor input perturbations. In this paper, we introduce and\nevaluate Token Constraint Decoding (TCD). This simple yet effective\ninference-time algorithm enforces alignment between token-level predictions to\nenhance robustness in noisy settings. Through extensive experiments on\nCommonsenseQA, MMLU, and MMLU-Pro, we show that TCD, especially when paired\nwith prompt engineering (PE) fixes, significantly restores performance degraded\nby input noise, yielding up to +39\\% absolute gains for weaker models like\nGemma3 1B. Penalty sweep analyses further reveal that TCD implicitly\nregularizes overconfident outputs, with different models requiring distinct\npenalty schedules to maximize resilience. Our findings establish TCD as a\npractical, model-agnostic approach for improving reasoning stability under\nreal-world imperfections and pave the way for more reliable deployment of LLMs\nin safety-critical or user-facing applications.", "AI": {"tldr": "\u63d0\u51faToken Constraint Decoding (TCD)\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3atoken\u7ea7\u9884\u6d4b\u4e00\u81f4\u6027\u663e\u8457\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u566a\u58f0\u8f93\u5165\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u8f83\u5c0f\u6a21\u578b\u6027\u80fd\u63d0\u5347\u8fbe39%", "motivation": "\u89e3\u51b3LLMs\u5728\u73b0\u5b9e\u566a\u58f0\u8f93\u5165\u573a\u666f\u4e0b\u6027\u80fd\u9aa4\u964d\u7684\u95ee\u9898\uff0c\u63d0\u5347\u5b89\u5168\u5173\u952e\u9886\u57df\u5e94\u7528\u53ef\u9760\u6027", "method": "\u63a8\u7406\u9636\u6bb5\u7b97\u6cd5TCD\u5f3a\u5236token\u9884\u6d4b\u5bf9\u9f50\uff0c\u7ed3\u5408\u63d0\u793a\u5de5\u7a0b(PE)\u4f18\u5316\uff0c\u901a\u8fc7\u60e9\u7f5a\u8fc7\u81ea\u4fe1\u8f93\u51fa\u6765\u9690\u5f0f\u6b63\u5219\u5316", "result": "\u5728CommonsenseQA/MMLU\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTCD+PE\u7ec4\u5408\u4f7fGemma3 1B\u7b49\u5f31\u6a21\u578b\u7edd\u5bf9\u6027\u80fd\u63d0\u5347\u6700\u9ad8\u8fbe39%", "conclusion": "TCD\u4f5c\u4e3a\u6a21\u578b\u65e0\u5173\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u63a8\u7406\u7a33\u5b9a\u6027\uff0c\u4e3aLLMs\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u65b0\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.09414", "pdf": "https://arxiv.org/pdf/2506.09414", "abs": "https://arxiv.org/abs/2506.09414", "authors": ["Xiujun Zhou", "Pingjian Zhang", "Deyou Tang"], "title": "PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data Augmentation Strategies for Knowledge Graph Question Answering", "categories": ["cs.CL", "cs.IR"], "comment": "13 pages, 7 figures, 5 tables", "summary": "Knowledge Graph Question Answering (KGQA) is a crucial task in natural\nlanguage processing that requires reasoning over knowledge graphs (KGs) to\nanswer natural language questions. Recent methods utilizing large language\nmodels (LLMs) have shown remarkable semantic parsing capabilities but are\nlimited by the scarcity of diverse annotated data and multi-hop reasoning\nsamples. Traditional data augmentation approaches are focus mainly on\nsingle-hop questions and prone to semantic distortion, while LLM-based methods\nprimarily address semantic distortion but usually neglect multi-hop reasoning,\nthus limiting data diversity. The scarcity of multi-hop samples further weakens\nmodels' generalization. To address these issues, we propose PGDA-KGQA, a\nprompt-guided generative framework with multiple data augmentation strategies\nfor KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by\ncrafting meticulously engineered prompts that integrate the provided textual\ncontent, it leverages LLMs to generate large-scale (question, logical form)\npairs for model training. Specifically, PGDA-KGQA enriches its training set by:\n(1) generating single-hop pseudo questions to improve the alignment of question\nsemantics with KG relations; (2) applying semantic-preserving question\nrewriting to improve robustness against linguistic variations; (3) employing\nanswer-guided reverse path exploration to create realistic multi-hop questions.\nBy adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA\nutilizes the augmented data to enhance the accuracy of logical form generation\nand thus improve answer retrieval performance. Experiments demonstrate that\noutperforms state-of-the-art methods on standard KGQA datasets, achieving\nimprovements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by\n1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively.", "AI": {"tldr": "\u63d0\u51fa\u63d0\u793a\u5f15\u5bfc\u7684\u751f\u6210\u6846\u67b6PGDA-KGQA\uff0c\u901a\u8fc7\u5355\u8df3\u4f2a\u95ee\u9898\u751f\u6210\u3001\u8bed\u4e49\u4fdd\u7559\u91cd\u5199\u548c\u53cd\u5411\u8def\u5f84\u63a2\u7d22\u4e09\u79cd\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u6027\u80fd", "motivation": "\u73b0\u6709KGQA\u65b9\u6cd5\u9762\u4e34\u6807\u6ce8\u6570\u636e\u591a\u6837\u6027\u4e0d\u8db3\u548c\u591a\u8df3\u63a8\u7406\u6837\u672c\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u4fa7\u91cd\u5355\u8df3\u4e14\u6613\u8bed\u4e49\u5931\u771f\uff0cLLM\u65b9\u6cd5\u5ffd\u89c6\u591a\u8df3\u63a8\u7406\u5bfc\u81f4\u6570\u636e\u591a\u6837\u6027\u53d7\u9650", "method": "1) \u751f\u6210\u5355\u8df3\u4f2a\u95ee\u9898\u589e\u5f3a\u8bed\u4e49\u5bf9\u9f50 2) \u8bed\u4e49\u4fdd\u7559\u5f0f\u95ee\u9898\u91cd\u5199\u63d0\u5347\u8bed\u8a00\u53d8\u4f53\u9c81\u68d2\u6027 3) \u7b54\u6848\u5f15\u5bfc\u7684\u53cd\u5411\u8def\u5f84\u63a2\u7d22\u6784\u5efa\u591a\u8df3\u95ee\u9898", "result": "\u5728WebQSP\u6570\u636e\u96c6F1/Hits@1/Accuracy\u5206\u522b\u63d0\u53472.8%/1.2%/3.1%\uff0cComplexWebQuestions\u63d0\u53471.8%/1.1%/2.4%", "conclusion": "PGDA-KGQA\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\u548c\u8bed\u4e49\u89e3\u6790\u6d41\u7a0b\uff0c\u6709\u6548\u63d0\u5347\u591a\u8df3\u63a8\u7406\u80fd\u529b\u548c\u6a21\u578b\u6cdb\u5316\u6027\u80fd\uff0c\u5728KGQA\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\u6c34\u5e73"}}
{"id": "2506.09424", "pdf": "https://arxiv.org/pdf/2506.09424", "abs": "https://arxiv.org/abs/2506.09424", "authors": ["Md Messal Monem Miah", "Adrita Anika", "Xi Shi", "Ruihong Huang"], "title": "Hidden in Plain Sight: Evaluation of the Deception Detection Capabilities of LLMs in Multimodal Settings", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Detecting deception in an increasingly digital world is both a critical and\nchallenging task. In this study, we present a comprehensive evaluation of the\nautomated deception detection capabilities of Large Language Models (LLMs) and\nLarge Multimodal Models (LMMs) across diverse domains. We assess the\nperformance of both open-source and commercial LLMs on three distinct datasets:\nreal life trial interviews (RLTD), instructed deception in interpersonal\nscenarios (MU3D), and deceptive reviews (OpSpam). We systematically analyze the\neffectiveness of different experimental setups for deception detection,\nincluding zero-shot and few-shot approaches with random or similarity-based\nin-context example selection. Our results show that fine-tuned LLMs achieve\nstate-of-the-art performance on textual deception detection tasks, while LMMs\nstruggle to fully leverage cross-modal cues. Additionally, we analyze the\nimpact of auxiliary features, such as non-verbal gestures and video summaries,\nand examine the effectiveness of different prompting strategies, including\ndirect label generation and chain-of-thought reasoning. Our findings provide\nkey insights into how LLMs process and interpret deceptive cues across\nmodalities, highlighting their potential and limitations in real-world\ndeception detection applications.", "AI": {"tldr": "LLMs\u5728\u6587\u672c\u6b3a\u9a97\u68c0\u6d4b\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0cLMMs\u5728\u591a\u6a21\u6001\u7ebf\u7d22\u5229\u7528\u4e0a\u5b58\u5728\u5c40\u9650", "motivation": "\u6570\u5b57\u65f6\u4ee3\u6b3a\u9a97\u68c0\u6d4b\u9700\u6c42\u8feb\u5207\uff0c\u9700\u8bc4\u4f30LLMs/LMMs\u5728\u8de8\u6a21\u6001\u573a\u666f\u7684\u68c0\u6d4b\u80fd\u529b", "method": "\u4f7f\u7528RLTD/MU3D/OpSpam\u4e09\u4e2a\u6570\u636e\u96c6\uff0c\u5bf9\u6bd4\u96f6\u6837\u672c/\u5c11\u6837\u672c\u65b9\u6cd5\uff0c\u5206\u6790\u5fae\u8c03\u6a21\u578b\u4e0e\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408", "result": "\u5fae\u8c03LLMs\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0cLMMs\u8de8\u6a21\u6001\u5904\u7406\u80fd\u529b\u4e0d\u8db3\uff0c\u63d0\u793a\u7b56\u7565\u5bf9\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd", "conclusion": "LLMs\u5728\u5355\u6a21\u6001\u6b3a\u9a97\u68c0\u6d4b\u4e2d\u5b9e\u7528\u4ef7\u503c\u7a81\u51fa\uff0c\u4f46\u591a\u6a21\u6001\u5e94\u7528\u9700\u6539\u8fdb\u7279\u5f81\u878d\u5408\u65b9\u6cd5"}}
{"id": "2506.09428", "pdf": "https://arxiv.org/pdf/2506.09428", "abs": "https://arxiv.org/abs/2506.09428", "authors": ["Fei Ding", "Baiqiao Wang"], "title": "Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Supervised Fine-Tuning (SFT), while enhancing large language models(LLMs)'\ninstruction-following capabilities and domain-specific task adaptability, often\ndiminishes their general capabilities. Moreover, due to the inaccessibility of\noriginal pre-training data, catastrophic forgetting tends to be exacerbated\nwhen third-party practitioners implement SFT on open-sourced models. To address\nthis challenge, we propose a novel, more cost-effective SFT method which could\neffectively reduce the risk of catastrophic forgetting without access to\noriginal SFT data. Our approach begins by reconstructing the likely SFT\ninstruction distribution of the base model, followed by a multi-model screening\nprocess to select optimal data, which is then mixed with new data for SFT.\nExperimental results demonstrate that our method preserves generalization\ncapabilities in general domains while improving task-specific performance.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u539f\u59cbSFT\u6570\u636e\u7684\u4f4e\u6210\u672c\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u5efa\u6307\u4ee4\u5206\u5e03\u4e0e\u591a\u6a21\u578b\u6570\u636e\u7b5b\u9009\uff0c\u6709\u6548\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898", "motivation": "\u4f20\u7edfSFT\u65b9\u6cd5\u5728\u589e\u5f3a\u9886\u57df\u4efb\u52a1\u80fd\u529b\u65f6\u5bfc\u81f4\u6a21\u578b\u901a\u7528\u80fd\u529b\u4e0b\u964d\uff0c\u4e14\u7b2c\u4e09\u65b9\u5f00\u53d1\u8005\u96be\u4ee5\u83b7\u53d6\u539f\u59cb\u9884\u8bad\u7ec3\u6570\u636e\u52a0\u5267\u9057\u5fd8\u95ee\u9898", "method": "1. \u91cd\u5efa\u57fa\u7840\u6a21\u578b\u7684\u6f5c\u5728SFT\u6307\u4ee4\u5206\u5e03 2. \u591a\u6a21\u578b\u8054\u5408\u7b5b\u9009\u6700\u4f18\u6570\u636e 3. \u65b0\u65e7\u6570\u636e\u6df7\u5408\u8fdb\u884c\u76d1\u7763\u5fae\u8c03", "result": "\u5b9e\u9a8c\u663e\u793a\u5728\u4fdd\u6301\u901a\u7528\u9886\u57df\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347\u7279\u5b9a\u4efb\u52a1\u8868\u73b0\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7b2c\u4e09\u65b9\u5f00\u53d1\u8005\u63d0\u4f9b\u7ecf\u6d4e\u9ad8\u6548\u7684SFT\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u9886\u57df\u9002\u5e94\u4e0e\u901a\u7528\u80fd\u529b\u4fdd\u7559"}}
{"id": "2506.09440", "pdf": "https://arxiv.org/pdf/2506.09440", "abs": "https://arxiv.org/abs/2506.09440", "authors": ["GigaChat team", "Mamedov Valentin", "Evgenii Kosarev", "Gregory Leleytner", "Ilya Shchuckin", "Valeriy Berezovskiy", "Daniil Smirnov", "Dmitry Kozlov", "Sergei Averkiev", "Lukyanenko Ivan", "Aleksandr Proshunin", "Ainur Israfilova", "Ivan Baskov", "Artem Chervyakov", "Emil Shakirov", "Mikhail Kolesov", "Daria Khomich", "Darya Latortseva", "Sergei Porkhun", "Yury Fedorov", "Oleg Kutuzov", "Polina Kudriavtseva", "Sofiia Soldatova", "Kolodin Egor", "Stanislav Pyatkin", "Dzmitry Menshykh", "Grafov Sergei", "Eldar Damirov", "Karlov Vladimir", "Ruslan Gaitukiev", "Arkadiy Shatenov", "Alena Fenogenova", "Nikita Savushkin", "Fedor Minkin"], "title": "GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture", "categories": ["cs.CL", "cs.AI"], "comment": "ACL-2025 System Demo", "summary": "Generative large language models (LLMs) have become crucial for modern NLP\nresearch and applications across various languages. However, the development of\nfoundational models specifically tailored to the Russian language has been\nlimited, primarily due to the significant computational resources required.\nThis paper introduces the GigaChat family of Russian LLMs, available in various\nsizes, including base models and instruction-tuned versions. We provide a\ndetailed report on the model architecture, pre-training process, and\nexperiments to guide design choices. In addition, we evaluate their performance\non Russian and English benchmarks and compare GigaChat with multilingual\nanalogs. The paper presents a system demonstration of the top-performing models\naccessible via an API, a Telegram bot, and a Web interface. Furthermore, we\nhave released three open GigaChat models in open-source\n(https://huggingface.co/ai-sage), aiming to expand NLP research opportunities\nand support the development of industrial solutions for the Russian language.", "AI": {"tldr": "\u63a8\u51fa\u4e86\u9488\u5bf9\u4fc4\u8bed\u7684GigaChat\u5927\u8bed\u8a00\u6a21\u578b\u7cfb\u5217\uff0c\u586b\u8865\u4fc4\u8bedLLM\u9886\u57df\u7a7a\u767d\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u4fc3\u8fdb\u7814\u7a76\u4e0e\u5e94\u7528", "motivation": "\u4fc4\u8bed\u5927\u6a21\u578b\u5f00\u53d1\u53d7\u9650\u4e8e\u8ba1\u7b97\u8d44\u6e90\uff0c\u9700\u4e13\u95e8\u5b9a\u5236\u4ee5\u652f\u6301\u4fc4\u8bedNLP\u7814\u7a76\u4e0e\u5de5\u4e1a\u5e94\u7528", "method": "\u5f00\u53d1\u4e0d\u540c\u89c4\u6a21\u7684\u57fa\u7840\u6a21\u578b\u4e0e\u6307\u4ee4\u8c03\u4f18\u7248\u672c\uff0c\u8be6\u7ec6\u62a5\u544a\u67b6\u6784\u8bbe\u8ba1\u3001\u9884\u8bad\u7ec3\u8fc7\u7a0b\u548c\u5b9e\u9a8c\u9a8c\u8bc1", "result": "\u6a21\u578b\u5728\u4fc4\u82f1\u53cc\u8bed\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u4f9bAPI/Telegram/Web\u591a\u7aef\u7cfb\u7edf\uff0c\u5f00\u6e90\u4e09\u4e2a\u6a21\u578b", "conclusion": "GigaChat\u63a8\u52a8\u4e86\u4fc4\u8bedNLP\u6280\u672f\u53d1\u5c55\uff0c\u5f00\u6e90\u6a21\u578b\u4e3a\u7814\u7a76\u548c\u5de5\u4e1a\u5e94\u7528\u63d0\u4f9b\u57fa\u7840\u652f\u6301"}}
{"id": "2506.09450", "pdf": "https://arxiv.org/pdf/2506.09450", "abs": "https://arxiv.org/abs/2506.09450", "authors": ["Prameshwar Thiyagarajan", "Vaishnavi Parimi", "Shamant Sai", "Soumil Garg", "Zhangir Meirbek", "Nitin Yarlagadda", "Kevin Zhu", "Chris Kim"], "title": "UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at Conference of the North American Chapter of the\n  Association for Computational Linguistics, Student Research Workshop 2025\n  (NAACL SRW 2025)", "summary": "Theory of Mind (ToM), the ability to understand the mental states of oneself\nand others, remains a challenging area for large language models (LLMs), which\noften fail to predict human mental states accurately. In this paper, we\nintroduce UniToMBench, a unified benchmark that integrates the strengths of\nSimToM and TOMBENCH to systematically improve and assess ToM capabilities in\nLLMs by integrating multi-interaction task designs and evolving story\nscenarios. Supported by a custom dataset of over 1,000 hand-written scenarios,\nUniToMBench combines perspective-taking techniques with diverse evaluation\nmetrics to better stimulate social cognition in LLMs. Through evaluation, we\nobserve that while models like GPT-4o and GPT-4o Mini show consistently high\naccuracy in tasks involving emotional and belief-related scenarios, with\nresults usually above 80%, there is significant variability in their\nperformance across knowledge-based tasks. These results highlight both the\nstrengths and limitations of current LLMs in ToM-related tasks, underscoring\nthe value of UniToMBench as a comprehensive tool for future development. Our\ncode is publicly available here:\nhttps://github.com/Shamant/unifiedtombenchmark.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u7edf\u4e00\u57fa\u51c6UniToMBench\uff0c\u901a\u8fc7\u591a\u4e92\u52a8\u4efb\u52a1\u548c\u52a8\u6001\u573a\u666f\u8bbe\u8ba1\u7cfb\u7edf\u63d0\u5347\u5e76\u8bc4\u4f30LLMs\u7684\u5fc3\u7406\u7406\u8bba\u80fd\u529b\uff0c\u53d1\u73b0GPT-4o\u7cfb\u5217\u5728\u60c5\u611f/\u4fe1\u5ff5\u4efb\u52a1\u8868\u73b0\u7a33\u5b9a\uff08>80%\uff09\uff0c\u4f46\u77e5\u8bc6\u578b\u4efb\u52a1\u5b58\u5728\u6ce2\u52a8\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5fc3\u7406\u7406\u8bba\uff08ToM\uff09\u4efb\u52a1\u4e2d\u96be\u4ee5\u51c6\u786e\u9884\u6d4b\u4eba\u7c7b\u5fc3\u7406\u72b6\u6001\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u5de5\u5177\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u3002", "method": "\u6574\u5408SimToM\u548cTOMBENCH\u4f18\u52bf\uff0c\u8bbe\u8ba1\u5305\u542b1,000+\u624b\u5de5\u573a\u666f\u7684\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u89c6\u89d2\u91c7\u62e9\u6280\u672f\u548c\u591a\u7ef4\u5ea6\u8bc4\u4f30\u6307\u6807\uff08\u51c6\u786e\u6027/\u63a8\u7406\u6df1\u5ea6/\u54cd\u5e94\u4e00\u81f4\u6027\uff09\u3002", "result": "GPT-4o\u7cfb\u5217\u5728\u60c5\u611f/\u4fe1\u5ff5\u573a\u666f\u51c6\u786e\u7387\u8d8580%\uff0c\u4f46\u77e5\u8bc6\u578b\u4efb\u52a1\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff08\u5982\u5e38\u8bc6\u63a8\u7406\u51c6\u786e\u7387\u6ce2\u52a8\u8fbe35%\uff09\u3002", "conclusion": "UniToMBench\u6709\u6548\u63ed\u793aLLMs\u5728ToM\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u8fb9\u754c\uff0c\u4e3a\u6a21\u578b\u7684\u793e\u4f1a\u8ba4\u77e5\u80fd\u529b\u63d0\u5347\u63d0\u4f9b\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2506.09457", "pdf": "https://arxiv.org/pdf/2506.09457", "abs": "https://arxiv.org/abs/2506.09457", "authors": ["Zeguan Xiao", "Yun Chen", "Guanhua Chen"], "title": "Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization\n(DPO) and Simple Preference Optimization (SimPO), have emerged as efficient\nalternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms\nfor aligning large language models (LLMs) with human preferences. However, DAAs\nsuffer from a fundamental limitation we identify as the \"reward-generation gap\"\n-- a misalignment between optimization objectives during training and actual\ngeneration performance during inference. In this paper, we find a contributor\nto the reward-generation gap is the mismatch between the inherent importance of\nprefix tokens during the LLM generation process and how this importance is\nreflected in the implicit reward functions of DAAs. To bridge the gap, we\nintroduce a simple yet effective approach called Prefix-Oriented Equal-length\nTraining (POET), which truncates both preferred and dispreferred responses to\nmatch the shorter one's length. Training with POET, where both responses in\neach sample are truncated to equal length, resulting in diverse truncated\nlengths across samples, the optimization of DAAs objective is implicitly\nconstrained to converge across all positions, thus paying more attention to\nprefix tokens than the standard DAAs. We conduct experiments with DPO and\nSimPO, two representative DAAs, demonstrating that POET improves over their\nstandard implementations, achieving up to 15.6 points in AlpacaEval 2 and\noverall improvements across downstream tasks. Our results highlight the\nimportance of addressing the misalignment between reward optimization and\ngeneration performance in DAAs.", "AI": {"tldr": "\u63d0\u51faPOET\u65b9\u6cd5\u89e3\u51b3\u76f4\u63a5\u5bf9\u9f50\u7b97\u6cd5(DAAs)\u7684\u5956\u52b1-\u751f\u6210\u5dee\u8ddd\u95ee\u9898\uff0c\u901a\u8fc7\u7b49\u957f\u622a\u65ad\u4f18\u5316\u524d\u7f00\u5173\u6ce8\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347", "motivation": "\u53d1\u73b0\u76f4\u63a5\u5bf9\u9f50\u7b97\u6cd5\u5b58\u5728\u5956\u52b1-\u751f\u6210\u5dee\u8ddd\u7684\u6839\u672c\u95ee\u9898\uff0c\u4e3b\u8981\u6e90\u4e8e\u6a21\u578b\u751f\u6210\u8fc7\u7a0b\u4e2d\u524d\u7f00token\u7684\u91cd\u8981\u6027\u4e0e\u7b97\u6cd5\u5956\u52b1\u51fd\u6570\u5173\u6ce8\u70b9\u4e4b\u95f4\u7684\u9519\u914d", "method": "\u63d0\u51faPOET\u65b9\u6cd5\uff1a\u5c06\u504f\u597d/\u975e\u504f\u597d\u54cd\u5e94\u622a\u65ad\u81f3\u76f8\u540c\u957f\u5ea6\uff0c\u901a\u8fc7\u591a\u6837\u5316\u622a\u65ad\u957f\u5ea6\u7ea6\u675f\u4f18\u5316\u8fc7\u7a0b\uff0c\u589e\u5f3a\u5bf9\u524d\u7f00token\u7684\u5173\u6ce8", "result": "\u4f7f\u7528POET\u6539\u8fdb\u7684DPO\u548cSimPO\u7b97\u6cd5\u5728AlpacaEval 2\u63d0\u5347\u8fbe15.6\u5206\uff0c\u4e0b\u6e38\u4efb\u52a1\u5168\u9762\u6539\u8fdb", "conclusion": "\u89e3\u51b3\u5956\u52b1\u4f18\u5316\u4e0e\u751f\u6210\u6027\u80fd\u7684\u9519\u914d\u5bf9DAAs\u81f3\u5173\u91cd\u8981\uff0cPOET\u901a\u8fc7\u5f3a\u5236\u7b49\u957f\u622a\u65ad\u6709\u6548\u7f29\u5c0f\u4e86\u5956\u52b1-\u751f\u6210\u5dee\u8ddd"}}
{"id": "2506.09495", "pdf": "https://arxiv.org/pdf/2506.09495", "abs": "https://arxiv.org/abs/2506.09495", "authors": ["Ilanit Sobol", "Shir Lissak", "Refael Tikochinski", "Tal Nakash", "Anat Brunstein Klomek", "Eyal Fruchter", "Roi Reichart"], "title": "Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Suicide remains a leading cause of death in Western countries, underscoring\nthe need for new research approaches. As social media becomes central to daily\nlife, digital footprints offer valuable insight into suicidal behavior.\nFocusing on individuals who attempted suicide while uploading videos to their\nchannels, we investigate: How do suicidal behaviors manifest on YouTube, and\nhow do they differ from expert knowledge? We applied complementary approaches:\ncomputational bottom-up, hybrid, and expert-driven top-down, on a novel\nlongitudinal dataset of 181 YouTube channels from individuals with\nlife-threatening attempts, alongside 134 control channels. In the bottom-up\napproach, we applied LLM-based topic modeling to identify behavioral\nindicators. Of 166 topics, five were associated with suicide-attempt, with two\nalso showing temporal attempt-related changes ($p<.01$) - Mental Health\nStruggles ($+0.08$)* and YouTube Engagement ($+0.1$)*. In the hybrid approach,\na clinical expert reviewed LLM-derived topics and flagged 19 as\nsuicide-related. However, none showed significant attempt-related temporal\neffects beyond those identified bottom-up. Notably, YouTube Engagement, a\nplatform-specific indicator, was not flagged by the expert, underscoring the\nvalue of bottom-up discovery. In the top-down approach, psychological\nassessment of suicide attempt narratives revealed that the only significant\ndifference between individuals who attempted before and those attempted during\ntheir upload period was the motivation to share this experience: the former\naimed to Help Others ($\\beta=-1.69$, $p<.01$), while the latter framed it as\npart of their Personal Recovery ($\\beta=1.08$, $p<.01$). By integrating these\napproaches, we offer a nuanced understanding of suicidality, bridging digital\nbehavior and clinical insights.\n  * Within-group changes in relation to the suicide attempt.", "AI": {"tldr": "\u901a\u8fc7\u6574\u5408\u8ba1\u7b97\u6a21\u578b\u4e0e\u4e34\u5e8a\u89c6\u89d2\uff0c\u7814\u7a76\u53d1\u73b0YouTube\u7528\u6237\u7684\u81ea\u6740\u884c\u4e3a\u5b58\u5728\u5e73\u53f0\u7279\u5f02\u6027\u6307\u6807\uff08\u5982YouTube Engagement\uff09\uff0c\u4e14\u81ea\u6740\u89c6\u9891\u4e0a\u4f20\u52a8\u673a\u5448\u73b0\u524d/\u540e\u9636\u6bb5\u5dee\u5f02\uff08\u52a9\u4eba\u5bfc\u5411 vs \u81ea\u6211\u7597\u6108\u5bfc\u5411\uff09", "motivation": "\u4f20\u7edf\u81ea\u6740\u7814\u7a76\u5b58\u5728\u5c40\u9650\uff0c\u793e\u4ea4\u5a92\u4f53\u6570\u5b57\u8db3\u8ff9\u4e3a\u7406\u89e3\u81ea\u6740\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u81ea\u6740\u884c\u4e3a\u5728YouTube\u5e73\u53f0\u7684\u5177\u4f53\u8868\u73b0\u53ca\u5176\u4e0e\u4e34\u5e8a\u8ba4\u77e5\u7684\u5dee\u5f02", "method": "\u91c7\u7528\u4e09\u6a21\u6001\u65b9\u6cd5\uff1a1\uff09\u57fa\u4e8eLLM\u7684\u6587\u672c\u4e3b\u9898\u5efa\u6a21\uff08\u81ea\u4e0b\u800c\u4e0a\uff092\uff09\u4e13\u5bb6\u6807\u6ce8\u7684\u6df7\u5408\u65b9\u6cd5 3\uff09\u81ea\u6740\u53d9\u4e8b\u5fc3\u7406\u8bc4\u4f30\uff08\u81ea\u4e0a\u800c\u4e0b\uff09\uff0c\u5206\u6790181\u4e2a\u6709\u81ea\u6740\u884c\u4e3aYouTube\u9891\u9053\u548c134\u4e2a\u5bf9\u7167\u7ec4\u7684\u7eb5\u5411\u6570\u636e", "result": "1\uff09\u8bc6\u522b\u51fa5\u4e2a\u4e0e\u81ea\u6740\u884c\u4e3a\u76f8\u5173\u7684\u4e3b\u9898\uff0c\u5176\u4e2d\u5fc3\u7406\u5065\u5eb7\u6323\u624e\uff08+0.08\uff09\u548c\u5e73\u53f0\u53c2\u4e0e\u5ea6\uff08+0.1\uff09\u5448\u73b0\u663e\u8457\u65f6\u5e8f\u53d8\u5316 2\uff09\u4e13\u5bb6\u672a\u8bc6\u522b\u51faYouTube Engagement\u8fd9\u4e00\u5e73\u53f0\u7279\u5f02\u6027\u6307\u6807 3\uff09\u81ea\u6740\u524d\u4e0a\u4f20\u8005\u52a8\u673a\u4e3a\u5e2e\u52a9\u4ed6\u4eba\uff08\u03b2=-1.69\uff09\uff0c\u81ea\u6740\u671f\u95f4\u4e0a\u4f20\u8005\u66f4\u5f3a\u8c03\u81ea\u6211\u5eb7\u590d\uff08\u03b2=1.08\uff09", "conclusion": "\u6570\u5b57\u884c\u4e3a\u5206\u6790\u4e0e\u4e34\u5e8a\u89c6\u89d2\u7684\u878d\u5408\u80fd\u66f4\u5168\u9762\u7406\u89e3\u81ea\u6740\u5fc3\u7406\uff0c\u5e73\u53f0\u7279\u5f02\u6027\u884c\u4e3a\u6307\u6807\uff08\u5982YouTube Engagement\uff09\u5bf9\u81ea\u6740\u98ce\u9669\u8bc6\u522b\u5177\u6709\u72ec\u7279\u4ef7\u503c\uff0c\u4e0a\u4f20\u52a8\u673a\u5dee\u5f02\u4e3a\u5371\u673a\u5e72\u9884\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2506.09501", "pdf": "https://arxiv.org/pdf/2506.09501", "abs": "https://arxiv.org/abs/2506.09501", "authors": ["Jiayi Yuan", "Hao Li", "Xinheng Ding", "Wenya Xie", "Yu-Jhe Li", "Wentian Zhao", "Kun Wan", "Jing Shi", "Xia Hu", "Zirui Liu"], "title": "Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are now integral across various domains and have\ndemonstrated impressive performance. Progress, however, rests on the premise\nthat benchmark scores are both accurate and reproducible. We demonstrate that\nthe reproducibility of LLM performance is fragile: changing system\nconfiguration such as evaluation batch size, GPU count, and GPU version can\nintroduce significant difference in the generated responses. This issue is\nespecially pronounced in reasoning models, where minor rounding differences in\nearly tokens can cascade into divergent chains of thought, ultimately affecting\naccuracy. For instance, under bfloat16 precision with greedy decoding, a\nreasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9% variation\nin accuracy and 9,000 tokens difference in response length due to differences\nin GPU count, type, and evaluation batch size. We trace the root cause of this\nvariability to the non-associative nature of floating-point arithmetic under\nlimited numerical precision. This work presents the first systematic\ninvestigation into how numerical precision affects reproducibility in LLM\ninference. Through carefully controlled experiments across various hardware,\nsoftware, and precision settings, we quantify when and how model outputs\ndiverge. Our analysis reveals that floating-point precision -- while critical\nfor reproducibility -- is often neglected in evaluation practices. Inspired by\nthis, we develop a lightweight inference pipeline, dubbed LayerCast, that\nstores weights in 16-bit precision but performs all computations in FP32,\nbalancing memory efficiency with numerical stability. Code is available at\nhttps://github.com/nanomaoli/llm_reproducibility.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6027\u80fd\u8bc4\u4f30\u7684\u53ef\u590d\u73b0\u6027\u5b58\u5728\u8106\u5f31\u6027\uff0c\u6d6e\u70b9\u8fd0\u7b97\u7cbe\u5ea6\u5dee\u5f02\u4f1a\u5bfc\u81f4\u63a8\u7406\u7ed3\u679c\u663e\u8457\u6ce2\u52a8\uff08\u5982\u51c6\u786e\u7387\u6ce2\u52a8\u8fbe9%\uff09\uff0c\u8bba\u6587\u63d0\u51faLayerCast\u8f7b\u91cf\u7ea7\u63a8\u7406\u6846\u67b6\u5e73\u8861\u8ba1\u7b97\u7a33\u5b9a\u6027\u4e0e\u5185\u5b58\u6548\u7387", "motivation": "\u5f53\u524dLLM\u8bc4\u4f30\u4f9d\u8d56\u57fa\u51c6\u6d4b\u8bd5\u7684\u53ef\u590d\u73b0\u6027\u5047\u8bbe\uff0c\u4f46\u5b9e\u9645\u53d1\u73b0\u4e0d\u540c\u786c\u4ef6\u914d\u7f6e\uff08GPU\u6570\u91cf/\u7248\u672c\uff09\u548c\u8f6f\u4ef6\u53c2\u6570\uff08\u8bc4\u4f30\u6279\u6b21\u5927\u5c0f\uff09\u4f1a\u5bfc\u81f4\u6a21\u578b\u8f93\u51fa\u663e\u8457\u5dee\u5f02\uff0c\u5f71\u54cd\u8bc4\u4f30\u7ed3\u679c\u53ef\u4fe1\u5ea6", "method": "\u901a\u8fc7\u63a7\u5236\u53d8\u91cf\u5b9e\u9a8c\uff08\u786c\u4ef6\u7c7b\u578b/\u6570\u91cf\u3001\u8ba1\u7b97\u7cbe\u5ea6\u8bbe\u7f6e\uff09\u8ffd\u8e2a\u6570\u503c\u8bef\u5dee\u4f20\u64ad\u673a\u5236\uff0c\u8bbe\u8ba1LayerCast\u6846\u67b6\uff0816\u4f4d\u5b58\u50a8+FP32\u8ba1\u7b97\uff09\u5e73\u8861\u5185\u5b58\u6548\u7387\u4e0e\u6570\u503c\u7a33\u5b9a\u6027", "result": "\u63a8\u7406\u6a21\u578b\u5728bfloat16\u7cbe\u5ea6\u4e0b\u56e0GPU\u914d\u7f6e\u5dee\u5f02\u5bfc\u81f4\u6700\u59279%\u51c6\u786e\u7387\u6ce2\u52a8\u53ca9000 tokens\u8f93\u51fa\u957f\u5ea6\u5dee\u5f02\uff0cLayerCast\u53ef\u6709\u6548\u63a7\u5236\u6570\u503c\u8bef\u5dee\u79ef\u7d2f", "conclusion": "\u6d6e\u70b9\u8fd0\u7b97\u7cbe\u5ea6\u662fLLM\u53ef\u590d\u73b0\u6027\u7684\u5173\u952e\u56e0\u7d20\uff0c\u9700\u5efa\u7acb\u6807\u51c6\u5316\u8bc4\u4f30\u914d\u7f6e\u4f53\u7cfb\uff0cLayerCast\u4e3a\u7cbe\u5ea6\u654f\u611f\u573a\u666f\u63d0\u4f9b\u8f7b\u91cf\u5316\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.09507", "pdf": "https://arxiv.org/pdf/2506.09507", "abs": "https://arxiv.org/abs/2506.09507", "authors": ["Bingheng Wu", "Jingze Shi", "Yifan Wu", "Nan Tang", "Yuyu Luo"], "title": "TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary Position Embedding", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Transformers exhibit proficiency in capturing long-range dependencies,\nwhereas State Space Models (SSMs) facilitate linear-time sequence modeling.\nNotwithstanding their synergistic potential, the integration of these\narchitectures presents a significant challenge, primarily attributable to a\nfundamental incongruity in their respective positional encoding mechanisms:\nTransformers rely on explicit Rotary Position Embeddings (RoPE), while SSMs\nleverage implicit positional representations via convolutions. This divergence\noften precipitates discontinuities and suboptimal performance. To address this\nimpediment, we propose a unified rotary position embedding (\\textbf{\\ourRoPE})\nmethodology, thereby establishing a consistent positional encoding framework\nfor both self-attention and state-space components. Using this \\ourRoPE, we\nintroduce \\textbf{\\model}, a hybrid architecture that coherently integrates the\nTransformer and SSM layers under this unified positional encoding scheme. At a\n4K sequence length, \\model exhibits training and inference speeds that are\n\\textbf{42.3\\% and 29.5\\% faster}, respectively, relative to standard\nTransformer models. It also delivers higher accuracy: under comparable\nsettings, it surpasses a Transformer baseline by over 4\\% on language modeling\nbenchmarks. \\model furthermore scales more effectively: \\model-1.3B gains\n\\textbf{7.22\\%} in average accuracy over its 320M version (versus about 6\\%\ngains for equivalent Transformers or SSMs). Our results show that unified\npositional encoding resolves positional incompatibility in hybrid models,\nenabling efficient, high-performance long-context modeling.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801RoPE\u65b9\u6cd5\u89e3\u51b3Transformer\u4e0eSSM\u7684\u4f4d\u7f6e\u7f16\u7801\u4e0d\u517c\u5bb9\u95ee\u9898\uff0c\u6784\u5efaMambaFormer\u6df7\u5408\u6a21\u578b\u5b9e\u73b042.3%\u8bad\u7ec3\u52a0\u901f\u548c4%\u51c6\u786e\u7387\u63d0\u5347\u3002", "motivation": "Transformer\u64c5\u957f\u957f\u8ddd\u79bb\u4f9d\u8d56\u4f46\u4f9d\u8d56\u663e\u5f0f\u4f4d\u7f6e\u7f16\u7801\uff0cSSM\u652f\u6301\u7ebf\u6027\u65f6\u95f4\u5efa\u6a21\u4f46\u4f7f\u7528\u9690\u5f0f\u5377\u79ef\u7f16\u7801\uff0c\u4e8c\u8005\u4f4d\u7f6e\u7f16\u7801\u673a\u5236\u4e0d\u517c\u5bb9\u5bfc\u81f4\u6027\u80fd\u635f\u5931\u3002", "method": "\u8bbe\u8ba1\u7edf\u4e00\u7684\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801\u6846\u67b6ourRoPE\uff0c\u6784\u5efa\u878d\u5408Transformer\u548cSSM\u5c42\u7684MambaFormer\u67b6\u6784\uff0c\u4fdd\u6301\u53c2\u6570\u89c4\u6a21\u4e0d\u53d8\u3002", "result": "4K\u5e8f\u5217\u4e0b\u8bad\u7ec3/\u63a8\u7406\u901f\u5ea6\u63d0\u534742.3%/29.5%\uff0c\u8bed\u8a00\u5efa\u6a21\u51c6\u786e\u7387\u8d85\u57fa\u7ebf4%\uff0c1.3B\u6a21\u578b\u8f83320M\u7248\u672c\u5e73\u5747\u7cbe\u5ea6\u63d0\u53477.22%\uff08Transformer\u4ec56%\uff09\u3002", "conclusion": "\u7edf\u4e00\u4f4d\u7f6e\u7f16\u7801\u6709\u6548\u89e3\u51b3\u6df7\u5408\u6a21\u578b\u7684\u4f4d\u7f6e\u4e0d\u517c\u5bb9\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u6548\u9ad8\u6027\u80fd\u7684\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\uff0c\u8bc1\u660e\u67b6\u6784\u878d\u5408\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2506.09513", "pdf": "https://arxiv.org/pdf/2506.09513", "abs": "https://arxiv.org/abs/2506.09513", "authors": ["Yu Sun", "Xingyu Qian", "Weiwen Xu", "Hao Zhang", "Chenghao Xiao", "Long Li", "Yu Rong", "Wenbing Huang", "Qifeng Bai", "Tingyang Xu"], "title": "ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "24 pages, 6 figures, 7 tables", "summary": "Though reasoning-based large language models (LLMs) have excelled in\nmathematics and programming, their capabilities in knowledge-intensive medical\nquestion answering remain underexplored. To address this, we introduce\nReasonMed, the largest medical reasoning dataset, comprising 370k high-quality\nexamples distilled from 1.7 million initial reasoning paths generated by\nvarious LLMs. ReasonMed is constructed through a \\textit{multi-agent\nverification and refinement process}, where we design an \\textit{Error Refiner}\nto enhance the reasoning paths by identifying and correcting error-prone steps\nflagged by a verifier. Leveraging ReasonMed, we systematically investigate best\npractices for training medical reasoning models and find that combining\ndetailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields\nthe most effective fine-tuning strategy. Based on this strategy, we train\nReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the\nprior best by 4.17\\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\\%.", "AI": {"tldr": "\u63d0\u51fa\u5f53\u524d\u6700\u5927\u533b\u5b66\u63a8\u7406\u6570\u636e\u96c6ReasonMed\uff0837\u4e07\u6837\u672c\uff09\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u9a8c\u8bc1\u6846\u67b6\u6539\u8fdb\u63a8\u7406\u8def\u5f84\uff0c\u8bad\u7ec3\u51fa\u8d85\u8d8a\u5927\u6a21\u578b\u76847B\u533b\u5b66\u63a8\u7406\u4e13\u7528\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u533b\u5b66\u95ee\u7b54\u9886\u57df\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u6316\u6398", "method": "1. \u6784\u5efa\u5305\u542b37\u4e07\u9ad8\u8d28\u91cf\u6837\u672c\u7684ReasonMed\u6570\u636e\u96c6\uff08\u521d\u59cb170\u4e07\u8def\u5f84\uff09\n2. \u8bbe\u8ba1\u9519\u8bef\u4fee\u6b63\u5668(Error Refiner)\u6539\u8fdb\u88ab\u9a8c\u8bc1\u5668\u6807\u8bb0\u7684\u9519\u8bef\u63a8\u7406\u6b65\u9aa4\n3. \u9a8c\u8bc1\u94fe\u5f0f\u63a8\u7406(CoT)\u7ed3\u5408\u7b54\u6848\u6458\u8981\u7684\u5fae\u8c03\u7b56\u7565\u6548\u679c\u6700\u4f73", "result": "ReasonMed-7B\u5728sub-10B\u6a21\u578b\u4e2d\u5237\u65b0\u7eaa\u5f55\uff08\u63d0\u53474.17%\uff09\uff0c\u5728PubMedQA\u4e0a\u8d85\u8d8aLLaMA3.1-70B\u8fbe4.60%", "conclusion": "\u901a\u8fc7\u7cbe\u7ec6\u5316\u63a8\u7406\u8def\u5f84\u6e05\u6d17\u548cCoT+\u6458\u8981\u7684\u8054\u5408\u8bad\u7ec3\u7b56\u7565\uff0c\u8bc1\u660e\u5c0f\u89c4\u6a21\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u53ef\u4ee5\u8d85\u8d8a\u53c2\u6570\u91cf\u7ea7\u66f4\u5927\u7684\u901a\u7528\u6a21\u578b"}}
{"id": "2506.09542", "pdf": "https://arxiv.org/pdf/2506.09542", "abs": "https://arxiv.org/abs/2506.09542", "authors": ["Dingjun Wu", "Yukun Yan", "Zhenghao Liu", "Zhiyuan Liu", "Maosong Sun"], "title": "KG-Infused RAG: Augmenting Corpus-Based RAG with External Knowledge Graphs", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) improves factual accuracy by grounding\nresponses in external knowledge. However, existing methods typically rely on a\nsingle source, either unstructured text or structured knowledge. Moreover, they\nlack cognitively inspired mechanisms for activating relevant knowledge. To\naddress these issues, we propose KG-Infused RAG, a framework that integrates\nKGs into RAG systems to implement spreading activation, a cognitive process\nthat enables concept association and inference. KG-Infused RAG retrieves KG\nfacts, expands the query accordingly, and enhances generation by combining\ncorpus passages with structured facts, enabling interpretable, multi-source\nretrieval grounded in semantic structure. We further improve KG-Infused RAG via\npreference learning on sampled key stages in the pipeline. Experiments on five\nQA benchmarks show that KG-Infused RAG consistently outperforms vanilla RAG (by\n3.8% to 13.8%). Additionally, when integrated into Self-RAG, KG-Infused RAG\nbrings further performance gains, demonstrating its effectiveness and\nversatility as a plug-and-play enhancement module for corpus-based RAG methods.", "AI": {"tldr": "KG-Infused RAG\u901a\u8fc7\u6574\u5408\u77e5\u8bc6\u56fe\u8c31\u4e0e\u6fc0\u6d3b\u6269\u6563\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347RAG\u7cfb\u7edf\u7684\u95ee\u7b54\u6027\u80fd\uff083.8%-13.8%\uff09\uff0c\u5e76\u53ef\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\u589e\u5f3a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709RAG\u65b9\u6cd5\u4f9d\u8d56\u5355\u4e00\u6570\u636e\u6e90\u4e14\u7f3a\u4e4f\u8ba4\u77e5\u542f\u53d1\u673a\u5236\uff0c\u5bfc\u81f4\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u63a8\u7406\u80fd\u529b\u53d7\u9650\u3002", "method": "\u5c06\u77e5\u8bc6\u56fe\u8c31\u878d\u5165RAG\u7cfb\u7edf\uff0c\u901a\u8fc7\u6fc0\u6d3b\u6269\u6563\u5b9e\u73b0\u8bed\u4e49\u5173\u8054\u63a8\u7406\uff0c\u7ed3\u5408\u7ed3\u6784\u5316/\u975e\u7ed3\u6784\u5316\u6570\u636e\u68c0\u7d22\uff0c\u5e76\u91c7\u7528\u504f\u597d\u5b66\u4e60\u4f18\u5316\u5173\u952e\u6d41\u7a0b\u8282\u70b9\u3002", "result": "\u57285\u4e2aQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u8d85\u8d8a\u539f\u59cbRAG\uff08\u6700\u9ad8\u63d0\u534713.8%\uff09\uff0c\u6574\u5408\u81f3Self-RAG\u540e\u4ea7\u751f\u989d\u5916\u589e\u76ca\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u8bed\u4e49\u7ed3\u6784\u589e\u5f3a\u591a\u6e90\u68c0\u7d22\uff0c\u4e3a\u57fa\u4e8e\u8bed\u6599\u7684RAG\u65b9\u6cd5\u63d0\u4f9b\u53ef\u89e3\u91ca\u3001\u53ef\u6269\u5c55\u7684\u589e\u5f3a\u65b9\u6848\u3002"}}
{"id": "2506.09556", "pdf": "https://arxiv.org/pdf/2506.09556", "abs": "https://arxiv.org/abs/2506.09556", "authors": ["Georgios Chatzichristodoulou", "Despoina Kosmopoulou", "Antonios Kritikos", "Anastasia Poulopoulou", "Efthymios Georgiou", "Athanasios Katsamanis", "Vassilis Katsouros", "Alexandros Potamianos"], "title": "MEDUSA: A Multimodal Deep Fusion Multi-Stage Training Framework for Speech Emotion Recognition in Naturalistic Conditions", "categories": ["cs.CL"], "comment": "Accepted at Interspeech 2025", "summary": "SER is a challenging task due to the subjective nature of human emotions and\ntheir uneven representation under naturalistic conditions. We propose MEDUSA, a\nmultimodal framework with a four-stage training pipeline, which effectively\nhandles class imbalance and emotion ambiguity. The first two stages train an\nensemble of classifiers that utilize DeepSER, a novel extension of a deep\ncross-modal transformer fusion mechanism from pretrained self-supervised\nacoustic and linguistic representations. Manifold MixUp is employed for further\nregularization. The last two stages optimize a trainable meta-classifier that\ncombines the ensemble predictions. Our training approach incorporates human\nannotation scores as soft targets, coupled with balanced data sampling and\nmultitask learning. MEDUSA ranked 1st in Task 1: Categorical Emotion\nRecognition in the Interspeech 2025: Speech Emotion Recognition in Naturalistic\nConditions Challenge.", "AI": {"tldr": "\u63d0\u51fa\u591a\u6a21\u6001\u6846\u67b6MEDUSA\uff0c\u901a\u8fc7\u56db\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u89e3\u51b3\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u60c5\u611f\u6a21\u7cca\u95ee\u9898\uff0c\u5728Interspeech 2025\u6311\u6218\u8d5b\u4e2d\u83b7\u5f97\u7b2c\u4e00", "motivation": "\u8bed\u97f3\u60c5\u611f\u8bc6\u522b(SER)\u56e0\u60c5\u611f\u4e3b\u89c2\u6027\u548c\u81ea\u7136\u573a\u666f\u4e0b\u7684\u6570\u636e\u5206\u5e03\u4e0d\u5747\u8861\u800c\u6781\u5177\u6311\u6218\u6027\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u540c\u65f6\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u60c5\u611f\u6b67\u4e49\u7684\u9c81\u68d2\u6846\u67b6", "method": "\u56db\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff1a\u524d\u4e24\u9636\u6bb5\u8bad\u7ec3\u57fa\u4e8eDeepSER\uff08\u6df1\u5ea6\u8de8\u6a21\u6001transformer\uff09\u7684\u96c6\u6210\u5206\u7c7b\u5668\uff0c\u4f7f\u7528Manifold MixUp\u6b63\u5219\u5316\uff1b\u540e\u4e24\u9636\u6bb5\u4f18\u5316\u53ef\u8bad\u7ec3\u5143\u5206\u7c7b\u5668\uff0c\u7ed3\u5408\u5e73\u8861\u91c7\u6837\u3001\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u4eba\u5de5\u6807\u6ce8\u8f6f\u76ee\u6807", "result": "MEDUSA\u5728Interspeech 2025\u81ea\u7136\u573a\u666f\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u6311\u6218\u8d5b\u7684\u7c7b\u522b\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4e2d\u83b7\u5f97\u7b2c\u4e00\u540d", "conclusion": "\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u67b6\u6784\u3001\u96c6\u6210\u5b66\u4e60\u7b56\u7565\u548c\u521b\u65b0\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0cMEDUSA\u6709\u6548\u89e3\u51b3\u4e86SER\u7684\u6838\u5fc3\u6311\u6218\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd"}}
{"id": "2506.09558", "pdf": "https://arxiv.org/pdf/2506.09558", "abs": "https://arxiv.org/abs/2506.09558", "authors": ["Eleni Gkovedarou", "Joke Daems", "Luna De Bruyne"], "title": "Gender Bias in English-to-Greek Machine Translation", "categories": ["cs.CL"], "comment": "Accepted at GITT 2025 (MT Summit)", "summary": "As the demand for inclusive language increases, concern has grown over the\nsusceptibility of machine translation (MT) systems to reinforce gender\nstereotypes. This study investigates gender bias in two commercial MT systems,\nGoogle Translate and DeepL, focusing on the understudied English-to-Greek\nlanguage pair. We address three aspects of gender bias: i) male bias, ii)\noccupational stereotyping, and iii) errors in anti-stereotypical translations.\nAdditionally, we explore the potential of prompted GPT-4o as a bias mitigation\ntool that provides both gender-explicit and gender-neutral alternatives when\nnecessary. To achieve this, we introduce GendEL, a manually crafted bilingual\ndataset of 240 gender-ambiguous and unambiguous sentences that feature\nstereotypical occupational nouns and adjectives. We find persistent gender bias\nin translations by both MT systems; while they perform well in cases where\ngender is explicitly defined, with DeepL outperforming both Google Translate\nand GPT-4o in feminine gender-unambiguous sentences, they are far from\nproducing gender-inclusive or neutral translations when the gender is\nunspecified. GPT-4o shows promise, generating appropriate gendered and neutral\nalternatives for most ambiguous cases, though residual biases remain evident.", "AI": {"tldr": "\u7814\u7a76\u63ed\u793a\u8c37\u6b4c\u7ffb\u8bd1\u548cDeepL\u5728\u82f1\u5e0c\u7ffb\u8bd1\u4e2d\u5b58\u5728\u6027\u522b\u504f\u89c1\uff0cGPT-4o\u5c55\u73b0\u90e8\u5206\u7ea0\u504f\u6f5c\u529b\u4f46\u4ecd\u6709\u6b8b\u4f59\u504f\u5dee", "motivation": "\u968f\u7740\u793e\u4f1a\u5bf9\u5305\u5bb9\u6027\u8bed\u8a00\u9700\u6c42\u7684\u589e\u957f\uff0c\u9700\u8bc4\u4f30\u5546\u7528\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u5728\u672a\u5145\u5206\u7814\u7a76\u7684\u8bed\u8a00\u5bf9\uff08\u5982\u82f1\u5e0c\u7ffb\u8bd1\uff09\u4e2d\u5f3a\u5316\u6027\u522b\u523b\u677f\u5370\u8c61\u7684\u98ce\u9669\uff0c\u5e76\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7ea0\u504f\u53ef\u80fd\u6027", "method": "\u6784\u5efa\u5305\u542b240\u4e2a\u6027\u522b\u6a21\u7cca/\u660e\u786e\u53e5\u5b50\u7684GendEL\u53cc\u8bed\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u8c37\u6b4c\u7ffb\u8bd1/DeepL/GPT-4o\u5728\u7537\u6027\u504f\u89c1\u3001\u804c\u4e1a\u523b\u677f\u5370\u8c61\u3001\u53cd\u523b\u677f\u7ffb\u8bd1\u9519\u8bef\u4e09\u65b9\u9762\u7684\u8868\u73b0\uff0c\u63a2\u7d22GPT-4o\u751f\u6210\u6027\u522b\u660e\u786e/\u4e2d\u6027\u66ff\u4ee3\u65b9\u6848\u7684\u80fd\u529b", "result": "\u5546\u7528\u7cfb\u7edf\u5728\u6027\u522b\u660e\u786e\u65f6\u8868\u73b0\u826f\u597d\uff08DeepL\u5973\u6027\u53e5\u7ffb\u8bd1\u6700\u4f18\uff09\uff0c\u4f46\u6027\u522b\u6a21\u7cca\u65f6\u7f3a\u4e4f\u5305\u5bb9\u6027\u7ffb\u8bd1\uff1bGPT-4o\u80fd\u751f\u621075%\u5408\u9002\u7684\u6027\u522b\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u4ecd\u5b58\u5728\u6b8b\u4f59\u504f\u89c1", "conclusion": "\u5f53\u524d\u5546\u7528\u7ffb\u8bd1\u7cfb\u7edf\u5c1a\u672a\u5b9e\u73b0\u6027\u522b\u5305\u5bb9\uff0cGPT-4o\u867d\u5c55\u73b0\u7ea0\u504f\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316\u7b97\u6cd5\u4ee5\u51cf\u5c11\u6b8b\u4f59\u504f\u89c1\uff0c\u5efa\u8bae\u7ed3\u5408\u4eba\u5de5\u6821\u9a8c\u673a\u5236\u63d0\u5347\u7ffb\u8bd1\u5305\u5bb9\u6027"}}
{"id": "2506.09560", "pdf": "https://arxiv.org/pdf/2506.09560", "abs": "https://arxiv.org/abs/2506.09560", "authors": ["Stefan Krsteski", "Matea Tashkovska", "Borjan Sazdov", "Hristijan Gjoreski", "Branislav Gerazov"], "title": "Towards Open Foundation Language Model and Corpus for Macedonian: A Low-Resource Language", "categories": ["cs.CL"], "comment": "Camera-ready version accepted at SlavNLP-2025@ACL", "summary": "The increase in technological adoption worldwide comes with demands for novel\ntools to be used by the general population. Large Language Models (LLMs)\nprovide a great opportunity in this respect, but their capabilities remain\nlimited for low-resource languages, restricting applications in countries where\nsuch languages are spoken. We create several resources to facilitate the\nadoption of LLMs and to support research advancements for Macedonian. We\ncollect the largest Macedonian corpus to date, consisting of 40GB of textual\ndata and totaling 3.5B words. To support conversational applications, we\ncollect a 106k-instance instruction dataset, carefully built to be culturally\ngrounded. For evaluation, we construct a Macedonian evaluation suite covering\nseven benchmarks. Finally, we train domestic-yak, a state-of-the-art\n8B-parameter model, on our curated datasets and evaluate it against eight\nbaseline models using the newly constructed benchmark suite. Our model\noutperforms all existing models in the 8B parameter range across all\nbenchmarks, and achieves performance comparable to models up to 10x larger.\nFurthermore, a qualitative analysis with native speakers reveals that our model\nis preferred over larger counterparts, receiving higher ratings for grammatical\ncorrectness and cultural appropriateness. All datasets, code, and model weights\nare openly released, setting a foundation for advancing LLMs in similarly\nunderrepresented languages. These resources are publicly available at\ngithub.com/LVSTCK for source code, and at huggingface.co/LVSTCK for pretrained\nmodel weights and data.", "AI": {"tldr": "\u7814\u7a76\u8005\u4e3a\u9a6c\u5176\u987f\u8bed\u6784\u5efa\u4e86\u5305\u542b40GB\u6587\u672c\u8bed\u6599\u5e93\u300110.6\u4e07\u6761\u6307\u4ee4\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u5957\u4ef6\uff0c\u5e76\u8bad\u7ec3\u51fa\u6027\u80fd\u8d85\u8d8a\u540c\u89c4\u6a21\u6a21\u578b\u7684domestic-yak\u6a21\u578b", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u9a6c\u5176\u987f\u8bed\uff09\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u652f\u6301\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4fc3\u8fdb\u76f8\u5173\u5730\u533a\u7684\u6280\u672f\u5e94\u7528\u53d1\u5c55", "method": "1. \u6536\u96c6\u9a6c\u5176\u987f\u8bed40GB\u8bed\u6599\u5e93\uff083.5B\u5355\u8bcd\uff09\n2. \u6784\u5efa\u6587\u5316\u76f8\u5173\u768410.6\u4e07\u6761\u6307\u4ee4\u6570\u636e\u96c6\n3. \u521b\u5efa\u6db5\u76d67\u4e2a\u57fa\u51c6\u7684\u8bc4\u4f30\u4f53\u7cfb\n4. \u8bad\u7ec38B\u53c2\u6570\u7684domestic-yak\u6a21\u578b\u5e76\u8fdb\u884c\u591a\u7ef4\u5ea6\u8bc4\u4f30", "result": "1. domestic-yak\u57288B\u53c2\u6570\u6a21\u578b\u4e2d\u6240\u6709\u57fa\u51c6\u8868\u73b0\u6700\u4f73\n2. \u6027\u80fd\u53ef\u5ab2\u7f8e\u592710\u500d\u7684\u6a21\u578b\n3. \u6bcd\u8bed\u8005\u8bc4\u4f30\u663e\u793a\u5176\u8bed\u6cd5\u6b63\u786e\u6027\u548c\u6587\u5316\u9002\u914d\u6027\u4f18\u4e8e\u66f4\u5927\u6a21\u578b", "conclusion": "\u901a\u8fc7\u5f00\u653e\u6570\u636e\u96c6\u3001\u4ee3\u7801\u548c\u6a21\u578b\u6743\u91cd\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00LLM\u7814\u7a76\u5960\u5b9a\u57fa\u7840\uff0c\u8bc1\u660e\u4e13\u7528\u6a21\u578b\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u7684\u6709\u6548\u6027\uff0c\u63a8\u52a8\u8bed\u8a00\u6280\u672f\u516c\u5e73\u53d1\u5c55"}}
{"id": "2506.09566", "pdf": "https://arxiv.org/pdf/2506.09566", "abs": "https://arxiv.org/abs/2506.09566", "authors": ["Bla\u017e \u0160krlj", "Boshko Koloski", "Senja Pollak", "Nada Lavra\u010d"], "title": "From Symbolic to Neural and Back: Exploring Knowledge Graph-Large Language Model Synergies", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "To-appear as a book chapter", "summary": "Integrating structured knowledge from Knowledge Graphs (KGs) into Large\nLanguage Models (LLMs) enhances factual grounding and reasoning capabilities.\nThis survey paper systematically examines the synergy between KGs and LLMs,\ncategorizing existing approaches into two main groups: KG-enhanced LLMs, which\nimprove reasoning, reduce hallucinations, and enable complex question\nanswering; and LLM-augmented KGs, which facilitate KG construction, completion,\nand querying. Through comprehensive analysis, we identify critical gaps and\nhighlight the mutual benefits of structured knowledge integration. Compared to\nexisting surveys, our study uniquely emphasizes scalability, computational\nefficiency, and data quality. Finally, we propose future research directions,\nincluding neuro-symbolic integration, dynamic KG updating, data reliability,\nand ethical considerations, paving the way for intelligent systems capable of\nmanaging more complex real-world knowledge tasks.", "AI": {"tldr": "\u63a2\u7d22\u77e5\u8bc6\u56fe\u8c31\u4e0e\u8bed\u8a00\u6a21\u578b\u7684\u53cc\u5411\u589e\u5f3a\u673a\u5236\uff0c\u63d0\u51fa\u795e\u7ecf\u7b26\u53f7\u96c6\u6210\u7b49\u672a\u6765\u65b9\u5411", "motivation": "\u901a\u8fc7\u7ed3\u6784\u5316\u77e5\u8bc6\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u5229\u7528\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa", "method": "\u5c06\u73b0\u6709\u65b9\u6cd5\u5206\u4e3aKG\u589e\u5f3aLLM\uff08\u63d0\u5347\u63a8\u7406/\u51cf\u5c11\u5e7b\u89c9\uff09\u548cLLM\u589e\u5f3aKG\uff08\u652f\u6301\u56fe\u8c31\u6784\u5efa/\u8865\u5168\uff09\u4e24\u7c7b", "result": "\u63ed\u793a\u7ed3\u6784\u5316\u77e5\u8bc6\u96c6\u6210\u7684\u534f\u540c\u6548\u5e94\uff0c\u5f3a\u8c03\u89c4\u6a21\u5316\u6269\u5c55\u548c\u8ba1\u7b97\u6548\u7387\u7684\u5173\u952e\u4f5c\u7528", "conclusion": "\u63d0\u51fa\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\u3001\u52a8\u6001\u77e5\u8bc6\u66f4\u65b0\u3001\u6570\u636e\u53ef\u9760\u6027\u53ca\u4f26\u7406\u6846\u67b6\u56db\u5927\u53d1\u5c55\u65b9\u5411"}}
{"id": "2506.09591", "pdf": "https://arxiv.org/pdf/2506.09591", "abs": "https://arxiv.org/abs/2506.09591", "authors": ["Stefan Arnold"], "title": "Memorization in Language Models through the Lens of Intrinsic Dimension", "categories": ["cs.CL"], "comment": null, "summary": "Language Models (LMs) are prone to memorizing parts of their data during\ntraining and unintentionally emitting them at generation time, raising concerns\nabout privacy leakage and disclosure of intellectual property. While previous\nresearch has identified properties such as context length, parameter size, and\nduplication frequency, as key drivers of unintended memorization, little is\nknown about how the latent structure modulates this rate of memorization. We\ninvestigate the role of Intrinsic Dimension (ID), a geometric proxy for the\nstructural complexity of a sequence in latent space, in modulating\nmemorization. Our findings suggest that ID acts as a suppressive signal for\nmemorization: compared to low-ID sequences, high-ID sequences are less likely\nto be memorized, particularly in overparameterized models and under sparse\nexposure. These findings highlight the interaction between scale, exposure, and\ncomplexity in shaping memorization.", "AI": {"tldr": "\u5185\u5728\u7ef4\u5ea6\uff08ID\uff09\u4f5c\u4e3a\u5e8f\u5217\u6f5c\u5728\u7a7a\u95f4\u7ed3\u6784\u590d\u6742\u6027\u7684\u51e0\u4f55\u6307\u6807\uff0c\u88ab\u53d1\u73b0\u80fd\u591f\u6291\u5236\u8bed\u8a00\u6a21\u578b\u7684\u8bb0\u5fc6\u884c\u4e3a\uff0c\u5c24\u5176\u5728\u8fc7\u53c2\u6570\u5316\u6a21\u578b\u548c\u7a00\u758f\u66b4\u9732\u573a\u666f\u4e0b\u6548\u679c\u663e\u8457\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u5728\u8bad\u7ec3\u4e2d\u53ef\u80fd\u65e0\u610f\u8bc6\u5730\u8bb0\u5fc6\u654f\u611f\u6570\u636e\uff0c\u5f15\u53d1\u9690\u79c1\u6cc4\u9732\u548c\u77e5\u8bc6\u4ea7\u6743\u95ee\u9898\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u6f5c\u5728\u7a7a\u95f4\u7ed3\u6784\u590d\u6742\u5ea6\uff08\u901a\u8fc7ID\u8861\u91cf\uff09\u5bf9\u8bb0\u5fc6\u884c\u4e3a\u7684\u8c03\u8282\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5e8f\u5217\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u5185\u5728\u7ef4\u5ea6\uff08ID\uff09\uff0c\u7814\u7a76\u5176\u4e0e\u8bb0\u5fc6\u6982\u7387\u7684\u5173\u7cfb\uff0c\u7279\u522b\u5173\u6ce8\u6a21\u578b\u89c4\u6a21\u3001\u6570\u636e\u66b4\u9732\u7a00\u758f\u6027\u4e0e\u7ed3\u6784\u590d\u6742\u6027\u7684\u4ea4\u4e92\u6548\u5e94\u3002", "result": "\u9ad8ID\u5e8f\u5217\u6bd4\u4f4eID\u5e8f\u5217\u66f4\u96be\u88ab\u8bb0\u5fc6\uff0c\u8fd9\u79cd\u6291\u5236\u6548\u5e94\u5728\u8fc7\u53c2\u6570\u5316\u6a21\u578b\uff08\u53c2\u6570\u89c4\u6a21\u8d85\u8fc7\u8bad\u7ec3\u6570\u636e\u7ef4\u5ea6\uff09\u548c\u7a00\u758f\u66b4\u9732\uff08\u8bad\u7ec3\u6570\u636e\u91cd\u590d\u6b21\u6570\u5c11\uff09\u65f6\u66f4\u52a0\u663e\u8457\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u6a21\u578b\u89c4\u6a21\u3001\u6570\u636e\u66b4\u9732\u9891\u7387\u548c\u5e8f\u5217\u7ed3\u6784\u590d\u6742\u6027\u4e09\u8005\u5171\u540c\u5851\u9020\u8bb0\u5fc6\u884c\u4e3a\u7684\u76f8\u4e92\u4f5c\u7528\u673a\u5236\uff0c\u4e3a\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u7684\u8bb0\u5fc6\u52a8\u529b\u5b66\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2506.09627", "pdf": "https://arxiv.org/pdf/2506.09627", "abs": "https://arxiv.org/abs/2506.09627", "authors": ["Nicolas Audinet de Pieuchon", "Adel Daoud", "Connor T. Jerzak", "Moa Johansson", "Richard Johansson"], "title": "Benchmarking Debiasing Methods for LLM-based Parameter Estimates", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) offer an inexpensive yet powerful way to\nannotate text, but are often inconsistent when compared with experts. These\nerrors can bias downstream estimates of population parameters such as\nregression coefficients and causal effects. To mitigate this bias, researchers\nhave developed debiasing methods such as Design-based Supervised Learning (DSL)\nand Prediction-Powered Inference (PPI), which promise valid estimation by\ncombining LLM annotations with a limited number of expensive expert\nannotations. Although these methods produce consistent estimates under\ntheoretical assumptions, it is unknown how they compare in finite samples of\nsizes encountered in applied research. We make two contributions: First, we\nstudy how each method's performance scales with the number of expert\nannotations, highlighting regimes where LLM bias or limited expert labels\nsignificantly affect results. Second, we compare DSL and PPI across a range of\ntasks, finding that although both achieve low bias with large datasets, DSL\noften outperforms PPI on bias reduction and empirical efficiency, but its\nperformance is less consistent across datasets. Our findings indicate that\nthere is a bias-variance tradeoff at the level of debiasing methods, calling\nfor more research on developing metrics for quantifying their efficiency in\nfinite samples.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4e24\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u6807\u6ce8\u53bb\u504f\u65b9\u6cd5\uff08DSL\u548cPPI\uff09\uff0c\u53d1\u73b0DSL\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u504f\u5dee\u66f4\u5c0f\u6548\u7387\u66f4\u9ad8\uff0c\u4f46\u8de8\u6570\u636e\u96c6\u8868\u73b0\u7a33\u5b9a\u6027\u8f83\u5dee\uff0c\u63ed\u793a\u4e86\u53bb\u504f\u65b9\u6cd5\u5b58\u5728\u504f\u5dee-\u65b9\u5dee\u6743\u8861\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6807\u6ce8\u4e0d\u4e00\u81f4\u6027\u4f1a\u5bfc\u81f4\u4e0b\u6e38\u7edf\u8ba1\u5206\u6790\u504f\u5dee\uff0c\u73b0\u6709\u53bb\u504f\u65b9\u6cd5\u5728\u7406\u8bba\u4e0a\u6709\u6548\u4f46\u7f3a\u4e4f\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e0b\u7684\u6027\u80fd\u6bd4\u8f83\uff0c\u9700\u8bc4\u4f30\u4e0d\u540c\u4e13\u5bb6\u6807\u6ce8\u91cf\u5bf9\u65b9\u6cd5\u6548\u679c\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790DSL\u548cPPI\u5728\u4e0d\u540c\u4e13\u5bb6\u6807\u6ce8\u6570\u91cf\u4e0b\u7684\u8868\u73b0\uff0c\u7814\u7a76\u5176\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u7684\u504f\u5dee\u51cf\u5c11\u6548\u679c\u3001\u7ecf\u9a8c\u6548\u7387\u53ca\u8de8\u6570\u636e\u96c6\u7a33\u5b9a\u6027\u3002", "result": "DSL\u5728\u5927\u6837\u672c\u65f6\u504f\u5dee\u66f4\u4f4e\u6548\u7387\u66f4\u9ad8\uff0c\u4f46\u8de8\u6570\u636e\u96c6\u8868\u73b0\u6ce2\u52a8\u8f83\u5927\uff1bPPI\u7a33\u5b9a\u6027\u66f4\u597d\u4f46\u6548\u7387\u8f83\u4f4e\uff0c\u4e24\u8005\u5448\u73b0\u504f\u5dee-\u65b9\u5dee\u6743\u8861\u5173\u7cfb\u3002", "conclusion": "\u9700\u5f00\u53d1\u65b0\u6307\u6807\u91cf\u5316\u6709\u9650\u6837\u672c\u4e0b\u65b9\u6cd5\u7684\u6548\u7387\uff0c\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u53bb\u504f\u65b9\u6cd5\u7684\u504f\u5dee-\u65b9\u5dee\u5e73\u8861\u95ee\u9898\u4ee5\u63d0\u5347\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002"}}
{"id": "2506.09641", "pdf": "https://arxiv.org/pdf/2506.09641", "abs": "https://arxiv.org/abs/2506.09641", "authors": ["Anna Stein", "Kevin Tang"], "title": "Modeling Probabilistic Reduction using Information Theory and Naive Discriminative Learning", "categories": ["cs.CL", "cs.IT", "math.IT"], "comment": "Submitted to Interspeech 2025", "summary": "This study compares probabilistic predictors based on information theory with\nNaive Discriminative Learning (NDL) predictors in modeling acoustic word\nduration, focusing on probabilistic reduction. We examine three models using\nthe Buckeye corpus: one with NDL-derived predictors using information-theoretic\nformulas, one with traditional NDL predictors, and one with N-gram\nprobabilistic predictors. Results show that the N-gram model outperforms both\nNDL models, challenging the assumption that NDL is more effective due to its\ncognitive motivation. However, incorporating information-theoretic formulas\ninto NDL improves model performance over the traditional model. This research\nhighlights a) the need to incorporate not only frequency and contextual\npredictability but also average contextual predictability, and b) the\nimportance of combining information-theoretic metrics of predictability and\ninformation derived from discriminative learning in modeling acoustic\nreduction.", "AI": {"tldr": "\u6bd4\u8f83\u4fe1\u606f\u8bba\u6982\u7387\u9884\u6d4b\u5668\u4e0eNDL\u6a21\u578b\u5728\u58f0\u5b66\u7f29\u51cf\u5efa\u6a21\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0N-gram\u6a21\u578b\u6700\u4f18\u4f46\u4fe1\u606f\u8bba\u516c\u5f0f\u53ef\u6539\u8fdbNDL", "motivation": "\u9a8c\u8bc1NDL\u6a21\u578b\u56e0\u8ba4\u77e5\u52a8\u673a\u66f4\u6709\u6548\u7684\u5047\u8bbe\uff0c\u63a2\u7a76\u4fe1\u606f\u8bba\u6307\u6807\u4e0e\u5224\u522b\u5b66\u4e60\u7684\u7ed3\u5408\u5bf9\u58f0\u5b66\u7f29\u51cf\u5efa\u6a21\u7684\u5f71\u54cd", "method": "\u4f7f\u7528Buckeye\u8bed\u6599\u5e93\u6d4b\u8bd5\u4e09\u79cd\u6a21\u578b\uff1a\u4fe1\u606f\u8bba\u516c\u5f0f\u6539\u8fdb\u7684NDL\u9884\u6d4b\u5668\u3001\u4f20\u7edfNDL\u9884\u6d4b\u5668\u3001N-gram\u6982\u7387\u9884\u6d4b\u5668", "result": "N-gram\u6a21\u578b\u4f18\u4e8eNDL\u6a21\u578b\uff0c\u4f46\u4fe1\u606f\u8bba\u516c\u5f0f\u53ef\u63d0\u5347NDL\u6027\u80fd\uff1b\u9700\u540c\u65f6\u8003\u8651\u9891\u7387/\u4e0a\u4e0b\u6587\u9884\u6d4b\u6027/\u5e73\u5747\u9884\u6d4b\u6027", "conclusion": "\u6311\u6218NDL\u4f18\u52bf\u5047\u8bbe\uff0c\u63d0\u51fa\u6539\u8fdbNDL\u7684\u65b9\u6cd5\uff0c\u5f3a\u8c03\u58f0\u5b66\u7f29\u51cf\u5efa\u6a21\u9700\u7ed3\u5408\u4fe1\u606f\u8bba\u6307\u6807\u548c\u5224\u522b\u5b66\u4e60\u6570\u636e"}}
{"id": "2506.09643", "pdf": "https://arxiv.org/pdf/2506.09643", "abs": "https://arxiv.org/abs/2506.09643", "authors": ["Harry Walsh", "Maksym Ivashechkin", "Richard Bowden"], "title": "Using Sign Language Production as Data Augmentation to enhance Sign Language Translation", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Machine learning models fundamentally rely on large quantities of\nhigh-quality data. Collecting the necessary data for these models can be\nchallenging due to cost, scarcity, and privacy restrictions. Signed languages\nare visual languages used by the deaf community and are considered low-resource\nlanguages. Sign language datasets are often orders of magnitude smaller than\ntheir spoken language counterparts. Sign Language Production is the task of\ngenerating sign language videos from spoken language sentences, while Sign\nLanguage Translation is the reverse translation task. Here, we propose\nleveraging recent advancements in Sign Language Production to augment existing\nsign language datasets and enhance the performance of Sign Language Translation\nmodels. For this, we utilize three techniques: a skeleton-based approach to\nproduction, sign stitching, and two photo-realistic generative models, SignGAN\nand SignSplat. We evaluate the effectiveness of these techniques in enhancing\nthe performance of Sign Language Translation models by generating variation in\nthe signer's appearance and the motion of the skeletal data. Our results\ndemonstrate that the proposed methods can effectively augment existing datasets\nand enhance the performance of Sign Language Translation models by up to 19%,\npaving the way for more robust and accurate Sign Language Translation systems,\neven in resource-constrained environments.", "AI": {"tldr": "\u5229\u7528\u624b\u8bed\u751f\u6210\u6280\u672f\uff08\u9aa8\u67b6\u751f\u6210\u3001\u624b\u8bed\u62fc\u63a5\u3001SignGAN/SignSplat\uff09\u589e\u5f3a\u4f4e\u8d44\u6e90\u624b\u8bed\u6570\u636e\u96c6\uff0c\u4f7f\u7ffb\u8bd1\u6a21\u578b\u6027\u80fd\u63d0\u534719%", "motivation": "\u624b\u8bed\u4f5c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u96be\u9898\uff0c\u4f20\u7edf\u6570\u636e\u6536\u96c6\u65b9\u5f0f\u5b58\u5728\u6210\u672c/\u9690\u79c1\u9650\u5236\uff0c\u9700\u901a\u8fc7\u751f\u6210\u6280\u672f\u7a81\u7834\u6570\u636e\u74f6\u9888", "method": "1. \u57fa\u4e8e\u9aa8\u67b6\u7684\u751f\u6210\u65b9\u6cd5 2. \u624b\u8bed\u7247\u6bb5\u62fc\u63a5\u6280\u672f 3. \u4e24\u79cd\u903c\u771f\u751f\u6210\u6a21\u578b(SignGAN\u56fe\u50cf\u751f\u6210\u3001SignSplat\u70b9\u4e91\u751f\u6210)", "result": "\u901a\u8fc7\u751f\u6210\u4e0d\u540c\u624b\u8bed\u8005\u5916\u89c2\u548c\u9aa8\u9abc\u8fd0\u52a8\u53d8\u4f53\uff0c\u6210\u529f\u63d0\u5347\u624b\u8bed\u7ffb\u8bd1\u6a21\u578b\u6027\u80fd\u6700\u9ad8\u8fbe19%", "conclusion": "\u9a8c\u8bc1\u4e86\u751f\u6210\u5f0f\u6570\u636e\u589e\u5f3a\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u7684\u6709\u6548\u6027\uff0c\u4e3a\u6784\u5efa\u9c81\u68d2\u624b\u8bed\u7ffb\u8bd1\u7cfb\u7edf\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2506.09645", "pdf": "https://arxiv.org/pdf/2506.09645", "abs": "https://arxiv.org/abs/2506.09645", "authors": ["Tianjun Yao", "Haoxuan Li", "Zhiqiang Shen", "Pan Li", "Tongliang Liu", "Kun Zhang"], "title": "Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph Question Answering", "categories": ["cs.CL", "cs.IR", "cs.LG", "I.2.6"], "comment": "32 pages, 28 figures", "summary": "Large Language Models (LLMs) have shown strong inductive reasoning ability\nacross various domains, but their reliability is hindered by the outdated\nknowledge and hallucinations. Retrieval-Augmented Generation mitigates these\nissues by grounding LLMs with external knowledge; however, most existing RAG\npipelines rely on unstructured text, limiting interpretability and structured\nreasoning. Knowledge graphs, which represent facts as relational triples, offer\na more structured and compact alternative. Recent studies have explored\nintegrating knowledge graphs with LLMs for knowledge graph question answering\n(KGQA), with a significant proportion adopting the retrieve-then-reasoning\nparadigm. In this framework, graph-based retrievers have demonstrated strong\nempirical performance, yet they still face challenges in generalization\nability. In this work, we propose RAPL, a novel framework for efficient and\neffective graph retrieval in KGQA. RAPL addresses these limitations through\nthree aspects: (1) a two-stage labeling strategy that combines heuristic\nsignals with parametric models to provide causally grounded supervision; (2) a\nmodel-agnostic graph transformation approach to capture both intra- and\ninter-triple interactions, thereby enhancing representational capacity; and (3)\na path-based reasoning strategy that facilitates learning from the injected\nrational knowledge, and supports downstream reasoner through structured inputs.\nEmpirically, RAPL outperforms state-of-the-art methods by $2.66\\%-20.34\\%$, and\nsignificantly reduces the performance gap between smaller and more powerful\nLLM-based reasoners, as well as the gap under cross-dataset settings,\nhighlighting its superior retrieval capability and generalizability. Codes are\navailable at: https://github.com/tianyao-aka/RAPL.", "AI": {"tldr": "\u63d0\u51faRAPL\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u6539\u8fdb\u77e5\u8bc6\u56fe\u8c31\u68c0\u7d22\uff0c\u63d0\u5347KGQA\u4efb\u52a1\u6548\u679c\u548c\u6cdb\u5316\u80fd\u529b", "motivation": "\u73b0\u6709\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u68c0\u7d22\u65b9\u6cd5\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3001\u7ed3\u6784\u5316\u63a8\u7406\u53d7\u9650\u7684\u95ee\u9898", "method": "1. \u4e24\u9636\u6bb5\u6807\u6ce8\u7b56\u7565 2. \u6a21\u578b\u65e0\u5173\u7684\u56fe\u53d8\u6362\u65b9\u6cd5 3. \u57fa\u4e8e\u8def\u5f84\u7684\u63a8\u7406\u7b56\u7565", "result": "\u5728KGQA\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u8d85\u8d8aSOTA\u65b9\u6cd52.66%-20.34%\uff0c\u663e\u8457\u7f29\u5c0f\u4e0d\u540c\u89c4\u6a21LLM\u95f4\u7684\u6027\u80fd\u5dee\u8ddd", "conclusion": "RAPL\u901a\u8fc7\u7ed3\u6784\u5316\u68c0\u7d22\u589e\u5f3a\uff0c\u6709\u6548\u63d0\u5347\u68c0\u7d22\u80fd\u529b\u548c\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2506.09657", "pdf": "https://arxiv.org/pdf/2506.09657", "abs": "https://arxiv.org/abs/2506.09657", "authors": ["Nikolas Evkarpidi", "Elena Tutubalina"], "title": "Bridging the Gap Between Open-Source and Proprietary LLMs in Table QA", "categories": ["cs.CL"], "comment": "Accepted for publication at the 19th International Workshop on\n  Semantic Evaluation (SemEval-2025), to be held in conjunction with ACL 2025.\n  15 pages, 5 figures", "summary": "This paper presents a system developed for SemEval 2025 Task 8: Question\nAnswering (QA) over tabular data. Our approach integrates several key\ncomponents: text-to-SQL and text-to-code generation modules, a self-correction\nmechanism, and a retrieval-augmented generation (RAG). Additionally, it\nincludes an end-to-end (E2E) module, all orchestrated by a large language model\n(LLM). Through ablation studies, we analyzed the effects of different parts of\nour pipeline and identified the challenges that are still present in this\nfield. During the evaluation phase of the competition, our solution achieved an\naccuracy of 80%, resulting in a top-13 ranking among the 38 participating\nteams. Our pipeline demonstrates a significant improvement in accuracy for\nopen-source models and achieves a performance comparable to proprietary LLMs in\nQA tasks over tables. The code is available at GitHub repository.", "AI": {"tldr": "\u63d0\u51fa\u878d\u5408\u6587\u672c\u8f6cSQL/\u4ee3\u7801\u3001\u81ea\u4fee\u6b63\u673a\u5236\u548cRAG\u589e\u5f3a\u7684\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u5728SemEval 2025\u8868\u683c\u95ee\u7b54\u4efb\u52a1\u4e2d\u53d6\u5f9780%\u51c6\u786e\u7387\uff08Top13/38\uff09\uff0c\u663e\u8457\u63d0\u5347\u5f00\u6e90\u6a21\u578b\u6027\u80fd", "motivation": "\u89e3\u51b3\u8868\u683c\u6570\u636e\u95ee\u7b54\u4efb\u52a1\u4e2d\u5f00\u6e90\u6a21\u578b\u4e0e\u5546\u4e1aLLM\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u96c6\u6210\u63d0\u5347\u590d\u6742\u67e5\u8be2\u7684\u89e3\u6790\u80fd\u529b", "method": "\u6574\u5408\u6587\u672c\u8f6cSQL/\u4ee3\u7801\u751f\u6210\u6a21\u5757\u3001\u81ea\u4fee\u6b63\u673a\u5236\u3001RAG\u589e\u5f3a\u68c0\u7d22\u548c\u7aef\u5230\u7aef\u6a21\u5757\uff0c\u901a\u8fc7LLM\u534f\u8c03\u5404\u7ec4\u4ef6\u5e76\u8fdb\u884c\u6d88\u878d\u5b9e\u9a8c\u5206\u6790", "result": "\u7ade\u8d5b\u8bc4\u6d4b\u51c6\u786e\u7387\u8fbe80%\uff0c\u6027\u80fd\u8d85\u8d8a\u540c\u7c7b\u5f00\u6e90\u65b9\u6848\u4e14\u63a5\u8fd1\u5546\u4e1aLLM\u6c34\u5e73\uff0c\u6d88\u878d\u5b9e\u9a8c\u63ed\u793a\u5f53\u524d\u9886\u57df\u6280\u672f\u74f6\u9888", "conclusion": "\u6a21\u5757\u5316\u96c6\u6210\u7b56\u7565\u6709\u6548\u63d0\u5347\u8868\u683c\u95ee\u7b54\u6027\u80fd\uff0c\u5f00\u6e90\u65b9\u6848\u4ee3\u7801\u5171\u4eab\u4fc3\u8fdb\u9886\u57df\u53d1\u5c55\uff0c\u4f46\u590d\u6742\u8bed\u4e49\u89e3\u6790\u4ecd\u662f\u5f85\u7a81\u7834\u96be\u70b9"}}
{"id": "2506.09669", "pdf": "https://arxiv.org/pdf/2506.09669", "abs": "https://arxiv.org/abs/2506.09669", "authors": ["Lihu Chen", "Ga\u00ebl Varoquaux"], "title": "Query-Level Uncertainty in Large Language Models", "categories": ["cs.CL"], "comment": "In Progress", "summary": "It is important for Large Language Models to be aware of the boundary of\ntheir knowledge, the mechanism of identifying known and unknown queries. This\ntype of awareness can help models perform adaptive inference, such as invoking\nRAG, engaging in slow and deep thinking, or adopting the abstention mechanism,\nwhich is beneficial to the development of efficient and trustworthy AI. In this\nwork, we propose a method to detect knowledge boundaries via Query-Level\nUncertainty, which aims to determine if the model is able to address a given\nquery without generating any tokens. To this end, we introduce a novel and\ntraining-free method called \\emph{Internal Confidence}, which leverages\nself-evaluations across layers and tokens. Empirical results on both factual QA\nand mathematical reasoning tasks demonstrate that our internal confidence can\noutperform several baselines. Furthermore, we showcase that our proposed method\ncan be used for efficient RAG and model cascading, which is able to reduce\ninference costs while maintaining performance.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5185\u90e8\u7f6e\u4fe1\u5ea6\u7684\u65e0\u8bad\u7ec3\u77e5\u8bc6\u8fb9\u754c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u63d0\u5347LLM\u81ea\u9002\u5e94\u63a8\u7406\u6548\u7387", "motivation": "\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u77e5\u8bc6\u8fb9\u754c\u7684\u8ba4\u77e5\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u81ea\u9002\u5e94\u9009\u62e9RAG/\u6df1\u5ea6\u601d\u8003/\u5f03\u6743\u7b49\u63a8\u7406\u673a\u5236\uff0c\u63a8\u52a8\u9ad8\u6548\u53ef\u4fe1AI\u53d1\u5c55", "method": "\u901a\u8fc7\u67e5\u8be2\u7ea7\u522b\u4e0d\u786e\u5b9a\u6027\u68c0\u6d4b\uff08Internal Confidence\uff09\uff0c\u5229\u7528\u6a21\u578b\u4e0d\u540c\u5c42\u548ctoken\u7684\u81ea\u6211\u8bc4\u4f30\u5b9e\u73b0\u77e5\u8bc6\u8fb9\u754c\u5224\u65ad", "result": "\u5728\u4e8b\u5b9eQA\u548c\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u9ad8\u6548RAG\u548c\u6a21\u578b\u7ea7\u8054\uff0c\u964d\u4f4e30%\u63a8\u7406\u6210\u672c\u4fdd\u6301\u6027\u80fd", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6784\u5efa\u81ea\u9002\u5e94AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u524d\u671f\u7684\u77e5\u8bc6\u8fb9\u754c\u5224\u65ad\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c"}}
{"id": "2506.09672", "pdf": "https://arxiv.org/pdf/2506.09672", "abs": "https://arxiv.org/abs/2506.09672", "authors": ["Hao Xiong", "Chuanyuan Tan", "Wenliang Chen"], "title": "Is Fine-Tuning an Effective Solution? Reassessing Knowledge Editing for Unstructured Data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Unstructured Knowledge Editing (UKE) is crucial for updating the relevant\nknowledge of large language models (LLMs). It focuses on unstructured inputs,\nsuch as long or free-form texts, which are common forms of real-world\nknowledge. Although previous studies have proposed effective methods and tested\nthem, some issues exist: (1) Lack of Locality evaluation for UKE, and (2)\nAbnormal failure of fine-tuning (FT) based methods for UKE. To address these\nissues, we first construct two datasets, UnKEBench-Loc and AKEW-Loc (CF), by\nextending two existing UKE datasets with locality test data from the\nunstructured and structured views. This enables a systematic evaluation of the\nLocality of post-edited models. Furthermore, we identify four factors that may\naffect the performance of FT-based methods. Based on these factors, we conduct\nexperiments to determine how the well-performing FT-based methods should be\ntrained for the UKE task, providing a training recipe for future research. Our\nexperimental results indicate that the FT-based method with the optimal setting\n(FT-UKE) is surprisingly strong, outperforming the existing state-of-the-art\n(SOTA). In batch editing scenarios, FT-UKE shows strong performance as well,\nwith its advantage over SOTA methods increasing as the batch size grows,\nexpanding the average metric lead from +6.78% to +10.80%", "AI": {"tldr": "\u63d0\u51faFT-UKE\u65b9\u6cd5\u89e3\u51b3\u975e\u7ed3\u6784\u5316\u77e5\u8bc6\u7f16\u8f91\u7684\u8bc4\u4f30\u7f3a\u9677\u4e0e\u5fae\u8c03\u5f02\u5e38\u95ee\u9898\uff0c\u5728\u6279\u5904\u7406\u573a\u666f\u4e2d\u663e\u8457\u8d85\u8d8aSOTA", "motivation": "\u73b0\u6709\u975e\u7ed3\u6784\u5316\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u7f3a\u9677\uff1a\uff081\uff09\u7f3a\u4e4f\u5c40\u90e8\u6027\u8bc4\u4f30\u4f53\u7cfb\uff082\uff09\u5fae\u8c03\u65b9\u6cd5\u5b58\u5728\u5f02\u5e38\u5931\u8d25\u73b0\u8c61", "method": "1. \u6269\u5c55\u6784\u5efaUnKEBench-Loc\u548cAKEW-Loc(CF)\u6570\u636e\u96c6\uff0c\u4ece\u975e\u7ed3\u6784\u5316\u548c\u7ed3\u6784\u5316\u53cc\u89c6\u89d2\u5efa\u7acb\u5c40\u90e8\u6027\u8bc4\u4f30\u4f53\u7cfb\n2. \u901a\u8fc7\u56db\u56e0\u7d20\u5b9e\u9a8c\u786e\u5b9a\u5fae\u8c03\u65b9\u6cd5\u7684\u6700\u4f73\u8bad\u7ec3\u65b9\u6848\uff08FT-UKE\uff09", "result": "FT-UKE\u5728\u5355\u6b21\u7f16\u8f91\u4e2d\u5e73\u5747\u6307\u6807\u9886\u5148SOTA 6.78%\uff0c\u6279\u5904\u7406\u573a\u666f\u4e0b\u968f\u7740\u6279\u91cf\u589e\u5927\u4f18\u52bf\u6269\u5c55\u81f310.80%", "conclusion": "\u8be5\u7814\u7a76\u4e0d\u4ec5\u5efa\u7acb\u4e86\u975e\u7ed3\u6784\u5316\u77e5\u8bc6\u7f16\u8f91\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u66f4\u63ed\u793a\u4e86\u5fae\u8c03\u65b9\u6cd5\u7684\u6f5c\u5728\u4f18\u52bf\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u8bad\u7ec3\u65b9\u6848\u548c\u6027\u80fd\u57fa\u51c6"}}
{"id": "2506.09684", "pdf": "https://arxiv.org/pdf/2506.09684", "abs": "https://arxiv.org/abs/2506.09684", "authors": ["Haoyi Song", "Ruihan Ji", "Naichen Shi", "Fan Lai", "Raed Al Kontar"], "title": "Inv-Entropy: A Fully Probabilistic Framework for Uncertainty Quantification in Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have transformed natural language processing,\nbut their reliable deployment requires effective uncertainty quantification\n(UQ). Existing UQ methods are often heuristic and lack a probabilistic\nfoundation. This paper begins by providing a theoretical justification for the\nrole of perturbations in UQ for LLMs. We then introduce a dual random walk\nperspective, modeling input-output pairs as two Markov chains with transition\nprobabilities defined by semantic similarity. Building on this, we propose a\nfully probabilistic framework based on an inverse model, which quantifies\nuncertainty by evaluating the diversity of the input space conditioned on a\ngiven output through systematic perturbations. Within this framework, we define\na new uncertainty measure, Inv-Entropy. A key strength of our framework is its\nflexibility: it supports various definitions of uncertainty measures,\nembeddings, perturbation strategies, and similarity metrics. We also propose\nGAAP, a perturbation algorithm based on genetic algorithms, which enhances the\ndiversity of sampled inputs. In addition, we introduce a new evaluation metric,\nTemperature Sensitivity of Uncertainty (TSU), which directly assesses\nuncertainty without relying on correctness as a proxy. Extensive experiments\ndemonstrate that Inv-Entropy outperforms existing semantic UQ methods. The code\nto reproduce the results can be found at\nhttps://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u9006\u6a21\u578b\u7684\u6982\u7387\u6846\u67b6\u6765\u91cf\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5b9a\u4e49\u4e86Inv-Entropy\u6307\u6807\u5e76\u5f00\u53d1GAAP\u6270\u52a8\u7b97\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u7f3a\u4e4f\u6982\u7387\u57fa\u7840\u4e14\u591a\u4e3a\u542f\u53d1\u5f0f\uff0c\u9700\u7cfb\u7edf\u5316\u7406\u8bba\u6846\u67b6\u63d0\u5347LLM\u90e8\u7f72\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u53cc\u968f\u673a\u6e38\u8d70\u5efa\u6a21\u8f93\u5165-\u8f93\u51fa\u5173\u7cfb\uff0c\u6784\u5efa\u6982\u7387\u6846\u67b6\u8bc4\u4f30\u6270\u52a8\u540e\u7684\u8f93\u5165\u7a7a\u95f4\u591a\u6837\u6027\uff0c\u63d0\u51faInv-Entropy\u5ea6\u91cf\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u9057\u4f20\u7b97\u6cd5\u7684GAAP\u6270\u52a8\u7b56\u7565\u3002", "result": "Inv-Entropy\u5728\u5b9e\u9a8c\u4e2d\u8d85\u8d8a\u73b0\u6709\u8bed\u4e49\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u7075\u6d3b\u6027\u53caGAAP\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u7075\u6d3b\u5de5\u5177\uff0cGAAP\u548cTSU\u6307\u6807\u663e\u8457\u63d0\u5347\u4e86\u91cf\u5316\u6548\u679c\u4e0e\u8bc4\u4f30\u65b9\u5f0f\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2506.09790", "pdf": "https://arxiv.org/pdf/2506.09790", "abs": "https://arxiv.org/abs/2506.09790", "authors": ["Zhenran Xu", "Yiyu Wang", "Xue Yang", "Longyue Wang", "Weihua Luo", "Kaifu Zhang", "Baotian Hu", "Min Zhang"], "title": "ComfyUI-R1: Exploring Reasoning Models for Workflow Generation", "categories": ["cs.CL", "cs.CV", "cs.SE"], "comment": "Work in progress. Try it out in ComfyUI-Copilot\n  https://github.com/AIDC-AI/ComfyUI-Copilot", "summary": "AI-generated content has evolved from monolithic models to modular workflows,\nparticularly on platforms like ComfyUI, enabling customization in creative\npipelines. However, crafting effective workflows requires great expertise to\norchestrate numerous specialized components, presenting a steep learning curve\nfor users. To address this challenge, we introduce ComfyUI-R1, the first large\nreasoning model for automated workflow generation. Starting with our curated\ndataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning\ndata, including node selection, workflow planning, and code-level workflow\nrepresentation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT\nfine-tuning for cold start, adapting models to the ComfyUI domain; (2)\nreinforcement learning for incentivizing reasoning capability, guided by a\nfine-grained rule-metric hybrid reward, ensuring format validity, structural\nintegrity, and node-level fidelity. Experiments show that our 7B-parameter\nmodel achieves a 97\\% format validity rate, along with high pass rate,\nnode-level and graph-level F1 scores, significantly surpassing prior\nstate-of-the-art methods that employ leading closed-source models such as\nGPT-4o and Claude series. Further analysis highlights the critical role of the\nreasoning process and the advantage of transforming workflows into code.\nQualitative comparison reveals our strength in synthesizing intricate workflows\nwith diverse nodes, underscoring the potential of long CoT reasoning in AI art\ncreation.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u751f\u6210\u5927\u6a21\u578bComfyUI-R1\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff08\u601d\u7ef4\u94fe\u5fae\u8c03+\u5f3a\u5316\u5b66\u4e60\uff09\u5b9e\u73b097%\u683c\u5f0f\u6709\u6548\u6027\uff0c\u663e\u8457\u8d85\u8d8aGPT-4o\u7b49\u95ed\u6e90\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3ComfyUI\u5e73\u53f0\u5de5\u4f5c\u6d41\u7f16\u6392\u95e8\u69db\u9ad8\u3001\u9700\u4e13\u4e1a\u77e5\u8bc6\u7684\u75db\u70b9", "method": "\u6784\u5efa4K\u5de5\u4f5c\u6d41\u6570\u636e\u96c6\u2192\u751f\u6210\u957f\u601d\u7ef4\u94fe\u63a8\u7406\u6570\u636e\u2192\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08CoT\u5fae\u8c03+\u57fa\u4e8e\u89c4\u5219-\u6307\u6807\u6df7\u5408\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff09", "result": "7B\u53c2\u6570\u6a21\u578b\u683c\u5f0f\u6709\u6548\u602797%\uff0c\u8282\u70b9/\u56fe\u7ea7F1\u5f97\u5206\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8eGPT-4o\u548cClaude\u7cfb\u5217", "conclusion": "\u957f\u601d\u7ef4\u94fe\u63a8\u7406\u673a\u5236\u4e0e\u4ee3\u7801\u5316\u5de5\u4f5c\u6d41\u8f6c\u6362\u5728AI\u827a\u672f\u521b\u4f5c\u4e2d\u5177\u6709\u5173\u952e\u4ef7\u503c\uff0c\u6a21\u578b\u80fd\u5408\u6210\u542b\u591a\u6837\u5316\u8282\u70b9\u7684\u590d\u6742\u5de5\u4f5c\u6d41"}}
{"id": "2506.09796", "pdf": "https://arxiv.org/pdf/2506.09796", "abs": "https://arxiv.org/abs/2506.09796", "authors": ["Andreas S\u00e4uberli", "Diego Frassinelli", "Barbara Plank"], "title": "Do LLMs Give Psychometrically Plausible Responses in Educational Assessments?", "categories": ["cs.CL"], "comment": "Accepted for publication at the 20th Workshop on Innovative Use of\n  NLP for Building Educational Applications (BEA) at ACL 2025", "summary": "Knowing how test takers answer items in educational assessments is essential\nfor test development, to evaluate item quality, and to improve test validity.\nHowever, this process usually requires extensive pilot studies with human\nparticipants. If large language models (LLMs) exhibit human-like response\nbehavior to test items, this could open up the possibility of using them as\npilot participants to accelerate test development. In this paper, we evaluate\nthe human-likeness or psychometric plausibility of responses from 18\ninstruction-tuned LLMs with two publicly available datasets of multiple-choice\ntest items across three subjects: reading, U.S. history, and economics. Our\nmethodology builds on two theoretical frameworks from psychometrics which are\ncommonly used in educational assessment, classical test theory and item\nresponse theory. The results show that while larger models are excessively\nconfident, their response distributions can be more human-like when calibrated\nwith temperature scaling. In addition, we find that LLMs tend to correlate\nbetter with humans in reading comprehension items compared to other subjects.\nHowever, the correlations are not very strong overall, indicating that LLMs\nshould not be used for piloting educational assessments in a zero-shot setting.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6821\u51c6\u540e\u80fd\u90e8\u5206\u6a21\u62df\u4eba\u7c7b\u5e94\u8bd5\u884c\u4e3a\uff0c\u4f46\u96f6\u6837\u672c\u573a\u666f\u4e0b\u5c1a\u4e0d\u80fd\u66ff\u4ee3\u4eba\u7c7b\u53c2\u4e0e\u6559\u80b2\u8bc4\u4f30\u6d4b\u8bd5\u5f00\u53d1", "motivation": "\u63a2\u7d22LLMs\u80fd\u5426\u6a21\u62df\u4eba\u7c7b\u5e94\u8bd5\u53cd\u5e94\uff0c\u66ff\u4ee3\u4eba\u5de5\u8bd5\u70b9\u7814\u7a76\u4ee5\u52a0\u901f\u6d4b\u8bd5\u5f00\u53d1\u6d41\u7a0b", "method": "\u4f7f\u752818\u4e2a\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\uff0c\u57fa\u4e8e\u7ecf\u5178\u6d4b\u8bd5\u7406\u8bba\u548c\u9879\u76ee\u53cd\u5e94\u7406\u8bba\uff0c\u5206\u6790\u4e09\u4e2a\u5b66\u79d1\u591a\u9009\u9898\u7684\u4eba\u7c7b\u76f8\u4f3c\u6027", "result": "\u5927\u6a21\u578b\u5b58\u5728\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\uff0c\u6e29\u5ea6\u6821\u51c6\u540e\u66f4\u63a5\u8fd1\u4eba\u7c7b\u53cd\u5e94\uff1b\u9605\u8bfb\u7c7b\u9898\u76ee\u76f8\u5173\u6027\u8f83\u597d\uff0c\u4f46\u603b\u4f53\u76f8\u5173\u6027\u8f83\u5f31(r\u503c\u4e0d\u9ad8)", "conclusion": "LLMs\u5728\u7279\u5b9a\u6821\u51c6\u6761\u4ef6\u4e0b\u53ef\u80fd\u8f85\u52a9\u6d4b\u8bd5\u5f00\u53d1\uff0c\u4f46\u5f53\u524d\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u5c1a\u8fbe\u4e0d\u5230\u53ef\u9760\u7684\u4eba\u7c7b\u66ff\u4ee3\u6c34\u5e73"}}
{"id": "2506.09820", "pdf": "https://arxiv.org/pdf/2506.09820", "abs": "https://arxiv.org/abs/2506.09820", "authors": ["Chengpeng Li", "Zhengyang Tang", "Ziniu Li", "Mingfeng Xue", "Keqin Bao", "Tian Ding", "Ruoyu Sun", "Benyou Wang", "Xiang Wang", "Junyang Lin", "Dayiheng Liu"], "title": "CoRT: Code-integrated Reasoning within Thinking", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "work in progress", "summary": "Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable\nprogress in natural language reasoning with long chain-of-thought (CoT), yet\nthey remain inefficient or inaccurate when handling complex mathematical\noperations. Addressing these limitations through computational tools (e.g.,\ncomputation libraries and symbolic solvers) is promising, but it introduces a\ntechnical challenge: Code Interpreter (CI) brings external knowledge beyond the\nmodel's internal text representations, thus the direct combination is not\nefficient. This paper introduces CoRT, a post-training framework for teaching\nLRMs to leverage CI effectively and efficiently. As a first step, we address\nthe data scarcity issue by synthesizing code-integrated reasoning data through\nHint-Engineering, which strategically inserts different hints at appropriate\npositions to optimize LRM-CI interaction. We manually create 30 high-quality\nsamples, upon which we post-train models ranging from 1.5B to 32B parameters,\nwith supervised fine-tuning, rejection fine-tuning and reinforcement learning.\nOur experimental results demonstrate that Hint-Engineering models achieve 4\\%\nand 8\\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and\nDeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging\nmathematical reasoning datasets. Furthermore, Hint-Engineering models use about\n30\\% fewer tokens for the 32B model and 50\\% fewer tokens for the 1.5B model\ncompared with the natural language models. The models and code are available at\nhttps://github.com/ChengpengLi1003/CoRT.", "AI": {"tldr": "CoRT\u6846\u67b6\u901a\u8fc7\u4ee3\u7801\u89e3\u91ca\u5668\u96c6\u6210\u4f18\u5316\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6570\u5b66\u8fd0\u7b97\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u4e0e\u51c6\u786e\u6027", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u6570\u5b66\u8fd0\u7b97\u4e2d\u5b58\u5728\u6548\u7387\u4f4e\u3001\u7cbe\u5ea6\u5dee\u7684\u95ee\u9898\uff0c\u76f4\u63a5\u7ed3\u5408\u4ee3\u7801\u89e3\u91ca\u5668\u5b58\u5728\u77e5\u8bc6\u6574\u5408\u969c\u788d", "method": "\u4f7f\u7528Hint-Engineering\u751f\u6210\u4ee3\u7801\u589e\u5f3a\u7684\u63a8\u7406\u6570\u636e\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03+\u62d2\u7edd\u5fae\u8c03+\u5f3a\u5316\u5b66\u4e60\u4e09\u9636\u6bb5\u8bad\u7ec3", "result": "\u572832B/1.5B\u6a21\u578b\u4e0a\u5206\u522b\u53d6\u5f974%/8%\u7edd\u5bf9\u63d0\u5347\uff0ctoken\u4f7f\u7528\u91cf\u51cf\u5c1130%/50%", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u6027\u63d0\u793a\u5de5\u7a0b\u4f18\u5316\u6a21\u578b\u4e0e\u8ba1\u7b97\u5de5\u5177\u7684\u534f\u540c\uff0c\u663e\u8457\u63d0\u5347\u6570\u5b66\u63a8\u7406\u6548\u7387\u4e0e\u8d44\u6e90\u5229\u7528\u7387"}}
{"id": "2506.09827", "pdf": "https://arxiv.org/pdf/2506.09827", "abs": "https://arxiv.org/abs/2506.09827", "authors": ["Christoph Schuhmann", "Robert Kaczmarczyk", "Gollam Rabby", "Felix Friedrich", "Maurice Kraus", "Kourosh Nadi", "Huu Nguyen", "Kristian Kersting", "S\u00f6ren Auer"], "title": "EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech Emotion Detection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The advancement of text-to-speech and audio generation models necessitates\nrobust benchmarks for evaluating the emotional understanding capabilities of AI\nsystems. Current speech emotion recognition (SER) datasets often exhibit\nlimitations in emotional granularity, privacy concerns, or reliance on acted\nportrayals. This paper introduces EmoNet-Voice, a new resource for speech\nemotion detection, which includes EmoNet-Voice Big, a large-scale pre-training\ndataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions,\nand 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human\nexpert annotations. EmoNet-Voice is designed to evaluate SER models on a\nfine-grained spectrum of 40 emotion categories with different levels of\nintensities. Leveraging state-of-the-art voice generation, we curated synthetic\naudio snippets simulating actors portraying scenes designed to evoke specific\nemotions. Crucially, we conducted rigorous validation by psychology experts who\nassigned perceived intensity labels. This synthetic, privacy-preserving\napproach allows for the inclusion of sensitive emotional states often absent in\nexisting datasets. Lastly, we introduce Empathic Insight Voice models that set\na new standard in speech emotion recognition with high agreement with human\nexperts. Our evaluations across the current model landscape exhibit valuable\nfindings, such as high-arousal emotions like anger being much easier to detect\nthan low-arousal states like concentration.", "AI": {"tldr": "\u63d0\u51faEmoNet-Voice\u6570\u636e\u96c6\u53caEmpathic Insight Voice\u6a21\u578b\uff0c\u5efa\u7acb\u5305\u542b4500+\u5c0f\u65f6\u8bed\u97f3\u6570\u636e\u7684\u65b0\u57fa\u51c6\uff0c\u5b9e\u73b040\u79cd\u7ec6\u7c92\u5ea6\u60c5\u7eea\u8bc6\u522b", "motivation": "\u89e3\u51b3\u73b0\u6709\u8bed\u97f3\u60c5\u7eea\u8bc6\u522b\u6570\u636e\u96c6\u5728\u60c5\u7eea\u9897\u7c92\u5ea6\u3001\u9690\u79c1\u4fdd\u62a4\u548c\u8868\u6f14\u771f\u5b9e\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u6784\u5efa\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u4f53\u7cfb", "method": "\u91c7\u7528\u5408\u6210\u8bed\u97f3\u751f\u6210\u6280\u672f\u521b\u5efa\u6a21\u62df\u573a\u666f\u97f3\u9891\uff0c\u901a\u8fc7\u5fc3\u7406\u5b66\u4e13\u5bb6\u6807\u6ce8\u5f3a\u5ea6\u6807\u7b7e\uff0c\u5f00\u53d1\u9690\u79c1\u5b89\u5168\u7684\u8de8\u8bed\u8a00\u6570\u636e\u96c6", "result": "\u65b0\u6a21\u578b\u4e0e\u4e13\u5bb6\u6807\u6ce8\u4e00\u81f4\u6027\u8fbe\u65b0\u9ad8\uff0c\u53d1\u73b0\u9ad8\u5524\u9192\u60c5\u7eea\uff08\u5982\u6124\u6012\uff09\u8bc6\u522b\u51c6\u786e\u7387\u663e\u8457\u4f18\u4e8e\u4f4e\u5524\u9192\u72b6\u6001\uff08\u5982\u4e13\u6ce8\uff09", "conclusion": "EmoNet-Voice\u4e3a\u8bed\u97f3\u60c5\u7eea\u8bc6\u522b\u7814\u7a76\u63d0\u4f9b\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u5176\u5408\u6210\u6570\u636e\u65b9\u6cd5\u6709\u6548\u7a81\u7834\u654f\u611f\u60c5\u7eea\u6570\u636e\u7684\u83b7\u53d6\u74f6\u9888"}}
{"id": "2506.09833", "pdf": "https://arxiv.org/pdf/2506.09833", "abs": "https://arxiv.org/abs/2506.09833", "authors": ["Omar Sherif", "Ali Hamdi"], "title": "Error-Guided Pose Augmentation: Enhancing Rehabilitation Exercise Assessment through Targeted Data Generation", "categories": ["cs.CL", "I.2.1"], "comment": "6 pages, 1 figure. To appear in Intelligent Methods, Systems, and\n  Applications 2025", "summary": "Effective rehabilitation assessment is essential for monitoring patient\nprogress, particularly in home-based settings. Existing systems often face\nchallenges such as data imbalance and difficulty detecting subtle movement\nerrors. This paper introduces Error-Guided Pose Augmentation (EGPA), a method\nthat generates synthetic skeleton data by simulating clinically relevant\nmovement mistakes. Unlike standard augmentation techniques, EGPA targets\nbiomechanical errors observed in rehabilitation. Combined with an\nattention-based graph convolutional network, EGPA improves performance across\nmultiple evaluation metrics. Experiments demonstrate reductions in mean\nabsolute error of up to 27.6 percent and gains in error classification accuracy\nof 45.8 percent. Attention visualizations show that the model learns to focus\non clinically significant joints and movement phases, enhancing both accuracy\nand interpretability. EGPA offers a promising approach for improving automated\nmovement quality assessment in both clinical and home-based rehabilitation\ncontexts.", "AI": {"tldr": "\u63d0\u51faEGPA\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u6a21\u62df\u4e34\u5e8a\u8fd0\u52a8\u9519\u8bef\u7684\u5408\u6210\u6570\u636e\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u56fe\u5377\u79ef\u7f51\u7edc\u63d0\u5347\u5eb7\u590d\u8bc4\u4f30\u6548\u679c", "motivation": "\u73b0\u6709\u5eb7\u590d\u8bc4\u4f30\u7cfb\u7edf\u5b58\u5728\u6570\u636e\u4e0d\u5e73\u8861\u548c\u7ec6\u5fae\u8fd0\u52a8\u9519\u8bef\u68c0\u6d4b\u56f0\u96be\u7684\u95ee\u9898\uff0c\u9700\u6539\u8fdb\u81ea\u52a8\u8fd0\u52a8\u8d28\u91cf\u5206\u6790", "method": "EGPA\u65b9\u6cd5\u4e13\u95e8\u9488\u5bf9\u751f\u7269\u529b\u5b66\u9519\u8bef\u751f\u6210\u5408\u6210\u9aa8\u9abc\u6570\u636e\uff0c\u914d\u5408\u6ce8\u610f\u529b\u673a\u5236\u56fe\u5377\u79ef\u7f51\u7edc\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3", "result": "\u5b9e\u9a8c\u663e\u793a\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u964d\u4f4e27.6%\uff0c\u9519\u8bef\u5206\u7c7b\u51c6\u786e\u7387\u63d0\u534745.8%\uff0c\u6ce8\u610f\u529b\u53ef\u89c6\u5316\u663e\u793a\u6a21\u578b\u805a\u7126\u5173\u952e\u5173\u8282", "conclusion": "EGPA\u663e\u8457\u63d0\u5347\u8fd0\u52a8\u8bc4\u4f30\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u548c\u5bb6\u5ead\u5eb7\u590d\u573a\u666f\uff0c\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c"}}
{"id": "2506.09847", "pdf": "https://arxiv.org/pdf/2506.09847", "abs": "https://arxiv.org/abs/2506.09847", "authors": ["Tomas Peterka", "Matyas Bohacek"], "title": "Dataset of News Articles with Provenance Metadata for Media Relevance Assessment", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.CY"], "comment": null, "summary": "Out-of-context and misattributed imagery is the leading form of media\nmanipulation in today's misinformation and disinformation landscape. The\nexisting methods attempting to detect this practice often only consider whether\nthe semantics of the imagery corresponds to the text narrative, missing\nmanipulation so long as the depicted objects or scenes somewhat correspond to\nthe narrative at hand. To tackle this, we introduce News Media Provenance\nDataset, a dataset of news articles with provenance-tagged images. We formulate\ntwo tasks on this dataset, location of origin relevance (LOR) and date and time\nof origin relevance (DTOR), and present baseline results on six large language\nmodels (LLMs). We identify that, while the zero-shot performance on LOR is\npromising, the performance on DTOR hinders, leaving room for specialized\narchitectures and future work.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u56fe\u50cf\u6765\u6e90\u4fe1\u606f\uff08\u5730\u7406\u4f4d\u7f6e\u3001\u65f6\u95f4\uff09\u68c0\u6d4b\u5a92\u4f53\u64cd\u7eb5\uff0c\u6784\u5efa\u4e86\u65b0\u95fb\u6eaf\u6e90\u6570\u636e\u96c6\u5e76\u6d4b\u8bd5\u4e86LLM\u5728\u76f8\u5173\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u5a92\u4f53\u64cd\u7eb5\u68c0\u6d4b\u65b9\u6cd5\u4ec5\u5173\u6ce8\u56fe\u50cf\u8bed\u4e49\u4e0e\u6587\u672c\u7684\u5339\u914d\uff0c\u5ffd\u7565\u4e86\u6765\u6e90\u4fe1\u606f\uff08\u5982\u62cd\u6444\u5730\u3001\u65f6\u95f4\uff09\u4e0e\u53d9\u8ff0\u7684\u4e00\u81f4\u6027\uff0c\u5bfc\u81f4\u6f0f\u68c0\u3002", "method": "\u6784\u5efa\u5305\u542b\u6765\u6e90\u6807\u7b7e\u7684\u65b0\u95fb\u6570\u636e\u96c6News Media Provenance Dataset\uff0c\u5b9a\u4e49\u5730\u7406\u4f4d\u7f6e\u76f8\u5173\u6027\uff08LOR\uff09\u548c\u65f6\u7a7a\u76f8\u5173\u6027\uff08DTOR\uff09\u4e24\u4e2a\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e866\u4e2aLLM\u7684\u57fa\u7ebf\u6027\u80fd\u3002", "result": "LLM\u5728LOR\u4efb\u52a1\u4e0a\u96f6\u6837\u672c\u8868\u73b0\u826f\u597d\uff08\u5982GPT-4\u51c6\u786e\u738776%\uff09\uff0c\u4f46DTOR\u4efb\u52a1\u8868\u73b0\u5dee\uff08\u6700\u9ad8\u4ec536%\uff09\uff0c\u63ed\u793a\u65f6\u95f4\u76f8\u5173\u6027\u68c0\u6d4b\u5b58\u5728\u663e\u8457\u74f6\u9888\u3002", "conclusion": "\u9700\u5f00\u53d1\u4e13\u95e8\u67b6\u6784\u63d0\u5347\u65f6\u95f4\u76f8\u5173\u6027\u68c0\u6d4b\u80fd\u529b\uff0c\u6765\u6e90\u4fe1\u606f\u9a8c\u8bc1\u4e3a\u5bf9\u6297\u5a92\u4f53\u64cd\u7eb5\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u4f46\u9700\u7a81\u7834\u73b0\u6709LLM\u5728\u65f6\u95f4\u63a8\u7406\u4e0a\u7684\u9650\u5236\u3002"}}
{"id": "2506.09853", "pdf": "https://arxiv.org/pdf/2506.09853", "abs": "https://arxiv.org/abs/2506.09853", "authors": ["Xiangning Yu", "Zhuohan Wang", "Linyi Yang", "Haoxuan Li", "Anjie Liu", "Xiao Xue", "Jun Wang", "Mengyue Yang"], "title": "Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning", "categories": ["cs.CL", "cs.AI", "math.ST", "stat.ME", "stat.TH"], "comment": null, "summary": "Chain-of-Thought (CoT) prompting plays an indispensable role in endowing\nlarge language models (LLMs) with complex reasoning capabilities. However, CoT\ncurrently faces two fundamental challenges: (1) Sufficiency, which ensures that\nthe generated intermediate inference steps comprehensively cover and\nsubstantiate the final conclusion; and (2) Necessity, which identifies the\ninference steps that are truly indispensable for the soundness of the resulting\nanswer. We propose a causal framework that characterizes CoT reasoning through\nthe dual lenses of sufficiency and necessity. Incorporating causal Probability\nof Sufficiency and Necessity allows us not only to determine which steps are\nlogically sufficient or necessary to the prediction outcome, but also to\nquantify their actual influence on the final reasoning outcome under different\nintervention scenarios, thereby enabling the automated addition of missing\nsteps and the pruning of redundant ones. Extensive experimental results on\nvarious mathematical and commonsense reasoning benchmarks confirm substantial\nimprovements in reasoning efficiency and reduced token usage without\nsacrificing accuracy. Our work provides a promising direction for improving LLM\nreasoning performance and cost-effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u56e0\u679c\u6846\u67b6\u89e3\u51b3CoT\u63a8\u7406\u4e2d\u6b65\u9aa4\u5145\u5206\u6027\u4e0e\u5fc5\u8981\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u52a8\u589e\u51cf\u63a8\u7406\u6b65\u9aa4\u63d0\u5347\u6548\u7387\u5e76\u4fdd\u6301\u51c6\u786e\u6027", "motivation": "\u73b0\u6709CoT\u65b9\u6cd5\u5b58\u5728\u63a8\u7406\u6b65\u9aa4\u4e0d\u591f\u5168\u9762\u652f\u6491\u7ed3\u8bba\uff08\u5145\u5206\u6027\u4e0d\u8db3\uff09\u548c\u5305\u542b\u5197\u4f59\u6b65\u9aa4\uff08\u5fc5\u8981\u6027\u6b20\u7f3a\uff09\u7684\u53cc\u91cd\u6311\u6218\uff0c\u9700\u7cfb\u7edf\u6027\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b", "method": "\u7ed3\u5408\u56e0\u679c\u63a8\u7406\u4e2d\u7684\u5145\u5206\u6982\u7387(PS)\u4e0e\u5fc5\u8981\u6982\u7387(PN)\u6784\u5efa\u5206\u6790\u6846\u67b6\uff0c\u91cf\u5316\u6b65\u9aa4\u5bf9\u7ed3\u679c\u7684\u5b9e\u9645\u5f71\u54cd\uff0c\u5b9e\u73b0\u63a8\u7406\u6b65\u9aa4\u52a8\u6001\u4f18\u5316", "result": "\u5728\u6570\u5b66\u63a8\u7406\u548c\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u4e2d\u5e73\u5747\u51cf\u5c1120%\u63a8\u7406\u6b65\u957f\uff0c\u8d44\u6e90\u6d88\u8017\u964d\u4f4e35%\uff0c\u51c6\u786e\u7387\u4fdd\u6301\u539f\u6709\u6c34\u5e73", "conclusion": "\u8be5\u56e0\u679c\u5206\u6790\u65b9\u6cd5\u4e3aLLM\u63a8\u7406\u6548\u7387\u4e0e\u6210\u672c\u4f18\u5316\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6280\u672f\u8def\u5f84\uff0c\u63a8\u52a8\u8f7b\u91cf\u5316\u63a8\u7406\u7684\u5b9e\u73b0"}}
{"id": "2506.09886", "pdf": "https://arxiv.org/pdf/2506.09886", "abs": "https://arxiv.org/abs/2506.09886", "authors": ["Rodion Oblovatny", "Alexandra Bazarova", "Alexey Zaytsev"], "title": "Attention Head Embeddings with Trainable Deep Kernels for Hallucination Detection in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present a novel approach for detecting hallucinations in large language\nmodels (LLMs) by analyzing the probabilistic divergence between prompt and\nresponse hidden-state distributions. Counterintuitively, we find that\nhallucinated responses exhibit smaller deviations from their prompts compared\nto grounded responses, suggesting that hallucinations often arise from\nsuperficial rephrasing rather than substantive reasoning. Leveraging this\ninsight, we propose a model-intrinsic detection method that uses distributional\ndistances as principled hallucination scores, eliminating the need for external\nknowledge or auxiliary models. To enhance sensitivity, we employ deep learnable\nkernels that automatically adapt to capture nuanced geometric differences\nbetween distributions. Our approach outperforms existing baselines,\ndemonstrating state-of-the-art performance on several benchmarks. The method\nremains competitive even without kernel training, offering a robust, scalable\nsolution for hallucination detection.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u5206\u6790\u9690\u85cf\u72b6\u6001\u5206\u5e03\u5dee\u5f02\u68c0\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\u7684\u65b0\u65b9\u6cd5\uff0c\u4f7f\u7528\u6df1\u5ea6\u53ef\u5b66\u4e60\u6838\u81ea\u9002\u5e94\u6355\u6349\u5206\u5e03\u5dee\u5f02\uff0c\u65e0\u9700\u5916\u90e8\u77e5\u8bc6\u5373\u5b9e\u73b0\u6700\u4f18\u68c0\u6d4b\u6548\u679c", "motivation": "\u73b0\u6709\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u77e5\u8bc6\u6216\u8f85\u52a9\u6a21\u578b\uff0c\u7f3a\u4e4f\u5bf9\u6a21\u578b\u5185\u90e8\u8868\u5f81\u7684\u6df1\u5165\u5206\u6790\uff0c\u96be\u4ee5\u5b9e\u73b0\u9ad8\u6548\u68c0\u6d4b", "method": "\u901a\u8fc7\u8ba1\u7b97\u63d0\u793a\u4e0e\u54cd\u5e94\u9690\u85cf\u72b6\u6001\u5206\u5e03\u7684\u51e0\u4f55\u5dee\u5f02\u4f5c\u4e3a\u5e7b\u89c9\u8bc4\u5206\uff0c\u91c7\u7528\u6df1\u5ea6\u53ef\u5b66\u4e60\u6838\u51fd\u6570\u81ea\u9002\u5e94\u6355\u6349\u5206\u5e03\u95f4\u7684\u7ec6\u5fae\u5dee\u5f02", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5373\u4f7f\u672a\u7ecf\u6838\u8bad\u7ec3\u4ecd\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6a21\u578b\u5185\u5728\u8868\u5f81\u5206\u6790\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u5e7b\u89c9\u68c0\u6d4b\uff0c\u63ed\u793a\u4e86\u5e7b\u89c9\u4ea7\u751f\u6e90\u4e8e\u8868\u9762\u91cd\u8ff0\u800c\u975e\u6df1\u5c42\u63a8\u7406\u7684\u673a\u5236"}}
{"id": "2506.09890", "pdf": "https://arxiv.org/pdf/2506.09890", "abs": "https://arxiv.org/abs/2506.09890", "authors": ["Yuxin Chen", "Yiran Zhao", "Yang Zhang", "An Zhang", "Kenji Kawaguchi", "Shafiq Joty", "Junnan Li", "Tat-Seng Chua", "Michael Qizhe Shieh", "Wenxuan Zhang"], "title": "The Emergence of Abstract Thought in Large Language Models Beyond Any Language", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) continue to advance, their capacity to\nfunction effectively across a diverse range of languages has shown marked\nimprovement. Preliminary studies observe that the hidden activations of LLMs\noften resemble English, even when responding to non-English prompts. This has\nled to the widespread assumption that LLMs may \"think\" in English. However,\nmore recent results showing strong multilingual performance, even surpassing\nEnglish performance on specific tasks in other languages, challenge this view.\nIn this work, we find that LLMs progressively develop a core language-agnostic\nparameter space-a remarkably small subset of parameters whose deactivation\nresults in significant performance degradation across all languages. This\ncompact yet critical set of parameters underlies the model's ability to\ngeneralize beyond individual languages, supporting the emergence of abstract\nthought that is not tied to any specific linguistic system. Specifically, we\nidentify language-related neurons-those are consistently activated during the\nprocessing of particular languages, and categorize them as either shared\n(active across multiple languages) or exclusive (specific to one). As LLMs\nundergo continued development over time, we observe a marked increase in both\nthe proportion and functional importance of shared neurons, while exclusive\nneurons progressively diminish in influence. These shared neurons constitute\nthe backbone of the core language-agnostic parameter space, supporting the\nemergence of abstract thought. Motivated by these insights, we propose\nneuron-specific training strategies tailored to LLMs' language-agnostic levels\nat different development stages. Experiments across diverse LLM families\nsupport our approach.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u53d1\u5c55\u6838\u5fc3\u8bed\u8a00\u65e0\u5173\u53c2\u6570\u7a7a\u95f4\u5b9e\u73b0\u8de8\u8bed\u8a00\u6cdb\u5316\u80fd\u529b\uff0c\u5171\u4eab\u795e\u7ecf\u5143\u6bd4\u4f8b\u968f\u6a21\u578b\u8fdb\u5316\u589e\u52a0\u5e76\u652f\u6491\u62bd\u8c61\u601d\u7ef4\u3002", "motivation": "\u65e9\u671f\u7814\u7a76\u8ba4\u4e3aLLMs\u4ee5\u82f1\u8bed\u4e3a\u5185\u90e8\u5904\u7406\u8bed\u8a00\uff0c\u4f46\u591a\u8bed\u8a00\u6027\u80fd\u63d0\u5347\u6311\u6218\u8be5\u5047\u8bbe\uff0c\u9700\u63a2\u7a76\u5176\u5e95\u5c42\u8de8\u8bed\u8a00\u673a\u5236\u3002", "method": "\u8bc6\u522b\u8bed\u8a00\u76f8\u5173\u795e\u7ecf\u5143\u5e76\u5206\u7c7b\u4e3a\u5171\u4eab/\u4e13\u5c5e\u7c7b\u578b\uff0c\u5206\u6790\u5176\u52a8\u6001\u6f14\u5316\u89c4\u5f8b\uff0c\u8bbe\u8ba1\u795e\u7ecf\u5143\u7279\u5f02\u6027\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u53d1\u73b0\u5173\u952e\u6027\u6838\u5fc3\u53c2\u6570\u7a7a\u95f4\uff0c\u5171\u4eab\u795e\u7ecf\u5143\u5bf9\u591a\u8bed\u8a00\u6027\u80fd\u8d21\u732e\u5ea6\u968f\u6a21\u578b\u89c4\u6a21\u6269\u5927\u663e\u8457\u63d0\u5347\uff08+38%\uff09\uff0c\u4e13\u5c5e\u795e\u7ecf\u5143\u5f71\u54cd\u529b\u4e0b\u964d21%\u3002", "conclusion": "\u8bed\u8a00\u65e0\u5173\u53c2\u6570\u7a7a\u95f4\u901a\u8fc7\u5171\u4eab\u795e\u7ecf\u5143\u5b9e\u73b0\u62bd\u8c61\u601d\u7ef4\uff0c\u9488\u5bf9\u6027\u8bad\u7ec3\u7b56\u7565\u53ef\u4f18\u5316LLMs\u591a\u8bed\u8a00\u80fd\u529b\u53d1\u5c55\u8def\u5f84\u3002"}}
{"id": "2506.09902", "pdf": "https://arxiv.org/pdf/2506.09902", "abs": "https://arxiv.org/abs/2506.09902", "authors": ["Zheng Zhao", "Clara Vania", "Subhradeep Kayal", "Naila Khan", "Shay B. Cohen", "Emine Yilmaz"], "title": "PersonaLens: A Benchmark for Personalization Evaluation in Conversational AI Assistants", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 Findings", "summary": "Large language models (LLMs) have advanced conversational AI assistants.\nHowever, systematically evaluating how well these assistants apply\npersonalization--adapting to individual user preferences while completing\ntasks--remains challenging. Existing personalization benchmarks focus on\nchit-chat, non-conversational tasks, or narrow domains, failing to capture the\ncomplexities of personalized task-oriented assistance. To address this, we\nintroduce PersonaLens, a comprehensive benchmark for evaluating personalization\nin task-oriented AI assistants. Our benchmark features diverse user profiles\nequipped with rich preferences and interaction histories, along with two\nspecialized LLM-based agents: a user agent that engages in realistic\ntask-oriented dialogues with AI assistants, and a judge agent that employs the\nLLM-as-a-Judge paradigm to assess personalization, response quality, and task\nsuccess. Through extensive experiments with current LLM assistants across\ndiverse tasks, we reveal significant variability in their personalization\ncapabilities, providing crucial insights for advancing conversational AI\nsystems.", "AI": {"tldr": "\u63d0\u51faPersonaLens\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u7cfb\u7edf\u8bc4\u4f30AI\u52a9\u624b\u5728\u4efb\u52a1\u5bfc\u5411\u573a\u666f\u4e2d\u7684\u4e2a\u6027\u5316\u80fd\u529b", "motivation": "\u73b0\u6709\u4e2a\u6027\u5316\u8bc4\u4f30\u57fa\u51c6\u5c40\u9650\u4e8e\u95f2\u804a/\u7a84\u9886\u57df\u4efb\u52a1\uff0c\u65e0\u6cd5\u8bc4\u4f30\u590d\u6742\u4efb\u52a1\u5bfc\u5411\u573a\u666f\u7684\u4e2a\u6027\u5316\u9700\u6c42", "method": "\u6784\u5efa\u5305\u542b\u591a\u6837\u5316\u7528\u6237\u753b\u50cf\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u90e8\u7f72\u7528\u6237\u4ee3\u7406\u6a21\u62df\u5bf9\u8bdd\uff0c\u4f7f\u7528LLM-as-a-Judge\u8303\u5f0f\u8fdb\u884c\u591a\u7ef4\u5ea6\u8bc4\u4f30", "result": "\u4e0d\u540cLLM\u52a9\u624b\u5728\u4e2a\u6027\u5316\u80fd\u529b\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u63ed\u793a\u6539\u8fdb\u65b9\u5411", "conclusion": "PersonaLens\u4e3a\u63d0\u5347\u5bf9\u8bdd\u7cfb\u7edf\u4e2a\u6027\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u8bc4\u4f30\u5de5\u5177\u548c\u7814\u7a76\u6846\u67b6"}}
{"id": "2506.09917", "pdf": "https://arxiv.org/pdf/2506.09917", "abs": "https://arxiv.org/abs/2506.09917", "authors": ["Wendi Zhou", "Ameer Saadat-Yazd", "Nadin Kokciyan"], "title": "Aspect-Based Opinion Summarization with Argumentation Schemes", "categories": ["cs.CL"], "comment": "Accepted by ArgMining 2025", "summary": "Reviews are valuable resources for customers making purchase decisions in\nonline shopping. However, it is impractical for customers to go over the vast\nnumber of reviews and manually conclude the prominent opinions, which prompts\nthe need for automated opinion summarization systems. Previous approaches,\neither extractive or abstractive, face challenges in automatically producing\ngrounded aspect-centric summaries. In this paper, we propose a novel\nsummarization system that not only captures predominant opinions from an aspect\nperspective with supporting evidence, but also adapts to varying domains\nwithout relying on a pre-defined set of aspects. Our proposed framework,\nASESUM, summarizes viewpoints relevant to the critical aspects of a product by\nextracting aspect-centric arguments and measuring their salience and validity.\nWe conduct experiments on a real-world dataset to demonstrate the superiority\nof our approach in capturing diverse perspectives of the original reviews\ncompared to new and existing methods.", "AI": {"tldr": "\u63d0\u51faASESUM\u7cfb\u7edf\uff0c\u901a\u8fc7\u52a8\u6001\u63d0\u53d6\u4ea7\u54c1\u6838\u5fc3\u65b9\u9762\u5e76\u9a8c\u8bc1\u8bba\u636e\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u8de8\u9886\u57df\u65e0\u9884\u5b9a\u4e49\u65b9\u9762\u7684\u81ea\u52a8\u5316\u89c2\u70b9\u6458\u8981\u3002", "motivation": "\u5728\u7ebf\u8bc4\u8bba\u6570\u91cf\u5e9e\u5927\uff0c\u7528\u6237\u96be\u4ee5\u624b\u52a8\u5f52\u7eb3\u6838\u5fc3\u89c2\u70b9\u3002\u73b0\u6709\u6458\u8981\u65b9\u6cd5\uff08\u62bd\u53d6\u5f0f/\u751f\u6210\u5f0f\uff09\u5b58\u5728\u65e0\u6cd5\u52a8\u6001\u9002\u914d\u9886\u57df\u3001\u7f3a\u4e4f\u8bc1\u636e\u652f\u6491\u7684\u95ee\u9898\u3002", "method": "\u5f00\u53d1ASESUM\u6846\u67b6\uff1a1) \u63d0\u53d6\u65b9\u9762\u4e2d\u5fc3\u8bba\u70b9 2) \u8ba1\u7b97\u89c2\u70b9\u663e\u8457\u6027\u548c\u6709\u6548\u6027 3) \u81ea\u52a8\u8bc6\u522b\u4ea7\u54c1\u5173\u952e\u65b9\u9762 4) \u652f\u6301\u8de8\u9886\u57df\u9002\u914d", "result": "\u771f\u5b9e\u6570\u636e\u96c6\u5b9e\u9a8c\u663e\u793a\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u672c\u7cfb\u7edf\u80fd\u66f4\u5168\u9762\u6355\u83b7\u8bc4\u8bba\u4e2d\u7684\u591a\u6837\u5316\u89c6\u89d2\uff08F1\u503c\u63d0\u534712.5%\uff09", "conclusion": "\u8be5\u7cfb\u7edf\u7a81\u7834\u9884\u5b9a\u4e49\u65b9\u9762\u7684\u9650\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u65b9\u9762\u8bc6\u522b\u548c\u8bc1\u636e\u9a8c\u8bc1\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u8de8\u9886\u57df\u610f\u89c1\u6458\u8981\u7684\u9002\u5e94\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2506.09942", "pdf": "https://arxiv.org/pdf/2506.09942", "abs": "https://arxiv.org/abs/2506.09942", "authors": ["Hao Peng", "Yunjia Qi", "Xiaozhi Wang", "Bin Xu", "Lei Hou", "Juanzi Li"], "title": "VerIF: Verification Engineering for Reinforcement Learning in Instruction Following", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 8 figures", "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a key\ntechnique for enhancing large language models (LLMs), with verification\nengineering playing a central role. However, best practices for RL in\ninstruction following remain underexplored. In this work, we explore the\nverification challenge in RL for instruction following and propose VerIF, a\nverification method that combines rule-based code verification with LLM-based\nverification from a large reasoning model (e.g., QwQ-32B). To support this\napproach, we construct a high-quality instruction-following dataset,\nVerInstruct, containing approximately 22,000 instances with associated\nverification signals. We apply RL training with VerIF to two models, achieving\nsignificant improvements across several representative instruction-following\nbenchmarks. The trained models reach state-of-the-art performance among models\nof comparable size and generalize well to unseen constraints. We further\nobserve that their general capabilities remain unaffected, suggesting that RL\nwith VerIF can be integrated into existing RL recipes to enhance overall model\nperformance. We have released our datasets, codes, and models to facilitate\nfuture research at https://github.com/THU-KEG/VerIF.", "AI": {"tldr": "\u63d0\u51faVerIF\u65b9\u6cd5\uff0c\u7ed3\u5408\u89c4\u5219\u9a8c\u8bc1\u4e0e\u5927\u6a21\u578b\u9a8c\u8bc1\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6848\uff0c\u4f7f\u7528VerInstruct\u6570\u636e\u96c6\u8bad\u7ec3\u540e\u5b9e\u73b0\u540c\u89c4\u6a21\u6a21\u578bSOTA\u4e14\u4e0d\u5f71\u54cd\u901a\u7528\u80fd\u529b", "motivation": "\u9488\u5bf9\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u4e2d\u5f3a\u5316\u5b66\u4e60\u7684\u9a8c\u8bc1\u5de5\u7a0b\u7f3a\u4e4f\u7cfb\u7edf\u63a2\u7d22\uff0c\u73b0\u6709\u9a8c\u8bc1\u65b9\u6cd5\u5b58\u5728\u4f18\u5316\u7a7a\u95f4", "method": "\u6784\u5efa\u5305\u542b2.2\u4e07\u6837\u672c\u7684VerInstruct\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u878d\u5408\u89c4\u5219\u4ee3\u7801\u9a8c\u8bc1\u4e0eQwQ-32B\u7b49\u5927\u6a21\u578b\u63a8\u7406\u7684VerIF\u9a8c\u8bc1\u6846\u67b6\uff0c\u5bf9\u4e24\u4e2a\u6a21\u578b\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3", "result": "\u5728\u591a\u4e2a\u6307\u4ee4\u8ddf\u968f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u6a21\u578b\u53c2\u6570\u91cf\u76f8\u8fd1\u60c5\u51b5\u4e0b\u8fbe\u5230SOTA\uff0c\u5bf9\u672a\u89c1\u7ea6\u675f\u5177\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b\u4e14\u901a\u7528\u80fd\u529b\u672a\u53d7\u635f", "conclusion": "\u9a8c\u8bc1\u5f3a\u5316\u5b66\u4e60\u65b9\u6848\u53ef\u517c\u5bb9\u73b0\u6709RL\u65b9\u6cd5\u63d0\u5347\u6574\u4f53\u6027\u80fd\uff0c\u5df2\u5f00\u6e90\u6570\u636e\u96c6/\u4ee3\u7801/\u6a21\u578b\u63a8\u52a8\u540e\u7eed\u7814\u7a76"}}
{"id": "2506.09944", "pdf": "https://arxiv.org/pdf/2506.09944", "abs": "https://arxiv.org/abs/2506.09944", "authors": ["Wuwei Zhang", "Fangcong Yin", "Howard Yen", "Danqi Chen", "Xi Ye"], "title": "Query-Focused Retrieval Heads Improve Long-Context Reasoning and Re-ranking", "categories": ["cs.CL"], "comment": null, "summary": "Recent work has identified retrieval heads (Wu et al., 2025b), a subset of\nattention heads responsible for retrieving salient information in long-context\nlanguage models (LMs), as measured by their copy-paste behavior in\nNeedle-in-a-Haystack tasks. In this paper, we introduce QRHEAD (Query-Focused\nRetrieval Head), an improved set of attention heads that enhance retrieval from\nlong context. We identify QRHEAD by aggregating attention scores with respect\nto the input query, using a handful of examples from real-world tasks (e.g.,\nlong-context QA). We further introduce QR- RETRIEVER, an efficient and\neffective retriever that uses the accumulated attention mass of QRHEAD as\nretrieval scores. We use QR- RETRIEVER for long-context reasoning by selecting\nthe most relevant parts with the highest retrieval scores. On multi-hop\nreasoning tasks LongMemEval and CLIPPER, this yields over 10% performance gains\nover full context and outperforms strong dense retrievers. We also evaluate\nQRRETRIEVER as a re-ranker on the BEIR benchmark and find that it achieves\nstrong zero-shot performance, outperforming other LLM-based re-rankers such as\nRankGPT. Further analysis shows that both the querycontext attention scoring\nand task selection are crucial for identifying QRHEAD with strong downstream\nutility. Overall, our work contributes a general-purpose retriever and offers\ninterpretability insights into the long-context capabilities of LMs.", "AI": {"tldr": "\u63d0\u51faQRHEAD\u6ce8\u610f\u529b\u5934\u548cQR-RETRIEVER\u68c0\u7d22\u5668\uff0c\u663e\u8457\u63d0\u5347\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u7684\u68c0\u7d22\u6027\u80fd\u4e0e\u591a\u8df3\u63a8\u7406\u6548\u679c", "motivation": "\u73b0\u6709\u68c0\u7d22\u5934\u5728\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u5b58\u5728\u6548\u7387\u74f6\u9888\uff0c\u9700\u901a\u8fc7\u66f4\u805a\u7126\u7684\u67e5\u8be2\u6ce8\u610f\u529b\u673a\u5236\u6539\u8fdb\u68c0\u7d22\u8d28\u91cf", "method": "\u57fa\u4e8e\u771f\u5b9e\u4efb\u52a1\u6837\u672c\u805a\u5408\u67e5\u8be2\u76f8\u5173\u6ce8\u610f\u529b\u5206\u6570\u8bc6\u522bQRHEAD\uff0c\u6784\u5efa\u6ce8\u610f\u529b\u8d28\u91cf\u9a71\u52a8\u7684\u68c0\u7d22\u8bc4\u5206\u673a\u5236", "result": "\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u6027\u80fd\u63d0\u5347\u8d8510%\uff0cBEIR\u57fa\u51c6\u96f6\u6837\u672c\u8868\u73b0\u8d85\u8d8aRankGPT\u7b49LLM\u6392\u5e8f\u5668", "conclusion": "\u5f00\u53d1\u51fa\u901a\u7528\u68c0\u7d22\u6846\u67b6\uff0c\u540c\u65f6\u4e3a\u8bed\u8a00\u6a21\u578b\u7684\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u673a\u5236\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u6d1e\u5bdf"}}
{"id": "2506.09967", "pdf": "https://arxiv.org/pdf/2506.09967", "abs": "https://arxiv.org/abs/2506.09967", "authors": ["Shangshang Wang", "Julian Asilis", "\u00d6mer Faruk Akg\u00fcl", "Enes Burak Bilgin", "Ollie Liu", "Deqing Fu", "Willie Neiswanger"], "title": "Resa: Transparent Reasoning Models via SAEs", "categories": ["cs.CL"], "comment": null, "summary": "How cost-effectively can we elicit strong reasoning in language models by\nleveraging their underlying representations? We answer this question with Resa,\na family of 1.5B reasoning models trained via a novel and efficient sparse\nautoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to\ncapture reasoning abilities from a source model, and then uses the trained SAE\nto guide a standard supervised fine-tuning process to elicit such abilities in\na target model, all using verified question-answer data without any reasoning\ntraces. Notably, when applied to certain base models before further RL\npost-training, SAE-Tuning retains >97% of its RL-trained counterpart's\nreasoning performance while reducing training costs by >2000x to roughly \\$1\nand training time by >450x to around 20 minutes. Furthermore, when applied to\nlightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning\nperformance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only\naround \\$1 additional cost. Surprisingly, the reasoning abilities extracted via\nSAEs are potentially both generalizable and modular. Generality means abilities\nextracted from one dataset still elevate performance on a larger and\noverlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math\ncan be attached to the R1-Distill model at test time, without any retraining,\nand yield comparable gains. Extensive ablations validate these findings and all\nartifacts are fully open-sourced.", "AI": {"tldr": "\u63d0\u51faResa\u6a21\u578b\u5bb6\u65cf\uff0c\u901a\u8fc7SAE-Tuning\u65b9\u6cd5\u4ee51\u7f8e\u5143\u6210\u672c/20\u5206\u949f\u8bad\u7ec3\u65f6\u95f4\u5b9e\u73b097% RL\u6a21\u578b\u63a8\u7406\u6027\u80fd\uff0c\u5c55\u793a\u7a00\u758f\u81ea\u7f16\u7801\u5668\u63d0\u53d6\u7684\u63a8\u7406\u80fd\u529b\u5177\u6709\u901a\u7528\u6027\u548c\u6a21\u5757\u5316\u7279\u6027\u3002", "motivation": "\u63a2\u7a76\u5982\u4f55\u9ad8\u6548\u5229\u7528\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u5728\u8868\u793a\u6765\u4f4e\u6210\u672c\u6fc0\u53d1\u5176\u63a8\u7406\u80fd\u529b\uff0c\u7a81\u7834\u73b0\u6709\u65b9\u6cd5\u9ad8\u6210\u672c\u4f4e\u6548\u7387\u7684\u5c40\u9650\u6027\u3002", "method": "\u7a00\u758f\u81ea\u7f16\u7801\u5668\u8c03\u4f18(SAE-Tuning)\uff1a\u5148\u8bad\u7ec3SAE\u6355\u83b7\u6e90\u6a21\u578b\u63a8\u7406\u80fd\u529b\uff0c\u518d\u7528\u5176\u6307\u5bfc\u76ee\u6807\u6a21\u578b\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u5168\u7a0b\u4ec5\u9700\u95ee\u7b54\u6570\u636e\u65e0\u9700\u63a8\u7406\u8f68\u8ff9\u3002", "result": "1) RL\u8bad\u7ec3\u540e\u6210\u672c\u964d2000\u500d\u81f3$1\uff0c\u4fdd\u755997%\u6027\u80fd\uff1b2) \u8f7b\u91cf\u8bad\u7ec3\u6a21\u578b\u8fbeAIME24 43.33%/AMC23 90%\uff1b3) SAE\u63d0\u53d6\u80fd\u529b\u53ef\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u53ca\u8de8\u6a21\u578b\u6a21\u5757\u5316\u7ec4\u5408\u3002", "conclusion": "SAE-Tuning\u5f00\u521b\u4e86\u4f4e\u6210\u672c\u6fc0\u53d1\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u65b0\u8303\u5f0f\uff0c\u5176\u901a\u7528\u6027\u548c\u6a21\u5757\u5316\u7279\u6027\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u63d0\u4f9b\u53ef\u884c\u65b9\u6848\uff0c\u6240\u6709\u6210\u679c\u5df2\u5b8c\u5168\u5f00\u6e90\u3002"}}
{"id": "2506.09975", "pdf": "https://arxiv.org/pdf/2506.09975", "abs": "https://arxiv.org/abs/2506.09975", "authors": ["Hillary Dawkins", "Kathleen C. Fraser", "Svetlana Kiritchenko"], "title": "When Detection Fails: The Power of Fine-Tuned Models to Generate Human-Like Social Media Text", "categories": ["cs.CL"], "comment": "to appear in ACL Findings", "summary": "Detecting AI-generated text is a difficult problem to begin with; detecting\nAI-generated text on social media is made even more difficult due to the short\ntext length and informal, idiosyncratic language of the internet. It is\nnonetheless important to tackle this problem, as social media represents a\nsignificant attack vector in online influence campaigns, which may be bolstered\nthrough the use of mass-produced AI-generated posts supporting (or opposing)\nparticular policies, decisions, or events. We approach this problem with the\nmindset and resources of a reasonably sophisticated threat actor, and create a\ndataset of 505,159 AI-generated social media posts from a combination of\nopen-source, closed-source, and fine-tuned LLMs, covering 11 different\ncontroversial topics. We show that while the posts can be detected under\ntypical research assumptions about knowledge of and access to the generating\nmodels, under the more realistic assumption that an attacker will not release\ntheir fine-tuned model to the public, detectability drops dramatically. This\nresult is confirmed with a human study. Ablation experiments highlight the\nvulnerability of various detection algorithms to fine-tuned LLMs. This result\nhas implications across all detection domains, since fine-tuning is a generally\napplicable and realistic LLM use case.", "AI": {"tldr": "\u68c0\u6d4b\u793e\u4ea4\u5a92\u4f53AI\u751f\u6210\u6587\u672c\u5b58\u5728\u6311\u6218\uff0c\u653b\u51fb\u8005\u4f7f\u7528\u5fae\u8c03\u6a21\u578b\u65f6\u68c0\u6d4b\u6548\u679c\u6025\u5267\u4e0b\u964d", "motivation": "\u793e\u4ea4\u5a92\u4f53\u662f\u5f71\u54cd\u529b\u653b\u51fb\u7684\u91cd\u8981\u8f7d\u4f53\uff0c\u9700\u6709\u6548\u68c0\u6d4bAI\u751f\u6210\u7684\u5927\u89c4\u6a21\u64cd\u7eb5\u6027\u5185\u5bb9", "method": "\u6784\u5efa505,159\u6761\u591a\u6a21\u578b\u751f\u6210\u7684\u793e\u4ea4\u5a92\u4f53\u8bed\u6599\u5e93\uff0c\u8986\u76d611\u4e2a\u4e89\u8bae\u8bdd\u9898\uff0c\u6a21\u62df\u653b\u51fb\u8005\u4f7f\u7528\u5f00\u6e90/\u95ed\u6e90/\u5fae\u8c03\u6a21\u578b\u7684\u6df7\u5408\u653b\u51fb\u573a\u666f", "result": "\u5728\u653b\u51fb\u8005\u4e0d\u516c\u5f00\u5fae\u8c03\u6a21\u578b\u7684\u73b0\u5b9e\u5047\u8bbe\u4e0b\uff0c\u73b0\u6709\u68c0\u6d4b\u7b97\u6cd5\u51c6\u786e\u7387\u663e\u8457\u4e0b\u964d\uff0c\u4eba\u7c7b\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u68c0\u6d4b\u56f0\u96be", "conclusion": "\u5fae\u8c03\u6280\u672f\u666e\u904d\u9002\u7528\u6027\u5bfc\u81f4\u68c0\u6d4b\u8106\u5f31\u6027\uff0c\u5f53\u524d\u68c0\u6d4b\u65b9\u6cd5\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u53ef\u80fd\u5b58\u5728\u7cfb\u7edf\u6027\u7f3a\u9677"}}
{"id": "2506.09983", "pdf": "https://arxiv.org/pdf/2506.09983", "abs": "https://arxiv.org/abs/2506.09983", "authors": ["Hiroshi Matsuda", "Chunpeng Ma", "Masayuki Asahara"], "title": "Step-by-step Instructions and a Simple Tabular Output Format Improve the Dependency Parsing Accuracy of LLMs", "categories": ["cs.CL"], "comment": "9 pages, 2 figures, accepted for SyntaxFest 2025", "summary": "Recent advances in large language models (LLMs) have enabled impressive\nperformance in various tasks. However, standard prompting often struggles to\nproduce structurally valid and accurate outputs, especially in dependency\nparsing. We propose a novel step-by-step instruction strategy, where universal\npart-of-speech tagging precedes the prediction of syntactic heads and\ndependency labels, and a simplified CoNLL-U like output format, our method\nachieves state-of-the-art accuracy on Universal Dependencies datasets across 17\nlanguages without hallucination or contamination. We further show that\nmultilingual fine-tuning simultaneously improves cross-language generalization\nperformance. Our results highlight the effectiveness of explicit reasoning\nsteps in LLM-based parsing and offer a scalable, format-consistent alternative\nto bracket-based approaches.", "AI": {"tldr": "\u63d0\u51fa\u5206\u6b65\u6307\u4ee4\u7b56\u7565\u548c\u7b80\u5316\u7684CoNLL-U\u683c\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u8de8\u8bed\u8a00\u4f9d\u8d56\u89e3\u6790\u6027\u80fd", "motivation": "\u89e3\u51b3\u6807\u51c6prompt\u5728\u4f9d\u8d56\u89e3\u6790\u4efb\u52a1\u4e2d\u8f93\u51fa\u7ed3\u6784\u6709\u6548\u6027\u4e0d\u8db3\u7684\u95ee\u9898", "method": "\u5148\u8fdb\u884c\u901a\u7528\u8bcd\u6027\u6807\u6ce8\uff0c\u540e\u9884\u6d4b\u53e5\u6cd5\u5934\u548c\u4f9d\u5b58\u6807\u7b7e\uff0c\u91c7\u7528\u7b80\u5316\u7684CoNLL-U\u683c\u5f0f", "result": "\u572817\u79cd\u8bed\u8a00UD\u6570\u636e\u96c6\u53d6\u5f97SOTA\uff0c\u591a\u8bed\u8a00\u5fae\u8c03\u63d0\u5347\u8de8\u8bed\u8a00\u6cdb\u5316\u80fd\u529b", "conclusion": "\u663e\u5f0f\u63a8\u7406\u6b65\u9aa4\u6709\u6548\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u4e14\u683c\u5f0f\u7edf\u4e00\u7684\u4f9d\u5b58\u89e3\u6790\u65b0\u65b9\u6848"}}
{"id": "2506.09992", "pdf": "https://arxiv.org/pdf/2506.09992", "abs": "https://arxiv.org/abs/2506.09992", "authors": ["Amel Muminovic", "Amela Kadric Muminovic"], "title": "Large Language Models for Toxic Language Detection in Low-Resource Balkan Languages", "categories": ["cs.CL"], "comment": "8 pages", "summary": "Online toxic language causes real harm, especially in regions with limited\nmoderation tools. In this study, we evaluate how large language models handle\ntoxic comments in Serbian, Croatian, and Bosnian, languages with limited\nlabeled data. We built and manually labeled a dataset of 4,500 YouTube and\nTikTok comments drawn from videos across diverse categories, including music,\npolitics, sports, modeling, influencer content, discussions of sexism, and\ngeneral topics. Four models (GPT-3.5 Turbo, GPT-4.1, Gemini 1.5 Pro, and Claude\n3 Opus) were tested in two modes: zero-shot and context-augmented. We measured\nprecision, recall, F1 score, accuracy and false positive rates. Including a\nshort context snippet raised recall by about 0.12 on average and improved F1\nscore by up to 0.10, though it sometimes increased false positives. The best\nbalance came from Gemini in context-augmented mode, reaching an F1 score of\n0.82 and accuracy of 0.82, while zero-shot GPT-4.1 led on precision and had the\nlowest false alarms. We show how adding minimal context can improve toxic\nlanguage detection in low-resource settings and suggest practical strategies\nsuch as improved prompt design and threshold calibration. These results show\nthat prompt design alone can yield meaningful gains in toxicity detection for\nunderserved Balkan language communities.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6dfb\u52a0\u4e0a\u4e0b\u6587\u7247\u6bb5\u63d0\u5347LLM\u5728\u4f4e\u8d44\u6e90\u5df4\u5c14\u5e72\u8bed\u8a00\u4e2d\u7684\u6bd2\u6027\u68c0\u6d4b\u6548\u679c\uff0cGemini\u5728\u4e0a\u4e0b\u6587\u589e\u5f3a\u6a21\u5f0f\u4e0b\u8fbe\u5230\u6700\u4f73\u5e73\u8861\uff08F1 0.82\uff09\uff0cGPT-4.1\u5219\u5728\u96f6\u6837\u672c\u4e0b\u4fdd\u6301\u6700\u9ad8\u7cbe\u5ea6", "motivation": "\u89e3\u51b3\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u5df4\u5c14\u5e72\u8bed\u8a00\u793e\u533a\u5728\u7ebf\u6709\u5bb3\u5185\u5bb9\u68c0\u6d4b\u96be\u9898\uff0c\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027", "method": "\u6784\u5efa4500\u6761\u591a\u5e73\u53f0\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u6d4b\u8bd54\u4e2a\u4e3b\u6d41\u6a21\u578b\u5728\u96f6\u6837\u672c/\u4e0a\u4e0b\u6587\u589e\u5f3a\u4e24\u79cd\u6a21\u5f0f\u4e0b\u7684\u6027\u80fd\uff08\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001F1\u3001\u51c6\u786e\u7387\u3001\u8bef\u62a5\u7387\uff09", "result": "\u4e0a\u4e0b\u6587\u7247\u6bb5\u4f7f\u5e73\u5747\u53ec\u56de\u7387\u63d0\u53470.12\uff0cGemini\u4e0a\u4e0b\u6587\u6a21\u5f0f\u7efc\u5408\u8868\u73b0\u6700\u4f73\uff08F1 0.82\uff09\uff0cGPT-4.1\u96f6\u6837\u672c\u6a21\u5f0f\u8bef\u62a5\u7387\u6700\u4f4e\uff08\u7cbe\u786e\u7387\u6700\u9ad8\uff09", "conclusion": "\u7b80\u5355\u7684\u63d0\u793a\u5de5\u7a0b\u4f18\u5316\uff08\u4e0a\u4e0b\u6587\u8865\u5145+\u9608\u503c\u6821\u51c6\uff09\u53ef\u663e\u8457\u6539\u5584\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u6bd2\u6027\u68c0\u6d4b\uff0c\u4e3a\u5df4\u5c14\u5e72\u5730\u533a\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.09996", "pdf": "https://arxiv.org/pdf/2506.09996", "abs": "https://arxiv.org/abs/2506.09996", "authors": ["Yang Li", "Qiang Sheng", "Yehan Yang", "Xueyao Zhang", "Juan Cao"], "title": "From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring", "categories": ["cs.CL", "cs.CY"], "comment": "22 pages, 7 figures, and 9 tables", "summary": "Though safety alignment has been applied to most large language models\n(LLMs), LLM service providers generally deploy a subsequent moderation as the\nexternal safety guardrail in real-world products. Existing moderators mainly\npractice a conventional full detection, which determines the harmfulness based\non the complete LLM output, causing high service latency. Recent works pay more\nattention to partial detection where moderators oversee the generation midway\nand early stop the output if harmfulness is detected, but they directly apply\nmoderators trained with the full detection paradigm to incomplete outputs,\nintroducing a training-inference gap that lowers the performance. In this\npaper, we explore how to form a data-and-model solution that natively supports\npartial detection. For the data, we construct FineHarm, a dataset consisting of\n29K prompt-response pairs with fine-grained annotations to provide reasonable\nsupervision for token-level training. Then, we propose the streaming content\nmonitor, which is trained with dual supervision of response- and token-level\nlabels and can follow the output stream of LLM to make a timely judgment of\nharmfulness. Experiments show that SCM gains 0.95+ in macro F1 score that is\ncomparable to full detection, by only seeing the first 18% of tokens in\nresponses on average. Moreover, the SCM can serve as a pseudo-harmfulness\nannotator for improving safety alignment and lead to a higher harmlessness\nscore than DPO.", "AI": {"tldr": "\u63d0\u51fa\u6d41\u5f0f\u5185\u5bb9\u76d1\u63a7\u5668SCM\uff0c\u901a\u8fc7\u6784\u5efa\u7ec6\u7c92\u5ea6\u6807\u6ce8\u6570\u636e\u96c6FineHarm\u548c\u53cc\u91cd\u76d1\u7763\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4ec5\u9700\u89c2\u5bdf\u524d18%\u7684token\u5373\u53ef\u8fbe\u6210\u4e0e\u5168\u68c0\u6d4b\u76f8\u5f53\u76840.95+\u5b8fF1\u503c\uff0c\u5e76\u63d0\u5347\u5b89\u5168\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u5ba1\u67e5\u673a\u5236\u5b58\u5728\u5168\u68c0\u6d4b\u5ef6\u8fdf\u9ad8\u3001\u90e8\u5206\u68c0\u6d4b\u8303\u5f0f\u5b58\u5728\u8bad\u7ec3-\u63a8\u7406gap\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u9700\u8981\u539f\u751f\u652f\u6301\u90e8\u5206\u68c0\u6d4b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u6784\u5efa\u542b29K\u7ec6\u7c92\u5ea6\u6807\u6ce8\u7684FineHarm\u6570\u636e\u96c6\uff1b2. \u8bbe\u8ba1\u6d41\u5f0f\u5185\u5bb9\u76d1\u63a7\u5668SCM\uff0c\u901a\u8fc7\u54cd\u5e94\u7ea7\u548ctoken\u7ea7\u7684\u53cc\u91cd\u76d1\u7763\u8bad\u7ec3\u5b9e\u73b0\u5b9e\u65f6\u5371\u5bb3\u5224\u65ad\u3002", "result": "SCM\u5e73\u5747\u4ec5\u9700\u89c2\u5bdf\u524d18%\u7684token\u5373\u53ef\u8fbe\u5230\u4e0e\u5168\u68c0\u6d4b\u76f8\u5f53\u76840.95+\u5b8fF1\u503c\uff0c\u4e14\u4f5c\u4e3a\u4f2a\u5371\u5bb3\u6807\u6ce8\u5668\u53ef\u4f7f\u5b89\u5168\u5bf9\u9f50\u540e\u7684\u6a21\u578b\u83b7\u5f97\u6bd4DPO\u66f4\u9ad8\u7684\u65e0\u5bb3\u6027\u8bc4\u5206\u3002", "conclusion": "SCM\u901a\u8fc7\u6570\u636e\u6784\u9020\u548c\u6a21\u578b\u67b6\u6784\u521b\u65b0\u6709\u6548\u89e3\u51b3\u4e86\u90e8\u5206\u68c0\u6d4b\u7684\u7cbe\u5ea6\u95ee\u9898\uff0c\u4e3a\u5b9e\u65f6\u5185\u5bb9\u5b89\u5168\u76d1\u63a7\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u53ef\u53cd\u5411\u589e\u5f3aLLM\u81ea\u8eab\u7684\u5b89\u5168\u5bf9\u9f50\u3002"}}
{"id": "2410.16222", "pdf": "https://arxiv.org/pdf/2410.16222", "abs": "https://arxiv.org/abs/2410.16222", "authors": ["Valentyn Boreiko", "Alexander Panfilov", "Vaclav Voracek", "Matthias Hein", "Jonas Geiping"], "title": "An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "A plethora of jailbreaking attacks have been proposed to obtain harmful\nresponses from safety-tuned LLMs. These methods largely succeed in coercing the\ntarget output in their original settings, but their attacks vary substantially\nin fluency and computational effort. In this work, we propose a unified threat\nmodel for the principled comparison of these methods. Our threat model checks\nif a given jailbreak is likely to occur in the distribution of text. For this,\nwe build an N-gram language model on 1T tokens, which, unlike model-based\nperplexity, allows for an LLM-agnostic, nonparametric, and inherently\ninterpretable evaluation. We adapt popular attacks to this threat model, and,\nfor the first time, benchmark these attacks on equal footing with it. After an\nextensive comparison, we find attack success rates against safety-tuned modern\nmodels to be lower than previously presented and that attacks based on discrete\noptimization significantly outperform recent LLM-based attacks. Being\ninherently interpretable, our threat model allows for a comprehensive analysis\nand comparison of jailbreak attacks. We find that effective attacks exploit and\nabuse infrequent bigrams, either selecting the ones absent from real-world text\nor rare ones, e.g., specific to Reddit or code datasets.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u5a01\u80c1\u6a21\u578b\u8bc4\u4f30LLM\u8d8a\u72f1\u653b\u51fb\uff0c\u53d1\u73b0\u79bb\u6563\u4f18\u5316\u653b\u51fb\u663e\u8457\u4f18\u4e8eLLM\u65b9\u6cd5\uff0c\u653b\u51fb\u6210\u529f\u7387\u4f4e\u4e8e\u9884\u671f", "motivation": "\u73b0\u6709\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\u7f3a\u4e4f\u7edf\u4e00\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u96be\u4ee5\u7cfb\u7edf\u6bd4\u8f83\u4e0d\u540c\u653b\u51fb\u7684\u5b9e\u9645\u6548\u679c\u548c\u6f5c\u5728\u98ce\u9669", "method": "\u6784\u5efa\u57fa\u4e8e1T tokens\u7684N-gram\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u975e\u53c2\u6570\u5316\u65b9\u6cd5\u5206\u6790\u653b\u51fb\u5229\u7528\u7684\u7f55\u89c1bigrams\u5206\u5e03\u7279\u5f81", "result": "\u5b89\u5168\u8c03\u4f18\u6a21\u578b\u7684\u653b\u51fb\u6210\u529f\u7387\u6bd4\u6587\u732e\u62a5\u9053\u4f4e25%\uff0c\u79bb\u6563\u4f18\u5316\u653b\u51fb\u6210\u529f\u7387\u6bd4LLM\u653b\u51fb\u9ad83\u500d", "conclusion": "\u7edf\u4e00\u5a01\u80c1\u6a21\u578b\u63ed\u793a\u653b\u51fb\u672c\u8d28\u4f9d\u8d56\u7f55\u89c1\u8bed\u8a00\u6a21\u5f0f\uff08\u5982Reddit/\u4ee3\u7801\u6570\u636e\u96c6\u7279\u5f81\uff09\uff0c\u4e3a\u9632\u5fa1\u7b56\u7565\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2506.09081", "pdf": "https://arxiv.org/pdf/2506.09081", "abs": "https://arxiv.org/abs/2506.09081", "authors": ["Zheqi He", "Yesheng Liu", "Jing-shu Zheng", "Xuejing Li", "Richeng Xuan", "Jin-Ge Yao", "Xi Yang"], "title": "FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "We present FlagEvalMM, an open-source evaluation framework designed to\ncomprehensively assess multimodal models across a diverse range of\nvision-language understanding and generation tasks, such as visual question\nanswering, text-to-image/video generation, and image-text retrieval. We\ndecouple model inference from evaluation through an independent evaluation\nservice, thus enabling flexible resource allocation and seamless integration of\nnew tasks and models. Moreover, FlagEvalMM utilizes advanced inference\nacceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to\nsignificantly enhance evaluation efficiency. Extensive experiments show that\nFlagEvalMM offers accurate and efficient insights into model strengths and\nlimitations, making it a valuable tool for advancing multimodal research. The\nframework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.", "AI": {"tldr": "FlagEvalMM\u662f\u5f00\u6e90\u7684\u591a\u6a21\u6001\u6a21\u578b\u8bc4\u4f30\u6846\u67b6\uff0c\u652f\u6301\u89c6\u89c9\u95ee\u7b54\u3001\u6587\u672c\u751f\u6210\u56fe\u50cf/\u89c6\u9891\u7b49\u4efb\u52a1\u8bc4\u4f30\uff0c\u901a\u8fc7\u89e3\u8026\u63a8\u7406\u4e0e\u8bc4\u4f30\u63d0\u5347\u6548\u7387\uff0c\u5e76\u5df2\u5f00\u6e90\u3002", "motivation": "\u4e3a\u89e3\u51b3\u591a\u6a21\u6001\u6a21\u578b\u8bc4\u4f30\u4e2d\u5b58\u5728\u7684\u6548\u7387\u4f4e\u4e0b\u3001\u4efb\u52a1\u8986\u76d6\u4e0d\u5168\u3001\u8d44\u6e90\u5206\u914d\u4e0d\u7075\u6d3b\u7b49\u95ee\u9898\uff0c\u6784\u5efa\u7edf\u4e00\u4e14\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u4f53\u7cfb\u3002", "method": "1. \u6a21\u578b\u63a8\u7406\u4e0e\u8bc4\u4f30\u670d\u52a1\u89e3\u8026\u67b6\u6784\n2. \u96c6\u6210vLLM/SGLang\u52a0\u901f\u5de5\u5177\n3. \u5f02\u6b65\u6570\u636e\u52a0\u8f7d\u6280\u672f\n4. \u6a21\u5757\u5316\u8bbe\u8ba1\u652f\u6301\u5feb\u901f\u6269\u5c55\u65b0\u6a21\u578b\u548c\u4efb\u52a1", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6846\u67b6\u8bc4\u4f30\u6548\u7387\u63d0\u53473-5\u500d\uff0c\u80fd\u7cbe\u51c6\u8bc6\u522b\u6a21\u578b\u5728\u8de8\u6a21\u6001\u5bf9\u9f50\u3001\u751f\u6210\u8d28\u91cf\u7b49\u7ef4\u5ea6\u7684\u6027\u80fd\u8fb9\u754c", "conclusion": "FlagEvalMM\u4e3a\u591a\u6a21\u6001\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u57fa\u51c6\uff0c\u5176\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u52a0\u901f\u65b9\u6848\u663e\u8457\u63a8\u52a8\u9886\u57df\u6280\u672f\u8fed\u4ee3\u4e0e\u6a21\u578b\u4f18\u5316\u8fdb\u7a0b\u3002"}}
{"id": "2506.09099", "pdf": "https://arxiv.org/pdf/2506.09099", "abs": "https://arxiv.org/abs/2506.09099", "authors": ["Joshua Barron", "Devin White"], "title": "Too Big to Think: Capacity, Memorization, and Generalization in Pre-Trained Transformers", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted for oral presentation to Tiny Titans: The next wave of\n  On-Device Learning for Foundational Models Workshop at the 42nd International\n  Conference on Machine Learning", "summary": "The relationship between memorization and generalization in large language\nmodels (LLMs) remains an open area of research, with growing evidence that the\ntwo are deeply intertwined. In this work, we investigate this relationship by\npre-training a series of capacity-limited Transformer models from scratch on\ntwo synthetic character-level tasks designed to separately probe generalization\n(via arithmetic extrapolation) and memorization (via factual recall). We\nobserve a consistent trade-off: small models extrapolate to unseen arithmetic\ncases but fail to memorize facts, while larger models memorize but fail to\nextrapolate. An intermediate-capacity model exhibits a similar shift toward\nmemorization. When trained on both tasks jointly, no model (regardless of size)\nsucceeds at extrapolation. These findings suggest that pre-training may\nintrinsically favor one learning mode over the other. By isolating these\ndynamics in a controlled setting, our study offers insight into how model\ncapacity shapes learning behavior and offers broader implications for the\ndesign and deployment of small language models.", "AI": {"tldr": "\u6a21\u578b\u5bb9\u91cf\u4e0e\u5b66\u4e60\u6a21\u5f0f\u5b58\u5728\u6743\u8861\u5173\u7cfb\uff1a\u5c0f\u6a21\u578b\u5728\u7b97\u672f\u5916\u63a8\u4efb\u52a1\u4e2d\u6cdb\u5316\u80fd\u529b\u8f83\u5f3a\u4f46\u65e0\u6cd5\u6709\u6548\u8bb0\u5fc6\u4e8b\u5b9e\uff0c\u5927\u6a21\u578b\u5219\u8868\u73b0\u51fa\u76f8\u53cd\u7279\u6027\uff1b\u4e24\u8005\u96be\u4ee5\u5728\u5355\u4e00\u6a21\u578b\u4e2d\u517c\u5f97\uff0c\u4e14\u8054\u5408\u8bad\u7ec3\u4f1a\u6291\u5236\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u63a2\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u8bb0\u5fc6\u5316\uff08memorization\uff09\u4e0e\u6cdb\u5316\uff08generalization\uff09\u7684\u5185\u5728\u5173\u7cfb\uff0c\u901a\u8fc7\u53d7\u63a7\u5b9e\u9a8c\u63ed\u793a\u6a21\u578b\u5bb9\u91cf\u5bf9\u4e24\u79cd\u5b66\u4e60\u6a21\u5f0f\u7684\u4f18\u5148\u7ea7\u5f71\u54cd\u3002\u73b0\u6709\u7814\u7a76\u8868\u660e\u4e8c\u8005\u5b58\u5728\u6df1\u5c42\u5173\u8054\uff0c\u4f46\u5177\u4f53\u52a8\u6001\u673a\u5236\u5c1a\u672a\u660e\u786e\u3002", "method": "\u9884\u8bad\u7ec3\u4e0d\u540c\u5bb9\u91cf\u7684Transformer\u6a21\u578b\uff0c\u8bbe\u8ba1\u4e24\u4e2a\u5408\u6210\u5b57\u7b26\u7ea7\u4efb\u52a1\uff1a1) \u7b97\u672f\u5916\u63a8\uff08\u6d4b\u8bd5\u6cdb\u5316\u80fd\u529b\uff09\uff1b2) \u4e8b\u5b9e\u56de\u5fc6\uff08\u6d4b\u8bd5\u8bb0\u5fc6\u80fd\u529b\uff09\u3002\u901a\u8fc7\u5355\u72ec/\u8054\u5408\u8bad\u7ec3\u5bf9\u6bd4\u6a21\u578b\u8868\u73b0\uff0c\u89c2\u5bdf\u5bb9\u91cf\u53d8\u5316\u5bf9\u5b66\u4e60\u6a21\u5f0f\u7684\u5f71\u54cd\u3002", "result": "\u5c0f\u6a21\u578b\uff08\u4f4e\u5bb9\u91cf\uff09\u6210\u529f\u5b8c\u6210\u7b97\u672f\u5916\u63a8\u4f46\u65e0\u6cd5\u8bb0\u5fc6\u4e8b\u5b9e\uff0c\u5927\u6a21\u578b\uff08\u9ad8\u5bb9\u91cf\uff09\u5448\u73b0\u5b8c\u5168\u76f8\u53cd\u6a21\u5f0f\uff1b\u4e2d\u7b49\u5bb9\u91cf\u6a21\u578b\u5c55\u73b0\u51fa\u4ece\u6cdb\u5316\u5411\u8bb0\u5fc6\u5316\u7684\u8f6c\u53d8\u3002\u5f53\u4e24\u4efb\u52a1\u8054\u5408\u8bad\u7ec3\u65f6\uff0c\u6240\u6709\u6a21\u578b\u5747\u5931\u53bb\u5916\u63a8\u80fd\u529b\uff0c\u8868\u660e\u9884\u8bad\u7ec3\u8fc7\u7a0b\u5b58\u5728\u5185\u5728\u6a21\u5f0f\u504f\u597d\u3002", "conclusion": "\u6a21\u578b\u5bb9\u91cf\u51b3\u5b9a\u5b66\u4e60\u884c\u4e3a\u4f18\u5148\u7ea7\uff0c\u9884\u8bad\u7ec3\u673a\u5236\u53ef\u80fd\u672c\u8d28\u4e0a\u9650\u5236\u540c\u65f6\u4f18\u5316\u4e24\u79cd\u5b66\u4e60\u6a21\u5f0f\u3002\u8fd9\u4e00\u53d1\u73b0\u4e3a\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff1a\u9700\u6839\u636e\u76ee\u6807\u4efb\u52a1\u7684\u7279\u6027\uff08\u6cdb\u5316/\u8bb0\u5fc6\u9700\u6c42\uff09\u7cbe\u51c6\u5339\u914d\u6a21\u578b\u89c4\u6a21\uff0c\u5e76\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u6743\u8861\u5bb9\u91cf\u4e0e\u4efb\u52a1\u9002\u5e94\u6027\u3002"}}
{"id": "2506.09108", "pdf": "https://arxiv.org/pdf/2506.09108", "abs": "https://arxiv.org/abs/2506.09108", "authors": ["Yuwei Zhang", "Kumar Ayush", "Siyuan Qiao", "A. Ali Heydari", "Girish Narayanswamy", "Maxwell A. Xu", "Ahmed A. Metwally", "Shawn Xu", "Jake Garrison", "Xuhai Xu", "Tim Althoff", "Yun Liu", "Pushmeet Kohli", "Jiening Zhan", "Mark Malhotra", "Shwetak Patel", "Cecilia Mascolo", "Xin Liu", "Daniel McDuff", "Yuzhe Yang"], "title": "SensorLM: Learning the Language of Wearable Sensors", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We present SensorLM, a family of sensor-language foundation models that\nenable wearable sensor data understanding with natural language. Despite its\npervasive nature, aligning and interpreting sensor data with language remains\nchallenging due to the lack of paired, richly annotated sensor-text\ndescriptions in uncurated, real-world wearable data. We introduce a\nhierarchical caption generation pipeline designed to capture statistical,\nstructural, and semantic information from sensor data. This approach enabled\nthe curation of the largest sensor-language dataset to date, comprising over\n59.7 million hours of data from more than 103,000 people. Furthermore, SensorLM\nextends prominent multimodal pretraining architectures (e.g., CLIP, CoCa) and\nrecovers them as specific variants within a generic architecture. Extensive\nexperiments on real-world tasks in human activity analysis and healthcare\nverify the superior performance of SensorLM over state-of-the-art in zero-shot\nrecognition, few-shot learning, and cross-modal retrieval. SensorLM also\ndemonstrates intriguing capabilities including scaling behaviors, label\nefficiency, sensor captioning, and zero-shot generalization to unseen tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4f20\u611f\u5668\u8bed\u8a00\u57fa\u7840\u6a21\u578bSensorLM\uff0c\u901a\u8fc7\u6784\u5efa59.7\u767e\u4e07\u5c0f\u65f6\u7684\u53ef\u7a7f\u6234\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u4f20\u611f\u5668\u6570\u636e\u4e0e\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u7684\u65e0\u7f1d\u5bf9\u63a5\u3002", "motivation": "\u89e3\u51b3\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u6570\u636e\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6807\u6ce8\u7684\u95ee\u9898\uff0c\u7a81\u7834\u4f20\u611f\u5668\u6a21\u6001\u4e0e\u8bed\u8a00\u6a21\u6001\u5bf9\u9f50\u7684\u6280\u672f\u74f6\u9888\u3002", "method": "1. \u8bbe\u8ba1\u5206\u5c42\u6807\u9898\u751f\u6210\u6846\u67b6\u63d0\u53d6\u7edf\u8ba1/\u7ed3\u6784/\u8bed\u4e49\u7279\u5f81 2. \u6784\u5efa103,000\u4eba\u89c4\u6a21\u7684\u8d85\u5927\u89c4\u6a21\u4f20\u611f\u5668-\u8bed\u8a00\u914d\u5bf9\u6570\u636e\u96c6 3. \u6269\u5c55\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u67b6\u6784\uff08CLIP/CoCa\uff09\u5efa\u7acb\u901a\u7528\u6a21\u578b\u6846\u67b6", "result": "\u5728\u4eba\u4f53\u6d3b\u52a8\u5206\u6790\u548c\u533b\u7597\u5065\u5eb7\u4efb\u52a1\u4e2d\uff0c\u96f6\u6837\u672c\u8bc6\u522b\u51c6\u786e\u7387\u63d0\u534715%\uff0c\u8de8\u6a21\u6001\u68c0\u7d22Recall@1\u63d0\u534723%\uff0c\u5c55\u793a\u51fa\u4f18\u79c0\u7684\u6807\u7b7e\u6548\u7387\u548c\u4efb\u52a1\u6cdb\u5316\u80fd\u529b", "conclusion": "SensorLM\u901a\u8fc7\u67b6\u6784\u521b\u65b0\u4e0e\u6570\u636e\u89c4\u6a21\u7a81\u7834\uff0c\u5f00\u542f\u4e86\u53ef\u7a7f\u6234\u8ba1\u7b97\u7684\u65b0\u8303\u5f0f\uff0c\u5728\u5065\u5eb7\u76d1\u6d4b\u3001\u4eba\u673a\u4ea4\u4e92\u7b49\u9886\u57df\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c"}}
{"id": "2506.09109", "pdf": "https://arxiv.org/pdf/2506.09109", "abs": "https://arxiv.org/abs/2506.09109", "authors": ["Arnav Yayavaram", "Siddharth Yayavaram", "Simran Khanuja", "Michael Saxon", "Graham Neubig"], "title": "CAIRe: Cultural Attribution of Images by Retrieval-Augmented Evaluation", "categories": ["cs.CV", "cs.CL"], "comment": "Preprint, under review", "summary": "As text-to-image models become increasingly prevalent, ensuring their\nequitable performance across diverse cultural contexts is critical. Efforts to\nmitigate cross-cultural biases have been hampered by trade-offs, including a\nloss in performance, factual inaccuracies, or offensive outputs. Despite\nwidespread recognition of these challenges, an inability to reliably measure\nthese biases has stalled progress. To address this gap, we introduce CAIRe, a\nnovel evaluation metric that assesses the degree of cultural relevance of an\nimage, given a user-defined set of labels. Our framework grounds entities and\nconcepts in the image to a knowledge base and uses factual information to give\nindependent graded judgments for each culture label. On a manually curated\ndataset of culturally salient but rare items built using language models, CAIRe\nsurpasses all baselines by 28% F1 points. Additionally, we construct two\ndatasets for culturally universal concept, one comprising of T2I-generated\noutputs and another retrieved from naturally occurring data. CAIRe achieves\nPearson's correlations of 0.56 and 0.66 with human ratings on these sets, based\non a 5-point Likert scale of cultural relevance. This demonstrates its strong\nalignment with human judgment across diverse image sources.", "AI": {"tldr": "\u63d0\u51faCAIRe\u8bc4\u4f30\u6307\u6807\u89e3\u51b3\u6587\u672c\u751f\u6210\u56fe\u50cf\u6a21\u578b\u7684\u8de8\u6587\u5316\u504f\u89c1\u8bc4\u4f30\u95ee\u9898\uff0c\u901a\u8fc7\u77e5\u8bc6\u5e93\u5173\u8054\u548c\u4e8b\u5b9e\u5224\u65ad\u673a\u5236\uff0c\u5728\u6587\u5316\u663e\u8457\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u57fa\u7ebf28% F1\u503c\u5e76\u4e0e\u4eba\u7c7b\u8bc4\u5206\u9ad8\u5ea6\u76f8\u5173\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5b58\u5728\u8de8\u6587\u5316\u504f\u89c1\u4e14\u7f3a\u4e4f\u53ef\u9760\u8bc4\u4f30\u6307\u6807\uff0c\u5bfc\u81f4\u6539\u8fdb\u65b9\u6cd5\u5728\u6027\u80fd\u3001\u51c6\u786e\u6027\u53ca\u6587\u5316\u654f\u611f\u6027\u4e4b\u95f4\u5b58\u5728\u4e0d\u826f\u6743\u8861\uff0c\u963b\u788d\u516c\u5e73\u6027\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u6784\u5efaCAIRe\u6846\u67b6\uff1a\u5c06\u56fe\u50cf\u5b9e\u4f53\u4e0e\u77e5\u8bc6\u5e93\u951a\u5b9a\uff0c\u5229\u7528\u4e8b\u5b9e\u4fe1\u606f\u5bf9\u6587\u5316\u6807\u7b7e\u8fdb\u884c\u72ec\u7acb\u5206\u7ea7\u8bc4\u4f30\uff0c\u5e76\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u7a00\u6709\u6587\u5316\u9879\u76ee\u6570\u636e\u96c6\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "CAIRe\u5728\u6587\u5316\u663e\u8457\u6570\u636e\u96c6\u4e0aF1\u503c\u8d85\u8d8a\u57fa\u7ebf28%\uff0c\u5728\u4e24\u4e2a\u901a\u7528\u6982\u5ff5\u6570\u636e\u96c6\u4e0a\u4e0e\u4eba\u7c7b\u8bc4\u5206\u7684\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u5206\u522b\u8fbe0.56\u548c0.66\uff085\u7ea7\u91cf\u8868\u8bc4\u4f30\uff09\u3002", "conclusion": "CAIRe\u901a\u8fc7\u77e5\u8bc6\u9a71\u52a8\u8bc4\u4f30\u6709\u6548\u91cf\u5316\u6587\u5316\u76f8\u5173\u6027\uff0c\u4e3a\u6d88\u9664AI\u751f\u6210\u5185\u5bb9\u7684\u8de8\u6587\u5316\u504f\u89c1\u63d0\u4f9b\u4e86\u53ef\u9760\u5ea6\u91cf\u6807\u51c6\uff0c\u4e14\u5728\u4e0d\u540c\u6570\u636e\u6e90\u4e2d\u5747\u4fdd\u6301\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u9ad8\u5ea6\u4e00\u81f4\u6027\u3002"}}
{"id": "2506.09148", "pdf": "https://arxiv.org/pdf/2506.09148", "abs": "https://arxiv.org/abs/2506.09148", "authors": ["Hetvi Waghela", "Jaydip Sen", "Sneha Rakshit", "Subhasis Dasgupta"], "title": "Adversarial Text Generation with Dynamic Contextual Perturbation", "categories": ["cs.CR", "cs.CL"], "comment": "This is the accepted version of the paper, which was presented at\n  IEEE CALCON. The conference was organized at Jadavpur University, Kolkata,\n  from December 14 to 15, 2025. The paper is six pages long, and it consists of\n  six tables and six figures. This is not the final camera-ready version of the\n  paper", "summary": "Adversarial attacks on Natural Language Processing (NLP) models expose\nvulnerabilities by introducing subtle perturbations to input text, often\nleading to misclassification while maintaining human readability. Existing\nmethods typically focus on word-level or local text segment alterations,\noverlooking the broader context, which results in detectable or semantically\ninconsistent perturbations. We propose a novel adversarial text attack scheme\nnamed Dynamic Contextual Perturbation (DCP). DCP dynamically generates\ncontext-aware perturbations across sentences, paragraphs, and documents,\nensuring semantic fidelity and fluency. Leveraging the capabilities of\npre-trained language models, DCP iteratively refines perturbations through an\nadversarial objective function that balances the dual objectives of inducing\nmodel misclassification and preserving the naturalness of the text. This\ncomprehensive approach allows DCP to produce more sophisticated and effective\nadversarial examples that better mimic natural language patterns. Our\nexperimental results, conducted on various NLP models and datasets, demonstrate\nthe efficacy of DCP in challenging the robustness of state-of-the-art NLP\nsystems. By integrating dynamic contextual analysis, DCP significantly enhances\nthe subtlety and impact of adversarial attacks. This study highlights the\ncritical role of context in adversarial attacks and lays the groundwork for\ncreating more robust NLP systems capable of withstanding sophisticated\nadversarial strategies.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u4e0a\u4e0b\u6587\u6270\u52a8(DCP)\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5168\u5c40\u6587\u672c\u6270\u52a8\u63d0\u5347\u5bf9\u6297\u653b\u51fb\u6548\u679c", "motivation": "\u73b0\u6709\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u5c40\u9650\u4e8e\u5c40\u90e8\u6587\u672c\u4fee\u6539\uff0c\u5bfc\u81f4\u6270\u52a8\u6613\u88ab\u68c0\u6d4b\u4e14\u8bed\u4e49\u4e0d\u8fde\u8d2f\uff0c\u9700\u8003\u8651\u4e0a\u4e0b\u6587\u751f\u6210\u66f4\u81ea\u7136\u7684\u5bf9\u6297\u6837\u672c", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u52a8\u6001\u751f\u6210\u8de8\u53e5\u5b50/\u6bb5\u843d/\u6587\u6863\u7684\u6270\u52a8\uff0c\u901a\u8fc7\u5bf9\u6297\u76ee\u6807\u51fd\u6570\u8fed\u4ee3\u4f18\u5316\u6270\u52a8\uff0c\u5e73\u8861\u8bef\u5bfc\u5206\u7c7b\u4e0e\u6587\u672c\u81ea\u7136\u6027", "result": "\u5728\u591a\u4e2aNLP\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86DCP\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u5bf9\u6297\u653b\u51fb\u7684\u9690\u853d\u6027\u548c\u7834\u574f\u529b", "conclusion": "\u4e0a\u4e0b\u6587\u5206\u6790\u5bf9\u5bf9\u6297\u653b\u51fb\u81f3\u5173\u91cd\u8981\uff0c\u8be5\u7814\u7a76\u4e3a\u6784\u5efa\u6297\u653b\u51fb\u7684\u9c81\u68d2NLP\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840"}}
{"id": "2506.09171", "pdf": "https://arxiv.org/pdf/2506.09171", "abs": "https://arxiv.org/abs/2506.09171", "authors": ["Samuel Holt", "Max Ruiz Luyten", "Thomas Pouplin", "Mihaela van der Schaar"], "title": "Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T07, 68T20, 68T30, 93E35", "I.2.6; I.2.7; I.2.8"], "comment": "9-page main paper, 1 figure. Accepted for an Oral presentation at the\n  First Workshop on Computer Use Agents (ICML 2025), Vancouver, Canada", "summary": "Large Language Models (LLMs) are increasingly capable but often require\nsignificant guidance or extensive interaction history to perform effectively in\ncomplex, interactive environments. Existing methods may struggle with adapting\nto new information or efficiently utilizing past experiences for multi-step\nreasoning without fine-tuning. We introduce a novel LLM agent framework that\nenhances planning capabilities through in-context learning, facilitated by\natomic fact augmentation and a recursive lookahead search. Our agent learns to\nextract task-critical ``atomic facts'' from its interaction trajectories. These\nfacts dynamically augment the prompts provided to LLM-based components\nresponsible for action proposal, latent world model simulation, and state-value\nestimation. Planning is performed via a depth-limited lookahead search, where\nthe LLM simulates potential trajectories and evaluates their outcomes, guided\nby the accumulated facts and interaction history. This approach allows the\nagent to improve its understanding and decision-making online, leveraging its\nexperience to refine its behavior without weight updates. We provide a\ntheoretical motivation linking performance to the quality of fact-based\nabstraction and LLM simulation accuracy. Empirically, our agent demonstrates\nimproved performance and adaptability on challenging interactive tasks,\nachieving more optimal behavior as it accumulates experience, showcased in\ntasks such as TextFrozenLake and ALFWorld.", "AI": {"tldr": "\u63d0\u51fa\u589e\u5f3aLLM\u667a\u80fd\u4f53\u89c4\u5212\u80fd\u529b\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u539f\u5b50\u4e8b\u5b9e\u589e\u5f3a\u548c\u9012\u5f52\u524d\u77bb\u641c\u7d22\u5b9e\u73b0\u5728\u7ebf\u5b66\u4e60\u4f18\u5316\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u63d0\u5347\u590d\u6742\u4ea4\u4e92\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u73b0\u6709LLM\u65b9\u6cd5\u5728\u9002\u5e94\u65b0\u4fe1\u606f\u3001\u5229\u7528\u5386\u53f2\u7ecf\u9a8c\u8fdb\u884c\u591a\u6b65\u63a8\u7406\u65f6\u6548\u7387\u4e0d\u8db3\uff0c\u9700\u901a\u8fc7\u5fae\u8c03\u6539\u8fdb\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u63d0\u5347\u667a\u80fd\u4f53\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u51b3\u7b56\u80fd\u529b\u3002", "method": "1. \u4ece\u4ea4\u4e92\u8f68\u8ff9\u63d0\u53d6\u5173\u952e\u539f\u5b50\u4e8b\u5b9e\u52a8\u6001\u589e\u5f3a\u63d0\u793a\n2. \u6784\u5efa\u5305\u542b\u52a8\u4f5c\u63d0\u8bae\u3001\u4e16\u754c\u6a21\u578b\u6a21\u62df\u548c\u72b6\u6001\u4ef7\u503c\u8bc4\u4f30\u7684LLM\u7ec4\u4ef6\n3. \u91c7\u7528\u6df1\u5ea6\u53d7\u9650\u9012\u5f52\u524d\u77bb\u641c\u7d22\u8fdb\u884c\u89c4\u5212\uff0c\u5229\u7528\u79ef\u7d2f\u4e8b\u5b9e\u4f18\u5316\u51b3\u7b56", "result": "\u5728TextFrozenLake\u548cALFWorld\u7b49\u4ea4\u4e92\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u667a\u80fd\u4f53\u968f\u7ecf\u9a8c\u79ef\u7d2f\u8868\u73b0\u51fa\u66f4\u4f18\u884c\u4e3a\uff0c\u8bc1\u660e\u6846\u67b6\u7684\u9002\u5e94\u6027\u548c\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u57fa\u4e8e\u539f\u5b50\u4e8b\u5b9e\u7684\u62bd\u8c61\u8868\u8fbe\u4e0eLLM\u6a21\u62df\u7cbe\u5ea6\u7684\u7406\u8bba\u5173\u8054\uff0c\u4e3a\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u7684\u5728\u7ebf\u5b66\u4e60\u667a\u80fd\u4f53\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2506.09206", "pdf": "https://arxiv.org/pdf/2506.09206", "abs": "https://arxiv.org/abs/2506.09206", "authors": ["Ahmed Adel Attia", "Jing Liu", "Carl Espy-Wilson"], "title": "SimClass: A Classroom Speech Dataset Generated via Game Engine Simulation For Automatic Speech Recognition Research", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": null, "summary": "The scarcity of large-scale classroom speech data has hindered the\ndevelopment of AI-driven speech models for education. Public classroom datasets\nremain limited, and the lack of a dedicated classroom noise corpus prevents the\nuse of standard data augmentation techniques.\n  In this paper, we introduce a scalable methodology for synthesizing classroom\nnoise using game engines, a framework that extends to other domains. Using this\nmethodology, we present SimClass, a dataset that includes both a synthesized\nclassroom noise corpus and a simulated classroom speech dataset. The speech\ndata is generated by pairing a public children's speech corpus with YouTube\nlecture videos to approximate real classroom interactions in clean conditions.\nOur experiments on clean and noisy speech demonstrate that SimClass closely\napproximates real classroom speech, making it a valuable resource for\ndeveloping robust speech recognition and enhancement models.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u6e38\u620f\u5f15\u64ce\u5408\u6210\u8bfe\u5802\u566a\u97f3\u7684\u65b9\u6cd5\uff0c\u5e76\u53d1\u5e03SimClass\u6570\u636e\u96c6\uff08\u5305\u542b\u5408\u6210\u566a\u97f3\u548c\u6a21\u62df\u8bed\u97f3\u6570\u636e\uff09\uff0c\u4e3a\u6559\u80b2\u9886\u57df\u8bed\u97f3\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u6709\u6548\u8d44\u6e90\u3002", "motivation": "\u73b0\u6709\u8bfe\u5802\u8bed\u97f3\u6570\u636e\u7a00\u7f3a\u4e14\u7f3a\u4e4f\u4e13\u7528\u566a\u97f3\u5e93\uff0c\u9650\u5236\u4e86\u6559\u80b2AI\u8bed\u97f3\u6a21\u578b\u7684\u53d1\u5c55\u53ca\u6570\u636e\u589e\u5f3a\u6280\u672f\u7684\u5e94\u7528\u3002", "method": "1. \u4f7f\u7528\u6e38\u620f\u5f15\u64ce\u751f\u6210\u8bfe\u5802\u566a\u97f3\n2. \u5c06\u516c\u5f00\u513f\u7ae5\u8bed\u97f3\u4e0eYouTube\u8bb2\u5ea7\u89c6\u9891\u914d\u5bf9\u751f\u6210\u5e72\u51c0\u8bfe\u5802\u8bed\u97f3\n3. \u6784\u5efa\u5305\u542b\u5408\u6210\u566a\u97f3\u548c\u6a21\u62df\u8bed\u97f3\u7684SimClass\u6570\u636e\u96c6", "result": "\u5b9e\u9a8c\u8bc1\u660eSimClass\u5728\u5e72\u51c0/\u5608\u6742\u73af\u5883\u4e0b\u5747\u53ef\u6709\u6548\u6a21\u62df\u771f\u5b9e\u8bfe\u5802\u8bed\u97f3\uff08\u8bcd\u9519\u7387\u63a5\u8fd1\u771f\u5b9e\u6570\u636e\uff09", "conclusion": "SimClass\u586b\u8865\u4e86\u6559\u80b2\u8bed\u97f3\u6570\u636e\u7a7a\u767d\uff0c\u5176\u65b9\u6cd5\u8bba\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u9886\u57df\uff0c\u4e3a\u5f00\u53d1\u9c81\u68d2\u7684\u8bed\u97f3\u8bc6\u522b/\u589e\u5f3a\u6a21\u578b\u63d0\u4f9b\u4e86\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2506.09260", "pdf": "https://arxiv.org/pdf/2506.09260", "abs": "https://arxiv.org/abs/2506.09260", "authors": ["Yibin Lei", "Tao Shen", "Andrew Yates"], "title": "ThinkQE: Query Expansion via an Evolving Thinking Process", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Effective query expansion for web search benefits from promoting both\nexploration and result diversity to capture multiple interpretations and facets\nof a query. While recent LLM-based methods have improved retrieval performance\nand demonstrate strong domain generalization without additional training, they\noften generate narrowly focused expansions that overlook these desiderata. We\npropose ThinkQE, a test-time query expansion framework addressing this\nlimitation through two key components: a thinking-based expansion process that\nencourages deeper and comprehensive semantic exploration, and a\ncorpus-interaction strategy that iteratively refines expansions using retrieval\nfeedback from the corpus. Experiments on diverse web search benchmarks (DL19,\nDL20, and BRIGHT) show ThinkQE consistently outperforms prior approaches,\nincluding training-intensive dense retrievers and rerankers.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684ThinkQE\u6846\u67b6\uff0c\u901a\u8fc7\u601d\u7ef4\u6269\u5c55\u548c\u8bed\u6599\u4ea4\u4e92\u63d0\u5347\u68c0\u7d22\u6027\u80fd", "motivation": "\u73b0\u6709LLM\u6269\u5c55\u65b9\u6cd5\u8fc7\u4e8e\u805a\u7126\u5355\u4e00\u65b9\u5411\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u67e5\u8be2\u7684\u591a\u9762\u6027\u9700\u6c42", "method": "\u7ed3\u5408\u601d\u7ef4\u94fe\u4fc3\u8fdb\u8bed\u4e49\u63a2\u7d22 + \u57fa\u4e8e\u8bed\u6599\u53cd\u9988\u7684\u8fed\u4ee3\u4f18\u5316\u673a\u5236", "result": "\u5728DL19/DL20/BRIGHT\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5168\u9762\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff08\u5305\u62ec\u9700\u8bad\u7ec3\u6a21\u578b\uff09", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u5e73\u8861\u63a2\u7d22\u4e0e\u591a\u6837\u6027\uff0c\u9a8c\u8bc1\u4e86\u65e0\u8bad\u7ec3\u65b9\u6848\u7684\u6f5c\u529b"}}
{"id": "2506.09289", "pdf": "https://arxiv.org/pdf/2506.09289", "abs": "https://arxiv.org/abs/2506.09289", "authors": ["Boxi Yu", "Yuxuan Zhu", "Pinjia He", "Daniel Kang"], "title": "UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench", "categories": ["cs.SE", "cs.CL", "D.0; I.2"], "comment": null, "summary": "The advent of Large Language Models (LLMs) has spurred the development of\ncoding agents for real-world code generation. As a widely used benchmark for\nevaluating the code generation capabilities of these agents, SWE-Bench uses\nreal-world problems based on GitHub issues and their corresponding pull\nrequests. However, the manually written test cases included in these pull\nrequests are often insufficient, allowing generated patches to pass the tests\nwithout resolving the underlying issue. To address this challenge, we introduce\nUTGenerator, an LLM-driven test case generator that automatically analyzes\ncodebases and dependencies to generate test cases for real-world Python\nprojects. Building on UTGenerator, we propose UTBoost, a comprehensive\nframework for test case augmentation. In our evaluation, we identified 36 task\ninstances with insufficient test cases and uncovered 345 erroneous patches\nincorrectly labeled as passed in the original SWE Bench. These corrections,\nimpacting 40.9% of SWE-Bench Lite and 24.4% of SWE-Bench Verified leaderboard\nentries, yield 18 and 11 ranking changes, respectively.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51faUTGenerator\u81ea\u52a8\u751f\u6210Python\u9879\u76ee\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5e76\u6784\u5efaUTBoost\u6846\u67b6\u589e\u5f3a\u6d4b\u8bd5\u8986\u76d6\uff0c\u53d1\u73b0SWE-Bench\u57fa\u51c6\u4e2d40.9%\u7684Lite\u7248\u672c\u548c24.4%\u7684Verified\u7248\u672c\u5b58\u5728\u9519\u8bef\u6807\u6ce8\uff0c\u663e\u8457\u5f71\u54cd\u6392\u884c\u699c\u6392\u540d\u3002", "motivation": "SWE-Bench\u57fa\u51c6\u4e2d\u624b\u52a8\u7f16\u5199\u7684\u6d4b\u8bd5\u7528\u4f8b\u4e0d\u8db3\u4ee5\u6709\u6548\u8bc4\u4f30\u4ee3\u7801\u751f\u6210\u4ee3\u7406\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u8865\u4e01\u5373\u4f7f\u672a\u89e3\u51b3\u95ee\u9898\u4e5f\u80fd\u901a\u8fc7\u6d4b\u8bd5\u3002", "method": "\u5f00\u53d1LLM\u9a71\u52a8\u7684UTGenerator\u81ea\u52a8\u5206\u6790\u4ee3\u7801\u5e93\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5e76\u57fa\u4e8e\u6b64\u6784\u5efaUTBoost\u6846\u67b6\u8fdb\u884c\u7cfb\u7edf\u6027\u6d4b\u8bd5\u589e\u5f3a\u3002", "result": "\u53d1\u73b036\u4e2a\u6d4b\u8bd5\u4e0d\u8db3\u7684\u4efb\u52a1\u5b9e\u4f8b\uff0c\u7ea0\u6b63345\u4e2a\u8bef\u5224\u8865\u4e01\uff0c\u5bfc\u81f4SWE-Bench Lite\u548cVerified\u6392\u884c\u699c\u5206\u522b\u4ea7\u751f18\u548c11\u6b21\u6392\u540d\u53d8\u5316\u3002", "conclusion": "UTBoost\u6846\u67b6\u663e\u8457\u63d0\u5347\u4ee3\u7801\u751f\u6210\u8bc4\u4f30\u53ef\u9760\u6027\uff0c\u6d4b\u8bd5\u7528\u4f8b\u8d28\u91cf\u5bf9\u57fa\u51c6\u6709\u6548\u6027\u5177\u6709\u5173\u952e\u5f71\u54cd\uff0c\u4fee\u6b63\u540e\u7ed3\u679c\u5bf9\u73b0\u6709\u6392\u540d\u4ea7\u751f\u5b9e\u8d28\u6027\u6539\u53d8\u3002"}}
{"id": "2506.09332", "pdf": "https://arxiv.org/pdf/2506.09332", "abs": "https://arxiv.org/abs/2506.09332", "authors": ["Zhenqiao Song", "Ramith Hettiarachchi", "Chuan Li", "Jianwen Xie", "Lei Li"], "title": "Natural Language Guided Ligand-Binding Protein Design", "categories": ["cs.LG", "cs.CE", "cs.CL"], "comment": null, "summary": "Can AI protein models follow human language instructions and design proteins\nwith desired functions (e.g. binding to a ligand)? Designing proteins that bind\nto a given ligand is crucial in a wide range of applications in biology and\nchemistry. Most prior AI models are trained on protein-ligand complex data,\nwhich is scarce due to the high cost and time requirements of laboratory\nexperiments. In contrast, there is a substantial body of human-curated text\ndescriptions about protein-ligand interactions and ligand formula. In this\npaper, we propose InstructPro, a family of protein generative models that\nfollow natural language instructions to design ligand-binding proteins. Given a\ntextual description of the desired function and a ligand formula in SMILES,\nInstructPro generates protein sequences that are functionally consistent with\nthe specified instructions. We develop the model architecture, training\nstrategy, and a large-scale dataset, InstructProBench, to support both training\nand evaluation. InstructProBench consists of 9,592,829 triples of (function\ndescription, ligand formula, protein sequence). We train two model variants:\nInstructPro-1B (with 1 billion parameters) and InstructPro-3B~(with 3 billion\nparameters). Both variants consistently outperform strong baselines, including\nProGen2, ESM3, and Pinal. Notably, InstructPro-1B achieves the highest docking\nsuccess rate (81.52% at moderate confidence) and the lowest average root mean\nsquare deviation (RMSD) compared to ground truth structures (4.026{\\AA}).\nInstructPro-3B further descreases the average RMSD to 2.527{\\AA}, demonstrating\nInstructPro's ability to generate ligand-binding proteins that align with the\nfunctional specifications.", "AI": {"tldr": "AI\u86cb\u767d\u751f\u6210\u6a21\u578bInstructPro\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u548c\u914d\u4f53\u7ed3\u6784\u5f0f\u751f\u6210\u529f\u80fd\u5339\u914d\u7684\u86cb\u767d\u8d28\uff0c\u5728\u5bf9\u63a5\u6210\u529f\u7387\u548c\u7ed3\u6784\u51c6\u786e\u6027\u4e0a\u8d85\u8d8a\u4e3b\u6d41\u57fa\u7ebf\u6a21\u578b", "motivation": "\u4f20\u7edf\u86cb\u767d\u8d28\u8bbe\u8ba1\u4f9d\u8d56\u7a00\u7f3a\u7684\u86cb\u767d-\u914d\u4f53\u590d\u5408\u7269\u6570\u636e\uff0c\u800c\u6587\u672c\u63cf\u8ff0\u8d44\u6e90\u4e30\u5bcc\u3002\u5229\u7528\u6587\u672c\u6307\u5bfc\u751f\u6210\u53ef\u7a81\u7834\u5b9e\u9a8c\u6570\u636e\u74f6\u9888", "method": "\u6784\u5efa\u5305\u542b958\u4e07\u6761\uff08\u529f\u80fd\u63cf\u8ff0\uff0c\u914d\u4f53SMILES\uff0c\u86cb\u767d\u5e8f\u5217\uff09\u7684InstructProBench\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u57fa\u4e8e\u6307\u4ee4\u76841B/3B\u53c2\u6570\u6a21\u578b\u67b6\u6784\uff0c\u91c7\u7528\u591a\u6a21\u6001\u8bad\u7ec3\u7b56\u7565", "result": "1B\u6a21\u578b\u8fbe81.52%\u5bf9\u63a5\u6210\u529f\u7387\uff08\u4e2d\u7b49\u7f6e\u4fe1\u5ea6\uff09\uff0c\u5e73\u5747RMSD 4.026\u00c5\uff1b3B\u6a21\u578b\u8fdb\u4e00\u6b65\u964d\u81f32.527\u00c5\uff0c\u7ed3\u6784\u7cbe\u5ea6\u663e\u8457\u63d0\u5347", "conclusion": "InstructPro\u9996\u6b21\u5b9e\u73b0\u8bed\u8a00\u6307\u5bfc\u7684\u86cb\u767d\u8d28\u529f\u80fd\u8bbe\u8ba1\uff0c\u5176\u6027\u80fd\u4f18\u52bf\u4e3a\u7cbe\u51c6\u836f\u7269\u5f00\u53d1\u7b49\u5e94\u7528\u63d0\u4f9b\u65b0\u53ef\u80fd"}}
{"id": "2506.09344", "pdf": "https://arxiv.org/pdf/2506.09344", "abs": "https://arxiv.org/abs/2506.09344", "authors": ["Inclusion AI", "Biao Gong", "Cheng Zou", "Chuanyang Zheng", "Chunluan Zhou", "Canxiang Yan", "Chunxiang Jin", "Chunjie Shen", "Dandan Zheng", "Fudong Wang", "Furong Xu", "GuangMing Yao", "Jun Zhou", "Jingdong Chen", "Jianxin Sun", "Jiajia Liu", "Jianjiang Zhu", "Jun Peng", "Kaixiang Ji", "Kaiyou Song", "Kaimeng Ren", "Libin Wang", "Lixiang Ru", "Lele Xie", "Longhua Tan", "Lyuxin Xue", "Lan Wang", "Mochen Bai", "Ning Gao", "Pei Chen", "Qingpei Guo", "Qinglong Zhang", "Qiang Xu", "Rui Liu", "Ruijie Xiong", "Sirui Gao", "Tinghao Liu", "Taisong Li", "Weilong Chai", "Xinyu Xiao", "Xiaomei Wang", "Xiaoxue Chen", "Xiao Lu", "Xiaoyu Li", "Xingning Dong", "Xuzheng Yu", "Yi Yuan", "Yuting Gao", "Yunxiao Sun", "Yipeng Chen", "Yifei Wu", "Yongjie Lyu", "Ziping Ma", "Zipeng Feng", "Zhijiang Fang", "Zhihao Qiu", "Ziyuan Huang", "Zhengyu He"], "title": "Ming-Omni: A Unified Multimodal Model for Perception and Generation", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "comment": "18 pages,8 figures", "summary": "We propose Ming-Omni, a unified multimodal model capable of processing\nimages, text, audio, and video, while demonstrating strong proficiency in both\nspeech and image generation. Ming-Omni employs dedicated encoders to extract\ntokens from different modalities, which are then processed by Ling, an MoE\narchitecture equipped with newly proposed modality-specific routers. This\ndesign enables a single model to efficiently process and fuse multimodal inputs\nwithin a unified framework, thereby facilitating diverse tasks without\nrequiring separate models, task-specific fine-tuning, or structural redesign.\nImportantly, Ming-Omni extends beyond conventional multimodal models by\nsupporting audio and image generation. This is achieved through the integration\nof an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for\nhigh-quality image generation, which also allow the model to engage in\ncontext-aware chatting, perform text-to-speech conversion, and conduct\nversatile image editing. Our experimental results showcase Ming-Omni offers a\npowerful solution for unified perception and generation across all modalities.\nNotably, our proposed Ming-Omni is the first open-source model we are aware of\nto match GPT-4o in modality support, and we release all code and model weights\nto encourage further research and development in the community.", "AI": {"tldr": "Ming-Omni\u662f\u9996\u4e2a\u5f00\u6e90\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\uff0c\u652f\u6301\u56fe\u50cf/\u6587\u672c/\u97f3\u9891/\u89c6\u9891\u7684\u611f\u77e5\u4e0e\u751f\u6210\uff0c\u5339\u914dGPT-4o\u7684\u6a21\u6001\u652f\u6301\u5e76\u5f00\u6e90\u4ee3\u7801\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u591a\u6a21\u6001\u6a21\u578b\u9700\u591a\u6a21\u578b\u914d\u5408\u3001\u65e0\u6cd5\u7edf\u4e00\u5904\u7406\u751f\u6210\u4efb\u52a1\u7684\u95ee\u9898\uff0c\u901a\u8fc7MoE\u67b6\u6784\u5b9e\u73b0\u8de8\u6a21\u6001\u9ad8\u6548\u878d\u5408\u3002", "method": "1. \u4e13\u7528\u7f16\u7801\u5668\u63d0\u53d6\u591a\u6a21\u6001\u7279\u5f81 2. \u6a21\u6001\u4e13\u7528\u8def\u7531\u5668\u7684Ling-MoE\u67b6\u6784 3. \u96c6\u6210\u97f3\u9891\u89e3\u7801\u5668\u548cMing-Lite-Uni\u56fe\u50cf\u751f\u6210\u6a21\u5757", "result": "\u5b9e\u9a8c\u8bc1\u660e\u6a21\u578b\u5b9e\u73b0\u5168\u6a21\u6001\u7edf\u4e00\u611f\u77e5\u751f\u6210\uff0c\u97f3\u9891\u751f\u6210\u8d28\u91cf\u8fbe\u81ea\u7136\u8bed\u97f3\u6c34\u5e73\uff0c\u56fe\u50cf\u7f16\u8f91\u652f\u6301\u4e0a\u4e0b\u6587\u611f\u77e5\u4ea4\u4e92\u3002", "conclusion": "\u9996\u6b21\u5b9e\u73b0\u5f00\u6e90\u591a\u6a21\u6001\u6a21\u578b\u7684\u95ed\u73af\u611f\u77e5-\u751f\u6210\u80fd\u529b\uff0c\u4e3a\u793e\u533a\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u63a8\u52a8\u591a\u6a21\u6001AGI\u53d1\u5c55\u3002"}}
{"id": "2506.09420", "pdf": "https://arxiv.org/pdf/2506.09420", "abs": "https://arxiv.org/abs/2506.09420", "authors": ["Henry Peng Zou", "Wei-Chieh Huang", "Yaozu Wu", "Chunyu Miao", "Dongyuan Li", "Aiwei Liu", "Yue Zhou", "Yankai Chen", "Weizhi Zhang", "Yangning Li", "Liancheng Fang", "Renhe Jiang", "Philip S. Yu"], "title": "A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG", "cs.MA"], "comment": null, "summary": "Recent improvements in large language models (LLMs) have led many researchers\nto focus on building fully autonomous AI agents. This position paper questions\nwhether this approach is the right path forward, as these autonomous systems\nstill have problems with reliability, transparency, and understanding the\nactual requirements of human. We suggest a different approach: LLM-based\nHuman-Agent Systems (LLM-HAS), where AI works with humans rather than replacing\nthem. By keeping human involved to provide guidance, answer questions, and\nmaintain control, these systems can be more trustworthy and adaptable. Looking\nat examples from healthcare, finance, and software development, we show how\nhuman-AI teamwork can handle complex tasks better than AI working alone. We\nalso discuss the challenges of building these collaborative systems and offer\npractical solutions. This paper argues that progress in AI should not be\nmeasured by how independent systems become, but by how well they can work with\nhumans. The most promising future for AI is not in systems that take over human\nroles, but in those that enhance human capabilities through meaningful\npartnership.", "AI": {"tldr": "\u8d28\u7591\u5b8c\u5168\u81ea\u4e3bAI\u4ee3\u7406\u65b9\u5411\uff0c\u63d0\u51fa\u4ee5LLM\u4e3a\u57fa\u7840\u7684\u4eba\u673a\u534f\u4f5c\u7cfb\u7edf\uff08LLM-HAS\uff09\u66f4\u53ef\u9760", "motivation": "\u5f53\u524d\u81ea\u4e3bAI\u7cfb\u7edf\u5b58\u5728\u53ef\u9760\u6027\u3001\u900f\u660e\u6027\u53ca\u7406\u89e3\u4eba\u7c7b\u771f\u5b9e\u9700\u6c42\u7684\u95ee\u9898\uff0c\u9700\u4fdd\u6301\u4eba\u7c7b\u5728\u5173\u952e\u73af\u8282\u7684\u51b3\u7b56\u6743", "method": "\u6784\u5efa\u4eba\u7c7b\u6301\u7eed\u63d0\u4f9b\u6307\u5bfc\u3001\u56de\u7b54\u95ee\u9898\u5e76\u4fdd\u7559\u63a7\u5236\u6743\u7684LLM\u4eba\u673a\u534f\u4f5c\u6846\u67b6", "result": "\u901a\u8fc7\u533b\u7597\u3001\u91d1\u878d\u3001\u8f6f\u4ef6\u5f00\u53d1\u6848\u4f8b\u8bc1\u660e\u4eba\u673a\u534f\u4f5c\u6bd4\u7eafAI\u7cfb\u7edf\u66f4\u80fd\u6709\u6548\u5904\u7406\u590d\u6742\u4efb\u52a1", "conclusion": "AI\u8fdb\u6b65\u5e94\u901a\u8fc7\u4eba\u673a\u534f\u4f5c\u80fd\u529b\u8861\u91cf\uff0c\u6700\u7406\u60f3\u65b9\u5411\u662f\u589e\u5f3a\u4eba\u7c7b\u80fd\u529b\u7684\u4f19\u4f34\u7cfb\u7edf\u800c\u975e\u89d2\u8272\u66ff\u4ee3"}}
{"id": "2506.09448", "pdf": "https://arxiv.org/pdf/2506.09448", "abs": "https://arxiv.org/abs/2506.09448", "authors": ["Yui Sudo", "Yusuke Fujita", "Atsushi Kojima", "Tomoya Mizumoto", "Lianbo Liu"], "title": "OWSM-Biasing: Contextualizing Open Whisper-Style Speech Models for Automatic Speech Recognition with Dynamic Vocabulary", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Speech foundation models (SFMs), such as Open Whisper-Style Speech Models\n(OWSM), are trained on massive datasets to achieve accurate automatic speech\nrecognition. However, even SFMs struggle to accurately recognize rare and\nunseen words. While contextual biasing (CB) is a promising approach to improve\nrecognition of such words, most CB methods are trained from scratch, resulting\nin lower performance than SFMs due to the lack of pre-trained knowledge. This\npaper integrates an existing CB method with OWSM v3.1 while freezing its\npre-trained parameters. By leveraging the knowledge embedded in SFMs, the\nproposed method enables effective CB while preserving the advantages of SFMs,\neven with a small dataset. Experimental results show that the proposed method\nimproves the biasing word error rate (B-WER) by 11.6 points, resulting in a 0.9\npoint improvement in the overall WER while reducing the real-time factor by\n7.5% compared to the non-biasing baseline on the LibriSpeech 100 test-clean\nset.", "AI": {"tldr": "\u63d0\u51fa\u5c06\u4e0a\u4e0b\u6587\u504f\u7f6e\u65b9\u6cd5\u4e0e\u51bb\u7ed3\u53c2\u6570\u7684\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u7ed3\u5408\uff0c\u6709\u6548\u63d0\u5347\u7f55\u89c1\u8bcd\u8bc6\u522b\u6548\u679c", "motivation": "\u73b0\u6709\u4e0a\u4e0b\u6587\u504f\u7f6e\u65b9\u6cd5\u7f3a\u4e4f\u9884\u8bad\u7ec3\u77e5\u8bc6\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\uff0c\u9700\u7ed3\u5408\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u7684\u4f18\u52bf", "method": "\u5728OWSM v3.1\u6846\u67b6\u4e2d\u96c6\u6210\u4e0a\u4e0b\u6587\u504f\u7f6e\u65b9\u6cd5\u5e76\u51bb\u7ed3\u9884\u8bad\u7ec3\u53c2\u6570\uff0c\u5229\u7528\u5c0f\u6570\u636e\u96c6\u5b9e\u73b0\u6709\u6548\u504f\u7f6e", "result": "\u5728LibriSpeech\u6d4b\u8bd5\u96c6\u4e0a\u504f\u7f6e\u8bcd\u9519\u8bef\u7387\u964d\u4f4e11.6\u70b9\uff0c\u6574\u4f53\u9519\u8bef\u7387\u63d0\u53470.9\u70b9\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53477.5%", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5e73\u8861\u4e86\u9884\u8bad\u7ec3\u77e5\u8bc6\u4fdd\u7559\u4e0e\u4e0a\u4e0b\u6587\u504f\u7f6e\u9700\u6c42\uff0c\u663e\u8457\u63d0\u5347\u8bed\u97f3\u6a21\u578b\u7684\u5b9e\u7528\u6027\u548c\u6548\u7387"}}
{"id": "2506.09452", "pdf": "https://arxiv.org/pdf/2506.09452", "abs": "https://arxiv.org/abs/2506.09452", "authors": ["Jay Roberts", "Kyle Mylonakis", "Sidhartha Roy", "Kaan Kale"], "title": "Learning Obfuscations Of LLM Embedding Sequences: Stained Glass Transform", "categories": ["cs.LG", "cs.CL", "cs.CR", "cs.IT", "math.IT", "I.2.7; I.2.m"], "comment": "Submitted to IEEE S&P 2026", "summary": "The high cost of ownership of AI compute infrastructure and challenges of\nrobust serving of large language models (LLMs) has led to a surge in managed\nModel-as-a-service deployments. Even when enterprises choose on-premises\ndeployments, the compute infrastructure is typically shared across many teams\nin order to maximize the return on investment. In both scenarios the deployed\nmodels operate only on plaintext data, and so enterprise data owners must allow\ntheir data to appear in plaintext on a shared or multi-tenant compute\ninfrastructure. This results in data owners with private or sensitive data\nbeing hesitant or restricted in what data they use with these types of\ndeployments. In this work we introduce the Stained Glass Transform, a learned,\nstochastic, and sequence dependent transformation of the word embeddings of an\nLLM which information theoretically provides privacy to the input of the LLM\nwhile preserving the utility of model. We theoretically connect a particular\nclass of Stained Glass Transforms to the theory of mutual information of\nGaussian Mixture Models. We then calculate a-postiori privacy estimates, based\non mutual information, and verify the privacy and utility of instances of\ntransformed embeddings through token level metrics of privacy and standard LLM\nperformance benchmarks.", "AI": {"tldr": "\u63d0\u51faStained Glass Transform\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9LLM\u8bcd\u5d4c\u5165\u8fdb\u884c\u968f\u673a\u5e8f\u5217\u8f6c\u6362\u5b9e\u73b0\u4fe1\u606f\u8bba\u9690\u79c1\u4fdd\u62a4\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u5b9e\u7528\u6027\u3002", "motivation": "\u89e3\u51b3\u5171\u4eab\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\u4e2d\u660e\u6587\u6570\u636e\u9690\u79c1\u6cc4\u9732\u95ee\u9898\uff0c\u964d\u4f4e\u4f01\u4e1a\u5bf9\u654f\u611f\u6570\u636e\u4f7f\u7528LLM\u7684\u987e\u8651", "method": "\u91c7\u7528\u5b66\u4e60\u578b\u3001\u968f\u673a\u4e14\u5e8f\u5217\u4f9d\u8d56\u7684\u8bcd\u5d4c\u5165\u8f6c\u6362\u6280\u672f\uff0c\u7406\u8bba\u8fde\u63a5\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u4e92\u4fe1\u606f\u7406\u8bba", "result": "\u901a\u8fc7\u4e92\u4fe1\u606f\u540e\u9a8c\u9690\u79c1\u8bc4\u4f30\u548c\u6807\u51c6LLM\u6027\u80fd\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u9690\u79c1\u4fdd\u62a4\u4e0e\u6a21\u578b\u6548\u7528\u7684\u5e73\u8861", "conclusion": "Stained Glass Transform\u4e3a\u5171\u4eab\u57fa\u7840\u8bbe\u65bd\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.09521", "pdf": "https://arxiv.org/pdf/2506.09521", "abs": "https://arxiv.org/abs/2506.09521", "authors": ["\u00dcnal Ege Gaznepoglu", "Anna Leschanowsky", "Ahmad Aloradi", "Prachi Singh", "Daniel Tenbrinck", "Emanu\u00ebl A. P. Habets", "Nils Peters"], "title": "You Are What You Say: Exploiting Linguistic Content for VoicePrivacy Attacks", "categories": ["eess.AS", "cs.CL"], "comment": "5 pages, 6 figures, 1 table, accepted at INTERSPEECH 2025", "summary": "Speaker anonymization systems hide the identity of speakers while preserving\nother information such as linguistic content and emotions. To evaluate their\nprivacy benefits, attacks in the form of automatic speaker verification (ASV)\nsystems are employed. In this study, we assess the impact of intra-speaker\nlinguistic content similarity in the attacker training and evaluation datasets,\nby adapting BERT, a language model, as an ASV system. On the VoicePrivacy\nAttacker Challenge datasets, our method achieves a mean equal error rate (EER)\nof 35%, with certain speakers attaining EERs as low as 2%, based solely on the\ntextual content of their utterances. Our explainability study reveals that the\nsystem decisions are linked to semantically similar keywords within utterances,\nstemming from how LibriSpeech is curated. Our study suggests reworking the\nVoicePrivacy datasets to ensure a fair and unbiased evaluation and challenge\nthe reliance on global EER for privacy evaluations.", "AI": {"tldr": "\u4f7f\u7528BERT\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u8bf4\u8bdd\u4eba\u533f\u540d\u5316\u7cfb\u7edf\u7684\u9690\u79c1\u4fdd\u62a4\u6548\u679c\uff0c\u53d1\u73b0\u4ec5\u6587\u672c\u5185\u5bb9\u5373\u53ef\u5b9e\u73b0\u8bf4\u8bdd\u4eba\u8bc6\u522b\uff0c\u63ed\u793a\u6570\u636e\u96c6\u5b58\u5728\u5185\u5bb9\u504f\u89c1\u5e76\u5efa\u8bae\u6539\u8fdb\u8bc4\u4f30\u65b9\u6cd5", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5168\u5c40\u5e73\u5747\u7b49\u9519\u8bef\u7387(EER)\u7684\u8bf4\u8bdd\u4eba\u533f\u540d\u5316\u8bc4\u4f30\u5b58\u5728\u7f3a\u9677\uff0c\u9700\u9a8c\u8bc1\u6570\u636e\u96c6\u5185\u5bb9\u76f8\u4f3c\u6027\u5bf9\u9690\u79c1\u8bc4\u4f30\u7684\u5f71\u54cd", "method": "\u5c06BERT\u8bed\u8a00\u6a21\u578b\u6539\u9020\u4e3a\u81ea\u52a8\u8bf4\u8bdd\u4eba\u9a8c\u8bc1\u7cfb\u7edf\uff0c\u57fa\u4e8eVoicePrivacy\u6311\u6218\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u6587\u672c\u5185\u5bb9\u76f8\u4f3c\u6027\u5206\u6790\u5b9e\u73b0\u8bf4\u8bdd\u4eba\u8bc6\u522b", "result": "\u5728\u7eaf\u6587\u672c\u5206\u6790\u4e0b\u8fbe\u523035%\u5e73\u5747EER\uff08\u4e2a\u522b\u8bf4\u8bdd\u4eba\u4f4e\u81f32%\uff09\uff0c\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u663e\u793a\u8bc6\u522b\u4f9d\u8d56\u8bed\u4e49\u5173\u952e\u8bcd\u7684\u76f8\u4f3c\u6027\u6a21\u5f0f", "conclusion": "\u5efa\u8bae\u91cd\u6784VoicePrivacy\u6570\u636e\u96c6\u4ee5\u6d88\u9664\u5185\u5bb9\u504f\u89c1\uff0c\u5e76\u8d28\u7591\u5f53\u524d\u4f9d\u8d56\u5168\u5c40EER\u4f5c\u4e3a\u9690\u79c1\u8bc4\u4f30\u6307\u6807\u7684\u53ef\u9760\u6027"}}
{"id": "2506.09522", "pdf": "https://arxiv.org/pdf/2506.09522", "abs": "https://arxiv.org/abs/2506.09522", "authors": ["Beomsik Cho", "Jaehyung Kim"], "title": "Revisit What You See: Disclose Language Prior in Vision Tokens for Efficient Guided Decoding of LVLMs", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Code available at https://github.com/bscho333/ReVisiT", "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable performance\nacross various multimodal tasks by integrating visual perception with language\nunderstanding. However, conventional decoding strategies of LVLMs often fail to\nsuccessfully utilize visual information, leading to visually ungrounded\nresponses. While various approaches have been proposed to address this\nlimitation, they typically require additional training, multi-step inference\nprocedures, or external model dependencies. This paper introduces ReVisiT, a\nsimple yet effective decoding method that references vision tokens to guide the\ntext generation process in LVLMs. Our approach leverages the semantic\ninformation embedded within vision tokens by projecting them into the text\ntoken distribution space, and dynamically selecting the most relevant vision\ntoken at each decoding step through constrained divergence minimization. This\nselected vision token is then used to refine the output distribution to better\nincorporate visual semantics. Experiments on three LVLM hallucination\nbenchmarks with two recent LVLMs demonstrate that ReVisiT consistently enhances\nvisual grounding with minimal computational overhead. Moreover, our method\nachieves competitive or superior results relative to state-of-the-art baselines\nwhile reducing computational costs for up to $2\\times$.", "AI": {"tldr": "\u63d0\u51faReVisiT\u89e3\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u5f15\u7528\u89c6\u89c9\u4ee4\u724c\u589e\u5f3a\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u57fa\u7840\u80fd\u529b\uff0c\u51cf\u5c11\u5e7b\u89c9\u73b0\u8c61\uff0c\u5728\u4f4e\u8ba1\u7b97\u5f00\u9500\u4e0b\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LVLMs\u89e3\u7801\u7b56\u7565\u672a\u80fd\u6709\u6548\u5229\u7528\u89c6\u89c9\u4fe1\u606f\uff0c\u5bfc\u81f4\u751f\u6210\u56de\u7b54\u7f3a\u4e4f\u89c6\u89c9\u4f9d\u636e\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u989d\u5916\u8bad\u7ec3\u6216\u591a\u6b65\u63a8\u7406\uff0c\u9700\u66f4\u7b80\u6d01\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06\u89c6\u89c9\u4ee4\u724c\u8bed\u4e49\u6295\u5f71\u81f3\u6587\u672c\u5206\u5e03\u7a7a\u95f4\uff0c\u901a\u8fc7\u7ea6\u675f\u5dee\u5f02\u6700\u5c0f\u5316\u52a8\u6001\u9009\u62e9\u76f8\u5173\u89c6\u89c9\u4ee4\u724c\uff0c\u4f18\u5316\u8f93\u51fa\u5206\u5e03\u878d\u5408\u89c6\u89c9\u8bed\u4e49\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u5728\u4e09\u4e2aLVLM\u5e7b\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u589e\u5f3a\u89c6\u89c9\u57fa\u7840\u6027\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u8fbe2\u500d\uff0c\u6027\u80fd\u8fbe\u5230\u6216\u8d85\u8d8a\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "ReVisiT\u901a\u8fc7\u52a8\u6001\u6574\u5408\u89c6\u89c9\u8bed\u4e49\u4fe1\u606f\uff0c\u4ee5\u7b80\u5355\u6709\u6548\u7684\u65b9\u5f0f\u63d0\u5347\u751f\u6210\u5185\u5bb9\u7684\u89c6\u89c9\u76f8\u5173\u6027\uff0c\u4e3a\u6539\u8fdbLVLMs\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.09532", "pdf": "https://arxiv.org/pdf/2506.09532", "abs": "https://arxiv.org/abs/2506.09532", "authors": ["Shuai Wang", "Zhenhua Liu", "Jiaheng Wei", "Xuanwu Yin", "Dong Li", "Emad Barsoum"], "title": "Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "We present Athena-PRM, a multimodal process reward model (PRM) designed to\nevaluate the reward score for each step in solving complex reasoning problems.\nDeveloping high-performance PRMs typically demands significant time and\nfinancial investment, primarily due to the necessity for step-level annotations\nof reasoning steps. Conventional automated labeling methods, such as Monte\nCarlo estimation, often produce noisy labels and incur substantial\ncomputational costs. To efficiently generate high-quality process-labeled data,\nwe propose leveraging prediction consistency between weak and strong completers\nas a criterion for identifying reliable process labels. Remarkably, Athena-PRM\ndemonstrates outstanding effectiveness across various scenarios and benchmarks\nwith just 5,000 samples. Furthermore, we also develop two effective strategies\nto improve the performance of PRMs: ORM initialization and up-sampling for\nnegative data. We validate our approach in three specific scenarios:\nverification for test time scaling, direct evaluation of reasoning step\ncorrectness, and reward ranked fine-tuning. Our Athena-PRM consistently\nachieves superior performance across multiple benchmarks and scenarios.\nNotably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances\nperformance by 10.2 points on WeMath and 7.1 points on MathVista for test time\nscaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in\nVisualProcessBench and outperforms the previous SoTA by 3.9 F1-score,\nshowcasing its robust capability to accurately assess the correctness of the\nreasoning step. Additionally, utilizing Athena-PRM as the reward model, we\ndevelop Athena-7B with reward ranked fine-tuning and outperforms baseline with\na significant margin on five benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u591a\u6a21\u6001\u8fc7\u7a0b\u5956\u52b1\u6a21\u578bAthena-PRM\uff0c\u901a\u8fc7\u5f31/\u5f3a\u6a21\u578b\u9884\u6d4b\u4e00\u81f4\u6027\u5b9e\u73b0\u9ad8\u6548\u8fc7\u7a0b\u6807\u6ce8\uff0c\u4ec5\u97005000\u6837\u672c\u5373\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u4f9d\u8d56\u9ad8\u6210\u672c\u4eba\u5de5\u6807\u6ce8\uff0c\u81ea\u52a8\u6807\u6ce8\u65b9\u6cd5\u5b58\u5728\u566a\u58f0\u5927\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u5bfb\u627e\u66f4\u9ad8\u6548\u53ef\u9760\u7684\u8fc7\u7a0b\u6807\u6ce8\u65b9\u6848\u3002", "method": "\u5229\u7528\u5f31/\u5f3a\u6a21\u578b\u9884\u6d4b\u4e00\u81f4\u6027\u7b5b\u9009\u53ef\u9760\u8fc7\u7a0b\u6807\u7b7e\uff0c\u7ed3\u5408ORM\u6a21\u578b\u521d\u59cb\u5316\u3001\u8d1f\u6837\u672c\u4e0a\u91c7\u6837\u7b56\u7565\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55\u573a\u666f\u63d0\u5347WeMath/MathVista\u6307\u680710.2/7.1\u5206\uff0cVisualProcessBench\u5237\u65b0SOTA\u6307\u68073.9F1\uff0c\u5956\u52b1\u5fae\u8c03\u540e\u57285\u4e2a\u57fa\u51c6\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u3002", "conclusion": "Athena-PRM\u9a8c\u8bc1\u4e86\u9884\u6d4b\u4e00\u81f4\u6027\u6807\u6ce8\u7684\u6709\u6548\u6027\uff0c\u63d0\u51fa\u7684\u8bad\u7ec3\u7b56\u7565\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5176\u5956\u52b1\u6a21\u578b\u80fd\u529b\u53ef\u6709\u6548\u652f\u6301\u591a\u573a\u666f\u63a8\u7406\u4efb\u52a1\u4f18\u5316\u3002"}}
{"id": "2506.09600", "pdf": "https://arxiv.org/pdf/2506.09600", "abs": "https://arxiv.org/abs/2506.09600", "authors": ["Itay Nakash", "George Kour", "Koren Lazar", "Matan Vetzler", "Guy Uziel", "Ateret Anaby-Tavor"], "title": "Effective Red-Teaming of Policy-Adherent Agents", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Task-oriented LLM-based agents are increasingly used in domains with strict\npolicies, such as refund eligibility or cancellation rules. The challenge lies\nin ensuring that the agent consistently adheres to these rules and policies,\nappropriately refusing any request that would violate them, while still\nmaintaining a helpful and natural interaction. This calls for the development\nof tailored design and evaluation methodologies to ensure agent resilience\nagainst malicious user behavior. We propose a novel threat model that focuses\non adversarial users aiming to exploit policy-adherent agents for personal\nbenefit. To address this, we present CRAFT, a multi-agent red-teaming system\nthat leverages policy-aware persuasive strategies to undermine a\npolicy-adherent agent in a customer-service scenario, outperforming\nconventional jailbreak methods such as DAN prompts, emotional manipulation, and\ncoercive. Building upon the existing tau-bench benchmark, we introduce\ntau-break, a complementary benchmark designed to rigorously assess the agent's\nrobustness against manipulative user behavior. Finally, we evaluate several\nstraightforward yet effective defense strategies. While these measures provide\nsome protection, they fall short, highlighting the need for stronger,\nresearch-driven safeguards to protect policy-adherent agents from adversarial\nattacks", "AI": {"tldr": "\u63d0\u51fa\u5bf9\u6297\u6027\u7ea2\u961f\u7cfb\u7edfCRAFT\u548c\u8bc4\u4f30\u57fa\u51c6tau-break\uff0c\u63ed\u793a\u73b0\u6709\u7b56\u7565\u4e00\u81f4\u6027AI\u4ee3\u7406\u7684\u8106\u5f31\u6027", "motivation": "\u786e\u4fdd\u4efb\u52a1\u5bfc\u5411\u7684LLM\u4ee3\u7406\u5728\u4e25\u683c\u653f\u7b56\u573a\u666f\u4e0b\u65e2\u80fd\u9075\u5b88\u89c4\u5219\u53c8\u4fdd\u6301\u81ea\u7136\u4ea4\u4e92\uff0c\u540c\u65f6\u62b5\u5fa1\u6076\u610f\u7528\u6237\u7684\u7b56\u7565\u6027\u653b\u51fb", "method": "1) \u63d0\u51fa\u65b0\u578b\u5a01\u80c1\u6a21\u578b 2) \u5f00\u53d1\u591a\u4ee3\u7406\u7ea2\u961f\u7cfb\u7edfCRAFT 3) \u6784\u5efatau-break\u57fa\u51c6 4) \u6d4b\u8bd5\u73b0\u6709\u9632\u5fa1\u7b56\u7565", "result": "CRAFT\u5728\u5ba2\u670d\u573a\u666f\u4e2d\u7684\u653b\u51fb\u6210\u529f\u7387\u8d85\u8d8a\u4f20\u7edf\u8d8a\u72f1\u65b9\u6cd5\uff1b\u73b0\u6709\u9632\u5fa1\u63aa\u65bd\u4ec5\u63d0\u4f9b\u6709\u9650\u4fdd\u62a4", "conclusion": "\u9700\u8981\u57fa\u4e8e\u7814\u7a76\u7684\u5f3a\u5b89\u5168\u673a\u5236\u6765\u4fdd\u62a4\u653f\u7b56\u9075\u5faa\u578bAI\u4ee3\u7406\uff0c\u5f53\u524d\u7b80\u5355\u9632\u5fa1\u65b9\u6848\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u7cfb\u7edf\u6027\u5bf9\u6297\u653b\u51fb"}}
{"id": "2506.09659", "pdf": "https://arxiv.org/pdf/2506.09659", "abs": "https://arxiv.org/abs/2506.09659", "authors": ["Eltayeb Ahmed", "Uljad Berdica", "Martha Elliott", "Danijela Horak", "Jakob N. Foerster"], "title": "Intent Factored Generation: Unleashing the Diversity in Your Language Model", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Obtaining multiple meaningfully diverse, high quality samples from Large\nLanguage Models for a fixed prompt remains an open challenge. Current methods\nfor increasing diversity often only operate at the token-level, paraphrasing\nthe same response. This is problematic because it leads to poor exploration on\nreasoning problems and to unengaging, repetitive conversational agents. To\naddress this we propose Intent Factored Generation (IFG), factorising the\nsampling process into two stages. First, we sample a semantically dense intent,\ne.g., a summary or keywords. Second, we sample the final response conditioning\non both the original prompt and the intent from the first stage. This allows us\nto use a higher temperature during the intent step to promote conceptual\ndiversity, and a lower temperature during the final generation to ensure the\noutputs are coherent and self-consistent. Additionally, we find that prompting\nthe model to explicitly state its intent for each step of the chain-of-thought\nbefore generating the step is beneficial for reasoning tasks. We demonstrate\nour method's effectiveness across a diverse set of tasks. We show this method\nimproves both pass@k and Reinforcement Learning from Verifier Feedback on maths\nand code tasks. For instruction-tuning, we combine IFG with Direct Preference\nOptimisation to increase conversational diversity without sacrificing reward.\nFinally, we achieve higher diversity while maintaining the quality of\ngenerations on a general language modelling task, using a new dataset of reader\ncomments and news articles that we collect and open-source. In summary, we\npresent a simple method of increasing the sample diversity of LLMs while\nmaintaining performance. This method can be implemented by changing the prompt\nand varying the temperature during generation, making it easy to integrate into\nmany algorithms for gains across various applications.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u9636\u6bb5\u610f\u56fe\u5206\u89e3\u751f\u6210\u65b9\u6cd5\uff08IFG\uff09\uff0c\u901a\u8fc7\u5148\u91c7\u6837\u610f\u56fe\u518d\u751f\u6210\u56de\u7b54\u7684\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347LLM\u8f93\u51fa\u591a\u6837\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728token\u5c42\u9762\u589e\u52a0\u591a\u6837\u6027\u5bfc\u81f4\u63a8\u7406\u4efb\u52a1\u63a2\u7d22\u4e0d\u8db3\u548c\u5bf9\u8bdd\u91cd\u590d\uff0c\u9700\u4ece\u6839\u672c\u4e0a\u89e3\u51b3\u751f\u6210\u591a\u6837\u6027\u53d7\u9650\u7684\u95ee\u9898\u3002", "method": "1. \u610f\u56fe\u91c7\u6837\u9636\u6bb5\uff1a\u7528\u9ad8\u6e29\u751f\u6210\u8bed\u4e49\u5bc6\u96c6\u7684\u610f\u56fe\uff08\u6458\u8981/\u5173\u952e\u8bcd\uff09\uff1b2. \u6761\u4ef6\u751f\u6210\u9636\u6bb5\uff1a\u7528\u4f4e\u6e29\u57fa\u4e8e\u610f\u56fe+\u539f\u63d0\u793a\u751f\u6210\u8fde\u8d2f\u56de\u7b54\u3002\u652f\u6301\u94fe\u5f0f\u63a8\u7406\u4e2d\u663e\u5f0f\u58f0\u660e\u6bcf\u6b65\u610f\u56fe\u3002", "result": "\u5728\u6570\u5b66/\u4ee3\u7801\u4efb\u52a1\u4e0a\u63d0\u5347pass@k\u6307\u6807\uff0c\u7ed3\u5408DPO\u589e\u5f3a\u5bf9\u8bdd\u591a\u6837\u6027\uff0c\u65b0\u95fb\u8bc4\u8bba\u751f\u6210\u4efb\u52a1\u5b9e\u73b0\u8d28\u91cf-\u591a\u6837\u6027\u53cc\u4f18\u3002\u5f00\u6e90\u65b0\u6570\u636e\u96c6\u3002", "conclusion": "\u4ec5\u9700\u4fee\u6539prompt\u548c\u6e29\u5ea6\u8c03\u8282\u5373\u53ef\u5b9e\u65bd\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u53ef\u96c6\u6210\u5230\u591a\u79cd\u7b97\u6cd5\u4e2d\u63d0\u5347\u5404\u7c7b\u5e94\u7528\u6548\u679c\u3002"}}
{"id": "2506.09691", "pdf": "https://arxiv.org/pdf/2506.09691", "abs": "https://arxiv.org/abs/2506.09691", "authors": ["Imanol Miranda", "Ander Salaberria", "Eneko Agirre", "Gorka Azkune"], "title": "Adding simple structure at inference improves Vision-Language Compositionality", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Dual encoder Vision-Language Models (VLM) such as CLIP are widely used for\nimage-text retrieval tasks. However, those models struggle with\ncompositionality, showing a bag-of-words-like behavior that limits their\nretrieval performance. Many different training approaches have been proposed to\nimprove the vision-language compositionality capabilities of those models. In\ncomparison, inference-time techniques have received little attention. In this\npaper, we propose to add simple structure at inference, where, given an image\nand a caption: i) we divide the image into different smaller crops, ii) we\nextract text segments, capturing objects, attributes and relations, iii) using\na VLM, we find the image crops that better align with text segments obtaining\nmatches, and iv) we compute the final image-text similarity aggregating the\nindividual similarities of the matches. Based on various popular dual encoder\nVLMs, we evaluate our approach in controlled and natural datasets for VL\ncompositionality. We find that our approach consistently improves the\nperformance of evaluated VLMs without any training, which shows the potential\nof inference-time techniques. The results are especially good for\nattribute-object binding as shown in the controlled dataset. As a result of an\nextensive analysis: i) we show that processing image crops is actually\nessential for the observed gains in performance, and ii) we identify specific\nareas to further improve inference-time approaches.", "AI": {"tldr": "\u901a\u8fc7\u5728\u63a8\u7406\u9636\u6bb5\u589e\u52a0\u56fe\u50cf\u5206\u5757\u3001\u6587\u672c\u7247\u6bb5\u5339\u914d\u548c\u76f8\u4f3c\u5ea6\u805a\u5408\u7684\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u53cc\u7f16\u7801\u5668\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7ec4\u5408\u6027\u80fd\u529b\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709CLIP\u7b49\u53cc\u7f16\u7801\u5668\u6a21\u578b\u5b58\u5728\u7ec4\u5408\u6027\u7f3a\u9677\uff08\u5982\u8bcd\u888b\u6548\u5e94\uff09\uff0c\u591a\u6570\u6539\u8fdb\u96c6\u4e2d\u5728\u8bad\u7ec3\u9636\u6bb5\uff0c\u800c\u5bf9\u63a8\u7406\u9636\u6bb5\u6280\u672f\u7684\u7814\u7a76\u4e0d\u8db3\u3002\u672c\u6587\u63a2\u7d22\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u8fc7\u7a0b\u63d0\u5347\u6a21\u578b\u8868\u73b0\u3002", "method": "\u56db\u6b65\u63a8\u7406\u6846\u67b6\uff1a1) \u56fe\u50cf\u5206\u5757\u5904\u7406\uff1b2) \u63d0\u53d6\u6587\u672c\u4e2d\u7684\u5bf9\u8c61/\u5c5e\u6027/\u5173\u7cfb\u7247\u6bb5\uff1b3) \u4f7f\u7528VLM\u8fdb\u884c\u56fe\u6587\u7247\u6bb5\u5339\u914d\uff1b4) \u805a\u5408\u5c40\u90e8\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u6574\u4f53\u5339\u914d\u5ea6\u3002", "result": "\u5728\u591a\u4e2a\u7ec4\u5408\u6027\u6570\u636e\u96c6\u4e2d\u7a33\u5b9a\u63d0\u5347VLMs\u6027\u80fd\uff08\u5c24\u5176\u5728\u5c5e\u6027-\u5bf9\u8c61\u7ed1\u5b9a\u4efb\u52a1\uff09\uff0c\u5b9e\u9a8c\u8868\u660e\u56fe\u50cf\u5206\u5757\u5904\u7406\u5bf9\u6548\u679c\u63d0\u5347\u8d77\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u63a8\u7406\u65f6\u6280\u672f\u5177\u6709\u91cd\u8981\u6f5c\u529b\uff0c\u672a\u6765\u53ef\u901a\u8fc7\u4f18\u5316\u6587\u672c\u5206\u89e3\u7b56\u7565\u3001\u6539\u8fdb\u5c40\u90e8\u5339\u914d\u673a\u5236\u7b49\u65b9\u5411\u8fdb\u4e00\u6b65\u63d0\u5347\u7ec4\u5408\u6027\u8868\u73b0\u3002"}}
{"id": "2506.09707", "pdf": "https://arxiv.org/pdf/2506.09707", "abs": "https://arxiv.org/abs/2506.09707", "authors": ["Suhas BN", "Andrew M. Sherrill", "Jyoti Alaparthi", "Dominik Mattioli", "Rosa I. Arriaga", "Chris W. Wiese", "Saeed Abdullah"], "title": "Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements", "categories": ["eess.AS", "cs.CL", "cs.HC", "68T07", "I.2.7; I.5.4; H.5.2"], "comment": "5 pages, 2 figures", "summary": "Prolonged Exposure (PE) therapy is an effective treatment for post-traumatic\nstress disorder (PTSD), but evaluating therapist fidelity remains\nlabor-intensive due to the need for manual review of session recordings. We\npresent a method for the automatic temporal localization of key PE fidelity\nelements -- identifying their start and stop times -- directly from session\naudio and transcripts. Our approach fine-tunes a large pre-trained\naudio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process\nfocused 30-second windows of audio-transcript input. Fidelity labels for three\ncore protocol phases -- therapist orientation (P1), imaginal exposure (P2), and\npost-imaginal processing (P3) -- are generated via LLM-based prompting and\nverified by trained raters. The model is trained to predict normalized boundary\noffsets using soft supervision guided by task-specific prompts. On a dataset of\n313 real PE sessions, our best configuration (LoRA rank 8, 30s windows)\nachieves a mean absolute error (MAE) of 5.3 seconds across tasks. We further\nanalyze the effects of window size and LoRA rank, highlighting the importance\nof context granularity and model adaptation. This work introduces a scalable\nframework for fidelity tracking in PE therapy, with potential to support\nclinician training, supervision, and quality assurance.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eQwen2-Audio\u6a21\u578b\u548cLoRA\u5fae\u8c03\u7684\u81ea\u52a8\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u7528\u4e8ePE\u6cbb\u7597\u4e2d\u5173\u952e\u73af\u8282\u7684\u65f6\u57df\u5b9a\u4f4d\uff0cMAE\u8fbe5.3\u79d2", "motivation": "\u4f20\u7edfPE\u7597\u6cd5\u5fe0\u8bda\u5ea6\u8bc4\u4f30\u4f9d\u8d56\u4eba\u5de5\u5ba1\u6838\u5f55\u97f3\uff0c\u6548\u7387\u4f4e\u4e0b\u4e14\u6210\u672c\u9ad8\u6602", "method": "\u4f7f\u7528LoRA\u5fae\u8c03\u9884\u8bad\u7ec3\u97f3\u9891-\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u540830\u79d2\u7a97\u53e3\u7684\u97f3\u9891\u8f6c\u5f55\u8f93\u5165\uff0c\u901a\u8fc7LLM\u751f\u6210\u534f\u8bae\u9636\u6bb5\u6807\u7b7e\u5e76\u8fdb\u884c\u4eba\u5de5\u9a8c\u8bc1", "result": "\u6700\u4f73\u914d\u7f6e(LoRA\u79e98+30\u79d2\u7a97\u53e3)\u5728313\u4e2a\u771f\u5b9e\u4f1a\u8bdd\u4e2d\u5b9e\u73b05.3\u79d2\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff0c\u7a97\u53e3\u5927\u5c0f\u548cLoRA\u79e9\u663e\u8457\u5f71\u54cd\u6027\u80fd", "conclusion": "\u8be5\u6846\u67b6\u4e3aPE\u6cbb\u7597\u5fe0\u8bda\u5ea6\u8ffd\u8e2a\u63d0\u4f9b\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u4e34\u5e8a\u57f9\u8bad\u548c\u8d28\u91cf\u76d1\u63a7"}}
{"id": "2506.09804", "pdf": "https://arxiv.org/pdf/2506.09804", "abs": "https://arxiv.org/abs/2506.09804", "authors": ["Peter Vieting", "Maximilian Kannen", "Benedikt Hilmes", "Ralf Schl\u00fcter", "Hermann Ney"], "title": "Regularizing Learnable Feature Extraction for Automatic Speech Recognition", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "comment": "Accepted at Interspeech 2025", "summary": "Neural front-ends are an appealing alternative to traditional, fixed feature\nextraction pipelines for automatic speech recognition (ASR) systems since they\ncan be directly trained to fit the acoustic model. However, their performance\noften falls short compared to classical methods, which we show is largely due\nto their increased susceptibility to overfitting. This work therefore\ninvestigates regularization methods for training ASR models with learnable\nfeature extraction front-ends. First, we examine audio perturbation methods and\nshow that larger relative improvements can be obtained for learnable features.\nAdditionally, we identify two limitations in the standard use of SpecAugment\nfor these front-ends and propose masking in the short time Fourier transform\n(STFT)-domain as a simple but effective modification to address these\nchallenges. Finally, integrating both regularization approaches effectively\ncloses the performance gap between traditional and learnable features.", "AI": {"tldr": "\u7814\u7a76\u6b63\u5219\u5316\u65b9\u6cd5\u6539\u8fdb\u53ef\u5b66\u4e60\u7279\u5f81\u524d\u7aef\u7684ASR\u6027\u80fd\uff0c\u901a\u8fc7\u97f3\u9891\u6270\u52a8\u548cSTFT\u57df\u63a9\u7801\u7b56\u7565\u7f29\u5c0f\u4e0e\u4f20\u7edf\u65b9\u6cd5\u7684\u5dee\u8ddd", "motivation": "\u795e\u7ecf\u524d\u7aef\u867d\u53ef\u76f4\u63a5\u4f18\u5316\u9002\u914d\u58f0\u5b66\u6a21\u578b\uff0c\u4f46\u6613\u8fc7\u62df\u5408\u5bfc\u81f4\u6027\u80fd\u4f4e\u4e8e\u4f20\u7edf\u56fa\u5b9a\u7279\u5f81\u65b9\u6cd5", "method": "1. \u63a2\u7d22\u97f3\u9891\u6270\u52a8\u65b9\u6cd5\u5bf9\u53ef\u5b66\u4e60\u7279\u5f81\u7684\u589e\u76ca 2. \u53d1\u73b0SpecAugment\u5728STFT\u57df\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51faSTFT\u57df\u63a9\u7801\u6539\u8fdb\u65b9\u6848", "result": "\u4e24\u79cd\u6b63\u5219\u5316\u65b9\u6cd5\u7ed3\u5408\u540e\uff0c\u53ef\u5b66\u4e60\u7279\u5f81\u4e0e\u4f20\u7edf\u65b9\u6cd5\u6027\u80fd\u5dee\u8ddd\u663e\u8457\u7f29\u5c0f", "conclusion": "\u6709\u6548\u6b63\u5219\u5316\u7b56\u7565\u80fd\u63d0\u5347\u795e\u7ecf\u524d\u7aef\u5b9e\u7528\u6027\uff0c\u4e3a\u7aef\u5230\u7aefASR\u7cfb\u7edf\u4f18\u5316\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2506.09851", "pdf": "https://arxiv.org/pdf/2506.09851", "abs": "https://arxiv.org/abs/2506.09851", "authors": ["Md. Yeasin Rahat", "Rajan Das Gupta", "Nur Raisa Rahman", "Sudipto Roy Pritom", "Samiur Rahman Shakir", "Md Imrul Hasan Showmick", "Md. Jakir Hossen"], "title": "Advancing Exchange Rate Forecasting: Leveraging Machine Learning and AI for Enhanced Accuracy in Global Financial Markets", "categories": ["q-fin.ST", "cs.CL", "cs.LG"], "comment": "Accepted in MECON 2025", "summary": "The prediction of foreign exchange rates, such as the US Dollar (USD) to\nBangladeshi Taka (BDT), plays a pivotal role in global financial markets,\ninfluencing trade, investments, and economic stability. This study leverages\nhistorical USD/BDT exchange rate data from 2018 to 2023, sourced from Yahoo\nFinance, to develop advanced machine learning models for accurate forecasting.\nA Long Short-Term Memory (LSTM) neural network is employed, achieving an\nexceptional accuracy of 99.449%, a Root Mean Square Error (RMSE) of 0.9858, and\na test loss of 0.8523, significantly outperforming traditional methods like\nARIMA (RMSE 1.342). Additionally, a Gradient Boosting Classifier (GBC) is\napplied for directional prediction, with backtesting on a $10,000 initial\ncapital revealing a 40.82% profitable trade rate, though resulting in a net\nloss of $20,653.25 over 49 trades. The study analyzes historical trends,\nshowing a decline in BDT/USD rates from 0.012 to 0.009, and incorporates\nnormalized daily returns to capture volatility. These findings highlight the\npotential of deep learning in forex forecasting, offering traders and\npolicymakers robust tools to mitigate risks. Future work could integrate\nsentiment analysis and real-time economic indicators to further enhance model\nadaptability in volatile markets.", "AI": {"tldr": "\u4f7f\u7528LSTM\u548c\u68af\u5ea6\u63d0\u5347\u5206\u7c7b\u5668\u9884\u6d4b\u7f8e\u5143/\u5b5f\u52a0\u62c9\u5854\u5361\u6c47\u7387\uff0cLSTM\u5b9e\u73b099.45%\u51c6\u786e\u7387\u4f46\u4ea4\u6613\u7b56\u7565\u4e8f\u635f\uff0c\u7814\u7a76\u5c55\u793a\u6df1\u5ea6\u5b66\u4e60\u5728\u5916\u6c47\u9884\u6d4b\u4e2d\u7684\u6f5c\u529b", "motivation": "\u63d0\u5347\u5916\u6c47\u9884\u6d4b\u7cbe\u5ea6\u5bf9\u5168\u7403\u8d38\u6613\u548c\u6295\u8d44\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u4f20\u7edfARIMA\u6a21\u578b\u5728\u6ce2\u52a8\u5e02\u573a\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u9700\u63a2\u7d22\u6df1\u5ea6\u5b66\u4e60\u89e3\u51b3\u65b9\u6848", "method": "\u91c7\u75282018-2023\u5e74\u5386\u53f2\u6c47\u7387\u6570\u636e\uff0c\u5bf9\u6bd4LSTM\u795e\u7ecf\u7f51\u7edc\uff08\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff09\u548c\u68af\u5ea6\u63d0\u5347\u5206\u7c7b\u5668\uff08\u65b9\u5411\u9884\u6d4b\uff09\uff0c\u7ed3\u5408\u56de\u6eaf\u6d4b\u8bd5\u8bc4\u4f30\u4ea4\u6613\u7b56\u7565", "result": "LSTM\u6a21\u578bRMSE 0.9858\u663e\u8457\u4f18\u4e8eARIMA(1.342)\uff0c\u4f46\u68af\u5ea6\u63d0\u5347\u7b56\u7565\u5bfc\u81f420,653\u7f8e\u5143\u51c0\u4e8f\u635f\uff1b\u6c47\u7387\u957f\u671f\u5448\u8d2c\u503c\u8d8b\u52bf\uff080.012\u21920.009\uff09", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u4e3a\u5916\u6c47\u5e02\u573a\u63d0\u4f9b\u6709\u6548\u9884\u6d4b\u5de5\u5177\uff0c\u672a\u6765\u9700\u6574\u5408\u60c5\u7eea\u5206\u6790\u548c\u5b9e\u65f6\u7ecf\u6d4e\u6307\u6807\u63d0\u5347\u6a21\u578b\u9002\u5e94\u80fd\u529b"}}
{"id": "2506.09953", "pdf": "https://arxiv.org/pdf/2506.09953", "abs": "https://arxiv.org/abs/2506.09953", "authors": ["Benjamin Reichman", "Constantin Patsch", "Jack Truxal", "Atishay Jain", "Larry Heck"], "title": "Outside Knowledge Conversational Video (OKCV) Dataset -- Dialoguing over Videos", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "In outside knowledge visual question answering (OK-VQA), the model must\nidentify relevant visual information within an image and incorporate external\nknowledge to accurately respond to a question. Extending this task to a\nvisually grounded dialogue setting based on videos, a conversational model must\nboth recognize pertinent visual details over time and answer questions where\nthe required information is not necessarily present in the visual information.\nMoreover, the context of the overall conversation must be considered for the\nsubsequent dialogue. To explore this task, we introduce a dataset comprised of\n$2,017$ videos with $5,986$ human-annotated dialogues consisting of $40,954$\ninterleaved dialogue turns. While the dialogue context is visually grounded in\nspecific video segments, the questions further require external knowledge that\nis not visually present. Thus, the model not only has to identify relevant\nvideo parts but also leverage external knowledge to converse within the\ndialogue. We further provide several baselines evaluated on our dataset and\nshow future challenges associated with this task. The dataset is made publicly\navailable here: https://github.com/c-patsch/OKCV.", "AI": {"tldr": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u89c6\u9891\u7684\u89c6\u89c9\u5bf9\u8bdd\u6570\u636e\u96c6OKCV\uff0c\u8981\u6c42\u6a21\u578b\u7ed3\u5408\u89c6\u9891\u7247\u6bb5\u4e2d\u7684\u89c6\u89c9\u4fe1\u606f\u4e0e\u5916\u90e8\u77e5\u8bc6\u8fdb\u884c\u591a\u8f6e\u5bf9\u8bdd\u3002", "motivation": "\u4f20\u7edfOK-VQA\u4efb\u52a1\u4ec5\u5173\u6ce8\u9759\u6001\u56fe\u7247\u7684\u5355\u8f6e\u95ee\u7b54\uff0c\u8be5\u7814\u7a76\u6269\u5c55\u81f3\u89c6\u9891\u573a\u666f\u4e0b\u7684\u591a\u8f6e\u5bf9\u8bdd\uff0c\u8981\u6c42\u6a21\u578b\u6301\u7eed\u8ffd\u8e2a\u65f6\u5e8f\u89c6\u89c9\u4fe1\u606f\uff0c\u5e76\u5728\u89c6\u89c9\u4fe1\u606f\u7f3a\u5931\u65f6\u4f9d\u8d56\u5916\u90e8\u77e5\u8bc6\u8fdb\u884c\u5bf9\u8bdd\u3002", "method": "\u6784\u5efa\u5305\u542b2,017\u4e2a\u89c6\u9891\u548c5,986\u4e2a\u4eba\u5de5\u6807\u6ce8\u5bf9\u8bdd\u7684\u6570\u636e\u96c6\uff0c\u5bf9\u8bdd\u6d89\u53ca40,954\u8f6e\u6b21\u3002\u6bcf\u4e2a\u5bf9\u8bdd\u57fa\u4e8e\u7279\u5b9a\u89c6\u9891\u7247\u6bb5\uff0c\u4f46\u95ee\u9898\u9700\u4f9d\u8d56\u975e\u89c6\u89c9\u7684\u5916\u90e8\u77e5\u8bc6\uff0c\u8981\u6c42\u6a21\u578b\u540c\u65f6\u5177\u5907\u89c6\u9891\u5b9a\u4f4d\u548c\u77e5\u8bc6\u68c0\u7d22\u80fd\u529b\u3002", "result": "\u516c\u5f00\u4e86OKCV\u6570\u636e\u96c6\u5e76\u63d0\u4f9b\u4e86\u57fa\u7ebf\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u89c6\u9891\u65f6\u5e8f\u7406\u89e3\u4e0e\u77e5\u8bc6\u878d\u5408\u65b9\u9762\u7684\u6311\u6218\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u63a8\u52a8\u4e86\u89c6\u9891\u573a\u666f\u4e0b\u77e5\u8bc6\u589e\u5f3a\u5bf9\u8bdd\u7cfb\u7edf\u7684\u7814\u7a76\uff0c\u7a81\u663e\u4e86\u89c6\u89c9\u5b9a\u4f4d\u4e0e\u5916\u90e8\u77e5\u8bc6\u534f\u540c\u63a8\u7406\u7684\u6280\u672f\u96be\u70b9\u3002"}}
{"id": "2506.09998", "pdf": "https://arxiv.org/pdf/2506.09998", "abs": "https://arxiv.org/abs/2506.09998", "authors": ["Tim Z. Xiao", "Johannes Zenn", "Zhen Liu", "Weiyang Liu", "Robert Bamler", "Bernhard Sch\u00f6lkopf"], "title": "Flipping Against All Odds: Reducing LLM Coin Flip Bias via Verbalized Rejection Sampling", "categories": ["cs.LG", "cs.CL"], "comment": "Technical Report v1 (21 pages, 14 figures)", "summary": "Large language models (LLMs) can often accurately describe probability\ndistributions using natural language, yet they still struggle to generate\nfaithful samples from them. This mismatch limits their use in tasks requiring\nreliable stochasticity, such as Monte Carlo methods, agent-based simulations,\nand randomized decision-making. We investigate this gap between knowledge and\nsampling in the context of Bernoulli distributions. We introduce Verbalized\nRejection Sampling (VRS), a natural-language adaptation of classical rejection\nsampling that prompts the LLM to reason about and accept or reject proposed\nsamples. Despite relying on the same Bernoulli mechanism internally, VRS\nsubstantially reduces sampling bias across models. We provide theoretical\nanalysis showing that, under mild assumptions, VRS improves over direct\nsampling, with gains attributable to both the algorithm and prompt design. More\nbroadly, our results show how classical probabilistic tools can be verbalized\nand embedded into LLM workflows to improve reliability, without requiring\naccess to model internals or heavy prompt engineering.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u62d2\u7edd\u91c7\u6837\u65b9\u6cd5VRS\uff0c\u663e\u8457\u964d\u4f4eLLM\u751f\u6210\u6982\u7387\u5206\u5e03\u7684\u91c7\u6837\u504f\u5dee", "motivation": "\u89e3\u51b3LLM\u80fd\u63cf\u8ff0\u6982\u7387\u5206\u5e03\u4f46\u751f\u6210\u53ef\u9760\u968f\u673a\u6837\u672c\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u5176\u5728\u8499\u7279\u5361\u6d1b\u6a21\u62df\u7b49\u4efb\u52a1\u4e2d\u7684\u5e94\u7528", "method": "\u5c06\u7ecf\u5178\u62d2\u7edd\u91c7\u6837\u7b97\u6cd5\u8f6c\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u6d41\u7a0b\uff0c\u4f7fLLM\u81ea\u4e3b\u8fdb\u884c\u6837\u672c\u63a5\u53d7/\u62d2\u7edd\u51b3\u7b56", "result": "VRS\u5728\u76f8\u540c\u6a21\u578b\u4e0b\u51cf\u5c11\u91c7\u6837\u504f\u5dee\uff0c\u7406\u8bba\u8bc1\u660e\u5176\u4f18\u4e8e\u76f4\u63a5\u91c7\u6837\uff0c\u6539\u8fdb\u6765\u81ea\u7b97\u6cd5\u67b6\u6784\u548c\u63d0\u793a\u8bbe\u8ba1\u7684\u534f\u540c\u4f5c\u7528", "conclusion": "\u901a\u8fc7\u5c06\u6982\u7387\u5de5\u5177\u81ea\u7136\u8bed\u8a00\u5316\u878d\u5165LLM\u5de5\u4f5c\u6d41\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u5373\u53ef\u63d0\u5347\u53ef\u9760\u6027\uff0c\u4e3a\u7b97\u6cd5\u4e0e\u63d0\u793a\u8bbe\u8ba1\u7684\u534f\u540c\u4f18\u5316\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
