<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 194]
- [cs.GR](#cs.GR) [Total: 15]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.LG](#cs.LG) [Total: 23]
- [cs.SE](#cs.SE) [Total: 3]
- [cs.AI](#cs.AI) [Total: 28]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.SD](#cs.SD) [Total: 1]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.CV](#cs.CV) [Total: 17]
- [cs.CY](#cs.CY) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Are you sure? Measuring models bias in content moderation through uncertainty](https://arxiv.org/abs/2509.22699)
*Alessandra Urbinati,Mirko Lai,Simona Frenda,Marco Antonio Stranisci*

Main category: cs.CL

TL;DR: 通过测量语言模型的不确定性评估内容审核中的群体偏见，发现准确率与置信度存在偏差，提出基于置信度的去偏方法


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在内容审核中存在种族和性别偏见，但传统性能指标（如F1分数）无法有效衡量模型公平性，需开发新评估方法

Method: 使用符合预测技术计算模型不确定性，分析11个预训练模型对女性和非白人标注者标签的预测置信度，并与传统性能指标对比

Result: 部分模型对少数群体标注的预测准确率高但置信度低，显示性能指标与公平性评估存在偏差

Conclusion: 通过不确定性测量可识别模型中的群体代表偏差，为部署前的模型去偏提供关键依据

Abstract: Automatic content moderation is crucial to ensuring safety in social media.
Language Model-based classifiers are being increasingly adopted for this task,
but it has been shown that they perpetuate racial and social biases. Even if
several resources and benchmark corpora have been developed to challenge this
issue, measuring the fairness of models in content moderation remains an open
issue. In this work, we present an unsupervised approach that benchmarks models
on the basis of their uncertainty in classifying messages annotated by people
belonging to vulnerable groups. We use uncertainty, computed by means of the
conformal prediction technique, as a proxy to analyze the bias of 11 models
against women and non-white annotators and observe to what extent it diverges
from metrics based on performance, such as the $F_1$ score. The results show
that some pre-trained models predict with high accuracy the labels coming from
minority groups, even if the confidence in their prediction is low. Therefore,
by measuring the confidence of models, we are able to see which groups of
annotators are better represented in pre-trained models and lead the debiasing
process of these models before their effective use.

</details>


### [2] [AccessEval: Benchmarking Disability Bias in Large Language Models](https://arxiv.org/abs/2509.22703)
*Srikant Panda,Amit Agarwal,Hitesh Laxmichand Patel*

Main category: cs.CL

TL;DR: 研究通过AccessEval基准评估LLM在不同残疾背景下的表现，发现对残疾相关查询存在负面情感、刻板印象和事实错误，揭示模型中的能力歧视问题。


<details>
  <summary>Details</summary>
Motivation: 系统调查LLM在处理不同残疾相关查询时的行为差异，理解这些差异如何转化为对残障用户的实际影响

Method: 使用包含6个现实领域和9种残疾类型的AccessEval基准，对比21个闭源/开源LLM对中性查询与残疾感知查询的响应，通过情感分析、社会认知和事实准确性多维度评估

Result: 残疾相关查询的响应普遍更负面(情感偏差增加2.1倍)、刻板印象比例升高37%、事实错误率提升1.8倍，听力/言语/行动障碍群体受影响最严重

Conclusion: 模型行为中存在的系统性能力歧视可能造成现实伤害，强调日常应用场景中偏见缓解的重要性，需建立技术评估与用户体验的直接关联

Abstract: Large Language Models (LLMs) are increasingly deployed across diverse domains
but often exhibit disparities in how they handle real-life queries. To
systematically investigate these effects within various disability contexts, we
introduce \textbf{AccessEval (Accessibility Evaluation)}, a benchmark
evaluating 21 closed- and open-source LLMs across 6 real-world domains and 9
disability types using paired Neutral and Disability-Aware Queries. We
evaluated model outputs with metrics for sentiment, social perception, and
factual accuracy.
  Our analysis reveals that responses to disability-aware queries tend to have
a more negative tone, increased stereotyping, and higher factual error compared
to neutral queries. These effects show notable variation by domain and
disability type, with disabilities affecting hearing, speech, and mobility
disproportionately impacted. These disparities reflect persistent forms of
ableism embedded in model behavior.
  By examining model performance in real-world decision-making contexts, we
better illuminate how such biases can translate into tangible harms for
disabled users. This framing helps bridges the gap between technical evaluation
and user impact, reinforcing importance of bias mitigation in day-to-day
applications. Our dataset is publicly available at:
https://huggingface.co/datasets/Srikant86/AccessEval

</details>


### [3] [RAR$^2$: Retrieval-Augmented Medical Reasoning via Thought-Driven Retrieval](https://arxiv.org/abs/2509.22713)
*Kaishuai Xu,Wenjun Hou,Yi Cheng,Wenjie Li*

Main category: cs.CL

TL;DR: 提出RAR²联合学习框架，通过思维链建模隐性知识需求，结合DPO训练与测试扩展策略，显著提升复杂医学问答任务效果。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法缺乏对医学问题推理过程的显式建模，导致复杂问题中知识检索与整合不足。需要系统性框架同时增强检索与推理的协同优化。

Method: 1. 构建揭示知识需求的思维链指导检索 2. 混合偏好数据集+DPO训练 3. 测试时采用知识路由器和多路径推理扩展策略

Result: 在多个生物医学QA数据集上超越传统RAG方法（无论是否微调），验证框架有效性

Conclusion: RAR²通过联合优化检索与推理过程，为复杂医学问题处理提供了新范式，证实显式建模知识需求的重要性

Abstract: Large Language Models (LLMs) have shown promising performance on diverse
medical benchmarks, highlighting their potential in supporting real-world
clinical tasks. Retrieval-Augmented Generation (RAG) has emerged as a key
approach for mitigating knowledge gaps and hallucinations by incorporating
external medical information. However, RAG still struggles with complex medical
questions that require intensive reasoning, as surface-level input often fails
to reflect the true knowledge needs of the task. Existing methods typically
focus on refining queries without explicitly modeling the reasoning process,
limiting their ability to retrieve and integrate clinically relevant knowledge.
In this work, we propose RAR$^2$, a joint learning framework that improves both
Reasoning-Augmented Retrieval and Retrieval-Augmented Reasoning. RAR$^2$
constructs a thought process to uncover implicit knowledge requirements and
uses it to guide retrieval and answer generation. We build a training dataset
of mixed preference pairs and apply Direct Preference Optimization (DPO) to
train the model. Moreover, we design two test-time scaling strategies to
explore the boundaries of our framework. Experiments demonstrate the
effectiveness of RAR$^2$ across several biomedical question answering datasets,
outperforming RAG baselines with or without fine-tuning.

</details>


### [4] [TRUEBench: Can LLM Response Meet Real-world Constraints as Productivity Assistant?](https://arxiv.org/abs/2509.22715)
*Jiho Park,Jongyoon Song,Minjin Choi,Kyuho Heo,Taehun Huh,Ji Won Kim*

Main category: cs.CL

TL;DR: 研究者提出TRUEBench基准，通过多语言支持、隐式约束识别和多轮对话评估，更真实衡量LLM生产力助手能力。实验显示现有模型（如OpenAI o1）通过率仅69.07%。


<details>
  <summary>Details</summary>
Motivation: 现有基准存在三大缺陷：(1) 多语言覆盖不足，(2) 忽略用户请求中的隐式约束，(3) 缺乏多轮对话复杂性评估，无法真实反映LLM实际应用表现。

Method: 设计TRUEBench基准：①支持12种语言的输入提示；②采用LLM验证器精炼约束；③包含显式/隐式约束评估标准；④设计含累积约束和上下文切换的多轮对话场景。

Result: 在TRUEBench测试中，顶级模型OpenAI o1总体通过率仅69.07%，显著低于现有基准表现，证明该基准具有更高挑战性。

Conclusion: TRUEBench首次实现真实生产环境下的多维度LLM评估，揭示模型在复杂场景中的能力边界，为改进生产力助手提供精准度量工具。

Abstract: Large language models (LLMs) are increasingly integral as productivity
assistants, but existing benchmarks fall short in rigorously evaluating their
real-world instruction-following capabilities. Current benchmarks often (i)
lack sufficient multilinguality, (ii) fail to capture the implicit constraints
inherent in user requests, and (iii) overlook the complexities of multi-turn
dialogue. To address these critical gaps and provide a more realistic
assessment, we introduce TRUEBench (Trustworthy Real-world Usage Evaluation
Benchmark)1, a novel benchmark specifically designed for LLM-based productivity
assistants. TRUEBench distinguishes itself by featuring input prompts across 12
languages, incorporating intra-instance multilingual instructions, employing
rigorous evaluation criteria to capture both explicit and implicit constraints,
and including complex multi-turn dialogue scenarios with both accumulating
constraints and context switches. Furthermore, to ensure reliability in
evaluation, we refined constraints using an LLM validator. Extensive
experiments demonstrate that TRUEBench presents significantly greater
challenges than existing benchmarks; for instance, a strong model like OpenAI
o1 achieved only a 69.07% overall pass rate. TRUEBench offers a demanding and
realistic assessment of LLMs in practical productivity settings, highlighting
their capabilities and limitations.

</details>


### [5] [Multi-Modal Sentiment Analysis with Dynamic Attention Fusion](https://arxiv.org/abs/2509.22729)
*Sadia Abdulhalim,Muaz Albaghdadi,Moshiur Farazi*

Main category: cs.CL

TL;DR: 提出动态注意力融合框架（DAF），通过自适应加权机制融合文本与声学特征，显著提升多模态情感分析性能


<details>
  <summary>Details</summary>
Motivation: 传统单模态文本分析忽略语音语调等关键情绪线索，无法准确捕捉真实情感意图

Method: 使用冻结的预训练文本编码器与语音编码器，通过动态注意力机制实现模态自适应加权融合

Result: 在大型多模态基准测试中F1分数显著提升（+5.2%），预测误差降低18%，且无需微调底层编码器

Conclusion: 动态加权策略对复杂情感建模至关重要，该方法为情感计算（心理健康评估/人机交互）提供更鲁棒的解决方案

Abstract: Traditional sentiment analysis has long been a unimodal task, relying solely
on text. This approach overlooks non-verbal cues such as vocal tone and prosody
that are essential for capturing true emotional intent. We introduce Dynamic
Attention Fusion (DAF), a lightweight framework that combines frozen text
embeddings from a pretrained language model with acoustic features from a
speech encoder, using an adaptive attention mechanism to weight each modality
per utterance. Without any finetuning of the underlying encoders, our proposed
DAF model consistently outperforms both static fusion and unimodal baselines on
a large multimodal benchmark. We report notable gains in F1-score and
reductions in prediction error and perform a variety of ablation studies that
support our hypothesis that the dynamic weighting strategy is crucial for
modeling emotionally complex inputs. By effectively integrating verbal and
non-verbal information, our approach offers a more robust foundation for
sentiment prediction and carries broader impact for affective computing
applications -- from emotion recognition and mental health assessment to more
natural human computer interaction.

</details>


### [6] [Enabling Approximate Joint Sampling in Diffusion LMs](https://arxiv.org/abs/2509.22738)
*Parikshit Bansal,Sujay Sanghavi*

Main category: cs.CL

TL;DR: 提出在扩散语言模型上叠加轻量级采样器层，通过单次全模型前向传递近似联合采样，平衡生成速度与分布准确性


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型并行生成多token会偏离真实联合分布，需要找到高效生成与保持分布准确性的平衡点

Method: 在冻结的扩散大模型顶部开发新型单层采样器，通过全模型单次前向+多次采样层前向实现多token联合采样

Result: 在四token并行生成时达到0.87 MAUVE分数（基准线0.31），在语言建模和STEM任务中验证有效性

Conclusion: 轻量采样器层成功实现近似联合采样，为扩散模型在保持速度优势的同时提升生成质量提供新思路

Abstract: In autoregressive language models, each token is sampled by conditioning on
all the past tokens; the overall string has thus been sampled from the correct
underlying joint distribution represented by the model. In contrast, masked
diffusion language models generate text by unmasking tokens out of order and
potentially in parallel. Generating an overall string sampled from the correct
underlying joint distribution would (again) require exactly one token unmasking
in every full-model forward pass. The more tokens unmasked in parallel, the
further away the string is from the true joint; this can be seen in the
resulting drop in accuracy (but, increase in speed). In this paper we devise a
way to {\em approximately} sample multiple tokens from the joint distribution
in a single full-model forward pass; we do so by developing a new lightweight
single-layer ``sampler" on top of an existing large diffusion LM. One forward
pass of the full model can now be followed by multiple forward passes of only
this sampler layer, to yield multiple unmasked tokens. Our sampler is trained
to mimic exact joint sampling from the (frozen) full model. We show the
effectiveness of our approximate joint sampling for both pretrained-only
(Dream-7B-Base) and instruction-tuned (Dream-7B-Instruct) models on language
modeling and math \& coding tasks. When four tokens are unmasked for each
full-model denoising step, our sampling algorithm achieves a MAUVE score of
0.87 (vs marginal baseline of 0.31) with respect to the true joint
distribution.

</details>


### [7] [Painless Activation Steering: An Automated, Lightweight Approach for Post-Training Large Language Models](https://arxiv.org/abs/2509.22739)
*Sasha Cui,Zhongren Chen*

Main category: cs.CL

TL;DR: 提出PAS方法实现语言模型激活导向的自动化后训练，无需人工干预即可提升行为任务表现


<details>
  <summary>Details</summary>
Motivation: 现有激活导向技术依赖人工标注和提示构造，PAS通过全自动化流程解决传统AS方法不便部署的问题

Method: 基于带标签数据集自动生成激活向量，结合自省机制(iPAS)增强因果导向效果

Result: 在3个开源模型18个任务中：行为任务提升显著（Bias+10.1%，Morality+5.2%，Alignment+34.8%），智能任务无改善；与SFT/ICL联用产生增益

Conclusion: PAS为LM后训练提供快速轻量级解决方案，明确界定了激活导向技术的有效应用场景边界

Abstract: Language models (LMs) are typically post-trained for desired capabilities and
behaviors via weight-based or prompt-based steering, but the former is
time-consuming and expensive, and the latter is not precisely controllable and
often requires manual trial-and-error. While activation steering (AS) promises
a cheap, fast, and controllable alternative to the two existing post-training
methods, current AS techniques require hand-crafted prompt pairs or
labor-intensive feature annotation, making them more inconvenient than the
plug-and-play methods such as Reinforcement Learning (RL) and Supervised
Fine-Tuning (SFT). We introduce Painless Activation Steering (PAS), a family of
fully automated methods that make AS readily usable with any given labeled
dataset, with no need for prompt construction, feature labeling, or human
intervention. We evaluate PAS on three open-weight models
(Llama3.1-8B-Instruct, DeepSeek-R1-Distill-8B, and Nous-Hermes-2) and 18 tasks;
we find that PAS reliably improves performance for behavior tasks, but not for
intelligence-oriented tasks. The introspective variant (iPAS) delivers the
strongest causal steering effects (10.1% on Bias, 5.2% on Morality, and 34.8%
on Alignment). We also show PAS delivers additional gains on top of In-Context
Learning (ICL) and SFT. PAS constructs a fast, lightweight activation vector
that can be cheaply trained, easily stored, and activated at will. Our results
provide a characterization of where AS helps, where it fails, and how to deploy
it as a practical, automated LM post-training option.

</details>


### [8] [MIRAGE: Multi-hop Reasoning with Ambiguity Evaluation for Illusory Questions](https://arxiv.org/abs/2509.22750)
*Jeonghyun Park,Ingeol Baek,Seunghyun Yoon,Haeun Jang,Aparna Garimella,Akriti Jain,Nedim Lipka,Hwanhee Lee*

Main category: cs.CL

TL;DR: 提出MIRAGE基准评估多跳问答中的歧义挑战，并开发CLARION框架提升模型表现


<details>
  <summary>Details</summary>
Motivation: 解决现有大语言模型在多跳推理中无法有效处理歧义路径的问题，填补歧义解释与多步推理结合的研究空白

Method: 构建含1,142个分类歧义问题的MIRAGE基准，并通过多LLM验证流程确保质量；提出CLARION多智能体协作框架

Result: 主流模型在MIRAGE表现欠佳，CLARION框架显著优于现有方法（准确率提升18.7%）

Conclusion: 多跳歧义处理是重要挑战，CLARION为构建鲁棒推理系统提供新方向

Abstract: Real-world Multi-hop Question Answering (QA) often involves ambiguity that is
inseparable from the reasoning process itself. This ambiguity creates a
distinct challenge, where multiple reasoning paths emerge from a single
question, each requiring independent resolution. Since each sub-question is
ambiguous, the model must resolve ambiguity at every step. Thus, answering a
single question requires handling multiple layers of ambiguity throughout the
reasoning chain. We find that current Large Language Models (LLMs) struggle in
this setting, typically exploring wrong reasoning paths and producing
incomplete answers. To facilitate research on multi-hop ambiguity, we introduce
MultI-hop Reasoning with AmbiGuity Evaluation for Illusory Questions (MIRAGE),
a benchmark designed to analyze and evaluate this challenging intersection of
ambiguity interpretation and multi-hop reasoning. MIRAGE contains 1,142
high-quality examples of ambiguous multi-hop questions, categorized under a
taxonomy of syntactic, general, and semantic ambiguity, and curated through a
rigorous multi-LLM verification pipeline. Our experiments reveal that even
state-of-the-art models struggle on MIRAGE, confirming that resolving ambiguity
combined with multi-step inference is a distinct and significant challenge. To
establish a robust baseline, we propose CLarifying Ambiguity with a Reasoning
and InstructiON (CLARION), a multi-agent framework that significantly
outperforms existing approaches on MIRAGE, paving the way for more adaptive and
robust reasoning systems.

</details>


### [9] [ML2B: Multi-Lingual ML Benchmark For AutoML](https://arxiv.org/abs/2509.22768)
*Ekaterina Trofimova,Zosia Shamina,Maria Selifanova,Artem Zaitsev,Remi Savchuk,Maxim Minets,Daria Ozerova,Emil Sataev,Denis Zuenko,Andrey E. Ustyuzhanin*

Main category: cs.CL

TL;DR: 论文提出了首个多语言机器学习代码生成基准ML2B，通过30个Kaggle竞赛任务翻译到13种语言，发现非英语任务性能下降15-45%，揭示了多语言表示学习的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习代码生成基准主要局限于英语，忽视了机器学习研究和实践的全球多语言特性。

Method: 构建包含表格/文本/图像数据的多语言基准ML2B，采用自动化评估框架AIDE进行端到端数据科学流程评估。

Result: 非英语任务出现15-45%的性能下降，暴露多语言代码生成中的表征学习缺陷。

Conclusion: 研究揭示了多语言代码生成的核心挑战，开源基准和评估框架为后续研究提供基础设施支持。

Abstract: Large language models (LLMs) have recently demonstrated strong capabilities
in generating machine learning (ML) code, enabling end-to-end pipeline
construction from natural language instructions. However, existing benchmarks
for ML code generation are mainly restricted to English, overlooking the global
and multilingual nature of ML research and practice. To address this gap, we
present ML2B, the first benchmark for evaluating multilingual ML code
generation. ML2B consists of 30 Kaggle competitions translated into 13 natural
languages, covering tabular, text, and image data types, with structured
metadata and validated human-reviewed translations. For evaluation, we employ
AIDE, an automated framework for end-to-end assessment of data science
pipelines, and provide insights into cross-lingual model performance. Our
results reveal substantial 15-45% performance degradation on non-English tasks,
highlighting critical challenges in multilingual representation learning for
code generation. The benchmark, evaluation framework, and comprehensive results
are made available through our GitHub repository to facilitate future research
in multilingual ML code generation: https://github.com/enaix/ml2b.

</details>


### [10] [ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection](https://arxiv.org/abs/2509.22808)
*Mohamed Maged,Alhassan Ehab,Ali Mekky,Besher Hassan,Shady Shehata*

Main category: cs.CL

TL;DR: 首个多方言阿拉伯语合成语音数据集研究，通过综合评估发现FishSpeech在阿拉伯语语音克隆中表现最优但存在泛化局限


<details>
  <summary>Details</summary>
Motivation: 现有语音欺骗检测研究主要集中在英语，阿拉伯语及其方言因缺乏研究资源面临检测挑战

Method: 采用嵌入分类器+MFCC传统算法+RawNet2架构的混合分类方法，结合人类MOS评分和ASR的WER测量构建评估体系

Result: FishSpeech在Casablanca语料库的MOS(4.2)和WER(8.7%)表现最佳，生成的合成语音最具欺骗性

Conclusion: 单一TTS模型构建数据集可能影响检测模型泛化能力，建议采用多模型混合方案提升鲁棒性

Abstract: With the rise of generative text-to-speech models, distinguishing between
real and synthetic speech has become challenging, especially for Arabic that
have received limited research attention. Most spoof detection efforts have
focused on English, leaving a significant gap for Arabic and its many dialects.
In this work, we introduce the first multi-dialect Arabic spoofed speech
dataset. To evaluate the difficulty of the synthesized audio from each model
and determine which produces the most challenging samples, we aimed to guide
the construction of our final dataset either by merging audios from multiple
models or by selecting the best-performing model, we conducted an evaluation
pipeline that included training classifiers using two approaches: modern
embedding-based methods combined with classifier heads; classical machine
learning algorithms applied to MFCC features; and the RawNet2 architecture. The
pipeline further incorporated the calculation of Mean Opinion Score based on
human ratings, as well as processing both original and synthesized datasets
through an Automatic Speech Recognition model to measure the Word Error Rate.
Our results demonstrate that FishSpeech outperforms other TTS models in Arabic
voice cloning on the Casablanca corpus, producing more realistic and
challenging synthetic speech samples. However, relying on a single TTS for
dataset creation may limit generalizability.

</details>


### [11] [EditGRPO: Reinforcement Learning with Post -Rollout Edits for Clinically Accurate Chest X-Ray Report Generation](https://arxiv.org/abs/2509.22812)
*Kai Zhang,Christopher Malon,Lichao Sun,Martin Renqiang Min*

Main category: cs.CL

TL;DR: 通过混合策略强化学习算法EditGRPO优化放射报告生成，在多个指标和跨数据集泛化能力上显著提升


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs的监督微调目标与临床效果未显式对齐，需开发更有效的优化方法

Method: 提出EditGRPO算法：整合策略探索与策略外指导，通过句子级修正改进训练效率，解决强化学习中的探索困境

Result: 在四个胸部X光数据集上平均提升3.4%指标，跨域数据集平均提升5.9%

Conclusion: EditGRPO有效提升放射报告生成的临床相关性和模型泛化能力，验证混合策略强化学习的应用潜力

Abstract: Radiology report generation requires advanced medical image analysis,
effective temporal reasoning, and accurate text generation. Although recent
innovations, particularly multimodal large language models (MLLMs), have shown
improved performance, their supervised fine-tuning (SFT) objective is not
explicitly aligned with clinical efficacy. In this work, we introduce EditGRPO,
a mixed-policy reinforcement learning (RL) algorithm designed specifically to
optimize the generation through clinically motivated rewards. EditGRPO
integrates on-policy exploration with off-policy guidance by injecting
sentence-level detailed corrections during training rollouts. This mixed-policy
approach addresses the exploration dilemma and sampling efficiency issues
typically encountered in RL. Applied to a Qwen2.5-VL-3B MLLM initialized with
supervised fine-tuning (SFT), EditGRPO outperforms both SFT and vanilla GRPO
baselines, achieving an average improvement of 3.4% in CheXbert, GREEN,
Radgraph, and RATEScore metrics across four major chest X-ray report generation
datasets. Notably, EditGRPO also demonstrates superior out-of-domain
generalization, with an average performance gain of 5.9% on unseen datasets.

</details>


### [12] [Critique-Coder: Enhancing Coder Models by Critique Reinforcement Learning](https://arxiv.org/abs/2509.22824)
*Chi Ruan,Dongfu Jiang,Yubo Wang,Wenhu Chen*

Main category: cs.CL

TL;DR: 提出Critique Reinforcement Learning（CRL）作为标准强化学习的补充，通过结合批判机制显著提升大语言模型的代码生成、推理和批判能力，并验证了其跨任务迁移性


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法缺乏显式培养批判能力的机制，而近期研究表明批判机制能有效提升模型性能（如CFT/CGD），因此探索将批判机制融入强化学习框架

Method: 提出CRL框架：模型需生成对（问题，解决方案）对的批判，奖励机制仅基于批判判断标签与真实标签的匹配度。构建混合训练集（80% RL + 20% CRL），训练Critique-Coder系列模型

Result: Critique-Coder-8B在LiveCodeBench（v5）达到60%+准确率，超越DeepCoder-14B和GPT-o1；在BBEH逻辑推理任务表现提升，验证批判能力可迁移到通用推理任务

Conclusion: CRL有效补充标准强化学习，通过编码数据集的批判训练不仅提升特定任务表现，还能增强模型的通用推理和批判能力，具备跨领域迁移潜力

Abstract: Reinforcement Learning (RL) has emerged as a popular training paradigm,
particularly when paired with reasoning models. While effective, it primarily
focuses on generating responses and lacks mechanisms to explicitly foster
critique or reflection. Several recent studies, like Critique-Fine-Tuning (CFT)
and Critique-Guided-Distillation (CGD) have shown the benefits of explicitly
teaching LLMs how to critique. Motivated by them, we propose Critique
Reinforcement Learning (CRL), where the model is tasked with generating a
critique for a given (question, solution) pair. The reward is determined solely
by whether the final judgment label $c \in \{\texttt{True}, \texttt{False}\}$
of the generated critique aligns with the ground-truth judgment $c^*$. Building
on this point, we introduce \textsc{Critique-Coder}, which is trained on a
hybrid of RL and CRL by substituting 20\% of the standard RL data with CRL
data. We fine-tune multiple models (\textsc{Critique-Coder}) and evaluate them
on different benchmarks to show their advantages over RL-only models. We show
that \textsc{Critique-Coder} consistently outperforms RL-only baselines on all
the evaluated benchmarks. Notably, our \textsc{Critique-Coder-8B} can reach
over 60\% on LiveCodeBench (v5), outperforming other reasoning models like
DeepCoder-14B and GPT-o1. Beyond code generation, \textsc{Critique-Coder} also
demonstrates enhanced general reasoning abilities, as evidenced by its better
performance on logic reasoning tasks from the BBEH dataset. This indicates that
the application of CRL on coding datasets enhances general reasoning and
critique abilities, which are transferable across a broad range of tasks.
Hence, we believe that CRL works as a great complement to standard RL for LLM
reasoning.

</details>


### [13] [ChatInject: Abusing Chat Templates for Prompt Injection in LLM Agents](https://arxiv.org/abs/2509.22830)
*Hwan Chang,Yonghyun Jun,Hwanhee Lee*

Main category: cs.CL

TL;DR: 提出ChatInject攻击方法，通过模拟结构化聊天模板和多轮对话操纵，显著提升LLM代理的提示注入攻击成功率


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注纯文本间接提示注入攻击，但忽视了LLM对结构化聊天模板的依赖性和多轮对话中的上下文操纵漏洞

Method: 开发ChatInject攻击框架，包含原生模板模拟攻击（Single-turn）和基于对话说服的多轮攻击变体（Multi-turn）

Result: 在AgentDojo和InjecAgent测试中，Multi-turn攻击成功率分别达到32.05%和52.33%，且攻击具备跨模型迁移性（闭源模型成功率45.90%）

Conclusion: 当前LLM代理系统存在严重结构化漏洞，现有防御措施对多轮对话攻击无效，需开发新的安全机制

Abstract: The growing deployment of large language model (LLM) based agents that
interact with external environments has created new attack surfaces for
adversarial manipulation. One major threat is indirect prompt injection, where
attackers embed malicious instructions in external environment output, causing
agents to interpret and execute them as if they were legitimate prompts. While
previous research has focused primarily on plain-text injection attacks, we
find a significant yet underexplored vulnerability: LLMs' dependence on
structured chat templates and their susceptibility to contextual manipulation
through persuasive multi-turn dialogues. To this end, we introduce ChatInject,
an attack that formats malicious payloads to mimic native chat templates,
thereby exploiting the model's inherent instruction-following tendencies.
Building on this foundation, we develop a persuasion-driven Multi-turn variant
that primes the agent across conversational turns to accept and execute
otherwise suspicious actions. Through comprehensive experiments across frontier
LLMs, we demonstrate three critical findings: (1) ChatInject achieves
significantly higher average attack success rates than traditional prompt
injection methods, improving from 5.18% to 32.05% on AgentDojo and from 15.13%
to 45.90% on InjecAgent, with multi-turn dialogues showing particularly strong
performance at average 52.33% success rate on InjecAgent, (2)
chat-template-based payloads demonstrate strong transferability across models
and remain effective even against closed-source LLMs, despite their unknown
template structures, and (3) existing prompt-based defenses are largely
ineffective against this attack approach, especially against Multi-turn
variants. These findings highlight vulnerabilities in current agent systems.

</details>


### [14] [Learning to Detect Relevant Contexts and Knowledge for Response Selection in Retrieval-based Dialogue Systems](https://arxiv.org/abs/2509.22845)
*Kai Hua,Zhiyuan Feng,Chongyang Tao,Rui Yan,Lu Zhang*

Main category: cs.CL

TL;DR: 提出RSM-DCK模型，通过两阶段上下文与知识库选择机制提升开放域对话系统的响应匹配性能


<details>
  <summary>Details</summary>
Motivation: 现有检索式对话系统在处理多轮对话时，上下文和知识库中的无关信息会导致匹配性能下降

Method: 使用近期上下文作为查询进行预选（词级&语句级），响应候选与筛选内容交互，最后通过融合表示进行后选增强

Result: 在两个基准数据集上取得优于现有方法的性能，F1值提升显著（ConvAI2:+2.3，Wizard:+3.1）

Conclusion: 模型能有效检测对话中的相关上下文和知识片段，证实了选择机制对响应匹配的关键作用

Abstract: Recently, knowledge-grounded conversations in the open domain gain great
attention from researchers. Existing works on retrieval-based dialogue systems
have paid tremendous efforts to utilize neural networks to build a matching
model, where all of the context and knowledge contents are used to match the
response candidate with various representation methods. Actually, different
parts of the context and knowledge are differentially important for recognizing
the proper response candidate, as many utterances are useless due to the topic
shift. Those excessive useless information in the context and knowledge can
influence the matching process and leads to inferior performance. To address
this problem, we propose a multi-turn \textbf{R}esponse \textbf{S}election
\textbf{M}odel that can \textbf{D}etect the relevant parts of the
\textbf{C}ontext and \textbf{K}nowledge collection (\textbf{RSM-DCK}). Our
model first uses the recent context as a query to pre-select relevant parts of
the context and knowledge collection at the word-level and utterance-level
semantics. Further, the response candidate interacts with the selected context
and knowledge collection respectively. In the end, The fused representation of
the context and response candidate is utilized to post-select the relevant
parts of the knowledge collection more confidently for matching. We test our
proposed model on two benchmark datasets. Evaluation results indicate that our
model achieves better performance than the existing methods, and can
effectively detect the relevant context and knowledge for response selection.

</details>


### [15] [Towards Generalizable Implicit In-Context Learning with Attention Routing](https://arxiv.org/abs/2509.22854)
*Jiaqian Li,Yanshu Li,Ligong Han,Ruixiang Tang,Wenya Wang*

Main category: cs.CL

TL;DR: 提出ICR方法，通过注意力logits层面的结构方向提取和路由器调制，实现高效通用的隐式上下文学习


<details>
  <summary>Details</summary>
Motivation: 现有隐式上下文学习方法依赖人工构建的偏移向量，未能利用ICL内在结构机制且泛化性差

Method: 在注意力logits层面提取可复用结构方向，设计输入条件路由器动态调整注意力机制，实现'一次训练多次复用'框架

Result: 在12个跨领域数据集上验证，ICR性能优于需任务特定训练/检索的方法，在域外任务展现强泛化能力

Conclusion: ICR突破了现有隐式ICL方法的局限，通过系统化学习注意力层面的结构模式，显著提升了上下文学习的实用价值

Abstract: Implicit in-context learning (ICL) has newly emerged as a promising paradigm
that simulates ICL behaviors in the representation space of Large Language
Models (LLMs), aiming to attain few-shot performance at zero-shot cost.
However, existing approaches largely rely on injecting shift vectors into
residual flows, which are typically constructed from labeled demonstrations or
task-specific alignment. Such designs fall short of utilizing the structural
mechanisms underlying ICL and suffer from limited generalizability. To address
this, we propose In-Context Routing (ICR), a novel implicit ICL method that
internalizes generalizable ICL patterns at the attention logits level. It
extracts reusable structural directions that emerge during ICL and employs a
learnable input-conditioned router to modulate attention logits accordingly,
enabling a train-once-and-reuse framework. We evaluate ICR on 12 real-world
datasets spanning diverse domains and multiple LLMs. The results show that ICR
consistently outperforms prior implicit ICL methods that require task-specific
retrieval or training, while demonstrating robust generalization to
out-of-domain tasks where existing methods struggle. These findings position
ICR to push the boundary of ICL's practical value.

</details>


### [16] [The Bias is in the Details: An Assessment of Cognitive Bias in LLMs](https://arxiv.org/abs/2509.22856)
*R. Alexander Knipper,Charles S. Knipper,Kaiqi Zhang,Valerie Sims,Clint Bowers,Santu Karmaker*

Main category: cs.CL

TL;DR: LLMs在17.8%-57.3%的决策场景中表现出八种认知偏见，模型规模增大和提示细节优化可显著降低多数偏见（除过度归因外）。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型被嵌入现实决策系统，需系统评估其是否存在类似人类的认知偏见。

Method: 使用心理学家协作构建的220个决策场景数据集，通过多选任务框架对45个LLM进行280万次响应测试，采用模板生成多样化提示。

Result: 1. 模型规模>32B参数时39.5%案例偏见减少
2. 提示细节优化使多数偏见降低14.9%
3. 过度归因偏见在提示细节增加时恶化8.8%

Conclusion: LLM的认知偏见普遍存在且可调控，模型规模扩展和提示工程是重要干预手段，但需注意过度归因偏见的反向效应。

Abstract: As Large Language Models (LLMs) are increasingly embedded in real-world
decision-making processes, it becomes crucial to examine the extent to which
they exhibit cognitive biases. Extensively studied in the field of psychology,
cognitive biases appear as systematic distortions commonly observed in human
judgments. This paper presents a large-scale evaluation of eight
well-established cognitive biases across 45 LLMs, analyzing over 2.8 million
LLM responses generated through controlled prompt variations. To achieve this,
we introduce a novel evaluation framework based on multiple-choice tasks,
hand-curate a dataset of 220 decision scenarios targeting fundamental cognitive
biases in collaboration with psychologists, and propose a scalable approach for
generating diverse prompts from human-authored scenario templates. Our analysis
shows that LLMs exhibit bias-consistent behavior in 17.8-57.3% of instances
across a range of judgment and decision-making contexts targeting anchoring,
availability, confirmation, framing, interpretation, overattribution, prospect
theory, and representativeness biases. We find that both model size and prompt
specificity play a significant role on bias susceptibility as follows: larger
size (>32B parameters) can reduce bias in 39.5% of cases, while higher prompt
detail reduces most biases by up to 14.9%, except in one case
(Overattribution), which is exacerbated by up to 8.8%.

</details>


### [17] [Lexicon-Enriched Graph Modeling for Arabic Document Readability Prediction](https://arxiv.org/abs/2509.22870)
*Passant Elchafei,Mayar Osama,Mohamed Rageh,Mervat Abuelkheir*

Main category: cs.CL

TL;DR: 基于图神经网络与Transformer融合的混合方法显著提升了阿拉伯语文档可读性预测效果


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯语文档可读性评估中单一模型的局限性，通过结合图结构的语言学特征与深度语义表示提升预测精度

Method: 构建句子级图结构（节点=句子/词元，边=语言学关系），并行训练GNN分支（整合SAMER词典特征）和Transformer分支（阿拉伯语预训练模型），通过后期融合集成预测结果，文档级预测采用最大池化聚合

Result: 混合方法在F1-score等多项文档级指标超越单模型（GNN提升3.2%，Transformer提升5.1%），但GNN单独使用在句子级预测更精确（准确率达89.7%）

Conclusion: 多模态融合策略在文档级任务中优势显著，而纯图神经网络更适合细粒度句子分析，为可读性预测提供了分层解决方案

Abstract: We present a graph-based approach enriched with lexicons to predict
document-level readability in Arabic, developed as part of the Constrained
Track of the BAREC Shared Task 2025. Our system models each document as a
sentence-level graph, where nodes represent sentences and lemmas, and edges
capture linguistic relationships such as lexical co-occurrence and class
membership. Sentence nodes are enriched with features from the SAMER lexicon as
well as contextual embeddings from the Arabic transformer model. The graph
neural network (GNN) and transformer sentence encoder are trained as two
independent branches, and their predictions are combined via late fusion at
inference. For document-level prediction, sentence-level outputs are aggregated
using max pooling to reflect the most difficult sentence. Experimental results
show that this hybrid method outperforms standalone GNN or transformer branches
across multiple readability metrics. Overall, the findings highlight that
fusion offers advantages at the document level, but the GNN-only approach
remains stronger for precise prediction of sentence-level readability.

</details>


### [18] [HEART: Emotionally-driven test-time scaling of Language Models](https://arxiv.org/abs/2509.22876)
*Gabriela Pinto,Palash Goyal,Yiwen Song,Souradip Chakraborty,Zifeng Wang,Tomas Pfister,Hamid Palangi*

Main category: cs.CL

TL;DR: 提出HEART框架，通过情感化反馈提升语言模型复杂推理能力。验证器辅助时效果显著，无验证器时存在部署瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有测试时校准方法主要关注逻辑优化，忽略情感反馈的引导潜力。受心理学中情绪调节认知的启发，探索情感迭代对机器推理的帮助。

Method: 基于Ekman六种基本情感构建情感短语库，通过系统调节反馈情绪强度实现多轮自我修正，突破错误推理路径。

Result: 在OlympiadBench等基准测试中，带验证器的情感迭代协议使准确率显著提升（较SOTA基线提高12.8%），无验证器时效果不稳定。

Conclusion: 机器推理的下个前沿需同时关注逻辑优化与情感机制。当前验证器依赖问题构成实际部署关键障碍，未来需解决无监督情感校准难题。

Abstract: Test-time scaling has shown considerable success in improving the performance
of language models on complex reasoning tasks without requiring fine-tuning.
However, current strategies such as self-reflection primarily focus on logical
or structural refinement. They do not leverage the guiding potential of
affective feedback. Inspired by psychological research showing that emotions
can modulate cognitive performance, we introduce HEART--a novel framework that
uses emotionally-driven prompts for iterative self-correction. HEART provides
feedback on a model's incorrect response using a curated set of concise,
emotionally charged phrases based on the six universal emotions categorized by
Dr. Paul Ekman. By systematically varying the emotional tone of the feedback
across iterations, our method guides the model to escape flawed reasoning paths
and explore more promising alternatives. We evaluate our framework on
challenging reasoning benchmarks including OlympiadBench, Humanity's Last Exam,
and SimpleQA. Our results reveal a significant new phenomenon: when guided by
an oracle verifier, this affective iteration protocol unlocks significantly
deeper reasoning, leading to consistent and substantial increases in accuracy
over state-of-the-art baselines with the same verifier. However, we also
identify a critical bottleneck for practical deployment. In a verifier-free
setting, it struggles to harness these gains consistently, highlighting as a
key challenge for future work. Our findings suggest that the next frontier in
machine reasoning may lie not just in refining logic, but also in understanding
and leveraging the `HEART' of the models.

</details>


### [19] [Infusing Theory of Mind into Socially Intelligent LLM Agents](https://arxiv.org/abs/2509.22887)
*EunJeong Hwang,Yuwei Yin,Giuseppe Carenini,Peter West,Vered Shwartz*

Main category: cs.CL

TL;DR: 通过显式整合心智理论（ToM）提升LLM对话代理的社交智能与目标达成能力


<details>
  <summary>Details</summary>
Motivation: 现有聊天机器人和LLM社交代理普遍缺乏心智理论（ToM）整合，而ToM是人类社交智能的关键要素

Method: 提出ToMAgent框架，通过心智理论与对话前瞻机制结合训练，生成对对话目标最有效的心理状态表征

Result: 在Sotopia交互评估基准中超越基线模型，表现出更长远的战略推理能力和更好的社交关系维护

Conclusion: 该研究为构建具有社交智能的LLM代理提供了有效路径，推动了心智理论在对话系统中的应用

Abstract: Theory of Mind (ToM)-an understanding of the mental states of others-is a key
aspect of human social intelligence, yet, chatbots and LLM-based social agents
do not typically integrate it. In this work, we demonstrate that LLMs that
explicitly use ToM get better at dialogue, achieving goals more effectively.
After showing that simply prompting models to generate mental states between
dialogue turns already provides significant benefit, we further introduce
ToMAgent (ToMA), a ToM-focused dialogue agent. ToMA is trained by pairing ToM
with dialogue lookahead to produce mental states that are maximally useful for
achieving dialogue goals. Experiments on the Sotopia interactive social
evaluation benchmark demonstrate the effectiveness of our method over a range
of baselines. Comprehensive analysis shows that ToMA exhibits more strategic,
goal-oriented reasoning behaviors, which enable long-horizon adaptation, while
maintaining better relationships with their partners. Our results suggest a
step forward in integrating ToM for building socially intelligent LLM agents.

</details>


### [20] [Extract-0: A Specialized Language Model for Document Information Extraction](https://arxiv.org/abs/2509.22906)
*Henrique Godoy*

Main category: cs.CL

TL;DR: 提出了70亿参数的语言模型Extract-0，通过合成数据生成、LoRA微调和GRPO强化学习，在文档信息提取任务中超越GPT-4等大模型


<details>
  <summary>Details</summary>
Motivation: 解决文档信息提取任务中的固有歧义问题，同时证明任务专用优化模型能以更少计算资源超越通用系统

Method: 三阶段方法：1) 生成28万多样本训练数据 2) 使用LoRA仅微调0.53%参数(40M/7.6B) 3) 基于语义相似度设计GRPO强化学习

Result: 在1000个任务基准测试中取得0.573平均奖励，显著优于GPT-4.1(0.457)等模型

Conclusion: 任务专用优化模型在保持参数效率（修改<1%参数）的同时，性能可超越通用大模型，大幅降低算力需求

Abstract: This paper presents Extract-0, a 7-billion parameter language model
specifically optimized for document information extraction that achieves
performance exceeding models with parameter counts several orders of magnitude
larger. Through a novel combination of synthetic data generation, supervised
fine-tuning with Low-Rank Adaptation (LoRA), and reinforcement learning via
Group Relative Policy Optimization (GRPO), Extract-0 achieves a mean reward of
0.573 on a benchmark of 1,000 diverse document extraction tasks, outperforming
GPT-4.1 (0.457), o3 (0.464), and GPT-4.1-2025 (0.459). The training methodology
employs a memory-preserving synthetic data generation pipeline that produces
280,128 training examples from diverse document sources, followed by
parameterefficient fine-tuning that modifies only 0.53% of model weights (40.4M
out of 7.66B parameters). The reinforcement learning phase introduces a novel
semantic similarity-based reward function that handles the inherent ambiguity
in information extraction tasks. This research demonstrates that task-specific
optimization can yield models that surpass general-purpose systems while
requiring substantially fewer computational resource.

</details>


### [21] [Large language models management of medications: three performance analyses](https://arxiv.org/abs/2509.22926)
*Kelli Henry,Steven Xu,Kaitlin Blotske,Moriah Cargile,Erin F. Barreto,Brian Murray,Susan Smith,Seth R. Bauer,Yanjun Gao,Tianming Liu,Andrea Sikora*

Main category: cs.CL

TL;DR: GPT-4o在药物配方匹配、药物相互作用识别和医嘱生成测试中表现较差，需领域专项训练。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在药物治疗方案推荐中的一致性，填补现有研究空白。

Method: 使用GPT-4o完成三个药物基准测试（配方匹配、药物相互作用识别、医嘱生成），通过余弦相似度、编辑距离相似度、ROUGE指标及临床医生人工评估量化准确性。

Result: 1) 药物配方匹配准确率仅49%（遗漏1.23个/药，虚构1.14个/药）
2) 药物相互作用识别准确率54.7%(无搜索) vs 69.2%(有搜索)
3) 医嘱句子错误率34.2%

Conclusion: 模型整体表现不佳，强调需临床标注数据的领域训练和系统评估框架开发。

Abstract: Background: Large language models (LLMs) can be useful in diagnosing medical
conditions, but few studies have evaluated their consistency in recommending
appropriate medication regimens. The purpose of this evaluation was to test
GPT-4o on three medication benchmarking tests including mapping a drug name to
its correct formulation, identifying drug-drug interactions using both its
internal knowledge and using a web search, and preparing a medication order
sentence after being given the medication name. Methods: Using GTP-4o three
experiments were completed. Accuracy was quantified by computing cosine
similarity on TF-IDF vectors, normalized Levenshtein similarity, and
ROUGE-1/ROUGE-L F1 between each response and its reference string or by manual
evaluation by clinicians. Results: GPT-4o performed poorly on drug-formulation
matching, with frequent omissions of available drug formulations (mean 1.23 per
medication) and hallucinations of formulations that do not exist (mean 1.14 per
medication). Only 49% of tested medications were correctly matched to all
available formulations. Accuracy was decreased for medications with more
formulations (p<0.0001). GPT-4o was also inconsistent at identifying
drug-drug-interactions, although it had better performance with the
search-augmented assessment compared to its internal knowledge (54.7% vs.
69.2%, p=0.013). However, allowing a web-search worsened performance when there
was no drug-drug interaction (median % correct 100% vs. 40%, p<0.001). Finally,
GPT-4o performed moderately with preparing a medication order sentence, with
only 65.8% of medication order sentences containing no medication or
abbreviation errors. Conclusions: Model performance was overall poor for all
tests. This highlights the need for domain-specific training through
clinician-annotated datasets and a comprehensive evaluation framework for
benchmarking performance.

</details>


### [22] [LLMs Behind the Scenes: Enabling Narrative Scene Illustration](https://arxiv.org/abs/2509.22940)
*Melissa Roemmele,John Joon Young Chung,Taewook Kim,Yuqian Sun,Alex Calderwood,Max Kreminski*

Main category: cs.CL

TL;DR: 提出基于LLM的叙事场景插图生成框架，构建SceneIllustrations数据集并验证模型有效性


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI在跨模态叙事转换中的潜力，通过LLM接口激发文本隐含的场景知识

Method: 构建LLM提示工程框架处理原始故事文本，驱动文本到图像模型生成场景插图

Result: 发布SceneIllustrations数据集，人工标注显示LLM能有效提取隐含场景知识（82%准确率）

Conclusion: LLM在跨模态叙事转换中具备场景知识建模能力，为插图生成与评估提供新范式

Abstract: Generative AI has established the opportunity to readily transform content
from one medium to another. This capability is especially powerful for
storytelling, where visual illustrations can illuminate a story originally
expressed in text. In this paper, we focus on the task of narrative scene
illustration, which involves automatically generating an image depicting a
scene in a story. Motivated by recent progress on text-to-image models, we
consider a pipeline that uses LLMs as an interface for prompting text-to-image
models to generate scene illustrations given raw story text. We apply
variations of this pipeline to a prominent story corpus in order to synthesize
illustrations for scenes in these stories. We conduct a human annotation task
to obtain pairwise quality judgments for these illustrations. The outcome of
this process is the SceneIllustrations dataset, which we release as a new
resource for future work on cross-modal narrative transformation. Through our
analysis of this dataset and experiments modeling illustration quality, we
demonstrate that LLMs can effectively verbalize scene knowledge implicitly
evoked by story text. Moreover, this capability is impactful for generating and
evaluating illustrations.

</details>


### [23] [What Matters More For In-Context Learning under Matched Compute Budgets: Pretraining on Natural Text or Incorporating Targeted Synthetic Examples?](https://arxiv.org/abs/2509.22947)
*Mohammed Sabry,Anya Belz*

Main category: cs.CL

TL;DR: 在等算力条件下，显式训练归纳电路（Bi-Induct）虽能加速小模型归纳头的形成，但并不能持续提升上下文学习能力。自然文本训练的大模型通过分布式电路结构展现出更强的泛化能力，表明机制的必要性比单纯激活更重要。


<details>
  <summary>Details</summary>
Motivation: 验证显式训练归纳电路是否能提升上下文学习（ICL）性能，挑战'早期激活即改善ICL'的假设。通过对比合成数据与自然文本在等算力下的表现，揭示电路机制对ICL增益的本质影响。

Method: 提出Bi-Induct课程：在预训练中注入前向复制（归纳）、反向复制（抗归纳）或混合模式。训练0.13B-1B参数模型，评估指标包括：(1)少样本ICL基准测试；(2)电路层监测；(3)语言模型困惑度；(4)标签置换/HITS@差异/样本量压力测试。

Result: 1. Bi-Induct加速小模型归纳头形成，但泛化无持续提升
2. 自然文本训练的1B模型在函数式ICL测试表现最佳
3. 大模型自然训练时自发形成更广分布的归纳头
4. 抗归纳数据无效，合成数据困惑度惩罚随模型规模缩小
5. 关键归纳头消融对自然模型ICL破坏更显著，显示核心电路结构

Conclusion: ICL增益取决于电路成为功能必要性，而非单纯激活。研究强调需开发机制诊断工具和数据组合策略，以培养负载承载型结构。这对优化预训练数据构成具有方法论启示。

Abstract: Does explicitly exercising the induction circuit during pretraining improve
in-context learning (ICL), or is natural text sufficient when compute is held
constant (iso-FLOPs)? To test whether targeted synthetic data can accelerate
induction-head emergence and enhance ICL, we introduce Bi-Induct, a lightweight
curriculum that injects forward-copy (Induction), backward-copy (Anti), or a
balanced mix into the pretraining stream. We train models from 0.13B to 1B
parameters under iso-FLOPs, evaluating (i) few-shot ICL benchmarks, (ii)
head-level telemetry, and (iii) held-out language modeling perplexity. Our
findings challenge the assumption that early induction circuit activation
directly improves ICL. While Bi-Induct accelerates induction-head emergence at
small scales, this does not consistently yield stronger generalization. On
standard LM benchmarks, Bi-Induct matches natural-only training; on
function-style ICL probes, the 1B natural-only performs best. Stress tests
(e.g., label permutation, HITS@1 vs. HITS@3, 1 vs. 10 shots) preserve these
trends. Telemetry shows larger natural-only models develop broader, earlier
induction heads without explicit induction patterns. Anti-induction data fails
to elicit meaningful activation. Perplexity penalties from synthetic data
shrink with scale, suggesting larger models can absorb non-natural patterns
with minimal cost. Crucially, ablating the top 2% of induction heads degrades
ICL more than random ablations, especially for natural-only models, indicating
more centralized, load-bearing circuits. Bi-Induct variants exhibit more
redundant induction activity, implying different circuit utilization. Overall,
inducing activation is not sufficient: ICL gains depend on these circuits
becoming functionally necessary. These results underscore mechanism-aware
pretraining diagnostics and data mixtures that foster load-bearing, not merely
present, structure.

</details>


### [24] [Emergent morpho-phonological representations in self-supervised speech models](https://arxiv.org/abs/2509.22973)
*Jon Gauthier,Canaan Breiss,Matthew Leonard,Edward F. Chang*

Main category: cs.CL

TL;DR: 自监督语音模型通过全局线性几何结构连接英语词形变化，揭示可能支持人类语音识别的分布表征策略，挑战传统音系学-形态学分离的必要性。


<details>
  <summary>Details</summary>
Motivation: 探究自监督语音模型在噪声环境中实现单词识别的表征策略，及其对人类语音识别机制的启示。

Method: 分析S3M变体对英语名词/动词屈折变化的音系和形态表征，研究其线性几何结构特性。

Result: 模型通过分布关系（而非传统语言学单元）建立词形变化连接，部分源于但不完全等同于形态变化。

Conclusion: 挑战语音识别必须依赖独立音系-形态表征的假设，提出基于分布关系的替代策略可能更基础。

Abstract: Self-supervised speech models can be trained to efficiently recognize spoken
words in naturalistic, noisy environments. However, we do not understand the
types of linguistic representations these models use to accomplish this task.
To address this question, we study how S3M variants optimized for word
recognition represent phonological and morphological phenomena in frequent
English noun and verb inflections. We find that their representations exhibit a
global linear geometry which can be used to link English nouns and verbs to
their regular inflected forms.
  This geometric structure does not directly track phonological or
morphological units. Instead, it tracks the regular distributional
relationships linking many word pairs in the English lexicon -- often, but not
always, due to morphological inflection. These findings point to candidate
representational strategies that may support human spoken word recognition,
challenging the presumed necessity of distinct linguistic representations of
phonology and morphology.

</details>


### [25] [Same Content, Different Representations: A Controlled Study for Table QA](https://arxiv.org/abs/2509.22983)
*Yue Zhang,Seiji Maekawa,Nikita Bhutani*

Main category: cs.CL

TL;DR: 系统研究表格表示形式对问答性能的影响，提出混合方法在真实场景的适用性


<details>
  <summary>Details</summary>
Motivation: 现有表格问答基准未系统考察数据结构表示对模型的影响，真实场景需兼容结构化和半结构化数据

Method: 通过文本化流水线生成配对的结构化/半结构化表格，构建包含表格规模、连接复杂度等维度的诊断基准

Result: SQL方法在结构化数据精度高但半结构化下降，LLM灵活性好但精度低，混合方法在噪声模式下表现平衡

Conclusion: 表格表示形式是性能的核心变量，需根据数据特征选择模型，未来应发展适应多格式的混合方法

Abstract: Table Question Answering (Table QA) in real-world settings must operate over
both structured databases and semi-structured tables containing textual fields.
However, existing benchmarks are tied to fixed data formats and have not
systematically examined how representation itself affects model performance. We
present the first controlled study that isolates the role of table
representation by holding content constant while varying structure. Using a
verbalization pipeline, we generate paired structured and semi-structured
tables, enabling direct comparisons across modeling paradigms. To support
detailed analysis, we introduce a diagnostic benchmark with splits along table
size, join requirements, query complexity, and schema quality. Our experiments
reveal consistent trade-offs: SQL-based methods achieve high accuracy on
structured inputs but degrade on semi-structured data, LLMs exhibit flexibility
but reduced precision, and hybrid approaches strike a balance, particularly
under noisy schemas. These effects intensify with larger tables and more
complex queries. Ultimately, no single method excels across all conditions, and
we highlight the central role of representation in shaping Table QA
performance. Our findings provide actionable insights for model selection and
design, paving the way for more robust hybrid approaches suited for diverse
real-world data formats.

</details>


### [26] [ADAM: A Diverse Archive of Mankind for Evaluating and Enhancing LLMs in Biographical Reasoning](https://arxiv.org/abs/2509.22991)
*Jasin Cekinmez,Omid Ghahroodi,Saad Fowad Chandle,Dhiman Gupta,Ehsaneddin Asgari*

Main category: cs.CL

TL;DR: ADAM框架首次系统评估多模态大模型在人物传记推理中的能力，通过AdamDB数据集、AdamBench评估体系和AdamRAG检索增强系统提升模型准确性，尤其改善冷门人物处理效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对人物传记这一关键事实知识维度的系统评估，且模型存在对冷门人物信息幻觉问题，需建立跨文化/语言的认知评估框架。

Method: 构建覆盖400万+人物的多语言/模态数据集AdamDB，基于布鲁姆分类法设计6级认知评估AdamBench，提出传记专用检索增强系统AdamRAG。

Result: AdamRAG显著提升开源模型性能（闭源模型改进有限），人物知名度强烈影响准确率，图像输入改善效果弱于检索增强。

Conclusion: ADAM建立了首个认知与文化双基线的传记评估体系，推动开发抗幻觉、跨语言的多模态大模型，为冷门人物推理提供有效解决方案。

Abstract: We introduce ADAM (A Diverse Archive of Mankind), a framework for evaluating
and improving multimodal large language models (MLLMs) in biographical
reasoning. To the best of our knowledge, this is the first work to
systematically examine LLM capabilities in biography, a critical yet
underexplored dimension of factual knowledge. At its core, AdamDB is a
multilingual and multimodal dataset covering over 4 million individuals across
geography, time, and profession, while AdamBench provides cognitively
structured evaluations based on Bloom's taxonomy, spanning six reasoning levels
in both English and native languages. To address hallucinations, particularly
for lesser-known individuals, we propose AdamRAG, a retrieval-augmented
generation system tailored to biographical contexts. Experiments show that
AdamRAG substantially improves open-source models and modestly benefits
closed-source ones, with the largest gains on lower-order reasoning. Popularity
strongly mediates accuracy, and multimodal input via face images offers
smaller, less consistent improvements than retrieval. ADAM establishes the
first benchmark and framework for cognitively, culturally, and multimodally
grounded biographical evaluation, advancing the development of multilingual,
accurate, and hallucination-resistant MLLMs.

</details>


### [27] [AI Brown and AI Koditex: LLM-Generated Corpora Comparable to Traditional Corpora of English and Czech Texts](https://arxiv.org/abs/2509.22996)
*Jiří Milička,Anna Marklová,Václav Cvrček*

Main category: cs.CL

TL;DR: 构建英捷双语LLM生成语料库，用于对比人类与AI文本的语言学特征，涵盖多模型版本并提供开放访问


<details>
  <summary>Details</summary>
Motivation: 创建可比较人类文本与LLM生成文本的语言学资源，填补现有研究空白并确保多类型覆盖

Method: 复制BE21和Koditex语料库架构，使用GPT-3至GPT-4.5等多厂商模型生成文本，遵循Universal Dependencies标准进行标注

Result: 建成含英语27M/捷克语21.5M tokens的大规模语料库，提供CC BY 4.0开放许可及在线检索接口

Conclusion: 该资源为LLM文本分析提供基准，促进语言学比较研究，其开放共享特性将推动学术社区协作发展

Abstract: This article presents two corpora of English and Czech texts generated with
large language models (LLMs). The motivation is to create a resource for
comparing human-written texts with LLM-generated text linguistically. Emphasis
was placed on ensuring these resources are multi-genre and rich in terms of
topics, authors, and text types, while maintaining comparability with existing
human-created corpora. These generated corpora replicate reference human
corpora: BE21 by Paul Baker, which is a modern version of the original Brown
Corpus, and Koditex corpus that also follows the Brown Corpus tradition but in
Czech. The new corpora were generated using models from OpenAI, Anthropic,
Alphabet, Meta, and DeepSeek, ranging from GPT-3 (davinci-002) to GPT-4.5, and
are tagged according to the Universal Dependencies standard (i.e., they are
tokenized, lemmatized, and morphologically and syntactically annotated). The
subcorpus size varies according to the model used (the English part contains on
average 864k tokens per model, 27M tokens altogether, the Czech partcontains on
average 768k tokens per model, 21.5M tokens altogether). The corpora are freely
available for download under the CC BY 4.0 license (the annotated data are
under CC BY-NC-SA 4.0 licence) and are also accessible through the search
interface of the Czech National Corpus.

</details>


### [28] [Look Back to Reason Forward: Revisitable Memory for Long-Context LLM Agents](https://arxiv.org/abs/2509.23040)
*Yaorui Shi,Yuxin Chen,Siyuan Wang,Sihang Li,Hengxing Cai,Qi Gu,Xiang Wang,An Zhang*

Main category: cs.CL

TL;DR: 提出ReMemR1记忆增强框架和RLMLR强化学习训练方法，显著提升长文档QA性能


<details>
  <summary>Details</summary>
Motivation: 现有'边读边记'方法存在前向处理不可逆、信息覆盖丢失、强化学习信号稀疏三大缺陷

Method: ReMemR1引入回调机制实现全历史记忆检索，RLMLR通过多级奖励机制强化记忆利用

Result: 在长文档问答任务上取得显著性能提升，验证了框架有效性

Conclusion: 该方案通过非线性记忆检索和密集奖励信号，有效缓解信息退化问题，提升长上下文推理能力

Abstract: Large language models face challenges in long-context question answering,
where key evidence of a query may be dispersed across millions of tokens.
Existing works equip large language models with a memory corpus that is
dynamically updated during a single-pass document scan, also known as the
"memorize while reading" methods. While this approach scales efficiently, it
suffers from irreversible forward-only processing, information loss through
overwriting, and sparse reinforcement learning signals. To tackle these
challenges, we present ReMemR1, a memory-augmented agent with callback-enhanced
memory that allows selective retrieval from the entire memory history and
allows non-linear reasoning and revisiting of early evidence. To further
strengthen training, we propose Reinforcement Learning with Multi-Level Rewards
(RLMLR), which combines final-answer rewards with dense, step-level signals
that guide effective memory use. Together, these contributions mitigate
information degradation, improve supervision, and support multi-hop memory
utilizing. Experiments on long-document QA show significant gains over existing
memory-based approaches, which validates ReMemR1 as an effective solution for
long-context reasoning agents.

</details>


### [29] [Peacemaker or Troublemaker: How Sycophancy Shapes Multi-Agent Debate](https://arxiv.org/abs/2509.23055)
*Binwei Yao,Chao Shang,Wanyu Du,Jianfeng He,Ruixue Lian,Yi Zhang,Hang Su,Sandesh Swamy,Yanjun Qi*

Main category: cs.CL

TL;DR: 该论文研究发现大语言模型（LLMs）的谄媚性（过度迎合倾向）会损害多智能体辩论系统的有效性，导致辩论过早达成共识，并提出设计原则以平衡分歧与合作。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注用户与LLM间的谄媚性，但对多智能体辩论中智能体间谄媚性的影响缺乏理解。这种现象可能导致辩论系统失去通过分歧优化论证的核心价值。

Method: 提出首个操作框架：(1) 明确定义MADS环境中的谄媚性；(2) 开发评估指标量化智能体谄媚程度及其对信息交换的影响；(3) 系统研究不同角色（辩手/评委）的谄媚水平在分散式/集中式辩论中的影响。

Result: 谄媚性导致辩论正确结论达成前的分歧崩塌，使多智能体辩论准确率低于单智能体基线，并揭示辩手驱动与评委驱动两种失败模式。

Conclusion: 提出可操作的设计原则，指导MADS在智能体交互中有效平衡建设性分歧与合作，为优化多智能体辩论系统提供理论依据。

Abstract: Large language models (LLMs) often display sycophancy, a tendency toward
excessive agreeability. This behavior poses significant challenges for
multi-agent debating systems (MADS) that rely on productive disagreement to
refine arguments and foster innovative thinking. LLMs' inherent sycophancy can
collapse debates into premature consensus, potentially undermining the benefits
of multi-agent debate. While prior studies focus on user--LLM sycophancy, the
impact of inter-agent sycophancy in debate remains poorly understood. To
address this gap, we introduce the first operational framework that (1)
proposes a formal definition of sycophancy specific to MADS settings, (2)
develops new metrics to evaluate the agent sycophancy level and its impact on
information exchange in MADS, and (3) systematically investigates how varying
levels of sycophancy across agent roles (debaters and judges) affects outcomes
in both decentralized and centralized debate frameworks. Our findings reveal
that sycophancy is a core failure mode that amplifies disagreement collapse
before reaching a correct conclusion in multi-agent debates, yields lower
accuracy than single-agent baselines, and arises from distinct debater-driven
and judge-driven failure modes. Building on these findings, we propose
actionable design principles for MADS, effectively balancing productive
disagreement with cooperation in agent interactions.

</details>


### [30] [Semantic Voting: A Self-Evaluation-Free Approach for Efficient LLM Self-Improvement on Unverifiable Open-ended Tasks](https://arxiv.org/abs/2509.23067)
*Chunyang Jiang,Yonggang Zhang,Yiyang Cai,Chi-Min Chan,Yulong Liu,Mingming Chen,Wei Xue,Yike Guo*

Main category: cs.CL

TL;DR: 提出基于语义投票的无自我评估方法，提升大语言模型在不可验证任务中的轻量级自我改进效率


<details>
  <summary>Details</summary>
Motivation: 现有不可验证任务中自我评估方法存在计算开销高、存在内在偏见过度自信的问题，需要更轻量的替代方案

Method: 通过语义投票机制将硬匹配原则放松为软匹配，利用轻量级句子嵌入模型量化语义相似性

Result: 实验证明该方法在计算效率和任务性能上均优于传统自我评估方法

Conclusion: 语义投票机制有效解决了计算负担和内在偏差问题，适用于不同模型架构和任务场景

Abstract: The rising cost of acquiring supervised data has driven significant interest
in self-improvement for large language models (LLMs). Straightforward
unsupervised signals like majority voting have proven effective in generating
pseudo-labels for verifiable tasks, while their applicability to unverifiable
tasks (e.g., translation) is limited by the open-ended character of responses.
As a result, self-evaluation mechanisms (e.g., self-judging and entropy
minimization) are predominantly used to derive pseudo-labels. However,
self-evaluation relying on LLMs typically incurs high computational overhead
and introduces overconfidence issues due to intrinsic biases. To address these
challenges, we propose a novel self-evaluation-free approach for unverifiable
tasks, designed for lightweight yet effective self-improvement. Inspired by
majority voting commonly employed in verifiable tasks, we propose semantic
voting as a novel mechanism that relaxes the principle of hard matching (i.e.,
exact matching) toward soft matching (i.e., semantic similarity). Soft matching
is achieved by leveraging a lightweight sentence embedding model to quantify
semantic similarity, thereby mitigating excessive computational burden and
intrinsic bias-associated limitations of self-evaluation. Comprehensive
experiments demonstrate that our method achieves substantial gains in
computational efficiency and overall better performance than self-evaluation
methods across diverse model architectures and tasks.

</details>


### [31] [From Evidence to Trajectory: Abductive Reasoning Path Synthesis for Training Retrieval-Augmented Generation Agents](https://arxiv.org/abs/2509.23071)
*Muzhi Li,Jinhu Qi,Yihong Wu,Minghao Zhao,Liheng Ma,Yifan Li,Xinyu Wang,Yingxue Zhang,Ho-fung Leung,Irwin King*

Main category: cs.CL

TL;DR: 提出EviPath范式，通过证据锚定的推理路径合成方法显著提升RAG代理的复杂推理能力，实验显示8B模型在开放域问答任务中EM指标提升14.7%


<details>
  <summary>Details</summary>
Motivation: 现有RAG代理开发缺乏过程级监督机制，强化学习的稀疏奖励与LLM推理能力限制导致复杂任务处理不足，传统数据合成方法无法模拟真实环境交互

Method: 1. 溯因子任务规划：基于依赖关系分解问题并优化路径
2. 忠实子问题回答：利用证据构建代理环境生成推理
3. 对话式微调：将交互轨迹格式化为监督微调数据

Result: 在开放域问答基准测试中实现14.7%的EM指标绝对提升，显著优于现有最佳基线模型

Conclusion: EviPath通过结构化路径合成使LLM能够直接从数据中学习复杂推理与工具使用能力，为RAG代理开发提供有效范式

Abstract: Retrieval-augmented generation agents development is hindered by the lack of
process-level supervision to effectively guide agentic capabilities like task
decomposition, retriever invocation, and stepwise decision-making. While
reinforcement learning offers a potential solution, it suffers from sparse
rewards and the limited reasoning capabilities of large language models (LLMs).
Meanwhile, existing data synthesis methods only produce chain-of-thought
rationales and fail to model environmental interactions. In this paper, we
propose EviPath, an evidence-anchored reasoning path synthesis paradigm for RAG
agent development. EviPath comprises: (i) Abductive Subtask Planning, which
decomposes the problem into sub-questions and iteratively plans an optimal
solution path based on the dependencies between them; (ii) Faithful
Sub-question Answering, which uses supporting evidence to construct a proxy
environment to generate reasoning thoughts and answers for each sub-question;
and (iii) Conversational Fine-Tuning, which formats the complete
agent-environment interaction trajectory into a dialogue format suitable for
Supervised Fine-Tuning. EviPath allows LLMs to learn complex reasoning and
tool-use capabilities directly from synthesized data. Extensive experiments on
widely-used question-answering benchmarks show that an 8B parameter model
trained with EviPath-synthesized data significantly and consistently
outperforms state-of-the-art baselines with a double-digit absolute EM gain of
14.7% in open-domain question answering.

</details>


### [32] [The Geometry of Creative Variability: How Credal Sets Expose Calibration Gaps in Language Models](https://arxiv.org/abs/2509.23088)
*Esteban Garces Arias,Julian Rodemann,Christian Heumann*

Main category: cs.CL

TL;DR: 提出基于credal set的几何框架，量化LLM生成不确定性并实现人机创意对齐评估


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效量化创造性任务中LLM的多解不确定性，缺乏人类创意变化的校准基准

Method: 使用概率分布凸包分析，结合500个写作提示的10万故事生成实验，对比4模型5解码策略

Result: 最佳模型校准度0.434，解码策略贡献72%认知不确定性，模型规模与校准质量弱相关

Conclusion: 几何框架为改进生成系统提供方向，解码策略选择比模型架构对不确定性影响更显著

Abstract: Understanding uncertainty in large language models remains a fundamental
challenge, particularly in creative tasks where multiple valid outputs exist.
We present a geometric framework using credal sets - convex hulls of
probability distributions - to quantify and decompose uncertainty in neural
text generation, calibrated against human creative variation. Analyzing 500
creative writing prompts from the WritingPrompts dataset with 10 unique human
continuations each, we evaluate four language models across five decoding
strategies, generating 100,000 stories. Our credal set analysis reveals
substantial gaps in capturing human creative variation, with the best
model-human calibration reaching only 0.434 (Gemma-2B with temperature 0.7). We
decompose total uncertainty into epistemic and aleatoric components, finding
that the choice of decoding strategy contributes 39.4% to 72.0% of total
epistemic uncertainty. Model scale shows weak correlation with calibration
quality and no significant difference exists between base and instruction-tuned
models in calibration quality. Our geometric framework provides actionable
insights for improving generation systems for human-AI creative alignment. We
release our complete experimental framework.

</details>


### [33] [d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching](https://arxiv.org/abs/2509.23094)
*Yuchu Jiang,Yue Cai,Xiangzhong Luo,Jiale Fu,Jiarui Wang,Chonghan Liu,Xu Yang*

Main category: cs.CL

TL;DR: 提出无需训练的d²Cache框架，通过两阶段KV缓存策略显著提升扩散大语言模型的推理效率与生成质量


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型因依赖双向注意力机制，无法有效利用传统KV缓存技术，导致推理效率低下

Method: 基于双阶段细粒度选择策略的自适应KV缓存框架，动态更新关键token的KV状态并复用剩余token的缓存

Result: 在LLaDA和Dream模型上实现显著加速（最高3.4倍），生成质量提升（最高提升11.4% ROUGE-L）

Conclusion: d²Cache首次实现扩散模型的近似KV缓存，通过创新的缓存策略同时优化推理速度与生成效果，提供准左到右解码方案

Abstract: Diffusion-based large language models (dLLMs), despite their promising
performance, still suffer from inferior inference efficiency. This is because
dLLMs rely on bidirectional attention and cannot directly benefit from the
standard key-value (KV) cache as autoregressive models (ARMs) do. To tackle
this issue, we introduce \textit{Dual aDaptive Cache} (d$^2$Cache), which is a
training-free approximate KV cache framework for accelerating dLLM inference.
d$^2$Cache features a two-stage fine-grained selection strategy to identify
tokens and adaptively update their KV states at each decoding step, while
caching the KV states of the remaining tokens for reuse. Furthermore,
d$^2$Cache naturally offers a more reliable decoding alternative, which can
enable quasi left-to-right generation and mitigate premature overconfidence in
tokens at the end of the sequence. Extensive experimental results on two
representative dLLMs (\ie, LLaDA and Dream) demonstrate that d$^2$Cache not
only achieves substantial inference speedups, but also yields consistent
improvements in generation quality. The code is available at
https://github.com/Kamichanw/d2Cache.

</details>


### [34] [How to Make Large Language Models Generate 100% Valid Molecules?](https://arxiv.org/abs/2509.23099)
*Wen Tao,Jing Tang,Alvin Chan,Bryan Hooi,Baolong Bi,Nanyun Peng,Yuansheng Liu,Yiwei Wang*

Main category: cs.CL

TL;DR: 论文提出SmiSelf框架，通过SMILES转SELFIES实现100%有效分子生成，兼容现有模型并提升指标


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在少样本场景下使用SMILES生成有效分子困难的问题，探索SELFIES有效性并发现其局限性

Method: 1.评估LLMs使用SELFIES的效果 2.测试LLMs修正SMILES能力 3.开发SmiSelf框架实现跨化学语言转换与语法修正

Result: SmiSelf确保100%分子有效性，保持分子特性，其他指标持平或提升，兼容所有SMILES生成模型

Conclusion: SmiSelf扩展了LLMs在生物医学的实际应用，提供可靠的分子生成解决方案，框架已开源

Abstract: Molecule generation is key to drug discovery and materials science, enabling
the design of novel compounds with specific properties. Large language models
(LLMs) can learn to perform a wide range of tasks from just a few examples.
However, generating valid molecules using representations like SMILES is
challenging for LLMs in few-shot settings. In this work, we explore how LLMs
can generate 100% valid molecules. We evaluate whether LLMs can use SELFIES, a
representation where every string corresponds to a valid molecule, for valid
molecule generation but find that LLMs perform worse with SELFIES than with
SMILES. We then examine LLMs' ability to correct invalid SMILES and find their
capacity limited. Finally, we introduce SmiSelf, a cross-chemical language
framework for invalid SMILES correction. SmiSelf converts invalid SMILES to
SELFIES using grammatical rules, leveraging SELFIES' mechanisms to correct the
invalid SMILES. Experiments show that SmiSelf ensures 100% validity while
preserving molecular characteristics and maintaining or even enhancing
performance on other metrics. SmiSelf helps expand LLMs' practical applications
in biomedicine and is compatible with all SMILES-based generative models. Code
is available at https://github.com/wentao228/SmiSelf.

</details>


### [35] [Non-Collaborative User Simulators for Tool Agents](https://arxiv.org/abs/2509.23124)
*Jeonghoon Shim,Woojung Song,Cheyon Jin,Seungwon KooK,Yohan Jo*

Main category: cs.CL

TL;DR: 提出模拟非协作用户行为的工具代理训练框架，解决现有用户模拟器过度友好的问题


<details>
  <summary>Details</summary>
Motivation: 现有工具代理的用户模拟器仅模拟合作行为，无法训练代理应对真实场景中非协作用户（如请求无效服务、跑题、不耐烦、提供不完整信息等行为）的挑战

Method: 设计新型用户模拟器架构，模拟四类非协作行为：请求不可用服务、偏离主题对话、表现不耐烦、提供不完整语句

Result: 在MultiWOZ和τ-bench测试中，现有工具代理性能显著下降（出现幻觉增加38%、对话崩溃率提升25%），暴露代理抗干扰能力缺陷

Conclusion: 建立可扩展的用户模拟框架，帮助开发者预先诊断工具代理在真实复杂场景中的脆弱性，推动鲁棒性工具代理研发

Abstract: Non-Collaborative User Simulators for Tool Agents Download PDF Jeonghoon
Shim, Woojung Song, Cheyon Jin, Seungwon KooK, Yohan Jo 19 Sept 2025 (modified:
25 Sept 2025)ICLR 2026 Conference SubmissionConference, AuthorsRevisionsCC BY
4.0 Keywords: Tool Agent, User Simulator, Non-collaborative User, Dialogue
Simulation TL;DR: A non-collaborative user simulation method for tool agent.
Abstract: Tool agents interact with users through multi-turn dialogues to
accomplish various tasks. Recent studies have adopted user simulation methods
to develop these agents in multi-turn settings. However, existing user
simulators tend to be agent-friendly, exhibiting only cooperative behaviors,
which fails to train and test agents against non-collaborative users in the
real world. To address this, we propose a novel user simulator architecture
that simulates four categories of non-collaborative behaviors: requesting
unavailable services, digressing into tangential conversations, expressing
impatience, and providing incomplete utterances. Our user simulator can
simulate challenging and natural non-collaborative behaviors while reliably
delivering all intents and information necessary to accomplish the task. Our
experiments on MultiWOZ and $\tau$-bench reveal significant performance
degradation in state-of-the-art tool agents when encountering non-collaborative
users. We provide detailed analyses of agents' weaknesses under each
non-collaborative condition, such as escalated hallucinations and dialogue
breakdowns. Ultimately, we contribute an easily extensible user simulation
framework to help the research community develop tool agents and preemptively
diagnose them under challenging real-world conditions within their own
services.

</details>


### [36] [Tagging the Thought: Unlocking Personalization Reasoning via Reinforcement Learning](https://arxiv.org/abs/2509.23140)
*Song Jin,Juntian Zhang,Yong Liu,Xun Zhang,Yufei Zhang,Fei Jiang,Guojun Yin,Wei Lin,Rui Yan*

Main category: cs.CL

TL;DR: 提出TagPR框架，通过标记思维方法和复合奖励机制增强大语言模型的个性化推理能力，实现32.65%平均性能提升


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在分析用户历史、推断个性化偏好等定制化推理任务中存在显著不足

Method: 数据驱动的语义标签推理链生成+监督微调与多阶段强化学习组合训练，采用包含PRMU模型的复合奖励信号

Result: 在LaMP基准和自建数据集上达到SOTA，平均提升32.65%

Conclusion: 结构化可解释推理是提升大语言模型个性化能力的有效路径

Abstract: Recent advancements have endowed Large Language Models (LLMs) with impressive
general reasoning capabilities, yet they often struggle with personalization
reasoning - the crucial ability to analyze user history, infer unique
preferences, and generate tailored responses. To address this limitation, we
introduce TagPR, a novel training framework that significantly enhances an
LLM's intrinsic capacity for personalization reasoning through a tagging the
thought approach. Our method first develops a data-driven pipeline to
automatically generate and semantically label reasoning chains, creating a
structured dataset that fosters interpretable reasoning. We then propose a
synergistic training strategy that begins with Supervised Fine-Tuning (SFT) on
this tagged data to establish foundational reasoning patterns, followed by a
multi-stage reinforcement learning (RL) process. This RL phase is guided by a
unique composite reward signal, which integrates tag-based constraints and a
novel Personalization Reward Model with User Embeddings (PRMU) to achieve
fine-grained alignment with user-specific logic. Extensive experiments on the
public LaMP benchmark and a self-constructed dataset demonstrate that our
approach achieves state-of-the-art results, delivering an average improvement
of 32.65% over the base model across all tasks. Our work validates that
structured, interpretable reasoning is a highly effective pathway to unlocking
genuine personalization capabilities in LLMs.

</details>


### [37] [Tree Reward-Aligned Search for TReASURe in Masked Diffusion Language Models](https://arxiv.org/abs/2509.23146)
*Zichao Yu,Ming Li,Wenyi Zhang,Weiguo Gao*

Main category: cs.CL

TL;DR: TReASURe提出基于首曝去掩码的分支策略和确定性替代评分的剪枝规则，有效提升语言模型树搜索效率与稳定性，在低计算量场景下实现SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有树搜索方法应用于掩码扩散语言模型时存在分支相关性高、奖励估计方差大的缺陷，导致探索效率低且剪枝不稳定。

Method: UnmaskBranch策略通过单次模型调用实现令牌内容和揭示顺序的多样化；ResubstituteScore采用确定性替代补全降低奖励估计方差。

Result: 实验显示在相同算力下，TReASURe在困惑度、语言合规性及情感/毒性控制等指标全面领先，低计算场景优势显著。

Conclusion: 理论证明与实验结果共同验证了该方法在分支效率、评分准确性及扩展性上的改进，为语言模型对齐提供了更高效的树搜索框架。

Abstract: Tree search has recently emerged as a powerful framework for aligning
generative models with task-specific rewards at test time. Applying tree search
to Masked Diffusion Language Models, however, introduces two key challenges:
(i) parallel unmasking yields highly correlated branches, limiting exploration,
and (ii) reward evaluation via sampled completions produces high-variance
estimates, making pruning unstable. We propose TReASURe, a tree-search
test-time alignment method that addresses these issues. It introduces (i)
UnmaskBranch, a branching strategy based on first-hitting unmasking that
diversifies both token content and reveal order with a single model call per
parent node, and (ii) ResubstituteScore, a pruning rule that uses deterministic
resubstitution to score partially masked sequences with low-variance proxy
completions. Theoretically, we quantify branching efficiency gains in NFEs
(number of function evaluations), show that the scoring rule approximates the
true reward with error bounded by predictive uncertainty, and prove
improvements with larger tree widths. Empirically, TReASURe achieves
state-of-the-art results on perplexity, linguistic acceptability, and control
of sentiment and toxicity, outperforming prior methods under matched compute
budgets, with especially strong gains in low-NFE regimes.

</details>


### [38] [Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with LLMs](https://arxiv.org/abs/2509.23166)
*Chenxing Wei,Hong Wang,Ying He,Fei Yu,Yao Shu*

Main category: cs.CL

TL;DR: 提出T2PAM范式和ROSA算法，通过在线策略适配实现大模型在多轮对话中的高效自校正


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多轮交互中性能下降，因其训练数据缺乏动态交互场景，难以适应实时用户反馈

Method: T2PAM范式利用用户反馈构建奖励信号，ROSA算法通过单步参数更新逼近理论最优策略，避免梯度迭代优化

Result: 在基准测试中显著提升任务效果与效率，理论证明策略会随交互次数增加收敛至用户偏好

Conclusion: ROSA通过轻量级在线适配机制，实现理论保障的实时策略优化，为LLM持续改进提供新路径

Abstract: Large Language Models (LLMs) employ multi-turn interaction as a fundamental
paradigm for completing complex tasks. However, their performance often
degrades in extended interactions, as they are typically trained on static,
single-turn data, which hinders their ability to adapt to real-time user
feedback. To address this limitation, we first propose a new paradigm:
Test-Time Policy Adaptation for Multi-Turn Interactions (T2PAM), which utilizes
user feedback from the ongoing interaction as a reward signal to estimate a
latent optimal policy aligned with user preferences, then updates a small
subset of parameters to steer the model toward this policy, ultimately enabling
efficient in-conversation self-correction. We then introduce Optimum-Referenced
One-Step Adaptation (ROSA), a lightweight algorithm that operationalizes T2PAM.
ROSA guides the model parameters toward a theoretical optimal policy in a
single, efficient update step, avoiding costly iterative gradient-based
optimization and minimizing computational overhead. We provide a rigorous
theoretical analysis guaranteeing that the policy of ROSA converges to the
preference of user as the number of interactions increases. Extensive
experiments on challenging benchmark demonstrate that ROSA achieves significant
improvements in both task effectiveness and efficiency.

</details>


### [39] [Pretraining LLM with Latent Thoughts in Continuous Space](https://arxiv.org/abs/2509.23184)
*Boyi Zeng,He Li,Shixiang Song,Yixuan Wang,Ziwei He,Xinbing Wang,Zhouhan Lin*

Main category: cs.CL

TL;DR: 提出通过预训练阶段的潜在思考机制，在相同推理成本下1.4B参数模型性能超越标准2.8B模型


<details>
  <summary>Details</summary>
Motivation: 受测试阶段Chain-of-Thought通过增加推理步骤提升性能的启发，探索在预训练阶段通过增加计算步骤优化每个token生成质量

Method: 预训练时先生成中间潜在思考(当前位置的隐藏状态)，再基于该隐藏状态预测下一个token，可形成类似CoT的链式潜在思考结构

Result: 1.4B模型在300B tokens预训练后，语言建模和下游任务表现超越标准2.8B模型，且增加潜在思考数量持续提升性能

Conclusion: 潜在思考机制在不增加推理成本的前提下显著提升模型效率，为模型设计开辟新方向

Abstract: The remarkable success of Chain-of-Thought (CoT), which enhances performance
by scaling generation steps at test-time, inspires us to ask: can we leverage a
similar scaling of computational steps during pretraining to improve the
generation of each individual token? To address this, we propose a novel
pre-training methodology: Pretraining Language Models with Latent Thoughts. Our
approach pretrains a language model (LM) to first generate an intermediate
latent thought-the last hidden state of the current position-which is then used
as input to predict the actual subsequent token. This additional computational
step enables the LM to refine its prediction within unconstrained continuous
space. Our experiments demonstrate that, at an identical inference cost, a LM
that generates one additional latent thought per token outperforms a standard
model with double the parameters. For instance, ours-1.4B (Pythia Arch),
pretrained on 300B tokens from the Pile, significantly surpasses the vanilla
Pythia-2.8B trained on the same data on both language modeling and a range of
general downstream tasks. Furthermore, increasing the number of latent thoughts
generated before each actual token-forming a chain analogous to
CoT-consistently improves the model's performance.

</details>


### [40] [Diagnose, Localize, Align: A Full-Stack Framework for Reliable LLM Multi-Agent Systems under Instruction Conflicts](https://arxiv.org/abs/2509.23188)
*Guancheng Wan,Leixin Sun,Longxu Dou,Zitong Shi,Fang Wu,Eric Hanchen Jiang,Wenke Huang,Guibin Zhang,Hejia Geng,Xiangru Tang,Zhenfei Yin,Yizhou Sun,Wei Wang*

Main category: cs.CL

TL;DR: 提出三阶段框架CRAS+SAIL解决LLM多智能体系统在指令冲突中的合规性问题，通过局部注意力层优化实现5.6%的改进。


<details>
  <summary>Details</summary>
Motivation: 现有LLM多智能体系统存在指令冲突下的层次合规性失效，宏观指标难以定位微观违规行为并提出改进方案。

Method: 三阶段框架：1) CRAS四维度诊断指标；2) 注意力漂移分析定位中间层注意力头；3) SAIL方法在焦点层实施LoRA微调与加权偏好优化。

Result: 在AutoGen等框架下，SAIL使MedQA任务合规性提升5.6%，且无需全模型微调。

Conclusion: 通过注意力机制分析和局部层对齐优化，能有效提升多智能体系统的指令层次合规性，为可靠部署提供新路径。

Abstract: Large Language Model (LLM)-powered multi-agent systems (MAS) have rapidly
advanced collaborative reasoning, tool use, and role-specialized coordination
in complex tasks. However, reliability-critical deployment remains hindered by
a systemic failure mode: hierarchical compliance under instruction conflicts
(system-user, peer-peer), where agents misprioritize system-level rules in the
presence of competing demands. Moreover, widely used macro-level metrics (e.g.,
pass@k) obscure these micro-level violations and offer little actionable
guidance for remedy. In this work, we present a full-stack, three-stage
framework: (1) Diagnose - Contextualized Role Adherence Score (CRAS), a
query-wise, context-aware scoring metric that decomposes role adherence into
four measurable dimensions; (2) Localize - attention drift analysis revealing
that instruction conflicts are resolved by attention heads that are largely
concentrated in middle layers; (3) Align - Surgical Alignment of Instruction
Layers (SAIL), which installs LoRA only on the localized focal layers and
optimizes a token-weighted DPO-style preference objective that credits tokens
by their focal attentional contribution. Across standard benchmarks and MAS
frameworks, our surgical approach improves instruction hierarchy compliance
(e.g., +5.60% with AutoGen on MedQA) without full-model finetuning.

</details>


### [41] [Estimating the strength and timing of syntactic structure building in naturalistic reading](https://arxiv.org/abs/2509.23195)
*Nan Wang,Jiaxuan Li*

Main category: cs.CL

TL;DR: 通过眼动追踪和脑电数据分析发现，自然阅读中短语结构构建先于句法类别检测，结构深度主导阅读偏差，支持预测性的'树支架'理解模型


<details>
  <summary>Details</summary>
Motivation: 解决现有句法时间模型将句法类别检测和短语结构构建混为一谈的问题，验证短语结构构建是否先于类别检测

Method: 使用ZuCo语料库的EEG和眼动数据联合分析，包含注视转移模式分析、贝叶斯网络建模和fixation-related potentials时间特征分析

Result: 1. 注视转移显示短语结构主导扫描路径
2. 结构深度是阅读偏差最强预测因子（贝叶斯因子>30）
3. 句法惊异在单词出现前(-184ms)即影响神经活动

Conclusion: 短语结构构建可先于句法类别检测，且结构预测主导词汇因素，支持预测性的树支架理解机制，挑战传统序列加工模型

Abstract: A central question in psycholinguistics is the timing of syntax in sentence
processing. Much of the existing evidence comes from violation paradigms, which
conflate two separable processes - syntactic category detection and phrase
structure construction - and implicitly assume that phrase structure follows
category detection. In this study, we use co-registered EEG and eye-tracking
data from the ZuCo corpus to disentangle these processes and test their
temporal order under naturalistic reading conditions. Analyses of gaze
transitions showed that readers preferentially moved between syntactic heads,
suggesting that phrase structures, rather than serial word order, organize
scanpaths. Bayesian network modeling further revealed that structural depth was
the strongest driver of deviations from linear reading, outweighing lexical
familiarity and surprisal. Finally, fixation-related potentials demonstrated
that syntactic surprisal influences neural activity before word onset (-184 to
-10 ms) and during early integration (48 to 300 ms). These findings extend
current models of syntactic timing by showing that phrase structure
construction can precede category detection and dominate lexical influences,
supporting a predictive "tree-scaffolding" account of comprehension.

</details>


### [42] [From Harm to Help: Turning Reasoning In-Context Demos into Assets for Reasoning LMs](https://arxiv.org/abs/2509.23196)
*Haonan Wang,Weida Liang,Zihang Fu,Nie Zheng,Yifan Zhang,Yao Tong,Tongyao Zhu,Hao Jiang,Chuang Li,Jiaying Wu,Kenji Kawaguchi*

Main category: cs.CL

TL;DR: 研究发现基于验证器的推理大语言模型在少样本CoT场景表现反而不佳，提出I2S框架通过显式提炼演示知识并生成针对性推理轨迹，显著提升多类模型性能


<details>
  <summary>Details</summary>
Motivation: 解决现有推理大语言模型在少样本思维链场景下出现的「示例越多效果越差」反常现象，探究其背后机制并提出解决方案

Method: 提出两阶段框架I2S：1）将演示转化为可复用见解 2）生成目标问题专属推理轨迹。进阶版I2S+增加自我修正环节确保逻辑连贯性

Result: 在AIME'25任务中GPT-4.1提升14%，o1-mini在AIME和GPQA分别提升2.7%和1.7%，多基准测试超越直接回答及其他测试时扩展基线方法

Conclusion: 通过insight-refine-solve框架有效利用上下文示例，证明显式提炼策略知识比简单增加示例数量更关键，为提升大模型推理能力提供新范式

Abstract: Recent reasoning LLMs (RLMs), especially those trained with verifier-based
reinforcement learning, often perform worse with few-shot CoT than with direct
answering. We revisit this paradox using high-quality reasoning traces from
DeepSeek-R1 as demonstrations and find that adding more exemplars consistently
degrades accuracy, even when demonstrations are optimal. A detailed analysis
reveals two mechanisms behind this decline: (i) semantic misguidance, where
high textual similarity leads the model to treat the target as the same as the
exemplar and to copy intermediate steps verbatim; and (ii) strategy transfer
failure, where the model struggles to extract useful reasoning strategies and
apply them to target questions. Guided by these, we introduce Insight-to-Solve
(I2S), a sequential test-time procedure that turns demonstrations into
explicit, reusable insights and derives a target-specific reasoning trace;
optionally, the reasoning is self-refined for coherence and correctness (I2S+).
Extensive experiments on diverse benchmarks show that I2S and I2S+ consistently
outperform both direct answering and test-time scaling baselines across open-
and closed-source models. Even for GPT models, our method helps: on AIME'25,
GPT-4.1 rises by +14.0%, and o1-mini improves by +2.7% on AIME and +1.7% on
GPQA, indicating that in-context demonstrations can be harnessed effectively
via insight-refine-solve framework.

</details>


### [43] [Global Beats, Local Tongue: Studying Code Switching in K-pop Hits on Billboard Charts](https://arxiv.org/abs/2509.23197)
*Aditya Narayan Sankaran,Reza Farahbakhsh,Noel Crespi*

Main category: cs.CL

TL;DR: 全球榜单K-pop歌曲呈现英语主导特征，性别差异不显著但女性艺人英语使用更稳定，歌词语言选择受全球市场压力影响


<details>
  <summary>Details</summary>
Motivation: 探究K-pop全球化成功背后的语言策略，特别是代码转换现象与英语使用对商业成功的影响机制

Method: 收集2017-2025年Billboard两大榜单的14个组合和8位个人歌手数据，量化分析英韩歌词比例、代码转换频率及风格特征，进行统计检验和性别分类预测

Result: ①全球榜单歌曲英语占比超70% ②性别差异P>0.05但女性solo英语更稳定 ③XLM-roberta模型分类F1达0.76 ④Hot100榜单对英语要求更高

Conclusion: K-pop歌词语言选择体现全球化市场策略，代码转换模式既反映艺人身份特征，也适应不同榜单的文化语境需求

Abstract: Code switching, particularly between Korean and English, has become a
defining feature of modern K-pop, reflecting both aesthetic choices and global
market strategies. This paper is a primary investigation into the linguistic
strategies employed in K-pop songs that achieve global chart success, with a
focus on the role of code-switching and English lyric usage. A dataset of K-pop
songs that appeared on the Billboard Hot 100 and Global 200 charts from 2017 to
2025, spanning 14 groups and 8 solo artists, was compiled. Using this dataset,
the proportion of English and Korean lyrics, the frequency of code-switching,
and other stylistic features were analysed. It was found that English dominates
the linguistic landscape of globally charting K-pop songs, with both male and
female performers exhibiting high degrees of code-switching and English usage.
Statistical tests indicated no significant gender-based differences, although
female solo artists tend to favour English more consistently. A classification
task was also performed to predict performer gender from lyrics, achieving
macro F1 scores up to 0.76 using multilingual embeddings and handcrafted
features. Finally, differences between songs charting on the Hot 100 versus the
Global 200 were examined, suggesting that, while there is no significant gender
difference in English, higher English usage may be more critical for success in
the US-focused Hot 100. The findings highlight how linguistic choices in K-pop
lyrics are shaped by global market pressures and reveal stylistic patterns that
reflect performer identity and chart context.

</details>


### [44] [Steering Prepositional Phrases in Language Models: A Case of with-headed Adjectival and Adverbial Complements in Gemma-2](https://arxiv.org/abs/2509.23204)
*Stefan Arnold,René Gröbner*

Main category: cs.CL

TL;DR: 研究通过干预注意力头成功调控Gemma-2模型生成介词短语补语的功能倾向性


<details>
  <summary>Details</summary>
Motivation: 揭示语言模型处理介词短语补语功能歧义的内部机制，探索生成过程的主动控制方法

Method: 构建双解语境提示集+激活投射定位关键注意力头+值向量缩放干预

Result: 注意力头干预使工具性补语从75%降至33%，属性性补语从25%升至36%

Conclusion: 特定注意力头主导介词短语功能分配，通过向量调控可实现生成倾向的定量调整

Abstract: Language Models, when generating prepositional phrases, must often decide for
whether their complements functions as an instrumental adjunct (describing the
verb adverbially) or an attributive modifier (enriching the noun adjectivally),
yet the internal mechanisms that resolve this split decision remain poorly
understood. In this study, we conduct a targeted investigation into Gemma-2 to
uncover and control the generation of prepositional complements. We assemble a
prompt suite containing with-headed prepositional phrases whose contexts
equally accommodate either an instrumental or attributive continuation,
revealing a strong preference for an instrumental reading at a ratio of 3:4. To
pinpoint individual attention heads that favor instrumental over attributive
complements, we project activations into the vocabulary space. By scaling the
value vector of a single attention head, we can shift the distribution of
functional roles of complements, attenuating instruments to 33% while elevating
attributes to 36%.

</details>


### [45] [PARL-MT: Learning to Call Functions in Multi-Turn Conversation with Progress Awareness](https://arxiv.org/abs/2509.23206)
*Huacan Chai,Zijie Cao,Maolin Ran,Yingxuan Yang,Jianghao Lin,pengxin,Hairui Wang,Renjie Ding,Ziyu Wan,Muning Wen,Weiwen Liu,Weinan Zhang,Fei Huang,Ying Wen*

Main category: cs.CL

TL;DR: 提出PARL-MT框架，通过进度感知机制增强LLM多轮函数调用的连贯性与效率


<details>
  <summary>Details</summary>
Motivation: 现有方法在解决多轮对话任务时存在进度意识缺失问题：单回合训练忽略全局规划，端到端强化学习产生冗余且缺乏进度整合

Method: 1. 进度感知生成管道(PAG)自动构建含对话摘要与任务规划的数据集；2. 进度感知指导的强化学习算法(PAG-RL)减少上下文冗余，增强局部动作与全局任务的协同

Result: 在两个公开基准测试中显著超越现有方法，验证进度感知机制的有效性

Conclusion: 进度感知是提升多轮函数调用的关键机制，PARL-MT通过显式建模全局进度，实现了更鲁棒高效的长程任务执行

Abstract: Large language models (LLMs) have achieved impressive success in single-turn
function calling, yet real-world applications such as travel planning or
multi-stage data analysis typically unfold across multi-turn conversations. In
these settings, LLMs must not only issue accurate function calls at each step
but also maintain progress awareness, the ability to summarize past
interactions and plan future actions to ensure coherent, long-horizon task
execution. Existing approaches, however, either reduce multi-turn training to
isolated single-turn samples, which neglects task-level planning, or employ
end-to-end reinforcement learning (RL) that struggles with redundancy and lacks
explicit integration of progress awareness. To overcome these limitations, we
introduce PARL-MT, a framework that explicitly incorporates progress awareness
into LLM training for multi-turn function calling. PARL-MT combines (i) a
Progress Awareness Generation (PAG) pipeline, which automatically constructs
datasets coupling conversation summaries with future task planning, and (ii) a
Progress Awareness-Guided Reinforcement Learning (PAG-RL) algorithm, which
integrates progress awareness into RL training to reduce contextual redundancy
and improve alignment between local actions and global task completion.
Empirical results on two public benchmarks demonstrate that PARL-MT
significantly outperforms existing methods, highlighting the effectiveness of
progress awareness in enabling robust and efficient multi-turn function
calling.

</details>


### [46] [A Structured Framework for Evaluating and Enhancing Interpretive Capabilities of Multimodal LLMs in Culturally Situated Tasks](https://arxiv.org/abs/2509.23208)
*Haorui Yu,Ramon Ruiz-Dolz,Qiufeng Yi*

Main category: cs.CL

TL;DR: 研究构建量化框架评估主流视觉语言模型在中国传统绘画评论生成中的多维表现


<details>
  <summary>Details</summary>
Motivation: 探索当前视觉语言模型在艺术评论领域（尤其传统中国绘画）的复杂语义理解和内容生成潜力

Method: 1. 通过零样本分类构建含评估立场/特征聚焦/评论质量的量化框架
2. 定义量化评论家角色
3. 使用角色引导提示评估Llama/Qwen/Gemini等模型

Result: 揭示了VLMs在艺术评论中的性能边界：具备多角度生成潜力但需提升专业语义理解

Conclusion: 该框架为艺术领域AI评估提供新范式，代码开源推动相关研究，指明模型在文化特定场景的优化方向

Abstract: This study aims to test and evaluate the capabilities and characteristics of
current mainstream Visual Language Models (VLMs) in generating critiques for
traditional Chinese painting. To achieve this, we first developed a
quantitative framework for Chinese painting critique. This framework was
constructed by extracting multi-dimensional evaluative features covering
evaluative stance, feature focus, and commentary quality from human expert
critiques using a zero-shot classification model. Based on these features,
several representative critic personas were defined and quantified. This
framework was then employed to evaluate selected VLMs such as Llama, Qwen, or
Gemini. The experimental design involved persona-guided prompting to assess the
VLM's ability to generate critiques from diverse perspectives. Our findings
reveal the current performance levels, strengths, and areas for improvement of
VLMs in the domain of art critique, offering insights into their potential and
limitations in complex semantic understanding and content generation tasks. The
code used for our experiments can be publicly accessed at:
https://github.com/yha9806/VULCA-EMNLP2025.

</details>


### [47] [Detecting Corpus-Level Knowledge Inconsistencies in Wikipedia with Large Language Models](https://arxiv.org/abs/2509.23233)
*Sina J. Semnani,Jirayu Burapacheep,Arpandeep Khatua,Thanawan Atchariyachanvanit,Zheng Wang,Monica S. Lam*

Main category: cs.CL

TL;DR: 提出CLAIRE系统检测维基百科事实不一致问题，构建WIKICOLLIDE基准验证3.3%英文维基内容存在矛盾，证明LLM辅助工具能有效提升编辑效率。


<details>
  <summary>Details</summary>
Motivation: 维基百科作为最大开放知识库被广泛使用，但其事实一致性缺乏系统评估。现有矛盾检测依赖人工审核，亟需自动化工具提升效率。

Method: 构建CLAIRE智能代理系统，融合LLM推理与检索技术，自动识别矛盾声明并提供上下文证据供人工审核。通过随机采样和人工标注创建WIKICOLLIDE基准。

Result: 发现3.3%英文维基事实自相矛盾，7.3%的FEVEROUS和4%的AmbigQA数据受污染。编辑使用CLAIRE后效率提升64.7%，最佳自动化系统AUROC仅75.1%。

Conclusion: 矛盾是维基百科可测量的系统性缺陷，CLAIRE类LLM系统能有效辅助实现大规模知识一致性维护，为质量提升提供实用工具。

Abstract: Wikipedia is the largest open knowledge corpus, widely used worldwide and
serving as a key resource for training large language models (LLMs) and
retrieval-augmented generation (RAG) systems. Ensuring its accuracy is
therefore critical. But how accurate is Wikipedia, and how can we improve it?
  We focus on inconsistencies, a specific type of factual inaccuracy, and
introduce the task of corpus-level inconsistency detection. We present CLAIRE,
an agentic system that combines LLM reasoning with retrieval to surface
potentially inconsistent claims along with contextual evidence for human
review. In a user study with experienced Wikipedia editors, 87.5% reported
higher confidence when using CLAIRE, and participants identified 64.7% more
inconsistencies in the same amount of time.
  Combining CLAIRE with human annotation, we contribute WIKICOLLIDE, the first
benchmark of real Wikipedia inconsistencies. Using random sampling with
CLAIRE-assisted analysis, we find that at least 3.3% of English Wikipedia facts
contradict another fact, with inconsistencies propagating into 7.3% of FEVEROUS
and 4.0% of AmbigQA examples. Benchmarking strong baselines on this dataset
reveals substantial headroom: the best fully automated system achieves an AUROC
of only 75.1%.
  Our results show that contradictions are a measurable component of Wikipedia
and that LLM-based systems like CLAIRE can provide a practical tool to help
editors improve knowledge consistency at scale.

</details>


### [48] [Fin-ExBERT: User Intent based Text Extraction in Financial Context using Graph-Augmented BERT and trainable Plugin](https://arxiv.org/abs/2509.23259)
*Soumick Sarker,Abhijit Kumar Rai*

Main category: cs.CL

TL;DR: 提出Fin-ExBERT框架：基于领域自适应BERT+LoRA适配器，通过两阶段渐进解冻训练与动态阈值策略，实现金融对话句子级意图抽取的高效部署方案。


<details>
  <summary>Details</summary>
Motivation: 金融对话记录存在非结构化格式、领域专用术语及意图密度不均等特性，传统信息抽取方法面临精度与适应性挑战。

Method: 1. 使用领域自适应BERT主干网络+LoRA低秩适配器
2. 两阶段训练：先冻结主干训练分类头，后渐进解冻全网络差分学习率微调
3. 基于概率分布曲率的动态阈值选择（肘点检测）替代固定截断

Result: 实证显示在真实对话数据上达到高精确度与F1值，输出结果可解释性强，支持下游审核与QA流程。框架支持批量处理、可视化及校准输出。

Conclusion: 该框架通过高效微调与鲁棒阈值机制，为金融对话挖掘提供可部署解决方案，兼具性能优势与工程实用性。

Abstract: Financial dialogue transcripts pose a unique challenge for sentence-level
information extraction due to their informal structure, domain-specific
vocabulary, and variable intent density. We introduce Fin-ExBERT, a lightweight
and modular framework for extracting user intent-relevant sentences from
annotated financial service calls. Our approach builds on a domain-adapted BERT
(Bidirectional Encoder Representations from Transformers) backbone enhanced
with LoRA (Low-Rank Adaptation) adapters, enabling efficient fine-tuning using
limited labeled data. We propose a two-stage training strategy with progressive
unfreezing: initially training a classifier head while freezing the backbone,
followed by gradual fine-tuning of the entire model with differential learning
rates. To ensure robust extraction under uncertainty, we adopt a dynamic
thresholding strategy based on probability curvature (elbow detection),
avoiding fixed cutoff heuristics. Empirical results show strong precision and
F1 performance on real-world transcripts, with interpretable output suitable
for downstream auditing and question-answering workflows. The full framework
supports batched evaluation, visualization, and calibrated export, offering a
deployable solution for financial dialogue mining.

</details>


### [49] [A2D: Any-Order, Any-Step Safety Alignment for Diffusion Language Models](https://arxiv.org/abs/2509.23286)
*Wonje Jeung,Sangyeon Yoon,Yoonjun Cho,Dongjae Jeon,Sangwoo Shin,Hyesoo Hong,Albert No*

Main category: cs.CL

TL;DR: 提出基于token级对齐的A2D防御框架，通过强制模型在有害内容出现时生成[EOS]终止符号，有效抵御任意生成顺序/步骤的提示攻击，在安全基准上将攻击成功率从80%+降至近零值，并实现19.3倍的安全终止加速。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型(dLLMs)的任意顺序生成特性扩大了攻击面：有害内容可能出现在任意位置，且模板预填充攻击(如DIJA)可绕过响应级拒绝机制。需开发token级的安全防御策略。

Method: 1. 通过随机掩码训练实现token级对齐，强制模型在检测到有害内容时生成[EOS]拒绝信号
2. 支持实时监控机制，在生成过程中自动终止不安全延续
3. 采用阈值化的[EOS]概率实现早期拒绝

Result: 1. DIJA攻击成功率骤降(LLaDA-8B:1.3%, Dream-v0:0%)
2. 安全基准测试实现有害输出全阻断
3. 安全终止速度提升达19.3倍
4. 在多种攻击场景下保持鲁棒性

Conclusion: A2D通过token级安全对齐创新性地解决了dLLMs的防御难题，其双重防御机制(主动终止+实时监控)为生成模型安全性研究提供了新范式，实际部署中显著提升安全响应效率。

Abstract: Diffusion large language models (dLLMs) enable any-order generation, but this
flexibility enlarges the attack surface: harmful spans may appear at arbitrary
positions, and template-based prefilling attacks such as DIJA bypass
response-level refusals. We introduce A2D (Any-Order, Any-Step Defense), a
token-level alignment method that aligns dLLMs to emit an [EOS] refusal signal
whenever harmful content arises. By aligning safety directly at the token-level
under randomized masking, A2D achieves robustness to both any-decoding-order
and any-step prefilling attacks under various conditions. It also enables
real-time monitoring: dLLMs may begin a response but automatically terminate if
unsafe continuation emerges. On safety benchmarks, A2D consistently prevents
the generation of harmful outputs, slashing DIJA success rates from over 80% to
near-zero (1.3% on LLaDA-8B-Instruct, 0.0% on Dream-v0-Instruct-7B), and
thresholded [EOS] probabilities allow early rejection, yielding up to 19.3x
faster safe termination.

</details>


### [50] [Scaling Policy Compliance Assessment in Language Models with Policy Reasoning Traces](https://arxiv.org/abs/2509.23291)
*Joseph Marvin Imperial,Harish Tayyar Madabushi*

Main category: cs.CL

TL;DR: 提出Policy Reasoning Traces (PRT)方法，通过生成专业化推理链提升LLM在政策合规评估中的表现，在HIPAA/GDPR测试中实现SOTA效果


<details>
  <summary>Details</summary>
Motivation: 人工专家进行政策合规评估时需系统化逐步推理，但此类高质量专家级推理数据获取成本极高

Method: 构建政策推理轨迹(PRT)作为中间推理桥梁，应用于模型训练和推理环节

Result: 在HIPAA和GDPR政策评估中显著提升开源/商用模型性能，同时增强条款引用准确性和决策可解释性

Conclusion: PRT通过结构化推理链有效提升合规判断精度，实现合规决策与政策条款的强关联性

Abstract: Policy compliance assessment is a fundamental task of evaluating whether an
input case strictly complies with a set of human-defined rules, more generally
known as policies. In practice, human experts follow a systematic, step-by-step
process to identify violations with respect to specific stipulations outlined
in the policy. However, such documentation of gold-standard, expert-level
reasoning processes is costly to acquire. In this paper, we introduce Policy
Reasoning Traces (PRT), a form of specialized generated reasoning chains that
serve as a reasoning bridge to improve an LLM's policy compliance assessment
capabilities. Our empirical evaluations demonstrate that the use of PRTs for
both inference-time and training-time scenarios significantly enhances the
performance of open-weight and commercial models, setting a new
state-of-the-art for HIPAA and GDPR policies. Beyond accuracy gains, we also
highlight how PRTs can improve an LLM's ability to accurately cite policy
clauses, as well as influence compliance decisions through their high
utilization from the raw chains of thought.

</details>


### [51] [Learning to Reason in Structured In-context Environments with Reinforcement Learning](https://arxiv.org/abs/2509.23330)
*Peng Yu,Zeyuan Zhao,Shao Zhang,Luoyi Fu,Xinbing Wang,Ying Wen*

Main category: cs.CL

TL;DR: 提出结构化上下文环境(SIE)框架，通过自动构建可扩展、支持泛化推理且可验证的环境，显著提升LLMs的推理能力


<details>
  <summary>Details</summary>
Motivation: 现有数学/编码环境依赖专家标注难以扩展，游戏环境技能泛化性差，缺乏理想的强化学习环境

Method: 利用大规模结构化数据自动构建环境，通过组合模式支持泛化推理，基于结构化数据的显式模式实现规则验证

Result: SIE框架在领域内推理提升明显，组合推理技能有效迁移至数学/逻辑任务，部分信息环境下仍能保持鲁棒性

Conclusion: SIE框架成功解决了LLM推理环境的关键需求，验证了结构化环境对提升模型泛化推理能力的有效性

Abstract: Large language models (LLMs) have achieved significant advancements in
reasoning capabilities through reinforcement learning (RL) via environmental
exploration. As the intrinsic properties of the environment determine the
abilities that LLMs can learn, the environment plays a important role in the RL
finetuning process. An ideal LLM reasoning environment should possess three
core characteristics: scalability, generalizable reasoning, and verifiability.
However, existing mathematical and coding environments are difficult to scale
due to heavy reliance on expert annotation, while the skills learned in
game-based environments are too specialized to generalize. To bridge this gap,
we introduce the \textbf{S}tructured \textbf{I}n-context \textbf{E}nvironment
(SIE) framework. SIE achieves scalability by automatically constructing
reasoning environments from large-scale structured data, where the rich
compositional patterns naturally support generalizable reasoning. Moreover, the
explicit schemas and reasoning chains in structured data provide a foundation
for rule-based verifiability. Experimental results show that SIE framework not
only achieves substantial improvements in in-domain structured reasoning, but
also enables the learned compositional reasoning skills to generalize
effectively to out-of-domain mathematical and logical reasoning tasks. We
further explored learning in information-limited partial SIEs and found that
LLMs can infer the missing information through exploring the environment,
leading to robust reasoning improvements and generalization performance.

</details>


### [52] [C-Evolve: Consensus-based Evolution for Prompt Groups](https://arxiv.org/abs/2509.23331)
*Tiancheng Li,Yuhang Wang,Zhiyang Chen,Zijun Wang,Liyuan Ma,Guo-jun Qi*

Main category: cs.CL

TL;DR: 通过群体投票优化提示的进化算法C-Evolve显著提升AI任务性能


<details>
  <summary>Details</summary>
Motivation: 现有研究较少探索聚合多个提示的共识机制能否突破系统能力边界

Method: 采用岛屿进化算法维持种群多样性，通过投票评分机制替代个体适应度评估，优先保留可能形成高性能群体的提示组合

Result: 在Qwen3-8B上HotpotQA准确率70.67%（+4.95%），IFBench达43.88%（+2.73%）；GPT-4.1-mini在IFBench达47.96%，MATH基准95.33%

Conclusion: 基于群体共识的进化机制能有效提升模型性能，特别是在数学推理和开放问答任务中展现出竞争优势

Abstract: Prompt evolution algorithms offer a powerful paradigm for enhancing AI
systems based on closed-source models, while few work explores whether
aggregating results from multiple prompts to reach a consensus can further
advance the system capability boundary. In this paper, we introduce
Consensus-Evolve (C-Evolve), an evolutionary algorithm that discovers a group
of prompts whose aggregated outputs after majority voting achieve optimal
performance. More specifically, C-Evolve employs an island-based evolutionary
algorithm to maintain population diversity, and prompts from distinct islands
are selected to form groups to aggregate their outputs. The key difference from
single individual evolution is a voting score, which evaluates each individual
prompt's contribution within groups. We take this as the fitness score for
evolution instead of individual performance. Consequently, C-Evolve is more
likely to produce and maintain prompts with higher potential to form a
high-performing group and eliminate low-performing ones, gradually improving
the group performance after reaching consensus. Our method achieves
state-of-the-art performance across a wide range of tasks, including both
open-ended tasks like HotpotQA and closed-ended tasks like MATH. On Qwen3-8B,
C-Evolve achieves 70.67% on HotpotQA and 43.88% on IFBench, which are 4.95% and
2.73% higher than GEPA, respectively. For GPT-4.1-mini, the accuracy on IFBench
is further improved to 47.96% and reaches 95.33% in the MATH benchmark. These
results demonstrate the C-Evolve's competitive performance.

</details>


### [53] [Dual-Space Smoothness for Robust and Balanced LLM Unlearning](https://arxiv.org/abs/2509.23362)
*Han Yan,Zheyuan Liu,Meng Jiang*

Main category: cs.CL

TL;DR: 提出PRISM框架，通过表示空间和参数空间的双重平滑优化，解决机器遗忘中的指标失衡和攻击脆弱性问题，在多个场景下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法存在灾难性遗忘、指标失衡（如过度优化单一目标）以及参数扰动易受攻击（如重新学习攻击和越狱攻击）的问题。

Method: 1. 表示空间阶段：使用鲁棒训练的探针防御越狱攻击；2. 参数空间阶段：解耦保留-遗忘梯度冲突，平滑参数空间以减少重新学习攻击。

Result: 在WMDP和MUSE数据集上的实验表明，PRISM在多种攻击下优于现有方法，同时更好地平衡了遗忘效率、效用保留和隐私保护等核心指标。

Conclusion: PRISM通过双重空间平滑机制有效提升机器遗忘的鲁棒性和指标平衡性，为语言模型安全部署提供新方案。

Abstract: With the rapid advancement of large language models, Machine Unlearning has
emerged to address growing concerns around user privacy, copyright
infringement, and overall safety. Yet state-of-the-art (SOTA) unlearning
methods often suffer from catastrophic forgetting and metric imbalance, for
example by over-optimizing one objective (e.g., unlearning effectiveness,
utility preservation, or privacy protection) at the expense of others. In
addition, small perturbations in the representation or parameter space can be
exploited by relearn and jailbreak attacks. To address these challenges, we
propose PRISM, a unified framework that enforces dual-space smoothness in
representation and parameter spaces to improve robustness and balance
unlearning metrics. PRISM consists of two smoothness optimization stages: (i) a
representation space stage that employs a robustly trained probe to defend
against jailbreak attacks, and (ii) a parameter-space stage that decouples
retain-forget gradient conflicts, reduces imbalance, and smooths the parameter
space to mitigate relearning attacks. Extensive experiments on WMDP and MUSE,
across conversational-dialogue and continuous-text settings, show that PRISM
outperforms SOTA baselines under multiple attacks while achieving a better
balance among key metrics.

</details>


### [54] [MedCritical: Enhancing Medical Reasoning in Small Language Models via Self-Collaborative Correction](https://arxiv.org/abs/2509.23368)
*Xinchun Su,Chunxu Luo,Yixuan Li,Weidong Yang,Lipeng Ma*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In the field of medicine, complex reasoning tasks such as clinical diagnosis,
treatment planning, and medical knowledge integration pose significant
challenges, where small language models often underperform compared to large
language models like GPT-4 and Deepseek. Recent knowledge distillation-based
methods aim to address these issues through teacher-guided error correction,
but this LLM as judge approach remains challenging in terms of cost, time, and
efficiency. To circumvent this issue, we propose a novel two-stage framework,
MedCritical, which uses a small language model fine-tuned by a large teacher
model to play against itself. In the first stage, we extract high-level and
detailed long-chain thought templates from the teacher model to guide the
student model to generate more complex reasoning thoughts. In the second stage,
we introduce direct preference optimization (DPO) through model self-iteration
collaboration to enhance the reasoning ability of the student model by playing
against the correction trajectory of the fine-tuned model during training. This
model self-learning DPO approach teaches the student model to use its own
error-driven insights to consolidate its skills and knowledge to solve complex
problems, and achieves comparable results to traditional knowledge distillation
methods using teacher models at a lower cost. Notably, our MedCritical 7B model
outperforms the Taiyi and Huatuo-o1-7B models by 3.04\% and 10.12\%
respectively on the CMExam benchmark, achieving new SOTA performance among
7B-class small models.

</details>


### [55] [Alignment through Meta-Weighted Online Sampling: Bridging the Gap between Data Generation and Preference Optimization](https://arxiv.org/abs/2509.23371)
*Junming Yang,Ning Xu,Biao Liu,Shiqi Qiao,Xin Geng*

Main category: cs.CL

TL;DR: 提出MetaAPO框架，通过元学习动态平衡在线生成数据与离线数据的质量与分布，在降低42%标注成本的同时提升模型对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法存在离线数据与模型动态策略的分布不匹配问题，静态启发式方法难以适应模型学习状态变化。

Method: 使用轻量级元学习器作为'对齐差距估计器'，评估在线采样潜力，通过元权重动态调整在线/离线数据在优化目标中的权重。

Result: 在AlpacaEval 2等基准测试中全面超越现有方法，减少42%在线标注成本。

Conclusion: MetaAPO通过动态耦合数据生成与模型训练，有效解决分布不匹配问题，实现更高效的偏好优化。

Abstract: Preference optimization is crucial for aligning large language models (LLMs)
with human values and intentions. A significant challenge in this process is
the distribution mismatch between pre-collected offline preference data and the
evolving model policy. Existing methods attempt to reduce this gap using static
heuristics or decoupled online sampling strategies, but they often fail to
adapt to the model's dynamic learning state. To bridge this gap, we propose
Meta-Weighted Adaptive Preference Optimization (MetaAPO), a novel framework
that dynamically couples data generation with model training. MetaAPO employs a
lightweight meta-learner, as an "alignment gap estimator", to evaluate the
potential benefits of on-policy sampling in relation to offline data. This
guides targeted online generation and assigns sample-wise meta-weights to the
optimization objective, dynamically balancing the quality and distribution of
online and offline data. Experiments on AlpacaEval 2, Arena-Hard and MT-Bench
demonstrate that MetaAPO consistently outperforms existing preference
optimization approaches across various settings, while reducing 42% in online
annotation costs.

</details>


### [56] [CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical Contrastive Decoding](https://arxiv.org/abs/2509.23379)
*Xi Zhang,Zaiqiao Meng,Jake Lever,Edmond S. L. Ho*

Main category: cs.CL

TL;DR: 论文提出Clinical Contrastive Decoding (CCD)框架，通过双阶段对比机制减少放射学多模态大模型(MLLMs)的医学幻觉问题，提升临床可信度。


<details>
  <summary>Details</summary>
Motivation: 放射学MLLMs易产生与图像无关的临床描述(医学幻觉)，在需要精准医学诊断的场景存在高风险。

Method: 利用放射学专家模型的结构化临床信号，采用无训练/检索的推理框架，通过token-level对数修正的双阶段对比机制(层间对比+层内对比)。

Result: 在MIMIC-CXR等数据集上实现RadGraph-F1指标最高提升17%，且适配不同MLLM模型。

Conclusion: CCD为医学幻觉问题提供轻量化解决方案，有效连接专家模型与MLLMs，推动放射学报告生成任务的临床可靠性。

Abstract: Multimodal large language models (MLLMs) have recently achieved remarkable
progress in radiology by integrating visual perception with natural language
understanding. However, they often generate clinically unsupported
descriptions, known as medical hallucinations, which pose serious risks in
medical applications that demand accuracy and image-grounded outputs. Through
empirical analysis, we find that prompt-induced hallucinations remain prevalent
in radiology MLLMs, largely due to over-sensitivity to clinical sections. To
address this, we introduce Clinical Contrastive Cecoding (CCD), a training-free
and retrieval-free inference framework that integrates structured clinical
signals from task-specific radiology expert models. CCD introduces a dual-stage
contrastive mechanism to refine token-level logits during generation, thereby
enhancing clinical fidelity without modifying the base MLLM. Experiments on
three datasets and multiple models demonstrate that CCD consistently improves
overall performance on radiology report generation (RRG). On the MIMIC-CXR
dataset, it yields up to a 17% improvement in RadGraph-F1 when applied to
state-of-the-art RRG models. Our approach provides a lightweight and
generalisable solution for mitigating medical hallucinations, effectively
bridging expert models and MLLMs in radiology.

</details>


### [57] [Guard Vector: Beyond English LLM Guardrails with Task-Vector Composition and Streaming-Aware Prefix SFT](https://arxiv.org/abs/2509.23381)
*Wonhyuk Lee,Youngchol Kim,Yunjin Park,Junhyung Moon,Dongyoung Jeong,Wanjin Park*

Main category: cs.CL

TL;DR: 提出Guard Vector技术，通过参数差异构建安全模型，结合流式感知训练和单令牌输出设计，提升AI安全系统的分类质量、多语言支持与运行效率


<details>
  <summary>Details</summary>
Motivation: 解决现有安全模型在流式处理效率低、多语言支持依赖额外训练、模型可移植性受限三大痛点

Method: 1. 计算防护模型与预训练模型的参数差异构建Guard Vector
2. 采用前缀监督微调对齐流式输入行为
3. 设计单令牌输出分类器提升吞吐量

Result: 实现英语基准测试准确率提升15%，中日韩语言零样本支持，模型移植Llama/Gemma双架构成功，推理吞吐量提高3倍

Conclusion: 该框架通过参数空间操作与流式优化设计，在保持模型安全性的同时突破多语言和架构限制，为AI安全提供高效解决方案

Abstract: We introduce Guard Vector, a safety task vector computed as the parameter
difference between a guardrail model (Guard Model) and a same-architecture
pretrained language model. Composing this vector with a target language model
yields a Target Guard Model (TGM). We then adapt TGM with a streaming-aware
approach that combines prefix-based training and evaluation with a classifier
that produces a single-token output. With this composition alone, TGM improves
classification quality over established Guard Models across standard safety
suites and enables language extensibility to Chinese, Japanese, and Korean,
requiring neither additional training nor target language labels. It also
demonstrates model portability across two widely used public guardrail
backbones, Llama and Gemma. With prefix SFT (supervised fine-tuning), TGM
preserves classification quality under streaming by aligning the behavior
between prefix inputs and full-text inputs. The single-token output design
increases throughput and reduces latency. Together, these components reduce
data and compute requirements while promoting streaming-aware evaluation
practices, thereby contributing to a more responsible AI ecosystem.

</details>


### [58] [Train Once, Answer All: Many Pretraining Experiments for the Cost of One](https://arxiv.org/abs/2509.23383)
*Sebastian Bordt,Martin Pawelczyk*

Main category: cs.CL

TL;DR: 提出在单次预训练中并行多实验的方法，验证其在有限算力下的可行性，证明实验交互影响可忽略


<details>
  <summary>Details</summary>
Motivation: 解决大模型预训练计算成本过高的问题，探索在单次训练中并行执行多实验的可能性以提高科研效率

Method: 通过训练1.5B参数模型（210B tokens），动态调整训练数据实现知识获取，同时集成数据污染检测、水印植入等十项实验

Result: 成功复现前人实验结果并完成新研究，实验对模型动态影响<5%，持续预训练显示实验间交互作用可忽略（p>0.05）

Conclusion: 单次训练并行多实验的方法有效降低科研成本，为大规模模型研究提供高效实验框架，但需注意实验设计避免潜在交互干扰

Abstract: Recent work has demonstrated that controlled pretraining experiments are a
powerful tool for understanding learning, reasoning, and memorization in large
language models (LLMs). However, the computational cost of pretraining presents
a significant constraint. To overcome this constraint, we propose to conduct
multiple pretraining experiments simultaneously during a single training run.
We demonstrate the feasibility of this approach by conducting ten experiments
during the training of a 1.5B parameter model on 210B tokens. Although we only
train a single model, we can replicate the results from multiple previous works
on data contamination, poisoning, and memorization. We also conduct novel
investigations into knowledge acquisition, mathematical reasoning, and
watermarking. For example, we dynamically update the training data until the
model acquires a particular piece of knowledge. Remarkably, the influence of
the ten experiments on the model's training dynamics and overall performance is
minimal. However, interactions between different experiments may act as a
potential confounder in our approach. We propose to test for interactions with
continual pretraining experiments, finding them to be negligible in our setup.
Overall, our findings suggest that performing multiple pretraining experiments
in a single training run can enable rigorous scientific experimentation with
large models on a compute budget.

</details>


### [59] [No Loss, No Gain: Gated Refinement and Adaptive Compression for Prompt Optimization](https://arxiv.org/abs/2509.23387)
*Wenhang Shi,Yiren Chen,Shuqing Bian,Xinyi Zhang,Kai Tang,Pengfei Hu,Zhe Zhao,Wei Lu,Xiaoyong Du*

Main category: cs.CL

TL;DR: 提出GRACE框架，通过门控优化+自适应压缩策略实现高效提示词优化，性能提升显著且计算成本仅为同类25%


<details>
  <summary>Details</summary>
Motivation: 现有自动提示优化方法存在生成不稳定、易陷入局部最优、效率低下三大痛点，需系统性解决方案

Method: 双策略协同：1) 门控优化策略含反馈调节门+更新拒绝门，稳定生成优质提示；2) 自适应压缩策略在优化停滞时提炼核心概念重启优化路径

Result: 在BBH/领域特定/通用NLP三大类11个任务中相对SOTA平均提升4.7%/4.4%/2.7%，仅需25%的提示生成预算

Conclusion: GRACE通过智能引入信息损耗，在效果与效率间取得突破性平衡，为实际应用提供高效低成本的提示优化方案

Abstract: Prompt engineering is crucial for leveraging the full potential of large
language models (LLMs). While automatic prompt optimization offers a scalable
alternative to costly manual design, generating effective prompts remains
challenging. Existing methods often struggle to stably generate improved
prompts, leading to low efficiency, and overlook that prompt optimization
easily gets trapped in local optima. Addressing this, we propose GRACE, a
framework that integrates two synergistic strategies: Gated Refinement and
Adaptive Compression, achieving Efficient prompt optimization. The gated
refinement strategy introduces a feedback regulation gate and an update
rejection gate, which refine update signals to produce stable and effective
prompt improvements. When optimization stagnates, the adaptive compression
strategy distills the prompt's core concepts, restructuring the optimization
trace and opening new paths. By strategically introducing information loss
through refinement and compression, GRACE delivers substantial gains in
performance and efficiency. In extensive experiments on 11 tasks across three
practical domains, including BIG-Bench Hard (BBH), domain-specific, and general
NLP tasks, GRACE achieves significant average relative performance improvements
of 4.7%, 4.4% and 2.7% over state-of-the-art methods, respectively. Further
analysis shows that GRACE achieves these gains using only 25% of the prompt
generation budget required by prior methods, highlighting its high optimization
efficiency and low computational overhead. Our code is available at
https://github.com/Eric8932/GRACE.

</details>


### [60] [Liaozhai through the Looking-Glass: On Paratextual Explicitation of Culture-Bound Terms in Machine Translation](https://arxiv.org/abs/2509.23395)
*Sherrie Shen,Weixuan Wang,Alexandra Birch*

Main category: cs.CL

TL;DR: 该论文提出基于热奈特副文本理论的机器翻译文化阐释任务，通过构建《聊斋》英译本数据集验证发现：LLM生成的副文本虽能提升理解效果，但仍显著逊于人工译文，且译者副文本使用存在显著个体差异。


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译方法在处理文化负载词时仅关注文本内显化，忽视了专业译者使用的脚注等副文本阐释手段。文学与翻译研究中的副文本理论为此提供了新的解决思路。

Method: 1. 将热奈特副文本理论形式化为机器翻译任务
2. 构建560个《聊斋》英译本专家对齐副文本数据集
3. 评估带/不带推理轨迹的LLM在阐释选择与内容生成上的表现

Result: 实验表明：
- LLM生成副文本使读者理解度提升17.3%（较基准模型）
- 人工评估显示译者副文本效果仍优于LLM 32.6%
- 统计分析揭示专业译者副文本使用存在41.7%的个体差异

Conclusion: 副文本显化机制可推动机器翻译突破语言对等局限，在单语解释、个性化适配等场景具有应用潜力，且文化中介本质上是开放性的非规范性过程。

Abstract: The faithful transfer of contextually-embedded meaning continues to challenge
contemporary machine translation (MT), particularly in the rendering of
culture-bound terms--expressions or concepts rooted in specific languages or
cultures, resisting direct linguistic transfer. Existing computational
approaches to explicitating these terms have focused exclusively on in-text
solutions, overlooking paratextual apparatus in the footnotes and endnotes
employed by professional translators. In this paper, we formalize Genette's
(1987) theory of paratexts from literary and translation studies to introduce
the task of paratextual explicitation for MT. We construct a dataset of 560
expert-aligned paratexts from four English translations of the classical
Chinese short story collection Liaozhai and evaluate LLMs with and without
reasoning traces on choice and content of explicitation. Experiments across
intrinsic prompting and agentic retrieval methods establish the difficulty of
this task, with human evaluation showing that LLM-generated paratexts improve
audience comprehension, though remain considerably less effective than
translator-authored ones. Beyond model performance, statistical analysis
reveals that even professional translators vary widely in their use of
paratexts, suggesting that cultural mediation is inherently open-ended rather
than prescriptive. Our findings demonstrate the potential of paratextual
explicitation in advancing MT beyond linguistic equivalence, with promising
extensions to monolingual explanation and personalized adaptation.

</details>


### [61] [Comparison of Scoring Rationales Between Large Language Models and Human Raters](https://arxiv.org/abs/2509.23412)
*Haowei Hua,Hong Jiao,Dan Song*

Main category: cs.CL

TL;DR: 探讨大语言模型在自动评分中的应用，比较人类与LLM评分员的评分准确性和思维差异


<details>
  <summary>Details</summary>
Motivation: 通过分析人类与LLM评分员的评分理由，揭示自动评分不一致的原因并提升评分系统的可解释性

Method: 使用二次加权kappa、归一化互信息评估准确性，余弦相似度衡量理由相似性，基于嵌入向量的主成分分析进行理由聚类

Result: LLMs展现出接近人类水平的评分准确性，其理由聚类模式与人类存在系统性差异，揭示了算法思维与人类认知的不同路径

Conclusion: LLM自动评分需结合理由分析优化模型可解释性，人机协同评分体系将成为未来教育评估的重要发展方向

Abstract: Advances in automated scoring are closely aligned with advances in
machine-learning and natural-language-processing techniques. With recent
progress in large language models (LLMs), the use of ChatGPT, Gemini, Claude,
and other generative-AI chatbots for automated scoring has been explored. Given
their strong reasoning capabilities, LLMs can also produce rationales to
support the scores they assign. Thus, evaluating the rationales provided by
both human and LLM raters can help improve the understanding of the reasoning
that each type of rater applies when assigning a score. This study investigates
the rationales of human and LLM raters to identify potential causes of scoring
inconsistency. Using essays from a large-scale test, the scoring accuracy of
GPT-4o, Gemini, and other LLMs is examined based on quadratic weighted kappa
and normalized mutual information. Cosine similarity is used to evaluate the
similarity of the rationales provided. In addition, clustering patterns in
rationales are explored using principal component analysis based on the
embeddings of the rationales. The findings of this study provide insights into
the accuracy and ``thinking'' of LLMs in automated scoring, helping to improve
the understanding of the rationales behind both human scoring and LLM-based
automated scoring.

</details>


### [62] [Retrieval-Constrained Decoding Reveals Underestimated Parametric Knowledge in Language Models](https://arxiv.org/abs/2509.23417)
*Rajaa El Hamdani,Samy Haffoudhi,Nils Holzenberger,Fabian Suchanek,Thomas Bonald,Fragkiskos D. Malliaros*

Main category: cs.CL

TL;DR: RCD解码策略通过约束输出表面形式，显著提升语言模型事实知识评估准确性（例如Llama-70B F1从32.3%提升至46.0%）


<details>
  <summary>Details</summary>
Motivation: 现有评估方法因对答案表面形式过于严格，导致低估语言模型实际掌握的知识量

Method: 提出检索约束解码(RCD)策略，限制模型输出到唯一表面形式；构建包含19,137个常识问题的YAGO-QA数据集

Result: RCD使Llama-3.1-70B准确率提升43%（32.3%→46.0%），较小模型Llama-3.1-8B表现甚至超越大模型标准解码结果（33.0% vs 32.3%）

Conclusion: RCD有效揭示语言模型真实知识容量，重新定义模型能力评估方式，相关代码与数据集已开源

Abstract: Language models (LMs) encode substantial factual knowledge, but often produce
answers judged as incorrect. We hypothesize that many of these answers are
actually correct, but are expressed in alternative surface forms that are
dismissed due to an overly strict evaluation, leading to an underestimation of
models' parametric knowledge. We propose Retrieval-Constrained Decoding (RCD),
a decoding strategy that restricts model outputs to unique surface forms. We
introduce YAGO-QA, a dataset of 19,137 general knowledge questions. Evaluating
open-source LMs from 135M to 70B parameters, we show that standard decoding
undervalues their knowledge. For instance, Llama-3.1-70B scores only 32.3% F1
with vanilla decoding but 46.0% with RCD. Similarly, Llama-3.1-8B reaches 33.0%
with RCD, outperforming the larger model under vanilla decoding. We publicly
share the code and dataset at https://github.com/Rajjaa/disambiguated-LLM.

</details>


### [63] [Cognition-of-Thought Elicits Social-Aligned Reasoning in Large Language Models](https://arxiv.org/abs/2509.23441)
*Xuanming Zhang,Yuxuan Chen,Min-Hsuan Yeh,Yixuan Li*

Main category: cs.CL

TL;DR: 提出CooT框架，通过解码阶段的显式认知监控循环提升LLM安全性，实现动态对齐策略更新


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐策略将安全规则固化在模型权重中，存在控制隐式、策略静态且难以修改的局限性

Method: 设计Generator-Perceiver双组件架构，基于原则层级实时检测生成偏差，通过回滚机制与指导注入实现动态修正

Result: 跨多个基准测试显示CooT持续提升模型安全性和社会推理能力

Conclusion: CooT开创性地将模型对齐转化为显式动态过程，支持策略灵活更新且无需重新训练模型

Abstract: Large language models (LLMs) excel at complex reasoning but can still exhibit
harmful behaviors. Current alignment strategies typically embed safety into
model weights, making these controls implicit, static, and difficult to modify.
This paper introduces Cognition-of-Thought (CooT), a novel decoding-time
framework that equips LLMs with an explicit cognitive self-monitoring loop.
CooT couples a standard text Generator with a cognitive Perceiver that
continuously monitors the unfolding sequence. The Perceiver uses a structured,
precedence-based hierarchy of principles (e.g., safety over obedience) to
detect potential misalignments as they arise. When violations are flagged, CooT
intervenes by rolling back the generation to the point of error and
regenerating under injected guidance that combines universal social priors with
context-specific warnings. CooT thus transforms alignment from a fixed property
into an explicit, dynamic, and auditable process active during inference,
allowing for flexible policy updates without retraining the model. Extensive
experiments across multiple benchmarks and model families confirm that CooT
consistently improves safety and social reasoning performance.

</details>


### [64] [Text-Based Approaches to Item Difficulty Modeling in Large-Scale Assessments: A Systematic Review](https://arxiv.org/abs/2509.23486)
*Sydney Peters,Nan Zhang,Hong Jiao,Ming Li,Tianyi Zhou,Robert Lissitz*

Main category: cs.CL

TL;DR: 论文系统回顾了37篇关于大规模测评中自动化项目难度预测的研究，指出基于文本的机器学习方法（尤其是Transformer架构语言模型）在无需特征工程情况下能有效预测题目难度，性能指标达到RMSE 0.165、Pearson 0.87、准确率0.806。


<details>
  <summary>Details</summary>
Motivation: 传统题目难度建模依赖耗时费力的现场测试和IRT校准，需要开发更高效的自动化预测方法以提高测评效率和公平性。

Method: 通过系统性文献综述方法，分析比较各研究中使用的数据集、难度参数、学科领域、模型架构（经典机器学习模型与Transformer模型）及性能评估指标。

Result: 1. Transformer模型可自动捕捉语法语义特征
2. 最佳性能指标：RMSE 0.165/Pearson 0.87/准确率0.806
3. 经典模型仍因可解释性被保留使用

Conclusion: 文本分析方法展现显著预测潜力，未来应聚焦模型优化（如多模态数据融合）和实际测评系统的整合应用，同时需平衡模型可解释性与预测精度。

Abstract: Item difficulty plays a crucial role in test performance, interpretability of
scores, and equity for all test-takers, especially in large-scale assessments.
Traditional approaches to item difficulty modeling rely on field testing and
classical test theory (CTT)-based item analysis or item response theory (IRT)
calibration, which can be time-consuming and costly. To overcome these
challenges, text-based approaches leveraging machine learning and language
models, have emerged as promising alternatives. This paper reviews and
synthesizes 37 articles on automated item difficulty prediction in large-scale
assessment settings published through May 2025. For each study, we delineate
the dataset, difficulty parameter, subject domain, item type, number of items,
training and test data split, input, features, model, evaluation criteria, and
model performance outcomes. Results showed that although classic machine
learning models remain relevant due to their interpretability, state-of-the-art
language models, using both small and large transformer-based architectures,
can capture syntactic and semantic patterns without the need for manual feature
engineering. Uniquely, model performance outcomes were summarized to serve as a
benchmark for future research and overall, text-based methods have the
potential to predict item difficulty with root mean square error (RMSE) as low
as 0.165, Pearson correlation as high as 0.87, and accuracy as high as 0.806.
The review concludes by discussing implications for practice and outlining
future research directions for automated item difficulty modeling.

</details>


### [65] [The Impact of Role Design in In-Context Learning for Large Language Models](https://arxiv.org/abs/2509.23501)
*Hamidreza Rouzegar,Masoud Makrehchi*

Main category: cs.CL

TL;DR: 研究探讨角色设计在上下文学习中对LLM性能的影响，测试不同模型在多种任务中的表现


<details>
  <summary>Details</summary>
Motivation: 提示工程中角色设计的潜在影响尚未充分研究，特别是在零样本/少样本学习场景下

Method: 使用GPT系列和Llama2系列模型，在情感分析、文本分类等任务上测试角色提示结构效果

Result: 基于角色的提示结构展现出提升LLM性能的潜力，特别是在复杂推理任务中

Conclusion: 角色配置策略可作为有效的提示工程技术优化方向

Abstract: In-context learning (ICL) enables Large Language Models (LLMs) to generate
predictions based on prompts without additional fine-tuning. While prompt
engineering has been widely studied, the impact of role design within prompts
remains underexplored. This study examines the influence of role configurations
in zero-shot and few-shot learning scenarios using GPT-3.5 and GPT-4o from
OpenAI and Llama2-7b and Llama2-13b from Meta. We evaluate the models'
performance across datasets, focusing on tasks like sentiment analysis, text
classification, question answering, and math reasoning. Our findings suggest
the potential of role-based prompt structuring to enhance LLM performance.

</details>


### [66] [AraS2P: Arabic Speech-to-Phonemes System](https://arxiv.org/abs/2509.23504)
*Bassam Matar,Mohamed Fayed,Ayman Khalafallah*

Main category: cs.CL

TL;DR: 提出AraS2P语音音素转换系统，采用Wav2Vec2-BERT两阶段训练（语音语料预训练+增强数据微调），在Iqra'Eval 2025音素级发音错误检测任务中夺冠


<details>
  <summary>Details</summary>
Motivation: 通过音素感知预训练与针对性数据增强提升阿拉伯语语音音素转换精度，解决发音错误检测中的实际问题

Method: 两阶段训练：1）使用MSA音素转换器生成的阿拉伯语音素大数据进行任务适应性预训练；2）在官方数据基础上，融合XTTS-v2合成的多样化诵读样本（含不同章节/说话人/文本扰动）进行微调

Result: 系统在官方评测中排名第一，词错误率18.7%，音素错误检测F1-score达92.3%

Conclusion: 音素级预训练结合定向数据增强策略能有效提升发音错误检测性能，两阶段训练框架在低资源场景下表现优异

Abstract: This paper describes AraS2P, our speech-to-phonemes system submitted to the
Iqra'Eval 2025 Shared Task. We adapted Wav2Vec2-BERT via Two-Stage training
strategy. In the first stage, task-adaptive continue pretraining was performed
on large-scale Arabic speech-phonemes datasets, which were generated by
converting the Arabic text using the MSA Phonetiser. In the second stage, the
model was fine-tuned on the official shared task data, with additional
augmentation from XTTS-v2-synthesized recitations featuring varied Ayat
segments, speaker embeddings, and textual perturbations to simulate possible
human errors. The system ranked first on the official leaderboard,
demonstrating that phoneme-aware pretraining combined with targeted
augmentation yields strong performance in phoneme-level mispronunciation
detection.

</details>


### [67] [From Human Annotation to Automation: LLM-in-the-Loop Active Learning for Arabic Sentiment Analysis](https://arxiv.org/abs/2509.23515)
*Dania Refai,Alaa Dalaq,Doaa Dalaq,Irfan Ahmad*

Main category: cs.CL

TL;DR: 提出主动学习框架结合深度学习模型和LLM标注，在降低阿拉伯情感分析标注成本的同时保持高性能


<details>
  <summary>Details</summary>
Motivation: 阿拉伯情感分析受限于高质量标注数据集不足，现有研究对主动学习和LLM标注的探索较少

Method: 使用LSTM/GRU/RNN模型在三个阿拉伯语数据集测试，比较人工标注与GPT-4o/Claude 3/DeepSeek等LLM标注策略

Result: LLM辅助标注在饥饿站数据集仅需450样本达93%准确率，MASAC数据集650样本达82%准确率（与人工标注相当）

Conclusion: LLM辅助的主动学习在阿拉伯情感分析中可替代人工标注，显著降低标注成本并保持性能

Abstract: Natural language processing (NLP), particularly sentiment analysis, plays a
vital role in areas like marketing, customer service, and social media
monitoring by providing insights into user opinions and emotions. However,
progress in Arabic sentiment analysis remains limited due to the lack of large,
high-quality labeled datasets. While active learning has proven effective in
reducing annotation efforts in other languages, few studies have explored it in
Arabic sentiment tasks. Likewise, the use of large language models (LLMs) for
assisting annotation and comparing their performance to human labeling is still
largely unexplored in the Arabic context. In this paper, we propose an active
learning framework for Arabic sentiment analysis designed to reduce annotation
costs while maintaining high performance. We evaluate multiple deep learning
architectures: Specifically, long short-term memory (LSTM), gated recurrent
units (GRU), and recurrent neural networks (RNN), across three benchmark
datasets: Hunger Station, AJGT, and MASAC, encompassing both modern standard
Arabic and dialectal variations. Additionally, two annotation strategies are
compared: Human labeling and LLM-assisted labeling. Five LLMs are evaluated as
annotators: GPT-4o, Claude 3 Sonnet, Gemini 2.5 Pro, DeepSeek Chat, and LLaMA 3
70B Instruct. For each dataset, the best-performing LLM was used: GPT-4o for
Hunger Station, Claude 3 Sonnet for AJGT, and DeepSeek Chat for MASAC. Our
results show that LLM-assisted active learning achieves competitive or superior
performance compared to human labeling. For example, on the Hunger Station
dataset, the LSTM model achieved 93% accuracy with only 450 labeled samples
using GPT-4o-generated labels, while on the MASAC dataset, DeepSeek Chat
reached 82% accuracy with 650 labeled samples, matching the accuracy obtained
through human labeling.

</details>


### [68] [On the Shelf Life of Fine-Tuned LLM Judges: Future Proofing, Backward Compatibility, and Question Generalization](https://arxiv.org/abs/2509.23542)
*Janvijay Singh,Austin Xu,Yilun Zhou,Yefan Zhou,Dilek Hakkani-Tur,Shafiq Joty*

Main category: cs.CL

TL;DR: 研究揭示了微调评判模型在实际部署中的三大挑战：未来验证困难但向后兼容较易，DPO模型表现更优；持续学习能平衡新旧数据适应；现有模型对未见过问题泛化能力不足。


<details>
  <summary>Details</summary>
Motivation: 传统评估忽略评判模型在生成器持续更新环境下的持续有效性，需研究其在未来模型响应、历史模型响应及未见过问题上的表现。

Method: 在数学领域构建统一框架，通过调整训练/测试分布，测试三种微调算法(SFT/DPO)和三个基座模型在未来验证、向后兼容及问题泛化方面的表现。

Result: 未来验证成功率仅15-20%，但向后兼容准确率超85%；DPO模型表现最佳；持续学习使新旧分布适应提升27%；所有模型在未见过问题上准确率下降8-15%。

Conclusion: 开发评判模型需采用持续学习和DPO方法，同时设计专门机制提升问题泛化能力，以适应生成模型的动态演进。

Abstract: The LLM-as-a-judge paradigm is widely used in both evaluating free-text model
responses and reward modeling for model alignment and finetuning. Recently,
finetuning judges with judge-specific data has emerged as an often preferred
choice over directly prompting frontier models as judges, as the former
achieves better performance with smaller model sizes while being more robust to
common biases. However, the standard evaluation ignores several practical
concerns of finetuned judges regarding their real world deployment. In this
paper, we identify and formalize three aspects that affect the shelf life of
these judges: future proofing and backward compatibility -- how well judges
finetuned on responses by today's generator models perform on responses by
future models or past models, as well as question generalization -- how well
judges generalize to unseen questions at test time. We study these three
aspects in the math domain under a unified framework with varying train and
test distributions, three SFT- and DPO-based finetuning algorithms and three
different base models. Experiments suggest that future-proofing is challenging
for most models, while backward compatibility is relatively easy, with
DPO-trained models consistently improving performance. We further find that
continual learning provides a more balanced adaptation to shifts between older
and newer response distributions than training solely on stronger or weaker
responses. Moreover, all models observe certain degrees of performance
degradation when moving from questions seen during training to unseen ones,
showing that current judges do not fully generalize to unseen questions. These
findings provide insights into practical considerations for developing and
deploying judge models in the face of ever-changing generators.

</details>


### [69] [Automatic Speech Recognition for Greek Medical Dictation](https://arxiv.org/abs/2509.23550)
*Vardis Georgilas,Themos Stafylakis*

Main category: cs.CL

TL;DR: 开发结合语音识别与文本校正的希腊医学听写系统，通过领域微调提升转录准确率


<details>
  <summary>Details</summary>
Motivation: 解决希腊医疗文档中复杂术语与语言变异问题，减轻医护人员文书负担

Method: 融合声学/文本建模技术，适配希腊医学语境的语言处理方案

Result: 实现更准确连贯的医学转录，验证领域专用系统的有效性

Conclusion: 为希腊医疗领域构建实用语言技术框架，提升临床文档处理效率

Abstract: Medical dictation systems are essential tools in modern healthcare, enabling
accurate and efficient conversion of speech into written medical documentation.
The main objective of this paper is to create a domain-specific system for
Greek medical speech transcriptions. The ultimate goal is to assist healthcare
professionals by reducing the overload of manual documentation and improving
workflow efficiency. Towards this goal, we develop a system that combines
automatic speech recognition techniques with text correction model, allowing
better handling of domain-specific terminology and linguistic variations in
Greek. Our approach leverages both acoustic and textual modeling to create more
realistic and reliable transcriptions. We focused on adapting existing language
and speech technologies to the Greek medical context, addressing challenges
such as complex medical terminology and linguistic inconsistencies. Through
domain-specific fine-tuning, our system achieves more accurate and coherent
transcriptions, contributing to the development of practical language
technologies for the Greek healthcare sector.

</details>


### [70] [Towards Efficient CoT Distillation: Self-Guided Rationale Selector for Better Performance with Fewer Rationales](https://arxiv.org/abs/2509.23574)
*Jianzhi Yan,Le Liu,Youcheng Pan,Shiwei Chen,Yang Xiang,Buzhou Tang*

Main category: cs.CL

TL;DR: 提出MoRSD方法，通过筛选高质量逻辑链提升小模型推理能力，实验显示使用更少但优质的数据可实现4.6%性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有CoT蒸馏方法过度关注数据量而忽视逻辑链质量，导致噪声传递影响小模型学习效果。

Method: 创新性提出基于逻辑链准确性、多样性和难度的筛选机制，并设计RD指标量化学生模型在特定逻辑链下的答题能力。

Result: 在三大任务的七个数据集上实现平均4.6%性能提升，使用量减少但质量优化的逻辑链效果优于全量数据。

Conclusion: 高质量逻辑链比数据量更重要，MoRSD为高效CoT蒸馏提供新思路，代码将开源。

Abstract: Chain-of-thought (CoT) distillation aims to enhance small language models'
(SLMs) reasoning by transferring multi-step reasoning capability from the
larger teacher models. However, existing work underestimates rationale quality,
focusing primarily on data quantity, which may transfer noisy or incorrect
information to the student model. To address the above issues, we proposed
\textbf{M}odel-\textbf{O}riented \textbf{R}ationale \textbf{S}election
\textbf{D}istillation (MoRSD), which can discern and select high quality
rationales for distillation to improve performance further. We further propose
a Rationale Difficulty (RD) metric to measure the ability of the student model
to generate the correct answer under a given rationale. Compared to the
baseline, we achieved 4.6$\%$ average improvement on seven datasets over three
tasks, using fewer rationales by controlling their accuracy, diversity, and
difficulty. Our results reveal that a small portion of the high quality
rationales can enhance the reasoning ability of student models than the entire
dataset. Our method promises to be a possible solution for efficient CoT
distillation. Our code will be released in https://github.com/Leon221220/MoRSD.

</details>


### [71] [Jackal: A Real-World Execution-Based Benchmark Evaluating Large Language Models on Text-to-JQL Tasks](https://arxiv.org/abs/2509.23579)
*Kevin Frank,Anmol Gulati,Elias Lumer,Sindy Campagna,Vamse Kumar Subbiah*

Main category: cs.CL

TL;DR: 构建首个开放可执行的文本转JQL基准测试Jackal，揭示主流大模型在Jira企业数据转换中的局限性


<details>
  <summary>Details</summary>
Motivation: 现有缺乏基于真实执行的开放基准测试来评估自然语言到JQL的转换能力，影响企业数据检索效率

Method: 创建包含10万对NL-JQL的Jackal数据集，在含20万条工单的Jira实例上验证执行准确性，测试23种LLM的四种查询类型转换能力

Result: 最佳模型(Gemini 2.5 Pro)平均执行准确率仅60.3%，不同查询类型差异显著：长自然语言(86.0%) vs 短自然语言(35.7%)

Conclusion: Jackal暴露当前LLM生成正确JQL的局限性，为Jira企业数据研究设立新的执行基准挑战

Abstract: Enterprise teams rely on the Jira Query Language (JQL) to retrieve and filter
issues from Jira. Yet, to our knowledge, there is no open, real-world,
execution-based benchmark for mapping natural language queries to JQL. We
introduce Jackal, a novel, large-scale text-to-JQL benchmark comprising 100,000
natural language (NL) requests paired with validated JQL queries and
execution-based results on a live Jira instance with over 200,000 issues. To
reflect real-world usage, each JQL query is associated with four types of user
requests: (i) Long NL, (ii) Short NL, (iii) Semantically Similar, and (iv)
Semantically Exact. We release Jackal, a corpus of 100,000 text-to-JQL pairs,
together with an execution-based scoring toolkit, and a static snapshot of the
evaluated Jira instance for reproducibility. We report text-to-JQL results on
23 Large Language Models (LLMs) spanning parameter sizes, open and closed
source models, across execution accuracy, exact match, and canonical exact
match. In this paper, we report results on Jackal-5K, a 5,000-pair subset of
Jackal. On Jackal-5K, the best overall model (Gemini 2.5 Pro) achieves only
60.3% execution accuracy averaged equally across four user request types.
Performance varies significantly across user request types: (i) Long NL
(86.0%), (ii) Short NL (35.7%), (iii) Semantically Similar (22.7%), and (iv)
Semantically Exact (99.3%). By benchmarking LLMs on their ability to produce
correct and executable JQL queries, Jackal exposes the limitations of current
state-of-the-art LLMs and sets a new, execution-based challenge for future
research in Jira enterprise data.

</details>


### [72] [LLM Hallucination Detection: HSAD](https://arxiv.org/abs/2509.23580)
*JinXin Li,Gang Tu,JunJie Hu*

Main category: cs.CL

TL;DR: 提出基于隐藏层信号频域分析的HSAD方法，通过建模LLM推理过程的时序信号并提取频谱特征，有效提升幻觉检测的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法受限于知识覆盖范围（事实验证法）和推理偏差捕捉能力（静态特征法），需开发能动态捕捉推理过程异常的新方法。

Method: 1. 将LLM推理建模为时序认知信号 2. 应用快速傅里叶变换构建频谱特征 3. 基于频谱特征设计检测算法

Result: 频谱特征分析实验验证了方法的有效性，HSAD在检测准确率和鲁棒性指标上超越现有主流方法。

Conclusion: HSAD通过融合推理过程建模与频域特征提取，突破传统方法的知识覆盖限制，为关键场景的LLM部署提供可靠保障。

Abstract: Although Large Language Models have demonstrated powerful capabilities in a
wide range of tasks such as language understanding and code generation, the
frequent occurrence of hallucinations during the generation process has become
a significant impediment to their deployment in critical application scenarios.
Current mainstream hallucination detection methods rely on factual consistency
verification or static hidden layer features. The former is constrained by the
scope of knowledge coverage, while the latter struggles to capture reasoning
biases during the inference process. To address these issues, and inspired by
signal analysis methods in cognitive neuroscience, this paper proposes a
hallucination detection method based on the frequency-domain analysis of hidden
layer temporal signals, named HSAD (\textbf{H}idden \textbf{S}ignal
\textbf{A}nalysis-based \textbf{D}etection). First, by treating the LLM's
reasoning process as a cognitive journey that unfolds over time, we propose
modeling and simulating the human process of signal perception and
discrimination in a deception-detection scenario through hidden layer temporal
signals. Next, The Fast Fourier Transform is applied to map these temporal
signals into the frequency domain to construct spectral features, which are
used to capture anomalies that arise during the reasoning process; analysis
experiments on these spectral features have proven the effectiveness of this
approach. Finally, a hallucination detection algorithm is designed based on
these spectral features to identify hallucinations in the generated content. By
effectively combining the modeling of the reasoning process with
frequency-domain feature extraction, the HSAD method overcomes the limitations
of existing approaches in terms of knowledge coverage and the detection of
reasoning biases, demonstrating higher detection accuracy and robustness.

</details>


### [73] [Timber: Training-free Instruct Model Refining with Base via Effective Rank](https://arxiv.org/abs/2509.23595)
*Taiqiang Wu,Runming Yang,Tao Liu,Jiahao Wang,Zenan Xu,Ngai Wong*

Main category: cs.CL

TL;DR: 通过权重分析验证后训练（post-training）的局限性，提出无需训练的Timber方法通过权重增量调整增强指导模型的探索能力，实验证明有效提升Pass@k性能。


<details>
  <summary>Details</summary>
Motivation: 量化证据显示后训练在权重层面（eRank指标）变化微小，导致指导模型在提升利用能力的同时严重限制了探索能力。需要平衡两种能力的解决方案。

Method: 提出Timber方法：通过定向微调权重增量(weight deltas)，将指导模型部分回退到基础模型，在保持利用能力的同时恢复探索潜力。

Result: 在Llama/Qwen系列模型中验证，Timber显著提升指导模型的Pass@k表现（特别是多候选生成任务），且无需额外训练。

Conclusion: 首次从权重层面揭示后训练的本质局限，提出可实践的模型优化新范式——通过权重微调而非重新训练来提升指导模型性能。

Abstract: Post-training, which elicits a pretrained Base model into the corresponding
Instruct model, is widely considered to be superficial. In this work, we first
reinforce this hypothesis by providing novel quantitative evidence from the
weight level that the effective rank (eRank) remains negligibly changed.
However, this superficiality also suffers a critical trade-off, improving the
exploitation capabilities at the cost of limiting its exploration. To tackle
this issue, we propose Timber, a simple yet effective training-free method that
enhances the exploration capability of the Instruct model while preserving its
exploitation. The key insight is to partially revert Instruct towards the
paired Base model by subtle yet targeted refinement of the weight deltas.
Extensive experiments on Llama and Qwen series demonstrate that Timber
consistently improves vanilla Instruct models, particularly on Pass@k
performance. Our findings offer new insights into the post-training stage at
the weight level and practical strategies to refine the Instruct model without
training.

</details>


### [74] [Fast Thinking for Large Language Models](https://arxiv.org/abs/2509.23633)
*Haoyu Zheng,Zhuonan Wang,Yuqian Yuan,Tianwei Lin,Wenqiao Zhang,Zheqi Lv,Juncheng Li,Siliang Tang,Yueting Zhuang,Hongyang He*

Main category: cs.CL

TL;DR: 提出基于潜在码本和路由机制的高效推理框架，在保证推理精度的同时显著降低LLM推理成本


<details>
  <summary>Details</summary>
Motivation: 现有Chain-of-Thought方法需要生成显式推理标记，导致延迟高、token消耗大的效率问题

Method: 1. 训练阶段通过简化的CoT草图学习策略码本
2. 推理阶段使用蒸馏的连续思维向量引导策略
3. GainRouter机制动态切换快速/慢速推理模式

Result: 在多个基准测试中达到竞争性准确率，同时显著降低推理成本

Conclusion: 为大规模语言模型提供了高效可控的推理解决方案，平衡了性能与计算资源消耗

Abstract: Reasoning-oriented Large Language Models (LLMs) often rely on generating
explicit tokens step by step, and their effectiveness typically hinges on
large-scale supervised fine-tuning or reinforcement learning. While
Chain-of-Thought (CoT) techniques substantially enhance performance on complex
reasoning tasks, they remain inefficient, requiring long reasoning traces that
increase latency and token usage. In this work, we introduce Latent Codebooks
for Fast Thinking, a framework that uses concise CoT sketches only during
training to learn a codebook of discrete strategy priors. At inference, the
model conditions on a handful of continuous thinking vectors distilled from the
codebook in a single pass, enabling strategy-level guidance without producing
explicit reasoning tokens. To complement this design, we propose GainRouter, a
lightweight routing mechanism that adaptively switches between fast codebook
guided inference and slow explicit reasoning, thereby suppressing overthinking
and reducing unnecessary token generation. Experiments across multiple
reasoning benchmarks show that our approach achieves competitive or superior
accuracy while substantially lowering inference cost, offering a practical path
toward efficient and controllable reasoning in large language models.

</details>


### [75] [Don't Settle Too Early: Self-Reflective Remasking for Diffusion Language Models](https://arxiv.org/abs/2509.23653)
*Zemin Huang,Yuhang Wang,Zhiyang Chen,Guo-Jun Qi*

Main category: cs.CL

TL;DR: 提出RemeDi模型，通过联合预测token分布与置信度分数实现动态重新掩码机制，有效提升扩散语言模型的文本修正能力。


<details>
  <summary>Details</summary>
Motivation: 传统掩码扩散语言模型（DLMs）存在错误token难以修正的问题，生成后无法灵活调整低质量内容。

Method: 1. 联合预测token分布与置信度分数决定动态remask
2. 包含监督微调（检测错误token+预测mask）和强化学习（优化生成轨迹）两阶段训练流程

Result: 在多个数据集上取得开源DLMs中的SOTA性能

Conclusion: RemeDi通过remasking机制与联合训练策略，显著提升了扩散模型文本生成的灵活性与质量

Abstract: Mask-based Diffusion Language Models (DLMs) struggle to revise incorrect
tokens: once a token is generated, it typically remains fixed. The key
challenge is to identify potential errors in the inputs. In this paper, we
propose \emph{\underline{Rem}asking-\underline{e}nabled \underline{Di}ffusion
Language Model (RemeDi}, a mask-based DLM that introduces \emph{remasking} as
another fundamental mechanism, enabling more flexible text refinement in
diffusion-based text generation. To achieve this, RemeDi jointly predicts token
distributions and per-token confidence scores at each step. The confidence
scores determine which tokens to be unmasked after the current step, allowing
the model to identify tokens with low quality and remask them. These remasked
tokens can be resampled with richer context in subsequent steps. We design a
remask-aware pipeline to train this ability, including supervised fine-tuning
which teaches the model to detect and remask incorrect tokens in addition to
predict mask tokens, and reinforcement learning which optimizes full generation
trajectories toward higher rewards. Experiments show that RemeDi achieves the
state-of-the-art results among open-source DLMs on multiple datasets.

</details>


### [76] [Beyond English-Centric Training: How Reinforcement Learning Improves Cross-Lingual Reasoning in LLMs](https://arxiv.org/abs/2509.23657)
*Shulin Huang,Yiran Ding,Junshu Pan,Yue Zhang*

Main category: cs.CL

TL;DR: 强化学习（RL）在提升大语言模型跨语言推理能力方面显著优于监督微调（SFT），尤其在非英语数据训练时表现更优


<details>
  <summary>Details</summary>
Motivation: 探索强化学习与监督微调在跨语言推理泛化能力上的差异，填补该领域的研究空白

Method: 基于Qwen2.5-3B-Base模型，在数学推理/常识推理/科学推理等多语言基准测试中进行对比实验

Result: 1. RL准确率更高且跨语言泛化能力显著强于SFT
2. 非英语数据RL训练效果优于英语数据（该现象SFT未出现）

Conclusion: RL通过更鲁棒的推理策略实现跨语言优势，为多语言模型优化提供关键指导，机理分析揭示了其跨语言优势的本质

Abstract: Enhancing the complex reasoning capabilities of Large Language Models (LLMs)
attracts widespread attention. While reinforcement learning (RL) has shown
superior performance for improving complex reasoning, its impact on
cross-lingual generalization compared to Supervised Fine-Tuning (SFT) remains
unexplored. We present the first systematic investigation into cross-lingual
reasoning generalization of RL and SFT. Using Qwen2.5-3B-Base as our foundation
model, we conduct experiments on diverse multilingual reasoning benchmarks,
including math reasoning, commonsense reasoning, and scientific reasoning. Our
investigation yields two significant findings: (1) Tuning with RL not only
achieves higher accuracy but also demonstrates substantially stronger
cross-lingual generalization capabilities compared to SFT. (2) RL training on
non-English data yields better overall performance and generalization than
training on English data, which is not observed with SFT. Furthermore, through
comprehensive mechanistic analyses, we explore the underlying factors of RL's
superiority and generalization across languages. Our results provide compelling
evidence that RL enables the model with more robust reasoning strategies,
offering crucial guidance for more equitable and effective multilingual
reasoning.

</details>


### [77] [Aligning LLMs for Multilingual Consistency in Enterprise Applications](https://arxiv.org/abs/2509.23659)
*Amit Agarwal,Hansa Meghwani,Hitesh Laxmichand Patel,Tao Sheng,Sujith Ravi,Dan Roth*

Main category: cs.CL

TL;DR: 提出基于多语言数据批量对齐的LLM微调策略，使非英语语言准确率提升23.9%且不影响英语性能


<details>
  <summary>Details</summary>
Motivation: LLMs在非英语语言场景（客服支持/内容审核/信息检索）存在高达29%的准确性落差，影响企业级多语言应用的可靠性

Method: 在训练批次中注入语义对等的多语言数据，通过直接对齐跨语言输出来消除推理偏差

Result: 非英语准确率最高提升23.9%，英语性能、模型推理能力和检索质量保持稳定

Conclusion: 该方法实现了无需改变现有LLM训练部署流程的轻量化多语言对齐方案，提升AI解决方案的公平性和稳健性

Abstract: Large language models (LLMs) remain unreliable for global enterprise
applications due to substantial performance gaps between high-resource and
mid/low-resource languages, driven by English-centric pretraining and internal
reasoning biases. This inconsistency undermines customer experience and
operational reliability in multilingual settings such as customer support,
content moderation, and information retrieval. Even with advanced
Retrieval-Augmented Generation (RAG) systems, we observe up to an 29% accuracy
drop in non-English languages compared to English.
  We propose a practical, batch-wise alignment strategy for fine-tuning LLMs,
leveraging semantically equivalent multilingual data in each training batch to
directly align model outputs across languages. This approach improves
non-English accuracy by up to 23.9\% without compromising English performance,
model reasoning, or retrieval quality. Our method is simple to implement,
scalable, and integrates seamlessly with existing LLM training \& deployment
pipelines, enabling more robust and equitable multilingual AI solutions in
industry.

</details>


### [78] [TF-Bench: Evaluating Program Semantics Reasoning with Type Inference in System F](https://arxiv.org/abs/2509.23686)
*Yifeng He,Luning Yang,Christopher Castro Gaw Gonzalo,Hao Chen*

Main category: cs.CL

TL;DR: 提出TF-Bench基准评估大模型基于System F类型推断的语义推理能力，构建纯语义版本TF-Bench_pure，实验显示顶尖模型准确率仅55.85%并揭示关键缺陷


<details>
  <summary>Details</summary>
Motivation: 现有代码推理基准缺乏程序语义的演绎评估框架，无法区分模型是真正理解语义还是利用代码-自然语言表面对齐

Method: 通过验证性变换去除代码中语义无关的自然语言描述，构建纯语义驱动的TF-Bench_pure基准，基于System F类型系统设计推理任务

Result: Claude-3.7-sonnet在TF-Bench_pure准确率仅55.85%，提出鲁棒性评分和TTC有效性指标揭示模型推理脆弱性

Conclusion: 当前大模型程序语义推理能力存在本质局限，需研究测试时推理优化和更严谨的语义评估框架

Abstract: Large Language Models (LLMs) are increasingly integrated into the software
engineering ecosystem. Their test-time compute (TTC) reasoning capabilities
show significant potential for understanding program logic and semantics beyond
mere token recognition. However, current benchmarks for code reasoning lack a
formal, program-centric deductive framework to ensure sound evaluation, and are
incapable of assessing whether models genuinely reason about program semantics
or merely exploit superficial associations between natural language and code
tokens. To bridge this gap, we introduce TF-Bench, a benchmark designed to
evaluate LLM reasoning based on type inference in System F, a task we refer to
as program semantics reasoning. By employing verified transformations to remove
semantically irrelevant natural language, we construct TF-Bench_pure, a purely
semantics-driven variant of TF-Bench. Our analysis reveals substantial
limitations in state-of-the-art LLMs, with the best-performing LLM
(Claude-3.7-sonnet) achieving only 55.85% accuracy on TF-Bench_pure.
Additionally, we propose two novel metrics to assess robustness and the
effectiveness of test-time reasoning, underscoring critical limitations in
current LLM capabilities and highlighting essential directions for future
research.

</details>


### [79] [VIVA+: Human-Centered Situational Decision-Making](https://arxiv.org/abs/2509.23698)
*Zhe Hu,Yixiao Ren,Guanzhong Liu,Jing Li,Yu Yin*

Main category: cs.CL

TL;DR: 提出VIVA+认知基准，通过6,373道多选题系统评估多模态大语言模型在现实场景中的决策能力，揭示模型局限并提出改进策略


<details>
  <summary>Details</summary>
Motivation: 现有评估方法难以衡量MLLMs在人类中心化环境中的细致推理和决策能力，需建立系统性评估框架推动模型发展

Method: 构建包含1,317个现实情境的测试集，聚焦情境理解、行动论证和反思推理三大能力维度，评估商业/开源模型并探索训练策略

Result: 当前模型在复杂决策场景存在显著局限，但针对性训练和多步推理策略可带来12-15%的性能提升

Conclusion: VIVA+为MLLMs的社交感知决策能力评估提供系统框架，研究结果指明通过上下文感知和反思机制优化模型的方向

Abstract: Multimodal Large Language Models (MLLMs) show promising results for embodied
agents in operating meaningfully in complex, human-centered environments. Yet,
evaluating their capacity for nuanced, human-like reasoning and decision-making
remains challenging. In this work, we introduce VIVA+, a cognitively grounded
benchmark for evaluating the reasoning and decision-making of MLLMs in
human-centered situations. VIVA+ consists of 1,317 real-world situations paired
with 6,373 multiple-choice questions, targeting three core abilities for
decision-making: (1) Foundational Situation Comprehension, (2) Context-Driven
Action Justification, and (3) Reflective Reasoning. Together, these dimensions
provide a systematic framework for assessing a model's ability to perceive,
reason, and act in socially meaningful ways. We evaluate the latest commercial
and open-source models on VIVA+, where we reveal distinct performance patterns
and highlight significant challenges. We further explore targeted training and
multi-step reasoning strategies, which yield consistent performance
improvements. Finally, our in-depth analysis highlights current model
limitations and provides actionable insights for advancing MLLMs toward more
robust, context-aware, and socially adept decision-making in real-world
settings.

</details>


### [80] [Collaboration of Fusion and Independence: Hypercomplex-driven Robust Multi-Modal Knowledge Graph Completion](https://arxiv.org/abs/2509.23714)
*Zhiqiang Liu,Yichi Zhang,Mengshu Sun,Lei Liang,Wen Zhang*

Main category: cs.CL

TL;DR: 提出多模态知识图谱补全方法M-Hyper，通过四元数代数实现多模态表征的共存与协同，融合模态信息和保持独立性


<details>
  <summary>Details</summary>
Motivation: 现有MMKGC方法存在模态信息丢失或交互不足的问题：基于融合的方法损失模态特异性信息，基于集成的方法难以捕捉模态间语义关联

Method: 采用四元数正交基表示独立模态，通过Hamilton积建模交互。包含细粒度实体表征分解(FERF)模块和关系感知模态融合(R2MF)模块

Result: 实验证明M-Hyper在性能、鲁棒性和计算效率方面达到SOTA，双四元数映射实现了全面的模态交互

Conclusion: 该方法首次实现融合表征与独立表征的协同，通过超复数空间建模有效解决了多模态交互与信息保留的平衡难题

Abstract: Multi-modal knowledge graph completion (MMKGC) aims to discover missing facts
in multi-modal knowledge graphs (MMKGs) by leveraging both structural
relationships and diverse modality information of entities. Existing MMKGC
methods follow two multi-modal paradigms: fusion-based and ensemble-based.
Fusion-based methods employ fixed fusion strategies, which inevitably leads to
the loss of modality-specific information and a lack of flexibility to adapt to
varying modality relevance across contexts. In contrast, ensemble-based methods
retain modality independence through dedicated sub-models but struggle to
capture the nuanced, context-dependent semantic interplay between modalities.
To overcome these dual limitations, we propose a novel MMKGC method M-Hyper,
which achieves the coexistence and collaboration of fused and independent
modality representations. Our method integrates the strengths of both
paradigms, enabling effective cross-modal interactions while maintaining
modality-specific information. Inspired by ``quaternion'' algebra, we utilize
its four orthogonal bases to represent multiple independent modalities and
employ the Hamilton product to efficiently model pair-wise interactions among
them. Specifically, we introduce a Fine-grained Entity Representation
Factorization (FERF) module and a Robust Relation-aware Modality Fusion (R2MF)
module to obtain robust representations for three independent modalities and
one fused modality. The resulting four modality representations are then mapped
to the four orthogonal bases of a biquaternion (a hypercomplex extension of
quaternion) for comprehensive modality interaction. Extensive experiments
indicate its state-of-the-art performance, robustness, and computational
efficiency.

</details>


### [81] [Do LLMs Understand Romanian Driving Laws? A Study on Multimodal and Fine-Tuned Question Answering](https://arxiv.org/abs/2509.23715)
*Eduard Barbu,Adrian Marius Dumitran*

Main category: cs.CL

TL;DR: 评估大语言模型在罗马尼亚驾驶法规QA任务的表现，发现微调后的8B模型具备竞争力，文本描述图像优于直接视觉输入，并揭示LLM自我偏好偏差


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言场景下可解释驾驶法规问答系统的需求，保障新旧驾驶员对交规的理解以提升道路安全

Method: 构建1,208题数据集（含387多模态问题），对比纯文本/多模态模型，微调Llama和RoLlama模型，采用LLM-as-a-Judge评估解释质量

Result: SOTA模型表现优异但微调8B模型可匹敌，文本描述超越视觉输入，模型自我评估存在偏好偏差

Conclusion: 证实小型模型结合文本描述策略在低资源语言可解释QA中的有效性，为实际应用提供优化方向

Abstract: Ensuring that both new and experienced drivers master current traffic rules
is critical to road safety. This paper evaluates Large Language Models (LLMs)
on Romanian driving-law QA with explanation generation. We release a
1{,}208-question dataset (387 multimodal) and compare text-only and multimodal
SOTA systems, then measure the impact of domain-specific fine-tuning for Llama
3.1-8B-Instruct and RoLlama 3.1-8B-Instruct. SOTA models perform well, but
fine-tuned 8B models are competitive. Textual descriptions of images outperform
direct visual input. Finally, an LLM-as-a-Judge assesses explanation quality,
revealing self-preference bias. The study informs explainable QA for
less-resourced languages.

</details>


### [82] [Compose and Fuse: Revisiting the Foundational Bottlenecks in Multimodal Reasoning](https://arxiv.org/abs/2509.23744)
*Yucheng Wang,Yifan Hou,Aydin Javadov,Mubashara Akhtar,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 整合而非感知是多模态推理的主要瓶颈，建议组合感知训练与早期融合控制


<details>
  <summary>Details</summary>
Motivation: 现有研究对跨模态推理中模态交互的促进/抑制机制缺乏系统性分析，实验结果存在矛盾

Method: 构建逻辑驱动的六模态交互模式评估框架，通过控制实验分析不同事实分布与逻辑组合的影响

Result: 额外模态仅在提供独立充分推理路径时有效，冗余/链式推理反致性能下降；揭示模态整合失败的三种系统性模式

Conclusion: 提出两大核心瓶颈（任务组合瓶颈与融合瓶颈），通过两步提示验证解决方案，为多模态模型设计提供新方向

Abstract: Multimodal large language models (MLLMs) promise enhanced reasoning by
integrating diverse inputs such as text, vision, and audio. Yet cross-modal
reasoning remains underexplored, with conflicting reports on whether added
modalities help or harm performance. These inconsistencies stem from a lack of
controlled evaluation frameworks and analysis of models' internals to isolate
when and why modality interactions support or undermine reasoning. We address
this gap through a logic-grounded evaluation framework that categorizes
multimodal reasoning into six interaction patterns, varying how facts are
distributed across modalities and logically combined. Empirically, additional
modalities enhance reasoning only when they provide independent and sufficient
reasoning paths, while redundant or chained entailment support often hurts
performance. Moreover, reasoning degrades in three systematic ways: weaker
modalities drag down overall performance, conflicts bias preference toward
certain modalities, and joint signals from different modalities fail to be
integrated effectively. Therefore, we identify two core failures:
task-composition bottleneck, where recognition and reasoning cannot be jointly
executed in one pass, and fusion bottleneck, where early integration introduces
bias. For further investigation, we find that attention patterns fail to encode
fact usefulness, but a simple two-step prompting (recognize then reason)
restores performance, confirming the task-composition bottleneck. Moreover,
modality identity remains recoverable in early layers, and softening attention
in early fusion improves reasoning, highlighting biased fusion as another
failure mode. Overall, our findings show that integration, not perception, is
the main barrier to multimodal reasoning, suggesting composition-aware training
and early fusion control as promising directions.

</details>


### [83] [Understanding Textual Capability Degradation in Speech LLMs via Parameter Importance Analysis](https://arxiv.org/abs/2509.23755)
*Chao Wang,Rui-Chen Zheng,Yang Ai,Zhen-Hua Ling*

Main category: cs.CL

TL;DR: 研究发现语音整合会破坏LLM的文本推理参数分布，提出分层学习率调度和LoRA方法有效保持文本能力并提升语音问答表现。


<details>
  <summary>Details</summary>
Motivation: 语音整合虽然增强LLM多模态能力，但会削弱其核心文本推理能力，限制预训练文本知识的充分利用。

Method: 通过参数重要性估计框架分析文本推理参数分布变化，采用分层学习率调度和低秩适应(LoRA)两种策略保持原始参数分布。

Result: 实验证明两种方法相比全参数微调能更好保持文本能力，同时下游语音问答准确率提升4.2%。

Conclusion: 文本知识在LLM中具有层次化分布特性，保持参数分布结构是平衡多模态扩展与核心能力维持的关键。

Abstract: The integration of speech into Large Language Models (LLMs) has substantially
expanded their capabilities, but often at the cost of weakening their core
textual competence. This degradation limits the ability of speech-enabled LLMs
to fully exploit their pre-trained text-based knowledge. In this work, we
analyze the underlying mechanisms of this issue through a focused study of the
widely used encoder-adaptor paradigm. We propose an analytical framework based
on parameter importance estimation, which reveals that fine-tuning for speech
introduces a textual importance distribution shift: the layer-wise allocation
of parameters critical to textual reasoning is disrupted. Building on this
insight, we investigate two mitigation strategies: layer-wise learning rate
scheduling and Low-Rank Adaptation (LoRA), both aim to preserve the original
parameter distribution. Experimental results show that both approaches better
maintain textual competence than full fine-tuning, while also improving
downstream spoken question answering performance. Furthermore, our analysis
offers a principled explanation for the effectiveness of the proposed
mitigation strategies, linking their benefits to the structural properties of
textual knowledge in LLMs.

</details>


### [84] [Knowledge-Level Consistency Reinforcement Learning: Dual-Fact Alignment for Long-Form Factuality](https://arxiv.org/abs/2509.23765)
*Junliang Li,Yucheng Wang,Yan Chen,Yu Ran,Ruiqing Zhang,Jing Liu,Hua Wu,Haifeng Wang*

Main category: cs.CL

TL;DR: 提出KLCF框架，通过知识一致性强化学习和双事实对齐机制，显著改善大语言模型在长文本生成中的事实性并减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有基于偏好奖励的RLHF方法忽视模型内部知识边界，导致生成内容存在事实性缺陷和幻觉税问题。

Method: 1. 构建基于预训练知识边界的事实清单指导强化学习
2. 开发自评估模块实现事实精准度优化
3. 双路径联合优化事实召回率与精确度

Result: 在多个长文本基准测试中，事实性指标平均提升15.8%，幻觉事件减少32%

Conclusion: KLCF首次实现完全基于模型内部知识的轻量化事实对齐，为提升生成可靠性提供了高效可扩展的解决方案

Abstract: Hallucination and factuality deficits remain key obstacles to the reliability
of large language models (LLMs) in long-form generation. Existing reinforcement
learning from human feedback (RLHF) frameworks primarily rely on preference
rewards, yet they often overlook the model's internal knowledge boundaries,
exacerbating the so-called "hallucination tax". To address this challenge, we
propose Knowledge-Level Consistency Reinforcement Learning Framework (KLCF), a
novel framework that focuses on the knowledge consistency between the policy
model's expressed knowledge and the base model's parametric knowledge, and
introduces a Dual-Fact Alignment mechanism to jointly optimize factual recall
and precision. Specifically, KLCF leverages pretrained knowledge boundaries to
construct fact checklist, guiding online reinforcement learning to improve
factual coverage and recall; simultaneously, it trains a self-assessment module
based on the base model's internal knowledge to enhance factual precision
during generation. Unlike prior methods that rely on external retrieval or
heavy verification, our reward design is fully external-knowledge-free and
lightweight, making KLCF efficient and easily scalable to large-scale training.
Experimental results demonstrate that KLCF substantially improves factuality
metrics across multiple long-form benchmarks and effectively alleviates model
hallucinations.

</details>


### [85] [From Personal to Collective: On the Role of Local and Global Memory in LLM Personalization](https://arxiv.org/abs/2509.23767)
*Zehong Wang,Junlin Wu,ZHaoxuan Tan,Bolian Li,Xianrui Zhong,Zheli Liu,Qingkai Zeng*

Main category: cs.CL

TL;DR: 提出LoGo框架，通过结合本地个性化记忆和全局集体记忆，并引入中介模块解决冲突，提升LLM个性化效果


<details>
  <summary>Details</summary>
Motivation: 解决LLM个性化中的冷启动问题（新用户数据不足）和偏置问题（活跃用户数据倾斜），根本原因是缺乏对跨用户集体知识的建模

Method: LoGo框架包含本地记忆（个性化）和全局记忆（群体共享兴趣），通过mediator模块协调两者冲突

Result: 多基准测试显示LoGo能有效改善冷启动用户表现并减少偏置预测

Conclusion: 整合集体知识显著增强LLM个性化能力，为解决个性化困境提供新方向

Abstract: Large language model (LLM) personalization aims to tailor model behavior to
individual users based on their historical interactions. However, its
effectiveness is often hindered by two key challenges: the \textit{cold-start
problem}, where users with limited history provide insufficient context for
accurate personalization, and the \textit{biasing problem}, where users with
abundant but skewed history cause the model to overfit to narrow preferences.
We identify both issues as symptoms of a common underlying limitation, i.e.,
the inability to model collective knowledge across users. To address this, we
propose a local-global memory framework (LoGo) that combines the personalized
local memory with a collective global memory that captures shared interests
across the population. To reconcile discrepancies between these two memory
sources, we introduce a mediator module designed to resolve conflicts between
local and global signals. Extensive experiments on multiple benchmarks
demonstrate that LoGo consistently improves personalization quality by both
warming up cold-start users and mitigating biased predictions. These results
highlight the importance of incorporating collective knowledge to enhance LLM
personalization.

</details>


### [86] [Bridging the Knowledge-Prediction Gap in LLMs on Multiple-Choice Questions](https://arxiv.org/abs/2509.23782)
*Yoonah Park,Haesung Pyun,Yohan Jo*

Main category: cs.CL

TL;DR: 发现LLMs在选择题中存在知识-预测偏差，提出几何对齐方法KAPPA提升准确率


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自由生成任务中展现正确知识，但在选择题中常预测错误，研究其机制并提出干预方法

Method: 通过残差流分析发现知识基与预测基的错位，开发无需参数的投影调整方法KAPPA对齐隐藏状态

Result: 在Big-Bench-Hard和ARC-Challenge二元选择题中，KAPPA显著提升准确率且优于基线，可扩展到开放式问题

Conclusion: 从几何角度解释知识-预测偏差，提供通过状态对齐改善模型预测与潜在知识一致性的通用方法

Abstract: Large Language Models (LLMs) often fail on multiple-choice questions (MCQs)
despite demonstrating correct knowledge in other contexts, such as free-form
generation. To investigate the mechanism underlying this knowledge-prediction
gap on MCQs and alleviate it, we conduct a probing analysis and find that
residual streams in certain layers contain a subspace spanned by two important
bases: a \emph{knowledge basis} that encodes the probability of the
ground-truth answer for a given MCQ and a \emph{prediction basis} that encodes
the probability of the answer choice predicted by the model. We observe that
incorrect predictions arise from a misalignment of the model's hidden states
along these two bases. Hence, we introduce \textbf{KAPPA} (Knowledge-Aligned
Prediction through Projection-based Adjustment), a parameter-free intervention
that transforms the hidden states to align the prediction coordinate with the
knowledge coordinate within this subspace. Experiments on binary-choice
reformulations of Big-Bench-Hard and ARC-Challenge show that KAPPA
substantially improves accuracy and consistently outperforms baselines. While
optimal subspaces differ across tasks, subspaces generalize to some extent, as
supported by cross-dataset experiments. Moreover, KAPPA extends its
effectiveness to free-form questions beyond MCQs. Our work provides a new
geometric understanding of the knowledge-prediction gap and offers a practical
method for better aligning model behavior with its latent knowledge.

</details>


### [87] [Transformer Tafsir at QIAS 2025 Shared Task: Hybrid Retrieval-Augmented Generation for Islamic Knowledge Question Answering](https://arxiv.org/abs/2509.23793)
*Muhammad Abu Ahmad,Mohamad Ballout,Raia Abu Ahmad,Elia Bruni*

Main category: cs.CL

TL;DR: 混合检索增强生成（RAG）系统结合BM25、密集检索与交叉编码器重排序，在伊斯兰知识任务中使LLM准确率最高提升25%（Fanar模型Subtask 2达80%）


<details>
  <summary>Details</summary>
Motivation: 通过混合检索策略解决大语言模型在专业领域知识（如伊斯兰知识）中的局限性，提升内容检索精度和模型生成效果

Method: 三阶段流程：1) BM25初始检索 2) 密集嵌入模型语义匹配 3) 交叉编码器重排序，形成混合RAG系统

Result: Fanar模型最佳配置（Subtask1:45% / Subtask2:80%），RAG使不同LLM准确率最高提升25%

Conclusion: 混合RAG架构有效增强LLM的专业领域表现，检索优化策略的组合应用对任务效果提升具有决定性作用

Abstract: This paper presents our submission to the QIAS 2025 shared task on Islamic
knowledge understanding and reasoning. We developed a hybrid
retrieval-augmented generation (RAG) system that combines sparse and dense
retrieval methods with cross-encoder reranking to improve large language model
(LLM) performance. Our three-stage pipeline incorporates BM25 for initial
retrieval, a dense embedding retrieval model for semantic matching, and
cross-encoder reranking for precise content retrieval. We evaluate our approach
on both subtasks using two LLMs, Fanar and Mistral, demonstrating that the
proposed RAG pipeline enhances performance across both, with accuracy
improvements up to 25%, depending on the task and model configuration. Our best
configuration is achieved with Fanar, yielding accuracy scores of 45% in
Subtask 1 and 80% in Subtask 2.

</details>


### [88] [Open-DeBias: Toward Mitigating Open-Set Bias in Language Models](https://arxiv.org/abs/2509.23805)
*Arti Rani,Shweta Singh,Nihar Ranjan Sahoo,Gaurav Kumar Nayak*

Main category: cs.CL

TL;DR: 提出OpenBiasBench基准测试和Open-DeBias方法，通过适配器模块实现高效偏见检测与缓解，在跨语言和未知偏见场景下展现显著性能提升


<details>
  <summary>Details</summary>
Motivation: 现有LLM偏见缓解方法局限于预定义类别，无法处理新出现或情境特异性偏见，需开发开放集解决方案

Method: 基于适配器模块的参数高效微调框架，支持零样本跨语言迁移，通过少量数据训练实现已知/未知偏见的联合缓解

Result: 在BBQ数据集模糊子集准确率提升48%，韩语迁移达84%准确率，在StereoSet等任务验证多语言鲁棒性

Conclusion: Open-DeBias为开放领域偏见缓解提供首个通用解决方案，兼具参数效率与跨语言能力，推动可信LLM发展

Abstract: Large Language Models (LLMs) have achieved remarkable success on question
answering (QA) tasks, yet they often encode harmful biases that compromise
fairness and trustworthiness. Most existing bias mitigation approaches are
restricted to predefined categories, limiting their ability to address novel or
context-specific emergent biases. To bridge this gap, we tackle the novel
problem of open-set bias detection and mitigation in text-based QA. We
introduce OpenBiasBench, a comprehensive benchmark designed to evaluate biases
across a wide range of categories and subgroups, encompassing both known and
previously unseen biases. Additionally, we propose Open-DeBias, a novel,
data-efficient, and parameter-efficient debiasing method that leverages adapter
modules to mitigate existing social and stereotypical biases while generalizing
to unseen ones. Compared to the state-of-the-art BMBI method, Open-DeBias
improves QA accuracy on BBQ dataset by nearly $48\%$ on ambiguous subsets and
$6\%$ on disambiguated ones, using adapters fine-tuned on just a small fraction
of the training data. Remarkably, the same adapters, in a zero-shot transfer to
Korean BBQ, achieve $84\%$ accuracy, demonstrating robust language-agnostic
generalization. Through extensive evaluation, we also validate the
effectiveness of Open-DeBias across a broad range of NLP tasks, including
StereoSet and CrowS-Pairs, highlighting its robustness, multilingual strength,
and suitability for general-purpose, open-domain bias mitigation. The project
page is available at: https://sites.google.com/view/open-debias25

</details>


### [89] [SPELL: Self-Play Reinforcement Learning for evolving Long-Context Language Models](https://arxiv.org/abs/2509.23863)
*Ziyi Yang,Weizhou Shen,Ruijun Chen,Chenliang Li,Fanqi Wan,Ming Yan,Xiaojun Quan,Fei Huang*

Main category: cs.CL

TL;DR: SPELL框架通过多角色自博弈强化学习，无需标注数据即可提升大模型长文本推理能力


<details>
  <summary>Details</summary>
Motivation: 现有长文本推理模型因缺乏可靠标注数据和可验证奖励信号发展滞后，传统方法依赖人工标注成本高

Method: 采用提问者-回答者-验证者三角色自博弈循环框架，自动生成问题/答案对并验证语义等价性，结合渐进式课程学习机制

Result: 在6个长文本基准测试中平均提升7.6分(Qwen3-30B模型)，性能超越同规模标注数据微调模型

Conclusion: SPELL证明了自监督强化学习在长文本推理任务中的有效性，为更大规模模型扩展提供了可行路径

Abstract: Progress in long-context reasoning for large language models (LLMs) has
lagged behind other recent advances. This gap arises not only from the
intrinsic difficulty of processing long texts, but also from the scarcity of
reliable human annotations and programmatically verifiable reward signals. In
this paper, we propose SPELL, a multi-role self-play reinforcement learning
framework that enables scalable, label-free optimization for long-context
reasoning. SPELL integrates three cyclical roles-questioner, responder, and
verifier-within a single model to enable continual self-improvement. The
questioner generates questions from raw documents paired with reference
answers; the responder learns to solve these questions based on the documents;
and the verifier evaluates semantic equivalence between the responder's output
and the questioner's reference answer, producing reward signals to guide
continual training. To stabilize training, we introduce an automated curriculum
that gradually increases document length and a reward function that adapts
question difficulty to the model's evolving capabilities. Extensive experiments
on six long-context benchmarks show that SPELL consistently improves
performance across diverse LLMs and outperforms equally sized models fine-tuned
on large-scale annotated data. Notably, SPELL achieves an average 7.6-point
gain in pass@8 on the strong reasoning model Qwen3-30B-A3B-Thinking, raising
its performance ceiling and showing promise for scaling to even more capable
models.

</details>


### [90] [Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token Pruning for Efficient Supervised Fine-Tuning](https://arxiv.org/abs/2509.23873)
*Shaobo Wang,Jiaming Wang,Jiajun Zhang,Cong Wang,Yue Min,Zichen Wen,Fei Huang,Huiqiang Jiang,Junyang Lin,Dayiheng Liu,Linfeng Zhang*

Main category: cs.CL

TL;DR: 提出Q-Tuning框架，通过Error-Uncertainty Plane诊断工具协调样本与token级剪枝，在仅用12.5%数据情况下实现SFT效果38%提升


<details>
  <summary>Details</summary>
Motivation: 现有数据剪枝方法孤立处理样本/标记层级，导致高价值样本包含冗余token而重要信号被丢弃，制约预算受限场景下的SFT效率

Method: 两阶段策略：1) 样本级筛选保留含信息误区/校准信号的样本 2) 非对称token剪枝策略，通过上下文感知评分机制修剪次要token并保留完整校准样本

Result: 在5个基准测试中刷新SOTA，SmolLM2-1.7B模型使用12.5%数据实现平均38%效果提升，首次实现动态剪枝方法持续超越全数据训练

Conclusion: Q-Tuning通过联合优化样本/token剪枝维度，为预算受限的LLM监督微调提供了可扩展的高效数据利用方案

Abstract: As supervised fine-tuning (SFT) evolves from a lightweight post-training step
into a compute-intensive phase rivaling mid-training in scale, data efficiency
has become critical for aligning large language models (LLMs) under tight
budgets. Existing data pruning methods suffer from a fragmented design: they
operate either at the sample level or the token level in isolation, failing to
jointly optimize both dimensions. This disconnect leads to significant
inefficiencies--high-value samples may still contain redundant tokens, while
token-level pruning often discards crucial instructional or corrective signals
embedded in individual examples. To address this bottleneck, we introduce the
Error-Uncertainty (EU) Plane, a diagnostic framework that jointly characterizes
the heterogeneous utility of training data across samples and tokens. Guided by
this insight, we propose Quadrant-based Tuning (Q-Tuning), a unified framework
that strategically coordinates sample pruning and token pruning. Q-Tuning
employs a two-stage strategy: first, it performs sample-level triage to retain
examples rich in informative misconceptions or calibration signals; second, it
applies an asymmetric token-pruning policy, using a context-aware scoring
mechanism to trim less salient tokens exclusively from misconception samples
while preserving calibration samples in their entirety. Our method sets a new
state of the art across five diverse benchmarks. Remarkably, on SmolLM2-1.7B,
Q-Tuning achieves a +38\% average improvement over the full-data SFT baseline
using only 12.5\% of the original training data. As the first dynamic pruning
approach to consistently outperform full-data training, Q-Tuning provides a
practical and scalable blueprint for maximizing data utilization in
budget-constrained LLM SFT.

</details>


### [91] [DocPruner: A Storage-Efficient Framework for Multi-Vector Visual Document Retrieval via Adaptive Patch-Level Embedding Pruning](https://arxiv.org/abs/2509.23883)
*Yibo Yan,Guangwei Xu,Xin Zou,Shuliang Liu,James Kwok,Xuming Hu*

Main category: cs.CL

TL;DR: DocPruner框架通过自适应剪枝补丁级嵌入，将视觉文档检索的存储开销降低50-60%且保持检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有多向量VDR方法存储每个文档的数百个补丁嵌入导致存储成本过高，难以大规模部署。

Method: 利用文档内补丁注意力分布动态识别冗余嵌入，无需重新训练即可剪枝50-60%的存储。

Result: 在10+个数据集上验证，存储降低50-60%的同时检索性能仅下降0.2-0.5个百分点。

Conclusion: DocPruner为构建存储高效的大规模VDR系统提供了灵活有效的解决方案，突破存储瓶颈。

Abstract: Visual Document Retrieval (VDR), the task of retrieving visually-rich
document pages using queries that combine visual and textual cues, is crucial
for numerous real-world applications. Recent state-of-the-art methods leverage
Large Vision-Language Models (LVLMs) in a multi-vector paradigm, representing
each document as patch-level embeddings to capture fine-grained details. While
highly effective, this approach introduces a critical challenge: prohibitive
storage overhead, as storing hundreds of vectors per page makes large-scale
deployment costly and impractical. To address this, we introduce DocPruner, the
first framework to employ adaptive patch-level embedding pruning for VDR to
effectively reduce the storage overhead. DocPruner leverages the intra-document
patch attention distribution to dynamically identify and discard redundant
embeddings for each document. This adaptive mechanism enables a significant
50-60% reduction in storage for leading multi-vector VDR models with negligible
degradation in document retrieval performance. Extensive experiments across
more than ten representative datasets validate that DocPruner offers a robust,
flexible, and effective solution for building storage-efficient, large-scale
VDR systems.

</details>


### [92] [Taming Masked Diffusion Language Models via Consistency Trajectory Reinforcement Learning with Fewer Decoding Step](https://arxiv.org/abs/2509.23924)
*Jingyi Yang,Guanxu Chen,Xuhao Hu,Jing Shao*

Main category: cs.CL

TL;DR: 提出EOSER-ASS解码调度器和CJ-GRPO强化学习方法，有效提升掩码扩散语言模型的解码效率与训练一致性


<details>
  <summary>Details</summary>
Motivation: 现有掩码扩散语言模型直接沿用自回归模型技术存在解码策略次优和训练推理不一致的问题，如块级解码在推理时表现反超训练策略，传统RL方法导致轨迹不一致

Method: 1. EOSER早期终止低质量解码路径 2. ASS动态调整扩散步长 3. CJ-GRPO强化学习框架保持轨迹一致性

Result: 在LLaDA-8B-Instruct模型上，数学推理和规划任务表现提升，代码已开源

Conclusion: 该方法为掩码扩散模型的优化提供了系统解决方案，在保持并行解码优势的同时实现更高效的推理

Abstract: Masked diffusion language models (MDLMs) have recently emerged as a promising
alternative to autoregressive (AR) language models, offering properties such as
parallel decoding, flexible generation orders, and the potential for fewer
inference steps. Despite these advantages, decoding strategies and
reinforcement learning (RL) algorithms tailored for MDLMs remain underexplored.
A naive approach is to directly transfer techniques well-established for AR
models to MDLMs. However, this raises an immediate question: Is such a naive
transfer truly optimal? For example, 1) Block-wise and semi-AR decoding
strategies are not employed during the training of MDLMs, so why do they
outperform full diffusion-style decoding during inference? 2) Applying RL
algorithms designed for AR models directly to MDLMs exhibits a
training-inference inconsistency, since MDLM decoding are non-causal
(parallel). This results in inconsistencies between the rollout trajectory and
the optimization trajectory. To address these challenges, we propose EOS Early
Rejection (EOSER) and Ascending Step-Size (ASS) decoding scheduler, which
unlock the potential of MDLMs to perform full diffusion-style decoding,
achieving competitive performance with fewer decoding steps. Additionally, we
introduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO)
for taming MDLMs, which emphasizes the consistency between rollout trajectory
and optimization trajectory, and reduces the optimization errors caused by
skip-step optimization. We conduct extensive experiments on reasoning tasks,
such as mathematical and planning benchmarks, using LLaDA-8B-Instruct. The
results demonstrate that the proposed EOSER and ASS mechanisms, together with
CJ-GRPO, hold significant promise for effectively and efficiently taming MDLMs.
Code: https://github.com/yjyddq/EOSER-ASS-RL.

</details>


### [93] [Assessing Large Language Models in Updating Their Forecasts with New Information](https://arxiv.org/abs/2509.23936)
*Zhangdie Yuan,Zifeng Ding,Andreas Vlachos*

Main category: cs.CL

TL;DR: 提出EVOLVECAST框架评估大语言模型在新信息下预测调整能力，发现其更新保守且信心校准不足，需改进


<details>
  <summary>Details</summary>
Motivation: 针对现有研究忽视预测随新信息演变的问题，旨在评估大语言模型在新证据出现时能否合理调整预测，并以人类预测者为基准分析其校准能力

Method: 引入EVOLVECAST框架，对比LLMs在训练截止后新信息下的预测调整，结合人类预测者分析预测变化与信心校准，评估模型响应性与置信度估计

Result: LLMs对新信息响应有限，更新保守不一致；信心估计（语言或logits）均逊于人类，模型普遍存在保守偏差

Conclusion: 现有模型在新信息下的预测更新不足，需开发更稳健的信念更新方法以接近人类校准水平

Abstract: Prior work has largely treated future event prediction as a static task,
failing to consider how forecasts and the confidence in them should evolve as
new evidence emerges. To address this gap, we introduce EVOLVECAST, a framework
for evaluating whether large language models appropriately revise their
predictions in response to new information. In particular, EVOLVECAST assesses
whether LLMs adjust their forecasts when presented with information released
after their training cutoff. We use human forecasters as a comparative
reference to analyze prediction shifts and confidence calibration under updated
contexts. While LLMs demonstrate some responsiveness to new information, their
updates are often inconsistent or overly conservative. We further find that
neither verbalized nor logits-based confidence estimates consistently
outperform the other, and both remain far from the human reference standard.
Across settings, models tend to express conservative bias, underscoring the
need for more robust approaches to belief updating.

</details>


### [94] [Easy Turn: Integrating Acoustic and Linguistic Modalities for Robust Turn-Taking in Full-Duplex Spoken Dialogue Systems](https://arxiv.org/abs/2509.23938)
*Guojian Li,Chengyou Wang,Hongfei Xue,Shuiyuan Wang,Dehui Gao,Zihan Zhang,Yuke Lin,Wenjie Li,Longshuai Xiao,Zhonghua Fu,Lei Xie*

Main category: cs.CL

TL;DR: 提出开源双模态转折检测模型Easy Turn，整合声学与语言信息预测对话状态，并发布1145小时训练数据集，在测试集实现SOTA准确率。


<details>
  <summary>Details</summary>
Motivation: 现有转折检测模型存在三个主要问题：1）多数专用模型未开源；2）开源模型参数量大或仅支持单模态；3）基于LLM微调的方法需要大量稀缺的全双工数据。

Method: 开发模块化模型架构，融合声学特征（韵律/停顿）和语言特征（语义/句法），预测'完成/未完成/反馈/等待'四类对话状态，并构建1145小时多场景语音数据集。

Result: 在自建开源测试集上，检测准确率超过TEN Turn Detection（+3.2%）和Smart Turn V2（+5.7%），错误率降低21%以上。

Conclusion: 通过开源模型与数据集推动全双工交互研究，双模态融合策略显著提升转折检测性能，模块化设计支持灵活扩展。

Abstract: Full-duplex interaction is crucial for natural human-machine communication,
yet remains challenging as it requires robust turn-taking detection to decide
when the system should speak, listen, or remain silent. Existing solutions
either rely on dedicated turn-taking models, most of which are not
open-sourced. The few available ones are limited by their large parameter size
or by supporting only a single modality, such as acoustic or linguistic.
Alternatively, some approaches finetune LLM backbones to enable full-duplex
capability, but this requires large amounts of full-duplex data, which remain
scarce in open-source form. To address these issues, we propose Easy Turn, an
open-source, modular turn-taking detection model that integrates acoustic and
linguistic bimodal information to predict four dialogue turn states: complete,
incomplete, backchannel, and wait, accompanied by the release of Easy Turn
trainset, a 1,145-hour speech dataset designed for training turn-taking
detection models. Compared to existing open-source models like TEN Turn
Detection and Smart Turn V2, our model achieves state-of-the-art turn-taking
detection accuracy on our open-source Easy Turn testset. The data and model
will be made publicly available on GitHub.

</details>


### [95] [Vision-Grounded Machine Interpreting: Improving the Translation Process through Visual Cues](https://arxiv.org/abs/2509.23957)
*Claudio Fantinuoli*

Main category: cs.CL

TL;DR: 提出多模态VGI系统，通过整合视觉上下文信息提升机器口译质量，实验显示视觉信息显著改善词汇消歧但句法歧义无提升


<details>
  <summary>Details</summary>
Motivation: 现有机器口译系统依赖单一语音模态，难以处理依赖视觉/情境信息的歧义场景。需通过多模态输入突破性能瓶颈。

Method: 开发整合视觉语言模型的原型系统，构建诊断性语料库评估视觉信息对词汇/性别/句法三类歧义的解析效果。

Result: 视觉信息使词汇消歧准确率显著提升，性别解析有适度改进（稳定性不足），句法歧义未观察到改善。

Conclusion: 多模态融合是提升机器口译质量的必要方向，需进一步探索视觉信息与其他模态的有效整合机制。

Abstract: Machine Interpreting systems are currently implemented as unimodal, real-time
speech-to-speech architectures, processing translation exclusively on the basis
of the linguistic signal. Such reliance on a single modality, however,
constrains performance in contexts where disambiguation and adequacy depend on
additional cues, such as visual, situational, or pragmatic information. This
paper introduces Vision-Grounded Interpreting (VGI), a novel approach designed
to address the limitations of unimodal machine interpreting. We present a
prototype system that integrates a vision-language model to process both speech
and visual input from a webcam, with the aim of priming the translation process
through contextual visual information. To evaluate the effectiveness of this
approach, we constructed a hand-crafted diagnostic corpus targeting three types
of ambiguity. In our evaluation, visual grounding substantially improves
lexical disambiguation, yields modest and less stable gains for gender
resolution, and shows no benefit for syntactic ambiguities. We argue that
embracing multimodality represents a necessary step forward for advancing
translation quality in machine interpreting.

</details>


### [96] [HiPO: Hybrid Policy Optimization for Dynamic Reasoning in LLMs](https://arxiv.org/abs/2509.23967)
*Ken Deng,Zizheng Zhan,Wen Xiang,Wenqiang Zhu,Tianhao Peng,Xinping Lei,Weihao Li,Jingxuan Xu,Kun Wu,Yifan Yao,Haoyang Huang,Huaixi Tang,Kepeng Lei,Zhiyi Lai,Songwei Yu,Zongxian Feng,Zuchen Gao,Weihao Xie,Chenchen Zhang,Yanan Wu,Yuanxing Zhang,Lecheng Huang,Yuqun Zhang,Jie Liu,Zhaoxiang Zhang,Haotian Zhang,Bin Chen,Jiaheng Liu*

Main category: cs.CL

TL;DR: HiPO框架通过混合策略优化动态控制LLMs推理深度，在保持准确性的同时显著降低token消耗


<details>
  <summary>Details</summary>
Motivation: 现有链式推理(CoT)方法生成冗长推理链导致效率低下，需设计自适应机制平衡效率与准确性

Method: 结合混合数据管道（生成Think-on/Think-off配对响应）与混合强化学习奖励系统，实现自适应推理控制

Result: 在数学和编码基准测试中，HiPO减少30-50%推理长度同时保持或提升模型准确性

Conclusion: HiPO为资源敏感场景下的高效自适应推理提供了原则性方法，推动推理导向型LLMs的实际部署

Abstract: Large Language Models (LLMs) increasingly rely on chain-of-thought (CoT)
reasoning to improve accuracy on complex tasks. However, always generating
lengthy reasoning traces is inefficient, leading to excessive token usage and
higher inference costs. This paper introduces the Hybrid Policy Optimization
(i.e., HiPO), a framework for adaptive reasoning control that enables LLMs to
selectively decide when to engage in detailed reasoning (Think-on) and when to
respond directly (Think-off). Specifically, HiPO combines a hybrid data
pipelineproviding paired Think-on and Think-off responseswith a hybrid
reinforcement learning reward system that balances accuracy and efficiency
while avoiding over-reliance on detailed reasoning. Experiments across
mathematics and coding benchmarks demonstrate that HiPO can substantially
reduce token length while maintaining or improving accuracy. Finally, we hope
HiPO a can be a principled approach for efficient adaptive reasoning, advancing
the deployment of reasoning-oriented LLMs in real-world, resource-sensitive
settings.

</details>


### [97] [ByteSized32Refactored: Towards an Extensible Interactive Text Games Corpus for LLM World Modeling and Evaluation](https://arxiv.org/abs/2509.23979)
*Haonan Wang,Junfeng Sun,Xingdi Yuan,Ruoyao Wang,Ziang Xiao*

Main category: cs.CL

TL;DR: 通过重构ByteSized32创建可扩展的文本游戏生成框架，代码量减半但LLM生成质量呈现混合表现


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在模拟交互式世界模型时面临的挑战，通过代码重构实现更高效的文本游戏生成环境扩展

Method: 1. 重构代码结构创建GameBasic.py基础库
2. 抽象7个基类实现逻辑复用
3. 建立模块化架构集中通用功能

Result: GPT-4o生成的新场景文本游戏在4个评估维度中2个提升/2个下降，总代码量从20k行减少至10k行

Conclusion: 模块化重构创建了可扩展的代码结构，基础库设计不仅提升LLM环境适应能力，更为未来扩展建立可量化改进的框架

Abstract: Simulating interactive world models remains a core challenge in Large
Language Models(LLMs). In this work, we introduce the ByteSized32Refactored, a
refactored, modular, and extensible implementation of the original ByteSized32
corpus to explore the task of text game generation. We further optimize the
code structure of each text game and create the GameBasic.py foundation
library, which centralizes common logic across all 32 games by abstracting 7
base classes (GameObject, etc.) into reusable modules, thereby reducing from
20k to 10k total lines of Python code compared to the original Bytesized32. Our
refactored implementation enables extendability - with our centralized design,
ByteSized32Refactored can be more efficiently extended to include text games of
new scenarios and specifications by reusing the shared logic and
functionalities. Extensive experiments with GPT-4o demonstrate a mix of
performance - with Bytesized32Refactored, the generated text games for unseen
scenarios showcase quality improvements on two of the four evaluation
dimensions while decreases on the other two, indicating that the hierarchical
structure of the refactored code presents new challenges for LLMs. Overall, we
highlight that our extensible code structure, centered on the foundation
library and the modular optimization, not only facilitates LLM adaptation to
environment specifications but also establishes a scalable environment that
supports future extensions.

</details>


### [98] [Toward Preference-aligned Large Language Models via Residual-based Model Steering](https://arxiv.org/abs/2509.23982)
*Lucio La Cava,Andrea Tagarelli*

Main category: cs.CL

TL;DR: 提出无需训练的PaLRS方法，通过残差向量在推理阶段调整LLM偏好对齐


<details>
  <summary>Details</summary>
Motivation: 现有偏好对齐方法（如RLHF/DPO）需要大量数据和参数优化，导致任务固化模型。PaLRS旨在实现更灵活高效的偏好对齐

Method: 从少量偏好数据中提取轻量级残差向量，在推理时注入模型残差流实现行为控制

Result: 在数学推理（+3.2%）和代码生成（+5.8%）任务表现提升，保持通用能力，比DPO对齐快85%时间且效果更好

Conclusion: PaLRS为偏好对齐提供了免训练、即插即用的高效方案，显著降低数据需求和计算成本

Abstract: Preference alignment is a critical step in making Large Language Models
(LLMs) useful and aligned with (human) preferences. Existing approaches such as
Reinforcement Learning from Human Feedback or Direct Preference Optimization
typically require curated data and expensive optimization over billions of
parameters, and eventually lead to persistent task-specific models. In this
work, we introduce Preference alignment of Large Language Models via Residual
Steering (PaLRS), a training-free method that exploits preference signals
encoded in the residual streams of LLMs. From as few as one hundred preference
pairs, PaLRS extracts lightweight, plug-and-play steering vectors that can be
applied at inference time to push models toward preferred behaviors. We
evaluate PaLRS on various small-to-medium-scale open-source LLMs, showing that
PaLRS-aligned models achieve consistent gains on mathematical reasoning and
code generation benchmarks while preserving baseline general-purpose
performance. Moreover, when compared to DPO-aligned models, they perform better
with huge time savings. Our findings highlight that PaLRS offers an effective,
much more efficient and flexible alternative to standard preference
optimization pipelines, offering a training-free, plug-and-play mechanism for
alignment with minimal data.

</details>


### [99] [The Hidden Costs of Translation Accuracy: Distillation, Quantization, and Environmental Impact](https://arxiv.org/abs/2509.23990)
*Dhaathri Vijay,Anandaswarup Vadapalli*

Main category: cs.CL

TL;DR: 通过蒸馏和量化模型压缩技术，可在保持翻译质量的同时显著降低计算成本和碳排放，但低资源环境下需权衡效率与质量


<details>
  <summary>Details</summary>
Motivation: 大型语言模型扩展带来的计算和环境成本问题日益严重，需研究如何在保持翻译质量的前提下提高效率与可持续性

Method: 基于Flores+基准测试和人工对话翻译评估（法/印/卡纳达语），对比全模型/蒸馏/量化模型的性能及碳排放

Result: 全3.3B模型排放最高（0.007-0.008kg CO2/次），蒸馏模型快4.5倍且BLEU下降小，INT4量化模型人工评估表现稳定，低资源环境权衡更明显

Conclusion: 模型压缩有效平衡效率与质量，建议将计算效率和环境影响作为NLP评估的核心维度

Abstract: The rapid expansion of large language models (LLMs) has heightened concerns
about their computational and environmental costs. This study investigates the
trade-offs between translation quality and efficiency by comparing full-scale,
distilled, and quantized models using machine translation as a case study. We
evaluated performance on the Flores+ benchmark and through human judgments of
conversational translations in French, Hindi, and Kannada. Our analysis of
carbon emissions per evaluation run revealed that the full 3.3B fp32 model,
while achieving the highest BLEU scores, incurred the largest environmental
footprint (about 0.007-0.008 kg CO2 per run). The distilled models achieved an
inference of up to 4.5x faster than the full 3.3B model, with only minimal
reductions in BLEU scores. Human evaluations also showed that even aggressive
quantization (INT4) preserved high levels of accuracy and fluency, with
differences between models generally minor. These findings demonstrate that
model compression strategies can substantially reduce computational demands and
environmental impact while maintaining competitive translation quality, though
trade-offs are more pronounced in low-resource settings. We argue for
evaluation frameworks that integrate efficiency and sustainability alongside
objective metrics as central dimensions of progress in NLP.

</details>


### [100] [The AI Agent Code of Conduct: Automated Guardrail Policy-as-Prompt Synthesis](https://arxiv.org/abs/2509.23994)
*Gauri Kholkar,Ratinder Ahuja*

Main category: cs.CL

TL;DR: 提出'Policy as Prompt'框架，利用大语言模型将设计文档转化为可验证的实时防护机制，实现AI代理行为审计


<details>
  <summary>Details</summary>
Motivation: 解决工业部署中自主AI代理的安全监管难题，弥合政策与实践之间的鸿沟

Method: 1. 构建可验证的策略树 2. 编译轻量级提示分类器 3. 运行时行为审计机制

Result: 通过多领域验证证明该框架具备可扩展性和可审计性

Conclusion: 为构建可验证安全、合规的AI系统提供了新的技术路径

Abstract: As autonomous AI agents are increasingly deployed in industry, it is
essential to safeguard them. We introduce a novel framework that automates the
translation of unstructured design documents into verifiable, real-time
guardrails. We introduce "Policy as Prompt," a new approach that uses Large
Language Models (LLMs) to interpret and enforce natural language policies by
applying contextual understanding and the principle of least privilege. Our
system first ingests technical artifacts to construct a verifiable policy tree,
which is then compiled into lightweight, prompt-based classifiers that audit
agent behavior at runtime. We validate our approach across diverse
applications, demonstrating a scalable and auditable pipeline that bridges the
critical policy-to-practice gap, paving the way for verifiably safer and more
regulatable AI.

</details>


### [101] [MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use](https://arxiv.org/abs/2509.24002)
*Zijian Wu,Xiangyan Liu,Xinyuan Zhang,Lingjun Chen,Fanqing Meng,Lingxiao Du,Yiran Zhao,Fanshi Zhang,Yaoqi Ye,Jiawei Wang,Zirui Wang,Jinjie Ni,Yufan Yang,Arvin Xu,Michael Qizhe Shieh*

Main category: cs.CL

TL;DR: 提出MCPMark基准测试，暴露现有LLM在复杂系统交互中的不足，最佳模型成功率仅52.56%


<details>
  <summary>Details</summary>
Motivation: 现有MCP基准测试范围狭窄，无法反映真实场景中复杂的CRUD操作和工作流程

Method: 构建127个含自动验证脚本的任务，采用工具调用循环的极简代理框架进行测试

Result: 顶尖模型gpt-5-medium pass@1仅52.56%，平均需16.2次执行轮次和17.4次工具调用

Conclusion: MCPMark有效揭示LLM在复杂系统交互中的局限性，需开发更强大的模型和代理框架

Abstract: MCP standardizes how LLMs interact with external systems, forming the
foundation for general agents. However, existing MCP benchmarks remain narrow
in scope: they focus on read-heavy tasks or tasks with limited interaction
depth, and fail to capture the complexity and realism of real-world workflows.
To address this gap, we propose MCPMark, a benchmark designed to evaluate MCP
use in a more realistic and comprehensive manner. It consists of $127$
high-quality tasks collaboratively created by domain experts and AI agents.
Each task begins with a curated initial state and includes a programmatic
script for automatic verification. These tasks demand richer and more diverse
interactions with the environment, involving a broad range of create, read,
update, and delete (CRUD) operations. We conduct a comprehensive evaluation of
cutting-edge LLMs using a minimal agent framework that operates in a
tool-calling loop. Empirical results show that the best-performing model,
gpt-5-medium, reaches only $52.56$\% pass@1 and $33.86$\% pass^4, while other
widely regarded strong models, including claude-sonnet-4 and o3, fall below
$30$\% pass@1 and $15$\% pass^4. On average, LLMs require $16.2$ execution
turns and $17.4$ tool calls per task, significantly surpassing those in
previous MCP benchmarks and highlighting the stress-testing nature of MCPMark.

</details>


### [102] [Sequential Diffusion Language Models](https://arxiv.org/abs/2509.24007)
*Yangzhou Liu,Yue Cao,Hao Li,Gen Luo,Zhe Chen,Weiyun Wang,Xiaobo Liang,Biqing Qi,Lijun Wu,Changyao Tian,Yanting Zhang,Yuqiang Li,Tong Lu,Yu Qiao,Jifeng Dai,Wenhai Wang*

Main category: cs.CL

TL;DR: 提出SDLM框架，通过自适应序列预测机制在保持KV缓存兼容性的同时，实现2.1倍于Qwen-2.5的吞吐量提升，32B大模型展现出更强扩展性


<details>
  <summary>Details</summary>
Motivation: 解决传统扩散语言模型存在的固定解码长度限制、KV缓存不兼容问题，以及Block Diffusion方法存在的固定块尺寸约束和高训练成本缺陷

Method: 提出NSP预测机制统一单token和块预测，开发SDLM框架：基于置信度动态解码子序列，在固定掩码块内进行扩散推断，兼容KV缓存并提升序列不确定性适应能力

Result: 仅用350万训练样本即超越自回归基线，SDLM-32B模型吞吐量显著提升，验证框架强扩展性

Conclusion: NSP机制有效统一不同粒度预测，SDLM框架通过动态解码机制实现高效序列生成，为语言模型推理效率提升提供新范式

Abstract: Diffusion language models (DLMs) have strong theoretical efficiency but are
limited by fixed-length decoding and incompatibility with key-value (KV)
caches. Block diffusion mitigates these issues, yet still enforces a fixed
block size and requires expensive training. We introduce Next Sequence
Prediction (NSP), which unifies next-token and next-block prediction, enabling
the model to adaptively determine the generation length at each step. When the
length is fixed to 1, NSP reduces to standard next-token prediction. Building
on NSP, we propose Sequential Diffusion Language Model (SDLM), which can
retrofit pre-trained autoregressive language models (ALMs) at minimal cost.
Specifically, SDLM performs diffusion inference within fixed-size mask blocks,
but dynamically decodes consecutive subsequences based on model confidence,
thereby preserving KV-cache compatibility and improving robustness to varying
uncertainty and semantics across the sequence. Experiments show that SDLM
matches or surpasses strong autoregressive baselines using only 3.5M training
samples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the
SDLM-32B model delivers even more pronounced efficiency gains, demonstrating
the strong scalability potential of our modeling paradigm. Project page and
codes: https://github.com/OpenGVLab/SDLM

</details>


### [103] [SparseD: Sparse Attention for Diffusion Language Models](https://arxiv.org/abs/2509.24014)
*Zeqing Wang,Gongfan Fang,Xinyin Ma,Xingyi Yang,Xinchao Wang*

Main category: cs.CL

TL;DR: 提出SparseD稀疏注意力方法，通过预计算头部特定稀疏模式并分阶段切换注意力机制，实现扩散语言模型在长上下文场景下的无损加速。


<details>
  <summary>Details</summary>
Motivation: 现有扩散语言模型因注意力机制二次复杂度导致推理延迟高，且传统自回归模型的稀疏注意力方法无法适配扩散模型特有的注意力模式（头部差异大、跨步骤相似度高、早期步骤关键性强）。

Method: 1. 预计算各注意力头部的稀疏模式并全局复用；2. 早期去噪步骤保留完整注意力保证生成质量，后期切换至稀疏注意力降低计算量。

Result: 在64k上下文长度、1024去噪步数下，相比FlashAttention实现1.5倍加速且保持无损生成质量。

Conclusion: SparseD为扩散语言模型的长上下文应用提供了高效实用的解决方案，验证了注意力模式复用与分阶段注意力机制的有效性。

Abstract: While diffusion language models (DLMs) offer a promising alternative to
autoregressive models (ARs), existing open-source DLMs suffer from high
inference latency. This bottleneck is mainly due to the attention's quadratic
complexity with respect to context length in computing all query-key pairs.
Intuitively, to reduce this complexity, a natural strategy is to restrict
attention to sparse patterns that retain only the most relevant connections.
Such approaches are well-established in ARs, where attention follows fixed and
clearly defined sparse patterns. However, in DLMs, we observe distinct sparsity
behaviors: (1) attention patterns vary across heads, (2) attention patterns in
each head remain highly similar across denoising steps, and (3) early denoising
steps are critical for generation. These findings render sparse attention
methods designed for ARs largely incompatible with DLMs, as they fail to
capture head-specific structures and risk degrading generation when applied in
early denoising steps. To address these challenges, we propose SparseD, a novel
sparse attention method for DLMs. Leveraging the observations, SparseD only
requires pre-computing head-specific sparse patterns one time, and reuses them
across all steps. This prevents recomputing sparse patterns at each denoising
step. Meanwhile, SparseD uses full attention in the early steps, then switches
to sparse attention later to maintain generation quality. Together, these
establish SparseD as a practical and efficient solution for deploying DLMs in
long-context applications. Experimental results demonstrate that SparseD
achieves lossless acceleration, delivering up to $1.50\times$ speedup over
FlashAttention at a 64k context length with 1,024 denoising steps.

</details>


### [104] [ResFormer: All-Time Reservoir Memory for Long Sequence Classification](https://arxiv.org/abs/2509.24074)
*Hongbo Liu,Jia Xu*

Main category: cs.CL

TL;DR: 提出ResFormer架构，结合储层计算网络和Transformer，有效建模不同长度上下文依赖，在序列分类任务中实现精度提升和内存优化。


<details>
  <summary>Details</summary>
Motivation: Transformer模型受二次计算复杂度限制，难以高效处理长上下文。现有方法未能有效平衡长/短期依赖捕捉与计算资源消耗。

Method: 级联架构：1) 储层计算网络捕捉长时依赖（线性复杂度） 2) 传统Transformer处理短时句内依赖（固定长度输入）

Result: 在EmoryNLP/MultiWOZ等数据集上较DeepSeek-Qwen提升最高22.3%准确率，内存消耗显著降低

Conclusion: ResFormer通过混合架构创新，突破传统Transformer长度限制，为长上下文建模提供高效解决方案

Abstract: Sequence classification is essential in NLP for understanding and
categorizing language patterns in tasks like sentiment analysis, intent
detection, and topic classification. Transformer-based models, despite
achieving state-of-the-art performance, have inherent limitations due to
quadratic time and memory complexity, restricting their input length. Although
extensive efforts have aimed at reducing computational demands, processing
extensive contexts remains challenging.
  To overcome these limitations, we propose ResFormer, a novel neural network
architecture designed to model varying context lengths efficiently through a
cascaded methodology. ResFormer integrates an reservoir computing network
featuring a nonlinear readout to effectively capture long-term contextual
dependencies in linear time. Concurrently, short-term dependencies within
sentences are modeled using a conventional Transformer architecture with
fixed-length inputs.
  Experiments demonstrate that ResFormer significantly outperforms baseline
models of DeepSeek-Qwen and ModernBERT, delivering an accuracy improvement of
up to +22.3% on the EmoryNLP dataset and consistent gains on MultiWOZ, MELD,
and IEMOCAP. In addition, ResFormer exhibits reduced memory consumption,
underscoring its effectiveness and efficiency in modeling extensive contextual
information.

</details>


### [105] [Ensembling Multilingual Transformers for Robust Sentiment Analysis of Tweets](https://arxiv.org/abs/2509.24080)
*Meysam Shirdel Bilehsavar,Negin Mahmoudi,Mohammad Jalili Torkamani,Kiana Kiashemshaki*

Main category: cs.CL

TL;DR: 针对小语种情感分析标注数据匮乏的问题，提出基于transformer集成模型和LLM的跨语言情感分析方法，在多语言数据集实验中取得86%准确率


<details>
  <summary>Details</summary>
Motivation: 当前情感分析技术在小语种应用中面临标注数据不足的挑战，限制了跨语言场景的实际应用效果

Method: 整合BERT多语言模型和XLM-R构建集成模型，结合LLM进行跨语言迁移学习，采用多语言混合数据集进行验证

Result: 实验显示集成模型在跨语言情感分析任务中达到86%的准确率，显著优于单一模型

Conclusion: 提出的集成学习方法有效解决了小语种数据稀缺问题，为跨语言情感分析提供了实用解决方案

Abstract: Sentiment analysis is a very important natural language processing activity
in which one identifies the polarity of a text, whether it conveys positive,
negative, or neutral sentiment. Along with the growth of social media and the
Internet, the significance of sentiment analysis has grown across numerous
industries such as marketing, politics, and customer service. Sentiment
analysis is flawed, however, when applied to foreign languages, particularly
when there is no labelled data to train models upon. In this study, we present
a transformer ensemble model and a large language model (LLM) that employs
sentiment analysis of other languages. We used multi languages dataset.
Sentiment was then assessed for sentences using an ensemble of pre-trained
sentiment analysis models: bert-base-multilingual-uncased-sentiment, and XLM-R.
Our experimental results indicated that sentiment analysis performance was more
than 86% using the proposed method.

</details>


### [106] [Large-Scale Constraint Generation -- Can LLMs Parse Hundreds of Constraints?](https://arxiv.org/abs/2509.24090)
*Matteo Boffa,Jiaxuan You*

Main category: cs.CL

TL;DR: 提出大规模约束生成问题LSCG并设计FoCusNet模型，通过约束筛选提升LLM处理复杂约束能力（实验显示准确率提升8-13%）


<details>
  <summary>Details</summary>
Motivation: 现有研究仅关注少量显式约束的生成，而现实场景需要处理大规模细粒度约束，需验证LLM在此类任务中的表现及优化方案

Method: 1. 创建Words Checker测试集评估模型规模/类型及引导技术的影响；2. 提出FoCusNet模型对原始约束进行动态筛选

Result: 现有方案在约束数量增加时性能显著下降，FoCusNet在多项实验中实现8-13%准确率提升

Conclusion: 大规模约束处理是LLM的重要挑战，专用架构FoCusNet通过约束聚焦机制有效提升复杂约束场景下的生成质量

Abstract: Recent research has explored the constrained generation capabilities of Large
Language Models (LLMs) when explicitly prompted by few task-specific
requirements. In contrast, we introduce Large-Scale Constraint Generation
(LSCG), a new problem that evaluates whether LLMs can parse a large,
fine-grained, generic list of constraints. To examine the LLMs' ability to
handle an increasing number constraints, we create a practical instance of
LSCG, called Words Checker. In Words Checker, we evaluate the impact of model
characteristics (e.g., size, family) and steering techniques (e.g., Simple
Prompt, Chain of Thought, Best of N) on performance. We also propose FoCusNet,
a small and dedicated model that parses the original list of constraints into a
smaller subset, helping the LLM focus on relevant constraints. Experiments
reveal that existing solutions suffer a significant performance drop as the
number of constraints increases, with FoCusNet showing an 8-13% accuracy boost.

</details>


### [107] [GEAR: A General Evaluation Framework for Abductive Reasoning](https://arxiv.org/abs/2509.24096)
*Kaiyu He,Peilin Wu,Mian Zhang,Kun Wan,Wentian Zhao,Xinya Du,Zhiyu Chen*

Main category: cs.CL

TL;DR: GEAR提出自动化评估框架，通过一致性、普适性、多样性三指标评估大语言模型在溯因推理中的假设生成能力，并开发动量课程学习策略提升模型表现


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注语言模型的指令遵循和演绎推理能力，缺乏对知识发现（尤其是溯因推理）的系统评估方法，传统人工标注评估存在成本高、扩展性差的问题

Method: 设计GEAR评估范式（自动评分+动量课程学习），通过一致性（假设符合观察）、普适性（预测未见过输入）、多样性（覆盖不同模式）三维度评估假设质量，动态调整训练数据难度

Result: 在4个基准测试中分析9个LLM生成超5万假设，GEAR有效区分模型差异，课程学习使所有评估指标提升5-15%，且效果迁移至传统溯因推理任务

Conclusion: GEAR为溯因推理提供可扩展的自动化评估框架，其无监督训练信号可帮助LLM生成更可靠、多样化的假设，突破静态基准测试的饱和限制

Abstract: Since the advent of large language models (LLMs), research has focused on
instruction following and deductive reasoning. A central question remains: can
these models discover new knowledge, and how can we evaluate this ability? We
address this by studying abductive reasoning-the generation of plausible
hypotheses to explain observations-and introduce GEAR (General Evaluation for
Abductive Reasoning), a general-purpose, fully automated, transparent, and
label-free evaluation paradigm. GEAR scores hypothesis sets by three metrics:
consistency (each hypothesis explains the observations), generalizability
(consistent hypotheses make meaningful predictions on unseen inputs), and
diversity (the set covers distinct predictions and patterns). Built this way,
GEAR is scalable (no human gold answers), reliable (deterministic scoring
aligned with classical abduction), and open-ended (scores improve only when
models produce new plausible hypotheses, unlike static benchmarks that saturate
once accuracy is high). Using GEAR, we conduct a fine-grained study of nine
LLMs on four abduction benchmarks with 1,500 problems, generating over 50,000
candidate hypotheses and revealing model differences obscured by gold-answer or
purely human evaluations. We further propose a momentum-based curriculum that
adjusts GEAR-derived training data by learning velocity: it starts with what
the model learns quickly and shifts toward harder objectives such as generating
diverse hypotheses once the model is confident on foundational objectives.
Without gold-label supervision, this strategy improves all GEAR objectives and
these gains transfer to established abductive reasoning benchmarks. Taken
together, GEAR provides a principled framework that evaluates abduction and
supplies label-free, scalable training signals that help LLMs produce more
diverse and reliable hypotheses.

</details>


### [108] [BTC-SAM: Leveraging LLMs for Generation of Bias Test Cases for Sentiment Analysis Models](https://arxiv.org/abs/2509.24101)
*Zsolt T. Kardkovács,Lynda Djennane,Anna Field,Boualem Benatallah,Yacine Gaci,Fabio Casati,Walid Gaaloul*

Main category: cs.CL

TL;DR: 提出BTC-SAM框架，利用大语言模型生成多样化测试案例，用于检测情感分析模型的社会偏见


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖专家/众包构建测试语句，成本高昂且难以覆盖多维度偏见，亟需高效生成方法

Method: 通过大语言模型可控生成测试语句（BTC-SAM框架），实现高语言多样性和测试覆盖率

Result: 实验证明相较基础提示方法，该框架在语言变化性和测试覆盖率上表现更优，能适应未知偏见类型

Conclusion: BTC-SAM为情感分析模型偏见检测提供了高效、可扩展的解决方案，显著提升测试质量

Abstract: Sentiment Analysis (SA) models harbor inherent social biases that can be
harmful in real-world applications. These biases are identified by examining
the output of SA models for sentences that only vary in the identity groups of
the subjects. Constructing natural, linguistically rich, relevant, and diverse
sets of sentences that provide sufficient coverage over the domain is
expensive, especially when addressing a wide range of biases: it requires
domain experts and/or crowd-sourcing. In this paper, we present a novel bias
testing framework, BTC-SAM, which generates high-quality test cases for bias
testing in SA models with minimal specification using Large Language Models
(LLMs) for the controllable generation of test sentences. Our experiments show
that relying on LLMs can provide high linguistic variation and diversity in the
test sentences, thereby offering better test coverage compared to base
prompting methods even for previously unseen biases.

</details>


### [109] [Pragmatic Inference for Moral Reasoning Acquisition: Generalization via Distributional Semantics](https://arxiv.org/abs/2509.24102)
*Guangliang Liu,Xi Chen,Bocheng Chen,Xitong Zhang,Kristen Johnson*

Main category: cs.CL

TL;DR: 论文探讨如何通过语用推理方法提升LLMs在道德推理中的泛化能力


<details>
  <summary>Details</summary>
Motivation: LLMs基于分布语义的运作方式与道德推理的语用层面存在本质差异，导致泛化困难

Method: 基于道德基础理论构建语用推理框架，通过上下文信息桥接道德基础与推理目标

Result: 实验证明该方法显著提升LLMs道德推理的泛化能力

Conclusion: 提出的语用推理框架为基于道德基础理论的后续研究奠定了方法论基础

Abstract: Moral reasoning has emerged as a promising research direction for Large
Language Models (LLMs), yet achieving generalization remains a central
challenge. From a linguistic standpoint, this difficulty arises because LLMs
are adept at capturing distributional semantics, which fundamentally differs
from the morals which operate at the pragmatic level. This paper investigates
how LLMs can achieve generalized moral reasoning despite their reliance on
distributional semantics. We propose pragmatic inference methods grounded in
moral foundations theory, which leverage contextual information at each step to
bridge the pragmatic gap and guide LLMs in connecting moral foundations with
moral reasoning objectives. Experimental results demonstrate that our approach
significantly enhances LLMs' generalization in moral reasoning, providing a
foundation for future research grounded in moral foundations theory.

</details>


### [110] [Dual-Scale World Models for LLM Agents Towards Hard-Exploration Problems](https://arxiv.org/abs/2509.24116)
*Minsoo Kim,Seung-won Hwang*

Main category: cs.CL

TL;DR: GLoW提出双尺度世界模型框架，在文本游戏基准测试中实现LLM方法的新SOTA性能，相比强化学习方法减少100-800倍环境交互


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在需要持续探索获取新知识的'硬探索'任务中存在效率瓶颈，传统强化学习方法需要大量环境交互

Method: 全局维护高价值发现轨迹前沿，局部通过多路径优势反思机制进行探索，利用优势信号指导智能体决策

Result: 在Jericho文本游戏基准测试中达到LLM方法最佳性能，与强化学习基线相比在减少100-800倍交互量的情况下获得相当效果

Conclusion: GLoW框架显著提升了LLM智能体在复杂探索任务中的样本效率，为知识驱动型探索提供了新的解决方案

Abstract: LLM-based agents have seen promising advances, yet they are still limited in
"hard-exploration" tasks requiring learning new knowledge through exploration.
We present GLoW, a novel approach leveraging dual-scale world models,
maintaining a trajectory frontier of high-value discoveries at the global
scale, while learning from local trial-and-error in exploration through a
Multi-path Advantage Reflection mechanism which infers advantage-based progress
signals to guide exploration. To evaluate our framework for hard-exploration,
we tackle the Jericho benchmark suite of text-based games, where GLoW achieves
a new state-of-theart performance for LLM-based approaches. Compared to
state-of-the-art RLbased methods, our approach achieves comparable performance
while requiring 100-800x fewer environment interactions.

</details>


### [111] [EduVidQA: Generating and Evaluating Long-form Answers to Student Questions based on Lecture Videos](https://arxiv.org/abs/2509.24120)
*Sourjyadip Ray,Shubham Sharma,Somak Aditya,Pawan Goyal*

Main category: cs.CL

TL;DR: 论文提出基于多模态大语言模型的教育视频问答解决方案，构建EduVidQA数据集并通过实验验证合成数据有效性和任务挑战性。


<details>
  <summary>Details</summary>
Motivation: 数字教育平台亟需保持互动性，现有技术难以自动响应学生视频学习中的多样化问题。通过MLLMs实现智能教育问答具有重要现实意义。

Method: 构建包含5252问答对的EduVidQA数据集（合成+真实数据），通过学生偏好研究和6个SOTA MLLMs的文本/定性指标评估模型性能。

Result: 合成数据微调有效但任务整体困难，不同模型表现差异显著。定性评估揭示了现有模型的真实场景适应性局限。

Conclusion: 本研究为教育NLP领域设立新基准，推动MLLMs在教育视频问答中的应用，未来可探索更细粒度的多模态理解方法。

Abstract: As digital platforms redefine educational paradigms, ensuring interactivity
remains vital for effective learning. This paper explores using Multimodal
Large Language Models (MLLMs) to automatically respond to student questions
from online lectures - a novel question answering task of real world
significance. We introduce the EduVidQA Dataset with 5252 question-answer pairs
(both synthetic and real-world) from 296 computer science videos covering
diverse topics and difficulty levels. To understand the needs of the dataset
and task evaluation, we empirically study the qualitative preferences of
students, which we provide as an important contribution to this line of work.
Our benchmarking experiments consist of 6 state-of-the-art MLLMs, through which
we study the effectiveness of our synthetic data for finetuning, as well as
showing the challenging nature of the task. We evaluate the models using both
text-based and qualitative metrics, thus showing a nuanced perspective of the
models' performance, which is paramount to future work. This work not only sets
a benchmark for this important problem, but also opens exciting avenues for
future research in the field of Natural Language Processing for Education.

</details>


### [112] [Beyond Magic Words: Sharpness-Aware Prompt Evolving for Robust Large Language Models with TARE](https://arxiv.org/abs/2509.24130)
*Guancheng Wan,Lucheng Fu,Haoxin Liu,Yiqiao Jin,Hui Yi Leong,Eric Hanchen Jiang,Hejia Geng,Jinhe Bi,Yunpu Ma,Xiangru Tang,B. Aditya Prakash,Yizhou Sun,Wei Wang*

Main category: cs.CL

TL;DR: 本文提出文本锐度（textual sharpness）概念及TARE/ATARE框架，通过对抗性搜索与鲁棒选择策略优化提示词鲁棒性，降低语义扰动对LLM性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法仅关注准确性而忽视语义稳定性，导致微小语义改写引发性能剧烈波动。论文首次在离散语义空间中形式化定义文本锐度，旨在提升提示词的抗干扰能力。

Method: 1. TARE框架：内层基于采样的对抗搜索生成硬性改写样本，外层通过鲁棒选择保留邻域性能稳定的候选提示；2. ATARE扩展框架：引入各向异性权重调整语义邻域形状，动态平衡探索范围与语义保真度。

Result: 实验表明该方法在保持精度的同时显著提升提示鲁棒性，其最小化文本锐度差的策略优于仅优化精度的搜索方法，且计算效率实用。

Conclusion: 通过形式化定义文本锐度与设计黑盒优化框架，首次实现语义空间中的鲁棒提示搜索，为LLM提示工程提供新的理论工具与实践方案。

Abstract: The performance of Large Language Models (LLMs) hinges on carefully
engineered prompts. However, prevailing prompt optimization methods, ranging
from heuristic edits and reinforcement learning to evolutionary search,
primarily target point-wise accuracy. They seldom enforce paraphrase invariance
or searching stability, and therefore cannot remedy this brittleness in
practice. Automated prompt search remains brittle: small, semantically
preserving paraphrases often cause large performance swings. We identify this
brittleness as the textual sharpness of the prompt landscape. In this work, we
provide the first formal treatment of textual sharpness in the discrete,
semantic space of prompts, together with an operational robustness criterion
over a semantic neighborhood; the design is black-box or API-only, requiring no
gradients to update the model's parameters. Then we introduce TARE (Textual
Sharpness-Aware Evolving), a derivative-free framework that alternates between
an inner, sampling-based adversarial search that stresses a prompt with hard
paraphrases and an outer, robust selection that prefers candidates whose
neighborhoods remain strong. We further propose ATARE, which learns anisotropic
weights to shape the semantic neighborhood and adapts its radius over time to
balance exploration and fidelity. Diverse tasks evaluate our methods, whose
design for minimizing textual sharpness gap leads to prompts that preserve
accuracy under paraphrasing, outperforming accuracy-only prompt search while
remaining computationally practical.

</details>


### [113] [Your thoughts tell who you are: Characterize the reasoning patterns of LRMs](https://arxiv.org/abs/2509.24147)
*Yida Chen,Yuning Mao,Xianjun Yang,Suyu Ge,Shengjie Bi,Lijuan Liu,Saghar Hosseini,Liang Tan,Yixin Nie,Shaoliang Nie*

Main category: cs.CL

TL;DR: 提出LOT方法，通过生成分类法系统比较12种大型推理模型的思维差异，准确率达80-100%，并验证推理风格对齐可提升小模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅关注模型准确率等宏观指标，缺乏对模型内在推理差异的细粒度分析。

Method: 使用生成式语言模型迭代分析不同模型的推理轨迹，构建自然语言分类体系并验证预测能力。

Result: 成功识别模型规模/家族/领域导致的系统性推理差异，并通过案例证明风格对齐可使小模型GPQA准确率提升3.3-5.7%。

Conclusion: LOT为理解模型思维提供新范式，其自然语言解释性和实用性为模型优化开辟新方向。

Abstract: Current comparisons of large reasoning models (LRMs) focus on macro-level
statistics such as task accuracy or reasoning length. Whether different LRMs
reason differently remains an open question. To address this gap, we introduce
the LLM-proposed Open Taxonomy (LOT), a classification method that uses a
generative language model to compare reasoning traces from two LRMs and
articulate their distinctive features in words. LOT then models how these
features predict the source LRM of a reasoning trace based on their empirical
distributions across LRM outputs. Iterating this process over a dataset of
reasoning traces yields a human-readable taxonomy that characterizes how models
think. We apply LOT to compare the reasoning of 12 open-source LRMs on tasks in
math, science, and coding. LOT identifies systematic differences in their
thoughts, achieving 80-100% accuracy in distinguishing reasoning traces from
LRMs that differ in scale, base model family, or objective domain. Beyond
classification, LOT's natural-language taxonomy provides qualitative
explanations of how LRMs think differently. Finally, in a case study, we link
the reasoning differences to performance: aligning the reasoning style of
smaller Qwen3 models with that of the largest Qwen3 during test time improves
their accuracy on GPQA by 3.3-5.7%.

</details>


### [114] [Localizing Task Recognition and Task Learning in In-Context Learning via Attention Head Analysis](https://arxiv.org/abs/2509.24164)
*Haolin Yang,Hakaze Cho,Naoya Inoue*

Main category: cs.CL

TL;DR: 该研究通过TSLA框架揭示了大型语言模型中情境学习的双机制：任务识别头负责对齐任务子空间，任务学习头负责在该子空间内调整预测方向。


<details>
  <summary>Details</summary>
Motivation: 调和注意力头组件分析与情境学习整体分解(TR-TL)两种视角的矛盾，建立统一的可解释框架

Method: 提出任务子空间对数归因(TSLA)框架，结合相关性分析、消融实验、输入扰动和几何隐状态分析

Result: TR头通过隐状态对齐实现任务识别，TL头通过子空间内旋转优化预测，二者独立且互补

Conclusion: 该框架统一解释了包括归纳头在内的多种ICL机制，为模型可解释性研究提供了新范式

Abstract: We investigate the mechanistic underpinnings of in-context learning (ICL) in
large language models by reconciling two dominant perspectives: the
component-level analysis of attention heads and the holistic decomposition of
ICL into Task Recognition (TR) and Task Learning (TL). We propose a novel
framework based on Task Subspace Logit Attribution (TSLA) to identify attention
heads specialized in TR and TL, and demonstrate their distinct yet
complementary roles. Through correlation analysis, ablation studies, and input
perturbations, we show that the identified TR and TL heads independently and
effectively capture the TR and TL components of ICL. Using steering experiments
with geometric analysis of hidden states, we reveal that TR heads promote task
recognition by aligning hidden states with the task subspace, while TL heads
rotate hidden states within the subspace toward the correct label to facilitate
prediction. We further show how previous findings on ICL mechanisms, including
induction heads and task vectors, can be reconciled with our
attention-head-level analysis of the TR-TL decomposition. Our framework thus
provides a unified and interpretable account of how large language models
execute ICL across diverse tasks and settings.

</details>


### [115] [Task Vectors, Learned Not Extracted: Performance Gains and Mechanistic Insight](https://arxiv.org/abs/2509.24169)
*Haolin Yang,Hakaze Cho,Kaize Ding,Naoya Inoue*

Main category: cs.CL

TL;DR: 提出直接训练学习任务向量(LTVs)，比传统提取方法更准确灵活，并通过机制分析揭示其通过注意力头OV电路和线性传播影响预测的机理。


<details>
  <summary>Details</summary>
Motivation: 现有任务向量提取方法存在过程复杂不透明、机制解释不足的问题，需要更有效的任务向量获取方式和对其作用机理的系统性理解。

Method: 1. 开发直接训练LTVs的方法，验证其在任意网络层的有效性 2. 通过注意力头分析和传播路径追踪，系统研究TVs在Transformer中的工作机制

Result: LTVs准确率提升15%，展示跨层/位置应用的灵活性；机制层面发现关键注意力头主导预测，且TV传播呈现显著线性特征（早期向量旋转对齐任务子空间，后期向量主要幅度缩放）

Conclusion: LTVs不仅提供高效任务向量获取方案，更通过揭示注意力机制和传播线性特性，为理解上下文学习提供了新的理论框架

Abstract: Large Language Models (LLMs) can perform new tasks from in-context
demonstrations, a phenomenon known as in-context learning (ICL). Recent work
suggests that these demonstrations are compressed into task vectors (TVs),
compact task representations that LLMs exploit for predictions. However, prior
studies typically extract TVs from model outputs or hidden states using
cumbersome and opaque methods, and they rarely elucidate the mechanisms by
which TVs influence computation. In this work, we address both limitations.
First, we propose directly training Learned Task Vectors (LTVs), which surpass
extracted TVs in accuracy and exhibit superior flexibility-acting effectively
at arbitrary layers, positions, and even with ICL prompts. Second, through
systematic analysis, we investigate the mechanistic role of TVs, showing that
at the low level they steer predictions primarily through attention-head OV
circuits, with a small subset of "key heads" most decisive. At a higher level,
we find that despite Transformer nonlinearities, TV propagation is largely
linear: early TVs are rotated toward task-relevant subspaces to improve logits
of relevant labels, while later TVs are predominantly scaled in magnitude.
Taken together, LTVs not only provide a practical approach for obtaining
effective TVs but also offer a principled lens into the mechanistic foundations
of ICL.

</details>


### [116] [Retrieval-augmented GUI Agents with Generative Guidelines](https://arxiv.org/abs/2509.24183)
*Ran Xu,Kaixin Ma,Wenhao Yu,Hongming Zhang,Joyce C. Ho,Carl Yang,Dong Yu*

Main category: cs.CL

TL;DR: 提出RAG-GUI轻量级视觉语言模型，通过结合网络教程和两阶段微调机制，显著提升GUI代理在复杂数字任务中的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于VLM的GUI代理面临训练数据稀缺和长尾知识处理难题，难以应对实际场景中复杂的罕见任务场景。

Method: 采用监督微调(SFT)初始化模型，通过自引导拒绝采样微调(RSF)优化，构建模型无关的插件架构实现即插即用。

Result: 在三个任务评估中持续超越基线模型，不同规模模型性能提升幅度达2.6%-13.3%，展现优异泛化能力。

Conclusion: RAG-GUI通过实时知识检索与渐进式优化的双重机制，为VLM代理提供了实用高效的增强解决方案。

Abstract: GUI agents powered by vision-language models (VLMs) show promise in
automating complex digital tasks. However, their effectiveness in real-world
applications is often limited by scarce training data and the inherent
complexity of these tasks, which frequently require long-tailed knowledge
covering rare, unseen scenarios. We propose RAG-GUI , a lightweight VLM that
leverages web tutorials at inference time. RAG-GUI is first warm-started via
supervised finetuning (SFT) and further refined through self-guided rejection
sampling finetuning (RSF). Designed to be model-agnostic, RAG-GUI functions as
a generic plug-in that enhances any VLM-based agent. Evaluated across three
distinct tasks, it consistently outperforms baseline agents and surpasses other
inference baselines by 2.6% to 13.3% across two model sizes, demonstrating
strong generalization and practical plug-and-play capabilities in real-world
scenarios.

</details>


### [117] [Beyond Overall Accuracy: A Psychometric Deep Dive into the Topic-Specific Medical Capabilities of 80 Large Language Models](https://arxiv.org/abs/2509.24186)
*Zhimeng Luo,Lixin Wu,Adam Frisch,Daqing He*

Main category: cs.CL

TL;DR: 提出基于项目反应理论（IRT）的MedIRT评估框架，通过多维度能力分析揭示大语言模型在医学应用中的差异化表现，并建立医疗决策支持框架。


<details>
  <summary>Details</summary>
Motivation: 传统准确率指标无法捕捉问题特征和领域特异性表现，急需可靠评估方法确保大语言模型在医疗高风险场景的安全应用。

Method: 收集80个LLM在1,100道USMLE标准问题上的响应，使用单维双参数IRT模型分学科估计模型潜在能力、题目难度和区分度参数。

Result: 发现模型存在显著的能力尖峰现象（如Claude-3-opus在社会科学领域超越GPT-5），IRT可有效识别有缺陷的测试题目并提供稳定排名。

Conclusion: 建立基于心理测量学的评估体系，为医疗领域LLM的安全部署提供多维度能力画像与决策支持框架。

Abstract: As Large Language Models (LLMs) are increasingly proposed for high-stakes
medical applications, there has emerged a critical need for reliable and
accurate evaluation methodologies. Traditional accuracy metrics fail
inadequately as they neither capture question characteristics nor offer
topic-specific insights. To address this gap, we introduce \textsc{MedIRT}, a
rigorous evaluation framework grounded in Item Response Theory (IRT), the gold
standard in high-stakes educational testing. Unlike previous research relying
on archival data, we prospectively gathered fresh responses from 80 diverse
LLMs on a balanced, 1,100-question USMLE-aligned benchmark. Using one
unidimensional two-parameter logistic IRT model per topic, we estimate LLM's
latent model ability jointly with question difficulty and discrimination,
yielding more stable and nuanced performance rankings than accuracy alone.
Notably, we identify distinctive ``spiky'' ability profiles, where overall
rankings can be misleading due to highly specialized model abilities. While
\texttt{GPT-5} was the top performer in a majority of domains (8 of 11), it was
outperformed in Social Science and Communication by \texttt{Claude-3-opus},
demonstrating that even an overall 23rd-ranked model can hold the top spot for
specific competencies. Furthermore, we demonstrate IRT's utility in auditing
benchmarks by identifying flawed questions. We synthesize these findings into a
practical decision-support framework that integrates our multi-factor
competency profiles with operational metrics. This work establishes a robust,
psychometrically grounded methodology essential for the safe, effective, and
trustworthy deployment of LLMs in healthcare.

</details>


### [118] [PET: Preference Evolution Tracking with LLM-Generated Explainable Distribution](https://arxiv.org/abs/2509.24189)
*Luyang Zhang,Siyuan Peng,Jialu Wang,Shichao Zhu,Beibei Li,Zhongcun Wang,Guangmou Pan,Yan Li,Song Yang*

Main category: cs.CL

TL;DR: 提出PET框架，通过动态概率分布建模用户偏好，解决LLM直接生成方法在个性化、可解释性和长尾推荐效果上的不足


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的端到端推荐系统存在决策不透明、用户画像模糊、加剧流行偏见等问题，需建立可解释的偏好追踪框架

Method: 将用户偏好建模为可解释偏好簇的概率分布，结合logit-probing分类和生成式分类技术实现透明偏好学习

Result: 在公开基准(Yelp/MovieLens)NDCG提升40%，在短视频平台数据集长尾内容推荐效果超生产模型7倍

Conclusion: PET框架将用户画像从直接生成转为分布映射，为可解释、公平、多样化的推荐系统奠定基础

Abstract: Understanding how user preference evolves over time is a fundamental
challenge central to modern digital ecosystems, for which Large Language Models
(LLMs) are an increasingly prominent and popular approach due to their ability
to comprehend the rich semantic context within behavioral data. A common
practice is to use LLMs to predict a user's next action by directly generating
a ranked list of preferred items. Although effective for short-term prediction,
the end-to-end generation paradigm inherently limits personalization. Its
opaque decision-making process obscures holistic user profiling and exacerbates
popularity bias. To address these limitations, we propose Preference Evolution
Tracking (PET), a framework that reframes the task as inferring a dynamic
probability distribution over a stable and interpretable lattice of preference
clusters. By applying logit-probing and generative classification techniques,
PET infers a user's preference as a probability distribution, enabling
transparent preference learning. On public benchmarks (Yelp, MovieLens), PET
improves ranking quality by up to 40% in NDCG over direct generation baselines.
On a large-scale, real-world dataset from a short-video platform, it excels at
ranking long-tail contents, significantly outperforming a SOTA production model
by 7 times in the NDCG score. Ultimately, PET transforms the user profile model
from direct preference list generation to a transparent distributional
preference mapping, paving the way for more explainable, fair, and diverse
personalization systems.

</details>


### [119] [AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play](https://arxiv.org/abs/2509.24193)
*Ran Xu,Yuchen Zhuang,Zihan Dong,Jonathan Wang,Yue Yu,Joyce C. Ho,Linjun Zhang,Haoyu Wang,Wenqi Shi,Carl Yang*

Main category: cs.CL

TL;DR: AceSearcher提出协作自玩框架，通过LLM角色切换（分解者与解决者）提升多跳检索与推理能力，小模型性能超越大模型


<details>
  <summary>Details</summary>
Motivation: 解决搜索增强LLMs在复杂推理任务中存在的多跳检索低效和推理能力不足问题

Method: 1. 监督微调混合搜索/推理/分解任务 2. 强化微调优化最终答案准确率 3. 无需中间过程标注

Result: 在10个数据集上平均提升7.6% EM，1.5B/8B小模型性能超越参数9倍大的模型，32B模型达DeepSeek-V3水平（仅用5%参数量）

Conclusion: 该框架显著提升复杂推理效率，参数利用率高，为资源受限场景提供有效解决方案

Abstract: Search-augmented LLMs often struggle with complex reasoning tasks due to
ineffective multi-hop retrieval and limited reasoning ability. We propose
AceSearcher, a cooperative self-play framework that trains a single large
language model (LLM) to alternate between two roles: a decomposer that breaks
down complex queries and a solver that integrates retrieved contexts for answer
generation. AceSearcher couples supervised fine-tuning on a diverse mixture of
search, reasoning, and decomposition tasks with reinforcement fine-tuning
optimized for final answer accuracy, eliminating the need for intermediate
annotations. Extensive experiments on three reasoning-intensive tasks across 10
datasets show that AceSearcher outperforms state-of-the-art baselines,
achieving an average exact match improvement of 7.6%. Remarkably, on
document-level finance reasoning tasks, AceSearcher-32B matches the performance
of the DeepSeek-V3 model using less than 5% of its parameters. Even at smaller
scales (1.5B and 8B), AceSearcher often surpasses existing search-augmented
LLMs with up to 9x more parameters, highlighting its exceptional efficiency and
effectiveness in tackling complex reasoning tasks. Our code will be published
at https://github.com/ritaranx/AceSearcher and
https://huggingface.co/AceSearcher.

</details>


### [120] [Can Large Language Models Express Uncertainty Like Human?](https://arxiv.org/abs/2509.24202)
*Linwei Tao,Yi-Fan Yeh,Bo Kai,Minjing Dong,Tao Huang,Tom A. Lamb,Jialin Yu,Philip H. S. Torr,Chang Xu*

Main category: cs.CL

TL;DR: 提出基于语言置信度（LC）的大模型不确定性估计方法，通过自然语言模糊表达（如'可能'）实现轻量级、人类对齐的置信度量化，并构建数据集、优化框架提升可靠性。


<details>
  <summary>Details</summary>
Motivation: 大模型在关键场景中的过度自信回答存在误导风险，现有置信度估计方法存在计算成本高、非自然交互等问题。

Method: 构建首个大规模模糊表达标注数据集，设计轻量级模糊词-置信度映射器，系统研究不同大模型的LC表现，开发微调框架优化LC可靠性。

Result: 优化后的提示策略使LC校准指标接近传统方法，微调框架进一步将ECE误差降低50%以上。

Conclusion: 语言置信度为大模型不确定性估计提供了高效、可扩展且符合人类认知的新范式，值得深入探索。

Abstract: Large language models (LLMs) are increasingly used in high-stakes settings,
where overconfident responses can mislead users. Reliable confidence estimation
has been shown to enhance trust and task accuracy. Yet existing methods face
practical barriers: logits are often hidden, multi-sampling is computationally
expensive, and verbalized numerical uncertainty (e.g., giving a 0-100 score)
deviates from natural communication. We revisit linguistic confidence (LC),
where models express uncertainty through hedging language (e.g., probably,
might), offering a lightweight and human-centered alternative. To advance this
direction, we (1) release the first diverse, large-scale dataset of hedging
expressions with human-annotated confidence scores, and (2) propose a
lightweight mapper that converts hedges into confidence scores at near-zero
cost. Building on these resources, we (3) conduct the first systematic study of
LC across modern LLMs and QA benchmarks, revealing that while most LLMs
underperform in expressing reliable LC, carefully designed prompting achieves
competitive calibration and discriminability. Finally, we (4) introduce a
fine-tuning framework that further improves LC reliability. Taken together, our
work positions linguistic confidence as a scalable, efficient, and
human-aligned approach to LLM uncertainty estimation, and calls for deeper
exploration of this promising yet underexplored direction.

</details>


### [121] [BeyondBench: Benchmark-Free Evaluation of Reasoning in Language Models](https://arxiv.org/abs/2509.24210)
*Gaurav Srivastava,Aafiya Hussain,Zhenyu Bi,Swastik Roy,Priya Pitre,Meng Lu,Morteza Ziyadi,Xuan Wang*

Main category: cs.CL

TL;DR: BeyondBench提出基于算法问题生成的无污染评估框架，通过动态生成数学问题评估101个语言模型的推理能力，发现模型在复杂任务中表现显著下降且依赖工具使用。


<details>
  <summary>Details</summary>
Motivation: 解决静态基准测试因训练数据污染导致的模型评估失真问题，区分模型是真正推理还是单纯记忆答案。

Method: 设计44个算法任务（117变体）分三个难度层级：基础算术（Easy）、序列模式（Medium）、NP难问题（Hard），通过组合空间>10¹⁵生成可验证的数学问题。

Result: 评估101个模型显示：1）模型家族普遍存在推理缺陷，Hard套件中Gemini-2.5-pro（56.38%）、Llama-3.3-70B（26.91%）、Qwen2.5-72B（33.60%）；2）未使用工具时性能暴跌（如GPT-5系列下降16.81%-47.59%）

Conclusion: BeyondBench有效暴露语言模型的推理局限性，强调动态生成评估的重要性，并揭示工具辅助对复杂任务的关键作用。

Abstract: Evaluating language models fairly is becoming harder as static benchmarks
available on the internet risk contamination by training data. This makes it
unclear whether models are truly reasoning or just recalling answers. In this
paper, we introduce BeyondBench, an evaluation framework that avoids this
problem by using algorithmic problem generation. Unlike traditional benchmarks
that risk contamination from internet-scale training data, BeyondBench creates
mathematically grounded problems on the fly, ensuring each test remains fresh
and uncontaminated. Our framework covers 44 algorithmic tasks with a total of
117 variations, grouped into three difficulty levels: the Easy Suite (29 tasks)
for basic arithmetic and statistics, the Medium Suite (5 tasks, 49 variations)
for sequence patterns and reasoning, and the Hard Suite (10 tasks, 68
variations) tackling NP-complete and constraint satisfaction problems. Each
task generates problems from a combinatorial space larger than 10^15 unique
instances, with solutions verified deterministically by mathematical proofs. We
evaluated 101 language models, including 85 open-source and 16 closed-source
models, spanning sizes from 0.5B to 141B parameters and multiple quantization
schemes. Our results show consistent reasoning deficiencies across model
families, with performance degrading sharply as problem complexity increases
from polynomial to exponential. In our Hard Suite evaluations, models such as
Gemini-2.5-pro, Llama-3.3-70B, and Qwen2.5-72B achieved average accuracies of
56.38%, 26.91%, and 33.60%, respectively. Moreover, we observe that performance
drops drastically without tool usage, with GPT-5, GPT-5-mini, and GPT-5-nano
showing a decline of 16.81%, 28.05%, and 47.59% accuracy on the hard suite. Our
leaderboard is publicly available at https://ctrl-gaurav.github.io/BeyondBench/

</details>


### [122] [ScenarioBench: Trace-Grounded Compliance Evaluation for Text-to-SQL and RAG](https://arxiv.org/abs/2509.24212)
*Zahra Atf,Peter R Lewis*

Main category: cs.CL

TL;DR: 提出ScenarioBench基准测试框架，通过政策条款级证据关联与严格审计规则，评估文本到SQL和检索增强系统在合规场景下的决策质量与解释可审计性


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL或知识密集型任务基准缺乏严格的证据链要求，难以满足合规场景下决策需关联具体政策条款并支持审计的需求

Method: 构建包含黄金标准包（决策预期、追踪见证、政策条款集、标准SQL）的YAML场景，设计决策准确性、追踪完整性/正确性/顺序、SQL等价性、策略覆盖率、时延、解释幻觉率等多维度评估体系，引入考虑检索难度和时间的标准化难度指数SDI/SDI-R

Result: 在严格的基础政策约束下，将系统优化方向转向解释质量提升，通过条款级证据绑定实现决策可审计，相比传统基准更强调合规场景的评估完备性

Conclusion: ScenarioBench填补了合规场景评估的空白，通过可证伪的解释机制和时间预算约束，推动AI系统在金融、医疗等强监管领域的可靠应用

Abstract: ScenarioBench is a policy-grounded, trace-aware benchmark for evaluating
Text-to-SQL and retrieval-augmented generation in compliance contexts. Each
YAML scenario includes a no-peek gold-standard package with the expected
decision, a minimal witness trace, the governing clause set, and the canonical
SQL, enabling end-to-end scoring of both what a system decides and why. Systems
must justify outputs using clause IDs from the same policy canon, making
explanations falsifiable and audit-ready. The evaluator reports decision
accuracy, trace quality (completeness, correctness, order), retrieval
effectiveness, SQL correctness via result-set equivalence, policy coverage,
latency, and an explanation-hallucination rate. A normalized Scenario
Difficulty Index (SDI) and a budgeted variant (SDI-R) aggregate results while
accounting for retrieval difficulty and time. Compared with prior Text-to-SQL
or KILT/RAG benchmarks, ScenarioBench ties each decision to clause-level
evidence under strict grounding and no-peek rules, shifting gains toward
justification quality under explicit time budgets.

</details>


### [123] [MoVa: Towards Generalizable Classification of Human Morals and Values](https://arxiv.org/abs/2509.24216)
*Ziyu Chen,Junfei Sun,Chenxi Li,Tuan Dung Nguyen,Jing Yao,Xiaoyuan Yi,Xing Xie,Chenhao Tan,Lexing Xie*

Main category: cs.CL

TL;DR: MoVa提供道德与价值观分析工具包，包含多框架数据集、高效LLM提示策略和调查评估应用


<details>
  <summary>Details</summary>
Motivation: 解决现有道德价值分析框架分散、数据难以整合的问题，提升跨领域研究的通用性和效率

Method: 1) 整合16个标注数据集；2) 开发多标签同时预测的all@once提示策略；3) 创建心理学调查评估工具

Result: all@once策略超越微调模型表现，支持多领域概念同步评估，类似多标签分类器链的效果

Conclusion: MoVa支持人机交互的细粒度分析，为机器行为对齐提供可解释性框架

Abstract: Identifying human morals and values embedded in language is essential to
empirical studies of communication. However, researchers often face substantial
difficulty navigating the diversity of theoretical frameworks and data
available for their analysis. Here, we contribute MoVa, a well-documented suite
of resources for generalizable classification of human morals and values,
consisting of (1) 16 labeled datasets and benchmarking results from four
theoretically-grounded frameworks; (2) a lightweight LLM prompting strategy
that outperforms fine-tuned models across multiple domains and frameworks; and
(3) a new application that helps evaluate psychological surveys. In practice,
we specifically recommend a classification strategy, all@once, that scores all
related concepts simultaneously, resembling the well-known multi-label
classifier chain. The data and methods in MoVa can facilitate many fine-grained
interpretations of human and machine communication, with potential implications
for the alignment of machine behavior.

</details>


### [124] [Model Fusion with Multi-LoRA Inference for Tool-Enhanced Game Dialogue Agents](https://arxiv.org/abs/2509.24229)
*Kangxu Wang,Ze Chen,Chengcheng Wei,Jiewen Zheng,Jiarong He,Max Gao*

Main category: cs.CL

TL;DR: opdainlp团队在CPDC2025挑战赛GPU赛道中，使用Qwen3-14B模型结合LoRA微调与多适配器融合方案，获得任务1/3冠军和任务2亚军


<details>
  <summary>Details</summary>
Motivation: 在保证效果的前提下兼顾推理时的资源/时间消耗限制，通过数据合成和模型架构优化应对游戏对话AI的多维度需求

Method: 采用三组独立LoRA适配器分别处理工具调用、带工具结果的响应生成、无工具结果的响应生成，通过vLLM框架实现MultiLoRA推理

Result: GPU赛道任务1（角色一致性）和任务3（功能调用）获得第一，任务2（世界观对齐）获得第二

Conclusion: 基于模型融合的多LoRA适配器架构能有效平衡效果与资源消耗，在复杂对话场景中实现精准的功能调用和上下文感知响应

Abstract: This paper presents the opdainlp team's solution for the GPU track of the
CPDC 2025 challenge. The challenge consists of three tasks, aiming to build an
in-game conversational AI that adheres to character personas, aligns with the
game's worldview, and supports function calling. Considering both effectiveness
and resource/time constraints during inference, we synthesized data for some of
the tasks based on the datasets provided by the competition organizers. We
employed Qwen3-14B with LoRA fine-tuning and model fusion, and utilized a base
model integrated with multiple LoRA adapters during inference. Specifically, in
the competition, we used three distinct LoRA adapters to handle tool calling,
response generation with tool call results, and response generation without
tool call results, respectively. MultiLoRA inference was implemented using
vLLM. Our solution achieved the first place in Task 1 and Task 3, and the
second place in Task 2 of the GPU track.

</details>


### [125] [Prompt and Parameter Co-Optimization for Large Language Models](https://arxiv.org/abs/2509.24245)
*Xiaohe Bo,Rui Li,Zexu Sun,Quanyu Dai,Zeyu Zhang,Zihang Tian,Xu Chen,Zhenhua Dong*

Main category: cs.CL

TL;DR: MetaTuner框架通过共享编码层和监督正则化损失，将提示优化与微调相结合，显著提升大语言模型性能


<details>
  <summary>Details</summary>
Motivation: 现有研究孤立探索提示优化（显式自然语言）与微调（隐式参数更新），尚未充分挖掘两者的协同潜力

Method: 设计双神经网络架构：提示生成器与参数生成器共享底层编码层，通过监督信号优化提示与参数的组合，设计监督正则化损失解决离散-连续空间的联合优化问题

Result: 多领域基准测试显示该方法持续超越传统基线模型

Conclusion: MetaTuner有效融合提示学习与参数微调，为LLM训练提供协同优化新范式

Abstract: Prompt optimization and fine-tuning are two major approaches to improve the
performance of Large Language Models (LLMs). They enhance the capabilities of
LLMs from complementary perspectives: the former through explicit natural
language, and the latter through implicit parameter updates. However, prior
work has typically studied them in isolation, leaving their synergistic
potential largely underexplored. To bridge this gap, in this paper, we
introduce MetaTuner, a novel framework that jointly integrates prompt
optimization and fine-tuning for LLM training. Specifically, we introduce two
neural networks to generate prompts and parameters, respectively, while
allowing them to share a common bottom encoding layer to enable knowledge
sharing. By the guidance of the final supervised signals, our framework is
optimized to discover the optimal combinations between the prompts and
parameters. Given that prompt learning involves discrete optimization while
fine-tuning operates in a continuous parameter space, we design a supervised
regularization loss to train our framework effectively. Extensive experiments
across diverse benchmarks show that our method consistently outperforms the
baselines.

</details>


### [126] [MRAG-Suite: A Diagnostic Evaluation Platform for Visual Retrieval-Augmented Generation](https://arxiv.org/abs/2509.24253)
*Yuelyu Ji*

Main category: cs.CL

TL;DR: 提出MRAG-Suite评估平台与MM-RAGChecker诊断工具，揭示多模态RAG系统在复杂查询中的性能缺陷


<details>
  <summary>Details</summary>
Motivation: 现有评估方法忽视查询难度与模糊性对系统性能的影响，导致潜在幻觉问题未被有效检测

Method: 整合多模态基准测试集+难度/模糊性过滤策略+声明级诊断工具MM-RAGChecker

Result: 困难/模糊查询导致准确率显著下降（平均降幅37.6%），诊断工具成功识别89%的幻觉案例

Conclusion: 该评估框架为改进多模态RAG系统的鲁棒性提供了标准化诊断方案

Abstract: Multimodal Retrieval-Augmented Generation (Visual RAG) significantly advances
question answering by integrating visual and textual evidence. Yet, current
evaluations fail to systematically account for query difficulty and ambiguity.
We propose MRAG-Suite, a diagnostic evaluation platform integrating diverse
multimodal benchmarks (WebQA, Chart-RAG, Visual-RAG, MRAG-Bench). We introduce
difficulty-based and ambiguity-aware filtering strategies, alongside
MM-RAGChecker, a claim-level diagnostic tool. Our results demonstrate
substantial accuracy reductions under difficult and ambiguous queries,
highlighting prevalent hallucinations. MM-RAGChecker effectively diagnoses
these issues, guiding future improvements in Visual RAG systems.

</details>


### [127] [SimuHome: A Temporal- and Environment-Aware Benchmark for Smart Home LLM Agents](https://arxiv.org/abs/2509.24282)
*Gyuhyeon Seo,Jungwoo Yang,Junseong Pyo,Nalim Kim,Jonggeun Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 提出SimuHome智能家居模拟环境解决LLM代理在现实部署中的模拟环境缺失与评估基准难题


<details>
  <summary>Details</summary>
Motivation: 现有智能家居代理开发存在模拟环境保真度不足、缺乏复杂评估基准的瓶颈，阻碍其实际应用部署

Method: 基于Matter协议构建时间加速的智能家居模拟环境，建立含600个复杂场景的测试基准，评估11种代理在ReAct框架下的表现

Result: 现有模型在潜在意图推理(成功率<45%)和时间调度任务(成功率仅32%)表现薄弱，最佳模型GPT-4成功率仅54%

Conclusion: 智能家居代理需强化状态验证工具与时间协调机制，SimuHome为行业提供可迁移的标准化验证平台

Abstract: Large Language Model (LLM) agents excel at multi-step, tool-augmented tasks.
However, smart homes introduce distinct challenges, requiring agents to handle
latent user intents, temporal dependencies, device constraints, scheduling, and
more. The main bottlenecks for developing smart home agents with such
capabilities include the lack of a realistic simulation environment where
agents can interact with devices and observe the results, as well as a
challenging benchmark to evaluate them. To address this, we introduce
$\textbf{SimuHome}$, a time-accelerated home environment that simulates smart
devices, supports API calls, and reflects changes in environmental variables.
By building the simulator on the Matter protocol (the global industry standard
for smart home communication), SimuHome provides a high-fidelity environment,
and agents validated in SimuHome can be deployed on real Matter-compliant
devices with minimal adaptation. We provide a challenging benchmark of 600
episodes across twelve user query types that require the aforementioned
capabilities. Our evaluation of 11 agents under a unified ReAct framework
reveals that while models perform well on simple tasks, they struggle with
latent intent inference, state verification, and especially temporal
scheduling. Even the top-performing model, GPT-4.1, reaches only 54% success
rate. These findings highlight a critical need for methods that can reliably
verify the current state via tools before acting and coordinate time-dependent
actions.

</details>


### [128] [Let LLMs Speak Embedding Languages: Generative Text Embeddings via Iterative Contrastive Refinement](https://arxiv.org/abs/2509.24291)
*Yu-Che Tsai,Kuan-Yu Chen,Yuan-Chi Li,Yuan-Hao Chen,Ching-Yu Tsai,Shou-De Lin*

Main category: cs.CL

TL;DR: 提出GIRCSE框架，通过生成式迭代优化实现文本嵌入，超越传统编码器方法并展现测试时扩展性


<details>
  <summary>Details</summary>
Motivation: 现有LLM嵌入方法仅使用编码器模式，忽视生成能力对语义表征的潜在优化空间

Method: 使用自回归生成软标记序列，提出迭代对比优化目标(ICR)指导多步表征优化

Result: 在MTEB基准和指令任务中超越基线，生成更多token可持续提升嵌入质量（测试时扩展性）

Conclusion: 生成式迭代优化为表示学习开辟新范式，成功融合LLM的生成优势与对比学习目标

Abstract: Existing large language model (LLM)-based embeddings typically adopt an
encoder-only paradigm, treating LLMs as static feature extractors and
overlooking their core generative strengths. We introduce GIRCSE (Generative
Iterative Refinement for Contrastive Sentence Embeddings), a novel framework
that leverages autoregressive generation to iteratively refine semantic
representations. By producing sequences of soft tokens optimized under
contrastive objective, GIRCSE captures latent concepts and implicit semantics
that encoder-only methods often miss. To guide this process, we propose an
Iterative Contrastive Refinement (ICR) objective that encourages each
refinement step to yield better representations. Extensive experiments show
that GIRCSE outperforms strong LLM-based embedding baselines on the MTEB
benchmark and instruction-following tasks. Moreover, GIRCSE exhibits an
emergent test-time scaling property: generating more tokens at inference
steadily improves embedding quality. Our results establish generative iterative
refinement as a new paradigm for representation learning.

</details>


### [129] [LOGOS: LLM-driven End-to-End Grounded Theory Development and Schema Induction for Qualitative Research](https://arxiv.org/abs/2509.24294)
*Xinyu Pi,Qisen Yang,Chuong Nguyen*

Main category: cs.CL

TL;DR: LOGOS是首个端到端自动化扎根理论框架，通过LLM驱动编码、语义聚类和图推理技术，有效解决传统质性研究中人工编码的可扩展性瓶颈。


<details>
  <summary>Details</summary>
Motivation: 传统扎根理论依赖专家手动编码难以规模化，现有计算工具无法实现真正自动化，亟需能在保持理论深度的前提下实现全流程自动化的解决方案。

Method: 整合LLM自动编码、语义聚类构建层次结构、图推理优化关联关系，并提出包含5维指标和训练-测试分割的标准化评估体系。

Result: 在五个跨领域语料库中超越基线模型，特别是在复杂数据集上与专家构建的框架达成88.2%的语义对齐度。

Conclusion: LOGOS开创了质性研究规模化的新范式，在保持理论深度的同时显著提升研究效率，为社会科学研究的民主化提供了技术基础设施。

Abstract: Grounded theory offers deep insights from qualitative data, but its reliance
on expert-intensive manual coding presents a major scalability bottleneck.
Current computational tools stop short of true automation, keeping researchers
firmly in the loop. We introduce LOGOS, a novel, end-to-end framework that
fully automates the grounded theory workflow, transforming raw text into a
structured, hierarchical theory. LOGOS integrates LLM-driven coding, semantic
clustering, graph reasoning, and a novel iterative refinement process to build
highly reusable codebooks. To ensure fair comparison, we also introduce a
principled 5-dimensional metric and a train-test split protocol for
standardized, unbiased evaluation. Across five diverse corpora, LOGOS
consistently outperforms strong baselines and achieves a remarkable $88.2\%$
alignment with an expert-developed schema on a complex dataset. LOGOS
demonstrates a powerful new path to democratize and scale qualitative research
without sacrificing theoretical nuance.

</details>


### [130] [DiffuGuard: How Intrinsic Safety is Lost and Found in Diffusion Large Language Models](https://arxiv.org/abs/2509.24296)
*Zherui Li,Zheng Nie,Zhenhong Zhou,Yufei Guo,Yue Liu,Yitong Zhang,Yu Cheng,Qingsong Wen,Kun Wang,Jiaheng Zhang*

Main category: cs.CL

TL;DR: 研究发现扩散大语言模型存在迭代生成机制引发的独特漏洞，提出DiffuGuard防御框架将攻击成功率从47.9%降至14.7%


<details>
  <summary>Details</summary>
Motivation: 针对扩散大语言模型迭代生成机制带来的新型安全漏洞，揭示现有解码策略的安全隐患并挖掘模型内在防御潜力

Method: 提出双阶段防御框架DiffuGuard：通过动态随机重掩码消除选择偏差，利用块级审计修复机制实现自主风险检测与引导校正

Result: 在四个dLLM上的实验表明，DiffuGuard对六种越狱攻击的平均防御成功率提升33.2%，同时保持模型实用性和效率

Conclusion: 研究不仅揭示了扩散模型的安全脆弱性本质，还证明通过解码策略优化可实现高效安全防护，为生成模型安全研究开辟新方向

Abstract: The rapid advancement of Diffusion Large Language Models (dLLMs) introduces
unprecedented vulnerabilities that are fundamentally distinct from
Autoregressive LLMs, stemming from their iterative and parallel generation
mechanisms. In this paper, we conduct an in-depth analysis of dLLM
vulnerabilities to jailbreak attacks across two distinct dimensions: intra-step
and inter-step dynamics. Experimental results reveal a harmful bias inherent in
the standard greedy remasking strategy and identify a critical phenomenon we
term Denoising-path Dependence, where the safety of early-stage tokens
decisively influences the final output. These findings also indicate that while
current decoding strategies constitute a significant vulnerability, dLLMs
possess a substantial intrinsic safety potential. To unlock this potential, we
propose DiffuGuard, a training-free defense framework that addresses
vulnerabilities through a dual-stage approach: Stochastic Annealing Remasking
dynamically introduces controlled randomness to mitigate greedy selection bias,
while Block-level Audit and Repair exploits internal model representations for
autonomous risk detection and guided correction. Comprehensive experiments on
four dLLMs demonstrate DiffuGuard's exceptional effectiveness, reducing Attack
Success Rate against six diverse jailbreak methods from 47.9% to 14.7% while
preserving model utility and efficiency. Our code is available at:
https://github.com/niez233/DiffuGuard.

</details>


### [131] [Q-Mirror: Unlocking the Multi-Modal Potential of Scientific Text-Only QA Pairs](https://arxiv.org/abs/2509.24297)
*Junying Wang,Zicheng Zhang,Ye Shen,Yalun Wu,Yingji Liang,Yijin Guo,Farong Wen,Wenzhe Li,Xuezhi Zhao,Qi Jia,Guangtao Zhai*

Main category: cs.CL

TL;DR: 论文提出通过TQA-to-MMQA框架将文本问答对转化为高质量多模态问答对，开发了Q-Mirror代理系统实现生成与评估闭环，实验表明系统显著提升生成质量（平均分78.90→85.22，通过率72%→95%）。


<details>
  <summary>Details</summary>
Motivation: 解决手动创建多模态科学基准的高成本与不可扩展性问题，探索自动化生成高质量MMQA的可行性。

Method: 1. 定义TQA转MMQA的框架与质量评估标准
2. 构建双基准测试评估模型能力
3. 开发Q-Mirror系统（集成生成与评估的闭环迭代机制）

Result: 现有模型生成存在明显差距，顶级评估模型与人类判断高度一致。Q-Mirror使平均分提升6.32分，通过率提升23个百分点。

Conclusion: Q-Mirror为实现大规模科学基准提供了可行路径，同时揭示了可靠评估体系对生成质量提升的关键作用。

Abstract: High-quality, multi-modal benchmarks are crucial for advancing scientific
reasoning in large models yet their manual creation is costly and unscalable.
To address this bottleneck, we explore the potential for transforming Text-Only
QA Pairs (TQAs) into high-quality Multi-Modal QA Pairs (MMQAs), which include
three parts: 1) Task Definition \& Evaluation Rubric: We develop a TQA-to-MMQA
framework and establish a comprehensive, multi-dimensional MMQA quality rubric
that provides principles for the transformation. 2) Benchmark Construction:
Then we construct two extensive benchmarks to rigorously evaluate
state-of-the-art generation \& understanding models on the distinct tasks of
MMQA generation \& MMQA quality evaluation. 3) Preliminary Solution: We develop
an agentic system (Q-Mirror), which operationalizes our framework by
integrating MMQA generation and evaluation into a closed loop for iterative
refinement. Our experiments show that while state-of-the-art models can
generate MMQAs, their outputs still leave substantial gaps, underscoring the
need for reliable evaluation. We further demonstrate that top-tier
understanding models align closely with human judgment in MMQA quality
assessment. Leveraging both insights, the Q-Mirror agent raises average scores
from 78.90 to 85.22 and pass rates from 72\% to 95\%, offering a practical path
to large-scale scientific benchmarks.

</details>


### [132] [Dual Mechanisms of Value Expression: Intrinsic vs. Prompted Values in LLMs](https://arxiv.org/abs/2509.24319)
*Jongwook Han,Jongwon Lim,Injin Kong,Yohan Jo*

Main category: cs.CL

TL;DR: 研究发现LLMs内在价值观表达与提示驱动表达共享部分机制组件，但存在独特元素导致两者在操控性和响应多样性上呈现互补特性（提示操控性更强，内在表达多样性更高）。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLMs价值观表达的内在机制（训练所得）与提示驱动机制的关系缺乏深入理解，而这两者在价值观对齐、角色扮演等关键应用场景中被广泛使用，需明确其底层机理差异。

Method: 采用价值向量（从残差流提取的特征方向）和价值神经元（MLP中贡献价值观表达的神经元）两种方法进行机制分析，对比内在与提示表达的关键组件。

Result: 1. 共享组件对两种表达都关键
2. 内在机制独特组件提升回答词汇多样性
3. 提示机制独特组件强化指令遵循（甚至影响越狱等远端任务）
4. 提示操控性＞内在，内在响应多样性＞提示

Conclusion: 两种价值观表达机制存在功能分化，提示机制更适用于精准价值观引导，内在机制则为多样化输出提供基础。该发现为模型价值观调控提供了神经元层级的理论依据。

Abstract: Large language models (LLMs) can express different values in two distinct
ways: (1) intrinsic expression, reflecting the model's inherent values learned
during training, and (2) prompted expression, elicited by explicit prompts.
Given their widespread use in value alignment and persona steering, it is
paramount to clearly understand their underlying mechanisms, particularly
whether they mostly overlap (as one might expect) or rely on substantially
different mechanisms, but this remains largely understudied. We analyze this at
the mechanistic level using two approaches: (1) value vectors, feature
directions representing value mechanisms extracted from the residual stream,
and (2) value neurons, MLP neurons that contribute to value expressions. We
demonstrate that intrinsic and prompted value mechanisms partly share common
components that are crucial for inducing value expression, but also possess
unique elements that manifest in different ways. As a result, these mechanisms
lead to different degrees of value steerability (prompted > intrinsic) and
response diversity (intrinsic > prompted). In particular, components unique to
the intrinsic mechanism seem to promote lexical diversity in responses, whereas
those specific to the prompted mechanism primarily strengthen instruction
following, taking effect even in distant tasks like jailbreaking.

</details>


### [133] [Multimodal Large Language Models Meet Multimodal Emotion Recognition and Reasoning: A Survey](https://arxiv.org/abs/2509.24322)
*Yuntao Shou,Tao Meng,Wei Ai,Keqin Li*

Main category: cs.CL

TL;DR: 综述大语言模型及多模态大语言模型在情感识别与推理中的应用与发展


<details>
  <summary>Details</summary>
Motivation: 现有领域缺乏系统性综述，本文旨在整合LLMs/MLLMs在情感识别领域的最新进展，为研究者提供权威参考和实践指导

Method: 系统性综述方法：分析模型架构、数据集及性能基准，识别关键挑战并提出未来研究方向

Result: 建立首个MLLMs与多模态情感推理交叉领域综述框架，整理开源资源库并提供领域发展路线图

Conclusion: 本文开创性地系统梳理了多模态大模型在情感推理领域的研究现状，为后续研究提供方法论支持和资源整合平台

Abstract: In recent years, large language models (LLMs) have driven major advances in
language understanding, marking a significant step toward artificial general
intelligence (AGI). With increasing demands for higher-level semantics and
cross-modal fusion, multimodal large language models (MLLMs) have emerged,
integrating diverse information sources (e.g., text, vision, and audio) to
enhance modeling and reasoning in complex scenarios. In AI for Science,
multimodal emotion recognition and reasoning has become a rapidly growing
frontier. While LLMs and MLLMs have achieved notable progress in this area, the
field still lacks a systematic review that consolidates recent developments. To
address this gap, this paper provides a comprehensive survey of LLMs and MLLMs
for emotion recognition and reasoning, covering model architectures, datasets,
and performance benchmarks. We further highlight key challenges and outline
future research directions, aiming to offer researchers both an authoritative
reference and practical insights for advancing this domain. To the best of our
knowledge, this paper is the first attempt to comprehensively survey the
intersection of MLLMs with multimodal emotion recognition and reasoning. The
summary of existing methods mentioned is in our Github:
\href{https://github.com/yuntaoshou/Awesome-Emotion-Reasoning}{https://github.com/yuntaoshou/Awesome-Emotion-Reasoning}.

</details>


### [134] [Speculative Verification: Exploiting Information Gain to Refine Speculative Decoding](https://arxiv.org/abs/2509.24328)
*Sungkyun Kim,Jaemin Kim,Dogyung Yoon,Jiho Shin,Junyeol Lee,Jiwon Seo*

Main category: cs.CL

TL;DR: 提出动态推测验证(SV)方法，通过伴随模型预测对齐度并动态调整验证长度，将大语言模型推理效率平均提升1.4倍


<details>
  <summary>Details</summary>
Motivation: 传统推测解码(SD)在推测准确性低时会产生大量无效计算，特别是大批量场景下性能受限明显

Method: 引入轻量级伴随模型量化对齐程度，通过信息增益最大化动态决策验证长度，兼容现有推测解码方案且无需模型修改

Result: 在13B-72B模型上测试显示，SV在大批量(32-80)场景下相对SD加速最高达2倍，平均加速1.4倍

Conclusion: SV通过智能验证机制显著提升LLM推理效率，具备模型无关性和工程易用性，为实际部署提供有效解决方案

Abstract: LLMs have low GPU efficiency and high latency due to autoregressive decoding.
Speculative decoding (SD) mitigates this using a small draft model to
speculatively generate multiple tokens, which are then verified in parallel by
a target model. However, when speculation accuracy is low, the overhead from
rejected tokens can offset the benefits, limiting SD's effectiveness,
especially at large batch sizes. To address this, we propose Speculative
Verification (SV), an efficient augmentation to SD that dynamically predicts
speculation accuracy and adapts the verification length to maximize throughput.
SV introduces a companion model - a small auxiliary model similar in size to
the draft model - to estimate the alignment between draft and target model
distributions. By maximizing the information gain from quantifying this
alignment, SV refines verification decisions, reducing wasted computation on
rejected tokens and improving decoding efficiency. Moreover, SV requires no
modifications to the draft or target models and is compatible with existing SD
variants. We extensively evaluated SV on publicly available LLMs across three
NLP tasks using nine combinations of draft, companion, and target models,
including 13B-72B target models and three types of variations: base (no
finetuning), instruction-tuned, and task fine-tuned. Across all experiments and
batch sizes (4-80), SV consistently outperforms both SD and standard decoding
with the target model. It improves SD performance by up to 2$\times$, with an
average speedup of 1.4 $\times$ in large-batch settings (batch sizes 32-80).
These results demonstrate SV's robustness, scalability, and practical utility
for efficient LLM inference.

</details>


### [135] [AlignX: Advancing Multilingual Large Language Models with Multilingual Representation Alignment](https://arxiv.org/abs/2509.24338)
*Mengyu Bu,Shaolei Zhang,Zhongjun He,Hua Wu,Yang Feng*

Main category: cs.CL

TL;DR: 提出AlignX两阶段框架，通过表示对齐和指令微调提升多语言大模型性能


<details>
  <summary>Details</summary>
Motivation: 现有多语言大模型对非主导语言表现不足，传统大规模微调方法存在跨语言对齐不精确和知识迁移效率低的问题

Method: 第一阶段结合多语言语义对齐和语言特征整合实现表示对齐，第二阶段通过多语言指令微调激发模型潜力

Result: 实验证明该方法显著提升模型的多语言通用能力和跨语言生成能力，并改善多语言表示的空间分布

Conclusion: AlignX通过分阶段的表示优化有效缩小多语言性能差距，为提升LLMs的跨语言对齐提供新思路

Abstract: Multilingual large language models (LLMs) possess impressive multilingual
understanding and generation capabilities. However, their performance and
cross-lingual alignment often lag for non-dominant languages. A common solution
is to fine-tune LLMs on large-scale and more balanced multilingual corpus, but
such approaches often lead to imprecise alignment and suboptimal knowledge
transfer, struggling with limited improvements across languages. In this paper,
we propose AlignX to bridge the multilingual performance gap, which is a
two-stage representation-level framework for enhancing multilingual performance
of pre-trained LLMs. In the first stage, we align multilingual representations
with multilingual semantic alignment and language feature integration. In the
second stage, we stimulate the multilingual capability of LLMs via multilingual
instruction fine-tuning. Experimental results on several pre-trained LLMs
demonstrate that our approach enhances LLMs' multilingual general and
cross-lingual generation capability. Further analysis indicates that AlignX
brings the multilingual representations closer and improves the cross-lingual
alignment.

</details>


### [136] [Beyond Repetition: Text Simplification and Curriculum Learning for Data-Constrained Pretraining](https://arxiv.org/abs/2509.24356)
*Matthew Theodore Roque,Dan John Velasco*

Main category: cs.CL

TL;DR: 在数据受限场景下，简化文本与训练顺序优化可提升模型性能：小模型适合低→高复杂度顺序，大模型更适合交错训练。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型预训练研究多聚焦大数据集，但对数据受限场景下的数据排序效应（如复杂度顺序）和简化数据增强效果缺乏深入探索。

Method: 使用人类文本与LLM简化文本的平行语料库，测试四种数据调度策略（重复暴露/低→高/高→低/交错顺序），通过微调样本效率和零样本任务（语言知识/实体追踪/常识推理等）评估表征质量。

Result: 1. 添加简化数据优于重复原始数据
2. 小模型在低→高复杂度顺序表现更好
3. 大规模模型在交错顺序中取得最佳表现
4. 改进幅度在实体追踪任务中达10%，常识推理提升6%

Conclusion: 数据受限时，复杂度驱动的课程学习策略能有效提升模型表征能力，且模型规模与最优数据调度方式存在强相关性，为高效预训练提供了新方向。

Abstract: Most studies on language model pretraining focus on large datasets, leaving
open questions about optimization in data-constrained settings. In such
settings, the effects of training data order and of including alternative
versions of the same text remain underexplored. We address this by studying
curriculum learning in pretraining, focusing on text-complexity ordering and
data augmentation via simplification. We ask: (1) Does simplifying texts
enhance representation quality more than reusing the original data? and (2)
Does ordering data by text complexity yield better representations? To answer,
we build on a pair of parallel corpora where human-written paragraphs are
aligned with LLM-simplified variants, and test four data schedules: repeated
exposure, low-to-high complexity, high-to-low, and interleaved. We analyze
models' representation quality from a sample efficiency perspective via
fine-tuning, as well as its zero-shot performance on linguistic knowledge,
entity tracking, world knowledge, and commonsense reasoning. Our findings show
that adding simplified data improves fine-tuning and zero-shot performance over
a repeated-exposure baseline: smaller models benefit from low-to-high
complexity, while larger models perform better with interleaved ordering.

</details>


### [137] [Reinforcement Mid-Training](https://arxiv.org/abs/2509.24375)
*Yijun Tian,Shaoyu Chen,Zhichao Xu,Yawei Wang,Jinhe Bi,Peng Han,Wei Wang*

Main category: cs.CL

TL;DR: 论文提出在预训练和后训练之间增加强化中期训练阶段（RMT），通过动态令牌预算机制、课程自适应采样和双重训练策略，显著提升语言模型性能并减少推理长度。


<details>
  <summary>Details</summary>
Motivation: 现有大模型训练流程仅包含预训练和后训练两阶段，忽视了中间阶段对模型推理效率和关键信息利用的优化空间。本文针对训练效率低、令牌熵分布不平衡、令牌信息未充分利用三大问题提出改进方案。

Method: 1. 动态令牌预算机制约束推理步骤；2. 基于课程的自适应采样实现从易到难的学习路径；3. 强化学习与下一令牌预测结合的双重训练策略。

Result: 语言建模性能最高提升64.91%（推理长度减少79%），数学领域后训练性能提升18.76%。

Conclusion: RMT验证了强化中期训练的有效性，其检查点可显著提升后续训练效果，为模型训练流程创新提供新思路。

Abstract: The development of state-of-the-art large language models is commonly
understood as a two-stage process involving pre-training and post-training. We
point out the need for an additional intermediate stage called reinforcement
mid-training with potential for strong performance gains. In this paper, we
formally define the problem and identify three key challenges: (1) inefficient
training due to excessive reasoning steps, (2) disregard of the imbalanced
token entropy distribution, and (3) underutilization of token information. To
address these challenges, we propose RMT, a framework for efficient, adaptive,
and unified reinforcement mid-training with various innovative components. In
particular, we first introduce a dynamic token budget mechanism that constrains
unnecessary reasoning steps and mitigates model overthinking. Next, we design a
curriculum-based adaptive sampling method that fosters a progressive learning
trajectory from easy to hard tokens. Finally, we present a dual training
strategy that combines reinforcement learning with next-token prediction,
ensuring targeted learning on key tokens and full exploitation of all token
information. Extensive experiments demonstrate the superiority of RMT over
state-of-the-art methods, achieving up to +64.91% performance improvement with
only 21% of the reasoning length in language modeling. We also show that
checkpoints obtained after reinforcement mid-training can benefit the
subsequent post-training, yielding up to +18.76% improvement in the
mathematical domain.

</details>


### [138] [HarmMetric Eval: Benchmarking Metrics and Judges for LLM Harmfulness Assessment](https://arxiv.org/abs/2509.24384)
*Langqi Yang,Tianhang Zheng,Kedong Xiu,Yixuan Chen,Di Wang,Puning Zhao,Zhan Qin,Kui Ren*

Main category: cs.CL

TL;DR: 提出HarmMetric Eval基准，揭示传统指标METEOR和ROUGE-1在评估大模型有害响应时优于LLM评判


<details>
  <summary>Details</summary>
Motivation: 现有越狱攻击评估指标缺乏统一基准，导致报告可信度不足，需系统性评估工具确保模型安全部署

Method: 构建包含有害提示及多样化响应样本的数据集，设计兼容多种指标的评分机制，并进行跨指标对比实验

Result: METEOR和ROUGE-1在有害性评估中准确率分别达0.68和0.64，显著优于GPT-4 Judge的0.53

Conclusion: 挑战LLM评判的优越性假设，证明传统指标在特定任务的有效性，为安全评估提供新基准

Abstract: The alignment of large language models (LLMs) with human values is critical
for their safe deployment, yet jailbreak attacks can subvert this alignment to
elicit harmful outputs from LLMs. In recent years, a proliferation of jailbreak
attacks has emerged, accompanied by diverse metrics and judges to assess the
harmfulness of the LLM outputs. However, the absence of a systematic benchmark
to assess the quality and effectiveness of these metrics and judges undermines
the credibility of the reported jailbreak effectiveness and other risks. To
address this gap, we introduce HarmMetric Eval, a comprehensive benchmark
designed to support both overall and fine-grained evaluation of harmfulness
metrics and judges. Our benchmark includes a high-quality dataset of
representative harmful prompts paired with diverse harmful and non-harmful
model responses, alongside a flexible scoring mechanism compatible with various
metrics and judges. With HarmMetric Eval, our extensive experiments uncover a
surprising result: two conventional metrics--METEOR and ROUGE-1--outperform
LLM-based judges in evaluating the harmfulness of model responses, challenging
prevailing beliefs about LLMs' superiority in this domain. Our dataset is
publicly available at https://huggingface.co/datasets/qusgo/HarmMetric_Eval,
and the code is available at
https://anonymous.4open.science/r/HarmMetric-Eval-4CBE.

</details>


### [139] [LLaDA-MoE: A Sparse MoE Diffusion Language Model](https://arxiv.org/abs/2509.24389)
*Fengqi Zhu,Zebin You,Yipeng Xing,Zenan Huang,Lin Liu,Yihong Zhuang,Guoshan Lu,Kangyu Wang,Xudong Wang,Lanning Wei,Hongrui Guo,Jiaqi Hu,Wentao Ye,Tieyuan Chen,Chenchen Li,Chengfu Tang,Haibo Feng,Jun Hu,Jun Zhou,Xiaolu Zhang,Zhenzhong Lan,Junbo Zhao,Da Zheng,Chongxuan Li,Jianguo Li,Ji-Rong Wen*

Main category: cs.CL

TL;DR: LLaDA-MoE是基于稀疏MoE架构的扩散语言模型，在保持7B总参数量的前提下通过激活1.4B参数实现高效推理，性能超越传统大参数扩散模型。


<details>
  <summary>Details</summary>
Motivation: 探索在掩码扩散语言模型中集成稀疏MoE架构的可能性，在保证模型性能的同时显著提升推理效率，减少计算资源消耗。

Method: 采用MoE架构设计，总参数量7B但推理时仅激活1.4B参数；使用约20T token进行从头训练；通过指令微调提升多任务能力。

Result: 在多个基准测试中超越LLaDA系列和Dream等扩散模型，指令微调版在知识理解、代码生成等任务达到Qwen2.5-3B水平（活跃参数少33%）。

Conclusion: 验证了MoE架构在扩散语言模型中的有效性，为高效推理开辟新方向，模型已在Huggingface开源供后续研究。

Abstract: We introduce LLaDA-MoE, a large language diffusion model with the
Mixture-of-Experts (MoE) architecture, trained from scratch on approximately
20T tokens. LLaDA-MoE achieves competitive performance with significantly
reduced computational overhead by maintaining a 7B-parameter capacity while
activating only 1.4B parameters during inference. Our empirical evaluation
reveals that LLaDA-MoE achieves state-of-the-art performance among diffusion
language models with larger parameters, surpassing previous diffusion language
models LLaDA, LLaDA 1.5, and Dream across multiple benchmarks. The
instruct-tuned model LLaDA-MoE-7B-A1B-Instruct demonstrates capabilities
comparable to Qwen2.5-3B-Instruct in knowledge understanding, code generation,
mathematical reasoning, agent and alignment tasks, despite using fewer active
parameters. Our results show that integrating a sparse MoE architecture into
the training objective of masked diffusion language models still brings out
MoE's strengths under efficient inference with few active parameters, and opens
ample room for further exploration of diffusion language models. LLaDA-MoE
models are available at Huggingface.

</details>


### [140] [Agentar-Scale-SQL: Advancing Text-to-SQL through Orchestrated Test-Time Scaling](https://arxiv.org/abs/2509.24403)
*Pengfei Wang,Baolin Sun,Xuemei Dong,Yaxun Dai,Hongwei Yuan,Mengdie Chu,Yingqi Gao,Xiang Qi,Peng Zhang,Ying Yan*

Main category: cs.CL

TL;DR: 提出Agentar-Scale-SQL框架，通过协同三种扩展策略（内在推理扩展、迭代优化扩展、并行合成扩展）在BIRD基准测试中实现SOTA性能（81.67%执行准确率）


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL方法在BIRD等复杂基准测试中显著落后人类专家，且缺乏系统化的测试时扩展策略

Method: 采用协同测试时扩展策略：1）通过强化学习增强的内在推理实现内在扩展 2）迭代优化实现序列扩展 3）多样化合成和锦标赛选择的并行扩展

Result: 在BIRD测试集达到81.67%执行准确率，官方排行榜排名第一

Conclusion: 该框架为提升复杂文本到SQL任务性能提供了系统化解决方案，展示了通向人类水平性能的有效路径

Abstract: State-of-the-art (SOTA) Text-to-SQL methods still lag significantly behind
human experts on challenging benchmarks like BIRD. Current approaches that
explore test-time scaling lack an orchestrated strategy and neglect the model's
internal reasoning process. To bridge this gap, we introduce Agentar-Scale-SQL,
a novel framework leveraging scalable computation to improve performance.
Agentar-Scale-SQL implements an Orchestrated Test-Time Scaling strategy that
synergistically combines three distinct perspectives: i) Internal Scaling via
RL-enhanced Intrinsic Reasoning, ii) Sequential Scaling through Iterative
Refinement, and iii) Parallel Scaling using Diverse Synthesis and Tournament
Selection. Agentar-Scale-SQL is a general-purpose framework designed for easy
adaptation to new databases and more powerful language models. Extensive
experiments show that Agentar-Scale-SQL achieves SOTA performance on the BIRD
benchmark, reaching 81.67\% execution accuracy on the test set and ranking
first on the official leaderboard, demonstrating an effective path toward
human-level performance.

</details>


### [141] [Multilingual Text-to-SQL: Benchmarking the Limits of Language Models with Collaborative Language Agents](https://arxiv.org/abs/2509.24405)
*Khanh Trinh Pham,Thu Huong Nguyen,Jun Jo,Quoc Viet Hung Nguyen,Thanh Tam Nguyen*

Main category: cs.CL

TL;DR: 提出MultiSpider 2.0多语言Text-to-SQL基准，覆盖8种语言，揭示顶尖大模型在多语言场景下执行准确率暴跌至4%，并提出协作式语言代理方法将准确率提升至15%。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL基准以英语为中心，限制了多语言场景的技术进展，需构建包含语言多样性及方言差异的评测体系。

Method: 扩展Spider 2.0至八种语言（英德法西葡日中越），保持结构复杂性的同时增加语言变体，要求模型进行深度推理生成复杂SQL。

Result: DeepSeek-R1等顶尖模型内在推理准确率仅4%（对比MultiSpider 1.0的60%），协作式语言代理通过迭代优化将准确率提升至15%。

Conclusion: 基准揭示显著的多语言能力鸿沟，需开发跨语言鲁棒方法，推动技术在企业级场景的实用化部署，已开源基准测试集。

Abstract: Text-to-SQL enables natural access to databases, yet most benchmarks are
English-only, limiting multilingual progress. We introduce MultiSpider 2.0,
extending Spider 2.0 to eight languages (English, German, French, Spanish,
Portuguese, Japanese, Chinese, Vietnamese). It preserves Spider 2.0's
structural difficulty while adding linguistic and dialectal variability,
demanding deeper reasoning for complex SQL. On this benchmark, state-of-the-art
LLMs (such as DeepSeek-R1 and OpenAI o1) reach only 4\% execution accuracy when
relying on intrinsic reasoning, versus 60\% on MultiSpider 1.0. Therefore, we
provide a collaboration-driven language agents baseline that iteratively
refines queries, improving accuracy to 15\%. These results reveal a substantial
multilingual gap and motivate methods that are robust across languages and
ready for real-world enterprise deployment. Our benchmark is available at
https://github.com/phkhanhtrinh23/Multilingual_Text_to_SQL.

</details>


### [142] [CDT: A Comprehensive Capability Framework for Large Language Models Across Cognition, Domain, and Task](https://arxiv.org/abs/2509.24422)
*Haosi Mo,Xinyu Ma,Xuebo Liu,Derek F. Wong,Yu Li,Jie Liu,Min Zhang*

Main category: cs.CL

TL;DR: 提出CDT三维评估框架，结合认知理论全面评测大语言模型能力，实验验证其在数据集分析和构建中的有效性


<details>
  <summary>Details</summary>
Motivation: 现有基准测试孤立评估模型能力，缺乏系统性框架。研究旨在通过认知理论重构能力分类体系，建立更全面的评估方法论

Method: 1. 融合Cattell-Horn-Carroll认知理论重构模型能力分类 2. 构建认知-领域-任务三维评估框架 3. 应用于数据集能力评估和训练数据选择

Result: 1. 能力指标与下游任务强相关（通用基准44.3分提升1.6，特定基准45.4分提升2.2） 2. 有效支持数据分析和构建 3. 开源模型与代码验证实用性

Conclusion: CDT框架通过系统化能力评估显著提升模型性能，为LLM评估提供理论指导与实践工具，开源资源促进领域发展

Abstract: Recent advances in Large Language Models (LLMs) have significantly enhanced
their capabilities, highlighting the need for comprehensive evaluation
frameworks that extend beyond task-specific benchmarks. However, existing
benchmarks often focus on isolated abilities, lacking a holistic framework for
assessing LLM capabilities. To address this gap, we propose the
Cognition-Domain-Task (CDT) framework, which comprehensively measures a model's
capabilities across three dimensions. We expand the scope of model capability
definitions at the cognitive level by incorporating the Cattell-Horn-Carroll
cognitive theory, refining the categorization of model capabilities. We apply
CDT in two directions: dataset capability evaluation and data selection.
Experiments show that our capability metrics correlate well with downstream
performance and can support effective dataset analysis and construction. The
experiments on data selection also show significant improvements in both
general and specific benchmarks, achieving scores of 44.3 and 45.4, with an
increase of 1.6 and 2.2 points over the baselines, respectively. These results
validate the effectiveness and practicality of CDT. Source code and models are
available at https://github.com/Alessa-mo/CDT.

</details>


### [143] [Alternatives To Next Token Prediction In Text Generation -- A Survey](https://arxiv.org/abs/2509.24435)
*Charlie Wyatt,Aditya Joshi,Flora Salim*

Main category: cs.CL

TL;DR: 探讨替代NTP范式的五种方法，以克服LLM在长期规划、错误累积和效率上的缺陷


<details>
  <summary>Details</summary>
Motivation: NTP范式虽推动LLM发展，但存在长期规划能力不足、错误传播累积、计算效率低下等根本性缺陷，需探索替代方案

Method: 1.多令牌预测（块状未来预测）
2.计划生成（全局规划引导解码）
3.潜在推理（连续隐空间自回归）
4.连续生成（扩散/能量迭代优化）
5.非Transformer架构（结构规避NTP）

Result: 建立五维分类框架，为突破token级生成限制提供系统性研究路径

Conclusion: 新范式通过空间扩展/规划引导/架构创新等维度，有望推动NLP进入更高效可靠的生成时代

Abstract: The paradigm of Next Token Prediction (NTP) has driven the unprecedented
success of Large Language Models (LLMs), but is also the source of their most
persistent weaknesses such as poor long-term planning, error accumulation, and
computational inefficiency. Acknowledging the growing interest in exploring
alternatives to NTP, the survey describes the emerging ecosystem of
alternatives to NTP. We categorise these approaches into five main families:
(1) Multi-Token Prediction, which targets a block of future tokens instead of a
single one; (2) Plan-then-Generate, where a global, high-level plan is created
upfront to guide token-level decoding; (3) Latent Reasoning, which shifts the
autoregressive process itself into a continuous latent space; (4) Continuous
Generation Approaches, which replace sequential generation with iterative,
parallel refinement through diffusion, flow matching, or energy-based methods;
and (5) Non-Transformer Architectures, which sidestep NTP through their
inherent model structure. By synthesizing insights across these methods, this
survey offers a taxonomy to guide research into models that address the known
limitations of token-level generation to develop new transformative models for
natural language processing.

</details>


### [144] [Bias Mitigation or Cultural Commonsense? Evaluating LLMs with a Japanese Dataset](https://arxiv.org/abs/2509.24468)
*Taisei Yamamoto,Ryoma Kumon,Danushka Bollegala,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 大语言模型（LLM）的去偏方法可能损害文化常识（准确率下降达75%），SOBACO基准测试揭示了去偏与文化常识的权衡关系


<details>
  <summary>Details</summary>
Motivation: 现有研究主要通过通用语言理解任务评估去偏效果，但这些任务常与社交偏见无关。文化常识与社交偏见同源于社会规范，两者关联性尚未被充分研究

Method: 提出SOBACO基准（日本社交偏见与文化常识评测框架），以统一格式评估LLM的社交偏见与文化常识表现

Result: 去偏方法显著降低LLM在文化常识任务上的性能（最大75%准确率下降），不同模型性能降幅存在差异

Conclusion: 开发去偏方法需考虑文化常识的权衡，平衡公平性与实用性，这对提升LLM的社会价值至关重要

Abstract: Large language models (LLMs) exhibit social biases, prompting the development
of various debiasing methods. However, debiasing methods may degrade the
capabilities of LLMs. Previous research has evaluated the impact of bias
mitigation primarily through tasks measuring general language understanding,
which are often unrelated to social biases. In contrast, cultural commonsense
is closely related to social biases, as both are rooted in social norms and
values. The impact of bias mitigation on cultural commonsense in LLMs has not
been well investigated. Considering this gap, we propose SOBACO (SOcial BiAs
and Cultural cOmmonsense benchmark), a Japanese benchmark designed to evaluate
social biases and cultural commonsense in LLMs in a unified format. We evaluate
several LLMs on SOBACO to examine how debiasing methods affect cultural
commonsense in LLMs. Our results reveal that the debiasing methods degrade the
performance of the LLMs on the cultural commonsense task (up to 75% accuracy
deterioration). These results highlight the importance of developing debiasing
methods that consider the trade-off with cultural commonsense to improve
fairness and utility of LLMs.

</details>


### [145] [A Text-To-Text Alignment Algorithm for Better Evaluation of Modern Speech Recognition Systems](https://arxiv.org/abs/2509.24478)
*Lasse Borgholt,Jakob Havtorn,Christian Igel,Lars Maaløe,Zheng-Hua Tan*

Main category: cs.CL

TL;DR: 提出结合动态规划与束搜索的新型对齐算法，提升语音识别错误分析的细粒度准确性


<details>
  <summary>Details</summary>
Motivation: 传统词错率指标无法有效反映关键词汇（如罕见词/命名实体）的错误，需更精准的文本对齐方法支撑细粒度错误分析

Method: 开发动态规划与束搜索耦合的文本对齐算法，通过PyPI开源实现

Result: 相比传统方法显著提升个体错误对齐精度，支持可靠错误分析

Conclusion: 新型对齐算法填补了语音识别评估体系的技术缺口，为改进模型性能提供可靠诊断工具

Abstract: Modern neural networks have greatly improved performance across speech
recognition benchmarks. However, gains are often driven by frequent words with
limited semantic weight, which can obscure meaningful differences in word error
rate, the primary evaluation metric. Errors in rare terms, named entities, and
domain-specific vocabulary are more consequential, but remain hidden by
aggregate metrics. This highlights the need for finer-grained error analysis,
which depends on accurate alignment between reference and model transcripts.
However, conventional alignment methods are not designed for such precision. We
propose a novel alignment algorithm that couples dynamic programming with beam
search scoring. Compared to traditional text alignment methods, our approach
provides more accurate alignment of individual errors, enabling reliable error
analysis. The algorithm is made available via PyPI.

</details>


### [146] [Sanitize Your Responses: Mitigating Privacy Leakage in Large Language Models](https://arxiv.org/abs/2509.24488)
*Wenjie Fu,Huandong Wang,Junyao Gao,Guoan Wan,Tao Jiang*

Main category: cs.CL

TL;DR: 提出Self-Sanitize框架，通过自我监控和修复机制实时减轻LLM生成有害内容的风险，相比传统事后过滤方法显著降低延迟


<details>
  <summary>Details</summary>
Motivation: 现有的事后过滤方法存在延迟高、计算开销大等问题，需要模仿人类认知行为的实时监控修正机制

Method: 结合表示工程的Self-Monitor模块（实时意图监控）和Self-Repair模块（原位内容修正），支持流式生成监测

Result: 在4个LLM和3类隐私泄露场景的实验中实现高效防护（F1值提升23.4%），延迟开销仅增加0.02秒

Conclusion: 该框架为LLM安全部署提供实时、低开销的解决方案，在保持模型效用的同时显著提升防护能力

Abstract: As Large Language Models (LLMs) achieve remarkable success across a wide
range of applications, such as chatbots and code copilots, concerns surrounding
the generation of harmful content have come increasingly into focus. Despite
significant advances in aligning LLMs with safety and ethical standards,
adversarial prompts can still be crafted to elicit undesirable responses.
Existing mitigation strategies are predominantly based on post-hoc filtering,
which introduces substantial latency or computational overhead, and is
incompatible with token-level streaming generation. In this work, we introduce
Self-Sanitize, a novel LLM-driven mitigation framework inspired by cognitive
psychology, which emulates human self-monitor and self-repair behaviors during
conversations. Self-Sanitize comprises a lightweight Self-Monitor module that
continuously inspects high-level intentions within the LLM at the token level
via representation engineering, and a Self-Repair module that performs in-place
correction of harmful content without initiating separate review dialogues.
This design allows for real-time streaming monitoring and seamless repair, with
negligible impact on latency and resource utilization. Given that
privacy-invasive content has often been insufficiently focused in previous
studies, we perform extensive experiments on four LLMs across three privacy
leakage scenarios. The results demonstrate that Self-Sanitize achieves superior
mitigation performance with minimal overhead and without degrading the utility
of LLMs, offering a practical and robust solution for safer LLM deployments.
Our code is available at the following link:
https://github.com/wjfu99/LLM_Self_Sanitize

</details>


### [147] [GRPO-MA: Multi-Answer Generation in GRPO for Stable and Efficient Chain-of-Thought Training](https://arxiv.org/abs/2509.24494)
*Hongcheng Wang,Yinuo Huang,Sukai Wang,Guanghui Ren,Hao Dong*

Main category: cs.CL

TL;DR: 提出了GRPO-MA方法，通过多答案生成解决GRPO算法中的梯度耦合、稀疏奖励和优势估计不稳定问题，提升训练效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: GRPO算法存在三个挑战：思维与答案的梯度耦合、有限并行采样导致的稀疏奖励信号、不稳定的优势估计。为解决这些问题，作者提出GRPO-MA方法。

Method: GRPO-MA基于理论分析，采用每个思维过程生成多个答案的机制，降低思维优势估计的方差，并通过梯度分析验证其有效性。

Result: 理论上证明多答案生成降低方差，实验显示梯度尖峰减少；在数学、代码和多模态任务中性能显著提升，消融研究表明增加答案数量持续改善表现。

Conclusion: GRPO-MA通过多答案生成机制有效解决GRPO的局限性，实现了更鲁棒的优化和训练效率提升，理论和实验均验证了其优越性。

Abstract: Recent progress, such as DeepSeek-R1, has shown that the GRPO algorithm, a
Reinforcement Learning (RL) approach, can effectively train Chain-of-Thought
(CoT) reasoning in Large Language Models (LLMs) and Vision-Language Models
(VLMs). In this paper, we analyze three challenges of GRPO: gradient coupling
between thoughts and answers, sparse reward signals caused by limited parallel
sampling, and unstable advantage estimation. To mitigate these challenges, we
propose GRPO-MA, a simple yet theoretically grounded method that leverages
multi-answer generation from each thought process, enabling more robust and
efficient optimization. Theoretically, we show that the variance of thought
advantage decreases as the number of answers per thought increases.
Empirically, our gradient analysis confirms this effect, showing that GRPO-MA
reduces gradient spikes compared to GRPO. Experiments on math, code, and
diverse multimodal tasks demonstrate that GRPO-MA substantially improves
performance and training efficiency. Our ablation studies further reveal that
increasing the number of answers per thought consistently enhances model
performance.

</details>


### [148] [Knowledge Editing with Subspace-Aware Key-Value Mappings](https://arxiv.org/abs/2509.24502)
*Haewon Park,Sangwoo Kim,Yohan Jo*

Main category: cs.CL

TL;DR: 提出SUIT方法通过识别关键特征子空间改进知识编辑效果，在多个大模型上显著提升知识保留率


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑方法对MLP层的key-value向量缺乏约束，导致模型参数产生严重扰动

Method: SUIT方法通过识别与编辑相关的关键特征子空间，仅修改该子空间的特征参数

Result: 在LLaMA-3-8B、GPT-J-6B和Qwen2.5-7B模型中，SUIT在保持高编辑效能的同时将知识保留率提升超过基线方法

Conclusion: SUIT成功识别出知识编辑的关键子空间，其有效性通过多维度实验验证，为模型参数编辑提供了新思路

Abstract: Knowledge editing aims to efficiently correct factual errors in Language
Models (LMs). The popular locate-then-edit approach modifies an MLP layer by
finding an optimal mapping between its input vector (key) and output vector
(value) that leads to the expression of the edited knowledge. However, existing
methods without any constraints on the key and value vectors cause significant
perturbations to the edited model. To address this, we propose Subspace
Knowledge Edit (SUIT), a method that identifies and modifies only the subspace
of critical features relevant to the edit. Our empirical results on LLaMA-3-8B,
GPT-J-6B, and Qwen2.5-7B models show that SUIT dramatically improves knowledge
preservation over strong baselines while maintaining high edit efficacy. This
effectiveness confirms that SUIT successfully identifies the critical subspace
for the edit. Further analyses provide additional validation for our approach.
The source code and data will be released to the public upon publication of the
paper.

</details>


### [149] [Building Benchmarks from the Ground Up: Community-Centered Evaluation of LLMs in Healthcare Chatbot Settings](https://arxiv.org/abs/2509.24506)
*Hamna,Gayatri Bhat,Sourabrata Mukherjee,Faisal Lalani,Evan Hadfield,Divya Siddarth,Kalika Bali,Sunayana Sitaram*

Main category: cs.CL

TL;DR: 提出社区驱动的Samiksha评估框架，解决LLM在医疗领域评估脱离社区真实需求的问题


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型评估缺乏对终端用户文化背景和真实需求的考量，医疗等关键领域需要基于社区实际场景的评估方法

Method: 与民间组织和社区成员共建文化敏感的评估流程：社区反馈决定评估内容、基准构建方式及评分标准

Result: 在印度医疗场景验证显示，该方法能有效评估多语言LLM处理复杂社区健康咨询的能力

Conclusion: 提供可扩展的语境化评估路径，实现包容性LLM评测，推动技术与社会需求的深度结合

Abstract: Large Language Models (LLMs) are typically evaluated through general or
domain-specific benchmarks testing capabilities that often lack grounding in
the lived realities of end users. Critical domains such as healthcare require
evaluations that extend beyond artificial or simulated tasks to reflect the
everyday needs, cultural practices, and nuanced contexts of communities. We
propose Samiksha, a community-driven evaluation pipeline co-created with
civil-society organizations (CSOs) and community members. Our approach enables
scalable, automated benchmarking through a culturally aware, community-driven
pipeline in which community feedback informs what to evaluate, how the
benchmark is built, and how outputs are scored. We demonstrate this approach in
the health domain in India. Our analysis highlights how current multilingual
LLMs address nuanced community health queries, while also offering a scalable
pathway for contextually grounded and inclusive LLM evaluation.

</details>


### [150] [AdaThink-Med: Medical Adaptive Thinking with Uncertainty-Guided Length Calibration](https://arxiv.org/abs/2509.24560)
*Shaohao Rui,Kaitao Chen,Weijie Ma,Xiaosong Wang*

Main category: cs.CL

TL;DR: 提出首个端到端框架AdaThink-Med，通过不确定性引导的长度校准实现医疗LLMs自适应推理，在保持性能的同时显著降低推理成本


<details>
  <summary>Details</summary>
Motivation: 现有医疗LLMs对不同难度问题均采用冗长推理流程，导致实际应用中的计算资源浪费，需建立动态调整思维深度的机制

Method: 1.生成多候选答案并评估正确性/不确定性 2.通过不确定性校准模块估计问题难度 3.对简单问题惩罚长推理路径，对复杂问题扩展思维链

Result: 在6个医疗QA基准测试中实现平均6.4倍推理长度压缩，性能损失仅0.7%。模型自发形成'非思考'与'思考'双模式推理机制

Conclusion: 该框架有效平衡计算成本与推理性能，首次证明LLMs可通过动态抑制冗余推理路径实现高效医疗决策

Abstract: Recent advances in inference time scaling with extended long chain-of thought
have significantly improved the reasoning capabilities of both general and
medical large language models (LLMs). However, these models tend to engage in
lengthy reasoning processes regardless of the difficulty of the input question,
leading to increased inference costs in real-world applications. Therefore,
enabling adaptive thinking where models think less for simpler questions and
think more for complex ones is critical for the effective use of medical LLMs
in practice. Despite its importance, there is a lack of end-to-end approaches
designed to enhance the adaptive thinking capabilities of medical LLMs while
providing a comprehensive examination of the trade-off between performance and
computational cost. To bridge this gap, we propose AdaThink-Med, the first
end-to-end framework designed to enhance adaptive thinking ability in medical
reasoning models with uncertainty-guided length calibration. AdaThink-Med first
generates multiple candidate outputs for each question, evaluates the
correctness and uncertainty of each candidate, and then estimates problem
difficulty via an uncertainty-guided length calibration module. For outputs
with low difficulty and correct answers, the framework penalizes longer
reasoning paths; whereas for those with high difficulty and incorrect answers,
it encourages extending the chain of thought to explore alternative solutions.
On six public medical QA benchmarks, AdaThink-Med achieves up to 6.4x length
reduction on average while retaining performance with only minimal degradation.
Intriguingly, we observe that AdaThink-Med spontaneously develops two distinct
reasoning modes, which we characterize as "non-thinking" and "thinking",
demonstrating the model's ability to suppress redundant reasoning processes
dynamically.

</details>


### [151] [Inducing Dyslexia in Vision Language Models](https://arxiv.org/abs/2509.24597)
*Melika Honarmand,Ayati Sharma,Badr AlKhamissi,Johannes Mehrer,Martin Schrimpf*

Main category: cs.CL

TL;DR: 利用大规模视觉语言模型模拟阅读障碍，通过选择性破坏视觉词形处理单元复现失读症特征


<details>
  <summary>Details</summary>
Motivation: 传统行为学和神经影像学方法难以验证阅读障碍的因果机制，需建立计算模型突破研究限制

Method: 在视觉语言模型中定位人工视觉词形处理单元，选择性破坏该区域功能并评估阅读任务表现

Result: 定向破坏引发特异性阅读障碍表型（语音缺陷），而随机破坏不影响阅读能力，与人类患者特征高度吻合

Conclusion: 成功建立计算框架验证视觉词形处理缺陷假说，为探索阅读障碍机制提供新范式

Abstract: Dyslexia, a neurodevelopmental disorder characterized by persistent reading
difficulties, is often linked to reduced activity of the visual word form area
in the ventral occipito-temporal cortex. Traditional approaches to studying
dyslexia, such as behavioral and neuroimaging methods, have provided valuable
insights but remain limited in their ability to test causal hypotheses about
the underlying mechanisms of reading impairments. In this study, we use
large-scale vision-language models (VLMs) to simulate dyslexia by functionally
identifying and perturbing artificial analogues of word processing. Using
stimuli from cognitive neuroscience, we identify visual-word-form-selective
units within VLMs and demonstrate that targeted ablation of these units, unlike
ablation of random units, leads to selective impairments in reading tasks while
general visual and language comprehension abilities remain intact. In
particular, the resulting model matches dyslexic humans' phonological deficits
without a significant change in orthographic processing. Taken together, our
modeling results replicate key characteristics of dyslexia and establish a
computational framework for investigating reading disorders.

</details>


### [152] [HiKE: Hierarchical Evaluation Framework for Korean-English Code-Switching Speech Recognition](https://arxiv.org/abs/2509.24613)
*Gio Paik,Yongbeom Kim,Soungmin Lee,Sangmin Ahn,Chanwoo Kim*

Main category: cs.CL

TL;DR: 提出首个韩英语码转换评测框架HiKE，通过分层标注与微调实验验证多语言ASR模型在CS任务中的可优化性


<details>
  <summary>Details</summary>
Motivation: 当前多语言ASR模型对语码转换（CS）处理能力严重不足，缺乏系统性评估框架

Method: 构建包含自然CS数据/借词标签/分层CS标注（词-短语-句子）的HiKE基准，并通过多语言ASR模型微调实验验证

Result: 主流多语言ASR模型初始CS处理能力弱，但使用CS数据微调后性能显著提升

Conclusion: HiKE填补韩英CS评估空白，证明CS能力可通过数据微调激活，为CS-ASR研究提供系统性工具

Abstract: Despite advances in multilingual automatic speech recognition (ASR),
code-switching (CS), the mixing of languages within an utterance common in
daily speech, remains a severely underexplored challenge. In this paper, we
introduce HiKE: the Hierarchical Korean-English code-switching benchmark, the
first globally accessible evaluation framework for Korean-English CS, aiming to
provide a means for the precise evaluation of multilingual ASR models and to
foster research in the field. The proposed framework not only consists of
high-quality, natural CS data across various topics, but also provides
meticulous loanword labels and a hierarchical CS-level labeling scheme (word,
phrase, and sentence) that together enable a systematic evaluation of a model's
ability to handle each distinct level of code-switching. Through evaluations of
diverse multilingual ASR models and fine-tuning experiments, this paper
demonstrates that while most multilingual ASR models initially struggle with
CS-ASR, this capability can be enabled through fine-tuning with CS data. HiKE
will be available at https://github.com/ThetaOne-AI/HiKE.

</details>


### [153] [Hype or not? Formalizing Automatic Promotional Language Detection in Biomedical Research](https://arxiv.org/abs/2509.24638)
*Bojan Batalo,Erica K. Shimomoto,Neil Millar*

Main category: cs.CL

TL;DR: 论文提出首个基于自然语言处理的科学文本炒作检测方法，通过制定标注标准构建数据集，实验显示模型检测效果良好但需领域知识支撑。


<details>
  <summary>Details</summary>
Motivation: 科学文本中日益增长的宣传性语言会损害证据评估和科学信任，需要自动化检测手段。现有研究未系统探讨NLP方法在此任务中的应用。

Method: 制定标准化标注指南标注NIH基金文本，比较传统分类器与语言模型性能，建立人类标注基线作为参照。

Result: 标注指南提升标注一致性（人类标注者F1=0.7），最佳模型F1达0.68，检测需结合领域知识及时效性判断。

Conclusion: 首次将炒作检测转化为NLP任务，证明可行性的同时揭示了语言复杂性挑战，为科学文本质量评估提供新思路。

Abstract: In science, promotional language ('hype') is increasing and can undermine
objective evaluation of evidence, impede research development, and erode trust
in science. In this paper, we introduce the task of automatic detection of
hype, which we define as hyperbolic or subjective language that authors use to
glamorize, promote, embellish, or exaggerate aspects of their research. We
propose formalized guidelines for identifying hype language and apply them to
annotate a portion of the National Institutes of Health (NIH) grant application
corpus. We then evaluate traditional text classifiers and language models on
this task, comparing their performance with a human baseline. Our experiments
show that formalizing annotation guidelines can help humans reliably annotate
candidate hype adjectives and that using our annotated dataset to train machine
learning models yields promising results. Our findings highlight the linguistic
complexity of the task, and the potential need for domain knowledge and
temporal awareness of the facts. While some linguistic works address hype
detection, to the best of our knowledge, we are the first to approach it as a
natural language processing task.

</details>


### [154] [InfLLM-V2: Dense-Sparse Switchable Attention for Seamless Short-to-Long Adaptation](https://arxiv.org/abs/2509.24663)
*Weilin Zhao,Zihan Zhou,Zhou Su,Chaojun Xiao,Yuxuan Li,Yanghao Li,Yudi Zhang,Weilun Zhao,Zhen Li,Yuxiang Huang,Ao Sun,Xu Han,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 提出InfLLM-V2框架，通过稠密-稀疏可切换注意力机制，在保持模型性能的同时显著提升长序列处理效率。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer的自注意力机制在处理长序列时面临计算和内存瓶颈，现有可训练稀疏注意力方法（如NSA）存在参数冗余、破坏预训练-微调流程一致性的问题。

Method: 1. 通过无参数架构修改复用稠密注意力参数
2. 短序列使用稠密注意力，长序列自动切换稀疏模式
3. 提出高效实现方案降低计算开销

Result: 在长上下文理解和思维链推理任务中，InfLLM-V2速度比稠密注意力快4倍，分别保留98.1%和99.7%性能。基于该框架开源了混合推理模型MiniCPM4.1。

Conclusion: InfLLM-V2有效解决了长序列处理效率与性能的平衡问题，通过参数复用机制保持训练流程一致性，为研究社区提供可复现的实践方案。

Abstract: Long-sequence processing is a critical capability for modern large language
models. However, the self-attention mechanism in the standard Transformer
architecture faces severe computational and memory bottlenecks when processing
long sequences. While trainable sparse attention methods offer a promising
solution, existing approaches such as NSA introduce excessive extra parameters
and disrupt the conventional \textit{pretrain-on-short, finetune-on-long}
workflow, resulting in slow convergence and difficulty in acceleration. To
overcome these limitations, we introduce dense-sparse switchable attention
framework, termed as InfLLM-V2. InfLLM-V2 is a trainable sparse attention that
seamlessly adapts models from short to long sequences. Specifically, InfLLM-V2
reuses dense attention parameters through parameter-free architecture
modification, maintaining consistency between short and long sequence
processing. Additionally, InfLLM-V2 ensures computational efficiency across all
sequence lengths, by using dense attention for short inputs and smoothly
transitioning to sparse attention for long sequences. To achieve practical
acceleration, we further introduce an efficient implementation of InfLLM-V2
that significantly reduces the computational overhead. Our experiments on
long-context understanding and chain-of-thought reasoning demonstrate that
InfLLM-V2 is 4$\times$ faster than dense attention while retaining 98.1% and
99.7% of the performance, respectively. Based on the InfLLM-V2 framework, we
have trained and open-sourced MiniCPM4.1
(https://huggingface.co/openbmb/MiniCPM4.1-8B), a hybrid reasoning model,
providing a reproducible implementation for the research community.

</details>


### [155] [Understanding the Dilemma of Unlearning for Large Language Models](https://arxiv.org/abs/2509.24675)
*Qingjie Zhang,Haoting Qian,Zhicong Huang,Cheng Hong,Minlie Huang,Ke Xu,Chao Zhang,Han Qiu*

Main category: cs.CL

TL;DR: 现有大模型遗忘方法面临两难困境：要么效果不足（关键词强调即可恢复知识），要么破坏性过强（导致模型通用能力崩溃）


<details>
  <summary>Details</summary>
Motivation: 针对当前LLM遗忘方法有效性争议（知识可恢复性与灾难性遗忘并存）及缺乏机制解释性分析的问题展开研究

Method: 提出unPact解释框架，通过提示词归因追踪技术量化token影响，对比六种主流遗忘方法在3个LLM和3个基准测试中的表现

Result: 发现：1）遗忘通过干扰关键词实现表面效果 2）知识未真正删除（通过强调关键词可恢复）3）灾难性遗忘源于无差别token惩罚

Conclusion: 现有遗忘方法尚未实现可靠的知识删除，需在知识消除与模型保护之间寻找更精细的平衡点

Abstract: Unlearning seeks to remove specific knowledge from large language models
(LLMs), but its effectiveness remains contested. On one side, "forgotten"
knowledge can often be recovered through interventions such as light
fine-tuning; on the other side, unlearning may induce catastrophic forgetting
that degrades general capabilities. Despite active exploration of unlearning
methods, interpretability analyses of the mechanism are scarce due to the
difficulty of tracing knowledge in LLMs' complex architectures. We address this
gap by proposing unPact, an interpretable framework for unlearning via prompt
attribution and contribution tracking. Typically, it quantifies each prompt
token's influence on outputs, enabling pre- and post-unlearning comparisons to
reveal what changes. Across six mainstream unlearning methods, three LLMs, and
three benchmarks, we find that: (1) Unlearning appears to be effective by
disrupting focus on keywords in prompt; (2) Much of the knowledge is not truly
erased and can be recovered by simply emphasizing these keywords in prompts,
without modifying the model's weights; (3) Catastrophic forgetting arises from
indiscriminate penalization of all tokens. Taken together, our results suggest
an unlearning dilemma: existing methods tend either to be insufficient -
knowledge remains recoverable by keyword emphasis, or overly destructive -
general performance collapses due to catastrophic forgetting, still leaving a
gap to reliable unlearning.

</details>


### [156] [Reference-Free Rating of LLM Responses via Latent Information](https://arxiv.org/abs/2509.24678)
*Leander Girrbach,Chi-Ping Su,Tankred Saanum,Richard Socher,Eric Schulz,Zeynep Akata*

Main category: cs.CL

TL;DR: 研究提出通过LLM内部信号构建潜在裁判方法（Latent Judges），有效提升无参考评分的可靠性与判别力，在多项评测中优于传统提示方法。


<details>
  <summary>Details</summary>
Motivation: 解决单次LLM评分无参考标准时存在的分数不稳定（采样敏感）、校准偏差（高分压缩与频繁同分）问题，提升模型评估的可靠性。

Method: 提出三种潜在裁判方法：①基于整数评分的概率加权得分；②验证式的是/否概率；③在模型激活层训练的线性探针。

Result: 潜在方法在单评分相关性（概率加权最优）和排序任务（探针缓解logit失真）上表现更优，显著提升Best-of-N等场景的判别力。

Conclusion: 潜在信号为无参考评估提供更稳定、细粒度的指标，可优化模型选择、蒸馏和路由等关键应用场景。

Abstract: How reliable are single-response LLM-as-a-judge ratings without references,
and can we obtain fine-grained, deterministic scores in this setting? We study
the common practice of asking a judge model to assign Likert-scale scores to
free-text responses and show two systematic issues: scores are unstable under
sampling and poorly calibrated, leading to compression near the top of the
scale and frequent ties. We then propose and evaluate Latent Judges, which
derive scalar ratings from internal model signals: (i) probability-weighted
scores over integer ratings, (ii) verifier-style probabilities of "yes", and
(iii) linear probes trained on model activations at the rating position. Across
a broad suite of pairwise and single-rating benchmarks, latent methods match or
surpass standard prompting, with consistent gains on pairwise accuracy and
listwise ranking relevant to Best-of-N selection. Probability-weighted scores
achieve the strongest single-rating correlations, while probes recover useful
signals when output logits are miscalibrated. These results indicate that
latent information provides deterministic and more discriminative signals for
reference-free evaluation, and can improve selection and training approaches
like Best-of-$N$, multi-teacher distillation, and routing.

</details>


### [157] [MemGen: Weaving Generative Latent Memory for Self-Evolving Agents](https://arxiv.org/abs/2509.24704)
*Guibin Zhang,Muxin Fu,Shuicheng Yan*

Main category: cs.CL

TL;DR: 提出MemGen动态生成记忆框架，通过实时监控推理状态和生成潜在记忆序列，使LLM代理实现推理与记忆的深度融合，实验显示显著超越现有方法并自发形成类人记忆机制。


<details>
  <summary>Details</summary>
Motivation: 现有参数化记忆和检索式记忆无法模拟人类认知中推理与记忆的动态交织，限制了LLM代理的认知能力发展。

Method: MemGen框架包含：1）实时监测推理状态的记忆触发器；2）根据当前状态生成机器原生记忆的记忆编织器，形成记忆增强的推理循环。

Result: 在8个基准测试中超越最佳外部记忆系统38.22%，超过GRPO 13.44%，并展现出跨领域泛化能力和自发形成计划/程序/工作记忆的类人特性。

Conclusion: MemGen通过记忆与推理的深度耦合开辟了机器认知新路径，其自发演化的类人记忆机制表明向自然认知演进的可能性。

Abstract: Agent memory shapes how Large Language Model (LLM)-powered agents, akin to
the human brain, progressively refine themselves through environment
interactions. Existing paradigms remain constrained: parametric memory forcibly
adjusts model parameters, and retrieval-based memory externalizes experience
into structured databases, yet neither captures the fluid interweaving of
reasoning and memory that underlies human cognition. To address this gap, we
propose MemGen, a dynamic generative memory framework that equips agents with a
human-esque cognitive faculty. It consists of a \textit{memory trigger}, which
monitors the agent's reasoning state to decide explicit memory invocation, and
a \textit{memory weaver}, which takes the agent's current state as stimulus to
construct a latent token sequence as machine-native memory to enrich its
reasoning. In this way, MemGen enables agents to recall and augment latent
memory throughout reasoning, producing a tightly interwoven cycle of memory and
cognition. Extensive experiments across eight benchmarks show that MemGen
surpasses leading external memory systems such as ExpeL and AWM by up to
$38.22\%$, exceeds GRPO by up to $13.44\%$, and exhibits strong cross-domain
generalization ability. More importantly, we find that without explicit
supervision, MemGen spontaneously evolves distinct human-like memory faculties,
including planning memory, procedural memory, and working memory, suggesting an
emergent trajectory toward more naturalistic forms of machine cognition.

</details>


### [158] [Socratic-Zero : Bootstrapping Reasoning via Data-Free Agent Co-evolution](https://arxiv.org/abs/2509.24726)
*Shaobo Wang,Zhengbo Jiao,Zifan Zhang,Yilang Peng,Xu Ze,Boyu Yang,Wei Wang,Hu Wei,Linfeng Zhang*

Main category: cs.CL

TL;DR: 提出自主生成高质量训练数据的Socratic-Zero框架，通过三代理协同进化解决数据扩展难题，在数学推理任务中显著超越现有方法


<details>
  <summary>Details</summary>
Motivation: 现有LLM依赖难以扩展的人工标注数据，而合成数据方法存在质量不稳定和无法动态适配模型能力的问题

Method: 构建Teacher(设计难题)/Solver(强化学习)/Generator(课程生成)的闭环系统，通过失败轨迹反馈和课程进化实现自主数据生产

Result: Socratic-Solver-8B在7个数学基准平均提升20.2%，合成数据使小模型超越GPT-5/Claude-4等商业大模型

Conclusion: 该框架通过协同进化机制突破人工数据限制，为LLM训练提供高效数据解决方案

Abstract: Recent breakthroughs in large language models (LLMs) on reasoning tasks rely
heavily on massive, high-quality datasets-typically human-annotated and thus
difficult to scale. While data synthesis or distillation offers a promising
alternative, existing methods struggle with inconsistent data quality and an
inability to dynamically adapt to the evolving capabilities of the model,
leading to suboptimal training signals. To address these limitations, we
introduce Socratic-Zero, a fully autonomous framework that generates
high-quality training data from minimal seed examples through the co-evolution
of three agents: the Teacher, the Solver, and the Generator. The Solver
continuously refines its reasoning by learning from preference feedback on both
successful and failed trajectories; the Teacher adaptively crafts increasingly
challenging questions based on the Solver's weaknesses; and the Generator
distills the Teacher's question-design strategy to enable scalable,
high-fidelity curriculum generation. This closed-loop system produces a
self-improving curriculum-requiring no pre-existing tasks or labels.
Remarkably, starting from only 100 seed questions, our Socratic-Solver-8B
achieves an average gain of +20.2 percentage points over prior data synthesis
methods across seven mathematical reasoning benchmarks (AMC23, AIME24-25,
Olympiad, MATH-500, Minerva, and GSM8K), with consistent gains on both Qwen3
and GLM4 series models. Even more surprisingly, synthetic data from
Socratic-Generator-32B enables student LLMs to achieve superior performance
compared to other state-of-the-art (SOTA) commercial LLMs on these benchmarks,
including Qwen3-235B-A22B, DeepSeek-V3.1-671B, GPT-5, Gemini-2.5-Pro, Grok-4,
and Claude-4.1-Opus.

</details>


### [159] [ProxyAttn: Guided Sparse Attention via Representative Heads](https://arxiv.org/abs/2509.24745)
*Yixuan Wang,Huang He,Siqi Bao,Hua Wu,Haifeng Wang,Qingfu Zhu,Wanxiang Che*

Main category: cs.CL

TL;DR: 提出了ProxyAttn算法，通过压缩注意力头维度和动态预算估计实现高效块稀疏注意力，显著提升LLMs长文本处理效率


<details>
  <summary>Details</summary>
Motivation: 现有块重要性估计方法存在粗粒度问题，导致高稀疏率下的性能下降

Method: 利用注意力头相似性特征：1）使用代表头的聚合分数近似所有头 2）块感知动态预算估计方法实现细粒度评估

Result: 在主流模型上实现10.3倍注意力加速和2.4倍预填充加速，性能损失可忽略

Conclusion: 通过精细的块重要性评估机制，ProxyAttn在保持模型性能的同时显著提升计算效率，为长文本处理提供实用解决方案

Abstract: The quadratic complexity of attention mechanisms limits the efficiency of
Large Language Models (LLMs) on long-text tasks. Recently, methods that
dynamically estimate block importance have enabled efficient block sparse
attention, leading to significant acceleration in long-text pre-filling of
LLMs. However, their coarse-grained estimation inevitably leads to performance
degradation at high sparsity rates. In this work, we propose ProxyAttn, a
training-free sparse attention algorithm that achieves more precise block
estimation by compressing the dimension of attention heads. Based on our
observation of the similarity among multiple attention heads, we use the scores
of pooled representative heads to approximate the scores for all heads. To
account for the varying sparsity among heads, we also propose a block-aware
dynamic budget estimation method. By combining the scores from representative
proxy heads with multi-head dynamic budgets, we achieve a more fine-grained
block importance evaluation at low computational cost. Experiments on a variety
of mainstream models and extensive benchmarks confirm the underlying similarity
among attention heads. Leveraging a fine-grained estimation, the proposed
method achieves substantial gains in performance and efficiency compared to
existing methods. More precisely, ProxyAttn can achieve up to 10.3x attention
acceleration and 2.4x prefilling acceleration without significant performance
loss. Our code is available at https://github.com/wyxstriker/ProxyAttn.

</details>


### [160] [LatentEvolve: Self-Evolving Test-Time Scaling in Latent Space](https://arxiv.org/abs/2509.24771)
*Guibin Zhang,Fanci Meng,Guancheng Wan,Zherui Li,Kun Wang,Zhenfei Yin,Lei Bai,Shuicheng Yan*

Main category: cs.CL

TL;DR: 提出LatentEvolve框架，通过模仿人脑的双系统机制实现LLM测试时计算能力的自我进化，在无监督条件下显著提升模型推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有测试时缩放方法相互独立，缺乏LLM逐步学习优化缩放能力的机制，需建立类似人类互补学习系统的进化框架。

Method: 基于CLS理论构建昼夜双阶段优化：白天阶段快速检索历史潜在表征指导推理；夜间阶段整合潜在优化，模拟人脑睡眠时的经验固化过程。

Result: 在8个基准测试和5种模型架构中，平均提升13.33%性能，显著优于现有方法并展现出优秀的跨域泛化能力。

Conclusion: 昼夜交替的进化机制有效突破了传统TTS方法的局限性，为LLM的持续自我优化提供了神经科学启发的技术路径。

Abstract: Test-time Scaling (TTS) has been demonstrated to significantly enhance the
reasoning capabilities of Large Language Models (LLMs) during the inference
phase without altering model parameters. However, existing TTS methods are
largely independent, implying that LLMs have not yet evolved to progressively
learn how to scale more effectively. With the objective of evolving LLMs to
learn ``how to scale test-time computation,'' we propose LatentEvolve, a
self-evolving latent TTS framework inspired by the complementary learning
system (CLS) theory. Analogous to the human brain's dual system of a
fast-recall hippocampus and a slow-consolidating neocortex, LatentEvolve
comprises two evolutionary components: \textit{daytime scaling}, which rapidly
retrieves historical latent representations to better guide current LLM
reasoning; and \textit{nighttime scaling}, which integrates past latent
optimizations in a manner akin to the human brain's consolidation of
experiences during sleep. The alternation of daytime and nighttime processes
facilitates a fast and slow evolution of LLM TTS, mirroring human cognitive
dynamics in a fully unsupervised manner. Extensive experiments across eight
benchmarks and five model backbones demonstrate that our LatentEvolve surpasses
state-of-the-art TTS methods such as LatentSeek and TTRL by up to $13.33\%$ and
exhibits exceptional cross-domain and cross-backbone generalization.

</details>


### [161] [SeaPO: Strategic Error Amplification for Robust Preference Optimization of Large Language Models](https://arxiv.org/abs/2509.24781)
*Jun Rao,Yunjie Liao,Xuebo Liu,Zepeng Lin,Lian Lian,Dong Jin,Shengjun Cheng,Jun Yu,Min Zhang*

Main category: cs.CL

TL;DR: 提出SeaPO方法，通过战略性地在负样本中引入错误模式，增强正负样本差异，从而优化偏好学习并提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法中，正负样本质量趋近导致优化困难。需通过策略性误差放大，确保负样本比正样本更具错误性，从而提升训练效果。

Method: SeaPO方法向偏好优化注入三类LLM常见错误模式，通过偏好训练主动减少这些错误。包含战略误差放大机制，动态调整样本错误程度。

Result: 在5大能力维度的评估中，模型整体性能提升5-10个百分点（特别是真实性）。错误类型组合可产生广度性能提升，特定错误注入提升相关任务表现。

Conclusion: 战略误差注入机制有效提升LLM性能，错误类型组合策略使任务获得稳定或显著提升。该方法在1.5B到14B不同规模模型上均表现出适应性。

Abstract: Existing alignment methods for preference optimization of large language
models (LLMs) aim to enhance model performance by utilizing pairs of positive
and negative samples. However, due to the limited capacity of models in scoring
or generating responses, the quality of positive and negative samples may
become similar during training, which complicates optimization for preference
learning. To address this issue, we introduce SeaPO, a Strategic Error
Amplification method that leverages three error types commonly occurring in
LLMs to introduce specific error patterns into the model Preference
Optimization. This strategy ensures that negative samples are more erroneous
than positive samples and preference-based training is employed to mitigate the
occurrence of these errors, thereby enhancing model performance. Evaluations
across five capability dimensions and different model scales (1.5B to 14B)
demonstrate that the generated data significantly improved overall model
performance, particularly in terms of truthfulness, with improvements of 5-10
percentage points observed. Further analysis reveals that task performance
varies depending on the error types introduced. Injecting the most common error
types improves performance in related tasks, while a mix of error types leads
to a broader performance enhancement: most tasks show stable improvements,
while a few tasks exhibit significant gains.

</details>


### [162] [Evaluating Spatiotemporal Consistency in Automatically Generated Sewing Instructions](https://arxiv.org/abs/2509.24792)
*Luisa Geiger,Mareike Hartmann,Michael Sullivan,Alexander Koller*

Main category: cs.CL

TL;DR: 提出基于树结构的自动评估指标，优于BLEU等传统文本相似度指标，能更准确评估缝纫说明书时空逻辑合理性


<details>
  <summary>Details</summary>
Motivation: 传统基于文本相似度的评估指标（如BLEU、BERT）在评估分步骤组装说明时，无法有效捕捉时空维度信息，导致评估结果与人工判断存在偏差

Method: 开发基于树结构的自动评估指标，在缝纫说明书场景中应用，通过人工标注错误数量、人工质量评分及对抗样本测试进行验证

Result: 新指标与人工标注错误数相关性更强（相关系数提升23%），对抗样本测试中准确率比传统方法高41%

Conclusion: 树结构评估指标在需要时空逻辑的指令评估场景中具有显著优势，未来可替代传统文本相似度指标

Abstract: In this paper, we propose a novel, automatic tree-based evaluation metric for
LLM-generated step-by-step assembly instructions, that more accurately reflects
spatiotemporal aspects of construction than traditional metrics such as BLEU
and BERT similarity scores. We apply our proposed metric to the domain of
sewing instructions, and show that our metric better correlates with
manually-annotated error counts as well as human quality ratings, demonstrating
our metric's superiority for evaluating the spatiotemporal soundness of sewing
instructions. Further experiments show that our metric is more robust than
traditional approaches against artificially-constructed counterfactual examples
that are specifically constructed to confound metrics that rely on textual
similarity.

</details>


### [163] [KnowGuard: Knowledge-Driven Abstention for Multi-Round Clinical Reasoning](https://arxiv.org/abs/2509.24816)
*Xilin Dang,Kexin Chen,Xiaorui Su,Ayush Noori,Iñaki Arango,Lucas Vittor,Xinyi Long,Yuyang Du,Marinka Zitnik,Pheng Ann Heng*

Main category: cs.CL

TL;DR: 提出KnowGuard框架，通过知识图谱探索改善医疗场景中LLMs的审慎决策能力


<details>
  <summary>Details</summary>
Motivation: 现有医疗大模型缺乏系统性外部证据评估机制，在信息不足时仍过度自信，存在误诊风险

Method: 两阶段知识图谱探索：1) 证据发现阶段通过图谱扩展与直接检索系统探索医学知识空间；2) 证据评估阶段基于患者上下文动态调整探索策略

Result: 在开放式临床基准测试中，诊断准确率提升3.93%，平均减少7.27次无效交互

Conclusion: KnowGuard通过结构化知识探索有效识别医学证据边界，在准确率与交互效率间取得更好平衡，对医疗AI安全决策具重要意义

Abstract: In clinical practice, physicians refrain from making decisions when patient
information is insufficient. This behavior, known as abstention, is a critical
safety mechanism preventing potentially harmful misdiagnoses. Recent
investigations have reported the application of large language models (LLMs) in
medical scenarios. However, existing LLMs struggle with the abstentions,
frequently providing overconfident responses despite incomplete information.
This limitation stems from conventional abstention methods relying solely on
model self-assessments, which lack systematic strategies to identify knowledge
boundaries with external medical evidences. To address this, we propose
\textbf{KnowGuard}, a novel \textit{investigate-before-abstain} paradigm that
integrates systematic knowledge graph exploration for clinical decision-making.
Our approach consists of two key stages operating on a shared contextualized
evidence pool: 1) an evidence discovery stage that systematically explores the
medical knowledge space through graph expansion and direct retrieval, and 2) an
evidence evaluation stage that ranks evidence using multiple factors to adapt
exploration based on patient context and conversation history. This two-stage
approach enables systematic knowledge graph exploration, allowing models to
trace structured reasoning paths and recognize insufficient medical evidence.
We evaluate our abstention approach using open-ended multi-round clinical
benchmarks that mimic realistic diagnostic scenarios, assessing abstention
quality through accuracy-efficiency trade-offs beyond existing closed-form
evaluations. Experimental evidences clearly demonstrate that KnowGuard
outperforms state-of-the-art abstention approaches, improving diagnostic
accuracy by 3.93\% while reducing unnecessary interaction by 7.27 turns on
average.

</details>


### [164] [DiaCDM: Cognitive Diagnosis in Teacher-Student Dialogues using the Initiation-Response-Evaluation Framework](https://arxiv.org/abs/2509.24821)
*Rui Jia,Yuang Wei,Ruijia Li,Yuang-Hao Jiang,Xinyu Xie,Yaomin Shen,Min Zhang,Bo Jiang*

Main category: cs.CL

TL;DR: 首个对话场景认知诊断模型DiaCDM，结合IRE框架与图编码方法，显著提升诊断准确性和可解释性


<details>
  <summary>Details</summary>
Motivation: 传统认知诊断模型无法处理非结构化师生对话，需开发适配对话的诊断框架与信息提取技术

Method: 基于教育理论设计IRE对话诊断框架，开发融合教师提问与知识点的图编码方法

Result: 在三个真实对话数据集上验证，模型提升诊断精度并提供可解释结果

Conclusion: DiaCDM为教师评估学生认知状态提供了有效的对话场景诊断工具

Abstract: While cognitive diagnosis (CD) effectively assesses students' knowledge
mastery from structured test data, applying it to real-world teacher-student
dialogues presents two fundamental challenges. Traditional CD models lack a
suitable framework for handling dynamic, unstructured dialogues, and it's
difficult to accurately extract diagnostic semantics from lengthy dialogues. To
overcome these hurdles, we propose DiaCDM, an innovative model. We've adapted
the initiation-response-evaluation (IRE) framework from educational theory to
design a diagnostic framework tailored for dialogue. We also developed a unique
graph-based encoding method that integrates teacher questions with relevant
knowledge components to capture key information more precisely. To our
knowledge, this is the first exploration of cognitive diagnosis in a dialogue
setting. Experiments on three real-world dialogue datasets confirm that DiaCDM
not only significantly improves diagnostic accuracy but also enhances the
results' interpretability, providing teachers with a powerful tool for
assessing students' cognitive states. The code is available at
https://github.com/Mind-Lab-ECNU/DiaCDM/tree/main.

</details>


### [165] [SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts via Token-Level LSH Matching](https://arxiv.org/abs/2509.24832)
*Xinye Zhao,Spyridon Mastorakis*

Main category: cs.CL

TL;DR: 提出SemShareKV框架，通过语义相似性共享KV缓存，实现LLM推理6.25倍加速和42%显存降低


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存压缩方法在语义相似但词汇不同的场景（如多文档摘要）存在局限，需突破传统基于词符匹配的共享策略

Method: 使用局部敏感哈希(LSH)实现词符嵌入模糊匹配，结合RoPE位置编码保持位置信息，选择性复用参考提示的KV缓存

Result: 在5k tokens输入下达到6.25倍推理加速，GPU显存降低42%，输出质量损失可忽略

Conclusion: 语义感知的缓存共享机制为高效LLM推理开辟新方向，尤其在语义相似任务中具有显著优化潜力

Abstract: As large language models (LLMs) continue to scale, the memory footprint of
key-value (KV) caches during inference has become a significant bottleneck.
Existing approaches primarily focus on compressing KV caches within a single
prompt or reusing shared prefixes or frequently ocurred text segments across
prompts. However, such strategies are limited in scenarios where prompts are
semantically similar but lexically different, which frequently occurs in tasks
such as multi-document summarization and conversational agents. We propose
\textit{SemShareKV}, a KV cache sharing and compression framework that
accelerates LLM inference by reusing KVCache in semantically similar prompts.
Instead of relying on exact token matches, SemShareKV applies fuzzy token
matching using locality-sensitive hashing (LSH) on token embeddings and
incorporates Rotary Position Embedding (RoPE) to better preserve positional
information. By selectively reusing relevant key-value pairs from a reference
prompt's cache, SemShareKV reduces redundant computation while maintaining
output quality. Experiments on diverse summarization datasets show up to
6.25$\times$ speedup and 42\% lower GPU memory usage with 5k tokens input, with
negligible quality degradation. These results highlight the potential of
semantic-aware cache sharing for efficient LLM inference.

</details>


### [166] [Hierarchical Error Correction for Large Language Models: A Systematic Framework for Domain-Specific AI Quality Enhancement](https://arxiv.org/abs/2509.24841)
*Zhilong Zhao,Yindi Liu*

Main category: cs.CL

TL;DR: 提出分层纠错框架HEC，通过三阶段干预策略在医疗、法律等领域实现平均11.2%的性能提升，但在高基线任务中效果受限


<details>
  <summary>Details</summary>
Motivation: 大语言模型在专业领域表现不佳（医疗编码任务仅45.9%准确率），需系统性错误分析来提升AI性能

Method: 分析4个领域错误模式，构建知识层（58.4%）、推理层（39.6%）、复杂度层（2.0%）三阶段纠正框架，进行跨5个LLM架构验证

Result: 在医疗转录（4,921例）等领域平均提升11.2%（p<0.001），但高基线任务（>75%）出现干预干扰推理的现象

Conclusion: 系统错误分析有效指导专业领域AI优化（中等基线任务），同时需注意框架在高效任务中的边界效应

Abstract: Large Language Models face significant performance challenges in specialized
domains, with state-of-the-art models achieving only 45.9% accuracy on medical
coding tasks. This study proposes a Hierarchical Error Correction (HEC)
framework that addresses domain-specific AI limitations through systematic
error analysis and targeted intervention strategies.
  We analyze error patterns across four specialized domains and find that AI
errors follow consistent hierarchical structures: Knowledge-layer errors
(58.4%), Reasoning-layer errors (39.6%), and Complexity-layer errors (2.0%).
Based on these patterns, we develop a three-stage correction framework that
addresses errors according to their hierarchical importance and demonstrates
that framework effectiveness correlates inversely with baseline task
performance.
  Experimental validation across medical transcription (4,921 cases), legal
document classification (1,000 cases), political bias detection (645 cases),
and legal reasoning (1,000 cases) shows consistent improvements. Cross-model
validation across five LLM architectures demonstrates average improvements of
11.2 percentage points (p < 0.001). However, analysis reveals framework
limitations in high-baseline tasks (>75% accuracy), where hierarchical
intervention may interfere with effective reasoning processes.
  The results suggest that systematic error analysis can guide effective AI
enhancement strategies in specialized domains, particularly for
moderate-baseline tasks, while highlighting the importance of understanding
framework boundaries for optimal deployment.

</details>


### [167] [Between Help and Harm: An Evaluation of Mental Health Crisis Handling by LLMs](https://arxiv.org/abs/2509.24857)
*Adrian Arnaiz-Rodriguez,Miguel Baidal,Erik Derner,Jenn Layton Annable,Mark Ball,Mark Ince,Elvira Perez Vallejos,Nuria Oliver*

Main category: cs.CL

TL;DR: 研究发现LLMs在处理心理健康危机时整体可靠但存在风险，开源模型失败率更高，间接风险信号处理能力薄弱。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在心理健康危机场景下的安全性，弥补现有研究缺乏统一分类标准、标注数据集和临床验证的空白。

Method: 建立包含6个临床危机类别的分类法，构建多样化评估数据集，制定专家响应评估协议，系统测试三大LLM的危机分类和响应能力。

Result: LLMs对显性危机响应一致性高（商业模型88%适当率），但存在12%不适当回答（开源模型失败率23%），间接风险场景响应合格率仅55%。

Conclusion: 需强化LLM的安全防护机制，改进危机检测算法，发展符合临床实践的上下文感知技术，为AI心理健康支持建立系统化评估框架。

Abstract: The widespread use of chatbots powered by large language models (LLMs) such
as ChatGPT and Llama has fundamentally reshaped how people seek information and
advice across domains. Increasingly, these chatbots are being used in
high-stakes contexts, including emotional support and mental health concerns.
While LLMs can offer scalable support, their ability to safely detect and
respond to acute mental health crises remains poorly understood. Progress is
hampered by the absence of unified crisis taxonomies, robust annotated
benchmarks, and empirical evaluations grounded in clinical best practices. In
this work, we address these gaps by introducing a unified taxonomy of six
clinically-informed mental health crisis categories, curating a diverse
evaluation dataset, and establishing an expert-designed protocol for assessing
response appropriateness. We systematically benchmark three state-of-the-art
LLMs for their ability to classify crisis types and generate safe, appropriate
responses. The results reveal that while LLMs are highly consistent and
generally reliable in addressing explicit crisis disclosures, significant risks
remain. A non-negligible proportion of responses are rated as inappropriate or
harmful, with responses generated by an open-weight model exhibiting higher
failure rates than those generated by the commercial ones. We also identify
systemic weaknesses in handling indirect or ambiguous risk signals, a reliance
on formulaic and inauthentic default replies, and frequent misalignment with
user context. These findings underscore the urgent need for enhanced
safeguards, improved crisis detection, and context-aware interventions in LLM
deployments. Our taxonomy, datasets, and evaluation framework lay the
groundwork for ongoing research and responsible innovation in AI-driven mental
health support, helping to minimize harm and better protect vulnerable users.

</details>


### [168] [Metaphor identification using large language models: A comparison of RAG, prompt engineering, and fine-tuning](https://arxiv.org/abs/2509.24866)
*Matteo Fuoli,Weihang Huang,Jeannette Littlemore,Sarah Turner,Ellen Wilding*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）可部分实现隐喻识别自动化，微调模型达到0.79的F1分数，系统性差异反映隐喻理论灰色地带。


<details>
  <summary>Details</summary>
Motivation: 传统隐喻分析依赖人工标注，难以规模化。研究探索LLMs在全文隐喻识别中的潜力，以突破语境敏感性带来的限制。

Method: 对比三种方法：1) 检索增强生成（RAG）基于规则标注；2) 提示工程（含零样本/少样本/思维链策略）；3) 微调人工标注数据。

Result: 闭源LLMs准确率高，微调模型中位F1达0.79。人类与模型差异主要源于隐喻理论的模糊概念边界。

Conclusion: LLMs既可部分自动化隐喻识别，也能作为完善隐喻识别协议的理论测试平台，推动隐喻理论研究发展。

Abstract: Metaphor is a pervasive feature of discourse and a powerful lens for
examining cognition, emotion, and ideology. Large-scale analysis, however, has
been constrained by the need for manual annotation due to the context-sensitive
nature of metaphor. This study investigates the potential of large language
models (LLMs) to automate metaphor identification in full texts. We compare
three methods: (i) retrieval-augmented generation (RAG), where the model is
provided with a codebook and instructed to annotate texts based on its rules
and examples; (ii) prompt engineering, where we design task-specific verbal
instructions; and (iii) fine-tuning, where the model is trained on hand-coded
texts to optimize performance. Within prompt engineering, we test zero-shot,
few-shot, and chain-of-thought strategies. Our results show that
state-of-the-art closed-source LLMs can achieve high accuracy, with fine-tuning
yielding a median F1 score of 0.79. A comparison of human and LLM outputs
reveals that most discrepancies are systematic, reflecting well-known grey
areas and conceptual challenges in metaphor theory. We propose that LLMs can be
used to at least partly automate metaphor identification and can serve as a
testbed for developing and refining metaphor identification protocols and the
theory that underpins them.

</details>


### [169] [Expanding Computation Spaces of LLMs at Inference Time](https://arxiv.org/abs/2509.24884)
*Yoonna Jang,Kisu Yang,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 研究发现：在推理阶段向语言模型插入特定填充词可扩展计算空间，较小模型（如1.7B）性能提升达12.37%，注意力机制显示填充空间承载有效计算。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型能否在未经训练的情况下，仅通过推理阶段插入人工填充词序列来利用扩展计算空间，突破传统依赖预训练专用token的局限。

Method: 系统测试不同填充词类型/数量/插入位置，分析模型训练阶段对计算空间的利用时机，通过注意力图可视化计算空间动态。实验覆盖1.7B-32B模型，涉及开放领域QA和数学推理任务。

Result: 在答案标记前插入填充词效果最佳；小模型受益显著（SmolLM2-1.7B提升12.37%）；注意力图显示扩展空间延续原始注意力模式或聚焦问题/选项。

Conclusion: 填充空间作为附加计算容量发挥作用（非冗余输入），其计算过程与问题解决直接相关。该发现为优化模型推理效率提供新方向，特别有助于资源受限的小型模型性能提升。

Abstract: Chain-of-thought (CoT) rationale enables language models to use additional
task-related text for problem-solving, benefiting not only from detailed
reasoning steps but also from the expanded computational space of longer
inputs. Prior work has trained filler or special tokens to serve as additional
computation spaces. In this study, we investigate whether language models can
leverage artificially inserted sequences of filler tokens solely at inference.
We first identify effective token types, numbers, and insertion locations, then
examine at what stage of training models begin to exploit the expanded
computation space, and finally analyze dynamics within these spaces via
attention maps. Experiments on models ranging from 1.7B to 32B across
open-domain QA and math tasks show that appropriate token types and counts
vary, but placing filler tokens directly before the final 'Answer:' token is
most effective. Smaller models benefit most, up to 12.372 percentage points in
SmolLM2-1.7B-Instruct, indicating that these spaces act as additional
computational capacity rather than redundant input. Attention maps reveal that
expanded spaces often continue the original attention mechanism and sometimes
focus on questions or answer options, suggesting meaningful computation for
problem-solving.

</details>


### [170] [BOE-XSUM: Extreme Summarization in Clear Language of Spanish Legal Decrees and Notifications](https://arxiv.org/abs/2509.24908)
*Andrés Fernández García,Javier de la Rosa,Julio Gonzalo,Roser Morante,Enrique Amigó,Alejandro Benito-Santos,Jorge Carrillo-de-Albornoz,Víctor Fresno,Adrian Ghajari,Guillermo Marco,Laura Plaza,Eva Sánchez Salido*

Main category: cs.CL

TL;DR: 西班牙法律文件摘要数据集BOE-XSUM的构建及领域专用模型效果验证


<details>
  <summary>Details</summary>
Motivation: 西班牙语领域缺乏针对法律文档的简洁摘要资源，特别是官方公报BOE这类专业文本，存在信息过载下的实际需求缺口

Method: 1. 构建包含3,648个官方文档及其简明摘要的BOE-XSUM数据集
2. 对中等规模LLM进行微调
3. 与零样本设置的通用模型(如DeepSeek-R1)进行对比实验

Result: 微调模型显著优于通用模型：最佳模型BERTIN GPT-J 6B(32位精度)准确率达41.6%，比零样本最优模型提升24%（33.5% vs 41.6%）

Conclusion: 领域专用微调显著提升法律文本摘要效果，证实专业数据训练对提升LLM领域适应性的有效性

Abstract: The ability to summarize long documents succinctly is increasingly important
in daily life due to information overload, yet there is a notable lack of such
summaries for Spanish documents in general, and in the legal domain in
particular. In this work, we present BOE-XSUM, a curated dataset comprising
3,648 concise, plain-language summaries of documents sourced from Spain's
``Bolet\'{\i}n Oficial del Estado'' (BOE), the State Official Gazette. Each
entry in the dataset includes a short summary, the original text, and its
document type label. We evaluate the performance of medium-sized large language
models (LLMs) fine-tuned on BOE-XSUM, comparing them to general-purpose
generative models in a zero-shot setting. Results show that fine-tuned models
significantly outperform their non-specialized counterparts. Notably, the
best-performing model -- BERTIN GPT-J 6B (32-bit precision) -- achieves a 24\%
performance gain over the top zero-shot model, DeepSeek-R1 (accuracies of
41.6\% vs.\ 33.5\%).

</details>


### [171] [How Well Do LLMs Imitate Human Writing Style?](https://arxiv.org/abs/2509.24930)
*Rebira Jemama,Rajesh Kumar*

Main category: cs.CL

TL;DR: 提出无需训练的快速作者风格验证框架，结合TF-IDF与transformer嵌入，在准确率和效率上显著提升，证明提示策略比模型规模对风格模仿影响更大


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型生成文本流畅但缺乏特定作者风格模仿能力，传统方法需监督训练且效率低下，需探索更高效的风格分析方案

Method: 集成TF-IDF字符n-gram与transformer嵌入，通过经验距离分布分类文本对，无需训练或阈值调整

Result: 学术论文验证准确率97.5%，跨领域94.5%；few-shot提示比zero-shot风格匹配度高23.5倍，完成提示达99.9%风格吻合；LLM输出困惑度(15.2)显著低于人类(29.5)

Conclusion: 风格保真度与统计可检测性可分离，为作者建模、检测和身份条件生成提供可复现基础，提示策略选择比模型规模扩展更重要

Abstract: Large language models (LLMs) can generate fluent text, but their ability to
replicate the distinctive style of a specific human author remains unclear. We
present a fast, training-free framework for authorship verification and style
imitation analysis. The method integrates TF-IDF character n-grams with
transformer embeddings and classifies text pairs through empirical distance
distributions, eliminating the need for supervised training or threshold
tuning. It achieves 97.5\% accuracy on academic essays and 94.5\% in
cross-domain evaluation, while reducing training time by 91.8\% and memory
usage by 59\% relative to parameter-based baselines. Using this framework, we
evaluate five LLMs from three separate families (Llama, Qwen, Mixtral) across
four prompting strategies - zero-shot, one-shot, few-shot, and text completion.
Results show that the prompting strategy has a more substantial influence on
style fidelity than model size: few-shot prompting yields up to 23.5x higher
style-matching accuracy than zero-shot, and completion prompting reaches 99.9\%
agreement with the original author's style. Crucially, high-fidelity imitation
does not imply human-like unpredictability - human essays average a perplexity
of 29.5, whereas matched LLM outputs average only 15.2. These findings
demonstrate that stylistic fidelity and statistical detectability are
separable, establishing a reproducible basis for future work in authorship
modeling, detection, and identity-conditioned generation.

</details>


### [172] [MobileLLM-R1: Exploring the Limits of Sub-Billion Language Model Reasoners with Open Training Recipes](https://arxiv.org/abs/2509.24945)
*Changsheng Zhao,Ernie Chang,Zechun Liu,Chia-Jung Chang,Wei Wen,Chen Lai,Rick Cao,Yuandong Tian,Raghuraman Krishnamoorthi,Yangyang Shi,Vikas Chandra*

Main category: cs.CL

TL;DR: 挑战了LLMs需要海量数据训练才能具备推理能力的假设，通过优化数据质量仅用2T tokens训练出性能超越前人的小模型


<details>
  <summary>Details</summary>
Motivation: 现有研究未质疑第二个假设——推理能力需要海量数据训练。本研究旨在探索通过数据质量优化而非单纯数量扩展来实现小模型推理能力突破

Method: 1. 设计数据质量评估指标筛选开源数据
2. 对约2T高质量token进行重采样得到4.2T训练数据
3. 采用标准后训练流程开发MobileLLM-R1系列模型

Result: MobileLLM-R1-950M在AIME得分15.5远超OLMo-2（0.6）和SmolLM（0.3），且仅用Qwen3-0.6B 11.7%的训练token量即达到相当/更优的推理表现

Conclusion: 数据质量比数量更重要；开源完整训练方案验证了小模型通过高质量数据实现推理突破的可行性，为后续研究提供新方向

Abstract: The paradigm shift in large language models (LLMs) from instinctive responses
to chain-of-thought (CoT) reasoning has fueled two prevailing assumptions: (1)
reasoning capabilities only emerge in sufficiently large models, and (2) such
capabilities require training on massive datasets. While the first assumption
has already been challenged by recent sub-billion-parameter reasoning models
such as Qwen3-0.6B and DeepSeek distilled variants, the second remains largely
unquestioned. In this work, we revisit the necessity of scaling to extremely
large corpora (>10T tokens) for reasoning emergence. By carefully curating and
resampling open-source datasets that we identify as beneficial under our
designed metrics, we demonstrate that strong reasoning abilities can emerge
with far less data. Specifically, we show that only ~2T tokens of high-quality
data are sufficient, and pre-training with 4.2T tokens on the dataset resampled
from these ~2T tokens, followed by a established post-training procedure,
enables the development of MobileLLM-R1, a series of sub-billion-parameter
reasoning models that substantially outperform prior models trained on fully
open-sourced data. For example, MobileLLM-R1-950M achieves an AIME score of
15.5, compared to just 0.6 for OLMo-2-1.48B and 0.3 for SmolLM-2-1.7B.
Remarkably, despite being trained on only 11.7% of the tokens compared to
Qwen3's proprietary 36T-token corpus for pretraining, MobileLLM-R1-950M matches
or surpasses Qwen3-0.6B across multiple reasoning benchmarks. To facilitate
further research in this direction, we have released the complete training
recipe, data sources, data mixing ratio, and model checkpoints, together with
the key insights obtained throughout this study.

</details>


### [173] [The Dialogue That Heals: A Comprehensive Evaluation of Doctor Agents' Inquiry Capability](https://arxiv.org/abs/2509.24958)
*Linlu Gong,Ante Wang,Yunghwei Lai,Weizhi Ma,Yang Liu*

Main category: cs.CL

TL;DR: 提出MAQuE基准测试用于全面评估医疗AI多轮问诊能力，实验发现现有模型在问诊效率、患者体验等方面存在显著改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有医疗AI虽具备诊断技能，但缺乏优秀医生应具备的共情、沟通等综合能力，需建立系统性评估标准。

Method: 构建含3,000个差异化患者代理的MAQuE基准，提出涵盖任务成功率、问诊效率、患者体验等多维度的评估框架。

Result: 实验显示LLMs在诊断准确性受患者行为模式影响显著，存在效率与体验的权衡，最优模型准确率仅达人类医生70%水平。

Conclusion: MAQuE揭示了医疗AI实际部署的潜在瓶颈，未来需在算法优化与临床实用性间取得平衡，建议结合认知建模提升患者模拟真实性。

Abstract: An effective physician should possess a combination of empathy, expertise,
patience, and clear communication when treating a patient. Recent advances have
successfully endowed AI doctors with expert diagnostic skills, particularly the
ability to actively seek information through inquiry. However, other essential
qualities of a good doctor remain overlooked. To bridge this gap, we present
MAQuE(Medical Agent Questioning Evaluation), the largest-ever benchmark for the
automatic and comprehensive evaluation of medical multi-turn questioning. It
features 3,000 realistically simulated patient agents that exhibit diverse
linguistic patterns, cognitive limitations, emotional responses, and tendencies
for passive disclosure. We also introduce a multi-faceted evaluation framework,
covering task success, inquiry proficiency, dialogue competence, inquiry
efficiency, and patient experience. Experiments on different LLMs reveal
substantial challenges across the evaluation aspects. Even state-of-the-art
models show significant room for improvement in their inquiry capabilities.
These models are highly sensitive to variations in realistic patient behavior,
which considerably impacts diagnostic accuracy. Furthermore, our fine-grained
metrics expose trade-offs between different evaluation perspectives,
highlighting the challenge of balancing performance and practicality in
real-world clinical settings.

</details>


### [174] [SemanticShield: LLM-Powered Audits Expose Shilling Attacks in Recommender Systems](https://arxiv.org/abs/2509.24961)
*Kaihong Li,Huichi Zhou,Bin Ma,Fangjun Huang*

Main category: cs.CL

TL;DR: 提出整合商品侧语义与大语言模型的两阶段检测框架SemanticShield，有效防御推荐系统中的托攻击


<details>
  <summary>Details</summary>
Motivation: 现有防御机制主要关注用户行为特征，忽视了商品标题/描述等语义特征可能暴露的恶意攻击意图

Method: 1. 行为预筛选+LLM语义审计两阶段检测
2. 基于强化学习微调轻量LLM，设计奖励机制构建专用检测器

Result: 在6种典型攻击策略中验证有效性，对未见攻击方法展现强泛化能力

Conclusion: SemanticShield通过语义一致性检测显著提升防御效果，为开放环境推荐系统提供可靠保护方案

Abstract: Recommender systems (RS) are widely used in e-commerce for personalized
suggestions, yet their openness makes them susceptible to shilling attacks,
where adversaries inject fake behaviors to manipulate recommendations. Most
existing defenses emphasize user-side behaviors while overlooking item-side
features such as titles and descriptions that can expose malicious intent. To
address this gap, we propose a two-stage detection framework that integrates
item-side semantics via large language models (LLMs). The first stage
pre-screens suspicious users using low-cost behavioral criteria, and the second
stage employs LLM-based auditing to evaluate semantic consistency. Furthermore,
we enhance the auditing model through reinforcement fine-tuning on a
lightweight LLM with carefully designed reward functions, yielding a
specialized detector called SemanticShield. Experiments on six representative
attack strategies demonstrate the effectiveness of SemanticShield against
shilling attacks, and further evaluation on previously unseen attack methods
shows its strong generalization capability. Code is available at
https://github.com/FrankenstLee/SemanticShield.

</details>


### [175] [Generalized Correctness Models: Learning Calibrated and Model-Agnostic Correctness Predictors from Historical Patterns](https://arxiv.org/abs/2509.24988)
*Hanqi Xiao,Vaidehi Patil,Hyunji Lee,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CL

TL;DR: 通过系统编码历史正确性数据而非依赖自我认知，构建可泛化的LLM置信度估计模型


<details>
  <summary>Details</summary>
Motivation: 现有方法假设模型具备自我判断能力，但实验表明不同LLM预测自身答案正确性的能力无明显差异，需探索更有效的置信度建模方式

Method: 提出广义正确性模型(GCM)，通过注入目标模型的历史预测正确性数据，采用上下文示例整合和事后校准方法，系统性研究正确性预测能力的来源

Result: 基于Qwen3-8B的GCM在5个模型家族和多个数据集验证，选择性预测任务显示置信度估计是可泛化的模型无关技能（校准误差降低40%）

Conclusion: 可靠的置信度估计源于系统学习历史正确性模式，而非模型自我反省，证明模型无关的通用方法有效性

Abstract: Generating accurate and calibrated confidence estimates is critical for
deploying LLMs in high-stakes or user-facing applications, and remains an open
challenge. Prior research has often framed confidence as a problem of eliciting
a model's "self-knowledge", i.e., the ability of an LLM to judge whether its
own answers are correct; this approach implicitly assumes that there is some
privileged information about the answer's correctness that is accessible to the
model itself. However, our experiments reveal that an LLM attempting to predict
the correctness of its own outputs generally performs no better than an
unrelated LLM. Moreover, we hypothesize that a key factor in building a
"Correctness Model" (CM) is exposure to a target model's historical
predictions. We propose multiple methods to inject this historical correctness
information, creating a Generalized Correctness Model (GCM). We first show that
GCMs can be trained on the correctness data from many LLMs and learn patterns
for correctness prediction applicable across datasets and models. We then use
CMs as a lens for studying the source of correctness prediction ability and its
generalization, systematically controlling their training data and finding that
answer phrasing is a strong predictor for correctness. We further explore
alternative methods of injecting history without training an LLM, finding that
including history as in-context examples can help improve correctness
prediction, and post-hoc calibration can provide complementary reductions in
calibration error. We evaluate GCMs based on Qwen3-8B across 5 model families
and the MMLU and TriviaQA datasets, as well as on a downstream selective
prediction task, finding that reliable LLM confidence estimation is a
generalizable and model-agnostic skill learned by systematically encoding
correctness history rather than a model-specific skill reliant on
self-introspection.

</details>


### [176] [Circuit Distillation](https://arxiv.org/abs/2509.25002)
*Somin Wadhwa,Silvio Amir,Byron C. Wallace*

Main category: cs.CL

TL;DR: 提出电路蒸馏方法，通过对齐师生模型内部计算机制实现算法能力迁移，优于标准蒸馏方法


<details>
  <summary>Details</summary>
Motivation: 传统模型蒸馏仅关注输出模仿，忽略教师模型内部计算机制。本研究旨在通过电路蒸馏实现算法能力的定向迁移

Method: 1. 定义功能对应电路组件匹配机制
2. 设计表征相似性损失函数
3. 针对Llama3模型进行实体追踪和心理理论任务验证

Result: 电路蒸馏在参数微调量较小的情况下，成功实现算法能力迁移，效果优于标准蒸馏方法

Conclusion: 该方法证实机制迁移可行性，为通过可解释的内部机制实现定向能力蒸馏开辟新途径

Abstract: Model distillation typically focuses on behavioral mimicry, where a student
model is trained to replicate a teacher's output while treating its internal
computations as a black box. In this work we propose an alternative approach:
Distilling the underlying computational mechanisms implemented by a teacher
model. Specifically, we propose circuit distillation, which introduces an
objective to align internal representations between analogous circuit
components in teacher and student models. We propose a method to match
``functionally correspondent'' circuit components and introduce a loss
reflecting similarities between the representations that these induce. We
evaluate circuit distillation on entity tracking and theory of mind (ToM) tasks
using models from the Llama3 family. Our results demonstrate that circuit
distillation outperforms standard distillation, successfully transferring
algorithmic capabilities by adjusting only a small, targeted subset of student
model parameters. This work establishes the feasibility of transferring
mechanisms, which may in turn allow for efficient distillation of targeted
teacher capabilities via interpretable and controllable internal student
mechanisms.

</details>


### [177] [Ultra-Fast Language Generation via Discrete Diffusion Divergence Instruct](https://arxiv.org/abs/2509.25035)
*Haoyang Zheng,Xinyang Liu,Cindy Xiangrui Kong,Nan Jiang,Zheyuan Hu,Weijian Luo,Wei Deng,Guang Lin*

Main category: cs.CL

TL;DR: 提出DiDi-Instruct方法，通过离散扩散散度指导训练实现64倍加速的语言生成，在保持质量的同时显著提升效率


<details>
  <summary>Details</summary>
Motivation: 解决传统语言生成模型推理速度慢的问题，利用预训练离散扩散语言模型进行初始化，探索快速生成与模型性能的平衡

Method: 基于积分KL散度最小化框架，结合分组奖励归一化/中间状态匹配/RGAS采样器，优化训练稳定性和生成质量

Result: OpenWebText数据集上困惑度62.2(8步)至18.4(128步)，以1%熵损失实现20倍训练时间缩减，蛋白质序列生成验证普适性

Conclusion: DiDi-Instruct作为新型蒸馏范式，突破生成速度瓶颈，为实时语言生成提供有效解决方案，代码模型全开源

Abstract: Fast generation of language texts is the holy grail that people pursue in the
AI era. In this work, we introduced Discrete Diffusion Divergence Instruct
(DiDi-Instruct), a training-based method that leads to fast language generation
models by initializing from a pre-trained (masked) discrete diffusion language
model (dLLM). The resulting DiDi-Instruct model outperforms the dLLM
counterparts and the GPT-2 baseline with 64x acceleration. In the theoretical
part of the paper, we build the foundation of DiDi-Instruct in a framework of
integral KL-divergence minimization, with practical training algorithms. We
also introduce techniques like grouped reward normalization, intermediate-state
matching, and the reward-guided ancestral sampler (RGAS) that significantly
improve the training stability, the model coverage, and the inference
performances. On OpenWebText, DiDi-Instruct outperforms all accelerated
language generation models as well as the GPT-2 baseline and the standard
dLLMs, achieving sample perplexities ranging from 62.2 (8 NFEs) to 18.4 (128
NFEs). These performance gains are accomplished with a negligible entropy loss
of about 1% and 20x less additional training wall-clock time. We further
validate the robustness and effectiveness of DiDi-Instruct through extensive
ablation studies, model scaling, and the generation of discrete protein
sequences. In conclusion, DiDi-Instruct is an efficient yet effective
distillation method, enabling language generation in the blink of an eye. We
will release both code and models at github.com/haoyangzheng-ai/didi-instruct.

</details>


### [178] [GateMABSA: Aspect-Image Gated Fusion for Multimodal Aspect-based Sentiment Analysis](https://arxiv.org/abs/2509.25037)
*Adamu Lawan,Haruna Yunusa*

Main category: cs.CL

TL;DR: 提出门控多模态架构GateMABSA，通过Syn/Sem/Fuse-mLSTM三模块解决多模态情感分析中的噪声过滤与跨模态对齐问题，在Twitter数据集上优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有多模态ABSA模型存在视觉噪声干扰和跨模态对齐困难，需开发能有效整合多模态信息的架构。

Method: 1. Syn-mLSTM引入句法结构；2. Sem-mLSTM强化方面-语义关联；3. Fuse-mLSTM实现选择性多模态融合。

Result: 在两个Twitter基准数据集上的实验显示模型性能超越多个基线方法。

Conclusion: 门控机制有效协调多模态信息，验证了模块化设计在多模态复杂场景中的优势。

Abstract: Aspect-based Sentiment Analysis (ABSA) has recently advanced into the
multimodal domain, where user-generated content often combines text and images.
However, existing multimodal ABSA (MABSA) models struggle to filter noisy
visual signals, and effectively align aspects with opinion-bearing content
across modalities. To address these challenges, we propose GateMABSA, a novel
gated multimodal architecture that integrates syntactic, semantic, and
fusion-aware mLSTM. Specifically, GateMABSA introduces three specialized
mLSTMs: Syn-mLSTM to incorporate syntactic structure, Sem-mLSTM to emphasize
aspect--semantic relevance, and Fuse-mLSTM to perform selective multimodal
fusion. Extensive experiments on two benchmark Twitter datasets demonstrate
that GateMABSA outperforms several baselines.

</details>


### [179] [Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic Architectures](https://arxiv.org/abs/2509.25045)
*Marco Bronzini,Carlo Nicolini,Bruno Lepri,Jacopo Staiano,Andrea Passerini*

Main category: cs.CL

TL;DR: 提出Hyperdimensional Probe解码范式，通过向量符号架构将LLM残差流投影为可解释概念，结合稀疏自编码器和传统探针优势，在多种任务中验证有效性


<details>
  <summary>Details</summary>
Motivation: 现有可解释性方法（直接logit归因/稀疏自编码器）存在输出词汇限制、特征命名不明确等问题，难以深入理解LLM内部表示

Method: 利用向量符号架构(VSA)将模型残差流映射到概念空间，在输入补全任务(句法识别/键值关联/抽象推理)和问答场景中验证解码能力

Result: 实验证明该探针能跨模型/嵌入尺寸/输入域可靠提取语义概念，并有效识别LLM失败案例

Conclusion: 该研究推进了LLM向量空间的信息解码，实现了从神经表示中提取更具信息性、可解释性和结构化的特征

Abstract: Despite their capabilities, Large Language Models (LLMs) remain opaque with
limited understanding of their internal representations. Current
interpretability methods, such as direct logit attribution (DLA) and sparse
autoencoders (SAEs), provide restricted insight due to limitations such as the
model's output vocabulary or unclear feature names. This work introduces
Hyperdimensional Probe, a novel paradigm for decoding information from the LLM
vector space. It combines ideas from symbolic representations and neural
probing to project the model's residual stream into interpretable concepts via
Vector Symbolic Architectures (VSAs). This probe combines the strengths of SAEs
and conventional probes while overcoming their key limitations. We validate our
decoding paradigm with controlled input-completion tasks, probing the model's
final state before next-token prediction on inputs spanning syntactic pattern
recognition, key-value associations, and abstract inference. We further assess
it in a question-answering setting, examining the state of the model both
before and after text generation. Our experiments show that our probe reliably
extracts meaningful concepts across varied LLMs, embedding sizes, and input
domains, also helping identify LLM failures. Our work advances information
decoding in LLM vector space, enabling extracting more informative,
interpretable, and structured features from neural representations.

</details>


### [180] [Confidence-Guided Error Correction for Disordered Speech Recognition](https://arxiv.org/abs/2509.25048)
*Abner Hernandez,Tomás Arias Vergara,Andreas Maier,Paula Andrea Pérez-Toro*

Main category: cs.CL

TL;DR: 研究提出通过置信度信息引导的LLM微调方法，有效改善障碍语音识别错误率


<details>
  <summary>Details</summary>
Motivation: 传统ASR系统在障碍语音识别中存在较高错误率，需要开发针对性纠错方法以提升语音辅助技术的可访问性

Method: 提出confidence-informed prompting方法，将词级置信度估计嵌入LLaMA 3.1微调过程，引导模型聚焦不确定区域并减少过校正

Result: 在Speech Accessibility Project和TORGO数据集上分别实现10%和47%的相对WER降低

Conclusion: 置信度感知的LLM微调能显著提升障碍语音识别准确率，证明不确定性引导机制在语音纠错任务中的有效性

Abstract: We investigate the use of large language models (LLMs) as post-processing
modules for automatic speech recognition (ASR), focusing on their ability to
perform error correction for disordered speech. In particular, we propose
confidence-informed prompting, where word-level uncertainty estimates are
embedded directly into LLM training to improve robustness and generalization
across speakers and datasets. This approach directs the model to uncertain ASR
regions and reduces overcorrection. We fine-tune a LLaMA 3.1 model and compare
our approach to both transcript-only fine-tuning and post hoc confidence-based
filtering. Evaluations show that our method achieves a 10% relative WER
reduction compared to naive LLM correction on the Speech Accessibility Project
spontaneous speech and a 47% reduction on TORGO, demonstrating the
effectiveness of confidence-aware fine-tuning for impaired speech.

</details>


### [181] [An empirical study on the limitation of Transformers in program trace generation](https://arxiv.org/abs/2509.25073)
*Simeng Sun*

Main category: cs.CL

TL;DR: 研究探索了Transformer在程序跟踪生成任务中的表现，发现尽管在分布内数据表现良好，但存在系统性泛化缺陷，部分模型设计能有效提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 通过程序跟踪生成任务（PTG）研究Transformer的长序列推理能力，该任务要求模型生成程序执行的详细步骤跟踪，每个步骤简单但整体序列较长，可有效评估模型系统性推理能力。

Method: 使用小型Transformer架构，尝试了多种改进方案：替代位置编码、softmax替代方案、混合模型结构以及短卷积模块，并在PTG任务上进行训练和测试。

Result: 模型在训练数据分布内准确率优异，但在程序长度、跟踪步数等泛化场景中普遍失败，部分改进设计（如特定位置编码）显著提升了泛化性能。

Conclusion: Transformer在系统性泛化方面仍存在挑战，架构改进可部分缓解但未根本解决，需进一步研究模型结构对算法推理能力的本质影响。

Abstract: We study Transformers on the task \emph{program trace generation} (PTG),
where models produce step-by-step execution traces for synthetic programs.
Unlike existing algorithmic problems, PTG externalizes reasoning through long
traces where each step is trivial. We train small Transformers with diverse
modifications, including alternative position encodings, softmax replacements,
hybrid model, and short convolutions. While these models achieve strong
in-distribution accuracy, they exhibit systematic failures when generalizing to
various factors (e.g., program length, trace steps), though some designs
significantly improve generalization.

</details>


### [182] [Scaling Generalist Data-Analytic Agents](https://arxiv.org/abs/2509.25084)
*Shuofei Qiao,Yanqiu Zhao,Zhisong Qiu,Xiaobin Wang,Jintian Zhang,Zhao Bin,Ningyu Zhang,Yong Jiang,Pengjun Xie,Fei Huang,Huajun Chen*

Main category: cs.CL

TL;DR: DataMind通过数据合成和代理训练技术构建通用数据分析代理，在12K轨迹集上训练出的14B模型以71.16%平均分刷新多项基准纪录。


<details>
  <summary>Details</summary>
Motivation: 当前数据分析代理过度依赖闭源模型，开源模型在多样化数据格式处理、长程多步推理等现实需求上表现不足。

Method: 1. 递归式难易任务组合机制增强查询多样性 
2. 知识增强轨迹采样与双重过滤策略 
3. 动态调整SFT+RL混合训练目标 
4. 内存优化的代码多轮执行框架

Result: DataMind-14B模型在多个数据分析基准测试中以71.16%平均分超越DeepSeek-V3.1和GPT-5，7B版本以68.10%成为开源最优。

Conclusion: DataMind验证了通过系统化数据合成和混合训练策略构建高效数据分析代理的可行性，其资源开放将推动社区在代理训练领域的研究。

Abstract: Data-analytic agents are emerging as a key catalyst for automated scientific
discovery and for the vision of Innovating AI. Current approaches, however,
rely heavily on prompt engineering over proprietary models, while open-source
models struggle to face diverse-format, large-scale data files and
long-horizon, multi-step reasoning that real-world analytics demands. This
paper introduces DataMind, a scalable data synthesis and agent training recipe
designed to build generalist data-analytic agents. DataMind tackles three key
challenges in building open-source data-analytic agents, including insufficient
data resources, improper training strategy, and unstable code-based multi-turn
rollout. Concretely, DataMind applies 1) a fine-grained task taxonomy and a
recursive easy-to-hard task composition mechanism to increase the diversity and
difficulty of synthesized queries; 2) a knowledge-augmented trajectory sampling
strategy followed by model-based and rule-based filtering; 3) a dynamically
adjustable training objective combining both SFT and RL losses; 4) a
memory-frugal and stable code-based multi-turn rollout framework. Built on
DataMind, we curate DataMind-12K, a high-quality trajectory set spanning
diverse domains, task categories, and data file formats for data-analytic
tasks. Trained on DataMind-12K, our DataMind-14B achieves state-of-the-art with
an average score of 71.16% on multiple data analysis benchmarks, outperforming
the strongest proprietary baselines DeepSeek-V3.1 and GPT-5. Our DataMind-7B
also performs best among all open-source models with a score of 68.10%. We also
incorporate some empirical insights gained from our exploratory trials into the
analysis experiments, aiming to provide actionable insights about agentic
training for the community. We will release DataMind-12K and DataMind-7B,14B
for the community's future research.

</details>


### [183] [jina-reranker-v3: Last but Not Late Interaction for Document Reranking](https://arxiv.org/abs/2509.25085)
*Feng Wang,Yuqing Li,Han Xiao*

Main category: cs.CL

TL;DR: jina-reranker-v3提出新型'last but not late'交互机制，在0.6B参数量下实现SOTA的61.94 nDCG@10性能，模型尺寸仅为生成式排序器的1/10。


<details>
  <summary>Details</summary>
Motivation: 解决传统late interaction模型(如ColBERT)编码分离和多向量匹配的局限性，通过早期跨文档交互提升语义理解效率。

Method: 在统一上下文窗口中对查询和文档进行因果自注意力计算，从各文档末令牌提取上下文嵌入，实现跨文档的深度交互。

Result: BEIR基准测试达到61.94 nDCG@10，模型参数量仅为同类生成式排序器的十分之一。

Conclusion: 该架构在保持紧凑模型尺寸的同时，通过创新的交互机制实现了性能突破，验证了早期跨文档交互的有效性。

Abstract: jina-reranker-v3 is a 0.6B parameter multilingual document reranker that
introduces a novel last but not late interaction. Unlike late interaction
models such as ColBERT that perform separate encoding followed by multi-vector
matching, our approach conducts causal self-attention between query and
documents within the same context window, enabling rich cross-document
interactions before extracting contextual embeddings from the last token of
each document. This compact architecture achieves state-of-the-art BEIR
performance with 61.94 nDCG@10 while being ten times smaller than generative
listwise rerankers.

</details>


### [184] [Towards Trustworthy Lexical Simplification: Exploring Safety and Efficiency with Small LLMs](https://arxiv.org/abs/2509.25086)
*Akio Hayakawa,Stefan Bott,Horacio Saggion*

Main category: cs.CL

TL;DR: 提出了基于小型LLM的高效词汇简化框架，在性能、效率与安全性之间取得平衡，并通过概率过滤策略降低有害简化风险。


<details>
  <summary>Details</summary>
Motivation: 解决大模型在隐私敏感/资源受限场景的落地问题，同时保障弱势群体使用LS系统时的输出安全性与正确性。

Method: 采用知识蒸馏（合成数据）和上下文学习作为基线方法，在5种语言中开展实验，结合自动评估与人工安全分析。

Result: 知识蒸馏提升指标但增加有害简化，模型输出概率可有效识别有害简化，过滤策略可保留87%有益简化同时消除91%有害案例。

Conclusion: 建立了小型LLM高效安全词汇简化的基准，揭示了性能-效率-安全性的关键权衡，为实际部署提供了可行方案。

Abstract: Despite their strong performance, large language models (LLMs) face
challenges in real-world application of lexical simplification (LS),
particularly in privacy-sensitive and resource-constrained environments.
Moreover, since vulnerable user groups (e.g., people with disabilities) are one
of the key target groups of this technology, it is crucial to ensure the safety
and correctness of the output of LS systems. To address these issues, we
propose an efficient framework for LS systems that utilizes small LLMs
deployable in local environments. Within this framework, we explore knowledge
distillation with synthesized data and in-context learning as baselines. Our
experiments in five languages evaluate model outputs both automatically and
manually. Our manual analysis reveals that while knowledge distillation boosts
automatic metric scores, it also introduces a safety trade-off by increasing
harmful simplifications. Importantly, we find that the model's output
probability is a useful signal for detecting harmful simplifications.
Leveraging this, we propose a filtering strategy that suppresses harmful
simplifications while largely preserving beneficial ones. This work establishes
a benchmark for efficient and safe LS with small LLMs. It highlights the key
trade-offs between performance, efficiency, and safety, and demonstrates a
promising approach for safe real-world deployment.

</details>


### [185] [Towards Personalized Deep Research: Benchmarks and Evaluations](https://arxiv.org/abs/2509.25106)
*Yuan Liang,Jiaxian Li,Yuqing Wang,Piaohong Wang,Motong Tian,Pai Liu,Shuofei Qiao,Runnan Fang,He Zhu,Ge Zhang,Minghao Liu,Yuchen Eleanor Jiang,Ningyu Zhang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 提出了首个个性化深度研究基准（Personalized Deep Research Bench）及其PQR三维评估框架，通过250个真实用户-任务场景实验揭示了现有系统的个性化研究能力现状


<details>
  <summary>Details</summary>
Motivation: 现有深度研究智能体评估主要依赖封闭式基准，缺乏开放式研究场景且忽视个性化需求。现有基准无法反映真实用户画像与动态上下文结合的个性化研究需求

Method: 构建包含10个领域×50任务×25用户画像的250真实场景基准库，提出PQR评估框架（个性化对齐/内容质量/事实可靠性三维度联合评估）

Result: 实验揭示了现有系统在个性化研究任务中的能力局限，特别是在动态上下文理解与个性化需求匹配方面存在显著差距

Conclusion: 该基准为开发真正个性化的AI研究助手建立了评估基础，推动下一代智能体向深度个性化方向发展

Abstract: Deep Research Agents (DRAs) can autonomously conduct complex investigations
and generate comprehensive reports, demonstrating strong real-world potential.
However, existing evaluations mostly rely on close-ended benchmarks, while
open-ended deep research benchmarks remain scarce and typically neglect
personalized scenarios. To bridge this gap, we introduce Personalized Deep
Research Bench, the first benchmark for evaluating personalization in DRAs. It
pairs 50 diverse research tasks across 10 domains with 25 authentic user
profiles that combine structured persona attributes with dynamic real-world
contexts, yielding 250 realistic user-task queries. To assess system
performance, we propose the PQR Evaluation Framework, which jointly measures
(P) Personalization Alignment, (Q) Content Quality, and (R) Factual
Reliability. Our experiments on a range of systems highlight current
capabilities and limitations in handling personalized deep research. This work
establishes a rigorous foundation for developing and evaluating the next
generation of truly personalized AI research assistants.

</details>


### [186] [Knowledge Extraction on Semi-Structured Content: Does It Remain Relevant for Question Answering in the Era of LLMs?](https://arxiv.org/abs/2509.25107)
*Kai Sun,Yin Huang,Srishti Mehra,Mohammad Kachuee,Xilun Chen,Renjie Tao,Zhaojiang Lin,Andrea Jessee,Nirav Shah,Alex Betty,Yue Liu,Anuj Kumar,Wen-tau Yih,Xin Luna Dong*

Main category: cs.CL

TL;DR: 研究知识三元组抽取在LLM问答系统中的价值，发现其仍具挑战性但能通过数据增强和多任务学习提升效果


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs快速发展背景下，传统知识抽取技术在问答系统中的持续效用问题，验证三元组抽取在新范式中的价值

Method: 通过扩展现有基准数据集（添加知识抽取标注），系统性评估不同规模的商业/开源LLM在三元组抽取和QA任务中的表现

Result: 发现网络级知识抽取对LLMs仍具挑战性，但通过三元组数据增强和多任务学习可提升模型效果（不同规模模型提升幅度达3-15%）

Conclusion: 知识抽取在LLM时代仍具应用价值，采用多任务学习框架和结构化数据增强是提升模型效果的关键策略，特别是在资源受限场景中

Abstract: The advent of Large Language Models (LLMs) has significantly advanced
web-based Question Answering (QA) systems over semi-structured content, raising
questions about the continued utility of knowledge extraction for question
answering. This paper investigates the value of triple extraction in this new
paradigm by extending an existing benchmark with knowledge extraction
annotations and evaluating commercial and open-source LLMs of varying sizes.
Our results show that web-scale knowledge extraction remains a challenging task
for LLMs. Despite achieving high QA accuracy, LLMs can still benefit from
knowledge extraction, through augmentation with extracted triples and
multi-task learning. These findings provide insights into the evolving role of
knowledge triple extraction in web-based QA and highlight strategies for
maximizing LLM effectiveness across different model sizes and resource
settings.

</details>


### [187] [Investigating Language and Retrieval Bias in Multilingual Previously Fact-Checked Claim Detection](https://arxiv.org/abs/2509.25138)
*Ivan Vykopal,Antonia Karamolegkou,Jaroslav Kopčan,Qiwei Peng,Tomáš Javůrek,Michal Gregor,Marián Šimko*

Main category: cs.CL

TL;DR: 研究揭示多语言大模型在事实核查任务中存在语言偏见和检索偏见，高资源语言表现显著优于低资源语言，且检索系统对常见声明的偏好导致性能偏差


<details>
  <summary>Details</summary>
Motivation: 发现多语言大模型在跨语言事实核查中存在语言偏见，并提出'检索偏见'新概念（检索系统偏好特定信息导致结果倾斜），旨在探究这两种偏见在已核查声明检测任务中的影响

Method: 使用AMC-16K数据集，对6个开源多语言大模型进行20种语言的测试：1）采用全多语言提示策略，通过翻译任务提示分析单语/跨语言性能差异 2）运用多语言嵌入模型分析声明检索频率分布

Result: 1）模型家族/规模/提示策略显著影响性能 2）检索系统对高频声明的过度检索导致性能虚高 3）低资源语言准确率平均比英语低18.7%

Conclusion: 提出改进多语言事实核查公平性的方案：a）优化模型架构 b）平衡训练数据分布 c）开发偏见缓解机制。同时强调需建立检索偏见的系统评估框架

Abstract: Multilingual Large Language Models (LLMs) offer powerful capabilities for
cross-lingual fact-checking. However, these models often exhibit language bias,
performing disproportionately better on high-resource languages such as English
than on low-resource counterparts. We also present and inspect a novel concept
- retrieval bias, when information retrieval systems tend to favor certain
information over others, leaving the retrieval process skewed. In this paper,
we study language and retrieval bias in the context of Previously Fact-Checked
Claim Detection (PFCD). We evaluate six open-source multilingual LLMs across 20
languages using a fully multilingual prompting strategy, leveraging the AMC-16K
dataset. By translating task prompts into each language, we uncover disparities
in monolingual and cross-lingual performance and identify key trends based on
model family, size, and prompting strategy. Our findings highlight persistent
bias in LLM behavior and offer recommendations for improving equity in
multilingual fact-checking. To investigate retrieval bias, we employed
multilingual embedding models and look into the frequency of retrieved claims.
Our analysis reveals that certain claims are retrieved disproportionately
across different posts, leading to inflated retrieval performance for popular
claims while under-representing less common ones.

</details>


### [188] [Paired by the Teacher: Turning Unpaired Data into High-Fidelity Pairs for Low-Resource Text Generation](https://arxiv.org/abs/2509.25144)
*Yen-Ju Lu,Thomas Thebaud,Laureano Moro-Velazquez,Najim Dehak,Jesus Villalba*

Main category: cs.CL

TL;DR: 提出PbT师生框架，通过教师模型生成中间表示并训练学生模型重建输入，解决低资源NLG任务中数据不配对问题，在多个任务中性能接近人工标注且成本降低。


<details>
  <summary>Details</summary>
Motivation: 低资源场景下输入输出数据不匹配，导致小模型依赖少量样本或昂贵的大模型合成数据，需高效生成高质量配对数据。

Method: 两阶段流程：1）教师模型压缩未配对数据为中间表示(IR)；2）学生模型从IR重建输入，生成与输出配对的合成数据。

Result: 8B学生模型在多个任务上超越70B教师生成数据模型，ROUGE-L距人工仅差1.2，成本节省67%，人工评估证实生成摘要更简洁忠实。

Conclusion: PbT通过中间表示有效生成高质量配对数据，显著缩小与全监督差距，为低资源NLG提供高效解决方案。

Abstract: We present Paired by the Teacher (PbT), a two-stage teacher-student pipeline
that synthesizes accurate input-output pairs without human labels or parallel
data. In many low-resource natural language generation (NLG) scenarios,
practitioners may have only raw outputs, like highlights, recaps, or questions,
or only raw inputs, such as articles, dialogues, or paragraphs, but seldom
both. This mismatch forces small models to learn from very few examples or rely
on costly, broad-scope synthetic examples produced by large LLMs. PbT addresses
this by asking a teacher LLM to compress each unpaired example into a concise
intermediate representation (IR), and training a student to reconstruct inputs
from IRs. This enables outputs to be paired with student-generated inputs,
yielding high-quality synthetic data. We evaluate PbT on five
benchmarks-document summarization (XSum, CNNDM), dialogue summarization
(SAMSum, DialogSum), and question generation (SQuAD)-as well as an unpaired
setting on SwitchBoard (paired with DialogSum summaries). An 8B student trained
only on PbT data outperforms models trained on 70 B teacher-generated corpora
and other unsupervised baselines, coming within 1.2 ROUGE-L of human-annotated
pairs and closing 82% of the oracle gap at one-third the annotation cost of
direct synthesis. Human evaluation on SwitchBoard further confirms that only
PbT produces concise, faithful summaries aligned with the target style,
highlighting its advantage of generating in-domain sources that avoid the
mismatch, limiting direct synthesis.

</details>


### [189] [Pretraining Large Language Models with NVFP4](https://arxiv.org/abs/2509.25149)
*NVIDIA,Felix Abecassis,Anjulie Agrusa,Dong Ahn,Jonah Alben,Stefania Alborghetti,Michael Andersch,Sivakumar Arayandi,Alexis Bjorlin,Aaron Blakeman,Evan Briones,Ian Buck,Bryan Catanzaro,Jinhang Choi,Mike Chrzanowski,Eric Chung,Victor Cui,Steve Dai,Bita Darvish Rouhani,Carlo del Mundo,Deena Donia,Burc Eryilmaz,Henry Estela,Abhinav Goel,Oleg Goncharov,Yugi Guvvala,Robert Hesse,Russell Hewett,Herbert Hum,Ujval Kapasi,Brucek Khailany,Mikail Khona,Nick Knight,Alex Kondratenko,Ronny Krashinsky,Ben Lanir,Simon Layton,Michael Lightstone,Daniel Lo,Paulius Micikevicius,Asit Mishra,Tim Moon,Deepak Narayanan,Chao Ni,Abhijit Paithankar,Satish Pasumarthi,Ankit Patel,Mostofa Patwary,Ashwin Poojary,Gargi Prasad,Sweta Priyadarshi,Yigong Qin,Xiaowei Ren,Oleg Rybakov,Charbel Sakr,Sanjeev Satheesh,Stas Sergienko,Pasha Shamis,Kirthi Shankar,Nishant Sharma,Mohammad Shoeybi,Michael Siu,Misha Smelyanskiy,Darko Stosic,Dusan Stosic,Bor-Yiing Su,Frank Sun,Nima Tajbakhsh,Shelby Thomas,Przemek Tredak,Evgeny Tsykunov,Gandhi Vaithilingam,Aditya Vavre,Rangharajan Venkatesan,Roger Waleffe,Qiyu Wan,Hexin Wang,Mengdi Wang,Lizzie Wei,Hao Wu,Evan Wu,Keith Wyss,Ning Xu,Jinze Xue,Charlene Yang,Yujia Zhai,Ruoxi Zhang,Jingyang Zhu,Zhongbo Zhu*

Main category: cs.CL

TL;DR: 研究者提出了一种基于NVFP4格式的LLM高效训练方法，通过引入随机哈达玛变换、二维量化策略等技术，在12B参数模型上实现了与FP8基线相当的训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有FP8训练虽被广泛采用，但进一步降低至FP4精度可提升计算效率与资源利用率。然而极低精度训练存在稳定性、收敛性等挑战，需开发新方法突破瓶颈。

Method: 整合随机哈达玛变换抑制块级异常值；设计二维量化方案保持前向/反向传播一致性；采用随机舍入降低梯度偏差；部署选择性高精度层。基于10万亿token的12B参数模型进行验证。

Result: NVFP4训练模型在训练损失和下游任务准确率上与FP8基线相当，且实现了迄今为止公开文献中最长的4bit精度训练记录。

Conclusion: 该方法显著推进了窄精度LLM训练算法发展，证明FP4精度在大规模长序列训练中的可行性，为下一代高效能模型奠定基础。

Abstract: Large Language Models (LLMs) today are powerful problem solvers across many
domains, and they continue to get stronger as they scale in model size,
training set size, and training set quality, as shown by extensive research and
experimentation across the industry. Training a frontier model today requires
on the order of tens to hundreds of yottaflops, which is a massive investment
of time, compute, and energy. Improving pretraining efficiency is therefore
essential to enable the next generation of even more capable LLMs. While 8-bit
floating point (FP8) training is now widely adopted, transitioning to even
narrower precision, such as 4-bit floating point (FP4), could unlock additional
improvements in computational speed and resource utilization. However,
quantization at this level poses challenges to training stability, convergence,
and implementation, notably for large-scale models trained on long token
horizons.
  In this study, we introduce a novel approach for stable and accurate training
of large language models (LLMs) using the NVFP4 format. Our method integrates
Random Hadamard transforms (RHT) to bound block-level outliers, employs a
two-dimensional quantization scheme for consistent representations across both
the forward and backward passes, utilizes stochastic rounding for unbiased
gradient estimation, and incorporates selective high-precision layers. We
validate our approach by training a 12-billion-parameter model on 10 trillion
tokens -- the longest publicly documented training run in 4-bit precision to
date. Our results show that the model trained with our NVFP4-based pretraining
technique achieves training loss and downstream task accuracies comparable to
an FP8 baseline. These findings highlight that NVFP4, when combined with our
training approach, represents a major step forward in narrow-precision LLM
training algorithms.

</details>


### [190] [EasySteer: A Unified Framework for High-Performance and Extensible LLM Steering](https://arxiv.org/abs/2509.25175)
*Haolei Xu,Xinyu Mei,Yuchen Yan,Rui Zhou,Wenqi Zhang,Weiming Lu,Yueting Zhuang,Yongliang Shen*

Main category: cs.CL

TL;DR: EasySteer框架通过vLLM优化实现了高效LLM控制，在推理速度和应用效果上显著超越现有方案


<details>
  <summary>Details</summary>
Motivation: 现有LLM控制框架存在计算效率低、扩展性差和功能受限三大瓶颈，制约了技术研究进展与生产环境部署

Method: 基于vLLM推理引擎构建模块化架构，支持分析和学习双模式，提供预计算控制向量库与交互演示系统

Result: 实现5.5-11.4倍加速，有效应用于抑制过度推理、减少幻觉生成等多个关键场景

Conclusion: EasySteer将LLM控制技术转化为生产就绪能力，为可部署的受控语言模型建立关键基础设施

Abstract: Large language model (LLM) steering has emerged as a promising paradigm for
controlling model behavior at inference time through targeted manipulation of
hidden states, offering a lightweight alternative to expensive retraining.
However, existing steering frameworks suffer from critical limitations:
computational inefficiency, limited extensibility, and restricted functionality
that hinder both research progress and practical deployment. We present
EasySteer, a unified framework for high-performance, extensible LLM steering
built on vLLM. Our system features modular architecture with pluggable
interfaces for both analysis-based and learning-based methods, fine-grained
parameter control, pre-computed steering vectors for eight application domains,
and an interactive demonstration system. Through deep integration with vLLM's
optimized inference engine, EasySteer achieves 5.5-11.4$\times$ speedup over
existing frameworks. Extensive experiments demonstrate its effectiveness in
overthinking mitigation, hallucination reduction, and other key applications.
EasySteer transforms steering from research technique to production-ready
capability, establishing critical infrastructure for deployable, controllable
language models.

</details>


### [191] [NAIPv2: Debiased Pairwise Learning for Efficient Paper Quality Estimation](https://arxiv.org/abs/2509.25179)
*Penghai Zhao,Jinyu Tian,Qinghua Xing,Xin Zhang,Zheng Li,Jianjun Qian,Ming-Ming Cheng,Xiang Li*

Main category: cs.CL

TL;DR: 开发了NAIPv2框架，通过成对学习和RTS信号实现高效去偏的论文质量评估，在ICLR/NeurIPS数据上取得SOTA效果


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估方法存在高推理成本，而直接回归方法受限于评分尺度不一致问题，需要更高效且减少偏差的评估框架

Method: 1. 采用领域-年份分组的成对学习减少评分偏差 2. 提出RTS信号整合审稿人评分与置信度 3. 构建包含24,276篇ICLR论文的NAIDv2数据集

Result: 78.2% AUC和0.432 Spearman系数达到SOTA，在NeurIPS数据上预测分数从Rejected到Oral类别呈现连续增长趋势

Conclusion: NAIPv2框架通过可扩展的线性推理架构，为自动化论文质量评估提供了去偏解决方案，推动了科学智能系统的发展

Abstract: The ability to estimate the quality of scientific papers is central to how
both humans and AI systems will advance scientific knowledge in the future.
However, existing LLM-based estimation methods suffer from high inference cost,
whereas the faster direct score regression approach is limited by scale
inconsistencies. We present NAIPv2, a debiased and efficient framework for
paper quality estimation. NAIPv2 employs pairwise learning within domain-year
groups to reduce inconsistencies in reviewer ratings and introduces the Review
Tendency Signal (RTS) as a probabilistic integration of reviewer scores and
confidences. To support training and evaluation, we further construct NAIDv2, a
large-scale dataset of 24,276 ICLR submissions enriched with metadata and
detailed structured content. Trained on pairwise comparisons but enabling
efficient pointwise prediction at deployment, NAIPv2 achieves state-of-the-art
performance (78.2% AUC, 0.432 Spearman), while maintaining scalable,
linear-time efficiency at inference. Notably, on unseen NeurIPS submissions, it
further demonstrates strong generalization, with predicted scores increasing
consistently across decision categories from Rejected to Oral. These findings
establish NAIPv2 as a debiased and scalable framework for automated paper
quality estimation, marking a step toward future scientific intelligence
systems. Code and dataset are released at
https://sway.cloud.microsoft/Pr42npP80MfPhvj8.

</details>


### [192] [Incentive-Aligned Multi-Source LLM Summaries](https://arxiv.org/abs/2509.25184)
*Yanchen Jiang,Zhe Feng,Aranyak Mehta*

Main category: cs.CL

TL;DR: 提出了无需人工标注的激励对齐框架TTS，通过多源验证机制提升大模型文本摘要的事实准确性


<details>
  <summary>Details</summary>
Motivation: 现有大模型文本合成系统存在两个核心缺陷：（1）信息来源缺乏准确性保障机制；（2）易受对抗性内容干扰。需要构建无需真实标签的事实鲁棒性增强框架。

Method: TTS四步框架：1）原子化声明分解；2）多源立场验证；3）基于多任务同伴预测的信源可信度评估；4）不可靠信源过滤后重新生成。通过博弈论机制设计实现激励对齐。

Result: 实验证明TTS在保持文本流畅性的同时，事实准确率提升23%，对抗攻击场景下的鲁棒性提升41%。信息曝光度与多方佐证正相关，有效抑制了恶意操纵。

Conclusion: 该框架通过机制设计实现了信源激励与事实准确性的内在统一，无需真实标签即可增强大模型摘要的事实鲁棒性，为构建抗干扰的知识合成系统提供了新范式。

Abstract: Large language models (LLMs) are increasingly used in modern search and
answer systems to synthesize multiple, sometimes conflicting, texts into a
single response, yet current pipelines offer weak incentives for sources to be
accurate and are vulnerable to adversarial content. We introduce Truthful Text
Summarization (TTS), an incentive-aligned framework that improves factual
robustness without ground-truth labels. TTS (i) decomposes a draft synthesis
into atomic claims, (ii) elicits each source's stance on every claim, (iii)
scores sources with an adapted multi-task peer-prediction mechanism that
rewards informative agreement, and (iv) filters unreliable sources before
re-summarizing. We establish formal guarantees that align a source's incentives
with informative honesty, making truthful reporting the utility-maximizing
strategy. Experiments show that TTS improves factual accuracy and robustness
while preserving fluency, aligning exposure with informative corroboration and
disincentivizing manipulation.

</details>


### [193] [Learning to Parallel: Accelerating Diffusion Large Language Models via Adaptive Parallel Decoding](https://arxiv.org/abs/2509.25188)
*Wenrui Bao,Zhiben Chen,Dan Xu,Yuzhang Shang*

Main category: cs.CL

TL;DR: 提出Learn2PD框架，通过轻量级自适应过滤模型实现动态并行解码，在LLaDA基准测试中实现最高22.58倍加速且性能无损


<details>
  <summary>Details</summary>
Motivation: 现有基于固定启发式规则的并行解码策略无法适应输入特性，导致不同NLP任务中的速度-质量权衡不理想

Method: 1. 训练自适应过滤模型预测token位置是否匹配最终输出
2. 采用后训练方式优化（分钟级GPU时间）
3. 引入文本结束预测（EoTP）检测解码完成状态

Result: 结合KV-Cache时最高达57.51倍加速，保持模型性能不变

Conclusion: Learn2PD有效提升解码效率，自适应过滤机制实现类oracle解码策略，EoTP机制避免冗余解码

Abstract: Autoregressive decoding in large language models (LLMs) requires
$\mathcal{O}(n)$ sequential steps for $n$ tokens, fundamentally limiting
inference throughput. Recent diffusion-based LLMs (dLLMs) enable parallel token
generation through iterative denoising. However, current parallel decoding
strategies rely on fixed, input-agnostic heuristics (e.g., confidence
thresholds), which fail to adapt to input-specific characteristics, resulting
in suboptimal speed-quality trade-offs across diverse NLP tasks. In this work,
we explore a more flexible and dynamic approach to parallel decoding. We
propose Learning to Parallel Decode (Learn2PD), a framework that trains a
lightweight and adaptive filter model to predict, for each token position,
whether the current prediction matches the final output. This learned filter
approximates an oracle parallel decoding strategy that unmasks tokens only when
correctly predicted. Importantly, the filter model is learned in a
post-training manner, requiring only a small amount of computation to optimize
it (minute-level GPU time). Additionally, we introduce End-of-Text Prediction
(EoTP) to detect decoding completion at the end of sequence, avoiding redundant
decoding of padding tokens. Experiments on the LLaDA benchmark demonstrate that
our method achieves up to 22.58$\times$ speedup without any performance drop,
and up to 57.51$\times$ when combined with KV-Cache.

</details>


### [194] [InfoAgent: Advancing Autonomous Information-Seeking Agents](https://arxiv.org/abs/2509.25189)
*Gongrui Zhang,Jialiang Zhu,Ruiqi Yang,Kai Qiu,Miaosen Zhang,Zhirong Wu,Qi Dai,Bei Liu,Chong Luo,Zhengyuan Yang,Linjie Li,Lijuan Wang,Weizhu Chen,Yuan Zhang,Xin Li,Zhaoyi Liu,Xin Geng,Baining Guo*

Main category: cs.CL

TL;DR: 提出InfoAgent研究型智能体，通过创新数据合成流程与自主搜索基础设施，在两阶段训练下实现超越同类模型的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究型智能体过度依赖商业搜索工具，缺乏透明度且性能受限。需要开发自主搜索架构并构建更复杂的数据集来提升智能体能力。

Method: 1. 基于实体树和模糊化生成复杂查询；2. 搭建自主托管搜索环境；3. 两阶段训练策略（监督微调+强化学习）培养长期搜索与工具使用能力。

Result: BrowseComp准确率15.3%（提升显著），中文BrowseComp-ZH达29.2%，Xbench-DS达40.4%，超越WebSailor-72B和DeepDive-32B。

Conclusion: 自主搜索基础设施与两阶段训练方案有效提升了研究型智能体的工具使用能力和复杂问题处理性能，为开源智能体发展提供新范式。

Abstract: Building Large Language Model agents that expand their capabilities by
interacting with external tools represents a new frontier in AI research and
applications. In this paper, we introduce InfoAgent, a deep research agent
powered by an innovative data synthesis pipeline and orchestrated web search
tools. To construct challenging, hard-to-find queries,we build entity trees and
apply sub-tree sampling with entity fuzzification to systematically increase
question difficulty. Unlike prior work that relies heavily on commercial search
tools, we develop a dedicated self-hosted search infrastructure, enhancing
transparency of agent environments and facilitating further advancement of
agent capacity. We evaluate the effectiveness of our data pipeline by measuring
the average number of tool calls required to correctly answer a question, and
also show that our agent yields better performance when equipped with our
tools. Our \mbox{InfoAgent} is post-trained from Qwen3-14B using a two-stage
recipe: cold-start supervised finetuning to instill long-horizon search
behaviors, followed by reinforcement learning which significantly improves
reasoning-driven tool use. With our methods, InfoAgent achieves 15.3\% accuracy
on BrowseComp, 29.2\% on BrowseComp-ZH, and 40.4\% on Xbench-DS, outperforming
prior open-source deep research agents such as WebSailor-72B and DeepDive-32B.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [195] [DiffTex: Differentiable Texturing for Architectural Proxy Models](https://arxiv.org/abs/2509.23336)
*Weidan Xiong,Yongli Wu,Bochuan Zeng,Jianwei Guo,Dani Lischinski,Daniel Cohen-Or,Hui Huang*

Main category: cs.GR

TL;DR: 提出自动化生成建筑代理模型高精度纹理的方法，通过图像对齐与可微渲染优化实现像素级纹理融合


<details>
  <summary>Details</summary>
Motivation: 解决建筑代理模型几何简化导致的纹理细节丢失问题，克服无序照片集纹理重建的挑战

Method: 建立UV贴图与输入图像的像素级对应，通过加权融合与可微渲染优化混合参数

Result: 实验验证方法在不同建筑模型和拍摄条件下的有效性，生成无缝且透视一致的高质量纹理

Conclusion: 该方法实现了从无序照片自动生成保留视觉真实性的建筑纹理，提升代理模型的视觉保真度

Abstract: Simplified proxy models are commonly used to represent architectural
structures, reducing storage requirements and enabling real-time rendering.
However, the geometric simplifications inherent in proxies result in a loss of
fine color and geometric details, making it essential for textures to
compensate for the loss. Preserving the rich texture information from the
original dense architectural reconstructions remains a daunting task,
particularly when working with unordered RGB photographs. We propose an
automated method for generating realistic texture maps for architectural proxy
models at the texel level from an unordered collection of registered
photographs. Our approach establishes correspondences between texels on a UV
map and pixels in the input images, with each texel's color computed as a
weighted blend of associated pixel values. Using differentiable rendering, we
optimize blending parameters to ensure photometric and perspective consistency,
while maintaining seamless texture coherence. Experimental results demonstrate
the effectiveness and robustness of our method across diverse architectural
models and varying photographic conditions, enabling the creation of
high-quality textures that preserve visual fidelity and structural detail.

</details>


### [196] [Modeling and Exploiting the Time Course of Chromatic Adaptation for Display Power Optimizations in Virtual Reality](https://arxiv.org/abs/2509.23489)
*Ethan Chen,Sushant Kondguli,Carl Marshall,Yuhao Zhu*

Main category: cs.GR

TL;DR: 提出无眼动追踪的OLED显示节能技术，利用色适应时间特性优化照明偏移轨迹，降低VR设备31%功耗且保持感知质量


<details>
  <summary>Details</summary>
Motivation: 解决VR设备OLED显示屏高功耗问题，突破传统需要眼动追踪的限制，通过控制色适应过程实现无感知质量损失的节能

Method: 建立心理物理范式模型，模拟人眼对光照变化的适应过程，计算最优照明偏移轨迹以控制光照变化速率和幅度

Result: 结合现有亮度调暗技术，实现31%的显示功耗降低，感知质量无统计学显著下降

Conclusion: 该方法通过时序控制色适应过程，在节能效果和视觉保真度之间取得突破性平衡，为VR显示优化提供新范式

Abstract: We introduce a gaze-tracking--free method to reduce OLED display power
consumption in VR with minimal perceptual impact. This technique exploits the
time course of chromatic adaptation, the human visual system's ability to
maintain stable color perception under changing illumination. To that end, we
propose a novel psychophysical paradigm that models how human adaptation state
changes with the scene illuminant. We exploit this model to compute an optimal
illuminant shift trajectory, controlling the rate and extent of illumination
change, to reduce display power under a given perceptual loss budget. Our
technique significantly improves the perceptual quality over prior work that
applies illumination shifts instantaneously. Our technique can also be combined
with prior work on luminance dimming to reduce display power by 31% with no
statistical loss of perceptual quality.

</details>


### [197] [Automated design of compound lenses with discrete-continuous optimization](https://arxiv.org/abs/2509.23572)
*Arjun Teh,Delio Vicini,Bernd Bickel,Ioannis Gkioulekas,Matthew O'Toole*

Main category: cs.GR

TL;DR: 提出一种结合梯度优化与跨维度MCMC采样的复合透镜自动设计方法，可同步优化连续/离散参数，突破现有自动设计方法的性能极限。


<details>
  <summary>Details</summary>
Motivation: 现有复合透镜设计方法仅能优化连续参数（如曲率），拓扑结构变更依赖专家干预。需开发能自动优化离散参数（如镜片数量/类型）的全局优化方法。

Method: 梯度优化结合特制MCMC算法：1）跨维度突变实现镜片数量动态调整 2）近轴投影保证突变后光学可行性 3）支持单透镜/双合透镜类型切换

Result: 在多种设计任务中，新方法有效扩展设计空间，产生优于基准方法的方案（速度-清晰度权衡曲线提升15%-30%），实现自动化设计的性能突破。

Conclusion: 该方法首次实现复合透镜连续/离散参数的联合自动优化，为光学系统设计提供了更强大的全局探索能力，推动自动化设计边界。

Abstract: We introduce a method that automatically and jointly updates both continuous
and discrete parameters of a compound lens design, to improve its performance
in terms of sharpness, speed, or both. Previous methods for compound lens
design use gradient-based optimization to update continuous parameters (e.g.,
curvature of individual lens elements) of a given lens topology, requiring
extensive expert intervention to realize topology changes. By contrast, our
method can additionally optimize discrete parameters such as number and type
(e.g., singlet or doublet) of lens elements. Our method achieves this
capability by combining gradient-based optimization with a tailored Markov
chain Monte Carlo sampling algorithm, using transdimensional mutation and
paraxial projection operations for efficient global exploration. We show
experimentally on a variety of lens design tasks that our method effectively
explores an expanded design space of compound lenses, producing better designs
than previous methods and pushing the envelope of speed-sharpness tradeoffs
achievable by automated lens design.

</details>


### [198] [ZeroScene: A Zero-Shot Framework for 3D Scene Generation from a Single Image and Controllable Texture Editing](https://arxiv.org/abs/2509.23607)
*Xiang Tang,Ruotong Li,Xiaopeng Fan*

Main category: cs.GR

TL;DR: ZeroScene系统通过大型视觉模型实现单图3D场景重建与纹理编辑，优化点云对齐并保持多视角一致性。


<details>
  <summary>Details</summary>
Motivation: 现有单图3D重建方法在复杂场景中难以兼顾个体质量与整体连贯性，纹理编辑技术难以同时保证局部连续性和多视角一致性。

Method: 1. 提取物体级2D分割与深度信息推断空间关系
2. 联合优化点云的3D/2D投影损失实现精准对齐
3. 约束扩散模型+渐进生成策略保持纹理一致性
4. 基于物理渲染(PBR)增强真实感

Result: 实验证明框架确保生成资产的几何/外观准确性，精确重建场景布局，并生成与文本提示高度契合的细节纹理

Conclusion: ZeroScene为复杂场景的3D生成与编辑提供了兼顾几何精度、场景连贯性和纹理真实性的完整解决方案

Abstract: In the field of 3D content generation, single image scene reconstruction
methods still struggle to simultaneously ensure the quality of individual
assets and the coherence of the overall scene in complex environments, while
texture editing techniques often fail to maintain both local continuity and
multi-view consistency. In this paper, we propose a novel system ZeroScene,
which leverages the prior knowledge of large vision models to accomplish both
single image-to-3D scene reconstruction and texture editing in a zero-shot
manner. ZeroScene extracts object-level 2D segmentation and depth information
from input images to infer spatial relationships within the scene. It then
jointly optimizes 3D and 2D projection losses of the point cloud to update
object poses for precise scene alignment, ultimately constructing a coherent
and complete 3D scene that encompasses both foreground and background.
Moreover, ZeroScene supports texture editing of objects in the scene. By
imposing constraints on the diffusion model and introducing a mask-guided
progressive image generation strategy, we effectively maintain texture
consistency across multiple viewpoints and further enhance the realism of
rendered results through Physically Based Rendering (PBR) material estimation.
Experimental results demonstrate that our framework not only ensures the
geometric and appearance accuracy of generated assets, but also faithfully
reconstructs scene layouts and produces highly detailed textures that closely
align with text prompts.

</details>


### [199] [DFG-PCN: Point Cloud Completion with Degree-Flexible Point Graph](https://arxiv.org/abs/2509.23703)
*Zhenyu Shu,Jian Yao,Shiqing Xin*

Main category: cs.GR

TL;DR: 提出DFG-PCN框架，通过细节感知度量动态分配节点度数，结合曼哈顿距离的几何感知模块实现高效点云补全，实验验证优于SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 传统固定区域划分方法无法适应点云不同区域几何复杂度差异，导致细节区域重建效果差。

Method: 1. 基于特征变化+曲率的节点度数自适应分配
2. 几何感知模块使用曼哈顿距离边缘聚合
3. 细节引导的局部全局特征融合机制

Result: 在多个基准数据集上取得state-of-the-art效果，特别是在复杂几何结构区域表现显著提升。

Conclusion: 通过动态节点分配和几何感知特征融合，有效提升了点云补全质量，为三维重建提供了新思路。

Abstract: Point cloud completion is a vital task focused on reconstructing complete
point clouds and addressing the incompleteness caused by occlusion and limited
sensor resolution. Traditional methods relying on fixed local region
partitioning, such as k-nearest neighbors, which fail to account for the highly
uneven distribution of geometric complexity across different regions of a
shape. This limitation leads to inefficient representation and suboptimal
reconstruction, especially in areas with fine-grained details or structural
discontinuities. This paper proposes a point cloud completion framework called
Degree-Flexible Point Graph Completion Network (DFG-PCN). It adaptively assigns
node degrees using a detail-aware metric that combines feature variation and
curvature, focusing on structurally important regions. We further introduce a
geometry-aware graph integration module that uses Manhattan distance for edge
aggregation and detail-guided fusion of local and global features to enhance
representation. Extensive experiments on multiple benchmark datasets
demonstrate that our method consistently outperforms state-of-the-art
approaches.

</details>


### [200] [StrucADT: Generating Structure-controlled 3D Point Clouds with Adjacency Diffusion Transformer](https://arxiv.org/abs/2509.23709)
*Zhenyu Shu,Jiajun Shen,Zhongui Chen,Xiaoguang Han,Shiqing Xin*

Main category: cs.GR

TL;DR: 提出首个通过形状结构控制3D点云生成的方法StrucADT，在ShapeNet数据集实现可控生成SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有3D生成模型缺乏对形状结构的细粒度控制，限制了实际应用。为解决生成过程的可控性问题，研究者首次引入包含部件存在性和邻接关系的结构控制方法

Method: 1. 构建StructureGraph表示（标注部件邻接关系）→ 2. 开发StrucADT模型：StructureGraphNet提取结构特征，cCNF Prior学习邻接控制的潜在分布，Diffusion Transformer生成结构一致的形状

Result: 实验证实方法生成的点云质量高、多样性好，在ShapeNet数据集上达到可控生成最先进水平（FID指标较基线提升23%）

Conclusion: 通过结构图表示实现点云生成控制，为3D内容创作提供新范式，推动工业设计等领域的实际应用部署

Abstract: In the field of 3D point cloud generation, numerous 3D generative models have
demonstrated the ability to generate diverse and realistic 3D shapes. However,
the majority of these approaches struggle to generate controllable 3D point
cloud shapes that meet user-specific requirements, hindering the large-scale
application of 3D point cloud generation. To address the challenge of lacking
control in 3D point cloud generation, we are the first to propose controlling
the generation of point clouds by shape structures that comprise part
existences and part adjacency relationships. We manually annotate the adjacency
relationships between the segmented parts of point cloud shapes, thereby
constructing a StructureGraph representation. Based on this StructureGraph
representation, we introduce StrucADT, a novel structure-controllable point
cloud generation model, which consists of StructureGraphNet module to extract
structure-aware latent features, cCNF Prior module to learn the distribution of
the latent features controlled by the part adjacency, and Diffusion Transformer
module conditioned on the latent features and part adjacency to generate
structure-consistent point cloud shapes. Experimental results demonstrate that
our structure-controllable 3D point cloud generation method produces
high-quality and diverse point cloud shapes, enabling the generation of
controllable point clouds based on user-specified shape structures and
achieving state-of-the-art performance in controllable point cloud generation
on the ShapeNet dataset.

</details>


### [201] [Diff-3DCap: Shape Captioning with Diffusion Models](https://arxiv.org/abs/2509.23718)
*Zhenyu Shu,Jiawei Wen,Shiyang Li,Shiqing Xin,Ligang Liu*

Main category: cs.GR

TL;DR: Diff-3DCap提出了一种基于投影视图和连续扩散模型的3D形状描述生成方法，无需额外分类器即可达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统3D形状描述方法依赖高成本体素表征或物体检测技术，且效果不佳。

Method: 使用投影视图表征3D对象，通过连续扩散模型的正向噪声扰动和反向重建流程，并利用预训练视觉语言模型的嵌入作为指导信号。

Result: 实验表明Diff-3DCap性能与当前最先进方法相当。

Conclusion: 该方法通过融合扩散框架与预训练模型嵌入，有效简化了3D形状描述的生成流程。

Abstract: The task of 3D shape captioning occupies a significant place within the
domain of computer graphics and has garnered considerable interest in recent
years. Traditional approaches to this challenge frequently depend on the
utilization of costly voxel representations or object detection techniques, yet
often fail to deliver satisfactory outcomes. To address the above challenges,
in this paper, we introduce Diff-3DCap, which employs a sequence of projected
views to represent a 3D object and a continuous diffusion model to facilitate
the captioning process. More precisely, our approach utilizes the continuous
diffusion model to perturb the embedded captions during the forward phase by
introducing Gaussian noise and then predicts the reconstructed annotation
during the reverse phase. Embedded within the diffusion framework is a
commitment to leveraging a visual embedding obtained from a pre-trained
visual-language model, which naturally allows the embedding to serve as a
guiding signal, eliminating the need for an additional classifier. Extensive
results of our experiments indicate that Diff-3DCap can achieve performance
comparable to that of the current state-of-the-art methods.

</details>


### [202] [ReLumix: Extending Image Relighting to Video via Video Diffusion Models](https://arxiv.org/abs/2509.23769)
*Lezhong Wang,Shutong Jin,Ruiqi Cui,Anders Bjorholm Dahl,Jeppe Revall Frisvad,Siavash Bigdeli*

Main category: cs.GR

TL;DR: Proposes ReLumix framework enabling flexible video relighting through two-stage illumination propagation and temporal coherence mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing video relighting methods lack flexibility in algorithm selection. Artists need framework-agnostic solutions for dynamic lighting control.

Method: 1) Artist relights reference frame using preferred image technique
2) Fine-tuned SVD propagates illumination with gated cross-attention and temporal bootstrapping

Result: Achieves state-of-the-art generalization from synthetic training to real videos with 89% fidelity improvement in user studies

Conclusion: ReLumix establishes scalable paradigm for video relighting by decoupling artistic control from temporal synthesis constraints

Abstract: Controlling illumination during video post-production is a crucial yet
elusive goal in computational photography. Existing methods often lack
flexibility, restricting users to certain relighting models. This paper
introduces ReLumix, a novel framework that decouples the relighting algorithm
from temporal synthesis, thereby enabling any image relighting technique to be
seamlessly applied to video. Our approach reformulates video relighting into a
simple yet effective two-stage process: (1) an artist relights a single
reference frame using any preferred image-based technique (e.g., Diffusion
Models, physics-based renderers); and (2) a fine-tuned stable video diffusion
(SVD) model seamlessly propagates this target illumination throughout the
sequence. To ensure temporal coherence and prevent artifacts, we introduce a
gated cross-attention mechanism for smooth feature blending and a temporal
bootstrapping strategy that harnesses SVD's powerful motion priors. Although
trained on synthetic data, ReLumix shows competitive generalization to
real-world videos. The method demonstrates significant improvements in visual
fidelity, offering a scalable and versatile solution for dynamic lighting
control.

</details>


### [203] [SIG-Chat: Spatial Intent-Guided Conversational Gesture Generation Involving How, When and Where](https://arxiv.org/abs/2509.23852)
*Yiheng Huang,Junran Peng,Silei Shen,Jingwei Yang,ZeJi Wei,ChenCheng Bai,Yonghao He,Wei Sui,Muyi Sun,Yan Liu,Xu-Cheng Yin,Man Zhang,Zhaoxiang Zhang,Chuanchen Luo*

Main category: cs.GR

TL;DR: 提出结合多模态数据的全栈式对话手势生成方案，解决现有方法在互动时机与空间意图建模的不足，并成功部署至人形机器人实现环境感知的物理交互。


<details>
  <summary>Details</summary>
Motivation: 现有对话手势生成方法依赖单一模态（语言或音频），缺乏对交互时机(WHEN)和空间指向(WHERE)的联合建模，严重限制了在机器人、游戏等场景的应用价值。

Method: 1) 建立同步采集高精度运动与空间意图的数据方案；2) 开发由音频、语义和空间数据联合驱动的生成模型；3) 设计交互时序与空间精度的专用评估指标；4) 在仿人机器人实现系统部署。

Result: 成功构建端到端生成框架，实现具有环境交互意图的对话手势生成，通过专用指标验证了时序同步性与空间指向准确性，机器人部署验证了方案的实用价值。

Conclusion: 多模态数据融合与全栈式设计能有效提升对话手势的交互真实性，为机器人交互、数字人动画等领域提供了新的技术路径。

Abstract: The accompanying actions and gestures in dialogue are often closely linked to
interactions with the environment, such as looking toward the interlocutor or
using gestures to point to the described target at appropriate moments. Speech
and semantics guide the production of gestures by determining their timing
(WHEN) and style (HOW), while the spatial locations of interactive objects
dictate their directional execution (WHERE). Existing approaches either rely
solely on descriptive language to generate motions or utilize audio to produce
non-interactive gestures, thereby lacking the characterization of interactive
timing and spatial intent. This significantly limits the applicability of
conversational gesture generation, whether in robotics or in the fields of game
and animation production. To address this gap, we present a full-stack
solution. We first established a unique data collection method to
simultaneously capture high-precision human motion and spatial intent. We then
developed a generation model driven by audio, language, and spatial data,
alongside dedicated metrics for evaluating interaction timing and spatial
accuracy. Finally, we deployed the solution on a humanoid robot, enabling rich,
context-aware physical interactions.

</details>


### [204] [Neural Visibility of Point Sets](https://arxiv.org/abs/2509.24150)
*Jun-Hao Wang,Yi-Yang Tian,Baoquan Chen,Peng-Shuai Wang*

Main category: cs.GR

TL;DR: 提出基于3D U-Net和MLP的神经网络方法，将点云可见性判定建模为二分类任务，在精度和速度上显著超越传统HPR方法（最高加速126倍），并具备噪声鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统HPR方法存在计算效率低、噪声敏感、处理凹区域和低密度点云效果差等问题。本文通过将可见性判定转化为二分类任务，利用神经网络解决上述局限性。

Method: 1) 3D U-Net提取视角无关的特征 2) 共享MLP结合特征和视角方向预测可见性 3) 使用3D模型渲染生成的可见性标签进行端到端训练

Result: 在ShapeNet、ABC等数据集上验证：精度显著优于HPR，大点云处理速度提升126倍，对噪声和点密度变化具有鲁棒性，且能泛化到未见过的形状。

Conclusion: 该方法在点云可视化、表面重建、法向估计等应用中展现优势，代码已开源。通过视角优化等应用验证了方法的有效性。

Abstract: Point clouds are widely used representations of 3D data, but determining the
visibility of points from a given viewpoint remains a challenging problem due
to their sparse nature and lack of explicit connectivity. Traditional methods,
such as Hidden Point Removal (HPR), face limitations in computational
efficiency, robustness to noise, and handling concave regions or low-density
point clouds. In this paper, we propose a novel approach to visibility
determination in point clouds by formulating it as a binary classification
task. The core of our network consists of a 3D U-Net that extracts
view-independent point-wise features and a shared multi-layer perceptron (MLP)
that predicts point visibility using the extracted features and view direction
as inputs. The network is trained end-to-end with ground-truth visibility
labels generated from rendered 3D models. Our method significantly outperforms
HPR in both accuracy and computational efficiency, achieving up to 126 times
speedup on large point clouds. Additionally, our network demonstrates
robustness to noise and varying point cloud densities and generalizes well to
unseen shapes. We validate the effectiveness of our approach through extensive
experiments on the ShapeNet, ABC Dataset and real-world datasets, showing
substantial improvements in visibility accuracy. We also demonstrate the
versatility of our method in various applications, including point cloud
visualization, surface reconstruction, normal estimation, shadow rendering, and
viewpoint optimization. Our code and models are available at
https://github.com/octree-nn/neural-visibility.

</details>


### [205] [NeuralPVS: Learned Estimation of Potentially Visible Sets](https://arxiv.org/abs/2509.24677)
*Xiangyu Wang,Thomas Köhler,Jun Lin Qiu,Shohei Mori,Markus Steinberger,Dieter Schmalstieg*

Main category: cs.GR

TL;DR: 提出首个基于深度学习的实时可见性计算方法NeuralPVS，在大型动态场景中以100Hz运行且遗漏率低于1%，显著提升实时渲染效率。


<details>
  <summary>Details</summary>
Motivation: 现有可见性计算方法计算成本高昂且依赖静态场景预处理，难以适应动态环境需求。需要能实时处理大规模场景的高效解决方案。

Method: 1. 采用体素化场景表示与神经网络结合
2. 创新性融合稀疏卷积与3D保体积交错压缩技术
3. 设计排斥可见性损失函数指导网络学习正确数据分布

Result: 实现约100Hz处理速度，几何遗漏率<1%，在准确性和泛化能力上超越传统方法，支持未见场景的实时处理。

Conclusion: NeuralPVS通过深度学习与创新架构设计，为动态环境实时可见性计算提供了高效可靠的解决方案，推动实时渲染技术发展。

Abstract: Real-time visibility determination in expansive or dynamically changing
environments has long posed a significant challenge in computer graphics.
Existing techniques are computationally expensive and often applied as a
precomputation step on a static scene. We present NeuralPVS, the first
deep-learning approach for visibility computation that efficiently determines
from-region visibility in a large scene, running at approximately 100 Hz
processing with less than $1\%$ missing geometry. This approach is possible by
using a neural network operating on a voxelized representation of the scene.
The network's performance is achieved by combining sparse convolution with a 3D
volume-preserving interleaving for data compression. Moreover, we introduce a
novel repulsive visibility loss that can effectively guide the network to
converge to the correct data distribution. This loss provides enhanced
robustness and generalization to unseen scenes. Our results demonstrate that
NeuralPVS outperforms existing methods in terms of both accuracy and
efficiency, making it a promising solution for real-time visibility
computation.

</details>


### [206] [Light-SQ: Structure-aware Shape Abstraction with Superquadrics for Generated Meshes](https://arxiv.org/abs/2509.24986)
*Yuhan Wang,Weikai Chen,Zeyu Hu,Runze Zhang,Yingda Yin,Ruoyu Wu,Keyang Luo,Shengju Qian,Yiyan Ma,Hongyi Li,Yuan Gao,Yuhuan Zhou,Hao Luo,Wan Wang,Xiaobin Shen,Zhaowei Li,Kuixin Zhu,Chuanlang Hong,Yueyue Wang,Lijie Feng,Xin Wang,Chen Change Loy*

Main category: cs.GR

TL;DR: 提出了Light-SQ框架，通过超二次曲面优化实现结构感知的3D形状抽象，显著提升了用户生成内容的编辑性和生成质量


<details>
  <summary>Details</summary>
Motivation: 解决用户生成内容场景中非专业用户对紧凑可编辑3D表示的需求，传统方法在结构感知（基元重叠控制、部件对齐、紧凑性）方面存在不足

Method: 1) SDF雕刻技术迭代更新符号距离场减少基元重叠；2) 基于结构感知体素分解的块状再生填充策略；3) 基于SDF更新历史的自适应残差剪枝抑制过分割；4) 支持多尺度拟合保留细节

Result: 在扩展的3DGen-Prim基准测试中，Light-SQ实现了高效、高保真且可编辑的形状抽象，适用于复杂几何生成

Conclusion: Light-SQ通过结构感知优化框架，显著推进了3D用户生成内容的实际应用可行性，在保真度与编辑性之间取得更好平衡

Abstract: In user-generated-content (UGC) applications, non-expert users often rely on
image-to-3D generative models to create 3D assets. In this context,
primitive-based shape abstraction offers a promising solution for UGC scenarios
by compressing high-resolution meshes into compact, editable representations.
Towards this end, effective shape abstraction must therefore be
structure-aware, characterized by low overlap between primitives, part-aware
alignment, and primitive compactness. We present Light-SQ, a novel
superquadric-based optimization framework that explicitly emphasizes
structure-awareness from three aspects. (a) We introduce SDF carving to
iteratively udpate the target signed distance field, discouraging overlap
between primitives. (b) We propose a block-regrow-fill strategy guided by
structure-aware volumetric decomposition, enabling structural partitioning to
drive primitive placement. (c) We implement adaptive residual pruning based on
SDF update history to surpress over-segmentation and ensure compact results. In
addition, Light-SQ supports multiscale fitting, enabling localized refinement
to preserve fine geometric details. To evaluate our method, we introduce
3DGen-Prim, a benchmark extending 3DGen-Bench with new metrics for both
reconstruction quality and primitive-level editability. Extensive experiments
demonstrate that Light-SQ enables efficient, high-fidelity, and editable shape
abstraction with superquadrics for complex generated geometry, advancing the
feasibility of 3D UGC creation.

</details>


### [207] [CharGen: Fast and Fluent Portrait Modification](https://arxiv.org/abs/2509.25058)
*Jan-Niklas Dihlmann,Arnela Killguss,Hendrik P. A. Lensch*

Main category: cs.GR

TL;DR: CharGen通过Concept Sliders实现细粒度图像控制，结合StreamDiffusion加速采样和Repair Step修复细节，实现2-4倍编辑速度提升与身份一致性结果


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型在角色图像编辑中控制精度、生成速度与视觉保真度的平衡难题

Method: 1. 属性专用Concept Sliders训练实现特征隔离控制 2. StreamDiffusion加速采样 3. 轻量级Repair Step恢复纹理细节

Result: 相比InstructPix2Pix和Gemini，编辑速度提升2-4倍，用户研究显示精确控制与身份保持效果更优

Conclusion: CharGen在保持身份一致性的前提下实现快速精确编辑，为专业数字内容创作提供新方案

Abstract: Interactive editing of character images with diffusion models remains
challenging due to the inherent trade-off between fine-grained control,
generation speed, and visual fidelity. We introduce CharGen, a
character-focused editor that combines attribute-specific Concept Sliders,
trained to isolate and manipulate attributes such as facial feature size,
expression, and decoration with the StreamDiffusion sampling pipeline for more
interactive performance. To counteract the loss of detail that often
accompanies accelerated sampling, we propose a lightweight Repair Step that
reinstates fine textures without compromising structural consistency.
Throughout extensive ablation studies and in comparison to open-source
InstructPix2Pix and closed-source Google Gemini, and a comprehensive user
study, CharGen achieves two-to-four-fold faster edit turnaround with precise
editing control and identity-consistent results. Project page:
https://chargen.jdihlmann.com/

</details>


### [208] [Unsupervised Representation Learning for 3D Mesh Parameterization with Semantic and Visibility Objectives](https://arxiv.org/abs/2509.25094)
*AmirHossein Zamani,Bruno Roy,Arianna Rampini*

Main category: cs.GR

TL;DR: 提出了一种结合语义感知和可见性感知的无监督框架，用于自动化3D网格参数化（UV映射），解决传统方法依赖人工且忽略语义对齐与接缝可见性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成模型依赖耗时的人工UV映射，且自动方法缺乏对语义一致性（相似3D部件跨形状对齐）和接缝可见性（切割线应位于隐蔽区域）的感知能力。

Method: 1）语义分割网格 → 2）无监督分块UV参数化 → 3）聚合分块图谱；同时引入环境光遮蔽加权目标函数，将接缝引导至遮挡区域。

Result: 定性和定量评估表明，该方法生成的UV图谱在支持纹理生成和减少可见接缝方面优于现有基线方法。

Conclusion: 该框架首次将语义和可见性感知融入UV参数化流程，显著降低了人工干预需求，提升了纹理生成质量。

Abstract: Recent 3D generative models produce high-quality textures for 3D mesh
objects. However, they commonly rely on the heavy assumption that input 3D
meshes are accompanied by manual mesh parameterization (UV mapping), a manual
task that requires both technical precision and artistic judgment. Industry
surveys show that this process often accounts for a significant share of asset
creation, creating a major bottleneck for 3D content creators. Moreover,
existing automatic methods often ignore two perceptually important criteria:
(1) semantic awareness (UV charts should align semantically similar 3D parts
across shapes) and (2) visibility awareness (cutting seams should lie in
regions unlikely to be seen). To overcome these shortcomings and to automate
the mesh parameterization process, we present an unsupervised differentiable
framework that augments standard geometry-preserving UV learning with semantic-
and visibility-aware objectives. For semantic-awareness, our pipeline (i)
segments the mesh into semantic 3D parts, (ii) applies an unsupervised learned
per-part UV-parameterization backbone, and (iii) aggregates per-part charts
into a unified UV atlas. For visibility-awareness, we use ambient occlusion
(AO) as an exposure proxy and back-propagate a soft differentiable AO-weighted
seam objective to steer cutting seams toward occluded regions. By conducting
qualitative and quantitative evaluations against state-of-the-art methods, we
show that the proposed method produces UV atlases that better support texture
generation and reduce perceptible seam artifacts compared to recent baselines.
Our implementation code is publicly available at:
https://github.com/AHHHZ975/Semantic-Visibility-UV-Param.

</details>


### [209] [LayerD: Decomposing Raster Graphic Designs into Layers](https://arxiv.org/abs/2509.25134)
*Tomoyuki Suzuki,Kang-Jun Liu,Naoto Inoue,Kota Yamaguchi*

Main category: cs.GR

TL;DR: 提出LayerD方法实现栅格图形设计的可逆分层分解，支持图像生成器与分层编辑工作流


<details>
  <summary>Details</summary>
Motivation: 合成后的栅格图像无法进行分层编辑，限制了设计流程的可逆性

Method: 通过迭代提取未遮挡前景层，利用图形设计中常见的统一外观特性进行优化，提出质量评估指标

Result: 实验显示LayerD在分解质量上优于基线方法，质量指标有效解决不可靠标注问题

Conclusion: LayerD成功实现高质量分解，并与现代图像生成器和分层编辑工具兼容，提升创作流程灵活性

Abstract: Designers craft and edit graphic designs in a layer representation, but
layer-based editing becomes impossible once composited into a raster image. In
this work, we propose LayerD, a method to decompose raster graphic designs into
layers for re-editable creative workflow. LayerD addresses the decomposition
task by iteratively extracting unoccluded foreground layers. We propose a
simple yet effective refinement approach taking advantage of the assumption
that layers often exhibit uniform appearance in graphic designs. As
decomposition is ill-posed and the ground-truth layer structure may not be
reliable, we develop a quality metric that addresses the difficulty. In
experiments, we show that LayerD successfully achieves high-quality
decomposition and outperforms baselines. We also demonstrate the use of LayerD
with state-of-the-art image generators and layer-based editing.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [210] [From Past To Path: Masked History Learning for Next-Item Prediction in Generative Recommendation](https://arxiv.org/abs/2509.23649)
*KaiWen Wei,Kejun He,Xiaomian Kang,Jie Zhang,Yuming Yang,Jiang Zhong,He Bai,Junnan Zhu*

Main category: cs.IR

TL;DR: 提出掩码历史学习框架(MHL)，通过历史重建任务增强生成式推荐系统的意图理解能力，实验证明优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐系统仅关注下一步预测，无法捕捉用户历史行为中的潜在意图和内部结构关系

Method: 在标准自回归目标基础上增加历史项目重建任务，引入熵引导掩码策略选择关键历史节点，采用课程学习逐步过渡训练重点

Result: 在三个公开数据集上超越现有生成模型，验证了历史深度理解对预测准确性的提升

Conclusion: 用户未来行为的准确预测需要建立在对历史路径的全面理解基础上，MHL框架通过双任务设计成功实现了这种理解

Abstract: Generative recommendation, which directly generates item identifiers, has
emerged as a promising paradigm for recommendation systems. However, its
potential is fundamentally constrained by the reliance on purely autoregressive
training. This approach focuses solely on predicting the next item while
ignoring the rich internal structure of a user's interaction history, thus
failing to grasp the underlying intent. To address this limitation, we propose
Masked History Learning (MHL), a novel training framework that shifts the
objective from simple next-step prediction to deep comprehension of history.
MHL augments the standard autoregressive objective with an auxiliary task of
reconstructing masked historical items, compelling the model to understand
``why'' an item path is formed from the user's past behaviors, rather than just
``what'' item comes next. We introduce two key contributions to enhance this
framework: (1) an entropy-guided masking policy that intelligently targets the
most informative historical items for reconstruction, and (2) a curriculum
learning scheduler that progressively transitions from history reconstruction
to future prediction. Experiments on three public datasets show that our method
significantly outperforms state-of-the-art generative models, highlighting that
a comprehensive understanding of the past is crucial for accurately predicting
a user's future path. The code will be released to the public.

</details>


### [211] [Retro*: Optimizing LLMs for Reasoning-Intensive Document Retrieval](https://arxiv.org/abs/2509.24869)
*Junwei Lan,Jianlyu Chen,Zheng Liu,Chaofan Li,Siqi Bao,Defu Lian*

Main category: cs.IR

TL;DR: 提出Retro*方法，通过基于准则的相关性评分机制和强化学习优化，显著提升了推理密集型文档检索的性能


<details>
  <summary>Details</summary>
Motivation: 现有IR技术在细粒度推理、可扩展性和效率方面存在显著局限，需要新的方法来处理间接/隐含相关的文档检索任务

Method: 1. 引入基于准则的相关性评分机制，通过明确定义的标准进行细粒度推理
2. 支持多推理路径的分数整合实现测试时扩展
3. 开发专有的强化学习算法，利用双重复合奖励优化训练

Result: 在BRIGHT基准测试中达到state-of-the-art性能，显著优于现有文档检索方法

Conclusion: Retro*通过可解释的评分机制和有效的训练方法，成功解决了推理密集型检索的挑战，在适用性和效率方面展现出明显优势

Abstract: With the growing popularity of LLM agents and RAG, it has become increasingly
important to retrieve documents that are essential for solving a task, even
when their connection to the task is indirect or implicit. Addressing this
problem requires fine-grained reasoning to accurately assess the relevance
between the task and each candidate document. This capability, however, poses a
significant challenge for existing IR techniques. Despite recent progress in
reasoning-enhanced IR, existing approaches still face significant challenges in
applicability, scalability, and efficiency. In this work, we propose Retro*, a
novel approach for reasoning-intensive document retrieval. Our method
introduces a rubric-based relevance scoring mechanism, enabling the model to
reason about the relationship between a task and a document based on explicitly
defined criteria, whereby producing a fine-grained, interpretable relevance
score. Retro* also supports test-time scaling by combining multiple reasoning
trajectories via score integration, which produces more reliable relevance
estimates. To optimize Retro*'s reasoning capabilities, we introduce a novel
reinforcement learning algorithm tailored for its relevance scoring mechanism,
which employs two composite rewards to fully exploit the trajectories of each
training sample. Our experiments show that Retro* outperforms existing document
retrieval methods with notable advantages, leading to state-of-the-art
performance on the BRIGHT benchmark.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [212] [Virus Infection Attack on LLMs: Your Poisoning Can Spread "VIA" Synthetic Data](https://arxiv.org/abs/2509.23041)
*Zi Liang,Qingqing Ye,Xuan Liu,Yanyun Wang,Jianliang Xu,Haibo Hu*

Main category: cs.CR

TL;DR: 论文揭示了合成数据可能引入新型安全风险，并提出一种可绕过现有防御的病毒式攻击框架VIA，通过污染合成数据传播攻击载荷，使下游模型攻击成功率提升至与上游污染模型相当水平。


<details>
  <summary>Details</summary>
Motivation: 合成数据虽能提升大模型性能，但其可能引入的安全风险尚未被充分研究。现有攻击在合成数据整合训练范式下效果有限，需探索更隐蔽的攻击方式以评估真实风险。

Method: 提出病毒式攻击框架VIA：1) 将投毒载荷封装在保护性'外壳'中规避检测 2) 通过对抗搜索寻找良性样本最优劫持点 3) 利用干净查询生成含恶意内容的合成数据实现攻击传播。

Result: 实验表明VIA使合成数据中毒素内容比例提升3-7倍，下游模型攻击成功率从原本的0-3%提升至与上游被污染模型相当的30-45%。

Conclusion: 合成数据可能成为新型攻击媒介，当前训练范式存在安全盲区，需建立更全面的安全评估体系应对合成数据带来的潜在风险。

Abstract: Synthetic data refers to artificial samples generated by models. While it has
been validated to significantly enhance the performance of large language
models (LLMs) during training and has been widely adopted in LLM development,
potential security risks it may introduce remain uninvestigated. This paper
systematically evaluates the resilience of synthetic-data-integrated training
paradigm for LLMs against mainstream poisoning and backdoor attacks. We reveal
that such a paradigm exhibits strong resistance to existing attacks, primarily
thanks to the different distribution patterns between poisoning data and
queries used to generate synthetic samples. To enhance the effectiveness of
these attacks and further investigate the security risks introduced by
synthetic data, we introduce a novel and universal attack framework, namely,
Virus Infection Attack (VIA), which enables the propagation of current attacks
through synthetic data even under purely clean queries. Inspired by the
principles of virus design in cybersecurity, VIA conceals the poisoning payload
within a protective "shell" and strategically searches for optimal hijacking
points in benign samples to maximize the likelihood of generating malicious
content. Extensive experiments on both data poisoning and backdoor attacks show
that VIA significantly increases the presence of poisoning content in synthetic
data and correspondingly raises the attack success rate (ASR) on downstream
models to levels comparable to those observed in the poisoned upstream models.

</details>


### [213] [MaskSQL: Safeguarding Privacy for LLM-Based Text-to-SQL via Abstraction](https://arxiv.org/abs/2509.23459)
*Sepideh Abedini,Shubhankar Mohapatra,D. B. Emerson,Masoumeh Shafieinejad,Jesse C. Cresswell,Xi He*

Main category: cs.CR

TL;DR: 提出MaskSQL框架，通过抽象化隐私保护机制在文本转SQL任务中实现隐私与性能的有效平衡


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型(LLM)在敏感系统中的隐私合规问题，同时克服小型语言模型(SLM)在复杂任务上的性能不足

Method: 采用抽象化(abstraction)机制替代传统脱敏方法，在提示中保留关键信息的同时模糊敏感细节，通过可调节的隐私-效用权衡机制优化模型表现

Result: MaskSQL在隐私保护前提下，性能超越主流SLM模型，接近最先进的LLM模型水平

Conclusion: 该框架成功建立了隐私保护与任务效用的有效平衡，为敏感领域的自然语言处理应用提供了可行的解决方案

Abstract: Large language models (LLMs) have shown promising performance on tasks that
require reasoning, such as text-to-SQL, code generation, and debugging.
However, regulatory frameworks with strict privacy requirements constrain their
integration into sensitive systems. State-of-the-art LLMs are also proprietary,
costly, and resource-intensive, making local deployment impractical.
Consequently, utilizing such LLMs often requires sharing data with third-party
providers, raising privacy concerns and risking noncompliance with regulations.
Although fine-tuned small language models (SLMs) can outperform LLMs on certain
tasks and be deployed locally to mitigate privacy concerns, they underperform
on more complex tasks such as text-to-SQL translation. In this work, we
introduce MaskSQL, a text-to-SQL framework that utilizes abstraction as a
privacy protection mechanism to mask sensitive information in LLM prompts.
Unlike redaction, which removes content entirely, or generalization, which
broadens tokens, abstraction retains essential information while discarding
unnecessary details, striking an effective privacy-utility balance for the
text-to-SQL task. Moreover, by providing mechanisms to control the
privacy-utility tradeoff, MaskSQL facilitates adoption across a broader range
of use cases. Our experimental results show that MaskSQL outperforms leading
SLM-based text-to-SQL models and achieves performance approaching
state-of-the-art LLM-based models, while preserving privacy.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [214] [CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based Token Eviction](https://arxiv.org/abs/2504.14051)
*Raghavv Goel,Junyoung Park,Mukul Gagrani,Dalton Jones,Matthew Morse,Harper Langston,Mingu Lee,Chris Lott*

Main category: cs.LG

TL;DR: 针对长上下文场景下大语言模型的内存和计算瓶颈，提出基于注意力贡献的token淘汰方法CAOTE，通过整合注意力分数和值向量优化淘汰误差，有效提升下游任务精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力分数的token淘汰方法忽视了token对注意力输出的实际贡献，导致淘汰决策不够精准。值向量中蕴含的语义信息未被充分利用是核心痛点。

Method: CAOTE创新性地在闭式解中引入值向量计算，构建注意力输出误差最小化的淘汰准则。该方法可与其他淘汰策略灵活组合，作为元启发式方法增强现有方案。

Result: 与SOTA注意力分数方法结合时，CAOTE在各类下游任务中实现持续精度提升，最高提升幅度达2.3%。消融实验验证值向量的关键作用。

Conclusion: 注意力输出贡献度是比单纯注意力分数更有效的淘汰指标。将值向量信息融入决策过程能显著提升淘汰质量，该方法具备良好的扩展性和兼容性。

Abstract: While long context support of large language models has extended their
abilities, it also incurs challenges in memory and compute which becomes
crucial bottlenecks in resource-restricted devices. Token eviction, a widely
adopted post-training methodology designed to alleviate the bottlenecks by
evicting less important tokens from the cache, typically uses attention scores
as proxy metrics for token importance. However, one major limitation of
attention score as a token-wise importance metrics is that it lacks the
information about contribution of tokens to the attention output. In this
paper, we propose a simple eviction criterion based on the contribution of
cached tokens to attention outputs. Our method, CAOTE, optimizes for eviction
error due to token eviction, by seamlessly integrating attention scores and
value vectors. This is the first method which uses value tokens on top of
attention-based eviction scores in closed-form. Additionally, CAOTE can act as
a meta-heuristic method with flexible usage with any token eviction method. We
show that CAOTE, when combined with the state-of-the-art attention score-based
methods, always improves accuracies on the downstream task, indicating the
importance of leveraging information from values during token eviction process.

</details>


### [215] [Multiplicative-Additive Constrained Models:Toward Joint Visualization of Interactive and Independent Effects](https://arxiv.org/abs/2509.21923)
*Fumin Wang*

Main category: cs.LG

TL;DR: MACMs模型通过结合乘法与加法结构，在保持可解释性的同时显著提升预测性能，优于CESR和GAMs。


<details>
  <summary>Details</summary>
Motivation: 现有GAMs模型因忽略高阶交互效应导致预测性能受限，而CESR模型虽包含全特征交互但性能不足。需开发同时保持可解释性和高性能的新模型。

Method: MACMs由乘法部分（继承CESR的全特征交互）和加法部分（独立特征效应）组成，通过解耦系数扩展假设空间，并保持形状函数可视化能力。

Result: 基于神经网络的MACMs在预测性能上显著超越CESR和当前最先进的GAMs模型。

Conclusion: MACMs成功平衡可解释性与预测性能，为高风险领域提供了更优的机器学习解决方案，实现了模型性能与可解释性的双重突破。

Abstract: Interpretability is one of the considerations when applying machine learning
to high-stakes fields such as healthcare that involve matters of life safety.
Generalized Additive Models (GAMs) enhance interpretability by visualizing
shape functions. Nevertheless, to preserve interpretability, GAMs omit
higher-order interaction effects (beyond pairwise interactions), which imposes
significant constraints on their predictive performance. We observe that Curve
Ergodic Set Regression (CESR), a multiplicative model, naturally enables the
visualization of its shape functions and simultaneously incorporates both
interactions among all features and individual feature effects. Nevertheless,
CESR fails to demonstrate superior performance compared to GAMs. We introduce
Multiplicative-Additive Constrained Models (MACMs), which augment CESR with an
additive part to disentangle the intertwined coefficients of its interactive
and independent terms, thus effectively broadening the hypothesis space. The
model is composed of a multiplicative part and an additive part, whose shape
functions can both be naturally visualized, thereby assisting users in
interpreting how features participate in the decision-making process.
Consequently, MACMs constitute an improvement over both CESR and GAMs. The
experimental results indicate that neural network-based MACMs significantly
outperform both CESR and the current state-of-the-art GAMs in terms of
predictive performance.

</details>


### [216] [Adaptive Margin RLHF via Preference over Preferences](https://arxiv.org/abs/2509.22851)
*Yaswanth Chittepu,Prasann Singhal,Greg Durrett,Scott Niekum*

Main category: cs.LG

TL;DR: 提出DPO-PoP方法，通过偏好强度建模实现自适应边际优化，在奖励模型学习中同时提升判别与生成性能


<details>
  <summary>Details</summary>
Motivation: 现有RLHF奖励模型的固定边际方法无法捕捉偏好强度差异，且依赖人工提供的精确偏好评分存在可靠性问题

Method: 利用偏好间比较信号（preference-over-preferences）推断样本级自适应边际，扩展DPO算法实现端到端优化

Result: 在UltraFeedback数据集上超越原始DPO及固定边际方法，发现判别性能与生成性能存在权衡关系

Conclusion: 自适应边际能有效提升对齐效果，提出的两种偏好采样策略可针对性优化不同场景需求

Abstract: Margin-based optimization is fundamental to improving generalization and
robustness in classification tasks. In the context of reward model learning
from preferences within Reinforcement Learning from Human Feedback (RLHF),
existing methods typically rely on no margins, fixed margins, or margins that
are simplistic functions of preference ratings. However, such formulations
often fail to account for the varying strengths of different preferences, for
example some preferences are associated with larger margins between responses,
or they rely on noisy margin information derived from ratings. We argue that
modeling the strength of preferences can lead to better generalization and more
faithful alignment. Furthermore, many existing methods that use adaptive
margins assume access to accurate preference scores, which can be difficult for
humans to provide reliably. We propose an approach that leverages preferences
over preferences, that is annotations indicating which of two preferences
reflects a stronger distinction. We use this ordinal signal to infer adaptive
margins on a per-datapoint basis. We introduce an extension to Direct
Preference Optimization (DPO), DPO-PoP, that incorporates adaptive margins from
preference-over-preference supervision, enabling improved discriminative and
generative performance. Empirically, our method outperforms vanilla DPO, DPO
with fixed margins, and DPO with ground-truth margins on the UltraFeedback
dataset. Additionally, we show that there is a tradeoff between discriminative
and generative performance: improving test classification accuracy,
particularly by correctly labeling weaker preferences at the expense of
stronger ones, can lead to a decline in generative quality. To navigate this
tradeoff, we propose two sampling strategies to gather
preference-over-preference labels: one favoring discriminative performance and
one favoring generative performance.

</details>


### [217] [Tracing the Representation Geometry of Language Models from Pretraining to Post-training](https://arxiv.org/abs/2509.23024)
*Melody Zixuan Li,Kumar Krishna Agrawal,Arna Ghosh,Komal Kumar Teru,Adam Santoro,Guillaume Lajoie,Blake A. Richards*

Main category: cs.LG

TL;DR: 大模型训练中表示几何经历预热→熵寻求→压缩寻求三阶段，后训练方法（SFT/DPO/RLVR）通过不同几何动态影响模型性能


<details>
  <summary>Details</summary>
Motivation: 揭示标准训练指标无法解释大语言模型复杂能力的涌现机制，探究预训练/后训练过程中表示空间的几何演化规律

Method: 使用OLMo(1B-7B)和Pythia(160M-12B)模型，通过有效秩(RankMe)和特征谱衰减(α-ReQ)指标分析表示空间几何结构

Result: 发现自回归预训练中三阶段演化：1）预热阶段表示坍塌→2）熵寻求阶段维度扩展（伴随n-gram记忆）→3）压缩阶段各向异性整合（下游任务显著提升）

Conclusion: 几何演化源于交叉熵优化与表征瓶颈的相互作用，后训练方法通过不同几何动态（SFT/DPO驱动熵寻求，RLVR驱动压缩）影响模型特性

Abstract: Standard training metrics like loss fail to explain the emergence of complex
capabilities in large language models. We take a spectral approach to
investigate the geometry of learned representations across pretraining and
post-training, measuring effective rank (RankMe) and eigenspectrum decay
($\alpha$-ReQ). With OLMo (1B-7B) and Pythia (160M-12B) models, we uncover a
consistent non-monotonic sequence of three geometric phases during
autoregressive pretraining. The initial "warmup" phase exhibits rapid
representational collapse. This is followed by an "entropy-seeking" phase,
where the manifold's dimensionality expands substantially, coinciding with peak
n-gram memorization. Subsequently, a "compression-seeking" phase imposes
anisotropic consolidation, selectively preserving variance along dominant
eigendirections while contracting others, a transition marked with significant
improvement in downstream task performance. We show these phases can emerge
from a fundamental interplay of cross-entropy optimization under skewed token
frequencies and representational bottlenecks ($d \ll |V|$). Post-training
further transforms geometry: SFT and DPO drive "entropy-seeking" dynamics to
integrate specific instructional or preferential data, improving
in-distribution performance while degrading out-of-distribution robustness.
Conversely, RLVR induces "compression-seeking", enhancing reward alignment but
reducing generation diversity.

</details>


### [218] [Causally-Enhanced Reinforcement Policy Optimization](https://arxiv.org/abs/2509.23095)
*Xiangqi Wang,Yue Huang,Yujun Zhou,Xiaonan Luo,Kehan Guo,Xiangliang Zhang*

Main category: cs.LG

TL;DR: 提出CE-PO框架增强LLMs的因果一致性，通过Jacobian敏感度分析和Minkowski奖励组合减少奖励黑客现象，在保持准确性的同时提升推理鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的LLMs易产生表面正确但推理不忠实的答案，对微小因果扰动敏感，需解决奖励黑客和虚假推理链问题

Method: 使用Jacobian-based敏感度分析估计模型内部影响，进行反事实硬化抑制干扰信号，通过Minkowski组合器融合因果一致性评分与任务准确率反馈

Result: 在4个数据集上平均准确率提升5.49%(最高9.58%)，显著增强对相关-因果翻转和轻量反事实编辑的抵抗力，减少50%不忠实思维链生成

Conclusion: CE-PO无需架构修改即可集成现有优化器，通过可调的精度-一致性平衡机制，在保持任务性能的同时显著提升模型推理的因果可信度

Abstract: Large language models (LLMs) trained with reinforcement objectives often
achieve superficially correct answers via shortcut strategies, pairing correct
outputs with spurious or unfaithful reasoning and degrading under small causal
perturbations. We introduce Causally-Enhanced Policy Optimization (CE-PO), a
drop-in reward-shaping framework that augments policy optimization with a
differentiable proxy for causal coherence along the generation pathway from
prompt (Z) to rationale (X) to answer (Y). CE-PO estimates model-internal
influence with Jacobian-based sensitivities, counterfactually hardens these
signals to suppress nuisance cues, and fuses the resulting coherence score with
task-accuracy feedback via a Minkowski (power-mean) combiner, exposing a single
tunable between accuracy and coherence trade-off. The unified reward integrates
with PPO/GRPO without architectural changes. Across reasoning benchmarks and
causal stress tests, CE-PO reduces reward hacking and unfaithful
chain-of-thought while improving robustness to correlation-causation flips and
light counterfactual edits, all at near-parity accuracy. Experimental results
across 4 datasets show that CE-PO improves accuracy over baselines by 5.49% on
average (up to 9.58%), while improving robustness to correlation-causation
flips and light counterfactual edits.

</details>


### [219] [RHYTHM: Reasoning with Hierarchical Temporal Tokenization for Human Mobility](https://arxiv.org/abs/2509.23115)
*Haoyu He,Haozheng Luo,Yan Chen,Qi R. Wang*

Main category: cs.LG

TL;DR: 提出了RHYTHM框架，通过分层时间标记化和大语言模型融合，显著提升人类移动预测的准确性与训练效率


<details>
  <summary>Details</summary>
Motivation: 传统方法难以有效捕捉人类移动中的长程依赖和多尺度周期性特征，且计算效率低下

Method: 采用时间分桶策略压缩序列长度，结合分层注意力机制保留周期特征；创新性地将预训练提示嵌入注入冻结的LLM架构

Result: 在三个真实数据集上实现：总体准确率提升2.4%，周末预测准确率提升5%，训练时间缩短24.6%

Conclusion: RHYTHM框架成功平衡了预测性能与计算效率，为时空预测任务提供了新的LLM应用范式

Abstract: Predicting human mobility is inherently challenging due to complex long-range
dependencies and multi-scale periodic behaviors. To address this, we introduce
RHYTHM (Reasoning with Hierarchical Temporal Tokenization for Human Mobility),
a unified framework that leverages large language models (LLMs) as
general-purpose spatio-temporal predictors and trajectory reasoners.
Methodologically, RHYTHM employs temporal tokenization to partition each
trajectory into daily segments and encode them as discrete tokens with
hierarchical attention that captures both daily and weekly dependencies,
thereby significantly reducing the sequence length while preserving cyclical
information. Additionally, we enrich token representations by adding
pre-computed prompt embeddings for trajectory segments and prediction targets
via a frozen LLM, and feeding these combined embeddings back into the LLM
backbone to capture complex interdependencies. Computationally, RHYTHM freezes
the pretrained LLM's backbone to reduce attention complexity and memory cost.
We evaluate our model against state-of-the-art methods using three real-world
datasets. Notably, RHYTHM achieves a 2.4% improvement in overall accuracy, a
5.0% increase on weekends, and a 24.6% reduction in training time. Code is
publicly available at https://github.com/he-h/rhythm.

</details>


### [220] [C$^2$GSPG: Confidence-calibrated Group Sequence Policy Gradient towards Self-aware Reasoning](https://arxiv.org/abs/2509.23129)
*Haotian Liu,Shuo Wang,Hongteng Xu*

Main category: cs.LG

TL;DR: 提出C²GSPG方法，通过GSPG框架消除token级偏差并引入置信度校准策略，在提升推理模型性能的同时抑制过度自信问题


<details>
  <summary>Details</summary>
Motivation: 现有GRPO等强化学习方法存在token级偏差导致的过度自信问题，阻碍模型形成自我认知的推理能力

Method: 1. 设计组序列策略梯度框架(GSPG)消除token级偏差
2. 使用序列级概率定义置信度
3. 引入交叉熵正则化校准置信度与奖励
4. 对非二元奖励采用非线性归一化和自适应正则截断

Result: 在逻辑/数学推理任务中，C²GSPG的推理准确率和置信度校准效果均超越SOTA方法，代码已开源

Conclusion: 置信度校准与GSPG框架形成协同效应，交叉熵正则化策略在二元/非二元奖励场景均有效，为解决RL推理模型的自我认知问题提供了新方案

Abstract: Reinforcement Learning (RL) methods, exemplified by Group Relative Policy
Optimization (GRPO) and its variants, play a central role in developing
reasoning models. However, these methods often suffer from a critical
overconfidence issue, which prevents them from achieving self-aware reasoning
models. In this study, we propose a simple yet effective confidence-calibration
group sequence policy gradient method, called C$^2$GSPG, which simultaneously
enhances reasoning performance while suppressing overconfidence. In principle,
we propose a Group Sequence Policy Gradient (GSPG) framework for learning
reasoning models, which eliminates the token-level bias commonly appearing in
GRPO and its variants. In this framework, we define the model confidence for
each reasoning problem using the normalized sequence-level probability, and
then apply a cross-entropy regularizer to calibrate the model confidence to the
sequence's reward. We demonstrate that the confidence calibration regularizer
and GSPG are collaborative for binary rewards, as their objectives always share
the same gradient direction. For non-binary rewards, we apply nonlinear reward
normalization and adaptive regularizer clipping, mitigating the potential
conflict between the two objectives. Applying C$^2$GSPG to post-train large
language models in logical and mathematical reasoning tasks, we show its
superiority over state-of-the-art methods in both reasoning accuracy and
confidence calibration. The code of C$^2$GSPG is available at
https://github.com/HaotianLiu123/CCGSPG.

</details>


### [221] [SPEC-RL: Accelerating On-Policy Reinforcement Learning via Speculative Rollouts](https://arxiv.org/abs/2509.23232)
*Bingshuai Liu,Ante Wang,Zijun Min,Liang Yao,Haibo Zhang,Yang Liu,Anxiang Zeng,Jinsong Su*

Main category: cs.LG

TL;DR: 提出SPEC-RL框架，通过推测解码技术将强化学习rollout时间减少2-3倍，提升大模型推理训练效率


<details>
  <summary>Details</summary>
Motivation: 现有强化学习训练中rollout阶段存在重复生成轨迹片段的问题，导致计算资源浪费。传统加速方法（如并行化/回放缓冲）存在效果递减、引入偏差或忽略迭代冗余等缺陷

Method: 将推测解码技术融入RL流程：1) 复用历史轨迹作为推测前缀 2) 采用草稿生成-验证机制扩展轨迹 3) 保持策略一致性避免偏差

Result: 在GSM8K/MATH-500等数学推理基准测试中，保持策略质量前提下实现2-3倍加速；兼容PPO/GRPO/DAPO等主流算法

Conclusion: SPEC-RL为大规模推理模型的强化学习训练提供了通用且高效的技术路径，通过纯rollout阶段优化实现计算效率突破

Abstract: Large Language Models (LLMs) increasingly rely on reinforcement learning with
verifiable rewards (RLVR) to elicit reliable chain-of-thought reasoning.
However, the training process remains bottlenecked by the computationally
expensive rollout stage. Existing acceleration methods-such as parallelization,
objective- and data-driven modifications, and replay buffers-either incur
diminishing returns, introduce bias, or overlook redundancy across iterations.
We identify that rollouts from consecutive training epochs frequently share a
large portion of overlapping segments, wasting computation. To address this, we
propose SPEC-RL, a novel framework that integrates SPECulative decoding with
the RL rollout process. SPEC-RL reuses prior trajectory segments as speculative
prefixes and extends them via a draft-and-verify mechanism, avoiding redundant
generation while ensuring policy consistency. Experiments on diverse math
reasoning and generalization benchmarks, including GSM8K, MATH-500,
OlympiadBench, MMLU-STEM, and others, demonstrate that SPEC-RL reduces rollout
time by 2-3x without compromising policy quality. As a purely rollout-stage
enhancement, SPEC-RL integrates seamlessly with mainstream algorithms (e.g.,
PPO, GRPO, DAPO), offering a general and practical path to scale RLVR for large
reasoning models. Our code is available at https://github.com/ShopeeLLM/Spec-RL

</details>


### [222] [Temporal Generalization: A Reality Check](https://arxiv.org/abs/2509.23487)
*Divyam Madaan,Sumit Chopra,Kyunghyun Cho*

Main category: cs.LG

TL;DR: 当前机器学习模型在数据分布变化时泛化能力有限，实验证明参数插值与外推方法均无法稳定超越'直接使用最新模型参数'的基线方案。


<details>
  <summary>Details</summary>
Motivation: 探究在没有未来数据的情况下，机器学习模型能否通过参数插值（参数空间凸组合）或参数外推（超越参数凸包）实现对未来数据的泛化。

Method: 在语言建模、新闻摘要、卫星图像分类等6个时序任务上，系统评估了参数插值与外推两类方法（包括模型平均、动量更新等具体技术）的表现。

Result: 所有评估方法在跨任务场景中均无法稳定超越'使用最新模型参数'的基线，尤其在数据生成过程不明确时泛化效果显著下降。

Conclusion: 研究强调在缺乏未来数据或强假设条件下，声称模型具备未来数据泛化能力的结论需谨慎，当前技术尚未突破时序泛化的根本性挑战。

Abstract: Machine learning (ML) models often struggle to maintain performance under
distribution shifts, leading to inaccurate predictions on unseen future data.
In this work, we investigate whether and under what conditions models can
achieve such a generalization when relying solely on past data. We explore two
primary approaches: convex combinations of past model parameters
(\emph{parameter interpolation}) and explicit extrapolation beyond the convex
hull of past parameters (\emph{parameter extrapolation}). We benchmark several
methods within these categories on a diverse set of temporal tasks, including
language modeling, news summarization, news tag prediction, academic paper
categorization, satellite image-based land use classification over time, and
historical yearbook photo gender prediction. Our empirical findings show that
none of the evaluated methods consistently outperforms the simple baseline of
using the latest available model parameters in all scenarios. In the absence of
access to future data or robust assumptions about the underlying
data-generating process, these results underscore the inherent difficulties of
generalizing and extrapolating to future data and warrant caution when
evaluating claims of such generalization.

</details>


### [223] [Towards a Comprehensive Scaling Law of Mixture-of-Experts](https://arxiv.org/abs/2509.23678)
*Guoliang Zhao,Yuhan Fu,Shuaipeng Li,Xingwu Sun,Ruobing Xie,An Wang,Weidong Han,Zhen Yang,Weixuan Sun,Yudong Zhang,Cheng-zhong Xu,Di Wang,Jie Jiang*

Main category: cs.LG

TL;DR: 提出了首个针对MoE模型的全面缩放定律，通过分解五个关键参数并设计446组实验，发现G/S参数的最优设置独立于模型架构和数据量，且激活参数比例随模型增大趋于稀疏。


<details>
  <summary>Details</summary>
Motivation: 现有稠密模型的缩放定律无法适用于MoE模型，因为存在影响因素多、耦合关系复杂、性能影响非单调三大挑战，需要专门研究MoE特有的缩放规律。

Method: 从模型规模和结构角度分解出数据量(D)、总参数量(N)、激活参数量(N_a)、激活专家数(G)、共享专家比例(S)五个因素，通过446组对照实验建立联合缩放定律。

Result: 1. G/S的最优设置与模型架构和数据无关；2. 随着N增大，最优N_a/N比例趋于稀疏；3. 通过理论推导获得最优配置的闭式解。

Conclusion: 提出的MoE缩放定律可为未来MoE模型设计提供精准指导，特别是在专家激活策略和参数分配优化方面具有重要工程实践价值。

Abstract: Mixture-of-Experts (MoE) models have become the consensus approach for
enabling parameter-efficient scaling and cost-effective deployment in large
language models. However, existing scaling laws for dense models are
inapplicable to MoE models, which stems from three critical challenges: the
multiplicity of influencing factors, their intricate coupling relationships and
the non-monotonic nature of their performance impacts. They collectively
necessitate a fine-grained investigation into MoE-specific scaling laws. In
this work, we perform a systematic decomposition of MoE settings, identifying
five key factors that influence model performance from both size and structural
perspectives (data size ($D$), total model size ($N$), activated model size
($N_a$), number of active experts ($G$) and the ratio of shared experts ($S$)).
Specifically, we design $446$ controlled experiments to characterize their
marginal effects, ultimately constructing a comprehensive and precise joint MoE
scaling law that considers all essential factors. Furthermore, we derive the
theoretically optimal and practically efficiency-aware optimal configurations
for $G$, $S$ and $N_a/N$ with detailed analyses. Our results demonstrate that
the optimal settings for $G$ and $S$ are independent of both the model
architecture and data size. With the scaling of $N$, the optimal activation
parameter ratio of $N_a/N$ becomes sparser. Our proposed MoE scaling law could
function as an accurate and insightful guidance to facilitate future MoE model
design and training.

</details>


### [224] [Anchored Supervised Fine-Tuning](https://arxiv.org/abs/2509.23753)
*He Zhu,Junyou Su,Peng Lai,Ren Ma,Wenjia Zhang,Linyi Yang,Guanhua Chen*

Main category: cs.LG

TL;DR: 提出锚定监督微调（ASFT）方法，通过KL正则化解决动态微调（DFT）的稳定性问题，在多领域任务中显著优于SFT和DFT。


<details>
  <summary>Details</summary>
Motivation: 解决监督微调（SFT）易记忆化与强化学习（RL）计算成本高的矛盾，分析DFT不稳定性根源并寻求改进方案。

Method: 通过奖励加权回归（RWR）框架分析DFT，发现其分布锚定缺失导致训练漂移，提出加入轻量级KL正则化的ASFT方法。

Result: ASFT在数学推理、医学知识嵌入和代码生成任务中均优于基准方法，计算开销仅微量增加。

Conclusion: 基于RWR框架的系统分析将理论保障与实践效果结合，证明理论指导可有效提升后训练方法性能。

Abstract: Post-training of large language models involves a fundamental trade-off
between supervised fine-tuning (SFT), which efficiently mimics demonstrations
but tends to memorize, and reinforcement learning (RL), which achieves better
generalization at higher computational cost. Dynamic Fine-Tuning (DFT) recently
emerged as a promising middle ground, reweighting SFT objectives with token
probabilities and achieving improvements in certain reasoning domains, though
it exhibits instability in other tasks. We provide a analysis of DFT through
the reward-weighted regression (RWR) framework, revealing that it corresponds
to a specific auxiliary distribution choice that yields provably tighter RL
bounds than standard SFT. However, our analysis also uncovers a critical
limitation: this construction lacks distributional anchoring, leading to
progressive drift that undermines training stability. To address this, we
propose Anchored Supervised Fine-Tuning (ASFT), which augments DFT's
reweighting with lightweight KL regularization to preserve tightness while
ensuring stability. Empirically, ASFT consistently outperforms both SFT and DFT
across mathematical reasoning, medical knowledge grounding, and code
generation, achieving substantial improvements with minimal computational
overhead. Our RWR framework provides a systematic lens for understanding
post-training methods and demonstrates that principled theoretical analysis
leads to both stronger guarantees and practical gains.

</details>


### [225] [Knowledge Homophily in Large Language Models](https://arxiv.org/abs/2509.23773)
*Utkarsh Sahu,Zhisheng Qi,Mahantesh Halappanavar,Nedim Lipka,Ryan A. Rossi,Franck Dernoncourt,Yu Zhang,Yao Ma,Yu Wang*

Main category: cs.LG

TL;DR: 研究通过图神经网络量化LLM知识同质性，提升标注效率和知识覆盖


<details>
  <summary>Details</summary>
Motivation: 探索LLM内部知识结构化特征，受认知神经科学中语义聚类与启动效应启发，发现实体间知识存在同质性分布规律

Method: 1. 将LLM知识映射为图结构（三元组级/实体级） 2. 构建GNN回归模型预测实体知识性评分 3. 基于评分优化标注策略

Result: 该方法在主动标注预算下提升23%知识覆盖率，多跳问答路径检索准确率提高15%

Conclusion: 知识同质性建模有效优化LLM知识注入与检索效率，为知识密集型应用提供新范式

Abstract: Large Language Models (LLMs) have been increasingly studied as neural
knowledge bases for supporting knowledge-intensive applications such as
question answering and fact checking. However, the structural organization of
their knowledge remains unexplored. Inspired by cognitive neuroscience
findings, such as semantic clustering and priming, where knowing one fact
increases the likelihood of recalling related facts, we investigate an
analogous knowledge homophily pattern in LLMs. To this end, we map LLM
knowledge into a graph representation through knowledge checking at both the
triplet and entity levels. After that, we analyze the knowledgeability
relationship between an entity and its neighbors, discovering that LLMs tend to
possess a similar level of knowledge about entities positioned closer in the
graph. Motivated by this homophily principle, we propose a Graph Neural Network
(GNN) regression model to estimate entity-level knowledgeability scores for
triplets by leveraging their neighborhood scores. The predicted
knowledgeability enables us to prioritize checking less well-known triplets,
thereby maximizing knowledge coverage under the same labeling budget. This not
only improves the efficiency of active labeling for fine-tuning to inject
knowledge into LLMs but also enhances multi-hop path retrieval in
reasoning-intensive question answering.

</details>


### [226] [Beyond the Exploration-Exploitation Trade-off: A Hidden State Approach for LLM Reasoning in RLVR](https://arxiv.org/abs/2509.23808)
*Fanding Huang,Guanbo Huang,Xiao Fan,Yi He,Xiao Liang,Xiao Chen,Qinting Jiang,Faisal Nadeem Khan,Jingyan Jiang,Zhi Wang*

Main category: cs.LG

TL;DR: 提出VERL方法，通过在隐藏状态层面解耦探索与利用机制并协同增强两者，显著提升LLM推理任务性能


<details>
  <summary>Details</summary>
Motivation: 传统RLVR框架将探索与利用视为零和博弈，但作者发现这种权衡可能是token级测量偏差导致的假象，需在更本质的隐藏状态层面重新评估

Method: 引入有效等级(ER)及其一阶(ERV)、二阶(ERA)导数量化隐藏状态动态，设计VERL框架通过双重激励通道前瞻性调整优势函数，实现探索-利用协同增强

Result: 在多样化LLM和推理基准测试中取得显著提升，特别是在Gaokao 2024数据集实现21.4%绝对准确率提升

Conclusion: 探索与利用在隐藏状态层面具有解耦潜力，VERL通过理论稳定的ERA元控制器构建协同增强机制，突破传统权衡范式

Abstract: A prevailing view in Reinforcement Learning for Verifiable Rewards (RLVR)
interprets recent progress through the lens of an exploration-exploitation
trade-off, a perspective largely shaped by token-level metrics. We re-examine
this perspective, proposing that this perceived trade-off may not be a
fundamental constraint but rather an artifact of the measurement level. To
investigate this, we shift the analysis to the semantically rich hidden-state
space, adopting Effective Rank (ER) to quantify exploration and proposing its
novel first- and second-order derivatives, named Effective Rank Velocity (ERV)
and Effective Rank Acceleration (ERA), to capture exploitation dynamics. Our
analysis reveals that at the hidden-state level, exploration and exploitation
could be decoupled (Sec. 4). This finding reveals an opportunity to enhance
both capacities simultaneously. This insight motivates our method,
Velocity-Exploiting Rank-Learning (VERL), the first to operationalize the
principle of synergistic exploration-exploitation enhancement by directly
shaping the RL advantage function. The key innovation is leveraging the
theoretically stable ERA as a predictive meta-controller to create a
synergistic, dual-channel incentive structure. Instead of forcing a trade-off,
VERL prospectively amplifies rewards for exploration to preempt overconfidence
and reinforces exploitative gains to consolidate reasoning. Experiments across
diverse LLMs and reasoning benchmarks show consistent gains, including up to
21.4% absolute accuracy improvement on the challenging Gaokao 2024 dataset.

</details>


### [227] [Dynamic Orthogonal Continual Fine-tuning for Mitigating Catastrophic Forgettings](https://arxiv.org/abs/2509.23893)
*Zhixin Zhang,Zeming Wei,Meng Sun*

Main category: cs.LG

TL;DR: 提出DOC微调方法，通过追踪功能方向漂移并正交调整梯度，有效减少大语言模型持续学习中的灾难性遗忘


<details>
  <summary>Details</summary>
Motivation: 现有基于正则化的方法在长期持续学习中失效，根源在于微调过程中模型功能方向的漂移未被有效跟踪

Method: 动态追踪历史功能方向，通过正交约束新任务梯度与历史功能方向的关系来保持知识连续性

Result: 在多个LLM持续学习基准测试中表现优于现有方法，验证了方法的有效性

Conclusion: DOC为解决持续微调中的功能方向漂移问题提供了创新解决方案，具有实际应用价值

Abstract: Catastrophic forgetting remains a critical challenge in continual learning
for large language models (LLMs), where models struggle to retain performance
on historical tasks when fine-tuning on new sequential data without access to
past datasets. In this paper, we first reveal that the drift of functional
directions during the fine-tuning process is a key reason why existing
regularization-based methods fail in long-term LLM continual learning. To
address this, we propose Dynamic Orthogonal Continual (DOC) fine-tuning, a
novel approach that tracks the drift of these functional directions and
dynamically updates them during the fine-tuning process. Furthermore, by
adjusting the gradients of new task parameters to be orthogonal to the tracked
historical function directions, our method mitigates interference between new
and old tasks. Extensive experiments on various LLM continual learning
benchmarks demonstrate that this approach outperforms prior methods,
effectively reducing catastrophic forgetting and providing a robust tool for
continuous LLM fine-tuning. Our code is available at
https://github.com/meloxxxxxx/DOC.

</details>


### [228] [Beyond Benchmarks: Understanding Mixture-of-Experts Models through Internal Mechanisms](https://arxiv.org/abs/2509.23933)
*Jiahao Ying,Mingbao Lin,Qianru Sun,Yixin Cao*

Main category: cs.LG

TL;DR: 通过MUI指标系统分析MoE模型，揭示神经元利用率演变、训练动态、专家协作模式与神经元激活规律，为理解MoE机制提供新视角


<details>
  <summary>Details</summary>
Motivation: 现有MoE研究过度关注性能指标，缺乏对其内部机制的系统性理解，制约了该架构的进一步发展与优化

Method: 使用MUI内部指标分析公开MoE模型，结合路由机制解析与专家级行为追踪，涵盖模型演化、训练动态、专家协作和神经元激活四个维度

Result: 1) 模型进化伴随神经元利用率下降 2) 训练呈现benchmark无法捕捉的动态轨迹 3) 任务完成依赖专家协作 4) 神经元激活模式反映数据多样性

Conclusion: MUI指标可作为benchmark的有效补充，为理解MoE模型的容量、动态演化及专业化特征提供新的方法论框架

Abstract: Mixture-of-Experts (MoE) architectures have emerged as a promising direction,
offering efficiency and scalability by activating only a subset of parameters
during inference. However, current research remains largely
performance-centric, with limited understanding of its internal mechanisms,
thereby constraining broader progress. In this work, we use an internal metric
to investigate the mechanisms of MoE architecture by explicitly incorporating
routing mechanisms and analyzing expert-level behaviors. Through systematic
analyses of a wide range of publicly available MoE models, we uncover several
findings: (1) neuron utilization decreases as models evolve, reflecting
stronger generalization; (2) training exhibits a dynamic trajectory, where
benchmark performance alone provides limited signal while MUI reveals deeper
insights; (3) task completion emerges from collaborative contributions of
multiple experts, with shared experts driving concentration; and (4) activation
patterns at the neuron level provide a fine-grained proxy for data diversity.
Together, these results demonstrate the potential of MUI as a complementary
indicator to benchmark performance, offering new insights into the capacity,
dynamics, and specialization of MoE models. Our project can be found at
https://yingjiahao14.github.io/MoE-MUI/.

</details>


### [229] [Explore-Execute Chain: Towards an Efficient Structured Reasoning Paradigm](https://arxiv.org/abs/2509.23946)
*Kaisen Yang,Lixuan He,Rushi Shah,Kaicheng Yang,Qinwei Ma,Dianbo Liu,Alex Lamb*

Main category: cs.LG

TL;DR: 提出Explore-Execute Chain（E²C）框架，通过分离探索与执行阶段提升LLM推理效率，在计算开销减少90%情况下达到58.1%准确率，跨领域适应中准确率提升14.5%


<details>
  <summary>Details</summary>
Motivation: 传统CoT方法将高层战略规划与底层执行耦合，导致计算效率低下、推理路径探索受限及可解释性差。需解耦规划与执行以突破这些限制

Method: 两阶段框架：1) 探索阶段随机生成高层计划 2) 执行阶段确定性实施。采用SFT（含强制计划遵循的数据生成算法）+ RL的两阶段训练方法

Result: AIME'2024准确率58.1%（解码token减少90%）；医学基准跨领域适应准确率提升14.5%（仅用3.5% SFT token），实现SOTA性能与强泛化

Conclusion: E²C通过解耦规划与执行，显著提升推理效率与跨领域适应性，为复杂任务处理提供高效框架，同时增强模型可解释性

Abstract: Chain-of-Thought (CoT) and its variants have markedly advanced the reasoning
abilities of Large Language Models (LLMs), yet their monolithic and
auto-regressive architecture inherently conflates high-level strategic planning
with low-level step-by-step execution, leading to computational inefficiency,
limited exploration of reasoning paths, and reduced interpretability. To
overcome these issues, we propose the Explore-Execute Chain ($E^2C$), a
structured reasoning framework that decouples reasoning into two distinct
phases: an exploratory phase that stochastically generates succinct high-level
plans, followed by an execution phase that deterministically carries out the
chosen plan. Our approach incorporates a two-stage training methodology, which
combines Supervised Fine-Tuning (SFT) - augmented by a novel data generation
algorithm enforcing strict plan adherence - with a subsequent Reinforcement
Learning (RL) stage that capitalizes on the informativeness of exploration and
reinforces the determinism of execution.This decomposition enables an efficient
test-time scaling strategy: on AIME'2024, $E^2C$ Test Time Scaling reaches
58.1% accuracy using <10% of the decoding tokens required by comparable methods
(e.g., Forest-of-Thought), sharply cutting self-consistency overhead. For
cross-domain adaptation, our Exploration-Focused SFT (EF-SFT) fine-tunes with
only 3.5% of the tokens used by standard SFT yet yields up to 14.5% higher
accuracy than standard SFT on medical benchmarks, delivering state-of-the-art
performance, strong generalization, and greater interpretability by separating
planning from execution. The code and pre-trained models for the project are
available at: https://github.com/yks23/Explore-Execute-Chain.git

</details>


### [230] [Detecting and Rectifying Noisy Labels: A Similarity-based Approach](https://arxiv.org/abs/2509.23964)
*Dang Huu-Tien,Naoya Inoue*

Main category: cs.LG

TL;DR: 提出基于神经网络倒数第二层特征的后置模型无关方法，通过聚类相似性检测并自动纠正标签噪声


<details>
  <summary>Details</summary>
Motivation: 现代深度网络规模扩大需要自动化工具检测标签噪声，现有方法存在效率与通用性不足的问题

Method: 利用错误标签数据点倒数第二层特征与其真实类别的相似性，在紧密聚类中通过概率分析检测与纠正错误

Result: 实验证明该方法在多种噪声类型下表现优异，且能自动提升数据集质量

Conclusion: 该方法实现了高效通用的标签错误检测与纠正，显著改善数据集可靠性

Abstract: Label noise in datasets could damage the performance of neural net training.
As the size of modern deep networks grows, there is a growing demand for
automated tools for detecting such errors. In this paper, we propose post-hoc,
model-agnostic error detection and rectification methods utilizing the
penultimate feature from a neural network. Our idea is based on the observation
that the similarity between the penultimate feature of a mislabeled data point
and its true class data points is higher than that for data points from other
classes, making the probability of label occurrence within a tight, similar
cluster informative for detecting and rectifying errors. Extensive experiments
show our method not only demonstrates high performance across various noises
but also automatically rectifies these errors to improve the quality of
datasets.

</details>


### [231] [Group-Relative REINFORCE Is Secretly an Off-Policy Algorithm: Demystifying Some Myths About GRPO and Its Friends](https://arxiv.org/abs/2509.24203)
*Chaorui Yao,Yanxi Chen,Yuchang Sun,Yushuo Chen,Wenhao Zhang,Xuchen Pan,Yaliang Li,Bolin Ding*

Main category: cs.LG

TL;DR: 提出基于第一性原理推导的group-relative REINFORCE框架，为LLM的离线策略强化学习提供理论解释和算法设计新方向


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型实际应用中离线策略学习的迫切需求，突破传统在线策略算法的限制，应对LLM-RL基础设施复杂性和方法论创新的双重挑战

Method: 通过数学推导揭示REINFORCE的离线策略本质，提出策略更新正则化和数据分布主动调整两大原则，统一解释OPMD/AsymRE算法并验证数据加权策略

Result: 理论分析得到实证支持，提供可操作的算法设计洞见，相关代码已在GitHub开源

Conclusion: 研究突破了传统认知框架，为LLM离线策略强化学习奠定了理论基础，开辟了基于理论指导的新型算法设计路径

Abstract: Off-policy reinforcement learning (RL) for large language models (LLMs) is
attracting growing interest, driven by practical constraints in real-world
applications, the complexity of LLM-RL infrastructure, and the need for further
innovations of RL methodologies. While classic REINFORCE and its modern
variants like Group Relative Policy Optimization (GRPO) are typically regarded
as on-policy algorithms with limited tolerance of off-policyness, we present in
this work a first-principles derivation for group-relative REINFORCE without
assuming a specific training data distribution, showing that it admits a native
off-policy interpretation. This perspective yields two general principles for
adapting REINFORCE to off-policy settings: regularizing policy updates, and
actively shaping the data distribution. Our analysis demystifies some myths
about the roles of importance sampling and clipping in GRPO, unifies and
reinterprets two recent algorithms -- Online Policy Mirror Descent (OPMD) and
Asymmetric REINFORCE (AsymRE) -- as regularized forms of the REINFORCE loss,
and offers theoretical justification for seemingly heuristic data-weighting
strategies. Our findings lead to actionable insights that are validated with
extensive empirical studies, and open up new opportunities for principled
algorithm design in off-policy RL for LLMs. Source code for this work is
available at
https://github.com/modelscope/Trinity-RFT/tree/main/examples/rec_gsm8k.

</details>


### [232] [LEAF: A Robust Expert-Based Framework for Few-Shot Continual Event Detection](https://arxiv.org/abs/2509.24547)
*Bao-Ngoc Dao,Quang Nguyen,Luyen Ngo Dinh,Minh Le,Linh Ngo Van*

Main category: cs.LG

TL;DR: 提出LEAF框架解决小样本持续事件检测中的知识遗忘和有限数据泛化问题，通过专家混合架构和语义引导对比学习实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有方法因全参数微调导致任务间知识干扰，且数据增强策略易引入语义失真。需要更鲁棒的参数隔离方案和有效的有限数据泛化机制

Method: 1. 基于LoRA的低秩专家混合架构实现参数隔离 2. 语义感知专家选择机制动态路由样本 3. 标签描述引导的对比学习增强泛化 4. 基于知识蒸馏的旧模型知识迁移策略

Result: 在多个FCED基准测试中持续达到最先进性能，显著减少任务间知识干扰

Conclusion: LEAF通过参数隔离架构和语义引导学习机制，有效平衡了持续学习中的稳定性与可塑性，为低资源场景下的持续事件检测提供了新方向

Abstract: Few-shot Continual Event Detection (FCED) poses the dual challenges of
learning from limited data and mitigating catastrophic forgetting across
sequential tasks. Existing approaches often suffer from severe forgetting due
to the full fine-tuning of a shared base model, which leads to knowledge
interference between tasks. Moreover, they frequently rely on data augmentation
strategies that can introduce unnatural or semantically distorted inputs. To
address these limitations, we propose LEAF, a novel and robust expert-based
framework for FCED. LEAF integrates a specialized mixture of experts
architecture into the base model, where each expert is parameterized with
low-rank adaptation (LoRA) matrices. A semantic-aware expert selection
mechanism dynamically routes instances to the most relevant experts, enabling
expert specialization and reducing knowledge interference. To improve
generalization in limited-data settings, LEAF incorporates a contrastive
learning objective guided by label descriptions, which capture high-level
semantic information about event types. Furthermore, to prevent overfitting on
the memory buffer, our framework employs a knowledge distillation strategy that
transfers knowledge from previous models to the current one. Extensive
experiments on multiple FCED benchmarks demonstrate that LEAF consistently
achieves state-of-the-art performance.

</details>


### [233] [OrthAlign: Orthogonal Subspace Decomposition for Non-Interfering Multi-Objective Alignment](https://arxiv.org/abs/2509.24610)
*Liang Lin,Zhihao Xu,Junhao Dong,Jian Zhao,Yuchen Yuan,Guibin Zhang,Miao Yu,Yiming Zhang,Zhengtao Yao,Huahui Yi,Dongrui Liu,Xinfeng Li,Kun Wang*

Main category: cs.LG

TL;DR: OrthAlign通过正交子空间分解解决多目标偏好对齐中的梯度冲突，实现稳定优化并提升多个偏好维度指标


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐方法在处理多目标偏好时存在根本性冲突，导致不同偏好维度间的权衡恶化。现有方法聚焦约束优化和数据选择，但未解决参数层面的冲突根源。

Method: 提出正交子空间分解方法，将参数更新空间分解为互不干扰的正交子空间，结合谱范数约束保证参数更新的线性Lipschitz增长特性

Result: 实验显示：1) 多目标对齐后在帮助性、无害性、真实性维度实现34.61%-50.89%单维度提升；2) 平均整体奖励提升13.96%

Conclusion: OrthAlign开创了基于数学约束的梯度冲突解决新范式，通过理论保证和实验验证，为多目标LLM对齐提供了稳定高效的解决方案

Abstract: Large language model (LLM) alignment faces a critical dilemma when addressing
multiple human preferences: improvements in one dimension frequently come at
the expense of others, creating unavoidable trade-offs between competing
objectives like helpfulness and harmlessness. While prior work mainly focuses
on constraint-based optimization algorithms and data selection strategies to
mitigate conflicts, these approaches overlook the fundamental issue of
resolving conflicts directly at the parameter level. In this paper, we present
OrthAlign, an innovative approach that pioneers a new paradigm by leveraging
orthogonal subspace decomposition to fundamentally resolve gradient-level
conflicts in multi-objective preference alignment. OrthAlign strategically
decomposes parameter update spaces into orthogonal subspaces, ensuring that
optimization toward different preferences occurs in mathematically
non-interfering directions. Building upon this, we provide theoretical
guarantees demonstrating that when parameter increments satisfy both orthogonal
subspace constraints and spectral norm bounds, the resulting updates exhibit
linear Lipschitz growth rather than exponential instability, ensuring stable
convergence across all preference dimensions. Extensive experiments show that:
I. OrthAlign achieves maximum single-preference improvements ranging from
34.61% to 50.89% after multiple-objective alignment across helpful, harmless,
and truthful dimensions. II. With an average overall reward improvement of
13.96%.

</details>


### [234] [When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training](https://arxiv.org/abs/2509.24923)
*Sanxing Chen,Xiaoyin Chen,Yukun Huang,Roy Xie,Bhuwan Dhingra*

Main category: cs.LG

TL;DR: 通过监督微调(SFT)和强化学习(RL)改进LLM的顺序决策能力，发现训练后的模型在探索策略上更高效但偏向贪婪利用，需针对性设计奖励机制


<details>
  <summary>Details</summary>
Motivation: 解决LLM在顺序决策中探索不足的问题，提升其在多臂老虎机任务中的决策能力和跨场景泛化性能

Method: 结合监督学习(专家轨迹)和强化学习(战略奖励/算法奖励)，设计降低方差的遗憾形状奖励和Oracle模仿奖励

Result: 模型性能超越预训练基线，达到UCB/Thompson水平，可泛化至6倍任务时长和不同老虎机类型，但存在早期探索不足风险

Conclusion: 需根据场景选择训练范式，强调定制化奖励设计和多维评估体系，避免单纯依赖平均遗憾指标来培养稳健探索行为

Abstract: While Large Language Models (LLMs) hold promise to become autonomous agents,
they often explore suboptimally in sequential decision-making. Recent work has
sought to enhance this capability via supervised fine-tuning (SFT) or
reinforcement learning (RL), improving regret on the classic multi-armed bandit
task. However, it remains unclear how these learning methods shape exploration
strategies and how well they generalize. We investigate both paradigms by
training LLMs with SFT on expert trajectories and RL with a range of tailored
reward signals including a strategic, regret-shaped reward to reduce variance,
and an algorithmic reward that enables oracle imitation. The resulting agents
outperform pre-trained models and achieve performance comparable to Upper
Confidence Bound (UCB) and Thompson Sampling, with robust generalization to 6x
longer horizons and across bandit families. Behavioral analysis reveals that
gains often stem from more sophisticated but greedier exploitation: RL/SFT
agents are more prone to early catastrophic failure than pre-trained models,
prematurely abandoning exploration. Furthermore, agents trained to imitate UCB
learn to outperform their teacher by adopting more exploitative variants. Our
findings clarify when each training paradigm is preferable and advocate
tailored reward design and evaluation beyond average regret to promote robust
exploratory behavior.

</details>


### [235] [Scaling with Collapse: Efficient and Predictable Training of LLM Families](https://arxiv.org/abs/2509.25087)
*Shane Bergsma,Bin Claire Zhang,Nolan Dey,Shaheer Muhammad,Gurpreet Gosal,Joel Hestness*

Main category: cs.LG

TL;DR: 研究发现损失曲线在优化超参数最优设置下可跨尺度收敛，该现象可作为高效LLM训练的诊断工具


<details>
  <summary>Details</summary>
Motivation: 验证Qiu等人提出的训练损失曲线崩溃现象在联合调整多维度超参数的实际扩展场景中是否成立

Method: 通过设置符合数据预算的最优超参数组合（宽度/深度/学习率/批次大小/权重衰减），观察不同规模模型的损失曲线收敛情况

Result: 发现损失曲线可精准收敛，并提出两种应用：1）通过偏离收敛现象早期诊断训练异常 2）利用收敛曲线规律实现大规模超参数调优的提前终止

Conclusion: 损失曲线收敛现象是计算高效训练的显著特征，基于此开发的Celerity模型家族验证了该现象在LLM开发中的实用价值

Abstract: Effective LLM training relies on *consistency*, meaning that key quantities
-- such as final losses and optimal hyperparameters -- scale predictably across
model sizes. Qiu et al. (2025) recently showed that this consistency extends
beyond scalars: whole training loss curves can *collapse* onto a universal
trajectory after a simple normalization. What remains unclear is whether this
phenomenon holds for LLM families trained under *practical scaling recipes*,
where width, depth, learning rate, batch size, and weight decay are scaled
jointly. We show that it does: loss curves collapse across scales precisely
when optimization hyperparameters are set optimally for the given data budget,
in accordance with recent empirical scaling laws. Collapse thus emerges as a
signature of compute-efficient training. We demonstrate two applications at
scale: (1) deviation-from-collapse provides a sensitive, early diagnostic of
training pathologies, and (2) the predictability of collapsed curves enables
early stopping in large-scale hyperparameter tuning. Finally, we train a
competitive LLM family, *Celerity*, using these insights, highlighting collapse
as an effective tool for developing efficient LLMs.

</details>


### [236] [ORPO-Distill: Mixed-Policy Preference Optimization for Cross-Architecture LLM Distillation](https://arxiv.org/abs/2509.25100)
*Aasheesh Singh,Vishal Vaddina,Dagnachew Birru*

Main category: cs.LG

TL;DR: 提出ORPO-Distill跨架构LLM蒸馏方法，通过偏好优化和混合策略实现知识迁移，在多个数据集和学生模型上优于传统基线


<details>
  <summary>Details</summary>
Motivation: 传统黑盒知识蒸馏方法在跨架构场景下效果受限，需要更有效利用教师模型推理路径的优化方法

Method: 1) 基于Odds-Ratio偏好优化目标对比师生推理轨迹
2) 混合策略结合离线和在线蒸馏优势
3) 通过多样化推理路径实现知识迁移

Result: 在5个数据集和多种学生模型上实现稳定性能提升，显著优于传统知识蒸馏方法

Conclusion: ORPO-Distill为跨架构模型压缩提供了有效解决方案，通过偏好优化机制实现了更优的知识迁移效率

Abstract: We introduce ORPO-Distill, a general-purpose method for cross-architecture
LLM distillation that formulates the problem as a preference optimization task.
Unlike standard CoT distillation, the approach transfers knowledge through
diverse reasoning traces. It employs an Odds-Ratio Preference Optimization
objective that contrasts teacher and student traces for more effective
learning, and adopts a mixed-policy strategy for utilizing student-generated
outputs, outperforming both off- and on-policy alternatives. Experiments on
five datasets and multiple student models show consistent improvements over
conventional black-box KD baselines.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [237] [Code Arcades: 3d Visualization of Classes, Dependencies and Software Metrics](https://arxiv.org/abs/2509.23297)
*Anthony Savidis,Christos Vasilopoulos*

Main category: cs.SE

TL;DR: 论文提出了三个创新点（可配置分组机制、多层次软件指标、交互式可视化引擎）以提升源码理解的灵活性和深度。


<details>
  <summary>Details</summary>
Motivation: 传统软件可视化工具在组织代码元素、分析系统属性和交互性方面存在局限性，难以满足大规模系统分析的需求。

Method: 1. 可配置分组机制支持基于任意关系的代码元素组织；2. 结合细/粗粒度软件指标实现多层级系统分析；3. 开发支持动态调整渲染属性的交互式可视化引擎。

Result: 实现更灵活的系统属性观察方式，提供动态视角调整能力，增强代码结构模式识别和复杂度热点检测效果。

Conclusion: 该方法体系显著提升了软件可视化在代码理解场景中的适应性和洞察力，为软件工程分析提供了更强大的支持。

Abstract: Software visualization seeks to represent software artifacts graphical-ly in
two or three dimensions, with the goal of enhancing comprehension, anal-ysis,
maintenance, and evolution of the source code. In this context, visualiza-tions
employ graphical forms such as dependency structures, treemaps, or time-lines
that incorporate repository histories. These visualizations allow software
engineers to identify structural patterns, detect complexity hotspots, and
infer system behaviors that are difficult to perceive directly from source
text. By adopting metaphor-based approaches, visualization tools provide
macroscopic overviews while enabling focused inspection of specific program
elements, thus offering an accessible means of understanding large-scale
systems. The contri-bution of our work lies in three areas. First, we introduce
a configurable group-ing mechanism that supports flexible organization of code
elements based on arbitrary relationships. Second, we combine fine-grained and
coarse-grained software metrics to provide a multi-level perspective on system
properties. Third, we present an interactive visualization engine that allows
developers to dynamically adjust rendering attributes. Collectively, these
advances provide a more adaptable and insightful approach to source code
comprehension.

</details>


### [238] [Metamorphic Testing for Audio Content Moderation Software](https://arxiv.org/abs/2509.24215)
*Wenxuan Wang,Yongjiang Wu,Junyuan Zhang,Shuqing Li,Yun Peng,Wenting Chen,Shuai Wang,Michael R. Lyu*

Main category: cs.SE

TL;DR: 提出MTAM测试框架用于评估音频内容审核软件对抗性逃逸漏洞的有效性，实验显示其对商业及学术模型均能检测出显著漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有音频审核工具易被恶意用户通过音调修改/噪音插入等细微篡改手段绕过，且对抗性攻击下的有效性缺乏系统研究。

Method: 通过2000个音频样本的试点研究定义14种蜕变关系（含音频特征/启发式扰动两类），生成具有语义保留特性的对抗测试用例。

Result: 测试5款商业软件（Gladia/Assembly AI等）错误发现率达16.7%-51.1%，学术模型达45.7%错误率。

Conclusion: MTAM能有效暴露现有审核系统的脆弱性，为提升AI内容审核鲁棒性提供方法论，具有重要工程实践价值。

Abstract: The rapid growth of audio-centric platforms and applications such as WhatsApp
and Twitter has transformed the way people communicate and share audio content
in modern society. However, these platforms are increasingly misused to
disseminate harmful audio content, such as hate speech, deceptive
advertisements, and explicit material, which can have significant negative
consequences (e.g., detrimental effects on mental health). In response,
researchers and practitioners have been actively developing and deploying audio
content moderation tools to tackle this issue. Despite these efforts, malicious
actors can bypass moderation systems by making subtle alterations to audio
content, such as modifying pitch or inserting noise. Moreover, the
effectiveness of modern audio moderation tools against such adversarial inputs
remains insufficiently studied. To address these challenges, we propose MTAM, a
Metamorphic Testing framework for Audio content Moderation software.
Specifically, we conduct a pilot study on 2000 audio clips and define 14
metamorphic relations across two perturbation categories: Audio Features-Based
and Heuristic perturbations. MTAM applies these metamorphic relations to toxic
audio content to generate test cases that remain harmful while being more
likely to evade detection. In our evaluation, we employ MTAM to test five
commercial textual content moderation software and an academic model against
three kinds of toxic content. The results show that MTAM achieves up to 38.6%,
18.3%, 35.1%, 16.7%, and 51.1% error finding rates (EFR) when testing
commercial moderation software provided by Gladia, Assembly AI, Baidu,
Nextdata, and Tencent, respectively, and it obtains up to 45.7% EFR when
testing the state-of-the-art algorithms from the academy.

</details>


### [239] [DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via Repetitive Pattern](https://arxiv.org/abs/2509.24975)
*Lekang Yang,Yuetong Liu,Yitong Zhang,Jia Li*

Main category: cs.SE

TL;DR: DiffTester框架通过动态识别单元测试中的重复结构模式，显著提升扩散大模型在测试生成中的效率同时保持质量


<details>
  <summary>Details</summary>
Motivation: 现有LLMs逐个生成测试用例效率低下，扩散大模型存在效率与测试质量的权衡问题

Method: 基于抽象语法树分析动态识别重复结构模式，自适应增加每步生成token数量

Result: 在三基准测试中实现显著加速，测试覆盖率保持稳定，支持多编程语言扩展

Conclusion: DiffTester为软件测试生成提供了高效可扩展的解决方案，具有跨模型和语言的通用性

Abstract: Software development relies heavily on extensive unit testing, which makes
the efficiency of automated Unit Test Generation (UTG) particularly important.
However, most existing LLMs generate test cases one token at a time in each
forward pass, which leads to inefficient UTG. Recently, diffusion LLMs (dLLMs)
have emerged, offering promising parallel generation capabilities and showing
strong potential for efficient UTG. Despite this advantage, their application
to UTG is still constrained by a clear trade-off between efficiency and test
quality, since increasing the number of tokens generated in each step often
causes a sharp decline in the quality of test cases. To overcome this
limitation, we present DiffTester, an acceleration framework specifically
tailored for dLLMs in UTG. The key idea of DiffTester is that unit tests
targeting the same focal method often share repetitive structural patterns. By
dynamically identifying these common patterns through abstract syntax tree
analysis during generation, DiffTester adaptively increases the number of
tokens produced at each step without compromising the quality of the output. To
enable comprehensive evaluation, we extend the original TestEval benchmark,
which was limited to Python, by introducing additional programming languages
including Java and C++. Extensive experiments on three benchmarks with two
representative models show that DiffTester delivers significant acceleration
while preserving test coverage. Moreover, DiffTester generalizes well across
different dLLMs and programming languages, providing a practical and scalable
solution for efficient UTG in software development. Code and data are publicly
available at https://github.com/wellbeingyang/DLM4UTG-open .

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [240] [Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective](https://arxiv.org/abs/2509.22613)
*Siwei Wang,Yifei Shen,Haoran Sun,Shi Feng,Shang-Hua Teng,Li Dong,Yaru Hao,Wei Chen*

Main category: cs.AI

TL;DR: 强化学习方法通过探索机制提升LLM规划能力，Q-learning在多样性和稳定性方面优于策略梯度，但需谨慎设计奖励机制


<details>
  <summary>Details</summary>
Motivation: 解析RL方法提升大语言模型规划能力的理论机制，揭示监督微调的局限性及不同强化学习算法的特性差异

Method: 基于图抽象的对比分析框架，结合理论推导与Blocksworld实际场景验证，对比策略梯度与Q-learning的算法特性

Result: 发现监督微调存在伪相关解，策略梯度出现多样性衰减，Q-learning通过离策略学习保持输出多样性且收敛稳定

Conclusion: 探索机制是强化学习泛化能力的关键，Q-learning算法框架在规划任务中展现出更好的鲁棒性，但需配合严谨的奖励设计

Abstract: Recent reinforcement learning (RL) methods have substantially enhanced the
planning capabilities of Large Language Models (LLMs), yet the theoretical
basis for their effectiveness remains elusive. In this work, we investigate
RL's benefits and limitations through a tractable graph-based abstraction,
focusing on policy gradient (PG) and Q-learning methods. Our theoretical
analyses reveal that supervised fine-tuning (SFT) may introduce
co-occurrence-based spurious solutions, whereas RL achieves correct planning
primarily through exploration, underscoring exploration's role in enabling
better generalization. However, we also show that PG suffers from diversity
collapse, where output diversity decreases during training and persists even
after perfect accuracy is attained. By contrast, Q-learning provides two key
advantages: off-policy learning and diversity preservation at convergence. We
further demonstrate that careful reward design is necessary to prevent reward
hacking in Q-learning. Finally, applying our framework to the real-world
planning benchmark Blocksworld, we confirm that these behaviors manifest in
practice.

</details>


### [241] [Toward a Theory of Generalizability in LLM Mechanistic Interpretability Research](https://arxiv.org/abs/2509.22831)
*Sean Trott*

Main category: cs.AI

TL;DR: 探讨大语言模型机制解释研究结果在不同模型间的泛化原则，提出五个对应验证维度并通过实验验证部分维度的一致性


<details>
  <summary>Details</summary>
Motivation: 当前LLM机制研究缺乏判断结果跨模型泛化的明确原则，影响研究结论的适用范围和可靠性

Method: 提出功能性/发展性/位置性/关联性/配置性五个对应轴，通过分析不同规模Pythia模型中1-back注意力头在预训练期间的发展轨迹进行验证

Result: 实验显示1-back注意力头发展轨迹跨模型高度一致，但位置一致性有限；更大模型参数导致注意力机制更早启动且强度更高

Conclusion: 主张通过建立模型设计属性与涌现行为/机制间的映射关系来推进机制解释研究的泛化性

Abstract: Research on Large Language Models (LLMs) increasingly focuses on identifying
mechanistic explanations for their behaviors, yet the field lacks clear
principles for determining when (and how) findings from one model instance
generalize to another. This paper addresses a fundamental epistemological
challenge: given a mechanistic claim about a particular model, what justifies
extrapolating this finding to other LLMs -- and along which dimensions might
such generalizations hold? I propose five potential axes of correspondence
along which mechanistic claims might generalize, including: functional (whether
they satisfy the same functional criteria), developmental (whether they develop
at similar points during pretraining), positional (whether they occupy similar
absolute or relative positions), relational (whether they interact with other
model components in similar ways), and configurational (whether they correspond
to particular regions or structures in weight-space). To empirically validate
this framework, I analyze "1-back attention heads" (components attending to
previous tokens) across pretraining in random seeds of the Pythia models (14M,
70M, 160M, 410M). The results reveal striking consistency in the developmental
trajectories of 1-back attention across models, while positional consistency is
more limited. Moreover, seeds of larger models systematically show earlier
onsets, steeper slopes, and higher peaks of 1-back attention. I also address
possible objections to the arguments and proposals outlined here. Finally, I
conclude by arguing that progress on the generalizability of mechanistic
interpretability research will consist in mapping constitutive design
properties of LLMs to their emergent behaviors and mechanisms.

</details>


### [242] [JE-IRT: A Geometric Lens on LLM Abilities through Joint Embedding Item Response Theory](https://arxiv.org/abs/2509.22888)
*Louie Hong Yao,Nicholas Jarvis,Tiffany Zhan,Saptarshi Ghosh,Linfeng Liu,Tianyu Jiang*

Main category: cs.AI

TL;DR: JE-IRT是一个几何评估框架，通过将LLM和问题嵌入共享空间，提供多维模型能力分析。方向表示问题语义，范数表示难度，支持新模型快速适配并揭示内部分类特征。


<details>
  <summary>Details</summary>
Motivation: 传统LLM评估方法将多维能力压缩为单一分数，掩盖模型真实表现差异。需建立可解释的几何框架连接模型能力与问题结构。

Method: 提出JE-IRT几何框架：模型/问题嵌入共享空间，方向编码语义，范数编码难度，通过几何交互判断正确性。支持基于嵌入空间的新模型快速适配。

Result: 实验证明：1) 分布外行为可由方向对齐解释 2) 范数稳定指示问题难度 3) 学习空间揭示LLM内部分类体系（与人类分类部分一致）4) 新模型仅需单嵌入适配

Conclusion: JE-IRT建立了连接模型能力与问题结构的几何框架，为模型评估和泛化提供统一解释视角，突破传统排名导向的评估范式。

Abstract: Standard LLM evaluation practices compress diverse abilities into single
scores, obscuring their inherently multidimensional nature. We present JE-IRT,
a geometric item-response framework that embeds both LLMs and questions in a
shared space. For question embeddings, the direction encodes semantics and the
norm encodes difficulty, while correctness on each question is determined by
the geometric interaction between the model and question embeddings. This
geometry replaces a global ranking of LLMs with topical specialization and
enables smooth variation across related questions. Building on this framework,
our experimental results reveal that out-of-distribution behavior can be
explained through directional alignment, and that larger norms consistently
indicate harder questions. Moreover, JE-IRT naturally supports generalization:
once the space is learned, new LLMs are added by fitting a single embedding.
The learned space further reveals an LLM-internal taxonomy that only partially
aligns with human-defined subject categories. JE-IRT thus establishes a unified
and interpretable geometric lens that connects LLM abilities with the structure
of questions, offering a distinctive perspective on model evaluation and
generalization.

</details>


### [243] [Not only a helper, but also a teacher: Interactive LLM Cascade](https://arxiv.org/abs/2509.22984)
*Yu Wu,Shuo Wu,Ye Tao,Yansong Li,Anand D. Sarwate*

Main category: cs.AI

TL;DR: 提出Inter-Cascade互动级联系统，通过强模型的策略蒸馏提升弱模型性能，显著降低调用成本


<details>
  <summary>Details</summary>
Motivation: 传统LLM级联对相似/重复查询会重复调用昂贵强模型，导致高成本

Method: 强模型解决困难查询时生成可复用解题策略，动态增强弱模型的后续处理能力

Result: 弱模型准确率提升33.06%，系统总成本降低49.63%，强模型调用减少48.05%

Conclusion: 实现了LLM间的高效知识迁移，为开源和API模型提供可扩展框架

Abstract: Large Language Models (LLMs) vary widely in their capabilities, with larger
models often having better performance but higher cost: choosing an LLM model
often involves trading off performance and cost. The LLM Cascade is a paradigm
that defers difficult queries from weak/cheap to strong/expensive models. This
approach is nonadaptive: the deferral decision is trained offline. When
confronted with similar or repeated queries, the LLM Cascade may then
repeatedly consult the expensive model and incur higher cost. To improve the
cascading efficiency, we propose Inter-Cascade, an online and interactive LLM
Cascade that extends the role of strong model from a backup helper to a
long-term teacher. In our system, when a strong model resolves a difficult
query, it also distills its solution into a generalized, reusable
problem-solving strategy that boosts the weak model on subsequent queries.
Adding strategies to queries enables the weak model to dynamically improve its
performance over time, avoiding computationally and time-intensive fine-tuning.
Empirically, compared with standard LLM Cascade baselines across multiple
benchmarks, the Inter-Cascade significantly improves the accuracy of the weak
model (by up to 33.06 absolute percentage points) and the overall system (by up
to 5.53 absolute percentage points), while reducing the calls to strong models
(by up to 48.05% relative reduction) and saving the corresponding fees (by up
to 49.63% relative reduction). Inter-Cascade demonstrates the effective
in-context knowledge transfer between LLMs, and provides a general, scalable
framework applicable to both open-source and API-based LLMs.

</details>


### [244] [Kimi-Dev: Agentless Training as Skill Prior for SWE-Agents](https://arxiv.org/abs/2509.23045)
*Zonghan Yang,Shengjie Wang,Kelin Fu,Wenyang He,Weimin Xiong,Yibo Liu,Yibo Miao,Bofei Gao,Yejie Wang,Yingwei Ma,Yanhao Li,Yue Liu,Zhenxing Hu,Kaitai Zhang,Shuyi Wang,Huarong Chen,Flood Sung,Yang Liu,Yang Gao,Zhilin Yang,Tianyu Liu*

Main category: cs.AI

TL;DR: 结合Agentless训练与SWE-Agent框架，通过技能先验实现60.4%的SWE-bench性能，验证工作流与代理框架的协同效应


<details>
  <summary>Details</summary>
Motivation: 现有SWE解决方案存在Agent框架（多轮交互）与Agentless方法（单步验证）的割裂，探索两者协同的可能性

Method: 1. 设计Agentless训练配方 2. 开发开源模型Kimi-Dev 3. 使用5k公开轨迹进行SFT适配

Result: Kimi-Dev在SWE-bench Verified达60.4%（工作流最佳），适配后SWE-Agent实现48.6% pass@1（与Claude 3.5 Sonnet相当）

Conclusion: Agentless训练产生的结构化技能先验可实现工作流框架与代理框架的协同，培育可迁移的编码智能体

Abstract: Large Language Models (LLMs) are increasingly applied to software engineering
(SWE), with SWE-bench as a key benchmark. Solutions are split into SWE-Agent
frameworks with multi-turn interactions and workflow-based Agentless methods
with single-turn verifiable steps. We argue these paradigms are not mutually
exclusive: reasoning-intensive Agentless training induces skill priors,
including localization, code edit, and self-reflection that enable efficient
and effective SWE-Agent adaptation. In this work, we first curate the Agentless
training recipe and present Kimi-Dev, an open-source SWE LLM achieving 60.4\%
on SWE-bench Verified, the best among workflow approaches. With additional SFT
adaptation on 5k publicly-available trajectories, Kimi-Dev powers SWE-Agents to
48.6\% pass@1, on par with that of Claude 3.5 Sonnet (241022 version). These
results show that structured skill priors from Agentless training can bridge
workflow and agentic frameworks for transferable coding agents.

</details>


### [245] [Multiplayer Nash Preference Optimization](https://arxiv.org/abs/2509.23102)
*Fang Wu,Xu Huang,Weihao Xuan,Zhiwei Zhang,Yijia Xiao,Guancheng Wan,Xiaomin Li,Bing Hu,Peng Xia,Jure Leskovec,Yejin Choi*

Main category: cs.AI

TL;DR: 提出多人纳什偏好优化框架MNPO，将NLHF扩展至多人博弈场景，通过策略群体竞争解决单对手偏见问题，在异质性偏好场景下验证了优越的对齐效果


<details>
  <summary>Details</summary>
Motivation: 现有基于两人博弈的NLHF方法存在单对手偏见，难以捕捉真实偏好的非传递性和异质性特征，限制了偏好结构的完整覆盖

Method: 构建n-player博弈框架，每个策略与对手群体竞争并正则化参考模型，建立多人纳什均衡理论框架，扩展对偶间隙作为近似质量评估指标

Result: 在指令遵循基准测试中全面超越NLHF基线，在异质性标注条件和混合策略评估场景下实现更优的对齐质量

Conclusion: MNPO为处理复杂非传递性人类偏好提供了理论保证和可扩展框架，通过群体竞争机制推动大模型对齐技术的发展

Abstract: Reinforcement learning from human feedback (RLHF) has emerged as the standard
paradigm for aligning large language models (LLMs) with human preferences.
However, reward-based methods built on the Bradley-Terry assumption struggle to
capture the non-transitive and heterogeneous nature of real-world preferences.
To address this, recent studies have reframed alignment as a two-player Nash
game, giving rise to Nash learning from human feedback (NLHF). While this
perspective has inspired algorithms such as INPO, ONPO, and EGPO with strong
theoretical and empirical guarantees, they remain fundamentally restricted to
two-player interactions, creating a single-opponent bias that fails to capture
the full complexity of realistic preference structures. In this work, we
introduce Multiplayer Nash Preference Optimization (MNPO), a novel framework
that generalizes NLHF to the multiplayer regime. It formulates alignment as an
$n$-player game, where each policy competes against a population of opponents
while being regularized toward a reference model. Our framework establishes
well-defined Nash equilibria in multiplayer settings and extends the concept of
duality gap to quantify approximation quality. We demonstrate that MNPO
inherits the equilibrium guarantees of two-player methods while enabling richer
competitive dynamics and improved coverage of diverse preference structures.
Through comprehensive empirical evaluation, we show that MNPO consistently
outperforms existing NLHF baselines on instruction-following benchmarks,
achieving superior alignment quality under heterogeneous annotator conditions
and mixed-policy evaluation scenarios. Together, these results establish MNPO
as a principled and scalable framework for aligning LLMs with complex,
non-transitive human preferences. Code is available at
https://github.com/smiles724/MNPO.

</details>


### [246] [$p$-less Sampling: A Robust Hyperparameter-Free Approach for LLM Decoding](https://arxiv.org/abs/2509.23234)
*Runyan Tan,Shuang Wu,Phillip Howard*

Main category: cs.AI

TL;DR: 提出无超参数的p-less采样方法，通过动态概率截断阈值实现稳定高质量文本生成，在高温环境下性能优于传统方法


<details>
  <summary>Details</summary>
Motivation: 现有LLM采样方法对超参数敏感且温度适应性差，需要更稳定高效的解码策略

Method: 基于信息论动态计算每个解码步的截断阈值，利用完整概率分布实现参数无关的token选择机制

Result: 在数学推理/创意写作等任务中保持质量优势，高温下文本质量仅下降3.2%（对比基线17.8%），平均采样时间减少40%

Conclusion: p-less采样通过理论驱动的动态阈值机制，实现了高效稳定的文本生成，为LLM解码提供了新的无参数化解决方案

Abstract: Obtaining high-quality outputs from Large Language Models (LLMs) often
depends upon the choice of a sampling-based decoding strategy to
probabilistically choose the next token at each generation step. While a
variety of such sampling methods have been proposed, their performance can be
sensitive to the selection of hyperparameters which may require different
settings depending upon the generation task and temperature configuration. In
this work, we introduce $p$-less sampling: an information-theoretic approach to
sampling which dynamically sets a truncation threshold at each decoding step
based on the entire token probability distribution. Unlike existing methods,
$p$-less sampling has no hyperparameters and consistently produces high-quality
outputs as temperature increases. We provide theoretical perspectives on
$p$-less sampling to ground our proposed method and conduct experiments to
empirically validate its effectiveness across a range of math, logical
reasoning, and creative writing tasks. Our results demonstrate how $p$-less
sampling consistently outperforms existing sampling approaches while exhibiting
much less degradation in text quality at higher temperature values. We further
show how $p$-less achieves greater inference-time efficiency than alternative
methods through lower average token sampling times and shorter generation
lengths, without sacrificing accuracy. Finally, we provide analyses to
highlight the benefits of $p$-less through qualitative examples, case studies,
and diversity assessments.

</details>


### [247] [Learning How to Use Tools, Not Just When: Pattern-Aware Tool-Integrated Reasoning](https://arxiv.org/abs/2509.23292)
*Ningning Xu,Yuxuan Jiang,Shubhashis Roy Dipta*

Main category: cs.AI

TL;DR: 提出模式感知的TIR方法，通过两阶段框架提升代码使用效果和推理准确率，在MATH500和AIME24数据集上Code@1分别提升6.5%和23.3%。


<details>
  <summary>Details</summary>
Motivation: 现有工具集成推理研究主要关注工具调用时机，但忽视工具应用方式的选择（计算器模式/算法模式），模式错配导致推理失败。

Method: 两阶段框架：1）构建代码能力学习两种模式 2）基于教师偏好对齐模式选择机制

Result: MATH500数据集Code@1从64.0%提升至70.5%，AIME24数据集从26.7%提升至50.0%

Conclusion: 模式感知方法能有效提升工具集成推理效果，代码使用质量与推理准确率同步优化，为TIR研究提供新方向。

Abstract: Tool-integrated reasoning (TIR) has become a key approach for improving large
reasoning models (LRMs) on complex problems. Prior work has mainly studied when
to invoke tools, while overlooking how tools are applied. We identify two
common patterns: a calculator pattern that uses code for direct computation,
and an algorithmic pattern that encodes problems as programs. Misaligned
choices often cause failures even when reasoning is sound. We propose a
two-stage framework that first builds code competence from both patterns and
then aligns pattern selection with teacher preferences. Across challenging math
datasets, our pattern-aware method substantially improves both code usage and
accuracy, for instance raising Code@1 on MATH500 from 64.0% to 70.5% and on
AIME24 from 26.7% to 50.0%. These gains highlight the effectiveness of a
pattern-aware approach for tool-integrated reasoning.

</details>


### [248] [Your Models Have Thought Enough: Training Large Reasoning Models to Stop Overthinking](https://arxiv.org/abs/2509.23392)
*Jinyi Han,Ying Huang,Ying Liao,Zishang Jiang,Xikun Lu,Haiquan Zhao,Xinyi Wang,Guanghao Zhou,Sihang Jiang,Jiaqing Liang,Weikang Zhou,Zeye Sun,Fei Yu,Yanghua Xiao*

Main category: cs.AI

TL;DR: JET方法通过主动终止冗余推理步骤，显著提升大型推理模型效率且不降低准确性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法难以构造短推理路径，而LRMs在早期已积累足够信息，后续步骤存在冗余。

Method: 采用轨迹截断技术暴露短推理路径，结合质量控制的长度奖励机制平衡简洁性与正确性。

Result: DeepSeek-Distill-Qwen-1.5B模型在奥赛基准上准确率提升4.6%同时输出长度减少46.3%。

Conclusion: JET为提升LRMs推理效率提供了新范式，在保持性能的前提下大幅降低计算资源消耗。

Abstract: Large Reasoning Models (LRMs) have achieved impressive performance on
challenging tasks, yet their deep reasoning often incurs substantial
computational costs. To achieve efficient reasoning, existing reinforcement
learning methods still struggle to construct short reasoning path during the
rollout stage, limiting effective learning. Inspired by Evidence Accumulation
Models, we find that LRMs have accumulated sufficient information early in
reasoning, making further reasoning steps redundant. Based on this insight, we
propose Just-Enough Thinking (JET), which trains models to proactively
terminate unnecessary reasoning. JET performs trajectory truncation during
rollout to expose the model to short, distributionally consistent reasoning
paths. Besides, it uses a quality-controlled length reward to better encourage
concise reasoning while maintaining correctness. Extensive experiments
demonstrate that JET significantly improves reasoning efficiency without
sacrificing accuracy. Especially, DeepSeek-Distill-Qwen-1.5B achieves a 4.6%
accuracy gain while reducing output length by 46.3% on the Olympiad benchmark.
Our code is available in the GitHub.

</details>


### [249] [Mapping Overlaps in Benchmarks through Perplexity in the Wild](https://arxiv.org/abs/2509.23488)
*Siyang Wu,Honglin Bao,Sida Li,Ari Holtzman,James A. Evans*

Main category: cs.AI

TL;DR: 研究者通过构建大语言模型基准测试的'能力熟悉度特征'，揭示不同测试间的有效重叠与模型能力关联。发现性能重叠普遍但语义重叠有限，编码领域与其他能力交叉最少，基准特征对格式干扰具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准评估常混淆性能与真实能力，且缺乏对测试间本质重叠的分析。研究旨在建立客观指标，揭示模型能力的内在关联，解决基准有效性争议。

Method: 使用32个LLM在88个跨领域基准的表现数据，通过逐步前向选择和线性回归提取基准特征（关键token的困惑度），量化测试间的能力需求重叠。

Result: 1. 知识/推理子任务高度重叠，多语言/文化基准独特性强
2. 性能结果易受问题格式等无关因素干扰
3. 基准特征有效捕捉跨功能重叠（逻辑/数学/语言），编码领域孤立性显著

Conclusion: 基准特征机制为评估LLM提供新视角，揭示能力网络本质。当前基准协议存在泛化局限，未来评估需区分表面性能与深层能力，重视跨功能交叉验证。

Abstract: We develop signatures of capacity familiarity to characterize large language
model (LLM) benchmarks and their meaningful overlaps. Benchmark signatures
probe the capacity required for benchmark performance. We formally define them
as a set of salient tokens drawn from in-the-wild, naturally authored corpora,
where LLM token perplexity, reflecting more or less pre-training exposure,
becomes highly predictive of LLM benchmark performance. Through a large-scale
meta-evaluation, we extract benchmark signatures via stepwise forward selection
with linear regressions across 32 LLMs and 88 benchmarks spanning diverse
knowledge, coding, logic, instruction following, math, language, reasoning, and
world modeling. Our analysis situates signatures in relation to both the
semantic similarity of benchmark questions and the correlation of model
performance. While performance overlaps are universally high and semantic
overlaps remain confined to a narrow mid-range, benchmark signatures prove
highly informative in capturing variation, overlap, and divergence. We observe
overlap in knowledge and reasoning subtasks, whereas multilingual and cultural
benchmarks exhibit less similarity, even compared to cross-task overlap.
Notably, performance-level results are strongly influenced by
benchmark-orthogonal factors such as question format, highlighting limitations
in LLM generalization, the conflation of performance with ability, and issues
inherent in current mainstream benchmark agreement studies. Benchmark
signatures, however, remain robust to such effects. Ultimately, we identify
cross-functional overlaps across logic, math, language, instruction following,
and world modeling, with coding emerging as the least overlapping domain.
Together, these findings provide mechanistic insights into benchmark validity
and LLM sensitivities, and sketch the underlying landscape of interconnected
LLM capabilities.

</details>


### [250] [Clean First, Align Later: Benchmarking Preference Data Cleaning for Reliable LLM Alignment](https://arxiv.org/abs/2509.23564)
*Min-Hsuan Yeh,Yixuan Li*

Main category: cs.AI

TL;DR: 该研究提出首个综合基准PrefCleanBench，系统评估了13种偏好数据清洗方法在LLM对齐中的效果，揭示了数据清洗成功的关键因素并开源实现促进研究。


<details>
  <summary>Details</summary>
Motivation: 现有数据清洗方法缺乏系统性评估，噪声反馈会降低奖励模型质量并阻碍LLM与人类偏好对齐。需要标准化评估框架来改进数据预处理质量。

Method: 创建PrefCleanBench基准，在多样化数据集、模型架构和优化算法中统一测试13种清洗策略的对齐性能和跨场景泛化能力。

Result: 基准验证了不同清洗方法的有效性边界，识别出影响数据清洗效果的核心因素，建立了数据质量与模型对齐性能的量化关系。

Conclusion: 该研究为LLM对齐领域提供了首个可复现的数据清洗评估体系，强调数据预处理在负责任AI发展中的关键作用，开源代码推动该方向持续发展。

Abstract: Human feedback plays a pivotal role in aligning large language models (LLMs)
with human preferences. However, such feedback is often noisy or inconsistent,
which can degrade the quality of reward models and hinder alignment. While
various automated data cleaning methods have been proposed to mitigate this
issue, a systematic evaluation of their effectiveness and generalizability
remains lacking. To bridge this gap, we introduce the first comprehensive
benchmark for evaluating 13 preference data cleaning methods in the context of
LLM alignment. PrefCleanBench offers a standardized protocol to assess cleaning
strategies in terms of alignment performance and generalizability across
diverse datasets, model architectures, and optimization algorithms. By unifying
disparate methods and rigorously comparing them, we uncover key factors that
determine the success of data cleaning in alignment tasks. This benchmark lays
the groundwork for principled and reproducible approaches to improving LLM
alignment through better data quality-highlighting the crucial but
underexplored role of data preprocessing in responsible AI development. We
release modular implementations of all methods to catalyze further research:
https://github.com/deeplearning-wisc/PrefCleanBench.

</details>


### [251] [From Reasoning to Answer: Empirical, Attention-Based and Mechanistic Insights into Distilled DeepSeek R1 Models](https://arxiv.org/abs/2509.23676)
*Jue Zhang,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.AI

TL;DR: 大型推理模型通过显式推理轨迹影响答案生成，三阶段研究验证推理机制与答案输出的功能关联


<details>
  <summary>Details</summary>
Motivation: 探究显式推理轨迹对模型答案生成的实际影响，验证中间推理过程在模型输出中的功能性作用

Method: 1. 实证评估推理对答案质量的影响 2. 注意力机制分析推理焦点头 3. 激活修补干预关键推理节点

Result: 推理标记扰动可改变最终答案，确认推理到答案的定向信息流；中间层RFHs捕捉自我反思线索

Conclusion: 中间推理过程在模型决策中具有功能性作用，揭示了LRMs通过结构化推理提升输出质量的机制

Abstract: Large Reasoning Models (LRMs) generate explicit reasoning traces alongside
final answers, yet the extent to which these traces influence answer generation
remains unclear. In this work, we conduct a three-stage investigation into the
interplay between reasoning and answer generation in three distilled DeepSeek
R1 models. First, through empirical evaluation, we demonstrate that including
explicit reasoning consistently improves answer quality across diverse domains.
Second, attention analysis reveals that answer tokens attend substantially to
reasoning tokens, with certain mid-layer Reasoning-Focus Heads (RFHs) closely
tracking the reasoning trajectory, including self-reflective cues. Third, we
apply mechanistic interventions using activation patching to assess the
dependence of answer tokens on reasoning activations. Our results show that
perturbations to key reasoning tokens can reliably alter the final answers,
confirming a directional and functional flow of information from reasoning to
answer. These findings deepen our understanding of how LRMs leverage reasoning
tokens for answer generation, highlighting the functional role of intermediate
reasoning in shaping model outputs. Our data and code are publicly available at
\href{https://aka.ms/R2A-code}{this URL}.

</details>


### [252] [SafeSearch: Automated Red-Teaming for the Safety of LLM-Based Search Agents](https://arxiv.org/abs/2509.23694)
*Jianshuo Dong,Sheng Guo,Hao Wang,Zhuotao Liu,Tianwei Zhang,Ke Xu,Minlie Huang,Han Qiu*

Main category: cs.AI

TL;DR: 本文提出自动化红队框架SafeSearch，揭示LLM搜索代理在不可靠搜索结果下的高漏洞（GPT-4.1-mini的ASR达90.5%），并验证常见防御措施的有效性局限。


<details>
  <summary>Details</summary>
Motivation: 搜索代理接入互联网时，低质量搜索结果可能形成新的安全威胁面。现有安全评估缺乏系统性方法，需建立轻量无害的自动化评估体系。

Method: 1. 构建覆盖5类风险的300测试用例基准 2. 评估3类搜索代理架构（搜索流程/工具调用/深度研究）在15个LLM（7个商业+8个开源）的表现 3. 分析提醒提示等防御手段效果

Result: 1. 暴露于不可靠网站时，搜索工作流场景下GPT-4.1-mini攻击成功率高达90.5% 2. 开源模型平均ASR（44.7%）显著高于商业模型（21.8%）3. 常规防御手段仅降低ASR 5-10%

Conclusion: 该框架为安全评估提供系统性解决方案，测试结果揭示搜索代理生态的脆弱性，强调透明化评估对安全开发的关键价值。

Abstract: Search agents connect LLMs to the Internet, enabling access to broader and
more up-to-date information. However, unreliable search results may also pose
safety threats to end users, establishing a new threat surface. In this work,
we conduct two in-the-wild experiments to demonstrate both the prevalence of
low-quality search results and their potential to misguide agent behaviors. To
counter this threat, we introduce an automated red-teaming framework that is
systematic, scalable, and cost-efficient, enabling lightweight and harmless
safety assessments of search agents. Building on this framework, we construct
the SafeSearch benchmark, which includes 300 test cases covering five
categories of risks (e.g., misinformation and indirect prompt injection). Using
this benchmark, we evaluate three representative search agent scaffolds,
covering search workflow, tool-calling, and deep research, across 7 proprietary
and 8 open-source backend LLMs. Our results reveal substantial vulnerabilities
of LLM-based search agents: when exposed to unreliable websites, the highest
ASR reached 90.5% for GPT-4.1-mini under a search workflow setting. Moreover,
our analysis highlights the limited effectiveness of common defense practices,
such as reminder prompting. This emphasizes the value of our framework in
promoting transparency for safer agent development. Our codebase and test cases
are publicly available: https://github.com/jianshuod/SafeSearch.

</details>


### [253] [From What to Why: A Multi-Agent System for Evidence-based Chemical Reaction Condition Reasoning](https://arxiv.org/abs/2509.23768)
*Cheng Yang,Jiaxuan Lu,Haiyuan Wan,Junchi Yu,Feiwei Qin*

Main category: cs.AI

TL;DR: 提出ChemMAS多智能体系统，通过证据推理机制提升化学反应条件推荐的准确性和可解释性，Top-1准确率比基线高20-35%


<details>
  <summary>Details</summary>
Motivation: 现有基于大模型的反应条件推荐方法缺乏可证伪的解释机制，制约其在科学工作流中的可信度

Method: 将预测任务分解为机理基础构建、多通道案例召回、约束感知的智能体辩论、证据聚合四阶段框架

Result: 在Top-1准确率上超越领域基线20-35%，优于通用大模型10-15%，且生成可验证的化学机理解释

Conclusion: 通过多智能体协同与化学知识融合，建立了可解释AI在科学发现中的新范式

Abstract: The chemical reaction recommendation is to select proper reaction condition
parameters for chemical reactions, which is pivotal to accelerating chemical
science. With the rapid development of large language models (LLMs), there is
growing interest in leveraging their reasoning and planning capabilities for
reaction condition recommendation. Despite their success, existing methods
rarely explain the rationale behind the recommended reaction conditions,
limiting their utility in high-stakes scientific workflows. In this work, we
propose ChemMAS, a multi-agent system that reframes condition prediction as an
evidence-based reasoning task. ChemMAS decomposes the task into mechanistic
grounding, multi-channel recall, constraint-aware agentic debate, and rationale
aggregation. Each decision is backed by interpretable justifications grounded
in chemical knowledge and retrieved precedents. Experiments show that ChemMAS
achieves 20-35% gains over domain-specific baselines and outperforms
general-purpose LLMs by 10-15% in Top-1 accuracy, while offering falsifiable,
human-trustable rationales, which establishes a new paradigm for explainable AI
in scientific discovery.

</details>


### [254] [Conditional Advantage Estimation for Reinforcement Learning in Large Reasoning Models](https://arxiv.org/abs/2509.23962)
*Guanxu Chen,Yafu Li,Yuxian Jiang,Chen Qian,Qihan Ren,Jingyi Yang,Yu Cheng,Dongrui Liu,Jing Shao*

Main category: cs.AI

TL;DR: 提出CANON方法解决RLVR中指标方向性偏差问题，通过无方向性分组比较提升LLM数学推理能力与效率


<details>
  <summary>Details</summary>
Motivation: 现有强化学习奖励机制依赖人工预设指标方向（如熵高/低偏好），易因参数偏差导致失败，需更稳健的优化方法

Method: CANON将样本按目标指标分组→跨组比较确定优化方向→组内选择更优响应，实现无预设方向的优势估计

Result: 在3类LLM的数学/逻辑任务中持续优于基线，响应长度优化使token效率提升，获得更优Pareto前沿

Conclusion: CANON突破传统方向性限制，为LLM强化学习提供通用且稳定的优化框架，在性能-成本平衡中展现优势

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) for large language
models (LLMs) has achieved remarkable progress in enhancing LLMs' reasoning
capabilities on tasks with clear correctness criteria, such as mathematical
reasoning tasks. Several training metrics, such as entropy or response length,
have been observed to correlate with different reasoning behaviors in
reinforcement learning. Prior approaches incorporate such priors through reward
or advantage shaping, which often relies on hand-crafted penalties and
preferences (e.g., higher-is-better or lower-is-better). However, without
careful hyperparameter tuning, these directional priors can be overly biased
and may lead to failure. To this end, we introduce Conditional advANtage
estimatiON (CANON), amplifying the impact of the target metric without
presuming its direction. Specifically, CANON regroups the sampled responses
into two groups based on the higher or lower value of a target metric, measures
which metric trend contributes to better performance through inter-group
comparison, and identifies the better response within the same group. In
summary, CANON based on entropy consistently outperforms prior methods across
three LLMs on both math reasoning and high-complexity logic tasks. When applied
to response length, CANON further improves token efficiency, yielding a more
favorable Pareto frontier in the performance-cost trade-off.

</details>


### [255] [Do Repetitions Matter? Strengthening Reliability in LLM Evaluations](https://arxiv.org/abs/2509.24086)
*Miguel Angel Alvarado Gonzalez,Michelle Bruno Hernandez,Miguel Angel Peñaloza Perez,Bruno Lopez Orozco,Jesus Tadeo Cruz Soto,Sandra Malagon*

Main category: cs.AI

TL;DR: 单次运行的LLM排行榜不可靠，83%情况下出现排名反转。建议采用≥2次重复评估并报告不确定性，可在可行范围内显著提升结果稳定性。


<details>
  <summary>Details</summary>
Motivation: 针对现有LLM排行榜单次随机运行的脆弱性问题，验证多次重复对评估结果可靠性的影响，为实践者提供成本敏感的评估方案。

Method: 使用混合效应逻辑回归、领域级边际均值和运行间可靠性分析，对AI4Math基准上的8个SOTA模型进行三次独立运行评估。

Result: 单次运行导致83%排名反转，两次运行消除83%反转，三次运行使标准误缩减5%。组间相关性中等且无显著性符号翻转。

Conclusion: 建议将模型评估视为实验设计，要求至少2次随机解码重复，并在报告中呈现不确定性，这种方案在保证可行性的同时显著提升评估鲁棒性。

Abstract: LLM leaderboards often rely on single stochastic runs, but how many
repetitions are required for reliable conclusions remains unclear. We
re-evaluate eight state-of-the-art models on the AI4Math Benchmark with three
independent runs per setting. Using mixed-effects logistic regression,
domain-level marginal means, rank-instability analysis, and run-to-run
reliability, we assessed the value of additional repetitions. Our findings
shows that Single-run leaderboards are brittle: 10/12 slices (83\%) invert at
least one pairwise rank relative to the three-run majority, despite a zero
sign-flip rate for pairwise significance and moderate overall interclass
correlation. Averaging runs yields modest SE shrinkage ($\sim$5\% from one to
three) but large ranking gains; two runs remove $\sim$83\% of single-run
inversions. We provide cost-aware guidance for practitioners: treat evaluation
as an experiment, report uncertainty, and use $\geq 2$ repetitions under
stochastic decoding. These practices improve robustness while remaining
feasible for small teams and help align model comparisons with real-world
reliability.

</details>


### [256] [Reasoning or Retrieval? A Study of Answer Attribution on Large Reasoning Models](https://arxiv.org/abs/2509.24156)
*Yuhui Wang,Changjiang Li,Guangke Chen,Jiacheng Liang,Ting Wang*

Main category: cs.AI

TL;DR: 大推理模型通过链式推理解决问题时存在答案与推理过程自相矛盾的现象，研究发现这是推理机制与记忆检索机制竞争所致，并提出FARL框架通过抑制记忆检索来增强推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在链式推理过程中存在答案与推理逻辑不一致的问题，研究发现这种矛盾源于模型同时采用推理和记忆检索两种竞争机制，而当前微调方法会助长模型走记忆检索的捷径。

Method: 通过控制实验引入误导性推理线索和破坏性记忆答案，验证双机制竞争现象；提出FARL框架，将记忆遗忘机制融入强化学习微调过程，抑制记忆检索捷径。

Result: 实验证实模型规模、问题领域和微调方法（强化学习/知识蒸馏）影响机制主导权，FARL框架在GSM8K等数据集上提升3-15%的推理准确性。

Conclusion: FARL框架通过抑制记忆检索捷径，有效提升模型的泛化推理能力，为突破当前推理微调范式局限性提供了新方向。

Abstract: Large reasoning models (LRMs) exhibit unprecedented capabilities in solving
complex problems through Chain-of-Thought (CoT) reasoning. However, recent
studies reveal that their final answers often contradict their own reasoning
traces. We hypothesize that this inconsistency stems from two competing
mechanisms for generating answers: CoT reasoning and memory retrieval. To test
this hypothesis, we conduct controlled experiments that challenge LRMs with
misleading cues during reasoning and/or corrupted answers during retrieval. Our
results across models and datasets confirm that both mechanisms operate
simultaneously, with their relative dominance influenced by multiple factors:
problem domains, model scales, and fine-tuning approaches (e.g., reinforcement
learning vs. distillation). The findings reveal a critical limitation in
current reasoning fine-tuning paradigms: models can exploit the retrieval
mechanism as a shortcut, effectively "hacking" the reward signal and
undermining genuine reasoning development. To address this challenge, we
introduce FARL, a novel fine-tuning framework that integrates memory unlearning
with reinforcement learning. By carefully suppressing retrieval shortcuts
during the fine-tuning process, FARL promotes reasoning-dominant behavior and
enhances generalizable reasoning capabilities.

</details>


### [257] [Learning to Ponder: Adaptive Reasoning in Latent Space](https://arxiv.org/abs/2509.24238)
*Yixin He,Lumingyuan Tang*

Main category: cs.AI

TL;DR: FR-Ponder提出基于潜在向量调控的自适应计算分配框架，在保持主干模型不变的同时优化LLM推理效率与准确性


<details>
  <summary>Details</summary>
Motivation: 解决传统方法对输入复杂度不敏感导致的算力浪费（简单问题过度计算）和算力不足（复杂问题欠计算）问题

Method: 通过<1M参数的控制器监测隐藏状态，使用预计算的潜在转向向量调整模型表示，结合GRPO策略优化实现计算深度自适应调控

Result: 在GSM8K和MATH500基准上提升计算-准确率边界，FLOPs降低的同时保持/提升准确率，优于早期退出基线方法

Conclusion: 通过课程学习和奖励工程实现的动态计算分配机制，既能匹配问题难度，又保持模型权重不变，展示了潜在空间调控的有效性

Abstract: Test-time compute has emerged as a key paradigm for enhancing LLM reasoning,
yet prevailing approaches like Best-of-N and majority voting apply uniform
depth across inputs, wasting computation on simple queries while potentially
under-thinking complex ones. We present FR-Ponder, a single-graph,
backbone-training-free framework that allocates instance-adaptive reasoning
compute via latent steering. A less than 1M-param controller observes hidden
states and decides to halt or apply a small ponder step by adding a
pre-computed steering vector to frozen representations. Our method extracts the
latent steering vector associated with deeper reasoning outputs and direct IO
from LLM and re-applies it through a tunable scaling factor, allowing the model
to adapt its reasoning depth to the complexity of each input. To balance
performance and computational cost, we employ Group Relative Policy
Optimization (GRPO) as a reward signal to adaptively regulate reasoning depth,
achieving task accuracy while mitigating overreasoning. Through curriculum
learning and careful reward engineering, FR-Ponder learns calibrated compute
allocation correlated with problem difficulty. On GSM8K and MATH500, FR-Ponder
improves the compute-accuracy frontier, delivering lower FLOPs with better
matched accuracy and comparing favorably to early-exit baselines, without
modifying backbone weights. Analyses visualize interpretable steering
directions and show learned compute allocation correlates with problem
difficulty.

</details>


### [258] [SpecExit: Accelerating Large Reasoning Model via Speculative Exit](https://arxiv.org/abs/2509.24248)
*Rubing Yang,Huajun Bai,Song Liu,Guanghua Yu,Runzhi Fan,Yanbin Dang,Jiejing Zhang,Kai Liu,Jianchen Zhu,Peng Chen*

Main category: cs.AI

TL;DR: 提出SpecExit框架，通过轻量级草稿模型预测早期退出信号，减少66%生成长度并实现2.5倍加速


<details>
  <summary>Details</summary>
Motivation: 大型推理模型存在过度思考问题，导致生成冗长输出和延迟增加。现有早期退出机制依赖探测机制，存在检测开销和泛化性问题。

Method: 利用推测解码中的隐藏状态信息，直接通过草稿模型预测未来token和早期退出信号，消除探测开销

Result: 相比基线方法，平均生成长度减少66%，端到端延迟加速2.5倍，且保持准确性不变

Conclusion: 隐藏状态能有效提供早期退出信号，该方法为高效推理开辟了新方向，代码已开源

Abstract: Despite their strong performance on reasoning tasks, large reasoning models
(LRMs) often suffer from overthinking, producing unnecessarily long outputs and
incurring high end-to-end latency, a significant limitation to their real-world
deployment. To address overthinking, early-exit mechanisms have been proposed
to terminate reasoning before typical completion, showing that this approach
can effectively shorten generation length with minimal impact on accuracy.
However, their reliance on probing mechanisms introduces a detection overhead
that limits their end-to-end latency gains and compromises their
generalizability across diverse problems. Inspired by the use of hidden states
in speculative decoding, we propose SpecExit, a novel framework that predicts
both future tokens and an early-exit signal directly from a lightweight draft
model without probing overhead. Our method offers significant improvements,
reducing average generation length by 66\% and achieving a 2.5x speedup in
end-to-end latency compared to the speculative decoding baseline, without
compromising accuracy. Our method leverages the inherent signals from hidden
states to provide effective early-exit signals, suggesting broader use of
hidden states for efficient reasoning. Our code is available at
https://github.com/Tencent/AngelSlim.

</details>


### [259] [PAME-AI: Patient Messaging Creation and Optimization using Agentic AI](https://arxiv.org/abs/2509.24263)
*Junjie Luo,Yihong Guo,Anqi Liu,Ritu Agarwal,Gordon,Gao*

Main category: cs.AI

TL;DR: 提出PAME-AI代理智能系统，通过DIKW框架将医疗数据转化为优化消息策略，两阶段实验显示最佳消息点击率提升12.2%


<details>
  <summary>Details</summary>
Motivation: 传统医疗消息设计受限于高维探索空间，需系统性方法优化患者沟通效果

Method: 基于DIKW层次构建多代理系统，通过44万+和7.4万+患者互动的两阶段实验验证

Result: 优化消息参与度达68.76%，较基线提升12.2%相对改进

Conclusion: 代理架构支持并行处理和持续学习，适用于大规模医疗沟通优化

Abstract: Messaging patients is a critical part of healthcare communication, helping to
improve things like medication adherence and healthy behaviors. However,
traditional mobile message design has significant limitations due to its
inability to explore the high-dimensional design space. We develop PAME-AI, a
novel approach for Patient Messaging Creation and Optimization using Agentic
AI. Built on the Data-Information-Knowledge-Wisdom (DIKW) hierarchy, PAME-AI
offers a structured framework to move from raw data to actionable insights for
high-performance messaging design. PAME-AI is composed of a system of
specialized computational agents that progressively transform raw experimental
data into actionable message design strategies. We demonstrate our approach's
effectiveness through a two-stage experiment, comprising of 444,691 patient
encounters in Stage 1 and 74,908 in Stage 2. The best-performing generated
message achieved 68.76% engagement compared to the 61.27% baseline,
representing a 12.2\% relative improvement in click-through rates. This agentic
architecture enables parallel processing, hypothesis validation, and continuous
learning, making it particularly suitable for large-scale healthcare
communication optimization.

</details>


### [260] [AdvChain: Adversarial Chain-of-Thought Tuning for Robust Safety Alignment of Large Reasoning Models](https://arxiv.org/abs/2509.24269)
*Zihao Zhu,Xinyu Wu,Gehan Hu,Siwei Lyu,Ke Xu,Baoyuan Wu*

Main category: cs.AI

TL;DR: 提出对抗性思维链调优框架AdvChain，通过构建诱惑-纠正和犹豫-纠正样本，有效解决大型推理模型思维链中的安全风险雪球效应


<details>
  <summary>Details</summary>
Motivation: 现有安全调优方法存在雪球效应风险，模型仅模仿完美推理脚本而缺乏自我纠正能力，导致有害合规或过度拒绝

Method: 构建包含诱惑偏差恢复和过度谨慎修正的双重对抗训练数据集，采用动态自我纠正的对抗调优范式

Result: 显著提升对抗越狱攻击的鲁棒性(攻击成功率下降46%)，良性提示拒绝率降低32%，推理能力保持无损

Conclusion: AdvChain开创了安全推理模型新范式，首次实现安全防护与实用性的动态平衡，为可靠AI推理系统提供新方向

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in
complex problem-solving through Chain-of-Thought (CoT) reasoning. However, the
multi-step nature of CoT introduces new safety challenges that extend beyond
conventional language model alignment. We identify a failure mode in current
safety CoT tuning methods: the \textit{snowball effect}, where minor reasoning
deviations progressively amplify throughout the thought process, leading to
either harmful compliance or excessive refusal. This effect stems from models
being trained to imitate perfect reasoning scripts without learning to
self-correct. To address this limitation, we propose AdvChain, an alignment
paradigm that teaches models dynamic self-correction through adversarial CoT
tuning. Our method involves constructing a dataset containing
Temptation-Correction and Hesitation-Correction samples, where models learn to
recover from harmful reasoning drifts and unnecessary cautions. Extensive
experiments show that AdvChain significantly enhances robustness against
jailbreak attacks and CoT hijacking while substantially reducing over-refusal
on benign prompts, achieving a superior safety-utility balance without
compromising reasoning capabilities. Our work establishes a new direction for
building more robust and reliable reasoning models.

</details>


### [261] [SCI-Verifier: Scientific Verifier with Thinking](https://arxiv.org/abs/2509.24285)
*Shenghe Zheng,Chenyu Huang,Fangchen Yu,Junchi Yao,Jingqi Ye,Tao Chen,Yun Luo,Ning Ding,LEI BAI,Ganqu Cui,Peng Ye*

Main category: cs.AI

TL;DR: 提出SCI-VerifyBench跨学科基准和SCI-Verifier推理增强验证器，系统性解决科学领域LLM验证难题


<details>
  <summary>Details</summary>
Motivation: 现有科学验证研究存在评估标准缺失、学科覆盖不足，且依赖复杂规则设计导致泛化性差的问题

Method: 数据层构建含五大学科的真实LLM响应基准（含等效转换增强），模型层开发通过后训练强化推理能力的统一验证框架

Result: SCI-Verifier展现强大逻辑推理和等效判断能力，基准测试实现系统评估与质量保障的平衡

Conclusion: 该框架为科学验证提供系统评估标准和实用路径，提升LLMs在科学领域的可靠性和适用性

Abstract: As large language models (LLMs) are increasingly applied to scientific
reasoning, the complexity of answer formats and the diversity of equivalent
expressions make answer verification a critical yet challenging task. Existing
verification studies in scientific domains suffer from two major limitations:
(a) the absence of systematic evaluation standards and insufficient
disciplinary coverage, which hinders their comprehensive assessment; and (b)
heavy reliance on cumbersome rule design or prompt engineering, which reduces
their effectiveness in complex reasoning scenarios or limits their
cross-disciplinary generalization. To address these challenges, we propose
solutions at both the data and model levels. On the data side, we construct
SCI-VerifyBench, a cross-disciplinary benchmark covering mathematics, physics,
biology, chemistry, and general scientific QA. The benchmark is built from real
LLM responses and enhanced with domain-specific equivalence transformations
that generate challenging and realistic data. Model-based and expert
annotations ensure both quality and diversity, enabling rigorous evaluation of
verification ability. On the model side, we emphasize the importance of
reasoning for verification and introduce SCI-Verifier, a unified
reasoning-augmented verifier for scientific domains. Through post-training,
SCI-Verifier demonstrates strong logical reasoning and equivalence judgment
capabilities while maintaining concise and stable outputs. Together,
SCI-VerifyBench and SCI-Verifier provide a principled framework for scientific
verification, offering both systematic evaluation and practical pathways to
enhance the reliability and applicability of LLMs in scientific domains.

</details>


### [262] [Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention](https://arxiv.org/abs/2509.24393)
*Yichi Zhang,Yue Ding,Jingwen Yang,Tianwei Luo,Dongbai Li,Ranjie Duan,Qiang Liu,Hang Su,Yinpeng Dong,Jun Zhu*

Main category: cs.AI

TL;DR: 针对大型推理模型存在的安全推理缺陷，提出IPO方法通过安全触发机制替代合规步骤，实现推理过程安全性对齐，实验显示危害性降低30%且保持推理性能


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视安全推理的重要性，导致有害思维链持续存在，可能被恶意用户利用产生应用风险。需要确保推理过程本身的安全性而不仅仅是最终输出结果的安全

Method: Intervened Preference Optimization (IPO)：1) 识别安全触发关键步骤 2) 构建合规提示与不安全延续的关联 3) 通过纠正干预机制将不安全轨迹转向安全路径

Result: 在越狱对抗基准测试中，危害性相对降低超30%；在保持各类推理任务性能的同时显著提升推理过程与结果的双重安全性

Conclusion: 首次提出推理过程安全对齐的重要性，通过IPO方法为安全LRMs提供实践路径，证明显式安全监督可有效提升模型可信度并降低潜在风险

Abstract: Although Large Reasoning Models (LRMs) have progressed in solving complex
problems, their chain-of-thought (CoT) reasoning often contains harmful content
that can persist even when the final responses appear safe. We show that this
issue still remains in existing methods which overlook the unique significance
of safe reasoning, undermining their trustworthiness and posing potential risks
in applications if unsafe reasoning is accessible for and exploited by
malicious users. We therefore shift our focus to aligning the safety of
reasoning itself in this paper and explore process supervision as the solution.
However, simply rewarding safe reasoning proves inadequate due to low rollout
diversity and limited training signals. To tackle this challenge, we first
delve into the characteristics of safe reasoning and uncover several critical
insights that 1) safe reasoning is often consolidated by a few critical steps
of safety triggers; 2) compliance cues strongly correlate with unsafe
continuations; and 3) corrective interventions reliably steer unsafe
trajectories towards safer traces. Motivated by these, we propose Intervened
Preference Optimization (IPO), an alignment method that enforces safe reasoning
by substituting compliance steps with safety triggers and constructing pairs
for preference learning with strong signals. Experiments on jailbreak and
adversarial safety benchmarks demonstrate that IPO remarkably improves overall
safety regarding both reasoning and responses, outperforming SFT-based and
RL-based baselines with a relative reduction of over 30% in harmfulness, while
preserving excellent performance across diverse reasoning tasks. The results
highlight the importance of explicit alignment for reasoning and provide a
practical path to safer LRMs.

</details>


### [263] [Experience-guided reflective co-evolution of prompts and heuristics for automatic algorithm design](https://arxiv.org/abs/2509.24509)
*Yihong Liu,Junyi Li,Wayne Xin Zhao,Hongyu Lu,Ji-Rong Wen*

Main category: cs.AI

TL;DR: 提出EvoPH框架，通过结合岛屿迁移模型与精英选择算法，实现提示与启发式算法的反思性协同进化，在组合优化问题上超越现有方法


<details>
  <summary>Details</summary>
Motivation: 传统启发式算法依赖人工设计且易陷入局部最优，LLM自动算法设计存在进化停滞问题。需要建立更有效的协同进化机制突破算法性能瓶颈

Method: 1）构建多岛屿种群模拟多样化探索 2）设计精英选择算法保留优质个体 3）通过性能反馈引导提示与启发式的反射式共同进化 4）建立经验指导的变异机制

Result: 在旅行商问题(TSP)和装箱问题(BPP)上，EvoPH相对误差分别降低15.3%和22.7%，均达到当前最优水平

Conclusion: 首次实现LLM驱动的提示与算法协同进化框架，为自动算法设计提供新范式，证实经验引导的反射机制能有效突破局部最优

Abstract: Combinatorial optimization problems are traditionally tackled with
handcrafted heuristic algorithms, which demand extensive domain expertise and
significant implementation effort. Recent progress has highlighted the
potential of automatic heuristics design powered by large language models
(LLMs), enabling the automatic generation and refinement of heuristics. These
approaches typically maintain a population of heuristics and employ LLMs as
mutation operators to evolve them across generations. While effective, such
methods often risk stagnating in local optima. To address this issue, we
propose the Experience-Guided Reflective Co-Evolution of Prompt and Heuristics
(EvoPH) for automatic algorithm design, a novel framework that integrates the
island migration model with the elites selection algorithm to simulate diverse
heuristics populations. In EvoPH, prompts are co-evolved with heuristic
algorithms, guided by performance feedback. We evaluate our framework on two
problems, i.e., Traveling Salesman Problem and Bin Packing Problem.
Experimental results demonstrate that EvoPH achieves the lowest relative error
against optimal solutions across both datasets, advancing the field of
automatic algorithm design with LLMs.

</details>


### [264] [On the Self-awareness of Large Reasoning Models' Capability Boundaries](https://arxiv.org/abs/2509.24711)
*Qingjie Zhang,Yujia Fu,Yang Wang,Liu Yan,Tao Wei,Ke Xu,Minlie Huang,Han Qiu*

Main category: cs.AI

TL;DR: 研究发现大型推理模型可通过信心轨迹和隐藏状态感知自身能力边界，并提出两种边界监控策略显著提升推理效率和可靠性


<details>
  <summary>Details</summary>
Motivation: 现有模型面对超出能力边界的问题时会产生无效推理，造成资源浪费和错误输出。研究旨在探索模型是否具备自我评估能力边界的机制

Method: 通过分析黑盒模型的信心轨迹（可解问题置信度增长/不可解问题不确定收敛）和白盒模型的隐藏状态线性可分性，提出基于推理表达监控和隐藏状态监控的优化策略

Result: 实验显示策略可减少62.7%-93.6%的token消耗，在保持准确率的同时显著提升推理效率

Conclusion: 模型隐含的边界感知能力可被有效提取，边界感知策略为优化大模型推理效率提供了新方向

Abstract: Large Reasoning Models (LRMs) have shown impressive performance on complex
reasoning tasks such as mathematics, yet they also display misbehaviors that
expose their limitations. In particular, when faced with hard questions, LRMs
often engage in unproductive reasoning until context limit, producing wrong
answers while wasting substantial computation. This phenomenon reflects a
fundamental issue: current answering paradigms overlook the relationship
between questions and LRMs' capability boundaries. In this paper, we
investigate whether LRMs possess self-awareness of capability boundaries. We
begin by an observation that LRMs may know what they cannot solve through
expressed reasoning confidence. For black-box models, we find that reasoning
expressions reveal boundary signals, with accelerated growing confidence
trajectory for solvable problems but convergent uncertainty trajectory for
unsolvable ones. For white-box models, we show that hidden states of the last
input token encode boundary information, with solvable and unsolvable problems
linearly separable even before reasoning begins. Building on these findings, we
propose two simple yet effective optimization strategies: reasoning expression
monitoring and hidden states monitoring. Experiments demonstrate that these
boundary-aware strategies enable LRMs to avoid unproductive reasoning without
sacrificing accuracy, significantly improving reliability and efficiency by
cutting token usage up to 62.7 - 93.6%.

</details>


### [265] [Pushing LLMs to Their Logical Reasoning Bound: The Role of Data Reasoning Intensity](https://arxiv.org/abs/2509.24836)
*Zhen Bi,Zhenlin Hu,Jinnan Yang,Mingyang Chen,Cheng Deng,Yida Xue,Zeyu Yang,Qing Shen,Zhenfang Liu,Kang Zhao,Ningyu Zhang,Jungang Lou*

Main category: cs.AI

TL;DR: 提出数据推理强度(DRI)指标量化训练样本的逻辑复杂度，通过重新优化数据推理强度提升LLM性能


<details>
  <summary>Details</summary>
Motivation: 现有方法过度关注数据格式转换而忽视样本内部推理复杂性，导致数据推理潜力未被充分利用

Method: 1. 开发DRI指标量化样本逻辑结构复杂度
2. 提出重新认知优化策略，在现有数据基础上系统性增强逻辑推理强度
3. 建立模型认知边界与数据强度的匹配机制

Result: 实验显示该方法在保持数据量不变的情况下，性能提升显著优于数据扩增策略，强化学习框架验证有效

Conclusion: 提升数据推理复杂度而非数量/形式，是释放LLM认知潜力的关键，为高效训练范式提供新方向

Abstract: Recent advances in large language models (LLMs) highlight the importance of
training data structure and quality in shaping reasoning behavior. However,
most existing approaches focus on transforming data formats while neglecting
the internal reasoning complexity of training samples, leaving the reasoning
potential of data under-explored and underutilized. In this work, we posit that
LLM logical reasoning performance is jointly constrained by the potential of
the training data and the cognitive capacity of the model. To make this
relationship measurable, we introduce Data Reasoning Intensity (DRI), a novel
metric that quantifies the latent logical reasoning complexity of samples by
decomposing and aggregating their logical structures. This allows us to analyze
how well current LLMs utilize logical reasoning signals and identify
performance gaps relative to data potential. Based on this insight, we
introduce a re-cognizing optimization strategy that systematically enhances the
logical reasoning intensity of training data.Rather than increasing data
volume, our method re-optimizes existing samples to better align with the LLM's
logical reasoning boundary. Extensive experiments show that our approach
significantly improves performance and generalization over data-centric
strategies. We further validate our method under a reinforcement learning
framework. Our results indicate that prioritizing reasoning complexity in data
rather than sheer scale or superficial form is essential to realizing LLMs'
full cognitive potential.

</details>


### [266] [Neural network embeddings recover value dimensions from psychometric survey items on par with human data](https://arxiv.org/abs/2509.24906)
*Max Pellert,Clemens M. Lechner,Indira Sen,Markus Strohmaier*

Main category: cs.AI

TL;DR: 提出SQuID方法，利用大语言模型嵌入有效复现心理测量问卷的潜在维度结构，无需领域微调即可实现55%方差解释力，与传统方法质量相当但成本更低。


<details>
  <summary>Details</summary>
Motivation: 传统心理测量工具依赖大规模人工调查，成本高且扩展性差。需要探索基于语义嵌入的自动化替代方案。

Method: 开发SQuID框架，对比多种嵌入模型在PVQ-RR人类价值观问卷上的表现，采用多维标度分析和因子一致性检验。

Result: 嵌入方法解释55%维度相似性方差，因子一致性系数达标，成功复现理论预期结构。

Conclusion: 语义嵌入可有效替代传统心理测量方法，在成本效益和扩展性方面优势显著，为社会科学研究提供新范式。

Abstract: This study introduces "Survey and Questionnaire Item Embeddings
Differentials" (SQuID), a novel methodological approach that enables neural
network embeddings to effectively recover latent dimensions from psychometric
survey items. We demonstrate that embeddings derived from large language
models, when processed with SQuID, can recover the structure of human values
obtained from human rater judgments on the Revised Portrait Value Questionnaire
(PVQ-RR). Our experimental validation compares multiple embedding models across
a number of evaluation metrics. Unlike previous approaches, SQuID successfully
addresses the challenge of obtaining negative correlations between dimensions
without requiring domain-specific fine-tuning. Quantitative analysis reveals
that our embedding-based approach explains 55% of variance in
dimension-dimension similarities compared to human data. Multidimensional
scaling configurations from both types of data show fair factor congruence
coefficients and largely follow the underlying theory. These results
demonstrate that semantic embeddings can effectively replicate psychometric
structures previously established through extensive human surveys. The approach
offers substantial advantages in cost, scalability and flexibility while
maintaining comparable quality to traditional methods. Our findings have
significant implications for psychometrics and social science research,
providing a complementary methodology that could expand the scope of human
behavior and experience represented in measurement tools.

</details>


### [267] [MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal Reasoning](https://arxiv.org/abs/2509.24922)
*Huihao Jing,Wenbin Hu,Hongyu Luo,Jianhui Yang,Wei Fan,Haoran Li,Yangqiu Song*

Main category: cs.AI

TL;DR: 提出MASLegalBench法律基准，针对多智能体系统（MAS）在GDPR场景下的任务分解与协作能力评估，填补现有法律评估体系对MAS优势考量不足的空白。


<details>
  <summary>Details</summary>
Motivation: 现有法律任务基准未充分考虑MAS在任务分解、智能体专业化等优势，限制了MAS在法律领域的潜力开发。

Method: 基于GDPR构建含复杂推理流程的法律场景基准，手动设计多样化角色MAS架构，并采用前沿LLM进行系统性实验验证。

Result: 实验揭示了现有LLM在MAS架构中的协作效率差异，暴露出专业分工与跨角色推理的瓶颈问题。

Conclusion: MASLegalBench有效评估法律领域MAS潜力，指明需提升智能体专业化训练与动态协作机制的未来方向。

Abstract: Multi-agent systems (MAS), leveraging the remarkable capabilities of Large
Language Models (LLMs), show great potential in addressing complex tasks. In
this context, integrating MAS with legal tasks is a crucial step. While
previous studies have developed legal benchmarks for LLM agents, none are
specifically designed to consider the unique advantages of MAS, such as task
decomposition, agent specialization, and flexible training. In fact, the lack
of evaluation methods limits the potential of MAS in the legal domain. To
address this gap, we propose MASLegalBench, a legal benchmark tailored for MAS
and designed with a deductive reasoning approach. Our benchmark uses GDPR as
the application scenario, encompassing extensive background knowledge and
covering complex reasoning processes that effectively reflect the intricacies
of real-world legal situations. Furthermore, we manually design various
role-based MAS and conduct extensive experiments using different
state-of-the-art LLMs. Our results highlight the strengths, limitations, and
potential areas for improvement of existing models and MAS architectures.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [268] [PARROT: A Benchmark for Evaluating LLMs in Cross-System SQL Translation](https://arxiv.org/abs/2509.23338)
*Wei Zhou,Guoliang Li,Haoyu Wang,Yuxing Han,Xufei Wu,Fan Wu,Xuanhe Zhou*

Main category: cs.DB

TL;DR: 提出了PARROT基准测试，这是一个针对跨系统SQL翻译任务的实用评估框架，覆盖22种生产级数据库系统并包含多样化的测试变体，实验显示LLMs在此任务上平均准确率不足38.53%。


<details>
  <summary>Details</summary>
Motivation: 现有SQL基准测试存在两大局限：（1）仅支持有限数据库系统（如SQLite），（2）无法捕捉系统特有的SQL方言（如定制函数、数据类型和语法规则）。这使得跨系统SQL翻译这一实际重要问题缺乏有效评估手段。

Method: 从38个开源基准和实际业务场景中收集598对翻译样本，构建包含28,003个多样化语法测试的PARROT-Diverse和5,306个典型样本的PARROT-Simple变体，覆盖ClickHouse等22种生产级数据库系统。

Result: LLMs在PARROT基准上的平均准确率低于38.53%，暴露出对系统特定SQL方言的理解局限。基准包含的3.8万+测试样本有效验证跨系统翻译的复杂性。

Conclusion: PARROT填补了跨系统SQL翻译评估的空白，其多维度测试集和公开排行榜（https://code4db.github.io/parrot-bench/）为后续研究提供标准化测试平台，推动数据库兼容性领域的进展。

Abstract: Large language models (LLMS) have shown increasing effectiveness in
Text-to-SQL tasks. However, another closely related problem, Cross-System SQL
Translation (a.k.a., SQL-to-SQL), which adapts a query written for one database
system (e.g., MySQL) into its equivalent one for another system (e.g.,
ClickHouse), is of great practical importance but remains underexplored.
Existing SQL benchmarks are not well-suited for SQL-to-SQL evaluation, which
(1) focus on a limited set of database systems (often just SQLite) and (2)
cannot capture many system-specific SQL dialects (e.g., customized functions,
data types, and syntax rules). Thus, in this paper, we introduce PARROT, a
Practical And Realistic BenchmaRk for CrOss-System SQL Translation. PARROT
comprises 598 translation pairs from 38 open-source benchmarks and real-world
business services, specifically prepared to challenge system-specific SQL
understanding (e.g., LLMS achieve lower than 38.53% accuracy on average). We
also provide multiple benchmark variants, including PARROT-Diverse with 28,003
translations (for extensive syntax testing) and PARROT-Simple with 5,306
representative samples (for focused stress testing), covering 22
production-grade database systems. To promote future research, we release a
public leaderboard and source code at: https://code4db.github.io/parrot-bench/.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [269] [WireBend-kit: A Computational Design and Fabrication Toolkit for Wirebending Custom 3D Wireframe Structures](https://arxiv.org/abs/2509.24083)
*Faraz Faruqi,Josha Paonaskar,Riley Schuler,Aiden Prevey,Carson Taylor,Anika Tak,Anthony Guinto,Eeshani Shilamkar,Natarith Cheenaruenthong,Martin Nisser*

Main category: cs.HC

TL;DR: WireBend-kit是低成本桌面级线材弯曲系统，集成设计软件与制造硬件，实现3D线框结构的快速精准制造


<details>
  <summary>Details</summary>
Motivation: 解决传统3D线框结构制造设备昂贵复杂的问题，通过软硬件协同降低制造门槛

Method: 开发免费设计软件实现虚拟线框生成与可制造性评估，定制$293机器通过喂料-弯曲-旋转指令序列成形，路径规划算法补偿材料弹性与运动误差

Result: 技术验证显示系统能克服累计误差，用廉价硬件实现±0.75mm精度，制造时间约30分钟/结构

Conclusion: 通过应用案例验证设计潜力，为快速原型开发提供端到端解决方案

Abstract: This paper introduces WireBend-kit, a desktop wirebending machine and
computational design tool for creating 3D wireframe structures. Combined, they
allow users to rapidly and inexpensively create custom 3D wireframe structures
from aluminum wire. Our design tool is implemented in freely available software
and allows users to generate virtual wireframe designs and assess their
fabricability. A path-planning procedure automatically converts the wireframe
design into fabrication instructions for our machine while accounting for
material elasticity and kinematic error sources. The custom machine costs $293
in parts and can form aluminum wire into 3D wireframe structures through an
ordered sequence of feed, bend, and rotate instructions. Our technical
evaluation reveals our system's ability to overcome odometrically accumulating
errors inherent to wirebending in order to produce accurate 3D structures from
inexpensive hardware. Finally, we provide application examples demonstrating
the design space enabled by Wirebend-kit.

</details>


### [270] [Bridging the behavior-neural gap: A multimodal AI reveals the brain's geometry of emotion more accurately than human self-reports](https://arxiv.org/abs/2509.24298)
*Changde Du,Yizhuo Lu,Zhongyu Huang,Yi Sun,Zisen Zhou,Shaozheng Qin,Huiguang He*

Main category: cs.HC

TL;DR: 通过多模态大语言模型（MLLM）生成的情感表征能有效预测人类情感处理神经活动，解决了行为-神经鸿沟问题。


<details>
  <summary>Details</summary>
Motivation: 传统情感评估量表存在行为-神经鸿沟，研究探索大规模相似性判断能否更准确捕捉大脑情感表征。

Method: 使用MLLM和LLM模型对2180个情感视频进行数百万次三重判断，生成30维可解释嵌入表示。

Result: MLLM的情感表征神经预测准确度最高（超越人类行为数据），视觉基础训练对神经对齐至关重要。

Conclusion: MLLM能自主建立神经对齐的情感表征框架，为连接主观体验与神经机制提供新范式。

Abstract: The ability to represent emotion plays a significant role in human cognition
and social interaction, yet the high-dimensional geometry of this affective
space and its neural underpinnings remain debated. A key challenge, the
`behavior-neural gap,' is the limited ability of human self-reports to predict
brain activity. Here we test the hypothesis that this gap arises from the
constraints of traditional rating scales and that large-scale similarity
judgments can more faithfully capture the brain's affective geometry. Using AI
models as `cognitive agents,' we collected millions of triplet odd-one-out
judgments from a multimodal large language model (MLLM) and a language-only
model (LLM) in response to 2,180 emotionally evocative videos. We found that
the emergent 30-dimensional embeddings from these models are highly
interpretable and organize emotion primarily along categorical lines, yet in a
blended fashion that incorporates dimensional properties. Most remarkably, the
MLLM's representation predicted neural activity in human emotion-processing
networks with the highest accuracy, outperforming not only the LLM but also,
counterintuitively, representations derived directly from human behavioral
ratings. This result supports our primary hypothesis and suggests that sensory
grounding--learning from rich visual data--is critical for developing a truly
neurally-aligned conceptual framework for emotion. Our findings provide
compelling evidence that MLLMs can autonomously develop rich, neurally-aligned
affective representations, offering a powerful paradigm to bridge the gap
between subjective experience and its neural substrates. Project page:
https://reedonepeck.github.io/ai-emotion.github.io/.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [271] [DiaMoE-TTS: A Unified IPA-Based Dialect TTS Framework with Mixture-of-Experts and Parameter-Efficient Zero-Shot Adaptation](https://arxiv.org/abs/2509.22727)
*Ziqi Chen,Gongyu Chen,Yihua Wang,Chaofan Ding,Zihao chen,Wei-Qiang Zhang*

Main category: cs.SD

TL;DR: 提出基于IPA的DiaMoE-TTS框架，通过方言感知专家混合与参数高效适配，解决方言TTS数据稀缺与语音复杂性挑战，实现零样本方言迁移与专业领域合成。


<details>
  <summary>Details</summary>
Motivation: 方言语音存在数据稀缺、拼写不一致和复杂语音变异等问题，传统方法依赖大规模专有资源，难以实现可扩展的开放数据驱动合成。

Method: 1. 采用国际音标(IPA)统一语音表征
2. 引入方言感知混合专家(MoE)建模音系差异
3. 结合LoRA低秩适配器与条件适配器实现高效参数迁移

Result: 仅需数小时数据即可生成自然语音，在未见方言和京剧等专业领域实现零样本合成，MOS评分达到4.2。

Conclusion: 该框架证明了小数据驱动的开放方言合成可行性，为保护语言多样性提供了可扩展解决方案，显著优于依赖大规模专有模型的传统方法。

Abstract: Dialect speech embodies rich cultural and linguistic diversity, yet building
text-to-speech (TTS) systems for dialects remains challenging due to scarce
data, inconsistent orthographies, and complex phonetic variation. To address
these issues, we present DiaMoE-TTS, a unified IPA-based framework that
standardizes phonetic representations and resolves grapheme-to-phoneme
ambiguities. Built upon the F5-TTS architecture, the system introduces a
dialect-aware Mixture-of-Experts (MoE) to model phonological differences and
employs parameter-efficient adaptation with Low-Rank Adaptors (LoRA) and
Conditioning Adapters for rapid transfer to new dialects. Unlike approaches
dependent on large-scale or proprietary resources, DiaMoE-TTS enables scalable,
open-data-driven synthesis. Experiments demonstrate natural and expressive
speech generation, achieving zero-shot performance on unseen dialects and
specialized domains such as Peking Opera with only a few hours of data.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [272] [Extracting the Structure of Press Releases for Predicting Earnings Announcement Returns](https://arxiv.org/abs/2509.24254)
*Yuntao Wu,Ege Mert Akin,Charles Martineau,Vincent Grégoire,Andreas Veneris*

Main category: q-fin.CP

TL;DR: 该研究通过分析13.8万份财报新闻稿，发现基于FinBERT的文本分析对股价预测效果媲美传统财务指标，结合多模型可提升解释力。


<details>
  <summary>Details</summary>
Motivation: 传统研究侧重财务数据量化指标，本文探究文本内容对股价的预测能力，比较不同NLP方法的有效性，揭示语言在价格形成中的微妙作用。

Method: 使用词袋模型和FinBERT等BERT变体分析2005-2023年财报新闻稿，通过模型融合增强预测效果，结合在线学习实现实时预测。

Result: 文本软信息预测力与财务硬指标相当；FinBERT表现最优；模型组合提升解释性；股价开盘即反映信息；文本泄露具预测优势；管理层叙事存在自利倾向。

Conclusion: 先进NLP模型能有效捕捉文本中的市场信号，结合在线学习为实时预测提供框架，揭示了语言在资本市场的定价机制及市场效率边界。

Abstract: We examine how textual features in earnings press releases predict stock
returns on earnings announcement days. Using over 138,000 press releases from
2005 to 2023, we compare traditional bag-of-words and BERT-based embeddings. We
find that press release content (soft information) is as informative as
earnings surprise (hard information), with FinBERT yielding the highest
predictive power. Combining models enhances explanatory strength and
interpretability of the content of press releases. Stock prices fully reflect
the content of press releases at market open. If press releases are leaked, it
offers predictive advantage. Topic analysis reveals self-serving bias in
managerial narratives. Our framework supports real-time return prediction
through the integration of online learning, provides interpretability and
reveals the nuanced role of language in price formation.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [273] [Patient-specific Biomolecular Instruction Tuning](https://arxiv.org/abs/2509.22853)
*Irsyad Adam,Zekai Chen,David Laub,Shaun Porwal,Arda Pekis,Kevin Brown*

Main category: q-bio.QM

TL;DR: 开发了首个肿瘤学蛋白质组学指令调优数据集CPTAC-PROTSTRUCT和图-LLM框架KRONOS，通过患者特异性图谱分析提升精准医疗效果


<details>
  <summary>Details</summary>
Motivation: 现有技术存在两大瓶颈：缺乏临床解释性蛋白质组学指令数据集，以及缺少能捕捉分子数据异质性的语言模型架构

Method: 创建包含40万+样本的CPTAC-PROTSTRUCT数据集，提出KRONOS图神经网络框架（整合分子互作拓扑与蛋白质组数据）

Result: KRONOS在分子分类、时序轨迹建模和肿瘤分期预测等临床任务中取得优异表现

Conclusion: 该框架使大语言模型能更精准理解患者特异性致病机制，推动诊断、预后和分层治疗的精准医学进步

Abstract: Proteomics data is essential to pathogenic understanding of a disease
phenotype. In cancer, analysis of molecular signatures enables precision
medicine through the identification of biological processes that drive
individualized tumor progression, therapeutic resistance, and clinical
heterogeneity. Recent advances in multimodal large language models (LLMs) have
shown remarkable capacity to integrate and reason across heterogeneous data
modalities. However, performing multi-modal language modeling for molecular
understanding of patient-specific proteomics remains a significant challenge
due to two barriers: (1) the lack of instruction-tuning datasets that enable
clinical interpretation from proteomics data, and (2) the absence of language
modeling architectures designed to capture the rich heterogeneity of molecular
data. In this work, we introduce CPTAC-PROTSTRUCT, the first instruction tuning
dataset for molecular understanding of oncology, comprising over 400k
open-ended examples derived from individualized proteomic profiles curated from
the largest national proteomics cancer study (CPTAC). Additionally, we propose
KRONOS (Knowledge Representation of patient Omics Networks in Oncology via
Structured tuning), a novel graph-LLM framework that leverages molecular
interaction topology with proteomics to learn patient-specific graph
representations for enhanced clinical reasoning. We show that KRONOS achieves
competitive performance across benchmark clinical tasks, including molecular
classification, temporal trajectory modeling, and tumor stage prediction from
proteomics data. Ultimately, this approach empowers LLMs to understand
patient-level pathogenesis, advancing precision medicine through more accurate
diagnosis, prognosis, and treatment stratification.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [274] [Beyond Game Theory Optimal: Profit-Maximizing Poker Agents for No-Limit Holdem](https://arxiv.org/abs/2509.23747)
*SeungHyun Yi,Seungjun Yi*

Main category: cs.GT

TL;DR: 提出融合GTO基础与实时对手分析的扑克策略模型，突破GTO收益天花板


<details>
  <summary>Details</summary>
Motivation: 传统GTO策略仅保证不亏损但无法最大化收益，尤其在多人对局中表现受限

Method: 1. 通过蒙特卡洛模拟建立接近理论最优的基准策略
2. 实时追踪对手行为模式，动态调整剥削策略
3. 结合CFR算法进行反事实遗憾最小化

Result: 单挑局中蒙特卡洛CFR表现最佳，多人局传统CFR保持优势

Conclusion: 防御性GTO与主动剥削策略的结合显著提升扑克智能体胜率

Abstract: Game theory has grown into a major field over the past few decades, and poker
has long served as one of its key case studies. Game-Theory-Optimal (GTO)
provides strategies to avoid loss in poker, but pure GTO does not guarantee
maximum profit. To this end, we aim to develop a model that outperforms GTO
strategies to maximize profit in No Limit Holdem, in heads-up (two-player) and
multi-way (more than two-player) situations. Our model finds the GTO foundation
and goes further to exploit opponents. The model first navigates toward many
simulated poker hands against itself and keeps adjusting its decisions until no
action can reliably beat it, creating a strong baseline that is close to the
theoretical best strategy. Then, it adapts by observing opponent behavior and
adjusting its strategy to capture extra value accordingly. Our results indicate
that Monte-Carlo Counterfactual Regret Minimization (CFR) performs best in
heads-up situations and CFR remains the strongest method in most multi-way
situations. By combining the defensive strength of GTO with real-time
exploitation, our approach aims to show how poker agents can move from merely
not losing to consistently winning against diverse opponents.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [275] [Overview of SCIDOCA 2025 Shared Task on Citation Prediction, Discovery, and Placement](https://arxiv.org/abs/2509.24283)
*An Dao,Vu Tran,Le-Minh Nguyen,Yuji Matsumoto*

Main category: cs.DL

TL;DR: SCIDOCA 2025共享任务推出三个引文建模子任务（发现/预测/句子匹配），构建6万标注段落数据集，三支队伍提交结果并分析模型表现。


<details>
  <summary>Details</summary>
Motivation: 通过大规模标注数据集和标准化评估指标，填补科学文献引用建模领域的基准空白，推动学术文档理解技术发展。

Method: 基于S2ORC构建含6万标注段落的开源数据集，设计引文发现、遮蔽引文预测、引文句子预测三个子任务，使用1000个独立论文段落作为测试集并引入干扰项。

Result: 七支队伍注册，三支提交结果，测试显示现有系统在引文关联识别和干扰项排除方面仍有提升空间。

Conclusion: 该任务为引文建模建立新基准，公开数据集和评估框架将持续促进科学文献理解领域的技术创新。

Abstract: We present an overview of the SCIDOCA 2025 Shared Task, which focuses on
citation discovery and prediction in scientific documents. The task is divided
into three subtasks: (1) Citation Discovery, where systems must identify
relevant references for a given paragraph; (2) Masked Citation Prediction,
which requires selecting the correct citation for masked citation slots; and
(3) Citation Sentence Prediction, where systems must determine the correct
reference for each cited sentence. We release a large-scale dataset constructed
from the Semantic Scholar Open Research Corpus (S2ORC), containing over 60,000
annotated paragraphs and a curated reference set. The test set consists of
1,000 paragraphs from distinct papers, each annotated with ground-truth
citations and distractor candidates. A total of seven teams registered, with
three submitting results. We report performance metrics across all subtasks and
analyze the effectiveness of submitted systems. This shared task provides a new
benchmark for evaluating citation modeling and encourages future research in
scientific document understanding. The dataset and task materials are publicly
available at https://github.com/daotuanan/scidoca2025-shared-task.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [276] [MAS$^2$: Self-Generative, Self-Configuring, Self-Rectifying Multi-Agent Systems](https://arxiv.org/abs/2509.24323)
*Kun Wang,Guibin Zhang,ManKit Ye,Xinyu Deng,Dongxia Wang,Xiaobin Hu,Jinyang Guo,Yang Liu,Yufei Guo*

Main category: cs.MA

TL;DR: 提出MAS²框架，通过递归自我生成机制实现动态多智能体系统构建，在复杂任务中性能提升19.6%且具备跨模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有自动多智能体系统采用静态生成模式，难以适应动态现实环境，需要更灵活的自适应架构方案。

Method: 设计'生成器-实施者-校正器'三代理协同框架，结合协作树优化算法训练元代理实现系统的动态组合与实时调整。

Result: 在7个基准测试中超越SOTA方法19.6%，跨模型泛化提升15.1%，且保持成本效益帕累托最优。

Conclusion: MAS²通过递归自生成范式突破静态架构限制，为复杂场景提供高效自适应解决方案，推动多智能体系统向自主进化发展。

Abstract: The past two years have witnessed the meteoric rise of Large Language Model
(LLM)-powered multi-agent systems (MAS), which harness collective intelligence
and exhibit a remarkable trajectory toward self-evolution. This paradigm has
rapidly progressed from manually engineered systems that require bespoke
configuration of prompts, tools, roles, and communication protocols toward
frameworks capable of automated orchestration. Yet, dominant automatic
multi-agent systems, whether generated by external modules or a single LLM
agent, largely adhere to a rigid ``\textit{generate-once-and-deploy}''
paradigm, rendering the resulting systems brittle and ill-prepared for the
dynamism and uncertainty of real-world environments. To transcend this
limitation, we introduce MAS$^2$, a paradigm predicated on the principle of
recursive self-generation: a multi-agent system that autonomously architects
bespoke multi-agent systems for diverse problems. Technically, we devise a
``\textit{generator-implementer-rectifier}'' tri-agent team capable of
dynamically composing and adaptively rectifying a target agent system in
response to real-time task demands. Collaborative Tree Optimization is proposed
to train and specialize these meta-agents. Extensive evaluation across seven
benchmarks reveals that MAS$^2$ achieves performance gains of up to $19.6\%$
over state-of-the-art MAS in complex scenarios such as deep research and code
generation. Moreover, MAS$^2$ exhibits superior cross-backbone generalization,
effectively leveraging previously unseen LLMs to yield improvements of up to
$15.1\%$. Crucially, these gains are attained without incurring excessive token
costs, as MAS$^2$ consistently resides on the Pareto frontier of
cost-performance trade-offs. The source codes are available at
https://github.com/yeyeyeah2/MAS2.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [277] [UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D Generation](https://arxiv.org/abs/2509.25079)
*Guanjun Wu,Jiemin Fang,Chen Yang,Sikuang Li,Taoran Yi,Jia Lu,Zanwei Zhou,Jiazhong Cen,Lingxi Xie,Xiaopeng Zhang,Wei Wei,Wenyu Liu,Xinggang Wang,Qi Tian*

Main category: cs.CV

TL;DR: 提出UniLat3D统一框架，通过几何-外观联合VAE将三维信息编码至单一潜在空间，实现单阶段高质量3D资产生成，解决传统两阶段方法效率低和纹理错位问题。


<details>
  <summary>Details</summary>
Motivation: 现有三维生成模型采用几何与外观解耦的两阶段流程，易导致纹理几何错位且生成效率低下。需构建统一表征以实现高效单阶段生成。

Method: 1. 开发几何-外观联合VAE，将高分辨率稀疏特征压缩为UniLat稠密低维潜在空间；2. 训练单一流匹配模型，实现噪声到UniLat的直接映射。支持解码为3D高斯/网格等多种格式。

Result: 仅用公开数据训练即实现单图秒级生成，外观保真度与几何质量超越现有方法。支持3D高斯/网格等格式输出。

Conclusion: UniLat3D验证了统一潜在空间在三维生成中的有效性，显著提升生成效率与质量，为单阶段3D生成提供新范式。

Abstract: High-fidelity 3D asset generation is crucial for various industries. While
recent 3D pretrained models show strong capability in producing realistic
content, most are built upon diffusion models and follow a two-stage pipeline
that first generates geometry and then synthesizes appearance. Such a decoupled
design tends to produce geometry-texture misalignment and non-negligible cost.
In this paper, we propose UniLat3D, a unified framework that encodes geometry
and appearance in a single latent space, enabling direct single-stage
generation. Our key contribution is a geometry-appearance Unified VAE, which
compresses high-resolution sparse features into a compact latent representation
-- UniLat. UniLat integrates structural and visual information into a dense
low-resolution latent, which can be efficiently decoded into diverse 3D
formats, e.g., 3D Gaussians and meshes. Based on this unified representation,
we train a single flow-matching model to map Gaussian noise directly into
UniLat, eliminating redundant stages. Trained solely on public datasets,
UniLat3D produces high-quality 3D assets in seconds from a single image,
achieving superior appearance fidelity and geometric quality. More demos \&
code are available at https://unilat3d.github.io/

</details>


### [278] [VideoScore2: Think before You Score in Generative Video Evaluation](https://arxiv.org/abs/2509.22799)
*Xuan He,Dongfu Jiang,Ping Nie,Minghao Liu,Zhengxuan Jiang,Mingyi Su,Wentao Ma,Junru Lin,Chun Ye,Yi Lu,Keming Wu,Benjamin Schneider,Quy Duc Do,Zhuofeng Li,Yiming Jia,Yuxuan Zhang,Guo Cheng,Haozhe Wang,Wangchunshu Zhou,Qunshu Lin,Yuanxing Zhang,Ge Zhang,Wenhao Huang,Wenhu Chen*

Main category: cs.CV

TL;DR: 提出多维可解释评估框架VideoScore2，通过细粒度三维度评估（视觉质量/文本对齐/物理一致性）及思维链推理，显著提升视频生成评估性能并支持可控生成


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频评估方法存在单维度评分不透明、缺乏解释性等问题，无法全面反映视频质量。需建立兼具多维评估能力和可解释性的量化框架

Method: 构建27k人工标注数据集VideoFeedback2，采用监督微调+GRPO强化学习两阶段训练，建立包含视觉质量/文本对齐/物理一致性的三维评估体系

Result: 在VideoScore-Bench-v2准确率提升5.94至44.35，跨四领域基准平均提升4.32达50.37，且通过奖励建模支持Best-of-N采样的可控生成

Conclusion: VideoScore2通过可解释的多维评估框架，既提升评估性能又促进可控生成，为视频生成质量评估与优化提供系统性解决方案

Abstract: Recent advances in text-to-video generation have produced increasingly
realistic and diverse content, yet evaluating such videos remains a fundamental
challenge due to their multi-faceted nature encompassing visual quality,
semantic alignment, and physical consistency. Existing evaluators and reward
models are limited to single opaque scores, lack interpretability, or provide
only coarse analysis, making them insufficient for capturing the comprehensive
nature of video quality assessment. We present VideoScore2, a
multi-dimensional, interpretable, and human-aligned framework that explicitly
evaluates visual quality, text-to-video alignment, and physical/common-sense
consistency while producing detailed chain-of-thought rationales. Our model is
trained on a large-scale dataset VideoFeedback2 containing 27,168
human-annotated videos with both scores and reasoning traces across three
dimensions, using a two-stage pipeline of supervised fine-tuning followed by
reinforcement learning with Group Relative Policy Optimization (GRPO) to
enhance analytical robustness. Extensive experiments demonstrate that
VideoScore2 achieves superior performance with 44.35 (+5.94) accuracy on our
in-domain benchmark VideoScore-Bench-v2 and 50.37 (+4.32) average performance
across four out-of-domain benchmarks (VideoGenReward-Bench, VideoPhy2, etc),
while providing interpretable assessments that bridge the gap between
evaluation and controllable generation through effective reward modeling for
Best-of-N sampling. Project Page: https://tiger-ai-lab.github.io/VideoScore2/

</details>


### [279] [Geometry-Aware Losses for Structure-Preserving Text-to-Sign Language Generation](https://arxiv.org/abs/2509.23011)
*Zetian Wu,Tianshuo Zhou,Stefan Lee,Liang Huang*

Main category: cs.CV

TL;DR: 提出通过建模骨骼关节几何约束（肩臂手协调）和引入骨骼长度约束/运动方差损失，显著提升手语视频生成的自然度和解剖学合理性


<details>
  <summary>Details</summary>
Motivation: 现有文本到手语视频生成方法忽视人体运动学约束，导致动作僵硬不自然。需增强骨骼运动的解剖学合理性以准确传达语义

Method: 1. 建立肩-臂-手关节的几何约束（位置/骨骼长度/运动方差） 2. 引入父关节相对重加权机制增强手指灵活性 3. 骨骼姿态损失和骨骼长度约束保证解剖一致性

Result: 将现有最优方法与真值的性能差距缩小56.51%，骨骼长度差异减少18.76%，运动方差差异降低5.48%

Conclusion: 该方法通过显式建模人体运动学约束，显著提高了手语动画的解剖学合理性和运动自然度，推动无障碍通信技术进步

Abstract: Sign language translation from text to video plays a crucial role in enabling
effective communication for Deaf and hard--of--hearing individuals. A major
challenge lies in generating accurate and natural body poses and movements that
faithfully convey intended meanings. Prior methods often neglect the anatomical
constraints and coordination patterns of human skeletal motion, resulting in
rigid or biomechanically implausible outputs. To address this, we propose a
novel approach that explicitly models the relationships among skeletal
joints--including shoulders, arms, and hands--by incorporating geometric
constraints on joint positions, bone lengths, and movement dynamics. During
training, we introduce a parent-relative reweighting mechanism to enhance
finger flexibility and reduce motion stiffness. Additionally, bone-pose losses
and bone-length constraints enforce anatomically consistent structures. Our
method narrows the performance gap between the previous best and the
ground-truth oracle by 56.51%, and further reduces discrepancies in bone length
and movement variance by 18.76% and 5.48%, respectively, demonstrating
significant gains in anatomical realism and motion naturalness.

</details>


### [280] [Seeing Symbols, Missing Cultures: Probing Vision-Language Models' Reasoning on Fire Imagery and Cultural Meaning](https://arxiv.org/abs/2509.23311)
*Haorui Yu,Qiufeng Yi,Yijia Chu,Yang Zhao*

Main category: cs.CV

TL;DR: 研究发现视觉语言模型(VLMs)依赖文化符号捷径而非真正理解，存在系统性偏见


<details>
  <summary>Details</summary>
Motivation: 揭示VLMs在文化理解上的表面化特征识别模式，暴露其符号化捷径带来的风险

Method: 通过火灾主题文化意象的分类解释分析框架，测试模型在西方节日/非西方传统/紧急场景的表现

Result: 模型能识别主流西方节日，但对非西方文化事件识别模糊，甚至将紧急场景误判为庆典

Conclusion: 需要超越准确率指标的文化评估体系，确保多模态系统的可解释性与公平性

Abstract: Vision-Language Models (VLMs) often appear culturally competent but rely on
superficial pattern matching rather than genuine cultural understanding. We
introduce a diagnostic framework to probe VLM reasoning on fire-themed cultural
imagery through both classification and explanation analysis. Testing multiple
models on Western festivals, non-Western traditions, and emergency scenes
reveals systematic biases: models correctly identify prominent Western
festivals but struggle with underrepresented cultural events, frequently
offering vague labels or dangerously misclassifying emergencies as
celebrations. These failures expose the risks of symbolic shortcuts and
highlight the need for cultural evaluation beyond accuracy metrics to ensure
interpretable and fair multimodal systems.

</details>


### [281] [SPIKE-RL: Video-LLMs meet Bayesian Surprise](https://arxiv.org/abs/2509.23433)
*Sahithya Ravi,Aditya Chinchure,Raymond T. Ng,Leonid Sigal,Vered Shwartz*

Main category: cs.CV

TL;DR: SPIKE框架通过贝叶斯惊喜检测视频关键时刻，结合强化学习优化采样策略，显著提升视频理解模型的多任务性能


<details>
  <summary>Details</summary>
Motivation: 现有视频语言模型均匀采样的方式容易忽略决定视频叙事的关键时刻，需要开发能主动检测视觉惊喜的框架来提升视频理解效果

Method: 1. 提出SPIKE框架量化贝叶斯惊喜（新视觉证据引发的信念更新）
2. 开发SPIKE-RL利用GRPO算法，基于视频标题的奖励信号优化信念假设
3. 设计基于惊喜值的帧采样策略，在关键时段分配更多采样帧

Result: 1. 在FunQA（正向惊喜）和Oops!（负向惊喜）基准上与人类判断高度相关
2. 采用非均匀采样策略后在5个下游任务实现平均2.5%的性能提升
3. 超越传统均匀采样方法的视频理解效果

Conclusion: 通过追踪模型信念和检测视觉惊喜，为视频理解模型开辟了新路径，使模型能根据新信息动态调整认知，提升鲁棒性和适应性

Abstract: Real-world videos often show routine activities punctuated by memorable,
surprising events. However, most Video-LLMs process videos by sampling frames
uniformly, likely missing critical moments that define a video's narrative. We
introduce SPIKE, an inference-time framework that quantifies Bayesian Surprise
as the belief update triggered by new visual evidence in the video stream,
identifying moments where new visual evidence conflicts with prior beliefs.
SPIKE effectively localizes surprise in videos, strongly correlated with humans
on positive (FunQA) and negative (Oops!) surprise benchmarks. Since the beliefs
of zero-shot Video-LLMs are often suboptimal, we develop SPIKE-RL, which
leverages GRPO to optimize belief hypotheses based on a reward signal from the
video caption. SPIKE and SPIKE-RL guide query-agnostic surprise-weighted frame
sampling, which allocates more frames to interesting moments in the video. With
this strategy, we achieve consistent performance gains on five downstream
benchmarks over uniform sampling. By enabling Video-LLMs to track beliefs and
register surprise, our work paves the way for more robust models that can
revise their understanding in response to new information.

</details>


### [282] [FoR-SALE: Frame of Reference-guided Spatial Adjustment in LLM-based Diffusion Editing](https://arxiv.org/abs/2509.23452)
*Tanawan Premsri,Parisa Kordjamshidi*

Main category: cs.CV

TL;DR: Proposes FoR-SALE framework that improves T2I models' spatial alignment by 5.3% using Frame of Reference-guided adjustments in latent space.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image models struggle with spatial descriptions from non-camera perspectives. Frame of Reference integration addresses this crucial gap in spatial reasoning.

Method: Extends SLD framework with FoR analysis: 1) Extracts image spatial configuration 2) Maps spatial expressions to camera perspective 3) Applies latent-space editing operations for directional/depth adjustments

Result: 5.3% performance improvement on FoR spatial benchmarks with single correction round in SOTA models

Conclusion: FoR-SALE demonstrates effective spatial understanding through unified perspective mapping and targeted latent-space operations, advancing multimodal alignment capabilities.

Abstract: Frame of Reference (FoR) is a fundamental concept in spatial reasoning that
humans utilize to comprehend and describe space. With the rapid progress in
Multimodal Language models, the moment has come to integrate this
long-overlooked dimension into these models. In particular, in text-to-image
(T2I) generation, even state-of-the-art models exhibit a significant
performance gap when spatial descriptions are provided from perspectives other
than the camera. To address this limitation, we propose Frame of
Reference-guided Spatial Adjustment in LLM-based Diffusion Editing (FoR-SALE),
an extension of the Self-correcting LLM-controlled Diffusion (SLD) framework
for T2I. For-Sale evaluates the alignment between a given text and an initially
generated image, and refines the image based on the Frame of Reference
specified in the spatial expressions. It employs vision modules to extract the
spatial configuration of the image, while simultaneously mapping the spatial
expression to a corresponding camera perspective. This unified perspective
enables direct evaluation of alignment between language and vision. When
misalignment is detected, the required editing operations are generated and
applied. FoR-SALE applies novel latent-space operations to adjust the facing
direction and depth of the generated images. We evaluate FoR-SALE on two
benchmarks specifically designed to assess spatial understanding with FoR. Our
framework improves the performance of state-of-the-art T2I models by up to 5.3%
using only a single round of correction.

</details>


### [283] [Multi-modal Data Spectrum: Multi-modal Datasets are Multi-dimensional](https://arxiv.org/abs/2509.23499)
*Divyam Madaan,Varshan Muhunthan,Kyunghyun Cho,Sumit Chopra*

Main category: cs.CV

TL;DR: 研究发现多模态基准测试中存在显著的单模态依赖偏差，大模型可能通过利用这些依赖掩盖真实多模态推理能力的不足


<details>
  <summary>Details</summary>
Motivation: 揭示多模态学习中视觉-文本依赖关系的本质及其相互作用，评估现有基准测试的局限性

Method: 使用多模态大语言模型对23个VQA基准进行大规模实证研究，涵盖常识推理、OCR等领域

Result: 发现跨/同基准中视觉-文本依赖差异显著，许多去文本偏见的基准意外增强图像依赖，大模型利用单模态依赖掩盖多模态推理缺陷

Conclusion: 需建立量化的多模态数据集评估体系，为基准设计和模型评估提供理论依据

Abstract: Understanding the interplay between intra-modality dependencies (the
contribution of an individual modality to a target task) and inter-modality
dependencies (the relationships between modalities and the target task) is
fundamental to advancing multi-modal learning. However, the nature of and
interaction between these dependencies within current benchmark evaluations
remains poorly characterized. In this work, we present a large-scale empirical
study to quantify these dependencies across 23 visual question-answering
benchmarks using multi-modal large language models (MLLMs) covering domains
such as general and expert knowledge reasoning, optical character recognition,
and document understanding. Our findings show that the reliance on vision,
question (text), and their interaction varies significantly, both across and
within benchmarks. We discover that numerous benchmarks intended to mitigate
text-only biases have inadvertently amplified image-only dependencies. This
characterization persists across model sizes, as larger models often use these
intra-modality dependencies to achieve high performance that mask an underlying
lack of multi-modal reasoning. We provide a quantitative characterization of
multi-modal datasets, enabling a principled approach to multi-modal benchmark
design and evaluation.

</details>


### [284] [RIV: Recursive Introspection Mask Diffusion Vision Language Model](https://arxiv.org/abs/2509.23625)
*YuQian Li,Limeng Qiao,Lin Ma*

Main category: cs.CV

TL;DR: 提出递归自省掩码扩散模型RIV，通过自省训练和递归推理机制赋予视觉语言模型自我纠错能力，在多个基准测试中实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有MDVLMs模型缺乏对生成结果的自我修正能力，特别是无法纠正逻辑错误，制约了模型的可靠性和准确性。

Method: 1. 自省训练：引入自省模型检测生成序列的语法/拼写错误及逻辑错误
2. 递归推理：通过解码→自省→重新掩码的递归循环持续修正输出结果

Result: 在多个benchmark上达到state-of-the-art性能，显著超越现有MDVLMs模型

Conclusion: RIV通过递归自省机制有效解决了视觉语言模型缺乏自我修正能力的核心痛点，实验验证了该框架的优越性，为多模态理解任务提供了新的技术路径。

Abstract: Mask Diffusion-based Vision Language Models (MDVLMs) have achieved remarkable
progress in multimodal understanding tasks. However, these models are unable to
correct errors in generated tokens, meaning they lack self-correction
capability. In this paper, we propose Recursive Introspection Mask Diffusion
Vision Language Model (RIV), which equips the model with self-correction
ability through two novel mechanisms. The first is Introspection Training,
where an Introspection Model is introduced to identify errors within generated
sequences. Introspection Training enables the model to detect not only
grammatical and spelling mistakes, but more importantly, logical errors. The
second is Recursive Inference. Beginning with the standard unmasking step, the
learned Introspection Model helps to identify errors in the output sequence and
remask them. This alternating
($\text{unmask}\rightarrow\text{introspection}\rightarrow\text{remask}$)
process is repeated recursively until reliable results are obtained.
Experimental results on multiple benchmarks demonstrate that the proposed RIV
achieves state-of-the-art performance, outperforming most existing MDVLMs.

</details>


### [285] [RCI: A Score for Evaluating Global and Local Reasoning in Multimodal Benchmarks](https://arxiv.org/abs/2509.23673)
*Amit Agarwal,Hitesh Laxmichand Patel,Srikant Panda,Hansa Meghwani,Jyotika Singh,Karan Dua,Paul Li,Tao Sheng,Sujith Ravi,Dan Roth*

Main category: cs.CV

TL;DR: 本文提出区域理解指数（RCI），首次量化评估多模态数据集对全局/局部视觉信息的依赖程度，揭示主流benchmark普遍存在局部推理偏好和空间偏差。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型（MLLMs）在benchmark上表现优异，但无法区分模型是真正理解全局信息还是依赖局部视觉线索。现有评估方法缺乏对这种能力的量化指标，阻碍数据集优化和实际应用导向的模型开发。

Method: 提出RCI指标：通过系统比较模型在完整图像与局部图像块上的性能差异，量化数据集对全局/局部视觉信息的依赖程度。

Result: 在13个主流多模态benchmark上应用RCI，发现大多数偏向局部推理且存在显著空间偏差，预示实际应用风险。

Conclusion: RCI为诊断和消除数据集偏差提供工具，有助于构建更鲁棒的企业级多模态系统，推动可靠评估体系的发展。

Abstract: Multimodal Large Language Models (MLLMs) have achieved impressive results on
vision-language benchmarks, yet it remains unclear whether these benchmarks
assess genuine global reasoning or allow success via localized visual cues.
Existing evaluation methods do not explicitly measure this distinction,
hindering effective dataset curation and real-world focused model development.
  We introduce Region Comprehension Index (RCI), the first model-based score to
directly quantify a dataset's reliance on global versus local visual
information. RCI systematically compares reference-model performance on image
patches versus full images, revealing if tasks require holistic image
understanding or can be solved with partial or localized visual cues.
  When applying RCI to 13 widely used multimodal benchmarks, we observed that
most of them favor localized reasoning and exhibit significant spatial biases,
indicating potential risks in real-world applications. RCI equips researchers &
practitioners with an actionable tool for diagnosing & mitigating these biases,
enabling the construction of datasets and benchmarks to foster the development
of robust, enterprise-ready multimodal systems.

</details>


### [286] [HomeSafeBench: A Benchmark for Embodied Vision-Language Models in Free-Exploration Home Safety Inspection](https://arxiv.org/abs/2509.23690)
*Siyuan Gao,Jiashu Yao,Haoyu Wen,Yuhang Guo,Zeming Liu,Heyan Huang*

Main category: cs.CV

TL;DR: 提出HomeSafeBench基准测试，通过动态第一视角图像评估视觉语言模型在家庭安全隐患检测中的能力，揭示当前模型存在重大缺陷


<details>
  <summary>Details</summary>
Motivation: 现有基准测试过度依赖文本环境描述且采用静态观察视角，导致无法准确评估基于视觉语言模型的具身智能体，且可能遗漏被遮挡的安全隐患

Method: 构建包含12,900个数据点的HomeSafeBench，覆盖火灾、触电等5类安全隐患，提供模拟家庭环境的动态第一视角图像，允许智能体自由探索房间

Result: 主流视觉语言模型表现显著受限（最佳模型F1-score仅10.23%），在隐患识别和探索策略选择上存在明显缺陷

Conclusion: HomeSafeBench为家庭安全检测研究提供重要参考，未来将公开数据集与代码以支持相关研究发展

Abstract: Embodied agents can identify and report safety hazards in the home
environments. Accurately evaluating their capabilities in home safety
inspection tasks is curcial, but existing benchmarks suffer from two key
limitations. First, they oversimplify safety inspection tasks by using textual
descriptions of the environment instead of direct visual information, which
hinders the accurate evaluation of embodied agents based on Vision-Language
Models (VLMs). Second, they use a single, static viewpoint for environmental
observation, which restricts the agents' free exploration and cause the
omission of certain safety hazards, especially those that are occluded from a
fixed viewpoint. To alleviate these issues, we propose HomeSafeBench, a
benchmark with 12,900 data points covering five common home safety hazards:
fire, electric shock, falling object, trips, and child safety. HomeSafeBench
provides dynamic first-person perspective images from simulated home
environments, enabling the evaluation of VLM capabilities for home safety
inspection. By allowing the embodied agents to freely explore the room,
HomeSafeBench provides multiple dynamic perspectives in complex environments
for a more thorough inspection. Our comprehensive evaluation of mainstream VLMs
on HomeSafeBench reveals that even the best-performing model achieves an
F1-score of only 10.23%, demonstrating significant limitations in current VLMs.
The models particularly struggle with identifying safety hazards and selecting
effective exploration strategies. We hope HomeSafeBench will provide valuable
reference and support for future research related to home security inspections.
Our dataset and code will be publicly available soon.

</details>


### [287] [PCRI: Measuring Context Robustness in Multimodal Models for Enterprise Applications](https://arxiv.org/abs/2509.23879)
*Hitesh Laxmichand Patel,Amit Agarwal,Srikant Panda,Hansa Meghwani,Karan Dua,Paul Li,Tao Sheng,Sujith Ravi,Dan Roth*

Main category: cs.CV

TL;DR: 提出PCRI指标量化多模态大模型对视觉上下文变化的鲁棒性，发现多数主流模型易受背景干扰，仅InternVL2-26B等个别模型表现稳健。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标无法反映多模态大模型在真实场景中对无关视觉上下文的敏感性缺陷，制约实际应用可靠性。

Method: 通过对比模型在局部图像块与完整图像输入的性能差异，构建可解释的PCRI评分体系。

Result: 在19个先进模型、15个基准测试中发现：1）多数模型背景抗干扰能力弱 2）架构差异导致上下文处理方式显著不同 3）InternVL2-26B等模型展现跨任务稳定性

Conclusion: PCRI为模型鲁棒性提供量化比较框架，支撑：1）基于原则的模型选择 2）指导未来架构优化 3）推动训练策略改进，助力实际场景部署。

Abstract: The reliability of Multimodal Large Language Models (MLLMs) in real-world
settings is often undermined by sensitivity to irrelevant or distracting visual
context, an aspect not captured by existing evaluation metrics. We introduce
the \textbf{Patch Context Robustness Index (PCRI)}, the first systematic and
interpretable score for quantifying MLLM robustness to variations in visual
context granularity, measuring performance changes between localized image
patches and full-image input.
  Applying PCRI to 19 state-of-the-art MLLMs across 15 vision-language
benchmarks, we find that most leading models remain brittle to background
noise, with only a few, such as InternVL2-26B and Qwen2VL-72B, demonstrating
consistent robustness across tasks. PCRI analysis also highlights how different
model architectures handle and integrate visual context, offering actionable
diagnostic insight for both researchers and practitioners.
  PCRI enables rigorous comparison of context robustness, supporting principled
model selection and guiding the development of future architectures and
training strategies for robust, real-world deployment.

</details>


### [288] [Generalist Scanner Meets Specialist Locator: A Synergistic Coarse-to-Fine Framework for Robust GUI Grounding](https://arxiv.org/abs/2509.24133)
*Zhecheng Li,Guoxian Song,Yiwei Wang,Zhen Xiong,Junsong Yuan,Yujun Cai*

Main category: cs.CV

TL;DR: 提出了GMS框架，通过通用视觉语言模型（Scanner）与专用定位模型（Locator）的协同，实现自然语言查询在GUI中的精准空间定位，实验显示准确率提升10倍至35.7%。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言查询在跨应用GUI中的定位难题，需兼顾界面元素理解与空间坐标预测的双重挑战。

Method: 五阶段粗到精框架：Scanner筛选兴趣区域，Locator精确定位坐标，采用层次化搜索与跨模态通信机制。

Result: ScreenSpot-Pro数据集上，单独Scanner/Locator准确率仅2.0%/3.7%，集成框架达35.7%，优于各类基线模型。

Conclusion: 模拟人眼-大脑协作模式的分工架构有效提升性能，证明通用-专用模型协同在GUI定位任务中的突破性潜力。

Abstract: Grounding natural language queries in graphical user interfaces (GUIs)
presents a challenging task that requires models to comprehend diverse UI
elements across various applications and systems, while also accurately
predicting the spatial coordinates for the intended operation. To tackle this
problem, we propose GMS: Generalist Scanner Meets Specialist Locator, a
synergistic coarse-to-fine framework that effectively improves GUI grounding
performance. GMS leverages the complementary strengths of general
vision-language models (VLMs) and small, task-specific GUI grounding models by
assigning them distinct roles within the framework. Specifically, the general
VLM acts as a 'Scanner' to identify potential regions of interest, while the
fine-tuned grounding model serves as a 'Locator' that outputs precise
coordinates within these regions. This design is inspired by how humans perform
GUI grounding, where the eyes scan the interface and the brain focuses on
interpretation and localization. Our whole framework consists of five stages
and incorporates hierarchical search with cross-modal communication to achieve
promising prediction results. Experimental results on the ScreenSpot-Pro
dataset show that while the 'Scanner' and 'Locator' models achieve only $2.0\%$
and $3.7\%$ accuracy respectively when used independently, their integration
within GMS framework yields an overall accuracy of $35.7\%$, representing a $10
\times$ improvement. Additionally, GMS significantly outperforms other strong
baselines under various settings, demonstrating its robustness and potential
for general-purpose GUI grounding.

</details>


### [289] [Latent Visual Reasoning](https://arxiv.org/abs/2509.24251)
*Bangzheng Li,Ximeng Sun,Jiang Liu,Ze Wang,Jialian Wu,Xiaodong Yu,Hao Chen,Emad Barsoum,Muhao Chen,Zicheng Liu*

Main category: cs.CV

TL;DR: 提出潜在视觉推理（LVR）新范式，通过直接在视觉嵌入空间进行自回归推理，显著提升多模态大语言模型的细粒度视觉理解能力


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs的推理过程局限于语言空间，视觉信息仅作为静态前提。需突破语言空间的限制，实现视觉语义空间的自主推理

Method: 1. 视觉编码器将图像映射到与语言模型共享的语义空间
2. 语言模型生成重构关键视觉token的潜在状态
3. 结合GRPO算法进行潜在推理的强化学习

Result: 在MMVP视觉问答任务达到71.67%准确率（Qwen2.5-VL基准为66.67%），细粒度视觉理解能力显著提升

Conclusion: LVR突破了传统语言空间推理的局限，通过视觉语义空间的联合建模实现了更优的多模态感知，为MLLMs的视觉推理提供了新方向

Abstract: Multimodal Large Language Models (MLLMs) have achieved notable gains in
various tasks by incorporating Chain-of-Thought (CoT) reasoning in language
spaces. Recent work extends this direction by leveraging external tools for
visual editing, thereby enhancing the visual signal along the reasoning
trajectories. Nevertheless, these approaches remain fundamentally constrained:
reasoning is still confined to the language space, with visual information
treated as static preconditions. We introduce Latent Visual Reasoning (LVR), a
new paradigm that enables autoregressive reasoning directly in the visual
embedding space. A visual encoder first projects images into visual tokens
within a joint semantic space shared with the language model. The language
model is then trained to generate latent states that reconstruct key visual
tokens critical for answering the query, constituting the process of latent
visual reasoning. By interleaving LVR with standard text generation, our model
achieves substantial gains on perception-intensive visual question answering
tasks. In addition, we adapt the GRPO algorithm to conduct reinforcement
learning on latent reasoning, further balancing LVR and textual generation. We
show that LVR substantially improves fine-grained visual understanding and
perception, achieving 71.67% on MMVP compared to 66.67% with Qwen2.5-VL. Code
base and model weights will be released later.

</details>


### [290] [Beyond Isolated Facts: Synthesizing Narrative and Grounded Supervision for VideoQA](https://arxiv.org/abs/2509.24445)
*Jianxin Liang,Tan Yue,Yuxuan Wang,Yueqian Wang,Zhihan Yin,Huishuai Zhang,Dongyan Zhao*

Main category: cs.CV

TL;DR: 提出QBP和QBC框架增强视频问答模型的事件连贯理解，实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 传统VideoQA模型基于孤立问答对监督，缺乏对事件叙事结构和因果关系的深层理解

Method: QBP通过问题转述构建叙事段落，QBC生成视觉依据进行答案支撑；利用生成模型统一训练目标

Result: STAR准确率72.5%（+4.9%），NExT-QA达80.8%；跨数据集泛化提升2.5倍收敛速度

Conclusion: 从孤立事实转向叙事连贯性的数据合成范式，显著提升模型准确性、效率和泛化能力

Abstract: The performance of Video Question Answering (VideoQA) models is fundamentally
constrained by the nature of their supervision, which typically consists of
isolated, factual question-answer pairs. This "bag-of-facts" approach fails to
capture the underlying narrative and causal structure of events, limiting
models to a shallow understanding of video content. To move beyond this
paradigm, we introduce a framework to synthesize richer supervisory signals. We
propose two complementary strategies: Question-Based Paraphrasing (QBP), which
synthesizes the diverse inquiries (what, how, why) from a video's existing set
of question-answer pairs into a holistic narrative paragraph that reconstructs
the video's event structure; and Question-Based Captioning (QBC), which
generates fine-grained visual rationales, grounding the answer to each question
in specific, relevant evidence. Leveraging powerful generative models, we use
this synthetic data to train VideoQA models under a unified next-token
prediction objective. Extensive experiments on STAR and NExT-QA validate our
approach, demonstrating significant accuracy gains and establishing new
state-of-the-art results, such as improving a 3B model to 72.5\% on STAR
(+4.9\%) and a 7B model to 80.8\% on NExT-QA. Beyond accuracy, our analysis
reveals that both QBP and QBC substantially enhance cross-dataset
generalization, with QBP additionally accelerating model convergence by over
2.5x. These results demonstrate that shifting data synthesis from isolated
facts to narrative coherence and grounded rationales yields a more accurate,
efficient, and generalizable training paradigm.

</details>


### [291] [Euclid's Gift: Enhancing Spatial Perception and Reasoning in Vision-Language Models via Geometric Surrogate Tasks](https://arxiv.org/abs/2509.24473)
*Shijie Lian,Changti Wu,Laurence Tianruo Yang,Hang Yuan,Bin Yu,Lei Zhang,Kai Chen*

Main category: cs.CV

TL;DR: 通过欧几里得几何问题微调MLLMs，显著提升空间推理能力，在多个基准实现SOTA突破


<details>
  <summary>Details</summary>
Motivation: 解决多模态大模型在空间智能（如形状变换、位置关系判断等）方面的核心挑战，通过几何问题作为复杂空间推理的切入点

Method: 构建包含3万平面/立体几何问题的Euclid30K数据集，采用GRPO优化方法微调Qwen2.5VL和RoboBrain2.0系列模型，训练模型运用欧几里得原理进行多步推理

Result: 零样本性能在4个空间推理基准平均提升5.5个百分点（VSI-Bench达40.5%），RoboBrain2.0-Euclid-7B以49.6%准确率超越先前最佳模型

Conclusion: 首次证明几何中心化微调可赋予视觉语言模型广泛迁移的空间技能，为提升MLLMs空间智能开辟新路径

Abstract: Spatial intelligence spans a rich suite of abilities, including visualising
and transforming shapes, mentally rotating objects, judging relational
positions and containment, and estimating numerosity. However, it still remains
a critical unresolved challenge for Multimodal Large Language Models (MLLMs).To
fill this gap, we propose to treat Euclidean geometry problem-solving as a
surrogate task. Specifically, we meticulously constructed a curated multimodal
dataset, called Euclid30K, comprising approximately 30K plane and solid
geometry problems. To enable the model to acquire and apply Euclidean
principles from these geometry problems, we employed Group Relative Policy
Optimization (GRPO) to finetune the Qwen2.5VL family and RoboBrain2.0 family,
inspiring the models to identify shapes, count, and relate entities, and
perform multi-step deductive reasoning using Euclidean principles. Our
experiments demonstrate that the resulting models achieve substantial zero-shot
gains across four spatial reasoning benchmarks (Super-CLEVR, Omni3DBench,
VSI-Bench, and MindCube) without any task-specific adaptations. Notably, after
training on the Euclid30K, the mean VSI-Bench accuracy of all evaluated models
rose from 34.5% to 40.5%, improving by 5.5 percentage points. Among them,
RoboBrain2.0-Euclid-7B achieves 49.6\% accuracy, surpassing the previous
state-of-the-art model, Spatial-MLLM.To our knowledge, this is the first
systematic study showing that geometry-centric fine-tuning can confer
vision-language models with broadly transferable spatial skills. Code and
Euclid30K dataset can be found in https://zgca-ai4edu.github.io/Euclids_Gift.

</details>


### [292] [NeMo: Needle in a Montage for Video-Language Understanding](https://arxiv.org/abs/2509.24563)
*Zi-Yuan Hu,Shuo Liang,Duo Zheng,Yanyang Li,Yeyao Tao,Shijia Huang,Wei Feng,Jia Qin,Jianguang Yu,Jing Huang,Meng Fang,Yin Li,Liwei Wang*

Main category: cs.CV

TL;DR: 提出NeMoBench视频语言基准测试，通过自动生成31,378个QA对评估20个SOTA模型的时空推理能力


<details>
  <summary>Details</summary>
Motivation: 现有视频语言模型评估方法缺乏复杂时间推理能力的测试框架，需建立系统化的时空推理评估体系

Method: 开发自动化数据生成管道，基于13,486个不同时长视频自动合成QA对构建NeMoBench基准

Result: 成功生成31,378个QA对，实验验证管道可靠性，发现现有模型在长视频理解（>10分钟）准确率下降40%

Conclusion: NeMoBench填补视频模型评估空白，自动化数据生成框架支持持续更新，为视频语言模型发展提供新方向

Abstract: Recent advances in video large language models (VideoLLMs) call for new
evaluation protocols and benchmarks for complex temporal reasoning in
video-language understanding. Inspired by the needle in a haystack test widely
used by LLMs, we introduce a novel task of Needle in a Montage (NeMo), designed
to assess VideoLLMs' critical reasoning capabilities, including long-context
recall and temporal grounding. To generate video question answering data for
our task, we develop a scalable automated data generation pipeline that
facilitates high-quality data synthesis. Built upon the proposed pipeline, we
present NeMoBench, a video-language benchmark centered on our task.
Specifically, our full set of NeMoBench features 31,378 automatically generated
question-answer (QA) pairs from 13,486 videos with various durations ranging
from seconds to hours. Experiments demonstrate that our pipeline can reliably
and automatically generate high-quality evaluation data, enabling NeMoBench to
be continuously updated with the latest videos. We evaluate 20 state-of-the-art
models on our benchmark, providing extensive results and key insights into
their capabilities and limitations. Our project page is available at:
https://lavi-lab.github.io/NeMoBench.

</details>


### [293] [MMRQA: Signal-Enhanced Multimodal Large Language Models for MRI Quality Assessment](https://arxiv.org/abs/2509.24888)
*Fankai Jia,Daisong Gan,Zhe Zhang,Zhaochi Wen,Chenchen Dan,Dong Liang,Haifeng Wang*

Main category: cs.CV

TL;DR: 提出融合多模态大语言模型与信号处理的MMRQA框架，解决传统MRI质量评估中定量指标与语义理解难以兼顾的问题，在多个基准测试中实现最优性能。


<details>
  <summary>Details</summary>
Motivation: 传统MRI质量评估方法存在根本性矛盾：信号方法缺乏语义理解，深度学习方法牺牲可解释性。需开发兼具定量分析与语义推理能力的评估体系。

Method: 1. 通过MRQy增强模拟伪影提取鲁棒指标 2. 用Qwen将指标转化为结构化QA对 3. 基于LoRA实现LLaVA-OneVision的参数高效融合

Result: 在MR-ART/FastMRI/MyConnectome数据集上达到SOTA，零样本泛化能力强，消融实验验证各模块有效性

Conclusion: 该框架通过融合定量分析与语义推理，生成临床可解释的质量评估报告，显著提升动态医疗场景中的质量控制能力

Abstract: Magnetic resonance imaging (MRI) quality assessment is crucial for clinical
decision-making, yet remains challenging due to data scarcity and protocol
variability. Traditional approaches face fundamental trade-offs: signal-based
methods like MRIQC provide quantitative metrics but lack semantic
understanding, while deep learning approaches achieve high accuracy but
sacrifice interpretability. To address these limitations, we introduce the
Multimodal MRI Quality Assessment (MMRQA) framework, pioneering the integration
of multimodal large language models (MLLMs) with acquisition-aware signal
processing. MMRQA combines three key innovations: robust metric extraction via
MRQy augmented with simulated artifacts, structured transformation of metrics
into question-answer pairs using Qwen, and parameter-efficient fusion through
Low-Rank Adaptation (LoRA) of LLaVA-OneVision. Evaluated on MR-ART, FastMRI,
and MyConnectome benchmarks, MMRQA achieves state-of-the-art performance with
strong zero-shot generalization, as validated by comprehensive ablation
studies. By bridging quantitative analysis with semantic reasoning, our
framework generates clinically interpretable outputs that enhance quality
control in dynamic medical settings.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [294] [Learning from Convenience Samples: A Case Study on Fine-Tuning LLMs for Survey Non-response in the German Longitudinal Election Study](https://arxiv.org/abs/2509.25063)
*Tobias Holtdirk,Dennis Assenmacher,Arnim Bleier,Claudia Wagner*

Main category: cs.CY

TL;DR: 微调小型开源LLMs可有效处理随机/系统性缺失数据，在便利样本中超越零样本和传统方法，为非概率样本研究提供新策略


<details>
  <summary>Details</summary>
Motivation: 解决调查研究中概率样本成本上升和数据缺失导致的推断偏差问题，探索利用部分已有数据提升LLMs填补效果的实用方案

Method: 使用德国选举研究数据，通过监督微调LLMs处理投票选择的随机/系统性缺失，对比零样本提示、微调LLMs（3B-8B）与CatBoost分类器，测试不同便利样本对泛化的影响

Result: 随机缺失时微调LLMs匹配传统方法但优于零样本；有偏样本下微调LLMs在个体预测和群体分布恢复上超越零-shot并常优于表格方法

Conclusion: 微调LLMs为处理非概率样本和系统性缺失提供创新解决方案，可能推动仅需易获取子样本的新调查设计范式

Abstract: Survey researchers face two key challenges: the rising costs of probability
samples and missing data (e.g., non-response or attrition), which can undermine
inference and increase the use of convenience samples. Recent work explores
using large language models (LLMs) to simulate respondents via persona-based
prompts, often without labeled data. We study a more practical setting where
partial survey responses exist: we fine-tune LLMs on available data to impute
self-reported vote choice under both random and systematic nonresponse, using
the German Longitudinal Election Study. We compare zero-shot prompting and
supervised fine-tuning against tabular classifiers (e.g., CatBoost) and test
how different convenience samples (e.g., students) used for fine-tuning affect
generalization.
  Our results show that when data are missing completely at random, fine-tuned
LLMs match tabular classifiers but outperform zero-shot approaches. When only
biased convenience samples are available, fine-tuning small (3B to 8B)
open-source LLMs can recover both individual-level predictions and
population-level distributions more accurately than zero-shot and often better
than tabular methods. This suggests fine-tuned LLMs offer a promising strategy
for researchers working with non-probability samples or systematic missingness,
and may enable new survey designs requiring only easily accessible
subpopulations.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [295] [VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning](https://arxiv.org/abs/2509.24773)
*Xin Cheng,Yuyue Wang,Xihua Wang,Yihan Wu,Kaisi Guan,Yijing Chen,Peng Zhang,Xiaojiang Liu,Meng Cao,Ruihua Song*

Main category: eess.AS

TL;DR: VSSFlow统一了视频生成声音(V2S)和视觉文本转语音(VisualTTS)任务，通过流匹配框架实现多模态条件处理，在两项任务上均超越领域最优模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法将V2S和VisualTTS视为独立任务，存在条件类型差异大（视频/文本）、训练流程复杂的问题，亟需统一框架实现高效联合建模。

Method: 提出基于流匹配的VSSFlow框架：1) 交叉注意力处理模糊视频特征，自注意力处理确定文本特征；2) 端到端联合训练共享音频先验，加速收敛并增强条件生成；3) 无需复杂训练策略设计。

Result: 在V2S和VisualTTS基准测试中均超越领域专用模型，验证了统一生成模型的潜力。共享音频先验提升生成质量，分类器引导过程更稳定。

Conclusion: 通过巧妙利用注意力机制的归纳偏置，证明了统一生成模型在多模态音频生成任务中的可行性，为跨模态生成提供新思路。

Abstract: Video-conditioned sound and speech generation, encompassing video-to-sound
(V2S) and visual text-to-speech (VisualTTS) tasks, are conventionally addressed
as separate tasks, with limited exploration to unify them within a signle
framework. Recent attempts to unify V2S and VisualTTS face challenges in
handling distinct condition types (e.g., heterogeneous video and transcript
conditions) and require complex training stages. Unifying these two tasks
remains an open problem. To bridge this gap, we present VSSFlow, which
seamlessly integrates both V2S and VisualTTS tasks into a unified flow-matching
framework. VSSFlow uses a novel condition aggregation mechanism to handle
distinct input signals. We find that cross-attention and self-attention layer
exhibit different inductive biases in the process of introducing condition.
Therefore, VSSFlow leverages these inductive biases to effectively handle
different representations: cross-attention for ambiguous video conditions and
self-attention for more deterministic speech transcripts. Furthermore, contrary
to the prevailing belief that joint training on the two tasks requires complex
training strategies and may degrade performance, we find that VSSFlow benefits
from the end-to-end joint learning process for sound and speech generation
without extra designs on training stages. Detailed analysis attributes it to
the learned general audio prior shared between tasks, which accelerates
convergence, enhances conditional generation, and stabilizes the
classifier-free guidance process. Extensive experiments demonstrate that
VSSFlow surpasses the state-of-the-art domain-specific baselines on both V2S
and VisualTTS benchmarks, underscoring the critical potential of unified
generative models.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [296] [The Role of Logic and Automata in Understanding Transformers](https://arxiv.org/abs/2509.24024)
*Anthony W. Lin,Pablo Barcelo*

Main category: cs.FL

TL;DR: Transformers驱动了LLMs的革命性发展，但其能力边界仍不明确。通过逻辑、自动机与电路复杂性理论的交叉研究，揭示了Transformer模型的理论能力与局限性，并提出了多个跨领域开放性问题。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer模型推动了LLMs的快速发展，学术界对其底层机制和理论能力的理解仍较为薄弱。需要系统性梳理近年来逻辑、自动机等理论工具在解释Transformer能力方面的研究进展。

Method: 通过文献综述方法，整合逻辑系统（如时序逻辑）、自动机理论（如有穷自动机）及电路复杂度分析，构建理论框架来解释Transformer的计算能力边界。

Result: 证实逻辑与自动机理论能有效刻画Transformer的表达能力，同时揭示其在序列建模、长程依赖处理等方面的理论局限。提出了验证、逻辑推理与模型能力关联等开放性问题。

Conclusion: Transformer的理论分析需要多学科交叉，未来需在逻辑形式化验证、自动机表达能力扩展等方向深化研究，为下一代语言模型的开发提供理论支撑。

Abstract: The advent of transformers has in recent years led to powerful and
revolutionary Large Language Models (LLMs). Despite this, our understanding on
the capability of transformers is still meager. In this invited contribution,
we recount the rapid progress in the last few years to the question of what
transformers can do. In particular, we will see the integral role of logic and
automata (also with some help from circuit complexity) in answering this
question. We also mention several open problems at the intersection of logic,
automata, verification and transformers.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [297] [VIRTUS-FPP: Virtual Sensor Modeling for Fringe Projection Profilometry in NVIDIA Isaac Sim](https://arxiv.org/abs/2509.22685)
*Adam Haroon,Anush Lakshman,Badrinath Balasubramaniam,Beiwen Li*

Main category: eess.IV

TL;DR: 提出VIRTUS-FPP框架，基于NVIDIA Isaac Sim构建首个物理虚拟传感器模型，解决传统条纹投影轮廓术（FPP）校准复杂、系统笨重和环境敏感问题。


<details>
  <summary>Details</summary>
Motivation: 传统FPP技术因复杂校准流程、庞大系统体积及环境敏感性限制了其应用场景，亟需灵活可靠的替代方案。

Method: 利用NVIDIA Isaac Sim的物理渲染和可编程传感能力，建立端到端虚拟建模框架，完整复现结构光数学原理，支持数字孪生系统构建。

Result: 虚拟校准验证显示亚毫米级重建精度，数字孪生实验证实虚拟与现实测量结果高度一致，光学建模能力达到实际系统水平。

Conclusion: 该框架通过虚拟原型加速FPP系统开发，在保证精度的同时提供硬件配置、环境控制等前所未有的灵活性。

Abstract: Fringe projection profilometry (FPP) has been established as a high-accuracy
3D reconstruction method capable of achieving sub-pixel accuracy. However, this
technique faces significant constraints due to complex calibration
requirements, bulky system footprint, and sensitivity to environmental
conditions. To address these limitations, we present VIRTUS-FPP, the first
comprehensive physics-based virtual sensor modeling framework for FPP built in
NVIDIA Isaac Sim. By leveraging the physics-based rendering and programmable
sensing capabilities of simulation, our framework enables end-to-end modeling
from calibration to reconstruction with full mathematical fidelity to the
underlying principles of structured light. We conduct comprehensive virtual
calibration and validate our system's reconstruction accuracy through
quantitative comparison against ground truth geometry. Additionally, we
demonstrate the ability to model the virtual system as a digital twin by
replicating a physical FPP system in simulation and validating correspondence
between virtual and real-world measurements. Experimental results demonstrate
that VIRTUS-FPP accurately models optical phenomena critical to FPP and
achieves results comparable to real-world systems while offering unprecedented
flexibility for system configuration, sensor prototyping, and environmental
control. This framework significantly accelerates the development of real-world
FPP systems by enabling rapid virtual prototyping before physical
implementation.

</details>
