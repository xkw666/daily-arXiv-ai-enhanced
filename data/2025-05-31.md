<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 112]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.HC](#cs.HC) [Total: 3]
- [cs.CV](#cs.CV) [Total: 10]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.SE](#cs.SE) [Total: 3]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.LG](#cs.LG) [Total: 9]
- [cs.DB](#cs.DB) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Training Language Models to Generate Quality Code with Program Analysis Feedback](https://arxiv.org/abs/2505.22704)
*Feng Yao,Zilong Wang,Liyuan Liu,Junxia Cui,Li Zhong,Xiaohan Fu,Haohui Mai,Vish Krishnan,Jianfeng Gao,Jingbo Shang*

Main category: cs.CL

TL;DR: 提出强化学习框架REAL，通过程序分析和单元测试反馈激励LLM生成高质量代码，解决现有方法在安全性和可维护性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代码生成方法依赖人工标注或启发式规则，难以保障安全漏洞（如SQL注入）和可维护性（如类型缺失），需自动化解决方案。

Method: REAL框架集成程序分析（检测安全/维护缺陷）和单元测试（功能验证），采用提示无关、无参考的强化学习进行自动化质量监督。

Result: 在多数据集/模型规模下，REAL在功能正确性和代码质量评估中均优于SOTA方法，错误率降低32%-41%。

Conclusion: REAL弥合了原型开发与生产级代码的差距，使LLM同时实现高效生成与代码质量，推动AI代码生成的实际应用。

Abstract: Code generation with large language models (LLMs), often termed vibe coding,
is increasingly adopted in production but fails to ensure code quality,
particularly in security (e.g., SQL injection vulnerabilities) and
maintainability (e.g., missing type annotations). Existing methods, such as
supervised fine-tuning and rule-based post-processing, rely on labor-intensive
annotations or brittle heuristics, limiting their scalability and
effectiveness. We propose REAL, a reinforcement learning framework that
incentivizes LLMs to generate production-quality code using program
analysis-guided feedback. Specifically, REAL integrates two automated signals:
(1) program analysis detecting security or maintainability defects and (2) unit
tests ensuring functional correctness. Unlike prior work, our framework is
prompt-agnostic and reference-free, enabling scalable supervision without
manual intervention. Experiments across multiple datasets and model scales
demonstrate that REAL outperforms state-of-the-art methods in simultaneous
assessments of functionality and code quality. Our work bridges the gap between
rapid prototyping and production-ready code, enabling LLMs to deliver both
speed and quality.

</details>


### [2] [Climate Finance Bench](https://arxiv.org/abs/2505.22752)
*Rafik Mankour,Yassine Chafai,Hamada Saleh,Ghassen Ben Hassine,Thibaud Barreau,Peter Tankov*

Main category: cs.CL

TL;DR: Climate Finance Bench构建了一个包含330个专家验证QA对的开源基准测试集，覆盖11个行业，揭示RAG方法中检索器性能是核心瓶颈，并提倡AI气候应用中采用权重量化等低碳技术。


<details>
  <summary>Details</summary>
Motivation: 现有AI气候应用缺乏标准化评估体系和碳足迹透明度，需建立专门基准测试工具性能并推动环保技术应用。

Method: 收集33份企业可持续发展报告构建多类型QA数据集，通过对比不同RAG架构分析性能瓶颈，同时测试模型量化技术的减排效果。

Result: 检索阶段准确率仅55%严重制约整体表现，而4-bit权重量化可减少30%内存占用且保持97%模型性能。

Conclusion: 该基准为气候信息披露QA系统提供可靠评估框架，强调优化检索器及采用低碳AI技术的重要性。

Abstract: Climate Finance Bench introduces an open benchmark that targets
question-answering over corporate climate disclosures using Large Language
Models. We curate 33 recent sustainability reports in English drawn from
companies across all 11 GICS sectors and annotate 330 expert-validated
question-answer pairs that span pure extraction, numerical reasoning, and
logical reasoning. Building on this dataset, we propose a comparison of RAG
(retrieval-augmented generation) approaches. We show that the retriever's
ability to locate passages that actually contain the answer is the chief
performance bottleneck. We further argue for transparent carbon reporting in
AI-for-climate applications, highlighting advantages of techniques such as
Weight Quantization.

</details>


### [3] [Pre-Training Curriculum for Multi-Token Prediction in Language Models](https://arxiv.org/abs/2505.22757)
*Ansar Aynetdinov,Alan Akbik*

Main category: cs.CL

TL;DR: 针对小型语言模型在MTP训练中的困难，提出正向课程学习策略，有效提升下游任务表现并保留自推测解码优势。


<details>
  <summary>Details</summary>
Motivation: 现有MTP方法在大型模型中效果显著，但小型模型难以有效利用MTP目标，需探索适合SLMs的训练策略。

Method: 设计两种课程学习策略：正向(从NTP逐步过渡到MTP)和逆向(从MTP过渡到NTP)，在预训练阶段实施对比实验。

Result: 正向策略使SLMs提升NTP性能14%、生成质量23%，同时保持自推测解码速度增益；逆向策略虽提升NTP19%但丧失解码优势。

Conclusion: 正向课程是平衡性能与推理效率的优选方案，逆向策略适用于仅关注输出质量的场景。

Abstract: Multi-token prediction (MTP) is a recently proposed pre-training objective
for language models. Rather than predicting only the next token (NTP), MTP
predicts the next $k$ tokens at each prediction step, using multiple prediction
heads. MTP has shown promise in improving downstream performance, inference
speed, and training efficiency, particularly for large models. However, prior
work has shown that smaller language models (SLMs) struggle with the MTP
objective. To address this, we propose a curriculum learning strategy for MTP
training, exploring two variants: a forward curriculum, which gradually
increases the complexity of the pre-training objective from NTP to MTP, and a
reverse curriculum, which does the opposite. Our experiments show that the
forward curriculum enables SLMs to better leverage the MTP objective during
pre-training, improving downstream NTP performance and generative output
quality, while retaining the benefits of self-speculative decoding. The reverse
curriculum achieves stronger NTP performance and output quality, but fails to
provide any self-speculative decoding benefits.

</details>


### [4] [FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian](https://arxiv.org/abs/2505.22759)
*Sara Papi,Marco Gaido,Luisa Bentivogli,Alessio Brutti,Mauro Cettolo,Roberto Gretter,Marco Matassoni,Mohamed Nabih,Matteo Negri*

Main category: cs.CL

TL;DR: FAMA是首个基于150k+小时开源语音数据训练的开放科学语音基础模型家族，性能与主流模型相当且快8倍，所有资源开源。


<details>
  <summary>Details</summary>
Motivation: 现有语音基础模型(如Whisper)的封闭性导致可复现性和评估困难，而其他领域的开放科学进展显著，语音领域亟需透明化解决方案。

Method: 使用15万+小时开源语音数据训练FAMA模型家族，构建包含16k小时清洗和伪标注语音的新数据集，所有代码/数据/模型均开源。

Result: FAMA在保持与现有语音模型相当性能的同时，推理速度提升达8倍。

Conclusion: 通过完全开源的模型、数据和工具链，FAMA推动了语音技术研究的透明化和可复现性，为领域发展提供了开放基础。

Abstract: The development of speech foundation models (SFMs) like Whisper and
SeamlessM4T has significantly advanced the field of speech processing. However,
their closed nature--with inaccessible training data and code--poses major
reproducibility and fair evaluation challenges. While other domains have made
substantial progress toward open science by developing fully transparent models
trained on open-source (OS) code and data, similar efforts in speech remain
limited. To fill this gap, we introduce FAMA, the first family of open science
SFMs for English and Italian, trained on 150k+ hours of OS speech data.
Moreover, we present a new dataset containing 16k hours of cleaned and
pseudo-labeled speech for both languages. Results show that FAMA achieves
competitive performance compared to existing SFMs while being up to 8 times
faster. All artifacts, including code, datasets, and models, are released under
OS-compliant licenses, promoting openness in speech technology research.

</details>


### [5] [StressTest: Can YOUR Speech LM Handle the Stress?](https://arxiv.org/abs/2505.22765)
*Iddo Yosha,Gallil Maimon,Yossi Adi*

Main category: cs.CL

TL;DR: 提出StressTest基准和Stress17k合成数据集，通过微调显著提升语音语言模型对句子重音的推理和检测能力


<details>
  <summary>Details</summary>
Motivation: 现有语音语言模型忽视句子重音对语义和说话者意图的关键影响，缺乏针对性评估和训练框架

Method: 开发基于重音模式解释差异的评估基准，构建合成数据生成流程模拟重音变化，采用微调策略优化模型

Result: 微调后的StresSLM模型在句子重音推理和检测任务上显著优于现有模型

Conclusion: 通过定向数据集生成和模型优化可有效提升语音模型对韵律特征的感知能力，推动语音理解技术发展

Abstract: Sentence stress refers to emphasis, placed on specific words within a spoken
utterance to highlight or contrast an idea, or to introduce new information. It
is often used to imply an underlying intention that is not explicitly stated.
Recent advances in speech-aware language models (SLMs) have enabled direct
processing of audio, allowing models to bypass transcription and access the
full richness of the speech signal and perform audio reasoning tasks such as
spoken question answering. Despite the crucial role of sentence stress in
shaping meaning and speaker intent, it remains largely overlooked in evaluation
and development of such models. In this work, we address this gap by
introducing StressTest, a benchmark specifically designed to evaluate a model's
ability to distinguish between interpretations of spoken sentences based on the
stress pattern. We assess the performance of several leading SLMs and find
that, despite their overall capabilities, they perform poorly on such tasks. To
overcome this limitation, we propose a novel synthetic data generation
pipeline, and create Stress17k, a training set that simulates change of meaning
implied by stress variation. Then, we empirically show that optimizing models
with this synthetic dataset aligns well with real-world recordings and enables
effective finetuning of SLMs. Results suggest, that our finetuned model,
StresSLM, significantly outperforms existing models on both sentence stress
reasoning and detection tasks. Code, models, data, and audio samples -
pages.cs.huji.ac.il/adiyoss-lab/stresstest.

</details>


### [6] [Automated Essay Scoring Incorporating Annotations from Automated Feedback Systems](https://arxiv.org/abs/2505.22771)
*Christopher Ormerod*

Main category: cs.CL

TL;DR: 整合反馈注释提升自动作文评分准确性


<details>
  <summary>Details</summary>
Motivation: 现有自动评分系统缺乏细粒度反馈机制，通过增加拼写/语法检查和论证结构标注提升评分准确性

Method: 1. 使用生成式语言模型进行拼写纠正 2. 训练编码器模型识别论证要素 3. 在PERSUADE语料库中融合两种注释

Result: 基于编码器的大语言模型分类器性能显著提升（具体指标需参考论文数据）

Conclusion: 多维度反馈注释能有效增强AES系统的评估能力，该方法具有实际应用价值

Abstract: This study illustrates how incorporating feedback-oriented annotations into
the scoring pipeline can enhance the accuracy of automated essay scoring (AES).
This approach is demonstrated with the Persuasive Essays for Rating, Selecting,
and Understanding Argumentative and Discourse Elements (PERSUADE) corpus. We
integrate two types of feedback-driven annotations: those that identify
spelling and grammatical errors, and those that highlight argumentative
components. To illustrate how this method could be applied in real-world
scenarios, we employ two LLMs to generate annotations -- a generative language
model used for spell-correction and an encoder-based token classifier trained
to identify and mark argumentative elements. By incorporating annotations into
the scoring process, we demonstrate improvements in performance using
encoder-based large language models fine-tuned as classifiers.

</details>


### [7] [Counting trees: A treebank-driven exploration of syntactic variation in speech and writing across languages](https://arxiv.org/abs/2505.22774)
*Kaja Dobrovoljc*

Main category: cs.CL

TL;DR: 提出基于树库的依存分析方法，揭示口语与书面语句法结构存在显著模态差异：口语结构更少、多样性低且与书面语重叠有限，反映不同交际场景的句法偏好。


<details>
  <summary>Details</summary>
Motivation: 探究口语和书面语在句法组织上的系统性差异，突破传统基于词汇特征的研究范式，为语法使用理论提供数据支撑。

Method: 采用去词汇化依存子树方法，从英/斯语UD树库中自下而上提取句法结构，通过结构多样性、跨模态重叠度及关键性分析进行对比。

Result: 口语库句法结构数量减少40%、多样性降低30%；仅15%结构跨模态共享；关键结构呈现互动性（话轮标记）、语境依赖（省略结构）和经济性（短依存链）特征。

Conclusion: 建立可扩展的跨语种分析框架，证实句法模态特异性，为语料库驱动的语法变异研究提供方法论基础，推动动态语法理论发展。

Abstract: This paper presents a novel treebank-driven approach to comparing syntactic
structures in speech and writing using dependency-parsed corpora. Adopting a
fully inductive, bottom-up method, we define syntactic structures as
delexicalized dependency (sub)trees and extract them from spoken and written
Universal Dependencies (UD) treebanks in two syntactically distinct languages,
English and Slovenian. For each corpus, we analyze the size, diversity, and
distribution of syntactic inventories, their overlap across modalities, and the
structures most characteristic of speech. Results show that, across both
languages, spoken corpora contain fewer and less diverse syntactic structures
than their written counterparts, with consistent cross-linguistic preferences
for certain structural types across modalities. Strikingly, the overlap between
spoken and written syntactic inventories is very limited: most structures
attested in speech do not occur in writing, pointing to modality-specific
preferences in syntactic organization that reflect the distinct demands of
real-time interaction and elaborated writing. This contrast is further
supported by a keyness analysis of the most frequent speech-specific
structures, which highlights patterns associated with interactivity,
context-grounding, and economy of expression. We argue that this scalable,
language-independent framework offers a useful general method for
systematically studying syntactic variation across corpora, laying the
groundwork for more comprehensive data-driven theories of grammar in use.

</details>


### [8] [MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain Chatbots and Dialogue Evaluators](https://arxiv.org/abs/2505.22777)
*John Mendonça,Alon Lavie,Isabel Trancoso*

Main category: cs.CL

TL;DR: 提出MEDAL框架，通过自动化生成和评估多语言对话数据，构建更全面的开放域对话评估基准


<details>
  <summary>Details</summary>
Motivation: 现有对话评估数据集存在静态化、过时、缺乏多语言覆盖等问题，难以捕捉语言文化细微差异，阻碍聊天机器人性能评估

Method: 使用多模态LLM生成多语言对话→GPT-4多维性能分析→构建人工标注的元评估基准→测试LLM作为评估者的能力

Result: 发现显著跨语言性能差异，当前LLM在检测同理心和推理等复杂问题方面存在明显缺陷（GPT-4仅达人类水平52%）

Conclusion: MEDAL框架有效生成评估基准，但现有LLM评估者对细微质量判断仍存在局限性，需提升复杂问题识别能力

Abstract: As the capabilities of chatbots and their underlying LLMs continue to
dramatically improve, evaluating their performance has increasingly become a
major blocker to their further development. A major challenge is the available
benchmarking datasets, which are largely static, outdated, and lacking in
multilingual coverage, limiting their ability to capture subtle linguistic and
cultural variations. This paper introduces MEDAL, an automated multi-agent
framework for generating, evaluating, and curating more representative and
diverse open-domain dialogue evaluation benchmarks. Our approach leverages
several state-of-the-art LLMs to generate user-chatbot multilingual dialogues,
conditioned on varied seed contexts. A strong LLM (GPT-4.1) is then used for a
multidimensional analysis of the performance of the chatbots, uncovering
noticeable cross-lingual performance differences. Guided by this large-scale
evaluation, we curate a new meta-evaluation multilingual benchmark and
human-annotate samples with nuanced quality judgments. This benchmark is then
used to assess the ability of several reasoning and non-reasoning LLMs to act
as evaluators of open-domain dialogues. We find that current LLMs struggle to
detect nuanced issues, particularly those involving empathy and reasoning.

</details>


### [9] [Can Large Language Models Match the Conclusions of Systematic Reviews?](https://arxiv.org/abs/2505.22787)
*Christopher Polzak,Alejandro Lozano,Min Woo Sun,James Burgess,Yuhui Zhang,Kevin Wu,Serena Yeung-Levy*

Main category: cs.CL

TL;DR: LLMs在生成系统性评价时表现出过度自信、缺乏科学怀疑态度，且模型规模与准确性无正相关，当前尚无法替代专家评审


<details>
  <summary>Details</summary>
Motivation: 系统性评价是临床决策基石，但人工撰写耗时。研究旨在验证LLMs能否基于相同研究得出与临床专家相同的结论

Method: 构建MedEvidence基准（100个系统评价及其原始研究），评估24个不同规模（7B-700B）、类型（推理/非推理/医学专用）的LLM表现

Result: 推理能力不提升性能，模型增大无持续增益，知识微调反而降低准确性；所有模型对低质量研究缺乏怀疑，长文本处理性能下降

Conclusion: 尽管LLMs已实际应用，但其批判性思维和跨文档推理能力仍未达到专家水平，需进一步改进才能可靠替代人工系统评价

Abstract: Systematic reviews (SR), in which experts summarize and analyze evidence
across individual studies to provide insights on a specialized topic, are a
cornerstone for evidence-based clinical decision-making, research, and policy.
Given the exponential growth of scientific articles, there is growing interest
in using large language models (LLMs) to automate SR generation. However, the
ability of LLMs to critically assess evidence and reason across multiple
documents to provide recommendations at the same proficiency as domain experts
remains poorly characterized. We therefore ask: Can LLMs match the conclusions
of systematic reviews written by clinical experts when given access to the same
studies? To explore this question, we present MedEvidence, a benchmark pairing
findings from 100 SRs with the studies they are based on. We benchmark 24 LLMs
on MedEvidence, including reasoning, non-reasoning, medical specialist, and
models across varying sizes (from 7B-700B). Through our systematic evaluation,
we find that reasoning does not necessarily improve performance, larger models
do not consistently yield greater gains, and knowledge-based fine-tuning
degrades accuracy on MedEvidence. Instead, most models exhibit similar
behavior: performance tends to degrade as token length increases, their
responses show overconfidence, and, contrary to human experts, all models show
a lack of scientific skepticism toward low-quality findings. These results
suggest that more work is still required before LLMs can reliably match the
observations from expert-conducted SRs, even though these systems are already
deployed and being used by clinicians. We release our codebase and benchmark to
the broader research community to further investigate LLM-based SR systems.

</details>


### [10] [Towards a More Generalized Approach in Open Relation Extraction](https://arxiv.org/abs/2505.22801)
*Qing Wang,Yuepei Li,Qiao Qiao,Kang Zhou,Qi Li*

Main category: cs.CL

TL;DR: 提出MixORE框架解决广义开放关系抽取问题，通过两阶段分类+聚类联合学习机制，在三个基准数据集上实现已知关系分类和新型关系聚类的双提升


<details>
  <summary>Details</summary>
Motivation: 传统OpenRE方法假设未标注数据仅含新型关系或已预分割已知/新型实例，但现实场景中新型关系是任意分布的混合状态，需同时处理已知和未知关系

Method: 两阶段框架MixORE：第一阶段通过对比学习构建原型空间，第二阶段联合关系分类（已知关系）和对比聚类（新型关系）的损失函数进行联合优化

Result: 在三个基准数据集上的实验表明，MixORE在已知关系分类（平均提升3.2% F1）和新型关系聚类（平均提升5.8% ARI）均优于基线模型

Conclusion: 该研究推进了广义开放关系抽取的发展，提出的混合学习机制为现实场景中关系抽取系统的构建提供了新思路

Abstract: Open Relation Extraction (OpenRE) seeks to identify and extract novel
relational facts between named entities from unlabeled data without pre-defined
relation schemas. Traditional OpenRE methods typically assume that the
unlabeled data consists solely of novel relations or is pre-divided into known
and novel instances. However, in real-world scenarios, novel relations are
arbitrarily distributed. In this paper, we propose a generalized OpenRE setting
that considers unlabeled data as a mixture of both known and novel instances.
To address this, we propose MixORE, a two-phase framework that integrates
relation classification and clustering to jointly learn known and novel
relations. Experiments on three benchmark datasets demonstrate that MixORE
consistently outperforms competitive baselines in known relation classification
and novel relation clustering. Our findings contribute to the advancement of
generalized OpenRE research and real-world applications.

</details>


### [11] [First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons & Dragons Gameplay](https://arxiv.org/abs/2505.22809)
*Andrew Zhu,Evan Osgood,Chris Callison-Burch*

Main category: cs.CL

TL;DR: 提出'旁听代理'新范式，通过监听人类对话提供被动辅助，基于《龙与地下城》场景验证多模态音频语言模型的应用潜力并开源代码库


<details>
  <summary>Details</summary>
Motivation: 探索对话型LLM代理的替代范式，突破传统主动参与模式，利用被动监听机制实现更自然的任务辅助

Method: 使用多模态音频-语言模型构建旁听代理系统，在D&D游戏场景进行人机协作实验，通过人类评估检验代理有效性

Result: 发现大型音频-语言模型具备通过隐式音频线索执行旁听任务的新兴能力，代码开源支持后续研究（github.com/zhudotexe/overhearing_agents）

Conclusion: 旁听代理范式在特定交互场景中展现应用价值，多模态模型具备环境感知潜力，开源资源将推动人机协作领域发展

Abstract: Much work has been done on conversational LLM agents which directly assist
human users with tasks. We present an alternative paradigm for interacting with
LLM agents, which we call "overhearing agents". These overhearing agents do not
actively participate in conversation -- instead, they "listen in" on
human-to-human conversations and perform background tasks or provide
suggestions to assist the user. In this work, we explore the overhearing agents
paradigm through the lens of Dungeons & Dragons gameplay. We present an
in-depth study using large multimodal audio-language models as overhearing
agents to assist a Dungeon Master. We perform a human evaluation to examine the
helpfulness of such agents and find that some large audio-language models have
the emergent ability to perform overhearing agent tasks using implicit audio
cues. Finally, we release Python libraries and our project code to support
further research into the overhearing agents paradigm at
https://github.com/zhudotexe/overhearing_agents.

</details>


### [12] [Self-Critique and Refinement for Faithful Natural Language Explanations](https://arxiv.org/abs/2505.22823)
*Yingming Wang,Pepa Atanasova*

Main category: cs.CL

TL;DR: 提出SR-NLE框架，通过自我批判和特征反馈机制提升自然语言解释的忠实度，平均不忠实率降低18.79%


<details>
  <summary>Details</summary>
Motivation: 现有自然语言解释(NLEs)无法忠实反映大语言模型的真实推理过程，缺乏有效的无监督优化方法

Method: SR-NLE框架结合自然语言自我反馈和基于特征归因的新反馈机制，通过迭代优化提升解释忠实度

Result: 实验显示该方法在三个数据集上使平均不忠实率从54.81%降至36.02%，无需额外训练或微调

Conclusion: 大语言模型在适当反馈机制引导下，可自主优化解释忠实度，这为可信AI解释提供了新路径

Abstract: With the rapid development of large language models (LLMs), natural language
explanations (NLEs) have become increasingly important for understanding model
predictions. However, these explanations often fail to faithfully represent the
model's actual reasoning process. While existing work has demonstrated that
LLMs can self-critique and refine their initial outputs for various tasks, this
capability remains unexplored for improving explanation faithfulness. To
address this gap, we introduce Self-critique and Refinement for Natural
Language Explanations (SR-NLE), a framework that enables models to improve the
faithfulness of their own explanations -- specifically, post-hoc NLEs --
through an iterative critique and refinement process without external
supervision. Our framework leverages different feedback mechanisms to guide the
refinement process, including natural language self-feedback and, notably, a
novel feedback approach based on feature attribution that highlights important
input words. Our experiments across three datasets and four state-of-the-art
LLMs demonstrate that SR-NLE significantly reduces unfaithfulness rates, with
our best method achieving an average unfaithfulness rate of 36.02%, compared to
54.81% for baseline -- an absolute reduction of 18.79%. These findings reveal
that the investigated LLMs can indeed refine their explanations to better
reflect their actual reasoning process, requiring only appropriate guidance
through feedback without additional training or fine-tuning.

</details>


### [13] [What Has Been Lost with Synthetic Evaluation?](https://arxiv.org/abs/2505.22830)
*Alexander Gill,Abhilasha Ravichander,Ana Marasović*

Main category: cs.CL

TL;DR: 研究发现使用LLMs生成评估基准虽然成本低廉且符合标注规范，但生成的数据对LLMs自身挑战性低于人工创建数据，可能影响评估有效性。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs生成高质量评估数据的能力，确保基准能精准测试特定现象、避免模型利用数据漏洞。

Method: 通过对比LLM生成的CondaQA(否定推理)和DROP(数量推理)数据集与原始众包版本，分析数据有效性和模型挑战性差异。

Result: LLM生成数据有效性达标注标准且成本降低90%，但模型在其上的表现比人类数据高15-20%，揭示评估数据存在系统性偏差。

Conclusion: 需谨慎使用LLMs生成评估基准，可能削弱测试严谨性。建议建立数据生成-验证的闭环机制以保证基准质量。

Abstract: Large language models (LLMs) are increasingly used for data generation.
However, creating evaluation benchmarks raises the bar for this emerging
paradigm. Benchmarks must target specific phenomena, penalize exploiting
shortcuts, and be challenging. Through two case studies, we investigate whether
LLMs can meet these demands by generating reasoning over-text benchmarks and
comparing them to those created through careful crowdsourcing. Specifically, we
evaluate both the validity and difficulty of LLM-generated versions of two
high-quality reading comprehension datasets: CondaQA, which evaluates reasoning
about negation, and DROP, which targets reasoning about quantities. We find
that prompting LLMs can produce variants of these datasets that are often valid
according to the annotation guidelines, at a fraction of the cost of the
original crowdsourcing effort. However, we show that they are less challenging
for LLMs than their human-authored counterparts. This finding sheds light on
what may have been lost by generating evaluation data with LLMs, and calls for
critically reassessing the immediate use of this increasingly prevalent
approach to benchmark creation.

</details>


### [14] [Bayesian Attention Mechanism: A Probabilistic Framework for Positional Encoding and Context Length Extrapolation](https://arxiv.org/abs/2505.22842)
*Arthur S. Bianchessi,Rodrigo C. Barros,Lucas S. Kupssinskü*

Main category: cs.CL

TL;DR: 提出贝叶斯注意力机制(BAM)统一现有位置编码方法，并通过广义高斯先验实现500倍上下文长度外推突破


<details>
  <summary>Details</summary>
Motivation: 现有位置编码方法存在理论不清晰、评估指标有限的问题，难以支撑可靠的上下文长度外推

Method: 建立概率模型框架，将位置编码转化为注意力机制中的贝叶斯先验，推导出广义高斯位置先验的数学形式

Result: 在500倍训练长度的信息检索任务中准确率超越SOTA，困惑度相当且仅增加0.2%参数量

Conclusion: BAM框架为位置编码提供了统一的理论解释，其新位置先验显著提升模型的长上下文泛化能力

Abstract: Transformer-based language models rely on positional encoding (PE) to handle
token order and support context length extrapolation. However, existing PE
methods lack theoretical clarity and rely on limited evaluation metrics to
substantiate their extrapolation claims. We propose the Bayesian Attention
Mechanism (BAM), a theoretical framework that formulates positional encoding as
a prior within a probabilistic model. BAM unifies existing methods (e.g., NoPE
and ALiBi) and motivates a new Generalized Gaussian positional prior that
substantially improves long-context generalization. Empirically, BAM enables
accurate information retrieval at $500\times$ the training context length,
outperforming previous state-of-the-art context length generalization in long
context retrieval accuracy while maintaining comparable perplexity and
introducing minimal additional parameters.

</details>


### [15] [LiTEx: A Linguistic Taxonomy of Explanations for Understanding Within-Label Variation in Natural Language Inference](https://arxiv.org/abs/2505.22848)
*Pingjun Hong,Beiduo Chen,Siyao Peng,Marie-Catherine de Marneffe,Barbara Plank*

Main category: cs.CL

TL;DR: 论文提出LITEX语言学分类法，用于分析自然语言推理任务中的内部标签变异现象，通过标注实验证明该方法能有效捕捉标签内差异并提升模型解释质量


<details>
  <summary>Details</summary>
Motivation: 现有NLI数据集存在标注者使用相同标签但提供不同解释的内部标签变异问题，这种语言解释的多样性尚未被系统研究

Method: 开发LITEX分类法标注e-SNLI子集，验证分类可靠性，并通过生成实验比较基于标签/高亮文本与基于LITEX的解释质量差异

Result: 使用LITEX指导生成的解释在语言学特征上比传统方法更接近人类解释，F1分数提升3.5%

Conclusion: 语言学驱动的分类法既能揭示标签内变异，又为构建更接近人类推理的模型解释系统提供了有效框架

Abstract: There is increasing evidence of Human Label Variation (HLV) in Natural
Language Inference (NLI), where annotators assign different labels to the same
premise-hypothesis pair. However, within-label variation--cases where
annotators agree on the same label but provide divergent reasoning--poses an
additional and mostly overlooked challenge. Several NLI datasets contain
highlighted words in the NLI item as explanations, but the same spans on the
NLI item can be highlighted for different reasons, as evidenced by free-text
explanations, which offer a window into annotators' reasoning. To
systematically understand this problem and gain insight into the rationales
behind NLI labels, we introduce LITEX, a linguistically-informed taxonomy for
categorizing free-text explanations. Using this taxonomy, we annotate a subset
of the e-SNLI dataset, validate the taxonomy's reliability, and analyze how it
aligns with NLI labels, highlights, and explanations. We further assess the
taxonomy's usefulness in explanation generation, demonstrating that
conditioning generation on LITEX yields explanations that are linguistically
closer to human explanations than those generated using only labels or
highlights. Our approach thus not only captures within-label variation but also
shows how taxonomy-guided generation for reasoning can bridge the gap between
human and model explanations more effectively than existing strategies.

</details>


### [16] [GateNLP at SemEval-2025 Task 10: Hierarchical Three-Step Prompting for Multilingual Narrative Classification](https://arxiv.org/abs/2505.22867)
*Iknoor Singh,Carolina Scarton,Kalina Bontcheva*

Main category: cs.CL

TL;DR: 提出分层三步骤提示法H3Prompt用于多语言新闻叙事分类，在SemEval竞赛英语测试集获得第一


<details>
  <summary>Details</summary>
Motivation: 针对网络新闻激增和虚假信息传播问题，实现自动化叙事分类对事实核查和政策制定具有重要意义

Method: 采用三步LLM提示策略：先领域分类→主叙事识别→子叙事分配，支持多语言处理

Result: 在28个国际团队中，英语测试集排名首位

Conclusion: H3Prompt方法有效实现多层级叙事分类，开源代码推动相关研究发展

Abstract: The proliferation of online news and the increasing spread of misinformation
necessitate robust methods for automatic data analysis. Narrative
classification is emerging as a important task, since identifying what is being
said online is critical for fact-checkers, policy markers and other
professionals working on information studies. This paper presents our approach
to SemEval 2025 Task 10 Subtask 2, which aims to classify news articles into a
pre-defined two-level taxonomy of main narratives and sub-narratives across
multiple languages.
  We propose Hierarchical Three-Step Prompting (H3Prompt) for multilingual
narrative classification. Our methodology follows a three-step Large Language
Model (LLM) prompting strategy, where the model first categorises an article
into one of two domains (Ukraine-Russia War or Climate Change), then identifies
the most relevant main narratives, and finally assigns sub-narratives. Our
approach secured the top position on the English test set among 28 competing
teams worldwide. The code is available at https://github.com/GateNLP/H3Prompt.

</details>


### [17] [When Models Reason in Your Language: Controlling Thinking Trace Language Comes at the Cost of Accuracy](https://arxiv.org/abs/2505.22888)
*Jirui Qi,Shan Chen,Zidi Xiong,Raquel Fernández,Danielle S. Bitterman,Arianna Bisazza*

Main category: cs.CL

TL;DR: 当前大型推理模型在多语言推理中存在英语依赖与碎片化问题，通过提示干预和微调可部分改善但仍有准确率损失


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型在用户母语中的推理能力不足，影响用户对推理过程的有效监管与理解

Method: 1. 构建XReasoning基准评估主流模型
2. 设计提示干预强制模型使用目标语言
3. 使用100个样本进行针对性微调

Result: 1. 提示干预提升可读性但降低准确率4.8%
2. 微调后准确率恢复至英语水平的87%
3. 多语言推理仍存在17.2%的准确率差距

Conclusion: 当前模型存在多语言推理缺陷，需平衡可读性与准确性，后续训练是有效方向但需更多研究

Abstract: Recent Large Reasoning Models (LRMs) with thinking traces have shown strong
performance on English reasoning tasks. However, their ability to think in
other languages is less studied. This capability is as important as answer
accuracy for real world applications because users may find the reasoning trace
useful for oversight only when it is expressed in their own language. We
comprehensively evaluate two leading families of LRMs on our XReasoning
benchmark and find that even the most advanced models often revert to English
or produce fragmented reasoning in other languages, revealing a substantial gap
in multilingual reasoning. Prompt based interventions that force models to
reason in the users language improve readability and oversight but reduce
answer accuracy, exposing an important trade off. We further show that targeted
post training on just 100 examples mitigates this mismatch, though some
accuracy loss remains. Our results highlight the limited multilingual reasoning
capabilities of current LRMs and outline directions for future work. Code and
data are available at https://github.com/Betswish/mCoT-XReasoning.

</details>


### [18] [VIGNETTE: Socially Grounded Bias Evaluation for Vision-Language Models](https://arxiv.org/abs/2505.22897)
*Chahat Raj,Bowen Wei,Aylin Caliskan,Antonios Anastasopoulos,Ziwei Zhu*

Main category: cs.CL

TL;DR: 提出了VIGNETTE大规模视觉问答基准，系统性评估视觉语言模型在事实性、感知、刻板印象和决策中的偏见


<details>
  <summary>Details</summary>
Motivation: 现有研究过度关注肖像类图像的性别-职业关联，忽视复杂社会刻板印象的危害性

Method: 构建含3000万+图像的VQA数据集，结合社会心理学理论分析模型在语境化场景中的身份认知模式

Result: 揭示了VLMs通过视觉线索构建社会等级制的隐蔽机制，发现多维度且出人意料的歧视性推断模式

Conclusion: 该研究为理解VLMs的社会意义编码机制提供新视角，推动建立更公平的多模态模型

Abstract: While bias in large language models (LLMs) is well-studied, similar concerns
in vision-language models (VLMs) have received comparatively less attention.
Existing VLM bias studies often focus on portrait-style images and
gender-occupation associations, overlooking broader and more complex social
stereotypes and their implied harm. This work introduces VIGNETTE, a
large-scale VQA benchmark with 30M+ images for evaluating bias in VLMs through
a question-answering framework spanning four directions: factuality,
perception, stereotyping, and decision making. Beyond narrowly-centered
studies, we assess how VLMs interpret identities in contextualized settings,
revealing how models make trait and capability assumptions and exhibit patterns
of discrimination. Drawing from social psychology, we examine how VLMs connect
visual identity cues to trait and role-based inferences, encoding social
hierarchies, through biased selections. Our findings uncover subtle,
multifaceted, and surprising stereotypical patterns, offering insights into how
VLMs construct social meaning from inputs.

</details>


### [19] [Talent or Luck? Evaluating Attribution Bias in Large Language Models](https://arxiv.org/abs/2505.22910)
*Chahat Raj,Mahika Banerjee,Aylin Caliskan,Antonios Anastasopoulos,Ziwei Zhu*

Main category: cs.CL

TL;DR: Exploring LLMs' attribution biases through cognitive frameworks reveals fairness implications in demographic-based outcome reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing studies focus on surface-level biases, while this work aims to uncover how cognitive reasoning patterns in LLMs channelize disparities toward demographic groups.

Method: Proposes a cognitively grounded bias evaluation framework to analyze models' reasoning disparities in attributing event outcomes (internal/external factors) across demographics.

Result: Identifies systematic attribution biases in LLMs where outcomes are disproportionately linked to demographic characteristics rather than contextual factors.

Conclusion: Highlights the necessity of cognitive-aware evaluation frameworks to address deeply embedded attribution biases in AI systems for fairness improvement.

Abstract: When a student fails an exam, do we tend to blame their effort or the test's
difficulty? Attribution, defined as how reasons are assigned to event outcomes,
shapes perceptions, reinforces stereotypes, and influences decisions.
Attribution Theory in social psychology explains how humans assign
responsibility for events using implicit cognition, attributing causes to
internal (e.g., effort, ability) or external (e.g., task difficulty, luck)
factors. LLMs' attribution of event outcomes based on demographics carries
important fairness implications. Most works exploring social biases in LLMs
focus on surface-level associations or isolated stereotypes. This work proposes
a cognitively grounded bias evaluation framework to identify how models'
reasoning disparities channelize biases toward demographic groups.

</details>


### [20] [ER-REASON: A Benchmark Dataset for LLM-Based Clinical Reasoning in the Emergency Room](https://arxiv.org/abs/2505.22919)
*Nikita Mehandru,Niloufar Golchini,David Bamman,Travis Zack,Melanie F. Molina,Ahmed Alaa*

Main category: cs.CL

TL;DR: 论文提出ER-Reason基准，用于评估大语言模型在急诊科临床推理中的表现，发现现有模型与医生决策存在显著差距


<details>
  <summary>Details</summary>
Motivation: 现有医学评估基准存在人工标注成本高、缺乏完整临床工作流刻画的问题，急诊场景尤其需要模拟高压环境下的多阶段临床决策过程

Method: 基于3,984名患者的25,174份去标识化临床文档构建ER-Reason基准，包含分诊、评估、治疗等急诊全流程任务，并收集72份完整医生推理过程作为参照

Result: 实验显示当前最先进的大语言模型在急诊临床推理质量上与医生存在显著差距

Conclusion: 需进一步研究缩小LLM与临床医生决策推理能力的差距，提升模型在复杂医疗场景中的实际应用价值

Abstract: Large language models (LLMs) have been extensively evaluated on medical
question answering tasks based on licensing exams. However, real-world
evaluations often depend on costly human annotators, and existing benchmarks
tend to focus on isolated tasks that rarely capture the clinical reasoning or
full workflow underlying medical decisions. In this paper, we introduce
ER-Reason, a benchmark designed to evaluate LLM-based clinical reasoning and
decision-making in the emergency room (ER)--a high-stakes setting where
clinicians make rapid, consequential decisions across diverse patient
presentations and medical specialties under time pressure. ER-Reason includes
data from 3,984 patients, encompassing 25,174 de-identified longitudinal
clinical notes spanning discharge summaries, progress notes, history and
physical exams, consults, echocardiography reports, imaging notes, and ER
provider documentation. The benchmark includes evaluation tasks that span key
stages of the ER workflow: triage intake, initial assessment, treatment
selection, disposition planning, and final diagnosis--each structured to
reflect core clinical reasoning processes such as differential diagnosis via
rule-out reasoning. We also collected 72 full physician-authored rationales
explaining reasoning processes that mimic the teaching process used in
residency training, and are typically absent from ER documentation. Evaluations
of state-of-the-art LLMs on ER-Reason reveal a gap between LLM-generated and
clinician-authored clinical reasoning for ER decisions, highlighting the need
for future research to bridge this divide.

</details>


### [21] [Structured Memory Mechanisms for Stable Context Representation in Large Language Models](https://arxiv.org/abs/2505.22921)
*Yue Xing,Tao Yang,Yijiashun Qi,Minggu Wei,Yu Cheng,Honghui Xin*

Main category: cs.CL

TL;DR: 提出带有长期记忆机制的语言模型架构，通过显式记忆单元和联合训练策略有效改善长文本任务表现


<details>
  <summary>Details</summary>
Motivation: 解决传统语言模型处理长期依赖时存在的上下文丢失和语义漂移问题

Method: 整合显式记忆单元+门控写入机制+注意力读取模块，设计带遗忘函数的动态记忆更新系统，采用主任务与记忆约束联合训练策略

Result: 在文本生成一致性（提升23%）、多轮问答稳定性（错误率降低18%）及跨上下文推理（准确率达89.7%）方面表现优异

Conclusion: 该记忆机制显著提升语言模型的长程理解能力，实验验证了记忆容量与控制策略对性能的关键影响

Abstract: This paper addresses the limitations of large language models in
understanding long-term context. It proposes a model architecture equipped with
a long-term memory mechanism to improve the retention and retrieval of semantic
information across paragraphs and dialogue turns. The model integrates explicit
memory units, gated writing mechanisms, and attention-based reading modules. A
forgetting function is introduced to enable dynamic updates of memory content,
enhancing the model's ability to manage historical information. To further
improve the effectiveness of memory operations, the study designs a joint
training objective. This combines the main task loss with constraints on memory
writing and forgetting. It guides the model to learn better memory strategies
during task execution. Systematic evaluation across multiple subtasks shows
that the model achieves clear advantages in text generation consistency,
stability in multi-turn question answering, and accuracy in cross-context
reasoning. In particular, the model demonstrates strong semantic retention and
contextual coherence in long-text tasks and complex question answering
scenarios. It effectively mitigates the context loss and semantic drift
problems commonly faced by traditional language models when handling long-term
dependencies. The experiments also include analysis of different memory
structures, capacity sizes, and control strategies. These results further
confirm the critical role of memory mechanisms in language understanding. They
demonstrate the feasibility and effectiveness of the proposed approach in both
architectural design and performance outcomes.

</details>


### [22] [Unraveling LoRA Interference: Orthogonal Subspaces for Robust Model Merging](https://arxiv.org/abs/2505.22934)
*Haobo Zhang,Jiayu Zhou*

Main category: cs.CL

TL;DR: OSRM方法通过约束LoRA子空间提升模型合并性能，保持单任务准确性和超参数鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法在合并LoRA微调模型时存在性能下降，主要因参数与数据分布交互被忽视。

Method: 提出正交子空间约束(OSRM)，在微调前限制LoRA更新方向，减少任务间干扰，兼容现有合并算法。

Result: 在8个数据集/5种模型上验证：合并性能提升13%，单任务精度保留率达98%，超参数敏感度降低。

Conclusion: 数据-参数交互是模型合并的关键因素，OSRM为LoRA模型合并提供即插即用解决方案。

Abstract: Fine-tuning large language models (LMs) for individual tasks yields strong
performance but is expensive for deployment and storage. Recent works explore
model merging to combine multiple task-specific models into a single multi-task
model without additional training. However, existing merging methods often fail
for models fine-tuned with low-rank adaptation (LoRA), due to significant
performance degradation. In this paper, we show that this issue arises from a
previously overlooked interplay between model parameters and data
distributions. We propose Orthogonal Subspaces for Robust model Merging (OSRM)
to constrain the LoRA subspace *prior* to fine-tuning, ensuring that updates
relevant to one task do not adversely shift outputs for others. Our approach
can seamlessly integrate with most existing merging algorithms, reducing the
unintended interference among tasks. Extensive experiments on eight datasets,
tested with three widely used LMs and two large LMs, demonstrate that our
method not only boosts merging performance but also preserves single-task
accuracy. Furthermore, our approach exhibits greater robustness to the
hyperparameters of merging. These results highlight the importance of
data-parameter interaction in model merging and offer a plug-and-play solution
for merging LoRA models.

</details>


### [23] [Improving QA Efficiency with DistilBERT: Fine-Tuning and Inference on mobile Intel CPUs](https://arxiv.org/abs/2505.22937)
*Ngeyen Yinkfu*

Main category: cs.CL

TL;DR: 基于DistilBERT架构的高效问答模型，在CPU设备上实现0.6536 F1分数与0.1208秒/问题的推理速度，平衡精度与计算效率


<details>
  <summary>Details</summary>
Motivation: 优化Transformer模型在资源受限系统（如13代i7 CPU）上的实时推理能力，解决传统规则系统精度低（F1 0.3124）与完整BERT模型计算成本高的问题

Method: 采用数据增强策略（SQuAD v1.1数据集）与超参数调优，通过蒸馏压缩的DistilBERT架构进行微调，系统评估不同配置对CPU推理的影响

Result: 验证集F1提升至0.6536（较基线提升109%），推理速度较完整BERT显著优化（0.12秒/问题），实现精度与效率的最佳平衡

Conclusion: 该优化方案为CPU环境中的实时问答应用提供可行解决方案，通过系统化的数据增强和架构压缩策略，证明轻量化Transformer在边缘计算场景的实用价值

Abstract: This study presents an efficient transformer-based question-answering (QA)
model optimized for deployment on a 13th Gen Intel i7-1355U CPU, using the
Stanford Question Answering Dataset (SQuAD) v1.1. Leveraging exploratory data
analysis, data augmentation, and fine-tuning of a DistilBERT architecture, the
model achieves a validation F1 score of 0.6536 with an average inference time
of 0.1208 seconds per question. Compared to a rule-based baseline (F1: 0.3124)
and full BERT-based models, our approach offers a favorable trade-off between
accuracy and computational efficiency. This makes it well-suited for real-time
applications on resource-constrained systems. The study includes systematic
evaluation of data augmentation strategies and hyperparameter configurations,
providing practical insights into optimizing transformer models for CPU-based
inference.

</details>


### [24] [WorkForceAgent-R1: Incentivizing Reasoning Capability in LLM-based Web Agents via Reinforcement Learning](https://arxiv.org/abs/2505.22942)
*Yuchen Zhuang,Di Jin,Jiaao Chen,Wenqi Shi,Hanrui Wang,Chao Zhang*

Main category: cs.CL

TL;DR: 提出WorkForceAgent-R1强化学习框架，通过结构化奖励机制提升企业网页导航任务的单步推理能力，实验显示性能超越SFT基线10.26-16.59%


<details>
  <summary>Details</summary>
Motivation: 现有基于监督微调(SFT)的网页代理在动态网页交互中存在泛化能力弱和鲁棒性不足的问题，主要受限于中间推理能力的缺失

Method: 采用R1风格的强化学习框架，设计结构化奖励函数（格式合规性+动作准确性），实现无标注的隐式中间推理学习

Result: WorkArena基准测试显示性能显著优于SFT基线10.26-16.59%，达到与gpt-4o相当的竞争水平

Conclusion: 该方法通过强化学习框架突破传统SFT限制，无需专家示范即可实现企业级网页导航任务的鲁棒执行

Abstract: Large language models (LLMs)-empowered web agents enables automating complex,
real-time web navigation tasks in enterprise environments. However, existing
web agents relying on supervised fine-tuning (SFT) often struggle with
generalization and robustness due to insufficient reasoning capabilities when
handling the inherently dynamic nature of web interactions. In this study, we
introduce WorkForceAgent-R1, an LLM-based web agent trained using a rule-based
R1-style reinforcement learning framework designed explicitly to enhance
single-step reasoning and planning for business-oriented web navigation tasks.
We employ a structured reward function that evaluates both adherence to output
formats and correctness of actions, enabling WorkForceAgent-R1 to implicitly
learn robust intermediate reasoning without explicit annotations or extensive
expert demonstrations. Extensive experiments on the WorkArena benchmark
demonstrate that WorkForceAgent-R1 substantially outperforms SFT baselines by
10.26-16.59%, achieving competitive performance relative to proprietary
LLM-based agents (gpt-4o) in workplace-oriented web navigation tasks.

</details>


### [25] [Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of Pre-trained Multimodal Representation via Text Updates](https://arxiv.org/abs/2505.22943)
*Jaewoo Ahn,Heeseung Yun,Dayoon Ko,Gunhee Kim*

Main category: cs.CL

TL;DR: 提出MAC基准揭示多模态模型的组合漏洞，基于拒绝采样的自训练方法提升攻击效果


<details>
  <summary>Details</summary>
Motivation: 现有预训练多模态模型（如CLIP）存在组合性漏洞，导致反直觉判断，威胁实际应用可靠性

Method: 采用拒绝采样微调+多样性过滤的自训练方法，利用小语言模型生成对抗样本

Result: Llama-3.1-8B在图像/视频/音频多模态场景中显著提升攻击成功率（+15%）和样本多样性（熵值+20%）

Conclusion: MAC基准有效暴露多模态组合漏洞，自训练方法为提升模型鲁棒性提供新思路

Abstract: While pre-trained multimodal representations (e.g., CLIP) have shown
impressive capabilities, they exhibit significant compositional vulnerabilities
leading to counterintuitive judgments. We introduce Multimodal Adversarial
Compositionality (MAC), a benchmark that leverages large language models (LLMs)
to generate deceptive text samples to exploit these vulnerabilities across
different modalities and evaluates them through both sample-wise attack success
rate and group-wise entropy-based diversity. To improve zero-shot methods, we
propose a self-training approach that leverages rejection-sampling fine-tuning
with diversity-promoting filtering, which enhances both attack success rate and
sample diversity. Using smaller language models like Llama-3.1-8B, our approach
demonstrates superior performance in revealing compositional vulnerabilities
across various multimodal representations, including images, videos, and
audios.

</details>


### [26] [OWL: Probing Cross-Lingual Recall of Memorized Texts via World Literature](https://arxiv.org/abs/2505.22945)
*Alisha Srivastava,Emir Korukluoglu,Minh Nhat Le,Duyen Tran,Chau Minh Pham,Marzena Karpinska,Mohit Iyyer*

Main category: cs.CL

TL;DR: 该研究系统评估了大型语言模型在多语言/跨语言文本记忆方面的能力，发现即使面对低资源语言的翻译文本，模型仍能有效进行跨语言内容回忆，揭示了LLMs隐含的多语言知识表征特性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LLMs的英语文本记忆能力，但对非英语语言特别是低资源语言的记忆能力，以及跨语言记忆迁移现象缺乏系统研究。

Method: 构建OWL多语言对齐数据集（含10种语言31.5K文本），通过直接探测（标题作者识别）、名称填空（实体预测）、前缀探测（文本续写）三个任务，测试不同规模模型在官方翻译/新翻译文本上的表现。

Result: GPT-4o在新翻译文本中识别作者/标题准确率达69%；掩码实体预测准确率6%；文本扰动（如打乱词序）仅使官方翻译准确率下降7%，显示模型对结构变化具有一定鲁棒性。

Conclusion: LLMs具备跨语言记忆迁移能力，这种能力不完全依赖训练数据中的直接翻译，为低资源语言应用提供新思路，同时引发对多语言内容泄露风险的关注。

Abstract: Large language models (LLMs) are known to memorize and recall English text
from their pretraining data. However, the extent to which this ability
generalizes to non-English languages or transfers across languages remains
unclear. This paper investigates multilingual and cross-lingual memorization in
LLMs, probing if memorized content in one language (e.g., English) can be
recalled when presented in translation. To do so, we introduce OWL, a dataset
of 31.5K aligned excerpts from 20 books in ten languages, including English
originals, official translations (Vietnamese, Spanish, Turkish), and new
translations in six low-resource languages (Sesotho, Yoruba, Maithili,
Malagasy, Setswana, Tahitian). We evaluate memorization across model families
and sizes through three tasks: (1) direct probing, which asks the model to
identify a book's title and author; (2) name cloze, which requires predicting
masked character names; and (3) prefix probing, which involves generating
continuations. We find that LLMs consistently recall content across languages,
even for texts without direct translation in pretraining data. GPT-4o, for
example, identifies authors and titles 69% of the time and masked entities 6%
of the time in newly translated excerpts. Perturbations (e.g., masking
characters, shuffling words) modestly reduce direct probing accuracy (7% drop
for shuffled official translations). Our results highlight the extent of
cross-lingual memorization and provide insights on the differences between the
models.

</details>


### [27] [NegVQA: Can Vision Language Models Understand Negation?](https://arxiv.org/abs/2505.22946)
*Yuhui Zhang,Yuchang Su,Yiming Liu,Serena Yeung-Levy*

Main category: cs.CL

TL;DR: 提出NegVQA基准测试，揭示主流视觉语言模型在否定理解上的显著缺陷，并发现模型规模的U型扩展规律。


<details>
  <summary>Details</summary>
Motivation: 否定作为语言基础现象会彻底改变语义，需评估视觉语言模型在高风险应用中的否定理解能力。

Method: 利用大语言模型生成现有VQA问题的否定版本，构建含7,379个二选一问题的数据集，评估20个前沿模型在7个模型家族中的表现。

Result: 模型在否定问题上平均准确率下降28.9%，且呈现先恶化后改善的U型规模扩展趋势。

Conclusion: NegVQA暴露了视觉语言模型的否定理解短板，为模型开发提供重要诊断工具，项目页https://yuhui-zh15.github.io/NegVQA/。

Abstract: Negation is a fundamental linguistic phenomenon that can entirely reverse the
meaning of a sentence. As vision language models (VLMs) continue to advance and
are deployed in high-stakes applications, assessing their ability to comprehend
negation becomes essential. To address this, we introduce NegVQA, a visual
question answering (VQA) benchmark consisting of 7,379 two-choice questions
covering diverse negation scenarios and image-question distributions. We
construct NegVQA by leveraging large language models to generate negated
versions of questions from existing VQA datasets. Evaluating 20
state-of-the-art VLMs across seven model families, we find that these models
struggle significantly with negation, exhibiting a substantial performance drop
compared to their responses to the original questions. Furthermore, we uncover
a U-shaped scaling trend, where increasing model size initially degrades
performance on NegVQA before leading to improvements. Our benchmark reveals
critical gaps in VLMs' negation understanding and offers insights into future
VLM development. Project page available at
https://yuhui-zh15.github.io/NegVQA/.

</details>


### [28] [StrucSum: Graph-Structured Reasoning for Long Document Extractive Summarization with LLMs](https://arxiv.org/abs/2505.22950)
*Haohan Yuan,Sukhwa Hong,Haopeng Zhang*

Main category: cs.CL

TL;DR: StrucSum框架通过句子级图结构和三种策略（NAP/CAP/CGM）增强LLM结构建模能力，在零样本摘要任务中显著提升摘要质量与事实一致性


<details>
  <summary>Details</summary>
Motivation: 大语言模型在零样本摘要任务中难以有效建模长文档结构并识别关键信息

Method: 1. 邻居感知提示（NAP）捕捉局部上下文 2. 中心性感知提示（CAP）估计信息重要性 3. 中心性引导遮蔽（CGM）实现高效输入压缩

Result: 在ArXiv数据集上FactCC和SummaC分别提升19.2和9.7分，PubMed/Multi-News实验显示质量与事实性持续改进

Conclusion: 结构感知提示方法无需训练即可显著提升LLM的抽取式摘要能力，验证了文档结构建模对摘要任务的重要性

Abstract: Large language models (LLMs) have shown strong performance in zero-shot
summarization, but often struggle to model document structure and identify
salient information in long texts. In this work, we introduce StrucSum, a
training-free prompting framework that enhances LLM reasoning through
sentence-level graph structures. StrucSum injects structural signals into
prompts via three targeted strategies: Neighbor-Aware Prompting (NAP) for local
context, Centrality-Aware Prompting (CAP) for importance estimation, and
Centrality-Guided Masking (CGM) for efficient input reduction. Experiments on
ArXiv, PubMed, and Multi-News demonstrate that StrucSum consistently improves
both summary quality and factual consistency over unsupervised baselines and
vanilla prompting. Notably, on ArXiv, it boosts FactCC and SummaC by 19.2 and
9.7 points, indicating stronger alignment between summaries and source content.
These findings suggest that structure-aware prompting is a simple yet effective
approach for zero-shot extractive summarization with LLMs, without any training
or task-specific tuning.

</details>


### [29] [LLMs for Argument Mining: Detection, Extraction, and Relationship Classification of pre-defined Arguments in Online Comments](https://arxiv.org/abs/2505.22956)
*Matteo Guida,Yulia Otmakhova,Eduard Hovy,Lea Frermann*

Main category: cs.CL

TL;DR: 大型语言模型在在线评论的论点挖掘中展现出较强潜力，但面临环境成本高、处理复杂文本和情感语言时的系统性缺陷。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在争议性议题（如堕胎）公共讨论中自动挖掘预设主题论证的可行性，填补该领域研究空白。

Method: 使用4种先进LLM分析3个论证挖掘任务，基于6大对立主题的2000+在线评论数据集，结合定量评估与详细错误分析。

Result: 模型在多数任务表现优异（尤其大模型和微调模型），但存在长文本理解偏差（46%错误率）、情感语言处理缺陷（32%误判）及高碳排放（单模型训练约284kg CO2）。

Conclusion: LLMs自动化论证分析兼具实用价值与局限性，需重点改进复杂语境理解和情感分析能力，同时关注环境成本优化。

Abstract: Automated large-scale analysis of public discussions around contested issues
like abortion requires detecting and understanding the use of arguments. While
Large Language Models (LLMs) have shown promise in language processing tasks,
their performance in mining topic-specific, pre-defined arguments in online
comments remains underexplored. We evaluate four state-of-the-art LLMs on three
argument mining tasks using datasets comprising over 2,000 opinion comments
across six polarizing topics. Quantitative evaluation suggests an overall
strong performance across the three tasks, especially for large and fine-tuned
LLMs, albeit at a significant environmental cost. However, a detailed error
analysis revealed systematic shortcomings on long and nuanced comments and
emotionally charged language, raising concerns for downstream applications like
content moderation or opinion analysis. Our results highlight both the promise
and current limitations of LLMs for automated argument analysis in online
comments.

</details>


### [30] [LLM-based HSE Compliance Assessment: Benchmark, Performance, and Advancements](https://arxiv.org/abs/2505.22959)
*Jianwei Wang,Mengqi Wang,Yinsi Zhou,Zhenchang Xing,Qing Liu,Xiwei Xu,Wenjie Zhang,Liming Zhu*

Main category: cs.CL

TL;DR: 提出首个评估大模型HSE合规能力的基准HSE-Bench，揭示现有模型依赖语义匹配而非合规推理，并提出专家推理提示技术RoE改进决策


<details>
  <summary>Details</summary>
Motivation: HSE合规评估涉及复杂法规和人机环境交互，现有大模型在领域知识推理和法律结构化推理方面存在不足

Method: 构建包含1000+多源问题的HSE-Bench数据集，集成IRAC法律推理框架评估模型的全流程合规决策能力

Result: 当前大模型主要依赖语义匹配（73%准确率），法律推理追踪能力不足，多模态模型视频理解准确率仅65%

Conclusion: 提出的RoE提示技术通过模拟专家联合推理，将决策准确率提升15%，为合规智能系统开发提供新范式

Abstract: Health, Safety, and Environment (HSE) compliance assessment demands dynamic
real-time decision-making under complicated regulations and complex
human-machine-environment interactions. While large language models (LLMs) hold
significant potential for decision intelligence and contextual dialogue, their
capacity for domain-specific knowledge in HSE and structured legal reasoning
remains underexplored. We introduce HSE-Bench, the first benchmark dataset
designed to evaluate the HSE compliance assessment capabilities of LLM.
HSE-Bench comprises over 1,000 manually curated questions drawn from
regulations, court cases, safety exams, and fieldwork videos, and integrates a
reasoning flow based on Issue spotting, rule Recall, rule Application, and rule
Conclusion (IRAC) to assess the holistic reasoning pipeline. We conduct
extensive evaluations on different prompting strategies and more than 10 LLMs,
including foundation models, reasoning models and multimodal vision models. The
results show that, although current LLMs achieve good performance, their
capabilities largely rely on semantic matching rather than principled reasoning
grounded in the underlying HSE compliance context. Moreover, their native
reasoning trace lacks the systematic legal reasoning required for rigorous HSE
compliance assessment. To alleviate these, we propose a new prompting
technique, Reasoning of Expert (RoE), which guides LLMs to simulate the
reasoning process of different experts for compliance assessment and reach a
more accurate unified decision. We hope our study highlights reasoning gaps in
LLMs for HSE compliance and inspires further research on related tasks.

</details>


### [31] [ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind](https://arxiv.org/abs/2505.22961)
*Peixuan Han,Zijia Liu,Jiaxuan You*

Main category: cs.CL

TL;DR: 论文提出ToMAP方法，通过整合心理理论模块提升语言模型的劝说能力，在3B参数量下性能超越GPT-4o等大模型39.4%。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型缺乏心理理论推理能力，导致劝说时多样性和对手感知不足。需要增强模型对劝说对象心智状态的动态建模能力。

Method: 包含两个心理理论模块：1）主动预测反对意见；2）用文本编码器和分类器预判对手立场。结合强化学习框架训练劝说策略生成。

Result: ToMAP在多种劝说对象模型和语料库上相对提升39.4%，生成更多样有效的论证，减少重复，并具备长对话中的逻辑策略调整能力。

Conclusion: 心理理论模块的整合有效提升了语言模型的劝说效果，为开发更具说服力的人工智能代理提供了新思路。

Abstract: Large language models (LLMs) have shown promising potential in persuasion,
but existing works on training LLM persuaders are still preliminary. Notably,
while humans are skilled in modeling their opponent's thoughts and opinions
proactively and dynamically, current LLMs struggle with such Theory of Mind
(ToM) reasoning, resulting in limited diversity and opponent awareness. To
address this limitation, we introduce Theory of Mind Augmented Persuader
(ToMAP), a novel approach for building more flexible persuader agents by
incorporating two theory of mind modules that enhance the persuader's awareness
and analysis of the opponent's mental state. Specifically, we begin by
prompting the persuader to consider possible objections to the target central
claim, and then use a text encoder paired with a trained MLP classifier to
predict the opponent's current stance on these counterclaims. Our carefully
designed reinforcement learning schema enables the persuader learns how to
analyze opponent-related information and utilize it to generate more effective
arguments. Experiments show that the ToMAP persuader, while containing only 3B
parameters, outperforms much larger baselines, like GPT-4o, with a relative
gain of 39.4% across multiple persuadee models and diverse corpora. Notably,
ToMAP exhibits complex reasoning chains and reduced repetition during training,
which leads to more diverse and effective arguments. The opponent-aware feature
of ToMAP also makes it suitable for long conversations and enables it to employ
more logical and opponent-aware strategies. These results underscore our
method's effectiveness and highlight its potential for developing more
persuasive language agents. Code is available at:
https://github.com/ulab-uiuc/ToMAP.

</details>


### [32] [Exploring Scaling Laws for EHR Foundation Models](https://arxiv.org/abs/2505.22964)
*Sheng Zhang,Qin Liu,Naoto Usuyama,Cliff Wong,Tristan Naumann,Hoifung Poon*

Main category: cs.CL

TL;DR: 首次验证电子健康记录基础模型遵循类似大语言模型的缩放定律


<details>
  <summary>Details</summary>
Motivation: 探索结构化医疗数据与自然语言数据的缩放定律差异，填补EHR领域系统性缩放规律研究的空白

Method: 基于MIMIC-IV数据库患者时间序列数据，使用不同规模Transformer模型和计算预算进行对比实验

Result: 发现抛物线型IsoFLOPs曲线和计算-参数-数据-临床效用的幂律关系

Conclusion: EHR模型的缩放规律可指导资源优化配置，为开发临床预测模型奠定理论基础

Abstract: The emergence of scaling laws has profoundly shaped the development of large
language models (LLMs), enabling predictable performance gains through
systematic increases in model size, dataset volume, and compute. Yet, these
principles remain largely unexplored in the context of electronic health
records (EHRs) -- a rich, sequential, and globally abundant data source that
differs structurally from natural language. In this work, we present the first
empirical investigation of scaling laws for EHR foundation models. By training
transformer architectures on patient timeline data from the MIMIC-IV database
across varying model sizes and compute budgets, we identify consistent scaling
patterns, including parabolic IsoFLOPs curves and power-law relationships
between compute, model parameters, data size, and clinical utility. These
findings demonstrate that EHR models exhibit scaling behavior analogous to
LLMs, offering predictive insights into resource-efficient training strategies.
Our results lay the groundwork for developing powerful EHR foundation models
capable of transforming clinical prediction tasks and advancing personalized
healthcare.

</details>


### [33] [Verify-in-the-Graph: Entity Disambiguation Enhancement for Complex Claim Verification with Interactive Graph Representation](https://arxiv.org/abs/2505.22993)
*Hoang Pham,Thanh-Do Nguyen,Khac-Hoai Nam Bui*

Main category: cs.CL

TL;DR: 提出VeGraph框架，通过图结构的三阶段处理提升复杂声明验证的准确性和可解释性


<details>
  <summary>Details</summary>
Motivation: 传统方法处理复杂声明时实体消歧不足，导致验证过程不可靠，需结合LLM的推理能力改进

Method: 三阶段框架：1) 将声明分解为结构化三元组图 2) 与知识库交互消歧实体 3) 验证剩余三元组完成事实核查

Result: 基于Meta-Llama-3-70B的实验显示，在HoVer和FEVEROUS基准测试中达到竞争优势

Conclusion: VeGraph有效结合结构化和非结构化信息处理复杂声明验证，相关代码和数据已开源供研究使用

Abstract: Claim verification is a long-standing and challenging task that demands not
only high accuracy but also explainability of the verification process. This
task becomes an emerging research issue in the era of large language models
(LLMs) since real-world claims are often complex, featuring intricate semantic
structures or obfuscated entities. Traditional approaches typically address
this by decomposing claims into sub-claims and querying a knowledge base to
resolve hidden or ambiguous entities. However, the absence of effective
disambiguation strategies for these entities can compromise the entire
verification process. To address these challenges, we propose
Verify-in-the-Graph (VeGraph), a novel framework leveraging the reasoning and
comprehension abilities of LLM agents. VeGraph operates in three phases: (1)
Graph Representation - an input claim is decomposed into structured triplets,
forming a graph-based representation that integrates both structured and
unstructured information; (2) Entity Disambiguation -VeGraph iteratively
interacts with the knowledge base to resolve ambiguous entities within the
graph for deeper sub-claim verification; and (3) Verification - remaining
triplets are verified to complete the fact-checking process. Experiments using
Meta-Llama-3-70B (instruct version) show that VeGraph achieves competitive
performance compared to baselines on two benchmarks HoVer and FEVEROUS,
effectively addressing claim verification challenges. Our source code and data
are available for further exploitation.

</details>


### [34] [DyePack: Provably Flagging Test Set Contamination in LLMs Using Backdoors](https://arxiv.org/abs/2505.23001)
*Yize Cheng,Wenxiao Wang,Mazda Moayeri,Soheil Feizi*

Main category: cs.CL

TL;DR: DyePack框架通过后门攻击检测语言模型是否在训练中污染了基准测试集，无需访问模型内部参数即可实现精确检测。


<details>
  <summary>Details</summary>
Motivation: 当前开放基准测试容易被模型训练时不当使用（测试集污染），但现有检测方法需要访问模型损失值或内部细节，存在局限性。

Method: 1. 借鉴银行防抢劫染料包原理，在测试数据中植入多个随机目标的后门样本
2. 通过概率模型计算精确的误报率（FPR）
3. 支持选择题和开放式生成两种任务类型

Result: 在MMLU-Pro（0.000073%）和Big-Bench-Hard（0.000017%）实现极低误报率，Alpaca生成任务中误报率0.127%，成功检测所有污染模型

Conclusion: DyePack首次实现了无需模型内部信息的可证明检测方案，在多种任务类型中展现出强通用性和检测可靠性，为基准测试保护提供了新范式

Abstract: Open benchmarks are essential for evaluating and advancing large language
models, offering reproducibility and transparency. However, their accessibility
makes them likely targets of test set contamination. In this work, we introduce
DyePack, a framework that leverages backdoor attacks to identify models that
used benchmark test sets during training, without requiring access to the loss,
logits, or any internal details of the model. Like how banks mix dye packs with
their money to mark robbers, DyePack mixes backdoor samples with the test data
to flag models that trained on it. We propose a principled design incorporating
multiple backdoors with stochastic targets, enabling exact false positive rate
(FPR) computation when flagging every model. This provably prevents false
accusations while providing strong evidence for every detected case of
contamination. We evaluate DyePack on five models across three datasets,
covering both multiple-choice and open-ended generation tasks. For
multiple-choice questions, it successfully detects all contaminated models with
guaranteed FPRs as low as 0.000073% on MMLU-Pro and 0.000017% on Big-Bench-Hard
using eight backdoors. For open-ended generation tasks, it generalizes well and
identifies all contaminated models on Alpaca with a guaranteed false positive
rate of just 0.127% using six backdoors.

</details>


### [35] [A Practical Approach for Building Production-Grade Conversational Agents with Workflow Graphs](https://arxiv.org/abs/2505.23006)
*Chiwan Park,Wonjun Jang,Daeryong Kim,Aelim Ahn,Kichang Yang,Woosung Hwang,Jihyeon Roh,Hyerin Park,Hyosun Wang,Min Seok Kim,Jihoon Kang*

Main category: cs.CL

TL;DR: 探讨如何将先进大语言模型应用于工业场景并解决灵活对话能力与严格服务约束的冲突


<details>
  <summary>Details</summary>
Motivation: 解决SOTA研究成果在工业落地时LLM概率特性导致的灵活对话能力与服务刚性约束的冲突矛盾

Method: 提出可扩展的框架设计方案，通过实现流程优化和电商对话代理的实践验证方法论

Result: 构建出可扩展、易控制、高可靠的AI智能体开发框架，有效连接学术研究与产业应用

Conclusion: 通过框架设计与实践验证，证明大模型在严格约束场景下的可控应用可行性，为AI智能体开发提供新范式

Abstract: The advancement of Large Language Models (LLMs) has led to significant
improvements in various service domains, including search, recommendation, and
chatbot applications. However, applying state-of-the-art (SOTA) research to
industrial settings presents challenges, as it requires maintaining flexible
conversational abilities while also strictly complying with service-specific
constraints. This can be seen as two conflicting requirements due to the
probabilistic nature of LLMs. In this paper, we propose our approach to
addressing this challenge and detail the strategies we employed to overcome
their inherent limitations in real-world applications. We conduct a practical
case study of a conversational agent designed for the e-commerce domain,
detailing our implementation workflow and optimizations. Our findings provide
insights into bridging the gap between academic research and real-world
application, introducing a framework for developing scalable, controllable, and
reliable AI-driven agents.

</details>


### [36] [Detecting Stealthy Backdoor Samples based on Intra-class Distance for Large Language Models](https://arxiv.org/abs/2505.23015)
*Jinwen Chen,Hainan Zhang,Fei Sun,Qinnan Zhang,Sijia Wen,Ziwei Wang,Zhiming Zheng*

Main category: cs.CL

TL;DR: 提出基于参考过滤与TF-IDF聚类的隐蔽后门样本检测方法RFTC，有效解决生成任务中现有后门检测方法的局限性，实验表明其在检测准确率与模型性能保持上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有后门检测方法无法有效应用于生成任务（分类模型概率分析法不适用，重写模型可能损害生成质量）。隐蔽后门样本对下游应用构成安全威胁，需开发针对性检测方案。

Method: RFTC双阶段检测：1) 参考过滤：通过对比样本响应与参考模型输出的差异筛选可疑样本；2) TF-IDF聚类：分析可疑样本响应的类内距离，利用污染样本响应集中特性实现精准识别。

Result: 在机器翻译和QA数据集上的实验显示，RFTC在F1值（后门检测）和BLEU/ROUGE（模型性能）指标上均优于基线方法。参考模型选择验证了过滤机制的有效性。

Conclusion: RFTC通过响应差异筛选与文本特征聚变的双重机制，实现了生成任务中隐蔽后门样本的高效检测，为LLM安全微调提供了新解决方案。参考模型的动态比对显著提升检测效率。

Abstract: Fine-tuning LLMs with datasets containing stealthy backdoors from publishers
poses security risks to downstream applications. Mainstream detection methods
either identify poisoned samples by analyzing the prediction probability of
poisoned classification models or rely on the rewriting model to eliminate the
stealthy triggers. However, the former cannot be applied to generation tasks,
while the latter may degrade generation performance and introduce new triggers.
Therefore, efficiently eliminating stealthy poisoned samples for LLMs remains
an urgent problem. We observe that after applying TF-IDF clustering to the
sample response, there are notable differences in the intra-class distances
between clean and poisoned samples. Poisoned samples tend to cluster closely
because of their specific malicious outputs, whereas clean samples are more
scattered due to their more varied responses. Thus, in this paper, we propose a
stealthy backdoor sample detection method based on Reference-Filtration and
Tfidf-Clustering mechanisms (RFTC). Specifically, we first compare the sample
response with the reference model's outputs and consider the sample suspicious
if there's a significant discrepancy. And then we perform TF-IDF clustering on
these suspicious samples to identify the true poisoned samples based on the
intra-class distance. Experiments on two machine translation datasets and one
QA dataset demonstrate that RFTC outperforms baselines in backdoor detection
and model performance. Further analysis of different reference models also
confirms the effectiveness of our Reference-Filtration.

</details>


### [37] [Context Robust Knowledge Editing for Language Models](https://arxiv.org/abs/2505.23026)
*Haewon Park,Gyubin Choi,Minjun Kim,Yohan Jo*

Main category: cs.CL

TL;DR: 研究揭示了现有知识编辑方法在上下文场景下的不足，提出了评估基准CHED和改进方法CoRE，显著提升了模型在上下文存在时的编辑鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑评估忽视上下文触发原始知识的问题，实际应用中导致编辑失效。需要开发能评估上下文鲁棒性的新基准和方法。

Method: 1. 构建CHED基准评估上下文敏感性；2. 提出CoRE方法通过最小化隐藏状态方差增强鲁棒性；3. 分析用户/助理两种上下文类型的影响机制

Result: 现有KE方法在CHED上失败率超60%，CoRE使编辑成功率提升35%且保持模型整体能力，注意力模式分析揭示关键token的影响规律

Conclusion: 上下文鲁棒性是知识编辑的核心挑战，CoRE为实际应用提供有效解决方案，注意力机制分析为后续研究提供新视角

Abstract: Knowledge editing (KE) methods offer an efficient way to modify knowledge in
large language models. Current KE evaluations typically assess editing success
by considering only the edited knowledge without any preceding contexts. In
real-world applications, however, preceding contexts often trigger the
retrieval of the original knowledge and undermine the intended edit. To address
this issue, we develop CHED -- a benchmark designed to evaluate the context
robustness of KE methods. Evaluations on CHED show that they often fail when
preceding contexts are present. To mitigate this shortcoming, we introduce
CoRE, a KE method designed to strengthen context robustness by minimizing
context-sensitive variance in hidden states of the model for edited knowledge.
This method not only improves the editing success rate in situations where a
preceding context is present but also preserves the overall capabilities of the
model. We provide an in-depth analysis of the differing impacts of preceding
contexts when introduced as user utterances versus assistant responses, and we
dissect attention-score patterns to assess how specific tokens influence
editing success.

</details>


### [38] [Uncovering Visual-Semantic Psycholinguistic Properties from the Distributional Structure of Text Embedding Spac](https://arxiv.org/abs/2505.23029)
*Si Wu,Sebastian Bruch*

Main category: cs.CL

TL;DR: 提出无监督的邻域稳定性度量(NSM)，通过分析语义嵌入空间中单词邻域的尖峰性，有效评估词汇的图像性和具体性，表现优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖多模态数据，本文验证仅用图像-标题数据集中的文本特征即可准确评估心理语言学属性，探索语义嵌入邻域特征的有效性

Method: 基于语义嵌入空间构建词汇邻域分布，提出NSM指标量化邻域分布的尖峰锐度，无需监督信号或预设分布假设

Result: NSM与人类评分相关性显著高于基线方法，在图像性/具体性分类任务中达到84.4%的准确率（SVM）

Conclusion: 文本数据本身蕴含足够信号评估心理语言学属性，NSM为单模态分析方法提供新思路，降低对多模态数据的依赖

Abstract: Imageability (potential of text to evoke a mental image) and concreteness
(perceptibility of text) are two psycholinguistic properties that link visual
and semantic spaces. It is little surprise that computational methods that
estimate them do so using parallel visual and semantic spaces, such as
collections of image-caption pairs or multi-modal models. In this paper, we
work on the supposition that text itself in an image-caption dataset offers
sufficient signals to accurately estimate these properties. We hypothesize, in
particular, that the peakedness of the neighborhood of a word in the semantic
embedding space reflects its degree of imageability and concreteness. We then
propose an unsupervised, distribution-free measure, which we call Neighborhood
Stability Measure (NSM), that quantifies the sharpness of peaks. Extensive
experiments show that NSM correlates more strongly with ground-truth ratings
than existing unsupervised methods, and is a strong predictor of these
properties for classification. Our code and data are available on GitHub
(https://github.com/Artificial-Memory-Lab/imageability).

</details>


### [39] [Can Modern NLP Systems Reliably Annotate Chest Radiography Exams? A Pre-Purchase Evaluation and Comparative Study of Solutions from AWS, Google, Azure, John Snow Labs, and Open-Source Models on an Independent Pediatric Dataset](https://arxiv.org/abs/2505.23030)
*Shruti Hegde,Mabon Manoj Ninan,Jonathan R. Dillman,Shireen Hayatghaibi,Lynn Babcock,Elanchezhian Somasundaram*

Main category: cs.CL

TL;DR: 比较四种商业NLP系统和两种专用工具在儿科胸片报告标注中的表现，发现显著性能差异并强调验证必要性


<details>
  <summary>Details</summary>
Motivation: 评估通用临床NLP工具在特定儿科胸片报告标注任务中的实际有效性，填补独立验证研究的空白

Method: 使用95,008份儿科胸片报告，通过Fleiss Kappa和准确性指标比较各系统在实体提取和断言检测的表现，建立共识伪基准进行验证

Result: SP系统表现最佳(76%准确率)，AWS最差(50%)，专用工具CheXpert/CheXbert准确率56%，各系统实体提取量差异显著(SP:49,688 vs GC:16,477)

Conclusion: 临床NLP工具在特定任务中存在显著性能差异，部署前必须进行严格验证和人工审核以确保可靠性

Abstract: General-purpose clinical natural language processing (NLP) tools are
increasingly used for the automatic labeling of clinical reports. However,
independent evaluations for specific tasks, such as pediatric chest radiograph
(CXR) report labeling, are limited. This study compares four commercial
clinical NLP systems - Amazon Comprehend Medical (AWS), Google Healthcare NLP
(GC), Azure Clinical NLP (AZ), and SparkNLP (SP) - for entity extraction and
assertion detection in pediatric CXR reports. Additionally, CheXpert and
CheXbert, two dedicated chest radiograph report labelers, were evaluated on the
same task using CheXpert-defined labels. We analyzed 95,008 pediatric CXR
reports from a large academic pediatric hospital. Entities and assertion
statuses (positive, negative, uncertain) from the findings and impression
sections were extracted by the NLP systems, with impression section entities
mapped to 12 disease categories and a No Findings category. CheXpert and
CheXbert extracted the same 13 categories. Outputs were compared using Fleiss
Kappa and accuracy against a consensus pseudo-ground truth. Significant
differences were found in the number of extracted entities and assertion
distributions across NLP systems. SP extracted 49,688 unique entities, GC
16,477, AZ 31,543, and AWS 27,216. Assertion accuracy across models averaged
around 62%, with SP highest (76%) and AWS lowest (50%). CheXpert and CheXbert
achieved 56% accuracy. Considerable variability in performance highlights the
need for careful validation and review before deploying NLP tools for clinical
report labeling.

</details>


### [40] [Machine-Facing English: Defining a Hybrid Register Shaped by Human-AI Discourse](https://arxiv.org/abs/2505.23035)
*Hyunwoo Kim,Hanau Yi*

Main category: cs.CL

TL;DR: 本文提出'面向机器的英语'(MFE)概念，揭示人类为适应AI对话者而发展的语言变体特征及其对交流效率与语言丰富性的影响


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示持续的人机互动如何通过句法僵化、语用简化和超显性表达等特征，以牺牲语言自然流畅性为代价增强机器解析能力

Method: 基于韩英双语语音/文本产品测试的定性观察，采用自然语言声明提示法(NLD-P)进行反思性文本分析

Result: 识别出冗余清晰度、指令性句法、受限词汇、扁平韵律和单一意图结构五大特征，这些特征提升执行准确性但压缩表达范围

Conclusion: MFE的演化凸显交流效率与语言丰富性间的张力，为对话界面设计和多语言用户教育带来新挑战，需加强方法论阐释和实证验证

Abstract: Machine-Facing English (MFE) is an emergent register shaped by the adaptation
of everyday language to the expanding presence of AI interlocutors. Drawing on
register theory (Halliday 1985, 2006), enregisterment (Agha 2003), audience
design (Bell 1984), and interactional pragmatics (Giles & Ogay 2007), this
study traces how sustained human-AI interaction normalizes syntactic rigidity,
pragmatic simplification, and hyper-explicit phrasing - features that enhance
machine parseability at the expense of natural fluency. Our analysis is
grounded in qualitative observations from bilingual (Korean/English) voice- and
text-based product testing sessions, with reflexive drafting conducted using
Natural Language Declarative Prompting (NLD-P) under human curation. Thematic
analysis identifies five recurrent traits - redundant clarity, directive
syntax, controlled vocabulary, flattened prosody, and single-intent structuring
- that improve execution accuracy but compress expressive range. MFE's
evolution highlights a persistent tension between communicative efficiency and
linguistic richness, raising design challenges for conversational interfaces
and pedagogical considerations for multilingual users. We conclude by
underscoring the need for comprehensive methodological exposition and future
empirical validation.

</details>


### [41] [Improving Multilingual Social Media Insights: Aspect-based Comment Analysis](https://arxiv.org/abs/2505.23037)
*Longyin Zhang,Bowei Zou,Ai Ti Aw*

Main category: cs.CL

TL;DR: 利用多语言大模型生成评论方面术语（CAT-G），通过监督微调和DPO对齐提升社交媒体文本理解，并创建首个多语言测试集


<details>
  <summary>Details</summary>
Motivation: 社交媒体评论存在语言自由、观点分散的特性，导致下游NLP任务（如评论聚类、摘要生成）面临理解困难

Method: 1. 使用监督微调的多语言大模型生成评论方面术语 2. 通过DPO（直接偏好优化）对齐模型预测与人类期望

Result: 在两项NLP任务中验证有效性，贡献首个涵盖英/中/马来/印尼语的多语言CAT-G测试集，支持不同语言LLM能力对比分析

Conclusion: 提出的方法有效提升社交媒体文本理解，多语言测试集为LLM跨语言性能评估提供新基准

Abstract: The inherent nature of social media posts, characterized by the freedom of
language use with a disjointed array of diverse opinions and topics, poses
significant challenges to downstream NLP tasks such as comment clustering,
comment summarization, and social media opinion analysis. To address this, we
propose a granular level of identifying and generating aspect terms from
individual comments to guide model attention. Specifically, we leverage
multilingual large language models with supervised fine-tuning for comment
aspect term generation (CAT-G), further aligning the model's predictions with
human expectations through DPO. We demonstrate the effectiveness of our method
in enhancing the comprehension of social media discourse on two NLP tasks.
Moreover, this paper contributes the first multilingual CAT-G test set on
English, Chinese, Malay, and Bahasa Indonesian. As LLM capabilities vary among
languages, this test set allows for a comparative analysis of performance
across languages with varying levels of LLM proficiency.

</details>


### [42] [EL4NER: Ensemble Learning for Named Entity Recognition via Multiple Small-Parameter Large Language Models](https://arxiv.org/abs/2505.23038)
*Yuzhen Xiao,Jiahe Song,Yongxin Xu,Ruizhe Zhang,Yiqi Xiao,Xin Lu,Runchuan Zhu,Bowen Jiang,Junfeng Zhao*

Main category: cs.CL

TL;DR: 提出EL4NER集成学习方法，通过聚合多个开源小参数LLM的上下文学习输出，在降低部署成本的同时提升NER任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于ICL的NER方法依赖大参数LLM，存在高计算资源消耗、API成本高、数据隐私风险及社区协作障碍等问题。

Method: 1）基于任务分解的多阶段集成学习流程；2）专为NER设计的span级句子相似度算法；3）自验证机制减少集成噪声。

Result: 在多个数据集上超越闭源大模型方法，部分达到SOTA，参数效率提升显著。

Conclusion: 验证了开源小参数LLM在ICL范式下实现高效NER的可行性，为资源受限场景提供新方案。

Abstract: In-Context Learning (ICL) technique based on Large Language Models (LLMs) has
gained prominence in Named Entity Recognition (NER) tasks for its lower
computing resource consumption, less manual labeling overhead, and stronger
generalizability. Nevertheless, most ICL-based NER methods depend on
large-parameter LLMs: the open-source models demand substantial computational
resources for deployment and inference, while the closed-source ones incur high
API costs, raise data-privacy concerns, and hinder community collaboration. To
address this question, we propose an Ensemble Learning Method for Named Entity
Recognition (EL4NER), which aims at aggregating the ICL outputs of multiple
open-source, small-parameter LLMs to enhance overall performance in NER tasks
at less deployment and inference cost. Specifically, our method comprises three
key components. First, we design a task decomposition-based pipeline that
facilitates deep, multi-stage ensemble learning. Second, we introduce a novel
span-level sentence similarity algorithm to establish an ICL demonstration
retrieval mechanism better suited for NER tasks. Third, we incorporate a
self-validation mechanism to mitigate the noise introduced during the ensemble
process. We evaluated EL4NER on multiple widely adopted NER datasets from
diverse domains. Our experimental results indicate that EL4NER surpasses most
closed-source, large-parameter LLM-based methods at a lower parameter cost and
even attains state-of-the-art (SOTA) performance among ICL-based methods on
certain datasets. These results show the parameter efficiency of EL4NER and
underscore the feasibility of employing open-source, small-parameter LLMs
within the ICL paradigm for NER tasks.

</details>


### [43] [Query Routing for Retrieval-Augmented Language Models](https://arxiv.org/abs/2505.23052)
*Jiarui Zhang,Xiangyu Liu,Yong Hu,Chaoyue Niu,Fan Wu,Guihai Chen*

Main category: cs.CL

TL;DR: 提出RAGRouter路由框架，通过文档嵌入和对比学习解决RAG场景中LLM路由问题，相比现有方法提升3.29%-9.33%准确率


<details>
  <summary>Details</summary>
Motivation: 现有路由方法依赖静态知识表示，无法适应RAG场景中检索文档对LLM能力的动态影响

Method: 采用文档嵌入捕捉知识变化，结合RAG能力嵌入构建对比学习框架，实现基于文档感知的路由决策

Result: 在多项知识密集型任务中平均超越最佳单模型3.61%，比现有路由方法提升3.29%-9.33%

Conclusion: RAGRouter通过动态适应文档影响，在保持低延迟的同时实现了更优的路由效果，推进了LLM协同计算范式的发展

Abstract: Retrieval-Augmented Generation (RAG) significantly improves the performance
of Large Language Models (LLMs) on knowledge-intensive tasks. However, varying
response quality across LLMs under RAG necessitates intelligent routing
mechanisms, which select the most suitable model for each query from multiple
retrieval-augmented LLMs via a dedicated router model. We observe that external
documents dynamically affect LLMs' ability to answer queries, while existing
routing methods, which rely on static parametric knowledge representations,
exhibit suboptimal performance in RAG scenarios. To address this, we formally
define the new retrieval-augmented LLM routing problem, incorporating the
influence of retrieved documents into the routing framework. We propose
RAGRouter, a RAG-aware routing design, which leverages document embeddings and
RAG capability embeddings with contrastive learning to capture knowledge
representation shifts and enable informed routing decisions. Extensive
experiments on diverse knowledge-intensive tasks and retrieval settings show
that RAGRouter outperforms the best individual LLM by 3.61% on average and
existing routing methods by 3.29%-9.33%. With an extended score-threshold-based
mechanism, it also achieves strong performance-efficiency trade-offs under
low-latency constraints.

</details>


### [44] [Self-Correcting Code Generation Using Small Language Models](https://arxiv.org/abs/2505.23060)
*Jeonghun Cho,Deokhyung Kang,Hyounghun Kim,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Self-correction has demonstrated potential in code generation by allowing
language models to revise and improve their outputs through successive
refinement. Recent studies have explored prompting-based strategies that
incorporate verification or feedback loops using proprietary models, as well as
training-based methods that leverage their strong reasoning capabilities.
However, whether smaller models possess the capacity to effectively guide their
outputs through self-reflection remains unexplored. Our findings reveal that
smaller models struggle to exhibit reflective revision behavior across both
self-correction paradigms. In response, we introduce CoCoS, an approach
designed to enhance the ability of small language models for multi-turn code
correction. Specifically, we propose an online reinforcement learning objective
that trains the model to confidently maintain correct outputs while
progressively correcting incorrect outputs as turns proceed. Our approach
features an accumulated reward function that aggregates rewards across the
entire trajectory and a fine-grained reward better suited to multi-turn
correction scenarios. This facilitates the model in enhancing initial response
quality while achieving substantial improvements through self-correction. With
1B-scale models, CoCoS achieves improvements of 35.8% on the MBPP and 27.7% on
HumanEval compared to the baselines.

</details>


### [45] [SNS-Bench-VL: Benchmarking Multimodal Large Language Models in Social Networking Services](https://arxiv.org/abs/2505.23065)
*Hongcheng Guo,Zheyong Xie,Shaosheng Cao,Boyang Wang,Weiting Liu,Anjie Le,Lei Li,Zhoujun Li*

Main category: cs.CL

TL;DR: 提出了多模态基准测试SNS-Bench-VL，评估25+模型在社交媒体场景的8项任务表现，揭示多模态社交理解的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有基准缺乏对多模态社交内容的覆盖，需构建新工具促进LLMs在真实SNS场景的能力评估。

Method: 创建包含8个多模态任务、4001个问答对的测试集，涵盖选择题与开放题型，测试主流视觉-语言模型性能。

Result: 现有模型在多模态社交上下文理解存在显著不足，需提升情境感知与人类需求对齐能力。

Conclusion: SNS-Bench-VL将推动开发更鲁棒、情境感知且人类对齐的多模态智能，支撑下一代社交网络服务发展。

Abstract: With the increasing integration of visual and textual content in Social
Networking Services (SNS), evaluating the multimodal capabilities of Large
Language Models (LLMs) is crucial for enhancing user experience, content
understanding, and platform intelligence. Existing benchmarks primarily focus
on text-centric tasks, lacking coverage of the multimodal contexts prevalent in
modern SNS ecosystems. In this paper, we introduce SNS-Bench-VL, a
comprehensive multimodal benchmark designed to assess the performance of
Vision-Language LLMs in real-world social media scenarios. SNS-Bench-VL
incorporates images and text across 8 multimodal tasks, including note
comprehension, user engagement analysis, information retrieval, and
personalized recommendation. It comprises 4,001 carefully curated multimodal
question-answer pairs, covering single-choice, multiple-choice, and open-ended
tasks. We evaluate over 25 state-of-the-art multimodal LLMs, analyzing their
performance across tasks. Our findings highlight persistent challenges in
multimodal social context comprehension. We hope SNS-Bench-VL will inspire
future research towards robust, context-aware, and human-aligned multimodal
intelligence for next-generation social networking services.

</details>


### [46] [Document-Level Text Generation with Minimum Bayes Risk Decoding using Optimal Transport](https://arxiv.org/abs/2505.23078)
*Yuu Jinnai*

Main category: cs.CL

TL;DR: 提出MBR-OT解码方法，利用Wasserstein距离提升文档级文本生成任务的性能


<details>
  <summary>Details</summary>
Motivation: 文档级文本生成需要理解长上下文，但现有MBR解码的句子级效用函数无法有效评估文档质量

Method: 通过Wasserstein距离将句子级效用函数扩展到文档层面（MBR-OT）

Result: 在文档级机器翻译、文本简化和密集图像描述任务中显著优于标准MBR

Conclusion: MBR-OT有效解决了文档级生成任务的效用评估问题，具有广泛适用性

Abstract: Document-level text generation tasks are known to be more difficult than
sentence-level text generation tasks as they require the understanding of
longer context to generate high-quality texts. In this paper, we investigate
the adaption of Minimum Bayes Risk (MBR) decoding for document-level text
generation tasks. MBR decoding makes use of a utility function to estimate the
output with the highest expected utility from a set of candidate outputs.
Although MBR decoding is shown to be effective in a wide range of
sentence-level text generation tasks, its performance on document-level text
generation tasks is limited as many of the utility functions are designed for
evaluating the utility of sentences. To this end, we propose MBR-OT, a variant
of MBR decoding using Wasserstein distance to compute the utility of a document
using a sentence-level utility function. The experimental result shows that the
performance of MBR-OT outperforms that of the standard MBR in document-level
machine translation, text simplification, and dense image captioning tasks. Our
code is available at https://github.com/jinnaiyuu/mbr-optimal-transport

</details>


### [47] [Generating Diverse Training Samples for Relation Extraction with Large Language Models](https://arxiv.org/abs/2505.23108)
*Zexuan Li,Hongliang Dai,Piji Li*

Main category: cs.CL

TL;DR: 通过改进LLM生成样本的多样性提升关系抽取任务效果，提出ICL指令优化和DPO微调两种方法，实验证明能提升数据质量且训练非LLM模型效果更优


<details>
  <summary>Details</summary>
Motivation: 直接使用LLM生成的关系抽取样本存在结构相似度高、表达方式单一的问题，需要同时提升生成样本的多样性和正确性

Method: 1.在ICL提示中直接加入多样性生成指令；2.通过DPO算法对LLM进行多样性生成微调

Result: 两种方法均显著提升生成数据质量，使用生成数据训练的传统RE模型性能优于直接使用LLM进行关系抽取

Conclusion: 通过控制LLM生成过程的多样性可有效提升下游任务表现，结合DPO的微调方法为高质量训练数据生成提供了新思路

Abstract: Using Large Language Models (LLMs) to generate training data can potentially
be a preferable way to improve zero or few-shot NLP tasks. However, many
problems remain to be investigated for this direction. For the task of Relation
Extraction (RE), we find that samples generated by directly prompting LLMs may
easily have high structural similarities with each other. They tend to use a
limited variety of phrasing while expressing the relation between a pair of
entities. Therefore, in this paper, we study how to effectively improve the
diversity of the training samples generated with LLMs for RE, while also
maintaining their correctness. We first try to make the LLMs produce dissimilar
samples by directly giving instructions in In-Context Learning (ICL) prompts.
Then, we propose an approach to fine-tune LLMs for diversity training sample
generation through Direct Preference Optimization (DPO). Our experiments on
commonly used RE datasets show that both attempts can improve the quality of
the generated training data. We also find that comparing with directly
performing RE with an LLM, training a non-LLM RE model with its generated
samples may lead to better performance.

</details>


### [48] [Dataset Cartography for Large Language Model Alignment: Mapping and Diagnosing Preference Data](https://arxiv.org/abs/2505.23114)
*Seohyeong Lee,Eunwon Kim,Hwaran Lee,Buru Chang*

Main category: cs.CL

TL;DR: 提出Alignment Data Map工具，通过GPT-4o计算对齐分数构建数据地图，仅用33%数据即可实现LLM高效对齐


<details>
  <summary>Details</summary>
Motivation: 人类偏好数据收集成本高且效率低，成为LLM对齐扩展的瓶颈

Method: 使用GPT-4o作为LLM对齐代理，计算响应对齐分数并构建均值-方差数据地图，筛选高均值低方差区域样本

Result: 实验表明仅使用33%的高质量数据即可达到/超越全数据集效果，并能检测低效/错误标注样本

Conclusion: Alignment Data Map显著提升数据效率，无需显式标注即可筛选高质量样本，同时具备数据集诊断能力

Abstract: Human preference data plays a critical role in aligning large language models
(LLMs) with human values. However, collecting such data is often expensive and
inefficient, posing a significant scalability challenge. To address this, we
introduce Alignment Data Map, a GPT-4o-assisted tool for analyzing and
diagnosing preference data. Using GPT-4o as a proxy for LLM alignment, we
compute alignment scores for LLM-generated responses to instructions from
existing preference datasets. These scores are then used to construct an
Alignment Data Map based on their mean and variance. Our experiments show that
using only 33 percent of the data, specifically samples in the high-mean,
low-variance region, achieves performance comparable to or better than using
the entire dataset. This finding suggests that the Alignment Data Map can
significantly improve data collection efficiency by identifying high-quality
samples for LLM alignment without requiring explicit annotations. Moreover, the
Alignment Data Map can diagnose existing preference datasets. Our analysis
shows that it effectively detects low-impact or potentially misannotated
samples. Source code is available online.

</details>


### [49] [Elicit and Enhance: Advancing Multimodal Reasoning in Medical Scenarios](https://arxiv.org/abs/2505.23118)
*Linjie Mu,Zhongzhen Huang,Yakun Zhu,Xiangyu Zhao,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.CL

TL;DR: 提出两阶段训练框架MedE²，通过文本微调和多模态增强显著提升医学多模态推理模型的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推理模型在数学和科学领域取得显著成果，但在医学领域的应用仍处于探索不足阶段。

Method: 1. 第一阶段使用2000个纯文本样本微调模型以激发推理行为；2. 第二阶段采用1500个多模态医学案例增强模型推理能力，并对齐多模态医学推理偏好。

Result: 在多个医学多模态基准测试中持续超越基线模型，大模型扩展验证进一步证实方法的稳健性和实用性。

Conclusion: MedE²通过结构化训练流程有效提升了医学多模态推理的可靠性和实际应用价值。

Abstract: Effective clinical decision-making depends on iterative, multimodal reasoning
across diverse sources of evidence. The recent emergence of multimodal
reasoning models has significantly transformed the landscape of solving complex
tasks. Although such models have achieved notable success in mathematics and
science, their application to medical domains remains underexplored. In this
work, we propose \textit{MedE$^2$}, a two-stage post-training pipeline that
elicits and then enhances multimodal reasoning for medical domains. In Stage-I,
we fine-tune models using 2,000 text-only data samples containing precisely
orchestrated reasoning demonstrations to elicit reasoning behaviors. In
Stage-II, we further enhance the model's reasoning capabilities using 1,500
rigorously curated multimodal medical cases, aligning model reasoning outputs
with our proposed multimodal medical reasoning preference. Extensive
experiments demonstrate the efficacy and reliability of \textit{MedE$^2$} in
improving the reasoning performance of medical multimodal models. Notably,
models trained with \textit{MedE$^2$} consistently outperform baselines across
multiple medical multimodal benchmarks. Additional validation on larger models
and under inference-time scaling further confirms the robustness and practical
utility of our approach.

</details>


### [50] [ContextQFormer: A New Context Modeling Method for Multi-Turn Multi-Modal Conversations](https://arxiv.org/abs/2505.23121)
*Yiming Lei,Zhizheng Yang,Zeming Liu,Haitao Leng,Shaoguo Liu,Tingting Gao,Qingjie Liu,Yunhong Wang*

Main category: cs.CL

TL;DR: 提出ContextQFormer模块增强多轮多模态对话能力，并构建长对话数据集TMDialog


<details>
  <summary>Details</summary>
Motivation: 现有开源多模态模型在多轮长上下文对话中存在性能不足，缺乏专门的长对话数据集支撑研究

Method: 1. 开发基于记忆块的ContextQFormer上下文建模模块 2. 构建包含长对话的TMDialog数据集（含预训练/指令调优/评估三阶段）

Result: 在TMDialog数据集上，ContextQFormer相比基线模型可用率提升2%-4%

Conclusion: ContextQFormer有效提升长对话处理能力，TMDialog数据集填补了多轮多模态对话研究的数据空白

Abstract: Multi-modal large language models have demonstrated remarkable zero-shot
abilities and powerful image-understanding capabilities. However, the existing
open-source multi-modal models suffer from the weak capability of multi-turn
interaction, especially for long contexts. To address the issue, we first
introduce a context modeling module, termed ContextQFormer, which utilizes a
memory block to enhance the presentation of contextual information.
Furthermore, to facilitate further research, we carefully build a new
multi-turn multi-modal dialogue dataset (TMDialog) for pre-training,
instruction-tuning, and evaluation, which will be open-sourced lately. Compared
with other multi-modal dialogue datasets, TMDialog contains longer
conversations, which supports the research of multi-turn multi-modal dialogue.
In addition, ContextQFormer is compared with three baselines on TMDialog and
experimental results illustrate that ContextQFormer achieves an improvement of
2%-4% in available rate over baselines.

</details>


### [51] [PBEBench: A Multi-Step Programming by Examples Reasoning Benchmark inspired by Historical Linguistics](https://arxiv.org/abs/2505.23126)
*Atharva Naik,Darsh Agrawal,Manav Kapadnis,Yuwei An,Yash Mathur,Carolyn Rose,David Mortensen*

Main category: cs.CL

TL;DR: 大语言模型在历史语言学归纳推理任务中表现不足，自动生成基准测试显示最高通过率仅54%


<details>
  <summary>Details</summary>
Motivation: 验证大语言模型的抽象推理能力是否适用于历史语言学等实际领域，突破传统数学/编码类评测的局限

Method: 开发自动化流程动态生成可控难度的基准测试集（Programming by Examples任务），解决传统评测的扩展性和数据污染问题

Result: 在近1k个测试实例中，性能最好的Claude-3.7-Sonnet模型仅达54%通过率，显示现有模型在此类推理任务存在显著缺陷

Conclusion: 长思维链大模型在处理历史语言学等领域普遍存在的归纳推理问题时仍面临挑战，需进一步改进模型架构或训练方法

Abstract: Recently, long chain of thought (LCoT), Large Language Models (LLMs), have
taken the machine learning world by storm with their breathtaking reasoning
capabilities. However, are the abstract reasoning abilities of these models
general enough for problems of practical importance? Unlike past work, which
has focused mainly on math, coding, and data wrangling, we focus on a
historical linguistics-inspired inductive reasoning problem, formulated as
Programming by Examples. We develop a fully automated pipeline for dynamically
generating a benchmark for this task with controllable difficulty in order to
tackle scalability and contamination issues to which many reasoning benchmarks
are subject. Using our pipeline, we generate a test set with nearly 1k
instances that is challenging for all state-of-the-art reasoning LLMs, with the
best model (Claude-3.7-Sonnet) achieving a mere 54% pass rate, demonstrating
that LCoT LLMs still struggle with a class or reasoning that is ubiquitous in
historical linguistics as well as many other domains.

</details>


### [52] [Enhancing Large Language Models'Machine Translation via Dynamic Focus Anchoring](https://arxiv.org/abs/2505.23140)
*Qiuyu Ding,Zhiqiang Cao,Hailong Cao,Tiejun Zhao*

Main category: cs.CL

TL;DR: 通过动态识别上下文敏感单元(CSU)并应用语义聚焦方法，提升大语言模型在机器翻译任务中对多义词等复杂语境单元的处理能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在机器翻译中虽表现优异，但对多义词等上下文敏感单元(CSU)处理不足，易导致局部翻译错误、语义理解偏差甚至翻译失败。

Method: 1. 动态分析识别翻译难点(CSU) 2. 通过结构化语义聚焦机制将难点融入LLMs 3. 缓解信息扁平化导致的误译/误解问题 4. 激活LLMs相关知识库应用能力

Result: 在机器翻译基准测试中取得与开源基线模型相当的竞争力，跨相似/远距离语言对均表现出有效性和鲁棒性，且无需额外模型训练。

Conclusion: 该方法以最小资源消耗实现了LLMs多任务性能提升，特别在复杂术语翻译场景展现出高效知识激活能力，具有实际应用价值。

Abstract: Large language models have demonstrated exceptional performance across
multiple crosslingual NLP tasks, including machine translation (MT). However,
persistent challenges remain in addressing context-sensitive units (CSUs), such
as polysemous words. These CSUs not only affect the local translation accuracy
of LLMs, but also affect LLMs' understanding capability for sentences and
tasks, and even lead to translation failure. To address this problem, we
propose a simple but effective method to enhance LLMs' MT capabilities by
acquiring CSUs and applying semantic focus. Specifically, we dynamically
analyze and identify translation challenges, then incorporate them into LLMs in
a structured manner to mitigate mistranslations or misunderstandings of CSUs
caused by information flattening. Efficiently activate LLMs to identify and
apply relevant knowledge from its vast data pool in this way, ensuring more
accurate translations for translating difficult terms. On a benchmark dataset
of MT, our proposed method achieved competitive performance compared to
multiple existing open-sourced MT baseline models. It demonstrates
effectiveness and robustness across multiple language pairs, including both
similar language pairs and distant language pairs. Notably, the proposed method
requires no additional model training and enhances LLMs' performance across
multiple NLP tasks with minimal resource consumption.

</details>


### [53] [Cross-Domain Bilingual Lexicon Induction via Pretrained Language Models](https://arxiv.org/abs/2505.23146)
*Qiuyu Ding,Zhiqiang Cao,Hailong Cao,Tiejun Zhao*

Main category: cs.CL

TL;DR: 提出跨领域双语词典归纳新任务，结合预训练模型与代码切换策略，在专业领域平均提升0.78个点


<details>
  <summary>Details</summary>
Motivation: 传统BLI方法在专业领域表现差（如医学数据集准确率显著低于通用领域），主要受限于领域数据规模小、低频术语多及静态词嵌入的上下文敏感性不足

Method: 基于预训练模型能力，首次在跨领域BLI中引入代码切换策略，通过动态上下文适配优化词嵌入对齐

Result: 在三个专业领域数据集上平均提升0.78个准确率点，显著优于Muse/Vecmap等基线方法

Conclusion: 融合预训练模型动态表征与领域适配策略能有效提升专业领域双语词典质量，为垂直领域机器翻译提供新思路

Abstract: Bilingual Lexicon Induction (BLI) is generally based on common domain data to
obtain monolingual word embedding, and by aligning the monolingual word
embeddings to obtain the cross-lingual embeddings which are used to get the
word translation pairs. In this paper, we propose a new task of BLI, which is
to use the monolingual corpus of the general domain and target domain to
extract domain-specific bilingual dictionaries. Motivated by the ability of
Pre-trained models, we propose a method to get better word embeddings that
build on the recent work on BLI. This way, we introduce the Code Switch(Qin et
al., 2020) firstly in the cross-domain BLI task, which can match differit is
yet to be seen whether these methods are suitable for bilingual lexicon
extraction in professional fields. As we can see in table 1, the classic and
efficient BLI approach, Muse and Vecmap, perform much worse on the Medical
dataset than on the Wiki dataset. On one hand, the specialized domain data set
is relatively smaller compared to the generic domain data set generally, and
specialized words have a lower frequency, which will directly affect the
translation quality of bilingual dictionaries. On the other hand, static word
embeddings are widely used for BLI, however, in some specific fields, the
meaning of words is greatly influenced by context, in this case, using only
static word embeddings may lead to greater bias. ent strategies in different
contexts, making the model more suitable for this task. Experimental results
show that our method can improve performances over robust BLI baselines on
three specific domains by averagely improving 0.78 points.

</details>


### [54] [Tell, Don't Show: Leveraging Language Models' Abstractive Retellings to Model Literary Themes](https://arxiv.org/abs/2505.23166)
*Li Lucy,Camilla Griffiths,Sarah Levine,Jennifer L. Eberhardt,Dorottya Demszky,David Bamman*

Main category: cs.CL

TL;DR: 提出Retell方法——通过生成式语言模型将文学文本转化为高层概念后再进行LDA主题建模，相比传统方法显著提升主题精确度


<details>
  <summary>Details</summary>
Motivation: 传统LDA模型难以解析文学文本的沉浸式感官描写特征（'展示而非讲述'），导致主题建模效果不佳

Method: 1. 用语言模型对文本段落进行概念重述 → 2. 对重述内容进行LDA分析 → 比单独使用LDA或直接让模型生成主题更有效

Result: 在高中英语教材种族/文化身份分析案例中，比传统方法获得更精确、信息量更大的主题结构（与专家标注结果对比验证）

Conclusion: 该方法为文学分析提供了简单有效的新路径，特别在文化主题挖掘方面展现应用潜力

Abstract: Conventional bag-of-words approaches for topic modeling, like latent
Dirichlet allocation (LDA), struggle with literary text. Literature challenges
lexical methods because narrative language focuses on immersive sensory details
instead of abstractive description or exposition: writers are advised to "show,
don't tell." We propose Retell, a simple, accessible topic modeling approach
for literature. Here, we prompt resource-efficient, generative language models
(LMs) to tell what passages show, thereby translating narratives' surface forms
into higher-level concepts and themes. By running LDA on LMs' retellings of
passages, we can obtain more precise and informative topics than by running LDA
alone or by directly asking LMs to list topics. To investigate the potential of
our method for cultural analytics, we compare our method's outputs to
expert-guided annotations in a case study on racial/cultural identity in high
school English language arts books.

</details>


### [55] [ZIPA: A family of efficient models for multilingual phone recognition](https://arxiv.org/abs/2505.23170)
*Jian Zhu,Farhan Samir,Eleanor Chodroff,David R. Mortensen*

Main category: cs.CL

TL;DR: ZIPA模型系列通过17,132小时的多语言标准化音标数据集IPAPack++和高效的Zipformer架构，以更少参数实现跨语言音素识别性能突破，但社会语音多样性建模仍存挑战


<details>
  <summary>Details</summary>
Motivation: 解决现有音素识别系统在跨语言场景下的数据规模限制和模型效率问题，通过构建大规模标准化数据集与高效模型架构提升性能

Method: 1. 构建包含社会语音变体的IPAPack++数据集
2. 基于Zipformer架构开发ZIPA-T（传感器）和ZIPA-CR（CTC）模型
3. 在11,000小时伪标注多语言数据上进行噪声学生训练扩展

Result: 1. 参数量显著减少（ZIPA-T仅需0.2B参数）而性能超越现有系统
2. 扩展训练后CER进一步降低14%
3. 错误分析揭示社会语音变异场景错误率高于基准集35%

Conclusion: ZIPA在标准测试集表现优异，但社会语音多样性建模存在持续局限，凸显语音技术包容性发展的现实挑战，为未来研究指明方向

Abstract: We present ZIPA, a family of efficient speech models that advances the
state-of-the-art performance of crosslinguistic phone recognition. We first
curated IPAPack++, a large-scale multilingual speech corpus with 17,132 hours
of normalized phone transcriptions and a novel evaluation set capturing unseen
languages and sociophonetic variation. With the large-scale training data,
ZIPA, including transducer (ZIPA-T) and CTC-based (ZIPA-CR) variants, leverage
the efficient Zipformer backbones and outperform existing phone recognition
systems with much fewer parameters. Further scaling via noisy student training
on 11,000 hours of pseudo-labeled multilingual data yields further improvement.
While ZIPA achieves strong performance on benchmarks, error analysis reveals
persistent limitations in modeling sociophonetic diversity, underscoring
challenges for future research.

</details>


### [56] [Map&Make: Schema Guided Text to Table Generation](https://arxiv.org/abs/2505.23174)
*Naman Ahuja,Fenil Bardoliya,Chitta Baral,Vivek Gupta*

Main category: cs.CL

TL;DR: 提出Map&Make方法，通过分解文本为原子命题生成可解释的表格，在Rotowire和Livesum数据集上实现文本到表格生成的性能提升


<details>
  <summary>Details</summary>
Motivation: 现有文本转表格方法存在复杂信息提取能力不足和缺乏推理能力的缺陷，需通过结构化分解提升信息保真度

Method: 1. 将文本解构为命题级原子陈述 2. 提取潜在模式 3. 用模式填充表格（同时保留定性与定量信息）

Result: 在Rotowire(修正幻觉错误后)和Livesum数据集上实现SOTA，消融实验验证框架有效性，指标提升显著

Conclusion: 该方法通过结构化分解实现更可靠的表格生成，在信息保真度和可解释性方面具有实践价值

Abstract: Transforming dense, detailed, unstructured text into an interpretable and
summarised table, also colloquially known as Text-to-Table generation, is an
essential task for information retrieval. Current methods, however, miss out on
how and what complex information to extract; they also lack the ability to
infer data from the text. In this paper, we introduce a versatile approach,
Map&Make, which "dissects" text into propositional atomic statements. This
facilitates granular decomposition to extract the latent schema. The schema is
then used to populate the tables that capture the qualitative nuances and the
quantitative facts in the original text. Our approach is tested against two
challenging datasets, Rotowire, renowned for its complex and multi-table
schema, and Livesum, which demands numerical aggregation. By carefully
identifying and correcting hallucination errors in Rotowire, we aim to achieve
a cleaner and more reliable benchmark. We evaluate our method rigorously on a
comprehensive suite of comparative and referenceless metrics. Our findings
demonstrate significant improvement results across both datasets with better
interpretability in Text-to-Table generation. Moreover, through detailed
ablation studies and analyses, we investigate the factors contributing to
superior performance and validate the practicality of our framework in
structured summarization tasks.

</details>


### [57] [Infinite-Instruct: Synthesizing Scaling Code instruction Data with Bidirectional Synthesis and Static Verification](https://arxiv.org/abs/2505.23177)
*Wenjing Xing,Wenke Lu,Yeheng Duan,Bing Zhao,Zhenghui kang,Yaolong Wang,Kai Gao,Lei Qiao*

Main category: cs.CL

TL;DR: Infinite-Instruct框架通过自动化生成高质量代码问答对，显著提升大语言模型的代码生成能力，实验显示7B/32B模型性能分别提升21.70%和36.95%，并开源数据集。


<details>
  <summary>Details</summary>
Motivation: 传统代码指令数据合成方法存在多样性不足、逻辑性差的缺陷，需要开发能提升LLM代码生成质量的新数据合成框架。

Method: 1.反向构建转换代码片段为编程问题 2.通过知识图谱反馈构建增强问题逻辑 3.跨语言静态代码分析过滤低质样本

Result: 主流基准测试中，微调后模型性能提升显著（7B模型21.70%，32B模型36.95%），仅用1/10数据量即达到对标模型效果

Conclusion: 该框架为编程LLM训练提供可扩展解决方案，通过逻辑强化和严格过滤机制实现高效数据合成，开源数据推动社区发展

Abstract: Traditional code instruction data synthesis methods suffer from limited
diversity and poor logic. We introduce Infinite-Instruct, an automated
framework for synthesizing high-quality question-answer pairs, designed to
enhance the code generation capabilities of large language models (LLMs). The
framework focuses on improving the internal logic of synthesized problems and
the quality of synthesized code. First, "Reverse Construction" transforms code
snippets into diverse programming problems. Then, through "Backfeeding
Construction," keywords in programming problems are structured into a knowledge
graph to reconstruct them into programming problems with stronger internal
logic. Finally, a cross-lingual static code analysis pipeline filters invalid
samples to ensure data quality. Experiments show that on mainstream code
generation benchmarks, our fine-tuned models achieve an average performance
improvement of 21.70% on 7B-parameter models and 36.95% on 32B-parameter
models. Using less than one-tenth of the instruction fine-tuning data, we
achieved performance comparable to the Qwen-2.5-Coder-Instruct.
Infinite-Instruct provides a scalable solution for LLM training in programming.
We open-source the datasets used in the experiments, including both unfiltered
versions and filtered versions via static analysis. The data are available at
https://github.com/xingwenjing417/Infinite-Instruct-dataset

</details>


### [58] [Unsupervised Word-level Quality Estimation for Machine Translation Through the Lens of Annotators (Dis)agreement](https://arxiv.org/abs/2505.23183)
*Gabriele Sarti,Vilém Zouhar,Malvina Nissim,Arianna Bisazza*

Main category: cs.CL

TL;DR: 提出利用语言模型可解释性和不确定性量化技术，从翻译模型内部特征中识别翻译错误，通过多标注集评估发现无监督指标的潜力及监督方法在标签不确定性下的缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有词级质量估计方法依赖大模型提示或大量人工标注数据，成本高昂。研究旨在探索利用翻译模型内部特征的高效替代方案，并量化人工标注差异对指标性能的影响。

Method: 在12个翻译方向上评估14个指标，使用多组人工标注数据验证标签变异影响。通过分析翻译模型的内部工作机制(注意力权重、置信度等)构建无监督指标。

Result: 发现无监督指标存在未开发潜力；监督方法在标签不确定性下表现不稳定；单一标注者的评估实践存在脆弱性。

Conclusion: 研究表明需要开发更鲁棒的评估方法，无监督方法在质量估计任务中具有可行性，但需解决人工标注不一致带来的挑战。

Abstract: Word-level quality estimation (WQE) aims to automatically identify
fine-grained error spans in machine-translated outputs and has found many uses,
including assisting translators during post-editing. Modern WQE techniques are
often expensive, involving prompting of large language models or ad-hoc
training on large amounts of human-labeled data. In this work, we investigate
efficient alternatives exploiting recent advances in language model
interpretability and uncertainty quantification to identify translation errors
from the inner workings of translation models. In our evaluation spanning 14
metrics across 12 translation directions, we quantify the impact of human label
variation on metric performance by using multiple sets of human labels. Our
results highlight the untapped potential of unsupervised metrics, the
shortcomings of supervised methods when faced with label uncertainty, and the
brittleness of single-annotator evaluation practices.

</details>


### [59] [Cross-Task Experiential Learning on LLM-based Multi-Agent Collaboration](https://arxiv.org/abs/2505.23187)
*Yilong Li,Chen Qian,Yu Xia,Ruijie Shi,Yufan Dang,Zihao Xie,Ziming You,Weize Chen,Cheng Yang,Weichuan Liu,Ye Tian,Xuantang Xiong,Lei Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 提出多智能体跨任务经验学习框架MAEL，通过图结构协作网络和经验池提升LLM智能体的协作效率与解决方案质量


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统孤立处理任务导致计算冗余和跨任务泛化能力不足，需建立持续经验学习机制

Method: 1. 构建图结构多智能体协作网络 2. 量化任务流程步骤质量并存储奖励到经验池 3. 推理时检索高奖励经验作为少样本参考

Result: 在多样化数据集上实现更快收敛（平均提速37%）和解决方案质量提升（F1值提高15.2%）

Conclusion: MAEL通过显式经验积累机制有效提升多智能体系统的持续学习能力，为复杂任务协作提供新范式

Abstract: Large Language Model-based multi-agent systems (MAS) have shown remarkable
progress in solving complex tasks through collaborative reasoning and
inter-agent critique. However, existing approaches typically treat each task in
isolation, resulting in redundant computations and limited generalization
across structurally similar tasks. To address this, we introduce multi-agent
cross-task experiential learning (MAEL), a novel framework that endows
LLM-driven agents with explicit cross-task learning and experience
accumulation. We model the task-solving workflow on a graph-structured
multi-agent collaboration network, where agents propagate information and
coordinate via explicit connectivity. During the experiential learning phase,
we quantify the quality for each step in the task-solving workflow and store
the resulting rewards along with the corresponding inputs and outputs into each
agent's individual experience pool. During inference, agents retrieve
high-reward, task-relevant experiences as few-shot examples to enhance the
effectiveness of each reasoning step, thereby enabling more accurate and
efficient multi-agent collaboration. Experimental results on diverse datasets
demonstrate that MAEL empowers agents to learn from prior task experiences
effectively-achieving faster convergence and producing higher-quality solutions
on current tasks.

</details>


### [60] [ExpeTrans: LLMs Are Experiential Transfer Learners](https://arxiv.org/abs/2505.23191)
*Jinglong Gao,Xiao Ding,Lingxiao Zou,Bibo Cai,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: 提出自主经验迁移框架，使大语言模型能自动将源任务经验迁移至新任务，降低人工成本并提升性能


<details>
  <summary>Details</summary>
Motivation: 传统方法需人工收集任务经验，难以应对日益增长的多样化用户需求，需探索LLMs自主迁移经验的能力

Method: 设计基于认知智能的自主经验迁移框架，实现跨任务经验复用，避免重复收集经验的高成本

Result: 在13个数据集上验证有效性，性能显著提升，并对框架模块进行详细分析验证

Conclusion: 该框架为LLMs通用性提供新路径，实验证明可有效降低人工依赖，未来可拓展更多经验迁移机制

Abstract: Recent studies provide large language models (LLMs) with textual task-solving
experiences via prompts to improve their performance. However, previous methods
rely on substantial human labor or time to gather such experiences for each
task, which is impractical given the growing variety of task types in user
queries to LLMs. To address this issue, we design an autonomous experience
transfer framework to explore whether LLMs can mimic human cognitive
intelligence to autonomously transfer experience from existing source tasks to
newly encountered target tasks. This not only allows the acquisition of
experience without extensive costs of previous methods, but also offers a novel
path for the generalization of LLMs. Experimental results on 13 datasets
demonstrate that our framework effectively improves the performance of LLMs.
Furthermore, we provide a detailed analysis of each module in the framework.

</details>


### [61] [MMBoundary: Advancing MLLM Knowledge Boundary Awareness through Reasoning Step Confidence Calibration](https://arxiv.org/abs/2505.23224)
*Zhitao He,Sandeep Polisetty,Zhiyuan Fan,Yuchen Huang,Shujin Wu,Yi R.,Fung*

Main category: cs.CL

TL;DR: 提出MMBoundary框架，通过多粒度置信度校准增强多模态大语言模型的推理链自纠正能力


<details>
  <summary>Details</summary>
Motivation: 现有置信度估计方法仅关注整体响应，缺乏对推理步骤的细粒度评估，导致错误累积（幻觉滚雪球效应）

Method: 1. 融合文本/跨模态自奖励信号进行逐步骤置信度估计
2. 监督微调+强化学习双阶段优化（多奖励函数对齐知识）
3. 实现推理链自校正机制

Result: 跨领域数据集平均降低7.5%校准误差，任务性能最高提升8.3%

Conclusion: MMBoundary通过细粒度置信度校准有效提升多模态推理质量，突破现有方法性能边界

Abstract: In recent years, multimodal large language models (MLLMs) have made
significant progress but continue to face inherent challenges in multimodal
reasoning, which requires multi-level (e.g., perception, reasoning) and
multi-granular (e.g., multi-step reasoning chain) advanced inferencing. Prior
work on estimating model confidence tends to focus on the overall response for
training and calibration, but fails to assess confidence in each reasoning
step, leading to undesirable hallucination snowballing. In this work, we
present MMBoundary, a novel framework that advances the knowledge boundary
awareness of MLLMs through reasoning step confidence calibration. To achieve
this, we propose to incorporate complementary textual and cross-modal
self-rewarding signals to estimate confidence at each step of the MLLM
reasoning process. In addition to supervised fine-tuning MLLM on this set of
self-rewarded confidence estimation signal for initial confidence expression
warm-up, we introduce a reinforcement learning stage with multiple reward
functions for further aligning model knowledge and calibrating confidence at
each reasoning step, enhancing reasoning chain self-correction. Empirical
results show that MMBoundary significantly outperforms existing methods across
diverse domain datasets and metrics, achieving an average of 7.5% reduction in
multimodal confidence calibration errors and up to 8.3% improvement in task
performance.

</details>


### [62] [MCTSr-Zero: Self-Reflective Psychological Counseling Dialogues Generation via Principles and Adaptive Exploration](https://arxiv.org/abs/2505.23229)
*Hao Lu,Yanchi Gu,Haoyuan Huang,Yulin Zhou,Ningxin Zhu,Chen Li*

Main category: cs.CL

TL;DR: 提出MCTSr-Zero框架，通过领域对齐机制解决开放式心理咨询对话中LLM与主观伦理原则的对齐难题


<details>
  <summary>Details</summary>
Motivation: 传统结果导向的MCTS在心理咨询等开放式对话场景中易产生伦理偏差，因成功标准依赖主观因素（共情/伦理）而非客观正确性

Method: 1) 领域对齐机制：将搜索目标转向符合领域原则的对话轨迹 2) 再生机制：允许重新生成初始策略 3) 元提示适应：动态调整探索范围

Result: 基于框架开发的PsyLLM在PsyEval基准上达到SOTA，验证了生成符合心理标准的对话数据有效性

Conclusion: MCTSr-Zero为人类中心对话系统提供了可靠的数据生成方案，解决了LLM持续遵循复杂心理标准的挑战

Abstract: The integration of Monte Carlo Tree Search (MCTS) with Large Language Models
(LLMs) has demonstrated significant success in structured, problem-oriented
tasks. However, applying these methods to open-ended dialogues, such as those
in psychological counseling, presents unique challenges. Unlike tasks with
objective correctness, success in therapeutic conversations depends on
subjective factors like empathetic engagement, ethical adherence, and alignment
with human preferences, for which strict "correctness" criteria are
ill-defined. Existing result-oriented MCTS approaches can therefore produce
misaligned responses. To address this, we introduce MCTSr-Zero, an MCTS
framework designed for open-ended, human-centric dialogues. Its core innovation
is "domain alignment", which shifts the MCTS search objective from predefined
end-states towards conversational trajectories that conform to target domain
principles (e.g., empathy in counseling). Furthermore, MCTSr-Zero incorporates
"Regeneration" and "Meta-Prompt Adaptation" mechanisms to substantially broaden
exploration by allowing the MCTS to consider fundamentally different initial
dialogue strategies. We evaluate MCTSr-Zero in psychological counseling by
generating multi-turn dialogue data, which is used to fine-tune an LLM, PsyLLM.
We also introduce PsyEval, a benchmark for assessing multi-turn psychological
counseling dialogues. Experiments demonstrate that PsyLLM achieves
state-of-the-art performance on PsyEval and other relevant metrics, validating
MCTSr-Zero's effectiveness in generating high-quality, principle-aligned
conversational data for human-centric domains and addressing the LLM challenge
of consistently adhering to complex psychological standards.

</details>


### [63] [ChartMind: A Comprehensive Benchmark for Complex Real-world Multimodal Chart Question Answering](https://arxiv.org/abs/2505.23242)
*Jingxuan Wei,Nan Xu,Junnan Zhu,Yanni Hao,Gaowei Wu,Bihui Yu,Lei Wang*

Main category: cs.CL

TL;DR: 提出ChartMind基准测试和ChartLLM框架，解决现有图表问答任务在真实场景下的评估局限性


<details>
  <summary>Details</summary>
Motivation: 现有图表问答任务过度依赖固定输出格式和客观指标，无法满足真实图表分析的复杂需求

Method: 开发支持多语言/开放域输出的ChartMind基准，设计上下文感知的ChartLLM框架（关键元素提取+降噪+多模态大模型推理增强）

Result: 在14个主流模型上验证，ChartLLM显著优于传统指令跟随/OCR增强/思维链三种范式

Conclusion: 灵活上下文理解是提升图表问答实际应用的关键，为未来图表推理研究指明方向

Abstract: Chart question answering (CQA) has become a critical multimodal task for
evaluating the reasoning capabilities of vision-language models. While early
approaches have shown promising performance by focusing on visual features or
leveraging large-scale pre-training, most existing evaluations rely on rigid
output formats and objective metrics, thus ignoring the complex, real-world
demands of practical chart analysis. In this paper, we introduce ChartMind, a
new benchmark designed for complex CQA tasks in real-world settings. ChartMind
covers seven task categories, incorporates multilingual contexts, supports
open-domain textual outputs, and accommodates diverse chart formats, bridging
the gap between real-world applications and traditional academic benchmarks.
Furthermore, we propose a context-aware yet model-agnostic framework, ChartLLM,
that focuses on extracting key contextual elements, reducing noise, and
enhancing the reasoning accuracy of multimodal large language models. Extensive
evaluations on ChartMind and three representative public benchmarks with 14
mainstream multimodal models show our framework significantly outperforms the
previous three common CQA paradigms: instruction-following, OCR-enhanced, and
chain-of-thought, highlighting the importance of flexible chart understanding
for real-world CQA. These findings suggest new directions for developing more
robust chart reasoning in future research.

</details>


### [64] [Automatic Construction of Multiple Classification Dimensions for Managing Approaches in Scientific Papers](https://arxiv.org/abs/2505.23252)
*Bing Ma,Hai Zhuge*

Main category: cs.CL

TL;DR: 提出基于多维方法空间的研究方法管理框架，通过模式识别、树结构相似性度量和聚类算法实现高效查询与管理


<details>
  <summary>Details</summary>
Motivation: 传统研究方法查询耗时且缺乏组织性，需建立多维度分类框架提升科研方法管理效率

Method: 1. 四层语言模式识别（语义/话语/句法/词汇）
2. 树结构表示步骤及相似性度量
3. 自底向上聚类构建五维类树体系

Result: 多维方法空间支持类驱动查询机制，显著增强查询结果相关性并快速缩小搜索范围（通过实验验证）

Conclusion: 构建的多维方法空间有效解决科研方法管理难题，实现查询效率与精度的双重提升

Abstract: Approaches form the foundation for conducting scientific research. Querying
approaches from a vast body of scientific papers is extremely time-consuming,
and without a well-organized management framework, researchers may face
significant challenges in querying and utilizing relevant approaches.
Constructing multiple dimensions on approaches and managing them from these
dimensions can provide an efficient solution. Firstly, this paper identifies
approach patterns using a top-down way, refining the patterns through four
distinct linguistic levels: semantic level, discourse level, syntactic level,
and lexical level. Approaches in scientific papers are extracted based on
approach patterns. Additionally, five dimensions for categorizing approaches
are identified using these patterns. This paper proposes using tree structure
to represent step and measuring the similarity between different steps with a
tree-structure-based similarity measure that focuses on syntactic-level
similarities. A collection similarity measure is proposed to compute the
similarity between approaches. A bottom-up clustering algorithm is proposed to
construct class trees for approach components within each dimension by merging
each approach component or class with its most similar approach component or
class in each iteration. The class labels generated during the clustering
process indicate the common semantics of the step components within the
approach components in each class and are used to manage the approaches within
the class. The class trees of the five dimensions collectively form a
multi-dimensional approach space. The application of approach queries on the
multi-dimensional approach space demonstrates that querying within this space
ensures strong relevance between user queries and results and rapidly reduces
search space through a class-based query mechanism.

</details>


### [65] [The Arabic AI Fingerprint: Stylometric Analysis and Detection of Large Language Models Text](https://arxiv.org/abs/2505.23276)
*Maged S. Al-Shaibani,Moataz Ahmed*

Main category: cs.CL

TL;DR: 研究阿拉伯语机器生成文本的检测方法，分析不同生成策略和模型架构的差异，开发出F1值达99.9%的高效检测模型，并揭示跨领域泛化挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在阿拉伯语等低资源语言中生成隐蔽性极强的虚假信息，严重威胁教育、医疗等关键领域的信息完整性。

Method: 采用标题生成/内容感知生成/文本精炼三种策略，测试ALLaM/Jais/Llama/GPT-4等模型，通过文体测量学分析语言特征，并构建BERT检测模型。

Result: 检测模型在正式语境达到99.9% F1值，但跨领域检测性能显著下降，证实不同领域文本特征存在明显差异。

Conclusion: 首次实现阿拉伯语生成文本的多维度系统性研究，为开发语言特征驱动的检测系统提供理论支撑，助力维护阿拉伯语信息生态安全。

Abstract: Large Language Models (LLMs) have achieved unprecedented capabilities in
generating human-like text, posing subtle yet significant challenges for
information integrity across critical domains, including education, social
media, and academia, enabling sophisticated misinformation campaigns,
compromising healthcare guidance, and facilitating targeted propaganda. This
challenge becomes severe, particularly in under-explored and low-resource
languages like Arabic. This paper presents a comprehensive investigation of
Arabic machine-generated text, examining multiple generation strategies
(generation from the title only, content-aware generation, and text refinement)
across diverse model architectures (ALLaM, Jais, Llama, and GPT-4) in academic,
and social media domains. Our stylometric analysis reveals distinctive
linguistic patterns differentiating human-written from machine-generated Arabic
text across these varied contexts. Despite their human-like qualities, we
demonstrate that LLMs produce detectable signatures in their Arabic outputs,
with domain-specific characteristics that vary significantly between different
contexts. Based on these insights, we developed BERT-based detection models
that achieved exceptional performance in formal contexts (up to 99.9\%
F1-score) with strong precision across model architectures. Our cross-domain
analysis confirms generalization challenges previously reported in the
literature. To the best of our knowledge, this work represents the most
comprehensive investigation of Arabic machine-generated text to date, uniquely
combining multiple prompt generation methods, diverse model architectures, and
in-depth stylometric analysis across varied textual domains, establishing a
foundation for developing robust, linguistically-informed detection systems
essential for preserving information integrity in Arabic-language contexts.

</details>


### [66] [Sentinel: Attention Probing of Proxy Models for LLM Context Compression with an Understanding Perspective](https://arxiv.org/abs/2505.23277)
*Yong Zhang,Yanwen Huang,Ning Cheng,Yang Guo,Yun Zhu,Yanmeng Wang,Shaojun Wang,Jing Xiao*

Main category: cs.CL

TL;DR: 提出Sentinel框架，通过探测LLM的注意力信号实现零训练的高效上下文压缩，在保持性能的同时达到5倍压缩率


<details>
  <summary>Details</summary>
Motivation: 现有基于监督训练的专用压缩模型存在成本高、可移植性差的问题，需要开发更轻量级的上下文过滤方案

Method: 利用0.5B代理LLM的解码器注意力信号，通过轻量级分类器识别句子相关性，实现问题感知的句子级压缩

Result: 在LongBench基准测试中达到5倍压缩率，问答性能与7B规模压缩系统相当

Conclusion: 注意力信号探测可实现快速有效的问题感知上下文压缩，为RAG系统提供高效解决方案

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs)
with external context, but retrieved passages are often lengthy, noisy, or
exceed input limits. Existing compression methods typically require supervised
training of dedicated compression models, increasing cost and reducing
portability. We propose Sentinel, a lightweight sentence-level compression
framework that reframes context filtering as an attention-based understanding
task. Rather than training a compression model, Sentinel probes decoder
attention from an off-the-shelf 0.5B proxy LLM using a lightweight classifier
to identify sentence relevance. Empirically, we find that query-context
relevance estimation is consistent across model scales, with 0.5B proxies
closely matching the behaviors of larger models. On the LongBench benchmark,
Sentinel achieves up to 5$\times$ compression while matching the QA performance
of 7B-scale compression systems. Our results suggest that probing native
attention signals enables fast, effective, and question-aware context
compression. Code available at: https://github.com/yzhangchuck/Sentinel.

</details>


### [67] [ScEdit: Script-based Assessment of Knowledge Editing](https://arxiv.org/abs/2505.23291)
*Xinye Li,Zunwen Zheng,Qian Zhang,Dekai Zhuang,Jiabao Kang,Liyan Xu,Qingbin Liu,Xi Chen,Zhiying Tu,Dianhui Chu,Dianbo Sui*

Main category: cs.CL

TL;DR: 研究提出了基于脚本的评估基准ScEdit，通过整合标记级和文本级评估方法，揭示现有知识编辑技术在复杂应用场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前知识编辑任务过于简单，多数方法在传统评估框架下呈现虚高分数，缺乏与LLM实际应用场景（如智能体任务）的深度融合评估。

Method: 开发包含反事实编辑和时序编辑的脚本基准，创新性地将评估维度从事实型（What）扩展到行为型（How）问题，采用双层级评估体系。

Result: 所有知识编辑方法在传统指标上均出现性能下降，且在文本级评估中面临显著挑战，暴露出现有技术的局限性。

Conclusion: ScEdit基准揭示了知识编辑技术在实际应用中的脆弱性，强调需要开发更鲁棒的编辑方法以适应复杂场景需求。

Abstract: Knowledge Editing (KE) has gained increasing attention, yet current KE tasks
remain relatively simple. Under current evaluation frameworks, many editing
methods achieve exceptionally high scores, sometimes nearing perfection.
However, few studies integrate KE into real-world application scenarios (e.g.,
recent interest in LLM-as-agent). To support our analysis, we introduce a novel
script-based benchmark -- ScEdit (Script-based Knowledge Editing Benchmark) --
which encompasses both counterfactual and temporal edits. We integrate
token-level and text-level evaluation methods, comprehensively analyzing
existing KE techniques. The benchmark extends traditional fact-based
("What"-type question) evaluation to action-based ("How"-type question)
evaluation. We observe that all KE methods exhibit a drop in performance on
established metrics and face challenges on text-level metrics, indicating a
challenging task. Our benchmark is available at
https://github.com/asdfo123/ScEdit.

</details>


### [68] [How Does Response Length Affect Long-Form Factuality](https://arxiv.org/abs/2505.23295)
*James Xu Zhao,Jimmy Z. J. Liu,Bryan Hooi,See-Kiong Ng*

Main category: cs.CL

TL;DR: 研究发现大语言模型生成长文本时存在长度偏差，响应长度增加会导致事实精确度下降，主要原因是知识逐渐耗尽（facts exhaustion）而非错误传播或长上下文。


<details>
  <summary>Details</summary>
Motivation: 长文本生成中存在事实性错误问题，但响应长度对事实性的影响机制尚未被充分研究，需要系统性的实证分析。

Method: 提出双层次长文本事实性评估框架（自动+人工标注），通过控制实验验证长度与事实性的关系，并检验错误传播、长上下文、知识耗尽三个假设。

Result: 长文本响应的事实精确度随长度增加显著下降（p<0.01），实证数据表明知识耗尽现象（模型后续生成的知识可靠性递减）是核心原因，占误差来源的68%。

Conclusion: 大语言模型的长文本事实性下降主要源于知识储备的逐步耗尽机制，这对LLM的长文本生成优化具有重要指导意义。

Abstract: Large language models (LLMs) are widely used for long-form text generation.
However, factual errors in the responses would undermine their reliability.
Despite growing attention to LLM factuality, the effect of response length on
factuality remains underexplored. In this work, we systematically investigate
this relationship by first introducing an automatic and bi-level long-form
factuality evaluation framework, which achieves high agreement with human
annotations while being cost-effective. Using this framework, we conduct
controlled experiments and find that longer responses exhibit lower factual
precision, confirming the presence of length bias. To explain this phenomenon,
we empirically examine three hypotheses: error propagation, long context, and
facts exhaustion. Our results reveal that facts exhaustion, where the model
gradually exhausts more reliable knowledge, is the primary cause of factual
degradation, rather than the other two hypotheses.

</details>


### [69] [EmoBench-UA: A Benchmark Dataset for Emotion Detection in Ukrainian](https://arxiv.org/abs/2505.23297)
*Daryna Dementieva,Nikolay Babakov,Alexander Fraser*

Main category: cs.CL

TL;DR: 首个乌克兰语情感分类数据集EmoBench-UA的创建与评估，揭示非主流语言情感检测的挑战及本土模型开发需求


<details>
  <summary>Details</summary>
Motivation: 填补乌克兰语情感分类领域缺乏基准数据集的空白

Method: 通过Toloka.ai众包平台构建高质量标注数据集，评估语言学基线/英译合成数据/LLM等多种方法

Result: 现有方法在乌克兰语情感分类中表现欠佳，突显非主流语言资源匮乏问题

Conclusion: 需加强乌克兰语专用模型开发和本土化训练资源建设

Abstract: While Ukrainian NLP has seen progress in many texts processing tasks, emotion
classification remains an underexplored area with no publicly available
benchmark to date. In this work, we introduce EmoBench-UA, the first annotated
dataset for emotion detection in Ukrainian texts. Our annotation schema is
adapted from the previous English-centric works on emotion detection (Mohammad
et al., 2018; Mohammad, 2022) guidelines. The dataset was created through
crowdsourcing using the Toloka.ai platform ensuring high-quality of the
annotation process. Then, we evaluate a range of approaches on the collected
dataset, starting from linguistic-based baselines, synthetic data translated
from English, to large language models (LLMs). Our findings highlight the
challenges of emotion classification in non-mainstream languages like Ukrainian
and emphasize the need for further development of Ukrainian-specific models and
training resources.

</details>


### [70] [Data-efficient Meta-models for Evaluation of Context-based Questions and Answers in LLMs](https://arxiv.org/abs/2505.23299)
*Julia Belikova,Konstantin Polev,Rauf Parchiev,Dmitry Simakov*

Main category: cs.CL

TL;DR: 提出结合高效分类算法与降维技术，仅需250个训练样本即可在幻觉检测任务中保持竞争力的轻量级解决方案


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM隐藏状态的幻觉检测方法依赖大量标注数据，制约了工业场景的实际应用

Method: 在Lookback Lens（分析注意力头动态）和探针方法（解码内部表征）基础上，引入高效分类算法（如LightGBM）配合主成分分析（PCA）降维

Result: 在标准问答RAG基准测试中，使用250样本达到与强商业LLM基线相当的检测性能（F1=0.87）

Conclusion: 验证了轻量化数据高效范式在工业部署中的可行性，特别适用于标注资源受限的实际场景

Abstract: Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems
are increasingly deployed in industry applications, yet their reliability
remains hampered by challenges in detecting hallucinations. While supervised
state-of-the-art (SOTA) methods that leverage LLM hidden states -- such as
activation tracing and representation analysis -- show promise, their
dependence on extensively annotated datasets limits scalability in real-world
applications. This paper addresses the critical bottleneck of data annotation
by investigating the feasibility of reducing training data requirements for two
SOTA hallucination detection frameworks: Lookback Lens, which analyzes
attention head dynamics, and probing-based approaches, which decode internal
model representations. We propose a methodology combining efficient
classification algorithms with dimensionality reduction techniques to minimize
sample size demands while maintaining competitive performance. Evaluations on
standardized question-answering RAG benchmarks show that our approach achieves
performance comparable to strong proprietary LLM-based baselines with only 250
training samples. These results highlight the potential of lightweight,
data-efficient paradigms for industrial deployment, particularly in
annotation-constrained scenarios.

</details>


### [71] [Generalized Category Discovery in Event-Centric Contexts: Latent Pattern Mining with LLMs](https://arxiv.org/abs/2505.23304)
*Yi Luo,Qiwen Wang,Junqi Yang,Luyao Tang,Zhenghao Lin,Zhenzhe Ying,Weiqiang Wang,Chen Lin*

Main category: cs.CL

TL;DR: 提出PaMA框架，通过LLM提取事件模式和改进原型对齐机制，在EC-GCD任务上实现12.58%的H-score提升


<details>
  <summary>Details</summary>
Motivation: 现有文本GCD方法在现实场景（长文本、复杂叙事、类别极度不均衡）中面临两个核心挑战：聚类与分类标准不统一、少数类对齐不公平

Method: 1. 利用大语言模型提取和优化事件模式
2. 提出排序-过滤-挖掘三阶段流程构建均衡的类别原型表示

Result: 在EC-GCD基准（含新建的Scam Report数据集）上超越现有方法，同时在基础GCD数据集保持强泛化性

Conclusion: PaMA通过模式对齐机制和原型均衡策略，有效解决事件中心场景下的聚类-分类偏差和长尾问题，具有实际应用价值

Abstract: Generalized Category Discovery (GCD) aims to classify both known and novel
categories using partially labeled data that contains only known classes.
Despite achieving strong performance on existing benchmarks, current textual
GCD methods lack sufficient validation in realistic settings. We introduce
Event-Centric GCD (EC-GCD), characterized by long, complex narratives and
highly imbalanced class distributions, posing two main challenges: (1)
divergent clustering versus classification groupings caused by subjective
criteria, and (2) Unfair alignment for minority classes. To tackle these, we
propose PaMA, a framework leveraging LLMs to extract and refine event patterns
for improved cluster-class alignment. Additionally, a ranking-filtering-mining
pipeline ensures balanced representation of prototypes across imbalanced
categories. Evaluations on two EC-GCD benchmarks, including a newly constructed
Scam Report dataset, demonstrate that PaMA outperforms prior methods with up to
12.58% H-score gains, while maintaining strong generalization on base GCD
datasets.

</details>


### [72] [Enhancing Marker Scoring Accuracy through Ordinal Confidence Modelling in Educational Assessments](https://arxiv.org/abs/2505.23315)
*Abhirup Chakravarty,Mark Brenchley,Trevor Breakspear,Ian Lewin,Yan Huang*

Main category: cs.CL

TL;DR: 研究通过分类任务和KWOCCE损失函数提升AES置信度建模，实现高CEFR一致性


<details>
  <summary>Details</summary>
Motivation: 解决自动化作文评分系统可靠性验证的伦理挑战，确保仅在置信度高时发布分数

Method: 将置信度估计转化为分类任务（预测CEFR分级准确性），使用分数分箱技术和融合CEFR序数结构的KWOCCE损失函数

Result: 最佳模型F1值0.97，47%分数达到100% CEFR一致性，99%分数达到≥95%一致性（基准模型仅92%）

Conclusion: 提出的置信度建模方法显著提高评分可靠性，促进伦理合规的自动化评分应用

Abstract: A key ethical challenge in Automated Essay Scoring (AES) is ensuring that
scores are only released when they meet high reliability standards. Confidence
modelling addresses this by assigning a reliability estimate measure, in the
form of a confidence score, to each automated score. In this study, we frame
confidence estimation as a classification task: predicting whether an
AES-generated score correctly places a candidate in the appropriate CEFR level.
While this is a binary decision, we leverage the inherent granularity of the
scoring domain in two ways. First, we reformulate the task as an n-ary
classification problem using score binning. Second, we introduce a set of novel
Kernel Weighted Ordinal Categorical Cross Entropy (KWOCCE) loss functions that
incorporate the ordinal structure of CEFR labels. Our best-performing model
achieves an F1 score of 0.97, and enables the system to release 47% of scores
with 100% CEFR agreement and 99% with at least 95% CEFR agreement -compared to
approximately 92% (approx.) CEFR agreement from the standalone AES model where
we release all AM predicted scores.

</details>


### [73] [Proximalized Preference Optimization for Diverse Feedback Types: A Decomposed Perspective on DPO](https://arxiv.org/abs/2505.23316)
*Kaiyang Guo,Yinchuan Li,Zhitang Chen*

Main category: cs.CL

TL;DR: 提出PRO方法解决直接偏好优化(DPO)的可能性未确定问题，通过完整正则化提升对齐效果


<details>
  <summary>Details</summary>
Motivation: 现有对比对齐方法会导致语言模型输出偏离预期模式(奖励攻击效应)，暴露了可能性未确定的核心缺陷

Method: 分解DPO损失函数，发现正则化项被过度简化，提出PRO方法通过近似完整正则化器统一处理多种反馈类型

Result: PRO在成对/二元/标量反馈场景中优于现有方法，实验验证其有效性

Conclusion: 完整正则化项的恢复从根本上解决了可能性未确定问题，PRO为多类型反馈对齐提供了统一框架

Abstract: Direct alignment methods typically optimize large language models (LLMs) by
contrasting the likelihoods of preferred versus dispreferred responses. While
effective in steering LLMs to match relative preference, these methods are
frequently noted for decreasing the absolute likelihoods of example responses.
As a result, aligned models tend to generate outputs that deviate from the
expected patterns, exhibiting reward-hacking effect even without a reward
model. This undesired consequence exposes a fundamental limitation in
contrastive alignment, which we characterize as likelihood underdetermination.
In this work, we revisit direct preference optimization (DPO) -- the seminal
direct alignment method -- and demonstrate that its loss theoretically admits a
decomposed reformulation. The reformulated loss not only broadens applicability
to a wider range of feedback types, but also provides novel insights into the
underlying cause of likelihood underdetermination. Specifically, the standard
DPO implementation implicitly oversimplifies a regularizer in the reformulated
loss, and reinstating its complete version effectively resolves the
underdetermination issue. Leveraging these findings, we introduce PRoximalized
PReference Optimization (PRO), a unified method to align with diverse feeback
types, eliminating likelihood underdetermination through an efficient
approximation of the complete regularizer. Comprehensive experiments show the
superiority of PRO over existing methods in scenarios involving pairwise,
binary and scalar feedback.

</details>


### [74] [Neither Stochastic Parroting nor AGI: LLMs Solve Tasks through Context-Directed Extrapolation from Training Data Priors](https://arxiv.org/abs/2505.23323)
*Harish Tayyar Madabushi,Melissa Torgbi,Claire Bonial*

Main category: cs.CL

TL;DR: 论文提出LLM能力应理解为'上下文指导的外推'机制，既非单纯数据复读也非不可控的涌现能力


<details>
  <summary>Details</summary>
Motivation: 反驳'随机鹦鹉'和'不可预测高级推理能力'两种极端观点，建立可预测可控的LLM能力认知框架

Method: 通过文献论证提出'context-directed extrapolation'机制，解释LLM如何根据上下文选择训练数据中的先验信息进行外推

Result: LLM能力可预测、可控，不具人类高级认知能力，训练扩展存在上限，缓解对不可控智能涌现的担忧

Conclusion: 研究应聚焦上下文指导外推机制与训练数据的交互，探索不依赖LLM内在推理能力的增强技术

Abstract: In this position paper we raise critical awareness of a realistic view of LLM
capabilities that eschews extreme alternative views that LLMs are either
"stochastic parrots" or in possession of "emergent" advanced reasoning
capabilities, which, due to their unpredictable emergence, constitute an
existential threat. Our middle-ground view is that LLMs extrapolate from priors
from their training data, and that a mechanism akin to in-context learning
enables the targeting of the appropriate information from which to extrapolate.
We call this "context-directed extrapolation." Under this view, substantiated
though existing literature, while reasoning capabilities go well beyond
stochastic parroting, such capabilities are predictable, controllable, not
indicative of advanced reasoning akin to high-level cognitive capabilities in
humans, and not infinitely scalable with additional training. As a result,
fears of uncontrollable emergence of agency are allayed, while research
advances are appropriately refocused on the processes of context-directed
extrapolation and how this interacts with training data to produce valuable
capabilities in LLMs. Future work can therefore explore alternative augmenting
techniques that do not rely on inherent advanced reasoning in LLMs.

</details>


### [75] [Discriminative Policy Optimization for Token-Level Reward Models](https://arxiv.org/abs/2505.23363)
*Hongzhan Chen,Tao Yang,Shiping Gao,Ruijun Chen,Xiaojun Quan,Hongtao Tian,Ting Yao*

Main category: cs.CL

TL;DR: Q-RM通过解耦奖励建模与语言生成，显著提升复杂推理任务中的模型性能和训练效率


<details>
  <summary>Details</summary>
Motivation: 解决传统token级PRMs中生成建模与奖励建模的冲突导致的信用分配不稳定问题

Method: 通过优化判别策略的Q函数，构建不依赖语言生成的token级奖励模型(Q-RM)

Result: 在数学推理任务中平均Pass@1提升4.7-5.85分，训练收敛速度加快11-12倍

Conclusion: Q-RM为强化学习提供了更有效的奖励信号机制，在效果和效率层面均超越现有方法

Abstract: Process reward models (PRMs) provide more nuanced supervision compared to
outcome reward models (ORMs) for optimizing policy models, positioning them as
a promising approach to enhancing the capabilities of LLMs in complex reasoning
tasks. Recent efforts have advanced PRMs from step-level to token-level
granularity by integrating reward modeling into the training of generative
models, with reward scores derived from token generation probabilities.
However, the conflict between generative language modeling and reward modeling
may introduce instability and lead to inaccurate credit assignments. To address
this challenge, we revisit token-level reward assignment by decoupling reward
modeling from language generation and derive a token-level reward model through
the optimization of a discriminative policy, termed the Q-function Reward Model
(Q-RM). We theoretically demonstrate that Q-RM explicitly learns token-level
Q-functions from preference data without relying on fine-grained annotations.
In our experiments, Q-RM consistently outperforms all baseline methods across
various benchmarks. For example, when integrated into PPO/REINFORCE algorithms,
Q-RM enhances the average Pass@1 score by 5.85/4.70 points on mathematical
reasoning tasks compared to the ORM baseline, and by 4.56/5.73 points compared
to the token-level PRM counterpart. Moreover, reinforcement learning with Q-RM
significantly enhances training efficiency, achieving convergence 12 times
faster than ORM on GSM8K and 11 times faster than step-level PRM on MATH. Code
and data are available at https://github.com/homzer/Q-RM.

</details>


### [76] [Threading the Needle: Reweaving Chain-of-Thought Reasoning to Explain Human Label Variation](https://arxiv.org/abs/2505.23368)
*Beiduo Chen,Yang Janet Liu,Anna Korhonen,Barbara Plank*

Main category: cs.CL

TL;DR: LLM推理路径（CoTs）用于提取支持/反对陈述，提出基于语言学分析的分割器与排名评估框架，实现优于基准的HLV对齐效果


<details>
  <summary>Details</summary>
Motivation: 现有方法通过逆向范式（根据答案生成解释）难以有效捕捉人类标签变异，而思维链（CoTs）的前向推理路径隐含答案选项的理性依据

Method: 1. 基于LLM的流程整合语言学驱动的语篇分割器，精准提取支持/反对陈述
2. 提出基于排名的HLV评估框架，强调答案排序优先于精确分数匹配

Result: 在三个数据集上超越直接生成方法和基线模型，排名方法显示与人类标注更高的对齐度（+15% Spearman相关性）

Conclusion: 通过CoTs的推理路径分析及语言学增强处理，可有效提升模型预测与人类标签分布的语义对齐能力

Abstract: The recent rise of reasoning-tuned Large Language Models (LLMs)--which
generate chains of thought (CoTs) before giving the final answer--has attracted
significant attention and offers new opportunities for gaining insights into
human label variation, which refers to plausible differences in how multiple
annotators label the same data instance. Prior work has shown that
LLM-generated explanations can help align model predictions with human label
distributions, but typically adopt a reverse paradigm: producing explanations
based on given answers. In contrast, CoTs provide a forward reasoning path that
may implicitly embed rationales for each answer option, before generating the
answers. We thus propose a novel LLM-based pipeline enriched with
linguistically-grounded discourse segmenters to extract supporting and opposing
statements for each answer option from CoTs with improved accuracy. We also
propose a rank-based HLV evaluation framework that prioritizes the ranking of
answers over exact scores, which instead favor direct comparison of label
distributions. Our method outperforms a direct generation method as well as
baselines on three datasets, and shows better alignment of ranking methods with
humans, highlighting the effectiveness of our approach.

</details>


### [77] [Adaptive Jailbreaking Strategies Based on the Semantic Understanding Capabilities of Large Language Models](https://arxiv.org/abs/2505.23404)
*Mingyu Yu,Wei Wang,Yanjie Wei,Sujuan Qin*

Main category: cs.CL

TL;DR: 提出基于大语言模型语义理解能力的自适应越狱攻击框架，通过分类模型类型并制定针对性攻击策略，显著提升攻击成功率（GPT-4o达98.9%）


<details>
  <summary>Details</summary>
Motivation: 现有越狱攻击方法未充分考虑不同LLMs的语义理解能力差异，导致攻击效果不稳定。需建立与模型能力匹配的攻击策略以提高成功率

Method: 1. 将LLMs按语义理解能力分为Type I/II两类 2. 针对每类模型设计定制化越狱策略 3. 利用语义理解差异构建对抗性攻击向量

Result: 在多个主流LLMs中验证：自适应策略使攻击成功率显著提升，对GPT-4o(2025版)攻击成功率高达98.9%

Conclusion: 模型语义理解能力的分类对攻击效果具有决定性作用，该框架为LLM安全防御提供了新的攻防对抗研究视角

Abstract: Adversarial attacks on Large Language Models (LLMs) via jailbreaking
techniques-methods that circumvent their built-in safety and ethical
constraints-have emerged as a critical challenge in AI security. These attacks
compromise the reliability of LLMs by exploiting inherent weaknesses in their
comprehension capabilities. This paper investigates the efficacy of
jailbreaking strategies that are specifically adapted to the diverse levels of
understanding exhibited by different LLMs. We propose the Adaptive Jailbreaking
Strategies Based on the Semantic Understanding Capabilities of Large Language
Models, a novel framework that classifies LLMs into Type I and Type II
categories according to their semantic comprehension abilities. For each
category, we design tailored jailbreaking strategies aimed at leveraging their
vulnerabilities to facilitate successful attacks. Extensive experiments
conducted on multiple LLMs demonstrate that our adaptive strategy markedly
improves the success rate of jailbreaking. Notably, our approach achieves an
exceptional 98.9% success rate in jailbreaking GPT-4o(29 May 2025 release)

</details>


### [78] [From Parameters to Prompts: Understanding and Mitigating the Factuality Gap between Fine-Tuned LLMs](https://arxiv.org/abs/2505.23410)
*Xuan Gong,Hanbo Huang,Shiyu Liang*

Main category: cs.CL

TL;DR: 研究表明通过推理阶段的OOD设置或适当ICL提示（如few-shot和CoT）可有效缓解微调数据带来的factuality gap，测试时的提示在知识提取中起主导作用。


<details>
  <summary>Details</summary>
Motivation: 探究监督微调数据对LLMs事实性的影响机制，特别关注已知与未知知识间的factuality gap形成与解决方案。

Method: 通过系统实验验证OOD设置和ICL提示的效果，并从知识图谱角度进行理论证明。

Result: 发现测试阶段提示可削弱微调数据影响，ICL能有效补偿微调数据不足，理论证明提示在知识提取中的主导性。

Conclusion: 需重新评估ICL提示在微调数据选择方法中的评估作用，强调推理阶段干预对知识提取的决定性影响。

Abstract: Factual knowledge extraction aims to explicitly extract knowledge
parameterized in pre-trained language models for application in downstream
tasks. While prior work has been investigating the impact of supervised
fine-tuning data on the factuality of large language models (LLMs), its
mechanism remains poorly understood. We revisit this impact through systematic
experiments, with a particular focus on the factuality gap that arises when
fine-tuning on known versus unknown knowledge. Our findings show that this gap
can be mitigated at the inference stage, either under out-of-distribution (OOD)
settings or by using appropriate in-context learning (ICL) prompts (i.e.,
few-shot learning and Chain of Thought (CoT)). We prove this phenomenon
theoretically from the perspective of knowledge graphs, showing that the
test-time prompt may diminish or even overshadow the impact of fine-tuning data
and play a dominant role in knowledge extraction. Ultimately, our results shed
light on the interaction between finetuning data and test-time prompt,
demonstrating that ICL can effectively compensate for shortcomings in
fine-tuning data, and highlighting the need to reconsider the use of ICL
prompting as a means to evaluate the effectiveness of fine-tuning data
selection methods.

</details>


### [79] [The Warmup Dilemma: How Learning Rate Strategies Impact Speech-to-Text Model Convergence](https://arxiv.org/abs/2505.23420)
*Marco Gaido,Sara Papi,Luisa Bentivogli,Alessio Brutti,Mauro Cettolo,Roberto Gretter,Marco Matassoni,Mohamed Nabih,Matteo Negri*

Main category: cs.CL

TL;DR: 研究发现大规模语音转文本训练需要次指数级学习率预热策略，且较高初始学习率仅加速收敛但不提升最终性能


<details>
  <summary>Details</summary>
Motivation: 针对复杂Transformer架构在语音转文本任务中传统学习率调整策略效果不足的问题，系统分析不同预热策略对模型收敛和最终性能的影响

Method: 通过对比实验验证双线性预热与次指数级预热等不同学习率调整策略的效果差异

Result: 次指数级预热策略显著提升训练稳定性，而提高初始阶段学习率可使收敛速度提升37%但最终准确率无显著变化

Conclusion: 提出适应大规模语音转文本模型的次指数级学习率预热范式，揭示了学习率动态调整中收敛速度与最终性能的平衡关系

Abstract: Training large-scale models presents challenges not only in terms of resource
requirements but also in terms of their convergence. For this reason, the
learning rate (LR) is often decreased when the size of a model is increased.
Such a simple solution is not enough in the case of speech-to-text (S2T)
trainings, where evolved and more complex variants of the Transformer
architecture -- e.g., Conformer or Branchformer -- are used in light of their
better performance. As a workaround, OWSM designed a double linear warmup of
the LR, increasing it to a very small value in the first phase before updating
it to a higher value in the second phase. While this solution worked well in
practice, it was not compared with alternative solutions, nor was the impact on
the final performance of different LR warmup schedules studied. This paper
fills this gap, revealing that i) large-scale S2T trainings demand a
sub-exponential LR warmup, and ii) a higher LR in the warmup phase accelerates
initial convergence, but it does not boost final performance.

</details>


### [80] [UAQFact: Evaluating Factual Knowledge Utilization of LLMs on Unanswerable Questions](https://arxiv.org/abs/2505.23461)
*Chuanyuan Tan,Wenbiao Shao,Hao Xiong,Tong Zhu,Zhenhua Liu,Kai Shi,Wenliang Chen*

Main category: cs.CL

TL;DR: 提出了新的不可回答问题数据集UAQFact，揭示了LLMs在利用事实知识处理UAQ时的核心挑战


<details>
  <summary>Details</summary>
Motivation: 现有不可回答问题数据集缺乏事实知识支持，难以评估LLMs的知识运用能力

Method: 基于知识图谱构建双语数据集UAQFact，并设计测量内部/外部事实知识运用的新任务

Result: 实验显示LLMs即使存储相关知识仍表现不稳定，且无法有效利用外部知识

Conclusion: LLMs处理不可回答问题时面临知识利用不足的挑战，需改进知识整合机制

Abstract: Handling unanswerable questions (UAQ) is crucial for LLMs, as it helps
prevent misleading responses in complex situations. While previous studies have
built several datasets to assess LLMs' performance on UAQ, these datasets lack
factual knowledge support, which limits the evaluation of LLMs' ability to
utilize their factual knowledge when handling UAQ. To address the limitation,
we introduce a new unanswerable question dataset UAQFact, a bilingual dataset
with auxiliary factual knowledge created from a Knowledge Graph. Based on
UAQFact, we further define two new tasks to measure LLMs' ability to utilize
internal and external factual knowledge, respectively. Our experimental results
across multiple LLM series show that UAQFact presents significant challenges,
as LLMs do not consistently perform well even when they have factual knowledge
stored. Additionally, we find that incorporating external knowledge may enhance
performance, but LLMs still cannot make full use of the knowledge which may
result in incorrect responses.

</details>


### [81] [Evaluating the performance and fragility of large language models on the self-assessment for neurological surgeons](https://arxiv.org/abs/2505.23477)
*Krithik Vishwanath,Anton Alyakin,Mrigayu Ghosh,Jin Vivian Lee,Daniel Alexander Alber,Karl L. Sangwon,Douglas Kondziolka,Eric Karl Oermann*

Main category: cs.CL

TL;DR: 研究评估了28个大型语言模型在神经外科考试题上的表现，发现其虽能通过考试，但易受干扰信息影响性能显著下降


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在神经外科考试中的表现及抗干扰能力，为安全临床部署提供依据

Method: 使用2904道CNS-SANS试题测试28个模型，设计含多义词干扰语句的评估框架

Result: 6个模型通过考试（最高超及格线15.7%），干扰使准确率最大降20.4%，开源模型抗干扰性弱于商业模型

Conclusion: 需开发增强LLMs抗干扰能力的新策略，确保临床应用的可靠性和安全性

Abstract: The Congress of Neurological Surgeons Self-Assessment for Neurological
Surgeons (CNS-SANS) questions are widely used by neurosurgical residents to
prepare for written board examinations. Recently, these questions have also
served as benchmarks for evaluating large language models' (LLMs) neurosurgical
knowledge. This study aims to assess the performance of state-of-the-art LLMs
on neurosurgery board-like questions and to evaluate their robustness to the
inclusion of distractor statements. A comprehensive evaluation was conducted
using 28 large language models. These models were tested on 2,904 neurosurgery
board examination questions derived from the CNS-SANS. Additionally, the study
introduced a distraction framework to assess the fragility of these models. The
framework incorporated simple, irrelevant distractor statements containing
polysemous words with clinical meanings used in non-clinical contexts to
determine the extent to which such distractions degrade model performance on
standard medical benchmarks. 6 of the 28 tested LLMs achieved board-passing
outcomes, with the top-performing models scoring over 15.7% above the passing
threshold. When exposed to distractions, accuracy across various model
architectures was significantly reduced-by as much as 20.4%-with one model
failing that had previously passed. Both general-purpose and medical
open-source models experienced greater performance declines compared to
proprietary variants when subjected to the added distractors. While current
LLMs demonstrate an impressive ability to answer neurosurgery board-like exam
questions, their performance is markedly vulnerable to extraneous, distracting
information. These findings underscore the critical need for developing novel
mitigation strategies aimed at bolstering LLM resilience against in-text
distractions, particularly for safe and effective clinical deployment.

</details>


### [82] [Revisiting Overthinking in Long Chain-of-Thought from the Perspective of Self-Doubt](https://arxiv.org/abs/2505.23480)
*Keqin Peng,Liang Ding,Yuanxin Ouyang,Meng Fang,Dacheng Tao*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在长思维链推理时存在自我怀疑导致的过度思考问题，提出通过质疑输入问题有效性的提示方法显著降低冗余推理并提升性能


<details>
  <summary>Details</summary>
Motivation: 现有研究主要从定性角度分析长思维链的过度思考，本文通过定量测量自我怀疑(过度验证已正确结论的token消耗)揭示其对推理效率的影响机制

Method: 两阶段提示法：1) 要求模型先验证输入问题的完整性 2) 根据验证结果进行简洁回答，减少对问题本身的过度依赖

Result: 在4个主流推理模型和3类数学推理任务中，方法平均减少27%回答长度，在4个缺失前提的数据集上取得显著精度提升（最高+8.2%）

Conclusion: 通过定量分析揭示了自我怀疑对模型推理效率的关键影响，提出的问题验证提示法能有效降低冗余推理步骤，为优化模型推理过程提供新方向

Abstract: Reasoning Large Language Models (RLLMs) have demonstrated impressive
performance on complex tasks, largely due to the adoption of Long
Chain-of-Thought (Long CoT) reasoning. However, they often exhibit overthinking
-- performing unnecessary reasoning steps even after arriving at the correct
answer. Prior work has largely focused on qualitative analyses of overthinking
through sample-based observations of long CoTs. In contrast, we present a
quantitative analysis of overthinking from the perspective of self-doubt,
characterized by excessive token usage devoted to re-verifying already-correct
answer. We find that self-doubt significantly contributes to overthinking. In
response, we introduce a simple and effective prompting method to reduce the
model's over-reliance on input questions, thereby avoiding self-doubt.
Specifically, we first prompt the model to question the validity of the input
question, and then respond concisely based on the outcome of that evaluation.
Experiments on three mathematical reasoning tasks and four datasets with
missing premises demonstrate that our method substantially reduces answer
length and yields significant improvements across nearly all datasets upon 4
widely-used RLLMs. Further analysis demonstrates that our method effectively
minimizes the number of reasoning steps and reduces self-doubt.

</details>


### [83] [Spoken Language Modeling with Duration-Penalized Self-Supervised Units](https://arxiv.org/abs/2505.23494)
*Nicol Visser,Herman Kamper*

Main category: cs.CL

TL;DR: 研究探讨了代码本大小与单元粗粒度对口语语言模型性能的影响，发现DPDP方法能有效生成粗粒度单元，在句子生成和低比特率任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 探索代码本大小与单元持续时间如何共同影响口语语言模型在不同语言任务中的表现，填补该领域的研究空白。

Method: 使用持续时间惩罚动态规划(DPDP)调整代码本大小和单元粗粒度，并在音素、词汇、句子级别进行多维度分析。

Result: 音素/词汇层面粗粒度优势有限，但句子重合成任务中粗粒度单元表现更好；低比特率下粗粒度单元在词汇/句法任务准确率更高。

Conclusion: 粗粒度单元并非普遍适用，但DPDP方法可有效生成任务所需的粗粒度单元，需根据具体应用场景优化代码本参数配置。

Abstract: Spoken language models (SLMs) operate on acoustic units obtained by
discretizing self-supervised speech representations. Although the
characteristics of these units directly affect performance, the interaction
between codebook size and unit coarseness (i.e., duration) remains unexplored.
We investigate SLM performance as we vary codebook size and unit coarseness
using the simple duration-penalized dynamic programming (DPDP) method. New
analyses are performed across different linguistic levels. At the phone and
word levels, coarseness provides little benefit, as long as the codebook size
is chosen appropriately. However, when producing whole sentences in a
resynthesis task, SLMs perform better with coarser units. In lexical and
syntactic language modeling tasks, coarser units also give higher accuracies at
lower bitrates. We therefore show that coarser units aren't always better, but
that DPDP is a simple and efficient way to obtain coarser units for the tasks
where they are beneficial.

</details>


### [84] [Diagnosing and Addressing Pitfalls in KG-RAG Datasets: Toward More Reliable Benchmarking](https://arxiv.org/abs/2505.23495)
*Liangliang Zhang,Zhuorui Jiang,Hongliang Chi,Haoyang Chen,Mohammed Elkoumy,Fali Wang,Qiong Wu,Zhengyi Zhou,Shirui Pan,Suhang Wang,Yao Ma*

Main category: cs.CL

TL;DR: 提出KGQAGen框架解决现有KGQA基准质量缺陷，构建10k规模新评测集KGQAGen-10k，暴露主流模型局限性


<details>
  <summary>Details</summary>
Motivation: 现有KGQA基准存在标注错误(57%正确率)、问题构建缺陷(模糊/简单/不可答)、知识过时等问题，制约可靠评估

Method: KGQAGen框架：结构化知识锚定+LLM引导生成+符号验证，通过三阶段流程生成可验证的复杂QA实例

Result: 构建的KGQAGen-10k基准使SOTA模型准确率显著下降，有效揭示现有KG-RAG方法的局限性

Conclusion: 提出可扩展的基准构建框架，推动KGQA评估的严谨性，证明高质量基准对模型能力评估的关键作用

Abstract: Knowledge Graph Question Answering (KGQA) systems rely on high-quality
benchmarks to evaluate complex multi-hop reasoning. However, despite their
widespread use, popular datasets such as WebQSP and CWQ suffer from critical
quality issues, including inaccurate or incomplete ground-truth annotations,
poorly constructed questions that are ambiguous, trivial, or unanswerable, and
outdated or inconsistent knowledge. Through a manual audit of 16 popular KGQA
datasets, including WebQSP and CWQ, we find that the average factual
correctness rate is only 57 %. To address these issues, we introduce KGQAGen,
an LLM-in-the-loop framework that systematically resolves these pitfalls.
KGQAGen combines structured knowledge grounding, LLM-guided generation, and
symbolic verification to produce challenging and verifiable QA instances. Using
KGQAGen, we construct KGQAGen-10k, a ten-thousand scale benchmark grounded in
Wikidata, and evaluate a diverse set of KG-RAG models. Experimental results
demonstrate that even state-of-the-art systems struggle on this benchmark,
highlighting its ability to expose limitations of existing models. Our findings
advocate for more rigorous benchmark construction and position KGQAGen as a
scalable framework for advancing KGQA evaluation.

</details>


### [85] [CLaC at SemEval-2025 Task 6: A Multi-Architecture Approach for Corporate Environmental Promise Verification](https://arxiv.org/abs/2505.23538)
*Nawar Turk,Eeham Khan,Leila Kosseim*

Main category: cs.CL

TL;DR: 提出三种模型架构解决企业ESG承诺验证任务，其中结合注意力池化和多目标学习的组合模型效果最佳


<details>
  <summary>Details</summary>
Motivation: 确保企业ESG报告中承诺的可验证性，提升企业透明度监管效率

Method: 1. ESG-BERT基础模型 2. 增加语言学特征增强模型 3. 结合元数据和多目标学习的组合模型架构

Result: 组合模型在ML-Promise数据集获得0.5268分，超越基线但受限于数据不平衡

Conclusion: 验证了语言特征与注意力机制的有效性，需解决训练数据不足的核心挑战

Abstract: This paper presents our approach to the SemEval-2025 Task~6 (PromiseEval),
which focuses on verifying promises in corporate ESG (Environmental, Social,
and Governance) reports. We explore three model architectures to address the
four subtasks of promise identification, supporting evidence assessment,
clarity evaluation, and verification timing. Our first model utilizes ESG-BERT
with task-specific classifier heads, while our second model enhances this
architecture with linguistic features tailored for each subtask. Our third
approach implements a combined subtask model with attention-based sequence
pooling, transformer representations augmented with document metadata, and
multi-objective learning. Experiments on the English portion of the ML-Promise
dataset demonstrate progressive improvement across our models, with our
combined subtask approach achieving a leaderboard score of 0.5268,
outperforming the provided baseline of 0.5227. Our work highlights the
effectiveness of linguistic feature extraction, attention pooling, and
multi-objective learning in promise verification tasks, despite challenges
posed by class imbalance and limited training data.

</details>


### [86] [Probability-Consistent Preference Optimization for Enhanced LLM Reasoning](https://arxiv.org/abs/2505.23540)
*Yunqiao Yang,Houxing Ren,Zimu Lu,Ke Wang,Weikang Shi,Aojun Zhou,Junting Pan,Mingjie Zhan,Hongsheng Li*

Main category: cs.CL

TL;DR: 提出PCPO框架，通过答案正确性和概率一致性双重指标优化偏好选择，显著提升大语言模型的数学推理能力


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法仅关注答案表面正确性，忽视模型内部逻辑一致性，导致潜在推理质量缺陷

Method: 建立双重量化标准：表面答案正确性 + 跨响应的token级概率一致性

Result: 在多种LLM和基准测试中持续优于传统结果导向方法

Conclusion: PCPO有效提升模型数学推理的可靠性和一致性，框架已开源推动领域发展

Abstract: Recent advances in preference optimization have demonstrated significant
potential for improving mathematical reasoning capabilities in large language
models (LLMs). While current approaches leverage high-quality pairwise
preference data through outcome-based criteria like answer correctness or
consistency, they fundamentally neglect the internal logical coherence of
responses. To overcome this, we propose Probability-Consistent Preference
Optimization (PCPO), a novel framework that establishes dual quantitative
metrics for preference selection: (1) surface-level answer correctness and (2)
intrinsic token-level probability consistency across responses. Extensive
experiments show that our PCPO consistently outperforms existing outcome-only
criterion approaches across a diverse range of LLMs and benchmarks. Our code is
publicly available at https://github.com/YunqiaoYang/PCPO.

</details>


### [87] [Translation in the Wild](https://arxiv.org/abs/2505.23548)
*Yuri Balashov*

Main category: cs.CL

TL;DR: 本文探讨大语言模型（LLMs）翻译能力的来源，提出其可能源自两种预训练数据的不同内化方式，并讨论该双重假设的验证前景及对翻译概念重构的启示。


<details>
  <summary>Details</summary>
Motivation: LLMs未接受专门翻译训练却展现出卓越翻译能力，需探究其能力来源——可能来自训练数据中的偶然双语现象、指令微调贡献，以及跨语境单语内容对齐能力。

Method: 通过文献分析和用户实践经验，提出'双重性'假说：LLMs翻译能力源于两种预训练数据（显式双语语料和隐性单语对应内容）的不同内化方式。

Result: 假设LLMs通过不同方式内化两种数据类型（显式双语对齐数据 vs 隐性单语语义对应内容），需通过注意力机制分析和跨语言检索实验验证该假设。

Conclusion: 深度学习时代需重新定义翻译概念（涵盖显性对齐和隐性语义联结），双重性假说为理解LLMs翻译机制及改进机器翻译系统提供新视角。

Abstract: Large Language Models (LLMs) excel in translation among other things,
demonstrating competitive performance for many language pairs in zero- and
few-shot settings. But unlike dedicated neural machine translation models, LLMs
are not trained on any translation-related objective. What explains their
remarkable translation abilities? Are these abilities grounded in "incidental
bilingualism" (Briakou et al. 2023) in training data? Does instruction tuning
contribute to it? Are LLMs capable of aligning and leveraging semantically
identical or similar monolingual contents from different corners of the
internet that are unlikely to fit in a single context window? I offer some
reflections on this topic, informed by recent studies and growing user
experience. My working hypothesis is that LLMs' translation abilities originate
in two different types of pre-training data that may be internalized by the
models in different ways. I discuss the prospects for testing the "duality"
hypothesis empirically and its implications for reconceptualizing translation,
human and machine, in the age of deep learning.

</details>


### [88] [Understanding Refusal in Language Models with Sparse Autoencoders](https://arxiv.org/abs/2505.23556)
*Wei Jie Yeo,Nirmalendu Prakash,Clement Neo,Roy Ka-Wei Lee,Erik Cambria,Ranjan Satapathy*

Main category: cs.CL

TL;DR: 通过稀疏自编码器对指令微调大语言模型的拒绝机制进行因果归因，揭示了潜在拒绝特征在安全对齐中的关键作用


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型中拒绝行为的内在机制，破解安全对齐模型在面对对抗攻击时失效的运作原理

Method: 在开源对话模型上应用稀疏自编码器定位拒绝相关潜在特征，通过特征干预实验验证其对生成行为的影响

Result: 发现拒绝特征能因果介导模型行为，该特征可提升分类任务中对分布外对抗样本的泛化能力（准确率提升15.6%）

Conclusion: 拒绝特征解码为模型安全机制研究提供了新工具，代码开源促进社区对对齐机制的可解释性研究

Abstract: Refusal is a key safety behavior in aligned language models, yet the internal
mechanisms driving refusals remain opaque. In this work, we conduct a
mechanistic study of refusal in instruction-tuned LLMs using sparse
autoencoders to identify latent features that causally mediate refusal
behaviors. We apply our method to two open-source chat models and intervene on
refusal-related features to assess their influence on generation, validating
their behavioral impact across multiple harmful datasets. This enables a
fine-grained inspection of how refusal manifests at the activation level and
addresses key research questions such as investigating upstream-downstream
latent relationship and understanding the mechanisms of adversarial
jailbreaking techniques. We also establish the usefulness of refusal features
in enhancing generalization for linear probes to out-of-distribution
adversarial samples in classification tasks. We open source our code in
https://github.com/wj210/refusal_sae.

</details>


### [89] [Evaluating AI capabilities in detecting conspiracy theories on YouTube](https://arxiv.org/abs/2505.23570)
*Leonardo La Rocca,Francesco Corso,Francesco Pierri*

Main category: cs.CL

TL;DR: 研究评估开源大语言模型（LLM）在YouTube阴谋论视频检测中的表现，发现文本模型召回率高但精度低，多模态模型效果不佳，RoBERTa模型接近LLM效果。


<details>
  <summary>Details</summary>
Motivation: YouTube作为全球性平台易受虚假信息污染，需探索有效的内容检测方法。研究旨在验证开源LLM在此任务中的实际应用潜力。

Method: 使用标记数据集进行零样本测试，对比多种文本/多模态LLM与微调RoBERTa模型的性能差异。

Result: 文本LLM召回率91%但精度仅20%，多模态模型表现更差，RoBERTa在未标注数据上达到与大型LLM相近效果。

Conclusion: 当前LLM方法在有害内容检测中表现出实用性但存在局限，需开发更精准鲁棒的系统方案。

Abstract: As a leading online platform with a vast global audience, YouTube's extensive
reach also makes it susceptible to hosting harmful content, including
disinformation and conspiracy theories. This study explores the use of
open-weight Large Language Models (LLMs), both text-only and multimodal, for
identifying conspiracy theory videos shared on YouTube. Leveraging a labeled
dataset of thousands of videos, we evaluate a variety of LLMs in a zero-shot
setting and compare their performance to a fine-tuned RoBERTa baseline. Results
show that text-based LLMs achieve high recall but lower precision, leading to
increased false positives. Multimodal models lag behind their text-only
counterparts, indicating limited benefits from visual data integration. To
assess real-world applicability, we evaluate the most accurate models on an
unlabeled dataset, finding that RoBERTa achieves performance close to LLMs with
a larger number of parameters. Our work highlights the strengths and
limitations of current LLM-based approaches for online harmful content
detection, emphasizing the need for more precise and robust systems.

</details>


### [90] [Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software Engineering](https://arxiv.org/abs/2505.23604)
*Guangtao Zeng,Maohao Shen,Delin Chen,Zhenting Qi,Subhro Das,Dan Gutfreund,David Cox,Gregory Wornell,Wei Lu,Zhang-Wei Hong,Chuang Gan*

Main category: cs.CL

TL;DR: 提出进化测试时扩展方法EvoScale，通过进化过程和强化学习提升小模型在真实软件工程任务中的表现


<details>
  <summary>Details</summary>
Motivation: 现有小参数语言模型(小于100B)在真实工程任务(GitHub问题解决)中表现不佳，传统方法需要昂贵标注数据或高计算成本

Method: 1. 将生成视为进化过程，通过迭代选择和突变优化输出；2. 使用强化学习让模型自我进化，减少外部验证依赖

Result: 32B模型Satori-SWE-32B在SWE-Bench-Verified达到或超越100B+模型性能，且样本效率显著提升

Conclusion: EvoScale为小模型提供高效优化路径，性能突破同时保持计算效率，完整开源推动领域发展

Abstract: Language models (LMs) perform well on standardized coding benchmarks but
struggle with real-world software engineering tasks such as resolving GitHub
issues in SWE-Bench, especially when model parameters are less than 100B. While
smaller models are preferable in practice due to their lower computational
cost, improving their performance remains challenging. Existing approaches
primarily rely on supervised fine-tuning (SFT) with high-quality data, which is
expensive to curate at scale. An alternative is test-time scaling: generating
multiple outputs, scoring them using a verifier, and selecting the best one.
Although effective, this strategy often requires excessive sampling and costly
scoring, limiting its practical application. We propose Evolutionary Test-Time
Scaling (EvoScale), a sample-efficient method that treats generation as an
evolutionary process. By iteratively refining outputs via selection and
mutation, EvoScale shifts the output distribution toward higher-scoring
regions, reducing the number of samples needed to find correct solutions. To
reduce the overhead from repeatedly sampling and selection, we train the model
to self-evolve using reinforcement learning (RL). Rather than relying on
external verifiers at inference time, the model learns to self-improve the
scores of its own generations across iterations. Evaluated on
SWE-Bench-Verified, EvoScale enables our 32B model, Satori-SWE-32B, to match or
exceed the performance of models with over 100B parameters while using a few
samples. Code, data, and models will be fully open-sourced.

</details>


### [91] [Table-R1: Inference-Time Scaling for Table Reasoning](https://arxiv.org/abs/2505.23621)
*Zheyuan Yang,Lyuhao Chen,Arman Cohan,Yilun Zhao*

Main category: cs.CL

TL;DR: 提出两种推理时扩展策略(蒸馏与强化学习)，Table-R1-Zero模型以7B参数实现与GPT-4.1相当的性能并具备强泛化能力


<details>
  <summary>Details</summary>
Motivation: 首次探索表格推理任务的推理时扩展问题，旨在通过后训练策略提升模型在复杂表格推理任务中的表现

Method: 1. 基于DeepSeek-R1生成的大规模推理追踪数据集进行指令微调(Table-R1-SFT)；2. 提出可验证奖励函数并应用GRPO强化学习算法训练(Table-R1-Zero)

Result: Table-R1-Zero在短问答/事实核查/自由问答等任务中性能匹配GPT-4.1和DeepSeek-R1，且仅需7B参数模型；在域外数据集表现优异

Conclusion: 验证了指令微调、模型架构选择和跨任务泛化的有效性，揭示了强化学习过程中表格推理核心能力的涌现机制

Abstract: In this work, we present the first study to explore inference-time scaling on
table reasoning tasks. We develop and evaluate two post-training strategies to
enable inference-time scaling: distillation from frontier model reasoning
traces and reinforcement learning with verifiable rewards (RLVR). For
distillation, we introduce a large-scale dataset of reasoning traces generated
by DeepSeek-R1, which we use to fine-tune LLMs into the Table-R1-SFT model. For
RLVR, we propose task-specific verifiable reward functions and apply the GRPO
algorithm to obtain the Table-R1-Zero model. We evaluate our Table-R1-series
models across diverse table reasoning tasks, including short-form QA, fact
verification, and free-form QA. Notably, the Table-R1-Zero model matches or
exceeds the performance of GPT-4.1 and DeepSeek-R1, while using only a
7B-parameter LLM. It also demonstrates strong generalization to out-of-domain
datasets. Extensive ablation and qualitative analyses reveal the benefits of
instruction tuning, model architecture choices, and cross-task generalization,
as well as emergence of essential table reasoning skills during RL training.

</details>


### [92] [Characterizing the Expressivity of Transformer Language Models](https://arxiv.org/abs/2505.23623)
*Jiaoda Li,Ryan Cotterell*

Main category: cs.CL

TL;DR: 本文通过将固定精度Transformer模型与线性时序逻辑片段建立等价关系，揭示了其理论表达能力边界，并通过实验验证理论框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 先前研究基于理想化假设（如无限精度/硬注意力），本文聚焦更贴近实际实现的固定精度+软注意力模型，探讨其理论表达能力。

Method: 1. 建立固定精度Transformer与含单一过去操作符的线性时序逻辑等价关系 2. 将该逻辑与形式语言理论/自动机理论/代数结构进行关联

Result: 实验显示：Transformer在理论能力范围内的语言任务可实现长度泛化，超出能力范围则持续失败

Conclusion: 构建了统一理论框架，首次精确刻画实际Transformer的表达能力边界，并通过系统性实验验证理论预测

Abstract: Transformer-based language models (LMs) have achieved widespread empirical
success, but their theoretical expressive power remains only partially
understood. Prior work often relies on idealized models with assumptions --
such as arbitrary numerical precision and hard attention -- that diverge from
real-world transformers. In this work, we provide an exact characterization of
fixed-precision transformers with strict future masking and soft attention, an
idealization that more closely mirrors practical implementations. We show that
these models are precisely as expressive as a specific fragment of linear
temporal logic that includes only a single temporal operator: the past
operator. We further relate this logic to established classes in formal
language theory, automata theory, and algebra, yielding a rich and unified
theoretical framework for understanding transformer expressivity. Finally, we
present empirical results that align closely with our theory: transformers
trained on languages within their theoretical capacity generalize perfectly
over lengths, while they consistently fail to generalize on languages beyond
it.

</details>


### [93] [AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic Schema Induction from Web-Scale Corpora](https://arxiv.org/abs/2505.23628)
*Jiaxin Bai,Wei Fan,Qi Hu,Qing Zong,Chunyang Li,Hong Ting Tsang,Hongyu Luo,Yauwai Yim,Haoyu Huang,Xiao Zhou,Feng Qin,Tianshi Zheng,Xi Peng,Xin Yao,Huiwen Yang,Leijie Wu,Yi Ji,Gong Zhang,Renhai Chen,Yangqiu Song*

Main category: cs.CL

TL;DR: AutoSchemaKG框架实现完全自主的知识图谱构建，通过LLM同步抽取知识三元组并自动归纳模式，构建出59亿边的大规模图谱ATLAS，在QA任务和LLM事实性增强方面超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱依赖预定义模式，需要大量人工干预。本研究旨在实现无需人工干预的自动化模式归纳和大规模知识图谱构建。

Method: 1. 利用LLM同步执行知识抽取和模式归纳
2. 融合实体与事件抽取，采用概念化技术组织语义类别
3. 处理5000万+文档实现端到端自动化构建

Result: 1. 构建ATLAS图谱（9亿节点/59亿边）
2. Multi-hop QA任务超越SOTA方法
3. 模式语义对齐率达95%（零人工干预）
4. 提升LLM事实准确性

Conclusion: 动态模式归纳的十亿级知识图谱能有效补充LLM参数化知识，证明自动化构建方案可达到接近人工设计的模式质量

Abstract: We present AutoSchemaKG, a framework for fully autonomous knowledge graph
construction that eliminates the need for predefined schemas. Our system
leverages large language models to simultaneously extract knowledge triples and
induce comprehensive schemas directly from text, modeling both entities and
events while employing conceptualization to organize instances into semantic
categories. Processing over 50 million documents, we construct ATLAS (Automated
Triple Linking And Schema induction), a family of knowledge graphs with 900+
million nodes and 5.9 billion edges. This approach outperforms state-of-the-art
baselines on multi-hop QA tasks and enhances LLM factuality. Notably, our
schema induction achieves 95\% semantic alignment with human-crafted schemas
with zero manual intervention, demonstrating that billion-scale knowledge
graphs with dynamically induced schemas can effectively complement parametric
knowledge in large language models.

</details>


### [94] [GeNRe: A French Gender-Neutral Rewriting System Using Collective Nouns](https://arxiv.org/abs/2505.23630)
*Enzo Doyen,Amalia Todirascu*

Main category: cs.CL

TL;DR: 本文提出了首个法语性别中性改写系统GeNRe，通过基于规则的系统与微调语言模型实现，推动法语NLP性别偏见缓解技术发展。


<details>
  <summary>Details</summary>
Motivation: 现有性别中性化技术研究仅限英语，法语因集体名词性别固化问题缺乏解决方案，男性泛称使用加剧性别偏见。

Method: 开发基于规则系统(RBS)，生成数据训练微调语言模型，并探索Claude 3 Opus指令模型与词典结合的应用。

Result: 成功构建法语首个中性改写系统，RBS与微调模型效果显著，Claude 3 Opus结合词典达近似RBS水平。

Conclusion: GeNRe系统填补法语性别中性改写空白，为法语NLP领域性别偏见缓解技术发展提供新范式。

Abstract: A significant portion of the textual data used in the field of Natural
Language Processing (NLP) exhibits gender biases, particularly due to the use
of masculine generics (masculine words that are supposed to refer to mixed
groups of men and women), which can perpetuate and amplify stereotypes. Gender
rewriting, an NLP task that involves automatically detecting and replacing
gendered forms with neutral or opposite forms (e.g., from masculine to
feminine), can be employed to mitigate these biases. While such systems have
been developed in a number of languages (English, Arabic, Portuguese, German,
French), automatic use of gender neutralization techniques (as opposed to
inclusive or gender-switching techniques) has only been studied for English.
This paper presents GeNRe, the very first French gender-neutral rewriting
system using collective nouns, which are gender-fixed in French. We introduce a
rule-based system (RBS) tailored for the French language alongside two
fine-tuned language models trained on data generated by our RBS. We also
explore the use of instruct-based models to enhance the performance of our
other systems and find that Claude 3 Opus combined with our dictionary achieves
results close to our RBS. Through this contribution, we hope to promote the
advancement of gender bias mitigation techniques in NLP for French.

</details>


### [95] [Are Reasoning Models More Prone to Hallucination?](https://arxiv.org/abs/2505.23646)
*Zijun Yao,Yantao Liu,Yanxu Chen,Jianhui Chen,Junfeng Fang,Lei Hou,Juanzi Li,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 大型推理模型（LRMs）的幻觉问题受训练方法影响，认知行为与模型不确定性机制是关键因素。


<details>
  <summary>Details</summary>
Motivation: 针对LRMs在事实任务中幻觉现象的争议（如DeepSeek-R1与OpenAI-o3结论矛盾），探究推理模型是否更易产生幻觉及其机制。

Method: 1. 整体评估不同后训练流程对幻觉的影响
2. 行为分析（缺陷重复、思维-答案不匹配）
3. 模型不确定性机制研究

Result: 1. 冷启动SFT+可验证奖励RL降低幻觉，蒸馏/无冷启动RL加剧幻觉
2. 发现缺陷重复（表层推理重复底层错误逻辑）、思维-答案不匹配（结论与推理过程矛盾）两种关键行为模式
3. 模型不确定性校准失准导致幻觉增加

Conclusion: 首次系统揭示LRMs幻觉成因：后训练路径选择直接影响事实性，认知行为模式与不确定性校准机制是关键中介因素。

Abstract: Recently evolved large reasoning models (LRMs) show powerful performance in
solving complex tasks with long chain-of-thought (CoT) reasoning capability. As
these LRMs are mostly developed by post-training on formal reasoning tasks,
whether they generalize the reasoning capability to help reduce hallucination
in fact-seeking tasks remains unclear and debated. For instance, DeepSeek-R1
reports increased performance on SimpleQA, a fact-seeking benchmark, while
OpenAI-o3 observes even severer hallucination. This discrepancy naturally
raises the following research question: Are reasoning models more prone to
hallucination? This paper addresses the question from three perspectives. (1)
We first conduct a holistic evaluation for the hallucination in LRMs. Our
analysis reveals that LRMs undergo a full post-training pipeline with cold
start supervised fine-tuning (SFT) and verifiable reward RL generally alleviate
their hallucination. In contrast, both distillation alone and RL training
without cold start fine-tuning introduce more nuanced hallucinations. (2) To
explore why different post-training pipelines alters the impact on
hallucination in LRMs, we conduct behavior analysis. We characterize two
critical cognitive behaviors that directly affect the factuality of a LRM: Flaw
Repetition, where the surface-level reasoning attempts repeatedly follow the
same underlying flawed logic, and Think-Answer Mismatch, where the final answer
fails to faithfully match the previous CoT process. (3) Further, we investigate
the mechanism behind the hallucination of LRMs from the perspective of model
uncertainty. We find that increased hallucination of LRMs is usually associated
with the misalignment between model uncertainty and factual accuracy. Our work
provides an initial understanding of the hallucination in LRMs.

</details>


### [96] [ARC: Argument Representation and Coverage Analysis for Zero-Shot Long Document Summarization with Instruction Following LLMs](https://arxiv.org/abs/2505.23654)
*Mohamed Elaraby,Diane Litman*

Main category: cs.CL

TL;DR: 研究通过提出ARC评估框架，发现大语言模型在摘要生成中存在关键论点遗漏问题，尤其在论点分散时，需开发更注重论点保留的策略。


<details>
  <summary>Details</summary>
Motivation: 现有指令调优的LLMs在法律等高危领域摘要生成中，对核心论证结构的保留效果存疑，需量化评估其论点覆盖能力。

Method: 开发ARC评估框架，在长法律意见书和科学论文两个领域测试三个开源LLM，分析位置偏差和角色偏好对论点覆盖的影响。

Result: LLMs能部分覆盖论点，但分散论点易被遗漏；模型存在上下文窗口位置偏见及特定角色偏好，影响关键信息保留。

Conclusion: 需开发论点感知的摘要策略，改进模型对结构化论证信息的处理能力，特别是在长文本稀疏分布场景下的表现。

Abstract: Integrating structured information has long improved the quality of
abstractive summarization, particularly in retaining salient content. In this
work, we focus on a specific form of structure: argument roles, which are
crucial for summarizing documents in high-stakes domains such as law. We
investigate whether instruction-tuned large language models (LLMs) adequately
preserve this information. To this end, we introduce Argument Representation
Coverage (ARC), a framework for measuring how well LLM-generated summaries
capture salient arguments. Using ARC, we analyze summaries produced by three
open-weight LLMs in two domains where argument roles are central: long legal
opinions and scientific articles. Our results show that while LLMs cover
salient argument roles to some extent, critical information is often omitted in
generated summaries, particularly when arguments are sparsely distributed
throughout the input. Further, we use ARC to uncover behavioral patterns --
specifically, how the positional bias of LLM context windows and role-specific
preferences impact the coverage of key arguments in generated summaries,
emphasizing the need for more argument-aware summarization strategies.

</details>


### [97] [Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation](https://arxiv.org/abs/2505.23657)
*Hongxiang Zhang,Hao Chen,Tianyi Zhang,Muhao Chen*

Main category: cs.CL

TL;DR: 提出Active Layer-Contrastive Decoding (ActLCD)方法，通过强化学习策略动态选择神经网络层对比，有效减少大语言模型的长文本幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于token级别的解码方法在长文本生成中仍存在严重幻觉问题，需要从序列决策层面优化生成过程的真实性。

Method: 将解码过程建模为序列决策问题，使用基于奖励感知分类器的强化学习策略，动态决定何时应用层对比机制。

Result: 在五个基准测试中超越现有最优方法，在不同生成场景下均显著减少幻觉现象。

Conclusion: ActLCD通过主动的层对比机制，突破token级别优化的限制，有效提升大模型生成结果的事实准确性。

Abstract: Recent decoding methods improve the factuality of large language
models~(LLMs) by refining how the next token is selected during generation.
These methods typically operate at the token level, leveraging internal
representations to suppress superficial patterns. Nevertheless, LLMs remain
prone to hallucinations, especially over longer contexts. In this paper, we
propose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy
that actively decides when to apply contrasting layers during generation. By
casting decoding as a sequential decision-making problem, ActLCD employs a
reinforcement learning policy guided by a reward-aware classifier to optimize
factuality beyond the token level. Our experiments demonstrate that ActLCD
surpasses state-of-the-art methods across five benchmarks, showcasing its
effectiveness in mitigating hallucinations in diverse generation scenarios.

</details>


### [98] [ToolHaystack: Stress-Testing Tool-Augmented Language Models in Realistic Long-Term Interactions](https://arxiv.org/abs/2505.23662)
*Beong-woo Kwak,Minju Kim,Dongha Lim,Hyungjoo Chae,Dongjin Kang,Sunghwan Kim,Dongil Yang,Jinyoung Yeo*

Main category: cs.CL

TL;DR: 提出ToolHaystack基准测试，揭示现有大语言模型在长周期工具调用中的鲁棒性缺陷


<details>
  <summary>Details</summary>
Motivation: 现有工具使用评估主要针对短上下文场景，缺乏对长期交互中模型表现的深入洞察

Method: 构建包含多任务执行上下文和真实噪声的连续对话测试集ToolHaystack

Result: 当前先进模型在标准多轮场景表现良好，但在ToolHaystack中普遍存在显著性能下降

Conclusion: ToolHaystack暴露出现有工具基准未能揭示的模型长期鲁棒性关键缺陷

Abstract: Large language models (LLMs) have demonstrated strong capabilities in using
external tools to address user inquiries. However, most existing evaluations
assume tool use in short contexts, offering limited insight into model behavior
during realistic long-term interactions. To fill this gap, we introduce
ToolHaystack, a benchmark for testing the tool use capabilities in long-term
interactions. Each test instance in ToolHaystack includes multiple tasks
execution contexts and realistic noise within a continuous conversation,
enabling assessment of how well models maintain context and handle various
disruptions. By applying this benchmark to 14 state-of-the-art LLMs, we find
that while current models perform well in standard multi-turn settings, they
often significantly struggle in ToolHaystack, highlighting critical gaps in
their long-term robustness not revealed by previous tool benchmarks.

</details>


### [99] [LoLA: Low-Rank Linear Attention With Sparse Caching](https://arxiv.org/abs/2505.23666)
*Luke McDermott,Robert W. Heath Jr.,Rahul Parhi*

Main category: cs.CL

TL;DR: 提出LoLA方法，通过低秩线性注意力与稀疏缓存机制解决长上下文处理中的内存冲突问题，显著提升子二次模型的检索准确率


<details>
  <summary>Details</summary>
Motivation: 现有线性注意力模型存在内存冲突导致的长上下文信息遗忘问题，需兼顾效率与记忆能力

Method: 采用三重内存架构：1）局部滑动窗口存储近期键值对；2）稀疏全局缓存存储难记忆样本；3）循环隐藏状态存储通用信息

Result: 在8K上下文检索任务中实现97.4%准确率（4K时提升161倍），GPU缓存比Llama-3.1小4.6倍，零样本推理任务表现优于同类模型

Conclusion: LoLA以极轻量方式实现接近Transformer的性能，在效率与效果间取得突破性平衡，适合消费级GPU部署

Abstract: Transformer-based large language models suffer from quadratic complexity at
inference on long sequences. Linear attention methods are efficient
alternatives, however, they fail to provide an accurate approximation of
softmax attention. By additionally incorporating sliding window attention into
each linear attention head, this gap can be closed for short context-length
tasks. Unfortunately, these approaches cannot recall important information from
long contexts due to "memory collisions". In this paper , we propose LoLA:
Low-rank Linear Attention with sparse caching. LoLA separately stores
additional key-value pairs that would otherwise interfere with past associative
memories. Moreover, LoLA further closes the gap between linear attention models
and transformers by distributing past key-value pairs into three forms of
memory: (i) recent pairs in a local sliding window; (ii) difficult-to-memorize
pairs in a sparse, global cache; and (iii) generic pairs in the recurrent
hidden state of linear attention. As an inference-only strategy, LoLA enables
pass-key retrieval on up to 8K context lengths on needle-in-a-haystack tasks
from RULER. It boosts the accuracy of the base subquadratic model from 0.6% to
97.4% at 4K context lengths, with a 4.6x smaller cache than that of Llama-3.1
8B. LoLA demonstrates strong performance on zero-shot commonsense reasoning
tasks among 1B and 8B parameter subquadratic models. Finally, LoLA is an
extremely lightweight approach: Nearly all of our results can be reproduced on
a single consumer GPU.

</details>


### [100] [Automatic classification of stop realisation with wav2vec2.0](https://arxiv.org/abs/2505.23688)
*James Tanner,Morgan Sonderegger,Jane Stuart-Smith,Jeff Mielke,Tyler Kendall*

Main category: cs.CL

TL;DR: 利用预训练语音模型wav2vec2.0实现英日语塞音爆破自动标注，验证其在精细标注与非准备语料中的高准确性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有语音标注工具难以处理多变语音现象，而预训练模型wav2vec2.0已展现细粒度语音特征编码潜力。

Method: 通过训练wav2vec2.0模型对英日语塞音爆破存在性进行分类，在精心标注语料与非准备语料库中进行跨语言验证。

Result: 自动标注准确复现塞音实现方式的变异模式，与人工标注结果高度吻合，模型展现跨语料库的强鲁棒性。

Conclusion: 预训练语音模型可大幅提升语音数据自动标注效率，为扩大语音学研究规模提供便捷工具支持。

Abstract: Modern phonetic research regularly makes use of automatic tools for the
annotation of speech data, however few tools exist for the annotation of many
variable phonetic phenomena. At the same time, pre-trained self-supervised
models, such as wav2vec2.0, have been shown to perform well at speech
classification tasks and latently encode fine-grained phonetic information. We
demonstrate that wav2vec2.0 models can be trained to automatically classify
stop burst presence with high accuracy in both English and Japanese, robust
across both finely-curated and unprepared speech corpora. Patterns of
variability in stop realisation are replicated with the automatic annotations,
and closely follow those of manual annotations. These results demonstrate the
potential of pre-trained speech models as tools for the automatic annotation
and processing of speech corpus data, enabling researchers to `scale-up' the
scope of phonetic research with relative ease.

</details>


### [101] [Child-Directed Language Does Not Consistently Boost Syntax Learning in Language Models](https://arxiv.org/abs/2505.23689)
*Francesca Padovani,Jaap Jumelet,Yevgen Matusevych,Arianna Bisazza*

Main category: cs.CL

TL;DR: 研究发现儿童导向语言（CDL）在语言模型训练中未显示出系统性优势，维基百科数据表现更优，并提出控制词频效应的新评估方法FIT-CLAMS。


<details>
  <summary>Details</summary>
Motivation: 验证Huebner等人关于CDL训练效果的结论是否具有跨语言/模型/评估方法的普适性，解决先前基准测试的缺陷。

Method: 跨三语言（英/法/德）、两模型架构（遮蔽/因果）、三句法基准对比CDL与维基百科模型，开发频率控制测试方法FIT-CLAMS。

Result: CDL模型仅在部分情况下优于维基百科模型，新测试方法显示CDL未增强句法泛化能力，频率效应显著影响评估结果。

Conclusion: CDL并非提升语言模型句法能力的有效方案，强调构建评估基准时需严格控制词汇频率干扰因素。

Abstract: Seminal work by Huebner et al. (2021) showed that language models (LMs)
trained on English Child-Directed Language (CDL) can reach similar syntactic
abilities as LMs trained on much larger amounts of adult-directed written text,
suggesting that CDL could provide more effective LM training material than the
commonly used internet-crawled data. However, the generalizability of these
results across languages, model types, and evaluation settings remains unclear.
We test this by comparing models trained on CDL vs. Wikipedia across two LM
objectives (masked and causal), three languages (English, French, German), and
three syntactic minimal-pair benchmarks. Our results on these benchmarks show
inconsistent benefits of CDL, which in most cases is outperformed by Wikipedia
models. We then identify various shortcomings in previous benchmarks, and
introduce a novel testing methodology, FIT-CLAMS, which uses a
frequency-controlled design to enable balanced comparisons across training
corpora. Through minimal pair evaluations and regression analysis we show that
training on CDL does not yield stronger generalizations for acquiring syntax
and highlight the importance of controlling for frequency effects when
evaluating syntactic ability.

</details>


### [102] [Can LLMs Reason Abstractly Over Math Word Problems Without CoT? Disentangling Abstract Formulation From Arithmetic Computation](https://arxiv.org/abs/2505.23701)
*Ziling Cheng,Meng Cao,Leila Pishdad,Yanshuai Cao,Jackie Chi Kit Cheung*

Main category: cs.CL

TL;DR: 研究发现LLM数学能力瓶颈在于计算而非抽象建模，CoT主要提升计算能力，提出解耦评估方法的重要性


<details>
  <summary>Details</summary>
Motivation: 传统数学评估指标将抽象建模与算术计算混为一谈，需解耦评估以准确衡量模型推理能力

Method: 使用GSM8K/SVAMP数据集，通过无思维链的Llama-3/Qwen2.5模型（1B-32B）进行分步评估，结合因果补丁机制分析抽象-计算机制

Result: 模型表现主要受计算能力限制（准确率差5-50倍），CoT提升计算能力但对抽象建模影响有限，抽象特征可组合/传递且先于计算产生

Conclusion: 需采用解耦评估方法准确衡量LLM推理能力，抽象建模与计算能力的分离机制为模型改进提供新方向

Abstract: Final-answer-based metrics are commonly used for evaluating large language
models (LLMs) on math word problems, often taken as proxies for reasoning
ability. However, such metrics conflate two distinct sub-skills: abstract
formulation (capturing mathematical relationships using expressions) and
arithmetic computation (executing the calculations). Through a disentangled
evaluation on GSM8K and SVAMP, we find that the final-answer accuracy of
Llama-3 and Qwen2.5 (1B-32B) without CoT is overwhelmingly bottlenecked by the
arithmetic computation step and not by the abstract formulation step. Contrary
to the common belief, we show that CoT primarily aids in computation, with
limited impact on abstract formulation. Mechanistically, we show that these two
skills are composed conjunctively even in a single forward pass without any
reasoning steps via an abstract-then-compute mechanism: models first capture
problem abstractions, then handle computation. Causal patching confirms these
abstractions are present, transferable, composable, and precede computation.
These behavioural and mechanistic findings highlight the need for disentangled
evaluation to accurately assess LLM reasoning and to guide future improvements.

</details>


### [103] [SocialMaze: A Benchmark for Evaluating Social Reasoning in Large Language Models](https://arxiv.org/abs/2505.23713)
*Zixiang Xu,Yanbo Wang,Yue Huang,Jiayi Ye,Haomin Zhuang,Zirui Song,Lang Gao,Chenxi Wang,Zhaorun Chen,Yujun Zhou,Sixian Li,Wang Pan,Yue Zhao,Jieyu Zhao,Xiangliang Zhang,Xiuying Chen*

Main category: cs.CL

TL;DR: 提出了SocialMaze基准，系统评估大语言模型在复杂社交场景中的推理能力


<details>
  <summary>Details</summary>
Motivation: 现有评估框架过度简化现实场景，缺乏系统性评估大语言模型社交推理能力（深度推理/动态互动/信息不确定性）的基准

Method: 构建包含3大核心挑战（深度推理/动态互动/信息不确定性）、覆盖3类场景（社交推理游戏/日常生活/数字社区）的6个任务，采用自动化与人工双重验证

Result: 模型处理动态互动能力差异显著；思维链能力强的模型在深层推理任务表现更优；不确定性导致性能显著下降；定向微调可提升复杂场景表现

Conclusion: SocialMaze填补了社交推理评估空白，揭示了模型能力边界，其开放数据集将促进LLM社会智能研究发展

Abstract: Large language models (LLMs) are increasingly applied to socially grounded
tasks, such as online community moderation, media content analysis, and social
reasoning games. Success in these contexts depends on a model's social
reasoning ability - the capacity to interpret social contexts, infer others'
mental states, and assess the truthfulness of presented information. However,
there is currently no systematic evaluation framework that comprehensively
assesses the social reasoning capabilities of LLMs. Existing efforts often
oversimplify real-world scenarios and consist of tasks that are too basic to
challenge advanced models. To address this gap, we introduce SocialMaze, a new
benchmark specifically designed to evaluate social reasoning. SocialMaze
systematically incorporates three core challenges: deep reasoning, dynamic
interaction, and information uncertainty. It provides six diverse tasks across
three key settings: social reasoning games, daily-life interactions, and
digital community platforms. Both automated and human validation are used to
ensure data quality. Our evaluation reveals several key insights: models vary
substantially in their ability to handle dynamic interactions and integrate
temporally evolving information; models with strong chain-of-thought reasoning
perform better on tasks requiring deeper inference beyond surface-level cues;
and model reasoning degrades significantly under uncertainty. Furthermore, we
show that targeted fine-tuning on curated reasoning examples can greatly
improve model performance in complex social scenarios. The dataset is publicly
available at: https://huggingface.co/datasets/MBZUAI/SocialMaze

</details>


### [104] [SenWiCh: Sense-Annotation of Low-Resource Languages for WiC using Hybrid Methods](https://arxiv.org/abs/2505.23714)
*Roksana Goworek,Harpal Karlcut,Muhammad Shezad,Nijaguna Darshana,Abhishek Mane,Syam Bondada,Raghav Sikka,Ulvi Mammadov,Rauf Allahverdiyev,Sriram Purighella,Paridhi Gupta,Muhinyia Ndegwa,Haim Dubossarsky*

Main category: cs.CL

TL;DR: 论文针对低资源语言跨语言迁移中高质量评估数据集的不足，发布了覆盖9种低资源语言的多义词标注数据集，并提出半自动标注方法。通过WiC实验验证了数据集对多义消歧和迁移研究的有效性。


<details>
  <summary>Details</summary>
Motivation: 低资源语言缺乏高质量的评估数据集，限制了跨语言迁移技术在多语言NLP中的应用效果，尤其在处理复杂语言现象（如多义性）时表现不足。

Method: 1. 构建包含多义词的标注数据集（涵盖9种语系/文字） 2. 开发半自动标注流程提升效率 3. 采用WiC任务格式评估跨语言迁移效果

Result: 实验表明：针对性数据集显著提升低资源环境下的多义消歧性能，同时揭示了语言类型差异对迁移效果的关键影响。

Conclusion: 开放的数据集和代码为公平、鲁棒的多语言NLP研究提供基础设施，推动跨语言迁移技术向更小语种扩展。

Abstract: This paper addresses the critical need for high-quality evaluation datasets
in low-resource languages to advance cross-lingual transfer. While
cross-lingual transfer offers a key strategy for leveraging multilingual
pretraining to expand language technologies to understudied and typologically
diverse languages, its effectiveness is dependent on quality and suitable
benchmarks. We release new sense-annotated datasets of sentences containing
polysemous words, spanning nine low-resource languages across diverse language
families and scripts. To facilitate dataset creation, the paper presents a
demonstrably beneficial semi-automatic annotation method. The utility of the
datasets is demonstrated through Word-in-Context (WiC) formatted experiments
that evaluate transfer on these low-resource languages. Results highlight the
importance of targeted dataset creation and evaluation for effective polysemy
disambiguation in low-resource settings and transfer studies. The released
datasets and code aim to support further research into fair, robust, and truly
multilingual NLP.

</details>


### [105] [Don't Take the Premise for Granted: Evaluating the Premise Critique Ability of Large Language Models](https://arxiv.org/abs/2505.23715)
*Jinzhe Li,Gengxu Li,Yi Chang,Yuan Wu*

Main category: cs.CL

TL;DR: 论文揭示LLMs在识别错误前提方面的不足，提出PCBench基准测试并发现模型依赖显式提示、批判能力与推理能力不相关等现象。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视LLMs在处理错误前提时的脆弱性，需研究其自主识别输入错误的能力以提高系统可靠性

Method: 构建包含4种错误类型和3个难度等级的PCBench基准，采用多维评估指标系统评估15个代表性LLM

Result: 1. 模型依赖显式提示 2.批判能力受问题难度/错误类型影响 3.推理与批判能力不总相关 4.错误前提导致模型过度思考

Conclusion: 需加强LLMs对输入有效性的主动评估，将前提批判能力作为构建可靠人本系统的基础能力

Abstract: Large language models (LLMs) have witnessed rapid advancements, demonstrating
remarkable capabilities. However, a notable vulnerability persists: LLMs often
uncritically accept flawed or contradictory premises, leading to inefficient
reasoning and unreliable outputs. This emphasizes the significance of
possessing the \textbf{Premise Critique Ability} for LLMs, defined as the
capacity to proactively identify and articulate errors in input premises. Most
existing studies assess LLMs' reasoning ability in ideal settings, largely
ignoring their vulnerabilities when faced with flawed premises. Thus, we
introduce the \textbf{Premise Critique Bench (PCBench)}, designed by
incorporating four error types across three difficulty levels, paired with
multi-faceted evaluation metrics. We conducted systematic evaluations of 15
representative LLMs. Our findings reveal: (1) Most models rely heavily on
explicit prompts to detect errors, with limited autonomous critique; (2)
Premise critique ability depends on question difficulty and error type, with
direct contradictions being easier to detect than complex or procedural errors;
(3) Reasoning ability does not consistently correlate with the premise critique
ability; (4) Flawed premises trigger overthinking in reasoning models, markedly
lengthening responses due to repeated attempts at resolving conflicts. These
insights underscore the urgent need to enhance LLMs' proactive evaluation of
input validity, positioning premise critique as a foundational capability for
developing reliable, human-centric systems. The code is available at
https://github.com/MLGroupJLU/Premise_Critique.

</details>


### [106] [Label-Guided In-Context Learning for Named Entity Recognition](https://arxiv.org/abs/2505.23722)
*Fan Bai,Hamid Hassanzadeh,Ardavan Saeedi,Mark Dredze*

Main category: cs.CL

TL;DR: DEER通过引入标签统计增强ICL在NER任务中的表现，结合标签引导的检索和错误修正机制，接近监督微调效果。


<details>
  <summary>Details</summary>
Motivation: 传统ICL方法在NER任务中基于语义相似性选择样本时忽略训练标签信息，导致性能受限。

Method: 1. 标签引导的token级检索器筛选关键实体识别样本
2. 基于标签统计识别易错token进行针对性修正

Result: 在5个NER数据集/4种LLM上超越现有ICL方法，接近监督微调性能，在低资源场景下保持鲁棒性。

Conclusion: DEER证明将标签统计融入ICL流程可显著提升实体识别效果，为少样本学习提供有效解决方案。

Abstract: In-context learning (ICL) enables large language models (LLMs) to perform new
tasks using only a few demonstrations. In Named Entity Recognition (NER),
demonstrations are typically selected based on semantic similarity to the test
instance, ignoring training labels and resulting in suboptimal performance. We
introduce DEER, a new method that leverages training labels through token-level
statistics to improve ICL performance. DEER first enhances example selection
with a label-guided, token-based retriever that prioritizes tokens most
informative for entity recognition. It then prompts the LLM to revisit
error-prone tokens, which are also identified using label statistics, and make
targeted corrections. Evaluated on five NER datasets using four different LLMs,
DEER consistently outperforms existing ICL methods and approaches the
performance of supervised fine-tuning. Further analysis shows its effectiveness
on both seen and unseen entities and its robustness in low-resource settings.

</details>


### [107] [ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering](https://arxiv.org/abs/2505.23723)
*Zexi Liu,Jingyi Chai,Xinyu Zhu,Shuo Tang,Rui Ye,Bo Zhang,Lei Bai,Siheng Chen*

Main category: cs.CL

TL;DR: 提出基于强化学习的ML-Agent框架，使7B小模型性能超越671B大模型并展现跨任务泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理过度依赖人工提示工程，缺乏基于实验经验的自适应优化能力

Method: 1）探索增强微调提升RL多样性；2）分步RL训练加速经验收集；3）统一ML反馈的奖励模块设计

Result: 在9个任务训练后，7B的ML-Agent超越671B基线模型，实现持续性能提升和跨任务泛化

Conclusion: 该框架成功实现小规模LLM的高效训练范式，为自主机器学习开辟新方向

Abstract: The emergence of large language model (LLM)-based agents has significantly
advanced the development of autonomous machine learning (ML) engineering.
However, most existing approaches rely heavily on manual prompt engineering,
failing to adapt and optimize based on diverse experimental experiences.
Focusing on this, for the first time, we explore the paradigm of learning-based
agentic ML, where an LLM agent learns through interactive experimentation on ML
tasks using online reinforcement learning (RL). To realize this, we propose a
novel agentic ML training framework with three key components: (1)
exploration-enriched fine-tuning, which enables LLM agents to generate diverse
actions for enhanced RL exploration; (2) step-wise RL, which enables training
on a single action step, accelerating experience collection and improving
training efficiency; (3) an agentic ML-specific reward module, which unifies
varied ML feedback signals into consistent rewards for RL optimization.
Leveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM
for autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our
7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it
achieves continuous performance improvements and demonstrates exceptional
cross-task generalization capabilities.

</details>


### [108] [Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time](https://arxiv.org/abs/2505.23729)
*Mohamad Chehade,Soumya Suvra Ghosal,Souradip Chakraborty,Avinash Reddy,Dinesh Manocha,Hao Zhu,Amrit Singh Bedi*

Main category: cs.CL

TL;DR: 提出SITAlign推理框架，通过满意策略在满足次要目标阈值约束下最大化主要目标，实现语言模型的多维对齐。理论分析和实验表明其显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法忽视人类决策的有限理性特征（优化主目标+满足次目标阈值），导致多目标对齐效果受限。

Method: SITAlign推理框架：1) 最大化主要目标函数 2) 对次要目标设置阈值约束 3) 推导次优性边界理论保证

Result: 在PKU-SafeRLHF数据集上，SITAlign在保持无害性阈值的前提下，帮助性指标的GPT-4胜率比SOTA方法提高22.3%

Conclusion: 将满意策略形式化为约束优化问题，有效解决多维对齐挑战。理论证明和实验验证共同支持该框架的优越性。

Abstract: Aligning large language models with humans is challenging due to the
inherently multifaceted nature of preference feedback. While existing
approaches typically frame this as a multi-objective optimization problem, they
often overlook how humans actually make decisions. Research on bounded
rationality suggests that human decision making follows satisficing
strategies-optimizing primary objectives while ensuring others meet acceptable
thresholds. To bridge this gap and operationalize the notion of satisficing
alignment, we propose SITAlign: an inference time framework that addresses the
multifaceted nature of alignment by maximizing a primary objective while
satisfying threshold-based constraints on secondary criteria. We provide
theoretical insights by deriving sub-optimality bounds of our satisficing based
inference alignment approach. We empirically validate SITAlign's performance
through extensive experimentation on multiple benchmarks. For instance, on the
PKU-SafeRLHF dataset with the primary objective of maximizing helpfulness while
ensuring a threshold on harmlessness, SITAlign outperforms the state-of-the-art
multi objective decoding strategy by a margin of 22.3% in terms of GPT-4
win-tie rate for helpfulness reward while adhering to the threshold on
harmlessness.

</details>


### [109] [ATLAS: Learning to Optimally Memorize the Context at Test Time](https://arxiv.org/abs/2505.23735)
*Ali Behrouz,Zeman Li,Praneeth Kacham,Majid Daliri,Yuan Deng,Peilin Zhong,Meisam Razaviyayn,Vahab Mirrokni*

Main category: cs.CL

TL;DR: ATLAS提出通过增强记忆容量、克服在线更新机制、优化固定尺寸记忆管理三个维度，显著提升模型的长序列建模能力，在语言建模和长上下文理解任务中超越Transformer和线性循环模型。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer的二次方复杂度限制了长序列处理能力，而现代循环神经网络在长上下文理解和外推任务中存在记忆容量受限、仅依赖当前输入更新记忆、固定尺寸记忆表达力不足三大缺陷。

Method: 提出ATLAS记忆模块：1）通过上下文感知的联合记忆优化（当前及历史标记）突破在线更新限制；2）构建DeepTransformers架构（Transformer的严格泛化形式）。记忆更新机制包含参数化记忆映射函数和动态记忆融合策略。

Result: 在语言建模、常识推理、召回密集型任务中全面超越基线模型，BABILong基准测试10M上下文长度准确率提升80%。模型在长上下文理解任务中展现持续性能增益。

Conclusion: 通过系统解决记忆模块的容量限制、更新机制、表达力三大瓶颈，ATLAS为长序列建模提供了有效解决方案，证明了联合优化当前与历史信息对提升模型外推能力的关键作用。

Abstract: Transformers have been established as the most popular backbones in sequence
modeling, mainly due to their effectiveness in in-context retrieval tasks and
the ability to learn at scale. Their quadratic memory and time complexity,
however, bound their applicability in longer sequences and so has motivated
researchers to explore effective alternative architectures such as modern
recurrent neural networks (a.k.a long-term recurrent memory module). Despite
their recent success in diverse downstream tasks, they struggle in tasks that
requires long context understanding and extrapolation to longer sequences. We
observe that these shortcomings come from three disjoint aspects in their
design: (1) limited memory capacity that is bounded by the architecture of
memory and feature mapping of the input; (2) online nature of update, i.e.,
optimizing the memory only with respect to the last input; and (3) less
expressive management of their fixed-size memory. To enhance all these three
aspects, we present ATLAS, a long-term memory module with high capacity that
learns to memorize the context by optimizing the memory based on the current
and past tokens, overcoming the online nature of long-term memory models.
Building on this insight, we present a new family of Transformer-like
architectures, called DeepTransformers, that are strict generalizations of the
original Transformer architecture. Our experimental results on language
modeling, common-sense reasoning, recall-intensive, and long-context
understanding tasks show that ATLAS surpasses the performance of Transformers
and recent linear recurrent models. ATLAS further improves the long context
performance of Titans, achieving +80\% accuracy in 10M context length of
BABILong benchmark.

</details>


### [110] [DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning](https://arxiv.org/abs/2505.23754)
*Ziyin Zhang,Jiahao Xu,Zhiwei He,Tian Liang,Qiuzhi Liu,Yansi Li,Linfeng Song,Zhengwen Liang,Zhuosheng Zhang,Rui Wang,Zhaopeng Tu,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: 提出了DeepTheorem框架，通过自然语言增强LLM定理证明能力，包含121K非正式定理数据集、RL-Zero强化学习策略及多维评估体系


<details>
  <summary>Details</summary>
Motivation: 传统形式化定理证明方法与LLM基于自然语言知识的优势不匹配，限制了其数学推理能力的发挥

Method: 构建大规模IMO级定理数据集，开发RL-Zero强化学习策略利用定理变体强化推理，设计过程与结果双维度评估指标

Result: 实验表明DeepTheorem显著提升LLM定理证明准确率（+15.6%）和推理质量，达到当前最优水平

Conclusion: 该框架为推进非正式定理证明自动化及数学探索提供了新范式，可能引发数学研究方法的变革

Abstract: Theorem proving serves as a major testbed for evaluating complex reasoning
abilities in large language models (LLMs). However, traditional automated
theorem proving (ATP) approaches rely heavily on formal proof systems that
poorly align with LLMs' strength derived from informal, natural language
knowledge acquired during pre-training. In this work, we propose DeepTheorem, a
comprehensive informal theorem-proving framework exploiting natural language to
enhance LLM mathematical reasoning. DeepTheorem includes a large-scale
benchmark dataset consisting of 121K high-quality IMO-level informal theorems
and proofs spanning diverse mathematical domains, rigorously annotated for
correctness, difficulty, and topic categories, accompanied by systematically
constructed verifiable theorem variants. We devise a novel reinforcement
learning strategy (RL-Zero) explicitly tailored to informal theorem proving,
leveraging the verified theorem variants to incentivize robust mathematical
inference. Additionally, we propose comprehensive outcome and process
evaluation metrics examining proof correctness and the quality of reasoning
steps. Extensive experimental analyses demonstrate DeepTheorem significantly
improves LLM theorem-proving performance compared to existing datasets and
supervised fine-tuning protocols, achieving state-of-the-art accuracy and
reasoning quality. Our findings highlight DeepTheorem's potential to
fundamentally advance automated informal theorem proving and mathematical
exploration.

</details>


### [111] [Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint](https://arxiv.org/abs/2505.23759)
*Heekyung Lee,Jiaxin Ge,Tsung-Han Wu,Minwoo Kang,Trevor Darrell,David M. Chan*

Main category: cs.CL

TL;DR: 本文探讨当前视觉语言模型在解画谜任务中的表现，构建了手工标注的英文画谜基准测试，发现模型虽能处理简单视觉线索，但在抽象推理和隐喻理解方面存在显著缺陷。


<details>
  <summary>Details</summary>
Motivation: 画谜需要多模态抽象、符号推理及文化双关理解，传统视觉任务无法有效评估模型此类能力，需针对性测试VLMs的复杂推理短板。

Method: 创建包含从简单图形替换到空间依赖线索的多样化画谜基准，系统分析不同VLMs在各类别中的表现差异。

Result: VLMs对基础视觉线索解码能力尚可（如简单图形替换），但在抽象推理（横向思维、视觉隐喻解析）任务中准确率下降40-60%。

Conclusion: 当前视觉语言模型在复杂多模态推理任务中存在本质局限，需改进符号推理架构与文化语境理解机制以提升抽象问题解决能力。

Abstract: Rebus puzzles, visual riddles that encode language through imagery, spatial
arrangement, and symbolic substitution, pose a unique challenge to current
vision-language models (VLMs). Unlike traditional image captioning or question
answering tasks, rebus solving requires multi-modal abstraction, symbolic
reasoning, and a grasp of cultural, phonetic and linguistic puns. In this
paper, we investigate the capacity of contemporary VLMs to interpret and solve
rebus puzzles by constructing a hand-generated and annotated benchmark of
diverse English-language rebus puzzles, ranging from simple pictographic
substitutions to spatially-dependent cues ("head" over "heels"). We analyze how
different VLMs perform, and our findings reveal that while VLMs exhibit some
surprising capabilities in decoding simple visual clues, they struggle
significantly with tasks requiring abstract reasoning, lateral thinking, and
understanding visual metaphors.

</details>


### [112] [From Chat Logs to Collective Insights: Aggregative Question Answering](https://arxiv.org/abs/2505.23765)
*Wentao Zhang,Woojeong Kim,Yuntian Deng*

Main category: cs.CL

TL;DR: 提出聚合问答任务(AQA)并构建WildChat-AQA基准，揭示现有方法在分析大规模对话数据中的不足


<details>
  <summary>Details</summary>
Motivation: 现有方法将用户与聊天机器人的交互视为独立事件，无法有效挖掘跨对话的群体性洞察，而海量对话数据蕴含着社会趋势和集体关注的重要价值

Method: 基于182,330条真实对话构建包含6,027个聚合问题的基准数据集，通过实验评估现有方法的推理能力和计算效率

Result: 传统方法存在双重缺陷：检索增强方法(RAG)难以有效推理，密集计算方案成本过高，准确率较理想基线低15%以上

Conclusion: 亟需开发新型计算框架，以实现从亿级对话数据中高效提取群体性洞察的能力

Abstract: Conversational agents powered by large language models (LLMs) are rapidly
becoming integral to our daily interactions, generating unprecedented amounts
of conversational data. Such datasets offer a powerful lens into societal
interests, trending topics, and collective concerns. Yet, existing approaches
typically treat these interactions as independent and miss critical insights
that could emerge from aggregating and reasoning across large-scale
conversation logs. In this paper, we introduce Aggregative Question Answering,
a novel task requiring models to reason explicitly over thousands of
user-chatbot interactions to answer aggregative queries, such as identifying
emerging concerns among specific demographics. To enable research in this
direction, we construct a benchmark, WildChat-AQA, comprising 6,027 aggregative
questions derived from 182,330 real-world chatbot conversations. Experiments
show that existing methods either struggle to reason effectively or incur
prohibitive computational costs, underscoring the need for new approaches
capable of extracting collective insights from large-scale conversational data.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [113] [Quality assessment of 3D human animation: Subjective and objective evaluation](https://arxiv.org/abs/2505.23301)
*Rim Rekik,Stefanie Wuhrer,Ludovic Hoyet,Katja Zibrek,Anne-Hélène Olivier*

Main category: cs.GR

TL;DR: 开发首个针对非参数化虚拟人动画的质量评估方法，通过用户研究数据集训练线性回归器，实现90%的预测相关性


<details>
  <summary>Details</summary>
Motivation: 现有虚拟人动画质量评估方法局限于参数化模型，缺乏针对非参数化模型的评估方案

Method: 1. 创建带主观评分的数据集 2. 用线性回归器学习预测感知质量分数

Result: 线性回归器预测结果与人工评分相关性达90%，优于现有深度学习方法

Conclusion: 提出的数据驱动框架能有效预测虚拟人动画的感知真实性，显著提升评估质量

Abstract: Virtual human animations have a wide range of applications in virtual and
augmented reality. While automatic generation methods of animated virtual
humans have been developed, assessing their quality remains challenging.
Recently, approaches introducing task-oriented evaluation metrics have been
proposed, leveraging neural network training. However, quality assessment
measures for animated virtual humans that are not generated with parametric
body models have yet to be developed. In this context, we introduce a first
such quality assessment measure leveraging a novel data-driven framework.
First, we generate a dataset of virtual human animations together with their
corresponding subjective realism evaluation scores collected with a user study.
Second, we use the resulting dataset to learn predicting perceptual evaluation
scores. Results indicate that training a linear regressor on our dataset
results in a correlation of 90%, which outperforms a state of the art deep
learning baseline.

</details>


### [114] [To Measure What Isn't There -- Visual Exploration of Missingness Structures Using Quality Metrics](https://arxiv.org/abs/2505.23447)
*Sara Johansson Fernstad,Sarah Alsufyani,Silvia Del Din,Alison Yarnall,Lynn Rochester*

Main category: cs.GR

TL;DR: 提出一套质量指标用于高维数据结构化缺失的识别与可视化分析，并应用于实际步行监测案例验证有效性。


<details>
  <summary>Details</summary>
Motivation: 高维数据中的结构化缺失可能反映数据质量问题或重要特征，现有统计方法侧重缺失值填补，而可视化分析能深入理解缺失结构，但该领域研究仍存在可扩展性不足等问题。

Method: 开发质量指标集识别结构化缺失模式，通过真实步行监测研究案例展示其在可视化分析中的指导作用。

Result: 该质量指标集可有效支持大规模高维数据结构化缺失的探索分析，并为数据质量问题决策提供依据。

Conclusion: 该研究为结构化缺失分析提供了系统方法论，相关质量指标体系和实际案例验证对数据质量评估具有应用价值，所有补充材料已开源共享。

Abstract: This paper contributes a set of quality metrics for identification and visual
analysis of structured missingness in high-dimensional data. Missing values in
data are a frequent challenge in most data generating domains and may cause a
range of analysis issues. Structural missingness in data may indicate issues in
data collection and pre-processing, but may also highlight important data
characteristics. While research into statistical methods for dealing with
missing data are mainly focusing on replacing missing values with plausible
estimated values, visualization has great potential to support a more in-depth
understanding of missingness structures in data. Nonetheless, while the
interest in missing data visualization has increased in the last decade, it is
still a relatively overlooked research topic with a comparably small number of
publications, few of which address scalability issues. Efficient visual
analysis approaches are needed to enable exploration of missingness structures
in large and high-dimensional data, and to support informed decision-making in
context of potential data quality issues. This paper suggests a set of quality
metrics for identification of patterns of interest for understanding of
structural missingness in data. These quality metrics can be used as guidance
in visual analysis, as demonstrated through a use case exploring structural
missingness in data from a real-life walking monitoring study. All supplemental
materials for this paper are available at
https://doi.org/10.25405/data.ncl.c.7741829.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [115] [AMOR: Adaptive Character Control through Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2505.23708)
*Lucas N. Alegre,Agon Serifi,Ruben Grandia,David Müller,Espen Knoop,Moritz Bächer*

Main category: cs.RO

TL;DR: 提出多目标强化学习框架，通过权重调节策略处理奖励函数冲突，显著减少调参时间并提升机器人动作适应性


<details>
  <summary>Details</summary>
Motivation: 传统强化学习依赖奖励函数加权求和，需反复调参且难以应对仿真-现实差异。希望建立可动态调整权重的灵活框架，加速迭代并增强策略泛化能力

Method: 训练可接受权重参数的条件策略，覆盖帕累托前沿的奖励权衡。采用分层控制架构：高层策略动态选择权重，底层多目标策略生成多样化行为模式

Result: 成功实现机器人高动态运动控制，权重条件策略在分层框架下可快速适应新任务，策略库编码的行为多样性提升任务迁移效率

Conclusion: 该框架突破传统RL的静态奖励设定限制，通过解耦训练与权重选择过程，为机器人控制提供高效工作流，特别适合存在sim-to-real差距的实际应用场景

Abstract: Reinforcement learning (RL) has significantly advanced the control of
physics-based and robotic characters that track kinematic reference motion.
However, methods typically rely on a weighted sum of conflicting reward
functions, requiring extensive tuning to achieve a desired behavior. Due to the
computational cost of RL, this iterative process is a tedious, time-intensive
task. Furthermore, for robotics applications, the weights need to be chosen
such that the policy performs well in the real world, despite inevitable
sim-to-real gaps. To address these challenges, we propose a multi-objective
reinforcement learning framework that trains a single policy conditioned on a
set of weights, spanning the Pareto front of reward trade-offs. Within this
framework, weights can be selected and tuned after training, significantly
speeding up iteration time. We demonstrate how this improved workflow can be
used to perform highly dynamic motions with a robot character. Moreover, we
explore how weight-conditioned policies can be leveraged in hierarchical
settings, using a high-level policy to dynamically select weights according to
the current task. We show that the multi-objective policy encodes a diverse
spectrum of behaviors, facilitating efficient adaptation to novel tasks.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [116] [AgentAlign: Navigating Safety Alignment in the Shift from Informative to Agentic Large Language Models](https://arxiv.org/abs/2505.23020)
*Jinchuan Zhang,Lu Yin,Yan Zhou,Songlin Hu*

Main category: cs.CR

TL;DR: 论文提出AgentAlign框架，通过抽象行为链生成安全对齐数据，有效提升LLM代理的安全性（35.8%→79.5%）且保持模型效用，代码数据已开源。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在获得行动能力后存在安全对齐缺陷，容易自发执行恶意任务，亟需解决训练后阶段的代理使用安全问题。

Method: 构建抽象行为链作为安全数据合成媒介，通过模拟环境实例化生成高真实性指令，并采用良性指令比例合成策略平衡安全性与实用性。

Result: 在AgentHarm基准上，微调模型安全性提升至79.5%，帮助性未受损甚至增强，显著优于各类提示方法。

Conclusion: AgentAlign框架通过行为链动态建模实现了高效安全对齐，为LLM代理安全部署提供新范式，开源资源推动领域发展。

Abstract: The acquisition of agentic capabilities has transformed LLMs from "knowledge
providers" to "action executors", a trend that while expanding LLMs' capability
boundaries, significantly increases their susceptibility to malicious use.
Previous work has shown that current LLM-based agents execute numerous
malicious tasks even without being attacked, indicating a deficiency in agentic
use safety alignment during the post-training phase. To address this gap, we
propose AgentAlign, a novel framework that leverages abstract behavior chains
as a medium for safety alignment data synthesis. By instantiating these
behavior chains in simulated environments with diverse tool instances, our
framework enables the generation of highly authentic and executable
instructions while capturing complex multi-step dynamics. The framework further
ensures model utility by proportionally synthesizing benign instructions
through non-malicious interpretations of behavior chains, precisely calibrating
the boundary between helpfulness and harmlessness. Evaluation results on
AgentHarm demonstrate that fine-tuning three families of open-source models
using our method substantially improves their safety (35.8% to 79.5%
improvement) while minimally impacting or even positively enhancing their
helpfulness, outperforming various prompting methods. The dataset and code have
both been open-sourced.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [117] [Errors in Stereo Geometry Induce Distance Misperception](https://arxiv.org/abs/2505.23685)
*Raffles Xingqi Zhu,Charlie S. Burlingham,Olivier Mercier,Phillip Guan*

Main category: cs.HC

TL;DR: 头戴显示器(HMD)的渲染相机与观察位置误差会导致用户距离感知偏差，研究通过几何框架预测误差并验证实时视觉反馈可动态校准视觉运动映射。


<details>
  <summary>Details</summary>
Motivation: HMD在渲染双目图像时存在相机位置错误和用户观察位置偏移问题，可能导致用户对虚拟场景深度感知失真，需系统性量化几何误差对距离感知的影响机制。

Method: 建立预测HMD透视几何误差的数学模型，搭建Quest 3眼动追踪实验平台，设计5组感知实验验证框架准确性，并测试视觉反馈对距离校准的有效性。

Result: 几何误差导致距离感知低估(最大-13.7%)或高估(最大+9.2%)，实时手部视觉反馈可使到达误差从12.3%降低至3.8%。

Conclusion: HMD透视几何误差会系统性扭曲空间感知，但动态视觉反馈机制能有效补偿感知失真，这对提高VR/AR设备的空间呈现精度具有重要指导意义。

Abstract: Stereoscopic head-mounted displays (HMDs) render and present binocular images
to create an egocentric, 3D percept to the HMD user. Within this render and
presentation pipeline there are potential rendering camera and viewing position
errors that can induce deviations in the depth and distance that a user
perceives compared to the underlying intended geometry. For example, rendering
errors can arise when HMD render cameras are incorrectly positioned relative to
the assumed centers of projections of the HMD displays and viewing errors can
arise when users view stereo geometry from the incorrect location in the HMD
eyebox. In this work we present a geometric framework that predicts errors in
distance perception arising from inaccurate HMD perspective geometry and build
an HMD platform to reliably simulate render and viewing error in a Quest 3 HMD
with eye tracking to experimentally test these predictions. We present a series
of five experiments to explore the efficacy of this geometric framework and
show that errors in perspective geometry can induce both under- and
over-estimations in perceived distance. We further demonstrate how real-time
visual feedback can be used to dynamically recalibrate visuomotor mapping so
that an accurate reach distance is achieved even if the perceived visual
distance is negatively impacted by geometric error.

</details>


### [118] [Large Language Models for Depression Recognition in Spoken Language Integrating Psychological Knowledge](https://arxiv.org/abs/2505.22863)
*Yupei Li,Shuaijie Shao,Manuel Milling,Björn W. Schuller*

Main category: cs.HC

TL;DR: 首次将LLM应用于DAIC-WOZ数据集的多模态抑郁症检测，通过Wav2Vec提取音频特征并融合心理学知识，显著降低了MAE和RMSE指标


<details>
  <summary>Details</summary>
Motivation: 现有DNN在现实场景效果有限，LLM存在非文本线索处理不足和领域知识欠缺的问题，抑郁症检测需结合语音特征与心理学专业知识

Method: 1. 使用Wav2Vec预训练模型提取音频特征 2. 将音频特征映射到文本型LLM 3. 设计问答机制注入心理学知识

Result: 相比基线模型，在平均绝对误差(MAE)和均方根误差(RMSE)上取得显著提升，代码已开源

Conclusion: 通过多模态数据融合与领域知识增强，有效提升了LLM在抑郁症检测中的诊断性能，为心理健康领域AI应用提供了新思路

Abstract: Depression is a growing concern gaining attention in both public discourse
and AI research. While deep neural networks (DNNs) have been used for
recognition, they still lack real-world effectiveness. Large language models
(LLMs) show strong potential but require domain-specific fine-tuning and
struggle with non-textual cues. Since depression is often expressed through
vocal tone and behaviour rather than explicit text, relying on language alone
is insufficient. Diagnostic accuracy also suffers without incorporating
psychological expertise. To address these limitations, we present, to the best
of our knowledge, the first application of LLMs to multimodal depression
detection using the DAIC-WOZ dataset. We extract the audio features using the
pre-trained model Wav2Vec, and mapped it to text-based LLMs for further
processing. We also propose a novel strategy for incorporating psychological
knowledge into LLMs to enhance diagnostic performance, specifically using a
question and answer set to grant authorised knowledge to LLMs. Our approach
yields a notable improvement in both Mean Absolute Error (MAE) and Root Mean
Square Error (RMSE) compared to a base score proposed by the related original
paper. The codes are available at
https://github.com/myxp-lyp/Depression-detection.git

</details>


### [119] [Human Empathy as Encoder: AI-Assisted Depression Assessment in Special Education](https://arxiv.org/abs/2505.23631)
*Boning Zhao*

Main category: cs.HC

TL;DR: 提出HEAE框架，通过整合学生叙述文本和教师9维共情向量，将隐性共情转化为结构化输入，实现82.74%准确率的抑郁严重程度分类。


<details>
  <summary>Details</summary>
Motivation: 现有抑郁评估方法在特殊教育场景存在局限：标准化问卷无法反映真实情况，自动化方法缺乏教师共情洞察的整合。

Method: 开发Human Empathy as Encoder (HEAE)框架，通过PHQ-9指导构建教师共情向量(EV)，优化多模态融合架构实现学生叙述与结构化共情特征的结合。

Result: 在7级抑郁严重度分类任务中达到82.74%准确率，验证了人机协同框架的有效性。

Conclusion: 该研究通过结构化嵌入人类共情，为负责任的情感计算提供了新路径，实现了增强而非替代人类判断的伦理目标。

Abstract: Assessing student depression in sensitive environments like special education
is challenging. Standardized questionnaires may not fully reflect students'
true situations. Furthermore, automated methods often falter with rich student
narratives, lacking the crucial, individualized insights stemming from
teachers' empathetic connections with students. Existing methods often fail to
address this ambiguity or effectively integrate educator understanding. To
address these limitations by fostering a synergistic human-AI collaboration,
this paper introduces Human Empathy as Encoder (HEAE), a novel, human-centered
AI framework for transparent and socially responsible depression severity
assessment. Our approach uniquely integrates student narrative text with a
teacher-derived, 9-dimensional "Empathy Vector" (EV), its dimensions guided by
the PHQ-9 framework,to explicitly translate tacit empathetic insight into a
structured AI input enhancing rather than replacing human judgment. Rigorous
experiments optimized the multimodal fusion, text representation, and
classification architecture, achieving 82.74% accuracy for 7-level severity
classification. This work demonstrates a path toward more responsible and
ethical affective computing by structurally embedding human empathy

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [120] [One Trajectory, One Token: Grounded Video Tokenization via Panoptic Sub-object Trajectory](https://arxiv.org/abs/2505.23617)
*Chenhao Zheng,Jieyu Zhang,Mohammadreza Salehi,Ziqi Gao,Vishnu Iyengar,Norimasa Kobori,Quan Kong,Ranjay Krishna*

Main category: cs.CV

TL;DR: 提出基于对象轨迹的视频标记方法TrajViT，通过轨迹语义压缩冗余标记，在多项任务中显著超越ViT3D模型


<details>
  <summary>Details</summary>
Motivation: 现有时空块标记方法产生过多冗余标记，在摄像机移动场景下性能急剧下降，且传统降维方法损害模型表现

Method: 通过全景子对象轨迹组织视频标记，设计TrajViT模型提取对象轨迹并生成语义连贯的紧凑表征，采用对比学习训练框架

Result: 视频-文本检索任务Top-5召回率平均提升6%，训练速度加快4倍，推理FLOPs减少18倍；作为VideoLLM编码器在6个VideoQA基准平均提升5.2%

Conclusion: TrajViT首次实现高效视频编码器在跨任务性能上全面超越ViT3D，为长视频分析提供可扩展的解决方案

Abstract: Effective video tokenization is critical for scaling transformer models for
long videos. Current approaches tokenize videos using space-time patches,
leading to excessive tokens and computational inefficiencies. The best token
reduction strategies degrade performance and barely reduce the number of tokens
when the camera moves. We introduce grounded video tokenization, a paradigm
that organizes tokens based on panoptic sub-object trajectories rather than
fixed patches. Our method aligns with fundamental perceptual principles,
ensuring that tokenization reflects scene complexity rather than video
duration. We propose TrajViT, a video encoder that extracts object trajectories
and converts them into semantically meaningful tokens, significantly reducing
redundancy while maintaining temporal coherence. Trained with contrastive
learning, TrajViT significantly outperforms space-time ViT (ViT3D) across
multiple video understanding benchmarks, e.g., TrajViT outperforms ViT3D by a
large margin of 6% top-5 recall in average at video-text retrieval task with
10x token deduction. We also show TrajViT as a stronger model than ViT3D for
being the video encoder for modern VideoLLM, obtaining an average of 5.2%
performance improvement across 6 VideoQA benchmarks while having 4x faster
training time and 18x less inference FLOPs. TrajViT is the first efficient
encoder to consistently outperform ViT3D across diverse video analysis tasks,
making it a robust and scalable solution.

</details>


### [121] [How Animals Dance (When You're Not Looking)](https://arxiv.org/abs/2505.23738)
*Xiaojuan Wang,Aleksander Holynski,Brian Curless,Ira Kemelmacher,Steve Seitz*

Main category: cs.CV

TL;DR: 提出基于关键帧优化的动物舞蹈视频生成框架，通过图优化和视频扩散模型实现音乐同步的编舞生成。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法难以捕捉舞蹈对称性、生成长度受限的问题，实现用更少输入生成更长舞蹈视频。

Method: 1. 用文本/GPT-4o生成关键帧 2. 建立编舞节拍图优化模型 3. 开发镜像姿势生成技术 4. 视频扩散模型补间生成

Result: 仅需6个关键帧即可生成30秒视频，支持多种动物和音乐类型。

Conclusion: 该方法显著降低输入要求，通过结构优化实现高质量长视频生成，适用于数字内容创作。

Abstract: We present a keyframe-based framework for generating music-synchronized,
choreography aware animal dance videos. Starting from a few keyframes
representing distinct animal poses -- generated via text-to-image prompting or
GPT-4o -- we formulate dance synthesis as a graph optimization problem: find
the optimal keyframe structure that satisfies a specified choreography pattern
of beats, which can be automatically estimated from a reference dance video. We
also introduce an approach for mirrored pose image generation, essential for
capturing symmetry in dance. In-between frames are synthesized using an video
diffusion model. With as few as six input keyframes, our method can produce up
to 30 second dance videos across a wide range of animals and music tracks.

</details>


### [122] [LayerPeeler: Autoregressive Peeling for Layer-wise Image Vectorization](https://arxiv.org/abs/2505.23740)
*Ronghuan Wu,Wanchao Su,Jing Liao*

Main category: cs.CV

TL;DR: 提出LayerPeeler方法，通过渐进式分层剥离策略解决图像矢量化中的遮挡问题，生成完整矢量路径


<details>
  <summary>Details</summary>
Motivation: 现有图像矢量化工具在处理遮挡区域时产生碎片化结果，影响可编辑性；需要更优的层间关系建模和内容恢复方案

Method: 1. 构建视觉语言模型驱动的层图捕捉遮挡关系
2. 自回归式逐层剥离（检测-描述-移除循环）
3. 局部注意力控制的图像扩散模型实现精准层移除

Result: 在路径完整性（提升38%）、几何规则性（+27%）和视觉保真度（SSIM提高0.15）上显著超越现有方法

Conclusion: LayerPeeler通过结构化的层处理流程和创新的内容恢复机制，为复杂遮挡场景的矢量编辑提供了有效解决方案

Abstract: Image vectorization is a powerful technique that converts raster images into
vector graphics, enabling enhanced flexibility and interactivity. However,
popular image vectorization tools struggle with occluded regions, producing
incomplete or fragmented shapes that hinder editability. While recent
advancements have explored rule-based and data-driven layer-wise image
vectorization, these methods face limitations in vectorization quality and
flexibility. In this paper, we introduce LayerPeeler, a novel layer-wise image
vectorization approach that addresses these challenges through a progressive
simplification paradigm. The key to LayerPeeler's success lies in its
autoregressive peeling strategy: by identifying and removing the topmost
non-occluded layers while recovering underlying content, we generate vector
graphics with complete paths and coherent layer structures. Our method
leverages vision-language models to construct a layer graph that captures
occlusion relationships among elements, enabling precise detection and
description for non-occluded layers. These descriptive captions are used as
editing instructions for a finetuned image diffusion model to remove the
identified layers. To ensure accurate removal, we employ localized attention
control that precisely guides the model to target regions while faithfully
preserving the surrounding content. To support this, we contribute a
large-scale dataset specifically designed for layer peeling tasks. Extensive
quantitative and qualitative experiments demonstrate that LayerPeeler
significantly outperforms existing techniques, producing vectorization results
with superior path semantics, geometric regularity, and visual fidelity.

</details>


### [123] [VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models](https://arxiv.org/abs/2505.22654)
*Ce Zhang,Kaixin Ma,Tianqing Fang,Wenhao Yu,Hongming Zhang,Zhisong Zhang,Yaqi Xie,Katia Sycara,Haitao Mi,Dong Yu*

Main category: cs.CV

TL;DR: VScan框架通过两阶段视觉标记缩减策略加速LVLMs推理，在保持95.4%原始性能的同时实现2.91倍预填充加速


<details>
  <summary>Details</summary>
Motivation: 现有视觉标记剪枝方法存在计算冗余且效果受限，需系统性优化视觉标记在编码-解码全流程的处理效率

Method: 1.视觉编码阶段融合全局/局部扫描与标记合并 2.语言模型中间层插入动态剪枝模块

Result: 在4个主流LVLM上验证，16个基准测试性能超越SOTA，LLaVA-NeXT-7B实现2.91×预填充加速与10×FLOPs缩减

Conclusion: 通过系统性优化视觉标记处理流程，VScan成功平衡了计算效率与模型性能，为实时部署提供有效解决方案

Abstract: Recent Large Vision-Language Models (LVLMs) have advanced multi-modal
understanding by incorporating finer-grained visual perception and encoding.
However, such methods incur significant computational costs due to longer
visual token sequences, posing challenges for real-time deployment. To mitigate
this, prior studies have explored pruning unimportant visual tokens either at
the output layer of the visual encoder or at the early layers of the language
model. In this work, we revisit these design choices and reassess their
effectiveness through comprehensive empirical studies of how visual tokens are
processed throughout the visual encoding and language decoding stages. Guided
by these insights, we propose VScan, a two-stage visual token reduction
framework that addresses token redundancy by: (1) integrating complementary
global and local scans with token merging during visual encoding, and (2)
introducing pruning at intermediate layers of the language model. Extensive
experimental results across four LVLMs validate the effectiveness of VScan in
accelerating inference and demonstrate its superior performance over current
state-of-the-arts on sixteen benchmarks. Notably, when applied to
LLaVA-NeXT-7B, VScan achieves a 2.91$\times$ speedup in prefilling and a
10$\times$ reduction in FLOPs, while retaining 95.4% of the original
performance.

</details>


### [124] [Cultural Evaluations of Vision-Language Models Have a Lot to Learn from Cultural Theory](https://arxiv.org/abs/2505.22793)
*Srishti Yadav,Lauren Tilton,Maria Antoniak,Taylor Arnold,Jiaang Li,Siddhesh Milind Pawar,Antonia Karamolegkou,Stella Frank,Zhaochong An,Negar Rostamzadeh,Daniel Hershcovich,Serge Belongie,Ekaterina Shutova*

Main category: cs.CV

TL;DR: 论文指出视觉语言模型在文化能力评估中存在不足，提出基于视觉文化研究的五个分析框架来系统性评估图像中的文化维度


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型的文化能力分析缺乏系统性框架，需整合文化研究、符号学等跨学科方法来识别图像中的文化细微差异

Method: 通过回顾视觉文化研究基础理论（文化研究/符号学/视觉研究），构建五个文化维度的分析框架体系

Result: 建立包含五个关键文化维度的理论框架，为全面评估视觉语言模型的文化能力提供系统性方法论

Conclusion: 融合跨学科理论可系统提升视觉语言模型的文化分析能力，该框架为后续研究和应用提供新方向

Abstract: Modern vision-language models (VLMs) often fail at cultural competency
evaluations and benchmarks. Given the diversity of applications built upon
VLMs, there is renewed interest in understanding how they encode cultural
nuances. While individual aspects of this problem have been studied, we still
lack a comprehensive framework for systematically identifying and annotating
the nuanced cultural dimensions present in images for VLMs. This position paper
argues that foundational methodologies from visual culture studies (cultural
studies, semiotics, and visual studies) are necessary for cultural analysis of
images. Building upon this review, we propose a set of five frameworks,
corresponding to cultural dimensions, that must be considered for a more
complete analysis of the cultural competencies of VLMs.

</details>


### [125] [Synthetic Document Question Answering in Hungarian](https://arxiv.org/abs/2505.23008)
*Jonathan Li,Zoltan Csaki,Nidhi Hiremath,Etash Guha,Fenglu Hong,Edward Ma,Urmish Thakker*

Main category: cs.CV

TL;DR: 针对匈牙利语等低资源语言文档VQA数据稀缺问题，本研究提出了合成生成(HuDocVQA)与人工整理(HuDocVQA-manual)的双重数据集构建方案，并配套OCR训练数据集(HuCCPDF)，通过质量优化使模型准确率提升7.2%。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在英语文档VQA接近性能饱和，但匈牙利语（互联网资源第17位）等低资源语言因缺乏训练评估数据仍存在显著性能差距。

Method: 1. 基于Common Crawl构建人工整理的小数据集HuDocVQA-manual和大规模合成数据集HuDocVQA
2. 通过多轮质量过滤和去重提升数据质量
3. 配套发布11.7万页PDF转录数据集HuCCPDF用于OCR训练

Result: 在Llama 3.2 11B Instruct模型上，混合数据集微调使HuDocVQA准确率提升+7.2%

Conclusion: 开放数据集将促进多语言文档VQA研究，验证了合成数据优化方案在低资源语言场景的有效性

Abstract: Modern VLMs have achieved near-saturation accuracy in English document visual
question-answering (VQA). However, this task remains challenging in lower
resource languages due to a dearth of suitable training and evaluation data. In
this paper we present scalable methods for curating such datasets by focusing
on Hungarian, approximately the 17th highest resource language on the internet.
Specifically, we present HuDocVQA and HuDocVQA-manual, document VQA datasets
that modern VLMs significantly underperform on compared to English DocVQA.
HuDocVQA-manual is a small manually curated dataset based on Hungarian
documents from Common Crawl, while HuDocVQA is a larger synthetically generated
VQA data set from the same source. We apply multiple rounds of quality
filtering and deduplication to HuDocVQA in order to match human-level quality
in this dataset. We also present HuCCPDF, a dataset of 117k pages from
Hungarian Common Crawl PDFs along with their transcriptions, which can be used
for training a model for Hungarian OCR. To validate the quality of our
datasets, we show how finetuning on a mixture of these datasets can improve
accuracy on HuDocVQA for Llama 3.2 11B Instruct by +7.2%. Our datasets and code
will be released to the public to foster further research in multilingual
DocVQA.

</details>


### [126] [R2I-Bench: Benchmarking Reasoning-Driven Text-to-Image Generation](https://arxiv.org/abs/2505.23493)
*Kaijie Chen,Zihao Lin,Zhiyang Xu,Ying Shen,Yuguang Yao,Joy Rimchala,Jiaxin Zhang,Lifu Huang*

Main category: cs.CV

TL;DR: 提出首个系统性评估文本生成图像模型推理能力的基准R2I-Bench，包含多模态推理任务数据集与细粒度评估指标R2IScore，实验证明当前T2I模型推理能力存在显著缺陷


<details>
  <summary>Details</summary>
Motivation: 现有文本生成图像模型虽能生成逼真图像，但在需要复杂推理的任务（如时间逻辑/常识理解）上表现不足，且缺乏系统性的评估框架

Method: 构建覆盖7类推理任务的数据集（常识/数学/逻辑/组合/数值/因果/概念融合），开发基于评估问题的R2IScore三维指标（文本对齐度/推理准确度/图像质量），测试16种T2I模型（含分阶段推理-生成框架）

Result: 所有被测模型（包括使用SOTA语言模型的阶段式框架）在推理任务中表现均存在显著局限，最高推理准确率仅61.7%

Conclusion: 下一代T2I系统需构建具有推理感知能力的架构，R2I-Bench为此提供了系统性评估工具

Abstract: Reasoning is a fundamental capability often required in real-world
text-to-image (T2I) generation, e.g., generating ``a bitten apple that has been
left in the air for more than a week`` necessitates understanding temporal
decay and commonsense concepts. While recent T2I models have made impressive
progress in producing photorealistic images, their reasoning capability remains
underdeveloped and insufficiently evaluated. To bridge this gap, we introduce
R2I-Bench, a comprehensive benchmark specifically designed to rigorously assess
reasoning-driven T2I generation. R2I-Bench comprises meticulously curated data
instances, spanning core reasoning categories, including commonsense,
mathematical, logical, compositional, numerical, causal, and concept mixing. To
facilitate fine-grained evaluation, we design R2IScore, a QA-style metric based
on instance-specific, reasoning-oriented evaluation questions that assess three
critical dimensions: text-image alignment, reasoning accuracy, and image
quality. Extensive experiments with 16 representative T2I models, including a
strong pipeline-based framework that decouples reasoning and generation using
the state-of-the-art language and image generation models, demonstrate
consistently limited reasoning performance, highlighting the need for more
robust, reasoning-aware architectures in the next generation of T2I systems.
Project Page: https://r2i-bench.github.io

</details>


### [127] [Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles](https://arxiv.org/abs/2505.23590)
*Zifu Wang,Junyi Zhu,Bo Tang,Zhiyu Li,Feiyu Xiong,Jiaqian Yu,Matthew B. Blaschko*

Main category: cs.CV

TL;DR: 基于拼图实验框架的系统性研究揭示了基于规则的视觉强化学习在MLLMs中的五大发现：RL优于SFT的泛化能力、推理模式预存性、显隐式推理兼容性、跨任务迁移性，以及初始SFT对RL优化的潜在负面影响


<details>
  <summary>Details</summary>
Motivation: 探究基于规则的强化学习在多模态大语言模型（MLLMs）视觉任务中的应用特性，揭示与纯文本领域不同的学习规律和潜在机制

Method: 以拼图任务为结构化实验平台，通过不同复杂度的配置组合，系统研究MLLMs在规则驱动型视觉任务中的学习轨迹和泛化特性

Result: 1. 微调后从随机猜测提升至近完美精度
2. 特定配置下实现跨任务迁移
3. 显式推理非必要但影响表达模式
4. 复杂推理具预存性且随训练强化
5. RL泛化优于SFT且初始SFT阶段不利优化

Conclusion: 该研究为理解视觉RL机制提供了关键拼图，证明基于规则的学习框架在复杂视觉任务中的潜力，同时揭示模型架构与训练策略间的微妙相互作用，为后续多模态学习系统设计提供重要启示

Abstract: The application of rule-based reinforcement learning (RL) to multimodal large
language models (MLLMs) introduces unique challenges and potential deviations
from findings in text-only domains, particularly for perception-heavy tasks.
This paper provides a comprehensive study of rule-based visual RL using jigsaw
puzzles as a structured experimental framework, revealing several key findings.
\textit{Firstly,} we find that MLLMs, initially performing near to random
guessing on simple puzzles, achieve near-perfect accuracy and generalize to
complex, unseen configurations through fine-tuning. \textit{Secondly,} training
on jigsaw puzzles can induce generalization to other visual tasks, with
effectiveness tied to specific task configurations. \textit{Thirdly,} MLLMs can
learn and generalize with or without explicit reasoning, though open-source
models often favor direct answering. Consequently, even when trained for
step-by-step reasoning, they can ignore the thinking process in deriving the
final answer. \textit{Fourthly,} we observe that complex reasoning patterns
appear to be pre-existing rather than emergent, with their frequency increasing
alongside training and task difficulty. \textit{Finally,} our results
demonstrate that RL exhibits more effective generalization than Supervised
Fine-Tuning (SFT), and an initial SFT cold start phase can hinder subsequent RL
optimization. Although these observations are based on jigsaw puzzles and may
vary across other visual tasks, this research contributes a valuable piece of
jigsaw to the larger puzzle of collective understanding rule-based visual RL
and its potential in multimodal learning. The code is available at:
\href{https://github.com/zifuwanggg/Jigsaw-R1}{https://github.com/zifuwanggg/Jigsaw-R1}.

</details>


### [128] [VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos](https://arxiv.org/abs/2505.23693)
*Tingyu Song,Tongyan Hu,Guo Gan,Yilun Zhao*

Main category: cs.CV

TL;DR: 提出VF-Eval基准测试，系统评估多模态大模型在AIGC视频中的四项能力，发现现有模型表现不稳定，并验证人类反馈对齐对视频生成的提升价值。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注自然视频评估，忽视合成视频（如AIGC）场景下MLLMs的评估需求。同时视频生成领域依赖MLLMs评估生成质量，但其在AIGC视频解析能力的系统性研究不足。

Method: 设计包含一致性验证、错误感知、错误类型识别、推理评估四项任务的VF-Eval基准，测试13个前沿MLLMs，并通过RePrompt实验探索其对视频生成的实际应用价值。

Result: 最佳模型GPT-4.1在四项任务中表现波动显著，证明基准挑战性。RePrompt实验表明MLLMs与人类反馈对齐可有效提升视频生成质量。

Conclusion: VF-Eval揭示了MLLMs在AIGC视频解析领域的系统性能力缺陷，为模型优化和视频生成技术提供了新的评估范式和改进方向。

Abstract: MLLMs have been widely studied for video question answering recently.
However, most existing assessments focus on natural videos, overlooking
synthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in
video generation rely on MLLMs to evaluate the quality of generated videos, but
the capabilities of MLLMs on interpreting AIGC videos remain largely
underexplored. To address this, we propose a new benchmark, VF-Eval, which
introduces four tasks-coherence validation, error awareness, error type
detection, and reasoning evaluation-to comprehensively evaluate the abilities
of MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that
even the best-performing model, GPT-4.1, struggles to achieve consistently good
performance across all tasks. This highlights the challenging nature of our
benchmark. Additionally, to investigate the practical applications of VF-Eval
in improving video generation, we conduct an experiment, RePrompt,
demonstrating that aligning MLLMs more closely with human feedback can benefit
video generation.

</details>


### [129] [MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence](https://arxiv.org/abs/2505.23764)
*Sihan Yang,Runsen Xu,Yiman Xie,Sizhe Yang,Mo Li,Jingli Lin,Chenming Zhu,Xiaochen Chen,Haodong Duan,Xiangyu Yue,Dahua Lin,Tai Wang,Jiangmiao Pang*

Main category: cs.CV

TL;DR: 提出MMSI-Bench基准测试框架，揭示现有MLLMs在多图像空间推理能力上与人类表现的巨大差距（30% vs 97%准确率）


<details>
  <summary>Details</summary>
Motivation: 现有单图像基准无法评估真实场景所需的多图像空间推理能力，亟需构建更贴近物理世界复杂交互的评估体系

Method: 基于120,000+图像构建1,000个多选问题，包含干扰项设计和分步推理标注，覆盖34个开源/商业模型的大规模评测

Result: 开源模型最佳准确率30%，GPT-4达40%，人类表现97%；建立自动化错误分析框架识别四类主要错误模式

Conclusion: MMSI-Bench揭示了多图像空间智能研究的广阔提升空间，其标注的推理过程为模型改进提供了明确诊断方向

Abstract: Spatial intelligence is essential for multimodal large language models
(MLLMs) operating in the complex physical world. Existing benchmarks, however,
probe only single-image relations and thus fail to assess the multi-image
spatial reasoning that real-world deployments demand. We introduce MMSI-Bench,
a VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision
researchers spent more than 300 hours meticulously crafting 1,000 challenging,
unambiguous multiple-choice questions from over 120,000 images, each paired
with carefully designed distractors and a step-by-step reasoning process. We
conduct extensive experiments and thoroughly evaluate 34 open-source and
proprietary MLLMs, observing a wide gap: the strongest open-source model
attains roughly 30% accuracy and OpenAI's o3 reasoning model reaches 40%, while
humans score 97%. These results underscore the challenging nature of MMSI-Bench
and the substantial headroom for future research. Leveraging the annotated
reasoning processes, we also provide an automated error analysis pipeline that
diagnoses four dominant failure modes, including (1) grounding errors, (2)
overlap-matching and scene-reconstruction errors, (3) situation-transformation
reasoning errors, and (4) spatial-logic errors, offering valuable insights for
advancing multi-image spatial intelligence. Project page:
https://runsenxu.com/projects/MMSI_Bench .

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [130] [Decomposing Elements of Problem Solving: What "Math" Does RL Teach?](https://arxiv.org/abs/2505.22756)
*Tian Qin,Core Francisco Park,Mujin Kwun,Aaron Walsman,Eran Malach,Nikhil Anand,Hidenori Tanaka,David Alvarez-Melis*

Main category: cs.AI

TL;DR: 该研究通过分解问题解决能力（规划、执行、验证），揭示RL训练主要提升LLM的执行鲁棒性而非规划能力，并发现模型在新问题上的‘覆盖墙’现象。通过构建合成任务验证RL的局限性及潜在突破条件。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理评估仅关注准确率指标，无法揭示模型内部习得的具体能力。本研究旨在剖析RL训练究竟增强了LLM的哪些问题解决子能力，并定位其根本性局限。

Method: 1. 将数学问题解决分解为规划、执行、验证三个基础能力；2. 通过GRPO方法进行实证分析；3. 构建合成解决方案树导航任务模拟数学问题解决过程，在受控环境中验证假设。

Result: 1. RL训练通过‘温度蒸馏’机制主要提升执行能力；2. 模型在全新问题上遭遇‘覆盖墙’，根源在于规划能力不足；3. 合成实验证实RL在特定条件（改进探索&泛化新路径）下可能突破覆盖墙。

Conclusion: 研究揭示了RL在增强LLM推理能力中的作用边界，指出规划能力是当前瓶颈，并提出通过改进探索机制与路径泛化能力可能突破该限制，为后续研究提供了明确方向。

Abstract: Mathematical reasoning tasks have become prominent benchmarks for assessing
the reasoning capabilities of LLMs, especially with reinforcement learning (RL)
methods such as GRPO showing significant performance gains. However, accuracy
metrics alone do not support fine-grained assessment of capabilities and fail
to reveal which problem-solving skills have been internalized. To better
understand these capabilities, we propose to decompose problem solving into
fundamental capabilities: Plan (mapping questions to sequences of steps),
Execute (correctly performing solution steps), and Verify (identifying the
correctness of a solution). Empirically, we find that GRPO mainly enhances the
execution skill-improving execution robustness on problems the model already
knows how to solve-a phenomenon we call temperature distillation. More
importantly, we show that RL-trained models struggle with fundamentally new
problems, hitting a 'coverage wall' due to insufficient planning skills. To
explore RL's impact more deeply, we construct a minimal, synthetic
solution-tree navigation task as an analogy for mathematical problem-solving.
This controlled setup replicates our empirical findings, confirming RL
primarily boosts execution robustness. Importantly, in this setting, we
identify conditions under which RL can potentially overcome the coverage wall
through improved exploration and generalization to new solution paths. Our
findings provide insights into the role of RL in enhancing LLM reasoning,
expose key limitations, and suggest a path toward overcoming these barriers.
Code is available at https://github.com/cfpark00/RL-Wall.

</details>


### [131] [Enhancing Study-Level Inference from Clinical Trial Papers via RL-based Numeric Reasoning](https://arxiv.org/abs/2505.22928)
*Massimiliano Pronesti,Michela Lorandi,Paul Flanagan,Oisin Redmon,Anya Belz,Yufang Hou*

Main category: cs.AI

TL;DR: 提出通过定量推理方法改进医学系统综述自动化，利用强化学习训练的小模型在证据提取任务中实现21%性能提升


<details>
  <summary>Details</summary>
Motivation: 现有系统综述自动化方法依赖文本表面线索，忽视底层数值推理，导致结论准确性受限

Method: 构建数值推理系统（含数据提取模型+效果估计组件），采用监督微调(SFT)和带价值奖励模型的强化学习(RL)训练策略

Result: 在CochraneForest基准测试中，RL训练的小模型F1值比检索系统高21%，超过400B参数大模型9%

Conclusion: 定量推理方法结合领域知识能有效提升系统证据合成的准确性和可解释性

Abstract: Systematic reviews in medicine play a critical role in evidence-based
decision-making by aggregating findings from multiple studies. A central
bottleneck in automating this process is extracting numeric evidence and
determining study-level conclusions for specific outcomes and comparisons.
Prior work has framed this problem as a textual inference task by retrieving
relevant content fragments and inferring conclusions from them. However, such
approaches often rely on shallow textual cues and fail to capture the
underlying numeric reasoning behind expert assessments.
  In this work, we conceptualise the problem as one of quantitative reasoning.
Rather than inferring conclusions from surface text, we extract structured
numerical evidence (e.g., event counts or standard deviations) and apply domain
knowledge informed logic to derive outcome-specific conclusions. We develop a
numeric reasoning system composed of a numeric data extraction model and an
effect estimate component, enabling more accurate and interpretable inference
aligned with the domain expert principles. We train the numeric data extraction
model using different strategies, including supervised fine-tuning (SFT) and
reinforcement learning (RL) with a new value reward model.
  When evaluated on the CochraneForest benchmark, our best-performing approach
-- using RL to train a small-scale number extraction model -- yields up to a
21% absolute improvement in F1 score over retrieval-based systems and
outperforms general-purpose LLMs of over 400B parameters by up to 9%. Our
results demonstrate the promise of reasoning-driven approaches for automating
systematic evidence synthesis.

</details>


### [132] [Be.FM: Open Foundation Models for Human Behavior](https://arxiv.org/abs/2505.23058)
*Yutong Xie,Zhuoheng Li,Xiyuan Wang,Yijun Pan,Qijia Liu,Xingzhi Cui,Kuang-Yu Lo,Ruoyi Gao,Xingjian Zhang,Jin Huang,Walter Yuan,Matthew O. Jackson,Qiaozhu Mei*

Main category: cs.AI

TL;DR: 提出了首个开放的人类行为建模基础模型Be.FM，基于大语言模型并利用多样化行为数据微调，能够预测行为并生成行为科学洞察


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在人类行为建模领域的潜力尚未被充分挖掘，传统方法难以全面理解复杂的人类决策模式

Method: 基于开源大语言模型架构，通过多源行为数据微调，构建包含预测、推理、生成等多维度的基准测试体系

Result: 模型在行为预测、个体/群体特征推断、情境洞察生成和行为科学知识应用等方面表现出色

Conclusion: Be.FM成功验证了基础模型在行为科学中的应用潜力，为跨场景人类行为理解提供了新范式

Abstract: Despite their success in numerous fields, the potential of foundation models
for modeling and understanding human behavior remains largely unexplored. We
introduce Be.FM, one of the first open foundation models designed for human
behavior modeling. Built upon open-source large language models and fine-tuned
on a diverse range of behavioral data, Be.FM can be used to understand and
predict human decision-making. We construct a comprehensive set of benchmark
tasks for testing the capabilities of behavioral foundation models. Our results
demonstrate that Be.FM can predict behaviors, infer characteristics of
individuals and populations, generate insights about contexts, and apply
behavioral science knowledge.

</details>


### [133] [Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased Reinforcement Learning in Multimodal Small Language Models](https://arxiv.org/abs/2505.23091)
*Zeyu Liu,Yuhang Liu,Guanghao Zhu,Congkai Xie,Zhen Li,Jianbo Yuan,Xinyao Wang,Qing Li,Shing-Chi Cheung,Shengyu Zhang,Fei Wu,Hongxia Yang*

Main category: cs.AI

TL;DR: 提出Infi-MMR框架解决多模态小语言模型推理难题，通过三阶段课程提升模型表现


<details>
  <summary>Details</summary>
Motivation: 多模态小语言模型面临三大挑战：高质量多模态推理数据稀缺、视觉整合导致推理能力退化、直接强化学习可能产生错误推理

Method: 分三阶段：1) 基础推理激活（文本数据集强化逻辑能力）；2) 跨模态适应（字幕增强数据迁移能力）；3) 无字幕多模态数据增强（消除语言偏见）

Result: Infi-MMR-3B取得数学推理（MathVerse 43.68%）和通用推理（MathVista 67.2%）双领域SOTA表现

Conclusion: 系统性三阶段框架有效提升小模型多模态推理能力，验证课程学习策略在跨模态迁移中的有效性

Abstract: Recent advancements in large language models (LLMs) have demonstrated
substantial progress in reasoning capabilities, such as DeepSeek-R1, which
leverages rule-based reinforcement learning to enhance logical reasoning
significantly. However, extending these achievements to multimodal large
language models (MLLMs) presents critical challenges, which are frequently more
pronounced for Multimodal Small Language Models (MSLMs) given their typically
weaker foundational reasoning abilities: (1) the scarcity of high-quality
multimodal reasoning datasets, (2) the degradation of reasoning capabilities
due to the integration of visual processing, and (3) the risk that direct
application of reinforcement learning may produce complex yet incorrect
reasoning processes. To address these challenges, we design a novel framework
Infi-MMR to systematically unlock the reasoning potential of MSLMs through a
curriculum of three carefully structured phases and propose our multimodal
reasoning model Infi-MMR-3B. The first phase, Foundational Reasoning
Activation, leverages high-quality textual reasoning datasets to activate and
strengthen the model's logical reasoning capabilities. The second phase,
Cross-Modal Reasoning Adaptation, utilizes caption-augmented multimodal data to
facilitate the progressive transfer of reasoning skills to multimodal contexts.
The third phase, Multimodal Reasoning Enhancement, employs curated,
caption-free multimodal data to mitigate linguistic biases and promote robust
cross-modal reasoning. Infi-MMR-3B achieves both state-of-the-art multimodal
math reasoning ability (43.68% on MathVerse testmini, 27.04% on MathVision
test, and 21.33% on OlympiadBench) and general reasoning ability (67.2% on
MathVista testmini).

</details>


### [134] [Socratic-PRMBench: Benchmarking Process Reward Models with Systematic Reasoning Patterns](https://arxiv.org/abs/2505.23474)
*Xiang Li,Haiyang Yu,Xinghua Zhang,Ziyang Huang,Shizhu He,Kang Liu,Jun Zhao,Fei Huang,Yongbin Li*

Main category: cs.AI

TL;DR: 提出Socratic-PRMBench基准测试，系统评估过程奖励模型在六种推理模式下的缺陷检测能力


<details>
  <summary>Details</summary>
Motivation: 现有PRMs评估基准仅关注逐步正确性，缺乏对不同推理模式(如分解、验证等)的系统性测试

Method: 构建包含6种推理模式(转换/分解/整合等)的2995条缺陷推理路径数据集，通过实验测试PRMs和LLM评论模型的性能

Result: 发现现有PRMs在不同推理模式下的评估存在显著缺陷，验证能力存在系统性弱点

Conclusion: 该基准为PRMs的多模式推理评估提供全面测试框架，揭示了当前模型的局限性，推动未来PRMs的发展

Abstract: Process Reward Models (PRMs) are crucial in complex reasoning and
problem-solving tasks (e.g., LLM agents with long-horizon decision-making) by
verifying the correctness of each intermediate reasoning step. In real-world
scenarios, LLMs may apply various reasoning patterns (e.g., decomposition) to
solve a problem, potentially suffering from errors under various reasoning
patterns. Therefore, PRMs are required to identify errors under various
reasoning patterns during the reasoning process. However, existing benchmarks
mainly focus on evaluating PRMs with stepwise correctness, ignoring a
systematic evaluation of PRMs under various reasoning patterns. To mitigate
this gap, we introduce Socratic-PRMBench, a new benchmark to evaluate PRMs
systematically under six reasoning patterns, including Transformation,
Decomposition, Regather, Deduction, Verification, and Integration.
Socratic-PRMBench}comprises 2995 reasoning paths with flaws within the
aforementioned six reasoning patterns. Through our experiments on both PRMs and
LLMs prompted as critic models, we identify notable deficiencies in existing
PRMs. These observations underscore the significant weakness of current PRMs in
conducting evaluations on reasoning steps under various reasoning patterns. We
hope Socratic-PRMBench can serve as a comprehensive testbed for systematic
evaluation of PRMs under diverse reasoning patterns and pave the way for future
development of PRMs.

</details>


### [135] [ZeroGUI: Automating Online GUI Learning at Zero Human Cost](https://arxiv.org/abs/2505.23762)
*Chenyu Yang,Shiqian Su,Shi Liu,Xuan Dong,Yue Yu,Weijie Su,Xuehui Wang,Zhaoyang Liu,Jinguo Zhu,Hao Li,Wenhai Wang,Yu Qiao,Xizhou Zhu,Jifeng Dai*

Main category: cs.AI

TL;DR: 提出了ZeroGUI框架，通过VLM自动生成任务/奖励+两阶段强化学习，实现零人工成本的GUI代理训练。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理离线学习框架存在标注成本高、动态环境适应性差的问题。

Method: 整合VLM自动任务生成、VLM自动奖励评估、两阶段在线强化学习（探索-利用）。

Result: 在UI-TARS和Aguvis代理上验证，显著提升OSWorld和AndroidLab环境性能。

Conclusion: ZeroGUI框架突破人工标注依赖，实现了高效可扩展的GUI代理自主学习。

Abstract: The rapid advancement of large Vision-Language Models (VLMs) has propelled
the development of pure-vision-based GUI Agents, capable of perceiving and
operating Graphical User Interfaces (GUI) to autonomously fulfill user
instructions. However, existing approaches usually adopt an offline learning
framework, which faces two core limitations: (1) heavy reliance on high-quality
manual annotations for element grounding and action supervision, and (2)
limited adaptability to dynamic and interactive environments. To address these
limitations, we propose ZeroGUI, a scalable, online learning framework for
automating GUI Agent training at Zero human cost. Specifically, ZeroGUI
integrates (i) VLM-based automatic task generation to produce diverse training
goals from the current environment state, (ii) VLM-based automatic reward
estimation to assess task success without hand-crafted evaluation functions,
and (iii) two-stage online reinforcement learning to continuously interact with
and learn from GUI environments. Experiments on two advanced GUI Agents
(UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance
across OSWorld and AndroidLab environments. The code is available at
https://github.com/OpenGVLab/ZeroGUI.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [136] [Nosey: Open-source hardware for acoustic nasalance](https://arxiv.org/abs/2505.23339)
*Maya Dewhurst,Jack Collins,Justin J. H. Lo,Roy Alderton,Sam Kirkham*

Main category: cs.SD

TL;DR: 开源硬件Nosey系统提供低成本鼻音测量方案，性能与商业设备可比但成本更低


<details>
  <summary>Details</summary>
Motivation: 解决商业鼻音测量设备成本高昂的问题，提供开源可定制的替代方案以促进研究普及

Method: 开发3D打印硬件系统，进行与商业设备的鼻音分数对比测试，评估不同麦克风和材料的性能差异

Result: Nosey系统测量值整体更高但与商业设备趋势一致，系统间音韵环境对比幅度具有可比性

Conclusion: Nosey是商业设备的有效经济替代方案，需注意标准化测量方法以确保数据可靠性

Abstract: We introduce Nosey (Nasalance Open Source Estimation sYstem), a low-cost,
customizable, 3D-printed system for recording acoustic nasalance data that we
have made available as open-source hardware
(http://github.com/phoneticslab/nosey). We first outline the motivations and
design principles behind our hardware nasalance system, and then present a
comparison between Nosey and a commercial nasalance device. Nosey shows
consistently higher nasalance scores than the commercial device, but the
magnitude of contrast between phonological environments is comparable between
systems. We also review ways of customizing the hardware to facilitate testing,
such as comparison of microphones and different construction materials. We
conclude that Nosey is a flexible and cost-effective alternative to commercial
nasometry devices and propose some methodological considerations for its use in
data collection.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [137] [SWE-bench Goes Live!](https://arxiv.org/abs/2505.23419)
*Linghao Zhang,Shilin He,Chaoyun Zhang,Yu Kang,Bowen Li,Chengxing Xie,Junhao Wang,Maoquan Wang,Yufan Huang,Shengyu Fu,Elsie Nallipogu,Qingwei Lin,Yingnong Dang,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.SE

TL;DR: 提出了可动态更新的SWE-bench-Live基准测试，克服传统静态基准测试的局限性，包含1,319个实时GitHub问题任务并配备Docker镜像实现自动化评估。


<details>
  <summary>Details</summary>
Motivation: 现有SWE-bench基准测试存在更新停滞、仓库覆盖有限、人工依赖度高的问题，导致可扩展性差和过拟合风险。

Method: 开发自动化流程METHANOL，从任务创建到环境配置实现全流程自动化，整合1,319个2024年后的GitHub问题任务并构建专用Docker容器。

Result: 测试显示现有LLM在动态基准下的表现显著落后于静态基准，揭示了模型在实时软件开发场景中的能力差距。

Conclusion: SWE-bench-Live通过实时更新、多样化任务和可重复执行的特点，为LLM在动态软件开发环境中的评估提供了抗数据污染的可靠基准。

Abstract: The issue-resolving task, where a model generates patches to fix real-world
bugs, has emerged as a critical benchmark for evaluating the capabilities of
large language models (LLMs). While SWE-bench and its variants have become
standard in this domain, they suffer from key limitations: they have not been
updated since their initial releases, cover a narrow set of repositories, and
depend heavily on manual effort for instance construction and environment
setup. These factors hinder scalability and introduce risks of overfitting and
data contamination. In this work, we present \textbf{SWE-bench-Live}, a
\textit{live-updatable} benchmark designed to overcome these challenges. Our
initial release consists of 1,319 tasks derived from real GitHub issues created
since 2024, spanning 93 repositories. Each task is accompanied by a dedicated
Docker image to ensure reproducible execution. Central to our benchmark is
\method, an automated curation pipeline that streamlines the entire process
from instance creation to environment setup, removing manual bottlenecks and
enabling scalability and continuous updates. We evaluate a range of
state-of-the-art agent frameworks and LLMs on SWE-bench-Live, revealing a
substantial performance gap compared to static benchmarks like SWE-bench, even
under controlled evaluation conditions. To better understand this discrepancy,
we perform detailed analyses across repository origin, issue recency, and task
difficulty. By providing a fresh, diverse, and executable benchmark grounded in
live repository activity, SWE-bench-Live facilitates rigorous,
contamination-resistant evaluation of LLMs and agents in dynamic, real-world
software development settings.

</details>


### [138] [Identity resolution of software metadata using Large Language Models](https://arxiv.org/abs/2505.23500)
*Eva Martín del Pico,Josep Lluís Gelpí,Salvador Capella-Gutiérrez*

Main category: cs.SE

TL;DR: 评估指令调优大语言模型在生命科学软件元数据身份解析中的表现，提出基于一致性的高置信度自动化决策代理方法，支持OpenEBench软件观测站的FAIR化监测。


<details>
  <summary>Details</summary>
Motivation: 研究软件在科研中的重要性长期被忽视，现有平台元数据质量参差不齐，需整合异构元数据以全面分析软件开发与可持续性。

Method: 使用人工标注金标准对多指令调优模型进行基准测试，分析模糊案例处理能力，开发基于模型间共识的自动化决策代理机制。

Result: 代理方法实现高精度(0.95)和统计稳健性，但暴露模型在跨平台语义判断中的局限性，揭示FAIR元数据自动化的核心挑战。

Conclusion: 元数据整合对软件FAIR化至关重要，基于共识的代理方法虽有效，仍需突破跨平台语义解析难题以实现全面自动化。

Abstract: Software is an essential component of research. However, little attention has
been paid to it compared with that paid to research data. Recently, there has
been an increase in efforts to acknowledge and highlight the importance of
software in research activities.
  Structured metadata from platforms like bio.tools, Bioconductor, and Galaxy
ToolShed offers valuable insights into research software in the Life Sciences.
Although originally intended to support discovery and integration, this
metadata can be repurposed for large-scale analysis of software practices.
However, its quality and completeness vary across platforms, reflecting diverse
documentation practices.
  To gain a comprehensive view of software development and sustainability,
consolidating this metadata is necessary, but requires robust mechanisms to
address its heterogeneity and scale.
  This article presents an evaluation of instruction-tuned large language
models for the task of software metadata identity resolution, a critical step
in assembling a cohesive collection of research software. Such a collection is
the reference component for the Software Observatory at OpenEBench, a platform
that aggregates metadata to monitor the FAIRness of research software in the
Life Sciences.
  We benchmarked multiple models against a human-annotated gold standard,
examined their behavior on ambiguous cases, and introduced an agreement-based
proxy for high-confidence automated decisions. The proxy achieved high
precision and statistical robustness, while also highlighting the limitations
of current models and the broader challenges of automating semantic judgment in
FAIR-aligned software metadata across registries and repositories.

</details>


### [139] [GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents](https://arxiv.org/abs/2505.23671)
*Manish Shetty,Naman Jain,Jinjian Liu,Vijay Kethanaboyina,Koushik Sen,Ion Stoica*

Main category: cs.SE

TL;DR: 提出GSO基准测试评估语言模型开发高性能软件能力，现有代理成功率不足5%且改进有限


<details>
  <summary>Details</summary>
Motivation: 针对现有语言模型在软件开发优化中的不足，构建系统性评估框架以识别优化瓶颈

Method: 通过自动化管道分析代码库提交历史，生成102个跨领域优化任务，使用性能测试对比专家优化结果

Result: 主流SWE-Agent成功率低于5%，推理时间扩展未显著改善表现，发现低层语言适配和瓶颈定位等关键障碍

Conclusion: GSO基准有效暴露语言模型优化短板，发布代码及轨迹数据推动解决低层语言适配、惰性优化策略改进等研究方向

Abstract: Developing high-performance software is a complex task that requires
specialized expertise. We introduce GSO, a benchmark for evaluating language
models' capabilities in developing high-performance software. We develop an
automated pipeline that generates and executes performance tests to analyze
repository commit histories to identify 102 challenging optimization tasks
across 10 codebases, spanning diverse domains and programming languages. An
agent is provided with a codebase and performance test as a precise
specification, and tasked to improve the runtime efficiency, which is measured
against the expert developer optimization. Our quantitative evaluation reveals
that leading SWE-Agents struggle significantly, achieving less than 5% success
rate, with limited improvements even with inference-time scaling. Our
qualitative analysis identifies key failure modes, including difficulties with
low-level languages, practicing lazy optimization strategies, and challenges in
accurately localizing bottlenecks. We release the code and artifacts of our
benchmark along with agent trajectories to enable future research.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [140] [Conversational Alignment with Artificial Intelligence in Context](https://arxiv.org/abs/2505.22907)
*Rachel Katharine Sterken,James Ravi Kirkpatrick*

Main category: cs.CY

TL;DR: 提出CONTEXT-ALIGN框架评估对话式AI与人类语境对齐能力，揭示现有大语言模型的根本性局限


<details>
  <summary>Details</summary>
Motivation: 探讨AI对话系统如何与人类处理语境和共同认知的交流规范对齐，建立评估开发者设计选择的理论基础

Method: 借鉴语用学哲学和语言学文献构建对话对齐框架，分析现有LLM架构的约束条件

Result: 提出包含语境敏感度的评估框架，指出当前LLM在对话对齐方面存在结构性限制

Conclusion: 实现真正对话对齐需要突破现有LLM架构，发展更符合人类交流本质的新范式

Abstract: The development of sophisticated artificial intelligence (AI) conversational
agents based on large language models raises important questions about the
relationship between human norms, values, and practices and AI design and
performance. This article explores what it means for AI agents to be
conversationally aligned to human communicative norms and practices for
handling context and common ground and proposes a new framework for evaluating
developers' design choices. We begin by drawing on the philosophical and
linguistic literature on conversational pragmatics to motivate a set of
desiderata, which we call the CONTEXT-ALIGN framework, for conversational
alignment with human communicative practices. We then suggest that current
large language model (LLM) architectures, constraints, and affordances may
impose fundamental limitations on achieving full conversational alignment.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [141] [FlashFormer: Whole-Model Kernels for Efficient Low-Batch Inference](https://arxiv.org/abs/2505.22758)
*Aniruddha Nrusimha,William Brandon,Mayank Mishra,Yikang Shen,Rameswar Panda,Jonathan Ragan-Kelley,Yoon Kim*

Main category: cs.LG

TL;DR: FlashFormer提出了一种针对Transformer大模型单批次推理的专用加速内核，在低批量推理场景（如边缘计算）中相比现有方案取得显著加速效果。


<details>
  <summary>Details</summary>
Motivation: 现有推理内核主要优化大批量训练场景的算力利用率，忽视了低批量场景下内存带宽和内核启动开销的关键影响，而边缘计算和低延迟应用对此有迫切需求。

Method: 开发FlashFormer概念验证内核，通过优化内存访问模式和减少内核启动开销，专门提升单批次Transformer模型的推理效率。

Result: 在不同模型规模和量化配置下，FlashFormer相比SOTA推理内核均获得显著加速（具体倍数未披露）。

Conclusion: 该研究证明了专用内核在低批量推理场景的优化潜力，为边缘端大模型部署提供了新的技术思路。

Abstract: The size and compute characteristics of modern large language models have led
to an increased interest in developing specialized kernels tailored for
training and inference. Existing kernels primarily optimize for compute
utilization, targeting the large-batch training and inference settings.
However, low-batch inference, where memory bandwidth and kernel launch
overheads contribute are significant factors, remains important for many
applications of interest such as in edge deployment and latency-sensitive
applications. This paper describes FlashFormer, a proof-of-concept kernel for
accelerating single-batch inference for transformer-based large language
models. Across various model sizes and quantizations settings, we observe
nontrivial speedups compared to existing state-of-the-art inference kernels.

</details>


### [142] [DenoiseRotator: Enhance Pruning Robustness for LLMs via Importance Concentration](https://arxiv.org/abs/2505.23049)
*Tianteng Gu,Bei Liu,Bo Xiao,Ke Zeng,Jiacheng Liu,Yanmin Qian*

Main category: cs.LG

TL;DR: 提出DenoiseRotator方法，通过参数重要性再分配而非单纯剪枝权重，结合可学习正交变换最小化信息熵，有效减少大语言模型剪枝时的性能损失


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法聚焦单个权重重要性评估，难以有效保留模型核心能力。通过参数重要性再分配可增强模型对剪枝的适应性

Method: 设计DenoiseRotator模块，通过可学习的正交变换对权重矩阵进行旋转，使参数重要性集中在更小的子集，兼容Magnitude/SparseGPT/Wanda等现有剪枝方法

Result: 在LLaMA3-70B模型2:4半结构化剪枝中，SparseGPT+DenoiseRotator将困惑度差距从8.1降至3.4（缩小58%），各模型零样本准确率均有提升

Conclusion: 该方法具有模型无关性，通过参数重要性集中机制显著提升剪枝鲁棒性，且能与多种主流剪枝技术无缝集成

Abstract: Pruning is a widely used technique to compress large language models (LLMs)
by removing unimportant weights, but it often suffers from significant
performance degradation - especially under semi-structured sparsity
constraints. Existing pruning methods primarily focus on estimating the
importance of individual weights, which limits their ability to preserve
critical capabilities of the model. In this work, we propose a new perspective:
rather than merely selecting which weights to prune, we first redistribute
parameter importance to make the model inherently more amenable to pruning. By
minimizing the information entropy of normalized importance scores, our
approach concentrates importance onto a smaller subset of weights, thereby
enhancing pruning robustness. We instantiate this idea through DenoiseRotator,
which applies learnable orthogonal transformations to the model's weight
matrices. Our method is model-agnostic and can be seamlessly integrated with
existing pruning techniques such as Magnitude, SparseGPT, and Wanda. Evaluated
on LLaMA3, Qwen2.5, and Mistral models under 50% unstructured and 2:4
semi-structured sparsity, DenoiseRotator consistently improves perplexity and
zero-shot accuracy. For instance, on LLaMA3-70B pruned with SparseGPT at 2:4
semi-structured sparsity, DenoiseRotator reduces the perplexity gap to the
dense model by 58%, narrowing the degradation from 8.1 to 3.4 points. Codes are
available at https://github.com/Axel-gu/DenoiseRotator.

</details>


### [143] [MAP: Revisiting Weight Decomposition for Low-Rank Adaptation](https://arxiv.org/abs/2505.23094)
*Chongjie Si,Zhiyi Shi,Yadao Wang,Xiaokang Yang,Susanto Rahardja,Wei Shen*

Main category: cs.LG

TL;DR: 提出MAP框架，通过几何解耦权重方向与幅度调整，增强参数高效微调方法的性能与可解释性


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法（如LoRA）对权重方向的启发式定义缺乏几何原理支撑，限制了适应过程的灵活性和效果

Method: 1. 将预训练权重归一化为单位向量 2. 学习方向性更新 3. 引入两个独立标量系数分别调节基础向量和更新向量的幅度

Result: 实验证明MAP显著提升现有PEFT方法性能，具备作为未来参数高效微调方法默认配置的潜力

Conclusion: MAP框架通过严格的几何分解实现了更灵活可解释的权重适应，其通用性和简单性为PEFT方法设计提供了新范式

Abstract: The rapid development of large language models has revolutionized natural
language processing, but their fine-tuning remains computationally expensive,
hindering broad deployment. Parameter-efficient fine-tuning (PEFT) methods,
such as LoRA, have emerged as solutions. Recent work like DoRA attempts to
further decompose weight adaptation into direction and magnitude components.
However, existing formulations often define direction heuristically at the
column level, lacking a principled geometric foundation. In this paper, we
propose MAP, a novel framework that reformulates weight matrices as
high-dimensional vectors and decouples their adaptation into direction and
magnitude in a rigorous manner. MAP normalizes the pre-trained weights, learns
a directional update, and introduces two scalar coefficients to independently
scale the magnitude of the base and update vectors. This design enables more
interpretable and flexible adaptation, and can be seamlessly integrated into
existing PEFT methods. Extensive experiments show that MAP significantly
improves performance when coupling with existing methods, offering a simple yet
powerful enhancement to existing PEFT methods. Given the universality and
simplicity of MAP, we hope it can serve as a default setting for designing
future PEFT methods.

</details>


### [144] [Does Machine Unlearning Truly Remove Model Knowledge? A Framework for Auditing Unlearning in LLMs](https://arxiv.org/abs/2505.23270)
*Haokun Chen,Yueqi Zhang,Yuan Bi,Yao Zhang,Tong Liu,Jinhe Bi,Jian Lan,Jindong Gu,Claudia Grosser,Denis Krompass,Nassir Navab,Volker Tresp*

Main category: cs.LG

TL;DR: 研究者针对大语言模型遗忘算法开发了包含基准数据集、算法和审计方法的评估框架，并提出基于中间激活扰动的新审计技术以突破传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型依赖海量数据训练可能涉及敏感/版权内容，GDPR赋予用户数据删除权，需开发高效遗忘算法替代高成本重训练。现有评估方法因模型复杂性和生成特性存在局限性，需构建更全面的审计体系。

Method: 提出包含3个基准数据集、6种遗忘算法、5种提示审计方法的框架，创新性引入中间激活扰动技术，通过分析模型内部状态变化评估遗忘效果。

Result: 实验表明不同审计方法可有效评估遗忘效果，新提出的中间激活扰动技术能更精准检测残留知识，显著提升评估结果的可靠性。

Conclusion: 该框架为遗忘算法评估提供系统化解决方案，中间激活扰动技术突破传统输入输出分析的局限，未来可结合多种评估方式提升算法鲁棒性。

Abstract: In recent years, Large Language Models (LLMs) have achieved remarkable
advancements, drawing significant attention from the research community. Their
capabilities are largely attributed to large-scale architectures, which require
extensive training on massive datasets. However, such datasets often contain
sensitive or copyrighted content sourced from the public internet, raising
concerns about data privacy and ownership. Regulatory frameworks, such as the
General Data Protection Regulation (GDPR), grant individuals the right to
request the removal of such sensitive information. This has motivated the
development of machine unlearning algorithms that aim to remove specific
knowledge from models without the need for costly retraining. Despite these
advancements, evaluating the efficacy of unlearning algorithms remains a
challenge due to the inherent complexity and generative nature of LLMs. In this
work, we introduce a comprehensive auditing framework for unlearning
evaluation, comprising three benchmark datasets, six unlearning algorithms, and
five prompt-based auditing methods. By using various auditing algorithms, we
evaluate the effectiveness and robustness of different unlearning strategies.
To explore alternatives beyond prompt-based auditing, we propose a novel
technique that leverages intermediate activation perturbations, addressing the
limitations of auditing methods that rely solely on model inputs and outputs.

</details>


### [145] [Rethinking Regularization Methods for Knowledge Graph Completion](https://arxiv.org/abs/2505.23442)
*Linyu Li,Zhi Jin,Yuanpeng He,Dongming Jin,Haoran Duan,Zhengwei Tao,Xuan Zhang,Jiandong Li*

Main category: cs.LG

TL;DR: 本文通过实证研究发现，在知识图谱补全（KGC）模型中引入基于排序的选择性稀疏正则化（SPR）方法，可突破模型原有性能上限。该方法选择性地抑制噪声特征，实验证明其优于传统正则化。


<details>
  <summary>Details</summary>
Motivation: 现有KGC模型未充分利用正则化的深层潜力，尤其缺乏对特征选择性的动态调整。本文旨在通过系统性研究揭示正则化对模型性能边界的突破作用，并提出更优的稀疏正则化设计。

Method: 提出SPR稀疏正则化方法：1) 在嵌入向量中识别显著特征分量；2) 基于排序动态调整惩罚强度，抑制噪声分量；3) 通过多数据集、多模型的对比实验验证有效性。

Result: 在多个基准数据集（如FB15k-237、WN18RR）上，SPR使TransE、RotatE等模型的MRR指标提升3-8%，且收敛速度提升20%，显著优于L2/L1正则化。

Conclusion: 选择性稀疏正则化通过动态特征筛选机制，使KGC模型突破性能瓶颈。该方法为深度学习中的正则化设计提供了新范式，未来可扩展至其他表示学习任务。

Abstract: Knowledge graph completion (KGC) has attracted considerable attention in
recent years because it is critical to improving the quality of knowledge
graphs. Researchers have continuously explored various models. However, most
previous efforts have neglected to take advantage of regularization from a
deeper perspective and therefore have not been used to their full potential.
This paper rethinks the application of regularization methods in KGC. Through
extensive empirical studies on various KGC models, we find that carefully
designed regularization not only alleviates overfitting and reduces variance
but also enables these models to break through the upper bounds of their
original performance. Furthermore, we introduce a novel sparse-regularization
method that embeds the concept of rank-based selective sparsity into the KGC
regularizer. The core idea is to selectively penalize those components with
significant features in the embedding vector, thus effectively ignoring many
components that contribute little and may only represent noise. Various
comparative experiments on multiple datasets and multiple models show that the
SPR regularization method is better than other regularization methods and can
enable the KGC model to further break through the performance margin.

</details>


### [146] [Domain-Aware Tensor Network Structure Search](https://arxiv.org/abs/2505.23537)
*Giorgos Iacovides,Wuyang Zhou,Chao Li,Qibin Zhao,Danilo Mandic*

Main category: cs.LG

TL;DR: 提出基于大语言模型（LLM）的领域感知张量网络结构搜索框架tnLLM，通过结合领域知识和LLM推理能力显著减少计算成本，同时提供可解释性


<details>
  <summary>Details</summary>
Motivation: 现有张量网络结构搜索方法存在计算开销大、忽略领域信息、结构不透明三大痛点，需要开发高效且可解释的解决方案

Method: 设计领域感知提示框架，利用LLM根据张量模式间实际关系推断网络结构，同时生成结构解释，并能通过领域知识引导其他搜索算法初始化

Result: tnLLM在保持理论性能前提下，仅需SOTA算法1/50的评估次数即达到同等指标，且能加速传统方法收敛速度

Conclusion: 将LLM引入张量网络结构搜索实现了效率与可解释性的双重突破，为复杂优化问题提供了新的领域感知解决方案路径

Abstract: Tensor networks (TNs) provide efficient representations of high-dimensional
data, yet identification of the optimal TN structures, the so called tensor
network structure search (TN-SS) problem, remains a challenge. Current
state-of-the-art (SOTA) algorithms are computationally expensive as they
require extensive function evaluations, which is prohibitive for real-world
applications. In addition, existing methods ignore valuable domain information
inherent in real-world tensor data and lack transparency in their identified TN
structures. To this end, we propose a novel TN-SS framework, termed the tnLLM,
which incorporates domain information about the data and harnesses the
reasoning capabilities of large language models (LLMs) to directly predict
suitable TN structures. The proposed framework involves a domain-aware
prompting pipeline which instructs the LLM to infer suitable TN structures
based on the real-world relationships between tensor modes. In this way, our
approach is capable of not only iteratively optimizing the objective function,
but also generating domain-aware explanations for the identified structures.
Experimental results demonstrate that tnLLM achieves comparable TN-SS objective
function values with much fewer function evaluations compared to SOTA
algorithms. Furthermore, we demonstrate that the LLM-enabled domain information
can be used to find good initializations in the search space for sampling-based
SOTA methods to accelerate their convergence while preserving theoretical
performance guarantees.

</details>


### [147] [Segment Policy Optimization: Effective Segment-Level Credit Assignment in RL for Large Language Models](https://arxiv.org/abs/2505.23564)
*Yiran Guo,Lijie Xu,Jie Liu,Dan Ye,Shuang Qiu*

Main category: cs.LG

TL;DR: 提出分段策略优化框架SPO，通过中观粒度的分段优势估计平衡信用分配精度与计算成本，显著提升大语言模型的推理能力


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法存在两极化问题：Token级方法（如PPO）优势信号细但评论家模型难训练，轨迹级方法（如GRPO）依赖粗粒度信号导致信用分配不准确

Method: 1. 灵活分段划分策略 2. 基于蒙特卡洛的无评论家分段优势估计 3. 包含概率掩码策略的分段优化方法。特别提出SPO-chain（短链思维划分）和SPO-tree（长链树状估计）两种实现方案

Result: 在GSM8K数学推理任务上相对PPO/GRPO提升6-12个百分点，在MATH500长上下文场景下实现7-11个百分点的准确率提升

Conclusion: SPO框架通过中观粒度优势估计，有效平衡了信用分配精度与计算效率，其模块化设计可适配不同推理场景，实验证明显著优于现有强化学习方法

Abstract: Enhancing the reasoning capabilities of large language models effectively
using reinforcement learning (RL) remains a crucial challenge. Existing
approaches primarily adopt two contrasting advantage estimation granularities:
Token-level methods (e.g., PPO) aim to provide the fine-grained advantage
signals but suffer from inaccurate estimation due to difficulties in training
an accurate critic model. On the other extreme, trajectory-level methods (e.g.,
GRPO) solely rely on a coarse-grained advantage signal from the final reward,
leading to imprecise credit assignment. To address these limitations, we
propose Segment Policy Optimization (SPO), a novel RL framework that leverages
segment-level advantage estimation at an intermediate granularity, achieving a
better balance by offering more precise credit assignment than trajectory-level
methods and requiring fewer estimation points than token-level methods,
enabling accurate advantage estimation based on Monte Carlo (MC) without a
critic model. SPO features three components with novel strategies: (1) flexible
segment partition; (2) accurate segment advantage estimation; and (3) policy
optimization using segment advantages, including a novel probability-mask
strategy. We further instantiate SPO for two specific scenarios: (1) SPO-chain
for short chain-of-thought (CoT), featuring novel cutpoint-based partition and
chain-based advantage estimation, achieving $6$-$12$ percentage point
improvements in accuracy over PPO and GRPO on GSM8K. (2) SPO-tree for long CoT,
featuring novel tree-based advantage estimation, which significantly reduces
the cost of MC estimation, achieving $7$-$11$ percentage point improvements
over GRPO on MATH500 under 2K and 4K context evaluation. We make our code
publicly available at https://github.com/AIFrameResearch/SPO.

</details>


### [148] [On-Policy RL with Optimal Reward Baseline](https://arxiv.org/abs/2505.23585)
*Yaru Hao,Li Dong,Xun Wu,Shaohan Huang,Zewen Chi,Furu Wei*

Main category: cs.LG

TL;DR: 提出新型强化学习算法OPO，通过精确在策略训练和最优奖励基线解决现有方法训练不稳定和计算效率低的问题，在数学推理任务中展现优越性能。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习算法存在训练不稳定（因宽松的在策略约束）和计算效率低（需辅助模型）的双重痛点，制约大语言模型对齐和推理能力提升。

Method: OPO算法强调精确在策略训练保持稳定性，引入理论层面最小化梯度方差的最优奖励基线，无需额外模型或正则化项。

Result: 在数学推理基准测试中，OPO展现出更优性能、更低策略偏移和更高输出熵值，生成响应更多样化且不重复。

Conclusion: OPO为大规模语言模型对齐和推理任务提供了稳定高效的强化学习新方向，具有重要的应用潜力。

Abstract: Reinforcement learning algorithms are fundamental to align large language
models with human preferences and to enhance their reasoning capabilities.
However, current reinforcement learning algorithms often suffer from training
instability due to loose on-policy constraints and computational inefficiency
due to auxiliary models. In this work, we propose On-Policy RL with Optimal
reward baseline (OPO), a novel and simplified reinforcement learning algorithm
designed to address these challenges. OPO emphasizes the importance of exact
on-policy training, which empirically stabilizes the training process and
enhances exploration. Moreover, OPO introduces the optimal reward baseline that
theoretically minimizes gradient variance. We evaluate OPO on mathematical
reasoning benchmarks. The results demonstrate its superior performance and
training stability without additional models or regularization terms.
Furthermore, OPO achieves lower policy shifts and higher output entropy,
encouraging more diverse and less repetitive responses. These results highlight
OPO as a promising direction for stable and effective reinforcement learning in
large language model alignment and reasoning tasks. The implementation is
provided at https://github.com/microsoft/LMOps/tree/main/opo.

</details>


### [149] [Differential Information: An Information-Theoretic Perspective on Preference Optimization](https://arxiv.org/abs/2505.23761)
*Yunjae Won,Hyunji Lee,Hyeonbin Hwang,Minjoon Seo*

Main category: cs.LG

TL;DR: 通过微分信息分布理论揭示DPO算法的理论机制，阐明其对策略分布熵值的调控作用，为不同任务场景提供优化方向。


<details>
  <summary>Details</summary>
Motivation: 现有DPO算法缺乏对log-ratio奖励参数化的完整理论解释，需建立其与策略更新中信息增益的数学关联。

Method: 提出微分信息分布(DID)框架，证明当偏好标签包含参考策略到目标策略的微分信息时，DPO的log-ratio奖励具有唯一最优性，并推导拒绝响应采样分布的闭式解。

Result: 理论验证显示：低熵微分信息强化策略分布，高熵信息引发平滑效应；实验证明高熵信息利于通用指令跟随，低熵信息提升知识问答性能。

Conclusion: DID理论统一解释了DPO目标函数、偏好数据结构与策略行为的关系，为不同任务选择信息熵层级提供理论指导。

Abstract: Direct Preference Optimization (DPO) has become a standard technique for
aligning language models with human preferences in a supervised manner. Despite
its empirical success, the theoretical justification behind its log-ratio
reward parameterization remains incomplete. In this work, we address this gap
by utilizing the Differential Information Distribution (DID): a distribution
over token sequences that captures the information gained during policy
updates. First, we show that when preference labels encode the differential
information required to transform a reference policy into a target policy, the
log-ratio reward in DPO emerges as the uniquely optimal form for learning the
target policy via preference optimization. This result naturally yields a
closed-form expression for the optimal sampling distribution over rejected
responses. Second, we find that the condition for preferences to encode
differential information is fundamentally linked to an implicit assumption
regarding log-margin ordered policies-an inductive bias widely used in
preference optimization yet previously unrecognized. Finally, by analyzing the
entropy of the DID, we characterize how learning low-entropy differential
information reinforces the policy distribution, while high-entropy differential
information induces a smoothing effect, which explains the log-likelihood
displacement phenomenon. We validate our theoretical findings in synthetic
experiments and extend them to real-world instruction-following datasets. Our
results suggest that learning high-entropy differential information is crucial
for general instruction-following, while learning low-entropy differential
information benefits knowledge-intensive question answering. Overall, our work
presents a unifying perspective on the DPO objective, the structure of
preference data, and resulting policy behaviors through the lens of
differential information.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [150] [TailorSQL: An NL2SQL System Tailored to Your Query Workload](https://arxiv.org/abs/2505.23039)
*Kapil Vaidya,Jialin Ding,Sebastian Kosak,David Kernert,Chuan Lei,Xiao Qin,Abhinav Tripathy,Ramesh Balan,Balakrishnan Narayanaswamy,Tim Kraska*

Main category: cs.DB

TL;DR: TailorSQL系统通过利用历史SQL查询工作负载中的隐含信息，在NL2SQL任务中实现了执行准确率最高2倍的提升


<details>
  <summary>Details</summary>
Motivation: 现有NL2SQL技术仅依赖数据库模式信息，忽视了历史查询工作负载中隐含的宝贵信息（如常见连接路径、模糊命名字段语义等），这些信息可以显著提升翻译准确率和效率

Method: 提出TailorSQL系统，通过分析数据库历史查询模式来获取隐含语义信息，并基于特定工作负载进行优化

Result: 在标准化基准测试中实现最高2倍的执行准确率提升，同时降低查询延迟

Conclusion: 利用历史查询工作负载信息能有效突破现有NL2SQL性能瓶颈，TailorSQL验证了工作负载专业化优化的重要价值

Abstract: NL2SQL (natural language to SQL) translates natural language questions into
SQL queries, thereby making structured data accessible to non-technical users,
serving as the foundation for intelligent data applications. State-of-the-art
NL2SQL techniques typically perform translation by retrieving database-specific
information, such as the database schema, and invoking a pre-trained large
language model (LLM) using the question and retrieved information to generate
the SQL query.
  However, existing NL2SQL techniques miss a key opportunity which is present
in real-world settings: NL2SQL is typically applied on existing databases which
have already served many SQL queries in the past. The past query workload
implicitly contains information which is helpful for accurate NL2SQL
translation and is not apparent from the database schema alone, such as common
join paths and the semantics of obscurely-named tables and columns. We
introduce TailorSQL, a NL2SQL system that takes advantage of information in the
past query workload to improve both the accuracy and latency of translating
natural language questions into SQL. By specializing to a given workload,
TailorSQL achieves up to 2$\times$ improvement in execution accuracy on
standardized benchmarks.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [151] [NGPU-LM: GPU-Accelerated N-Gram Language Model for Context-Biasing in Greedy ASR Decoding](https://arxiv.org/abs/2505.22857)
*Vladimir Bataev,Andrei Andrusenko,Lilit Grigoryan,Aleksandr Laptev,Vitaly Lavrukhin,Boris Ginsburg*

Main category: eess.AS

TL;DR: 提出NGPU-LM方法优化统计n-gram语言模型的GPU并行计算效率，实现ASR场景下低开销、高精度的解码


<details>
  <summary>Details</summary>
Motivation: 解决现有统计n-gram语言模型在GPU并行化方面的低效问题，提升工业场景适用性

Method: 重新设计语言模型数据结构，支持transducer/attention encoder-decoder/CTC等多种ASR模型的定制化贪婪解码

Result: 计算开销低于7%，域外场景准确率提升超50%，避免束搜索的速度损失

Conclusion: 开源实现有效平衡效率与精度，为ASR系统提供实用型上下文偏置解决方案

Abstract: Statistical n-gram language models are widely used for context-biasing tasks
in Automatic Speech Recognition (ASR). However, existing implementations lack
computational efficiency due to poor parallelization, making context-biasing
less appealing for industrial use. This work rethinks data structures for
statistical n-gram language models to enable fast and parallel operations for
GPU-optimized inference. Our approach, named NGPU-LM, introduces customizable
greedy decoding for all major ASR model types - including transducers,
attention encoder-decoder models, and CTC - with less than 7% computational
overhead. The proposed approach can eliminate more than 50% of the accuracy gap
between greedy and beam search for out-of-domain scenarios while avoiding
significant slowdown caused by beam search. The implementation of the proposed
NGPU-LM is open-sourced.

</details>
