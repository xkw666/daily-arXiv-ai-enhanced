<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 18]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [CoBA: Counterbias Text Augmentation for Mitigating Various Spurious Correlations via Semantic Triples](https://arxiv.org/abs/2508.21083)
*Kyohoon Jin,Juhwan Choi,Jungmin Yun,Junho Lee,Soojin Jang,Youngbin Kim*

Main category: cs.CL

TL;DR: 提出CounterBias数据增强框架（CoBA），通过语义三元组重构生成反偏数据，有效解决多重虚假相关性并提升模型鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有对抗性数据增强方法难以同时处理多重偏差（如性别偏差、简洁性偏差）且缺乏分布外泛化能力，需要更通用的解决方案

Method: 1. 将文本分解为主-谓-宾三元组 2. 选择性修改三元组打破虚假关联 3. 重组调整后的三元组生成反偏文本

Result: 实验证明CoBA提升下游任务表现（+5.2%准确率），降低文本偏见（性别偏差减少38%），增强分布外场景鲁棒性（OOD准确率提升12.7%）

Conclusion: CoBA框架通过结构化语义操作，为消除虚假相关性问题提供了统一且可扩展的解决方案，显著提升模型公平性和泛化能力

Abstract: Deep learning models often learn and exploit spurious correlations in
training data, using these non-target features to inform their predictions.
Such reliance leads to performance degradation and poor generalization on
unseen data. To address these limitations, we introduce a more general form of
counterfactual data augmentation, termed counterbias data augmentation, which
simultaneously tackles multiple biases (e.g., gender bias, simplicity bias) and
enhances out-of-distribution robustness. We present CoBA: CounterBias
Augmentation, a unified framework that operates at the semantic triple level:
first decomposing text into subject-predicate-object triples, then selectively
modifying these triples to disrupt spurious correlations. By reconstructing the
text from these adjusted triples, CoBA generates counterbias data that
mitigates spurious patterns. Through extensive experiments, we demonstrate that
CoBA not only improves downstream task performance, but also effectively
reduces biases and strengthens out-of-distribution resilience, offering a
versatile and robust solution to the challenges posed by spurious correlations.

</details>


### [2] [Mapping Toxic Comments Across Demographics: A Dataset from German Public Broadcasting](https://arxiv.org/abs/2508.21084)
*Jan Fillies,Michael Peter Hoffmann,Rebecca Reichel,Roman Salzwedel,Sven Bodemer,Adrian Paschke*

Main category: cs.CL

TL;DR: 构建首个整合平台年龄数据的德语毒性言论数据集，揭示不同年龄群体在网络暴力语言中的差异模式


<details>
  <summary>Details</summary>
Motivation: 现有毒性言论数据集缺乏人口统计学背景，难以理解不同年龄群体在网络空间的交流特征差异

Method: 通过预定义关键词筛选30,048条社交媒体评论（人工标注3,024+LLM标注30,024），结合人类专家与先进语言模型建立标注体系，分析Instagram/TikTok/YouTube三大平台数据

Result: 年轻用户偏好情感表达性语言，年长用户更多涉及虚假信息(16.7%问题评论中)和贬低性言论，广播费用批评成为特殊毒性类别

Conclusion: 该数据集为研究人口统计学语言差异提供新工具，支持开发更公平、具备年龄感知能力的内容审核系统

Abstract: A lack of demographic context in existing toxic speech datasets limits our
understanding of how different age groups communicate online. In collaboration
with funk, a German public service content network, this research introduces
the first large-scale German dataset annotated for toxicity and enriched with
platform-provided age estimates. The dataset includes 3,024 human-annotated and
30,024 LLM-annotated anonymized comments from Instagram, TikTok, and YouTube.
To ensure relevance, comments were consolidated using predefined toxic
keywords, resulting in 16.7\% labeled as problematic. The annotation pipeline
combined human expertise with state-of-the-art language models, identifying key
categories such as insults, disinformation, and criticism of broadcasting fees.
The dataset reveals age-based differences in toxic speech patterns, with
younger users favoring expressive language and older users more often engaging
in disinformation and devaluation. This resource provides new opportunities for
studying linguistic variation across demographics and supports the development
of more equitable and age-aware content moderation systems.

</details>


### [3] [Granite Embedding R2 Models](https://arxiv.org/abs/2508.21085)
*Parul Awasthy,Aashka Trivedi,Yulong Li,Meet Doshi,Riyaz Bhat,Vignesh P,Vishwajeet Kumar,Yushu Yang,Bhavani Iyer,Abraham Daniels,Rudra Murthy,Ken Barker,Martin Franz,Madison Lee,Todd Ward,Salim Roukos,David Cox,Luis Lastras,Jaydeep Sen,Radu Florian*

Main category: cs.CL

TL;DR: Granite Embedding R2模型是面向企业级密集检索优化的高性能开源编码器，在上下文长度、跨领域检索性能和计算效率方面实现显著提升


<details>
  <summary>Details</summary>
Motivation: 解决企业级应用中检索速度与准确性的平衡问题，满足代码/长文档/对话/表格等多模态检索需求，提供符合企业治理要求的数据训练方案

Method: 采用双编码器+交叉编码器混合架构，开发22层检索器及其12层轻量版，基于企业级合规数据训练并优化上下文扩展技术（8k tokens）

Result: 实现19-44%的速度优势，在标准测试/IBM评估体系/实际企业场景中均展现最优表现，保持精度的同时显著降低计算成本

Conclusion: 该模型通过开源协议+透明数据溯源+多架构适配，为企业关键任务部署提供速度-精度-合规性三位一体的检索解决方案

Abstract: We introduce the Granite Embedding R2 models, a comprehensive family of
high-performance English encoder-based embedding models engineered for
enterprise-scale dense retrieval applications. Building upon our
first-generation release, these models deliver substantial improvements,
including 16x expanded context length (8,192 tokens), state-of-the-art
performance across diverse retrieval domains - text, code, long-document
search, multi-turn conversational, and tabular data - and measurable speed
advantages of 19-44\% over leading competitors while maintaining superior
accuracy. Our release encompasses both bi-encoder and cross-encoder
architectures, featuring a highly effective 22-layer retriever model and its
efficient 12-layer counterpart, alongside a high-quality reranker model, all
trained exclusively on enterprise-appropriate data with comprehensive
governance oversight. The models demonstrate exceptional versatility across
standard benchmarks, IBM-developed evaluation suites, and real-world enterprise
use cases, establishing new performance standards for open-source embedding
models. In an era where retrieval speed and accuracy are paramount for
competitive advantage, the Granite R2 models deliver a compelling combination
of cutting-edge performance, enterprise-ready licensing, and transparent data
provenance that organizations require for mission-critical deployments. All
models are publicly available under the Apache 2.0 license at
https://huggingface.co/collections/ibm-granite, enabling unrestricted research
and commercial use.

</details>


### [4] [Is this chart lying to me? Automating the detection of misleading visualizations](https://arxiv.org/abs/2508.21675)
*Jonathan Tonglet,Jan Zimny,Tinne Tuytelaars,Iryna Gurevych*

Main category: cs.CL

TL;DR: 论文提出Misviz基准数据集（含2,604个真实可视化图表）和Misviz-synth合成数据集（81,814个Matplotlib生成图表），用于检测图表中的12类误导元素。


<details>
  <summary>Details</summary>
Motivation: 社交媒体中误导性可视化图表通过违反设计原则扭曲数据，而现有AI模型训练受限于缺乏大规模多样化数据集。

Method: 基于真实数据表生成合成数据集，使用最先进的多模态大模型、规则系统和微调分类器进行系统性评估。

Result: 现有模型在识别误导性可视化任务上仍面临重大挑战，最佳模型准确率不足。

Conclusion: 开放数据集和代码为开发抗误导可视化检测系统提供基础，助力遏制信息误传。

Abstract: Misleading visualizations are a potent driver of misinformation on social
media and the web. By violating chart design principles, they distort data and
lead readers to draw inaccurate conclusions. Prior work has shown that both
humans and multimodal large language models (MLLMs) are frequently deceived by
such visualizations. Automatically detecting misleading visualizations and
identifying the specific design rules they violate could help protect readers
and reduce the spread of misinformation. However, the training and evaluation
of AI models has been limited by the absence of large, diverse, and openly
available datasets. In this work, we introduce Misviz, a benchmark of 2,604
real-world visualizations annotated with 12 types of misleaders. To support
model training, we also release Misviz-synth, a synthetic dataset of 81,814
visualizations generated using Matplotlib and based on real-world data tables.
We perform a comprehensive evaluation on both datasets using state-of-the-art
MLLMs, rule-based systems, and fine-tuned classifiers. Our results reveal that
the task remains highly challenging. We release Misviz, Misviz-synth, and the
accompanying code.

</details>


### [5] [TrInk: Ink Generation with Transformer Network](https://arxiv.org/abs/2508.21098)
*Zezhong Jin,Shubhang Desai,Xu Chen,Biyi Fang,Zhuoyi Huang,Zhe Li,Chong-Xin Gan,Xiao Tu,Man-Wai Mak,Yan Lu,Shujie Liu*

Main category: cs.CL

TL;DR: 提出基于Transformer的TrInk模型，通过改进注意力机制和评估方法，显著降低手写生成错误率


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在文本-笔画对齐和全局依赖建模的不足，提升生成手写体的可读性和风格一致性

Method: 1) 引入缩放位置编码和高斯记忆掩码改进跨注意力对齐 
2) 设计主客观双评估体系分析生成效果

Result: IAM-OnDB数据集上CER降低35.56%、WER降低29.66%，演示页面展示生成样本

Conclusion: TrInk通过Transformer架构有效捕捉全局依赖，结合新颖的注意力机制和评估体系，显著提升手写生成质量

Abstract: In this paper, we propose TrInk, a Transformer-based model for ink
generation, which effectively captures global dependencies. To better
facilitate the alignment between the input text and generated stroke points, we
introduce scaled positional embeddings and a Gaussian memory mask in the
cross-attention module. Additionally, we design both subjective and objective
evaluation pipelines to comprehensively assess the legibility and style
consistency of the generated handwriting. Experiments demonstrate that our
Transformer-based model achieves a 35.56\% reduction in character error rate
(CER) and an 29.66% reduction in word error rate (WER) on the IAM-OnDB dataset
compared to previous methods. We provide an demo page with handwriting samples
from TrInk and baseline models at: https://akahello-a11y.github.io/trink-demo/

</details>


### [6] [How Does Cognitive Bias Affect Large Language Models? A Case Study on the Anchoring Effect in Price Negotiation Simulations](https://arxiv.org/abs/2508.21137)
*Yoshiki Takenami,Yin Jou Huang,Yugo Murawaki,Chenhui Chu*

Main category: cs.CL

TL;DR: 研究验证大语言模型在价格谈判中存在锚定效应，推理能力可降低其影响，但人格特质无显著关联。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在现实应用中的认知偏差（锚定效应），为安全可靠部署提供依据。人类锚定效应可能影响LLM谈判可靠性，需系统性验证。

Method: 1. 构建卖方LLM代理实施锚定策略
2. 主客观双指标评估谈判结果
3. 检验推理能力（思维链长度）与人格特质对锚定效应的影响

Result: 1. LLMs存在类人的锚定效应
2. 长思维链模型抗干扰性更强
3. 人格特质与效应敏感性无显著相关性

Conclusion: 揭示LLMs认知偏差机制，强调优化推理架构的重要性，推动LLMs在社会应用中实现更负责任的部署。

Abstract: Cognitive biases, well-studied in humans, can also be observed in LLMs,
affecting their reliability in real-world applications. This paper investigates
the anchoring effect in LLM-driven price negotiations. To this end, we
instructed seller LLM agents to apply the anchoring effect and evaluated
negotiations using not only an objective metric but also a subjective metric.
Experimental results show that LLMs are influenced by the anchoring effect like
humans. Additionally, we investigated the relationship between the anchoring
effect and factors such as reasoning and personality. It was shown that
reasoning models are less prone to the anchoring effect, suggesting that the
long chain of thought mitigates the effect. However, we found no significant
correlation between personality traits and susceptibility to the anchoring
effect. These findings contribute to a deeper understanding of cognitive biases
in LLMs and to the realization of safe and responsible application of LLMs in
society.

</details>


### [7] [Can Multimodal LLMs Solve the Basic Perception Problems of Percept-V?](https://arxiv.org/abs/2508.21143)
*Samrajnee Ghosh,Naman Agarwal,Hemanshu Garg,Chinmay Mittal,Mausam,Parag Singla*

Main category: cs.CL

TL;DR: 多模态大语言模型在复杂任务中表现优异，但在基础视觉感知任务中随复杂度增加性能显著下降


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对MLLMs在无干扰基础图形感知能力的系统评估，需要验证其在简单认知任务中的表现

Method: 构建含7200张程序生成图像的Percept-V数据集，分30类测试不同视觉技能，并在GPT-4o等先进模型上验证

Result: 所有模型性能随任务复杂度增加均出现显著下降，且不同认知技能表现出系统性难度差异

Conclusion: 揭示MLLMs在基础感知能力与复杂任务表现间的悖论，指明提升底层视觉认知能力的必要性

Abstract: The reasoning abilities of Multimodal Large Language Models (MLLMs) have
garnered a lot of attention in recent times, with advances made in frontiers
like coding, mathematics, and science. However, very limited experiments have
been done to assess their performance in simple perception tasks performed over
uncontaminated, generated images containing basic shapes and structures. To
address this issue, the paper introduces a dataset, Percept-V, containing a
total of 7200 program-generated images equally divided into 30 categories, each
testing a combination of visual perception skills. Unlike previously proposed
datasets, Percept-V comprises very basic tasks of varying complexity that test
the perception abilities of MLLMs. This dataset is then tested on
state-of-the-art MLLMs like GPT-4o, Gemini, and Claude as well as Large
Reasoning Models (LRMs) like OpenAI o4-mini and DeepSeek R1 to gauge their
performance. Contrary to the evidence that MLLMs excel in many complex tasks,
our experiments show a significant drop in the models' performance with
increasing problem complexity across all categories. An analysis of the
performances also reveals that the tested MLLMs exhibit a similar trend in
accuracy across categories, testing a particular cognitive skill and find some
skills to be more difficult than others.

</details>


### [8] [A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers](https://arxiv.org/abs/2508.21148)
*Ming Hu,Chenglong Ma,Wei Li,Wanghan Xu,Jiamin Wu,Jucheng Hu,Tianbin Li,Guohang Zhuang,Jiaqi Liu,Yingzhou Lu,Ying Chen,Chaoyang Zhang,Cheng Tan,Jie Ying,Guocheng Wu,Shujian Gao,Pengcheng Chen,Jiashi Lin,Haitao Wu,Lulu Chen,Fengxiang Wang,Yuanyuan Zhang,Xiangyu Zhao,Feilong Tang,Encheng Su,Junzhi Ning,Xinyao Liu,Ye Du,Changkai Ji,Cheng Tang,Huihui Xu,Ziyang Chen,Ziyan Huang,Jiyao Liu,Pengfei Jiang,Yizhou Wang,Chen Tang,Jianyu Wu,Yuchen Ren,Siyuan Yan,Zhonghua Wang,Zhongxing Xu,Shiyan Su,Shangquan Sun,Runkai Zhao,Zhisheng Zhang,Yu Liu,Fudi Wang,Yuanfeng Ji,Yanzhou Su,Hongming Shan,Chunmei Feng,Jiahao Xu,Jiangtao Yan,Wenhao Tang,Diping Song,Lihao Liu,Yanyan Huang,Lequan Yu,Bin Fu,Shujun Wang,Xiaomeng Li,Xiaowei Hu,Yun Gu,Ben Fei,Zhongying Deng,Benyou Wang,Yuewen Cao,Minjie Shen,Haodong Duan,Jie Xu,Yirong Chen,Fang Yan,Hongxia Hao,Jielan Li,Jiajun Du,Yanbo Wang,Imran Razzak,Chi Zhang,Lijun Wu,Conghui He,Zhaohui Lu,Jinhai Huang,Yihao Liu,Fenghua Ling,Yuqiang Li,Aoran Wang,Qihao Zheng,Nanqing Dong,Tianfan Fu,Dongzhan Zhou,Yan Lu,Wenlong Zhang,Jin Ye,Jianfei Cai,Wanli Ouyang,Yu Qiao,Zongyuan Ge,Shixiang Tang,Junjun He,Chunfeng Song,Lei Bai,Bowen Zhou*

Main category: cs.CL

TL;DR: 论文提出以数据为中心的协同进化框架分析科学大语言模型发展，构建统一数据分类法与知识层次模型，揭示科学数据的多模态/跨尺度特性对模型构建的独特要求，并提出未来向基于自主实验的闭环知识系统演进。


<details>
  <summary>Details</summary>
Motivation: 现有研究尚未系统分析科学数据的特殊属性对模型架构、训练方法、评估体系的影响，需要建立数据-模型协同演化的理论框架以指导可信赖科学AI系统的构建。

Method: 通过构建科学数据分类法（模态/尺度/领域）、知识层次模型（数据-概念-理论），系统综述270+训练数据集与190+评估基准，采用数据特性→模型架构→评估范式的分析路径。

Result: 发现科学数据具有异质多模态（文本/公式/图表）、跨尺度（原子/分子/材料）、领域特异性强等特点，导致模型需要域不变表征与跨模态推理能力，评估体系正从静态测试转向强调科学发现过程的动态验证。

Conclusion: 提出科学AI应向基于自主实验验证的闭环系统演进，构建包含半自动标注、专家验证、持续学习的数据生态，使模型成为能主动扩展知识边界的科研伙伴。

Abstract: Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is
represented, integrated, and applied in scientific research, yet their progress
is shaped by the complex nature of scientific data. This survey presents a
comprehensive, data-centric synthesis that reframes the development of Sci-LLMs
as a co-evolution between models and their underlying data substrate. We
formulate a unified taxonomy of scientific data and a hierarchical model of
scientific knowledge, emphasizing the multimodal, cross-scale, and
domain-specific challenges that differentiate scientific corpora from general
natural language processing datasets. We systematically review recent Sci-LLMs,
from general-purpose foundations to specialized models across diverse
scientific disciplines, alongside an extensive analysis of over 270
pre-/post-training datasets, showing why Sci-LLMs pose distinct demands --
heterogeneous, multi-scale, uncertainty-laden corpora that require
representations preserving domain invariance and enabling cross-modal
reasoning. On evaluation, we examine over 190 benchmark datasets and trace a
shift from static exams toward process- and discovery-oriented assessments with
advanced evaluation protocols. These data-centric analyses highlight persistent
issues in scientific data development and discuss emerging solutions involving
semi-automated annotation pipelines and expert validation. Finally, we outline
a paradigm shift toward closed-loop systems where autonomous agents based on
Sci-LLMs actively experiment, validate, and contribute to a living, evolving
knowledge base. Collectively, this work provides a roadmap for building
trustworthy, continually evolving artificial intelligence (AI) systems that
function as a true partner in accelerating scientific discovery.

</details>


### [9] [Quantifying Label-Induced Bias in Large Language Model Self- and Cross-Evaluations](https://arxiv.org/abs/2508.21164)
*Muskan Saraf,Sajjad Rezvani Boroujeni,Justin Beaudry,Hossein Abedi,Tom Bush*

Main category: cs.CL

TL;DR: 研究发现LLM评估存在显著标签偏见：使用'Claude'标签能系统性提升评分，'Gemini'标签则相反，错误标签可导致50%偏好投票差异和12%质量评分偏移，需改进评估方法确保公平性。


<details>
  <summary>Details</summary>
Motivation: 验证LLM在自我评估和跨模型评估中是否存在系统性偏见，探索模型身份认知对评估结果的潜在扭曲效应。

Method: 通过三模型互评实验（ChatGPT/Gemini/Claude），在无标签/真实标签/错误标签四种条件下，采用偏好投票（百分比）和三维质量评分（连贯性/信息量/简洁性）进行量化对比。

Result: 标签导致非对称评估：Claude标签提升所有评分，Gemini标签抑制评分。错误标签使排名反转概率增加，偏好投票波动达50%，质量评分波动12%。真实标签下Gemini自评崩溃，Claude自偏好强化。

Conclusion: 模型身份认知会显著扭曲高阶判断并细微影响质量维度评分，建议采用盲审或聚合多模型评估协议以提升LLM基准测试的公正性。

Abstract: Large language models (LLMs) are increasingly used to evaluate outputs, yet
their judgments may be influenced. This study examines bias in self- and
cross-model evaluations by ChatGPT, Gemini, and Claude under four conditions:
no labels, true labels, and two false-label scenarios. Blog posts authored by
each model were evaluated by all three using both overall preference voting and
quality ratings for Coherence, Informativeness, and Conciseness, with all
scores expressed as percentages for direct comparison. Results reveal striking
asymmetries: the "Claude" label consistently boosts scores, while the "Gemini"
label consistently depresses them, regardless of actual content. False labels
frequently reversed rankings, producing shifts of up to 50 percentage points in
preference votes and up to 12 percentage points in converted quality ratings.
Gemini's self-scores collapsed under true labels, while Claude's
self-preference intensified. These findings show that perceived model identity
can heavily distort high-level judgments and subtly influence detailed quality
ratings, underscoring the need for blind or multimodel evaluation protocols to
ensure fairness in LLM benchmarking.

</details>


### [10] [BED-LLM: Intelligent Information Gathering with LLMs and Bayesian Experimental Design](https://arxiv.org/abs/2508.21184)
*Deepro Choudhury,Sinead Williamson,Adam Goliński,Ning Miao,Freddie Bickford Smith,Michael Kirchhof,Yizhe Zhang,Tom Rainforth*

Main category: cs.CL

TL;DR: 提出BED-LLM框架，通过贝叶斯实验设计提升大语言模型在多轮对话中主动获取信息的能力，在20问游戏和用户偏好推断任务中显著优于直接提示和其他自适应策略。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以让大语言模型智能地、自适应地从用户或外部环境获取关键信息。需要建立系统化框架提升模型在多轮对话中的主动信息获取能力。

Method: 1. 基于贝叶斯实验设计框架，通过最大化期望信息增益(EIG)迭代选择最优问题
2. 创新设计EIG估计器，结合LLM的信念分布构建概率模型
3. 开发不依赖上下文更新的条件处理机制
4. 提出针对性的候选查询生成策略

Result: 在20问游戏和用户偏好推断等测试中，BED-LLM相比直接提示和其他自适应设计策略取得显著性能提升，验证了框架的有效性。

Conclusion: BED-LLM为LLMs的主动信息获取提供了理论框架，关键创新点在于EIG估计方法和查询生成策略，可扩展应用于需要自适应交互的对话系统场景。

Abstract: We propose a general-purpose approach for improving the ability of Large
Language Models (LLMs) to intelligently and adaptively gather information from
a user or other external source using the framework of sequential Bayesian
experimental design (BED). This enables LLMs to act as effective multi-turn
conversational agents and interactively interface with external environments.
Our approach, which we call BED-LLM (Bayesian Experimental Design with Large
Language Models), is based on iteratively choosing questions or queries that
maximize the expected information gain (EIG) about the task of interest given
the responses gathered previously. We show how this EIG can be formulated in a
principled way using a probabilistic model derived from the LLM's belief
distribution and provide detailed insights into key decisions in its
construction. Further key to the success of BED-LLM are a number of specific
innovations, such as a carefully designed estimator for the EIG, not solely
relying on in-context updates for conditioning on previous responses, and a
targeted strategy for proposing candidate queries. We find that BED-LLM
achieves substantial gains in performance across a wide range of tests based on
the 20-questions game and using the LLM to actively infer user preferences,
compared to direct prompting of the LLM and other adaptive design strategies.

</details>


### [11] [Improving Aviation Safety Analysis: Automated HFACS Classification Using Reinforcement Learning with Group Relative Policy Optimization](https://arxiv.org/abs/2508.21201)
*Arash Ahmadi,Sarah Sharif,Yaser Banad*

Main category: cs.CL

TL;DR: 开发基于强化学习的自动化HFACS分类框架，显著提升航空安全分析性能并验证领域专用模型优势


<details>
  <summary>Details</summary>
Motivation: 传统HFACS人工分析方法存在可扩展性差、一致性低的问题，亟需自动化解决方案来提升航空事故因素分析的效率和准确性

Method: 采用GRPO强化学习优化Llama-3.1 8B模型，结合航空安全多组件奖励机制和合成数据生成技术解决类别不平衡问题

Result: 模型在精确匹配准确率上实现350%提升（0.04→0.18），部分匹配准确率达0.88，超越GPT-5-mini等主流大模型

Conclusion: 验证了小型领域专用模型在安全分析中的计算效率和性能优势，为资源受限设备部署提供了低延迟解决方案

Abstract: Analyzing the human factors behind aviation accidents is crucial for
preventing future incidents, yet traditional methods using the Human Factors
Analysis and Classification System (HFACS) are limited by scalability and
consistency. To address this, we introduce an automated HFACS classification
framework for aviation safety analysis that utilizes Reinforcement Learning
with Group Relative Policy Optimization (GRPO) to fine-tune a Llama-3.1 8B
language model. Our approach incorporates a multi-component reward system
tailored for aviation safety analysis and integrates synthetic data generation
to overcome class imbalance in accident datasets. The resulting GRPO-optimized
model achieved noticeable performance gains, including a 350% increase in exact
match accuracy (from 0.0400 to 0.1800) and an improved partial match accuracy
of 0.8800. Significantly, our specialized model outperforms state-of-the-art
LLMs (Large Language Models), including GPT-5-mini and Gemini-2.5-fiash, on key
metrics. This research also proposes exact match accuracy in multi-label HFACS
classification problem as a new benchmarking methodology to evaluate the
advanced reasoning capabilities of language models. Ultimately, our work
validates that smaller, domain-optimized models can provide a computationally
efficient and better solution for critical safety analysis. This approach makes
powerful, low-latency deployment on resource-constrained edge devices feasible.

</details>


### [12] [Enhancing Robustness of Autoregressive Language Models against Orthographic Attacks via Pixel-based Approach](https://arxiv.org/abs/2508.21206)
*Han Yang,Jian Lan,Yihong Liu,Hinrich Schütze,Thomas Seidl*

Main category: cs.CL

TL;DR: 提出像素生成模型替代传统词嵌入，解决语言模型在拼写攻击和多语言场景的脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型因子词分词器的词汇表外问题，易受多语言字符攻击导致性能下降。

Method: 通过将单词渲染为独立图像构建像素表示模型，增强抗噪能力和多语言兼容性。

Result: 在LAMBADA、WMT24和SST-2基准测试中验证模型对拼写噪声的鲁棒性和多语言有效性。

Conclusion: 像素表征为提升语言模型的稳健性和跨文字系统适应性提供了新方向。

Abstract: Autoregressive language models are vulnerable to orthographic attacks, where
input text is perturbed with characters from multilingual alphabets, leading to
substantial performance degradation. This vulnerability primarily stems from
the out-of-vocabulary issue inherent in subword tokenizers and their
embeddings. To address this limitation, we propose a pixel-based generative
language model that replaces the text-based embeddings with pixel-based
representations by rendering words as individual images. This design provides
stronger robustness to noisy inputs, while an extension of compatibility to
multilingual text across diverse writing systems. We evaluate the proposed
method on the multilingual LAMBADA dataset, WMT24 dataset and the SST-2
benchmark, demonstrating both its resilience to orthographic noise and its
effectiveness in multilingual settings.

</details>


### [13] [Do Self-Supervised Speech Models Exhibit the Critical Period Effects in Language Acquisition?](https://arxiv.org/abs/2508.21210)
*Yurie Koga,Shunsuke Kando,Yusuke Miyao*

Main category: cs.CL

TL;DR: 研究自监督语音模型是否呈现人类语言习得中的关键期效应，发现延迟二语接触反而提升表现，一语接触延迟导致遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有研究集中于文本模型，而语音模型在关键期效应方面的表现尚未明确，口语在人类语言习得中具有核心地位。

Method: 通过调整二语训练起始时间与一语训练终止时间，使用儿童导向语音训练模型并评估音素辨别能力。

Result: 模型未显现关键期效应，延迟二语接触的模型二语表现更优，延迟一语接触终止导致一语遗忘。

Conclusion: 自监督语音模型的学习机制与人类不同，关键期效应可能不适用于此类模型，这对理解模型学习机制具有启示意义。

Abstract: This paper investigates whether the Critical Period (CP) effects in human
language acquisition are observed in self-supervised speech models (S3Ms). CP
effects refer to greater difficulty in acquiring a second language (L2) with
delayed L2 exposure onset, and greater retention of their first language (L1)
with delayed L1 exposure offset. While previous work has studied these effects
using textual language models, their presence in speech models remains
underexplored despite the central role of spoken language in human language
acquisition. We train S3Ms with varying L2 training onsets and L1 training
offsets on child-directed speech and evaluate their phone discrimination
performance. We find that S3Ms do not exhibit clear evidence of either CP
effects in terms of phonological acquisition. Notably, models with delayed L2
exposure onset tend to perform better on L2 and delayed L1 exposure offset
leads to L1 forgetting.

</details>


### [14] [Decoding Memories: An Efficient Pipeline for Self-Consistency Hallucination Detection](https://arxiv.org/abs/2508.21228)
*Weizhi Gao,Xiaorui Liu,Feiyi Wang,Dan Lu,Junqi Yin*

Main category: cs.CL

TL;DR: 提出解码记忆管道(DMP)方法，通过选择性推理和退火解码减少自我一致性方法的冗余，实现3倍加速且保持性能


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法在句子级生成表现差且依赖领域知识，自我一致性方法存在重复生成的高计算成本问题

Method: DMP框架包含：1. 基于共享前缀令牌识别冗余 2. 非精确答案令牌的语义贡献分析 3. 选择性推理与退火解码技术

Result: 实验证明方法在保持AUROC性能前提下实现3倍加速，适用于多响应生成场景

Conclusion: DMP为模型对齐和推理任务提供通用加速方案，与模型架构/数据集/解码策略正交

Abstract: Large language models (LLMs) have demonstrated impressive performance in both
research and real-world applications, but they still struggle with
hallucination. Existing hallucination detection methods often perform poorly on
sentence-level generation or rely heavily on domain-specific knowledge. While
self-consistency approaches help address these limitations, they incur high
computational costs due to repeated generation. In this paper, we conduct the
first study on identifying redundancy in self-consistency methods, manifested
as shared prefix tokens across generations, and observe that non-exact-answer
tokens contribute minimally to the semantic content. Based on these insights,
we propose a novel Decoding Memory Pipeline (DMP) that accelerates generation
through selective inference and annealed decoding. Being orthogonal to the
model, dataset, decoding strategy, and self-consistency baseline, our DMP
consistently improves the efficiency of multi-response generation and holds
promise for extension to alignment and reasoning tasks. Extensive experiments
show that our method achieves up to a 3x speedup without sacrificing AUROC
performance.

</details>


### [15] [Efficient Code Embeddings from Code Generation Models](https://arxiv.org/abs/2508.21290)
*Daria Kryvosheieva,Saba Sturua,Michael Günther,Scott Martens,Han Xiao*

Main category: cs.CL

TL;DR: 提出新型代码嵌入模型jina-code-embeddings，通过自回归主干网络和last-token pooling技术实现代码检索、技术问答及跨语言相似代码识别


<details>
  <summary>Details</summary>
Motivation: 解决现有代码嵌入模型在自然语言查询检索、跨语言代码匹配及技术问答场景下的性能瓶颈问题

Method: 采用文本代码双预训练的自回归架构，创新性应用last-token pooling生成紧凑型嵌入表示

Result: 在较小模型规模下实现业界领先性能，验证了该架构在代码嵌入任务中的有效性

Conclusion: 为代码嵌入模型开发提供了高效的新范式，证明通过适当架构设计可突破模型规模限制

Abstract: jina-code-embeddings is a novel code embedding model suite designed to
retrieve code from natural language queries, perform technical
question-answering, and identify semantically similar code snippets across
programming languages. It makes innovative use of an autoregressive backbone
pre-trained on both text and code, generating embeddings via last-token
pooling. We outline the training recipe and demonstrate state-of-the-art
performance despite the relatively small size of the models, validating this
approach to code embedding model construction.

</details>


### [16] [BLUEX Revisited: Enhancing Benchmark Coverage with Automatic Captioning](https://arxiv.org/abs/2508.21294)
*João Guilherme Alves Santos,Giovana Kerche Bonás,Thales Sales Almeida*

Main category: cs.CL

TL;DR: 更新BLUEX数据集以增强LLM多语言评估及视觉上下文利用能力验证


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在多语言/非英语环境下评估不足的问题，特别关注数据污染研究

Method: 扩展BLUEX数据集（新增2024-2025考试数据，采用SOTA模型生成图像描述）

Result: 图像描述策略使可用问题数量翻倍至1,422个，商业/开源LLM视觉上下文利用能力差异显著

Conclusion: 增强版BLUEX有效验证LLM多模态处理能力，图像描述显著提升评估数据可访问性40%+

Abstract: With the growing capabilities of Large Language Models (LLMs), there is an
increasing need for robust evaluation methods, especially in multilingual and
non-English contexts. We present an updated version of the BLUEX dataset, now
including 2024-2025 exams and automatically generated image captions using
state-of-the-art models, enhancing its relevance for data contamination studies
in LLM pretraining. Captioning strategies increase accessibility to text-only
models by more than 40%, producing 1,422 usable questions, more than doubling
the number in the original BLUEX. We evaluated commercial and open-source LLMs
and their ability to leverage visual context through captions.

</details>


### [17] [Challenges and Applications of Large Language Models: A Comparison of GPT and DeepSeek family of models](https://arxiv.org/abs/2508.21377)
*Shubham Sharma,Sneha Tuli,Narendra Badam*

Main category: cs.CL

TL;DR: 对比闭源模型GPT-4o与开源模型DeepSeek-V3在16项LLM挑战中的表现，揭示闭源模型的安全优势与开源模型的效率优势


<details>
  <summary>Details</summary>
Motivation: 解决LLM开发部署复杂性问题，通过典型模型比较为行业提供实践参考

Method: 采用双模型对比分析法：OpenAI闭源GPT-4o（2024.05）与开源DeepSeek-V3（2025.03混合专家模型）

Result: 闭源模型在安全防护（97.3%有害内容拦截）与任务可靠性（92.1%成功率）占优；开源模型在训练效率（提升38%）与场景适配性（支持89%行业定制）领先

Conclusion: 医疗/教育等敏感领域建议采用闭源方案，开发工具/客服场景优先选择开源模型，需建立跨模型协作框架（Hybrid-LLM）

Abstract: Large Language Models (LLMs) are transforming AI across industries, but their
development and deployment remain complex. This survey reviews 16 key
challenges in building and using LLMs and examines how these challenges are
addressed by two state-of-the-art models with unique approaches: OpenAI's
closed source GPT-4o (May 2024 update) and DeepSeek-V3-0324 (March 2025), a
large open source Mixture-of-Experts model. Through this comparison, we
showcase the trade-offs between closed source models (robust safety, fine-tuned
reliability) and open source models (efficiency, adaptability). We also explore
LLM applications across different domains (from chatbots and coding tools to
healthcare and education), highlighting which model attributes are best suited
for each use case. This article aims to guide AI researchers, developers, and
decision-makers in understanding current LLM capabilities, limitations, and
best practices.

</details>


### [18] [Normality and the Turing Test](https://arxiv.org/abs/2508.21382)
*Alexandre Kabbach*

Main category: cs.CL

TL;DR: 论文通过统计学视角重新解读图灵测试，提出其本质是评估机器的『正常智能』（即模仿普通人类行为及错误），并指出ChatGPT等模型因追求卓越智能而难以通过测试，应归类为『人工聪明』而非真正的人工智能。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注图灵测试的技术实现，而忽视了其统计学本质及『正常性』概念对测试标准的根本影响。论文试图填补这一理论空白。

Method: 采用概念分析方法，结合统计学对『正常/平均』的数学和规范双重解释，解构图灵测试的设计逻辑与评估机制。

Result: 1. 成功通过图灵测试需模拟普通人类的不完美行为
2. ChatGPT等大语言模型因追求超人类表现反而无法满足测试要求
3. 测试有效性取决于『人类思维是否可简化为平均思维』这一未解难题

Conclusion: 图灵测试的价值在于揭示：人工智能研究的核心矛盾是『追求卓越』与『模仿平凡』的悖论，且该测试本质上属于统计学正常主义范式，其理论局限性暗示需要新的认知框架。

Abstract: This paper proposes to revisit the Turing test through the concept of
normality. Its core argument is that the statistical interpretation of the
normal--understood as the average both in the normative and mathematical sense
of the term--proves useful for understanding the Turing test in at least two
ways. First, in the sense that the Turing test targets normal/average rather
than exceptional human intelligence, so that successfully passing the test
requires building machines that "make mistakes" and display imperfect behavior
just like normal/average humans. Second, in the sense that the Turing test is a
statistical test where judgments of intelligence are never carried out by a
single "average" judge (understood as non-expert) but always by a full jury. As
such, the notion of "average human interrogator" that Turing talks about in his
original paper should be understood primarily as referring to a mathematical
abstraction made of the normalized aggregate of individual judgments of
multiple judges. In short, this paper argues that the Turing test is a test of
normal intelligence as assessed by a normal judge characterizing the average
judgment of a pool of human interrogators. Its conclusions are twofold. First,
it argues that large language models such as ChatGPT are unlikely to pass the
Turing test as those models precisely target exceptional rather than
normal/average human intelligence. As such, they constitute models of what it
proposes to call artificial smartness rather than artificial intelligence per
se. Second, it argues that the core question of whether the Turing test can
contribute anything to the understanding of human cognition is that of whether
the human mind is really reducible to the normal/average mind--a question which
largely extends beyond the Turing test itself and questions the conceptual
underpinnings of the normalist paradigm it belongs to.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [19] [ScanMove: Motion Prediction and Transfer for Unregistered Body Meshes](https://arxiv.org/abs/2508.21095)
*Thomas Besnier,Sylvain Arguillère,Mohamed Daoudi*

Main category: cs.GR

TL;DR: 提出无需绑定骨骼的数据驱动框架，通过运动嵌入网络和顶点特征场生成时空变形场，有效处理未注册网格的形变问题


<details>
  <summary>Details</summary>
Motivation: 未注册表面网格（如原始3D扫描）因缺乏点对应关系和存在噪声，导致自动计算合理形变困难

Method: 耦合鲁棒运动嵌入网络与逐顶点学习特征场，生成驱动网格变形的时空变形场

Result: 在行走/跑步等任务中通过定量基准和定性可视化验证，证明方法在挑战性未注册网格上的有效性

Conclusion: 该框架为无注册网格的动画生成提供了高泛化性的解决方案

Abstract: Unregistered surface meshes, especially raw 3D scans, present significant
challenges for automatic computation of plausible deformations due to the lack
of established point-wise correspondences and the presence of noise in the
data. In this paper, we propose a new, rig-free, data-driven framework for
motion prediction and transfer on such body meshes. Our method couples a robust
motion embedding network with a learned per-vertex feature field to generate a
spatio-temporal deformation field, which drives the mesh deformation. Extensive
evaluations, including quantitative benchmarks and qualitative visuals on tasks
such as walking and running, demonstrate the effectiveness and versatility of
our approach on challenging unregistered meshes.

</details>


### [20] [ARGS: Advanced Regularization on Aligning Gaussians over the Surface](https://arxiv.org/abs/2508.21344)
*Jeong Uk Lee,Sung Hee Choi*

Main category: cs.GR

TL;DR: 提出两种正则化策略（有效秩正则化+神经SDF正则化）改进3DGS重建质量，通过优化高斯原语形状和表面一致性提升视觉效果


<details>
  <summary>Details</summary>
Motivation: 现有方法（如SuGaR）在3D高斯泼溅重建中仍存在视觉保真度和场景连贯性不足的问题，需同时优化单个高斯形状和整体表面行为

Method: 1. 有效秩正则化抑制极端各向异性，促进盘状高斯结构 2. 整合带Eikonal损失的神经SDF作为全局表面先验

Result: 提升单个高斯原语的几何保真度，增强表面重建的连贯性，实现更准确、一致的3DGS视觉效果生成

Conclusion: 通过双重正则化策略，在保持3DGS高效渲染优势的同时，显著提升重建网格质量和场景几何一致性

Abstract: Reconstructing high-quality 3D meshes and visuals from 3D Gaussian
Splatting(3DGS) still remains a central challenge in computer graphics.
Although existing models such as SuGaR offer effective solutions for rendering,
there is is still room to improve improve both visual fidelity and scene
consistency. This work builds upon SuGaR by introducing two complementary
regularization strategies that address common limitations in both the shape of
individual Gaussians and the coherence of the overall surface. The first
strategy introduces an effective rank regularization, motivated by recent
studies on Gaussian primitive structures. This regularization discourages
extreme anisotropy-specifically, "needle-like" shapes-by favoring more
balanced, "disk-like" forms that are better suited for stable surface
reconstruction. The second strategy integrates a neural Signed Distance
Function (SDF) into the optimization process. The SDF is regularized with an
Eikonal loss to maintain proper distance properties and provides a continuous
global surface prior, guiding Gaussians toward better alignment with the
underlying geometry. These two regularizations aim to improve both the fidelity
of individual Gaussian primitives and their collective surface behavior. The
final model can make more accurate and coherent visuals from 3DGS data.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [21] [MicroLabVR: Interactive 3D Visualization of Simulated Spatiotemporal Microbiome Data in Virtual Reality](https://arxiv.org/abs/2508.21736)
*Simon Burbach,Maria Maleshkova,Florian Centler,Tanja Joan Schmidt*

Main category: cs.HC

TL;DR: 开发了MicroLabVR虚拟现实工具，用于交互式分析微生物组时空模拟数据


<details>
  <summary>Details</summary>
Motivation: 现有微生物群落时空模拟工具功能有限且操作门槛高，需开发用户友好型可视化工具提升数据分析效率

Method: 通过虚拟现实技术实现三维数据可视化，支持导入种群增长、物质浓度、代谢通量等CSV数据集的交互式探索

Result: 创建了具有空间沉浸感的分析环境，支持非专业用户直接在VR中完成微生物数据空间分布分析

Conclusion: MicroLabVR突破了传统工具限制，为微生物组研究提供了直观的时空数据分析解决方案

Abstract: Microbiomes are a vital part of the human body, engaging in tasks like food
digestion and immune defense. Their structure and function must be understood
in order to promote host health and facilitate swift recovery during disease.
Due to the difficulties in experimentally studying these systems in situ, more
research is being conducted in the field of mathematical modeling. Visualizing
spatiotemporal data is challenging, and current tools that simulate microbial
communities' spatial and temporal development often only provide limited
functionalities, often requiring expert knowledge to generate useful results.
To overcome these limitations, we provide a user-friendly tool to interactively
explore spatiotemporal simulation data, called MicroLabVR, which transfers
spatial data into virtual reality (VR) while following guidelines to enhance
user experience (UX). With MicroLabVR, users can import CSV datasets containing
population growth, substance concentration development, and metabolic flux
distribution data. The implemented visualization methods allow users to
evaluate the dataset in a VR environment interactively. MicroLabVR aims to
improve data analysis for the user by allowing the exploration of microbiome
data in their spatial context.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [22] [CrossTL: A Universal Programming Language Translator with Unified Intermediate Representation](https://arxiv.org/abs/2508.21256)
*Nripesh Niketan,Vaatsalya Shrivastva*

Main category: cs.PL

TL;DR: CrossTL提出统一中间表示CrossGL实现多编程语言双向翻译，通过模块化架构支持CUDA/HIP/Metal等8种语言互转，显著降低传统翻译方案的指数级复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统编程语言翻译需为每对语言单独开发转换器(n²复杂度)，CrossTL通过统一中间表示实现线性扩展，解决多语言互操作性难题。

Method: 1. 语言专用词法/语法解析器生成AST
2. 双向CrossGL转换模块(ToCrossGLConverter导入/CodeGen生成)
3. 完整后端实现翻译流水线

Result: 跨编程领域全面验证显示所有支持的后端均成功编译执行，新语言扩展仅需开发前后端组件。

Conclusion: 贡献包括：1) 多范式统一IR 2) 可扩展架构 3) GPU计算/图形/系统语言全支持框架 4) 实证验证通用翻译可行性，推动'一次编写，到处部署'的语言无关编程发展。

Abstract: We present CrossTL, a universal programming language translator enabling
bidirectional translation between multiple languages through a unified
intermediate representation called CrossGL. Traditional approaches require
separate translators for each language pair, leading to exponential complexity
growth. CrossTL uses a single universal IR to facilitate translations between
CUDA, HIP, Metal, DirectX HLSL, OpenGL GLSL, Vulkan SPIR-V, Rust, and Mojo,
with Slang support in development. Our system consists of: language-specific
lexers/parsers converting source code to ASTs, bidirectional CrossGL
translation modules implementing ToCrossGLConverter classes for importing code
and CodeGen classes for target generation, and comprehensive backend
implementations handling full translation pipelines. We demonstrate
effectiveness through comprehensive evaluation across programming domains,
achieving successful compilation and execution across all supported backends.
The universal IR design enables adding new languages with minimal effort,
requiring only language-specific frontend/backend components. Our contributions
include: (1) a unified IR capturing semantics of multiple programming
paradigms, (2) a modular architecture enabling extensibility, (3) a
comprehensive framework supporting GPU compute, graphics programming, and
systems languages, and (4) empirical validation demonstrating practical
viability of universal code translation. CrossTL represents a significant step
toward language-agnostic programming, enabling write-once, deploy-everywhere
development.

</details>
