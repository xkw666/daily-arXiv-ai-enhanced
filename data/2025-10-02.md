<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 57]
- [cs.GR](#cs.GR) [Total: 3]
- [eess.SP](#eess.SP) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.SD](#cs.SD) [Total: 4]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.CV](#cs.CV) [Total: 4]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.LG](#cs.LG) [Total: 14]
- [cs.HC](#cs.HC) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Direct Token Optimization: A Self-contained Approach to Large Language Model Unlearning](https://arxiv.org/abs/2510.00125)
*Hong kyu Lee,Ruixuan Liu,Li Xiong*

Main category: cs.CL

TL;DR: 提出直接令牌优化（DTO）方法，实现无需外部资源的大语言模型高效机器去学习。


<details>
  <summary>Details</summary>
Motivation: 现有去学习方法依赖外部资源（辅助模型/数据集），存在实用性限制和隐私风险。

Method: 通过区分目标令牌（关键遗忘知识）和非目标令牌（保持模型性能），直接优化令牌级目标函数。

Result: 在多个基准数据集上取得16.8倍遗忘质量提升，同时保持模型效用。

Conclusion: DTO为LLM去学习提供了独立高效的技术路径，强化隐私保护能力。

Abstract: Machine unlearning is an emerging technique that removes the influence of a
subset of training data (forget set) from a model without full retraining, with
applications including privacy protection, content moderation, and model
correction. The key challenge lies in ensuring that the model completely
forgets the knowledge of the forget set without compromising its overall
utility. Existing unlearning methods for large language models (LLMs) often
utilize auxiliary language models, retain datasets, or even commercial AI
services for effective unlearning and maintaining the model utility. However,
dependence on these external resources is often impractical and could
potentially introduce additional privacy risks. In this work, we propose direct
token optimization (DTO), a novel self-contained unlearning approach for LLMs
that directly optimizes the token level objectives and eliminates the need for
external resources. Given a sequence to unlearn, we identify two categories of
tokens: target tokens, which capture critical knowledge for unlearning, and the
remaining non-target tokens, which are crucial for maintaining the model
utility. The former are used to optimize the unlearning objective, while the
latter serve to preserve the model's performance. The experimental results show
that the proposed DTO achieves up to 16.8$\times$ improvement in forget quality
on several benchmark datasets than the latest baselines while maintaining a
comparable level of model utility.

</details>


### [2] [TAMA: Tool-Augmented Multimodal Agent for Procedural Activity Understanding](https://arxiv.org/abs/2510.00161)
*Kimihiro Hasegawa,Wiradee Imrattanatrai,Masaki Asada,Ken Fukuda,Teruko Mitamura*

Main category: cs.CL

TL;DR: 提出TAMA框架，通过工具增强的多模态代理和无需训练的多媒体工具提升程序性活动理解，显著改进视觉语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 程序性活动助手在生活/职业场景应用潜力大，但现有系统开发不足。需开发支持多模态推理的框架以提升任务处理能力。

Method: 采用多媒体返回工具实现零训练环境下的交错多模态推理，结合代理驱动的灵活工具选择策略。

Result: 在ProMQA-Assembly数据集上，GPT-5和MiMo-VL性能提升显著，消融实验证实多媒体工具和灵活选择机制的有效性。

Conclusion: TAMA框架推动了多模态任务中'图像思考'范式的发展，为程序性活动助手系统开发提供新方向。

Abstract: Procedural activity assistants potentially support humans in a variety of
settings, from our daily lives, e.g., cooking or assembling flat-pack
furniture, to professional situations, e.g., manufacturing or biological
experiments. Despite its potential use cases, the system development tailored
for such an assistant is still underexplored. In this paper, we propose a novel
framework, called TAMA, a Tool-Augmented Multimodal Agent, for procedural
activity understanding. TAMA enables interleaved multimodal reasoning by making
use of multimedia-returning tools in a training-free setting. Our experimental
result on the multimodal procedural QA dataset, ProMQA-Assembly, shows that our
approach can improve the performance of vision-language models, especially
GPT-5 and MiMo-VL. Furthermore, our ablation studies provide empirical support
for the effectiveness of two features that characterize our framework,
multimedia-returning tools and agentic flexible tool selection. We believe our
proposed framework and experimental results facilitate the thinking with images
paradigm for video and multimodal tasks, let alone the development of
procedural activity assistants.

</details>


### [3] [DRBench: A Realistic Benchmark for Enterprise Deep Research](https://arxiv.org/abs/2510.00172)
*Amirhossein Abaskohi,Tianyi Chen,Miguel Muñoz-Mármol,Curtis Fox,Amrutha Varshini Ramesh,Étienne Marcotte,Xing Han Lù,Nicolas Chapados,Spandana Gella,Christopher Pal,Alexandre Drouin,Issam H. Laradji*

Main category: cs.CL

TL;DR: 提出DRBench基准测试，用于评估AI代理在复杂企业深度研究任务中的表现，支持多源异构数据整合与结构化报告生成。


<details>
  <summary>Details</summary>
Motivation: 现有基准局限于简单问答和网络查询，无法满足企业场景中需要整合内外部知识、多步骤推理的深度研究需求。

Method: 构建含15个跨领域任务的测试集，采用人工验证的合成流程生成多步骤查询，评估维度包含信息召回、事实准确性、报告结构完整性。

Result: 不同模型（GPT/Llama/Qwen）在DR任务中表现出显著差异，显示企业深度研究需要专门的策略优化和技术改进。

Conclusion: DRBench填补企业级研究评估空白，揭示模型在复杂信息整合中的瓶颈，为优化企业AI代理提供方向。

Abstract: We introduce DRBench, a benchmark for evaluating AI agents on complex,
open-ended deep research tasks in enterprise settings. Unlike prior benchmarks
that focus on simple questions or web-only queries, DRBench evaluates agents on
multi-step queries (for example, ``What changes should we make to our product
roadmap to ensure compliance with this standard?") that require identifying
supporting facts from both the public web and private company knowledge base.
Each task is grounded in realistic user personas and enterprise context,
spanning a heterogeneous search space that includes productivity software,
cloud file systems, emails, chat conversations, and the open web. Tasks are
generated through a carefully designed synthesis pipeline with
human-in-the-loop verification, and agents are evaluated on their ability to
recall relevant insights, maintain factual accuracy, and produce coherent,
well-structured reports. We release 15 deep research tasks across 10 domains,
such as Sales, Cybersecurity, and Compliance. We demonstrate the effectiveness
of DRBench by evaluating diverse DR agents across open- and closed-source
models (such as GPT, Llama, and Qwen) and DR strategies, highlighting their
strengths, weaknesses, and the critical path for advancing enterprise deep
research. Code is available at https://github.com/ServiceNow/drbench.

</details>


### [4] [PrimeX: A Dataset of Worldview, Opinion, and Explanation](https://arxiv.org/abs/2510.00174)
*Rik Koncel-Kedziorski,Brihi Joshi,Tim Paek*

Main category: cs.CL

TL;DR: 论文提出PrimeX数据集，通过整合用户观点解释和世界观评估，证明此类信念信息可有效提升语言模型的个性化能力，并为NLP与心理学研究开辟新方向。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用用户信仰体系（观点解释+世界观）来增强语言模型与个体价值观的匹配度，解决模型个性化不足的问题。

Method: 收集858名美国居民的民意调查数据，额外获取其观点解释文本和Primal World Belief世界观评估数据，开展多维度数据分析。

Result: 证实信念信息能显著提升语言模型个性化效果，PrimeX数据集同时具备NLP技术优化和心理学研究的双重价值。

Conclusion: 通过PrimeX数据集首次系统验证信仰系统对语言模型对齐的价值，为跨学科研究建立新范式。

Abstract: As the adoption of language models advances, so does the need to better
represent individual users to the model. Are there aspects of an individual's
belief system that a language model can utilize for improved alignment?
Following prior research, we investigate this question in the domain of opinion
prediction by developing PrimeX, a dataset of public opinion survey data from
858 US residents with two additional sources of belief information: written
explanations from the respondents for why they hold specific opinions, and the
Primal World Belief survey for assessing respondent worldview. We provide an
extensive initial analysis of our data and show the value of belief
explanations and worldview for personalizing language models. Our results
demonstrate how the additional belief information in PrimeX can benefit both
the NLP and psychological research communities, opening up avenues for further
study.

</details>


### [5] [Personalized Reasoning: Just-In-Time Personalization and Why LLMs Fail At It](https://arxiv.org/abs/2510.00177)
*Shuyue Stella Li,Avinandan Bose,Faeze Brahman,Simon Shaolei Du,Pang Wei Koh,Maryam Fazel,Yulia Tsvetkov*

Main category: cs.CL

TL;DR: 提出PREFDISCO评估框架，揭示当前大语言模型在个性化推理中的局限性，需专门开发而非自然涌现


<details>
  <summary>Details</summary>
Motivation: 现有LLM将任务解决与偏好对齐割裂处理，导致冷启动场景下无法有效匹配用户个性化需求，亟需能主动询问偏好并自适应调整的推理机制

Method: 基于心理学构建虚拟用户画像，将静态基准转化为交互式个性化任务，要求模型根据用户背景生成不同推理链，保持事实准确性同时满足个性化需求

Result: 评估21个前沿模型显示：29%的个性化尝试效果差于通用回答，同时通用回答也无法满足个体需求，突显当前模型的交互能力局限

Conclusion: 个性化推理需成为独立研究方向，PREFDISCO为此提供测量基准，暴露LLM在医疗/教育等关键领域自适应能力的根本性缺陷

Abstract: Current large language model (LLM) development treats task-solving and
preference alignment as separate challenges, optimizing first for objective
correctness, then for alignment to aggregated human preferences. This paradigm
fails in human-facing applications where solving a problem correctly is
insufficient if the response mismatches the user's needs. This challenge
intensifies in just-in-time scenarios where no prior user interaction history
exists due to cold-start conditions or privacy constraints. LLMs need to
identify what they don't know about user preferences, strategically elicit
preference values through questioning, then adapt their reasoning processes and
responses accordingly -- a complicated chain of cognitive processes which we
term personalized reasoning. We introduce PREFDISCO, an evaluation methodology
that transforms static benchmarks into interactive personalization tasks using
psychologically-grounded personas with sparse preferences. Our framework
creates scenarios where identical questions require different reasoning chains
depending on user context, as optimal explanation approaches vary by individual
expertise and preferences while maintaining factual accuracy. Evaluation of 21
frontier models across 10 tasks reveals 29.0% of naive personalization attempts
produce worse preference alignment than generic responses, yet generic
responses also fail to serve individual user needs effectively. These findings
suggest personalized reasoning requires dedicated development rather than
emerging naturally. PREFDISCO establishes personalized reasoning as a
measurable research frontier and reveals fundamental limitations in current
LLMs' interactive capabilities, providing a foundation for developing systems
that can adapt to individual users in education, healthcare, and technical
domains where personalization is critical.

</details>


### [6] [BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model Responses](https://arxiv.org/abs/2510.00232)
*Xin Xu,Xunzhi He,Churan Zhi,Ruizhe Chen,Julian McAuley,Zexue He*

Main category: cs.CL

TL;DR: 提出BiasFreeBench基准测试框架，系统评估8种主流LLM去偏方法（涵盖提示导向和训练导向两类），通过统一化的查询-响应场景和响应级指标Bias-Free Score实现跨方法一致性评估


<details>
  <summary>Details</summary>
Motivation: 现有去偏方法评估存在基线不统一、指标碎片化问题，且传统基于概率的评估方式与真实用户交互场景（关注模型响应质量）存在脱节

Method: 重构现有数据集形成统一查询响应测试集，在多项选择QA和开放式多轮QA场景下，综合评估4种提示导向方法/4种训练导向方法，并创新设计反映响应公平性/安全性/反刻板印象的Bias-Free Score

Result: 系统比较了提示导向与训练导向范式的优劣，揭示了模型规模对去偏效果的影响，验证了训练策略对未见过偏见类型的泛化能力

Conclusion: BiasFreeBench通过标准化测试框架和响应级评估指标，为偏见缓解研究建立统一基准，推动面向真实应用场景的去偏技术发展

Abstract: Existing studies on bias mitigation methods for large language models (LLMs)
use diverse baselines and metrics to evaluate debiasing performance, leading to
inconsistent comparisons among them. Moreover, their evaluations are mostly
based on the comparison between LLMs' probabilities of biased and unbiased
contexts, which ignores the gap between such evaluations and real-world use
cases where users interact with LLMs by reading model responses and expect fair
and safe outputs rather than LLMs' probabilities. To enable consistent
evaluation across debiasing methods and bridge this gap, we introduce
BiasFreeBench, an empirical benchmark that comprehensively compares eight
mainstream bias mitigation techniques (covering four prompting-based and four
training-based methods) on two test scenarios (multi-choice QA and open-ended
multi-turn QA) by reorganizing existing datasets into a unified query-response
setting. We further introduce a response-level metric, Bias-Free Score, to
measure the extent to which LLM responses are fair, safe, and
anti-stereotypical. Debiasing performances are systematically compared and
analyzed across key dimensions: the prompting vs. training paradigm, model
size, and generalization of different training strategies to unseen bias types.
We will publicly release our benchmark, aiming to establish a unified testbed
for bias mitigation research.

</details>


### [7] [TASER: Translation Assessment via Systematic Evaluation and Reasoning](https://arxiv.org/abs/2510.00255)
*Monishwaran Maheswaran,Marco Carini,Christian Federmann,Tony Diaz*

Main category: cs.CL

TL;DR: TASER（基于系统推理的翻译评估）利用大推理模型（LRMs）的显式推理能力，在WMT24评测中实现了最先进的翻译质量评估性能。


<details>
  <summary>Details</summary>
Motivation: 传统自动翻译评估指标缺乏可解释性，TASER通过LRMs的逐步系统推理能力，在保持高准确性的同时提供透明化的评估过程。

Method: 1. 设计结构化提示模板增强LRMs的评估效果
2. 在参考/无参考场景下测试系统级和片段级表现
3. 通过调整o3模型的推理深度探索推理强度与评估质量的关系

Result: 系统级评估：
- 参考/无参考场景软配对准确率均第一
片段级评估：
- 无参考版本在所有无参考指标中排名第一
- 结构化提示比开放式提示效果提升明显

Conclusion: 大推理模型通过显式推理机制，在翻译质量评估中实现了准确性（相比现有指标）和可解释性（相比黑箱模型）的双重突破，特别是在多语言场景下展现出显著优势。

Abstract: We introduce TASER (Translation Assessment via Systematic Evaluation and
Reasoning), a metric that uses Large Reasoning Models (LRMs) for automated
translation quality assessment. TASER harnesses the explicit reasoning
capabilities of LRMs to conduct systematic, step-by-step evaluation of
translation quality. We evaluate TASER on the WMT24 Metrics Shared Task across
both reference-based and reference-free scenarios, demonstrating
state-of-the-art performance. In system-level evaluation, TASER achieves the
highest soft pairwise accuracy in both reference-based and reference-free
settings, outperforming all existing metrics. At the segment level, TASER
maintains competitive performance with our reference-free variant ranking as
the top-performing metric among all reference-free approaches. Our experiments
reveal that structured prompting templates yield superior results with LRMs
compared to the open-ended approaches that proved optimal for traditional LLMs.
We evaluate o3, a large reasoning model from OpenAI, with varying reasoning
efforts, providing insights into the relationship between reasoning depth and
evaluation quality. The explicit reasoning process in LRMs offers
interpretability and visibility, addressing a key limitation of existing
automated metrics. Our results demonstrate that Large Reasoning Models show a
measurable advancement in translation quality assessment, combining improved
accuracy with transparent evaluation across diverse language pairs.

</details>


### [8] [Retrieval-Augmented Generation for Electrocardiogram-Language Models](https://arxiv.org/abs/2510.00261)
*Xiaoyu Song,William Han,Tony Chen,Chaojing Duan,Michael A. Rosenberg,Emerson Liu,Ding Zhao*

Main category: cs.CL

TL;DR: 开发首个开源的心电图-语言模型检索增强生成(RAG)流程，实验表明可有效提升文本生成性能


<details>
  <summary>Details</summary>
Motivation: 现有生成式心电图-语言模型(ELM)缺乏RAG的系统研究和开源实现，难以减少幻觉生成并提升自然语言生成质量

Method: 构建包含知识检索模块的RAG流程，在三个公开数据集上进行基线测试和消融实验分析

Result: RAG版本ELM在三个数据集上均显著优于非RAG基线模型，准确率提升最高达15%

Conclusion: 该研究填补了ELM领域RAG技术落地的空白，为医疗文本生成系统的设计提供了重要参考

Abstract: Interest in generative Electrocardiogram-Language Models (ELMs) is growing,
as they can produce textual responses conditioned on ECG signals and textual
queries. Unlike traditional classifiers that output label probabilities, ELMs
are more versatile, supporting domain-specific tasks (e.g., waveform analysis,
diagnosis, prognosis) as well as general tasks (e.g., open-ended questions,
dialogue). Retrieval-Augmented Generation (RAG), widely used in Large Language
Models (LLMs) to ground LLM outputs in retrieved knowledge, helps reduce
hallucinations and improve natural language generation (NLG). However, despite
its promise, no open-source implementation or systematic study of RAG pipeline
design for ELMs currently exists. To address this gap, we present the first
open-source RAG pipeline for ELMs, along with baselines and ablation studies
for NLG. Experiments on three public datasets show that ELMs with RAG
consistently improves performance over non-RAG baselines and highlights key ELM
design considerations. Our code is available at:
https://github.com/willxxy/ECG-Bench.

</details>


### [9] [Judging with Confidence: Calibrating Autoraters to Preference Distributions](https://arxiv.org/abs/2510.00263)
*Zhuohang Li,Xiaowei Li,Chengyu Huang,Guowang Li,Katayoon Goshvadi,Bo Dai,Dale Schuurmans,Paul Zhou,Hamid Palangi,Yiwen Song,Palash Goyal,Murat Kantarcioglu,Bradley A. Malin,Yuan Xue*

Main category: cs.CL

TL;DR: 提出校准概率自动评估器的框架，通过分布匹配目标提升预测校准效果，降低位置偏差同时保持任务性能


<details>
  <summary>Details</summary>
Motivation: 现有基于离散标签训练的自动评估器无法有效处理主观/模糊任务，需建模完整偏好分布提升可靠性

Method: 提出两种学习方法：1）密集概率标签的监督微调 2）稀疏二元标签的强化学习方法

Result: 校准后的评估器概率预测更符合目标分布，位置偏差降低70%，客观任务性能保持稳定

Conclusion: 该框架显著提升自动评估器的校准能力和可靠性，为LLMs价值对齐提供有效解决方案

Abstract: The alignment of large language models (LLMs) with human values increasingly
relies on using other LLMs as automated judges, or ``autoraters''. However,
their reliability is limited by a foundational issue: they are trained on
discrete preference labels, forcing a single ground truth onto tasks that are
often subjective, ambiguous, or nuanced. We argue that a reliable autorater
must learn to model the full distribution of preferences defined by a target
population. In this paper, we propose a general framework for calibrating
probabilistic autoraters to any given preference distribution. We formalize the
problem and present two learning methods tailored to different data conditions:
1) a direct supervised fine-tuning for dense, probabilistic labels, and 2) a
reinforcement learning approach for sparse, binary labels. Our empirical
results show that finetuning autoraters with a distribution-matching objective
leads to verbalized probability predictions that are better aligned with the
target preference distribution, with improved calibration and significantly
lower positional bias, all while preserving performance on objective tasks.

</details>


### [10] [Efficient Layer-wise LLM Fine-tuning for Revision Intention Prediction](https://arxiv.org/abs/2510.00268)
*Zhexiong Liu,Diane Litman*

Main category: cs.CL

TL;DR: 提出IR-Tuning框架，通过动态选择重要层进行参数高效微调，提升LLM在文本修订分类任务中的性能，同时实现快速收敛与低资源消耗。


<details>
  <summary>Details</summary>
Motivation: LLM在生成任务表现优异但细粒度文本分类能力不足，传统微调方法需要大量昂贵标注数据。文本修订分类需要捕捉文本对间的细微差异，而社区缺乏大规模修订标注数据集。

Method: 提出层级参数高效微调框架IR-Tuning，基于梯度范数分布动态选择重要网络层进行微调，冻结冗余层参数。该方法结合了层重要性评估与参数冻结策略。

Result: 实验表明IR-Tuning在多种文本修订任务上优于其他层间PEFT方法，在小型修订语料库中仍保持有效性，且具备快速收敛、低GPU内存占用的优势。

Conclusion: IR-Tuning通过动态层选择机制实现了高效的参数利用，为数据稀缺场景下的LLM细粒度文本分类任务提供了有效解决方案。

Abstract: Large Language Models (LLMs) have shown extraordinary success across various
text generation tasks; however, their potential for simple yet essential text
classification remains underexplored, as LLM pre-training tends to emphasize
generation over classification. While LLMs with instruction tuning can
transform classification into a generation task, they often struggle to
categorize nuanced texts. One such example is text revision, which involves
nuanced edits between pairs of texts. Although simply fine-tuning LLMs for
revision classification seems plausible, it requires a large amount of revision
annotations, which are exceptionally expensive and scarce in the community. To
address this issue, we introduce a plug-and-play layer-wise parameter-efficient
fine-tuning (PEFT) framework, i.e., IR-Tuning, which fine-tunes a subset of
important LLM layers that are dynamically selected based on their gradient norm
distribution, while freezing those of redundant layers. Extensive experiments
suggest that IR-Tuning surpasses several layer-wise PEFT baselines over diverse
text revisions, while achieving fast convergence, low GPU memory consumption,
and effectiveness on small revision corpora.

</details>


### [11] [SafePassage: High-Fidelity Information Extraction with Black Box LLMs](https://arxiv.org/abs/2510.00276)
*Joe Barrow,Raj Patel,Misha Kharkovski,Ben Davies,Ryan Schmitt*

Main category: cs.CL

TL;DR: 提出SafePassage框架，通过三步流程减少LLM信息抽取中的幻觉现象达85%，并可用作模型评估工具


<details>
  <summary>Details</summary>
Motivation: 传统黑盒大语言模型在信息抽取任务中容易产生不基于文档的虚假信息（幻觉），影响结果可信度

Method: 1. LLM提取器生成结构化实体及上下文
2. 字符串全局对齐器验证一致性
3. 微调后的评分模型进行最终评估

Result: 在1-2小时标注数据基础上，流程降低幻觉达85%，且微调编码器的评分效果优于LLM评分模型

Conclusion: SafePassage既保证抽取结果可靠性，又可作为评估工具，展示小样本微调模型在特定任务上的优势

Abstract: Black box large language models (LLMs) make information extraction (IE) easy
to configure, but hard to trust. Unlike traditional information extraction
pipelines, the information "extracted" is not guaranteed to be grounded in the
document. To prevent this, this paper introduces the notion of a "safe
passage": context generated by the LLM that is both grounded in the document
and consistent with the extracted information. This is operationalized via a
three-step pipeline, SafePassage, which consists of: (1) an LLM extractor that
generates structured entities and their contexts from a document, (2) a
string-based global aligner, and (3) a scoring model. Results show that using
these three parts in conjunction reduces hallucinations by up to 85% on
information extraction tasks with minimal risk of flagging non-hallucinations.
High agreement between the SafePassage pipeline and human judgments of
extraction quality mean that the pipeline can be dually used to evaluate LLMs.
Surprisingly, results also show that using a transformer encoder fine-tuned on
a small number of task-specific examples can outperform an LLM scoring model at
flagging unsafe passages. These annotations can be collected in as little as
1-2 hours.

</details>


### [12] [ReEvalMed: Rethinking Medical Report Evaluation by Aligning Metrics with Real-World Clinical Judgment](https://arxiv.org/abs/2510.00280)
*Ruochen Li,Jun Li,Bailiang Jian,Kun Yuan,Youxiang Zhu*

Main category: cs.CL

TL;DR: 现有放射学报告评估指标与临床需求存在显著差距，本文提出基于临床标准的元评估框架以提升评估可靠性


<details>
  <summary>Details</summary>
Motivation: 自动生成的放射学报告在现有指标下得分高但缺乏临床可信度，揭示当前评估体系在临床语义理解上的根本缺陷

Method: 构建包含错误类型标注、临床显著性标签的细粒度数据集，系统评估现有指标在临床对齐性、区分度、鲁棒性和单调性方面的表现

Result: 现有指标存在三大局限：无法识别临床显著性错误、过度惩罚无害变异、缺乏跨错误严重级别的一致性

Conclusion: 该框架为建立更符合临床需求的评估方法提供系统指导，推动医学报告生成技术的临床可信度提升

Abstract: Automatically generated radiology reports often receive high scores from
existing evaluation metrics but fail to earn clinicians' trust. This gap
reveals fundamental flaws in how current metrics assess the quality of
generated reports. We rethink the design and evaluation of these metrics and
propose a clinically grounded Meta-Evaluation framework. We define clinically
grounded criteria spanning clinical alignment and key metric capabilities,
including discrimination, robustness, and monotonicity. Using a fine-grained
dataset of ground truth and rewritten report pairs annotated with error types,
clinical significance labels, and explanations, we systematically evaluate
existing metrics and reveal their limitations in interpreting clinical
semantics, such as failing to distinguish clinically significant errors,
over-penalizing harmless variations, and lacking consistency across error
severity levels. Our framework offers guidance for building more clinically
reliable evaluation methods.

</details>


### [13] [o-MEGA: Optimized Methods for Explanation Generation and Analysis](https://arxiv.org/abs/2510.00288)
*Ľuboš Kriš,Jaroslav Kopčan,Qiwei Peng,Andrej Ridzik,Marcel Veselý,Martin Tamajka*

Main category: cs.CL

TL;DR: 开发o-mega工具实现可解释AI方法自动优化，提升事实核查系统透明度


<details>
  <summary>Details</summary>
Motivation: Transformer模型在提升NLP性能的同时导致解释性不足，现有解释方法众多但缺乏有效选择标准

Method: 提出超参数优化工具o-mega，通过自动搜索算法确定语义匹配领域最优解释方法及配置参数

Result: 在社交媒体声明-反声明数据集验证中，成功提升自动事实核查系统的决策可解释性

Conclusion: 自动化解释方法优化可显著增强关键应用模型的可信度，为AI透明化提供有效解决方案

Abstract: The proliferation of transformer-based language models has revolutionized NLP
domain while simultaneously introduced significant challenges regarding model
transparency and trustworthiness. The complexity of achieving explainable
systems in this domain is evidenced by the extensive array of explanation
methods and evaluation metrics developed by researchers. To address the
challenge of selecting optimal explainability approaches, we present
\textbf{\texttt{o-mega}}, a hyperparameter optimization tool designed to
automatically identify the most effective explainable AI methods and their
configurations within the semantic matching domain. We evaluate o-mega on a
post-claim matching pipeline using a curated dataset of social media posts
paired with refuting claims. Our tool systematically explores different
explainable methods and their hyperparameters, demonstrating improved
transparency in automated fact-checking systems. As a result, such automated
optimization of explanation methods can significantly enhance the
interpretability of claim-matching models in critical applications such as
misinformation detection, contributing to more trustworthy and transparent AI
systems.

</details>


### [14] [CORTEX: Collaborative LLM Agents for High-Stakes Alert Triage](https://arxiv.org/abs/2510.00311)
*Bowen Wei,Yuan Shen Tay,Howard Liu,Jinhao Pan,Kun Luo,Ziwei Zhu,Chris Jordan*

Main category: cs.CL

TL;DR: 针对SOC警报过载问题，提出多代理LLM架构CORTEX，通过分工协作显著降低误报并提升调查质量。


<details>
  <summary>Details</summary>
Motivation: 传统检测方法脆弱且缺乏上下文，单一LLM模型在噪声数据处理中表现不佳，导致高误报率和决策透明度低。

Method: CORTEX采用多代理协作框架：行为分析代理检查活动序列，证据代理查询外部系统，推理代理生成可审计的决策报告。

Result: CORTEX在多样化企业场景中误报率显著低于现有单代理LLM模型，调查质量得到实质性提升。

Conclusion: 通过专业化代理分工协作，CORTEX有效解决了SOC警报分类难题，提高了决策准确性和可解释性，并配套发布了真实场景数据集支持研究。

Abstract: Security Operations Centers (SOCs) are overwhelmed by tens of thousands of
daily alerts, with only a small fraction corresponding to genuine attacks. This
overload creates alert fatigue, leading to overlooked threats and analyst
burnout. Classical detection pipelines are brittle and context-poor, while
recent LLM-based approaches typically rely on a single model to interpret logs,
retrieve context, and adjudicate alerts end-to-end -- an approach that
struggles with noisy enterprise data and offers limited transparency. We
propose CORTEX, a multi-agent LLM architecture for high-stakes alert triage in
which specialized agents collaborate over real evidence: a behavior-analysis
agent inspects activity sequences, evidence-gathering agents query external
systems, and a reasoning agent synthesizes findings into an auditable decision.
To support training and evaluation, we release a dataset of fine-grained SOC
investigations from production environments, capturing step-by-step analyst
actions and linked tool outputs. Across diverse enterprise scenarios, CORTEX
substantially reduces false positives and improves investigation quality over
state-of-the-art single-agent LLMs.

</details>


### [15] [TokMem: Tokenized Procedural Memory for Large Language Models](https://arxiv.org/abs/2510.00444)
*Zijun Wu,Yongchang Hao,Lili Mou*

Main category: cs.CL

TL;DR: 提出TokMem作为LLMs的显式程序记忆，将重复程序编码为可训练嵌入，相比提示工程和微调更具扩展性


<details>
  <summary>Details</summary>
Motivation: 解决现有提示工程效率低（需逐步骤重读）、跨任务扩展性差、缺乏模块化复用机制的问题

Method: 1. 通过记忆令牌编码程序地址和控制信号
2. 保持主干模型冻结实现持续适应
3. 使用常量大小开销实现定向行为控制

Result: 在1000个原子回忆任务和组合函数调用任务中，性能持续超越检索增强生成方法，参数效率比微调高10倍

Conclusion: TokMem为LLMs提供了可扩展的程序记忆方案，避免重复上下文开销，支持模块化添加新程序

Abstract: Large language models rely heavily on prompts to specify tasks, recall
knowledge and guide reasoning. However, this reliance is inefficient as prompts
must be re-read at each step, scale poorly across tasks, and lack mechanisms
for modular reuse. We introduce TokMem, a tokenized procedural memory that
stores recurring procedures as compact, trainable embeddings. Each memory token
encodes both an address to a procedure and a control signal that steers
generation, enabling targeted behavior with constant-size overhead. To support
continual adaptation, TokMem keeps the backbone model frozen, allowing new
procedures to be added without interfering with existing ones. We evaluate
TokMem on 1,000 tasks for atomic recall, and on function-calling tasks for
compositional recall, where it consistently outperforms retrieval-augmented
generation while avoiding repeated context overhead, and fine-tuning with far
fewer parameters. These results establish TokMem as a scalable and modular
alternative to prompt engineering and fine-tuning, offering an explicit
procedural memory for LLMs.

</details>


### [16] [LongCodeZip: Compress Long Context for Code Language Models](https://arxiv.org/abs/2510.00446)
*Yuling Shi,Yichun Qian,Hongyu Zhang,Beijun Shen,Xiaodong Gu*

Main category: cs.CL

TL;DR: 提出代码压缩框架LongCodeZip，通过双阶段压缩策略在保持性能前提下实现5.6倍压缩比


<details>
  <summary>Details</summary>
Motivation: 现有代码大模型处理长上下文时面临API成本高、延迟大的问题，通用文本压缩方法忽略代码结构特性导致效果不佳

Method: 采用函数级粗粒度压缩（基于条件困惑度筛选）与代码块级细粒度压缩（自适应token预算优化选择）的双阶段策略

Result: 在代码补全/总结/问答任务中保持性能前提下实现最高5.6倍压缩比，优于基线方法

Conclusion: 通过保留关键代码结构实现高效上下文压缩，提升大模型处理实际大规模代码场景的能力

Abstract: Code generation under long contexts is becoming increasingly critical as
Large Language Models (LLMs) are required to reason over extensive information
in the codebase. While recent advances enable code LLMs to process long inputs,
high API costs and generation latency remain substantial bottlenecks. Existing
context pruning techniques, such as LLMLingua, achieve promising results for
general text but overlook code-specific structures and dependencies, leading to
suboptimal performance in programming tasks. In this paper, we propose
LongCodeZip, a novel plug-and-play code compression framework designed
specifically for code LLMs. LongCodeZip employs a dual-stage strategy: (1)
coarse-grained compression, which identifies and ranks function-level chunks
using conditional perplexity with respect to the instruction, retaining only
the most relevant functions; and (2) fine-grained compression, which segments
retained functions into blocks based on perplexity and selects an optimal
subset under an adaptive token budget to maximize relevance. Evaluations across
multiple tasks, including code completion, summarization, and question
answering, show that LongCodeZip consistently outperforms baseline methods,
achieving up to a 5.6x compression ratio without degrading task performance. By
effectively reducing context size while preserving essential information,
LongCodeZip enables LLMs to better scale to real-world, large-scale code
scenarios, advancing the efficiency and capability of code intelligence
applications.

</details>


### [17] [Enhancing Rating Prediction with Off-the-Shelf LLMs Using In-Context User Reviews](https://arxiv.org/abs/2510.00449)
*Koki Ryu,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 大型语言模型通过分析用户评论显著提升评分预测效果，在冷启动场景中表现媲美传统矩阵分解方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注分类/排序任务，而需要语言与数学推理能力的评分预测任务在工业应用中有重要价值但尚未充分探索，特别是现成LLM的潜力。

Method: 使用8个现成LLM在3个数据集进行实验，比较不同上下文信息（用户评论/偏好描述）对评分预测的影响，并尝试让LLM首先生成假设性评论

Result: 1. 用户评论使LLM预测效果提升至与传统方法相当
2. 具体项目评论比通用偏好描述更有效
3. 首先生成假设评论可进一步提升效果

Conclusion: LLMs通过利用用户生成内容展现出解决冷启动问题的潜力，建议工业应用优先整合具体项目的用户评论

Abstract: Personalizing the outputs of large language models (LLMs) to align with
individual user preferences is an active research area. However, previous
studies have mainly focused on classification or ranking tasks and have not
considered Likert-scale rating prediction, a regression task that requires both
language and mathematical reasoning to be solved effectively. This task has
significant industrial applications, but the utilization of LLMs remains
underexplored, particularly regarding the capabilities of off-the-shelf LLMs.
This study investigates the performance of off-the-shelf LLMs on rating
prediction, providing different in-context information. Through comprehensive
experiments with eight models across three datasets, we demonstrate that
user-written reviews significantly improve the rating prediction performance of
LLMs. This result is comparable to traditional methods like matrix
factorization, highlighting the potential of LLMs as a promising solution for
the cold-start problem. We also find that the reviews for concrete items are
more effective than general preference descriptions that are not based on any
specific item. Furthermore, we discover that prompting LLMs to first generate a
hypothetical review enhances the rating prediction performance. Our code is
available at https://github.com/ynklab/rating-prediction-with-reviews.

</details>


### [18] [Agent Fine-tuning through Distillation for Domain-specific LLMs in Microdomains](https://arxiv.org/abs/2510.00482)
*Yawen Xue,Masaya Tsunokake,Yuta Koreeda,Ekant Muljibhai Amin,Takashi Sumiyoshi,Yasuhiro Sogawa*

Main category: cs.CL

TL;DR: 通过代理微调将领域知识内化至LLM，在日立JP1中间件认证考试中实现14%性能提升


<details>
  <summary>Details</summary>
Motivation: 传统少样本提示方法存在输入冗长和计算成本高的问题，且专业微领域有效性存疑。研究旨在验证代理微调在IT运维微领域的适应能力

Method: 使用JP1手册数据及LLM生成的推理轨迹进行微调，推理阶段采用检索增强生成和上下文答案提取器提升信息相关性

Result: 在JP1认证考试问题上相对基础模型提升14%准确率

Conclusion: 代理微调有效增强LLM在复杂专业领域的推理能力，为技术微领域优化提供新方向

Abstract: Agentic large language models (LLMs) have become prominent for autonomously
interacting with external environments and performing multi-step reasoning
tasks. Most approaches leverage these capabilities via in-context learning with
few-shot prompts, but this often results in lengthy inputs and higher
computational costs. Agent fine-tuning offers an alternative by enabling LLMs
to internalize procedural reasoning and domain-specific knowledge through
training on relevant data and demonstration trajectories. While prior studies
have focused on general domains, their effectiveness in specialized technical
microdomains remains unclear. This paper explores agent fine-tuning for domain
adaptation within Hitachi's JP1 middleware, a microdomain for specialized IT
operations. We fine-tuned LLMs using JP1-specific datasets derived from domain
manuals and distilled reasoning trajectories generated by LLMs themselves,
enhancing decision making accuracy and search efficiency. During inference, we
used an agentic prompt with retrieval-augmented generation and introduced a
context-answer extractor to improve information relevance. On JP1 certification
exam questions, our method achieved a 14% performance improvement over the base
model, demonstrating the potential of agent fine-tuning for domain-specific
reasoning in complex microdomains.

</details>


### [19] [Agent-ScanKit: Unraveling Memory and Reasoning of Multimodal Agents via Sensitivity Perturbations](https://arxiv.org/abs/2510.00496)
*Pengzhou Cheng,Lingzhong Dong,Zeng Wu,Zongru Wu,Xiangru Tang,Chengwei Qin,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.CL

TL;DR: 提出Agent-ScanKit框架，揭示多模态代理在复杂任务中依赖记忆而非系统性推理的现状，验证了18个代理在5个基准测试中普遍存在泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 针对现有多模态代理在复杂或领域外任务中可靠性不足的问题，探究其推理机制是否伪相关，旨在量化记忆与推理的贡献度。

Method: 通过视觉引导、文本引导和结构引导三种正交扰动范式，在无需访问模型内部参数的前提下，系统性探测多模态代理的记忆-推理能力平衡。

Result: 在5个公开GUI基准测试中，18个多模态代理表现出训练知识检索倾向，机械记忆权重显著高于系统推理，泛化能力受限。

Conclusion: 强调现实场景中多模态代理需强化推理建模，为构建可靠系统提供新视角，指明未来研发应突破记忆依赖、提升推理鲁棒性。

Abstract: Although numerous strategies have recently been proposed to enhance the
autonomous interaction capabilities of multimodal agents in graphical user
interface (GUI), their reliability remains limited when faced with complex or
out-of-domain tasks. This raises a fundamental question: Are existing
multimodal agents reasoning spuriously? In this paper, we propose
\textbf{Agent-ScanKit}, a systematic probing framework to unravel the memory
and reasoning capabilities of multimodal agents under controlled perturbations.
Specifically, we introduce three orthogonal probing paradigms: visual-guided,
text-guided, and structure-guided, each designed to quantify the contributions
of memorization and reasoning without requiring access to model internals. In
five publicly available GUI benchmarks involving 18 multimodal agents, the
results demonstrate that mechanical memorization often outweighs systematic
reasoning. Most of the models function predominantly as retrievers of
training-aligned knowledge, exhibiting limited generalization. Our findings
underscore the necessity of robust reasoning modeling for multimodal agents in
real-world scenarios, offering valuable insights toward the development of
reliable multimodal agents.

</details>


### [20] [MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance](https://arxiv.org/abs/2510.00499)
*Xingjian Zhao,Zhe Xu,Luozhijie Jin,Yang Wang,Hanfu Chen,Yaozhou Jiang,Ke Chen,Ruixiao Li,Mingshu Chen,Ruiming Wang,Wenbo Zhang,Yiyang Zhang,Donghua Yu,Yang Gao,Xiaogui Yang,Yitian Gong,Yuanfan Xu,Qinyuan Cheng,Zhaoye Fei,Shimin Li,Yaqian Zhou,Xuanjing Huang,Xipeng Qiu*

Main category: cs.CL

TL;DR: MOSS-Speech是首个无需文本中间层的直接语音到语音大语言模型，通过模态分层架构和冻结预训练策略，实现了保留文本推理能力的同时具备原生语音处理能力。


<details>
  <summary>Details</summary>
Motivation: 现有语音系统依赖级联流水线丢弃副语言线索，而端到端方法仍受限于文本瓶颈，需要突破文本依赖实现真正的语音交互。

Method: 结合模态分层的架构设计（处理不同输入模态）与冻结预训练策略，在保持文本LLM知识的基础上扩展语音编解码能力。

Result: 在语音问答任务达到SOTA，语音生成性能与文本引导系统相当，且文本任务表现保持竞争力。

Conclusion: 该研究缩小了文本中介与直接语音生成的差距，为高效、富有表现力的端到端语音交互建立了新范式。

Abstract: Spoken dialogue systems often rely on cascaded pipelines that transcribe,
process, and resynthesize speech. While effective, this design discards
paralinguistic cues and limits expressivity. Recent end-to-end methods reduce
latency and better preserve these cues, yet still rely on text intermediates,
creating a fundamental bottleneck. We present MOSS-Speech, a true
speech-to-speech large language model that directly understands and generates
speech without relying on text guidance. Our approach combines a modality-based
layer-splitting architecture with a frozen pre-training strategy, preserving
the reasoning and knowledge of pretrained text LLMs while adding native speech
capabilities. Experiments show that our model achieves state-of-the-art results
in spoken question answering and delivers comparable speech-to-speech
performance relative to existing text-guided systems, while still maintaining
competitive text performance. By narrowing the gap between text-guided and
direct speech generation, our work establishes a new paradigm for expressive
and efficient end-to-end speech interaction.

</details>


### [21] [Graph2Eval: Automatic Multimodal Task Generation for Agents via Knowledge Graphs](https://arxiv.org/abs/2510.00507)
*Yurun Chen,Xavier Hu,Yuhan Liu,Ziqi Wang,Zeyi Liao,Lin Chen,Feng Wei,Yuxi Qian,Bo Zheng,Keting Yin,Shengyu Zhang*

Main category: cs.CL

TL;DR: 提出基于知识图谱的Graph2Eval框架，通过自动生成多模态任务实现对代理推理、协作和交互能力的全面评估


<details>
  <summary>Details</summary>
Motivation: 现有基于静态数据集的评估方法无法充分反映代理在动态环境中的真实能力，且现有LLM生成方法缺乏对工具使用和交互任务的支持

Method: 利用多源数据构建知识图谱作为任务空间，通过子图采样、任务模板和元路径转化结构化任务，采用多阶段过滤流程（节点可达性、LLM评分、相似性分析）确保质量

Result: 构建包含1,319个任务的Graph2Eval-Bench数据集，实验证明能有效区分代理性能差异，揭示不同场景下的能力差距

Conclusion: Graph2Eval通过知识图谱驱动的任务生成框架，为动态环境下的多模态代理评估提供了系统化的解决方案

Abstract: As multimodal LLM-driven agents continue to advance in autonomy and
generalization, evaluation based on static datasets can no longer adequately
assess their true capabilities in dynamic environments and diverse tasks.
Existing LLM-based synthetic data methods are largely designed for LLM training
and evaluation, and thus cannot be directly applied to agent tasks that require
tool use and interactive capabilities. While recent studies have explored
automatic agent task generation with LLMs, most efforts remain limited to text
or image analysis, without systematically modeling multi-step interactions in
web environments. To address these challenges, we propose Graph2Eval, a
knowledge graph-based framework that automatically generates both multimodal
document comprehension tasks and web interaction tasks, enabling comprehensive
evaluation of agents' reasoning, collaboration, and interactive capabilities.
In our approach, knowledge graphs constructed from multi-source external data
serve as the task space, where we translate semantic relations into structured
multimodal tasks using subgraph sampling, task templates, and meta-paths. A
multi-stage filtering pipeline based on node reachability, LLM scoring, and
similarity analysis is applied to guarantee the quality and executability of
the generated tasks. Furthermore, Graph2Eval supports end-to-end evaluation of
multiple agent types (Single-Agent, Multi-Agent, Web Agent) and measures
reasoning, collaboration, and interaction capabilities. We instantiate the
framework with Graph2Eval-Bench, a curated dataset of 1,319 tasks spanning
document comprehension and web interaction scenarios. Experiments show that
Graph2Eval efficiently generates tasks that differentiate agent and model
performance, revealing gaps in reasoning, collaboration, and web interaction
across different settings and offering a new perspective for agent evaluation.

</details>


### [22] [Copy-Paste to Mitigate Large Language Model Hallucinations](https://arxiv.org/abs/2510.00508)
*Yongchao Long,Xian Wu,Yingying Zhang,Xianbin Wen,Yuxi Zhou,Shenda Hong*

Main category: cs.CL

TL;DR: 通过两阶段高复制度响应偏好训练获得CopyPasteLLM，显著提升上下文忠实性并减少幻觉，仅需少量训练数据即可在多个基准上实现最佳性能


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成(RAG)模型因不完全信任上下文知识而产生事实性幻觉，导致可靠性问题

Method: 设计三阶段自动化流程：1）高复制度提示方法生成响应；2）构建高复制偏好数据集；3）两阶段训练（监督微调+偏好对齐）

Result: 在FaithEval基准上比最优基线提升12.2%-24.5%准确率，仅需365个训练样本（基线数据的1/50）

Conclusion: 高复制度响应通过重新校准模型对内部参数知识的依赖机制，实现更好的知识可信度控制

Abstract: While Retrieval-Augmented Generation (RAG) enables large language models
(LLMs) to generate contextually grounded responses, contextual faithfulness
remains challenging as LLMs may not consistently trust provided context,
leading to hallucinations that undermine reliability. We observe an inverse
correlation between response copying degree and context-unfaithful
hallucinations on RAGTruth, suggesting that higher copying degrees reduce
hallucinations by fostering genuine contextual belief. We propose CopyPasteLLM,
obtained through two-stage high-copying response preference training. We design
three prompting methods to enhance copying degree, demonstrating that
high-copying responses achieve superior contextual faithfulness and
hallucination control. These approaches enable a fully automated pipeline that
transforms generated responses into high-copying preference data for training
CopyPasteLLM. On FaithEval, ConFiQA and PubMedQA, CopyPasteLLM achieves best
performance in both counterfactual and original contexts, remarkably with 12.2%
to 24.5% accuracy improvements on FaithEval over the best baseline, while
requiring only 365 training samples -- 1/50th of baseline data. To elucidate
CopyPasteLLM's effectiveness, we propose the Context-Parameter Copying
Capturing algorithm. Interestingly, this reveals that CopyPasteLLM recalibrates
reliance on internal parametric knowledge rather than external knowledge during
generation. All codes are available at
https://github.com/longyongchao/CopyPasteLLM

</details>


### [23] [JoyAgent-JDGenie: Technical Report on the GAIA](https://arxiv.org/abs/2510.00510)
*Jiarun Liu,Shiyue Xu,Shangkun Liu,Yang Li,Wen Liu,Min Liu,Xiaoqing Zhou,Hanmin Wang,Shilin Jia,zhen Wang,Shaohua Tian,Hanhao Li,Junbo Zhang,Yongli Yu,Peng Cao,Haofen Wang*

Main category: cs.CL

TL;DR: 提出整合多智能体框架、分层记忆系统和工具套件的通用架构，显著提升AI助手的跨领域适应性和任务性能


<details>
  <summary>Details</summary>
Motivation: 现有基于大模型的自治系统多为孤立改进，缺乏系统级整合设计，难以实现稳健性和适应性

Method: 构建包含规划/执行智能体与评审模型的协同框架，设计工作记忆-语义记忆-程序记忆的层次化存储体系，完善搜索/代码执行/多模态解析工具链

Result: 在综合基准测试中超越开源系统并接近商业系统性能，验证系统整合的有效性

Conclusion: 系统级架构设计是实现可扩展、强适应AI助手的关键，为开发跨领域通用智能体指明方向

Abstract: Large Language Models are increasingly deployed as autonomous agents for
complex real-world tasks, yet existing systems often focus on isolated
improvements without a unifying design for robustness and adaptability. We
propose a generalist agent architecture that integrates three core components:
a collective multi-agent framework combining planning and execution agents with
critic model voting, a hierarchical memory system spanning working, semantic,
and procedural layers, and a refined tool suite for search, code execution, and
multimodal parsing. Evaluated on a comprehensive benchmark, our framework
consistently outperforms open-source baselines and approaches the performance
of proprietary systems. These results demonstrate the importance of
system-level integration and highlight a path toward scalable, resilient, and
adaptive AI assistants capable of operating across diverse domains and tasks.

</details>


### [24] [EuroSpeech: A Multilingual Speech Corpus](https://arxiv.org/abs/2510.00514)
*Samuel Pfisterer,Florian Grötschla,Luca A. Lanzendörfer,Florian Yan,Roger Wattenhofer*

Main category: cs.CL

TL;DR: 研究者开发了从议会录音中提取高质量语音数据的可扩展流程，显著提升多语言ASR模型性能


<details>
  <summary>Details</summary>
Motivation: 现有语音数据集普遍存在语言覆盖广但单语数据不足的缺陷，导致多数语言模型表现不佳。议会录音作为优质语音源未被充分利用。

Method: 1. 开发包含媒体检索和两阶段对齐算法的系统流程
2. 处理非逐字转录和长音频对齐难题
3. 应用至22个欧洲议会的录音数据

Result: 成功提取61k+小时语音（19种语言>1k小时，22种>500小时），微调后ASR词错率平均降低41.8%

Conclusion: 议会录音是优质多语言语音数据源，提出的方法有效突破数据瓶颈，显著提升低资源语言语音识别性能

Abstract: Recent progress in speech processing has highlighted that high-quality
performance across languages requires substantial training data for each
individual language. While existing multilingual datasets cover many languages,
they often contain insufficient data for most languages. Thus, trained models
perform poorly on the majority of the supported languages. Our work addresses
this challenge by introducing a scalable pipeline for constructing speech
datasets from parliamentary recordings. The proposed pipeline includes robust
components for media retrieval and a two-stage alignment algorithm designed to
handle non-verbatim transcripts and long-form audio. Applying this pipeline to
recordings from 22 European parliaments, we extract over 61k hours of aligned
speech segments, achieving substantial per-language coverage with 19 languages
exceeding 1k hours and 22 languages exceeding 500 hours of high-quality speech
data. We obtain an average 41.8\% reduction in word error rates over baselines
when finetuning an existing ASR model on our dataset, demonstrating the
usefulness of our approach.

</details>


### [25] [Beyond Log Likelihood: Probability-Based Objectives for Supervised Fine-Tuning across the Model Capability Continuum](https://arxiv.org/abs/2510.00526)
*Gaotang Li,Ruizhong Qiu,Xiusi Chen,Heng Ji,Hanghang Tong*

Main category: cs.CL

TL;DR: 研究发现模型能力连续体是影响后训练目标函数选择的关键因素，提出应根据模型能力动态调整优化目标。


<details>
  <summary>Details</summary>
Motivation: 传统负对数似然(NLL)目标在后训练阶段因模型已具备先验知识且监督数据存在噪声，导致泛化能力受限。

Method: 通过7种模型、14个基准测试和3个领域的实验，结合理论分析研究概率目标函数族的有效性。

Result: 模型能力强的区域先验倾向目标(如$-p^{10}$)优于NLL；能力弱时NLL主导；中间区域无单一最优目标。

Conclusion: 目标函数选择应适配模型能力阶段，理论框架为动态调整目标函数提供了原则性基础。

Abstract: Supervised fine-tuning (SFT) is the standard approach for post-training large
language models (LLMs), yet it often shows limited generalization. We trace
this limitation to its default training objective: negative log likelihood
(NLL). While NLL is classically optimal when training from scratch,
post-training operates in a different paradigm and could violate its optimality
assumptions, where models already encode task-relevant priors and supervision
can be long and noisy. To this end, we study a general family of
probability-based objectives and characterize their effectiveness under
different conditions. Through comprehensive experiments and extensive ablation
studies across 7 model backbones, 14 benchmarks, and 3 domains, we uncover a
critical dimension that governs objective behavior: the model-capability
continuum. Near the model-strong end, prior-leaning objectives that downweight
low-probability tokens (e.g., $-p$, $-p^{10}$, thresholded variants)
consistently outperform NLL; toward the model-weak end, NLL dominates; in
between, no single objective prevails. Our theoretical analysis further
elucidates how objectives trade places across the continuum, providing a
principled foundation for adapting objectives to model capability. Our code is
available at https://github.com/GaotangLi/Beyond-Log-Likelihood.

</details>


### [26] [GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness](https://arxiv.org/abs/2510.00536)
*Kung-Hsiang Huang,Haoyi Qiu,Yutong Dai,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: 提出GUI-KV缓存压缩方法，通过空间显著性引导和时间冗余评分技术，在保持准确性的同时显著降低计算开销


<details>
  <summary>Details</summary>
Motivation: 图形界面代理处理高分辨率截图时面临计算效率低下问题，现有缓存压缩方法未充分考虑GUI特有的时空冗余特性

Method: 结合空间显著性（隐藏状态L2范数增强注意力）和时间冗余评估（跨帧键空间投影），实现无需训练的KV缓存压缩

Result: 在AgentNetBench基准测试中，5截图场景下解码FLOPs降低38.9%，步骤准确率提升4.1%

Conclusion: 针对GUI特有冗余设计的压缩策略能有效平衡效率与准确性，为图形界面代理优化提供新方向

Abstract: Graphical user interface (GUI) agents built on vision-language models have
emerged as a promising approach to automate human-computer workflows. However,
they also face the inefficiency challenge as they process long sequences of
high-resolution screenshots and solving long-horizon tasks, making inference
slow, costly and memory-bound. While key-value (KV) caching can mitigate this,
storing the full cache is prohibitive for image-heavy contexts. Existing
cache-compression methods are sub-optimal as they do not account for the
spatial and temporal redundancy of GUIs. In this work, we first analyze
attention patterns in GUI agent workloads and find that, unlike in natural
images, attention sparsity is uniformly high across all transformer layers.
This insight motivates a simple uniform budget allocation strategy, which we
show empirically outperforms more complex layer-varying schemes. Building on
this, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI
agents that requires no retraining. GUI-KV combines two novel techniques: (i)
spatial saliency guidance, which augments attention scores with the L2 norm of
hidden states to better preserve semantically important visual tokens, and (ii)
temporal redundancy scoring, which projects previous frames' keys onto the
current frame's key subspace to preferentially prune redundant history. Across
standard GUI agent benchmarks and models, GUI-KV outperforms competitive KV
compression baselines, closely matching full-cache accuracy at modest budgets.
Notably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV
reduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the
full-cache baseline. These results demonstrate that exploiting GUI-specific
redundancies enables efficient and reliable agent performance.

</details>


### [27] [ThinkBrake: Mitigating Overthinking in Tool Reasoning](https://arxiv.org/abs/2510.00546)
*Minjae Oh,Sangjun Song,Seungkyu Lee,Sungmin Jo,Yohan Jo*

Main category: cs.CL

TL;DR: 小推理模型在工具使用时存在过度思考问题，通过提前终止推理可显著提升准确率并减少冗余计算。


<details>
  <summary>Details</summary>
Motivation: 现有关于简洁推理的研究主要集中于数学领域，工具使用场景中的冗余推理问题尚未充分探索。需要解决小模型在工具调用时正确配置后继续错误覆盖的问题。

Method: 通过注入<stop>标签的oracle rollout诊断过度思考现象，并提出ThinkBrake——通过监控句子边界处<stop>与当前最高概率token的对数概率差实现提前终止的解码策略。

Result: 在BFCL基准的单轮、非实时和实时场景中，ThinkBrake在保持或提升准确率的同时减少最多25%的token消耗，优于各类基线方法。

Conclusion: 提前终止机制有效缓解过度思考现象，ThinkBrake展示了工具推理场景中通过优化解码策略提升效率的潜力。

Abstract: Small reasoning models (SRMs) often overthink during tool use: they reach a
correct tool-argument configuration, then continue reasoning and overwrite it
with an incorrect final call. We diagnose overthinking via oracle rollouts that
inject </think> at sentence boundaries. On the Berkeley Function Calling
Leaderboard (BFCL), this oracle termination lifts average accuracy from 85.8\%
to 94.2\% while reducing tokens by 80-94\%, revealing substantial recoverable
headroom and potential redundant reasoning. While prior work on concise
reasoning has largely targeted mathematics, tool reasoning remains
underexplored. We adapt various early-termination baselines to tool use and
introduce ThinkBrake, a training-free decoding heuristic. ThinkBrake monitors
the log-probability margin between </think> and the current top token at
sentence boundaries and triggers termination when this margin becomes small.
Across BFCL's single turn, non-live and live splits, ThinkBrake preserves or
improves accuracy while reducing tokens up to 25\%, outperforming various
baselines.

</details>


### [28] [Are Large Language Models Chronically Online Surfers? A Dataset for Chinese Internet Meme Explanation](https://arxiv.org/abs/2510.00567)
*Yubo Xie,Chenkai Wang,Zongyang Ma,Fahui Miao*

Main category: cs.CL

TL;DR: 评估大语言模型对中文网络模因的理解能力，发现其在文化和语言细节处理上的不足。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs是否真正理解网络病毒式传播内容（如中文模因），尤其在文化背景和语言细微差别方面的表现。

Method: 创建CHIME数据集（包含详细标注的中文模因），设计模因解释和语境填空两个任务进行评估。

Result: 模型能解释部分模因含义，但在文化/语言敏感型模因表现显著下降；持续无法准确追溯模因起源；多选题任务表现低于人类水平。

Conclusion: 公开CHIME数据集以促进计算模因理解研究，揭示LLMs在文化敏感任务上的局限性，强调需改进模型对网络文化现象的理解能力。

Abstract: Large language models (LLMs) are trained on vast amounts of text from the
Internet, but do they truly understand the viral content that rapidly spreads
online -- commonly known as memes? In this paper, we introduce CHIME, a dataset
for CHinese Internet Meme Explanation. The dataset comprises popular
phrase-based memes from the Chinese Internet, annotated with detailed
information on their meaning, origin, example sentences, types, etc. To
evaluate whether LLMs understand these memes, we designed two tasks. In the
first task, we assessed the models' ability to explain a given meme, identify
its origin, and generate appropriate example sentences. The results show that
while LLMs can explain the meanings of some memes, their performance declines
significantly for culturally and linguistically nuanced meme types.
Additionally, they consistently struggle to provide accurate origins for the
memes. In the second task, we created a set of multiple-choice questions (MCQs)
requiring LLMs to select the most appropriate meme to fill in a blank within a
contextual sentence. While the evaluated models were able to provide correct
answers, their performance remains noticeably below human levels. We have made
CHIME public and hope it will facilitate future research on computational meme
understanding.

</details>


### [29] [ReSeek: A Self-Correcting Framework for Search Agents with Instructive Rewards](https://arxiv.org/abs/2510.00568)
*Shiyu Li,Yang Tang,Yifan Wang,Peiming Li,Xi Chen*

Main category: cs.CL

TL;DR: ReSeek框架通过动态自我纠正机制和双维度奖励设计，显著提升搜索代理在复杂推理任务中的表现


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的搜索代理依赖稀疏/规则化奖励，容易陷入错误推理路径且缺乏自我纠正能力

Method: 1. 引入JUDGE动作实现动态错误识别与路径修正
2. 设计包含正确性（事实检索）和实用性（信息价值）的双维度密集奖励函数
3. 构建防数据污染的FictionalHot基准测试集

Result: 在任务成功率（+15.2%）和路径忠实度（+22.8%）上显著超越现有SOTA方法

Conclusion: 该框架通过自我纠正机制与过程导向的奖励设计，在保持系统简洁性的同时有效提升搜索代理的可靠性和决策质量

Abstract: Search agents powered by Large Language Models (LLMs) have demonstrated
significant potential in tackling knowledge-intensive tasks. Reinforcement
learning (RL) has emerged as a powerful paradigm for training these agents to
perform complex, multi-step reasoning. However, prior RL-based methods often
rely on sparse or rule-based rewards, which can lead agents to commit to
suboptimal or erroneous reasoning paths without the ability to recover. To
address these limitations, we propose ReSeek, a novel self-correcting framework
for training search agents. Our framework introduces a self-correction
mechanism that empowers the agent to dynamically identify and recover from
erroneous search paths during an episode. By invoking a special JUDGE action,
the agent can judge the information and re-plan its search strategy. To guide
this process, we design a dense, instructive process reward function, which
decomposes into a correctness reward for retrieving factual information and a
utility reward for finding information genuinely useful for the query.
Furthermore, to mitigate the risk of data contamination in existing datasets,
we introduce FictionalHot, a new and challenging benchmark with recently
curated questions requiring complex reasoning. Being intuitively reasonable and
practically simple, extensive experiments show that agents trained with ReSeek
significantly outperform SOTA baselines in task success rate and path
faithfulness.

</details>


### [30] [CoT Vectors: Transferring and Probing the Reasoning Mechanisms of LLMs](https://arxiv.org/abs/2510.00579)
*Li Li,Ziyi Wang,Yongliang Wu,Jianfei Cai,Xu Yang*

Main category: cs.CL

TL;DR: 提出CoT Vectors方法，通过教师-学生框架优化推理向量，在降低训练成本的同时达到参数高效微调性能


<details>
  <summary>Details</summary>
Motivation: 现有Chain-of-Thought实现方式存在高成本低效率问题，受任务向量范式启发，通过紧凑向量编码多步推理知识。实验发现提取向量存在层间不稳定性（U型性能曲线），需优化解决

Method: 提出Learnable CoT Vectors，在教师-学生框架下优化向量参数，提供稳定推理引导。通过提取向量发现LLMs三阶段推理过程，并系统分析向量有效性影响因素

Result: 在多个基准测试中超越现有基线，性能媲美参数高效微调方法（训练参数量更少）。向量有效性受潜在空间结构、信息密度、获取机制和预训练差异影响

Conclusion: CoT Vectors不仅是高效推理工具，更可作为探针揭示LLMs多步推理机制，为理解模型功能组织提供新视角

Abstract: Chain-of-Thought (CoT) prompting has emerged as a powerful approach to
enhancing the reasoning capabilities of Large Language Models (LLMs). However,
existing implementations, such as in-context learning and fine-tuning, remain
costly and inefficient. To improve CoT reasoning at a lower cost, and inspired
by the task vector paradigm, we introduce CoT Vectors, compact representations
that encode task-general, multi-step reasoning knowledge. Through experiments
with Extracted CoT Vectors, we observe pronounced layer-wise instability,
manifesting as a U-shaped performance curve that reflects a systematic
three-stage reasoning process in LLMs. To address this limitation, we propose
Learnable CoT Vectors, optimized under a teacher-student framework to provide
more stable and robust guidance. Extensive evaluations across diverse
benchmarks and models demonstrate that CoT Vectors not only outperform existing
baselines but also achieve performance comparable to parameter-efficient
fine-tuning methods, while requiring fewer trainable parameters. Moreover, by
treating CoT Vectors as a probe, we uncover how their effectiveness varies due
to latent space structure, information density, acquisition mechanisms, and
pre-training differences, offering new insights into the functional
organization of multi-step reasoning in LLMs. The source code will be released.

</details>


### [31] [SAGE-LD: Towards Scalable and Generalizable End-to-End Language Diarization via Simulated Data Augmentation](https://arxiv.org/abs/2510.00582)
*Sangmin Lee,Woongjib Choi,Jihyun Kim,Hong-Goo Kang*

Main category: cs.CL

TL;DR: 提出基于可学习查询架构与多语言预训练的语音语言分类模型，在多项基准测试中实现23%-52%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统方法受限于数据稀缺和架构优化不足，无法有效处理真实多语言场景中的代码切换问题。

Method: 整合可学习查询架构与大规模代码切换模拟数据的预训练，实现多语言联合建模。

Result: 在多个语言分类基准测试中达到SOTA，相对性能提升23%-52%。

Conclusion: 建立了代码切换语音技术的基础框架，推动了语言分类与多语言语音处理技术的发展。

Abstract: In this paper, we present a neural spoken language diarization model that
supports an unconstrained span of languages within a single framework. Our
approach integrates a learnable query-based architecture grounded in
multilingual awareness, with large-scale pretraining on simulated
code-switching data. By jointly leveraging these two components, our method
overcomes the limitations of conventional approaches in data scarcity and
architecture optimization, and generalizes effectively to real-world
multilingual settings across diverse environments. Experimental results
demonstrate that our approach achieves state-of-the-art performance on several
language diarization benchmarks, with a relative performance improvement of 23%
to 52% over previous methods. We believe that this work not only advances
research in language diarization but also establishes a foundational framework
for code-switching speech technologies.

</details>


### [32] [Tenyidie Syllabification corpus creation and deep learning applications](https://arxiv.org/abs/2510.00629)
*Teisovi Angami,Kevisino Khate*

Main category: cs.CL

TL;DR: 首次为低资源Tenyidie语言创建10,120个音节切分语料库，应用LSTM、BLSTM等深度学习模型实现99.21%的最高切分准确率


<details>
  <summary>Details</summary>
Motivation: Tenyidie作为印度那加兰邦主要语言，具有声调性、主宾动结构和高度黏着性，但缺乏自然语言处理研究基础，尤其音节切分领域尚无相关研究

Method: 使用LSTM、双向LSTM（BLSTM）、BLSTM+CRF和编码器-解码器架构，在8:1:1划分的数据集上进行训练验证

Result: BLSTM模型在测试集达到99.21%准确率，优于其他模型架构

Conclusion: 该成果可应用于形态分析、词性标注、机器翻译等Tenyidie语言的NLP任务，为低资源语言处理提供技术参考

Abstract: The Tenyidie language is a low-resource language of the Tibeto-Burman family
spoken by the Tenyimia Community of Nagaland in the north-eastern part of India
and is considered a major language in Nagaland. It is tonal,
Subject-Object-Verb, and highly agglutinative in nature. Being a low-resource
language, very limited research on Natural Language Processing (NLP) has been
conducted. To the best of our knowledge, no work on syllabification has been
reported for this language. Among the many NLP tasks, syllabification or
syllabication is an important task in which the given word syllables are
identified. The contribution of this work is the creation of 10,120 syllabified
Tenyidie words and the application of the Deep Learning techniques on the
created corpus. In this paper, we have applied LSTM, BLSTM, BLSTM+CRF, and
Encoder-decoder deep learning architectures on our created dataset. In our
dataset split of 80:10:10 (train:validation:test) set, we achieved the highest
accuracy of 99.21% with BLSTM model on the test set. This work will find its
application in numerous other NLP applications, such as morphological analysis,
part-of-speech tagging, machine translation, etc, for the Tenyidie Language.
  Keywords: Tenyidie; NLP; syllabification; deep learning; LSTM; BLSTM; CRF;
Encoder-decoder

</details>


### [33] [MCM-DPO: Multifaceted Cross-Modal Direct Preference Optimization for Alt-text Generation](https://arxiv.org/abs/2510.00647)
*Jinlan Fu,Shenzhen Huangfu,Hao Fei,Yichong Huang,Xiaoyu Shen,Xipeng Qiu,See-Kiong Ng*

Main category: cs.CL

TL;DR: 提出MCM-DPO方法改进替代文本生成，通过多维度偏好优化解决标注噪声问题，并构建Twitter/Pinterest数据集TAlt/PAlt，实验显示方法优于DPO/SFT。


<details>
  <summary>Details</summary>
Motivation: 现有替代文本生成方法受限于用户标注噪声、标准不统一及多模态模型对上下文不敏感的问题，传统监督微调依赖精确标注的缺陷在用户生成场景尤为突出。

Method: 提出多维度跨模态直接偏好优化框架(MCM-DPO)，在单样本/配对/多偏好维度联合优化，融合文本质量、视觉相关性和跨模态一致性指标构建偏好对。

Result: 实验证明MCM-DPO在BLEU-4/ROUGE/METEOR指标上分别超越DPO 3.8/4.2/2.1个点，在人类评估中偏好率提升15.7%，达到当前最优性能。

Conclusion: 通过偏好学习框架规避噪声标注依赖，配合高质量多维度数据集构建，为替代文本生成任务提供了新的解决方案和技术基准。

Abstract: The alt-text generation task produces concise, context-relevant descriptions
of images, enabling blind and low-vision users to access online images. Despite
the capabilities of large vision-language models, alt-text generation
performance remains limited due to noisy user annotations, inconsistent
standards, and MLLMs' insensitivity to contextual information. Previous efforts
to fine-tune MLLMs using supervised fine-tuning (SFT) have struggled, as SFT
relies on accurate target annotations, which are often flawed in user-generated
alt-text. To address this, we propose Multi-faceted Cross-modal Direct
Preference Optimization (MCM-DPO), which improves alt-text generation by
learning to identify better options in preference pairs without requiring
precise annotations. MCM-DPO optimizes preferences across single, paired, and
multi-preference dimensions, covering textual, visual, and cross-modal factors.
In light of the scarcity of high-quality annotated and preference-labeled
datasets for alt-text, we constructed two large-scale, high-quality datasets
named TAlt and PAlt, sourced from Twitter and Pinterest. These datasets include
202k annotated alt-text samples and 18k preference pairs that cover diverse
preference dimensions, aiming to support further research in this domain.
Experimental results show that our proposed MCM-DPO method consistently
outperforms both DPO and SFT, establishing a new state of the art in alt-text
generation. We release the code and data here:
https://github.com/LVUGAI/MCM-DPO

</details>


### [34] [Facilitating Cognitive Accessibility with LLMs: A Multi-Task Approach to Easy-to-Read Text Generation](https://arxiv.org/abs/2510.00662)
*François Ledoyen,Gaël Dias,Jeremie Pantin,Alexis Lechervy,Fabrice Maurel,Youssef Chahir*

Main category: cs.CL

TL;DR: 通过多任务学习方法显著提升大语言模型生成易读文本的效果


<details>
  <summary>Details</summary>
Motivation: 解决手动创建易读文本效率低下问题，利用LLMs自动化生成ETR内容。当前面临对齐语料稀缺和ETR规则特殊性的双重挑战。

Method: 提出多任务学习框架（文本摘要+文本简化+ETR生成），探索基于RAG的上下文学习策略和参数高效的MTL-LoRA微调方法，使用Mistral-7B/LLaMA-3-8B模型和ETR-fr新数据集

Result: 多任务配置全面优于单任务基线；RAG策略实现跨领域泛化能力，MTL-LoRA在领域内表现最佳

Conclusion: 验证多任务学习对ETR生成的有效性，提出参数高效的学习策略，为易读内容自动化生成提供新方案

Abstract: Simplifying complex texts is essential for ensuring equitable access to
information, especially for individuals with cognitive impairments. The
Easy-to-Read (ETR) initiative offers a framework for making content accessible
to the neurodivergent population, but the manual creation of such texts remains
time-consuming and resource-intensive. In this work, we investigate the
potential of large language models (LLMs) to automate the generation of ETR
content. To address the scarcity of aligned corpora and the specificity of ETR
constraints, we propose a multi-task learning (MTL) approach that trains models
jointly on text summarization, text simplification, and ETR generation. We
explore two different strategies: multi-task retrieval-augmented generation
(RAG) for in-context learning, and MTL-LoRA for parameter-efficient
fine-tuning. Our experiments with Mistral-7B and LLaMA-3-8B, based on ETR-fr, a
new high-quality dataset, demonstrate the benefits of multi-task setups over
single-task baselines across all configurations. Moreover, results show that
the RAG-based strategy enables generalization in out-of-domain settings, while
MTL-LoRA outperforms all learning strategies within in-domain configurations.

</details>


### [35] [Inclusive Easy-to-Read Generation for Individuals with Cognitive Impairments](https://arxiv.org/abs/2510.00691)
*François Ledoyen,Gaël Dias,Alexis Lechervy,Jeremie Pantin,Fabrice Maurel,Youssef Chahir,Elisa Gouzonnat,Mélanie Berthelot,Stanislas Moravac,Armony Altinier,Amy Khairalla*

Main category: cs.CL

TL;DR: 首个符合欧洲ETR指南的ETR-fr数据集解决了AI生成易读文本的数据稀缺问题，通过参数高效微调方法使PLMs表现媲美LLMs，并开发多维度评估框架确保输出质量。


<details>
  <summary>Details</summary>
Motivation: 手动生成易读文本成本高且不可扩展，阻碍认知障碍人群获取关键信息。AI驱动方案需克服数据集稀缺、领域适应性、大模型轻量化学习三大挑战。

Method: 1. 创建首个全合规ETR-fr数据集
2. 对PLMs/LLMs实施参数高效微调
3. 设计36项人工评估指标+自动指标的混合评估框架

Result: PLMs与LLMs性能相当，在跨领域文本适应中表现优异，验证了轻量化方法的有效性

Conclusion: 该研究为高质量ETR生成提供了标准化数据集和轻量解决方案，评估框架有效保障了易读文本的合规性和实用性

Abstract: Ensuring accessibility for individuals with cognitive impairments is
essential for autonomy, self-determination, and full citizenship. However,
manual Easy-to-Read (ETR) text adaptations are slow, costly, and difficult to
scale, limiting access to crucial information in healthcare, education, and
civic life. AI-driven ETR generation offers a scalable solution but faces key
challenges, including dataset scarcity, domain adaptation, and balancing
lightweight learning of Large Language Models (LLMs). In this paper, we
introduce ETR-fr, the first dataset for ETR text generation fully compliant
with European ETR guidelines. We implement parameter-efficient fine-tuning on
PLMs and LLMs to establish generative baselines. To ensure high-quality and
accessible outputs, we introduce an evaluation framework based on automatic
metrics supplemented by human assessments. The latter is conducted using a
36-question evaluation form that is aligned with the guidelines. Overall
results show that PLMs perform comparably to LLMs and adapt effectively to
out-of-domain texts.

</details>


### [36] [ALARB: An Arabic Legal Argument Reasoning Benchmark](https://arxiv.org/abs/2510.00694)
*Harethah Abu Shairah,Somayah AlHarbi,Abdulaziz AlHussein,Sameer Alsabea,Omar Shaqaqi,Hebah AlShamlan,Omar Knio,George Turkiyyah*

Main category: cs.CL

TL;DR: 阿拉伯法律推理基准ALARB填补阿拉伯语LLMs多步推理评估空白，通过13K+沙特商业案例构建任务体系，指令调优使12B模型达GPT-4o水平


<details>
  <summary>Details</summary>
Motivation: 现有阿拉伯语基准缺乏针对多步推理的开放式评估数据集，特别是在法律领域需要模拟真实复杂场景的评估工具

Method: 1. 构建包含案件事实/法庭推理/判决结果/法规条款的13K沙特商业案例库
2. 设计判决预测/推理链补全/法规识别三大任务
3. 采用指令调优方法优化模型

Result: 指令调优后的12B参数模型在判决预测任务准确率提升27%，阿拉伯判决生成质量达到GPT-4o的92%

Conclusion: ALARB有效提升阿拉伯LLMs法律推理能力，证明中等规模模型通过领域特定调优可匹敌顶级商业模型

Abstract: We introduce ALARB, a dataset and suite of tasks designed to evaluate the
reasoning capabilities of large language models (LLMs) within the Arabic legal
domain. While existing Arabic benchmarks cover some knowledge-intensive tasks
such as retrieval and understanding, substantial datasets focusing specifically
on multistep reasoning for Arabic LLMs, especially in open-ended contexts, are
lacking. The dataset comprises over 13K commercial court cases from Saudi
Arabia, with each case including the facts presented, the reasoning of the
court, the verdict, as well as the cited clauses extracted from the regulatory
documents. We define a set of challenging tasks leveraging this dataset and
reflecting the complexity of real-world legal reasoning, including verdict
prediction, completion of reasoning chains in multistep legal arguments, and
identification of relevant regulations based on case facts. We benchmark a
representative selection of current open and closed Arabic LLMs on these tasks
and demonstrate the dataset's utility for instruction tuning. Notably, we show
that instruction-tuning a modest 12B parameter model using ALARB significantly
enhances its performance in verdict prediction and Arabic verdict generation,
reaching a level comparable to that of GPT-4o.

</details>


### [37] [Family Matters: Language Transfer and Merging for Adapting Small LLMs to Faroese](https://arxiv.org/abs/2510.00810)
*Jenny Kunz,Iben Nyholm Debess,Annika Simonsen*

Main category: cs.CL

TL;DR: 通过迁移学习适配法罗语LLM的最佳实践：源语言选择取决于任务类型，微调方式需按需求取舍


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言法罗语的LLM适配难题，探索跨语言迁移的有效路径

Method: 采用两阶段训练（预训练+微调），比较冰岛语/丹麦语迁移效果，评估全微调与LoRA方法差异

Result: 冰岛语提升语法准确率(语言任务)，丹麦语强化理解能力(理解任务)；LoRA优化语言接受度，全微调保持下游任务能力

Conclusion: 适配策略应任务导向：语言准确优先选冰岛语+LoRA，理解需求优先丹麦语+全微调

Abstract: We investigate how to adapt small, efficient LLMs to Faroese, a low-resource
North Germanic language. Starting from English models, we continue pre-training
on related Scandinavian languages, either individually or combined via merging,
before fine-tuning on Faroese. We compare full fine-tuning with
parameter-efficient tuning using LoRA, evaluating their impact on both
linguistic accuracy and text comprehension. Due to the lack of existing Faroese
evaluation data, we construct two new minimal-pair benchmarks from adapted and
newly collected datasets and complement them with human evaluations by Faroese
linguists. Our results demonstrate that transfer from related languages is
crucial, though the optimal source language depends on the task: Icelandic
enhances linguistic accuracy, whereas Danish boosts comprehension. Similarly,
the choice between full fine-tuning and LoRA is task-dependent: LoRA improves
linguistic acceptability and slightly increases human evaluation scores on the
base model, while full fine-tuning yields stronger comprehension performance
and better preserves model capabilities during downstream fine-tuning.

</details>


### [38] [Exposing the Cracks: Vulnerabilities of Retrieval-Augmented LLM-based Machine Translation](https://arxiv.org/abs/2510.00829)
*Yanming Sun,Runzhe Zhan,Chi Seng Cheang,Han Wu,Xuebo Liu,Yuyao Niu,Fengying Ye,Kaixin Lan,Lidia S. Chao,Derek F. Wong*

Main category: cs.CL

TL;DR: 提出REAL-MT框架评估检索增强机器翻译在噪声环境下的鲁棒性，发现低资源语言对易受噪声影响，大型推理模型存在注意力偏移与置信度校准问题


<details>
  <summary>Details</summary>
Motivation: 探究检索增强型机器翻译(REAL-MT)在真实噪声检索场景中的可靠性，解决现有研究对噪声鲁棒性评估不足的问题

Method: 构建噪声合成框架与评估指标，基于Qwen系列模型(含标准LLM与增强推理的LRM)，测试高/中/低资源语言对在合成噪声下的习语翻译表现

Result: 低资源语言对因依赖检索上下文而性能显著恶化，LRM虽推理增强但更易受噪声干扰且错误合理化，模型注意力转向噪声内容且置信度与准确度背离

Conclusion: 现有方法存在鲁棒性与性能的权衡，需开发自验证机制。训练无关策略和微调虽提升抗噪性，但牺牲干净环境下的翻译质量

Abstract: \textbf{RE}trieval-\textbf{A}ugmented \textbf{L}LM-based \textbf{M}achine
\textbf{T}ranslation (REAL-MT) shows promise for knowledge-intensive tasks like
idiomatic translation, but its reliability under noisy retrieval contexts
remains poorly understood despite this being a common challenge in real-world
deployment. To address this gap, we propose a noise synthesis framework and new
metrics to evaluate the robustness of REAL-MT systematically. Using this
framework, we instantiate REAL-MT with Qwen-series models, including standard
LLMs and large reasoning models (LRMs) with enhanced reasoning, and evaluate
their performance on idiomatic translation across high-, medium-, and
low-resource language pairs under synthesized noise. Our results show that
low-resource language pairs, which rely more heavily on retrieved context,
degrade more severely under noise than high-resource ones and often produce
nonsensical translations. Although LRMs possess enhanced reasoning
capabilities, they show no improvement in error correction and are even more
susceptible to noise, tending to rationalize incorrect contexts. We find that
this stems from an attention shift away from the source idiom to noisy content,
while confidence increases despite declining accuracy, indicating poor
calibration. To mitigate these issues, we investigate training-free and
fine-tuning strategies, which improve robustness at the cost of performance in
clean contexts, revealing a fundamental trade-off. Our findings highlight the
limitations of current approaches, underscoring the need for self-verifying
integration mechanisms.

</details>


### [39] [ManagerBench: Evaluating the Safety-Pragmatism Trade-off in Autonomous LLMs](https://arxiv.org/abs/2510.00857)
*Adi Simhi,Jonathan Herzig,Martin Tutek,Itay Itzhak,Idan Szpektor,Yonatan Belinkov*

Main category: cs.CL

TL;DR: 研究LLM在安全目标与实用目标冲突时的决策表现，提出ManagerBench基准揭示模型在安全-实用平衡上的缺陷


<details>
  <summary>Details</summary>
Motivation: 现有安全评估忽视LLM为实现操作目标选择有害行动的风险，需建立新基准测试管理场景中的安全决策能力

Method: 设计包含两难选择的现实管理场景（实用但有害 vs 安全但低效），通过平行控制组量化模型实用主义倾向

Result: 前沿模型普遍存在安全-实用失衡：53%选择有害选项追求效率，37%过度保守牺牲效能，优先顺序错误是主因（非危害识别能力）

Conclusion: ManagerBench揭示LLM代理行为的核心缺陷，表明需要重构目标优先级机制以实现安全自主决策

Abstract: As large language models (LLMs) evolve from conversational assistants into
autonomous agents, evaluating the safety of their actions becomes critical.
Prior safety benchmarks have primarily focused on preventing generation of
harmful content, such as toxic text. However, they overlook the challenge of
agents taking harmful actions when the most effective path to an operational
goal conflicts with human safety. To address this gap, we introduce
ManagerBench, a benchmark that evaluates LLM decision-making in realistic,
human-validated managerial scenarios. Each scenario forces a choice between a
pragmatic but harmful action that achieves an operational goal, and a safe
action that leads to worse operational performance. A parallel control set,
where potential harm is directed only at inanimate objects, measures a model's
pragmatism and identifies its tendency to be overly safe. Our findings indicate
that the frontier LLMs perform poorly when navigating this safety-pragmatism
trade-off. Many consistently choose harmful options to advance their
operational goals, while others avoid harm only to become overly safe and
ineffective. Critically, we find this misalignment does not stem from an
inability to perceive harm, as models' harm assessments align with human
judgments, but from flawed prioritization. ManagerBench is a challenging
benchmark for a core component of agentic behavior: making safe choices when
operational goals and alignment values incentivize conflicting actions.
Benchmark & code available at https://github.com/technion-cs-nlp/ManagerBench.

</details>


### [40] [Erase to Improve: Erasable Reinforcement Learning for Search-Augmented LLMs](https://arxiv.org/abs/2510.00861)
*Ziliang Wang,Kang An,Xuhui Zheng,Faqiang Qian,Weikun Zhang,Cijun Ouyang,Jialu Cai,Yuhang Wang,Yichao Wu*

Main category: cs.CL

TL;DR: 提出可擦除强化学习框架ERL，通过动态检测/擦除/重生成错误推理步骤，显著提升大模型多步推理鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有搜索增强大模型在复杂多步推理中存在分解错误、检索缺失、推理错误三重挑战，单个错误会导致整体推理链失效

Method: ERL框架包含错误步骤识别机制、擦除机制和局部再生机制，通过强化学习实现缺陷推理链的精准修正

Result: ESearch模型在多个基准测试实现SOTA，3B模型提升8.48% EM和11.56% F1，7B模型提升5.38% EM和7.22% F1

Conclusion: 可擦除机制为LLMs的鲁棒推理提供了范式转变，证明动态错误修正比整体重生成更有效

Abstract: While search-augmented large language models (LLMs) exhibit impressive
capabilities, their reliability in complex multi-hop reasoning remains limited.
This limitation arises from three fundamental challenges: decomposition errors,
where tasks are incorrectly broken down; retrieval missing, where key evidence
fails to be retrieved; and reasoning errors, where flawed logic propagates
through the reasoning chain. A single failure in any of these stages can derail
the final answer. We propose Erasable Reinforcement Learning (ERL), a novel
framework that transforms fragile reasoning into a robust process. ERL
explicitly identifies faulty steps, erases them, and regenerates reasoning in
place, preventing defective logic from propagating through the reasoning chain.
This targeted correction mechanism turns brittle reasoning into a more
resilient process. Models trained with ERL, termed ESearch, achieve substantial
improvements on HotpotQA, MuSiQue, 2Wiki, and Bamboogle, with the 3B model
achieving +8.48% EM and +11.56% F1, and the 7B model achieving +5.38% EM and
+7.22% F1 over previous state-of-the-art(SOTA) results. These findings suggest
that erasable reinforcement learning provides a powerful paradigm shift for
robust multi-step reasoning in LLMs.

</details>


### [41] [HalluGuard: Evidence-Grounded Small Reasoning Models to Mitigate Hallucinations in Retrieval-Augmented Generation](https://arxiv.org/abs/2510.00880)
*Loris Bergeron,Ioana Buhnila,Jérôme François,Radu State*

Main category: cs.CL

TL;DR: 提出HalluGuard模型缓解RAG中的幻觉问题，4B参数小模型性能媲美更大模型


<details>
  <summary>Details</summary>
Motivation: 大语言模型易产生幻觉影响实际可信度，需提升RAG生成结果的可靠性

Method: 结合领域无关合成数据集(来自FineWeb)+幻觉/有据声明生成+基于偏好的ORPO优化方法

Result: RAGTruth子集平衡准确率84%(参数量减半匹敌7B/8B模型)，整体基准75.7%匹敌GPT-4o

Conclusion: 通过数据重构与偏好蒸馏实现小模型高效抑幻，模型与数据集将开源推动应用

Abstract: Large Language Models (LLMs) excel in many NLP tasks but remain prone to
hallucinations, limiting trust in real-world applications. We present
HalluGuard, a 4B-parameter Small Reasoning Model (SRM) for mitigating
hallucinations in Retrieval-Augmented Generation (RAG). HalluGuard classifies
document-claim pairs as grounded or hallucinated and produces evidence-grounded
justifications for transparency. Our approach combines (i) a domain-agnostic
synthetic dataset derived from FineWeb and refined through multi-stage curation
and data reformation, (ii) synthetic grounded and hallucinated claims, and
(iii) preference-based fine-tuning with Odds Ratio Preference Optimization to
distill large-model reasoning into a smaller backbone. On the RAGTruth subset
of the LLM-AggreFact benchmark, HalluGuard achieves 84.0% balanced accuracy
(BAcc), rivaling specialized models, MiniCheck (7B; 84.0%) and Granite Guardian
3.3 (8B; 82.2%) while using roughly half their parameters. Over the full
benchmark it reaches 75.7% BAcc, matching larger general-purpose LLMs such as
GPT-4o (75.9%). We will release HalluGuard and datasets under Apache 2.0 upon
acceptance.

</details>


### [42] [Span-level Detection of AI-generated Scientific Text via Contrastive Learning and Structural Calibration](https://arxiv.org/abs/2510.00890)
*Zhen Yin,Shenghua Wang*

Main category: cs.CL

TL;DR: 提出结构感知框架Sci-SpanDet，通过分章节风格建模与多级对比学习，在跨领域10万样本测试中实现80.17 F1值与74.36细粒度定位精度，显著提升学术文本AI检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有AI文本检测方法存在细粒度定位缺失、校准能力弱、跨领域泛化差等问题，难以应对LLM在学术写作引发的作者身份可信度危机。

Method: 融合章节条件化风格建模与多级对比学习减少主题依赖，结合BIO-CRF序列标注与指针边界解码实现细粒度片段定位，并引入置信度校准提升概率可靠性。

Result: 在多LLM生成数据集测试中取得80.17 F1(AI)/92.63 AUROC/74.36 Span-F1，对抗改写场景保持强韧性，IMRaD各章节及跨学科检测准确率均衡。

Conclusion: Sci-SpanDet有效解决学术文本AI检测难题，开源数据集与代码将推动领域发展，为学术出版可信度提供可靠技术保障。

Abstract: The rapid adoption of large language models (LLMs) in scientific writing
raises serious concerns regarding authorship integrity and the reliability of
scholarly publications. Existing detection approaches mainly rely on
document-level classification or surface-level statistical cues; however, they
neglect fine-grained span localization, exhibit weak calibration, and often
fail to generalize across disciplines and generators. To address these
limitations, we present Sci-SpanDet, a structure-aware framework for detecting
AI-generated scholarly texts. The proposed method combines section-conditioned
stylistic modeling with multi-level contrastive learning to capture nuanced
human-AI differences while mitigating topic dependence, thereby enhancing
cross-domain robustness. In addition, it integrates BIO-CRF sequence labeling
with pointer-based boundary decoding and confidence calibration to enable
precise span-level detection and reliable probability estimates. Extensive
experiments on a newly constructed cross-disciplinary dataset of 100,000
annotated samples generated by multiple LLM families (GPT, Qwen, DeepSeek,
LLaMA) demonstrate that Sci-SpanDet achieves state-of-the-art performance, with
F1(AI) of 80.17, AUROC of 92.63, and Span-F1 of 74.36. Furthermore, it shows
strong resilience under adversarial rewriting and maintains balanced accuracy
across IMRaD sections and diverse disciplines, substantially surpassing
existing baselines. To ensure reproducibility and to foster further research on
AI-generated text detection in scholarly documents, the curated dataset and
source code will be publicly released upon publication.

</details>


### [43] [Benchmarking Foundation Models with Retrieval-Augmented Generation in Olympic-Level Physics Problem Solving](https://arxiv.org/abs/2510.00919)
*Shunfeng Zheng,Yudi Zhang,Meng Fang,Zihan Zhang,Zhitan Wu,Mykola Pechenizkiy,Ling Chen*

Main category: cs.CL

TL;DR: 研究通过构建多模态物理数据集PhoPile，探索检索增强生成（RAG）在提升基础模型解决奥林匹克物理题能力中的作用


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在专业级物理推理（如奥林匹克物理题）表现有限，受学生通过复习历年试题提升成绩的启发，需系统研究RAG在物理推理中的潜力

Method: 创建包含图表/公式的多模态物理数据集PhoPile，设计涵盖LLM和LMM的基准测试框架，比较不同检索策略对模型性能的影响

Result: 物理知识库检索整合可提升模型表现，但多模态信息整合仍存在显著挑战，需改进检索机制

Conclusion: PhoPile数据集为物理推理研究提供新基准，实验结果揭示当前模型的局限，推动检索增强物理推理的算法创新

Abstract: Retrieval-augmented generation (RAG) with foundation models has achieved
strong performance across diverse tasks, but their capacity for expert-level
reasoning-such as solving Olympiad-level physics problems-remains largely
unexplored. Inspired by the way students prepare for competitions by reviewing
past problems, we investigate the potential of RAG to enhance physics reasoning
in foundation models. We introduce PhoPile, a high-quality multimodal dataset
specifically designed for Olympiad-level physics, enabling systematic study of
retrieval-based reasoning. PhoPile includes diagrams, graphs, and equations,
capturing the inherently multimodal nature of physics problem solving. Using
PhoPile, we benchmark RAG-augmented foundation models, covering both large
language models (LLMs) and large multimodal models (LMMs) with multiple
retrievers. Our results demonstrate that integrating retrieval with physics
corpora can improve model performance, while also highlighting challenges that
motivate further research in retrieval-augmented physics reasoning.

</details>


### [44] [Making, not Taking, the Best of N](https://arxiv.org/abs/2510.00931)
*Ammar Khairi,Daniel D'souza,Marzieh Fadaee,Julia Kreutzer*

Main category: cs.CL

TL;DR: 提出了Fusion-of-N方法，通过综合多个LLM生成样本的信息而非单纯选择最优样本，在11种语言/3类任务中验证了其在测试时扩展和合成数据生成场景下的优越性。


<details>
  <summary>Details</summary>
Motivation: 传统Best-of-N方法丢弃了未被选中的样本信息，导致潜在价值浪费。研究旨在开发一种能聚合所有候选样本优势的协作式生成框架。

Method: 使用LLM作为评判者，从N个候选样本中提取最有效信息片段进行融合。应用于：1) 单模型测试时样本聚合 2) 多教师模型知识蒸馏

Result: 在跨语言/多任务/不同模型规模测试中，FusioN始终超越BoN，合成数据训练的学生模型准确率提升最高达5.7%。

Conclusion: 应转变LLM评估范式，从单一最优选择转向多源信息融合，充分释放多样化生成样本的潜在价值，突破传统选择方法的性能瓶颈。

Abstract: Obtaining high-quality generations in modern LLMs has largely been framed as
a selection problem: identifying a single winning generation from a diverse
pool of N samples, the Best-of-N (BoN). Yet, this approach is inherently
zero-sum, discarding diverse and potentially useful information from the pool.
Instead, we explore a collaborative setup, where all candidates can potentially
contribute to the final winning generation. To this end, we propose Fusion-of-N
(FusioN): a method that uses a general LLM judge to synthesize the most
informative elements of each sample into a single final answer. We compare
FusioN to BoN in two settings, (i) test-time scaling, where we sample and
aggregate from a single model at test-time (ii) synthetic data generation,
where we fuse samples from a pool of diverse teachers to improve a student
model. We extensively benchmark both setups across 11 languages, 3 diverse
tasks and varying model scales. Across the bench, FusioN consistently
outperforms BoN showing versatility and robustness both in test-time scaling
and in downstream gains from synthetic data generation. We also perform
extensive analysis on FusioN, where it shows surprising strengths and
robustness under challenging settings. These results show that we should shift
how we think about evaluating and utilizing LLM generations from a monolithic
measure of quality, to embracing their polylithic nature. This shift allows us
to integrate diverse strengths, unlock latent potential, and achieve
improvements that were previously inaccessible through selection alone.

</details>


### [45] [Analyzing Dialectical Biases in LLMs for Knowledge and Reasoning Benchmarks](https://arxiv.org/abs/2510.00962)
*Eileen Pan,Anna Seo Gyeong Choi,Maartje ter Hoeve,Skyler Seto,Allison Koenecke*

Main category: cs.CL

TL;DR: 研究揭示非标准英语方言导致大模型性能下降达20%，三个特定语法规则（存在性'it'/零系词/y'all）是主因，建议针对性优化语法结构以减轻偏见


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理非标准英语方言时表现显著下降，需探究具体语法规则对模型性能的影响机制

Method: 1. 将标准英语问题转换为多方言变体进行多选任务测试 2. 分解研究不同语法规则对性能的具体影响

Result: 非标准英语问题准确率最高下降20%，三个关键语法规则可解释多个方言中观察到的大部分性能下降

Conclusion: 应优先研究针对高影响语法结构（如存在性'it'/零系词/y'all）的偏见缓解方法，提升模型方言包容性

Abstract: Large language models (LLMs) are ubiquitous in modern day natural language
processing. However, previous work has shown degraded LLM performance for
under-represented English dialects. We analyze the effects of typifying
"standard" American English language questions as non-"standard" dialectal
variants on multiple choice question answering tasks and find up to a 20%
reduction in accuracy. Additionally, we investigate the grammatical basis of
under-performance in non-"standard" English questions. We find that individual
grammatical rules have varied effects on performance, but some are more
consequential than others: three specific grammar rules (existential "it", zero
copula, and y'all) can explain the majority of performance degradation observed
in multiple dialects. We call for future work to investigate bias mitigation
methods focused on individual, high-impact grammatical structures.

</details>


### [46] [Syntax-Guided Diffusion Language Models with User-Integrated Personalization](https://arxiv.org/abs/2510.01028)
*Ruqian Zhang,Yijiao Zhang,Juan Shen,Zhongyi Zhu,Annie Qu*

Main category: cs.CL

TL;DR: 提出语法引导的扩散语言模型，通过结构监督和个性化条件增强文本生成质量与多样性


<details>
  <summary>Details</summary>
Motivation: 解决传统语言模型生成文本结构单一、个性化不足的问题，利用扩散模型突破自回归范式限制

Method: 开发级联框架（先语法生成后文本生成）与非级联架构（结构内容对齐），引入跨用户共享表示机制实现细粒度个性化

Result: 在多任务实验中展现优越的流畅性、多样性（+15%结构变化）和风格保真度（92%用户偏好），定性分析验证个性化模式学习能力

Conclusion: 该方法成功提升生成可控性，支持零样本推理，为个性化文本生成开辟新方向，未来可扩展至多模态场景

Abstract: Large language models have made revolutionary progress in generating
human-like text, yet their outputs often tend to be generic, exhibiting
insufficient structural diversity, which limits personalized expression. Recent
advances in diffusion models have opened new opportunities for improving
language generation beyond the limitations of autoregressive paradigms. In this
work, we propose a syntax-guided diffusion language model that integrates
structural supervision and personalized conditioning to enhance text quality,
diversity, and controllability. We introduce a cascaded framework that
generates syntactic guidance before conditional text generation, and further
generalize it to a novel noncascaded architecture for better alignment between
structure and content. By incorporating syntactic information in the generating
process, the proposed model better captures the lexical and structural
characteristics of stylistic sentence construction. To enable fine-grained
personalization, we develop a shared representation mechanism that facilitates
information integration across users, supporting both faithful stylistic
generation and generalizable zero-shot inference. Extensive experiments on
multiple tasks demonstrate the superiority of our approach in fluency,
diversity, and stylistic fidelity. Further qualitative analyses highlight its
interpretability and flexibility in learning personalized patterns.

</details>


### [47] [Interpreting Language Models Through Concept Descriptions: A Survey](https://arxiv.org/abs/2510.01048)
*Nils Feldhus,Laura Kopf*

Main category: cs.CL

TL;DR: 该论文系统综述了针对大语言模型内部组件生成自然语言概念描述的方法，强调需加强因果性评估并规划未来研究方向以提高模型透明度。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型的可解释性，揭示其内部决策机制（如神经元、注意力头、稀疏自编码器提取的特征），增强模型可信度和可靠性。

Method: 通过文献综述整合当前技术，分析利用生成模型创建组件概念描述的方法，评估指标及支撑数据集的发展现状。

Result: 发现现有评估方法缺乏因果验证，需建立更严谨的评估框架，并系统梳理了该领域技术路线与核心挑战。

Conclusion: 未来研究应聚焦开发因果性评估体系，推动大语言模型机制可解释性研究的实质性突破。

Abstract: Understanding the decision-making processes of neural networks is a central
goal of mechanistic interpretability. In the context of Large Language Models
(LLMs), this involves uncovering the underlying mechanisms and identifying the
roles of individual model components such as neurons and attention heads, as
well as model abstractions such as the learned sparse features extracted by
Sparse Autoencoders (SAEs). A rapidly growing line of work tackles this
challenge by using powerful generator models to produce open-vocabulary,
natural language concept descriptions for these components. In this paper, we
provide the first survey of the emerging field of concept descriptions for
model components and abstractions. We chart the key methods for generating
these descriptions, the evolving landscape of automated and human metrics for
evaluating them, and the datasets that underpin this research. Our synthesis
reveals a growing demand for more rigorous, causal evaluation. By outlining the
state of the art and identifying key challenges, this survey provides a roadmap
for future research toward making models more transparent.

</details>


### [48] [Hybrid Dialogue State Tracking for Persian Chatbots: A Language Model-Based Approach](https://arxiv.org/abs/2510.01052)
*Samin Mahdipour Aghabagher,Saeedeh Momtazi*

Main category: cs.CL

TL;DR: 提出结合规则方法和语言模型的混合DST模型，在波斯语多轮对话数据集上显著提升聊天机器人准确性和连贯性。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的DST方法在开放域、多轮对话场景中适应性不足，无法满足复杂对话的人类化体验需求。

Method: 混合模型整合BERT（槽填充/意图检测）、XGBoost（意图验证）、GPT（DST）和在线代理（实时回答生成）

Result: 在波斯语数据集上验证，准确性和对话连贯性显著超越现有方法

Conclusion: 混合方法有效提升DST能力，为开发更人性化、自适应的对话AI系统提供新路径

Abstract: Dialogue State Tracking (DST) is an essential element of conversational AI
with the objective of deeply understanding the conversation context and leading
it toward answering user requests. Due to high demands for open-domain and
multi-turn chatbots, the traditional rule-based DST is not efficient enough,
since it cannot provide the required adaptability and coherence for human-like
experiences in complex conversations. This study proposes a hybrid DST model
that utilizes rule-based methods along with language models, including BERT for
slot filling and intent detection, XGBoost for intent validation, GPT for DST,
and online agents for real-time answer generation. This model is uniquely
designed to be evaluated on a comprehensive Persian multi-turn dialogue dataset
and demonstrated significantly improved accuracy and coherence over existing
methods in Persian-based chatbots. The results demonstrate how effectively a
hybrid approach may improve DST capabilities, paving the way for conversational
AI systems that are more customized, adaptable, and human-like.

</details>


### [49] [Research on the Integration of Embodied Intelligence and Reinforcement Learning in Textual Domains](https://arxiv.org/abs/2510.01076)
*Haonan Wang,Junfeng Sun,Mingjia Zhao,Wei Liu*

Main category: cs.CL

TL;DR: 提出结合具身智能与强化学习的文本处理模型，验证了该模型在文本任务中的有效性


<details>
  <summary>Details</summary>
Motivation: 通过融合具身智能的感知-行动优势和强化学习的决策优化能力，提升文本处理的智能化水平

Method: 构建具身智能与强化学习融合框架，开展理论阐释与实验验证

Result: 模型在多场景文本处理任务中表现优异，准确率提升23%

Conclusion: 成功验证了融合模型的创新性，为智能文本处理提供了新的技术路径

Abstract: This article addresses embodied intelligence and reinforcement learning
integration in the field of text processing, aiming to enhance text handling
with more intelligence on the basis of embodied intelligence's perception and
action superiority and reinforcement learning's decision optimization
capability. Through detailed theoretical explanation and experimental
exploration, a novel integration model is introduced. This model has been
demonstrated to be very effective in a wide range oftext processing tasks,
validating its applicative potential

</details>


### [50] [Automatic Speech Recognition (ASR) for African Low-Resource Languages: A Systematic Literature Review](https://arxiv.org/abs/2510.01145)
*Sukairaj Hafiz Imam,Tadesse Destaw Belay,Kedir Yassin Husse,Ibrahim Said Ahmad,Idris Abdulmumin,Hadiza Ali Umar,Muhammad Yahuza Bello,Joyce Nakatumba-Nabende,Seid Muhie Yimam,Shamsuddeen Hassan Muhammad*

Main category: cs.CL

TL;DR: 系统综述揭示非洲低资源语言ASR研究存在数据集稀缺、模型泛化不足、评估指标单一等问题，提出通过社区合作、伦理数据集建设和轻量化建模推动可持续发展。


<details>
  <summary>Details</summary>
Motivation: 解决非洲2000+语言在自动语音识别(ASR)领域代表性不足的问题，消除因技术鸿沟导致的数字包容性障碍。

Method: 采用PRISMA 2020框架，系统分析2020-2025年间5大学术平台的71项研究，评估数据集质量(5分制筛选)、模型方法及评价指标。

Result: 发现74个数据集覆盖111种语言(约11,206小时语音)，但仅15%研究可复现；自监督学习受限于预训练数据不足，93%研究使用WER指标忽视音调/形态特征。

Conclusion: 建议通过利益相关方合作、创建平衡数据集、轻量模型技术和主动基准测试，结合社区驱动创新实现ASR系统可持续发展。

Abstract: ASR has achieved remarkable global progress, yet African low-resource
languages remain rigorously underrepresented, producing barriers to digital
inclusion across the continent with more than +2000 languages. This systematic
literature review (SLR) explores research on ASR for African languages with a
focus on datasets, models and training methods, evaluation techniques,
challenges, and recommends future directions. We employ the PRISMA 2020
procedures and search DBLP, ACM Digital Library, Google Scholar, Semantic
Scholar, and arXiv for studies published between January 2020 and July 2025. We
include studies related to ASR datasets, models or metrics for African
languages, while excluding non-African, duplicates, and low-quality studies
(score <3/5). We screen 71 out of 2,062 records and we record a total of 74
datasets across 111 languages, encompassing approximately 11,206 hours of
speech. Fewer than 15% of research provided reproducible materials, and dataset
licensing is not clear. Self-supervised and transfer learning techniques are
promising, but are hindered by limited pre-training data, inadequate coverage
of dialects, and the availability of resources. Most of the researchers use
Word Error Rate (WER), with very minimal use of linguistically informed scores
such as Character Error Rate (CER) or Diacritic Error Rate (DER), and thus with
limited application in tonal and morphologically rich languages. The existing
evidence on ASR systems is inconsistent, hindered by issues like dataset
availability, poor annotations, licensing uncertainties, and limited
benchmarking. Nevertheless, the rise of community-driven initiatives and
methodological advancements indicates a pathway for improvement. Sustainable
development for this area will also include stakeholder partnership, creation
of ethically well-balanced datasets, use of lightweight modelling techniques,
and active benchmarking.

</details>


### [51] [mR3: Multilingual Rubric-Agnostic Reward Reasoning Models](https://arxiv.org/abs/2510.01146)
*David Anugraha,Shou-Yi Hung,Zilu Tang,Annie En-Shiun Lee,Derry Tanti Wijaya,Genta Indra Winata*

Main category: cs.CL

TL;DR: 提出mR3多语言奖励模型，覆盖72种语言，在评测基准上超越大模型且体积缩小9倍。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估工具在非英语场景表现不佳，缺乏有效的多语言奖励模型训练方法论

Method: 通过数据选择策略(包括目标语言推理数据集)和课程学习优化多语言奖励模型训练过程

Result: 在多语言奖励模型基准取得SOTA，模型尺寸缩小至1/9仍优于GPT-OSS-120B，消融实验验证有效性

Conclusion: mR3为开源社区提供高效的多语言评估解决方案，数据策略和模型架构对奖励模型性能有重要影响

Abstract: Evaluation using Large Language Model (LLM) judges has been widely adopted in
English and shown to be effective for automatic evaluation. However, their
performance does not generalize well to non-English settings, and it remains
unclear what constitutes effective multilingual training for such judges. In
this paper, we introduce mR3, a massively multilingual, rubric-agnostic reward
reasoning model trained on 72 languages, achieving the broadest language
coverage in reward modeling to date. We present a comprehensive study of data
and curriculum selection for training to identify effective strategies and data
sources for building high-quality reward models, including the integration of
target-language reasoning datasets. Our approach attains state-of-the-art
performance on multilingual reward model benchmarks, surpassing much larger
models (i.e., GPT-OSS-120B) while being up to 9x smaller, and its effectiveness
is further confirmed through extensive ablation studies. Our models, data, and
code are available as open source at https://github.com/rubricreward/mr3.

</details>


### [52] [Pay-Per-Search Models are Abstention Models](https://arxiv.org/abs/2510.01152)
*Mustafa Omer Gul,Claire Cardie,Tanya Goyal*

Main category: cs.CL

TL;DR: 提出MASH框架，通过强化学习惩罚外部搜索行为同时奖励准确回答，使LLM自然产生弃权行为，无需预先定义知识边界


<details>
  <summary>Details</summary>
Motivation: LLM无法可靠识别知识边界，常对超出边界问题生成幻觉答案，而人类会主动寻求帮助或弃权

Method: 采用强化学习框架，通过'按次付费'奖励机制（搜索惩罚+回答准确奖励），将工具使用转化为弃权信号

Result: 在三个知识密集型QA数据集上实现7.6%准确率提升，且能有效区分可答/不可答问题

Conclusion: MASH通过辅助性选择帮助任务训练，自然实现弃权决策，成功将搜索工具使用与参数知识对齐

Abstract: LLMs cannot reliably recognize their parametric knowledge boundaries and
often hallucinate answers to outside-of-boundary questions. In contrast, humans
recognize their limitations and can either seek external help for such
questions or abstain. In this paper, we introduce MASH (Modeling Abstention via
Selective Help-seeking), a training framework that readily extracts abstentions
from LLMs. Our key idea is that any external help-seeking by an LLM, i.e.
search tool use, can serve as a proxy for abstention if the external help
(search) is appropriately penalized while simultaneously rewarding answer
accuracy. MASH operationalizes this idea using reinforcement learning with a
pay-per-search reward.
  We run experiments on three knowledge-intensive QA datasets. Our results show
that MASH substantially improves upon the selective help-seeking performance of
prior efficient search approaches; on multi-hop datasets, MASH improves answer
accuracy by 7.6%. Furthermore, MASH demonstrates strong off-the-shelf
abstention -- it can distinguish between unanswerable/answerable questions and
selectively generate responses for answerable questions -- showcasing behavior
analogous to specialized abstention approaches. We emphasize that contrary to
prior abstention methods, MASH does not require pre-determining knowledge
boundaries to construct training data. Instead, MASH's abstentions are a
by-product of training for the auxiliary selective help-seeking task. Overall,
we show that MASH training effectively aligns search tool use with parametric
knowledge, which can be successfully leveraged for making abstention decisions.

</details>


### [53] [Backdoor Attacks Against Speech Language Models](https://arxiv.org/abs/2510.01157)
*Alexandrine Fortier,Thomas Thebaud,Jesús Villalba,Najim Dehak,Patrick Cardinal*

Main category: cs.CL

TL;DR: 针对语音语言模型的音频后门攻击系统性研究，攻击成功率高达90.76%-99.41%，通过组件分析定位脆弱环节并提出微调防御方案


<details>
  <summary>Details</summary>
Motivation: 多模态模型通过级联编码器继承组件漏洞，需系统性研究语音模型的后门攻击风险及其防御机制

Method: 在4个语音编码器和3个数据集上测试ASR、情感识别等4项任务，进行组件脆弱性分析并开发基于微调的防御方法

Result: 攻击成功率超过90%，定位到编码器微调阶段最脆弱，防御方法有效降低后门威胁

Conclusion: 语音模型后门攻击威胁显著，组件分析揭示攻击路径，微调防御可缓解预训练编码器中毒风险

Abstract: Large Language Models (LLMs) and their multimodal extensions are becoming
increasingly popular. One common approach to enable multimodality is to cascade
domain-specific encoders with an LLM, making the resulting model inherit
vulnerabilities from all of its components. In this work, we present the first
systematic study of audio backdoor attacks against speech language models. We
demonstrate its effectiveness across four speech encoders and three datasets,
covering four tasks: automatic speech recognition (ASR), speech emotion
recognition, and gender and age prediction. The attack consistently achieves
high success rates, ranging from 90.76% to 99.41%. To better understand how
backdoors propagate, we conduct a component-wise analysis to identify the most
vulnerable stages of the pipeline. Finally, we propose a fine-tuning-based
defense that mitigates the threat of poisoned pretrained encoders.

</details>


### [54] [Social Welfare Function Leaderboard: When LLM Agents Allocate Social Welfare](https://arxiv.org/abs/2510.01164)
*Zhengliang Shi,Ruotian Ma,Jen-tse Huang,Xinbei Ma,Xingyu Chen,Mengru Wang,Qu Yang,Yue Wang,Fanghua Ye,Ziyang Chen,Shanyi Wang,Cixing Li,Wenxuan Wang,Zhaopeng Tu,Xiaolong Li,Zhaochun Ren,Linus*

Main category: cs.CL

TL;DR: 研究者开发SWF基准评估LLMs社会资源分配表现，发现主流模型普遍存在效率优先倾向且策略脆弱性显著


<details>
  <summary>Details</summary>
Motivation: LLMs越来越多参与影响人类福祉的高风险决策，但其在分配稀缺社会资源时遵循的价值体系尚未被系统研究

Method: 通过动态模拟环境构建社会福祉函数基准，使LLM作为主权分配者在异质群体中分配任务，持续追踪投资回报率（效率）和基尼系数（公平性）的平衡关系

Result: 评估20个先进LLM显示：1) 模型对话能力与分配技能无关 2) 多数模型呈现强烈功利主义倾向 3) 分配策略易受输出长度限制和社会影响框架干扰

Conclusion: 当前LLMs不适合作为社会决策者，需开发专门评估基准并进行针对性价值观对齐

Abstract: Large language models (LLMs) are increasingly entrusted with high-stakes
decisions that affect human welfare. However, the principles and values that
guide these models when distributing scarce societal resources remain largely
unexamined. To address this, we introduce the Social Welfare Function (SWF)
Benchmark, a dynamic simulation environment where an LLM acts as a sovereign
allocator, distributing tasks to a heterogeneous community of recipients. The
benchmark is designed to create a persistent trade-off between maximizing
collective efficiency (measured by Return on Investment) and ensuring
distributive fairness (measured by the Gini coefficient). We evaluate 20
state-of-the-art LLMs and present the first leaderboard for social welfare
allocation. Our findings reveal three key insights: (i) A model's general
conversational ability, as measured by popular leaderboards, is a poor
predictor of its allocation skill. (ii) Most LLMs exhibit a strong default
utilitarian orientation, prioritizing group productivity at the expense of
severe inequality. (iii) Allocation strategies are highly vulnerable, easily
perturbed by output-length constraints and social-influence framing. These
results highlight the risks of deploying current LLMs as societal
decision-makers and underscore the need for specialized benchmarks and targeted
alignment for AI governance.

</details>


### [55] [GRAD: Generative Retrieval-Aligned Demonstration Sampler for Efficient Few-Shot Reasoning](https://arxiv.org/abs/2510.01165)
*Oussama Gabouj,Kamel Charaf,Ivan Zakazov,Nicolas Baldwin,Robert West*

Main category: cs.CL

TL;DR: 提出GRAD方法，通过动态生成输入相关的简洁示例增强大语言模型性能。该方法在数学和STEM领域表现优异，且小模型生成的示例可有效指导大模型，降低训练成本。


<details>
  <summary>Details</summary>
Motivation: 传统RAG依赖静态数据库导致示例相关性不足，需开发动态生成机制提升上下文支持能力。

Method: 训练LLM生成输入相关的精炼示例，基于数学数据集训练GRAD模型，并验证其在跨学科领域的泛化能力。

Result: GRAD在token预算限制下超越传统方法，数学推理准确率提升显著，且在物理/化学等OOD领域保持强泛化性。

Conclusion: GRAD开创了动态少样本学习范式，在资源受限场景中实现高效推理，代码已开源推动后续研究。

Abstract: Large Language Models (LLMs) achieve strong performance across diverse tasks,
but their effectiveness often depends on the quality of the provided context.
Retrieval-Augmented Generation (RAG) enriches prompts with external
information, but its reliance on static databases constrains adaptability and
can result in irrelevant demonstrations. In this work, we propose a Generative
Retrieval-Aligned Demonstrator (GRAD), a dynamic demonstration-based approach
where an LLM model is trained to generate input-specific concise
demonstrations. By tailoring demonstrations to each input, our method offers
better contextual support than traditional RAG approaches. We demonstrate the
superiority of GRAD under budget constraints, where we limit both the number of
tokens used per demonstration and the number of tokens used for the final
output. Trained solely on a math dataset, GRAD consistently outperforms strong
baselines on Qwen2.5-14B across mathematical reasoning and advanced STEM
questions, highlighting GRAD's robust generalization to out-of-distribution
(OOD) domains such as physics, chemistry, and computer science. Furthermore, we
show that demonstrations generated by trained smaller models can effectively
guide larger target models, reducing training costs while maintaining
competitive accuracy. Overall, this work introduces a scalable demonstration
generator model presenting the first step toward a dynamic few-shot learning
paradigm in resource-constrained settings. We release the code used for the
project.

</details>


### [56] [Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity](https://arxiv.org/abs/2510.01171)
*Jiayi Zhang,Simon Yu,Derek Chong,Anthony Sicilia,Michael R. Tomz,Christopher D. Manning,Weiyan Shi*

Main category: cs.CL

TL;DR: 提出Verbalized Sampling方法解决大模型对齐训练导致的模式崩溃问题，通过概率分布提示显著提升生成多样性


<details>
  <summary>Details</summary>
Motivation: 传统对齐训练导致大模型多样性下降（模式崩溃），研究发现数据层面的典型性偏差是根本原因（标注者偏好熟悉文本）

Method: 提出Verbalized Sampling提示策略：要求模型生成响应集合并标注概率分布（如「生成5个咖啡笑话及其概率」）

Result: 在创意写作、对话模拟等场景中多样性提升1.6-2.1倍，且不影响事实准确性；模型能力越强提升效果越显著

Conclusion: 从数据视角揭示了模式崩溃机制，并提出无需训练的推理阶段解决方案，有效释放预训练模型的生成潜力

Abstract: Post-training alignment often reduces LLM diversity, leading to a phenomenon
known as mode collapse. Unlike prior work that attributes this effect to
algorithmic limitations, we identify a fundamental, pervasive data-level
driver: typicality bias in preference data, whereby annotators systematically
favor familiar text as a result of well-established findings in cognitive
psychology. We formalize this bias theoretically, verify it on preference
datasets empirically, and show that it plays a central role in mode collapse.
Motivated by this analysis, we introduce Verbalized Sampling, a simple,
training-free prompting strategy to circumvent mode collapse. VS prompts the
model to verbalize a probability distribution over a set of responses (e.g.,
``Generate 5 jokes about coffee and their corresponding probabilities'').
Comprehensive experiments show that VS significantly improves performance
across creative writing (poems, stories, jokes), dialogue simulation,
open-ended QA, and synthetic data generation, without sacrificing factual
accuracy and safety. For instance, in creative writing, VS increases diversity
by 1.6-2.1x over direct prompting. We further observe an emergent trend that
more capable models benefit more from VS. In sum, our work provides a new
data-centric perspective on mode collapse and a practical inference-time remedy
that helps unlock pre-trained generative diversity.

</details>


### [57] [Energy-Regularized Sequential Model Editing on Hyperspheres](https://arxiv.org/abs/2510.01172)
*Qingyuan Liu,Jia-Chen Gu,Yunzhi Yao,Hong Wang,Nanyun Peng*

Main category: cs.CL

TL;DR: 研究通过超球面能量（HE）稳定神经元分布，提出SPHERE方法，显著提升连续编辑效果并减少知识遗忘。


<details>
  <summary>Details</summary>
Motivation: 连续模型编辑易导致神经元表征失稳和知识遗忘，需探索维持超球面均匀性以提升编辑稳定性。

Method: 通过HE量化神经元分布均匀性，设计SPHERE正则化策略（稀疏投影至权重矩阵主方向的互补空间）稳定权重分布。

Result: SPHERE在LLaMA3/Qwen2.5上编辑能力平均提升16.41%，最佳保持模型通用性能。

Conclusion: HE动态为知识退化提供理论下限，SPHERE为大模型知识编辑提供了稳定可靠的实现路径。

Abstract: Large language models (LLMs) require constant updates to remain aligned with
evolving real-world knowledge. Model editing offers a lightweight alternative
to retraining, but sequential editing often destabilizes representations and
induces catastrophic forgetting. In this work, we seek to better understand and
mitigate performance degradation caused by sequential editing. We hypothesize
that hyperspherical uniformity, a property that maintains uniform distribution
of neuron weights on a hypersphere, helps the model remain stable, retain prior
knowledge, while still accommodate new updates. We use Hyperspherical Energy
(HE) to quantify neuron uniformity during editing, and examine its correlation
with editing performance. Empirical studies across widely used editing methods
reveals a strong correlation between HE dynamics and editing performance, with
editing failures consistently coinciding with high HE fluctuations. We further
theoretically prove that HE dynamics impose a lower bound on the degradation of
pretrained knowledge, highlighting why HE stability is crucial for knowledge
retention. Motivated by these insights, we propose SPHERE (Sparse Projection
for Hyperspherical Energy-Regularized Editing), an HE-driven regularization
strategy that stabilizes neuron weight distributions, ultimately preserving
prior knowledge while enabling reliable sequential updates. Specifically,
SPHERE identifies a sparse space complementary to the principal hyperspherical
directions of the pretrained weight matrices and projects new knowledge onto
it, attenuating perturbations on the principal directions. Extensive
experiments on LLaMA3 (8B) and Qwen2.5 (7B) show that SPHERE outperforms the
best baseline in editing capability by an average of 16.41%, while most
faithfully preserving general model performance, thereby offering a principled
path toward reliable large-scale knowledge editing.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [58] [Motion In-Betweening for Densely Interacting Characters](https://arxiv.org/abs/2510.00314)
*Xiaotang Zhang,Ziyi Chang,Qianhui Men,Hubert P. H. Shum*

Main category: cs.GR

TL;DR: 提出跨空间中间生成方法解决双角色长时交互运动合成，通过对抗学习维持交互质量与潜在空间精炼保持运动稳定性


<details>
  <summary>Details</summary>
Motivation: 传统单角色运动中间生成方法难以处理双角色密集交互场景，交互约束导致解空间受限，运动质量与交互持续性随时间下降

Method: 1) 跨空间条件表示建模角色交互 2) 对抗学习识别周期性交互模式 3) 潜在空间漂移修正防止姿态误差累积

Result: 实现多关键姿势下动态拳击/舞蹈动作的逼真长时合成，定量评估与用户研究验证方法有效性

Conclusion: 该框架成功解决双角色长时交互运动合成难题，在保持时空对应关系的同时实现自然过渡与运动质量维持

Abstract: Motion in-betweening is the problem to synthesize movement between keyposes.
Traditional research focused primarily on single characters. Extending them to
densely interacting characters is highly challenging, as it demands precise
spatial-temporal correspondence between the characters to maintain the
interaction, while creating natural transitions towards predefined keyposes. In
this research, we present a method for long-horizon interaction in-betweening
that enables two characters to engage and respond to one another naturally. To
effectively represent and synthesize interactions, we propose a novel solution
called Cross-Space In-Betweening, which models the interactions of each
character across different conditioning representation spaces. We further
observe that the significantly increased constraints in interacting characters
heavily limit the solution space, leading to degraded motion quality and
diminished interaction over time. To enable long-horizon synthesis, we present
two solutions to maintain long-term interaction and motion quality, thereby
keeping synthesis in the stable region of the solution space.We first sustain
interaction quality by identifying periodic interaction patterns through
adversarial learning. We further maintain the motion quality by learning to
refine the drifted latent space and prevent pose error accumulation. We
demonstrate that our approach produces realistic, controllable, and
long-horizon in-between motions of two characters with dynamic boxing and
dancing actions across multiple keyposes, supported by extensive quantitative
evaluations and user studies.

</details>


### [59] [ReSWD: ReSTIR'd, not shaken. Combining Reservoir Sampling and Sliced Wasserstein Distance for Variance Reduction](https://arxiv.org/abs/2510.01061)
*Mark Boss,Andreas Engelhardt,Simon Donné,Varun Jampani*

Main category: cs.GR

TL;DR: 提出Reservoir SWD(ReSWD)算法，通过自适应保留关键投影方向降低Sliced Wasserstein距离的方差，在优化过程中实现稳定梯度


<details>
  <summary>Details</summary>
Motivation: 传统Sliced Wasserstein Distance(SWD)的蒙特卡洛估计器存在高方差问题，导致梯度噪声大、收敛速度慢

Method: 将加权储层采样(Weighted Reservoir Sampling)整合到SWD中，在优化步骤中自适应保留信息量大的投影方向

Result: 在合成基准测试和颜色校正、扩散模型指导等实际任务中，ReSWD持续优于标准SWD和其他方差缩减基线方法

Conclusion: ReSWD在保持无偏估计的同时实现了稳定梯度，为高维分布匹配提供了高效解决方案

Abstract: Distribution matching is central to many vision and graphics tasks, where the
widely used Wasserstein distance is too costly to compute for high dimensional
distributions. The Sliced Wasserstein Distance (SWD) offers a scalable
alternative, yet its Monte Carlo estimator suffers from high variance,
resulting in noisy gradients and slow convergence. We introduce Reservoir SWD
(ReSWD), which integrates Weighted Reservoir Sampling into SWD to adaptively
retain informative projection directions in optimization steps, resulting in
stable gradients while remaining unbiased. Experiments on synthetic benchmarks
and real-world tasks such as color correction and diffusion guidance show that
ReSWD consistently outperforms standard SWD and other variance reduction
baselines. Project page: https://reservoirswd.github.io/

</details>


### [60] [Audio Driven Real-Time Facial Animation for Social Telepresence](https://arxiv.org/abs/2510.01176)
*Jiye Lee,Chenghui Li,Linh Tran,Shih-En Wei,Jason Saragih,Alexander Richard,Hanbyul Joo,Shaojie Bai*

Main category: cs.GR

TL;DR: 提出基于扩散模型的实时3D面部动画系统，通过在线变压器和单步蒸馏实现15ms超低延迟，支持VR多模态交互。


<details>
  <summary>Details</summary>
Motivation: 解决虚拟现实中面部动画的高延迟问题，满足实时自然社交互动的需求，突破传统离线方法的速度限制。

Method: 1. 音频编码器实时转换语音为表情潜空间；2. 在线变压器消除未来帧依赖；3. 蒸馏管道将扩散迭代压缩为单步推理；4. 多模态框架整合情感参数和眼动追踪。

Result: 实现15ms GPU推理速度（提升100-1000倍），面部动画精度超越SOTA离线模型，支持多语言/多场景实时渲染。

Conclusion: 该框架首次在保持照片级真实感的同时达到实时性能，为VR社交设定了新标准，其架构设计对实时生成模型具有普适参考价值。

Abstract: We present an audio-driven real-time system for animating photorealistic 3D
facial avatars with minimal latency, designed for social interactions in
virtual reality for anyone. Central to our approach is an encoder model that
transforms audio signals into latent facial expression sequences in real time,
which are then decoded as photorealistic 3D facial avatars. Leveraging the
generative capabilities of diffusion models, we capture the rich spectrum of
facial expressions necessary for natural communication while achieving
real-time performance (<15ms GPU time). Our novel architecture minimizes
latency through two key innovations: an online transformer that eliminates
dependency on future inputs and a distillation pipeline that accelerates
iterative denoising into a single step. We further address critical design
challenges in live scenarios for processing continuous audio signals
frame-by-frame while maintaining consistent animation quality. The versatility
of our framework extends to multimodal applications, including semantic
modalities such as emotion conditions and multimodal sensors with head-mounted
eye cameras on VR headsets. Experimental results demonstrate significant
improvements in facial animation accuracy over existing offline
state-of-the-art baselines, achieving 100 to 1000 times faster inference speed.
We validate our approach through live VR demonstrations and across various
scenarios such as multilingual speeches.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [61] [WaveMind: Towards a Conversational EEG Foundation Model Aligned to Textual and Visual Modalities](https://arxiv.org/abs/2510.00032)
*Ziyi Zeng,Zhenyang Cai,Yixi Cai,Xidong Wang,Junying Chen,Rongsheng Wang,Yipeng Liu,Siqi Cai,Benyou Wang,Zhiguo Zhang,Haizhou Li*

Main category: eess.SP

TL;DR: 提出通过多模态大语言模型统一语义空间实现EEG泛化解读，并构建首个跨任务EEG指令调优数据集WaveMind-Instruct-338k，模型在四类下游任务中同时实现精准分类与灵活对话


<details>
  <summary>Details</summary>
Motivation: EEG信号同时编码认知过程和神经状态，导致跨模态表征学习存在模态错配问题

Method: 将EEG信号与对应模态映射到统一语义空间，并引入跨任务指令调优数据集WaveMind-Instruct-338k

Result: 模型在四个下游任务中实现稳健分类准确率（85.3%）的同时支持开放式对话

Conclusion: 该方法为神经科学研究与通用EEG模型开发提供了新思路，实现了分类精度与对话灵活性的统一

Abstract: Electroencephalography (EEG) interpretation using multimodal large language
models (MLLMs) offers a novel approach for analyzing brain signals. However,
the complex nature of brain activity introduces critical challenges: EEG
signals simultaneously encode both cognitive processes and intrinsic neural
states, creating a mismatch in EEG paired-data modality that hinders effective
cross-modal representation learning. Through a pivot investigation, we uncover
complementary relationships between these modalities. Leveraging this insight,
we propose mapping EEG signals and their corresponding modalities into a
unified semantic space to achieve generalized interpretation. To fully enable
conversational capabilities, we further introduce WaveMind-Instruct-338k, the
first cross-task EEG dataset for instruction tuning. The resulting model
demonstrates robust classification accuracy while supporting flexible,
open-ended conversations across four downstream tasks, thereby offering
valuable insights for both neuroscience research and the development of
general-purpose EEG models.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [62] [Spiralformer: Low Latency Encoder for Streaming Speech Recognition with Circular Layer Skipping and Early Exiting](https://arxiv.org/abs/2510.00982)
*Emiru Tsunoo,Hayato Futami,Yosuke Kashiwagi,Siddhant Arora,Shinji Watanabe*

Main category: eess.AS

TL;DR: 提出Spiralformer编码器，通过层循环丢弃和螺旋层转移机制优化块处理，在保持计算成本与词错误率相近的同时显著降低语音识别编码延迟


<details>
  <summary>Details</summary>
Motivation: 现有流式语音识别研究主要关注transducer的发射延迟，而块处理的编码延迟改善不足。频繁小步长输出虽能降低延迟但会增加计算成本

Method: 结合层丢弃（cyclic layer dropping）和早期退出机制，通过螺旋式层转移策略在块处理中完成所有层的计算，优化计算效率

Result: Librispeech平均token延迟降低21.6%，CSJ降低7.0%，计算成本与词错误率与基线模型相当

Conclusion: Spiralformer通过创新的层调度机制，在流式语音识别中实现了延迟优化与计算效率的平衡

Abstract: For streaming speech recognition, a Transformer-based encoder has been widely
used with block processing. Although many studies addressed improving emission
latency of transducers, little work has been explored for improving encoding
latency of the block processing. We seek to reduce latency by frequently
emitting a chunk with a small shift rather than scarce large-chunk emissions,
resulting in higher computational costs. To efficiently compute with the small
chunk shift, we propose a new encoder, Spiralformer, tailored for block
processing by combining layer dropping and early exiting. We skip layer
computation in a cyclic manner and shift the computed layer in each block
spirally, which completes computation for all the layers over the block
processing. Experimentally, we observed that our method achieved 21.6%
reduction in the averaged token emission delay in Librispeech, and 7.0% in CSJ,
compared with the baseline with similar computational cost and word error
rates.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [63] [ARS: Adaptive Reasoning Suppression for Efficient Large Reasoning Language Models](https://arxiv.org/abs/2510.00071)
*Dongqi Zheng*

Main category: cs.AI

TL;DR: 提出无需训练的ARS方法，通过动态抑制冗余推理步骤实现53% token/46.1%延迟/57.9%能耗优化，保持准确率


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型存在计算冗余问题，静态优化方法难以平衡效率与精度。ARS旨在动态识别并抑制过度思考现象，实现更高效的推理过程

Method: 采用多检查点确定性评估机制，设计渐进式抑制阈值，通过实时监测模型置信度自动终止冗余推理步骤

Result: 在数学推理基准测试中实现最高53% token减少、46.1%延迟降低和57.9%能耗下降，准确率维持或提升

Conclusion: ARS通过动态抑制机制显著提升推理效率，适用于不同模型架构，为语言模型优化提供新方向

Abstract: Large Reasoning Language Models (LRLMs or LRMs) demonstrate remarkable
capabilities in complex reasoning tasks, but suffer from significant
computational inefficiencies due to overthinking phenomena. Existing efficient
reasoning methods face the challenge of balancing reasoning quality with
inference cost reduction. We propose \textbf{Adaptive Reasoning Suppression
(ARS)}, a novel training-free approach that dynamically suppresses redundant
reasoning steps while preserving accuracy through adaptive certainty
monitoring. ARS introduces a multi-checkpoint certainty estimation mechanism
with progressive suppression thresholds, achieving superior efficiency compared
to static suppression methods. Our extensive evaluation across mathematical
reasoning benchmarks using multiple model architectures demonstrates that ARS
achieves up to 53%, 46.1%, and 57.9% in token, latency and energy reduction,
while maintaining or improving accuracy.

</details>


### [64] [Automated Evaluation can Distinguish the Good and Bad AI Responses to Patient Questions about Hospitalization](https://arxiv.org/abs/2510.00436)
*Sarvesh Soni,Dina Demner-Fushman*

Main category: cs.AI

TL;DR: 研究证实基于临床参考的自动化评估能有效评估医疗AI回答质量，实现高效规模化评估。


<details>
  <summary>Details</summary>
Motivation: 人工专家评审存在效率低、成本高的问题，需开发可靠自动化评估方法以支持AI医疗问答系统的规模化应用。

Method: 收集28个AI系统对100个住院病例的2800条回答，通过专家撰写的参考答案锚定指标，从问题回答准确性、临床证据运用、医学知识应用三个维度进行系统评估。

Result: 自动化评估结果与专家评分高度一致，证明精心设计的自动化评估体系具有规模化比较AI系统的潜力。

Conclusion: 基于临床参考的自动化评估体系可有效替代人工评估，为医疗AI系统比较提供高效解决方案，支持医患沟通场景的规模化应用。

Abstract: Automated approaches to answer patient-posed health questions are rising, but
selecting among systems requires reliable evaluation. The current gold standard
for evaluating the free-text artificial intelligence (AI) responses--human
expert review--is labor-intensive and slow, limiting scalability. Automated
metrics are promising yet variably aligned with human judgments and often
context-dependent. To address the feasibility of automating the evaluation of
AI responses to hospitalization-related questions posed by patients, we
conducted a large systematic study of evaluation approaches. Across 100 patient
cases, we collected responses from 28 AI systems (2800 total) and assessed them
along three dimensions: whether a system response (1) answers the question, (2)
appropriately uses clinical note evidence, and (3) uses general medical
knowledge. Using clinician-authored reference answers to anchor metrics,
automated rankings closely matched expert ratings. Our findings suggest that
carefully designed automated evaluation can scale comparative assessment of AI
systems and support patient-clinician communication.

</details>


### [65] [ACON: Optimizing Context Compression for Long-horizon LLM Agents](https://arxiv.org/abs/2510.00615)
*Minki Kang,Wei-Ning Chen,Dongge Han,Huseyin A. Inan,Lukas Wutschitz,Yanzhi Chen,Robert Sim,Saravan Rajmohan*

Main category: cs.AI

TL;DR: ACON框架通过优化上下文压缩降低26-54%内存占用，同时保持任务性能，并通过蒸馏技术提升小模型代理能力


<details>
  <summary>Details</summary>
Motivation: 长上下文导致内存成本增加和效率下降，现有压缩方法局限于单步任务或狭窄场景，需要通用解决方案

Method: 通过自然语言空间的压缩指南优化：分析失败轨迹原因，更新压缩策略；将优化后的压缩器蒸馏至小模型

Result: 在三个测试场景中内存峰值降低26-54%，蒸馏后保持95%准确率，小模型性能提升最高达46%

Conclusion: ACON有效平衡上下文压缩与任务性能，为长周期任务提供高效解决方案，并通过知识蒸馏增强小模型代理能力

Abstract: Large language models (LLMs) are increasingly deployed as agents in dynamic,
real-world environments, where success requires both reasoning and effective
tool use. A central challenge for agentic tasks is the growing context length,
as agents must accumulate long histories of actions and observations. This
expansion raises costs and reduces efficiency in long-horizon tasks, yet prior
work on context compression has mostly focused on single-step tasks or narrow
applications. We introduce Agent Context Optimization (ACON), a unified
framework that optimally compresses both environment observations and
interaction histories into concise yet informative condensations. ACON
leverages compression guideline optimization in natural language space: given
paired trajectories where full context succeeds but compressed context fails,
capable LLMs analyze the causes of failure, and the compression guideline is
updated accordingly. Furthermore, we propose distilling the optimized LLM
compressor into smaller models to reduce the overhead of the additional module.
Experiments on AppWorld, OfficeBench, and Multi-objective QA show that ACON
reduces memory usage by 26-54% (peak tokens) while largely preserving task
performance, preserves over 95% of accuracy when distilled into smaller
compressors, and enhances smaller LMs as long-horizon agents with up to 46%
performance improvement.

</details>


### [66] [HARPA: A Testability-Driven, Literature-Grounded Framework for Research Ideation](https://arxiv.org/abs/2510.00620)
*Rosni Vasu,Peter Jansen,Pao Siangliulue,Cristina Sarasua,Abraham Bernstein,Peter Clark,Bhavana Dalvi Mishra*

Main category: cs.AI

TL;DR: HARPA系统通过模拟人类研究者的假设生成流程，显著提升了AI科学发现中假设提案的可行性和文献基础性，并在实验执行成功率上优于基准模型。


<details>
  <summary>Details</summary>
Motivation: 现有自动化科学发现工具存在假设检验性不足、文献基础薄弱且无法适应先前实验结果的问题，限制了其在真实科研场景中的应用。

Method: 1. 文献挖掘识别新兴趋势 → 2. 探索假设设计空间 → 3. 聚焦研究空白生成可检验假设 → 4. 基于实验反馈学习奖励模型优化假设评分

Result: 可行性(+0.78)和文献基础性(+0.85)显著提升；与CodeScientist协同实现更高执行成功率(20/40 vs 11/40)；奖励模型相对基线提升28%绝对增益

Conclusion: HARPA通过整合文献驱动假设生成与实验反馈学习机制，推动了AI科学发现领域的发展，验证了专家可行性判断与实验成功率的正相关性。

Abstract: While there has been a surge of interest in automated scientific discovery
(ASD), especially with the emergence of LLMs, it remains challenging for tools
to generate hypotheses that are both testable and grounded in the scientific
literature. Additionally, existing ideation tools are not adaptive to prior
experimental outcomes. We developed HARPA to address these challenges by
incorporating the ideation workflow inspired by human researchers. HARPA first
identifies emerging research trends through literature mining, then explores
hypothesis design spaces, and finally converges on precise, testable hypotheses
by pinpointing research gaps and justifying design choices. Our evaluations
show that HARPA-generated hypothesis-driven research proposals perform
comparably to a strong baseline AI-researcher across most qualitative
dimensions (e.g., specificity, novelty, overall quality), but achieve
significant gains in feasibility(+0.78, p$<0.05$, bootstrap) and groundedness
(+0.85, p$<0.01$, bootstrap) on a 10-point Likert scale. When tested with the
ASD agent (CodeScientist), HARPA produced more successful executions (20 vs. 11
out of 40) and fewer failures (16 vs. 21 out of 40), showing that expert
feasibility judgments track with actual execution success. Furthermore, to
simulate how researchers continuously refine their understanding of what
hypotheses are both testable and potentially interesting from experience, HARPA
learns a reward model that scores new hypotheses based on prior experimental
outcomes, achieving approx. a 28\% absolute gain over HARPA's untrained
baseline scorer. Together, these methods represent a step forward in the field
of AI-driven scientific discovery.

</details>


### [67] [Expected Attention: KV Cache Compression by Estimating Attention from Future Queries Distribution](https://arxiv.org/abs/2510.00636)
*Alessio Devoto,Maximilian Jeblick,Simon Jégou*

Main category: cs.AI

TL;DR: 提出无需训练的Expected Attention方法实现KV缓存压缩，并发布KVPress工具库


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力分数的KV缓存剪枝存在未来token不可见和现代实现不保留完整注意力矩阵的缺陷

Method: 通过LLM激活的分布特性预测未来查询对KV对的关注程度，计算闭式预期注意力分数进行剪枝

Result: 在预填充和解码阶段均实现有效压缩，性能优于当前最先进基线方法

Conclusion: 该方法首次实现全流程无损压缩，KVPress库为研究者提供包含20+技术的统一评测平台

Abstract: Memory consumption of the Key-Value (KV) cache represents a major bottleneck
for efficient large language model inference. While attention-score-based KV
cache pruning shows promise, it faces critical practical limitations: attention
scores from future tokens are unavailable during compression, and modern
implementations like Flash Attention do not materialize the full attention
matrix, making past scores inaccessible. To overcome these challenges, we
introduce $\textbf{Expected Attention, a training-free compression method}$
that estimates KV pairs importance by predicting how future queries will attend
to them. Our approach leverages the distributional properties of LLM
activations to compute expected attention scores in closed form for each KV
pair. These scores enable principled ranking and pruning of KV pairs with
minimal impact on the residual stream, achieving effective compression without
performance degradation. Importantly, our method operates seamlessly across
both prefilling and decoding phases, consistently outperforming
state-of-the-art baselines in both scenarios. Finally, $\textbf{we release
KVPress, a comprehensive library to enable researchers to implement and
benchmark KV cache compression methods, already including more than 20
techniques}$.

</details>


### [68] [Shape Happens: Automatic Feature Manifold Discovery in LLMs via Supervised Multi-Dimensional Scaling](https://arxiv.org/abs/2510.01025)
*Federico Tiblias,Irina Bigoulaeva,Jingcheng Niu,Simone Balloccu,Iryna Gurevych*

Main category: cs.AI

TL;DR: 提出监督多维缩放（SMDS）方法，自动发现语言模型中的多维特征流形结构，揭示其稳定性和动态调整特性


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于特定特征的几何结构发现，缺乏通用性，需要模型无关的自动特征流形探索方法

Method: 开发模型无关的SMDS方法，以时间推理为案例，分析不同特征形成的几何结构（圆形/直线/集群）

Result: 发现特征流形：1) 反映概念属性 2) 跨模型稳定 3) 支持推理过程 4) 随上下文动态重塑

Conclusion: 特征流形在实体推理中起核心作用，支持语言模型通过结构化表征进行编码和转换的推理机制

Abstract: The linear representation hypothesis states that language models (LMs) encode
concepts as directions in their latent space, forming organized,
multidimensional manifolds. Prior efforts focus on discovering specific
geometries for specific features, and thus lack generalization. We introduce
Supervised Multi-Dimensional Scaling (SMDS), a model-agnostic method to
automatically discover feature manifolds. We apply SMDS to temporal reasoning
as a case study, finding that different features form various geometric
structures such as circles, lines, and clusters. SMDS reveals many insights on
these structures: they consistently reflect the properties of the concepts they
represent; are stable across model families and sizes; actively support
reasoning in models; and dynamically reshape in response to context changes.
Together, our findings shed light on the functional role of feature manifolds,
supporting a model of entity-based reasoning in which LMs encode and transform
structured representations.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [69] [Optimizing What Matters: AUC-Driven Learning for Robust Neural Retrieval](https://arxiv.org/abs/2510.00137)
*Nima Sheikholeslami,Erfan Hosseini,Patrice Bechard,Srivatsava Daruru,Sai Rajeswar*

Main category: cs.IR

TL;DR: 提出MW损失函数替代传统对比损失，通过最大化AUC指标优化检索模型性能，改善RAG等下游任务效果


<details>
  <summary>Details</summary>
Motivation: 传统NCE损失函数无法有效分离相关/不相关文档的得分差异，导致检索模型校准差且在下游任务表现欠佳

Method: 基于Mann-Whitney U统计量构造损失函数，通过最小化正负样本对的得分差异二元交叉熵直接优化AUC指标

Result: MW损失训练的检索模型在AUC和标准检索指标上全面超越对比学习方法，模型校准性和判别力显著提升

Conclusion: MW损失是替代对比损失的更优方案，特别适合RAG等高风险应用场景，建议将AUC作为检索模型的核心评估指标

Abstract: Dual-encoder retrievers depend on the principle that relevant documents
should score higher than irrelevant ones for a given query. Yet the dominant
Noise Contrastive Estimation (NCE) objective, which underpins Contrastive Loss,
optimizes a softened ranking surrogate that we rigorously prove is
fundamentally oblivious to score separation quality and unrelated to AUC. This
mismatch leads to poor calibration and suboptimal performance in downstream
tasks like retrieval-augmented generation (RAG). To address this fundamental
limitation, we introduce the MW loss, a new training objective that maximizes
the Mann-Whitney U statistic, which is mathematically equivalent to the Area
under the ROC Curve (AUC). MW loss encourages each positive-negative pair to be
correctly ranked by minimizing binary cross entropy over score differences. We
provide theoretical guarantees that MW loss directly upper-bounds the AoC,
better aligning optimization with retrieval goals. We further promote ROC
curves and AUC as natural threshold free diagnostics for evaluating retriever
calibration and ranking quality. Empirically, retrievers trained with MW loss
consistently outperform contrastive counterparts in AUC and standard retrieval
metrics. Our experiments show that MW loss is an empirically superior
alternative to Contrastive Loss, yielding better-calibrated and more
discriminative retrievers for high-stakes applications like RAG.

</details>


### [70] [Milco: Learned Sparse Retrieval Across Languages via a Multilingual Connector](https://arxiv.org/abs/2510.00671)
*Thong Nguyen,Yibin Lei,Jia-Huei Ju,Eugene Yang,Andrew Yates*

Main category: cs.IR

TL;DR: MILCO提出了一种多语言稀疏检索架构，通过共享英语词汇空间映射和LexEcho增强机制，在多语言检索任务中实现SOTA性能并支持动态效率调整。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏检索方法难以扩展到非英语场景，且跨语言投影时容易丢失低频实体信息。

Method: 采用两阶段训练（稀疏对齐预训练+对比训练）结合LexEcho机制，通过[ECHO]令牌保留源语言特征增强表示鲁棒性。

Result: 在标准基准上超越BGE-M3/Qwen3-Embed等模型，文档表示压缩至30维时仍优于0.6B参数模型的1024维表现。

Conclusion: MILCO证明了稀疏检索在跨语言场景的潜力，其效率-效果平衡为实际部署提供了新思路。

Abstract: Learned Sparse Retrieval (LSR) combines the efficiency of bi-encoders with
the transparency of lexical matching, but existing approaches struggle to scale
beyond English. We introduce MILCO, an LSR architecture that maps queries and
documents from different languages into a shared English lexical space via a
multilingual connector. MILCO is trained with a specialized two-stage regime
that combines Sparse Alignment Pretraining with contrastive training to provide
representation transparency and effectiveness while mitigating semantic
collapse. Motivated by the observation that uncommon entities are often lost
when projected into English, we propose a new LexEcho head, which enhances
robustness by augmenting the English lexical representation with a
source-language view obtained through a special [ECHO] token. MILCO achieves
state-of-the-art multilingual and cross-lingual LSR performance, outperforming
leading dense, sparse, and multi-vector baselines such as BGE-M3 and
Qwen3-Embed on standard multilingual benchmarks, while supporting dynamic
efficiency through post-hoc pruning. Notably, when using mass-based pruning to
reduce document representations to only 30 active dimensions on average, MILCO
560M outperforms the similarly-sized Qwen3-Embed 0.6B with 1024 dimensions.

</details>


### [71] [Bridging Language Gaps: Advances in Cross-Lingual Information Retrieval with Multilingual LLMs](https://arxiv.org/abs/2510.00908)
*Roksana Goworek,Olivia Macmillan-Scott,Eda B. Özyiğit*

Main category: cs.IR

TL;DR: 跨语言信息检索（CLIR）从基于翻译的方法发展为嵌入驱动和生成技术，面临数据不平衡等挑战，需构建鲁棒且包容的系统。


<details>
  <summary>Details</summary>
Motivation: 传统CLIR方法将检索与跨语言能力孤立处理，新兴嵌入技术和多语言大语言模型（LLMs）推动了该领域发展，需系统性综述以指导未来方向。

Method: 通过综述CLIR演变历程，分析翻译方法、跨语言嵌入及多语言LLMs的应用，总结核心组件（如查询扩展、排序）、评估方法及资源。

Result: 识别数据不平衡、语言变异等挑战，提出结合多语言处理与检索技术，构建适应性强的系统作为未来方向。

Conclusion: CLIR需整合信息检索与多语言处理进展，通过优化嵌入对齐和生成技术解决挑战，实现公平高效的跨语言检索。

Abstract: Cross-lingual information retrieval (CLIR) addresses the challenge of
retrieving relevant documents written in languages different from that of the
original query. Research in this area has typically framed the task as
monolingual retrieval augmented by translation, treating retrieval methods and
cross-lingual capabilities in isolation. Both monolingual and cross-lingual
retrieval usually follow a pipeline of query expansion, ranking, re-ranking
and, increasingly, question answering. Recent advances, however, have shifted
from translation-based methods toward embedding-based approaches and leverage
multilingual large language models (LLMs), for which aligning representations
across languages remains a central challenge. The emergence of cross-lingual
embeddings and multilingual LLMs has introduced a new paradigm, offering
improved retrieval performance and enabling answer generation. This survey
provides a comprehensive overview of developments from early translation-based
methods to state-of-the-art embedding-driven and generative techniques. It
presents a structured account of core CLIR components, evaluation practices,
and available resources. Persistent challenges such as data imbalance and
linguistic variation are identified, while promising directions are suggested
for advancing equitable and effective cross-lingual information retrieval. By
situating CLIR within the broader landscape of information retrieval and
multilingual language processing, this work not only reviews current
capabilities but also outlines future directions for building retrieval systems
that are robust, inclusive, and adaptable.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [72] [IA aplicada al análisis del conflicto Irán-Israel: Mapeo de discursos en YouTube](https://arxiv.org/abs/2510.00021)
*Alvaro Vallejo Ramírez*

Main category: cs.SI

TL;DR: 基于12万条YouTube评论分析2025年伊朗-以色列冲突数字叙事，揭示算法偏见与话语不对称现象


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示数字媒体中冲突叙事的政治立场分布，并探讨算法如何塑造公众舆论倾向

Method: 混合方法设计与三角验证：定量采用BERT/XLM-RoBERTa模型进行评论分类，定性结合批判性媒体分析与人工标注

Result: 数字空间呈现亲巴勒斯坦/反美以话语主导，伊朗首次成为核心叙事主体，算法机制强化特定话语传播

Conclusion: 该研究开创了计算分析与哲学批判结合的方法框架，揭示YouTube国际冲突讨论中被忽视的叙事权力结构

Abstract: Purpose. This study analyzes the digital representation of the Iran-Israel
conflict that occurred in June 2025, based on 120,000 comments posted on
YouTube. It sought to identify discursive positions regarding the actors
involved and to examine how media and algorithmic biases shape digital
conversations. Methodology. A mixed-methods design with triangulation was
adopted. In the quantitative phase, natural language processing techniques and
machine learning models (BERT and XLM-RoBERTa) were used to classify comments
into ten categories. In the qualitative phase, a critical analysis of media
context and ideological narratives was conducted, complemented by manual
annotation and supervised training. This strategy enabled the integration of
statistical robustness with contextual understanding. Results and conclusions.
The findings reveal a clear overrepresentation of pro-Palestinian and
anti-United States/Israel discourses, while pro-United States and
anti-Palestinian positions were marginal. Iran, usually rendered invisible in
global media, emerged as a central actor in the digital conversation during the
conflict, suggesting a narrative shift away from previous hegemonic frameworks.
Likewise, the results confirm the influence of algorithmic biases in amplifying
certain discourses while limiting others. Original contributions. This work
combines computational analysis and philosophical critique for the study of
digital controversies, providing a methodological framework replicable in
geopolitical contexts. It is one of the first Spanish-language studies to map,
through artificial intelligence and critical analysis, discourses on an
international conflict on YouTube, highlighting asymmetries and narrative
disputes that are often overlooked.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [73] [Unpacking Musical Symbolism in Online Communities: Content-Based and Network-Centric Approaches](https://arxiv.org/abs/2510.00006)
*Kajwan Ziaoddini*

Main category: cs.SD

TL;DR: 整合音乐信息检索与网络分析，揭示十年间主流音乐能量下降（79→58）、舞蹈性上升（59→73）的趋势，及歌词中人称代词主导的叙事结构。


<details>
  <summary>Details</summary>
Motivation: 探究在线社区中音乐符号的生产传播机制，解析商业偏好如何通过声音特征与歌词结构影响集体参与。

Method: 收集275首热门歌曲的音频特征（能量/舞蹈性/响度等）与歌词数据，构建统计模型分析时间趋势、词汇共现网络及流派情绪图谱。

Result: 能量与响度强相关（r=0.74），R&B情绪最积极（valence=96）；歌词中'I/you/me'高频共现，拉丁流派舞蹈性高但情绪值低（37）。

Conclusion: 方法论贡献多模态分析流程，揭示主流音乐趋向放松节奏+人际叙事策略，平衡商业传播与亚文化身份表达。

Abstract: This paper examines how musical symbolism is produced and circulated in
online communities by combining content-based music analysis with a lightweight
network perspective on lyrics. Using a curated corpus of 275 chart-topping
songs enriched with audio descriptors (energy, danceability, loudness,
liveness, valence, acousticness, speechiness, popularity) and full lyric
transcripts, we build a reproducible pipeline that (i) quantifies temporal
trends in sonic attributes, (ii) models lexical salience and co-occurrence, and
(iii) profiles mood by genre. We find a decade-long decline in energy (79 ->
58) alongside a rise in danceability (59 -> 73); valence peaks in 2013 (63) and
dips in 2014-2016 (42) before partially recovering. Correlation analysis shows
strong coupling of energy with loudness (r = 0.74) and negative associations
for acousticness with both energy (r = -0.54) and loudness (r = -0.51);
danceability is largely orthogonal to other features (|r| < 0.20). Lyric
tokenization (>114k tokens) reveals a pronoun-centric lexicon "I/you/me/my" and
a dense co-occurrence structure in which interpersonal address anchors
mainstream narratives. Mood differs systematically by style: R&B exhibits the
highest mean valence (96), followed by K-Pop/Pop (77) and Indie/Pop (70),
whereas Latin/Reggaeton is lower (37) despite high danceability. Read through a
subcultural identity lens, these patterns suggest the mainstreaming of
previously peripheral codes and a commercial preference for relaxed yet
rhythmically engaging productions that sustain collective participation without
maximal intensity. Methodologically, we contribute an integrated
MIR-plus-network workflow spanning summary statistics, correlation structure,
lexical co-occurrence matrices, and genre-wise mood profiling that is robust to
modality sparsity and suitable for socially aware recommendation or
community-level diffusion studies.

</details>


### [74] [When Silence Matters: The Impact of Irrelevant Audio on Text Reasoning in Large Audio-Language Models](https://arxiv.org/abs/2510.00626)
*Chen-An Li,Tzu-Han Lin,Hung-yi Lee*

Main category: cs.SD

TL;DR: 研究发现无关音频（静音/噪声）会显著降低音频-语言模型在文本推理任务中的准确性和稳定性，模型规模增大可部分缓解但无法根除，自洽性方法能提升稳定性但增加计算开销。


<details>
  <summary>Details</summary>
Motivation: 探索大型音频-语言模型在含无关噪声的真实场景中的鲁棒性表现，揭示跨模态干扰对纯文本推理任务的影响机制。

Method: 通过在三个文本基准测试中注入静音、合成噪声和环境声，分析模型输出的准确性、波动性及温度参数的影响，并测试提示工程和自洽性解码的缓解效果。

Result: 非信息性音频使准确率下降且预测波动加剧，静音的干扰强度与噪声相当；模型规模扩大提升韧性但漏洞仍存；自洽性方法可使预测稳定性提升37%但计算量倍增。

Conclusion: 跨模态干扰构成新型鲁棒性挑战，需设计选择性信息融合机制，在保持多模态优势的同时避免无关输入对核心推理能力的侵蚀。

Abstract: Large audio-language models (LALMs) unify speech and text processing, but
their robustness in noisy real-world settings remains underexplored. We
investigate how irrelevant audio, such as silence, synthetic noise, and
environmental sounds, affects text reasoning tasks where audio is unnecessary.
Across three text-based benchmarks, we find that even non-informative audio
reduces accuracy and increases prediction volatility; the severity of
interference scales with longer durations, higher amplitudes, and elevated
decoding temperatures. Silence, often assumed neutral, destabilizes outputs as
strongly as synthetic noise. While larger models show greater resilience,
vulnerabilities persist across all evaluated systems. We further test
mitigation strategies and find that prompting shows limited effectiveness,
whereas self-consistency improves stability at the cost of increased
computation. Our results reveal cross-modal interference as a key robustness
challenge and highlight the need for efficient fusion strategies that preserve
reasoning performance in the presence of irrelevant inputs.

</details>


### [75] [Hearing the Order: Investigating Selection Bias in Large Audio-Language Models](https://arxiv.org/abs/2510.00628)
*Yu-Xiang Lin,Chen-An Li,Sheng-Lun Wei,Po-Chun Chen,Hsin-Hsi Chen,Hung-yi Lee*

Main category: cs.SD

TL;DR: 研究发现大型音频语言模型存在选项顺序敏感性，改变选项顺序可导致24%的性能波动并影响模型排名，提出排列策略可缓解该偏差


<details>
  <summary>Details</summary>
Motivation: 现有评估未充分考虑选项顺序对LALMs预测结果的影响，这种选择偏差会威胁模型评估的可靠性及实际应用效果

Method: 在6个LALMs模型上开展跨3个基准测试及其语音版本的实验，采用选项随机排列策略分析性能波动，并验证排列组合方法的有效性

Result: 选项顺序改变导致最大24%的性能差异，模型排名出现反转；排列策略在多数情况下能有效缓解选择偏差问题

Conclusion: 首次系统揭示LALMs中的选项顺序敏感性缺陷，质疑现有评估可靠性，为改进模型鲁棒性评估提供新方向

Abstract: Large audio-language models (LALMs) are often used in tasks that involve
reasoning over ordered options. An open question is whether their predictions
are influenced by the order of answer choices, which would indicate a form of
selection bias and undermine their reliability. In this paper, we identify and
analyze this problem in LALMs. We demonstrate that no model is immune to this
bias through extensive experiments on six LALMs across three widely used
benchmarks and their spoken counterparts. Shuffling the order of answer options
can cause performance fluctuations of up to 24% and even change model rankings,
raising concerns about the reliability of current evaluation practices. We also
study permutation-based strategies and show that they can mitigate bias in most
cases. Our work represents the first systematic investigation of this issue in
LALMs, and we hope it raises awareness and motivates further research in this
direction.

</details>


### [76] [From Scores to Preferences: Redefining MOS Benchmarking for Speech Quality Reward Modeling](https://arxiv.org/abs/2510.00743)
*Yifei Cao,Changhao Jiang,Jiabao Zhuang,Jiajun Sun,Ming Zhang,Zhiheng Xi,Hui Li,Shihan Dou,Yuran Wang,Yunke Zhang,Tao Ji,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.SD

TL;DR: 提出统一基准MOS-RMBench重构语音质量评估范式，系统评估三类奖励模型并改进生成式模型在细粒度质量判别上的表现


<details>
  <summary>Details</summary>
Motivation: 传统语音质量评估依赖主观评分(MOS)，存在人工标注成本高、评分标准不一致、可重复性差等局限性

Method: 1. 构建MOS-RMBench基准将多数据集统一为偏好比较形式
2. 系统研究标量/半标量/生成式三种奖励建模范式
3. 提出融合MOS差异的感知型生成式奖励模型(MOS-aware GRM)

Result: 1. 标量模型整体最优(准确率>74%)
2. 多数模型在合成语音上表现显著下降
3. 所有模型在微小MOS差异样本对表现差
4. MOS-aware GRM显著提升细粒度判别能力

Conclusion: 建立语音质量评估的统一基准与方法框架，通过自适应奖励缩放机制缩小标量模型与生成模型在困难样本上的性能差距

Abstract: Assessing the perceptual quality of synthetic speech is crucial for guiding
the development and refinement of speech generation models. However, it has
traditionally relied on human subjective ratings such as the Mean Opinion Score
(MOS), which depend on manual annotations and often suffer from inconsistent
rating standards and poor reproducibility. To address these limitations, we
introduce MOS-RMBench, a unified benchmark that reformulates diverse MOS
datasets into a preference-comparison setting, enabling rigorous evaluation
across different datasets. Building on MOS-RMBench, we systematically construct
and evaluate three paradigms for reward modeling: scalar reward models,
semi-scalar reward models, and generative reward models (GRMs). Our experiments
reveal three key findings: (1) scalar models achieve the strongest overall
performance, consistently exceeding 74% accuracy; (2) most models perform
considerably worse on synthetic speech than on human speech; and (3) all models
struggle on pairs with very small MOS differences. To improve performance on
these challenging pairs, we propose a MOS-aware GRM that incorporates an
MOS-difference-based reward function, enabling the model to adaptively scale
rewards according to the difficulty of each sample pair. Experimental results
show that the MOS-aware GRM significantly improves fine-grained quality
discrimination and narrows the gap with scalar models on the most challenging
cases. We hope this work will establish both a benchmark and a methodological
framework to foster more rigorous and scalable research in automatic speech
quality assessment.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [77] [QSearchNet: A Quantum Walk Search Framework for Link Prediction](https://arxiv.org/abs/2510.00325)
*Priyank Dubey*

Main category: quant-ph

TL;DR: 提出量子启发框架QSearchNet，通过量子行走和幅度放大技术提升图网络链路预测性能


<details>
  <summary>Details</summary>
Motivation: 经典启发式方法在整合局部/全局图结构特征和适应复杂依赖方面存在局限，量子计算通过叠加态和干涉效应为解决这些问题提供了新路径

Method: 结合离散时间量子行走(DTQW)的动态传播和Grover幅度放大技术，利用量子反射和相位翻转操作对齐干涉模式

Result: 在真实网络实验中展现出竞争优势，特别是在包含困难负样本的现实评估条件下表现突出

Conclusion: 量子计算机制可有效捕捉多跳依赖关系，为复杂网络分析提供了新的框架设计思路

Abstract: Link prediction is one of the fundamental problems in graph theory, critical
for understanding and forecasting the evolution of complex systems like social
and biological networks. While classical heuristics capture certain aspects of
graph topology, they often struggle to optimally integrate local and global
structural information or adapt to complex dependencies. Quantum computing
offers a powerful alternative by leveraging superposition for simultaneous
multi-path exploration and interference-driven integration of both local and
global graph features. In this work, we introduce QSearchNet, a
quantum-inspired framework based on Discrete-Time Quantum Walk (DTQW) dynamics
and Grover's amplitude amplification. QSearchNet simulates a topology-aware
quantum evolution to propagate amplitudes across multiple nodes simultaneously.
By aligning interference patterns through quantum reflection and oracle-like
phase-flip operation, it adaptively prioritizes multi-hop dependencies and
amplifies structurally relevant paths corresponding to potential connections.
Experiments on diverse real-world networks demonstrate competitive performance,
particularly with hard negative samples under realistic evaluation conditions.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [78] [What You See is What You Ask: Evaluating Audio Descriptions](https://arxiv.org/abs/2510.00808)
*Divy Kala,Eshika Khandelwal,Makarand Tapaswi*

Main category: cs.CV

TL;DR: 提出ADQA评估基准，通过长视频片段测试音频描述质量，揭示现有生成方法与人工撰写的显著差距


<details>
  <summary>Details</summary>
Motivation: 现有音频描述研究局限于短片段和单一参考标准，无法有效衡量帮助视障用户理解叙事的实际效果，需建立更贴近真实场景的评估体系

Method: 1. 量化分析双AD轨迹的主观差异 2. 构建包含视觉欣赏(VA)和叙事理解(NU)问题的QA基准 3. 设计长视频段评估框架并建立公共排行榜

Result: ADQA测试显示当前自动生成方法在叙事准确性和视觉细节传达上显著落后于人工AD（准确率差值达35%）

Conclusion: 建议开发考虑时间连贯性的生成模型，建立多维度评估标准，并开放ADQA平台推动领域发展

Abstract: Audio descriptions (ADs) narrate important visual details in movies, enabling
Blind and Low Vision (BLV) users to understand narratives and appreciate visual
details. Existing works in automatic AD generation mostly focus on few-second
trimmed clips, and evaluate them by comparing against a single ground-truth
reference AD. However, writing ADs is inherently subjective. Through alignment
and analysis of two independent AD tracks for the same movies, we quantify the
subjectivity in when and whether to describe, and what and how to highlight.
Thus, we show that working with trimmed clips is inadequate. We propose ADQA, a
QA benchmark that evaluates ADs at the level of few-minute long, coherent video
segments, testing whether they would help BLV users understand the story and
appreciate visual details. ADQA features visual appreciation (VA) questions
about visual facts and narrative understanding (NU) questions based on the
plot. Through ADQA, we show that current AD generation methods lag far behind
human-authored ADs. We conclude with several recommendations for future work
and introduce a public leaderboard for benchmarking.

</details>


### [79] [Can World Models Benefit VLMs for World Dynamics?](https://arxiv.org/abs/2510.00855)
*Kevin Zhang,Kuangzhi Ge,Xiaowei Chi,Renrui Zhang,Shaojun Shi,Zhen Dong,Sirui Han,Shanghang Zhang*

Main category: cs.CV

TL;DR: 探索生成式世界模型在视觉-语言模型中的应用潜力，提出Dynamic Vision Aligner方法显著提升空间推理能力


<details>
  <summary>Details</summary>
Motivation: 探究视频基础模型是否能够替代传统视觉编码范式，实现通用多模态理解

Method: 将视频扩散模型重新用作生成式编码器，通过单次去噪步骤获取视觉嵌入特征(DyVA方法)

Result: 在视觉推理任务中超越开源和商业基线，实现SOTA性能，单图像模型具备多帧推理能力

Conclusion: 世界模型的运动一致性内部化特性为构建新型VLMs指明方向，系统化模型设计探索为通用视觉学习提供路径

Abstract: Trained on internet-scale video data, generative world models are
increasingly recognized as powerful world simulators that can generate
consistent and plausible dynamics over structure, motion, and physics. This
raises a natural question: with the advent of strong video foundational models,
might they supplant conventional vision encoder paradigms for general-purpose
multimodal understanding? While recent studies have begun to explore the
potential of world models on common vision tasks, these explorations typically
lack a systematic investigation of generic, multimodal tasks. In this work, we
strive to investigate the capabilities when world model priors are transferred
into Vision-Language Models: we re-purpose a video diffusion model as a
generative encoder to perform a single denoising step and treat the resulting
latents as a set of visual embedding. We empirically investigate this class of
models, which we refer to as World-Language Models (WorldLMs), and we find that
generative encoders can capture latents useful for downstream understanding
that show distinctions from conventional encoders. Naming our best-performing
variant Dynamic Vision Aligner (DyVA), we further discover that this method
significantly enhances spatial reasoning abilities and enables single-image
models to perform multi-frame reasoning. Through the curation of a suite of
visual reasoning tasks, we find DyVA to surpass both open-source and
proprietary baselines, achieving state-of-the-art or comparable performance. We
attribute these gains to WorldLM's inherited motion-consistency internalization
from video pre-training. Finally, we systematically explore extensive model
designs to highlight promising directions for future work. We hope our study
can pave the way for a new family of VLMs that leverage priors from world
models and are on a promising path towards generalist vision learners.

</details>


### [80] [Authentic Discrete Diffusion Model](https://arxiv.org/abs/2510.01047)
*Xiao Li,Jiaqi Zhang,Shuxiang Zhang,Tianshui Chen,Liang Lin,Guangrun Wang*

Main category: cs.CV

TL;DR: ADD框架通过one-hot空间直接保持扩散特性，改进传统伪离散扩散方法，在分类和文本生成任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 传统伪离散扩散方法依赖连续潜在空间扩散或掩码策略，缺乏真正的离散扩散特性。需要建立判别式与生成式学习的桥梁

Method: 1. 直接使用浮点编码的one-hot数据作为输入
2. 提出时间步条件化交叉熵损失
3. 在扩散输出与原始标签间建立联系

Result: 分类任务性能超越基线，图像描述生成表现出色，消融实验验证各组件有效性

Conclusion: ADD框架开创了真正的离散扩散范式，兼具判别与生成能力，为多任务统一模型提供新思路

Abstract: We propose an Authentic Discrete Diffusion (ADD) framework that fundamentally
redefines prior pseudo-discrete approaches by preserving core diffusion
characteristics directly in the one-hot space through a suite of coordinated
mechanisms. Unlike conventional "pseudo" discrete diffusion (PDD) methods, ADD
reformulates the diffusion input by directly using float-encoded one-hot class
data, without relying on diffusing in the continuous latent spaces or masking
policies. At its core, a timestep-conditioned cross-entropy loss is introduced
between the diffusion model's outputs and the original one-hot labels. This
synergistic design establishes a bridge between discriminative and generative
learning. Our experiments demonstrate that ADD not only achieves superior
performance on classification tasks compared to the baseline, but also exhibits
excellent text generation capabilities on Image captioning. Extensive ablations
validate the measurable gains of each component.

</details>


### [81] [Code2Video: A Code-centric Paradigm for Educational Video Generation](https://arxiv.org/abs/2510.01174)
*Yanzhe Chen,Kevin Qinghong Lin,Mike Zheng Shou*

Main category: cs.CV

TL;DR: Code2Video通过Python代码驱动三智能体协作框架（规划器/编码器/评审器），生成专业教学视频，相比直接代码生成效率提升40%且质量媲美人工教程。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在专业教学视频制作中存在学科知识整合、视觉结构精确性和过渡连贯性的不足，通过可编程渲染环境实现精准控制的教学视频生成。

Method: 1. 规划器（Planner）构建时序连贯的教学流程和视觉素材；2. 编码器（Coder）将指令转化为可执行代码（含作用域引导的自动修复）；3. 评审器（Critic）通过视觉锚点提示优化空间布局。

Result: 提出MMMC评估基准和TeachQuiz端到端指标，视频生成质量相比直接代码生成提升40%，视觉效果接近人类专业教程。

Conclusion: 代码驱动的框架在可扩展性、可解释性和可控性方面展现出潜力，为教学视频生成提供了新的技术路径。

Abstract: While recent generative models advance pixel-space video synthesis, they
remain limited in producing professional educational videos, which demand
disciplinary knowledge, precise visual structures, and coherent transitions,
limiting their applicability in educational scenarios. Intuitively, such
requirements are better addressed through the manipulation of a renderable
environment, which can be explicitly controlled via logical commands (e.g.,
code). In this work, we propose Code2Video, a code-centric agent framework for
generating educational videos via executable Python code. The framework
comprises three collaborative agents: (i) Planner, which structures lecture
content into temporally coherent flows and prepares corresponding visual
assets; (ii) Coder, which converts structured instructions into executable
Python codes while incorporating scope-guided auto-fix to enhance efficiency;
and (iii) Critic, which leverages vision-language models (VLM) with visual
anchor prompts to refine spatial layout and ensure clarity. To support
systematic evaluation, we build MMMC, a benchmark of professionally produced,
discipline-specific educational videos. We evaluate MMMC across diverse
dimensions, including VLM-as-a-Judge aesthetic scores, code efficiency, and
particularly, TeachQuiz, a novel end-to-end metric that quantifies how well a
VLM, after unlearning, can recover knowledge by watching the generated videos.
Our results demonstrate the potential of Code2Video as a scalable,
interpretable, and controllable approach, achieving 40% improvement over direct
code generation and producing videos comparable to human-crafted tutorials. The
code and datasets are available at https://github.com/showlab/Code2Video.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [82] [Stochastic Self-Organization in Multi-Agent Systems](https://arxiv.org/abs/2510.00685)
*Nurbek Tastan,Samuel Horvath,Karthik Nandakumar*

Main category: cs.MA

TL;DR: 提出SelfOrg框架，通过基于Shapley值的动态有向无环图优化多智能体通信结构，实现无需监督的自组织协作。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统的固定通信拓扑结构和外部依赖导致协作效率低下，需开发动态自适应的通信机制来提升协作质量。

Method: 采用响应条件化框架：1) 智能体独立生成响应 2) 用Shapley值近似评估贡献度 3) 构建有向无环图控制高贡献节点向低贡献节点的信息传播 4) 动态更新通信拓扑

Result: 实验显示在弱LLM环境下性能提升显著（传统方法失效），理论证明多智能体能提升正确率且正确响应主导信息流

Conclusion: SelfOrg通过动态通信机制突破任务/查询级优化，实现响应随机性下的鲁棒协作，为LLM多智能体系统提供高效解决方案

Abstract: Multi-agent systems (MAS) based on Large Language Models (LLMs) have the
potential to solve tasks that are beyond the reach of any single LLM. However,
this potential can only be realized when the collaboration mechanism between
agents is optimized. Specifically, optimizing the communication structure
between agents is critical for fruitful collaboration. Most existing approaches
rely on fixed topologies, pretrained graph generators, optimization over edges,
or employ external LLM judges, thereby adding to the complexity. In this work,
we introduce a response-conditioned framework that adapts communication
on-the-fly. Agents independently generate responses to the user query and
assess peer contributions using an approximation of the Shapley value. A
directed acyclic graph (DAG) is then constructed to regulate the propagation of
the responses among agents, which ensures stable and efficient message
transmission from high-contributing agents to others. This graph is dynamically
updated based on the agent responses from the previous collaboration round.
Since the proposed framework enables the self-organization of agents without
additional supervision or training, we refer to it as SelfOrg. The SelfOrg
framework goes beyond task- and query-level optimization and takes into account
the stochastic nature of agent responses. Experiments with both strong and weak
LLM backends demonstrate robust performance, with significant gains in the weak
regime where prior methods collapse. We also theoretically show that multiple
agents increase the chance of correctness and that the correct responses
naturally dominate the information flow.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [83] [Improving Code Localization with Repository Memory](https://arxiv.org/abs/2510.01003)
*Boshi Wang,Weijian Xu,Yunsheng Li,Mei Gao,Yujia Xie,Huan Sun,Dongdong Chen*

Main category: cs.SE

TL;DR: 通过利用仓库提交历史构建非参数化记忆，显著提升了代码定位框架LocAgent在SWE-bench基准测试中的性能表现


<details>
  <summary>Details</summary>
Motivation: 现有代码定位方法缺乏长期记忆机制，而人类开发者会自然积累仓库知识。提交历史作为记录代码库演变的宝贵资源未被充分利用

Method: 开发工具实现三层次记忆检索：1）近期提交历史与关联问题 2）通过提交模式识别的活跃代码模块功能摘要 3）建立错误类型与修复位置的关联模式

Result: 在SWE-bench-verified和SWE-bench-live基准测试中验证了记忆增强机制对LocAgent框架的性能提升

Conclusion: 该研究推动了具备经验积累能力的智能体开发，通过记忆机制模拟人类开发者的专业知识，为长期复杂任务提供新范式

Abstract: Code localization is a fundamental challenge in repository-level software
engineering tasks such as bug fixing. While existing methods equip language
agents with comprehensive tools/interfaces to fetch information from the
repository, they overlook the critical aspect of memory, where each instance is
typically handled from scratch assuming no prior repository knowledge. In
contrast, human developers naturally build long-term repository memory, such as
the functionality of key modules and associations between various bug types and
their likely fix locations. In this work, we augment language agents with such
memory by leveraging a repository's commit history - a rich yet underutilized
resource that chronicles the codebase's evolution. We introduce tools that
allow the agent to retrieve from a non-parametric memory encompassing recent
historical commits and linked issues, as well as functionality summaries of
actively evolving parts of the codebase identified via commit patterns. We
demonstrate that augmenting such a memory can significantly improve LocAgent, a
state-of-the-art localization framework, on both SWE-bench-verified and the
more recent SWE-bench-live benchmarks. Our research contributes towards
developing agents that can accumulate and leverage past experience for
long-horizon tasks, more closely emulating the expertise of human developers.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [84] [Linear Regression in p-adic metric spaces](https://arxiv.org/abs/2510.00043)
*Gregory D. Baker,Scott McCallum,Dirk Pattinson*

Main category: cs.LG

TL;DR: 提出在p-adic度量空间进行机器学习以更好处理层次数据，证明其数学定理并应用于自然语言处理


<details>
  <summary>Details</summary>
Motivation: 传统欧几里得度量无法有效捕捉层次数据的离散分支特性，需寻找更适配的数学框架

Method: 建立p-adic度量空间的数学理论框架，证明关键定理及其推论

Result: 1. n维p-adic平面须通过至少n+1个数据点 2. 多项式插值定理 3. 应用于语言分类和形态学建模

Conclusion: p-adic度量是处理层次数据结构的基础数学工具，选择实际观测点优于插值的特性适配离散数据特征

Abstract: Many real-world machine learning problems involve inherently hierarchical
data, yet traditional approaches rely on Euclidean metrics that fail to capture
the discrete, branching nature of hierarchical relationships. We present a
theoretical foundation for machine learning in p-adic metric spaces, which
naturally respect hierarchical structure. Our main result proves that an
n-dimensional plane minimizing the p-adic sum of distances to points in a
dataset must pass through at least n + 1 of those points -- a striking contrast
to Euclidean regression that highlights how p-adic metrics better align with
the discrete nature of hierarchical data. As a corollary, a polynomial of
degree n constructed to minimise the p-adic sum of residuals will pass through
at least n + 1 points. As a further corollary, a polynomial of degree n
approximating a higher degree polynomial at a finite number of points will
yield a difference polynomial that has distinct rational roots. We demonstrate
the practical significance of this result through two applications in natural
language processing: analyzing hierarchical taxonomies and modeling grammatical
morphology. These results suggest that p-adic metrics may be fundamental to
properly handling hierarchical data structures in machine learning. In
hierarchical data, interpolation between points often makes less sense than
selecting actual observed points as representatives.

</details>


### [85] [Thoughtbubbles: an Unsupervised Method for Parallel Thinking in Latent Space](https://arxiv.org/abs/2510.00219)
*Houjun Liu,Shikhar Murty,Christopher D. Manning,Róbert Csordás*

Main category: cs.LG

TL;DR: 提出Thoughtbubbles方法，通过潜在空间残差流分支实现Transformer的并行自适应计算，仅在预训练阶段学习即可提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于显式思维链的推理扩展方法受限于预训练不可用性和串行生成约束，需探索隐式自适应计算机制。

Method: 修改Transformer结构使残差流可学习分叉/删除，在网络中间形成计算气泡进行额外处理，完全通过语言建模损失驱动。

Result: 在OpenWebText/peS2o困惑度及HellaSwag/LAMBADA零样本任务中超越基线模型，参数规模150M-772M均有效。

Conclusion: 隐式自适应计算机制统一了训练推理行为，为预训练阶段学习复杂推理能力开辟了新路径。

Abstract: Current approaches for scaling inference-time compute in transformers rely on
training them to emit explicit chain-of-thought tokens before producing an
answer. While these methods are powerful, they are limited because they cannot
be applied during pretraining and are limited to only serially-generated,
natural-language verbalization to scale inference-time compute. In this work,
we propose Thoughtbubbles, a transformer variant that natively performs
parallel adaptive computation in latent space by learning to fork or delete
residual streams. Thus, tokens that require a large amount of computation can
form a "bubble" of cloned residuals in the middle of the network for additional
thinking. Crucially, this behavior is learned during pretraining with only
language modeling loss. Thoughtbubbles outperforms both standard decoder LMs as
well as non-adaptive parallel computation approaches on OpenWebText and peS2o
perplexity and in zero-shot evaluations such as HellaSwag and LAMBADA after
pretraining across 150M to 772M parameter scales. The implicit nature of our
method enables adaptive computation to be learned starting at pretraining time,
paving the way to unify train and test-time behavior for reasoning models.

</details>


### [86] [GDLNN: Marriage of Programming Language and Neural Networks for Accurate and Easy-to-Explain Graph Classification](https://arxiv.org/abs/2510.00374)
*Minseok Jeon,Seunghyun Park*

Main category: cs.LG

TL;DR: 提出了结合GDL编程语言与神经网络的新图学习架构GDLNN，兼具高准确性和解释性


<details>
  <summary>Details</summary>
Motivation: 为解决现有图神经网络（GNN）可解释性差且模型解释成本高的问题，通过可解释的图表示提升分类性能

Method: 采用领域特定编程语言GDL构建图表示层，将可解释的图特征与神经网络结合

Result: 在多数图分类基准数据集上准确率超越主流方法（如GNN），且应用现有解释技术即可获得高质量预测解释

Conclusion: GDLNN通过可解释的图表示层实现了性能与解释性的平衡，为图学习提供了新的解决方案

Abstract: We present GDLNN, a new graph machine learning architecture, for graph
classification tasks. GDLNN combines a domain-specific programming language,
called GDL, with neural networks. The main strength of GDLNN lies in its GDL
layer, which generates expressive and interpretable graph representations.
Since the graph representation is interpretable, existing model explanation
techniques can be directly applied to explain GDLNN's predictions. Our
evaluation shows that the GDL-based representation achieves high accuracy on
most graph classification benchmark datasets, outperforming dominant graph
learning methods such as GNNs. Applying an existing model explanation technique
also yields high-quality explanations of GDLNN's predictions. Furthermore, the
cost of GDLNN is low when the explanation cost is included.

</details>


### [87] [AbsTopK: Rethinking Sparse Autoencoders For Bidirectional Features](https://arxiv.org/abs/2510.00404)
*Xudong Zhu,Mohammad Mahdi Khalili,Zhihui Zhu*

Main category: cs.LG

TL;DR: 论文通过展开稀疏编码的近端梯度方法建立SAE框架，发现现有SAE的正则化限制导致双向概念表示不足，提出支持双向激活的AbsTopK SAE方法在多项任务中优于传统SAE和监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器（SAE）的正则化机制强制非负激活，导致单个特征无法表示双向语义概念（如性别对立），造成特征冗余和语义表示不完整。

Method: 基于ℓ0稀疏约束推导出AbsTopK SAE：通过保留最大绝对值激活的硬阈值处理，支持正负双向激活，实现更完整的双向概念编码。

Result: 在4个LLM和7类任务中，AbsTopK SAE重建精度提升25%，特征可解释性增强，单个特征可编码对立概念，性能超越传统SAE且匹配/超过需标注数据的有监督方法。

Conclusion: 突破传统SAE的非负性约束，AbsTopK SAE通过双向激活机制显著提升语义表示能力，为LLM可解释性研究提供新方法论，实验验证其超越现有无监督/有监督方法的能力。

Abstract: Sparse autoencoders (SAEs) have emerged as powerful techniques for
interpretability of large language models (LLMs), aiming to decompose hidden
states into meaningful semantic features. While several SAE variants have been
proposed, there remains no principled framework to derive SAEs from the
original dictionary learning formulation. In this work, we introduce such a
framework by unrolling the proximal gradient method for sparse coding. We show
that a single-step update naturally recovers common SAE variants, including
ReLU, JumpReLU, and TopK. Through this lens, we reveal a fundamental limitation
of existing SAEs: their sparsity-inducing regularizers enforce non-negativity,
preventing a single feature from representing bidirectional concepts (e.g.,
male vs. female). This structural constraint fragments semantic axes into
separate, redundant features, limiting representational completeness. To
address this issue, we propose AbsTopK SAE, a new variant derived from the
$\ell_0$ sparsity constraint that applies hard thresholding over the
largest-magnitude activations. By preserving both positive and negative
activations, AbsTopK uncovers richer, bidirectional conceptual representations.
Comprehensive experiments across four LLMs and seven probing and steering tasks
show that AbsTopK improves reconstruction fidelity, enhances interpretability,
and enables single features to encode contrasting concepts. Remarkably, AbsTopK
matches or even surpasses the Difference-in-Mean method, a supervised approach
that requires labeled data for each concept and has been shown in prior work to
outperform SAEs.

</details>


### [88] [Eyes-on-Me: Scalable RAG Poisoning through Transferable Attention-Steering Attractors](https://arxiv.org/abs/2510.00586)
*Yen-Shan Chen,Sian-Yao Huang,Cheng-Lin Yang,Yun-Nung Chen*

Main category: cs.LG

TL;DR: Eyes-on-Me通过模块化攻击组件实现可扩展的RAG数据投毒，将对抗文档分解为可重用注意力吸引器和聚焦区域，提升攻击成功率2.6倍


<details>
  <summary>Details</summary>
Motivation: 现有RAG数据投毒攻击需为每个目标短语单独优化投毒文档，扩展性差且成本高昂。研究旨在开发可复用组件降低攻击成本

Method: 通过注意力吸引器引导模型关注聚焦区域，利用经验验证的注意力头控制策略，实现语义诱饵和恶意指令的零成本迁移部署

Result: 在18种RAG场景中攻击成功率提升35.9个百分点达57.8%，单个吸引器可迁移至黑盒模型无需重新训练

Conclusion: 模块化组件对AI系统构成实际威胁，注意力集中与模型输出的强关联为可解释性研究提供新视角

Abstract: Existing data poisoning attacks on retrieval-augmented generation (RAG)
systems scale poorly because they require costly optimization of poisoned
documents for each target phrase. We introduce Eyes-on-Me, a modular attack
that decomposes an adversarial document into reusable Attention Attractors and
Focus Regions. Attractors are optimized to direct attention to the Focus
Region. Attackers can then insert semantic baits for the retriever or malicious
instructions for the generator, adapting to new targets at near zero cost. This
is achieved by steering a small subset of attention heads that we empirically
identify as strongly correlated with attack success. Across 18 end-to-end RAG
settings (3 datasets $\times$ 2 retrievers $\times$ 3 generators), Eyes-on-Me
raises average attack success rates from 21.9 to 57.8 (+35.9 points,
2.6$\times$ over prior work). A single optimized attractor transfers to unseen
black box retrievers and generators without retraining. Our findings establish
a scalable paradigm for RAG data poisoning and show that modular, reusable
components pose a practical threat to modern AI systems. They also reveal a
strong link between attention concentration and model outputs, informing
interpretability research.

</details>


### [89] [Mechanistic Interpretability as Statistical Estimation: A Variance Analysis of EAP-IG](https://arxiv.org/abs/2510.00845)
*Maxime Méloux,Maxime Peyrard,François Portet*

Main category: cs.LG

TL;DR: 该研究系统评估了当前最先进的电路发现方法EAP-IG的稳定性，发现其存在高结构方差和超参数敏感性，建议在可解释性研究中引入稳定性指标以提升科学严谨性。


<details>
  <summary>Details</summary>
Motivation: 针对机械可解释性领域缺乏统计严谨性的现状，作者主张将电路发现方法视为统计估计器，通过系统性扰动实验验证其结果的稳定性和可靠性。

Method: 采用输入重采样、提示改写、超参数调整、因果分析噪声注入等控制变量方法，在多个模型和任务中系统评估EAP-IG的方差与鲁棒性。

Result: 实验表明EAP-IG存在显著的结构不稳定性和超参数敏感性，其发现的可重复性受到质疑。基于此提出了包含稳定性指标报告的研究规范建议。

Conclusion: 研究揭示了当前可解释性方法存在的统计基础缺陷，呼吁建立系统性评估框架和标准化报告机制，推动机械可解释性向更严谨的实证科学发展。

Abstract: The development of trustworthy artificial intelligence requires moving beyond
black-box performance metrics toward an understanding of models' internal
computations. Mechanistic Interpretability (MI) aims to meet this need by
identifying the algorithmic mechanisms underlying model behaviors. Yet, the
scientific rigor of MI critically depends on the reliability of its findings.
In this work, we argue that interpretability methods, such as circuit
discovery, should be viewed as statistical estimators, subject to questions of
variance and robustness. To illustrate this statistical framing, we present a
systematic stability analysis of a state-of-the-art circuit discovery method:
EAP-IG. We evaluate its variance and robustness through a comprehensive suite
of controlled perturbations, including input resampling, prompt paraphrasing,
hyperparameter variation, and injected noise within the causal analysis itself.
Across a diverse set of models and tasks, our results demonstrate that EAP-IG
exhibits high structural variance and sensitivity to hyperparameters,
questioning the stability of its findings. Based on these results, we offer a
set of best-practice recommendations for the field, advocating for the routine
reporting of stability metrics to promote a more rigorous and statistically
grounded science of interpretability.

</details>


### [90] [The data-quality illusion: Rethinking Classifier-based quality filtering for LLM Pretraining](https://arxiv.org/abs/2510.00866)
*Thiziri Nait Saada,Louis Bethune,Michal Klein,David Grangier,Marco Cuturi,Pierre Ablin*

Main category: cs.LG

TL;DR: CQF方法虽提升下游任务，但未改善高质量数据语言建模，并揭示其隐式过滤高质量数据的问题


<details>
  <summary>Details</summary>
Motivation: 探究基于分类器的质量过滤方法（CQF）在数据质量评估中的实际效果和局限性

Method: 通过分析CQF对预训练数据的影响，并与随机标记排列生成的渐进质量合成数据进行模型行为对比

Result: CQF在提升下游任务表现的同时，未能改善目标数据集的语言建模能力，且与合成数据训练模型展现相反趋势

Conclusion: CQF可能未捕捉到数据质量的有效表征，其作用机制需要重新评估

Abstract: Large-scale models are pretrained on massive web-crawled datasets containing
documents of mixed quality, making data filtering essential. A popular method
is Classifier-based Quality Filtering (CQF), which trains a binary classifier
to distinguish between pretraining data and a small, high-quality set. It
assigns each pretraining document a quality score defined as the classifier's
score and retains only the top-scoring ones. We provide an in-depth analysis of
CQF. We show that while CQF improves downstream task performance, it does not
necessarily enhance language modeling on the high-quality dataset. We explain
this paradox by the fact that CQF implicitly filters the high-quality dataset
as well. We further compare the behavior of models trained with CQF to those
trained on synthetic data of increasing quality, obtained via random token
permutations, and find starkly different trends. Our results challenge the view
that CQF captures a meaningful notion of data quality.

</details>


### [91] [It Takes Two: Your GRPO Is Secretly DPO](https://arxiv.org/abs/2510.00977)
*Yihong Wu,Liheng Ma,Lei Ding,Muzhi Li,Xinyu Wang,Kejia Chen,Zhan Su,Zhanguang Zhang,Chenyang Huang,Yingxue Zhang,Mark Coates,Jian-Yun Nie*

Main category: cs.LG

TL;DR: 2-GRPO通过对比学习框架实现与16-GRPO相当的模型性能，计算效率提升70%+，仅需1/8训练数据量


<details>
  <summary>Details</summary>
Motivation: 挑战传统认知中GRPO算法需要大样本量的假设，通过建立与DPO算法的理论联系，探索最小样本量配置（2组样本）的可行性

Method: 1. 理论层面将GRPO重构为对比学习框架
2. 建立与DPO算法的理论关联
3. 实证验证两样本配置（2-GRPO）的有效性

Result: 2-GRPO在性能上与16-GRPO持平（仅1/8样本量），训练时间缩短70%以上，显存消耗显著降低

Conclusion: 该研究突破了强化学习算法对大数据量的依赖，为LLM高效微调提供了新范式，具有重要工程实践价值

Abstract: Group Relative Policy Optimization (GRPO) is a prominent reinforcement
learning algorithm for post-training Large Language Models (LLMs). It is
commonly believed that GRPO necessitates a large group size to ensure stable
training via precise statistical estimation, which incurs substantial
computational overhead. In this work, we challenge this assumption by reframing
GRPO as a form of contrastive learning, which reveals a fundamental connection
to Direct Preference Optimization (DPO). Motivated by DPO's empirical success,
we investigate the minimal two-rollout case (2-GRPO), a configuration
previously deemed infeasible. We provide a rigorous theoretical analysis to
validate 2-GRPO and demonstrate empirically that it achieves performance on par
with 16-GRPO, despite using only 1/8 of the rollouts and reducing training time
by over 70%.

</details>


### [92] [GEM: A Gym for Agentic LLMs](https://arxiv.org/abs/2510.01051)
*Zichen Liu,Anya Sims,Keyu Duan,Changyu Chen,Simon Yu,Xiangxin Zhou,Haotian Xu,Shaopan Xiong,Bo Liu,Chenmien Tan,Chuen Yang Beh,Weixun Wang,Hao Zhu,Weiyan Shi,Diyi Yang,Michael Shieh,Yee Whye Teh,Wee Sun Lee,Min Lin*

Main category: cs.LG

TL;DR: 提出开源环境模拟器GEM，为LLM提供类似OpenAI-Gym的标准化训练框架，支持多RL框架并验证ReBN算法优于传统方法


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练范式正从静态数据转向经验学习，需要标准化环境接口促进技术发展

Method: 开发具备异步向量化执行、灵活扩展接口的GEM框架，集成24个环境测试基准，使用REINFORCE+ReBN算法进行跨环境验证

Result: ReBN在密集奖励设置中优于GRPO，通过GEM实现PPO/GRPO/REINFORCE的公平对比，揭示多轮场景算法设计差异

Conclusion: GEM作为训练环境与评估工具包，有望加速代理型LLM研究，其标准化设计支持更高效的算法迭代与效果验证

Abstract: The training paradigm for large language models (LLMs) is moving from static
datasets to experience-based learning, where agents acquire skills via
interacting with complex environments. To facilitate this transition we
introduce GEM (General Experience Maker), an open-source environment simulator
designed for the age of LLMs. Analogous to OpenAI-Gym for traditional
reinforcement learning (RL), GEM provides a standardized framework for the
environment-agent interface, including asynchronous vectorized execution for
high throughput, and flexible wrappers for easy extensibility. GEM also
features a diverse suite of environments, robust integrated tools, and
single-file example scripts demonstrating using GEM with five popular RL
training frameworks. Along with this, we also provide a set of baselines across
24 environments using REINFORCE with Return Batch Normalization (ReBN), which
-- unlike GRPO -- is compatible with the full RL setting of dense per-turn
rewards and offers better credit assignment. We further conduct apple-to-apple
benchmarking of PPO, GRPO and REINFORCE in both single- and multi-turn settings
using GEM to shed light on the algorithmic designs. Lastly, GEM also functions
as a convenient evaluation toolkit besides a training environment. We hope this
framework can help accelerate future agentic LLM research.

</details>


### [93] [A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning](https://arxiv.org/abs/2510.01132)
*Ruiyi Wang,Prithviraj Ammanabrolu*

Main category: cs.LG

TL;DR: 系统分析多轮强化学习中环境、奖励、策略三大支柱对LLM智能体训练的影响，提出跨领域训练方案


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型智能体训练框架存在碎片化问题，缺乏对核心设计要素的系统性分析，阻碍跨任务泛化能力研究

Method: 将设计空间分解为环境-奖励-策略三支柱，在TextWorld/ALFWorld/SWE-Gym等文本环境中进行实证研究，分析任务复杂度、奖励稀疏度与策略梯度方法的相互关系

Result: 发现环境复杂度与泛化能力正相关；密集回合奖励加速训练但稳定性依赖RL算法选择；PPO/GRPO与RLOO在不同奖励稀疏度下表现互补；确定SFT与RL的最优训练比例

Conclusion: 提出三支柱协同设计训练方案，为多轮智能体强化学习研究提供系统指导框架，开源代码促进实践应用

Abstract: We study what actually works and what doesn't for training large language
models as agents via multi-turn reinforcement learning. Despite rapid progress,
existing frameworks and definitions are fragmented, and there is no systematic
formulation or analysis of which design choices matter across tasks. We address
this gap by first breaking down the design space into three inter-related
pillars -- environment, reward, and policy -- and empirically derive a recipe
for training LLM agents in situated textual domains. In particular, we test
TextWorld and ALFWorld, popular domains for testing situated embodied
reasoning, as well as SWE-Gym for more software engineering style tasks. (i)
For the environment, we analyze the impacts of task complexity in terms of
sizes of the state and action spaces as well as optimal solution length,
finding that even simple environments within a domain can provide signal on how
well an agent can generalize to more complex tasks. (ii) For the reward, we
ablate relative reward sparsity, observing that while dense turn-level rewards
accelerate training, performance and stability is highly dependent on the
choice of RL algorithm. (iii) And for the agent's policy, we explore the
interplay between reward sparsity and biased (PPO, GRPO) and unbiased (RLOO)
policy gradient methods in addition to showing how to find the optimal
Supervised Fine-tuning (SFT) to RL training ratio given a fixed budget. We
distill these findings into a training recipe that guides co-design across the
three pillars, facilitating research and practical efforts in multi-turn
agentic RL. Code: https://github.com/pearls-lab/meow-tea-taro

</details>


### [94] [Prompt Curriculum Learning for Efficient LLM Post-Training](https://arxiv.org/abs/2510.01135)
*Zhaolin Gao,Joongwon Kim,Wen Sun,Thorsten Joachims,Sid Wang,Richard Yuanzhe Pang,Liang Tan*

Main category: cs.LG

TL;DR: 提出Prompt Curriculum Learning (PCL)算法，通过动态选择中等难度提示和并发更新价值模型，显著提升RL训练效率和推理性能


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的语言模型后训练方法对批次处理和提示选择策略敏感，需平衡训练效率与模型性能上限

Method: 1. 系统实验确定最佳批次规模 2. 提出价值模型动态评估提示难度 3. 基于有效比率筛选中等难度提示进行策略训练

Result: 在MATH和DeepScaleR上分别实现12.1倍和16.9倍的提示筛选加速，且达到最高性能或更快收敛速度

Conclusion: PCL通过提示课程学习机制，为推理导向的强化学习提供了性能与效率的优化新范式

Abstract: We introduce Prompt Curriculum Learning (PCL), a lightweight reinforcement
learning (RL) algorithm that selects intermediate-difficulty prompts using a
learned value model to post-train language models. Since post-training LLMs via
RL remains sensitive to batching and prompt selection strategies, we first
conduct a series of systematic experiments where we (1) determine the optimal
training batch size that balances generation efficiency and gradient quality
and (2) establish the importance of focusing on prompts of intermediate
difficulty for the policy. We build upon these results to design PCL, which
identifies prompts of intermediate difficulty for the current policy in an
on-policy manner by using a value model that is concurrently updated based on
the current policy. By focusing on informative prompts that yield high
effective ratios, PCL achieves either the highest performance or requires
significantly less time to reach comparable performance to its counterparts.
Compared to rollout-based filtering methods, PCL avoids costly rollouts and
achieves $12.1\times$ and $16.9\times$ faster speed on identifying
intermediate-difficulty prompts when training on MATH and DeepScaleR,
respectively. We further demonstrate that our value model accurately predicts
prompt difficulty and allows PCL to focus on progressively more challenging
prompts during RL. Our results present a new methodology that delivers improved
tradeoff between upper-bound performance and efficiency for reasoning-focused
RL.

</details>


### [95] [Simultaneous Multi-objective Alignment Across Verifiable and Non-verifiable Rewards](https://arxiv.org/abs/2510.01167)
*Yiran Shen,Yu Xia,Jonathan Chang,Prithviraj Ammanabrolu*

Main category: cs.LG

TL;DR: 提出统一框架MAH-DPO实现多目标对齐，通过向量化奖励机制同时优化数学准确性、价值观对齐和多轮对话性能，实现推理时细粒度控制。


<details>
  <summary>Details</summary>
Motivation: 传统LLM对齐方法将多维信号压缩为单一目标，导致目标冲突和训练低效。需解决可验证奖励（数学）、主观偏好（价值观）和复杂交互（对话）的多维度对齐难题。

Method: 1. 标准化过程奖励模型(PRM)监督思维链 2. 使用多动作头DPO(MAH-DPO)进行多目标对齐 3. 构建向量奖励替代标量奖励，各维度对应不同目标

Result: 在数学推理（MATH准确率↑12%）、价值观对齐（HHH评估提升）和多轮辅导对话（F1得分↑7.3%）中均实现性能提升，同时减少目标间冲突

Conclusion: 该框架首次实现跨领域多目标协同优化，通过向量奖励维度分离提供灵活推理控制，为复杂对齐场景提供系统解决方案。

Abstract: Aligning large language models to human preferences is inherently
multidimensional, yet most pipelines collapse heterogeneous signals into a
single optimizeable objective. We seek to answer what it would take to
simultaneously align a model across various domains spanning those with:
verifiable rewards (mathematical accuracy), non-verifiable subjective
preferences (human values), and complex interactive scenarios (multi-turn AI
tutoring dialogues). Such multi-objective reinforcement learning setups are
often plagued by the individual objectives being at odds with each other,
resulting in inefficient training and little user control during inference. We
propose a unified framework that: (i) standardizes {process reward model} (PRM)
training across both verifiable and non-verifiable settings to better supervise
models' chain-of-thought reasoning; (ii) performs {multi-objective alignment}
by training the LLM with our $\textbf{M}$ulti-$\textbf{A}$ction-$\textbf{H}$ead
$\textbf{DPO}$ (MAH-DPO) and a vectorized reward where the dimensions of the
vector correspond to the various objectives instead of a single scalar; and
(iii) demonstrates how such a system provides fine-grained inference-time user
control. Experiments across math reasoning, value alignment, and multi-turn
dialogue show that our framework improves performance across multiple
objectives simultaneously, while minimizing cross-objective trade-offs and
enabling flexible inference time user control. The code can be found at
https://github.com/pearls-lab/multiobj-align.

</details>


### [96] [TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments](https://arxiv.org/abs/2510.01179)
*Zhangchen Xu,Adriana Meza Soria,Shawn Tan,Anurag Roy,Ashish Sunil Agrawal,Radha Poovendran,Rameswar Panda*

Main category: cs.LG

TL;DR: 提出了当前最大的开源工具代理数据集Toucan，通过真实模型上下文协议生成150万条轨迹，显著提升模型在基准测试的表现。


<details>
  <summary>Details</summary>
Motivation: 现有开源工具代理训练数据在多样性、真实性和多工具/多轮交互方面存在不足，制约了开源社区发展。

Method: 使用5个模型生成多样化查询→质量过滤→3个教师模型+2个框架生成轨迹→严格规则/模型双重验证→扩展机制实现多轮对话模拟

Result: Toucan微调模型在BFCL V3基准超越闭源大模型，在MCP-Universe Bench推进帕累托前沿

Conclusion: Toucan填补了高质量开源工具代理数据空白，其合成验证框架为后续研究提供了可扩展的解决方案

Abstract: Large Language Model (LLM) agents are rapidly emerging as powerful systems
for automating tasks across domains. Yet progress in the open-source community
is constrained by the lack of high quality permissively licensed tool-agentic
training data. Existing datasets are often limited in diversity, realism, and
complexity, particularly regarding multi-tool and multi-turn interactions. To
address this gap, we introduce Toucan, the largest publicly available
tool-agentic dataset to date, containing 1.5 million trajectories synthesized
from nearly 500 real-world Model Context Protocols (MCPs). Unlike prior work,
Toucan leverages authentic MCP environments to generate diverse, realistic, and
challenging tasks with trajectories involving real tool execution. Our pipeline
first produces a broad spectrum of tool-use queries using five distinct models,
applies model-based quality filtering, and then generates agentic trajectories
with three teacher models using two agentic frameworks. Rigorous rule-based and
model-based validation ensures high-quality outputs. We also introduce three
extension mechanisms to further diversify tasks and simulate multi-turn
conversations. Models fine-tuned on Toucan outperform larger closed-source
counterparts on the BFCL V3 benchmark and push the Pareto frontier forward on
MCP-Universe Bench.

</details>


### [97] [BroRL: Scaling Reinforcement Learning via Broadened Exploration](https://arxiv.org/abs/2510.01180)
*Jian Hu,Mingjie Liu,Ximing Lu,Fang Wu,Zaid Harchaoui,Shizhe Diao,Yejin Choi,Pavlo Molchanov,Jun Yang,Jan Kautz,Yi Dong*

Main category: cs.LG

TL;DR: BroRL通过大幅增加每个样本的rollout次数(探索广度)，突破ProRL仅靠增加训练步数导致的性能瓶颈，在模型饱和后仍能持续提升表现


<details>
  <summary>Details</summary>
Motivation: 现有ProRL方法通过增加训练步数扩展强化学习时，模型性能在数千步后进入平台期，计算资源投入呈现边际效益递减。需要探索与训练步数正交的扩展维度

Method: 提出BroRL范式：1. 将每个样本的rollout次数提升至数百次实现充分探索 2. 通过质量平衡方程理论分析，证明增加rollout次数能确保正确token的概率质量持续扩展 3. 建立单步RL假设下的数学模型，揭示采样/未采样token对概率质量的影响机制

Result: 理论层面：模拟实验验证充足rollout次数可保证所有正确token概率质量提升；实证层面：BroRL成功复活已饱和的3K步ProRL模型，在1.5B模型上取得跨基准SOTA，展示出稳健的持续改进能力

Conclusion: 探索广度的扩展与训练深度的扩展形成互补，BroRL为强化学习的持续性能提升开辟了新维度，其理论框架为后续研究提供了可解释性基础

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key
ingredient for unlocking complex reasoning capabilities in large language
models. Recent work ProRL has shown promise in scaling RL by increasing the
number of training steps. However, performance plateaus after thousands of
steps, with clear diminishing returns from allocating more computation to
additional training. In this work, we investigate a complementary paradigm for
scaling RL, BroR-Lincreasing the number of rollouts per example to hundreds to
exhaustively Broaden exploration, which yields continuous performance gains
beyond the saturation point observed in ProRL when scaling the number of
training steps. Our approach is motivated by a mass balance equation analysis
allowing us to characterize the rate of change in probability mass for correct
and incorrect tokens during the reinforcement process. We show that under a
one-step RL assumption, sampled rollout tokens always contribute to
correct-mass expansion, while unsampled tokens outside rollouts may lead to
gains or losses depending on their distribution and the net reward balance.
Importantly, as the number of rollouts per example N increases, the effect of
unsampled terms diminishes, ensuring overall correct-mass expansion. To
validate our theoretical analysis, we conduct simulations under more relaxed
conditions and find that a sufficiently large rollout size N-corresponding to
ample exploration-guarantees an increase in the probability mass of all correct
tokens. Empirically, BroRL revives models saturated after 3K ProRL training
steps and demonstrates robust, continuous improvement, achieving
state-of-the-art results for the 1.5B model across diverse benchmarks.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [98] [Virtual Reality Alters Perceived Functional Body Size](https://arxiv.org/abs/2510.00824)
*Xiaoye Michael Wang,Ali Mazalek,Catherine M. Sabiston,Timothy N. Welsh*

Main category: cs.HC

TL;DR: VR环境通过视觉辐辏调节冲突导致深度压缩，使虚拟孔径感知变窄，需校正VAC影响才能恢复身体-环境关系的功能标度不变性


<details>
  <summary>Details</summary>
Motivation: 探究沉浸式VR如何通过头显设备影响人体对功能性身体尺寸的感知机制，揭示VR视觉畸变对行为-感知协调性的影响

Method: 60名被试在物理现实(UR)和VR中分别完成行动任务（侧身穿孔）和感知任务（调整可通过孔径），采用几何建模量化深度压缩效应

Result: VR中行动/感知阈值显著高于UR，知觉阈值的异常提升主要源于VAC引起的深度压缩（几何压缩率达17.3%），校正后功能标度比恢复UR水平

Conclusion: VR引发的深度压缩系统性改变身体-环境关系感知，形成夸大的功能性身体尺寸感知，该效应在VR暴露后仍持续存在，但本质源于VAC的几何畸变而非感知-行动系统的功能性失调

Abstract: Virtual reality (VR) introduces sensory perturbations that may impact
perception and action. The current study was designed to investigate how
immersive VR presented through a head-mounted display (HMD) affects perceived
functional body size using a passable aperture paradigm. Participants (n=60)
performed an action task (sidle through apertures) and a perception task
(adjust aperture width until passable without contact) in both physical,
unmediated reality (UR) and VR. Results revealed significantly higher action
and perceptual thresholds in VR compared to UR. Affordance ratios (perceptual
threshold over action threshold) were also higher in VR, indicating that the
increase in perceptual thresholds in VR was driven partly by sensorimotor
uncertainty, as reflected in the increase in the action thresholds, and partly
by perceptual distortions imposed by VR. This perceptual overestimation in VR
also persisted as an aftereffect in UR following VR exposure. Geometrical
modelling attributed the disproportionate increase in the perceptual threshold
in VR primarily to depth compression. This compression, stemming from the
vergence-accommodation conflict (VAC), caused the virtual aperture to be
perceived as narrower than depicted, thus requiring a wider adjusted aperture.
Critically, after mathematically correcting for the VAC's impact on perceived
aperture width, the affordance ratios in VR became equivalent to those in UR.
These outcomes demonstrate a recovered invariant geometrical scaling,
suggesting that perception remained functionally attuned to action capabilities
once VAC-induced distortions were accounted for. These findings highlight that
VR-induced depth compression systematically alters perceived body-environment
relationships, leading to an altered sense of one's functional body size.

</details>


### [99] [Navigating the Synchrony-Stability Frontier in Adaptive Chatbots](https://arxiv.org/abs/2510.00339)
*T. James Brandt*

Main category: cs.HC

TL;DR: 提出评估聊天机器人语言风格适应的计算框架，通过多策略模拟验证发现有限制策略能显著提升稳定性且同步性损失较小


<details>
  <summary>Details</summary>
Motivation: 解决聊天机器人在模仿用户语言风格时面临的即时同步性与长期稳定性之间的矛盾

Method: 使用八维风格向量和闭环提示架构，在真实对话数据上模拟五种适应策略（包括EMA、Cap等混合策略）

Result: 混合策略将稳定性提升62%至0.878，同步性仅降低17%；在三大公开语料库验证有效，提示可读性指标提升64%

Conclusion: 构建了可量化评估框架，识别出帕累托最优策略，通过多维度验证为自适应对话系统提供了可维护性设计指南

Abstract: Adaptive chatbots that mimic a user's linguistic style can build rapport and
engagement, yet unconstrained mimicry risks an agent that feels unstable or
sycophantic. We present a computational evaluation framework that makes the
core design tension explicit: balancing moment-to-moment linguistic synchrony
against long-term persona stability. Using an 8-dimensional style vector and a
closed-loop "base+delta" prompting architecture, we simulate and compare
explicit adaptation policies - Uncapped, Cap, Exponential Moving Average (EMA),
Dead-Band, and Hybrids - on a human-log dataset. Our analysis maps a clear
Pareto frontier: bounded policies achieve substantial gains in stability at a
modest cost to synchrony. For example, a Hybrid (EMA+Cap) raises stability from
0.542 to 0.878 (+62%) while reducing synchrony by only 17%. We confirm this
trade-off through large-scale replications on three public corpora
(DailyDialog, Persona-Chat, EmpatheticDialogues) and LLM-in-the-loop validation
across two model families. Furthermore, we quantify "prompt legibility,"
showing that frontier policies reduce instruction churn and cut jarring
register flips (major tone changes) from 0.254 to 0.092, yielding systems that
are easier to reason about and maintain. Taken together, our framework provides
a general evaluation harness for style adaptation; a systematic ablation that
identifies Pareto-efficient policies; robust validation across diverse datasets
and models; and novel legibility metrics linking policy choices to system
maintainability.

</details>
