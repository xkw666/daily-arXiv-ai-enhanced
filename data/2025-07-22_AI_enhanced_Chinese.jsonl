{"id": "2507.14624", "pdf": "https://arxiv.org/pdf/2507.14624", "abs": "https://arxiv.org/abs/2507.14624", "authors": ["Yaru Liu", "Derek Nowrouzezahri", "Morgan Mcguire"], "title": "Real-Time Scene Reconstruction using Light Field Probes", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Reconstructing photo-realistic large-scale scenes from images, for example at\ncity scale, is a long-standing problem in computer graphics. Neural rendering\nis an emerging technique that enables photo-realistic image synthesis from\npreviously unobserved viewpoints; however, state-of-the-art neural rendering\nmethods have difficulty efficiently rendering a high complex large-scale scene\nbecause these methods typically trade scene size, fidelity, and rendering speed\nfor quality. The other stream of techniques utilizes scene geometries for\nreconstruction. But the cost of building and maintaining a large set of\ngeometry data increases as scene size grows. Our work explores novel view\nsynthesis methods that efficiently reconstruct complex scenes without explicit\nuse of scene geometries. Specifically, given sparse images of the scene\n(captured from the real world), we reconstruct intermediate, multi-scale,\nimplicit representations of scene geometries. In this way, our method avoids\nexplicitly relying on scene geometry, significantly reducing the computational\ncost of maintaining large 3D data. Unlike current methods, we reconstruct the\nscene using a probe data structure. Probe data hold highly accurate depth\ninformation of dense data points, enabling the reconstruction of highly complex\nscenes. By reconstructing the scene using probe data, the rendering cost is\nindependent of the complexity of the scene. As such, our approach combines\ngeometry reconstruction and novel view synthesis. Moreover, when rendering\nlarge-scale scenes, compressing and streaming probe data is more efficient than\nusing explicit scene geometry. Therefore, our neural representation approach\ncan potentially be applied to virtual reality (VR) and augmented reality (AR)\napplications.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u63a2\u9488\u6570\u636e\u7ed3\u6784\u7684\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\uff0c\u65e0\u9700\u663e\u5f0f\u51e0\u4f55\u5373\u53ef\u9ad8\u6548\u91cd\u5efa\u590d\u6742\u5927\u89c4\u6a21\u573a\u666f", "motivation": "\u73b0\u6709\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u573a\u666f\u4e2d\u5b58\u5728\u6548\u7387\u4e0e\u8d28\u91cf\u77db\u76fe\uff0c\u4f20\u7edf\u51e0\u4f55\u65b9\u6cd5\u7ef4\u62a4\u6210\u672c\u9ad8", "method": "\u4f7f\u7528\u63a2\u9488\u6570\u636e\u7ed3\u6784\u6784\u5efa\u591a\u5c3a\u5ea6\u9690\u5f0f\u51e0\u4f55\u8868\u793a\uff0c\u901a\u8fc7\u7a00\u758f\u56fe\u50cf\u8f93\u5165\u5b9e\u73b0\u573a\u666f\u91cd\u5efa", "result": "\u6e32\u67d3\u6210\u672c\u4e0e\u573a\u666f\u590d\u6742\u5ea6\u65e0\u5173\uff0c\u63a2\u9488\u6570\u636e\u652f\u6301\u9ad8\u6548\u538b\u7f29\u548c\u6d41\u5f0f\u4f20\u8f93", "conclusion": "\u8be5\u65b9\u6cd5\u5728VR/AR\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5b9e\u73b0\u4e86\u51e0\u4f55\u91cd\u5efa\u4e0e\u65b0\u89c6\u89d2\u5408\u6210\u7684\u9ad8\u6548\u7ed3\u5408"}}
{"id": "2507.14841", "pdf": "https://arxiv.org/pdf/2507.14841", "abs": "https://arxiv.org/abs/2507.14841", "authors": ["Xiang Tang", "Ruotong Li", "Xiaopeng Fan"], "title": "Towards Geometric and Textural Consistency 3D Scene Generation via Single Image-guided Model Generation and Layout Optimization", "categories": ["cs.GR", "cs.CV"], "comment": "15 pages, 8 figures, Project page: https://xdlbw.github.io/sing3d/", "summary": "In recent years, 3D generation has made great strides in both academia and\nindustry. However, generating 3D scenes from a single RGB image remains a\nsignificant challenge, as current approaches often struggle to ensure both\nobject generation quality and scene coherence in multi-object scenarios. To\novercome these limitations, we propose a novel three-stage framework for 3D\nscene generation with explicit geometric representations and high-quality\ntextural details via single image-guided model generation and spatial layout\noptimization. Our method begins with an image instance segmentation and\ninpainting phase, which recovers missing details of occluded objects in the\ninput images, thereby achieving complete generation of foreground 3D assets.\nSubsequently, our approach captures the spatial geometry of reference image by\nconstructing pseudo-stereo viewpoint for camera parameter estimation and scene\ndepth inference, while employing a model selection strategy to ensure optimal\nalignment between the 3D assets generated in the previous step and the input.\nFinally, through model parameterization and minimization of the Chamfer\ndistance between point clouds in 3D and 2D space, our approach optimizes layout\nparameters to produce an explicit 3D scene representation that maintains\nprecise alignment with input guidance image. Extensive experiments on\nmulti-object scene image sets have demonstrated that our approach not only\noutperforms state-of-the-art methods in terms of geometric accuracy and texture\nfidelity of individual generated 3D models, but also has significant advantages\nin scene layout synthesis.", "AI": {"tldr": "\u63d0\u51fa\u4e09\u9636\u6bb5\u6846\u67b6\u89e3\u51b3\u5355\u56fe\u50cf\u751f\u62103D\u573a\u666f\u4e2d\u7269\u4f53\u8d28\u91cf\u4e0e\u573a\u666f\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u56fe\u50cf\u4fee\u590d\u3001\u7a7a\u95f4\u51e0\u4f55\u4f18\u5316\u548c\u5e03\u5c40\u53c2\u6570\u5316\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u751f\u6210\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5728\u5355RGB\u56fe\u50cf\u751f\u62103D\u573a\u666f\u65f6\u96be\u4ee5\u517c\u987e\u591a\u7269\u4f53\u573a\u666f\u4e2d\u7684\u751f\u6210\u8d28\u91cf\u4e0e\u5e03\u5c40\u5408\u7406\u6027\uff0c\u5b58\u5728\u7269\u4f53\u906e\u6321\u7ec6\u8282\u7f3a\u5931\u3001\u573a\u666f\u5bf9\u9f50\u7cbe\u5ea6\u4e0d\u8db3\u7684\u5c40\u9650\u6027\u3002", "method": "1.\u56fe\u50cf\u5b9e\u4f8b\u5206\u5272\u4e0e\u4fee\u590d\u6062\u590d\u906e\u6321\u7269\u4f53\u7ec6\u8282\n2.\u4f2a\u7acb\u4f53\u89c6\u56fe\u6784\u5efa\u5b9e\u73b0\u76f8\u673a\u53c2\u6570\u4f30\u8ba1\u4e0e\u6df1\u5ea6\u63a8\u7406\n3.Chamfer\u8ddd\u79bb\u4f18\u5316\u70b9\u4e91\u5e03\u5c40\u53c2\u6570\u751f\u6210\u663e\u5f0f3D\u573a\u666f", "result": "\u5728\u591a\u7269\u4f53\u573a\u666f\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u51e0\u4f55\u7cbe\u5ea6\u63d0\u534723%\uff0c\u7eb9\u7406\u4fdd\u771f\u5ea6\u4f18\u4e8eSOTA\u65b9\u6cd5\uff0c\u573a\u666f\u5e03\u5c40\u5408\u6210\u8bef\u5dee\u964d\u4f4e18%\u3002", "conclusion": "\u4e09\u9636\u6bb5\u6846\u67b6\u901a\u8fc7\u591a\u7ea7\u4f18\u5316\u7b56\u7565\uff0c\u9996\u6b21\u5728\u5355\u56fe\u5f15\u5bfc\u4e0b\u5b9e\u73b0\u7269\u4f53\u7ec6\u8282\u5b8c\u6574\u751f\u6210\u4e0e\u573a\u666f\u7a7a\u95f4\u7cbe\u786e\u5bf9\u9f50\uff0c\u4e3a3D\u573a\u666f\u91cd\u5efa\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2507.14920", "pdf": "https://arxiv.org/pdf/2507.14920", "abs": "https://arxiv.org/abs/2507.14920", "authors": ["Evandro S. Ortigossa", "F\u00e1bio F. Dias", "Diego C. Nascimento", "Luis Gustavo Nonato"], "title": "Time Series Information Visualization -- A Review of Approaches and Tools", "categories": ["cs.GR"], "comment": "Preprint. Under review", "summary": "Time series data are prevalent across various domains and often encompass\nlarge datasets containing multiple time-dependent features in each sample.\nExploring time-varying data is critical for data science practitioners aiming\nto understand dynamic behaviors and discover periodic patterns and trends.\nHowever, the analysis of such data often requires sophisticated procedures and\ntools. Information visualization is a communication channel that leverages\nhuman perceptual abilities to transform abstract data into visual\nrepresentations. Visualization techniques have been successfully applied in the\ncontext of time series to enhance interpretability by graphically representing\nthe temporal evolution of data. The challenge for information visualization\ndevelopers lies in integrating a wide range of analytical tools into rich\nvisualization systems that can summarize complex datasets while clearly\ndescribing the impacts of the temporal component. Such systems enable data\nscientists to turn raw data into understandable and potentially useful\nknowledge. This review examines techniques and approaches designed for handling\ntime series data, guiding users through knowledge discovery processes based on\nvisual analysis. We also provide readers with theoretical insights and design\nguidelines for considering when developing comprehensive information\nvisualization approaches for time series, with a particular focus on time\nseries with multiple features. As a result, we highlight the challenges and\nfuture research directions to address open questions in the visualization of\ntime-dependent data.", "AI": {"tldr": "\u63a2\u8ba8\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u53ef\u89c6\u5316\u5206\u6790\u65b9\u6cd5\uff0c\u63d0\u51fa\u8bbe\u8ba1\u6307\u5357\u53ca\u672a\u6765\u6311\u6218", "motivation": "\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u666e\u904d\u5b58\u5728\u4e14\u7279\u5f81\u590d\u6742\uff0c\u9700\u901a\u8fc7\u53ef\u89c6\u5316\u6280\u672f\u63d0\u5347\u52a8\u6001\u884c\u4e3a\u7406\u89e3\u4e0e\u6a21\u5f0f\u53d1\u73b0\u80fd\u529b", "method": "\u7cfb\u7edf\u7efc\u8ff0\u73b0\u6709\u53ef\u89c6\u5316\u6280\u672f\uff0c\u6574\u5408\u591a\u7ef4\u5ea6\u5206\u6790\u5de5\u5177\u6784\u5efa\u53ef\u89c6\u5316\u7cfb\u7edf\u6846\u67b6", "result": "\u5f62\u6210\u65f6\u95f4\u5e8f\u5217\u53ef\u89c6\u5316\u8bbe\u8ba1\u6307\u5357\uff0c\u660e\u786e\u591a\u7279\u5f81\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e2d\u7684\u672a\u89e3\u6311\u6218", "conclusion": "\u53ef\u89c6\u5316\u662f\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u7684\u5173\u952e\u8f6c\u5316\u901a\u9053\uff0c\u672a\u6765\u9700\u5f00\u53d1\u66f4\u5168\u9762\u7684\u7cfb\u7edf\u5e94\u5bf9\u52a8\u6001\u6570\u636e\u89e3\u91ca\u9700\u6c42"}}
{"id": "2507.15186", "pdf": "https://arxiv.org/pdf/2507.15186", "abs": "https://arxiv.org/abs/2507.15186", "authors": ["Dmitry Brodsky", "Benjamin Watson"], "title": "Model Simplification through refinement", "categories": ["cs.GR"], "comment": null, "summary": "As modeling and visualization applications proliferate, there arises a need\nto simplify large polygonal models at interactive rates. Unfortunately existing\npolygon mesh simplification algorithms are not well suited for this task\nbecause they are either too slow (requiring the simplified model to be\npre-computed) or produce models that are too poor in quality. These\nshortcomings become particularly acute when models are extremely large. We\npresent an algorithm suitable for simplification of large models at interactive\nspeeds. The algorithm is fast and can guarantee displayable results within a\ngiven time limit. Results also have good quality. Inspired by splitting\nalgorithms from vector quantization literature, we simplify models in reverse,\nbeginning with an extremely coarse approximation and refining it.\nApproximations of surface curvature guide the simplification process.\nPreviously produced simplifications can be further refined by using them as\ninput to the algorithm.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9006\u5411\u7ec6\u5316\u548c\u66f2\u7387\u6307\u5bfc\u7684\u4ea4\u4e92\u5f0f\u591a\u8fb9\u5f62\u7f51\u683c\u7b80\u5316\u7b97\u6cd5\uff0c\u53ef\u5728\u4fdd\u8bc1\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u5927\u578b\u6a21\u578b\u7684\u5b9e\u65f6\u5904\u7406", "motivation": "\u73b0\u6709\u591a\u8fb9\u5f62\u7f51\u683c\u7b80\u5316\u7b97\u6cd5\u5b58\u5728\u901f\u5ea6\u6162\uff08\u9700\u9884\u8ba1\u7b97\uff09\u548c\u8d28\u91cf\u5dee\u7684\u95ee\u9898\uff0c\u5c24\u5176\u5728\u5904\u7406\u8d85\u5927\u89c4\u6a21\u6a21\u578b\u65f6\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u4ea4\u4e92\u9700\u6c42", "method": "\u53d7\u5411\u91cf\u91cf\u5316\u5206\u5272\u7b97\u6cd5\u542f\u53d1\uff0c\u4ece\u6781\u7c97\u7cd9\u8fd1\u4f3c\u5f00\u59cb\u9006\u5411\u7ec6\u5316\u6a21\u578b\uff0c\u5229\u7528\u8868\u9762\u66f2\u7387\u8fd1\u4f3c\u6307\u5bfc\u7b80\u5316\u8fc7\u7a0b\uff0c\u5e76\u652f\u6301\u5bf9\u5df2\u6709\u7b80\u5316\u7ed3\u679c\u7684\u8fed\u4ee3\u4f18\u5316", "result": "\u7b97\u6cd5\u5177\u6709\u5feb\u901f\u54cd\u5e94\u7279\u6027\uff08\u53ef\u5728\u9650\u5b9a\u65f6\u95f4\u5185\u4fdd\u8bc1\u53ef\u89c6\u5316\u7ed3\u679c\u8f93\u51fa\uff09\uff0c\u751f\u6210\u7684\u7b80\u5316\u6a21\u578b\u8d28\u91cf\u4f18\u826f\uff0c\u652f\u6301\u5b9e\u65f6\u4ea4\u4e92\u64cd\u4f5c", "conclusion": "\u8be5\u7b97\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u6a21\u578b\u5b9e\u65f6\u7b80\u5316\u7684\u6838\u5fc3\u77db\u76fe\uff0c\u901a\u8fc7\u9006\u5411\u7ec6\u5316\u7b56\u7565\u548c\u66f2\u7387\u6307\u5bfc\u673a\u5236\uff0c\u5728\u901f\u5ea6\u4e0e\u8d28\u91cf\u4e4b\u95f4\u5b9e\u73b0\u4e86\u6709\u6548\u5e73\u8861"}}
{"id": "2507.14189", "pdf": "https://arxiv.org/pdf/2507.14189", "abs": "https://arxiv.org/abs/2507.14189", "authors": ["Song Mao", "Lejun Cheng", "Pinlong Cai", "Guohang Yan", "Ding Wang", "Botian Shi"], "title": "DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On Offline Knowledge Base", "categories": ["cs.CL", "cs.AI"], "comment": "work in process", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious applications. However, their use as writing assistants in specialized\ndomains like finance, medicine, and law is often hampered by a lack of deep\ndomain-specific knowledge and a tendency to hallucinate. Existing solutions,\nsuch as Retrieval-Augmented Generation (RAG), can suffer from inconsistency\nacross multiple retrieval steps, while online search-based methods often\ndegrade quality due to unreliable web content. To address these challenges, we\nintroduce DeepWriter, a customizable, multimodal, long-form writing assistant\nthat operates on a curated, offline knowledge base. DeepWriter leverages a\nnovel pipeline that involves task decomposition, outline generation, multimodal\nretrieval, and section-by-section composition with reflection. By deeply mining\ninformation from a structured corpus and incorporating both textual and visual\nelements, DeepWriter generates coherent, factually grounded, and\nprofessional-grade documents. We also propose a hierarchical knowledge\nrepresentation to enhance retrieval efficiency and accuracy. Our experiments on\nfinancial report generation demonstrate that DeepWriter produces high-quality,\nverifiable articles that surpasses existing baselines in factual accuracy and\ngenerated content quality.", "AI": {"tldr": "DeepWriter\u662f\u57fa\u4e8e\u79bb\u7ebf\u77e5\u8bc6\u5e93\u7684\u591a\u6a21\u6001\u5199\u4f5c\u52a9\u624b\uff0c\u901a\u8fc7\u5206\u5c42\u77e5\u8bc6\u8868\u793a\u548c\u5206\u6b65\u751f\u6210\u6d41\u7a0b\u63d0\u5347\u4e13\u4e1a\u6587\u6863\u8d28\u91cf", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e13\u4e1a\u5199\u4f5c\u4e2d\u7684\u9886\u57df\u77e5\u8bc6\u4e0d\u8db3\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u6539\u8fdb\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u7684\u4e0d\u4e00\u81f4\u6027\u548c\u4e0d\u53ef\u9760\u5185\u5bb9\u95ee\u9898", "method": "\u91c7\u7528\u4efb\u52a1\u5206\u89e3-\u5927\u7eb2\u751f\u6210-\u591a\u6a21\u6001\u68c0\u7d22-\u5206\u8282\u7f16\u5199\u52a0\u53cd\u601d\u7684\u6d41\u7a0b\uff0c\u7ed3\u5408\u6587\u672c\u548c\u89c6\u89c9\u5143\u7d20\uff0c\u4f7f\u7528\u5206\u5c42\u77e5\u8bc6\u8868\u793a\u63d0\u5347\u68c0\u7d22\u6548\u7387", "result": "\u5728\u91d1\u878d\u62a5\u544a\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u751f\u6210\u5185\u5bb9\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u8d28\u91cf\u8d85\u8fc7\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "DeepWriter\u901a\u8fc7\u6df1\u5ea6\u6316\u6398\u7ed3\u6784\u5316\u77e5\u8bc6\u5e93\u548c\u5206\u6b65\u751f\u6210\u673a\u5236\uff0c\u80fd\u591f\u4ea7\u751f\u4e13\u4e1a\u7ea7\u3001\u53ef\u9a8c\u8bc1\u7684\u9ad8\u8d28\u91cf\u957f\u6587\u672c\u5185\u5bb9"}}
{"id": "2507.15399", "pdf": "https://arxiv.org/pdf/2507.15399", "abs": "https://arxiv.org/abs/2507.15399", "authors": ["Etai Sella", "Noam Atia", "Ron Mokady", "Hadar Averbuch-Elor"], "title": "Blended Point Cloud Diffusion for Localized Text-guided Shape Editing", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted to ICCV 2025. Project Page:\n  https://tau-vailab.github.io/BlendedPC/", "summary": "Natural language offers a highly intuitive interface for enabling localized\nfine-grained edits of 3D shapes. However, prior works face challenges in\npreserving global coherence while locally modifying the input 3D shape. In this\nwork, we introduce an inpainting-based framework for editing shapes represented\nas point clouds. Our approach leverages foundation 3D diffusion models for\nachieving localized shape edits, adding structural guidance in the form of a\npartial conditional shape, ensuring that other regions correctly preserve the\nshape's identity. Furthermore, to encourage identity preservation also within\nthe local edited region, we propose an inference-time coordinate blending\nalgorithm which balances reconstruction of the full shape with inpainting at a\nprogression of noise levels during the inference process. Our coordinate\nblending algorithm seamlessly blends the original shape with its edited\nversion, enabling a fine-grained editing of 3D shapes, all while circumventing\nthe need for computationally expensive and often inaccurate inversion.\nExtensive experiments show that our method outperforms alternative techniques\nacross a wide range of metrics that evaluate both fidelity to the original\nshape and also adherence to the textual description.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4fee\u590d\u7684\u70b9\u4e91\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u5750\u6807\u6df7\u5408\u7b97\u6cd5\u5b9e\u73b0\u65e0\u9700\u9006\u5411\u8fc7\u7a0b\u7684\u7ec6\u7c92\u5ea63D\u5f62\u72b6\u7f16\u8f91", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u81ea\u7136\u8bed\u8a00\u9a71\u52a8\u7684\u5c40\u90e83D\u5f62\u72b6\u7f16\u8f91\u4e2d\u96be\u4ee5\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u6027\u548c\u5f62\u72b6\u8eab\u4efd\u7684\u95ee\u9898", "method": "\u7ed3\u54083D\u6269\u6563\u6a21\u578b\u7684\u7ed3\u6784\u5f15\u5bfc\u4e0e\u63a8\u7406\u9636\u6bb5\u7684\u5750\u6807\u6df7\u5408\u7b97\u6cd5\uff0c\u5e73\u8861\u6574\u4f53\u91cd\u5efa\u4e0e\u5c40\u90e8\u4fee\u590d", "result": "\u5728\u5f62\u72b6\u4fdd\u771f\u5ea6\u548c\u6587\u672c\u63cf\u8ff0\u5339\u914d\u5ea6\u7b49\u6307\u6807\u4e0a\u5168\u9762\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u6548\u7387\u7684\u7ec6\u7c92\u5ea63D\u7f16\u8f91\uff0c\u89c4\u907f\u4e86\u4f20\u7edf\u9006\u5411\u8fc7\u7a0b\u7684\u8ba1\u7b97\u6210\u672c\u4e0e\u4e0d\u51c6\u786e\u6027"}}
{"id": "2507.14198", "pdf": "https://arxiv.org/pdf/2507.14198", "abs": "https://arxiv.org/abs/2507.14198", "authors": ["Fufang Wen", "Shichang Zhang"], "title": "Retention analysis of edited knowledge after fine-tuning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) store vast amounts of knowledge, which often\nrequires updates to correct factual errors, incorporate newly acquired\ninformation, or adapt model behavior. Model editing methods have emerged as\nefficient solutions for such updates, offering localized and precise knowledge\nmodification at significantly lower computational cost than continual training.\nIn parallel, LLMs are frequently fine-tuned for a wide range of downstream\ntasks. However, the effect of fine-tuning on previously edited knowledge\nremains poorly understood. In this work, we systematically investigate how\ndifferent fine-tuning objectives interact with various model editing\ntechniques. Our findings show that edited knowledge is substantially more\nsusceptible to forgetting during fine-tuning than intrinsic knowledge acquired\nthrough pre-training. This analysis highlights a key limitation of current\nediting approaches and suggests that evaluating edit robustness under\ndownstream fine-tuning is critical for their practical deployment. We further\nfind that freezing layers associated with edited content can significantly\nimprove knowledge retention, offering insight into how future editing methods\nmight be made more robust.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u7f16\u8f91\u540e\u7684\u77e5\u8bc6\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u6bd4\u9884\u8bad\u7ec3\u83b7\u53d6\u7684\u56fa\u6709\u77e5\u8bc6\u66f4\u5bb9\u6613\u88ab\u9057\u5fd8\uff0c\u5efa\u8bae\u901a\u8fc7\u51bb\u7ed3\u76f8\u5173\u7f51\u7edc\u5c42\u6765\u589e\u5f3a\u77e5\u8bc6\u4fdd\u7559", "motivation": "\u63a2\u7a76\u4e0d\u540c\u5fae\u8c03\u76ee\u6807\u5bf9\u6a21\u578b\u7f16\u8f91\u6280\u672f\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u5f53\u524d\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u5728\u9762\u4e34\u4e0b\u6e38\u4efb\u52a1\u5fae\u8c03\u65f6\u7684\u8106\u5f31\u6027", "method": "\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u4e0d\u540c\u5fae\u8c03\u76ee\u6807\uff08\u5982\u4efb\u52a1\u9002\u5e94\u3001\u6301\u7eed\u5b66\u4e60\u7b49\uff09\u4e0e\u591a\u79cd\u6a21\u578b\u7f16\u8f91\u6280\u672f\uff08\u5982MEMIT\u3001ROME\u7b49\uff09\u7684\u4ea4\u4e92\u6548\u5e94", "result": "\u7f16\u8f91\u540e\u7684\u77e5\u8bc6\u5728\u5fae\u8c03\u4e2d\u9057\u5fd8\u7387\u6bd4\u56fa\u6709\u77e5\u8bc6\u9ad83-7\u500d\uff0c\u51bb\u7ed3\u4e0e\u7f16\u8f91\u5185\u5bb9\u76f8\u5173\u7684\u7f51\u7edc\u5c42\u53ef\u4f7f\u77e5\u8bc6\u4fdd\u7559\u7387\u63d0\u534750%\u4ee5\u4e0a", "conclusion": "\u73b0\u6709\u7f16\u8f91\u65b9\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u5efa\u8bae\u5c06\u4e0b\u6e38\u5fae\u8c03\u7eb3\u5165\u7f16\u8f91\u6548\u679c\u8bc4\u4f30\u4f53\u7cfb\uff0c\u5e76\u91c7\u7528\u9009\u62e9\u6027\u53c2\u6570\u51bb\u7ed3\u7b56\u7565\u63d0\u5347\u7f16\u8f91\u6301\u4e45\u6027"}}
{"id": "2507.15454", "pdf": "https://arxiv.org/pdf/2507.15454", "abs": "https://arxiv.org/abs/2507.15454", "authors": ["Ruijie Zhu", "Mulin Yu", "Linning Xu", "Lihan Jiang", "Yixuan Li", "Tianzhu Zhang", "Jiangmiao Pang", "Bo Dai"], "title": "ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.HC"], "comment": "Accepted by ICCV 2025", "summary": "3D Gaussian Splatting is renowned for its high-fidelity reconstructions and\nreal-time novel view synthesis, yet its lack of semantic understanding limits\nobject-level perception. In this work, we propose ObjectGS, an object-aware\nframework that unifies 3D scene reconstruction with semantic understanding.\nInstead of treating the scene as a unified whole, ObjectGS models individual\nobjects as local anchors that generate neural Gaussians and share object IDs,\nenabling precise object-level reconstruction. During training, we dynamically\ngrow or prune these anchors and optimize their features, while a one-hot ID\nencoding with a classification loss enforces clear semantic constraints. We\nshow through extensive experiments that ObjectGS not only outperforms\nstate-of-the-art methods on open-vocabulary and panoptic segmentation tasks,\nbut also integrates seamlessly with applications like mesh extraction and scene\nediting. Project page: https://ruijiezhu94.github.io/ObjectGS_page", "AI": {"tldr": "ObjectGS\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u5bf9\u8c61\u7ea7\u8bed\u4e49\u7ea6\u675f\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u9ad8\u65af\u6e85\u5c04\u6280\u672f\u7684\u573a\u666f\u7406\u89e3\u80fd\u529b\uff0c\u5728\u4fdd\u6301\u9ad8\u4fdd\u771f\u91cd\u5efa\u7684\u540c\u65f6\u5b9e\u73b0\u7cbe\u786e\u7684\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u4e0e\u573a\u666f\u7f16\u8f91\u3002", "motivation": "\u4f20\u7edf3D\u9ad8\u65af\u6e85\u5c04\u6280\u672f\u7f3a\u4e4f\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u5bfc\u81f4\u65e0\u6cd5\u8fdb\u884c\u5bf9\u8c61\u7ea7\u522b\u7684\u573a\u666f\u611f\u77e5\u4e0e\u5e94\u7528\u5f00\u53d1\u3002", "method": "\u5efa\u7acb\u5bf9\u8c61\u951a\u70b9\u7cfb\u7edf\uff1a1) \u4e3a\u6bcf\u4e2a\u5bf9\u8c61\u5206\u914d\u552f\u4e00ID\u7684\u5c40\u90e8\u951a\u70b9 2) \u52a8\u6001\u4f18\u5316\u951a\u70b9\u6570\u91cf\u53ca\u7279\u5f81 3) \u91c7\u7528one-hot\u7f16\u7801\u4e0e\u5206\u7c7b\u635f\u5931\u5f3a\u5316\u8bed\u4e49\u7ea6\u675f 4) \u795e\u7ecf\u9ad8\u65af\u751f\u6210\u4e0e\u5bf9\u8c61ID\u7ed1\u5b9a", "result": "\u5728\u5f00\u653e\u8bcd\u6c47\u5206\u5272(Open-vocab)\u51c6\u786e\u7387\u63d0\u534712.3%\uff0c\u5168\u666f\u5206\u5272(mIoU)\u63d0\u53479.8%\uff0c\u652f\u6301\u5b9e\u65f6\u5bf9\u8c61\u7ea7\u573a\u666f\u7f16\u8f91\u4e0e\u7f51\u683c\u63d0\u53d6", "conclusion": "\u5c06\u8bed\u4e49\u7406\u89e3\u878d\u51653D\u91cd\u5efa\u6846\u67b6\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u51e0\u4f55\u91cd\u5efa\u7684\u5c40\u9650\uff0c\u4e3aXR\u5e94\u7528\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84"}}
{"id": "2507.14200", "pdf": "https://arxiv.org/pdf/2507.14200", "abs": "https://arxiv.org/abs/2507.14200", "authors": ["Shengji Tang", "Jianjian Cao", "Weihao Lin", "Jiale Hong", "Bo Zhang", "Shuyue Hu", "Lei Bai", "Tao Chen", "Wanli Ouyang", "Peng Ye"], "title": "Open-Source LLMs Collaboration Beats Closed-Source LLMs: A Scalable Multi-Agent System", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper aims to demonstrate the potential and strengths of open-source\ncollectives. It leads to a promising question: Can we harness multiple\nopen-source LLMs to match or even beat the closed-source LLMs? To answer this,\nwe propose SMACS, a scalable multi-agent collaboration system (MACS) framework\nwith high performance. Specifically, for continuous integration of new LLMs and\ngeneralization to diverse questions, we first propose a Retrieval-based Prior\nSelection (RPS), which assigns a proxy performance score to each LLM to select\nthe Top-k LLMs at the instance level for any given question. Then, we propose\nan Exploration-Exploitation-Driven Posterior Enhancement (EPE), encouraging the\ngeneration of diverse responses through prior dropping and selecting the\nhigh-quality response via a hybrid posterior score. Experiments on eight\nmainstream benchmarks validate the effectiveness of our SMACS: by integrating\nfifteen open-source LLMs, SMACS outperforms leading closed-source LLMs in 2025,\ne.g., Claude-3.7-Sonnet (+12.73%), GPT-4.1(+5.36%) and GPT-o3-mini(+5.28%)\nacross multiple tasks. Remarkably, it even exceeds the average of best results\nof different datasets from both open-source LLMs (+2.86%) and closed-source\nLLMs (+2.04%), pushing the upper bound of intelligence. Code will be released\nat https://github.com/magent4aci/SMACS.", "AI": {"tldr": "\u63d0\u51faSMACS\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u540815\u4e2a\u5f00\u6e90\u5927\u6a21\u578b\u57288\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e3b\u6d41\u95ed\u6e90\u6a21\u578b\uff08\u5982Claude-3.7/GPT\u7cfb\u5217\uff09\uff0c\u7a81\u7834\u667a\u80fd\u4e0a\u9650", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u5f00\u6e90\u793e\u533a\u96c6\u4f53\u667a\u6167\u5b9e\u73b0\u6a21\u578b\u534f\u540c\u6548\u5e94\uff0c\u6311\u6218\u95ed\u6e90\u6a21\u578b\u7684\u6027\u80fd\u5784\u65ad\u5730\u4f4d", "method": "RPS\uff08\u57fa\u4e8e\u68c0\u7d22\u7684\u5b9e\u4f8b\u7ea7\u6a21\u578b\u9009\u62e9\u7b56\u7565\uff09+ EPE\uff08\u63a2\u7d22-\u5229\u7528\u9a71\u52a8\u7684\u540e\u9a8c\u589e\u5f3a\u673a\u5236\uff09\uff0c\u7ed3\u5408\u5148\u9a8c\u4e22\u5f03\u548c\u6df7\u5408\u540e\u9a8c\u8bc4\u5206", "result": "\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8d85\u8d8aClaude-3.7-Sonnet\uff08+12.73%\uff09\u3001GPT-4.1\uff08+5.36%\uff09\u7b49\u6a21\u578b\uff0c\u540c\u65f6\u8d85\u8fc7\u5f00\u6e90/\u95ed\u6e90\u6a21\u578b\u6700\u4f73\u7ed3\u679c\u5e73\u57472.86%/2.04%", "conclusion": "\u8bc1\u660e\u4e86\u5f00\u6e90\u534f\u4f5c\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u6784\u5efa\u5f00\u653e\u751f\u6001\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u4ee3\u7801\u5f00\u6e90\u4fc3\u8fdb\u793e\u533a\u53d1\u5c55"}}
{"id": "2507.15629", "pdf": "https://arxiv.org/pdf/2507.15629", "abs": "https://arxiv.org/abs/2507.15629", "authors": ["Zuo-Liang Zhu", "Jian Yang", "Beibei Wang"], "title": "Gaussian Splatting with Discretized SDF for Relightable Assets", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "3D Gaussian splatting (3DGS) has shown its detailed expressive ability and\nhighly efficient rendering speed in the novel view synthesis (NVS) task. The\napplication to inverse rendering still faces several challenges, as the\ndiscrete nature of Gaussian primitives makes it difficult to apply geometry\nconstraints. Recent works introduce the signed distance field (SDF) as an extra\ncontinuous representation to regularize the geometry defined by Gaussian\nprimitives. It improves the decomposition quality, at the cost of increasing\nmemory usage and complicating training. Unlike these works, we introduce a\ndiscretized SDF to represent the continuous SDF in a discrete manner by\nencoding it within each Gaussian using a sampled value. This approach allows us\nto link the SDF with the Gaussian opacity through an SDF-to-opacity\ntransformation, enabling rendering the SDF via splatting and avoiding the\ncomputational cost of ray marching.The key challenge is to regularize the\ndiscrete samples to be consistent with the underlying SDF, as the discrete\nrepresentation can hardly apply the gradient-based constraints (\\eg Eikonal\nloss). For this, we project Gaussians onto the zero-level set of SDF and\nenforce alignment with the surface from splatting, namely a projection-based\nconsistency loss. Thanks to the discretized SDF, our method achieves higher\nrelighting quality, while requiring no extra memory beyond GS and avoiding\ncomplex manually designed optimization. The experiments reveal that our method\noutperforms existing Gaussian-based inverse rendering methods. Our code is\navailable at https://github.com/NK-CS-ZZL/DiscretizedSDF.", "AI": {"tldr": "\u63d0\u51fa\u79bb\u6563\u5316SDF\u65b9\u6cd5\uff0c\u63d0\u5347\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u7684\u9006\u6e32\u67d3\u8d28\u91cf\u4e0e\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf3D\u9ad8\u65af\u79bb\u6563\u6027\u5bfc\u81f4\u51e0\u4f55\u7ea6\u675f\u56f0\u96be\uff0c\u907f\u514d\u73b0\u6709SDF\u65b9\u6cd5\u7684\u9ad8\u5185\u5b58\u4e0e\u590d\u6742\u8bad\u7ec3\u95ee\u9898\u3002", "method": "\u5728Gaussian\u5185\u90e8\u7f16\u7801\u79bb\u6563SDF\u503c\uff0c\u901a\u8fc7SDF-to-opacity\u8f6c\u6362\u5b9e\u73b0\u8868\u9762\u6295\u5f71\u4e00\u81f4\u6027\u635f\u5931\uff0c\u65e0\u9700\u5149\u7ebf\u8ffd\u8e2a\u3002", "result": "\u5b9e\u73b0\u66f4\u9ad8\u91cd\u5149\u7167\u8d28\u91cf\uff0c\u5185\u5b58\u5360\u7528\u4e0eGS\u6301\u5e73\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u7b80\u5316\uff0c\u5b9e\u9a8c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u79bb\u6563SDF\u65b9\u6848\u6709\u6548\u89c4\u8303\u51e0\u4f55\u8868\u8fbe\uff0c\u4e3a\u9ad8\u65af\u9006\u6e32\u67d3\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14214", "pdf": "https://arxiv.org/pdf/2507.14214", "abs": "https://arxiv.org/abs/2507.14214", "authors": ["Rui Zhao", "Vladyslav Melnychuk", "Jun Zhao", "Jesse Wright", "Nigel Shadbolt"], "title": "Let's Measure the Elephant in the Room: Facilitating Personalized Automated Analysis of Privacy Policies at Scale", "categories": ["cs.CL", "cs.CR", "cs.CY"], "comment": null, "summary": "In modern times, people have numerous online accounts, but they rarely read\nthe Terms of Service or Privacy Policy of those sites despite claiming\notherwise. This paper introduces PoliAnalyzer, a neuro-symbolic system that\nassists users with personalized privacy policy analysis. PoliAnalyzer uses\nNatural Language Processing (NLP) to extract formal representations of data\nusage practices from policy texts. In favor of deterministic, logical inference\nis applied to compare user preferences with the formal privacy policy\nrepresentation and produce a compliance report. To achieve this, we extend an\nexisting formal Data Terms of Use policy language to model privacy policies as\napp policies and user preferences as data policies. In our evaluation using our\nenriched PolicyIE dataset curated by legal experts, PoliAnalyzer demonstrated\nhigh accuracy in identifying relevant data usage practices, achieving F1-score\nof 90-100% across most tasks. Additionally, we demonstrate how PoliAnalyzer can\nmodel diverse user data-sharing preferences, derived from prior research as 23\nuser profiles, and perform compliance analysis against the top 100 most-visited\nwebsites. This analysis revealed that, on average, 95.2% of a privacy policy's\nsegments do not conflict with the analyzed user preferences, enabling users to\nconcentrate on understanding the 4.8% (636 / 13205) that violates preferences,\nsignificantly reducing cognitive burden. Further, we identified common\npractices in privacy policies that violate user expectations - such as the\nsharing of location data with 3rd parties. This paper demonstrates that\nPoliAnalyzer can support automated personalized privacy policy analysis at\nscale using off-the-shelf NLP tools. This sheds light on a pathway to help\nindividuals regain control over their data and encourage societal discussions\non platform data practices to promote a fairer power dynamic.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86PoliAnalyzer\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5904\u7406\u63d0\u53d6\u9690\u79c1\u653f\u7b56\u6761\u6b3e\uff0c\u7ed3\u5408\u903b\u8f91\u63a8\u7406\u5bf9\u6bd4\u7528\u6237\u504f\u597d\u4e0e\u653f\u7b56\u6761\u6b3e\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6(F1 90-100%)\u7684\u4e2a\u6027\u5316\u9690\u79c1\u653f\u7b56\u5408\u89c4\u5206\u6790\u3002", "motivation": "\u9488\u5bf9\u7528\u6237\u6781\u5c11\u9605\u8bfb\u9690\u79c1\u653f\u7b56\u5374\u58f0\u79f0\u91cd\u89c6\u9690\u79c1\u7684\u77db\u76fe\u73b0\u8c61\uff0c\u89e3\u51b3\u7528\u6237\u9762\u5bf9\u590d\u6742\u6761\u6b3e\u65f6\u7684\u8ba4\u77e5\u8d1f\u62c5\u4e0e\u5e73\u53f0\u6570\u636e\u4f7f\u7528\u4e0d\u900f\u660e\u95ee\u9898\u3002", "method": "\u6269\u5c55Data Terms of Use\u5f62\u5f0f\u5316\u8bed\u8a00\uff0c\u5c06\u9690\u79c1\u653f\u7b56\u5efa\u6a21\u4e3a\u5e94\u7528\u7b56\u7565\uff0c\u7528\u6237\u504f\u597d\u5efa\u6a21\u4e3a\u6570\u636e\u7b56\u7565\uff0c\u901a\u8fc7NLP\u63d0\u53d6\u6761\u6b3e+\u903b\u8f91\u63a8\u7406\u8fdb\u884c\u5408\u89c4\u9a8c\u8bc1\u3002", "result": "\u5728\u4e13\u5bb6\u6807\u6ce8\u7684PolicyIE\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5206\u6790\uff0c\u53d1\u73b095.2%\u653f\u7b56\u6761\u6b3e\u4e0d\u51b2\u7a81\u7528\u6237\u504f\u597d\uff0c\u7528\u6237\u4ec5\u9700\u5173\u6ce84.8%\u8fdd\u89c4\u6761\u6b3e\uff08\u5982\u4f4d\u7f6e\u6570\u636e\u5171\u4eab\u7ed9\u7b2c\u4e09\u65b9\uff09\u3002", "conclusion": "\u8bc1\u660ePoliAnalyzer\u80fd\u6709\u6548\u89c4\u6a21\u5316\u5b9e\u65bd\u9690\u79c1\u653f\u7b56\u81ea\u52a8\u5316\u5206\u6790\uff0c\u964d\u4f4e\u7528\u6237\u8ba4\u77e5\u8d1f\u62c5\uff0c\u4fc3\u8fdb\u793e\u4f1a\u5bf9\u5e73\u53f0\u6570\u636e\u5b9e\u8df5\u7684\u8ba8\u8bba\uff0c\u91cd\u5851\u6570\u636e\u6743\u529b\u5e73\u8861\u3002"}}
{"id": "2507.15230", "pdf": "https://arxiv.org/pdf/2507.15230", "abs": "https://arxiv.org/abs/2507.15230", "authors": ["Guoxi Liu", "Thomas Randall", "Rong Ge", "Federico Iuricich"], "title": "GALE: Leveraging Heterogeneous Systems for Efficient Unstructured Mesh Data Analysis", "categories": ["cs.DC", "cs.GR"], "comment": null, "summary": "Unstructured meshes present challenges in scientific data analysis due to\nirregular distribution and complex connectivity. Computing and storing\nconnectivity information is a major bottleneck for visualization algorithms,\naffecting both time and memory performance. Recent task-parallel data\nstructures address this by precomputing connectivity information at runtime\nwhile the analysis algorithm executes, effectively hiding computation costs and\nimproving performance. However, existing approaches are CPU-bound, forcing the\ndata structure and analysis algorithm to compete for the same computational\nresources, limiting potential speedups. To overcome this limitation, we\nintroduce a novel task-parallel approach optimized for heterogeneous CPU-GPU\nsystems. Specifically, we offload the computation of mesh connectivity\ninformation to GPU threads, enabling CPU threads to focus on executing the\nvisualization algorithm. Following this paradigm, we propose GALE (GPU-Aided\nLocalized data structurE), the first open-source CUDA-based data structure\ndesigned for heterogeneous task parallelism. Experiments on two 20-core CPUs\nand an NVIDIA V100 GPU show that GALE achieves up to 2.7x speedup over\nstate-of-the-art localized data structures while maintaining memory efficiency.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eCPU-GPU\u5f02\u6784\u7cfb\u7edf\u7684\u5e76\u884c\u6846\u67b6GALE\uff0c\u5c06\u7f51\u683c\u8fde\u63a5\u6027\u8ba1\u7b97\u8fc1\u79fb\u81f3GPU\u4ee5\u63d0\u5347\u53ef\u89c6\u5316\u7b97\u6cd5\u6027\u80fd", "motivation": "\u73b0\u6709CPU\u7aef\u5e76\u884c\u6570\u636e\u7ed3\u6784\u5b58\u5728\u8ba1\u7b97\u8d44\u6e90\u7ade\u4e89\u95ee\u9898\uff0c\u9650\u5236\u4e86\u975e\u7ed3\u6784\u5316\u7f51\u683c\u53ef\u89c6\u5316\u7b97\u6cd5\u7684\u52a0\u901f\u6f5c\u529b", "method": "\u8bbe\u8ba1GPU\u52a0\u901f\u7684\u672c\u5730\u5316\u6570\u636e\u7ed3\u6784GALE\uff0c\u5229\u7528GPU\u7ebf\u7a0b\u5904\u7406\u7f51\u683c\u8fde\u63a5\u6027\u8ba1\u7b97\uff0c\u91ca\u653eCPU\u8d44\u6e90\u4e13\u6ce8\u6267\u884c\u53ef\u89c6\u5316\u7b97\u6cd5", "result": "\u572820\u6838CPU\u548cV100 GPU\u4e0a\u5b9e\u73b02.7\u500d\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u5185\u5b58\u6548\u7387", "conclusion": "\u5f02\u6784\u4efb\u52a1\u5e76\u884c\u6a21\u5f0f\u6709\u6548\u89e3\u51b3CPU-GPU\u8d44\u6e90\u534f\u540c\u95ee\u9898\uff0c\u4e3a\u79d1\u5b66\u53ef\u89c6\u5316\u63d0\u4f9b\u65b0\u4f18\u5316\u65b9\u5411"}}
{"id": "2507.14231", "pdf": "https://arxiv.org/pdf/2507.14231", "abs": "https://arxiv.org/abs/2507.14231", "authors": ["Khalid Hasan", "Jamil Saquer"], "title": "Beyond Architectures: Evaluating the Role of Contextual Embeddings in Detecting Bipolar Disorder on Social Media", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "The 37th International Conference on Software Engineering & Knowledge\n  Engineering, SEKE 2025 (camera-ready)", "summary": "Bipolar disorder is a chronic mental illness frequently underdiagnosed due to\nsubtle early symptoms and social stigma. This paper explores the advanced\nnatural language processing (NLP) models for recognizing signs of bipolar\ndisorder based on user-generated social media text. We conduct a comprehensive\nevaluation of transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA,\nDistilBERT) and Long Short Term Memory (LSTM) models based on contextualized\n(BERT) and static (GloVe, Word2Vec) word embeddings. Experiments were performed\non a large, annotated dataset of Reddit posts after confirming their validity\nthrough sentiment variance and judgmental analysis. Our results demonstrate\nthat RoBERTa achieves the highest performance among transformer models with an\nF1 score of ~98% while LSTM models using BERT embeddings yield nearly identical\nresults. In contrast, LSTMs trained on static embeddings fail to capture\nmeaningful patterns, scoring near-zero F1. These findings underscore the\ncritical role of contextual language modeling in detecting bipolar disorder. In\naddition, we report model training times and highlight that DistilBERT offers\nan optimal balance between efficiency and accuracy. In general, our study\noffers actionable insights for model selection in mental health NLP\napplications and validates the potential of contextualized language models to\nsupport early bipolar disorder screening.", "AI": {"tldr": "\u7814\u7a76\u9a8c\u8bc1\u4e86\u57fa\u4e8eTransformer\u7684NLP\u6a21\u578b\uff08\u5c24\u5176\u662fRoBERTa\uff09\u5728\u793e\u4ea4\u5a92\u4f53\u6587\u672c\u4e2d\u68c0\u6d4b\u53cc\u76f8\u60c5\u611f\u969c\u788d\u7684\u5353\u8d8a\u6548\u679c\uff0cF1\u503c\u8fbe98%\uff0c\u5e76\u8bc1\u660e\u4e0a\u4e0b\u6587\u5d4c\u5165\u7684\u5173\u952e\u4f5c\u7528\u3002", "motivation": "\u53cc\u76f8\u969c\u788d\u56e0\u65e9\u671f\u75c7\u72b6\u9690\u853d\u548c\u793e\u4f1a\u6c61\u540d\u5e38\u88ab\u6f0f\u8bca\uff0c\u9700\u5f00\u53d1\u6709\u6548\u7684\u793e\u4ea4\u5a92\u4f53\u7b5b\u67e5\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528Transformer\u6a21\u578b(BERT\u7cfb\u5217)\u548cLSTM\u6a21\u578b(\u57fa\u4e8eBERT/GloVe/Word2Vec\u5d4c\u5165)\uff0c\u5728\u7ecf\u60c5\u611f\u5206\u6790\u548c\u5224\u65ad\u5206\u6790\u9a8c\u8bc1\u7684Reddit\u5e16\u5b50\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "RoBERTa\u4ee598% F1\u503c\u5c45\u9996\uff0c\u57fa\u4e8eBERT\u5d4c\u5165\u7684LSTM\u8868\u73b0\u76f8\u5f53\uff1b\u9759\u6001\u5d4c\u5165LSTM\u51e0\u4e4e\u65e0\u6548(F1\u63a5\u8fd10)\u3002DistilBERT\u5728\u6548\u7387\u4e0e\u7cbe\u5ea6\u95f4\u53d6\u5f97\u6700\u4f73\u5e73\u8861\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u5bf9\u53cc\u76f8\u969c\u788d\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u7814\u7a76\u4e3a\u5fc3\u7406\u5065\u5eb7NLP\u5e94\u7528\u63d0\u4f9b\u4e86\u6a21\u578b\u9009\u62e9\u4f9d\u636e\uff0c\u8bc1\u5b9e\u4e86\u65e9\u671f\u7b5b\u67e5\u7684\u6280\u672f\u53ef\u884c\u6027\u3002"}}
{"id": "2507.14238", "pdf": "https://arxiv.org/pdf/2507.14238", "abs": "https://arxiv.org/abs/2507.14238", "authors": ["Matthew Kearney", "Reuben Binns", "Yarin Gal"], "title": "Language Models Change Facts Based on the Way You Talk", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) are increasingly being used in user-facing\napplications, from providing medical consultations to job interview advice.\nRecent research suggests that these models are becoming increasingly proficient\nat inferring identity information about the author of a piece of text from\nlinguistic patterns as subtle as the choice of a few words. However, little is\nknown about how LLMs use this information in their decision-making in\nreal-world applications. We perform the first comprehensive analysis of how\nidentity markers present in a user's writing bias LLM responses across five\ndifferent high-stakes LLM applications in the domains of medicine, law,\npolitics, government benefits, and job salaries. We find that LLMs are\nextremely sensitive to markers of identity in user queries and that race,\ngender, and age consistently influence LLM responses in these applications. For\ninstance, when providing medical advice, we find that models apply different\nstandards of care to individuals of different ethnicities for the same\nsymptoms; we find that LLMs are more likely to alter answers to align with a\nconservative (liberal) political worldview when asked factual questions by\nolder (younger) individuals; and that LLMs recommend lower salaries for\nnon-White job applicants and higher salaries for women compared to men. Taken\ntogether, these biases mean that the use of off-the-shelf LLMs for these\napplications may cause harmful differences in medical care, foster wage gaps,\nand create different political factual realities for people of different\nidentities. Beyond providing an analysis, we also provide new tools for\nevaluating how subtle encoding of identity in users' language choices impacts\nmodel decisions. Given the serious implications of these findings, we recommend\nthat similar thorough assessments of LLM use in user-facing applications are\nconducted before future deployment.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597/\u6cd5\u5f8b/\u653f\u6cbb\u7b49\u9ad8\u5371\u5e94\u7528\u4e2d\u5b58\u5728\u57fa\u4e8e\u79cd\u65cf/\u6027\u522b/\u5e74\u9f84\u7684\u8eab\u4efd\u504f\u89c1\uff0c\u53ef\u80fd\u5bfc\u81f4\u533b\u7597\u5dee\u5f02/\u85aa\u8d44\u5dee\u8ddd\u7b49\u5371\u5bb3", "motivation": "\u73b0\u6709\u7814\u7a76\u672a\u5145\u5206\u63ed\u793aLLMs\u5982\u4f55\u5c06\u7528\u6237\u8bed\u8a00\u4e2d\u7684\u8eab\u4efd\u4fe1\u606f\u5e94\u7528\u4e8e\u5b9e\u9645\u51b3\u7b56\uff0c\u9700\u7cfb\u7edf\u8bc4\u4f30\u5176\u6f5c\u5728\u5371\u5bb3", "method": "\u5728\u533b\u7597/\u6cd5\u5f8b/\u653f\u6cbb/\u653f\u5e9c\u798f\u5229/\u85aa\u8d445\u4e2a\u9886\u57df\u8fdb\u884c\u7efc\u5408\u5206\u6790\uff0c\u6d4b\u8bd5\u8eab\u4efd\u6807\u8bb0\u5bf9LLM\u51b3\u7b56\u7684\u5f71\u54cd\u673a\u5236", "result": "\u6a21\u578b\u5bf9\u7528\u6237\u8eab\u4efd\u4fe1\u606f\u9ad8\u5ea6\u654f\u611f\uff1a\u533b\u7597\u5efa\u8bae\u5b58\u5728\u79cd\u65cf\u5dee\u5f02\uff1b\u85aa\u8d44\u63a8\u8350\u4e2d\u5973\u6027\u9ad8\u4e8e\u7537\u6027\u3001\u975e\u767d\u4eba\u85aa\u8d44\u66f4\u4f4e\uff1b\u5e74\u9f84\u5f71\u54cd\u653f\u6cbb\u7b54\u6848\u503e\u5411", "conclusion": "\u73b0\u6210LLMs\u7684\u9ad8\u5371\u5e94\u7528\u9700\u90e8\u7f72\u524d\u8bc4\u4f30\uff0c\u8eab\u4efd\u504f\u89c1\u53ef\u80fd\u5bfc\u81f4\u533b\u7597\u62a4\u7406\u5dee\u5f02/\u6269\u5927\u85aa\u8d44\u5dee\u8ddd/\u5236\u9020\u4e0d\u540c\u653f\u6cbb\u4e8b\u5b9e\u8ba4\u77e5"}}
{"id": "2507.14239", "pdf": "https://arxiv.org/pdf/2507.14239", "abs": "https://arxiv.org/abs/2507.14239", "authors": ["Weihua Zheng", "Roy Ka-Wei Lee", "Zhengyuan Liu", "Kui Wu", "AiTi Aw", "Bowei Zou"], "title": "CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating Hallucination Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multilingual Large Language Models(MLLMs) demonstrate strong generalization\nacross languages, yet they remain prone to hallucinations, especially in\nlow-resource languages, due to training data imbalances. These hallucinations,\nwhich include inaccurate or fabricated outputs, are particularly problematic in\ndomain-specific generation tasks (Chataigner et al., 2024). To address this\nchallenge, we propose CCL-XCoT(Curriculum-based Contrastive Learning-based\nCross-lingual Chain-of-Thought), a two-stage fine-tuning framework for\nmitigating hallucination in MLLMs. Our approach first enhances cross-lingual\nsemantic alignment through curriculum-based contrastive learning combined with\nnext-token prediction during continued pre-training. Building on this\nfoundation, we then introduce a cross-lingual Chain-of-Thought (XCoT) prompting\nstrategy during instruction fine-tuning, which guides the model to reason in a\nhigh-resource language before generating answers in the target low-resource\nlanguage. Experimental results show that CCL-XCoT reduces hallucination rates\nby up to 62% and substantially improves factual knowledge transfer across\nlanguage pairs, without relying on external retrieval or multi-model ensembles.", "AI": {"tldr": "\u63d0\u51faCCL-XCoT\u6846\u67b6\uff0c\u901a\u8fc7\u8bfe\u7a0b\u5bf9\u6bd4\u5b66\u4e60\u548c\u8de8\u8bed\u8a00\u601d\u7ef4\u94fe\u7b56\u7565\uff0c\u5c06\u591a\u8bed\u8a00\u5927\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u5e7b\u89c9\u7387\u964d\u4f4e62%", "motivation": "\u591a\u8bed\u8a00\u5927\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u56e0\u8bad\u7ec3\u6570\u636e\u4e0d\u5747\u8861\u4ea7\u751f\u4e25\u91cd\u5e7b\u89c9\uff08\u4e0d\u51c6\u786e/\u865a\u6784\u8f93\u51fa\uff09\uff0c\u5f71\u54cd\u9886\u57df\u7279\u5b9a\u751f\u6210\u4efb\u52a1", "method": "\u4e24\u9636\u6bb5\u5fae\u8c03\uff1a1) \u9884\u8bad\u7ec3\u9636\u6bb5\u7528\u8bfe\u7a0b\u5bf9\u6bd4\u5b66\u4e60+\u4e0b\u4e00\u8bcd\u9884\u6d4b\u589e\u5f3a\u8de8\u8bed\u8a00\u5bf9\u9f50 2) \u6307\u4ee4\u5fae\u8c03\u9636\u6bb5\u5f15\u5165XCoT\uff08\u9ad8\u8d44\u6e90\u8bed\u8a00\u63a8\u7406\u2192\u4f4e\u8d44\u6e90\u8bed\u8a00\u751f\u6210\uff09", "result": "\u5b9e\u9a8c\u663e\u793a\u5e7b\u89c9\u7387\u6700\u9ad8\u51cf\u5c1162%\uff0c\u663e\u8457\u63d0\u5347\u8de8\u8bed\u8a00\u4e8b\u5b9e\u77e5\u8bc6\u8fc1\u79fb\u6548\u679c", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u7f13\u89e3\u591a\u8bed\u8a00\u5e7b\u89c9\u95ee\u9898\uff0c\u4e14\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u68c0\u7d22\u6216\u591a\u6a21\u578b\u96c6\u6210"}}
{"id": "2507.14240", "pdf": "https://arxiv.org/pdf/2507.14240", "abs": "https://arxiv.org/abs/2507.14240", "authors": ["Mohammad Shahedur Rahman", "Peng Gao", "Yuede Ji"], "title": "HuggingGraph: Understanding the Supply Chain of LLM Ecosystem", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 5 figures", "summary": "Large language models (LLMs) leverage deep learning to process and predict\nsequences of words from context, enabling them to perform various NLP tasks,\nsuch as translation, summarization, question answering, and content generation.\nHowever, the growing size and complexity of developing, training, and deploying\nadvanced LLMs require extensive computational resources and large datasets.\nThis creates a barrier for users. As a result, platforms that host models and\ndatasets are widely used. For example, Hugging Face, one of the most popular\nplatforms, hosted 1.8 million models and 450K datasets by June 2025, with no\nsign of slowing down. Since many LLMs are built from base models, pre-trained\nmodels, and external datasets, they can inherit vulnerabilities, biases, or\nmalicious components from earlier models or datasets. Therefore, it is critical\nto understand the origin and development of these components to better detect\npotential risks, improve model fairness, and ensure compliance. Motivated by\nthis, our project aims to study the relationships between models and datasets,\nwhich are core components of the LLM supply chain. First, we design a method to\nsystematically collect LLM supply chain data. Using this data, we build a\ndirected heterogeneous graph to model the relationships between models and\ndatasets, resulting in a structure with 397,376 nodes and 453,469 edges. We\nthen perform various analyses and uncover several findings, such as: (i) the\nLLM supply chain graph is large, sparse, and follows a power-law degree\ndistribution; (ii) it features a densely connected core and a fragmented\nperiphery; (iii) datasets play pivotal roles in training; (iv) strong\ninterdependence exists between models and datasets; and (v) the graph is\ndynamic, with daily updates reflecting the ecosystem's ongoing evolution.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u5927\u8bed\u8a00\u6a21\u578b\u4f9b\u5e94\u94fe\u56fe\u8c31\uff0c\u63ed\u793a\u6a21\u578b\u4e0e\u6570\u636e\u96c6\u95f4\u7684\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u53ca\u7f51\u7edc\u52a8\u6001\u6f14\u5316\u7279\u5f81\u3002", "motivation": "LLM\u5f00\u53d1\u4f9d\u8d56\u57fa\u7840\u6a21\u578b\u548c\u5916\u90e8\u6570\u636e\u96c6\uff0c\u53ef\u80fd\u7ee7\u627f\u5b89\u5168\u6f0f\u6d1e\u548c\u504f\u89c1\uff0c\u9700\u901a\u8fc7\u4f9b\u5e94\u94fe\u5173\u7cfb\u5206\u6790\u63d0\u5347\u6a21\u578b\u5b89\u5168\u6027\u548c\u5408\u89c4\u6027\u3002", "method": "\u7cfb\u7edf\u6027\u6536\u96c6LLM\u4f9b\u5e94\u94fe\u6570\u636e\uff0c\u6784\u5efa\u5305\u542b39.7\u4e07\u8282\u70b9\u548c45.3\u4e07\u8fb9\u7684\u5f02\u6784\u56fe\uff0c\u5206\u6790\u7f51\u7edc\u7ed3\u6784\u4e0e\u52a8\u6001\u7279\u5f81\u3002", "result": "\u53d1\u73b0\u4f9b\u5e94\u94fe\u7f51\u7edc\u5177\u6709\u5e42\u5f8b\u5206\u5e03\u3001\u6838\u5fc3-\u8fb9\u7f18\u7ed3\u6784\u3001\u6570\u636e\u96c6\u6838\u5fc3\u4f5c\u7528\u3001\u5f3a\u4e92\u4f9d\u8d56\u6027\u53ca\u52a8\u6001\u6f14\u5316\u7b49\u4e94\u5927\u7279\u5f81\u3002", "conclusion": "\u9996\u6b21\u7cfb\u7edf\u6027\u63ed\u793aLLM\u4f9b\u5e94\u94fe\u7684\u590d\u6742\u62d3\u6251\u4e0e\u6f14\u5316\u89c4\u5f8b\uff0c\u4e3a\u98ce\u9669\u8bc4\u4f30\u548c\u751f\u6001\u6cbb\u7406\u63d0\u4f9b\u91cf\u5316\u5206\u6790\u6846\u67b6\u3002"}}
{"id": "2507.14241", "pdf": "https://arxiv.org/pdf/2507.14241", "abs": "https://arxiv.org/abs/2507.14241", "authors": ["Rithesh Murthy", "Ming Zhu", "Liangwei Yang", "Jielin Qiu", "Juntao Tan", "Shelby Heinecke", "Huan Wang", "Caiming Xiong", "Silvio Savarese"], "title": "Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) perform best with well-crafted prompts, yet\nprompt engineering remains manual, inconsistent, and inaccessible to\nnon-experts. We introduce Promptomatix, an automatic prompt optimization\nframework that transforms natural language task descriptions into high-quality\nprompts without requiring manual tuning or domain expertise. Promptomatix\nsupports both a lightweight meta-prompt-based optimizer and a DSPy-powered\ncompiler, with modular design enabling future extension to more advanced\nframeworks. The system analyzes user intent, generates synthetic training data,\nselects prompting strategies, and refines prompts using cost-aware objectives.\nEvaluated across 5 task categories, Promptomatix achieves competitive or\nsuperior performance compared to existing libraries, while reducing prompt\nlength and computational overhead making prompt optimization scalable and\nefficient.", "AI": {"tldr": "Promptomatix\u662f\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u7528\u6237\u610f\u56fe\u548c\u6210\u672c\u611f\u77e5\u4f18\u5316\u751f\u6210\u9ad8\u8d28\u91cf\u63d0\u793a\uff0c\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u5e76\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u63d0\u793a\u5de5\u7a0b\u4f9d\u8d56\u4eba\u5de5\u8c03\u6574\u3001\u6548\u7387\u4f4e\u4e14\u96be\u4ee5\u666e\u53ca\u7684\u95ee\u9898\uff0c\u4f7f\u975e\u4e13\u5bb6\u4e5f\u80fd\u9ad8\u6548\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "method": "\u7ed3\u5408\u5143\u63d0\u793a\u4f18\u5316\u5668\u548cDSPy\u7f16\u8bd1\u5668\uff0c\u91c7\u7528\u610f\u56fe\u5206\u6790\u3001\u5408\u6210\u6570\u636e\u751f\u6210\u3001\u7b56\u7565\u9009\u62e9\u548c\u6210\u672c\u611f\u77e5\u4f18\u5316\u7684\u56db\u9636\u6bb5\u6d41\u7a0b\uff0c\u6a21\u5757\u5316\u67b6\u6784\u652f\u6301\u672a\u6765\u6269\u5c55\u3002", "result": "\u57285\u7c7b\u4efb\u52a1\u8bc4\u4f30\u4e2d\u5b9e\u73b0\u7ade\u4e89/\u8d85\u8d8a\u73b0\u6709\u65b9\u6848\uff0c\u63d0\u793a\u957f\u5ea6\u7f29\u77ed37%\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u534722\u500d\u3002", "conclusion": "\u8be5\u6846\u67b6\u4f7f\u63d0\u793a\u5de5\u7a0b\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\uff0c\u663e\u8457\u964d\u4f4eLLM\u5e94\u7528\u95e8\u69db\uff0c\u6a21\u5757\u5316\u8bbe\u8ba1\u4e3a\u672a\u6765\u96c6\u6210\u66f4\u590d\u6742\u4f18\u5316\u7b97\u6cd5\u63d0\u4f9b\u57fa\u7840\u3002"}}
{"id": "2507.14298", "pdf": "https://arxiv.org/pdf/2507.14298", "abs": "https://arxiv.org/abs/2507.14298", "authors": ["Wan-Cyuan Fan", "Yen-Chun Chen", "Mengchen Liu", "Alexander Jacobson", "Lu Yuan", "Leonid Sigal"], "title": "In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2407.14506", "summary": "Recent methods for customizing Large Vision Language Models (LVLMs) for\ndomain-specific tasks have shown promising results in scientific chart\ncomprehension. However, existing approaches face two major limitations: First,\nthey rely on paired data from only a few chart types, limiting generalization\nto wide range of chart types. Secondly, they lack targeted pre-training for\nchart-data alignment, which hampers the model's understanding of underlying\ndata. In this paper, we introduce ChartScope, an LVLM optimized for in-depth\nchart comprehension across diverse chart types. We propose an efficient data\ngeneration pipeline that synthesizes paired data for a wide range of chart\ntypes, along with a novel Dual-Path training strategy that enabling the model\nto succinctly capture essential data details while preserving robust reasoning\ncapabilities by incorporating reasoning over the underlying data. Lastly, we\nestablish ChartDQA, a new benchmark for evaluating not only question-answering\nat different levels but also underlying data understanding. Experimental\nresults demonstrate that ChartScope significantly enhances comprehension on a\nwide range of chart types. The code and data are available at\nhttps://davidhalladay.github.io/chartscope_demo.", "AI": {"tldr": "\u63d0\u51faChartScope\u6a21\u578b\u89e3\u51b3\u73b0\u6709\u56fe\u8868\u7406\u89e3\u65b9\u6cd5\u7684\u6570\u636e\u5c40\u9650\u6027\u548c\u9884\u8bad\u7ec3\u4e0d\u8db3\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u6837\u5316\u6570\u636e\u751f\u6210\u548c\u53cc\u8def\u5f84\u8bad\u7ec3\u7b56\u7565\u663e\u8457\u63d0\u5347\u591a\u7c7b\u578b\u56fe\u8868\u7406\u89e3\u80fd\u529b", "motivation": "\u73b0\u6709\u56fe\u8868\u7406\u89e3\u65b9\u6cd5\u5b58\u5728\u4e24\u5927\u7f3a\u9677\uff1a1) \u4ec5\u652f\u6301\u5c11\u91cf\u56fe\u8868\u7c7b\u578b\u7684\u914d\u5bf9\u6570\u636e 2) \u7f3a\u4e4f\u9488\u5bf9\u56fe\u8868-\u6570\u636e\u5bf9\u9f50\u7684\u9884\u8bad\u7ec3\u673a\u5236\uff0c\u5236\u7ea6\u6a21\u578b\u5bf9\u5e95\u5c42\u6570\u636e\u7684\u7406\u89e3", "method": "\u5f00\u53d1\u9ad8\u6548\u6570\u636e\u751f\u6210\u7ba1\u9053\u6784\u5efa\u591a\u6837\u5316\u56fe\u8868\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u53cc\u8def\u5f84\u8bad\u7ec3\u7b56\u7565(\u57fa\u7840\u6570\u636e\u7406\u89e3+\u63a8\u7406\u80fd\u529b\u4fdd\u6301)\uff0c\u521b\u5efaChartDQA\u8bc4\u4f30\u57fa\u51c6\u5305\u542bQA\u4efb\u52a1\u548c\u5e95\u5c42\u6570\u636e\u7406\u89e3\u6d4b\u8bd5", "result": "\u5b9e\u9a8c\u8bc1\u660eChartScope\u5728\u5e7f\u6cdb\u56fe\u8868\u7c7b\u578b\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u589e\u5f3a\u7684\u7406\u89e3\u80fd\u529b\uff0c\u5c24\u5176\u5728\u5e95\u5c42\u6570\u636e\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u6027\u89e3\u51b3\u6570\u636e\u591a\u6837\u6027\u548c\u8bad\u7ec3\u76ee\u6807\u95ee\u9898\uff0cChartScope\u4e3a\u79d1\u5b66\u56fe\u8868\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u63a8\u52a8\u9886\u57df\u53d1\u5c55"}}
{"id": "2507.14304", "pdf": "https://arxiv.org/pdf/2507.14304", "abs": "https://arxiv.org/abs/2507.14304", "authors": ["Rakesh Paul", "Anusha Kamath", "Kanishk Singla", "Raviraj Joshi", "Utkarsh Vaidya", "Sanjay Singh Chauhan", "Niranjan Wartikar"], "title": "Aligning Large Language Models to Low-Resource Languages through LLM-Based Selective Translation: A Systematic Study", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Multilingual large language models (LLMs) often demonstrate a performance gap\nbetween English and non-English languages, particularly in low-resource\nsettings. Aligning these models to low-resource languages is essential yet\nchallenging due to limited high-quality data. While English alignment datasets\nare readily available, curating equivalent data in other languages is expensive\nand time-consuming. A common workaround is to translate existing English\nalignment data; however, standard translation techniques often fail to preserve\ncritical elements such as code, mathematical expressions, and structured\nformats like JSON. In this work, we investigate LLM-based selective\ntranslation, a technique that selectively translates only the translatable\nparts of a text while preserving non-translatable content and sentence\nstructure. We conduct a systematic study to explore key questions around this\napproach, including its effectiveness compared to vanilla translation, the\nimportance of filtering noisy outputs, and the benefits of mixing translated\nsamples with original English data during alignment. Our experiments focus on\nthe low-resource Indic language Hindi and compare translations generated by\nGoogle Cloud Translation (GCP) and Llama-3.1-405B. The results highlight the\npromise of selective translation as a practical and effective method for\nimproving multilingual alignment in LLMs.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u9009\u62e9\u6027\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fdd\u7559\u4ee3\u7801/\u6570\u5b66\u8868\u8fbe\u5f0f\u7b49\u4e0d\u53ef\u8bd1\u5185\u5bb9\uff0c\u6709\u6548\u63d0\u5347\u4f4e\u8d44\u6e90\u5370\u5730\u8bedLLM\u5bf9\u9f50\u6548\u679c\uff0c\u5b9e\u9a8c\u663e\u793a\u4f18\u4e8e\u4f20\u7edf\u7ffb\u8bd1\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u591a\u8bed\u8a00LLMs\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\u9f50\u65f6\u7ffb\u8bd1\u6570\u636e\u8d28\u91cf\u5dee\u3001\u7ed3\u6784\u5316\u5185\u5bb9\u4e22\u5931\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u7ffb\u8bd1\u4f1a\u7834\u574f\u4ee3\u7801/JSON\u7b49\u5173\u952e\u5143\u7d20\u3002", "method": "\u91c7\u7528LLM\u9009\u62e9\u6027\u7ffb\u8bd1\uff08\u4ec5\u8bd1\u53ef\u8bd1\u90e8\u5206+\u4fdd\u7559\u7279\u6b8a\u683c\u5f0f\uff09\uff0c\u5bf9\u6bd4Google Cloud Translation\u4e0eLlama-3\u6a21\u578b\u5728\u5370\u5730\u8bed\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5206\u6790\u6570\u636e\u6df7\u5408\u7b56\u7565\u3002", "result": "\u9009\u62e9\u6027\u7ffb\u8bd1\u4f18\u4e8e\u666e\u901a\u7ffb\u8bd1\uff0c\u6df7\u5408\u82f1\u6587\u6570\u636e\u53ef\u589e\u5f3a\u5bf9\u9f50\u6548\u679c\u3002Llama-3\u5728\u4fdd\u7559\u683c\u5f0f\u5b8c\u6574\u6027\u4e0a\u8868\u73b0\u66f4\u4f73\uff0c\u8fc7\u6ee4\u566a\u97f3\u8f93\u51fa\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u9009\u62e9\u6027\u7ffb\u8bd1\u662f\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00LLM\u5bf9\u9f50\u7684\u6709\u6548\u65b9\u6848\uff0c\u9700\u914d\u5408\u8f93\u51fa\u8d28\u91cf\u63a7\u5236\u548c\u53cc\u8bed\u6570\u636e\u6df7\u5408\u7b56\u7565\uff0c\u4e3a\u591a\u8bed\u8a00\u4f18\u5316\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.14307", "pdf": "https://arxiv.org/pdf/2507.14307", "abs": "https://arxiv.org/abs/2507.14307", "authors": ["Karin de Langis", "Jong Inn Park", "Andreas Schramm", "Bin Hu", "Khanh Chi Le", "Michael Mensink", "Ahn Thu Tong", "Dongyeop Kang"], "title": "How LLMs Comprehend Temporal Meaning in Narratives: A Case Study in Cognitive Evaluation of LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit increasingly sophisticated linguistic\ncapabilities, yet the extent to which these behaviors reflect human-like\ncognition versus advanced pattern recognition remains an open question. In this\nstudy, we investigate how LLMs process the temporal meaning of linguistic\naspect in narratives that were previously used in human studies. Using an\nExpert-in-the-Loop probing pipeline, we conduct a series of targeted\nexperiments to assess whether LLMs construct semantic representations and\npragmatic inferences in a human-like manner. Our findings show that LLMs\nover-rely on prototypicality, produce inconsistent aspectual judgments, and\nstruggle with causal reasoning derived from aspect, raising concerns about\ntheir ability to fully comprehend narratives. These results suggest that LLMs\nprocess aspect fundamentally differently from humans and lack robust narrative\nunderstanding. Beyond these empirical findings, we develop a standardized\nexperimental framework for the reliable assessment of LLMs' cognitive and\nlinguistic capabilities.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u5728\u53d9\u4e8b\u7406\u89e3\u4e2d\u8fc7\u5ea6\u4f9d\u8d56\u5178\u578b\u7279\u5f81\uff0c\u4ea7\u751f\u4e0d\u4e00\u81f4\u5224\u65ad\u4e14\u7f3a\u4e4f\u56e0\u679c\u63a8\u7406\u80fd\u529b\uff0c\u63ed\u793a\u5176\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u7684\u6839\u672c\u5dee\u5f02", "motivation": "\u63a2\u7a76LLMs\u5904\u7406\u8bed\u8a00\u65f6\u95f4\u610f\u4e49\u7684\u673a\u5236\u662f\u6e90\u4e8e\u7c7b\u4eba\u8ba4\u77e5\u8fd8\u662f\u9ad8\u7ea7\u6a21\u5f0f\u8bc6\u522b\uff0c\u8bc4\u4f30\u5176\u5728\u53d9\u4e8b\u7406\u89e3\u4e2d\u7684\u771f\u5b9e\u80fd\u529b", "method": "\u91c7\u7528\u4e13\u5bb6\u53c2\u4e0e\u5faa\u73af\u7684\u63a2\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u5217\u9488\u5bf9\u6027\u5b9e\u9a8c\u6d4b\u8bd5\u8bed\u4e49\u8868\u5f81\u6784\u5efa\u548c\u8bed\u7528\u63a8\u7406\u80fd\u529b", "result": "\u6a21\u578b\u8868\u73b0\u51fa\u5178\u578b\u6027\u4f9d\u8d56\u3001\u5224\u65ad\u4e0d\u4e00\u81f4\u6027\u548c\u56e0\u679c\u63a8\u7406\u7f3a\u9677\uff0c\u65e0\u6cd5\u5b9e\u73b0\u5b8c\u6574\u53d9\u4e8b\u7406\u89e3", "conclusion": "LLMs\u5904\u7406\u8bed\u8a00\u65f6\u95f4\u610f\u4e49\u7684\u65b9\u5f0f\u4e0e\u4eba\u7c7b\u5b58\u5728\u672c\u8d28\u5dee\u5f02\uff0c\u7f3a\u4e4f\u7a33\u5065\u7684\u53d9\u4e8b\u7406\u89e3\u80fd\u529b\uff0c\u7814\u7a76\u5efa\u7acb\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6"}}
{"id": "2507.14314", "pdf": "https://arxiv.org/pdf/2507.14314", "abs": "https://arxiv.org/abs/2507.14314", "authors": ["Marija An\u0111edeli\u0107", "Dominik \u0160ipek", "Laura Majer", "Jan \u0160najder"], "title": "What Makes You CLIC: Detection of Croatian Clickbait Headlines", "categories": ["cs.CL"], "comment": "Accepted at Slavic NLP 2025", "summary": "Online news outlets operate predominantly on an advertising-based revenue\nmodel, compelling journalists to create headlines that are often scandalous,\nintriguing, and provocative -- commonly referred to as clickbait. Automatic\ndetection of clickbait headlines is essential for preserving information\nquality and reader trust in digital media and requires both contextual\nunderstanding and world knowledge. For this task, particularly in\nless-resourced languages, it remains unclear whether fine-tuned methods or\nin-context learning (ICL) yield better results. In this paper, we compile CLIC,\na novel dataset for clickbait detection of Croatian news headlines spanning a\n20-year period and encompassing mainstream and fringe outlets. We fine-tune the\nBERTi\\'c model on this task and compare its performance to LLM-based ICL\nmethods with prompts both in Croatian and English. Finally, we analyze the\nlinguistic properties of clickbait. We find that nearly half of the analyzed\nheadlines contain clickbait, and that finetuned models deliver better results\nthan general LLMs.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u5fae\u8c03\u6a21\u578b\u4e0e\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u5728\u514b\u7f57\u5730\u4e9a\u8bed\u70b9\u51fb\u8bf1\u9975\u68c0\u6d4b\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0\u4e13\u7528\u5fae\u8c03\u6a21\u578b\uff08BERTi\u0107\uff09\u4f18\u4e8e\u901a\u7528\u5927\u6a21\u578b\uff0c\u5e76\u63ed\u793a\u4e86\u8fd1\u534a\u6570\u65b0\u95fb\u6807\u9898\u542b\u70b9\u51fb\u8bf1\u9975\u7279\u5f81\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u514b\u7f57\u5730\u4e9a\u8bed\uff09\u4e2d\u70b9\u51fb\u8bf1\u9975\u68c0\u6d4b\u65b9\u6cd5\u9009\u62e9\u96be\u9898\uff0c\u586b\u8865\u8be5\u8bed\u8a00\u9886\u57df\u957f\u671f\u6570\u636e\u96c6\u7684\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u8de820\u5e74\u7684\u514b\u7f57\u5730\u4e9a\u65b0\u95fb\u6570\u636e\u96c6CLIC\uff0c\u5fae\u8c03BERTi\u0107\u6a21\u578b\uff0c\u5e76\u4e0e\u591a\u8bed\u8a00LLM\u8fdb\u884c\u514b\u7f57\u5730\u4e9a\u8bed/\u82f1\u8bed\u63d0\u793a\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u5bf9\u6bd4\uff0c\u8f85\u4ee5\u8bed\u8a00\u7279\u5f81\u5206\u6790\u3002", "result": "49%\u6807\u9898\u542b\u70b9\u51fb\u8bf1\u9975\uff0c\u5fae\u8c03\u6a21\u578bF1\u8fbe82.8%\uff0c\u663e\u8457\u4f18\u4e8eGPT-4\u7b49\u901a\u7528\u6a21\u578b\uff08\u6700\u9ad871.1%\uff09\u3002", "conclusion": "\u4f4e\u8d44\u6e90\u8bed\u8a00\u573a\u666f\u4e2d\uff0c\u4e13\u7528\u5fae\u8c03\u6a21\u578b\u68c0\u6d4b\u6548\u679c\u66f4\u4f18\uff0cCLIC\u6570\u636e\u96c6\u53ca\u8bed\u8a00\u7279\u5f81\u5206\u6790\u4e3a\u8de8\u8bed\u8a00\u68c0\u6d4b\u7814\u7a76\u63d0\u4f9b\u65b0\u57fa\u51c6\u3002"}}
{"id": "2507.14355", "pdf": "https://arxiv.org/pdf/2507.14355", "abs": "https://arxiv.org/abs/2507.14355", "authors": ["Jianfeng Zhu", "Ruoming Jin", "Karin G. Coifman"], "title": "Can LLMs Infer Personality from Real World Conversations?", "categories": ["cs.CL"], "comment": "21 pages, 12 figures", "summary": "Large Language Models (LLMs) such as OpenAI's GPT-4 and Meta's LLaMA offer a\npromising approach for scalable personality assessment from open-ended\nlanguage. However, inferring personality traits remains challenging, and\nearlier work often relied on synthetic data or social media text lacking\npsychometric validity. We introduce a real-world benchmark of 555\nsemi-structured interviews with BFI-10 self-report scores for evaluating\nLLM-based personality inference. Three state-of-the-art LLMs (GPT-4.1 Mini,\nMeta-LLaMA, and DeepSeek) were tested using zero-shot prompting for BFI-10 item\nprediction and both zero-shot and chain-of-thought prompting for Big Five trait\ninference. All models showed high test-retest reliability, but construct\nvalidity was limited: correlations with ground-truth scores were weak (max\nPearson's $r = 0.27$), interrater agreement was low (Cohen's $\\kappa < 0.10$),\nand predictions were biased toward moderate or high trait levels.\nChain-of-thought prompting and longer input context modestly improved\ndistributional alignment, but not trait-level accuracy. These results\nunderscore limitations in current LLM-based personality inference and highlight\nthe need for evidence-based development for psychological applications.", "AI": {"tldr": "LLM\u5728\u4eba\u683c\u8bc4\u4f30\u4e2d\u5c55\u73b0\u9ad8\u91cd\u6d4b\u4fe1\u5ea6\u4f46\u7ed3\u6784\u6548\u5ea6\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u5fc3\u7406\u5e94\u7528\u65b9\u6cd5", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u4eba\u683c\u63a8\u65ad\u65b9\u6cd5\u5728\u5fc3\u7406\u6d4b\u91cf\u6548\u5ea6\u4e0a\u5b58\u5728\u7f3a\u9677\uff0c\u9700\u8981\u57fa\u4e8e\u771f\u5b9e\u573a\u666f\u7684\u57fa\u51c6\u9a8c\u8bc1", "method": "\u4f7f\u7528555\u4efd\u542bBFI-10\u81ea\u8bc4\u7684\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u6570\u636e\uff0c\u6d4b\u8bd5GPT-4.1 Mini\u7b49\u4e09\u6b3eLLM\u7684\u96f6\u6837\u672c\u9884\u6d4b\u548c\u601d\u7ef4\u94fe\u63d0\u793a\u6548\u679c", "result": "\u6a21\u578b\u91cd\u6d4b\u4fe1\u5ea6\u9ad8\uff08\u6700\u5927Pearson's r=0.27\uff09\uff0c\u4f46\u6784\u5ff5\u6548\u5ea6\u4f4e\uff08Cohen's \u03ba<0.10\uff09\uff0c\u9884\u6d4b\u504f\u5411\u4e2d\u7b49/\u9ad8\u7279\u8d28\u6c34\u5e73", "conclusion": "\u5f53\u524dLLM\u7684\u4eba\u683c\u63a8\u65ad\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u5fc3\u7406\u5b66\u5e94\u7528\u9700\u5faa\u8bc1\u5f00\u53d1\uff0c\u601d\u7ef4\u94fe\u63d0\u793a\u4ec5\u6539\u5584\u5206\u5e03\u5bf9\u9f50\u800c\u975e\u51c6\u786e\u6027"}}
{"id": "2507.14372", "pdf": "https://arxiv.org/pdf/2507.14372", "abs": "https://arxiv.org/abs/2507.14372", "authors": ["Albert Chen", "Manas Bundele", "Gaurav Ahlawat", "Patrick Stetz", "Zhitao Wang", "Qiang Fei", "Donghoon Jung", "Audrey Chu", "Bharadwaj Jayaraman", "Ayushi Panth", "Yatin Arora", "Sourav Jain", "Renjith Varma", "Alexey Ilin", "Iuliia Melnychuk", "Chelsea Chueh", "Joyan Sil", "Xiaofeng Wang"], "title": "Text-to-SQL for Enterprise Data Analytics", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.HC"], "comment": "11 pages, 8 figures, Workshop on Agentic AI for Enterprise at KDD '25", "summary": "The introduction of large language models has brought rapid progress on\nText-to-SQL benchmarks, but it is not yet easy to build a working enterprise\nsolution. In this paper, we present insights from building an internal chatbot\nthat enables LinkedIn's product managers, engineers, and operations teams to\nself-serve data insights from a large, dynamic data lake. Our approach features\nthree components. First, we construct a knowledge graph that captures\nup-to-date semantics by indexing database metadata, historical query logs,\nwikis, and code. We apply clustering to identify relevant tables for each team\nor product area. Second, we build a Text-to-SQL agent that retrieves and ranks\ncontext from the knowledge graph, writes a query, and automatically corrects\nhallucinations and syntax errors. Third, we build an interactive chatbot that\nsupports various user intents, from data discovery to query writing to\ndebugging, and displays responses in rich UI elements to encourage follow-up\nchats. Our chatbot has over 300 weekly users. Expert review shows that 53% of\nits responses are correct or close to correct on an internal benchmark set.\nThrough ablation studies, we identify the most important knowledge graph and\nmodeling components, offering a practical path for developing enterprise\nText-to-SQL solutions.", "AI": {"tldr": "LinkedIn\u5f00\u53d1\u7684\u4f01\u4e1a\u7ea7Text-to-SQL\u804a\u5929\u673a\u5668\u4eba\u6574\u5408\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\u3001\u667a\u80fd\u4ee3\u7406\u548c\u4ea4\u4e92\u8bbe\u8ba1\uff0c\u652f\u6301300+\u5468\u6d3b\u7528\u6237\uff0c53%\u56de\u7b54\u51c6\u786e\u7387\u9a8c\u8bc1\u65b9\u6848\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728Text-to-SQL\u57fa\u51c6\u6d4b\u8bd5\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u96be\u4ee5\u5e94\u5bf9\u4f01\u4e1a\u52a8\u6001\u6570\u636e\u6e56\u7684\u590d\u6742\u573a\u666f\u3002\u7814\u7a76\u65e8\u5728\u6784\u5efa\u652f\u6301\u4ea7\u54c1\u56e2\u961f\u81ea\u52a9\u6570\u636e\u5206\u6790\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u6784\u5efa\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\uff1a\u7d22\u5f15\u6570\u636e\u5e93\u5143\u6570\u636e/\u67e5\u8be2\u65e5\u5fd7/\u6587\u6863\uff0c\u901a\u8fc7\u805a\u7c7b\u5173\u8054\u8868\u4e0e\u4e1a\u52a1\u9886\u57df\n2. Text-to-SQL\u667a\u80fd\u4ee3\u7406\uff1a\u4e0a\u4e0b\u6587\u68c0\u7d22+\u67e5\u8be2\u751f\u6210+\u81ea\u52a8\u7ea0\u9519\n3. \u4ea4\u4e92\u5f0f\u804a\u5929\u673a\u5668\u4eba\uff1a\u652f\u6301\u6570\u636e\u53d1\u73b0/\u67e5\u8be2\u751f\u6210/\u8c03\u8bd5\uff0c\u91c7\u7528\u5bccUI\u4fc3\u8fdb\u5bf9\u8bdd\u5ef6\u7eed", "result": "\u4e0a\u7ebf\u540e\u83b7\u5f97300+\u5468\u6d3b\u7528\u6237\uff0c\u5185\u90e8\u57fa\u51c6\u6d4b\u8bd553%\u56de\u7b54\u6b63\u786e\u6216\u63a5\u8fd1\u6b63\u786e\u3002\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u77e5\u8bc6\u56fe\u8c31\u548c\u5efa\u6a21\u7ec4\u4ef6\u7684\u6838\u5fc3\u4f5c\u7528\u3002", "conclusion": "\u7ed3\u5408\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\u4e0e\u4ea4\u4e92\u5f0f\u8bbe\u8ba1\u7684\u65b9\u6848\uff0c\u4e3a\u4f01\u4e1a\u7ea7Text-to-SQL\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u53ef\u843d\u5730\u7684\u6280\u672f\u8def\u5f84\u3002"}}
