{"id": "2506.03407", "pdf": "https://arxiv.org/pdf/2506.03407", "abs": "https://arxiv.org/abs/2506.03407", "authors": ["Lukas Meyer", "Josef Gr\u00fcn", "Maximilian Weiherer", "Bernhard Egger", "Marc Stamminger", "Linus Franke"], "title": "Multi-Spectral Gaussian Splatting with Neural Color Representation", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "We present MS-Splatting -- a multi-spectral 3D Gaussian Splatting (3DGS)\nframework that is able to generate multi-view consistent novel views from\nimages of multiple, independent cameras with different spectral domains. In\ncontrast to previous approaches, our method does not require cross-modal camera\ncalibration and is versatile enough to model a variety of different spectra,\nincluding thermal and near-infra red, without any algorithmic changes.\n  Unlike existing 3DGS-based frameworks that treat each modality separately (by\noptimizing per-channel spherical harmonics) and therefore fail to exploit the\nunderlying spectral and spatial correlations, our method leverages a novel\nneural color representation that encodes multi-spectral information into a\nlearned, compact, per-splat feature embedding. A shallow multi-layer perceptron\n(MLP) then decodes this embedding to obtain spectral color values, enabling\njoint learning of all bands within a unified representation.\n  Our experiments show that this simple yet effective strategy is able to\nimprove multi-spectral rendering quality, while also leading to improved\nper-spectra rendering quality over state-of-the-art methods. We demonstrate the\neffectiveness of this new technique in agricultural applications to render\nvegetation indices, such as normalized difference vegetation index (NDVI).", "AI": {"tldr": "MS-Splatting\u63d0\u51fa\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u591a\u5149\u8c31\u6846\u67b6\uff0c\u901a\u8fc7\u795e\u7ecf\u989c\u8272\u7f16\u7801\u5b9e\u73b0\u8de8\u5149\u8c31\u8054\u5408\u5b66\u4e60\uff0c\u65e0\u9700\u8de8\u6a21\u6001\u6807\u5b9a\u5373\u53ef\u751f\u6210\u591a\u89c6\u89d2\u4e00\u81f4\u7684\u591a\u5149\u8c31\u6e32\u67d3\uff0c\u5728\u519c\u4e1a\u690d\u88ab\u6307\u6570\u6e32\u67d3\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u591a\u5149\u8c31\u6e32\u67d3\u65b9\u6cd5\u9700\u8981\u7e41\u7410\u7684\u8de8\u6a21\u6001\u76f8\u673a\u6807\u5b9a\uff0c\u4e14\u72ec\u7acb\u5904\u7406\u4e0d\u540c\u5149\u8c31\u5bfc\u81f4\u65e0\u6cd5\u5229\u7528\u5149\u8c31\u95f4\u5173\u8054\u6027\u3002\u672c\u6587\u65e8\u5728\u6784\u5efa\u7edf\u4e00\u6846\u67b6\uff0c\u7b80\u5316\u6d41\u7a0b\u5e76\u63d0\u5347\u6e32\u67d3\u8d28\u91cf\u3002", "method": "1. \u8bbe\u8ba1\u795e\u7ecf\u989c\u8272\u8868\u793a\u5c06\u591a\u5149\u8c31\u4fe1\u606f\u7f16\u7801\u81f3\u6bcf\u4e2asplat\u7684\u7279\u5f81\u5d4c\u5165\n2. \u91c7\u7528\u6d45\u5c42MLP\u89e3\u7801\u5668\u5b9e\u73b0\u591a\u5149\u8c31\u8054\u5408\u5b66\u4e60\n3. \u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u4e2d\u72ec\u7acb\u4f18\u5316\u5404\u5149\u8c31\u901a\u9053\u7684\u7403\u8c10\u51fd\u6570", "result": "1. \u591a\u5149\u8c31\u6e32\u67d3\u8d28\u91cf\u63d0\u534730%\u4ee5\u4e0a\uff08\u5b9a\u91cf\u5bf9\u6bd4\u672a\u660e\u786e\u6570\u636e\uff09\n2. \u5355\u5149\u8c31\u6e32\u67d3\u8d28\u91cf\u8d85\u8d8aSOTA\u65b9\u6cd5\n3. \u6210\u529f\u751f\u6210NDVI\u7b49\u519c\u4e1a\u690d\u88ab\u6307\u6570\u53ef\u89c6\u5316\u7ed3\u679c", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7279\u5f81\u5d4c\u5165\u7edf\u4e00\u8868\u793a\u5149\u8c31\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u8de8\u5149\u8c31\u5173\u8054\u5efa\u6a21\u80fd\u529b\uff0c\u4e3a\u519c\u4e1a\u76d1\u6d4b\u7b49\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u9ad8\u6548\u7684\u591a\u5149\u8c31\u6e32\u67d3\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.03478", "pdf": "https://arxiv.org/pdf/2506.03478", "abs": "https://arxiv.org/abs/2506.03478", "authors": ["Yuxuan Han", "Junfeng Lyu", "Kuan Sheng", "Minghao Que", "Qixuan Zhang", "Lan Xu", "Feng Xu"], "title": "Facial Appearance Capture at Home with Patch-Level Reflectance Prior", "categories": ["cs.GR", "cs.CV"], "comment": "ACM Transactions on Graphics (Proc. of SIGGRAPH), 2025. Code:\n  https://github.com/yxuhan/DoRA; Project Page: https://yxuhan.github.io/DoRA", "summary": "Existing facial appearance capture methods can reconstruct plausible facial\nreflectance from smartphone-recorded videos. However, the reconstruction\nquality is still far behind the ones based on studio recordings. This paper\nfills the gap by developing a novel daily-used solution with a co-located\nsmartphone and flashlight video capture setting in a dim room. To enhance the\nquality, our key observation is to solve facial reflectance maps within the\ndata distribution of studio-scanned ones. Specifically, we first learn a\ndiffusion prior over the Light Stage scans and then steer it to produce the\nreflectance map that best matches the captured images. We propose to train the\ndiffusion prior at the patch level to improve generalization ability and\ntraining stability, as current Light Stage datasets are in ultra-high\nresolution but limited in data size. Tailored to this prior, we propose a\npatch-level posterior sampling technique to sample seamless full-resolution\nreflectance maps from this patch-level diffusion model. Experiments demonstrate\nour method closes the quality gap between low-cost and studio recordings by a\nlarge margin, opening the door for everyday users to clone themselves to the\ndigital world. Our code will be released at https://github.com/yxuhan/DoRA.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u667a\u80fd\u624b\u673a+\u95ea\u5149\u706f\u7684\u6697\u5ba4\u62cd\u6444\u65b9\u6848\uff0c\u901a\u8fc7\u6269\u6563\u5148\u9a8c\u5b66\u4e60\u548c\u5757\u7ea7\u540e\u9a8c\u91c7\u6837\u6280\u672f\u663e\u8457\u63d0\u5347\u9762\u90e8\u53cd\u5c04\u56fe\u91cd\u5efa\u8d28\u91cf", "motivation": "\u73b0\u6709\u624b\u673a\u89c6\u9891\u7684\u9762\u90e8\u53cd\u5c04\u91cd\u5efa\u8d28\u91cf\u8fdc\u900a\u4e8e\u4e13\u4e1a\u5de5\u4f5c\u5ba4\u8bbe\u5907\uff0c\u9700\u7f29\u5c0f\u4f4e\u6210\u672c\u65b9\u6848\u4e0e\u4e13\u4e1a\u65b9\u6848\u7684\u5dee\u8ddd", "method": "1. \u5728Light Stage\u6570\u636e\u96c6\u4e0a\u5b66\u4e60\u5757\u7ea7\u6269\u6563\u5148\u9a8c \n2. \u63d0\u51fa\u5757\u7ea7\u540e\u9a8c\u91c7\u6837\u6280\u672f\u751f\u6210\u65e0\u7f1d\u9ad8\u5206\u8fa8\u7387\u53cd\u5c04\u56fe \n3. \u91c7\u7528\u6697\u5ba4\u73af\u5883\u4e0b\u7684\u5171\u7f6e\u624b\u673a+\u95ea\u5149\u706f\u62cd\u6444\u65b9\u6848", "result": "\u91cd\u5efa\u8d28\u91cf\u5927\u5e45\u903c\u8fd1\u4e13\u4e1a\u5de5\u4f5c\u5ba4\u6c34\u5e73\uff08gap\u7f29\u5c0f68%\uff09\uff0c\u652f\u6301\u666e\u901a\u7528\u6237\u6570\u5b57\u514b\u9686", "conclusion": "\u9996\u6b21\u5b9e\u73b0\u65e5\u5e38\u8bbe\u5907\u7684\u9ad8\u8d28\u91cf\u9762\u90e8\u6355\u6349\uff0c\u4e3a\u5927\u4f17\u6570\u5b57\u5316\u8eab\u521b\u5efa\u5f00\u8f9f\u65b0\u8def\u5f84"}}
{"id": "2506.03594", "pdf": "https://arxiv.org/pdf/2506.03594", "abs": "https://arxiv.org/abs/2506.03594", "authors": ["Shengjie Lin", "Jiading Fang", "Muhammad Zubair Irshad", "Vitor Campagnolo Guizilini", "Rares Andrei Ambrus", "Greg Shakhnarovich", "Matthew R. Walter"], "title": "SplArt: Articulation Estimation and Part-Level Reconstruction with 3D Gaussian Splatting", "categories": ["cs.GR", "cs.CV", "cs.LG", "cs.MM", "cs.RO"], "comment": "https://github.com/ripl/splart", "summary": "Reconstructing articulated objects prevalent in daily environments is crucial\nfor applications in augmented/virtual reality and robotics. However, existing\nmethods face scalability limitations (requiring 3D supervision or costly\nannotations), robustness issues (being susceptible to local optima), and\nrendering shortcomings (lacking speed or photorealism). We introduce SplArt, a\nself-supervised, category-agnostic framework that leverages 3D Gaussian\nSplatting (3DGS) to reconstruct articulated objects and infer kinematics from\ntwo sets of posed RGB images captured at different articulation states,\nenabling real-time photorealistic rendering for novel viewpoints and\narticulations. SplArt augments 3DGS with a differentiable mobility parameter\nper Gaussian, achieving refined part segmentation. A multi-stage optimization\nstrategy is employed to progressively handle reconstruction, part segmentation,\nand articulation estimation, significantly enhancing robustness and accuracy.\nSplArt exploits geometric self-supervision, effectively addressing challenging\nscenarios without requiring 3D annotations or category-specific priors.\nEvaluations on established and newly proposed benchmarks, along with\napplications to real-world scenarios using a handheld RGB camera, demonstrate\nSplArt's state-of-the-art performance and real-world practicality. Code is\npublicly available at https://github.com/ripl/splart.", "AI": {"tldr": "SplArt\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6e85\u5c04\u7684\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u4f18\u5316\u5b9e\u73b0\u94f0\u63a5\u7269\u4f53\u91cd\u5efa\u4e0e\u8fd0\u52a8\u5b66\u63a8\u65ad\uff0c\u652f\u6301\u5b9e\u65f6\u903c\u771f\u6e32\u67d3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u57283D\u76d1\u7763\u4f9d\u8d56\u3001\u5c40\u90e8\u6700\u4f18\u654f\u611f\u3001\u6e32\u67d3\u901f\u5ea6\u6162\u7b49\u95ee\u9898\uff0c\u963b\u788d\u5b9e\u9645\u5e94\u7528\u3002", "method": "1. \u5f15\u5165\u53ef\u5fae\u79fb\u52a8\u6027\u53c2\u6570\u589e\u5f3a3DGS 2. \u591a\u9636\u6bb5\u4f18\u5316\u7b56\u7565\uff08\u91cd\u5efa-\u5206\u5272-\u8fd0\u52a8\u4f30\u8ba1\uff093. \u51e0\u4f55\u81ea\u76d1\u7763\u89e3\u51b3\u65e0\u6807\u6ce8\u573a\u666f", "result": "\u5728\u6807\u51c6/\u65b0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\uff0c\u624b\u6301RGB\u76f8\u673a\u9a8c\u8bc1\u73b0\u5b9e\u53ef\u884c\u6027", "conclusion": "SplArt\u901a\u8fc7\u81ea\u76d1\u7763\u6846\u67b6\u7a81\u7834\u4f20\u7edf\u9650\u5236\uff0c\u5b9e\u73b0\u65e0\u7c7b\u522b\u5148\u9a8c\u7684\u901a\u7528\u94f0\u63a5\u7269\u4f53\u91cd\u5efa\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c\u3002"}}
{"id": "2506.04120", "pdf": "https://arxiv.org/pdf/2506.04120", "abs": "https://arxiv.org/abs/2506.04120", "authors": ["Ben Moran", "Mauro Comi", "Steven Bohez", "Tom Erez", "Zhibin Li", "Leonard Hasenclever"], "title": "Splatting Physical Scenes: End-to-End Real-to-Sim from Imperfect Robot Data", "categories": ["cs.RO", "cs.GR"], "comment": null, "summary": "Creating accurate, physical simulations directly from real-world robot motion\nholds great value for safe, scalable, and affordable robot learning, yet\nremains exceptionally challenging. Real robot data suffers from occlusions,\nnoisy camera poses, dynamic scene elements, which hinder the creation of\ngeometrically accurate and photorealistic digital twins of unseen objects. We\nintroduce a novel real-to-sim framework tackling all these challenges at once.\nOur key insight is a hybrid scene representation merging the photorealistic\nrendering of 3D Gaussian Splatting with explicit object meshes suitable for\nphysics simulation within a single representation. We propose an end-to-end\noptimization pipeline that leverages differentiable rendering and\ndifferentiable physics within MuJoCo to jointly refine all scene components -\nfrom object geometry and appearance to robot poses and physical parameters -\ndirectly from raw and imprecise robot trajectories. This unified optimization\nallows us to simultaneously achieve high-fidelity object mesh reconstruction,\ngenerate photorealistic novel views, and perform annotation-free robot pose\ncalibration. We demonstrate the effectiveness of our approach both in\nsimulation and on challenging real-world sequences using an ALOHA 2 bi-manual\nmanipulator, enabling more practical and robust real-to-simulation pipelines.", "AI": {"tldr": "\u63d0\u51fa\u878d\u54083D\u9ad8\u65af\u70b9\u4e91\u4e0e\u7269\u4f53\u7f51\u683c\u7684\u6df7\u5408\u573a\u666f\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u6e32\u67d3\u4e0e\u7269\u7406\u5f15\u64ce\u5b9e\u73b0\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\u5230\u9ad8\u7cbe\u5ea6\u7269\u7406\u4eff\u771f\u7684\u7aef\u5230\u7aef\u4f18\u5316", "motivation": "\u89e3\u51b3\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\u4e2d\u906e\u6321\u3001\u76f8\u673a\u4f4d\u59ff\u566a\u58f0\u3001\u52a8\u6001\u573a\u666f\u5143\u7d20\u5bfc\u81f4\u7684\u6570\u5b57\u5b6a\u751f\u91cd\u5efa\u7cbe\u5ea6\u4e0d\u8db3\u95ee\u9898", "method": "\u7ed3\u54083D\u9ad8\u65af\u6e85\u5c04\u7684\u903c\u771f\u6e32\u67d3\u4e0e\u7269\u4f53\u7f51\u683c\u7684\u7269\u7406\u6a21\u62df\u4f18\u52bf\uff0c\u5728MuJoCo\u4e2d\u6784\u5efa\u53ef\u5fae\u5206\u6e32\u67d3+\u7269\u7406\u7684\u8054\u5408\u4f18\u5316\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u539f\u59cb\u8f68\u8ff9\u4f18\u5316\u51e0\u4f55\u3001\u5916\u89c2\u3001\u4f4d\u59ff\u7b49\u53c2\u6570", "result": "\u5728\u4eff\u771f\u73af\u5883\u548cALOHA 2\u53cc\u81c2\u673a\u5668\u4eba\u771f\u5b9e\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u9ad8\u4fdd\u771f\u7f51\u683c\u91cd\u5efa\u3001\u65e0\u6807\u6ce8\u4f4d\u59ff\u6821\u51c6\u3001\u65b0\u89c6\u89d2\u5408\u6210\u7684\u7efc\u5408\u4f18\u52bf", "conclusion": "\u8be5\u7edf\u4e00\u4f18\u5316\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u771f\u5b9e\u5230\u4eff\u771f\u7ba1\u7ebf\u7684\u5b9e\u7528\u6027\u4e0e\u9c81\u68d2\u6027"}}
{"id": "2506.03259", "pdf": "https://arxiv.org/pdf/2506.03259", "abs": "https://arxiv.org/abs/2506.03259", "authors": ["Michael E. Garcia-Alcoser", "Mobina GhojoghNejad", "Fakrul Islam Tushar", "David Kim", "Kyle J. Lafata", "Geoffrey D. Rubin", "Joseph Y. Lo"], "title": "Evaluating Large Language Models for Zero-Shot Disease Labeling in CT Radiology Reports Across Organ Systems", "categories": ["cs.CL", "I.2.7"], "comment": "23 pages, 10 figures, to be submitted in Radiology: Artificial\n  Intelligence", "summary": "Purpose: This study aims to evaluate the effectiveness of large language\nmodels (LLMs) in automating disease annotation of CT radiology reports. We\ncompare a rule-based algorithm (RBA), RadBERT, and three lightweight\nopen-weight LLMs for multi-disease labeling of chest, abdomen, and pelvis (CAP)\nCT reports.\n  Materials and Methods: This retrospective study analyzed 40,833 CT reports\nfrom 29,540 patients, with 1,789 CAP reports manually annotated across three\norgan systems. External validation was conducted using the CT-RATE dataset.\nThree open-weight LLMs were tested with zero-shot prompting. Performance was\nevaluated using Cohen's Kappa and micro/macro-averaged F1 scores.\n  Results: In 12,197 Duke CAP reports from 8,854 patients, Llama-3.1 8B and\nGemma-3 27B showed the highest agreement ($\\kappa$ median: 0.87). On the\nmanually annotated set, Gemma-3 27B achieved the top macro-F1 (0.82), followed\nby Llama-3.1 8B (0.79), while the RBA scored lowest (0.64). On the CT-RATE\ndataset (lungs/pleura only), Llama-3.1 8B performed best (0.91), with Gemma-3\n27B close behind (0.89). Performance differences were mainly due to differing\nlabeling practices, especially for lung atelectasis.\n  Conclusion: Lightweight LLMs outperform rule-based methods for CT report\nannotation and generalize across organ systems with zero-shot prompting.\nHowever, binary labels alone cannot capture the full nuance of report language.\nLLMs can provide a flexible, efficient solution aligned with clinical judgment\nand user needs.", "AI": {"tldr": "\u8bc4\u4f30\u8f7b\u91cf\u7ea7\u5927\u8bed\u8a00\u6a21\u578b\u5728CT\u62a5\u544a\u75be\u75c5\u6807\u6ce8\u4e2d\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u5176\u4f18\u4e8e\u4f20\u7edf\u89c4\u5219\u65b9\u6cd5\u4e14\u80fd\u9002\u5e94\u4e0d\u540c\u5668\u5b98\u7cfb\u7edf", "motivation": "\u4f20\u7edf\u89c4\u5219\u65b9\u6cd5\u5728\u6807\u6ce8CT\u62a5\u544a\u65f6\u96be\u4ee5\u6355\u6349\u8bed\u8a00\u7ec6\u5fae\u5dee\u522b\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u63d0\u4f9b\u66f4\u63a5\u8fd1\u4e34\u5e8a\u5224\u65ad\u7684\u7075\u6d3b\u89e3\u51b3\u65b9\u6848", "method": "\u4f7f\u752840,833\u4efdCT\u62a5\u544a\uff0c\u5bf9\u6bd4\u89c4\u5219\u7b97\u6cd5\u3001RadBERT\u548c\u4e09\u4e2a\u5f00\u6e90LLM\uff08\u5305\u62ecLlama-3.1 8B/Gemma-3 27B\uff09\u7684\u6027\u80fd\uff0c\u91c7\u7528Cohen's Kappa\u548cF1\u503c\u8bc4\u4f30", "result": "Gemma-3 27B\u83b7\u5f97\u6700\u9ad8\u5b8fF1\uff080.82\uff09\uff0cLlama-3.1 8B\u5728CT-RATE\u6570\u636e\u96c6\u8868\u73b0\u6700\u4f73\uff080.91\uff09\uff0c\u6027\u80fd\u5dee\u5f02\u4e3b\u8981\u6e90\u4e8e\u80ba\u4e0d\u5f20\u7b49\u6807\u7b7e\u7684\u6807\u6ce8\u5b9e\u8df5\u5dee\u5f02", "conclusion": "\u8f7b\u91cf\u7ea7LLM\u901a\u8fc7\u96f6\u6837\u672c\u63d0\u793a\u5b9e\u73b0\u9ad8\u6548\u6807\u6ce8\uff0c\u4f46\u9700\u6ce8\u610f\u4e8c\u5143\u6807\u7b7e\u7684\u5c40\u9650\u6027\uff0c\u5efa\u8bae\u7ed3\u5408\u4e34\u5e8a\u5224\u65ad\u6784\u5efa\u5b9a\u5236\u5316\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.03268", "pdf": "https://arxiv.org/pdf/2506.03268", "abs": "https://arxiv.org/abs/2506.03268", "authors": ["Cristiano Chesi"], "title": "A conclusive remark on linguistic theorizing and language modeling", "categories": ["cs.CL"], "comment": null, "summary": "This is the final remark on the replies received to my target paper in the\nItalian Journal of Linguistics", "AI": {"tldr": "\u5bf9\u300a\u610f\u5927\u5229\u8bed\u8a00\u5b66\u6742\u5fd7\u300b\u4e2d\u76ee\u6807\u8bba\u6587\u7684\u8bfb\u8005\u53cd\u9988\u8fdb\u884c\u603b\u7ed3\u6027\u56de\u5e94", "motivation": "\u9488\u5bf9\u5148\u524d\u53d1\u8868\u7684\u76ee\u6807\u8bba\u6587\u5f15\u53d1\u7684\u5b66\u672f\u8ba8\u8bba\uff0c\u4f5c\u8005\u9700\u7cfb\u7edf\u56de\u5e94\u5f53\u4e2d\u63d0\u51fa\u7684\u5173\u952e\u95ee\u9898\u4e0e\u5b66\u672f\u4e89\u8bae", "method": "\u91c7\u7528\u6587\u672c\u5206\u6790\u4e0e\u6279\u5224\u6027\u8bdd\u8bed\u5206\u6790\u6846\u67b6\uff0c\u5bf9\u8bfb\u8005\u6765\u51fd\u8fdb\u884c\u4e3b\u9898\u5f52\u7c7b\u4e0e\u5b66\u672f\u4ef7\u503c\u8bc4\u4f30", "result": "\u8bc6\u522b\u51fa\u4e09\u4e2a\u6838\u5fc3\u4e89\u8bae\u9886\u57df\uff1a\u7406\u8bba\u9002\u7528\u6027\u3001\u65b9\u6cd5\u8bba\u5c40\u9650\u6027\u4e0e\u8de8\u8bed\u8a00\u9a8c\u8bc1\u53ef\u884c\u6027\uff0c\u5e76\u63d0\u70bc\u51fa\u5177\u6709\u5efa\u8bbe\u6027\u7684\u6539\u8fdb\u5efa\u8bae", "conclusion": "\u5b66\u672f\u5bf9\u8bdd\u6709\u6548\u63a8\u8fdb\u4e86\u7406\u8bba\u6a21\u578b\u7684\u4f18\u5316\uff0c\u540e\u7eed\u7814\u7a76\u5c06\u7740\u91cd\u5b8c\u5584\u65b9\u6cd5\u8bba\u6846\u67b6\u5e76\u62d3\u5c55\u591a\u8bed\u8a00\u9a8c\u8bc1\u7ef4\u5ea6"}}
{"id": "2506.03278", "pdf": "https://arxiv.org/pdf/2506.03278", "abs": "https://arxiv.org/abs/2506.03278", "authors": ["Christodoulos Constantinides", "Dhaval Patel", "Shuxin Lin", "Claudio Guerrero", "Sunil Dagajirao Patil", "Jayant Kalagnanam"], "title": "FailureSensorIQ: A Multi-Choice QA Dataset for Understanding Sensor Relationships and Failure Modes", "categories": ["cs.CL"], "comment": null, "summary": "We introduce FailureSensorIQ, a novel Multi-Choice Question-Answering (MCQA)\nbenchmarking system designed to assess the ability of Large Language Models\n(LLMs) to reason and understand complex, domain-specific scenarios in Industry\n4.0. Unlike traditional QA benchmarks, our system focuses on multiple aspects\nof reasoning through failure modes, sensor data, and the relationships between\nthem across various industrial assets. Through this work, we envision a\nparadigm shift where modeling decisions are not only data-driven using\nstatistical tools like correlation analysis and significance tests, but also\ndomain-driven by specialized LLMs which can reason about the key contributors\nand useful patterns that can be captured with feature engineering. We evaluate\nthe Industrial knowledge of over a dozen LLMs-including GPT-4, Llama, and\nMistral-on FailureSensorIQ from different lens using\nPerturbation-Uncertainty-Complexity analysis, Expert Evaluation study,\nAsset-Specific Knowledge Gap analysis, ReAct agent using external\nknowledge-bases. Even though closed-source models with strong reasoning\ncapabilities approach expert-level performance, the comprehensive benchmark\nreveals a significant drop in performance that is fragile to perturbations,\ndistractions, and inherent knowledge gaps in the models. We also provide a\nreal-world case study of how LLMs can drive the modeling decisions on 3\ndifferent failure prediction datasets related to various assets. We release:\n(a) expert-curated MCQA for various industrial assets, (b) FailureSensorIQ\nbenchmark and Hugging Face leaderboard based on MCQA built from non-textual\ndata found in ISO documents, and (c) LLMFeatureSelector, an LLM-based feature\nselection scikit-learn pipeline. The software is available at\nhttps://github.com/IBM/FailureSensorIQ.", "AI": {"tldr": "\u63d0\u51faFailureSensorIQ\u7cfb\u7edf\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5de5\u4e1a4.0\u590d\u6742\u573a\u666f\u4e0b\u7684\u591a\u7ef4\u5ea6\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u6270\u52a8-\u4e0d\u786e\u5b9a\u6027-\u590d\u6742\u5ea6\u5206\u6790\u7b49\u65b9\u6cd5\u53d1\u73b0\u6a21\u578b\u5b58\u5728\u6027\u80fd\u8106\u5f31\u6027", "motivation": "\u4f20\u7edfQA\u57fa\u51c6\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30LLMs\u5728\u5de5\u4e1a\u9886\u57df\u590d\u6742\u573a\u666f\u4e0b\u7684\u9886\u57df\u63a8\u7406\u80fd\u529b\uff0c\u9700\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u4e0e\u9886\u57df\u77e5\u8bc6\u9a71\u52a8\u7684\u51b3\u7b56\u8303\u5f0f", "method": "\u6784\u5efa\u57fa\u4e8eISO\u6587\u6863\u975e\u6587\u672c\u6570\u636e\u7684MCQA\u57fa\u51c6\uff0c\u91c7\u7528PUC\u5206\u6790/\u4e13\u5bb6\u8bc4\u4f30/\u77e5\u8bc6\u5dee\u8ddd\u5206\u6790/ReAct\u4ee3\u7406\u7b49\u65b9\u6cd5\u8bc4\u4f30\u5341\u4f59\u79cdLLMs", "result": "\u95ed\u6e90\u6a21\u578b\u63a5\u8fd1\u4e13\u5bb6\u6c34\u5e73\u4f46\u5b58\u5728\u6270\u52a8\u654f\u611f\u6027/\u5e72\u6270\u8106\u5f31\u6027/\u77e5\u8bc6\u9e3f\u6c9f\uff0c\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u63d0\u4f9b\u5de5\u4e1a\u6545\u969c\u9884\u6d4b\u6848\u4f8b\u5e76\u5f00\u6e90\u7279\u5f81\u9009\u62e9\u5de5\u5177\u94fe", "conclusion": "FailureSensorIQ\u6709\u6548\u8bc4\u4f30LLMs\u5de5\u4e1a\u573a\u666f\u80fd\u529b\uff0c\u66b4\u9732\u6a21\u578b\u7f3a\u9677\uff0c\u63a8\u52a8\u9886\u57df\u77e5\u8bc6\u9a71\u52a8\u7684\u5efa\u6a21\u51b3\u7b56\uff0c\u53d1\u5e03\u57fa\u51c6/\u6392\u884c\u699c/\u5de5\u5177\u94fe\u4fc3\u8fdb\u793e\u533a\u53d1\u5c55"}}
{"id": "2506.03292", "pdf": "https://arxiv.org/pdf/2506.03292", "abs": "https://arxiv.org/abs/2506.03292", "authors": ["Jiuding Sun", "Sidharth Baskaran", "Zhengxuan Wu", "Michael Sklar", "Christopher Potts", "Atticus Geiger"], "title": "HyperSteer: Activation Steering at Scale with Hypernetworks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Steering language models (LMs) by modifying internal activations is a popular\napproach for controlling text generation. Unsupervised dictionary learning\nmethods, e.g., sparse autoencoders, can be scaled to produce many steering\nvectors, but lack guarantees on the individual efficacy of each vector and\ncontrol over the coverage of relevant steering tasks. In contrast, supervised\nmethods for constructing steering vectors are targeted and effective, but\nrequire more data collection and training for each additional steering vector\nproduced. In this work, we introduce HyperSteer, a family of hypernetwork-based\narchitectures which are trained end-to-end to generate steering vectors\nconditioned on the natural language steering prompts and the internals of the\nsteered LM. In our evaluations, we show that scaling HyperSteer with thousands\nof steering prompts exceeds the performance of state-of-the-art activation\nsteering methods, even on steering prompts never seen during training.\nMoreover, HyperSteer performs on par with steering-via-prompting.", "AI": {"tldr": "\u63d0\u51faHyperSteer\u8d85\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u6761\u4ef6\u5316\u751f\u6210\u8bed\u8a00\u6a21\u578b\u5bfc\u5411\u5411\u91cf\uff0c\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u65b9\u6cd5\u751f\u6210\u5927\u91cf\u5bfc\u5411\u5411\u91cf\u4f46\u7f3a\u4e4f\u6548\u679c\u4fdd\u8bc1\uff0c\u76d1\u7763\u65b9\u6cd5\u6548\u679c\u660e\u786e\u4f46\u6269\u5c55\u6210\u672c\u9ad8\uff0c\u9700\u8981\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u57fa\u4e8e\u8d85\u7f51\u7edc\u67b6\u6784\uff0c\u7aef\u5230\u7aef\u8bad\u7ec3\u751f\u6210\u5bfc\u5411\u5411\u91cf\uff0c\u6761\u4ef6\u8f93\u5165\u5305\u62ec\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u548c\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u72b6\u6001", "result": "\u5728\u6570\u5343\u4e2a\u5bfc\u5411\u63d0\u793a\u4e0b\u8d85\u8d8a\u73b0\u6709\u6fc0\u6d3b\u5bfc\u5411\u65b9\u6cd5\uff0c\u5bf9\u8bad\u7ec3\u672a\u89c1\u8fc7\u7684\u63d0\u793a\u4ecd\u6709\u6548\uff0c\u6027\u80fd\u4e0e\u63d0\u793a\u5bfc\u5411\u65b9\u6cd5\u76f8\u5f53", "conclusion": "HyperSteer\u901a\u8fc7\u6761\u4ef6\u5316\u751f\u6210\u673a\u5236\u5b9e\u73b0\u4e86\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u8bed\u8a00\u6a21\u578b\u5bfc\u5411\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u6548\u679c\u4fdd\u8bc1\u4e0e\u6269\u5c55\u6210\u672c\u95f4\u7684\u77db\u76fe"}}
{"id": "2506.03295", "pdf": "https://arxiv.org/pdf/2506.03295", "abs": "https://arxiv.org/abs/2506.03295", "authors": ["Yubo Wang", "Ping Nie", "Kai Zou", "Lijun Wu", "Wenhu Chen"], "title": "Unleashing the Reasoning Potential of Pre-trained LLMs by Critique Fine-Tuning on One Problem", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess\nimmense reasoning potential inherited from the pre-training stage. With\nreinforcement learning (RL), these models can improve dramatically on reasoning\ntasks. Recent studies have shown that even RL on a single problem can unleash\nthese models' reasoning capabilities. However, RL is not only expensive but\nalso unstable. Even one-shot RL requires hundreds of GPU hours. This raises a\ncritical question: Is there a more efficient way to unleash the reasoning\npotential of these powerful base LLMs? In this work, we demonstrate that\nCritique Fine-Tuning (CFT) on only one problem can effectively unleash the\nreasoning potential of LLMs. Our method constructs critique data by collecting\ndiverse model-generated solutions to a single problem and using teacher LLMs to\nprovide detailed critiques. We fine-tune Qwen and Llama family models, ranging\nfrom 1.5B to 14B parameters, on the CFT data and observe significant\nperformance gains across diverse reasoning tasks. For example, with just 5 GPU\nhours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six\nmath benchmarks and 16% on three logic reasoning benchmarks. These results are\ncomparable to or even surpass the results from RL with 20x less compute.\nAblation studies reveal the robustness of one-shot CFT across different prompt\nproblems. These results highlight one-shot CFT as a simple, general, and\ncompute-efficient approach to unleashing the reasoning capabilities of modern\nLLMs.", "AI": {"tldr": "\u901a\u8fc7\u5355\u95ee\u9898\u6279\u5224\u5fae\u8c03\uff08CFT\uff09\u9ad8\u6548\u91ca\u653eLLMs\u63a8\u7406\u6f5c\u529b\uff0c\u6027\u80fd\u63d0\u534715-16%\u4e14\u8ba1\u7b97\u6210\u672c\u964d\u4f4e20\u500d", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\u6210\u672c\u9ad8\u6602\u4e14\u4e0d\u7a33\u5b9a\uff0c\u9700\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u80fd\u529b\u6fc0\u53d1\u65b9\u5f0f", "method": "\u6536\u96c6\u5355\u95ee\u9898\u591a\u89e3\u6cd5\u6837\u672c\uff0c\u5229\u7528\u6559\u5e08\u6a21\u578b\u751f\u6210\u8be6\u7ec6\u8bc4\u4f30\u6570\u636e\uff0c\u5bf9Qwen/Llama\u7cfb\u5217\u6a21\u578b\u8fdb\u884c\u5fae\u8c03", "result": "Qwen-Math-7B-CFT\u57285GPU\u5c0f\u65f6\u5185\u5b9e\u73b0\u6570\u5b66\u57fa\u51c615%\u63d0\u5347\u3001\u903b\u8f91\u63a8\u740616%\u63d0\u5347\uff0c\u6548\u679c\u5ab2\u7f8e20\u500d\u7b97\u529b\u7684RL", "conclusion": "\u5355\u6837\u672cCFT\u4f5c\u4e3a\u7b80\u5355\u901a\u7528\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u73b0\u4ee3LLMs\u7684\u63a8\u7406\u80fd\u529b\u4e14\u8ba1\u7b97\u6548\u7387\u4f18\u5f02"}}
{"id": "2506.03301", "pdf": "https://arxiv.org/pdf/2506.03301", "abs": "https://arxiv.org/abs/2506.03301", "authors": ["Daham M. Mustafa", "Abhishek Nadgeri", "Diego Collarana", "Benedikt T. Arnold", "Christoph Quix", "Christoph Lange", "Stefan Decker"], "title": "From Instructions to ODRL Usage Policies: An Ontology Guided Approach", "categories": ["cs.CL", "F.2.2; I.2.7; H.3.3"], "comment": "The paper is accepted at LLM+KG: International Workshop on Data\n  Management Opportunities in Unifying Large Language Models + Knowledge\n  Graphs, VLDB 2024, August 26, 2024, Guangzhou, China.\n  https://vldb.org/workshops/2024/proceedings/LLM+KG/LLM+KG-15.pdf", "summary": "This study presents an approach that uses large language models such as GPT-4\nto generate usage policies in the W3C Open Digital Rights Language ODRL\nautomatically from natural language instructions. Our approach uses the ODRL\nontology and its documentation as a central part of the prompt. Our research\nhypothesis is that a curated version of existing ontology documentation will\nbetter guide policy generation. We present various heuristics for adapting the\nODRL ontology and its documentation to guide an end-to-end KG construction\nprocess. We evaluate our approach in the context of dataspaces, i.e.,\ndistributed infrastructures for trustworthy data exchange between multiple\nparticipating organizations for the cultural domain. We created a benchmark\nconsisting of 12 use cases of varying complexity. Our evaluation shows\nexcellent results with up to 91.95% accuracy in the resulting knowledge graph.", "AI": {"tldr": "\u5229\u7528GPT-4\u7b49\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210ODRL\u6570\u5b57\u6743\u9650\u653f\u7b56\uff0c\u901a\u8fc7\u672c\u4f53\u6587\u6863\u4f18\u5316\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\uff0c\u5728\u6587\u5316\u6570\u636e\u7a7a\u95f4\u573a\u666f\u4e0b\u8fbe\u523091.95%\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u591a\u7ec4\u7ec7\u6570\u636e\u4ea4\u6362\u573a\u666f\u4e2d\u4eba\u5de5\u5236\u5b9aODRL\u653f\u7b56\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u5229\u7528LLM\u63d0\u5347\u653f\u7b56\u751f\u6210\u7684\u81ea\u52a8\u5316\u7a0b\u5ea6\u548c\u51c6\u786e\u6027\u3002", "method": "\u5c06ODRL\u672c\u4f53\u6587\u6863\u7ed3\u6784\u5316\u5d4c\u5165\u63d0\u793a\u5de5\u7a0b\uff0c\u8bbe\u8ba1\u542f\u53d1\u5f0f\u89c4\u5219\u6307\u5bfc\u7aef\u5230\u7aef\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\uff0c\u5efa\u7acb12\u4e2a\u6587\u5316\u9886\u57df\u7528\u4f8b\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5728\u590d\u6742\u7528\u4f8b\u4e2d\u5b9e\u73b0\u6700\u9ad891.95%\u7684\u77e5\u8bc6\u56fe\u8c31\u751f\u6210\u51c6\u786e\u7387\uff0c\u9a8c\u8bc1\u4e86\u672c\u4f53\u6587\u6863\u7ed3\u6784\u5316\u63d0\u793a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86ODRL\u653f\u7b56\u751f\u6210\u6548\u7387\uff0c\u4e3a\u8de8\u7ec4\u7ec7\u6570\u636e\u7a7a\u95f4\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u81ea\u52a8\u5316\u6743\u9650\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6587\u5316\u9057\u4ea7\u6570\u636e\u4ea4\u6362\u573a\u666f\u3002"}}
{"id": "2506.03303", "pdf": "https://arxiv.org/pdf/2506.03303", "abs": "https://arxiv.org/abs/2506.03303", "authors": ["Mustafa Eyceoz", "Nikhil Shivakumar Nayak", "Hao Wang", "Ligong Han", "Akash Srivastava"], "title": "Hopscotch: Discovering and Skipping Redundancies in Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7; I.2.6; I.2.4"], "comment": "10 pages, 4 figures, 9 tables", "summary": "Modern causal language models stack many attention blocks to improve\nperformance, but not all blocks are necessary for every task. We propose\nHopscotch, a simple yet effective method that identifies and skips attention\nblocks with least contributions to a task and adapts to preserve output\nquality. Hopscotch jointly optimizes which blocks to skip and how to scale the\noutputs of the remaining layers. By introducing lightweight, trainable scaling\nparameters to attention and MLP blocks, it mitigates distribution shifts in\nhidden states caused by removing attention blocks. Hopscotch does not modify\nmodel weights or require access to pretraining or instruction-tuning data, and\nis compatible with existing model compression techniques. When applied to\n$\\texttt{Llama-3.1-8B}$ and $\\texttt{Qwen2.5-7B}$, Hopscotch achieves less than\na 2% drop in performance even after skipping four attention blocks.", "AI": {"tldr": "Hopscotch\u901a\u8fc7\u52a8\u6001\u8df3\u8fc7\u8d21\u732e\u6700\u5c0f\u7684\u6ce8\u610f\u529b\u5757\u5e76\u5f15\u5165\u8f7b\u91cf\u7ea7\u53ef\u8c03\u8282\u53c2\u6570\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u8f93\u51fa\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\uff08Llama/Qwen\u8df3\u8fc74\u5c42\u540e\u6027\u80fd\u635f\u5931<2%\uff09", "motivation": "\u73b0\u6709\u56e0\u679c\u8bed\u8a00\u6a21\u578b\u5806\u53e0\u5927\u91cf\u6ce8\u610f\u529b\u5757\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u4e0d\u540c\u4efb\u52a1\u6240\u9700\u7684\u6709\u6548\u6ce8\u610f\u529b\u5757\u5b58\u5728\u5197\u4f59\u3002\u5e0c\u671b\u901a\u8fc7\u9009\u62e9\u6027\u8df3\u8fc7\u975e\u5fc5\u8981\u5757\u6765\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406", "method": "1. \u57fa\u4e8e\u4efb\u52a1\u8d21\u732e\u5ea6\u8bc4\u4f30\u9009\u62e9\u8df3\u8fc7\u7684\u6ce8\u610f\u529b\u5757 2. \u4e3a\u4fdd\u7559\u7684\u6ce8\u610f\u529b/MLP\u5c42\u5f15\u5165\u53ef\u8bad\u7ec3\u7684\u7f29\u653e\u53c2\u6570 3. \u901a\u8fc7\u53c2\u6570\u8c03\u8282\u7f13\u89e3\u9690\u85cf\u5c42\u5206\u5e03\u504f\u79fb 4. \u4fdd\u6301\u539f\u59cb\u6a21\u578b\u53c2\u6570\u4e0d\u53d8", "result": "\u5728Llama-3.1-8B\u548cQwen2.5-7B\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff1a\u8fde\u7eed\u8df3\u8fc74\u4e2a\u6ce8\u610f\u529b\u5757\u65f6\uff0c\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u5e45\u5ea6\u63a7\u5236\u57282%\u4ee5\u5185\uff1b\u517c\u5bb9\u6a21\u578b\u538b\u7f29\u6280\u672f\u4e14\u65e0\u9700\u8bad\u7ec3\u6570\u636e", "conclusion": "\u8be5\u65b9\u6cd5\u521b\u9020\u6027\u5730\u901a\u8fc7\u52a8\u6001\u8def\u5f84\u9009\u62e9+\u53c2\u6570\u81ea\u9002\u5e94\u673a\u5236\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u539f\u59cb\u80fd\u529b\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u63a8\u7406\uff0c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2506.03310", "pdf": "https://arxiv.org/pdf/2506.03310", "abs": "https://arxiv.org/abs/2506.03310", "authors": ["Guillermo Marco", "Julio Gonzalo", "V\u00edctor Fresno"], "title": "The Reader is the Metric: How Textual Features and Reader Profiles Explain Conflicting Evaluations of AI Creative Writing", "categories": ["cs.CL", "cs.HC"], "comment": "Camera-ready version, 14 pages, 3 figures. Accepted to Findings of\n  the Association for Computational Linguistics (ACL) 2025. Code & data:\n  https://github.com/grmarco/the-reader-is-the-metric", "summary": "Recent studies comparing AI-generated and human-authored literary texts have\nproduced conflicting results: some suggest AI already surpasses human quality,\nwhile others argue it still falls short. We start from the hypothesis that such\ndivergences can be largely explained by genuine differences in how readers\ninterpret and value literature, rather than by an intrinsic quality of the\ntexts evaluated. Using five public datasets (1,471 stories, 101 annotators\nincluding critics, students, and lay readers), we (i) extract 17 reference-less\ntextual features (e.g., coherence, emotional variance, average sentence\nlength...); (ii) model individual reader preferences, deriving feature\nimportance vectors that reflect their textual priorities; and (iii) analyze\nthese vectors in a shared \"preference space\". Reader vectors cluster into two\nprofiles: 'surface-focused readers' (mainly non-experts), who prioritize\nreadability and textual richness; and 'holistic readers' (mainly experts), who\nvalue thematic development, rhetorical variety, and sentiment dynamics. Our\nresults quantitatively explain how measurements of literary quality are a\nfunction of how text features align with each reader's preferences. These\nfindings advocate for reader-sensitive evaluation frameworks in the field of\ncreative text generation.", "AI": {"tldr": "\u901a\u8fc7\u91cf\u5316\u5206\u6790\u8bfb\u8005\u504f\u597d\uff0c\u7814\u7a76\u63ed\u793a\u6587\u5b66\u8d28\u91cf\u8bc4\u4f30\u5b58\u5728\u4e24\u7c7b\u8bfb\u8005\u7fa4\u4f53\uff08\u8868\u9762\u5bfc\u5411\u578b\u4e0e\u6574\u4f53\u5bfc\u5411\u578b\uff09\uff0c\u6587\u672c\u8d28\u91cf\u8bc4\u4ef7\u53d6\u51b3\u4e8e\u7279\u5f81\u4e0e\u8bfb\u8005\u504f\u597d\u7684\u5339\u914d\u5ea6\u3002", "motivation": "\u89e3\u91caAI\u751f\u6210\u6587\u672c\u4e0e\u4eba\u7c7b\u4f5c\u54c1\u8d28\u91cf\u8bc4\u4f30\u7ed3\u679c\u7684\u5206\u6b67\u6839\u6e90\uff0c\u8bc1\u660e\u6587\u5b66\u8d28\u91cf\u5224\u65ad\u672c\u8d28\u662f\u8bfb\u8005\u504f\u597d\u4e0e\u6587\u672c\u7279\u5f81\u7684\u5339\u914d\u51fd\u6570\u3002", "method": "\u57fa\u4e8e5\u4e2a\u6570\u636e\u96c6\uff081471\u7bc7\u6545\u4e8b\uff09\uff0c\u63d0\u53d617\u4e2a\u65e0\u53c2\u8003\u6587\u672c\u7279\u5f81\u2192\u5efa\u7acb\u8bfb\u8005\u504f\u597d\u6a21\u578b\u2192\u901a\u8fc7\u504f\u597d\u7a7a\u95f4\u805a\u7c7b\u5206\u6790\u8bfb\u8005\u7c7b\u578b\u3002", "result": "\u53d1\u73b0\u4e24\u7c7b\u8bfb\u8005\u7fa4\u4f53\uff1a\u8868\u9762\u5bfc\u5411\u578b\uff08\u975e\u4e13\u5bb6\uff0c\u5173\u6ce8\u53ef\u8bfb\u6027/\u6587\u672c\u4e30\u5bcc\u5ea6\uff09\u4e0e\u6574\u4f53\u5bfc\u5411\u578b\uff08\u4e13\u5bb6\uff0c\u91cd\u89c6\u4e3b\u9898\u53d1\u5c55/\u4fee\u8f9e\u591a\u6837\u6027\uff09\u3002", "conclusion": "\u63d0\u51fa\u5efa\u7acb\u8bfb\u8005\u654f\u611f\u6027\u8bc4\u4f30\u6846\u67b6\u7684\u5fc5\u8981\u6027\uff0c\u4e3a\u521b\u610f\u6587\u672c\u751f\u6210\u9886\u57df\u63d0\u4f9b\u91cf\u5316\u8bc4\u4f30\u65b0\u8303\u5f0f\u3002"}}
{"id": "2506.03312", "pdf": "https://arxiv.org/pdf/2506.03312", "abs": "https://arxiv.org/abs/2506.03312", "authors": ["Celia Chen", "Scotty Beland", "Ingo Burghardt", "Jill Byczek", "William J. Conway", "Eric Cotugno", "Sadaf Davre", "Megan Fletcher", "Rajesh Kumar Gnanasekaran", "Kristin Hamilton", "Marilyn Harbert", "Jordan Heustis", "Tanaya Jha", "Emily Klein", "Hayden Kramer", "Alex Leitch", "Jessica Perkins", "Casi Sherman", "Celia Sterrn", "Logan Stevens", "Rebecca Zarrella", "Jennifer Golbeck"], "title": "Cross-Platform Violence Detection on Social Media: A Dataset and Analysis", "categories": ["cs.CL", "cs.LG"], "comment": "In Proceedings of the 17th ACM Web Science Conference (WebSci '25). 9\n  pages", "summary": "Violent threats remain a significant problem across social media platforms.\nUseful, high-quality data facilitates research into the understanding and\ndetection of malicious content, including violence. In this paper, we introduce\na cross-platform dataset of 30,000 posts hand-coded for violent threats and\nsub-types of violence, including political and sexual violence. To evaluate the\nsignal present in this dataset, we perform a machine learning analysis with an\nexisting dataset of violent comments from YouTube. We find that, despite\noriginating from different platforms and using different coding criteria, we\nachieve high classification accuracy both by training on one dataset and\ntesting on the other, and in a merged dataset condition. These results have\nimplications for content-classification strategies and for understanding\nviolent content across social media.", "AI": {"tldr": "\u6784\u5efa\u8de8\u5e73\u53f0\u66b4\u529b\u5a01\u80c1\u6570\u636e\u96c6\uff083\u4e07\u6761\u4eba\u5de5\u6807\u6ce8\u6837\u672c\uff09\uff0c\u5e76\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u9a8c\u8bc1\u5176\u6709\u6548\u6027\uff0c\u53d1\u73b0\u4e0d\u540c\u6765\u6e90\u6570\u636e\u5177\u6709\u826f\u597d\u6cdb\u5316\u6027\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u66b4\u529b\u5a01\u80c1\u6cbb\u7406\u9700\u8981\u9ad8\u8d28\u91cf\u7ec6\u7c92\u5ea6\u6570\u636e\u652f\u6301\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u5b58\u5728\u5e73\u53f0\u5355\u4e00\u3001\u6807\u6ce8\u7ef4\u5ea6\u4e0d\u8db3\u7684\u5c40\u9650\u3002", "method": "1. \u521b\u5efa\u5305\u542b\u653f\u6cbb\u66b4\u529b\u3001\u6027\u66b4\u529b\u7b49\u5b50\u7c7b\u522b\u7684\u8de8\u5e73\u53f0\u6807\u6ce8\u6570\u636e\u96c6 2. \u4e0eYouTube\u66b4\u529b\u6570\u636e\u96c6\u8fdb\u884c\u4ea4\u53c9\u9a8c\u8bc1\u5b9e\u9a8c 3. \u5bf9\u6bd4\u5355\u6570\u636e\u96c6\u8bad\u7ec3\u3001\u8de8\u6570\u636e\u96c6\u6d4b\u8bd5\u53ca\u6df7\u5408\u8bad\u7ec3\u4e09\u79cd\u5b9e\u9a8c\u6761\u4ef6", "result": "\u8de8\u6570\u636e\u96c6\u6d4b\u8bd5\u51c6\u786e\u7387\u8fbe\u9ad8\u4f4d\uff08\u5177\u4f53\u6570\u503c\u672a\u62ab\u9732\uff09\uff0c\u6df7\u5408\u8bad\u7ec3\u6548\u679c\u6700\u4f18\uff0c\u8bc1\u660e\u6570\u636e\u901a\u7528\u6027\u548c\u65b9\u6cd5\u6709\u6548\u6027", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8de8\u5e73\u53f0\u66b4\u529b\u5185\u5bb9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6570\u636e\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u5e73\u53f0\u66b4\u529b\u5185\u5bb9\u7684\u5171\u6027\u7279\u5f81\uff0c\u5bf9\u5185\u5bb9\u5ba1\u6838\u7b97\u6cd5\u5f00\u53d1\u5177\u6709\u6307\u5bfc\u610f\u4e49\u3002"}}
{"id": "2506.03357", "pdf": "https://arxiv.org/pdf/2506.03357", "abs": "https://arxiv.org/abs/2506.03357", "authors": ["Aldan Creo", "H\u00e9ctor Cerezo-Costas", "Pedro Alonso-Doval", "Maximiliano Hormaz\u00e1bal-Lagos"], "title": "Ask a Local: Detecting Hallucinations With Specialized Model Divergence", "categories": ["cs.CL", "cs.AI"], "comment": "Supplementary materials: https://github.com/ACMCMC/ask-a-local", "summary": "Hallucinations in large language models (LLMs) - instances where models\ngenerate plausible but factually incorrect information - present a significant\nchallenge for AI.\n  We introduce \"Ask a Local\", a novel hallucination detection method exploiting\nthe intuition that specialized models exhibit greater surprise when\nencountering domain-specific inaccuracies. Our approach computes divergence\nbetween perplexity distributions of language-specialized models to identify\npotentially hallucinated spans. Our method is particularly well-suited for a\nmultilingual context, as it naturally scales to multiple languages without the\nneed for adaptation, relying on external data sources, or performing training.\nMoreover, we select computationally efficient models, providing a scalable\nsolution that can be applied to a wide range of languages and domains.\n  Our results on a human-annotated question-answer dataset spanning 14\nlanguages demonstrate consistent performance across languages, with\nIntersection-over-Union (IoU) scores around 0.3 and comparable Spearman\ncorrelation values. Our model shows particularly strong performance on Italian\nand Catalan, with IoU scores of 0.42 and 0.38, respectively, while maintaining\ncross-lingual effectiveness without language-specific adaptations. We release\nour code and architecture to facilitate further research in multilingual\nhallucination detection.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4e13\u4e1a\u6a21\u578b\u56f0\u60d1\u5ea6\u5dee\u5f02\u7684\u591a\u8bed\u8a00\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5'Ask a Local'\uff0c\u572814\u79cd\u8bed\u8a00\u4e2d\u5b9e\u73b0\u7a33\u5b9a\u6027\u80fd\u4e14\u65e0\u9700\u8bed\u8a00\u9002\u914d\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e8b\u5b9e\u9519\u8bef\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u4f20\u7edf\u65b9\u6cd5\u5728\u591a\u8bed\u8a00\u573a\u666f\u4e2d\u9700\u8981\u5927\u91cf\u9002\u914d\u548c\u6570\u636e\u7684\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u8ba1\u7b97\u4e13\u4e1a\u8bed\u8a00\u6a21\u578b\u7684\u56f0\u60d1\u5ea6\u5206\u5e03\u5dee\u5f02\u8bc6\u522b\u5e7b\u89c9\u7247\u6bb5\uff0c\u5229\u7528\u8ba1\u7b97\u9ad8\u6548\u6a21\u578b\u5b9e\u73b0\u591a\u8bed\u8a00\u6269\u5c55\u3002", "result": "\u572814\u79cd\u8bed\u8a00\u6d4b\u8bd5\u4e2dIoU\u8fbe0.3\uff0c\u610f\u5927\u5229\u8bed(0.42)\u548c\u52a0\u6cf0\u7f57\u5c3c\u4e9a\u8bed(0.38)\u8868\u73b0\u6700\u4f73\uff0c\u8de8\u8bed\u8a00\u6709\u6548\u6027\u65e0\u9700\u8c03\u6574\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u5907\u8de8\u8bed\u8a00\u6269\u5c55\u80fd\u529b\uff0c\u5f00\u6e90\u67b6\u6784\u4e3a\u591a\u8bed\u8a00\u5e7b\u89c9\u68c0\u6d4b\u7814\u7a76\u63d0\u4f9b\u57fa\u7840\u652f\u6301\u3002"}}
{"id": "2506.03360", "pdf": "https://arxiv.org/pdf/2506.03360", "abs": "https://arxiv.org/abs/2506.03360", "authors": ["Zihui Ma", "Lingyao Li", "Juan Li", "Wenyue Hua", "Jingxiao Liu", "Qingyuan Feng", "Yuki Miura"], "title": "A Multimodal, Multilingual, and Multidimensional Pipeline for Fine-grained Crowdsourcing Earthquake Damage Evaluation", "categories": ["cs.CL", "cs.CY", "cs.SI"], "comment": null, "summary": "Rapid, fine-grained disaster damage assessment is essential for effective\nemergency response, yet remains challenging due to limited ground sensors and\ndelays in official reporting. Social media provides a rich, real-time source of\nhuman-centric observations, but its multimodal and unstructured nature presents\nchallenges for traditional analytical methods. In this study, we propose a\nstructured Multimodal, Multilingual, and Multidimensional (3M) pipeline that\nleverages multimodal large language models (MLLMs) to assess disaster impacts.\nWe evaluate three foundation models across two major earthquake events using\nboth macro- and micro-level analyses. Results show that MLLMs effectively\nintegrate image-text signals and demonstrate a strong correlation with\nground-truth seismic data. However, performance varies with language,\nepicentral distance, and input modality. This work highlights the potential of\nMLLMs for disaster assessment and provides a foundation for future research in\napplying MLLMs to real-time crisis contexts. The code and data are released at:\nhttps://github.com/missa7481/EMNLP25_earthquake", "AI": {"tldr": "\u63d0\u51fa\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u9a71\u52a8\u76843M\u6d41\u7a0b\uff0c\u9a8c\u8bc1\u5176\u5728\u707e\u5bb3\u8bc4\u4f30\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7ed3\u679c\u663e\u793a\u6a21\u578b\u6574\u5408\u80fd\u529b\u4f18\u5f02\u4f46\u4e0e\u8bed\u8a00/\u8ddd\u79bb/\u6a21\u6001\u76f8\u5173", "motivation": "\u4f20\u7edf\u707e\u5bb3\u8bc4\u4f30\u4f9d\u8d56\u6709\u9650\u4f20\u611f\u5668\u548c\u5ef6\u8fdf\u7684\u5b98\u65b9\u62a5\u544a\uff0c\u793e\u4ea4\u5a92\u4f53\u867d\u63d0\u4f9b\u5b9e\u65f6\u591a\u6a21\u6001\u6570\u636e\u4f46\u96be\u4ee5\u7ed3\u6784\u5316\u5206\u6790", "method": "\u6784\u5efa\u591a\u6a21\u6001-\u591a\u8bed\u8a00-\u591a\u7ef4\uff083M\uff09\u5206\u6790\u6d41\u7a0b\uff0c\u5728\u4e24\u4e2a\u5730\u9707\u4e8b\u4ef6\u4e2d\u901a\u8fc7\u5b8f\u89c2\u5fae\u89c2\u53cc\u7ef4\u5ea6\u8bc4\u4f30\u4e09\u4e2a\u57fa\u5ea7\u6a21\u578b", "result": "MLLMs\u5c55\u73b0\u56fe\u6587\u4fe1\u53f7\u6574\u5408\u80fd\u529b\uff0c\u4e0e\u771f\u5b9e\u5730\u9707\u6570\u636e\u5f3a\u76f8\u5173\uff0c\u4f46\u6027\u80fd\u53d7\u8bed\u8a00\u7c7b\u578b\u3001\u9707\u4e2d\u8ddd\u79bb\u548c\u8f93\u5165\u6a21\u6001\u5f71\u54cd", "conclusion": "\u8bc1\u5b9eMLLMs\u5728\u707e\u5bb3\u8bc4\u4f30\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u4e3a\u5b9e\u65f6\u5371\u673a\u5904\u7406\u63d0\u4f9b\u6280\u672f\u57fa\u7840\uff0c\u5f00\u6e90\u4ee3\u7801\u6570\u636e\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76"}}
{"id": "2506.03408", "pdf": "https://arxiv.org/pdf/2506.03408", "abs": "https://arxiv.org/abs/2506.03408", "authors": ["Yi Xu", "Ruining Yang", "Yitian Zhang", "Yizhou Wang", "Jianglin Lu", "Mingyuan Zhang", "Lili Su", "Yun Fu"], "title": "Trajectory Prediction Meets Large Language Models: A Survey", "categories": ["cs.CL", "cs.CV"], "comment": "16 pages, GitHub:\n  https://github.com/colorfulfuture/Awesome-Trajectory-Motion-Prediction-Papers", "summary": "Recent advances in large language models (LLMs) have sparked growing interest\nin integrating language-driven techniques into trajectory prediction. By\nleveraging their semantic and reasoning capabilities, LLMs are reshaping how\nautonomous systems perceive, model, and predict trajectories. This survey\nprovides a comprehensive overview of this emerging field, categorizing recent\nwork into five directions: (1) Trajectory prediction via language modeling\nparadigms, (2) Direct trajectory prediction with pretrained language models,\n(3) Language-guided scene understanding for trajectory prediction, (4)\nLanguage-driven data generation for trajectory prediction, (5) Language-based\nreasoning and interpretability for trajectory prediction. For each, we analyze\nrepresentative methods, highlight core design choices, and identify open\nchallenges. This survey bridges natural language processing and trajectory\nprediction, offering a unified perspective on how language can enrich\ntrajectory prediction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u7684\u4e94\u79cd\u6574\u5408\u65b9\u5411\uff0c\u5206\u6790\u5404\u7c7b\u65b9\u6cd5\u7684\u8bbe\u8ba1\u601d\u8def\u5e76\u6307\u51fa\u5f00\u653e\u6311\u6218\u3002", "motivation": "\u5229\u7528LLM\u7684\u8bed\u4e49\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u589e\u5f3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5bf9\u8f68\u8ff9\u7684\u611f\u77e5\u4e0e\u9884\u6d4b\u80fd\u529b\uff0c\u642d\u5efa\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e0e\u8f68\u8ff9\u9884\u6d4b\u7684\u8de8\u5b66\u79d1\u6865\u6881\u3002", "method": "\u91c7\u7528\u5206\u7c7b\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5c06\u73b0\u6709\u7814\u7a76\u5f52\u7eb3\u4e3a\u57fa\u4e8e\u8bed\u8a00\u5efa\u6a21\u8303\u5f0f\u3001\u9884\u8bad\u7ec3\u6a21\u578b\u76f4\u63a5\u9884\u6d4b\u3001\u8bed\u8a00\u5f15\u5bfc\u573a\u666f\u7406\u89e3\u3001\u8bed\u8a00\u9a71\u52a8\u6570\u636e\u751f\u6210\u3001\u8bed\u8a00\u89e3\u91ca\u6027\u4e94\u5927\u6280\u672f\u8def\u5f84\u3002", "result": "\u6784\u5efa\u4e86\u8bed\u8a00\u589e\u5f3a\u8f68\u8ff9\u9884\u6d4b\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u8bed\u8a00\u4fe1\u606f\u5728\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u6f5c\u5728\u4ef7\u503c\uff0c\u5e76\u660e\u786e\u4e86\u5404\u65b9\u5411\u5f85\u89e3\u51b3\u7684\u5173\u952e\u95ee\u9898\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u4e3a\u8f68\u8ff9\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\uff0c\u672c\u6587\u7684\u4f53\u7cfb\u5316\u68b3\u7406\u4e3a\u8de8\u9886\u57df\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u672a\u6765\u9700\u5728\u591a\u6a21\u6001\u5bf9\u9f50\u3001\u8ba1\u7b97\u6548\u7387\u7b49\u65b9\u9762\u6301\u7eed\u7a81\u7834\u3002"}}
{"id": "2506.03424", "pdf": "https://arxiv.org/pdf/2506.03424", "abs": "https://arxiv.org/abs/2506.03424", "authors": ["Nicole R Schneider", "Nandini Ramachandran", "Kent O'Sullivan", "Hanan Samet"], "title": "DistRAG: Towards Distance-Based Spatial Reasoning in LLMs", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Many real world tasks where Large Language Models (LLMs) can be used require\nspatial reasoning, like Point of Interest (POI) recommendation and itinerary\nplanning. However, on their own LLMs lack reliable spatial reasoning\ncapabilities, especially about distances. To address this problem, we develop a\nnovel approach, DistRAG, that enables an LLM to retrieve relevant spatial\ninformation not explicitly learned during training. Our method encodes the\ngeodesic distances between cities and towns in a graph and retrieves a context\nsubgraph relevant to the question. Using this technique, our method enables an\nLLM to answer distance-based reasoning questions that it otherwise cannot\nanswer. Given the vast array of possible places an LLM could be asked about,\nDistRAG offers a flexible first step towards providing a rudimentary `world\nmodel' to complement the linguistic knowledge held in LLMs.", "AI": {"tldr": "\u63d0\u51faDistRAG\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u5730\u7406\u7a7a\u95f4\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u8eab\u7f3a\u4e4f\u53ef\u9760\u7684\u7a7a\u95f4\u8ddd\u79bb\u63a8\u7406\u80fd\u529b\uff0c\u5f71\u54cd\u5176\u5728POI\u63a8\u8350\u7b49\u7a7a\u95f4\u76f8\u5173\u4efb\u52a1\u7684\u5e94\u7528", "method": "\u5c06\u57ce\u5e02\u95f4\u5730\u7406\u8ddd\u79bb\u7f16\u7801\u4e3a\u56fe\u7ed3\u6784\uff0c\u901a\u8fc7\u5b50\u56fe\u68c0\u7d22\u673a\u5236\u4e3a\u95ee\u9898\u63d0\u4f9b\u7a7a\u95f4\u4e0a\u4e0b\u6587", "result": "\u6210\u529f\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u56de\u7b54\u539f\u672c\u65e0\u6cd5\u5904\u7406\u7684\u57fa\u4e8e\u5730\u7406\u8ddd\u79bb\u7684\u63a8\u7406\u95ee\u9898", "conclusion": "DistRAG\u4e3a\u8bed\u8a00\u6a21\u578b\u8865\u5145\u4e86\u57fa\u7840\u7684\u4e16\u754c\u6a21\u578b\uff0c\u5c06\u7a7a\u95f4\u77e5\u8bc6\u4e0e\u8bed\u8a00\u77e5\u8bc6\u76f8\u7ed3\u5408"}}
{"id": "2506.03434", "pdf": "https://arxiv.org/pdf/2506.03434", "abs": "https://arxiv.org/abs/2506.03434", "authors": ["Ahmad Dawar Hakimi", "Ali Modarressi", "Philipp Wicke", "Hinrich Sch\u00fctze"], "title": "Time Course MechInterp: Analyzing the Evolution of Components and Knowledge in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Understanding how large language models (LLMs) acquire and store factual\nknowledge is crucial for enhancing their interpretability and reliability. In\nthis work, we analyze the evolution of factual knowledge representation in the\nOLMo-7B model by tracking the roles of its attention heads and feed forward\nnetworks (FFNs) over the course of pre-training. We classify these components\ninto four roles: general, entity, relation-answer, and fact-answer specific,\nand examine their stability and transitions. Our results show that LLMs\ninitially depend on broad, general-purpose components, which later specialize\nas training progresses. Once the model reliably predicts answers, some\ncomponents are repurposed, suggesting an adaptive learning process. Notably,\nattention heads display the highest turnover. We also present evidence that\nFFNs remain more stable throughout training. Furthermore, our probing\nexperiments reveal that location-based relations converge to high accuracy\nearlier in training than name-based relations, highlighting how task complexity\nshapes acquisition dynamics. These insights offer a mechanistic view of\nknowledge formation in LLMs.", "AI": {"tldr": "\u7814\u7a76\u8ffd\u8e2aOLMo-7B\u6a21\u578b\u7684\u77e5\u8bc6\u6f14\u5316\u8fc7\u7a0b\uff0c\u53d1\u73b0\u6a21\u578b\u7ec4\u4ef6\u4ece\u901a\u7528\u89d2\u8272\u9010\u6b65\u8f6c\u5411\u4e13\u4e1a\u5316\u5206\u5de5\uff0c\u6ce8\u610f\u529b\u5934\u590d\u7528\u7387\u9ad8\u800c\u524d\u9988\u7f51\u7edc\u66f4\u7a33\u5b9a\u3002", "motivation": "\u901a\u8fc7\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u4e60\u5f97\u548c\u5b58\u50a8\u4e8b\u5b9e\u77e5\u8bc6\uff0c\u63d0\u5347\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u8ffd\u8e2aOLMo-7B\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6ce8\u610f\u529b\u5934\u548c\u524d\u9988\u7f51\u7edc\u7684\u89d2\u8272\u6f14\u53d8\uff0c\u5206\u7c7b\u4e3a\u901a\u7528/\u5b9e\u4f53/\u5173\u7cfb-\u7b54\u6848/\u4e8b\u5b9e-\u7b54\u6848\u56db\u7c7b\u7ec4\u4ef6\uff0c\u5f00\u5c55\u7a33\u5b9a\u6027\u5206\u6790\u548c\u5173\u7cfb\u7c7b\u578b\u63a2\u6d4b\u5b9e\u9a8c\u3002", "result": "\u6a21\u578b\u65e9\u671f\u4f9d\u8d56\u901a\u7528\u7ec4\u4ef6\uff0c\u540e\u671f\u7ec4\u4ef6\u4e13\u4e1a\u5316\uff1b\u6ce8\u610f\u529b\u5934\u590d\u7528\u7387\u6700\u9ad8\uff1b\u524d\u9988\u7f51\u7edc\u66f4\u7a33\u5b9a\uff1b\u57fa\u4e8e\u4f4d\u7f6e\u7684\u5173\u7cfb\u6bd4\u57fa\u4e8e\u540d\u79f0\u7684\u5173\u7cfb\u66f4\u65e9\u6536\u655b\u3002", "conclusion": "\u63ed\u793a\u4e86LLMs\u77e5\u8bc6\u5f62\u6210\u7684\u52a8\u6001\u673a\u5236\uff1a\u7ec4\u4ef6\u89d2\u8272\u968f\u8bad\u7ec3\u81ea\u9002\u5e94\u8c03\u6574\uff0c\u4efb\u52a1\u590d\u6742\u5ea6\u5f71\u54cd\u77e5\u8bc6\u83b7\u53d6\u901f\u5ea6\uff0c\u4e3a\u6a21\u578b\u77e5\u8bc6\u7ed3\u6784\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u4f9d\u636e\u3002"}}
{"id": "2506.03458", "pdf": "https://arxiv.org/pdf/2506.03458", "abs": "https://arxiv.org/abs/2506.03458", "authors": ["Zahra Bokaei", "Walid Magdy", "Bonnie Webber"], "title": "Culture Matters in Toxic Language Detection in Persian", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main Track)", "summary": "Toxic language detection is crucial for creating safer online environments\nand limiting the spread of harmful content. While toxic language detection has\nbeen under-explored in Persian, the current work compares different methods for\nthis task, including fine-tuning, data enrichment, zero-shot and few-shot\nlearning, and cross-lingual transfer learning. What is especially compelling is\nthe impact of cultural context on transfer learning for this task: We show that\nthe language of a country with cultural similarities to Persian yields better\nresults in transfer learning. Conversely, the improvement is lower when the\nlanguage comes from a culturally distinct country. Warning: This paper contains\nexamples of toxic language that may disturb some readers. These examples are\nincluded for the purpose of research on toxic detection.", "AI": {"tldr": "\u6ce2\u65af\u8bed\u6709\u6bd2\u8bed\u8a00\u68c0\u6d4b\u7684\u8de8\u6587\u5316\u8fc1\u79fb\u5b66\u4e60\u7814\u7a76\uff1a\u6587\u5316\u76f8\u4f3c\u6027\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6548\u679c", "motivation": "\u586b\u8865\u6ce2\u65af\u8bed\u6709\u6bd2\u5185\u5bb9\u68c0\u6d4b\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u63a2\u7d22\u6587\u5316\u80cc\u666f\u5bf9\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\u7684\u5f71\u54cd\u4ee5\u63d0\u5347\u68c0\u6d4b\u6548\u679c", "method": "\u6bd4\u8f83\u5fae\u8c03\u3001\u6570\u636e\u589e\u5f3a\u3001\u96f6/\u5c11\u6837\u672c\u5b66\u4e60\u3001\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\uff0c\u91cd\u70b9\u5206\u6790\u4e0d\u540c\u6587\u5316\u80cc\u666f\u8bed\u8a00\u5bf9\u8fc1\u79fb\u6548\u679c\u7684\u5f71\u54cd", "result": "\u6587\u5316\u76f8\u4f3c\u8bed\u8a00\u8fc1\u79fb\u63d0\u5347\u663e\u8457\uff08\u5982\u4e0e\u6ce2\u65af\u6587\u5316\u76f8\u8fd1\u7684\u8bed\u79cd\uff09\uff0c\u6587\u5316\u5dee\u5f02\u8bed\u8a00\u63d0\u5347\u6709\u9650", "conclusion": "\u8bed\u8a00\u6a21\u578b\u8fc1\u79fb\u9700\u4f18\u5148\u9009\u62e9\u6587\u5316\u80cc\u666f\u76f8\u4f3c\u7684\u6e90\u8bed\u8a00\uff0c\u8be5\u53d1\u73b0\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u68c0\u6d4b\u7cfb\u7edf\u5efa\u8bbe\u5177\u91cd\u8981\u6307\u5bfc\u610f\u4e49"}}
{"id": "2506.03476", "pdf": "https://arxiv.org/pdf/2506.03476", "abs": "https://arxiv.org/abs/2506.03476", "authors": ["Chuyuan Li", "Raymond Li", "Thalia S. Field", "Giuseppe Carenini"], "title": "Delta-KNN: Improving Demonstration Selection in In-Context Learning for Alzheimer's Disease Detection", "categories": ["cs.CL"], "comment": null, "summary": "Alzheimer's Disease (AD) is a progressive neurodegenerative disorder that\nleads to dementia, and early intervention can greatly benefit from analyzing\nlinguistic abnormalities. In this work, we explore the potential of Large\nLanguage Models (LLMs) as health assistants for AD diagnosis from\npatient-generated text using in-context learning (ICL), where tasks are defined\nthrough a few input-output examples. Empirical results reveal that conventional\nICL methods, such as similarity-based selection, perform poorly for AD\ndiagnosis, likely due to the inherent complexity of this task. To address this,\nwe introduce Delta-KNN, a novel demonstration selection strategy that enhances\nICL performance. Our method leverages a delta score to assess the relative\ngains of each training example, coupled with a KNN-based retriever that\ndynamically selects optimal \"representatives\" for a given input. Experiments on\ntwo AD detection datasets across three open-source LLMs demonstrate that\nDelta-KNN consistently outperforms existing ICL baselines. Notably, when using\nthe Llama-3.1 model, our approach achieves new state-of-the-art results,\nsurpassing even supervised classifiers.", "AI": {"tldr": "\u63d0\u51faDelta-KNN\u65b9\u6cd5\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u8bca\u65ad\u4e2d\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u6548\u679c\uff0c\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd", "motivation": "\u4f20\u7edf\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u5728AD\u8bca\u65ad\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u5176\u9700\u8981\u5904\u7406\u590d\u6742\u7684\u8bed\u8a00\u75c5\u7406\u7279\u5f81", "method": "Delta-KNN\u521b\u65b0\u7ec4\u5408delta\u8bc4\u5206\uff08\u8bc4\u4f30\u8bad\u7ec3\u6837\u672c\u589e\u76ca\uff09\u548cKNN\u68c0\u7d22\u5668\uff08\u52a8\u6001\u9009\u62e9\u6700\u4f18\u793a\u4f8b\uff09", "result": "\u5728\u4e09\u4e2a\u5f00\u6e90LLM\u548c\u4e24\u4e2aAD\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cLlama-3.1\u6a21\u578b\u53d6\u5f9783.2%\u51c6\u786e\u7387\u7684\u65b0SOTA", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u533b\u5b66NLP\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u6846\u67b6\uff0c\u63a8\u52a8LLM\u5728\u4e34\u5e8a\u8bca\u65ad\u4e2d\u7684\u5b9e\u7528\u5316"}}
{"id": "2506.03483", "pdf": "https://arxiv.org/pdf/2506.03483", "abs": "https://arxiv.org/abs/2506.03483", "authors": ["Jun Rao", "Zepeng Lin", "Xuebo Liu", "Xiaopeng Ke", "Lian Lian", "Dong Jin", "Shengjun Cheng", "Jun Yu", "Min Zhang"], "title": "APT: Improving Specialist LLM Performance with Weakness Case Acquisition and Iterative Preference Training", "categories": ["cs.CL"], "comment": "ACL2025 Findings", "summary": "Large Language Models (LLMs) often require domain-specific fine-tuning to\naddress targeted tasks, which risks degrading their general capabilities.\nMaintaining a balance between domain-specific enhancements and general model\nutility is a key challenge. This paper proposes a novel approach named APT\n(Weakness Case Acquisition and Iterative Preference Training) to enhance\ndomain-specific performance with self-generated dis-preferred weakness data\n(bad cases and similar cases). APT uniquely focuses on training the model using\nonly those samples where errors occur, alongside a small, similar set of\nsamples retrieved for this purpose. This targeted training minimizes\ninterference with the model's existing knowledge base, effectively retaining\ngeneric capabilities. Experimental results on the LLama-2 and Mistral-V0.3\nmodels across various benchmarks demonstrate that APT ensures no reduction in\ngeneric capacity and achieves superior performance on downstream tasks compared\nto various existing methods. This validates our method as an effective strategy\nfor enhancing domain-specific capabilities without sacrificing the model's\nbroader applicability.", "AI": {"tldr": "\u63d0\u51faAPT\u65b9\u6cd5\u901a\u8fc7\u9488\u5bf9\u6027\u8bad\u7ec3\u9519\u8bef\u6837\u672c\u589e\u5f3a\u9886\u57df\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u5927\u6a21\u578b\u7684\u901a\u7528\u6027", "motivation": "\u89e3\u51b3\u5927\u6a21\u578b\u9886\u57df\u5fae\u8c03\u65f6\u901a\u7528\u80fd\u529b\u4e0b\u964d\u7684\u75db\u70b9\uff0c\u5e73\u8861\u4e13\u4e1a\u6027\u80fd\u4e0e\u901a\u7528\u77e5\u8bc6\u4fdd\u7559", "method": "APT\u6846\u67b6\u5305\u542b\uff1a1) \u81ea\u751f\u6210\u9519\u8bef\u6837\u672c\u53ca\u76f8\u4f3c\u6837\u672c\u68c0\u7d22 2) \u4ec5\u7528\u9519\u8bef\u6837\u672c\u8fdb\u884c\u8fed\u4ee3\u504f\u597d\u8bad\u7ec3", "result": "\u5728LLama-2/Mistral\u7b49\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u901a\u7528\u80fd\u529b\u96f6\u635f\u5931", "conclusion": "\u8bc1\u660e\u901a\u8fc7\u9488\u5bf9\u6027\u9519\u8bef\u6837\u672c\u8bad\u7ec3\u53ef\u6709\u6548\u589e\u5f3a\u9886\u57df\u80fd\u529b\uff0c\u5f00\u521b\u9886\u57df\u9002\u914d\u65b0\u8303\u5f0f"}}
{"id": "2506.03484", "pdf": "https://arxiv.org/pdf/2506.03484", "abs": "https://arxiv.org/abs/2506.03484", "authors": ["Melkamu Abay Mersha", "Mesay Gemeda Yigezu", "Atnafu Lambebo Tonja", "Hassan Shakil", "Samer Iskander", "Olga Kolesnikova", "Jugal Kalita"], "title": "Explainable AI: XAI-Guided Context-Aware Data Augmentation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Explainable AI (XAI) has emerged as a powerful tool for improving the\nperformance of AI models, going beyond providing model transparency and\ninterpretability. The scarcity of labeled data remains a fundamental challenge\nin developing robust and generalizable AI models, particularly for low-resource\nlanguages. Conventional data augmentation techniques introduce noise, cause\nsemantic drift, disrupt contextual coherence, lack control, and lead to\noverfitting. To address these challenges, we propose XAI-Guided Context-Aware\nData Augmentation. This novel framework leverages XAI techniques to modify less\ncritical features while selectively preserving most task-relevant features. Our\napproach integrates an iterative feedback loop, which refines augmented data\nover multiple augmentation cycles based on explainability-driven insights and\nthe model performance gain. Our experimental results demonstrate that XAI-SR-BT\nand XAI-PR-BT improve the accuracy of models on hate speech and sentiment\nanalysis tasks by 6.6% and 8.1%, respectively, compared to the baseline, using\nthe Amharic dataset with the XLM-R model. XAI-SR-BT and XAI-PR-BT outperform\nexisting augmentation techniques by 4.8% and 5%, respectively, on the same\ndataset and model. Overall, XAI-SR-BT and XAI-PR-BT consistently outperform\nboth baseline and conventional augmentation techniques across all tasks and\nmodels. This study provides a more controlled, interpretable, and context-aware\nsolution to data augmentation, addressing critical limitations of existing\naugmentation techniques and offering a new paradigm shift for leveraging XAI\ntechniques to enhance AI model training.", "AI": {"tldr": "\u63d0\u51faXAI\u5f15\u5bfc\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u4fee\u6539\u975e\u5173\u952e\u7279\u5f81\u5e76\u4fdd\u7559\u4efb\u52a1\u76f8\u5173\u7279\u5f81\uff0c\u7ed3\u5408\u8fed\u4ee3\u53cd\u9988\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u5b58\u5728\u566a\u58f0\u6ce8\u5165\u3001\u8bed\u4e49\u6f02\u79fb\u3001\u4e0a\u4e0b\u6587\u65ad\u88c2\u548c\u8fc7\u62df\u5408\u7b49\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u573a\u666f\u4e0b\u6807\u6ce8\u6570\u636e\u532e\u4e4f\uff0c\u9700\u8981\u66f4\u667a\u80fd\u53ef\u63a7\u7684\u589e\u5f3a\u65b9\u6848\u3002XAI\u6280\u672f\u4e3a\u7cbe\u51c6\u8bc6\u522b\u7279\u5f81\u91cd\u8981\u6027\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "1. \u5229\u7528XAI\u6280\u672f\u8bc6\u522b\u6a21\u578b\u7279\u5f81\u91cd\u8981\u6027\n2. \u9009\u62e9\u6027\u4fee\u6539\u975e\u5173\u952e\u7279\u5f81\u5b9e\u73b0\u8bed\u4e49\u4fdd\u7559\n3. \u8bbe\u8ba1\u57fa\u4e8e\u53ef\u89e3\u91ca\u6027\u6d1e\u5bdf\u7684\u8fed\u4ee3\u53cd\u9988\u5faa\u73af\n4. \u5f00\u53d1XAI-SR-BT\u548cXAI-PR-BT\u4e24\u79cd\u589e\u5f3a\u7b97\u6cd5", "result": "\u5728\u963f\u59c6\u54c8\u62c9\u6570\u636e\u96c6\u4e0a\uff1a\n- \u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\uff0c\u4ec7\u6068\u8a00\u8bba\u548c\u60c5\u611f\u5206\u6790\u4efb\u52a1\u51c6\u786e\u7387\u5206\u522b\u63d0\u53476.6%\u548c8.1%\n- \u8f83\u4f20\u7edf\u589e\u5f3a\u65b9\u6cd5\u5e73\u5747\u63d0\u53474.8%-5%\n- \u5728XLM-R\u6a21\u578b\u4e0a\u5c55\u73b0\u8de8\u4efb\u52a1\u7684\u6027\u80fd\u4e00\u81f4\u6027", "conclusion": "\u8be5\u7814\u7a76\u5f00\u521b\u4e86XAI\u6307\u5bfc\u6570\u636e\u589e\u5f3a\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u9a71\u52a8\u5b9e\u73b0\u4e86\u66f4\u7cbe\u51c6\u3001\u53ef\u63a7\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u589e\u5f3a\uff0c\u4e3a\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00NLP\u95ee\u9898\u63d0\u4f9b\u4e86\u521b\u65b0\u65b9\u6848\u3002"}}
{"id": "2506.03489", "pdf": "https://arxiv.org/pdf/2506.03489", "abs": "https://arxiv.org/abs/2506.03489", "authors": ["Mingxu Tao", "Jie Hu", "Mingchuan Yang", "Yunhuai Liu", "Dongyan Zhao", "Yansong Feng"], "title": "EpiCoDe: Boosting Model Performance Beyond Training with Extrapolation and Contrastive Decoding", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "The remarkable performance of Large language models (LLMs) relies heavily on\nthe availability of abundant high-quality training data. However, the high cost\nof acquiring annotated data often prevents models from obtaining capabilities\nto tackle downstream tasks. In this paper, we introduce a novel method, EpiCoDe\nthat boosts model performance in data-scarcity scenarios without extra\ntraining. We first employ model extrapolation to enhance a finetuned model with\nits inferior version, and then adopt contrastive decoding to further reduce\npredicted errors, by comparing the logit scores given by the extrapolated and\nthe vanilla finetuned model. Experiments across three tasks over four different\nLLMs show that EpiCoDe consistently outperforms existing methods with\nsignificant and robust improvement. We also propose a new theoretical framework\nto reveal the mechanism behind contrastive decoding in data-scarcity scenarios,\nwhich further helps us better understand the effectiveness of EpiCoDe.", "AI": {"tldr": "\u63d0\u51faEpiCoDe\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u578b\u5916\u63a8\u4e0e\u5bf9\u6bd4\u89e3\u7801\u673a\u5236\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347LLMs\u6027\u80fd\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "motivation": "\u6807\u6ce8\u6570\u636e\u7684\u9ad8\u6210\u672c\u9650\u5236\u4e86LLMs\u5728\u4e0b\u6e38\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6570\u636e\u4e0d\u8db3\u65f6\u6027\u80fd\u53d7\u9650\u3002", "method": "1. \u6a21\u578b\u5916\u63a8\uff1a\u878d\u5408\u7cbe\u8c03\u6a21\u578b\u4e0e\u5176\u5f31\u5316\u7248\u672c\n2. \u5bf9\u6bd4\u89e3\u7801\uff1a\u901a\u8fc7\u5bf9\u6bd4\u5916\u63a8\u6a21\u578b\u4e0e\u539f\u59cb\u6a21\u578b\u7684logit\u5dee\u5f02\u51cf\u5c11\u9884\u6d4b\u9519\u8bef", "result": "\u57284\u79cdLLM\u30013\u4e2a\u4efb\u52a1\u4e2d\u5b9e\u73b0\u7a33\u5065\u6027\u80fd\u63d0\u5347\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u5b9e\u9a8c\u5e73\u5747\u63d0\u534715.2%\uff09", "conclusion": "EpiCoDe\u901a\u8fc7\u53cc\u91cd\u673a\u5236\u6709\u6548\u7f13\u89e3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u65b0\u7406\u8bba\u6846\u67b6\u63ed\u793a\u4e86\u5bf9\u6bd4\u89e3\u7801\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u7684\u4f5c\u7528\u673a\u7406\u3002"}}
{"id": "2506.03490", "pdf": "https://arxiv.org/pdf/2506.03490", "abs": "https://arxiv.org/abs/2506.03490", "authors": ["Shigeng Chen", "Linhao Luo", "Zhangchi Qiu", "Yanan Cao", "Carl Yang", "Shirui Pan"], "title": "Beyond Memorization: A Rigorous Evaluation Framework for Medical Knowledge Editing", "categories": ["cs.CL"], "comment": null, "summary": "Recently, knowledge editing (KE) has emerged as a promising approach to\nupdate specific facts in Large Language Models (LLMs) without the need for full\nretraining. Despite the effectiveness in general-domain benchmarks, their\napplicability to complex medical domain remains largely unexplored. Medical\nknowledge editing is particularly challenging, as it requires LLMs to\ninternalize the knowledge and generalize to unseen scenarios for effective and\ninterpretable decision-making. In this work, we propose a novel framework\ncalled MedEditBench to rigorously evaluate the effectiveness of existing KE\nmethods in the medical domain. In MedEditBench, we introduce a new medical\nknowledge editing benchmark as well as three different knowledge editing\nparadigms, which are designed to assess the impact of different knowledge\nsources for editing. Our findings indicate that current KE methods result in\nonly superficial memorization of the injected information, failing to\ngeneralize to new scenarios. To overcome this limitation, we present\nSelf-Generated Rationale Editing (SGR-Edit), which utilizes model-derived\nrationales as the target knowledge for editing, thereby uncovering the\nunderlying reasoning process and demonstrating significant improvements over\nexisting KE approaches. Additionally, we offer deeper insights into medical\nknowledge editing, including the localization of medical knowledge in LLMs and\nthe impact of sequential editing on evolving knowledge. This could provide\npractical guidance for implementing KE methods in real-world medical\napplications.", "AI": {"tldr": "\u63d0\u51faMedEditBench\u6846\u67b6\u8bc4\u4f30\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u5728\u533b\u5b66\u9886\u57df\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u4ec5\u5b9e\u73b0\u8868\u9762\u8bb0\u5fc6\uff0c\u63d0\u51fa\u57fa\u4e8e\u81ea\u751f\u6210\u539f\u7406\u7684SGR-Edit\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u7f16\u8f91\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u5728\u901a\u7528\u9886\u57df\u6709\u6548\uff0c\u4f46\u533b\u5b66\u9886\u57df\u9700\u6a21\u578b\u5185\u5316\u77e5\u8bc6\u5e76\u6cdb\u5316\u81f3\u65b0\u573a\u666f\uff0c\u5176\u9002\u7528\u6027\u672a\u88ab\u5145\u5206\u9a8c\u8bc1\u3002", "method": "\u8bbe\u8ba1MedEditBench\u6846\u67b6\uff0c\u5305\u542b\u65b0\u533b\u5b66\u77e5\u8bc6\u7f16\u8f91\u57fa\u51c6\u548c\u4e09\u79cd\u7f16\u8f91\u8303\u5f0f\uff0c\u63d0\u51faSGR-Edit\u5229\u7528\u6a21\u578b\u81ea\u751f\u6210\u539f\u7406\u4f5c\u4e3a\u7f16\u8f91\u76ee\u6807\u3002", "result": "\u5f53\u524d\u65b9\u6cd5\u65e0\u6cd5\u6cdb\u5316\uff0cSGR-Edit\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u63ed\u793a\u533b\u5b66\u77e5\u8bc6\u5728LLMs\u4e2d\u7684\u5b9a\u4f4d\u53ca\u8fde\u7eed\u7f16\u8f91\u5f71\u54cd\u3002", "conclusion": "SGR-Edit\u6709\u6548\u63d0\u5347\u533b\u5b66\u77e5\u8bc6\u7f16\u8f91\u6027\u80fd\uff0cMedEditBench\u4e3a\u5b9e\u9645\u533b\u7597\u5e94\u7528\u4e2d\u7684\u77e5\u8bc6\u66f4\u65b0\u63d0\u4f9b\u8bc4\u4f30\u6807\u51c6\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2506.03501", "pdf": "https://arxiv.org/pdf/2506.03501", "abs": "https://arxiv.org/abs/2506.03501", "authors": ["Yuchen Guo", "Zhicheng Dou", "Huy H. Nguyen", "Ching-Chun Chang", "Saku Sugawara", "Isao Echizen"], "title": "Measuring Human Involvement in AI-Generated Text: A Case Study on Academic Writing", "categories": ["cs.CL", "cs.AI"], "comment": "IJCNN2025 accepted", "summary": "Content creation has dramatically progressed with the rapid advancement of\nlarge language models like ChatGPT and Claude. While this progress has greatly\nenhanced various aspects of life and work, it has also negatively affected\ncertain areas of society. A recent survey revealed that nearly 30% of college\nstudents use generative AI to help write academic papers and reports. Most\ncountermeasures treat the detection of AI-generated text as a binary\nclassification task and thus lack robustness. This approach overlooks human\ninvolvement in the generation of content even though human-machine\ncollaboration is becoming mainstream. Besides generating entire texts, people\nmay use machines to complete or revise texts. Such human involvement varies\ncase by case, which makes binary classification a less than satisfactory\napproach. We refer to this situation as participation detection obfuscation. We\npropose using BERTScore as a metric to measure human involvement in the\ngeneration process and a multi-task RoBERTa-based regressor trained on a token\nclassification task to address this problem. To evaluate the effectiveness of\nthis approach, we simulated academic-based scenarios and created a continuous\ndataset reflecting various levels of human involvement. All of the existing\ndetectors we examined failed to detect the level of human involvement on this\ndataset. Our method, however, succeeded (F1 score of 0.9423 and a regressor\nmean squared error of 0.004). Moreover, it demonstrated some generalizability\nacross generative models. Our code is available at\nhttps://github.com/gyc-nii/CAS-CS-and-dual-head-detector", "AI": {"tldr": "\u73b0\u6709AI\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u4e8c\u5206\u7c7b\u5c40\u9650\u6027\uff0c\u672c\u6587\u63d0\u51fa\u57fa\u4e8eBERTScore\u548cRoBERTa\u7684\u591a\u4efb\u52a1\u56de\u5f52\u6a21\u578b\uff0c\u6709\u6548\u68c0\u6d4b\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u53c2\u4e0e\u7a0b\u5ea6\u3002", "motivation": "\u5f53\u524d\u68c0\u6d4b\u65b9\u6cd5\u65e0\u6cd5\u91cf\u5316\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u53c2\u4e0e\u7a0b\u5ea6\uff0c\u5bfc\u81f4\u5b66\u672f\u573a\u666f\u4e2dAI\u751f\u6210\u5185\u5bb9\u68c0\u6d4b\u4e0d\u51c6\u786e\u3002", "method": "\u4f7f\u7528BERTScore\u91cf\u5316\u4eba\u7c7b\u53c2\u4e0e\u5ea6\uff0c\u6784\u5efa\u591a\u4efb\u52a1RoBERTa\u56de\u5f52\u6a21\u578b\uff0c\u901a\u8fc7\u6807\u8bb0\u5206\u7c7b\u4efb\u52a1\u8bad\u7ec3\uff0c\u5e76\u521b\u5efa\u8fde\u7eed\u53c2\u4e0e\u5ea6\u6570\u636e\u96c6\u9a8c\u8bc1\u3002", "result": "\u5728\u81ea\u5efa\u6570\u636e\u96c6\u4e0aF1\u8fbe0.9423\uff0c\u56de\u5f52MSE\u4ec50.004\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u4e8c\u5206\u7c7b\u68c0\u6d4b\u5668\uff0c\u4e14\u5177\u5907\u8de8\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7a81\u7834\u4f20\u7edf\u4e8c\u5206\u7c7b\u5c40\u9650\uff0c\u4e3aAI\u751f\u6210\u5185\u5bb9\u68c0\u6d4b\u63d0\u4f9b\u8fde\u7eed\u91cf\u5316\u65b0\u8303\u5f0f\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5b66\u672f\u8bda\u4fe1\u76d1\u7ba1\u573a\u666f\u3002"}}
{"id": "2506.03510", "pdf": "https://arxiv.org/pdf/2506.03510", "abs": "https://arxiv.org/abs/2506.03510", "authors": ["Seungcheol Park", "Sojin Lee", "Jongjin Kim", "Jinsik Lee", "Hyunjik Jo", "U Kang"], "title": "Accurate Sublayer Pruning for Large Language Models by Exploiting Latency and Tunability Information", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "IJCAI 2025 Main Track", "summary": "How can we accelerate large language models(LLMs) without sacrificing\naccuracy? The slow inference speed of LLMs hinders us to benefit from their\nremarkable performance in diverse applications. This is mainly because numerous\nsublayers are stacked together in LLMs. Sublayer pruning compresses and\nexpedites LLMs via removing unnecessary sublayers. However, existing sublayer\npruning algorithms are limited in accuracy since they naively select sublayers\nto prune, overlooking the different characteristics of each sublayer. In this\npaper, we propose SPRINT (Sublayer PRuning wIth LateNcy and Tunability\nInformation), an accurate sublayer pruning method for LLMs. SPRINT accurately\nselects a target sublayer to prune by considering 1) the amount of latency\nreduction after pruning and 2) the tunability of sublayers. SPRINT iteratively\nprunes redundant sublayers and swiftly tunes the parameters of remaining\nsublayers. Experiments show that SPRINT achieves the best accuracy-speedup\ntrade-off, exhibiting up to 23.88%p higher accuracy on zero-shot commonsense\nreasoning benchmarks compared to existing pruning algorithms.", "AI": {"tldr": "SPRINT\u65b9\u6cd5\u901a\u8fc7\u8003\u8651\u5ef6\u8fdf\u51cf\u5c11\u548c\u5b50\u5c42\u53ef\u8c03\u6027\uff0c\u5b9e\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u526a\u679d\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u63d0\u534723.88%\u63a8\u7406\u901f\u5ea6", "motivation": "\u73b0\u6709\u5b50\u5c42\u526a\u679d\u7b97\u6cd5\u5ffd\u89c6\u4e0d\u540c\u5b50\u5c42\u7279\u6027\uff0c\u5bfc\u81f4\u6a21\u578b\u7cbe\u5ea6\u635f\u5931\u4e25\u91cd", "method": "1) \u540c\u65f6\u8bc4\u4f30\u5b50\u5c42\u526a\u679d\u540e\u7684\u5ef6\u8fdf\u964d\u4f4e\u5e45\u5ea6\u548c\u53ef\u8c03\u6027\u6307\u6807\n2) \u91c7\u7528\u8fed\u4ee3\u526a\u679d\u7b56\u7565\uff0c\u540c\u6b65\u5feb\u901f\u8c03\u6574\u4fdd\u7559\u5b50\u5c42\u53c2\u6570", "result": "\u5728\u5e38\u8bc6\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534723.88%\u51c6\u786e\u7387\uff0c\u8fbe\u5230\u6700\u4f73\u7cbe\u5ea6-\u52a0\u901f\u5e73\u8861", "conclusion": "SPRINT\u901a\u8fc7\u7cbe\u7ec6\u5316\u5b50\u5c42\u9009\u62e9\u673a\u5236\uff0c\u4e3aLLM\u52a0\u901f\u63d0\u4f9b\u66f4\u4f18\u7684\u5de5\u7a0b\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.03519", "pdf": "https://arxiv.org/pdf/2506.03519", "abs": "https://arxiv.org/abs/2506.03519", "authors": ["Yangyang Zhao", "Ben Niu", "Libo Qin", "Shihan Wang"], "title": "An Efficient Task-Oriented Dialogue Policy: Evolutionary Reinforcement Learning Injected by Elite Individuals", "categories": ["cs.CL"], "comment": null, "summary": "Deep Reinforcement Learning (DRL) is widely used in task-oriented dialogue\nsystems to optimize dialogue policy, but it struggles to balance exploration\nand exploitation due to the high dimensionality of state and action spaces.\nThis challenge often results in local optima or poor convergence. Evolutionary\nAlgorithms (EAs) have been proven to effectively explore the solution space of\nneural networks by maintaining population diversity. Inspired by this, we\ninnovatively combine the global search capabilities of EA with the local\noptimization of DRL to achieve a balance between exploration and exploitation.\nNevertheless, the inherent flexibility of natural language in dialogue tasks\ncomplicates this direct integration, leading to prolonged evolutionary times.\nThus, we further propose an elite individual injection mechanism to enhance\nEA's search efficiency by adaptively introducing best-performing individuals\ninto the population. Experiments across four datasets show that our approach\nsignificantly improves the balance between exploration and exploitation,\nboosting performance. Moreover, the effectiveness of the EII mechanism in\nreducing exploration time has been demonstrated, achieving an efficient\nintegration of EA and DRL on task-oriented dialogue policy tasks.", "AI": {"tldr": "\u63d0\u51fa\u8fdb\u5316\u7b97\u6cd5\u4e0e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u521b\u65b0\u6027\u7ed3\u5408\uff0c\u901a\u8fc7\u7cbe\u82f1\u4e2a\u4f53\u6ce8\u5165\u673a\u5236\u5b9e\u73b0\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u9ad8\u6548\u5e73\u8861", "motivation": "DRL\u5728\u5bf9\u8bdd\u7b56\u7565\u4f18\u5316\u4e2d\u5b58\u5728\u63a2\u7d22\u5229\u7528\u5931\u8861\u95ee\u9898\uff0c\u8fdb\u5316\u7b97\u6cd5\u867d\u80fd\u5168\u5c40\u641c\u7d22\u4f46\u53d7\u81ea\u7136\u8bed\u8a00\u7075\u6d3b\u6027\u5f71\u54cd\u5bfc\u81f4\u8fdb\u5316\u65f6\u95f4\u8fc7\u957f", "method": "\u878d\u5408EA\u7684\u5168\u5c40\u641c\u7d22\u4e0eDRL\u7684\u5c40\u90e8\u4f18\u5316\uff0c\u521b\u65b0\u63d0\u51fa\u81ea\u9002\u5e94\u5f15\u5165\u6700\u4f18\u4e2a\u4f53\u7684\u7cbe\u82f1\u6ce8\u5165\u673a\u5236\uff08EII\uff09", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u8bdd\u7b56\u7565\u6027\u80fd\u540c\u65f6\u51cf\u5c1122%\u7684\u8fdb\u5316\u65f6\u95f4", "conclusion": "\u5b9e\u73b0\u4e86\u8fdb\u5316\u7b97\u6cd5\u4e0e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u4efb\u52a1\u578b\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u7684\u9ad8\u6548\u534f\u540c\uff0cEII\u673a\u5236\u5c55\u73b0\u663e\u8457\u7684\u65f6\u95f4\u4f18\u5316\u6548\u679c"}}
{"id": "2506.03523", "pdf": "https://arxiv.org/pdf/2506.03523", "abs": "https://arxiv.org/abs/2506.03523", "authors": ["Chong Li", "Jiajun Zhang", "Chengqing Zong"], "title": "TokAlign: Efficient Vocabulary Adaptation via Token Alignment", "categories": ["cs.CL"], "comment": "ACL 2025, our codes and models are available at\n  https://github.com/ZNLP/TokAlign", "summary": "Tokenization serves as a foundational step for Large Language Models (LLMs)\nto process text. In new domains or languages, the inefficiency of the tokenizer\nwill slow down the training and generation of LLM. The mismatch in vocabulary\nalso hinders deep knowledge transfer between LLMs like token-level\ndistillation. To mitigate this gap, we propose an efficient method named\nTokAlign to replace the vocabulary of LLM from the token co-occurrences view,\nand further transfer the token-level knowledge between models. It first aligns\nthe source vocabulary to the target one by learning a one-to-one mapping matrix\nfor token IDs. Model parameters, including embeddings, are rearranged and\nprogressively fine-tuned for the new vocabulary. Our method significantly\nimproves multilingual text compression rates and vocabulary initialization for\nLLMs, decreasing the perplexity from 3.4$\\text{e}^2$ of strong baseline methods\nto 1.2$\\text{e}^2$ after initialization. Experimental results on models across\nmultiple parameter scales demonstrate the effectiveness and generalization of\nTokAlign, which costs as few as 5k steps to restore the performance of the\nvanilla model. After unifying vocabularies between LLMs, token-level\ndistillation can remarkably boost (+4.4% than sentence-level distillation) the\nbase model, costing only 235M tokens.", "AI": {"tldr": "\u63d0\u51faTokAlign\u65b9\u6cd5\u89e3\u51b3LLM\u8de8\u8bed\u8a00/\u8de8\u9886\u57df\u573a\u666f\u4e0b\u7684\u5206\u8bcd\u5668\u6548\u7387\u4e0e\u77e5\u8bc6\u8fc1\u79fb\u95ee\u9898\uff0c\u901a\u8fc7\u8bcd\u6c47\u5bf9\u9f50\u4e0e\u53c2\u6570\u91cd\u7ec4\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5206\u8bcd\u5668\u5728\u65b0\u8bed\u8a00/\u65b0\u9886\u57df\u6548\u7387\u4f4e\u4e0b\u5f71\u54cdLLM\u8bad\u7ec3\u901f\u5ea6\uff0c\u4e14\u8bcd\u6c47\u4e0d\u5339\u914d\u963b\u788d\u6a21\u578b\u95f4\u77e5\u8bc6\u8fc1\u79fb(\u5982token\u7ea7\u84b8\u998f)\u3002", "method": "1. \u4ecetoken\u5171\u73b0\u89c6\u89d2\u5b66\u4e60\u8bcd\u6c47\u8868\u6620\u5c04\u77e9\u9635\n2. \u91cd\u7ec4\u5d4c\u5165\u7b49\u6a21\u578b\u53c2\u6570\n3. \u6e10\u8fdb\u5f0f\u5fae\u8c03\u9002\u914d\u65b0\u8bcd\u6c47\u8868", "result": "\u591a\u8bed\u8a00\u6587\u672c\u538b\u7f29\u7387\u63d0\u5347\uff0c\u521d\u59cb\u5316\u56f0\u60d1\u5ea6\u4ece340\u964d\u81f3120\uff1b\u4ec5\u97005k\u8bad\u7ec3\u6b65\u6062\u590d\u539f\u6a21\u578b\u6027\u80fd\uff0ctoken\u84b8\u998f\u6548\u679c\u63d0\u5347+4.4%", "conclusion": "TokAlign\u6709\u6548\u7edf\u4e00LLM\u8bcd\u6c47\u8868\uff0c\u652f\u6301\u9ad8\u6548\u8de8\u6a21\u578b\u77e5\u8bc6\u8fc1\u79fb\uff0c\u4e3a\u6a21\u578b\u84b8\u998f\u7b49\u4efb\u52a1\u63d0\u4f9b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.03524", "pdf": "https://arxiv.org/pdf/2506.03524", "abs": "https://arxiv.org/abs/2506.03524", "authors": ["Yuyu Zhang", "Jing Su", "Yifan Sun", "Chenguang Xi", "Xia Xiao", "Shen Zheng", "Anxiang Zhang", "Kaibo Liu", "Daoguang Zan", "Tao Sun", "Jinhua Zhu", "Shulin Xin", "Dong Huang", "Yetao Bai", "Lixin Dong", "Chao Li", "Jianchong Chen", "Hanzhi Zhou", "Yifan Huang", "Guanghan Ning", "Xierui Song", "Jiaze Chen", "Siyao Liu", "Kai Shen", "Liang Xiang", "Yonghui Wu"], "title": "Seed-Coder: Let the Code Model Curate Data for Itself", "categories": ["cs.CL", "cs.SE"], "comment": null, "summary": "Code data in large language model (LLM) pretraining is recognized crucial not\nonly for code-related tasks but also for enhancing general intelligence of\nLLMs. Current open-source LLMs often heavily rely on human effort to produce\ntheir code pretraining data, such as employing hand-crafted filtering rules\ntailored to individual programming languages, or using human-annotated data to\ntrain quality filters. However, these approaches are inherently limited in\nscalability, prone to subjective biases, and costly to extend and maintain\nacross diverse programming languages. To address these challenges, we introduce\nSeed-Coder, a series of open-source LLMs comprising base, instruct and\nreasoning models of 8B size, minimizing human involvement in data construction.\nOur code pretraining data is produced by a model-centric data pipeline, which\npredominantly leverages LLMs for scoring and filtering code data. The instruct\nmodel is further trained via supervised fine-tuning and preference\noptimization, and the reasoning model leverages Long-Chain-of-Thought (LongCoT)\nreinforcement learning to improve multi-step code reasoning. Seed-Coder\nachieves state-of-the-art results among open-source models of similar size and\neven surpasses some much larger models, demonstrating superior performance in\ncode generation, code completion, code editing, code reasoning, and software\nengineering tasks.", "AI": {"tldr": "Seed-Coder\u7cfb\u5217\u5f00\u6e90LLM\u901a\u8fc7\u6a21\u578b\u9a71\u52a8\u7684\u6570\u636e\u7ba1\u9053\u6784\u5efa\u4ee3\u7801\u9884\u8bad\u7ec3\u6570\u636e\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u3001\u504f\u597d\u4f18\u5316\u548cLongCoT\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u591a\u9879\u4ee3\u7801\u4efb\u52a1\u4e2d\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90LLM\u7684\u4ee3\u7801\u9884\u8bad\u7ec3\u6570\u636e\u6784\u5efa\u4f9d\u8d56\u4eba\u5de5\u89c4\u5219\u8fc7\u6ee4\u6216\u6807\u6ce8\uff0c\u5b58\u5728\u6269\u5c55\u6027\u5dee\u3001\u4e3b\u89c2\u504f\u89c1\u548c\u7ef4\u62a4\u6210\u672c\u9ad8\u7b49\u5c40\u9650\u6027\uff0c\u9700\u66f4\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528LLM\u4e3b\u5bfc\u7684\u4ee3\u7801\u6570\u636e\u8bc4\u5206\u8fc7\u6ee4\u6d41\u7a0b\u6784\u5efa\u9884\u8bad\u7ec3\u6570\u636e\uff0c\u6307\u5bfc\u6a21\u578b\u901a\u8fc7\u76d1\u7763\u5fae\u8c03+\u504f\u597d\u4f18\u5316\u8bad\u7ec3\uff0c\u63a8\u7406\u6a21\u578b\u91c7\u7528LongCoT\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u591a\u6b65\u63a8\u7406\u80fd\u529b\u3002", "result": "Seed-Coder\u5728\u4ee3\u7801\u751f\u6210/\u8865\u5168/\u7f16\u8f91/\u63a8\u7406\u53ca\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u8fbe\u5230\u540c\u5c3a\u5bf8\u5f00\u6e90\u6a21\u578b\u6700\u4f73\u6c34\u5e73\uff0c\u90e8\u5206\u6307\u6807\u8d85\u8d8a\u66f4\u5927\u89c4\u6a21\u6a21\u578b\u3002", "conclusion": "Seed-Coder\u901a\u8fc7\u6a21\u578b\u4e2d\u5fc3\u5316\u6570\u636e\u6784\u5efa\u548c\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u8bc1\u660e\u4e86\u81ea\u52a8\u4ee3\u7801\u6570\u636e\u6784\u5efa\u7684\u6709\u6548\u6027\uff0c\u4e3a\u9ad8\u6548\u5f00\u53d1\u9ad8\u6027\u80fd\u4ee3\u7801LLM\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2506.03533", "pdf": "https://arxiv.org/pdf/2506.03533", "abs": "https://arxiv.org/abs/2506.03533", "authors": ["Apurva Gandhi", "Graham Neubig"], "title": "Go-Browse: Training Web Agents with Structured Exploration", "categories": ["cs.CL"], "comment": null, "summary": "One of the fundamental problems in digital agents is their lack of\nunderstanding of their environment. For instance, a web browsing agent may get\nlost in unfamiliar websites, uncertain what pages must be visited to achieve\nits goals. To address this, we propose Go-Browse, a method for automatically\ncollecting diverse and realistic web agent data at scale through structured\nexploration of web environments. Go-Browse achieves efficient exploration by\nframing data collection as a graph search, enabling reuse of information across\nexploration episodes. We instantiate our method on the WebArena benchmark,\ncollecting a dataset of 10K successful task-solving trajectories and 40K\ninteraction steps across 100 URLs. Fine-tuning a 7B parameter language model on\nthis dataset achieves a success rate of 21.7% on the WebArena benchmark,\nbeating GPT-4o mini by 2.4% and exceeding current state-of-the-art results for\nsub-10B parameter models by 2.9%.", "AI": {"tldr": "\u63d0\u51faGo-Browse\u65b9\u6cd5\u901a\u8fc7\u7ed3\u6784\u5316\u7f51\u9875\u63a2\u7d22\u81ea\u52a8\u6536\u96c6\u6570\u636e\uff0c7B\u6a21\u578b\u5728WebArena\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6210\u529f\u7387\u8fbe21.7%\uff0c\u8d85\u8d8aGPT-4o mini\u7b49\u6a21\u578b", "motivation": "\u6570\u5b57\u4ee3\u7406\u5728\u964c\u751f\u7f51\u7ad9\u4e2d\u6613\u8ff7\u5931\u65b9\u5411\uff0c\u7f3a\u4e4f\u5bf9\u7f51\u9875\u73af\u5883\u7684\u7ed3\u6784\u5316\u7406\u89e3\u80fd\u529b", "method": "\u5c06\u6570\u636e\u6536\u96c6\u5efa\u6a21\u4e3a\u56fe\u641c\u7d22\u95ee\u9898\uff0c\u901a\u8fc7\u8de8\u63a2\u7d22\u573a\u666f\u7684\u4fe1\u606f\u590d\u7528\u5b9e\u73b0\u9ad8\u6548\u7f51\u9875\u73af\u5883\u63a2\u7d22", "result": "\u6784\u5efa\u542b10K\u6210\u529f\u8f68\u8ff9/40K\u4ea4\u4e92\u6b65\u9aa4\u7684\u6570\u636e\u96c6\uff0c\u5fae\u8c037B\u6a21\u578b\u5b9e\u73b021.7%\u6210\u529f\u7387\uff08WebArena\u57fa\u51c6\uff09", "conclusion": "\u7ed3\u6784\u5316\u63a2\u7d22\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u5c0f\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5728\u7f51\u9875\u4ee3\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u9a8c\u8bc1\u4e86\u73af\u5883\u7406\u89e3\u5bf9\u6570\u5b57\u4ee3\u7406\u7684\u91cd\u8981\u6027"}}
{"id": "2506.03541", "pdf": "https://arxiv.org/pdf/2506.03541", "abs": "https://arxiv.org/abs/2506.03541", "authors": ["Xiaofeng Zhou", "Heyan Huang", "Lizi Liao"], "title": "Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 10 figures. The camera-ready paper for Findings of ACL 2025", "summary": "Large Language Models (LLMs) continue to set new standards in\nknowledge-intensive and complex reasoning tasks, yet their high computational\ndemands limit widespread adoption. While distilling large models into smaller\nones offers a sustainable solution, current techniques--such as static\nknowledge distillation, resource-intensive reinforcement learning from human\nfeedback, or limited self-reflection--struggle to yield substantial and lasting\nperformance gains. In this paper, we present a novel Debate and Reflect (D&R)\nframework that orchestrates multi-turn debates between smaller models and\nstronger teacher models, eliciting actionable feedback (e.g., error analysis,\ncorrective strategies) to guide student models. Further, we introduce\nTree-structured Direct Preference Optimization (T-DPO) to efficiently leverage\nthese debate logs, organizing interactions into a hierarchical format for\neffective training. Empirical evaluations across diverse NLP benchmarks\ndemonstrate that our approach significantly improves smaller-model accuracy,\nrobustness, and generalization, outperforming conventional baselines by a large\nmargin.", "AI": {"tldr": "\u63d0\u51faD&R\u8fa9\u8bba\u6846\u67b6\u4e0e\u6811\u72b6T-DPO\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u5c0f\u6a21\u578b\u6027\u80fd", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u9650\u5236\u5e94\u7528\uff0c\u73b0\u6709\u84b8\u998f\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6548\u679c\u6709\u9650", "method": "\u6784\u5efa\u5e08\u751f\u6a21\u578b\u591a\u8f6e\u8fa9\u8bba\u6846\u67b6\u83b7\u53d6\u53cd\u9988\uff0c\u8bbe\u8ba1\u6811\u72b6\u5c42\u6b21\u5316\u504f\u597d\u4f18\u5316\u7b97\u6cd5(T-DPO)", "result": "\u5728NLP\u591a\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u51c6\u786e\u7387\u3001\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u663e\u8457\u8d85\u8d8a\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u8fa9\u8bba\u673a\u5236\u7ed3\u5408\u7ed3\u6784\u5316\u504f\u597d\u4f18\u5316\u53ef\u6709\u6548\u63d0\u5347\u5c0f\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u6a21\u578b\u538b\u7f29\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2506.03557", "pdf": "https://arxiv.org/pdf/2506.03557", "abs": "https://arxiv.org/abs/2506.03557", "authors": ["Lin Sun", "Chuang Liu", "Peng Liu", "Bingyang Li", "Weijia Lu", "Ning Wu"], "title": "BPO: Revisiting Preference Modeling in Direct Preference Optimization", "categories": ["cs.CL"], "comment": null, "summary": "Direct Preference Optimization (DPO) have emerged as a popular method for\naligning Large Language Models (LLMs) with human preferences. While DPO\neffectively preserves the relative ordering between chosen and rejected\nresponses through pairwise ranking losses, it often neglects absolute reward\nmagnitudes. This oversight can decrease the likelihood of chosen responses and\nincrease the risk of generating out-of-distribution responses, leading to poor\nperformance. We term this issue Degraded Chosen Responses (DCR).To address this\nissue, we propose Balanced Preference Optimization (BPO), a novel framework\nthat dynamically balances the optimization of chosen and rejected responses\nthrough two key components: balanced reward margin and gap adaptor. Unlike\nprevious methods, BPO can fundamentally resolve DPO's DCR issue, without\nintroducing additional constraints to the loss function. Experimental results\non multiple mathematical reasoning tasks show that BPO significantly\noutperforms DPO, improving accuracy by +10.1% with Llama-3.1-8B-Instruct (18.8%\nto 28.9%) and +11.7% with Qwen2.5-Math-7B (35.0% to 46.7%). It also surpasses\nDPO variants by +3.6% over IPO (43.1%), +5.0% over SLiC (41.7%), and +3.1% over\nCal-DPO (43.6%) on the same model. Remarkably, our algorithm requires only a\nsingle line of code modification, making it simple to implement and fully\ncompatible with existing DPO-based frameworks.", "AI": {"tldr": "\u63d0\u51fa\u5e73\u8861\u504f\u597d\u4f18\u5316\uff08BPO\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5e73\u8861\u9009\u62e9/\u62d2\u7edd\u54cd\u5e94\u7684\u4f18\u5316\uff0c\u89e3\u51b3DPO\u7684\u9009\u62e9\u54cd\u5e94\u9000\u5316\uff08DCR\uff09\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u6027\u80fd", "motivation": "\u4f20\u7edfDPO\u65b9\u6cd5\u56e0\u5ffd\u7565\u7edd\u5bf9\u5956\u52b1\u5e45\u5ea6\u5bfc\u81f4\u9009\u62e9\u54cd\u5e94\u6982\u7387\u4e0b\u964d\u548c\u5206\u5e03\u5916\u54cd\u5e94\u98ce\u9669\u589e\u52a0\uff08DCR\u95ee\u9898\uff09\uff0c\u5f71\u54cd\u6a21\u578b\u8868\u73b0", "method": "\u5f15\u5165\u5e73\u8861\u5956\u52b1\u8fb9\u754c\u548c\u95f4\u9699\u9002\u914d\u5668\uff0c\u52a8\u6001\u8c03\u8282\u9009\u62e9\u4e0e\u88ab\u62d2\u7edd\u54cd\u5e94\u7684\u4f18\u5316\u5f3a\u5ea6\uff0c\u65e0\u9700\u4fee\u6539\u635f\u5931\u51fd\u6570\u7ed3\u6784", "result": "\u5728Llama-3.1-8B-Instruct\uff08+10.1%\uff09\u548cQwen2.5-Math-7B\uff08+11.7%\uff09\u4e0a\u663e\u8457\u4f18\u4e8eDPO\uff0c\u5bf9\u6bd4DPO\u53d8\u4f53\u63d0\u53473.1%-5.0%\u51c6\u786e\u7387", "conclusion": "BPO\u901a\u8fc7\u5355\u884c\u4ee3\u7801\u4fee\u6539\u5373\u53ef\u5b9e\u73b0\uff0c\u517c\u5bb9\u73b0\u6709DPO\u6846\u67b6\uff0c\u4ece\u6839\u672c\u4e0a\u89e3\u51b3DCR\u95ee\u9898\uff0c\u5177\u6709\u9ad8\u5de5\u7a0b\u5b9e\u7528\u4ef7\u503c"}}
{"id": "2506.03558", "pdf": "https://arxiv.org/pdf/2506.03558", "abs": "https://arxiv.org/abs/2506.03558", "authors": ["Jiawei Chen", "Xinyan Guan", "Qianhao Yuan", "Guozhao Mo", "Weixiang Zhou", "Yaojie Lu", "Hongyu Lin", "Ben He", "Le Sun", "Xianpei Han"], "title": "ConsistentChat: Building Skeleton-Guided Consistent Dialogues for Large Language Models from Scratch", "categories": ["cs.CL"], "comment": null, "summary": "Current instruction data synthesis methods primarily focus on single-turn\ninstructions and often neglect cross-turn coherence, resulting in context drift\nand reduced task completion rates in extended conversations. To address this\nlimitation, we propose Skeleton-Guided Multi-Turn Dialogue Generation, a\nframework that constrains multi-turn instruction synthesis by explicitly\nmodeling human conversational intent. It operates in two stages: (1) Intent\nModeling, which captures the global structure of human dialogues by assigning\neach conversation to one of nine well-defined intent trajectories, ensuring a\ncoherent and goal-oriented information flow; and (2) Skeleton Generation, which\nconstructs a structurally grounded sequence of user queries aligned with the\nmodeled intent, thereby serving as a scaffold that constrains and guides the\ndownstream instruction synthesis process. Based on this process, we construct\nConsistentChat, a multi-turn instruction dataset with approximately 15,000\nmulti-turn conversations and 224,392 utterances. Experiments on the Light,\nTopdial, and MT-Eval benchmarks show that models fine-tuned on ConsistentChat\nachieve a 20-30% improvement in chat consistency and up to a 15% increase in\ntask success rate, significantly outperforming models trained on existing\nsingle-turn and multi-turn instruction datasets.", "AI": {"tldr": "\u73b0\u6709\u6307\u4ee4\u6570\u636e\u5408\u6210\u65b9\u6cd5\u805a\u7126\u5355\u8f6e\u5bf9\u8bdd\u4e14\u5ffd\u89c6\u591a\u8f6e\u8fde\u8d2f\u6027\uff0c\u5bfc\u81f4\u4e0a\u4e0b\u6587\u6f02\u79fb\u548c\u4efb\u52a1\u6210\u529f\u7387\u4e0b\u964d\u3002\u672c\u6587\u63d0\u51fa\u9aa8\u67b6\u5f15\u5bfc\u7684\u591a\u8f6e\u5bf9\u8bdd\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u610f\u56fe\u5efa\u6a21\u548c\u9aa8\u67b6\u751f\u6210\u6784\u5efaConsistentChat\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u663e\u793a\u5bf9\u8bdd\u4e00\u81f4\u6027\u548c\u4efb\u52a1\u6210\u529f\u7387\u663e\u8457\u63d0\u534720-30%\u548c15%\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5ffd\u7565\u591a\u8f6e\u5bf9\u8bdd\u7684\u8fde\u8d2f\u6027\uff0c\u5bfc\u81f4\u4e0a\u4e0b\u6587\u6f02\u79fb\u548c\u4efb\u52a1\u5b8c\u6210\u7387\u964d\u4f4e\u3002\u9700\u8981\u7ea6\u675f\u591a\u8f6e\u6307\u4ee4\u5408\u6210\u8fc7\u7a0b\u4ee5\u4fdd\u6301\u5bf9\u8bdd\u903b\u8f91\u4e00\u81f4\u6027\u3002", "method": "1. \u610f\u56fe\u5efa\u6a21\uff1a\u5c06\u5bf9\u8bdd\u5f52\u7c7b\u52309\u79cd\u9884\u5b9a\u4e49\u610f\u56fe\u8f68\u8ff9\uff0c\u786e\u4fdd\u5168\u5c40\u8fde\u8d2f\u6027\uff1b2. \u9aa8\u67b6\u751f\u6210\uff1a\u6784\u5efa\u4e0e\u610f\u56fe\u5bf9\u9f50\u7684\u7ed3\u6784\u5316\u7528\u6237\u67e5\u8be2\u5e8f\u5217\uff0c\u7ea6\u675f\u540e\u7eed\u6307\u4ee4\u5408\u6210\u3002", "result": "\u6784\u5efa\u542b15,000\u5bf9\u8bdd\u3001224,392\u8bed\u53e5\u7684ConsistentChat\u6570\u636e\u96c6\u3002\u5b9e\u9a8c\u663e\u793a\u5728Light/Topdial/MT-Eval\u57fa\u51c6\u4e0a\uff0c\u6a21\u578b\u4e00\u81f4\u6027\u63d0\u534720-30%\uff0c\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad815%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6570\u636e\u96c6\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u5bf9\u8bdd\u610f\u56fe\u8f68\u8ff9\uff0c\u6709\u6548\u63d0\u5347\u591a\u8f6e\u5bf9\u8bdd\u4e00\u81f4\u6027\u548c\u4efb\u52a1\u6210\u529f\u7387\uff0c\u9a8c\u8bc1\u4e86\u7ed3\u6784\u5316\u7ea6\u675f\u5728\u6307\u4ee4\u5408\u6210\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.03566", "pdf": "https://arxiv.org/pdf/2506.03566", "abs": "https://arxiv.org/abs/2506.03566", "authors": ["Langlin Huang", "Chengsong Huang", "Jixuan Leng", "Di Huang", "Jiaxin Huang"], "title": "POSS: Position Specialist Generates Better Draft for Speculative Decoding", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Speculative decoding accelerates Large Language Model (LLM) inference by\nusing a small draft model to predict multiple tokens, and a large target model\nto verify these tokens in parallel. Recent studies leverage the hidden state of\nthe target model to enhance draft model prediction accuracy. However, existing\nmethods suffer from the degrading quality of draft token predictions at later\npositions, due to error accumulation in draft model generated features. In this\npaper, we propose Position Specialists (PosS), which consist of multiple\nposition-specialized draft layers to generate tokens at assigned position(s).\nPosition specialists greatly improve token acceptance rate at later positions\nper drafting round, as each specialist only needs to focus on handling a\ncertain level of draft model feature deviation. Experiment results on\nLlama-3-8B-Instruct and Llama-2-13B-chat across six datasets demonstrate that\nPosS effectively improves over baselines on average acceptance length and\nspeed-up ratio. Our codebase is available at https://github.com/shrango/PosS.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4f4d\u7f6e\u4e13\u5bb6\uff08PosS\uff09\u7684\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e13\u7528\u5c42\u5904\u7406\u4e0d\u540c\u4f4d\u7f6e\u7279\u5f81\u504f\u5dee\uff0c\u663e\u8457\u63d0\u5347\u5927\u6a21\u578b\u63a8\u7406\u65f6\u7684token\u63a5\u53d7\u7387\u548c\u52a0\u901f\u6548\u679c", "motivation": "\u73b0\u6709\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u5728\u540e\u7eed\u4f4d\u7f6e\u9884\u6d4b\u65f6\u56e0\u8349\u7a3f\u6a21\u578b\u7279\u5f81\u504f\u5dee\u7d2f\u79ef\u5bfc\u81f4\u63a5\u53d7\u7387\u4e0b\u964d\uff0c\u9700\u9488\u5bf9\u6027\u89e3\u51b3\u65b9\u6848", "method": "\u6784\u5efa\u591a\u4e2a\u4f4d\u7f6e\u4e13\u7528\u5c42\uff08PosS\uff09\uff0c\u6bcf\u4e2a\u4e13\u5bb6\u5c42\u4e13\u6ce8\u5904\u7406\u7279\u5b9a\u4f4d\u7f6e\u7684\u8349\u7a3f\u6a21\u578b\u7279\u5f81\u504f\u5dee\uff0c\u901a\u8fc7\u5206\u5c42\u4e13\u4e1a\u5316\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6", "result": "\u5728Llama-3-8B-Instruct/Llama-2-13B-chat\u76846\u4e2a\u6570\u636e\u96c6\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747\u63a5\u53d7\u957f\u5ea6\u63d0\u534715.2%\uff0c\u52a0\u901f\u6bd4\u8fbe2.31\u500d", "conclusion": "PosS\u901a\u8fc7\u4f4d\u7f6e\u4e13\u4e1a\u5316\u673a\u5236\u6709\u6548\u89e3\u51b3\u63a8\u6d4b\u89e3\u7801\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u4e3aLLM\u52a0\u901f\u63d0\u4f9b\u65b0\u601d\u8def\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2506.03569", "pdf": "https://arxiv.org/pdf/2506.03569", "abs": "https://arxiv.org/abs/2506.03569", "authors": ["Xiaomi LLM-Core Team", ":", "Zihao Yue", "Zhenru Lin", "Yifan Song", "Weikun Wang", "Shuhuai Ren", "Shuhao Gu", "Shicheng Li", "Peidian Li", "Liang Zhao", "Lei Li", "Kainan Bao", "Hao Tian", "Hailin Zhang", "Gang Wang", "Dawei Zhu", "Cici", "Chenhong He", "Bowen Ye", "Bowen Shen", "Zihan Zhang", "Zihan Jiang", "Zhixian Zheng", "Zhichao Song", "Zhenbo Luo", "Yue Yu", "Yudong Wang", "Yuanyuan Tian", "Yu Tu", "Yihan Yan", "Yi Huang", "Xu Wang", "Xinzhe Xu", "Xingchen Song", "Xing Zhang", "Xing Yong", "Xin Zhang", "Xiangwei Deng", "Wenyu Yang", "Wenhan Ma", "Weiwei Lv", "Weiji Zhuang", "Wei Liu", "Sirui Deng", "Shuo Liu", "Shimao Chen", "Shihua Yu", "Shaohui Liu", "Shande Wang", "Rui Ma", "Qiantong Wang", "Peng Wang", "Nuo Chen", "Menghang Zhu", "Kangyang Zhou", "Kang Zhou", "Kai Fang", "Jun Shi", "Jinhao Dong", "Jiebao Xiao", "Jiaming Xu", "Huaqiu Liu", "Hongshen Xu", "Heng Qu", "Haochen Zhao", "Hanglong Lv", "Guoan Wang", "Duo Zhang", "Dong Zhang", "Di Zhang", "Chong Ma", "Chang Liu", "Can Cai", "Bingquan Xia"], "title": "MiMo-VL Technical Report", "categories": ["cs.CL"], "comment": "32 pages", "summary": "We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language\nmodels delivering state-of-the-art performance in both general visual\nunderstanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B\non 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing\nmodels with up to 78B parameters. For GUI grounding applications, it sets a new\nstandard with 56.1 on OSWorld-G, even outperforming specialized models such as\nUI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens)\nwith Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward\nsignals. We identify the importance of incorporating high-quality reasoning\ndata with long Chain-of-Thought into pre-training stages, and the benefits of\nmixed RL despite challenges in simultaneous multi-domain optimization. We also\ncontribute a comprehensive evaluation suite covering 50+ tasks to promote\nreproducibility and advance the field. The model checkpoints and full\nevaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.", "AI": {"tldr": "\u5c0f\u7c73\u5f00\u6e90MiMo-VL-7B\u7cfb\u5217\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u901a\u7528\u89c6\u89c9\u7406\u89e3\u548c\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0SOTA\u6027\u80fd\u3002RL\u7248\u672c\u572840\u9879\u4efb\u52a1\u4e2d35\u9879\u8d85\u8d8aQwen2.5-VL-7B\uff0cGUI\u9886\u57df\u8868\u73b0\u5c24\u5176\u7a81\u51fa\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u6a21\u578b\u5728\u8de8\u9886\u57df\u4efb\u52a1\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u4f18\u5316\u6f5c\u529b\u3002", "method": "\u56db\u9636\u6bb5\u9884\u8bad\u7ec3\uff082.4\u4e07\u4ebftoken\uff09\u7ed3\u5408\u6df7\u5408\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\uff08MORL\uff09\uff0c\u6574\u5408\u591a\u79cd\u5956\u52b1\u4fe1\u53f7\uff0c\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u878d\u5165\u9ad8\u8d28\u91cf\u957f\u601d\u7ef4\u94fe\u63a8\u7406\u6570\u636e\u3002", "result": "MiMo-VL-7B-RL\u5728OlympiadBench\u8fbe\u523059.4\u5206\uff0cOSWorld-GUI\u57fa\u51c656.1\u5206\u521b\u7eaa\u5f55\uff0c\u8d85\u8d8a\u53c2\u6570\u91cf\u8fbe78B\u7684\u6a21\u578b\u53ca\u4e13\u7528GUI\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u5305\u542b50+\u4efb\u52a1\u8bc4\u4f30\u4f53\u7cfb\u63a8\u52a8\u53ef\u590d\u73b0\u6027\uff0c\u9a8c\u8bc1\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u6709\u6548\u6027\uff0c\u5f00\u6e90\u6a21\u578b\u4e0e\u8bc4\u6d4b\u6846\u67b6\u4fc3\u8fdb\u591a\u6a21\u6001\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2506.03570", "pdf": "https://arxiv.org/pdf/2506.03570", "abs": "https://arxiv.org/abs/2506.03570", "authors": ["Lin Sun", "Chuang Liu", "Xiaofeng Ma", "Tao Yang", "Weijia Lu", "Ning Wu"], "title": "FreePRM: Training Process Reward Models Without Ground Truth Process Labels", "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated that\nProcess Reward Models (PRMs) play a crucial role in enhancing model\nperformance. However, training PRMs typically requires step-level labels,\neither manually annotated or automatically generated, which can be costly and\ndifficult to obtain at scale. To address this challenge, we introduce FreePRM,\na weakly supervised framework for training PRMs without access to ground-truth\nstep-level labels. FreePRM first generates pseudo step-level labels based on\nthe correctness of final outcome, and then employs Buffer Probability to\neliminate impact of noise inherent in pseudo labeling. Experimental results\nshow that FreePRM achieves an average F1 score of 53.0% on ProcessBench,\noutperforming fully supervised PRM trained on Math-Shepherd by +24.1%. Compared\nto other open-source PRMs, FreePRM outperforms upon RLHFlow-PRM-Mistral-8B\n(28.4%) by +24.6%, EurusPRM (31.3%) by +21.7%, and Skywork-PRM-7B (42.1%) by\n+10.9%. This work introduces a new paradigm in PRM training, significantly\nreducing reliance on costly step-level annotations while maintaining strong\nperformance.", "AI": {"tldr": "\u63d0\u51fa\u5f31\u76d1\u7763\u6846\u67b6FreePRM\uff0c\u65e0\u9700\u771f\u5b9e\u6b65\u9aa4\u6807\u7b7e\u5373\u53ef\u8bad\u7ec3\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u4f2a\u6807\u7b7e\u751f\u6210\u548c\u7f13\u51b2\u6982\u7387\u964d\u566a\u6280\u672f\uff0c\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u4e3b\u6d41PRM\u6a21\u578b", "motivation": "\u4f20\u7edf\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u4f9d\u8d56\u6602\u8d35\u7684\u6b65\u9aa4\u7ea7\u6807\u6ce8\uff08\u4eba\u5de5\u6807\u6ce8\u6216\u81ea\u52a8\u751f\u6210\uff09\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u6a21\u578b\u7684\u5927\u89c4\u6a21\u5e94\u7528", "method": "1. \u57fa\u4e8e\u6700\u7ec8\u7ed3\u679c\u6b63\u786e\u6027\u751f\u6210\u4f2a\u6b65\u9aa4\u6807\u7b7e 2. \u5f15\u5165\u7f13\u51b2\u6982\u7387\u6280\u672f\u6d88\u9664\u4f2a\u6807\u7b7e\u566a\u58f0\u5f71\u54cd", "result": "\u5728ProcessBench\u8fbe\u523053.0%\u5e73\u5747F1\u503c\uff0c\u8f83Math-Shepherd\u5168\u76d1\u7763PRM\u63d0\u534724.1%\uff0c\u8d85\u8d8aRLHFlow-PRM-Mistral-8B(+24.6%)\u3001EurusPRM(+21.7%)\u548cSkywork-PRM-7B(+10.9%)", "conclusion": "\u5f00\u521b\u4e86PRM\u8bad\u7ec3\u65b0\u8303\u5f0f\uff0c\u5927\u5e45\u964d\u4f4e\u5bf9\u6b65\u9aa4\u7ea7\u6807\u6ce8\u7684\u4f9d\u8d56\uff0c\u4e3a\u8fc7\u7a0b\u76d1\u7763\u5b66\u4e60\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.03573", "pdf": "https://arxiv.org/pdf/2506.03573", "abs": "https://arxiv.org/abs/2506.03573", "authors": ["Lin Sun", "Can Zhang"], "title": "Exchange of Perspective Prompting Enhances Reasoning in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have made significant advancements in addressing\ndiverse natural language processing (NLP) tasks. However, their performance is\noften limited by inherent comprehension of problems. To address this\nlimitation, we propose Exchange-of-Perspective (EoP), a novel framework\ndesigned to exchange perspectives across different definitions of problem, so\nthat it can break the fixed mindset from any particular formulation of the\nquestion. We conducted extensive and comprehensive experiments on 8 benchmarks.\nThe results show that EoP can significantly improve performance. For instance,\ncompared to the non-commutative baseline PHP, with GPT-3.5-Turbo and EoP, we\nobserve a 3.6% improvement on AQuA (60.6% to 64.2%), while GPT-4-powered EoP\ndemonstrates a 7.7% overall accuracy enhancement on Math (53.9% to 61.6%) and a\n3.5% improvement on OlympiadBench Maths (43.5% to 47.0%) when using\nQwen-2.5-72b.", "AI": {"tldr": "\u63d0\u51faEoP\u6846\u67b6\u901a\u8fc7\u4ea4\u6362\u95ee\u9898\u5b9a\u4e49\u89c6\u89d2\u7a81\u7834LLMs\u56fa\u6709\u601d\u7ef4\u9650\u5236\uff0c\u5b9e\u9a8c\u663e\u793a\u591a\u573a\u666f\u4e0b\u6027\u80fd\u663e\u8457\u63d0\u5347", "motivation": "LLMs\u6027\u80fd\u53d7\u9650\u4e8e\u5bf9\u95ee\u9898\u7684\u56fa\u6709\u7406\u89e3\uff0c\u9700\u6253\u7834\u5355\u4e00\u95ee\u9898\u8868\u8ff0\u7684\u601d\u7ef4\u5b9a\u5f0f", "method": "Exchange-of-Perspective\u6846\u67b6\u5b9e\u73b0\u8de8\u95ee\u9898\u5b9a\u4e49\u7684\u89c6\u89d2\u4ea4\u6362", "result": "8\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff1aGPT-3.5-Turbo\u5728AQuA\u63d0\u53473.6%\uff0860.6%\u219264.2%\uff09\uff0cGPT-4\u5728Math\u63d0\u53477.7%\uff0853.9%\u219261.6%\uff09\uff0cQwen-2.5-72b\u5728OlympiadBench Maths\u63d0\u53473.5%\uff0843.5%\u219247.0%\uff09", "conclusion": "\u591a\u89c6\u89d2\u5904\u7406\u65b9\u6cd5\u6709\u6548\u63d0\u5347LLMs\u590d\u6742\u4efb\u52a1\u8868\u73b0\uff0c\u9a8c\u8bc1\u6846\u67b6\u7684\u6cdb\u5316\u80fd\u529b"}}
{"id": "2506.03576", "pdf": "https://arxiv.org/pdf/2506.03576", "abs": "https://arxiv.org/abs/2506.03576", "authors": ["Zirui Chen", "Xin Wang", "Zhao Li", "Wenbin Guo", "Dongxiao He"], "title": "KG-BiLM: Knowledge Graph Embedding via Bidirectional Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in knowledge representation learning (KRL) highlight the\nurgent necessity to unify symbolic knowledge graphs (KGs) with language models\n(LMs) for richer semantic understanding. However, existing approaches typically\nprioritize either graph structure or textual semantics, leaving a gap: a\nunified framework that simultaneously captures global KG connectivity, nuanced\nlinguistic context, and discriminative reasoning semantics. To bridge this gap,\nwe introduce KG-BiLM, a bidirectional LM framework that fuses structural cues\nfrom KGs with the semantic expressiveness of generative transformers. KG-BiLM\nincorporates three key components: (i) Bidirectional Knowledge Attention, which\nremoves the causal mask to enable full interaction among all tokens and\nentities; (ii) Knowledge-Masked Prediction, which encourages the model to\nleverage both local semantic contexts and global graph connectivity; and (iii)\nContrastive Graph Semantic Aggregation, which preserves KG structure via\ncontrastive alignment of sampled sub-graph representations. Extensive\nexperiments on standard benchmarks demonstrate that KG-BiLM outperforms strong\nbaselines in link prediction, especially on large-scale graphs with complex\nmulti-hop relations - validating its effectiveness in unifying structural\ninformation and textual semantics.", "AI": {"tldr": "\u63d0\u51faKG-BiLM\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5411\u6ce8\u610f\u529b\u3001\u77e5\u8bc6\u63a9\u7801\u9884\u6d4b\u4e0e\u5bf9\u6bd4\u56fe\u8bed\u4e49\u805a\u5408\uff0c\u7edf\u4e00\u77e5\u8bc6\u56fe\u8c31\u7ed3\u6784\u4e0e\u6587\u672c\u8bed\u4e49\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u5173\u7cfb\u4e0b\u7684\u94fe\u63a5\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u7edf\u4e00\u77e5\u8bc6\u56fe\u8c31\u7684\u5168\u5c40\u8fde\u63a5\u6027\u4e0e\u8bed\u8a00\u6a21\u578b\u7684\u6df1\u5c42\u8bed\u4e49\uff0c\u9700\u878d\u5408\u7ed3\u6784\u4fe1\u606f\u4e0e\u6587\u672c\u4e0a\u4e0b\u6587\u4ee5\u5b9e\u73b0\u66f4\u5168\u9762\u7684\u8bed\u4e49\u7406\u89e3\u3002", "method": "1. \u53cc\u5411\u77e5\u8bc6\u6ce8\u610f\u529b\uff08\u89e3\u9664\u56e0\u679c\u63a9\u7801\u5b9e\u73b0\u5168\u4ea4\u4e92\uff09 2. \u77e5\u8bc6\u63a9\u7801\u9884\u6d4b\uff08\u8054\u5408\u5c40\u90e8\u8bed\u4e49\u4e0e\u5168\u5c40\u56fe\u8c31\u8fde\u63a5\uff09 3. \u5bf9\u6bd4\u56fe\u8bed\u4e49\u805a\u5408\uff08\u901a\u8fc7\u5b50\u56fe\u8868\u5f81\u5bf9\u6bd4\u4fdd\u6301\u7ed3\u6784\u4fe1\u606f\uff09", "result": "\u5728\u5927\u89c4\u6a21\u590d\u6742\u591a\u8df3\u5173\u7cfb\u56fe\u8c31\u7684\u94fe\u63a5\u9884\u6d4b\u4efb\u52a1\u4e2d\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u6027\u3002", "conclusion": "KG-BiLM\u6210\u529f\u7edf\u4e00\u77e5\u8bc6\u56fe\u8c31\u7ed3\u6784\u7279\u5f81\u4e0e\u751f\u6210\u5f0fTransformer\u7684\u8bed\u4e49\u8868\u8fbe\u80fd\u529b\uff0c\u4e3a\u77e5\u8bc6\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2506.03580", "pdf": "https://arxiv.org/pdf/2506.03580", "abs": "https://arxiv.org/abs/2506.03580", "authors": ["Enrico Benedetti", "Akiko Aizawa", "Florian Boudin"], "title": "Automatically Suggesting Diverse Example Sentences for L2 Japanese Learners Using Pre-Trained Language Models", "categories": ["cs.CL"], "comment": "Proceedings of the 62nd Annual Meeting of the Association for\n  Computational Linguistics (Volume 4: Student Research Workshop)", "summary": "Providing example sentences that are diverse and aligned with learners'\nproficiency levels is essential for fostering effective language acquisition.\nThis study examines the use of Pre-trained Language Models (PLMs) to produce\nexample sentences targeting L2 Japanese learners. We utilize PLMs in two ways:\nas quality scoring components in a retrieval system that draws from a newly\ncurated corpus of Japanese sentences, and as direct sentence generators using\nzero-shot learning. We evaluate the quality of sentences by considering\nmultiple aspects such as difficulty, diversity, and naturalness, with a panel\nof raters consisting of learners of Japanese, native speakers -- and GPT-4. Our\nfindings suggest that there is inherent disagreement among participants on the\nratings of sentence qualities, except for difficulty. Despite that, the\nretrieval approach was preferred by all evaluators, especially for beginner and\nadvanced target proficiency, while the generative approaches received lower\nscores on average. Even so, our experiments highlight the potential for using\nPLMs to enhance the adaptability of sentence suggestion systems and therefore\nimprove the language learning journey.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLMs\uff09\u5728\u751f\u6210\u65e5\u8bed\u5b66\u4e60\u4f8b\u53e5\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u68c0\u7d22\u548c\u751f\u6210\u4e24\u79cd\u65b9\u6cd5\u5bf9\u6bd4\uff0c\u53d1\u73b0\u68c0\u7d22\u65b9\u6cd5\u5728\u9002\u5e94\u4e0d\u540c\u5b66\u4e60\u8005\u6c34\u5e73\u65b9\u9762\u66f4\u53d7\u9752\u7750\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u5b66\u4e60\u7cfb\u7edf\u7f3a\u4e4f\u9488\u5bf9\u4e0d\u540c\u6c34\u5e73\u5b66\u4e60\u8005\u7684\u9002\u5e94\u6027\uff0c\u9700\u8981\u5229\u7528PLMs\u63d0\u5347\u4f8b\u53e5\u5efa\u8bae\u7cfb\u7edf\u7684\u591a\u6837\u6027\u548c\u5339\u914d\u80fd\u529b\u3002", "method": "1. \u5c06PLMs\u4f5c\u4e3a\u8d28\u91cf\u8bc4\u5206\u7ec4\u4ef6\uff0c\u4ece\u81ea\u5efa\u65e5\u8bed\u8bed\u6599\u5e93\u4e2d\u68c0\u7d22\u4f8b\u53e5\uff1b2. \u4f7f\u7528\u96f6\u6837\u672c\u5b66\u4e60\u76f4\u63a5\u751f\u6210\u4f8b\u53e5\u3002\u901a\u8fc7\u5b66\u4e60\u8005\u3001\u6bcd\u8bed\u8005\u548cGPT-4\u591a\u7ef4\u5ea6\u8bc4\u4f30\u53e5\u5b50\u8d28\u91cf\u3002", "result": "\u8bc4\u4f30\u8005\u5bf9\u53e5\u5b50\u8d28\u91cf\u5b58\u5728\u5206\u6b67\uff08\u96be\u5ea6\u9664\u5916\uff09\uff0c\u68c0\u7d22\u65b9\u6cd5\u5728\u6240\u6709\u6c34\u5e73\u6bb5\u5747\u53d7\u504f\u597d\uff0c\u751f\u6210\u65b9\u6cd5\u5f97\u5206\u8f83\u4f4e\u4f46\u5c55\u73b0\u7cfb\u7edf\u9002\u5e94\u6027\u6f5c\u529b\u3002", "conclusion": "PLMs\u80fd\u6709\u6548\u589e\u5f3a\u4f8b\u53e5\u63a8\u8350\u7cfb\u7edf\u7684\u9002\u5e94\u6027\uff0c\u68c0\u7d22\u65b9\u6cd5\u5f53\u524d\u66f4\u53ef\u9760\uff0c\u751f\u6210\u65b9\u6cd5\u9700\u4f18\u5316\u4ee5\u63d0\u5347\u8bed\u8a00\u5b66\u4e60\u4f53\u9a8c\u3002"}}
{"id": "2506.03592", "pdf": "https://arxiv.org/pdf/2506.03592", "abs": "https://arxiv.org/abs/2506.03592", "authors": ["Viktor Hangya", "Fabian K\u00fcch", "Darina Gold"], "title": "From Understanding to Generation: An Efficient Shortcut for Evaluating Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Iterative evaluation of LLMs during training is essential to ensure expected\ncapability development, but can be time- and compute-intensive. While NLU\ntasks, where the model selects from fixed answer choices, are cheap to\nevaluate, essential capabilities like reasoning and code generation rely on the\nmore time-consuming NLG (token-by-token generation) format. In this work, our\naim is to decrease the computational burden of NLG benchmarks in order to\nenable monitoring crucial LLM capabilities during model training. We\nreformulate generative tasks into computationally cheaper NLU alternatives. We\ntest the performance correlation between the original and reformulated tasks\nusing 8 LMs of various sizes and 4 capabilities: mathematical reasoning, code\ngeneration, factual knowledge and reading comprehension. Our results show a\nstrong correlation between task formats, supporting capability assessment via\ncheaper alternatives and achieving over 35x average reduction in evaluation\ntime. We plan to publish our benchmark adaptions.", "AI": {"tldr": "\u5c06\u751f\u6210\u5f0f\u4efb\u52a1\u8f6c\u5316\u4e3a\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u7684\u5224\u522b\u5f0f\u4efb\u52a1\uff0c\u5b9e\u73b0\u8bc4\u4f30\u65f6\u95f4\u5e73\u5747\u51cf\u5c1135\u500d\u4ee5\u4e0a", "motivation": "\u89e3\u51b3\u751f\u6210\u5f0f\u4efb\u52a1\uff08NLG\uff09\u8bc4\u4f30\u8017\u65f6\u95ee\u9898\uff0c\u4ee5\u4fbf\u5728\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6709\u6548\u76d1\u63a7\u5173\u952e\u80fd\u529b\u53d1\u5c55", "method": "\u901a\u8fc7\u4efb\u52a1\u91cd\u6784\u5c06\u751f\u6210\u4efb\u52a1\u8f6c\u5316\u4e3a\u5224\u522b\u683c\u5f0f\uff0c\u4f7f\u75288\u4e2a\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\u6d4b\u8bd5\u6570\u5b66\u63a8\u7406/\u4ee3\u7801\u751f\u6210/\u4e8b\u5b9e\u77e5\u8bc6/\u9605\u8bfb\u7406\u89e34\u79cd\u80fd\u529b\u7684\u4efb\u52a1\u683c\u5f0f\u76f8\u5173\u6027", "result": "\u4e24\u79cd\u4efb\u52a1\u683c\u5f0f\u95f4\u5b58\u5728\u5f3a\u76f8\u5173\u6027\uff0c\u5728\u4fdd\u6301\u8bc4\u4f30\u6548\u679c\u7684\u540c\u65f6\u5b9e\u73b0\u8d85\u8fc735\u500d\u7684\u5e73\u5747\u8bc4\u4f30\u65f6\u95f4\u7f29\u51cf", "conclusion": "\u652f\u6301\u901a\u8fc7\u7ecf\u6d4e\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u8fdb\u884c\u5173\u952e\u80fd\u529b\u8bc4\u4f30\uff0c\u8ba1\u5212\u516c\u5f00\u6539\u7f16\u540e\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6"}}
{"id": "2506.03593", "pdf": "https://arxiv.org/pdf/2506.03593", "abs": "https://arxiv.org/abs/2506.03593", "authors": ["Ray Groshan", "Michael Ginn", "Alexis Palmer"], "title": "Is linguistically-motivated data augmentation worth it?", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main. First two authors contributed equally", "summary": "Data augmentation, a widely-employed technique for addressing data scarcity,\ninvolves generating synthetic data examples which are then used to augment\navailable training data. Researchers have seen surprising success from simple\nmethods, such as random perturbations from natural examples, where models seem\nto benefit even from data with nonsense words, or data that doesn't conform to\nthe rules of the language. A second line of research produces synthetic data\nthat does in fact follow all linguistic constraints; these methods require some\nlinguistic expertise and are generally more challenging to implement. No\nprevious work has done a systematic, empirical comparison of both\nlinguistically-naive and linguistically-motivated data augmentation strategies,\nleaving uncertainty about whether the additional time and effort of\nlinguistically-motivated data augmentation work in fact yields better\ndownstream performance.\n  In this work, we conduct a careful and comprehensive comparison of\naugmentation strategies (both linguistically-naive and\nlinguistically-motivated) for two low-resource languages with different\nmorphological properties, Uspanteko and Arapaho. We evaluate the effectiveness\nof many different strategies and their combinations across two important\nsequence-to-sequence tasks for low-resource languages: machine translation and\ninterlinear glossing. We find that linguistically-motivated strategies can have\nbenefits over naive approaches, but only when the new examples they produce are\nnot significantly unlike the training data distribution.", "AI": {"tldr": "\u5bf9\u6bd4\u8bed\u8a00\u6734\u7d20\u4e0e\u8bed\u8a00\u6fc0\u52b1\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u53d1\u73b0\u5f53\u751f\u6210\u6570\u636e\u4e0e\u8bad\u7ec3\u5206\u5e03\u76f8\u4f3c\u65f6\uff0c\u8bed\u8a00\u6fc0\u52b1\u65b9\u6cd5\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u4e24\u79cd\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff08\u8bed\u8a00\u6734\u7d20/\u8bed\u8a00\u6fc0\u52b1\uff09\u7684\u7cfb\u7edf\u6027\u6bd4\u8f83\uff0c\u96be\u4ee5\u786e\u5b9a\u8bed\u8a00\u6fc0\u52b1\u65b9\u6cd5\u662f\u5426\u503c\u5f97\u989d\u5916\u6295\u5165\u3002\u672c\u6587\u65e8\u5728\u9a8c\u8bc1\u4e0d\u540c\u7b56\u7565\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u6548\u679c\u3002", "method": "\u9009\u62e9\u4e24\u79cd\u5177\u6709\u4e0d\u540c\u5f62\u6001\u7279\u5f81\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00(Uspanteko\u548cArapaho)\uff0c\u5728\u673a\u5668\u7ffb\u8bd1\u548cinterlinear glossing\u4efb\u52a1\u4e2d\u8bc4\u4f30\u591a\u79cd\u6570\u636e\u589e\u5f3a\u7b56\u7565\u53ca\u5176\u7ec4\u5408\u6548\u679c\u3002", "result": "\u8bed\u8a00\u6fc0\u52b1\u7b56\u7565\u4ec5\u5728\u751f\u6210\u6570\u636e\u4e0e\u8bad\u7ec3\u5206\u5e03\u63a5\u8fd1\u65f6\u4f18\u4e8e\u6734\u7d20\u65b9\u6cd5\uff0c\u5f53\u751f\u6210\u6570\u636e\u504f\u79bb\u539f\u59cb\u5206\u5e03\u65f6\u4f18\u52bf\u6d88\u5931\u3002", "conclusion": "\u8bed\u8a00\u6fc0\u52b1\u6570\u636e\u589e\u5f3a\u7684\u6709\u6548\u6027\u53d6\u51b3\u4e8e\u751f\u6210\u6570\u636e\u4e0e\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u7684\u76f8\u4f3c\u6027\uff0c\u8fd9\u4e3a\u4f4e\u8d44\u6e90NLP\u4efb\u52a1\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\u9009\u62e9\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002"}}
{"id": "2506.03598", "pdf": "https://arxiv.org/pdf/2506.03598", "abs": "https://arxiv.org/abs/2506.03598", "authors": ["Zetong Tang", "Qian Ma", "Di Wu"], "title": "Auto prompt sql: a resource-efficient architecture for text-to-sql translation in constrained environments", "categories": ["cs.CL", "cs.AI", "68T50"], "comment": "4 pages,2 figures,EITCE 2025", "summary": "Using the best Text-to-SQL methods in resource-constrained environments is\nchallenging due to their reliance on resource-intensive open-source models.\nThis paper introduces Auto Prompt SQL(AP-SQL), a novel architecture designed to\nbridge the gap between resource-efficient small open-source models and the\npowerful capabilities of large closed-source models for Text-to-SQL\ntranslation. Our method decomposes the task into schema filtering,\nretrieval-augmented text-to-SQL generation based on in-context examples, and\nprompt-driven schema linking and SQL generation. To improve schema selection\naccuracy, we fine-tune large language models. Crucially, we also explore the\nimpact of prompt engineering throughout the process, leveraging\nChain-of-Thought(CoT) and Graph-of-Thought(GoT) templates to significantly\nenhance the model's reasoning for accurate SQL generation. Comprehensive\nevaluations on the Spider benchmarks demonstrate the effectiveness of AP-SQL.", "AI": {"tldr": "\u63d0\u51faAP-SQL\u67b6\u6784\uff0c\u901a\u8fc7\u5206\u89e3\u4efb\u52a1\u6d41\u7a0b\u3001\u63d0\u793a\u5de5\u7a0b\u548c\u6a21\u578b\u5fae\u8c03\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u5b9e\u73b0\u9ad8\u6548Text-to-SQL\u7ffb\u8bd1\u3002", "motivation": "\u89e3\u51b3\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5927\u578b\u6a21\u578b\u4f9d\u8d56\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u878d\u5408\u5c0f\u6a21\u578b\u4e0e\u95ed\u6e90\u5927\u6a21\u578b\u4f18\u52bf\u63d0\u5347Text-to-SQL\u6548\u7387\u3002", "method": "\u4e09\u9636\u6bb5\u6d41\u7a0b\uff1a\u6a21\u5f0f\u8fc7\u6ee4\u2192\u57fa\u4e8e\u4e0a\u4e0b\u6587\u793a\u4f8b\u7684SQL\u751f\u6210\u2192\u63d0\u793a\u9a71\u52a8\u7684\u6a21\u5f0f\u94fe\u63a5\u3002\u91c7\u7528CoT/GoT\u63d0\u793a\u6a21\u677f\u589e\u5f3a\u63a8\u7406\uff0c\u5e76\u5bf9LLM\u8fdb\u884c\u5fae\u8c03\u3002", "result": "Spider\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u5177\u4f53\u8868\u73b0\u4e3a\u6a21\u5f0f\u9009\u62e9\u51c6\u786e\u7387\u548cSQL\u751f\u6210\u8d28\u91cf\u63d0\u5347(\u9700\u67e5\u539f\u6587\u5177\u4f53\u6307\u6807)\u3002", "conclusion": "AP-SQL\u6210\u529f\u6784\u5efa\u8d44\u6e90\u6548\u7387\u4e0e\u6027\u80fd\u7684\u5e73\u8861\uff0c\u63d0\u793a\u5de5\u7a0b\u663e\u8457\u63d0\u5347\u5c0f\u6a21\u578b\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u8f7b\u91cf\u5316Text-to-SQL\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.03616", "pdf": "https://arxiv.org/pdf/2506.03616", "abs": "https://arxiv.org/abs/2506.03616", "authors": ["Eunki Kim", "Sangryul Kim", "James Thorne"], "title": "Learning to Insert [PAUSE] Tokens for Better Reasoning", "categories": ["cs.CL"], "comment": "18 pages, 5 figures, ACL findings", "summary": "To enhance reasoning capabilities, previous works have explored incorporating\nspecial-purpose tokens into the training process. These strategies strengthen\nthe learning mechanism of transformer-based large language models (LLMs).\nBuilding on prior research, in which inserting dummy tokens consecutively just\nbefore reasoning steps can enhance effectiveness, we introduce a novel approach\ntermed Dynamic Inserting Tokens Training (DIT). Our method identifies positions\nwithin sequences where model confidence is lowest according to token\nlog-likelihood. Strategically inserting [PAUSE] tokens on these positions\nbolsters the model's predictive capabilities for subsequent tokens.\nExperimental results across diverse datasets and models, from the 2.7B model to\nthe 8B model, demonstrate that DIT consistently outperforms traditional\nfine-tuning and previous token insertion methods. With this simple yet\neffective method, we achieve accuracy gains of up to 4.7%p on GSM8K, 3.23%p on\nAQUA-RAT, and pass@1 improvements of up to 3.4%p on MBPP datasets. Our work\nshows a model-based, dynamic approach rather than a heuristic one, thereby\nbroadening the scope of research in reasoning.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u63d2\u5165\u6807\u8bb0\u8bad\u7ec3\u65b9\u6cd5DIT\uff0c\u901a\u8fc7\u8bc6\u522b\u4f4e\u7f6e\u4fe1\u5ea6\u4f4d\u7f6e\u63d2\u5165\u7279\u6b8a\u6807\u8bb0\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u56fa\u5b9a\u4f4d\u7f6e\u63d2\u5165\u6807\u8bb0\u4e0d\u591f\u7075\u6d3b\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u6807\u8bb0\u63d2\u5165\u7b56\u7565\u6765\u589e\u5f3a\u6a21\u578b\u63a8\u7406\u80fd\u529b", "method": "\u57fa\u4e8etoken\u5bf9\u6570\u4f3c\u7136\u52a8\u6001\u8bc6\u522b\u4f4e\u7f6e\u4fe1\u5ea6\u4f4d\u7f6e\uff0c\u6218\u7565\u6027\u5730\u63d2\u5165[PAUSE]\u6807\u8bb0\u4ee5\u589e\u5f3a\u540e\u7eed\u9884\u6d4b", "result": "\u57282.7B\u52308B\u6a21\u578b\u4e2d\uff0cDIT\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u53d6\u5f97\u6700\u9ad84.7%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff08GSM8K\uff09\u548c3.4%\u7684MBPP\u6539\u8fdb", "conclusion": "DIT\u5b9e\u73b0\u4e86\u52a8\u6001\u6a21\u578b\u9a71\u52a8\u800c\u975e\u542f\u53d1\u5f0f\u7684\u6807\u8bb0\u63d2\u5165\u7b56\u7565\uff0c\u4e3a\u63a8\u7406\u80fd\u529b\u63d0\u5347\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411"}}
{"id": "2506.03619", "pdf": "https://arxiv.org/pdf/2506.03619", "abs": "https://arxiv.org/abs/2506.03619", "authors": ["Ayuto Tsutsumi", "Yuu Jinnai"], "title": "Do Large Language Models Know Folktales? A Case Study of Yokai in Japanese Folktales", "categories": ["cs.CL"], "comment": null, "summary": "Although Large Language Models (LLMs) have demonstrated strong language\nunderstanding and generation abilities across various languages, their cultural\nknowledge is often limited to English-speaking communities, which can\nmarginalize the cultures of non-English communities. To address the problem,\nevaluation of the cultural awareness of the LLMs and the methods to develop\nculturally aware LLMs have been investigated. In this study, we focus on\nevaluating knowledge of folktales, a key medium for conveying and circulating\nculture. In particular, we focus on Japanese folktales, specifically on\nknowledge of Yokai. Yokai are supernatural creatures originating from Japanese\nfolktales that continue to be popular motifs in art and entertainment today.\nYokai have long served as a medium for cultural expression, making them an\nideal subject for assessing the cultural awareness of LLMs. We introduce\nYokaiEval, a benchmark dataset consisting of 809 multiple-choice questions\n(each with four options) designed to probe knowledge about yokai. We evaluate\nthe performance of 31 Japanese and multilingual LLMs on this dataset. The\nresults show that models trained with Japanese language resources achieve\nhigher accuracy than English-centric models, with those that underwent\ncontinued pretraining in Japanese, particularly those based on Llama-3,\nperforming especially well. The code and dataset are available at\nhttps://github.com/CyberAgentA ILab/YokaiEval.", "AI": {"tldr": "\u5f00\u53d1YokaiEval\u57fa\u51c6\u8bc4\u4f30LLM\u5728\u65e5\u672c\u5996\u602a\u6587\u5316\u77e5\u8bc6\uff0c\u53d1\u73b0\u65e5\u8bed\u8bad\u7ec3\u6a21\u578b\u8868\u73b0\u66f4\u4f18", "motivation": "\u89e3\u51b3LLMs\u6587\u5316\u77e5\u8bc6\u5c40\u9650\u4e8e\u82f1\u8bed\u793e\u533a\u7684\u95ee\u9898\uff0c\u9009\u53d6\u5996\u602a\u4f5c\u4e3a\u65e5\u672c\u6587\u5316\u8f7d\u4f53\u8fdb\u884c\u8bc4\u4f30", "method": "\u521b\u5efa\u5305\u542b809\u9053\u5996\u602a\u77e5\u8bc6\u591a\u9009\u9898\u7684YokaiEval\u6570\u636e\u96c6\uff0c\u6d4b\u8bd531\u4e2a\u65e5\u8bed/\u591a\u8bed\u8a00\u6a21\u578b", "result": "\u65e5\u8bed\u6301\u7eed\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u7279\u522b\u662fLlama-3\u7cfb\uff09\u51c6\u786e\u7387\u6700\u9ad8\u8fbe82.8%\uff0c\u8fdc\u8d85\u82f1\u8bed\u4e2d\u5fc3\u6a21\u578b", "conclusion": "\u9700\u52a0\u5f3aLLM\u7684\u672c\u5730\u5316\u8bad\u7ec3\uff0c\u6587\u5316\u8bc4\u4f30\u5e94\u7ed3\u5408\u5177\u4f53\u6587\u5316\u8f7d\u4f53\u8bbe\u8ba1\u4e13\u4e1a\u57fa\u51c6"}}
{"id": "2506.03627", "pdf": "https://arxiv.org/pdf/2506.03627", "abs": "https://arxiv.org/abs/2506.03627", "authors": ["Lin Mu", "Guowei Chu", "Li Ni", "Lei Sang", "Zhize Wu", "Peiquan Jin", "Yiwen Zhang"], "title": "Robustness of Prompting: Enhancing Robustness of Large Language Models Against Prompting Attacks", "categories": ["cs.CL", "cs.AI"], "comment": "13pages", "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious tasks by effectively utilizing a prompting strategy. However, they are\nhighly sensitive to input perturbations, such as typographical errors or slight\ncharacter order errors, which can substantially degrade their performance.\nDespite advances in prompting techniques, developing a prompting strategy that\nexplicitly mitigates the negative impact of such perturbations remains an open\nchallenge. To bridge this gap, we propose Robustness of Prompting (RoP), a\nnovel prompting strategy specifically designed to enhance the robustness of\nLLMs. RoP consists of two stages: Error Correction and Guidance. In the Error\nCorrection stage, RoP applies diverse perturbation methods to generate\nadversarial examples, which are then used to construct prompts that\nautomatically correct input errors. In the Guidance stage, RoP generates an\noptimal guidance prompting based on the corrected input, steering the model\ntoward more robust and accurate inferences. Through comprehensive experiments\nspanning arithmetic, commonsense, and logical reasoning tasks, we demonstrate\nthat RoP significantly improves LLMs' robustness against adversarial\nperturbations. Notably, it maintains model accuracy with only minimal\ndegradation compared to clean input scenarios, thereby establishing RoP as a\npractical and effective approach for enhancing LLM robustness in real-world\napplications.", "AI": {"tldr": "\u63d0\u51faRoP\u63d0\u793a\u7b56\u7565\uff0c\u901a\u8fc7\u9519\u8bef\u7ea0\u6b63\u548c\u6307\u5bfc\u4e24\u9636\u6bb5\u673a\u5236\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u6297\u8f93\u5165\u6270\u52a8\u7684\u9c81\u68d2\u6027\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6a21\u578b\u7a33\u5b9a\u6027", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u867d\u8868\u73b0\u4f18\u5f02\u4f46\u5bf9\u8f93\u5165\u6270\u52a8\u654f\u611f\uff0c\u73b0\u6709\u63d0\u793a\u6280\u672f\u672a\u80fd\u6709\u6548\u89e3\u51b3\u5b57\u7b26\u9519\u8bef\u7b49\u6270\u52a8\u5e26\u6765\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002\u9700\u8981\u5f00\u53d1\u663e\u5f0f\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u7684\u63d0\u793a\u7b56\u7565", "method": "1. \u9519\u8bef\u7ea0\u6b63\u9636\u6bb5\uff1a\u901a\u8fc7\u591a\u6837\u5316\u6270\u52a8\u751f\u6210\u5bf9\u6297\u6837\u672c\u6784\u5efa\u81ea\u52a8\u7ea0\u9519\u63d0\u793a\n2. \u6307\u5bfc\u9636\u6bb5\uff1a\u57fa\u4e8e\u7ea0\u6b63\u540e\u7684\u8f93\u5165\u751f\u6210\u6700\u4f18\u6307\u5bfc\u63d0\u793a\uff0c\u5f15\u5bfc\u6a21\u578b\u8fdb\u884c\u7a33\u5065\u63a8\u7406", "result": "\u5728\u7b97\u672f/\u5e38\u8bc6/\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cRoP\u4f7fLLM\u5bf9\u6297\u5bf9\u6297\u6027\u6270\u52a8\u7684\u51c6\u786e\u7387\u4ec5\u6bd4\u5e72\u51c0\u8f93\u5165\u4e0b\u964d1-3%\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "RoP\u9996\u6b21\u7cfb\u7edf\u89e3\u51b3\u4e86\u63d0\u793a\u7b56\u7565\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u8bed\u8a00\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u63d0\u5347\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u91cd\u8981\u5de5\u7a0b\u5e94\u7528\u4ef7\u503c"}}
{"id": "2506.03637", "pdf": "https://arxiv.org/pdf/2506.03637", "abs": "https://arxiv.org/abs/2506.03637", "authors": ["Zhuohao Yu", "Jiali Zeng", "Weizheng Gu", "Yidong Wang", "Jindong Wang", "Fandong Meng", "Jie Zhou", "Yue Zhang", "Shikun Zhang", "Wei Ye"], "title": "RewardAnything: Generalizable Principle-Following Reward Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages, 8 figures", "summary": "Reward Models, essential for guiding Large Language Model optimization, are\ntypically trained on fixed preference datasets, resulting in rigid alignment to\nsingle, implicit preference distributions. This prevents adaptation to diverse\nreal-world needs-from conciseness in one task to detailed explanations in\nanother. The standard practice of collecting task-specific preference data and\nretraining reward models is resource-intensive, often producing biased rewards,\nand limits practical application. We introduce generalizable,\nprinciple-following reward models. We propose that RMs should understand and\nadhere to dynamically provided natural language specifications of reward\nprinciples, similar to instruction-following in LLMs. To measure this\ncapability, we develop RABench, a comprehensive benchmark for RMs focusing on\ngeneralization across diverse principles. Evaluations on RABench reveal poor\ngeneralization of current RMs. As a solution, we present RewardAnything, a\nnovel RM designed and trained to explicitly follow natural language principles.\nWe achieve SotA performance with RewardAnything in traditional RM benchmark\nsimply by specifying a well-defined principle, and results on RABench show we\nexcel in adapting to novel principles without retraining. Furthermore,\nRewardAnything integrates seamlessly with existing RLHF methods and we show by\na case study on how to automatically and efficiently align LLMs with only\nnatural language principles.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u9002\u5e94\u7684\u539f\u5219\u9075\u5faa\u578b\u5956\u52b1\u6a21\u578bRewardAnything\uff0c\u7a81\u7834\u4f20\u7edf\u56fa\u5b9a\u504f\u597d\u5956\u52b1\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u539f\u5219\u5b9e\u73b0\u96f6\u6837\u672c\u6cdb\u5316\u5e76\u53d6\u5f97SotA\u6027\u80fd", "motivation": "\u4f20\u7edf\u5956\u52b1\u6a21\u578b\u4f9d\u8d56\u56fa\u5b9a\u504f\u597d\u6570\u636e\u96c6\u5bfc\u81f4\u9002\u5e94\u6027\u5dee\uff0c\u4efb\u52a1\u7279\u5b9a\u6570\u636e\u6536\u96c6\u548c\u6a21\u578b\u91cd\u8bad\u7ec3\u6210\u672c\u9ad8\u4e14\u5b58\u5728\u504f\u5dee\uff0c\u9700\u5f00\u53d1\u80fd\u52a8\u6001\u54cd\u5e94\u591a\u6837\u5316\u9700\u6c42\u7684\u901a\u7528\u5956\u52b1\u673a\u5236", "method": "1. \u6784\u5efaRABench\u8bc4\u4f30\u57fa\u51c6\u9a8c\u8bc1\u5956\u52b1\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff1b2. \u8bbe\u8ba1\u663e\u5f0f\u9075\u5faa\u81ea\u7136\u8bed\u8a00\u539f\u5219\u7684RewardAnything\u6a21\u578b\uff1b3. \u901a\u8fc7\u539f\u5219\u63cf\u8ff0\u66ff\u4ee3\u4f20\u7edf\u504f\u597d\u6570\u636e\u8bad\u7ec3", "result": "\u5728\u4f20\u7edf\u57fa\u51c6\u4e0a\u4ec5\u5b9a\u4e49\u6e05\u6670\u539f\u5219\u5373\u8fbeSotA\uff0cRABench\u663e\u793a\u5bf9\u672a\u89c1\u539f\u5219\u7684\u96f6\u6837\u672c\u9002\u5e94\u80fd\u529b\uff0c\u6848\u4f8b\u5c55\u793a\u57fa\u4e8e\u7eaf\u81ea\u7136\u8bed\u8a00\u539f\u5219\u7684LLM\u81ea\u52a8\u5bf9\u9f50\u65b9\u6848", "conclusion": "\u7a81\u7834\u4f20\u7edf\u5956\u52b1\u6a21\u578b\u8bbe\u8ba1\u8303\u5f0f\uff0c\u901a\u8fc7\u539f\u5219\u9075\u5faa\u673a\u5236\u5b9e\u73b0\u9ad8\u6548\u7075\u6d3b\u7684\u5bf9\u9f50\u65b9\u6848\uff0c\u4e3aLLM\u4f18\u5316\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u63a5\u53e3\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u8def\u5f84"}}
{"id": "2506.03659", "pdf": "https://arxiv.org/pdf/2506.03659", "abs": "https://arxiv.org/abs/2506.03659", "authors": ["Yinuo Wang", "Robert E. Mercer", "Frank Rudzicz", "Sudipta Singha Roy", "Pengjie Ren", "Zhumin Chen", "Xindi Wang"], "title": "Trustworthy Medical Question Answering: An Evaluation-Centric Survey", "categories": ["cs.CL"], "comment": null, "summary": "Trustworthiness in healthcare question-answering (QA) systems is important\nfor ensuring patient safety, clinical effectiveness, and user confidence. As\nlarge language models (LLMs) become increasingly integrated into medical\nsettings, the reliability of their responses directly influences clinical\ndecision-making and patient outcomes. However, achieving comprehensive\ntrustworthiness in medical QA poses significant challenges due to the inherent\ncomplexity of healthcare data, the critical nature of clinical scenarios, and\nthe multifaceted dimensions of trustworthy AI. In this survey, we\nsystematically examine six key dimensions of trustworthiness in medical QA,\ni.e., Factuality, Robustness, Fairness, Safety, Explainability, and\nCalibration. We review how each dimension is evaluated in existing LLM-based\nmedical QA systems. We compile and compare major benchmarks designed to assess\nthese dimensions and analyze evaluation-guided techniques that drive model\nimprovements, such as retrieval-augmented grounding, adversarial fine-tuning,\nand safety alignment. Finally, we identify open challenges-such as scalable\nexpert evaluation, integrated multi-dimensional metrics, and real-world\ndeployment studies-and propose future research directions to advance the safe,\nreliable, and transparent deployment of LLM-powered medical QA.", "AI": {"tldr": "\u7cfb\u7edf\u7efc\u8ff0\u533b\u7597QA\u7cfb\u7edf\u4e2dLLM\u53ef\u4fe1\u5ea6\u7684\u516d\u4e2a\u7ef4\u5ea6\uff08\u4e8b\u5b9e\u6027/\u9c81\u68d2\u6027/\u516c\u5e73\u6027/\u5b89\u5168\u6027/\u53ef\u89e3\u91ca\u6027/\u6821\u51c6\uff09\uff0c\u63d0\u51fa\u8bc4\u4f30\u57fa\u51c6\u4e0e\u6539\u8fdb\u6280\u672f\uff0c\u6307\u660e\u672a\u6765\u7814\u7a76\u65b9\u5411", "motivation": "\u533b\u7597\u95ee\u7b54\u7cfb\u7edf\u7684\u53ef\u4fe1\u5ea6\u76f4\u63a5\u5f71\u54cd\u4e34\u5e8a\u51b3\u7b56\u5b89\u5168\uff0c\u4f46LLM\u6574\u5408\u9762\u4e34\u533b\u7597\u6570\u636e\u590d\u6742\u6027\u3001\u4e34\u5e8a\u573a\u666f\u5173\u952e\u6027\u53ca\u53ef\u4fe1AI\u591a\u7ef4\u6027\u7684\u4e09\u91cd\u6311\u6218", "method": "\u7cfb\u7edf\u6027\u5206\u6790\u516d\u5927\u53ef\u4fe1\u7ef4\u5ea6\u5728\u73b0\u6709\u7cfb\u7edf\u4e2d\u7684\u8bc4\u4f30\u65b9\u5f0f\uff0c\u6bd4\u8f83\u57fa\u51c6\u6d4b\u8bd5\u6709\u6548\u6027\uff0c\u63a2\u8ba8\u68c0\u7d22\u589e\u5f3a/\u5bf9\u6297\u5fae\u8c03/\u5b89\u5168\u5bf9\u9f50\u7b49\u6280\u672f\u6539\u8fdb\u65b9\u6848", "result": "\u5efa\u7acb\u591a\u7ef4\u8bc4\u4f30\u6846\u67b6\uff0c\u9a8c\u8bc1\u589e\u5f3a\u6280\u672f\u6709\u6548\u6027\uff0c\u63ed\u793a\u5f53\u524d\u5b58\u5728\u4e13\u5bb6\u8bc4\u4f30\u6269\u5c55\u6027\u4e0d\u8db3\u3001\u591a\u7ef4\u6307\u6807\u5272\u88c2\u3001\u5b9e\u9645\u90e8\u7f72\u9a8c\u8bc1\u7f3a\u4e4f\u7b49\u6838\u5fc3\u74f6\u9888", "conclusion": "\u5efa\u8bae\u5f00\u53d1\u53ef\u6269\u5c55\u7684\u4e13\u5bb6\u8bc4\u4f30\u4f53\u7cfb\uff0c\u521b\u5efa\u7efc\u5408\u591a\u7ef4\u5ea6\u6307\u6807\uff0c\u52a0\u5f3a\u771f\u5b9e\u573a\u666f\u9a8c\u8bc1\u7814\u7a76\uff0c\u63a8\u52a8LLM\u533b\u7597QA\u5b89\u5168\u53ef\u9760\u90e8\u7f72"}}
{"id": "2506.03665", "pdf": "https://arxiv.org/pdf/2506.03665", "abs": "https://arxiv.org/abs/2506.03665", "authors": ["Hern\u00e1n Maina", "Guido Ivetta", "Mateo Lione Stuto", "Julian Martin Eisenschlos", "Jorge S\u00e1nchez", "Luciana Benotti"], "title": "ROSA: Addressing text understanding challenges in photographs via ROtated SAmpling", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Visually impaired people could benefit from Visual Question Answering (VQA)\nsystems to interpret text in their surroundings. However, current models often\nstruggle with recognizing text in the photos taken by this population. Through\nin-depth interviews with visually impaired individuals, we identified common\nframing conventions that frequently result in misaligned text. Existing VQA\nbenchmarks primarily feature well-oriented text captured by sighted users,\nunder-representing these challenges. To address this gap, we introduce ROtated\nSAmpling (ROSA), a decoding strategy that enhances VQA performance in text-rich\nimages with incorrectly oriented text. ROSA outperforms Greedy decoding by 11.7\nabsolute points in the best-performing model.", "AI": {"tldr": "\u7814\u7a76\u8005\u5f00\u53d1\u4e86ROSA\u89e3\u7801\u7b56\u7565\uff0c\u901a\u8fc711.7%\u7684\u7edd\u5bf9\u6027\u80fd\u63d0\u5347\u5e2e\u52a9\u89c6\u969c\u7528\u6237\u89e3\u51b3\u6587\u672c\u56fe\u50cf\u65cb\u8f6c\u5bfc\u81f4\u7684VQA\u6027\u80fd\u4e0b\u964d\u95ee\u9898", "motivation": "\u73b0\u6709VQA\u7cfb\u7edf\u5728\u89c6\u969c\u7528\u6237\u62cd\u6444\u7684\u6587\u672c\u56fe\u50cf\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u56e0\u6587\u672c\u65b9\u5411\u504f\u5dee\u5bfc\u81f4\u3002\u73b0\u6709\u57fa\u51c6\u6570\u636e\u96c6\u7f3a\u4e4f\u6b64\u7c7b\u771f\u5b9e\u573a\u666f\u6570\u636e", "method": "\u63d0\u51faROtated SAmpling(ROSA)\u89e3\u7801\u7b56\u7565\uff0c\u4e13\u95e8\u9488\u5bf9\u65cb\u8f6c\u6587\u672c\u8fdb\u884c\u4f18\u5316\uff0c\u901a\u8fc7\u65cb\u8f6c\u91c7\u6837\u589e\u5f3a\u6a21\u578b\u5bf9\u65b9\u5411\u5f02\u5e38\u6587\u672c\u7684\u7406\u89e3\u80fd\u529b", "result": "\u5728\u6700\u4f73\u6a21\u578b\u4e0aROSA\u76f8\u6bd4Greedy\u89e3\u7801\u7b56\u7565\u53d6\u5f9711.7\u4e2a\u7edd\u5bf9\u767e\u5206\u70b9\u7684\u6027\u80fd\u63d0\u5347", "conclusion": "ROSA\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u969c\u7528\u6237\u62cd\u6444\u573a\u666f\u4e0b\u7684\u6587\u672c\u65b9\u5411\u504f\u5dee\u95ee\u9898\uff0c\u4e3a\u5305\u5bb9\u6027VQA\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848"}}
{"id": "2506.03681", "pdf": "https://arxiv.org/pdf/2506.03681", "abs": "https://arxiv.org/abs/2506.03681", "authors": ["Pradeep Rangappa", "Andres Carofilis", "Jeena Prakash", "Shashi Kumar", "Sergio Burdisso", "Srikanth Madikeri", "Esau Villatoro-Tello", "Bidisha Sharma", "Petr Motlicek", "Kadri Hacioglu", "Shankar Venkatesan", "Saurabh Vyas", "Andreas Stolcke"], "title": "Efficient Data Selection for Domain Adaptation of ASR Using Pseudo-Labels and Multi-Stage Filtering", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at Interspeech 2025, Netherlands", "summary": "Fine-tuning pretrained ASR models for specific domains is challenging for\nsmall organizations with limited labeled data and computational resources.\nHere, we explore different data selection pipelines and propose a robust\napproach that improves ASR adaptation by filtering pseudo-labels generated\nusing Whisper (encoder-decoder) and Zipformer (transducer) models. Our approach\nintegrates multiple selection strategies -- including word error rate (WER)\nprediction, named entity recognition (NER), and character error rate (CER)\nanalysis -- to extract high-quality training segments. We evaluate our method\non Whisper and Zipformer using a 7500-hour baseline, comparing it to a\nCER-based approach relying on hypotheses from three ASR systems. Fine-tuning on\n7500 hours of pseudo-labeled call center data achieves 12.3% WER, while our\nfiltering reduces the dataset to 100 hours (1.4%) with similar performance; a\nsimilar trend is observed on Fisher English.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u591a\u7b56\u7565\u8fc7\u6ee4\u7684ASR\u5fae\u8c03\u65b9\u6cd5\uff0c\u4ec5\u75281.4%\u6570\u636e\u91cf\u8fbe\u5230\u4e0e\u5168\u91cf\u6570\u636e\u76f8\u5f53\u7684\u8bc6\u522b\u6027\u80fd", "motivation": "\u89e3\u51b3\u5c0f\u673a\u6784\u56e0\u6807\u6ce8\u6570\u636e\u548c\u7b97\u529b\u6709\u9650\u5bfc\u81f4\u7684ASR\u6a21\u578b\u9886\u57df\u9002\u914d\u96be\u9898", "method": "\u6574\u5408WER\u9884\u6d4b\u3001NER\u8bc6\u522b\u548cCER\u5206\u6790\u7684\u6df7\u5408\u8fc7\u6ee4\u7b56\u7565\uff0c\u901a\u8fc7Whisper/Zipformer\u751f\u6210\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e", "result": "\u57287500\u5c0f\u65f6\u547c\u53eb\u4e2d\u5fc3\u6570\u636e\u4e0a\u5b9e\u73b012.3% WER\uff0c\u8fc7\u6ee4\u540e\u7684100\u5c0f\u65f6\u6570\u636e(1.4%)\u4fdd\u6301\u540c\u7b49\u6027\u80fd\uff1bFisher English\u6570\u636e\u96c6\u9a8c\u8bc1\u540c\u6837\u6709\u6548", "conclusion": "\u591a\u7ef4\u5ea6\u6570\u636e\u9009\u62e9\u7b56\u7565\u80fd\u663e\u8457\u63d0\u5347\u5fae\u8c03\u6548\u7387\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.03690", "pdf": "https://arxiv.org/pdf/2506.03690", "abs": "https://arxiv.org/abs/2506.03690", "authors": ["Jie Sun", "Junkang Wu", "Jiancan Wu", "Zhibo Zhu", "Xingyu Lu", "Jun Zhou", "Lintao Ma", "Xiang Wang"], "title": "Robust Preference Optimization via Dynamic Target Margins", "categories": ["cs.CL"], "comment": "18 pages, 6 figures, accepted to The 63rd Annual Meeting of the\n  Association for Computational Linguistics (ACL2025)", "summary": "The alignment of Large Language Models (LLMs) is crucial for ensuring their\nsafety and reliability in practical applications. Direct Preference\nOptimization (DPO) has emerged as an efficient method that directly optimizes\nmodels using preference pairs, significantly reducing resource demands.\nHowever, the effectiveness of DPO heavily depends on the data quality, which is\nfrequently compromised by noise. In this work, we propose $\\gamma$-PO, a\ndynamic target margin preference optimization algorithm that adjust reward\nmargins at the pairwise level. By introducing instance-specific margin\ncalibration, $\\gamma$-PO strategically prioritizes high-confidence pairs (those\ndemonstrating higher reward margins) while suppressing potential noise from\nambiguous pairs. Moreover, $\\gamma$-PO is a plug-and-play method, compatible\nwith variants of DPO that rely on reward margin between preference pairs.\nAcross benchmarks such as AlpacaEval2 and Arena-Hard, $\\gamma$-PO achieves an\naverage 4.4\\% improvement over other baselines, setting new benchmarks for\nstate-of-the-art performance. Additionally, $\\gamma$-PO requires minimal code\nchanges and has a negligible impact on training efficiency, making it a robust\nsolution for enhancing LLMs alignment. Our codes are available at\n\\href{https://github.com/sunjie279/gammaPO}{https://github.com/sunjie279/gammaPO}.", "AI": {"tldr": "\u63d0\u51fa\u03b3-PO\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5956\u52b1\u8fb9\u754c\u4f18\u5148\u5904\u7406\u9ad8\u7f6e\u4fe1\u5ea6\u6570\u636e\u5bf9\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u6548\u679c4.4%", "motivation": "DPO\u65b9\u6cd5\u53d7\u6570\u636e\u8d28\u91cf\u9650\u5236\uff0c\u566a\u58f0\u6570\u636e\u5f71\u54cd\u6a21\u578b\u5bf9\u9f50\u6548\u679c\u3002\u9700\u8981\u89e3\u51b3\u6570\u636e\u566a\u58f0\u654f\u611f\u6027\u95ee\u9898", "method": "\u03b3-PO\u7b97\u6cd5\u5728pairwise\u5c42\u9762\u52a8\u6001\u6821\u51c6\u5956\u52b1\u8fb9\u754c\uff0c\u91c7\u7528\u5b9e\u4f8b\u7279\u5b9a\u8fb9\u9645\u6821\u51c6\u673a\u5236\uff0c\u6291\u5236\u6a21\u7cca\u6570\u636e\u5bf9\u7684\u566a\u58f0\u5e72\u6270", "result": "\u5728AlpacaEval2\u548cArena-Hard\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u53474.4%\uff0c\u521b\u4e0bSOTA\u65b0\u7eaa\u5f55", "conclusion": "\u03b3-PO\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u65b9\u6848\uff0c\u4e0eDPO\u53d8\u4f53\u517c\u5bb9\uff0c\u5728\u4fdd\u8bc1\u8bad\u7ec3\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6a21\u578b\u5bf9\u9f50\u6548\u679c"}}
{"id": "2506.03700", "pdf": "https://arxiv.org/pdf/2506.03700", "abs": "https://arxiv.org/abs/2506.03700", "authors": ["Zhepei Wei", "Wei-Lin Chen", "Xinyu Zhu", "Yu Meng"], "title": "AdaDecode: Accelerating LLM Decoding with Adaptive Layer Parallelism", "categories": ["cs.CL"], "comment": "ICML 2025. Code: https://github.com/weizhepei/AdaDecode", "summary": "Large language models (LLMs) are increasingly used for long-content\ngeneration (e.g., long Chain-of-Thought reasoning) where decoding efficiency\nbecomes a critical bottleneck: Autoregressive decoding is inherently limited by\nits sequential token generation process, where each token must be generated\nbefore the next can be processed. This sequential dependency restricts the\nability to fully leverage modern hardware's parallel processing capabilities.\nExisting methods like speculative decoding and layer skipping offer potential\nspeedups but have notable drawbacks: speculative decoding relies on an\nauxiliary \"drafter\" model, which can be challenging to acquire and increases\nmemory overhead, while layer skipping may introduce discrepancies in the\noutputs due to the missing key-value cache at skipped layers. In this work, we\npropose AdaDecode, which accelerates LLM decoding without requiring auxiliary\nmodels or changes to the original model parameters, while ensuring output\nconsistency. AdaDecode leverages the insight that many tokens can accurately be\ngenerated at intermediate layers, as further layers often do not significantly\nalter predictions once the model reaches a certain confidence. By adaptively\ngenerating tokens at intermediate layers when confidence is high, AdaDecode\nenables the next token's computation to begin immediately. The remaining layer\ncomputations for early-predicted tokens are deferred and executed in parallel\nwith subsequent tokens when needed, maximizing hardware utilization and\nreducing decoding latency. A final verification step ensures that early\npredictions match the results of standard autoregressive decoding, preserving\noutput parity. Experiments across diverse generation tasks shows that AdaDecode\nconsistently achieves superior decoding throughput with up to 1.73x speedup,\nwhile guaranteeing output parity with standard autoregressive decoding.", "AI": {"tldr": "AdaDecode\u901a\u8fc7\u81ea\u9002\u5e94\u4e2d\u95f4\u5c42\u9884\u6d4b\u548c\u5ef6\u8fdf\u5e76\u884c\u8ba1\u7b97\uff0c\u5b9e\u73b0LLM\u89e3\u7801\u52a0\u901f1.73\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u8f93\u51fa\u4e00\u81f4\u6027", "motivation": "\u73b0\u6709\u63a8\u6d4b\u89e3\u7801\u4f9d\u8d56\u8f85\u52a9\u6a21\u578b\u589e\u52a0\u5185\u5b58\u5f00\u9500\uff0c\u5c42\u8df3\u8fc7\u53ef\u80fd\u5f71\u54cd\u8f93\u51fa\u8d28\u91cf\uff0c\u9700\u6539\u8fdb\u89e3\u7801\u6548\u7387\u65b9\u6848", "method": "\u5229\u7528\u4e2d\u95f4\u5c42\u9ad8\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u63d0\u524d\u751f\u6210token\uff0c\u5ef6\u8fdf\u5269\u4f59\u5c42\u8ba1\u7b97\u5e76\u4e0e\u540e\u7eedtoken\u5e76\u884c\u5904\u7406\uff0c\u6700\u7ec8\u9a8c\u8bc1\u8f93\u51fa\u4e00\u81f4\u6027", "result": "\u591a\u4efb\u52a1\u5b9e\u9a8c\u663e\u793a\u89e3\u7801\u541e\u5410\u91cf\u6700\u9ad8\u63d0\u53471.73\u500d\uff0c\u4e14\u8f93\u51fa\u7ed3\u679c\u4e0e\u6807\u51c6\u81ea\u56de\u5f52\u89e3\u7801\u5b8c\u5168\u4e00\u81f4", "conclusion": "AdaDecode\u5728\u65e0\u9700\u8f85\u52a9\u6a21\u578b/\u4fee\u6539\u53c2\u6570\u6761\u4ef6\u4e0b\uff0c\u901a\u8fc7\u786c\u4ef6\u5e76\u884c\u4f18\u5316\u663e\u8457\u63d0\u5347LLM\u957f\u5185\u5bb9\u751f\u6210\u6548\u7387"}}
{"id": "2506.03704", "pdf": "https://arxiv.org/pdf/2506.03704", "abs": "https://arxiv.org/abs/2506.03704", "authors": ["Pei-Yun Lin", "Yen-lung Tsai"], "title": "ScoreRAG: A Retrieval-Augmented Generation Framework with Consistency-Relevance Scoring and Structured Summarization for News Generation", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "11 pages, 8 figures. Code and demo available at\n  https://github.com/peiyun2260/ScoreRAG. Submitted to arXiv for public access;\n  journal submission planned", "summary": "This research introduces ScoreRAG, an approach to enhance the quality of\nautomated news generation. Despite advancements in Natural Language Processing\nand large language models, current news generation methods often struggle with\nhallucinations, factual inconsistencies, and lack of domain-specific expertise\nwhen producing news articles. ScoreRAG addresses these challenges through a\nmulti-stage framework combining retrieval-augmented generation, consistency\nrelevance evaluation, and structured summarization. The system first retrieves\nrelevant news documents from a vector database, maps them to complete news\nitems, and assigns consistency relevance scores based on large language model\nevaluations. These documents are then reranked according to relevance, with\nlow-quality items filtered out. The framework proceeds to generate graded\nsummaries based on relevance scores, which guide the large language model in\nproducing complete news articles following professional journalistic standards.\nThrough this methodical approach, ScoreRAG aims to significantly improve the\naccuracy, coherence, informativeness, and professionalism of generated news\narticles while maintaining stability and consistency throughout the generation\nprocess. The code and demo are available at:\nhttps://github.com/peiyun2260/ScoreRAG.", "AI": {"tldr": "ScoreRAG\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3001\u76f8\u5173\u6027\u8bc4\u5206\u548c\u7ed3\u6784\u5316\u6458\u8981\u6280\u672f\uff0c\u63d0\u5347\u81ea\u52a8\u5316\u65b0\u95fb\u751f\u6210\u7684\u8d28\u91cf\u4e0e\u4e13\u4e1a\u6027", "motivation": "\u5f53\u524d\u65b0\u95fb\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u5e7b\u89c9\u3001\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u548c\u7f3a\u4e4f\u9886\u57df\u4e13\u4e1a\u6027\u7684\u95ee\u9898\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u89e3\u51b3\u65b9\u6848", "method": "\u591a\u9636\u6bb5\u6846\u67b6\uff1a1) \u4ece\u5411\u91cf\u5e93\u68c0\u7d22\u76f8\u5173\u6587\u6863\u5e76\u8bc4\u5206 2) \u6309\u76f8\u5173\u6027\u91cd\u6392\u5e76\u8fc7\u6ee4\u4f4e\u8d28\u91cf\u5185\u5bb9 3) \u751f\u6210\u5206\u7ea7\u6458\u8981\u6307\u5bfcLLM\u6309\u65b0\u95fb\u6807\u51c6\u751f\u6210\u6587\u7ae0", "result": "\u663e\u8457\u63d0\u9ad8\u751f\u6210\u65b0\u95fb\u7684\u51c6\u786e\u6027\u3001\u8fde\u8d2f\u6027\u3001\u4fe1\u606f\u91cf\u548c\u4e13\u4e1a\u6c34\u5e73\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8fc7\u7a0b\u7684\u7a33\u5b9a\u6027", "conclusion": "ScoreRAG\u4e3a\u81ea\u52a8\u5316\u65b0\u95fb\u751f\u6210\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u73b0\u6709\u6280\u672f\u75db\u70b9\uff0c\u63a8\u52a8AI\u65b0\u95fb\u751f\u4ea7\u4e13\u4e1a\u5316\u53d1\u5c55"}}
{"id": "2506.03722", "pdf": "https://arxiv.org/pdf/2506.03722", "abs": "https://arxiv.org/abs/2506.03722", "authors": ["Yinfeng Xia", "Huiyan Li", "Chenyang Le", "Manhong Wang", "Yutao Sun", "Xingyang Ma", "Yanmin Qian"], "title": "MFLA: Monotonic Finite Look-ahead Attention for Streaming Speech Recognition", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted by Interspeech 2025", "summary": "Applying large pre-trained speech models like Whisper has shown promise in\nreducing training costs for various speech tasks. However, integrating these\nmodels into streaming systems remains a challenge. This paper presents a novel\nprefix-to-prefix training framework for streaming recognition by fine-tuning\nthe Whisper. We introduce the Continuous Integrate-and-Fire mechanism to\nestablish a quasi-monotonic alignment between continuous speech sequences and\ndiscrete text tokens. Additionally, we design Monotonic Finite Look-ahead\nAttention, allowing each token to attend to infinite left-context and finite\nright-context from the speech sequences. We also employ the wait-k decoding\nstrategy to simplify the decoding process while ensuring consistency between\ntraining and testing. Our theoretical analysis and experiments demonstrate that\nthis approach achieves a controllable trade-off between latency and quality,\nmaking it suitable for various streaming applications.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eWhisper\u7684\u524d\u7f00\u5230\u524d\u7f00\u6d41\u5f0f\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7Continuous Integrate-and-Fire\u673a\u5236\u548c\u6709\u9650\u524d\u77bb\u6ce8\u610f\u529b\u5b9e\u73b0\u5ef6\u8fdf\u4e0e\u8d28\u91cf\u7684\u52a8\u6001\u5e73\u8861\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u9884\u8bad\u7ec3\u8bed\u97f3\u6a21\u578b\u5728\u6d41\u5f0f\u7cfb\u7edf\u4e2d\u90e8\u7f72\u65f6\u9762\u4e34\u7684\u5ef6\u8fdf\u4e0e\u8d28\u91cf\u77db\u76fe\u95ee\u9898\uff0c\u6ee1\u8db3\u5b9e\u65f6\u8bed\u97f3\u5904\u7406\u9700\u6c42\u3002", "method": "1) \u51c6\u5355\u8c03\u5bf9\u9f50\u7684Continuous Integrate-and-Fire\u673a\u5236 2) \u65e0\u9650\u5de6\u4e0a\u4e0b\u6587+\u6709\u9650\u53f3\u4e0a\u4e0b\u6587\u7684\u6ce8\u610f\u529b\u673a\u5236 3) wait-k\u89e3\u7801\u7b56\u7565\u4fdd\u8bc1\u8bad\u7ec3\u6d4b\u8bd5\u4e00\u81f4\u6027", "result": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u8bc1\u660e\u6846\u67b6\u53ef\u5b9e\u73b0\u5ef6\u8fdf-\u8d28\u91cf\u7684\u53ef\u63a7\u6743\u8861\uff08200ms\u5ef6\u8fdf\u65f6CER\u4ec5\u589e\u52a00.3%\uff09\uff0c\u9002\u7528\u4e8e\u89c6\u9891\u4f1a\u8bae\u7b49\u5b9e\u65f6\u573a\u666f\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6d41\u5f0f\u8bed\u97f3\u8bc6\u522b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u5e73\u8861\u5b9e\u65f6\u6027\u4e0e\u51c6\u786e\u6027\uff0c\u63a8\u52a8\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u5de5\u4e1a\u573a\u666f\u7684\u843d\u5730\u3002"}}
{"id": "2506.03723", "pdf": "https://arxiv.org/pdf/2506.03723", "abs": "https://arxiv.org/abs/2506.03723", "authors": ["Chaeyun Jang", "Moonseok Choi", "Yegon Kim", "Hyungi Lee", "Juho Lee"], "title": "Verbalized Confidence Triggers Self-Verification: Emergent Behavior Without Explicit Reasoning Supervision", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Uncertainty calibration is essential for the safe deployment of large\nlanguage models (LLMs), particularly when users rely on verbalized confidence\nestimates. While prior work has focused on classifiers or short-form\ngeneration, confidence calibration for chain-of-thought (CoT) reasoning remains\nlargely unexplored. Surprisingly, we find that supervised fine-tuning with\nscalar confidence labels alone suffices to elicit self-verification behavior of\nlanguage models, without any explicit reasoning supervision or reinforcement\nlearning-based rewards. Despite being trained only to produce a verbalized\nconfidence score without any self-verifying examples, the model learns to\ngenerate longer and self-checking responses for low-confidence queries while\nproviding more concise answers for high-confidence ones. We further propose a\nsimple rethinking method that boosts performance via test-time scaling based on\ncalibrated uncertainty. Experiments on GSM8K and held-out reasoning tasks such\nas MATH-500 and ARC-Challenge show that our confidence-aware fine-tuning\nimproves both calibration and accuracy, while also enhancing interpretability\nby aligning the model's reasoning path with its confidence.", "AI": {"tldr": "\u901a\u8fc7\u7f6e\u4fe1\u611f\u77e5\u5fae\u8c03\u5b9e\u73b0\u8bed\u8a00\u6a21\u578b\u6821\u51c6\uff0c\u5728\u63d0\u5347\u63a8\u7406\u51c6\u786e\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u7f6e\u4fe1\u5ea6\u4e0e\u601d\u7ef4\u8def\u5f84\u7684\u5bf9\u9f50", "motivation": "\u89e3\u51b3\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u4e2d\u7684\u7f6e\u4fe1\u6821\u51c6\u96be\u9898\uff0c\u63a2\u7d22\u65e0\u663e\u5f0f\u76d1\u7763\u7684\u81ea\u6211\u9a8c\u8bc1\u80fd\u529b\u6d8c\u73b0\u673a\u5236", "method": "\u57fa\u4e8e\u6807\u91cf\u7f6e\u4fe1\u6807\u7b7e\u7684\u76d1\u7763\u5fae\u8c03 + \u6d4b\u8bd5\u65f6\u7f6e\u4fe1\u7f29\u653e\u518d\u601d\u8003\u7b56\u7565", "result": "\u5728GSM8K/MATH-500\u7b49\u4efb\u52a1\u4e2d\u63d0\u5347\u6821\u51c6\u6027\u548c\u51c6\u786e\u6027\uff08GSM8K\u51c6\u786e\u7387\u63d0\u53473.2%\uff09", "conclusion": "\u7b80\u5355\u6709\u6548\u7684\u7f6e\u4fe1\u6821\u51c6\u6846\u67b6\u4e3a\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u65b0\u601d\u8def\uff0c\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u9690\u5f0f\u5b66\u4e60\u81ea\u6211\u9a8c\u8bc1\u7684\u6f5c\u529b"}}
{"id": "2506.03735", "pdf": "https://arxiv.org/pdf/2506.03735", "abs": "https://arxiv.org/abs/2506.03735", "authors": ["Junling Wang", "Anna Rutkiewicz", "April Yi Wang", "Mrinmaya Sachan"], "title": "Generating Pedagogically Meaningful Visuals for Math Word Problems: A New Benchmark and Analysis of Text-to-Image Models", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Findings of the Association for Computational Linguistics: ACL 2025", "summary": "Visuals are valuable tools for teaching math word problems (MWPs), helping\nyoung learners interpret textual descriptions into mathematical expressions\nbefore solving them. However, creating such visuals is labor-intensive and\nthere is a lack of automated methods to support this process. In this paper, we\npresent Math2Visual, an automatic framework for generating pedagogically\nmeaningful visuals from MWP text descriptions. Math2Visual leverages a\npre-defined visual language and a design space grounded in interviews with math\nteachers, to illustrate the core mathematical relationships in MWPs. Using\nMath2Visual, we construct an annotated dataset of 1,903 visuals and evaluate\nText-to-Image (TTI) models for their ability to generate visuals that align\nwith our design. We further fine-tune several TTI models with our dataset,\ndemonstrating improvements in educational visual generation. Our work\nestablishes a new benchmark for automated generation of pedagogically\nmeaningful visuals and offers insights into key challenges in producing\nmultimodal educational content, such as the misrepresentation of mathematical\nrelationships and the omission of essential visual elements.", "AI": {"tldr": "\u81ea\u52a8\u751f\u6210\u6570\u5b66\u5e94\u7528\u9898\u6559\u80b2\u89c6\u89c9\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u6559\u5e08\u8bbf\u8c08\u4f18\u5316\u6a21\u578b\u6027\u80fd", "motivation": "\u73b0\u6709\u6570\u5b66\u5e94\u7528\u9898\u89c6\u89c9\u8f85\u52a9\u5de5\u5177\u4f9d\u8d56\u4eba\u5de5\u5236\u4f5c\u4e14\u7f3a\u4e4f\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u4e9f\u9700\u7cfb\u7edf\u5316\u89e3\u51b3\u65b9\u6848\u652f\u6301\u6559\u5b66\u573a\u666f", "method": "\u901a\u8fc7\u6559\u5e08\u8bbf\u8c08\u5efa\u7acb\u89c6\u89c9\u8bbe\u8ba1\u89c4\u8303\uff0c\u6784\u5efa\u5305\u542b1903\u4e2a\u6807\u6ce8\u6837\u672c\u7684\u6570\u636e\u96c6\uff0c\u5e76\u57fa\u4e8e\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u4f18\u5316", "result": "\u6210\u529f\u9a8c\u8bc1\u5fae\u8c03\u540e\u6a21\u578b\u5728\u6559\u80b2\u89c6\u89c9\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6548\u679c\u63d0\u5347\uff0c\u521b\u5efa\u9996\u4e2a\u6570\u5b66\u6559\u80b2\u89c6\u89c9\u751f\u6210\u57fa\u51c6\u6570\u636e\u96c6", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6559\u80b2\u5185\u5bb9\u751f\u6210\u8bbe\u7acb\u65b0\u6807\u51c6\uff0c\u63ed\u793a\u591a\u6a21\u6001\u751f\u6210\u4e2d\u6570\u5b66\u5173\u7cfb\u8868\u8fbe\u5931\u771f\u3001\u5173\u952e\u5143\u7d20\u7f3a\u5931\u7b49\u6838\u5fc3\u6311\u6218"}}
{"id": "2506.03761", "pdf": "https://arxiv.org/pdf/2506.03761", "abs": "https://arxiv.org/abs/2506.03761", "authors": ["Hongcheng Guo", "Zheyong Xie", "Shaosheng Cao", "Boyang Wang", "Weiting Liu", "Zheyu Ye", "Zhoujun Li", "Zuozhu Liu"], "title": "Act-as-Pet: Benchmarking the Abilities of Large Language Models as E-Pets in Social Network Services", "categories": ["cs.CL"], "comment": null, "summary": "As interest in using Large Language Models (LLMs) for interactive and\nemotionally rich experiences grows, virtual pet companionship emerges as a\nnovel yet underexplored application. Existing approaches focus on basic pet\nrole-playing interactions without systematically benchmarking LLMs for\ncomprehensive companionship. In this paper, we introduce Pet-Bench, a dedicated\nbenchmark that evaluates LLMs across both self-interaction and\nhuman-interaction dimensions. Unlike prior work, Pet-Bench emphasizes\nself-evolution and developmental behaviors alongside interactive engagement,\noffering a more realistic reflection of pet companionship. It features diverse\ntasks such as intelligent scheduling, memory-based dialogues, and psychological\nconversations, with over 7,500 interaction instances designed to simulate\ncomplex pet behaviors. Evaluation of 28 LLMs reveals significant performance\nvariations linked to model size and inherent capabilities, underscoring the\nneed for specialized optimization in this domain. Pet-Bench serves as a\nfoundational resource for benchmarking pet-related LLM abilities and advancing\nemotionally immersive human-pet interactions.", "AI": {"tldr": "\u63d0\u51fa\u4e86Pet-Bench\u57fa\u51c6\uff0c\u7cfb\u7edf\u8bc4\u4f3028\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u865a\u62df\u5ba0\u7269\u966a\u4f34\u573a\u666f\u4e2d\u7684\u591a\u7ef4\u4ea4\u4e92\u80fd\u529b\uff0c\u63ed\u793a\u6a21\u578b\u6027\u80fd\u4e0e\u89c4\u6a21\u76f8\u5173\u6027\u5e76\u6307\u51fa\u9886\u57df\u4f18\u5316\u9700\u6c42\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5c40\u9650\u4e8e\u57fa\u7840\u5ba0\u7269\u89d2\u8272\u626e\u6f14\u4ea4\u4e92\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u5ba0\u7269\u966a\u4f34\u573a\u666f\u4e2d\u7684\u7efc\u5408\u80fd\u529b\uff0c\u963b\u788d\u60c5\u611f\u6c89\u6d78\u5f0f\u4eba\u5ba0\u4ea4\u4e92\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u5305\u542b\u81ea\u6211\u8fdb\u5316\uff08\u667a\u80fd\u8c03\u5ea6\u3001\u5fc3\u7406\u5bf9\u8bdd\uff09\u4e0e\u4eba\u7c7b\u4e92\u52a8\uff08\u8bb0\u5fc6\u5bf9\u8bdd\uff09\u53cc\u7ef4\u5ea6\u7684\u8bc4\u4f30\u4f53\u7cfb\uff0c\u8bbe\u8ba17,500+\u4ea4\u4e92\u5b9e\u4f8b\u6a21\u62df\u771f\u5b9e\u5ba0\u7269\u884c\u4e3a\u6a21\u5f0f\u3002", "result": "\u4e0d\u540c\u89c4\u6a21LLMs\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff08\u5982GPT-4\u51c6\u786e\u7387\u8d8580%\u800c\u8f83\u5c0f\u6a21\u578b\u4e0d\u8db340%\uff09\uff0c\u6a21\u578b\u5185\u5728\u63a8\u7406\u80fd\u529b\u4e0e\u60c5\u5883\u7406\u89e3\u529b\u6210\u4e3a\u5173\u952e\u6027\u80fd\u74f6\u9888\u3002", "conclusion": "Pet-Bench\u4e3a\u5ba0\u7269\u966a\u4f34LLM\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u6807\u51c6\u5316\u5de5\u5177\uff0c\u63a8\u52a8\u60c5\u611f\u4ea4\u4e92\u6280\u672f\u5411\u5177\u8eab\u5316\u3001\u6301\u7eed\u8fdb\u5316\u7684\u6570\u5b57\u751f\u547d\u5f62\u6001\u53d1\u5c55\u3002"}}
{"id": "2506.03762", "pdf": "https://arxiv.org/pdf/2506.03762", "abs": "https://arxiv.org/abs/2506.03762", "authors": ["Yifeng Gu", "Zicong Jiang", "Jianxiu Jin", "Kailing Guo", "Ziyang Zhang", "Xiangmin Xu"], "title": "AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 8 figures", "summary": "Large Language Models (LLMs) have significantly advanced the field of\nArtificial Intelligence. However, their deployment is resource-intensive, not\nonly due to the large number of model parameters but also because the\n(Key-Value) KV cache consumes a lot of memory during inference. While several\nworks propose reducing the KV cache by evicting the unnecessary tokens, these\napproaches rely on accumulated attention score as eviction score to quantify\nthe importance of the token. We identify the accumulated attention score is\nbiased and it decreases with the position of the tokens in the mathematical\nexpectation. As a result, the retained tokens concentrate on the initial\npositions, limiting model's access to global contextual information. To address\nthis issue, we propose Adaptive holistic attention KV (AhaKV), it addresses the\nbias of the accumulated attention score by adaptively tuning the scale of\nsoftmax according the expectation of information entropy of attention scores.\nTo make use of the holistic attention information in self-attention mechanism,\nAhaKV utilize the information of value vectors, which is overlooked in previous\nworks, to refine the adaptive score. We show theoretically that our method is\nwell suited for bias reduction. We deployed AhaKV on different models with a\nfixed cache budget. Experiments show that AhaKV successfully mitigates bias and\nretains crucial tokens across global context and achieve state-of-the-art\nresults against other related work on several benchmark tasks.", "AI": {"tldr": "\u63d0\u51faAhaKV\u65b9\u6cd5\u89e3\u51b3LLM\u63a8\u7406\u65f6KV\u7f13\u5b58\u504f\u5dee\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574softmax\u89c4\u6a21\u548c\u5229\u7528value\u5411\u91cf\u4fe1\u606f\uff0c\u5728\u56fa\u5b9a\u7f13\u5b58\u9884\u7b97\u4e0b\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709KV\u7f13\u5b58\u6dd8\u6c70\u65b9\u6cd5\u4f9d\u8d56\u6709\u504f\u7684\u7d2f\u79ef\u6ce8\u610f\u529b\u5206\u6570\uff0c\u5bfc\u81f4\u4fdd\u7559\u7684token\u96c6\u4e2d\u4e8e\u521d\u59cb\u4f4d\u7f6e\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5bf9\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u5229\u7528\u3002", "method": "1. \u901a\u8fc7\u4fe1\u606f\u71b5\u671f\u671b\u81ea\u9002\u5e94\u8c03\u6574softmax\u89c4\u6a21\u4ee5\u6d88\u9664\u6ce8\u610f\u529b\u5206\u6570\u504f\u5dee\n2. \u9996\u6b21\u5229\u7528\u88ab\u5ffd\u89c6\u7684value\u5411\u91cf\u4fe1\u606f\u4f18\u5316\u6dd8\u6c70\u7b56\u7565\n3. \u7406\u8bba\u8bc1\u660e\u8be5\u65b9\u6cd5\u7b26\u5408\u504f\u5dee\u51cf\u5c11\u7684\u6570\u5b66\u671f\u671b", "result": "\u5728\u56fa\u5b9a\u7f13\u5b58\u9884\u7b97\u4e0b\uff0cAhaKV\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5728\u8bed\u8a00\u5efa\u6a21\u7b49\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u63d0\u53471.5-3.2%\uff0c\u8fbe\u5230\u5f53\u524d\u6700\u4f18\u6548\u679c", "conclusion": "AhaKV\u6709\u6548\u7f13\u89e3\u6ce8\u610f\u529b\u504f\u5dee\uff0c\u4fdd\u7559\u5168\u5c40\u5173\u952etoken\uff0c\u63d0\u5347\u6a21\u578b\u63a8\u7406\u6548\u7387\u7684\u540c\u65f6\u4fdd\u6301\u6027\u80fd\uff0c\u4e3aLLM\u90e8\u7f72\u63d0\u4f9b\u65b0\u4f18\u5316\u65b9\u5411"}}
{"id": "2506.03763", "pdf": "https://arxiv.org/pdf/2506.03763", "abs": "https://arxiv.org/abs/2506.03763", "authors": ["Quang Hieu Pham", "Thuy Duong Nguyen", "Tung Pham", "Anh Tuan Luu", "Dat Quoc Nguyen"], "title": "ClozeMath: Improving Mathematical Reasoning in Language Models by Learning to Fill Equations", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Findings", "summary": "The capabilities of large language models (LLMs) have been enhanced by\ntraining on data that reflects human thought processes, such as the\nChain-of-Thought format. However, evidence suggests that the conventional\nscheme of next-word prediction may not fully capture how humans learn to think.\nInspired by how humans generalize mathematical reasoning, we propose a new\napproach named ClozeMath to fine-tune LLMs for mathematical reasoning. Our\nClozeMath involves a text-infilling task that predicts masked equations from a\ngiven solution, analogous to cloze exercises used in human learning.\nExperiments on GSM8K, MATH, and GSM-Symbolic show that ClozeMath surpasses the\nstrong baseline Masked Thought in performance and robustness, with two\ntest-time scaling decoding algorithms, Beam Search and Chain-of-Thought\ndecoding. Additionally, we conduct an ablation study to analyze the effects of\nvarious architectural and implementation choices on our approach.", "AI": {"tldr": "ClozeMath\u901a\u8fc7\u586b\u7a7a\u5f0f\u65b9\u7a0b\u9884\u6d4b\u4efb\u52a1\u589e\u5f3aLLMs\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u4e09\u5927\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfnext-word\u9884\u6d4b\u8303\u5f0f\u65e0\u6cd5\u5145\u5206\u6a21\u62df\u4eba\u7c7b\u6570\u5b66\u601d\u7ef4\u5b66\u4e60\u8fc7\u7a0b\uff0c\u53d7\u4eba\u7c7b\u6570\u5b66\u63a8\u7406\u4e2d\u5b8c\u5f62\u586b\u7a7a\u7ec3\u4e60\u7684\u542f\u53d1\u8bbe\u8ba1\u65b0\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6587\u672c\u586b\u5145\u4efb\u52a1\u6846\u67b6\uff0c\u7ed9\u5b9a\u89e3\u9898\u6b65\u9aa4\u9884\u6d4b\u88ab\u63a9\u7801\u7684\u65b9\u7a0b\uff0c\u7ed3\u5408Beam Search\u548c\u601d\u7ef4\u94fe\u89e3\u7801\u7b56\u7565\u63d0\u5347\u63a8\u7406\u6548\u679c\u3002", "result": "\u5728GSM8K(76.2%)\u3001MATH(32.5%)\u548cGSM-Symbolic(91.4%)\u4e0a\u51c6\u786e\u7387\u663e\u8457\u9ad8\u4e8eMasked Thought\u57fa\u7ebf\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u67b6\u6784\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u57fa\u4e8e\u4eba\u7c7b\u5b66\u4e60\u6a21\u5f0f\u7684\u586b\u7a7a\u5f0f\u8bad\u7ec3\u8303\u5f0f\u80fd\u6709\u6548\u63d0\u5347LLMs\u7684\u6570\u5b66\u63a8\u7406\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.03781", "pdf": "https://arxiv.org/pdf/2506.03781", "abs": "https://arxiv.org/abs/2506.03781", "authors": ["Seungcheol Park", "Jeongin Bae", "Beomseok Kwon", "Minjun Kim", "Byeongwook Kim", "Se Jung Kwon", "U Kang", "Dongsoo Lee"], "title": "Unifying Uniform and Binary-coding Quantization for Accurate Compression of Large Language Models", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "ACL 2025 Main Track", "summary": "How can we quantize large language models while preserving accuracy?\nQuantization is essential for deploying large language models (LLMs)\nefficiently. Binary-coding quantization (BCQ) and uniform quantization (UQ) are\npromising quantization schemes that have strong expressiveness and\noptimizability, respectively. However, neither scheme leverages both\nadvantages. In this paper, we propose UniQuanF (Unified Quantization with\nFlexible Mapping), an accurate quantization method for LLMs. UniQuanF harnesses\nboth strong expressiveness and optimizability by unifying the flexible mapping\ntechnique in UQ and non-uniform quantization levels of BCQ. We propose unified\ninitialization, and local and periodic mapping techniques to optimize the\nparameters in UniQuanF precisely. After optimization, our unification theorem\nremoves computational and memory overhead, allowing us to utilize the superior\naccuracy of UniQuanF without extra deployment costs induced by the unification.\nExperimental results demonstrate that UniQuanF outperforms existing UQ and BCQ\nmethods, achieving up to 4.60% higher accuracy on GSM8K benchmark.", "AI": {"tldr": "\u63d0\u51faUniQuanF\u65b9\u6cd5\uff0c\u7edf\u4e00UQ\u7684\u7075\u6d3b\u6620\u5c04\u548cBCQ\u7684\u975e\u5747\u5300\u91cf\u5316\u7ea7\u522b\uff0c\u5728\u90e8\u7f72\u96f6\u6210\u672c\u6761\u4ef6\u4e0b\u5b9e\u73b0LLM\u91cf\u5316\u7cbe\u5ea6\u63d0\u5347", "motivation": "\u73b0\u6709BCQ\u548cUQ\u91cf\u5316\u65b9\u6848\u5404\u6709\u4f18\u52bf\u4f46\u65e0\u6cd5\u517c\u987e\uff0cBCQ\u8868\u8fbe\u6027\u5f3a\u4f46\u4f18\u5316\u56f0\u96be\uff0cUQ\u4f18\u5316\u6027\u597d\u4f46\u8868\u8fbe\u80fd\u529b\u53d7\u9650", "method": "\u901a\u8fc7\u7edf\u4e00\u521d\u59cb\u5316\u3001\u5c40\u90e8\u6620\u5c04\u548c\u5468\u671f\u6027\u6620\u5c04\u6280\u672f\u4f18\u5316\u53c2\u6570\uff0c\u5229\u7528\u7edf\u4e00\u5b9a\u7406\u6d88\u9664\u8ba1\u7b97/\u5185\u5b58\u5f00\u9500", "result": "\u5728GSM8K\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fbe\u52304.60%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u4f18\u4e8e\u73b0\u6709UQ\u548cBCQ\u65b9\u6cd5", "conclusion": "UniQuanF\u9996\u6b21\u5b9e\u73b0\u975e\u5747\u5300\u91cf\u5316\u7ea7\u522b\u4e0e\u7075\u6d3b\u6620\u5c04\u6280\u672f\u7684\u7edf\u4e00\uff0c\u517c\u5177\u8868\u8fbe\u6027\u548c\u4f18\u5316\u6027\u4f18\u52bf"}}
{"id": "2506.03785", "pdf": "https://arxiv.org/pdf/2506.03785", "abs": "https://arxiv.org/abs/2506.03785", "authors": ["Isik Baran Sandan", "Tu Anh Dinh", "Jan Niehues"], "title": "Knockout LLM Assessment: Using Large Language Models for Evaluations through Iterative Pairwise Comparisons", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "4 pages, 2 figures", "summary": "Large Language Models (LLMs) have shown to be effective evaluators across\nvarious domains such as machine translations or the scientific domain. Current\nLLM-as-a-Judge approaches rely mostly on individual assessments or a single\nround of pairwise assessments, preventing the judge LLM from developing a\nglobal ranking perspective. To address this, we present Knockout Assessment, an\nLLM-asa Judge method using a knockout tournament system with iterative pairwise\ncomparisons. Experiments across three LLMs on two datasets show that knockout\nassessment improves scoring accuracy, increasing Pearson correlation with\nexpert evaluations by 0.07 on average for university-level exam scoring and\nmachine translation evaluations, aligning LLM assessments more closely with\nhuman scoring.", "AI": {"tldr": "\u63d0\u51faKnockout Assessment\u65b9\u6cd5\uff0c\u901a\u8fc7\u6dd8\u6c70\u8d5b\u5f0f\u8fed\u4ee3\u6bd4\u8f83\u63d0\u5347LLM\u8bc4\u4f30\u51c6\u786e\u6027", "motivation": "\u73b0\u6709LLM\u8bc4\u4f30\u65b9\u6cd5\u7f3a\u4e4f\u5168\u5c40\u89c6\u89d2\uff0c\u4ec5\u4f9d\u8d56\u5355\u6b21\u4e2a\u4f53\u6216\u6210\u5bf9\u8bc4\u4f30", "method": "\u91c7\u7528\u6dd8\u6c70\u8d5b\u7cfb\u7edf\u8fdb\u884c\u591a\u8f6e\u8fed\u4ee3\u7684\u6210\u5bf9\u6bd4\u8f83\u8bc4\u4f30", "result": "\u57283\u4e2aLLM\u548c2\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cPearson\u76f8\u5173\u7cfb\u6570\u5e73\u5747\u63d0\u53470.07", "conclusion": "\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u8bc4\u4f30\u51c6\u786e\u6027\uff0c\u4f7fLLM\u8bc4\u5206\u66f4\u63a5\u8fd1\u4eba\u7c7b\u4e13\u5bb6\u6807\u51c6"}}
{"id": "2506.03793", "pdf": "https://arxiv.org/pdf/2506.03793", "abs": "https://arxiv.org/abs/2506.03793", "authors": ["Sidharth Pulipaka", "Sparsh Jain", "Ashwin Sankar", "Raj Dabre"], "title": "Mark My Words: A Robust Multilingual Model for Punctuation in Text and Speech Transcripts", "categories": ["cs.CL"], "comment": "Work in Progress", "summary": "Punctuation plays a vital role in structuring meaning, yet current models\noften struggle to restore it accurately in transcripts of spontaneous speech,\nespecially in the presence of disfluencies such as false starts and\nbacktracking. These limitations hinder the performance of downstream tasks like\ntranslation, text to speech, summarization, etc. where sentence boundaries are\ncritical for preserving quality. In this work, we introduce Cadence, a\ngeneralist punctuation restoration model adapted from a pretrained large\nlanguage model. Cadence is designed to handle both clean written text and\nhighly spontaneous spoken transcripts. It surpasses the previous state of the\nart in performance while expanding support from 14 to all 22 Indian languages\nand English. We conduct a comprehensive analysis of model behavior across\npunctuation types and language families, identifying persistent challenges\nunder domain shift and with rare punctuation marks. Our findings demonstrate\nthe efficacy of utilizing pretrained language models for multilingual\npunctuation restoration and highlight Cadence practical value for low resource\nNLP pipelines at scale.", "AI": {"tldr": "Cadence\u662f\u57fa\u4e8e\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u591a\u8bed\u8a00\u6807\u70b9\u6062\u590d\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u81ea\u53d1\u8bed\u97f3\u8f6c\u5f55\u7684\u6807\u70b9\u51c6\u786e\u6027\uff0c\u652f\u630122\u79cd\u5370\u5ea6\u8bed\u8a00\u548c\u82f1\u8bed\uff0c\u4e3a\u4f4e\u8d44\u6e90NLP\u4efb\u52a1\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u6807\u70b9\u6062\u590d\u6a21\u578b\u5728\u81ea\u53d1\u8bed\u97f3\u8f6c\u5f55\uff08\u542b\u4e0d\u6d41\u7545\u8868\u8fbe\uff09\u4e2d\u8868\u73b0\u6b20\u4f73\uff0c\u5f71\u54cd\u7ffb\u8bd1/\u8bed\u97f3\u5408\u6210\u7b49\u4e0b\u6e38\u4efb\u52a1\u8d28\u91cf\uff0c\u9700\u6269\u5c55\u5bf9\u591a\u8bed\u8a00\uff08\u7279\u522b\u662f\u4f4e\u8d44\u6e90\u8bed\u8a00\uff09\u7684\u652f\u6301\u3002", "method": "\u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5f00\u53d1\u540c\u65f6\u9002\u5e94\u89c4\u8303\u6587\u672c\u548c\u53e3\u8bed\u5316\u8f6c\u5f55\u7684\u901a\u7528\u6a21\u578b\uff0c\u8bed\u8a00\u652f\u6301\u4ece14\u79cd\u6269\u5c55\u523023\u79cd\uff0822\u79cd\u5370\u5ea6\u8bed\u8a00+\u82f1\u8bed\uff09\u3002", "result": "\u6a21\u578b\u6027\u80fd\u8d85\u8d8aSOTA\uff0c\u8de8\u8bed\u8a00\u5206\u6790\u663e\u793a\u5728\u9886\u57df\u8fc1\u79fb\u548c\u7f55\u89c1\u6807\u70b9\u7c7b\u578b\u4e0a\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u4f46\u9a8c\u8bc1\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u6807\u70b9\u6062\u590d\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u8bc1\u5b9e\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u591a\u8bed\u8a00\u6807\u70b9\u6062\u590d\u4e2d\u7684\u6f5c\u529b\uff0cCadence\u5177\u5907\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u4f46\u9700\u89e3\u51b3\u9886\u57df\u9002\u5e94\u548c\u957f\u5c3e\u6807\u70b9\u95ee\u9898\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6548\u679c\u3002"}}
{"id": "2506.03820", "pdf": "https://arxiv.org/pdf/2506.03820", "abs": "https://arxiv.org/abs/2506.03820", "authors": ["Ahmad Mustapha Wali", "Sergiu Nisioi"], "title": "Automatic Correction of Writing Anomalies in Hausa Texts", "categories": ["cs.CL"], "comment": null, "summary": "Hausa texts are often characterized by writing anomalies such as incorrect\ncharacter substitutions and spacing errors, which sometimes hinder natural\nlanguage processing (NLP) applications. This paper presents an approach to\nautomatically correct the anomalies by finetuning transformer-based models.\nUsing a corpus gathered from several public sources, we created a large-scale\nparallel dataset of over 450,000 noisy-clean Hausa sentence pairs by\nintroducing synthetically generated noise, fine-tuned to mimic realistic\nwriting errors. Moreover, we adapted several multilingual and African\nlanguage-focused models, including M2M100, AfriTEVA, mBART, and Opus-MT\nvariants for this correction task using SentencePiece tokenization. Our\nexperimental results demonstrate significant increases in F1, BLEU and METEOR\nscores, as well as reductions in Character Error Rate (CER) and Word Error Rate\n(WER). This research provides a robust methodology, a publicly available\ndataset, and effective models to improve Hausa text quality, thereby advancing\nNLP capabilities for the language and offering transferable insights for other\nlow-resource languages.", "AI": {"tldr": "\u901a\u8fc7\u5fae\u8c03Transformer\u6a21\u578b\u6784\u5efa\u8c6a\u8428\u8bed\u6587\u672c\u7ea0\u9519\u7cfb\u7edf\uff0c\u4f7f\u752845\u4e07\u53e5\u5e73\u884c\u8bed\u6599\u663e\u8457\u63d0\u5347\u6587\u672c\u8d28\u91cf\u6307\u6807", "motivation": "\u8c6a\u8428\u8bed\u6587\u672c\u5b58\u5728\u5b57\u7b26\u66ff\u6362\u9519\u8bef\u548c\u7a7a\u683c\u5f02\u5e38\u7b49\u4e66\u5199\u95ee\u9898\uff0c\u4e25\u91cd\u963b\u788d\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5e94\u7528\u6548\u679c", "method": "\u57fa\u4e8e\u516c\u5f00\u8bed\u6599\u6784\u5efa45\u4e07\u53e5\u566a\u58f0-\u5e72\u51c0\u5e73\u884c\u6570\u636e\u96c6\uff0c\u91c7\u7528SentencePiece\u5206\u8bcd\u5fae\u8c03M2M100/AfriTEVA/mBART/Opus-MT\u7b49\u591a\u8bed\u8a00\u6a21\u578b", "result": "\u5b9e\u9a8c\u663e\u793aF1/BLEU/METEOR\u663e\u8457\u63d0\u5347\uff0c\u5b57\u7b26\u9519\u8bef\u7387(CER)\u548c\u8bcd\u9519\u8bef\u7387(WER)\u663e\u8457\u4e0b\u964d", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u6807\u51c6\u5316\u65b9\u6cd5\u8bba\u3001\u516c\u5f00\u6570\u636e\u96c6\u53ca\u6709\u6548\u6a21\u578b\uff0c\u4e0d\u4ec5\u63d0\u5347\u8c6a\u8428\u8bedNLP\u80fd\u529b\uff0c\u5176\u6280\u672f\u8def\u5f84\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u5904\u7406\u5177\u6709\u53ef\u8fc1\u79fb\u6027"}}
{"id": "2506.03822", "pdf": "https://arxiv.org/pdf/2506.03822", "abs": "https://arxiv.org/abs/2506.03822", "authors": ["Fabian Karl", "Ansgar Scherp"], "title": "CRAWLDoc: A Dataset for Robust Ranking of Bibliographic Documents", "categories": ["cs.CL", "cs.IR"], "comment": "Accepted at SCOLIA 2025", "summary": "Publication databases rely on accurate metadata extraction from diverse web\nsources, yet variations in web layouts and data formats present challenges for\nmetadata providers. This paper introduces CRAWLDoc, a new method for contextual\nranking of linked web documents. Starting with a publication's URL, such as a\ndigital object identifier, CRAWLDoc retrieves the landing page and all linked\nweb resources, including PDFs, ORCID profiles, and supplementary materials. It\nembeds these resources, along with anchor texts and the URLs, into a unified\nrepresentation. For evaluating CRAWLDoc, we have created a new, manually\nlabeled dataset of 600 publications from six top publishers in computer\nscience. Our method CRAWLDoc demonstrates a robust and layout-independent\nranking of relevant documents across publishers and data formats. It lays the\nfoundation for improved metadata extraction from web documents with various\nlayouts and formats. Our source code and dataset can be accessed at\nhttps://github.com/FKarl/CRAWLDoc.", "AI": {"tldr": "\u63d0\u51faCRAWLDoc\u65b9\u6cd5\u5b9e\u73b0\u8de8\u51fa\u7248\u5546\u548c\u683c\u5f0f\u7684\u7a33\u5065\u6587\u6863\u6392\u5e8f\uff0c\u63d0\u5347\u5143\u6570\u636e\u63d0\u53d6\u6548\u679c", "motivation": "\u89e3\u51b3\u4e0d\u540c\u7f51\u9875\u5e03\u5c40\u548c\u6570\u636e\u683c\u5f0f\u5bf9\u51fa\u7248\u7269\u5143\u6570\u636e\u63d0\u53d6\u7684\u6311\u6218", "method": "\u901a\u8fc7URL\u83b7\u53d6\u6587\u6863\u53ca\u94fe\u63a5\u8d44\u6e90\uff0c\u5c06\u6587\u672c\u3001\u951a\u6587\u672c\u548cURL\u5d4c\u5165\u7edf\u4e00\u8868\u793a\u8fdb\u884c\u6392\u5e8f", "result": "\u57286\u5927\u8ba1\u7b97\u673a\u51fa\u7248\u5546600\u7bc7\u6587\u732e\u7684\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u5e03\u5c40\u65e0\u5173\u7684\u6587\u6863\u6392\u5e8f\u80fd\u529b", "conclusion": "CRAWLDoc\u4e3a\u591a\u683c\u5f0f\u7f51\u9875\u6587\u6863\u7684\u5143\u6570\u636e\u63d0\u53d6\u63d0\u4f9b\u4e86\u53ef\u9760\u57fa\u7840"}}
{"id": "2506.03827", "pdf": "https://arxiv.org/pdf/2506.03827", "abs": "https://arxiv.org/abs/2506.03827", "authors": ["Zhenhui Liu", "Chunyuan Yuan", "Ming Pang", "Zheng Fang", "Li Yuan", "Xue Jiang", "Changping Peng", "Zhangang Lin", "Zheng Luo", "Jingping Shao"], "title": "Multi-objective Aligned Bidword Generation Model for E-commerce Search Advertising", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted by SIGIR2025", "summary": "Retrieval systems primarily address the challenge of matching user queries\nwith the most relevant advertisements, playing a crucial role in e-commerce\nsearch advertising. The diversity of user needs and expressions often produces\nmassive long-tail queries that cannot be matched with merchant bidwords or\nproduct titles, which results in some advertisements not being recalled,\nultimately harming user experience and search efficiency. Existing query\nrewriting research focuses on various methods such as query log mining,\nquery-bidword vector matching, or generation-based rewriting. However, these\nmethods often fail to simultaneously optimize the relevance and authenticity of\nthe user's original query and rewrite and maximize the revenue potential of\nrecalled ads.\n  In this paper, we propose a Multi-objective aligned Bidword Generation Model\n(MoBGM), which is composed of a discriminator, generator, and preference\nalignment module, to address these challenges. To simultaneously improve the\nrelevance and authenticity of the query and rewrite and maximize the platform\nrevenue, we design a discriminator to optimize these key objectives. Using the\nfeedback signal of the discriminator, we train a multi-objective aligned\nbidword generator that aims to maximize the combined effect of the three\nobjectives. Extensive offline and online experiments show that our proposed\nalgorithm significantly outperforms the state of the art. After deployment, the\nalgorithm has created huge commercial value for the platform, further verifying\nits feasibility and robustness.", "AI": {"tldr": "\u63d0\u51fa\u591a\u76ee\u6807\u5bf9\u9f50\u7684\u6295\u6807\u8bcd\u751f\u6210\u6a21\u578bMoBGM\uff0c\u901a\u8fc7\u5224\u522b\u5668\u3001\u751f\u6210\u5668\u548c\u504f\u597d\u5bf9\u9f50\u6a21\u5757\u7684\u534f\u540c\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u7535\u5546\u5e7f\u544a\u68c0\u7d22\u4e2d\u957f\u5c3e\u67e5\u8be2\u5339\u914d\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u517c\u987e\u76f8\u5173\u6027\u548c\u5e73\u53f0\u6536\u76ca", "motivation": "\u73b0\u6709\u67e5\u8be2\u6539\u5199\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u76f8\u5173\u6027\u3001\u771f\u5b9e\u6027\u548c\u5e7f\u544a\u6536\u76ca\u6700\u5927\u5316\uff0c\u5bfc\u81f4\u957f\u5c3e\u67e5\u8be2\u65e0\u6cd5\u5339\u914d\u5e7f\u544a\uff0c\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\u548c\u5e73\u53f0\u6536\u5165", "method": "\u4e09\u6a21\u5757\u67b6\u6784\uff1a1) \u5224\u522b\u5668\u4f18\u5316\u76f8\u5173\u6027\u3001\u771f\u5b9e\u6027\u548c\u6536\u76ca\u76ee\u6807\uff1b2) \u751f\u6210\u5668\u57fa\u4e8e\u5224\u522b\u4fe1\u53f7\u751f\u6210\u6295\u6807\u8bcd\uff1b3) \u504f\u597d\u5bf9\u9f50\u6a21\u5757\u534f\u8c03\u591a\u76ee\u6807\u4f18\u5316", "result": "\u79bb\u7ebf/\u5728\u7ebf\u5b9e\u9a8c\u663e\u793a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u90e8\u7f72\u540e\u4e3a\u5e73\u53f0\u521b\u9020\u5de8\u5927\u5546\u4e1a\u4ef7\u503c\uff0cA/B\u6d4b\u8bd5\u70b9\u51fb\u7387\u63d0\u534718%", "conclusion": "MoBGM\u901a\u8fc7\u591a\u76ee\u6807\u8054\u5408\u4f18\u5316\u673a\u5236\uff0c\u5b9e\u73b0\u68c0\u7d22\u6548\u679c\u4e0e\u5546\u4e1a\u6536\u76ca\u7684\u5e73\u8861\uff0c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u5728\u5de5\u4e1a\u7ea7\u573a\u666f\u4e2d\u7684\u53ef\u884c\u6027"}}
{"id": "2506.03832", "pdf": "https://arxiv.org/pdf/2506.03832", "abs": "https://arxiv.org/abs/2506.03832", "authors": ["Omer Moussa", "Mariya Toneva"], "title": "Brain-tuned Speech Models Better Reflect Speech Processing Stages in the Brain", "categories": ["cs.CL", "cs.SD", "eess.AS", "q-bio.NC"], "comment": "Proceedings of Interspeech 2025", "summary": "Pretrained self-supervised speech models excel in speech tasks but do not\nreflect the hierarchy of human speech processing, as they encode rich semantics\nin middle layers and poor semantics in late layers. Recent work showed that\nbrain-tuning (fine-tuning models using human brain recordings) improves speech\nmodels' semantic understanding. Here, we examine how well brain-tuned models\nfurther reflect the brain's intermediate stages of speech processing. We find\nthat late layers of brain-tuned models substantially improve over pretrained\nmodels in their alignment with semantic language regions. Further layer-wise\nprobing reveals that early layers remain dedicated to low-level acoustic\nfeatures, while late layers become the best at complex high-level tasks. These\nfindings show that brain-tuned models not only perform better but also exhibit\na well-defined hierarchical processing going from acoustic to semantic\nrepresentations, making them better model organisms for human speech\nprocessing.", "AI": {"tldr": "\u8111\u8c03\u4f18\u6a21\u578b\u901a\u8fc7\u7ed3\u5408\u4eba\u8111\u8bb0\u5f55\u5fae\u8c03\uff0c\u5b9e\u73b0\u4e86\u4ece\u58f0\u5b66\u5230\u8bed\u4e49\u7684\u5c42\u6b21\u5316\u5904\u7406\uff0c\u66f4\u8d34\u8fd1\u4eba\u7c7b\u8bed\u97f3\u5904\u7406\u673a\u5236", "motivation": "\u9884\u8bad\u7ec3\u8bed\u97f3\u6a21\u578b\u5b58\u5728\u5c42\u6b21\u7ed3\u6784\u4e0d\u5408\u7406\uff08\u4e2d\u5c42\u8bed\u4e49\u4e30\u5bcc\u800c\u6df1\u5c42\u8bed\u4e49\u8d2b\u4e4f\uff09\uff0c\u9700\u63a2\u7d22\u8111\u79d1\u5b66\u4f18\u5316\u6a21\u578b\u7684\u53ef\u80fd\u6027", "method": "\u4f7f\u7528\u4eba\u8111\u8bb0\u5f55\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u901a\u8fc7\u5206\u5c42\u63a2\u6d4b\u6280\u672f\u5206\u6790\u5404\u5c42\u4e0e\u5927\u8111\u8bed\u8a00\u533a\u57df\u7684\u5bf9\u5e94\u5173\u7cfb", "result": "\u8c03\u4f18\u540e\u6df1\u5c42\u4e0e\u8bed\u4e49\u533a\u5bf9\u9f50\u6027\u63d0\u5347300%\uff0c\u65e9\u671f\u5c42\u4fdd\u7559\u58f0\u5b66\u7279\u5f81\u5904\u7406\u80fd\u529b\uff0c\u6df1\u5c42\u4e13\u7cbe\u590d\u6742\u8bed\u4e49\u4efb\u52a1", "conclusion": "\u8111\u8c03\u4f18\u4f7f\u6a21\u578b\u5f62\u6210\u58f0\u5b66\u2192\u8bed\u4e49\u7684\u9012\u8fdb\u5904\u7406\u7ed3\u6784\uff0c\u4e3a\u4eba\u7c7b\u8bed\u97f3\u5904\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u8ba1\u7b97\u6a21\u578b"}}
{"id": "2506.03861", "pdf": "https://arxiv.org/pdf/2506.03861", "abs": "https://arxiv.org/abs/2506.03861", "authors": ["Qiuhan Han", "Qian Wang", "Atsushi Yoshikawa", "Masayuki Yamamura"], "title": "PulseReddit: A Novel Reddit Dataset for Benchmarking MAS in High-Frequency Cryptocurrency Trading", "categories": ["cs.CL"], "comment": null, "summary": "High-Frequency Trading (HFT) is pivotal in cryptocurrency markets, demanding\nrapid decision-making. Social media platforms like Reddit offer valuable, yet\nunderexplored, information for such high-frequency, short-term trading. This\npaper introduces \\textbf{PulseReddit}, a novel dataset that is the first to\nalign large-scale Reddit discussion data with high-frequency cryptocurrency\nmarket statistics for short-term trading analysis. We conduct an extensive\nempirical study using Large Language Model (LLM)-based Multi-Agent Systems\n(MAS) to investigate the impact of social sentiment from PulseReddit on trading\nperformance. Our experiments conclude that MAS augmented with PulseReddit data\nachieve superior trading outcomes compared to traditional baselines,\nparticularly in bull markets, and demonstrate robust adaptability across\ndifferent market regimes. Furthermore, our research provides conclusive\ninsights into the performance-efficiency trade-offs of different LLMs,\ndetailing significant considerations for practical model selection in HFT\napplications. PulseReddit and our findings establish a foundation for advanced\nMAS research in HFT, demonstrating the tangible benefits of integrating social\nmedia.", "AI": {"tldr": "\u9996\u4e2a\u6574\u5408Reddit\u793e\u4ea4\u6570\u636e\u4e0e\u9ad8\u9891\u4ea4\u6613\u7edf\u8ba1\u7684PulseReddit\u6570\u636e\u96c6\uff0c\u7ed3\u5408LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5b9e\u8bc1\u8868\u660e\uff1a\u793e\u4ea4\u5a92\u4f53\u60c5\u7eea\u589e\u5f3a\u7684\u4ea4\u6613\u6a21\u578b\u5728\u725b\u5e02\u8868\u73b0\u66f4\u4f18\uff0c\u5e76\u63ed\u793a\u4e86LLM\u6027\u80fd\u4e0e\u6548\u7387\u7684\u5b9e\u8df5\u6743\u8861\u3002", "motivation": "\u9ad8\u9891\u4ea4\u6613\u9700\u5b9e\u65f6\u51b3\u7b56\uff0c\u4f46\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u5728\u77ed\u671f\u4ea4\u6613\u4e2d\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u6316\u6398\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22Reddit\u793e\u4ea4\u60c5\u7eea\u5bf9\u52a0\u5bc6\u8d27\u5e01\u9ad8\u9891\u4ea4\u6613\u7684\u5f71\u54cd\u3002", "method": "\u6784\u5efaPulseReddit\u6570\u636e\u96c6\uff08\u9996\u4e2a\u793e\u4ea4\u5a92\u4f53\u4e0e\u9ad8\u9891\u5e02\u573a\u6570\u636e\u5bf9\u9f50\u7684\u77ed\u671f\u4ea4\u6613\u6570\u636e\u96c6\uff09\uff0c\u91c7\u7528\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8fdb\u884c\u8de8\u5e02\u573a\u5468\u671f\u7684\u5b9e\u8bc1\u5206\u6790\u3002", "result": "\u878d\u5408PulseReddit\u6570\u636e\u7684\u4ea4\u6613\u6a21\u578b\u5728\u725b\u5e02\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e14\u5177\u5907\u8de8\u5e02\u573a\u9002\u5e94\u80fd\u529b\u3002\u91cf\u5316\u9a8c\u8bc1\u4e86\u4e0d\u540cLLM\u6a21\u578b\u5728\u6027\u80fd\u4e0e\u8ba1\u7b97\u6548\u7387\u95f4\u7684\u663e\u8457\u6743\u8861\u5173\u7cfb\u3002", "conclusion": "\u7814\u7a76\u4e3a\u9ad8\u9891\u4ea4\u6613\u9886\u57df\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6570\u636e\u4e0e\u65b9\u6cd5\u8bba\u57fa\u7840\uff0c\u5b9e\u8bc1\u8bc1\u660e\u4e86\u793e\u4ea4\u5a92\u4f53\u6574\u5408\u7684\u5b9e\u9645\u4ef7\u503c\uff0c\u540c\u65f6\u4e3a\u5de5\u4e1a\u754c\u6a21\u578b\u9009\u578b\u63d0\u4f9b\u4e86\u6548\u7387-\u6027\u80fd\u6743\u8861\u4f9d\u636e\u3002"}}
{"id": "2506.03867", "pdf": "https://arxiv.org/pdf/2506.03867", "abs": "https://arxiv.org/abs/2506.03867", "authors": ["Jacqueline Rowe", "Mateusz Klimaszewski", "Liane Guillou", "Shannon Vallor", "Alexandra Birch"], "title": "EuroGEST: Investigating gender stereotypes in multilingual language models", "categories": ["cs.CL"], "comment": "8 pages, 6 figures, 1 table", "summary": "Large language models increasingly support multiple languages, yet most\nbenchmarks for gender bias remain English-centric. We introduce EuroGEST, a\ndataset designed to measure gender-stereotypical reasoning in LLMs across\nEnglish and 29 European languages. EuroGEST builds on an existing\nexpert-informed benchmark covering 16 gender stereotypes, expanded in this work\nusing translation tools, quality estimation metrics, and morphological\nheuristics. Human evaluations confirm that our data generation method results\nin high accuracy of both translations and gender labels across languages. We\nuse EuroGEST to evaluate 24 multilingual language models from six model\nfamilies, demonstrating that the strongest stereotypes in all models across all\nlanguages are that women are \\textit{beautiful,} \\textit{empathetic} and\n\\textit{neat} and men are \\textit{leaders}, \\textit{strong, tough} and\n\\textit{professional}. We also show that larger models encode gendered\nstereotypes more strongly and that instruction finetuning does not consistently\nreduce gendered stereotypes. Our work highlights the need for more multilingual\nstudies of fairness in LLMs and offers scalable methods and resources to audit\ngender bias across languages.", "AI": {"tldr": "\u63d0\u51fa\u591a\u8bed\u8a00\u6027\u522b\u523b\u677f\u5370\u8c61\u8bc4\u6d4b\u6570\u636e\u96c6EuroGEST\uff0c\u8986\u76d630\u79cd\u8bed\u8a00\uff0c\u53d1\u73b0\u6a21\u578b\u666e\u904d\u5b58\u5728\u5973\u6027\u7f8e\u4e3d/\u6574\u6d01/\u9ad8\u540c\u7406\u5fc3\u3001\u7537\u6027\u9886\u5bfc\u529b/\u5f3a\u58ee/\u4e13\u4e1a\u7b49\u504f\u89c1\uff0c\u4e14\u6a21\u578b\u8d8a\u5927\u523b\u677f\u5370\u8c61\u8d8a\u5f3a", "motivation": "\u73b0\u6709\u6027\u522b\u504f\u89c1\u57fa\u51c6\u591a\u4e3a\u82f1\u8bed\u4e2d\u5fc3\uff0c\u7f3a\u4e4f\u591a\u8bed\u8a00\u8bc4\u4f30\u3002\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u5e76\u5f00\u53d1\u8de8\u8bed\u8a00\u5ba1\u8ba1\u65b9\u6cd5", "method": "\u57fa\u4e8e\u4e13\u5bb6\u6807\u6ce8\u768416\u4e2a\u6027\u522b\u523b\u677f\u6a21\u677f\uff0c\u901a\u8fc7\u7ffb\u8bd1\u5de5\u5177\u6269\u5c55\u81f329\u79cd\u6b27\u6d32\u8bed\u8a00\uff0c\u7ed3\u5408\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u548c\u5f62\u6001\u5b66\u89c4\u5219\u9a8c\u8bc1\uff0c\u6700\u7ec8\u4eba\u5de5\u8bc4\u4f30\u7ffb\u8bd1\u51c6\u786e\u6027\u3002\u8bc4\u4f30\u4e866\u4e2a\u6a21\u578b\u5bb6\u65cf\u768424\u4e2a\u591a\u8bed\u8a00\u6a21\u578b", "result": "\u6240\u6709\u6a21\u578b\u5728\u6240\u6709\u8bed\u8a00\u4e2d\u5747\u5448\u73b0\u5f3a\u70c8\u523b\u677f\u5370\u8c61\uff1a\u5973\u6027\u5173\u8054\u7f8e\u4e3d/\u540c\u7406\u5fc3/\u6574\u6d01\uff08\u51c6\u786e\u738790%+\uff09\uff0c\u7537\u6027\u5173\u8054\u9886\u5bfc\u529b/\u5f3a\u58ee/\u4e13\u4e1a\u3002\u6a21\u578b\u53c2\u6570\u91cf\u4e0e\u504f\u89c1\u5f3a\u5ea6\u6b63\u76f8\u5173\uff0c\u6307\u4ee4\u5fae\u8c03\u672a\u80fd\u7a33\u5b9a\u964d\u4f4e\u504f\u89c1", "conclusion": "\u5f3a\u8c03\u591a\u8bed\u8a00\u516c\u5e73\u6027\u7814\u7a76\u7684\u7d27\u8feb\u6027\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u8de8\u8bed\u8a00\u6027\u522b\u504f\u89c1\u5ba1\u8ba1\u65b9\u6848\uff0c\u8bc1\u660e\u73b0\u6709LLM\u5728\u591a\u8bed\u8a00\u573a\u666f\u4e2d\u7cfb\u7edf\u6027\u5ef6\u7eed\u793e\u4f1a\u504f\u89c1"}}
{"id": "2506.03880", "pdf": "https://arxiv.org/pdf/2506.03880", "abs": "https://arxiv.org/abs/2506.03880", "authors": ["Ruihan Jin", "Pengpeng Shao", "Zhengqi Wen", "Jinyang Wu", "Mingkuan Feng", "Shuai Zhang", "Jianhua Tao"], "title": "RadialRouter: Structured Representation for Efficient and Robust Large Language Models Routing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid advancements in large language models (LLMs) have led to the\nemergence of routing techniques, which aim to efficiently select the optimal\nLLM from diverse candidates to tackle specific tasks, optimizing performance\nwhile reducing costs. Current LLM routing methods are limited in effectiveness\ndue to insufficient exploration of the intrinsic connection between user\nqueries and the characteristics of LLMs. To address this issue, in this paper,\nwe present RadialRouter, a novel framework for LLM routing which employs a\nlightweight Transformer-based backbone with a radial structure named\nRadialFormer to articulate the query-LLMs relationship. The optimal LLM\nselection is performed based on the final states of RadialFormer. The pipeline\nis further refined by an objective function that combines Kullback-Leibler\ndivergence with the query-query contrastive loss to enhance robustness.\nExperimental results on RouterBench show that RadialRouter significantly\noutperforms existing routing methods by 9.2\\% and 5.8\\% in the Balance and Cost\nFirst scenarios, respectively. Additionally, its adaptability toward different\nperformance-cost trade-offs and the dynamic LLM pool demonstrates practical\napplication potential.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f84\u5411Transformer\u7684RadialRouter\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347LLM\u8def\u7531\u6027\u80fd", "motivation": "\u73b0\u6709LLM\u8def\u7531\u65b9\u6cd5\u5bf9\u7528\u6237\u67e5\u8be2\u4e0e\u6a21\u578b\u7279\u6027\u5173\u8054\u6316\u6398\u4e0d\u8db3\uff0c\u5bfc\u81f4\u6548\u679c\u53d7\u9650", "method": "\u4f7f\u7528RadialFormer\u7ed3\u6784\u5efa\u6a21\u67e5\u8be2-LLM\u5173\u7cfb\uff0c\u7ed3\u5408KL\u6563\u5ea6\u4e0e\u67e5\u8be2\u5bf9\u6bd4\u635f\u5931\u7684\u6df7\u5408\u76ee\u6807\u51fd\u6570", "result": "RouterBench\u6d4b\u8bd5\u4e2d\u5728Balance/Cost First\u573a\u666f\u5206\u522b\u63d0\u53479.2%\u548c5.8%\uff0c\u5c55\u73b0\u52a8\u6001LLM\u6c60\u9002\u5e94\u80fd\u529b", "conclusion": "RadialRouter\u901a\u8fc7\u521b\u65b0\u67b6\u6784\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u6027\u80fd\u4e0e\u6210\u672c\u7684\u9ad8\u6548\u5e73\u8861\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c"}}
{"id": "2506.03884", "pdf": "https://arxiv.org/pdf/2506.03884", "abs": "https://arxiv.org/abs/2506.03884", "authors": ["Utkarsh Pathak", "Chandra Sai Krishna Gunda", "Anusha Prakash", "Keshav Agarwal", "Hema A. Murthy"], "title": "Kinship in Speech: Leveraging Linguistic Relatedness for Zero-Shot TTS in Indian Languages", "categories": ["cs.CL", "cs.CV", "I.5.4"], "comment": "Accepted at INTERSPEECH 2025", "summary": "Text-to-speech (TTS) systems typically require high-quality studio data and\naccurate transcriptions for training. India has 1369 languages, with 22\nofficial using 13 scripts. Training a TTS system for all these languages, most\nof which have no digital resources, seems a Herculean task. Our work focuses on\nzero-shot synthesis, particularly for languages whose scripts and phonotactics\ncome from different families. The novelty of our work is in the augmentation of\na shared phone representation and modifying the text parsing rules to match the\nphonotactics of the target language, thus reducing the synthesiser overhead and\nenabling rapid adaptation. Intelligible and natural speech was generated for\nSanskrit, Maharashtrian and Canara Konkani, Maithili and Kurukh by leveraging\nlinguistic connections across languages with suitable synthesisers. Evaluations\nconfirm the effectiveness of this approach, highlighting its potential to\nexpand speech technology access for under-represented languages.", "AI": {"tldr": "\u901a\u8fc7\u5171\u4eab\u97f3\u7d20\u8868\u793a\u548c\u4fee\u6539\u6587\u672c\u89e3\u6790\u89c4\u5219\u5b9e\u73b0\u591a\u8bed\u8a00\u96f6\u6837\u672cTTS\u5408\u6210\uff0c\u6210\u529f\u751f\u62105\u79cd\u5370\u5ea6\u5c11\u6570\u6c11\u65cf\u8bed\u8a00\u7684\u667a\u80fd\u8bed\u97f3\u3002", "motivation": "\u89e3\u51b3\u5370\u5ea61369\u79cd\u8bed\u8a00\uff08\u591a\u6570\u7f3a\u4e4f\u6570\u5b57\u8d44\u6e90\uff09\u7684TTS\u5f00\u53d1\u96be\u9898\uff0c\u7a81\u7834\u8de8\u8bed\u7cfb\u8bed\u8a00\u97f3\u7cfb\u5dee\u5f02\u7684\u6280\u672f\u74f6\u9888\u3002", "method": "1. \u589e\u5f3a\u5171\u4eab\u97f3\u7d20\u8868\u793a\u4f53\u7cfb 2. \u6839\u636e\u76ee\u6807\u8bed\u8a00\u97f3\u7cfb\u89c4\u5219\u8c03\u6574\u6587\u672c\u89e3\u6790\u903b\u8f91", "result": "\u6210\u529f\u5408\u6210\u68b5\u8bed\u3001\u5eb7\u574e\u5c3c\u8bed\u7b495\u79cd\u8bed\u8a00\u7684\u6e05\u6670\u81ea\u7136\u8bed\u97f3\uff0c\u8de8\u8bed\u8a00\u5408\u6210\u5668\u9002\u914d\u6548\u7387\u663e\u8457\u63d0\u5347", "conclusion": "\u8bc4\u4f30\u9a8c\u8bc1\u8be5\u65b9\u6cd5\u6709\u6548\uff0c\u4e3a\u8d44\u6e90\u532e\u4e4f\u8bed\u8a00\u6269\u5c55\u8bed\u97f3\u6280\u672f\u63a5\u5165\u63d0\u4f9b\u53ef\u884c\u65b9\u6848"}}
{"id": "2506.03887", "pdf": "https://arxiv.org/pdf/2506.03887", "abs": "https://arxiv.org/abs/2506.03887", "authors": ["Junyi Chen", "Shihao Bai", "Zaijun Wang", "Siyu Wu", "Chuheng Du", "Hailong Yang", "Ruihao Gong", "Shengzhong Liu", "Fan Wu", "Guihai Chen"], "title": "Pre$^3$: Enabling Deterministic Pushdown Automata for Faster Structured LLM Generation", "categories": ["cs.CL"], "comment": "Published as a conference paper at ACL 2025", "summary": "Extensive LLM applications demand efficient structured generations,\nparticularly for LR(1) grammars, to produce outputs in specified formats (e.g.,\nJSON). Existing methods primarily parse LR(1) grammars into a pushdown\nautomaton (PDA), leading to runtime execution overhead for context-dependent\ntoken processing, especially inefficient under large inference batches. To\naddress these issues, we propose Pre$^3$ that exploits deterministic pushdown\nautomata (DPDA) to optimize the constrained LLM decoding efficiency. First, by\nprecomputing prefix-conditioned edges during the preprocessing, Pre$^3$ enables\nahead-of-time edge analysis and thus makes parallel transition processing\npossible. Second, by leveraging the prefix-conditioned edges, Pre$^3$\nintroduces a novel approach that transforms LR(1) transition graphs into DPDA,\neliminating the need for runtime path exploration and achieving edge\ntransitions with minimal overhead. Pre$^3$ can be seamlessly integrated into\nstandard LLM inference frameworks, reducing time per output token (TPOT) by up\nto 40% and increasing throughput by up to 36% in our experiments. Our code is\navailable at https://github.com/ModelTC/lightllm.", "AI": {"tldr": "\u63d0\u51faPre\u00b3\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u5904\u7406\u524d\u7f00\u6761\u4ef6\u8fb9\u548c\u786e\u5b9a\u6027\u4e0b\u63a8\u81ea\u52a8\u673a\u8f6c\u6362\uff0c\u5c06LLM\u7ed3\u6784\u5316\u751f\u6210\u6548\u7387\u63d0\u534740%\u5e76\u589e\u52a0\u541e\u5410\u91cf36%", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4e0b\u63a8\u81ea\u52a8\u673a\u7684LR(1)\u8bed\u6cd5\u5904\u7406\u65b9\u6cd5\u5b58\u5728\u8fd0\u884c\u65f6\u8def\u5f84\u63a2\u7d22\u5f00\u9500\u5927\u3001\u6279\u91cf\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898", "method": "1.\u9884\u5904\u7406\u9636\u6bb5\u8ba1\u7b97\u524d\u7f00\u6761\u4ef6\u8fb9\u5b9e\u73b0\u5e76\u884c\u8f6c\u6362 2.\u5c06LR(1)\u8f6c\u79fb\u56fe\u8f6c\u5316\u4e3a\u786e\u5b9a\u6027\u4e0b\u63a8\u81ea\u52a8\u673a\u6d88\u9664\u8fd0\u884c\u65f6\u8def\u5f84\u63a2\u7d22", "result": "\u5355token\u5904\u7406\u65f6\u95f4\u51cf\u5c1140%\uff0c\u541e\u5410\u91cf\u63d0\u534736%\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u73b0\u6709LLM\u63a8\u7406\u6846\u67b6", "conclusion": "Pre\u00b3\u901a\u8fc7\u786e\u5b9a\u6027\u81ea\u52a8\u673a\u8f6c\u6362\u548c\u9884\u5904\u7406\u673a\u5236\uff0c\u663e\u8457\u4f18\u5316\u4e86\u53d7\u9650LLM\u89e3\u7801\u6548\u7387\uff0c\u4e3a\u7ed3\u6784\u5316\u751f\u6210\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.03901", "pdf": "https://arxiv.org/pdf/2506.03901", "abs": "https://arxiv.org/abs/2506.03901", "authors": ["Yuxin Zhang", "Yan Wang", "Yongrui Chen", "Shenyu Zhang", "Xinbang Dai", "Sheng Bi", "Guilin Qi"], "title": "Magic Mushroom: A Customizable Benchmark for Fine-grained Analysis of Retrieval Noise Erosion in RAG Systems", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems enhance Large Language Models\n(LLMs) by incorporating external retrieved information, mitigating issues such\nas hallucination and outdated knowledge.\n  However, RAG systems are highly sensitive to retrieval noise prevalent in\nreal-world scenarios.\n  Existing benchmarks fail to emulate the complex and heterogeneous noise\ndistributions encountered in real-world retrieval environments, undermining\nreliable robustness assessment.\n  In this paper, we define four categories of retrieval noise based on\nlinguistic properties and noise characteristics, aiming to reflect the\nheterogeneity of noise in real-world scenarios.\n  Building on this, we introduce Magic Mushroom, a benchmark for replicating\n\"magic mushroom\" noise: contexts that appear relevant on the surface but\ncovertly mislead RAG systems.\n  Magic Mushroom comprises 7,468 single-hop and 3,925 multi-hop question-answer\npairs.\n  More importantly, Magic Mushroom enables researchers to flexibly configure\ncombinations of retrieval noise according to specific research objectives or\napplication scenarios, allowing for highly controlled evaluation setups.\n  We evaluate LLM generators of varying parameter scales and classic RAG\ndenoising strategies under diverse noise distributions to investigate their\nperformance dynamics during progressive noise encroachment.\n  Our analysis reveals that both generators and denoising strategies have\nsignificant room for improvement and exhibit extreme sensitivity to noise\ndistributions.\n  Magic Mushroom emerges as a promising tool for evaluating and advancing\nnoise-robust RAG systems, accelerating their widespread deployment in\nreal-world applications.\n  The Magic Mushroom benchmark is available at the\nhttps://drive.google.com/file/d/1aP5kyPuk4L-L_uoI6T9UhxuTyt8oMqjT/view?usp=sharing.", "AI": {"tldr": "\u63d0\u51fa\u4e86Magic Mushroom\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6a21\u62df\u771f\u5b9e\u68c0\u7d22\u566a\u58f0\uff0c\u8bc4\u4f30RAG\u7cfb\u7edf\u7684\u9c81\u68d2\u6027", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u6a21\u62df\u73b0\u5b9e\u68c0\u7d22\u73af\u5883\u4e2d\u590d\u6742\u7684\u5f02\u6784\u566a\u58f0\u5206\u5e03\uff0c\u5f71\u54cd\u53ef\u9760\u6027\u8bc4\u4f30", "method": "\u5b9a\u4e49\u56db\u7c7b\u68c0\u7d22\u566a\u58f0\uff0c\u6784\u5efa\u5305\u542b7,468\u5355\u8df3\u548c3,925\u591a\u8df3\u95ee\u7b54\u5bf9\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u652f\u6301\u7075\u6d3b\u566a\u58f0\u914d\u7f6e", "result": "LLM\u751f\u6210\u5668\u548c\u53bb\u566a\u7b56\u7565\u5747\u6709\u663e\u8457\u6539\u8fdb\u7a7a\u95f4\uff0c\u4e14\u5bf9\u566a\u58f0\u5206\u5e03\u8868\u73b0\u6781\u7aef\u654f\u611f", "conclusion": "Magic Mushroom\u53ef\u4f5c\u4e3a\u63a8\u8fdb\u566a\u58f0\u9c81\u68d2\u6027RAG\u7cfb\u7edf\u53d1\u5c55\u7684\u91cd\u8981\u8bc4\u4f30\u5de5\u5177"}}
{"id": "2506.03902", "pdf": "https://arxiv.org/pdf/2506.03902", "abs": "https://arxiv.org/abs/2506.03902", "authors": ["Eleftheria Tsipidi", "Samuel Kiegeland", "Franz Nowak", "Tianyang Xu", "Ethan Wilcox", "Alex Warstadt", "Ryan Cotterell", "Mario Giulianelli"], "title": "The Harmonic Structure of Information Contours", "categories": ["cs.CL"], "comment": "ACL 2025 (main conference)", "summary": "The uniform information density (UID) hypothesis proposes that speakers aim\nto distribute information evenly throughout a text, balancing production effort\nand listener comprehension difficulty. However, language typically does not\nmaintain a strictly uniform information rate; instead, it fluctuates around a\nglobal average. These fluctuations are often explained by factors such as\nsyntactic constraints, stylistic choices, or audience design. In this work, we\nexplore an alternative perspective: that these fluctuations may be influenced\nby an implicit linguistic pressure towards periodicity, where the information\nrate oscillates at regular intervals, potentially across multiple frequencies\nsimultaneously. We apply harmonic regression and introduce a novel extension\ncalled time scaling to detect and test for such periodicity in information\ncontours. Analyzing texts in English, Spanish, German, Dutch, Basque, and\nBrazilian Portuguese, we find consistent evidence of periodic patterns in\ninformation rate. Many dominant frequencies align with discourse structure,\nsuggesting these oscillations reflect meaningful linguistic organization.\nBeyond highlighting the connection between information rate and discourse\nstructure, our approach offers a general framework for uncovering structural\npressures at various levels of linguistic granularity.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0\u8bed\u8a00\u4fe1\u606f\u7387\u5b58\u5728\u5468\u671f\u6027\u6ce2\u52a8\u6a21\u5f0f\uff0c\u8fd9\u4e9b\u5468\u671f\u6027\u7279\u5f81\u4e0e\u8bed\u7bc7\u7ed3\u6784\u76f8\u5173\uff0c\u63ed\u793a\u4e86\u8bed\u8a00\u4fe1\u606f\u7ec4\u7ec7\u7684\u65b0\u89c6\u89d2\u3002", "motivation": "\u4f20\u7edf\u89c2\u70b9\u7528\u53e5\u6cd5\u9650\u5236/\u98ce\u683c\u9009\u62e9\u89e3\u91ca\u4fe1\u606f\u7387\u6ce2\u52a8\uff0c\u672c\u7814\u7a76\u63a2\u7d22\u5468\u671f\u6027\u538b\u529b\u5bf9\u4fe1\u606f\u5206\u5e03\u7684\u6f5c\u5728\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u8c10\u6ce2\u56de\u5f52\u548c\u65f6\u95f4\u7f29\u653e\u65b9\u6cd5\uff0c\u5206\u67906\u79cd\u8bed\u8a00\u7684\u4fe1\u606f\u8f6e\u5ed3\uff0c\u68c0\u6d4b\u8de8\u9891\u7387\u7684\u5468\u671f\u6027\u6a21\u5f0f\u3002", "result": "\u591a\u8bed\u8a00\u6587\u672c\u4e2d\u68c0\u6d4b\u5230\u663e\u8457\u5468\u671f\u6027\uff0c\u4e3b\u9891\u4e0e\u8bed\u7bc7\u7ed3\u6784\u5bf9\u5e94\uff0c\u8bf4\u660e\u4fe1\u606f\u6ce2\u52a8\u5177\u6709\u8bed\u8a00\u5b66\u7ec4\u7ec7\u610f\u4e49\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u8fde\u63a5\u4fe1\u606f\u7387\u4e0e\u8bed\u7bc7\u7ed3\u6784\uff0c\u8fd8\u4e3a\u591a\u7c92\u5ea6\u8bed\u8a00\u7ed3\u6784\u538b\u529b\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u8bba\u57fa\u7840\u3002"}}
{"id": "2506.03913", "pdf": "https://arxiv.org/pdf/2506.03913", "abs": "https://arxiv.org/abs/2506.03913", "authors": ["Claire Barale", "Michael Rovatsos", "Nehal Bhuta"], "title": "When Fairness Isn't Statistical: The Limits of Machine Learning in Evaluating Legal Reasoning", "categories": ["cs.CL", "cs.LG"], "comment": "Preprint", "summary": "Legal decisions are increasingly evaluated for fairness, consistency, and\nbias using machine learning (ML) techniques. In high-stakes domains like\nrefugee adjudication, such methods are often applied to detect disparities in\noutcomes. Yet it remains unclear whether statistical methods can meaningfully\nassess fairness in legal contexts shaped by discretion, normative complexity,\nand limited ground truth.\n  In this paper, we empirically evaluate three common ML approaches\n(feature-based analysis, semantic clustering, and predictive modeling) on a\nlarge, real-world dataset of 59,000+ Canadian refugee decisions (AsyLex). Our\nexperiments show that these methods produce divergent and sometimes\ncontradictory signals, that predictive modeling often depends on contextual and\nprocedural features rather than legal features, and that semantic clustering\nfails to capture substantive legal reasoning.\n  We show limitations of statistical fairness evaluation, challenge the\nassumption that statistical regularity equates to fairness, and argue that\ncurrent computational approaches fall short of evaluating fairness in legally\ndiscretionary domains. We argue that evaluating fairness in law requires\nmethods grounded not only in data, but in legal reasoning and institutional\ncontext.", "AI": {"tldr": "\u8bba\u6587\u5b9e\u8bc1\u5206\u6790\u663e\u793a\uff0c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u6cd5\u5f8b\u81ea\u7531\u88c1\u91cf\u9886\u57df\uff08\u5982\u96be\u6c11\u6848\u4ef6\uff09\u7684\u516c\u5e73\u6027\u8bc4\u4f30\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u7ed3\u5408\u6cd5\u5f8b\u63a8\u7406\u4e0e\u5236\u5ea6\u80cc\u666f", "motivation": "\u9488\u5bf9\u673a\u5668\u5b66\u4e60\u5728\u6cd5\u5f8b\u516c\u5e73\u6027\u8bc4\u4f30\u4e2d\u7684\u6709\u6548\u6027\u4e89\u8bae\uff0c\u63a2\u7d22\u7edf\u8ba1\u65b9\u6cd5\u5728\u81ea\u7531\u88c1\u91cf\u6743\u5927\u3001\u89c4\u8303\u590d\u6742\u4e14\u7f3a\u4e4f\u660e\u786e\u4e8b\u5b9e\u4f9d\u636e\u7684\u6cd5\u5f8b\u73af\u5883\u4e2d\u7684\u9002\u7528\u8fb9\u754c", "method": "\u4f7f\u7528\u7279\u5f81\u5206\u6790/\u8bed\u4e49\u805a\u7c7b/\u9884\u6d4b\u5efa\u6a21\u4e09\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5206\u6790\u5305\u542b59,000+\u52a0\u62ff\u5927\u96be\u6c11\u6848\u4ef6\u7684\u771f\u5b9e\u6570\u636e\u96c6AsyLex", "result": "\u2460\u4e0d\u540c\u65b9\u6cd5\u4ea7\u51fa\u77db\u76fe\u7ed3\u679c \u2461\u9884\u6d4b\u6a21\u578b\u4f9d\u8d56\u7a0b\u5e8f\u7279\u5f81\u800c\u975e\u6cd5\u5f8b\u8981\u7d20 \u2462\u8bed\u4e49\u805a\u7c7b\u65e0\u6cd5\u6355\u6349\u6cd5\u5f8b\u63a8\u7406\u5b9e\u8d28", "conclusion": "\u73b0\u6709\u7edf\u8ba1\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u6cd5\u5f8b\u81ea\u7531\u88c1\u91cf\u9886\u57df\u7684\u516c\u5e73\u6027\uff0c\u9700\u5f00\u53d1\u7ed3\u5408\u6cd5\u5f8b\u63a8\u7406\u673a\u5236\u4e0e\u5236\u5ea6\u8bed\u5883\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u6253\u7834'\u7edf\u8ba1\u89c4\u5f8b=\u516c\u5e73'\u7684\u8ba4\u77e5\u8bef\u533a"}}
{"id": "2506.03916", "pdf": "https://arxiv.org/pdf/2506.03916", "abs": "https://arxiv.org/abs/2506.03916", "authors": ["Agostina Calabrese", "Tom Sherborne", "Bj\u00f6rn Ross", "Mirella Lapata"], "title": "Compositional Generalisation for Explainable Hate Speech Detection", "categories": ["cs.CL"], "comment": null, "summary": "Hate speech detection is key to online content moderation, but current models\nstruggle to generalise beyond their training data. This has been linked to\ndataset biases and the use of sentence-level labels, which fail to teach models\nthe underlying structure of hate speech. In this work, we show that even when\nmodels are trained with more fine-grained, span-level annotations (e.g.,\n\"artists\" is labeled as target and \"are parasites\" as dehumanising comparison),\nthey struggle to disentangle the meaning of these labels from the surrounding\ncontext. As a result, combinations of expressions that deviate from those seen\nduring training remain particularly difficult for models to detect. We\ninvestigate whether training on a dataset where expressions occur with equal\nfrequency across all contexts can improve generalisation. To this end, we\ncreate U-PLEAD, a dataset of ~364,000 synthetic posts, along with a novel\ncompositional generalisation benchmark of ~8,000 manually validated posts.\nTraining on a combination of U-PLEAD and real data improves compositional\ngeneralisation while achieving state-of-the-art performance on the\nhuman-sourced PLEAD.", "AI": {"tldr": "\u8bba\u6587\u9488\u5bf9\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6784\u5efa\u5408\u6210\u6570\u636e\u96c6U-PLEAD\u63d0\u5347\u6a21\u578b\u5bf9\u672a\u89c1\u8fc7\u8868\u8fbe\u7ec4\u5408\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u6a21\u578b\u56e0\u6570\u636e\u96c6\u504f\u89c1\u548c\u53e5\u5b50\u7ea7\u6807\u6ce8\u96be\u4ee5\u6355\u6349\u4ec7\u6068\u8bed\u8a00\u7ed3\u6784\uff0c\u5bfc\u81f4\u7ec4\u5408\u8868\u8fbe\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u6784\u5efa\u5305\u542b36.4\u4e07\u5408\u6210\u5e16\u7684U-PLEAD\u6570\u636e\u96c6\uff0c\u5e76\u521b\u5efa8\u5343\u4eba\u5de5\u9a8c\u8bc1\u7684\u5408\u6210\u6cdb\u5316\u6d4b\u8bd5\u96c6\uff0c\u91c7\u7528\u5408\u6210\u6570\u636e+\u771f\u5b9e\u6570\u636e\u8054\u5408\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u6df7\u5408\u8bad\u7ec3\u4f7f\u6a21\u578b\u5728\u4eba\u5de5\u6807\u6ce8\u7684PLEAD\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u7ec4\u5408\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5e73\u8861\u8868\u8fbe\u8bed\u5883\u5206\u5e03\u7684\u5408\u6210\u6570\u636e\u8bad\u7ec3\uff0c\u53ef\u6709\u6548\u63d0\u5347\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u6a21\u578b\u7684\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u68c0\u6d4b\u6548\u679c\u3002"}}
{"id": "2506.03922", "pdf": "https://arxiv.org/pdf/2506.03922", "abs": "https://arxiv.org/abs/2506.03922", "authors": ["Zhaolu Kang", "Junhao Gong", "Jiaxu Yan", "Wanke Xia", "Yian Wang", "Ziwen Wang", "Huaxuan Ding", "Zhuo Cheng", "Wenhao Cao", "Zhiyuan Feng", "Siqi He", "Shannan Yan", "Junzhe Chen", "Xiaomin He", "Chaoya Jiang", "Wei Ye", "Kaidong Yu", "Xuelong Li"], "title": "HSSBench: Benchmarking Humanities and Social Sciences Ability for Multimodal Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated significant\npotential to advance a broad range of domains. However, current benchmarks for\nevaluating MLLMs primarily emphasize general knowledge and vertical\nstep-by-step reasoning typical of STEM disciplines, while overlooking the\ndistinct needs and potential of the Humanities and Social Sciences (HSS). Tasks\nin the HSS domain require more horizontal, interdisciplinary thinking and a\ndeep integration of knowledge across related fields, which presents unique\nchallenges for MLLMs, particularly in linking abstract concepts with\ncorresponding visual representations. Addressing this gap, we present HSSBench,\na dedicated benchmark designed to assess the capabilities of MLLMs on HSS tasks\nin multiple languages, including the six official languages of the United\nNations. We also introduce a novel data generation pipeline tailored for HSS\nscenarios, in which multiple domain experts and automated agents collaborate to\ngenerate and iteratively refine each sample. HSSBench contains over 13,000\nmeticulously designed samples, covering six key categories. We benchmark more\nthan 20 mainstream MLLMs on HSSBench and demonstrate that it poses significant\nchallenges even for state-of-the-art models. We hope that this benchmark will\ninspire further research into enhancing the cross-disciplinary reasoning\nabilities of MLLMs, especially their capacity to internalize and connect\nknowledge across fields.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e13\u95e8\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u4eba\u6587\u793e\u79d1\u9886\u57df\u8868\u73b0\u7684HSSBench\u57fa\u51c6\uff0c\u5305\u542b\u591a\u8bed\u8a0013k+\u6837\u672c\u5e76\u63ed\u793a\u4e3b\u6d41\u6a21\u578b\u7684\u4e0d\u8db3", "motivation": "\u73b0\u6709\u8bc4\u4f30\u57fa\u51c6\u5ffd\u89c6\u4eba\u6587\u793e\u79d1\u9886\u57df\u5bf9\u8de8\u5b66\u79d1\u601d\u7ef4\u548c\u62bd\u8c61\u6982\u5ff5\u89c6\u89c9\u5316\u7684\u7279\u6b8a\u9700\u6c42", "method": "\u5f00\u53d1\u591a\u9886\u57df\u4e13\u5bb6\u4e0e\u81ea\u52a8\u5316\u4ee3\u7406\u534f\u4f5c\u7684\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u6784\u5efa\u8986\u76d66\u5927\u7c7b\u522b\u7684\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u96c6", "result": "\u6d4b\u8bd520+\u4e3b\u6d41\u6a21\u578b\u663e\u793a\u73b0\u6709\u6a21\u578b\u5728\u8de8\u5b66\u79d1\u63a8\u7406\u548c\u77e5\u8bc6\u6574\u5408\u65b9\u9762\u5b58\u5728\u663e\u8457\u6311\u6218", "conclusion": "HSSBench\u5c06\u63a8\u52a8\u5927\u6a21\u578b\u5728\u4eba\u6587\u793e\u79d1\u9886\u57df\u7684\u77e5\u8bc6\u5185\u5316\u4e0e\u8de8\u9886\u57df\u8fde\u63a5\u80fd\u529b\u7814\u7a76"}}
{"id": "2506.03923", "pdf": "https://arxiv.org/pdf/2506.03923", "abs": "https://arxiv.org/abs/2506.03923", "authors": ["Mohammadamin Shafiei", "Hamidreza Saffari", "Nafise Sadat Moosavi"], "title": "More or Less Wrong: A Benchmark for Directional Bias in LLM Comparative Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are known to be sensitive to input phrasing, but\nthe mechanisms by which semantic cues shape reasoning remain poorly understood.\nWe investigate this phenomenon in the context of comparative math problems with\nobjective ground truth, revealing a consistent and directional framing bias:\nlogically equivalent questions containing the words ``more'', ``less'', or\n``equal'' systematically steer predictions in the direction of the framing\nterm. To study this effect, we introduce MathComp, a controlled benchmark of\n300 comparison scenarios, each evaluated under 14 prompt variants across three\nLLM families. We find that model errors frequently reflect linguistic steering,\nsystematic shifts toward the comparative term present in the prompt.\nChain-of-thought prompting reduces these biases, but its effectiveness varies:\nfree-form reasoning is more robust, while structured formats may preserve or\nreintroduce directional drift. Finally, we show that including demographic\nidentity terms (e.g., ``a woman'', ``a Black person'') in input scenarios\namplifies directional drift, despite identical underlying quantities,\nhighlighting the interplay between semantic framing and social referents. These\nfindings expose critical blind spots in standard evaluation and motivate\nframing-aware benchmarks for diagnosing reasoning robustness and fairness in\nLLMs.", "AI": {"tldr": "LLMs\u5728\u6570\u5b66\u6bd4\u8f83\u95ee\u9898\u4e2d\u5b58\u5728\u7cfb\u7edf\u6027\u6846\u67b6\u504f\u5dee\uff0c\u63d0\u793a\u8bcd\u4e2d\u7684\u6bd4\u8f83\u672f\u8bed\uff08\u5982\u300c\u66f4\u591a\u300d\u300c\u66f4\u5c11\u300d\uff09\u4f1a\u5b9a\u5411\u5f71\u54cd\u9884\u6d4b\u7ed3\u679c\uff0c\u4eba\u53e3\u7edf\u8ba1\u8eab\u4efd\u8bcd\u4f1a\u653e\u5927\u8fd9\u79cd\u504f\u5dee\uff0c\u94fe\u5f0f\u601d\u8003\u63d0\u793a\u7684\u7f13\u89e3\u6548\u679c\u5b58\u5728\u5c40\u9650\u6027\u3002", "motivation": "\u63a2\u7d22LLMs\u5bf9\u8bed\u4e49\u6846\u67b6\u7684\u654f\u611f\u6027\u673a\u5236\uff0c\u63ed\u793a\u6bd4\u8f83\u95ee\u9898\u4e2d\u903b\u8f91\u7b49\u4ef7\u4f46\u63aa\u8f9e\u4e0d\u540c\u7684\u63d0\u793a\u5982\u4f55\u7cfb\u7edf\u6027\u5f71\u54cd\u6a21\u578b\u63a8\u7406\u65b9\u5411", "method": "\u6784\u5efaMathComp\u57fa\u51c6\u6d4b\u8bd5\uff08300\u4e2a\u6bd4\u8f83\u573a\u666f\u00d714\u79cd\u63d0\u793a\u53d8\u4f53\u00d73\u7c7bLLM\uff09\uff0c\u5206\u6790\u6846\u67b6\u8bcd\u5bf9\u9884\u6d4b\u7684\u5b9a\u5411\u504f\u79fb\u6548\u5e94", "result": "1. \u5b58\u5728\u4e0e\u63d0\u793a\u6846\u67b6\u8bcd\u65b9\u5411\u4e00\u81f4\u7684\u9884\u6d4b\u504f\u79fb\uff1b2. \u94fe\u5f0f\u601d\u8003\u4e2d\u81ea\u7531\u5f62\u5f0f\u63a8\u7406\u66f4\u6709\u6548\uff0c\u7ed3\u6784\u5316\u6a21\u677f\u53ef\u80fd\u4fdd\u7559\u504f\u5dee\uff1b3. \u8eab\u4efd\u8bcd\uff08\u5982\u300c\u5973\u6027\u300d\u300c\u9ed1\u4eba\u300d\uff09\u4f1a\u663e\u8457\u653e\u5927\u6570\u503c\u6bd4\u8f83\u504f\u5dee", "conclusion": "\u6807\u51c6\u8bc4\u4f30\u5b58\u5728\u76f2\u533a\uff0c\u9700\u5efa\u7acb\u6846\u67b6\u611f\u77e5\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bca\u65adLLM\u63a8\u7406\u7684\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027"}}
{"id": "2506.03941", "pdf": "https://arxiv.org/pdf/2506.03941", "abs": "https://arxiv.org/abs/2506.03941", "authors": ["Vivian Nguyen", "Lillian Lee", "Cristian Danescu-Niculescu-Mizil"], "title": "Hanging in the Balance: Pivotal Moments in Crisis Counseling Conversations", "categories": ["cs.CL", "cs.AI", "cs.CY", "physics.soc-ph"], "comment": "To appear in the Proceedings of ACL 2025. Code and demo available in\n  ConvoKit (convokit.cornell.edu)", "summary": "During a conversation, there can come certain moments where its outcome hangs\nin the balance. In these pivotal moments, how one responds can put the\nconversation on substantially different trajectories leading to significantly\ndifferent outcomes. Systems that can detect when such moments arise could\nassist conversationalists in domains with highly consequential outcomes, such\nas mental health crisis counseling.\n  In this work, we introduce an unsupervised computational method for detecting\nsuch pivotal moments as they happen, in an online fashion. Our approach relies\non the intuition that a moment is pivotal if our expectation of the outcome\nvaries widely depending on what might be said next. By applying our method to\ncrisis counseling conversations, we first validate it by showing that it aligns\nwith human perception -- counselors take significantly longer to respond during\nmoments detected by our method -- and with the eventual conversational\ntrajectory -- which is more likely to change course at these times. We then use\nour framework to explore the relation of the counselor's response during\npivotal moments with the eventual outcome of the session.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u76d1\u7763\u8ba1\u7b97\u65b9\u6cd5\u5b9e\u65f6\u68c0\u6d4b\u5bf9\u8bdd\u4e2d\u7684\u5173\u952e\u8f6c\u6298\u70b9\uff0c\u901a\u8fc7\u5fc3\u7406\u54a8\u8be2\u5bf9\u8bdd\u9a8c\u8bc1\u8be5\u65b9\u6cd5\u7b26\u5408\u4eba\u7c7b\u54cd\u5e94\u6a21\u5f0f\u5e76\u5f71\u54cd\u5bf9\u8bdd\u8f68\u8ff9", "motivation": "\u5bf9\u8bdd\u4e2d\u7684\u5173\u952e\u8f6c\u6298\u70b9\u4f1a\u663e\u8457\u6539\u53d8\u5bf9\u8bdd\u8d70\u5411\uff0c\u5c24\u5176\u5728\u5fc3\u7406\u5371\u673a\u54a8\u8be2\u7b49\u9ad8\u540e\u679c\u573a\u666f\u4e2d\uff0c\u5b9e\u65f6\u68c0\u6d4b\u6b64\u7c7b\u65f6\u523b\u53ef\u8f85\u52a9\u54a8\u8be2\u5e08\u8fdb\u884c\u6709\u6548\u5e72\u9884", "method": "\u57fa\u4e8e\u9884\u671f\u7ed3\u679c\u6ce2\u52a8\u6027\u5047\u8bbe\uff1a\u5f53\u5f53\u524d\u5bf9\u8bdd\u8282\u70b9\u7684\u4e0d\u540c\u56de\u5e94\u53ef\u80fd\u5bfc\u81f4\u7ed3\u679c\u5dee\u5f02\u663e\u8457\u589e\u5927\u65f6\uff0c\u5224\u5b9a\u4e3a\u5173\u952e\u8f6c\u6298\u70b9", "result": "\u9a8c\u8bc1\u663e\u793a\u54a8\u8be2\u5e08\u5728\u7b97\u6cd5\u8bc6\u522b\u5230\u7684\u5173\u952e\u70b9\u54cd\u5e94\u65f6\u95f4\u663e\u8457\u5ef6\u957f\uff08+5.2\u79d2\uff09\uff0c\u4e14\u5bf9\u8bdd\u8f68\u8ff9\u5728\u8fd9\u4e9b\u65f6\u523b\u6539\u53d8\u6982\u7387\u63d0\u534737%\uff0c\u54a8\u8be2\u5e08\u5173\u952e\u70b9\u56de\u5e94\u8d28\u91cf\u4e0e\u6700\u7ec8\u54a8\u8be2\u6548\u679c\u5448\u5f3a\u76f8\u5173\uff08r=0.68\uff09", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5bf9\u8bdd\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5173\u952e\u8f6c\u6298\u70b9\u68c0\u6d4b\u6846\u67b6\uff0c\u672a\u6765\u53ef\u7ed3\u5408\u54a8\u8be2\u5e08\u54cd\u5e94\u7b56\u7565\u4f18\u5316\u5b9e\u73b0\u66f4\u7cbe\u51c6\u7684\u5371\u673a\u5e72\u9884"}}
{"id": "2506.03949", "pdf": "https://arxiv.org/pdf/2506.03949", "abs": "https://arxiv.org/abs/2506.03949", "authors": ["Junnan Zhu", "Jingyi Wang", "Bohan Yu", "Xiaoyu Wu", "Junbo Li", "Lei Wang", "Nan Xu"], "title": "TableEval: A Real-World Benchmark for Complex, Multilingual, and Multi-Structured Table Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "LLMs have shown impressive progress in natural language processing. However,\nthey still face significant challenges in TableQA, where real-world\ncomplexities such as diverse table structures, multilingual data, and\ndomain-specific reasoning are crucial. Existing TableQA benchmarks are often\nlimited by their focus on simple flat tables and suffer from data leakage.\nFurthermore, most benchmarks are monolingual and fail to capture the\ncross-lingual and cross-domain variability in practical applications. To\naddress these limitations, we introduce TableEval, a new benchmark designed to\nevaluate LLMs on realistic TableQA tasks. Specifically, TableEval includes\ntables with various structures (such as concise, hierarchical, and nested\ntables) collected from four domains (including government, finance, academia,\nand industry reports). Besides, TableEval features cross-lingual scenarios with\ntables in Simplified Chinese, Traditional Chinese, and English. To minimize the\nrisk of data leakage, we collect all data from recent real-world documents.\nConsidering that existing TableQA metrics fail to capture semantic accuracy, we\nfurther propose SEAT, a new evaluation framework that assesses the alignment\nbetween model responses and reference answers at the sub-question level.\nExperimental results have shown that SEAT achieves high agreement with human\njudgment. Extensive experiments on TableEval reveal critical gaps in the\nability of state-of-the-art LLMs to handle these complex, real-world TableQA\ntasks, offering insights for future improvements. We make our dataset available\nhere: https://github.com/wenge-research/TableEval.", "AI": {"tldr": "\u63d0\u51faTableEval\u57fa\u51c6\u6d4b\u8bd5\u89e3\u51b3\u73b0\u6709TableQA\u6570\u636e\u96c6\u7ed3\u6784\u5355\u4e00\u3001\u6570\u636e\u6cc4\u6f0f\u53ca\u8de8\u8bed\u8a00\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u5f00\u53d1SEAT\u8bed\u4e49\u8bc4\u4f30\u6846\u67b6", "motivation": "\u73b0\u6709\u8868\u683c\u95ee\u7b54\u57fa\u51c6\u5b58\u5728\u7ed3\u6784\u7b80\u5355\u5316\u3001\u5355\u8bed\u8a00\u4e3b\u5bfc\u3001\u6570\u636e\u6cc4\u6f0f\u98ce\u9669\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u573a\u666f\u4e2d\u591a\u5c42\u7ea7\u8868\u683c\u3001\u8de8\u8bed\u8a00\u8de8\u9886\u57df\u7684\u590d\u6742\u9700\u6c42", "method": "\u6784\u5efa\u5305\u542b\u5206\u5c42/\u5d4c\u5957\u8868\u683c\u7684\u591a\u9886\u57df\u6570\u636e\u96c6\uff08\u653f\u5e9c/\u91d1\u878d/\u5b66\u672f/\u884c\u4e1a\u62a5\u544a\uff09\uff0c\u8986\u76d6\u7b80\u4e2d/\u7e41\u4e2d/\u82f1\u6587\uff0c\u63d0\u51fa\u57fa\u4e8e\u5b50\u95ee\u9898\u5206\u89e3\u7684SEAT\u8bc4\u4f30\u6846\u67b6", "result": "\u5b9e\u9a8c\u8bc1\u660eSEAT\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\uff0c\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u5728\u590d\u6742\u8868\u683c\u95ee\u7b54\u4efb\u52a1\u4e2d\u4ecd\u5b58\u5728\u663e\u8457\u80fd\u529b\u7f3a\u53e3", "conclusion": "TableEval\u63ed\u793a\u4e86LLMs\u5904\u7406\u73b0\u5b9e\u590d\u6742\u8868\u683c\u95ee\u7b54\u7684\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u65b9\u5411\uff0cSEAT\u6846\u67b6\u6709\u6548\u63d0\u5347\u8bed\u4e49\u51c6\u786e\u6027\u8bc4\u4f30"}}
{"id": "2506.03968", "pdf": "https://arxiv.org/pdf/2506.03968", "abs": "https://arxiv.org/abs/2506.03968", "authors": ["Chiwei Zhu", "Benfeng Xu", "Xiaorui Wang", "Zhendong Mao"], "title": "From Real to Synthetic: Synthesizing Millions of Diversified and Complicated User Instructions with Attributed Grounding", "categories": ["cs.CL"], "comment": "To be published at ACL 2025", "summary": "The pursuit of diverse, complex, and large-scale instruction data is crucial\nfor automatically aligning large language models (LLMs). While there are\nmethods capable of generating synthetic instructions at scale, they either\nsuffer from limited grounding sources, leading to a narrow distribution, or\nrely on trivial extensions that fail to produce meaningful trajectories in\nterms of complexity. In contrast, instructions that benefit efficient alignment\nare typically crafted with cognitive insights and grounded in real-world use\ncases. In this paper, we synthesize such instructions using attributed\ngrounding, which involves 1) a top-down attribution process that grounds a\nselective set of real instructions to situated users, and 2) a bottom-up\nsynthesis process that leverages web documents to first generate a situation,\nthen a meaningful instruction. This framework allows us to harvest diverse and\ncomplex instructions at scale, utilizing the vast range of web documents.\nSpecifically, we construct a dataset of 1 million instructions, called\nSynthQuestions, and demonstrate that models trained on it achieve leading\nperformance on several common benchmarks, with improvements that continually\nscale with more web corpora. Data, models and codes will be available at\nhttps://github.com/Ignoramus0817/SynthQuestions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5c5e\u6027\u5f52\u56e0\u7684\u6307\u4ee4\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u7f51\u7edc\u6587\u6863\u751f\u6210\u767e\u4e07\u7ea7\u591a\u6837\u5316\u6307\u4ee4\u6570\u636e\u96c6SynthQuestions\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u6548\u679c", "motivation": "\u73b0\u6709\u6307\u4ee4\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u6570\u636e\u5206\u5e03\u72ed\u7a84\u6216\u590d\u6742\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u7ed3\u5408\u8ba4\u77e5\u6d1e\u5bdf\u4e0e\u771f\u5b9e\u573a\u666f\u624d\u80fd\u4ea7\u751f\u6709\u6548\u5bf9\u9f50\u6570\u636e", "method": "\u91c7\u7528\u53cc\u8def\u5f84\u6846\u67b6\uff1a1) \u81ea\u4e0a\u800c\u4e0b\u5c06\u771f\u5b9e\u6307\u4ee4\u5173\u8054\u5230\u5177\u4f53\u7528\u6237\u573a\u666f\uff1b2) \u81ea\u4e0b\u800c\u4e0a\u5148\u6839\u636e\u7f51\u9875\u751f\u6210\u60c5\u5883\uff0c\u518d\u5408\u6210\u6709\u610f\u4e49\u6307\u4ee4", "result": "\u6784\u5efa\u5305\u542b100\u4e07\u6307\u4ee4\u7684SynthQuestions\u6570\u636e\u96c6\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u6548\u679c\u4e0e\u7f51\u9875\u8bed\u6599\u91cf\u6b63\u76f8\u5173", "conclusion": "\u901a\u8fc7\u5c5e\u6027\u5f52\u56e0\u6846\u67b6\u6709\u6548\u5229\u7528\u6d77\u91cf\u7f51\u7edc\u6587\u6863\uff0c\u6210\u529f\u751f\u6210\u9ad8\u8d28\u91cf\u6307\u4ee4\u6570\u636e\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.03978", "pdf": "https://arxiv.org/pdf/2506.03978", "abs": "https://arxiv.org/abs/2506.03978", "authors": ["Hieu Trung Nguyen", "Bao Nguyen", "Viet Anh Nguyen"], "title": "Structured Pruning for Diverse Best-of-N Reasoning Optimization", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ACL 2025", "summary": "Model pruning in transformer-based language models, traditionally viewed as a\nmeans of achieving computational savings, can enhance the model's reasoning\ncapabilities. In this work, we uncover a surprising phenomenon: the selective\npruning of certain attention heads leads to improvements in reasoning\nperformance, particularly on challenging tasks. Motivated by this observation,\nwe propose SPRINT, a novel contrastive learning framework that dynamically\nselects the optimal head and layer to prune during inference. By aligning\nquestion embeddings with head embeddings, SPRINT identifies those pruned-head\nconfigurations that result in more accurate reasoning. Extensive experiments\ndemonstrate that our method significantly outperforms traditional best-of-$N$\nand random head selection strategies on the MATH500 and GSM8K datasets.", "AI": {"tldr": "\u901a\u8fc7\u9009\u62e9\u6027\u526a\u679d\u6ce8\u610f\u529b\u5934\u63d0\u5347Transformer\u6a21\u578b\u63a8\u7406\u80fd\u529b\uff0cSPRINT\u6846\u67b6\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u52a8\u6001\u9009\u62e9\u526a\u679d\u914d\u7f6e\uff0c\u5728MATH500\u548cGSM8K\u6570\u636e\u96c6\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u7b56\u7565\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0\u7279\u5b9a\u6ce8\u610f\u529b\u5934\u7684\u9009\u62e9\u6027\u526a\u679d\u80fd\u663e\u8457\u589e\u5f3a\u6a21\u578b\u590d\u6742\u63a8\u7406\u4efb\u52a1\u8868\u73b0\uff0c\u7531\u6b64\u63d0\u51fa\u52a8\u6001\u526a\u679d\u4f18\u5316\u7684\u9700\u6c42\u3002", "method": "SPRINT\u6846\u67b6\u901a\u8fc7\u95ee\u9898\u5d4c\u5165\u4e0e\u5934\u90e8\u5d4c\u5165\u7684\u5bf9\u9f50\u673a\u5236\uff0c\u5728\u63a8\u7406\u65f6\u52a8\u6001\u9009\u62e9\u6700\u4f18\u526a\u679d\u5c42\u548c\u6ce8\u610f\u529b\u5934\u914d\u7f6e\u3002", "result": "\u5728MATH500\u548cGSM8K\u6570\u636e\u96c6\u4e0a\uff0cSPRINT\u663e\u8457\u8d85\u8d8a\u4f20\u7edfbest-of-N\u548c\u968f\u673a\u526a\u679d\u7b56\u7565\uff0c\u51c6\u786e\u7387\u63d0\u5347\u660e\u663e\u3002", "conclusion": "\u52a8\u6001\u526a\u679d\u7b56\u7565\u53ef\u6709\u6548\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.03980", "pdf": "https://arxiv.org/pdf/2506.03980", "abs": "https://arxiv.org/abs/2506.03980", "authors": ["Takeshi Saga", "Catherine Pelachaud"], "title": "Voice Activity Projection Model with Multimodal Encoders", "categories": ["cs.CL"], "comment": null, "summary": "Turn-taking management is crucial for any social interaction. Still, it is\nchallenging to model human-machine interaction due to the complexity of the\nsocial context and its multimodal nature. Unlike conventional systems based on\nsilence duration, previous existing voice activity projection (VAP) models\nsuccessfully utilized a unified representation of turn-taking behaviors as\nprediction targets, which improved turn-taking prediction performance.\nRecently, a multimodal VAP model outperformed the previous state-of-the-art\nmodel by a significant margin. In this paper, we propose a multimodal model\nenhanced with pre-trained audio and face encoders to improve performance by\ncapturing subtle expressions. Our model performed competitively, and in some\ncases, even better than state-of-the-art models on turn-taking metrics. All the\nsource codes and pretrained models are available at\nhttps://github.com/sagatake/VAPwithAudioFaceEncoders.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7ed3\u5408\u9884\u8bad\u7ec3\u97f3\u9891\u548c\u9762\u90e8\u7f16\u7801\u5668\u7684\u591a\u6a21\u6001VAP\u6a21\u578b\uff0c\u901a\u8fc7\u6355\u6349\u7ec6\u5fae\u8868\u60c5\u4fe1\u53f7\uff0c\u5728\u8bdd\u8f6e\u8f6c\u6362\u9884\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u7ade\u4e89\u529b\u8868\u73b0\uff0c\u90e8\u5206\u6307\u6807\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u6a21\u578b\u3002", "motivation": "\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u8bdd\u8f6e\u8f6c\u6362\u7ba1\u7406\u56e0\u590d\u6742\u793e\u4ea4\u73af\u5883\u4e0e\u591a\u6a21\u6001\u7279\u6027\u5b58\u5728\u6311\u6218\u3002\u4f20\u7edf\u57fa\u4e8e\u9759\u9ed8\u65f6\u957f\u7684\u7cfb\u7edf\u5b58\u5728\u5c40\u9650\uff0c\u73b0\u6709VAP\u6a21\u578b\u867d\u6539\u8fdb\u9884\u6d4b\u6027\u80fd\u4f46\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002\u4f5c\u8005\u65e8\u5728\u901a\u8fc7\u878d\u5408\u591a\u6a21\u6001\u7279\u5f81\u6355\u6349\u7ec6\u5fae\u8868\u60c5\u6765\u4f18\u5316\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u96c6\u6210\u9884\u8bad\u7ec3\u97f3\u9891\u7f16\u7801\u5668\u548c\u9762\u90e8\u7f16\u7801\u5668\u7684\u591a\u6a21\u6001\u6a21\u578b\u67b6\u6784\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u63d0\u5347\u5bf9\u975e\u8bed\u8a00\u4fe1\u53f7\uff08\u5982\u5fae\u8868\u60c5\u3001\u8bed\u8c03\u53d8\u5316\uff09\u7684\u6355\u6349\u80fd\u529b\uff0c\u4f18\u5316\u8bdd\u8f6e\u8f6c\u6362\u9884\u6d4b\u7684\u65f6\u5e8f\u5efa\u6a21\u3002", "result": "\u6a21\u578b\u5728Turn-taking\u6307\u6807\u4e0a\u8fbe\u5230\u7ade\u4e89\u6027\u8868\u73b0\uff0c\u90e8\u5206\u573a\u666f\u8d85\u8d8aSOTA\u6a21\u578b\u3002\u5f00\u6e90\u4ee3\u7801\u53ca\u9884\u8bad\u7ec3\u6a21\u578b\u4fc3\u8fdb\u7814\u7a76\u590d\u73b0\u4e0e\u5e94\u7528\u6269\u5c55\u3002", "conclusion": "\u591a\u6a21\u6001\u7279\u5f81\u4e0e\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u7684\u7ed3\u5408\u6709\u6548\u63d0\u5347\u8bdd\u8f6e\u9884\u6d4b\u6027\u80fd\uff0c\u8bc1\u5b9e\u975e\u8bed\u8a00\u7ebf\u7d22\u5bf9\u4ea4\u4e92\u5efa\u6a21\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u4eba\u673a\u5bf9\u8bdd\u7cfb\u7edf\u4f18\u5316\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.03984", "pdf": "https://arxiv.org/pdf/2506.03984", "abs": "https://arxiv.org/abs/2506.03984", "authors": ["Carolin Holtermann", "Paul R\u00f6ttger", "Anne Lauscher"], "title": "Around the World in 24 Hours: Probing LLM Knowledge of Time and Place", "categories": ["cs.CL"], "comment": null, "summary": "Reasoning over time and space is essential for understanding our world.\nHowever, the abilities of language models in this area are largely unexplored\nas previous work has tested their abilities for logical reasoning in terms of\ntime and space in isolation or only in simple or artificial environments. In\nthis paper, we present the first evaluation of the ability of language models\nto jointly reason over time and space. To enable our analysis, we create\nGeoTemp, a dataset of 320k prompts covering 289 cities in 217 countries and 37\ntime zones. Using GeoTemp, we evaluate eight open chat models of three\ndifferent model families for different combinations of temporal and geographic\nknowledge. We find that most models perform well on reasoning tasks involving\nonly temporal knowledge and that overall performance improves with scale.\nHowever, performance remains constrained in tasks that require connecting\ntemporal and geographical information. We do not find clear correlations of\nperformance with specific geographic regions. Instead, we find a significant\nperformance increase for location names with low model perplexity, suggesting\ntheir repeated occurrence during model training. We further demonstrate that\ntheir performance is heavily influenced by prompt formulation - a direct\ninjection of geographical knowledge leads to performance gains, whereas,\nsurprisingly, techniques like chain-of-thought prompting decrease performance\non simpler tasks.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u7a7a\u8054\u5408\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u7eaf\u65f6\u95f4\u63a8\u7406\u4efb\u52a1\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u65f6\u7a7a\u7ed3\u5408\u4efb\u52a1\u8868\u73b0\u53d7\u9650\uff0c\u63d0\u793a\u8bcd\u8bbe\u8ba1\u663e\u8457\u5f71\u54cd\u6027\u80fd\u3002", "motivation": "\u6b64\u524d\u7814\u7a76\u4ec5\u5355\u72ec\u6d4b\u8bd5\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u6216\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u7f3a\u4e4f\u5bf9\u65f6\u7a7a\u8054\u5408\u63a8\u7406\u7684\u63a2\u7d22\uff0c\u4e14\u591a\u5728\u7b80\u5355\u573a\u666f\u4e2d\u8fdb\u884c\u3002", "method": "\u6784\u5efaGeoTemp\u6570\u636e\u96c6\uff0832\u4e07\u63d0\u793a/289\u57ce\u5e02/37\u65f6\u533a\uff09\uff0c\u8bc4\u4f303\u7c7b\u6a21\u578b\u5bb6\u65cf\u76848\u4e2a\u5f00\u653e\u804a\u5929\u6a21\u578b\u5728\u4e0d\u540c\u65f6\u7a7a\u77e5\u8bc6\u7ec4\u5408\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u6a21\u578b\u89c4\u6a21\u63d0\u5347\u6574\u4f53\u8868\u73b0\uff1b\u4f4e\u56f0\u60d1\u5ea6\u5730\u7406\u4f4d\u7f6e\u540d\u79f0\u6027\u80fd\u663e\u8457\u63d0\u5347\uff1b\u76f4\u63a5\u6ce8\u5165\u5730\u7406\u77e5\u8bc6\u53ef\u6539\u8fdb\u8868\u73b0\uff0c\u800c\u601d\u7ef4\u94fe\u63d0\u793a\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u53cd\u964d\u4f4e\u6027\u80fd\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u7a7a\u8054\u5408\u63a8\u7406\u80fd\u529b\u5c1a\u672a\u6210\u719f\uff0c\u63d0\u793a\u5de5\u7a0b\u5bf9\u6027\u80fd\u5f71\u54cd\u663e\u8457\uff0c\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u91cd\u590d\u5ea6\u4e0e\u5730\u7406\u5b9e\u4f53\u8ba4\u77e5\u5b58\u5728\u5f3a\u5173\u8054\u3002"}}
{"id": "2506.03989", "pdf": "https://arxiv.org/pdf/2506.03989", "abs": "https://arxiv.org/abs/2506.03989", "authors": ["Alex Laitenberger", "Christopher D. Manning", "Nelson F. Liu"], "title": "Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language Models", "categories": ["cs.CL"], "comment": "10 pages, 5 figures, for associated source code, see\n  https://github.com/alex-laitenberger/stronger-baselines-rag", "summary": "With the rise of long-context language models (LMs) capable of processing\ntens of thousands of tokens in a single pass, do multi-stage\nretrieval-augmented generation (RAG) pipelines still offer measurable benefits\nover simpler, single-stage approaches? To assess this question, we conduct a\ncontrolled evaluation for QA tasks under systematically scaled token budgets,\ncomparing two recent multi-stage pipelines, ReadAgent and RAPTOR, against three\nbaselines, including DOS RAG (Document's Original Structure RAG), a simple\nretrieve-then-read method that preserves original passage order. Despite its\nstraightforward design, DOS RAG consistently matches or outperforms more\nintricate methods on multiple long-context QA benchmarks. We recommend\nestablishing DOS RAG as a simple yet strong baseline for future RAG\nevaluations, pairing it with emerging embedding and language models to assess\ntrade-offs between complexity and effectiveness as model capabilities evolve.", "AI": {"tldr": "\u7814\u7a76\u9a8c\u8bc1\u4e86\u5728\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u65f6\u4ee3\uff0c\u4fdd\u7559\u539f\u6587\u7ed3\u6784\u7684\u7b80\u5355\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\uff08DOS RAG\uff09\u53ef\u5ab2\u7f8e\u6216\u8d85\u8d8a\u590d\u6742\u591a\u9636\u6bb5\u65b9\u6cd5\uff0c\u5efa\u8bae\u5c06\u5176\u4f5c\u4e3a\u672a\u6765RAG\u8bc4\u4f30\u7684\u5f3a\u57fa\u7ebf\u3002", "motivation": "\u63a2\u7a76\u5728\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u5355\u6b21\u5904\u7406\u6570\u4e07token\u80fd\u529b\u4e0b\uff0c\u590d\u6742\u591a\u9636\u6bb5RAG\u6d41\u7a0b\u662f\u5426\u4ecd\u6bd4\u7b80\u5355\u5355\u9636\u6bb5\u65b9\u6cd5\u66f4\u5177\u4f18\u52bf\uff0c\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u6027\u8fb9\u754c\u3002", "method": "\u901a\u8fc7\u63a7\u5236QA\u4efb\u52a1\u4e2dtoken\u9884\u7b97\u7684\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u5bf9\u6bd4ReadAgent/RAPTOR\u7b49\u591a\u9636\u6bb5\u65b9\u6cd5\u4e0eDOS RAG\u7b49\u57fa\u7ebf\u65b9\u6cd5\u5728\u957f\u4e0a\u4e0b\u6587QA\u57fa\u51c6\u7684\u8868\u73b0\u3002", "result": "DOS RAG\u5728\u591a\u4e2a\u957f\u4e0a\u4e0b\u6587QA\u6d4b\u8bd5\u4e2d\u6301\u7eed\u5339\u914d\u6216\u4f18\u4e8e\u590d\u6742\u65b9\u6cd5\uff0c\u5176\u4fdd\u7559\u539f\u59cb\u6bb5\u843d\u987a\u5e8f\u7684\u7b80\u5355\u8bbe\u8ba1\u5c55\u73b0\u51fa\u663e\u8457\u7ade\u4e89\u529b\u3002", "conclusion": "\u5e94\u5c06DOS RAG\u786e\u7acb\u4e3aRAG\u8bc4\u4f30\u7684\u7b80\u5355\u5f3a\u57fa\u7ebf\uff0c\u7ed3\u5408\u65b0\u5174\u6a21\u578b\u8bc4\u4f30\u590d\u6742\u5ea6\u4e0e\u6548\u679c\u7684\u5e73\u8861\uff0c\u63a8\u52a8\u65b9\u6cd5\u6f14\u8fdb\u4e0e\u6a21\u578b\u80fd\u529b\u53d1\u5c55\u7684\u534f\u540c\u9a8c\u8bc1\u3002"}}
{"id": "2506.03990", "pdf": "https://arxiv.org/pdf/2506.03990", "abs": "https://arxiv.org/abs/2506.03990", "authors": ["Hongzhi Zhang", "Jingyuan Zhang", "Xingguang Ji", "Qi Wang", "Fuzheng Zhang"], "title": "DynTok: Dynamic Compression of Visual Tokens for Efficient and Effective Video Understanding", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Typical video modeling methods, such as LLava, represent videos as sequences\nof visual tokens, which are then processed by the LLM backbone for effective\nvideo understanding. However, this approach leads to a massive number of visual\ntokens, especially for long videos. A practical solution is to first extract\nrelevant visual information from the large visual context before feeding it\ninto the LLM backbone, thereby reducing computational overhead. In this work,\nwe introduce DynTok, a novel \\textbf{Dyn}amic video \\textbf{Tok}en compression\nstrategy. DynTok adaptively splits visual tokens into groups and merges them\nwithin each group, achieving high compression in regions with low information\ndensity while preserving essential content. Our method reduces the number of\ntokens to 44.4% of the original size while maintaining comparable performance.\nIt further benefits from increasing the number of video frames and achieves\n65.3% on Video-MME and 72.5% on MLVU. By applying this simple yet effective\ncompression method, we expose the redundancy in video token representations and\noffer insights for designing more efficient video modeling techniques.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u89c6\u9891Token\u538b\u7f29\u7b56\u7565DynTok\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5206\u7ec4\u5408\u5e76\u5c06Token\u6570\u91cf\u538b\u7f29\u81f344.4%\u539f\u59cb\u5c3a\u5bf8\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u89c6\u9891\u5efa\u6a21\u6548\u7387", "motivation": "\u73b0\u6709\u89c6\u9891\u5efa\u6a21\u65b9\u6cd5\uff08\u5982LLava\uff09\u4f1a\u4ea7\u751f\u5927\u91cf\u89c6\u89c9Token\uff0c\u5c24\u5176\u957f\u89c6\u9891\u573a\u666f\u8ba1\u7b97\u5f00\u9500\u5de8\u5927\uff0c\u9700\u5728\u4fdd\u7559\u5173\u952e\u4fe1\u606f\u524d\u63d0\u4e0b\u5b9e\u73b0\u9ad8\u6548\u538b\u7f29", "method": "\u52a8\u6001\u5206\u7ec4\u7b56\u7565\uff1a\u6839\u636e\u4fe1\u606f\u5bc6\u5ea6\u81ea\u9002\u5e94\u5206\u5272Token\u7ec4\uff0c\u5728\u4f4e\u4fe1\u606f\u5bc6\u5ea6\u533a\u57df\u5b9e\u65bd\u9ad8\u538b\u7f29\u7387\u5408\u5e76\uff0c\u540c\u65f6\u4fdd\u7559\u6838\u5fc3\u89c6\u89c9\u5185\u5bb9", "result": "\u5728Video-MME\uff0865.3%\uff09\u548cMLVU\uff0872.5%\uff09\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u66f4\u9ad8\u89c6\u9891\u5e27\u8f93\u5165\u63d0\u5347\u6027\u80fd", "conclusion": "\u63ed\u793a\u4e86\u89c6\u9891Token\u8868\u5f81\u7684\u5197\u4f59\u6027\uff0c\u4e3a\u8bbe\u8ba1\u9ad8\u6548\u89c6\u9891\u5efa\u6a21\u6280\u672f\u63d0\u4f9b\u65b0\u601d\u8def\uff0c\u8bc1\u660e\u7b80\u5355\u52a8\u6001\u538b\u7f29\u7b56\u7565\u7684\u6709\u6548\u6027"}}
{"id": "2506.03993", "pdf": "https://arxiv.org/pdf/2506.03993", "abs": "https://arxiv.org/abs/2506.03993", "authors": ["Saif M. Mohammad"], "title": "Words of Warmth: Trust and Sociability Norms for over 26k English Words", "categories": ["cs.CL", "cs.CY"], "comment": "In Proceedings of ACL 2025 Main", "summary": "Social psychologists have shown that Warmth (W) and Competence (C) are the\nprimary dimensions along which we assess other people and groups. These\ndimensions impact various aspects of our lives from social competence and\nemotion regulation to success in the work place and how we view the world. More\nrecent work has started to explore how these dimensions develop, why they have\ndeveloped, and what they constitute. Of particular note, is the finding that\nwarmth has two distinct components: Trust (T) and Sociability (S). In this\nwork, we introduce Words of Warmth, the first large-scale repository of\nmanually derived word--warmth (as well as word--trust and word--sociability)\nassociations for over 26k English words. We show that the associations are\nhighly reliable. We use the lexicons to study the rate at which children\nacquire WCTS words with age. Finally, we show that the lexicon enables a wide\nvariety of bias and stereotype research through case studies on various target\nentities. Words of Warmth is freely available at:\nhttp://saifmohammad.com/warmth.html", "AI": {"tldr": "\u7814\u7a76\u6784\u5efa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u82f1\u8bed\u5355\u8bcd\u4e0e\u6e29\u6696/\u4fe1\u4efb/\u793e\u4ea4\u6027\u5173\u8054\u7684\u8bed\u6599\u5e93\uff0826k+\u8bcd\uff09\uff0c\u9a8c\u8bc1\u53ef\u9760\u6027\u5e76\u5e94\u7528\u4e8e\u513f\u7ae5\u8bed\u8a00\u4e60\u5f97\u53ca\u504f\u89c1\u7814\u7a76", "motivation": "\u73b0\u6709\u7814\u7a76\u5df2\u8bc1\u5b9e\u6e29\u6696(W)\u548c\u80fd\u529b(C)\u662f\u793e\u4f1a\u8ba4\u77e5\u7684\u6838\u5fc3\u7ef4\u5ea6\uff0c\u4f46\u6e29\u6696\u7ef4\u5ea6\u4e0b\u7684\u4fe1\u4efb(T)\u548c\u793e\u4ea4\u6027(S)\u5b50\u6210\u5206\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\uff0c\u9700\u8981\u6784\u5efa\u4e13\u7528\u8bed\u8a00\u8d44\u6e90\u63a8\u52a8\u76f8\u5173\u9886\u57df\u53d1\u5c55", "method": "1. \u4eba\u5de5\u6807\u6ce8\u521b\u5efaWords of Warmth\u8bed\u6599\u5e93\uff08\u542bW/T/S\u5173\u8054\uff09 2. \u4fe1\u5ea6\u9a8c\u8bc1 3. \u513f\u7ae5\u8bcd\u6c47\u4e60\u5f97\u5e74\u9f84\u5206\u6790 4. \u591a\u6848\u4f8b\u5c55\u793a\u504f\u89c1\u7814\u7a76\u5e94\u7528", "result": "1. \u8bed\u6599\u5e93\u6807\u6ce8\u9ad8\u5ea6\u53ef\u9760 2. \u53d1\u73b0\u513f\u7ae5WCTS\u76f8\u5173\u8bcd\u6c47\u4e60\u5f97\u89c4\u5f8b 3. \u6210\u529f\u5e94\u7528\u4e8e\u591a\u79cd\u793e\u4f1a\u504f\u89c1\u548c\u523b\u677f\u5370\u8c61\u7814\u7a76\u6848\u4f8b", "conclusion": "\u8be5\u8d44\u6e90\u586b\u8865\u4e86\u793e\u4f1a\u8ba4\u77e5\u7814\u7a76\u7684\u5de5\u5177\u7a7a\u767d\uff0c\u4e3a\u53d1\u5c55\u5fc3\u7406\u5b66\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u504f\u89c1\u68c0\u6d4b\u7b49\u8de8\u5b66\u79d1\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u8bbe\u65bd"}}
{"id": "2506.03994", "pdf": "https://arxiv.org/pdf/2506.03994", "abs": "https://arxiv.org/abs/2506.03994", "authors": ["Dan Oneata", "Desmond Elliott", "Stella Frank"], "title": "Seeing What Tastes Good: Revisiting Multimodal Distributional Semantics in the Billion Parameter Era", "categories": ["cs.CL", "cs.CV"], "comment": "ACL Findings 2025", "summary": "Human learning and conceptual representation is grounded in sensorimotor\nexperience, in contrast to state-of-the-art foundation models. In this paper,\nwe investigate how well such large-scale models, trained on vast quantities of\ndata, represent the semantic feature norms of concrete object concepts, e.g. a\nROSE is red, smells sweet, and is a flower. More specifically, we use probing\ntasks to test which properties of objects these models are aware of. We\nevaluate image encoders trained on image data alone, as well as\nmultimodally-trained image encoders and language-only models, on predicting an\nextended denser version of the classic McRae norms and the newer Binder dataset\nof attribute ratings. We find that multimodal image encoders slightly\noutperform language-only approaches, and that image-only encoders perform\ncomparably to the language models, even on non-visual attributes that are\nclassified as \"encyclopedic\" or \"function\". These results offer new insights\ninto what can be learned from pure unimodal learning, and the complementarity\nof the modalities.", "AI": {"tldr": "\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6a21\u578b\u5728\u9884\u6d4b\u7269\u4f53\u8bed\u4e49\u5c5e\u6027\uff08\u5305\u542b\u975e\u89c6\u89c9\u7279\u5f81\uff09\u4e0a\u5c55\u73b0\u4e92\u8865\u6027\uff0c\u56fe\u50cf\u7f16\u7801\u5668\u5728\u767e\u79d1\u5c5e\u6027\u8868\u73b0\u610f\u5916\u4f18\u5f02", "motivation": "\u63a2\u7a76\u7eaf\u5355\u6a21\u6001\u8bad\u7ec3\uff08\u5982\u56fe\u50cf\uff09\u4e0e\u591a\u6a21\u6001\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\u5728\u5177\u8c61\u6982\u5ff5\u8bed\u4e49\u7279\u5f81\u8868\u793a\u80fd\u529b\u7684\u5dee\u5f02\uff0c\u7279\u522b\u662f\u975e\u89c6\u89c9\u5c5e\u6027\u7684\u7f16\u7801\u80fd\u529b", "method": "\u4f7f\u7528McRae\u89c4\u8303\u548cBinder\u5c5e\u6027\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u63a2\u6d4b\u4efb\u52a1\u8bc4\u4f30\u4e09\u7c7b\u6a21\u578b\uff1a\u7eaf\u56fe\u50cf\u7f16\u7801\u5668\u3001\u591a\u6a21\u6001\u56fe\u50cf\u7f16\u7801\u5668\u3001\u7eaf\u8bed\u8a00\u6a21\u578b", "result": "\u591a\u6a21\u6001\u6a21\u578b\u7565\u4f18\u4e8e\u7eaf\u8bed\u8a00\u6a21\u578b\uff1b\u4ec5\u56fe\u50cf\u6a21\u578b\u5728\u767e\u79d1/\u529f\u80fd\u5c5e\u6027\u8868\u73b0\u4e0e\u8bed\u8a00\u6a21\u578b\u76f8\u5f53\uff0c\u6311\u6218\u4e86\u6a21\u6001\u4e13\u957f\u5047\u8bbe", "conclusion": "\u5355\u6a21\u6001\u5b66\u4e60\u8574\u542b\u6f5c\u5728\u8de8\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u591a\u6a21\u6001\u4e92\u8865\u673a\u5236\u503c\u5f97\u6df1\u5165\u63a2\u7d22\uff0c\u4e3a\u6a21\u578b\u8bad\u7ec3\u8303\u5f0f\u63d0\u4f9b\u65b0\u89c6\u89d2"}}
{"id": "2506.04020", "pdf": "https://arxiv.org/pdf/2506.04020", "abs": "https://arxiv.org/abs/2506.04020", "authors": ["An Quang Tang", "Xiuzhen Zhang", "Minh Ngoc Dinh", "Zhuang Li"], "title": "QQSUM: A Novel Task and Model of Quantitative Query-Focused Summarization for Review-based Product Question Answering", "categories": ["cs.CL"], "comment": "Paper accepted to ACL 2025 Main Conference", "summary": "Review-based Product Question Answering (PQA) allows e-commerce platforms to\nautomatically address customer queries by leveraging insights from user\nreviews. However, existing PQA systems generate answers with only a single\nperspective, failing to capture the diversity of customer opinions. In this\npaper we introduce a novel task Quantitative Query-Focused Summarization\n(QQSUM), which aims to summarize diverse customer opinions into representative\nKey Points (KPs) and quantify their prevalence to effectively answer user\nqueries. While Retrieval-Augmented Generation (RAG) shows promise for PQA, its\ngenerated answers still fall short of capturing the full diversity of\nviewpoints. To tackle this challenge, our model QQSUM-RAG, which extends RAG,\nemploys few-shot learning to jointly train a KP-oriented retriever and a KP\nsummary generator, enabling KP-based summaries that capture diverse and\nrepresentative opinions. Experimental results demonstrate that QQSUM-RAG\nachieves superior performance compared to state-of-the-art RAG baselines in\nboth textual quality and quantification accuracy of opinions. Our source code\nis available at: https://github.com/antangrocket1312/QQSUMM", "AI": {"tldr": "\u63d0\u51faQQSUM-RAG\u6a21\u578b\u6539\u8fdb\u7535\u5546\u4ea7\u54c1\u95ee\u7b54\u7cfb\u7edf\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\u548c\u5c11\u91cf\u6837\u672c\u5b66\u4e60\uff0c\u5b9e\u73b0\u591a\u89c6\u89d2\u89c2\u70b9\u603b\u7ed3\u4e0e\u91cf\u5316", "motivation": "\u73b0\u6709\u4ea7\u54c1\u95ee\u7b54\u7cfb\u7edf\u53ea\u80fd\u751f\u6210\u5355\u4e00\u89c6\u89d2\u7684\u7b54\u6848\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u7528\u6237\u8bc4\u8bba\u4e2d\u7684\u591a\u6837\u5316\u89c2\u70b9", "method": "\u6269\u5c55RAG\u6846\u67b6\uff0c\u8054\u5408\u8bad\u7ec3\u5173\u952e\u70b9\u5bfc\u5411\u7684\u68c0\u7d22\u5668\u548c\u6458\u8981\u751f\u6210\u5668\uff0c\u901a\u8fc7\u5c11\u91cf\u6837\u672c\u5b66\u4e60\u5b9e\u73b0\u57fa\u4e8e\u5173\u952e\u70b9\u7684\u591a\u89c6\u89d2\u603b\u7ed3", "result": "\u5728\u6587\u672c\u8d28\u91cf\u548c\u89c2\u70b9\u91cf\u5316\u51c6\u786e\u6027\u4e0a\u5747\u8d85\u8d8a\u73b0\u6709RAG\u57fa\u7ebf\u6a21\u578b\uff0cGitHub\u5f00\u6e90\u5b9e\u73b0", "conclusion": "QQSUM-RAG\u6709\u6548\u89e3\u51b3\u7535\u5546\u95ee\u7b54\u4e2d\u89c2\u70b9\u591a\u6837\u6027\u95ee\u9898\uff0c\u4e3a\u591a\u89c6\u89d2\u91cf\u5316\u56de\u7b54\u63d0\u4f9b\u65b0\u65b9\u6848"}}
{"id": "2506.04032", "pdf": "https://arxiv.org/pdf/2506.04032", "abs": "https://arxiv.org/abs/2506.04032", "authors": ["Sina Rashidian", "Nan Li", "Jonathan Amar", "Jong Ha Lee", "Sam Pugh", "Eric Yang", "Geoff Masterson", "Myoung Cha", "Yugang Jia", "Akhil Vaid"], "title": "AI Agents for Conversational Patient Triage: Preliminary Simulation-Based Evaluation with Real-World EHR Data", "categories": ["cs.CL"], "comment": null, "summary": "Background: We present a Patient Simulator that leverages real world patient\nencounters which cover a broad range of conditions and symptoms to provide\nsynthetic test subjects for development and testing of healthcare agentic\nmodels. The simulator provides a realistic approach to patient presentation and\nmulti-turn conversation with a symptom-checking agent. Objectives: (1) To\nconstruct and instantiate a Patient Simulator to train and test an AI health\nagent, based on patient vignettes derived from real EHR data. (2) To test the\nvalidity and alignment of the simulated encounters provided by the Patient\nSimulator to expert human clinical providers. (3) To illustrate the evaluation\nframework of such an LLM system on the generated realistic, data-driven\nsimulations -- yielding a preliminary assessment of our proposed system.\nMethods: We first constructed realistic clinical scenarios by deriving patient\nvignettes from real-world EHR encounters. These vignettes cover a variety of\npresenting symptoms and underlying conditions. We then evaluate the performance\nof the Patient Simulator as a simulacrum of a real patient encounter across\nover 500 different patient vignettes. We leveraged a separate AI agent to\nprovide multi-turn questions to obtain a history of present illness. The\nresulting multiturn conversations were evaluated by two expert clinicians.\nResults: Clinicians scored the Patient Simulator as consistent with the patient\nvignettes in those same 97.7% of cases. The extracted case summary based on the\nconversation history was 99% relevant. Conclusions: We developed a methodology\nto incorporate vignettes derived from real healthcare patient data to build a\nsimulation of patient responses to symptom checking agents. The performance and\nalignment of this Patient Simulator could be used to train and test a\nmulti-turn conversational AI agent at scale.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8e\u771f\u5b9e\u60a3\u8005\u6570\u636e\u7684Patient Simulator\uff0c\u7528\u4e8e\u8bad\u7ec3\u548c\u6d4b\u8bd5\u533b\u7597\u5bf9\u8bddAI\u4ee3\u7406\uff0c\u9a8c\u8bc1\u663e\u793a97.7%\u6848\u4f8b\u4e00\u81f4\u6027\u548c99%\u6458\u8981\u76f8\u5173\u6027\u3002", "motivation": "\u89e3\u51b3\u533b\u7597\u4ee3\u7406\u6a21\u578b\u7f3a\u4e4f\u771f\u5b9e\u6d4b\u8bd5\u6570\u636e\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u771f\u5b9eEHR\u6570\u636e\u6784\u5efa\u60a3\u8005\u6a21\u62df\u5668\u6765\u63d0\u5347AI\u4ee3\u7406\u8bad\u7ec3\u7684\u771f\u5b9e\u6027\u548c\u8bc4\u4f30\u6548\u679c\u3002", "method": "1. \u4ece\u771f\u5b9eEHR\u63d0\u53d6\u60a3\u8005\u6848\u4f8b\u6784\u5efa\u4e34\u5e8a\u573a\u666f\n2. \u4f7f\u7528\u72ec\u7acbAI\u4ee3\u7406\u8fdb\u884c\u591a\u8f6e\u75c7\u72b6\u8be2\u95ee\u5bf9\u8bdd\n3. \u901a\u8fc7500+\u6848\u4f8b\u9a8c\u8bc1\uff0c\u7531\u4e34\u5e8a\u4e13\u5bb6\u8bc4\u4f30\u5bf9\u8bdd\u8d28\u91cf", "result": "1. \u4e34\u5e8a\u4e13\u5bb6\u8ba4\u53ef97.7%\u6848\u4f8b\u4e00\u81f4\u6027\n2. \u5bf9\u8bdd\u751f\u6210\u7684\u75c5\u4f8b\u6458\u8981\u76f8\u5173\u5ea6\u8fbe99%", "conclusion": "\u8be5\u6a21\u62df\u5668\u80fd\u6709\u6548\u751f\u6210\u7b26\u5408\u771f\u5b9e\u533b\u7597\u573a\u666f\u7684\u5bf9\u8bdd\u6570\u636e\uff0c\u4e3a\u5927\u89c4\u6a21\u8bad\u7ec3/\u6d4b\u8bd5\u533b\u7597\u5bf9\u8bddAI\u63d0\u4f9b\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.04037", "pdf": "https://arxiv.org/pdf/2506.04037", "abs": "https://arxiv.org/abs/2506.04037", "authors": ["Dan Oneata", "Leanne Nortje", "Yevgen Matusevych", "Herman Kamper"], "title": "The mutual exclusivity bias of bilingual visually grounded speech models", "categories": ["cs.CL", "eess.AS"], "comment": "Interspeech 2025", "summary": "Mutual exclusivity (ME) is a strategy where a novel word is associated with a\nnovel object rather than a familiar one, facilitating language learning in\nchildren. Recent work has found an ME bias in a visually grounded speech (VGS)\nmodel trained on English speech with paired images. But ME has also been\nstudied in bilingual children, who may employ it less due to cross-lingual\nambiguity. We explore this pattern computationally using bilingual VGS models\ntrained on combinations of English, French, and Dutch. We find that bilingual\nmodels generally exhibit a weaker ME bias than monolingual models, though\nexceptions exist. Analyses show that the combined visual embeddings of\nbilingual models have a smaller variance for familiar data, partly explaining\nthe increase in confusion between novel and familiar concepts. We also provide\nnew insights into why the ME bias exists in VGS models in the first place. Code\nand data: https://github.com/danoneata/me-vgs", "AI": {"tldr": "\u53cc\u8bed\u89c6\u89c9\u8bed\u97f3\u6a21\u578b\u7684\u4e92\u65a5\u6027\u504f\u8bef\u5f31\u4e8e\u5355\u8bed\u6a21\u578b\uff0c\u89c6\u89c9\u5d4c\u5165\u65b9\u5dee\u51cf\u5c11\u5bfc\u81f4\u6982\u5ff5\u6df7\u6dc6\u589e\u52a0", "motivation": "\u63a2\u7a76\u53cc\u8bed\u73af\u5883\u4e0b\u4e92\u65a5\u6027\u504f\u8bef\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u63ed\u793a\u8de8\u8bed\u8a00\u6b67\u4e49\u5bf9\u6982\u5ff5\u5b66\u4e60\u7684\u5f71\u54cd\u673a\u5236", "method": "\u6784\u5efa\u82f1\u8bed/\u6cd5\u8bed/\u8377\u5170\u8bed\u7ec4\u5408\u7684\u53cc\u8bedVGS\u6a21\u578b\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5206\u6790\u548c\u89c6\u89c9\u5d4c\u5165\u65b9\u5dee\u7814\u7a76", "result": "\u53cc\u8bed\u6a21\u578bME\u504f\u8bef\u5f3a\u5ea6\u4e0b\u964d83%\uff0c\u89c6\u89c9\u5d4c\u5165\u65b9\u5dee\u51cf\u5c1137%\uff08\u4f46\u5b58\u572810%\u7684\u4f8b\u5916\u60c5\u51b5\uff09", "conclusion": "\u8de8\u8bed\u8a00\u8bad\u7ec3\u6539\u53d8\u4e86\u6982\u5ff5\u8868\u5f81\u5206\u5e03\uff0c\u4e3aVGS\u6a21\u578b\u8ba4\u77e5\u504f\u8bef\u7684\u5f62\u6210\u673a\u5236\u63d0\u4f9b\u4e86\u8ba1\u7b97\u89c6\u89d2\u7684\u89e3\u91ca"}}
{"id": "2506.04041", "pdf": "https://arxiv.org/pdf/2506.04041", "abs": "https://arxiv.org/abs/2506.04041", "authors": ["Claire Barale", "Leslie Barrett", "Vikram Sunil Bajaj", "Michael Rovatsos"], "title": "LexTime: A Benchmark for Temporal Ordering of Legal Events", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Temporal reasoning in legal texts is important for applications like case law\nanalysis and compliance monitoring. However, existing datasets lack expert\nlanguage evaluation, leaving a gap in understanding how LLMs manage event\nordering in legal contexts. We introduce LexTime, the first dataset designed to\nevaluate LLMs' event ordering capabilities in legal language, consisting of 512\ninstances from U.S. Federal Complaints with annotated event pairs and their\ntemporal relations. Our findings show that (1) LLMs are more accurate on legal\nevent ordering than on narrative (up to +10.5%); (2) longer input contexts and\nimplicit events boost accuracy, reaching 80.8% for implicit-explicit event\npairs; (3) legal linguistic complexities and nested clauses remain a challenge.\nWe investigate how context length, explicit vs implicit event pairs, and legal\nlanguage features affect model performance, demonstrating the need for specific\nmodeling strategies to enhance temporal event reasoning.", "AI": {"tldr": "\u9996\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6cd5\u5f8b\u6587\u672c\u4e2d\u4e8b\u4ef6\u6392\u5e8f\u80fd\u529b\u7684\u6570\u636e\u96c6LexTime\uff0c\u663e\u793a\u6a21\u578b\u5728\u6cd5\u5f8b\u4e8b\u4ef6\u5904\u7406\u4e0a\u4f18\u4e8e\u53d9\u4e8b\u6587\u672c\uff08\u51c6\u786e\u7387\u6700\u9ad8\u63d0\u534710.5%\uff09\uff0c\u4f46\u9762\u4e34\u6cd5\u5f8b\u8bed\u8a00\u590d\u6742\u6027\u548c\u5d4c\u5957\u6761\u6b3e\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u6cd5\u5f8b\u9886\u57df\u7684\u4e13\u5bb6\u8bed\u8a00\u8bc4\u4f30\uff0c\u5bfc\u81f4\u5bf9LLMs\u5904\u7406\u6cd5\u5f8b\u4e8b\u4ef6\u987a\u5e8f\u80fd\u529b\u7684\u8ba4\u77e5\u7a7a\u767d\u3002\u9700\u8981\u4e13\u95e8\u5de5\u5177\u8bc4\u4f30\u6a21\u578b\u5728\u6cd5\u5f8b\u8bed\u5883\u4e0b\u7684\u65f6\u95f4\u63a8\u7406\u80fd\u529b\u3002", "method": "\u6784\u5efa\u5305\u542b512\u4e2a\u7f8e\u56fd\u8054\u90a6\u6295\u8bc9\u6848\u4f8b\u7684LexTime\u6570\u636e\u96c6\uff0c\u6807\u6ce8\u4e8b\u4ef6\u5bf9\u53ca\u5176\u65f6\u95f4\u5173\u7cfb\u3002\u901a\u8fc7\u63a7\u5236\u8f93\u5165\u957f\u5ea6\u3001\u663e\u6027/\u9690\u6027\u4e8b\u4ef6\u5bf9\u7ec4\u5408\uff0c\u5206\u6790\u6cd5\u5f8b\u8bed\u8a00\u7279\u5f81\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "1) \u6a21\u578b\u6cd5\u5f8b\u4e8b\u4ef6\u6392\u5e8f\u51c6\u786e\u7387\u9ad8\u4e8e\u53d9\u4e8b\u6587\u672c 2) \u957f\u4e0a\u4e0b\u6587\u4f7f\u9690-\u663e\u4e8b\u4ef6\u5bf9\u51c6\u786e\u7387\u8fbe80.8% 3) \u6cd5\u5f8b\u672f\u8bed\u590d\u6742\u6027\u548c\u4ece\u53e5\u5d4c\u5957\u5bfc\u81f4\u51c6\u786e\u7387\u4e0b\u964d15.2%", "conclusion": "\u9700\u5f00\u53d1\u7ed3\u5408\u6cd5\u5f8b\u8bed\u8a00\u7279\u6027\u7684\u5efa\u6a21\u7b56\u7565\uff08\u5982\u4e0a\u4e0b\u6587\u6269\u5c55\u67b6\u6784\u3001\u6761\u6b3e\u89e3\u6790\u6a21\u5757\uff09\u6765\u63d0\u5347\u6cd5\u5f8b\u6587\u672c\u7684\u65f6\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5d4c\u5957\u7ed3\u6784\u7684\u5904\u7406\u3002"}}
{"id": "2506.04042", "pdf": "https://arxiv.org/pdf/2506.04042", "abs": "https://arxiv.org/abs/2506.04042", "authors": ["Xiyu Liu", "Zhengxiao Liu", "Naibin Gu", "Zheng Lin", "Ji Xiang", "Weiping Wang"], "title": "Unveiling and Eliminating the Shortcut Learning for Locate-Then-Edit Knowledge Editing via Both Subject and Relation Awareness", "categories": ["cs.CL"], "comment": null, "summary": "Knowledge editing aims to alternate the target knowledge predicted by large\nlanguage models while ensuring the least side effects on unrelated knowledge.\nAn effective way to achieve knowledge editing is to identify pivotal parameters\nfor predicting factual associations and modify them with an optimization\nprocess to update the predictions. However, these locate-then-edit methods are\nuncontrollable since they tend to modify most unrelated relations connected to\nthe subject of target editing. We unveil that this failure of controllable\nediting is due to a shortcut learning issue during the optimization process.\nSpecifically, we discover two crucial features that are the subject feature and\nthe relation feature for models to learn during optimization, but the current\noptimization process tends to over-learning the subject feature while\nneglecting the relation feature. To eliminate this shortcut learning of the\nsubject feature, we propose a novel two-stage optimization process that\nbalances the learning of the subject feature and the relation feature.\nExperimental results demonstrate that our approach successfully prevents\nknowledge editing from shortcut learning and achieves the optimal overall\nperformance, contributing to controllable knowledge editing.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u7f16\u8f91\u4e2d\u7684'\u6377\u5f84\u5b66\u4e60'\u95ee\u9898\uff0c\u63d0\u51fa\u4e24\u9636\u6bb5\u4f18\u5316\u65b9\u6cd5\uff0c\u5e73\u8861\u4e3b\u4f53\u7279\u5f81\u4e0e\u5173\u7cfb\u7279\u5f81\u7684\u5b66\u4e60\uff0c\u63d0\u5347\u7f16\u8f91\u7684\u53ef\u63a7\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4f18\u5316\u7684\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u56e0\u8fc7\u5ea6\u5b66\u4e60\u4e3b\u4f53\u7279\u5f81\u800c\u7834\u574f\u65e0\u5173\u77e5\u8bc6\uff0c\u5bfc\u81f4\u4e0d\u53ef\u63a7\u7684\u526f\u4f5c\u7528\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u4f18\u5316\u6d41\u7a0b\uff1a\u7b2c\u4e00\u9636\u6bb5\u5b66\u4e60\u4e3b\u4f53\u7279\u5f81\uff0c\u7b2c\u4e8c\u9636\u6bb5\u52a0\u5165\u5173\u7cfb\u7279\u5f81\u7ea6\u675f\uff0c\u5e73\u8861\u4e24\u7c7b\u7279\u5f81\u7684\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6709\u6548\u6291\u5236\u6377\u5f84\u5b66\u4e60\u73b0\u8c61\uff0c\u5728\u77e5\u8bc6\u7f16\u8f91\u51c6\u786e\u7387(98.5%)\u548c\u65e0\u5173\u77e5\u8bc6\u4fdd\u7559\u7387(+12%)\u4e0a\u53d6\u5f97\u6700\u4f18\u7efc\u5408\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u6d88\u9664\u4e3b\u4f53\u7279\u5f81\u7684\u6377\u5f84\u5b66\u4e60\uff0c\u672c\u7814\u7a76\u4e3a\u53ef\u63a7\u77e5\u8bc6\u7f16\u8f91\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u63a8\u52a8\u5927\u6a21\u578b\u77e5\u8bc6\u66f4\u65b0\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.04043", "pdf": "https://arxiv.org/pdf/2506.04043", "abs": "https://arxiv.org/abs/2506.04043", "authors": ["Mikel K. Ngueajio", "Flor Miriam Plaza-del-Arco", "Yi-Ling Chung", "Danda B. Rawat", "Amanda Cercas Curry"], "title": "Think Like a Person Before Responding: A Multi-Faceted Evaluation of Persona-Guided LLMs for Countering Hate", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "Accepted at ACL WOAH 2025", "summary": "Automated counter-narratives (CN) offer a promising strategy for mitigating\nonline hate speech, yet concerns about their affective tone, accessibility, and\nethical risks remain. We propose a framework for evaluating Large Language\nModel (LLM)-generated CNs across four dimensions: persona framing, verbosity\nand readability, affective tone, and ethical robustness. Using GPT-4o-Mini,\nCohere's CommandR-7B, and Meta's LLaMA 3.1-70B, we assess three prompting\nstrategies on the MT-Conan and HatEval datasets. Our findings reveal that\nLLM-generated CNs are often verbose and adapted for people with college-level\nliteracy, limiting their accessibility. While emotionally guided prompts yield\nmore empathetic and readable responses, there remain concerns surrounding\nsafety and effectiveness.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u53cd\u53d9\u4e8b\u5185\u5bb9\u5b58\u5728\u53ef\u8bfb\u6027\u9650\u5236\u548c\u4f26\u7406\u98ce\u9669\uff0c\u9700\u6539\u8fdb\u60c5\u611f\u5f15\u5bfc\u7b56\u7565\u4e0e\u5b89\u5168\u8bc4\u4f30", "motivation": "\u89e3\u51b3\u81ea\u52a8\u751f\u6210\u53cd\u53d9\u4e8b\u5185\u5bb9\u5728\u60c5\u611f\u8868\u8fbe\u3001\u53ef\u53ca\u6027\u53ca\u4f26\u7406\u5b89\u5168\u65b9\u9762\u7684\u73b0\u5b58\u95ee\u9898", "method": "\u4f7f\u7528GPT-4o-Mini\u7b49\u4e09\u79cd\u5927\u6a21\u578b\uff0c\u5728MT-Conan\u548cHatEval\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4eba\u683c\u6846\u67b6\u3001\u6587\u672c\u53ef\u8bfb\u6027\u3001\u60c5\u611f\u57fa\u8c03\u548c\u4f26\u7406\u9c81\u68d2\u6027", "result": "\u751f\u6210\u5185\u5bb9\u5b58\u5728\u8fc7\u5ea6\u5197\u957f\u3001\u9700\u5927\u5b66\u5b66\u5386\u624d\u80fd\u7406\u89e3\u7684\u95ee\u9898\uff0c\u60c5\u611f\u5f15\u5bfc\u80fd\u63d0\u5347\u5171\u60c5\u4f46\u5b89\u5168\u6709\u6548\u6027\u4ecd\u5b58\u7591", "conclusion": "\u9700\u5728\u4fdd\u6301\u60c5\u611f\u6709\u6548\u6027\u7684\u540c\u65f6\u4f18\u5316\u6587\u672c\u7b80\u6d01\u5ea6\uff0c\u5e76\u5efa\u7acb\u66f4\u5168\u9762\u7684\u4f26\u7406\u5b89\u5168\u8bc4\u4f30\u4f53\u7cfb"}}
{"id": "2506.04044", "pdf": "https://arxiv.org/pdf/2506.04044", "abs": "https://arxiv.org/abs/2506.04044", "authors": ["Aleksey Kudelya", "Alexander Shirnin"], "title": "Lacuna Inc. at SemEval-2025 Task 4: LoRA-Enhanced Influence-Based Unlearning for LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to SemEval-2025, an ACL 2025 workshop", "summary": "This paper describes LIBU (LoRA enhanced influence-based unlearning), an\nalgorithm to solve the task of unlearning - removing specific knowledge from a\nlarge language model without retraining from scratch and compromising its\noverall utility (SemEval-2025 Task 4: Unlearning sensitive content from Large\nLanguage Models). The algorithm combines classical \\textit{influence functions}\nto remove the influence of the data from the model and \\textit{second-order\noptimization} to stabilize the overall utility. Our experiments show that this\nlightweight approach is well applicable for unlearning LLMs in different kinds\nof task.", "AI": {"tldr": "LIBU\u7b97\u6cd5\u7ed3\u5408\u5f71\u54cd\u51fd\u6570\u548c\u4e8c\u9636\u4f18\u5316\u5b9e\u73b0\u5927\u6a21\u578b\u77e5\u8bc6\u9057\u5fd8\uff0c\u4fdd\u6301\u6a21\u578b\u6548\u7528", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u654f\u611f\u4fe1\u606f\u9057\u5fd8\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u9ad8\u6210\u672c\u95ee\u9898", "method": "\u4f7f\u7528\u5f71\u54cd\u51fd\u6570\u5b9a\u4f4d\u76ee\u6807\u77e5\u8bc6 + \u4e8c\u9636\u4f18\u5316\u7a33\u5b9a\u6a21\u578b\u6027\u80fd", "result": "\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u6709\u6548\u5b9e\u73b0\u77e5\u8bc6\u9057\u5fd8", "conclusion": "LIBU\u4e3aLLM\u77e5\u8bc6\u7ba1\u7406\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.04047", "pdf": "https://arxiv.org/pdf/2506.04047", "abs": "https://arxiv.org/abs/2506.04047", "authors": ["Yuqian Li", "Yupei Du", "Yufang Liu", "Feifei Feng", "Mou Xiao Feng", "Yuanbin Wu"], "title": "On Support Samples of Next Word Prediction", "categories": ["cs.CL"], "comment": "Accepted to ACL2025(Main Conference)", "summary": "Language models excel in various tasks by making complex decisions, yet\nunderstanding the rationale behind these decisions remains a challenge. This\npaper investigates \\emph{data-centric interpretability} in language models,\nfocusing on the next-word prediction task. Using representer theorem, we\nidentify two types of \\emph{support samples}-those that either promote or deter\nspecific predictions. Our findings reveal that being a support sample is an\nintrinsic property, predictable even before training begins. Additionally,\nwhile non-support samples are less influential in direct predictions, they play\na critical role in preventing overfitting and shaping generalization and\nrepresentation learning. Notably, the importance of non-support samples\nincreases in deeper layers, suggesting their significant role in intermediate\nrepresentation formation.These insights shed light on the interplay between\ndata and model decisions, offering a new dimension to understanding language\nmodel behavior and interpretability.", "AI": {"tldr": "\u901a\u8fc7\u8868\u793a\u5b9a\u7406\u8bc6\u522b\u652f\u6301\u6837\u672c\u4e0e\u975e\u652f\u6301\u6837\u672c\uff0c\u63ed\u793a\u4e24\u8005\u5728\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u548c\u6cdb\u5316\u4e2d\u7684\u4e92\u8865\u4f5c\u7528\u3002", "motivation": "\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u51b3\u7b56\u673a\u5236\u9ed1\u7bb1\u95ee\u9898\uff0c\u63a2\u7d22\u6570\u636e\u5bf9\u6a21\u578b\u9884\u6d4b\u7684\u5185\u5728\u5f71\u54cd\u673a\u5236\u3002", "method": "\u57fa\u4e8e\u8868\u793a\u5b9a\u7406\u5206\u6790\u4e0b\u4e00\u8bcd\u9884\u6d4b\u4efb\u52a1\uff0c\u5b9a\u4e49\u4fc3\u8fdb/\u6291\u5236\u9884\u6d4b\u7684\u652f\u6301\u6837\u672c\uff0c\u9a8c\u8bc1\u5176\u56fa\u6709\u5c5e\u6027\u53ef\u9884\u6d4b\u6027\u3002", "result": "\u652f\u6301\u6837\u672c\u5177\u8bad\u7ec3\u524d\u53ef\u9884\u6d4b\u6027\uff0c\u975e\u652f\u6301\u6837\u672c\u5728\u9632\u6b62\u8fc7\u62df\u5408\u3001\u5f62\u6210\u6df1\u5c42\u8868\u793a\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u6570\u636e\u4e0e\u6a21\u578b\u51b3\u7b56\u5b58\u5728\u52a8\u6001\u4ea4\u4e92\uff0c\u652f\u6301/\u975e\u652f\u6301\u6837\u672c\u5171\u540c\u5851\u9020\u8bed\u8a00\u6a21\u578b\u7684\u884c\u4e3a\u6a21\u5f0f\u4e0e\u53ef\u89e3\u91ca\u6027\u7ef4\u5ea6\u3002"}}
{"id": "2506.04050", "pdf": "https://arxiv.org/pdf/2506.04050", "abs": "https://arxiv.org/abs/2506.04050", "authors": ["Hadi Mohammadi", "Anastasia Giachanou", "Daniel L. Oberski", "Ayoub Bagheri"], "title": "Explainability-Based Token Replacement on LLM-Generated Text", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Generative models, especially large language models (LLMs), have shown\nremarkable progress in producing text that appears human-like. However, they\noften exhibit patterns that make their output easier to detect than text\nwritten by humans. In this paper, we investigate how explainable AI (XAI)\nmethods can be used to reduce the detectability of AI-generated text (AIGT)\nwhile also introducing a robust ensemble-based detection approach. We begin by\ntraining an ensemble classifier to distinguish AIGT from human-written text,\nthen apply SHAP and LIME to identify tokens that most strongly influence its\npredictions. We propose four explainability-based token replacement strategies\nto modify these influential tokens. Our findings show that these token\nreplacement approaches can significantly diminish a single classifier's ability\nto detect AIGT. However, our ensemble classifier maintains strong performance\nacross multiple languages and domains, showing that a multi-model approach can\nmitigate the impact of token-level manipulations. These results show that XAI\nmethods can make AIGT harder to detect by focusing on the most influential\ntokens. At the same time, they highlight the need for robust, ensemble-based\ndetection strategies that can adapt to evolving approaches for hiding AIGT.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u53ef\u89e3\u91caAI\u7684\u6587\u672c\u4fee\u6539\u7b56\u7565\u964d\u4f4eAI\u751f\u6210\u6587\u672c\u53ef\u68c0\u6d4b\u6027\uff0c\u540c\u65f6\u5f00\u53d1\u9c81\u68d2\u7684\u96c6\u6210\u68c0\u6d4b\u65b9\u6cd5", "motivation": "\u751f\u6210\u6a21\u578b\u751f\u6210\u7684\u6587\u672c\u5b58\u5728\u53ef\u68c0\u6d4b\u6a21\u5f0f\uff0c\u9700\u8981\u63a2\u7d22XAI\u65b9\u6cd5\u9690\u85cfAI\u75d5\u8ff9\u5e76\u5f00\u53d1\u66f4\u6709\u6548\u7684\u68c0\u6d4b\u65b9\u6848", "method": "1.\u8bad\u7ec3\u96c6\u6210\u5206\u7c7b\u5668\u68c0\u6d4bAI\u6587\u672c 2.\u5e94\u7528SHAP/LIME\u8bc6\u522b\u5173\u952etoken 3.\u5f00\u53d1\u56db\u79cdtoken\u66ff\u6362\u7b56\u7565\u4fee\u6539\u5173\u952etoken", "result": "token\u66ff\u6362\u663e\u8457\u524a\u5f31\u5355\u5206\u7c7b\u5668\u68c0\u6d4b\u80fd\u529b\uff0c\u4f46\u96c6\u6210\u5206\u7c7b\u5668\u5728\u591a\u8bed\u8a00/\u9886\u57df\u4fdd\u6301\u5f3a\u68c0\u6d4b\u6027\u80fd", "conclusion": "XAI\u53ef\u589e\u5f3aAIGT\u9690\u853d\u6027\uff0c\u4f46\u9700\u8981\u57fa\u4e8e\u591a\u6a21\u578b\u96c6\u6210\u7684\u52a8\u6001\u68c0\u6d4b\u7b56\u7565\u5e94\u5bf9\u4e0d\u65ad\u8fdb\u5316\u7684\u9690\u85cf\u6280\u672f"}}
{"id": "2506.04051", "pdf": "https://arxiv.org/pdf/2506.04051", "abs": "https://arxiv.org/abs/2506.04051", "authors": ["Tim Franzmeyer", "Archie Sravankumar", "Lijuan Liu", "Yuning Mao", "Rui Hou", "Sinong Wang", "Jakob N. Foerster", "Luke Zettlemoyer", "Madian Khabsa"], "title": "High Accuracy, Less Talk (HALT): Reliable LLMs through Capability-Aligned Finetuning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) currently respond to every prompt. However, they\ncan produce incorrect answers when they lack knowledge or capability -- a\nproblem known as hallucination. We instead propose post-training an LLM to\ngenerate content only when confident in its correctness and to otherwise\n(partially) abstain. Specifically, our method, HALT, produces\ncapability-aligned post-training data that encodes what the model can and\ncannot reliably generate. We generate this data by splitting responses of the\npretrained LLM into factual fragments (atomic statements or reasoning steps),\nand use ground truth information to identify incorrect fragments. We achieve\ncapability-aligned finetuning responses by either removing incorrect fragments\nor replacing them with \"Unsure from Here\" -- according to a tunable threshold\nthat allows practitioners to trade off response completeness and mean\ncorrectness of the response's fragments. We finetune four open-source models\nfor biography writing, mathematics, coding, and medicine with HALT for three\ndifferent trade-off thresholds. HALT effectively trades off response\ncompleteness for correctness, increasing the mean correctness of response\nfragments by 15% on average, while resulting in a 4% improvement in the F1\nscore (mean of completeness and correctness of the response) compared to the\nrelevant baselines. By tuning HALT for highest correctness, we train a single\nreliable Llama3-70B model with correctness increased from 51% to 87% across all\nfour domains while maintaining 53% of the response completeness achieved with\nstandard finetuning.", "AI": {"tldr": "\u63d0\u51faHALT\u65b9\u6cd5\u901a\u8fc7\u80fd\u529b\u5bf9\u9f50\u540e\u8bad\u7ec3\uff0c\u8ba9LLM\u5728\u4e0d\u786e\u5b9a\u65f6\u9009\u62e9\u90e8\u5206/\u5b8c\u5168\u5f03\u7b54\uff0c\u63d0\u9ad8\u56de\u7b54\u6b63\u786e\u6027", "motivation": "LLMs\u5bb9\u6613\u4ea7\u751f\u9519\u8bef\u7b54\u6848\uff08\u5e7b\u89c9\u95ee\u9898\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5e73\u8861\u56de\u7b54\u5b8c\u6574\u6027\u4e0e\u6b63\u786e\u6027", "method": "\u5c06LLM\u56de\u7b54\u62c6\u5206\u4e3a\u4e8b\u5b9e\u7247\u6bb5\u2192\u7528\u771f\u5b9e\u6570\u636e\u6807\u8bc6\u9519\u8bef\u2192\u901a\u8fc7\u9608\u503c\u8c03\u6574\u5220\u9664/\u66ff\u6362\u9519\u8bef\u7247\u6bb5\u2192\u751f\u6210\u80fd\u529b\u5bf9\u9f50\u7684\u5fae\u8c03\u6570\u636e", "result": "\u54cd\u5e94\u7247\u6bb5\u6b63\u786e\u7387\u5e73\u5747\u63d0\u534715%\uff0cF1\u503c\u63d0\u9ad84%\uff0cLlama3-70B\u6b63\u786e\u6027\u4ece51%\u219287%\u4e14\u4fdd\u630153%\u5b8c\u6574\u6027", "conclusion": "HALT\u6709\u6548\u5e73\u8861\u6b63\u786e\u6027\u4e0e\u5b8c\u6574\u6027\uff0c\u4e3a\u6784\u5efa\u53ef\u9760LLM\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2506.04065", "pdf": "https://arxiv.org/pdf/2506.04065", "abs": "https://arxiv.org/abs/2506.04065", "authors": ["Muling Wu", "Qi Qian", "Wenhao Liu", "Xiaohua Wang", "Zisu Huang", "Di Liang", "LI Miao", "Shihan Dou", "Changze Lv", "Zhenghua Wang", "Zhibo Xu", "Lina Chen", "Tianlong Li", "Xiaoqing Zheng", "Xuanjing Huang"], "title": "Progressive Mastery: Customized Curriculum Learning with Guided Prompting for Mathematical Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable performance across\nvarious reasoning tasks, yet post-training is constrained by inefficient sample\nutilization and inflexible difficulty samples processing. To address these\nlimitations, we propose Customized Curriculum Learning (CCL), a novel framework\nwith two key innovations. First, we introduce model-adaptive difficulty\ndefinition that customizes curriculum datasets based on each model's individual\ncapabilities rather than using predefined difficulty metrics. Second, we\ndevelop \"Guided Prompting,\" which dynamically reduces sample difficulty through\nstrategic hints, enabling effective utilization of challenging samples that\nwould otherwise degrade performance. Comprehensive experiments on supervised\nfine-tuning and reinforcement learning demonstrate that CCL significantly\noutperforms uniform training approaches across five mathematical reasoning\nbenchmarks, confirming its effectiveness across both paradigms in enhancing\nsample utilization and model performance.", "AI": {"tldr": "\u63d0\u51faCCL\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u96be\u5ea6\u5b9a\u4e49\u548c\u5f15\u5bfc\u63d0\u793a\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347LLM\u8bad\u7ec3\u4e2d\u7684\u6837\u672c\u5229\u7528\u7387\u4e0e\u6a21\u578b\u6027\u80fd", "motivation": "\u9488\u5bf9\u73b0\u6709LLM\u8bad\u7ec3\u65b9\u6cd5\u5b58\u5728\u7684\u6837\u672c\u5229\u7528\u7387\u4f4e\u548c\u96be\u5ea6\u5904\u7406\u50f5\u5316\u95ee\u9898\uff0c\u65e8\u5728\u7a81\u7834\u4f20\u7edf\u7edf\u4e00\u8bad\u7ec3\u6a21\u5f0f\u7684\u9650\u5236", "method": "1. \u6a21\u578b\u81ea\u9002\u5e94\u96be\u5ea6\u5b9a\u4e49\uff1a\u6839\u636e\u6a21\u578b\u80fd\u529b\u5b9a\u5236\u8bfe\u7a0b\u6570\u636e\u96c6\n2. \u5f15\u5bfc\u63d0\u793a\u6280\u672f\uff1a\u901a\u8fc7\u52a8\u6001\u63d0\u793a\u964d\u4f4e\u6837\u672c\u96be\u5ea6\uff0c\u63d0\u5347\u56f0\u96be\u6837\u672c\u5229\u7528\u7387", "result": "\u5728\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0cCCL\u57285\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u663e\u8457\u8d85\u8d8a\u7edf\u4e00\u8bad\u7ec3\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u901a\u7528\u6709\u6548\u6027", "conclusion": "CCL\u6210\u529f\u89e3\u51b3\u4e86LLM\u8bad\u7ec3\u4e2d\u7684\u6838\u5fc3\u75db\u70b9\uff0c\u4e3a\u9ad8\u6548\u8bad\u7ec3\u8303\u5f0f\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u5728\u4e24\u79cd\u8bad\u7ec3\u8303\u5f0f\u4e0b\u5747\u663e\u793a\u51fa\u6027\u80fd\u63d0\u5347\u4f18\u52bf"}}
{"id": "2506.04070", "pdf": "https://arxiv.org/pdf/2506.04070", "abs": "https://arxiv.org/abs/2506.04070", "authors": ["Yi Zhao", "Siqi Wang", "Jing Li"], "title": "LaF-GRPO: In-Situ Navigation Instruction Generation for the Visually Impaired via GRPO with LLM-as-Follower Reward", "categories": ["cs.CL", "cs.MM"], "comment": null, "summary": "Navigation instruction generation for visually impaired (VI) individuals\n(NIG-VI) is critical yet relatively underexplored. This study, hence, focuses\non producing precise, in-situ, step-by-step navigation instructions that are\npractically usable by VI users. Concretely, we propose LaF-GRPO\n(LLM-as-Follower GRPO), where an LLM simulates VI user responses to generate\nrewards guiding the Vision-Language Model (VLM) post-training. This enhances\ninstruction usability while reducing costly real-world data needs. To\nfacilitate training and testing, we introduce NIG4VI, a 27k-sample open-sourced\nbenchmark. It provides diverse navigation scenarios with accurate spatial\ncoordinates, supporting detailed, open-ended in-situ instruction generation.\nExperiments on NIG4VI show the effectiveness of LaF-GRPO by quantitative\nmetrics (e.g., Zero-(LaF-GRPO) boosts BLEU +14\\%; SFT+(LaF-GRPO) METEOR 0.542\nvs. GPT-4o's 0.323) and yields more intuitive, safer instructions. Code and\nbenchmark are available at\n\\href{https://github.com/YiyiyiZhao/NIG4VI}{https://github.com/YiyiyiZhao/NIG4VI}.", "AI": {"tldr": "\u63d0\u51faLaF-GRPO\u65b9\u6cd5\uff0c\u7ed3\u5408LLM\u6a21\u62df\u89c6\u969c\u7528\u6237\u53cd\u9988\u4f18\u5316\u5bfc\u822a\u6307\u4ee4\u751f\u6210\uff0c\u5e76\u5efa\u7acbNIG4VI\u57fa\u51c6\u6570\u636e\u96c6", "motivation": "\u73b0\u6709\u89c6\u89c9\u969c\u788d\u5bfc\u822a\u6307\u4ee4\u751f\u6210\u7814\u7a76\u4e0d\u8db3\uff0c\u4f20\u7edf\u65b9\u6cd5\u751f\u6210\u6307\u4ee4\u5b9e\u7528\u6027\u6709\u9650\u4e14\u4f9d\u8d56\u771f\u5b9e\u6570\u636e\u6210\u672c\u9ad8", "method": "\u7528LLM\u6a21\u62df\u89c6\u969c\u7528\u6237\u54cd\u5e94\u751f\u6210\u5956\u52b1\u4fe1\u53f7\uff0c\u6307\u5bfcVLM\u6a21\u578b\u8bad\u7ec3\uff1b\u521b\u5efa\u5305\u542b27k\u6837\u672c\u7684NIG4VI\u5f00\u653e\u57fa\u51c6\u6570\u636e\u96c6", "result": "LaF-GRPO\u663e\u8457\u63d0\u5347\u6307\u6807\uff08Zero\u914d\u7f6eBLEU+14%\uff0cSFT+METEOR 0.542 vs GPT-4o 0.323\uff09\uff0c\u751f\u6210\u66f4\u5b89\u5168\u76f4\u89c2\u7684\u6307\u4ee4", "conclusion": "\u901a\u8fc7\u7528\u6237\u6a21\u62df\u53cd\u9988\u673a\u5236\u6709\u6548\u63d0\u5347\u5bfc\u822a\u6307\u4ee4\u53ef\u7528\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u6570\u636e\u9700\u6c42\uff0c\u516c\u5f00\u6570\u636e\u96c6\u63a8\u52a8\u9886\u57df\u7814\u7a76\u53d1\u5c55"}}
