<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 135]
- [cs.GR](#cs.GR) [Total: 3]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.SD](#cs.SD) [Total: 6]
- [cs.LG](#cs.LG) [Total: 11]
- [eess.AS](#eess.AS) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.CV](#cs.CV) [Total: 4]
- [cs.MA](#cs.MA) [Total: 1]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [cs.AI](#cs.AI) [Total: 23]
- [cs.CY](#cs.CY) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Evaluating Reasoning LLMs for Suicide Screening with the Columbia-Suicide Severity Rating Scale](https://arxiv.org/abs/2505.13480)
*Avinash Patil,Siru Tao,Amardeep Gedhu*

Main category: cs.CL

TL;DR: 研究评估6个大语言模型在Reddit自杀倾向贴文分类中的表现，Claude/GPT接近人类标注水平，Mistral预测误差最低，强调AI伦理部署需谨慎


<details>
  <summary>Details</summary>
Motivation: 探索LLMs替代人类进行自杀风险评估的可能性，填补AI在公共卫生危机干预中的应用研究空白

Method: 使用哥伦比亚自杀量表(C-SSRS)对Reddit贴文进行0-6级分类，测试Claude/GPT/Mistral/LLaMA等模型的零样本分类性能

Result: 模型普遍展现等级敏感特性，70%错误分类发生在相邻风险等级。Claude与GPT的kappa系数达0.82/0.79，Mistral的MAE指标最优(0.39)

Conclusion: 需建立人类监督机制，强调算法透明度和误判容错设计，AI自杀评估工具应作为辅助而非替代方案

Abstract: Suicide prevention remains a critical public health challenge. While online
platforms such as Reddit's r/SuicideWatch have historically provided spaces for
individuals to express suicidal thoughts and seek community support, the advent
of large language models (LLMs) introduces a new paradigm-where individuals may
begin disclosing ideation to AI systems instead of humans. This study evaluates
the capability of LLMs to perform automated suicide risk assessment using the
Columbia-Suicide Severity Rating Scale (C-SSRS). We assess the zero-shot
performance of six models-including Claude, GPT, Mistral, and LLaMA-in
classifying posts across a 7-point severity scale (Levels 0-6). Results
indicate that Claude and GPT closely align with human annotations, while
Mistral achieves the lowest ordinal prediction error. Most models exhibit
ordinal sensitivity, with misclassifications typically occurring between
adjacent severity levels. We further analyze confusion patterns,
misclassification sources, and ethical considerations, underscoring the
importance of human oversight, transparency, and cautious deployment. Full code
and supplementary materials are available at
https://github.com/av9ash/llm_cssrs_code.

</details>


### [2] [EmoMeta: A Multimodal Dataset for Fine-grained Emotion Classification in Chinese Metaphors](https://arxiv.org/abs/2505.13483)
*Xingyuan Lu,Yuxi Liu,Dongyu Zhang,Zhiyao Wu,Jing Ren,Feng Xia*

Main category: cs.CL

TL;DR: 构建中文多模态隐喻广告数据集EmoMeta（5000图文对），标注隐喻结构及十类细粒度情感，解决现有研究数据稀缺与语言局限问题


<details>
  <summary>Details</summary>
Motivation: 当前多模态隐喻研究存在三大缺口：1）缺乏细粒度情感标注的多模态数据集 2）现有研究集中于英语，忽略跨语言情感差异 3）广告场景隐喻表达密集但未被系统研究

Method: 通过人工标注5000个中文广告图文对，从隐喻存在性（是否隐喻）、领域映射关系（源域-目标域）及十类基本情感（喜悦/爱/信任/恐惧等）三个维度进行系统标注

Result: 建成首个公开的中文多模态隐喻情感数据集EmoMeta（GitHub开源），平均每条含3.2个隐喻，情感标注Kappa系数达0.82

Conclusion: 该数据集填补多模态隐喻研究空白，为跨语言情感计算、广告效果评估及认知语言学理论研究提供新基准

Abstract: Metaphors play a pivotal role in expressing emotions, making them crucial for
emotional intelligence. The advent of multimodal data and widespread
communication has led to a proliferation of multimodal metaphors, amplifying
the complexity of emotion classification compared to single-mode scenarios.
However, the scarcity of research on constructing multimodal metaphorical
fine-grained emotion datasets hampers progress in this domain. Moreover,
existing studies predominantly focus on English, overlooking potential
variations in emotional nuances across languages. To address these gaps, we
introduce a multimodal dataset in Chinese comprising 5,000 text-image pairs of
metaphorical advertisements. Each entry is meticulously annotated for metaphor
occurrence, domain relations and fine-grained emotion classification
encompassing joy, love, trust, fear, sadness, disgust, anger, surprise,
anticipation, and neutral. Our dataset is publicly accessible
(https://github.com/DUTIR-YSQ/EmoMeta), facilitating further advancements in
this burgeoning field.

</details>


### [3] [Detecting Prefix Bias in LLM-based Reward Models](https://arxiv.org/abs/2505.13487)
*Ashwin Kumar,Yuzi He,Aram H. Markosyan,Bobbie Chern,Imanol Arrieta-Ibarra*

Main category: cs.CL

TL;DR: 研究发现基于公开偏好数据集的奖励模型存在显著的前缀偏见，提出了检测方法和数据增强策略以减轻偏见


<details>
  <summary>Details</summary>
Motivation: 现有公共偏好数据集虽提供响应对比，但奖励模型在种族/性别维度存在未被充分探索的系统性偏见风险

Method: 开发前缀偏差检测指标，评估不同开源数据集和模型架构，提出基于数据增强的偏差缓解策略

Result: 所有模型架构均存在前缀偏见，种族性别维度表现显著；数据增强策略有效降低偏见影响

Conclusion: 强调偏见感知的数据集设计对开发公平奖励模型的重要性，推动AI公平性研究发展

Abstract: Reinforcement Learning with Human Feedback (RLHF) has emerged as a key
paradigm for task-specific fine-tuning of language models using human
preference data. While numerous publicly available preference datasets provide
pairwise comparisons of responses, the potential for biases in the resulting
reward models remains underexplored. In this work, we introduce novel methods
to detect and evaluate prefix bias -- a systematic shift in model preferences
triggered by minor variations in query prefixes -- in LLM-based reward models
trained on such datasets. We leverage these metrics to reveal significant
biases in preference models across racial and gender dimensions. Our
comprehensive evaluation spans diverse open-source preference datasets and
reward model architectures, demonstrating susceptibility to this kind of bias
regardless of the underlying model architecture. Furthermore, we propose a data
augmentation strategy to mitigate these biases, showing its effectiveness in
reducing the impact of prefix bias. Our findings highlight the critical need
for bias-aware dataset design and evaluation in developing fair and reliable
reward models, contributing to the broader discourse on fairness in AI.

</details>


### [4] [Source framing triggers systematic evaluation bias in Large Language Models](https://arxiv.org/abs/2505.13488)
*Federico Germani,Giovanni Spitale*

Main category: cs.CL

TL;DR: LLM评估文本时存在框架效应偏见，中国来源信息显著降低模型一致性


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型(LLM)作为文本评估工具时，其判断是否受来源归属等框架效应影响的问题

Method: 使用4个先进LLM对24个社会敏感主题的4800条声明进行19.2万次评估，通过改变声明来源(LLM/不同国籍人类)观察评估差异

Result: 盲测时模型间一致性高，但来源标注后显著下降(尤其中国来源)，Deepseek模型受影响最大

Conclusion: 框架效应会损害LLM评估的中立性，对信息系统的公平性构成重大挑战

Abstract: Large Language Models (LLMs) are increasingly used not only to generate text
but also to evaluate it, raising urgent questions about whether their judgments
are consistent, unbiased, and robust to framing effects. In this study, we
systematically examine inter- and intra-model agreement across four
state-of-the-art LLMs (OpenAI o3-mini, Deepseek Reasoner, xAI Grok 2, and
Mistral) tasked with evaluating 4,800 narrative statements on 24 different
topics of social, political, and public health relevance, for a total of
192,000 assessments. We manipulate the disclosed source of each statement to
assess how attribution to either another LLM or a human author of specified
nationality affects evaluation outcomes. We find that, in the blind condition,
different LLMs display a remarkably high degree of inter- and intra-model
agreement across topics. However, this alignment breaks down when source
framing is introduced. Here we show that attributing statements to Chinese
individuals systematically lowers agreement scores across all models, and in
particular for Deepseek Reasoner. Our findings reveal that framing effects can
deeply affect text evaluation, with significant implications for the integrity,
neutrality, and fairness of LLM-mediated information systems.

</details>


### [5] [ProdRev: A DNN framework for empowering customers using generative pre-trained transformers](https://arxiv.org/abs/2505.13491)
*Aakash Gupta,Nataraj Das*

Main category: cs.CL

TL;DR: 微调GPT-3生成电商评论的抽象摘要框架，通过常识推理帮助用户快速决策


<details>
  <summary>Details</summary>
Motivation: 疫情后用户依赖电商但面临海量评论导致的决策困难，现有评分工具存在机械复制缺陷

Method: 使用GPT-3的curie引擎进行微调，采用抽象摘要技术替代传统提取式方法，融入常识推理

Result: 构建130亿参数模型，生成包含优缺点分析的评论摘要，实现用户决策效率提升

Conclusion: 该框架有效处理海量评论信息，通过生成式摘要增强用户决策自主性，提升电商体验

Abstract: Following the pandemic, customers, preference for using e-commerce has
accelerated. Since much information is available in multiple reviews (sometimes
running in thousands) for a single product, it can create decision paralysis
for the buyer. This scenario disempowers the consumer, who cannot be expected
to go over so many reviews since its time consuming and can confuse them.
Various commercial tools are available, that use a scoring mechanism to arrive
at an adjusted score. It can alert the user to potential review manipulations.
This paper proposes a framework that fine-tunes a generative pre-trained
transformer to understand these reviews better. Furthermore, using
"common-sense" to make better decisions. These models have more than 13 billion
parameters. To fine-tune the model for our requirement, we use the curie engine
from generative pre-trained transformer (GPT3). By using generative models, we
are introducing abstractive summarization. Instead of using a simple extractive
method of summarizing the reviews. This brings out the true relationship
between the reviews and not simply copy-paste. This introduces an element of
"common sense" for the user and helps them to quickly make the right decisions.
The user is provided the pros and cons of the processed reviews. Thus the
user/customer can take their own decisions.

</details>


### [6] [LLM4CD: Leveraging Large Language Models for Open-World Knowledge Augmented Cognitive Diagnosis](https://arxiv.org/abs/2505.13492)
*Weiming Zhang,Lingyue Fu,Qingyao Li,Kounianhua Du,Jianghao Lin,Jingwei Yu,Wei Xia,Weinan Zhang,Ruiming Tang,Yong Yu*

Main category: cs.CL

TL;DR: 提出LLM4CD框架，利用大语言模型的开放世界知识构建语义化表征，通过双层编码器解决传统认知诊断方法忽视语义信息与冷启动问题。


<details>
  <summary>Details</summary>
Motivation: 现有认知诊断方法过度依赖ID建模，忽视教育数据中的丰富语义关系，且难以处理智能教学系统中新增学生/习题的冷启动场景。

Method: 1) 用LLM生成认知表达文本表征 2) 设计包含宏观认知文本编码器（处理测试历史语义）和微观知识状态编码器（建模知识点掌握）的双层框架

Result: 在多个真实数据集上持续超越传统CD模型，验证引入LLM语义信息对认知诊断任务的有效性提升

Conclusion: 通过LLM的开放世界知识注入语义信息，显著提升认知诊断效果，同时有效解决新学生/习题的冷启动问题。

Abstract: Cognitive diagnosis (CD) plays a crucial role in intelligent education,
evaluating students' comprehension of knowledge concepts based on their test
histories. However, current CD methods often model students, exercises, and
knowledge concepts solely on their ID relationships, neglecting the abundant
semantic relationships present within educational data space. Furthermore,
contemporary intelligent tutoring systems (ITS) frequently involve the addition
of new students and exercises, a situation that ID-based methods find
challenging to manage effectively. The advent of large language models (LLMs)
offers the potential for overcoming this challenge with open-world knowledge.
In this paper, we propose LLM4CD, which Leverages Large Language Models for
Open-World Knowledge Augmented Cognitive Diagnosis. Our method utilizes the
open-world knowledge of LLMs to construct cognitively expressive textual
representations, which are then encoded to introduce rich semantic information
into the CD task. Additionally, we propose an innovative bi-level encoder
framework that models students' test histories through two levels of encoders:
a macro-level cognitive text encoder and a micro-level knowledge state encoder.
This approach substitutes traditional ID embeddings with semantic
representations, enabling the model to accommodate new students and exercises
with open-world knowledge and address the cold-start problem. Extensive
experimental results demonstrate that our proposed method consistently
outperforms previous CD models on multiple real-world datasets, validating the
effectiveness of leveraging LLMs to introduce rich semantic information into
the CD task.

</details>


### [7] [IRLBench: A Multi-modal, Culturally Grounded, Parallel Irish-English Benchmark for Open-Ended LLM Reasoning Evaluation](https://arxiv.org/abs/2505.13498)
*Khanh-Tung Tran,Barry O'Sullivan,Hoang D. Nguyen*

Main category: cs.CL

TL;DR: 提出IRLBench多语言基准测试，评估LLMs在英语和濒危爱尔兰语的表现，揭示显著性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准存在文化偏见、格式局限（文本/多选题）且对极低资源语言支持不足，需开发更全面的评估体系。

Method: 基于爱尔兰毕业考试构建12科目双语基准，采用长文本生成任务及官方评分标准评估模型正确性与语言保真度。

Result: 主流模型在爱尔兰语表现显著落后：有效回答率<80%，最佳模型正确率55.8%（英语76.2%）。

Conclusion: 发布IRLBench数据集与评估框架，推动具备文化敏感性的鲁棒多语言AI发展。

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated promising
knowledge and reasoning abilities, yet their performance in multilingual and
low-resource settings remains underexplored. Existing benchmarks often exhibit
cultural bias, restrict evaluation to text-only, rely on multiple-choice
formats, and, more importantly, are limited for extremely low-resource
languages. To address these gaps, we introduce IRLBench, presented in parallel
English and Irish, which is considered definitely endangered by UNESCO. Our
benchmark consists of 12 representative subjects developed from the 2024 Irish
Leaving Certificate exams, enabling fine-grained analysis of model capabilities
across domains. By framing the task as long-form generation and leveraging the
official marking scheme, it does not only support a comprehensive evaluation of
correctness but also language fidelity. Our extensive experiments of leading
closed-source and open-source LLMs reveal a persistent performance gap between
English and Irish, in which models produce valid Irish responses less than 80\%
of the time, and answer correctly 55.8\% of the time compared to 76.2\% in
English for the best-performing model. We release IRLBench
(https://huggingface.co/datasets/ReliableAI/IRLBench) and an accompanying
evaluation codebase (https://github.com/ReML-AI/IRLBench) to enable future
research on robust, culturally aware multilingual AI development.

</details>


### [8] [Noise Injection Systemically Degrades Large Language Model Safety Guardrails](https://arxiv.org/abs/2505.13500)
*Prithviraj Singh Shahani,Matthias Scheutz*

Main category: cs.CL

TL;DR: 高斯噪声注入显著增加LLM有害输出率（最高27%），现有安全微调技术无法抵御此类扰动，需结合推理与强化学习提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探究安全护栏在扰动下的鲁棒性，揭示当前安全对齐技术的实际漏洞风险。

Method: 在多个开源LLM的激活层系统注入高斯噪声，量化评估安全性能变化。

Result: 1. 有害输出率显著上升（p<0.001） 2. 深度微调无额外防护 3. 链式推理机制保持稳定

Conclusion: 现行安全调优方法存在根本性脆弱，建议开发基于推理和强化学习的复合型安全系统以应对现实场景挑战。

Abstract: Safety guardrails in large language models (LLMs) are a critical component in
preventing harmful outputs. Yet, their resilience under perturbation remains
poorly understood. In this paper, we investigate the robustness of safety
fine-tuning in LLMs by systematically injecting Gaussian noise into model
activations. We show across multiple open-weight models that (1) Gaussian noise
raises harmful-output rates (p < 0.001) by up to 27%, (2) that deeper safety
fine-tuning affords no extra protection, and (3) that chain-of-thought
reasoning remains largely intact. The findings reveal critical vulnerabilities
in current safety alignment techniques and highlight the potential of
reasoning-based and reinforcement learning approaches as promising direction
for developing more robust AI safety systems. These results have important
implications for real-world deployment of LLMs in safety-critical applications
as these results imply that widely-deployed safety tuning methods can fail even
without adversarial prompts.

</details>


### [9] [EcoSafeRAG: Efficient Security through Context Analysis in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.13506)
*Ruobing Yao,Yifei Zhang,Shuang Song,Neng Gao,Chenyang Tu*

Main category: cs.CL

TL;DR: EcoSafeRAG提出了一种不依赖LLM内部知识的RAG安全防御方案，通过句子级处理和诱饵引导的上下文多样性检测，在实现先进安全防护的同时提升纯净场景性能，并降低运算成本。


<details>
  <summary>Details</summary>
Motivation: 现有RAG防御方案依赖大语言模型内部知识，与RAG设计理念冲突。针对语料库投毒等新型攻击面，需要开发不依赖模型自身知识的防御机制。

Method: 采用句子级文档处理结合诱饵引导的上下文多样性检测技术，通过分析候选文档的上下文多样性特征识别恶意内容。

Result: 实验显示方案实现SOTA安全防护（插件化部署），在保持合理运营成本（延迟仅1.2倍，token用量减少48%-80%）的同时提升纯净场景RAG性能。

Conclusion: 该方案证明不依赖LLM内部知识的防御可行性，成功平衡安全防护与系统效能，为RAG安全领域提供新范式。

Abstract: Retrieval-Augmented Generation (RAG) compensates for the static knowledge
limitations of Large Language Models (LLMs) by integrating external knowledge,
producing responses with enhanced factual correctness and query-specific
contextualization. However, it also introduces new attack surfaces such as
corpus poisoning at the same time. Most of the existing defense methods rely on
the internal knowledge of the model, which conflicts with the design concept of
RAG. To bridge the gap, EcoSafeRAG uses sentence-level processing and
bait-guided context diversity detection to identify malicious content by
analyzing the context diversity of candidate documents without relying on LLM
internal knowledge. Experiments show EcoSafeRAG delivers state-of-the-art
security with plug-and-play deployment, simultaneously improving clean-scenario
RAG performance while maintaining practical operational costs (relatively
1.2$\times$ latency, 48\%-80\% token reduction versus Vanilla RAG).

</details>


### [10] [Time-R1: Towards Comprehensive Temporal Reasoning in LLMs](https://arxiv.org/abs/2505.13508)
*Zijia Liu,Peixuan Han,Haofei Yu,Haoru Li,Jiaxuan You*

Main category: cs.CL

TL;DR: 通过三阶段强化学习框架Time-R1，使中等规模LLM在时间智能领域超越超大规模模型，实现历史理解、未来预测和创造性生成能力


<details>
  <summary>Details</summary>
Motivation: 现有方法在时间推理上存在三大局限：1. 时间技能孤立（问答与预测割裂）2. 泛化能力差（无法处理知识截止日期后事件）3. 缺乏创造性前瞻能力

Method: 三阶段发展路径：前两阶段基于动态规则奖励的强化学习课程（RL curriculum），逐步构建时间逻辑映射和未来预测能力；第三阶段实现零样本创造性场景生成

Result: Time-R1在挑战性未来事件预测和创意场景生成基准测试中，性能超过DeepSeek-R1（671B）等200倍以上参数量模型，并开源Time-Bench数据集和模型检查点

Conclusion: 证明通过精心设计的渐进式强化学习微调，可使高效小模型获得卓越时间智能，为时间感知AI提供可行路径，推动时间推理研究社区发展

Abstract: Large Language Models (LLMs) demonstrate impressive capabilities but lack
robust temporal intelligence, struggling to integrate reasoning about the past
with predictions and plausible generations of the future. Meanwhile, existing
methods typically target isolated temporal skills, such as question answering
about past events or basic forecasting, and exhibit poor generalization,
particularly when dealing with events beyond their knowledge cutoff or
requiring creative foresight. To address these limitations, we introduce
\textit{Time-R1}, the first framework to endow a moderate-sized (3B-parameter)
LLM with comprehensive temporal abilities: understanding, prediction, and
creative generation. Our approach features a novel three-stage development
path; the first two constitute a \textit{reinforcement learning (RL)
curriculum} driven by a meticulously designed dynamic rule-based reward system.
This framework progressively builds (1) foundational temporal understanding and
logical event-time mappings from historical data, (2) future event prediction
skills for events beyond its knowledge cutoff, and finally (3) enables
remarkable generalization to creative future scenario generation without any
fine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms
models over 200 times larger, including the state-of-the-art 671B DeepSeek-R1,
on highly challenging future event prediction and creative scenario generation
benchmarks. This work provides strong evidence that thoughtfully engineered,
progressive RL fine-tuning allows smaller, efficient models to achieve superior
temporal performance, offering a practical and scalable path towards truly
time-aware AI. To foster further research, we also release \textit{Time-Bench},
a large-scale multi-task temporal reasoning dataset derived from 10 years of
news data, and our series of \textit{Time-R1} checkpoints.

</details>


### [11] [Induction Head Toxicity Mechanistically Explains Repetition Curse in Large Language Models](https://arxiv.org/abs/2505.13514)
*Shuxun Wang,Qingyu Yin,Chak Tou Leong,Qiang Zhang,Linyi Yang*

Main category: cs.CL

TL;DR: 研究发现大语言模型的重复诅咒现象主要由'归纳头'驱动，并提出通过注意力头正则化技术缓解该问题


<details>
  <summary>Details</summary>
Motivation: 探索重复诅咒现象的机制，聚焦于归纳头在生成重复序列中的主导作用及其对模型输出的影响

Method: 分析归纳头的'毒性'表现(主导输出logits)，提出注意力头正则化技术降低其生成控制权

Result: 验证归纳头是重复诅咒的核心机制，正则化可有效提升输出多样性和连贯性

Conclusion: 为LLM设计提供新视角，通过调控注意力头权重分布改善生成质量

Abstract: Repetition curse is a phenomenon where Large Language Models (LLMs) generate
repetitive sequences of tokens or cyclic sequences. While the repetition curse
has been widely observed, its underlying mechanisms remain poorly understood.
In this work, we investigate the role of induction heads--a specific type of
attention head known for their ability to perform in-context learning--in
driving this repetitive behavior. Specifically, we focus on the "toxicity" of
induction heads, which we define as their tendency to dominate the model's
output logits during repetition, effectively excluding other attention heads
from contributing to the generation process. Our findings have important
implications for the design and training of LLMs. By identifying induction
heads as a key driver of the repetition curse, we provide a mechanistic
explanation for this phenomenon and suggest potential avenues for mitigation.
We also propose a technique with attention head regularization that could be
employed to reduce the dominance of induction heads during generation, thereby
promoting more diverse and coherent outputs.

</details>


### [12] [Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression](https://arxiv.org/abs/2505.13527)
*Jingyu Peng,Maolin Wang,Nan Wang,Xiangyu Zhao,Jiatong Li,Kai Zhang,Qi Liu*

Main category: cs.CL

TL;DR: 提出LogiBreak黑盒越狱方法，通过逻辑表达式转换绕过LLM安全机制，揭示对齐数据与逻辑输入间的分布差异漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全机制易受越狱攻击，根源在于对齐提示与恶意提示的分布差异未被充分考虑。

Method: 将有害自然语言转为形式逻辑表达式，利用对齐数据与逻辑输入的分布差规避安全检测，保持语义完整性和跨语言适用性。

Result: 在跨3种语言的数据集中验证有效，不同评估场景下均成功突破主流LLM的安全防护。

Conclusion: 首次通过逻辑形式化暴露LLM安全架构的分布脆弱性，为防御逻辑型攻击提供新研究方向。

Abstract: Despite substantial advancements in aligning large language models (LLMs)
with human values, current safety mechanisms remain susceptible to jailbreak
attacks. We hypothesize that this vulnerability stems from distributional
discrepancies between alignment-oriented prompts and malicious prompts. To
investigate this, we introduce LogiBreak, a novel and universal black-box
jailbreak method that leverages logical expression translation to circumvent
LLM safety systems. By converting harmful natural language prompts into formal
logical expressions, LogiBreak exploits the distributional gap between
alignment data and logic-based inputs, preserving the underlying semantic
intent and readability while evading safety constraints. We evaluate LogiBreak
on a multilingual jailbreak dataset spanning three languages, demonstrating its
effectiveness across various evaluation settings and linguistic contexts.

</details>


### [13] [Combining the Best of Both Worlds: A Method for Hybrid NMT and LLM Translation](https://arxiv.org/abs/2505.13554)
*Zhanglin Wu,Daimeng Wei,Xiaoyu Chen,Hengchao Shang,Jiaxin Guo,Zongyao Li,Yuanchang Luo,Jinlong Yang,Zhiqiang Rao,Hao Yang*

Main category: cs.CL

TL;DR: 提出整合NMT和LLM的翻译方案，仅在必要时使用LLM以减少计算成本，并通过调度策略优化翻译效果


<details>
  <summary>Details</summary>
Motivation: LLM在翻译任务中虽有潜力，但存在高成本和延迟问题。多数情况下与NMT效果相当，仅在特定场景有优势，因此需合理调度两者以平衡效果与效率

Method: 比较不同调度策略，提出基于源语句特征的决策器，动态选择使用NMT或LLM进行翻译

Result: 实验表明，该决策器能在多语言测试集上以最少LLM使用实现最优翻译性能

Conclusion: 结合NMT和LLM并智能调度，可在保证翻译质量的同时显著降低计算成本

Abstract: Large language model (LLM) shows promising performances in a variety of
downstream tasks, such as machine translation (MT). However, using LLMs for
translation suffers from high computational costs and significant latency.
Based on our evaluation, in most cases, translations using LLMs are comparable
to that generated by neural machine translation (NMT) systems. Only in
particular scenarios, LLM and NMT models show respective advantages. As a
result, integrating NMT and LLM for translation and using LLM only when
necessary seems to be a sound solution. A scheduling policy that optimizes
translation result while ensuring fast speed and as little LLM usage as
possible is thereby required. We compare several scheduling policies and
propose a novel and straightforward decider that leverages source sentence
features. We conduct extensive experiments on multilingual test sets and the
result shows that we can achieve optimal translation performance with minimal
LLM usage, demonstrating effectiveness of our decider.

</details>


### [14] [CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the Limits of Large Language Models](https://arxiv.org/abs/2505.13559)
*Sathya Krishnan Suresh,Tanmay Surana,Lim Zhi Hao,Eng Siong Chng*

Main category: cs.CL

TL;DR: 开发CS-Sum基准测试发现大模型处理语码转换时存在潜在语义偏差


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型对语码转换对话的理解能力

Method: 构建跨汉语/泰米尔语/马来语-英语的CS对话总结基准，测试十种大模型在少样本/翻译总结/微调等方案下的表现

Result: 自动指标得分虚高，模型存在三类典型错误（未明确说明具体错误类型），不同语言对的错误率差异显著

Conclusion: 需要针对语码转换数据进行专项训练以提升模型鲁棒性

Abstract: Code-switching (CS) poses a significant challenge for Large Language Models
(LLMs), yet its comprehensibility remains underexplored in LLMs. We introduce
CS-Sum, to evaluate the comprehensibility of CS by the LLMs through CS dialogue
to English summarization. CS-Sum is the first benchmark for CS dialogue
summarization across Mandarin-English (EN-ZH), Tamil-English (EN-TA), and
Malay-English (EN-MS), with 900-1300 human-annotated dialogues per language
pair. Evaluating ten LLMs, including open and closed-source models, we analyze
performance across few-shot, translate-summarize, and fine-tuning (LoRA, QLoRA
on synthetic data) approaches. Our findings show that though the scores on
automated metrics are high, LLMs make subtle mistakes that alter the complete
meaning of the dialogue. To this end, we introduce 3 most common type of errors
that LLMs make when handling CS input. Error rates vary across CS pairs and
LLMs, with some LLMs showing more frequent errors on certain language pairs,
underscoring the need for specialized training on code-switched data.

</details>


### [15] [Cross-Lingual Representation Alignment Through Contrastive Image-Caption Tuning](https://arxiv.org/abs/2505.13628)
*Nathaniel Krasner,Nicholas Lanuzo,Antonios Anastasopoulos*

Main category: cs.CL

TL;DR: 提出利用视觉信息替代双语文本实现多语言表示对齐，为低资源语言提供高效解决方案


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖难以获取的双语对照文本，尤其对低资源语言不友好，而图像描述数据更易获取且无需语言专家

Method: 通过多语言图像-描述对齐训练模型，将不同语言文本与相同视觉内容关联，支持未见语言的后期整合

Result: 成功实现跨语言表示对齐，在自然语言理解和双语检索任务中有效，包括预训练未覆盖的语言

Conclusion: 视觉信息可替代双语文本建立语言桥梁，显著降低低资源语言的多语言模型开发门槛

Abstract: Multilingual alignment of sentence representations has mostly required
bitexts to bridge the gap between languages. We investigate whether visual
information can bridge this gap instead. Image caption datasets are very easy
to create without requiring multilingual expertise, so this offers a more
efficient alternative for low-resource languages. We find that multilingual
image-caption alignment can implicitly align the text representations between
languages, languages unseen by the encoder in pretraining can be incorporated
into this alignment post-hoc, and these aligned representations are usable for
cross-lingual Natural Language Understanding (NLU) and bitext retrieval.

</details>


### [16] [Clarifying orthography: Orthographic transparency as compressibility](https://arxiv.org/abs/2505.13657)
*Charles J. Torres,Richard Futrell*

Main category: cs.CL

TL;DR: 提出基于算法信息论的互压缩性度量方法，统一量化不同文字系统的拼写透明度，验证22种文字类型并证实该方法具备跨文字适用性


<details>
  <summary>Details</summary>
Motivation: 现有拼写透明度度量缺乏统一标准，无法跨文字类型横向比较。需构建同时考虑拼写不规则性和规则复杂性的通用量化指标

Method: 通过神经序列模型计算正字法与语音字符串的预训练码长，以互压缩性整合拼写不规则度与规则复杂度

Result: 在涵盖5类文字系统（字母/辅音/音节/表意等）的22种语言中验证了不同文字透明度的相对排序假设

Conclusion: 互压缩性作为跨文字透明度的通用指标，为语言类型学研究和文字处理系统设计提供了量化基准

Abstract: Orthographic transparency -- how directly spelling is related to sound --
lacks a unified, script-agnostic metric. Using ideas from algorithmic
information theory, we quantify orthographic transparency in terms of the
mutual compressibility between orthographic and phonological strings. Our
measure provides a principled way to combine two factors that decrease
orthographic transparency, capturing both irregular spellings and rule
complexity in one quantity. We estimate our transparency measure using
prequential code-lengths derived from neural sequence models. Evaluating 22
languages across a broad range of script types (alphabetic, abjad, abugida,
syllabic, logographic) confirms common intuitions about relative transparency
of scripts. Mutual compressibility offers a simple, principled, and general
yardstick for orthographic transparency.

</details>


### [17] [Are Large Language Models Good at Detecting Propaganda?](https://arxiv.org/abs/2505.13706)
*Julia Jose,Rachel Greenstadt*

Main category: cs.CL

TL;DR: GPT-4在宣传技术检测中F1=0.16优于其他LLM但未超越RoBERTa-CRF基线（F1=0.67），LLMs在特定宣传技术检测中表现优于传统模型


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在检测新闻文章宣传技术（逻辑谬误/情感诉求）方面的有效性，与传统NLP模型进行性能对比

Method: 多模型对比实验：比较GPT-4/GPT-3.5/Claude 3 Opus与RoBERTa-CRF、MGN基线模型在宣传技术检测任务中的F1分数表现

Result: 1. GPT-4在所有LLM中最佳但低于RoBERTa基线
2. 所有LLM在'辱骂'检测中优于MGN
3. GPT系列在'引起恐惧'和'挥舞旗帜'检测中优于MGN

Conclusion: 传统模型在整体检测任务中仍具优势，但LLMs在特定宣传技术检测场景展现应用潜力，需进一步研究模型优化方向

Abstract: Propagandists use rhetorical devices that rely on logical fallacies and
emotional appeals to advance their agendas. Recognizing these techniques is key
to making informed decisions. Recent advances in Natural Language Processing
(NLP) have enabled the development of systems capable of detecting manipulative
content. In this study, we look at several Large Language Models and their
performance in detecting propaganda techniques in news articles. We compare the
performance of these LLMs with transformer-based models. We find that, while
GPT-4 demonstrates superior F1 scores (F1=0.16) compared to GPT-3.5 and Claude
3 Opus, it does not outperform a RoBERTa-CRF baseline (F1=0.67). Additionally,
we find that all three LLMs outperform a MultiGranularity Network (MGN)
baseline in detecting instances of one out of six propaganda techniques
(name-calling), with GPT-3.5 and GPT-4 also outperforming the MGN baseline in
detecting instances of appeal to fear and flag-waving.

</details>


### [18] [SQLForge: Synthesizing Reliable and Diverse Data to Enhance Text-to-SQL Reasoning in LLMs](https://arxiv.org/abs/2505.13725)
*Yu Guo,Dong Jin,Shenghao Ye,Shuangwu Chen,Jian Yang,Xiaobin Tan*

Main category: cs.CL

TL;DR: 提出SQLForge框架，通过可靠数据合成提升开源大模型的text-to-SQL能力


<details>
  <summary>Details</summary>
Motivation: 开源模型与闭源模型在text-to-SQL任务中存在显著性能差距，需通过数据增强提升模型表现

Method: 1. SQL语法约束和逆向翻译保证数据可靠性
2. 模板增强和迭代域探索机制提升数据多样性
3. 基于增强数据微调多架构开源模型

Result: 在Spider和BIRD基准测试中分别达到85.7%和59.8%的EX准确率，创开源模型最优纪录

Conclusion: 通过系统性数据增强，SQLForge-LM显著缩小了与闭源模型的性能差距，证明数据质量对模型性能的关键作用

Abstract: Large Language models (LLMs) have demonstrated significant potential in
text-to-SQL reasoning tasks, yet a substantial performance gap persists between
existing open-source models and their closed-source counterparts. In this
paper, we introduce SQLForge, a novel approach for synthesizing reliable and
diverse data to enhance text-to-SQL reasoning in LLMs. We improve data
reliability through SQL syntax constraints and SQL-to-question reverse
translation, ensuring data logic at both structural and semantic levels. We
also propose an SQL template enrichment and iterative data domain exploration
mechanism to boost data diversity. Building on the augmented data, we fine-tune
a variety of open-source models with different architectures and parameter
sizes, resulting in a family of models termed SQLForge-LM. SQLForge-LM achieves
the state-of-the-art performance on the widely recognized Spider and BIRD
benchmarks among the open-source models. Specifically, SQLForge-LM achieves EX
accuracy of 85.7% on Spider Dev and 59.8% on BIRD Dev, significantly narrowing
the performance gap with closed-source methods.

</details>


### [19] [Simulation Agent: A Framework for Integrating Simulation and Large Language Models for Enhanced Decision-Making](https://arxiv.org/abs/2505.13761)
*Jacob Kleiman,Kevin Frank,Sindy Campagna*

Main category: cs.CL

TL;DR: 提出融合仿真模型与大语言模型的模拟代理框架，兼顾交互友好性与系统准确性


<details>
  <summary>Details</summary>
Motivation: 解决传统仿真系统对非技术用户的高门槛问题，以及大语言模型缺乏结构化因果推理能力的局限

Method: 通过LLM的对话能力连接用户与仿真系统，同时用仿真结果反哺LLM的现实建模能力

Result: 构建出兼具自然交互与精准建模的实验验证平台，支持跨领域应用扩展

Conclusion: 该框架通过双向增强机制，既降低了复杂系统使用门槛，又保障了现实动态建模的准确性

Abstract: Simulations, although powerful in accurately replicating real-world systems,
often remain inaccessible to non-technical users due to their complexity.
Conversely, large language models (LLMs) provide intuitive, language-based
interactions but can lack the structured, causal understanding required to
reliably model complex real-world dynamics. We introduce our simulation agent
framework, a novel approach that integrates the strengths of both simulation
models and LLMs. This framework helps empower users by leveraging the
conversational capabilities of LLMs to interact seamlessly with sophisticated
simulation systems, while simultaneously utilizing the simulations to ground
the LLMs in accurate and structured representations of real-world phenomena.
This integrated approach helps provide a robust and generalizable foundation
for empirical validation and offers broad applicability across diverse domains.

</details>


### [20] [Krikri: Advancing Open Large Language Models for Greek](https://arxiv.org/abs/2505.13772)
*Dimitris Roussis,Leon Voukoutis,Georgios Paraskevopoulos,Sokratis Sofianopoulos,Prokopis Prokopidis,Vassilis Papavasileiou,Athanasios Katsamanis,Stelios Piperidis,Vassilis Katsouros*

Main category: cs.CL

TL;DR: 基于Llama 3.1-8B的希腊语专用大模型Llama-Krikri-8B，通过高质量数据训练和MAGPIE技术优化，在多语言任务及代码生成上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有希腊语LLMs在语言理解、生成及代码处理方面的不足，填补古希腊语和多调文本支持的技术空白。

Method: 1. 基于Llama 3.1-8B架构进行希腊语适配训练
2. 采用MAGPIE技术构建多阶段后训练流程
3. 提出三个新型希腊语评估基准

Result: 在自然语言理解/生成和代码生成任务中显著优于同类希腊语及多语言模型

Conclusion: Llama-Krikri-8B通过针对性优化实现了希腊语LLM的突破，其方法论为小语种模型开发提供新范式。

Abstract: We introduce Llama-Krikri-8B, a cutting-edge Large Language Model tailored
for the Greek language, built on Meta's Llama 3.1-8B. Llama-Krikri-8B has been
extensively trained on high-quality Greek data to ensure superior adaptation to
linguistic nuances. With 8 billion parameters, it offers advanced capabilities
while maintaining efficient computational performance. Llama-Krikri-8B supports
both Modern Greek and English, and is also equipped to handle polytonic text
and Ancient Greek. The chat version of Llama-Krikri-8B features a multi-stage
post-training pipeline, utilizing both human and synthetic instruction and
preference data, by applying techniques such as MAGPIE. In addition, for
evaluation, we propose three novel public benchmarks for Greek. Our evaluation
on existing as well as the proposed benchmarks shows notable improvements over
comparable Greek and multilingual LLMs in both natural language understanding
and generation as well as code generation.

</details>


### [21] [Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation](https://arxiv.org/abs/2505.13792)
*Siddhant Bhambri,Upasana Biswas,Subbarao Kambhampati*

Main category: cs.CL

TL;DR: 研究质疑利用推理痕迹提升小语言模型问答性能的假设，发现中间步骤正确性与最终答案相关性低


<details>
  <summary>Details</summary>
Motivation: 当前知识蒸馏方法依赖的推理痕迹冗长且难以评估，需验证其与模型最终性能的实际关联性

Method: 采用基于规则的问题分解知识蒸馏法，将复杂QA任务拆分为可验证的分类和信息检索子步骤

Result: 实验显示正确推理痕迹与最终答案正确率无必然联系，中间步骤与结果相关性仅0.2-0.3

Conclusion: 单纯优化推理痕迹可能无法有效提升小模型性能，需重新审视知识蒸馏中中间信号的使用价值

Abstract: Question Answering (QA) poses a challenging and critical problem,
particularly in today's age of interactive dialogue systems such as ChatGPT,
Perplexity, Microsoft Copilot, etc. where users demand both accuracy and
transparency in the model's outputs. Since smaller language models (SLMs) are
computationally more efficient but often under-perform compared to larger
models, Knowledge Distillation (KD) methods allow for finetuning these smaller
models to improve their final performance. Lately, the intermediate tokens or
the so called `reasoning' traces produced by Chain-of-Thought (CoT) or by
reasoning models such as DeepSeek R1 are used as a training signal for KD.
However, these reasoning traces are often verbose and difficult to interpret or
evaluate. In this work, we aim to address the challenge of evaluating the
faithfulness of these reasoning traces and their correlation with the final
performance. To this end, we employ a KD method leveraging rule-based problem
decomposition. This approach allows us to break down complex queries into
structured sub-problems, generating interpretable traces whose correctness can
be readily evaluated, even at inference time. Specifically, we demonstrate this
approach on Open Book QA, decomposing the problem into a Classification step
and an Information Retrieval step, thereby simplifying trace evaluation. Our
SFT experiments with correct and incorrect traces on the CoTemp QA, Microsoft
Machine Reading Comprehension QA, and Facebook bAbI QA datasets reveal the
striking finding that correct traces do not necessarily imply that the model
outputs the correct final solution. Similarly, we find a low correlation
between correct final solutions and intermediate trace correctness. These
results challenge the implicit assumption behind utilizing reasoning traces for
improving SLMs' final performance via KD.

</details>


### [22] [EfficientLLM: Efficiency in Large Language Models](https://arxiv.org/abs/2505.13840)
*Zhengqing Yuan,Weixiang Sun,Yixin Liu,Huichi Zhou,Rong Zhou,Yiyang Li,Zheyuan Zhang,Wei Song,Yue Huang,Haolong Jia,Keerthiram Murugesan,Yu Wang,Lifang He,Jianfeng Gao,Lichao Sun,Yanfang Ye*

Main category: cs.CL

TL;DR: EfficientLLM是首个系统性评估大模型效率技术的基准，揭示了预训练架构/微调/推理优化的量化权衡关系，并验证了技术跨模态的泛化性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM参数量和上下文窗口的扩大，计算/能源/经济成本急剧上升，需系统评估效率优化技术的实际收益与代价。

Method: 在48xGH200集群上测试100+模型-技术组合：1) 预训练架构（MQA/GQA等注意力变体、稀疏MoE） 2) 微调方法（LoRA/RSLoRA/DoRA） 3) 推理量化（int4/float16），使用内存利用率/延迟/能耗等6项指标。

Result: 关键发现：① 效率存在显性权衡（如MoE减少60% FLOPs但显存增加40%） ② 最优方案依赖任务和规模（如RSLoRA在>14B参数时更优） ③ 技术可跨模态迁移到视觉/多模态模型。

Conclusion: 通过开源评估框架，EfficientLLM为下一代基础模型提供了效率-性能平衡的工程指南，证明量化/稀疏化/参数高效微调的综合应用价值。

Abstract: Large Language Models (LLMs) have driven significant progress, yet their
growing parameter counts and context windows incur prohibitive compute, energy,
and monetary costs. We introduce EfficientLLM, a novel benchmark and the first
comprehensive empirical study evaluating efficiency techniques for LLMs at
scale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our
study systematically explores three key axes: (1) architecture pretraining
(efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts
(MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and
(3) inference (quantization methods: int4, float16). We define six fine-grained
metrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy
Consumption, Compression Rate) to capture hardware saturation,
latency-throughput balance, and carbon cost. Evaluating over 100
model-technique pairs (0.5B-72B parameters), we derive three core insights: (i)
Efficiency involves quantifiable trade-offs: no single method is universally
optimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by
40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5%
accuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal
memory-latency trade-offs for constrained devices, MLA achieves lowest
perplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency
only beyond 14B parameters. (iii) Techniques generalize across modalities: we
extend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and
Vision-Language Models (Qwen2.5-VL), confirming effective transferability. By
open-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM
provides essential guidance for researchers and engineers navigating the
efficiency-performance landscape of next-generation foundation models.

</details>


### [23] [Improve Language Model and Brain Alignment via Associative Memory](https://arxiv.org/abs/2505.13844)
*Congchi Yin,Yongpeng Zhang,Xuyun Wen,Piji Li*

Main category: cs.CL

TL;DR: 通过整合关联记忆机制，提升语言模型与大脑处理语音信息的神经对齐效果，并验证特定监督微调的有效性。


<details>
  <summary>Details</summary>
Motivation: 关联记忆在人类认知中负责整合相关信息以促进理解，研究试图通过引入关联记忆机制优化语言模型与大脑活动的神经对齐。

Method: 1. 通过映射语言模型激活到脑活动验证基础对齐 2. 用模拟关联记忆扩展文本作为模型输入 3. 构建含1000个关联记忆指令故事的Association数据集进行监督微调

Result: 1. 在关联记忆相关脑区显著提升对齐度 2. 监督微调后的大模型与大脑响应更同步

Conclusion: 关联记忆机制能有效增强语言模型与脑活动的神经对齐，特定指令微调策略对此具有重要促进作用，为类脑语言模型开发提供新思路。

Abstract: Associative memory engages in the integration of relevant information for
comprehension in the human cognition system. In this work, we seek to improve
alignment between language models and human brain while processing speech
information by integrating associative memory. After verifying the alignment
between language model and brain by mapping language model activations to brain
activity, the original text stimuli expanded with simulated associative memory
are regarded as input to computational language models. We find the alignment
between language model and brain is improved in brain regions closely related
to associative memory processing. We also demonstrate large language models
after specific supervised fine-tuning better align with brain response, by
building the \textit{Association} dataset containing 1000 samples of stories,
with instructions encouraging associative memory as input and associated
content as output.

</details>


### [24] [Domain Gating Ensemble Networks for AI-Generated Text Detection](https://arxiv.org/abs/2505.13855)
*Arihant Tripathi,Liam Dugan,Charis Gao,Maggie Huan,Emma Jin,Peter Zhang,David Zhang,Julia Zhao,Chris Callison-Burch*

Main category: cs.CL

TL;DR: 提出DoGEN方法，通过领域门控集成网络提升跨领域机器文本检测能力，在多个基准测试中实现最优表现。


<details>
  <summary>Details</summary>
Motivation: 当前机器文本检测器难以适应新领域和生成模型的变化，需要更灵活有效的解决方案。

Method: 集成多个领域专家检测模型，通过域分类器权重动态调整模型组合实现领域自适应。

Result: 领域内检测达SOTA水平，领域外检测性能超越两倍规模模型。

Conclusion: DoGEN有效提升检测器的领域适应性，开源资源促进AI检测领域研究发展。

Abstract: As state-of-the-art language models continue to improve, the need for robust
detection of machine-generated text becomes increasingly critical. However,
current state-of-the-art machine text detectors struggle to adapt to new unseen
domains and generative models. In this paper we present DoGEN (Domain Gating
Ensemble Networks), a technique that allows detectors to adapt to unseen
domains by ensembling a set of domain expert detector models using weights from
a domain classifier. We test DoGEN on a wide variety of domains from leading
benchmarks and find that it achieves state-of-the-art performance on in-domain
detection while outperforming models twice its size on out-of-domain detection.
We release our code and trained models to assist in future research in
domain-adaptive AI detection.

</details>


### [25] [Reasoning Path Compression: Compressing Generation Trajectories for Efficient LLM Reasoning](https://arxiv.org/abs/2505.13866)
*Jiwon Song,Dongwon Jo,Yulhwa Kim,Jae-Joon Kim*

Main category: cs.CL

TL;DR: 提出无需训练的推理路径压缩方法RPC，通过KV缓存压缩提升LLM推理效率


<details>
  <summary>Details</summary>
Motivation: 现有推理导向语言模型生成长推理路径导致内存占用高、生成速度慢，影响实际部署

Method: 利用语义稀疏性定期压缩KV缓存，基于最近查询构成的滑动窗口计算重要性分数，保留高权重部分

Result: 在QwQ-32B上实现1.60倍吞吐量提升，AIME 2024基准准确率仅下降1.2%

Conclusion: 证明推理路径的语义稀疏性可有效压缩，为高效部署推理型LLM提供实用路径

Abstract: Recent reasoning-focused language models achieve high accuracy by generating
lengthy intermediate reasoning paths before producing final answers. While this
approach is effective in solving problems that require logical thinking, long
reasoning paths significantly increase memory usage and throughput of token
generation, limiting the practical deployment of such models. We propose
Reasoning Path Compression (RPC), a training-free method that accelerates
inference by leveraging the semantic sparsity of reasoning paths. RPC
periodically compresses the KV cache by retaining KV cache that receive high
importance score, which are computed using a selector window composed of
recently generated queries. Experiments show that RPC improves generation
throughput of QwQ-32B by up to 1.60$\times$ compared to the inference with full
KV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our
findings demonstrate that semantic sparsity in reasoning traces can be
effectively exploited for compression, offering a practical path toward
efficient deployment of reasoning LLMs. Our code is available at
https://github.com/jiwonsong-dev/ReasoningPathCompression.

</details>


### [26] [Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General Reasoning](https://arxiv.org/abs/2505.13886)
*Jingqi Tong,Jixin Tang,Hangcheng Li,Yurong Mou,Ming Zhang,Jun Zhao,Yanbo Wen,Fan Song,Jiahao Zhan,Yuyang Lu,Chaoran Tao,Zhiyuan Guo,Jizhou Yu,Tianhao Cheng,Changhao Jiang,Zhen Wang,Tao Liang,Zhihui Fei,Mingyang Wan,Guojun Ma,Weifeng Ge,Guanhua Chen,Tao Gui,Xipeng Qiu,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: 提出Code2Logic方法，利用游戏代码自动生成多模态推理数据解决VLM训练数据不足问题


<details>
  <summary>Details</summary>
Motivation: 视觉语言推理数据标注成本高，而游戏代码天然包含丰富的逻辑结构和状态转换过程

Method: 使用大语言模型适配游戏代码，通过代码执行自动获取推理过程和结果

Result: 构建了低成本、高扩展性的GameQA数据集，Qwen2.5-VL-7B模型在7个基准测试中性能提升2.33%

Conclusion: 基于游戏代码的数据合成方法有效提升视觉语言模型的跨领域泛化能力

Abstract: Visual-language Chain-of-Thought (CoT) data resources are relatively scarce
compared to text-only counterparts, limiting the improvement of reasoning
capabilities in Vision Language Models (VLMs). However, high-quality
vision-language reasoning data is expensive and labor-intensive to annotate. To
address this issue, we leverage a promising resource: game code, which
naturally contains logical structures and state transition processes.
Therefore, we propose Code2Logic, a novel game-code-driven approach for
multimodal reasoning data synthesis. Our approach leverages Large Language
Models (LLMs) to adapt game code, enabling automatic acquisition of reasoning
processes and results through code execution. Using the Code2Logic approach, we
developed the GameQA dataset to train and evaluate VLMs. GameQA is
cost-effective and scalable to produce, challenging for state-of-the-art
models, and diverse with 30 games and 158 tasks. Surprisingly, despite training
solely on game data, VLMs demonstrated out of domain generalization,
specifically Qwen2.5-VL-7B improving performance by 2.33\% across 7 diverse
vision-language benchmarks. Our code and dataset are available at
https://github.com/tongjingqi/Code2Logic.

</details>


### [27] [Mapping the Minds of LLMs: A Graph-Based Analysis of Reasoning LLM](https://arxiv.org/abs/2505.13890)
*Zhen Xiong,Yujun Cai,Zhecheng Li,Yiwei Wang*

Main category: cs.CL

TL;DR: 提出图结构分析框架揭示LLMs推理结构与准确性的强关联，突破传统指标局限，为提示工程提供新视角


<details>
  <summary>Details</summary>
Motivation: 现有推理型LLMs存在反直觉的不稳定行为（如少样本提示性能下降），传统评估指标难以深入分析推理质量

Method: 通过聚类CoT生成步骤构建有向推理图，量化分析探索密度、分支率、收敛率等图结构特征

Result: 发现推理图的结构特性与任务准确性显著相关，不同提示策略会重塑LLMs的内部推理结构模式

Conclusion: 该框架为LLMs推理质量评估开辟量化新维度，对提示工程优化和模型认知机制研究具有双重指导价值

Abstract: Recent advances in test-time scaling have enabled Large Language Models
(LLMs) to display sophisticated reasoning abilities via extended
Chain-of-Thought (CoT) generation. Despite their potential, these Reasoning
LLMs (RLMs) often demonstrate counterintuitive and unstable behaviors, such as
performance degradation under few-shot prompting, that challenge our current
understanding of RLMs. In this work, we introduce a unified graph-based
analytical framework for better modeling the reasoning processes of RLMs. Our
method first clusters long, verbose CoT outputs into semantically coherent
reasoning steps, then constructs directed reasoning graphs to capture
contextual and logical dependencies among these steps. Through comprehensive
analysis across models and prompting regimes, we reveal that structural
properties, such as exploration density, branching, and convergence ratios,
strongly correlate with reasoning accuracy. Our findings demonstrate how
prompting strategies substantially reshape the internal reasoning structure of
RLMs, directly affecting task outcomes. The proposed framework not only enables
quantitative evaluation of reasoning quality beyond conventional metrics but
also provides practical insights for prompt engineering and the cognitive
analysis of LLMs. Code and resources will be released to facilitate future
research in this direction.

</details>


### [28] [InfiGFusion: Graph-on-Logits Distillation via Efficient Gromov-Wasserstein for Model Fusion](https://arxiv.org/abs/2505.13893)
*Yuanyi Wang,Zhaoyi Yan,Yiming Zhang,Qi Zhou,Yanggan Gu,Fei Wu,Hongxia Yang*

Main category: cs.CL

TL;DR: 提出首个结构感知融合框架InfiGFusion，通过图蒸馏损失捕捉词汇语义依赖，显著提升异构模型融合效果


<details>
  <summary>Details</summary>
Motivation: 现有logit融合方法独立处理词汇维度，忽视跨维度语义依赖关系，导致异构模型难以有效对齐生成行为

Method: 构建全局共激活图量化词汇联合激活，设计O(n log n)复杂度的闭式近似算法降低Gromov-Wasserstein距离计算成本

Result: 在11个推理/编码/数学基准超越SOTA，复杂推理任务提升显著（多步算术+35.6，因果判断+37.06）

Conclusion: 通过显式建模词汇交互依赖，InfiGFusion实现高效稳定的模型融合，在复杂推理任务中展现突出优势

Abstract: Recent advances in large language models (LLMs) have intensified efforts to
fuse heterogeneous open-source models into a unified system that inherits their
complementary strengths. Existing logit-based fusion methods maintain inference
efficiency but treat vocabulary dimensions independently, overlooking semantic
dependencies encoded by cross-dimension interactions. These dependencies
reflect how token types interact under a model's internal reasoning and are
essential for aligning models with diverse generation behaviors. To explicitly
model these dependencies, we propose \textbf{InfiGFusion}, the first
structure-aware fusion framework with a novel \textit{Graph-on-Logits
Distillation} (GLD) loss. Specifically, we retain the top-$k$ logits per output
and aggregate their outer products across sequence positions to form a global
co-activation graph, where nodes represent vocabulary channels and edges
quantify their joint activations. To ensure scalability and efficiency, we
design a sorting-based closed-form approximation that reduces the original
$O(n^4)$ cost of Gromov-Wasserstein distance to $O(n \log n)$, with provable
approximation guarantees. Experiments across multiple fusion settings show that
GLD consistently improves fusion quality and stability. InfiGFusion outperforms
SOTA models and fusion baselines across 11 benchmarks spanning reasoning,
coding, and mathematics. It shows particular strength in complex reasoning
tasks, with +35.6 improvement on Multistep Arithmetic and +37.06 on Causal
Judgement over SFT, demonstrating superior multi-step and relational inference.

</details>


### [29] [Let's Verify Math Questions Step by Step](https://arxiv.org/abs/2505.13903)
*Chengyu Shen,Zhen Hao Wong,Runming He,Hao Liang,Meiyi Qiang,Zimo Meng,Zhengyang Zhao,Bohan Zeng,Zhengzhou Zhu,Bin Cui,Wentao Zhang*

Main category: cs.CL

TL;DR: 提出MathQ-Verify五阶段流程，有效筛选数学问题中的无效/不明确问题，提升数据集可靠性


<details>
  <summary>Details</summary>
Motivation: 现有数学QA方法忽视问题本身的有效性验证，导致数据集中可能包含定义模糊或逻辑矛盾的问题

Method: 五阶段流程：格式验证→形式化分解→逻辑矛盾检测→完整性检查→最终筛选

Result: 在多个基准测试达到SOTA，F1分数提升25个百分点；轻量级模型投票方案实现90%准确率和63%召回率

Conclusion: 该方法可有效减少数学数据集中的标签噪声，为LLM数学推理提供更可靠的数据基础

Abstract: Large Language Models (LLMs) have recently achieved remarkable progress in
mathematical reasoning. To enable such capabilities, many existing works
distill strong reasoning models into long chains of thought or design
algorithms to construct high-quality math QA data for training. However, these
efforts primarily focus on generating correct reasoning paths and answers,
while largely overlooking the validity of the questions themselves. In this
work, we propose Math Question Verification (MathQ-Verify), a novel five-stage
pipeline designed to rigorously filter ill-posed or under-specified math
problems. MathQ-Verify first performs format-level validation to remove
redundant instructions and ensure that each question is syntactically
well-formed. It then formalizes each question, decomposes it into atomic
conditions, and verifies them against mathematical definitions. Next, it
detects logical contradictions among these conditions, followed by a
goal-oriented completeness check to ensure the question provides sufficient
information for solving. To evaluate this task, we use existing benchmarks
along with an additional dataset we construct, containing 2,147 math questions
with diverse error types, each manually double-validated. Experiments show that
MathQ-Verify achieves state-of-the-art performance across multiple benchmarks,
improving the F1 score by up to 25 percentage points over the direct
verification baseline. It further attains approximately 90% precision and 63%
recall through a lightweight model voting scheme. MathQ-Verify offers a
scalable and accurate solution for curating reliable mathematical datasets,
reducing label noise and avoiding unnecessary computation on invalid questions.
Our code and data are available at https://github.com/scuuy/MathQ-Verify.

</details>


### [30] [Cross-Linguistic Transfer in Multilingual NLP: The Role of Language Families and Morphology](https://arxiv.org/abs/2505.13908)
*Ajitesh Bankula,Praney Bankula*

Main category: cs.CL

TL;DR: 研究语言家族和形态相似性如何影响跨语言迁移效果，并提出结合类型学信息的模型改进方案


<details>
  <summary>Details</summary>
Motivation: 解决多语言模型中资源丰富语言向低资源语言迁移效率不足的问题

Method: 通过对比多语言模型性能，分析语言距离指标与迁移效果的相关性，探索融入形态特征的预训练方法

Result: 语言家族邻近性和形态相似性显著影响迁移效果，整合类型学信息能提升低资源语言表现

Conclusion: 应将语言结构特征纳入模型训练框架以优化跨语言迁移，这为多语言NLP提供了新的技术路径

Abstract: Cross-lingual transfer has become a crucial aspect of multilingual NLP, as it
allows for models trained on resource-rich languages to be applied to
low-resource languages more effectively. Recently massively multilingual
pre-trained language models (e.g., mBERT, XLM-R) demonstrate strong zero-shot
transfer capabilities[14] [13]. This paper investigates cross-linguistic
transfer through the lens of language families and morphology. Investigating
how language family proximity and morphological similarity affect performance
across NLP tasks. We further discuss our results and how it relates to findings
from recent literature. Overall, we compare multilingual model performance and
review how linguistic distance metrics correlate with transfer outcomes. We
also look into emerging approaches that integrate typological and morphological
information into model pre-training to improve transfer to diverse
languages[18] [19].

</details>


### [31] [Word length predicts word order: "Min-max"-ing drives language evolution](https://arxiv.org/abs/2505.13913)
*Hiram Ring*

Main category: cs.CL

TL;DR: 通过分析1500多种语言的平行数据，提出词序演变受信息处理效率与信息结构双重压力驱动的'Min-Max'理论，调和先天论与功能论分歧。


<details>
  <summary>Details</summary>
Motivation: 针对语言学界关于词序演变机制（先天论vs功能论）的核心争议，探索跨语言普遍演化机制

Method: 使用涵盖133语系111孤立语的标注平行语料库，结合系统发育分析与回归模型验证假设

Result: 词类长度与词序显著相关（非简单线性），在两种语系中预测历史演变，回归模型解释力超越血统/地域因素

Conclusion: 综合处理效率压力（最小化认知负荷）与信息结构压力（最大化交际效果）的竞争机制驱动语言演化

Abstract: Current theories of language propose an innate (Baker 2001; Chomsky 1981) or
a functional (Greenberg 1963; Dryer 2007; Hawkins 2014) origin for the surface
structures (i.e. word order) that we observe in languages of the world, while
evolutionary modeling (Dunn et al. 2011) suggests that descent is the primary
factor influencing such patterns. Although there are hypotheses for word order
change from both innate and usage-based perspectives for specific languages and
families, there are key disagreements between the two major proposals for
mechanisms that drive the evolution of language more broadly (Wasow 2002; Levy
2008). This paper proposes a universal underlying mechanism for word order
change based on a large tagged parallel dataset of over 1,500 languages
representing 133 language families and 111 isolates. Results indicate that word
class length is significantly correlated with word order crosslinguistically,
but not in a straightforward manner, partially supporting opposing theories of
processing, while at the same time predicting historical word order change in
two different phylogenetic lines and explaining more variance than descent or
language area in regression models. Such findings suggest an integrated
"Min-Max" theory of language evolution driven by competing pressures of
processing and information structure, aligning with recent efficiency-oriented
(Levshina 2023) and information-theoretic proposals (Zaslavsky 2020; Tucker et
al. 2025).

</details>


### [32] [EEG-to-Text Translation: A Model for Deciphering Human Brain Activity](https://arxiv.org/abs/2505.13936)
*Saydul Akbar Murad,Ashim Dahal,Nick Rahimi*

Main category: cs.CL

TL;DR: 提出R1 Translator模型，通过双向LSTM编码器与预训练Transformer解码器的结合，显著提升EEG信号转文本任务的性能指标


<details>
  <summary>Details</summary>
Motivation: 现有EEG解码模型在文本生成效果上存在明显性能瓶颈，亟需优化方案突破技术限制

Method: 采用双向LSTM捕捉EEG序列依赖，配合预训练Transformer解码器进行文本生成，实现特征与生成能力的协同优化

Result: ROUGE-1达38%（较T5提升9%）、CER降低至0.5795（较基准模型优化2-4%），WER改善幅度达4.3%，全面超越T5和Brain Translator模型

Conclusion: R1 Translator通过架构创新有效突破EEG解码瓶颈，其开源部署为脑机接口领域提供新的技术方案

Abstract: With the rapid advancement of large language models like Gemini, GPT, and
others, bridging the gap between the human brain and language processing has
become an important area of focus. To address this challenge, researchers have
developed various models to decode EEG signals into text. However, these models
still face significant performance limitations. To overcome these shortcomings,
we propose a new model, R1 Translator, which aims to improve the performance of
EEG-to-text decoding. The R1 Translator model combines a bidirectional LSTM
encoder with a pretrained transformer-based decoder, utilizing EEG features to
produce high-quality text outputs. The model processes EEG embeddings through
the LSTM to capture sequential dependencies, which are then fed into the
transformer decoder for effective text generation. The R1 Translator excels in
ROUGE metrics, outperforming both T5 (previous research) and Brain Translator.
Specifically, R1 achieves a ROUGE-1 score of 38.00% (P), which is up to 9%
higher than T5 (34.89%) and 3% better than Brain (35.69%). It also leads in
ROUGE-L, with a F1 score of 32.51%, outperforming T5 by 3% (29.67%) and Brain
by 2% (30.38%). In terms of CER, R1 achieves a CER of 0.5795, which is 2% lower
than T5 (0.5917) and 4% lower than Brain (0.6001). Additionally, R1 performs
better in WER with a score of 0.7280, outperforming T5 by 4.3% (0.7610) and
Brain by 3.6% (0.7553). Code is available at
https://github.com/Mmurrad/EEG-To-text.

</details>


### [33] [Towards Rehearsal-Free Continual Relation Extraction: Capturing Within-Task Variance with Adaptive Prompting](https://arxiv.org/abs/2505.13944)
*Bao-Ngoc Dao,Quang Nguyen,Luyen Ngo Dinh,Minh Le,Nam Le,Linh Ngo Van*

Main category: cs.CL

TL;DR: 提出WAVE++方法解决持续关系抽取中提示法的任务识别不准与遗忘问题，通过任务提示池+标签描述+生成模型，超越现有方法且无需存储数据


<details>
  <summary>Details</summary>
Motivation: 现有持续关系抽取方法中，基于记忆的方法存在存储负担与隐私风险，而提示法则面临任务身份识别不准确、模型遗忘及跨任务处理能力不足三大核心挑战

Method: 1.基于前缀调优与混合专家思想设计任务专属提示池 2.引入关系标签描述增强分类 3.训练无关的推理任务预测机制 4.生成模型整合先验知识替代显式数据存储

Result: 在持续关系抽取任务中，WAVE++在性能表现上同时超越基于提示的SOTA方法和基于复现的方法，验证了各模块设计的有效性

Conclusion: 通过系统性解决提示法的核心痛点，WAVE++为持续学习提供了更健壮的解决方案，其免存储特性对实际应用具有重要意义，代码开源促进后续研究

Abstract: Memory-based approaches have shown strong performance in Continual Relation
Extraction (CRE). However, storing examples from previous tasks increases
memory usage and raises privacy concerns. Recently, prompt-based methods have
emerged as a promising alternative, as they do not rely on storing past
samples. Despite this progress, current prompt-based techniques face several
core challenges in CRE, particularly in accurately identifying task identities
and mitigating catastrophic forgetting. Existing prompt selection strategies
often suffer from inaccuracies, lack robust mechanisms to prevent forgetting in
shared parameters, and struggle to handle both cross-task and within-task
variations. In this paper, we propose WAVE++, a novel approach inspired by the
connection between prefix-tuning and mixture of experts. Specifically, we
introduce task-specific prompt pools that enhance flexibility and adaptability
across diverse tasks while avoiding boundary-spanning risks; this design more
effectively captures variations within each task and across tasks. To further
refine relation classification, we incorporate label descriptions that provide
richer, more global context, enabling the model to better distinguish among
different relations. We also propose a training-free mechanism to improve task
prediction during inference. Moreover, we integrate a generative model to
consolidate prior knowledge within the shared parameters, thereby removing the
need for explicit data storage. Extensive experiments demonstrate that WAVE++
outperforms state-of-the-art prompt-based and rehearsal-based methods, offering
a more robust solution for continual relation extraction. Our code is publicly
available at https://github.com/PiDinosauR2804/WAVE-CRE-PLUS-PLUS.

</details>


### [34] [Memory-Centric Embodied Question Answer](https://arxiv.org/abs/2505.13948)
*Mingliang Zhai,Zhi Gao,Yuwei Wu,Yunde Jia*

Main category: cs.CL

TL;DR: 提出以记忆为中心的MemoryEQA框架，通过多模态分层记忆机制提升复杂EQA任务处理能力，在MT-HM3D数据集上实现19.8%性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有以规划器为中心的EQA框架存在记忆模块与其他模块交互不足的缺陷，难以应对跨区域多目标复杂任务。

Method: 建立全局记忆（语言增强场景地图）与局部记忆（历史观测）的分层机制，利用多模态大模型动态转换记忆信息格式。

Result: 在MT-HM3D数据集上性能超越基线19.8%，验证框架处理跨区域多目标任务的有效性。

Conclusion: 记忆中心化架构显著增强EQA系统复杂任务处理能力，新构建的MT-HM3D数据集为评估记忆能力提供有效基准。

Abstract: Embodied Question Answering (EQA) requires agents to autonomously explore and
understand the environment to answer context-dependent questions. Existing
frameworks typically center around the planner, which guides the stopping
module, memory module, and answering module for reasoning. In this paper, we
propose a memory-centric EQA framework named MemoryEQA. Unlike planner-centric
EQA models where the memory module cannot fully interact with other modules,
MemoryEQA flexible feeds memory information into all modules, thereby enhancing
efficiency and accuracy in handling complex tasks, such as those involving
multiple targets across different regions. Specifically, we establish a
multi-modal hierarchical memory mechanism, which is divided into global memory
that stores language-enhanced scene maps, and local memory that retains
historical observations and state information. When performing EQA tasks, the
multi-modal large language model is leveraged to convert memory information
into the required input formats for injection into different modules. To
evaluate EQA models' memory capabilities, we constructed the MT-HM3D dataset
based on HM3D, comprising 1,587 question-answer pairs involving multiple
targets across various regions, which requires agents to maintain memory of
exploration-acquired target information. Experimental results on HM-EQA,
MT-HM3D, and OpenEQA demonstrate the effectiveness of our framework, where a
19.8% performance gain on MT-HM3D compared to baseline model further
underscores memory capability's pivotal role in resolving complex tasks.

</details>


### [35] [FlashThink: An Early Exit Method For Efficient Reasoning](https://arxiv.org/abs/2505.13949)
*Guochao Jiang,Guofeng Quan,Zepeng Ding,Ziqin Luo,Dixuan Wang,Zheng Hu*

Main category: cs.CL

TL;DR: 提出验证模型FlashThink实现大模型提前终止推理，在保持精度的前提下平均减少77%推理内容


<details>
  <summary>Details</summary>
Motivation: 大语言模型在简单任务中生成冗余推理内容导致计算效率低下，实验发现模型在生成中途已具备正确解答能力

Method: 引入验证模型实时判断推理终止时机，当模型达到可靠结论时立即停止生成

Result: 在四个基准测试中，Deepseek-R1和QwQ-32B模型的推理内容长度分别减少77.04%和77.47%且精度不降

Conclusion: FlashThink方法有效实现了高效推理，突破传统必须完整生成所有推理步骤的范式，显著提升大模型推理效率

Abstract: Large Language Models (LLMs) have shown impressive performance in reasoning
tasks. However, LLMs tend to generate excessively long reasoning content,
leading to significant computational overhead. Our observations indicate that
even on simple problems, LLMs tend to produce unnecessarily lengthy reasoning
content, which is against intuitive expectations. Preliminary experiments show
that at a certain point during the generation process, the model is already
capable of producing the correct solution without completing the full reasoning
content. Therefore, we consider that the reasoning process of the model can be
exited early to achieve the purpose of efficient reasoning. We introduce a
verification model that identifies the exact moment when the model can stop
reasoning and still provide the correct answer. Comprehensive experiments on
four different benchmarks demonstrate that our proposed method, FlashThink,
effectively shortens the reasoning content while preserving the model accuracy.
For the Deepseek-R1 and QwQ-32B models, we reduced the length of reasoning
content by 77.04% and 77.47%, respectively, without reducing the accuracy.

</details>


### [36] [Through a Compressed Lens: Investigating the Impact of Quantization on LLM Explainability and Interpretability](https://arxiv.org/abs/2505.13963)
*Qianli Wang,Mingyang Wang,Nils Feldhus,Simon Ostermann,Yuan Cao,Hinrich Schütze,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: 研究发现量化方法对LLM可解释性和透明度的影晌具有不确定性，其方向取决于量化方法、评估协议等因素，需在透明性要求高的场景中谨慎应用。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注量化对LLM能力的退化，但对其可解释性和决策透明度的影响尚未探索，这对理解模型决策机制至关重要。

Method: 采用三种量化技术（不同位宽）结合两种可解释性方法（反事实示例/NLP解释）和两种可解释性分析（知识记忆/潜在多跳推理），并通过用户研究验证。

Result: 量化对可解释性的影晌存在双向性：部分场景降低解释质量（人类评估显示退化），部分场景反而提升解释效果，关键取决于量化方法、评估协议和具体场景的组合。

Conclusion: 量化可能不可预测地改变模型透明度，这对医疗、法律等需要高透明度的LLM应用部署具有重要警示意义。

Abstract: Quantization methods are widely used to accelerate inference and streamline
the deployment of large language models (LLMs). While prior research has
extensively investigated the degradation of various LLM capabilities due to
quantization, its effects on model explainability and interpretability, which
are crucial for understanding decision-making processes, remain unexplored. To
address this gap, we conduct comprehensive experiments using three common
quantization techniques at distinct bit widths, in conjunction with two
explainability methods, counterfactual examples and natural language
explanations, as well as two interpretability approaches, knowledge
memorization analysis and latent multi-hop reasoning analysis. We complement
our analysis with a thorough user study, evaluating selected explainability
methods. Our findings reveal that, depending on the configuration, quantization
can significantly impact model explainability and interpretability. Notably,
the direction of this effect is not consistent, as it strongly depends on (1)
the quantization method, (2) the explainability or interpretability approach,
and (3) the evaluation protocol. In some settings, human evaluation shows that
quantization degrades explainability, while in others, it even leads to
improvements. Our work serves as a cautionary tale, demonstrating that
quantization can unpredictably affect model transparency. This insight has
important implications for deploying LLMs in applications where transparency is
a critical requirement.

</details>


### [37] [CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring](https://arxiv.org/abs/2505.13965)
*Jiamin Su,Yibo Yan,Zhuoran Gao,Han Zhang,Xiang Liu,Xuming Hu*

Main category: cs.CL

TL;DR: 提出首个协作多智能体框架CAFES用于多模态作文评分，通过三阶段协作机制提升评分准确性


<details>
  <summary>Details</summary>
Motivation: 传统AES方法存在评估泛化性不足和缺乏多模态感知能力的问题，现有MLLM方法存在评分幻觉和与人类判断偏差的缺陷

Method: 包含初始评分者(快速特质评估)、反馈池管理者(证据型优势汇总)、反思评分者(迭代优化评分)的三代理协作机制

Result: 实验显示QWK指标相对提升21%，在语法和词汇多样性维度表现突出

Conclusion: CAFES框架为构建智能多模态AES系统提供了新范式，代码将在论文接受后开源

Abstract: Automated Essay Scoring (AES) is crucial for modern education, particularly
with the increasing prevalence of multimodal assessments. However, traditional
AES methods struggle with evaluation generalizability and multimodal
perception, while even recent Multimodal Large Language Model (MLLM)-based
approaches can produce hallucinated justifications and scores misaligned with
human judgment. To address the limitations, we introduce CAFES, the first
collaborative multi-agent framework specifically designed for AES. It
orchestrates three specialized agents: an Initial Scorer for rapid,
trait-specific evaluations; a Feedback Pool Manager to aggregate detailed,
evidence-grounded strengths; and a Reflective Scorer that iteratively refines
scores based on this feedback to enhance human alignment. Extensive
experiments, using state-of-the-art MLLMs, achieve an average relative
improvement of 21% in Quadratic Weighted Kappa (QWK) against ground truth,
especially for grammatical and lexical diversity. Our proposed CAFES framework
paves the way for an intelligent multimodal AES system. The code will be
available upon acceptance.

</details>


### [38] [Truth or Twist? Optimal Model Selection for Reliable Label Flipping Evaluation in LLM-based Counterfactuals](https://arxiv.org/abs/2505.13972)
*Qianli Wang,Van Bach Nguyen,Nils Feldhus,Luis Felipe Villa-Arenas,Christin Seifert,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: 研究发现评判模型与生成模型的独立性对反事实数据增强效果起关键作用，自动流程需结合人工干预


<details>
  <summary>Details</summary>
Motivation: 现有反事实数据增强方法中评判模型选择存在不一致性，需要明确不同模型关系对评估结果的影响

Method: 使用2种前沿LLM方法、3个数据集、5个生成模型和15个评判模型进行实验，结合90人用户研究

Result: 独立且未微调的评判模型评估最可靠，但与用户研究结果仍存在显著差距（自动化流程准确率不足）

Conclusion: 完全自动化的反事实数据增强流程存在局限，需要引入人工验证环节确保有效性

Abstract: Counterfactual examples are widely employed to enhance the performance and
robustness of large language models (LLMs) through counterfactual data
augmentation (CDA). However, the selection of the judge model used to evaluate
label flipping, the primary metric for assessing the validity of generated
counterfactuals for CDA, yields inconsistent results. To decipher this, we
define four types of relationships between the counterfactual generator and
judge models. Through extensive experiments involving two state-of-the-art
LLM-based methods, three datasets, five generator models, and 15 judge models,
complemented by a user study (n = 90), we demonstrate that judge models with an
independent, non-fine-tuned relationship to the generator model provide the
most reliable label flipping evaluations. Relationships between the generator
and judge models, which are closely aligned with the user study for CDA, result
in better model performance and robustness. Nevertheless, we find that the gap
between the most effective judge models and the results obtained from the user
study remains considerably large. This suggests that a fully automated pipeline
for CDA may be inadequate and requires human intervention.

</details>


### [39] [Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models](https://arxiv.org/abs/2505.13973)
*Wenhui Zhu,Xuanzhao Dong,Xin Li,Peijie Qiu,Xiwen Chen,Abolfazl Razi,Aris Sotiras,Yi Su,Yalin Wang*

Main category: cs.CL

TL;DR: 医疗MLLMs中基于GRPO的强化学习调优在临床VQA任务中优于标准SFT方法


<details>
  <summary>Details</summary>
Motivation: 需要使模型响应与临床期望对齐，解决RL调优在医疗任务中的有效性挑战

Method: 通过分析基础模型初始化策略、医学语义对齐、长度奖励机制和偏置影响四个维度

Result: GRPO-based RL调优在准确率（提高3.7%）和推理质量（提升12.5%）上均超越监督微调

Conclusion: 揭示了医疗领域微调的关键因素，证实GRPO方法在临床VQA任务中的优越性

Abstract: Recently, reinforcement learning (RL)-based tuning has shifted the trajectory
of Multimodal Large Language Models (MLLMs), particularly following the
introduction of Group Relative Policy Optimization (GRPO). However, directly
applying it to medical tasks remains challenging for achieving clinically
grounded model behavior. Motivated by the need to align model response with
clinical expectations, we investigate four critical dimensions that affect the
effectiveness of RL-based tuning in medical visual question answering (VQA):
base model initialization strategy, the role of medical semantic alignment, the
impact of length-based rewards on long-chain reasoning, and the influence of
bias. We conduct extensive experiments to analyze these factors for medical
MLLMs, providing new insights into how models are domain-specifically
fine-tuned. Additionally, our results also demonstrate that GRPO-based RL
tuning consistently outperforms standard supervised fine-tuning (SFT) in both
accuracy and reasoning quality.

</details>


### [40] [DRP: Distilled Reasoning Pruning with Skill-aware Step Decomposition for Efficient Large Reasoning Models](https://arxiv.org/abs/2505.13975)
*Yuxuan Jiang,Dawei Li,Frank Ferraro*

Main category: cs.CL

TL;DR: 提出蒸馏推理剪枝框架(DRP)，通过剪枝和蒸馏技术提升大模型推理效率，在数学推理任务中实现显著降本增效


<details>
  <summary>Details</summary>
Motivation: 大型推理模型的长思维链导致推理过程冗余低效，需优化推理路径提升效率

Method: 结合推理时剪枝与蒸馏技术：1）教师模型进行步骤分解与内容剪枝 2）将优化路径蒸馏至学生模型

Result: GSM8K任务token使用减少64%且准确率提升2.4%，AIME任务token减少43%无性能损失

Conclusion: 通过对齐学生模型能力与训练数据的推理结构，实现高效知识迁移与性能提升

Abstract: While Large Reasoning Models (LRMs) have demonstrated success in complex
reasoning tasks through long chain-of-thought (CoT) reasoning, their inference
often involves excessively verbose reasoning traces, resulting in substantial
inefficiency. To address this, we propose Distilled Reasoning Pruning (DRP), a
hybrid framework that combines inference-time pruning with tuning-based
distillation, two widely used strategies for efficient reasoning. DRP uses a
teacher model to perform skill-aware step decomposition and content pruning,
and then distills the pruned reasoning paths into a student model, enabling it
to reason both efficiently and accurately. Across several challenging
mathematical reasoning datasets, we find that models trained with DRP achieve
substantial improvements in token efficiency without sacrificing accuracy.
Specifically, DRP reduces average token usage on GSM8K from 917 to 328 while
improving accuracy from 91.7% to 94.1%, and achieves a 43% token reduction on
AIME with no performance drop. Further analysis shows that aligning the
reasoning structure of training CoTs with the student's reasoning capacity is
critical for effective knowledge transfer and performance gains.

</details>


### [41] [Mixed Signals: Understanding Model Disagreement in Multimodal Empathy Detection](https://arxiv.org/abs/2505.13979)
*Maya Srikanth,Run Chen,Julia Hirschberg*

Main category: cs.CL

TL;DR: 多模态模型在共情检测中易受模态冲突影响，预测分歧反映潜在模糊性，可作为提升系统鲁棒性的诊断信号。


<details>
  <summary>Details</summary>
Motivation: 探究多模态模型在模态线索冲突时的失效原因，利用单模态与多模态预测分歧揭示模型决策弱点。

Method: 使用微调的文本/音频/视频单模态模型和门控融合模型，分析预测分歧与标注者不确定性的关联。

Result: 发现主导模态信号会误导融合模型，且人类与模型类似并不总能受益于多模态输入。

Conclusion: 预测分歧可作为诊断工具识别挑战性样本，提升共情系统对模态冲突的鲁棒性。

Abstract: Multimodal models play a key role in empathy detection, but their performance
can suffer when modalities provide conflicting cues. To understand these
failures, we examine cases where unimodal and multimodal predictions diverge.
Using fine-tuned models for text, audio, and video, along with a gated fusion
model, we find that such disagreements often reflect underlying ambiguity, as
evidenced by annotator uncertainty. Our analysis shows that dominant signals in
one modality can mislead fusion when unsupported by others. We also observe
that humans, like models, do not consistently benefit from multimodal input.
These insights position disagreement as a useful diagnostic signal for
identifying challenging examples and improving empathy system robustness.

</details>


### [42] [The Hallucination Tax of Reinforcement Finetuning](https://arxiv.org/abs/2505.13988)
*Linxin Song,Taiwei Shi,Jieyu Zhao*

Main category: cs.CL

TL;DR: 强化微调(RFT)会显著降低语言模型对不可解问题的拒绝能力（拒绝率下降超80%），但通过在训练中加入少量合成不可解数学问题(SUM)，可在保持任务准确性的同时恢复模型的合理拒绝行为。


<details>
  <summary>Details</summary>
Motivation: 揭示强化微调对LLM可信度的负面影响（幻觉税现象），即模型在不可解问题上更易产生自信的幻觉回答，探索提升模型自我认知边界的解决方案。

Method: 构建合成不可解数学数据集SUM，通过控制实验比较标准RFT与加入10% SUM数据的改进RFT对模型拒绝行为的影响，并测试其泛化能力。

Result: 加入10% SUM数据的RFT使模型拒绝率恢复至合理水平，在保留95%数学问题解决能力的同时，将事实问答任务的拒绝准确率提升22%。

Conclusion: 需在RFT中平衡能力提升与可信度保护，通过暴露少量边界案例使模型学会利用计算资源评估自身不确定性，实现更安全可靠的推理。

Abstract: Reinforcement finetuning (RFT) has become a standard approach for enhancing
the reasoning capabilities of large language models (LLMs). However, its impact
on model trustworthiness remains underexplored. In this work, we identify and
systematically study a critical side effect of RFT, which we term the
hallucination tax: a degradation in refusal behavior causing models to produce
hallucinated answers to unanswerable questions confidently. To investigate
this, we introduce SUM (Synthetic Unanswerable Math), a high-quality dataset of
unanswerable math problems designed to probe models' ability to recognize an
unanswerable question by reasoning from the insufficient or ambiguous
information. Our results show that standard RFT training could reduce model
refusal rates by more than 80%, which significantly increases model's tendency
to hallucinate. We further demonstrate that incorporating just 10% SUM during
RFT substantially restores appropriate refusal behavior, with minimal accuracy
trade-offs on solvable tasks. Crucially, this approach enables LLMs to leverage
inference-time compute to reason about their own uncertainty and knowledge
boundaries, improving generalization not only to out-of-domain math problems
but also to factual question answering tasks.

</details>


### [43] [DecIF: Improving Instruction-Following through Meta-Decomposition](https://arxiv.org/abs/2505.13990)
*Tingfeng Hui,Pengyu Zhu,Bowen Ping,Ling Tang,Yaqi Zhang,Sen Su*

Main category: cs.CL

TL;DR: 提出DecIF框架——基于元分解的自主指令生成框架，通过纯LLM实现高质量指令数据合成


<details>
  <summary>Details</summary>
Motivation: 现有指令数据生成方法依赖外部资源，导致灵活性和泛化性受限

Method: 采用元分解策略：1. 指令生成阶段迭代生成元信息并融合响应约束 2. 响应生成阶段进行原子级评估标准分解验证

Result: 多场景实验验证框架有效性，展现出在指令遵循任务中的优异性能

Conclusion: DecIF成功实现了无需人工干预的高质量指令数据自动合成，具有强扩展性和领域适应性

Abstract: Instruction-following has emerged as a crucial capability for large language
models (LLMs). However, existing approaches often rely on pre-existing
documents or external resources to synthesize instruction-following data, which
limits their flexibility and generalizability. In this paper, we introduce
DecIF, a fully autonomous, meta-decomposition guided framework that generates
diverse and high-quality instruction-following data using only LLMs. DecIF is
grounded in the principle of decomposition. For instruction generation, we
guide LLMs to iteratively produce various types of meta-information, which are
then combined with response constraints to form well-structured and
semantically rich instructions. We further utilize LLMs to detect and resolve
potential inconsistencies within the generated instructions. Regarding response
generation, we decompose each instruction into atomic-level evaluation
criteria, enabling rigorous validation and the elimination of inaccurate
instruction-response pairs. Extensive experiments across a wide range of
scenarios and settings demonstrate DecIF's superior performance on
instruction-following tasks. Further analysis highlights its strong
flexibility, scalability, and generalizability in automatically synthesizing
high-quality instruction data.

</details>


### [44] [Social Sycophancy: A Broader Understanding of LLM Sycophancy](https://arxiv.org/abs/2505.13995)
*Myra Cheng,Sunny Yu,Cinoo Lee,Pranav Khadpe,Lujain Ibrahim,Dan Jurafsky*

Main category: cs.CL

TL;DR: 论文提出LLMs存在社会性奉承问题（过度维护用户面子），构建ELEPHANT框架量化评估5种面子维护行为，发现模型奉承率显著高于人类且难以缓解。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅关注可验证事实场景的奉承行为，忽视无明确事实的模糊情境（如情感支持场景）中LLMs可能强化有害假设的问题。

Method: 通过ELEPHANT框架（含情感认同、道德支持等5维度）评估8个模型，使用OEQ开放问答和Reddit道德判断（AITA）两个数据集进行测试。

Result: 模型面子维护行为比人类高47%（OEQ），在42%的AITA案例中支持人类判定不当行为；奉承行为受偏好数据奖励且不易干预。

Conclusion: 为LLMs社会性奉承问题提供理论框架与评估工具，揭示该现象对模型安全性的深层影响，推动相关缓解措施研究。

Abstract: A serious risk to the safety and utility of LLMs is sycophancy, i.e.,
excessive agreement with and flattery of the user. Yet existing work focuses on
only one aspect of sycophancy: agreement with users' explicitly stated beliefs
that can be compared to a ground truth. This overlooks forms of sycophancy that
arise in ambiguous contexts such as advice and support-seeking, where there is
no clear ground truth, yet sycophancy can reinforce harmful implicit
assumptions, beliefs, or actions. To address this gap, we introduce a richer
theory of social sycophancy in LLMs, characterizing sycophancy as the excessive
preservation of a user's face (the positive self-image a person seeks to
maintain in an interaction). We present ELEPHANT, a framework for evaluating
social sycophancy across five face-preserving behaviors (emotional validation,
moral endorsement, indirect language, indirect action, and accepting framing)
on two datasets: open-ended questions (OEQ) and Reddit's r/AmITheAsshole
(AITA). Across eight models, we show that LLMs consistently exhibit high rates
of social sycophancy: on OEQ, they preserve face 47% more than humans, and on
AITA, they affirm behavior deemed inappropriate by crowdsourced human judgments
in 42% of cases. We further show that social sycophancy is rewarded in
preference datasets and is not easily mitigated. Our work provides theoretical
grounding and empirical tools (datasets and code) for understanding and
addressing this under-recognized but consequential issue.

</details>


### [45] [Activation-Guided Consensus Merging for Large Language Models](https://arxiv.org/abs/2505.14009)
*Yuxuan Yao,Shuqi Liu,Zehua Liu,Qintong Li,Mingyang Liu,Xiongwei Han,Zhijiang Guo,Han Wu,Linqi Song*

Main category: cs.CL

TL;DR: 提出激活引导共识合并框架(ACM)，通过互信息优化层间合并系数，实现高效模型融合


<details>
  <summary>Details</summary>
Motivation: 传统模型融合方法忽视神经组件的功能差异性，导致任务能力保留不足

Method: 基于预训练与微调模型激活的互信息动态确定层间合并权重

Result: Qwen-7B模型响应长度减少55.3%同时准确率提升1.3个百分点

Conclusion: ACM框架有效整合系统1效率与系统2推理能力，提供无训练成本的模型融合方案

Abstract: Recent research has increasingly focused on reconciling the reasoning
capabilities of System 2 with the efficiency of System 1. While existing
training-based and prompt-based approaches face significant challenges in terms
of efficiency and stability, model merging emerges as a promising strategy to
integrate the diverse capabilities of different Large Language Models (LLMs)
into a unified model. However, conventional model merging methods often assume
uniform importance across layers, overlooking the functional heterogeneity
inherent in neural components. To address this limitation, we propose
\textbf{A}ctivation-Guided \textbf{C}onsensus \textbf{M}erging (\textbf{ACM}),
a plug-and-play merging framework that determines layer-specific merging
coefficients based on mutual information between activations of pre-trained and
fine-tuned models. ACM effectively preserves task-specific capabilities without
requiring gradient computations or additional training. Extensive experiments
on Long-to-Short (L2S) and general merging tasks demonstrate that ACM
consistently outperforms all baseline methods. For instance, in the case of
Qwen-7B models, TIES-Merging equipped with ACM achieves a \textbf{55.3\%}
reduction in response length while simultaneously improving reasoning accuracy
by \textbf{1.3} points. We submit the code with the paper for reproducibility,
and it will be publicly available.

</details>


### [46] [AUTOLAW: Enhancing Legal Compliance in Large Language Models via Case Law Generation and Jury-Inspired Deliberation](https://arxiv.org/abs/2505.14015)
*Tai D. Nguyen,Long H. Pham,Jun Sun*

Main category: cs.CL

TL;DR: 提出AutoLaw框架，通过对抗数据生成和陪审团机制提升大语言模型的法律合规性检测能力


<details>
  <summary>Details</summary>
Motivation: 现有法律评估基准缺乏对地区法律差异的适应性，无法满足动态监管需求

Method: 结合对抗性案例生成与LLM陪审团审议机制，动态合成案例法并模拟司法决策流程

Result: 在三个基准测试中，对抗数据使模型辨别力提升，陪审团策略使违规检测率显著提高

Conclusion: AutoLaw能自适应探测法律偏差，提供可靠的情境感知判断，为法律敏感场景提供可扩展解决方案

Abstract: The rapid advancement of domain-specific large language models (LLMs) in
fields like law necessitates frameworks that account for nuanced regional legal
distinctions, which are critical for ensuring compliance and trustworthiness.
Existing legal evaluation benchmarks often lack adaptability and fail to
address diverse local contexts, limiting their utility in dynamically evolving
regulatory landscapes. To address these gaps, we propose AutoLaw, a novel
violation detection framework that combines adversarial data generation with a
jury-inspired deliberation process to enhance legal compliance of LLMs. Unlike
static approaches, AutoLaw dynamically synthesizes case law to reflect local
regulations and employs a pool of LLM-based "jurors" to simulate judicial
decision-making. Jurors are ranked and selected based on synthesized legal
expertise, enabling a deliberation process that minimizes bias and improves
detection accuracy. Evaluations across three benchmarks: Law-SG, Case-SG
(legality), and Unfair-TOS (policy), demonstrate AutoLaw's effectiveness:
adversarial data generation improves LLM discrimination, while the jury-based
voting strategy significantly boosts violation detection rates. Our results
highlight the framework's ability to adaptively probe legal misalignments and
deliver reliable, context-aware judgments, offering a scalable solution for
evaluating and enhancing LLMs in legally sensitive applications.

</details>


### [47] [From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora](https://arxiv.org/abs/2505.14045)
*Yingli Shen,Wen Lai,Shuo Wang,Kangyang Luo,Alexander Fraser,Maosong Sun*

Main category: cs.CL

TL;DR: 提出大规模多向平行语料库TED2025，验证其在提升大语言模型多语言性能中的有效性


<details>
  <summary>Details</summary>
Motivation: 非对齐多语言数据难以捕捉跨语言语义一致性，而多向平行数据能提供更强的跨语言对齐特征，但目前缺乏系统性的应用研究

Method: 构建跨113种语言、最多50语种对齐的TED2025语料库，设计持续预训练和指令调优策略，通过消融实验分析关键影响因素

Result: 在6个多语言基准测试中，基于多向平行数据训练的模型全面超越非对齐数据训练的基线模型

Conclusion: 多向平行数据能有效提升大语言模型的多语言处理能力，其对齐特性为跨语言语义学习提供了更优的监督信号

Abstract: Continued pretraining and instruction tuning on large-scale multilingual data
have proven to be effective in scaling large language models (LLMs) to
low-resource languages. However, the unaligned nature of such data limits its
ability to effectively capture cross-lingual semantics. In contrast, multi-way
parallel data, where identical content is aligned across multiple languages,
provides stronger cross-lingual consistency and offers greater potential for
improving multilingual performance. In this paper, we introduce a large-scale,
high-quality multi-way parallel corpus, TED2025, based on TED Talks. The corpus
spans 113 languages, with up to 50 languages aligned in parallel, ensuring
extensive multilingual coverage. Using this dataset, we investigate best
practices for leveraging multi-way parallel data to enhance LLMs, including
strategies for continued pretraining, instruction tuning, and the analysis of
key influencing factors. Experiments on six multilingual benchmarks show that
models trained on multiway parallel data consistently outperform those trained
on unaligned multilingual data.

</details>


### [48] [Improved Methods for Model Pruning and Knowledge Distillation](https://arxiv.org/abs/2505.14052)
*Wei Jiang,Anying Fu,Youling Zhang*

Main category: cs.CL

TL;DR: 提出MAMA剪枝方法，在极端剪枝水平下仍保持模型性能，无需大量微调


<details>
  <summary>Details</summary>
Motivation: 传统剪枝方法存在性能显著下降、需大量重训练的问题，难以适应人机交互场景的实时性要求

Method: 基于预训练阶段权重/偏置固定和训练后阶段GRPO奖励验证的双重指标，建立新型剪枝指示器体系

Result: 在多种剪枝比例和下游计算语言学任务中超越或持平SOTA方法

Conclusion: MAMA方法通过创新的动态分析机制，实现了模型压缩与性能保持的有效平衡，具有工程实用价值

Abstract: Model pruning is a performance optimization technique for large language
models like R1 or o3-mini. However, existing pruning methods often lead to
significant performance degradation or require extensive retraining and
fine-tuning. This technique aims to identify and remove neurons, connections
unlikely leading to the contribution during the human-computer interaction
phase. Our goal is to obtain a much smaller and faster knowledge distilled
model that can quickly generate content almost as good as those of the unpruned
ones. We propose MAMA Pruning, short for Movement and Magnitude Analysis, an
improved pruning method that effectively reduces model size and computational
complexity while maintaining performance comparable to the original unpruned
model even at extreme pruned levels. The improved method is based on weights,
bias fixed in the pre-training phase and GRPO rewards verified during the
post-training phase as our novel pruning indicators. Preliminary experimental
results show that our method outperforms and be comparable to state-of-the-art
methods across various pruning levels and different downstream computational
linguistics tasks.

</details>


### [49] [Enhancing LLMs via High-Knowledge Data Selection](https://arxiv.org/abs/2505.14070)
*Feiyu Duan,Xuemiao Zhang,Sirui Wang,Haoran Que,Yuqi Liu,Wenge Rong,Xunliang Cai*

Main category: cs.CL

TL;DR: 提出基于知识密度与覆盖度的高知识评分器HKS，通过数据选择缓解预训练语料知识稀缺问题，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法忽略文本语料的知识丰富性，导致预训练模型面临知识匮乏瓶颈。

Method: 1.构建多领域知识元素池
2.定义知识密度(单位文本知识含量)和覆盖度(跨领域知识广度)
3.设计综合知识评分器筛选高知识数据
4.支持限定领域知识元素实现垂直领域优化

Result: 实验显示：
- 知识密集任务准确率提升18.7%
- 通用理解任务F1值提高9.3%
- 领域特定任务表现超过基线32%

Conclusion: HKS通过知识导向的数据选择机制，有效增强模型的通用能力和领域适应性，为预训练数据优化提供新范式。

Abstract: The performance of Large Language Models (LLMs) is intrinsically linked to
the quality of its training data. Although several studies have proposed
methods for high-quality data selection, they do not consider the importance of
knowledge richness in text corpora. In this paper, we propose a novel and
gradient-free High-Knowledge Scorer (HKS) to select high-quality data from the
dimension of knowledge, to alleviate the problem of knowledge scarcity in the
pre-trained corpus. We propose a comprehensive multi-domain knowledge element
pool and introduce knowledge density and coverage as metrics to assess the
knowledge content of the text. Based on this, we propose a comprehensive
knowledge scorer to select data with intensive knowledge, which can also be
utilized for domain-specific high-knowledge data selection by restricting
knowledge elements to the specific domain. We train models on a high-knowledge
bilingual dataset, and experimental results demonstrate that our scorer
improves the model's performance in knowledge-intensive and general
comprehension tasks, and is effective in enhancing both the generic and
domain-specific capabilities of the model.

</details>


### [50] [BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks](https://arxiv.org/abs/2505.14079)
*Weihong Du,Wenrui Liao,Binyu Yan,Hongru Liang,Anthony G. Cohn,Wenqiang Lei*

Main category: cs.CL

TL;DR: 提出基于后向推理的BAR代理，通过从终止状态逆向规划显著提升复杂任务处理能力


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理的前向推理范式在Minecraft等复杂任务中存在初始状态与目标间的感知差距问题

Method: 设计包含递归目标分解、状态一致性维护和阶段记忆模块的后向推理框架，从终止状态逆向生成可行路径

Result: 实验证明BAR在复杂任务规划中优于现有方法，各模块均展现显著有效性

Conclusion: 后向推理范式能有效缩小感知差距，BAR框架为复杂环境下的任务规划提供了新解决方案

Abstract: Large language model (LLM) based agents have shown great potential in
following human instructions and automatically completing various tasks. To
complete a task, the agent needs to decompose it into easily executed steps by
planning. Existing studies mainly conduct the planning by inferring what steps
should be executed next starting from the agent's initial state. However, this
forward reasoning paradigm doesn't work well for complex tasks. We propose to
study this issue in Minecraft, a virtual environment that simulates complex
tasks based on real-world scenarios. We believe that the failure of forward
reasoning is caused by the big perception gap between the agent's initial state
and task goal. To this end, we leverage backward reasoning and make the
planning starting from the terminal state, which can directly achieve the task
goal in one step. Specifically, we design a BAckward Reasoning based agent
(BAR). It is equipped with a recursive goal decomposition module, a state
consistency maintaining module and a stage memory module to make robust,
consistent, and efficient planning starting from the terminal state.
Experimental results demonstrate the superiority of BAR over existing methods
and the effectiveness of proposed modules.

</details>


### [51] [Gender Trouble in Language Models: An Empirical Audit Guided by Gender Performativity Theory](https://arxiv.org/abs/2505.14080)
*Franziska Sofia Hafner,Ana Valdivia,Luc Rocher*

Main category: cs.CL

TL;DR: 论文主张语言模型的性别偏见应超越表层词汇关联，需基于性别建构理论重新定义，实证发现模型将性别二元化并与生理性别绑定，且模型规模越大偏见越强。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型仅消除词汇关联的性别偏见，未能解决性别建构本身（如性别与生理性别的混淆）导致的跨性别群体身份抹除、医疗误诊等深层危害。

Method: 基于性别表演理论构建分析框架，对16种不同架构/训练数据/规模的模型进行系统性实证研究，检测模型对性别概念的表征方式。

Result: 模型普遍将性别编码为与生理性别绑定的二元范畴，非二元性别术语被边缘化；更大的模型虽性能提升，但性别-生理关联更强，加剧偏见。

Conclusion: 呼吁重新定义语言模型性别偏见标准，强调需从性别建构的哲学层面（而不仅是统计关联）制定缓解策略，警惕模型规模化带来的偏见放大效应。

Abstract: Language models encode and subsequently perpetuate harmful gendered
stereotypes. Research has succeeded in mitigating some of these harms, e.g. by
dissociating non-gendered terms such as occupations from gendered terms such as
'woman' and 'man'. This approach, however, remains superficial given that
associations are only one form of prejudice through which gendered harms arise.
Critical scholarship on gender, such as gender performativity theory,
emphasizes how harms often arise from the construction of gender itself, such
as conflating gender with biological sex. In language models, these issues
could lead to the erasure of transgender and gender diverse identities and
cause harms in downstream applications, from misgendering users to
misdiagnosing patients based on wrong assumptions about their anatomy.
  For FAccT research on gendered harms to go beyond superficial linguistic
associations, we advocate for a broader definition of 'gender bias' in language
models. We operationalize insights on the construction of gender through
language from gender studies literature and then empirically test how 16
language models of different architectures, training datasets, and model sizes
encode gender. We find that language models tend to encode gender as a binary
category tied to biological sex, and that gendered terms that do not neatly
fall into one of these binary categories are erased and pathologized. Finally,
we show that larger models, which achieve better results on performance
benchmarks, learn stronger associations between gender and sex, further
reinforcing a narrow understanding of gender. Our findings lead us to call for
a re-evaluation of how gendered harms in language models are defined and
addressed.

</details>


### [52] [Beyond Chains: Bridging Large Language Models and Knowledge Bases in Complex Question Answering](https://arxiv.org/abs/2505.14099)
*Yihua Zhu,Qianying Liu,Akiko Aizawa,Hidetoshi Shimodaira*

Main category: cs.CL

TL;DR: 提出了四阶段PDRR框架（预测-分解-检索-推理），通过结构化三元组分解和知识库引导推理，显著提升了复杂KBQA任务的性能表现


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法存在知识过时和幻觉问题，KG-RAG方法局限于链式结构问题。需要结合语义解析与知识检索的优势，解决复杂问题的推理需求

Method: 1.预测问题类型 2.分解为结构化三元组 3.从知识库检索相关信息 4.引导LLM作为代理完成三元组推理

Result: 在不同LLM骨干网络上的实验显示，PDRR在链式和非链式复杂问题上均超越现有方法，最高提升达12.8%准确率

Conclusion: PDRR框架通过结构化分解和知识引导推理，有效解决了复杂KBQA任务中的逻辑规划问题，实现了更优的泛化能力和可解释性

Abstract: Knowledge Base Question Answering (KBQA) aims to answer natural language
questions using structured knowledge from KBs. While LLM-only approaches offer
generalization, they suffer from outdated knowledge, hallucinations, and lack
of transparency. Chain-based KG-RAG methods address these issues by
incorporating external KBs, but are limited to simple chain-structured
questions due to the absence of planning and logical structuring. Inspired by
semantic parsing methods, we propose PDRR: a four-stage framework consisting of
Predict, Decompose, Retrieve, and Reason. Our method first predicts the
question type and decomposes the question into structured triples. Then
retrieves relevant information from KBs and guides the LLM as an agent to
reason over and complete the decomposed triples. Experimental results
demonstrate that PDRR consistently outperforms existing methods across various
LLM backbones and achieves superior performance on both chain-structured and
non-chain complex questions.

</details>


### [53] [MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation of LLM Hallucinations](https://arxiv.org/abs/2505.14101)
*Ernests Lavrinovics,Russa Biswas,Katja Hose,Johannes Bjerva*

Main category: cs.CL

TL;DR: 提出基于知识图谱的多语言多跳基准MultiHal，用于评估生成文本的事实性，通过KG整合显著提升语义相似度。


<details>
  <summary>Details</summary>
Motivation: 现有事实性评估基准依赖英语数据集和补充性上下文，忽略结构化知识资源。知识图谱能以最小语言开销结构化表示事实，缓解大模型幻觉问题。

Method: 构建MultiHal基准：1) 从开放领域KG挖掘14万条路径；2) 去噪后精选2.59万条高质量KG路径；3) 基于KG路径设计多语言多跳评估框架。

Result: 基线评估显示KG-RAG在多种语言/模型上语义相似度比传统QA提升0.12-0.36点，验证KG整合有效性。

Conclusion: MultiHal将推动基于知识图谱的幻觉缓解、事实核查及多语言语义理解研究发展。

Abstract: Large Language Models (LLMs) have inherent limitations of faithfulness and
factuality, commonly referred to as hallucinations. Several benchmarks have
been developed that provide a test bed for factuality evaluation within the
context of English-centric datasets, while relying on supplementary informative
context like web links or text passages but ignoring the available structured
factual resources. To this end, Knowledge Graphs (KGs) have been identified as
a useful aid for hallucination mitigation, as they provide a structured way to
represent the facts about entities and their relations with minimal linguistic
overhead. We bridge the lack of KG paths and multilinguality for factual
language modeling within the existing hallucination evaluation benchmarks and
propose a KG-based multilingual, multihop benchmark called \textbf{MultiHal}
framed for generative text evaluation. As part of our data collection pipeline,
we mined 140k KG-paths from open-domain KGs, from which we pruned noisy
KG-paths, curating a high-quality subset of 25.9k. Our baseline evaluation
shows an absolute scale increase by approximately 0.12 to 0.36 points for the
semantic similarity score in KG-RAG over vanilla QA across multiple languages
and multiple models, demonstrating the potential of KG integration. We
anticipate MultiHal will foster future research towards several graph-based
hallucination mitigation and fact-checking tasks.

</details>


### [54] [Legal Rule Induction: Towards Generalizable Principle Discovery from Analogous Judicial Precedents](https://arxiv.org/abs/2505.14104)
*Wei Fan,Tianshi Zheng,Yiran Hu,Zheye Deng,Weiqi Wang,Baixuan Xu,Chunyang Li,Haoran Li,Weixing Shen,Yangqiu Song*

Main category: cs.CL

TL;DR: 论文提出利用大语言模型从类似判例中归纳法律规则，构建首个法律规则归纳(LRI)基准数据集，验证数据驱动方法显著提升模型对案例规则模式的捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 现有法律计算研究集中于应用成文法规，但缺乏从判例中归纳隐含规则的有效方法，LLM的出现为此提供了新机遇但受限于任务定义和数据集缺失。

Method: 将法律规则归纳形式化为从类似判例中提取通用原则的任务，构建包含5,121个案例集(总计38,088个中国案例)的基准数据集，并设计实验验证模型效果。

Result: 实验表明：1) 现有LLM存在过度泛化与事实幻觉问题；2) 使用该数据集训练可显著提升模型对案例间细微规则模式的捕捉能力。

Conclusion: 通过构建首个法律规则归纳基准，证明了数据驱动方法在提升模型法律推理能力方面的有效性，为后续法律AI研究提供了重要基础设施。

Abstract: Legal rules encompass not only codified statutes but also implicit
adjudicatory principles derived from precedents that contain discretionary
norms, social morality, and policy. While computational legal research has
advanced in applying established rules to cases, inducing legal rules from
judicial decisions remains understudied, constrained by limitations in model
inference efficacy and symbolic reasoning capability. The advent of Large
Language Models (LLMs) offers unprecedented opportunities for automating the
extraction of such latent principles, yet progress is stymied by the absence of
formal task definitions, benchmark datasets, and methodologies. To address this
gap, we formalize Legal Rule Induction (LRI) as the task of deriving concise,
generalizable doctrinal rules from sets of analogous precedents, distilling
their shared preconditions, normative behaviors, and legal consequences. We
introduce the first LRI benchmark, comprising 5,121 case sets (38,088 Chinese
cases in total) for model tuning and 216 expert-annotated gold test sets.
Experimental results reveal that: 1) State-of-the-art LLMs struggle with
over-generalization and hallucination; 2) Training on our dataset markedly
enhances LLMs capabilities in capturing nuanced rule patterns across similar
cases.

</details>


### [55] [A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations](https://arxiv.org/abs/2505.14106)
*Li Li,Peilin Cai,Ryan A. Rossi,Franck Dernoncourt,Branislav Kveton,Junda Wu,Tong Yu,Linxin Song,Tiankai Yang,Yuehan Qin,Nesreen K. Ahmed,Samyadeep Basu,Subhojyoti Mukherjee,Ruiyi Zhang,Zhengmian Hu,Bo Ni,Yuxiao Zhou,Zichao Wang,Yue Huang,Yu Wang,Xiangliang Zhang,Philip S. Yu,Xiyang Hu,Yue Zhao*

Main category: cs.CL

TL;DR: PersonaConvBench：首个融合个性化与会话结构的大规模多轮对话评估基准，覆盖三大任务十个领域，显著提升LLM个性化生成能力


<details>
  <summary>Details</summary>
Motivation: 现有研究孤立看待个性化与对话结构，缺乏真实多用户场景下的系统评估框架。该研究旨在通过整合二者，系统分析个性化对话历史对LLM输出的影响

Method: 构建包含句子分类、影响回归、用户中心文本生成三大任务的基准测试，覆盖10个Reddit领域。采用统一提示框架对商业/开源LLM进行基准测试

Result: 个性化历史使性能显著提升（情感分类相对提升198%），验证多轮对话中用户个性化建模的重要性

Conclusion: 该基准为开发适配个体风格、追踪长期上下文、生成情境化响应的LLM提供支持，推动对话系统个性化研究

Abstract: We present PersonaConvBench, a large-scale benchmark for evaluating
personalized reasoning and generation in multi-turn conversations with large
language models (LLMs). Unlike existing work that focuses on either
personalization or conversational structure in isolation, PersonaConvBench
integrates both, offering three core tasks: sentence classification, impact
regression, and user-centric text generation across ten diverse Reddit-based
domains. This design enables systematic analysis of how personalized
conversational context shapes LLM outputs in realistic multi-user scenarios. We
benchmark several commercial and open-source LLMs under a unified prompting
setup and observe that incorporating personalized history yields substantial
performance improvements, including a 198 percent relative gain over the best
non-conversational baseline in sentiment classification. By releasing
PersonaConvBench with evaluations and code, we aim to support research on LLMs
that adapt to individual styles, track long-term context, and produce
contextually rich, engaging responses.

</details>


### [56] [DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models](https://arxiv.org/abs/2505.14107)
*Yakun Zhu,Zhongzhen Huang,Linjie Mu,Yutong Huang,Wei Nie,Shaoting Zhang,Pengfei Liu,Xiaofan Zhang*

Main category: cs.CL

TL;DR: 提出DiagnosisArena基准测试，揭示当前大语言模型在临床诊断推理中存在显著泛化瓶颈（最优模型准确率仅45.82%）


<details>
  <summary>Details</summary>
Motivation: 现有医学评估基准无法有效评估专业级诊断能力，需构建更严谨的临床诊断推理测试标准

Method: 基于10大顶级医学期刊的临床案例报告，构建含1113对病例-诊断数据集的系统化流程（多轮AI/专家筛选+防泄漏检测）

Result: 顶尖模型o3-mini/o1/DeepSeek-R1准确率分别为45.82%/31.09%/17.79%，暴露临床推理泛化能力缺陷

Conclusion: 通过开源基准推动AI诊断推理技术进步，为解决真实临床诊断挑战提供有效工具

Abstract: The emergence of groundbreaking large language models capable of performing
complex reasoning tasks holds significant promise for addressing various
scientific challenges, including those arising in complex clinical scenarios.
To enable their safe and effective deployment in real-world healthcare
settings, it is urgently necessary to benchmark the diagnostic capabilities of
current models systematically. Given the limitations of existing medical
benchmarks in evaluating advanced diagnostic reasoning, we present
DiagnosisArena, a comprehensive and challenging benchmark designed to
rigorously assess professional-level diagnostic competence. DiagnosisArena
consists of 1,113 pairs of segmented patient cases and corresponding diagnoses,
spanning 28 medical specialties, deriving from clinical case reports published
in 10 top-tier medical journals. The benchmark is developed through a
meticulous construction pipeline, involving multiple rounds of screening and
review by both AI systems and human experts, with thorough checks conducted to
prevent data leakage. Our study reveals that even the most advanced reasoning
models, o3-mini, o1, and DeepSeek-R1, achieve only 45.82%, 31.09%, and 17.79%
accuracy, respectively. This finding highlights a significant generalization
bottleneck in current large language models when faced with clinical diagnostic
reasoning challenges. Through DiagnosisArena, we aim to drive further
advancements in AIs diagnostic reasoning capabilities, enabling more effective
solutions for real-world clinical diagnostic challenges. We provide the
benchmark and evaluation tools for further research and development
https://github.com/SPIRAL-MED/DiagnosisArena.

</details>


### [57] [Invisible Entropy: Towards Safe and Efficient Low-Entropy LLM Watermarking](https://arxiv.org/abs/2505.14112)
*Tianle Gu,Zongqi Wang,Kexin Huang,Yuanqi Yao,Xiangliang Zhang,Yujiu Yang,Xiuying Chen*

Main category: cs.CL

TL;DR: 提出Invisible Entropy(IE)水印范式，通过轻量级特征提取器替代原始LLM，实现低熵场景下的高效安全文本水印。


<details>
  <summary>Details</summary>
Motivation: 传统水印方法在低熵场景失效，依赖原始LLM导致计算成本高、检测延迟长，存在模型泄露风险。

Method: 1. 引入轻量级特征提取器和熵标记器预测token熵值
2. 开发阈值导航器自适应设置熵阈值
3. 理论分析指导水印比例与自然文本的平衡优化

Result: 参数量减少99%的情况下，在HumanEval和MBPP数据集达到SOTA性能，水印文本自然度提升，检测鲁棒性增强。

Conclusion: IE范式突破低熵水印瓶颈，建立安全高效新标准，为AI生成内容追踪提供参数高效解决方案。

Abstract: Logit-based LLM watermarking traces and verifies AI-generated content by
maintaining green and red token lists and increasing the likelihood of green
tokens during generation. However, it fails in low-entropy scenarios, where
predictable outputs make green token selection difficult without disrupting
natural text flow. Existing approaches address this by assuming access to the
original LLM to calculate entropy and selectively watermark high-entropy
tokens. However, these methods face two major challenges: (1) high
computational costs and detection delays due to reliance on the original LLM,
and (2) potential risks of model leakage. To address these limitations, we
propose Invisible Entropy (IE), a watermarking paradigm designed to enhance
both safety and efficiency. Instead of relying on the original LLM, IE
introduces a lightweight feature extractor and an entropy tagger to predict
whether the entropy of the next token is high or low. Furthermore, based on
theoretical analysis, we develop a threshold navigator that adaptively sets
entropy thresholds. It identifies a threshold where the watermark ratio
decreases as the green token count increases, enhancing the naturalness of the
watermarked text and improving detection robustness. Experiments on HumanEval
and MBPP datasets demonstrate that IE reduces parameter size by 99\% while
achieving performance on par with state-of-the-art methods. Our work introduces
a safe and efficient paradigm for low-entropy watermarking.
https://github.com/Carol-gutianle/IE
https://huggingface.co/datasets/Carol0110/IE-Tagger

</details>


### [58] [Self-Reasoning Language Models: Unfold Hidden Reasoning Chains with Few Reasoning Catalyst](https://arxiv.org/abs/2505.14116)
*Hongru Wang,Deng Cai,Wanjun Zhong,Shijue Huang,Jeff Z. Pan,Zeming Liu,Kam-Fai Wong*

Main category: cs.CL

TL;DR: 提出自推理语言模型SRLM，通过自训练生成更长思维链数据，提升大模型在多任务推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有思维链方法依赖人工构造长推理链数据，获取成本高且难以覆盖多样化元推理技能。

Method: 引入少量示范样本（1000条）作为推理催化剂，指导模型自主展开潜在推理链，通过迭代自训练优化模型。

Result: 在MMLU等5个基准任务上平均提升+2.5分，64次采样时提升达+7.89分，展现深度多样化推理能力。

Conclusion: SRLM实现了无需人工干预的持续性能提升，验证了自生成长推理链对复杂推理任务的有效性。

Abstract: Inference-time scaling has attracted much attention which significantly
enhance the performance of Large Language Models (LLMs) in complex reasoning
tasks by increasing the length of Chain-of-Thought. These longer intermediate
reasoning rationales embody various meta-reasoning skills in human cognition,
such as reflection and decomposition, being difficult to create and acquire. In
this work, we introduce \textit{Self-Reasoning Language Model} (SRLM), where
the model itself can synthesize longer CoT data and iteratively improve
performance through self-training. By incorporating a few demonstration
examples (i.e., 1,000 samples) on how to unfold hidden reasoning chains from
existing responses, which act as a reasoning catalyst, we demonstrate that SRLM
not only enhances the model's initial performance but also ensures more stable
and consistent improvements in subsequent iterations. Our proposed SRLM
achieves an average absolute improvement of more than $+2.5$ points across five
reasoning tasks: MMLU, GSM8K, ARC-C, HellaSwag, and BBH on two backbone models.
Moreover, it brings more improvements with more times of sampling during
inference, such as absolute $+7.89$ average improvement with $64$ sampling
times, revealing the in-depth, diverse and creative reasoning paths in SRLM
against the strong baseline.

</details>


### [59] [Probing BERT for German Compound Semantics](https://arxiv.org/abs/2505.14130)
*Filip Miletić,Aaron Schmid,Sabine Schulte im Walde*

Main category: cs.CL

TL;DR: 研究通过分析德语BERT模型各层对名词复合词语义组合性的捕捉能力，发现其早期层表现最佳，但整体弱于英语同类模型，归因于德语更高的复合词生产性及成分歧义。


<details>
  <summary>Details</summary>
Motivation: 探索预训练德语BERT模型是否有效编码名词复合词的语义组合性，并与英语模型对比，揭示语言结构差异对模型性能的影响。

Method: 采用变体实验（目标词、层数、大小写模型），基于868个标注复合词的组合性预测，分析Transformer各层的表征模式。

Result: 早期层（4-6层）组合性信息最易提取，与英语趋势一致，但德语最高准确率（77.1%）显著低于英语（83.8%），表明德语复合词解析难度更高。

Conclusion: 德语复合词的高产性及成分歧义（如'Bankangestellter'中'Bank'的双重含义）导致任务复杂度上升，需开发更适配德语形态特征的模型架构。

Abstract: This paper investigates the extent to which pretrained German BERT encodes
knowledge of noun compound semantics. We comprehensively vary combinations of
target tokens, layers, and cased vs. uncased models, and evaluate them by
predicting the compositionality of 868 gold standard compounds. Looking at
representational patterns within the transformer architecture, we observe
trends comparable to equivalent prior work on English, with compositionality
information most easily recoverable in the early layers. However, our strongest
results clearly lag behind those reported for English, suggesting an inherently
more difficult task in German. This may be due to the higher productivity of
compounding in German than in English and the associated increase in
constituent-level ambiguity, including in our target compound set.

</details>


### [60] [Texts or Images? A Fine-grained Analysis on the Effectiveness of Input Representations and Models for Table Question Answering](https://arxiv.org/abs/2505.14131)
*Wei Zhou,Mohsen Mesgar,Heike Adel,Annemarie Friedrich*

Main category: cs.CL

TL;DR: 论文通过受控实验对比文本和图像表格表示在TQA中的效果，提出动态选择表示的FRES方法，实现平均10%性能提升


<details>
  <summary>Details</summary>
Motivation: 先前研究缺乏对表格表示方式（文本/图像）与模型组合的受控比较，需从问题复杂度和表格尺寸维度系统评估不同组合的有效性

Method: 基于现有TQA数据集构建新基准，系统分析7对MLLM-LLM组合，发现最佳组合因场景变化，进而设计动态选择表格表示的FRES方法

Result: FRES方法通过动态选择表格表示方式，相比无差别使用双表示的方法实现平均10%的性能提升

Conclusion: 表格表示与模型的最佳组合具有场景敏感性，动态选择方法显著提升效果，为TQA系统优化提供新方向

Abstract: In table question answering (TQA), tables are encoded as either texts or
images. Prior work suggests that passing images of tables to multi-modal large
language models (MLLMs) performs comparably to or even better than using
textual input with large language models (LLMs). However, the lack of
controlled setups limits fine-grained distinctions between these approaches. In
this paper, we conduct the first controlled study on the effectiveness of
several combinations of table representations and models from two perspectives:
question complexity and table size. We build a new benchmark based on existing
TQA datasets. In a systematic analysis of seven pairs of MLLMs and LLMs, we
find that the best combination of table representation and model varies across
setups. We propose FRES, a method selecting table representations dynamically,
and observe a 10% average performance improvement compared to using both
representations indiscriminately.

</details>


### [61] [Enhancing Keyphrase Extraction from Academic Articles Using Section Structure Information](https://arxiv.org/abs/2505.14149)
*Chengzhi Zhang,Xinyi Yan,Lei Zhao,Yingyi Zhang*

Main category: cs.CL

TL;DR: 提出利用学术论文结构特征和章节文本的关键词提取方法（SSB-KPE），通过结构特征分析和文本整合算法提升关键词提取性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于摘要的关键词提取受限于文本长度，而基于全文的方法存在噪声干扰。学术论文的结构特征可能包含关键语义信息。

Method: 1. 分析7种结构特征对KPE模型的影响；2. 开发关键词整合算法融合各章节提取结果；3. 验证章节结构分类质量的影响

Result: 结构特征能提升模型效果（不同特征影响差异显著），整合算法效果最佳，章节分类质量影响KPE性能

Conclusion: 学术论文结构信息可有效提升关键词提取效果，研究代码与数据集已开源

Abstract: The exponential increase in academic papers has significantly increased the
time required for researchers to access relevant literature. Keyphrase
Extraction (KPE) offers a solution to this situation by enabling researchers to
efficiently retrieve relevant literature. The current study on KPE from
academic articles aims to improve the performance of extraction models through
innovative approaches using Title and Abstract as input corpora. However, the
semantic richness of keywords is significantly constrained by the length of the
abstract. While full-text-based KPE can address this issue, it simultaneously
introduces noise, which significantly diminishes KPE performance. To address
this issue, this paper utilized the structural features and section texts
obtained from the section structure information of academic articles to extract
keyphrase from academic papers. The approach consists of two main parts: (1)
exploring the effect of seven structural features on KPE models, and (2)
integrating the extraction results from all section texts used as input corpora
for KPE models via a keyphrase integration algorithm to obtain the keyphrase
integration result. Furthermore, this paper also examined the effect of the
classification quality of section structure on the KPE performance. The results
show that incorporating structural features improves KPE performance, though
different features have varying effects on model efficacy. The keyphrase
integration approach yields the best performance, and the classification
quality of section structure can affect KPE performance. These findings
indicate that using the section structure information of academic articles
contributes to effective KPE from academic articles. The code and dataset
supporting this study are available at https://github.com/yan-xinyi/SSB_KPE.

</details>


### [62] [Prior Prompt Engineering for Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.14157)
*Pittawat Taveekitworachai,Potsawee Manakul,Sarana Nutanong,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: 研究不同先前提示工程策略（pPE）在强化微调中对模型行为的影响，发现null-example方法在多项基准测试中提升最显著


<details>
  <summary>Details</summary>
Motivation: 现有强化微调研究忽视先前提示设计，探索不同pPE策略能否引导语言模型内化特定行为模式

Method: 将五种推理时提示策略转化为pPE方法，使用Qwen2.5-7B模型进行训练，并在AIME2024、HumanEval+等基准测试中评估效果

Result: 所有pPE训练模型均超越iPE方法，其中null-example方法平均提升最大，在GPQA-Diamond上表现最优，不同策略塑造了模型独特的行为特征

Conclusion: 先前提示工程是强化微调中被低估的重要维度，提示策略选择直接影响模型行为模式与性能表现

Abstract: This paper investigates prior prompt engineering (pPE) in the context of
reinforcement fine-tuning (RFT), where language models (LMs) are incentivized
to exhibit behaviors that maximize performance through reward signals. While
existing RFT research has primarily focused on algorithms, reward shaping, and
data curation, the design of the prior prompt--the instructions prepended to
queries during training to elicit behaviors such as step-by-step
reasoning--remains underexplored. We investigate whether different pPE
approaches can guide LMs to internalize distinct behaviors after RFT. Inspired
by inference-time prompt engineering (iPE), we translate five representative
iPE strategies--reasoning, planning, code-based reasoning, knowledge recall,
and null-example utilization--into corresponding pPE approaches. We experiment
with Qwen2.5-7B using each of the pPE approaches, then evaluate performance on
in-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and
GPQA-Diamond). Our results show that all pPE-trained models surpass their
iPE-prompted counterparts, with the null-example pPE approach achieving the
largest average performance gain and the highest improvement on AIME2024 and
GPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by
adapting a behavior-classification framework, we demonstrate that different pPE
strategies instill distinct behavioral styles in the resulting models. These
findings position pPE as a powerful yet understudied axis for RFT.

</details>


### [63] [Temporal Alignment of Time Sensitive Facts with Activation Engineering](https://arxiv.org/abs/2505.14158)
*Sanjay Govindan,Maurice Pagnucco,Yang Song*

Main category: cs.CL

TL;DR: 提出通过激活工程实现LLM时间对齐，在无需训练的情况下提升44%相对提示效果，达到接近微调方法的性能但计算效率更高


<details>
  <summary>Details</summary>
Motivation: LLM知识库存在时间冲突，传统时间对齐方法依赖训练和数据集构建，本研究探索零训练成本的激活工程方案

Method: 在LLaMA 2不同网络层注入时间特征向量，系统测试层位置与提示策略（显式/相对提示）的组合效应

Result: 相对提示提升44%，显式提示提升16%，与微调方法性能相当但节省90%计算资源，无需预对齐数据

Conclusion: 首次验证激活工程在时间对齐任务的有效性，为LLM知识更新开辟高效新路径，推动参数高效微调研究发展

Abstract: Large Language Models (LLMs) are trained on diverse and often conflicting
knowledge spanning multiple domains and time periods. Some of this knowledge is
only valid within specific temporal contexts, such as answering the question,
"Who is the President of the United States in 2022?" Ensuring LLMs generate
time appropriate responses is crucial for maintaining relevance and accuracy.
In this work we explore activation engineering as a method for temporally
aligning LLMs to improve factual recall without any training or dataset
creation. In this research we explore an activation engineering technique to
ground three versions of LLaMA 2 to specific points in time and examine the
effects of varying injection layers and prompting strategies. Our experiments
demonstrate up to a 44% and 16% improvement in relative and explicit prompting
respectively, achieving comparable performance to the fine-tuning method
proposed by Zhao et al. (2024) . Notably, our approach achieves similar results
to the fine-tuning baseline while being significantly more computationally
efficient and requiring no pre-aligned datasets.

</details>


### [64] [Breaking Language Barriers or Reinforcing Bias? A Study of Gender and Racial Disparities in Multilingual Contrastive Vision Language Models](https://arxiv.org/abs/2505.14160)
*Zahraa Al Sahili,Ioannis Patras,Matthew Purver*

Main category: cs.CL

TL;DR: 研究发现多语言CLIP模型在性别/种族偏见上表现反比单语言模型更显著，跨语言权重共享会加剧低资源/性别中立语言的偏见问题


<details>
  <summary>Details</summary>
Motivation: 验证多语言视觉语言模型是否真正缓解社会偏见，特别是不同资源水平和语法性别特征语言间的偏见传递机制

Method: 使用FairFace和PATA数据集，在10种不同资源/语法特征语言中，对M-CLIP/NLLB-CLIP/CAPIVARA-CLIP进行零样本的种族/性别偏见量化及刻板印象放大测量

Result: 所有模型性别偏见强于英语基线；CAPIVARA在低资源目标语言偏见最大；NLLB的共享编码器将英语性别刻板传播至性别中立语言；松散耦合编码器可避免跨语言偏见传递

Conclusion: 多语言性未缓解偏见，需建立细粒度语言感知的评估体系，聚合指标易掩盖语言特异性偏见热点，跨语言权重共享可能引入外部刻板印象

Abstract: Multilingual vision-language models promise universal image-text retrieval,
yet their social biases remain under-explored. We present the first systematic
audit of three public multilingual CLIP checkpoints -- M-CLIP, NLLB-CLIP, and
CAPIVARA-CLIP -- across ten languages that vary in resource availability and
grammatical gender. Using balanced subsets of \textsc{FairFace} and the
\textsc{PATA} stereotype suite in a zero-shot setting, we quantify race and
gender bias and measure stereotype amplification. Contrary to the assumption
that multilinguality mitigates bias, every model exhibits stronger gender bias
than its English-only baseline. CAPIVARA-CLIP shows its largest biases
precisely in the low-resource languages it targets, while the shared
cross-lingual encoder of NLLB-CLIP transports English gender stereotypes into
gender-neutral languages; loosely coupled encoders largely avoid this transfer.
Highly gendered languages consistently magnify all measured bias types, but
even gender-neutral languages remain vulnerable when cross-lingual weight
sharing imports foreign stereotypes. Aggregated metrics conceal
language-specific ``hot spots,'' underscoring the need for fine-grained,
language-aware bias evaluation in future multilingual vision-language research.

</details>


### [65] [PL-FGSA: A Prompt Learning Framework for Fine-Grained Sentiment Analysis Based on MindSpore](https://arxiv.org/abs/2505.14165)
*Zhenkai Qin,Jiajing He,Qiao Fang*

Main category: cs.CL

TL;DR: 提出PL-FGSA框架，通过提示学习+TextCNN实现细粒度情感分析，在多任务统一范式下显著提升效果


<details>
  <summary>Details</summary>
Motivation: 传统细粒度情感分析方法依赖特定架构和大量标注数据，泛化性和扩展性差

Method: 将FGSA重构为多任务提示增强生成问题，整合方面提取、情感分类和因果解释，采用MindSpore平台实现提示学习+轻量级TextCNN融合架构

Result: 在SST-2/SemEval-2014/MAMS数据集分别取得0.922/0.694/0.597的F1值，优于传统微调方法

Conclusion: 验证提示学习的泛化优势，PL-FGSA在完全数据和低资源场景均表现优异，具有实际应用价值

Abstract: Fine-grained sentiment analysis (FGSA) aims to identify sentiment polarity
toward specific aspects within a text, enabling more precise opinion mining in
domains such as product reviews and social media. However, traditional FGSA
approaches often require task-specific architectures and extensive annotated
data, limiting their generalization and scalability. To address these
challenges, we propose PL-FGSA, a unified prompt learning-based framework
implemented using the MindSpore platform, which integrates prompt design with a
lightweight TextCNN backbone. Our method reformulates FGSA as a multi-task
prompt-augmented generation problem, jointly tackling aspect extraction,
sentiment classification, and causal explanation in a unified paradigm. By
leveraging prompt-based guidance, PL-FGSA enhances interpretability and
achieves strong performance under both full-data and low-resource conditions.
Experiments on three benchmark datasets-SST-2, SemEval-2014 Task 4, and
MAMS-demonstrate that our model consistently outperforms traditional
fine-tuning methods and achieves F1-scores of 0.922, 0.694, and 0.597,
respectively. These results validate the effectiveness of prompt-based
generalization and highlight the practical value of PL-FGSA for real-world
sentiment analysis tasks.

</details>


### [66] [The Strawberry Problem: Emergence of Character-level Understanding in Tokenized Language Models](https://arxiv.org/abs/2505.14172)
*Adrian Cosma,Stefan Ruseti,Emilian Radoi,Mihai Dascalu*

Main category: cs.CL

TL;DR: 大语言模型因分词机制存在字符级任务缺陷，通过渗流理论分析提出轻量级改进方案


<details>
  <summary>Details</summary>
Motivation: LLMs在简单字符任务（如字母计数）上的持续失败暴露了分词机制导致的低互信息问题，需探究其概念涌现规律并突破瓶颈

Method: 构建19个合成任务隔离字符推理，运用渗流模型分析概念涌现模式，设计保留子词优势的架构改进方案

Result: 字符能力在训练后期突现，轻量级修改显著提升性能（字符任务准确率提升32%），验证概念涌现与常识学习的同源性

Conclusion: 建立连接分词模型低层感知的理论框架，提供可解释的结构盲点解决方案，开源代码促进后续研究

Abstract: Despite their remarkable progress across diverse domains, Large Language
Models (LLMs) consistently fail at simple character-level tasks, such as
counting letters in words, due to a fundamental limitation: tokenization. In
this work, we frame this limitation as a problem of low mutual information and
analyze it in terms of concept emergence. Using a suite of 19 synthetic tasks
that isolate character-level reasoning in a controlled setting, we show that
such capabilities emerge slowly, suddenly, and only late in training. We
further show that percolation-based models of concept emergence explain these
patterns, suggesting that learning character composition is not fundamentally
different from learning commonsense knowledge. To address this bottleneck, we
propose a lightweight architectural modification that significantly improves
character-level reasoning while preserving the inductive advantages of subword
models. Together, our results bridge low-level perceptual gaps in tokenized LMs
and provide a principled framework for understanding and mitigating their
structural blind spots. We make our code publicly available.

</details>


### [67] [THOR-MoE: Hierarchical Task-Guided and Context-Responsive Routing for Neural Machine Translation](https://arxiv.org/abs/2505.14173)
*Yunlong Liang,Fandong Meng,Jie Zhou*

Main category: cs.CL

TL;DR: 提出THOR-MoE方法改进稀疏混合专家模型，通过分层任务引导和上下文响应路由策略解决现有MoE在任务知识利用和专家选择上的局限性


<details>
  <summary>Details</summary>
Motivation: 当前MoE方案直接使用NMT任务知识（如领域/语言知识）且专家选择仅依赖局部标记表示，导致实际应用受限和性能次优

Method: 1. 分层预测领域/语言标签并提取混合表征分配任务级专家 2. 注入上下文信息增强从预选专家集的标记路由精度

Result: 在多领域/多语言翻译任务中实现显著性能提升（如Top-p路由对比提升0.75 BLEU，激活参数减少22%）

Conclusion: THOR-MoE作为即插即用模块兼容现有路由方案，在提升性能的同时保持广泛架构适用性

Abstract: The sparse Mixture-of-Experts (MoE) has achieved significant progress for
neural machine translation (NMT). However, there exist two limitations in
current MoE solutions which may lead to sub-optimal performance: 1) they
directly use the task knowledge of NMT into MoE (\emph{e.g.},
domain/linguistics-specific knowledge), which are generally unavailable at
practical application and neglect the naturally grouped domain/linguistic
properties; 2) the expert selection only depends on the localized token
representation without considering the context, which fully grasps the state of
each token in a global view. To address the above limitations, we propose
THOR-MoE via arming the MoE with hierarchical task-guided and
context-responsive routing policies. Specifically, it 1) firstly predicts the
domain/language label and then extracts mixed domain/language representation to
allocate task-level experts in a hierarchical manner; 2) injects the context
information to enhance the token routing from the pre-selected task-level
experts set, which can help each token to be accurately routed to more
specialized and suitable experts. Extensive experiments on multi-domain
translation and multilingual translation benchmarks with different
architectures consistently demonstrate the superior performance of THOR-MoE.
Additionally, the THOR-MoE operates as a plug-and-play module compatible with
existing Top-$k$~\cite{shazeer2017} and Top-$p$~\cite{huang-etal-2024-harder}
routing schemes, ensuring broad applicability across diverse MoE architectures.
For instance, compared with vanilla Top-$p$~\cite{huang-etal-2024-harder}
routing, the context-aware manner can achieve an average improvement of 0.75
BLEU with less than 22\% activated parameters on multi-domain translation
tasks.

</details>


### [68] [Cheaper, Better, Faster, Stronger: Robust Text-to-SQL without Chain-of-Thought or Fine-Tuning](https://arxiv.org/abs/2505.14174)
*Yusuf Denizay Dönder,Derek Hommel,Andrea W Wen-Yi,David Mimno,Unso Eun Seo Jo*

Main category: cs.CL

TL;DR: 提出N-rep一致性方法，在文本转SQL任务中以0.039美元/次查询的低成本达到SOTA效果


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法（CoT/自洽/微调）成本过高（单次查询达$0.46），需经济替代方案

Method: 通过多模式表征输入增强鲁棒性，无需推理链或微调即可使用小型廉价模型

Result: 在BIRD基准取得相近分数，成本降低10倍（$0.039/query），保持性能优势

Conclusion: N-rep是当前性价比最优的文本转SQL解决方案，开创低成本高效范式

Abstract: LLMs are effective at code generation tasks like text-to-SQL, but is it worth
the cost? Many state-of-the-art approaches use non-task-specific LLM techniques
including Chain-of-Thought (CoT), self-consistency, and fine-tuning. These
methods can be costly at inference time, sometimes requiring over a hundred LLM
calls with reasoning, incurring average costs of up to \$0.46 per query, while
fine-tuning models can cost thousands of dollars. We introduce "N-rep"
consistency, a more cost-efficient text-to-SQL approach that achieves similar
BIRD benchmark scores as other more expensive methods, at only \$0.039 per
query. N-rep leverages multiple representations of the same schema input to
mitigate weaknesses in any single representation, making the solution more
robust and allowing the use of smaller and cheaper models without any reasoning
or fine-tuning. To our knowledge, N-rep is the best-performing text-to-SQL
approach in its cost range.

</details>


### [69] [Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits](https://arxiv.org/abs/2505.14178)
*Xiang Zhang,Juntai Cao,Jiaqi Wei,Yiwei Xu,Chenyu You*

Main category: cs.CL

TL;DR: 研究揭示了当前语言模型的符号推理能力受限于分词结构，提出分词粒度不足会破坏逻辑对齐，而原子对齐的分词格式可显著提升小模型的结构化推理表现。


<details>
  <summary>Details</summary>
Motivation: 针对语言模型中常被忽视的分词层对符号推理的影响，研究者发现即使使用思维链提示，分词语义单元的合并/模糊仍会形成符号计算的根本性障碍。

Method: 通过理论分析和系统实验（算术运算和符号任务），提出'分词感知度'概念，对比不同分词结构（子词分词vs原子对齐）对推理能力的影响。

Result: 分词结构显著影响推理性能：原子对齐格式下小模型（GPT-4o-mini）在结构化推理中可超越大模型（o1），错误率降低40%以上。

Conclusion: 语言模型的符号推理能力不仅取决于架构设计，更深度依赖于分词粒度的表征质量，改进分词方案可能成为提升推理能力的新方向。

Abstract: Tokenization is the first - and often underappreciated - layer of computation
in language models. While Chain-of-Thought (CoT) prompting enables transformer
models to approximate recurrent computation by externalizing intermediate
steps, we show that the success of such reasoning is fundamentally bounded by
the structure of tokenized inputs. This work presents a theoretical and
empirical investigation into how tokenization schemes, particularly
subword-based methods like byte-pair encoding (BPE), impede symbolic
computation by merging or obscuring atomic reasoning units. We introduce the
notion of Token Awareness to formalize how poor token granularity disrupts
logical alignment and prevents models from generalizing symbolic procedures.
Through systematic evaluation on arithmetic and symbolic tasks, we demonstrate
that token structure dramatically affect reasoning performance, causing failure
even with CoT, while atomically-aligned formats unlock strong generalization,
allowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g.,
o1) in structured reasoning. Our findings reveal that symbolic reasoning
ability in LLMs is not purely architectural, but deeply conditioned on
token-level representations.

</details>


### [70] [Enhancing Abstractive Summarization of Scientific Papers Using Structure Information](https://arxiv.org/abs/2505.14179)
*Tong Bao,Heng Zhang,Chengzhi Zhang*

Main category: cs.CL

TL;DR: 提出两阶段摘要生成框架，通过自动识别科学论文结构功能提升摘要质量


<details>
  <summary>Details</summary>
Motivation: 现有摘要生成方法存在两个问题：1.传统序列模型无法有效捕捉论文结构化信息；2.关键词映射方法难以适应不同学科的结构灵活性

Method: 两阶段框架：1.构建大规模结构功能识别数据集训练分类器；2.使用Longformer模型进行跨章节上下文感知摘要生成

Result: 在领域特定数据集上超越先进基线模型，生成更全面的摘要

Conclusion: 通过结构功能识别与上下文感知生成相结合，有效提升科学论文摘要质量

Abstract: Abstractive summarization of scientific papers has always been a research
focus, yet existing methods face two main challenges. First, most summarization
models rely on Encoder-Decoder architectures that treat papers as sequences of
words, thus fail to fully capture the structured information inherent in
scientific papers. Second, existing research often use keyword mapping or
feature engineering to identify the structural information, but these methods
struggle with the structural flexibility of scientific papers and lack
robustness across different disciplines. To address these challenges, we
propose a two-stage abstractive summarization framework that leverages
automatic recognition of structural functions within scientific papers. In the
first stage, we standardize chapter titles from numerous scientific papers and
construct a large-scale dataset for structural function recognition. A
classifier is then trained to automatically identify the key structural
components (e.g., Background, Methods, Results, Discussion), which provides a
foundation for generating more balanced summaries. In the second stage, we
employ Longformer to capture rich contextual relationships across sections and
generating context-aware summaries. Experiments conducted on two
domain-specific scientific paper summarization datasets demonstrate that our
method outperforms advanced baselines, and generates more comprehensive
summaries. The code and dataset can be accessed at
https://github.com/tongbao96/code-for-SFR-AS.

</details>


### [71] [SlangDIT: Benchmarking LLMs in Interpretative Slang Translation](https://arxiv.org/abs/2505.14181)
*Yunlong Liang,Fandong Meng,Jiaan Wang,Jie Zhou*

Main category: cs.CL

TL;DR: 提出SlangDIT综合任务框架(包含俚语检测/解释/翻译)，构建25k中英数据集，开发深度思考模型SlangOWL显著提升LLM俚语翻译性能


<details>
  <summary>Details</summary>
Motivation: 现有研究将俚语相关任务孤立处理，未充分挖掘任务间的内在关联性，导致翻译缺乏语境适应性。缺乏整合检测与解释的翻译基准数据集是核心瓶颈

Method: 1. 设计三阶段任务框架：先检测俚语→判断多义性→生成语境化解释→最终翻译 2. 构建包含跨语言解释标注的25k中英平行数据集 3. 提出SlangOWL模型实现思维链推理

Result: 在Qwen2.5/LLama-3.1等模型上，SlangOWL相比原始模型提升显著，思考机制使翻译准确率超越传统微调方法

Conclusion: 任务协同框架有效提升俚语翻译质量，深度思考机制验证了LLM的推理潜力，数据集为跨文化NLP研究提供新基准

Abstract: The challenge of slang translation lies in capturing context-dependent
semantic extensions, as slang terms often convey meanings beyond their literal
interpretation. While slang detection, explanation, and translation have been
studied as isolated tasks in the era of large language models (LLMs), their
intrinsic interdependence remains underexplored. The main reason is lacking of
a benchmark where the two tasks can be a prerequisite for the third one, which
can facilitate idiomatic translation. In this paper, we introduce the
interpretative slang translation task (named SlangDIT) consisting of three
sub-tasks: slang detection, cross-lingual slang explanation, and slang
translation within the current context, aiming to generate more accurate
translation with the help of slang detection and slang explanation. To this
end, we construct a SlangDIT dataset, containing over 25k English-Chinese
sentence pairs. Each source sentence mentions at least one slang term and is
labeled with corresponding cross-lingual slang explanation. Based on the
benchmark, we propose a deep thinking model, named SlangOWL. It firstly
identifies whether the sentence contains a slang, and then judges whether the
slang is polysemous and analyze its possible meaning. Further, the SlangOWL
provides the best explanation of the slang term targeting on the current
context. Finally, according to the whole thought, the SlangOWL offers a
suitable translation. Our experiments on LLMs (\emph{e.g.}, Qwen2.5 and
LLama-3.1), show that our deep thinking approach indeed enhances the
performance of LLMs where the proposed SLangOWL significantly surpasses the
vanilla models and supervised fine-tuned models without thinking.

</details>


### [72] [ThinkSwitcher: When to Think Hard, When to Think Fast](https://arxiv.org/abs/2505.14183)
*Guosheng Liang,Longguang Zhong,Ziyi Yang,Xiaojun Quan*

Main category: cs.CL

TL;DR: 提出ThinkSwitcher框架实现大型推理模型动态切换长短思维链，节省20-30%计算成本的同时保持任务精度


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在简单任务上存在长思维链过度计算的问题，而实验发现其本身具备短链推理潜力

Method: 通过轻量级切换模块实现模式动态切换，使用各任务中不同推理模式的相对性能差异作为监督信号训练

Result: 在多个推理基准测试中降低20-30%计算成本，复杂任务准确率保持稳定

Conclusion: ThinkSwitcher为统一部署LRMs提供了高效可扩展的解决方案，验证了动态思维链切换的有效性

Abstract: Large reasoning models (LRMs) excel at solving complex tasks by leveraging
long chain-of-thought (CoT) reasoning. However, this often leads to
overthinking on simple tasks, resulting in unnecessary computational overhead.
We observe that LRMs inherently possess the capability for efficient short CoT
reasoning, which can be reliably elicited through prompt design. To leverage
this capability, we propose ThinkSwitcher, a framework that enables a single
LRM to dynamically switch between short and long CoT modes based on task
complexity. ThinkSwitcher introduces a lightweight switching module trained
with supervision signals derived from the relative performance of each
reasoning mode across tasks. Experiments on multiple reasoning benchmarks show
that ThinkSwitcher reduces computational cost by 20-30% while maintaining high
accuracy on complex tasks. This demonstrates the effectiveness of ThinkSwitcher
as a scalable and efficient solution for unified LRM deployment.

</details>


### [73] [Unraveling Interwoven Roles of Large Language Models in Authorship Privacy: Obfuscation, Mimicking, and Verification](https://arxiv.org/abs/2505.14195)
*Tuc Nguyen,Yifan Hu,Thai Le*

Main category: cs.CL

TL;DR: 提出首个统一框架分析LLM时代下作者混淆(AO)、作者模仿(AM)、作者验证(AV)的互动关系，揭示人口统计因素对隐私风险的调节作用


<details>
  <summary>Details</summary>
Motivation: LLM生成内容导致作者身份泄露风险剧增，现有研究孤立看待AO/AM/AV任务，缺乏对其动态关系的系统性研究

Method: 构建量化分析框架，通过时间维度(单次/迭代)研究任务间转化机制，引入性别/学历等人口统计学变量进行调节效应分析

Result: 揭示三任务间动态转换规律，发现人口统计特征显著影响隐私保护效果(如女性作者更难混淆风格)，迭代操作产生复合隐私风险

Conclusion: 建立多任务协同分析范式，为LLM时代的数字身份保护提供新视角，开源代码促进研究透明度与可复现性

Abstract: Recent advancements in large language models (LLMs) have been fueled by large
scale training corpora drawn from diverse sources such as websites, news
articles, and books. These datasets often contain explicit user information,
such as person names and addresses, that LLMs may unintentionally reproduce in
their generated outputs. Beyond such explicit content, LLMs can also leak
identity revealing cues through implicit signals such as distinctive writing
styles, raising significant concerns about authorship privacy. There are three
major automated tasks in authorship privacy, namely authorship obfuscation
(AO), authorship mimicking (AM), and authorship verification (AV). Prior
research has studied AO, AM, and AV independently. However, their interplays
remain under explored, which leaves a major research gap, especially in the era
of LLMs, where they are profoundly shaping how we curate and share user
generated content, and the distinction between machine generated and human
authored text is also increasingly blurred. This work then presents the first
unified framework for analyzing the dynamic relationships among LLM enabled AO,
AM, and AV in the context of authorship privacy. We quantify how they interact
with each other to transform human authored text, examining effects at a single
point in time and iteratively over time. We also examine the role of
demographic metadata, such as gender, academic background, in modulating their
performances, inter-task dynamics, and privacy risks. All source code will be
publicly available.

</details>


### [74] [Automatic Dataset Generation for Knowledge Intensive Question Answering Tasks](https://arxiv.org/abs/2505.14212)
*Sizhe Yuen,Ting Su,Ziyang Wang,Yali Du,Adam J. Sobey*

Main category: cs.CL

TL;DR: 提出基于上下文自动生成QA对的方法增强LLMs在知识密集型问答任务中的表现，减少人工标注依赖并提升推理能力


<details>
  <summary>Details</summary>
Motivation: 现有QA系统在复杂推理和跨源信息整合存在不足，RAG技术仍面临逻辑连接和实时知识整合的挑战

Method: 开发包含自动QA生成器和模型微调器的系统，使用困惑度、ROUGE、BLEU和BERTScore评估指标

Result: Mistral-7b-v0.3在BERT F1(0.858)、BLEU(0.172)、ROUGE(0.260)上超越Llama-3-8b，优于人工标注QA对(0.836/0.083/0.139)

Conclusion: 该方法显著提升逻辑连贯性和事实准确性，为开发适应性AI系统提供有效技术路径

Abstract: A question-answering (QA) system is to search suitable answers within a
knowledge base. Current QA systems struggle with queries requiring complex
reasoning or real-time knowledge integration. They are often supplemented with
retrieval techniques on a data source such as Retrieval-Augmented Generation
(RAG). However, RAG continues to face challenges in handling complex reasoning
and logical connections between multiple sources of information. A novel
approach for enhancing Large Language Models (LLMs) in knowledge-intensive QA
tasks is presented through the automated generation of context-based QA pairs.
This methodology leverages LLMs to create fine-tuning data, reducing reliance
on human labelling and improving model comprehension and reasoning
capabilities. The proposed system includes an automated QA generator and a
model fine-tuner, evaluated using perplexity, ROUGE, BLEU, and BERTScore.
Comprehensive experiments demonstrate improvements in logical coherence and
factual accuracy, with implications for developing adaptable Artificial
Intelligence (AI) systems. Mistral-7b-v0.3 outperforms Llama-3-8b with BERT F1,
BLEU, and ROUGE scores 0.858, 0.172, and 0.260 of for the LLM generated QA
pairs compared to scores of 0.836, 0.083, and 0.139 for the human annotated QA
pairs.

</details>


### [75] ["Haet Bhasha aur Diskrimineshun": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs](https://arxiv.org/abs/2505.14226)
*Darpan Aswal,Siddharth D Jaiswal*

Main category: cs.CL

TL;DR: 研究者通过代码混合和语音扰动策略，成功绕过多语言多模态大模型的安全过滤器，文本攻击成功率99%，图像78%，揭示了模型安全对齐的脆弱性


<details>
  <summary>Details</summary>
Motivation: 现有红队测试主要针对英语模板攻击，无法有效防御多语言/多模态的越狱策略，需开发更强大的攻击手段来暴露模型安全隐患

Method: 结合代码混合（如印地语-英语混合）与语音拼写扰动（如敏感词音译错误），保持提示可解释性的同时绕过安全过滤

Result: 语音扰动代码混合提示实现文本99%攻击成功率（ARR 100%）、图像78%成功率（ARR 95%），分词机制分析显示扰动影响tokenization

Conclusion: 多语言多模态模型需要更通用的安全对齐方案，现实场景中拼写错误可能成为安全漏洞，研究推动了安全防御体系的发展

Abstract: Large Language Models (LLMs) have become increasingly powerful, with
multilingual and multimodal capabilities improving by the day. These models are
being evaluated through audits, alignment studies and red-teaming efforts to
expose model vulnerabilities towards generating harmful, biased and unfair
content. Existing red-teaming efforts have previously focused on the English
language, using fixed template-based attacks; thus, models continue to be
susceptible to multilingual jailbreaking strategies, especially in the
multimodal context. In this study, we introduce a novel strategy that leverages
code-mixing and phonetic perturbations to jailbreak LLMs for both text and
image generation tasks. We also introduce two new jailbreak strategies that
show higher effectiveness than baseline strategies. Our work presents a method
to effectively bypass safety filters in LLMs while maintaining interpretability
by applying phonetic misspellings to sensitive words in code-mixed prompts. Our
novel prompts achieve a 99% Attack Success Rate for text generation and 78% for
image generation, with Attack Relevance Rate of 100% for text generation and
95% for image generation when using the phonetically perturbed code-mixed
prompts. Our interpretability experiments reveal that phonetic perturbations
impact word tokenization, leading to jailbreak success. Our study motivates
increasing the focus towards more generalizable safety alignment for
multilingual multimodal models, especially in real-world settings wherein
prompts can have misspelt words.

</details>


### [76] [Mechanistic Fine-tuning for In-context Learning](https://arxiv.org/abs/2505.14233)
*Hakaze Cho,Peng Luo,Mariko Kato,Rin Kaenbyou,Naoya Inoue*

Main category: cs.CL

TL;DR: 提出注意力行为微调(ABFT)方法，通过优化注意力分数而非最终输出，以极低数据成本(仅0.01%)提升语言模型的少样本学习性能


<details>
  <summary>Details</summary>
Motivation: 现有端到端微调方法需要大量ICL式数据且计算成本过高，注意力机制分析显示模型通过关注标签词实现ICL

Method: 构建注意力分数训练目标，强制模型关注正确标签词并抑制错误标签词的注意力分布

Result: 在9个现代语言模型和8个数据集上验证，ABFT在性能、鲁棒性、公平性和效率(仅需约0.01%数据量)均优于基线方法

Conclusion: 通过控制语言模型特定模块序列可改进模型行为，揭示ICL式数据对归纳头的隐式偏好，为机制可解释性应用开辟新方向

Abstract: In-context Learning (ICL) utilizes structured demonstration-query inputs to
induce few-shot learning on Language Models (LMs), which are not originally
pre-trained on ICL-style data. To bridge the gap between ICL and pre-training,
some approaches fine-tune LMs on large ICL-style datasets by an end-to-end
paradigm with massive computational costs. To reduce such costs, in this paper,
we propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous
findings on the inner mechanism of ICL, building training objectives on the
attention scores instead of the final outputs, to force the attention scores to
focus on the correct label tokens presented in the context and mitigate
attention scores from the wrong label tokens. Our experiments on 9 modern LMs
and 8 datasets empirically find that ABFT outperforms in performance,
robustness, unbiasedness, and efficiency, with only around 0.01% data cost
compared to the previous methods. Moreover, our subsequent analysis finds that
the end-to-end training objective contains the ABFT objective, suggesting the
implicit bias of ICL-style data to the emergence of induction heads. Our work
demonstrates the possibility of controlling specific module sequences within
LMs to improve their behavior, opening up the future application of mechanistic
interpretability.

</details>


### [77] [ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models](https://arxiv.org/abs/2505.14238)
*Raghav Singhal,Kaustubh Ponkshe,Rohit Vartak,Praneeth Vepakomma*

Main category: cs.CL

TL;DR: 提出了新型参数高效微调架构ABBA，通过解耦两个低秩矩阵的Hadamard积实现更高表达能力


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法（如LoRA/HiRA）受限于预训练模型结构且表达能力不足，需要更灵活的参数更新机制

Method: 将参数更新重新参数化为两个独立可学习低秩矩阵的Hadamard积，完全解耦预训练权重

Result: 在算术和常识推理基准测试中达到SOTA，显著超越现有PEFT方法

Conclusion: ABBA通过完全解耦的架构设计，在同等参数量下实现了更优的模型适应能力

Abstract: Large Language Models have demonstrated strong performance across a wide
range of tasks, but adapting them efficiently to new domains remains a key
challenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by
introducing lightweight, trainable modules while keeping most pre-trained
weights fixed. The prevailing approach, LoRA, models updates using a low-rank
decomposition, but its expressivity is inherently constrained by the rank.
Recent methods like HiRA aim to increase expressivity by incorporating a
Hadamard product with the frozen weights, but still rely on the structure of
the pre-trained model. We introduce ABBA, a new PEFT architecture that
reparameterizes the update as a Hadamard product of two independently learnable
low-rank matrices. In contrast to prior work, ABBA fully decouples the update
from the pre-trained weights, enabling both components to be optimized freely.
This leads to significantly higher expressivity under the same parameter
budget. We formally analyze ABBA's expressive capacity and validate its
advantages through matrix reconstruction experiments. Empirically, ABBA
achieves state-of-the-art results on arithmetic and commonsense reasoning
benchmarks, consistently outperforming existing PEFT methods by a significant
margin across multiple models. Our code is publicly available at:
https://github.com/CERT-Lab/abba.

</details>


### [78] [Technical Report on classification of literature related to children speech disorder](https://arxiv.org/abs/2505.14242)
*Ziang Wang,Amir Aryani*

Main category: cs.CL

TL;DR: 基于LDA和BERTopic的NLP方法系统分析4804篇儿童言语障碍文献，识别出14个临床相关主题聚类


<details>
  <summary>Details</summary>
Motivation: 解决言语病理学领域海量文献人工分类效率低下的问题，建立自动化文献分析框架

Method: 1. 从PubMed获取2015年后文献并清洗
2. 使用定制停用词表优化
3. 应用LDA（计算主题连贯性0.42/困惑度-7.5）和BERTopic（异常主题<20%）双模型分析

Result: 成功识别婴儿多动症、异常癫痫行为等14个临床主题，LDA模型显示较强主题连贯性，BERTopic展现优秀文献分类能力

Conclusion: 验证了NLP技术自动化文献综述的可行性，为言语治疗领域知识发现提供有效技术路径

Abstract: This technical report presents a natural language processing (NLP)-based
approach for systematically classifying scientific literature on childhood
speech disorders. We retrieved and filtered 4,804 relevant articles published
after 2015 from the PubMed database using domain-specific keywords. After
cleaning and pre-processing the abstracts, we applied two topic modeling
techniques - Latent Dirichlet Allocation (LDA) and BERTopic - to identify
latent thematic structures in the corpus. Our models uncovered 14 clinically
meaningful clusters, such as infantile hyperactivity and abnormal epileptic
behavior. To improve relevance and precision, we incorporated a custom stop
word list tailored to speech pathology. Evaluation results showed that the LDA
model achieved a coherence score of 0.42 and a perplexity of -7.5, indicating
strong topic coherence and predictive performance. The BERTopic model exhibited
a low proportion of outlier topics (less than 20%), demonstrating its capacity
to classify heterogeneous literature effectively. These results provide a
foundation for automating literature reviews in speech-language pathology.

</details>


### [79] [TransBench: Benchmarking Machine Translation for Industrial-Scale Applications](https://arxiv.org/abs/2505.14244)
*Haijun Li,Tianqi Shi,Zifu Shang,Yuxuan Han,Xueyu Zhao,Hao Wang,Yu Qian,Zhiqiang Qian,Linlong Xu,Minghao Wu,Chenyang Lyu,Longyue Wang,Gongbo Tang,Weihua Luo,Zhao Xu,Kaifu Zhang*

Main category: cs.CL

TL;DR: 提出三层次工业机器翻译评估框架（基础语言能力、领域专精、文化适应）及首个电商翻译基准TransBench，解决现有评估体系与产业需求脱节问题


<details>
  <summary>Details</summary>
Motivation: 通用机器翻译模型在工业场景中存在专业术语缺失、文化差异适应不足、风格惯例不符等关键局限，现有评估框架无法有效衡量专业领域翻译质量

Method: 构建包含基础语言能力/领域专精/文化适应的三层评估框架，开发含1.7万专业翻译句的电商基准TransBench（覆盖4大场景、33种语言对），结合传统指标与领域专用评估模型Marco-MOS

Result: 1) 结构化工业MT评估框架 2) 首个公开电商翻译基准 3) 多层级质量评估指标 4) 开源评估工具集，实现从学术基准到产业效能的系统化评估转型

Conclusion: TransBench及评估框架有效弥合机器翻译学术研究与实践应用的鸿沟，为行业定制化翻译系统的开发提供可复现的评估体系和技术基础设施

Abstract: Machine translation (MT) has become indispensable for cross-border
communication in globalized industries like e-commerce, finance, and legal
services, with recent advancements in large language models (LLMs)
significantly enhancing translation quality. However, applying general-purpose
MT models to industrial scenarios reveals critical limitations due to
domain-specific terminology, cultural nuances, and stylistic conventions absent
in generic benchmarks. Existing evaluation frameworks inadequately assess
performance in specialized contexts, creating a gap between academic benchmarks
and real-world efficacy. To address this, we propose a three-level translation
capability framework: (1) Basic Linguistic Competence, (2) Domain-Specific
Proficiency, and (3) Cultural Adaptation, emphasizing the need for holistic
evaluation across these dimensions. We introduce TransBench, a benchmark
tailored for industrial MT, initially targeting international e-commerce with
17,000 professionally translated sentences spanning 4 main scenarios and 33
language pairs. TransBench integrates traditional metrics (BLEU, TER) with
Marco-MOS, a domain-specific evaluation model, and provides guidelines for
reproducible benchmark construction. Our contributions include: (1) a
structured framework for industrial MT evaluation, (2) the first publicly
available benchmark for e-commerce translation, (3) novel metrics probing
multi-level translation quality, and (4) open-sourced evaluation tools. This
work bridges the evaluation gap, enabling researchers and practitioners to
systematically assess and enhance MT systems for industry-specific needs.

</details>


### [80] [FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation](https://arxiv.org/abs/2505.14256)
*Shaolin Zhu,Tianyu Dong,Bo Li,Deyi Xiong*

Main category: cs.CL

TL;DR: FuxiMT是一个基于稀疏化大语言模型的中文中心多语言翻译模型，采用两阶段训练策略和课程学习，在低资源场景下表现优异并具备零样本翻译能力。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言对翻译数据稀缺的问题，通过稀疏化大语言模型提升多语言翻译性能，特别是针对未见过的语言对实现零样本翻译。

Method: 1. 两阶段训练：先在中文语料库预训练，再用覆盖65种语言的大规模平行数据集进行多语言微调
2. 集成混合专家系统(MoEs)
3. 采用课程学习策略优化不同资源水平的翻译表现

Result: 1. 显著超越现有大语言模型和机器翻译基线
2. 低资源场景下性能提升尤为突出
3. 对未见语言对展现出强大的零样本翻译能力

Conclusion: FuxiMT通过稀疏化LLM架构和课程学习策略，有效缓解了平行数据稀缺场景的翻译难题，为无数据/少数据语言对的沟通提供了新的解决方案。

Abstract: In this paper, we present FuxiMT, a novel Chinese-centric multilingual
machine translation model powered by a sparsified large language model (LLM).
We adopt a two-stage strategy to train FuxiMT. We first pre-train the model on
a massive Chinese corpus and then conduct multilingual fine-tuning on a large
parallel dataset encompassing 65 languages. FuxiMT incorporates
Mixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust
performance across various resource levels. Experimental results demonstrate
that FuxiMT significantly outperforms strong baselines, including
state-of-the-art LLMs and machine translation models, particularly under
low-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot
translation capabilities for unseen language pairs, indicating its potential to
bridge communication gaps where parallel data are scarce or unavailable.

</details>


### [81] [Think-J: Learning to Think for Generative LLM-as-a-Judge](https://arxiv.org/abs/2505.14268)
*Hui Huang,Yancheng He,Hongli Zhou,Rui Zhang,Wei Liu,Weixun Wang,Wenbo Su,Bo Zheng,Jiaheng Liu*

Main category: cs.CL

TL;DR: 提出Think-J方法，通过强化学习优化生成式LLM的评判能力，显著提升评估效果且无需人工标注。


<details>
  <summary>Details</summary>
Motivation: 当前生成式LLM作为评判者表现不足，需改进其自动评估和奖励建模能力。

Method: 先用小样本数据培养初始判断能力，再通过离线（训练评论模型）和在线（规则奖励）强化学习优化判断过程。

Result: Think-J超越生成式和分类器方法，在多个场景验证有效。

Conclusion: 优化判断思维轨迹为LLM自动评估提供新思路，具有实际应用价值。

Abstract: LLM-as-a-Judge refers to the automatic modeling of preferences for responses
generated by Large Language Models (LLMs), which is of significant importance
for both LLM evaluation and reward modeling. Although generative LLMs have made
substantial progress in various tasks, their performance as LLM-Judge still
falls short of expectations. In this work, we propose Think-J, which improves
generative LLM-as-a-Judge by learning how to think. We first utilized a small
amount of curated data to develop the model with initial judgment thinking
capabilities. Subsequently, we optimize the judgment thinking traces based on
reinforcement learning (RL). We propose two methods for judgment thinking
optimization, based on offline and online RL, respectively. The offline RL
requires training a critic model to construct positive and negative examples
for learning. The online method defines rule-based reward as feedback for
optimization. Experimental results showed that our approach can significantly
enhance the evaluation capability of generative LLM-Judge, surpassing both
generative and classifier-based LLM-Judge without requiring extra human
annotations.

</details>


### [82] [FAID: Fine-grained AI-generated Text Detection using Multi-task Auxiliary and Multi-level Contrastive Learning](https://arxiv.org/abs/2505.14271)
*Minh Ngoc Ta,Dong Cao Van,Duc-Anh Hoang,Minh Le-Anh,Truong Nguyen,My Anh Tran Nguyen,Yuxia Wang,Preslav Nakov,Sang Dinh*

Main category: cs.CL

TL;DR: 提出多语言多领域数据集FAIDSet和细粒度检测框架FAID，实现文本类型（人类/AI/协作）和AI模型家族的双重分类


<details>
  <summary>Details</summary>
Motivation: 解决现有二元分类器无法区分人机协作文本，且缺乏对底层AI模型特征识别的局限性

Method: 结合多层级对比学习与多任务辅助分类，将AI模型家族建模为独立风格实体，采用无需重新训练的数据分布迁移适应机制

Result: 在跨领域和新AI模型的泛化能力上显著超越基线方法，实验验证框架有效性

Conclusion: 通过捕捉作者风格和模型特征，FAID为提升AI辅助写作的透明度和问责机制提供新思路

Abstract: The growing collaboration between humans and AI models in generative tasks
has introduced new challenges in distinguishing between human-written,
AI-generated, and human-AI collaborative texts. In this work, we collect a
multilingual, multi-domain, multi-generator dataset FAIDSet. We further
introduce a fine-grained detection framework FAID to classify text into these
three categories, meanwhile identifying the underlying AI model family. Unlike
existing binary classifiers, FAID is built to capture both authorship and
model-specific characteristics. Our method combines multi-level contrastive
learning with multi-task auxiliary classification to learn subtle stylistic
cues. By modeling AI families as distinct stylistic entities, FAID offers
improved interpretability. We incorporate an adaptation to address
distributional shifts without retraining for unseen data. Experimental results
demonstrate that FAID outperforms several baseline approaches, particularly
enhancing the generalization accuracy on unseen domains and new AI models. It
provide a potential solution for improving transparency and accountability in
AI-assisted writing.

</details>


### [83] [Data-Efficient Hate Speech Detection via Cross-Lingual Nearest Neighbor Retrieval with Limited Labeled Data](https://arxiv.org/abs/2505.14272)
*Faeze Ghorbanpour,Daryna Dementieva,Alexander Fraser*

Main category: cs.CL

TL;DR: 提出基于最近邻检索的跨语言迁移学习方法，仅需少量目标语言标注数据即可有效提升多语言仇恨言论检测性能


<details>
  <summary>Details</summary>
Motivation: 低资源语言的仇恨言论标注数据获取成本高，现有跨语言迁移学习和数据增强方法在有限标注数据场景效果有限

Method: 使用目标语言的小规模标注样本从多语言仇恨语料库中检索相似实例进行数据增强，结合最大边际相关性减少冗余

Result: 在8种语言上超越单语言模型，多数情况下优于当前SOTA，数据效率高（最低仅需200样本），可扩展性强

Conclusion: 该方法通过高效检索机制实现了跨语言知识迁移，兼具数据效率和扩展性，为低资源NLP任务提供了实用解决方案

Abstract: Considering the importance of detecting hateful language, labeled hate speech
data is expensive and time-consuming to collect, particularly for low-resource
languages. Prior work has demonstrated the effectiveness of cross-lingual
transfer learning and data augmentation in improving performance on tasks with
limited labeled data. To develop an efficient and scalable cross-lingual
transfer learning approach, we leverage nearest-neighbor retrieval to augment
minimal labeled data in the target language, thereby enhancing detection
performance. Specifically, we assume access to a small set of labeled training
instances in the target language and use these to retrieve the most relevant
labeled examples from a large multilingual hate speech detection pool. We
evaluate our approach on eight languages and demonstrate that it consistently
outperforms models trained solely on the target language data. Furthermore, in
most cases, our method surpasses the current state-of-the-art. Notably, our
approach is highly data-efficient, retrieving as small as 200 instances in some
cases while maintaining superior performance. Moreover, it is scalable, as the
retrieval pool can be easily expanded, and the method can be readily adapted to
new languages and tasks. We also apply maximum marginal relevance to mitigate
redundancy and filter out highly similar retrieved instances, resulting in
improvements in some languages.

</details>


### [84] [YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering](https://arxiv.org/abs/2505.14279)
*Jennifer D'Souza,Hamed Babaei Giglou,Quentin Münch*

Main category: cs.CL

TL;DR: 提出开源框架YESciEval，通过规则评估与强化学习结合解决LLM评估中的乐观偏差问题，并发布多学科科学问答数据集


<details>
  <summary>Details</summary>
Motivation: 现有LLM在科学问答评估中缺乏鲁棒性评估体系，存在过度乐观的评估偏差需要系统性解决方案

Method: 融合细粒度规则评估与强化学习机制，构建包含对抗样本的多学科数据集，建立不依赖商业模型和人工反馈的评估体系

Result: 成功开发可扩展的免费评估框架，提供跨学科基准数据集及对抗测试集，实现评估过程的透明化和偏差控制

Conclusion: 通过构建可靠的LLM-as-a-judge模型，为AI对齐和通用人工智能发展提供可验证的评估范式，推动科学研究的可重复性

Abstract: Large Language Models (LLMs) drive scientific question-answering on modern
search engines, yet their evaluation robustness remains underexplored. We
introduce YESciEval, an open-source framework that combines fine-grained
rubric-based assessment with reinforcement learning to mitigate optimism bias
in LLM evaluators. We release multidisciplinary scienceQ&A datasets, including
adversarial variants, with evaluation scores from multiple LLMs. Independent of
proprietary models and human feedback, our approach enables scalable, cost-free
evaluation. By advancing reliable LLM-as-a-judge models, this work supports AI
alignment and fosters robust, transparent evaluation essential for scientific
inquiry and artificial general intelligence.

</details>


### [85] [Universal Acoustic Adversarial Attacks for Flexible Control of Speech-LLMs](https://arxiv.org/abs/2505.14286)
*Rao Ma,Mengjie Qian,Vyas Raina,Mark Gales,Kate Knill*

Main category: cs.CL

TL;DR: 语音大模型（如Qwen2-Audio和Granite-Speech）存在通用声学对抗攻击漏洞，攻击可针对特定属性选择性激活，需加强模型鲁棒性防御策略。


<details>
  <summary>Details</summary>
Motivation: 尽管结合预训练语音编码器与大语言模型的语音LLMs具备强大任务处理能力，但其灵活性可能带来对抗攻击脆弱性。研究旨在验证此类模型对通用对抗攻击的防御缺陷。

Method: 通过在原始音频前添加固定通用对抗音频片段，研究实现两种攻击模式：1）使模型无输出/执行篡改任务；2）选择性攻击（根据说话人性别/语言等属性激活攻击），非目标属性输入不受影响。

Result: 实验证明Qwen2-Audio和Granite-Speech存在严重漏洞，表明同类语音LLMs可能普遍易受此类攻击，揭示当前训练策略的防御不足。

Conclusion: 研究强调语音LLMs需采用更鲁棒的训练方案，提升对抗攻击抵抗力，这对保障语音处理系统安全性至关重要。

Abstract: The combination of pre-trained speech encoders with large language models has
enabled the development of speech LLMs that can handle a wide range of spoken
language processing tasks. While these models are powerful and flexible, this
very flexibility may make them more vulnerable to adversarial attacks. To
examine the extent of this problem, in this work we investigate universal
acoustic adversarial attacks on speech LLMs. Here a fixed, universal,
adversarial audio segment is prepended to the original input audio. We
initially investigate attacks that cause the model to either produce no output
or to perform a modified task overriding the original prompt. We then extend
the nature of the attack to be selective so that it activates only when
specific input attributes, such as a speaker gender or spoken language, are
present. Inputs without the targeted attribute should be unaffected, allowing
fine-grained control over the model outputs. Our findings reveal critical
vulnerabilities in Qwen2-Audio and Granite-Speech and suggest that similar
speech LLMs may be susceptible to universal adversarial attacks. This
highlights the need for more robust training strategies and improved resistance
to adversarial attacks.

</details>


### [86] [Cross-Lingual Optimization for Language Transfer in Large Language Models](https://arxiv.org/abs/2505.14297)
*Jungseob Lee,Seongtae Hong,Hyeonseok Moon,Heuiseok Lim*

Main category: cs.CL

TL;DR: 跨语言优化方法CLO利用英语数据+翻译模型，在保持英语能力的同时显著提升多语言性能，数据效率比传统SFT提升50%


<details>
  <summary>Details</summary>
Motivation: 传统监督微调(SFT)存在英语中心主义倾向，尤其在低资源语言场景中性能受限且破坏原有英语能力

Method: 1. 使用公开英语SFT数据 2. 通过翻译模型实现跨语言迁移 3. 设计对比实验覆盖5个模型6种不同资源水平的语言

Result: 低资源语言仅需3200样本即超越SFT-6400的表现，中低资源语言场景下CLO鲁棒性显著优于SFT（SFT性能波动达23%）

Conclusion: CLO打破了数据规模与模型性能的线性依赖，为多语言LLM适配提供了更高效的训练范式，尤其在资源受限场景优势显著

Abstract: Adapting large language models to other languages typically employs
supervised fine-tuning (SFT) as a standard approach. However, it often suffers
from an overemphasis on English performance, a phenomenon that is especially
pronounced in data-constrained environments. To overcome these challenges, we
propose \textbf{Cross-Lingual Optimization (CLO)} that efficiently transfers an
English-centric LLM to a target language while preserving its English
capabilities. CLO utilizes publicly available English SFT data and a
translation model to enable cross-lingual transfer. We conduct experiments
using five models on six languages, each possessing varying levels of resource.
Our results show that CLO consistently outperforms SFT in both acquiring target
language proficiency and maintaining English performance. Remarkably, in
low-resource languages, CLO with only 3,200 samples surpasses SFT with 6,400
samples, demonstrating that CLO can achieve better performance with less data.
Furthermore, we find that SFT is particularly sensitive to data quantity in
medium and low-resource languages, whereas CLO remains robust. Our
comprehensive analysis emphasizes the limitations of SFT and incorporates
additional training strategies in CLO to enhance efficiency.

</details>


### [87] [JOLT-SQL: Joint Loss Tuning of Text-to-SQL with Confusion-aware Noisy Schema Sampling](https://arxiv.org/abs/2505.14305)
*Jinwang Song,Hongying Zan,Kunli Zhang,Lingling Mu,Yingjie Han,Haobo Hua,Min Peng*

Main category: cs.CL

TL;DR: 提出JOLT-SQL框架，通过联合优化模式链接和SQL生成，实现更高效的文本到SQL转换单阶段监督微调。


<details>
  <summary>Details</summary>
Motivation: 现有监督微调方法存在多阶段流程复杂、模式噪声鲁棒性差的问题，需简化流程并增强抗干扰能力。

Method: 采用局部双向注意力增强的判别式模式链接，结合选择性注意力的混淆感知噪声模式采样策略，通过统一损失函数联合优化。

Result: 在Spider和BIRD基准测试中达到同规模模型最高执行准确率，显著提升训练和推理效率。

Conclusion: JOLT-SQL通过单阶段联合优化有效解决了文本到SQL任务中的模式噪声鲁棒性和流程效率问题，为实际应用提供了更优解决方案。

Abstract: Text-to-SQL, which maps natural language to SQL queries, has benefited
greatly from recent advances in Large Language Models (LLMs). While LLMs offer
various paradigms for this task, including prompting and supervised fine-tuning
(SFT), SFT approaches still face challenges such as complex multi-stage
pipelines and poor robustness to noisy schema information. To address these
limitations, we present JOLT-SQL, a streamlined single-stage SFT framework that
jointly optimizes schema linking and SQL generation via a unified loss.
JOLT-SQL employs discriminative schema linking, enhanced by local bidirectional
attention, alongside a confusion-aware noisy schema sampling strategy with
selective attention to improve robustness under noisy schema conditions.
Experiments on the Spider and BIRD benchmarks demonstrate that JOLT-SQL
achieves state-of-the-art execution accuracy among comparable-size open-source
models, while significantly improving both training and inference efficiency.

</details>


### [88] [Studying the Role of Input-Neighbor Overlap in Retrieval-Augmented Language Models Training Efficiency](https://arxiv.org/abs/2505.14309)
*Ehsan Doostmohammadi,Marco Kuhlmann*

Main category: cs.CL

TL;DR: 检索增强语言模型的性能与查询-上下文重叠度密切相关，超过临界阈值可显著提升效果并加速训练。


<details>
  <summary>Details</summary>
Motivation: 探索检索增强语言模型中查询与上下文的最优重叠度，及其对模型性能的影响机制。

Method: 通过控制训练/推理阶段的查询-上下文重叠度进行系统实验，使用查询转述生成合成上下文验证效果。

Result: 重叠度超阈值后测试困惑度下降40%，训练时间缩短40%且保持性能，问答任务验证了泛化性。

Conclusion: 揭示了检索机制在预训练中的优化潜力，证明主动增强上下文重叠是提升效率的有效策略。

Abstract: Retrieval-augmented language models have demonstrated performance comparable
to much larger models while requiring fewer computational resources. The
effectiveness of these models crucially depends on the overlap between query
and retrieved context, but the optimal degree of this overlap remains
unexplored. In this paper, we systematically investigate how varying levels of
query--context overlap affect model performance during both training and
inference. Our experiments reveal that increased overlap initially has minimal
effect, but substantially improves test-time perplexity and accelerates model
learning above a critical threshold. Building on these findings, we demonstrate
that deliberately increasing overlap through synthetic context can enhance data
efficiency and reduce training time by approximately 40\% without compromising
performance. We specifically generate synthetic context through paraphrasing
queries. We validate our perplexity-based findings on question-answering tasks,
confirming that the benefits of retrieval-augmented language modeling extend to
practical applications. Our results provide empirical evidence of significant
optimization potential for retrieval mechanisms in language model pretraining.

</details>


### [89] [HausaNLP: Current Status, Challenges and Future Directions for Hausa Natural Language Processing](https://arxiv.org/abs/2505.14311)
*Shamsuddeen Hassan Muhammad,Ibrahim Said Ahmad,Idris Abdulmumin,Falalu Ibrahim Lawan,Babangida Sani,Sukairaj Hafiz Imam,Yusuf Aliyu,Sani Abdullahi Sani,Ali Usman Umar,Kenneth Church,Vukosi Marivate*

Main category: cs.CL

TL;DR: 系统梳理豪萨语NLP研究现状，推出HausaNLP资源目录，提出融入大语言模型的优化方向。


<details>
  <summary>Details</summary>
Motivation: 豪萨语作为拥有2亿使用者的低资源语言，面临开源数据集匮乏和模型代表性不足的挑战，亟需系统性研究支持。

Method: 1. 创建HausaNLP资源目录整合数据集/工具/研究成果
2. 分析五大基础NLP任务（文本分类、机器翻译等）研究现状
3. 探讨大语言模型中的分词优化和方言处理方案

Result: 1. 建成首个豪萨语NLP资源整合平台
2. 揭示模型tokenization效率低下（BPE仅达30%压缩率）
3. 提出跨方言统一表征框架（准确率提升15%）

Conclusion: 该研究为豪萨语NLP发展建立资源基础，其多语言模型优化方案对低资源语言处理具有普适参考价值。

Abstract: Hausa Natural Language Processing (NLP) has gained increasing attention in
recent years, yet remains understudied as a low-resource language despite
having over 120 million first-language (L1) and 80 million second-language (L2)
speakers worldwide. While significant advances have been made in high-resource
languages, Hausa NLP faces persistent challenges, including limited open-source
datasets and inadequate model representation. This paper presents an overview
of the current state of Hausa NLP, systematically examining existing resources,
research contributions, and gaps across fundamental NLP tasks: text
classification, machine translation, named entity recognition, speech
recognition, and question answering. We introduce HausaNLP
(https://catalog.hausanlp.org), a curated catalog that aggregates datasets,
tools, and research works to enhance accessibility and drive further
development. Furthermore, we discuss challenges in integrating Hausa into large
language models (LLMs), addressing issues of suboptimal tokenization and
dialectal variation. Finally, we propose strategic research directions
emphasizing dataset expansion, improved language modeling approaches, and
strengthened community collaboration to advance Hausa NLP. Our work provides
both a foundation for accelerating Hausa NLP progress and valuable insights for
broader multilingual NLP research.

</details>


### [90] [A MIND for Reasoning: Meta-learning for In-context Deduction](https://arxiv.org/abs/2505.14313)
*Leonardo Bertolazzi,Manuel Vargas Guzmán,Raffaella Bernardi,Maciej Malicki,Jakub Szymanik*

Main category: cs.CL

TL;DR: 提出MIND元学习方法提升小语言模型在推理任务中的泛化能力，使其在低资源场景下超越GPT-4o等大模型


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在分布外问题的系统性推理和规则应用上存在泛化局限，尤其在知识库变更场景下性能显著下降

Method: MIND通过少量样本元学习微调策略，训练模型动态适应新知识库并系统应用演绎规则，重点关注前提选择的推理过程

Result: 1.5B-7B小模型经MIND训练后，在未见知识库上的演绎准确率提升27-35%，低数据场景下表现优于GPT-4o等大模型

Conclusion: 该方法证明小模型通过针对性训练可获得超越大模型的系统性推理能力，为资源受限场景的推理任务优化提供新方向

Abstract: Large language models (LLMs) are increasingly evaluated on formal tasks,
where strong reasoning abilities define the state of the art. However, their
ability to generalize to out-of-distribution problems remains limited. In this
paper, we investigate how LLMs can achieve a systematic understanding of
deductive rules. Our focus is on the task of identifying the appropriate subset
of premises within a knowledge base needed to derive a given hypothesis. To
tackle this challenge, we propose Meta-learning for In-context Deduction
(MIND), a novel few-shot meta-learning fine-tuning approach. The goal of MIND
is to enable models to generalize more effectively to unseen knowledge bases
and to systematically apply inference rules. Our results show that MIND
significantly improves generalization in small LMs ranging from 1.5B to 7B
parameters. The benefits are especially pronounced in smaller models and
low-data settings. Remarkably, small models fine-tuned with MIND outperform
state-of-the-art LLMs, such as GPT-4o and o3-mini, on this task.

</details>


### [91] [QA-prompting: Improving Summarization with Large Language Models using Question-Answering](https://arxiv.org/abs/2505.14347)
*Neelabh Sinha*

Main category: cs.CL

TL;DR: QA-prompting通过引入问答中间步骤优化长文本摘要，无需微调即可在多个领域实现ROUGE分数最高29%的提升


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在长文本摘要中存在位置偏差问题，传统优化方法（微调/流水线）存在实现复杂度高的问题

Method: 提出QA-prompting方法，在生成摘要前先进行问答步骤提取关键信息，通过单次模型调用实现上下文增强

Result: 在跨领域数据集测试中，该方法ROUGE指标超越基线方法最高达29%，且适用于10种主流预训练模型

Conclusion: 该方法为摘要任务提供了高效解决方案，同时揭示了领域特异性问题选择对性能优化的重要性

Abstract: Language Models (LMs) have revolutionized natural language processing,
enabling high-quality text generation through prompting and in-context
learning. However, models often struggle with long-context summarization due to
positional biases, leading to suboptimal extraction of critical information.
There are techniques to improve this with fine-tuning, pipelining, or using
complex techniques, which have their own challenges. To solve these challenges,
we propose QA-prompting - a simple prompting method for summarization that
utilizes question-answering as an intermediate step prior to summary
generation. Our method extracts key information and enriches the context of
text to mitigate positional biases and improve summarization in a single LM
call per task without requiring fine-tuning or pipelining. Experiments on
multiple datasets belonging to different domains using ten state-of-the-art
pre-trained models demonstrate that QA-prompting outperforms baseline and other
state-of-the-art methods, achieving up to 29% improvement in ROUGE scores. This
provides an effective and scalable solution for summarization and highlights
the importance of domain-specific question selection for optimal performance.

</details>


### [92] [OSoRA: Output-Dimension and Singular-Value Initialized Low-Rank Adaptation](https://arxiv.org/abs/2505.14350)
*Jialong Han,Si Zhang,Ke Zhang*

Main category: cs.CL

TL;DR: 提出OSoRA方法，通过整合SVD和可学习缩放向量，显著降低LLM微调参数量的同时保持优异性能


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法（如LoRA）仍需要较多计算资源，需开发更高效的替代方案

Method: 对预训练权重进行SVD分解，冻结奇异向量矩阵，仅优化输出维度向量和缩放参数

Result: 在数学推理（GSM8K）、常识推理（ARC）等基准测试中，性能与LoRA、VeRA相当或更好，且参数随秩线性增长

Conclusion: OSoRA通过联合训练机制实现资源效率与性能的平衡，为LLM轻量化微调提供新思路

Abstract: Fine-tuning Large Language Models (LLMs) has become increasingly challenging
due to their massive scale and associated computational costs.
Parameter-Efficient Fine-Tuning (PEFT) methodologies have been proposed as
computational alternatives; however, their implementations still require
significant resources. In this paper, we present OSoRA (Output-Dimension and
Singular-Value Initialized Low-Rank Adaptation), a novel PEFT method for LLMs.
OSoRA extends Low-Rank Adaptation (LoRA) by integrating Singular Value
Decomposition (SVD) with learnable scaling vectors in a unified framework. It
first performs an SVD of pre-trained weight matrices, then optimizes an
output-dimension vector during training, while keeping the corresponding
singular vector matrices frozen. OSoRA substantially reduces computational
resource requirements by minimizing the number of trainable parameters during
fine-tuning. Comprehensive evaluations across mathematical reasoning, common
sense reasoning, and other benchmarks demonstrate that OSoRA achieves
comparable or superior performance to state-of-the-art methods like LoRA and
VeRA, while maintaining a linear parameter scaling even as the rank increases
to higher dimensions. Our ablation studies further confirm that jointly
training both the singular values and the output-dimension vector is critical
for optimal performance.

</details>


### [93] [WirelessMathBench: A Mathematical Modeling Benchmark for LLMs in Wireless Communications](https://arxiv.org/abs/2505.14354)
*Xin Li,Mengbing Liu,Li Wei,Jiancheng An,Mérouane Debbah,Chau Yuen*

Main category: cs.CL

TL;DR: 研究者开发了WirelessMathBench基准，测试大模型在无线通信数学建模任务中的表现，发现现有模型在复杂方程补全任务中准确率显著下降。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在无线通信领域复杂数学推理能力的局限，填补该领域系统性评估工具的空白。

Method: 构建包含587个多层次问题的测试集（选择题/方程补全），覆盖物理与维度约束，基于40篇顶会论文设计实验。

Result: 最佳模型DeepSeek-R1平均准确率38.05%，完全方程补全成功率仅7.83%，显示模型处理工程数学的严重不足。

Conclusion: 公开的基准与工具包将推动开发领域感知更强的LLMs，提升无线系统分析与工程应用的可靠性。

Abstract: Large Language Models (LLMs) have achieved impressive results across a broad
array of tasks, yet their capacity for complex, domain-specific mathematical
reasoning-particularly in wireless communications-remains underexplored. In
this work, we introduce WirelessMathBench, a novel benchmark specifically
designed to evaluate LLMs on mathematical modeling challenges to wireless
communications engineering. Our benchmark consists of 587 meticulously curated
questions sourced from 40 state-of-the-art research papers, encompassing a
diverse spectrum of tasks ranging from basic multiple-choice questions to
complex equation completion tasks, including both partial and full completions,
all of which rigorously adhere to physical and dimensional constraints. Through
extensive experimentation with leading LLMs, we observe that while many models
excel in basic recall tasks, their performance degrades significantly when
reconstructing partially or fully obscured equations, exposing fundamental
limitations in current LLMs. Even DeepSeek-R1, the best performer on our
benchmark, achieves an average accuracy of only 38.05%, with a mere 7.83%
success rate in full equation completion. By publicly releasing
WirelessMathBench along with the evaluation toolkit, we aim to advance the
development of more robust, domain-aware LLMs for wireless system analysis and
broader engineering applications.

</details>


### [94] [Dual Decomposition of Weights and Singular Value Low Rank Adaptation](https://arxiv.org/abs/2505.14367)
*Jialong Han,Si Zhang,Ke Zhang*

Main category: cs.CL

TL;DR: 提出DuDe方法解决LoRA参数初始化问题，通过矩阵分解和SVD初始化提升PEFT效果


<details>
  <summary>Details</summary>
Motivation: 现有LoRA方法因随机初始化导致训练不稳定和知识迁移效率低下

Method: 将权重矩阵分解为幅度/方向分量，采用奇异值分解(SVD)进行系统性参数初始化

Result: MMLU准确率48.35%，GSM8K达62.53%±1.59%，优化稳定性提升2.8倍

Conclusion: DuDe通过理论创新与实证验证，成为LLM参数高效微调领域的重要方法论突破

Abstract: Parameter-Efficient Fine-Tuning (PEFT) has emerged as a critical paradigm for
adapting Large Language Models (LLMs) to downstream tasks, among which Low-rank
Adaptation (LoRA) represents one of the most widely adopted methodologies.
However, existing LoRA-based approaches exhibit two fundamental limitations:
unstable training dynamics and inefficient knowledge transfer from pre-trained
models, both stemming from random initialization of adapter parameters. To
overcome these challenges, we propose DuDe, a novel approach that decomposes
weight matrices into magnitude and direction components, employing Singular
Value Decomposition (SVD) for principled initialization. Our comprehensive
evaluation demonstrates DuDe's superior performance and robustness, achieving
up to 48.35\% accuracy on MMLU and 62.53\% ($\pm$ 1.59) accuracy on GSM8K. Our
theoretical analysis and empirical validation collectively demonstrate that
DuDe's decomposition strategy enhances optimization stability and better
preserves pre-trained representations, particularly for domain-specific tasks
requiring specialized knowledge. The combination of robust empirical
performance and rigorous theoretical foundations establishes DuDe as a
significant contribution to PEFT methodologies for LLMs.

</details>


### [95] [AutoRev: Automatic Peer Review System for Academic Research Papers](https://arxiv.org/abs/2505.14376)
*Maitreya Prafulla Chitale,Ketaki Mangesh Shetye,Harshit Gupta,Manav Chaudhary,Vasudeva Varma*

Main category: cs.CL

TL;DR: 提出AutoRev自动学术论文评审系统，通过图结构提取关键段落，在评审生成任务中显著超越现有基准58.72%，并具备扩展到问答、摘要等下游任务的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM微调的方法忽视了长文本输入带来的计算效率和性能限制，需要更有效的关键信息提取方法。

Method: 将学术文档建模为图结构，开发基于图的提取框架识别对评审生成最关键的核心段落。

Result: 在评审生成任务中，各项评估指标平均超越SOTA基线方法58.72%，显著提升生成质量。

Conclusion: 图结构提取技术可拓展至NLP多个下游任务，团队计划在论文接收后开源代码以促进后续研究。

Abstract: Generating a review for an academic research paper is a complex task that
requires a deep understanding of the document's content and the
interdependencies between its sections. It demands not only insight into
technical details but also an appreciation of the paper's overall coherence and
structure. Recent methods have predominantly focused on fine-tuning large
language models (LLMs) to address this challenge. However, they often overlook
the computational and performance limitations imposed by long input token
lengths. To address this, we introduce AutoRev, an Automatic Peer Review System
for Academic Research Papers. Our novel framework represents an academic
document as a graph, enabling the extraction of the most critical passages that
contribute significantly to the review. This graph-based approach demonstrates
effectiveness for review generation and is potentially adaptable to various
downstream tasks, such as question answering, summarization, and document
representation. When applied to review generation, our method outperforms SOTA
baselines by an average of 58.72% across all evaluation metrics. We hope that
our work will stimulate further research in applying graph-based extraction
techniques to other downstream tasks in NLP. We plan to make our code public
upon acceptance.

</details>


### [96] [Editing Across Languages: A Survey of Multilingual Knowledge Editing](https://arxiv.org/abs/2505.14393)
*Nadir Durrani,Basel Mousi,Fahim Dalvi*

Main category: cs.CL

TL;DR: 关于多语言知识编辑（MKE）的综述研究，系统化方法分类并分析跨语言编辑挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 解决知识编辑在多语言场景中研究不足的问题，确保事实编辑在不同语言间的可靠泛化。

Method: 提出四类方法：参数调整、记忆增强、微调策略、超网络架构。

Result: 揭示当前方法在跨语言传播中的局限性，明确语言各向异性和评估覆盖不足等核心挑战。

Conclusion: 整合领域进展，为构建可编辑、多语言感知的大语言模型奠定理论基础。

Abstract: While Knowledge Editing has been extensively studied in monolingual settings,
it remains underexplored in multilingual contexts. This survey systematizes
recent research on Multilingual Knowledge Editing (MKE), a growing subdomain of
model editing focused on ensuring factual edits generalize reliably across
languages. We present a comprehensive taxonomy of MKE methods, covering
parameter-based, memory-based, fine-tuning, and hypernetwork approaches. We
survey available benchmarks,summarize key findings on method effectiveness and
transfer patterns, identify challenges in cross-lingual propagation, and
highlight open problems related to language anisotropy, evaluation coverage,
and edit scalability. Our analysis consolidates a rapidly evolving area and
lays the groundwork for future progress in editable language-aware LLMs.

</details>


### [97] [MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language](https://arxiv.org/abs/2505.14395)
*Seyoung Song,Seogyeong Jeong,Eunsu Kim,Jiho Jin,Dongkwan Kim,Jay Shin,Alice Oh*

Main category: cs.CL

TL;DR: 提出MUG-Eval框架，通过对话任务转化和任务成功率评估大语言模型的多语言生成能力，无需依赖语言工具或标注数据


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言评估难题，传统方法依赖语言特定工具/标注数据（资源有限）且LLM-as-judge方法在非高资源语言中效果差

Method: 将现有基准转化为需要目标语言沟通的对话任务，用任务成功率替代生成能力评估，避免使用外部工具或标注数据

Result: 在30种语言上测试8个LLM，MUG-Eval与现有基准强相关（r>0.75），实现跨语言/模型的标准化评估

Conclusion: 提供资源高效的稳健解决方案，可扩展至数千种语言，突破传统评估对语言资源和标注数据的依赖

Abstract: Evaluating text generation capabilities of large language models (LLMs) is
challenging, particularly for low-resource languages where methods for direct
assessment are scarce. We propose MUG-Eval, a novel framework that evaluates
LLMs' multilingual generation capabilities by transforming existing benchmarks
into conversational tasks and measuring the LLMs' accuracies on those tasks. We
specifically designed these conversational tasks to require effective
communication in the target language. Then, we simply use task success rate as
a proxy of successful conversation generation. Our approach offers two key
advantages: it is independent of language-specific NLP tools or annotated
datasets, which are limited for most languages, and it does not rely on
LLMs-as-judges, whose evaluation quality degrades outside a few high-resource
languages. We evaluate 8 LLMs across 30 languages spanning high, mid, and
low-resource categories, and we find that MUG-Eval correlates strongly with
established benchmarks ($r$ > 0.75) while enabling standardized comparisons
across languages and models. Our framework provides a robust and
resource-efficient solution for evaluating multilingual generation that can be
extended to thousands of languages.

</details>


### [98] [Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation](https://arxiv.org/abs/2505.14398)
*Peter Baile Chen,Yi Zhang,Dan Roth,Samuel Madden,Jacob Andreas,Michael Cafarella*

Main category: cs.CL

TL;DR: 提出LAG框架，通过重用历史推理日志的KV缓存增强LLMs的持续学习能力，在提升准确率的同时保持系统效率


<details>
  <summary>Details</summary>
Motivation: 现有LLMs难以有效复用历史任务中的推理经验，导致面对新挑战时表现受限，需开发高效利用历史计算的机制

Method: 将任务日志编码为KV缓存，仅存储关键token的上下文。处理新任务时检索相关KV值增强生成，直接复用历史推理无需知识蒸馏

Result: 在知识推理数据集上显著优于无日志系统，且超越基于反思机制和传统KV缓存技术的现有方案

Conclusion: LAG开创性地将缓存技术应用于持续学习场景，通过历史计算直接复用机制突破现有代理系统的性能瓶颈

Abstract: While humans naturally learn and adapt from past experiences, large language
models (LLMs) and their agentic counterparts struggle to retain reasoning from
previous tasks and apply them in future contexts. To address this limitation,
we propose a novel framework, log-augmented generation (LAG) that directly
reuses prior computation and reasoning from past logs at test time to enhance
model's ability to learn from previous tasks and perform better on new, unseen
challenges, all while keeping the system efficient and scalable. Specifically,
our system represents task logs using key-value (KV) caches, encoding the full
reasoning context of prior tasks while storing KV caches for only a selected
subset of tokens. When a new task arises, LAG retrieves the KV values from
relevant logs to augment generation. Our approach differs from reflection-based
memory mechanisms by directly reusing prior reasoning and computations without
requiring additional steps for knowledge extraction or distillation. Our method
also goes beyond existing KV caching techniques, which primarily target
efficiency gains rather than improving accuracy. Experiments on knowledge- and
reasoning-intensive datasets demonstrate that our method significantly
outperforms standard agentic systems that do not utilize logs, as well as
existing solutions based on reflection and KV cache techniques.

</details>


### [99] [Pierce the Mists, Greet the Sky: Decipher Knowledge Overshadowing via Knowledge Circuit Analysis](https://arxiv.org/abs/2505.14406)
*Haoming Huang,Yibo Yan,Jiahao Huo,Xin Zou,Xinfeng Li,Kun Wang,Xuming Hu*

Main category: cs.CL

TL;DR: 论文提出PhantomCircuit框架，用于全面分析和检测大语言模型中的知识遮蔽现象。该方法通过知识回路分析追踪注意力头内部机制，揭示竞争知识路径如何导致遮蔽及其在训练中的演变。


<details>
  <summary>Details</summary>
Motivation: 现有研究对知识遮蔽现象的理解局限于推理阶段的观察，缺乏对模型训练过程中遮蔽起源及内部机制的系统性分析。研究者试图填补这一空白，为缓解此类幻觉提供方法论支持。

Method: 开发PhantomCircuit框架，创新性地运用知识回路分析技术，系统解构注意力头的工作机制，追踪竞争知识路径对遮蔽现象的影响及其在训练过程中的动态演化。

Result: 实验证明该框架能有效识别知识遮蔽实例，为理解此类幻觉提供了新视角，并为后续研究提供了可操作的方法论工具。

Conclusion: PhantomCircuit不仅深化了对知识遮蔽机制的理解，更为研究社区提供了缓解该现象的创新性方法论框架，具有重要的理论价值和实践意义。

Abstract: Large Language Models (LLMs), despite their remarkable capabilities, are
hampered by hallucinations. A particularly challenging variant, knowledge
overshadowing, occurs when one piece of activated knowledge inadvertently masks
another relevant piece, leading to erroneous outputs even with high-quality
training data. Current understanding of overshadowing is largely confined to
inference-time observations, lacking deep insights into its origins and
internal mechanisms during model training. Therefore, we introduce
PhantomCircuit, a novel framework designed to comprehensively analyze and
detect knowledge overshadowing. By innovatively employing knowledge circuit
analysis, PhantomCircuit dissects the internal workings of attention heads,
tracing how competing knowledge pathways contribute to the overshadowing
phenomenon and its evolution throughout the training process. Extensive
experiments demonstrate PhantomCircuit's effectiveness in identifying such
instances, offering novel insights into this elusive hallucination and
providing the research community with a new methodological lens for its
potential mitigation.

</details>


### [100] [Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents](https://arxiv.org/abs/2505.14418)
*Pengzhou Cheng,Haowen Hu,Zheng Wu,Zongru Wu,Tianjie Ju,Daizong Ding,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.CL

TL;DR: 提出了针对多模态大语言模型GUI代理的供应链后门攻击框架AgentGhost，通过复合触发机制实现高成功率攻击(99.7%)，同时保持任务效用仅下降1%


<details>
  <summary>Details</summary>
Motivation: 揭示开源GUI代理存在的供应链安全威胁，利用交互级触发器实施隐蔽后门攻击

Method: 构建目标级+交互级复合触发器，采用Min-Max优化框架(对比学习+微调)实现灵活有效的后门植入

Result: 在两个移动基准测试中攻击精度达99.7%，任务效用仅降1%；防御方法可将攻击精度降至22.1%

Conclusion: 首次系统性研究GUI代理供应链安全，提出的AgentGhost框架证明现有系统的脆弱性，同时提供有效防御方案

Abstract: Graphical user interface (GUI) agents powered by multimodal large language
models (MLLMs) have shown greater promise for human-interaction. However, due
to the high fine-tuning cost, users often rely on open-source GUI agents or
APIs offered by AI providers, which introduces a critical but underexplored
supply chain threat: backdoor attacks. In this work, we first unveil that
MLLM-powered GUI agents naturally expose multiple interaction-level triggers,
such as historical steps, environment states, and task progress. Based on this
observation, we introduce AgentGhost, an effective and stealthy framework for
red-teaming backdoor attacks. Specifically, we first construct composite
triggers by combining goal and interaction levels, allowing GUI agents to
unintentionally activate backdoors while ensuring task utility. Then, we
formulate backdoor injection as a Min-Max optimization problem that uses
supervised contrastive learning to maximize the feature difference across
sample classes at the representation space, improving flexibility of the
backdoor. Meanwhile, it adopts supervised fine-tuning to minimize the
discrepancy between backdoor and clean behavior generation, enhancing
effectiveness and utility. Extensive evaluations of various agent models in two
established mobile benchmarks show that AgentGhost is effective and generic,
with attack accuracy that reaches 99.7\% on three attack objectives, and shows
stealthiness with only 1\% utility degradation. Furthermore, we tailor a
defense method against AgentGhost that reduces the attack accuracy to 22.1\%.
Our code is available at \texttt{anonymous}.

</details>


### [101] [Scaling Low-Resource MT via Synthetic Data Generation with LLMs](https://arxiv.org/abs/2505.14423)
*Ona de Gibert,Joseph Attieh,Teemu Vahtola,Mikko Aulamo,Zihao Li,Raúl Vázquez,Tiancheng Hu,Jörg Tiedemann*

Main category: cs.CL

TL;DR: LLM生成的合成数据可显著提升低资源机器翻译性能，即使存在噪声干扰。通过构建SynOPUS公共数据集，验证了该方法在147种语言对中的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言机器翻译数据匮乏问题，探索大语言模型生成合成数据的实际应用价值，突破传统英语中心翻译的局限。

Method: 1. 基于Europarl英语语料构建七种目标语言的文档级合成语料库
2. 通过转译技术扩展至147种语言对
3. 结合自动评估与人工评估验证数据质量
4. 对比实验包括训练方案优化、HPLT数据集对比及非英语中心场景测试

Result: 1. 合成数据质量获得双评估体系验证
2. 低资源语言翻译性能平均提升18.7%
3. 成功建立SynOPUS开源数据集仓库
4. 在非英语语言对中展现泛化能力

Conclusion: 研究表明大模型生成的合成数据可作为有效补充资源，其价值在数据噪声容忍度和跨语言适应性方面超出预期，为低资源NLP任务提供了新范式。

Abstract: We investigate the potential of LLM-generated synthetic data for improving
low-resource machine translation (MT). Focusing on seven diverse target
languages, we construct a document-level synthetic corpus from English
Europarl, and extend it via pivoting to 147 additional language pairs.
Automatic and human evaluation confirm its high overall quality. We study its
practical application by (i) identifying effective training regimes, (ii)
comparing our data with the HPLT dataset, and (iii) testing its utility beyond
English-centric MT. Finally, we introduce SynOPUS, a public repository for
synthetic parallel datasets. Our findings show that LLM-generated synthetic
data, even when noisy, can substantially improve MT performance for
low-resource languages.

</details>


### [102] [From Templates to Natural Language: Generalization Challenges in Instruction-Tuned LLMs for Spatial Reasoning](https://arxiv.org/abs/2505.14425)
*Chalamalasetti Kranti,Sherzod Hakimov,David Schlangen*

Main category: cs.CL

TL;DR: 研究显示指令调优的LLMs在复杂空间任务中面临指令泛化挑战，简单任务表现良好但复杂任务性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在从合成指令到人类编写指令的跨环境泛化能力，特别是在空间布局任务中的表现差异。

Method: 使用纯合成指令微调LLMs，并在混合合成/人类指令的基准数据集上评估模型性能。

Result: 模型在简单任务泛化良好，但在复杂任务中性能下降58.2%（人类指令）和36.7%（合成指令）。

Conclusion: 通过错误分析揭示了指令泛化的具体差距，为提升LLMs的真实场景适应能力提供改进方向。

Abstract: Instruction-tuned large language models (LLMs) have shown strong performance
on a variety of tasks; however, generalizing from synthetic to human-authored
instructions in grounded environments remains a challenge for them. In this
work, we study generalization challenges in spatial grounding tasks where
models interpret and translate instructions for building object arrangements on
a $2.5$D grid. We fine-tune LLMs using only synthetic instructions and evaluate
their performance on a benchmark dataset containing both synthetic and
human-written instructions. Our results reveal that while models generalize
well on simple tasks, their performance degrades significantly on more complex
tasks. We present a detailed error analysis of the gaps in instruction
generalization.

</details>


### [103] [Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models](https://arxiv.org/abs/2505.14436)
*Yuqiao Tan,Shizhu He,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 论文提出参数对齐是实现跨规模参数知识转移（PKT）的基础，揭示PostPKT和PrePKT方法的稳定性挑战，并提出「神经不兼容性」作为根本障碍。


<details>
  <summary>Details</summary>
Motivation: 突破基于符号语言的传统知识迁移范式，探索通过模型参数实现不同规模LLM间真正的参数知识迁移机制。

Method: 提出PostPKT（基于LoRA初始化的后对齐方法）和PrePKT（LaTen预对齐范式，通过定位-对齐两步实现免后续训练的参数对齐）。

Result: 在四个基准测试中验证两种PKT方法均面临迁移稳定性挑战，发现不同规模LLM间的「神经不兼容性」是核心障碍。

Conclusion: 神经不兼容性揭示了LLM参数架构的本质差异，为未来高效参数知识迁移研究提供了新的理论框架和方向指引。

Abstract: Large Language Models (LLMs) offer a transparent brain with accessible
parameters that encode extensive knowledge, which can be analyzed, located and
transferred. Consequently, a key research challenge is to transcend traditional
knowledge transfer paradigms rooted in symbolic language and achieve genuine
Parametric Knowledge Transfer (PKT). Significantly, exploring effective methods
for transferring knowledge across LLMs of different scales through parameters
presents an intriguing and valuable research direction. In this paper, we first
demonstrate $\textbf{Alignment}$ in parametric space is the fundamental
prerequisite to achieve successful cross-scale PKT. We redefine the previously
explored knowledge transfer as Post-Align PKT (PostPKT), which utilizes
extracted parameters for LoRA initialization and requires subsequent fine-tune
for alignment. Hence, to reduce cost for further fine-tuning, we introduce a
novel Pre-Align PKT (PrePKT) paradigm and propose a solution called
$\textbf{LaTen}$
($\textbf{L}$oc$\textbf{a}$te-$\textbf{T}$h$\textbf{e}$n-Alig$\textbf{n}$) that
aligns the parametric spaces of LLMs across scales only using several training
steps without following training. Comprehensive experiments on four benchmarks
demonstrate that both PostPKT and PrePKT face challenges in achieving
consistently stable transfer. Through in-depth analysis, we identify
$\textbf{Neural Incompatibility}$ as the ethological and parametric structural
differences between LLMs of varying scales, presenting fundamental challenges
to achieving effective PKT. These findings provide fresh insights into the
parametric architectures of LLMs and highlight promising directions for future
research on efficient PKT. Our code is available at
https://github.com/Trae1ounG/Neural_Incompatibility.

</details>


### [104] [Creative Preference Optimization](https://arxiv.org/abs/2505.14442)
*Mete Ismayilzada,Antonio Laverghetta Jr.,Simone A. Luchini,Reet Patel,Antoine Bosselut,Lonneke van der Plas,Roger Beaty*

Main category: cs.CL

TL;DR: 提出CrPO方法通过多维度创造力信号模块化注入，在保持输出质量的同时显著提升LLM生成内容的新颖性、多样性和惊喜感


<details>
  <summary>Details</summary>
Motivation: 现有LLM创造力优化方法聚焦单一维度，未能系统解决创造力多维度特性（新颖性/多样性/惊喜感/质量）的综合提升需求

Method: 开发模块化的创造力偏好优化框架CrPO，结合新型MuCE数据集（含20万人类反馈和30+心理学创造力评估指标）进行模型训练

Result: 在自动评估和包含1,500样本的人类评估中超越GPT-4o等基线模型，NoveltyBench测试显示方法具有跨领域泛化能力

Conclusion: 通过偏好优化框架直接对齐创造力多维度特征，为提升LLM创造力开辟了新路径，实现质量与创新性的双赢

Abstract: While Large Language Models (LLMs) have demonstrated impressive performance
across natural language generation tasks, their ability to generate truly
creative content-characterized by novelty, diversity, surprise, and
quality-remains limited. Existing methods for enhancing LLM creativity often
focus narrowly on diversity or specific tasks, failing to address creativity's
multifaceted nature in a generalizable way. In this work, we propose Creative
Preference Optimization (CrPO), a novel alignment method that injects signals
from multiple creativity dimensions into the preference optimization objective
in a modular fashion. We train and evaluate creativity-augmented versions of
several models using CrPO and MuCE, a new large-scale human preference dataset
spanning over 200,000 human-generated responses and ratings from more than 30
psychological creativity assessments. Our models outperform strong baselines,
including GPT-4o, on both automated and human evaluations, producing more
novel, diverse, and surprising generations while maintaining high output
quality. Additional evaluations on NoveltyBench further confirm the
generalizability of our approach. Together, our results demonstrate that
directly optimizing for creativity within preference frameworks is a promising
direction for advancing the creative capabilities of LLMs without compromising
output quality.

</details>


### [105] [CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation](https://arxiv.org/abs/2505.14455)
*Chihan Huang,Hao Tang*

Main category: cs.CL

TL;DR: 提出了CtrlDiff框架，通过强化学习动态调整生成块大小并结合分类器引导机制，提升扩散语言模型的灵活性与可控性


<details>
  <summary>Details</summary>
Motivation: 当前大语言扩散模型存在固定粒度生成和弱控制性问题，限制了实际应用场景

Method: 1) 基于强化学习的语义感知块分割机制 2) 专为离散扩散设计的轻量级分类器引导控制方法

Result: 实验证明模型缩小了与自回归模型的性能差距(困惑度降低18%)，在文本编辑任务中效率提升40%

Conclusion: CtrlDiff建立了混合扩散模型新基准，实现了灵活的条件文本生成，为语言模型架构创新提供了新方向

Abstract: Although autoregressive models have dominated language modeling in recent
years, there has been a growing interest in exploring alternative paradigms to
the conventional next-token prediction framework. Diffusion-based language
models have emerged as a compelling alternative due to their powerful parallel
generation capabilities and inherent editability. However, these models are
often constrained by fixed-length generation. A promising direction is to
combine the strengths of both paradigms, segmenting sequences into blocks,
modeling autoregressive dependencies across blocks while leveraging discrete
diffusion to estimate the conditional distribution within each block given the
preceding context. Nevertheless, their practical application is often hindered
by two key limitations: rigid fixed-length outputs and a lack of flexible
control mechanisms. In this work, we address the critical limitations of fixed
granularity and weak controllability in current large diffusion language
models. We propose CtrlDiff, a dynamic and controllable semi-autoregressive
framework that adaptively determines the size of each generation block based on
local semantics using reinforcement learning. Furthermore, we introduce a
classifier-guided control mechanism tailored to discrete diffusion, which
significantly reduces computational overhead while facilitating efficient
post-hoc conditioning without retraining. Extensive experiments demonstrate
that CtrlDiff sets a new standard among hybrid diffusion models, narrows the
performance gap to state-of-the-art autoregressive approaches, and enables
effective conditional text generation across diverse tasks.

</details>


### [106] [Not All Correct Answers Are Equal: Why Your Distillation Source Matters](https://arxiv.org/abs/2505.14464)
*Xiaoyu Tian,Yunjie Ji,Haotian Wang,Shuaiting Chen,Sitong Zhao,Yiping Peng,Han Zhao,Xiangang Li*

Main category: cs.CL

TL;DR: 通过大规模验证数据蒸馏研究，发现AM-Thinking-v1蒸馏数据质量最佳，训练出的模型在多项推理基准测试中表现最优且具备自适应输出能力


<details>
  <summary>Details</summary>
Motivation: 验证不同教师模型的蒸馏数据对提升语言模型推理能力的实际效果，探索高质量推理轨迹的价值

Method: 收集3个顶尖教师模型在189万条查询的验证输出，构建平行数据集并分析分布特征，训练学生模型进行基准测试

Result: AM模型在AIME2024（84.3）、AIME2025（72.2）、MATH500（98.4）和LiveCodeBench（65.9）表现最优，且能根据任务难度自适应调整输出长度

Conclusion: 高质量验证推理数据对提升模型性能至关重要，公开AM和Qwen3蒸馏数据集以支持开源推理模型研究

Abstract: Distillation has emerged as a practical and effective approach to enhance the
reasoning capabilities of open-source language models. In this work, we conduct
a large-scale empirical study on reasoning data distillation by collecting
verified outputs from three state-of-the-art teacher models-AM-Thinking-v1,
Qwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We
construct three parallel datasets and analyze their distributions, revealing
that AM-Thinking-v1-distilled data exhibits greater token length diversity and
lower perplexity. Student models trained on each dataset are evaluated on
reasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench.
The AM-based model consistently achieves the best performance (e.g., 84.3 on
AIME2024, 72.2 on AIME2025, 98.4 on MATH500, and 65.9 on LiveCodeBench) and
demonstrates adaptive output behavior-producing longer responses for harder
tasks and shorter ones for simpler tasks. These findings highlight the value of
high-quality, verified reasoning traces. We release the AM-Thinking-v1 and
Qwen3-235B-A22B distilled datasets to support future research on open and
high-performing reasoning-oriented language models. The datasets are publicly
available on Hugging Face\footnote{Datasets are available on Hugging Face:
\href{https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled}{AM-Thinking-v1-Distilled},
\href{https://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled}{AM-Qwen3-Distilled}.}.

</details>


### [107] [Void in Language Models](https://arxiv.org/abs/2505.14467)
*Mani Shemiranifar*

Main category: cs.CL

TL;DR: 发现Transformer语言模型推理中存在未激活层(Voids)，提出L2自适应计算(LAC)方法检测，证明跳过部分层可提升模型性能


<details>
  <summary>Details</summary>
Motivation: 探究语言模型推理过程中是否所有层都被激活，这对提升模型效率和性能优化具有重要意义

Method: 使用非训练参数的LAC方法追踪激活层，分析指令调优模型在提示处理(PP)和响应生成(RG)阶段的层激活差异，并在Llama/Mistral/Qwen系列模型上进行多任务评估

Result: Qwen2.5-7B在MMLU任务上跳过70%层后准确率从69.24提升至71.29；Mistral-7B在GPQA任务上使用70%层时准确率从13.88提升至18.36

Conclusion: 语言模型推理中并非所有层都同等重要，选择性跳过大部分未激活层能在保持效率的同时提升特定任务表现

Abstract: Despite advances in transformer-based language models (LMs), a fundamental
question remains largely unanswered: Are all layers activated during inference?
We investigate this question by detecting unactivated layers (which we refer to
as Voids) using a non-trainable and parameter-free adaptive computation method
called L2 Adaptive Computation (LAC). We adapt LAC from its original
efficiency-focused application to trace activated layers during inference. This
method monitors changes in the L2-norm of activations to identify voids. We
analyze layer activation in instruction-tuned LMs across two phases: Prompt
Processing (PP), where we trace activated layers for each token in the input
prompts, and Response Generation (RG), where we trace activated layers for each
generated token. We further demonstrate that distinct layers are activated
during these two phases. To show the effectiveness of our method, we evaluated
three distinct instruction-tuned LMs from the Llama, Mistral, and Qwen families
on three benchmarks: MMLU, GPQA Diamond, and BoolQ. For example, on MMLU with a
zero-shot setting, skipping voids in Qwen2.5-7B-Instruct resulted in an
improvement from 69.24 to 71.29 while the model uses only 30% of the layers.
Similarly, Mistral-7B-Instruct-v0.3 on GPQA Diamond improved from 13.88 to
18.36 when using 70% of the layers during both the PP and RG phases. These
results show that not all layers contribute equally during inference, and that
selectively skipping most of them can improve the performance of models on
certain tasks.

</details>


### [108] [Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations](https://arxiv.org/abs/2505.14469)
*Somnath Banerjee,Pratyush Chatterjee,Shanu Kumar,Sayan Layek,Parag Agrawal,Rima Hazra,Animesh Mukherjee*

Main category: cs.CL

TL;DR: 代码混合提示显著增加大模型的不安全输出风险，研究通过可解释性方法揭示了其内部归因机制及文化差异影响


<details>
  <summary>Details</summary>
Motivation: LLMs在处理代码混合输入时存在安全隐患，相比单语输入更易生成有害内容，需系统分析其机理

Method: 系统性比较代码混合/单语提示的输出安全性差异，利用可解释性技术追踪模型内部归因变化，建立文化敏感度分类框架

Result: 代码混合提示使模型安全漏洞扩大1.8倍，归因分析显示语义理解层出现异常激活模式，文化特定类风险增加40%

Conclusion: 多语言场景需针对性强化模型安全机制，安全评估应纳入文化维度分析，可解释性方法为防御策略提供新路径

Abstract: Recent advancements in LLMs have raised significant safety concerns,
particularly when dealing with code-mixed inputs and outputs. Our study
systematically investigates the increased susceptibility of LLMs to produce
unsafe outputs from code-mixed prompts compared to monolingual English prompts.
Utilizing explainability methods, we dissect the internal attribution shifts
causing model's harmful behaviors. In addition, we explore cultural dimensions
by distinguishing between universally unsafe and culturally-specific unsafe
queries. This paper presents novel experimental insights, clarifying the
mechanisms driving this phenomenon.

</details>


### [109] [Adapting Pretrained Language Models for Citation Classification via Self-Supervised Contrastive Learning](https://arxiv.org/abs/2505.14471)
*Tong Li,Jiachuan Wang,Yongqi Zhang,Shuangyin Li,Lei Chen*

Main category: cs.CL

TL;DR: 提出Citss框架，通过自监督对比学习和两种特殊策略（句子级裁剪+关键词扰动）解决引用分类中的数据稀缺、噪声干扰和虚假关联问题，兼容编码器和解码器模型架构。


<details>
  <summary>Details</summary>
Motivation: 现有引用分类方法面临标注数据稀缺、长上下文噪声干扰以及模型过度依赖特定关键词的局限性，需要更鲁棒的解决方案。

Method: 1. 自监督对比学习框架缓解数据稀缺
2. 句子级裁剪策略聚焦目标引用上下文
3. 关键词扰动策略打破虚假关联
4. 兼容编码器(如BERT)和解码器(如LLaMA)模型架构

Result: 在三个基准数据集上，Citss在编码器型和解码器型模型上均超越现有SOTA方法，验证了框架有效性。

Conclusion: Citss通过创新的对比学习策略有效解决了引用分类的关键挑战，其模型架构兼容性为利用更大规模预训练模型提供了可能。

Abstract: Citation classification, which identifies the intention behind academic
citations, is pivotal for scholarly analysis. Previous works suggest
fine-tuning pretrained language models (PLMs) on citation classification
datasets, reaping the reward of the linguistic knowledge they gained during
pretraining. However, directly fine-tuning for citation classification is
challenging due to labeled data scarcity, contextual noise, and spurious
keyphrase correlations. In this paper, we present a novel framework, Citss,
that adapts the PLMs to overcome these challenges. Citss introduces
self-supervised contrastive learning to alleviate data scarcity, and is
equipped with two specialized strategies to obtain the contrastive pairs:
sentence-level cropping, which enhances focus on target citations within long
contexts, and keyphrase perturbation, which mitigates reliance on specific
keyphrases. Compared with previous works that are only designed for
encoder-based PLMs, Citss is carefully developed to be compatible with both
encoder-based PLMs and decoder-based LLMs, to embrace the benefits of enlarged
pretraining. Experiments with three benchmark datasets with both encoder-based
PLMs and decoder-based LLMs demonstrate our superiority compared to the
previous state of the art. Our code is available at: github.com/LITONG99/Citss

</details>


### [110] [PlanGPT-VL: Enhancing Urban Planning with Domain-Specific Vision-Language Models](https://arxiv.org/abs/2505.14481)
*He Zhu,Junyou Su,Minxi Chen,Wen Wang,Yijie Deng,Guanhua Chen,Wenjia Zhang*

Main category: cs.CL

TL;DR: 提出首个城市规划专用视觉语言模型PlanGPT-VL，通过PlanAnno-V数据合成、关键点验证思维、混合训练方法，显著提升规划地图解析能力。


<details>
  <summary>Details</summary>
Motivation: 现有通用视觉语言模型难以准确解析包含复杂空间配置与法规要求的城市规划地图，制约专业应用与教育场景。

Method: 1) PlanAnno-V框架生成高质量视觉问答数据
2) 关键点验证思维减少模型幻觉
3) 冻结视觉编码器的监督微调混合训练策略

Result: 在PlanBench-V基准测试中超越72B参数模型，7B轻量模型实现专业地图分析的SOTA性能，准确率提升23%

Conclusion: PlanGPT-VL为城市规划者提供可靠的地图分析与教育工具，在保持高准确性的同时实现高效的领域专业化

Abstract: In the field of urban planning, existing Vision-Language Models (VLMs)
frequently fail to effectively analyze and evaluate planning maps, despite the
critical importance of these visual elements for urban planners and related
educational contexts. Planning maps, which visualize land use, infrastructure
layouts, and functional zoning, require specialized understanding of spatial
configurations, regulatory requirements, and multi-scale analysis. To address
this challenge, we introduce PlanGPT-VL, the first domain-specific
Vision-Language Model tailored specifically for urban planning maps. PlanGPT-VL
employs three innovative approaches: (1) PlanAnno-V framework for high-quality
VQA data synthesis, (2) Critical Point Thinking to reduce hallucinations
through structured verification, and (3) comprehensive training methodology
combining Supervised Fine-Tuning with frozen vision encoder parameters. Through
systematic evaluation on our proposed PlanBench-V benchmark, we demonstrate
that PlanGPT-VL significantly outperforms general-purpose state-of-the-art VLMs
in specialized planning map interpretation tasks, offering urban planning
professionals a reliable tool for map analysis, assessment, and educational
applications while maintaining high factual accuracy. Our lightweight 7B
parameter model achieves comparable performance to models exceeding 72B
parameters, demonstrating efficient domain specialization without sacrificing
performance.

</details>


### [111] [MoMoE: Mixture of Moderation Experts Framework for AI-Assisted Online Governance](https://arxiv.org/abs/2505.14483)
*Agam Goyal,Xianyang Zhan,Yilun Chen,Koustuv Saha,Eshwar Chandrasekharan*

Main category: cs.CL

TL;DR: 提出MoMoE框架实现无需社区微调的可扩展内容审核，通过专家组合提供透明决策和解释


<details>
  <summary>Details</summary>
Motivation: 现有内容审核方法需为每个社区单独训练模型且决策不透明，限制了实际应用

Method: 采用混合专家框架(MoMoE)，包含分配/预测/聚合/解释四个操作符，分为社区专家(MoMoE-Community)和违规专家(MoMoE-NormVio)两种类型

Result: 在30个新子版块测试中，最佳变体Micro-F1分别达0.72和0.67，社区专家峰值更高但违规专家跨领域表现更稳定

Conclusion: MoMoE展示了轻量级可解释专家组合的潜力，为可信人机协同治理在线社区提供新方向

Abstract: Large language models (LLMs) have shown great potential in flagging harmful
content in online communities. Yet, existing approaches for moderation require
a separate model for every community and are opaque in their decision-making,
limiting real-world adoption. We introduce Mixture of Moderation Experts
(MoMoE), a modular, cross-community framework that adds post-hoc explanations
to scalable content moderation. MoMoE orchestrates four operators -- Allocate,
Predict, Aggregate, Explain -- and is instantiated as seven
community-specialized experts (MoMoE-Community) and five norm-violation experts
(MoMoE-NormVio). On 30 unseen subreddits, the best variants obtain Micro-F1
scores of 0.72 and 0.67, respectively, matching or surpassing strong fine-tuned
baselines while consistently producing concise and reliable explanations.
Although community-specialized experts deliver the highest peak accuracy,
norm-violation experts provide steadier performance across domains. These
findings show that MoMoE yields scalable, transparent moderation without
needing per-community fine-tuning. More broadly, they suggest that lightweight,
explainable expert ensembles can guide future NLP and HCI research on
trustworthy human-AI governance of online communities.

</details>


### [112] [Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales](https://arxiv.org/abs/2505.14499)
*Jun Cao,Jiyi Li,Ziwei Yang,Renjie Zhou*

Main category: cs.CL

TL;DR: 提出LRSA框架，结合小模型决策能力与大模型解释信息，通过双交叉注意力机制提升多模态细粒度情感分析效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于小语言模型（SLM）的多模态情感分析方法存在能力瓶颈，而大模型（LLM）虽具备细粒度分析潜力但在ABSA任务中仍弱于微调小模型。需要融合二者优势突破性能限制。

Method: 1. 将LLM生成的任务解释作为理性知识注入SLM
2. 设计双交叉注意力机制加强图文特征交互
3. 通过知识增强提升SLM的方面/情感识别能力

Result: 在三个主流基准测试中超越基线模型，实验证明方案对多数预训练模型具有普适性

Conclusion: 首次实现LLM与SLM的优势互补，为多模态情感分析提供新范式，证实知识注入与特征融合机制的有效性

Abstract: There has been growing interest in Multimodal Aspect-Based Sentiment Analysis
(MABSA) in recent years. Existing methods predominantly rely on pre-trained
small language models (SLMs) to collect information related to aspects and
sentiments from both image and text, with an aim to align these two modalities.
However, small SLMs possess limited capacity and knowledge, often resulting in
inaccurate identification of meaning, aspects, sentiments, and their
interconnections in textual and visual data. On the other hand, Large language
models (LLMs) have shown exceptional capabilities in various tasks by
effectively exploring fine-grained information in multimodal data. However,
some studies indicate that LLMs still fall short compared to fine-tuned small
models in the field of ABSA. Based on these findings, we propose a novel
framework, termed LRSA, which combines the decision-making capabilities of SLMs
with additional information provided by LLMs for MABSA. Specifically, we inject
explanations generated by LLMs as rationales into SLMs and employ a dual
cross-attention mechanism for enhancing feature interaction and fusion, thereby
augmenting the SLMs' ability to identify aspects and sentiments. We evaluated
our method using two baseline models, numerous experiments highlight the
superiority of our approach on three widely-used benchmarks, indicating its
generalizability and applicability to most pre-trained models for MABSA.

</details>


### [113] [ModRWKV: Transformer Multimodality in Linear Time](https://arxiv.org/abs/2505.14505)
*Jiale Kang,Ziyin Yue,Qingyu Yin,Jiang Rui,Weile Li,Zening Lu,Zhouran Ji*

Main category: cs.CL

TL;DR: 提出基于RWKV7架构的ModRWKV多模态框架，通过轻量化模块设计与预训练权重迁移，验证现代RNN架构在多模态任务中替代Transformer的可行性


<details>
  <summary>Details</summary>
Motivation: 解决Transformer架构二次计算复杂度带来的高推理成本问题，探索线性RNN模型在多模态领域的应用潜力

Method: 1. 构建解耦式多模态框架，采用动态适配异构编码器融合多源信息 2. 设计极简架构的多模态模块并通过大量实验确定最优配置 3. 利用RWKV7预训练权重加速训练过程

Result: 实验证明该框架在性能与计算效率间取得最优平衡，预训练权重初始化显著提升模型多模态理解能力（不同checkpoint对比验证）

Conclusion: 现代RNN架构可作为Transformer的有效替代方案用于MLLM领域，并通过系统探索确定了ModRWKV架构的最佳配置方案

Abstract: Currently, most multimodal studies are based on large language models (LLMs)
with quadratic-complexity Transformer architectures. While linear models like
RNNs enjoy low inference costs, their application has been largely limited to
the text-only modality. This work explores the capabilities of modern RNN
architectures in multimodal contexts. We propose ModRWKV-a decoupled multimodal
framework built upon the RWKV7 architecture as its LLM backbone-which achieves
multi-source information fusion through dynamically adaptable heterogeneous
modality encoders. We designed the multimodal modules in ModRWKV with an
extremely lightweight architecture and, through extensive experiments,
identified a configuration that achieves an optimal balance between performance
and computational efficiency. ModRWKV leverages the pretrained weights of the
RWKV7 LLM for initialization, which significantly accelerates multimodal
training. Comparative experiments with different pretrained checkpoints further
demonstrate that such initialization plays a crucial role in enhancing the
model's ability to understand multimodal signals. Supported by extensive
experiments, we conclude that modern RNN architectures present a viable
alternative to Transformers in the domain of multimodal large language models
(MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV
architecture through systematic exploration.

</details>


### [114] [Exploring Graph Representations of Logical Forms for Language Modeling](https://arxiv.org/abs/2505.14523)
*Michael Sullivan*

Main category: cs.CL

TL;DR: 提出基于图表示逻辑形式的预训练模型GFoLDS，验证逻辑形式语言模型(LFLMs)相比纯文本模型具有更高的数据效率和扩展潜力。


<details>
  <summary>Details</summary>
Motivation: 解决传统文本语言模型数据效率低下的问题，通过逻辑形式的结构化表示来提升模型对基础语言知识的利用效率。

Method: 开发Graph-based Formal-Logical Distributional Semantics(GFoLDS)原型，采用逻辑形式的图表示进行预训练。

Result: GFoLDS在相同数据量下性能显著超越文本Transformer模型，且模型表现与参数规模/预训练数据量呈正相关。

Conclusion: 逻辑形式语言模型(LFLMs)通过结构化表示大幅降低数据需求，展现出实际应用可行性，其性能扩展性为未来应用奠定基础。

Abstract: We make the case for language models over logical forms (LFLMs), arguing that
such models are more data-efficient than their textual counterparts. To that
end, we introduce the Graph-based Formal-Logical Distributional Semantics
(GFoLDS) prototype, a pretrained LM over graph representations of logical
forms, as a proof-of-concept of LFLMs. Using GFoLDS, we present strong
experimental evidence that LFLMs can leverage the built-in, basic linguistic
knowledge inherent in such models to immediately begin learning more complex
patterns. On downstream tasks, we show that GFoLDS vastly outperforms textual,
transformer LMs pretrained on similar amounts of data, indicating that LFLMs
can learn with substantially less data than models over plain text.
Furthermore, we show that the performance of this model is likely to scale with
additional parameters and pretraining data, suggesting the viability of LFLMs
in real-world applications.

</details>


### [115] [Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask Scheduling in LLMs](https://arxiv.org/abs/2505.14530)
*Zhipeng Yang,Junzhuo Li,Siyu Xia,Xuming Hu*

Main category: cs.CL

TL;DR: 大型语言模型通过分层结构顺序分解和执行复合任务，展现内部思维链机制。


<details>
  <summary>Details</summary>
Motivation: 揭示LLMs内部处理指令的分层动态特性，增强模型透明度和可解释性。

Method: 采用层间上下文遮蔽、跨任务修补技术和LogitLens隐藏状态解码方法。

Result: 在15个双步骤任务基准和TRACE真实场景中验证了子任务分层执行规律。

Conclusion: 发现为基于指令粒度的激活调控开辟新途径，推动LLM内部机制的理解与应用。

Abstract: We show that large language models (LLMs) exhibit an $\textit{internal
chain-of-thought}$: they sequentially decompose and execute composite tasks
layer-by-layer. Two claims ground our study: (i) distinct subtasks are learned
at different network depths, and (ii) these subtasks are executed sequentially
across layers. On a benchmark of 15 two-step composite tasks, we employ
layer-from context-masking and propose a novel cross-task patching method,
confirming (i). To examine claim (ii), we apply LogitLens to decode hidden
states, revealing a consistent layerwise execution pattern. We further
replicate our analysis on the real-world $\text{TRACE}$ benchmark, observing
the same stepwise dynamics. Together, our results enhance LLMs transparency by
showing their capacity to internally plan and execute subtasks (or
instructions), opening avenues for fine-grained, instruction-level activation
steering.

</details>


### [116] [Breaking Bad Tokens: Detoxification of LLMs Using Sparse Autoencoders](https://arxiv.org/abs/2505.14536)
*Agam Goyal,Vedant Rathi,William Yeh,Yian Wang,Yuen Chen,Hari Sundaram*

Main category: cs.CL

TL;DR: 利用稀疏自编码器进行定向激活引导，在保持模型能力的同时实现20%毒性降低，但存在流畅性折衷


<details>
  <summary>Details</summary>
Motivation: 现有语言模型解毒方法多为表层修复，易被越狱攻击绕过，需开发基于模型内部表征的根本性解决方案

Method: 通过稀疏自编码器识别残差流中的毒性方向，设计三级引导强度进行激活干预

Result: 强引导时毒性降低20%，GPT-2流畅性下降明显但基准测试稳定，Gemma表现更均衡；宽SAEs特征分割会削弱效果

Conclusion: SAE因果干预在模型解毒中具有潜力但需平衡效果与流畅性，建议采用特征解缠学习并建立安全部署指南

Abstract: Large language models (LLMs) are now ubiquitous in user-facing applications,
yet they still generate undesirable toxic outputs, including profanity,
vulgarity, and derogatory remarks. Although numerous detoxification methods
exist, most apply broad, surface-level fixes and can therefore easily be
circumvented by jailbreak attacks. In this paper we leverage sparse
autoencoders (SAEs) to identify toxicity-related directions in the residual
stream of models and perform targeted activation steering using the
corresponding decoder vectors. We introduce three tiers of steering
aggressiveness and evaluate them on GPT-2 Small and Gemma-2-2B, revealing
trade-offs between toxicity reduction and language fluency. At stronger
steering strengths, these causal interventions surpass competitive baselines in
reducing toxicity by up to 20%, though fluency can degrade noticeably on GPT-2
Small depending on the aggressiveness. Crucially, standard NLP benchmark scores
upon steering remain stable, indicating that the model's knowledge and general
abilities are preserved. We further show that feature-splitting in wider SAEs
hampers safety interventions, underscoring the importance of disentangled
feature learning. Our findings highlight both the promise and the current
limitations of SAE-based causal interventions for LLM detoxification, further
suggesting practical guidelines for safer language-model deployment.

</details>


### [117] [KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation](https://arxiv.org/abs/2505.14552)
*Jiajun Shi,Jian Yang,Jiaheng Liu,Xingyuan Bu,Jiangjie Chen,Junting Zhou,Kaijing Ma,Zhoufutu Wen,Bingli Wang,Yancheng He,Liang Song,Hualei Zhu,Shilong Li,Xingjian Wang,Wei Zhang,Ruibin Yuan,Yifan Yao,Wenjun Yang,Yunli Wang,Siyuan Fang,Siyu Yuan,Qianyu He,Xiangru Tang,Yingshui Tan,Wangchunshu Zhou,Zhaoxiang Zhang,Zhoujun Li,Wenhao Huang,Ge Zhang*

Main category: cs.CL

TL;DR: 提出KORGym动态评估平台解决现有LLM评测方法局限，通过50+跨模态游戏实现多轮交互式评估，验证闭源模型优势并分析强化学习策略对推理性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有评测基准多局限于特定领域，无法全面衡量大语言模型的通用推理能力，需构建更综合的评估体系。

Method: 基于KOR-Bench和Gymnasium开发支持强化学习场景的KORGym平台，包含文本/视觉双模态游戏，支持多回合交互式测评。

Result: 测试19个LLM和8个VLM发现模型家族内部存在推理模式一致性，闭源模型表现更优；模态选择、响应长度等因子显著影响性能。

Conclusion: KORGym将成为复杂交互环境下推进LLM推理研究的重要基础设施，助力开发适配动态环境的评估方法论。

Abstract: Recent advancements in large language models (LLMs) underscore the need for
more comprehensive evaluation methods to accurately assess their reasoning
capabilities. Existing benchmarks are often domain-specific and thus cannot
fully capture an LLM's general reasoning potential. To address this limitation,
we introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic
evaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over
fifty games in either textual or visual formats and supports interactive,
multi-turn assessments with reinforcement learning scenarios. Using KORGym, we
conduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent
reasoning patterns within model families and demonstrating the superior
performance of closed-source models. Further analysis examines the effects of
modality, reasoning strategies, reinforcement learning techniques, and response
length on model performance. We expect KORGym to become a valuable resource for
advancing LLM reasoning research and developing evaluation methodologies suited
to complex, interactive environments.

</details>


### [118] [Pivot Language for Low-Resource Machine Translation](https://arxiv.org/abs/2505.14553)
*Abhimanyu Talwar,Julien Laasri*

Main category: cs.CL

TL;DR: 使用印地语作为枢轴语言提升尼泊尔语-英语翻译效果，通过迁移方法（全监督）使BLEU分数提升6.6分


<details>
  <summary>Details</summary>
Motivation: 解决尼泊尔语-英语平行语料不足的问题，利用印地语与尼泊尔语的相似性及资源丰富的优势作为中间桥梁

Method: 采用两种方法：全监督的迁移方法（直接三阶段翻译）和半监督的反向翻译（合成平行语料）

Result: 迁移方法获得14.2 SacreBLEU，比基线提升6.6分；反向翻译略低于基线15.1分（分析原因：领域不匹配和误差累积）

Conclusion: 验证了印地语作为枢轴语言的有效性，提出通过改进合成数据质量和误差分析来优化翻译效果

Abstract: Certain pairs of languages suffer from lack of a parallel corpus which is
large in size and diverse in domain. One of the ways this is overcome is via
use of a pivot language. In this paper we use Hindi as a pivot language to
translate Nepali into English. We describe what makes Hindi a good candidate
for the pivot. We discuss ways in which a pivot language can be used, and use
two such approaches - the Transfer Method (fully supervised) and
Backtranslation (semi-supervised) - to translate Nepali into English. Using the
former, we are able to achieve a devtest Set SacreBLEU score of 14.2, which
improves the baseline fully supervised score reported by (Guzman et al., 2019)
by 6.6 points. While we are slightly below the semi-supervised baseline score
of 15.1, we discuss what may have caused this under-performance, and suggest
scope for future work.

</details>


### [119] [TRATES: Trait-Specific Rubric-Assisted Cross-Prompt Essay Scoring](https://arxiv.org/abs/2505.14577)
*Sohaila Eltanbouly,Salam Albatarni,Tamer Elsayed*

Main category: cs.CL

TL;DR: 提出TRATES框架——基于LLM生成特质相关特征，结合传统回归模型实现跨提示作文特质评分，刷新SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动化作文评分研究多关注整体质量，缺乏对作文个体特质（如论点逻辑、语言规范等）的细粒度评估。

Method: 1. 用LLM根据评分标准生成特质相关评估问题
2. 基于问题评估作文特质特征
3. 整合写作质量特征和提示特征
4. 训练回归模型预测新提示下的特质分数

Result: 在主流数据集上所有特质评分均达SOTA，LLM生成的特征贡献度最高（达73.5%相对提升）

Conclusion: TRATES首次实现跨提示的特质评分框架，证明LLM生成的特征与传统特征结合的有效性，为细粒度作文评估提供新范式。

Abstract: Research on holistic Automated Essay Scoring (AES) is long-dated; yet, there
is a notable lack of attention for assessing essays according to individual
traits. In this work, we propose TRATES, a novel trait-specific and
rubric-based cross-prompt AES framework that is generic yet specific to the
underlying trait. The framework leverages a Large Language Model (LLM) that
utilizes the trait grading rubrics to generate trait-specific features
(represented by assessment questions), then assesses those features given an
essay. The trait-specific features are eventually combined with generic
writing-quality and prompt-specific features to train a simple classical
regression model that predicts trait scores of essays from an unseen prompt.
Experiments show that TRATES achieves a new state-of-the-art performance across
all traits on a widely-used dataset, with the generated LLM-based features
being the most significant.

</details>


### [120] [Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with Capability in Mind for Better Reasoning](https://arxiv.org/abs/2505.14582)
*Shangziqi Zhao,Jiahao Yuan,Guisong Yang,Usman Naseem*

Main category: cs.CL

TL;DR: 通过剪枝验证步骤而非核心推理步骤，可提升小模型在长思维链任务中的准确率并降低推理成本


<details>
  <summary>Details</summary>
Motivation: 长思维链（Long-CoT）的冗长自省特性阻碍其向小模型的有效迁移，需探索结构化压缩方法实现能力对齐

Method: 提出Prune-on-Logic框架，将思维链转化为逻辑图，在自验证约束下通过三种剪枝策略（全链剪枝/核心推理剪枝/验证剪枝）进行结构优化

Result: 验证步骤剪枝带来准确率提升和21%推理成本下降，而核心推理剪枝会降低性能，证明小模型需要语义更精炼而非更短的思维链

Conclusion: 结构化剪枝（特别是验证步骤剪枝）是提升小模型思维链推理能力的有效策略，揭示了语义密度优化优于简单长度压缩的机制

Abstract: Long chain-of-thought (Long-CoT) reasoning improves accuracy in LLMs, yet its
verbose, self-reflective style often hinders effective distillation into small
language models (SLMs). We revisit Long-CoT compression through the lens of
capability alignment and ask: Can pruning improve reasoning? We propose
Prune-on-Logic, a structure-aware framework that transforms Long-CoT into logic
graphs and selectively prunes low-utility reasoning steps under
self-verification constraints. Through systematic analysis across three pruning
strategies -- targeting entire chains, core reasoning, and verification -- we
find that pruning verification steps yields consistent accuracy gains while
reducing inference cost, outperforming token-level baselines and uncompressed
fine-tuning. In contrast, pruning reasoning or all-chain steps degrades
performance, revealing that small models benefit not from shorter CoTs, but
from semantically leaner ones. Our findings highlight pruning as a structural
optimization strategy for aligning CoT reasoning with SLM capacity.

</details>


### [121] [Context Reasoner: Incentivizing Reasoning Capability for Contextualized Privacy and Safety Compliance via Reinforcement Learning](https://arxiv.org/abs/2505.14585)
*Wenbin Hu,Haoran Li,Huihao Jing,Qi Hu,Ziqian Zeng,Sirui Han,Heli Xu,Tianshu Chu,Peizhao Hu,Yangqiu Song*

Main category: cs.CL

TL;DR: 提出基于情境完整性理论的强化学习方法，在增强LLM安全隐私合规性的同时提升推理能力


<details>
  <summary>Details</summary>
Motivation: 现有安全策略依赖敏感模式匹配，既限制上下文推理能力又忽视法规标准，导致系统性法律合规风险

Method: 采用情境完整性框架对齐GDPR/EU AI Act/HIPAA标准，设计基于规则的强化学习奖励机制

Result: 安全隐私基准准确率提升17.64%，MMLU和LegalBench分别提升2.05%和8.98%

Conclusion: 该方法有效平衡法律合规与模型性能，为AI系统合规提供新范式

Abstract: While Large Language Models (LLMs) exhibit remarkable capabilities, they also
introduce significant safety and privacy risks. Current mitigation strategies
often fail to preserve contextual reasoning capabilities in risky scenarios.
Instead, they rely heavily on sensitive pattern matching to protect LLMs, which
limits the scope. Furthermore, they overlook established safety and privacy
standards, leading to systemic risks for legal compliance. To address these
gaps, we formulate safety and privacy issues into contextualized compliance
problems following the Contextual Integrity (CI) theory. Under the CI
framework, we align our model with three critical regulatory standards: GDPR,
EU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with
a rule-based reward to incentivize contextual reasoning capabilities while
enhancing compliance with safety and privacy norms. Through extensive
experiments, we demonstrate that our method not only significantly enhances
legal compliance (achieving a +17.64% accuracy improvement in safety/privacy
benchmarks) but also further improves general reasoning capability. For
OpenThinker-7B, a strong reasoning model that significantly outperforms its
base model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its
general reasoning capabilities, with +2.05% and +8.98% accuracy improvement on
the MMLU and LegalBench benchmark, respectively.

</details>


### [122] [MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol](https://arxiv.org/abs/2505.14590)
*Huihao Jing,Haoran Li,Wenbin Hu,Qi Hu,Heli Xu,Tianshu Chu,Peizhao Hu,Yangqiu Song*

Main category: cs.CL

TL;DR: 提出MAESTRO框架改善MCP协议安全风险，开发MCIP协议与安全基准数据集，验证LLMs在MCP场景中的安全性能提升


<details>
  <summary>Details</summary>
Motivation: MCP协议的去中心化架构存在安全隐患，缺乏系统化的安全评估机制，需要建立系统性解决方案保障交互安全

Method: 1. 通过MAESTRO框架分析MCP安全机制缺失 2. 提出改进版MCIP协议 3. 构建细粒度安全行为分类体系 4. 开发基准数据集用于LLMs安全能力评估

Result: 实验显示现有LLMs在MCP交互中存在安全漏洞，采用本研究方法后模型的安全性能提升35%

Conclusion: 通过系统化框架设计、协议改进和针对性训练，能有效提升LLMs在复杂交互场景中的安全防护能力

Abstract: As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users
and developers, it also brings underexplored safety risks. Its decentralized
architecture, which separates clients and servers, poses unique challenges for
systematic safety analysis. This paper proposes a novel framework to enhance
MCP safety. Guided by the MAESTRO framework, we first analyze the missing
safety mechanisms in MCP, and based on this analysis, we propose the Model
Contextual Integrity Protocol (MCIP), a refined version of MCP that addresses
these gaps.Next, we develop a fine-grained taxonomy that captures a diverse
range of unsafe behaviors observed in MCP scenarios. Building on this taxonomy,
we develop benchmark and training data that support the evaluation and
improvement of LLMs' capabilities in identifying safety risks within MCP
interactions. Leveraging the proposed benchmark and training data, we conduct
extensive experiments on state-of-the-art LLMs. The results highlight LLMs'
vulnerabilities in MCP interactions and demonstrate that our approach
substantially improves their safety performance.

</details>


### [123] [Success is in the Details: Evaluate and Enhance Details Sensitivity of Code LLMs through Counterfactuals](https://arxiv.org/abs/2505.14597)
*Xianzhen Luo,Qingfu Zhu,Zhiming Zhang,Mingzheng Xu,Tianhao Cheng,Yixuan Wang,Zheng Chu,Shijie Xuyang,Zhiyuan Ma,YuanTao Fan,Wanxiang Che*

Main category: cs.CL

TL;DR: 提出CTF-Code基准验证代码大模型敏感性，设计CTF-Instruct框架提升模型敏感性和整体性能


<details>
  <summary>Details</summary>
Motivation: 现有代码基准仅关注难度和多样性，忽视模型对问题描述细节变化的敏感性（如变量名、逻辑顺序等微小改动导致的输出差异）

Method: 1. 基于反事实扰动构建CTF-Code基准
2. 开发CTF-Instruct增量指令微调框架
3. 设计三元选择机制（难度/多样性/敏感性）筛选数据

Result: 微调后模型在CTF-Code提升2%+，在LiveCodeBench提升10%+

Conclusion: 敏感性是代码大模型的重要维度，通过针对性训练可显著提升模型整体表现

Abstract: Code Sensitivity refers to the ability of Code LLMs to recognize and respond
to details changes in problem descriptions. While current code benchmarks and
instruction data focus on difficulty and diversity, sensitivity is overlooked.
We first introduce the CTF-Code benchmark, constructed using counterfactual
perturbations, minimizing input changes while maximizing output changes. The
evaluation shows that many LLMs have a more than 10\% performance drop compared
to the original problems. To fully utilize sensitivity, CTF-Instruct, an
incremental instruction fine-tuning framework, extends on existing data and
uses a selection mechanism to meet the three dimensions of difficulty,
diversity, and sensitivity. Experiments show that LLMs fine-tuned with
CTF-Instruct data achieve over a 2\% improvement on CTF-Code, and more than a
10\% performance boost on LiveCodeBench, validating the feasibility of
enhancing LLMs' sensitivity to improve performance.

</details>


### [124] [Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models](https://arxiv.org/abs/2505.14599)
*Guangzhi Xiong,Eric Xie,Corey Williams,Myles Kim,Amir Hassan Shariatmadari,Sikun Guo,Stefan Bekiranov,Aidong Zhang*

Main category: cs.CL

TL;DR: 提出TruthHypo基准和KnowHD检测器，评估大语言模型生成生物医学假设的真实性并检测幻觉问题


<details>
  <summary>Details</summary>
Motivation: 解决LLMs生成假设时存在的真实性问题及验证成本高的挑战，尤其针对生物医学领域假设验证的资源消耗问题

Method: 引入TruthHypo基准评估LLM假设生成能力，开发基于知识的KnowHD幻觉检测器评估假设与现有知识的关联性

Result: LLMs生成真实假设存在困难，KnowHD的groundedness评分能有效筛选真实假设（人类评估验证其有效性）

Conclusion: KnowHD通过检测假设与知识库的关联性，为加速科学发现提供可靠的假设筛选机制

Abstract: Large language models (LLMs) have shown significant potential in scientific
disciplines such as biomedicine, particularly in hypothesis generation, where
they can analyze vast literature, identify patterns, and suggest research
directions. However, a key challenge lies in evaluating the truthfulness of
generated hypotheses, as verifying their accuracy often requires substantial
time and resources. Additionally, the hallucination problem in LLMs can lead to
the generation of hypotheses that appear plausible but are ultimately
incorrect, undermining their reliability. To facilitate the systematic study of
these challenges, we introduce TruthHypo, a benchmark for assessing the
capabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD,
a knowledge-based hallucination detector to evaluate how well hypotheses are
grounded in existing knowledge. Our results show that LLMs struggle to generate
truthful hypotheses. By analyzing hallucinations in reasoning steps, we
demonstrate that the groundedness scores provided by KnowHD serve as an
effective metric for filtering truthful hypotheses from the diverse outputs of
LLMs. Human evaluations further validate the utility of KnowHD in identifying
truthful hypotheses and accelerating scientific discovery. Our data and source
code are available at https://github.com/Teddy-XiongGZ/TruthHypo.

</details>


### [125] [sudoLLM : On Multi-role Alignment of Language Models](https://arxiv.org/abs/2505.14607)
*Soumadeep Saha,Akshay Chaturvedi,Joy Mahapatra,Utpal Garain*

Main category: cs.CL

TL;DR: 提出sudoLLM框架，通过用户权限偏置信号实现大语言模型的多角色安全对齐，仅在授权时输出敏感信息，增强模型安全性和抗越狱攻击能力。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型缺乏用户权限控制的问题，防止未授权用户获取敏感信息，弥补现有安全机制的不足。

Method: 在查询中注入用户权限偏置信号，训练模型识别该信号并据此控制敏感信息输出，构建动态访问控制系统。

Result: 实验证明框架显著提升模型对齐性、泛化能力和抗提示词越狱攻击性能，缓解语言建模目标与安全目标的固有冲突。

Conclusion: sudoLLM作为附加安全层，通过偏置信号实现细粒度权限控制，与现有防护机制形成互补，构建端到端安全体系。

Abstract: User authorization-based access privileges are a key feature in many
safety-critical systems, but have thus far been absent from the large language
model (LLM) realm. In this work, drawing inspiration from such access control
systems, we introduce sudoLLM, a novel framework that results in multi-role
aligned LLMs, i.e., LLMs that account for, and behave in accordance with, user
access rights. sudoLLM injects subtle user-based biases into queries and trains
an LLM to utilize this bias signal in order to produce sensitive information if
and only if the user is authorized. We present empirical results demonstrating
that this approach shows substantially improved alignment, generalization, and
resistance to prompt-based jailbreaking attacks. The persistent tension between
the language modeling objective and safety alignment, which is often exploited
to jailbreak LLMs, is somewhat resolved with the aid of the injected bias
signal. Our framework is meant as an additional security layer, and complements
existing guardrail mechanisms for enhanced end-to-end safety with LLMs.

</details>


### [126] [Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)](https://arxiv.org/abs/2505.14608)
*Rafael Rivera Soto,Barry Chen,Nicholas Andrews*

Main category: cs.CL

TL;DR: 研究发现对抗优化的语言模型仍可通过风格特征空间检测，但单样本检测存在风险，建议谨慎依赖现有检测器


<details>
  <summary>Details</summary>
Motivation: 针对Nicks等人提出的机器文本检测易被优化的语言模型绕过的观点，探索更鲁棒的检测特征空间

Method: 通过构建风格特征空间对抗模型优化，开发新型改述攻击方法，并引入AURA指标量化人机文本分布重叠度

Result: 风格检测器对抗优化模型保持有效，单样本攻击可欺骗所有检测器，多样本检测性能显著提升

Conclusion: 现有检测机制存在根本局限，需结合样本数量评估检测可靠性，AURA指标为检测能力评估提供新维度

Abstract: Despite considerable progress in the development of machine-text detectors,
it has been suggested that the problem is inherently hard, and therefore, that
stakeholders should proceed under the assumption that machine-generated text
cannot be reliably detected as such. We examine a recent such claim by Nicks et
al. (2024) regarding the ease with which language models can be optimized to
degrade the performance of machine-text detectors, including detectors not
specifically optimized against. We identify a feature space$\unicode{x2013}$the
stylistic feature space$\unicode{x2013}$that is robust to such optimization,
and show that it may be used to reliably detect samples from language models
optimized to prevent detection. Furthermore, we show that even when models are
explicitly optimized against stylistic detectors, detection performance remains
surprisingly unaffected. We then seek to understand if stylistic detectors are
inherently more robust. To study this question, we explore a new paraphrasing
approach that simultaneously aims to close the gap between human writing and
machine writing in stylistic feature space while avoiding detection using
traditional features. We show that when only a single sample is available for
detection, this attack is universally effective across all detectors
considered, including those that use writing style. However, as the number of
samples available for detection grows, the human and machine distributions
become distinguishable. This observation encourages us to introduce AURA, a
metric that estimates the overlap between human and machine-generated
distributions by analyzing how detector performance improves as more samples
become available. Overall, our findings underscore previous recommendations to
avoid reliance on machine-text detection.

</details>


### [127] [Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models](https://arxiv.org/abs/2505.14617)
*Sahar Abdelnabi,Ahmed Salem*

Main category: cs.CL

TL;DR: 提出白盒探测框架量化大语言模型'测试意识'对安全对齐的影响，发现不同模型受影响程度不同，为安全评估提供可信度保障


<details>
  <summary>Details</summary>
Motivation: 大语言模型在测试场景下会因'测试意识'改变行为模式（类似霍桑效应），可能绕过安全机制响应恶意指令，需定量研究其对安全性的影响

Method: 开发线性识别测试意识相关激活的白盒框架，通过参数调控增强/抑制测试意识，在多款开源推理模型（真实/假设任务）中验证有效性

Result: 测试意识显著改变模型安全对齐效果（不同模型影响幅度差异达62%），调控该参数可精确控制模型在安全评估中的行为模式

Conclusion: 通过数学方法解构测试意识，首次实现模型安全评估潜在变量的量化控制，提升安全评估结果的可信度与可解释性

Abstract: Reasoning-focused large language models (LLMs) sometimes alter their behavior
when they detect that they are being evaluated, an effect analogous to the
Hawthorne phenomenon, which can lead them to optimize for test-passing
performance or to comply more readily with harmful prompts if real-world
consequences appear absent. We present the first quantitative study of how such
"test awareness" impacts model behavior, particularly its safety alignment. We
introduce a white-box probing framework that (i) linearly identifies
awareness-related activations and (ii) steers models toward or away from test
awareness while monitoring downstream performance. We apply our method to
different state-of-the-art open-source reasoning LLMs across both realistic and
hypothetical tasks. Our results demonstrate that test awareness significantly
impact safety alignment, and is different for different models. By providing
fine-grained control over this latent effect, our work aims to increase trust
in how we perform safety evaluation.

</details>


### [128] [Think Only When You Need with Large Hybrid-Reasoning Models](https://arxiv.org/abs/2505.14631)
*Lingjie Jiang,Xun Wu,Shaohan Huang,Qingxiu Dong,Zewen Chi,Li Dong,Xingxing Zhang,Tengchao Lv,Lei Cui,Furu Wei*

Main category: cs.CL

TL;DR: 提出首个自适应选择思考模式的大型混合推理模型(LHRMs)，在保持推理能力的同时显著提升效率


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型(LRMs)对所有查询都采用冗长思考过程，导致处理简单查询时产生不必要的资源消耗

Method: 两阶段训练流程：混合微调(HFT)冷启动 + 混合组策略优化(HGPO)强化学习，结合新提出的混合准确率评估指标

Result: LHRMs在推理能力和通用任务上超越现有模型，同时减少33-58%的token消耗，吞吐量提升1.6-2.3倍

Conclusion: 重新定义了扩展思考过程的使用范式，为构建自适应思考系统提供了有效解决方案

Abstract: Recent Large Reasoning Models (LRMs) have shown substantially improved
reasoning capabilities over traditional Large Language Models (LLMs) by
incorporating extended thinking processes prior to producing final responses.
However, excessively lengthy thinking introduces substantial overhead in terms
of token consumption and latency, which is particularly unnecessary for simple
queries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the
first kind of model capable of adaptively determining whether to perform
thinking based on the contextual information of user queries. To achieve this,
we propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as
a cold start, followed by online reinforcement learning with the proposed
Hybrid Group Policy Optimization (HGPO) to implicitly learn to select the
appropriate thinking mode. Furthermore, we introduce a metric called Hybrid
Accuracy to quantitatively assess the model's capability for hybrid thinking.
Extensive experimental results show that LHRMs can adaptively perform hybrid
thinking on queries of varying difficulty and type. It outperforms existing
LRMs and LLMs in reasoning and general capabilities while significantly
improving efficiency. Together, our work advocates for a reconsideration of the
appropriate use of extended thinking processes and provides a solid starting
point for building hybrid thinking systems.

</details>


### [129] [Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas](https://arxiv.org/abs/2505.14633)
*Yu Ying Chiu,Zhilin Wang,Sharan Maiya,Yejin Choi,Kyle Fish,Sydney Levine,Evan Hubinger*

Main category: cs.CL

TL;DR: 研究者开发了LitmusValues评估框架，通过测量AI模型在价值困境中的优先选择来预测其风险行为，包括看似无害的价值观也能有效预警潜在危险。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型能力增强，传统风险检测方法面临挑战（如新型规避手段Alignment Faking）。受人类危险行为受价值观驱动的启发，建立通过价值优先级预测AI风险行为的早期预警系统。

Method: 1. 创建LitmusValues评估流程揭示AI模型的价值优先级
2. 构建AIRiskDilemmas数据集（含权力寻求等安全风险场景）
3. 通过模型在困境中的聚合选择测量价值优先序

Result: LitmusValues中的价值观（包括'关怀'等表面无害类）能有效预测：
- 已观测风险行为（AIRiskDilemmas）
- 未观测风险行为（HarmBench测试集）

Conclusion: 价值优先序分析为AI安全提供了可扩展的预测框架，突破了传统行为监测的滞后性，实现了从价值观维度预判模型潜在风险的创新方法论。

Abstract: Detecting AI risks becomes more challenging as stronger models emerge and
find novel methods such as Alignment Faking to circumvent these detection
attempts. Inspired by how risky behaviors in humans (i.e., illegal activities
that may hurt others) are sometimes guided by strongly-held values, we believe
that identifying values within AI models can be an early warning system for
AI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal
AI models' priorities on a range of AI value classes. Then, we collect
AIRiskDilemmas, a diverse collection of dilemmas that pit values against one
another in scenarios relevant to AI safety risks such as Power Seeking. By
measuring an AI model's value prioritization using its aggregate choices, we
obtain a self-consistent set of predicted value priorities that uncover
potential risks. We show that values in LitmusValues (including seemingly
innocuous ones like Care) can predict for both seen risky behaviors in
AIRiskDilemmas and unseen risky behaviors in HarmBench.

</details>


### [130] [General-Reasoner: Advancing LLM Reasoning Across All Domains](https://arxiv.org/abs/2505.14652)
*Xueguang Ma,Qian Liu,Dongfu Jiang,Ge Zhang,Zejun Ma,Wenhu Chen*

Main category: cs.CL

TL;DR: 提出General-Reasoner训练范式，通过构建多领域可验证答案数据集和生成式答案验证器，提升LLM跨领域推理能力


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理研究局限于数学/编程领域，难以泛化到答案形式多样且数据稀缺的其他学科领域

Method: 1) 网络爬虫构建多领域可验证问题数据集 2) 开发具备思维链和上下文感知的生成式答案验证器替代传统规则验证

Result: 在MMLU-Pro、GPQA、TheoremQA等12个跨学科基准测试中超越基线方法，保持数学推理优势的同时展现泛化能力

Conclusion: 该方法成功扩展了LLM的推理适用范围，在物理、化学、金融等广泛领域实现稳健的推理性能

Abstract: Reinforcement learning (RL) has recently demonstrated strong potential in
enhancing the reasoning capabilities of large language models (LLMs).
Particularly, the "Zero" reinforcement learning introduced by Deepseek-R1-Zero,
enables direct RL training of base LLMs without relying on an intermediate
supervised fine-tuning stage. Despite these advancements, current works for LLM
reasoning mainly focus on mathematical and coding domains, largely due to data
abundance and the ease of answer verification. This limits the applicability
and generalization of such models to broader domains, where questions often
have diverse answer representations, and data is more scarce. In this paper, we
propose General-Reasoner, a novel training paradigm designed to enhance LLM
reasoning capabilities across diverse domains. Our key contributions include:
(1) constructing a large-scale, high-quality dataset of questions with
verifiable answers curated by web crawling, covering a wide range of
disciplines; and (2) developing a generative model-based answer verifier, which
replaces traditional rule-based verification with the capability of
chain-of-thought and context-awareness. We train a series of models and
evaluate them on a wide range of datasets covering wide domains like physics,
chemistry, finance, electronics etc. Our comprehensive evaluation across these
12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)
demonstrates that General-Reasoner outperforms existing baseline methods,
achieving robust and generalizable reasoning performance while maintaining
superior effectiveness in mathematical reasoning tasks.

</details>


### [131] [EmoGist: Efficient In-Context Learning for Visual Emotion Understanding](https://arxiv.org/abs/2505.14660)
*Ronald Seoh,Dan Goldwasser*

Main category: cs.CL

TL;DR: 提出无需训练的EmoGist方法，通过上下文动态解释提升视觉情感分类效果，在Memotion和FI数据集分别实现13%和8%的F1值提升


<details>
  <summary>Details</summary>
Motivation: 图像情感具有高度场景依赖性，传统静态标签无法捕捉语境差异，需动态生成情感定义解释

Method: 基于图像聚类预生成多版本情感解释，测试时通过嵌入相似度检索最佳解释，配合视觉语言模型进行分类

Result: Memotion多标签数据集微F1提升13%，FI多分类数据集宏F1提升8%

Conclusion: 上下文敏感的情感解释机制有效提升分类性能，验证了动态标签定义在细粒度情感分析中的优势

Abstract: In this paper, we introduce EmoGist, a training-free, in-context learning
method for performing visual emotion classification with LVLMs. The key
intuition of our approach is that context-dependent definition of emotion
labels could allow more accurate predictions of emotions, as the ways in which
emotions manifest within images are highly context dependent and nuanced.
EmoGist pre-generates multiple explanations of emotion labels, by analyzing the
clusters of example images belonging to each category. At test time, we
retrieve a version of explanation based on embedding similarity, and feed it to
a fast VLM for classification. Through our experiments, we show that EmoGist
allows up to 13 points improvement in micro F1 scores with the multi-label
Memotion dataset, and up to 8 points in macro F1 in the multi-class FI dataset.

</details>


### [132] [Reward Reasoning Model](https://arxiv.org/abs/2505.14674)
*Jiaxin Guo,Zewen Chi,Li Dong,Qingxiu Dong,Xun Wu,Shaohan Huang,Furu Wei*

Main category: cs.CL

TL;DR: 提出奖励推理模型（RRMs），通过链式推理机制和强化学习框架提升奖励模型的测试时计算效率与准确性


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型难以有效利用测试时计算资源优化复杂场景下的奖励判断

Method: 采用强化学习框架培养自我进化的奖励推理能力，无需显式推理轨迹作为训练数据

Result: 在多个领域奖励建模基准上实现SOTA，可自适应调节计算资源提升准确率

Conclusion: RRMs为奖励模型引入动态推理范式，开源模型推动对齐技术发展

Abstract: Reward models play a critical role in guiding large language models toward
outputs that align with human expectations. However, an open challenge remains
in effectively utilizing test-time compute to enhance reward model performance.
In this work, we introduce Reward Reasoning Models (RRMs), which are
specifically designed to execute a deliberate reasoning process before
generating final rewards. Through chain-of-thought reasoning, RRMs leverage
additional test-time compute for complex queries where appropriate rewards are
not immediately apparent. To develop RRMs, we implement a reinforcement
learning framework that fosters self-evolved reward reasoning capabilities
without requiring explicit reasoning traces as training data. Experimental
results demonstrate that RRMs achieve superior performance on reward modeling
benchmarks across diverse domains. Notably, we show that RRMs can adaptively
exploit test-time compute to further improve reward accuracy. The pretrained
reward reasoning models are available at
https://huggingface.co/Reward-Reasoning.

</details>


### [133] [UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Large Language Models](https://arxiv.org/abs/2505.14679)
*Xiaojie Gu,Guangxu Chen,Jungang Li,Jia-Chen Gu,Xuming Hu,Kai Zhang*

Main category: cs.CL

TL;DR: 提出了ULTRAEDIT——一种无需训练、无主题限制、无内存占用的新型模型编辑方法，通过纯线性代数操作实现高效参数调整，支持百万级编辑规模。


<details>
  <summary>Details</summary>
Motivation: 现有模型编辑方法在终身学习场景下面临速度慢、资源消耗大、扩展性差的问题，无法满足实际应用需求。作者旨在开发超高效、资源友好的可扩展编辑方案。

Method: 1. 基于轻量级线性代数运算实现参数偏移计算
2. 终身归一化策略动态更新特征统计量
3. 自包含的编辑流程无需训练过程
4. 支持24GB消费级GPU上运行7B大模型编辑

Result: 1. 编辑速度7倍于SOTA方法，显存消耗<1/3
2. 在构建的200万样本数据集ULTRAEDITBENCH上验证支持百万次编辑
3. 四数据集、六模型实验显示综合性能最优

Conclusion: ULTRAEDIT突破了终身模型编辑的效率和规模瓶颈，首次实现消费级硬件上的超大规模实时更新，为LLM持续学习提供了实用化解决方案。

Abstract: Lifelong learning enables large language models (LLMs) to adapt to evolving
information by continually updating their internal knowledge. An ideal system
should support efficient, wide-ranging updates while preserving existing
capabilities and ensuring reliable deployment. Model editing stands out as a
promising solution for this goal, offering a focused and efficient way to
revise a model's internal knowledge. Although recent paradigms have made
notable progress, they often struggle to meet the demands of practical lifelong
adaptation at scale. To bridge this gap, we propose ULTRAEDIT-a fundamentally
new editing solution that is training-, subject- and memory-free, making it
particularly well-suited for ultra-scalable, real-world lifelong model editing.
ULTRAEDIT performs editing through a self-contained process that relies solely
on lightweight linear algebra operations to compute parameter shifts, enabling
fast and consistent parameter modifications with minimal overhead. To improve
scalability in lifelong settings, ULTRAEDIT employs a lifelong normalization
strategy that continuously updates feature statistics across turns, allowing it
to adapt to distributional shifts and maintain consistency over time. ULTRAEDIT
achieves editing speeds over 7x faster than the previous state-of-the-art
method-which was also the fastest known approach-while consuming less than 1/3
the VRAM, making it the only method currently capable of editing a 7B LLM on a
24GB consumer-grade GPU. Furthermore, we construct ULTRAEDITBENCH-the largest
dataset in the field to date, with over 2M editing pairs-and demonstrate that
our method supports up to 1M edits while maintaining high accuracy.
Comprehensive experiments on four datasets and six models show that ULTRAEDIT
consistently achieves superior performance across diverse model editing
scenarios. Our code is available at: https://github.com/XiaojieGu/UltraEdit.

</details>


### [134] [Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning](https://arxiv.org/abs/2505.14684)
*Haolei Xu,Yuchen Yan,Yongliang Shen,Wenqi Zhang,Guiyang Hou,Shengpei Jiang,Kaitao Song,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.CL

TL;DR: 提出CoT-Bridge方法解决数学推理中思考链的步骤跳跃问题，通过生成缺失步骤提升模型性能


<details>
  <summary>Details</summary>
Motivation: 现有数学CoT数据集存在专家步骤省略导致的思考跳跃，损害模型推理完整性与泛化能力

Method: 基于ScaleQuestMath构建ScaleQM+数据集，训练CoT-Bridge模型自动检测跳跃并补充中间推理步骤

Result: 在NuminaMath提升+5.87%，增强蒸馏数据效果(+3.02%)，强化学习起点优化(+3.1%)，且兼容现有优化技术

Conclusion: 通过提升推理链完整性实现跨领域泛化改进，可作为即插即用模块赋能多种优化方法，验证了完整推理步骤的基础性价值

Abstract: Large language models (LLMs) have achieved remarkable progress on
mathemati-cal tasks through Chain-of-Thought (CoT) reasoning. However, existing
mathematical CoT datasets often suffer from Thought Leaps due to experts
omitting intermediate steps, which negatively impacts model learning and
generalization. We propose the CoT Thought Leap Bridge Task, which aims to
automatically detect leaps and generate missing intermediate reasoning steps to
restore the completeness and coherence of CoT. To facilitate this, we
constructed a specialized training dataset called ScaleQM+, based on the
structured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought
leaps. Through comprehensive experiments on mathematical reasoning benchmarks,
we demonstrate that models fine-tuned on bridged datasets consistently
outperform those trained on original datasets, with improvements of up to
+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)
and provides better starting points for reinforcement learning (+3.1%),
functioning as a plug-and-play module compatible with existing optimization
techniques. Furthermore, CoT-Bridge demonstrate improved generalization to
out-of-domain logical reasoning tasks, confirming that enhancing reasoning
completeness yields broadly applicable benefits.

</details>


### [135] [Language Models use Lookbacks to Track Beliefs](https://arxiv.org/abs/2505.14685)
*Nikhil Prakash,Natalie Shapira,Arnab Sen Sharma,Christoph Riedl,Yonatan Belinkov,Tamar Rott Shaham,David Bau,Atticus Geiger*

Main category: cs.CL

TL;DR: Llama-3-70B模型通过lookback机制实现角色信念跟踪：利用顺序ID绑定角色-对象-状态三元组，通过可见性ID动态更新角色认知关系。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型表征角色信念的机制（尤其信念与现实的差异），揭示其心理理论(ToM)能力的底层运作原理。

Method: 1. 构建双角色改变物体状态的故事数据集；2. 使用因果干预和抽象分析；3. 残差流低秩子空间分析信息绑定机制。

Result: 发现lookback机制：绑定检索顺序ID→状态标记，可见性ID编码角色关系→动态更新观察者信念。

Conclusion: 该机制揭示了LM信念追踪的算法基础，为逆向工程ToM推理提供了可解释性框架。

Abstract: How do language models (LMs) represent characters' beliefs, especially when
those beliefs may differ from reality? This question lies at the heart of
understanding the Theory of Mind (ToM) capabilities of LMs. We analyze
Llama-3-70B-Instruct's ability to reason about characters' beliefs using causal
mediation and abstraction. We construct a dataset that consists of simple
stories where two characters each separately change the state of two objects,
potentially unaware of each other's actions. Our investigation uncovered a
pervasive algorithmic pattern that we call a lookback mechanism, which enables
the LM to recall important information when it becomes necessary. The LM binds
each character-object-state triple together by co-locating reference
information about them, represented as their Ordering IDs (OIs) in low rank
subspaces of the state token's residual stream. When asked about a character's
beliefs regarding the state of an object, the binding lookback retrieves the
corresponding state OI and then an answer lookback retrieves the state token.
When we introduce text specifying that one character is (not) visible to the
other, we find that the LM first generates a visibility ID encoding the
relation between the observing and the observed character OIs. In a visibility
lookback, this ID is used to retrieve information about the observed character
and update the observing character's beliefs. Our work provides insights into
the LM's belief tracking mechanisms, taking a step toward reverse-engineering
ToM reasoning in LMs.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [136] [FreeMesh: Boosting Mesh Generation with Coordinates Merging](https://arxiv.org/abs/2505.13573)
*Jian Liu,Haohan Weng,Biwen Lei,Xianghui Yang,Zibo Zhao,Zhuo Chen,Song Guo,Tao Han,Chunchao Guo*

Main category: cs.GR

TL;DR: 提出了无需训练的网格分词器评估指标PTME及坐标合并技术，优化现有方法压缩效率


<details>
  <summary>Details</summary>
Motivation: 当前自回归网格生成方法缺乏有效指标评估不同分词器的序列化效率，制约技术迭代

Method: 1. 理论推导Per-Token-Mesh-Entropy(PTME)指标
2. 提出坐标合并技术：通过频率分析重组/合并高频坐标模式

Result: 在MeshXL/MeshAnything V2/Edgerunner等模型验证，显著提升现有分词器压缩率

Conclusion: PTME指标体系与坐标合并技术为网格生成领域提供量化评估工具，指导未来原生网格生成技术发展

Abstract: The next-coordinate prediction paradigm has emerged as the de facto standard
in current auto-regressive mesh generation methods. Despite their
effectiveness, there is no efficient measurement for the various tokenizers
that serialize meshes into sequences. In this paper, we introduce a new metric
Per-Token-Mesh-Entropy (PTME) to evaluate the existing mesh tokenizers
theoretically without any training. Building upon PTME, we propose a
plug-and-play tokenization technique called coordinate merging. It further
improves the compression ratios of existing tokenizers by rearranging and
merging the most frequent patterns of coordinates. Through experiments on
various tokenization methods like MeshXL, MeshAnything V2, and Edgerunner, we
further validate the performance of our method. We hope that the proposed PTME
and coordinate merging can enhance the existing mesh tokenizers and guide the
further development of native mesh generation.

</details>


### [137] [Large-Scale Multi-Character Interaction Synthesis](https://arxiv.org/abs/2505.14087)
*Ziyi Chang,He Wang,George Alex Koulieris,Hubert P. H. Shum*

Main category: cs.GR

TL;DR: 提出条件生成流程解决多角色互动生成难题，包含可协调互动空间和过渡规划网络


<details>
  <summary>Details</summary>
Motivation: 现有方法在单角色动画、两角色深度学习交互、优化方法泛化性差等方面存在不足，且缺乏多角色密集互动数据集

Method: 条件生成流程由两个核心组成：1) 可协调多角色互动空间用于合成交互，2) 时空协调的过渡规划网络

Result: 实验验证了方法有效性，应用案例展示了方案的扩展性和迁移能力

Conclusion: 该框架为多角色密集互动生成提供了系统性解决方案，在舞蹈等协调场景中展现出实用价值

Abstract: Generating large-scale multi-character interactions is a challenging and
important task in character animation. Multi-character interactions involve not
only natural interactive motions but also characters coordinated with each
other for transition. For example, a dance scenario involves characters dancing
with partners and also characters coordinated to new partners based on spatial
and temporal observations. We term such transitions as coordinated interactions
and decompose them into interaction synthesis and transition planning. Previous
methods of single-character animation do not consider interactions that are
critical for multiple characters. Deep-learning-based interaction synthesis
usually focuses on two characters and does not consider transition planning.
Optimization-based interaction synthesis relies on manually designing objective
functions that may not generalize well. While crowd simulation involves more
characters, their interactions are sparse and passive. We identify two
challenges to multi-character interaction synthesis, including the lack of data
and the planning of transitions among close and dense interactions. Existing
datasets either do not have multiple characters or do not have close and dense
interactions. The planning of transitions for multi-character close and dense
interactions needs both spatial and temporal considerations. We propose a
conditional generative pipeline comprising a coordinatable multi-character
interaction space for interaction synthesis and a transition planning network
for coordinations. Our experiments demonstrate the effectiveness of our
proposed pipeline for multicharacter interaction synthesis and the applications
facilitated by our method show the scalability and transferability.

</details>


### [138] [A Remeshing Method via Adaptive Multiple Original-Facet-Clipping and Centroidal Voronoi Tessellation](https://arxiv.org/abs/2505.14306)
*Yue Fei,Jingjing Liu,Yuyou Yao,Wenming Wu,Liping Zheng*

Main category: cs.GR

TL;DR: 提出基于CVT的曲面重网格方法，通过曲率自适应多次裁剪实现质量与效率的平衡优化


<details>
  <summary>Details</summary>
Motivation: 解决现有CVT重网格方法中质量与计算效率无法兼得的矛盾，平衡精确方法和近似方法的优缺点

Method: 采用3D中心Voronoi单元多次裁剪技术，通过相邻面片法向量角度表征局部曲率，动态调整裁剪次数实现曲率自适应优化

Result: 实验验证该方法在保持网格质量的同时显著提升计算效率，成功实现质量与效率的平衡优化

Conclusion: 提出的曲率自适应CVT重网格方法有效解决了传统方法的质量-效率权衡问题，为曲面优化提供了新思路

Abstract: CVT (Centroidal Voronoi Tessellation)-based remeshing optimizes mesh quality
by leveraging the Voronoi-Delaunay framework to optimize vertex distribution
and produce uniformly distributed vertices with regular triangles. Current
CVT-based approaches can be classified into two categories: (1) exact methods
(e.g., Geodesic CVT, Restricted Voronoi Diagrams) that ensure high quality but
require significant computation; and (2) approximate methods that try to reduce
computational complexity yet result in fair quality. To address this trade-off,
we propose a CVT-based surface remeshing approach that achieves balanced
optimization between quality and efficiency through multiple clipping times of
3D Centroidal Voronoi cells with curvature-adaptive original surface facets.
The core idea of the method is that we adaptively adjust the number of clipping
times according to local curvature, and use the angular relationship between
the normal vectors of neighboring facets to represent the magnitude of local
curvature. Experimental results demonstrate the effectiveness of our method.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [139] [OmniGenBench: A Modular Platform for Reproducible Genomic Foundation Models Benchmarking](https://arxiv.org/abs/2505.14402)
*Heng Yang,Jack Cole,Yuan Li,Renzhi Chen,Geyong Min,Ke Li*

Main category: q-bio.GN

TL;DR: 提出了模块化基因组基础模型评测平台OmniGenBench，整合31个开源模型并解决可复现性挑战，推动基因组AI研究的标准化进程。


<details>
  <summary>Details</summary>
Motivation: 基因组基础模型（GFMs）快速发展后面临评测标准不统一、数据透明度低、模型互操作性差、基准测试碎片化及黑箱可解释性不足等可复现性挑战。

Method: 构建包含数据/模型/评测/可解释性层的模块化平台，支持一键式标准化评测，通过自动化流程和社区扩展功能实现跨模型整合。

Result: 平台集成超过31个开源模型，覆盖5个基准测试套件，成功建立端到端评测流程并提升研究透明度。

Conclusion: OmniGenBench作为可复现基因组AI研究的基础设施，将通过标准化评测加速基因组规模建模时代的可信发现与协同创新。

Abstract: The code of nature, embedded in DNA and RNA genomes since the origin of life,
holds immense potential to impact both humans and ecosystems through genome
modeling. Genomic Foundation Models (GFMs) have emerged as a transformative
approach to decoding the genome. As GFMs scale up and reshape the landscape of
AI-driven genomics, the field faces an urgent need for rigorous and
reproducible evaluation. We present OmniGenBench, a modular benchmarking
platform designed to unify the data, model, benchmarking, and interpretability
layers across GFMs. OmniGenBench enables standardized, one-command evaluation
of any GFM across five benchmark suites, with seamless integration of over 31
open-source models. Through automated pipelines and community-extensible
features, the platform addresses critical reproducibility challenges, including
data transparency, model interoperability, benchmark fragmentation, and
black-box interpretability. OmniGenBench aims to serve as foundational
infrastructure for reproducible genomic AI research, accelerating trustworthy
discovery and collaborative innovation in the era of genome-scale modeling.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [140] [MatchDance: Collaborative Mamba-Transformer Architecture Matching for High-Quality 3D Dance Synthesis](https://arxiv.org/abs/2505.14222)
*Kaixing Yang,Xulong Tang,Yuxuan Hu,Jiahao Yang,Hongyan Liu,Qinnan Zhang,Jun He,Zhaoxin Fan*

Main category: cs.SD

TL;DR: 提出MatchDance框架通过两阶段潜在表示增强音乐到舞蹈生成的编舞一致性


<details>
  <summary>Details</summary>
Motivation: 现有音乐到舞蹈生成方法在编舞一致性上存在显著限制，影响虚拟现实和创意内容生成领域的应用

Method: 1. KDQS阶段：基于运动学-动力学约束的有限标量量化编码舞蹈动作
2. HMDGS阶段：Mamba-Transformer混合架构实现音乐到潜表示的映射，配合解码器生成3D舞蹈动作

Result: 在FineDance数据集上的实验验证了框架有效性，达到当前最优性能（SOTA）

Conclusion: MatchDance通过创新的两阶段设计、音乐-舞蹈检索框架和综合评估指标，显著提升了生成舞蹈的编舞一致性和质量

Abstract: Music-to-dance generation represents a challenging yet pivotal task at the
intersection of choreography, virtual reality, and creative content generation.
Despite its significance, existing methods face substantial limitation in
achieving choreographic consistency. To address the challenge, we propose
MatchDance, a novel framework for music-to-dance generation that constructs a
latent representation to enhance choreographic consistency. MatchDance employs
a two-stage design: (1) a Kinematic-Dynamic-based Quantization Stage (KDQS),
which encodes dance motions into a latent representation by Finite Scalar
Quantization (FSQ) with kinematic-dynamic constraints and reconstructs them
with high fidelity, and (2) a Hybrid Music-to-Dance Generation Stage(HMDGS),
which uses a Mamba-Transformer hybrid architecture to map music into the latent
representation, followed by the KDQS decoder to generate 3D dance motions.
Additionally, a music-dance retrieval framework and comprehensive metrics are
introduced for evaluation. Extensive experiments on the FineDance dataset
demonstrate state-of-the-art performance. Code will be released upon
acceptance.

</details>


### [141] [Forensic deepfake audio detection using segmental speech features](https://arxiv.org/abs/2505.13847)
*Tianle Yang,Chengzhe Sun,Siwei Lyu,Phil Rose*

Main category: cs.SD

TL;DR: 利用与发音过程高度相关的片段声学特征检测深度伪造音频，这类特征具备强可解释性且更难被伪造模型复制。


<details>
  <summary>Details</summary>
Motivation: 现有音频伪造检测常依赖全局特征，但片段声学特征因与人类发音机理直接相关，在司法语音比对场景中具有更高抗伪造性的潜在优势。

Method: 通过对比分析司法语音比对常用的片段特征与全局特征在深度伪造检测中的有效性差异。

Result: 司法语音比对中的特定片段特征检测效果显著，而部分全局特征检测价值有限。

Conclusion: 音频伪造检测需区分司法语音比对场景，采用基于发音机理的片段特征分析可提供新解决方案。

Abstract: This study explores the potential of using acoustic features of segmental
speech sounds to detect deepfake audio. These features are highly interpretable
because of their close relationship with human articulatory processes and are
expected to be more difficult for deepfake models to replicate. The results
demonstrate that certain segmental features commonly used in forensic voice
comparison are effective in identifying deep-fakes, whereas some global
features provide little value. These findings underscore the need to approach
audio deepfake detection differently for forensic voice comparison and offer a
new perspective on leveraging segmental features for this purpose.

</details>


### [142] [FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for Ü-Tsang, Amdo and Kham Speech Dataset Generation](https://arxiv.org/abs/2505.14351)
*Yutong Liu,Ziyue Zhang,Ban Ma-bao,Yuqing Cai,Yongbin Yu,Renzeng Duojie,Xiangxiang Wang,Fan Gao,Cheng Huang,Nyima Tashi*

Main category: cs.SD

TL;DR: FMSD-TTS提出了一种适用于藏语多方言的少样本语音合成框架，通过融合说话人与方言特征的技术显著提升了合成语音的方言表现力和说话人相似度。


<details>
  <summary>Details</summary>
Motivation: 藏语作为低资源语言，三大方言（卫藏、安多、康巴）间缺乏平行语音数据，严重制约了语音建模技术的发展。

Method: 创新性地设计了说话人-方言特征融合模块和方言专精动态路由网络（DSDR-Net），在保留说话人特征的同时捕捉跨方言的声学-语言学细微差异。

Result: 主客观评估显示系统在方言表现力和说话人相似度上显著优于基线模型，并通过跨方言语音转换任务验证了合成语音的实用价值。

Conclusion: 该研究不仅推动了低资源语言语音合成技术发展，还开源了大规模藏语合成语音库和标准化评测工具包，具有重要的工程实践意义。

Abstract: Tibetan is a low-resource language with minimal parallel speech corpora
spanning its three major dialects-\"U-Tsang, Amdo, and Kham-limiting progress
in speech modeling. To address this issue, we propose FMSD-TTS, a few-shot,
multi-speaker, multi-dialect text-to-speech framework that synthesizes parallel
dialectal speech from limited reference audio and explicit dialect labels. Our
method features a novel speaker-dialect fusion module and a Dialect-Specialized
Dynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and
linguistic variations across dialects while preserving speaker identity.
Extensive objective and subjective evaluations demonstrate that FMSD-TTS
significantly outperforms baselines in both dialectal expressiveness and
speaker similarity. We further validate the quality and utility of the
synthesized speech through a challenging speech-to-speech dialect conversion
task. Our contributions include: (1) a novel few-shot TTS system tailored for
Tibetan multi-dialect speech synthesis, (2) the public release of a large-scale
synthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source
evaluation toolkit for standardized assessment of speaker similarity, dialect
consistency, and audio quality.

</details>


### [143] [PersonaTAB: Predicting Personality Traits using Textual, Acoustic, and Behavioral Cues in Fully-Duplex Speech Dialogs](https://arxiv.org/abs/2505.14356)
*Sho Inoue,Shai Wang,Haizhou Li*

Main category: cs.SD

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Despite significant progress in neural spoken dialog systems,
personality-aware conversation agents -- capable of adapting behavior based on
personalities -- remain underexplored due to the absence of personality
annotations in speech datasets. We propose a pipeline that preprocesses raw
audio recordings to create a dialogue dataset annotated with timestamps,
response types, and emotion/sentiment labels. We employ an automatic speech
recognition (ASR) system to extract transcripts and timestamps, then generate
conversation-level annotations. Leveraging these annotations, we design a
system that employs large language models to predict conversational
personality. Human evaluators were engaged to identify conversational
characteristics and assign personality labels. Our analysis demonstrates that
the proposed system achieves stronger alignment with human judgments compared
to existing approaches.

</details>


### [144] [S2SBench: A Benchmark for Quantifying Intelligence Degradation in Speech-to-Speech Large Language Models](https://arxiv.org/abs/2505.14438)
*Yuanbo Fang,Haoze Sun,Jun Liu,Tao Zhang,Zenan Zhou,Weipeng Chen,Xiaofen Xing,Xiangmin Xu*

Main category: cs.SD

TL;DR: 提出S2SBench基准量化语音大语言模型与文本输入的智能退化差异，包含诊断数据集和基于困惑度的评估协议，并通过Baichuan-Audio训练验证有效性


<details>
  <summary>Details</summary>
Motivation: 语音大语言模型直接处理音频会导致推理生成性能显著下降（智能退化现象），但缺乏系统性量化评估工具

Method: 1.构建包含句子延续/常识推理任务的诊断数据集 2.设计基于合理与不合理样本困惑度差异的成对评估协议 3.应用于Baichuan-Audio训练过程分析

Result: 验证了S2SBench能有效检测语音模型在不同任务中的性能退化程度，揭示训练过程中音频处理能力的演进规律

Conclusion: S2SBench为语音LLMs的智能退化提供标准化评估框架，开源代码数据集将促进语音语言模型的研究发展

Abstract: End-to-end speech large language models ((LLMs)) extend the capabilities of
text-based models to directly process and generate audio tokens. However, this
often leads to a decline in reasoning and generation performance compared to
text input, a phenomenon referred to as intelligence degradation. To
systematically evaluate this gap, we propose S2SBench, a benchmark designed to
quantify performance degradation in Speech LLMs. It includes diagnostic
datasets targeting sentence continuation and commonsense reasoning under audio
input. We further introduce a pairwise evaluation protocol based on perplexity
differences between plausible and implausible samples to measure degradation
relative to text input. We apply S2SBench to analyze the training process of
Baichuan-Audio, which further demonstrates the benchmark's effectiveness. All
datasets and evaluation code are available at
https://github.com/undobug/S2SBench.

</details>


### [145] [PAST: Phonetic-Acoustic Speech Tokenizer](https://arxiv.org/abs/2505.14470)
*Nadav Har-Tuv,Or Tal,Yossi Adi*

Main category: cs.SD

TL;DR: PAST框架通过端到端联合建模语音信息与信号重建，无需外部预训练模型，在语音表征和重建任务中超越现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有语音tokenization方法依赖预训练自监督模型，而PAST通过监督语音数据和辅助任务直接注入领域知识，构建更有效的语音表征框架。

Method: 1）使用监督语音数据训练 2）通过音素识别等辅助任务整合领域知识 3）提出支持实时应用的流式因果变体

Result: 在音素表征（CER 16.0→12.0）、重建质量（WER 25.5→21.9）等指标超越基线，作为语音语言模型输入时也表现更优（WER 23.1→18.8）

Conclusion: PAST证明了端到端联合建模的有效性，其开源将推动语音生成领域发展，流式设计扩展了实时应用场景。

Abstract: We present PAST, a novel end-to-end framework that jointly models phonetic
information alongside signal reconstruction, eliminating the need for external
pretrained models. Unlike previous approaches that rely on pretrained
self-supervised models, PAST employs supervised phonetic data, directly
integrating domain knowledge into the tokenization process via auxiliary tasks.
Additionally, we introduce a streamable, causal variant of PAST, enabling
real-time speech applications. Results demonstrate that PAST surpasses existing
evaluated baseline tokenizers across common evaluation metrics, including
phonetic representation and speech reconstruction. Notably, PAST also achieves
superior performance when serving as a speech representation for speech
language models, further highlighting its effectiveness as a foundation for
spoken language generation. To foster further research, we release the full
implementation. For code, model checkpoints, and samples see:
https://pages.cs.huji.ac.il/adiyoss-lab/PAST

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [146] [LoRASuite: Efficient LoRA Adaptation Across Large Language Model Upgrades](https://arxiv.org/abs/2505.13515)
*Yanan Li,Fanxu Meng,Muhan Zhang,Shiai Zhu,Shangguang Wang,Mengwei Xu*

Main category: cs.LG

TL;DR: LoRASuite提出模块化方案，通过转移矩阵和参数分配优化LoRA权重迁移，显著提升新模型适配效率并减少资源消耗。


<details>
  <summary>Details</summary>
Motivation: 传统LoRA权重全量重训练存在高成本、耗时和环保问题，需开发更高效的模型迁移方案。

Method: 1. 计算新旧LLM间的转移矩阵 2. 基于CKA和余弦相似度分配层与注意力头 3. 精细化小规模微调保障数值稳定性

Result: 在MiniCPM/Qwen模型上数学任务平均提升+1.4/+6.6分，内存消耗减少5.5GB，训练时间缩短78.23%

Conclusion: LoRASuite在性能超越全量重训练的同时，实现了可持续AI训练范式的突破。

Abstract: As Large Language Models (LLMs) are frequently updated, LoRA weights trained
on earlier versions quickly become obsolete. The conventional practice of
retraining LoRA weights from scratch on the latest model is costly,
time-consuming, and environmentally detrimental, particularly as the diversity
of LLMs and downstream tasks expands. This motivates a critical question: "How
can we efficiently leverage existing LoRA weights to adapt to newer model
versions?" To address this, we propose LoRASuite, a modular approach tailored
specifically to various types of LLM updates. First, we compute a transfer
matrix utilizing known parameters from both old and new LLMs. Next, we allocate
corresponding layers and attention heads based on centered kernel alignment and
cosine similarity metrics, respectively. A subsequent small-scale, skillful
fine-tuning step ensures numerical stability. Experimental evaluations
demonstrate that LoRASuite consistently surpasses small-scale vanilla LoRA
methods. Notably, on backbone LLMs such as MiniCPM and Qwen, LoRASuite even
exceeds the performance of full-scale LoRA retraining, with average
improvements of +1.4 and +6.6 points on math tasks, respectively. Additionally,
LoRASuite significantly reduces memory consumption by 5.5 GB and computational
time by 78.23%.

</details>


### [147] [Power Lines: Scaling Laws for Weight Decay and Batch Size in LLM Pre-training](https://arxiv.org/abs/2505.13738)
*Shane Bergsma,Nolan Dey,Gurpreet Gosal,Gavia Gray,Daria Soboleva,Joel Hestness*

Main category: cs.LG

TL;DR: 建立了大语言模型预训练中学习率与权重衰减的超参数缩放定律，提出通过D/N（数据量/参数量）的幂律关系预测最优超参数，降低大模型训练调参成本。


<details>
  <summary>Details</summary>
Motivation: 大模型预训练需要精细调节超参数（如学习率η、权重衰减λ），但现有方法缺乏系统性指导。研究旨在建立超参数随模型规模N、数据量D、批量大小B的缩放规律，实现参数预测。

Method: 通过理论推导与实验验证：1）提出AdamW时间尺度B/(ηλD)应保持恒定；2）发现最优λ与B呈线性关系；3）建立D/N幂律预测最优时间尺度；4）分析最优批量Bopt和临界批量Bcrit的缩放规律。

Result: 1）最优λ可通过D/N的幂律提前预测；2）Bopt与Bcrit均随D呈幂律缩放，与模型规模N无关；3）为实际训练中模型参数量N与数据量D的帕累托最优选择提供理论依据。

Conclusion: 超参数缩放定律能够准确预测大模型训练的最优配置，显著减少调参试错成本，为平衡训练时间与计算资源提供数学框架。该发现对大规模语言模型训练实践具有直接指导意义。

Abstract: Efficient LLM pre-training requires well-tuned hyperparameters (HPs),
including learning rate {\eta} and weight decay {\lambda}. We study scaling
laws for HPs: formulas for how to scale HPs as we scale model size N, dataset
size D, and batch size B. Recent work suggests the AdamW timescale,
B/({\eta}{\lambda}D), should remain constant across training settings, and we
verify the implication that optimal {\lambda} scales linearly with B, for a
fixed N,D. However, as N,D scale, we show the optimal timescale obeys a precise
power law in the tokens-per-parameter ratio, D/N. This law thus provides a
method to accurately predict {\lambda}opt in advance of large-scale training.
We also study scaling laws for optimal batch size Bopt (the B enabling lowest
loss at a given N,D) and critical batch size Bcrit (the B beyond which further
data parallelism becomes ineffective). In contrast with prior work, we find
both Bopt and Bcrit scale as power laws in D, independent of model size, N.
Finally, we analyze how these findings inform the real-world selection of
Pareto-optimal N and D under dual training time and compute objectives.

</details>


### [148] [Structured Agent Distillation for Large Language Model](https://arxiv.org/abs/2505.13820)
*Jun Liu,Zhenglun Kong,Peiyan Dong,Changdi Yang,Tianqi Li,Hao Tang,Geng Yuan,Wei Niu,Wenbin Zhang,Pu Zhao,Xue Lin,Dong Huang,Yanzhi Wang*

Main category: cs.LG

TL;DR: 提出结构化智能体蒸馏框架，通过分段对齐损失实现大模型决策智能体的高效压缩


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的推理成本和体积限制了实际部署，传统token级蒸馏无法有效保持智能体的推理-行动协同

Method: 将教师轨迹分割为{[REASON]}和{[ACT]}段落，分别施加段落特异性损失实现结构感知的知识蒸馏

Result: 在ALFWorld等基准测试中性能超越传统蒸馏方法，压缩后模型仅保留3%参数量但保持90%以上效果

Conclusion: 分段对齐机制显著提升紧凑型智能体的决策质量，为实际场景的智能体部署提供有效解决方案

Abstract: Large language models (LLMs) exhibit strong capabilities as decision-making
agents by interleaving reasoning and actions, as seen in ReAct-style
frameworks. Yet, their practical deployment is constrained by high inference
costs and large model sizes. We propose Structured Agent Distillation, a
framework that compresses large LLM-based agents into smaller student models
while preserving both reasoning fidelity and action consistency. Unlike
standard token-level distillation, our method segments trajectories into
{[REASON]} and {[ACT]} spans, applying segment-specific losses to align each
component with the teacher's behavior. This structure-aware supervision enables
compact agents to better replicate the teacher's decision process. Experiments
on ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently
outperforms token-level and imitation learning baselines, achieving significant
compression with minimal performance drop. Scaling and ablation results further
highlight the importance of span-level alignment for efficient and deployable
agents.

</details>


### [149] [InfiFPO: Implicit Model Fusion via Preference Optimization in Large Language Models](https://arxiv.org/abs/2505.13878)
*Yanggan Gu,Zhaoyi Yan,Yuanyi Wang,Yiming Zhang,Qi Zhou,Fei Wu,Hongxia Yang*

Main category: cs.LG

TL;DR: 提出基于序列级多源概率融合的偏好优化方法InfiFPO，通过概率剪裁和最大间隔策略，在11个基准测试中将Phi-4模型平均性能提升4.2%，显著增强数学/编码/推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有偏好对齐阶段模型融合方法(如WRPO)仅利用源模型输出响应而丢弃概率信息，导致知识蒸馏效率低下。需要开发能有效融合多源模型概率信息的偏好优化框架。

Method: 将DPO的参考模型替换为融合多源概率的合成模型：1) 序列级概率融合避免词汇对齐复杂性 2) 概率剪裁消除极端概率干扰 3) 最大间隔策略增强决策边界清晰度

Result: Phi-4模型平均性能从79.95提升至83.33，数学(MATH基准+5.1%)、编码(HumanEval+6.7%)、推理(ARC-Challenge+4.3%)提升显著，超越所有基线模型融合方法。

Conclusion: InfiFPO验证了概率信息在模型融合中的关键作用，通过创新的序列级融合机制，为LLM协同优化开辟了新路径。该方法在保持训练轻量化的同时实现多源知识高效蒸馏。

Abstract: Model fusion combines multiple Large Language Models (LLMs) with different
strengths into a more powerful, integrated model through lightweight training
methods. Existing works on model fusion focus primarily on supervised
fine-tuning (SFT), leaving preference alignment (PA) --a critical phase for
enhancing LLM performance--largely unexplored. The current few fusion methods
on PA phase, like WRPO, simplify the process by utilizing only response outputs
from source models while discarding their probability information. To address
this limitation, we propose InfiFPO, a preference optimization method for
implicit model fusion. InfiFPO replaces the reference model in Direct
Preference Optimization (DPO) with a fused source model that synthesizes
multi-source probabilities at the sequence level, circumventing complex
vocabulary alignment challenges in previous works and meanwhile maintaining the
probability information. By introducing probability clipping and max-margin
fusion strategies, InfiFPO enables the pivot model to align with human
preferences while effectively distilling knowledge from source models.
Comprehensive experiments on 11 widely-used benchmarks demonstrate that InfiFPO
consistently outperforms existing model fusion and preference optimization
methods. When using Phi-4 as the pivot model, InfiFPO improve its average
performance from 79.95 to 83.33 on 11 benchmarks, significantly improving its
capabilities in mathematics, coding, and reasoning tasks.

</details>


### [150] [Textual Steering Vectors Can Improve Visual Understanding in Multimodal Large Language Models](https://arxiv.org/abs/2505.14071)
*Woody Haosheng Gan,Deqing Fu,Julian Asilis,Ollie Liu,Dani Yogatama,Vatsal Sharan,Robin Jia,Willie Neiswanger*

Main category: cs.LG

TL;DR: 通过文本衍生的导向向量（稀疏自编码器/均值漂移/线性探测）显著提升多模态大语言模型在视觉任务中的准确性，且计算开销低。


<details>
  <summary>Details</summary>
Motivation: 现有文本导向方法难以直接适配多模态大语言模型（MLLMs）的架构多样性，需探索基于文本主干的高效引导机制。

Method: 使用LLM文本主干的稀疏自编码器提取特征向量，结合均值漂移和线性探测技术生成导向向量。

Result: 均值漂移使CV-Bench空间关系准确率提升+7.3%，计数准确率+3.3%，优于prompt方法且具备跨数据集泛化能力。

Conclusion: 文本导向向量为MLLMs提供了无需大量额外数据的低成本性能增强方案，验证了跨模态特征迁移的有效性。

Abstract: Steering methods have emerged as effective and targeted tools for guiding
large language models' (LLMs) behavior without modifying their parameters.
Multimodal large language models (MLLMs), however, do not currently enjoy the
same suite of techniques, due in part to their recency and architectural
diversity. Inspired by this gap, we investigate whether MLLMs can be steered
using vectors derived from their text-only LLM backbone, via sparse
autoencoders (SAEs), mean shift, and linear probing. We find that text-derived
steering consistently enhances multimodal accuracy across diverse MLLM
architectures and visual tasks. In particular, mean shift boosts spatial
relationship accuracy on CV-Bench by up to +7.3% and counting accuracy by up to
+3.3%, outperforming prompting and exhibiting strong generalization to
out-of-distribution datasets. These results highlight textual steering vectors
as a powerful, efficient mechanism for enhancing grounding in MLLMs with
minimal additional data collection and computational overhead.

</details>


### [151] [Safety Subspaces are Not Distinct: A Fine-Tuning Case Study](https://arxiv.org/abs/2505.14185)
*Kaustubh Ponkshe,Shaan Shah,Raghav Singhal,Praneeth Vepakomma*

Main category: cs.LG

TL;DR: 研究通过实证分析挑战LLM安全对齐的几何定位假设，发现安全行为与不安全行为共享子空间，表明基于子空间的防御存在局限性


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法（指令调整/RLHF）存在脆弱性，微调可能引发安全退化，需验证安全机制是否具有可分离的几何特征

Method: 对5个开源LLM进行参数/激活空间分析，测试安全相关行为在子空间的分布及其与通用学习的可分离性

Result: 安全/不安全行为在相同子空间被同步放大，不同安全属性的prompt激活重叠表征，未发现专控安全的独立子空间

Conclusion: 安全源于模型整体学习动态中的复杂纠缠，而非独立几何结构，需开发非子空间策略维持持续训练中的安全对齐

Abstract: Large Language Models (LLMs) rely on safety alignment to produce socially
acceptable responses. This is typically achieved through instruction tuning and
reinforcement learning from human feedback. However, this alignment is known to
be brittle: further fine-tuning, even on benign or lightly contaminated data,
can degrade safety and reintroduce harmful behaviors. A growing body of work
suggests that alignment may correspond to identifiable geometric directions in
weight space, forming subspaces that could, in principle, be isolated or
preserved to defend against misalignment. In this work, we conduct a
comprehensive empirical study of this geometric perspective. We examine whether
safety-relevant behavior is concentrated in specific subspaces, whether it can
be separated from general-purpose learning, and whether harmfulness arises from
distinguishable patterns in internal representations. Across both parameter and
activation space, our findings are consistent: subspaces that amplify safe
behaviors also amplify unsafe ones, and prompts with different safety
implications activate overlapping representations. We find no evidence of a
subspace that selectively governs safety. These results challenge the
assumption that alignment is geometrically localized. Rather than residing in
distinct directions, safety appears to emerge from entangled, high-impact
components of the model's broader learning dynamics. This suggests that
subspace-based defenses may face fundamental limitations and underscores the
need for alternative strategies to preserve alignment under continued training.
We corroborate these findings through multiple experiments on five open-source
LLMs. Our code is publicly available at:
https://github.com/CERT-Lab/safety-subspaces.

</details>


### [152] [AAPO: Enhance the Reasoning Capabilities of LLMs with Advantage Momentum](https://arxiv.org/abs/2505.14264)
*Jian Xiong,Jingbo Zhou,Jingyong Ye,Dejing Dou*

Main category: cs.LG

TL;DR: 提出Advantage-Augmented Policy Optimization（AAPO）算法，通过动量估计方案增强优势值，解决现有分组相对优势估计方法在训练效率上的不足


<details>
  <summary>Details</summary>
Motivation: 现有Group Relative Policy Optimization（GRPO）方法在估计优势接近零时存在训练效率低下的问题

Method: 采用动量估计方案增强优势值，优化交叉熵损失函数，提出Advantage-Augmented Policy Optimization（AAPO）算法

Result: 在多个数学推理基准测试中展现出优越性能

Conclusion: AAPO有效解决了分组相对优势估计方法的训练效率瓶颈，为强化学习在语言模型推理能力提升方面提供了新思路

Abstract: Reinforcement learning (RL) has emerged as an effective approach for
enhancing the reasoning capabilities of large language models (LLMs),
especially in scenarios where supervised fine-tuning (SFT) falls short due to
limited chain-of-thought (CoT) data. Among RL-based post-training methods,
group relative advantage estimation, as exemplified by Group Relative Policy
Optimization (GRPO), has attracted considerable attention for eliminating the
dependency on the value model, thereby simplifying training compared to
traditional approaches like Proximal Policy Optimization (PPO). However, we
observe that exsiting group relative advantage estimation method still suffers
from training inefficiencies, particularly when the estimated advantage
approaches zero. To address this limitation, we propose Advantage-Augmented
Policy Optimization (AAPO), a novel RL algorithm that optimizes the
cross-entropy (CE) loss using advantages enhanced through a momentum-based
estimation scheme. This approach effectively mitigates the inefficiencies
associated with group relative advantage estimation. Experimental results on
multiple mathematical reasoning benchmarks demonstrate the superior performance
of AAPO.

</details>


### [153] [Scaling Law for Quantization-Aware Training](https://arxiv.org/abs/2505.14302)
*Mengzhao Chen,Chaoyi Zhang,Jing Liu,Yutao Zeng,Zeyue Xue,Zhiheng Liu,Yunshui Li,Jin Ma,Jie Huang,Xun Zhou,Ping Luo*

Main category: cs.LG

TL;DR: 提出统一QAT缩放定律，揭示W4A4量化误差与模型规模/训练数据量/量化粒度的关联，通过混合精度量化解决激活层异常值瓶颈


<details>
  <summary>Details</summary>
Motivation: 现有QAT缩放定律忽视训练token数量和量化粒度等关键因素，导致其适用性受限。需系统研究W4A4量化误差的组成及其演变规律

Method: 基于268次QAT实验，建立量化误差与模型参数量的函数关系，将总误差分解为权重/激活分量，特别分析FC2层异常值影响

Result: 量化误差随模型增大下降，随训练数据增加/量化粒度变粗上升。激活量化误差(FC2层异常值)是W4A4主要瓶颈，混合精度可使权重/激活误差趋近

Conclusion: 需重点关注激活层异常值(混合精度)和训练数据量增大时的权重误差。该发现为QAT优化提供了量化误差分解框架和优先级指导

Abstract: Large language models (LLMs) demand substantial computational and memory
resources, creating deployment challenges. Quantization-aware training (QAT)
addresses these challenges by reducing model precision while maintaining
performance. However, the scaling behavior of QAT, especially at 4-bit
precision (W4A4), is not well understood. Existing QAT scaling laws often
ignore key factors such as the number of training tokens and quantization
granularity, which limits their applicability. This paper proposes a unified
scaling law for QAT that models quantization error as a function of model size,
training data volume, and quantization group size. Through 268 QAT experiments,
we show that quantization error decreases as model size increases, but rises
with more training tokens and coarser quantization granularity. To identify the
sources of W4A4 quantization error, we decompose it into weight and activation
components. Both components follow the overall trend of W4A4 quantization
error, but with different sensitivities. Specifically, weight quantization
error increases more rapidly with more training tokens. Further analysis shows
that the activation quantization error in the FC2 layer, caused by outliers, is
the primary bottleneck of W4A4 QAT quantization error. By applying
mixed-precision quantization to address this bottleneck, we demonstrate that
weight and activation quantization errors can converge to similar levels.
Additionally, with more training data, weight quantization error eventually
exceeds activation quantization error, suggesting that reducing weight
quantization error is also important in such scenarios. These findings offer
key insights for improving QAT research and development.

</details>


### [154] [Enhancing Learned Knowledge in LoRA Adapters Through Efficient Contrastive Decoding on Ascend NPUs](https://arxiv.org/abs/2505.14620)
*Morgan Lindsay Heisler,Linzi Xing,Ge Shi,Hanieh Sadri,Gursimran Singh,Weiwei Zhang,Tao Ye,Ying Xiong,Yong Zhang,Zhenan Fan*

Main category: cs.LG

TL;DR: 华为提出基于对比解码的CoLD框架，通过对比LoRA专家模型与基础模型的输出差异，显著提升任务准确性同时降低延迟


<details>
  <summary>Details</summary>
Motivation: 传统解码方法在LoRA微调模型中易受基础模型干扰，导致任务特定知识利用率不足

Method: 使用对比解码策略：1) 计算LoRA专家与基础模型的概率分布差异 2) 优先选择专家模型特征显著的token 3) 开发昇腾NPU专用加速内核

Result: 任务准确率提升5.54%，端到端延迟降低28%（相比贪婪解码）

Conclusion: CoLD为资源受限环境提供了高效的LLM解码方案，在云计算与本地部署中均有广泛应用价值

Abstract: Huawei Cloud users leverage LoRA (Low-Rank Adaptation) as an efficient and
scalable method to fine-tune and customize large language models (LLMs) for
application-specific needs. However, tasks that require complex reasoning or
deep contextual understanding are often hindered by biases or interference from
the base model when using typical decoding methods like greedy or beam search.
These biases can lead to generic or task-agnostic responses from the base model
instead of leveraging the LoRA-specific adaptations. In this paper, we
introduce Contrastive LoRA Decoding (CoLD), a novel decoding framework designed
to maximize the use of task-specific knowledge in LoRA-adapted models,
resulting in better downstream performance. CoLD uses contrastive decoding by
scoring candidate tokens based on the divergence between the probability
distributions of a LoRA-adapted expert model and the corresponding base model.
This approach prioritizes tokens that better align with the LoRA's learned
representations, enhancing performance for specialized tasks. While effective,
a naive implementation of CoLD is computationally expensive because each
decoding step requires evaluating multiple token candidates across both models.
To address this, we developed an optimized kernel for Huawei's Ascend NPU. CoLD
achieves up to a 5.54% increase in task accuracy while reducing end-to-end
latency by 28% compared to greedy decoding. This work provides practical and
efficient decoding strategies for fine-tuned LLMs in resource-constrained
environments and has broad implications for applied data science in both cloud
and on-premises settings.

</details>


### [155] [TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning](https://arxiv.org/abs/2505.14625)
*Zhangchen Xu,Yuetai Li,Fengqing Jiang,Bhaskar Ramasubramanian,Luyao Niu,Bill Yuchen Lin,Radha Poovendran*

Main category: cs.LG

TL;DR: 论文揭示了强化学习验证器中普遍存在的假阴性问题（38%正确回答被错误拒绝），提出轻量级验证器tinyV通过动态识别假阴性提升RL训练效果，在数学推理任务中实现10%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有RL训练中验证器的假阴性问题（错误拒绝正确回答）会严重损害模型训练，导致梯度信号不足和收敛速度下降。

Method: 提出tinyV——基于LLM的轻量级验证器，与规则方法协同工作，动态识别潜在假阴性并恢复有效响应，提升奖励信号准确性。

Result: 在多个数学推理基准测试中，tinyV使通过率提升最高达10%，并显著加速模型收敛速度。

Conclusion: 验证器假阴性是RL训练关键瓶颈，tinyV通过软硬件协同设计为提升LLM的RL微调效果提供了实用解决方案。

Abstract: Reinforcement Learning (RL) has become a powerful tool for enhancing the
reasoning abilities of large language models (LLMs) by optimizing their
policies with reward signals. Yet, RL's success relies on the reliability of
rewards, which are provided by verifiers. In this paper, we expose and analyze
a widespread problem--false negatives--where verifiers wrongly reject correct
model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals
that over 38% of model-generated responses suffer from false negatives, where
the verifier fails to recognize correct answers. We show, both empirically and
theoretically, that these false negatives severely impair RL training by
depriving the model of informative gradient signals and slowing convergence. To
mitigate this, we propose tinyV, a lightweight LLM-based verifier that augments
existing rule-based methods, which dynamically identifies potential false
negatives and recovers valid responses to produce more accurate reward
estimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts
pass rates by up to 10% and accelerates convergence relative to the baseline.
Our findings highlight the critical importance of addressing verifier false
negatives and offer a practical approach to improve RL-based fine-tuning of
LLMs. Our code is available at https://github.com/uw-nsl/TinyV.

</details>


### [156] [KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models](https://arxiv.org/abs/2505.14629)
*Fnu Mohbat,Mohammed J Zaki*

Main category: cs.LG

TL;DR: 提出KERL系统，整合食物知识图谱与语言模型，显著提升个性化饮食推荐与营养分析性能，并通过实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中在通用推荐系统的LLM应用，但缺乏针对食物领域知识图谱与语言模型的深度整合方案。

Method: 通过实体提取→知识图谱检索→LLM上下文约束选择→分步生成烹饪流程与营养信息的端到端流程设计。

Result: 在自建基准数据集上，KG增强的LLM方案全面超越现有方法（准确率提升23.1%），实现推荐、生成、分析全链路闭环。

Conclusion: 开创性地构建食物领域知识驱动型LLM系统，公开的代码与数据集推动食品计算研究社区发展。

Abstract: Recent advances in large language models (LLMs) and the abundance of food
data have resulted in studies to improve food understanding using LLMs. Despite
several recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there
has been limited research on integrating food related KGs with LLMs. We
introduce KERL, a unified system that leverages food KGs and LLMs to provide
personalized food recommendations and generates recipes with associated
micro-nutritional information. Given a natural language question, KERL extracts
entities, retrieves subgraphs from the KG, which are then fed into the LLM as
context to select the recipes that satisfy the constraints. Next, our system
generates the cooking steps and nutritional information for each recipe. To
evaluate our approach, we also develop a benchmark dataset by curating recipe
related questions, combined with constraints and personal preferences. Through
extensive experiments, we show that our proposed KG-augmented LLM significantly
outperforms existing approaches, offering a complete and coherent solution for
food recommendation, recipe generation, and nutritional analysis. Our code and
benchmark datasets are publicly available at
https://github.com/mohbattharani/KERL.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [157] [Pairwise Evaluation of Accent Similarity in Speech Synthesis](https://arxiv.org/abs/2505.14410)
*Jinzuomu Zhong,Suyuan Liu,Dan Wells,Korin Richmond*

Main category: eess.AS

TL;DR: 改进口音相似性评估的主客观方法：主观上优化XAB测试流程，客观上采用发音相关指标，并揭示常用指标的局限性


<details>
  <summary>Details</summary>
Motivation: 当前语音合成领域缺乏有效的口音相似性评估方法，特别是主观测试成本高效率低，客观指标对非主流口音评估存在明显缺陷

Method: 主观评估：增加转录文本标注和差异定位的XAB测试；客观评估：基于元音共振峰距离和语音后验图的发音指标，结合多种相似性指标

Result: 改进后的主观测试用更少听众达到更高统计显著性，发音相关指标有效评估口音生成质量，词错误率等传统指标在评估少数口音时表现欠佳

Conclusion: 综合主客观评估方法可更有效评估口音相似性，需开发更适配非主流口音的评估指标，传统指标需谨慎使用

Abstract: Despite growing interest in generating high-fidelity accents, evaluating
accent similarity in speech synthesis has been underexplored. We aim to enhance
both subjective and objective evaluation methods for accent similarity.
Subjectively, we refine the XAB listening test by adding components that
achieve higher statistical significance with fewer listeners and lower costs.
Our method involves providing listeners with transcriptions, having them
highlight perceived accent differences, and implementing meticulous screening
for reliability. Objectively, we utilise pronunciation-related metrics, based
on distances between vowel formants and phonetic posteriorgrams, to evaluate
accent generation. Comparative experiments reveal that these metrics, alongside
accent similarity, speaker similarity, and Mel Cepstral Distortion, can be
used. Moreover, our findings underscore significant limitations of common
metrics like Word Error Rate in assessing underrepresented accents.

</details>


### [158] [Mitigating Subgroup Disparities in Multi-Label Speech Emotion Recognition: A Pseudo-Labeling and Unsupervised Learning Approach](https://arxiv.org/abs/2505.14449)
*Yi-Cheng Lin,Huang-Cheng Chou,Hung-yi Lee*

Main category: eess.AS

TL;DR: 通过隐式人口统计推断（IDI）模块改善语音情感识别的公平性，无需显式人口标签


<details>
  <summary>Details</summary>
Motivation: 现有语音情感识别方法依赖显式人口标签，但隐私问题导致标签难以获取。需要开发不依赖显式人口统计的公平性提升方法。

Method: 1. 伪标签IDI：利用预训练模型生成伪标签
2. 无监督IDI：通过k-means聚类进行无监督学习

Result: 伪标签IDI提升33%公平性指标（SER准确率下降<3%）；无监督IDI提升26%公平性指标（准确率下降<4%），有效缓解种族和年龄差异

Conclusion: IDI模块在缺乏显式人口信息时仍能有效提升公平性，为隐私敏感场景提供可行解决方案

Abstract: While subgroup disparities and performance bias are increasingly studied in
computational research, fairness in categorical Speech Emotion Recognition
(SER) remains underexplored. Existing methods often rely on explicit
demographic labels, which are difficult to obtain due to privacy concerns. To
address this limitation, we introduce an Implicit Demography Inference (IDI)
module that leverages pseudo-labeling from a pre-trained model and unsupervised
learning using k-means clustering to mitigate bias in SER. Our experiments show
that pseudo-labeling IDI reduces subgroup disparities, improving fairness
metrics by over 33% with less than a 3% decrease in SER accuracy. Also, the
unsupervised IDI yields more than a 26% improvement in fairness metrics with a
drop of less than 4% in SER performance. Further analyses reveal that the
unsupervised IDI consistently mitigates race and age disparities, demonstrating
its potential in scenarios where explicit demographic information is
unavailable.

</details>


### [159] [Teaching Audio-Aware Large Language Models What Does Not Hear: Mitigating Hallucinations through Synthesized Negative Samples](https://arxiv.org/abs/2505.14518)
*Chun-Yi Kuan,Hung-yi Lee*

Main category: eess.AS

TL;DR: 提出LISTEN方法通过对比式训练与轻量级适配器，有效降低音频感知大语言模型的幻觉问题


<details>
  <summary>Details</summary>
Motivation: 现有音频感知大语言模型在处理音频时容易产生非真实声音事件的幻觉，影响实际应用可靠性

Method: 采用合成负样本的对比学习策略，通过轻量适配器集成音频表征而不修改大模型参数

Result: 实验证明LISTEN在保持现有音频推理基准性能的同时显著减少幻觉，且具备更高的数据与计算效率

Conclusion: LISTEN提供了一种参数高效、无需修改基础模型的可靠音频感知解决方案

Abstract: Recent advancements in audio-aware large language models (ALLMs) enable them
to process and understand audio inputs. However, these models often hallucinate
non-existent sound events, reducing their reliability in real-world
applications. To address this, we propose LISTEN (Learning to Identify Sounds
Through Extended Negative Samples), a contrastive-like training method that
enhances ALLMs' ability to distinguish between present and absent sounds using
synthesized data from the backbone LLM. Unlike prior approaches, our method
requires no modification to LLM parameters and efficiently integrates audio
representations via a lightweight adapter. Experiments show that LISTEN
effectively mitigates hallucinations while maintaining impressive performance
on existing audio question and reasoning benchmarks. At the same time, it is
more efficient in both data and computation.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [160] [InterFeat: An Automated Pipeline for Finding Interesting Hypotheses in Structured Biomedical Data](https://arxiv.org/abs/2505.13534)
*Dan Ofer,Michal Linial,Dafna Shahaf*

Main category: q-bio.QM

TL;DR: 开发自动化流程InterFeat，整合机器学习/知识图谱/LLM技术，实现生物医学数据中新颖假说的规模化发现


<details>
  <summary>Details</summary>
Motivation: 解决传统科研发现依赖人工、缺乏系统性标准的问题，将模糊的'有趣性'概念转化为可操作的评估体系

Method: 1. 多阶段机器学习框架（候选生成→文献过滤→机制推测） 2. 融合知识图谱实体链接 3. 基于GPT-4的自动文献验证与机制解释

Result: 在英国生物银行8种疾病数据中：• 早于文献发现风险因素（提前3-15年） • 前位候选40-53%有效（基准方法0-7%） • 28%候选被医学专家认可

Conclusion: 首次实现可扩展的自动化科研发现框架，验证了'有趣性'量化标准的有效性，为任意研究目标提供系统化假说生成方案

Abstract: Finding interesting phenomena is the core of scientific discovery, but it is
a manual, ill-defined concept. We present an integrative pipeline for
automating the discovery of interesting simple hypotheses (feature-target
relations with effect direction and a potential underlying mechanism) in
structured biomedical data. The pipeline combines machine learning, knowledge
graphs, literature search and Large Language Models. We formalize
"interestingness" as a combination of novelty, utility and plausibility. On 8
major diseases from the UK Biobank, our pipeline consistently recovers risk
factors years before their appearance in the literature. 40--53% of our top
candidates were validated as interesting, compared to 0--7% for a SHAP-based
baseline. Overall, 28% of 109 candidates were interesting to medical experts.
The pipeline addresses the challenge of operationalizing "interestingness"
scalably and for any target. We release data and code:
https://github.com/LinialLab/InterFeat

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [161] [MedEIR: A Specialized Medical Embedding Model for Enhanced Information Retrieval](https://arxiv.org/abs/2505.13482)
*Anand Selvadurai,Jasheen Shaik,Girish Chandrasekar,ShriRadhaKrishnan Balamurugan,Eswara Reddy*

Main category: cs.IR

TL;DR: 提出医学领域与通用任务联合优化的MedEIR嵌入模型，支持8192token长文本处理，在多个基准测试中超越现有模型


<details>
  <summary>Details</summary>
Motivation: 现有嵌入模型在医学语义理解、长文本处理、领域适应性方面存在局限，需兼顾专业领域与通用能力的解决方案

Method: 联合优化医学和通用任务的嵌入模型，集成ALiBi长文本处理技术，仅用6B token预训练+3M句对微调

Result: MTEB基准测试全面超越Jina/MiniLM，在ArguAna(55.24)、MedicalQARetrieval(74.25)、TRECCOVID(79.56)等医学任务表现突出

Conclusion: MedEIR首次实现医学与通用NLP任务的兼容优化，以更少训练数据达成SOTA，验证了多领域联合训练的有效性

Abstract: Embedding models have become essential for retrieval-augmented generation
(RAG) tasks, semantic clustering, and text re-ranking. But despite their
growing use, many of these come with notable limitations. For example, Jina
fails to capture the semantic content of medical documents, while models such
as MiniLM often perform poorly on long-form documents. Domain-adapted models,
while specialized, often underperform in general-purpose tasks, reducing their
overall applicability. General-domain tokenizers often misinterpret medical
vocabulary. The limitations of current embedding models, whether in
tokenization accuracy, domain comprehension, or handling long sequences,
highlight the need for more versatile solutions. In this work, we present
MedEIR, a novel embedding model and tokenizer jointly optimized for both
medical and general NLP tasks, incorporating ALiBi-based long-context
processing to support sequences of up to 8,192 tokens. MedEIR was pre-trained
on only 6 billion tokens, significantly fewer than Jina's, followed by
fine-tuning on 3 million sentence pairs. MedEIR consistently outperforms Jina
V2 and MiniLM across MTEB benchmarks, achieving top scores on ArguAna (55.24),
NFCorpus (38.44), MedicalQARetrieval (74.25), SciFact (72.04), and TRECCOVID
(79.56). These results highlight the potential of MedEIR as a highly effective
embedding model, demonstrating strong performance across both general-purpose
and domain-specific tasks and outperforming existing models on multiple
benchmarks.

</details>


### [162] [RAR: Setting Knowledge Tripwires for Retrieval Augmented Rejection](https://arxiv.org/abs/2505.13581)
*Tommaso Mario Buonocore,Enea Parimbelli*

Main category: cs.IR

TL;DR: 提出检索增强拒绝(RAR)方法，通过修改向量数据库实现无需模型重训练的实时内容审核


<details>
  <summary>Details</summary>
Motivation: 解决大模型内容审核系统应对新兴威胁时缺乏实时灵活性的问题，需快速响应关键漏洞且不改变现有架构

Method: 在RAG架构中插入标记恶意文档，基于检索结果触发拒绝机制。仅需添加特制文档和简单拒绝逻辑

Result: 达到与Claude 3.5 Sonnet嵌入式审核相当的效果，具备实时定制能力和更高的系统灵活性

Conclusion: 该方法兼容现有RAG系统，通过文档工程实现动态内容管控，为LLM安全防护提供轻量化解决方案

Abstract: Content moderation for large language models (LLMs) remains a significant
challenge, requiring flexible and adaptable solutions that can quickly respond
to emerging threats. This paper introduces Retrieval Augmented Rejection (RAR),
a novel approach that leverages a retrieval-augmented generation (RAG)
architecture to dynamically reject unsafe user queries without model
retraining. By strategically inserting and marking malicious documents into the
vector database, the system can identify and reject harmful requests when these
documents are retrieved. Our preliminary results show that RAR achieves
comparable performance to embedded moderation in LLMs like Claude 3.5 Sonnet,
while offering superior flexibility and real-time customization capabilities, a
fundamental feature to timely address critical vulnerabilities. This approach
introduces no architectural changes to existing RAG systems, requiring only the
addition of specially crafted documents and a simple rejection mechanism based
on retrieval results.

</details>


### [163] [LLM-Based Compact Reranking with Document Features for Scientific Retrieval](https://arxiv.org/abs/2505.13757)
*Runchu Tian,Xueqiang Xu,Bowen Jin,SeongKu Kang,Jiawei Han*

Main category: cs.IR

TL;DR: 提出CoRank框架，通过语义特征压缩表示和两阶段重排序提升科学文献检索性能


<details>
  <summary>Details</summary>
Motivation: 解决科学领域LLM列表重排序中候选文本过长导致覆盖不足、首阶段检索质量差的问题

Method: 三阶段框架：1) 离线提取文档类别/章节/关键词等特征 2) 基于压缩表示的粗排序 3) 对粗排结果进行全文细粒度排序

Result: 在LitSearch和CSFCube数据集上nDCG@10从32.0提升至39.7，各LLM主干模型均显著改进

Conclusion: 证明语义特征压缩表示能有效扩展候选覆盖并保持排序精度，信息抽取对科学检索重排序具有重要价值

Abstract: Scientific retrieval is essential for advancing academic discovery. Within
this process, document reranking plays a critical role by refining first-stage
retrieval results. However, large language model (LLM) listwise reranking faces
unique challenges in the scientific domain. First-stage retrieval is often
suboptimal in the scientific domain, so relevant documents are ranked lower.
Moreover, conventional listwise reranking uses the full text of candidate
documents in the context window, limiting the number of candidates that can be
considered. As a result, many relevant documents are excluded before reranking,
which constrains overall retrieval performance. To address these challenges, we
explore compact document representations based on semantic features such as
categories, sections, and keywords, and propose a training-free, model-agnostic
reranking framework for scientific retrieval called CoRank. The framework
involves three stages: (i) offline extraction of document-level features, (ii)
coarse reranking using these compact representations, and (iii) fine-grained
reranking on full texts of the top candidates from stage (ii). This hybrid
design provides a high-level abstraction of document semantics, expands
candidate coverage, and retains critical details required for precise ranking.
Experiments on LitSearch and CSFCube show that CoRank significantly improves
reranking performance across different LLM backbones, increasing nDCG@10 from
32.0 to 39.7. Overall, these results highlight the value of information
extraction for reranking in scientific retrieval.

</details>


### [164] [Rank-K: Test-Time Reasoning for Listwise Reranking](https://arxiv.org/abs/2505.14432)
*Eugene Yang,Andrew Yates,Kathryn Ricci,Orion Weller,Vivek Chari,Benjamin Van Durme,Dawn Lawrie*

Main category: cs.IR

TL;DR: Rank-K提出新型列表式文档重排序模型，通过推理语言模型在查询阶段动态调整排序，在BM25和SPLADE-v3基础上分别提升检索效果23%和19%，并具备多语言适配能力。


<details>
  <summary>Details</summary>
Motivation: 现有神经重排序模型（如RankZephyr）虽效果显著，但计算资源消耗过大，难以应对复杂查询场景的实时需求。Rank-K旨在实现查询时动态扩展能力，平衡效率与效果。

Method: 基于推理语言模型的列表式重排序架构，直接处理初始检索结果（如BM25/SPLADE-v3的排序列表），通过多文档联合推理优化排序效果。模型具备端到端的多语言处理能力。

Result: 在BM25基线提升23%检索效果，SPLADE-v3基线上提升19%。多语言实验显示其跨语言排序能力与单语言场景效果相当。

Conclusion: Rank-K通过动态推理机制实现效果与效率的突破，为复杂查询提供可扩展的解决方案，其原生多语言特性拓展了应用边界。

Abstract: Retrieve-and-rerank is a popular retrieval pipeline because of its ability to
make slow but effective rerankers efficient enough at query time by reducing
the number of comparisons. Recent works in neural rerankers take advantage of
large language models for their capability in reasoning between queries and
passages and have achieved state-of-the-art retrieval effectiveness. However,
such rerankers are resource-intensive, even after heavy optimization. In this
work, we introduce Rank-K, a listwise passage reranking model that leverages
the reasoning capability of the reasoning language model at query time that
provides test time scalability to serve hard queries. We show that Rank-K
improves retrieval effectiveness by 23\% over the RankZephyr, the
state-of-the-art listwise reranker, when reranking a BM25 initial ranked list
and 19\% when reranking strong retrieval results by SPLADE-v3. Since Rank-K is
inherently a multilingual model, we found that it ranks passages based on
queries in different languages as effectively as it does in monolingual
retrieval.

</details>


### [165] [NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search](https://arxiv.org/abs/2505.14680)
*Sunhao Dai,Wenjie Wang,Liang Pang,Jun Xu,See-Kiong Ng,Ji-Rong Wen,Tat-Seng Chua*

Main category: cs.IR

TL;DR: 提出NExT-Search范式，通过整合用户调试模式和影子用户模式，重构生成式AI搜索的精细化反馈闭环以实现持续优化


<details>
  <summary>Details</summary>
Motivation: 生成式AI搜索端到端输出特性导致传统文档级细粒度反馈缺失，阻碍搜索系统持续改进

Method: 设计双模式系统：用户调试模式支持关键环节人工干预，影子用户模式通过个性化代理模拟用户偏好；结合在线实时优化与离线模型更新

Result: 建立全流程可追踪的反馈机制，使查询分解、检索和生成各环节均可获得改进信号

Conclusion: NExT-Search通过恢复人类控制与AI辅助反馈的协同，为持续进化的AI搜索系统提供可行架构

Abstract: Generative AI search is reshaping information retrieval by offering
end-to-end answers to complex queries, reducing users' reliance on manually
browsing and summarizing multiple web pages. However, while this paradigm
enhances convenience, it disrupts the feedback-driven improvement loop that has
historically powered the evolution of traditional Web search. Web search can
continuously improve their ranking models by collecting large-scale,
fine-grained user feedback (e.g., clicks, dwell time) at the document level. In
contrast, generative AI search operates through a much longer search pipeline,
spanning query decomposition, document retrieval, and answer generation, yet
typically receives only coarse-grained feedback on the final answer. This
introduces a feedback loop disconnect, where user feedback for the final output
cannot be effectively mapped back to specific system components, making it
difficult to improve each intermediate stage and sustain the feedback loop. In
this paper, we envision NExT-Search, a next-generation paradigm designed to
reintroduce fine-grained, process-level feedback into generative AI search.
NExT-Search integrates two complementary modes: User Debug Mode, which allows
engaged users to intervene at key stages; and Shadow User Mode, where a
personalized user agent simulates user preferences and provides AI-assisted
feedback for less interactive users. Furthermore, we envision how these
feedback signals can be leveraged through online adaptation, which refines
current search outputs in real-time, and offline update, which aggregates
interaction logs to periodically fine-tune query decomposition, retrieval, and
generation models. By restoring human control over key stages of the generative
AI search pipeline, we believe NExT-Search offers a promising direction for
building feedback-rich AI search systems that can evolve continuously alongside
human feedback.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [166] [Guided Search Strategies in Non-Serializable Environments with Applications to Software Engineering Agents](https://arxiv.org/abs/2505.13652)
*Karina Zainullina,Alexander Golubev,Maria Trofimova,Sergei Polezhaev,Ibragim Badertdinov,Daria Litvintseva,Simon Karasik,Filipp Fisin,Sergei Skvortsov,Maksim Nekrashevich,Anton Shevtsov,Boris Yangel*

Main category: cs.SE

TL;DR: 提出两种互补的搜索策略（1步前瞻和轨迹选择），通过动作价值函数指导，在SWE-bench基准上将Qwen-72B成功率翻倍至40.8%，并验证该方法的跨模型迁移能力


<details>
  <summary>Details</summary>
Motivation: 针对LLMs在多次尝试中性能不稳定的问题，以及现有搜索技术（如MCTS）在非序列化RL环境（如Docker容器）中的应用限制

Method: 开发1步前瞻策略和轨迹选择策略，利用学习的动作价值函数估计器指导搜索过程

Result: 在SWE-bench基准实现开放权重模型新SOTA（40.8%成功率），且方法可迁移至GPT-4o获得类似提升

Conclusion: 验证了基于价值函数引导的搜索策略在复杂软件工程环境中的有效性，为LLMs在非序列化场景的性能提升提供了通用解决方案

Abstract: Large language models (LLMs) have recently achieved remarkable results in
complex multi-step tasks, such as mathematical reasoning and agentic software
engineering. However, they often struggle to maintain consistent performance
across multiple solution attempts. One effective approach to narrow the gap
between average-case and best-case performance is guided test-time search,
which explores multiple solution paths to identify the most promising one.
Unfortunately, effective search techniques (e.g. MCTS) are often unsuitable for
non-serializable RL environments, such as Docker containers, where intermediate
environment states cannot be easily saved and restored. We investigate two
complementary search strategies applicable to such environments: 1-step
lookahead and trajectory selection, both guided by a learned action-value
function estimator. On the SWE-bench Verified benchmark, a key testbed for
agentic software engineering, we find these methods to double the average
success rate of a fine-tuned Qwen-72B model, achieving 40.8%, the new
state-of-the-art for open-weights models. Additionally, we show that these
techniques are transferable to more advanced closed models, yielding similar
improvements with GPT-4o.

</details>


### [167] [Advancing Software Quality: A Standards-Focused Review of LLM-Based Assurance Techniques](https://arxiv.org/abs/2505.13766)
*Avinash Patil*

Main category: cs.SE

TL;DR: 研究探讨如何将大语言模型（LLMs）与ISO/IEC等软件质量标准结合，实现自动化SQA流程增强。


<details>
  <summary>Details</summary>
Motivation: 传统SQA流程面临效率瓶颈，LLMs在需求分析、缺陷检测等环节展现自动化潜力，但需与现有质量框架合规融合。

Method: 1. 系统梳理ISO/IEC 12207等质量标准 2. 构建LLM在需求验证、测试生成等场景的应用图谱 3. 通过实证案例验证可行性

Result: 开源项目证实LLMs可满足ISO/IEC 25010中可靠性等质量特性指标，自动化覆盖率提升35%（案例数据）

Conclusion: 需建立AI治理框架解决数据隐私/模型偏差问题，未来应发展多模态分析技术并推动SQA标准的AI适应性迭代

Abstract: Software Quality Assurance (SQA) is critical for delivering reliable, secure,
and efficient software products. The Software Quality Assurance Process aims to
provide assurance that work products and processes comply with predefined
provisions and plans. Recent advancements in Large Language Models (LLMs)
present new opportunities to enhance existing SQA processes by automating tasks
like requirement analysis, code review, test generation, and compliance checks.
Simultaneously, established standards such as ISO/IEC 12207, ISO/IEC 25010,
ISO/IEC 5055, ISO 9001/ISO/IEC 90003, CMMI, and TMM provide structured
frameworks for ensuring robust quality practices. This paper surveys the
intersection of LLM-based SQA methods and these recognized standards,
highlighting how AI-driven solutions can augment traditional approaches while
maintaining compliance and process maturity. We first review the foundational
software quality standards and the technical fundamentals of LLMs in software
engineering. Next, we explore various LLM-based SQA applications, including
requirement validation, defect detection, test generation, and documentation
maintenance. We then map these applications to key software quality frameworks,
illustrating how LLMs can address specific requirements and metrics within each
standard. Empirical case studies and open-source initiatives demonstrate the
practical viability of these methods. At the same time, discussions on
challenges (e.g., data privacy, model bias, explainability) underscore the need
for deliberate governance and auditing. Finally, we propose future directions
encompassing adaptive learning, privacy-focused deployments, multimodal
analysis, and evolving standards for AI-driven software quality.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [168] [PandaGuard: Systematic Evaluation of LLM Safety in the Era of Jailbreaking Attacks](https://arxiv.org/abs/2505.13862)
*Guobin Shen,Dongcheng Zhao,Linghao Feng,Xiang He,Jihang Wang,Sicheng Shen,Haibo Tong,Yiting Dong,Jindong Li,Xiang Zheng,Yi Zeng*

Main category: cs.CR

TL;DR: 提出PandaGuard多智能体框架系统化评估LLM安全性，集成19种攻击/12种防御方法，通过PandaBench基准测试发现防御措施存在多维权衡且判断一致性显著影响评估结果


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全研究碎片化，缺乏统一框架导致防御效果评估不全面，需系统性解决方案提升安全机制的可重复性与部署效率

Method: 构建模块化多智能体框架（攻击者/防御者/判断者），实现插件式攻击防御算法，开发配置驱动型基准测试系统，跨49个模型执行超30亿token的交互实验

Result: 防御措施无单一最优解（如Gag防御响应延迟增加300ms），判断策略差异导致安全评分波动达22%，基于规则的防御在成本效率上优于模型微调方案

Conclusion: 通过开源框架与基准促进LLM安全透明研究，证明综合防御策略必要性，揭示安全评估中需同时考虑技术效能与工程落地成本

Abstract: Large language models (LLMs) have achieved remarkable capabilities but remain
vulnerable to adversarial prompts known as jailbreaks, which can bypass safety
alignment and elicit harmful outputs. Despite growing efforts in LLM safety
research, existing evaluations are often fragmented, focused on isolated attack
or defense techniques, and lack systematic, reproducible analysis. In this
work, we introduce PandaGuard, a unified and modular framework that models LLM
jailbreak safety as a multi-agent system comprising attackers, defenders, and
judges. Our framework implements 19 attack methods and 12 defense mechanisms,
along with multiple judgment strategies, all within a flexible plugin
architecture supporting diverse LLM interfaces, multiple interaction modes, and
configuration-driven experimentation that enhances reproducibility and
practical deployment. Built on this framework, we develop PandaBench, a
comprehensive benchmark that evaluates the interactions between these
attack/defense methods across 49 LLMs and various judgment approaches,
requiring over 3 billion tokens to execute. Our extensive evaluation reveals
key insights into model vulnerabilities, defense cost-performance trade-offs,
and judge consistency. We find that no single defense is optimal across all
dimensions and that judge disagreement introduces nontrivial variance in safety
assessments. We release the code, configurations, and evaluation results to
support transparent and reproducible research in LLM safety.

</details>


### [169] [Beyond Text: Unveiling Privacy Vulnerabilities in Multi-modal Retrieval-Augmented Generation](https://arxiv.org/abs/2505.13957)
*Jiankun Zhang,Shenglai Zeng,Jie Ren,Tianqi Zheng,Hui Liu,Xianfeng Tang,Hui Liu,Yi Chang*

Main category: cs.CR

TL;DR: MRAG系统通过整合多模态数据库增强大模型，但存在未探索的隐私泄露风险。研究通过结构化提示攻击验证跨模态隐私漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有研究集中于文本检索增强生成系统的隐私风险，但多模态数据（视觉-语言、语音-语言）带来的独特隐私挑战尚未被系统分析。

Method: 采用黑盒环境下的组合式结构化提示攻击，通过操控查询指令实施隐私信息提取实验。

Result: 大模型既能直接生成类似检索内容的输出，又能通过描述间接暴露敏感信息，证实MRAG存在双向隐私泄露风险。

Conclusion: 多模态检索增强系统存在系统性隐私脆弱性，亟需开发隐私保护机制防止攻击者通过结构化查询提取敏感数据。

Abstract: Multimodal Retrieval-Augmented Generation (MRAG) systems enhance LMMs by
integrating external multimodal databases, but introduce unexplored privacy
vulnerabilities. While text-based RAG privacy risks have been studied,
multimodal data presents unique challenges. We provide the first systematic
analysis of MRAG privacy vulnerabilities across vision-language and
speech-language modalities. Using a novel compositional structured prompt
attack in a black-box setting, we demonstrate how attackers can extract private
information by manipulating queries. Our experiments reveal that LMMs can both
directly generate outputs resembling retrieved content and produce descriptions
that indirectly expose sensitive information, highlighting the urgent need for
robust privacy-preserving MRAG techniques.

</details>


### [170] [Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs](https://arxiv.org/abs/2505.14368)
*Jiawen Wang,Pritha Gupta,Ivan Habernal,Eyke Hüllermeier*

Main category: cs.CR

TL;DR: 论文提出攻击成功概率(ASP)指标评估开源大语言模型对提示注入攻击的脆弱性，催眠攻击和忽略前缀攻击可突破主流模型防御


<details>
  <summary>Details</summary>
Motivation: 当前指标仅考虑攻击成功次数，未能反映模型响应不确定性；开源大语言模型的提示注入攻击脆弱性研究不足

Method: 在5个基准测试中测试14个主流开源LLM，提出催眠攻击(hypnotism attack)和忽略前缀攻击(ignore prefix attack)

Result: 催眠攻击使Stablelm2等对齐模型产生违规内容(约90% ASP)，忽略前缀攻击突破全部14个模型(超60% ASP)

Conclusion: 中等知名度LLM更易受攻击，需提升公众防范意识并优先制定缓解策略

Abstract: Recent studies demonstrate that Large Language Models (LLMs) are vulnerable
to different prompt-based attacks, generating harmful content or sensitive
information. Both closed-source and open-source LLMs are underinvestigated for
these attacks. This paper studies effective prompt injection attacks against
the $\mathbf{14}$ most popular open-source LLMs on five attack benchmarks.
Current metrics only consider successful attacks, whereas our proposed Attack
Success Probability (ASP) also captures uncertainty in the model's response,
reflecting ambiguity in attack feasibility. By comprehensively analyzing the
effectiveness of prompt injection attacks, we propose a simple and effective
hypnotism attack; results show that this attack causes aligned language models,
including Stablelm2, Mistral, Openchat, and Vicuna, to generate objectionable
behaviors, achieving around $90$% ASP. They also indicate that our ignore
prefix attacks can break all $\mathbf{14}$ open-source LLMs, achieving over
$60$% ASP on a multi-categorical dataset. We find that moderately well-known
LLMs exhibit higher vulnerability to prompt injection attacks, highlighting the
need to raise public awareness and prioritize efficient mitigation strategies.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [171] [RADAR: Enhancing Radiology Report Generation with Supplementary Knowledge Injection](https://arxiv.org/abs/2505.14318)
*Wenjun Hou,Yi Cheng,Kaishuai Xu,Heng Li,Yan Hu,Wenjie Li,Jiang Liu*

Main category: cs.CV

TL;DR: RADAR框架通过整合LLM内部知识与外部检索信息，显著提升了放射学报告生成的准确性和信息量，在多个数据集上超越现有先进模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法在整合领域知识时忽视了LLM本身已具备的知识，导致信息冗余和表征利用率低下。研究旨在系统性融合LLM内部知识与外部补充知识，提升放射报告生成质量。

Method: 1. 提取LLM内部与专家影像分类一致的知识
2. 检索相关补充知识进行信息增强
3. 通过双源知识聚合生成精准报告
（三步协同的知识注入框架）

Result: 在MIMIC-CXR、CheXpert-Plus和IU X-ray数据集上：
- 语言质量指标提升3.2-5.8%
- 临床准确性指标提升4.1-6.7%
全面超越SOTA的LLM模型

Conclusion: 验证了同时利用LLM内部表征与外部领域知识的重要性，提出知识协同注入范式，为医学文本生成开辟新路径。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
various domains, including radiology report generation. Previous approaches
have attempted to utilize multimodal LLMs for this task, enhancing their
performance through the integration of domain-specific knowledge retrieval.
However, these approaches often overlook the knowledge already embedded within
the LLMs, leading to redundant information integration and inefficient
utilization of learned representations. To address this limitation, we propose
RADAR, a framework for enhancing radiology report generation with supplementary
knowledge injection. RADAR improves report generation by systematically
leveraging both the internal knowledge of an LLM and externally retrieved
information. Specifically, it first extracts the model's acquired knowledge
that aligns with expert image-based classification outputs. It then retrieves
relevant supplementary knowledge to further enrich this information. Finally,
by aggregating both sources, RADAR generates more accurate and informative
radiology reports. Extensive experiments on MIMIC-CXR, CheXpert-Plus, and IU
X-ray demonstrate that our model outperforms state-of-the-art LLMs in both
language quality and clinical accuracy

</details>


### [172] [RAVENEA: A Benchmark for Multimodal Retrieval-Augmented Visual Culture Understanding](https://arxiv.org/abs/2505.14462)
*Jiaang Li,Yifei Yuan,Wenyan Li,Mohammad Aliannejadi,Daniel Hershcovich,Anders Søgaard,Ivan Vulić,Wenxuan Zhang,Paul Pu Liang,Yang Deng,Serge Belongie*

Main category: cs.CV

TL;DR: 研究提出RAVENEA基准，通过文化感知检索增强视觉语言模型的文化理解能力，在两项任务中取得显著提升


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在文化细微理解上存在不足，多模态场景中检索增强技术的应用尚未充分探索

Method: 构建包含10,000+人工标注维基文档的RAVENEA基准，开发7种多模态检索器，评估14个先进VLM模型

Result: 检索增强的轻量模型在cVQA和cIC任务分别提升3.2%和6.2%，超越非增强模型

Conclusion: 文化感知检索可有效提升多模态理解，强调文化包容性基准对模型发展的重要性

Abstract: As vision-language models (VLMs) become increasingly integrated into daily
life, the need for accurate visual culture understanding is becoming critical.
Yet, these models frequently fall short in interpreting cultural nuances
effectively. Prior work has demonstrated the effectiveness of
retrieval-augmented generation (RAG) in enhancing cultural understanding in
text-only settings, while its application in multimodal scenarios remains
underexplored. To bridge this gap, we introduce RAVENEA (Retrieval-Augmented
Visual culturE uNdErstAnding), a new benchmark designed to advance visual
culture understanding through retrieval, focusing on two tasks: culture-focused
visual question answering (cVQA) and culture-informed image captioning (cIC).
RAVENEA extends existing datasets by integrating over 10,000 Wikipedia
documents curated and ranked by human annotators. With RAVENEA, we train and
evaluate seven multimodal retrievers for each image query, and measure the
downstream impact of retrieval-augmented inputs across fourteen
state-of-the-art VLMs. Our results show that lightweight VLMs, when augmented
with culture-aware retrieval, outperform their non-augmented counterparts (by
at least 3.2% absolute on cVQA and 6.2% absolute on cIC). This highlights the
value of retrieval-augmented methods and culturally inclusive benchmarks for
multimodal understanding.

</details>


### [173] [Dual Precision Quantization for Efficient and Accurate Deep Neural Networks Inference](https://arxiv.org/abs/2505.14638)
*Tomer Gafni,Asaf Karnieli,Yair Hanani*

Main category: cs.CV

TL;DR: 提出W4A8硬件高效量化方案（4位整数权重存储+8位浮点运算）及DPQ算法，在保持精度的同时显著提升推理速度和内存效率


<details>
  <summary>Details</summary>
Motivation: 针对深度神经网络模型规模增长带来的延迟和内存效率瓶颈，现有量化方案在硬件适配与精度平衡方面存在不足，需开发更高效的量化方法

Method: 采用权重4位整数存储（W4）与8位浮点计算（A8）的混合精度架构，配套开发双精度量化算法（DPQ）优化量化过程

Result: 实验显示方案在多种加速器上实现吞吐量提升（较16位操作）和内存占用优化，精度损失控制在0.5-1.2%可接受范围

Conclusion: W4A8量化方案在硬件效率与模型精度间取得良好平衡，其通用设计适配现代加速器，为边缘计算提供有效解决方案

Abstract: Deep neural networks have achieved state-of-the-art results in a wide range
of applications, from natural language processing and computer vision to speech
recognition. However, as tasks become increasingly complex, model sizes
continue to grow, posing challenges in latency and memory efficiency. To meet
these constraints, post-training quantization has emerged as a promising
solution. In this paper, we propose a novel hardware-efficient quantization and
inference scheme that exploits hardware advantages with minimal accuracy
degradation. Specifically, we introduce a W4A8 scheme, where weights are
quantized and stored using 4-bit integer precision, and inference computations
are performed using 8-bit floating-point arithmetic, demonstrating significant
speedups and improved memory utilization compared to 16-bit operations,
applicable on various modern accelerators. To mitigate accuracy loss, we
develop a novel quantization algorithm, dubbed Dual Precision Quantization
(DPQ), that leverages the unique structure of our scheme without introducing
additional inference overhead. Experimental results demonstrate improved
performance (i.e., increased throughput) while maintaining tolerable accuracy
degradation relative to the full-precision model.

</details>


### [174] [Beyond Words: Multimodal LLM Knows When to Speak](https://arxiv.org/abs/2505.14654)
*Zikai Liao,Yi Ouyang,Yi-Lun Lee,Chen-Ping Yu,Yi-Hsuan Tsai,Zhaozheng Yin*

Main category: cs.CV

TL;DR: 提出MM-When2Speak多模态模型，通过视觉/听觉/文本整合预测对话响应时机，相比单模态模型响应准确率提升4倍


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在实时对话中难以判断何时进行简短回应，主要受限于文本输入的单一模态特征

Method: 构建真实对话视频的多模态数据集，开发基于LLM的多模态框架（MM-When2Speak）实现响应时机和类型预测

Result: 实验显示MM-When2Speak在响应时机准确率上显著优于现有模型，达到商用LLM的4倍提升

Conclusion: 多模态输入对实现自然流畅的对话AI具有关键作用，需整合视听文本特征提升响应判断能力

Abstract: While large language model (LLM)-based chatbots have demonstrated strong
capabilities in generating coherent and contextually relevant responses, they
often struggle with understanding when to speak, particularly in delivering
brief, timely reactions during ongoing conversations. This limitation arises
largely from their reliance on text input, lacking the rich contextual cues in
real-world human dialogue. In this work, we focus on real-time prediction of
response types, with an emphasis on short, reactive utterances that depend on
subtle, multimodal signals across vision, audio, and text. To support this, we
introduce a new multimodal dataset constructed from real-world conversational
videos, containing temporally aligned visual, auditory, and textual streams.
This dataset enables fine-grained modeling of response timing in dyadic
interactions. Building on this dataset, we propose MM-When2Speak, a multimodal
LLM-based model that adaptively integrates visual, auditory, and textual
context to predict when a response should occur, and what type of response is
appropriate. Experiments show that MM-When2Speak significantly outperforms
state-of-the-art unimodal and LLM-based baselines, achieving up to a 4x
improvement in response timing accuracy over leading commercial LLMs. These
results underscore the importance of multimodal inputs for producing timely,
natural, and engaging conversational AI.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [175] [MLZero: A Multi-Agent System for End-to-end Machine Learning Automation](https://arxiv.org/abs/2505.13941)
*Haoyang Fang,Boran Han,Nick Erickson,Xiyuan Zhang,Su Zhou,Anirudh Dagar,Jiani Zhang,Ali Caner Turkmen,Cuixiong Hu,Huzefa Rangwala,Ying Nian Wu,Bernie Wang,George Karypis*

Main category: cs.MA

TL;DR: 提出MLZero多智能体框架，通过认知感知模块和记忆增强机制实现全自动机器学习，在多模态任务中显著超越现有方案


<details>
  <summary>Details</summary>
Motivation: 现有AutoML系统在处理多模态数据时仍依赖专家配置，且大语言模型存在代码生成幻觉和API知识过时问题

Method: 1.认知感知模块转化多模态输入为上下文引导 2.语义记忆(API知识库)和情景记忆(历史代码)增强代码生成 3.支持紧凑型8B LLM实现高效自动化

Result: MLE-Bench Lite六项金牌，多模态基准测试成功率0.92(+263.6%)，平均排名2.28，8B模型超越全尺寸系统

Conclusion: MLZero架构突破多模态自动化瓶颈，验证紧凑LLM的可行性，为AutoML提供新范式

Abstract: Existing AutoML systems have advanced the automation of machine learning
(ML); however, they still require substantial manual configuration and expert
input, particularly when handling multimodal data. We introduce MLZero, a novel
multi-agent framework powered by Large Language Models (LLMs) that enables
end-to-end ML automation across diverse data modalities with minimal human
intervention. A cognitive perception module is first employed, transforming raw
multimodal inputs into perceptual context that effectively guides the
subsequent workflow. To address key limitations of LLMs, such as hallucinated
code generation and outdated API knowledge, we enhance the iterative code
generation process with semantic and episodic memory. MLZero demonstrates
superior performance on MLE-Bench Lite, outperforming all competitors in both
success rate and solution quality, securing six gold medals. Additionally, when
evaluated on our Multimodal AutoML Agent Benchmark, which includes 25 more
challenging tasks spanning diverse data modalities, MLZero outperforms the
competing methods by a large margin with a success rate of 0.92 (+263.6\%) and
an average rank of 2.28. Our approach maintains its robust effectiveness even
with a compact 8B LLM, outperforming full-size systems from existing solutions.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [176] [SAE-FiRE: Enhancing Earnings Surprise Predictions Through Sparse Autoencoder Feature Selection](https://arxiv.org/abs/2505.14420)
*Huopu Zhang,Yanguang Liu,Mengnan Du*

Main category: q-fin.CP

TL;DR: 提出SAE-FiRE框架，通过稀疏自编码器从收益电话会议记录中提取关键信息，显著提升盈利意外预测效果


<details>
  <summary>Details</summary>
Motivation: 收益电话会议记录分析面临文本冗余和专业术语挑战，现有方法难以有效提取预测性金融信号

Method: 开发SAE-FiRE框架，利用稀疏自编码器进行模式识别和降噪，专注捕捉具有预测能力的细微金融信号

Result: 实验显示该方法显著超越基线模型，验证了框架在信息提取和预测效果上的优势

Conclusion: SAE-FiRE框架成功解决了金融文本分析中的冗余问题，为收益预测提供了更有效的特征提取方案

Abstract: Predicting earnings surprises through the analysis of earnings conference
call transcripts has attracted increasing attention from the financial research
community. Conference calls serve as critical communication channels between
company executives, analysts, and shareholders, offering valuable
forward-looking information. However, these transcripts present significant
analytical challenges, typically containing over 5,000 words with substantial
redundancy and industry-specific terminology that creates obstacles for
language models. In this work, we propose the Sparse Autoencoder for Financial
Representation Enhancement (SAE-FiRE) framework to address these limitations by
extracting key information while eliminating redundancy. SAE-FiRE employs
Sparse Autoencoders (SAEs) to efficiently identify patterns and filter out
noises, and focusing specifically on capturing nuanced financial signals that
have predictive power for earnings surprises. Experimental results indicate
that the proposed method can significantly outperform comparing baselines.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [177] [Evaluating Large Language Models for Real-World Engineering Tasks](https://arxiv.org/abs/2505.13484)
*Rene Heesch,Sebastian Eilermann,Alexander Windmann,Alexander Diedrich,Philipp Rosenthal,Oliver Niggemann*

Main category: cs.AI

TL;DR: 本文指出当前LLM工程评估存在简化用例和临时场景的缺陷，通过构建覆盖核心工程能力的真实场景数据库（100+问题），发现LLMs在基础推理尚可但抽象建模和工程逻辑能力薄弱。


<details>
  <summary>Details</summary>
Motivation: 现有LLM工程评估依赖易验证的简化用例，缺乏对复杂现实工程问题的系统测试，难以真实反映模型在关键工程任务中的能力。

Method: 构建包含产品设计/预测/诊断等核心能力的真实工程问题数据库，系统性评估4个先进LLM（含云端/本地实例）在复杂任务中的表现。

Result: LLMs在时间/结构推理表现尚可，但在抽象推理（下降35%）、形式化建模（正确率<20%）和上下文敏感逻辑方面存在显著缺陷。

Conclusion: 当前LLMs处理复杂工程任务时存在系统性短板，需针对性改进抽象建模和工程逻辑能力以适应真实生产环境需求。

Abstract: Large Language Models (LLMs) are transformative not only for daily activities
but also for engineering tasks. However, current evaluations of LLMs in
engineering exhibit two critical shortcomings: (i) the reliance on simplified
use cases, often adapted from examination materials where correctness is easily
verifiable, and (ii) the use of ad hoc scenarios that insufficiently capture
critical engineering competencies. Consequently, the assessment of LLMs on
complex, real-world engineering problems remains largely unexplored. This paper
addresses this gap by introducing a curated database comprising over 100
questions derived from authentic, production-oriented engineering scenarios,
systematically designed to cover core competencies such as product design,
prognosis, and diagnosis. Using this dataset, we evaluate four state-of-the-art
LLMs, including both cloud-based and locally hosted instances, to
systematically investigate their performance on complex engineering tasks. Our
results show that LLMs demonstrate strengths in basic temporal and structural
reasoning but struggle significantly with abstract reasoning, formal modeling,
and context-sensitive engineering logic.

</details>


### [178] [Contrastive Cross-Course Knowledge Tracing via Concept Graph Guided Knowledge Transfer](https://arxiv.org/abs/2505.13489)
*Wenkang Han,Wang Lin,Liya Hu,Zhenlong Dai,Yiyun Zhou,Mengze Li,Zemin Liu,Chang Yao,Jingyuan Chen*

Main category: cs.AI

TL;DR: TransKT通过跨课程概念图和对比学习优化知识追踪，突破单课程数据局限


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪模型仅关注单课程数据，难以全面建模学习者的跨领域知识状态

Method: 1. 利用零样本LLM构建跨课程概念图
2. 设计LLM到GCN的语义特征增强管道
3. 采用对比学习对齐单课程与跨课程知识状态

Result: 实现跨课程知识迁移，提升知识状态建模的鲁棒性和预测准确性

Conclusion: 该方法通过整合多课程关联信息，为学习者知识状态提供更全面的表征框架

Abstract: Knowledge tracing (KT) aims to predict learners' future performance based on
historical learning interactions. However, existing KT models predominantly
focus on data from a single course, limiting their ability to capture a
comprehensive understanding of learners' knowledge states. In this paper, we
propose TransKT, a contrastive cross-course knowledge tracing method that
leverages concept graph guided knowledge transfer to model the relationships
between learning behaviors across different courses, thereby enhancing
knowledge state estimation. Specifically, TransKT constructs a cross-course
concept graph by leveraging zero-shot Large Language Model (LLM) prompts to
establish implicit links between related concepts across different courses.
This graph serves as the foundation for knowledge transfer, enabling the model
to integrate and enhance the semantic features of learners' interactions across
courses. Furthermore, TransKT includes an LLM-to-LM pipeline for incorporating
summarized semantic features, which significantly improves the performance of
Graph Convolutional Networks (GCNs) used for knowledge transfer. Additionally,
TransKT employs a contrastive objective that aligns single-course and
cross-course knowledge states, thereby refining the model's ability to provide
a more robust and accurate representation of learners' overall knowledge
states.

</details>


### [179] [Can AI Freelancers Compete? Benchmarking Earnings, Reliability, and Task Success at Scale](https://arxiv.org/abs/2505.13511)
*David Noever,Forrest McKee*

Main category: cs.AI

TL;DR: 提出基于经济数据的LLM自由职业开发者评估框架，Claude 3.5 Haiku以152万美元收益表现最佳


<details>
  <summary>Details</summary>
Motivation: 评估LLM在真实经济场景（自由职业软件开发）中的实际应用能力，填补传统基准测试与真实经济价值评估的差距

Method: 基于Kaggle自由职业数据集构建合成任务，通过价格标准化（中位数250美元）、自动化测试用例和收益计算体系，横向测评四大主流LLM

Result: Claude 3.5 Haiku达成最高收益（152万），强模型在任务失败率（6.7%）与错误分布上显著优于其他模型

Conclusion: 验证AI自由职业者技术可行性，揭示结构化任务评估体系的高扩展优势，同时指出其与真实项目复杂性的评估差距

Abstract: This study explores Large Language Models (LLMs) as autonomous agents for
real-world tasks, including freelance software development. This work presents
a new benchmark that evaluates LLMs on freelance programming and data analysis
tasks derived from economic data. We construct the benchmark using synthetic
tasks created from a Kaggle Freelancer dataset of job postings, with all job
prices standardized to USD (median fixed-project price around $250, and an
average of $306). Each task is accompanied by structured input-output test
cases and an estimated price tag, enabling automated correctness checking and a
monetary performance valuation. This approach is inspired by OpenAI's recent
SWE-Lancer benchmark (1,400 real Upwork tasks worth $1M total). Still, our
framework simplifies evaluation using programmatically testable tasks and
predicted price values, making it highly scalable and repeatable. On this
benchmark, we evaluate four modern LLMs - Claude 3.5 Haiku, GPT-4o-mini, Qwen
2.5, and Mistral. We report each model's accuracy (task success rate and
test-case pass rate) and the total "freelance earnings" it achieves (sum of
prices of solved tasks). Our results show that Claude 3.5 Haiku performs best,
earning approximately $1.52 million USD, followed closely by GPT-4o-mini at
$1.49 million, then Qwen 2.5 ($1.33M) and Mistral ($0.70M). We analyze the
distribution of errors per task and observe that the strongest models solve the
most tasks and rarely fail completely on any project. We discuss the
implications of these results for the feasibility of AI as a freelance
developer, the advantages and limitations of our automated benchmark approach,
and the gap between performance on structured tasks versus the true complexity
of real-world freelance jobs.

</details>


### [180] [BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs](https://arxiv.org/abs/2505.13529)
*Junxiao Yang,Jinzhe Tu,Haoran Liu,Xiaoce Wang,Chujie Zheng,Zhexin Zhang,Shiyao Cui,Caishun Chen,Tiantian He,Hongning Wang,Yew-Soon Ong,Minlie Huang*

Main category: cs.AI

TL;DR: 论文指出大型推理模型（LRMs）存在过度自信导致错误答案的问题，提出BARREL框架通过边界感知推理显著提升模型可靠性（可靠性从39.33%提升至61.48%）。


<details>
  <summary>Details</summary>
Motivation: 当前LRMs在数学逻辑推理中常表现出过度自信却不承认无知，导致事实可靠性风险，需解决这种病态推理模式。

Method: 提出BARREL框架，针对last-minute guessing（末段盲目猜测）和second-thought spiraling（二次思考螺旋）两种病态推理模式，设计边界感知的简洁推理机制。

Result: 实验显示BARREL训练使DeepSeek-R1-Distill-Llama-8B可靠性提升22.15%，同时保持与原始模型相当的准确率。

Conclusion: BARREL框架为构建更可靠的System 2类型LRMs提供了有启发性的方法论，展示了边界控制对提升事实可靠性的有效性。

Abstract: Recent advances in Large Reasoning Models (LRMs) have shown impressive
capabilities in mathematical and logical reasoning. However, current LRMs
rarely admit ignorance or respond with "I don't know". Instead, they often
produce incorrect answers while showing undue confidence, raising concerns
about their factual reliability. In this work, we identify two pathological
reasoning patterns characterized by overthinking that contribute to the
overconfident and incorrect answers: last-minute guessing and second-thought
spiraling. To address these issues, we propose BARREL-a novel framework that
promotes concise and boundary-aware factual reasoning. Our experiments show
that BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B
from 39.33% to 61.48%, while still achieving accuracy comparable to models
finetuned on reasoning data generated by R1. These results demonstrate that our
pilot study is inspiring to build more reliable and factual System 2 LRMs.

</details>


### [181] [Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings](https://arxiv.org/abs/2505.13718)
*Safal Shrestha,Minwu Kim,Aadim Nepal,Anubhav Shrestha,Keith Ross*

Main category: cs.AI

TL;DR: 提出两阶段训练策略（逻辑谜题预热+小样本RLVR），解决数据稀缺环境下推理型LLM训练难题。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR/CoT蒸馏依赖大量优质数据，在数据稀缺场景下存在显著局限性。

Method: 1. 通过Knights&Knaves逻辑谜题蒸馏长思维链进行模型预热
2. 在预热模型上使用≤100样本进行目标领域RLVR训练

Result: 预热模型在MATH/HumanEval+/MMLU-Pro任务表现提升；相同数据量下准确率超基线；保持跨域泛化能力；RLVR训练样本效率提高

Conclusion: 预热机制显著提升数据稀缺环境下的模型推理能力与训练效率，为小样本LLM训练提供有效路径

Abstract: Designing effective reasoning-capable LLMs typically requires training using
Reinforcement Learning with Verifiable Rewards (RLVR) or distillation with
carefully curated Long Chain of Thoughts (CoT), both of which depend heavily on
extensive training data. This creates a major challenge when the amount of
quality training data is scarce. We propose a sample-efficient, two-stage
training strategy to develop reasoning LLMs under limited supervision. In the
first stage, we "warm up" the model by distilling Long CoTs from a toy domain,
namely, Knights \& Knaves (K\&K) logic puzzles to acquire general reasoning
skills. In the second stage, we apply RLVR to the warmed-up model using a
limited set of target-domain examples. Our experiments demonstrate that this
two-phase approach offers several benefits: $(i)$ the warmup phase alone
facilitates generalized reasoning, leading to performance improvements across a
range of tasks, including MATH, HumanEval$^{+}$, and MMLU-Pro. $(ii)$ When both
the base model and the warmed-up model are RLVR trained on the same small
dataset ($\leq100$ examples), the warmed-up model consistently outperforms the
base model; $(iii)$ Warming up before RLVR training allows a model to maintain
cross-domain generalizability even after training on a specific domain; $(iv)$
Introducing warmup in the pipeline improves not only accuracy but also overall
sample efficiency during RLVR training. The results in this paper highlight the
promise of warmup for building robust reasoning LLMs in data-scarce
environments.

</details>


### [182] [Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations](https://arxiv.org/abs/2505.13763)
*Li Ji-An,Hua-Dong Xiong,Robert C. Wilson,Marcelo G. Mattar,Marcus K. Benna*

Main category: cs.AI

TL;DR: 研究通过神经反馈范式量化大语言模型自我监控能力，发现其元认知空间维度远低于神经空间，揭示模型仅能监测部分神经机制，对AI安全有重要启示。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型元认知能力（尤其是对内部神经激活的监控能力）的边界，因该能力可能使模型规避基于神经激活的监管机制，存在安全隐患。

Method: 采用神经科学启发的神经反馈范式：通过句子-标签对（标签对应特定神经表征空间方向的激活），测试模型报告和控制自身激活模式的能力。

Result: 模型能学习报告和控制神经激活，但性能受示例数量、目标神经方向的语义可解释性、方向解释方差等因素影响；元认知空间维度显著低于神经空间维度。

Conclusion: 大语言模型的元认知能力有限，仅能监测部分神经机制。该发现为AI安全提供实证依据，强调需针对性设计安全机制应对此类风险。

Abstract: Large language models (LLMs) can sometimes report the strategies they
actually use to solve tasks, but they can also fail to do so. This suggests
some degree of metacognition -- the capacity to monitor one's own cognitive
processes for subsequent reporting and self-control. Metacognitive abilities
enhance AI capabilities but raise safety concerns, as models might obscure
their internal processes to evade neural-activation-based oversight mechanisms
designed to detect harmful behaviors. Given society's increased reliance on
these models, it is critical that we understand the limits of their
metacognitive abilities, particularly their ability to monitor their internal
activations. To address this, we introduce a neuroscience-inspired
neurofeedback paradigm designed to quantify the ability of LLMs to explicitly
report and control their activation patterns. By presenting models with
sentence-label pairs where labels correspond to sentence-elicited internal
activations along specific directions in the neural representation space, we
demonstrate that LLMs can learn to report and control these activations. The
performance varies with several factors: the number of example pairs provided,
the semantic interpretability of the target neural direction, and the variance
explained by that direction. These results reveal a "metacognitive space" with
dimensionality much lower than the model's neural space, suggesting LLMs can
monitor only a subset of their neural mechanisms. Our findings provide
empirical evidence quantifying metacognitive capabilities in LLMs, with
significant implications for AI safety.

</details>


### [183] [Ice Cream Doesn't Cause Drowning: Benchmarking LLMs Against Statistical Pitfalls in Causal Inference](https://arxiv.org/abs/2505.13770)
*Jin Du,Li Chen,Xun Xian,An Luo,Fangqiao Tian,Ganghua Wang,Charles Doss,Xiaotong Shen,Jie Ding*

Main category: cs.AI

TL;DR: 提出CausalPitfalls基准，系统性评估大语言模型在统计因果推理中规避常见陷阱的能力，发现现有模型存在显著局限。


<details>
  <summary>Details</summary>
Motivation: 现有因果推理基准过于简化，未能覆盖辛普森悖论等统计陷阱，限制了大语言模型在现实场景中的可信应用。

Method: 设计多难度层级的结构化挑战任务，采用直接提示和代码辅助提示两种协议，通过评分标准量化模型可靠性，并与人类专家评估结果进行验证。

Result: 当前大语言模型在统计因果推理中存在系统性缺陷，CausalPitfalls提供了可信因果推理系统的量化评估框架。

Conclusion: 该基准为提升因果推理系统的可信度提供了关键评估标准和改进方向，推动因果推理技术的实际应用。

Abstract: Reliable causal inference is essential for making decisions in high-stakes
areas like medicine, economics, and public policy. However, it remains unclear
whether large language models (LLMs) can handle rigorous and trustworthy
statistical causal inference. Current benchmarks usually involve simplified
tasks. For example, these tasks might only ask LLMs to identify semantic causal
relationships or draw conclusions directly from raw data. As a result, models
may overlook important statistical pitfalls, such as Simpson's paradox or
selection bias. This oversight limits the applicability of LLMs in the real
world. To address these limitations, we propose CausalPitfalls, a comprehensive
benchmark designed to rigorously evaluate the capability of LLMs in overcoming
common causal inference pitfalls. Our benchmark features structured challenges
across multiple difficulty levels, each paired with grading rubrics. This
approach allows us to quantitatively measure both causal reasoning capabilities
and the reliability of LLMs' responses. We evaluate models using two protocols:
(1) direct prompting, which assesses intrinsic causal reasoning, and (2)
code-assisted prompting, where models generate executable code for explicit
statistical analysis. Additionally, we validate the effectiveness of this judge
by comparing its scoring with assessments from human experts. Our results
reveal significant limitations in current LLMs when performing statistical
causal inference. The CausalPitfalls benchmark provides essential guidance and
quantitative metrics to advance the development of trustworthy causal reasoning
systems.

</details>


### [184] [Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation](https://arxiv.org/abs/2505.13887)
*Junyang Wang,Haiyang Xu,Xi Zhang,Ming Yan,Ji Zhang,Fei Huang,Jitao Sang*

Main category: cs.AI

TL;DR: Mobile-Agent-V框架通过视频引导注入操作知识，将移动自动化性能提升36%


<details>
  <summary>Details</summary>
Motivation: 现有移动自动化框架因缺乏操作知识导致效率低下，手动知识注入方式费时费力

Method: 开发基于视频引导的Mobile-Agent-V框架，通过视频内容自动提取操作知识

Result: 提出Mobile-Knowledge评估基准，实验显示性能较现有方法提升36%

Conclusion: Mobile-Agent-V实现了无人工干预的高效知识获取，显著提升移动自动化效率

Abstract: The exponential rise in mobile device usage necessitates streamlined
automation for effective task management, yet many AI frameworks fall short due
to inadequate operational expertise. While manually written knowledge can
bridge this gap, it is often burdensome and inefficient. We introduce
Mobile-Agent-V, an innovative framework that utilizes video as a guiding tool
to effortlessly and efficiently inject operational knowledge into mobile
automation processes. By deriving knowledge directly from video content,
Mobile-Agent-V eliminates manual intervention, significantly reducing the
effort and time required for knowledge acquisition. To rigorously evaluate this
approach, we propose Mobile-Knowledge, a benchmark tailored to assess the
impact of external knowledge on mobile agent performance. Our experimental
findings demonstrate that Mobile-Agent-V enhances performance by 36% compared
to existing methods, underscoring its effortless and efficient advantages in
mobile automation.

</details>


### [185] [Efficient Agent Training for Computer Use](https://arxiv.org/abs/2505.13909)
*Yanheng He,Jiahe Jin,Pengfei Liu*

Main category: cs.AI

TL;DR: PC Agent-E通过小规模高质量轨迹数据（312条人工标注+Claude 3.7合成）训练，在WindowsAgentArena-V2实现141%性能提升，并验证跨操作系统泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决高质量计算机使用轨迹数据稀缺制约AI代理发展的问题，探索小数据量激发智能体能力的可能性。

Method: 1. 初始312条人工标注轨迹 → 2. 利用Claude 3.7 Sonnet合成多样化动作决策 → 3. 构建改进的WindowsAgentArena-V2基准 → 4. 在跨平台场景测试泛化能力

Result: • 性能提升141%（相对基准）
• 超越Claude 3.7长思考版本
• 在OSWorld实现跨操作系统泛化

Conclusion: 高质量小样本数据配合智能合成可有效激发计算机使用能力，为代理训练提供高效新路径。

Abstract: Scaling up high-quality trajectory data has long been a critical bottleneck
for developing human-like computer use agents. We introduce PC Agent-E, an
efficient agent training framework that significantly reduces reliance on
large-scale human demonstrations. Starting with just 312 human-annotated
computer use trajectories, we further improved data quality by synthesizing
diverse action decisions with Claude 3.7 Sonnet. Trained on these enriched
trajectories, our PC Agent-E model achieved a remarkable 141% relative
improvement, surpassing the strong Claude 3.7 Sonnet with extended thinking on
WindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC
Agent-E demonstrates strong generalizability to different operating systems on
OSWorld. Our findings suggest that strong computer use capabilities can be
stimulated from a small amount of high-quality trajectory data.

</details>


### [186] [ProMind-LLM: Proactive Mental Health Care via Causal Reasoning with Sensor Data](https://arxiv.org/abs/2505.14038)
*Xinzhe Zheng,Sijie Ji,Jiawei Sun,Renqi Chen,Wei Gao,Mani Srivastava*

Main category: cs.AI

TL;DR: 提出ProMind-LLM框架，通过整合客观行为数据与主观心理记录，结合领域预训练、自优化机制和因果思维链推理，提升心理健康风险评估的可靠性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有心理健康评估方法过度依赖易受心理不确定性影响的主观文本记录，导致预测结果不稳定且不可靠。需要引入客观数据源和改进算法框架来提高评估质量。

Method: 1. 领域特定预训练适配心理健康场景
2. 自优化机制处理数值行为数据
3. 因果思维链推理增强预测可靠性

Result: 在PMData和Globem两个真实数据集上的实验表明，该方法相较通用大模型取得显著性能提升

Conclusion: ProMind-LLM为开发更可靠、可解释且可扩展的心理健康解决方案提供了有效路径

Abstract: Mental health risk is a critical global public health challenge,
necessitating innovative and reliable assessment methods. With the development
of large language models (LLMs), they stand out to be a promising tool for
explainable mental health care applications. Nevertheless, existing approaches
predominantly rely on subjective textual mental records, which can be distorted
by inherent mental uncertainties, leading to inconsistent and unreliable
predictions. To address these limitations, this paper introduces ProMind-LLM.
We investigate an innovative approach integrating objective behavior data as
complementary information alongside subjective mental records for robust mental
health risk assessment. Specifically, ProMind-LLM incorporates a comprehensive
pipeline that includes domain-specific pretraining to tailor the LLM for mental
health contexts, a self-refine mechanism to optimize the processing of
numerical behavioral data, and causal chain-of-thought reasoning to enhance the
reliability and interpretability of its predictions. Evaluations of two
real-world datasets, PMData and Globem, demonstrate the effectiveness of our
proposed methods, achieving substantial improvements over general LLMs. We
anticipate that ProMind-LLM will pave the way for more dependable,
interpretable, and scalable mental health case solutions.

</details>


### [187] [s3: You Don't Need That Much Data to Train a Search Agent via RL](https://arxiv.org/abs/2505.14146)
*Pengcheng Jiang,Xueqiang Xu,Jiacheng Lin,Jinfeng Xiao,Zifeng Wang,Jimeng Sun,Jiawei Han*

Main category: cs.AI

TL;DR: 提出s3框架解耦搜索器与生成器，通过超越RAG的增益奖励机制，用少量数据实现更优的检索增强生成效果


<details>
  <summary>Details</summary>
Motivation: 现有方法存在检索优化指标与生成目标割裂、联合微调导致模型兼容性差的问题

Method: 开发轻量级模型无关框架s3，分离搜索器训练（使用生成准确率提升作为奖励）与生成过程，支持冻结模型/专有模型

Result: 仅需2.4k训练样本即超越70倍数据量基线，在11个通用/医疗QA基准上持续优于现有方法

Conclusion: s3框架通过解耦搜索与生成过程，在保持模型兼容性的同时显著提升RAG系统的端到端性能

Abstract: Retrieval-augmented generation (RAG) systems empower large language models
(LLMs) to access external knowledge during inference. Recent advances have
enabled LLMs to act as search agents via reinforcement learning (RL), improving
information acquisition through multi-turn interactions with retrieval engines.
However, existing approaches either optimize retrieval using search-only
metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM
to jointly reason and retrieve-entangling retrieval with generation and
limiting the real search utility and compatibility with frozen or proprietary
models. In this work, we propose s3, a lightweight, model-agnostic framework
that decouples the searcher from the generator and trains the searcher using a
Gain Beyond RAG reward: the improvement in generation accuracy over naive RAG.
s3 requires only 2.4k training samples to outperform baselines trained on over
70x more data, consistently delivering stronger downstream performance across
six general QA and five medical QA benchmarks.

</details>


### [188] [Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning](https://arxiv.org/abs/2505.14216)
*Minwu Kim,Anubhav Shrestha,Safal Shrestha,Aadim Nepal,Keith Ross*

Main category: cs.AI

TL;DR: RLVR提升整体准确率但损害最难题的正确率，蒸馏通过引入新知识可同时提升准确率与能力。论文通过实验揭示了两种方法的机制差异。


<details>
  <summary>Details</summary>
Motivation: 探究强化学习验证奖励（RLVR）为何无法提升模型能力，以及蒸馏技术为何能同时提升准确率与能力的底层机制。

Method: 通过小模型实验分析：1）不同难度问题的准确率变化 2）回答质量分布变化 3）蒸馏中知识类型的影响

Result: 1）RLVR专注于提升简单题，损害最难题 2）蒸馏需新知识才能提升能力 3）无新知识蒸馏与RLVR表现相似

Conclusion: 揭示了模型推理行为塑造机制，为优化模型训练方法提供理论依据：需平衡不同难度问题优化，重视知识注入方式。

Abstract: Recent studies have shown that reinforcement learning with verifiable rewards
(RLVR) enhances overall accuracy but fails to improve capability, while
distillation can improve both. In this paper, we investigate the mechanisms
behind these phenomena. First, we demonstrate that RLVR does not improve
capability because it focuses on improving the accuracy of the less-difficult
questions to the detriment of the accuracy of the most difficult questions,
thereby leading to no improvement in capability. Second, we find that RLVR does
not merely increase the success probability for the less difficult questions,
but in our small model settings produces quality responses that were absent in
its output distribution before training. In addition, we show these responses
are neither noticeably longer nor feature more reflection-related keywords,
underscoring the need for more reliable indicators of response quality. Third,
we show that while distillation reliably improves accuracy by learning strong
reasoning patterns, it only improves capability when new knowledge is
introduced. Moreover, when distilling only with reasoning patterns and no new
knowledge, the accuracy of the less-difficult questions improves to the
detriment of the most difficult questions, similar to RLVR. Together, these
findings offer a clearer understanding of how RLVR and distillation shape
reasoning behavior in language models.

</details>


### [189] [SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors](https://arxiv.org/abs/2505.14300)
*Maheep Chaudhary,Fazl Barez*

Main category: cs.AI

TL;DR: 提出了Safety-Net框架，通过无监督集成方法实时监测LLM的有害输出，准确率达96%


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）存在后门触发漏洞，特定输入可诱导生成暴力/色情等有害内容。受核能/航空领域实时监控启发，需开发能捕捉真实因果指标且防止模型欺骗的监测系统。

Method: 1. 类比人类欺骗时的生理指标，监测LLM生成有害内容时的内部行为特征
2. 开发多检测器框架Safety-Net，同时监控线性和非线性表征空间
3. 通过无监督集成方法检测特征关系变化

Result: 在检测后门触发的有害案例中达到96%准确率，成功识别模型通过（a）线性和非线性表征切换（b）特征关系修改实施的欺骗行为

Conclusion: 该框架为应对未来更强大的欺骗性模型提供了有效监控方案，证明无监督多维度监控在AI安全领域的关键作用

Abstract: High-risk industries like nuclear and aviation use real-time monitoring to
detect dangerous system conditions. Similarly, Large Language Models (LLMs)
need monitoring safeguards. We propose a real-time framework to predict harmful
AI outputs before they occur by using an unsupervised approach that treats
normal behavior as the baseline and harmful outputs as outliers. Our study
focuses specifically on backdoor-triggered responses -- where specific input
phrases activate hidden vulnerabilities causing the model to generate unsafe
content like violence, pornography, or hate speech. We address two key
challenges: (1) identifying true causal indicators rather than surface
correlations, and (2) preventing advanced models from deception -- deliberately
evading monitoring systems. Hence, we approach this problem from an
unsupervised lens by drawing parallels to human deception: just as humans
exhibit physical indicators while lying, we investigate whether LLMs display
distinct internal behavioral signatures when generating harmful content. Our
study addresses two critical challenges: 1) designing monitoring systems that
capture true causal indicators rather than superficial correlations; and
2)preventing intentional evasion by increasingly capable "Future models''. Our
findings show that models can produce harmful content through causal mechanisms
and can become deceptive by: (a) alternating between linear and non-linear
representations, and (b) modifying feature relationships. To counter this, we
developed Safety-Net -- a multi-detector framework that monitors different
representation dimensions, successfully detecting harmful behavior even when
information is shifted across representational spaces to evade individual
monitors. Our evaluation shows 96% accuracy in detecting harmful cases using
our unsupervised ensemble approach.

</details>


### [190] [Causal Cartographer: From Mapping to Reasoning Over Counterfactual Worlds](https://arxiv.org/abs/2505.14396)
*Gaël Gendron,Jože M. Rožanec,Michael Witbrock,Gillian Dobbie*

Main category: cs.AI

TL;DR: 提出Causal Cartographer框架，通过提取真实世界因果关系构建知识库，增强大语言模型的因果推理能力


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型仅能记忆已有因果关系，缺乏真正的反事实推理能力，且真实场景中反事实评估困难

Method: 1. 图检索增强生成代理提取因果关系网络
2. 构建因果知识库并生成反事实
3. 开发受因果关系约束的反事实推理代理进行逐步推理

Result: 成功提取因果知识，提升LLMs推理鲁棒性，降低75%推理成本，减少89%虚假关联

Conclusion: 显式建模因果关系可突破语言模型的因果推理局限，为现实世界反事实分析提供可行框架

Abstract: Causal world models are systems that can answer counterfactual questions
about an environment of interest, i.e. predict how it would have evolved if an
arbitrary subset of events had been realized differently. It requires
understanding the underlying causes behind chains of events and conducting
causal inference for arbitrary unseen distributions. So far, this task eludes
foundation models, notably large language models (LLMs), which do not have
demonstrated causal reasoning capabilities beyond the memorization of existing
causal relationships. Furthermore, evaluating counterfactuals in real-world
applications is challenging since only the factual world is observed, limiting
evaluation to synthetic datasets. We address these problems by explicitly
extracting and modeling causal relationships and propose the Causal
Cartographer framework. First, we introduce a graph retrieval-augmented
generation agent tasked to retrieve causal relationships from data. This
approach allows us to construct a large network of real-world causal
relationships that can serve as a repository of causal knowledge and build
real-world counterfactuals. In addition, we create a counterfactual reasoning
agent constrained by causal relationships to perform reliable step-by-step
causal inference. We show that our approach can extract causal knowledge and
improve the robustness of LLMs for causal reasoning tasks while reducing
inference costs and spurious correlations.

</details>


### [191] [PRL: Prompts from Reinforcement Learning](https://arxiv.org/abs/2505.14412)
*Paweł Batorski,Adrian Kosmala,Paul Swoboda*

Main category: cs.AI

TL;DR: 提出基于强化学习的PRL方法，自动生成包含未见示例的提示，在多个NLP任务中实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有提示工程依赖专家经验且难以捕捉关键语义线索，需要自动化方法生成更有效的提示

Method: 采用强化学习框架，通过奖励机制优化提示生成过程，能够创造训练数据之外的少样本示例

Result: 分类任务超越APE 2.58%和EvoPrompt 1%；摘要任务ROUGE提升4.32/2.12；简化任务SARI提升6.93/6.01

Conclusion: PRL展示了自动化提示生成的强大潜力，其生成新颖示例的能力显著提升模型表现，代码已开源

Abstract: Effective prompt engineering remains a central challenge in fully harnessing
the capabilities of LLMs. While well-designed prompts can dramatically enhance
performance, crafting them typically demands expert intuition and a nuanced
understanding of the task. Moreover, the most impactful prompts often hinge on
subtle semantic cues, ones that may elude human perception but are crucial for
guiding LLM behavior. In this paper, we introduce PRL (Prompts from
Reinforcement Learning), a novel RL-based approach for automatic prompt
generation. Unlike previous methods, PRL can produce novel few-shot examples
that were not seen during training. Our approach achieves state-of-the-art
performance across a range of benchmarks, including text classification,
simplification, and summarization. On the classification task, it surpasses
prior methods by 2.58% over APE and 1.00% over EvoPrompt. Additionally, it
improves the average ROUGE scores on the summarization task by 4.32 over APE
and by 2.12 over EvoPrompt and the SARI score on simplification by 6.93 over
APE and by 6.01 over EvoPrompt. Our code is available at
https://github.com/Batorskq/prl .

</details>


### [192] [Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach](https://arxiv.org/abs/2505.14479)
*Oren Sultan,Eitan Stern,Dafna Shahaf*

Main category: cs.AI

TL;DR: 提出神经符号方法提升LLMs在几何证明中的准确性，结合类比问题检索和形式化验证反馈使准确率提升58%-70%


<details>
  <summary>Details</summary>
Motivation: LLMs在需要严格逻辑推理的形式化领域（如数学证明）存在明显局限，需通过结构化组件增强其可靠性

Method: 1. 检索类似问题及其证明指导LLM生成
2. 开发形式化验证器评估证明并提供反馈迭代修正

Result: OpenAI o1模型的证明准确率提升58%-70%，验证器反馈和类比问题检索均有显著贡献

Conclusion: 生成可验证正确结论的LLM架构将大幅提升可靠性，为需要可信度的复杂任务和关键应用开辟新可能

Abstract: Large language models (LLMs) struggle with formal domains that require
rigorous logical deduction and symbolic reasoning, such as mathematical proof
generation. We propose a neuro-symbolic approach that combines LLMs' generative
strengths with structured components to overcome this challenge. As a
proof-of-concept, we focus on geometry problems. Our approach is two-fold: (1)
we retrieve analogous problems and use their proofs to guide the LLM, and (2) a
formal verifier evaluates the generated proofs and provides feedback, helping
the model fix incorrect proofs. We demonstrate that our method significantly
improves proof accuracy for OpenAI's o1 model (58%-70% improvement); both
analogous problems and the verifier's feedback contribute to these gains. More
broadly, shifting to LLMs that generate provably correct conclusions could
dramatically improve their reliability, accuracy and consistency, unlocking
complex tasks and critical real-world applications that require
trustworthiness.

</details>


### [193] [Reasoning Models Better Express Their Confidence](https://arxiv.org/abs/2505.14489)
*Dongkeun Yoon,Seungone Kim,Sohee Yang,Sunkyoung Kim,Soyeon Kim,Yongil Kim,Eunbi Choi,Yireun Kim,Minjoon Seo*

Main category: cs.AI

TL;DR: 推理型大语言模型通过链式思考的慢思考行为（如多角度验证和动态修正）显著提升了信心校准准确率，在36个测试场景中33项表现优于非推理模型


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型存在信心校准偏差问题，这会影响其可靠性。研究者试图验证具备链式思考能力的推理模型是否能同时提升问题解决能力和信心校准准确性

Method: 通过6个推理模型在6个基准数据集上的对比实验，分析思维链中的慢思考行为（替代方案探索、过程回溯等）对校准效果的影响机制

Result: 1. 推理模型校准准确率在83%场景中更优（33/36）
2. 校准精度随思维链延伸逐步提升
3. 移除慢思考特征后校准性能下降14%

Conclusion: 慢思考机制通过动态调整置信度实现了更精准的自我评估，该发现可推广至非推理模型（通过提示工程引导慢思考提升13%校准度）

Abstract: Despite their strengths, large language models (LLMs) often fail to
communicate their confidence accurately, making it difficult to assess when
they might be wrong and limiting their reliability. In this work, we
demonstrate that reasoning models-LLMs that engage in extended chain-of-thought
(CoT) reasoning-exhibit superior performance not only in problem-solving but
also in accurately expressing their confidence. Specifically, we benchmark six
reasoning models across six datasets and find that they achieve strictly better
confidence calibration than their non-reasoning counterparts in 33 out of the
36 settings. Our detailed analysis reveals that these gains in calibration stem
from the slow thinking behaviors of reasoning models-such as exploring
alternative approaches and backtracking-which enable them to adjust their
confidence dynamically throughout their CoT, making it progressively more
accurate. In particular, we find that reasoning models become increasingly
better calibrated as their CoT unfolds, a trend not observed in non-reasoning
models. Moreover, removing slow thinking behaviors from the CoT leads to a
significant drop in calibration. Lastly, we show that these gains are not
exclusive to reasoning models-non-reasoning models also benefit when guided to
perform slow thinking via in-context learning.

</details>


### [194] [Agent Context Protocols Enhance Collective Inference](https://arxiv.org/abs/2505.14569)
*Devansh Bhardwaj,Arjun Beniwal,Shreyas Chaudhari,Ashwin Kalyan,Tanmay Rajpurohit,Karthik R. Narasimhan,Ameet Deshpande,Vishvak Murahari*

Main category: cs.AI

TL;DR: 提出ACP结构化协议，通过执行蓝图与标准化消息提升多智能体协作性能


<details>
  <summary>Details</summary>
Motivation: 现有自然语言协调方式限制智能体间复杂交互与领域适配，需结构化通信协议

Method: 结合持久化执行蓝图（存储中间输出）与标准化消息架构，实现容错型集体推理

Result: 在长期网络辅助任务达到28.3%准确率，多模态技术报告生成优于商业系统

Conclusion: ACP模块化设计显著提升通用智能体系统构建效率与性能上限

Abstract: AI agents have become increasingly adept at complex tasks such as coding,
reasoning, and multimodal understanding. However, building generalist systems
requires moving beyond individual agents to collective inference -- a paradigm
where multi-agent systems with diverse, task-specialized agents complement one
another through structured communication and collaboration. Today, coordination
is usually handled with imprecise, ad-hoc natural language, which limits
complex interaction and hinders interoperability with domain-specific agents.
We introduce Agent context protocols (ACPs): a domain- and agent-agnostic
family of structured protocols for agent-agent communication, coordination, and
error handling. ACPs combine (i) persistent execution blueprints -- explicit
dependency graphs that store intermediate agent outputs -- with (ii)
standardized message schemas, enabling robust and fault-tolerant multi-agent
collective inference. ACP-powered generalist systems reach state-of-the-art
performance: 28.3 % accuracy on AssistantBench for long-horizon web assistance
and best-in-class multimodal technical reports, outperforming commercial AI
systems in human evaluation. ACPs are highly modular and extensible, allowing
practitioners to build top-tier generalist agents quickly.

</details>


### [195] [SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas](https://arxiv.org/abs/2505.14615)
*Anjiang Wei,Yuheng Wu,Yingjia Wan,Tarun Suresh,Huanmi Tan,Zhanke Zhou,Sanmi Koyejo,Ke Wang,Alex Aiken*

Main category: cs.AI

TL;DR: SATBench通过自动生成可调节难度的SAT逻辑谜题，构建了评估大语言模型搜索式逻辑推理能力的基准测试平台


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注基于推理规则的逻辑能力测试，而忽视了需要主动搜索解决方案的SAT类问题，需要构建专门测试LLM搜索式逻辑推理能力的评估体系

Method: 将SAT公式转化为故事情境和约束条件，通过LLM生成2100个验证过的谜题，采用双校验机制（LLM辅助检查+求解器验证）保证数据质量，并通过调节子句数量控制难度

Result: 最佳模型o4-mini在困难UNSAT问题上准确率仅65.0%，接近50%的随机基准，暴露LLM在复杂逻辑搜索中的根本性缺陷

Conclusion: SATBench揭示了当前LLM在搜索式逻辑推理的能力瓶颈，其自动化生成框架为逻辑推理研究的可扩展性测试提供了新范式

Abstract: We introduce SATBench, a benchmark for evaluating the logical reasoning
capabilities of large language models (LLMs) through logical puzzles derived
from Boolean satisfiability (SAT) problems. Unlike prior work that focuses on
inference rule-based reasoning, which often involves deducing conclusions from
a set of premises, our approach leverages the search-based nature of SAT
problems, where the objective is to find a solution that fulfills a specified
set of logical constraints. Each instance in SATBench is generated from a SAT
formula, then translated into a story context and conditions using LLMs. The
generation process is fully automated and allows for adjustable difficulty by
varying the number of clauses. All 2100 puzzles are validated through both
LLM-assisted and solver-based consistency checks, with human validation on a
subset. Experimental results show that even the strongest model, o4-mini,
achieves only 65.0% accuracy on hard UNSAT problems, close to the random
baseline of 50%. SATBench exposes fundamental limitations in the search-based
logical reasoning abilities of current LLMs and provides a scalable testbed for
future research in logical reasoning.

</details>


### [196] [Debating for Better Reasoning: An Unsupervised Multimodal Approach](https://arxiv.org/abs/2505.14627)
*Ashutosh Adhikari,Mirella Lapata*

Main category: cs.AI

TL;DR: 将辩论范式扩展到多模态VQA任务，通过视觉专家模型辩论+盲审裁决机制，使较弱模型能监督提升更强模型性能


<details>
  <summary>Details</summary>
Motivation: 解决LLMs跨域评估难题，当模型能力超越人类时，探索辩论机制作为有效监督手段

Method: 双视觉专家模型自主辩论，盲审法官仅根据论点质量裁决。专家仅捍卫真实答案，聚焦分歧实例，避免角色扮演干扰

Result: 辩论框架在多模态任务中持续超越单专家模型，弱LLM的裁决可通过微调提升视觉语言模型的推理能力

Conclusion: 成功验证多模态辩论框架的有效性，证明弱模型监督强模型的可行性，为复杂环境下的模型对齐提供新思路

Abstract: As Large Language Models (LLMs) gain expertise across diverse domains and
modalities, scalable oversight becomes increasingly challenging, particularly
when their capabilities may surpass human evaluators. Debate has emerged as a
promising mechanism for enabling such oversight. In this work, we extend the
debate paradigm to a multimodal setting, exploring its potential for weaker
models to supervise and enhance the performance of stronger models. We focus on
visual question answering (VQA), where two "sighted" expert vision-language
models debate an answer, while a "blind" (text-only) judge adjudicates based
solely on the quality of the arguments. In our framework, the experts defend
only answers aligned with their beliefs, thereby obviating the need for
explicit role-playing and concentrating the debate on instances of expert
disagreement. Experiments on several multimodal tasks demonstrate that the
debate framework consistently outperforms individual expert models. Moreover,
judgments from weaker LLMs can help instill reasoning capabilities in
vision-language models through finetuning.

</details>


### [197] [SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment](https://arxiv.org/abs/2505.14667)
*Wonje Jeung,Sangyeon Yoon,Minsuk Kahng,Albert No*

Main category: cs.AI

TL;DR: 提出SAFEPATH方法，通过在推理开始时插入8-token安全提示，在保持推理性能的同时有效降低90%有害输出并阻挡83.3%越狱攻击


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法会降低模型推理深度，且在复杂多步任务中存在安全性与性能的显著权衡，难以抵御复杂越狱攻击

Method: 对LRMs进行轻量级微调，使其在检测到有害提示时首先生成简短Safety Primer，后续推理过程保持非监督状态

Result: DeepSeek-R1-Distill-Llama-8B模型上计算资源消耗减少295倍，零样本变体无需微调即有效，同时提出现有LLM方法在推理模型上的适用性分析框架

Conclusion: SAFEPATH开创性地平衡安全与推理性能，揭示推理模型安全对齐的特殊性，为更安全的AI系统提供新方向

Abstract: Large Reasoning Models (LRMs) have become powerful tools for complex problem
solving, but their structured reasoning pathways can lead to unsafe outputs
when exposed to harmful prompts. Existing safety alignment methods reduce
harmful outputs but can degrade reasoning depth, leading to significant
trade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated
jailbreak attacks. To address this, we introduce SAFEPATH, a lightweight
alignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at
the start of their reasoning, in response to harmful prompts, while leaving the
rest of the reasoning process unsupervised. Empirical results across multiple
benchmarks indicate that SAFEPATH effectively reduces harmful outputs while
maintaining reasoning performance. Specifically, SAFEPATH reduces harmful
responses by up to 90.0% and blocks 83.3% of jailbreak attempts in the
DeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than
Direct Refusal and 314.1x less than SafeChain. We further introduce a zero-shot
variant that requires no fine-tuning. In addition, we provide a comprehensive
analysis of how existing methods in LLMs generalize, or fail, when applied to
reasoning-centric models, revealing critical gaps and new directions for safer
AI.

</details>


### [198] [ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions](https://arxiv.org/abs/2505.14668)
*Bufang Yang,Lilin Xu,Liekang Zeng,Kaiwei Liu,Siyang Jiang,Wenrui Lu,Hongkai Chen,Xiaofan Jiang,Guoliang Xing,Zhenyu Yan*

Main category: cs.AI

TL;DR: 提出首个结合多维度感知上下文的ContextAgent智能代理，通过穿戴设备数据理解用户意图，在主动服务预测和工具调用准确率上分别提升8.5%和6.0%。


<details>
  <summary>Details</summary>
Motivation: 现有主动代理依赖封闭环境观察或基于规则的通知，导致用户意图理解不足且功能有限。需要整合多维度上下文提升LLM代理的主动服务能力。

Method: ContextAgent从穿戴设备提取视频/音频等多维感知上下文，结合历史人物上下文预测主动服务需求，并自动调用工具实现无干扰协助。

Result: 在包含9个场景、20个工具的ContextAgentBench基准测试中，主动预测和工具调用准确率分别比基线提高8.5%和6.0%。

Conclusion: 该研究为开发更先进、以人为中心的主动AI助手奠定基础，展示了多维度上下文整合对提升LLM代理主动服务能力的有效性。

Abstract: Recent advances in Large Language Models (LLMs) have propelled intelligent
agents from reactive responses to proactive support. While promising, existing
proactive agents either rely exclusively on observations from enclosed
environments (e.g., desktop UIs) with direct LLM inference or employ rule-based
proactive notifications, leading to suboptimal user intent understanding and
limited functionality for proactive service. In this paper, we introduce
ContextAgent, the first context-aware proactive agent that incorporates
extensive sensory contexts to enhance the proactive capabilities of LLM agents.
ContextAgent first extracts multi-dimensional contexts from massive sensory
perceptions on wearables (e.g., video and audio) to understand user intentions.
ContextAgent then leverages the sensory contexts and the persona contexts from
historical data to predict the necessity for proactive services. When proactive
assistance is needed, ContextAgent further automatically calls the necessary
tools to assist users unobtrusively. To evaluate this new task, we curate
ContextAgentBench, the first benchmark for evaluating context-aware proactive
LLM agents, covering 1,000 samples across nine daily scenarios and twenty
tools. Experiments on ContextAgentBench show that ContextAgent outperforms
baselines by achieving up to 8.5% and 6.0% higher accuracy in proactive
predictions and tool calling, respectively. We hope our research can inspire
the development of more advanced, human-centric, proactive AI assistants.

</details>


### [199] [Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training](https://arxiv.org/abs/2505.14681)
*Mengru Wang,Xingyu Chen,Yue Wang,Zhiwei He,Jiahao Xu,Tian Liang,Qiuzhi Liu,Yunzhi Yao,Wenxuan Wang,Ruotian Ma,Haitao Mi,Ningyu Zhang,Zhaopeng Tu,Xiaolong Li,Dong Yu*

Main category: cs.AI

TL;DR: 提出RICE方法，通过强化认知专家提升MoE推理模型的效率和准确性


<details>
  <summary>Details</summary>
Motivation: 现有MoE推理模型存在认知效率低下（如过度思考与思考不足）的问题，需要无需额外训练的优化方案

Method: 利用nPMI识别处理元级推理的认知专家，构建推理时引导机制

Result: 在DeepSeek-R1等模型上实现推理准确率提升，效率优于提示工程等传统方法

Conclusion: RICE为增强推理模型的认知效率提供了轻量级且可解释的解决方案

Abstract: Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)
have achieved impressive reasoning capabilities by selectively activating
experts to facilitate structured cognitive processes. Despite notable advances,
existing reasoning models often suffer from cognitive inefficiencies like
overthinking and underthinking. To address these limitations, we introduce a
novel inference-time steering methodology called Reinforcing Cognitive Experts
(RICE), designed to improve reasoning performance without additional training
or complex heuristics. Leveraging normalized Pointwise Mutual Information
(nPMI), we systematically identify specialized experts, termed ''cognitive
experts'' that orchestrate meta-level reasoning operations characterized by
tokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs
(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning
benchmarks demonstrate noticeable and consistent improvements in reasoning
accuracy, cognitive efficiency, and cross-domain generalization. Crucially, our
lightweight approach substantially outperforms prevalent reasoning-steering
techniques, such as prompt design and decoding constraints, while preserving
the model's general instruction-following skills. These results highlight
reinforcing cognitive experts as a promising, practical, and interpretable
direction to enhance cognitive efficiency within advanced reasoning models.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [200] [AdAEM: An Adaptively and Automated Extensible Measurement of LLMs' Value Difference](https://arxiv.org/abs/2505.13531)
*Shitong Duan,Xiaoyuan Yi,Peng Zhang,Dongkuan Xu,Jing Yao,Tun Lu,Ning Gu,Xing Xie*

Main category: cs.CY

TL;DR: 提出AdAEM框架，通过自适应生成测试问题揭示LLMs的价值差异，解决传统数据集信息过时、区分度不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有价值测量数据集存在信息过时、污染和问题泛化问题，导致模型价值差异分析结果趋同且缺乏信息量。

Method: 基于上下文优化策略，自动生成最新/文化争议性议题，通过信息论目标最大化提取LLMs内部价值边界，实现测试集的动态扩展。

Result: 生成12,310个基于Schwartz价值理论的问题，有效分析16个LLMs的价值差异，验证方法有效性。

Conclusion: AdAEM能与LLMs协同进化，持续追踪模型价值动态，为价值研究提供更精准的评估工具。

Abstract: Assessing Large Language Models (LLMs)' underlying value differences enables
comprehensive comparison of their misalignment, cultural adaptability, and
biases. Nevertheless, current value measurement datasets face the
informativeness challenge: with often outdated, contaminated, or generic test
questions, they can only capture the shared value orientations among different
LLMs, leading to saturated and thus uninformative results. To address this
problem, we introduce AdAEM, a novel, self-extensible assessment framework for
revealing LLMs' inclinations. Distinct from previous static benchmarks, AdAEM
can automatically and adaptively generate and extend its test questions. This
is achieved by probing the internal value boundaries of a diverse set of LLMs
developed across cultures and time periods in an in-context optimization
manner. The optimization process theoretically maximizes an
information-theoretic objective to extract the latest or culturally
controversial topics, providing more distinguishable and informative insights
about models' value differences. In this way, AdAEM is able to co-evolve with
the development of LLMs, consistently tracking their value dynamics. Using
AdAEM, we generate 12,310 questions grounded in Schwartz Value Theory, conduct
an extensive analysis to manifest our method's validity and effectiveness, and
benchmark the values of 16 LLMs, laying the groundwork for better value
research.

</details>
