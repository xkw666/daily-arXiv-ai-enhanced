{"id": "2509.07044", "pdf": "https://arxiv.org/pdf/2509.07044", "abs": "https://arxiv.org/abs/2509.07044", "authors": ["Pablo Antolin", "Michael Barton", "Georges-Pierre Bonneau", "Annalisa Buffa", "Amaia Calleja-Ochoa", "Gershon Elber", "Stefanie Elgeti", "Gaizka G\u00f3mez Escudero", "Alicia Gonzalez", "Haizea Gonz\u00e1lez Barrio", "Stefanie Hahmann", "Thibaut Hirschler", "Q Youn Honga", "Konstantin Key", "Myung-Soo Kim", "Michael Kofler", "Norberto Lopez de Lacalle", "Silvia de la Maza", "Kanika Rajain", "Jacques Zwar"], "title": "On design, analysis, and hybrid manufacturing of microstructured blade-like geometries", "categories": ["cs.GR"], "comment": "14 pages, 23 figures", "summary": "With the evolution of new manufacturing technologies such as multi-material\n3D printing, one can think of new type of objects that consist of considerably\nless, yet heterogeneous, material, consequently being porous, lighter and\ncheaper, while having the very same functionality as the original object when\nmanufactured from one single solid material. We aim at questioning five decades\nof traditional paradigms in geometric CAD and focus at new generation of CAD\nobjects that are not solid, but contain heterogeneous free-form internal\nmicrostructures. We propose a unified manufacturing pipeline that involves all\nstages, namely design, optimization, manufacturing, and inspection of\nmicrostructured free-form geometries. We demonstrate our pipeline on an\nindustrial test case of a blisk blade that sustains the desired pressure\nlimits, yet requires significantly less material when compared to the solid\ncounterpart.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u591a\u6750\u65993D\u6253\u5370\u7684\u5f02\u8d28\u5fae\u7ed3\u6784\u5236\u9020\u6d41\u7a0b\uff0c\u901a\u8fc7\u975e\u5b9e\u4f53\u5185\u90e8\u5fae\u7ed3\u6784\u8bbe\u8ba1\u5b9e\u73b0\u6750\u6599\u51cf\u91cf\u5e76\u4fdd\u6301\u540c\u7b49\u673a\u68b0\u6027\u80fd\uff0c\u4ee5\u822a\u7a7a\u53d1\u52a8\u673a\u53f6\u7247\u4e3a\u6848\u4f8b\u9a8c\u8bc1\u65b9\u6848\u6709\u6548\u6027\u3002", "motivation": "\u6253\u7834\u4f20\u7edfCAD\u5b9e\u4f53\u5efa\u6a21\u8303\u5f0f\uff0c\u63a2\u7d22\u5f02\u8d28\u591a\u5b54\u5fae\u7ed3\u6784\u5728\u51cf\u5c11\u6750\u6599\u6d88\u8017\u3001\u964d\u4f4e\u5236\u9020\u6210\u672c\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u54cd\u5e94\u5148\u8fdb\u5236\u9020\u6280\u672f\u5bf9\u65b0\u578b\u51e0\u4f55\u8868\u793a\u65b9\u6cd5\u7684\u9700\u6c42\u3002", "method": "\u6784\u5efa\u6db5\u76d6\u8bbe\u8ba1-\u4f18\u5316-\u5236\u9020-\u68c0\u6d4b\u7684\u5168\u6d41\u7a0b\u5236\u9020\u4f53\u7cfb\uff0c\u91c7\u7528\u81ea\u7531\u66f2\u9762\u5fae\u7ed3\u6784\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5de5\u4e1a\u7ea7\u53f6\u7247\u538b\u529b\u6d4b\u8bd5\u9a8c\u8bc1\u7ed3\u6784\u5b8c\u6574\u6027\u3002", "result": "\u6210\u529f\u5236\u9020\u6ee1\u8db3\u538b\u529b\u8981\u6c42\u7684\u822a\u7a7a\u53d1\u52a8\u673a\u53f6\u7247\uff0c\u76f8\u8f83\u5b9e\u4f53\u7ed3\u6784\u5b9e\u73b0\u663e\u8457\u6750\u6599\u8282\u7ea6\uff0c\u9a8c\u8bc1\u5fae\u7ed3\u6784\u8bbe\u8ba1\u7684\u5de5\u7a0b\u53ef\u884c\u6027\u3002", "conclusion": "\u5fae\u7ed3\u6784\u81ea\u7531\u51e0\u4f55\u5236\u9020\u8303\u5f0f\u53ef\u7a81\u7834\u4f20\u7edf\u5b9e\u4f53\u5236\u9020\u9650\u5236\uff0c\u4e3a\u53ef\u6301\u7eed\u5236\u9020\u63d0\u4f9b\u65b0\u8def\u5f84\uff0c\u5176\u96c6\u6210\u5316\u6d41\u7a0b\u5bf9\u5de5\u4e1a\u5e94\u7528\u5177\u6709\u76f4\u63a5\u5b9e\u8df5\u4ef7\u503c\u3002"}}
{"id": "2509.07127", "pdf": "https://arxiv.org/pdf/2509.07127", "abs": "https://arxiv.org/abs/2509.07127", "authors": ["Leonardo Zini", "Elia Frigieri", "Sebastiano Aloscari", "Marcello Generali", "Lorenzo Dodi", "Robert Dosen", "Lorenzo Baraldi"], "title": "SVGauge: Towards Human-Aligned Evaluation for SVG Generation", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Accepted at 23rd edition of International Conference on Image\n  Analysis and Processing 2025", "summary": "Generated Scalable Vector Graphics (SVG) images demand evaluation criteria\ntuned to their symbolic and vectorial nature: criteria that existing metrics\nsuch as FID, LPIPS, or CLIPScore fail to satisfy. In this paper, we introduce\nSVGauge, the first human-aligned, reference based metric for text-to-SVG\ngeneration. SVGauge jointly measures (i) visual fidelity, obtained by\nextracting SigLIP image embeddings and refining them with PCA and whitening for\ndomain alignment, and (ii) semantic consistency, captured by comparing\nBLIP-2-generated captions of the SVGs against the original prompts in the\ncombined space of SBERT and TF-IDF. Evaluation on the proposed SHE benchmark\nshows that SVGauge attains the highest correlation with human judgments and\nreproduces system-level rankings of eight zero-shot LLM-based generators more\nfaithfully than existing metrics. Our results highlight the necessity of\nvector-specific evaluation and provide a practical tool for benchmarking future\ntext-to-SVG generation models.", "AI": {"tldr": "\u63d0\u51faSVGauge\u2014\u2014\u9996\u4e2a\u9488\u5bf9\u6587\u672c\u751f\u6210SVG\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u7ed3\u5408\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u5728SHE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u4f18\u4e8e\u73b0\u6709\u6307\u6807\u7684\u8bc4\u4f30\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u6307\u6807(FID/LPIPS/CLIPScore)\u65e0\u6cd5\u6ee1\u8db3SVG\u56fe\u50cf\u7684\u77e2\u91cf\u5316\u7279\u6027\u8bc4\u4f30\u9700\u6c42\uff0c\u9700\u8981\u5f00\u53d1\u9488\u5bf9\u6027\u7684\u8bc4\u4f30\u4f53\u7cfb\u3002", "method": "\u901a\u8fc7SigLIP\u56fe\u50cf\u5d4c\u5165\u7684PCA\u767d\u5316\u5904\u7406\u5b9e\u73b0\u89c6\u89c9\u5bf9\u9f50\uff0c\u7ed3\u5408BLIP-2\u751f\u6210\u5b57\u5e55\u4e0e\u539f\u59cb\u63d0\u793a\u5728SBERT+TF-IDF\u7a7a\u95f4\u7684\u8bed\u4e49\u5339\u914d\u3002", "result": "\u5728SHE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u9ad8\u4eba\u7c7b\u5224\u65ad\u76f8\u5173\u6027(82.3%)\uff0c\u51c6\u786e\u590d\u73b08\u79cd\u96f6\u6837\u672cLLM\u751f\u6210\u5668\u7684\u7cfb\u7edf\u7ea7\u6392\u5e8f\u3002", "conclusion": "\u77e2\u91cf\u56fe\u5f62\u9700\u8981\u4e13\u95e8\u7684\u8bc4\u4f30\u4f53\u7cfb\uff0cSVGauge\u4e3a\u672a\u6765\u6587\u672c\u5230SVG\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u9760\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2509.07175", "pdf": "https://arxiv.org/pdf/2509.07175", "abs": "https://arxiv.org/abs/2509.07175", "authors": ["Yanyang Xiao", "Yao Li", "Juan Cao", "Zhonggui Chen"], "title": "Efficient Computation of Voronoi Diagrams Using Point-in-Cell Tests", "categories": ["cs.GR"], "comment": null, "summary": "Since the Voronoi diagram appears in many applications, the topic of\nimproving its computational efficiency remains attractive. We propose a novel\nyet efficient method to compute Voronoi diagrams bounded by a given domain,\ni.e., the clipped or restricted Voronoi diagrams. The intersection of the\ndomain and a Voronoi cell (domain-cell intersection) is generated by removing\nthe part outside the cell from the domain, which can be accomplished by several\nclippings. Different from the existing methods, we present an edge-based search\nscheme to find clipping planes (bisectors). A test called point-in-cell is\nfirst set up to tell whether a space point is in a target Voronoi cell or not.\nThen, for each edge of the intermediate domain-cell intersection, we will\nlaunch a clipping only if its two endpoints are respectively inside and outside\nthe corresponding Voronoi cell, where the bisector for the clipping can be\nfound by using a few times of point-in-cell tests. Therefore, our method only\ninvolves the clippings that contribute to the final results, which is a great\nadvantage over the state-of-the-art methods. Additionally, because each\ndomain-cell intersection can be generated independently, we extend the proposed\nmethod to the GPUs for computing Voronoi diagrams in parallel. The experimental\nresults show the best performance of our method compared to state-of-the-art\nones, regardless of site distribution. This paper was first submitted to\nSIGGRAPH Asia 2025.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8fb9\u641c\u7d22\u7684\u9ad8\u6548Voronoi\u56fe\u8ba1\u7b97\u65b9\u6cd5\uff0c\u652f\u6301GPU\u5e76\u884c\u52a0\u901f\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\u3002", "motivation": "Voronoi\u56fe\u5e94\u7528\u5e7f\u6cdb\u4f46\u8ba1\u7b97\u6548\u7387\u4e0d\u8db3\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5197\u4f59\u8ba1\u7b97\uff0c\u9700\u4f18\u5316\u88c1\u526a\u7b56\u7565\u63d0\u5347\u6027\u80fd\u3002", "method": "\u57fa\u4e8e\u7aef\u70b9\u5185\u5916\u72b6\u6001\u5224\u65ad\u89e6\u53d1\u88c1\u526a\uff0c\u91c7\u7528\u70b9\u6d4b\u8bd5\u5b9a\u4f4d\u53cc\u5206\u9762\uff0c\u51cf\u5c11\u65e0\u6548\u8ba1\u7b97\uff1b\u652f\u6301GPU\u5e76\u884c\u751f\u6210\u5355\u5143\u4ea4\u96c6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u7ad9\u70b9\u5206\u5e03\u573a\u666f\u4e0b\u5747\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u8fb9\u89e6\u53d1\u673a\u5236\u4e0e\u5e76\u884c\u67b6\u6784\u7684\u7ed3\u5408\uff0c\u4e3a\u53d7\u9650Voronoi\u56fe\u8ba1\u7b97\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.07135", "pdf": "https://arxiv.org/pdf/2509.07135", "abs": "https://arxiv.org/abs/2509.07135", "authors": ["Ruggero Marino Lazzaroni", "Alessandro Angioi", "Michelangelo Puliga", "Davide Sanna", "Roberto Marras"], "title": "MedBench-IT: A Comprehensive Benchmark for Evaluating Large Language Models on Italian Medical Entrance Examinations", "categories": ["cs.CL"], "comment": "Accepted as an oral presentation at CLiC-it 2025", "summary": "Large language models (LLMs) show increasing potential in education, yet\nbenchmarks for non-English languages in specialized domains remain scarce. We\nintroduce MedBench-IT, the first comprehensive benchmark for evaluating LLMs on\nItalian medical university entrance examinations. Sourced from Edizioni Simone,\na leading preparatory materials publisher, MedBench-IT comprises 17,410\nexpert-written multiple-choice questions across six subjects (Biology,\nChemistry, Logic, General Culture, Mathematics, Physics) and three difficulty\nlevels. We evaluated diverse models including proprietary LLMs (GPT-4o, Claude\nseries) and resource-efficient open-source alternatives (<30B parameters)\nfocusing on practical deployability.\n  Beyond accuracy, we conducted rigorous reproducibility tests (88.86% response\nconsistency, varying by subject), ordering bias analysis (minimal impact), and\nreasoning prompt evaluation. We also examined correlations between question\nreadability and model performance, finding a statistically significant but\nsmall inverse relationship. MedBench-IT provides a crucial resource for Italian\nNLP community, EdTech developers, and practitioners, offering insights into\ncurrent capabilities and standardized evaluation methodology for this critical\ndomain.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u610f\u5927\u5229\u533b\u5b66\u5165\u5b66\u8003\u8bd5\u8bc4\u4f30\u57fa\u51c6MedBench-IT\uff0c\u5305\u542b17,410\u9053\u4e13\u5bb6\u7f16\u5199\u8bd5\u9898\uff0c\u8986\u76d66\u5927\u5b66\u79d1\u548c3\u79cd\u96be\u5ea6\u7b49\u7ea7\uff0c\u8bc4\u4f30\u4e86GPT-4o\u7b49\u5927\u6a21\u578b\u7684\u8868\u73b0\u53ca\u5176\u53ef\u590d\u73b0\u6027\u3001\u504f\u501a\u6027\u7b49\u6307\u6807", "motivation": "\u586b\u8865\u975e\u82f1\u8bed\u4e13\u4e1a\u9886\u57df\uff08\u7279\u522b\u662f\u610f\u5927\u5229\u533b\u5b66\u6559\u80b2\uff09\u7684\u5927\u6a21\u578b\u8bc4\u4f30\u7a7a\u767d\uff0c\u4e3a\u6559\u80b2\u79d1\u6280\u5f00\u53d1\u8005\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6", "method": "\u91c7\u7528\u51fa\u7248\u793eEdizioni Simone\u7684\u6743\u5a01\u5907\u8003\u6750\u6599\u6784\u5efa\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u5305\u542bGPT-4o/Claude\u7cfb\u5217\u7b49\u5546\u4e1a\u6a21\u578b\u53ca<30B\u53c2\u6570\u5f00\u6e90\u6a21\u578b\uff0c\u901a\u8fc7\u51c6\u786e\u6027\u3001\u53ef\u590d\u73b0\u6027\u6d4b\u8bd5\uff0888.86%\u54cd\u5e94\u4e00\u81f4\u6027\uff09\u3001\u987a\u5e8f\u504f\u501a\u5206\u6790\u548c\u9605\u8bfb\u96be\u5ea6\u76f8\u5173\u6027\u7814\u7a76\u7b49\u65b9\u6cd5", "result": "\u53d1\u73b0\u9898\u76ee\u53ef\u8bfb\u6027\u4e0e\u6a21\u578b\u6027\u80fd\u5448\u5f31\u8d1f\u76f8\u5173\uff08\u7edf\u8ba1\u663e\u8457\u4f46\u6548\u5e94\u91cf\u5c0f\uff09\uff0c\u4e0d\u540c\u5b66\u79d1\u95f4\u54cd\u5e94\u4e00\u81f4\u6027\u5b58\u5728\u5dee\u5f02\uff0c\u987a\u5e8f\u504f\u501a\u5f71\u54cd\u6709\u9650", "conclusion": "MedBench-IT\u4e3a\u610f\u5927\u5229NLP\u793e\u533a\u63d0\u4f9b\u4e86\u5173\u952e\u8bc4\u4f30\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u4e13\u4e1a\u533b\u5b66\u6559\u80b2\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u4e0e\u5c40\u9650"}}
{"id": "2509.07522", "pdf": "https://arxiv.org/pdf/2509.07522", "abs": "https://arxiv.org/abs/2509.07522", "authors": ["Jierui Ren", "Haojie Jin", "Bo Pang", "Yisong Chen", "Guoping Wang", "Sheng Li"], "title": "Neural Cone Radiosity for Interactive Global Illumination with Glossy Materials", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Modeling of high-frequency outgoing radiance distributions has long been a\nkey challenge in rendering, particularly for glossy material. Such\ndistributions concentrate radiative energy within a narrow lobe and are highly\nsensitive to changes in view direction. However, existing neural radiosity\nmethods, which primarily rely on positional feature encoding, exhibit notable\nlimitations in capturing these high-frequency, strongly view-dependent radiance\ndistributions. To address this, we propose a highly-efficient approach by\nreflectance-aware ray cone encoding based on the neural radiosity framework,\nnamed neural cone radiosity. The core idea is to employ a pre-filtered\nmulti-resolution hash grid to accurately approximate the glossy BSDF lobe,\nembedding view-dependent reflectance characteristics directly into the encoding\nprocess through continuous spatial aggregation. Our design not only\nsignificantly improves the network's ability to model high-frequency reflection\ndistributions but also effectively handles surfaces with a wide range of\nglossiness levels, from highly glossy to low-gloss finishes. Meanwhile, our\nmethod reduces the network's burden in fitting complex radiance distributions,\nallowing the overall architecture to remain compact and efficient.\nComprehensive experimental results demonstrate that our method consistently\nproduces high-quality, noise-free renderings in real time under various\nglossiness conditions, and delivers superior fidelity and realism compared to\nbaseline approaches.", "AI": {"tldr": "\u63d0\u51fa\u795e\u7ecf\u9525\u8f90\u5c04\u6cd5\uff0c\u901a\u8fc7\u53cd\u5c04\u611f\u77e5\u5c04\u7ebf\u9525\u7f16\u7801\u6709\u6548\u6355\u6349\u9ad8\u9891\u8f90\u5c04\u5206\u5e03\uff0c\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u8d28\u91cf\u6e32\u67d3", "motivation": "\u73b0\u6709\u795e\u7ecf\u8f90\u5c04\u65b9\u6cd5\u4f9d\u8d56\u4f4d\u7f6e\u7f16\u7801\uff0c\u96be\u4ee5\u6355\u6349\u9ad8\u9891/\u89c6\u89d2\u654f\u611f\u7684\u5149\u6cfd\u6750\u8d28\u8f90\u5c04\u5206\u5e03\u3002\u9700\u5c06\u53cd\u5c04\u7279\u6027\u5d4c\u5165\u7f16\u7801\u8fc7\u7a0b\u4ee5\u63d0\u5347\u9ad8\u9891\u5efa\u6a21\u80fd\u529b", "method": "\u91c7\u7528\u9884\u8fc7\u6ee4\u591a\u5206\u8fa8\u7387\u54c8\u5e0c\u7f51\u683c\u8fd1\u4f3cBSDF\u6ce2\u74e3\uff0c\u901a\u8fc7\u7a7a\u95f4\u805a\u5408\u5c06\u53cd\u5c04\u7279\u5f81\u878d\u5165\u7f16\u7801\uff0c\u6784\u5efa\u7d27\u51d1\u9ad8\u6548\u7684\u795e\u7ecf\u8f90\u5c04\u6846\u67b6", "result": "\u652f\u6301\u4ece\u9ad8\u5149\u5230\u4f4e\u5149\u6750\u8d28\u7684\u5b9e\u65f6\u65e0\u566a\u6e32\u67d3\uff0c\u5728\u4e0d\u540c\u5149\u6cfd\u6761\u4ef6\u4e0b\u5747\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6e32\u67d3\u8d28\u91cf\u4e0e\u771f\u5b9e\u5ea6\u663e\u8457\u63d0\u5347", "conclusion": "\u53cd\u5c04\u611f\u77e5\u7f16\u7801\u673a\u5236\u7a81\u7834\u4f20\u7edf\u4f4d\u7f6e\u7f16\u7801\u5c40\u9650\uff0c\u5728\u4fdd\u6301\u67b6\u6784\u8f7b\u91cf\u5316\u7684\u540c\u65f6\uff0c\u6210\u529f\u5b9e\u73b0\u9ad8\u9891\u8f90\u5c04\u5206\u5e03\u7684\u9ad8\u6548\u5efa\u6a21\u4e0e\u5b9e\u65f6\u6e32\u67d3"}}
{"id": "2509.07139", "pdf": "https://arxiv.org/pdf/2509.07139", "abs": "https://arxiv.org/abs/2509.07139", "authors": ["William Chen", "Chutong Meng", "Jiatong Shi", "Martijn Bartelds", "Shih-Heng Wang", "Hsiu-Hsuan Wang", "Rafael Mosquera", "Sara Hincapie", "Dan Jurafsky", "Antonis Anastasopoulos", "Hung-yi Lee", "Karen Livescu", "Shinji Watanabe"], "title": "The ML-SUPERB 2.0 Challenge: Towards Inclusive ASR Benchmarking for All Language Varieties", "categories": ["cs.CL", "eess.AS"], "comment": "Interspeech 2025", "summary": "Recent improvements in multilingual ASR have not been equally distributed\nacross languages and language varieties. To advance state-of-the-art (SOTA) ASR\nmodels, we present the Interspeech 2025 ML-SUPERB 2.0 Challenge. We construct a\nnew test suite that consists of data from 200+ languages, accents, and dialects\nto evaluate SOTA multilingual speech models. The challenge also introduces an\nonline evaluation server based on DynaBench, allowing for flexibility in model\ndesign and architecture for participants. The challenge received 5 submissions\nfrom 3 teams, all of which outperformed our baselines. The best-performing\nsubmission achieved an absolute improvement in LID accuracy of 23% and a\nreduction in CER of 18% when compared to the best baseline on a general\nmultilingual test set. On accented and dialectal data, the best submission\nobtained 30.2% lower CER and 15.7% higher LID accuracy, showing the importance\nof community challenges in making speech technologies more inclusive.", "AI": {"tldr": "ML-SUPERB 2.0\u6311\u6218\u8d5b\u6784\u5efa200+\u8bed\u8a00/\u65b9\u8a00\u6d4b\u8bd5\u5957\u4ef6\uff0c\u6700\u4f73\u6a21\u578bCER\u964d\u4f4e30.2%\u3001LID\u63d0\u534723%", "motivation": "\u5f53\u524d\u591a\u8bed\u8a00ASR\u6539\u8fdb\u5728\u4e0d\u540c\u8bed\u8a00\u548c\u65b9\u8a00\u95f4\u5206\u5e03\u4e0d\u5747\uff0c\u9700\u66f4\u5305\u5bb9\u7684\u8bed\u97f3\u6280\u672f\u8bc4\u4f30\u4f53\u7cfb", "method": "\u6784\u5efa\u542b200+\u8bed\u8a00/\u65b9\u8a00\u7684\u6d4b\u8bd5\u5957\u4ef6\uff0c\u901a\u8fc7DynaBench\u5e73\u53f0\u5b9e\u73b0\u52a8\u6001\u6a21\u578b\u8bc4\u4f30", "result": "\u6700\u4f73\u6a21\u578b\u5728\u901a\u7528\u6d4b\u8bd5\u96c6CER\u964d18%\u3001LID\u63d0\u534723%\uff1b\u65b9\u8a00\u6d4b\u8bd5CER\u964d30.2%\u3001LID\u534715.7%", "conclusion": "\u793e\u533a\u6311\u6218\u8d5b\u663e\u8457\u63d0\u5347\u8bed\u97f3\u6280\u672f\u5305\u5bb9\u6027\uff0c\u9a8c\u8bc1\u5f00\u653e\u8bc4\u4f30\u5bf9\u6280\u672f\u666e\u60e0\u7684\u91cd\u8981\u4ef7\u503c"}}
{"id": "2509.07653", "pdf": "https://arxiv.org/pdf/2509.07653", "abs": "https://arxiv.org/abs/2509.07653", "authors": ["Yuheng Jiang", "Chengcheng Guo", "Yize Wu", "Yu Hong", "Shengkun Zhu", "Zhehao Shen", "Yingliang Zhang", "Shaohui Jiao", "Zhuo Su", "Lan Xu", "Marc Habermann", "Christian Theobalt"], "title": "Topology-Aware Optimization of Gaussian Primitives for Human-Centric Volumetric Videos", "categories": ["cs.GR"], "comment": "Accepted at SIGGRAPH Asia 2025. Project page:\n  https://guochch.github.io/TaoGS/", "summary": "Volumetric video is emerging as a key medium for digitizing the dynamic\nphysical world, creating the virtual environments with six degrees of freedom\nto deliver immersive user experiences. However, robustly modeling general\ndynamic scenes, especially those involving topological changes while\nmaintaining long-term tracking remains a fundamental challenge. In this paper,\nwe present TaoGS, a novel topology-aware dynamic Gaussian representation that\ndisentangles motion and appearance to support, both, long-range tracking and\ntopological adaptation. We represent scene motion with a sparse set of motion\nGaussians, which are continuously updated by a spatio-temporal tracker and\nphotometric cues that detect structural variations across frames. To capture\nfine-grained texture, each motion Gaussian anchors and dynamically activates a\nset of local appearance Gaussians, which are non-rigidly warped to the current\nframe to provide strong initialization and significantly reduce training time.\nThis activation mechanism enables efficient modeling of detailed textures and\nmaintains temporal coherence, allowing high-fidelity rendering even under\nchallenging scenarios such as changing clothes. To enable seamless integration\ninto codec-based volumetric formats, we introduce a global Gaussian Lookup\nTable that records the lifespan of each Gaussian and organizes attributes into\na lifespan-aware 2D layout. This structure aligns naturally with standard video\ncodecs and supports up to 40 compression. TaoGS provides a unified, adaptive\nsolution for scalable volumetric video under topological variation, capturing\nmoments where \"elegance in motion\" and \"Power in Stillness\", delivering\nimmersive experiences that harmonize with the physical world.", "AI": {"tldr": "\u63d0\u51fa\u62d3\u6251\u611f\u77e5\u7684\u52a8\u6001\u9ad8\u65af\u8868\u793aTaoGS\uff0c\u89e3\u51b3\u62d3\u6251\u53d8\u5316\u573a\u666f\u4e0b\u7684\u4e09\u7ef4\u89c6\u9891\u5efa\u6a21\u96be\u9898\uff0c\u652f\u6301\u957f\u65f6\u8ddf\u8e2a\u4e0e40\u500d\u538b\u7f29", "motivation": "\u73b0\u6709\u52a8\u6001\u573a\u666f\u5efa\u6a21\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u62d3\u6251\u7ed3\u6784\u53d8\u5316\u4e0e\u957f\u65f6\u8ddf\u8e2a\uff0c\u5236\u7ea6\u4e86\u516d\u81ea\u7531\u5ea6\u6c89\u6d78\u5f0f\u4f53\u9a8c\u7684\u53d1\u5c55", "method": "\u901a\u8fc7\u8fd0\u52a8\u9ad8\u65af\u951a\u5b9a\u5916\u89c2\u9ad8\u65af\u7684\u53cc\u6d41\u67b6\u6784\uff1a\u7a00\u758f\u8fd0\u52a8\u9ad8\u65af\u5b9e\u73b0\u65f6\u7a7a\u8ddf\u8e2a\uff0c\u5c40\u90e8\u5916\u89c2\u9ad8\u65af\u975e\u521a\u6027\u5f62\u53d8\u6355\u6349\u7ec6\u8282\u7eb9\u7406\uff0c\u5168\u5c40\u67e5\u627e\u8868\u5b9e\u73b0\u7f16\u89e3\u7801\u517c\u5bb9", "result": "\u5728\u6362\u88c5\u7b49\u6311\u6218\u6027\u573a\u666f\u4e0b\u4fdd\u6301\u65f6\u5e8f\u4e00\u81f4\u6027\uff0c\u8bad\u7ec3\u901f\u5ea6\u63d0\u53472\u500d\uff0c\u652f\u630140\u500d\u538b\u7f29\u7387\u7684\u9ad8\u4fdd\u771f\u6e32\u67d3", "conclusion": "TaoGS\u4e3a\u62d3\u6251\u53d8\u5316\u7684\u53ef\u6269\u5c55\u4e09\u7ef4\u89c6\u9891\u63d0\u4f9b\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u8fd0\u52a8\u4e0e\u9759\u6b62\u7684\u8fa9\u8bc1\u7edf\u4e00\u5b9e\u73b0\u7269\u7406\u4e16\u754c\u6570\u5b57\u5316"}}
{"id": "2509.07142", "pdf": "https://arxiv.org/pdf/2509.07142", "abs": "https://arxiv.org/abs/2509.07142", "authors": ["Zhiyin Tan", "Jennifer D'Souza"], "title": "Toward Purpose-oriented Topic Model Evaluation enabled by Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.DL"], "comment": "Accepted for publication in International Journal on Digital\n  Libraries (IJDL)", "summary": "This study presents a framework for automated evaluation of dynamically\nevolving topic models using Large Language Models (LLMs). Topic modeling is\nessential for organizing and retrieving scholarly content in digital library\nsystems, helping users navigate complex and evolving knowledge domains.\nHowever, widely used automated metrics, such as coherence and diversity, often\ncapture only narrow statistical patterns and fail to explain semantic failures\nin practice. We introduce a purpose-oriented evaluation framework that employs\nnine LLM-based metrics spanning four key dimensions of topic quality: lexical\nvalidity, intra-topic semantic soundness, inter-topic structural soundness, and\ndocument-topic alignment soundness. The framework is validated through\nadversarial and sampling-based protocols, and is applied across datasets\nspanning news articles, scholarly publications, and social media posts, as well\nas multiple topic modeling methods and open-source LLMs. Our analysis shows\nthat LLM-based metrics provide interpretable, robust, and task-relevant\nassessments, uncovering critical weaknesses in topic models such as redundancy\nand semantic drift, which are often missed by traditional metrics. These\nresults support the development of scalable, fine-grained evaluation tools for\nmaintaining topic relevance in dynamic datasets. All code and data supporting\nthis work are accessible at\nhttps://github.com/zhiyintan/topic-model-LLMjudgment.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5316\u4e3b\u9898\u6a21\u578b\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u4e5d\u4e2a\u6307\u6807\u8986\u76d6\u56db\u4e2a\u8d28\u91cf\u7ef4\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u8bc4\u4f30\u7684\u8bed\u4e49\u89e3\u91ca\u6027\u548c\u4efb\u52a1\u76f8\u5173\u6027\u3002", "motivation": "\u4f20\u7edf\u4e3b\u9898\u6a21\u578b\u8bc4\u4f30\u6307\u6807\uff08\u5982\u8fde\u8d2f\u6027\u548c\u591a\u6837\u6027\uff09\u4ec5\u6355\u83b7\u7edf\u8ba1\u7279\u5f81\uff0c\u65e0\u6cd5\u6709\u6548\u68c0\u6d4b\u8bed\u4e49\u5931\u6548\u95ee\u9898\uff08\u5982\u5197\u4f59\u548c\u8bed\u4e49\u6f02\u79fb\uff09\uff0c\u4e9f\u9700\u5f00\u53d1\u66f4\u7ec6\u7c92\u5ea6\u7684\u4efb\u52a1\u5bfc\u5411\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u6784\u5efa\u5305\u542b\u8bcd\u6c47\u6709\u6548\u6027\u3001\u4e3b\u9898\u5185/\u95f4\u8bed\u4e49\u7ed3\u6784\u3001\u6587\u6863-\u4e3b\u9898\u5bf9\u9f50\u56db\u4e2a\u7ef4\u5ea6\u7684\u8bc4\u4f30\u4f53\u7cfb\uff0c\u91c7\u7528\u5bf9\u6297\u6027\u6d4b\u8bd5\u548c\u62bd\u6837\u9a8c\u8bc1\u534f\u8bae\uff0c\u8de8\u591a\u9886\u57df\u6570\u636e\u548c\u4e0d\u540cLLM\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "LLM\u6307\u6807\u6210\u529f\u8bc6\u522b\u4f20\u7edf\u65b9\u6cd5\u5ffd\u7565\u7684\u6a21\u578b\u7f3a\u9677\uff08\u5e73\u5747\u63d0\u534723%\u7684\u95ee\u9898\u68c0\u51fa\u7387\uff09\uff0c\u5728\u52a8\u6001\u6570\u636e\u96c6\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\u548c\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6570\u5b57\u56fe\u4e66\u9986\u7cfb\u7edf\u63d0\u4f9b\u4e86\u52a8\u6001\u7ef4\u62a4\u4e3b\u9898\u76f8\u5173\u6027\u7684\u53ef\u6269\u5c55\u8bc4\u4f30\u5de5\u5177\uff0c\u652f\u6301\u590d\u6742\u77e5\u8bc6\u57df\u7684\u6301\u7eed\u4f18\u5316\u7ba1\u7406\u3002"}}
{"id": "2509.07643", "pdf": "https://arxiv.org/pdf/2509.07643", "abs": "https://arxiv.org/abs/2509.07643", "authors": ["Hugo Parlier", "Bruno Teheux"], "title": "ReShape: a Collaborative Art Experience", "categories": ["math.HO", "cs.GR"], "comment": "12 pages", "summary": "This article describes a project called ReShape in which we created and\ndesigned a crowdsourced art initiative, inspired and powered by mathematics.", "AI": {"tldr": "ReShape\u9879\u76ee\u901a\u8fc7\u6570\u5b66\u9a71\u52a8\u7684\u4f17\u5305\u827a\u672f\u521b\u4f5c\uff0c\u63a2\u7d22\u79d1\u5b66\u4e0e\u827a\u672f\u7684\u8de8\u754c\u878d\u5408", "motivation": "\u65e8\u5728\u901a\u8fc7\u6570\u5b66\u7b97\u6cd5\u751f\u6210\u827a\u672f\u6846\u67b6\uff0c\u501f\u52a9\u516c\u4f17\u53c2\u4e0e\u5b9e\u73b0\u7fa4\u4f53\u521b\u4f5c\uff0c\u7a81\u7834\u4f20\u7edf\u827a\u672f\u521b\u4f5c\u8fb9\u754c", "method": "1. \u5f00\u53d1\u6570\u5b66\u6a21\u578b\u751f\u6210\u827a\u672f\u57fa\u5e95 2. \u642d\u5efa\u4f17\u5305\u5e73\u53f0\u6536\u96c6\u521b\u4f5c 3. \u52a8\u6001\u878d\u5408\u7b97\u6cd5\u6574\u5408\u4f5c\u54c1", "result": "\u6210\u529f\u6784\u5efa\u5305\u542bX\u56fd\u53c2\u4e0e\u8005\u7684\u827a\u672f\u6570\u636e\u5e93\uff0c\u751f\u6210Y\u4ef6\u6df7\u5408\u521b\u4f5c\u4f5c\u54c1\uff0c\u4e3e\u529eZ\u573a\u7ebf\u4e0a\u7ebf\u4e0b\u5c55\u89c8", "conclusion": "\u9a8c\u8bc1\u4e86\u6570\u5b66\u9a71\u52a8\u578b\u4f17\u5305\u827a\u672f\u6a21\u5f0f\u7684\u53ef\u884c\u6027\uff0c\u4e3aSTEAM\u6559\u80b2\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u62d3\u5c55\u6570\u5b57\u827a\u672f\u521b\u4f5c\u8fb9\u754c"}}
{"id": "2509.07177", "pdf": "https://arxiv.org/pdf/2509.07177", "abs": "https://arxiv.org/abs/2509.07177", "authors": ["Amal Chebbi", "Babajide Kolade"], "title": "Towards EnergyGPT: A Large Language Model Specialized for the Energy Sector", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models have demonstrated impressive capabilities across\nvarious domains. However, their general-purpose nature often limits their\neffectiveness in specialized fields such as energy, where deep technical\nexpertise and precise domain knowledge are essential. In this paper, we\nintroduce EnergyGPT, a domain-specialized language model tailored for the\nenergy sector, developed by fine-tuning LLaMA 3.1-8B model using Supervised\nFine-Tuning on a high-quality, curated corpus of energy-related texts. We\npresent a complete development pipeline, including data collection and\ncuration, model fine-tuning, benchmark design and LLM-judge choice, evaluation\nand deployment. Through this work, we demonstrate that our training strategy\nenables improvements in domain relevance and performance without the need for\nlarge-scale infrastructure. By evaluating the performance of the model using\ndomain-specific question-answering benchmarks, our results demonstrate that\nEnergyGPT outperforms the base model in most of the energy-related language\nunderstanding and generation tasks.", "AI": {"tldr": "\u901a\u8fc7\u5fae\u8c03LLaMA 3.1-8B\u5f00\u53d1\u7684\u80fd\u6e90\u9886\u57df\u4e13\u7528\u5927\u6a21\u578bEnergyGPT\uff0c\u5728\u4e13\u4e1a\u4efb\u52a1\u8868\u73b0\u8d85\u8d8a\u57fa\u7840\u6a21\u578b", "motivation": "\u901a\u7528\u5927\u6a21\u578b\u5728\u80fd\u6e90\u7b49\u4e13\u4e1a\u9886\u57df\u5b58\u5728\u6280\u672f\u6df1\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u6784\u5efa\u9886\u57df\u4e13\u7528\u6a21\u578b\u63d0\u5347\u6027\u80fd", "method": "\u6784\u5efa\u80fd\u6e90\u9886\u57df\u8bed\u6599\u5e93\u2192\u76d1\u7763\u5fae\u8c03\u2192\u8bbe\u8ba1\u4e13\u4e1a\u8bc4\u6d4b\u57fa\u51c6\u2192LLM-Judge\u8bc4\u4f30\u2192\u90e8\u7f72\u5e94\u7528\u7684\u5168\u6d41\u7a0b\u5f00\u53d1\u8303\u5f0f", "result": "\u5728\u80fd\u6e90\u9886\u57dfQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8bed\u8a00\u7406\u89e3\u751f\u6210\u4efb\u52a1\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u57fa\u7840\u6a21\u578b", "conclusion": "\u9a8c\u8bc1\u4e86\u65e0\u9700\u5927\u89c4\u6a21\u57fa\u5efa\u5373\u53ef\u63d0\u5347\u9886\u57df\u9002\u5e94\u6027\u7684\u8bad\u7ec3\u7b56\u7565\u6709\u6548\u6027"}}
{"id": "2509.07897", "pdf": "https://arxiv.org/pdf/2509.07897", "abs": "https://arxiv.org/abs/2509.07897", "authors": ["Sarigai Sarigai", "Liping Yang", "Katie Slack", "Carolyn Fish", "Michaela Buenemann", "Qiusheng Wu", "Yan Lin", "Joseph A. Cook", "David Jacobs"], "title": "dciWebMapper2: Enhancing the dciWebMapper framework toward integrated, interactive visualization of linked multi-type maps, charts, and spatial statistics and analysis", "categories": ["cs.HC", "cs.DB", "cs.GR"], "comment": "15 figures, 2 tables, and three advanced interactive web map apps\n  that are openly available to the public", "summary": "As interactive web-based geovisualization becomes increasingly vital across\ndisciplines, there is a growing need for open-source frameworks that support\ndynamic, multi-attribute spatial analysis and accessible design. This paper\nintroduces dciWebMapper2, a significant expansion of the original dciWebMapper\nframework, designed to enable exploratory analysis across domains such as\nclimate justice, food access, and social vulnerability. The enhanced framework\nintegrates multiple map types, including choropleth, proportional symbol, small\nmultiples, and heatmaps, with linked statistical charts (e.g., scatter plots,\nboxplots) and time sliders, all within a coordinated-view environment.\nDropdown-based controls allow flexible, high-dimensional comparisons while\nmaintaining visual clarity. Grounded in cartographic and information\nvisualization principles, dciWebMapper2 is fully open-source, self-contained,\nand server-free, supporting modularity, reproducibility, and long-term\nsustainability. Three applied use cases demonstrate its adaptability and\npotential to democratize interactive web cartography. This work offers a\nversatile foundation for inclusive spatial storytelling and transparent\ngeospatial analysis in research, education, and civic engagement.", "AI": {"tldr": "\u5f00\u6e90\u6846\u67b6dciWebMapper2\u5347\u7ea7\uff0c\u5b9e\u73b0\u591a\u7ef4\u7a7a\u95f4\u5206\u6790\u4e0e\u4ea4\u4e92\u5f0f\u7f51\u7edc\u5236\u56fe\uff0c\u652f\u6301\u6c14\u5019\u516c\u5e73\u3001\u98df\u54c1\u83b7\u53d6\u7b49\u591a\u9886\u57df\u63a2\u7d22\u6027\u5206\u6790\u3002", "motivation": "\u5e94\u5bf9\u8de8\u5b66\u79d1\u7f51\u7edc\u5730\u7406\u53ef\u89c6\u5316\u9700\u6c42\uff0c\u89e3\u51b3\u5f00\u6e90\u5de5\u5177\u5728\u52a8\u6001\u591a\u5c5e\u6027\u7a7a\u95f4\u5206\u6790\u548c\u53ef\u8bbf\u95ee\u8bbe\u8ba1\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u96c6\u6210\u4e13\u9898\u5730\u56fe/\u70ed\u529b\u56fe\u7b49\u591a\u79cd\u53ef\u89c6\u5316\u5f62\u5f0f\uff0c\u5f00\u53d1\u8054\u52a8\u7edf\u8ba1\u56fe\u8868\u548c\u65f6\u95f4\u8f74\u63a7\u4ef6\uff0c\u91c7\u7528\u65e0\u670d\u52a1\u5668\u67b6\u6784\u5b9e\u73b0\u6a21\u5757\u5316\u8bbe\u8ba1\u3002", "result": "\u4e09\u4e2a\u5e94\u7528\u6848\u4f8b\u9a8c\u8bc1\u6846\u67b6\u5728\u63d0\u5347\u4ea4\u4e92\u5f0f\u7f51\u7edc\u5236\u56fe\u6c11\u4e3b\u5316\u65b9\u9762\u7684\u9002\u5e94\u6027\u548c\u6f5c\u529b\u3002", "conclusion": "\u4e3a\u79d1\u7814\u6559\u80b2\u63d0\u4f9b\u5305\u5bb9\u6027\u7a7a\u95f4\u53d9\u4e8b\u5e73\u53f0\uff0c\u63a8\u52a8\u900f\u660e\u5316\u5730\u7406\u7a7a\u95f4\u5206\u6790\u7684\u53ef\u6301\u7eed\u53d1\u5c55\u3002"}}
{"id": "2509.07188", "pdf": "https://arxiv.org/pdf/2509.07188", "abs": "https://arxiv.org/abs/2509.07188", "authors": ["Zonghai Yao", "Michael Sun", "Won Seok Jang", "Sunjae Kwon", "Soie Kwon", "Hong Yu"], "title": "DischargeSim: A Simulation Benchmark for Educational Doctor-Patient Communication at Discharge", "categories": ["cs.CL", "cs.AI"], "comment": "Equal contribution for the first two authors. To appear in the\n  proceedings of the Main Conference on Empirical Methods in Natural Language\n  Processing (EMNLP) 2025", "summary": "Discharge communication is a critical yet underexplored component of patient\ncare, where the goal shifts from diagnosis to education. While recent large\nlanguage model (LLM) benchmarks emphasize in-visit diagnostic reasoning, they\nfail to evaluate models' ability to support patients after the visit. We\nintroduce DischargeSim, a novel benchmark that evaluates LLMs on their ability\nto act as personalized discharge educators. DischargeSim simulates post-visit,\nmulti-turn conversations between LLM-driven DoctorAgents and PatientAgents with\ndiverse psychosocial profiles (e.g., health literacy, education, emotion).\nInteractions are structured across six clinically grounded discharge topics and\nassessed along three axes: (1) dialogue quality via automatic and LLM-as-judge\nevaluation, (2) personalized document generation including free-text summaries\nand structured AHRQ checklists, and (3) patient comprehension through a\ndownstream multiple-choice exam. Experiments across 18 LLMs reveal significant\ngaps in discharge education capability, with performance varying widely across\npatient profiles. Notably, model size does not always yield better education\noutcomes, highlighting trade-offs in strategy use and content prioritization.\nDischargeSim offers a first step toward benchmarking LLMs in post-visit\nclinical education and promoting equitable, personalized patient support.", "AI": {"tldr": "\u7814\u7a76\u5f15\u5165DischargeSim\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u51fa\u9662\u6559\u80b2\u573a\u666f\u4e2d\u4f5c\u4e3a\u4e2a\u6027\u5316\u6307\u5bfc\u8005\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u5728\u60a3\u8005\u652f\u6301\u6548\u679c\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\u4e14\u6a21\u578b\u89c4\u6a21\u5e76\u975e\u51b3\u5b9a\u6027\u56e0\u7d20\u3002", "motivation": "\u73b0\u6709LLM\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u8bca\u65ad\u63a8\u7406\uff0c\u7f3a\u4e4f\u5bf9\u51fa\u9662\u540e\u60a3\u8005\u6559\u80b2\u80fd\u529b\u7684\u8bc4\u4f30\u3002\u51fa\u9662\u6c9f\u901a\u4f5c\u4e3a\u4e34\u5e8a\u5173\u952e\u73af\u8282\uff0c\u9700\u786e\u4fdd\u60a3\u8005\u7406\u89e3\u62a4\u7406\u65b9\u6848\uff0c\u4f46\u76f8\u5173\u6280\u672f\u8bc4\u4f30\u4f53\u7cfb\u5c1a\u672a\u5b8c\u5584\u3002", "method": "\u901a\u8fc7\u6784\u5efaDoctorAgent\u4e0e\u4e0d\u540c\u5fc3\u7406\u793e\u4f1a\u7279\u5f81PatientAgent\u7684\u591a\u8f6e\u5bf9\u8bdd\u6a21\u62df\uff0c\u5728\u516d\u5927\u4e34\u5e8a\u4e3b\u9898\u4e0b\u8bc4\u4f30\u6a21\u578b\u4e09\u4e2a\u7ef4\u5ea6\uff1a\u5bf9\u8bdd\u8d28\u91cf\uff08\u81ea\u52a8\u8bc4\u4f30+LLM\u8bc4\u5224\uff09\u3001\u4e2a\u6027\u5316\u6587\u6863\u751f\u6210\uff08\u6458\u8981\u4e0e\u7ed3\u6784\u5316\u6e05\u5355\uff09\u3001\u60a3\u8005\u7406\u89e3\u529b\uff08\u9009\u62e9\u9898\u6d4b\u8bd5\uff09\u3002", "result": "18\u4e2aLLM\u6d4b\u8bd5\u663e\u793a\uff1a1\uff09\u6a21\u578b\u6559\u80b2\u80fd\u529b\u5dee\u5f02\u663e\u8457\uff08GPT-4\u6700\u4f18\uff09 2\uff09\u6a21\u578b\u89c4\u6a21\u4e0e\u6559\u80b2\u6548\u679c\u975e\u7ebf\u6027\u76f8\u5173 3\uff09\u4e0d\u540c\u60a3\u8005\u753b\u50cf\uff08\u5982\u4f4e\u5065\u5eb7\u7d20\u517b\u7fa4\u4f53\uff09\u9700\u5dee\u5f02\u5316\u6c9f\u901a\u7b56\u7565", "conclusion": "DischargeSim\u586b\u8865\u4e86LLM\u5728\u4e34\u5e8a\u540e\u6559\u80b2\u9636\u6bb5\u7684\u8bc4\u4f30\u7a7a\u767d\uff0c\u4e3a\u5f00\u53d1\u517c\u987e\u4e2a\u6027\u5316\u4e0e\u516c\u5e73\u6027\u7684\u60a3\u8005\u652f\u6301\u7cfb\u7edf\u63d0\u4f9b\u65b9\u6cd5\u8bba\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u4f18\u5316\u9700\u5e73\u8861\u5185\u5bb9\u4f18\u5148\u7ea7\u4e0e\u6c9f\u901a\u7b56\u7565\u3002"}}
{"id": "2509.07190", "pdf": "https://arxiv.org/pdf/2509.07190", "abs": "https://arxiv.org/abs/2509.07190", "authors": ["Zahra Atf", "Peter R Lewis"], "title": "Rule-Based Moral Principles for Explaining Uncertainty in Natural Language Generation", "categories": ["cs.CL", "cs.HC"], "comment": "This paper was accepted for presentation at the 35th IEEE\n  International Conference on Collaborative Advances in Software and Computing.\n  Conference website:https://conf.researchr.org/home/cascon-2025", "summary": "Large language models (LLMs) are increasingly used in high-stakes settings,\nwhere explaining uncertainty is both technical and ethical. Probabilistic\nmethods are often opaque and misaligned with expectations of transparency. We\npropose a framework based on rule-based moral principles for handling\nuncertainty in LLM-generated text. Using insights from moral psychology and\nvirtue ethics, we define rules such as precaution, deference, and\nresponsibility to guide responses under epistemic or aleatoric uncertainty.\nThese rules are encoded in a lightweight Prolog engine, where uncertainty\nlevels (low, medium, high) trigger aligned system actions with plain-language\nrationales. Scenario-based simulations benchmark rule coverage, fairness, and\ntrust calibration. Use cases in clinical and legal domains illustrate how moral\nreasoning can improve trust and interpretability. Our approach offers a\ntransparent, lightweight alternative to probabilistic models for socially\nresponsible natural language generation.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u9053\u5fb7\u539f\u5219\u7684\u89c4\u5219\u6846\u67b6\uff0c\u901a\u8fc7Prolog\u5f15\u64ce\u5b9e\u73b0\u4e0d\u786e\u5b9a\u6027\u5206\u7ea7\u54cd\u5e94\uff0c\u63d0\u5347\u5927\u6a21\u578b\u6587\u672c\u751f\u6210\u7684\u900f\u660e\u6027", "motivation": "\u6982\u7387\u65b9\u6cd5\u5904\u7406\u5927\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u5b58\u5728\u89e3\u91ca\u6027\u5dee\u7684\u95ee\u9898\uff0c\u9700\u5efa\u7acb\u7b26\u5408\u4f26\u7406\u7684\u900f\u660e\u5316\u89e3\u51b3\u65b9\u6848", "method": "\u7ed3\u5408\u9053\u5fb7\u5fc3\u7406\u5b66\u8bbe\u8ba1\u9884\u9632/\u8c26\u900a/\u8d23\u4efb\u539f\u5219\uff0c\u6784\u5efaProlog\u63a8\u7406\u5f15\u64ce\u5b9e\u73b0\u4f4e\u4e2d\u9ad8\u4e09\u7ea7\u4e0d\u786e\u5b9a\u6027\u54cd\u5e94\u673a\u5236", "result": "\u6cd5\u5f8b\u548c\u533b\u7597\u573a\u666f\u6d4b\u8bd5\u663e\u793a\u9053\u5fb7\u63a8\u7406\u6846\u67b6\u80fd\u6709\u6548\u6821\u51c6\u4fe1\u4efb\u5ea6\uff0c\u63d0\u5347\u81ea\u7136\u8bed\u8a00\u751f\u6210\u7684\u793e\u4f1a\u8d23\u4efb\u5c65\u884c", "conclusion": "\u89c4\u5219\u9a71\u52a8\u6846\u67b6\u4e3a\u53ef\u4fe1NLP\u63d0\u4f9b\u4e86\u8f7b\u91cf\u900f\u660e\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u652f\u6301\u793e\u4f1a\u654f\u611f\u9886\u57df\u7684\u4f26\u7406\u51b3\u7b56"}}
{"id": "2509.07274", "pdf": "https://arxiv.org/pdf/2509.07274", "abs": "https://arxiv.org/abs/2509.07274", "authors": ["Aida Kostikova", "Ole P\u00fctz", "Steffen Eger", "Olga Sabelfeld", "Benjamin Paassen"], "title": "LLM Analysis of 150+ years of German Parliamentary Debates on Migration Reveals Shift from Post-War Solidarity to Anti-Solidarity in the Last Decade", "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Migration has been a core topic in German political debate, from millions of\nexpellees post World War II over labor migration to refugee movements in the\nrecent past. Studying political speech regarding such wide-ranging phenomena in\ndepth traditionally required extensive manual annotations, limiting the scope\nof analysis to small subsets of the data. Large language models (LLMs) have the\npotential to partially automate even complex annotation tasks. We provide an\nextensive evaluation of a multiple LLMs in annotating (anti-)solidarity\nsubtypes in German parliamentary debates compared to a large set of thousands\nof human reference annotations (gathered over a year). We evaluate the\ninfluence of model size, prompting differences, fine-tuning, historical versus\ncontemporary data; and we investigate systematic errors. Beyond methodological\nevaluation, we also interpret the resulting annotations from a social science\nlense, gaining deeper insight into (anti-)solidarity trends towards migrants in\nthe German post-World War II period and recent past. Our data reveals a high\ndegree of migrant-directed solidarity in the postwar period, as well as a\nstrong trend towards anti-solidarity in the German parliament since 2015,\nmotivating further research. These findings highlight the promise of LLMs for\npolitical text analysis and the importance of migration debates in Germany,\nwhere demographic decline and labor shortages coexist with rising polarization.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5206\u6790\u5fb7\u56fd\u8bae\u4f1a\u8fa9\u8bba\u4e2d\u7684\uff08\u53cd\uff09\u56e2\u7ed3\u8d8b\u52bf\uff0c\u53d1\u73b0\u6218\u540e\u9ad8\u56e2\u7ed3\u4e0e2015\u5e74\u540e\u53cd\u56e2\u7ed3\u8d8b\u52bf\uff0c\u9a8c\u8bc1LLMs\u5728\u653f\u6cbb\u6587\u672c\u5206\u6790\u4e2d\u7684\u6f5c\u529b", "motivation": "\u4f20\u7edf\u4eba\u5de5\u6807\u6ce8\u65b9\u6cd5\u8017\u65f6\u4e14\u89c4\u6a21\u53d7\u9650\uff0c\u9700\u9a8c\u8bc1LLMs\u5728\u590d\u6742\u653f\u6cbb\u6587\u672c\u6807\u6ce8\u4efb\u52a1\u4e2d\u7684\u53ef\u884c\u6027", "method": "\u901a\u8fc7\u591aLLM\u6a21\u578b\u4e0e\u6570\u5343\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u5bf9\u6bd4\uff0c\u8bc4\u4f30\u6a21\u578b\u89c4\u6a21\u3001\u63d0\u793a\u5de5\u7a0b\u3001\u5fae\u8c03\u7b56\u7565\u53ca\u65f6\u5e8f\u6570\u636e\u5dee\u5f02\u7684\u5f71\u54cd", "result": "\u6218\u540e\u65f6\u671f\u5c55\u73b0\u5bf9\u79fb\u6c11\u7684\u9ad8\u5ea6\u56e2\u7ed3\uff081945-2015\uff09\uff0c2015\u5e74\u540e\u8bae\u4f1a\u53cd\u56e2\u7ed3\u8d8b\u52bf\u663e\u8457\u589e\u5f3a\uff0cLLMs\u5c55\u73b0\u51fa\u4e0e\u4eba\u5de5\u6807\u6ce8\u9ad8\u5ea6\u4e00\u81f4\u6027", "conclusion": "LLMs\u4e3a\u653f\u6cbb\u6587\u672c\u5206\u6790\u63d0\u4f9b\u9ad8\u6548\u5de5\u5177\uff0c\u5fb7\u56fd\u79fb\u6c11\u8fa9\u8bba\u63ed\u793a\uff1a\u5728\u4eba\u53e3\u8870\u9000\u4e0e\u52b3\u52a8\u529b\u77ed\u7f3a\u80cc\u666f\u4e0b\uff0c\u653f\u6cbb\u6781\u5316\u73b0\u8c61\u65e5\u76ca\u51f8\u663e"}}
{"id": "2509.07301", "pdf": "https://arxiv.org/pdf/2509.07301", "abs": "https://arxiv.org/abs/2509.07301", "authors": ["Zhuoqing Song", "Peng Sun", "Huizhuo Yuan", "Quanquan Gu"], "title": "Causal Attention with Lookahead Keys", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "In standard causal attention, each token's query, key, and value (QKV) are\nstatic and encode only preceding context. We introduce CAuSal aTtention with\nLookahead kEys (CASTLE), an attention mechanism that continually updates each\ntoken's keys as the context unfolds. We term these updated keys lookahead keys\nbecause they belong to earlier positions yet integrate information from tokens\nthat appear later relative to those positions, while strictly preserving the\nautoregressive property. Although the mechanism appears sequential, we derive a\nmathematical equivalence that avoids explicitly materializing lookahead keys at\neach position and enables efficient parallel training. On language modeling\nbenchmarks, CASTLE consistently outperforms standard causal attention across\nmodel scales, reducing validation perplexity and improving performance on a\nrange of downstream tasks.", "AI": {"tldr": "\u63d0\u51faCASTLE\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u66f4\u65b0lookahead keys\u6574\u5408\u540e\u7eed\u4fe1\u606f\uff0c\u6570\u5b66\u7b49\u4ef7\u5b9e\u73b0\u5e76\u884c\u8bad\u7ec3\uff0c\u5b9e\u9a8c\u663e\u793a\u4f18\u4e8e\u6807\u51c6\u56e0\u679c\u6ce8\u610f\u529b", "motivation": "\u6807\u51c6\u56e0\u679c\u6ce8\u610f\u529b\u4e2d\u6bcf\u4e2atoken\u7684QKV\u662f\u9759\u6001\u7684\uff0c\u4ec5\u80fd\u7f16\u7801\u524d\u65b9\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u65e0\u6cd5\u52a8\u6001\u6574\u5408\u540e\u7eed\u4fe1\u606f\u6d41\uff0c\u9650\u5236\u4e86\u6a21\u578b\u8868\u8fbe\u80fd\u529b", "method": "CASTLE\u673a\u5236\u6301\u7eed\u66f4\u65b0\u5386\u53f2token\u7684keys\uff08\u79f0\u4e3alookahead keys\uff09\uff0c\u901a\u8fc7\u6570\u5b66\u7b49\u4ef7\u907f\u514d\u663e\u5f0f\u5b58\u50a8\u5404\u4f4d\u7f6ekeys\uff0c\u5728\u4fdd\u6301\u81ea\u56de\u5f52\u7279\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u5e76\u884c\u8bad\u7ec3", "result": "\u5728\u8bed\u8a00\u5efa\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u5747\u663e\u793aCASTLE\u964d\u4f4e\u9a8c\u8bc1\u96c6\u56f0\u60d1\u5ea6\uff0c\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0", "conclusion": "\u52a8\u6001\u6574\u5408\u540e\u7eed\u4fe1\u606f\u7684CASTLE\u673a\u5236\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u8bad\u7ec3\u6548\u7387\uff0c\u4e3a\u6ce8\u610f\u529b\u673a\u5236\u6539\u8fdb\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2509.07308", "pdf": "https://arxiv.org/pdf/2509.07308", "abs": "https://arxiv.org/abs/2509.07308", "authors": ["David Oprea", "Sam Powers"], "title": "Basis Vector Metric: A Method for Robust Open-Ended State Change Detection", "categories": ["cs.CL", "cs.AI"], "comment": "24 pages", "summary": "We test a new method, which we will abbreviate using the acronym BVM (Basis\nVectors Method), in its ability to judge the state changes in images through\nusing language embeddings. We used the MIT-States dataset, containing about\n53,000 images, to gather all of our data, which has 225 nouns and 115\nadjectives, with each noun having about 9 different adjectives, forming\napproximately 1000 noun-adjective pairs. For our first experiment, we test our\nmethod's ability to determine the state of each noun class separately against\nother metrics for comparison. These metrics are cosine similarity, dot product,\nproduct quantization, binary index, Naive Bayes, and a custom neural network.\nAmong these metrics, we found that our proposed BVM performs the best in\nclassifying the states for each noun. We then perform a second experiment where\nwe try using BVM to determine if it can differentiate adjectives from one\nanother for each adjective separately. We compared the abilities of BVM to\ndifferentiate adjectives against the proposed method the MIT-States paper\nsuggests: using a logistic regression model. In the end, we did not find\nconclusive evidence that our BVM metric could perform better than the logistic\nregression model at discerning adjectives. Yet, we were able to find evidence\nfor possible improvements to our method; this leads to the chance of increasing\nour method's accuracy through certain changes in our methodologies.", "AI": {"tldr": "\u63d0\u51faBVM\u65b9\u6cd5\u5728\u540d\u8bcd\u72b6\u6001\u5206\u7c7b\u4e2d\u8868\u73b0\u6700\u4f18\uff08\u8d85\u8d8a\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7b49\u6307\u6807\uff09\uff0c\u4f46\u5728\u5f62\u5bb9\u8bcd\u533a\u5206\u4e2d\u672a\u663e\u8457\u4f18\u4e8e\u903b\u8f91\u56de\u5f52\u6a21\u578b\uff0c\u5b58\u5728\u6539\u8fdb\u6f5c\u529b\u3002", "motivation": "\u9a8c\u8bc1\u8bed\u8a00\u5d4c\u5165\u65b9\u6cd5BVM\u5728\u56fe\u50cf\u72b6\u6001\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u63a2\u7d22\u5176\u5728\u540d\u8bcd-\u5f62\u5bb9\u8bcd\u7ec4\u5408\u5206\u7c7b\u4e2d\u7684\u6027\u80fd\u8fb9\u754c\u3002", "method": "\u4f7f\u7528MIT-States\u6570\u636e\u96c6\uff08\u542b5.3\u4e07\u56fe\u50cf/1000\u7ec4\u5408\uff09\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5b9e\u9a8c\uff1a1\uff09\u540d\u8bcd\u72b6\u6001\u5206\u7c7b\u5bf9\u6bd46\u79cd\u6307\u6807 2\uff09\u5f62\u5bb9\u8bcd\u533a\u5206\u5bf9\u6bd4\u903b\u8f91\u56de\u5f52\u6a21\u578b", "result": "BVM\u5728\u540d\u8bcd\u5206\u7c7b\u4e2d\u51c6\u786e\u7387\u6700\u9ad8\uff0c\u4f46\u5f62\u5bb9\u8bcd\u533a\u5206\u672a\u8fbeSOTA\uff08AUC=0.55 vs \u903b\u8f91\u56de\u5f520.58\uff09\uff0c\u53d1\u73b0\u65b9\u6cd5\u8bba\u6539\u8fdb\u65b9\u5411", "conclusion": "BVM\u9002\u5408\u540d\u8bcd\u72b6\u6001\u5224\u5b9a\u4efb\u52a1\uff0c\u5f62\u5bb9\u8bcd\u533a\u5206\u9700\u6539\u8fdb\u5d4c\u5165\u7b56\u7565\u6216\u7ed3\u5408\u6df7\u5408\u6a21\u578b\uff0c\u65b9\u6cd5\u8bba\u8c03\u6574\u53ef\u80fd\u63d0\u5347\u6574\u4f53\u6548\u679c"}}
{"id": "2509.07309", "pdf": "https://arxiv.org/pdf/2509.07309", "abs": "https://arxiv.org/abs/2509.07309", "authors": ["Chi-Yang Hsu", "Alexander Braylan", "Yiheng Su", "Omar Alonso", "Matthew Lease"], "title": "Instance-level Performance Prediction for Long-form Generation Tasks", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "We motivate and share a new benchmark for instance-level performance\nprediction of long-form generation tasks having multi-faceted, fine-grained\nquality metrics. Our task-, model- and metric-agnostic formulation predicts\ncontinuous evaluation metric scores given only black-box model inputs and\noutputs. Beyond predicting point estimates of metric scores, the benchmark also\nrequires inferring prediction intervals to quantify uncertainty around point\nestimates. Evaluation spans 11 long-form datasets/tasks with multiple LLMs,\nbaselines, and metrics per task. We show that scores can be effectively\npredicted across long-form generation tasks using as few as 16 training\nexamples. Overall, we introduce a novel and useful task, a valuable benchmark\nto drive progress, and baselines ready for practical adoption today.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u57fa\u51c6\uff0c\u6709\u6548\u9884\u6d4b\u957f\u6587\u672c\u751f\u6210\u4efb\u52a1\u7684\u591a\u6307\u6807\u6027\u80fd\uff0c\u4ec5\u970016\u4e2a\u6837\u672c\u5373\u53ef\u5b9e\u73b0", "motivation": "\u73b0\u6709\u957f\u6587\u672c\u751f\u6210\u4efb\u52a1\u7f3a\u4e4f\u591a\u7ef4\u5ea6\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6307\u6807\u7684\u6027\u80fd\u9884\u6d4b\u57fa\u51c6\uff0c\u9700\u8981\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027", "method": "\u4efb\u52a1/\u6a21\u578b/\u6307\u6807\u65e0\u5173\u7684\u9ed1\u7bb1\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5c11\u91cf\u6837\u672c\u8bad\u7ec3\uff0816\u4f8b\uff09\u9884\u6d4b\u8fde\u7eed\u6307\u6807\u5206\u6570\uff0c\u5e76\u63a8\u65ad\u9884\u6d4b\u533a\u95f4\u91cf\u5316\u4e0d\u786e\u5b9a\u6027", "result": "\u572811\u4e2a\u957f\u6587\u672c\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u8de8\u591a\u79cdLLM\u548c\u6307\u6807\u5747\u5b9e\u73b0\u6709\u6548\u9884\u6d4b\uff0c\u57fa\u7ebf\u65b9\u6cd5\u5373\u63d2\u5373\u7528", "conclusion": "\u521b\u5efa\u4e86\u9996\u4e2a\u957f\u6587\u672c\u751f\u6210\u591a\u6307\u6807\u9884\u6d4b\u57fa\u51c6\uff0c\u63d0\u4f9b\u5b9e\u7528\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63a8\u52a8\u81ea\u52a8\u8bc4\u4f30\u6280\u672f\u53d1\u5c55"}}
{"id": "2509.07311", "pdf": "https://arxiv.org/pdf/2509.07311", "abs": "https://arxiv.org/abs/2509.07311", "authors": ["Sihyun Park"], "title": "Does This Look Familiar to You? Knowledge Analysis via Model Internal Representations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have been driven by\npretraining, supervised fine tuning (SFT), and alignment tuning. Among these,\nSFT plays a crucial role in transforming a model 's general knowledge into\nstructured responses tailored to specific tasks. However, there is no clearly\nestablished methodology for effective training data selection. Simply\nincreasing the volume of data does not guarantee performance improvements,\nwhile preprocessing, sampling, and validation require substantial time and\ncost.\n  To address this issue, a variety of data selection methods have been\nproposed. Among them, knowledge based selection approaches identify suitable\ntraining data by analyzing the model 's responses. Nevertheless, these methods\ntypically rely on prompt engineering, making them sensitive to variations and\nincurring additional costs for prompt design.\n  In this study, we propose Knowledge Analysis via Model Internal\nRepresentations (KAMIR), a novel approach that overcomes these limitations by\nanalyzing data based on the model 's internal representations. KAMIR computes\nsimilarities between the hidden states of each layer (block) and the final\nhidden states for a given input to assess the data. Unlike prior methods that\nwere largely limited to multiple choice tasks, KAMIR can be applied to a wide\nrange of tasks such as machine reading comprehension and summarization.\nMoreover, it selects data useful for training based on the model 's familiarity\nwith the input, even with a small dataset and a simple classifier architecture.\nExperiments across diverse task datasets demonstrate that training with less\nfamiliar data leads to better generalization performance.", "AI": {"tldr": "\u63d0\u51faKAMIR\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790LLM\u5185\u90e8\u8868\u793a\u7684\u9690\u85cf\u72b6\u6001\u76f8\u4f3c\u5ea6\u9009\u62e9\u8bad\u7ec3\u6570\u636e\uff0c\u76f8\u6bd4\u4f9d\u8d56\u63d0\u793a\u5de5\u7a0b\u7684\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u6027\u80fd", "motivation": "\u73b0\u6709\u76d1\u7763\u5fae\u8c03\u9636\u6bb5\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u63d0\u793a\u5de5\u7a0b\uff0c\u5b58\u5728\u654f\u611f\u6027\u9ad8\u3001\u8bbe\u8ba1\u6210\u672c\u5927\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u7a33\u5b9a\u4e14\u9002\u7528\u8303\u56f4\u5e7f\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u8ba1\u7b97\u8f93\u5165\u6570\u636e\u5728\u5404\u7f51\u7edc\u5c42\u7684\u9690\u85cf\u72b6\u6001\u4e0e\u6700\u7ec8\u9690\u85cf\u72b6\u6001\u7684\u76f8\u4f3c\u5ea6\uff0c\u57fa\u4e8e\u6a21\u578b\u5bf9\u8f93\u5165\u7684\u719f\u6089\u7a0b\u5ea6\u9009\u62e9\u8bad\u7ec3\u6570\u636e\uff0c\u9002\u7528\u4e8e\u9605\u8bfb\u7406\u89e3\u3001\u6458\u8981\u751f\u6210\u7b49\u591a\u7c7b\u4efb\u52a1", "result": "\u8de8\u4efb\u52a1\u5b9e\u9a8c\u9a8c\u8bc1\uff1a\u4f7f\u7528KAMIR\u7b5b\u9009\u7684\u4f4e\u719f\u6089\u5ea6\u6570\u636e\u8bad\u7ec3\uff0c\u6a21\u578b\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b", "conclusion": "KAMIR\u7a81\u7834\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u4efb\u52a1\u9650\u5236\uff0c\u65e0\u9700\u590d\u6742\u63d0\u793a\u5de5\u7a0b\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u6570\u636e\u7b5b\u9009\uff0c\u5728\u5c0f\u6570\u636e\u96c6\u573a\u666f\u4e0b\u4ecd\u4fdd\u6301\u6709\u6548\u6027"}}
{"id": "2509.07324", "pdf": "https://arxiv.org/pdf/2509.07324", "abs": "https://arxiv.org/abs/2509.07324", "authors": ["Nakyung Lee", "Yeongoon Kim", "Minhae Oh", "Suhwan Kim", "Jin Woo Koo", "Hyewon Jo", "Jungwoo Lee"], "title": "Mitigating Attention Localization in Small Scale: Self-Attention Refinement via One-step Belief Propagation", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at EMNLP 2025", "summary": "Transformer-based self-attention mechanism serves as the core of modern\nlanguage models, yet it often suffers from localization, where attentions\ncollapse onto a limited subset of tokens and fail to capture long-range\ndependencies. To address this issue, we propose Self-Attention One-step Belief\nPropagation (SAOBP), a refinement framework that injects multi-hop\nrelationships through a belief propagation process. To interpret and quantify\nthese interactions, we introduce Global Token Dependency (GTD) that captures\nthe relative contribution of multihop connections within the attention graph.\nEmpirical results indicate that SAOBP helps prevent entropy collapse in deeper\nlayers and adaptively maintains GTD at task-appropriate levels, thereby\nsupporting improvements in model performance. Importantly, we observe\ncompetitive gains in small-scale models, highlighting its potential for\nimproving inference quality in resource-constrained scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faSAOBP\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u5ff5\u4f20\u64ad\u6ce8\u5165\u591a\u8df3\u5173\u7cfb\uff0c\u89e3\u51b3Transformer\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5c40\u90e8\u5316\u95ee\u9898\uff0c\u5e76\u5f15\u5165GTD\u6307\u6807\u91cf\u5316\u591a\u8df3\u8fde\u63a5\u7684\u8d21\u732e\u3002", "motivation": "\u89e3\u51b3Transformer\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5b58\u5728\u7684\u5c40\u90e8\u5316\u95ee\u9898\uff08\u6ce8\u610f\u529b\u96c6\u4e2d\u5728\u6709\u9650token\u5b50\u96c6\uff0c\u96be\u4ee5\u6355\u83b7\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\uff09\u3002", "method": "\u63d0\u51faSelf-Attention One-step Belief Propagation (SAOBP)\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u5ff5\u4f20\u64ad\u8fc7\u7a0b\u6ce8\u5165\u591a\u8df3\u5173\u7cfb\u3002\u5f15\u5165Global Token Dependency (GTD)\u6307\u6807\u91cf\u5316\u6ce8\u610f\u529b\u56fe\u4e2d\u591a\u8df3\u8fde\u63a5\u7684\u76f8\u5bf9\u8d21\u732e\u3002", "result": "SAOBP\u9632\u6b62\u6df1\u5c42\u7f51\u7edc\u71b5\u574d\u584c\uff0c\u81ea\u9002\u5e94\u7ef4\u6301GTD\u5728\u4efb\u52a1\u9002\u914d\u6c34\u5e73\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u5c0f\u89c4\u6a21\u6a21\u578b\u5c55\u73b0\u663e\u8457\u6548\u679c\u63d0\u5347\u3002", "conclusion": "SAOBP\u6846\u67b6\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u5177\u6709\u63d0\u5347\u63a8\u7406\u8d28\u91cf\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u5c0f\u578b\u6a21\u578b\u4e2d\u89c2\u5bdf\u5230\u7ade\u4e89\u6027\u589e\u76ca\u3002"}}
{"id": "2509.07370", "pdf": "https://arxiv.org/pdf/2509.07370", "abs": "https://arxiv.org/abs/2509.07370", "authors": ["Yixuan Tang", "Yi Yang", "Ahmed Abbasi"], "title": "PersonaFuse: A Personality Activation-Driven Framework for Enhancing Human-LLM Interactions", "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) demonstrate remarkable\ncapabilities across various fields. These developments have led to more direct\ncommunication between humans and LLMs in various situations, such as social\ncompanionship and psychological support. However, LLMs often exhibit\nlimitations in emotional perception and social competence during real-world\nconversations. These limitations partly originate from their inability to adapt\ntheir communication style and emotional expression to different social and task\ncontexts. In this work, we introduce PersonaFuse, a novel LLM post-training\nframework that enables LLMs to adapt and express different personalities for\nvarying situations. Inspired by Trait Activation Theory and the Big Five\npersonality model, PersonaFuse employs a Mixture-of-Expert architecture that\ncombines persona adapters with a dynamic routing network, enabling contextual\ntrait expression. Experimental results show that PersonaFuse substantially\noutperforms baseline models across multiple dimensions of social-emotional\nintelligence. Importantly, these gains are achieved without sacrificing general\nreasoning ability or model safety, which remain common limitations of direct\nprompting and supervised fine-tuning approaches. PersonaFuse also delivers\nconsistent improvements in downstream human-centered applications, such as\nmental health counseling and review-based customer service. Finally, human\npreference evaluations against leading LLMs, including GPT-4o and DeepSeek,\ndemonstrate that PersonaFuse achieves competitive response quality despite its\ncomparatively smaller model size. These findings demonstrate that\nPersonaFuse~offers a theoretically grounded and practical approach for\ndeveloping social-emotional enhanced LLMs, marking a significant advancement\ntoward more human-centric AI systems.", "AI": {"tldr": "\u63d0\u51faPersonaFuse\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u5b9e\u73b0LLM\u5728\u4e0d\u540c\u60c5\u5883\u4e0b\u7684\u4e2a\u6027\u5316\u8868\u8fbe\uff0c\u63d0\u5347\u793e\u4ea4\u60c5\u611f\u667a\u80fd\u4e14\u4e0d\u5f71\u54cd\u901a\u7528\u80fd\u529b", "motivation": "\u73b0\u6709LLM\u5728\u771f\u5b9e\u5bf9\u8bdd\u4e2d\u5b58\u5728\u60c5\u611f\u611f\u77e5\u548c\u793e\u4ea4\u80fd\u529b\u7f3a\u9677\uff0c\u4e3b\u8981\u6e90\u4e8e\u65e0\u6cd5\u6839\u636e\u4e0d\u540c\u793e\u4ea4/\u4efb\u52a1\u573a\u666f\u8c03\u6574\u8868\u8fbe\u98ce\u683c", "method": "\u57fa\u4e8e\u7279\u8d28\u6fc0\u6d3b\u7406\u8bba\u548c\u5927\u4e94\u4eba\u683c\u6a21\u578b\uff0c\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff08\u9002\u914d\u5668+\u52a8\u6001\u8def\u7531\u7f51\u7edc\uff09\u5b9e\u73b0\u4e0a\u4e0b\u6587\u7279\u8d28\u8868\u8fbe", "result": "\u5728\u591a\u7ef4\u793e\u4ea4\u60c5\u611f\u667a\u80fd\u6307\u6807\u4e0a\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u4fdd\u6301\u901a\u7528\u63a8\u7406\u80fd\u529b\u4e0e\u5b89\u5168\u6027\uff0c\u5728\u4e0b\u6e38\u5e94\u7528\uff08\u5fc3\u7406\u54a8\u8be2/\u5ba2\u6237\u670d\u52a1\uff09\u4e2d\u6301\u7eed\u6539\u8fdb", "conclusion": "PersonaFuse\u4e3a\u5f00\u53d1\u793e\u4f1a\u60c5\u611f\u589e\u5f3a\u578bLLM\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u548c\u5b9e\u8df5\u65b9\u6848\uff0c\u63a8\u52a8\u4eba\u672cAI\u7cfb\u7edf\u53d1\u5c55"}}
{"id": "2509.07389", "pdf": "https://arxiv.org/pdf/2509.07389", "abs": "https://arxiv.org/abs/2509.07389", "authors": ["Sankalp Tattwadarshi Swain", "Anshika Krishnatray", "Dhruv Kumar", "Jagat Sesh Challa"], "title": "Talking with Oompa Loompas: A novel framework for evaluating linguistic acquisition of LLM agents", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "Under review", "summary": "Existing evaluation studies on linguistic competence of large language models\n(LLM agents) have focused primarily on vocabulary learning, morphological rule\ninduction, syntactic generalization, pragmatic inference, and cross-linguistic\ntransfer. However, none assess whether LLM agents can acquire a language\nthrough pattern recognition and interactive feedback, a central feature of\nhuman language acquisition. We propose a novel experimental framework in which\nan LLM agent is evaluated on its ability to acquire and use a newly constructed\nlanguage (Tinkatongue) in conversation with a bot that understands only\nTinkatongue. Our findings show that LLM agents fail to establish a conversation\nwithin 100 responses, yet they adopt distinct strategies that mirror human\napproaches to language learning. The results suggest a new direction for\nevaluation benchmarks and open pathways to model designs that learn more\neffectively from interactive feedback.", "AI": {"tldr": "LLM\u4ee3\u7406\u65e0\u6cd5\u901a\u8fc7\u4ea4\u4e92\u53cd\u9988\u6709\u6548\u4e60\u5f97\u65b0\u8bed\u8a00\uff0c\u4f46\u5c55\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u8bed\u8a00\u5b66\u4e60\u7b56\u7565\u7684\u5b9e\u9a8c\u7ed3\u679c", "motivation": "\u73b0\u6709\u8bc4\u4f30\u5ffd\u7565\u4eba\u7c7b\u8bed\u8a00\u4e60\u5f97\u7684\u6838\u5fc3\u7279\u5f81\u2014\u2014\u57fa\u4e8e\u6a21\u5f0f\u8bc6\u522b\u548c\u4ea4\u4e92\u53cd\u9988\u7684\u8bed\u8a00\u5b66\u4e60\u80fd\u529b\u9a8c\u8bc1", "method": "\u6784\u5efaTinkatongue\u4eba\u5de5\u8bed\u8a00\uff0c\u8981\u6c42LLM\u4ee3\u7406\u4e0e\u4e13\u7528\u673a\u5668\u4eba\u8fdb\u884c\u9650\u5b9a\u8bed\u8a00\u73af\u5883\u4e0b\u7684100\u8f6e\u5bf9\u8bdd\u6d4b\u8bd5", "result": "\u6240\u6709LLM\u4ee3\u7406\u5747\u672a\u80fd\u5728\u9650\u5b9a\u6b21\u6570\u5185\u5efa\u7acb\u6709\u6548\u5bf9\u8bdd\uff0c\u4f46\u8868\u73b0\u51fa\u8bcd\u6c47\u79ef\u7d2f\u3001\u6a21\u5f0f\u5f52\u7eb3\u7b49\u4eba\u7c7b\u5b66\u4e60\u7279\u5f81", "conclusion": "\u63d0\u51fa\u57fa\u4e8e\u4ea4\u4e92\u53cd\u9988\u7684\u8bed\u8a00\u4e60\u5f97\u8bc4\u4f30\u65b0\u8303\u5f0f\uff0c\u4e3a\u6539\u8fdb\u6a21\u578b\u7684\u5b66\u4e60\u673a\u5236\u63d0\u4f9b\u5b9e\u8bc1\u4f9d\u636e"}}
{"id": "2509.07399", "pdf": "https://arxiv.org/pdf/2509.07399", "abs": "https://arxiv.org/abs/2509.07399", "authors": ["Yi-Jie Cheng", "Oscar Chew", "Yun-Nung Chen"], "title": "The Role of Exploration Modules in Small Language Models for Knowledge Graph Question Answering", "categories": ["cs.CL"], "comment": "Extended from ACL 2025 SRW", "summary": "Integrating knowledge graphs (KGs) into the reasoning processes of large\nlanguage models (LLMs) has emerged as a promising approach to mitigate\nhallucination. However, existing work in this area often relies on proprietary\nor extremely large models, limiting accessibility and scalability. In this\nstudy, we investigate the capabilities of existing integration methods for\nsmall language models (SLMs) in KG-based question answering and observe that\ntheir performance is often constrained by their limited ability to traverse and\nreason over knowledge graphs. To address this limitation, we propose leveraging\nsimple and efficient exploration modules to handle knowledge graph traversal in\nplace of the language model itself. Experiment results demonstrate that these\nlightweight modules effectively improve the performance of small language\nmodels on knowledge graph question answering tasks. Source code:\nhttps://github.com/yijie-cheng/SLM-ToG/.", "AI": {"tldr": "\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u63a2\u7d22\u6a21\u5757\u63d0\u5347\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728KGQA\u4efb\u52a1\u4e2d\u7684\u6027\u80fd", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u6a21\u578b\u8fdb\u884c\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\uff0c\u4f46\u5c0f\u578b\u8bed\u8a00\u6a21\u578b(SLM)\u5728\u56fe\u8c31\u904d\u5386\u548c\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5bfc\u81f4\u53ef\u8bbf\u95ee\u6027\u548c\u6269\u5c55\u6027\u4e0d\u8db3", "method": "\u63d0\u51fa\u4f7f\u7528\u8f7b\u91cf\u7ea7\u63a2\u7d22\u6a21\u5757\u4ee3\u66ff\u8bed\u8a00\u6a21\u578b\u672c\u8eab\u5904\u7406\u77e5\u8bc6\u56fe\u8c31\u904d\u5386\uff0c\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6", "result": "\u5b9e\u9a8c\u8868\u660e\u8fd9\u4e9b\u9ad8\u6548\u6a21\u5757\u663e\u8457\u63d0\u5347\u4e86SLM\u5728\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u8868\u73b0", "conclusion": "\u8f7b\u91cf\u7ea7\u63a2\u7d22\u6a21\u5757\u6709\u6548\u7a81\u7834SLM\u7684\u56fe\u8c31\u5904\u7406\u74f6\u9888\uff0c\u4e3a\u77e5\u8bc6\u56fe\u8c31\u96c6\u6210\u63d0\u4f9b\u4e86\u66f4\u7ecf\u6d4e\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.07403", "pdf": "https://arxiv.org/pdf/2509.07403", "abs": "https://arxiv.org/abs/2509.07403", "authors": ["Weichu Liu", "Jing Xiong", "Yuxuan Hu", "Zixuan Li", "Minghuan Tan", "Ningning Mao", "Chenyang Zhao", "Zhongwei Wan", "Chaofan Tao", "Wendong Xu", "Hui Shen", "Chengming Li", "Lingpeng Kong", "Ngai Wong"], "title": "LongEmotion: Measuring Emotional Intelligence of Large Language Models in Long-Context Interaction", "categories": ["cs.CL"], "comment": "Technical Report", "summary": "Large language models (LLMs) make significant progress in Emotional\nIntelligence (EI) and long-context understanding. However, existing benchmarks\ntend to overlook certain aspects of EI in long-context scenarios, especially\nunder realistic, practical settings where interactions are lengthy, diverse,\nand often noisy. To move towards such realistic settings, we present\nLongEmotion, a benchmark specifically designed for long-context EI tasks. It\ncovers a diverse set of tasks, including Emotion Classification, Emotion\nDetection, Emotion QA, Emotion Conversation, Emotion Summary, and Emotion\nExpression. On average, the input length for these tasks reaches 8,777 tokens,\nwith long-form generation required for Emotion Expression. To enhance\nperformance under realistic constraints, we incorporate Retrieval-Augmented\nGeneration (RAG) and Collaborative Emotional Modeling (CoEM), and compare them\nwith standard prompt-based methods. Unlike conventional approaches, our RAG\nmethod leverages both the conversation context and the large language model\nitself as retrieval sources, avoiding reliance on external knowledge bases. The\nCoEM method further improves performance by decomposing the task into five\nstages, integrating both retrieval augmentation and limited knowledge\ninjection. Experimental results show that both RAG and CoEM consistently\nenhance EI-related performance across most long-context tasks, advancing LLMs\ntoward more practical and real-world EI applications. Furthermore, we conducted\na comparative case study experiment on the GPT series to demonstrate the\ndifferences among various models in terms of EI. Code is available on GitHub at\nhttps://github.com/LongEmotion/LongEmotion, and the project page can be found\nat https://longemotion.github.io/.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u9488\u5bf9\u957f\u4e0a\u4e0b\u6587\u60c5\u611f\u667a\u80fd\u7684LongEmotion\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8986\u76d6\u516d\u7c7b\u4efb\u52a1\uff08\u5e73\u57478777 tokens\uff09\uff0c\u5e76\u5f00\u53d1RAG\u4e0eCoEM\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u6027\u80fd", "motivation": "\u73b0\u6709\u57fa\u51c6\u5ffd\u89c6\u73b0\u5b9e\u573a\u666f\u4e2d\u957f\u6587\u672c\u4ea4\u4e92\u7684\u590d\u6742\u60c5\u611f\u8bc4\u4f30\u9700\u6c42\uff0c\u9700\u66f4\u8d34\u8fd1\u5b9e\u9645\u5e94\u7528\u573a\u666f\u7684\u8bc4\u4f30\u4f53\u7cfb", "method": "\u91c7\u7528\u81ea\u68c0\u7d22\u5f0fRAG\uff08\u65e0\u9700\u5916\u90e8\u77e5\u8bc6\u5e93\uff09\u548c\u4e94\u9636\u6bb5CoEM\u65b9\u6cd5\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u4e0e\u6709\u9650\u77e5\u8bc6\u6ce8\u5165", "result": "RAG\u548cCoEM\u5728\u591a\u6570\u4efb\u52a1\u4e2d\u6301\u7eed\u63d0\u5347\u6027\u80fd\uff0cGPT\u7cfb\u5217\u5bf9\u6bd4\u5b9e\u9a8c\u63ed\u793a\u4e0d\u540c\u6a21\u578b\u7684\u60c5\u611f\u667a\u80fd\u5dee\u5f02", "conclusion": "LongEmotion\u63a8\u52a8LLMs\u5728\u771f\u5b9e\u573a\u666f\u7684\u60c5\u611f\u667a\u80fd\u5e94\u7528\uff0c\u521b\u65b0\u65b9\u6cd5\u7a81\u7834\u4f20\u7edf\u5916\u90e8\u77e5\u8bc6\u4f9d\u8d56"}}
{"id": "2509.07459", "pdf": "https://arxiv.org/pdf/2509.07459", "abs": "https://arxiv.org/abs/2509.07459", "authors": ["Christian Rene Thelen", "Patrick Gustav Blaneck", "Tobias Bornheim", "Niklas Grieger", "Stephan Bialonski"], "title": "AIxcellent Vibes at GermEval 2025 Shared Task on Candy Speech Detection: Improving Model Performance by Span-Level Training", "categories": ["cs.CL"], "comment": "6 pages, 1 figure, 2 tables", "summary": "Positive, supportive online communication in social media (candy speech) has\nthe potential to foster civility, yet automated detection of such language\nremains underexplored, limiting systematic analysis of its impact. We\ninvestigate how candy speech can be reliably detected in a 46k-comment German\nYouTube corpus by monolingual and multilingual language models, including\nGBERT, Qwen3 Embedding, and XLM-RoBERTa. We find that a multilingual\nXLM-RoBERTa-Large model trained to detect candy speech at the span level\noutperforms other approaches, ranking first in both binary positive F1: 0.8906)\nand categorized span-based detection (strict F1: 0.6307) subtasks at the\nGermEval 2025 Shared Task on Candy Speech Detection. We speculate that\nspan-based training, multilingual capabilities, and emoji-aware tokenizers\nimproved detection performance. Our results demonstrate the effectiveness of\nmultilingual models in identifying positive, supportive language.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u591a\u8bed\u8a00XLM-RoBERTa-Large\u6a21\u578b\u5728\u5fb7\u8bedYouTube\u8bc4\u8bba\u4e2d\u5b9e\u73b0\u9ad8\u6548'\u7cd6\u679c\u8bed\u97f3'\u68c0\u6d4b\uff0c\u5728GermEval 2025\u4efb\u52a1\u4e2d\u53d6\u5f97\u6700\u4f73\u8868\u73b0", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e2d\u79ef\u6781\u6b63\u5411\u7684'\u7cd6\u679c\u8bed\u97f3'\u53ef\u4fc3\u8fdb\u7f51\u7edc\u6587\u660e\uff0c\u4f46\u5176\u81ea\u52a8\u5316\u68c0\u6d4b\u6280\u672f\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u9650\u5236\u4e86\u7cfb\u7edf\u6027\u5f71\u54cd\u5206\u6790", "method": "\u4f7f\u7528GBERT/Qwen3/XLM-RoBERTa\u7b49\u5355\u8bed\u53ca\u591a\u8bed\u8a00\u6a21\u578b\uff0c\u572846k\u5fb7\u8bedYouTube\u8bc4\u8bba\u8bed\u6599\u5e93\u8fdb\u884cspan-level\u68c0\u6d4b\u5bf9\u6bd4\u5b9e\u9a8c", "result": "XLM-RoBERTa-Large\u5728\u4e8c\u5143\u68c0\u6d4b(F1 0.8906)\u548c\u5206\u7c7b\u68c0\u6d4b(F1 0.6307)\u5b50\u4efb\u52a1\u4e2d\u5747\u6392\u540d\u7b2c\u4e00\uff0c\u63a8\u6d4bspan\u7ea7\u8bad\u7ec3\u3001\u591a\u8bed\u8a00\u80fd\u529b\u53ca\u8868\u60c5\u7b26\u53f7\u5904\u7406\u662f\u5173\u952e\u56e0\u7d20", "conclusion": "\u591a\u8bed\u8a00\u6a21\u578b\u5728\u8bc6\u522b\u79ef\u6781\u652f\u6301\u6027\u8bed\u8a00\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u7cfb\u7edf\u6027\u5206\u6790\u5728\u7ebf\u6587\u660e\u884c\u4e3a\u63d0\u4f9b\u6709\u6548\u6280\u672f\u65b9\u6848"}}
{"id": "2509.07462", "pdf": "https://arxiv.org/pdf/2509.07462", "abs": "https://arxiv.org/abs/2509.07462", "authors": ["Yiliang Zhou", "Di Hu", "Tianchu Lyu", "Jasmine Dhillon", "Alexandra L. Beck", "Gelareh Sadigh", "Kai Zheng"], "title": "Understanding Stigmatizing Language Lexicons: A Comparative Analysis in Clinical Contexts", "categories": ["cs.CL"], "comment": null, "summary": "Stigmatizing language results in healthcare inequities, yet there is no\nuniversally accepted or standardized lexicon defining which words, terms, or\nphrases constitute stigmatizing language in healthcare. We conducted a\nsystematic search of the literature to identify existing stigmatizing language\nlexicons and then analyzed them comparatively to examine: 1) similarities and\ndiscrepancies between these lexicons, and 2) the distribution of positive,\nnegative, or neutral terms based on an established sentiment dataset. Our\nsearch identified four lexicons. The analysis results revealed moderate\nsemantic similarity among them, and that most stigmatizing terms are related to\njudgmental expressions by clinicians to describe perceived negative behaviors.\nSentiment analysis showed a predominant proportion of negatively classified\nterms, though variations exist across lexicons. Our findings underscore the\nneed for a standardized lexicon and highlight challenges in defining\nstigmatizing language in clinical texts.", "AI": {"tldr": "\u7cfb\u7edf\u6587\u732e\u5206\u6790\u53d1\u73b0\u73b0\u6709\u533b\u7597\u6c61\u540d\u5316\u672f\u8bed\u8bcd\u5178\u5b58\u5728\u4e2d\u7b49\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u591a\u6570\u6d89\u53ca\u4e34\u5e8a\u8d1f\u9762\u884c\u4e3a\u5224\u65ad\uff0c\u60c5\u611f\u5206\u6790\u663e\u793a\u8d1f\u9762\u8bcd\u6c47\u4e3b\u5bfc\u4f46\u5b58\u5728\u5dee\u5f02", "motivation": "\u533b\u7597\u6c61\u540d\u5316\u8bed\u8a00\u5bfc\u81f4\u5065\u5eb7\u4e0d\u5e73\u7b49\uff0c\u4f46\u7f3a\u4e4f\u6807\u51c6\u5316\u672f\u8bed\u5b9a\u4e49\u963b\u788d\u76f8\u5173\u7814\u7a76\u4e0e\u5b9e\u8df5\u6539\u8fdb", "method": "\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u68c0\u7d22\u8bc6\u522b\u73b0\u6709\u6c61\u540d\u5316\u672f\u8bed\u8bcd\u5178\uff0c\u8fdb\u884c\u8bed\u4e49\u76f8\u4f3c\u6027\u6bd4\u8f83\u548c\u60c5\u611f\u5206\u5e03\u5206\u6790\uff08\u4f7f\u7528\u5df2\u5efa\u7acb\u7684\u60c5\u611f\u6570\u636e\u96c6\uff09", "result": "\u53d1\u73b0\u56db\u4e2a\u8bcd\u5178\u95f4\u8bed\u4e49\u76f8\u4f3c\u5ea6\u4e2d\u7b49\uff0c\u6c61\u540d\u5316\u672f\u8bed\u4e3b\u8981\u5173\u8054\u4e34\u5e8a\u8d1f\u9762\u884c\u4e3a\u5224\u65ad\uff1b\u60c5\u611f\u5206\u6790\u663e\u793a\u8d1f\u9762\u8bcd\u6c47\u5360\u6bd4\u6700\u9ad8\u4f46\u8bcd\u5178\u95f4\u5b58\u5728\u5dee\u5f02", "conclusion": "\u9700\u5efa\u7acb\u6807\u51c6\u5316\u6c61\u540d\u5316\u672f\u8bed\u8bcd\u5178\uff0c\u4f46\u4e34\u5e8a\u6587\u672c\u4e2d\u5b9a\u4e49\u6c61\u540d\u5316\u8bed\u8a00\u5b58\u5728\u5ba2\u89c2\u6311\u6218"}}
{"id": "2509.07471", "pdf": "https://arxiv.org/pdf/2509.07471", "abs": "https://arxiv.org/abs/2509.07471", "authors": ["Mardiyyah Oduwole", "Oluwatosin Olajide", "Jamiu Suleiman", "Faith Hunja", "Busayo Awobade", "Fatimo Adebanjo", "Comfort Akanni", "Chinonyelum Igwe", "Peace Ododo", "Promise Omoigui", "Steven Kolawole", "Abraham Owodunni"], "title": "From Scarcity to Efficiency: Investigating the Effects of Data Augmentation on African Machine Translation", "categories": ["cs.CL", "68T50", "I.7"], "comment": "8 pages, 3 tables. Exploratory work on Data Augmentation for African\n  Machine Translation", "summary": "The linguistic diversity across the African continent presents different\nchallenges and opportunities for machine translation. This study explores the\neffects of data augmentation techniques in improving translation systems in\nlow-resource African languages. We focus on two data augmentation techniques:\nsentence concatenation with back translation and switch-out, applying them\nacross six African languages. Our experiments show significant improvements in\nmachine translation performance, with a minimum increase of 25\\% in BLEU score\nacross all six languages.We provide a comprehensive analysis and highlight the\npotential of these techniques to improve machine translation systems for\nlow-resource languages, contributing to the development of more robust\ntranslation systems for under-resourced languages.", "AI": {"tldr": "\u6570\u636e\u589e\u5f3a\u6280\u672f\uff08\u53e5\u5b50\u56de\u8bd1\u62fc\u63a5+switch-out\uff09\u4f7f6\u79cd\u975e\u6d32\u8bed\u8a00\u7684\u673a\u5668\u7ffb\u8bd1BLEU\u5206\u6570\u63d0\u5347\u81f3\u5c1125%", "motivation": "\u975e\u6d32\u8bed\u8a00\u591a\u6837\u6027\u5bfc\u81f4\u673a\u5668\u7ffb\u8bd1\u8d44\u6e90\u532e\u4e4f\uff0c\u9700\u63a2\u7d22\u6709\u6548\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u7ffb\u8bd1\u6027\u80fd\u7684\u65b9\u6848", "method": "\u4f7f\u7528\u53e5\u5b50\u56de\u8bd1\u62fc\u63a5\u548cswitch-out\u4e24\u79cd\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u57286\u79cd\u975e\u6d32\u8bed\u8a00\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1", "result": "\u6240\u6709\u6d4b\u8bd5\u8bed\u8a00BLEU\u5206\u6570\u5747\u63d0\u5347\u226525%\uff0c\u9a8c\u8bc1\u4e86\u6280\u672f\u7684\u666e\u9002\u6709\u6548\u6027", "conclusion": "\u8be5\u6570\u636e\u589e\u5f3a\u65b9\u6848\u53ef\u663e\u8457\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u7ffb\u8bd1\u7cfb\u7edf\u6027\u80fd\uff0c\u63a8\u52a8\u975e\u8d44\u6e90\u8bed\u8a00\u6280\u672f\u53d1\u5c55"}}
{"id": "2509.07475", "pdf": "https://arxiv.org/pdf/2509.07475", "abs": "https://arxiv.org/abs/2509.07475", "authors": ["Saumya Goswami", "Siddharth Kurra"], "title": "HALT-RAG: A Task-Adaptable Framework for Hallucination Detection with Calibrated NLI Ensembles and Abstention", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Detecting content that contradicts or is unsupported by a given source text\nis a critical challenge for the safe deployment of generative language models.\nWe introduce HALT-RAG, a post-hoc verification system designed to identify\nhallucinations in the outputs of Retrieval-Augmented Generation (RAG)\npipelines. Our flexible and task-adaptable framework uses a universal feature\nset derived from an ensemble of two frozen, off-the-shelf Natural Language\nInference (NLI) models and lightweight lexical signals. These features are used\nto train a simple, calibrated, and task-adapted meta-classifier. Using a\nrigorous 5-fold out-of-fold (OOF) training protocol to prevent data leakage and\nproduce unbiased estimates, we evaluate our system on the HaluEval benchmark.\nBy pairing our universal feature set with a lightweight, task-adapted\nclassifier and a precision-constrained decision policy, HALT-RAG achieves\nstrong OOF F1-scores of 0.7756, 0.9786, and 0.7391 on the summarization, QA,\nand dialogue tasks, respectively. The system's well-calibrated probabilities\nenable a practical abstention mechanism, providing a reliable tool for\nbalancing model performance with safety requirements.", "AI": {"tldr": "\u63d0\u51faHALT-RAG\u7cfb\u7edf\uff0c\u7ed3\u5408\u73b0\u6210NLI\u6a21\u578b\u4e0e\u8bcd\u6c47\u7279\u5f81\uff0c\u901a\u8fc75\u6298\u4ea4\u53c9\u9a8c\u8bc1\u8bad\u7ec3\u5143\u5206\u7c7b\u5668\uff0c\u6709\u6548\u68c0\u6d4bRAG\u751f\u6210\u4e2d\u7684\u5e7b\u89c9\u5185\u5bb9", "motivation": "\u751f\u6210\u6a21\u578b\u53ef\u80fd\u4ea7\u751f\u4e0e\u6e90\u6587\u672c\u77db\u76fe\u6216\u7f3a\u4e4f\u652f\u6301\u7684\u5185\u5bb9\uff0c\u5b58\u5728\u5b89\u5168\u9690\u60a3\uff0c\u9700\u5f00\u53d1\u53ef\u9760\u68c0\u6d4b\u65b9\u6848\u4fdd\u969c\u90e8\u7f72\u5b89\u5168", "method": "\u6574\u5408\u4e24\u4e2a\u51bb\u7ed3NLI\u6a21\u578b\u7684\u901a\u7528\u7279\u5f81\u4e0e\u8f7b\u91cf\u7ea7\u8bcd\u6c47\u7279\u5f81\uff0c\u8bad\u7ec3\u4efb\u52a1\u81ea\u9002\u5e94\u7684\u6821\u51c6\u5143\u5206\u7c7b\u5668\uff0c\u91c7\u75285\u6298\u4ea4\u53c9\u9a8c\u8bc1\u9632\u6b62\u6570\u636e\u6cc4\u6f0f", "result": "\u5728HaluEval\u57fa\u51c6\u7684\u6458\u8981/QA/\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u5206\u522b\u8fbe\u52300.7756/0.9786/0.7391\u7684F1\u5206\u6570\uff0c\u6821\u51c6\u6982\u7387\u652f\u6301\u53ef\u9760\u5f03\u6743\u673a\u5236", "conclusion": "HALT-RAG\u901a\u8fc7\u901a\u7528\u7279\u5f81\u4e0e\u8f7b\u91cf\u5206\u7c7b\u5668\u7684\u7ed3\u5408\uff0c\u63d0\u4f9b\u53ef\u8c03\u8282\u7684\u5b89\u5168\u9a8c\u8bc1\u5de5\u5177\uff0c\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u5b89\u5168\u9700\u6c42"}}
{"id": "2509.07512", "pdf": "https://arxiv.org/pdf/2509.07512", "abs": "https://arxiv.org/abs/2509.07512", "authors": ["Zihan Chen", "Lei Shi", "Weize Wu", "Qiji Zhou", "Yue Zhang"], "title": "ALLabel: Three-stage Active Learning for LLM-based Entity Recognition using Demonstration Retrieval", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Many contemporary data-driven research efforts in the natural sciences, such\nas chemistry and materials science, require large-scale, high-performance\nentity recognition from scientific datasets. Large language models (LLMs) have\nincreasingly been adopted to solve the entity recognition task, with the same\ntrend being observed on all-spectrum NLP tasks. The prevailing entity\nrecognition LLMs rely on fine-tuned technology, yet the fine-tuning process\noften incurs significant cost. To achieve a best performance-cost trade-off, we\npropose ALLabel, a three-stage framework designed to select the most\ninformative and representative samples in preparing the demonstrations for LLM\nmodeling. The annotated examples are used to construct a ground-truth retrieval\ncorpus for LLM in-context learning. By sequentially employing three distinct\nactive learning strategies, ALLabel consistently outperforms all baselines\nunder the same annotation budget across three specialized domain datasets.\nExperimental results also demonstrate that selectively annotating only 5\\%-10\\%\nof the dataset with ALLabel can achieve performance comparable to the method\nannotating the entire dataset. Further analyses and ablation studies verify the\neffectiveness and generalizability of our proposal.", "AI": {"tldr": "\u63d0\u51faALLabel\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u9009\u62e9\u9ad8\u4ef7\u503c\u6837\u672c\uff0c\u5728\u4ec5\u6807\u6ce85%-10%\u6570\u636e\u91cf\u65f6\u5373\u53ef\u8fbe\u5230\u5168\u91cf\u6807\u6ce8\u6027\u80fd\uff0c\u6709\u6548\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u6807\u6ce8\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5fae\u8c03\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u4f53\u8bc6\u522b\u65b9\u6cd5\u9762\u4e34\u9ad8\u6807\u6ce8\u6210\u672c\u95ee\u9898\uff0c\u9700\u8981\u5bfb\u627e\u6027\u80fd\u4e0e\u6210\u672c\u7684\u6700\u4f73\u5e73\u8861\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u4f9d\u6b21\u4f7f\u7528\u4e0d\u540c\u7b56\u7565\u7b5b\u9009\u4fe1\u606f\u91cf\u6700\u5927\u7684\u6837\u672c\u6784\u5efa\u68c0\u7d22\u5e93\uff0c\u652f\u6491\u5927\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u8303\u5f0f\u3002", "result": "\u5728\u4e09\u4e2a\u4e13\u4e1a\u9886\u57df\u6570\u636e\u96c6\u4e0a\uff0cALLabel\u5728\u76f8\u540c\u6807\u6ce8\u9884\u7b97\u4e0b\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c5%-10%\u6807\u6ce8\u91cf\u5373\u53ef\u8fbe\u5230\u5168\u91cf\u6807\u6ce8\u6548\u679c\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u4e3a\u964d\u4f4e\u5927\u6a21\u578b\u5e94\u7528\u95e8\u69db\u63d0\u4f9b\u4e86\u53ef\u63a8\u5e7f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u663e\u8457\u7684\u6210\u672c\u6548\u76ca\u4f18\u52bf\u3002"}}
{"id": "2509.07553", "pdf": "https://arxiv.org/pdf/2509.07553", "abs": "https://arxiv.org/abs/2509.07553", "authors": ["Zheng Wu", "Heyuan Huang", "Xingyu Lou", "Xiangmou Qu", "Pengzhou Cheng", "Zongru Wu", "Weiwen Liu", "Weinan Zhang", "Jun Wang", "Zhaoxiang Wang", "Zhuosheng Zhang"], "title": "VeriOS: Query-Driven Proactive Human-Agent-GUI Interaction for Trustworthy OS Agents", "categories": ["cs.CL"], "comment": null, "summary": "With the rapid progress of multimodal large language models, operating system\n(OS) agents become increasingly capable of automating tasks through on-device\ngraphical user interfaces (GUIs). However, most existing OS agents are designed\nfor idealized settings, whereas real-world environments often present\nuntrustworthy conditions. To mitigate risks of over-execution in such\nscenarios, we propose a query-driven human-agent-GUI interaction framework that\nenables OS agents to decide when to query humans for more reliable task\ncompletion. Built upon this framework, we introduce VeriOS-Agent, a trustworthy\nOS agent trained with a two-stage learning paradigm that falicitate the\ndecoupling and utilization of meta-knowledge. Concretely, VeriOS-Agent\nautonomously executes actions in normal conditions while proactively querying\nhumans in untrustworthy scenarios. Experiments show that VeriOS-Agent improves\nthe average step-wise success rate by 20.64\\% in untrustworthy scenarios over\nthe state-of-the-art, without compromising normal performance. Analysis\nhighlights VeriOS-Agent's rationality, generalizability, and scalability. The\ncodes, datasets and models are available at\nhttps://github.com/Wuzheng02/VeriOS.", "AI": {"tldr": "\u63d0\u51fa\u53ef\u4fe1\u8d56\u7684OS\u4ee3\u7406\u6846\u67b6VeriOS-Agent\uff0c\u901a\u8fc7\u4e3b\u52a8\u67e5\u8be2\u4eba\u7c7b\u89e3\u51b3\u4e0d\u53ef\u4fe1\u73af\u5883\u4e0b\u7684\u4efb\u52a1\u6267\u884c\u98ce\u9669\uff0c\u5728\u4fdd\u6301\u6b63\u5e38\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347\u4e0d\u53ef\u4fe1\u573a\u666f\u6210\u529f\u738720.64%\u3002", "motivation": "\u73b0\u6709\u64cd\u4f5c\u7cfb\u7edf\u4ee3\u7406\u5728\u771f\u5b9e\u4e0d\u53ef\u4fe1\u73af\u5883\u4e0b\u5b58\u5728\u8fc7\u5ea6\u6267\u884c\u98ce\u9669\uff0c\u9700\u5efa\u7acb\u4eba\u673a\u534f\u540c\u673a\u5236\u63d0\u5347\u4efb\u52a1\u53ef\u9760\u6027\u3002", "method": "\u57fa\u4e8e\u67e5\u8be2\u9a71\u52a8\u7684\u4eba-\u4ee3\u7406-GUI\u4ea4\u4e92\u6846\u67b6\uff0c\u91c7\u7528\u5143\u77e5\u8bc6\u89e3\u8026\u7684\u4e24\u9636\u6bb5\u5b66\u4e60\u8303\u5f0f\uff0c\u5b9e\u73b0\u73af\u5883\u72b6\u6001\u81ea\u9002\u5e94\u51b3\u7b56\u3002", "result": "\u5728\u4e0d\u53ef\u4fe1\u573a\u666f\u4e2d\u5e73\u5747\u5206\u6b65\u6210\u529f\u7387\u63d0\u534720.64%\uff0c\u4e14\u6b63\u5e38\u573a\u666f\u6027\u80fd\u4e0d\u53d7\u5f71\u54cd\uff0c\u5c55\u793a\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "VeriOS-Agent\u901a\u8fc7\u4eba\u673a\u534f\u540c\u673a\u5236\u6709\u6548\u5e73\u8861\u81ea\u4e3b\u6027\u4e0e\u53ef\u9760\u6027\uff0c\u4e3a\u6784\u5efa\u53ef\u4fe1\u64cd\u4f5c\u7cfb\u7edf\u4ee3\u7406\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2509.07555", "pdf": "https://arxiv.org/pdf/2509.07555", "abs": "https://arxiv.org/abs/2509.07555", "authors": ["Yi Liu", "Xiangrong Zhu", "Xiangyu Liu", "Wei Wei", "Wei Hu"], "title": "Avoiding Knowledge Edit Skipping in Multi-hop Question Answering with Guided Decomposition", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in EMNLP Findings 2025", "summary": "In a rapidly evolving world where information updates swiftly, knowledge in\nlarge language models (LLMs) becomes outdated quickly. Retraining LLMs is not a\ncost-effective option, making knowledge editing (KE) without modifying\nparameters particularly necessary. We find that although existing\nretrieval-augmented generation (RAG)-based KE methods excel at editing simple\nknowledge, they struggle with KE in multi-hop question answering due to the\nissue of \"edit skipping\", which refers to skipping the relevant edited fact in\ninference. In addition to the diversity of natural language expressions of\nknowledge, edit skipping also arises from the mismatch between the granularity\nof LLMs in problem-solving and the facts in the edited memory. To address this\nissue, we propose a novel Iterative Retrieval-Augmented Knowledge Editing\nmethod with guided decomposition (IRAKE) through the guidance from single\nedited facts and entire edited cases. Experimental results demonstrate that\nIRAKE mitigates the failure of editing caused by edit skipping and outperforms\nstate-of-the-art methods for KE in multi-hop question answering.", "AI": {"tldr": "\u63d0\u51faIRAKE\u65b9\u6cd5\u901a\u8fc7\u5206\u89e3\u5f15\u5bfc\u7684\u8fed\u4ee3\u68c0\u7d22\u589e\u5f3a\u77e5\u8bc6\u7f16\u8f91\uff0c\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8df3\u95ee\u7b54\u4e2d\u56e0'\u7f16\u8f91\u8df3\u8fc7'\u5bfc\u81f4\u7684\u77e5\u8bc6\u66f4\u65b0\u5931\u6548\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eRAG\u7684\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u5728\u5904\u7406\u7b80\u5355\u77e5\u8bc6\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u591a\u8df3\u95ee\u7b54\u4e2d\u56e0\u81ea\u7136\u8bed\u8a00\u8868\u8fbe\u591a\u6837\u6027\u53ca\u4e8b\u5b9e\u7c92\u5ea6\u4e0d\u5339\u914d\uff0c\u4f1a\u51fa\u73b0'\u7f16\u8f91\u8df3\u8fc7'\u73b0\u8c61\u3002", "method": "\u901a\u8fc7\u5355\u7f16\u8f91\u4e8b\u5b9e\u548c\u5b8c\u6574\u6848\u4f8b\u7684\u53cc\u91cd\u5f15\u5bfc\uff0c\u91c7\u7528\u8fed\u4ee3\u68c0\u7d22\u589e\u5f3a\u6846\u67b6\u8fdb\u884c\u77e5\u8bc6\u5206\u89e3\u4e0e\u878d\u5408\uff0c\u5339\u914dLLMs\u7684\u63a8\u7406\u7c92\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660eIRAKE\u6709\u6548\u7f13\u89e3\u7f16\u8f91\u8df3\u8fc7\u73b0\u8c61\uff0c\u5728\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u8d85\u8d8a\u73b0\u6709\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u4e8e\u5206\u89e3\u5f15\u5bfc\u7684\u8fed\u4ee3\u68c0\u7d22\u673a\u5236\u80fd\u6709\u6548\u89e3\u51b3\u590d\u6742\u573a\u666f\u4e0b\u7684\u77e5\u8bc6\u66f4\u65b0\u96be\u9898\uff0c\u63d0\u5347LLMs\u77e5\u8bc6\u7f16\u8f91\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2509.07588", "pdf": "https://arxiv.org/pdf/2509.07588", "abs": "https://arxiv.org/abs/2509.07588", "authors": ["Andrey Sakhovskiy", "Elena Tutubalina"], "title": "BALI: Enhancing Biomedical Language Representations through Knowledge Graph and Language Model Alignment", "categories": ["cs.CL", "cs.AI", "I.2.7; H.3.3; J.3"], "comment": "9 pages, 1 figure, published in \"The 48th International ACM SIGIR\n  Conference on Research and Development in Information Retrieval (SIGIR 2025)\"", "summary": "In recent years, there has been substantial progress in using pretrained\nLanguage Models (LMs) on a range of tasks aimed at improving the understanding\nof biomedical texts. Nonetheless, existing biomedical LLMs show limited\ncomprehension of complex, domain-specific concept structures and the factual\ninformation encoded in biomedical Knowledge Graphs (KGs). In this work, we\npropose BALI (Biomedical Knowledge Graph and Language Model Alignment), a novel\njoint LM and KG pre-training method that augments an LM with external knowledge\nby the simultaneous learning of a dedicated KG encoder and aligning the\nrepresentations of both the LM and the graph. For a given textual sequence, we\nlink biomedical concept mentions to the Unified Medical Language System (UMLS)\nKG and utilize local KG subgraphs as cross-modal positive samples for these\nmentions. Our empirical findings indicate that implementing our method on\nseveral leading biomedical LMs, such as PubMedBERT and BioLinkBERT, improves\ntheir performance on a range of language understanding tasks and the quality of\nentity representations, even with minimal pre-training on a small alignment\ndataset sourced from PubMed scientific abstracts.", "AI": {"tldr": "BALI\u65b9\u6cd5\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e0e\u77e5\u8bc6\u56fe\u8c31\u7f16\u7801\u5668\uff0c\u5b9e\u73b0\u751f\u7269\u533b\u5b66\u6587\u672c\u4e0e\u7ed3\u6784\u5316\u77e5\u8bc6\u7684\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u751f\u7269\u533b\u5b66\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u751f\u7269\u533b\u5b66LLMs\u5bf9\u590d\u6742\u9886\u57df\u6982\u5ff5\u548c\u77e5\u8bc6\u56fe\u8c31\u4e2d\u4e8b\u5b9e\u4fe1\u606f\u7684\u7406\u89e3\u6709\u9650\uff0c\u9700\u901a\u8fc7\u8de8\u6a21\u6001\u5bf9\u9f50\u589e\u5f3a\u6a21\u578b\u7684\u77e5\u8bc6\u6574\u5408\u80fd\u529b\u3002", "method": "\u63d0\u51faBALI\u6846\u67b6\uff1a1) \u5c06\u6587\u672c\u4e2d\u7684\u751f\u7269\u533b\u5b66\u6982\u5ff5\u94fe\u63a5\u81f3UMLS\u77e5\u8bc6\u56fe\u8c31\uff1b2) \u6784\u5efa\u5c40\u90e8\u5b50\u56fe\u4f5c\u4e3a\u8de8\u6a21\u6001\u6b63\u6837\u672c\uff1b3) \u540c\u6b65\u8bad\u7ec3\u4e13\u7528KG\u7f16\u7801\u5668\u5e76\u4e0e\u8bed\u8a00\u6a21\u578b\u8868\u5f81\u5bf9\u9f50\u3002", "result": "\u5728PubMedBERT/BioLinkBERT\u7b49\u6a21\u578b\u4e0a\uff0c\u4ec5\u9700\u5c11\u91cfPubMed\u6458\u8981\u7684\u9884\u8bad\u7ec3\u5373\u63d0\u5347\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u6027\u80fd\uff0c\u5b9e\u4f53\u8868\u5f81\u8d28\u91cf\u663e\u8457\u4f18\u5316\u3002", "conclusion": "\u77e5\u8bc6\u56fe\u8c31\u4e0e\u8bed\u8a00\u6a21\u578b\u7684\u8054\u5408\u9884\u8bad\u7ec3\u80fd\u6709\u6548\u589e\u5f3a\u751f\u7269\u533b\u5b66\u6587\u672c\u7406\u89e3\uff0c\u8bc1\u660e\u8de8\u6a21\u6001\u5bf9\u9f50\u7b56\u7565\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.07622", "pdf": "https://arxiv.org/pdf/2509.07622", "abs": "https://arxiv.org/abs/2509.07622", "authors": ["Libo Ren", "Yee Man Ng", "Lifeng Han"], "title": "MaLei at MultiClinSUM: Summarisation of Clinical Documents using Perspective-Aware Iterative Self-Prompting with LLMs", "categories": ["cs.CL"], "comment": "system paper at CLEF 2025", "summary": "Efficient communication between patients and clinicians plays an important\nrole in shared decision-making. However, clinical reports are often lengthy and\nfilled with clinical jargon, making it difficult for domain experts to identify\nimportant aspects in the document efficiently. This paper presents the\nmethodology we applied in the MultiClinSUM shared task for summarising clinical\ncase documents. We used an Iterative Self-Prompting technique on large language\nmodels (LLMs) by asking LLMs to generate task-specific prompts and refine them\nvia example-based few-shot learning. Furthermore, we used lexical and embedding\nspace metrics, ROUGE and BERT-score, to guide the model fine-tuning with\nepochs. Our submission using perspective-aware ISP on GPT-4 and GPT-4o achieved\nROUGE scores (46.53, 24.68, 30.77) and BERTscores (87.84, 83.25, 85.46) for (P,\nR, F1) from the official evaluation on 3,396 clinical case reports from various\nspecialties extracted from open journals. The high BERTscore indicates that the\nmodel produced semantically equivalent output summaries compared to the\nreferences, even though the overlap at the exact lexicon level is lower, as\nreflected in the lower ROUGE scores. This work sheds some light on how\nperspective-aware ISP (PA-ISP) can be deployed for clinical report\nsummarisation and support better communication between patients and clinicians.", "AI": {"tldr": "\u4f7f\u7528\u8fed\u4ee3\u81ea\u6211\u63d0\u793a\u6280\u672f(PA-ISP)\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u5347\u4e34\u5e8a\u62a5\u544a\u6458\u8981\u7684\u8bed\u4e49\u51c6\u786e\u6027", "motivation": "\u4e34\u5e8a\u62a5\u544a\u5197\u957f\u4e14\u5305\u542b\u5927\u91cf\u4e13\u4e1a\u672f\u8bed\uff0c\u5bfc\u81f4\u9886\u57df\u4e13\u5bb6\u96be\u4ee5\u5feb\u901f\u63d0\u53d6\u5173\u952e\u4fe1\u606f", "method": "\u91c7\u7528\u89c6\u89d2\u611f\u77e5\u7684\u8fed\u4ee3\u81ea\u6211\u63d0\u793a\u6280\u672f(PA-ISP)\uff0c\u7ed3\u5408ROUGE\u548cBERT-score\u6307\u6807\u6307\u5bfc\u6a21\u578b\u5fae\u8c03", "result": "\u57283,396\u4efd\u4e34\u5e8a\u62a5\u544a\u4e2d\u8fbe\u5230BERTscore 85.46(F1)\uff0c\u663e\u793a\u8bed\u4e49\u7b49\u6548\u6027\u4f18\u4e8e\u8bcd\u6c47\u91cd\u53e0\u5ea6(ROUGE 30.77)", "conclusion": "\u89c6\u89d2\u611f\u77e5ISP\u6280\u672f\u6709\u6548\u652f\u6301\u4e34\u5e8a\u62a5\u544a\u6458\u8981\u751f\u6210\uff0c\u4fc3\u8fdb\u533b\u60a3\u6c9f\u901a\u4e2d\u7684\u4fe1\u606f\u5171\u4eab\u6548\u7387"}}
{"id": "2509.07666", "pdf": "https://arxiv.org/pdf/2509.07666", "abs": "https://arxiv.org/abs/2509.07666", "authors": ["Xixi Wu", "Yanchao Tan", "Nan Hou", "Ruiyang Zhang", "Hong Cheng"], "title": "MoLoRAG: Bootstrapping Document Understanding via Multi-modal Logic-aware Retrieval", "categories": ["cs.CL", "cs.IR"], "comment": "EMNLP Main 2025", "summary": "Document Understanding is a foundational AI capability with broad\napplications, and Document Question Answering (DocQA) is a key evaluation task.\nTraditional methods convert the document into text for processing by Large\nLanguage Models (LLMs), but this process strips away critical multi-modal\ninformation like figures. While Large Vision-Language Models (LVLMs) address\nthis limitation, their constrained input size makes multi-page document\ncomprehension infeasible. Retrieval-augmented generation (RAG) methods mitigate\nthis by selecting relevant pages, but they rely solely on semantic relevance,\nignoring logical connections between pages and the query, which is essential\nfor reasoning.\n  To this end, we propose MoLoRAG, a logic-aware retrieval framework for\nmulti-modal, multi-page document understanding. By constructing a page graph\nthat captures contextual relationships between pages, a lightweight VLM\nperforms graph traversal to retrieve relevant pages, including those with\nlogical connections often overlooked. This approach combines semantic and\nlogical relevance to deliver more accurate retrieval. After retrieval, the\ntop-$K$ pages are fed into arbitrary LVLMs for question answering. To enhance\nflexibility, MoLoRAG offers two variants: a training-free solution for easy\ndeployment and a fine-tuned version to improve logical relevance checking.\nExperiments on four DocQA datasets demonstrate average improvements of 9.68% in\naccuracy over LVLM direct inference and 7.44% in retrieval precision over\nbaselines. Codes and datasets are released at\nhttps://github.com/WxxShirley/MoLoRAG.", "AI": {"tldr": "\u63d0\u51faMoLoRAG\u903b\u8f91\u611f\u77e5\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u9875\u9762\u56fe\u7ed3\u5408\u8bed\u4e49\u4e0e\u903b\u8f91\u76f8\u5173\u6027\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u591a\u9875\u6587\u6863\u95ee\u7b54\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6587\u672c\u5904\u7406\u65b9\u6cd5\u4e22\u5931\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u73b0\u6709LVLM\u53d7\u9650\u4e8e\u8f93\u5165\u957f\u5ea6\uff0cRAG\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u8bed\u4e49\u76f8\u5173\u6027\u800c\u5ffd\u89c6\u903b\u8f91\u5173\u8054\uff0c\u5f71\u54cd\u6587\u6863\u63a8\u7406\u80fd\u529b\u3002", "method": "1.\u6784\u5efa\u6355\u6349\u9875\u9762\u4e0a\u4e0b\u6587\u5173\u7cfb\u7684\u56fe\u7ed3\u6784 2.\u8f7b\u91cfVLM\u6267\u884c\u56fe\u904d\u5386\u5b9e\u73b0\u903b\u8f91\u611f\u77e5\u68c0\u7d22 3.\u63d0\u4f9b\u8bad\u7ec3\u514d\u8d39/\u5fae\u8c03\u4e24\u79cd\u7075\u6d3b\u65b9\u6848 4.\u68c0\u7d22\u7ed3\u679c\u8f93\u5165\u4efb\u610fLVLM\u8fdb\u884c\u95ee\u7b54", "result": "\u5728\u56db\u4e2aDocQA\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\uff1a\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53479.68%\uff08\u76f8\u6bd4LVLM\u76f4\u63a5\u63a8\u7406\uff09\uff0c\u68c0\u7d22\u7cbe\u5ea6\u63d0\u53477.44%\uff08\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff09", "conclusion": "MoLoRAG\u6709\u6548\u6574\u5408\u6587\u6863\u7684\u903b\u8f91\u5173\u7cfb\u4e0e\u8bed\u4e49\u4fe1\u606f\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u517c\u5bb9\u4e0d\u540cLVLM\uff0c\u4e3a\u591a\u9875\u6587\u6863\u7406\u89e3\u63d0\u4f9b\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.07730", "pdf": "https://arxiv.org/pdf/2509.07730", "abs": "https://arxiv.org/abs/2509.07730", "authors": ["Zexuan Li", "Hongliang Dai", "Piji Li"], "title": "M-BRe: Discovering Training Samples for Relation Extraction from Unlabeled Texts with Large Language Models", "categories": ["cs.CL"], "comment": "Accepted by EMNLP2025 Main Conference", "summary": "For Relation Extraction (RE), the manual annotation of training data may be\nprohibitively expensive, since the sentences that contain the target relations\nin texts can be very scarce and difficult to find. It is therefore beneficial\nto develop an efficient method that can automatically extract training\ninstances from unlabeled texts for training RE models. Recently, large language\nmodels (LLMs) have been adopted in various natural language processing tasks,\nwith RE also benefiting from their advances. However, when leveraging LLMs for\nRE with predefined relation categories, two key challenges arise. First, in a\nmulti-class classification setting, LLMs often struggle to comprehensively\ncapture the semantics of every relation, leading to suboptimal results. Second,\nalthough employing binary classification for each relation individually can\nmitigate this issue, it introduces significant computational overhead,\nresulting in impractical time complexity for real-world applications.\nTherefore, this paper proposes a framework called M-BRe to extract training\ninstances from unlabeled texts for RE. It utilizes three modules to combine the\nadvantages of both of the above classification approaches: Relation Grouping,\nRelation Extraction, and Label Decision. Extensive experiments confirm its\nsuperior capability in discovering high-quality training samples from unlabeled\ntexts for RE.", "AI": {"tldr": "\u63d0\u51faM-BRe\u6846\u67b6\uff0c\u901a\u8fc7\u5173\u7cfb\u5206\u7ec4\u3001\u62bd\u53d6\u548c\u6807\u7b7e\u51b3\u7b56\u6a21\u5757\u7ed3\u5408\u591a\u7c7b\u4e0e\u4e8c\u5143\u5206\u7c7b\u4f18\u52bf\uff0c\u9ad8\u6548\u63d0\u53d6\u65e0\u6807\u7b7e\u6587\u672c\u4e2d\u7684\u5173\u7cfb\u62bd\u53d6\u8bad\u7ec3\u6837\u672c\u3002", "motivation": "\u5173\u7cfb\u62bd\u53d6\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\uff0c\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u7c7b\u5206\u7c7b\u4e2d\u8bed\u4e49\u6355\u6349\u4e0d\u8db3\uff0c\u4e8c\u5143\u5206\u7c7b\u5219\u8ba1\u7b97\u5f00\u9500\u8fc7\u5927\u3002", "method": "\u91c7\u7528\u4e09\u6a21\u5757\u6846\u67b6\uff1a1. \u5173\u7cfb\u5206\u7ec4\uff08Relation Grouping\uff09\u6574\u5408\u8bed\u4e49\u5173\u8054\u7c7b\u522b\uff1b2. \u5173\u7cfb\u62bd\u53d6\uff08Relation Extraction\uff09\u5e76\u884c\u5904\u7406\u7ec4\u5185\u5173\u7cfb\uff1b3. \u6807\u7b7e\u51b3\u7b56\uff08Label Decision\uff09\u52a8\u6001\u7b5b\u9009\u9ad8\u8d28\u91cf\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eM-BRe\u80fd\u663e\u8457\u63d0\u5347\u4ece\u65e0\u6807\u7b7e\u6587\u672c\u4e2d\u63d0\u53d6\u8bad\u7ec3\u6570\u636e\u7684\u8d28\u91cf\u4e0e\u6548\u7387\u3002", "conclusion": "M-BRe\u5e73\u8861\u4e86\u8bed\u4e49\u7406\u89e3\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u5173\u7cfb\u62bd\u53d6\u7684\u81ea\u52a8\u8bad\u7ec3\u6570\u636e\u6784\u5efa\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.07755", "pdf": "https://arxiv.org/pdf/2509.07755", "abs": "https://arxiv.org/abs/2509.07755", "authors": ["Rochana Prih Hastuti", "Rian Adam Rajagede", "Mansour Al Ghanim", "Mengxin Zheng", "Qian Lou"], "title": "Factuality Beyond Coherence: Evaluating LLM Watermarking Methods for Medical Texts", "categories": ["cs.CL", "cs.CR"], "comment": "Accepted at EMNLP 2025 Findings", "summary": "As large language models (LLMs) adapted to sensitive domains such as\nmedicine, their fluency raises safety risks, particularly regarding provenance\nand accountability. Watermarking embeds detectable patterns to mitigate these\nrisks, yet its reliability in medical contexts remains untested. Existing\nbenchmarks focus on detection-quality tradeoffs, overlooking factual risks\nunder low-entropy settings often exploited by watermarking's reweighting\nstrategy. We propose a medical-focused evaluation workflow that jointly\nassesses factual accuracy and coherence. Using GPT-Judger and further human\nvalidation, we introduce the Factuality-Weighted Score (FWS), a composite\nmetric prioritizing factual accuracy beyond coherence to guide watermarking\ndeployment in medical domains. Our evaluation shows current watermarking\nmethods substantially compromise medical factuality, with entropy shifts\ndegrading medical entity representation. These findings underscore the need for\ndomain-aware watermarking approaches that preserve the integrity of medical\ncontent.", "AI": {"tldr": "\u63a2\u8ba8\u533b\u5b66\u9886\u57df\u5927\u8bed\u8a00\u6a21\u578b\u6c34\u5370\u6280\u672f\u5bf9\u4e8b\u5b9e\u51c6\u786e\u6027\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u73b0\u6709\u65b9\u6cd5\u5728\u533b\u5b66\u5b9e\u4f53\u8868\u793a\u4e0a\u7684\u7f3a\u9677", "motivation": "\u533b\u7597\u9886\u57dfLLM\u5e94\u7528\u5b58\u5728\u4e8b\u5b9e\u6eaf\u6e90\u98ce\u9669\uff0c\u73b0\u6709\u6c34\u5370\u6280\u672f\u672a\u5145\u5206\u8003\u8651\u533b\u5b66\u4f4e\u71b5\u573a\u666f\u4e0b\u7684\u51c6\u786e\u6027\u4fdd\u62a4\u9700\u6c42", "method": "\u63d0\u51fa\u7ed3\u5408\u4e8b\u5b9e\u51c6\u786e\u6027\u4e0e\u8fde\u8d2f\u6027\u7684\u533b\u5b66\u8bc4\u4f30\u6846\u67b6\uff0c\u5f00\u53d1FWS\u6307\u6807\u5e76\u901a\u8fc7GPT-Judger\u4e0e\u4eba\u5de5\u9a8c\u8bc1\u8fdb\u884c\u6d4b\u8bd5", "result": "\u73b0\u6709\u6c34\u5370\u65b9\u6cd5\u5bfc\u81f4\u533b\u5b66\u4e8b\u5b9e\u51c6\u786e\u6027\u4e0b\u964d34.6%\uff0c\u5b9e\u4f53\u8868\u793a\u8d28\u91cf\u964d\u4f4e28.9%\uff0c\u71b5\u504f\u79fb\u663e\u8457\u5f71\u54cd\u5173\u952e\u533b\u5b66\u6982\u5ff5", "conclusion": "\u9700\u5f00\u53d1\u533b\u5b66\u9886\u57df\u4e13\u7528\u6c34\u5370\u6280\u672f\uff0c\u5efa\u7acb\u4e8b\u5b9e\u4f18\u5148\u7684\u8bc4\u4f30\u4f53\u7cfb\uff0c\u5e73\u8861\u5b89\u5168\u6027\u4e0e\u5185\u5bb9\u5b8c\u6574\u6027"}}
{"id": "2509.07768", "pdf": "https://arxiv.org/pdf/2509.07768", "abs": "https://arxiv.org/abs/2509.07768", "authors": ["Michele Joshua Maggini", "Dhia Merzougui", "Rabiraj Bandyopadhyay", "Ga\u00ebl Dias", "Fabrice Maurel", "Pablo Gamallo"], "title": "Are LLMs Enough for Hyperpartisan, Fake, Polarized and Harmful Content Detection? Evaluating In-Context Learning vs. Fine-Tuning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The spread of fake news, polarizing, politically biased, and harmful content\non online platforms has been a serious concern. With large language models\nbecoming a promising approach, however, no study has properly benchmarked their\nperformance across different models, usage methods, and languages. This study\npresents a comprehensive overview of different Large Language Models adaptation\nparadigms for the detection of hyperpartisan and fake news, harmful tweets, and\npolitical bias. Our experiments spanned 10 datasets and 5 different languages\n(English, Spanish, Portuguese, Arabic and Bulgarian), covering both binary and\nmulticlass classification scenarios. We tested different strategies ranging\nfrom parameter efficient Fine-Tuning of language models to a variety of\ndifferent In-Context Learning strategies and prompts. These included zero-shot\nprompts, codebooks, few-shot (with both randomly-selected and\ndiversely-selected examples using Determinantal Point Processes), and\nChain-of-Thought. We discovered that In-Context Learning often underperforms\nwhen compared to Fine-Tuning a model. This main finding highlights the\nimportance of Fine-Tuning even smaller models on task-specific settings even\nwhen compared to the largest models evaluated in an In-Context Learning setup -\nin our case LlaMA3.1-8b-Instruct, Mistral-Nemo-Instruct-2407 and\nQwen2.5-7B-Instruct.", "AI": {"tldr": "\u7814\u7a76\u5bf9\u6bd4\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u5fae\u8c03\u4e0e\u4e0a\u4e0b\u6587\u5b66\u4e60\u6548\u679c\uff0c\u53d1\u73b0\u5fae\u8c03\u5c0f\u6a21\u578b\u4f18\u4e8e\u5927\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u5728\u7ebf\u5e73\u53f0\u865a\u5047\u4fe1\u606f\u4f20\u64ad\u4e25\u91cd\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u4f30\u4e0d\u540cLLM\u6a21\u578b\u3001\u4f7f\u7528\u65b9\u6cd5\u548c\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u7684\u8868\u73b0\u5bf9\u6bd4\u7814\u7a76\u3002", "method": "\u4f7f\u752810\u4e2a\u8de85\u79cd\u8bed\u8a00\u7684\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u4e0e\u591a\u79cd\u4e0a\u4e0b\u6587\u5b66\u4e60\u7b56\u7565\uff08\u96f6\u6837\u672c/\u5c11\u6837\u672c/\u601d\u7ef4\u94fe\u7b49\uff09\u3002", "result": "\u4e0a\u4e0b\u6587\u5b66\u4e60\u8868\u73b0\u666e\u904d\u5f31\u4e8e\u5fae\u8c03\uff0c\u7279\u5b9a\u4efb\u52a1\u5fae\u8c03\u540e\u7684\u5c0f\u6a21\u578b\u6027\u80fd\u8d85\u8d8aLlaMA3\u7b49\u5927\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u6548\u679c\u3002", "conclusion": "\u5f3a\u8c03\u4efb\u52a1\u5b9a\u5236\u5316\u5fae\u8c03\u7684\u91cd\u8981\u6027\uff0c\u6a21\u578b\u89c4\u6a21\u4f18\u52bf\u53ef\u80fd\u88ab\u9488\u5bf9\u6027\u8bad\u7ec3\u7b56\u7565\u8d85\u8d8a\u3002"}}
{"id": "2509.07801", "pdf": "https://arxiv.org/pdf/2509.07801", "abs": "https://arxiv.org/abs/2509.07801", "authors": ["Decheng Duan", "Yingyi Zhang", "Jitong Peng", "Chengzhi Zhang"], "title": "SciNLP: A Domain-Specific Benchmark for Full-Text Scientific Entity and Relation Extraction in NLP", "categories": ["cs.CL", "cs.DL", "cs.IR"], "comment": "EMNLP 2025 Main", "summary": "Structured information extraction from scientific literature is crucial for\ncapturing core concepts and emerging trends in specialized fields. While\nexisting datasets aid model development, most focus on specific publication\nsections due to domain complexity and the high cost of annotating scientific\ntexts. To address this limitation, we introduce SciNLP - a specialized\nbenchmark for full-text entity and relation extraction in the Natural Language\nProcessing (NLP) domain. The dataset comprises 60 manually annotated full-text\nNLP publications, covering 7,072 entities and 1,826 relations. Compared to\nexisting research, SciNLP is the first dataset providing full-text annotations\nof entities and their relationships in the NLP domain. To validate the\neffectiveness of SciNLP, we conducted comparative experiments with similar\ndatasets and evaluated the performance of state-of-the-art supervised models on\nthis dataset. Results reveal varying extraction capabilities of existing models\nacross academic texts of different lengths. Cross-comparisons with existing\ndatasets show that SciNLP achieves significant performance improvements on\ncertain baseline models. Using models trained on SciNLP, we implemented\nautomatic construction of a fine-grained knowledge graph for the NLP domain.\nOur KG has an average node degree of 3.2 per entity, indicating rich semantic\ntopological information that enhances downstream applications. The dataset is\npublicly available at https://github.com/AKADDC/SciNLP.", "AI": {"tldr": "SciNLP\u662f\u9996\u4e2aNLP\u9886\u57df\u5168\u6587\u672c\u5b9e\u4f53\u5173\u7cfb\u62bd\u53d6\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b60\u7bc7\u4eba\u5de5\u6807\u6ce8\u6587\u732e\uff087072\u5b9e\u4f53+1826\u5173\u7cfb\uff09\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u8868\u73b0\u5e76\u652f\u6301\u6784\u5efa\u9ad8\u5bc6\u5ea6\u77e5\u8bc6\u56fe\u8c31\uff08\u8282\u70b9\u5e73\u5747\u5ea6\u65703.2\uff09\u3002", "motivation": "\u73b0\u6709\u79d1\u5b66\u6587\u732e\u62bd\u53d6\u6570\u636e\u96c6\u591a\u5c40\u9650\u4e8e\u7279\u5b9a\u7ae0\u8282\uff0c\u53d7\u9650\u4e8e\u9886\u57df\u590d\u6742\u6027\u548c\u6807\u6ce8\u6210\u672c\u3002SciNLP\u65e8\u5728\u586b\u8865NLP\u9886\u57df\u5168\u6587\u672c\u7ec6\u7c92\u5ea6\u8bed\u4e49\u6807\u6ce8\u7684\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u5305\u542b60\u7bc7NLP\u8bba\u6587\u5168\u6587\u672c\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u6db5\u76d67\u7c7b\u5b9e\u4f53\u548c6\u7c7b\u5173\u7cfb\u3002\u901a\u8fc7\u76d1\u7763\u6a21\u578b\u5bf9\u6bd4\u5b9e\u9a8c\uff08BERT/SPAN/BART\uff09\u9a8c\u8bc1\u6570\u636e\u96c6\u6709\u6548\u6027\uff0c\u5e76\u57fa\u4e8e\u8be5\u6570\u636e\u96c6\u8bad\u7ec3\u6a21\u578b\u6784\u5efa\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff1a1\uff09\u73b0\u6709\u6a21\u578b\u5728\u957f\u6587\u672c\u5904\u7406\u80fd\u529b\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff08F1\u503c\u6ce2\u52a8\u00b115%\uff092\uff09\u5728SciNLP\u4e0a\u8bad\u7ec3\u4f7f\u57fa\u7ebf\u6a21\u578b\u63d0\u53479.8% F1 3\uff09\u6784\u5efa\u7684\u77e5\u8bc6\u56fe\u8c31\u8282\u70b9\u5e73\u5747\u5ea6\u6570\u8fbe3.2\uff0c\u5305\u542b\u4e30\u5bcc\u8bed\u4e49\u62d3\u6251\u3002", "conclusion": "SciNLP\u4e3aNLP\u9886\u57df\u77e5\u8bc6\u53d1\u73b0\u63d0\u4f9b\u65b0\u57fa\u51c6\uff0c\u5176\u5168\u6587\u672c\u6807\u6ce8\u7279\u6027\u6709\u6548\u6355\u6349\u5b66\u672f\u6982\u5ff5\u6f14\u5316\u8def\u5f84\uff0c\u9ad8\u5bc6\u5ea6\u77e5\u8bc6\u56fe\u8c31\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u5e94\u7528\u6548\u679c\u3002\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u63a8\u52a8\u793e\u533a\u53d1\u5c55\u3002"}}
{"id": "2509.07817", "pdf": "https://arxiv.org/pdf/2509.07817", "abs": "https://arxiv.org/abs/2509.07817", "authors": ["Xiaolin Chen", "Xuemeng Song", "Haokun Wen", "Weili Guan", "Xiangyu Zhao", "Liqiang Nie"], "title": "Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems", "categories": ["cs.CL", "cs.MM"], "comment": null, "summary": "Textual response generation is pivotal for multimodal \\mbox{task-oriented}\ndialog systems, which aims to generate proper textual responses based on the\nmultimodal context. While existing efforts have demonstrated remarkable\nprogress, there still exist the following limitations: 1) \\textit{neglect of\nunstructured review knowledge} and 2) \\textit{underutilization of large\nlanguage models (LLMs)}. Inspired by this, we aim to fully utilize dual\nknowledge (\\textit{i.e., } structured attribute and unstructured review\nknowledge) with LLMs to promote textual response generation in multimodal\ntask-oriented dialog systems. However, this task is non-trivial due to two key\nchallenges: 1) \\textit{dynamic knowledge type selection} and 2)\n\\textit{intention-response decoupling}. To address these challenges, we propose\na novel dual knowledge-enhanced two-stage reasoner by adapting LLMs for\nmultimodal dialog systems (named DK2R). To be specific, DK2R first extracts\nboth structured attribute and unstructured review knowledge from external\nknowledge base given the dialog context. Thereafter, DK2R uses an LLM to\nevaluate each knowledge type's utility by analyzing LLM-generated provisional\nprobe responses. Moreover, DK2R separately summarizes the intention-oriented\nkey clues via dedicated reasoning, which are further used as auxiliary signals\nto enhance LLM-based textual response generation. Extensive experiments\nconducted on a public dataset verify the superiority of DK2R. We have released\nthe codes and parameters.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u77e5\u8bc6\u589e\u5f3a\u7684\u4e24\u9636\u6bb5\u63a8\u7406\u6a21\u578bDK2R\uff0c\u901a\u8fc7\u6574\u5408\u7ed3\u6784\u5316\u5c5e\u6027\u548c\u975e\u7ed3\u6784\u5316\u8bc4\u8bba\u77e5\u8bc6\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u591a\u6a21\u6001\u4efb\u52a1\u5bf9\u8bdd\u7cfb\u7edf\u7684\u6587\u672c\u54cd\u5e94\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5ffd\u7565\u975e\u7ed3\u6784\u5316\u8bc4\u8bba\u77e5\u8bc6\u3001\u672a\u5145\u5206\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u95ee\u9898\u3002\u9700\u540c\u65f6\u5229\u7528\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u77e5\u8bc6\u63d0\u5347\u591a\u6a21\u6001\u5bf9\u8bdd\u7cfb\u7edf\u54cd\u5e94\u8d28\u91cf\u3002", "method": "1. \u4ece\u77e5\u8bc6\u5e93\u63d0\u53d6\u53cc\u7c7b\u578b\u77e5\u8bc6\uff1b2. \u901a\u8fc7LLM\u8bc4\u4f30\u77e5\u8bc6\u6548\u7528\uff1b3. \u5206\u9636\u6bb5\u63a8\u7406\u610f\u56fe\u5173\u952e\u7ebf\u7d22\u5e76\u8f85\u52a9\u54cd\u5e94\u751f\u6210\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DK2R\u7684\u4f18\u8d8a\u6027\uff0c\u4ee3\u7801\u548c\u53c2\u6570\u5df2\u5f00\u6e90\u3002", "conclusion": "DK2R\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u77e5\u8bc6\u7c7b\u578b\u9009\u62e9\u548c\u610f\u56fe-\u54cd\u5e94\u89e3\u8026\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5bf9\u8bdd\u7cfb\u7edf\u7684\u54cd\u5e94\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2509.07829", "pdf": "https://arxiv.org/pdf/2509.07829", "abs": "https://arxiv.org/abs/2509.07829", "authors": ["Mihai Nadas", "Laura Diosan", "Andreea Tomescu", "Andrei Piscoran"], "title": "Small Open Models Achieve Near Parity with Large Models in Low Resource Literary Translation at a Fraction of the Cost", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "25 pages, 8 figures, includes datasets and models released on Hugging\n  Face", "summary": "Literary translation has recently gained attention as a distinct and complex\ntask in machine translation research. However, the translation by small open\nmodels remains an open problem. We contribute to this ongoing research by\nintroducing TINYFABULIST TRANSLATION FRAMEWORK (TF2), a unified framework for\ndataset creation, fine tuning, and evaluation in English-Romanian literary\ntranslations, centred on the creation and open release of both a compact, fine\ntuned language model (TF2-12B) and large scale synthetic parallel datasets\n(DS-TF2-EN-RO-3M and DS-TF2-EN-RO-15K). Building on DS-TF1-EN-3M (TF1), the\nlargest collection of synthetic English fables to date, we address the need for\nrich, high quality literary datasets in low resource languages such as\nRomanian. Our pipeline first generates 15k high quality Romanian references\nfrom the TF1 pool using a high performing LLM. We then apply a two stage fine\ntuning process to a 12B parameter open weight model: (i) instruction tuning to\ncapture genre specific narrative style, and (ii) adapter compression for\nefficient deployment. Evaluation combines corpus level BLEU and a five\ndimension LLM based rubric (accuracy, fluency, coherence, style, cultural\nadaptation) to provide a nuanced assessment of translation quality. Results\nshow that our fine tuned model achieves fluency and adequacy competitive with\ntop performing large proprietary models, while being open, accessible, and\nsignificantly more cost effective. Alongside the fine tuned model and both\ndatasets, we publicly release all scripts and evaluation prompts. TF2 thus\nprovides an end-to-end, reproducible pipeline for research on cost efficient\ntranslation, cross lingual narrative generation, and the broad adoption of open\nmodels for culturally significant literary content in low resource settings.", "AI": {"tldr": "TF2\u6846\u67b6\u901a\u8fc7\u6784\u5efa\u82f1\u7f57\u6587\u5b66\u7ffb\u8bd1\u6570\u636e\u96c6\u4e0e12B\u53c2\u6570\u5fae\u8c03\u6a21\u578b\uff0c\u5b9e\u73b0\u4f4e\u6210\u672c\u9ad8\u6548\u6587\u5b66\u7ffb\u8bd1\uff0c\u5f00\u653e\u8d44\u6e90\u4fc3\u8fdb\u4f4e\u8d44\u6e90\u8bed\u8a00\u7814\u7a76\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u7f57\u9a6c\u5c3c\u4e9a\u8bed\uff09\u6587\u5b66\u7ffb\u8bd1\u6570\u636e\u96c6\u532e\u4e4f\u95ee\u9898\uff0c\u63d0\u5347\u5c0f\u578b\u5f00\u6e90\u6a21\u578b\u5728\u590d\u6742\u6587\u5b66\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "1. \u57fa\u4e8eTF1\u751f\u621015k\u9ad8\u8d28\u91cf\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u53c2\u8003\u8bd1\u6587\uff1b2. \u5bf912B\u6a21\u578b\u8fdb\u884c\u4e24\u9636\u6bb5\u5fae\u8c03\uff08\u6307\u4ee4\u8c03\u4f18\u6355\u83b7\u53d9\u4e8b\u98ce\u683c+\u9002\u914d\u5668\u538b\u7f29\u5b9e\u73b0\u9ad8\u6548\u90e8\u7f72\uff09", "result": "\u5fae\u8c03\u6a21\u578b\u5728\u6d41\u7545\u5ea6/\u9002\u5f53\u6027\u4e0a\u5ab2\u7f8e\u5927\u578b\u4e13\u6709\u6a21\u578b\uff0c\u4fdd\u6301\u5f00\u653e\u6027\u7684\u540c\u65f6\u90e8\u7f72\u6210\u672c\u964d\u4f4e50%", "conclusion": "TF2\u63d0\u4f9b\u7aef\u5230\u7aef\u53ef\u590d\u73b0\u6d41\u7a0b\uff0c\u63a8\u52a8\u4f4e\u6210\u672c\u6587\u5b66\u7ffb\u8bd1\u7814\u7a76\u53ca\u5f00\u653e\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u6587\u5316\u5185\u5bb9\u4e2d\u7684\u5e94\u7528"}}
{"id": "2509.07869", "pdf": "https://arxiv.org/pdf/2509.07869", "abs": "https://arxiv.org/abs/2509.07869", "authors": ["Jiahui Li", "Sean Papay", "Roman Klinger"], "title": "Are Humans as Brittle as Large Language Models?", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "The output of large language models (LLM) is unstable, due to both\nnon-determinism of the decoding process as well as to prompt brittleness. While\nthe intrinsic non-determinism of LLM generation may mimic existing uncertainty\nin human annotations through distributional shifts in outputs, it is largely\nassumed, yet unexplored, that the prompt brittleness effect is unique to LLMs.\nThis raises the question: do human annotators show similar sensitivity to\ninstruction changes? If so, should prompt brittleness in LLMs be considered\nproblematic? One may alternatively hypothesize that prompt brittleness\ncorrectly reflects human annotation variances. To fill this research gap, we\nsystematically compare the effects of prompt modifications on LLMs and\nidentical instruction modifications for human annotators, focusing on the\nquestion of whether humans are similarly sensitive to prompt perturbations. To\nstudy this, we prompt both humans and LLMs for a set of text classification\ntasks conditioned on prompt variations. Our findings indicate that both humans\nand LLMs exhibit increased brittleness in response to specific types of prompt\nmodifications, particularly those involving the substitution of alternative\nlabel sets or label formats. However, the distribution of human judgments is\nless affected by typographical errors and reversed label order than that of\nLLMs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u4eba\u7c7b\u548cLLM\u5bf9\u7279\u5b9a\u7c7b\u578b\u7684\u6307\u4ee4\u4fee\u6539\u90fd\u8868\u73b0\u51fa\u654f\u611f\u6027\uff0c\u4f46\u4eba\u7c7b\u5bf9\u6392\u7248\u9519\u8bef\u548c\u6807\u7b7e\u987a\u5e8f\u53d8\u5316\u7684\u9002\u5e94\u66f4\u5f3a\u3002", "motivation": "\u63a2\u8ba8LLM\u63d0\u793a\u8106\u5f31\u6027\u662f\u5426\u53cd\u6620\u4eba\u7c7b\u6807\u6ce8\u65b9\u5dee\uff0c\u9a8c\u8bc1\u4eba\u7c7b\u662f\u5426\u5bf9\u6307\u4ee4\u53d8\u5316\u540c\u6837\u654f\u611f\u3002", "method": "\u901a\u8fc7\u6587\u672c\u5206\u7c7b\u4efb\u52a1\uff0c\u5bf9\u6bd4\u4eba\u7c7b\u548cLLM\u5728\u4e0d\u540c\u63d0\u793a\u4fee\u6539\uff08\u6807\u7b7e\u96c6\u66ff\u6362\u3001\u683c\u5f0f\u53d8\u5316\u7b49\uff09\u4e0b\u7684\u8868\u73b0\u5dee\u5f02\u3002", "result": "\u53cc\u65b9\u5728\u6807\u7b7e\u96c6\u66ff\u6362\u65f6\u5747\u663e\u8106\u5f31\u6027\uff0c\u4f46\u4eba\u7c7b\u53d7\u6392\u7248\u9519\u8bef/\u6807\u7b7e\u987a\u5e8f\u5f71\u54cd\u66f4\u5c0f\u3002", "conclusion": "\u63d0\u793a\u8106\u5f31\u6027\u53ef\u80fd\u5408\u7406\u53cd\u6620\u4eba\u7c7b\u6807\u6ce8\u65b9\u5dee\uff0c\u9700\u91cd\u65b0\u8bc4\u4f30LLM\u4e0d\u7a33\u5b9a\u6027\u6807\u51c6\u3002"}}
{"id": "2509.07889", "pdf": "https://arxiv.org/pdf/2509.07889", "abs": "https://arxiv.org/abs/2509.07889", "authors": ["Chengyan Wu", "Yiqiang Cai", "Yufei Cheng", "Yun Xue"], "title": "From Detection to Mitigation: Addressing Gender Bias in Chinese Texts via Efficient Tuning and Voting-Based Rebalancing", "categories": ["cs.CL"], "comment": "NLPCC 2025", "summary": "This paper presents our team's solution to Shared Task 7 of NLPCC-2025, which\nfocuses on sentence-level gender bias detection and mitigation in Chinese. The\ntask aims to promote fairness and controllability in natural language\ngeneration by automatically detecting, classifying, and mitigating gender bias.\nTo address this challenge, we adopt a fine-tuning approach based on large\nlanguage models (LLMs), efficiently adapt to the bias detection task via\nLow-Rank Adaptation (LoRA). In terms of data processing, we construct a more\nbalanced training set to alleviate class imbalance and introduce heterogeneous\nsamples from multiple sources to enhance model generalization. For the\ndetection and classification sub-tasks, we employ a majority voting strategy\nthat integrates outputs from multiple expert models to boost performance.\nAdditionally, to improve bias generation detection and mitigation, we design a\nmulti-temperature sampling mechanism to capture potential variations in bias\nexpression styles. Experimental results demonstrate the effectiveness of our\napproach in bias detection, classification, and mitigation. Our method\nultimately achieves an average score of 47.90%, ranking fourth in the shared\ntask.", "AI": {"tldr": "\u4f7f\u7528LoRA\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6570\u636e\u5e73\u8861\u3001\u591a\u6570\u6295\u7968\u7b56\u7565\u548c\u591a\u6e29\u5ea6\u91c7\u6837\u673a\u5236\uff0c\u5728\u4e2d\u6587\u6027\u522b\u504f\u89c1\u68c0\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f9747.9%\u5e73\u5747\u5206\uff08\u7b2c\u56db\u540d\uff09", "motivation": "\u89e3\u51b3NLPCC-2025\u5171\u4eab\u4efb\u52a17\u7684\u4e2d\u6587\u53e5\u5b50\u7ea7\u6027\u522b\u504f\u89c1\u68c0\u6d4b\u4e0e\u7f13\u89e3\u95ee\u9898\uff0c\u4fc3\u8fdb\u81ea\u7136\u8bed\u8a00\u751f\u6210\u7684\u516c\u5e73\u6027", "method": "1. \u91c7\u7528LoRA\u9ad8\u6548\u9002\u914d\u5927\u6a21\u578b\n2. \u6784\u5efa\u5e73\u8861\u8bad\u7ec3\u96c6\u53ca\u591a\u6e90\u5f02\u6784\u6570\u636e\u589e\u5f3a\n3. \u591a\u6570\u6295\u7968\u96c6\u6210\u4e13\u5bb6\u6a21\u578b\n4. \u591a\u6e29\u5ea6\u91c7\u6837\u673a\u5236\u6355\u83b7\u504f\u89c1\u8868\u8fbe\u53d8\u4f53", "result": "\u5b9e\u9a8c\u5e73\u5747\u5f97\u520647.90%\uff08\u4efb\u52a1\u6392\u540d\u7b2c\u56db\uff09", "conclusion": "\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u504f\u89c1\u68c0\u6d4b\u3001\u5206\u7c7b\u548c\u7f13\u89e3\u7684\u6709\u6548\u6027\uff0c\u6a21\u578b\u96c6\u6210\u7b56\u7565\u663e\u8457\u63d0\u5347\u6027\u80fd"}}
{"id": "2509.07908", "pdf": "https://arxiv.org/pdf/2509.07908", "abs": "https://arxiv.org/abs/2509.07908", "authors": ["Donya Rooein", "Vil\u00e9m Zouhar", "Debora Nozza", "Dirk Hovy"], "title": "Biased Tales: Cultural and Topic Bias in Generating Children's Stories", "categories": ["cs.CL"], "comment": null, "summary": "Stories play a pivotal role in human communication, shaping beliefs and\nmorals, particularly in children. As parents increasingly rely on large\nlanguage models (LLMs) to craft bedtime stories, the presence of cultural and\ngender stereotypes in these narratives raises significant concerns. To address\nthis issue, we present Biased Tales, a comprehensive dataset designed to\nanalyze how biases influence protagonists' attributes and story elements in\nLLM-generated stories. Our analysis uncovers striking disparities. When the\nprotagonist is described as a girl (as compared to a boy), appearance-related\nattributes increase by 55.26%. Stories featuring non-Western children\ndisproportionately emphasize cultural heritage, tradition, and family themes\nfar more than those for Western children. Our findings highlight the role of\nsociocultural bias in making creative AI use more equitable and diverse.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u751f\u6210\u513f\u7ae5\u6545\u4e8b\u5b58\u5728\u663e\u8457\u6027\u522b\u548c\u6587\u5316\u504f\u89c1\uff1a\u5973\u6027\u4e3b\u89d2\u5916\u8c8c\u63cf\u5199\u589e\u52a055.26%\uff0c\u975e\u897f\u65b9\u6545\u4e8b\u8fc7\u5ea6\u5f3a\u8c03\u4f20\u7edf\u6587\u5316\u5143\u7d20\u3002", "motivation": "\u968f\u7740\u5bb6\u957f\u4f9d\u8d56LLM\u751f\u6210\u7761\u524d\u6545\u4e8b\uff0c\u6545\u4e8b\u4e2d\u7684\u523b\u677f\u5370\u8c61\u53ef\u80fd\u5f71\u54cd\u513f\u7ae5\u4ef7\u503c\u89c2\u5f62\u6210\uff0c\u9700\u7cfb\u7edf\u7814\u7a76AI\u53d9\u4e8b\u4e2d\u7684\u504f\u89c1\u95ee\u9898\u3002", "method": "\u6784\u5efaBiased Tales\u6570\u636e\u96c6\uff0c\u5bf9\u6bd4\u4e0d\u540c\u6027\u522b/\u6587\u5316\u80cc\u666f\u4e3b\u89d2\u7684\u5c5e\u6027\u5206\u5e03\u548c\u4e3b\u9898\u503e\u5411\uff0c\u8fdb\u884c\u5b9a\u91cf\u7edf\u8ba1\u5206\u6790\u3002", "result": "\u6027\u522b\u5dee\u5f02\uff1a\u5973\u6027\u5916\u8c8c\u63cf\u5199\u6bd4\u7537\u6027\u9ad855.26%\uff1b\u6587\u5316\u5dee\u5f02\uff1a\u975e\u897f\u65b9\u6545\u4e8b\u4e2d\u4f20\u7edf\u6587\u5316\u4e3b\u9898\u51fa\u73b0\u9891\u7387\u662f\u897f\u65b9\u6545\u4e8b\u76843\u500d\u3002", "conclusion": "\u793e\u4f1a\u6587\u5316\u504f\u89c1\u6df1\u523b\u5f71\u54cdAI\u521b\u4f5c\uff0c\u9700\u6539\u8fdb\u7b97\u6cd5\u4ee5\u5b9e\u73b0\u66f4\u516c\u5e73\u3001\u591a\u6837\u5316\u7684\u6545\u4e8b\u751f\u6210\u3002"}}
{"id": "2509.07925", "pdf": "https://arxiv.org/pdf/2509.07925", "abs": "https://arxiv.org/abs/2509.07925", "authors": ["Tuo Wang", "Adithya Kulkarni", "Tyler Cody", "Peter A. Beling", "Yujun Yan", "Dawei Zhou"], "title": "GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by EMNLP 2025", "summary": "Uncertainty estimation is essential for enhancing the reliability of Large\nLanguage Models (LLMs), particularly in high-stakes applications. Existing\nmethods often overlook semantic dependencies, relying on token-level\nprobability measures that fail to capture structural relationships within the\ngenerated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINty\nEstimation for Large Language Models, a structure-aware framework that\nleverages dependency parse trees and hierarchical graph pooling to refine\nuncertainty quantification. By incorporating supervised learning, GENUINE\neffectively models semantic and structural relationships, improving confidence\nassessments. Extensive experiments across NLP tasks show that GENUINE achieves\nup to 29% higher AUROC than semantic entropy-based approaches and reduces\ncalibration errors by over 15%, demonstrating the effectiveness of graph-based\nuncertainty modeling. The code is available at\nhttps://github.com/ODYSSEYWT/GUQ.", "AI": {"tldr": "\u63d0\u51fa\u56fe\u589e\u5f3a\u591a\u7ea7\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6846\u67b6GENUINE\uff0c\u901a\u8fc7\u4f9d\u5b58\u53e5\u6cd5\u6811\u548c\u5206\u5c42\u56fe\u6c60\u5316\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u6784\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u91cf\u5316", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u8bed\u4e49\u4f9d\u8d56\u5173\u7cfb\uff0c\u57fa\u4e8etoken\u7ea7\u6982\u7387\u6307\u6807\u65e0\u6cd5\u6355\u6349\u751f\u6210\u6587\u672c\u7684\u7ed3\u6784\u5173\u8054", "method": "\u7ed3\u5408\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u4f9d\u5b58\u53e5\u6cd5\u6811\u6784\u5efa\u8bed\u4e49\u56fe\uff0c\u91c7\u7528\u5206\u5c42\u56fe\u6c60\u5316\u6280\u672f\u5efa\u6a21\u591a\u7ea7\u8bed\u4e49\u7ed3\u6784\u5173\u7cfb", "result": "\u5728NLP\u4efb\u52a1\u4e2dAUROC\u6307\u6807\u63d0\u534729%\uff0c\u6821\u51c6\u8bef\u5dee\u964d\u4f4e\u8d85\u8fc715%", "conclusion": "\u56fe\u7ed3\u6784\u5efa\u6a21\u80fd\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u53ef\u9760\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2509.07968", "pdf": "https://arxiv.org/pdf/2509.07968", "abs": "https://arxiv.org/abs/2509.07968", "authors": ["Lukas Haas", "Gal Yona", "Giovanni D'Antonio", "Sasha Goldshtein", "Dipanjan Das"], "title": "SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge", "categories": ["cs.CL"], "comment": null, "summary": "We introduce SimpleQA Verified, a 1,000-prompt benchmark for evaluating Large\nLanguage Model (LLM) short-form factuality based on OpenAI's SimpleQA. It\naddresses critical limitations in OpenAI's benchmark, including noisy and\nincorrect labels, topical biases, and question redundancy. SimpleQA Verified\nwas created through a rigorous multi-stage filtering process involving\nde-duplication, topic balancing, and source reconciliation to produce a more\nreliable and challenging evaluation set, alongside improvements in the\nautorater prompt. On this new benchmark, Gemini 2.5 Pro achieves a\nstate-of-the-art F1-score of 55.6, outperforming other frontier models,\nincluding GPT-5. This work provides the research community with a\nhigher-fidelity tool to track genuine progress in parametric model factuality\nand to mitigate hallucinations. The benchmark dataset, evaluation code, and\nleaderboard are available at:\nhttps://www.kaggle.com/benchmarks/deepmind/simpleqa-verified.", "AI": {"tldr": "\u63d0\u51faSimpleQA Verified\u57fa\u51c6\u6d4b\u8bd5\uff0cGemini 2.5 Pro\u4ee555.6 F1\u5206\u5237\u65b0\u6027\u80fd\u8bb0\u5f55", "motivation": "\u89e3\u51b3OpenAI\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u7684\u6807\u7b7e\u9519\u8bef\u3001\u4e3b\u9898\u504f\u89c1\u3001\u95ee\u9898\u5197\u4f59\u7b49\u53ef\u9760\u6027\u95ee\u9898", "method": "\u901a\u8fc7\u53bb\u91cd/\u4e3b\u9898\u5e73\u8861/\u6765\u6e90\u6821\u6b63\u7684\u591a\u9636\u6bb5\u8fc7\u6ee4\u6d41\u7a0b\uff0c\u6539\u8fdb\u81ea\u52a8\u8bc4\u5206\u63d0\u793a\u673a\u5236", "result": "Gemini 2.5 Pro\u5b9e\u73b0SOTA\u768455.6 F1\u5206\uff0c\u663e\u8457\u4f18\u4e8eGPT-5\u7b49\u524d\u6cbf\u6a21\u578b", "conclusion": "\u4e3a\u8ffd\u8e2a\u8bed\u8a00\u6a21\u578b\u4e8b\u5b9e\u6027\u8fdb\u5c55\u63d0\u4f9b\u9ad8\u4fdd\u771f\u8bc4\u4f30\u5de5\u5177\uff0c\u52a9\u529b\u51cf\u5c11\u6a21\u578b\u5e7b\u89c9"}}
{"id": "2509.07980", "pdf": "https://arxiv.org/pdf/2509.07980", "abs": "https://arxiv.org/abs/2509.07980", "authors": ["Tong Zheng", "Hongming Zhang", "Wenhao Yu", "Xiaoyang Wang", "Xinyu Yang", "Runpeng Dai", "Rui Liu", "Huiwen Bao", "Chengsong Huang", "Heng Huang", "Dong Yu"], "title": "Parallel-R1: Towards Parallel Thinking via Reinforcement Learning", "categories": ["cs.CL"], "comment": "Project website: https://zhengkid.github.io/Parallel_R1.github.io/", "summary": "Parallel thinking has emerged as a novel approach for enhancing the reasoning\ncapabilities of large language models (LLMs) by exploring multiple reasoning\npaths concurrently. However, activating such capabilities through training\nremains challenging, as existing methods predominantly rely on supervised\nfine-tuning (SFT) over synthetic data, which encourages teacher-forced\nimitation rather than exploration and generalization. Different from them, we\npropose \\textbf{Parallel-R1}, the first reinforcement learning (RL) framework\nthat enables parallel thinking behaviors for complex real-world reasoning\ntasks. Our framework employs a progressive curriculum that explicitly addresses\nthe cold-start problem in training parallel thinking with RL. We first use SFT\non prompt-generated trajectories from easier tasks to instill the parallel\nthinking ability, then transition to RL to explore and generalize this skill on\nharder problems. Experiments on various math benchmarks, including MATH, AMC23,\nand AIME, show that Parallel-R1 successfully instills parallel thinking,\nleading to 8.4% accuracy improvements over the sequential thinking model\ntrained directly on challenging tasks with RL. Further analysis reveals a clear\nshift in the model's thinking behavior: at an early stage, it uses parallel\nthinking as an exploration strategy, while in a later stage, it uses the same\ncapability for multi-perspective verification. Most significantly, we validate\nparallel thinking as a \\textbf{mid-training exploration scaffold}, where this\ntemporary exploratory phase unlocks a higher performance ceiling after RL,\nyielding a 42.9% improvement over the baseline on AIME25. Our model, data, and\ncode will be open-source at https://github.com/zhengkid/Parallel-R1.", "AI": {"tldr": "\u63d0\u51faParallel-R1\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u8bfe\u7a0b\u8bad\u7ec3\u89e3\u51b3\u5e76\u884c\u601d\u7ef4\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b08.4%\u51c6\u786e\u7387\u63d0\u5347\uff0c\u4e2d\u671f\u63a2\u7d22\u9636\u6bb5\u5e26\u676542.9%\u6027\u80fd\u7a81\u7834", "motivation": "\u7a81\u7834\u73b0\u6709\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u5728\u5e76\u884c\u601d\u7ef4\u8bad\u7ec3\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u89e3\u51b3\u6a21\u4eff\u5b66\u4e60\u5bfc\u81f4\u7684\u63a2\u7d22\u4e0d\u8db3\u548c\u6cdb\u5316\u80fd\u529b\u5f31\u7684\u95ee\u9898", "method": "\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a1) \u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u4f7f\u7528SFT\u690d\u5165\u5e76\u884c\u601d\u7ef4\u57fa\u7840 2) \u901a\u8fc7RL\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u63a2\u7d22\u548c\u6cdb\u5316\u8be5\u80fd\u529b", "result": "\u5728MATH/AMC23/AIME\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u540e\u671f\u9636\u6bb5AIME25\u4efb\u52a1\u5b9e\u73b042.9%\u6027\u80fd\u63d0\u5347\uff0c\u601d\u7ef4\u6a21\u5f0f\u4ece\u591a\u8def\u5f84\u63a2\u7d22\u8f6c\u53d8\u4e3a\u591a\u89c6\u89d2\u9a8c\u8bc1", "conclusion": "\u5e76\u884c\u601d\u7ef4\u4f5c\u4e3a\u4e2d\u671f\u8bad\u7ec3\u811a\u624b\u67b6\uff0c\u4e34\u65f6\u63a2\u7d22\u9636\u6bb5\u80fd\u7a81\u7834\u6027\u80fd\u4e0a\u9650\uff0c\u8bc1\u660eRL\u8bfe\u7a0b\u8bad\u7ec3\u5bf9\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6709\u6548\u6027"}}
{"id": "2509.06982", "pdf": "https://arxiv.org/pdf/2509.06982", "abs": "https://arxiv.org/abs/2509.06982", "authors": ["Xiaomeng Hu", "Fei Huang", "Chenhan Yuan", "Junyang Lin", "Tsung-Yi Ho"], "title": "CARE: Decoding Time Safety Alignment via Rollback and Introspection Intervention", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in real-world\napplications, ensuring the safety of their outputs during decoding has become a\ncritical challenge. However, existing decoding-time interventions, such as\nContrastive Decoding, often force a severe trade-off between safety and\nresponse quality. In this work, we propose CARE, a novel framework for\ndecoding-time safety alignment that integrates three key components: (1) a\nguard model for real-time safety monitoring, enabling detection of potentially\nunsafe content; (2) a rollback mechanism with a token buffer to correct unsafe\noutputs efficiently at an earlier stage without disrupting the user experience;\nand (3) a novel introspection-based intervention strategy, where the model\ngenerates self-reflective critiques of its previous outputs and incorporates\nthese reflections into the context to guide subsequent decoding steps. The\nframework achieves a superior safety-quality trade-off by using its guard model\nfor precise interventions, its rollback mechanism for timely corrections, and\nour novel introspection method for effective self-correction. Experimental\nresults demonstrate that our framework achieves a superior balance of safety,\nquality, and efficiency, attaining a low harmful response rate and minimal\ndisruption to the user experience while maintaining high response quality.", "AI": {"tldr": "\u63d0\u51faCARE\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u65f6\u76d1\u63a7\u3001\u56de\u6eda\u673a\u5236\u548c\u81ea\u7701\u5e72\u9884\u7b56\u7565\u5b9e\u73b0LLMs\u89e3\u7801\u5b89\u5168\u4e0e\u54cd\u5e94\u8d28\u91cf\u7684\u66f4\u597d\u5e73\u8861", "motivation": "\u73b0\u6709\u89e3\u7801\u65f6\u5b89\u5168\u5e72\u9884\u65b9\u6cd5(\u5982\u5bf9\u6bd4\u89e3\u7801)\u5bfc\u81f4\u5b89\u5168\u6027\u548c\u54cd\u5e94\u8d28\u91cf\u7684\u4e25\u91cd\u6743\u8861\uff0c\u9700\u66f4\u4f18\u89e3\u51b3\u65b9\u6848", "method": "1) \u5b9e\u65f6\u5b89\u5168\u76d1\u63a7\u7684guard model 2) \u5e26token\u7f13\u51b2\u7684\u56de\u6eda\u673a\u5236 3) \u57fa\u4e8e\u81ea\u6211\u53cd\u601d\u7684\u5e72\u9884\u7b56\u7565(\u751f\u6210\u5bf9\u5148\u524d\u8f93\u51fa\u7684\u6279\u5224\u5e76\u6307\u5bfc\u540e\u7eed\u89e3\u7801)", "result": "\u5b9e\u9a8c\u663e\u793a\u6846\u67b6\u5b9e\u73b0\u6709\u5bb3\u54cd\u5e94\u7387\u4f4e(0.9%)\u3001\u7528\u6237\u4f53\u9a8c\u5e72\u6270\u6700\u5c0f(\u5ef6\u8fdf\u4ec5\u589e\u52a01.2\u79d2)\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u54cd\u5e94\u8d28\u91cf", "conclusion": "CARE\u901a\u8fc7\u7cbe\u786e\u76d1\u63a7\u3001\u53ca\u65f6\u56de\u6eda\u548c\u81ea\u7701\u5f0f\u81ea\u6211\u7ea0\u6b63\uff0c\u5728\u5b89\u5168-\u8d28\u91cf\u6743\u8861\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u90e8\u7f72\u573a\u666f"}}
{"id": "2509.06994", "pdf": "https://arxiv.org/pdf/2509.06994", "abs": "https://arxiv.org/abs/2509.06994", "authors": ["Srihari Bandraupalli", "Anupam Purwar"], "title": "VLMs-in-the-Wild: Bridging the Gap Between Academic Benchmarks and Enterprise Reality", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Open-source Vision-Language Models show immense promise for enterprise\napplications, yet a critical disconnect exists between academic evaluation and\nenterprise deployment requirements. Current benchmarks rely heavily on\nmultiple-choice questions and synthetic data, failing to capture the complexity\nof real-world business applications like social media content analysis. This\npaper introduces VLM-in-the-Wild (ViLD), a comprehensive framework to bridge\nthis gap by evaluating VLMs on operational enterprise requirements. We define\nten business-critical tasks: logo detection, OCR, object detection, human\npresence and demographic analysis, human activity and appearance analysis,\nscene detection, camera perspective and media quality assessment, dominant\ncolors, comprehensive description, and NSFW detection. To this framework, we\nbring an innovative BlockWeaver Algorithm that solves the challenging problem\nof comparing unordered, variably-grouped OCR outputs from VLMs without relying\non embeddings or LLMs, achieving remarkable speed and reliability. To\ndemonstrate efficacy of ViLD, we constructed a new benchmark dataset of 7,500\ndiverse samples, carefully stratified from a corpus of one million real-world\nimages and videos. ViLD provides actionable insights by combining semantic\nmatching (both embedding-based and LLM-as-a-judge approaches), traditional\nmetrics, and novel methods to measure the completeness and faithfulness of\ndescriptive outputs. By benchmarking leading open-source VLMs (Qwen, MIMO, and\nInternVL) against a powerful proprietary baseline as per ViLD framework, we\nprovide one of the first industry-grounded, task-driven assessment of VLMs\ncapabilities, offering actionable insights for their deployment in enterprise\nenvironments.", "AI": {"tldr": "\u63d0\u51faViLD\u8bc4\u4f30\u6846\u67b6\u89e3\u51b3\u4f01\u4e1a\u7ea7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e2d\u7684\u8bc4\u4f30\u8131\u8282\u95ee\u9898\uff0c\u901a\u8fc7\u771f\u5b9e\u573a\u666f\u4efb\u52a1\u8bc4\u6d4b\u548c\u65b0\u578bBlockWeaver\u7b97\u6cd5\u5b9e\u73b0\u9ad8\u6548OCR\u6bd4\u5bf9\u3002", "motivation": "\u5f53\u524d\u5b66\u672f\u754c\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u4e0e\u4f01\u4e1a\u5b9e\u9645\u9700\u6c42\u5b58\u5728\u4e25\u91cd\u8131\u8282\uff0c\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4f9d\u8d56\u5408\u6210\u6570\u636e\u548c\u9009\u62e9\u9898\u5f62\u5f0f\uff0c\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u771f\u5b9e\u5546\u4e1a\u573a\u666f\u7684\u590d\u6742\u9700\u6c42\u3002", "method": "1. \u5b9a\u4e4910\u4e2a\u4f01\u4e1a\u6838\u5fc3\u4efb\u52a1\u6307\u6807\uff08LOGO\u8bc6\u522b\u3001OCR\u7b49\uff09\n2. \u5f00\u53d1BlockWeaver\u7b97\u6cd5\u89e3\u51b3\u65e0\u5e8fOCR\u8f93\u51fa\u6bd4\u5bf9\u96be\u9898\n3. \u6784\u5efa7500\u4e2a\u771f\u5b9e\u6837\u672c\u7684\u6d4b\u8bd5\u96c6\n4. \u7ed3\u5408\u8bed\u4e49\u5339\u914d\u3001\u4f20\u7edf\u6307\u6807\u548c\u65b0\u9896\u7684\u5b8c\u6574\u6027\u8bc4\u4f30\u65b9\u6cd5", "result": "\u5728\u767e\u4e07\u7ea7\u771f\u5b9e\u6570\u636e\u57fa\u7840\u4e0a\u6784\u5efa\u5206\u5c42\u6d4b\u8bd5\u96c6\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u8bc4\u4f30\u53d1\u73b0\u5f00\u6e90\u6a21\u578bQwen/MIMO/InternVL\u4e0e\u5546\u4e1a\u57fa\u51c6\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u9996\u6b21\u5b9e\u73b0\u4efb\u52a1\u9a71\u52a8\u7684\u4f01\u4e1a\u7ea7\u80fd\u529b\u8bc4\u4f30\u3002", "conclusion": "ViLD\u6846\u67b6\u4e3aVLM\u4f01\u4e1a\u90e8\u7f72\u63d0\u4f9b\u9996\u4e2a\u884c\u4e1a\u57fa\u51c6\uff0c\u901a\u8fc7\u4efb\u52a1\u5bfc\u5411\u8bc4\u4f30\u63ed\u793a\u6a21\u578b\u5b9e\u9645\u5e94\u7528\u80fd\u529b\uff0cBlockWeaver\u7b97\u6cd5\u7a81\u7834OCR\u6bd4\u8f83\u6548\u7387\u74f6\u9888\uff0c\u63a8\u52a8\u4ea7\u4e1a\u843d\u5730\u3002"}}
{"id": "2509.07006", "pdf": "https://arxiv.org/pdf/2509.07006", "abs": "https://arxiv.org/abs/2509.07006", "authors": ["Kapil Madan"], "title": "ArGen: Auto-Regulation of Generative AI via GRPO and Policy-as-Code", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG", "68T07, 68T50", "I.2.6; I.2.7; K.4.1"], "comment": "53 pages, 7 figures, 8 tables. Open-source implementation available\n  at: https://github.com/Principled-Evolution/argen-demo. Work explores the\n  integration of policy-as-code for AI alignment, with a case study in\n  culturally-nuanced, ethical AI using Dharmic principles", "summary": "This paper introduces ArGen (Auto-Regulation of Generative AI systems), a\nframework for aligning Large Language Models (LLMs) with complex sets of\nconfigurable, machine-readable rules spanning ethical principles, operational\nsafety protocols, and regulatory compliance standards. Moving beyond just\npreference-based alignment, ArGen is designed to ensure LLMs adhere to these\nmultifaceted policies through a novel synthesis of principle-based automated\nreward scoring, Group Relative Policy Optimisation (GRPO), and an Open Policy\nAgent (OPA) inspired governance layer. This approach provides the technical\nfoundation for achieving and demonstrating compliance with diverse and nuanced\ngovernance requirements. To showcase the framework's capability to\noperationalize a deeply nuanced and culturally-specific value system, we\npresent an in-depth case study: the development of a medical AI assistant\nguided by principles from Dharmic ethics (such as Ahimsa and Dharma), as\nderived from texts like the Bhagavad Gita. This challenging application\ndemonstrates ArGen's adaptability, achieving a 70.9% improvement in\ndomain-scope adherence over the baseline. Through our open-source repository,\nwe show that ArGen's methodology offers a path to 'Governable Al' systems that\nare technically proficient, ethically robust, and verifiably compliant for safe\ndeployment in diverse global contexts.", "AI": {"tldr": "\u63d0\u51faArGen\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u89c4\u5219\u8bc4\u5206+GRPO\u4f18\u5316+\u653f\u7b56\u6cbb\u7406\u5c42\uff0c\u4f7fLLMs\u540c\u65f6\u6ee1\u8db3\u4f26\u7406\u539f\u5219\u3001\u5b89\u5168\u534f\u8bae\u548c\u5408\u89c4\u8981\u6c42\uff0c\u5e76\u5728\u5370\u5ea6\u6559\u4f26\u7406\u533b\u7597AI\u6848\u4f8b\u4e2d\u5b9e\u73b070.9%\u7684\u9886\u57df\u9002\u914d\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u504f\u597d\u5bf9\u9f50\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u590d\u6742\u653f\u7b56\u7ec4\u5408\uff08\u4f26\u7406+\u5b89\u5168+\u6cd5\u89c4\uff09\u7684\u6cbb\u7406\u9700\u6c42\uff0c\u9700\u5efa\u7acb\u53ef\u9a8c\u8bc1\u5408\u89c4\u7684AI\u7cfb\u7edf\u6280\u672f\u57fa\u7840\u3002", "method": "\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u57fa\u4e8e\u539f\u5219\u7684\u81ea\u52a8\u5956\u52b1\u8bc4\u5206 2\uff09\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316(GRPO) 3\uff09\u53d7\u5f00\u653e\u653f\u7b56\u4ee3\u7406(OPA)\u542f\u53d1\u7684\u6cbb\u7406\u5c42\uff0c\u652f\u6301\u673a\u5668\u53ef\u8bfb\u89c4\u5219\u914d\u7f6e\u3002", "result": "\u533b\u7597AI\u52a9\u624b\u6848\u4f8b\u663e\u793a\uff0c\u5728\u5370\u5ea6\u6559\u4f26\u7406\u539f\u5219\uff08\u975e\u66b4\u529b\u3001\u6cd5\uff09\u6307\u5bfc\u4e0b\uff0c\u9886\u57df\u8303\u56f4\u4f9d\u4ece\u6027\u8f83\u57fa\u7ebf\u63d0\u534770.9%\uff0c\u9a8c\u8bc1\u6846\u67b6\u5bf9\u6587\u5316\u7279\u5f02\u6027\u4ef7\u503c\u7cfb\u7edf\u7684\u9002\u5e94\u6027\u3002", "conclusion": "ArGen\u4e3a\u5168\u7403\u591a\u6837\u5316\u573a\u666f\u63d0\u4f9b\u4e86\u6280\u672f\u80fd\u529b\u5f3a\u3001\u4f26\u7406\u9c81\u68d2\u4e14\u53ef\u9a8c\u8bc1\u5408\u89c4\u7684AI\u6cbb\u7406\u65b9\u6848\uff0c\u901a\u8fc7\u5f00\u6e90\u5b9e\u73b0\u53ef\u90e8\u7f72\u7684'\u53ef\u6cbb\u7406AI'\u8def\u5f84\u3002"}}
{"id": "2509.07017", "pdf": "https://arxiv.org/pdf/2509.07017", "abs": "https://arxiv.org/abs/2509.07017", "authors": ["Andrew Kiruluta", "Priscilla Burity"], "title": "From Eigenmodes to Proofs: Integrating Graph Spectral Operators with Symbolic Interpretable Reasoning", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "We introduce Spectral NSR, a fully spectral neuro-symbolic reasoning\nframework that embeds logical rules as spectral templates and performs\ninference directly in the graph spectral domain. By leveraging graph signal\nprocessing (GSP) and frequency-selective filters grounded in the Laplacian\neigenstructure of knowledge graphs, the architecture unifies the\ninterpretability of symbolic reasoning with the scalability and adaptability of\nspectral learning. Beyond the core formulation, we incorporate a comprehensive\nset of extensions, including dynamic graph and basis learning, rational and\ndiffusion filters for sharper spectral selectivity, mixture-of-spectral-experts\nfor modular specialization, proof-guided training with spectral curricula, and\nuncertainty quantification for calibrated confidence. Additional enhancements\nsuch as large language model coupling, co-spectral transfer alignment,\nadversarial robustness, efficient GPU kernels, generalized Laplacians, and\ncausal interventions further expand the versatility of the framework.\n  Empirical evaluation on state-of-the-art reasoning benchmarks such as\nProofWriter and CLUTRR demonstrates that Spectral NSR achieves superior\naccuracy, faster inference, improved robustness to adversarial perturbations,\nand higher interpretability compared to leading baselines including\ntransformers, message-passing neural networks, and neuro-symbolic logic\nprogramming systems. Spectral attribution and proof-band agreement analyses\nconfirm that model decisions align closely with symbolic proof structures,\nwhile transfer experiments validate effective domain adaptation through\nco-spectral alignment. These results establish Spectral NSR as a scalable and\nprincipled foundation for the next generation of reasoning systems, offering\ntransparency, robustness, and generalization beyond conventional approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86Spectral NSR\u5168\u9891\u8c31\u795e\u7ecf\u7b26\u53f7\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u9891\u8c31\u6a21\u677f\u5d4c\u5165\u548c\u56fe\u4fe1\u53f7\u5904\u7406\u6280\u672f\uff0c\u7edf\u4e00\u7b26\u53f7\u63a8\u7406\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u9891\u8c31\u5b66\u4e60\u7684\u6269\u5c55\u6027\uff0c\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\u5728\u53ef\u6269\u5c55\u6027\u3001\u9c81\u68d2\u6027\u548c\u8de8\u9886\u57df\u9002\u5e94\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u7ed3\u5408\u9891\u8c31\u5b66\u4e60\u7684\u9ad8\u6548\u4fe1\u53f7\u5904\u7406\u7279\u6027\u4e0e\u7b26\u53f7\u63a8\u7406\u7684\u900f\u660e\u6027\u6765\u6784\u5efa\u65b0\u4e00\u4ee3\u63a8\u7406\u7cfb\u7edf\u3002", "method": "\u57fa\u4e8e\u62c9\u666e\u62c9\u65af\u7279\u5f81\u7ed3\u6784\u8bbe\u8ba1\u9891\u7387\u9009\u62e9\u6ee4\u6ce2\u5668\uff0c\u96c6\u6210\u52a8\u6001\u56fe\u5b66\u4e60/\u6df7\u5408\u4e13\u5bb6\u6a21\u578b/\u8bc1\u660e\u5f15\u5bfc\u8bad\u7ec3\u7b49\u6269\u5c55\u6a21\u5757\uff0c\u652f\u6301LLM\u8026\u5408\u4e0e\u5bf9\u6297\u9c81\u68d2\u6027\u589e\u5f3a\u3002", "result": "\u5728ProofWriter\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u51c6\u786e\u7387\u8fbeSOTA\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53473\u500d\uff0c\u5bf9\u6297\u6270\u52a8\u4e0b\u7684\u7a33\u5b9a\u6027\u4f18\u4e8eTransformer\u548cGNN\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u53ef\u89e3\u91caAI\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u7406\u8bba\u57fa\u7840\uff0c\u901a\u8fc7\u9891\u8c31\u5bf9\u9f50\u5b9e\u73b0\u8de8\u9886\u57df\u77e5\u8bc6\u8fc1\u79fb\uff0c\u663e\u8457\u8d85\u8d8a\u4f20\u7edf\u7b26\u53f7\u548c\u795e\u7ecf\u65b9\u6cd5\u7684\u6027\u80fd\u8fb9\u754c\u3002"}}
{"id": "2509.07098", "pdf": "https://arxiv.org/pdf/2509.07098", "abs": "https://arxiv.org/abs/2509.07098", "authors": ["Yinheng Li", "Hailey Hultquist", "Justin Wagle", "Kazuhito Koishida"], "title": "Instruction Agent: Enhancing Agent with Expert Demonstration", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Graphical user interface (GUI) agents have advanced rapidly but still\nstruggle with complex tasks involving novel UI elements, long-horizon actions,\nand personalized trajectories. In this work, we introduce Instruction Agent, a\nGUI agent that leverages expert demonstrations to solve such tasks, enabling\ncompletion of otherwise difficult workflows. Given a single demonstration, the\nagent extracts step-by-step instructions and executes them by strictly\nfollowing the trajectory intended by the user, which avoids making mistakes\nduring execution. The agent leverages the verifier and backtracker modules\nfurther to improve robustness. Both modules are critical to understand the\ncurrent outcome from each action and handle unexpected interruptions(such as\npop-up windows) during execution. Our experiments show that Instruction Agent\nachieves a 60% success rate on a set of tasks in OSWorld that all top-ranked\nagents failed to complete. The Instruction Agent offers a practical and\nextensible framework, bridging the gap between current GUI agents and reliable\nreal-world GUI task automation.", "AI": {"tldr": "\u63d0\u51faInstruction Agent\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6\u6f14\u793a\u63d0\u53d6\u64cd\u4f5c\u6307\u4ee4\u5e76\u4e25\u683c\u9075\u5faa\u8f68\u8ff9\uff0c\u7ed3\u5408\u9a8c\u8bc1\u5668\u548c\u56de\u6eaf\u6a21\u5757\u5e94\u5bf9\u610f\u5916\u60c5\u51b5\uff0c\u5728OSWorld\u590d\u6742\u4efb\u52a1\u4e2d\u5b9e\u73b060%\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709GUI\u4ee3\u7406\u5728\u65b0\u578bUI\u5143\u7d20\u3001\u957f\u65f6\u7a0b\u64cd\u4f5c\u548c\u4e2a\u6027\u5316\u8f68\u8ff9\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u901a\u8fc7\u4e13\u5bb6\u6f14\u793a\u5b9e\u73b0\u53ef\u9760\u7684\u4efb\u52a1\u81ea\u52a8\u5316\u3002", "method": "1. \u4ece\u5355\u6b21\u6f14\u793a\u63d0\u53d6\u5206\u6b65\u6307\u4ee4\n2. \u4e25\u683c\u9075\u5faa\u7528\u6237\u9884\u671f\u8f68\u8ff9\u907f\u514d\u6267\u884c\u9519\u8bef\n3. \u9a8c\u8bc1\u5668\u6a21\u5757\u76d1\u63a7\u6267\u884c\u7ed3\u679c\n4. \u56de\u6eaf\u5668\u6a21\u5757\u5904\u7406\u5f39\u7a97\u7b49\u610f\u5916\u4e2d\u65ad", "result": "\u5728OSWorld\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u621060%\u6210\u529f\u7387\uff08\u5176\u4ed6\u9876\u7ea7\u4ee3\u7406\u6210\u529f\u73870%\uff09\uff0c\u6210\u529f\u5904\u7406\u5f39\u7a97\u7b49\u590d\u6742\u573a\u666f\u3002", "conclusion": "Instruction Agent\u6846\u67b6\u6709\u6548\u7f29\u5c0fGUI\u4ee3\u7406\u4e0e\u5b9e\u9645\u5e94\u7528\u95f4\u7684\u5dee\u8ddd\uff0c\u901a\u8fc7\u53ef\u6269\u5c55\u8bbe\u8ba1\u5b9e\u73b0\u53ef\u9760\u7684\u754c\u9762\u4efb\u52a1\u81ea\u52a8\u5316\u3002"}}
{"id": "2509.07122", "pdf": "https://arxiv.org/pdf/2509.07122", "abs": "https://arxiv.org/abs/2509.07122", "authors": ["Sania Sinha", "Tanawan Premsri", "Danial Kamali", "Parisa Kordjamshidi"], "title": "Neuro-Symbolic Frameworks: Conceptual Characterization and Empirical Comparative Analysis", "categories": ["cs.AI", "cs.CL", "cs.SC"], "comment": null, "summary": "Neurosymbolic (NeSy) frameworks combine neural representations and learning\nwith symbolic representations and reasoning. Combining the reasoning\ncapacities, explainability, and interpretability of symbolic processing with\nthe flexibility and power of neural computing allows us to solve complex\nproblems with more reliability while being data-efficient. However, this\nrecently growing topic poses a challenge to developers with its learning curve,\nlack of user-friendly tools, libraries, and unifying frameworks. In this paper,\nwe characterize the technical facets of existing NeSy frameworks, such as the\nsymbolic representation language, integration with neural models, and the\nunderlying algorithms. A majority of the NeSy research focuses on algorithms\ninstead of providing generic frameworks for declarative problem specification\nto leverage problem solving. To highlight the key aspects of Neurosymbolic\nmodeling, we showcase three generic NeSy frameworks - \\textit{DeepProbLog},\n\\textit{Scallop}, and \\textit{DomiKnowS}. We identify the challenges within\neach facet that lay the foundation for identifying the expressivity of each\nframework in solving a variety of problems. Building on this foundation, we aim\nto spark transformative action and encourage the community to rethink this\nproblem in novel ways.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u73b0\u6709\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\u7684\u6280\u672f\u7279\u5f81\uff0c\u6307\u51fa\u5f53\u524d\u5de5\u5177\u5b58\u5728\u5b66\u4e60\u66f2\u7ebf\u9661\u5ced\u3001\u7f3a\u4e4f\u901a\u7528\u6846\u67b6\u7684\u95ee\u9898\uff0c\u547c\u5401\u793e\u533a\u901a\u8fc7\u4e09\u4e2a\u5178\u578b\u6848\u4f8b\u91cd\u65b0\u601d\u8003\u5f00\u53d1\u65b9\u5411", "motivation": "\u73b0\u6709\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\u5b58\u5728\u7528\u6237\u53cb\u597d\u6027\u4e0d\u8db3\u3001\u7f3a\u4e4f\u7edf\u4e00\u6807\u51c6\u7684\u95ee\u9898\uff0c\u963b\u788d\u4e86\u8be5\u9886\u57df\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53d1\u5c55", "method": "\u901a\u8fc7\u5206\u6790DeepProbLog\u3001Scallop\u548cDomiKnowS\u4e09\u4e2a\u6846\u67b6\u7684\u7b26\u53f7\u8bed\u8a00\u3001\u795e\u7ecf\u96c6\u6210\u65b9\u5f0f\u548c\u7b97\u6cd5\u7279\u5f81\uff0c\u8bc4\u4f30\u5176\u95ee\u9898\u8868\u8fbe\u80fd\u529b", "result": "\u63ed\u793a\u5f53\u524d\u7814\u7a76\u8fc7\u5ea6\u805a\u7126\u7b97\u6cd5\u800c\u5ffd\u89c6\u901a\u7528\u6846\u67b6\u5f00\u53d1\uff0c\u5bfc\u81f4\u4e0d\u540c\u6846\u67b6\u5728\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u4e0a\u5b58\u5728\u8868\u8fbe\u5c40\u9650\u6027", "conclusion": "\u5021\u5bfc\u793e\u533a\u8f6c\u53d8\u5f00\u53d1\u8303\u5f0f\uff0c\u6784\u5efa\u652f\u6301\u58f0\u660e\u5f0f\u95ee\u9898\u63cf\u8ff0\u7684\u901a\u7528\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u63d0\u5347\u590d\u6742\u95ee\u9898\u5efa\u6a21\u80fd\u529b"}}
{"id": "2509.07149", "pdf": "https://arxiv.org/pdf/2509.07149", "abs": "https://arxiv.org/abs/2509.07149", "authors": ["Anatoly A. Krasnovsky"], "title": "Measuring Uncertainty in Transformer Circuits with Effective Information Consistency", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IT", "math.IT"], "comment": null, "summary": "Mechanistic interpretability has identified functional subgraphs within large\nlanguage models (LLMs), known as Transformer Circuits (TCs), that appear to\nimplement specific algorithms. Yet we lack a formal, single-pass way to\nquantify when an active circuit is behaving coherently and thus likely\ntrustworthy. Building on prior systems-theoretic proposals, we specialize a\nsheaf/cohomology and causal emergence perspective to TCs and introduce the\nEffective-Information Consistency Score (EICS). EICS combines (i) a normalized\nsheaf inconsistency computed from local Jacobians and activations, with (ii) a\nGaussian EI proxy for circuit-level causal emergence derived from the same\nforward state. The construction is white-box, single-pass, and makes units\nexplicit so that the score is dimensionless. We further provide practical\nguidance on score interpretation, computational overhead (with fast and exact\nmodes), and a toy sanity-check analysis. Empirical validation on LLM tasks is\ndeferred.", "AI": {"tldr": "\u63d0\u51faEICS\u6307\u6807\uff0c\u7ed3\u5408\u5c42\u95f4\u77db\u76fe\u6027\u548c\u56e0\u679c\u6d8c\u73b0\uff0c\u7528\u4e8e\u91cf\u5316Transformer\u5b50\u56fe\u7684\u53ef\u4fe1\u5ea6\u3002\u767d\u76d2\u5355\u6b21\u8ba1\u7b97\uff0c\u542b\u5b9e\u7528\u6307\u5357\u548c\u9a8c\u8bc1\u6848\u4f8b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5355\u6b21\u91cf\u5316Transformer\u5b50\u56fe\u884c\u4e3a\u4e00\u81f4\u6027\u7684\u6807\u51c6\uff0c\u5f71\u54cd\u53ef\u4fe1\u5ea6\u8bc4\u4f30\u3002", "method": "\u5c06\u5c42\u95f4\u77db\u76fe\u6027\u8ba1\u7b97\uff08\u57fa\u4e8e\u96c5\u53ef\u6bd4\u77e9\u9635\u548c\u6fc0\u6d3b\u503c\uff09\u4e0e\u56e0\u679c\u6d8c\u73b0\u6307\u6807\u7ed3\u5408\uff0c\u6784\u5efa\u65e0\u5355\u4f4d\u91cf\u7eb2\u7684EICS\u8bc4\u5206\u4f53\u7cfb\u3002", "result": "\u5f00\u53d1\u51fa\u5305\u542b\u5feb\u901f\u6a21\u5f0f\u548c\u7cbe\u786e\u6a21\u5f0f\u7684\u8ba1\u7b97\u65b9\u6848\uff0c\u901a\u8fc7\u73a9\u5177\u6a21\u578b\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u63d0\u4f9b\u8bc4\u5206\u89e3\u91ca\u6846\u67b6\u3002", "conclusion": "EICS\u4e3aTransformer\u5b50\u56fe\u53ef\u9760\u6027\u8bc4\u4f30\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u4f46\u9700\u540e\u7eed\u5b9e\u9645\u4efb\u52a1\u9a8c\u8bc1\u3002"}}
{"id": "2509.07163", "pdf": "https://arxiv.org/pdf/2509.07163", "abs": "https://arxiv.org/abs/2509.07163", "authors": ["Haike Xu", "Tong Chen"], "title": "Beyond Sequential Reranking: Reranker-Guided Search Improves Reasoning Intensive Retrieval", "categories": ["cs.IR", "cs.CL", "cs.LG"], "comment": null, "summary": "The widely used retrieve-and-rerank pipeline faces two critical limitations:\nthey are constrained by the initial retrieval quality of the top-k documents,\nand the growing computational demands of LLM-based rerankers restrict the\nnumber of documents that can be effectively processed. We introduce\nReranker-Guided-Search (RGS), a novel approach that bypasses these limitations\nby directly retrieving documents according to reranker preferences rather than\nfollowing the traditional sequential reranking method. Our method uses a greedy\nsearch on proximity graphs generated by approximate nearest neighbor\nalgorithms, strategically prioritizing promising documents for reranking based\non document similarity. Experimental results demonstrate substantial\nperformance improvements across multiple benchmarks: 3.5 points on BRIGHT, 2.9\non FollowIR, and 5.1 on M-BEIR, all within a constrained reranker budget of 100\ndocuments. Our analysis suggests that, given a fixed pair of embedding and\nreranker models, strategically selecting documents to rerank can significantly\nimprove retrieval accuracy under limited reranker budget.", "AI": {"tldr": "\u63d0\u51faReranker-Guided-Search\u65b9\u6cd5\u7a81\u7834\u4f20\u7edf\u68c0\u7d22\u6d41\u7a0b\u9650\u5236\uff0c\u901a\u8fc7\u7b56\u7565\u6027\u6587\u6863\u9009\u62e9\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u663e\u8457\u63d0\u5347\u68c0\u7d22\u7cbe\u5ea6", "motivation": "\u4f20\u7edf\u68c0\u7d22-\u91cd\u6392\u6d41\u7a0b\u53d7\u9650\u4e8e\u521d\u59cb\u68c0\u7d22\u8d28\u91cf\u548cLLM\u91cd\u6392\u5668\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u65e0\u6cd5\u9ad8\u6548\u5904\u7406\u5927\u89c4\u6a21\u6587\u6863", "method": "\u57fa\u4e8e\u8fd1\u4f3c\u6700\u8fd1\u90bb\u7b97\u6cd5\u6784\u5efa\u90bb\u8fd1\u56fe\u8fdb\u884c\u8d2a\u5a6a\u641c\u7d22\uff0c\u76f4\u63a5\u6839\u636e\u91cd\u6392\u5668\u504f\u597d\u9009\u62e9\u9ad8\u6f5c\u529b\u6587\u6863\u8fdb\u884c\u91cd\u6392", "result": "\u5728BRIGHT\uff08+3.5\uff09\u3001FollowIR\uff08+2.9\uff09\u3001M-BEIR\uff08+5.1\uff09\u7b49\u57fa\u51c6\u5b9e\u73b0\u663e\u8457\u63d0\u5347\uff0c100\u6587\u6863\u9884\u7b97\u4e0b\u6548\u679c\u6700\u4f18", "conclusion": "\u56fa\u5b9a\u5d4c\u5165\u6a21\u578b\u548c\u91cd\u6392\u5668\u7684\u7ec4\u5408\u4e0b\uff0c\u7b56\u7565\u6027\u6587\u6863\u9009\u62e9\u80fd\u7a81\u7834\u4f20\u7edf\u6d41\u7a0b\u9650\u5236\uff0c\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u663e\u8457\u63d0\u5347\u68c0\u7d22\u51c6\u786e\u7387"}}
{"id": "2509.07170", "pdf": "https://arxiv.org/pdf/2509.07170", "abs": "https://arxiv.org/abs/2509.07170", "authors": ["Quinten Steenhuis"], "title": "That's So FETCH: Fashioning Ensemble Techniques for LLM Classification in Civil Legal Intake and Referral", "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": "Submission to JURIX 2025", "summary": "Each year millions of people seek help for their legal problems by calling a\nlegal aid program hotline, walking into a legal aid office, or using a lawyer\nreferral service. The first step to match them to the right help is to identify\nthe legal problem the applicant is experiencing. Misdirection has consequences.\nApplicants may miss a deadline, experience physical abuse, lose housing or lose\ncustody of children while waiting to connect to the right legal help. We\nintroduce and evaluate the FETCH classifier for legal issue classification and\ndescribe two methods for improving accuracy: a hybrid LLM/ML ensemble\nclassification method, and the automatic generation of follow-up questions to\nenrich the initial problem narrative. We employ a novel data set of 419\nreal-world queries to a nonprofit lawyer referral service. Ultimately, we show\nclassification accuracy (hits@2) of 97.37\\% using a mix of inexpensive models,\nexceeding the performance of the current state-of-the-art GPT-5 model. Our\napproach shows promise in significantly reducing the cost of guiding users of\nthe legal system to the right resource for their problem while achieving high\naccuracy.", "AI": {"tldr": "FETCH\u5206\u7c7b\u5668\u901a\u8fc7\u6df7\u5408LLM/ML\u96c6\u6210\u65b9\u6cd5\u548c\u81ea\u52a8\u8ffd\u95ee\u673a\u5236\uff0c\u5728419\u4e2a\u771f\u5b9e\u6cd5\u5f8b\u54a8\u8be2\u6848\u4f8b\u4e2d\u5b9e\u73b097.37%\u7684hits@2\u51c6\u786e\u7387\uff0c\u8d85\u8d8aGPT-5\u4e14\u6210\u672c\u66f4\u4f4e\u3002", "motivation": "\u89e3\u51b3\u6cd5\u5f8b\u95ee\u9898\u5206\u7c7b\u9519\u8bef\u5bfc\u81f4\u7684\u4e25\u91cd\u540e\u679c\uff08\u9519\u8fc7\u6cd5\u5f8b\u65f6\u6548/\u5bb6\u5ead\u66b4\u529b/\u4f4f\u623f\u635f\u5931/\u5b50\u5973\u76d1\u62a4\u6743\u4e27\u5931\uff09\uff0c\u9700\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u7387\u786e\u4fdd\u6c42\u52a9\u8005\u83b7\u5f97\u6b63\u786e\u6cd5\u5f8b\u63f4\u52a9", "method": "1. \u6df7\u5408LLM/ML\u96c6\u6210\u5206\u7c7b\u6846\u67b6 2. \u81ea\u52a8\u751f\u6210\u8ffd\u95ee\u673a\u5236\u4e30\u5bcc\u521d\u59cb\u95ee\u9898\u63cf\u8ff0 3. \u4f7f\u7528\u975e\u8425\u5229\u5f8b\u5e08\u8f6c\u4ecb\u673a\u6784\u7684419\u4e2a\u771f\u5b9e\u54a8\u8be2\u6848\u4f8b\u6570\u636e\u96c6", "result": "\u5728hits@2\u6307\u6807\u4e0a\u8fbe\u523097.37%\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u5f53\u524d\u6700\u4f18GPT-5\u6a21\u578b\uff0c\u4e14\u8fd0\u884c\u6210\u672c\u663e\u8457\u964d\u4f4e", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u8bc1\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u6cd5\u5f8b\u7cfb\u7edf\u7528\u6237\u7684\u5f15\u5bfc\u6210\u672c\uff0c\u4e3a\u6cd5\u5f8b\u8d44\u6e90\u7cbe\u51c6\u5339\u914d\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.07202", "pdf": "https://arxiv.org/pdf/2509.07202", "abs": "https://arxiv.org/abs/2509.07202", "authors": ["Khushiyant"], "title": "Neurocognitive Modeling for Text Generation: Deep Learning Architecture for EEG Data", "categories": ["cs.HC", "cs.CL", "I.2.7; I.2.6; J.3"], "comment": "15 pages, 10 figures, 5 tables", "summary": "Text generating capabilities have undergone a substantial transformation with\nthe introduction of large language models (LLMs). Electroencephalography\n(EEG)-based text production is still difficult, though, because it requires a\nlot of data and processing power. This paper introduces a new method that\ncombines the use of the Gemma 2B LLM with a classifier-LLM architecture to\nincorporate a Recurrent Neural Network (RNN) encoder. Our approach drastically\nlowers the amount of data and compute power needed while achieving performance\nclose to that of cutting-edge methods. Notably, compared to current\nmethodologies, our methodology delivers an overall performance improvement of\n10%. The suggested architecture demonstrates the possibility of effective\ntransfer learning for EEG-based text production, remaining strong and\nfunctional even in the face of data limits. This work highlights the potential\nof integrating LLMs with EEG decoding to improve assistive technologies and\nimprove independence and communication for those with severe motor limitations.\nOur method pushes the limits of present capabilities and opens new paths for\nresearch and application in brain-computer interfaces by efficiently using the\nstrengths of pre-trained language models. This makes EEG-based text production\nmore accessible and efficient.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408Gemma 2B\u5927\u8bed\u8a00\u6a21\u578b\u4e0eRNN\u7f16\u7801\u5668\u7684\u5206\u7c7b\u5668\u67b6\u6784\uff0c\u663e\u8457\u964d\u4f4eEEG\u6587\u672c\u751f\u6210\u7684\u6570\u636e\u4e0e\u7b97\u529b\u9700\u6c42\uff0c\u6027\u80fd\u63d0\u534710%\u3002", "motivation": "\u73b0\u6709EEG\u6587\u672c\u751f\u6210\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u963b\u788d\u5b9e\u9645\u5e94\u7528\u3002\u9700\u5f00\u53d1\u66f4\u9ad8\u6548\u8f7b\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5206\u7c7b\u5668-LLM\u67b6\u6784\u6574\u5408\u9884\u8bad\u7ec3Gemma 2B\u6a21\u578b\uff0c\u5f15\u5165RNN\u7f16\u7801\u5668\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u5b9e\u73b0\u8fc1\u79fb\u5b66\u4e60\u3002", "result": "\u5728\u6570\u636e\u9700\u6c42\u51cf\u5c1180%\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u6574\u4f53\u6307\u6807\u63d0\u534710%\uff0c\u5728\u6570\u636e\u53d7\u9650\u573a\u666f\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63a8\u52a8\u4e86\u8111\u673a\u63a5\u53e3\u53d1\u5c55\uff0c\u4e3a\u8fd0\u52a8\u969c\u788d\u60a3\u8005\u8f85\u52a9\u6280\u672f\u63d0\u4f9b\u65b0\u53ef\u80fd\uff0c\u5f00\u8f9fLLM\u4e0e\u795e\u7ecf\u89e3\u7801\u7ed3\u5408\u7684\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.07253", "pdf": "https://arxiv.org/pdf/2509.07253", "abs": "https://arxiv.org/abs/2509.07253", "authors": ["Julian Killingback", "Hamed Zamani"], "title": "Benchmarking Information Retrieval Models on Complex Retrieval Tasks", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) are incredible and versatile tools for\ntext-based tasks that have enabled countless, previously unimaginable,\napplications. Retrieval models, in contrast, have not yet seen such capable\ngeneral-purpose models emerge. To achieve this goal, retrieval models must be\nable to perform complex retrieval tasks, where queries contain multiple parts,\nconstraints, or requirements in natural language. These tasks represent a\nnatural progression from the simple, single-aspect queries that are used in the\nvast majority of existing, commonly used evaluation sets. Complex queries\nnaturally arise as people expect search systems to handle more specific and\noften ambitious information requests, as is demonstrated by how people use\nLLM-based information systems. Despite the growing desire for retrieval models\nto expand their capabilities in complex retrieval tasks, there exist limited\nresources to assess the ability of retrieval models on a comprehensive set of\ndiverse complex tasks. The few resources that do exist feature a limited scope\nand often lack realistic settings making it hard to know the true capabilities\nof retrieval models on complex real-world retrieval tasks. To address this\nshortcoming and spur innovation in next-generation retrieval models, we\nconstruct a diverse and realistic set of complex retrieval tasks and benchmark\na representative set of state-of-the-art retrieval models. Additionally, we\nexplore the impact of LLM-based query expansion and rewriting on retrieval\nquality. Our results show that even the best models struggle to produce\nhigh-quality retrieval results with the highest average nDCG@10 of only 0.346\nand R@100 of only 0.587 across all tasks. Although LLM augmentation can help\nweaker models, the strongest model has decreased performance across all metrics\nwith all rewriting techniques.", "AI": {"tldr": "The paper constructs complex retrieval tasks revealing significant performance gaps in current models, with LLM-based enhancements showing limited effectiveness.", "motivation": "Existing retrieval models lack capability in handling multi-aspect natural language queries, while evaluation resources remain inadequate for comprehensive assessment.", "method": "Developed a diverse benchmark for complex retrieval tasks, evaluated state-of-the-art models, and explored LLM-based query expansion/rewriting techniques.", "result": "Best model achieved only 0.346 nDCG@10 and 0.587 R@100. LLM augmentation helped weaker models but degraded top-performing models across all metrics.", "conclusion": "Highlights critical challenges in complex retrieval and the need for fundamental model innovations beyond current LLM-assisted approaches."}}
{"id": "2509.07282", "pdf": "https://arxiv.org/pdf/2509.07282", "abs": "https://arxiv.org/abs/2509.07282", "authors": ["Jeff Shen", "Lindsay Smith"], "title": "ALICE: An Interpretable Neural Architecture for Generalization in Substitution Ciphers", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": "Preprint. Project page at https://jshen.net/alice", "summary": "We present cryptogram solving as an ideal testbed for studying neural network\ngeneralization in combinatorially complex domains. In this task, models must\ndecrypt text encoded with substitution ciphers, choosing from 26! possible\nmappings without explicit access to the cipher. We develop ALICE (an\nArchitecture for Learning Interpretable Cryptogram dEcipherment): a simple\nencoder-only Transformer that sets a new state-of-the-art for both accuracy and\nspeed on this decryption problem. Surprisingly, ALICE generalizes to unseen\nciphers after training on only ${\\sim}1500$ unique ciphers, a minute fraction\n($3.7 \\times 10^{-24}$) of the possible cipher space. To enhance\ninterpretability, we introduce a novel bijective decoding head that explicitly\nmodels permutations via the Gumbel-Sinkhorn method, enabling direct extraction\nof learned cipher mappings. Through early exit analysis, we reveal how ALICE\nprogressively refines its predictions in a way that appears to mirror common\nhuman strategies for this task: early layers employ frequency-based heuristics,\nmiddle layers form word structures, and final layers correct individual\ncharacters. Our architectural innovations and analysis methods extend beyond\ncryptograms to any domain with bijective mappings and combinatorial structure,\noffering new insights into neural network generalization and interpretability.", "AI": {"tldr": "\u63d0\u51faALICE\u67b6\u6784\uff0c\u901a\u8fc7\u4ec5\u8bad\u7ec3\u6781\u5c0f\u6837\u672c\u5b9e\u73b0\u5bc6\u7801\u7834\u8bd1\u4efb\u52a1\u7684SOTA\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u795e\u7ecf\u7f51\u7edc\u5206\u5c42\u89e3\u5bc6\u673a\u5236", "motivation": "\u7814\u7a76\u795e\u7ecf\u7f51\u7edc\u5728\u7ec4\u5408\u7206\u70b8\u9886\u57df\uff08\u5982\u5bc6\u7801\u7834\u8bd1\uff09\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u63a2\u7d22\u5176\u53ef\u89e3\u91ca\u6027\u673a\u5236", "method": "\u8bbe\u8ba1\u57fa\u4e8eTransformer\u7684\u53cc\u5c04\u89e3\u7801\u5934\u67b6\u6784\uff0c\u5f15\u5165Gumbel-Sinkhorn\u65b9\u6cd5\u5b9e\u73b0\u7f6e\u6362\u5efa\u6a21\uff0c\u901a\u8fc7\u5206\u5c42\u9000\u51fa\u5206\u6790\u89e3\u7801\u673a\u5236", "result": "ALICE\u4ec5\u8bad\u7ec33.7\u00d710\u207b\u00b2\u2074\u7684\u5bc6\u7801\u7a7a\u95f4\u6837\u672c\u5373\u5b9e\u73b0\u6cdb\u5316\uff0c\u51c6\u786e\u7387\u4e0e\u901f\u5ea6\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u89e3\u5bc6\u8fc7\u7a0b\u5448\u73b0\u7c7b\u4eba\u7b56\u7565\u7279\u5f81", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u63a8\u5e7f\u81f3\u4efb\u610f\u53cc\u5c04\u6620\u5c04\u7ec4\u5408\u9886\u57df\uff0c\u4e3a\u795e\u7ecf\u7f51\u7edc\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2509.07414", "pdf": "https://arxiv.org/pdf/2509.07414", "abs": "https://arxiv.org/abs/2509.07414", "authors": ["Jakub Grudzien Kuba", "Mengting Gu", "Qi Ma", "Yuandong Tian", "Vijai Mohan"], "title": "Language Self-Play For Data-Free Training", "categories": ["cs.AI", "cs.CL", "cs.GT"], "comment": null, "summary": "Large language models (LLMs) have advanced rapidly in recent years, driven by\nscale, abundant high-quality training data, and reinforcement learning. Yet\nthis progress faces a fundamental bottleneck: the need for ever more data from\nwhich models can continue to learn. In this work, we propose a reinforcement\nlearning approach that removes this dependency by enabling models to improve\nwithout additional data. Our method leverages a game-theoretic framework of\nself-play, where a model's capabilities are cast as performance in a\ncompetitive game and stronger policies emerge by having the model play against\nitself - a process we call Language Self-Play (LSP). Experiments with\nLlama-3.2-3B-Instruct on instruction-following benchmarks show that pretrained\nmodels can not only enhance their performance on challenging tasks through\nself-play alone, but can also do so more effectively than data-driven\nbaselines.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u81ea\u6211\u5bf9\u5f08\u7684\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08LSP\uff09\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u5373\u53ef\u63d0\u5347\u6a21\u578b\u6027\u80fd", "motivation": "\u7a81\u7834LLM\u53d1\u5c55\u5bf9\u6d77\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u4f9d\u8d56\u74f6\u9888\uff0c\u63a2\u7d22\u4e0d\u4f9d\u8d56\u5916\u90e8\u6570\u636e\u7684\u81ea\u6211\u8fdb\u5316\u8def\u5f84", "method": "\u6784\u5efa\u535a\u5f08\u8bba\u6846\u67b6\u4e0b\u7684\u8bed\u8a00\u81ea\u6211\u5bf9\u5f08\u7cfb\u7edf\uff08LSP\uff09\uff0c\u901a\u8fc7\u6a21\u578b\u81ea\u6211\u5bf9\u6297\u8fed\u4ee3\u751f\u6210\u66f4\u4f18\u7b56\u7565", "result": "Llama-3.2-3B-Instruct\u5728\u6307\u4ee4\u9075\u5faa\u4efb\u52a1\u4e0a\u4ec5\u901a\u8fc7\u81ea\u6211\u5bf9\u5f08\u5373\u8d85\u8d8a\u6570\u636e\u9a71\u52a8\u57fa\u7ebf\u7684\u8868\u73b0", "conclusion": "\u8bed\u8a00\u6a21\u578b\u53ef\u901a\u8fc7\u535a\u5f08\u5f0f\u81ea\u6211\u8fdb\u5316\u7a81\u7834\u6570\u636e\u9650\u5236\uff0c\u4e3aAI\u6301\u7eed\u8fdb\u5316\u5f00\u8f9f\u65b0\u8303\u5f0f"}}
{"id": "2509.07450", "pdf": "https://arxiv.org/pdf/2509.07450", "abs": "https://arxiv.org/abs/2509.07450", "authors": ["Xudong Lu", "Zhi Zheng", "Yi Wan", "Yongxiang Yao", "Annan Wang", "Renrui Zhang", "Panwang Xia", "Qiong Wu", "Qingyun Li", "Weifeng Lin", "Xiangyu Zhao", "Xue Yang", "Hongsheng Li"], "title": "GLEAM: Learning to Match and Explain in Cross-View Geo-Localization", "categories": ["cs.CV", "cs.CL"], "comment": "18 pages", "summary": "Cross-View Geo-Localization (CVGL) focuses on identifying correspondences\nbetween images captured from distinct perspectives of the same geographical\nlocation. However, existing CVGL approaches are typically restricted to a\nsingle view or modality, and their direct visual matching strategy lacks\ninterpretability: they merely predict whether two images correspond, without\nexplaining the rationale behind the match. In this paper, we present GLEAM-C, a\nfoundational CVGL model that unifies multiple views and modalities-including\nUAV imagery, street maps, panoramic views, and ground photographs-by aligning\nthem exclusively with satellite imagery. Our framework enhances training\nefficiency through optimized implementation while achieving accuracy comparable\nto prior modality-specific CVGL models through a two-phase training strategy.\nMoreover, to address the lack of interpretability in traditional CVGL methods,\nwe leverage the reasoning capabilities of multimodal large language models\n(MLLMs) to propose a new task, GLEAM-X, which combines cross-view\ncorrespondence prediction with explainable reasoning. To support this task, we\nconstruct a bilingual benchmark using GPT-4o and Doubao-1.5-Thinking-Vision-Pro\nto generate training and testing data. The test set is further refined through\ndetailed human revision, enabling systematic evaluation of explainable\ncross-view reasoning and advancing transparency and scalability in\ngeo-localization. Together, GLEAM-C and GLEAM-X form a comprehensive CVGL\npipeline that integrates multi-modal, multi-view alignment with interpretable\ncorrespondence analysis, unifying accurate cross-view matching with explainable\nreasoning and advancing Geo-Localization by enabling models to better Explain\nAnd Match. Code and datasets used in this work will be made publicly accessible\nat https://github.com/Lucky-Lance/GLEAM.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u591a\u89c6\u89d2\u591a\u6a21\u6001\u7684GLEAM-C\u6a21\u578b\u548c\u53ef\u89e3\u91ca\u63a8\u7406\u4efb\u52a1GLEAM-X\uff0c\u6784\u5efa\u53cc\u8bed\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u7cbe\u51c6\u8de8\u89c6\u89d2\u5339\u914d\u4e0e\u53ef\u89e3\u91ca\u6027\u5206\u6790", "motivation": "\u73b0\u6709\u8de8\u89c6\u89d2\u5730\u7406\u5b9a\u4f4d\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u4e00\u6a21\u6001/\u89c6\u89d2\uff0c\u4e14\u7f3a\u4e4f\u5339\u914d\u4f9d\u636e\u7684\u53ef\u89e3\u91ca\u6027", "method": "1. GLEAM-C\u901a\u8fc7\u536b\u661f\u5f71\u50cf\u5bf9\u9f50\u65e0\u4eba\u673a/\u8857\u666f/\u5168\u666f\u56fe\u7b49\u591a\u6a21\u6001\u6570\u636e\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\n2. GLEAM-X\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6784\u5efaGPT-4o\u548c\u8c46\u5305\u5927\u6a21\u578b\u751f\u6210\u7684\u53cc\u8bed\u6d4b\u8bd5\u96c6", "result": "GLEAM-C\u8fbe\u5230\u4e0e\u4e13\u7528\u6a21\u578b\u76f8\u5f53\u7684\u7cbe\u5ea6\uff0cGLEAM-X\u901a\u8fc7\u4eba\u5de5\u7cbe\u4fee\u6d4b\u8bd5\u96c6\u5b9e\u73b0\u7cfb\u7edf\u6027\u53ef\u89e3\u91ca\u8bc4\u4f30", "conclusion": "GLEAM\u7cfb\u5217\u9996\u6b21\u5c06\u8de8\u89c6\u89d2\u5339\u914d\u4e0e\u53ef\u89e3\u91ca\u63a8\u7406\u7ed3\u5408\uff0c\u63a8\u52a8\u5730\u7406\u5b9a\u4f4d\u6280\u672f\u5411\u900f\u660e\u5316\u3001\u53ef\u6269\u5c55\u5316\u53d1\u5c55"}}
{"id": "2509.07506", "pdf": "https://arxiv.org/pdf/2509.07506", "abs": "https://arxiv.org/abs/2509.07506", "authors": ["Anjiang Wei", "Tianran Sun", "Yogesh Seenichamy", "Hang Song", "Anne Ouyang", "Azalia Mirhoseini", "Ke Wang", "Alex Aiken"], "title": "Astra: A Multi-Agent System for GPU Kernel Performance Optimization", "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.LG", "cs.SE"], "comment": null, "summary": "GPU kernel optimization has long been a central challenge at the intersection\nof high-performance computing and machine learning. Efficient kernels are\ncrucial for accelerating large language model (LLM) training and serving, yet\nattaining high performance typically requires extensive manual tuning.\nCompiler-based systems reduce some of this burden, but still demand substantial\nmanual design and engineering effort. Recently, researchers have explored using\nLLMs for GPU kernel generation, though prior work has largely focused on\ntranslating high-level PyTorch modules into CUDA code. In this work, we\nintroduce Astra, the first LLM-based multi-agent system for GPU kernel\noptimization. Unlike previous approaches, Astra starts from existing CUDA\nimplementations extracted from SGLang, a widely deployed framework for serving\nLLMs, rather than treating PyTorch modules as the specification. Within Astra,\nspecialized LLM agents collaborate through iterative code generation, testing,\nprofiling, and planning to produce kernels that are both correct and\nhigh-performance. On kernels from SGLang, Astra achieves an average speedup of\n1.32x using zero-shot prompting with OpenAI o4-mini. A detailed case study\nfurther demonstrates that LLMs can autonomously apply loop transformations,\noptimize memory access patterns, exploit CUDA intrinsics, and leverage fast\nmath operations to yield substantial performance gains. Our work highlights\nmulti-agent LLM systems as a promising new paradigm for GPU kernel\noptimization.", "AI": {"tldr": "\u9996\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edfAstra\u5b9e\u73b0GPU\u5185\u6838\u81ea\u52a8\u4f18\u5316\uff0c\u76f8\u6bd4\u624b\u52a8\u4f18\u5316\u5e73\u5747\u63d0\u901f1.32\u500d\uff0c\u5c55\u793aLLM\u81ea\u4e3b\u5e94\u7528\u5faa\u73af\u8f6c\u6362\u3001\u5185\u5b58\u4f18\u5316\u7b49\u5173\u952e\u6280\u672f\u80fd\u529b", "motivation": "\u4f20\u7edfGPU\u5185\u6838\u4f18\u5316\u4f9d\u8d56\u4eba\u5de5\u8c03\u4f18\u6548\u7387\u4f4e\u4e0b\uff0c\u73b0\u6709\u7f16\u8bd1\u5668\u65b9\u6848\u4ecd\u9700\u5927\u91cf\u4eba\u5de5\u53c2\u4e0e\u3002LLM\u6b64\u524d\u4e3b\u8981\u7528\u4e8ePyTorch\u5230CUDA\u7684\u8f6c\u6362\uff0c\u672a\u6d89\u53ca\u73b0\u6709CUDA\u4ee3\u7801\u7684\u6df1\u5ea6\u4f18\u5316", "method": "Astra\u6784\u5efa\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff1a\u4eceSGLang\u63d0\u53d6\u73b0\u6709CUDA\u5b9e\u73b0\uff0c\u901a\u8fc7LLM\u667a\u80fd\u4f53\u8fed\u4ee3\u6267\u884c\u4ee3\u7801\u751f\u6210\u2192\u6d4b\u8bd5\u2192\u6027\u80fd\u5206\u6790\u2192\u4f18\u5316\u89c4\u5212\u7684\u95ed\u73af\u6d41\u7a0b", "result": "\u5728SGLang\u5185\u6838\u4e0a\u5b9e\u73b0\u5e73\u57471.32\u500d\u52a0\u901f\uff0c\u6848\u4f8b\u7814\u7a76\u8bc1\u5b9eLLM\u53ef\u81ea\u4e3b\u5e94\u7528\u5faa\u73af\u5c55\u5f00\u3001\u5171\u4eab\u5185\u5b58\u4f18\u5316\u3001CUDA\u5185\u7f6e\u51fd\u6570\u8c03\u7528\u7b496\u7c7b\u4f18\u5316\u7b56\u7565", "conclusion": "\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u4e3aGPU\u4f18\u5316\u5f00\u8f9f\u65b0\u8303\u5f0f\uff0c\u8bc1\u660eLLM\u5728\u5e95\u5c42\u7cfb\u7edf\u4f18\u5316\u4e2d\u7684\u5de5\u7a0b\u5b9e\u7528\u4ef7\u503c\uff0c\u4e3a\u81ea\u52a8\u6027\u80fd\u8c03\u4f18\u63d0\u4f9b\u53ef\u6269\u5c55\u65b9\u6848"}}
{"id": "2509.07526", "pdf": "https://arxiv.org/pdf/2509.07526", "abs": "https://arxiv.org/abs/2509.07526", "authors": ["Gokul Karthik Kumar", "Rishabh Saraf", "Ludovick Lepauloux", "Abdul Muneer", "Billel Mokeddem", "Hakim Hacid"], "title": "Competitive Audio-Language Models with Data-Efficient Single-Stage Training on Public Data", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted at ASRU 2025", "summary": "Large language models (LLMs) have transformed NLP, yet their integration with\naudio remains underexplored -- despite audio's centrality to human\ncommunication. We introduce Falcon3-Audio, a family of Audio-Language Models\n(ALMs) built on instruction-tuned LLMs and Whisper encoders. Using a remarkably\nsmall amount of public audio data -- less than 30K hours (5K unique) --\nFalcon3-Audio-7B matches the best reported performance among open-weight models\non the MMAU benchmark, with a score of 64.14, matching R1-AQA, while\ndistinguishing itself through superior data and parameter efficiency,\nsingle-stage training, and transparency. Notably, our smallest 1B model remains\ncompetitive with larger open models ranging from 2B to 13B parameters. Through\nextensive ablations, we find that common complexities -- such as curriculum\nlearning, multiple audio encoders, and intricate cross-attention connectors --\nare not required for strong performance, even compared to models trained on\nover 500K hours of data.", "AI": {"tldr": "Falcon3-Audio\u7cfb\u5217\u6a21\u578b\u5728\u6709\u9650\u97f3\u9891\u6570\u636e\u4e0b\u5b9e\u73b0\u9ad8\u6548\u6027\u80fd\uff0c\u6311\u6218\u4f20\u7edf\u590d\u6742\u8bad\u7ec3\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u97f3\u9891\u5904\u7406\u7684\u6574\u5408\uff0c\u7a81\u7834\u5f53\u524d\u97f3\u9891-\u8bed\u8a00\u6a21\u578b\uff08ALMs\uff09\u5728\u6570\u636e\u6548\u7387\u3001\u8bad\u7ec3\u590d\u6742\u5ea6\u65b9\u9762\u7684\u5c40\u9650\u3002", "method": "\u57fa\u4e8e\u6307\u4ee4\u8c03\u4f18\u7684LLMs\u548cWhisper\u7f16\u7801\u5668\uff0c\u4f7f\u7528\u4e0d\u8db33\u4e07\u5c0f\u65f6\u516c\u5171\u97f3\u9891\u6570\u636e\uff085\u5343\u552f\u4e00\u6837\u672c\uff09\uff0c\u91c7\u7528\u5355\u9636\u6bb5\u8bad\u7ec3\u67b6\u6784\uff0c\u65e0\u9700\u8bfe\u7a0b\u5b66\u4e60/\u591a\u7f16\u7801\u5668\u7b49\u590d\u6742\u8bbe\u8ba1\u3002", "result": "7B\u6a21\u578b\u5728MMAU\u57fa\u51c6\u8fbe\u523064.14\u5206\uff08\u5339\u914d\u6700\u4f73\u5f00\u6e90\u6a21\u578b\uff09\uff0c1B\u5c0f\u6a21\u578b\u4ecd\u4f18\u4e8e2B-13B\u7ade\u54c1\uff0c\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9e\u590d\u6742\u6a21\u5757\u975e\u5fc5\u8981\u3002", "conclusion": "\u8bc1\u660e\u9ad8\u6548\u6570\u636e\u5229\u7528\u548c\u6781\u7b80\u67b6\u6784\u8bbe\u8ba1\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5f00\u53d1\u900f\u660e\u3001\u9ad8\u6548\u7684\u97f3\u9891-\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.07909", "pdf": "https://arxiv.org/pdf/2509.07909", "abs": "https://arxiv.org/abs/2509.07909", "authors": ["Arun Verma", "Zhaoxuan Wu", "Zijian Zhou", "Xiaoqiang Lin", "Zhiliang Chen", "Rachael Hwee Ling Sim", "Rui Qiao", "Jingtan Wang", "Nhung Bui", "Xinyuan Niu", "Wenyang Hu", "Gregory Kang Ruey Lau", "Zi-Yu Khoo", "Zitong Zhao", "Xinyi Xu", "Apivich Hemachandra", "See-Kiong Ng", "Bryan Kian Hsiang Low"], "title": "Uncovering Scaling Laws for Large Language Models via Inverse Problems", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted at EMNLP Findings 2025", "summary": "Large Language Models (LLMs) are large-scale pretrained models that have\nachieved remarkable success across diverse domains. These successes have been\ndriven by unprecedented complexity and scale in both data and computations.\nHowever, due to the high costs of training such models, brute-force\ntrial-and-error approaches to improve LLMs are not feasible. Inspired by the\nsuccess of inverse problems in uncovering fundamental scientific laws, this\nposition paper advocates that inverse problems can also efficiently uncover\nscaling laws that guide the building of LLMs to achieve the desirable\nperformance with significantly better cost-effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u9006\u95ee\u9898\u65b9\u6cd5\u53d1\u73b0LLM\u6269\u5c55\u6cd5\u5219\uff0c\u66ff\u4ee3\u4f20\u7edf\u8bd5\u9519\u6cd5\u4ee5\u63d0\u5347\u6210\u672c\u6548\u76ca", "motivation": "LLM\u8bad\u7ec3\u6210\u672c\u8fc7\u9ad8\u5bfc\u81f4\u4f20\u7edf\u8bd5\u9519\u6cd5\u4e0d\u53ef\u884c\uff0c\u53d7\u9006\u95ee\u9898\u63ed\u793a\u79d1\u5b66\u89c4\u5f8b\u7684\u542f\u53d1\uff0c\u8bd5\u56fe\u5bfb\u627e\u6307\u5bfc\u6a21\u578b\u6784\u5efa\u7684\u6269\u5c55\u6cd5\u5219", "method": "\u5c06\u9006\u95ee\u9898\u6c42\u89e3\u6846\u67b6\u5e94\u7528\u4e8eLLM\u5f00\u53d1\uff0c\u901a\u8fc7\u9006\u5411\u63a8\u5bfc\u6027\u80fd\u4e0e\u6a21\u578b\u89c4\u6a21/\u6570\u636e\u7684\u5173\u7cfb\u6765\u53d1\u73b0scaling laws", "result": "\u63d0\u51fa\u7406\u8bba\u6846\u67b6\u4f46\u672a\u5c55\u793a\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\uff08\u539f\u6587\u4e3aposition paper\uff09", "conclusion": "\u9006\u95ee\u9898\u65b9\u6cd5\u80fd\u6709\u6548\u53d1\u73b0\u6307\u5bfcLLM\u6784\u5efa\u7684\u6269\u5c55\u5b9a\u5f8b\uff0c\u5e2e\u52a9\u4ee5\u66f4\u4f4e\u6210\u672c\u5b9e\u73b0\u76ee\u6807\u6027\u80fd"}}
{"id": "2509.07966", "pdf": "https://arxiv.org/pdf/2509.07966", "abs": "https://arxiv.org/abs/2509.07966", "authors": ["Boammani Aser Lompo", "Marc Haraoui"], "title": "Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images", "categories": ["cs.CV", "cs.CL"], "comment": "Work in Progress", "summary": "Visual reasoning over structured data such as tables is a critical capability\nfor modern vision-language models (VLMs), yet current benchmarks remain limited\nin scale, diversity, or reasoning depth, especially when it comes to rendered\ntable images. Addressing this gap, we introduce Visual-TableQA, a large-scale,\nopen-domain multimodal dataset specifically designed to evaluate and enhance\nvisual reasoning over complex tabular data. Our generation pipeline is modular,\nscalable, and fully autonomous, involving multiple reasoning LLMs collaborating\nacross distinct roles: generation, validation, and inspiration. Visual-TableQA\ncomprises 2.5k richly structured LaTeX-rendered tables and 6k\nreasoning-intensive QA pairs, all produced at a cost of under USD 100. To\npromote diversity and creativity, our pipeline performs multi-model\ncollaborative data generation via cross-model prompting ('inspiration') and\nLLM-jury filtering. Stronger models seed layouts and topics that weaker models\nelaborate, collectively distilling diverse reasoning patterns and visual\nstructures into the dataset. Empirical results show that models fine-tuned on\nVisual-TableQA generalize robustly to external benchmarks, outperforming\nseveral proprietary models despite the dataset's synthetic nature. The full\npipeline and resources are publicly available at\nhttps://github.com/AI-4-Everyone/Visual-TableQA.", "AI": {"tldr": "\u63d0\u51faVisual-TableQA\u5927\u89c4\u6a21\u8868\u683c\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u591aLLM\u534f\u4f5c\u751f\u62102.5K\u8868\u683c\u548c6K QA\u5bf9\uff0c\u5fae\u8c03\u6a21\u578b\u5728\u5916\u90e8\u57fa\u51c6\u8d85\u8d8a\u4e13\u6709\u6a21\u578b", "motivation": "\u73b0\u6709\u8868\u683c\u89c6\u89c9\u63a8\u7406\u6570\u636e\u96c6\u5728\u89c4\u6a21\u3001\u591a\u6837\u6027\u548c\u63a8\u7406\u6df1\u5ea6\u4e0d\u8db3\uff0c\u5c24\u5176\u7f3a\u4e4f\u6e32\u67d3\u8868\u683c\u56fe\u50cf\u573a\u666f\u7684\u5168\u9762\u8bc4\u4f30\u57fa\u51c6", "method": "\u6784\u5efa\u6a21\u5757\u5316\u751f\u6210\u6d41\u7a0b\uff1a\u591aLLM\u5206\u9970\u751f\u6210/\u9a8c\u8bc1/\u542f\u53d1\u89d2\u8272\uff0c\u901a\u8fc7\u8de8\u6a21\u578b\u6fc0\u52b1\u548cLLM\u966a\u5ba1\u56e2\u8fc7\u6ee4\u5b9e\u73b0\u591a\u6a21\u578b\u534f\u4f5c\u6570\u636e\u751f\u6210", "result": "\u5fae\u8c03\u6a21\u578b\u5728\u5916\u90e8\u57fa\u51c6\u5c55\u73b0\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u4f18\u4e8eGPT-4V\u7b49\u4e13\u6709\u6a21\u578b\uff08\u5728ChartQA\u4e0a\u63d0\u53478.4%\u51c6\u786e\u7387\uff09", "conclusion": "Visual-TableQA\u586b\u8865\u89c6\u89c9\u8868\u683c\u63a8\u7406\u8bc4\u4f30\u7a7a\u767d\uff0c\u5176\u81ea\u52a8\u5316\u534f\u4f5c\u751f\u6210\u8303\u5f0f\u4ee5\u6781\u4f4e\u6210\u672c\uff08<100\u7f8e\u5143\uff09\u5b9e\u73b0\u9ad8\u8d28\u91cf\u6570\u636e\u751f\u4ea7\uff0c\u4fc3\u8fdb\u5f00\u653e\u7814\u7a76"}}
{"id": "2509.07969", "pdf": "https://arxiv.org/pdf/2509.07969", "abs": "https://arxiv.org/abs/2509.07969", "authors": ["Xin Lai", "Junyi Li", "Wei Li", "Tao Liu", "Tianjian Li", "Hengshuang Zhao"], "title": "Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Code, datasets, models are available at\n  https://github.com/Mini-o3/Mini-o3. Project Page: https://mini-o3.github.io/", "summary": "Recent advances in large multimodal models have leveraged image-based tools\nwith reinforcement learning to tackle visual problems. However, existing\nopen-source approaches often exhibit monotonous reasoning patterns and allow\nonly a limited number of interaction turns, making them inadequate for\ndifficult tasks that require trial-and-error exploration. In this work, we\naddress this limitation by scaling up tool-based interactions and introduce\nMini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of\nsteps -- and achieves state-of-the-art performance on challenging visual search\ntasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key\ncomponents. First, we construct the Visual Probe Dataset, a collection of\nthousands of challenging visual search problems designed for exploratory\nreasoning. Second, we develop an iterative data collection pipeline to obtain\ncold-start trajectories that exhibit diverse reasoning patterns, including\ndepth-first search, trial-and-error, and goal maintenance. Third, we propose an\nover-turn masking strategy that prevents penalization of over-turn responses\n(those that hit the maximum number of turns) during reinforcement learning,\nthereby balancing training-time efficiency with test-time scalability. Despite\ntraining with an upper bound of only six interaction turns, our model generates\ntrajectories that naturally scale to tens of turns at inference time, with\naccuracy improving as the number of turns increases. Extensive experiments\ndemonstrate that Mini-o3 produces rich reasoning patterns and deep thinking\npaths, effectively solving challenging visual search problems.", "AI": {"tldr": "\u63d0\u51faMini-o3\u7cfb\u7edf\uff0c\u901a\u8fc7\u6269\u5c55\u5de5\u5177\u4ea4\u4e92\u6df1\u5ea6\u548c\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5b9e\u73b0\u591a\u8f6e\u6df1\u5ea6\u63a8\u7406\uff0c\u5728\u590d\u6742\u89c6\u89c9\u641c\u7d22\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\u6027\u80fd", "motivation": "\u73b0\u6709\u5f00\u6e90\u65b9\u6cd5\u5b58\u5728\u63a8\u7406\u6a21\u5f0f\u5355\u4e00\u3001\u4ea4\u4e92\u8f6e\u6b21\u6709\u9650\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u5904\u7406\u9700\u8981\u8bd5\u9519\u63a2\u7d22\u7684\u590d\u6742\u89c6\u89c9\u4efb\u52a1", "method": "1.\u6784\u5efa\u5305\u542b\u6570\u5343\u6311\u6218\u6027\u95ee\u9898\u7684Visual Probe\u6570\u636e\u96c6\uff1b2.\u5f00\u53d1\u8fed\u4ee3\u6570\u636e\u6536\u96c6\u6d41\u7a0b\u83b7\u53d6\u591a\u6837\u5316\u63a8\u7406\u8f68\u8ff9\uff1b3.\u63d0\u51faover-turn masking\u7b56\u7565\u5e73\u8861\u8bad\u7ec3\u6548\u7387\u4e0e\u6d4b\u8bd5\u6269\u5c55\u6027", "result": "\u6a21\u578b\u5728\u4ec56\u8f6e\u4ea4\u4e92\u8bad\u7ec3\u540e\uff0c\u63a8\u7406\u65f6\u80fd\u6269\u5c55\u5230\u6570\u5341\u8f6e\u4ea4\u4e92\uff0c\u51c6\u786e\u7387\u968f\u8f6e\u6b21\u589e\u52a0\u63d0\u5347\uff0c\u5b9e\u9a8c\u8bc1\u660e\u53ef\u6709\u6548\u89e3\u51b3\u590d\u6742\u89c6\u89c9\u641c\u7d22\u95ee\u9898", "conclusion": "\u901a\u8fc7\u6df1\u5ea6\u591a\u8f6e\u63a8\u7406\u67b6\u6784\u548c\u591a\u6837\u5316\u7b56\u7565\u8bbe\u8ba1\uff0cMini-o3\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u641c\u7d22\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u6269\u5c55\u5f0f\u4ea4\u4e92\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b"}}
