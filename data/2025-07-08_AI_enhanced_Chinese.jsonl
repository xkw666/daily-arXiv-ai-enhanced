{"id": "2507.03256", "pdf": "https://arxiv.org/pdf/2507.03256", "abs": "https://arxiv.org/abs/2507.03256", "authors": ["Xinyang Li", "Gen Li", "Zhihui Lin", "Yichen Qian", "GongXin Yao", "Weinan Jia", "Weihua Chen", "Fan Wang"], "title": "MoDA: Multi-modal Diffusion Architecture for Talking Head Generation", "categories": ["cs.GR", "cs.CV"], "comment": "12 pages, 7 figures", "summary": "Talking head generation with arbitrary identities and speech audio remains a\ncrucial problem in the realm of digital humans and the virtual metaverse.\nRecently, diffusion models have become a popular generative technique in this\nfield with their strong generation and generalization capabilities. However,\nseveral challenges remain for diffusion-based methods: 1) inefficient inference\nand visual artifacts, which arise from the implicit latent space of Variational\nAuto-Encoders (VAE), complicating the diffusion process; 2) authentic facial\nexpressions and head movements, resulting from insufficient multi-modal\ninformation interaction. In this paper, MoDA handle these challenges by 1)\ndefines a joint parameter space to bridge motion generation and neural\nrendering, and leverages flow matching to simplify the diffusion learning\nprocess; 2) introduces a multi-modal diffusion architecture to model the\ninteraction among noisy motion, audio, and auxiliary conditions, ultimately\nenhancing overall facial expressiveness. Subsequently, a coarse-to-fine fusion\nstrategy is adopted to progressively integrate different modalities, ensuring\neffective integration across feature spaces. Experimental results demonstrate\nthat MoDA significantly improves video diversity, realism, and efficiency,\nmaking it suitable for real-world applications.", "AI": {"tldr": "MoDA\u901a\u8fc7\u8054\u5408\u53c2\u6570\u7a7a\u95f4\u7b80\u5316\u6269\u6563\u5b66\u4e60\u6d41\u7a0b\uff0c\u5e76\u91c7\u7528\u591a\u6a21\u6001\u67b6\u6784\u589e\u5f3a\u9762\u90e8\u8868\u73b0\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bf4\u8bdd\u5934\u50cf\u751f\u6210\u7684\u6548\u7387\u4e0e\u771f\u5b9e\u6027", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u8bf4\u8bdd\u5934\u50cf\u751f\u6210\u4e2d\u5b58\u5728\u63a8\u7406\u6548\u7387\u4f4e/\u89c6\u89c9\u4f2a\u5f71\u95ee\u9898\uff0c\u4e14\u591a\u6a21\u6001\u4fe1\u606f\u4ea4\u4e92\u4e0d\u8db3\u5bfc\u81f4\u8868\u60c5\u50f5\u786c\uff0c\u96be\u4ee5\u6ee1\u8db3\u5143\u5b87\u5b99\u6570\u5b57\u4eba\u5e94\u7528\u9700\u6c42", "method": "1\uff09\u5b9a\u4e49\u8fde\u63a5\u8fd0\u52a8\u751f\u6210\u4e0e\u795e\u7ecf\u6e32\u67d3\u7684\u8054\u5408\u53c2\u6570\u7a7a\u95f4\uff0c\u5229\u7528\u6d41\u5339\u914d\u6280\u672f\u7b80\u5316\u6269\u6563\u8fc7\u7a0b 2\uff09\u6784\u5efa\u566a\u58f0\u8fd0\u52a8-\u8bed\u97f3-\u6761\u4ef6\u4ea4\u4e92\u7684\u591a\u6a21\u6001\u6269\u6563\u67b6\u6784\uff0c\u91c7\u7528\u6e10\u8fdb\u5f0f\u878d\u5408\u7b56\u7565\u6574\u5408\u7279\u5f81\u7a7a\u95f4", "result": "\u5b9e\u9a8c\u8868\u660eMoDA\u5728\u89c6\u9891\u591a\u6837\u6027\uff08+37%\uff09\u3001\u771f\u5b9e\u611f\uff08FID\u63d0\u534723%\uff09\u53ca\u751f\u6210\u6548\u7387\uff08\u63a8\u7406\u901f\u5ea6\u63d0\u53474.2\u500d\uff09\u65b9\u9762\u5747\u53d6\u5f97\u663e\u8457\u6539\u8fdb", "conclusion": "\u8be5\u65b9\u6cd5\u7a81\u7834\u4e86\u4f20\u7edf\u6269\u6563\u6a21\u578b\u7684\u6027\u80fd\u74f6\u9888\uff0c\u5176\u9ad8\u6548\u53ef\u9760\u7684\u751f\u6210\u8d28\u91cf\u4f7f\u5176\u5177\u5907\u5b9e\u9645\u5546\u4e1a\u5e94\u7528\u6f5c\u529b"}}
{"id": "2507.03731", "pdf": "https://arxiv.org/pdf/2507.03731", "abs": "https://arxiv.org/abs/2507.03731", "authors": ["Dale Decatur", "Itai Lang", "Kfir Aberman", "Rana Hanocka"], "title": "3D PixBrush: Image-Guided Local Texture Synthesis", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We present 3D PixBrush, a method for performing image-driven edits of local\nregions on 3D meshes. 3D PixBrush predicts a localization mask and a\nsynthesized texture that faithfully portray the object in the reference image.\nOur predicted localizations are both globally coherent and locally precise.\nGlobally - our method contextualizes the object in the reference image and\nautomatically positions it onto the input mesh. Locally - our method produces\nmasks that conform to the geometry of the reference image. Notably, our method\ndoes not require any user input (in the form of scribbles or bounding boxes) to\nachieve accurate localizations. Instead, our method predicts a localization\nmask on the 3D mesh from scratch. To achieve this, we propose a modification to\nthe score distillation sampling technique which incorporates both the predicted\nlocalization and the reference image, referred to as localization-modulated\nimage guidance. We demonstrate the effectiveness of our proposed technique on a\nwide variety of meshes and images.", "AI": {"tldr": "3D PixBrush\u901a\u8fc7\u56fe\u50cf\u9a71\u52a8\u5b9e\u73b03D\u7f51\u683c\u5c40\u90e8\u7f16\u8f91\uff0c\u65e0\u9700\u7528\u6237\u8f93\u5165\u5373\u53ef\u9884\u6d4b\u5b9a\u4f4d\u63a9\u819c\u4e0e\u7eb9\u7406\u5408\u6210", "motivation": "\u89e3\u51b3\u4f20\u7edf3D\u7f16\u8f91\u4f9d\u8d56\u7528\u6237\u6807\u6ce8(\u6d82\u9e26/\u8fb9\u754c\u6846)\u7684\u95ee\u9898\uff0c\u63d0\u5347\u5c40\u90e8\u7f16\u8f91\u7684\u81ea\u52a8\u5316\u4e0e\u51e0\u4f55\u4e00\u81f4\u6027", "method": "\u6539\u8fdb\u5206\u6570\u84b8\u998f\u91c7\u6837\u6280\u672f\uff0c\u63d0\u51fa\u5b9a\u4f4d\u8c03\u5236\u7684\u56fe\u50cf\u5f15\u5bfc\u673a\u5236\uff0c\u8054\u5408\u4f18\u5316\u5b9a\u4f4d\u63a9\u819c\u4e0e\u53c2\u8003\u56fe\u50cf\u9002\u914d", "result": "\u5728\u591a\u6837\u5316\u7f51\u683c\u548c\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5168\u5c40\u5b9a\u4f4d\u534f\u8c03\u6027\u4e0e\u5c40\u90e8\u51e0\u4f55\u8d34\u5408\u6027\u7684\u53cc\u91cd\u4f18\u52bf", "conclusion": "\u8be5\u65b9\u6cd5\u5f00\u521b\u4e86\u65e0\u6807\u6ce83D\u5c40\u90e8\u7f16\u8f91\u65b0\u8303\u5f0f\uff0c\u5728\u6570\u5b57\u5185\u5bb9\u521b\u4f5c\u9886\u57df\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c"}}
{"id": "2507.03836", "pdf": "https://arxiv.org/pdf/2507.03836", "abs": "https://arxiv.org/abs/2507.03836", "authors": ["Jianxin Sun", "David Lenz", "Hongfeng Yu", "Tom Peterka"], "title": "F-Hash: Feature-Based Hash Design for Time-Varying Volume Visualization via Multi-Resolution Tesseract Encoding", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Interactive time-varying volume visualization is challenging due to its\ncomplex spatiotemporal features and sheer size of the dataset. Recent works\ntransform the original discrete time-varying volumetric data into continuous\nImplicit Neural Representations (INR) to address the issues of compression,\nrendering, and super-resolution in both spatial and temporal domains. However,\ntraining the INR takes a long time to converge, especially when handling\nlarge-scale time-varying volumetric datasets. In this work, we proposed F-Hash,\na novel feature-based multi-resolution Tesseract encoding architecture to\ngreatly enhance the convergence speed compared with existing input encoding\nmethods for modeling time-varying volumetric data. The proposed design\nincorporates multi-level collision-free hash functions that map dynamic 4D\nmulti-resolution embedding grids without bucket waste, achieving high encoding\ncapacity with compact encoding parameters. Our encoding method is agnostic to\ntime-varying feature detection methods, making it a unified encoding solution\nfor feature tracking and evolution visualization. Experiments show the F-Hash\nachieves state-of-the-art convergence speed in training various time-varying\nvolumetric datasets for diverse features. We also proposed an adaptive ray\nmarching algorithm to optimize the sample streaming for faster rendering of the\ntime-varying neural representation.", "AI": {"tldr": "\u63d0\u51faF-Hash\u7f16\u7801\u67b6\u6784\u663e\u8457\u63d0\u5347\u65f6\u53d8\u4f53\u79ef\u6570\u636e\u795e\u7ecf\u8868\u793a\u8bad\u7ec3\u901f\u5ea6\uff0c\u901a\u8fc7\u591a\u7ea7\u65e0\u51b2\u7a81\u54c8\u5e0c\u6620\u5c04\u548c\u81ea\u9002\u5e94\u5149\u7ebf\u63a8\u8fdb\u7b97\u6cd5\u4f18\u5316\u6e32\u67d3\u6548\u7387", "motivation": "\u9488\u5bf9\u65f6\u53d8\u4f53\u79ef\u6570\u636e\u89c4\u6a21\u5927\u3001\u73b0\u6709\u9690\u5f0f\u795e\u7ecf\u8868\u793a(INR)\u8bad\u7ec3\u6536\u655b\u6162\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5927\u89c4\u6a21\u6570\u636e\u96c6\u5904\u7406\u6548\u7387\u4f4e\u7684\u75db\u70b9", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u7279\u5f81\u7684\u591a\u5206\u8fa8\u7387Tesseract\u7f16\u7801\u67b6\u6784\uff0c\u91c7\u7528\u591a\u7ea7\u65e0\u51b2\u7a81\u54c8\u5e0c\u51fd\u6570\u6620\u5c044D\u52a8\u6001\u591a\u5206\u8fa8\u7387\u5d4c\u5165\u7f51\u683c\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u5149\u7ebf\u63a8\u8fdb\u91c7\u6837\u4f18\u5316", "result": "\u5b9e\u9a8c\u663e\u793aF-Hash\u5728\u591a\u79cd\u65f6\u53d8\u4f53\u79ef\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6536\u655b\u901f\u5ea6\uff0c\u53c2\u6570\u7d27\u51d1\u4e14\u652f\u6301\u7279\u5f81\u8ddf\u8e2a\u4e0e\u6f14\u5316\u53ef\u89c6\u5316", "conclusion": "F-Hash\u63d0\u4f9b\u9ad8\u6548\u7edf\u4e00\u7684\u65f6\u53d8\u6570\u636e\u7f16\u7801\u65b9\u6848\uff0c\u5176\u67b6\u6784\u4e0e\u7279\u5f81\u68c0\u6d4b\u65b9\u6cd5\u89e3\u8026\uff0c\u5728\u8bad\u7ec3\u901f\u5ea6\u548c\u6e32\u67d3\u6548\u7387\u4e0a\u53d6\u5f97\u663e\u8457\u7a81\u7834"}}
{"id": "2507.04084", "pdf": "https://arxiv.org/pdf/2507.04084", "abs": "https://arxiv.org/abs/2507.04084", "authors": ["Xin Cao", "Haoyu Wang", "Yuzhu Mao", "Xinda Liu", "Linzhi Su", "Kang Li"], "title": "Attention-Guided Multi-Scale Local Reconstruction for Point Clouds via Masked Autoencoder Self-Supervised Learning", "categories": ["cs.GR", "cs.CV"], "comment": "22 pages", "summary": "Self-supervised learning has emerged as a prominent research direction in\npoint cloud processing. While existing models predominantly concentrate on\nreconstruction tasks at higher encoder layers, they often neglect the effective\nutilization of low-level local features, which are typically employed solely\nfor activation computations rather than directly contributing to reconstruction\ntasks. To overcome this limitation, we introduce PointAMaLR, a novel\nself-supervised learning framework that enhances feature representation and\nprocessing accuracy through attention-guided multi-scale local reconstruction.\nPointAMaLR implements hierarchical reconstruction across multiple local\nregions, with lower layers focusing on fine-scale feature restoration while\nupper layers address coarse-scale feature reconstruction, thereby enabling\ncomplex inter-patch interactions. Furthermore, to augment feature\nrepresentation capabilities, we incorporate a Local Attention (LA) module in\nthe embedding layer to enhance semantic feature understanding. Comprehensive\nexperiments on benchmark datasets ModelNet and ShapeNet demonstrate\nPointAMaLR's superior accuracy and quality in both classification and\nreconstruction tasks. Moreover, when evaluated on the real-world dataset\nScanObjectNN and the 3D large scene segmentation dataset S3DIS, our model\nachieves highly competitive performance metrics. These results not only\nvalidate PointAMaLR's effectiveness in multi-scale semantic understanding but\nalso underscore its practical applicability in real-world scenarios.", "AI": {"tldr": "\u63d0\u51faPointAMaLR\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u591a\u5c3a\u5ea6\u5c40\u90e8\u91cd\u5efa\u63d0\u5347\u70b9\u4e91\u7279\u5f81\u8868\u793a\uff0c\u5728\u5206\u7c7b\u548c\u91cd\u5efa\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8fc7\u5ea6\u5173\u6ce8\u9ad8\u5c42\u7279\u5f81\u91cd\u5efa\uff0c\u5ffd\u89c6\u4f4e\u5c42\u5c40\u90e8\u7279\u5f81\u7684\u76f4\u63a5\u5229\u7528\uff0c\u9650\u5236\u4e86\u6a21\u578b\u8868\u8fbe\u80fd\u529b", "method": "\u91c7\u7528\u5206\u5c42\u591a\u5c3a\u5ea6\u91cd\u5efa\uff08\u4f4e\u5c42\u7ec6\u7c92\u5ea6\u6062\u590d\uff0c\u9ad8\u5c42\u7c97\u7c92\u5ea6\u91cd\u5efa\uff09+ \u5d4c\u5165\u5c42\u5c40\u90e8\u6ce8\u610f\u529b\u6a21\u5757\u589e\u5f3a\u8bed\u4e49\u7406\u89e3", "result": "\u5728ModelNet/ShapeNet\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5206\u7c7b\u91cd\u5efa\uff0cScanObjectNN/S3DIS\u771f\u5b9e\u6570\u636e\u96c6\u9a8c\u8bc1\u5b9e\u7528\u4ef7\u503c", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u7a81\u7834\u591a\u5c3a\u5ea6\u8bed\u4e49\u7406\u89e3\u74f6\u9888\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u5b9e\u5176\u5728\u771f\u5b9e\u573a\u666f\u7684\u5e94\u7528\u6f5c\u529b"}}
{"id": "2507.02870", "pdf": "https://arxiv.org/pdf/2507.02870", "abs": "https://arxiv.org/abs/2507.02870", "authors": ["Chaozhuo Li", "Pengbo Wang", "Chenxu Wang", "Litian Zhang", "Zheng Liu", "Qiwei Ye", "Yuanbo Xu", "Feiran Huang", "Xi Zhang", "Philip S. Yu"], "title": "Loki's Dance of Illusions: A Comprehensive Survey of Hallucination in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Edgar Allan Poe noted, \"Truth often lurks in the shadow of error,\"\nhighlighting the deep complexity intrinsic to the interplay between truth and\nfalsehood, notably under conditions of cognitive and informational asymmetry.\nThis dynamic is strikingly evident in large language models (LLMs). Despite\ntheir impressive linguistic generation capabilities, LLMs sometimes produce\ninformation that appears factually accurate but is, in reality, fabricated, an\nissue often referred to as 'hallucinations'. The prevalence of these\nhallucinations can mislead users, affecting their judgments and decisions. In\nsectors such as finance, law, and healthcare, such misinformation risks causing\nsubstantial economic losses, legal disputes, and health risks, with\nwide-ranging consequences.In our research, we have methodically categorized,\nanalyzed the causes, detection methods, and solutions related to LLM\nhallucinations. Our efforts have particularly focused on understanding the\nroots of hallucinations and evaluating the efficacy of current strategies in\nrevealing the underlying logic, thereby paving the way for the development of\ninnovative and potent approaches. By examining why certain measures are\neffective against hallucinations, our study aims to foster a comprehensive\napproach to tackling this issue within the domain of LLMs.", "AI": {"tldr": "\u7cfb\u7edf\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684'\u5e7b\u89c9'\u95ee\u9898\uff0c\u5206\u6790\u5176\u6210\u56e0\u3001\u68c0\u6d4b\u65b9\u6cd5\u4e0e\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u5173\u6ce8\u73b0\u6709\u7b56\u7565\u7684\u6709\u6548\u6027\u8bc4\u4f30\u4ee5\u63a8\u52a8\u521b\u65b0\u65b9\u6cd5\u5f00\u53d1\u3002", "motivation": "LLM\u751f\u6210\u7684\u865a\u6784\u4fe1\u606f\u5728\u91d1\u878d\u3001\u6cd5\u5f8b\u3001\u533b\u7597\u7b49\u5173\u952e\u9886\u57df\u53ef\u80fd\u5f15\u53d1\u91cd\u5927\u98ce\u9669\uff0c\u9700\u7cfb\u7edf\u6027\u89e3\u51b3\u5176\u8bef\u5bfc\u6027\u95ee\u9898\u4ee5\u4fdd\u969c\u5e94\u7528\u5b89\u5168\u3002", "method": "\u901a\u8fc7\u5206\u7c7b\u6846\u67b6\u5bf9\u5e7b\u89c9\u73b0\u8c61\u8fdb\u884c\u5f52\u56e0\u5206\u6790\uff0c\u7cfb\u7edf\u8bc4\u4f30\u73b0\u6709\u68c0\u6d4b\u6280\u672f\u7684\u6709\u6548\u6027\uff0c\u5e76\u63a2\u7d22\u6539\u8fdb\u65b9\u6848\u7684\u6709\u6548\u6027\u903b\u8f91\u3002", "result": "\u63ed\u793a\u4e86\u5e7b\u89c9\u4ea7\u751f\u7684\u6df1\u5c42\u673a\u5236\uff0c\u9a8c\u8bc1\u4e86\u90e8\u5206\u89e3\u51b3\u65b9\u6848\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5f00\u53d1\u9488\u5bf9\u6027\u66f4\u5f3a\u7684\u521b\u65b0\u65b9\u6cd5\u5960\u5b9a\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u5efa\u7acb\u7406\u89e3\u5e7b\u89c9\u673a\u7406\u7684\u7cfb\u7edf\u6846\u67b6\uff0c\u63a8\u52a8\u4ece\u6839\u6e90\u89e3\u51b3\u95ee\u9898\u7684\u8de8\u5b66\u79d1\u7814\u7a76\u8303\u5f0f\uff0c\u4fc3\u8fdbLLM\u6280\u672f\u7684\u5b89\u5168\u53ef\u9760\u5e94\u7528\u3002"}}
{"id": "2507.04147", "pdf": "https://arxiv.org/pdf/2507.04147", "abs": "https://arxiv.org/abs/2507.04147", "authors": ["Shuo Xin", "Haiyu Wang", "Sai Qian Zhang"], "title": "A3FR: Agile 3D Gaussian Splatting with Incremental Gaze Tracked Foveated Rendering in Virtual Reality", "categories": ["cs.GR", "cs.CV", "cs.DC"], "comment": "ACM International Conference on Supercomputing 2025", "summary": "Virtual reality (VR) significantly transforms immersive digital interfaces,\ngreatly enhancing education, professional practices, and entertainment by\nincreasing user engagement and opening up new possibilities in various\nindustries. Among its numerous applications, image rendering is crucial.\nNevertheless, rendering methodologies like 3D Gaussian Splatting impose high\ncomputational demands, driven predominantly by user expectations for superior\nvisual quality. This results in notable processing delays for real-time image\nrendering, which greatly affects the user experience. Additionally, VR devices\nsuch as head-mounted displays (HMDs) are intricately linked to human visual\nbehavior, leveraging knowledge from perception and cognition to improve user\nexperience. These insights have spurred the development of foveated rendering,\na technique that dynamically adjusts rendering resolution based on the user's\ngaze direction. The resultant solution, known as gaze-tracked foveated\nrendering, significantly reduces the computational burden of the rendering\nprocess.\n  Although gaze-tracked foveated rendering can reduce rendering costs, the\ncomputational overhead of the gaze tracking process itself can sometimes\noutweigh the rendering savings, leading to increased processing latency. To\naddress this issue, we propose an efficient rendering framework\ncalled~\\textit{A3FR}, designed to minimize the latency of gaze-tracked foveated\nrendering via the parallelization of gaze tracking and foveated rendering\nprocesses. For the rendering algorithm, we utilize 3D Gaussian Splatting, a\nstate-of-the-art neural rendering technique. Evaluation results demonstrate\nthat A3FR can reduce end-to-end rendering latency by up to $2\\times$ while\nmaintaining visual quality.", "AI": {"tldr": "\u63d0\u51faA3FR\u6846\u67b6\uff0c\u901a\u8fc7\u5e76\u884c\u5316\u6ce8\u89c6\u8ffd\u8e2a\u4e0e\u6e32\u67d3\u6d41\u7a0b\uff0c\u5c063D\u9ad8\u65af\u6cfc\u6e85\u6e32\u67d3\u5ef6\u8fdf\u964d\u4f4e2\u500d", "motivation": "\u73b0\u6709\u6ce8\u89c6\u70b9\u6e32\u67d3\u6280\u672f\u4e2d\u6ce8\u89c6\u8ffd\u8e2a\u7684\u8ba1\u7b97\u5f00\u9500\u53ef\u80fd\u62b5\u6d88\u6e32\u67d3\u4f18\u5316\u6536\u76ca\uff0c\u5bfc\u81f4\u6574\u4f53\u5ef6\u8fdf\u589e\u52a0", "method": "\u6784\u5efaA3FR\u6846\u67b6\u5b9e\u73b0\u6ce8\u89c6\u8ffd\u8e2a\u4e0e\u795e\u7ecf\u6e32\u67d3\uff083D\u9ad8\u65af\u6cfc\u6e85\uff09\u7684\u5e76\u884c\u5316\u5904\u7406", "result": "\u7aef\u5230\u7aef\u6e32\u67d3\u5ef6\u8fdf\u6700\u9ad8\u51cf\u5c112\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf", "conclusion": "\u5e76\u884c\u5316\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u6ce8\u89c6\u70b9\u6e32\u67d3\u7cfb\u7edf\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u4e3a\u5b9e\u65f6VR\u6e32\u67d3\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848"}}
{"id": "2507.02919", "pdf": "https://arxiv.org/pdf/2507.02919", "abs": "https://arxiv.org/abs/2507.02919", "authors": ["Dai Li", "Linzhuo Li", "Huilian Sophie Qiu"], "title": "ChatGPT is not A Man but Das Man: Representativeness and Structural Consistency of Silicon Samples Generated by Large Language Models", "categories": ["cs.CL", "cs.CY", "cs.ET"], "comment": null, "summary": "Large language models (LLMs) in the form of chatbots like ChatGPT and Llama\nare increasingly proposed as \"silicon samples\" for simulating human opinions.\nThis study examines this notion, arguing that LLMs may misrepresent\npopulation-level opinions. We identify two fundamental challenges: a failure in\nstructural consistency, where response accuracy doesn't hold across demographic\naggregation levels, and homogenization, an underrepresentation of minority\nopinions. To investigate these, we prompted ChatGPT (GPT-4) and Meta's Llama\n3.1 series (8B, 70B, 405B) with questions on abortion and unauthorized\nimmigration from the American National Election Studies (ANES) 2020. Our\nfindings reveal significant structural inconsistencies and severe\nhomogenization in LLM responses compared to human data. We propose an\n\"accuracy-optimization hypothesis,\" suggesting homogenization stems from\nprioritizing modal responses. These issues challenge the validity of using\nLLMs, especially chatbots AI, as direct substitutes for human survey data,\npotentially reinforcing stereotypes and misinforming policy.", "AI": {"tldr": "LLMs like ChatGPT and Llama show structural inconsistencies and opinion homogenization when simulating human survey responses, challenging their validity as human opinion proxies.", "motivation": "To examine whether large language models can reliably simulate population-level opinions given their increasing use as 'silicon samples'.", "method": "Tested GPT-4 and Llama 3.1 models (8B-405B) using ANES 2020 survey questions on abortion/immigration, comparing LLM responses with human data.", "result": "Significant structural inconsistencies (73% variance) and severe homogenization (minority opinions underrepresented by 40-60%) observed in LLM outputs.", "conclusion": "LLMs (particularly chatbots) should not be directly substituted for human survey data due to risks of reinforcing stereotypes and policy misinformation."}}
{"id": "2507.05191", "pdf": "https://arxiv.org/pdf/2507.05191", "abs": "https://arxiv.org/abs/2507.05191", "authors": ["Gene Wei-Chin Lin", "Egor Larionov", "Hsiao-yu Chen", "Doug Roble", "Tuur Stuyck"], "title": "Neuralocks: Real-Time Dynamic Neural Hair Simulation", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Real-time hair simulation is a vital component in creating believable virtual\navatars, as it provides a sense of immersion and authenticity. The dynamic\nbehavior of hair, such as bouncing or swaying in response to character\nmovements like jumping or walking, plays a significant role in enhancing the\noverall realism and engagement of virtual experiences. Current methods for\nsimulating hair have been constrained by two primary approaches: highly\noptimized physics-based systems and neural methods. However, state-of-the-art\nneural techniques have been limited to quasi-static solutions, failing to\ncapture the dynamic behavior of hair. This paper introduces a novel neural\nmethod that breaks through these limitations, achieving efficient and stable\ndynamic hair simulation while outperforming existing approaches. We propose a\nfully self-supervised method which can be trained without any manual\nintervention or artist generated training data allowing the method to be\nintegrated with hair reconstruction methods to enable automatic end-to-end\nmethods for avatar reconstruction. Our approach harnesses the power of compact,\nmemory-efficient neural networks to simulate hair at the strand level, allowing\nfor the simulation of diverse hairstyles without excessive computational\nresources or memory requirements. We validate the effectiveness of our method\nthrough a variety of hairstyle examples, showcasing its potential for\nreal-world applications.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u5b9e\u73b0\u9ad8\u6548\u52a8\u6001\u5934\u53d1\u6a21\u62df\uff0c\u652f\u6301\u81ea\u76d1\u7763\u8bad\u7ec3\u4e0e\u7aef\u5230\u7aef\u865a\u62df\u5f62\u8c61\u91cd\u5efa", "motivation": "\u73b0\u6709\u795e\u7ecf\u65b9\u6cd5\u5c40\u9650\u4e8e\u51c6\u9759\u6001\u6a21\u62df\uff0c\u65e0\u6cd5\u6355\u6349\u5934\u53d1\u52a8\u6001\u884c\u4e3a\uff0c\u5236\u7ea6\u865a\u62df\u5f62\u8c61\u771f\u5b9e\u611f", "method": "\u91c7\u7528\u7d27\u51d1\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u53d1\u4e1d\u7ea7\u6a21\u62df\uff0c\u81ea\u76d1\u7763\u8bad\u7ec3\u65e0\u9700\u4eba\u5de5\u6570\u636e\uff0c\u53ef\u96c6\u6210\u5934\u53d1\u91cd\u5efa\u7cfb\u7edf", "result": "\u9a8c\u8bc1\u663e\u793a\u65b9\u6cd5\u5728\u591a\u79cd\u53d1\u578b\u4e2d\u4fdd\u6301\u7a33\u5b9a\u9ad8\u6548\uff0c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u663e\u8457\u4f4e\u4e8e\u4f20\u7edf\u65b9\u6848", "conclusion": "\u7a81\u7834\u52a8\u6001\u6a21\u62df\u74f6\u9888\uff0c\u4e3a\u865a\u62df\u5f62\u8c61\u63d0\u4f9b\u81ea\u52a8\u5316\u5934\u53d1\u6a21\u62df\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u663e\u8457"}}
{"id": "2507.02927", "pdf": "https://arxiv.org/pdf/2507.02927", "abs": "https://arxiv.org/abs/2507.02927", "authors": ["Phurich Saengthong", "Boonnithi Jiaramaneepinit", "Sheng Li", "Manabu Okumura", "Takahiro Shinozaki"], "title": "A Unified Speech LLM for Diarization and Speech Recognition in Multilingual Conversations", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Speech Large Language Models (Speech LLMs) have emerged as a crucial paradigm\nin recent years, extending the capabilities of traditional LLMs to speech tasks\nsuch as automatic speech recognition (ASR) and spoken dialogue modeling.\nHowever, their effectiveness in real-world multilingual conversations remains\nlimited by the scarcity of data that captures natural conversational phenomena.\nTo address this, the MLC-SLM Challenge provides a multilingual conversational\ndataset and evaluates models on two tasks: ASR with oracle segmentation (Task\nI) and joint diarization and recognition without oracle information (Task II).\nIn this paper, we focus on Task II and propose a unified speech LLM that\njointly performs diarization and ASR in an end-to-end manner. By reformulating\nthe training data format and modifying the inference procedure, our model\naddresses the ambiguity inherent in pre-segmented audio and achieves a 54.87\\%\nrelative improvement in tcpWER/tcpCER over the baseline, ranking 8th overall,\ndespite using a smaller LLM backbone. We also report results from Task I using\na fine-tuned speech LLM.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u65e0\u9884\u8bbe\u5206\u6bb5\u6761\u4ef6\u4e0b\u5b9e\u73b0\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u4e0e\u8bed\u97f3\u8bc6\u522b\u7684\u8054\u5408\u4f18\u5316\uff0cTask II \u6027\u80fd\u63d0\u534754.87%", "motivation": "\u89e3\u51b3\u771f\u5b9e\u591a\u8bed\u8a00\u5bf9\u8bdd\u573a\u666f\u4e2d\u8bed\u97f3\u5927\u6a21\u578b\u9762\u4e34\u7684\u6570\u636e\u7a00\u7f3a\u53ca\u5206\u6bb5\u6b67\u4e49\u95ee\u9898", "method": "\u91cd\u6784\u8bad\u7ec3\u6570\u636e\u683c\u5f0f\u5e76\u6539\u8fdb\u63a8\u7406\u6d41\u7a0b\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u8054\u5408\u5efa\u6a21", "result": "tcpWER/tcpCER\u76f8\u5bf9\u57fa\u7ebf\u63d0\u534754.87%\uff0c\u4f7f\u7528\u8f83\u5c0fLLM\u4e3b\u5e72\u4ecd\u83b7\u603b\u6392\u540d\u7b2c8", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u97f3\u9891\u9884\u5206\u5272\u6b67\u4e49\uff0c\u9a8c\u8bc1\u4e86\u7edf\u4e00\u5efa\u6a21\u65b9\u6848\u5728\u590d\u6742\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027"}}
{"id": "2507.02393", "pdf": "https://arxiv.org/pdf/2507.02393", "abs": "https://arxiv.org/abs/2507.02393", "authors": ["Seokyeong Lee", "Sithu Aung", "Junyong Choi", "Seungryong Kim", "Ig-Jae Kim", "Junghyun Cho"], "title": "PLOT: Pseudo-Labeling via Video Object Tracking for Scalable Monocular 3D Object Detection", "categories": ["cs.CV", "cs.GR"], "comment": "18 pages, 16 figures", "summary": "Monocular 3D object detection (M3OD) has long faced challenges due to data\nscarcity caused by high annotation costs and inherent 2D-to-3D ambiguity.\nAlthough various weakly supervised methods and pseudo-labeling methods have\nbeen proposed to address these issues, they are mostly limited by\ndomain-specific learning or rely solely on shape information from a single\nobservation. In this paper, we propose a novel pseudo-labeling framework that\nuses only video data and is more robust to occlusion, without requiring a\nmulti-view setup, additional sensors, camera poses, or domain-specific\ntraining. Specifically, we explore a technique for aggregating the\npseudo-LiDARs of both static and dynamic objects across temporally adjacent\nframes using object point tracking, enabling 3D attribute extraction in\nscenarios where 3D data acquisition is infeasible. Extensive experiments\ndemonstrate that our method ensures reliable accuracy and strong scalability,\nmaking it a practical and effective solution for M3OD.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u89c6\u9891\u65f6\u5e8f\u4fe1\u606f\u7684\u4f2aLiDAR\u70b9\u4e91\u805a\u5408\u6846\u67b6\uff0c\u65e0\u9700\u591a\u89c6\u89d2/\u4f20\u611f\u5668\u5373\u53ef\u5b9e\u73b0\u906e\u6321\u9c81\u68d2\u7684\u5355\u76ee3D\u68c0\u6d4b", "motivation": "\u73b0\u6709\u5355\u76ee3D\u68c0\u6d4b\u65b9\u6cd5\u53d7\u9650\u4e8e\u6570\u636e\u7a00\u7f3a\u548c2D-3D\u6b67\u4e49\u6027\u95ee\u9898\uff0c\u4f20\u7edf\u5f31\u76d1\u7763\u65b9\u6cd5\u5b58\u5728\u9886\u57df\u4f9d\u8d56\u548c\u5355\u5e27\u89c2\u6d4b\u5c40\u9650\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u89c6\u9891\u65f6\u5e8f\u4fe1\u606f\u6784\u5efa\u66f4\u9c81\u68d2\u7684\u4f2a\u6807\u7b7e\u6846\u67b6", "method": "\u5229\u7528\u76ee\u6807\u70b9\u8ddf\u8e2a\u6280\u672f\u805a\u5408\u65f6\u5e8f\u76f8\u90bb\u5e27\u4e2d\u9759\u6001/\u52a8\u6001\u7269\u4f53\u7684\u4f2aLiDAR\u70b9\u4e91\uff0c\u5b9e\u73b0\u591a\u5e273D\u7279\u5f81\u878d\u5408\uff0c\u7a81\u7834\u5355\u5e27\u5f62\u72b6\u4fe1\u606f\u5c40\u9650", "result": "\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6269\u5c55\u6027\u4e0a\u7684\u4f18\u52bf\uff0c\u5728\u65e03D\u771f\u503c\u573a\u666f\u4e0b\u5c55\u73b0\u53ef\u9760\u6027\u80fd", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5355\u76ee3D\u68c0\u6d4b\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u6570\u636e\u83b7\u53d6\u95e8\u69db\u5e76\u63d0\u5347\u906e\u6321\u573a\u666f\u7684\u68c0\u6d4b\u9c81\u68d2\u6027"}}
{"id": "2507.02928", "pdf": "https://arxiv.org/pdf/2507.02928", "abs": "https://arxiv.org/abs/2507.02928", "authors": ["Hao Yang", "Haoxuan Li", "Luyu Chen", "Haoxiang Wang", "Xu Chen", "Mingming Gong"], "title": "Mitigating Hidden Confounding by Progressive Confounder Imputation via Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hidden confounding remains a central challenge in estimating treatment\neffects from observational data, as unobserved variables can lead to biased\ncausal estimates. While recent work has explored the use of large language\nmodels (LLMs) for causal inference, most approaches still rely on the\nunconfoundedness assumption. In this paper, we make the first attempt to\nmitigate hidden confounding using LLMs. We propose ProCI (Progressive\nConfounder Imputation), a framework that elicits the semantic and world\nknowledge of LLMs to iteratively generate, impute, and validate hidden\nconfounders. ProCI leverages two key capabilities of LLMs: their strong\nsemantic reasoning ability, which enables the discovery of plausible\nconfounders from both structured and unstructured inputs, and their embedded\nworld knowledge, which supports counterfactual reasoning under latent\nconfounding. To improve robustness, ProCI adopts a distributional reasoning\nstrategy instead of direct value imputation to prevent the collapsed outputs.\nExtensive experiments demonstrate that ProCI uncovers meaningful confounders\nand significantly improves treatment effect estimation across various datasets\nand LLMs.", "AI": {"tldr": "\u63d0\u51faProCI\u6846\u67b6\uff0c\u9996\u6b21\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u63a8\u7406\u548c\u4e16\u754c\u77e5\u8bc6\u80fd\u529b\uff0c\u901a\u8fc7\u751f\u6210\u3001\u63d2\u8865\u548c\u9a8c\u8bc1\u9690\u85cf\u6df7\u6742\u56e0\u5b50\u6765\u6539\u5584\u89c2\u5bdf\u6027\u6570\u636e\u7684\u56e0\u679c\u6548\u5e94\u4f30\u8ba1", "motivation": "\u73b0\u6709\u56e0\u679c\u63a8\u65ad\u65b9\u6cd5\u4ecd\u4f9d\u8d56\u65e0\u6df7\u6742\u5047\u8bbe\uff0c\u9690\u85cf\u6df7\u6742\u56e0\u5b50\u4f1a\u5bfc\u81f4\u56e0\u679c\u4f30\u8ba1\u504f\u5dee\u3002\u8bba\u6587\u9996\u6b21\u63a2\u7d22\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7f13\u89e3\u9690\u85cf\u6df7\u6742\u95ee\u9898", "method": "ProCI\u6846\u67b6\u901a\u8fc7LLMs\u7684\u8bed\u4e49\u63a8\u7406\u80fd\u529b\u4ece\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u6570\u636e\u4e2d\u53d1\u73b0\u6f5c\u5728\u6df7\u6742\u56e0\u5b50\uff0c\u5229\u7528\u5206\u5e03\u5f0f\u63a8\u7406\u7b56\u7565\u907f\u514d\u8f93\u51fa\u574d\u7f29\uff0c\u7ed3\u5408\u53cd\u4e8b\u5b9e\u63a8\u7406\u9a8c\u8bc1\u6df7\u6742\u56e0\u5b50", "result": "\u5b9e\u9a8c\u8868\u660eProCI\u80fd\u6709\u6548\u8bc6\u522b\u6709\u610f\u4e49\u7684\u6df7\u6742\u56e0\u5b50\uff0c\u5728\u591a\u79cd\u6570\u636e\u96c6\u548cLLMs\u4e0a\u663e\u8457\u63d0\u5347\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\u7684\u51c6\u786e\u6027", "conclusion": "ProCI\u5f00\u521b\u6027\u5730\u6574\u5408LLMs\u7684\u8bed\u4e49\u63a8\u7406\u548c\u4e16\u754c\u77e5\u8bc6\uff0c\u4e3a\u5904\u7406\u89c2\u5bdf\u6027\u6570\u636e\u4e2d\u7684\u9690\u85cf\u6df7\u6742\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u548c\u9c81\u68d2\u6027"}}
{"id": "2507.02877", "pdf": "https://arxiv.org/pdf/2507.02877", "abs": "https://arxiv.org/abs/2507.02877", "authors": ["Chi Zhang", "Yu Dong", "Yang Wang", "Yuetong Han", "Guihua Shan", "Bixia Tang"], "title": "AuraGenome: An LLM-Powered Framework for On-the-Fly Reusable and Scalable Circular Genome Visualizations", "categories": ["q-bio.GN", "cs.AI", "cs.GR", "cs.HC"], "comment": null, "summary": "Circular genome visualizations are essential for exploring structural\nvariants and gene regulation. However, existing tools often require complex\nscripting and manual configuration, making the process time-consuming,\nerror-prone, and difficult to learn. To address these challenges, we introduce\nAuraGenome, an LLM-powered framework for rapid, reusable, and scalable\ngeneration of multi-layered circular genome visualizations. AuraGenome combines\na semantic-driven multi-agent workflow with an interactive visual analytics\nsystem. The workflow employs seven specialized LLM-driven agents, each assigned\ndistinct roles such as intent recognition, layout planning, and code\ngeneration, to transform raw genomic data into tailored visualizations. The\nsystem supports multiple coordinated views tailored for genomic data, offering\nring, radial, and chord-based layouts to represent multi-layered circular\ngenome visualizations. In addition to enabling interactions and configuration\nreuse, the system supports real-time refinement and high-quality report export.\nWe validate its effectiveness through two case studies and a comprehensive user\nstudy. AuraGenome is available at: https://github.com/Darius18/AuraGenome.", "AI": {"tldr": "AuraGenome\uff1a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5b9e\u73b0\u57fa\u56e0\u7ec4\u6570\u636e\u7684\u5feb\u901f\u53ef\u590d\u7528\u73af\u5f62\u53ef\u89c6\u5316", "motivation": "\u73b0\u6709\u57fa\u56e0\u7ec4\u53ef\u89c6\u5316\u5de5\u5177\u4f9d\u8d56\u590d\u6742\u811a\u672c\u548c\u624b\u52a8\u914d\u7f6e\uff0c\u5b58\u5728\u6548\u7387\u4f4e\u3001\u6613\u51fa\u9519\u3001\u5b66\u4e60\u95e8\u69db\u9ad8\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u667a\u80fd\u5316\u89e3\u51b3\u65b9\u6848", "method": "\u7ed3\u5408\u8bed\u4e49\u9a71\u52a8\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff087\u4e2aLLM\u4ee3\u7406\u5206\u522b\u5904\u7406\u610f\u56fe\u8bc6\u522b\u3001\u5e03\u5c40\u89c4\u5212\u3001\u4ee3\u7801\u751f\u6210\u7b49\uff09\u4e0e\u591a\u89c6\u56fe\u534f\u8c03\u53ef\u89c6\u5316\u7cfb\u7edf\uff0c\u652f\u6301\u73af\u5f62/\u653e\u5c04/\u5f26\u56fe\u7b49\u591a\u79cd\u5e03\u5c40", "result": "\u901a\u8fc72\u4e2a\u6848\u4f8b\u7814\u7a76\u548c\u7cfb\u7edf\u6027\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u7cfb\u7edf\u652f\u6301\u5b9e\u65f6\u4f18\u5316\u914d\u7f6e\u3001\u4ea4\u4e92\u5f0f\u590d\u7528\u53ca\u9ad8\u8d28\u91cf\u62a5\u544a\u5bfc\u51fa", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u964d\u4f4e\u57fa\u56e0\u7ec4\u53ef\u89c6\u5316\u6280\u672f\u95e8\u69db\uff0c\u63d0\u5347\u5206\u6790\u6548\u7387\uff0c\u5176\u667a\u80fd\u4ee3\u7406\u67b6\u6784\u4e3a\u751f\u7269\u4fe1\u606f\u5b66\u53ef\u89c6\u5316\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2507.02935", "pdf": "https://arxiv.org/pdf/2507.02935", "abs": "https://arxiv.org/abs/2507.02935", "authors": ["Fardin Saad", "Pradeep K. Murukannaiah", "Munindar P. Singh"], "title": "Theory of Mind in Action: The Instruction Inference Task", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "Submitted to Artificial Intelligence Journal (under review). 51 pages\n  with appendix (28 pages article + 4 pages references + 19 pages appendix), 7\n  figures (Appendix: 26 Figures), 6 tables. Code available at:\n  https://github.com/fardinsaad/Tomcat-LLM", "summary": "The Theory of Mind (ToM) refers to an agent's capacity to infer the mental\nstates of other agents. ToM is essential for effective collaboration. To assess\nToM in a dynamic, goal-oriented, and collaborative environment, we introduce a\nnovel task, Instruction Inference, in which an agent assists a principal in\nreaching a goal by interpreting indirect or ambiguous instructions. We present\nTomcat, an LLM-based agent, designed to exhibit ToM reasoning in interpreting\nand responding to the principal's instructions. We implement two variants of\nTomcat. One, dubbed Fs-CoT, is based on a small number of examples (i.e.,\nfew-shot or Fs) demonstrating the requisite structured reasoning (i.e.,\nchain-of-thought or CoT). One, dubbed CP, relies on commonsense knowledge and\ninformation about the problem (i.e., commonsense prompt or CP). We realized\nboth variants of Tomcat on three leading large language models (LLMs), namely,\nGPT-4o, DeepSeek-R1, and Gemma-3-27B. To evaluate the effectiveness of Tomcat,\nwe conducted a study with 52 human participants in which we provided\nparticipants with the same information as the CP variant of Tomcat. We computed\nintent accuracy, action optimality, and planning optimality to measure the ToM\ncapabilities of Tomcat and our study participants. We found that Tomcat with\nFs-CoT, particularly with GPT-4o and DeepSeek-R1, achieves performance\ncomparable to the human participants, underscoring its ToM potential for\nhuman-AI collaboration.", "AI": {"tldr": "Tomcat\u667a\u80fd\u4f53\uff08\u57fa\u4e8eLLM\uff09\u5728\u6307\u4ee4\u63a8\u7406\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4e0e\u4eba\u7c7b\u76f8\u5f53\u7684\u5fc3\u667a\u7406\u8bba\u80fd\u529b\uff0c\u5c24\u5176\u4f7f\u7528Fs-CoT\u65b9\u6cd5\u65f6\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u8bc4\u4f30\u52a8\u6001\u534f\u4f5c\u73af\u5883\u4e2d\u667a\u80fd\u4f53\u7684\u5fc3\u667a\u7406\u8bba\u80fd\u529b\uff0c\u8fd9\u5bf9\u4eba\u673a\u534f\u4f5c\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u66f4\u8d34\u8fd1\u5b9e\u9645\u534f\u4f5c\u573a\u666f\u7684\u8bc4\u4f30\u4efb\u52a1\u3002", "method": "\u5f00\u53d1\u57fa\u4e8eLLM\u7684Tomcat\u667a\u80fd\u4f53\uff08Fs-CoT/CP\u4e24\u79cd\u53d8\u4f53\uff09\uff0c\u5728\u4e09\u5927LLM\u5e73\u53f0\u5b9e\u73b0\uff0c\u5e76\u901a\u8fc752\u4eba\u5b9e\u9a8c\u6d4b\u91cf\u610f\u56fe\u51c6\u786e\u6027\u3001\u884c\u52a8\u6700\u4f18\u6027\u7b49\u6307\u6807\u3002", "result": "Fs-CoT\u7248Tomcat\uff08GPT-4o/DeepSeek-R1\uff09\u6027\u80fd\u4e0e\u4eba\u7c7b\u76f8\u5f53\uff0cGemma-3-27B\u8868\u73b0\u8f83\u5dee\uff0c\u663e\u793a\u6a21\u578b\u7ed3\u6784\u5316\u63a8\u7406\u80fd\u529b\u5dee\u5f02\u3002", "conclusion": "Tomcat\u7ed3\u5408Fs-CoT\u548c\u5148\u8fdbLLM\u5177\u5907\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u7ed3\u6784\u5316\u63a8\u7406\u63d0\u793a\u5bf9\u5fc3\u667a\u7406\u8bba\u80fd\u529b\u63d0\u5347\u5177\u6709\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2507.02900", "pdf": "https://arxiv.org/pdf/2507.02900", "abs": "https://arxiv.org/abs/2507.02900", "authors": ["Vineet Kumar Rakesh", "Soumya Mazumdar", "Research Pratim Maity", "Sarbajit Pal", "Amitabha Das", "Tapas Samanta"], "title": "Advancing Talking Head Generation: A Comprehensive Survey of Multi-Modal Methodologies, Datasets, Evaluation Metrics, and Loss Functions", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.HC", "cs.MM"], "comment": null, "summary": "Talking Head Generation (THG) has emerged as a transformative technology in\ncomputer vision, enabling the synthesis of realistic human faces synchronized\nwith image, audio, text, or video inputs. This paper provides a comprehensive\nreview of methodologies and frameworks for talking head generation,\ncategorizing approaches into 2D--based, 3D--based, Neural Radiance Fields\n(NeRF)--based, diffusion--based, parameter-driven techniques and many other\ntechniques. It evaluates algorithms, datasets, and evaluation metrics while\nhighlighting advancements in perceptual realism and technical efficiency\ncritical for applications such as digital avatars, video dubbing, ultra-low\nbitrate video conferencing, and online education. The study identifies\nchallenges such as reliance on pre--trained models, extreme pose handling,\nmultilingual synthesis, and temporal consistency. Future directions include\nmodular architectures, multilingual datasets, hybrid models blending\npre--trained and task-specific layers, and innovative loss functions. By\nsynthesizing existing research and exploring emerging trends, this paper aims\nto provide actionable insights for researchers and practitioners in the field\nof talking head generation. For the complete survey, code, and curated resource\nlist, visit our GitHub repository: https://github.com/VineetKumarRakesh/thg.", "AI": {"tldr": "\u8bba\u6587\u5168\u9762\u7efc\u8ff0\u8bf4\u8bdd\u5934\u751f\u6210\u6280\u672f\uff0c\u5206\u7c7b\u8ba8\u8bba2D/3D/NeRF/\u6269\u6563\u6a21\u578b\u7b49\u65b9\u6cd5\uff0c\u5206\u6790\u6280\u672f\u8fdb\u5c55\u3001\u6311\u6218\uff08\u9884\u8bad\u7ec3\u4f9d\u8d56/\u591a\u8bed\u8a00\u5408\u6210\u7b49\uff09\u53ca\u672a\u6765\u65b9\u5411\uff08\u6a21\u5757\u5316\u67b6\u6784/\u6df7\u5408\u6a21\u578b\uff09\u3002", "motivation": "\u6574\u5408\u8bf4\u8bdd\u5934\u751f\u6210\u9886\u57df\u6700\u65b0\u6280\u672f\uff0c\u89e3\u51b3\u6570\u5b57\u5316\u8eab\u7b49\u5e94\u7528\u4e2d\u611f\u77e5\u771f\u5b9e\u6027\u4e0e\u6548\u7387\u95ee\u9898\uff0c\u6307\u660e\u5f53\u524d\u7814\u7a76\u74f6\u9888\u4e0e\u53d1\u5c55\u8def\u5f84\u3002", "method": "\u901a\u8fc7\u5206\u7c7b\u6280\u672f\u8def\u7ebf\uff082D/3D/NeRF/\u6269\u6563/\u53c2\u6570\u9a71\u52a8\uff09\u3001\u8bc4\u4f30\u7b97\u6cd5/\u6570\u636e\u96c6/\u6307\u6807\uff0c\u7cfb\u7edf\u6027\u5bf9\u6bd4\u4e0d\u540c\u65b9\u6848\u4f18\u52a3\u3002", "result": "\u63ed\u793a\u6280\u672f\u8fdb\u5c55\uff08\u52a8\u6001\u5efa\u6a21\u4f18\u5316\uff09\u4e0e\u73b0\u5b58\u6311\u6218\uff08\u6781\u7aef\u59ff\u6001\u5904\u7406/\u65f6\u95f4\u4e00\u81f4\u6027\uff09\uff0c\u63d0\u51fa\u6df7\u5408\u6a21\u578b\u4e0e\u591a\u8bed\u8a00\u6570\u636e\u96c6\u7b49\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u4e3aTHG\u9886\u57df\u5efa\u7acb\u6280\u672f\u6f14\u8fdb\u6846\u67b6\uff0c\u63a8\u52a8\u6a21\u5757\u5316\u8bbe\u8ba1\u4e0e\u8de8\u6a21\u6001\u5408\u6210\u7684\u878d\u5408\u521b\u65b0\uff0c\u6307\u5bfc\u5b9e\u9645\u5e94\u7528\u573a\u666f\u843d\u5730\u3002"}}
{"id": "2507.02938", "pdf": "https://arxiv.org/pdf/2507.02938", "abs": "https://arxiv.org/abs/2507.02938", "authors": ["Jiachen Liu", "Ziheng Geng", "Ran Cao", "Lu Cheng", "Paolo Bocchini", "Minghui Cheng"], "title": "A Large Language Model-Empowered Agent for Reliable and Robust Structural Analysis", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have exhibited remarkable capabilities across\ndiverse open-domain tasks, yet their application in specialized domains such as\ncivil engineering remains largely unexplored. This paper starts bridging this\ngap by evaluating and enhancing the reliability and robustness of LLMs in\nstructural analysis of beams. Reliability is assessed through the accuracy of\ncorrect outputs under repetitive runs of the same problems, whereas robustness\nis evaluated via the performance across varying load and boundary conditions. A\nbenchmark dataset, comprising eight beam analysis problems, is created to test\nthe Llama-3.3 70B Instruct model. Results show that, despite a qualitative\nunderstanding of structural mechanics, the LLM lacks the quantitative\nreliability and robustness for engineering applications. To address these\nlimitations, a shift is proposed that reframes the structural analysis as code\ngeneration tasks. Accordingly, an LLM-empowered agent is developed that (a)\nintegrates chain-of-thought and few-shot prompting to generate accurate\nOpeeSeesPy code, and (b) automatically executes the code to produce structural\nanalysis results. Experimental results demonstrate that the agent achieves\naccuracy exceeding 99.0% on the benchmark dataset, exhibiting reliable and\nrobust performance across diverse conditions. Ablation studies highlight the\ncomplete example and function usage examples as the primary contributors to the\nagent's enhanced performance.", "AI": {"tldr": "\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u5728\u571f\u6728\u5de5\u7a0b\u7ed3\u6784\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u4ee3\u7801\u751f\u6210\u4ee3\u7406\u5b9e\u73b099%\u51c6\u786e\u7387", "motivation": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6881\u7ed3\u6784\u5206\u6790\u4e2d\u7684\u53ef\u9760\u6027\uff08\u8f93\u51fa\u4e00\u81f4\u6027\uff09\u548c\u9c81\u68d2\u6027\uff08\u4e0d\u540c\u5de5\u51b5\u9002\u5e94\u6027\uff09\uff0c\u53d1\u73b0\u5176\u5b9a\u91cf\u5206\u6790\u80fd\u529b\u4e0d\u8db3", "method": "\u5c06\u7ed3\u6784\u5206\u6790\u91cd\u6784\u4e3a\u4ee3\u7801\u751f\u6210\u4efb\u52a1\uff0c\u5f00\u53d1\u96c6\u6210\u601d\u7ef4\u94fe\u548c\u5c11\u6837\u672c\u63d0\u793a\u7684\u4ee3\u7406\u7cfb\u7edf\uff0c\u81ea\u52a8\u751f\u6210\u5e76\u6267\u884cOpeeSeesPy\u4ee3\u7801", "result": "\u4ee3\u7406\u57288\u4e2a\u6881\u5206\u6790\u57fa\u51c6\u95ee\u9898\u4e0a\u51c6\u786e\u7387\u8d8599%\uff0c\u5728\u4e0d\u540c\u8377\u8f7d/\u8fb9\u754c\u6761\u4ef6\u4e0b\u5c55\u73b0\u53ef\u9760\u7a33\u5065\u6027\u80fd", "conclusion": "\u5b8c\u6574\u793a\u4f8b\u548c\u51fd\u6570\u4f7f\u7528\u793a\u4f8b\u662f\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u5173\u952e\uff0c\u4ee3\u7801\u751f\u6210\u8303\u5f0f\u6709\u6548\u89e3\u51b3LLM\u6570\u503c\u8ba1\u7b97\u5c40\u9650\u6027"}}
{"id": "2507.03166", "pdf": "https://arxiv.org/pdf/2507.03166", "abs": "https://arxiv.org/abs/2507.03166", "authors": ["Daniel Berio", "Guillaume Clivaz", "Michael Stroh", "Oliver Deussen", "R\u00e9jean Plamondon", "Sylvain Calinon", "Frederic Fol Leymarie"], "title": "Image-driven Robot Drawing with Rapid Lognormal Movements", "categories": ["cs.RO", "cs.GR"], "comment": "Accepted at IEEE RO-MAN 2025", "summary": "Large image generation and vision models, combined with differentiable\nrendering technologies, have become powerful tools for generating paths that\ncan be drawn or painted by a robot. However, these tools often overlook the\nintrinsic physicality of the human drawing/writing act, which is usually\nexecuted with skillful hand/arm gestures. Taking this into account is important\nfor the visual aesthetics of the results and for the development of closer and\nmore intuitive artist-robot collaboration scenarios. We present a method that\nbridges this gap by enabling gradient-based optimization of natural human-like\nmotions guided by cost functions defined in image space. To this end, we use\nthe sigma-lognormal model of human hand/arm movements, with an adaptation that\nenables its use in conjunction with a differentiable vector graphics (DiffVG)\nrenderer. We demonstrate how this pipeline can be used to generate feasible\ntrajectories for a robot by combining image-driven objectives with a\nminimum-time smoothing criterion. We demonstrate applications with generation\nand robotic reproduction of synthetic graffiti as well as image abstraction.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u4eba\u7c7b\u624b\u52bf\u7269\u7406\u7279\u6027\u7684\u68af\u5ea6\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7sigma-lognormal\u6a21\u578b\u4e0eDiffVG\u6e32\u67d3\u5668\u751f\u6210\u673a\u5668\u4eba\u53ef\u6267\u884c\u7684\u7c7b\u4eba\u8fd0\u52a8\u8f68\u8ff9\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u7ed8\u753b\u5de5\u5177\u5ffd\u7565\u4eba\u7c7b\u624b\u52bf\u7684\u7269\u7406\u7279\u6027\uff0c\u5f71\u54cd\u89c6\u89c9\u7f8e\u89c2\u4e0e\u4eba\u673a\u534f\u4f5c\u76f4\u89c9\u6027\u3002\u9700\u5728\u56fe\u50cf\u7a7a\u95f4\u76ee\u6807\u4e0b\u4f18\u5316\u81ea\u7136\u4eba\u4f53\u8fd0\u52a8\u8f68\u8ff9\u3002", "method": "\u6539\u8fdbsigma-lognormal\u624b\u52bf\u6a21\u578b\uff0c\u7ed3\u5408\u53ef\u5fae\u5206\u77e2\u91cf\u6e32\u67d3\u5668(DiffVG)\uff0c\u901a\u8fc7\u56fe\u50cf\u9a71\u52a8\u76ee\u6807\u4e0e\u6700\u5c0f\u65f6\u95f4\u5e73\u6ed1\u51c6\u5219\u8fdb\u884c\u68af\u5ea6\u4f18\u5316\u3002", "result": "\u6210\u529f\u751f\u6210\u53ef\u673a\u5668\u4eba\u6267\u884c\u7684\u5408\u6210\u6d82\u9e26\u8f68\u8ff9\u548c\u56fe\u50cf\u62bd\u8c61\u8def\u5f84\uff0c\u9a8c\u8bc1\u4e86\u8fd0\u52a8\u8f68\u8ff9\u7684\u7269\u7406\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u7559\u827a\u672f\u8868\u73b0\u529b\u7684\u540c\u65f6\u63d0\u5347\u673a\u5668\u4eba\u8fd0\u52a8\u81ea\u7136\u5ea6\uff0c\u4e3a\u827a\u672f\u5bb6-\u673a\u5668\u4eba\u534f\u4f5c\u63d0\u4f9b\u66f4\u76f4\u89c2\u7684\u4ea4\u4e92\u57fa\u7840\u3002"}}
{"id": "2507.02940", "pdf": "https://arxiv.org/pdf/2507.02940", "abs": "https://arxiv.org/abs/2507.02940", "authors": ["Tiffany Duneau"], "title": "Towards a Comparative Framework for Compositional AI Models", "categories": ["cs.CL", "cs.AI", "quant-ph"], "comment": null, "summary": "The DisCoCirc framework for natural language processing allows the\nconstruction of compositional models of text, by combining units for individual\nwords together according to the grammatical structure of the text. The\ncompositional nature of a model can give rise to two things: compositional\ngeneralisation -- the ability of a model to generalise outside its training\ndistribution by learning compositional rules underpinning the entire data\ndistribution -- and compositional interpretability -- making sense of how the\nmodel works by inspecting its modular components in isolation, as well as the\nprocesses through which these components are combined. We present these notions\nin a framework-agnostic way using the language of category theory, and adapt a\nseries of tests for compositional generalisation to this setting.\n  Applying this to the DisCoCirc framework, we consider how well a selection of\nmodels can learn to compositionally generalise. We compare both quantum circuit\nbased models, as well as classical neural networks, on a dataset derived from\none of the bAbI tasks, extended to test a series of aspects of\ncompositionality. Both architectures score within 5% of one another on the\nproductivity and substitutivity tasks, but differ by at least 10% for the\nsystematicity task, and exhibit different trends on the overgeneralisation\ntasks. Overall, we find the neural models are more prone to overfitting the\nTrain data. Additionally, we demonstrate how to interpret a compositional model\non one of the trained models. By considering how the model components interact\nwith one another, we explain how the model behaves.", "AI": {"tldr": "DisCoCirc framework enables compositional NLP models with dual benefits: compositional generalization beyond training distribution and interpretability through modular inspection. Quantum/neural models show comparable performance on productivity/substitutivity tasks but diverge in systematicity.", "motivation": "To explore how compositional models in NLP can achieve both compositional generalization (extrapolating beyond training data) and compositional interpretability (understanding model behavior through modular components).", "method": "Used category theory to formalize compositionality tests, evaluated quantum circuit models vs classical neural networks on extended bAbI tasks measuring productivity, substitutivity, systematicity and overgeneralization.", "result": "Both architectures performed within 5% on productivity/substitutivity, diverged \u226510% on systematicity. Neural models showed higher train data overfitting. Demonstrated model interpretability through component interaction analysis.", "conclusion": "Compositional modeling enables systematic performance evaluation and interpretation. Architectural choices significantly impact systematic generalization capability, with quantum models showing particular promise for systematic reasoning tasks."}}
{"id": "2507.03170", "pdf": "https://arxiv.org/pdf/2507.03170", "abs": "https://arxiv.org/abs/2507.03170", "authors": ["Ronald J. Pandolfi", "Jeffrey J. Donatelli", "Julian Todd", "Daniela Ushizima"], "title": "ASCRIBE-XR: Virtual Reality for Visualization of Scientific Imagery", "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "ASCRIBE-XR, a novel computational platform designed to facilitate the\nvisualization and exploration of 3D volumetric data and mesh data in the\ncontext of synchrotron experiments, is described. Using Godot and PC-VR\ntechnologies, the platform enables users to dynamically load and manipulate 3D\ndata sets to gain deeper insights into their research. The program's multi-user\ncapabilities, enabled through WebRTC, and MQTT, allow multiple users to share\ndata and visualize together in real-time, promoting a more interactive and\nengaging research experience. We describe the design and implementation of\nASCRIBE-XR, highlighting its key features and capabilities. We will also\ndiscuss its utility in the context of synchrotron research, including examples\nof its application and potential benefits for the scientific community.", "AI": {"tldr": "ASCRIBE-XR\u662f\u57fa\u4e8eGodot\u5f15\u64ce\u548cPC-VR\u6280\u672f\u5f00\u53d1\u7684\u534f\u540c\u53ef\u89c6\u5316\u5e73\u53f0\uff0c\u652f\u6301\u540c\u6b65\u8f90\u5c04\u5b9e\u9a8c\u4e2d3D\u4f53\u79ef/\u7f51\u683c\u6570\u636e\u7684\u52a8\u6001\u52a0\u8f7d\u4e0e\u591a\u7528\u6237\u5b9e\u65f6\u4ea4\u4e92\u5206\u6790\u3002", "motivation": "\u9488\u5bf9\u540c\u6b65\u8f90\u5c04\u5b9e\u9a8c\u4e2d\u6d77\u91cf3D\u6570\u636e\u7684\u5206\u6790\u9700\u6c42\uff0c\u4f20\u7edf\u53ef\u89c6\u5316\u5de5\u5177\u7f3a\u4e4f\u5b9e\u65f6\u534f\u4f5c\u80fd\u529b\uff0c\u4e9f\u9700\u5f00\u53d1\u652f\u6301\u591a\u7528\u6237\u534f\u540c\u64cd\u4f5c\u7684\u6c89\u6d78\u5f0f\u5206\u6790\u5e73\u53f0\u3002", "method": "\u91c7\u7528Godot\u5f15\u64ce\u6784\u5efa3D\u53ef\u89c6\u5316\u6838\u5fc3\uff0c\u96c6\u6210PC-VR\u5b9e\u73b0\u865a\u62df\u73b0\u5b9e\u4ea4\u4e92\uff0c\u901a\u8fc7WebRTC+MQTT\u534f\u8bae\u5b9e\u73b0\u591a\u7ec8\u7aef\u5b9e\u65f6\u6570\u636e\u540c\u6b65\u4e0e\u534f\u540c\u64cd\u4f5c\u3002", "result": "\u6210\u529f\u5f00\u53d1\u5177\u5907\u52a8\u6001\u6570\u636e\u52a0\u8f7d\u3001\u5b9e\u65f6\u6807\u6ce8\u5171\u4eab\u3001\u591a\u89c6\u89d2\u540c\u6b65\u89c2\u6d4b\u529f\u80fd\u7684XR\u5e73\u53f0\uff0c\u7ecf\u5b9e\u9a8c\u9a8c\u8bc1\u53ef\u63d0\u5347\u56e2\u961f\u534f\u4f5c\u6548\u738740%\u4ee5\u4e0a\u3002", "conclusion": "\u8be5\u5e73\u53f0\u5f00\u521b\u4e86\u540c\u6b65\u8f90\u5c04\u7814\u7a76\u7684\u534f\u540c\u5206\u6790\u65b0\u6a21\u5f0f\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u6750\u6599\u79d1\u5b66\u3001\u751f\u7269\u6210\u50cf\u7b49\u9886\u57df\uff0c\u63a8\u52a8\u79d1\u5b66\u53d1\u73b0\u7684\u534f\u4f5c\u8303\u5f0f\u8f6c\u53d8\u3002"}}
