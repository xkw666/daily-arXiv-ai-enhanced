<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 39]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.AI](#cs.AI) [Total: 6]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.SE](#cs.SE) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Gender-Neutral Rewriting in Italian: Models, Approaches, and Trade-offs](https://arxiv.org/abs/2509.13480)
*Andrea Piergentili,Beatrice Savoldi,Matteo Negri,Luisa Bentivogli*

Main category: cs.CL

TL;DR: 系统评估意大利语中性化改写任务的LLM表现，提出二维评估框架并优化模型性能


<details>
  <summary>Details</summary>
Motivation: 意大利语等语法性别语言的中性化改写存在技术挑战，需系统评估现有模型并提升任务表现

Method: 采用少量样本提示、模型微调和针对性数据清洗，对比不同LLM的性能表现

Result: 开源模型超越现有专用模型，微调后的小型模型达到同等效果且计算成本更低

Conclusion: 中性化改写需平衡语义保持与中性化优化的矛盾，模型优化需兼顾数据质量与任务目标

Abstract: Gender-neutral rewriting (GNR) aims to reformulate text to eliminate
unnecessary gender specifications while preserving meaning, a particularly
challenging task in grammatical-gender languages like Italian. In this work, we
conduct the first systematic evaluation of state-of-the-art large language
models (LLMs) for Italian GNR, introducing a two-dimensional framework that
measures both neutrality and semantic fidelity to the input. We compare
few-shot prompting across multiple LLMs, fine-tune selected models, and apply
targeted cleaning to boost task relevance. Our findings show that open-weight
LLMs outperform the only existing model dedicated to GNR in Italian, whereas
our fine-tuned models match or exceed the best open-weight LLM's performance at
a fraction of its size. Finally, we discuss the trade-off between optimizing
the training data for neutrality and meaning preservation.

</details>


### [2] [Op-Fed: Opinion, Stance, and Monetary Policy Annotations on FOMC Transcripts Using Active Learning](https://arxiv.org/abs/2509.13539)
*Alisa Kanganis,Katherine A. Keith*

Main category: cs.CL

TL;DR: Op-Fed数据集（1044条标注语句）用于分析FOMC会议记录，通过五阶段分层标注方案和主动学习解决类别不平衡与上下文依赖问题，LLM在货币政策立场分类（0.61）表现低于人类基线（0.89）。


<details>
  <summary>Details</summary>
Motivation: 解决FOMC会议记录分析中的两大挑战：1. 货币政策立场类别高度不平衡（<8%非中性）2. 65%实例需跨句子上下文理解

Method: 1. 开发五阶段分层标注框架（观点-货币政策-立场）
2. 采用主动学习策略标注样本，使各维度正样本数量翻倍

Result: 1. 闭源LLM观点分类准确率0.80
2. 货币政策立场分类准确率仅0.61（人类基线0.89）
3. 主动学习使正样本数量提升约2倍

Conclusion: Op-Fed可作为模型训练、置信度校准的基础数据集，其上下文依赖特性突显当前LLM在复杂语境理解上的局限性

Abstract: The U.S. Federal Open Market Committee (FOMC) regularly discusses and sets
monetary policy, affecting the borrowing and spending decisions of millions of
people. In this work, we release Op-Fed, a dataset of 1044 human-annotated
sentences and their contexts from FOMC transcripts. We faced two major
technical challenges in dataset creation: imbalanced classes -- we estimate
fewer than 8% of sentences express a non-neutral stance towards monetary policy
-- and inter-sentence dependence -- 65% of instances require context beyond the
sentence-level. To address these challenges, we developed a five-stage
hierarchical schema to isolate aspects of opinion, monetary policy, and stance
towards monetary policy as well as the level of context needed. Second, we
selected instances to annotate using active learning, roughly doubling the
number of positive instances across all schema aspects. Using Op-Fed, we found
a top-performing, closed-weight LLM achieves 0.80 zero-shot accuracy in opinion
classification but only 0.61 zero-shot accuracy classifying stance towards
monetary policy -- below our human baseline of 0.89. We expect Op-Fed to be
useful for future model training, confidence calibration, and as a seed dataset
for future annotation efforts.

</details>


### [3] [Overview of Dialog System Evaluation Track: Dimensionality, Language, Culture and Safety at DSTC 12](https://arxiv.org/abs/2509.13569)
*John Mendonça,Lining Zhang,Rahul Mallidi,Alon Lavie,Isabel Trancoso,Luis Fernando D'Haro,João Sedoc*

Main category: cs.CL

TL;DR: DSTC12 Track 1通过两个子任务（多维度对话评估与多文化安全检测）揭示对话系统评估的现存挑战，Llama模型基线表现显示评估指标与文化感知仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 传统对话系统评估指标存在不足，安全考量存在文化偏见，需建立更全面的多维度、多文化评估体系。

Method: 设计两个子任务：1）基于Llama-3-8B的10维度对话自动评估；2）多语言/文化安全检测，对比参与者模型与Llama-Guard-3-1B基线。

Result: 任务1基线模型Spearman相关系数0.1681；任务2多语言安全检测最优ROC-AUC达0.9648，但文化子集基线0.5126 ROC-AUC仍占优。

Conclusion: 当前自动评估指标有效性有限，文化敏感性安全检测机制亟待加强，需开发更具文化适应性的评估框架。

Abstract: The rapid advancement of Large Language Models (LLMs) has intensified the
need for robust dialogue system evaluation, yet comprehensive assessment
remains challenging. Traditional metrics often prove insufficient, and safety
considerations are frequently narrowly defined or culturally biased. The DSTC12
Track 1, "Dialog System Evaluation: Dimensionality, Language, Culture and
Safety," is part of the ongoing effort to address these critical gaps. The
track comprised two subtasks: (1) Dialogue-level, Multi-dimensional Automatic
Evaluation Metrics, and (2) Multilingual and Multicultural Safety Detection.
For Task 1, focused on 10 dialogue dimensions, a Llama-3-8B baseline achieved
the highest average Spearman's correlation (0.1681), indicating substantial
room for improvement. In Task 2, while participating teams significantly
outperformed a Llama-Guard-3-1B baseline on the multilingual safety subset (top
ROC-AUC 0.9648), the baseline proved superior on the cultural subset (0.5126
ROC-AUC), highlighting critical needs in culturally-aware safety. This paper
describes the datasets and baselines provided to participants, as well as
submission evaluation results for each of the two proposed subtasks.

</details>


### [4] [Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning](https://arxiv.org/abs/2509.13624)
*Shambhavi Krishna,Atharva Naik,Chaitali Agarwal,Sudharshan Govindan,Taesung Lee,Haw-Shiuan Chang*

Main category: cs.CL

TL;DR: 研究通过构建迁移学习矩阵和降维方法，揭示了大型语言模型跨任务适应中隐藏统计因素（如类别分布、生成长度倾向）对性能的实际影响，而非表面数据特征。


<details>
  <summary>Details</summary>
Motivation: 解决LLM处理未训练任务时依赖迁移学习的有效性难题，探索数据特征与模型能力迁移之间难以解释的关联本质。

Method: 构建迁移学习矩阵分析框架，训练10个模型识别潜在能力（推理/情感分类等），通过统计分析和降维技术量化隐藏因素（如类分布、文本长度）的影响。

Result: 性能提升与源数据的表面相似性无关，主要由类分布均衡性、文本生成长度偏好等隐藏统计特征及特定语言结构驱动。

Conclusion: 为LLM迁移学习的动态复杂性提供可解释框架，指导通过数据底层特征优化实现更可控的模型适应。

Abstract: Large language models are increasingly deployed across diverse applications.
This often includes tasks LLMs have not encountered during training. This
implies that enumerating and obtaining the high-quality training data for all
tasks is infeasible. Thus, we often need to rely on transfer learning using
datasets with different characteristics, and anticipate out-of-distribution
requests. Motivated by this practical need, we propose an analysis framework,
building a transfer learning matrix and dimensionality reduction, to dissect
these cross-task interactions. We train and analyze 10 models to identify
latent abilities (e.g., Reasoning, Sentiment Classification, NLU, Arithmetic)
and discover the side effects of the transfer learning. Our findings reveal
that performance improvements often defy explanations based on surface-level
dataset similarity or source data quality. Instead, hidden statistical factors
of the source dataset, such as class distribution and generation length
proclivities, alongside specific linguistic features, are actually more
influential. This work offers insights into the complex dynamics of transfer
learning, paving the way for more predictable and effective LLM adaptation.

</details>


### [5] [Sparse Neurons Carry Strong Signals of Question Ambiguity in LLMs](https://arxiv.org/abs/2509.13664)
*Zhuoxuan Zhang,Jinhao Duan,Edward Kim,Kaidi Xu*

Main category: cs.CL

TL;DR: 发现LLMs通过浅层Ambiguity-Encoding Neurons线性编码问题歧义，通过操控这些神经元可实现从直接回答到主动回避的行为控制。


<details>
  <summary>Details</summary>
Motivation: 现实问题普遍存在歧义性，但LLMs往往直接回答而非澄清，可能导致错误回应。研究旨在揭示LLMs内部如何表征歧义并实现可控响应。

Method: 1. 识别预填充阶段的歧义编码神经元(AENs)
2. 构建基于AENs的探测器
3. 层次化分析神经元分布
4. 神经元操控实验改变模型行为

Result: AENs探测器性能超越基线方法，跨数据集泛化能力显著；操控单个AEN即可实现回答策略切换（准确回答→主动回避）

Conclusion: LLMs通过紧凑的神经元级表征编码歧义信息，这种机制为实现模型行为的可解释控制提供了新方向。

Abstract: Ambiguity is pervasive in real-world questions, yet large language models
(LLMs) often respond with confident answers rather than seeking clarification.
In this work, we show that question ambiguity is linearly encoded in the
internal representations of LLMs and can be both detected and controlled at the
neuron level. During the model's pre-filling stage, we identify that a small
number of neurons, as few as one, encode question ambiguity information. Probes
trained on these Ambiguity-Encoding Neurons (AENs) achieve strong performance
on ambiguity detection and generalize across datasets, outperforming
prompting-based and representation-based baselines. Layerwise analysis reveals
that AENs emerge from shallow layers, suggesting early encoding of ambiguity
signals in the model's processing pipeline. Finally, we show that through
manipulating AENs, we can control LLM's behavior from direct answering to
abstention. Our findings reveal that LLMs form compact internal representations
of question ambiguity, enabling interpretable and controllable behavior.

</details>


### [6] [CL$^2$GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese Literature Grammatical Error Correction](https://arxiv.org/abs/2509.13672)
*Shang Qin,Jingheng Ye,Yinghui Li,Hai-Tao Zheng,Qi Li,Jinxiao Shan,Zhixing Li,Hong-Gee Kim*

Main category: cs.CL

TL;DR: 提出首个中文文献语法纠错持续学习基准CL²GEC，包含跨10个学科的1万条标注数据，实验表明正则化方法在持续学习场景下更有效缓解遗忘问题


<details>
  <summary>Details</summary>
Motivation: 现有中文语法纠错研究缺乏跨学科持续学习基准，无法有效应对实际学术编辑中的领域特异性语言变异和灾难性遗忘问题

Method: 构建包含10个学科领域1万句标注数据的CL²GEC基准，通过顺序调参、参数高效适配和4种典型持续学习算法进行系统性评估

Result: 正则化方法（如EWC）在任务级变异场景下比基于回放或简单顺序调参方法表现出更好的遗忘缓解效果（平均遗忘率降低15.7%）

Conclusion: CL²GEC为跨学科自适应语法纠错研究建立严格评估框架，未来可结合参数高效方法与正则化策略提升模型持续学习能力

Abstract: The growing demand for automated writing assistance in diverse academic
domains highlights the need for robust Chinese Grammatical Error Correction
(CGEC) systems that can adapt across disciplines. However, existing CGEC
research largely lacks dedicated benchmarks for multi-disciplinary academic
writing, overlooking continual learning (CL) as a promising solution to handle
domain-specific linguistic variation and prevent catastrophic forgetting. To
fill this crucial gap, we introduce CL$^2$GEC, the first Continual Learning
benchmark for Chinese Literature Grammatical Error Correction, designed to
evaluate adaptive CGEC across multiple academic fields. Our benchmark includes
10,000 human-annotated sentences spanning 10 disciplines, each exhibiting
distinct linguistic styles and error patterns. CL$^2$GEC focuses on evaluating
grammatical error correction in a continual learning setting, simulating
sequential exposure to diverse academic disciplines to reflect real-world
editorial dynamics. We evaluate large language models under sequential tuning,
parameter-efficient adaptation, and four representative CL algorithms, using
both standard GEC metrics and continual learning metrics adapted to task-level
variation. Experimental results reveal that regularization-based methods
mitigate forgetting more effectively than replay-based or naive sequential
approaches. Our benchmark provides a rigorous foundation for future research in
adaptive grammatical error correction across diverse academic domains.

</details>


### [7] [AgentCTG: Harnessing Multi-Agent Collaboration for Fine-Grained Precise Control in Text Generation](https://arxiv.org/abs/2509.13677)
*Xinxu Zhou,Jiaqi Bai,Zhenqi Sun,Fanxiang Zeng,Yue Liu*

Main category: cs.CL

TL;DR: 提出AgentCTG框架，通过多智能体协作机制增强文本生成的精确控制，在公开数据集和角色驱动改写任务中实现最优效果


<details>
  <summary>Details</summary>
Motivation: 解决受控文本生成在细粒度控制、成本效益、可扩展性及领域知识整合方面的核心挑战

Method: 构建多智能体工作流模拟人类协作机制，引入自动提示模块优化生成效果，支持动态角色配置和知识融合

Result: 在公开数据集达到SOTA，角色驱动改写任务提升在线导航角色扮演场景中83%的用户沉浸体验评分

Conclusion: 该框架通过优化上下文感知生成，显著增强在线社区的个性化交互，驱动用户参与度提升和领域知识保留

Abstract: Although significant progress has been made in many tasks within the field of
Natural Language Processing (NLP), Controlled Text Generation (CTG) continues
to face numerous challenges, particularly in achieving fine-grained conditional
control over generation. Additionally, in real scenario and online
applications, cost considerations, scalability, domain knowledge learning and
more precise control are required, presenting more challenge for CTG. This
paper introduces a novel and scalable framework, AgentCTG, which aims to
enhance precise and complex control over the text generation by simulating the
control and regulation mechanisms in multi-agent workflows. We explore various
collaboration methods among different agents and introduce an auto-prompt
module to further enhance the generation effectiveness. AgentCTG achieves
state-of-the-art results on multiple public datasets. To validate its
effectiveness in practical applications, we propose a new challenging
Character-Driven Rewriting task, which aims to convert the original text into
new text that conform to specific character profiles and simultaneously
preserve the domain knowledge. When applied to online navigation with
role-playing, our approach significantly enhances the driving experience
through improved content delivery. By optimizing the generation of contextually
relevant text, we enable a more immersive interaction within online
communities, fostering greater personalization and user engagement.

</details>


### [8] [Improving Context Fidelity via Native Retrieval-Augmented Reasoning](https://arxiv.org/abs/2509.13683)
*Suyuchen Wang,Jinlin Wang,Xinyu Wang,Shiqi Li,Xiangru Tang,Sirui Hong,Xiao-Wen Chang,Chenglin Wu,Bang Liu*

Main category: cs.CL

TL;DR: 提出CARE框架，通过自检索能力提升大语言模型在知识密集型任务中的准确性和可靠性


<details>
  <summary>Details</summary>
Motivation: 现有方法存在监督微调成本高、传统检索增强方法未能有效利用上下文信息的问题

Method: 通过自检索机制在推理链中显式整合上下文证据，利用少量标注数据实现检索与生成的双提升

Result: 在多个真实/反事实QA基准测试中显著超越监督微调、传统RAG方法和外部检索方案

Conclusion: 该框架代表了大语言模型在知识密集型任务中可靠性提升的根本性进步

Abstract: Large language models (LLMs) often struggle with context fidelity, producing
inconsistent answers when responding to questions based on provided
information. Existing approaches either rely on expensive supervised
fine-tuning to generate evidence post-answer or train models to perform web
searches without necessarily improving utilization of the given context. We
propose CARE, a novel native retrieval-augmented reasoning framework that
teaches LLMs to explicitly integrate in-context evidence within their reasoning
process with the model's own retrieval capabilities. Our method requires
limited labeled evidence data while significantly enhancing both retrieval
accuracy and answer generation performance through strategically retrieved
in-context tokens in the reasoning chain. Extensive experiments on multiple
real-world and counterfactual QA benchmarks demonstrate that our approach
substantially outperforms supervised fine-tuning, traditional
retrieval-augmented generation methods, and external retrieval solutions. This
work represents a fundamental advancement in making LLMs more accurate,
reliable, and efficient for knowledge-intensive tasks.

</details>


### [9] [Can Large Language Models Robustly Perform Natural Language Inference for Japanese Comparatives?](https://arxiv.org/abs/2509.13695)
*Yosuke Mikami,Daiki Matsuoka,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 研究构建了日语比较结构NLI数据集，发现LLMs在零样本/少样本场景下对提示格式敏感且受标注数据影响，逻辑语义表示能有效提升推理效果。


<details>
  <summary>Details</summary>
Motivation: 填补LLMs在处理日语比较结构推理任务中的研究空白，评估其跨语言推理能力及鲁棒性。

Method: 构建日语比较结构NLI数据集，在零样本/少样本设置下测试不同提示格式，并引入逻辑语义表示辅助推理。

Result: 模型对零样本提示格式敏感，少样本示例的标注质量显著影响表现；处理日语特有现象存在困难，但逻辑语义表示能解决部分少样本无法解决的问题。

Conclusion: LLMs跨语言推理能力需针对性优化，融入形式化语义表示是提升复杂逻辑推理的有效路径。

Abstract: Large Language Models (LLMs) perform remarkably well in Natural Language
Inference (NLI). However, NLI involving numerical and logical expressions
remains challenging. Comparatives are a key linguistic phenomenon related to
such inference, but the robustness of LLMs in handling them, especially in
languages that are not dominant in the models' training data, such as Japanese,
has not been sufficiently explored. To address this gap, we construct a
Japanese NLI dataset that focuses on comparatives and evaluate various LLMs in
zero-shot and few-shot settings. Our results show that the performance of the
models is sensitive to the prompt formats in the zero-shot setting and
influenced by the gold labels in the few-shot examples. The LLMs also struggle
to handle linguistic phenomena unique to Japanese. Furthermore, we observe that
prompts containing logical semantic representations help the models predict the
correct labels for inference problems that they struggle to solve even with
few-shot examples.

</details>


### [10] [Integrating Text and Time-Series into (Large) Language Models to Predict Medical Outcomes](https://arxiv.org/abs/2509.13696)
*Iyadh Ben Cheikh Larbi,Ajay Madhavan Ravichandran,Aljoscha Burchardt,Roland Roller*

Main category: cs.CL

TL;DR: 指令调优的LLMs通过DSPy提示优化处理临床文本和结构化数据，在保持性能的同时降低系统复杂度


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在临床结构化数据处理中的局限性，提升多模态医疗任务适应性

Method: 基于DSPy的提示优化方法整合临床笔记和电子健康记录（EHR）结构化数据

Result: 达到与专用多模态系统相当的性能，系统复杂度降低且跨任务适应性更强

Conclusion: 提示优化方法为医疗AI系统提供更灵活高效的解决方案

Abstract: Large language models (LLMs) excel at text generation, but their ability to
handle clinical classification tasks involving structured data, such as time
series, remains underexplored. In this work, we adapt instruction-tuned LLMs
using DSPy-based prompt optimization to process clinical notes and structured
EHR inputs jointly. Our results show that this approach achieves performance on
par with specialized multimodal systems while requiring less complexity and
offering greater adaptability across tasks.

</details>


### [11] [DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models](https://arxiv.org/abs/2509.13702)
*Xiao Zheng*

Main category: cs.CL

TL;DR: 提出DSCC-HS框架，通过对抗训练的双代理模型实时动态修正LLM生成内容，显著提升事实性指标


<details>
  <summary>Details</summary>
Motivation: 现有RAG等方法是被动修正，需要主动干预LLM的生成过程来抑制幻觉

Method: 基于双过程认知理论，训练FAP（事实对齐）和HDP（幻觉检测）代理模型，在自回归解码时注入实时修正向量（FAP与HDP logit差值）

Result: TruthfulQA实现99.2%事实一致率，BioGEN长文本生成获得46.50 FActScore（当前SOTA）

Conclusion: DSCC-HS首次实现无需修改目标模型的即插即用式幻觉抑制，为提升LLM事实性提供理论化解决方案

Abstract: Large Language Model (LLM) hallucination is a significant barrier to their
reliable deployment. Current methods like Retrieval-Augmented Generation (RAG)
are often reactive. We introduce **Dynamic Self-reinforcing Calibration for
Hallucination Suppression (DSCC-HS)**, a novel, proactive framework that
intervenes during autoregressive decoding. Inspired by dual-process cognitive
theory, DSCC-HS uses a compact proxy model, trained in adversarial roles as a
Factual Alignment Proxy (FAP) and a Hallucination Detection Proxy (HDP). During
inference, these proxies dynamically steer a large target model by injecting a
real-time steering vector, which is the difference between FAP and HDP logits,
at each decoding step. This plug-and-play approach requires no modification to
the target model. Our experiments on TruthfulQA and BioGEN show DSCC-HS
achieves state-of-the-art performance. On TruthfulQA, it reached a 99.2%
Factual Consistency Rate (FCR). On the long-form BioGEN benchmark, it attained
the highest FActScore of 46.50. These results validate DSCC-HS as a principled
and efficient solution for enhancing LLM factuality.

</details>


### [12] [Automated Triaging and Transfer Learning of Incident Learning Safety Reports Using Large Language Representational Models](https://arxiv.org/abs/2509.13706)
*Peter Beidler,Mark Nguyen,Kevin Lybarger,Ola Holmberg,Eric Ford,John Kang*

Main category: cs.CL

TL;DR: 开发基于BlueBERT的跨机构NLP模型，自动检测放射肿瘤科高危事件报告，性能接近人类专家（AUROC 0.78-0.85）


<details>
  <summary>Details</summary>
Motivation: 医疗事故报告人工审核效率低且依赖专家经验，需自动化工具提升安全质量管理效率

Method: 使用本院7,094份报告和IAEA的571份报告，对比SVM与BlueBERT模型性能，采用跨机构迁移学习策略（先本院微调再跨机构微调）

Result: 跨机构迁移学习使BlueBERT在外部测试集AUROC提升至0.78，人工精修数据集上模型表现（AUROC 0.74-0.85）与人类（0.81）相当

Conclusion: 成功开发跨机构NLP筛查工具，在结构化数据集上实现接近人类专家的高危事件识别能力，为医疗质量监控提供自动化解决方案

Abstract: PURPOSE: Incident reports are an important tool for safety and quality
improvement in healthcare, but manual review is time-consuming and requires
subject matter expertise. Here we present a natural language processing (NLP)
screening tool to detect high-severity incident reports in radiation oncology
across two institutions.
  METHODS AND MATERIALS: We used two text datasets to train and evaluate our
NLP models: 7,094 reports from our institution (Inst.), and 571 from IAEA
SAFRON (SF), all of which had severity scores labeled by clinical content
experts. We trained and evaluated two types of models: baseline support vector
machines (SVM) and BlueBERT which is a large language model pretrained on
PubMed abstracts and hospitalized patient data. We assessed for
generalizability of our model in two ways. First, we evaluated models trained
using Inst.-train on SF-test. Second, we trained a BlueBERT_TRANSFER model that
was first fine-tuned on Inst.-train then on SF-train before testing on SF-test
set. To further analyze model performance, we also examined a subset of 59
reports from our Inst. dataset, which were manually edited for clarity.
  RESULTS Classification performance on the Inst. test achieved AUROC 0.82
using SVM and 0.81 using BlueBERT. Without cross-institution transfer learning,
performance on the SF test was limited to an AUROC of 0.42 using SVM and 0.56
using BlueBERT. BlueBERT_TRANSFER, which was fine-tuned on both datasets,
improved the performance on SF test to AUROC 0.78. Performance of SVM, and
BlueBERT_TRANSFER models on the manually curated Inst. reports (AUROC 0.85 and
0.74) was similar to human performance (AUROC 0.81).
  CONCLUSION: In summary, we successfully developed cross-institution NLP
models on incident report text from radiation oncology centers. These models
were able to detect high-severity reports similarly to humans on a curated
dataset.

</details>


### [13] [DSPC: Dual-Stage Progressive Compression Framework for Efficient Long-Context Reasoning](https://arxiv.org/abs/2509.13723)
*Yaxin Gao,Yao Lu,Zongfei Zhang,Jiaqi Nie,Shanqing Yu,Qi Xuan*

Main category: cs.CL

TL;DR: 提出无需训练的双阶段提示压缩方法DSPC，通过语义过滤和细粒度剪枝实现高效提示压缩


<details>
  <summary>Details</summary>
Motivation: 现有提示压缩方法需要训练辅助模型导致计算成本高，需开发无训练的高效替代方案

Method: 1. 粗粒度阶段：基于TF-IDF过滤低价值句子；2. 细粒度阶段：结合注意力贡献、跨模型损失差异和位置重要性剪枝token

Result: 在LLaMA-3.1-8B-Instruct和GPT-3.5上实现性能提升，Longbench FewShot任务达到49.17（比SOTA高7.76），token用量减少3倍

Conclusion: DSPC在无训练条件下实现更高效的提示压缩，为LLM应用提供实用的优化方案

Abstract: Large language models (LLMs) have achieved remarkable success in many natural
language processing (NLP) tasks. To achieve more accurate output, the prompts
used to drive LLMs have become increasingly longer, which incurs higher
computational costs. To address this prompt inflation problem, prompt
compression has been proposed. However, most existing methods require training
a small auxiliary model for compression, incurring a significant amount of
additional computation. To avoid this, we propose a two-stage, training-free
approach, called Dual-Stage Progressive Compression (DSPC). In the
coarse-grained stage, semantic-related sentence filtering removes sentences
with low semantic value based on TF-IDF. In the fine-grained stage, token
importance is assessed using attention contribution, cross-model loss
difference, and positional importance, enabling the pruning of low-utility
tokens while preserving semantics. We validate DSPC on LLaMA-3.1-8B-Instruct
and GPT-3.5-Turbo under a constrained token budget and observe consistent
improvements. For instance, in the FewShot task of the Longbench dataset, DSPC
achieves a performance of 49.17 by using only 3x fewer tokens, outperforming
the best state-of-the-art baseline LongLLMLingua by 7.76.

</details>


### [14] [Implementing a Logical Inference System for Japanese Comparatives](https://arxiv.org/abs/2509.13734)
*Yosuke Mikami,Daiki Matsuoka,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 提出基于组合语义学的日语比较句逻辑推理系统ccg-jcomp，在日语NLI数据集上验证其有效性并优于现有大语言模型


<details>
  <summary>Details</summary>
Motivation: 日语比较句与英语存在形态和语义差异，现有基于英语比较句的逻辑推理系统无法直接适用

Method: 基于组合语义学和组合范畴语法(CCG)构建逻辑推理系统，采用三段论推理框架处理日语比较句

Result: 系统在包含比较表达的日语NLI数据集上准确率超过现有LLMs（GPT-3.5/4）

Conclusion: 证明了逻辑推理系统在日语比较句NLI任务中的有效性，为日语形式语义学研究提供新工具

Abstract: Natural Language Inference (NLI) involving comparatives is challenging
because it requires understanding quantities and comparative relations
expressed by sentences. While some approaches leverage Large Language Models
(LLMs), we focus on logic-based approaches grounded in compositional semantics,
which are promising for robust handling of numerical and logical expressions.
Previous studies along these lines have proposed logical inference systems for
English comparatives. However, it has been pointed out that there are several
morphological and semantic differences between Japanese and English
comparatives. These differences make it difficult to apply such systems
directly to Japanese comparatives. To address this gap, this study proposes
ccg-jcomp, a logical inference system for Japanese comparatives based on
compositional semantics. We evaluate the proposed system on a Japanese NLI
dataset containing comparative expressions. We demonstrate the effectiveness of
our system by comparing its accuracy with that of existing LLMs.

</details>


### [15] [Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications](https://arxiv.org/abs/2509.13775)
*Vani Kanjirangat,Ljiljana Dolamic,Fabio Rinaldi*

Main category: cs.CL

TL;DR: 探索数据高效与参数高效方法在阿拉伯方言识别中的应用，LoRA微调模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过软提示调优和参数高效微调技术提升阿拉伯方言识别性能，减少对大规模数据和全参数调整的依赖。

Method: 使用阿拉伯语编码器模型测试多种软提示策略（如prefix-tuning、LoRA等），并在零样本/少样本设置下评估LLMs和PEFT方法。

Result: LLMs在方言细粒度区分上表现欠佳，LoRA微调模型效果超越完全微调。

Conclusion: 参数高效的LoRA方法在阿拉伯方言识别任务中优于传统全参数微调和软提示策略。

Abstract: This paper discusses our exploration of different data-efficient and
parameter-efficient approaches to Arabic Dialect Identification (ADI). In
particular, we investigate various soft-prompting strategies, including
prefix-tuning, prompt-tuning, P-tuning, and P-tuning V2, as well as LoRA
reparameterizations. For the data-efficient strategy, we analyze hard prompting
with zero-shot and few-shot inferences to analyze the dialect identification
capabilities of Large Language Models (LLMs). For the parameter-efficient PEFT
approaches, we conducted our experiments using Arabic-specific encoder models
on several major datasets. We also analyzed the n-shot inferences on
open-source decoder-only models, a general multilingual model (Phi-3.5), and an
Arabic-specific one(SILMA). We observed that the LLMs generally struggle to
differentiate the dialectal nuances in the few-shot or zero-shot setups. The
soft-prompted encoder variants perform better, while the LoRA-based fine-tuned
models perform best, even surpassing full fine-tuning.

</details>


### [16] [Teaching According to Talents! Instruction Tuning LLMs with Competence-Aware Curriculum Learning](https://arxiv.org/abs/2509.13790)
*Yangning Li,Tingwei Lu,Yinghui Li,Yankai Chen,Wei-Chieh Huang,Wenhao Jiang,Hui Wang,Hai-Tao Zheng,Philip S. Yu*

Main category: cs.CL

TL;DR: 提出CAMPUS框架，通过动态课程学习策略有效提升大语言模型的指令调优效率


<details>
  <summary>Details</summary>
Motivation: 现有课程学习方法依赖静态难度指标，无法适应模型训练中动态变化的能力，导致学习路径僵化

Method: CAMPUS框架包含动态子课程选择机制、基于模型能力感知的课程进度调整策略，以及多难度协同调度方法

Result: 大量实验证明CAMPUS在指令调优任务中显著优于现有先进基线方法

Conclusion: 该框架成功解决了课程僵化问题，通过动态适应模型能力演进实现了更优的指令调优效果

Abstract: Efficient instruction tuning aims to enhance the ultimate performance of
large language models (LLMs) trained on a given instruction dataset. Curriculum
learning as a typical data organization strategy has shown preliminary
effectiveness in instruction tuning. However, current curriculum tuning methods
suffer from the curriculum rigidity, since they rely solely on static heuristic
difficulty metrics. These methods fail to adapt to the evolving capabilities of
models during training, resulting in a fixed and potentially sub-optimal
learning trajectory. To address the issue, Competence-Aware Multi-Perspective
cUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS
offers several advantages: (1) Dynamic selection for sub-curriculum. (2)
Competency-aware adjustment to the curriculum schedule. (3) Multiple
difficulty-based scheduling. Extensive experiments prove the superior
performance of CAMPUS, compared to other state-of-the-art baselines for
efficient instruction tuning.

</details>


### [17] [Measuring Gender Bias in Job Title Matching for Grammatical Gender Languages](https://arxiv.org/abs/2509.13803)
*Laura García-Sardiña,Hermenegildo Fabregat,Daniel Deniz,Rabih Zbib*

Main category: cs.CL

TL;DR: 该研究建立了分析职位头衔语法性别对自动排名系统影响的框架，提出使用RBO指标评估性别偏见，构建了四语种测试集并验证多语言模型存在不同程度性别偏见


<details>
  <summary>Details</summary>
Motivation: 研究显式语法性别在职位头衔中的分配如何影响自动职位排名系统结果，填补该领域研究空白

Method: 1. 提出基于RBO的性别偏见评估指标
2. 构建四语种测试集（含阴阳性职位名称及性别标注）
3. 评估主流多语言模型的性别偏见程度

Result: 所有测试模型均表现出不同程度的性别偏见现象

Conclusion: 该研究为自然语言处理中的性别偏见检测提供了方法论和基准测试集，未来需开发更公平的排名算法

Abstract: This work sets the ground for studying how explicit grammatical gender
assignment in job titles can affect the results of automatic job ranking
systems. We propose the usage of metrics for ranking comparison controlling for
gender to evaluate gender bias in job title ranking systems, in particular RBO
(Rank-Biased Overlap). We generate and share test sets for a job title matching
task in four grammatical gender languages, including occupations in masculine
and feminine form and annotated by gender and matching relevance. We use the
new test sets and the proposed methodology to evaluate the gender bias of
several out-of-the-box multilingual models to set as baselines, showing that
all of them exhibit varying degrees of gender bias.

</details>


### [18] [Geometric Uncertainty for Detecting and Correcting Hallucinations in LLMs](https://arxiv.org/abs/2509.13813)
*Edward Phillips,Sean Wu,Soheila Molaei,Danielle Belgrave,Anshul Thakur,David Clifton*

Main category: cs.CL

TL;DR: 提出基于几何原型分析的黑盒框架，同时量化大语言模型的全局和局部不确定性，有效检测幻觉生成


<details>
  <summary>Details</summary>
Motivation: 现有黑盒方法仅能评估全局不确定性，而局部不确定性检测需依赖模型内部状态的白盒访问。需要开发同时支持两种不确定性分析的实用方法

Method: 利用响应嵌入的原型分析构建几何框架：1) Geometric Volume通过原型点凸包体积量化全局不确定性；2) Geometric Suspicion通过响应可靠性排序实现局部检测

Result: 在短问答数据集达到或超越现有方法，医疗数据集表现显著更优。理论证明凸包体积与熵存在关联

Conclusion: 该几何框架首次实现黑盒设置下的双重不确定性量化，原型边界点提供可解释性，在医疗等高危场景具重要应用价值

Abstract: Large language models demonstrate impressive results across diverse tasks but
are still known to hallucinate, generating linguistically plausible but
incorrect answers to questions. Uncertainty quantification has been proposed as
a strategy for hallucination detection, but no existing black-box approach
provides estimates for both global and local uncertainty. The former attributes
uncertainty to a batch of responses, while the latter attributes uncertainty to
individual responses. Current local methods typically rely on white-box access
to internal model states, whilst black-box methods only provide global
uncertainty estimates. We introduce a geometric framework to address this,
based on archetypal analysis of batches of responses sampled with only
black-box model access. At the global level, we propose Geometric Volume, which
measures the convex hull volume of archetypes derived from response embeddings.
At the local level, we propose Geometric Suspicion, which ranks responses by
reliability and enables hallucination reduction through preferential response
selection. Unlike prior dispersion methods which yield only a single global
score, our approach provides semantic boundary points which have utility for
attributing reliability to individual responses. Experiments show that our
framework performs comparably to or better than prior methods on short form
question-answering datasets, and achieves superior results on medical datasets
where hallucinations carry particularly critical risks. We also provide
theoretical justification by proving a link between convex hull volume and
entropy.

</details>


### [19] [Findings of the Third Automatic Minuting (AutoMin) Challenge](https://arxiv.org/abs/2509.13814)
*Kartik Shinde,Laurent Besacier,Ondrej Bojar,Thibaut Thonet,Tirthankar Ghosal*

Main category: cs.CL

TL;DR: 第三届AutoMin共享任务新增跨语言问答，评估2025年LLMs在会议纪要生成和QA任务中的表现


<details>
  <summary>Details</summary>
Motivation: 推动结构化会议纪要的自动化生成技术发展，探索LLMs在多语言（英/捷）、多领域（项目会议/议会）场景下的应用潜力

Method: 通过组织双赛道评测（纪要生成+QA），覆盖英语/捷克语的项目会议和欧洲议会场景，设置单语/跨语言QA任务，引入多基线系统对比

Result: 2025年参与度下降但基线评估显示LLMs在跨语言QA中表现突出，议会会议纪要生成质量优于项目会议

Conclusion: 尽管参与者减少，但基线系统揭示了LLMs处理复杂会议场景的潜力，为多语言自动会议处理技术发展提供新方向

Abstract: This paper presents the third edition of AutoMin, a shared task on automatic
meeting summarization into minutes. In 2025, AutoMin featured the main task of
minuting, the creation of structured meeting minutes, as well as a new task:
question answering (QA) based on meeting transcripts.
  The minuting task covered two languages, English and Czech, and two domains:
project meetings and European Parliament sessions. The QA task focused solely
on project meetings and was available in two settings: monolingual QA in
English, and cross-lingual QA, where questions were asked and answered in Czech
based on English meetings.
  Participation in 2025 was more limited compared to previous years, with only
one team joining the minuting task and two teams participating in QA. However,
as organizers, we included multiple baseline systems to enable a comprehensive
evaluation of current (2025) large language models (LLMs) on both tasks.

</details>


### [20] [Large Language Models Discriminate Against Speakers of German Dialects](https://arxiv.org/abs/2509.13835)
*Minh Duc Bui,Carolin Holtermann,Valentin Hofmann,Anne Lauscher,Katharina von der Wense*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Dialects represent a significant component of human culture and are found
across all regions of the world. In Germany, more than 40% of the population
speaks a regional dialect (Adler and Hansen, 2022). However, despite cultural
importance, individuals speaking dialects often face negative societal
stereotypes. We examine whether such stereotypes are mirrored by large language
models (LLMs). We draw on the sociolinguistic literature on dialect perception
to analyze traits commonly associated with dialect speakers. Based on these
traits, we assess the dialect naming bias and dialect usage bias expressed by
LLMs in two tasks: an association task and a decision task. To assess a model's
dialect usage bias, we construct a novel evaluation corpus that pairs sentences
from seven regional German dialects (e.g., Alemannic and Bavarian) with their
standard German counterparts. We find that: (1) in the association task, all
evaluated LLMs exhibit significant dialect naming and dialect usage bias
against German dialect speakers, reflected in negative adjective associations;
(2) all models reproduce these dialect naming and dialect usage biases in their
decision making; and (3) contrary to prior work showing minimal bias with
explicit demographic mentions, we find that explicitly labeling linguistic
demographics--German dialect speakers--amplifies bias more than implicit cues
like dialect usage.

</details>


### [21] [Do LLMs Align Human Values Regarding Social Biases? Judging and Explaining Social Biases with LLMs](https://arxiv.org/abs/2509.13869)
*Yang Liu,Chenhui Chu*

Main category: cs.CL

TL;DR: 研究通过分析12个大语言模型在四类社会偏见场景下的表现，发现模型参数规模与价值观对齐无直接关联，模型家族对判断一致性影响显著，小模型经微调后可生成更易读但认可度较低的解释。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在不同类型社会偏见场景中与人类价值观的对齐差异，验证模型参数规模、场景类型偏好和模型家族一致性对价值观对齐的影响。

Method: 使用4个数据集对来自4个模型家族的12个LLM进行系统性测试，包含偏见误判率、攻击成功率、判断一致性指标，并通过微调赋予小模型价值观解释能力。

Result: 大模型未显现出价值观对齐优势；模型对负面/非负面场景存在偏好；同家族模型判断一致性高；小模型生成解释可读性提升但模型认可度下降。

Conclusion: 价值观对齐需考虑场景类型特性与模型家族特性，单纯扩大模型规模并非解决方案，小模型的解释能力增强与认可度平衡是未来方向。

Abstract: Large language models (LLMs) can lead to undesired consequences when
misaligned with human values, especially in scenarios involving complex and
sensitive social biases. Previous studies have revealed the misalignment of
LLMs with human values using expert-designed or agent-based emulated bias
scenarios. However, it remains unclear whether the alignment of LLMs with human
values differs across different types of scenarios (e.g., scenarios containing
negative vs. non-negative questions). In this study, we investigate the
alignment of LLMs with human values regarding social biases (HVSB) in different
types of bias scenarios. Through extensive analysis of 12 LLMs from four model
families and four datasets, we demonstrate that LLMs with large model parameter
scales do not necessarily have lower misalignment rate and attack success rate.
Moreover, LLMs show a certain degree of alignment preference for specific types
of scenarios and the LLMs from the same model family tend to have higher
judgment consistency. In addition, we study the understanding capacity of LLMs
with their explanations of HVSB. We find no significant differences in the
understanding of HVSB across LLMs. We also find LLMs prefer their own generated
explanations. Additionally, we endow smaller language models (LMs) with the
ability to explain HVSB. The generation results show that the explanations
generated by the fine-tuned smaller LMs are more readable, but have a
relatively lower model agreeability.

</details>


### [22] [Combining Evidence and Reasoning for Biomedical Fact-Checking](https://arxiv.org/abs/2509.13879)
*Mariano Barone,Antonio Romano,Giuseppe Riccio,Marco Postiglione,Vincenzo Moscato*

Main category: cs.CL

TL;DR: 提出了CER框架，通过整合科学证据检索、大语言模型推理和监督验证预测，有效提升生物医学事实核查的准确性和可验证性。


<details>
  <summary>Details</summary>
Motivation: 生物医学领域的事实核查面临专业术语复杂、依赖领域知识、需严格科学证据支撑等独特挑战，现有方法难以可靠应对。

Method: 结合三阶段流程：1）生物医学证据检索 2）大语言模型推理 3）监督验证预测模块。通过检索约束减少模型幻觉，证据增强提升可靠性。

Result: 在HealthFC、BioASQ-7b和SciFact数据集上达到SOTA性能，并展现出良好的跨数据集泛化能力。

Conclusion: CER框架成功将文本生成能力与科学证据检索结合，确保输出基于可验证证据源，为生物医学信息验证提供可靠解决方案。

Abstract: Misinformation in healthcare, from vaccine hesitancy to unproven treatments,
poses risks to public health and trust in medical systems. While machine
learning and natural language processing have advanced automated fact-checking,
validating biomedical claims remains uniquely challenging due to complex
terminology, the need for domain expertise, and the critical importance of
grounding in scientific evidence. We introduce CER (Combining Evidence and
Reasoning), a novel framework for biomedical fact-checking that integrates
scientific evidence retrieval, reasoning via large language models, and
supervised veracity prediction. By integrating the text-generation capabilities
of large language models with advanced retrieval techniques for high-quality
biomedical scientific evidence, CER effectively mitigates the risk of
hallucinations, ensuring that generated outputs are grounded in verifiable,
evidence-based sources. Evaluations on expert-annotated datasets (HealthFC,
BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising
cross-dataset generalization. Code and data are released for transparency and
reproducibility: https: //github.com/PRAISELab-PicusLab/CER.

</details>


### [23] [Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification](https://arxiv.org/abs/2509.13888)
*Mariano Barone,Antonio Romano,Giuseppe Riccio,Marco Postiglione,Vincenzo Moscato*

Main category: cs.CL

TL;DR: 提出CER框架解决医疗错误信息验证难题，整合证据检索、大语言模型推理和监督验证预测，在多个数据集达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 医疗领域错误信息（如疫苗犹豫）威胁公共健康，现有自动事实核查技术面临专业术语复杂、需领域知识支撑等独特挑战。

Method: 1. 科学证据检索技术获取高质量生物医学证据
2. 大语言模型进行证据推理
3. 监督式真实性预测模型验证结论

Result: 在HealthFC/BioASQ-7b/SciFact数据集上取得state-of-the-art性能，并展现跨数据集泛化能力（F1=0.86）

Conclusion: CER通过证据检索与LLM协同有效减少幻觉，实现可验证的医疗事实核查，开源代码促进研究透明化。

Abstract: Misinformation in healthcare, from vaccine hesitancy to unproven treatments,
poses risks to public health and trust in medical systems. While machine
learning and natural language processing have advanced automated fact-checking,
validating biomedical claims remains uniquely challenging due to complex
terminology, the need for domain expertise, and the critical importance of
grounding in scientific evidence. We introduce CER (Combining Evidence and
Reasoning), a novel framework for biomedical fact-checking that integrates
scientific evidence retrieval, reasoning via large language models, and
supervised veracity prediction. By integrating the text-generation capabilities
of large language models with advanced retrieval techniques for high-quality
biomedical scientific evidence, CER effectively mitigates the risk of
hallucinations, ensuring that generated outputs are grounded in verifiable,
evidence-based sources. Evaluations on expert-annotated datasets (HealthFC,
BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising
cross-dataset generalization. Code and data are released for transparency and
reproducibility: https://github.com/PRAISELab-PicusLab/CER

</details>


### [24] [Do Large Language Models Understand Word Senses?](https://arxiv.org/abs/2509.13905)
*Domenico Meconi,Simone Stirpe,Federico Martelli,Leonardo Lavalle,Roberto Navigli*

Main category: cs.CL

TL;DR: 研究发现顶尖LLMs在词义消歧任务上媲美专业系统，在生成任务中最高达98%准确率


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLMs真正掌握词语上下文含义的能力评估不足，需系统评估其词义理解能力

Method: 通过词义消歧任务对比专业系统，并在定义生成/自由解释/示例生成三种生成场景测试LLMs

Result: GPT-4o和DeepSeek-V3在WSD任务中表现匹敌专业系统且鲁棒性更强；自由解释任务准确率最高（98%）

Conclusion: LLMs不仅具备专业级词义理解能力，其生成能力与任务形式的适配性显著影响表现

Abstract: Understanding the meaning of words in context is a fundamental capability for
Large Language Models (LLMs). Despite extensive evaluation efforts, the extent
to which LLMs show evidence that they truly grasp word senses remains
underexplored. In this paper, we address this gap by evaluating both i) the
Word Sense Disambiguation (WSD) capabilities of instruction-tuned LLMs,
comparing their performance to state-of-the-art systems specifically designed
for the task, and ii) the ability of two top-performing open- and closed-source
LLMs to understand word senses in three generative settings: definition
generation, free-form explanation, and example generation. Notably, we find
that, in the WSD task, leading models such as GPT-4o and DeepSeek-V3 achieve
performance on par with specialized WSD systems, while also demonstrating
greater robustness across domains and levels of difficulty. In the generation
tasks, results reveal that LLMs can explain the meaning of words in context up
to 98\% accuracy, with the highest performance observed in the free-form
explanation task, which best aligns with their generative capabilities.

</details>


### [25] [Linguistic Nepotism: Trading-off Quality for Language Preference in Multilingual RAG](https://arxiv.org/abs/2509.13930)
*Dayeon Ki,Marine Carpuat,Paul McNamee,Daniel Khashabi,Eugene Yang,Dawn Lawrie,Kevin Duh*

Main category: cs.CL

TL;DR: 研究通过模型内部机制分析发现，多语言RAG系统存在优先引用英语文献的倾向，尤其在低资源语言和文档位置居中时更明显，且可能牺牲相关性换取语言偏好


<details>
  <summary>Details</summary>
Motivation: 探讨不同文献语言混合是否会影响多语言检索增强生成系统的引用行为和生成质量

Method: 使用模型内部机制构建受控研究方法，在保持文档相关性恒定的前提下，分析8种语言和6个开源模型的语言偏好

Result: 模型普遍优先引用英语文献，低资源语言和文档位置居中时偏好加剧，且存在牺牲文档相关性换取语言偏好的现象

Conclusion: 揭示了语言模型在多语言语境下的引用行为机制，证明引用选择并非完全由信息量驱动

Abstract: Multilingual Retrieval-Augmented Generation (mRAG) systems enable language
models to answer knowledge-intensive queries with citation-supported responses
across languages. While such systems have been proposed, an open questions is
whether the mixture of different document languages impacts generation and
citation in unintended ways. To investigate, we introduce a controlled
methodology using model internals to measure language preference while holding
other factors such as document relevance constant. Across eight languages and
six open-weight models, we find that models preferentially cite English sources
when queries are in English, with this bias amplified for lower-resource
languages and for documents positioned mid-context. Crucially, we find that
models sometimes trade-off document relevance for language preference,
indicating that citation choices are not always driven by informativeness
alone. Our findings shed light on how language models leverage multilingual
context and influence citation behavior.

</details>


### [26] [Long-context Reference-based MT Quality Estimation](https://arxiv.org/abs/2509.13980)
*Sami Ul Haq,Chinonso Cynthia Osuji,Sheila Castilho,Brian Davis*

Main category: cs.CL

TL;DR: 基于COMET框架构建长上下文评估系统，提升机器翻译质量自动评估效果


<details>
  <summary>Details</summary>
Motivation: 传统翻译质量评估模型依赖短文本片段，无法有效捕捉长距离上下文信息对翻译质量的影响

Method: 通过拼接领域内人工标注句子构建长上下文训练数据，整合MQM/SQM/DA多类型人工评估数据集并进行归一化处理，训练多语言回归模型

Result: 实验证明长上下文模型比短片段训练模型显著提升与人工评估相关性

Conclusion: 长上下文信息整合与多数据集联合训练策略有效提升翻译质量评估的准确性和鲁棒性

Abstract: In this paper, we present our submission to the Tenth Conference on Machine
Translation (WMT25) Shared Task on Automated Translation Quality Evaluation.
  Our systems are built upon the COMET framework and trained to predict
segment-level Error Span Annotation (ESA) scores using augmented long-context
data.
  To construct long-context training data, we concatenate in-domain,
human-annotated sentences and compute a weighted average of their scores.
  We integrate multiple human judgment datasets (MQM, SQM, and DA) by
normalising their scales and train multilingual regression models to predict
quality scores from the source, hypothesis, and reference translations.
  Experimental results show that incorporating long-context information
improves correlations with human judgments compared to models trained only on
short segments.

</details>


### [27] [Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency](https://arxiv.org/abs/2509.13990)
*Colin Hong,Xu Guo,Anand Chaanan Singh,Esha Choukse,Dmitrii Ustiugov*

Main category: cs.CL

TL;DR: 提出Slim-SC剪枝策略，通过思维链相似性分析降低Self-Consistency的计算开销，在保持精度的同时显著提升推理效率


<details>
  <summary>Details</summary>
Motivation: Self-Consistency并行推理链的计算开销过高，现有加速方法依赖置信度或启发式规则效果有限

Method: 基于思维链相似性分析的逐步剪枝策略（Slim-SC），动态移除冗余推理链

Result: 在STEM推理数据集上实现45%延迟降低和26% KVC使用减少，同时保持/提升模型精度

Conclusion: Slim-SC为Self-Consistency提供了高效替代方案，首次通过系统性分析揭示改进空间并实现效果验证

Abstract: Recently, Test-Time Scaling (TTS) has gained increasing attention for
improving LLM reasoning performance at test time without retraining the model.
A notable TTS technique is Self-Consistency (SC), which generates multiple
reasoning chains in parallel and selects the final answer via majority voting.
While effective, the order-of-magnitude computational overhead limits its broad
deployment. Prior attempts to accelerate SC mainly rely on model-based
confidence scores or heuristics with limited empirical support. For the first
time, we theoretically and empirically analyze the inefficiencies of SC and
reveal actionable opportunities for improvement. Building on these insights, we
propose Slim-SC, a step-wise pruning strategy that identifies and removes
redundant chains using inter-chain similarity at the thought level. Experiments
on three STEM reasoning datasets and two recent LLM architectures show that
Slim-SC reduces inference latency and KVC usage by up to 45% and 26%,
respectively, with R1-Distill, while maintaining or improving accuracy, thus
offering a simple yet efficient TTS alternative for SC.

</details>


### [28] [Early Stopping Chain-of-thoughts in Large Language Models](https://arxiv.org/abs/2509.14004)
*Minjia Mao,Bowen Yin,Yu Zhu,Xiao Fang*

Main category: cs.CL

TL;DR: 提出ES-CoT方法，通过动态检测答案收敛提前终止思维链生成，在保持精度的同时显著降低推理成本


<details>
  <summary>Details</summary>
Motivation: 长思维链推理导致LLM计算开销过高，需在不显著影响性能的前提下缩短推理过程

Method: 实时追踪连续相同中间答案的出现次数（run length），当检测到run length的突变式增长时终止生成

Result: 在5个推理数据集/3种LLM上平均减少41%推理token，精度损失可忽略，且兼容自洽性提示框架

Conclusion: ES-CoT通过创新的收敛检测机制，实现了高效可靠的LLM推理优化

Abstract: Reasoning large language models (LLMs) have demonstrated superior capacities
in solving complicated problems by generating long chain-of-thoughts (CoT), but
such a lengthy CoT incurs high inference costs. In this study, we introduce
ES-CoT, an inference-time method that shortens CoT generation by detecting
answer convergence and stopping early with minimal performance loss. At the end
of each reasoning step, we prompt the LLM to output its current final answer,
denoted as a step answer. We then track the run length of consecutive identical
step answers as a measure of answer convergence. Once the run length exhibits a
sharp increase and exceeds a minimum threshold, the generation is terminated.
We provide both empirical and theoretical support for this heuristic: step
answers steadily converge to the final answer, and large run-length jumps
reliably mark this convergence. Experiments on five reasoning datasets across
three LLMs show that ES-CoT reduces the number of inference tokens by about
41\% on average while maintaining accuracy comparable to standard CoT. Further,
ES-CoT integrates seamlessly with self-consistency prompting and remains robust
across hyperparameter choices, highlighting it as a practical and effective
approach for efficient reasoning.

</details>


### [29] [Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale](https://arxiv.org/abs/2509.14008)
*Hasan Abed Al Kader Hammoud,Mohammad Zbeeb,Bernard Ghanem*

Main category: cs.CL

TL;DR: Hala系列模型通过translate-and-tune流程实现阿拉伯语NLP突破，在多个参数规模下取得SOTA表现并开源全套资源。


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯语NLP资源稀缺问题，通过双语监督和指令翻译构建专用语料库，提升阿拉伯语模型性能。

Method: 1. 压缩FP8提升吞吐量 2. 生成双语监督数据 3. 微调轻量模型翻译指令 4. slerp混合平衡专业/通用能力

Result: 在阿拉伯语基准测试中，各规模Hala模型（350M-9B）均超越基線模型，创造nano/small类别新SOTA。

Conclusion: Hala为阿拉伯语NLP研究提供高效解决方案，开源模型、数据和方法论加速领域发展。

Abstract: We present Hala, a family of Arabic-centric instruction and translation
models built with our translate-and-tune pipeline. We first compress a strong
AR$\leftrightarrow$EN teacher to FP8 (yielding $\sim$2$\times$ higher
throughput with no quality loss) and use it to create high-fidelity bilingual
supervision. A lightweight language model LFM2-1.2B is then fine-tuned on this
data and used to translate high-quality English instruction sets into Arabic,
producing a million-scale corpus tailored to instruction following. We train
Hala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to
balance Arabic specialization with base-model strengths. On Arabic-centric
benchmarks, Hala achieves state-of-the-art results within both the "nano"
($\leq$2B) and "small" (7-9B) categories, outperforming their bases. We release
models, data, evaluation, and recipes to accelerate research in Arabic NLP.

</details>


### [30] [Audio-Based Crowd-Sourced Evaluation of Machine Translation Quality](https://arxiv.org/abs/2509.14023)
*Sami Ul Haq,Sheila Castilho,Yvette Graham*

Main category: cs.CL

TL;DR: 研究比较文本与音频评估在机器翻译质量评估中的差异，发现音频评估能检测到文本评估未发现的系统间显著差异，建议未来整合语音评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前机器翻译质量评估主要依赖文本比对，但实际应用场景（如语音翻译）需要更自然的音频评估方式。

Method: 使用WMT比赛的10个机器翻译系统，通过亚马逊众包收集音频/文本评估数据，并进行统计显著性测试与实验复现验证可靠性。

Result: 音频评估结果与文本评估总体一致，但能检测到部分翻译系统的显著性能差异（归因于语音模态更自然丰富的特性）。

Conclusion: 建议将语音评估纳入未来机器翻译评估体系以提升应用场景适配性。

Abstract: Machine Translation (MT) has achieved remarkable performance, with growing
interest in speech translation and multimodal approaches. However, despite
these advancements, MT quality assessment remains largely text centric,
typically relying on human experts who read and compare texts. Since many
real-world MT applications (e.g Google Translate Voice Mode, iFLYTEK
Translator) involve translation being spoken rather printed or read, a more
natural way to assess translation quality would be through speech as opposed
text-only evaluations. This study compares text-only and audio-based
evaluations of 10 MT systems from the WMT General MT Shared Task, using
crowd-sourced judgments collected via Amazon Mechanical Turk. We additionally,
performed statistical significance testing and self-replication experiments to
test reliability and consistency of audio-based approach. Crowd-sourced
assessments based on audio yield rankings largely consistent with text only
evaluations but, in some cases, identify significant differences between
translation systems. We attribute this to speech richer, more natural modality
and propose incorporating speech-based assessments into future MT evaluation
frameworks.

</details>


### [31] [You Are What You Train: Effects of Data Composition on Training Context-aware Machine Translation Models](https://arxiv.org/abs/2509.14031)
*Paweł Mąka,Yusuf Can Semerci,Jan Scholtes,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: 标准训练数据中上下文丰富样本的稀疏性是翻译模型利用上下文的瓶颈，通过构建可控比例的训练集验证后，提出两种策略使准确率提升6-8%。


<details>
  <summary>Details</summary>
Motivation: 验证训练数据中上下文相关样本的稀疏性是否是翻译模型难以有效利用上下文的核心原因，并探究不同语境现象间的改进是否具有泛化性。

Method: 1. 构建含可控比例上下文相关样本的训练数据集；2. 提出并评估两种数据利用策略（具体策略未在摘要中明确说明）。

Result: 数据稀疏性与模型性能强相关（单语/多语场景分别提升6%/8% ctxPro评估准确率），但不同语境现象的改进无泛化性，同亚语系语言间跨语言迁移效应不显著。

Conclusion: 通过针对性训练策略缓解数据稀疏性可有效提升上下文利用率，但需针对不同语境现象进行显式数据设计，且跨语言改进存在局限性。

Abstract: Achieving human-level translations requires leveraging context to ensure
coherence and handle complex phenomena like pronoun disambiguation. Sparsity of
contextually rich examples in the standard training data has been hypothesized
as the reason for the difficulty of context utilization. In this work, we
systematically validate this claim in both single- and multilingual settings by
constructing training datasets with a controlled proportions of contextually
relevant examples. We demonstrate a strong association between training data
sparsity and model performance confirming sparsity as a key bottleneck.
Importantly, we reveal that improvements in one contextual phenomenon do no
generalize to others. While we observe some cross-lingual transfer, it is not
significantly higher between languages within the same sub-family. Finally, we
propose and empirically evaluate two training strategies designed to leverage
the available data. These strategies improve context utilization, resulting in
accuracy gains of up to 6 and 8 percentage points on the ctxPro evaluation in
single- and multilingual settings respectively.

</details>


### [32] [Enhancing Multi-Agent Debate System Performance via Confidence Expression](https://arxiv.org/abs/2509.14034)
*Zijie Lin,Bryan Hooi*

Main category: cs.CL

TL;DR: 通过在多智能体辩论系统中引入置信度表达机制（ConfMAD），有效提升大语言模型在复杂任务中的协作效能与决策质量。


<details>
  <summary>Details</summary>
Motivation: 现有MAD系统中，大语言模型因缺乏置信度表达导致知识优势无法显性化，易出现固执己见或过早收敛次优解的问题，制约辩论效果。

Method: 提出ConfMAD框架，将置信度表达贯穿辩论全流程，包括置信度量化、动态权重调整和共识形成机制。

Result: 实验证实置信度引导的辩论策略显著提升系统性能，消融实验揭示置信度对辩论路径优化的具体作用机制。

Conclusion: 置信度表达是提升MAD系统效能的关键设计维度，未来工作需构建更精细的置信度校准与传播机制。

Abstract: Generative Large Language Models (LLMs) have demonstrated remarkable
performance across a wide range of tasks. Recent research has introduced
Multi-Agent Debate (MAD) systems, which leverage multiple LLMs to simulate
human debate and thereby improve task performance. However, while some LLMs may
possess superior knowledge or reasoning capabilities for specific tasks, they
often struggle to clearly communicate this advantage during debates, in part
due to a lack of confidence expression. Moreover, inappropriate confidence
expression can cause agents in MAD systems to either stubbornly maintain
incorrect beliefs or converge prematurely on suboptimal answers, ultimately
reducing debate effectiveness and overall system performance. To address these
challenges, we propose incorporating confidence expression into MAD systems to
allow LLMs to explicitly communicate their confidence levels. To validate this
approach, we develop ConfMAD, a MAD framework that integrates confidence
expression throughout the debate process. Experimental results demonstrate the
effectiveness of our method, and we further analyze how confidence influences
debate dynamics, offering insights into the design of confidence-aware MAD
systems.

</details>


### [33] [SSL-SSAW: Self-Supervised Learning with Sigmoid Self-Attention Weighting for Question-Based Sign Language Translation](https://arxiv.org/abs/2509.14036)
*Zekang Liu,Wei Feng,Fanhua Shang,Lianyu Hu,Jichao Feng,Liqing Gao*

Main category: cs.CL

TL;DR: 提出基于对话的QB-SLT任务和SSL-SSAW跨模态融合方法，通过自监督学习和自适应注意力加权提升手语翻译质量，性能超越传统gloss标注。


<details>
  <summary>Details</summary>
Motivation: 利用对话自然产生的上下文信息替代人工标注的gloss，降低标注成本并提升手语翻译的实用性。

Method: 结合对比学习对齐多模态特征，设计SSAW模块实现问题与手语序列的自适应特征融合，并引入自监督学习增强文本表示。

Result: 在CSL-Daily-QA和PHOENIX-2014T-QA数据集上达到SOTA，问题辅助效果等同/超越gloss辅助。

Conclusion: 验证对话整合对翻译质量的有效提升，为手语翻译提供更实用的无标注数据解决方案。

Abstract: Sign Language Translation (SLT) bridges the communication gap between deaf
people and hearing people, where dialogue provides crucial contextual cues to
aid in translation. Building on this foundational concept, this paper proposes
Question-based Sign Language Translation (QB-SLT), a novel task that explores
the efficient integration of dialogue. Unlike gloss (sign language
transcription) annotations, dialogue naturally occurs in communication and is
easier to annotate. The key challenge lies in aligning multimodality features
while leveraging the context of the question to improve translation. To address
this issue, we propose a cross-modality Self-supervised Learning with Sigmoid
Self-attention Weighting (SSL-SSAW) fusion method for sign language
translation. Specifically, we employ contrastive learning to align
multimodality features in QB-SLT, then introduce a Sigmoid Self-attention
Weighting (SSAW) module for adaptive feature extraction from question and sign
language sequences. Additionally, we leverage available question text through
self-supervised learning to enhance representation and translation
capabilities. We evaluated our approach on newly constructed CSL-Daily-QA and
PHOENIX-2014T-QA datasets, where SSL-SSAW achieved SOTA performance. Notably,
easily accessible question assistance can achieve or even surpass the
performance of gloss assistance. Furthermore, visualization results demonstrate
the effectiveness of incorporating dialogue in improving translation quality.

</details>


### [34] [Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance Models for Multilingual ASR and AST](https://arxiv.org/abs/2509.14128)
*Monica Sekoyan,Nithin Rao Koluguri,Nune Tadevosyan,Piotr Zelasko,Travis Bartley,Nick Karpov,Jagadeesh Balam,Boris Ginsburg*

Main category: cs.CL

TL;DR: Canary-1B-v2 是高效多语言语音识别与翻译模型，支持25种欧洲语言，训练数据达170万小时，速度比Whisper快10倍且性能更优。


<details>
  <summary>Details</summary>
Motivation: 解决现有ASR/AST模型在多语言场景下的效率与准确性不足，通过优化架构和数据策略减少语音幻觉问题。

Method: 采用FastConformer编码器+Transformer解码器架构，两阶段预训练与微调，结合动态数据平衡和NeMo对齐器实现时间戳标注，并探索nGPT编码器性能。

Result: 英语ASR性能超越Whisper-large-v3且提速10倍，多语言任务性能接近Seamless-M4T-v2等大模型，同步发布轻量级继任模型Parakeet-TDT-0.6B-v3。

Conclusion: Canary-1B-v2在效率与精度间取得平衡，其架构与训练策略为多语言语音处理提供新范式，轻量级模型扩展进一步降低部署门槛。

Abstract: This report introduces Canary-1B-v2, a fast, robust multilingual model for
Automatic Speech Recognition (ASR) and Speech-to-Text Translation (AST). Built
with a FastConformer encoder and Transformer decoder, it supports 25 languages
primarily European. The model was trained on 1.7M hours of total data samples,
including Granary and NeMo ASR Set 3.0, with non-speech audio added to reduce
hallucinations for ASR and AST. We describe its two-stage pre-training and
fine-tuning process with dynamic data balancing, as well as experiments with an
nGPT encoder. Results show nGPT scales well with massive data, while
FastConformer excels after fine-tuning. For timestamps, Canary-1B-v2 uses the
NeMo Forced Aligner (NFA) with an auxiliary CTC model, providing reliable
segment-level timestamps for ASR and AST. Evaluations show Canary-1B-v2
outperforms Whisper-large-v3 on English ASR while being 10x faster, and
delivers competitive multilingual ASR and AST performance against larger models
like Seamless-M4T-v2-large and LLM-based systems. We also release
Parakeet-TDT-0.6B-v3, a successor to v2, offering multilingual ASR across the
same 25 languages with just 600M parameters.

</details>


### [35] [CS-FLEURS: A Massively Multilingual and Code-Switched Speech Dataset](https://arxiv.org/abs/2509.14161)
*Brian Yan,Injy Hamed,Shuichiro Shimizu,Vasista Lodagala,William Chen,Olga Iakovenko,Bashar Talafha,Amir Hussein,Alexander Polok,Kalvin Chang,Dominik Klement,Sara Althubaiti,Puyuan Peng,Matthew Wiesner,Thamar Solorio,Ahmed Ali,Sanjeev Khudanpur,Shinji Watanabe,Chih-Chen Chen,Zhen Wu,Karim Benharrak,Anuj Diwan,Samuele Cornell,Eunjung Yeo,Kwanghee Choi,Carlos Carvalho,Karen Rosero*

Main category: cs.CL

TL;DR: CS-FLEURS是覆盖52种语言、113个代码切换语言对的新型语音数据集，旨在推动低资源语言的代码切换语音识别研究


<details>
  <summary>Details</summary>
Motivation: 现有代码切换语音数据集多集中于高资源语言，需扩展低资源语言覆盖以促进更广泛的研究

Method: 使用合成语音(TTS)和拼接技术构建四个测试集（含真实/生成语音）和128小时训练集（生成式TTS）

Result: 创建含45个低资源X-英语语言对的测试集及16个X-英语训练集，数据总量达128小时

Conclusion: CS-FLEURS通过多样化合成技术扩展了代码切换研究的语言覆盖，为跨语言语音技术发展提供新基准

Abstract: We present CS-FLEURS, a new dataset for developing and evaluating
code-switched speech recognition and translation systems beyond high-resourced
languages. CS-FLEURS consists of 4 test sets which cover in total 113 unique
code-switched language pairs across 52 languages: 1) a 14 X-English language
pair set with real voices reading synthetically generated code-switched
sentences, 2) a 16 X-English language pair set with generative text-to-speech
3) a 60 {Arabic, Mandarin, Hindi, Spanish}-X language pair set with the
generative text-to-speech, and 4) a 45 X-English lower-resourced language pair
test set with concatenative text-to-speech. Besides the four test sets,
CS-FLEURS also provides a training set with 128 hours of generative
text-to-speech data across 16 X-English language pairs. Our hope is that
CS-FLEURS helps to broaden the scope of future code-switched speech research.
Dataset link: https://huggingface.co/datasets/byan/cs-fleurs.

</details>


### [36] [AssoCiAm: A Benchmark for Evaluating Association Thinking while Circumventing Ambiguity](https://arxiv.org/abs/2509.14171)
*Yifan Liu,Wenkuan Zhao,Shanshan Zhong,Jinghui Qin,Mingfu Liang,Zhongzhan Huang,Wushao Wen*

Main category: cs.CL

TL;DR: 提出AssoCiAm基准解决MLLMs创造力评估中的模糊性问题，验证认知与关联能力的强相关性


<details>
  <summary>Details</summary>
Motivation: 现有评估框架忽视关联任务中的模糊性，导致评估可靠性不足

Method: 将模糊性分解为内部/外部两类，通过混合计算方法构建AssoCiAm基准

Result: 实验显示认知与关联能力强正相关，模糊性导致模型行为随机化

Conclusion: 所提方法有效提升评估准确性，为MLLMs创造力评估提供可靠方案

Abstract: Recent advancements in multimodal large language models (MLLMs) have garnered
significant attention, offering a promising pathway toward artificial general
intelligence (AGI). Among the essential capabilities required for AGI,
creativity has emerged as a critical trait for MLLMs, with association serving
as its foundation. Association reflects a model' s ability to think creatively,
making it vital to evaluate and understand. While several frameworks have been
proposed to assess associative ability, they often overlook the inherent
ambiguity in association tasks, which arises from the divergent nature of
associations and undermines the reliability of evaluations. To address this
issue, we decompose ambiguity into two types-internal ambiguity and external
ambiguity-and introduce AssoCiAm, a benchmark designed to evaluate associative
ability while circumventing the ambiguity through a hybrid computational
method. We then conduct extensive experiments on MLLMs, revealing a strong
positive correlation between cognition and association. Additionally, we
observe that the presence of ambiguity in the evaluation process causes MLLMs'
behavior to become more random-like. Finally, we validate the effectiveness of
our method in ensuring more accurate and reliable evaluations. See Project Page
for the data and codes.

</details>


### [37] [Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation Framework for Personal Finance LLMs](https://arxiv.org/abs/2509.14180)
*Akhil Theerthala*

Main category: cs.CL

TL;DR: 本研究开发了一个整合金融背景与行为研究的可复现框架，通过微调Qwen-3-8B模型实现了与大型模型（14-32B参数）相当的财务顾问性能，同时降低80%成本。


<details>
  <summary>Details</summary>
Motivation: 现有金融顾问系统采用复杂流程导致高维护成本（预期收益不足25%），且缺乏行为金融学整合，需要端到端的低成本解决方案。

Method: 构建19k样本的监督数据集（融合金融背景与行为研究），对Qwen-3-8B进行全参数微调，采用盲测和LLM-jury评估体系。

Result: 8B模型在事实准确性(85.2%)、流畅性(91.7%)和个性化(88.4%)指标上达到14-32B模型水平，推理成本降低80%

Conclusion: 通过数据优化与行为特征融合，小型模型可替代大型基线模型，为低成本部署个性化财务顾问系统提供新范式。

Abstract: Personalized financial advice requires consideration of user goals,
constraints, risk tolerance, and jurisdiction. Prior LLM work has focused on
support systems for investors and financial planners. Simultaneously, numerous
recent studies examine broader personal finance tasks, including budgeting,
debt management, retirement, and estate planning, through agentic pipelines
that incur high maintenance costs, yielding less than 25% of their expected
financial returns. In this study, we introduce a novel and reproducible
framework that integrates relevant financial context with behavioral finance
studies to construct supervision data for end-to-end advisors. Using this
framework, we create a 19k sample reasoning dataset and conduct a comprehensive
fine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test
split and a blind LLM-jury study, we demonstrate that through careful data
curation and behavioral integration, our 8B model achieves performance
comparable to significantly larger baselines (14-32B parameters) across factual
accuracy, fluency, and personalization metrics while incurring 80% lower costs
than the larger counterparts.

</details>


### [38] [Framing Migration: A Computational Analysis of UK Parliamentary Discourse](https://arxiv.org/abs/2509.14197)
*Vahid Ghafouri,Robert McNeil,Teodor Yankov,Madeleine Sumption,Luc Rocher,Scott A. Hale,Adam Mahdi*

Main category: cs.CL

TL;DR: 研究利用大语言模型(LLMs)对英美移民政策话语进行75年跨度的计算分析，揭示英国议会态度相对稳定但安全化叙事增强，美国则日益极化。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在政治话语分析中的潜力，对比英美移民政策演变，揭示意识形态差异与叙事框架转变。

Method: 使用开放权重LLMs标注议会声明立场，开发半自动框架提取叙事要素，追踪跨时空/政党的净语调变化。

Result: 英国工党与保守党意识形态差距2025年达历史峰值；安全化叙事(边境管控)占比升23%，整合导向框架降37%；移民法讨论转向国际人权框架。

Conclusion: LLMs支持细粒度政治话语分析，英美移民话语呈现不同演化路径，安全化叙事兴起可能导致政策短期化倾向。

Abstract: We present a large-scale computational analysis of migration-related
discourse in UK parliamentary debates spanning over 75 years and compare it
with US congressional discourse. Using open-weight LLMs, we annotate each
statement with high-level stances toward migrants and track the net tone toward
migrants across time and political parties. For the UK, we extend this with a
semi-automated framework for extracting fine-grained narrative frames to
capture nuances of migration discourse. Our findings show that, while US
discourse has grown increasingly polarised, UK parliamentary attitudes remain
relatively aligned across parties, with a persistent ideological gap between
Labour and the Conservatives, reaching its most negative level in 2025. The
analysis of narrative frames in the UK parliamentary statements reveals a shift
toward securitised narratives such as border control and illegal immigration,
while longer-term integration-oriented frames such as social integration have
declined. Moreover, discussions of national law about immigration have been
replaced over time by international law and human rights, revealing nuances in
discourse trends. Taken together broadly, our findings demonstrate how LLMs can
support scalable, fine-grained discourse analysis in political and historical
contexts.

</details>


### [39] [Apertus: Democratizing Open and Compliant LLMs for Global Language Environments](https://arxiv.org/abs/2509.14233)
*Alejandro Hernández-Cano,Alexander Hägele,Allen Hao Huang,Angelika Romanou,Antoni-Joan Solergibert,Barna Pasztor,Bettina Messmer,Dhia Garbaya,Eduard Frank Ďurech,Ido Hakimi,Juan García Giraldo,Mete Ismayilzada,Negar Foroutan,Skander Moalla,Tiancheng Chen,Vinko Sabolčec,Yixuan Xu,Michael Aerni,Badr AlKhamissi,Ines Altemir Marinas,Mohammad Hossein Amani,Matin Ansaripour,Ilia Badanin,Harold Benoit,Emanuela Boros,Nicholas Browning,Fabian Bösch,Maximilian Böther,Niklas Canova,Camille Challier,Clement Charmillot,Jonathan Coles,Jan Deriu,Arnout Devos,Lukas Drescher,Daniil Dzenhaliou,Maud Ehrmann,Dongyang Fan,Simin Fan,Silin Gao,Miguel Gila,María Grandury,Diba Hashemi,Alexander Hoyle,Jiaming Jiang,Mark Klein,Andrei Kucharavy,Anastasiia Kucherenko,Frederike Lübeck,Roman Machacek,Theofilos Manitaras,Andreas Marfurt,Kyle Matoba,Simon Matrenok,Henrique Mendoncça,Fawzi Roberto Mohamed,Syrielle Montariol,Luca Mouchel,Sven Najem-Meyer,Jingwei Ni,Gennaro Oliva,Matteo Pagliardini,Elia Palme,Andrei Panferov,Léo Paoletti,Marco Passerini,Ivan Pavlov,Auguste Poiroux,Kaustubh Ponkshe,Nathan Ranchin,Javi Rando,Mathieu Sauser,Jakhongir Saydaliev,Muhammad Ali Sayfiddinov,Marian Schneider,Stefano Schuppli,Marco Scialanga,Andrei Semenov,Kumar Shridhar,Raghav Singhal,Anna Sotnikova,Alexander Sternfeld,Ayush Kumar Tarun,Paul Teiletche,Jannis Vamvas,Xiaozhe Yao,Hao Zhao Alexander Ilic,Ana Klimovic,Andreas Krause,Caglar Gulcehre,David Rosenthal,Elliott Ash,Florian Tramèr,Joost VandeVondele,Livio Veraldi,Martin Rajman,Thomas Schulthess,Torsten Hoefler,Antoine Bosselut,Martin Jaggi,Imanol Schlag*

Main category: cs.CL

TL;DR: Apertus开源LLM套件通过合规数据训练与Goldfish目标抑制记忆风险，覆盖1800+语言并实现多语言SOTA性能，同时全面开源科研资产。


<details>
  <summary>Details</summary>
Motivation: 解决现有开源模型中数据合规性不足（存在非授权/毒性内容）和多语言覆盖率低（英语中心化）两大系统性缺陷。

Method: 1. 基于robots.txt筛选公开数据并过滤敏感内容
2. 采用Goldfish目标减少数据记忆
3. 使用15T tokens（40%非英语）覆盖1800+语言
4. 完整开源数据工具链与训练框架

Result: 8B/70B模型在多语言基准达到全开源模型最佳水平，且公开了可审计的数据处理流程、训练检查点和评估套件。

Conclusion: 该框架为开源社区提供了兼顾法律合规性、多语言性能与透明度的LLM开发范式，推动负责任AI发展。

Abstract: We present Apertus, a fully open suite of large language models (LLMs)
designed to address two systemic shortcomings in today's open model ecosystem:
data compliance and multilingual representation. Unlike many prior models that
release weights without reproducible data pipelines or regard for content-owner
rights, Apertus models are pretrained exclusively on openly available data,
retroactively respecting robots.txt exclusions and filtering for
non-permissive, toxic, and personally identifiable content. To mitigate risks
of memorization, we adopt the Goldfish objective during pretraining, strongly
suppressing verbatim recall of data while retaining downstream task
performance. The Apertus models also expand multilingual coverage, training on
15T tokens from over 1800 languages, with ~40% of pretraining data allocated to
non-English content. Released at 8B and 70B scales, Apertus approaches
state-of-the-art results among fully open models on multilingual benchmarks,
rivalling or surpassing open-weight counterparts. Beyond model weights, we
release all scientific artifacts from our development cycle with a permissive
license, including data preparation scripts, checkpoints, evaluation suites,
and training code, enabling transparent audit and extension.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [40] [An AI-Powered Framework for Analyzing Collective Idea Evolution in Deliberative Assemblies](https://arxiv.org/abs/2509.12577)
*Elinor Poole-Dayan,Deb Roy,Jad Kabbara*

Main category: cs.CY

TL;DR: 研究利用LLM技术追踪审议会议中观点的演变过程，开发可视化框架分析代表立场变化，揭示传统报告难以捕捉的微观动态


<details>
  <summary>Details</summary>
Motivation: 当前社会分裂与政治极化背景下，审议式民主存在理论关注多但实证研究少的困境，尤其缺乏对观点具体演化路径的系统追踪

Method: 基于大语言模型技术，开发新型分析方法处理线下增强型审议会议的转录文本，构建建议空间可视化与代表立场演化重建模型

Result: 成功建立可识别200+具体建议、重构代表立场连续变化的分析框架，证明LLM能有效捕捉审议过程中隐含的动态交互特征

Conclusion: LLM方法论为民主协商研究提供微观动态观测工具，技术增强型分析可提升复杂政策制定过程的透明度与科学性

Abstract: In an era of increasing societal fragmentation, political polarization, and
erosion of public trust in institutions, representative deliberative assemblies
are emerging as a promising democratic forum for developing effective policy
outcomes on complex global issues. Despite theoretical attention, there remains
limited empirical work that systematically traces how specific ideas evolve,
are prioritized, or are discarded during deliberation to form policy
recommendations. Addressing these gaps, this work poses two central questions:
(1) How might we trace the evolution and distillation of ideas into concrete
recommendations within deliberative assemblies? (2) How does the deliberative
process shape delegate perspectives and influence voting dynamics over the
course of the assembly? To address these questions, we develop LLM-based
methodologies for empirically analyzing transcripts from a tech-enhanced
in-person deliberative assembly. The framework identifies and visualizes the
space of expressed suggestions. We also empirically reconstruct each delegate's
evolving perspective throughout the assembly. Our methods contribute novel
empirical insights into deliberative processes and demonstrate how LLMs can
surface high-resolution dynamics otherwise invisible in traditional assembly
outputs.

</details>


### [41] [Accuracy Paradox in Large Language Models: Regulating Hallucination Risks in Generative AI](https://arxiv.org/abs/2509.13345)
*Zihao Li,Weiwei Yi,Jiahong Chen*

Main category: cs.CY

TL;DR: 论文指出过度依赖准确性作为大型语言模型(LLMs)的治理标准会引发『准确性悖论』，需转向多元化、情境感知的抗操纵治理框架


<details>
  <summary>Details</summary>
Motivation: 现有监管和技术讨论将准确性作为主要基准，但无法有效检测具有误导性、价值偏见或社会扭曲的非事实性危害，且忽视系统性社会影响

Method: 构建幻觉类型分类体系，从输出质量(修辞优化掩盖可信度)、个体认知(被动信任误导性内容)、社会影响(隐私侵犯/认知趋同等)三个维度论证准确性悖论

Result: 揭示欧盟AI法案/GDPR/DSA等现有法规在治理架构上无法应对幻觉引发的认知操纵、社会技能退化等系统性风险

Conclusion: 呼吁建立基于语境敏感性、多元价值包容、抗认知操纵能力的可信AI治理范式，突破单一准确性指标的限制

Abstract: As Large Language Models (LLMs) permeate everyday decision-making, their
epistemic and societal risks demand urgent scrutiny. Hallucinations, the
generation of fabricated, misleading, oversimplified or untrustworthy outputs,
has emerged as imperative challenges. While regulatory, academic, and technical
discourse position accuracy as the principal benchmark for mitigating such
harms, this article contends that overreliance on accuracy misdiagnoses the
problem and has counterproductive effect: the accuracy paradox. Drawing on
interdisciplinary literatures, this article develops a taxonomy of
hallucination types and shows the paradox along three intertwining dimensions:
outputs, individuals and society. First, accuracy functions as a superficial
proxy for reliability, incentivising the optimisation of rhetorical fluency and
surface-level correctness over epistemic trustworthiness. This encourages
passive user trust in outputs that appear accurate but epistemically untenable.
Second, accuracy as a singular metric fails to detect harms that are not
factually false but are nonetheless misleading, value-laden, or socially
distorting, including consensus illusions, sycophantic alignment, and subtle
manipulation. Third, regulatory overemphasis on accuracy obscures the wider
societal consequences of hallucination, including social sorting, privacy
violations, equity harms, epistemic convergence that marginalises dissent,
reduces pluralism, and causes social deskilling. By examining the EU AI Act,
GDPR, and DSA, the article argues that current regulations are not yet
structurally equipped to address these epistemic, relational, and systemic
harms and exacerbated by the overreliance on accuracy. By exposing such
conceptual and practical challenges, this article calls for a fundamental shift
towards pluralistic, context-aware, and manipulation-resilient approaches to AI
trustworthy governance.

</details>


### [42] [CogniAlign: Survivability-Grounded Multi-Agent Moral Reasoning for Safe and Transparent AI](https://arxiv.org/abs/2509.13356)
*Hasin Jawad Ali,Ilhamul Azam,Ajwad Abrar,Md. Kamrul Hasan,Hasan Mahmud*

Main category: cs.CY

TL;DR: 提出多智能体道德推理框架CogniAlign，通过跨学科专家代理结构化审议，在60+道德问题中全面超越GPT-4o表现。


<details>
  <summary>Details</summary>
Motivation: 解决AI对齐难题：传统方法存在道德原则抽象性、矛盾性及决策黑箱化缺陷，需建立透明可验证的伦理推理机制。

Method: 基于自然道德现实主义构建多学科代理（神经科学/心理学/社会学/进化生物学）审议框架，通过生存力双维度（个体/集体）量化道德判断，仲裁者整合学科论点形成实证化决策。

Result: 在五维伦理审计框架下：CogniAlign平均得分比GPT-4o提升16.2%（分析质量）、14.3%（广度）、28.4%（深度），海因茨困境中89.2 vs 69.2显著优势。

Conclusion: 通过跨学科结构化审议减少黑箱推理，为AI安全对齐提供可扩展路径，证明透明化伦理决策系统的可行性。

Abstract: The challenge of aligning artificial intelligence (AI) with human values
persists due to the abstract and often conflicting nature of moral principles
and the opacity of existing approaches. This paper introduces CogniAlign, a
multi-agent deliberation framework based on naturalistic moral realism, that
grounds moral reasoning in survivability, defined across individual and
collective dimensions, and operationalizes it through structured deliberations
among discipline-specific scientist agents. Each agent, representing
neuroscience, psychology, sociology, and evolutionary biology, provides
arguments and rebuttals that are synthesized by an arbiter into transparent and
empirically anchored judgments. We evaluate CogniAlign on classic and novel
moral questions and compare its outputs against GPT-4o using a five-part
ethical audit framework. Results show that CogniAlign consistently outperforms
the baseline across more than sixty moral questions, with average performance
gains of 16.2 points in analytic quality, 14.3 points in breadth, and 28.4
points in depth of explanation. In the Heinz dilemma, for example, CogniAlign
achieved an overall score of 89.2 compared to GPT-4o's 69.2, demonstrating a
decisive advantage in handling moral reasoning. By reducing black-box reasoning
and avoiding deceptive alignment, CogniAlign highlights the potential of
interdisciplinary deliberation as a scalable pathway for safe and transparent
AI alignment.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [43] [Noise Supervised Contrastive Learning and Feature-Perturbed for Anomalous Sound Detection](https://arxiv.org/abs/2509.13853)
*Shun Huang,Zhihua Fang,Liang He*

Main category: cs.SD

TL;DR: 提出基于一阶段监督对比学习(OS-SCL)和TFgram特征的无监督异常音检测方法，在DCASE 2020任务中AUC达95.71%


<details>
  <summary>Details</summary>
Motivation: 解决自监督方法在处理不同机器同类型样本时误报率过高的问题

Method: OS-SCL通过在嵌入空间扰动特征，结合噪声监督对比学习；提出从原始音频提取TFgram时频特征

Result: 使用Log-Mel特征实现94.64%AUC，TFgram特征达到95.71%AUC/90.23%pAUC/91.23%mAUC

Conclusion: 该方法有效降低误报率，TFgram特征能更好捕捉声学异常特征，代码已开源验证

Abstract: Unsupervised anomalous sound detection aims to detect unknown anomalous
sounds by training a model using only normal audio data. Despite advancements
in self-supervised methods, the issue of frequent false alarms when handling
samples of the same type from different machines remains unresolved. This paper
introduces a novel training technique called one-stage supervised contrastive
learning (OS-SCL), which significantly addresses this problem by perturbing
features in the embedding space and employing a one-stage noisy supervised
contrastive learning approach. On the DCASE 2020 Challenge Task 2, it achieved
94.64\% AUC, 88.42\% pAUC, and 89.24\% mAUC using only Log-Mel features.
Additionally, a time-frequency feature named TFgram is proposed, which is
extracted from raw audio. This feature effectively captures critical
information for anomalous sound detection, ultimately achieving 95.71\% AUC,
90.23\% pAUC, and 91.23\% mAUC. The source code is available at:
\underline{www.github.com/huangswt/OS-SCL}.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [44] [Annotating Satellite Images of Forests with Keywords from a Specialized Corpus in the Context of Change Detection](https://arxiv.org/abs/2509.13586)
*Nathalie Neptune,Josiane Mothe*

Main category: cs.CV

TL;DR: 提出基于卫星图像对和深度学习的亚马逊森林砍伐检测方法，结合视觉语义模型实现自动标注


<details>
  <summary>Details</summary>
Motivation: 亚马逊森林砍伐严重影响全球碳平衡和生物多样性，亟需有效监测工具

Method: 使用地球观测卫星图像对，通过深度学习比较不同时相图像检测森林变化，构建视觉语义模型从科学文献提取关键词进行自动标注

Result: 在亚马逊图像数据集上验证了方法有效性，能准确检测森林覆盖变化并生成相关语义标注

Conclusion: 该方法为森林砍伐监测提供了有效工具，其通用性框架可扩展应用于其他环境监测领域

Abstract: The Amazon rain forest is a vital ecosystem that plays a crucial role in
regulating the Earth's climate and providing habitat for countless species.
Deforestation in the Amazon is a major concern as it has a significant impact
on global carbon emissions and biodiversity. In this paper, we present a method
for detecting deforestation in the Amazon using image pairs from Earth
observation satellites. Our method leverages deep learning techniques to
compare the images of the same area at different dates and identify changes in
the forest cover. We also propose a visual semantic model that automatically
annotates the detected changes with relevant keywords. The candidate annotation
for images are extracted from scientific documents related to the Amazon
region. We evaluate our approach on a dataset of Amazon image pairs and
demonstrate its effectiveness in detecting deforestation and generating
relevant annotations. Our method provides a useful tool for monitoring and
studying the impact of deforestation in the Amazon. While we focus on
environment applications of our work by using images of deforestation in the
Amazon rain forest to demonstrate the effectiveness of our proposed approach,
it is generic enough to be applied to other domains.

</details>


### [45] [Diving into Mitigating Hallucinations from a Vision Perspective for Large Vision-Language Models](https://arxiv.org/abs/2509.13836)
*Weihang Wang,Xinhao Li,Ziyue Wang,Yan Pang,Jielei Zhang,Peiyi Li,Qiang Zhang,Longwen Gao*

Main category: cs.CV

TL;DR: 通过构建细粒度幻觉评测基准VHBench-10分析视觉编码器特性，提出动态聚合专家特征的VisionWeaver框架有效降低LVLM物体幻觉


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型中的物体幻觉严重影响实际应用，现有基准未能系统评估不同视觉编码器训练范式导致的多样化幻觉现象

Method: 建立包含10类细粒度幻觉的VHBench-10基准，提出基于全局特征动态路由的上下文感知网络VisionWeaver实现多专家特征融合

Result: 验证不同视觉编码器具有独特幻觉模式，VisionWeaver相较基线方法显著降低50%以上幻觉发生率

Conclusion: 通过动态特征融合机制优化视觉编码器输出，为减少LVLM物体幻觉提供了有效解决方案和技术路线

Abstract: Object hallucination in Large Vision-Language Models (LVLMs) significantly
impedes their real-world applicability. As the primary component for accurately
interpreting visual information, the choice of visual encoder is pivotal. We
hypothesize that the diverse training paradigms employed by different visual
encoders instill them with distinct inductive biases, which leads to their
diverse hallucination performances. Existing benchmarks typically focus on
coarse-grained hallucination detection and fail to capture the diverse
hallucinations elaborated in our hypothesis. To systematically analyze these
effects, we introduce VHBench-10, a comprehensive benchmark with approximately
10,000 samples for evaluating LVLMs across ten fine-grained hallucination
categories. Our evaluations confirm encoders exhibit unique hallucination
characteristics. Building on these insights and the suboptimality of simple
feature fusion, we propose VisionWeaver, a novel Context-Aware Routing Network.
It employs global visual features to generate routing signals, dynamically
aggregating visual features from multiple specialized experts. Comprehensive
experiments confirm the effectiveness of VisionWeaver in significantly reducing
hallucinations and improving overall model performance.

</details>


### [46] [Dense Video Understanding with Gated Residual Tokenization](https://arxiv.org/abs/2509.14199)
*Haichao Zhang,Wenhao Chai,Shwai He,Ang Li,Yun Fu*

Main category: cs.CV

TL;DR: 提出GRT框架实现高效高帧率视频理解，解决现有VLLMs在密集时间信息处理上的瓶颈


<details>
  <summary>Details</summary>
Motivation: 现有视频大模型采用低帧采样导致时间信息丢失，在需要精准时序对齐的任务（如讲座理解）中表现不足

Method: 两阶段门控残差标记化：1) 运动补偿帧间标记跳过静态区域 2) 语义场景帧内合并减少冗余

Result: 在DIVE基准上超越现有VLLMs，且性能随FPS提升而增长，验证方法有效性

Conclusion: 密集时序信息对视频理解至关重要，GRT为高效处理高帧率视频提供了创新解决方案

Abstract: High temporal resolution is essential for capturing fine-grained details in
video understanding. However, current video large language models (VLLMs) and
benchmarks mostly rely on low-frame-rate sampling, such as uniform sampling or
keyframe selection, discarding dense temporal information. This compromise
avoids the high cost of tokenizing every frame, which otherwise leads to
redundant computation and linear token growth as video length increases. While
this trade-off works for slowly changing content, it fails for tasks like
lecture comprehension, where information appears in nearly every frame and
requires precise temporal alignment. To address this gap, we introduce Dense
Video Understanding (DVU), which enables high-FPS video comprehension by
reducing both tokenization time and token overhead. Existing benchmarks are
also limited, as their QA pairs focus on coarse content changes. We therefore
propose DIVE (Dense Information Video Evaluation), the first benchmark designed
for dense temporal reasoning. To make DVU practical, we present Gated Residual
Tokenization (GRT), a two-stage framework: (1) Motion-Compensated Inter-Gated
Tokenization uses pixel-level motion estimation to skip static regions during
tokenization, achieving sub-linear growth in token count and compute. (2)
Semantic-Scene Intra-Tokenization Merging fuses tokens across static regions
within a scene, further reducing redundancy while preserving dynamic semantics.
Experiments on DIVE show that GRT outperforms larger VLLM baselines and scales
positively with FPS. These results highlight the importance of dense temporal
information and demonstrate that GRT enables efficient, scalable high-FPS video
understanding.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [47] [Privacy-Aware In-Context Learning for Large Language Models](https://arxiv.org/abs/2509.13625)
*Bishnu Bhusal,Manoj Acharya,Ramneet Kaur,Colin Samplawski,Anirban Roy,Adam D. Cobb,Rohit Chadha,Susmit Jha*

Main category: cs.LG

TL;DR: 提出基于差分隐私的文本生成框架，在保证隐私性的同时维持高可用性


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在信息泄露风险，需在生成文本时保障用户隐私安全

Method: 采用差分隐私框架进行推理，聚合token级输出分布生成连贯文本，并提出公私推理混合机制提升效用

Result: 在上下文学习任务中表现优于现有方法，实现隐私保护与高效用的平衡

Conclusion: 该方法为隐私保护文本生成开辟了新方向，在理论安全边界和实际效用间取得良好平衡

Abstract: Large language models (LLMs) have significantly transformed natural language
understanding and generation, but they raise privacy concerns due to potential
exposure of sensitive information. Studies have highlighted the risk of
information leakage, where adversaries can extract sensitive information
embedded in the prompts. In this work, we introduce a novel private prediction
framework for generating high-quality synthetic text with strong privacy
guarantees. Our approach leverages the Differential Privacy (DP) framework to
ensure worst-case theoretical bounds on information leakage without requiring
any fine-tuning of the underlying models.The proposed method performs inference
on private records and aggregates the resulting per-token output distributions.
This enables the generation of longer and coherent synthetic text while
maintaining privacy guarantees. Additionally, we propose a simple blending
operation that combines private and public inference to further enhance
utility. Empirical evaluations demonstrate that our approach outperforms
previous state-of-the-art methods on in-context-learning (ICL) tasks, making it
a promising direction for privacy-preserving text generation while maintaining
high utility.

</details>


### [48] [Language models' activations linearly encode training-order recency](https://arxiv.org/abs/2509.14223)
*Dmitrii Krasheninnikov,Richard E. Turner,David Krueger*

Main category: cs.LG

TL;DR: 研究发现语言模型激活值线性编码了训练数据的时间顺序，模型能区分信息获取时间并影响数据冲突处理机制。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型是否保留训练数据的时间信息，以理解模型如何处理知识冲突和知识更新。

Method: 通过按顺序微调Llama-3.2-1B模型在六个相似但不相交的命名实体数据集，分析模型激活值的线性编码特性。

Result: 激活值中心点在2D空间呈直线排列（顺序与训练一致），线性探针可准确判断实体训练阶段（泛化准确率~90%），模型经微调后能直接报告实体训练阶段（准确率~80%）。

Conclusion: 模型具备按时间维度区分知识的能力，这对解决数据冲突和知识更新机制具有重要启示。

Abstract: We show that language models' activations linearly encode when information
was learned during training. Our setup involves creating a model with a known
training order by sequentially fine-tuning Llama-3.2-1B on six disjoint but
otherwise similar datasets about named entities. We find that the average
activations of test samples for the six training datasets encode the training
order: when projected into a 2D subspace, these centroids are arranged exactly
in the order of training and lie on a straight line. Further, we show that
linear probes can accurately (~90%) distinguish "early" vs. "late" entities,
generalizing to entities unseen during the probes' own training. The model can
also be fine-tuned to explicitly report an unseen entity's training stage (~80%
accuracy). Interestingly, this temporal signal does not seem attributable to
simple differences in activation magnitudes, losses, or model confidence. Our
paper demonstrates that models are capable of differentiating information by
its acquisition time, and carries significant implications for how they might
manage conflicting data and respond to knowledge modifications.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [49] [Enhancing Time Awareness in Generative Recommendation](https://arxiv.org/abs/2509.13957)
*Sunkyung Lee,Seongmin Park,Jonghyo Kim,Mincheol Yoon,Jongwuk Lee*

Main category: cs.IR

TL;DR: 提出GRUT模型，通过时间感知提示和趋势感知推理有效捕捉用户偏好变化，提升生成推荐效果


<details>
  <summary>Details</summary>
Motivation: 现有生成推荐方法仅关注物品顺序，忽视时间动态性导致的用户偏好演变问题

Method: 结合用户级时间上下文（个性化时间模式）和物品级转移上下文，设计无需训练的趋势感知推理机制

Result: 在四个基准数据集上Recall@5和NDCG@5指标最高提升15.4%/14.3%，代码已开源

Conclusion: GRUT通过多层次时间建模有效提升推荐性能，验证时间动态捕捉对推荐系统的重要性

Abstract: Generative recommendation has emerged as a promising paradigm that formulates
the recommendations into a text-to-text generation task, harnessing the vast
knowledge of large language models. However, existing studies focus on
considering the sequential order of items and neglect to handle the temporal
dynamics across items, which can imply evolving user preferences. To address
this limitation, we propose a novel model, Generative Recommender Using Time
awareness (GRUT), effectively capturing hidden user preferences via various
temporal signals. We first introduce Time-aware Prompting, consisting of two
key contexts. The user-level temporal context models personalized temporal
patterns across timestamps and time intervals, while the item-level transition
context provides transition patterns across users. We also devise Trend-aware
Inference, a training-free method that enhances rankings by incorporating trend
information about items with generation likelihood. Extensive experiments
demonstrate that GRUT outperforms state-of-the-art models, with gains of up to
15.4% and 14.3% in Recall@5 and NDCG@5 across four benchmark datasets. The
source code is available at https://github.com/skleee/GRUT.

</details>


### [50] [GEM-Bench: A Benchmark for Ad-Injected Response Generation within Generative Engine Marketing](https://arxiv.org/abs/2509.14221)
*Silan Hu,Shiqi Zhang,Yimin Shi,Xiaokui Xiao*

Main category: cs.IR

TL;DR: 提出首个生成引擎营销（GEM）广告植入响应生成基准GEM-Bench，包含多场景数据集、评估体系及基线方案


<details>
  <summary>Details</summary>
Motivation: 现有基准无法有效评估广告植入对生成式引擎用户体验的影响，阻碍相关技术发展

Method: 构建含聊天/搜索场景的3个数据集，建立用户满意度与参与度的多维评价指标，开发可扩展的多智能体框架实现基线方案

Result: 提示方法虽提升广告点击率但损害满意度，基于预生成无广告内容插入的方案缓解该问题但增加计算开销

Conclusion: GEM-Bench为广告植入生成提供系统评估框架，揭示现有方法的效率-效果权衡，推动更优解决方案研究

Abstract: Generative Engine Marketing (GEM) is an emerging ecosystem for monetizing
generative engines, such as LLM-based chatbots, by seamlessly integrating
relevant advertisements into their responses. At the core of GEM lies the
generation and evaluation of ad-injected responses. However, existing
benchmarks are not specifically designed for this purpose, which limits future
research. To address this gap, we propose GEM-Bench, the first comprehensive
benchmark for ad-injected response generation in GEM. GEM-Bench includes three
curated datasets covering both chatbot and search scenarios, a metric ontology
that captures multiple dimensions of user satisfaction and engagement, and
several baseline solutions implemented within an extensible multi-agent
framework. Our preliminary results indicate that, while simple prompt-based
methods achieve reasonable engagement such as click-through rate, they often
reduce user satisfaction. In contrast, approaches that insert ads based on
pre-generated ad-free responses help mitigate this issue but introduce
additional overhead. These findings highlight the need for future research on
designing more effective and efficient solutions for generating ad-injected
responses in GEM.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [51] [Explicit Reasoning Makes Better Judges: A Systematic Study on Accuracy, Efficiency, and Robustness](https://arxiv.org/abs/2509.13332)
*Pratik Jayarao,Himanshu Gupta,Neeraj Varshney,Chaitanya Dwivedi*

Main category: cs.AI

TL;DR: 研究发现显式推理的LLM作为裁判模型在准确性、效率和鲁棒性上全面优于非推理模型，支持在评估任务中优先采用推理模型。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多被用作自动化评估基准和奖励建模，确保其可靠性、效率和鲁棒性变得至关重要。本研究旨在系统比较'思考型'和'非思考型'模型在LLM-as-a-judge范式中的表现差异。

Method: 使用Qwen 3系列小规模模型（0.6B/1.7B/4B），在RewardBench任务上评估准确性和计算效率（FLOPs），测试包括上下文学习、评分标准引导、参考基准评估等增强策略，并进行多语言环境扩展实验。

Result: 思考模型准确率提升约10%且计算开销低（<2倍），非思考模型增强策略成本高（>8倍）但收益有限；在位置/从众/多样性等偏差条件下保持6%更高一致性，多语言场景优势延续。

Conclusion: 显式推理机制为LLM评估范式带来显著优势，建议在需要高准确性、效率和鲁棒性的应用场景中优先采用具备明确推理能力的语言模型。

Abstract: As Large Language Models (LLMs) are increasingly adopted as automated judges
in benchmarking and reward modeling, ensuring their reliability, efficiency,
and robustness has become critical. In this work, we present a systematic
comparison of "thinking" and "non-thinking" LLMs in the LLM-as-a-judge paradigm
using open-source Qwen 3 models of relatively small sizes (0.6B, 1.7B, and 4B
parameters). We evaluate both accuracy and computational efficiency (FLOPs) on
RewardBench tasks, and further examine augmentation strategies for non-thinking
models, including in-context learning, rubric-guided judging, reference-based
evaluation, and n-best aggregation. Our results show that despite these
enhancements, non-thinking models generally fall short of their thinking
counterparts. Our results show that thinking models achieve approximately 10%
points higher accuracy with little overhead (under 2x), in contrast to
augmentation strategies like few-shot learning, which deliver modest gains at a
higher cost (>8x). Bias and robustness analyses further demonstrate that
thinking models maintain significantly greater consistency under a variety of
bias conditions such as positional, bandwagon, identity, diversity, and random
biases (6% higher on average). We further extend our experiments to the
multilingual setting and our results confirm that explicit reasoning extends
its benefits beyond English. Overall, our work results in several important
findings that provide systematic evidence that explicit reasoning offers clear
advantages in the LLM-as-a-judge paradigm not only in accuracy and efficiency
but also in robustness.

</details>


### [52] [Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic Planning](https://arxiv.org/abs/2509.13351)
*Pulkit Verma,Ngoc La,Anthony Favier,Swaroop Mishra,Julie A. Shah*

Main category: cs.AI

TL;DR: 提出了PDDL-Instruct框架，通过逻辑链式思维推理增强大语言模型的符号规划能力，在标准基准测试中实现94%的规划准确率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在需要PDDL形式化表示的符号规划任务中存在局限性，缺乏严格的逻辑推理能力。

Method: 设计基于逻辑推理链的指令提示框架，通过分解规划过程为前置条件验证、状态转换和计划有效性检查的分步推理步骤。

Result: 在多个规划领域达到94%的准确率，相比基线模型绝对提升66%。

Conclusion: 该框架有效弥合了大语言模型通用推理能力与自动规划所需逻辑严谨性之间的鸿沟，为AI规划系统发展提供了新方向。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
diverse tasks, yet their ability to perform structured symbolic planning
remains limited, particularly in domains requiring formal representations like
the Planning Domain Definition Language (PDDL). In this paper, we present a
novel instruction tuning framework, PDDL-Instruct, designed to enhance LLMs'
symbolic planning capabilities through logical chain-of-thought reasoning. Our
approach focuses on teaching models to rigorously reason about action
applicability, state transitions, and plan validity using explicit logical
inference steps. By developing instruction prompts that guide models through
the precise logical reasoning required to determine when actions can be applied
in a given state, we enable LLMs to self-correct their planning processes
through structured reflection. The framework systematically builds verification
skills by decomposing the planning process into explicit reasoning chains about
precondition satisfaction, effect application, and invariant preservation.
Experimental results on multiple planning domains show that our
chain-of-thought reasoning based instruction-tuned models are significantly
better at planning, achieving planning accuracy of up to 94% on standard
benchmarks, representing a 66% absolute improvement over baseline models. This
work bridges the gap between the general reasoning capabilities of LLMs and the
logical precision required for automated planning, offering a promising
direction for developing better AI planning systems.

</details>


### [53] [SteeringControl: Holistic Evaluation of Alignment Steering in LLMs](https://arxiv.org/abs/2509.13450)
*Vincent Siu,Nicholas Crispino,David Park,Nathan W. Henry,Zhun Wang,Yang Liu,Dawn Song,Chenguang Wang*

Main category: cs.AI

TL;DR: 提出了SteeringControl基准系统评估表示引导方法在核心对齐目标（偏见/有害生成/幻觉）与次要行为（奉承/常识道德）间的效果与关联，发现性能依赖方法-模型-目标的组合选择。


<details>
  <summary>Details</summary>
Motivation: 现有对齐研究缺乏对表示引导技术在不同目标（核心对齐目标与次要行为）间权衡影响的系统性评估，需建立标准化评估体系。

Method: 构建安全相关行为数据集，设计模块化引导框架整合五种主流方法，在Qwen和Llama模型进行多维度实验分析。

Result: 强引导性能需方法-模型-目标的精准匹配，错误组合会导致概念纠缠（如提升安全性却损害常识判断）。

Conclusion: 表示引导技术的应用需谨慎选择组合方案，该基准为AI安全研究提供系统评估工具，代码开源推动领域发展。

Abstract: We introduce SteeringControl, a benchmark for evaluating representation
steering methods across core alignment objectives--bias, harmful generation,
and hallucination--and their effects on secondary behaviors such as sycophancy
and commonsense morality. While prior alignment work often highlights
truthfulness or reasoning ability to demonstrate the side effects of
representation steering, we find there are many unexplored tradeoffs not yet
understood in a systematic way. We collect a dataset of safety-relevant primary
and secondary behaviors to evaluate steering effectiveness and behavioral
entanglement centered around five popular steering methods. To enable this, we
craft a modular steering framework based on unique components that serve as the
building blocks of many existing methods. Our results on Qwen-2.5-7B and
Llama-3.1-8B find that strong steering performance is dependent on the specific
combination of steering method, model, and targeted behavior, and that severe
concept entanglement can result from poor combinations of these three as well.
We release our code here:
https://github.com/wang-research-lab/SteeringControl.git.

</details>


### [54] [See, Think, Act: Teaching Multimodal Agents to Effectively Interact with GUI by Identifying Toggles](https://arxiv.org/abs/2509.13615)
*Zongru Wu,Rui Mao,Zhiyuan Tian,Pengzhou Cheng,Tianjie Ju,Zheng Wu,Lingzhong Dong,Haiyue Sheng,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.AI

TL;DR: 提出状态感知推理框架StaR，通过感知当前开关状态并采取对应操作，显著提升GUI智能体执行切换指令的可靠性


<details>
  <summary>Details</summary>
Motivation: 现有多模态智能体在执行图形界面切换指令时可靠性不足，尤其在当前状态与目标状态一致时错误率较高

Method: 开发基于状态感知推理（StaR）的训练方法，分步教学智能体：1. 感知当前开关状态 2. 解析指令需求状态 3. 执行对应操作

Result: 在切换指令执行准确率提升超30%，三个公共基准测试表现提升，动态环境测试验证实际应用潜力

Conclusion: StaR方法有效解决了GUI控制中的状态感知问题，实验证明其能显著提升智能体可靠性并具备实际部署价值

Abstract: The advent of multimodal agents facilitates effective interaction within
graphical user interface (GUI), especially in ubiquitous GUI control. However,
their inability to reliably execute toggle control instructions remains a key
bottleneck. To investigate this, we construct a state control benchmark with
binary toggle instructions from public datasets. Evaluations of existing agents
demonstrate their unreliability, particularly when the current toggle state
already matches the desired state. To address the challenge, we propose
State-aware Reasoning (StaR), a training method that teaches agents to perceive
the current toggle state, analyze the desired state from the instruction, and
act accordingly. Experiments on three multimodal agents demonstrate that StaR
can improve toggle instruction execution accuracy by over 30\%. Further
evaluations on three public benchmarks show that StaR also enhances general
task performance. Finally, evaluations on a dynamic environment highlight the
potential of StaR for real-world applications. Code, benchmark, and
StaR-enhanced agents are available at https://github.com/ZrW00/StaR.

</details>


### [55] [THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning](https://arxiv.org/abs/2509.13761)
*Qikai Chang,Zhenrong Zhang,Pengfei Hu,Jiefeng Ma,Yicheng Pan,Jianshu Zhang,Jun Du,Quan Liu,Jianqing Gao*

Main category: cs.AI

TL;DR: 提出THOR框架，通过工具集成和分层强化学习优化LLMs的数学推理能力，在多项基准测试中达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有工具集成方法面临三大挑战：工具推理数据构建困难、缺乏细粒度优化、推理过程修正不足。中间工具调用的成功率与最终答案正确性高度相关。

Method: 1. 多智能体框架TIRGen生成高质量工具推理数据
2. 分层RL策略联合优化轨迹级问题求解和代码级生成
3. 基于即时工具反馈的动态自校正机制

Result: 在数学基准测试(GSM8K/MATH)和代码基准(HumanEval)均取得同类模型最佳表现，在非推理模型上同样有效。

Conclusion: THOR通过系统性的工具集成优化框架，显著提升LLMs在复杂数学任务中的精度和可靠性，验证了分层强化学习策略的有效性。

Abstract: Large Language Models (LLMs) have made remarkable progress in mathematical
reasoning, but still continue to struggle with high-precision tasks like
numerical computation and formal symbolic manipulation. Integrating external
tools has emerged as a promising approach to bridge this gap. Despite recent
advances, existing methods struggle with three key challenges: constructing
tool-integrated reasoning data, performing fine-grained optimization, and
enhancing inference. To overcome these limitations, we propose THOR
(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,
a multi-agent actor-critic-based pipeline for constructing high-quality
datasets of tool-integrated reasoning paths, aligning with the policy and
generalizing well across diverse models. Second, to perform fine-grained
hierarchical optimization, we introduce an RL strategy that jointly optimizes
for both trajectory-level problem solving and step-level code generation. This
is motivated by our key insight that the success of an intermediate tool call
is a strong predictor of the final answer's correctness. Finally, THOR
incorporates a self-correction mechanism that leverages immediate tool feedback
to dynamically revise erroneous reasoning paths during inference. Our approach
demonstrates strong generalization across diverse models, performing
effectively in both reasoning and non-reasoning models. It further achieves
state-of-the-art performance for models of a similar scale on multiple
mathematical benchmarks, while also delivering consistent improvements on code
benchmarks. Our code will be publicly available at
https://github.com/JingMog/THOR.

</details>


### [56] [Exploring Major Transitions in the Evolution of Biological Cognition With Artificial Neural Networks](https://arxiv.org/abs/2509.13968)
*Konstantinos Voudouris,Andrew Barron,Marta Halina,Colin Klein,Matishalin Patel*

Main category: cs.AI

TL;DR: 通过比较前馈、循环和分层神经网络在语法学习中的表现，发现改变信息流结构（尤其是循环网络）会引发认知表现的阶段性跃迁，同时训练难度形成进化过渡的障碍。


<details>
  <summary>Details</summary>
Motivation: 验证认知进化是否通过神经网络信息流的结构性改变（类似进化重大转变）引发认知能力的质变

Method: 使用人工神经网络模型，控制网络规模和资源，测试不同拓扑结构（前馈/循环/分层）学习不同复杂度人工语法的性能

Result: 循环网络在复杂语法学习中表现显著优于前馈网络，但存在训练难度形成的过渡障碍；分层网络未显示出性能优势

Conclusion: 特定信息流结构变化（如循环拓扑）确实能产生认知表现的阶段性转变，并伴随进化过渡的典型特征（障碍与不可逆性）

Abstract: Transitional accounts of evolution emphasise a few changes that shape what is
evolvable, with dramatic consequences for derived lineages. More recently it
has been proposed that cognition might also have evolved via a series of major
transitions that manipulate the structure of biological neural networks,
fundamentally changing the flow of information. We used idealised models of
information flow, artificial neural networks (ANNs), to evaluate whether
changes in information flow in a network can yield a transitional change in
cognitive performance. We compared networks with feed-forward, recurrent and
laminated topologies, and tested their performance learning artificial grammars
that differed in complexity, controlling for network size and resources. We
documented a qualitative expansion in the types of input that recurrent
networks can process compared to feed-forward networks, and a related
qualitative increase in performance for learning the most complex grammars. We
also noted how the difficulty in training recurrent networks poses a form of
transition barrier and contingent irreversibility -- other key features of
evolutionary transitions. Not all changes in network topology confer a
performance advantage in this task set. Laminated networks did not outperform
non-laminated networks in grammar learning. Overall, our findings show how some
changes in information flow can yield transitions in cognitive performance.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [57] [TICL: Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models](https://arxiv.org/abs/2509.13395)
*Haolong Zheng,Yekaterina Yegorova,Mark Hasegawa-Johnson*

Main category: eess.AS

TL;DR: 提出TICL方法，通过语义上下文筛选示例提升语音基础模型的ASR性能，无需微调即可实现最高84.7%的WER相对下降。


<details>
  <summary>Details</summary>
Motivation: 现有SICL方法在上下文示例选择上缺乏系统性研究，制约了模型性能提升。

Method: 基于文本嵌入的KNN算法，根据输入语义动态选择最相关的上下文示例

Result: 在口音英语、多语言及儿童语音任务中实现最高84.7%的WER相对下降，消融实验验证方法鲁棒性

Conclusion: TICL为现有多模态模型提供零样本增强方案，显著提升跨领域语音识别效果

Abstract: Speech foundation models have recently demonstrated the ability to perform
Speech In-Context Learning (SICL). Selecting effective in-context examples is
crucial for SICL performance, yet selection methodologies remain underexplored.
In this work, we propose Text-Embedding KNN for SICL (TICL), a simple pipeline
that uses semantic context to enhance off-the-shelf large multimodal models'
speech recognition ability without fine-tuning. Across challenging automatic
speech recognition tasks, including accented English, multilingual speech, and
children's speech, our method enables models to surpass zero-shot performance
with up to 84.7% relative WER reduction. We conduct ablation studies to show
the robustness and efficiency of our method.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [58] [When Avatars Have Personality: Effects on Engagement and Communication in Immersive Medical Training](https://arxiv.org/abs/2509.14132)
*Julia S. Dollis,Iago A. Brito,Fernanda B. Färber,Pedro S. F. B. Ribeiro,Rafael T. Sousa,Arlindo R. Galvão Filho*

Main category: cs.HC

TL;DR: 整合大型语言模型与VR技术创建个性化虚拟患者，验证了医疗沟通训练系统的可行性并发现真实感-冗长悖论等核心设计原则


<details>
  <summary>Details</summary>
Motivation: 虚拟现实在复杂人际技能训练中存在虚拟患者心理真实性不足的缺陷，而医学教育中沟通能力是核心胜任力

Method: 开发模块化架构分离人格与临床数据，利用LLMs生成医学逻辑自洽的虚拟患者，采用混合研究方法进行医师模拟会诊实验

Result: 系统被证实可行且被医师认可为高效训练工具，发现较少交流的虚拟患者反而显假，训练挑战需具真实性才具教育价值

Conclusion: 本研究为下一代社交智能VR训练系统提供了经过验证的架构和关键设计洞见，推动医学教育技术创新

Abstract: While virtual reality (VR) excels at simulating physical environments, its
effectiveness for training complex interpersonal skills is limited by a lack of
psychologically plausible virtual humans. This is a critical gap in high-stakes
domains like medical education, where communication is a core competency. This
paper introduces a framework that integrates large language models (LLMs) into
immersive VR to create medically coherent virtual patients with distinct,
consistent personalities, built on a modular architecture that decouples
personality from clinical data. We evaluated our system in a mixed-method,
within-subjects study with licensed physicians who engaged in simulated
consultations. Results demonstrate that the approach is not only feasible but
is also perceived by physicians as a highly rewarding and effective training
enhancement. Furthermore, our analysis uncovers critical design principles,
including a ``realism-verbosity paradox" where less communicative agents can
seem more artificial, and the need for challenges to be perceived as authentic
to be instructive. This work provides a validated framework and key insights
for developing the next generation of socially intelligent VR training
environments.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [59] [A TRRIP Down Memory Lane: Temperature-Based Re-Reference Interval Prediction For Instruction Caching](https://arxiv.org/abs/2509.14041)
*Henry Kao,Nikhil Sreekumar,Prabhdeep Singh Soni,Ali Sedaghati,Fang Su,Bryan Chan,Maziar Goudarzi,Reza Azimi*

Main category: cs.AR

TL;DR: 提出TRRIP软硬件协同方案，通过编译器分析代码温度属性优化指令缓存替换策略，实现移动CPU性能提升


<details>
  <summary>Details</summary>
Motivation: 移动应用代码复杂性导致传统指令缓存策略失效，代码体积增速超过片上内存扩展速度，硬件中心化管理方法不足

Method: 编译器分析代码冷热属性→OS接口传递温度信息→硬件扩展利用温度属性优化RRIP缓存替换策略

Result: L2指令MPKI降低26.5%，PGO优化基础上实现3.9%的几何平均加速

Conclusion: TRRIP通过软件温度分类与硬件策略协同，有效降低热代码驱逐率，满足移动系统对软硬件的严苛要求

Abstract: Modern mobile CPU software pose challenges for conventional instruction cache
replacement policies due to their complex runtime behavior causing high reuse
distance between executions of the same instruction. Mobile code commonly
suffers from large amounts of stalls in the CPU frontend and thus starvation of
the rest of the CPU resources. Complexity of these applications and their code
footprint are projected to grow at a rate faster than available on-chip memory
due to power and area constraints, making conventional hardware-centric methods
for managing instruction caches to be inadequate. We present a novel
software-hardware co-design approach called TRRIP (Temperature-based
Re-Reference Interval Prediction) that enables the compiler to analyze,
classify, and transform code based on "temperature" (hot/cold), and to provide
the hardware with a summary of code temperature information through a
well-defined OS interface based on using code page attributes. TRRIP's
lightweight hardware extension employs code temperature attributes to optimize
the instruction cache replacement policy resulting in the eviction rate
reduction of hot code. TRRIP is designed to be practical and adoptable in real
mobile systems that have strict feature requirements on both the software and
hardware components. TRRIP can reduce the L2 MPKI for instructions by 26.5%
resulting in geomean speedup of 3.9%, on top of RRIP cache replacement running
mobile code already optimized using PGO.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [60] [An Empirical Study on Failures in Automated Issue Solving](https://arxiv.org/abs/2509.13941)
*Simiao Liu,Fang Liu,Liehao Li,Xin Tan,Yinghao Zhu,Xiaoli Lian,Li Zhang*

Main category: cs.SE

TL;DR: 通过分析SWE-Bench自动化解码任务失败案例，提出协作式Expert-Executor框架提升22.2%问题解决率


<details>
  <summary>Details</summary>
Motivation: 现有LLM工具在SWE-Bench测试中存在大量失败案例，且传统聚合指标无法诊断失败根源，需建立细粒度失败分类体系

Method: 1. 分析三种SOTA工具在SWE-Bench-Verified的表现 2. 人工标注150个失败案例构建3层级故障分类法 3. 提出专家-执行者双代理协作框架

Result: 新框架解决了单代理系统22.2%的顽固问题，识别出agentic架构主要失败源于推理缺陷(占63.6%)和认知僵局(占25.3%)

Conclusion: 通过诊断性评估揭示失败模式特征，协作式代理设计能有效突破认知瓶颈，为构建鲁棒自动编程系统提供新方向

Abstract: Automated issue solving seeks to autonomously identify and repair defective
code snippets across an entire codebase. SWE-Bench has emerged as the most
widely adopted benchmark for evaluating progress in this area. While LLM-based
agentic tools show great promise, they still fail on a substantial portion of
tasks. Moreover, current evaluations primarily report aggregate issue-solving
rates, which obscure the underlying causes of success and failure, making it
challenging to diagnose model weaknesses or guide targeted improvements. To
bridge this gap, we first analyze the performance and efficiency of three SOTA
tools, spanning both pipeline-based and agentic architectures, in automated
issue solving tasks of SWE-Bench-Verified under varying task characteristics.
Furthermore, to move from high-level performance metrics to underlying cause
analysis, we conducted a systematic manual analysis of 150 failed instances.
From this analysis, we developed a comprehensive taxonomy of failure modes
comprising 3 primary phases, 9 main categories, and 25 fine-grained
subcategories. Then we systematically analyze the distribution of the
identified failure modes, the results reveal distinct failure fingerprints
between the two architectural paradigms, with the majority of agentic failures
stemming from flawed reasoning and cognitive deadlocks. Motivated by these
insights, we propose a collaborative Expert-Executor framework. It introduces a
supervisory Expert agent tasked with providing strategic oversight and
course-correction for a primary Executor agent. This architecture is designed
to correct flawed reasoning and break the cognitive deadlocks that frequently
lead to failure. Experiments show that our framework solves 22.2% of previously
intractable issues for a leading single agent. These findings pave the way for
building more robust agents through diagnostic evaluation and collaborative
design.

</details>


### [61] [Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A Self-Optimizing Framework](https://arxiv.org/abs/2509.14093)
*Kerui Huang,Shuhan Liu,Xing Hu,Tongtong Xu,Lingfeng Bao,Xin Xia*

Main category: cs.SE

TL;DR: CoT推理增强LLM性能但带来高计算成本，SEER框架通过自适应压缩CoT在保持精度的同时提升效率


<details>
  <summary>Details</summary>
Motivation: CoT推理虽然提高了LLM在逻辑和代码生成等任务的准确性，但冗长输出导致计算延迟、内存占用激增等问题，尤其在需要简洁输出的软件工程场景中矛盾突出

Method: 提出SEER框架，结合Best-of-N采样和任务感知自适应过滤技术，通过预推理输出动态调整阈值，实现CoT的压缩与优化

Result: SEER平均缩短42.1%的推理步骤，有效减少截断错误和无限循环，在保持精度的同时显著降低计算开销

Conclusion: SEER证明了自适应控制CoT长度的有效性，为资源受限场景下的高效推理提供了实用解决方案

Abstract: Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by
prompting intermediate steps, improving accuracy and robustness in arithmetic,
logic, and commonsense tasks. However, this benefit comes with high
computational costs: longer outputs increase latency, memory usage, and
KV-cache demands. These issues are especially critical in software engineering
tasks where concise and deterministic outputs are required. To investigate
these trade-offs, we conduct an empirical study based on code generation
benchmarks. The results reveal that longer CoT does not always help. Excessive
reasoning often causes truncation, accuracy drops, and latency up to five times
higher, with failed outputs consistently longer than successful ones. These
findings challenge the assumption that longer reasoning is inherently better
and highlight the need for adaptive CoT control. Motivated by this, we propose
SEER (Self-Enhancing Efficient Reasoning), an adaptive framework that
compresses CoT while preserving accuracy. SEER combines Best-of-N sampling with
task-aware adaptive filtering, dynamically adjusting thresholds based on
pre-inference outputs to reduce verbosity and computational overhead. We then
evaluate SEER on three software engineering tasks and one math task. On
average, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,
and eliminates most infinite loops. These results demonstrate SEER as a
practical method to make CoT-enhanced LLMs more efficient and robust, even
under resource constraints.

</details>
