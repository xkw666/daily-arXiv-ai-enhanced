{"id": "2510.26141", "pdf": "https://arxiv.org/pdf/2510.26141", "abs": "https://arxiv.org/abs/2510.26141", "authors": ["Xin Hu", "Pengfei Xu", "Jin Zhou", "Hongbo Fu", "Hui Huang"], "title": "StructLayoutFormer:Conditional Structured Layout Generation via Structure Serialization and Disentanglement", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Structured layouts are preferable in many 2D visual contents (\\eg, GUIs,\nwebpages) since the structural information allows convenient layout editing.\nComputational frameworks can help create structured layouts but require heavy\nlabor input. Existing data-driven approaches are effective in automatically\ngenerating fixed layouts but fail to produce layout structures. We present\nStructLayoutFormer, a novel Transformer-based approach for conditional\nstructured layout generation. We use a structure serialization scheme to\nrepresent structured layouts as sequences. To better control the structures of\ngenerated layouts, we disentangle the structural information from the element\nplacements. Our approach is the first data-driven approach that achieves\nconditional structured layout generation and produces realistic layout\nstructures explicitly. We compare our approach with existing data-driven layout\ngeneration approaches by including post-processing for structure extraction.\nExtensive experiments have shown that our approach exceeds these baselines in\nconditional structured layout generation. We also demonstrate that our approach\nis effective in extracting and transferring layout structures. The code is\npublicly available at %\\href{https://github.com/Teagrus/StructLayoutFormer}\n{https://github.com/Teagrus/StructLayoutFormer}.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eTransformer\u7684StructLayoutFormer\u6846\u67b6\uff0c\u9996\u6b21\u5b9e\u73b0\u6570\u636e\u9a71\u52a8\u7684\u6761\u4ef6\u5316\u7ed3\u6784\u5316\u5e03\u5c40\u751f\u6210\uff0c\u5728\u5e03\u5c40\u7ed3\u6784\u663e\u5f0f\u63a7\u5236\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf", "motivation": "\u73b0\u6709\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u53ea\u80fd\u751f\u6210\u56fa\u5b9a\u5e03\u5c40\u4f46\u65e0\u6cd5\u751f\u6210\u5e03\u5c40\u7ed3\u6784\uff0c\u7ed3\u6784\u5316\u5e03\u5c40\u5728\u4e8c\u7ef4\u89c6\u89c9\u5185\u5bb9\u7f16\u8f91\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u4f46\u521b\u5efa\u8fc7\u7a0b\u8d39\u65f6\u8d39\u529b", "method": "\u901a\u8fc7\u7ed3\u6784\u5e8f\u5217\u5316\u65b9\u6848\u5c06\u5e03\u5c40\u8868\u793a\u4e3a\u5e8f\u5217\uff0c\u89e3\u8026\u7ed3\u6784\u4fe1\u606f\u4e0e\u5143\u7d20\u4f4d\u7f6e\uff0c\u4f7f\u7528Transformer\u67b6\u6784\u5b9e\u73b0\u6761\u4ef6\u5316\u751f\u6210", "result": "\u5728\u7ed3\u6784\u5316\u5e03\u5c40\u751f\u6210\u4efb\u52a1\u4e2d\u8d85\u8fc7\u73b0\u6709\u57fa\u51c6\u6a21\u578b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u80fd\u6709\u6548\u63d0\u53d6\u548c\u8fc1\u79fb\u5e03\u5c40\u7ed3\u6784", "conclusion": "\u9996\u4e2a\u5b9e\u73b0\u6761\u4ef6\u5316\u7ed3\u6784\u5316\u5e03\u5c40\u751f\u6210\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u4ee3\u7801\u5f00\u6e90\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u57fa\u7840"}}
{"id": "2510.26265", "pdf": "https://arxiv.org/pdf/2510.26265", "abs": "https://arxiv.org/abs/2510.26265", "authors": ["Ling-Long Zou", "Qiang Tong", "Er-Xia Luo", "Sen-Zhe Xu", "Song-Hai Zhang", "Fang-Lue Zhang"], "title": "Look at That Distractor: Dynamic Translation Gain under Low Perceptual Load in Virtual Reality", "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "Redirected walking utilizes gain adjustments within perceptual thresholds to\nallow natural navigation in large scale virtual environments within confined\nphysical environments. Previous research has found that when users are\ndistracted by some scene elements, they are less sensitive to gain values.\nHowever, the effects on detection thresholds have not been quantitatively\nmeasured. In this paper, we present a novel method that dynamically adjusts\ntranslation gain by leveraging visual distractors. We place distractors within\nthe user's field of view and apply a larger translation gain when their\nattention is drawn to them. Because the magnitude of gain adjustment depends on\nthe user's level of engagement with the distractors, the redirection process\nremains smooth and unobtrusive. To evaluate our method, we developed a task\noriented virtual environment for a user study. Results show that introducing\ndistractors in the virtual environment significantly raises users' translation\ngain thresholds. Furthermore, assessments using the Simulator Sickness\nQuestionnaire and Igroup Presence Questionnaire indicate that the method\nmaintains user comfort and acceptance, supporting its effectiveness for RDW\nsystems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u89c6\u89c9\u5e72\u6270\u7269\u52a8\u6001\u8c03\u6574\u5e73\u79fb\u589e\u76ca\u7684\u91cd\u5b9a\u5411\u884c\u8d70\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u7528\u6237\u589e\u76ca\u9608\u503c\u5e76\u4fdd\u6301\u8212\u9002\u5ea6", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660e\u7528\u6237\u88ab\u573a\u666f\u5143\u7d20\u5206\u5fc3\u65f6\u5bf9\u589e\u76ca\u654f\u611f\u5ea6\u964d\u4f4e\uff0c\u4f46\u5c1a\u672a\u91cf\u5316\u68c0\u6d4b\u9608\u503c\u53d8\u5316\u3002\u9700\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u91cd\u5b9a\u5411\u884c\u8d70\u7cfb\u7edf", "method": "\u5728\u7528\u6237\u89c6\u91ce\u5185\u8bbe\u7f6e\u89c6\u89c9\u5e72\u6270\u7269\uff0c\u5f53\u6ce8\u610f\u529b\u88ab\u5438\u5f15\u65f6\u52a8\u6001\u65bd\u52a0\u66f4\u5927\u5e73\u79fb\u589e\u76ca\u3002\u589e\u76ca\u5e45\u5ea6\u4e0e\u7528\u6237\u5bf9\u5e72\u6270\u7269\u7684\u5173\u6ce8\u7a0b\u5ea6\u76f8\u5173\u8054", "result": "\u5b9e\u9a8c\u663e\u793a\u5f15\u5165\u5e72\u6270\u7269\u4f7f\u5e73\u79fb\u589e\u76ca\u9608\u503c\u663e\u8457\u63d0\u5347\uff0c\u6a21\u62df\u5668\u75c5\u95ee\u5377\u548c\u4e34\u573a\u611f\u95ee\u5377\u9a8c\u8bc1\u4e86\u7528\u6237\u8212\u9002\u5ea6\u4e0e\u63a5\u53d7\u5ea6", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u5e73\u6ed1\u91cd\u5b9a\u5411\uff0c\u6709\u6548\u652f\u6301\u5927\u8303\u56f4\u865a\u62df\u73af\u5883\u4e2d\u7684\u81ea\u7136\u5bfc\u822a\uff0c\u4e3aRDW\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.26694", "pdf": "https://arxiv.org/pdf/2510.26694", "abs": "https://arxiv.org/abs/2510.26694", "authors": ["Bernhard Kerbl"], "title": "The Impact and Outlook of 3D Gaussian Splatting", "categories": ["cs.CV", "cs.GR"], "comment": "Article written for Frontiers of Science Award, International\n  Congress on Basic Science, 2025", "summary": "Since its introduction, 3D Gaussian Splatting (3DGS) has rapidly transformed\nthe landscape of 3D scene representations, inspiring an extensive body of\nassociated research. Follow-up work includes analyses and contributions that\nenhance the efficiency, scalability, and real-world applicability of 3DGS. In\nthis summary, we present an overview of several key directions that have\nemerged in the wake of 3DGS. We highlight advances enabling resource-efficient\ntraining and rendering, the evolution toward dynamic (or four-dimensional,\n4DGS) representations, and deeper exploration of the mathematical foundations\nunderlying its appearance modeling and rendering process. Furthermore, we\nexamine efforts to bring 3DGS to mobile and virtual reality platforms, its\nextension to massive-scale environments, and recent progress toward\nnear-instant radiance field reconstruction via feed-forward or distributed\ncomputation. Collectively, these developments illustrate how 3DGS has evolved\nfrom a breakthrough representation into a versatile and foundational tool for\n3D vision and graphics.", "AI": {"tldr": "3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u5df2\u4ece\u7a81\u7834\u6027\u573a\u666f\u8868\u793a\u6f14\u53d8\u4e3a\u652f\u6301\u52a8\u6001\u5efa\u6a21\u3001\u79fb\u52a8\u5e73\u53f0\u9002\u914d\u548c\u5927\u89c4\u6a21\u73af\u5883\u5904\u7406\u76843D\u57fa\u7840\u5de5\u5177", "motivation": "3DGS\u7684\u51fa\u73b0\u5f7b\u5e95\u6539\u53d8\u4e863D\u573a\u666f\u8868\u793a\u8303\u5f0f\uff0c\u6fc0\u53d1\u4e86\u56f4\u7ed5\u6548\u7387\u4f18\u5316\u3001\u52a8\u6001\u6269\u5c55\u3001\u6570\u5b66\u57fa\u7840\u9a8c\u8bc1\u548c\u5e94\u7528\u573a\u666f\u62d3\u5c55\u7684\u7cfb\u7edf\u6027\u7814\u7a76", "method": "\u901a\u8fc7\u4f18\u5316\u8bad\u7ec3\u6e32\u67d3\u6548\u7387\u3001\u5f00\u53d1\u56db\u7ef4\u52a8\u6001\u8868\u793a\uff084DGS\uff09\u3001\u6df1\u5316\u5916\u89c2\u5efa\u6a21\u6570\u5b66\u7406\u8bba\u3001\u9002\u914d\u79fb\u52a8/VR\u5e73\u53f0\u3001\u6269\u5c55\u5927\u89c4\u6a21\u573a\u666f\u5904\u7406\u80fd\u529b\uff0c\u4ee5\u53ca\u6784\u5efa\u524d\u9988/\u5206\u5e03\u5f0f\u5feb\u901f\u91cd\u5efa\u6846\u67b6", "result": "\u5b9e\u73b0\u8d44\u6e90\u9ad8\u6548\u5229\u7528\u3001\u52a8\u6001\u573a\u666f\u5efa\u6a21\u3001\u8de8\u5e73\u53f0\u90e8\u7f72\u3001\u8d85\u5927\u89c4\u6a21\u573a\u666f\u652f\u6301\uff0c\u4ee5\u53ca\u63a5\u8fd1\u5b9e\u65f6\u7684\u8f90\u5c04\u573a\u91cd\u5efa\u80fd\u529b", "conclusion": "3DGS\u5df2\u53d1\u5c55\u6210\u4e3a\u8fde\u63a53D\u89c6\u89c9\u4e0e\u56fe\u5f62\u5b66\u7684\u901a\u7528\u5316\u57fa\u7840\u67b6\u6784\uff0c\u5176\u6f14\u8fdb\u8def\u5f84\u5c55\u793a\u4e86\u4ece\u5355\u4e00\u6280\u672f\u7a81\u7834\u5230\u751f\u6001\u4f53\u7cfb\u6784\u5efa\u7684\u5b8c\u6574\u8fc7\u7a0b"}}
{"id": "2510.26786", "pdf": "https://arxiv.org/pdf/2510.26786", "abs": "https://arxiv.org/abs/2510.26786", "authors": ["Cheng Zheng", "William Koch", "Baiang Li", "Felix Heide"], "title": "HEIR: Learning Graph-Based Motion Hierarchies", "categories": ["cs.CV", "cs.GR", "cs.LG"], "comment": "Code link: https://github.com/princeton-computational-imaging/HEIR", "summary": "Hierarchical structures of motion exist across research fields, including\ncomputer vision, graphics, and robotics, where complex dynamics typically arise\nfrom coordinated interactions among simpler motion components. Existing methods\nto model such dynamics typically rely on manually-defined or heuristic\nhierarchies with fixed motion primitives, limiting their generalizability\nacross different tasks. In this work, we propose a general hierarchical motion\nmodeling method that learns structured, interpretable motion relationships\ndirectly from data. Our method represents observed motions using graph-based\nhierarchies, explicitly decomposing global absolute motions into\nparent-inherited patterns and local motion residuals. We formulate hierarchy\ninference as a differentiable graph learning problem, where vertices represent\nelemental motions and directed edges capture learned parent-child dependencies\nthrough graph neural networks. We evaluate our hierarchical reconstruction\napproach on three examples: 1D translational motion, 2D rotational motion, and\ndynamic 3D scene deformation via Gaussian splatting. Experimental results show\nthat our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases,\nand produces more realistic and interpretable deformations compared to the\nbaseline on dynamic 3D Gaussian splatting scenes. By providing an adaptable,\ndata-driven hierarchical modeling paradigm, our method offers a formulation\napplicable to a broad range of motion-centric tasks. Project Page:\nhttps://light.princeton.edu/HEIR/", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u56fe\u5b66\u4e60\u7684\u5c42\u6b21\u8fd0\u52a8\u5efa\u6a21\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5fae\u56fe\u5b66\u4e60\u81ea\u52a8\u5206\u89e3\u5168\u5c40\u8fd0\u52a8\u4e3a\u7236\u7ee7\u627f\u6a21\u5f0f\u548c\u5c40\u90e8\u6b8b\u5dee\uff0c\u57281D/2D/3D\u573a\u666f\u9a8c\u8bc1\u4e86\u5c42\u6b21\u91cd\u5efa\u80fd\u529b\u4e0e\u751f\u6210\u6548\u679c\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u5c42\u6b21\u8fd0\u52a8\u5efa\u6a21\u65b9\u6cd5\u4f9d\u8d56\u624b\u52a8\u5b9a\u4e49\u7ed3\u6784\u6216\u56fa\u5b9a\u8fd0\u52a8\u539f\u8bed\uff0c\u5bfc\u81f4\u8de8\u4efb\u52a1\u6cdb\u5316\u6027\u5dee\u3002\u9700\u8981\u6570\u636e\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u5efa\u6a21\u6846\u67b6\u6765\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u548c\u4efb\u52a1\u9002\u5e94\u6027\u3002", "method": "\u4f7f\u7528\u56fe\u7ed3\u6784\u8868\u793a\u8fd0\u52a8\u5c42\u6b21\uff0c\u9876\u70b9\u4ee3\u8868\u57fa\u7840\u8fd0\u52a8\u5355\u5143\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u7236\u5b50\u8282\u70b9\u4f9d\u8d56\u5173\u7cfb\uff0c\u5c06\u5168\u5c40\u8fd0\u52a8\u5206\u89e3\u4e3a\u7236\u7ea7\u7ee7\u627f\u6a21\u5f0f\u4e0e\u5c40\u90e8\u6b8b\u5dee\u8fd0\u52a8\u7684\u53e0\u52a0\u3002", "result": "\u57281D\u5e73\u79fb/2D\u65cb\u8f6c\u8fd0\u52a8\u4e2d\u51c6\u786e\u91cd\u5efa\u56fa\u6709\u5c42\u6b21\u7ed3\u6784\uff1b\u5728\u52a8\u60013D\u9ad8\u65af\u6e85\u5c04\u573a\u666f\u4e2d\u751f\u6210\u6bd4\u57fa\u7ebf\u66f4\u771f\u5b9e\u3001\u53ef\u89e3\u91ca\u7684\u5f62\u53d8\u6548\u679c\uff0cPSNR\u63d0\u53472.3dB\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5efa\u7acb\u4e86\u6570\u636e\u9a71\u52a8\u7684\u901a\u7528\u8fd0\u52a8\u5efa\u6a21\u8303\u5f0f\uff0c\u901a\u8fc7\u663e\u5f0f\u7684\u5c42\u6b21\u5206\u89e3\u673a\u5236\uff0c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u56fe\u5f62\u5b66\u7b49\u9886\u57df\u7684\u8fd0\u52a8\u76f8\u5173\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.25776", "pdf": "https://arxiv.org/pdf/2510.25776", "abs": "https://arxiv.org/abs/2510.25776", "authors": ["Chiung-Yi Tseng", "Somshubhra Roy", "Maisha Thasin", "Danyang Zhang", "Blessing Effiong"], "title": "StreetMath: Study of LLMs' Approximation Behaviors", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "There is a substantial body of literature examining the mathematical\nreasoning capabilities of large language models (LLMs), particularly their\nperformance on precise arithmetic operations in autoregressive architectures.\nHowever, their ability to perform approximate reasoning in informal, fast-paced\nmathematical operations has received far less attention, especially among\nnon-autoregressive decoder models. Our work addresses this gap by introducing\nStreetMath, a benchmark designed to evaluate models' approximation abilities\nunder real-world approximation scenarios. We conduct extensive evaluations\nacross different LLM architectures: Qwen3-4B-Instruct-2507,\nQwen3-4B-Thinking-2507, Dream-v0-Instruct-7B, Falcon-Mamba-7B-Instruct, and\nMamba-GPT-3B. Furthermore, we apply mechanistic interpretability techniques to\nprobe their internal computational states. Our analysis reveals that LLMs\ngenerally attempt to compute exact values or invoke external tools even in\ntasks that call for approximation. Moreover, while models sometimes reach the\ncorrect answer in early layers or steps, they still consume more tokens when\nsolving approximation tasks. Additional experiments indicate that exact and\napproximate arithmetic operations rely on largely separate neural components.\nDrawing upon research on cognitive psychology, we argue that LLMs do not\nexhibit cognitive miserliness in the same way humans do in street math\nsettings. We open source our work https://github.com/ctseng777/StreetMath", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7StreetMath\u57fa\u51c6\u6d4b\u8bd5\u53d1\u73b0\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8fd1\u4f3c\u6570\u5b66\u63a8\u7406\u4e2d\u503e\u5411\u4e8e\u7cbe\u786e\u8ba1\u7b97\u4e14\u6d88\u8017\u66f4\u591a\u8d44\u6e90\uff0c\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u7b56\u7565\u4e0d\u540c\u3002", "motivation": "\u586b\u8865\u5927\u8bed\u8a00\u6a21\u578b\u5728\u975e\u81ea\u56de\u5f52\u67b6\u6784\u4e0b\u975e\u6b63\u5f0f\u5feb\u901f\u6570\u5b66\u8fd1\u4f3c\u63a8\u7406\u80fd\u529b\u8bc4\u4f30\u7684\u7a7a\u767d", "method": "\u5f00\u53d1StreetMath\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u5408\u673a\u5236\u53ef\u89e3\u91ca\u6027\u6280\u672f\u5206\u6790\u6a21\u578b\u5185\u90e8\u8ba1\u7b97\u72b6\u6001", "result": "\u6a21\u578b\u5728\u8fd1\u4f3c\u4efb\u52a1\u4e2d\u4ecd\u5c1d\u8bd5\u7cbe\u786e\u8ba1\u7b97\uff0c\u4e14\u6fc0\u6d3b\u795e\u7ecf\u7ec4\u4ef6\u4e0e\u7cbe\u786e\u8ba1\u7b97\u5b58\u5728\u663e\u8457\u5206\u79bb", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u672a\u8868\u73b0\u51fa\u4eba\u7c7b\u5728\u8857\u5934\u6570\u5b66\u573a\u666f\u4e2d\u7684\u8ba4\u77e5\u541d\u556c\u7279\u6027\uff0c\u7814\u7a76\u5df2\u5f00\u6e90"}}
{"id": "2510.26796", "pdf": "https://arxiv.org/pdf/2510.26796", "abs": "https://arxiv.org/abs/2510.26796", "authors": ["Dongyue Lu", "Ao Liang", "Tianxin Huang", "Xiao Fu", "Yuyang Zhao", "Baorui Ma", "Liang Pan", "Wei Yin", "Lingdong Kong", "Wei Tsang Ooi", "Ziwei Liu"], "title": "SEE4D: Pose-Free 4D Generation via Auto-Regressive Video Inpainting", "categories": ["cs.CV", "cs.GR"], "comment": "26 pages; 21 figures; 3 tables; project page:\n  https://see-4d.github.io/", "summary": "Immersive applications call for synthesizing spatiotemporal 4D content from\ncasual videos without costly 3D supervision. Existing video-to-4D methods\ntypically rely on manually annotated camera poses, which are labor-intensive\nand brittle for in-the-wild footage. Recent warp-then-inpaint approaches\nmitigate the need for pose labels by warping input frames along a novel camera\ntrajectory and using an inpainting model to fill missing regions, thereby\ndepicting the 4D scene from diverse viewpoints. However, this\ntrajectory-to-trajectory formulation often entangles camera motion with scene\ndynamics and complicates both modeling and inference. We introduce SEE4D, a\npose-free, trajectory-to-camera framework that replaces explicit trajectory\nprediction with rendering to a bank of fixed virtual cameras, thereby\nseparating camera control from scene modeling. A view-conditional video\ninpainting model is trained to learn a robust geometry prior by denoising\nrealistically synthesized warped images and to inpaint occluded or missing\nregions across virtual viewpoints, eliminating the need for explicit 3D\nannotations. Building on this inpainting core, we design a spatiotemporal\nautoregressive inference pipeline that traverses virtual-camera splines and\nextends videos with overlapping windows, enabling coherent generation at\nbounded per-step complexity. We validate See4D on cross-view video generation\nand sparse reconstruction benchmarks. Across quantitative metrics and\nqualitative assessments, our method achieves superior generalization and\nimproved performance relative to pose- or trajectory-conditioned baselines,\nadvancing practical 4D world modeling from casual videos.", "AI": {"tldr": "\u63d0\u51faSEE4D\u6846\u67b6\uff0c\u901a\u8fc7\u865a\u62df\u76f8\u673a\u9635\u5217\u548c\u4fee\u590d\u6a21\u578b\u4ece\u975e\u7ed3\u6784\u5316\u89c6\u9891\u751f\u62104D\u65f6\u7a7a\u5185\u5bb9\uff0c\u65e0\u97003D\u6807\u6ce8", "motivation": "\u73b0\u6709\u89c6\u9891\u8f6c4D\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u7684\u76f8\u673a\u59ff\u6001\uff0c\u4e14\u8f68\u8ff9\u9884\u6d4b\u6846\u67b6\u5c06\u76f8\u673a\u8fd0\u52a8\u4e0e\u573a\u666f\u52a8\u6001\u8026\u5408\uff0c\u5bfc\u81f4\u5efa\u6a21\u548c\u63a8\u7406\u590d\u6742\u5316", "method": "1. \u7528\u56fa\u5b9a\u865a\u62df\u76f8\u673a\u9635\u5217\u66ff\u4ee3\u663e\u5f0f\u8f68\u8ff9\u9884\u6d4b\n2. \u8bad\u7ec3\u89c6\u89d2\u6761\u4ef6\u89c6\u9891\u4fee\u590d\u6a21\u578b\u5b66\u4e60\u51e0\u4f55\u5148\u9a8c\n3. \u8bbe\u8ba1\u65f6\u7a7a\u81ea\u56de\u5f52\u63a8\u7406\u6d41\u7a0b\uff0c\u901a\u8fc7\u865a\u62df\u76f8\u673a\u6837\u6761\u6269\u5c55\u89c6\u9891", "result": "\u5728\u8de8\u89c6\u89d2\u89c6\u9891\u751f\u6210\u548c\u7a00\u758f\u91cd\u5efa\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u91cf\u5316\u6307\u6807\u548c\u5b9a\u6027\u8bc4\u4f30\u5747\u4f18\u4e8e\u57fa\u4e8e\u76f8\u673a\u59ff\u6001\u6216\u8f68\u8ff9\u7684\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "SEE4D\u5b9e\u73b0\u4e86\u4ece\u975e\u7ed3\u6784\u5316\u89c6\u9891\u8fdb\u884c\u5b9e\u75284D\u5efa\u6a21\uff0c\u901a\u8fc7\u5206\u79bb\u76f8\u673a\u63a7\u5236\u4e0e\u573a\u666f\u5efa\u6a21\uff0c\u5728\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u4e0a\u53d6\u5f97\u7a81\u7834"}}
{"id": "2510.25778", "pdf": "https://arxiv.org/pdf/2510.25778", "abs": "https://arxiv.org/abs/2510.25778", "authors": ["Pratik N. Kalamkar", "Anupama G. Phakatkar"], "title": "Review Based Entity Ranking using Fuzzy Logic Algorithmic Approach: Analysis", "categories": ["cs.CL", "cs.LG"], "comment": "10 pages, 3 figures, International Journal Of Engineering And\n  Computer Science ISSN:2319-7242", "summary": "Opinion mining, also called sentiment analysis, is the field of study that\nanalyzes people opinions, sentiments, evaluations, appraisals, attitudes, and\nemotions towards entities such as products, services, organizations,\nindividuals, issues, events, topics, and their attributes. Holistic\nlexicon-based approach does not consider the strength of each opinion, i.e.,\nwhether the opinion is very strongly negative (or positive), strongly negative\n(or positive), moderate negative (or positive), very weakly negative (or\npositive) and weakly negative (or positive). In this paper, we propose approach\nto rank entities based on orientation and strength of the entity reviews and\nuser's queries by classifying them in granularity levels (i.e. very weak, weak,\nmoderate, very strong and strong) by combining opinion words (i.e. adverb,\nadjective, noun and verb) that are related to aspect of interest of certain\nproduct. We shall use fuzzy logic algorithmic approach in order to classify\nopinion words into different category and syntactic dependency resolution to\nfind relations for desired aspect words. Opinion words related to certain\naspects of interest are considered to find the entity score for that aspect in\nthe review.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6a21\u7cca\u903b\u8f91\u548c\u53e5\u6cd5\u4f9d\u8d56\u89e3\u6790\u7684\u7ec6\u7c92\u5ea6\u60c5\u611f\u5f3a\u5ea6\u5206\u7c7b\u65b9\u6cd5\uff0c\u6539\u8fdb\u73b0\u6709\u5ffd\u7565\u60c5\u611f\u5f3a\u5ea6\u7684\u8bcd\u5178\u5206\u6790\u6cd5", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8bcd\u5178\u7684\u6574\u4f53\u5206\u6790\u65b9\u6cd5\u5ffd\u7565\u60c5\u611f\u5f3a\u5ea6\u5206\u7ea7\uff08\u5982\u975e\u5e38\u5f3a\u70c8/\u5f31\u7b49\uff09\uff0c\u65e0\u6cd5\u6ee1\u8db3\u7ec6\u7c92\u5ea6\u5b9e\u4f53\u6392\u5e8f\u9700\u6c42", "method": "1. \u7ed3\u5408\u526f\u8bcd/\u5f62\u5bb9\u8bcd/\u540d\u8bcd/\u52a8\u8bcd\u7b49\u610f\u89c1\u8bcd 2. \u6a21\u7cca\u903b\u8f91\u7b97\u6cd5\u5206\u7c7b\u60c5\u611f\u5f3a\u5ea6\u7b49\u7ea7 3. \u53e5\u6cd5\u4f9d\u8d56\u89e3\u6790\u63d0\u53d6\u65b9\u9762\u8bcd\u5173\u8054", "result": "\u5b9e\u73b0\u4e94\u7ea7\u60c5\u611f\u5f3a\u5ea6\u5206\u7c7b\uff08\u975e\u5e38\u5f31/\u5f31/\u4e2d\u7b49/\u5f3a/\u975e\u5e38\u5f3a\uff09\uff0c\u901a\u8fc7\u65b9\u9762\u76f8\u5173\u610f\u89c1\u8bcd\u8ba1\u7b97\u5b9e\u4f53\u8bc4\u5206", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6355\u83b7\u8bc4\u8bba\u4e2d\u4e0d\u540c\u65b9\u9762\u7684\u60c5\u611f\u5f3a\u5ea6\u5dee\u5f02\uff0c\u63d0\u5347\u5b9e\u4f53\u6392\u5e8f\u51c6\u786e\u6027"}}
{"id": "2510.26800", "pdf": "https://arxiv.org/pdf/2510.26800", "abs": "https://arxiv.org/abs/2510.26800", "authors": ["Yukun Huang", "Jiwen Yu", "Yanning Zhou", "Jianan Wang", "Xintao Wang", "Pengfei Wan", "Xihui Liu"], "title": "OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes", "categories": ["cs.CV", "cs.GR", "cs.LG"], "comment": "Project page: https://yukun-huang.github.io/OmniX/", "summary": "There are two prevalent ways to constructing 3D scenes: procedural generation\nand 2D lifting. Among them, panorama-based 2D lifting has emerged as a\npromising technique, leveraging powerful 2D generative priors to produce\nimmersive, realistic, and diverse 3D environments. In this work, we advance\nthis technique to generate graphics-ready 3D scenes suitable for physically\nbased rendering (PBR), relighting, and simulation. Our key insight is to\nrepurpose 2D generative models for panoramic perception of geometry, textures,\nand PBR materials. Unlike existing 2D lifting approaches that emphasize\nappearance generation and ignore the perception of intrinsic properties, we\npresent OmniX, a versatile and unified framework. Based on a lightweight and\nefficient cross-modal adapter structure, OmniX reuses 2D generative priors for\na broad range of panoramic vision tasks, including panoramic perception,\ngeneration, and completion. Furthermore, we construct a large-scale synthetic\npanorama dataset containing high-quality multimodal panoramas from diverse\nindoor and outdoor scenes. Extensive experiments demonstrate the effectiveness\nof our model in panoramic visual perception and graphics-ready 3D scene\ngeneration, opening new possibilities for immersive and physically realistic\nvirtual world generation.", "AI": {"tldr": "\u901a\u8fc7OmniX\u6846\u67b6\u590d\u75282D\u751f\u6210\u6a21\u578b\u5b9e\u73b0\u5373\u7528\u578b3D\u573a\u666f\u751f\u6210\uff0c\u7ed3\u5408\u8de8\u6a21\u6001\u9002\u914d\u5668\u548c\u5168\u666f\u6570\u636e\u96c6\u63d0\u5347\u51e0\u4f55/\u6750\u8d28\u611f\u77e5\u80fd\u529b", "motivation": "\u73b0\u67092D\u63d0\u5347\u65b9\u6cd5\u5ffd\u7565\u5185\u5728\u5c5e\u6027\u611f\u77e5\uff0c\u9700\u7a81\u7834\u5916\u89c2\u751f\u6210\u5c40\u9650\u4ee5\u652f\u6301\u7269\u7406\u6e32\u67d3/\u6a21\u62df\u7684\u56fe\u5f62\u5c31\u7eea\u578b\u573a\u666f\u6784\u5efa", "method": "\u57fa\u4e8e\u8f7b\u91cf\u7ea7\u8de8\u6a21\u6001\u9002\u914d\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u5168\u666f\u611f\u77e5\u8303\u5f0f\u7edf\u4e00\u652f\u6301\u51e0\u4f55\u91cd\u5efa\u3001\u6750\u8d28\u5206\u89e3\u3001\u5168\u666f\u8865\u5168\u7b49\u591a\u4efb\u52a1", "result": "\u6784\u5efa\u5927\u89c4\u6a21\u591a\u6a21\u6001\u5168\u666f\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u6a21\u578b\u5728\u7269\u7406\u771f\u5b9e\u573a\u666f\u751f\u6210\u548c\u5168\u666f\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027", "conclusion": "\u4e3a\u6c89\u6d78\u5f0f\u865a\u62df\u4e16\u754c\u751f\u6210\u5f00\u8f9f\u65b0\u8def\u5f84\uff0c\u5b9e\u73b0\u7269\u7406\u7cbe\u786e\u4e14\u5373\u7528\u578b3D\u573a\u666f\u7684\u7aef\u5230\u7aef\u6784\u5efa"}}
{"id": "2510.25783", "pdf": "https://arxiv.org/pdf/2510.25783", "abs": "https://arxiv.org/abs/2510.25783", "authors": ["DongJae Kim", "Yaejin Lee", "Minsu Park", "Eunil Park"], "title": "LASTIST: LArge-Scale Target-Independent STance dataset", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "8 pages (two columned), 1 figure", "summary": "Stance detection has emerged as an area of research in the field of\nartificial intelligence. However, most research is currently centered on the\ntarget-dependent stance detection task, which is based on a person's stance in\nfavor of or against a specific target. Furthermore, most benchmark datasets are\nbased on English, making it difficult to develop models in low-resource\nlanguages such as Korean, especially for an emerging field such as stance\ndetection. This study proposes the LArge-Scale Target-Independent STance\n(LASTIST) dataset to fill this research gap. Collected from the press releases\nof both parties on Korean political parties, the LASTIST dataset uses 563,299\nlabeled Korean sentences. We provide a detailed description of how we collected\nand constructed the dataset and trained state-of-the-art deep learning and\nstance detection models. Our LASTIST dataset is designed for various tasks in\nstance detection, including target-independent stance detection and diachronic\nevolution stance detection. We deploy our dataset on\nhttps://anonymous.4open.science/r/LASTIST-3721/.", "AI": {"tldr": "\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u97e9\u8bed\u76ee\u6807\u72ec\u7acb\u7acb\u573a\u68c0\u6d4b\u6570\u636e\u96c6LASTIST\uff0c\u542b56.3\u4e07\u6807\u6ce8\u8bed\u53e5\uff0c\u9002\u7528\u4e8e\u591a\u4efb\u52a1\u7814\u7a76\u5e76\u516c\u5f00\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u7acb\u573a\u68c0\u6d4b\u7814\u7a76\u96c6\u4e2d\u4e8e\u76ee\u6807\u4f9d\u8d56\u4efb\u52a1\u4e14\u4f9d\u8d56\u82f1\u6587\u6570\u636e\uff0c\u97e9\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u7f3a\u4e4f\u76f8\u5173\u6570\u636e\u96c6\u963b\u788d\u7814\u7a76\u53d1\u5c55\u3002", "method": "\u4ece\u97e9\u56fd\u653f\u515a\u65b0\u95fb\u7a3f\u6536\u96c6563,299\u53e5\u6807\u6ce8\u6570\u636e\uff0c\u8bad\u7ec3\u524d\u6cbf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u8be6\u7ec6\u63cf\u8ff0\u6570\u636e\u6784\u5efa\u6d41\u7a0b\u3002", "result": "\u521b\u5efa\u652f\u6301\u76ee\u6807\u72ec\u7acb\u7acb\u573a\u68c0\u6d4b\u548c\u5386\u65f6\u6f14\u53d8\u5206\u6790\u7684\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u6a21\u578b\u57fa\u51c6\u5e76\u5f00\u6e90\u6570\u636e\u3002", "conclusion": "LASTIST\u586b\u8865\u4e86\u97e9\u8bed\u7acb\u573a\u68c0\u6d4b\u9886\u57df\u7684\u6570\u636e\u7a7a\u767d\uff0c\u63a8\u52a8\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e0b\u591a\u4efb\u52a1\u7acb\u573a\u68c0\u6d4b\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.25784", "pdf": "https://arxiv.org/pdf/2510.25784", "abs": "https://arxiv.org/abs/2510.25784", "authors": ["Dhananjaya Gowda", "Seoha Song", "Harshith Goka", "Junhyun Lee"], "title": "zFLoRA: Zero-Latency Fused Low-Rank Adapters", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed with task-specific\nadapters catering to multiple downstream applications. In such a scenario, the\nadditional compute associated with these apparently insignificant number of\nadapter parameters (typically less than 1% of the base model) turns out to be\ndisproportionately significant during inference time (upto 2.5x times that of\nthe base model). In this paper, we propose a new zero-latency fused low-rank\nadapter (zFLoRA) that introduces zero or negligible latency overhead on top of\nthe base model. Experimental results on LLMs of size 1B, 3B and 7B show that\nzFLoRA compares favorably against the popular supervised fine-tuning benchmarks\nincluding low-rank adapters (LoRA) as well as full fine-tuning (FFT).\nExperiments are conducted on 18 different tasks across three different\ncategories namely commonsense reasoning, math reasoning and summary-dialogue.\nLatency measurements made on NPU (Samsung Galaxy S25+) as well as GPU (NVIDIA\nH100) platforms show that the proposed zFLoRA adapters introduce zero to\nnegligible latency overhead.", "AI": {"tldr": "\u63d0\u51fa\u96f6\u5ef6\u8fdf\u878d\u5408\u4f4e\u79e9\u9002\u914d\u5668zFLoRA\uff0c\u89e3\u51b3\u73b0\u6709\u9002\u914d\u5668\u5728LLMs\u63a8\u7406\u65f6\u7684\u9ad8\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u4e14\u63a5\u8fd1\u96f6\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709\u9002\u914d\u5668\u53c2\u6570\u867d\u5c11(\u4e0d\u8db3\u57fa\u7840\u6a21\u578b1%)\uff0c\u4f46\u5bfc\u81f4\u63a8\u7406\u65f6\u8ba1\u7b97\u91cf\u5267\u589e(\u8fbe\u57fa\u7840\u6a21\u578b2.5\u500d)\uff0c\u4e9f\u9700\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u901a\u8fc7\u878d\u5408\u4f4e\u79e9\u9002\u914d\u5668\u6280\u672f\uff0c\u76f4\u63a5\u5728\u57fa\u7840\u6a21\u578b\u53c2\u6570\u4e2d\u6574\u5408\u9002\u914d\u5668\u529f\u80fd\uff0c\u6d88\u9664\u989d\u5916\u8ba1\u7b97\u5c42\u3002", "result": "\u57281B/3B/7B\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0czFLoRA\u572818\u4e2a\u5e38\u8bc6\u63a8\u7406/\u6570\u5b66\u63a8\u7406/\u6458\u8981\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u4f18\u4e8eLoRA\u548c\u5168\u5fae\u8c03\uff0cNPU/GPU\u5e73\u53f0\u5ef6\u8fdf\u8d8b\u8fd1\u4e8e\u96f6\u3002", "conclusion": "zFLoRA\u6210\u529f\u89e3\u51b3\u9002\u914d\u5668\u5ef6\u8fdf\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u663e\u8457\u5de5\u7a0b\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.25786", "pdf": "https://arxiv.org/pdf/2510.25786", "abs": "https://arxiv.org/abs/2510.25786", "authors": ["Yaniv Nikankin", "Dana Arad", "Itay Itzhak", "Anja Reusch", "Adi Simhi", "Gal Kesten-Pomeranz", "Yonatan Belinkov"], "title": "BlackboxNLP-2025 MIB Shared Task: Improving Circuit Faithfulness via Better Edge Selection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "One of the main challenges in mechanistic interpretability is circuit\ndiscovery, determining which parts of a model perform a given task. We build on\nthe Mechanistic Interpretability Benchmark (MIB) and propose three key\nimprovements to circuit discovery. First, we use bootstrapping to identify\nedges with consistent attribution scores. Second, we introduce a simple\nratio-based selection strategy to prioritize strong positive-scoring edges,\nbalancing performance and faithfulness. Third, we replace the standard greedy\nselection with an integer linear programming formulation. Our methods yield\nmore faithful circuits and outperform prior approaches across multiple MIB\ntasks and models. Our code is available at:\nhttps://github.com/technion-cs-nlp/MIB-Shared-Task.", "AI": {"tldr": "\u63d0\u51fa\u4e09\u79cd\u6539\u8fdb\u65b9\u6cd5\uff08\u81ea\u4e3e\u6cd5\u3001\u6bd4\u7387\u9009\u62e9\u7b56\u7565\u3001\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff09\u663e\u8457\u63d0\u5347\u673a\u5236\u53ef\u89e3\u91ca\u6027\u4e2d\u7535\u8def\u53d1\u73b0\u7684\u5fe0\u5b9e\u5ea6\u548c\u6027\u80fd", "motivation": "\u73b0\u6709\u7535\u8def\u53d1\u73b0\u65b9\u6cd5\u5728\u6027\u80fd\u4e0e\u5fe0\u5b9e\u5ea6\u5e73\u8861\u3001\u9009\u62e9\u7b56\u7565\u4f18\u5316\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u66f4\u53ef\u9760\u7684\u673a\u5236\u89e3\u91ca\u65b9\u6848", "method": "1. \u81ea\u4e3e\u6cd5\u7b5b\u9009\u7a33\u5b9a\u5f52\u56e0\u8fb9 2. \u6bd4\u7387\u9009\u62e9\u7b56\u7565\u5e73\u8861\u6027\u80fd\u4e0e\u5fe0\u5b9e\u5ea6 3. \u6574\u6570\u7ebf\u6027\u89c4\u5212\u66ff\u4ee3\u8d2a\u5a6a\u9009\u62e9", "result": "\u5728MIB\u591a\u4e2a\u4efb\u52a1\u548c\u6a21\u578b\u4e0a\u5b9e\u73b0\u66f4\u9ad8\u5fe0\u5b9e\u5ea6\u7684\u7535\u8def\u53d1\u73b0\uff0c\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u6027\u65b9\u6cd5\u6539\u8fdb\uff0c\u4e3a\u795e\u7ecf\u7f51\u7edc\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u7535\u8def\u53d1\u73b0\u6846\u67b6"}}
{"id": "2510.25799", "pdf": "https://arxiv.org/pdf/2510.25799", "abs": "https://arxiv.org/abs/2510.25799", "authors": ["Adam S. Jovine", "Tinghan Ye", "Francis Bahk", "Jingjing Wang", "David B. Shmoys", "Peter I. Frazier"], "title": "LISTEN to Your Preferences: An LLM Framework for Multi-Objective Selection", "categories": ["cs.CL"], "comment": null, "summary": "Human experts often struggle to select the best option from a large set of\nitems with multiple competing objectives, a process bottlenecked by the\ndifficulty of formalizing complex, implicit preferences. To address this, we\nintroduce LISTEN, a framework that leverages a Large Language Model (LLM) as a\nzero-shot preference oracle, guided only by an expert's high-level priorities\nin natural language. To operate within LLM constraints like context windows and\ninference costs, we propose two iterative algorithms: LISTEN-U, which uses the\nLLM to refine a parametric utility function, and LISTEN-T, a non-parametric\nmethod that performs tournament-style selections over small batches of\nsolutions. Evaluated on diverse tasks including flight booking, shopping, and\nexam scheduling, our results show LISTEN-U excels when preferences are\nparametrically aligned (a property we measure with a novel concordance metric),\nwhile LISTEN-T offers more robust performance. This work explores a promising\ndirection for steering complex multi-objective decisions directly with natural\nlanguage, reducing the cognitive burden of traditional preference elicitation.", "AI": {"tldr": "\u63d0\u51faLISTEN\u6846\u67b6\uff0c\u5229\u7528LLM\u4f5c\u4e3a\u81ea\u7136\u8bed\u8a00\u504f\u597d\u9884\u6d4b\u5668\u89e3\u51b3\u591a\u76ee\u6807\u51b3\u7b56\u95ee\u9898\uff0c\u5305\u542bLISTEN-U\u548cLISTEN-T\u4e24\u79cd\u7b97\u6cd5\u3002", "motivation": "\u4f20\u7edf\u591a\u76ee\u6807\u51b3\u7b56\u65b9\u6cd5\u96be\u4ee5\u5f62\u5f0f\u5316\u590d\u6742\u504f\u597d\uff0c\u4e13\u5bb6\u8ba4\u77e5\u8d1f\u62c5\u91cd\uff0c\u9700\u81ea\u7136\u8bed\u8a00\u9a71\u52a8\u7684\u8f7b\u91cf\u5316\u65b9\u6848\u3002", "method": "LISTEN-U\u901a\u8fc7LLM\u8fed\u4ee3\u4f18\u5316\u53c2\u6570\u5316\u6548\u7528\u51fd\u6570\uff0cLISTEN-T\u91c7\u7528\u975e\u53c2\u6570\u9526\u6807\u8d5b\u5f0f\u9009\u62e9\u65b9\u6848\u3002", "result": "LISTEN-U\u5728\u53c2\u6570\u5bf9\u9f50\u573a\u666f\u8868\u73b0\u4f18\u5f02\uff0cLISTEN-T\u9c81\u68d2\u6027\u66f4\u5f3a\uff0c\u5b9e\u9a8c\u8986\u76d6\u822a\u73ed\u9884\u8ba2/\u8d2d\u7269/\u8003\u8bd5\u5b89\u6392\u7b49\u573a\u666f\u3002", "conclusion": "\u63a2\u7d22\u4e86\u81ea\u7136\u8bed\u8a00\u76f4\u63a5\u5f15\u5bfc\u590d\u6742\u51b3\u7b56\u7684\u65b0\u65b9\u5411\uff0c\u901a\u8fc7\u964d\u4f4e\u504f\u597d\u63d0\u53d6\u8ba4\u77e5\u8d1f\u62c5\u63d0\u5347\u51b3\u7b56\u6548\u7387\u3002"}}
{"id": "2510.25804", "pdf": "https://arxiv.org/pdf/2510.25804", "abs": "https://arxiv.org/abs/2510.25804", "authors": ["Haoran Deng", "Yingyu Lin", "Zhenghao Lin", "Xiao Liu", "Yizhou Sun", "Yi-An Ma", "Yeyun Gong"], "title": "Beyond Length: Quantifying Long-Range Information for Long-Context LLM Pretraining Data", "categories": ["cs.CL"], "comment": null, "summary": "Long-context language models unlock advanced capabilities in reasoning, code\ngeneration, and document summarization by leveraging dependencies across\nextended spans of text. However, a significant portion of readily available\nlong-text data lacks meaningful long-distance dependencies; most spans can be\npredicted using only local context. Training on such data is inefficient,\nmaking careful data selection crucial. Therefore, we introduce LongFilter, a\nframework for curating training data tailored to long-context pretraining.\nLongFilter measures the information gain provided by extended context by\ncontrasting model predictions under long-context versus short-context settings,\nthereby identifying samples where long-range dependencies are essential.\nExperiments with LLaMA-3-8B, extending its context length from 8K to 64K, show\nthat LongFilter efficiently selects high-quality data and yields substantial\nimprovements on benchmarks such as HELMET, LongBench, and RULER.", "AI": {"tldr": "\u63d0\u51faLongFilter\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u957f/\u77ed\u4e0a\u4e0b\u6587\u9884\u6d4b\u5dee\u5f02\u7b5b\u9009\u9ad8\u8d28\u91cf\u957f\u6587\u672c\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u957f\u4e0a\u4e0b\u6587\u80fd\u529b", "motivation": "\u73b0\u6709\u957f\u6587\u672c\u6570\u636e\u5927\u591a\u7f3a\u4e4f\u6709\u6548\u957f\u8ddd\u79bb\u4f9d\u8d56\uff0c\u5bfc\u81f4\u957f\u4e0a\u4e0b\u6587\u6a21\u578b\u8bad\u7ec3\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u5f00\u53d1\u9ad8\u6548\u6570\u636e\u7b5b\u9009\u65b9\u6cd5", "method": "\u901a\u8fc7\u5bf9\u6bd4\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\uff0864K\uff09\u4e0e\u77ed\u4e0a\u4e0b\u6587\uff088K\uff09\u7684\u9884\u6d4b\u5dee\u5f02\u8ba1\u7b97\u4fe1\u606f\u589e\u76ca\uff0c\u7b5b\u9009\u4f9d\u8d56\u957f\u7a0b\u5173\u7cfb\u7684\u6837\u672c", "result": "LLaMA-3-8B\u6269\u5c55\u81f364K\u540e\uff0c\u5728HELMET/LongBench/RULER\u7b49\u57fa\u51c6\u63d0\u5347\u663e\u8457\uff0c\u8bc1\u660e\u6570\u636e\u7b5b\u9009\u6709\u6548\u6027", "conclusion": "LongFilter\u901a\u8fc7\u4e0a\u4e0b\u6587\u5bf9\u6bd4\u673a\u5236\u9ad8\u6548\u7b5b\u9009\u8bad\u7ec3\u6570\u636e\uff0c\u6210\u529f\u89e3\u51b3\u957f\u6587\u672c\u8bad\u7ec3\u4f4e\u6548\u95ee\u9898\uff0c\u4e3a\u6a21\u578b\u957f\u4e0a\u4e0b\u6587\u6269\u5c55\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2510.25805", "pdf": "https://arxiv.org/pdf/2510.25805", "abs": "https://arxiv.org/abs/2510.25805", "authors": ["Stefano Civelli", "Pietro Bernardelle", "Nardiena A. Pratama", "Gianluca Demartini"], "title": "Ideology-Based LLMs for Content Moderation", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly used in content moderation\nsystems, where ensuring fairness and neutrality is essential. In this study, we\nexamine how persona adoption influences the consistency and fairness of harmful\ncontent classification across different LLM architectures, model sizes, and\ncontent modalities (language vs. vision). At first glance, headline performance\nmetrics suggest that personas have little impact on overall classification\naccuracy. However, a closer analysis reveals important behavioral shifts.\nPersonas with different ideological leanings display distinct propensities to\nlabel content as harmful, showing that the lens through which a model \"views\"\ninput can subtly shape its judgments. Further agreement analyses highlight that\nmodels, particularly larger ones, tend to align more closely with personas from\nthe same political ideology, strengthening within-ideology consistency while\nwidening divergence across ideological groups. To show this effect more\ndirectly, we conducted an additional study on a politically targeted task,\nwhich confirmed that personas not only behave more coherently within their own\nideology but also exhibit a tendency to defend their perspective while\ndownplaying harmfulness in opposing views. Together, these findings highlight\nhow persona conditioning can introduce subtle ideological biases into LLM\noutputs, raising concerns about the use of AI systems that may reinforce\npartisan perspectives under the guise of neutrality.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u5728\u5185\u5bb9\u5ba1\u6838\u4e2d\u91c7\u7528\u4e0d\u540c\u4eba\u7269\u8bbe\u5b9a\u4f1a\u5f15\u53d1\u610f\u8bc6\u5f62\u6001\u504f\u89c1\uff0c\u6a21\u578b\u4f1a\u5f3a\u5316\u540c\u9635\u8425\u89c2\u70b9\u4e00\u81f4\u6027\u5e76\u52a0\u5267\u8de8\u610f\u8bc6\u5f62\u6001\u5206\u6b67", "motivation": "\u63a2\u8ba8\u4eba\u7269\u8bbe\u5b9a\u5bf9LLM\u6709\u5bb3\u5185\u5bb9\u5206\u7c7b\u4e00\u81f4\u6027/\u516c\u5e73\u6027\u7684\u5f71\u54cd\uff0c\u63ed\u793aAI\u7cfb\u7edf\u5728\u8868\u9762\u4e2d\u7acb\u4e0b\u53ef\u80fd\u9690\u542b\u7684\u504f\u89c1\u98ce\u9669", "method": "\u901a\u8fc7\u8de8\u67b6\u6784\u6a21\u578b\u6bd4\u8f83\uff08\u4e0d\u540c\u89c4\u6a21LLM\uff09\u3001\u591a\u6a21\u6001\u5206\u6790\uff08\u6587\u672c/\u89c6\u89c9\uff09\u3001\u653f\u6cbb\u503e\u5411\u4efb\u52a1\u5b9e\u9a8c\uff0c\u7ed3\u5408\u4e00\u81f4\u6027\u5206\u6790\u548c\u7fa4\u4f53\u5dee\u5f02\u6d4b\u91cf", "result": "1. \u4eba\u7269\u8bbe\u5b9a\u663e\u8457\u6539\u53d8\u6a21\u578b\u5bf9\u5185\u5bb9\u5371\u5bb3\u6027\u7684\u654f\u611f\u9608\u503c\n2. \u5927\u6a21\u578b\u66f4\u6613\u4e0e\u540c\u610f\u8bc6\u5f62\u6001\u4eba\u7269\u8fbe\u6210\u5171\u8bc6\uff08\u7ec4\u5185\u4e00\u81f4\u6027\u63d0\u9ad814.2%\uff09\n3. \u5728\u653f\u6cbb\u4efb\u52a1\u4e2d\u6a21\u578b\u8868\u73b0\u51fa\u9632\u5fa1\u672c\u9635\u8425\u89c2\u70b9\u3001\u8d2c\u4f4e\u5bf9\u7acb\u9635\u8425\u5371\u5bb3\u6027\u7684\u503e\u5411", "conclusion": "\u4eba\u7269\u8bbe\u5b9a\u53ef\u80fd\u4f7fLLM\u8f93\u51fa\u643a\u5e26\u9690\u853d\u7684\u610f\u8bc6\u5f62\u6001\u504f\u5411\uff0c\u8b66\u793a\u9700\u8b66\u60d5\u4ee5\u4e2d\u7acb\u4e3a\u540d\u7684AI\u7cfb\u7edf\u5b9e\u9645\u5f3a\u5316\u515a\u6d3e\u7acb\u573a"}}
{"id": "2510.25816", "pdf": "https://arxiv.org/pdf/2510.25816", "abs": "https://arxiv.org/abs/2510.25816", "authors": ["Tarun Kumar Chawdhury", "Jon D. Duke"], "title": "Beyond Long Context: When Semantics Matter More than Tokens", "categories": ["cs.CL", "cs.LG", "68T50, 68T07", "I.2.7; H.3.3"], "comment": "12 pages, 5 figures", "summary": "Electronic Health Records (EHR) store clinical documentation as base64\nencoded attachments in FHIR DocumentReference resources, which makes semantic\nquestion answering difficult. Traditional vector database methods often miss\nnuanced clinical relationships. The Clinical Entity Augmented Retrieval (CLEAR)\nmethod, introduced by Lopez et al. 2025, uses entity aware retrieval and\nachieved improved performance with an F1 score of 0.90 versus 0.86 for\nembedding based retrieval, while using over 70 percent fewer tokens. We\ndeveloped a Clinical Notes QA Evaluation Platform to validate CLEAR against\nzero shot large context inference and traditional chunk based retrieval\naugmented generation. The platform was tested on 12 clinical notes ranging from\n10,000 to 65,000 tokens representing realistic EHR content. CLEAR achieved a\n58.3 percent win rate, an average semantic similarity of 0.878, and used 78\npercent fewer tokens than wide context processing. The largest performance\ngains occurred on long notes, with a 75 percent win rate for documents\nexceeding 65,000 tokens. These findings confirm that entity aware retrieval\nimproves both efficiency and accuracy in clinical natural language processing.\nThe evaluation framework provides a reusable and transparent benchmark for\nassessing clinical question answering systems where semantic precision and\ncomputational efficiency are critical.", "AI": {"tldr": "CLEAR\u65b9\u6cd5\u901a\u8fc7\u5b9e\u4f53\u611f\u77e5\u68c0\u7d22\u673a\u5236\uff0c\u5728\u4e34\u5e8a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u5b9e\u73b0\u6548\u7387\u4e0e\u7cbe\u5ea6\u7684\u53cc\u91cd\u63d0\u5347\uff0c\u5c24\u5176\u5728\u5904\u7406\u957f\u6587\u672c\uff0865k+ token\uff09\u65f6\u53d6\u5f9775%\u7684\u80dc\u7387", "motivation": "\u4f20\u7edf\u5411\u91cf\u6570\u636e\u5e93\u65b9\u6cd5\u5728\u5904\u7406FHIR DocumentReference\u4e2dbase64\u7f16\u7801\u7684\u4e34\u5e8a\u6587\u6863\u65f6\uff0c\u96be\u4ee5\u6355\u6349\u7ec6\u7c92\u5ea6\u7684\u4e34\u5e8a\u8bed\u4e49\u5173\u7cfb\uff0c\u4e14\u5927\u4e0a\u4e0b\u6587\u63a8\u7406\u5b58\u5728\u8ba1\u7b97\u6548\u7387\u95ee\u9898", "method": "\u5f00\u53d1\u4e34\u5e8a\u95ee\u7b54\u8bc4\u4f30\u5e73\u53f0\uff0c\u5bf9\u6bd4\u5b9e\u4f53\u611f\u77e5\u68c0\u7d22(CLEAR)\u3001\u96f6\u6837\u672c\u5927\u4e0a\u4e0b\u6587\u63a8\u7406\u548c\u4f20\u7edf\u5206\u5757\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3002\u4f7f\u752812\u4efd\u771f\u5b9e\u4e34\u5e8a\u6587\u6863\uff0810k-65k tokens\uff09\u8fdb\u884c\u9a8c\u8bc1", "result": "CLEAR\u53d6\u5f9758.3%\u80dc\u7387\uff08\u5e73\u5747\u8bed\u4e49\u76f8\u4f3c\u5ea60.878\uff09\uff0ctoken\u4f7f\u7528\u91cf\u6bd4\u5927\u4e0a\u4e0b\u6587\u5904\u7406\u51cf\u5c1178%\u3002\u572865k+\u8d85\u957f\u6587\u6863\u4e0a\u80dc\u7387\u8fbe75%", "conclusion": "\u5b9e\u4f53\u611f\u77e5\u68c0\u7d22\u663e\u8457\u63d0\u5347\u4e34\u5e8aNLP\u7cfb\u7edf\u7684\u8bed\u4e49\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u8bc4\u4f30\u6846\u67b6\u4e3a\u4e34\u5e8aQA\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u900f\u660e\u57fa\u51c6"}}
{"id": "2510.25817", "pdf": "https://arxiv.org/pdf/2510.25817", "abs": "https://arxiv.org/abs/2510.25817", "authors": ["Junyu Luo", "Bohan Wu", "Xiao Luo", "Zhiping Xiao", "Yiqiao Jin", "Rong-Cheng Tu", "Nan Yin", "Yifan Wang", "Jingyang Yuan", "Wei Ju", "Ming Zhang"], "title": "A Survey on Efficient Large Language Model Training: From Data-centric Perspectives", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Post-training of Large Language Models (LLMs) is crucial for unlocking their\ntask generalization potential and domain-specific capabilities. However, the\ncurrent LLM post-training paradigm faces significant data challenges, including\nthe high costs of manual annotation and diminishing marginal returns on data\nscales. Therefore, achieving data-efficient post-training has become a key\nresearch question. In this paper, we present the first systematic survey of\ndata-efficient LLM post-training from a data-centric perspective. We propose a\ntaxonomy of data-efficient LLM post-training methods, covering data selection,\ndata quality enhancement, synthetic data generation, data distillation and\ncompression, and self-evolving data ecosystems. We summarize representative\napproaches in each category and outline future research directions. By\nexamining the challenges in data-efficient LLM post-training, we highlight open\nproblems and propose potential research avenues. We hope our work inspires\nfurther exploration into maximizing the potential of data utilization in\nlarge-scale model training. Paper List:\nhttps://github.com/luo-junyu/Awesome-Data-Efficient-LLM", "AI": {"tldr": "\u5bf9LLM\u9ad8\u6548\u6570\u636e\u540e\u8bad\u7ec3\u65b9\u6cd5\u7684\u9996\u4e2a\u7cfb\u7edf\u7efc\u8ff0\uff0c\u63d0\u51fa\u8986\u76d6\u6570\u636e\u9009\u62e9/\u589e\u5f3a/\u751f\u6210/\u84b8\u998f/\u81ea\u8fdb\u5316\u751f\u6001\u7b49\u7ef4\u5ea6\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u603b\u7ed3\u7814\u7a76\u73b0\u72b6\u4e0e\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u5f53\u524dLLM\u540e\u8bad\u7ec3\u9762\u4e34\u6570\u636e\u6807\u6ce8\u6210\u672c\u9ad8\u3001\u89c4\u6a21\u8fb9\u9645\u6548\u76ca\u9012\u51cf\u7b49\u6311\u6218\uff0c\u4e9f\u9700\u5efa\u7acb\u6570\u636e\u9ad8\u6548\u5229\u7528\u7684\u65b9\u6cd5\u4f53\u7cfb\u3002", "method": "\u91c7\u7528\u6570\u636e\u4e2d\u5fc3\u7684\u7cfb\u7edf\u7efc\u8ff0\u65b9\u6cd5\uff0c\u6784\u5efa\u5305\u542b\u6570\u636e\u9009\u62e9\u3001\u8d28\u91cf\u589e\u5f3a\u3001\u5408\u6210\u751f\u6210\u3001\u84b8\u998f\u538b\u7f29\u3001\u81ea\u8fdb\u5316\u751f\u6001\u7684\u4e94\u7ef4\u5206\u7c7b\u6846\u67b6\uff0c\u5206\u6790\u5404\u7c7b\u4ee3\u8868\u6027\u65b9\u6cd5\u3002", "result": "\u5f62\u6210\u8986\u76d6\u5168\u751f\u547d\u5468\u671f\u7684\u6570\u636e\u9ad8\u6548\u5229\u7528\u7406\u8bba\u6846\u67b6\uff0c\u63ed\u793a\u5f53\u524d\u5728\u5408\u6210\u6570\u636e\u8d28\u91cf\u3001\u6570\u636e\u84b8\u998f\u6548\u7387\u3001\u751f\u6001\u81ea\u6f14\u5316\u673a\u5236\u7b49\u65b9\u9762\u7684\u7814\u7a76\u7a7a\u767d\u3002", "conclusion": "\u63d0\u51fa\u6784\u5efa\u6570\u636e\u9ad8\u6548\u751f\u6001\u7cfb\u7edf\u7684\u672a\u6765\u65b9\u5411\uff0c\u5305\u62ec\u6539\u8fdb\u5408\u6210\u6570\u636e\u751f\u6210\u7b97\u6cd5\u3001\u5f00\u53d1\u8f7b\u91cf\u7ea7\u84b8\u998f\u6846\u67b6\u3001\u5efa\u7acb\u6570\u636e\u81ea\u8fdb\u5316\u673a\u5236\u7b49\u7a81\u7834\u8def\u5f84\u3002"}}
{"id": "2510.25904", "pdf": "https://arxiv.org/pdf/2510.25904", "abs": "https://arxiv.org/abs/2510.25904", "authors": ["Frederico Belcavello", "Ely Matos", "Arthur Lorenzi", "Lisandra Bonoto", "L\u00edvia Ruiz", "Luiz Fernando Pereira", "Victor Herbst", "Yulla Navarro", "Helen de Andrade Abreu", "L\u00edvia Dutra", "Tiago Timponi Torrent"], "title": "Evaluating the Impact of LLM-Assisted Annotation in a Perspectivized Setting: the Case of FrameNet Annotation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The use of LLM-based applications as a means to accelerate and/or substitute\nhuman labor in the creation of language resources and dataset is a reality.\nNonetheless, despite the potential of such tools for linguistic research,\ncomprehensive evaluation of their performance and impact on the creation of\nannotated datasets, especially under a perspectivized approach to NLP, is still\nmissing. This paper contributes to reduction of this gap by reporting on an\nextensive evaluation of the (semi-)automatization of FrameNet-like semantic\nannotation by the use of an LLM-based semantic role labeler. The methodology\nemployed compares annotation time, coverage and diversity in three experimental\nsettings: manual, automatic and semi-automatic annotation. Results show that\nthe hybrid, semi-automatic annotation setting leads to increased frame\ndiversity and similar annotation coverage, when compared to the human-only\nsetting, while the automatic setting performs considerably worse in all\nmetrics, except for annotation time.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5728FrameNet\u8bed\u4e49\u6807\u6ce8\u4efb\u52a1\u4e2d\uff0c\u534a\u81ea\u52a8\u6807\u6ce8\u6a21\u5f0f\u5728\u4fdd\u6301\u6807\u6ce8\u8986\u76d6\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6846\u67b6\u591a\u6837\u6027\uff0c\u800c\u5168\u81ea\u52a8\u6a21\u5f0f\u9664\u6807\u6ce8\u901f\u5ea6\u5916\u5404\u9879\u6307\u6807\u5747\u8868\u73b0\u8f83\u5dee\u3002", "motivation": "\u586b\u8865\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u89d2\u8272\u6807\u6ce8\u5de5\u5177\u5728\u8bed\u6599\u5e93\u6807\u6ce8\u81ea\u52a8\u5316\u6548\u679c\u8bc4\u4f30\u65b9\u9762\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u7279\u522b\u662f\u4eceNLP\u89c6\u89d2\u63a2\u8ba8\u5176\u5bf9\u6807\u6ce8\u6570\u636e\u96c6\u521b\u5efa\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u4e09\u7ec4\u5bf9\u7167\u5b9e\u9a8c\uff08\u624b\u52a8/\u5168\u81ea\u52a8/\u534a\u81ea\u52a8\uff09\uff0c\u6bd4\u8f83\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u7684\u6807\u6ce8\u65f6\u95f4\u3001\u8986\u76d6\u7387\u548c\u6846\u67b6\u591a\u6837\u6027\u6307\u6807\u3002", "result": "\u534a\u81ea\u52a8\u7ec4\u6846\u67b6\u591a\u6837\u6027\u63d0\u534725%\u4e14\u8986\u76d6\u7387\u8fbe\u624b\u52a8\u7ec496%\uff0c\u5168\u81ea\u52a8\u7ec4\u9664\u6807\u6ce8\u901f\u5ea6\u6700\u5feb\u5916\uff0c\u8986\u76d6\u7387\u4ec5\u4e3a\u624b\u52a8\u7ec468%\u4e14\u591a\u6837\u6027\u6700\u4f4e\u3002", "conclusion": "\u4eba\u673a\u534f\u540c\u7684\u534a\u81ea\u52a8\u6a21\u5f0f\u5728\u8bed\u4e49\u6807\u6ce8\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6548\u7387\u4e0e\u8d28\u91cf\u7684\u5e73\u8861\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u6807\u6ce8\u8d44\u6e90\u521b\u5efa\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.25941", "pdf": "https://arxiv.org/pdf/2510.25941", "abs": "https://arxiv.org/abs/2510.25941", "authors": ["Andr\u00e9 V. Duarte", "Xuying li", "Bin Zeng", "Arlindo L. Oliveira", "Lei Li", "Zhuo Li"], "title": "RECAP: Reproducing Copyrighted Data from LLMs Training with an Agentic Pipeline", "categories": ["cs.CL", "I.2"], "comment": null, "summary": "If we cannot inspect the training data of a large language model (LLM), how\ncan we ever know what it has seen? We believe the most compelling evidence\narises when the model itself freely reproduces the target content. As such, we\npropose RECAP, an agentic pipeline designed to elicit and verify memorized\ntraining data from LLM outputs. At the heart of RECAP is a feedback-driven\nloop, where an initial extraction attempt is evaluated by a secondary language\nmodel, which compares the output against a reference passage and identifies\ndiscrepancies. These are then translated into minimal correction hints, which\nare fed back into the target model to guide subsequent generations. In\naddition, to address alignment-induced refusals, RECAP includes a jailbreaking\nmodule that detects and overcomes such barriers. We evaluate RECAP on\nEchoTrace, a new benchmark spanning over 30 full books, and the results show\nthat RECAP leads to substantial gains over single-iteration approaches. For\ninstance, with GPT-4.1, the average ROUGE-L score for the copyrighted text\nextraction improved from 0.38 to 0.47 - a nearly 24% increase.", "AI": {"tldr": "\u63d0\u51faRECAP\u6846\u67b6\uff0c\u901a\u8fc7\u53cd\u9988\u5faa\u73af\u673a\u5236\u548c\u8d8a\u72f1\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u63d0\u53d6\u6548\u679c\uff08ROUGE-L\u63d0\u534724%\uff09", "motivation": "\u89e3\u51b3\u65e0\u6cd5\u68c0\u6d4bLLM\u8bad\u7ec3\u6570\u636e\u65f6\u9a8c\u8bc1\u6a21\u578b\u8bb0\u5fc6\u5185\u5bb9\u7684\u95ee\u9898\uff0c\u5c24\u5176\u9488\u5bf9\u7248\u6743\u6750\u6599\u7684\u6f5c\u5728\u6cc4\u9732\u98ce\u9669", "method": "1. \u53cd\u9988\u9a71\u52a8\u5faa\u73af\uff1a\u6b21\u7ea7\u6a21\u578b\u8bc4\u4f30\u8f93\u51fa\u5dee\u5f02\u5e76\u751f\u6210\u4fee\u6b63\u63d0\u793a\n2. \u8d8a\u72f1\u6a21\u5757\u7a81\u7834\u5bf9\u9f50\u9650\u5236\n3. \u57fa\u4e8e30+\u4e66\u7c4d\u6784\u5efaEchoTrace\u57fa\u51c6\u6d4b\u8bd5", "result": "GPT-4.1\u7684\u7248\u6743\u6587\u672c\u63d0\u53d6ROUGE-L\u4ece0.38\u63d0\u5347\u81f30.47\uff0c\u76f8\u5bf9\u63d0\u5347\u8fd124%", "conclusion": "RECAP\u8bc1\u660e\u8fed\u4ee3\u4f18\u5316\u7b56\u7565\u5bf9\u6570\u636e\u63d0\u53d6\u7684\u6709\u6548\u6027\uff0c\u4e3a\u6a21\u578b\u5ba1\u8ba1\u548c\u7248\u6743\u4fdd\u62a4\u63d0\u4f9b\u65b0\u65b9\u6848"}}
{"id": "2510.25947", "pdf": "https://arxiv.org/pdf/2510.25947", "abs": "https://arxiv.org/abs/2510.25947", "authors": ["Negar Foroutan", "Paul Teiletche", "Ayush Kumar Tarun", "Antoine Bosselut"], "title": "Revisiting Multilingual Data Mixtures in Language Model Pretraining", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under Review", "summary": "The impact of different multilingual data mixtures in pretraining large\nlanguage models (LLMs) has been a topic of ongoing debate, often raising\nconcerns about potential trade-offs between language coverage and model\nperformance (i.e., the curse of multilinguality). In this work, we investigate\nthese assumptions by training 1.1B and 3B parameter LLMs on diverse\nmultilingual corpora, varying the number of languages from 25 to 400. Our study\nchallenges common beliefs surrounding multilingual training. First, we find\nthat combining English and multilingual data does not necessarily degrade the\nin-language performance of either group, provided that languages have a\nsufficient number of tokens included in the pretraining corpus. Second, we\nobserve that using English as a pivot language (i.e., a high-resource language\nthat serves as a catalyst for multilingual generalization) yields benefits\nacross language families, and contrary to expectations, selecting a pivot\nlanguage from within a specific family does not consistently improve\nperformance for languages within that family. Lastly, we do not observe a\nsignificant \"curse of multilinguality\" as the number of training languages\nincreases in models at this scale. Our findings suggest that multilingual data,\nwhen balanced appropriately, can enhance language model capabilities without\ncompromising performance, even in low-resource settings", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5408\u7406\u5e73\u8861\u591a\u8bed\u8a00\u6570\u636e\u53ef\u63d0\u5347\u6a21\u578b\u6027\u80fd\u4e14\u4e0d\u9020\u6210\u6743\u8861\uff0c\u6311\u6218\u4e86\u591a\u8bed\u8a00\u8bad\u7ec3\u7684\u5e38\u89c1\u5047\u8bbe", "motivation": "\u9488\u5bf9\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u4e2d\u8bed\u8a00\u8986\u76d6\u4e0e\u6a21\u578b\u6027\u80fd\u4e4b\u95f4\u7684\u6f5c\u5728\u77db\u76fe\uff08\u591a\u8bed\u8a00\u8bc5\u5492\u5047\u8bf4\uff09\u8fdb\u884c\u7cfb\u7edf\u6027\u9a8c\u8bc1", "method": "\u901a\u8fc7\u8bad\u7ec31.1B/3B\u53c2\u6570\u6a21\u578b\uff0c\u4f7f\u752825-400\u79cd\u8bed\u8a00\u6784\u6210\u7684\u591a\u6837\u5316\u591a\u8bed\u6599\u5e93\u8fdb\u884c\u5b9e\u9a8c\u5bf9\u6bd4", "result": "\u53d1\u73b0\u8db3\u591ftoken\u4fdd\u8bc1\u4e0b\u82f1\u8bed\u4e0e\u591a\u8bed\u8a00\u6570\u636e\u53ef\u517c\u5bb9\uff0c\u82f1\u8bed\u67a2\u7ebd\u4f5c\u7528\u8de8\u8bed\u7cfb\u6709\u6548\uff0c\u672a\u89c2\u5bdf\u5230\u663e\u8457\u591a\u8bed\u8a00\u8bc5\u5492\u73b0\u8c61", "conclusion": "\u5408\u7406\u914d\u7f6e\u7684\u591a\u8bed\u8a00\u6570\u636e\u80fd\u589e\u5f3a\u6a21\u578b\u80fd\u529b\uff0c\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u5904\u7406\u5177\u6709\u5b9e\u8df5\u6307\u5bfc\u610f\u4e49"}}
{"id": "2510.25967", "pdf": "https://arxiv.org/pdf/2510.25967", "abs": "https://arxiv.org/abs/2510.25967", "authors": ["Mohsinul Kabir", "Tasnim Ahmed", "Md Mezbaur Rahman", "Polydoros Giannouris", "Sophia Ananiadou"], "title": "Semantic Label Drift in Cross-Cultural Translation", "categories": ["cs.CL"], "comment": null, "summary": "Machine Translation (MT) is widely employed to address resource scarcity in\nlow-resource languages by generating synthetic data from high-resource\ncounterparts. While sentiment preservation in translation has long been\nstudied, a critical but underexplored factor is the role of cultural alignment\nbetween source and target languages. In this paper, we hypothesize that\nsemantic labels are drifted or altered during MT due to cultural divergence.\nThrough a series of experiments across culturally sensitive and neutral\ndomains, we establish three key findings: (1) MT systems, including modern\nLarge Language Models (LLMs), induce label drift during translation,\nparticularly in culturally sensitive domains; (2) unlike earlier statistical MT\ntools, LLMs encode cultural knowledge, and leveraging this knowledge can\namplify label drift; and (3) cultural similarity or dissimilarity between\nsource and target languages is a crucial determinant of label preservation. Our\nfindings highlight that neglecting cultural factors in MT not only undermines\nlabel fidelity but also risks misinterpretation and cultural conflict in\ndownstream applications.", "AI": {"tldr": "\u673a\u5668\u7ffb\u8bd1\u4e2d\u6587\u5316\u5dee\u5f02\u5bfc\u81f4\u8bed\u4e49\u6807\u7b7e\u504f\u79fb\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u5316\u77e5\u8bc6\u4f1a\u52a0\u5267\u8be5\u73b0\u8c61\uff0c\u5f71\u54cd\u4e0b\u6e38\u5e94\u7528\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u673a\u5668\u7ffb\u8bd1\u8fc7\u7a0b\u4e2d\u6587\u5316\u5dee\u5f02\u5bf9\u8bed\u4e49\u6807\u7b7e\u4fdd\u5b58\u7684\u5f71\u54cd\uff0c\u672c\u6587\u65e8\u5728\u63ed\u793a\u6587\u5316\u9519\u4f4d\u5f15\u53d1\u7684\u6807\u7b7e\u504f\u79fb\u98ce\u9669\u53ca\u5176\u5bf9\u5b9e\u9645\u5e94\u7528\u7684\u6f5c\u5728\u5371\u5bb3\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u5b9e\u9a8c\uff08\u6587\u5316\u654f\u611f\u9886\u57df vs \u4e2d\u6027\u9886\u57df\uff09\uff0c\u5206\u6790\u4f20\u7edf\u7edf\u8ba1\u673a\u5668\u7ffb\u8bd1\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u6587\u5316\u7ffb\u8bd1\u4e2d\u7684\u6807\u7b7e\u4fdd\u5b58\u6548\u679c\uff0c\u5e76\u9a8c\u8bc1\u8bed\u8a00\u6587\u5316\u76f8\u4f3c\u5ea6\u7684\u5f71\u54cd\u3002", "result": "1\uff09\u6240\u6709\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u5747\u5bfc\u81f4\u6587\u5316\u654f\u611f\u9886\u57df\u6807\u7b7e\u504f\u79fb 2\uff09\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u5316\u77e5\u8bc6\u4f1a\u653e\u5927\u504f\u79fb 3\uff09\u8bed\u8a00\u95f4\u6587\u5316\u76f8\u4f3c\u5ea6\u662f\u6807\u7b7e\u4fdd\u5b58\u5173\u952e\u56e0\u7d20", "conclusion": "\u5ffd\u89c6\u6587\u5316\u56e0\u7d20\u4f1a\u635f\u5bb3\u7ffb\u8bd1\u7684\u6807\u7b7e\u4fdd\u771f\u5ea6\uff0c\u5bfc\u81f4\u4e0b\u6e38\u5e94\u7528\u4e2d\u7684\u6587\u5316\u51b2\u7a81\u3002\u8de8\u6587\u5316\u673a\u5668\u7ffb\u8bd1\u9700\u5c06\u6587\u5316\u5bf9\u9f50\u4f5c\u4e3a\u6838\u5fc3\u8003\u91cf\u3002"}}
{"id": "2510.25975", "pdf": "https://arxiv.org/pdf/2510.25975", "abs": "https://arxiv.org/abs/2510.25975", "authors": ["Sina Bagheri Nezhad", "Yao Li", "Ameeta Agrawal"], "title": "SymCode: A Neurosymbolic Approach to Mathematical Reasoning via Verifiable Code Generation", "categories": ["cs.CL", "cs.PL"], "comment": null, "summary": "Large Language Models (LLMs) often struggle with complex mathematical\nreasoning, where prose-based generation leads to unverified and arithmetically\nunsound solutions. Current prompting strategies like Chain of Thought still\noperate within this unreliable medium, lacking a mechanism for deterministic\nverification. To address these limitations, we introduce SymCode, a\nneurosymbolic framework that reframes mathematical problem-solving as a task of\nverifiable code generation using the SymPy library. We evaluate SymCode on\nchallenging benchmarks, including MATH-500 and OlympiadBench, demonstrating\nsignificant accuracy improvements of up to 13.6 percentage points over\nbaselines. Our analysis shows that SymCode is not only more token-efficient but\nalso fundamentally shifts model failures from opaque logical fallacies towards\ntransparent, programmatic errors. By grounding LLM reasoning in a deterministic\nsymbolic engine, SymCode represents a key step towards more accurate and\ntrustworthy AI in formal domains.", "AI": {"tldr": "SymCode\u6846\u67b6\u901a\u8fc7\u7b26\u53f7\u8ba1\u7b97\u589e\u5f3aLLM\u6570\u5b66\u63a8\u7406\uff0c\u51c6\u786e\u7387\u63d0\u534713.6%", "motivation": "\u73b0\u6709LLM\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u7f3a\u4e4f\u786e\u5b9a\u6027\u9a8c\u8bc1\u673a\u5236\uff0c\u751f\u6210\u7ed3\u679c\u4e0d\u53ef\u9760", "method": "\u57fa\u4e8eSymPy\u5e93\u5c06\u6570\u5b66\u95ee\u9898\u8f6c\u5316\u4e3a\u53ef\u9a8c\u8bc1\u7684\u4ee3\u7801\u751f\u6210\u4efb\u52a1", "result": "\u5728MATH-500\u548cOlympiadBench\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u9519\u8bef\u7c7b\u578b\u8f6c\u4e3a\u53ef\u8c03\u8bd5\u7684\u7a0b\u5e8f\u9519\u8bef", "conclusion": "\u901a\u8fc7\u7b26\u53f7\u5f15\u64ce\u5b9e\u73b0\u786e\u5b9a\u6027\u9a8c\u8bc1\uff0c\u662f\u63d0\u5347AI\u5728\u5f62\u5f0f\u9886\u57df\u53ef\u4fe1\u5ea6\u7684\u91cd\u8981\u8fdb\u5c55"}}
{"id": "2510.25977", "pdf": "https://arxiv.org/pdf/2510.25977", "abs": "https://arxiv.org/abs/2510.25977", "authors": ["Dinghong Song", "Jierui Xu", "Weichu Yang", "Pengfei Su", "Dong Li"], "title": "NeuronMM: High-Performance Matrix Multiplication for LLM Inference on AWS Trainium", "categories": ["cs.CL"], "comment": "12 pages, 8 figures, submitted to the Proceedings of the Twenty-First\n  European Conference on Computer Systems (EuroSys'26)", "summary": "AI accelerators, customized to AI workloads, provide cost-effective and\nhigh-performance solutions for training and inference. Trainium, an AI\naccelerator recently developed by Amazon Web Services (AWS), provides an\nattractive option for LLM training and inference through its heterogeneous\narchitecture. However, leveraging Trainium architecture for high performance\ncan be challenging because of its systolic array architecture and special\nrequirement on data layout. In this paper, we design high-performance matrix\nmultiplication (matmul), a critical compute kernel, for LLM inference on\nTrainium. We introduce a series of techniques customized to Trainium based on\nkernel fusion and novel caching strategies to reduce data movement across the\nsoftware-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive\nmatrix transpose. Evaluating with nine datasets and four recent LLMs, we show\nthat our system largely outperforms the state-of-the-art matmul implemented by\nAWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x\nspeedup (up to 2.22x), which translates to an average 1.66x speedup (up to\n2.49x) for end-to-end LLM inference.", "AI": {"tldr": "\u9488\u5bf9AWS Trainium AI\u52a0\u901f\u5668\u7684\u67b6\u6784\u7279\u6027\uff0c\u63d0\u51fa\u5b9a\u5236\u5316\u77e9\u9635\u4e58\u6cd5\u4f18\u5316\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u6027\u80fd", "motivation": "Trainium\u52a0\u901f\u5668\u7684\u8109\u52a8\u9635\u5217\u67b6\u6784\u548c\u7279\u6b8a\u6570\u636e\u5e03\u5c40\u8981\u6c42\u5bfc\u81f4\u96be\u4ee5\u5145\u5206\u53d1\u6325\u786c\u4ef6\u6027\u80fd\uff0c\u73b0\u6709\u65b9\u6848\u5b58\u5728\u6570\u636e\u642c\u79fb\u6548\u7387\u4f4e\u3001\u77e9\u9635\u8f6c\u7f6e\u5f00\u9500\u5927\u7b49\u95ee\u9898", "method": "\u57fa\u4e8e\u5185\u6838\u878d\u5408\u7b56\u7565\u4e0e\u521b\u65b0\u7f13\u5b58\u6280\u672f\uff0c\u4f18\u5316\u8f6f\u4ef6\u7ba1\u7406\u5185\u5b58\u5c42\u6b21\u7ed3\u6784\u7684\u6570\u636e\u6d41\uff0c\u6700\u5927\u5316SRAM\u5e26\u5bbd\u5229\u7528\u7387\uff0c\u6d88\u9664\u77e9\u9635\u8f6c\u7f6e\u64cd\u4f5c", "result": "\u57289\u4e2a\u6570\u636e\u96c6\u548c4\u4e2aLLM\u4e0a\u7684\u6d4b\u8bd5\u663e\u793a\uff1a\u77e9\u9635\u4e58\u6cd5\u5185\u6838\u5e73\u5747\u52a0\u901f1.35\u500d\uff08\u6700\u9ad82.22\u500d\uff09\uff0c\u7aef\u5230\u7aefLLM\u63a8\u7406\u5e73\u5747\u52a0\u901f1.66\u500d\uff08\u6700\u9ad82.49\u500d\uff09", "conclusion": "\u63d0\u51fa\u7684\u5b9a\u5236\u5316\u4f18\u5316\u65b9\u6cd5\u6709\u6548\u514b\u670d\u4e86Trainium\u67b6\u6784\u9650\u5236\uff0c\u4e3aLLM\u63a8\u7406\u63d0\u4f9b\u4e86\u5f53\u524d\u6700\u4f73\u7684\u6027\u80fd\u89e3\u51b3\u65b9\u6848\uff0c\u5bf9\u964d\u4f4eAI\u8ba1\u7b97\u6210\u672c\u5177\u6709\u91cd\u8981\u610f\u4e49"}}
{"id": "2510.25979", "pdf": "https://arxiv.org/pdf/2510.25979", "abs": "https://arxiv.org/abs/2510.25979", "authors": ["Dinghong Song", "Yuan Feng", "Yiwei Wang", "Shangye Chen", "Cyril Guyot", "Filip Blagojevic", "Hyeran Jeon", "Pengfei Su", "Dong Li"], "title": "AttnCache: Accelerating Self-Attention Inference for LLM Prefill via Attention Cache", "categories": ["cs.CL", "cs.LG"], "comment": "10 pages, 6 figures, submitted to Ninth Annual Conference on Machine\n  Learning and Systems (MLSys'26)", "summary": "Large Language Models (LLMs) are widely used in generative applications such\nas chatting, code generation, and reasoning. However, many realworld workloads\nsuch as classification, question answering, recommendation, and text embedding\nrely solely on the prefill stage of inference, where the model encodes input\nsequences without performing autoregressive decoding. In these prefill only\nscenarios, the self-attention computation becomes the primary performance\nbottleneck due to its quadratic complexity with respect to sequence length. In\nthis paper, we observe that semantically different sentences often produce\nsimilar attention maps across layers and heads. Building on this insight, we\npropose AttnCache, a framework that accelerates the prefill stage of LLM\ninference by retrieving and reusing similar attention maps. Based on an\nattention map memorization database, AttnCache employs efficient caching and\nsimilarity search techniques to identify and reuse pre-cached attention maps\nduring inference, thereby reducing the computational overhead of\nself-attention. Experimental results show that AttnCache achieves an average of\n1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x\nattention speedup on GPU, with negligible accuracy degradation.", "AI": {"tldr": "\u63d0\u51faAttnCache\u6846\u67b6\uff0c\u901a\u8fc7\u7f13\u5b58\u548c\u91cd\u7528\u76f8\u4f3c\u6ce8\u610f\u529b\u56fe\u52a0\u901fLLM\u9884\u586b\u5145\u9636\u6bb5\u63a8\u7406\uff0c\u5728CPU/GPU\u4e0a\u5206\u522b\u5b9e\u73b01.2x/1.6x\u7aef\u5230\u7aef\u52a0\u901f\u4e14\u7cbe\u5ea6\u635f\u5931\u53ef\u5ffd\u7565", "motivation": "LLM\u9884\u586b\u5145\u9636\u6bb5\u7684\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\u5b58\u5728\u4e8c\u6b21\u590d\u6742\u5ea6\u74f6\u9888\uff0c\u5f71\u54cd\u5206\u7c7b/\u95ee\u7b54/\u63a8\u8350\u7b49\u975e\u751f\u6210\u5f0f\u4efb\u52a1\u7684\u63a8\u7406\u6548\u7387", "method": "\u57fa\u4e8e\u6ce8\u610f\u529b\u56fe\u8de8\u5c42/\u5934\u7684\u76f8\u4f3c\u6027\u89c2\u5bdf\uff0c\u5efa\u7acb\u6ce8\u610f\u529b\u56fe\u8bb0\u5fc6\u6570\u636e\u5e93\uff0c\u91c7\u7528\u7f13\u5b58\u548c\u76f8\u4f3c\u6027\u68c0\u7d22\u6280\u672f\u590d\u7528\u5386\u53f2\u8ba1\u7b97\u7ed3\u679c", "result": "CPU\u7aef\u5230\u7aef\u52a0\u901f1.2x\uff08\u6ce8\u610f\u529b2x\uff09\uff0cGPU\u7aef\u5230\u7aef\u52a0\u901f1.6x\uff08\u6ce8\u610f\u529b3x\uff09\uff0c\u7cbe\u5ea6\u4e0b\u964d<0.5%", "conclusion": "AttnCache\u6709\u6548\u7a81\u7834\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\u74f6\u9888\uff0c\u4e3aLLM\u975e\u751f\u6210\u5f0f\u5e94\u7528\u63d0\u4f9b\u9ad8\u6548\u63a8\u7406\u65b9\u6848"}}
{"id": "2510.25992", "pdf": "https://arxiv.org/pdf/2510.25992", "abs": "https://arxiv.org/abs/2510.25992", "authors": ["Yihe Deng", "I-Hung Hsu", "Jun Yan", "Zifeng Wang", "Rujun Han", "Gufeng Zhang", "Yanfei Chen", "Wei Wang", "Tomas Pfister", "Chen-Yu Lee"], "title": "Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) often struggle with problems that require\nmulti-step reasoning. For small-scale open-source models, Reinforcement\nLearning with Verifiable Rewards (RLVR) fails when correct solutions are rarely\nsampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to\noverfit long demonstrations through rigid token-by-token imitation. To address\nthis gap, we propose Supervised Reinforcement Learning (SRL), a framework that\nreformulates problem solving as generating a sequence of logical \"actions\". SRL\ntrains the model to generate an internal reasoning monologue before committing\nto each action. It provides smoother rewards based on the similarity between\nthe model's actions and expert actions extracted from the SFT dataset in a\nstep-wise manner. This supervision offers richer learning signals even when all\nrollouts are incorrect, while encouraging flexible reasoning guided by expert\ndemonstrations. As a result, SRL enables small models to learn challenging\nproblems previously unlearnable by SFT or RLVR. Moreover, initializing training\nwith SRL before refining with RLVR yields the strongest overall performance.\nBeyond reasoning benchmarks, SRL generalizes effectively to agentic software\nengineering tasks, establishing it as a robust and versatile training framework\nfor reasoning-oriented LLMs.", "AI": {"tldr": "\u63d0\u51fa\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\uff08SRL\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u76d1\u7763\u4e0e\u5f3a\u5316\u5b66\u4e60\u4f18\u52bf\uff0c\u6709\u6548\u63d0\u5347\u5c0f\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u5176\u901a\u7528\u6027\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u5728\u6b63\u786e\u65b9\u6848\u6781\u5c11\u88ab\u91c7\u6837\u65f6\u5931\u6548\uff0c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5b58\u5728\u5bf9\u957f\u6f14\u793a\u7684\u50f5\u5316\u6a21\u4eff\u95ee\u9898\u3002\u9700\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u89e3\u51b3\u5c0f\u6a21\u578b\u591a\u6b65\u63a8\u7406\u7684\u5b66\u4e60\u74f6\u9888\u3002", "method": "\u5c06\u95ee\u9898\u89e3\u51b3\u91cd\u6784\u4e3a\u903b\u8f91\u52a8\u4f5c\u5e8f\u5217\u751f\u6210\uff0c\u8bad\u7ec3\u6a21\u578b\u5728\u8f93\u51fa\u6bcf\u4e2a\u52a8\u4f5c\u524d\u751f\u6210\u5185\u90e8\u63a8\u7406\u72ec\u767d\u3002\u901a\u8fc7\u9010\u6b65\u6bd4\u5bf9\u6a21\u578b\u52a8\u4f5c\u4e0e\u4e13\u5bb6\u52a8\u4f5c\u7684\u76f8\u4f3c\u6027\u63d0\u4f9b\u5e73\u6ed1\u5956\u52b1\u4fe1\u53f7\u3002", "result": "SRL\u4f7f\u5c0f\u6a21\u578b\u638c\u63e1RLVR/SFT\u65e0\u6cd5\u5b66\u4e60\u7684\u590d\u6742\u4efb\u52a1\uff0cSRL+RLVR\u7ec4\u5408\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\uff0c\u4e14\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u5c55\u73b0\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SRL\u901a\u8fc7\u878d\u5408\u76d1\u7763\u4fe1\u53f7\u4e0e\u5f3a\u5316\u5b66\u4e60\u673a\u5236\uff0c\u5efa\u7acb\u4e86\u9762\u5411\u63a8\u7406\u4efb\u52a1\u7684\u901a\u7528\u8bad\u7ec3\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u5c0f\u6a21\u578b\u590d\u6742\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002"}}
{"id": "2510.26020", "pdf": "https://arxiv.org/pdf/2510.26020", "abs": "https://arxiv.org/abs/2510.26020", "authors": ["Feijie Wu", "Weiwu Zhu", "Yuxiang Zhang", "Soumya Chatterjee", "Jiarong Zhu", "Fan Mo", "Rodin Luo", "Jing Gao"], "title": "PORTool: Tool-Use LLM Training with Rewarded Tree", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Current tool-use large language models (LLMs) are trained on static datasets,\nenabling them to interact with external tools and perform multi-step,\ntool-integrated reasoning, which produces tool-call trajectories. However,\nthese models imitate how a query is resolved in a generic tool-call routine,\nthereby failing to explore possible solutions and demonstrating limited\nperformance in an evolved, dynamic tool-call environment. In this work, we\npropose PORTool, a reinforcement learning (RL) method that encourages a\ntool-use LLM to explore various trajectories yielding the correct answer.\nSpecifically, this method starts with generating multiple rollouts for a given\nquery, and some of them share the first few tool-call steps, thereby forming a\ntree-like structure. Next, we assign rewards to each step, based on its ability\nto produce a correct answer and make successful tool calls. A shared step\nacross different trajectories receives the same reward, while different steps\nunder the same fork receive different rewards. Finally, these step-wise rewards\nare used to calculate fork-relative advantages, blended with\ntrajectory-relative advantages, to train the LLM for tool use. The experiments\nutilize 17 tools to address user queries, covering both time-sensitive and\ntime-invariant topics. We conduct ablation studies to systematically justify\nthe necessity and the design robustness of step-wise rewards. Furthermore, we\ncompare the proposed PORTool with other training approaches and demonstrate\nsignificant improvements in final accuracy and the number of tool-call steps.", "AI": {"tldr": "\u63d0\u51faPORTool\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a2\u7d22\u591a\u79cd\u5de5\u5177\u8c03\u7528\u8f68\u8ff9\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5de5\u5177\u8c03\u7528\u6a21\u578b\u4f9d\u8d56\u9759\u6001\u6570\u636e\u96c6\u6a21\u4eff\u5e38\u89c4\u6d41\u7a0b\uff0c\u7f3a\u4e4f\u5bf9\u52a8\u6001\u73af\u5883\u7684\u9002\u5e94\u6027\u63a2\u7d22\u3002", "method": "1. \u751f\u6210\u591a\u8def\u5f84\u63a8\u7406\u6811 2. \u57fa\u4e8e\u7b54\u6848\u6b63\u786e\u6027\u548c\u5de5\u5177\u8c03\u7528\u6210\u529f\u7387\u7684\u5c42\u7ea7\u5956\u52b1\u673a\u5236 3. \u878d\u5408\u5206\u652f\u76f8\u5173\u4e0e\u8f68\u8ff9\u76f8\u5173\u4f18\u52bf\u7684\u6df7\u5408\u8bad\u7ec3\u7b56\u7565", "result": "\u572817\u4e2a\u5de5\u5177\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u6700\u7ec8\u51c6\u786e\u7387\uff08+8.9%\uff09\u5e76\u51cf\u5c11\u5e73\u5747\u5de5\u5177\u8c03\u7528\u6b65\u9aa4\uff08-2.1\u6b65\uff09", "conclusion": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5f15\u5bfc\u7684\u63a2\u7d22\u6027\u8bad\u7ec3\u53ef\u6709\u6548\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u5de5\u5177\u73af\u5883\u4e2d\u7684\u9002\u5e94\u80fd\u529b\u3002"}}
{"id": "2510.26024", "pdf": "https://arxiv.org/pdf/2510.26024", "abs": "https://arxiv.org/abs/2510.26024", "authors": ["HyoJung Han", "Sweta Agrawal", "Eleftheria Briakou"], "title": "Rethinking Cross-lingual Alignment: Balancing Transfer and Cultural Erasure in Multilingual LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Cross-lingual alignment (CLA) aims to align multilingual representations,\nenabling Large Language Models (LLMs) to seamlessly transfer knowledge across\nlanguages. While intuitive, we hypothesize, this pursuit of representational\nconvergence can inadvertently cause \"cultural erasure\", the functional loss of\nproviding culturally-situated responses that should diverge based on the query\nlanguage. In this work, we systematically analyze this trade-off by introducing\na holistic evaluation framework, the transfer-localization plane, which\nquantifies both desirable knowledge transfer and undesirable cultural erasure.\nUsing this framework, we re-evaluate recent CLA approaches and find that they\nconsistently improve factual transfer at the direct cost of cultural\nlocalization across all six languages studied. Our investigation into the\ninternal representations of these models reveals a key insight: universal\nfactual transfer and culturally-specific knowledge are optimally steerable at\ndifferent model layers. Based on this finding, we propose Surgical Steering, a\nnovel inference-time method that disentangles these two objectives. By applying\ntargeted activation steering to distinct layers, our approach achieves a better\nbalance between the two competing dimensions, effectively overcoming the\nlimitations of current alignment techniques.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86\u8de8\u8bed\u8a00\u5bf9\u9f50\uff08CLA\uff09\u5728\u4fc3\u8fdb\u77e5\u8bc6\u8fc1\u79fb\u65f6\u4f1a\u5bfc\u81f4\u6587\u5316\u64e6\u9664\u95ee\u9898\uff0c\u63d0\u51fa\u8bc4\u4f30\u6846\u67b6'\u8fc1\u79fb-\u672c\u5730\u5316\u5e73\u9762'\u5e76\u901a\u8fc7\u5c42\u6b21\u89e3\u8026\u7684Surgical Steering\u65b9\u6cd5\u5b9e\u73b0\u5e73\u8861", "motivation": "\u73b0\u6709\u8de8\u8bed\u8a00\u5bf9\u9f50\u65b9\u6cd5\u8ffd\u6c42\u8868\u5f81\u7edf\u4e00\uff0c\u4f46\u53ef\u80fd\u635f\u5bb3\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u8a00\u6587\u5316\u80cc\u666f\u4e0b\u7684\u5dee\u5f02\u5316\u54cd\u5e94\u80fd\u529b\uff08\u6587\u5316\u64e6\u9664\u73b0\u8c61\uff09", "method": "1. \u5efa\u7acb\u8fc1\u79fb-\u672c\u5730\u5316\u5e73\u9762\u8bc4\u4f30\u6846\u67b6\uff1b2. \u5206\u6790\u516d\u79cd\u8bed\u8a00\u7684CLA\u6a21\u578b\uff1b3. \u53d1\u73b0\u8868\u5f81\u5c42\u6b21\u5206\u5de5\u89c4\u5f8b\uff1b4. \u63d0\u51fa\u57fa\u4e8e\u5c42\u6b21\u6fc0\u6d3b\u63a7\u5236\u7684Surgical Steering\u65b9\u6cd5", "result": "1. CLA\u65b9\u6cd5\u5728\u516d\u79cd\u8bed\u8a00\u4e2d\u5747\u663e\u793a\u77e5\u8bc6\u8fc1\u79fb\u63d0\u5347\u4f34\u968f\u6587\u5316\u672c\u5730\u5316\u4e0b\u964d\uff1b2. \u6a21\u578b\u6d45\u5c42\u9002\u5408\u6587\u5316\u7279\u5f02\u6027\u63a7\u5236\uff0c\u6df1\u5c42\u9002\u5408\u901a\u7528\u77e5\u8bc6\u8fc1\u79fb\uff1b3. Surgical Steering\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5b9e\u73b0\u66f4\u4f18\u5e73\u8861", "conclusion": "\u901a\u8fc7\u5c42\u6b21\u89e3\u8026\u7684\u6fc0\u6d3b\u63a7\u5236\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u8de8\u8bed\u8a00\u77e5\u8bc6\u8fc1\u79fb\u4f18\u52bf\u7684\u540c\u65f6\u6709\u6548\u4fdd\u7559\u6587\u5316\u7279\u5f02\u6027\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u5bf9\u9f50\u6280\u672f\u7684\u5c40\u9650\u6027"}}
{"id": "2510.26032", "pdf": "https://arxiv.org/pdf/2510.26032", "abs": "https://arxiv.org/abs/2510.26032", "authors": ["Felipe Larios", "Mariana Borras-Osorio", "Yuqi Wu", "Ana Gabriela Claros", "David Toro-Tobon", "Esteban Cabezas", "Ricardo Loor-Torres", "Maria Mateo Chavez", "Kerly Guevara Maldonado", "Luis Vilatuna Andrango", "Maria Lizarazo Jimenez", "Ivan Mateo Alzamora", "Misk Al Zahidy", "Marcelo Montero", "Ana Cristina Proano", "Cristian Soto Jacome", "Jungwei W. Fan", "Oscar J. Ponce-Ponte", "Megan E. Branda", "Naykky Singh Ospina", "Juan P. Brito"], "title": "Artificial Intelligence-Enabled Analysis of Radiology Reports: Epidemiology and Consequences of Incidental Thyroid Findings", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Importance Incidental thyroid findings (ITFs) are increasingly detected on\nimaging performed for non-thyroid indications. Their prevalence, features, and\nclinical consequences remain undefined. Objective To develop, validate, and\ndeploy a natural language processing (NLP) pipeline to identify ITFs in\nradiology reports and assess their prevalence, features, and clinical outcomes.\nDesign, Setting, and Participants Retrospective cohort of adults without prior\nthyroid disease undergoing thyroid-capturing imaging at Mayo Clinic sites from\nJuly 1, 2017, to September 30, 2023. A transformer-based NLP pipeline\nidentified ITFs and extracted nodule characteristics from image reports from\nmultiple modalities and body regions. Main Outcomes and Measures Prevalence of\nITFs, downstream thyroid ultrasound, biopsy, thyroidectomy, and thyroid cancer\ndiagnosis. Logistic regression identified demographic and imaging-related\nfactors. Results Among 115,683 patients (mean age, 56.8 [SD 17.2] years; 52.9%\nwomen), 9,077 (7.8%) had an ITF, of which 92.9% were nodules. ITFs were more\nlikely in women, older adults, those with higher BMI, and when imaging was\nordered by oncology or internal medicine. Compared with chest CT, ITFs were\nmore likely via neck CT, PET, and nuclear medicine scans. Nodule\ncharacteristics were poorly documented, with size reported in 44% and other\nfeatures in fewer than 15% (e.g. calcifications). Compared with patients\nwithout ITFs, those with ITFs had higher odds of thyroid nodule diagnosis,\nbiopsy, thyroidectomy and thyroid cancer diagnosis. Most cancers were\npapillary, and larger when detected after ITFs vs no ITF. Conclusions ITFs were\ncommon and strongly associated with cascades leading to the detection of small,\nlow-risk cancers. These findings underscore the role of ITFs in thyroid cancer\noverdiagnosis and the need for standardized reporting and more selective\nfollow-up.", "AI": {"tldr": "\u7532\u72b6\u817a\u5076\u53d1\u53d1\u73b0\uff08ITFs\uff09\u5e38\u89c1\u4e14\u4e0e\u7532\u72b6\u817a\u764c\u8fc7\u5ea6\u8bca\u65ad\u663e\u8457\u76f8\u5173\uff0c\u9700\u6807\u51c6\u5316\u62a5\u544a\u548c\u9009\u62e9\u6027\u968f\u8bbf", "motivation": "\u968f\u7740\u5f71\u50cf\u5b66\u68c0\u67e5\u666e\u53ca\uff0c\u7532\u72b6\u817a\u5076\u53d1\u53d1\u73b0\u7684\u4e34\u5e8a\u610f\u4e49\u548c\u540e\u679c\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u7cfb\u7edf\u6027\u8bc4\u4f30\u5176\u6d41\u884c\u75c5\u5b66\u7279\u5f81\u548c\u8bca\u7597\u5f71\u54cd", "method": "\u5f00\u53d1\u57fa\u4e8eTransformer\u7684NLP\u6d41\u7a0b\uff0c\u5206\u67902017-2023\u5e74115,683\u4f8b\u65e0\u7532\u72b6\u817a\u75c5\u53f2\u60a3\u8005\u7684\u5f71\u50cf\u62a5\u544a\uff0c\u8ffd\u8e2a\u540e\u7eed\u8d85\u58f0\u3001\u6d3b\u68c0\u3001\u624b\u672f\u53ca\u764c\u75c7\u8bca\u65ad", "result": "7.8%\u60a3\u8005\u53d1\u73b0ITFs\uff0892.9%\u4e3a\u7ed3\u8282\uff09\uff0c\u5973\u6027/\u9ad8\u9f84/\u80bf\u7624\u79d1\u7533\u8bf7\u8005\u66f4\u6613\u51fa\u73b0\uff1bITFs\u60a3\u8005\u7532\u72b6\u817a\u764c\u8bca\u65ad\u7387\u663e\u8457\u5347\u9ad8\uff08\u591a\u4e3a\u5fae\u5c0f\u4e73\u5934\u72b6\u764c\uff09", "conclusion": "ITFs\u5bfc\u81f4\u4f4e\u98ce\u9669\u7532\u72b6\u817a\u764c\u7684\u8fc7\u5ea6\u8bca\u65ad\uff0c\u5efa\u8bae\u89c4\u8303\u5f71\u50cf\u62a5\u544a\u6807\u51c6\u5e76\u4f18\u5316\u968f\u8bbf\u7b56\u7565\u4ee5\u964d\u4f4e\u533b\u7597\u98ce\u9669"}}
{"id": "2510.26101", "pdf": "https://arxiv.org/pdf/2510.26101", "abs": "https://arxiv.org/abs/2510.26101", "authors": ["Taku Mikuriya", "Tatsuya Ishigaki", "Masayuki Kawarada", "Shunya Minami", "Tadashi Kadowaki", "Yohichi Suzuki", "Soshun Naito", "Shunya Takata", "Takumi Kato", "Tamotsu Basseda", "Reo Yamada", "Hiroya Takamura"], "title": "QCoder Benchmark: Bridging Language Generation and Quantum Hardware through Simulator-Based Feedback", "categories": ["cs.CL", "cs.PL", "quant-ph"], "comment": null, "summary": "Large language models (LLMs) have increasingly been applied to automatic\nprogramming code generation. This task can be viewed as a language generation\ntask that bridges natural language, human knowledge, and programming logic.\nHowever, it remains underexplored in domains that require interaction with\nhardware devices, such as quantum programming, where human coders write Python\ncode that is executed on a quantum computer. To address this gap, we introduce\nQCoder Benchmark, an evaluation framework that assesses LLMs on quantum\nprogramming with feedback from simulated hardware devices. Our benchmark offers\ntwo key features. First, it supports evaluation using a quantum simulator\nenvironment beyond conventional Python execution, allowing feedback of\ndomain-specific metrics such as circuit depth, execution time, and error\nclassification, which can be used to guide better generation. Second, it\nincorporates human-written code submissions collected from real programming\ncontests, enabling both quantitative comparisons and qualitative analyses of\nLLM outputs against human-written codes. Our experiments reveal that even\nadvanced models like GPT-4o achieve only around 18.97% accuracy, highlighting\nthe difficulty of the benchmark. In contrast, reasoning-based models such as o3\nreach up to 78% accuracy, outperforming averaged success rates of human-written\ncodes (39.98%). We release the QCoder Benchmark dataset and public evaluation\nAPI to support further research.", "AI": {"tldr": "\u63d0\u51fa\u91cf\u5b50\u7f16\u7a0b\u8bc4\u4f30\u6846\u67b6QCoder Benchmark\uff0c\u901a\u8fc7\u6a21\u62df\u786c\u4ef6\u53cd\u9988\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u6027\u80fd\uff0c\u53d1\u73b0\u63a8\u7406\u578b\u6a21\u578bo3\u51c6\u786e\u738778%\u8fdc\u8d85\u4eba\u7c7b\u5e73\u5747\u6c34\u5e7339.98%", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9700\u786c\u4ef6\u4ea4\u4e92\u7684\u91cf\u5b50\u7f16\u7a0b\u9886\u57df\u7814\u7a76\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u7ed3\u5408\u786c\u4ef6\u53cd\u9988\u548c\u4eba\u7c7b\u4ee3\u7801\u5bf9\u6bd4\u7684\u7cfb\u7edf\u8bc4\u4f30\u65b9\u6cd5", "method": "\u5f00\u53d1\u652f\u6301\u91cf\u5b50\u6a21\u62df\u5668\u73af\u5883\u8bc4\u6d4b\u7684\u6846\u67b6\uff0c\u6574\u5408\u7535\u8def\u6df1\u5ea6/\u6267\u884c\u65f6\u95f4\u7b49\u786c\u4ef6\u6307\u6807\uff0c\u5e76\u91c7\u7528\u771f\u5b9e\u7ade\u8d5b\u7684\u4eba\u7c7b\u4ee3\u7801\u8fdb\u884c\u5b9a\u91cf\u5b9a\u6027\u5206\u6790", "result": "GPT-4o\u51c6\u786e\u7387\u4ec518.97%\uff0c\u63a8\u7406\u6a21\u578bo3\u8fbe78%\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u4eba\u7c7b\u4ee3\u780139.98%\u7684\u5e73\u5747\u6210\u529f\u7387", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u8bc4\u4f30\u6a21\u578b\u91cf\u5b50\u7f16\u7a0b\u80fd\u529b\uff0c\u63ed\u793a\u63a8\u7406\u6a21\u578b\u4f18\u52bf\uff0c\u53d1\u5e03\u6570\u636e\u96c6\u548cAPI\u63a8\u52a8\u9886\u57df\u53d1\u5c55"}}
{"id": "2510.26122", "pdf": "https://arxiv.org/pdf/2510.26122", "abs": "https://arxiv.org/abs/2510.26122", "authors": ["Feng Ju", "Zeyu Qin", "Rui Min", "Zhitao He", "Lingpeng Kong", "Yi R. Fung"], "title": "Reasoning Path Divergence: A New Metric and Curation Strategy to Unlock LLM Diverse Thinking", "categories": ["cs.CL"], "comment": null, "summary": "While Test-Time Scaling (TTS) has proven effective in improving the reasoning\nability of large language models (LLMs), low diversity in model outputs often\nbecomes a bottleneck; this is partly caused by the common \"one problem, one\nsolution\" (1P1S) training practice, which provides a single canonical answer\nand can push models toward a narrow set of reasoning paths. To address this, we\npropose a \"one problem, multiple solutions\" (1PNS) training paradigm that\nexposes the model to a variety of valid reasoning trajectories and thus\nincreases inference diversity. A core challenge for 1PNS is reliably measuring\nsemantic differences between multi-step chains of thought, so we introduce\nReasoning Path Divergence (RPD), a step-level metric that aligns and scores\nLong Chain-of-Thought solutions to capture differences in intermediate\nreasoning. Using RPD, we curate maximally diverse solution sets per problem and\nfine-tune Qwen3-4B-Base. Experiments show that RPD-selected training yields\nmore varied outputs and higher pass@k, with an average +2.80% gain in pass@16\nover a strong 1P1S baseline and a +4.99% gain on AIME24, demonstrating that\n1PNS further amplifies the effectiveness of TTS. Our code is available at\nhttps://github.com/fengjujf/Reasoning-Path-Divergence .", "AI": {"tldr": "\u63d0\u51fa1PNS\u8bad\u7ec3\u8303\u5f0f\u548cRPD\u6307\u6807\uff0c\u901a\u8fc7\u589e\u52a0\u8bad\u7ec3\u6570\u636e\u7684\u63a8\u7406\u8def\u5f84\u591a\u6837\u6027\u663e\u8457\u63d0\u5347LLM\u7684\u63a8\u7406\u6548\u679c\u548c\u8f93\u51fa\u591a\u6837\u6027", "motivation": "\u4f20\u7edf1P1S\u8bad\u7ec3\u6a21\u5f0f\u5bfc\u81f4LLM\u8f93\u51fa\u591a\u6837\u6027\u4f4e\uff0c\u9650\u5236TTS\u6548\u679c\u63d0\u5347\u3002\u9700\u8981\u91cf\u5316\u601d\u7ef4\u94fe\u5dee\u5f02\u5e76\u5f15\u5165\u591a\u6837\u5316\u63a8\u7406\u8f68\u8ff9", "method": "\u5f00\u53d1RPD\u6307\u6807\u8861\u91cf\u591a\u6b65\u63a8\u7406\u8def\u5f84\u5dee\u5f02\uff0c\u57fa\u4e8e\u8be5\u6307\u6807\u7b5b\u9009\u591a\u6837\u5316\u89e3\u51b3\u65b9\u6848\u6784\u5efa1PNS\u8bad\u7ec3\u96c6\uff0c\u5fae\u8c03Qwen3-4B-Base\u6a21\u578b", "result": "\u5728pass@16\u6307\u6807\u4e0a\u5e73\u5747\u63d0\u53472.80%\uff0cAIME24\u6570\u636e\u96c6\u63d0\u53474.99%\u3002\u9a8c\u8bc1\u4e86\u591a\u6837\u5316\u8bad\u7ec3\u5bf9TTS\u6548\u679c\u7684\u589e\u5f3a\u4f5c\u7528", "conclusion": "1PNS\u8bad\u7ec3\u8303\u5f0f\u7ed3\u5408RPD\u6307\u6807\u80fd\u6709\u6548\u7a81\u7834\u4f20\u7edf\u8bad\u7ec3\u6a21\u5f0f\u9650\u5236\uff0c\u8bc1\u660e\u63a8\u7406\u8def\u5f84\u591a\u6837\u6027\u662f\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u7684\u5173\u952e\u56e0\u7d20"}}
{"id": "2510.26124", "pdf": "https://arxiv.org/pdf/2510.26124", "abs": "https://arxiv.org/abs/2510.26124", "authors": ["Nawar Turk", "Sevag Kaspar", "Leila Kosseim"], "title": "On the Influence of Discourse Relations in Persuasive Texts", "categories": ["cs.CL", "I.2.7; I.2.6"], "comment": "Published in Proceedings of the 38th Canadian Conference on\n  Artificial Intelligence CanAI 2025 Calgary Alberta May 26-27 2025. 5 figures\n  7 tables", "summary": "This paper investigates the relationship between Persuasion Techniques (PTs)\nand Discourse Relations (DRs) by leveraging Large Language Models (LLMs) and\nprompt engineering. Since no dataset annotated with both PTs and DRs exists, we\ntook the SemEval 2023 Task 3 dataset labelled with 19 PTs as a starting point\nand developed LLM-based classifiers to label each instance of the dataset with\none of the 22 PDTB 3.0 level-2 DRs. In total, four LLMs were evaluated using 10\ndifferent prompts, resulting in 40 unique DR classifiers. Ensemble models using\ndifferent majority-pooling strategies were used to create 5 silver datasets of\ninstances labelled with both persuasion techniques and level-2 PDTB senses. The\nsilver dataset sizes vary from 1,281 instances to 204 instances, depending on\nthe majority pooling technique used. Statistical analysis of these silver\ndatasets shows that six discourse relations (namely Cause, Purpose, Contrast,\nCause+Belief, Concession, and Condition) play a crucial role in persuasive\ntexts, especially in the use of Loaded Language, Exaggeration/Minimisation,\nRepetition and to cast Doubt. This insight can contribute to detecting online\npropaganda and misinformation, as well as to our general understanding of\neffective communication.", "AI": {"tldr": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5206\u6790\u8bf4\u670d\u6280\u5de7\u4e0e\u8bdd\u8bed\u5173\u7cfb\uff0c\u53d1\u73b0\u516d\u4e2a\u5173\u952e\u8bdd\u8bed\u5173\u7cfb\u5728\u5ba3\u4f20\u68c0\u6d4b\u548c\u6709\u6548\u6c9f\u901a\u4e2d\u7684\u4f5c\u7528\u3002", "motivation": "\u7f3a\u4e4f\u540c\u65f6\u6807\u6ce8\u8bf4\u670d\u6280\u5de7\u548c\u8bdd\u8bed\u5173\u7cfb\u7684\u6570\u636e\u96c6\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e8c\u8005\u5173\u8054\u4ee5\u5e2e\u52a9\u68c0\u6d4b\u7f51\u7edc\u5ba3\u4f20\u548c\u865a\u5047\u4fe1\u606f\u3002", "method": "\u4f7f\u75284\u79cdLLM\u6a21\u578b\u914d\u540810\u79cd\u63d0\u793a\u7b56\u7565\u751f\u621040\u4e2a\u5206\u7c7b\u5668\uff0c\u901a\u8fc7\u591a\u6570\u8868\u51b3\u96c6\u6210\u521b\u5efa5\u4e2a\u94f6\u6807\u6570\u636e\u96c6\uff08204-1281\u4e2a\u6837\u672c\uff09\u3002", "result": "\u8bc6\u522b\u51fa\u56e0\u679c\u3001\u76ee\u7684\u3001\u5bf9\u6bd4\u7b49\u516d\u79cd\u6838\u5fc3\u8bdd\u8bed\u5173\u7cfb\uff0c\u4e0e\u717d\u52a8\u6027\u8bed\u8a00\u3001\u5938\u5f20\u3001\u91cd\u590d\u7b49\u8bf4\u670d\u6280\u5de7\u5b58\u5728\u663e\u8457\u5173\u8054\u3002", "conclusion": "\u8be5\u53d1\u73b0\u4e3a\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u63d0\u4f9b\u65b0\u89c6\u89d2\uff0c\u540c\u65f6\u6df1\u5316\u5bf9\u6709\u6548\u4f20\u64ad\u673a\u5236\u7684\u7406\u8bba\u7406\u89e3\uff0c\u5177\u6709\u53cc\u91cd\u5e94\u7528\u4ef7\u503c\u3002"}}
