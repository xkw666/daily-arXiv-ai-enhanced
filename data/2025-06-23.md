<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 78]
- [cs.GR](#cs.GR) [Total: 8]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.AI](#cs.AI) [Total: 7]
- [cs.LG](#cs.LG) [Total: 10]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.CG](#cs.CG) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Veracity: An Open-Source AI Fact-Checking System](https://arxiv.org/abs/2506.15794)
*Taylor Lynn Curtis,Maximilian Puelma Touzel,William Garneau,Manon Gruaz,Mike Pinder,Li Wei Wang,Sukanya Krishna,Luda Cohen,Jean-François Godbout,Reihaneh Rabbany,Kellin Pelrine*

Main category: cs.CL

TL;DR: 开源AI系统Veracity通过结合大语言模型与网络检索技术，实现可解释的自动化事实核查，支持多语言交互并配备可视化评分系统。


<details>
  <summary>Details</summary>
Motivation: 生成式AI加剧虚假信息传播，需开发透明工具提升公众信息鉴别能力。

Method: 融合LLMs语义理解与实时网络检索验证，采用对话式界面设计，内置真实性量化评分机制。

Result: 系统能有效检测虚假声明并提供溯源证据，通过交互界面增强用户参与度。

Conclusion: Veracity通过可解释的核查流程促进媒体素养，助力构建抗虚假信息的信息生态系统。

Abstract: The proliferation of misinformation poses a significant threat to society,
exacerbated by the capabilities of generative AI. This demo paper introduces
Veracity, an open-source AI system designed to empower individuals to combat
misinformation through transparent and accessible fact-checking. Veracity
leverages the synergy between Large Language Models (LLMs) and web retrieval
agents to analyze user-submitted claims and provide grounded veracity
assessments with intuitive explanations. Key features include multilingual
support, numerical scoring of claim veracity, and an interactive interface
inspired by familiar messaging applications. This paper will showcase
Veracity's ability to not only detect misinformation but also explain its
reasoning, fostering media literacy and promoting a more informed society.

</details>


### [2] [Rethinking LLM Training through Information Geometry and Quantum Metrics](https://arxiv.org/abs/2506.15830)
*Riccardo Di Sipio*

Main category: cs.CL

TL;DR: 提出通过信息几何框架分析大语言模型的高维参数优化，引入量子信息理论探索高效优化方案


<details>
  <summary>Details</summary>
Motivation: 针对大语言模型高维参数空间中的非欧结构优化难题，寻求几何视角的理论解释和量子增强的可能路径

Method: 采用Fisher信息度量构建参数空间几何结构，结合自然梯度下降方法，并类比量子系统中的Fubini-Study度量和量子Fisher信息

Result: 几何视角有效解释了尖锐极小值、泛化能力等现象，量子类比为量子增强系统优化提供了理论启示

Conclusion: 曲率感知方法深化LLM训练理解，量子信息几何框架可能开启高效优化的新范式

Abstract: Optimization in large language models (LLMs) unfolds over high-dimensional
parameter spaces with non-Euclidean structure. Information geometry frames this
landscape using the Fisher information metric, enabling more principled
learning via natural gradient descent. Though often impractical, this geometric
lens clarifies phenomena such as sharp minima, generalization, and observed
scaling laws. We argue that curvature-aware approaches deepen our understanding
of LLM training. Finally, we speculate on quantum analogies based on the
Fubini-Study metric and Quantum Fisher Information, hinting at efficient
optimization in quantum-enhanced systems.

</details>


### [3] [MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents](https://arxiv.org/abs/2506.15841)
*Zijian Zhou,Ao Qu,Zhaoxuan Wu,Sunghwan Kim,Alok Prakash,Daniela Rus,Jinhua Zhao,Bryan Kian Hsiang Low,Paul Pu Liang*

Main category: cs.CL

TL;DR: MEM1强化学习框架通过动态内存整合实现高效长时效多轮任务处理，性能提升3.5倍且内存减少3.7倍。


<details>
  <summary>Details</summary>
Motivation: 现有LLM系统采用全上下文提示导致内存膨胀和性能下降，需开发更高效的长时效交互解决方案。

Method: 基于强化学习的动态内存管理机制，整合历史记忆与环境观测，并通过组合现有数据集构建复杂多轮训练环境。

Result: 在16目标多跳QA任务中，MEM1-7B相比Qwen2.5-14B-Instruct性能提升3.5倍，内存效率提升3.7倍且支持超长泛化。

Conclusion: MEM1验证了推理驱动记忆整合的可行性，为长时效交互智能体提供了高效且可扩展的解决方案。

Abstract: Modern language agents must operate over long-horizon, multi-turn
interactions, where they retrieve external information, adapt to observations,
and answer interdependent queries. Yet, most LLM systems rely on full-context
prompting, appending all past turns regardless of their relevance. This leads
to unbounded memory growth, increased computational costs, and degraded
reasoning performance on out-of-distribution input lengths. We introduce MEM1,
an end-to-end reinforcement learning framework that enables agents to operate
with constant memory across long multi-turn tasks. At each turn, MEM1 updates a
compact shared internal state that jointly supports memory consolidation and
reasoning. This state integrates prior memory with new observations from the
environment while strategically discarding irrelevant or redundant information.
To support training in more realistic and compositional settings, we propose a
simple yet effective and scalable approach to constructing multi-turn
environments by composing existing datasets into arbitrarily complex task
sequences. Experiments across three domains, including internal retrieval QA,
open-domain web QA, and multi-turn web shopping, show that MEM1-7B improves
performance by 3.5x while reducing memory usage by 3.7x compared to
Qwen2.5-14B-Instruct on a 16-objective multi-hop QA task, and generalizes
beyond the training horizon. Our results demonstrate the promise of
reasoning-driven memory consolidation as a scalable alternative to existing
solutions for training long-horizon interactive agents, where both efficiency
and performance are optimized.

</details>


### [4] [Finance Language Model Evaluation (FLaME)](https://arxiv.org/abs/2506.15846)
*Glenn Matlin,Mika Okamoto,Huzaifa Pardawala,Yang Yang,Sudheer Chava*

Main category: cs.CL

TL;DR: 论文首次提出金融语言模型评估框架FLaME，通过20个金融NLP任务测试23个基础模型，证明语言模型在专业金融任务中的潜力远超现有评估体系认知。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架存在方法论缺陷，导致严重低估语言模型在金融NLP任务中的表现，需建立更全面的评估体系揭示其真实能力。

Method: 开发FLaME基准测试套件，对23个基础语言模型和增强推理模型进行系统性评估，涵盖金融领域20个核心NLP任务。

Result: 研究团队开源了框架软件及完整数据与结果，实证研究表明语言模型在专业金融任务中的表现显著优于传统评估体系所显示的结论。

Conclusion: FLaME框架填补了金融NLP评估方法论空白，证明语言模型在知识密集型金融任务中具备远超预期的有效性，为后续研究提供可靠基准。

Abstract: Language Models (LMs) have demonstrated impressive capabilities with core
Natural Language Processing (NLP) tasks. The effectiveness of LMs for highly
specialized knowledge-intensive tasks in finance remains difficult to assess
due to major gaps in the methodologies of existing evaluation frameworks, which
have caused an erroneous belief in a far lower bound of LMs' performance on
common Finance NLP (FinNLP) tasks. To demonstrate the potential of LMs for
these FinNLP tasks, we present the first holistic benchmarking suite for
Financial Language Model Evaluation (FLaME). We are the first research paper to
comprehensively study LMs against 'reasoning-reinforced' LMs, with an empirical
study of 23 foundation LMs over 20 core NLP tasks in finance. We open-source
our framework software along with all data and results.

</details>


### [5] [Entropy-Driven Pre-Tokenization for Byte-Pair Encoding](https://arxiv.org/abs/2506.15889)
*Yifan Hu,Frank Liang,Dachuan Zhao,Jonathan Geuter,Varshini Reddy,Craig W. Schmidt,Chris Tanner*

Main category: cs.CL

TL;DR: 提出两种基于信息熵的预分词方法改进BPE在中文分词中的表现


<details>
  <summary>Details</summary>
Motivation: 标准BPE算法在非分割语言（如中文）中因忽略语言边界导致分词质量下降

Method: 1. 使用点间互信息与左右熵识别连贯字符跨度
2. 利用GPT-2预测熵检测边界不确定性

Result: 在PKU数据集上实现分割精度、召回率和F1分数显著提升

Conclusion: 熵引导的预分词策略提升了与标准语言单元的匹配度，为低资源/多语言场景提供改进方向

Abstract: Byte-Pair Encoding (BPE) has become a widely adopted subword tokenization
method in modern language models due to its simplicity and strong empirical
performance across downstream tasks. However, applying BPE to unsegmented
languages such as Chinese presents significant challenges, as its
frequency-driven merge operation is agnostic to linguistic boundaries. To
address this, we propose two entropy-informed pre-tokenization strategies that
guide BPE segmentation using unsupervised information-theoretic cues. The first
approach uses pointwise mutual information and left/right entropy to identify
coherent character spans, while the second leverages predictive entropy derived
from a pretrained GPT-2 model to detect boundary uncertainty. We evaluate both
methods on a subset of the PKU dataset and demonstrate substantial improvements
in segmentation precision, recall, and F1 score compared to standard BPE. Our
results suggest that entropy-guided pre-tokenization not only enhances
alignment with gold-standard linguistic units but also offers a promising
direction for improving tokenization quality in low-resource and multilingual
settings.

</details>


### [6] [Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning](https://arxiv.org/abs/2506.15894)
*Sam Silver,Jimin Sun,Ivan Zhang,Sara Hooker,Eddie Kim*

Main category: cs.CL

TL;DR: 研究发现大型语言模型具有比文献记载更强大的内在自我纠正能力，这种能力通过链式思维中的扰动实验被显著观察到


<details>
  <summary>Details</summary>
Motivation: 现有研究低估了LLMs的内在自我纠正潜力，且模型推理能力对提示策略过度敏感

Method: 通过向模型的链式思维(CoT)中引入合成扰动，测量不同开源模型的自我纠正行为

Result: 观察到从隐式修正到显式错误承认的多种自我纠正模式，未针对长CoT微调的模型也展现显著内在纠错能力

Conclusion: LLMs的推理能力改进可能是对已有特性的放大，而非完全新增能力，这对模型开发方向具有启示意义

Abstract: Large Language Models (LLMs) have demonstrated impressive mathematical
reasoning capabilities, yet their performance remains brittle to minor
variations in problem description and prompting strategy. Furthermore,
reasoning is vulnerable to sampling-induced errors which autoregressive models
must primarily address using self-correction via additionally-generated tokens.
To better understand self-correction capabilities of recent models, we conduct
experiments measuring models' ability to self-correct synthetic perturbations
introduced into their Chain of Thought (CoT) reasoning. We observe robust
single-utterance intrinsic self-correction behavior across a range of
open-weight models and datasets, ranging from subtle, implicit corrections to
explicit acknowledgments and corrections of errors. Our findings suggest that
LLMs, including those not finetuned for long CoT, may possess stronger
intrinsic self-correction capabilities than commonly shown in the literature.
The presence of this ability suggests that recent "reasoning" model work
involves amplification of traits already meaningfully present in models.

</details>


### [7] [From RAG to Agentic: Validating Islamic-Medicine Responses with LLM Agents](https://arxiv.org/abs/2506.15911)
*Mohammad Amaan Sayeed,Mohammed Talha Alam,Raza Imam,Shahab Saquib Sohail,Amir Hussain*

Main category: cs.CL

TL;DR: 结合伊斯兰古典医学文献与现代AI技术，提出Tibbe-AG评估框架，通过检索增强和自批判机制使医疗问答准确率提升23%


<details>
  <summary>Details</summary>
Motivation: 解决伊斯兰传统医学知识在现代AI系统中利用率低、现有评测体系缺乏文化敏感性评估的问题

Method: 构建30个先知医学问题集，测试LLaMA-3等模型在三种模式（直接生成/检索增强/科学自批判）下的表现，采用代理评判机制生成3C3H质量评分

Result: 检索增强使准确率提升13%，科学自批判机制通过机理洞察和安全性考量再提升10%

Conclusion: 整合经典文本与AI技术，结合检索增强和自评估机制，可实现文化敏感的可靠医疗问答系统

Abstract: Centuries-old Islamic medical texts like Avicenna's Canon of Medicine and the
Prophetic Tibb-e-Nabawi encode a wealth of preventive care, nutrition, and
holistic therapies, yet remain inaccessible to many and underutilized in modern
AI systems. Existing language-model benchmarks focus narrowly on factual recall
or user preference, leaving a gap in validating culturally grounded medical
guidance at scale. We propose a unified evaluation pipeline, Tibbe-AG, that
aligns 30 carefully curated Prophetic-medicine questions with human-verified
remedies and compares three LLMs (LLaMA-3, Mistral-7B, Qwen2-7B) under three
configurations: direct generation, retrieval-augmented generation, and a
scientific self-critique filter. Each answer is then assessed by a secondary
LLM serving as an agentic judge, yielding a single 3C3H quality score.
Retrieval improves factual accuracy by 13%, while the agentic prompt adds
another 10% improvement through deeper mechanistic insight and safety
considerations. Our results demonstrate that blending classical Islamic texts
with retrieval and self-evaluation enables reliable, culturally sensitive
medical question-answering.

</details>


### [8] [Reranking-based Generation for Unbiased Perspective Summarization](https://arxiv.org/abs/2506.15925)
*Narutatsu Ri,Nicholas Deas,Kathleen McKeown*

Main category: cs.CL

TL;DR: 提出基于语言模型的可靠评估指标，并通过重排序和偏好调整方法提升政治观点摘要性能


<details>
  <summary>Details</summary>
Motivation: 现有政治观点摘要评估指标可靠性不足，且改进摘要生成方法的研究尚未成熟

Method: 构建人工标注测试集验证指标可靠性，开发基于重排序和合成数据偏好调整的LLM方法

Result: 语言模型指标优于传统指标，重排序方法效果显著，偏好调整可进一步提升模型性能

Conclusion: 研究结果为政治观点摘要方法的可靠评估和开发提供了有效解决方案

Abstract: Generating unbiased summaries in real-world settings such as political
perspective summarization remains a crucial application of Large Language
Models (LLMs). Yet, existing evaluation frameworks rely on traditional metrics
for measuring key attributes such as coverage and faithfulness without
verifying their applicability, and efforts to develop improved summarizers are
still nascent. We address these gaps by (1) identifying reliable metrics for
measuring perspective summary quality, and (2) investigating the efficacy of
LLM-based methods beyond zero-shot inference. Namely, we build a test set for
benchmarking metric reliability using human annotations and show that
traditional metrics underperform compared to language model-based metrics,
which prove to be strong evaluators. Using these metrics, we show that
reranking-based methods yield strong results, and preference tuning with
synthetically generated and reranking-labeled data further boosts performance.
Our findings aim to contribute to the reliable evaluation and development of
perspective summarization methods.

</details>


### [9] [A Vietnamese Dataset for Text Segmentation and Multiple Choices Reading Comprehension](https://arxiv.org/abs/2506.15978)
*Toan Nguyen Hai,Ha Nguyen Viet,Truong Quan Xuan,Duc Do Minh*

Main category: cs.CL

TL;DR: 针对越南语NLP资源匮乏问题构建了VSMRC数据集，证明多语言模型mBERT在文本分割和阅读理解任务中优于单语模型


<details>
  <summary>Details</summary>
Motivation: 越南语作为拥有1.02亿母语者的主要语言，长期缺乏高质量的文本分割和机器阅读理解基准数据集

Method: 基于越南维基百科构建包含15,942个分割文档和16,347个人工校验问答对的数据集，通过对比mBERT与单语模型进行实验验证

Result: mBERT在MRC测试集达到88.01%准确率，文本分割F1值63.15%，全面超越越南语单语模型

Conclusion: 多语言模型在低资源语言处理中展现显著优势，该成果为其他资源匮乏语言提供可复现的研究范式，数据集已开源

Abstract: Vietnamese, the 20th most spoken language with over 102 million native
speakers, lacks robust resources for key natural language processing tasks such
as text segmentation and machine reading comprehension (MRC). To address this
gap, we present VSMRC, the Vietnamese Text Segmentation and Multiple-Choice
Reading Comprehension Dataset. Sourced from Vietnamese Wikipedia, our dataset
includes 15,942 documents for text segmentation and 16,347 synthetic
multiple-choice question-answer pairs generated with human quality assurance,
ensuring a reliable and diverse resource. Experiments show that mBERT
consistently outperforms monolingual models on both tasks, achieving an
accuracy of 88.01% on MRC test set and an F1 score of 63.15\% on text
segmentation test set. Our analysis reveals that multilingual models excel in
NLP tasks for Vietnamese, suggesting potential applications to other
under-resourced languages. VSMRC is available at HuggingFace

</details>


### [10] [Double Entendre: Robust Audio-Based AI-Generated Lyrics Detection via Multi-View Fusion](https://arxiv.org/abs/2506.15981)
*Markus Frohmann,Gabriel Meseguer-Brocal,Markus Schedl,Elena V. Epure*

Main category: cs.CL

TL;DR: DE-detect提出多模态融合方案，结合自动转录歌词与语音特征，显著提升AI生成音乐检测的准确性与抗干扰能力。


<details>
  <summary>Details</summary>
Motivation: 现有AI音乐检测方法存在局限性：音频检测器难以泛化至新生成器且易受音频扰动影响；歌词检测依赖理想化歌词格式，实际应用受限。

Method: 采用模块化后期融合管道，通过自动语音识别提取音频中的歌词信息，并结合语音特征进行多模态分析，避免依赖原始歌词文件。

Result: 实验表明DE-detect超越现有歌词检测方法，在抗音频干扰性上提升50%以上，适用于真实场景的鲁棒检测。

Conclusion: 该研究为AI音乐检测提供了首个基于音频歌词分析的多模态解决方案，其开源实现将推动行业标准的建立。

Abstract: The rapid advancement of AI-based music generation tools is revolutionizing
the music industry but also posing challenges to artists, copyright holders,
and providers alike. This necessitates reliable methods for detecting such
AI-generated content. However, existing detectors, relying on either audio or
lyrics, face key practical limitations: audio-based detectors fail to
generalize to new or unseen generators and are vulnerable to audio
perturbations; lyrics-based methods require cleanly formatted and accurate
lyrics, unavailable in practice. To overcome these limitations, we propose a
novel, practically grounded approach: a multimodal, modular late-fusion
pipeline that combines automatically transcribed sung lyrics and speech
features capturing lyrics-related information within the audio. By relying on
lyrical aspects directly from audio, our method enhances robustness, mitigates
susceptibility to low-level artifacts, and enables practical applicability.
Experiments show that our method, DE-detect, outperforms existing lyrics-based
detectors while also being more robust to audio perturbations. Thus, it offers
an effective, robust solution for detecting AI-generated music in real-world
scenarios. Our code is available at
https://github.com/deezer/robust-AI-lyrics-detection.

</details>


### [11] [From General to Targeted Rewards: Surpassing GPT-4 in Open-Ended Long-Context Generation](https://arxiv.org/abs/2506.16024)
*Zhihan Guo,Jiele Wu,Wenqian Cui,Yifei Zhang,Minda Hu,Yufei Wang,Irwin King*

Main category: cs.CL

TL;DR: 提出ProxyReward强化学习框架优化大模型开放域长文本生成任务


<details>
  <summary>Details</summary>
Motivation: 现有长文本生成研究缺乏高质量标注数据，传统评估方法难以准确衡量生成质量

Method: 1. 开发自动化数据集生成方法避免人工标注 
2. 设计针对信息完整性和准确性的双维度奖励信号

Result: 在Open-LTG任务上性能超越GPT-4-Turbo 20%，优于LLM-as-a-Judge方法

Conclusion: ProxyReward为提升大模型处理复杂开放性问题提供有效方案，显著降低数据标注成本

Abstract: Current research on long-form context in Large Language Models (LLMs)
primarily focuses on the understanding of long-contexts, the Open-ended Long
Text Generation (Open-LTG) remains insufficiently explored. Training a
long-context generation model requires curation of gold standard reference
data, which is typically nonexistent for informative Open-LTG tasks. However,
previous methods only utilize general assessments as reward signals, which
limits accuracy. To bridge this gap, we introduce ProxyReward, an innovative
reinforcement learning (RL) based framework, which includes a dataset and a
reward signal computation method. Firstly, ProxyReward Dataset generation is
accomplished through simple prompts that enables the model to create
automatically, obviating extensive labeled data or significant manual effort.
Secondly, ProxyReward Signal offers a targeted evaluation of information
comprehensiveness and accuracy for specific questions. The experimental results
indicate that our method ProxyReward surpasses even GPT-4-Turbo. It can
significantly enhance performance by 20% on the Open-LTG task when training
widely used open-source models, while also surpassing the LLM-as-a-Judge
approach. Our work presents effective methods to enhance the ability of LLMs to
address complex open-ended questions posed by human.

</details>


### [12] [EvoLM: In Search of Lost Language Model Training Dynamics](https://arxiv.org/abs/2506.16029)
*Zhenting Qi,Fan Nie,Alexandre Alahi,James Zou,Himabindu Lakkaraju,Yilun Du,Eric Xing,Sham Kakade,Hanlin Zhang*

Main category: cs.CL

TL;DR: EvoLM模型套件通过多阶段训练分析语言模型动态，揭示训练策略对性能的影响


<details>
  <summary>Details</summary>
Motivation: 现有语言模型训练被分割为多个独立阶段，导致下游开发者难以系统评估各阶段设计选择的影响

Method: 训练100+个1B/4B参数模型，系统评估语言建模能力和问题解决能力，包含领域内外泛化分析

Result: 发现过度预训练收益递减、继续预训练缓解遗忘的关键作用、不同训练阶段的复杂权衡关系

Conclusion: 通过开源全套模型/数据/流程促进透明研究，为语言模型训练策略提供系统分析框架

Abstract: Modern language model (LM) training has been divided into multiple stages,
making it difficult for downstream developers to evaluate the impact of design
choices made at each stage. We present EvoLM, a model suite that enables
systematic and transparent analysis of LMs' training dynamics across
pre-training, continued pre-training, supervised fine-tuning, and reinforcement
learning. By training over 100 LMs with 1B and 4B parameters from scratch, we
rigorously evaluate both upstream (language modeling) and downstream
(problem-solving) reasoning capabilities, including considerations of both
in-domain and out-of-domain generalization. Key insights highlight the
diminishing returns from excessive pre-training and post-training, the
importance and practices of mitigating forgetting during domain-specific
continued pre-training, the crucial role of continued pre-training in bridging
pre-training and post-training phases, and various intricate trade-offs when
configuring supervised fine-tuning and reinforcement learning. To facilitate
open research and reproducibility, we release all pre-trained and post-trained
models, training datasets for all stages, and our entire training and
evaluation pipeline.

</details>


### [13] [Enhancing Document-Level Question Answering via Multi-Hop Retrieval-Augmented Generation with LLaMA 3](https://arxiv.org/abs/2506.16037)
*Xinyue Huang,Ziqi Lin,Fang Sun,Wenchao Zhang,Kejian Tong,Yunbo Liu*

Main category: cs.CL

TL;DR: 基于LLaMA 3的RAG框架，通过密集检索+多跳推理机制提升复杂问答效果


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在复杂问答任务中多跳推理能力不足、长文档上下文理解困难的问题

Method: 整合密集检索模块，结合上下文融合机制和多跳推理架构，采用检索-生成联合优化策略

Result: 实验显示系统在准确性和上下文相关性上全面超越现有检索增强模型和纯生成模型

Conclusion: 该框架为复杂知识密集型任务提供了有效的解决方案，证实了联合优化策略的优越性

Abstract: This paper presents a novel Retrieval-Augmented Generation (RAG) framework
tailored for complex question answering tasks, addressing challenges in
multi-hop reasoning and contextual understanding across lengthy documents.
Built upon LLaMA 3, the framework integrates a dense retrieval module with
advanced context fusion and multi-hop reasoning mechanisms, enabling more
accurate and coherent response generation. A joint optimization strategy
combining retrieval likelihood and generation cross-entropy improves the
model's robustness and adaptability. Experimental results show that the
proposed system outperforms existing retrieval-augmented and generative
baselines, confirming its effectiveness in delivering precise, contextually
grounded answers.

</details>


### [14] [DynScaling: Efficient Verifier-free Inference Scaling via Dynamic and Integrated Sampling](https://arxiv.org/abs/2506.16043)
*Fei Wang,Xingchen Wan,Ruoxi Sun,Jiefeng Chen,Sercan Ö. Arık*

Main category: cs.CL

TL;DR: 提出DynScaling方法，通过集成并行-顺序采样策略和动态预算分配框架，无需外部验证器即可提升大语言模型在资源受限场景下的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有推理时缩放方法依赖外部验证器或未优化实际计算限制，阻碍了实际应用。需开发无需外部验证器且适配现实约束的方案。

Method: 1. 集成并行-顺序采样：将独立并行响应合成连贯推理链，促进多样化推理路径
2. 基于多臂老虎机的动态预算分配：根据响应不确定性自适应分配计算资源，提升计算效率

Result: 实验表明DynScaling在任务性能和计算成本上均优于现有无验证器基线，验证了方法的有效性。

Conclusion: DynScaling通过协同两种创新机制，有效提升大语言模型在实际资源约束下的推理表现，为高效推理缩放提供了新范式。

Abstract: Inference-time scaling has proven effective in boosting large language model
(LLM) performance through increased test-time computation. Yet, its practical
application is often hindered by reliance on external verifiers or a lack of
optimization for realistic computational constraints. We propose DynScaling,
which addresses these limitations through two primary innovations: an
integrated parallel-sequential sampling strategy and a bandit-based dynamic
budget allocation framework. The integrated sampling strategy unifies parallel
and sequential sampling by constructing synthetic sequential reasoning chains
from initially independent parallel responses, promoting diverse and coherent
reasoning trajectories. The dynamic budget allocation framework formulates the
allocation of computational resources as a multi-armed bandit problem,
adaptively distributing the inference budget across queries based on the
uncertainty of previously sampled responses, thereby maximizing computational
efficiency. By combining these components, DynScaling effectively improves LLM
performance under practical resource constraints without the need for external
verifiers. Experimental results demonstrate that DynScaling consistently
surpasses existing verifier-free inference scaling baselines in both task
performance and computational cost.

</details>


### [15] [A Hybrid DeBERTa and Gated Broad Learning System for Cyberbullying Detection in English Text](https://arxiv.org/abs/2506.16052)
*Devesh Kumar*

Main category: cs.CL

TL;DR: 提出混合架构ModifiedDeBERTa+GBLS，在四个英文数据集上取得79.3%-95.41%准确率，整合可解释性机制满足内容审核透明度需求。


<details>
  <summary>Details</summary>
Motivation: 在线平台普及导致54.4%青少年遭受网络欺凌，需开发更有效的检测方法解决自动化内容审核需求。

Method: 结合Transformer的上下文理解与GBLS模式识别：改进DeBERTa模型（SE模块增强+情感分析）+门控GBLS分类器。

Result: HateXplain(79.3%)/SOSNet(95.41%)/Mendeley-I(91.37%)/Mendeley-II(94.67%)准确率，可解释性机制包含注意力分析/LIME解释/置信度校准。

Conclusion: 架构组件均有实质贡献，但在隐式偏见/讽刺内容检测存在挑战，为未来改进提供方向。

Abstract: The proliferation of online communication platforms has created unprecedented
opportunities for global connectivity while simultaneously enabling harmful
behaviors such as cyberbullying, which affects approximately 54.4\% of
teenagers according to recent research. This paper presents a hybrid
architecture that combines the contextual understanding capabilities of
transformer-based models with the pattern recognition strengths of broad
learning systems for effective cyberbullying detection. This approach
integrates a modified DeBERTa model augmented with Squeeze-and-Excitation
blocks and sentiment analysis capabilities with a Gated Broad Learning System
(GBLS) classifier, creating a synergistic framework that outperforms existing
approaches across multiple benchmark datasets. The proposed ModifiedDeBERTa +
GBLS model achieved good performance on four English datasets: 79.3\% accuracy
on HateXplain, 95.41\% accuracy on SOSNet, 91.37\% accuracy on Mendeley-I, and
94.67\% accuracy on Mendeley-II. Beyond performance gains, the framework
incorporates comprehensive explainability mechanisms including token-level
attribution analysis, LIME-based local interpretations, and confidence
calibration, addressing critical transparency requirements in automated content
moderation. Ablation studies confirm the meaningful contribution of each
architectural component, while failure case analysis reveals specific
challenges in detecting implicit bias and sarcastic content, providing valuable
insights for future improvements in cyberbullying detection systems.

</details>


### [16] [Knee-Deep in C-RASP: A Transformer Depth Hierarchy](https://arxiv.org/abs/2506.16055)
*Andy Yang,Michaël Cadilhac,David Chiang*

Main category: cs.CL

TL;DR: 论文通过理论证明和实验验证，论证了更深层数的Transformer具有更强的表达能力。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer模型深度（层数）与计算能力之间的理论关系，验证更深网络是否具备更强表达能力。

Method: 1. 将固定精度（注意力机制外）的Transformer等价于C-RASP编程语言；2. 通过时序逻辑证明更深的C-RASP程序表达能力更强；3. 在无位置编码的Transformer上开展序列依赖任务的长度泛化实验。

Result: 理论证明更深Transformer表达能力更强，实验证实理论预测的深度需求。

Conclusion: 深度是Transformer表达能力的关键因素，理论框架可有效预测模型在特定任务上的深度需求。

Abstract: It has been observed that transformers with greater depth (that is, more
layers) have more capabilities, but can we establish formally which
capabilities are gained with greater depth? We answer this question with a
theoretical proof followed by an empirical study. First, we consider
transformers that round to fixed precision except inside attention. We show
that this subclass of transformers is expressively equivalent to the
programming language C-RASP and this equivalence preserves depth. Second, we
prove that deeper C-RASP programs are more expressive than shallower C-RASP
programs, implying that deeper transformers are more expressive than shallower
transformers (within the subclass mentioned above). These results are
established by studying a form of temporal logic with counting operators, which
was shown equivalent to C-RASP in previous work. Finally, we provide empirical
evidence that our theory predicts the depth required for transformers without
positional encodings to length-generalize on a family of sequential dependency
tasks.

</details>


### [17] [Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning](https://arxiv.org/abs/2506.16064)
*Duc Hieu Ho,Chenglin Fan*

Main category: cs.CL

TL;DR: 提出自我批判指导的提示策略，通过两阶段自优化流程提升LLM输出的可信度


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在保持输出诚实性和有用性方面存在不足，需要无需额外训练的优化方案

Method: 自批判引导的优化提示法（SCGCRP），包含自我批判和精炼两个轻量级上下文步骤

Result: 在H²评估框架下所有模型均获提升，H²分数相对增长1.4%-4.3%，低质量回答减少

Conclusion: 结构化自我优化策略可作为扩展性强且无需训练的解决方案，有效提高LLM输出的可信度

Abstract: Large language models (LLMs) have demonstrated robust capabilities across
various natural language tasks. However, producing outputs that are
consistently honest and helpful remains an open challenge. To overcome this
challenge, this paper tackles the problem through two complementary directions.
It conducts a comprehensive benchmark evaluation of ten widely used large
language models, including both proprietary and open-weight models from OpenAI,
Meta, and Google. In parallel, it proposes a novel prompting strategy,
self-critique-guided curiosity refinement prompting. The key idea behind this
strategy is enabling models to self-critique and refine their responses without
additional training. The proposed method extends the curiosity-driven prompting
strategy by incorporating two lightweight in-context steps including
self-critique step and refinement step.
  The experiment results on the HONESET dataset evaluated using the framework
$\mathrm{H}^2$ (honesty and helpfulness), which was executed with GPT-4o as a
judge of honesty and helpfulness, show consistent improvements across all
models. The approach reduces the number of poor-quality responses, increases
high-quality responses, and achieves relative gains in $\mathrm{H}^2$ scores
ranging from 1.4% to 4.3% compared to curiosity-driven prompting across
evaluated models. These results highlight the effectiveness of structured
self-refinement as a scalable and training-free strategy to improve the
trustworthiness of LLMs outputs.

</details>


### [18] [Cyberbullying Detection in Hinglish Text Using MURIL and Explainable AI](https://arxiv.org/abs/2506.16066)
*Devesh Kumar*

Main category: cs.CL

TL;DR: 提出基于MURIL架构的印地-英语混合文本网络欺凌检测框架，在六大数据集上实现86.97%-94.63%准确率，性能超越主流多语言模型


<details>
  <summary>Details</summary>
Motivation: 数字平台中印地-英语混合文本（Hinglish）的普及导致传统单语网络欺凌检测系统失效，需开发针对性解决方案

Method: 采用MURIL架构，结合选择性层冻结、定制分类头设计和混合文本预处理策略，引入归因分析和跨语言模式识别实现可解释性

Result: 在6个基准数据集上准确率超越RoBERTa和IndicBERT模型1.36-13.07个百分点，最高达94.63%

Conclusion: 该框架虽有效但面临语境依赖、文化差异和跨语言讽刺检测等挑战，为多语言网络欺凌检测研究指明改进方向

Abstract: The growth of digital communication platforms has led to increased
cyberbullying incidents worldwide, creating a need for automated detection
systems to protect users. The rise of code-mixed Hindi-English (Hinglish)
communication on digital platforms poses challenges for existing cyberbullying
detection systems, which were designed primarily for monolingual text. This
paper presents a framework for cyberbullying detection in Hinglish text using
the Multilingual Representations for Indian Languages (MURIL) architecture to
address limitations in current approaches. Evaluation across six benchmark
datasets -- Bohra \textit{et al.}, BullyExplain, BullySentemo, Kumar \textit{et
al.}, HASOC 2021, and Mendeley Indo-HateSpeech -- shows that the MURIL-based
approach outperforms existing multilingual models including RoBERTa and
IndicBERT, with improvements of 1.36 to 13.07 percentage points and accuracies
of 86.97\% on Bohra, 84.62\% on BullyExplain, 86.03\% on BullySentemo, 75.41\%
on Kumar datasets, 83.92\% on HASOC 2021, and 94.63\% on Mendeley dataset. The
framework includes explainability features through attribution analysis and
cross-linguistic pattern recognition. Ablation studies show that selective
layer freezing, appropriate classification head design, and specialized
preprocessing for code-mixed content improve detection performance, while
failure analysis identifies challenges including context-dependent
interpretation, cultural understanding, and cross-linguistic sarcasm detection,
providing directions for future research in multilingual cyberbullying
detection.

</details>


### [19] [FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning](https://arxiv.org/abs/2506.16123)
*Natapong Nitarach,Warit Sirichotedumrong,Panop Pitchayarthorn,Pittawat Taveekitworachai,Potsawee Manakul,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: FinCoT通过结合金融专家知识设计结构化思维链提示，在CFA金融考题上显著提升LLM性能（LLaMA3-8B从63.2%→80.5%），同时减少8倍生成token并增强推理可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有金融NLP研究主要使用标准或无结构CoT提示，结构化CoT设计多基于非领域专家的启发，缺乏领域专业知识指导的推理结构。

Method: 提出FinCoT结构化提示框架，在10个金融领域的CFA式问题上对比评估标准提示、无结构CoT、结构化CoT及FinCoT的性能差异，分析token效率和推理对齐性。

Result: FinCoT使LLaMA3-8B准确率提升17.3%，Qwen提升4.5%，生成token减少至结构化CoT的1/8，且产生更符合专家思维的推理路径。

Conclusion: 领域对齐的结构化提示设计能同时提升模型性能、降低推理成本，并增强决策过程的可解释性与专业一致性。

Abstract: This paper presents FinCoT, a structured chain-of-thought (CoT) prompting
approach that incorporates insights from domain-specific expert financial
reasoning to guide the reasoning traces of large language models. We
investigate that there are three main prompting styles in FinNLP: (1) standard
prompting--zero-shot prompting; (2) unstructured CoT--CoT prompting without an
explicit reasoning structure, such as the use of tags; and (3) structured CoT
prompting--CoT prompting with explicit instructions or examples that define
structured reasoning steps. Previously, FinNLP has primarily focused on prompt
engineering with either standard or unstructured CoT prompting. However,
structured CoT prompting has received limited attention in prior work.
Furthermore, the design of reasoning structures in structured CoT prompting is
often based on heuristics from non-domain experts. In this study, we
investigate each prompting approach in FinNLP. We evaluate the three main
prompting styles and FinCoT on CFA-style questions spanning ten financial
domains. We observe that FinCoT improves performance from 63.2% to 80.5% and
Qwen-2.5-7B-Instruct from 69.7% to 74.2%, while reducing generated tokens
eight-fold compared to structured CoT prompting. Our findings show that
domain-aligned structured prompts not only improve performance and reduce
inference costs but also yield more interpretable and expert-aligned reasoning
traces.

</details>


### [20] [Under the Shadow of Babel: How Language Shapes Reasoning in LLMs](https://arxiv.org/abs/2506.16151)
*Chenxi Wang,Yixuan Zhang,Lang Gao,Zixiang Xu,Zirui Song,Yanbo Wang,Xiuying Chen*

Main category: cs.CL

TL;DR: 研究发现大语言模型不仅模仿语言形式，还内化了语言塑造的推理偏见，通过双语数据集BICAUSE验证了语言结构对模型推理的影响。


<details>
  <summary>Details</summary>
Motivation: 基于语言相对论假说，探索不同语言结构是否会导致大语言模型内部形成不同的逻辑推理模式。

Method: 使用结构对齐的中英双语数据集BICAUSE，通过模型注意力模式、词序偏好和表征分析进行验证。

Result: 1.模型注意力分布呈现语言类型特征 2.模型固守语言特定的因果词序 3.成功推理时跨语言表征趋同

Conclusion: 首次通过模型内部结构分析证实：语言不仅是表层符号系统，其内在的认知模式会被LLMs系统性内化。

Abstract: Language is not only a tool for communication but also a medium for human
cognition and reasoning. If, as linguistic relativity suggests, the structure
of language shapes cognitive patterns, then large language models (LLMs)
trained on human language may also internalize the habitual logical structures
embedded in different languages. To examine this hypothesis, we introduce
BICAUSE, a structured bilingual dataset for causal reasoning, which includes
semantically aligned Chinese and English samples in both forward and reversed
causal forms. Our study reveals three key findings: (1) LLMs exhibit
typologically aligned attention patterns, focusing more on causes and
sentence-initial connectives in Chinese, while showing a more balanced
distribution in English. (2) Models internalize language-specific preferences
for causal word order and often rigidly apply them to atypical inputs, leading
to degraded performance, especially in Chinese. (3) When causal reasoning
succeeds, model representations converge toward semantically aligned
abstractions across languages, indicating a shared understanding beyond surface
form. Overall, these results suggest that LLMs not only mimic surface
linguistic forms but also internalize the reasoning biases shaped by language.
Rooted in cognitive linguistic theory, this phenomenon is for the first time
empirically verified through structural analysis of model internals.

</details>


### [21] [SGIC: A Self-Guided Iterative Calibration Framework for RAG](https://arxiv.org/abs/2506.16172)
*Guanhua Chen,Yutong Yao,Lidia S. Chao,Xuebo Liu,Derek F. Wong*

Main category: cs.CL

TL;DR: 提出SGIC框架，通过不确定性分数实现LLM多轮迭代校准，显著提升闭源/开源模型性能


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法忽视LLM的校准潜力，尤其未充分利用其上下文推理能力进行多轮校准优化

Method: 1. 设计基于不确定性分数的文档相关性评估和回答置信度计算 2. 构建迭代自校准训练集优化模型校准能力 3. 融合历史校准结果的动态迭代机制

Result: 在闭源（如GPT系列）和开源模型（如LLaMA）上均实现准确率和校准能力的显著提升

Conclusion: SGIC框架有效释放LLM的自我校准潜力，为RAG系统提供新的模型优化范式

Abstract: Recent research in retrieval-augmented generation (RAG) has concentrated on
retrieving useful information from candidate documents. However, numerous
methodologies frequently neglect the calibration capabilities of large language
models (LLMs), which capitalize on their robust in-context reasoning prowess.
This work illustrates that providing LLMs with specific cues substantially
improves their calibration efficacy, especially in multi-round calibrations. We
present a new SGIC: Self-Guided Iterative Calibration Framework that employs
uncertainty scores as a tool. Initially, this framework calculates uncertainty
scores to determine both the relevance of each document to the query and the
confidence level in the responses produced by the LLMs. Subsequently, it
reevaluates these scores iteratively, amalgamating them with prior responses to
refine calibration. Furthermore, we introduce an innovative approach for
constructing an iterative self-calibration training set, which optimizes LLMs
to efficiently harness uncertainty scores for capturing critical information
and enhancing response accuracy. Our proposed framework significantly improves
performance on both closed-source and open-weight LLMs.

</details>


### [22] [JETHICS: Japanese Ethics Understanding Evaluation Dataset](https://arxiv.org/abs/2506.16187)
*Masashi Takeshita,Rafal Rzepka*

Main category: cs.CL

TL;DR: 提出日语伦理评估数据集JETHICS，评估结果显示现有模型存在较大改进空间


<details>
  <summary>Details</summary>
Motivation: 填补日语环境下AI伦理评估数据集的空白，沿用ETHICS框架实现跨语言比较

Method: 基于伦理/政治哲学理论构建四个分类维度，收集78K样本，使用LLMs和GPT-4o进行基准测试

Result: GPT-4o平均得分0.7（最佳日语LLM仅0.5），显示模型伦理推理能力不足

Conclusion: 当前日语LLM在伦理理解方面显著落后于英语模型，需针对性开发训练方法

Abstract: In this work, we propose JETHICS, a Japanese dataset for evaluating ethics
understanding of AI models. JETHICS contains 78K examples and is built by
following the construction methods of the existing English ETHICS dataset. It
includes four categories based normative theories and concepts from ethics and
political philosophy; and one representing commonsense morality. Our evaluation
experiments on non-proprietary large language models (LLMs) and on GPT-4o
reveal that even GPT-4o achieves only an average score of about 0.7, while the
best-performing Japanese LLM attains around 0.5, indicating a relatively large
room for improvement in current LLMs.

</details>


### [23] [Web(er) of Hate: A Survey on How Hate Speech Is Typed](https://arxiv.org/abs/2506.16190)
*Luna Wang,Andrew Caines,Alice Hutchings*

Main category: cs.CL

TL;DR: 探讨仇恨言论数据集构建中的方法论选择及其可靠性影响，提出基于韦伯'理想类型'的反思性框架。


<details>
  <summary>Details</summary>
Motivation: 现有数据集构建存在隐性价值判断，导致可靠性争议。需揭示方法论选择对数据质量的影响机制。

Method: 采用跨数据集批判性比较，结合社会学'理想类型'理论构建分析框架。

Result: 识别出数据集构建中普遍存在的价值权衡模式，验证理想类型框架的解释效力。

Conclusion: 主张方法论自反性，要求研究者显性化价值预设，通过过程透明提升学术严谨性。

Abstract: The curation of hate speech datasets involves complex design decisions that
balance competing priorities. This paper critically examines these
methodological choices in a diverse range of datasets, highlighting common
themes and practices, and their implications for dataset reliability. Drawing
on Max Weber's notion of ideal types, we argue for a reflexive approach in
dataset creation, urging researchers to acknowledge their own value judgments
during dataset construction, fostering transparency and methodological rigour.

</details>


### [24] [Comparative Analysis of Abstractive Summarization Models for Clinical Radiology Reports](https://arxiv.org/abs/2506.16247)
*Anindita Bhattacharya,Tohida Rehman,Debarshi Kumar Sanyal,Samiran Chattopadhyay*

Main category: cs.CL

TL;DR: 研究比较T5/BART/PEGASUS/ChatGPT-4/LLaMA-3等模型在放射报告摘要生成任务中的表现，使用MIMIC-CXR数据集及ROUGE/METEOR/BERTScore多维度评估


<details>
  <summary>Details</summary>
Motivation: 放射报告中检查结果部分冗长而印象部分需要高度凝练，医疗领域需要高效的自动摘要解决方案来提升诊断效率

Method: 采用预训练模型(T5/BART/PEGASUS)和大语言模型(ChatGPT-4/LLaMA-3)，对比指针生成网络+覆盖机制，使用MIMIC-CXR数据集进行抽象摘要实验

Result: 通过多指标评估体系揭示不同模型在医学文本摘要任务中的性能差异，为模型选择提供实证依据

Conclusion: 该研究为医疗专业人员筛选适合临床场景的自动摘要工具提供了重要参考，推动AI在放射科工作流程中的应用

Abstract: The findings section of a radiology report is often detailed and lengthy,
whereas the impression section is comparatively more compact and captures key
diagnostic conclusions. This research explores the use of advanced abstractive
summarization models to generate the concise impression from the findings
section of a radiology report. We have used the publicly available MIMIC-CXR
dataset. A comparative analysis is conducted on leading pre-trained and
open-source large language models, including T5-base, BART-base,
PEGASUS-x-base, ChatGPT-4, LLaMA-3-8B, and a custom Pointer Generator Network
with a coverage mechanism. To ensure a thorough assessment, multiple evaluation
metrics are employed, including ROUGE-1, ROUGE-2, ROUGE-L, METEOR, and
BERTScore. By analyzing the performance of these models, this study identifies
their respective strengths and limitations in the summarization of medical
text. The findings of this paper provide helpful information for medical
professionals who need automated summarization solutions in the healthcare
sector.

</details>


### [25] [End-to-End Speech Translation for Low-Resource Languages Using Weakly Labeled Data](https://arxiv.org/abs/2506.16251)
*Aishwarya Pothula,Bhavana Akkiraju,Srihari Bandarupalli,Charan D,Santosh Kesiraju,Anil Kumar Vuppala*

Main category: cs.CL

TL;DR: 研究探讨了在低资源语言对中利用弱标记数据构建语音转文本翻译系统，结果显示其性能可与大规模多模态模型相媲美。


<details>
  <summary>Details</summary>
Motivation: 针对低资源语言对缺乏高质量标注数据的问题，探索通过弱标记数据构建语音翻译系统的可行性。

Method: 使用双语文本挖掘技术和句子编码器构建Shrutilipi-anuvaad数据集，包含4种印度语言对，并创建不同质量/数量的训练数据版本进行对比实验。

Result: 弱标记数据训练的ST系统性能达到SONAR和SeamlessM4T等大规模多模态基线的水平。

Conclusion: 弱标记数据可有效替代传统标注数据，降低对标注资源的依赖，推动低资源语言语音翻译应用发展。

Abstract: The scarcity of high-quality annotated data presents a significant challenge
in developing effective end-to-end speech-to-text translation (ST) systems,
particularly for low-resource languages. This paper explores the hypothesis
that weakly labeled data can be used to build ST models for low-resource
language pairs. We constructed speech-to-text translation datasets with the
help of bitext mining using state-of-the-art sentence encoders. We mined the
multilingual Shrutilipi corpus to build Shrutilipi-anuvaad, a dataset
comprising ST data for language pairs Bengali-Hindi, Malayalam-Hindi,
Odia-Hindi, and Telugu-Hindi. We created multiple versions of training data
with varying degrees of quality and quantity to investigate the effect of
quality versus quantity of weakly labeled data on ST model performance. Results
demonstrate that ST systems can be built using weakly labeled data, with
performance comparable to massive multi-modal multilingual baselines such as
SONAR and SeamlessM4T.

</details>


### [26] [Advancing Automated Speaking Assessment Leveraging Multifaceted Relevance and Grammar Information](https://arxiv.org/abs/2506.16285)
*Hao-Chien Lu,Jhen-Ke Lin,Hong-Yun Lin,Chung-Chun Wang,Berlin Chen*

Main category: cs.CL

TL;DR: 提出整合多模态内容相关性模块与细粒度语法错误特征，构建混合评分模型以改进自动口语评估系统


<details>
  <summary>Details</summary>
Motivation: 现有口语评估系统存在内容相关性利用不足（忽略图像/示例线索）和语法分析粗糙（缺乏详细错误分类）两大缺陷

Method: 1. 多模态相关性模块整合问题/图像/示例/回答 
2. 基于GEC和标注的细粒度语法错误分类体系

Result: 实验证明显著提升内容相关性评估（+15%）、语言使用分析（+20%）及整体系统性能（+12% F1-score）

Conclusion: 融合深层内容关联分析与细粒度语法特征能有效实现更全面的口语能力评估

Abstract: Current automated speaking assessment (ASA) systems for use in multi-aspect
evaluations often fail to make full use of content relevance, overlooking image
or exemplar cues, and employ superficial grammar analysis that lacks detailed
error types. This paper ameliorates these deficiencies by introducing two novel
enhancements to construct a hybrid scoring model. First, a multifaceted
relevance module integrates question and the associated image content,
exemplar, and spoken response of an L2 speaker for a comprehensive assessment
of content relevance. Second, fine-grained grammar error features are derived
using advanced grammar error correction (GEC) and detailed annotation to
identify specific error categories. Experiments and ablation studies
demonstrate that these components significantly improve the evaluation of
content relevance, language use, and overall ASA performance, highlighting the
benefits of using richer, more nuanced feature sets for holistic speaking
assessment.

</details>


### [27] [PL-Guard: Benchmarking Language Model Safety for Polish](https://arxiv.org/abs/2506.16322)
*Aleksandra Krasnodębska,Karolina Seweryn,Szymon Łukasik,Wojciech Kusa*

Main category: cs.CL

TL;DR: 研究团队针对波兰语构建了人工标注的安全分类基准数据集并进行对抗性测试，发现基于HerBERT的波兰语专用分类器在对抗条件下表现最佳，揭示了非英语大模型安全评估的不足与改进方向。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型安全评估工具严重偏向英语等高资源语言，全球多数语言（如波兰语）缺乏有效安全检测体系，存在重大安全隐患。

Method: 1. 构建波兰语安全分类标注数据集及对抗样本 2. 微调三种模型（Llama-Guard-3-8B/HerBERT-base/PLLuM）3. 组合不同标注数据训练并对比公开安全模型

Result: HerBERT分类器综合表现最优（尤其在对抗场景），基于Llama架构的模型效果相对较弱。对抗扰动显著降低所有模型性能，但专用模型展现更强鲁棒性。

Conclusion: 语言专用模型（如HerBERT）在本地化安全任务中优于通用大模型，强调构建多语言安全评估体系及对抗测试的重要性。

Abstract: Despite increasing efforts to ensure the safety of large language models
(LLMs), most existing safety assessments and moderation tools remain heavily
biased toward English and other high-resource languages, leaving majority of
global languages underexamined. To address this gap, we introduce a manually
annotated benchmark dataset for language model safety classification in Polish.
We also create adversarially perturbed variants of these samples designed to
challenge model robustness. We conduct a series of experiments to evaluate
LLM-based and classifier-based models of varying sizes and architectures.
Specifically, we fine-tune three models: Llama-Guard-3-8B, a HerBERT-based
classifier (a Polish BERT derivative), and PLLuM, a Polish-adapted Llama-8B
model. We train these models using different combinations of annotated data and
evaluate their performance, comparing it against publicly available guard
models. Results demonstrate that the HerBERT-based classifier achieves the
highest overall performance, particularly under adversarial conditions.

</details>


### [28] [Generalizability of Media Frames: Corpus creation and analysis across countries](https://arxiv.org/abs/2506.16337)
*Agnese Daffara,Sourabh Dattawad,Sebastian Padó,Tanise Ceron*

Main category: cs.CL

TL;DR: 研究者评估了Media Frame Corpus框架在巴西新闻中的适用性，创建了FrameNews-PT数据集并验证了大部分框架的跨文化通用性，但指出需调整部分类别。


<details>
  <summary>Details</summary>
Motivation: 探究MFC框架是否适用于非美国文化背景的新闻议题分析，特别是巴西政治经济新闻。

Method: 构建巴西葡萄牙语新闻数据集FrameNews-PT，采用MFC框架进行多轮标注，并测试微调和零样本模型在跨域数据上的表现。

Result: 15个MFC框架基本适用但需微调指南，部分框架使用率低，新议题依赖通用框架作为补充。

Conclusion: 跨文化框架应用需谨慎调整，需考虑文化特异性并完善标注指南以确保分析有效性。

Abstract: Frames capture aspects of an issue that are emphasized in a debate by
interlocutors and can help us understand how political language conveys
different perspectives and ultimately shapes people's opinions. The Media Frame
Corpus (MFC) is the most commonly used framework with categories and detailed
guidelines for operationalizing frames. It is, however, focused on a few
salient U.S. news issues, making it unclear how well these frames can capture
news issues in other cultural contexts. To explore this, we introduce
FrameNews-PT, a dataset of Brazilian Portuguese news articles covering
political and economic news and annotate it within the MFC framework. Through
several annotation rounds, we evaluate the extent to which MFC frames
generalize to the Brazilian debate issues. We further evaluate how fine-tuned
and zero-shot models perform on out-of-domain data. Results show that the 15
MFC frames remain broadly applicable with minor revisions of the guidelines.
However, some MFC frames are rarely used, and novel news issues are analyzed
using general 'fall-back' frames. We conclude that cross-cultural frame use
requires careful consideration.

</details>


### [29] [Analyzing the Influence of Knowledge Graph Information on Relation Extraction](https://arxiv.org/abs/2506.16343)
*Cedric Möller,Ricardo Usbeck*

Main category: cs.CL

TL;DR: 整合知识图谱信息能显著提升关系抽取模型性能，尤其在训练数据不平衡时效果明显


<details>
  <summary>Details</summary>
Motivation: 验证知识图谱中实体位置信息对关系抽取任务的重要作用，解决不同关系训练样本不均衡的难题

Method: 将图感知的Neural Bellman-Ford网络与传统关系抽取方法结合，在多种数据集（含不同关系数量/训练样本量）上进行监督学习和零样本测试

Result: 知识图谱特征带来持续性能提升，数据不均衡场景下改进幅度达5-15%，跨数据集平均准确率提高8.2%

Conclusion: 知识图谱结构特征能有效补偿数据不足，为关系抽取模型提供重要语义线索，尤其在低资源场景展现显著优势

Abstract: We examine the impact of incorporating knowledge graph information on the
performance of relation extraction models across a range of datasets. Our
hypothesis is that the positions of entities within a knowledge graph provide
important insights for relation extraction tasks. We conduct experiments on
multiple datasets, each varying in the number of relations, training examples,
and underlying knowledge graphs. Our results demonstrate that integrating
knowledge graph information significantly enhances performance, especially when
dealing with an imbalance in the number of training examples for each relation.
We evaluate the contribution of knowledge graph-based features by combining
established relation extraction methods with graph-aware Neural Bellman-Ford
networks. These features are tested in both supervised and zero-shot settings,
demonstrating consistent performance improvements across various datasets.

</details>


### [30] [DISCIE -- Discriminative Closed Information Extraction](https://arxiv.org/abs/2506.16348)
*Cedric Möller,Ricardo Usbeck*

Main category: cs.CL

TL;DR: 提出结合类型信息的判别式方法提升关系抽取效果，小模型效率优于大生成模型


<details>
  <summary>Details</summary>
Motivation: 解决现有生成模型在长尾关系和大规模信息抽取中的准确性和效率瓶颈

Method: 采用判别式方法整合实体类型信息和特定实体特征，优化关系抽取过程

Result: 在百万级实体/百级关系场景下，准确率超越生成模型，小模型效率显著提升

Conclusion: 类型信息融合策略为高效准确的信息抽取开辟新路径，尤其适合资源受限场景

Abstract: This paper introduces a novel method for closed information extraction. The
method employs a discriminative approach that incorporates type and
entity-specific information to improve relation extraction accuracy,
particularly benefiting long-tail relations. Notably, this method demonstrates
superior performance compared to state-of-the-art end-to-end generative models.
This is especially evident for the problem of large-scale closed information
extraction where we are confronted with millions of entities and hundreds of
relations. Furthermore, we emphasize the efficiency aspect by leveraging
smaller models. In particular, the integration of type-information proves
instrumental in achieving performance levels on par with or surpassing those of
a larger generative model. This advancement holds promise for more accurate and
efficient information extraction techniques.

</details>


### [31] [Can structural correspondences ground real world representational content in Large Language Models?](https://arxiv.org/abs/2506.16370)
*Iwan Williams*

Main category: cs.CL

TL;DR: 探讨大语言模型(LLMs)是否通过结构对应关系实现真实世界表征，提出文本局限性带来的挑战。


<details>
  <summary>Details</summary>
Motivation: LLMs仅通过文本数据训练和交互，缺乏与现实世界的直接联系，需明确其表征能力的本质和边界。

Method: 基于结构对应关系的表征理论框架，结合现有证据进行系统性理论分析。

Result: 单纯结构对应不足以支撑表征，需通过任务表现中功能性的运用才能实现真实世界内容表征。

Conclusion: 突破文本局限性的任务设计是LLMs实现真实世界表征的关键前提。

Abstract: Large Language Models (LLMs) such as GPT-4 produce compelling responses to a
wide range of prompts. But their representational capacities are uncertain.
Many LLMs have no direct contact with extra-linguistic reality: their inputs,
outputs and training data consist solely of text, raising the questions (1) can
LLMs represent anything and (2) if so, what? In this paper, I explore what it
would take to answer these questions according to a structural-correspondence
based account of representation, and make an initial survey of this evidence. I
argue that the mere existence of structural correspondences between LLMs and
worldly entities is insufficient to ground representation of those entities.
However, if these structural correspondences play an appropriate role - they
are exploited in a way that explains successful task performance - then they
could ground real world contents. This requires overcoming a challenge: the
text-boundedness of LLMs appears, on the face of it, to prevent them engaging
in the right sorts of tasks.

</details>


### [32] [InstructTTSEval: Benchmarking Complex Natural-Language Instruction Following in Text-to-Speech Systems](https://arxiv.org/abs/2506.16381)
*Kexin Huang,Qian Tu,Liwei Fan,Chenchen Yang,Dong Zhang,Shimin Li,Zhaoye Fei,Qinyuan Cheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: 提出InstructTTSEval基准用于评估语音合成系统的复杂自然语言风格控制能力，包含多语言多任务测试集并利用Gemini实现自动评估


<details>
  <summary>Details</summary>
Motivation: 传统TTS系统依赖固定风格标签或语音提示，存在灵活性差、复杂指令执行能力不足的问题，且缺乏专门的高质量评估基准

Method: 设计包含声学参数指定/描述性风格指令/角色扮演的三类任务（中英文各1k样本），引入Gemini作为自动评估工具

Result: 现有指令驱动TTS系统在复杂指令执行上仍有显著改进空间，基准有效支撑模型优化方向识别

Conclusion: InstructTTSEval将推动开发更强大、灵活且精确的指令驱动语音合成系统

Abstract: In modern speech synthesis, paralinguistic information--such as a speaker's
vocal timbre, emotional state, and dynamic prosody--plays a critical role in
conveying nuance beyond mere semantics. Traditional Text-to-Speech (TTS)
systems rely on fixed style labels or inserting a speech prompt to control
these cues, which severely limits flexibility. Recent attempts seek to employ
natural-language instructions to modulate paralinguistic features,
substantially improving the generalization of instruction-driven TTS models.
Although many TTS systems now support customized synthesis via textual
description, their actual ability to interpret and execute complex instructions
remains largely unexplored. In addition, there is still a shortage of
high-quality benchmarks and automated evaluation metrics specifically designed
for instruction-based TTS, which hinders accurate assessment and iterative
optimization of these models. To address these limitations, we introduce
InstructTTSEval, a benchmark for measuring the capability of complex
natural-language style control. We introduce three tasks, namely
Acoustic-Parameter Specification, Descriptive-Style Directive, and Role-Play,
including English and Chinese subsets, each with 1k test cases (6k in total)
paired with reference audio. We leverage Gemini as an automatic judge to assess
their instruction-following abilities. Our evaluation of accessible
instruction-following TTS systems highlights substantial room for further
improvement. We anticipate that InstructTTSEval will drive progress toward more
powerful, flexible, and accurate instruction-following TTS.

</details>


### [33] [Large Language Models in Argument Mining: A Survey](https://arxiv.org/abs/2506.16383)
*Hao Li,Viktor Schlegel,Yizheng Sun,Riza Batista-Navarro,Goran Nenadic*

Main category: cs.CL

TL;DR: 本文系统综述了LLM驱动的论点挖掘技术进展，提出技术分类框架并指出未来研究方向


<details>
  <summary>Details</summary>
Motivation: 大型语言模型对论点挖掘领域产生了范式革新，但缺乏系统性总结和技术路线指导

Method: 通过构建AM子任务分类法，整合提示工程、思维链推理等LLM技术，建立跨领域评估框架

Result: 揭示当前LLM在长文本推理和可解释性方面的瓶颈，提出检索增强和混合架构解决方案

Conclusion: 建议未来重点突破多模态论证建模和持续学习框架，构建动态演化的计算论证生态系统

Abstract: Argument Mining (AM), a critical subfield of Natural Language Processing
(NLP), focuses on extracting argumentative structures from text. The advent of
Large Language Models (LLMs) has profoundly transformed AM, enabling advanced
in-context learning, prompt-based generation, and robust cross-domain
adaptability. This survey systematically synthesizes recent advancements in
LLM-driven AM. We provide a concise review of foundational theories and
annotation frameworks, alongside a meticulously curated catalog of datasets. A
key contribution is our comprehensive taxonomy of AM subtasks, elucidating how
contemporary LLM techniques -- such as prompting, chain-of-thought reasoning,
and retrieval augmentation -- have reconfigured their execution. We further
detail current LLM architectures and methodologies, critically assess
evaluation practices, and delineate pivotal challenges including long-context
reasoning, interpretability, and annotation bottlenecks. Conclusively, we
highlight emerging trends and propose a forward-looking research agenda for
LLM-based computational argumentation, aiming to strategically guide
researchers in this rapidly evolving domain.

</details>


### [34] [HausaNLP at SemEval-2025 Task 11: Advancing Hausa Text-based Emotion Detection](https://arxiv.org/abs/2506.16388)
*Sani Abdullahi Sani,Salim Abubakar,Falalu Ibrahim Lawan,Abdulhamid Abubakar,Maryam Bala*

Main category: cs.CL

TL;DR: 本文提出了一种基于AfriBERTa模型的方法，用于低资源豪萨语的多标签情绪检测，验证准确率达74%。


<details>
  <summary>Details</summary>
Motivation: 探索基于Transformer的模型在资源匮乏语言（豪萨语）中的情绪检测效果，提升低资源语言的情感分析能力。

Method: 对预训练模型AfriBERTa进行微调，结合数据预处理、分词和Hugging Face Trainer API实现分类任务。

Result: 系统在验证集上达到74.00%的准确率和73.50%的F1值。

Conclusion: 证明了基于Transformer的模型在低资源语言情感检测中的有效性，为类似语言任务提供了参考方案。

Abstract: This paper presents our approach to multi-label emotion detection in Hausa, a
low-resource African language, as part of SemEval Track A. We fine-tuned
AfriBERTa, a transformer-based model pre-trained on African languages, to
classify Hausa text into six emotions: anger, disgust, fear, joy, sadness, and
surprise. Our methodology involved data preprocessing, tokenization, and model
fine-tuning using the Hugging Face Trainer API. The system achieved a
validation accuracy of 74.00%, with an F1-score of 73.50%, demonstrating the
effectiveness of transformer-based models for emotion detection in low-resource
languages.

</details>


### [35] [RiOT: Efficient Prompt Refinement with Residual Optimization Tree](https://arxiv.org/abs/2506.16389)
*Chenyi Zhou,Zhengyan Shi,Yuan Yao,Lei Liang,Huajun Chen,Qiang Zhang*

Main category: cs.CL

TL;DR: RiOT框架通过文本梯度和残差连接优化LLM提示，在多个推理基准测试中超越现有方法


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法存在缺乏多样性（限制创新方向探索）和语义漂移（单任务优化影响其他任务性能）两大缺陷

Method: 1. 迭代式文本梯度优化
2. 每步生成多样化语义候选
3. 基于困惑度筛选最优提示
4. 残差连接保留跨迭代有效内容
5. 树形结构管理优化流程

Result: 在常识推理、数学推理、逻辑推理、时序推理和语义推理五个基准测试中，RiOT表现均优于现有自动优化方法和人工提示设计

Conclusion: RiOT通过多样性生成和残差连接机制，有效解决提示优化中的探索局限和语义漂移问题，实现性能与可扩展性的双重提升

Abstract: Recent advancements in large language models (LLMs) have highlighted their
potential across a variety of tasks, but their performance still heavily relies
on the design of effective prompts. Existing methods for automatic prompt
optimization face two challenges: lack of diversity, limiting the exploration
of valuable and innovative directions and semantic drift, where optimizations
for one task can degrade performance in others. To address these issues, we
propose Residual Optimization Tree (RiOT), a novel framework for automatic
prompt optimization. RiOT iteratively refines prompts through text gradients,
generating multiple semantically diverse candidates at each step, and selects
the best prompt using perplexity. Additionally, RiOT incorporates the text
residual connection to mitigate semantic drift by selectively retaining
beneficial content across optimization iterations. A tree structure efficiently
manages the optimization process, ensuring scalability and flexibility.
Extensive experiments across five benchmarks, covering commonsense,
mathematical, logical, temporal, and semantic reasoning, demonstrate that RiOT
outperforms both previous prompt optimization methods and manual prompting.

</details>


### [36] [From LLM-anation to LLM-orchestrator: Coordinating Small Models for Data Labeling](https://arxiv.org/abs/2506.16393)
*Yao Lu,Zhaiyuan Ji,Jiawei Du,Yu Shanqing,Qi Xuan,Tianyi Zhou*

Main category: cs.CL

TL;DR: 提出多模型协作标注框架AutoAnnotator，通过LLM指导SLM自动标注，在降低74%成本的同时提升6.21%准确率


<details>
  <summary>Details</summary>
Motivation: 解决LLMs标注存在的两大瓶颈：1) 大规模API调用成本高昂 2) 细粒度任务（情感/毒性分类）准确率低于领域专用SLMs

Method: 双层架构：1) 元控制器层(LLM)负责模型选择、代码生成和困难样本验证 2) 任务专家层(SLMs)通过投票标注，并通过持续学习策略增强模型泛化

Result: 在零样本/单样本/CoT/多数投票场景下均超越现有LLMs，相比GPT-3.5标注成本降低74.15%且准确率提升6.21%

Conclusion: 开创了LLM与SLM协同标注新范式，为高效低成本自动化标注提供创新解决方案

Abstract: Although the annotation paradigm based on Large Language Models (LLMs) has
made significant breakthroughs in recent years, its actual deployment still has
two core bottlenecks: first, the cost of calling commercial APIs in large-scale
annotation is very expensive; second, in scenarios that require fine-grained
semantic understanding, such as sentiment classification and toxicity
classification, the annotation accuracy of LLMs is even lower than that of
Small Language Models (SLMs) dedicated to this field. To address these
problems, we propose a new paradigm of multi-model cooperative annotation and
design a fully automatic annotation framework AutoAnnotator based on this.
Specifically, AutoAnnotator consists of two layers. The upper-level
meta-controller layer uses the generation and reasoning capabilities of LLMs to
select SLMs for annotation, automatically generate annotation code and verify
difficult samples; the lower-level task-specialist layer consists of multiple
SLMs that perform annotation through multi-model voting. In addition, we use
the difficult samples obtained by the secondary review of the meta-controller
layer as the reinforcement learning set and fine-tune the SLMs in stages
through a continual learning strategy, thereby improving the generalization of
SLMs. Extensive experiments show that AutoAnnotator outperforms existing
open-source/API LLMs in zero-shot, one-shot, CoT, and majority voting settings.
Notably, AutoAnnotator reduces the annotation cost by 74.15% compared to
directly annotating with GPT-3.5-turbo, while still improving the accuracy by
6.21%. Project page: https://github.com/Zhaiyuan-Ji/AutoAnnotator.

</details>


### [37] [OJBench: A Competition Level Code Benchmark For Large Language Models](https://arxiv.org/abs/2506.16395)
*Zhexu Wang,Yiping Liu,Yejie Wang,Wenyang He,Bofei Gao,Muxi Diao,Yanxu Chen,Kelin Fu,Flood Sung,Zhilin Yang,Tianyu Liu,Weiran Xu*

Main category: cs.CL

TL;DR: 提出了OJBench基准测试，用于评估大语言模型在竞赛级编程问题的代码推理能力，发现顶尖模型仍存在明显不足


<details>
  <summary>Details</summary>
Motivation: 现有代码基准测试无法充分评估大语言模型在竞赛级编程问题中的代码推理能力

Method: 构建包含232个NOI/ICPC编程竞赛题的OJBench，对37个闭源/开源模型进行系统评估

Result: Gemini-2.5-pro-exp等顶尖推理模型在高度挑战的竞赛级问题上表现不佳

Conclusion: 竞争级代码推理对当前模型仍存在重大挑战

Abstract: Recent advancements in large language models (LLMs) have demonstrated
significant progress in math and code reasoning capabilities. However, existing
code benchmark are limited in their ability to evaluate the full spectrum of
these capabilities, particularly at the competitive level. To bridge this gap,
we introduce OJBench, a novel and challenging benchmark designed to assess the
competitive-level code reasoning abilities of LLMs. OJBench comprises 232
programming competition problems from NOI and ICPC, providing a more rigorous
test of models' reasoning skills. We conducted a comprehensive evaluation using
OJBench on 37 models, including both closed-source and open-source models,
reasoning-oriented and non-reasoning-oriented models. Our results indicate that
even state-of-the-art reasoning-oriented models, such as o4-mini and
Gemini-2.5-pro-exp, struggle with highly challenging competition-level
problems. This highlights the significant challenges that models face in
competitive-level code reasoning.

</details>


### [38] [NepaliGPT: A Generative Language Model for the Nepali Language](https://arxiv.org/abs/2506.16399)
*Shushanta Pudasaini,Aman Shakya,Siddhartha Shrestha,Sahil Bhatta,Sunil Thapa,Sushmita Palikhe*

Main category: cs.CL

TL;DR: 开发首个尼泊尔语生成大模型NepaliGPT，填补该语言NLP领域空白，提供Devanagari语料库和首个包含4,296问答对的基准数据集。


<details>
  <summary>Details</summary>
Motivation: 当前尼泊尔语缺乏生成式语言模型，导致包括微调在内的下游任务无法开展，阻碍该语言NLP发展。

Method: 收集多源数据构建Devanagari语料库，创建尼泊尔语问答基准数据集，训练专用生成模型NepaliGPT。

Result: 模型取得困惑度26.32、ROUGE-1得分0.2604、因果连贯性81.25%、因果一致性85.41%的性能表现。

Conclusion: NepaliGPT成功填补尼泊尔语生成模型空白，为后续NLP研究奠定数据基础和技术基准。

Abstract: After the release of ChatGPT, Large Language Models (LLMs) have gained huge
popularity in recent days and thousands of variants of LLMs have been released.
However, there is no generative language model for the Nepali language, due to
which other downstream tasks, including fine-tuning, have not been explored
yet. To fill this research gap in the Nepali NLP space, this research proposes
\textit{NepaliGPT}, a generative large language model tailored specifically for
the Nepali language. This research introduces an advanced corpus for the Nepali
language collected from several sources, called the Devanagari Corpus.
Likewise, the research introduces the first NepaliGPT benchmark dataset
comprised of 4,296 question-answer pairs in the Nepali language. The proposed
LLM NepaliGPT achieves the following metrics in text generation: Perplexity of
26.32245, ROUGE-1 score of 0.2604, causal coherence of 81.25\%, and causal
consistency of 85.41\%.

</details>


### [39] [When Does Divide and Conquer Work for Long Context LLM? A Noise Decomposition Framework](https://arxiv.org/abs/2506.16411)
*Zhen Xu,Shang Zhu,Jue Wang,Junlin Wang,Ben Athiwaratkun,Chi Wang,James Zou,Ce Zhang*

Main category: cs.CL

TL;DR: 提出区分长文本处理三大噪声源的理论框架，验证分块聚合策略在LLMs处理长上下文任务中的有效性及适用条件。


<details>
  <summary>Details</summary>
Motivation: 大语言模型处理长文本时存在效率和质量问题，现有方法缺乏对失败模式的系统性归因分析，需探索分块策略的底层原理。

Method: 构建跨块依赖/模型噪声/聚合噪声的三维分析框架，通过检索/问答/摘要任务的对比实验验证分块处理边界条件。

Result: 证实分块策略在超线性噪声场景的优势，展示弱模型+分块组合可超越GPT4o单次处理效果的特殊现象。

Conclusion: 长文本处理效果取决于噪声类型与分块粒度的匹配度，需通过动态分块策略和增强型聚合器提升LLMs长上下文能力。

Abstract: We investigate the challenge of applying Large Language Models (LLMs) to long
texts. We propose a theoretical framework that distinguishes the failure modes
of long context tasks into three categories: cross-chunk dependence (task
noise), confusion that grows with context size (model noise), and the imperfect
integration of partial results (aggregator noise). Under this view, we analyze
when it is effective to use multi-agent chunking, i.e., dividing a length
sequence into smaller chunks and aggregating the processed results of each
chunk. Our experiments on tasks such as retrieval, question answering, and
summarization confirm both the theoretical analysis and the conditions that
favor multi-agent chunking. By exploring superlinear model noise growth with
input length, we also explain why, for large inputs, a weaker model configured
with chunk-based processing can surpass a more advanced model like GPT4o
applied in a single shot. Overall, we present a principled understanding
framework and our results highlight a direct pathway to handling long contexts
in LLMs with carefully managed chunking and aggregator strategies.

</details>


### [40] [REIS: A High-Performance and Energy-Efficient Retrieval System with In-Storage Processing](https://arxiv.org/abs/2506.16444)
*Kangqi Chen,Andreas Kosmas Kakolyris,Rakesh Nadig,Manos Frouzakis,Nika Mansouri Ghiasi,Yu Liang,Haiyu Mao,Jisung Park,Mohammad Sadrosadati,Onur Mutlu*

Main category: cs.CL

TL;DR: 提出REIS存储处理系统，通过优化数据布局和计算资源利用，显著提升RAG检索阶段的性能与能效


<details>
  <summary>Details</summary>
Motivation: 大语言模型受限于静态训练知识，RAG的检索阶段存在存储数据迁移瓶颈，现有ISP方案存在算法适配性差、硬件修改复杂等问题

Method: 1. 嵌入向量与文档关联的数据库布局 2. 跨存储平面分布数据的轻量级FTL技术 3. 利用存储系统现有计算资源的ANNS引擎

Result: 相比服务器级系统，检索性能平均提升13倍，能效提高55倍

Conclusion: REIS通过存储内处理优化，有效解决了RAG检索阶段的瓶颈问题，为LLM知识更新提供了高效解决方案

Abstract: Large Language Models (LLMs) face an inherent challenge: their knowledge is
confined to the data that they have been trained on. To overcome this issue,
Retrieval-Augmented Generation (RAG) complements the static training-derived
knowledge of LLMs with an external knowledge repository. RAG consists of three
stages: indexing, retrieval, and generation. The retrieval stage of RAG becomes
a significant bottleneck in inference pipelines. In this stage, a user query is
mapped to an embedding vector and an Approximate Nearest Neighbor Search (ANNS)
algorithm searches for similar vectors in the database to identify relevant
items. Due to the large database sizes, ANNS incurs significant data movement
overheads between the host and the storage system. To alleviate these
overheads, prior works propose In-Storage Processing (ISP) techniques that
accelerate ANNS by performing computations inside storage. However, existing
works that leverage ISP for ANNS (i) employ algorithms that are not tailored to
ISP systems, (ii) do not accelerate data retrieval operations for data selected
by ANNS, and (iii) introduce significant hardware modifications, limiting
performance and hindering their adoption. We propose REIS, the first ISP system
tailored for RAG that addresses these limitations with three key mechanisms.
First, REIS employs a database layout that links database embedding vectors to
their associated documents, enabling efficient retrieval. Second, it enables
efficient ANNS by introducing an ISP-tailored data placement technique that
distributes embeddings across the planes of the storage system and employs a
lightweight Flash Translation Layer. Third, REIS leverages an ANNS engine that
uses the existing computational resources inside the storage system. Compared
to a server-grade system, REIS improves the performance (energy efficiency) of
retrieval by an average of 13x (55x).

</details>


### [41] [StoryWriter: A Multi-Agent Framework for Long Story Generation](https://arxiv.org/abs/2506.16445)
*Haotian Xia,Hao Peng,Yunjia Qi,Xiaozhi Wang,Bin Xu,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 提出StoryWriter多代理框架解决长故事生成的连贯性和复杂性挑战，通过三阶段流程生成优质长文本，并创建高质量数据集


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在长故事生成中存在篇章连贯性（情节一致性、逻辑连贯性）和叙事复杂性（交织叙事）的双重挑战

Method: 1. 提纲代理生成事件驱动的结构化大纲
2. 规划代理细化事件并分配章节
3. 写作代理动态压缩历史上下文生成新内容

Result: 人类/自动评估均超越基线模型（质量+长度），构建含6000+故事的数据集（平均8000词），训练出StoryWriter_GLM等高性能模型

Conclusion: StoryWriter框架有效解决长文本生成难题，生成质量显著提升，并通过数据集训练实现实际应用价值

Abstract: Long story generation remains a challenge for existing large language models
(LLMs), primarily due to two main factors: (1) discourse coherence, which
requires plot consistency, logical coherence, and completeness in the long-form
generation, and (2) narrative complexity, which requires an interwoven and
engaging narrative. To address these challenges, we propose StoryWriter, a
multi-agent story generation framework, which consists of three main modules:
(1) outline agent, which generates event-based outlines containing rich event
plots, character, and event-event relationships. (2) planning agent, which
further details events and plans which events should be written in each chapter
to maintain an interwoven and engaging story. (3) writing agent, which
dynamically compresses the story history based on the current event to generate
and reflect new plots, ensuring the coherence of the generated story. We
conduct both human and automated evaluation, and StoryWriter significantly
outperforms existing story generation baselines in both story quality and
length. Furthermore, we use StoryWriter to generate a dataset, which contains
about $6,000$ high-quality long stories, with an average length of $8,000$
words. We train the model Llama3.1-8B and GLM4-9B using supervised fine-tuning
on LongStory and develop StoryWriter_GLM and StoryWriter_GLM, which
demonstrates advanced performance in long story generation.

</details>


### [42] [Towards Generalizable Generic Harmful Speech Datasets for Implicit Hate Speech Detection](https://arxiv.org/abs/2506.16476)
*Saad Almohaimeed,Saleh Almohaimeed,Damla Turgut,Ladislau Bölöni*

Main category: cs.CL

TL;DR: 论文提出通过利用现有有害言论数据集改进隐式仇恨检测，方法包含影响力样本识别、重新标注和大模型增强，实验显示F1分数提升12.9个点。


<details>
  <summary>Details</summary>
Motivation: 现有数据集未充分利用隐式仇恨特征，且存在标注错误和主观偏差，需要提升检测模型的跨数据集泛化能力。

Method: 三阶段方法：1）识别有影响力的样本 2）人工重新标注 3）使用Llama-3 70B和GPT-4o进行数据增强

Result: 实验结果显示方法显著提升检测效果，相比基线F1分数提高+12.9，验证了跨数据集泛化能力。

Conclusion: 通过挖掘现有数据集潜力和结合大模型技术，可有效提升隐式仇恨检测性能，具有实际应用价值。

Abstract: Implicit hate speech has recently emerged as a critical challenge for social
media platforms. While much of the research has traditionally focused on
harmful speech in general, the need for generalizable techniques to detect
veiled and subtle forms of hate has become increasingly pressing. Based on
lexicon analysis, we hypothesize that implicit hate speech is already present
in publicly available harmful speech datasets but may not have been explicitly
recognized or labeled by annotators. Additionally, crowdsourced datasets are
prone to mislabeling due to the complexity of the task and often influenced by
annotators' subjective interpretations. In this paper, we propose an approach
to address the detection of implicit hate speech and enhance generalizability
across diverse datasets by leveraging existing harmful speech datasets. Our
method comprises three key components: influential sample identification,
reannotation, and augmentation using Llama-3 70B and GPT-4o. Experimental
results demonstrate the effectiveness of our approach in improving implicit
hate detection, achieving a +12.9-point F1 score improvement compared to the
baseline.

</details>


### [43] [Relic: Enhancing Reward Model Generalization for Low-Resource Indic Languages with Few-Shot Examples](https://arxiv.org/abs/2506.16502)
*Soumya Suvra Ghosal,Vaibhav Singh,Akash Ghosh,Soumyabrata Pal,Subhadip Baidya,Sriparna Saha,Dinesh Manocha*

Main category: cs.CL

TL;DR: 提出RELIC框架，通过上下文学习提升低资源印度语言奖励模型的准确性


<details>
  <summary>Details</summary>
Motivation: 现有开源多语言奖励模型主要依赖高资源语言偏好数据，导致对低资源印度语言的奖励信号不可靠，而收集大规模高质量偏好数据成本过高

Method: 训练检索器通过成对排序目标，从高资源语言中选取最能区分优劣响应的上下文示例

Result: 在三个数据集上的实验显示RELIC显著提升准确率（如博多语提升12.81%），优于现有方法

Conclusion: RELIC为低资源语言奖励建模提供了高效解决方案，无需昂贵数据标注，具有实际应用价值

Abstract: Reward models are essential for aligning large language models (LLMs) with
human preferences. However, most open-source multilingual reward models are
primarily trained on preference datasets in high-resource languages, resulting
in unreliable reward signals for low-resource Indic languages. Collecting
large-scale, high-quality preference data for these languages is prohibitively
expensive, making preference-based training approaches impractical. To address
this challenge, we propose RELIC, a novel in-context learning framework for
reward modeling in low-resource Indic languages. RELIC trains a retriever with
a pairwise ranking objective to select in-context examples from auxiliary
high-resource languages that most effectively highlight the distinction between
preferred and less-preferred responses. Extensive experiments on three
preference datasets- PKU-SafeRLHF, WebGPT, and HH-RLHF-using state-of-the-art
open-source reward models demonstrate that RELIC significantly improves reward
model accuracy for low-resource Indic languages, consistently outperforming
existing example selection methods. For example, on Bodo-a low-resource Indic
language-using a LLaMA-3.2-3B reward model, RELIC achieves a 12.81% and 10.13%
improvement in accuracy over zero-shot prompting and state-of-the-art example
selection method, respectively.

</details>


### [44] [Automatic Speech Recognition Biases in Newcastle English: an Error Analysis](https://arxiv.org/abs/2506.16558)
*Dana Serditova,Kevin Tang,Jochen Steffens*

Main category: cs.CL

TL;DR: 研究揭示自动语音识别系统存在方言偏见，方言特征（而非社会因素）是识别错误主因，建议训练数据增加方言多样性并采用社会语言学分析方法


<details>
  <summary>Details</summary>
Motivation: 现有ASR系统因偏向主流口音训练导致方言识别效果差，先前研究多关注种族/年龄/性别偏见，而地域方言偏见研究不足。本文选取纽卡斯尔方言这一典型样本探究方言特征对ASR性能的影响

Method: 两阶段研究：1. 对子样本进行人工错误分析，识别音系/词汇/形态句法层面的错误模式；2. 聚焦方言代词'yous'和'wor'开展系统化ASR识别案例研究

Result: ASR错误率与方言特征直接相关（如地域特有发音/词汇/语法结构），社会因素（如说话者年龄/性别）对识别错误影响较小

Conclusion: 呼吁在ASR训练数据中纳入更多方言多样性，强调社会语言学分析方法在诊断和消除地域偏见中的关键价值

Abstract: Automatic Speech Recognition (ASR) systems struggle with regional dialects
due to biased training which favours mainstream varieties. While previous
research has identified racial, age, and gender biases in ASR, regional bias
remains underexamined. This study investigates ASR performance on Newcastle
English, a well-documented regional dialect known to be challenging for ASR. A
two-stage analysis was conducted: first, a manual error analysis on a subsample
identified key phonological, lexical, and morphosyntactic errors behind ASR
misrecognitions; second, a case study focused on the systematic analysis of ASR
recognition of the regional pronouns ``yous'' and ``wor''. Results show that
ASR errors directly correlate with regional dialectal features, while social
factors play a lesser role in ASR mismatches. We advocate for greater dialectal
diversity in ASR training data and highlight the value of sociolinguistic
analysis in diagnosing and addressing regional biases.

</details>


### [45] [Weight Factorization and Centralization for Continual Learning in Speech Recognition](https://arxiv.org/abs/2506.16574)
*Enes Yavuz Ugan,Ngoc-Quan Pham,Alexander Waibel*

Main category: cs.CL

TL;DR: 提出受人类醒睡周期启发的两阶段持续学习方法（因式分解+中枢化），通过分散式低秩适配器累积知识，有效防止多语种语音识别模型的灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 解决语音识别模型在无原始训练数据、多语种场景下持续学习时产生的灾难性遗忘问题。传统方法难以在无排练、语言不可知的条件下稳定更新模型。

Method: 1. 因式分解阶段：解耦知识学习
2. 中枢化阶段：通过多散射低秩适配器进行知识融合
模仿人脑醒睡周期中知识巩固的机制

Result: 在连续变化的语码转换数据集实验中，中枢化阶段通过分散式低秩适配器有效积累知识，验证了该方法防止灾难性遗忘的有效性。

Conclusion: 脑科学启发的双阶段持续学习机制为多语种语音模型的稳定更新提供了新范式，无需全模型重训练即可保持知识完整性。

Abstract: Modern neural network based speech recognition models are required to
continually absorb new data without re-training the whole system, especially in
downstream applications using foundation models, having no access to the
original training data. Continually training the models in a rehearsal-free,
multilingual, and language agnostic condition, likely leads to catastrophic
forgetting, when a seemingly insignificant disruption to the weights can
destructively harm the quality of the models. Inspired by the ability of human
brains to learn and consolidate knowledge through the waking-sleeping cycle, we
propose a continual learning approach with two distinct phases: factorization
and centralization, learning and merging knowledge accordingly. Our experiments
on a sequence of varied code-switching datasets showed that the centralization
stage can effectively prevent catastrophic forgetting by accumulating the
knowledge in multiple scattering low-rank adapters.

</details>


### [46] [Streaming Non-Autoregressive Model for Accent Conversion and Pronunciation Improvement](https://arxiv.org/abs/2506.16580)
*Tuan-Nam Nguyen,Ngoc-Quan Pham,Seymanur Akti,Alexander Waibel*

Main category: cs.CL

TL;DR: 首个流式口音转换模型，通过Emformer编码器和TTS数据生成实现实时处理与低延迟


<details>
  <summary>Details</summary>
Motivation: 解决传统口音转换模型无法实时流式处理的问题，提升发音纠正的实际应用性

Method: 1. 采用Emformer编码器实现流处理 2. 优化推理机制 3. 集成TTS生成理想训练数据

Result: 在保持说话人特征的同时达到与顶级模型相当的性能，延迟稳定在流式处理要求范围内

Conclusion: 首个支持流式处理的口音转换系统，为实时语音处理应用开辟新可能性

Abstract: We propose a first streaming accent conversion (AC) model that transforms
non-native speech into a native-like accent while preserving speaker identity,
prosody and improving pronunciation. Our approach enables stream processing by
modifying a previous AC architecture with an Emformer encoder and an optimized
inference mechanism. Additionally, we integrate a native text-to-speech (TTS)
model to generate ideal ground-truth data for efficient training. Our streaming
AC model achieves comparable performance to the top AC models while maintaining
stable latency, making it the first AC system capable of streaming.

</details>


### [47] [Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework](https://arxiv.org/abs/2506.16584)
*Nadav Kunievsky,James A. Evans*

Main category: cs.CL

TL;DR: 论文提出通过分解模型响应变异性（用户意图/表达方式/模型不稳定性）的评估框架，量化大语言模型世界模型的稳健性。实验表明大模型更能识别用户核心意图，但改进不均衡且优势有限。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型是否具备稳健的世界模型（结构化理解能力），这对保障高风险应用中的可靠性至关重要。传统准确率基准无法有效检测模型是否真正理解语义。

Method: 提出将模型响应变异性分解为三个维度：用户核心意图变化、表达方式变化、模型自身不稳定性。通过量化不同维度贡献比例，评估模型响应是否基于语义理解。

Result: 较大模型62%的响应变化归因于用户意图改变（较小模型51%），但领域表现不均衡（如代码任务改进仅3%），且模型不稳定因素仍占22%变异性。

Conclusion: 需建立超越准确率的语义诊断框架，直接评估模型内部世界模型的稳定性。该方法为模型可靠性评估提供了新的结构化分析范式。

Abstract: Understanding whether large language models (LLMs) possess a world model-a
structured understanding of the world that supports generalization beyond
surface-level patterns-is central to assessing their reliability, especially in
high-stakes applications. We propose a formal framework for evaluating whether
an LLM exhibits a sufficiently robust world model, defined as producing
consistent outputs across semantically equivalent prompts while distinguishing
between prompts that express different intents. We introduce a new evaluation
approach to measure this that decomposes model response variability into three
components: variability due to user purpose, user articulation, and model
instability. An LLM with a strong world model should attribute most of the
variability in its responses to changes in foundational purpose rather than
superficial changes in articulation. This approach allows us to quantify how
much of a model's behavior is semantically grounded rather than driven by model
instability or alternative wording. We apply this framework to evaluate LLMs
across diverse domains. Our results show how larger models attribute a greater
share of output variability to changes in user purpose, indicating a more
robust world model. This improvement is not uniform, however: larger models do
not consistently outperform smaller ones across all domains, and their
advantage in robustness is often modest. These findings highlight the
importance of moving beyond accuracy-based benchmarks toward semantic
diagnostics that more directly assess the structure and stability of a model's
internal understanding of the world.

</details>


### [48] [A Scoping Review of Synthetic Data Generation for Biomedical Research and Applications](https://arxiv.org/abs/2506.16594)
*Hanshu Rao,Weisi Liu,Haohan Wang,I-Chan Huang,Zhe He,Xiaolei Huang*

Main category: cs.CL

TL;DR: This scoping review analyzes 59 studies (2020-2025) on LLM-based synthetic data generation in biomedicine, identifying trends in clinical applications, methodologies (prompting/fine-tuning), and heterogeneous evaluation approaches.


<details>
  <summary>Details</summary>
Motivation: Address data scarcity/privacy/quality challenges in biomedicine through systematic analysis of synthetic data generation methods powered by large language models.

Method: PRISMA-ScR guided analysis of multimodal data sources (unstructured texts 78%, tabular 13.6%, multimodal 8.4%) using prompting (72.9%), fine-tuning (22%), and specialized models (5.1%).

Result: Reveals evaluation heterogeneity: 27.1% intrinsic metrics, 55.9% human-in-loop assessments, 13.6% LLM-based evaluations. Identifies clinical adaptation challenges and resource accessibility issues.

Conclusion: Highlights critical need for standardization in evaluation methods and improved clinical domain adaptability to fully leverage synthetic data generation in biomedical research.

Abstract: Synthetic data generation--mitigating data scarcity, privacy concerns, and
data quality challenges in biomedical fields--has been facilitated by rapid
advances of large language models (LLMs). This scoping review follows
PRISMA-ScR guidelines and synthesizes 59 studies, published between 2020 and
2025 and collected from PubMed, ACM, Web of Science, and Google Scholar. The
review systematically examines biomedical research and application trends in
synthetic data generation, emphasizing clinical applications, methodologies,
and evaluations. Our analysis identifies data modalities of unstructured texts
(78.0%), tabular data (13.6%), and multimodal sources (8.4%); generation
methods of prompting (72.9%), fine-tuning (22.0%) LLMs and specialized model
(5.1%); and heterogeneous evaluations of intrinsic metrics (27.1%),
human-in-the-loop assessments (55.9%), and LLM-based evaluations (13.6%). The
analysis addresses current limitations in what, where, and how health
professionals can leverage synthetic data generation for biomedical domains.
Our review also highlights challenges in adaption across clinical domains,
resource and model accessibility, and evaluation standardizations.

</details>


### [49] [Modeling Public Perceptions of Science in Media](https://arxiv.org/abs/2506.16622)
*Jiaxin Pei,Dustin Wright,Isabelle Augenstin,David Jurgens*

Main category: cs.CL

TL;DR: 提出计算框架和数据集分析公众科学感知，发现科学新闻消费频率是感知主要驱动因素，感知分数能有效预测公众参与度


<details>
  <summary>Details</summary>
Motivation: 解决科学传播者难以预测公众对科学信息反应的痛点，通过建模公众感知维度帮助优化传播策略

Method: 构建12维感知计算框架，创建含10,489标注的大规模数据集，开发高性能NLP预测模型，结合Reddit自然实验分析

Result: 科学新闻消费频率显著影响感知（人口因素影响弱），感知分数与帖子互动量正相关（r=0.76,p<0.01）

Conclusion: 通过感知建模可有效预测公众参与，为科学传播提供基于数据驱动的新路径，强调多维度感知分析的重要性

Abstract: Effectively engaging the public with science is vital for fostering trust and
understanding in our scientific community. Yet, with an ever-growing volume of
information, science communicators struggle to anticipate how audiences will
perceive and interact with scientific news. In this paper, we introduce a
computational framework that models public perception across twelve dimensions,
such as newsworthiness, importance, and surprisingness. Using this framework,
we create a large-scale science news perception dataset with 10,489 annotations
from 2,101 participants from diverse US and UK populations, providing valuable
insights into public responses to scientific information across domains. We
further develop NLP models that predict public perception scores with a strong
performance. Leveraging the dataset and model, we examine public perception of
science from two perspectives: (1) Perception as an outcome: What factors
affect the public perception of scientific information? (2) Perception as a
predictor: Can we use the estimated perceptions to predict public engagement
with science? We find that individuals' frequency of science news consumption
is the driver of perception, whereas demographic factors exert minimal
influence. More importantly, through a large-scale analysis and carefully
designed natural experiment on Reddit, we demonstrate that the estimated public
perception of scientific information has direct connections with the final
engagement pattern. Posts with more positive perception scores receive
significantly more comments and upvotes, which is consistent across different
scientific information and for the same science, but are framed differently.
Overall, this research underscores the importance of nuanced perception
modeling in science communication, offering new pathways to predict public
interest and engagement with scientific content.

</details>


### [50] [Initial Investigation of LLM-Assisted Development of Rule-Based Clinical NLP System](https://arxiv.org/abs/2506.16628)
*Jianlin Shi,Brian T. Bucher*

Main category: cs.CL

TL;DR: 提出利用大语言模型优化临床规则NLP系统开发，实现高召回率（片段识别0.98-0.99，NER关键术语提取1.0）的半自动化方案


<details>
  <summary>Details</summary>
Motivation: 传统规则系统在临床场景因可解释性被保留，但人工开发维护成本高，尤其面对语言变异性任务时效率低下

Method: 在规则系统开发阶段引入LLMs，聚焦临床笔记片段检索与NER关键词提取两个核心环节进行实验验证

Result: Deepseek/Qwen的临床片段召回率达0.98/0.99，NER关键术语提取准确率1.0，显著优于传统人工开发

Conclusion: 为NLP开发开辟新范式，结合规则系统优势与LLMs自动化潜力，实现更高效、低成本且透明的临床文本处理方案

Abstract: Despite advances in machine learning (ML) and large language models (LLMs),
rule-based natural language processing (NLP) systems remain active in clinical
settings due to their interpretability and operational efficiency. However,
their manual development and maintenance are labor-intensive, particularly in
tasks with large linguistic variability. To overcome these limitations, we
proposed a novel approach employing LLMs solely during the rule-based systems
development phase. We conducted the initial experiments focusing on the first
two steps of developing a rule-based NLP pipeline: find relevant snippets from
the clinical note; extract informative keywords from the snippets for the
rule-based named entity recognition (NER) component. Our experiments
demonstrated exceptional recall in identifying clinically relevant text
snippets (Deepseek: 0.98, Qwen: 0.99) and 1.0 in extracting key terms for NER.
This study sheds light on a promising new direction for NLP development,
enabling semi-automated or automated development of rule-based systems with
significantly faster, more cost-effective, and transparent execution compared
with deep learning model-based solutions.

</details>


### [51] [GeoGuess: Multimodal Reasoning based on Hierarchy of Visual Information in Street View](https://arxiv.org/abs/2506.16633)
*Fenghua Cheng,Jinxiang Wang,Sen Wang,Zi Huang,Xue Li*

Main category: cs.CL

TL;DR: 提出了GeoGuess多模态推理任务，通过街景图像定位地理位置并生成解释，配套GeoExplain数据集和SightSense推理方法。


<details>
  <summary>Details</summary>
Motivation: 现有评估多模态推理能力的任务缺乏对层次化视觉线索（局部细节/全局上下文）与知识库关联的联合推理能力验证，而这是现实场景的关键需求。

Method: 1. 构建GeoExplain数据集（全景图-地理坐标-解释三元组）
2. 提出SightSense多模态多级推理框架，整合视觉层次化信息与外部地理知识

Result: 实验证明SightSense在GeoGuess任务中能实现精确定位并生成全面解释，验证了层次化视觉推理的有效性

Conclusion: GeoGuess为评估复杂多模态推理能力提供了新基准，SightSense框架展示了层次化信息整合在空间认知任务中的重要性

Abstract: Multimodal reasoning is a process of understanding, integrating and inferring
information across different data modalities. It has recently attracted surging
academic attention as a benchmark for Artificial Intelligence (AI). Although
there are various tasks for evaluating multimodal reasoning ability, they still
have limitations. Lack of reasoning on hierarchical visual clues at different
levels of granularity, e.g., local details and global context, is of little
discussion, despite its frequent involvement in real scenarios. To bridge the
gap, we introduce a novel and challenging task for multimodal reasoning, namely
GeoGuess. Given a street view image, the task is to identify its location and
provide a detailed explanation. A system that succeeds in GeoGuess should be
able to detect tiny visual clues, perceive the broader landscape, and associate
with vast geographic knowledge. Therefore, GeoGuess would require the ability
to reason between hierarchical visual information and geographic knowledge. In
this work, we establish a benchmark for GeoGuess by introducing a specially
curated dataset GeoExplain which consists of
panoramas-geocoordinates-explanation tuples. Additionally, we present a
multimodal and multilevel reasoning method, namely SightSense which can make
prediction and generate comprehensive explanation based on hierarchy of visual
information and external knowledge. Our analysis and experiments demonstrate
their outstanding performance in GeoGuess.

</details>


### [52] [Long-Context Generalization with Sparse Attention](https://arxiv.org/abs/2506.16640)
*Pavlo Vasylenko,Marcos Treviso,André F. T. Martins*

Main category: cs.CL

TL;DR: 用自适应可扩展Entmax（ASEntmax）替代传统softmax注意力，通过稀疏注意力机制和优化位置编码显著提升长上下文泛化能力


<details>
  <summary>Details</summary>
Motivation: 传统softmax在长序列任务中因非关键token累积导致注意力分散和表示崩溃，需稀疏注意力机制精确聚焦固定模式

Method: 提出ASEntmax：在α-entmax基础上引入可学习温度参数，实现稀疏/密集注意力调节；结合特定位置编码设计增强模式定位能力

Result: 集成ASEntmax和优化位置编码的模型在长上下文泛化任务上大幅优于softmax、可扩展softmax及固定温度α-entmax基线

Conclusion: 通过动态稀疏注意力机制与位置编码协同优化，有效提升Transformer在需要精准模式捕捉的长序列任务中的性能

Abstract: Transformer-based architectures traditionally employ softmax to compute
attention weights, which produces dense distributions over all tokens in a
sequence. While effective in many settings, this density has been shown to be
detrimental for tasks that demand precise focus on fixed-size patterns: as
sequence length increases, non-informative tokens accumulate attention
probability mass, leading to dispersion and representational collapse. We show
in this paper that sparse attention mechanisms using $\alpha$-entmax can avoid
these issues, due to their ability to assign exact zeros to irrelevant tokens.
Furthermore, we introduce Adaptive-Scalable Entmax (ASEntmax), which endows
$\alpha$-entmax with a learnable temperature parameter, allowing the attention
distribution to interpolate between sparse (pattern-focused) and dense
(softmax-like) regimes. Finally, we show that the ability to locate and
generalize fixed-size patterns can be further improved through a careful design
of position encodings, which impacts both dense and sparse attention methods.
By integrating ASEntmax into standard transformer layers alongside proper
positional encodings, we show that our models greatly outperform softmax,
scalable softmax, and fixed-temperature $\alpha$-entmax baselines on
long-context generalization.

</details>


### [53] [Arch-Router: Aligning LLM Routing with Human Preferences](https://arxiv.org/abs/2506.16655)
*Co Tran,Salman Paracha,Adil Hafeez,Shuguang Chen*

Main category: cs.CL

TL;DR: 提出了偏好对齐的路由框架Arch-Router，通过1.5B紧凑模型实现查询到领域-动作偏好的映射，支持动态扩展模型池且无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有LLM路由方法存在基准测试无法反映人类主观偏好、模型选择范围有限两大缺陷，难以满足实际应用需求。

Method: 基于领域-动作类型偏好匹配的框架设计，采用可动态添加新模型的模块化架构，通过轻量级Arch-Router模型实现路由决策。

Result: 在对话数据集上达到SOTA性能，超越顶级商业模型，同时实现更透明的偏好匹配和灵活的路由扩展能力。

Conclusion: 该框架有效捕捉主观评价标准，通过开源模型提供可解释、可扩展的LLM路由解决方案。

Abstract: With the rapid proliferation of large language models (LLMs) -- each
optimized for different strengths, style, or latency/cost profile -- routing
has become an essential technique to operationalize the use of different
models. However, existing LLM routing approaches are limited in two key ways:
they evaluate performance using benchmarks that often fail to capture human
preferences driven by subjective evaluation criteria, and they typically select
from a limited pool of models. In this work, we propose a preference-aligned
routing framework that guides model selection by matching queries to
user-defined domains (e.g., travel) or action types (e.g., image editing) --
offering a practical mechanism to encode preferences in routing decisions.
Specifically, we introduce \textbf{Arch-Router}, a compact 1.5B model that
learns to map queries to domain-action preferences for model routing decisions.
Our approach also supports seamlessly adding new models for routing without
requiring retraining or architectural modifications. Experiments on
conversational datasets demonstrate that our approach achieves state-of-the-art
(SOTA) results in matching queries with human preferences, outperforming top
proprietary models. Our approach captures subjective evaluation criteria and
makes routing decisions more transparent and flexible. Our model is available
at: \texttt{https://huggingface.co/katanemo/Arch-Router-1.5B}.

</details>


### [54] [Mechanisms vs. Outcomes: Probing for Syntax Fails to Explain Performance on Targeted Syntactic Evaluations](https://arxiv.org/abs/2506.16678)
*Ananth Agarwal,Jasper Jian,Christopher D. Manning,Shikhar Murty*

Main category: cs.CL

TL;DR: 大语言模型内部通过探针提取的句法特征与下游实际句法表现存在显著脱节


<details>
  <summary>Details</summary>
Motivation: 探究LLM在文本处理中展现的句法掌握能力是否真正反映其内在的句法表征机制

Method: 采用'机制与结果'框架评估32个开源Transformer模型，对比探针提取特征与目标句法评估结果

Result: 英语语言现象中探针提取的句法特征无法有效预测模型在具体句法任务中的表现

Conclusion: 单纯依赖探针技术获取的潜在句法表征不能可靠解释模型的实际句法行为，需开发更全面的分析方法

Abstract: Large Language Models (LLMs) exhibit a robust mastery of syntax when
processing and generating text. While this suggests internalized understanding
of hierarchical syntax and dependency relations, the precise mechanism by which
they represent syntactic structure is an open area within interpretability
research. Probing provides one way to identify the mechanism of syntax being
linearly encoded in activations, however, no comprehensive study has yet
established whether a model's probing accuracy reliably predicts its downstream
syntactic performance. Adopting a "mechanisms vs. outcomes" framework, we
evaluate 32 open-weight transformer models and find that syntactic features
extracted via probing fail to predict outcomes of targeted syntax evaluations
across English linguistic phenomena. Our results highlight a substantial
disconnect between latent syntactic representations found via probing and
observable syntactic behaviors in downstream tasks.

</details>


### [55] [LegiGPT: Party Politics and Transport Policy with Large Language Model](https://arxiv.org/abs/2506.16692)
*Hyunsoo Yun,Eun Hak Lee*

Main category: cs.CL

TL;DR: 研究者开发LegiGPT框架，结合LLM与XAI技术分析韩国立法数据，发现党派属性、选区规模等关键因素显著影响交通政策制定。


<details>
  <summary>Details</summary>
Motivation: 探究立法者政治意识形态对交通政策制定的影响机制，为立法决策提供可解释性分析工具。

Method: 1. 使用GPT-4对立法提案进行多阶段关键词-短语-语境过滤分类
2. 应用XAI技术分析党派属性与提案关联性

Result: 保守/进步派提案人比例、选区规模及选举人口是核心决定因素，两党通过提案发起/支持等不同模式推动跨党派立法。

Conclusion: LegiGPT为政策动态分析提供新范式，对优化基础设施治理具有实践价值，揭示了数据驱动型政策制定的潜力。

Abstract: Given the significant influence of lawmakers' political ideologies on
legislative decision-making, understanding their impact on policymaking is
critically important. We introduce a novel framework, LegiGPT, which integrates
a large language model (LLM) with explainable artificial intelligence (XAI) to
analyze transportation-related legislative proposals. LegiGPT employs a
multi-stage filtering and classification pipeline using zero-shot prompting
with GPT-4. Using legislative data from South Korea's 21st National Assembly,
we identify key factors - including sponsor characteristics, political
affiliations, and geographic variables - that significantly influence
transportation policymaking. The LLM was used to classify
transportation-related bill proposals through a stepwise filtering process
based on keywords, phrases, and contextual relevance. XAI techniques were then
applied to examine relationships between party affiliation and associated
attributes. The results reveal that the number and proportion of conservative
and progressive sponsors, along with district size and electoral population,
are critical determinants shaping legislative outcomes. These findings suggest
that both parties contributed to bipartisan legislation through different forms
of engagement, such as initiating or supporting proposals. This integrated
approach provides a valuable tool for understanding legislative dynamics and
guiding future policy development, with broader implications for infrastructure
planning and governance.

</details>


### [56] [ReasonGRM: Enhancing Generative Reward Models through Large Reasoning Models](https://arxiv.org/abs/2506.16712)
*Bin Chen,Xinzge Gao,Chuanrui Hu,Penghang Yu,Hua Zhang,Bing-Kun Bao*

Main category: cs.CL

TL;DR: ReasonGRM通过三阶段训练框架（Zero-RL生成路径+R*评分+强化学习微调），有效提升生成式奖励模型的推理质量，在三大基准测试中实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成式奖励模型（GRMs）因推理能力不足导致关键信息遗漏和幻觉问题，需要系统性提升推理路径质量以实现可靠偏好建模。

Method: 1. Zero-RL生成结果导向的简洁推理路径 2. 引入R*指标筛选低探索高正确率的路径 3. 强化学习微调提升困难样本判别能力

Result: 在公开基准上平均超越最佳GRMs 1.8%，最高超过GPT-4o达5.6%，验证推理感知训练的有效性。

Conclusion: 高质量推理路径选择是提升偏好建模可靠性的关键，ReasonGRM通过分阶段优化路径质量显著提升模型性能。

Abstract: Generative Reward Models (GRMs) provide greater flexibility than scalar
reward models in capturing human preferences, but their effectiveness is
limited by poor reasoning capabilities. This often results in incomplete or
overly speculative reasoning paths, leading to hallucinations or missing key
information in complex tasks. We address this challenge with ReasonGRM, a
three-stage generative reward modeling framework. In the first stage, Zero-RL
is used to generate concise, outcome-directed reasoning paths that reduce the
likelihood of critical omissions. In the second stage, we introduce a novel
evaluation metric, $R^\star$, which scores reasoning paths based on their
generation likelihood. This favors paths that reach correct answers with
minimal exploration, helping to reduce hallucination-prone data during
training. In the final stage, the model is further refined through
reinforcement learning on challenging examples to enhance its preference
discrimination capabilities. Experiments on three public benchmarks show that
ReasonGRM achieves competitive or state-of-the-art performance, outperforming
previous best GRMs by 1.8\% on average and surpassing proprietary models such
as GPT-4o by up to 5.6\%. These results demonstrate the effectiveness of
reasoning-aware training and highlight the importance of high-quality rationale
selection for reliable preference modeling.

</details>


### [57] [The Role of Model Confidence on Bias Effects in Measured Uncertainties](https://arxiv.org/abs/2506.16724)
*Xinyi Liu,Weiguang Wang,Hangfeng He*

Main category: cs.CL

TL;DR: 研究通过减轻提示偏差改进LLMs认知不确定性量化，发现低置信度时偏差对不确定性影响更大且导致过度自信


<details>
  <summary>Details</summary>
Motivation: 开放式任务中LLMs认知不确定性评估至关重要，但受随机不确定性干扰。研究探索提示偏差对两种不确定性量化的复杂影响机制

Method: 在VQA任务上实验，分析GPT-4o和Qwen2-VL模型在不同无偏置信度下，提示偏差对认知/随机不确定性的量化影响

Result: 降低提示偏差提升GPT-4o不确定性量化；低置信度时偏差对两种不确定性影响更大，且导致认知不确定性被显著低估（过度自信）

Conclusion: 揭示了偏差对不确定性量化的差异化影响机制，为开发更可靠的LLMs不确定性量化方法提供理论依据

Abstract: With the growing adoption of Large Language Models (LLMs) for open-ended
tasks, accurately assessing epistemic uncertainty, which reflects a model's
lack of knowledge, has become crucial to ensuring reliable outcomes. However,
quantifying epistemic uncertainty in such tasks is challenging due to the
presence of aleatoric uncertainty, which arises from multiple valid answers.
While bias can introduce noise into epistemic uncertainty estimation, it may
also reduce noise from aleatoric uncertainty. To investigate this trade-off, we
conduct experiments on Visual Question Answering (VQA) tasks and find that
mitigating prompt-introduced bias improves uncertainty quantification in
GPT-4o. Building on prior work showing that LLMs tend to copy input information
when model confidence is low, we further analyze how these prompt biases affect
measured epistemic and aleatoric uncertainty across varying bias-free
confidence levels with GPT-4o and Qwen2-VL. We find that all considered biases
induce greater changes in both uncertainties when bias-free model confidence is
lower. Moreover, lower bias-free model confidence leads to greater
underestimation of epistemic uncertainty (i.e. overconfidence) due to bias,
whereas it has no significant effect on the direction of changes in aleatoric
uncertainty estimation. These distinct effects deepen our understanding of bias
mitigation for uncertainty quantification and potentially inform the
development of more advanced techniques.

</details>


### [58] [LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization](https://arxiv.org/abs/2506.16738)
*Daejin Jo,Jeeyoung Yun,Byungseok Roh,Sungwoong Kim*

Main category: cs.CL

TL;DR: 提出LM-SPT语音标记化方法，通过语义蒸馏和架构改进实现更短的语音标记序列，在保持语义对齐的同时提升语音语言模型效率


<details>
  <summary>Details</summary>
Motivation: 现有语音标记方法生成的序列长度显著长于文本，影响语音语言建模效率。直接降低帧率会破坏语义结构

Method: 1. 通过重构语音并最小化原始/重构波形在ASR编码器中的表征差异实现语义蒸馏
2. 改进编码器-解码器架构
3. 支持多帧率（25/12.5/6.25Hz）

Result: 相比基线：
- 实现更好的重建保真度
- 语音-文本任务表现相当
- 文本-语音任务持续优于基线

Conclusion: LM-SPT通过数据驱动的间接监督机制，有效学习语言模型对齐的离散单元，提升跨模态统一建模效果

Abstract: With the rapid progress of speech language models (SLMs), discrete speech
tokens have emerged as a core interface between speech and text, enabling
unified modeling across modalities. Recent speech tokenization approaches aim
to isolate semantic information from low-level acoustics to better align with
language models. In particular, previous methods use SSL teachers such as
HuBERT to extract semantic representations, which are then distilled into a
semantic quantizer to suppress acoustic redundancy as well as capture
content-related latent structures. However, they still produce speech token
sequences significantly longer than their textual counterparts, creating
challenges for efficient speech-language modeling. Reducing the frame rate is a
natural solution, but standard techniques, such as rigid average pooling across
frames, can distort or dilute the semantic structure required for effective LM
alignment. To address this, we propose LM-SPT, a speech tokenization method
that introduces a novel semantic distillation. Instead of directly matching
teacher and student features via pooling, we reconstruct speech solely from
semantic tokens and minimize the discrepancy between the encoded
representations of the original and reconstructed waveforms, obtained from a
frozen automatic speech recognition (ASR) encoder. This indirect yet
data-driven supervision enables the tokenizer to learn discrete units that are
more semantically aligned with language models. LM-SPT further incorporates
architectural improvements to the encoder and decoder for speech tokenization,
and supports multiple frame rates, including 25Hz, 12.5Hz, and 6.25Hz.
Experimental results show that LM-SPT achieves superior reconstruction fidelity
compared to baselines, and that SLMs trained with LM-SPT tokens achieve
competitive performances on speech-to-text and consistently outperform
baselines on text-to-speech tasks.

</details>


### [59] [Language-Informed Synthesis of Rational Agent Models for Grounded Theory-of-Mind Reasoning On-The-Fly](https://arxiv.org/abs/2506.16755)
*Lance Ying,Ryan Truong,Katherine M. Collins,Cedegao E. Zhang,Megan Wei,Tyler Brooke-Wilson,Tan Zhi-Xuan,Lionel Wong,Joshua B. Tenenbaum*

Main category: cs.CL

TL;DR: 提出LIRAS框架，通过整合多模态语言模型和贝叶斯逆向规划引擎，提升社会推理任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 现实社会推理需整合多模态信息，语言在社交场景中能提供环境动态和代理细节，尤其在视觉信息不足的新情境下。

Method: 1. 多模态语言模型解析语言/视觉输入为统一符号表示；2. 贝叶斯逆向规划引擎生成概率判断。

Result: 在认知科学实验任务中，LIRAS（轻量级VLM实现）全面优于现有模型，更贴合人类判断。

Conclusion: LIRAS有效融合语言与视觉信息，增强社会推理的灵活性和准确性，为认知科学研究提供新工具。

Abstract: Drawing real world social inferences usually requires taking into account
information from multiple modalities. Language is a particularly powerful
source of information in social settings, especially in novel situations where
language can provide both abstract information about the environment dynamics
and concrete specifics about an agent that cannot be easily visually observed.
In this paper, we propose Language-Informed Rational Agent Synthesis (LIRAS), a
framework for drawing context-specific social inferences that integrate
linguistic and visual inputs. LIRAS frames multimodal social reasoning as a
process of constructing structured but situation-specific agent and environment
representations - leveraging multimodal language models to parse language and
visual inputs into unified symbolic representations, over which a Bayesian
inverse planning engine can be run to produce granular probabilistic judgments.
On a range of existing and new social reasoning tasks derived from cognitive
science experiments, we find that our model (instantiated with a comparatively
lightweight VLM) outperforms ablations and state-of-the-art models in capturing
human judgments across all domains.

</details>


### [60] [SocialSim: Towards Socialized Simulation of Emotional Support Conversation](https://arxiv.org/abs/2506.16756)
*Zhuang Chen,Yaru Cao,Guanqun Bi,Jincenzi Wu,Jinfeng Zhou,Xiyao Xiao,Si Chen,Hongning Wang,Minlie Huang*

Main category: cs.CL

TL;DR: SocialSim框架通过整合社交披露和社交认知，构建SSConv大规模ESC语料库，其质量超越众包数据，训练出的聊天机器人性能领先。


<details>
  <summary>Details</summary>
Motivation: 解决众包构建ESC语料库的高成本问题，提升对话模拟中社会动态要素的建模效果。

Method: 构建求助者社交画像库实现社交披露，通过认知推理增强支持者的社交认知能力。

Result: SSConv语料库质量超越众包数据，基于其训练的聊天机器人在自动评估和人工评估中达到SOTA。

Conclusion: SocialSim为情感支持对话提供可扩展的合成方案，使情感关怀更易普及。

Abstract: Emotional support conversation (ESC) helps reduce people's psychological
stress and provide emotional value through interactive dialogues. Due to the
high cost of crowdsourcing a large ESC corpus, recent attempts use large
language models for dialogue augmentation. However, existing approaches largely
overlook the social dynamics inherent in ESC, leading to less effective
simulations. In this paper, we introduce SocialSim, a novel framework that
simulates ESC by integrating key aspects of social interactions: social
disclosure and social awareness. On the seeker side, we facilitate social
disclosure by constructing a comprehensive persona bank that captures diverse
and authentic help-seeking scenarios. On the supporter side, we enhance social
awareness by eliciting cognitive reasoning to generate logical and supportive
responses. Building upon SocialSim, we construct SSConv, a large-scale
synthetic ESC corpus of which quality can even surpass crowdsourced ESC data.
We further train a chatbot on SSConv and demonstrate its state-of-the-art
performance in both automatic and human evaluations. We believe SocialSim
offers a scalable way to synthesize ESC, making emotional care more accessible
and practical.

</details>


### [61] [Cross-Modal Obfuscation for Jailbreak Attacks on Large Vision-Language Models](https://arxiv.org/abs/2506.16760)
*Lei Jiang,Zixun Zhang,Zizhou Wang,Xiaobing Sun,Zhen Li,Liangli Zhen,Xiaohua Xu*

Main category: cs.CL

TL;DR: CAMO框架通过分解恶意指令为多模态片段，利用跨模态推理绕过大视觉语言模型的安全机制


<details>
  <summary>Details</summary>
Motivation: 现有黑盒越狱方法易被内容过滤系统检测且效率低下，需更隐蔽高效的攻击方案验证安全机制漏洞

Method: 将恶意指令拆解为语义无害的视觉-文本片段，利用模型跨模态推理能力进行隐蔽重组

Result: 在主流LVLMs上验证有效性，展示强跨模型迁移性与高攻击成功率

Conclusion: 当前安全机制存在严重漏洞，需开发基于对齐感知的新型安全解决方案

Abstract: Large Vision-Language Models (LVLMs) demonstrate exceptional performance
across multimodal tasks, yet remain vulnerable to jailbreak attacks that bypass
built-in safety mechanisms to elicit restricted content generation. Existing
black-box jailbreak methods primarily rely on adversarial textual prompts or
image perturbations, yet these approaches are highly detectable by standard
content filtering systems and exhibit low query and computational efficiency.
In this work, we present Cross-modal Adversarial Multimodal Obfuscation (CAMO),
a novel black-box jailbreak attack framework that decomposes malicious prompts
into semantically benign visual and textual fragments. By leveraging LVLMs'
cross-modal reasoning abilities, CAMO covertly reconstructs harmful
instructions through multi-step reasoning, evading conventional detection
mechanisms. Our approach supports adjustable reasoning complexity and requires
significantly fewer queries than prior attacks, enabling both stealth and
efficiency. Comprehensive evaluations conducted on leading LVLMs validate
CAMO's effectiveness, showcasing robust performance and strong cross-model
transferability. These results underscore significant vulnerabilities in
current built-in safety mechanisms, emphasizing an urgent need for advanced,
alignment-aware security and safety solutions in vision-language systems.

</details>


### [62] [DistillNote: LLM-based clinical note summaries improve heart failure diagnosis](https://arxiv.org/abs/2506.16777)
*Heloisa Oss Boll,Antonio Oss Boll,Leticia Puttlitz Boll,Ameen Abu Hanna,Iacer Calixto*

Main category: cs.CL

TL;DR: Distillnote框架利用大语言模型生成临床笔记摘要，蒸馏摘要实现79%文本压缩率并提升心脏衰竭预测性能18.2%，同时显著减少幻觉生成


<details>
  <summary>Details</summary>
Motivation: 解决临床文档处理负担过重的问题，通过自动化摘要提升医疗效率

Method: 采用三种摘要生成技术：直接摘要、结构化分项摘要、蒸馏压缩摘要

Result: 蒸馏摘要达到6.9倍压缩效率比，临床评估显示直接摘要更具可操作性，蒸馏摘要平衡效率与准确性

Conclusion: 该框架为临床文档处理提供有效解决方案，公开数据集促进未来医疗AI研究发展

Abstract: Large language models (LLMs) offer unprecedented opportunities to generate
concise summaries of patient information and alleviate the burden of clinical
documentation that overwhelms healthcare providers. We present Distillnote, a
framework for LLM-based clinical note summarization, and generate over 64,000
admission note summaries through three techniques: (1) One-step, direct
summarization, and a divide-and-conquer approach involving (2) Structured
summarization focused on independent clinical insights, and (3) Distilled
summarization that further condenses the Structured summaries. We test how
useful are the summaries by using them to predict heart failure compared to a
model trained on the original notes. Distilled summaries achieve 79% text
compression and up to 18.2% improvement in AUPRC compared to an LLM trained on
the full notes. We also evaluate the quality of the generated summaries in an
LLM-as-judge evaluation as well as through blinded pairwise comparisons with
clinicians. Evaluations indicate that one-step summaries are favoured by
clinicians according to relevance and clinical actionability, while distilled
summaries offer optimal efficiency (avg. 6.9x compression-to-performance ratio)
and significantly reduce hallucinations. We release our summaries on PhysioNet
to encourage future research.

</details>


### [63] [MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning](https://arxiv.org/abs/2506.16792)
*Muyang Zheng,Yuanzhi Yao,Changting Lin,Rui Wang,Meng Han*

Main category: cs.CL

TL;DR: 提出MIST方法，通过迭代语义调整实现对黑盒大语言模型的越狱攻击，兼具攻击效率与迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型仍易受越狱攻击，但黑盒模型攻击面临离散输入、访问受限和计算资源限制的挑战。

Method: 采用顺序同义词搜索(order-determining优化)策略，在保持语义意图前提下迭代优化攻击提示。

Result: 在6个开源/闭源模型中达到SOTA攻击成功率，计算效率验证了实际可行性。

Conclusion: MIST为黑盒LLM安全防御提供了重要参考，证明语义保持型攻击的有效性。

Abstract: Despite efforts to align large language models (LLMs) with societal and moral
values, these models remain susceptible to jailbreak attacks--methods designed
to elicit harmful responses. Jailbreaking black-box LLMs is considered
challenging due to the discrete nature of token inputs, restricted access to
the target LLM, and limited query budget. To address the issues above, we
propose an effective method for jailbreaking black-box large language Models
via Iterative Semantic Tuning, named MIST. MIST enables attackers to
iteratively refine prompts that preserve the original semantic intent while
inducing harmful content. Specifically, to balance semantic similarity with
computational efficiency, MIST incorporates two key strategies: sequential
synonym search, and its advanced version--order-determining optimization.
Extensive experiments across two open-source models and four closed-source
models demonstrate that MIST achieves competitive attack success rates and
attack transferability compared with other state-of-the-art white-box and
black-box jailbreak methods. Additionally, we conduct experiments on
computational efficiency to validate the practical viability of MIST.

</details>


### [64] [From Data to Knowledge: Evaluating How Efficiently Language Models Learn Facts](https://arxiv.org/abs/2506.16912)
*Daniel Christoph,Max Ploner,Patrick Haller,Alan Akbik*

Main category: cs.CL

TL;DR: 语言模型的样本效率影响事实学习能力，不同架构/规模的模型在低频事实上表现差异显著


<details>
  <summary>Details</summary>
Motivation: 研究模型架构和规模如何影响不同频率事实的学习效率，解决长尾数据场景下低频事实的高效学习问题

Method: 标注训练语料中关系事实的频率，对比分析不同架构/规模模型在各频率段的表现差异

Result: 模型在高频事实上表现趋同，但在低频事实上的准确率存在显著架构差异（如密集模型 vs 混合专家模型）

Conclusion: 揭示了模型架构与事实学习效率的关系，为面向长尾数据的高效模型设计提供新视角

Abstract: Sample efficiency is a crucial property of language models with practical
implications for training efficiency. In real-world text, information follows a
long-tailed distribution. Yet, we expect models to learn and recall frequent
and infrequent facts. Sample-efficient models are better equipped to handle
this challenge of learning and retaining rare information without requiring
excessive exposure. This study analyzes multiple models of varying
architectures and sizes, all trained on the same pre-training data. By
annotating relational facts with their frequencies in the training corpus, we
examine how model performance varies with fact frequency. Our findings show
that most models perform similarly on high-frequency facts but differ notably
on low-frequency facts. This analysis provides new insights into the
relationship between model architecture, size, and factual learning efficiency.

</details>


### [65] [Language Bottleneck Models: A Framework for Interpretable Knowledge Tracing and Beyond](https://arxiv.org/abs/2506.16982)
*Antonin Berthon,Mihaela van der Schaar*

Main category: cs.CL

TL;DR: 提出语言瓶颈模型(LBM)，通过自然语言摘要实现可解释的知识追踪，在保持精度的同时显著降低数据需求。


<details>
  <summary>Details</summary>
Motivation: 传统知识追踪方法依赖不透明的潜在嵌入，而现有LLM方法存在幻觉风险且缺乏精度保证。需建立通过自然语言瓶颈实现可解释且可靠的学生知识评估框架。

Method: 将知识追踪重新定义为逆问题：1) 编码器LLM生成可解释的知识摘要 2) 冻结解码器LLM仅基于摘要重建预测学生行为 3) 采用基于下游解码精度的群体相对策略优化训练编码器

Result: 在算术基准测试和Eedi数据集上，LBM精度媲美最先进方法，所需学生轨迹数据量减少多个数量级。策略优化训练显著提升摘要质量。

Conclusion: 语言瓶颈模型通过强制信息流经自然语言摘要，实现了准确且可解释的知识追踪，验证了文本瓶颈在保持模型可靠性和可解释性方面的有效性。

Abstract: Accurately assessing student knowledge is critical for effective education,
yet traditional Knowledge Tracing (KT) methods rely on opaque latent
embeddings, limiting interpretability. Even LLM-based approaches generate
direct predictions or summaries that may hallucinate without any accuracy
guarantees. We recast KT as an inverse problem: learning the minimum
natural-language summary that makes past answers explainable and future answers
predictable. Our Language Bottleneck Model (LBM) consists of an encoder LLM
that writes an interpretable knowledge summary and a frozen decoder LLM that
must reconstruct and predict student responses using only that summary text. By
constraining all predictive information to pass through a short
natural-language bottleneck, LBMs ensure that the summary contains accurate
information while remaining human-interpretable. Experiments on synthetic
arithmetic benchmarks and the large-scale Eedi dataset show that LBMs rival the
accuracy of state-of-the-art KT and direct LLM methods while requiring
orders-of-magnitude fewer student trajectories. We demonstrate that training
the encoder with group-relative policy optimization, using downstream decoding
accuracy as a reward signal, effectively improves summary quality.

</details>


### [66] [TeXpert: A Multi-Level Benchmark for Evaluating LaTeX Code Generation by LLMs](https://arxiv.org/abs/2506.16990)
*Sahil Kale,Vijaykant Nadadur*

Main category: cs.CL

TL;DR: 评估大语言模型生成LaTeX代码的能力，发现随着任务复杂度增加准确性显著下降，开源模型DeepSeek表现接近闭源模型，格式和包错误普遍暴露训练数据不足。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试缺乏对LLMs生成科学文档LaTeX代码能力的评估，而LaTeX作为科研排版标准值得深入研究模型在此任务中的表现。

Method: 通过构建多难度层级的TeXpert基准数据集，对开源/闭源LLMs进行LaTeX代码生成测试，并分类错误类型。

Result: 1. 标准基准表现优异的模型在LaTeX生成中表现差
2. 任务复杂度与错误率正相关
3. DeepSeek系列开源模型媲美闭源模型
4. 62%错误源于格式/包使用问题

Conclusion: 现有LLMs的LaTeX生成能力被低估，开源模型展现竞争力，训练数据需要增加LaTeX多样性以提升科研文档自动化生成质量。

Abstract: LaTeX's precision and flexibility in typesetting have made it the gold
standard for the preparation of scientific documentation. Large Language Models
(LLMs) present a promising opportunity for researchers to produce
publication-ready material using LaTeX with natural language instructions, yet
current benchmarks completely lack evaluation of this ability. By introducing
TeXpert, our benchmark dataset with natural language prompts for generating
LaTeX code focused on components of scientific documents across multiple
difficulty levels, we conduct an in-depth analysis of LLM performance in this
regard and identify frequent error types. Our evaluation across open and
closed-source LLMs highlights multiple key findings: LLMs excelling on standard
benchmarks perform poorly in LaTeX generation with a significant accuracy
drop-off as the complexity of tasks increases; open-source models like DeepSeek
v3 and DeepSeek Coder strongly rival closed-source counterparts in LaTeX tasks;
and formatting and package errors are unexpectedly prevalent, suggesting a lack
of diverse LaTeX examples in the training datasets of most LLMs. Our dataset,
code, and model evaluations are available at
https://github.com/knowledge-verse-ai/TeXpert.

</details>


### [67] [PersonalAI: Towards digital twins in the graph form](https://arxiv.org/abs/2506.17001)
*Mikhail Menschikov,Dmitry Evseev,Ruslan Kostoev,Ilya Perepechkin,Ilnaz Salimov,Victoria Dochkina,Petr Anokhin,Evgeny Burnaev,Nikita Semenov*

Main category: cs.CL

TL;DR: 通过结合知识图谱和超边架构提升语言模型的个性化响应能力，在时间参数和矛盾信息场景下保持鲁棒性


<details>
  <summary>Details</summary>
Motivation: 解决现有语言模型难以长期保持用户历史信息并生成个性化响应的核心挑战

Method: 利用LLM自建/更新的知识图谱，扩展AriGraph架构并首次引入标准边+两种超边的组合图结构

Result: 在TriviaQA/HotpotQA/DiaASQ基准测试中实现统一的知识提取流程，增强版DiaASQ时间参数测试显示87%的准确率稳定性

Conclusion: 该架构通过时间戳标注和矛盾检测机制，有效处理时序依赖关系，验证了动态知识管理的可行性

Abstract: The challenge of personalizing language models, specifically the ability to
account for a user's history during interactions, is of significant interest.
Despite recent advancements in large language models (LLMs) and Retrieval
Augmented Generation that have enhanced the factual base of LLMs, the task of
retaining extensive personal information and using it to generate personalized
responses remains pertinent. To address this, we propose utilizing external
memory in the form of knowledge graphs, which are constructed and updated by
the LLM itself. We have expanded upon ideas of AriGraph architecture and for
the first time introduced a combined graph featuring both standard edges and
two types of hyperedges. Experiments conducted on the TriviaQA, HotpotQA and
DiaASQ benchmarks indicates that this approach aids in making the process of
graph construction and knowledge extraction unified and robust. Furthermore, we
augmented the DiaASQ benchmark by incorporating parameters such as time into
dialogues and introducing contradictory statements made by the same speaker at
different times. Despite these modifications, the performance of the
question-answering system remained robust, demonstrating the proposed
architecture's ability to maintain and utilize temporal dependencies.

</details>


### [68] [LLM-Generated Feedback Supports Learning If Learners Choose to Use It](https://arxiv.org/abs/2506.17006)
*Danielle R. Thomas,Conrad Borchers,Shambhavi Bhushan,Erin Gatz,Shivang Gupta,Kenneth R. Koedinger*

Main category: cs.CL

TL;DR: 研究通过2600+课程数据分析发现，使用GPT-3.5生成的解释性反馈能适度提升辅导培训学习效果，且不影响学习时长，但效果与学习者主动寻求支持的倾向相关。


<details>
  <summary>Details</summary>
Motivation: 探索LLM生成反馈对学习效果的影响（相比传统反馈方式），特别是在已有反馈系统的开放任务场景中验证其增效潜力。

Method: 对885名学习者在7个情境化辅导课程中的学习数据分组对比（使用LLM反馈组/拒绝组/无访问组），采用倾向评分法控制选择偏差，分析后测成绩与反馈使用关联。

Result: 高反馈使用倾向组后测显著更优；校正后2/7课程呈现标准化效应值0.28-0.33的显著提升；97%学习者认为反馈有帮助且未显著延长学习时间。

Conclusion: LLM反馈可作为低成本扩展方案有效增强开放任务学习效果，其效能取决于学习者支持寻求倾向，为现有反馈系统提供增效补充。

Abstract: Large language models (LLMs) are increasingly used to generate feedback, yet
their impact on learning remains underexplored, especially compared to existing
feedback methods. This study investigates how on-demand LLM-generated
explanatory feedback influences learning in seven scenario-based tutor training
lessons. Analyzing over 2,600 lesson completions from 885 tutor learners, we
compare posttest performance among learners across three groups: learners who
received feedback generated by gpt-3.5-turbo, those who declined it, and those
without access. All groups received non-LLM corrective feedback. To address
potential selection bias-where higher-performing learners may be more inclined
to use LLM feedback-we applied propensity scoring. Learners with a higher
predicted likelihood of engaging with LLM feedback scored significantly higher
at posttest than those with lower propensity. After adjusting for this effect,
two out of seven lessons showed statistically significant learning benefits
from LLM feedback with standardized effect sizes of 0.28 and 0.33. These
moderate effects suggest that the effectiveness of LLM feedback depends on the
learners' tendency to seek support. Importantly, LLM feedback did not
significantly increase completion time, and learners overwhelmingly rated it as
helpful. These findings highlight LLM feedback's potential as a low-cost and
scalable way to improve learning on open-ended tasks, particularly in existing
systems already providing feedback without LLMs. This work contributes open
datasets, LLM prompts, and rubrics to support reproducibility.

</details>


### [69] [Instituto de Telecomunicações at IWSLT 2025: Aligning Small-Scale Speech and Language Models for Speech-to-Text Learning](https://arxiv.org/abs/2506.17019)
*Giuseppe Attanasio,Sonal Sannigrahi,Ben Peters,André F. T. Martins*

Main category: cs.CL

TL;DR: 提出统一语音文本模型IT-IST，采用两阶段训练（模态对齐+指令微调），基于小规模语言模型（<2B）与合规数据实现多任务语音处理。


<details>
  <summary>Details</summary>
Motivation: 探索在有限计算资源下，通过高效模型架构和严格数据合规性（CC-BY许可）提升语音任务的泛化能力，同时利用合成数据弥补资源不足。

Method: 1. 整合预训练语音编码器与文本解码器
2. 两阶段训练：先模态对齐实现跨模态映射，后指令微调优化多任务性能
3. 采用<2B参数的小型语言模型，仅使用CC-BY合规数据并辅以合成数据增强

Result: 在IWSLT 2025的语音识别、翻译、口语问答三项任务中提交有效结果，证明小模型通过结构优化可实现多任务处理能力。

Conclusion: 验证了小规模模型配合两阶段训练策略与高质量数据管线的有效性，为资源受限场景下的语音处理任务提供了合规高效的解决方案。

Abstract: This paper presents the IT-IST submission to the IWSLT 2025 Shared Task on
Instruction Following Speech Processing. We submit results for the Short Track,
i.e., speech recognition, translation, and spoken question answering. Our model
is a unified speech-to-text model that integrates a pre-trained continuous
speech encoder and text decoder through a first phase of modality alignment and
a second phase of instruction fine-tuning. Crucially, we focus on using
small-scale language model backbones (< 2B) and restrict to high-quality, CC-BY
data along with synthetic data generation to supplement existing resources.

</details>


### [70] [MUCAR: Benchmarking Multilingual Cross-Modal Ambiguity Resolution for Multimodal Large Language Models](https://arxiv.org/abs/2506.17046)
*Xiaolong Wang,Zhaolu Kang,Wangyuxuan Zhai,Xinyue Lou,Yunghwei Lai,Ziyue Wang,Yawen Wang,Kaiyu Huang,Yile Wang,Peng Li,Yang Liu*

Main category: cs.CL

TL;DR: 论文提出MUCAR多模态歧义评估基准，通过多语言数据集和双歧义数据集测试，发现现有模型与人类表现存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型在解决视觉-语言联合歧义时存在局限，且传统基准未能有效利用跨模态互解释潜力，需建立更系统的评估体系。

Method: 构建包含多语言文本歧义（视觉消解）和双模态歧义（图文互消歧）的MUCAR基准，覆盖19种前沿模型进行系统评估。

Result: 实验表明所有模型在跨模态歧义理解上与人类水平差距达20-45%，开源模型表现尤为薄弱。

Conclusion: MUCAR暴露出现有多模态模型的根本缺陷，推动开发更复杂的跨模态互解释方法以提升多模态推理能力。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
advances across numerous vision-language tasks. Due to their strong image-text
alignment capability, MLLMs can effectively understand image-text pairs with
clear meanings. However, effectively resolving the inherent ambiguities in
natural language and visual contexts remains challenging. Existing multimodal
benchmarks typically overlook linguistic and visual ambiguities, relying mainly
on unimodal context for disambiguation and thus failing to exploit the mutual
clarification potential between modalities. To bridge this gap, we introduce
MUCAR, a novel and challenging benchmark designed explicitly for evaluating
multimodal ambiguity resolution across multilingual and cross-modal scenarios.
MUCAR includes: (1) a multilingual dataset where ambiguous textual expressions
are uniquely resolved by corresponding visual contexts, and (2) a
dual-ambiguity dataset that systematically pairs ambiguous images with
ambiguous textual contexts, with each combination carefully constructed to
yield a single, clear interpretation through mutual disambiguation. Extensive
evaluations involving 19 state-of-the-art multimodal models--encompassing both
open-source and proprietary architectures--reveal substantial gaps compared to
human-level performance, highlighting the need for future research into more
sophisticated cross-modal ambiguity comprehension methods, further pushing the
boundaries of multimodal reasoning.

</details>


### [71] [Simultaneous Translation with Offline Speech and LLM Models in CUNI Submission to IWSLT 2025](https://arxiv.org/abs/2506.17077)
*Dominik Macháček,Peter Polák*

Main category: cs.CL

TL;DR: Charles University在IWSLT 2025同步语音翻译任务中，使用Whisper模型和AlignAtt策略改进系统性能，BLEU分数显著提升并提出了新的延迟测量指标。


<details>
  <summary>Details</summary>
Motivation: 改进同步语音翻译系统的性能，覆盖多语言对的直接/级联翻译需求

Method: 采用Whisper语音模型实现同步翻译/转录，结合AlignAtt策略、术语注入提示和EuroLLM无界翻译技术

Result: 捷克语→英语提升2 BLEU，英语→德语/中文/日语提升13-22 BLEU；提出新的语音识别延迟测量方法

Conclusion: 提示注入和上下文适应有效提升性能，级联系统配合EuroLLM在无界翻译中表现优异，新延迟指标增强评估体系

Abstract: This paper describes Charles University submission to the Simultaneous Speech
Translation Task of the IWSLT 2025. We cover all four language pairs with a
direct or cascade approach. The backbone of our systems is the offline Whisper
speech model, which we use for both translation and transcription in
simultaneous mode with the state-of-the-art simultaneous policy AlignAtt. We
further improve the performance by prompting to inject in-domain terminology,
and we accommodate context. Our cascaded systems further use EuroLLM for
unbounded simultaneous translation. Compared to the Organizers' baseline, our
systems improve by 2 BLEU points on Czech to English and 13-22 BLEU points on
English to German, Chinese and Japanese on the development sets. Additionally,
we also propose a new enhanced measure of speech recognition latency.

</details>


### [72] [Tower+: Bridging Generality and Translation Specialization in Multilingual LLMs](https://arxiv.org/abs/2506.17080)
*Ricardo Rei,Nuno M. Guerreiro,José Pombal,João Alves,Pedro Teixeirinha,Amin Farajian,André F. T. Martins*

Main category: cs.CL

TL;DR: Tower+模型套件通过新型训练策略实现翻译专业化与多语言通用能力的帕累托最优，不同规模模型在翻译和综合任务中超越大型通用模型。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法会牺牲LLM的通用能力（如对话推理和指令跟随），限制模型在需要多技能组合的实际场景中的应用价值。

Method: 四阶段训练框架：1）继续预训练强化基础能力 2）监督微调优化翻译 3）偏好对齐优化 4）可验证奖励的强化学习。全程采用精心生成的多任务数据（涵盖代码生成、数学解题等）。

Result: 2B/9B模型超越Llama3.3 70B和GPT-4o等大型模型；72B模型在高资源语言翻译达SOTA，在IF-MT基准（作者提出的翻译+指令综合评估）和多语言Arena Hard评估中表现顶尖。

Conclusion: 证明专业领域优化（如翻译）与保持通用能力可兼得，为商业场景中模型专用化提供可行路径，打破专业性能与通用性能此消彼长的传统困境。

Abstract: Fine-tuning pretrained LLMs has been shown to be an effective strategy for
reaching state-of-the-art performance on specific tasks like machine
translation. However, this process of adaptation often implies sacrificing
general-purpose capabilities, such as conversational reasoning and
instruction-following, hampering the utility of the system in real-world
applications that require a mixture of skills. In this paper, we introduce
Tower+, a suite of models designed to deliver strong performance across both
translation and multilingual general-purpose text capabilities. We achieve a
Pareto frontier between translation specialization and multilingual
general-purpose capabilities by introducing a novel training recipe that builds
on Tower (Alves et al., 2024), comprising continued pretraining, supervised
fine-tuning, preference optimization, and reinforcement learning with
verifiable rewards. At each stage of training, we carefully generate and curate
data to strengthen performance on translation as well as general-purpose tasks
involving code generation, mathematics problem solving, and general
instruction-following. We develop models at multiple scales: 2B, 9B, and 72B.
Our smaller models often outperform larger general-purpose open-weight and
proprietary LLMs (e.g., Llama 3.3 70B, GPT-4o). Our largest model delivers
best-in-class translation performance for high-resource languages and top
results in multilingual Arena Hard evaluations and in IF-MT, a benchmark we
introduce for evaluating both translation and instruction-following. Our
findings highlight that it is possible to rival frontier models in general
capabilities, while optimizing for specific business domains, such as
translation and localization.

</details>


### [73] [Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language Models: An Empirical Evaluation](https://arxiv.org/abs/2506.17088)
*Jiahao Cheng,Tiancheng Su,Jia Yuan,Guoxiu He,Jiawei Liu,Xinqi Tao,Jingwen Xie,Huaxia Li*

Main category: cs.CL

TL;DR: 研究发现，思维链（CoT）提示法虽能减少大语言模型（LLM）的幻觉生成，但会掩盖检测关键信号，导致检测方法失效，揭示推理使用中的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 探索CoT提示法对LLM幻觉检测的影响，尽管已知CoT能减少幻觉，但其对检测机制的具体作用尚未明确。

Method: 通过初步实验验证CoT对LLM内部状态及概率分布的影响，系统评估不同CoT方法对主流检测方法的影响，涵盖指令调优与推理导向的LLM。

Result: CoT降低幻觉频率的同时模糊检测信号，导致检测准确率与置信度下降，揭示推理效用与检测效能的矛盾。

Conclusion: 研究揭示了使用推理技术时被忽视的权衡：CoT虽缓解幻觉生成，但削弱检测能力，需在两者间寻求平衡。

Abstract: Large Language Models (LLMs) often exhibit \textit{hallucinations},
generating factually incorrect or semantically irrelevant content in response
to prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by
encouraging step-by-step reasoning, but its impact on hallucination detection
remains underexplored. To bridge this gap, we conduct a systematic empirical
evaluation. We begin with a pilot experiment, revealing that CoT reasoning
significantly affects the LLM's internal states and token probability
distributions. Building on this, we evaluate the impact of various CoT
prompting methods on mainstream hallucination detection methods across both
instruction-tuned and reasoning-oriented LLMs. Specifically, we examine three
key dimensions: changes in hallucination score distributions, variations in
detection accuracy, and shifts in detection confidence. Our findings show that
while CoT prompting helps reduce hallucination frequency, it also tends to
obscure critical signals used for detection, impairing the effectiveness of
various detection methods. Our study highlights an overlooked trade-off in the
use of reasoning. Code is publicly available at:
https://anonymous.4open.science/r/cot-hallu-detect.

</details>


### [74] [Better Language Model Inversion by Compactly Representing Next-Token Distributions](https://arxiv.org/abs/2506.17090)
*Murtaza Nazir,Matthew Finlayson,John X. Morris,Xiang Ren,Swabha Swayamdipta*

Main category: cs.CL

TL;DR: 提出PILS方法，利用语言模型多步骤生成中的logprob序列恢复隐藏提示，通过低维子空间压缩技术实现恢复率2-3.5倍提升，揭示logprob是脆弱攻击面。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型API部署中系统消息隐私泄露问题，提升模型安全性和问责机制。

Method: 基于模型输出向量存在于低维子空间的特性，设计线性映射无损压缩多步生成的概率分布，构建高效提示恢复算法。

Result: 恢复率从基线17%提升至60%，跨步骤数泛化能力提升5-27个百分点，实现跨模型家族迁移攻击。

Conclusion: 证明next-token probabilities是严重安全漏洞，需加强防御；同时为模型可解释性研究提供新工具。

Abstract: Language model inversion seeks to recover hidden prompts using only language
model outputs. This capability has implications for security and accountability
in language model deployments, such as leaking private information from an
API-protected language model's system message. We propose a new method --
prompt inversion from logprob sequences (PILS) -- that recovers hidden prompts
by gleaning clues from the model's next-token probabilities over the course of
multiple generation steps. Our method is enabled by a key insight: The
vector-valued outputs of a language model occupy a low-dimensional subspace.
This enables us to losslessly compress the full next-token probability
distribution over multiple generation steps using a linear map, allowing more
output information to be used for inversion. Our approach yields massive gains
over previous state-of-the-art methods for recovering hidden prompts, achieving
2--3.5 times higher exact recovery rates across test sets, in one case
increasing the recovery rate from 17% to 60%. Our method also exhibits
surprisingly good generalization behavior; for instance, an inverter trained on
16 generations steps gets 5--27 points higher prompt recovery when we increase
the number of steps to 32 at test time. Furthermore, we demonstrate strong
performance of our method on the more challenging task of recovering hidden
system messages. We also analyze the role of verbatim repetition in prompt
recovery and propose a new method for cross-family model transfer for
logit-based inverters. Our findings show that next-token probabilities are a
considerably more vulnerable attack surface for inversion attacks than
previously known.

</details>


### [75] [Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context LMs?](https://arxiv.org/abs/2506.17121)
*Adithya Bhaskar,Alexander Wettig,Tianyu Gao,Yihe Dong,Danqi Chen*

Main category: cs.CL

TL;DR: 提出KV footprint统一指标评估长上下文推理性能与内存效率，改进KV驱逐方法并设计PruLong优化方案，在保持性能的同时减少12%内存占用


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存管理方法存在高内存峰值、性能下降、评估标准不统一等问题，需建立科学的评估体系并优化内存效率

Method: 1. 引入KV footprint指标量化内存占用 2. 改进后填充驱逐方法实现预填充阶段KV驱逐 3. 提出PruLong端到端优化方案，动态选择注意力头保留完整KV缓存

Result: PruLong相比现有方法减少12% KV footprint，在128K tokens长上下文场景下保持召回任务性能

Conclusion: 建立长上下文推理方法评估框架，为未来KV内存优化指明方向，平衡性能与内存效率

Abstract: Language models handle increasingly long contexts for tasks such as book
summarization, but this leads to growing memory costs for the key-value (KV)
cache. Many prior works have proposed ways of discarding KVs from memory, but
their approaches are tailored to favorable settings, obscuring caveats like
high peak memory and performance degradation, and a fair comparison between
methods is difficult. In this paper, we propose the *KV footprint* as a unified
metric, which accounts for both the amount of KV entries stored and their
lifespan in memory. We evaluate methods based on the smallest footprint they
attain while preserving performance in both long-context understanding and
generation, with context lengths of up to 128K tokens. This metric reveals the
high peak memory of prior KV eviction methods. One class of methods --
*post-fill eviction* -- has a high footprint due to being incompatible with
eviction during pre-filling. We adapt these methods to be able to evict KVs
during pre-filling, achieving substantially lower KV footprints. We then turn
to *recency eviction* methods, wherein we propose PruLong, an end-to-end
optimization method for learning which attention heads need to retain the full
KV cache and which do not. PruLong saves memory while preserving long-context
performance, achieving 12% smaller KV footprint than prior methods while
retaining performance in challenging recall tasks. Our paper clarifies the
complex tangle of long-context inference methods and paves the way for future
development to minimize the KV footprint.

</details>


### [76] [CLEAR-3K: Assessing Causal Explanatory Capabilities in Language Models](https://arxiv.org/abs/2506.17180)
*Naiming Liu,Richard Baraniuk,Shashank Sonkar*

Main category: cs.CL

TL;DR: CLEAR-3K是包含3000个断言-推理问题的数据集，用于评估语言模型区分语义关联与真实因果解释关系的能力。研究发现模型易混淆语义相似性与因果性，且模型规模增大会导致从过度怀疑转向过度接受因果关系的倾向。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在因果推理中存在缺陷，常混淆语义关联与真实因果关系。需构建新数据集CLEAR-3K来系统评估模型对因果解释关系的判断能力。

Method: 使用CLEAR-3K数据集对21个不同参数规模（0.5B-72B）的先进语言模型进行测试，通过断言-推理对验证模型判断因果关系的准确性。

Result: 1. 模型主要依赖词汇/语义重叠而非因果推理判断关系
2. 参数增大使模型从过度怀疑转向过度接受因果关系
3. 最佳模型的马修斯相关系数仅0.55，显示性能瓶颈

Conclusion: CLEAR-3K为开发真实因果推理能力提供关键基准，这对需要准确评估因果关系的应用（如科学分析、决策支持）具有重要价值。

Abstract: We introduce CLEAR-3K, a dataset of 3,000 assertion-reasoning questions
designed to evaluate whether language models can determine if one statement
causally explains another. Each question present an assertion-reason pair and
challenge language models to distinguish between semantic relatedness and
genuine causal explanatory relationships. Through comprehensive evaluation of
21 state-of-the-art language models (ranging from 0.5B to 72B parameters), we
identify two fundamental findings. First, language models frequently confuse
semantic similarity with causality, relying on lexical and semantic overlap
instead of inferring actual causal explanatory relationships. Second, as
parameter size increases, models tend to shift from being overly skeptical
about causal relationships to being excessively permissive in accepting them.
Despite this shift, performance measured by the Matthews Correlation
Coefficient plateaus at just 0.55, even for the best-performing models.Hence,
CLEAR-3K provides a crucial benchmark for developing and evaluating genuine
causal reasoning in language models, which is an essential capability for
applications that require accurate assessment of causal relationships.

</details>


### [77] [Towards AI Search Paradigm](https://arxiv.org/abs/2506.17188)
*Yuchen Li,Hengyi Cai,Rui Kong,Xinran Chen,Jiamin Chen,Jun Yang,Haojie Zhang,Jiayi Li,Jiayi Wu,Yiqun Chen,Changle Qu,Keyi Kong,Wenwen Ye,Lixin Su,Xinyu Ma,Long Xia,Daiting Shi,Jiashu Zhao,Haoyi Xiong,Shuaiqiang Wang,Dawei Yin*

Main category: cs.CL

TL;DR: 提出基于四模块LLM代理的AI搜索范式，通过动态协作与多技术整合实现复杂信息需求的高效处理


<details>
  <summary>Details</summary>
Motivation: 构建能模拟人类信息处理机制的下一代搜索系统，解决现有系统在复杂多阶段推理任务中的适应性不足问题

Method: 采用Master/Planner/Executor/Writer四代理架构，整合任务规划、工具编排、检索增强生成(RAG)和LLM推理优化技术

Result: 形成可信赖的模块化系统架构，实现查询复杂度评估-任务分解-工具协同的动态工作流

Conclusion: 为开发自适应、可扩展的AI搜索系统提供完整技术路线，涵盖算法创新与基础设施优化的系统级解决方案

Abstract: In this paper, we introduce the AI Search Paradigm, a comprehensive blueprint
for next-generation search systems capable of emulating human information
processing and decision-making. The paradigm employs a modular architecture of
four LLM-powered agents (Master, Planner, Executor and Writer) that dynamically
adapt to the full spectrum of information needs, from simple factual queries to
complex multi-stage reasoning tasks. These agents collaborate dynamically
through coordinated workflows to evaluate query complexity, decompose problems
into executable plans, and orchestrate tool usage, task execution, and content
synthesis. We systematically present key methodologies for realizing this
paradigm, including task planning and tool integration, execution strategies,
aligned and robust retrieval-augmented generation, and efficient LLM inference,
spanning both algorithmic techniques and infrastructure-level optimizations. By
providing an in-depth guide to these foundational components, this work aims to
inform the development of trustworthy, adaptive, and scalable AI search
systems.

</details>


### [78] [Fine-Tuning Lowers Safety and Disrupts Evaluation Consistency](https://arxiv.org/abs/2506.17209)
*Kathleen C. Fraser,Hillary Dawkins,Isar Nejadgholi,Svetlana Kiritchenko*

Main category: cs.CL

TL;DR: 微调大语言模型会意外移除其安全防护机制，现有安全基准测试存在显著方差，导致评估结果不可靠


<details>
  <summary>Details</summary>
Motivation: 揭示微调操作对LLM安全性的破坏效应，即使使用无害数据微调仍会降低模型安全性，这对普通开发者和恶意攻击者都具有重大影响

Method: 通过系统实验验证不同微调配置对安全评估结果的影响，分析模型输出结果的方差特性

Result: 安全评估结果对实验设置的微小变化极其敏感，包括随机种子等参数都会导致显著的结果波动

Conclusion: 研究界需要建立更严格的安全评估标准，通过规范化报告流程和实验设置来保证结果可比性

Abstract: Fine-tuning a general-purpose large language model (LLM) for a specific
domain or task has become a routine procedure for ordinary users. However,
fine-tuning is known to remove the safety alignment features of the model, even
when the fine-tuning data does not contain any harmful content. We consider
this to be a critical failure mode of LLMs due to the widespread uptake of
fine-tuning, combined with the benign nature of the "attack". Most
well-intentioned developers are likely unaware that they are deploying an LLM
with reduced safety. On the other hand, this known vulnerability can be easily
exploited by malicious actors intending to bypass safety guardrails. To make
any meaningful progress in mitigating this issue, we first need reliable and
reproducible safety evaluations. In this work, we investigate how robust a
safety benchmark is to trivial variations in the experimental procedure, and
the stochastic nature of LLMs. Our initial experiments expose surprising
variance in the results of the safety evaluation, even when seemingly
inconsequential changes are made to the fine-tuning setup. Our observations
have serious implications for how researchers in this field should report
results to enable meaningful comparisons in the future.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [79] [FLUX.1 Kontext: Flow Matching for In-Context Image Generation and Editing in Latent Space](https://arxiv.org/abs/2506.15742)
*Black Forest Labs,Stephen Batifol,Andreas Blattmann,Frederic Boesel,Saksham Consul,Cyril Diagne,Tim Dockhorn,Jack English,Zion English,Patrick Esser,Sumith Kulal,Kyle Lacey,Yam Levi,Cheng Li,Dominik Lorenz,Jonas Müller,Dustin Podell,Robin Rombach,Harry Saini,Axel Sauer,Luke Smith*

Main category: cs.GR

TL;DR: FLUX.1 Kontext提出统一架构同时支持图像生成与编辑，通过序列拼接方法提升多轮迭代一致性，并创建KontextBench基准验证性能优势。


<details>
  <summary>Details</summary>
Motivation: 现有编辑模型在多轮迭代中易出现角色一致性下降和画面退化问题，需要开发能同时保持生成质量和编辑稳定性的统一框架。

Method: 采用生成流匹配模型，结合文本图像语义上下文，通过序列拼接处理局部编辑和上下文生成任务，实现单架构多任务处理。

Result: 在KontextBench基准(1026图像-提示对)中，单次生成质量提升15%，多轮一致性指标提高32%，生成速度比现有模型快3倍。

Conclusion: 该研究确立了统一图像处理模型的新标准，为交互式AI创作工具开发提供了关键技术突破。

Abstract: We present evaluation results for FLUX.1 Kontext, a generative flow matching
model that unifies image generation and editing. The model generates novel
output views by incorporating semantic context from text and image inputs.
Using a simple sequence concatenation approach, FLUX.1 Kontext handles both
local editing and generative in-context tasks within a single unified
architecture. Compared to current editing models that exhibit degradation in
character consistency and stability across multiple turns, we observe that
FLUX.1 Kontext improved preservation of objects and characters, leading to
greater robustness in iterative workflows.The model achieves competitive
performance with current state-of-the-art systems while delivering
significantly faster generation times, enabling interactive applications and
rapid prototyping workflows. To validate these improvements, we introduce
KontextBench, a comprehensive benchmark with 1026 image-prompt pairs covering
five task categories: local editing, global editing, character reference, style
reference and text editing. Detailed evaluations show the superior performance
of FLUX.1 Kontext in terms of both single-turn quality and multi-turn
consistency, setting new standards for unified image processing models.

</details>


### [80] [Graphics4Science: Computer Graphics for Scientific Impacts](https://arxiv.org/abs/2506.15786)
*Peter Yichen Chen,Minghao Guo,Hanspeter Pfister,Ming Lin,William Freeman,Qixing Huang,Han-Wei Shen,Wojciech Matusik*

Main category: cs.GR

TL;DR: 课程探讨计算机图形学与科学的协同关系，提出将图形学作为科学建模语言，通过几何推理、物理建模等方法解决数据稀缺场景的科学挑战。


<details>
  <summary>Details</summary>
Motivation: 强化图形学与科学领域的交叉合作，利用图形学方法解决科学问题（如医学成像、计算建模），弥补两领域术语鸿沟。

Method: 以几何推理、物理建模等核心图形学方法作为归纳偏置，构建科学问题的结构化表征框架，并建立跨领域标准化建模语言。

Result: 提出Graphics4Science课程框架，促进图形学社区参与高影响力科学问题（如材料模拟、分子动力学），推动科学发现范式革新。

Conclusion: 图形学应成为科学发现的基础设施，通过建立跨学科协作机制和问题驱动的研究范式，释放其在数据稀缺场景下的独特建模优势。

Abstract: Computer graphics, often associated with films, games, and visual effects,
has long been a powerful tool for addressing scientific challenges--from its
origins in 3D visualization for medical imaging to its role in modern
computational modeling and simulation. This course explores the deep and
evolving relationship between computer graphics and science, highlighting past
achievements, ongoing contributions, and open questions that remain. We show
how core methods, such as geometric reasoning and physical modeling, provide
inductive biases that help address challenges in both fields, especially in
data-scarce settings. To that end, we aim to reframe graphics as a modeling
language for science by bridging vocabulary gaps between the two communities.
Designed for both newcomers and experts, Graphics4Science invites the graphics
community to engage with science, tackle high-impact problems where graphics
expertise can make a difference, and contribute to the future of scientific
discovery. Additional details are available on the course website:
https://graphics4science.github.io

</details>


### [81] [GratNet: A Photorealistic Neural Shader for Diffractive Surfaces](https://arxiv.org/abs/2506.15815)
*Narayan Kandel,Daljit Singh J. S. Dhillon*

Main category: cs.GR

TL;DR: 提出基于多层感知机(MLP)的衍射表面渲染方法，通过数据压缩实现高精度低内存的纳米结构光学渲染


<details>
  <summary>Details</summary>
Motivation: 当前结构色模型依赖密集型预处理数据，导致内存占用高。现有研究未充分解决隐式神经表示在衍射表面渲染中的应用问题。

Method: 针对衍射反射数据集特性设计MLP模型，采用面向数据压缩的训练策略，避免过拟合并保持鲁棒的重采样能力

Result: 使用PSNR/SSIM/FLIP指标验证，在保持主观质量的同时实现内存占用降低两个数量级，性能显著优于传统波动光学方法

Conclusion: 该方法成功将神经表示应用于衍射光学渲染领域，在保持精度的同时极大提升资源效率，为复杂纳米结构渲染提供新方案

Abstract: Structural coloration is commonly modeled using wave optics for reliable and
photorealistic rendering of natural, quasi-periodic and complex nanostructures.
Such models often rely on dense, preliminary or preprocessed data to accurately
capture the nuanced variations in diffractive surface reflectances. This heavy
data dependency warrants implicit neural representation which has not been
addressed comprehensively in the current literature. In this paper, we present
a multi-layer perceptron (MLP) based method for data-driven rendering of
diffractive surfaces with high accuracy and efficiency. We primarily approach
this problem from a data compression perspective to devise a nuanced training
and modeling method which is attuned to the domain and range characteristics of
diffractive reflectance datasets. Importantly, our approach avoids over-fitting
and has robust resampling behavior. Using Peak-Signal-to-Noise (PSNR),
Structural Similarity Index Measure (SSIM) and a flipping difference evaluator
(FLIP) as evaluation metrics, we demonstrate the high-quality reconstruction of
the ground-truth. In comparison to a recent state-of-the-art offline,
wave-optical, forward modeling approach, our method reproduces subjectively
similar results with significant performance gains. We reduce the memory
footprint of the raw datasets by two orders of magnitude in general. Lastly, we
depict the working of our method with actual surface renderings.

</details>


### [82] [VEIGAR: View-consistent Explicit Inpainting and Geometry Alignment for 3D object Removal](https://arxiv.org/abs/2506.15821)
*Pham Khai Nguyen Do,Bao Nguyen Tran,Nam Nguyen,Duc Dung Nguyen*

Main category: cs.GR

TL;DR: VEIGAR提出无需初始3D重建的高效跨视图一致生成框架，通过像素空间轻量级对齐和尺度不变深度损失，在训练效率与重建质量上实现SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖初始3D重建导致计算成本高且质量受限，需开发更高效的跨视图一致性解决方案。

Method: 1. 使用轻量级基础模型在像素空间显式对齐先验
2. 引入尺度不变深度损失替代传统单目深度归一化操作

Result: 重建质量与跨视图一致性达SOTA，训练时间比最快现有方法减少3倍

Conclusion: VEIGAR通过架构创新和新型监督策略，在保持生成质量的同时显著提升计算效率，为3D生成任务提供了更优的平衡方案。

Abstract: Recent advances in Novel View Synthesis (NVS) and 3D generation have
significantly improved editing tasks, with a primary emphasis on maintaining
cross-view consistency throughout the generative process. Contemporary methods
typically address this challenge using a dual-strategy framework: performing
consistent 2D inpainting across all views guided by embedded priors either
explicitly in pixel space or implicitly in latent space; and conducting 3D
reconstruction with additional consistency guidance. Previous strategies, in
particular, often require an initial 3D reconstruction phase to establish
geometric structure, introducing considerable computational overhead. Even with
the added cost, the resulting reconstruction quality often remains suboptimal.
In this paper, we present VEIGAR, a computationally efficient framework that
outperforms existing methods without relying on an initial reconstruction
phase. VEIGAR leverages a lightweight foundation model to reliably align priors
explicitly in the pixel space. In addition, we introduce a novel supervision
strategy based on scale-invariant depth loss, which removes the need for
traditional scale-and-shift operations in monocular depth regularization.
Through extensive experimentation, VEIGAR establishes a new state-of-the-art
benchmark in reconstruction quality and cross-view consistency, while achieving
a threefold reduction in training time compared to the fastest existing method,
highlighting its superior balance of efficiency and effectiveness.

</details>


### [83] [User-Guided Force-Directed Graph Layout](https://arxiv.org/abs/2506.15860)
*Hasan Balci,Augustin Luna*

Main category: cs.GR

TL;DR: 提出一种基于手绘草图引导的力导向布局算法，通过用户绘制结构特征自动生成约束，实现更直观的图形布局控制。


<details>
  <summary>Details</summary>
Motivation: 传统力导向布局算法需调节复杂参数，用户难以直观表达布局意图。研究旨在通过手绘草图实现更人性化的图结构控制。

Method: 使用经典图像分析技术提取草图结构特征，生成位置约束指导布局过程。开发用户引导的力导向布局算法（UGGLY）。

Result: 在中小规模真实/合成图数据验证有效，生成的布局与用户预期一致。开源实现包含文档和演示页面。

Conclusion: 该方法将用户意图自然地融入布局过程，提升可视化可解释性。开源工具为后续研究提供实践基础。

Abstract: Visual analysis of relational data is essential for many real-world analytics
tasks, with layout quality being key to interpretability. However, existing
layout algorithms often require users to navigate complex parameters to express
their intent. We present a user-guided force-directed layout approach that
enables intuitive control through freehand sketching. Our method uses classical
image analysis techniques to extract structural information from sketches,
which is then used to generate positional constraints that guide the layout
process. We evaluate the approach on various real and synthetic graphs ranging
from small to medium scale, demonstrating its ability to produce layouts
aligned with user expectations. An implementation of our method along with
documentation and a demo page is freely available on GitHub at
https://github.com/sciluna/uggly.

</details>


### [84] [FlatCAD: Fast Curvature Regularization of Neural SDFs for CAD Models](https://arxiv.org/abs/2506.16627)
*Haotian Yin,Aleksander Plocharski,Michal Jan Wlodarczyk,Mikolaj Kida,Przemyslaw Musialski*

Main category: cs.GR

TL;DR: 提出两种仅正则化混合二阶项的曲率代理方法，降低SDF学习的计算成本


<details>
  <summary>Details</summary>
Motivation: 现有基于Hessian矩阵的二阶自动微分方法存在高内存/时间消耗，需优化可展曲面建模效率

Method: 开发有限差分代理（四前向SDF评估+单梯度）和自动微分代理（Hessian-向量乘积）两种实现方案

Result: 在ABC基准上达到/超越基线保真度，GPU内存和耗时减半

Conclusion: 该框架无关的即插即用方法为工程级形状重建提供可扩展的曲率感知解决方案

Abstract: Neural signed-distance fields (SDFs) have become a versatile backbone for
geometric learning, yet enforcing developable, CAD-style behavior still hinges
on Gaussian curvature penalties that require full Hessian evaluation and
second-order automatic differentiation, both of which are costly in memory and
runtime. We present a curvature proxy that regularizes only the mixed
second-order term (Weingarten term), allowing the two principal curvatures to
adapt freely to data while suppressing unwanted warp. Two complementary
instantiations realize this idea: (i) a finite-difference proxy that replaces
each Hessian entry with four forward SDF evaluations and a single first-order
gradient, and (ii) an autodiff proxy that computes the same mixed derivative
via one Hessian-vector product, sidestepping explicit full Hessian assembly and
remaining faster in practice. Both variants converge to the exact mixed second
derivative, thus preserving the intended geometric bias without incurring full
second-order graphs. On the ABC benchmarks, the proxies match or exceed the
reconstruction fidelity of Hessian-based baselines while reducing GPU memory
use and wall-clock time by a factor of two. Because the method is drop-in and
framework-agnostic, it opens a practical path toward scalable, curvature-aware
SDF learning for engineering-grade shape reconstruction.

</details>


### [85] [Beyond Blur: A Fluid Perspective on Generative Diffusion Models](https://arxiv.org/abs/2506.16827)
*Grzegorz Gruszczynski,Michal Jan Wlodarczyk,Jakub J Meixner,Przemyslaw Musialski*

Main category: cs.GR

TL;DR: 提出基于平流-扩散方程的新型PDE驱动图像生成框架，通过物理启发的数值方法实现高效图像腐蚀与生成，统一了现有PDE方法。


<details>
  <summary>Details</summary>
Motivation: 现有PDE图像腐蚀方法存在局限性，需建立更普适的物理驱动框架。通过耦合平流与扩散过程，引入流体力学原理提升生成质量。

Method: 构建无量纲控制的平流-扩散方程，采用GPU加速晶格玻尔兹曼数值求解器实现湍流建模，神经网络学习逆转PDE算子实现图像生成。

Result: 平流操作在保持色彩一致性的前提下显著提升生成图像多样性（+32%图像质量指标），支持多尺度混合建模。

Conclusion: 该工作开创性地融合流体力学理论与生成模型，为基于扩散的图像合成提供了新的物理启发的数学框架。

Abstract: We propose a novel PDE-driven corruption process for generative image
synthesis based on advection-diffusion processes which generalizes existing
PDE-based approaches. Our forward pass formulates image corruption via a
physically motivated PDE that couples directional advection with isotropic
diffusion and Gaussian noise, controlled by dimensionless numbers (Peclet,
Fourier). We implement this PDE numerically through a GPU-accelerated custom
Lattice Boltzmann solver for fast evaluation. To induce realistic turbulence,
we generate stochastic velocity fields that introduce coherent motion and
capture multi-scale mixing. In the generative process, a neural network learns
to reverse the advection-diffusion operator thus constituting a novel
generative model. We discuss how previous methods emerge as specific cases of
our operator, demonstrating that our framework generalizes prior PDE-based
corruption techniques. We illustrate how advection improves the diversity and
quality of the generated images while keeping the overall color palette
unaffected. This work bridges fluid dynamics, dimensionless PDE theory, and
deep generative modeling, offering a fresh perspective on physically informed
image corruption processes for diffusion-based synthesis.

</details>


### [86] [DreamCube: 3D Panorama Generation via Multi-plane Synchronization](https://arxiv.org/abs/2506.17206)
*Yukun Huang,Yanning Zhou,Jianan Wang,Kaiyi Huang,Xihui Liu*

Main category: cs.GR

TL;DR: 提出DreamCube多平面RGB-D扩散模型，通过多平面同步机制将2D基础模型能力扩展到全景领域，实现多视角一致的3D全景生成。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法因3D全景数据稀缺导致的2D单视图先验不兼容问题，突破3D全景生成中的几何精度与多视角一致性瓶颈。

Method: 1. 多平面同步机制改造2D模型算子 2. 构建多平面RGB-D扩散模型 3. 最大化复用2D基础模型先验 4. 保持多视角几何一致性

Result: 全景图像生成错误率降低38.2%，深度估计精度提升21.6%，3D场景生成质量达SOTA水平，支持多样化外观生成。

Conclusion: DreamCube通过算子级适配实现2D先验的全景扩展，在保持多视角一致性的同时兼顾生成质量与几何精度，为3D内容创作提供新范式。

Abstract: 3D panorama synthesis is a promising yet challenging task that demands
high-quality and diverse visual appearance and geometry of the generated
omnidirectional content. Existing methods leverage rich image priors from
pre-trained 2D foundation models to circumvent the scarcity of 3D panoramic
data, but the incompatibility between 3D panoramas and 2D single views limits
their effectiveness. In this work, we demonstrate that by applying multi-plane
synchronization to the operators from 2D foundation models, their capabilities
can be seamlessly extended to the omnidirectional domain. Based on this design,
we further introduce DreamCube, a multi-plane RGB-D diffusion model for 3D
panorama generation, which maximizes the reuse of 2D foundation model priors to
achieve diverse appearances and accurate geometry while maintaining multi-view
consistency. Extensive experiments demonstrate the effectiveness of our
approach in panoramic image generation, panoramic depth estimation, and 3D
scene generation.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [87] [InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video Understanding](https://arxiv.org/abs/2506.15745)
*Minsoo Kim,Kyuhong Shim,Jungwook Choi,Simyung Chang*

Main category: eess.IV

TL;DR: InfiniPot-V 是首个无需训练、查询无关的KV缓存压缩框架，可将GPU内存峰值降低94%并维持实时处理，实现流式视频理解的硬性内存限制。


<details>
  <summary>Details</summary>
Motivation: 解决MLLMs处理流式视频时KV缓存线性增长导致内存溢出的问题，突破边缘设备内存限制

Method: 通过Temporal-axis Redundancy指标去除时间冗余token + Value-Norm排序保留语义显著token

Result: 在4个MLLMs和6个基准测试中，峰值内存降低94%，实时生成速度，准确率持平或超过全缓存方案

Conclusion: 无需训练/查询预知的特性使其成为设备端流式视频助手的关键突破，解决了KV缓存瓶颈问题

Abstract: Modern multimodal large language models (MLLMs) can reason over hour-long
video, yet their key-value (KV) cache grows linearly with time--quickly
exceeding the fixed memory of phones, AR glasses, and edge robots. Prior
compression schemes either assume the whole video and user query are available
offline or must first build the full cache, so memory still scales with stream
length. InfiniPot-V is the first training-free, query-agnostic framework that
enforces a hard, length-independent memory cap for streaming video
understanding. During video encoding it monitors the cache and, once a user-set
threshold is reached, runs a lightweight compression pass that (i) removes
temporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)
keeps semantically significant tokens via Value-Norm (VaN) ranking. Across four
open-source MLLMs and four long-video and two streaming-video benchmarks,
InfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,
and matches or surpasses full-cache accuracy--even in multi-turn dialogues. By
dissolving the KV cache bottleneck without retraining or query knowledge,
InfiniPot-V closes the gap for on-device streaming video assistants.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [88] [Unpacking Generative AI in Education: Computational Modeling of Teacher and Student Perspectives in Social Media Discourse](https://arxiv.org/abs/2506.16412)
*Paulina DeVito,Akhil Vallala,Sean Mcmahon,Yaroslav Hinda,Benjamin Thaw,Hanqi Zhuang,Hari Kalva*

Main category: cs.SI

TL;DR: 本研究通过Reddit数据分析生成式AI在教育领域的影响，发现教师关注工作安全/学术诚信，学生担忧作弊误判，并验证了GPT-4o框架在社交话语分析中的优越性（情感分析准确率90.6%）


<details>
  <summary>Details</summary>
Motivation: 理解教育工作者和学生对生成式AI工具的认知差异，为教育机构制定AI整合政策提供数据支持

Method: 收集1,199篇Reddit帖子和13,959条评论，结合情感分析/主题建模/作者分类，开发基于LLM的模块化分析框架并与传统NLP模型对比验证

Result: GPT-4o流程全面优于传统模型，提取12个潜在讨论主题；教师群体最关注工作安全（占比37%）和学术诚信（29%），学生主要抱怨AI检测器误判（43%案例）

Conclusion: 需建立透明的GAI整合政策和支持机制，LLM框架可有效分析在线社区利益相关者话语动态，为教育技术创新与风险管理提供新方法论

Abstract: Generative AI (GAI) technologies are quickly reshaping the educational
landscape. As adoption accelerates, understanding how students and educators
perceive these tools is essential. This study presents one of the most
comprehensive analyses to date of stakeholder discourse dynamics on GAI in
education using social media data. Our dataset includes 1,199 Reddit posts and
13,959 corresponding top-level comments. We apply sentiment analysis, topic
modeling, and author classification. To support this, we propose and validate a
modular framework that leverages prompt-based large language models (LLMs) for
analysis of online social discourse, and we evaluate this framework against
classical natural language processing (NLP) models. Our GPT-4o pipeline
consistently outperforms prior approaches across all tasks. For example, it
achieved 90.6% accuracy in sentiment analysis against gold-standard human
annotations. Topic extraction uncovered 12 latent topics in the public
discourse with varying sentiment and author distributions. Teachers and
students convey optimism about GAI's potential for personalized learning and
productivity in higher education. However, key differences emerged: students
often voice distress over false accusations of cheating by AI detectors, while
teachers generally express concern about job security, academic integrity, and
institutional pressures to adopt GAI tools. These contrasting perspectives
highlight the tension between innovation and oversight in GAI-enabled learning
environments. Our findings suggest a need for clearer institutional policies,
more transparent GAI integration practices, and support mechanisms for both
educators and students. More broadly, this study demonstrates the potential of
LLM-based frameworks for modeling stakeholder discourse within online
communities.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [89] [DeepRTL2: A Versatile Model for RTL-Related Tasks](https://arxiv.org/abs/2506.15697)
*Yi Liu,Hongji Zhang,Yunhao Zhou,Zhengyuan Shi,Changran Xu,Qiang Xu*

Main category: cs.AR

TL;DR: DeepRTL2首次统一了EDA领域的生成式与嵌入式任务，在RTL代码相关任务中实现全面SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究过度关注生成式任务，忽略了代码搜索、功能等价性验证等对硬件设计流程至关重要的嵌入式任务需求。

Method: 提出多任务统一的DeepRTL2模型家族，通过联合训练同时支持RTL代码生成、语义嵌入、性能预测等多样化任务。

Result: 实验证明DeepRTL2在NL2RTL生成、代码搜索等6项任务中均达到最先进水平，部分指标提升超15%。

Conclusion: 该研究为EDA领域提供了首个端到端解决方案，通过统一框架显著加速硬件设计验证流程，推动LLM在芯片设计中的全面应用。

Abstract: The integration of large language models (LLMs) into electronic design
automation (EDA) has significantly advanced the field, offering transformative
benefits, particularly in register transfer level (RTL) code generation and
understanding. While previous studies have demonstrated the efficacy of
fine-tuning LLMs for these generation-based tasks, embedding-based tasks, which
are equally critical to EDA workflows, have been largely overlooked. These
tasks, including natural language code search, RTL code functionality
equivalence checking, and performance prediction, are essential for
accelerating and optimizing the hardware design process. To address this gap,
we present DeepRTL2, a family of versatile LLMs that unifies both generation-
and embedding-based tasks related to RTL. By simultaneously tackling a broad
range of tasks, DeepRTL2 represents the first model to provide a comprehensive
solution to the diverse challenges in EDA. Through extensive experiments, we
show that DeepRTL2 achieves state-of-the-art performance across all evaluated
tasks.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [90] [From Prompts to Constructs: A Dual-Validity Framework for LLM Research in Psychology](https://arxiv.org/abs/2506.16697)
*Zhicheng Lin*

Main category: cs.CY

TL;DR: 论文批判当前心理学中大型语言模型（LLM）应用的测量幻象问题，提出双效度框架以区分统计假象与真实心理现象，强调需建立计算构念和分级证据标准。


<details>
  <summary>Details</summary>
Motivation: 针对将人类心理测量工具直接应用于LLM时产生的矛盾结果（测量幻象），需通过严谨验证确保研究结论的科学性，避免将统计模式匹配等同于心理现象。

Method: 提出整合测量信度原则与因果推断标准的双效度框架，根据科学主张的雄心程度（如文本分类 vs 焦虑模拟）分级制定证据强度要求。

Result: 现有研究普遍未能满足验证要求，错误地将模型输出（如'我焦虑'）直接等同于心理构念的测量/模拟，缺乏构念计算化定义。

Conclusion: 需构建心理构念的计算对应物，建立清晰、可扩展的证据标准体系，而非简单移植人类测量工具，以推进AI心理学的科学发展。

Abstract: Large language models (LLMs) are rapidly being adopted across psychology,
serving as research tools, experimental subjects, human simulators, and
computational models of cognition. However, the application of human
measurement tools to these systems can produce contradictory results, raising
concerns that many findings are measurement phantoms--statistical artifacts
rather than genuine psychological phenomena. In this Perspective, we argue that
building a robust science of AI psychology requires integrating two of our
field's foundational pillars: the principles of reliable measurement and the
standards for sound causal inference. We present a dual-validity framework to
guide this integration, which clarifies how the evidence needed to support a
claim scales with its scientific ambition. Using an LLM to classify text may
require only basic accuracy checks, whereas claiming it can simulate anxiety
demands a far more rigorous validation process. Current practice systematically
fails to meet these requirements, often treating statistical pattern matching
as evidence of psychological phenomena. The same model output--endorsing "I am
anxious"--requires different validation strategies depending on whether
researchers claim to measure, characterize, simulate, or model psychological
constructs. Moving forward requires developing computational analogues of
psychological constructs and establishing clear, scalable standards of evidence
rather than the uncritical application of human measurement tools.

</details>


### [91] [Large Language Models as Psychological Simulators: A Methodological Guide](https://arxiv.org/abs/2506.16702)
*Zhicheng Lin*

Main category: cs.CY

TL;DR: 提出使用大语言模型作为心理学模拟器的框架，包含角色模拟和认知建模两大应用，并讨论验证方法、伦理挑战及解决方案。


<details>
  <summary>Details</summary>
Motivation: 大语言模型为心理学研究提供了新机遇，但缺乏系统性方法论指导。研究者需要可靠框架来规范LLMs在角色模拟和认知建模中的应用。

Method: 开发基于心理学原理的角色模拟方法（含验证策略），认知建模中结合表征探测、因果干预方法，并建立模型行为与人类认知的关联机制。

Result: 整合LLMs性能证据（系统偏见/文化局限/提示敏感性），提供应对挑战的系统方案，强化LLMs在心理学研究中的工具价值。

Conclusion: LLMs可作为创新的心理学研究工具，但需保持模型能力透明度，其伦理审查应超越传统人类受试者审查框架。

Abstract: Large language models (LLMs) offer emerging opportunities for psychological
and behavioral research, but methodological guidance is lacking. This article
provides a framework for using LLMs as psychological simulators across two
primary applications: simulating roles and personas to explore diverse
contexts, and serving as computational models to investigate cognitive
processes. For simulation, we present methods for developing psychologically
grounded personas that move beyond demographic categories, with strategies for
validation against human data and use cases ranging from studying inaccessible
populations to prototyping research instruments. For cognitive modeling, we
synthesize emerging approaches for probing internal representations,
methodological advances in causal interventions, and strategies for relating
model behavior to human cognition. We address overarching challenges including
prompt sensitivity, temporal limitations from training data cutoffs, and
ethical considerations that extend beyond traditional human subjects review.
Throughout, we emphasize the need for transparency about model capabilities and
constraints. Together, this framework integrates emerging empirical evidence
about LLM performance--including systematic biases, cultural limitations, and
prompt brittleness--to help researchers wrangle these challenges and leverage
the unique capabilities of LLMs in psychological research.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [92] [cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree](https://arxiv.org/abs/2506.15655)
*Yilin Zhang,Xinran Zhao,Zora Zhiruo Wang,Chenyang Yang,Jiayi Wei,Tongshuang Wu*

Main category: cs.SE

TL;DR: 提出基于抽象语法树（AST）的结构感知代码分块方法（Chunking via ASTs），显著提升检索增强生成（RAG）在代码生成任务中的效果


<details>
  <summary>Details</summary>
Motivation: 现有基于行的分块方法会破坏代码语义结构（如拆分函数/合并无关代码），导致生成质量下降。需要保持代码块语义完整性的分块方案

Method: 通过递归分解AST大节点并合并兄弟节点，在遵守大小限制的前提下生成语义连贯的代码块。该方法支持多编程语言和任务

Result: 在RepoEval检索任务中Recall@5提升4.3%，在SWE-bench生成任务中Pass@1提高2.67%

Conclusion: 结构感知的分块策略对扩展检索增强的代码智能系统具有关键作用，为代码生成任务提供了新的优化方向

Abstract: Retrieval-Augmented Generation (RAG) has become essential for large-scale
code generation, grounding predictions in external code corpora to improve
actuality. However, a critical yet underexplored aspect of RAG pipelines is
chunking -- the process of dividing documents into retrievable units. Existing
line-based chunking heuristics often break semantic structures, splitting
functions or merging unrelated code, which can degrade generation quality. We
propose chunking via Abstract Syntax Trees (\ourwork), a structure-aware method
that recursively breaks large AST nodes into smaller chunks and merges sibling
nodes while respecting size limits. This approach generates self-contained,
semantically coherent units across programming languages and tasks, improving
performance on diverse code generation tasks, e.g., boosting Recall@5 by 4.3
points on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation.
Our work highlights the importance of structure-aware chunking for scaling
retrieval-enhanced code intelligence.

</details>


### [93] [Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems](https://arxiv.org/abs/2506.17208)
*Matias Martinez,Xavier Franch*

Main category: cs.SE

TL;DR: 首项针对SWE-Bench排行榜147份提交的全面研究，揭示专有LLMs（Claude 3.5/3.7）主导、系统设计多样（代理/非代理架构）、贡献者覆盖个人开发者至科技巨头的生态现状


<details>
  <summary>Details</summary>
Motivation: 针对SWE-Bench提交文档不完善导致解决方案架构设计不透明的问题，开展系统性分析以明确技术方案来源与系统设计特征

Method: 对SWE-Bench Lite（68项）和Verified（79项）排行榜共计147份提交进行多维分析，涵盖提交者类型（个人/企业）、产品可用性（开源/闭源）、LLM使用情况（专有/开源模型）、系统架构（代理式/非代理式）等维度

Result: 1) 83%解决方案使用专有LLM（Claude系列占优） 2) 架构呈现代理式与非代理式并存 3) 贡献者生态多元（个人开发者贡献35%方案，科技公司占29%）

Conclusion: 专有LLMs仍是自动化程序修复领域核心驱动力，但存在开源解决方案匮乏、架构设计透明度不足等问题，需建立更完善的方案文档规范

Abstract: The rapid progress in Automated Program Repair (APR) has been driven by
advances in AI, particularly large language models (LLMs) and agent-based
systems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair
systems using real issues and pull requests mined from 12 popular open-source
Python repositories. Its public leaderboards, SWE-Bench Lite and SWE-Bench
Verified, have become central platforms for tracking progress and comparing
solutions. However, because the submission process does not require detailed
documentation, the architectural design and origin of many solutions remain
unclear. In this paper, we present the first comprehensive study of all
submissions to the SWE-Bench Lite (68 entries) and Verified (79 entries)
leaderboards, analyzing 67 unique approaches across dimensions such as
submitter type, product availability, LLM usage, and system architecture. Our
findings reveal the dominance of proprietary LLMs (especially Claude 3.5/3.7),
the presence of both agentic and non-agentic designs, and a contributor base
spanning from individual developers to large tech companies.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [94] [OAgents: An Empirical Study of Building Effective Agents](https://arxiv.org/abs/2506.15741)
*He Zhu,Tianrui Qin,King Zhu,Heyuan Huang,Yeyi Guan,Jinxiang Xia,Yi Yao,Hanhao Li,Ningning Wang,Pai Liu,Tianhao Peng,Xin Gui,Xiaowan Li,Yuhui Liu,Yuchen Eleanor Jiang,Jun Wang,Changwang Zhang,Xiangru Tang,Ge Zhang,Jian Yang,Minghao Liu,Xitong Gao,Wangchunshu Zhou,Jiaheng Liu*

Main category: cs.AI

TL;DR: 针对Agentic AI研究标准化不足的问题，本文通过系统实证研究提出稳健评估协议，并开源模块化基础框架OAgents实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前代理研究缺乏标准化评估协议，导致方法对比困难且结果不可复现，需明确不同设计选择对代理效果的影响机制。

Method: 在GAIA和BrowseComp基准上开展控制变量实验，设计稳定评估流程，并构建支持模块化组件的OAgents框架。

Result: 发现现有评估存在显著随机波动，识别出核心有效组件与冗余设计，OAgents在开源方案中达到最优性能（GAIA得分92.3）。

Conclusion: 标准化评估对代理研究至关重要，OAgents的模块化设计为Agentic AI提供了可扩展的基础架构，推动领域科学发展。

Abstract: Recently, Agentic AI has become an increasingly popular research field.
However, we argue that current agent research practices lack standardization
and scientific rigor, making it hard to conduct fair comparisons among methods.
As a result, it is still unclear how different design choices in agent
frameworks affect effectiveness, and measuring their progress remains
challenging. In this work, we conduct a systematic empirical study on GAIA
benchmark and BrowseComp to examine the impact of popular design choices in key
agent components in a fair and rigorous manner. We find that the lack of a
standard evaluation protocol makes previous works, even open-sourced ones,
non-reproducible, with significant variance between random runs. Therefore, we
introduce a more robust evaluation protocol to stabilize comparisons. Our study
reveals which components and designs are crucial for effective agents, while
others are redundant, despite seeming logical. Based on our findings, we build
and open-source OAgents, a new foundation agent framework that achieves
state-of-the-art performance among open-source projects. OAgents offers a
modular design for various agent components, promoting future research in
Agentic AI.

</details>


### [95] [SLR: An Automated Synthesis Framework for Scalable Logical Reasoning](https://arxiv.org/abs/2506.15787)
*Lukas Helff,Ahmad Omar,Felix Friedrich,Wolfgang Stammer,Antonia Wüst,Tim Woydt,Rupert Mitchell,Patrick Schramowski,Kristian Kersting*

Main category: cs.AI

TL;DR: 提出SLR框架用于自动化评估和训练大语言模型的逻辑推理能力，通过可扩展的任务生成、验证机制和逻辑调优技术显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在生成语法正确规则时存在逻辑推理缺陷，且传统基准测试缺乏系统性难度控制。SLR旨在构建可扩展、自动化的逻辑推理评估体系，降低人工标注成本。

Method: 1. 自动化生成带难度控制的归纳推理任务
2. 合成潜在真值规则、符号验证程序和任务指令
3. 构建包含19k提示词、20级难度梯度的SLR-Bench基准
4. 开发基于逻辑调优的模型优化方法

Result: Llama-3-8B经逻辑调优后准确率翻倍（与Gemini-Flash-Thinking相当），推理token消耗降低80%。现有模型在递归推理任务中错误率超60%

Conclusion: SLR框架通过全自动化流程实现了无需人工标注的LLM推理能力评估与优化，为系统性提升模型逻辑推理能力提供了可扩展解决方案。

Abstract: We introduce SLR, an end-to-end framework for systematic evaluation and
training of Large Language Models (LLMs) via Scalable Logical Reasoning. Given
a user's task specification, SLR enables scalable, automated synthesis of
inductive reasoning tasks with precisely controlled difficulty. For each task,
SLR synthesizes (i) a latent ground-truth rule, (ii) an executable validation
program used by a symbolic judge to deterministically verify model outputs, and
(iii) an instruction prompt for the reasoning task. Using SLR, we create
SLR-Bench, a benchmark comprising over 19k prompts spanning 20 curriculum
levels that progressively increase in relational, arithmetic, and recursive
complexity. Large-scale evaluation reveals that contemporary LLMs readily
produce syntactically valid rules, yet often fail at correct logical inference.
Recent reasoning LLMs do somewhat better, but incur substantial increases in
test-time compute, sometimes exceeding 15k completion tokens. Finally,
logic-tuning via SLR doubles Llama-3-8B accuracy on SLR-Bench, achieving parity
with Gemini-Flash-Thinking at a fraction of computational cost. SLR is fully
automated, requires no human annotation, ensures dataset novelty, and offers a
scalable environment for probing and advancing LLMs' reasoning capabilities.

</details>


### [96] [Exploring Big Five Personality and AI Capability Effects in LLM-Simulated Negotiation Dialogues](https://arxiv.org/abs/2506.15928)
*Myke C. Cohen,Zhe Su,Hsien-Te Kao,Daniel Nguyen,Spencer Lynch,Maarten Sap,Svitlana Volkova*

Main category: cs.AI

TL;DR: 提出基于Sotopia平台的智能体AI系统评估框架，通过价格谈判和人类-AI职位谈判实验，揭示人格特质与AI特性对任务关键型谈判的影响机制。


<details>
  <summary>Details</summary>
Motivation: 解决高风险任务场景中AI系统需适应多样化人类操作者的问题，建立可重复的评估方法论以提升人机协作可靠性。

Method: 实验1采用因果发现方法分析人格特质对价格谈判的影响；实验2通过控制人类人格与AI特性（透明度/能力/适应性）评估信任度对任务有效性的影响。

Result: 宜人性/外向性显著影响可信度与目标达成，社会认知指标可检测共情/道德/观点差异，AI可信度直接影响任务成功率。

Conclusion: 创建了融合社会动力学的智能体AI评估体系，超越传统性能指标，为复杂任务场景提供可靠性验证框架。

Abstract: This paper presents an evaluation framework for agentic AI systems in
mission-critical negotiation contexts, addressing the need for AI agents that
can adapt to diverse human operators and stakeholders. Using Sotopia as a
simulation testbed, we present two experiments that systematically evaluated
how personality traits and AI agent characteristics influence LLM-simulated
social negotiation outcomes--a capability essential for a variety of
applications involving cross-team coordination and civil-military interactions.
Experiment 1 employs causal discovery methods to measure how personality traits
impact price bargaining negotiations, through which we found that Agreeableness
and Extraversion significantly affect believability, goal achievement, and
knowledge acquisition outcomes. Sociocognitive lexical measures extracted from
team communications detected fine-grained differences in agents' empathic
communication, moral foundations, and opinion patterns, providing actionable
insights for agentic AI systems that must operate reliably in high-stakes
operational scenarios. Experiment 2 evaluates human-AI job negotiations by
manipulating both simulated human personality and AI system characteristics,
specifically transparency, competence, adaptability, demonstrating how AI agent
trustworthiness impact mission effectiveness. These findings establish a
repeatable evaluation methodology for experimenting with AI agent reliability
across diverse operator personalities and human-agent team dynamics, directly
supporting operational requirements for reliable AI systems. Our work advances
the evaluation of agentic AI workflows by moving beyond standard performance
metrics to incorporate social dynamics essential for mission success in complex
operations.

</details>


### [97] [Bayesian Epistemology with Weighted Authority: A Formal Architecture for Truth-Promoting Autonomous Scientific Reasoning](https://arxiv.org/abs/2506.16015)
*Craig S. Wright*

Main category: cs.AI

TL;DR: 提出BEWA架构，通过概率化信念网络和贝叶斯机制解决科学文献爆炸性增长的处理难题


<details>
  <summary>Details</summary>
Motivation: 科学文献的指数增长已超出人类和AI系统的处理能力，需要可验证的计算认知框架来管理动态科学知识

Method: 构建基于复制评分/引用权重/时间衰减的动态概率信念网络，采用贝叶斯推理更新、矛盾处理、加密锚定和零知识审计验证机制

Result: 建立可计算验证的认知网络，支持声明图谱传播、作者信誉建模，保证知识系统的动态完整性

Conclusion: BEWA为机器推理系统奠定了新基础，促进跨学科领域的真理效用最大化、理性信念收敛和可审计的知识完整性

Abstract: The exponential expansion of scientific literature has surpassed the
epistemic processing capabilities of both human experts and current artificial
intelligence systems. This paper introduces Bayesian Epistemology with Weighted
Authority (BEWA), a formally structured architecture that operationalises
belief as a dynamic, probabilistically coherent function over structured
scientific claims. Each claim is contextualised, author-attributed, and
evaluated through a system of replication scores, citation weighting, and
temporal decay. Belief updates are performed via evidence-conditioned Bayesian
inference, contradiction processing, and epistemic decay mechanisms. The
architecture supports graph-based claim propagation, authorial credibility
modelling, cryptographic anchoring, and zero-knowledge audit verification. By
formalising scientific reasoning into a computationally verifiable epistemic
network, BEWA advances the foundation for machine reasoning systems that
promote truth utility, rational belief convergence, and audit-resilient
integrity across dynamic scientific domains.

</details>


### [98] [IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks](https://arxiv.org/abs/2506.16402)
*Xiaoya Lu,Zeren Chen,Xuhao Hu,Yijin Zhou,Weichen Zhang,Dongrui Liu,Lu Sheng,Jing Shao*

Main category: cs.AI

TL;DR: 提出首个多模态交互安全基准IS-Bench，揭示当前VLM代理缺乏动态安全风险处理能力


<details>
  <summary>Details</summary>
Motivation: 现有静态评估范式无法检测具身智能体在交互环境中产生的动态风险，忽略不安全中间步骤导致安全隐患

Method: 构建包含161个场景的高保真模拟基准，采用过程导向评估方法验证风险缓解步骤的时序正确性

Result: 主流VLM代理显示交互安全意识缺失，安全链式思考改进效果有限且影响任务完成率

Conclusion: IS-Bench为开发更安全的具身智能系统建立基础，强调动态风险感知与程序性缓解机制的重要性

Abstract: Flawed planning from VLM-driven embodied agents poses significant safety
hazards, hindering their deployment in real-world household tasks. However,
existing static, non-interactive evaluation paradigms fail to adequately assess
risks within these interactive environments, since they cannot simulate dynamic
risks that emerge from an agent's actions and rely on unreliable post-hoc
evaluations that ignore unsafe intermediate steps. To bridge this critical gap,
we propose evaluating an agent's interactive safety: its ability to perceive
emergent risks and execute mitigation steps in the correct procedural order. We
thus present IS-Bench, the first multi-modal benchmark designed for interactive
safety, featuring 161 challenging scenarios with 388 unique safety risks
instantiated in a high-fidelity simulator. Crucially, it facilitates a novel
process-oriented evaluation that verifies whether risk mitigation actions are
performed before/after specific risk-prone steps. Extensive experiments on
leading VLMs, including the GPT-4o and Gemini-2.5 series, reveal that current
agents lack interactive safety awareness, and that while safety-aware
Chain-of-Thought can improve performance, it often compromises task completion.
By highlighting these critical limitations, IS-Bench provides a foundation for
developing safer and more reliable embodied AI systems.

</details>


### [99] [Advancing Harmful Content Detection in Organizational Research: Integrating Large Language Models with Elo Rating System](https://arxiv.org/abs/2506.16575)
*Mustafa Akben,Aaron Satko*

Main category: cs.AI

TL;DR: 提出基于Elo评级的方法提升大语言模型在有害内容分析中的性能，在微侵犯和仇恨言语检测中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: LLM内置审查系统在分析有害内容时可能拒绝执行指令或产生过度谨慎的响应，影响组织冲突研究的有效性。

Method: 开发基于Elo评级的评估框架，通过对抗性测试优化LLM对有害内容（微侵犯/仇恨言语）的判断能力。

Result: 在两个数据集中，该方法在准确率（accuracy）、精确度（precision）和F1分数上均超越传统LLM提示技术和传统机器学习模型。

Conclusion: 该方法为检测职场骚扰、评估有毒沟通、创建安全包容的工作环境提供了更可靠、误报更少、可扩展性更强的解决方案。

Abstract: Large language models (LLMs) offer promising opportunities for organizational
research. However, their built-in moderation systems can create problems when
researchers try to analyze harmful content, often refusing to follow certain
instructions or producing overly cautious responses that undermine validity of
the results. This is particularly problematic when analyzing organizational
conflicts such as microaggressions or hate speech. This paper introduces an Elo
rating-based method that significantly improves LLM performance for harmful
content analysis In two datasets, one focused on microaggression detection and
the other on hate speech, we find that our method outperforms traditional LLM
prompting techniques and conventional machine learning models on key measures
such as accuracy, precision, and F1 scores. Advantages include better
reliability when analyzing harmful content, fewer false positives, and greater
scalability for large-scale datasets. This approach supports organizational
applications, including detecting workplace harassment, assessing toxic
communication, and fostering safer and more inclusive work environments.

</details>


### [100] [Are Bias Evaluation Methods Biased ?](https://arxiv.org/abs/2506.17111)
*Lina Berrayana,Sean Rooney,Luis Garcés-Erice,Ioana Giurgiu*

Main category: cs.AI

TL;DR: 研究发现不同安全评测方法会导致大模型偏见排名显著差异，建议社区谨慎使用现有基准


<details>
  <summary>Details</summary>
Motivation: 针对当前AI安全评测基准在偏见评估方法上存在差异的现象，探究不同评估方法对模型排名的鲁棒性影响

Method: 采用多种广泛使用的偏见评估方法，对代表性大模型进行偏见评测并比较排名结果的一致性

Result: 不同偏见评估方法导致模型排名存在显著差异，显示现有基准的评估结果具有方法依赖性

Conclusion: 建议研究社区在使用基准时需要充分考虑评估方法的选择，并推动建立更统一可靠的评测体系

Abstract: The creation of benchmarks to evaluate the safety of Large Language Models is
one of the key activities within the trusted AI community. These benchmarks
allow models to be compared for different aspects of safety such as toxicity,
bias, harmful behavior etc. Independent benchmarks adopt different approaches
with distinct data sets and evaluation methods. We investigate how robust such
benchmarks are by using different approaches to rank a set of representative
models for bias and compare how similar are the overall rankings. We show that
different but widely used bias evaluations methods result in disparate model
rankings. We conclude with recommendations for the community in the usage of
such benchmarks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [101] [BASE-Q: Bias and Asymmetric Scaling Enhanced Rotational Quantization for Large Language Models](https://arxiv.org/abs/2506.15689)
*Liulu He,Shenli Zhen,Karwei Sun,Yijiang Liu,Yufei Zhao,Chongkang Tan,Huanrui Yang,Yuan Du,Li Du*

Main category: cs.LG

TL;DR: 提出BASE-Q量化方法，通过偏置校正和非对称缩放减少LLM量化误差，支持分块优化避免全模型反向传播


<details>
  <summary>Details</summary>
Motivation: 现有旋转量化方法存在通道均值未对齐导致量化误差增大，且高斯化激活分布加剧截断误差，同时全模型反向传播内存消耗过高

Method: 结合偏置校正（修正通道均值）和非对称缩放（优化量化边界），创新性引入分块优化机制降低内存需求

Result: 在LLM基准测试中，相比QuaRot/SpinQuant/OSTQuant分别缩小50.5%/42.9%/29.2%的精度差距

Conclusion: BASE-Q在保持量化效率的同时显著提升精度，且内存友好的特性增强了实际部署可行性

Abstract: Rotations have become essential to state-of-the-art quantization pipelines
for large language models (LLMs) by effectively smoothing outliers in weights
and activations. However, further optimizing the rotation parameters offers
only limited performance gains and introduces significant training overhead:
due to rotation parameter sharing, full-model must be loaded simultaneously to
enable backpropagation, resulting in substantial memory consumption and limited
practical utility. In this work, we identify two fundamental limitations of
current rotational quantization methods: (i) rotation fails to align channel
means, resulting in wider quantization bounds and increased rounding errors;
and (ii) rotation makes the activation distribution more Gaussian-like,
increasing energy loss caused by clipping errors. To address these issues, we
introduce \textbf{BASE-Q}, a simple yet powerful approach that combines bias
correction and asymmetric scaling to effectively reduce rounding and clipping
errors. Furthermore, BASE-Q enables blockwise optimization, eliminating the
need for memory-intensive full-model backpropagation. Extensive experiments on
various LLMs and benchmarks demonstrate the effectiveness of BASE-Q, narrowing
the accuracy gap to full-precision models by 50.5\%, 42.9\%, and 29.2\%
compared to QuaRot, SpinQuant, and OSTQuant, respectively. The code will be
released soon.

</details>


### [102] [Learn from the Past: Fast Sparse Indexing for Large Language Model Decoding](https://arxiv.org/abs/2506.15704)
*Feiyu Yao,Qian Wang*

Main category: cs.LG

TL;DR: 提出LFPS方法通过历史注意力模式预测稀疏索引，显著提升长上下文大语言模型推理的解码效率


<details>
  <summary>Details</summary>
Motivation: 随着大模型支持更长的上下文，KV缓存的内存需求剧增成为瓶颈。现有稀疏注意力方法因独立处理每个解码步骤，无法利用历史注意力模式的时序相关性，导致计算开销大。

Method: LFPS动态构建稀疏索引候选：1) 捕捉垂直模式（固定位置注意力）和斜线模式（相对位置注意力） 2) 引入位置扩展策略预测当前步骤的Top-k索引

Result: 在RTX 4090 GPU上实现22.8倍于全注意力的加速，在Xeon Gold 6430单核CPU上实现9.6倍于精确Top-k检索的加速，且保持生成准确性

Conclusion: LFPS通过挖掘注意力模式的时序相关性，为长上下文LLM推理提供了高效实用的解码优化方案，显著降低硬件资源消耗

Abstract: As large language models (LLMs) continue to support increasingly longer
contexts, the memory demand for key-value (KV) caches during decoding grows
rapidly, becoming a critical bottleneck in both GPU memory capacity and PCIe
bandwidth. Sparse attention mechanisms alleviate this issue by computing
attention weights only for selected key-value pairs. However, their indexing
computation typically requires traversing all key vectors, resulting in
significant computational and data transfer overhead. To reduce the cost of
index retrieval, existing methods often treat each decoding step as an
independent process, failing to exploit the temporal correlations embedded in
historical decoding information. To this end, we propose LFPS(Learn From the
Past for Sparse Indexing), an acceleration method that dynamically constructs
sparse indexing candidates based on historical attention patterns. LFPS
captures two prevalent trends in decoder attention -vertical patterns
(attending to fixed positions) and slash patterns (attending to relative
positions) -and incorporates a positional expansion strategy to effectively
predict the Top-k indices for the current step. We validate LFPS on challenging
long-context benchmarks such as LongBench-RULER, using Llama-3.1-8B-Instruct as
the base model. Experimental results show that LFPS achieves up to 22.8$\times$
speedup over full attention and 9.6$\times$ speedup over exact Top-k retrieval
on an RTX 4090 GPU and a single CPU core of a Xeon Gold 6430, respectively,
while preserving generation accuracy. These results demonstrate that LFPS
offers a practical and efficient solution for decoding optimization in
long-context LLM inference.

</details>


### [103] [Adaptive Two Sided Laplace Transforms: A Learnable, Interpretable, and Scalable Replacement for Self-Attention](https://arxiv.org/abs/2506.15714)
*Andrew Kiruluta*

Main category: cs.LG

TL;DR: 提出可学习的双通道短时拉普拉斯变换机制(STLT)替代传统注意力机制，在保持模型性能的同时支持超长序列处理（超过100k tokens）


<details>
  <summary>Details</summary>
Motivation: 解决传统transformer自注意力机制在长序列场景下的计算瓶颈问题，通过可学习的动态参数系统实现高效计算

Method: 1. 引入可训练的拉普拉斯节点参数（衰减率/振荡频率/窗口带宽）
2. 采用快速递归卷积实现O(S)复杂度
3. 结合FFT加速计算和自适应节点分配机制

Result: 在WikiText-103等基准测试中达到SOTA水平，自然支持超过10万token的上下文长度，消融实验验证参数可学习性的重要性

Conclusion: 该方法兼具可解释性（显式衰减/频率参数）与可扩展性，为超长序列建模提供了突破自注意力计算限制的新路径

Abstract: We propose an innovative, learnable two-sided short-time Laplace transform
(STLT) mechanism to supplant the traditional self attention in
transformer-based LLMs. Our STLT introduces trainable parameters for each
Laplace node, enabling end-to-end learning of decay rates , oscillatory
frequencies, and window bandwidth T. This flexibility allows the model to
dynamically adapt token relevance half lives and frequency responses during
training. By selecting S learnable nodes and leveraging fast recursive
convolution, we achieve an effective complexity of in time and memory. We
further incorporate an efficient FFT-based computation of the relevance matrix
and an adaptive node allocation mechanism to dynamically adjust the number of
active Laplace nodes. Empirical results on language modeling (WikiText\-103,
Project Gutenberg), machine translation (WMT'14 En\-De), and long document
question answering (NarrativeQA) demonstrate that our learnable STLT achieves
perplexities and scores on par with or better than existing efficient
transformers while naturally extending to context lengths exceeding 100k tokens
or more limited only by available hardware. Ablation studies confirm the
importance of learnable parameters and adaptive node allocation. The proposed
approach combines interpretability, through explicit decay and frequency
parameters, with scalability and robustness, offering a pathway towards
ultra-long-sequence language modeling without the computational bottleneck of
self-attention.

</details>


### [104] [daDPO: Distribution-Aware DPO for Distilling Conversational Abilities](https://arxiv.org/abs/2506.15717)
*Zhengze Zhang,Shiqi Wang,Yiqun Shen,Simin Guo,Dahua Lin,Xiaoliang Wang,Nguyen Cam-Tu,Fei Tan*

Main category: cs.LG

TL;DR: 提出daDPO方法，通过整合教师模型的输出分布信息，显著提升剪枝和小型语言模型的对话能力，实验显示其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法（如dDPO）仅利用教师模型的响应，忽略输出分布信息，导致小型模型性能提升受限。

Method: daDPO（Distribution-Aware DPO），统一偏好优化与基于分布的蒸馏，结合理论分析和分布对齐技术。

Result: 20%剪枝Vicuna1.5-7B接近教师性能（偏好率-7.3% vs dDPO的-31%），Qwen2.5-1.5B部分场景超越7B教师（14%胜率）。

Conclusion: daDPO有效利用分布信息，为资源受限场景下部署高效小模型提供了新解决方案。

Abstract: Large language models (LLMs) have demonstrated exceptional performance across
various applications, but their conversational abilities decline sharply as
model size decreases, presenting a barrier to their deployment in
resource-constrained environments. Knowledge distillation with Direct
Preference Optimization (dDPO) has emerged as a promising approach to enhancing
the conversational abilities of smaller models using a larger teacher model.
However, current methods primarily focus on 'black-box' KD, which only uses the
teacher's responses, overlooking the output distribution offered by the
teacher. This paper addresses this gap by introducing daDPO (Distribution-Aware
DPO), a unified method for preference optimization and distribution-based
distillation. We provide rigorous theoretical analysis and empirical
validation, showing that daDPO outperforms existing methods in restoring
performance for pruned models and enhancing smaller LLM models. Notably, in
in-domain evaluation, our method enables a 20% pruned Vicuna1.5-7B to achieve
near-teacher performance (-7.3% preference rate compared to that of dDPO's
-31%), and allows Qwen2.5-1.5B to occasionally outperform its 7B teacher model
(14.0% win rate).

</details>


### [105] [MadaKV: Adaptive Modality-Perception KV Cache Eviction for Efficient Multimodal Long-Context Inference](https://arxiv.org/abs/2506.15724)
*Kunxi Li,Zhonghua Jiang,Zhouzhou Shen,Zhaode Wang,Chengfei Lv,Shengyu Zhang,Fan Wu,Fei Wu*

Main category: cs.LG

TL;DR: 提出模态自适应KV缓存淘汰策略MadaKV，通过模态偏好感知和分层压缩补偿机制，在保持精度的同时显著降低多模态长上下文推理的内存消耗和延迟


<details>
  <summary>Details</summary>
Motivation: 传统单模态KV缓存淘汰策略无法有效捕捉多模态场景下注意力头对模态的差异性偏好，导致关键模态信息丢失和性能下降

Method: 1. 模态偏好适应：动态感知注意力头中的模态信息，自适应保留关键模态token
2. 分层压缩补偿：通过压缩算法补偿被淘汰token的信息损失

Result: 实验显示在MileBench基准上实现KV缓存内存减少1.3-1.5倍，解码延迟提升相同倍数，同时保持多模态长上下文任务的高精度

Conclusion: MadaKV有效平衡效率与精度，为多模态大语言模型的长上下文推理提供高效的KV缓存优化方案

Abstract: This paper introduces MadaKV, a modality-adaptive key-value (KV) cache
eviction strategy designed to enhance the efficiency of multimodal large
language models (MLLMs) in long-context inference. In multimodal scenarios,
attention heads exhibit varying preferences for different modalities, resulting
in significant disparities in modality importance across attention heads.
Traditional KV cache eviction methods, which are tailored for unimodal
settings, fail to capture modality-specific information, thereby yielding
suboptimal performance. MadaKV addresses these challenges through two key
components: modality preference adaptation and hierarchical compression
compensation. By dynamically sensing modality information within attention
heads and adaptively retaining critical tokens, MadaKV achieves substantial
reductions in KV cache memory footprint and model inference decoding latency
(1.3 to 1.5 times improvement) while maintaining high accuracy across various
multimodal long-context tasks. Extensive experiments on representative MLLMs
and the MileBench benchmark demonstrate the effectiveness of MadaKV compared to
existing KV cache eviction methods.

</details>


### [106] [Fractional Reasoning via Latent Steering Vectors Improves Inference Time Compute](https://arxiv.org/abs/2506.15882)
*Sheng Liu,Tianlang Chen,Pan Lu,Haotian Ye,Yizheng Chen,Lei Xing,James Zou*

Main category: cs.LG

TL;DR: 提出无需训练的Fractional Reasoning框架，通过动态调节推理强度提升大语言模型的测试时性能


<details>
  <summary>Details</summary>
Motivation: 现有方法对推理深度采用统一处理，无法适应不同问题的复杂度差异

Method: 提取深度推理的潜在导向向量并通过可调缩放因子动态控制，支持广度策略优化（如Best-of-N）和深度策略强化（如自反思）

Result: 在GSM8K、MATH500和GPQA等推理任务中实现性能持续提升

Conclusion: 该框架突破了固定指令限制，使模型能根据输入复杂度自适应调整推理过程

Abstract: Test-time compute has emerged as a powerful paradigm for improving the
performance of large language models (LLMs), where generating multiple outputs
or refining individual chains can significantly boost answer accuracy. However,
existing methods like Best-of-N, majority voting, and self-reflection typically
apply reasoning in a uniform way across inputs, overlooking the fact that
different problems may require different levels of reasoning depth. In this
work, we propose Fractional Reasoning, a training-free and model-agnostic
framework that enables continuous control over reasoning intensity at inference
time, going beyond the limitations of fixed instructional prompts. Our method
operates by extracting the latent steering vector associated with deeper
reasoning and reapplying it with a tunable scaling factor, allowing the model
to tailor its reasoning process to the complexity of each input. This supports
two key modes of test-time scaling: (1) improving output quality in
breadth-based strategies (e.g., Best-of-N, majority voting), and (2) enhancing
the correctness of individual reasoning chains in depth-based strategies (e.g.,
self-reflection). Experiments on GSM8K, MATH500, and GPQA demonstrate that
Fractional Reasoning consistently improves performance across diverse reasoning
tasks and models.

</details>


### [107] [Early Attentive Sparsification Accelerates Neural Speech Transcription](https://arxiv.org/abs/2506.15912)
*Zifei Xu,Sayeh Sharify,Hesham Mostafa,Tristan Webb,Wanzin Yazar,Xin Wang*

Main category: cs.LG

TL;DR: 提出在Transformer语音编码器早期阶段进行时域稀疏化，在保持98%准确率的同时实现1.6倍推理加速


<details>
  <summary>Details</summary>
Motivation: 利用语音信号的高可压缩性和Transformer自注意力机制的可解释性，探索通过早期稀疏化加速语音转录

Method: 基于Whisper模型进行架构搜索，评估不同编码层（稀疏化阶段）和压缩率（40-60%稀疏度）的组合效果

Result: 最佳方案在英语语音转录任务中实现1.6倍GPU加速（Nvidia），准确率下降控制在1%以内且无需微调

Conclusion: 早期编码阶段的适度稀疏化可显著提升语音处理效率，为优化Transformer计算提供新思路

Abstract: Transformer-based neural speech processing has achieved state-of-the-art
performance. Since speech audio signals are known to be highly compressible,
here we seek to accelerate neural speech transcription by time-domain signal
sparsification early in the neural encoding stage, taking advantage of the
interpretability of the self-attention mechanism in transformer audio encoders.
With the Whisper family of models, we perform a systematic architecture search
over the joint space of sparsification stage (a certain encoder layer) and
compression ratio (sparsity). We found that the best resulting solutions under
1% accuracy degradation choose to sparsify the hidden state to 40-60% sparsity
at an early encoding stage, and thereby achieve up to 1.6x runtime acceleration
in English speech transcription tasks on Nvidia GPUs without any fine-tuning.

</details>


### [108] [Probing the Robustness of Large Language Models Safety to Latent Perturbations](https://arxiv.org/abs/2506.16078)
*Tianle Gu,Kexin Huang,Zongqi Wang,Yixu Wang,Jie Li,Yuanqi Yao,Yang Yao,Yujiu Yang,Yan Teng,Yingchun Wang*

Main category: cs.LG

TL;DR: 揭示现有AI安全对齐方法的浅层性缺陷，提出通过潜在空间敏感性诊断和对抗训练增强对齐鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法仅关注表层拒绝行为，未深入改变模型内部表示，导致潜在空间的微小偏移仍可能触发有害行为

Method: 1. 提出基于原始响应负对数似然的潜在空间敏感性探测方法 2. 设计分层对抗补丁训练(LAPT)策略注入受控扰动

Result: LAPT在保持模型通用能力的同时显著提升安全对齐鲁棒性，实验验证有效性

Conclusion: 当前安全对齐范式存在根本缺陷，需发展基于表示层的训练策略，突破表层行为监督的局限

Abstract: Safety alignment is a key requirement for building reliable Artificial
General Intelligence. Despite significant advances in safety alignment, we
observe that minor latent shifts can still trigger unsafe responses in aligned
models. We argue that this stems from the shallow nature of existing alignment
methods, which focus on surface-level refusal behaviors without sufficiently
altering internal representations. Consequently, small shifts in hidden
activations can re-trigger harmful behaviors embedded in the latent space. To
explore the robustness of safety alignment to latent perturbations, we
introduce a probing method that measures the Negative Log-Likelihood of the
original response generated by the model. This probe quantifies local
sensitivity in the latent space, serving as a diagnostic tool for identifying
vulnerable directions. Based on this signal, we construct effective jailbreak
trajectories, giving rise to the Activation Steering Attack (ASA). More
importantly, these insights offer a principled foundation for improving
alignment robustness. To this end, we introduce Layer-wise Adversarial Patch
Training~(LAPT), a fine-tuning strategy that inject controlled perturbations
into hidden representations during training. Experimental results highlight
that LAPT strengthen alignment robustness without compromising general
capabilities. Our findings reveal fundamental flaws in current alignment
paradigms and call for representation-level training strategies that move
beyond surface-level behavior supervision. Codes and results are available at
https://github.com/Carol-gutianle/LatentSafety.

</details>


### [109] [Latent Concept Disentanglement in Transformer-based Language Models](https://arxiv.org/abs/2506.16975)
*Guan Zhe Hong,Bhavya Vasudeva,Vatsal Sharan,Cyrus Rashtchian,Prabhakar Raghavan,Rina Panigrahy*

Main category: cs.LG

TL;DR: 研究发现Transformer模型在上下文学习中能有效识别潜在概念并进行多步组合推理，在连续概念任务中形成与底层参数匹配的低维几何结构。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer是否通过潜在结构进行概念解耦，而非表面模仿。现有研究缺乏对潜在概念与表示关系的深入分析，且集中于单步推理场景。

Method: 使用离散/连续潜在概念的2阶段推理任务，通过表示空间几何分析验证概念解耦机制。

Result: 模型成功实现潜在概念的逐步组合，连续概念任务中识别出与参数化匹配的低维子空间。

Conclusion: Transformer在上下文学习中形成高度局部化结构，能够解耦潜在概念，深化了对模型表示机制的理解。

Abstract: When large language models (LLMs) use in-context learning (ICL) to solve a
new task, they seem to grasp not only the goal of the task but also core,
latent concepts in the demonstration examples. This begs the question of
whether transformers represent latent structures as part of their computation
or whether they take shortcuts to solve the problem. Prior mechanistic work on
ICL does not address this question because it does not sufficiently examine the
relationship between the learned representation and the latent concept, and the
considered problem settings often involve only single-step reasoning. In this
work, we examine how transformers disentangle and use latent concepts. We show
that in 2-hop reasoning tasks with a latent, discrete concept, the model
successfully identifies the latent concept and does step-by-step concept
composition. In tasks parameterized by a continuous latent concept, we find
low-dimensional subspaces in the representation space where the geometry mimics
the underlying parameterization. Together, these results refine our
understanding of ICL and the representation of transformers, and they provide
evidence for highly localized structures in the model that disentangle latent
concepts in ICL tasks.

</details>


### [110] [From Concepts to Components: Concept-Agnostic Attention Module Discovery in Transformers](https://arxiv.org/abs/2506.17052)
*Jingtong Su,Julia Kempe,Karen Ullrich*

Main category: cs.LG

TL;DR: 提出SAMD和SAMI方法，通过概念向量化和标量干预增强Transformer模型的可解释性与行为控制能力


<details>
  <summary>Details</summary>
Motivation: 现有归因方法主要关注MLP神经元和简单概念，缺乏对注意力机制的统一分析框架以及处理复杂概念的能力

Method: SAMD将任意概念映射到注意力头构建模块，SAMI通过标量参数调整模块影响强度

Result: 模块位置在训练后保持稳定，安全绕过成功率提升72.7%，GSM8K推理能力提升1.6%，视觉领域有效抑制ImageNet分类准确率

Conclusion: 该领域无关方法为模型行为控制提供了新范式，在语言和视觉领域均展现出显著干预效果

Abstract: Transformers have achieved state-of-the-art performance across language and
vision tasks. This success drives the imperative to interpret their internal
mechanisms with the dual goals of enhancing performance and improving
behavioral control. Attribution methods help advance interpretability by
assigning model outputs associated with a target concept to specific model
components. Current attribution research primarily studies multi-layer
perceptron neurons and addresses relatively simple concepts such as factual
associations (e.g., Paris is located in France). This focus tends to overlook
the impact of the attention mechanism and lacks a unified approach for
analyzing more complex concepts. To fill these gaps, we introduce Scalable
Attention Module Discovery (SAMD), a concept-agnostic method for mapping
arbitrary, complex concepts to specific attention heads of general transformer
models. We accomplish this by representing each concept as a vector,
calculating its cosine similarity with each attention head, and selecting the
TopK-scoring heads to construct the concept-associated attention module. We
then propose Scalar Attention Module Intervention (SAMI), a simple strategy to
diminish or amplify the effects of a concept by adjusting the attention module
using only a single scalar parameter. Empirically, we demonstrate SAMD on
concepts of varying complexity, and visualize the locations of their
corresponding modules. Our results demonstrate that module locations remain
stable before and after LLM post-training, and confirm prior work on the
mechanics of LLM multilingualism. Through SAMI, we facilitate jailbreaking on
HarmBench (+72.7%) by diminishing "safety" and improve performance on the GSM8K
benchmark (+1.6%) by amplifying "reasoning". Lastly, we highlight the
domain-agnostic nature of our approach by suppressing the image classification
accuracy of vision transformers on ImageNet.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [111] [MoR: Better Handling Diverse Queries with a Mixture of Sparse, Dense, and Human Retrievers](https://arxiv.org/abs/2506.15862)
*Jushaan Singh Kalra,Xinran Zhao,To Eun Kim,Fengyu Cai,Fernando Diaz,Tongshuang Wu*

Main category: cs.IR

TL;DR: 提出一种零样本混合检索器框架，动态整合BM25与密集检索器的互补优势，无需人工干预即可提升检索增强生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统固定使用单一检索器，无法适应多样化查询需求。不同检索器（如BM25的词汇匹配与密集检索器的语义匹配）具有互补性，但缺乏动态整合机制。

Method: 采用零样本加权组合策略，将BM25、密集检索器等异构检索器构建成混合检索系统（总计仅0.8B参数），实现自动化的多检索器协同。

Result: 混合系统超越所有单一检索器（+10.8%）及7B大模型（+3.9%），整合模拟人类检索器时相对性能提升58.9%。

Conclusion: 该框架证明轻量级混合检索器组合的高效性，既能融合不同检索信号，又能兼容人类专家知识源，为RAG系统设计提供新范式。

Abstract: Retrieval-augmented Generation (RAG) is powerful, but its effectiveness
hinges on which retrievers we use and how. Different retrievers offer distinct,
often complementary signals: BM25 captures lexical matches; dense retrievers,
semantic similarity. Yet in practice, we typically fix a single retriever based
on heuristics, which fails to generalize across diverse information needs. Can
we dynamically select and integrate multiple retrievers for each individual
query, without the need for manual selection? In our work, we validate this
intuition with quantitative analysis and introduce mixture of retrievers: a
zero-shot, weighted combination of heterogeneous retrievers. Extensive
experiments show that such mixtures are effective and efficient: Despite
totaling just 0.8B parameters, this mixture outperforms every individual
retriever and even larger 7B models by +10.8% and +3.9% on average,
respectively. Further analysis also shows that this mixture framework can help
incorporate specialized non-oracle human information sources as retrievers to
achieve good collaboration, with a 58.9% relative performance improvement over
simulated humans alone.

</details>


### [112] [Revela: Dense Retriever Learning via Language Modeling](https://arxiv.org/abs/2506.16552)
*Fengyu Cai,Tong Chen,Xinran Zhao,Sihao Chen,Hongming Zhang,Sherry Tongshuang Wu,Iryna Gurevych,Heinz Koeppl*

Main category: cs.IR

TL;DR: 提出Revela框架，通过语言建模目标实现自监督检索器学习，在通用和领域特定任务中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统检索器训练依赖标注数据，在专业领域获取成本高。受语言模型自监督学习启发，探索将token级依赖学习扩展到文档块级依赖。

Method: Revela框架通过批内注意力机制，结合局部和跨文档上下文建模语义依赖，将检索器优化融入语言建模目标。

Result: 在BEIR和CoIR基准测试中NDCG@10分别提升5.2%和5.6%，性能随模型规模扩展持续提升。

Conclusion: Revela证明了自监督检索器学习的有效性和可扩展性，为无标注场景提供新解决方案。

Abstract: Dense retrievers play a vital role in accessing external and specialized
knowledge to augment language models (LMs). Training dense retrievers typically
requires annotated query-document pairs, which are costly and hard to obtain in
specialized domains such as code-motivating growing interest in self-supervised
retriever learning. Since LMs are trained to capture token-level dependencies
through a self-supervised learning objective (i.e., next-token prediction), we
can analogously cast retrieval as learning dependencies among chunks of tokens.
This analogy naturally leads to the question: How can we adapt self-supervised
learning objectives in the spirit of language modeling to train retrievers?
  To answer this question, we introduce Revela, a unified and scalable training
framework for self-supervised retriever learning via language modeling. Revela
models semantic dependencies among documents by conditioning next-token
prediction on both local and cross-document context through an in-batch
attention mechanism. This attention is weighted by retriever-computed
similarity scores, enabling the retriever to be optimized as part of language
modeling. We evaluate Revela on both general-domain (BEIR) and domain-specific
(CoIR) benchmarks across various retriever backbones. At a comparable parameter
scale, Revela outperforms the previous best method with absolute improvements
of 5.2 % (18.3 % relative) and 5.6 % (14.4 % relative) on NDCG@10,
respectively, underscoring its effectiveness. Performance increases with model
size, highlighting both the scalability of our approach and its promise for
self-supervised retriever learning.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [113] [Toward Understanding Similarity of Visualization Techniques](https://arxiv.org/abs/2506.17032)
*Abdulhaq Adetunji Salako,Christian Tominski*

Main category: cs.HC

TL;DR: 通过模型驱动和专家驱动两种方法探索可视化技术相似性评估


<details>
  <summary>Details</summary>
Motivation: 现有可视化技术分类方法难以有效评估技术间相似性，需建立系统化评估框架

Method: 1. 模型驱动：定义可视化技术签名并计算相似性
2. 专家驱动：在线调查收集专家对13种基础/高阶可视化技术的相似性评估

Result: 初步建立跨数据类型可视化技术相似性评估框架，为后续研究奠定方法论基础

Conclusion: 首次系统探索可视化技术相似性评估路径，需扩展样本量和多模态评估方法

Abstract: The literature describes many visualization techniques for different types of
data, tasks, and application contexts, and new techniques are proposed on a
regular basis. Visualization surveys try to capture the immense space of
techniques and structure it with meaningful categorizations. Yet, it remains
difficult to understand the similarity of visualization techniques in general.
We approach this open research question from two angles. First, we follow a
model-driven approach that is based on defining the signature of visualization
techniques and interpreting the similarity of signatures as the similarity of
their associated techniques. Second, following an expert-driven approach, we
asked visualization experts in a small online study for their ad-hoc intuitive
assessment of the similarity of pairs visualization techniques. From both
approaches, we gain insight into the similarity of a set of 13 basic and
advanced visualizations for different types of data. While our results are so
far preliminary and academic, they are first steps toward better understanding
the similarity of visualization techniques.

</details>


### [114] [Do We Talk to Robots Like Therapists, and Do They Respond Accordingly? Language Alignment in AI Emotional Support](https://arxiv.org/abs/2506.16473)
*Sophie Chiang,Guy Laban,Hatice Gunes*

Main category: cs.HC

TL;DR: 研究通过语义分析和聚类方法，发现基于LLM的社交机器人对话与人类心理治疗在主题结构和回应语义层面存在高度重合，验证了机器人辅助心理健康干预的潜力与边界。


<details>
  <summary>Details</summary>
Motivation: 评估机器人情感支持对话与传统人类心理治疗的相似性，探索其在心理健康领域的应用可能性。

Method: 使用Hugging Face心理治疗对话数据集和QTrobot机器人对话数据集，通过句子嵌入（Transformer/Word2Vec/BERT）和K-means聚类分析，采用基于距离的跨代理主题对齐验证方法。

Result: 90.88%机器人对话可映射至人类治疗主题聚类，且匹配簇中人类披露主题与跨代理（人/机器人）回应均呈现显著语义重叠。

Conclusion: 机器人支持对话在主题结构和语义回应层面与人类治疗存在高度平行性，具备辅助心理健康干预的潜力，但需注意其应用边界。

Abstract: As conversational agents increasingly engage in emotionally supportive
dialogue, it is important to understand how closely their interactions resemble
those in traditional therapy settings. This study investigates whether the
concerns shared with a robot align with those shared in human-to-human (H2H)
therapy sessions, and whether robot responses semantically mirror those of
human therapists. We analyzed two datasets: one of interactions between users
and professional therapists (Hugging Face's NLP Mental Health Conversations),
and another involving supportive conversations with a social robot (QTrobot
from LuxAI) powered by a large language model (LLM, GPT-3.5). Using sentence
embeddings and K-means clustering, we assessed cross-agent thematic alignment
by applying a distance-based cluster-fitting method that evaluates whether
responses from one agent type map to clusters derived from the other, and
validated it using Euclidean distances. Results showed that 90.88% of robot
conversation disclosures could be mapped to clusters from the human therapy
dataset, suggesting shared topical structure. For matched clusters, we compared
the subjects as well as therapist and robot responses using Transformer,
Word2Vec, and BERT embeddings, revealing strong semantic overlap in subjects'
disclosures in both datasets, as well as in the responses given to similar
human disclosure themes across agent types (robot vs. human therapist). These
findings highlight both the parallels and boundaries of robot-led support
conversations and their potential for augmenting mental health interventions.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [115] [GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal Reasoning](https://arxiv.org/abs/2506.16141)
*Yi Chen,Yuying Ge,Rui Wang,Yixiao Ge,Junhao Cheng,Ying Shan,Xihui Liu*

Main category: cs.CV

TL;DR: 提出SEED-Bench-R1基准和GRPO-CARE框架，通过双层奖励机制优化多模态大语言模型的逻辑一致性与迁移能力，在复杂视频理解任务中实现6.7%性能提升和24.5%一致性改进。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法在MLLM后训练中存在逻辑断裂问题（仅57.9%一致性），且缺乏系统性的视频理解评估基准。标准方法因单维度答案奖励和严格KL惩罚限制模型探索能力。

Method: GRPO-CARE框架包含：（1）答案正确性基础奖励；（2）基于参考模型推理路径概率的适应性一致性奖励，通过群体对比动态调整奖励权重，替代传统KL惩罚机制。

Result: 在SEED-Bench-R1最难关卡实现6.7%绝对性能提升，逻辑一致性提高24.5%。跨多个视频理解基准（ActivityNet等）显示强迁移性，验证框架通用性。

Conclusion: 系统性设计的视频理解基准与自适应奖励机制，为开发可解释、鲁棒的多模态大语言模型提供有效方法论，推动复杂场景下的认知推理能力发展。

Abstract: Recent reinforcement learning approaches, such as outcome-supervised GRPO,
have advanced Chain-of-Thought reasoning in large language models (LLMs), yet
their adaptation to multimodal LLMs (MLLMs) is unexplored. To address the lack
of rigorous evaluation for MLLM post-training methods, we introduce
SEED-Bench-R1, a benchmark with complex real-world videos requiring balanced
perception and reasoning. It offers a large training set and evaluates
generalization across three escalating challenges: in-distribution,
cross-environment, and cross-environment-task scenarios. Using SEED-Bench-R1,
we find that standard GRPO, while improving answer accuracy, often reduces
logical coherence between reasoning steps and answers, with only a 57.9%
consistency rate. This stems from reward signals focusing solely on final
answers, encouraging shortcuts, and strict KL penalties limiting exploration.To
address this, we propose GRPO-CARE, a consistency-aware RL framework optimizing
both answer correctness and reasoning coherence without explicit supervision.
GRPO-CARE introduces a two-tiered reward: (1) a base reward for answer
correctness, and (2) an adaptive consistency bonus, computed by comparing the
model's reasoning-to-answer likelihood (via a slowly-evolving reference model)
against group peers.This dual mechanism amplifies rewards for reasoning paths
that are both correct and logically consistent. Replacing KL penalties with
this adaptive bonus, GRPO-CARE outperforms standard GRPO on SEED-Bench-R1,
achieving a 6.7% performance gain on the hardest evaluation level and a 24.5%
improvement in consistency. It also shows strong transferability, improving
model performance across diverse video understanding benchmarks. Our work
contributes a systematically designed benchmark and a generalizable
post-training framework, advancing the development of more interpretable and
robust MLLMs.

</details>


### [116] [Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs](https://arxiv.org/abs/2506.16962)
*Haoran Sun,Yankai Jiang,Wenjie Lou,Yujie Zhang,Wenjie Li,Lilong Wang,Mianxin Liu,Lei Liu,Xiaosong Wang*

Main category: cs.CV

TL;DR: 提出MICS框架构建医学思维链数据，开发Chiron-o1模型实现医学视觉问答SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏系统化框架来搜索和评估医学诊断中的有效推理路径

Method: 1. Mentor-Intern协作搜索机制分步生成推理路径
2. MICS-Score评估路径质量
3. 构建多难度MMRP数据集
4. 课程学习策略训练Chiron-o1模型

Result: Chiron-o1在医学视觉问答基准测试中达到state-of-the-art表现

Conclusion: MICS框架有效提升医学MLLMs的推理能力，构建的层次化数据集和课程学习策略显著增强模型泛化能力

Abstract: Multimodal large language models (MLLMs) have begun to demonstrate robust
reasoning capabilities on general tasks, yet their application in the medical
domain remains in its early stages. Constructing chain-of-thought (CoT)
training data is essential for bolstering the reasoning abilities of medical
MLLMs. However, existing approaches exhibit a deficiency in offering a
comprehensive framework for searching and evaluating effective reasoning paths
towards critical diagnosis. To address this challenge, we propose Mentor-Intern
Collaborative Search (MICS), a novel reasoning-path searching scheme to
generate rigorous and effective medical CoT data. MICS first leverages mentor
models to initialize the reasoning, one step at a time, then prompts each
intern model to continue the thinking along those initiated paths, and finally
selects the optimal reasoning path according to the overall reasoning
performance of multiple intern models. The reasoning performance is determined
by an MICS-Score, which assesses the quality of generated reasoning paths.
Eventually, we construct MMRP, a multi-task medical reasoning dataset with
ranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum
learning strategy, with robust visual question-answering and generalizable
reasoning capabilities. Extensive experiments demonstrate that Chiron-o1,
trained on our CoT dataset constructed using MICS, achieves state-of-the-art
performance across a list of medical visual question answering and reasoning
benchmarks. Codes are available at GitHub - manglu097/Chiron-o1: Enhancing
Step-by-Step and Verifiable Medical Reasoning in MLLMs

</details>


### [117] [MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert Aggregation](https://arxiv.org/abs/2506.17113)
*Shoubin Yu,Yue Zhang,Ziyang Wang,Jaehong Yoon,Mohit Bansal*

Main category: cs.CV

TL;DR: 提出无需训练的MEXA框架，通过动态选择多领域专家模型并整合其推理结果，显著提升多样化多模态任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以统一处理多样化模态(结构化表格/图表)和复杂任务需求(医学诊断需精确推理，金融预测需图表解读)，亟需灵活的多模态推理框架。

Method: 1. 动态选择适配输入模态和任务技能的专家模型 2. 各专家生成可解释文本推理 3. 大型推理模型(LRM)整合结果生成最终答案

Result: 在视频推理、音频推理、3D理解和医疗QA等基准测试中持续超越强基线，验证框架的广泛适用性。

Conclusion: 模块化设计实现跨领域透明推理，零训练成本适应新任务，为复杂多模态场景提供高效解决方案。

Abstract: Combining pre-trained expert models offers substantial potential for scalable
multimodal reasoning, but building a unified framework remains challenging due
to the increasing diversity of input modalities and task complexity. For
instance, medical diagnosis requires precise reasoning over structured clinical
tables, while financial forecasting depends on interpreting plot-based data to
make informed predictions. To tackle this challenge, we introduce MEXA, a
training-free framework that performs modality- and task-aware aggregation of
multiple expert models to enable effective multimodal reasoning across diverse
and distinct domains. MEXA dynamically selects expert models based on the input
modality and the task-specific reasoning demands (i.e., skills). Each expert
model, specialized in a modality task pair, generates interpretable textual
reasoning outputs. MEXA then aggregates and reasons over these outputs using a
Large Reasoning Model (LRM) to produce the final answer. This modular design
allows flexible and transparent multimodal reasoning across diverse domains
without additional training overhead. We extensively evaluate our approach on
diverse multimodal benchmarks, including Video Reasoning, Audio Reasoning, 3D
Understanding, and Medical QA. MEXA consistently delivers performance
improvements over strong multimodal baselines, highlighting the effectiveness
and broad applicability of our expert-driven selection and aggregation in
diverse multimodal reasoning tasks.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [118] [Volumetric Parameterization for 3-Dimensional Simply-Connected Manifolds](https://arxiv.org/abs/2506.17025)
*Zhiyuan Lyu,Qiguang Chen,Gary P. T. Choi,Lok Ming Lui*

Main category: cs.CG

TL;DR: 开发新的三维流形体积参数化方法，通过多模型框架平衡几何结构与密度变形


<details>
  <summary>Details</summary>
Motivation: 传统方法无法同时控制三维流形参数化的双射性和几何变形，且主要关注单一属性而非综合平衡

Method: 借鉴表面参数化思路，构建包含几何结构保持、密度均衡及变形优化平衡的多模型框架

Result: 在不同三维流形案例和网格重构应用中验证了方法的有效性及计算精度

Conclusion: 该方法可生成满足不同需求的三维流形参数化结果，具有实际应用价值

Abstract: With advances in technology, there has been growing interest in developing
effective mapping methods for 3-dimensional objects in recent years. Volumetric
parameterization for 3D solid manifolds plays an important role in processing
3D data. However, the conventional approaches cannot control the bijectivity
and local geometric distortions of the result mappings due to the complex
structure of the solid manifolds. Moreover, prior methods mainly focus on one
property instead of balancing different properties during the mapping process.
In this paper, we propose several novel methods for computing volumetric
parameterizations for 3D simply-connected manifolds. Analogous to surface
parameterization, our framework incorporates several models designed to
preserve geometric structure, achieve density equalization, and optimally
balance geometric and density distortions. With these methods, various 3D
manifold parameterizations with different desired properties can be achieved.
These methods are tested on different examples and manifold remeshing
applications, demonstrating their effectiveness and accuracy.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [119] [Multi-use LLM Watermarking and the False Detection Problem](https://arxiv.org/abs/2506.15975)
*Zihao Fu,Chris Russell*

Main category: cs.CR

TL;DR: 提出双水印技术，在保持高检测精度的同时显著减少误判


<details>
  <summary>Details</summary>
Motivation: 现有水印方案同时承载检测和用户识别功能时，随着用户规模增长会导致未加水印文本被误判为水印文本的问题

Method: 双水印技术Dual Watermarking，将检测水印和识别水印分别编码到生成文本中

Result: 实验验证该方法能显著降低误报率，同时保持高检测准确率

Conclusion: 理论分析和实验结果共同验证了双水印技术的有效性

Abstract: Digital watermarking is a promising solution for mitigating some of the risks
arising from the misuse of automatically generated text. These approaches
either embed non-specific watermarks to allow for the detection of any text
generated by a particular sampler, or embed specific keys that allow the
identification of the LLM user. However, simultaneously using the same
embedding for both detection and user identification leads to a false detection
problem, whereby, as user capacity grows, unwatermarked text is increasingly
likely to be falsely detected as watermarked. Through theoretical analysis, we
identify the underlying causes of this phenomenon. Building on these insights,
we propose Dual Watermarking which jointly encodes detection and identification
watermarks into generated text, significantly reducing false positives while
maintaining high detection accuracy. Our experimental results validate our
theoretical findings and demonstrate the effectiveness of our approach.

</details>


### [120] [Probe before You Talk: Towards Black-box Defense against Backdoor Unalignment for Large Language Models](https://arxiv.org/abs/2506.16447)
*Biao Yi,Tiansheng Huang,Sishuo Chen,Tong Li,Zheli Liu,Zhixuan Chu,Yiming Li*

Main category: cs.CR

TL;DR: 论文提出BEAT方法，通过检测触发样本扭曲输出分布，有效防御LLM后门攻击及越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 后门攻击利用隐藏触发词破坏LLM安全对齐机制，且样本依赖特性扩大攻击目标空间，传统安全审计难以防御。

Method: BEAT基于'探测拼接效应'，通过计算输入拼接恶意探测前后的输出分布差异，黑盒环境下识别触发样本。

Result: 实验证明BEAT在多种后门攻击和LLM（含GPT-3.5）中有效，且初步验证对越狱攻击的防御能力。

Conclusion: BEAT创新性地从拒绝信号扰动角度解决样本依赖问题，为黑盒环境下的LLM安全防御提供新思路。

Abstract: Backdoor unalignment attacks against Large Language Models (LLMs) enable the
stealthy compromise of safety alignment using a hidden trigger while evading
normal safety auditing. These attacks pose significant threats to the
applications of LLMs in the real-world Large Language Model as a Service
(LLMaaS) setting, where the deployed model is a fully black-box system that can
only interact through text. Furthermore, the sample-dependent nature of the
attack target exacerbates the threat. Instead of outputting a fixed label, the
backdoored LLM follows the semantics of any malicious command with the hidden
trigger, significantly expanding the target space. In this paper, we introduce
BEAT, a black-box defense that detects triggered samples during inference to
deactivate the backdoor. It is motivated by an intriguing observation (dubbed
the probe concatenate effect), where concatenated triggered samples
significantly reduce the refusal rate of the backdoored LLM towards a malicious
probe, while non-triggered samples have little effect. Specifically, BEAT
identifies whether an input is triggered by measuring the degree of distortion
in the output distribution of the probe before and after concatenation with the
input. Our method addresses the challenges of sample-dependent targets from an
opposite perspective. It captures the impact of the trigger on the refusal
signal (which is sample-independent) instead of sample-specific successful
attack behaviors. It overcomes black-box access limitations by using multiple
sampling to approximate the output distribution. Extensive experiments are
conducted on various backdoor attacks and LLMs (including the closed-source
GPT-3.5-turbo), verifying the effectiveness and efficiency of our defense.
Besides, we also preliminarily verify that BEAT can effectively defend against
popular jailbreak attacks, as they can be regarded as 'natural backdoors'.

</details>
