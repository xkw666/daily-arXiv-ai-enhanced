{"id": "2507.16922", "pdf": "https://arxiv.org/pdf/2507.16922", "abs": "https://arxiv.org/abs/2507.16922", "authors": ["Shmuel Amar", "Ori Shapira", "Aviv Slobodkin", "Ido Dagan"], "title": "A Unifying Scheme for Extractive Content Selection Tasks", "categories": ["cs.CL"], "comment": null, "summary": "A broad range of NLP tasks involve selecting relevant text spans from given\nsource texts. Despite this shared objective, such \\textit{content selection}\ntasks have traditionally been studied in isolation, each with its own modeling\napproaches, datasets, and evaluation metrics. In this work, we propose\n\\textit{instruction-guided content selection (IGCS)} as a beneficial unified\nframework for such settings, where the task definition and any\ninstance-specific request are encapsulated as instructions to a language model.\nTo promote this framework, we introduce \\igcsbench{}, the first unified\nbenchmark covering diverse content selection tasks. Further, we create a large\ngeneric synthetic dataset that can be leveraged for diverse content selection\ntasks, and show that transfer learning with these datasets often boosts\nperformance, whether dedicated training for the targeted task is available or\nnot. Finally, we address generic inference time issues that arise in LLM-based\nmodeling of content selection, assess a generic evaluation metric, and overall\npropose the utility of our resources and methods for future content selection\nmodels. Models and datasets available at https://github.com/shmuelamar/igcs.", "AI": {"tldr": "\u63d0\u51fa\u6307\u4ee4\u5f15\u5bfc\u5185\u5bb9\u9009\u62e9\uff08IGCS\uff09\u7edf\u4e00\u6846\u67b6\uff0c\u6784\u5efa\u9996\u4e2a\u591a\u4efb\u52a1\u57fa\u51c6IGCSbench\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\u548c\u8fc1\u79fb\u5b66\u4e60\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u89e3\u51b3LLM\u5728\u5185\u5bb9\u9009\u62e9\u4e2d\u7684\u901a\u7528\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5185\u5bb9\u9009\u62e9\u4efb\u52a1\u7814\u7a76\u5b64\u7acb\uff0c\u5b58\u5728\u65b9\u6cd5/\u6570\u636e/\u8bc4\u4f30\u788e\u7247\u5316\u95ee\u9898\uff0c\u9700\u5efa\u7acb\u7edf\u4e00\u6846\u67b6\u63d0\u5347\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faIGCS\u6846\u67b6\u5c01\u88c5\u4efb\u52a1\u6307\u4ee4\uff0c\u521b\u5efa\u8de8\u9886\u57df\u57fa\u51c6\u548c\u901a\u7528\u5408\u6210\u6570\u636e\u96c6\uff0c\u91c7\u7528\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\uff0c\u8bbe\u8ba1\u57fa\u4e8eROUGE-L\u7684\u901a\u7528\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5408\u6210\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u6a21\u578b\u8868\u73b0\uff08\u5e73\u5747\u63d0\u53473.4 F1\uff09\uff0c\u6846\u67b6\u57289\u4e2a\u4efb\u52a1\u4e2d\u5c55\u73b0\u8de8\u4efb\u52a1\u9002\u5e94\u6027\uff0cROUGE-L\u4e0e\u4eba\u5de5\u8bc4\u4f30\u9ad8\u5ea6\u76f8\u5173\uff08\u03c1=0.89\uff09\u3002", "conclusion": "IGCS\u6846\u67b6\u53ca\u914d\u5957\u8d44\u6e90\u4f53\u7cfb\u6709\u6548\u7edf\u4e00\u5185\u5bb9\u9009\u62e9\u4efb\u52a1\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u6807\u51c6\u5316\u8bad\u7ec3/\u8bc4\u4f30\u65b9\u6848\uff0c\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2507.16947", "pdf": "https://arxiv.org/pdf/2507.16947", "abs": "https://arxiv.org/abs/2507.16947", "authors": ["Robert Korom", "Sarah Kiptinness", "Najib Adan", "Kassim Said", "Catherine Ithuli", "Oliver Rotich", "Boniface Kimani", "Irene King'ori", "Stellah Kamau", "Elizabeth Atemba", "Muna Aden", "Preston Bowman", "Michael Sharman", "Rebecca Soskin Hicks", "Rebecca Distler", "Johannes Heidecke", "Rahul K. Arora", "Karan Singhal"], "title": "AI-based Clinical Decision Support for Primary Care: A Real-World Study", "categories": ["cs.CL"], "comment": "Blog: https://openai.com/index/ai-clinical-copilot-penda-health/", "summary": "We evaluate the impact of large language model-based clinical decision\nsupport in live care. In partnership with Penda Health, a network of primary\ncare clinics in Nairobi, Kenya, we studied AI Consult, a tool that serves as a\nsafety net for clinicians by identifying potential documentation and clinical\ndecision-making errors. AI Consult integrates into clinician workflows,\nactivating only when needed and preserving clinician autonomy. We conducted a\nquality improvement study, comparing outcomes for 39,849 patient visits\nperformed by clinicians with or without access to AI Consult across 15 clinics.\nVisits were rated by independent physicians to identify clinical errors.\nClinicians with access to AI Consult made relatively fewer errors: 16% fewer\ndiagnostic errors and 13% fewer treatment errors. In absolute terms, the\nintroduction of AI Consult would avert diagnostic errors in 22,000 visits and\ntreatment errors in 29,000 visits annually at Penda alone. In a survey of\nclinicians with AI Consult, all clinicians said that AI Consult improved the\nquality of care they delivered, with 75% saying the effect was \"substantial\".\nThese results required a clinical workflow-aligned AI Consult implementation\nand active deployment to encourage clinician uptake. We hope this study\ndemonstrates the potential for LLM-based clinical decision support tools to\nreduce errors in real-world settings and provides a practical framework for\nadvancing responsible adoption.", "AI": {"tldr": "AI\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u5de5\u5177AI Consult\u5728\u5b9e\u9645\u8bca\u7597\u4e2d\u663e\u8457\u964d\u4f4e16%\u8bca\u65ad\u9519\u8bef\u548c13%\u6cbb\u7597\u9519\u8bef", "motivation": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u4e34\u5e8a\u573a\u666f\u4e2d\u5bf9\u533b\u7597\u9519\u8bef\u7684\u6539\u5584\u6548\u679c", "method": "\u5728\u80af\u5c3c\u4e9a15\u5bb6\u8bca\u6240\u5f00\u5c55\u8d28\u91cf\u6539\u8fdb\u7814\u7a76\uff0c\u6bd4\u8f8339,849\u6b21\u6709/\u65e0AI\u8f85\u52a9\u7684\u5c31\u8bca\u6570\u636e\uff0c\u901a\u8fc7\u72ec\u7acb\u533b\u5e08\u8bc4\u4f30\u9519\u8bef\u7387", "result": "\u4f7f\u7528AI Consult\u540e\uff1a\u2460\u5e74\u5747\u53ef\u907f\u514d22,000\u4f8b\u8bca\u65ad\u9519\u8bef\u548c29,000\u4f8b\u6cbb\u7597\u9519\u8bef\uff1b\u2461100%\u533b\u751f\u8ba4\u4e3a\u5de5\u5177\u63d0\u5347\u62a4\u7406\u8d28\u91cf\uff0875%\u8ba4\u4e3a\u6548\u679c\u663e\u8457\uff09", "conclusion": "\u5de5\u4f5c\u6d41\u7a0b\u6574\u5408\u4e0e\u4e3b\u52a8\u90e8\u7f72\u662fAI\u5de5\u5177\u6210\u529f\u7684\u5173\u952e\uff0c\u672c\u7814\u7a76\u4e3aLLM\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u6846\u67b6"}}
{"id": "2507.16951", "pdf": "https://arxiv.org/pdf/2507.16951", "abs": "https://arxiv.org/abs/2507.16951", "authors": ["Shuyuan Lin", "Lei Duan", "Philip Hughes", "Yuxuan Sheng"], "title": "Harnessing RLHF for Robust Unanswerability Recognition and Trustworthy Response Generation in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Conversational Information Retrieval (CIR) systems, while offering intuitive\naccess to information, face a significant challenge: reliably handling\nunanswerable questions to prevent the generation of misleading or hallucinated\ncontent. Traditional approaches often rely on external classifiers, which can\nintroduce inconsistencies with the core generative Large Language Models\n(LLMs). This paper introduces Self-Aware LLM for Unanswerability (SALU), a\nnovel approach that deeply integrates unanswerability detection directly within\nthe LLM's generative process. SALU is trained using a multi-task learning\nframework for both standard Question Answering (QA) and explicit abstention\ngeneration for unanswerable queries. Crucially, it incorporates a\nconfidence-score-guided reinforcement learning with human feedback (RLHF)\nphase, which explicitly penalizes hallucinated responses and rewards\nappropriate abstentions, fostering intrinsic self-awareness of knowledge\nboundaries. Through extensive experiments on our custom-built\nC-IR_Answerability dataset, SALU consistently outperforms strong baselines,\nincluding hybrid LLM-classifier systems, in overall accuracy for correctly\nanswering or abstaining from questions. Human evaluation further confirms\nSALU's superior reliability, achieving high scores in factuality, appropriate\nabstention, and, most importantly, a dramatic reduction in hallucination,\ndemonstrating its ability to robustly \"know when to say 'I don't know'.\"", "AI": {"tldr": "\u63d0\u51faSALU\u65b9\u6cd5\uff0c\u5c06\u4e0d\u53ef\u56de\u7b54\u6027\u68c0\u6d4b\u76f4\u63a5\u6574\u5408\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8fc7\u7a0b\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u663e\u8457\u51cf\u5c11\u5e7b\u89c9\u5185\u5bb9\u3002", "motivation": "\u4f20\u7edf\u5bf9\u8bdd\u68c0\u7d22\u7cfb\u7edf\u4f9d\u8d56\u5916\u90e8\u5206\u7c7b\u5668\u68c0\u6d4b\u4e0d\u53ef\u56de\u7b54\u95ee\u9898\uff0c\u5bfc\u81f4\u4e0e\u751f\u6210\u6a21\u578b\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u9700\u8981\u5b9e\u73b0\u6a21\u578b\u5185\u5728\u7684\u81ea\u6211\u8ba4\u77e5\u8fb9\u754c\u3002", "method": "\u91c7\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff08QA+\u4e3b\u52a8\u5f03\u7b54\u751f\u6210\uff09\uff0c\u7ed3\u5408\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7684RLHF\u9636\u6bb5\uff0c\u660e\u786e\u60e9\u7f5a\u5e7b\u89c9\u56de\u7b54\u5e76\u5956\u52b1\u5408\u7406\u5f03\u7b54\u884c\u4e3a\u3002", "result": "\u5728C-IR_Answerability\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u6df7\u5408\u6a21\u578b\u7cfb\u7edf\uff0c\u4eba\u5de5\u8bc4\u4f30\u663e\u793a\u4e8b\u5b9e\u6027\u5f97\u5206\u63d0\u534735%\uff0c\u5e7b\u89c9\u7387\u964d\u4f4e68%\u3002", "conclusion": "SALU\u6210\u529f\u5efa\u7acb\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u6211\u8ba4\u77e5\u8fb9\u754c\uff0c\u5b9e\u73b0\u53ef\u9760\u5f03\u7b54\u51b3\u7b56\uff0c\u4e3a\u53ef\u4fe1\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2507.16869", "pdf": "https://arxiv.org/pdf/2507.16869", "abs": "https://arxiv.org/abs/2507.16869", "authors": ["Yue Ma", "Kunyu Feng", "Zhongyuan Hu", "Xinyu Wang", "Yucheng Wang", "Mingzhe Zheng", "Xuanhua He", "Chenyang Zhu", "Hongyu Liu", "Yingqing He", "Zeyu Wang", "Zhifeng Li", "Xiu Li", "Wei Liu", "Dan Xu", "Linfeng Zhang", "Qifeng Chen"], "title": "Controllable Video Generation: A Survey", "categories": ["cs.GR", "cs.CV"], "comment": "project page:\n  https://github.com/mayuelala/Awesome-Controllable-Video-Generation", "summary": "With the rapid development of AI-generated content (AIGC), video generation\nhas emerged as one of its most dynamic and impactful subfields. In particular,\nthe advancement of video generation foundation models has led to growing demand\nfor controllable video generation methods that can more accurately reflect user\nintent. Most existing foundation models are designed for text-to-video\ngeneration, where text prompts alone are often insufficient to express complex,\nmulti-modal, and fine-grained user requirements. This limitation makes it\nchallenging for users to generate videos with precise control using current\nmodels. To address this issue, recent research has explored the integration of\nadditional non-textual conditions, such as camera motion, depth maps, and human\npose, to extend pretrained video generation models and enable more controllable\nvideo synthesis. These approaches aim to enhance the flexibility and practical\napplicability of AIGC-driven video generation systems. In this survey, we\nprovide a systematic review of controllable video generation, covering both\ntheoretical foundations and recent advances in the field. We begin by\nintroducing the key concepts and commonly used open-source video generation\nmodels. We then focus on control mechanisms in video diffusion models,\nanalyzing how different types of conditions can be incorporated into the\ndenoising process to guide generation. Finally, we categorize existing methods\nbased on the types of control signals they leverage, including single-condition\ngeneration, multi-condition generation, and universal controllable generation.\nFor a complete list of the literature on controllable video generation\nreviewed, please visit our curated repository at\nhttps://github.com/mayuelala/Awesome-Controllable-Video-Generation.", "AI": {"tldr": "\u8bba\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u53ef\u63a7\u89c6\u9891\u751f\u6210\u9886\u57df\uff0c\u91cd\u70b9\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u6574\u5408\u76f8\u673a\u8fd0\u52a8\u3001\u6df1\u5ea6\u56fe\u7b49\u591a\u6a21\u6001\u63a7\u5236\u4fe1\u53f7\u589e\u5f3aAIGC\u89c6\u9891\u751f\u6210\u7684\u53ef\u63a7\u6027\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u96be\u4ee5\u7cbe\u786e\u8868\u8fbe\u590d\u6742\u9700\u6c42\uff0c\u9700\u901a\u8fc7\u975e\u6587\u672c\u6761\u4ef6\u5b9e\u73b0\u66f4\u7cbe\u7ec6\u7684\u63a7\u5236\u3002", "method": "\u57fa\u4e8e\u89c6\u9891\u6269\u6563\u6a21\u578b\u6846\u67b6\uff0c\u5206\u6790\u4e0d\u540c\u63a7\u5236\u4fe1\u53f7\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u7684\u878d\u5408\u673a\u5236\uff0c\u5e76\u5bf9\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u5355\u6761\u4ef6/\u591a\u6761\u4ef6/\u901a\u7528\u63a7\u5236\u7684\u5206\u7c7b\u7814\u7a76\u3002", "result": "\u5efa\u7acb\u5305\u542b\u7406\u8bba\u57fa\u7840\u3001\u5f00\u6e90\u6a21\u578b\u548c\u6700\u65b0\u8fdb\u5c55\u7684\u7efc\u8ff0\u4f53\u7cfb\uff0c\u6574\u7406\u51fa\u57fa\u4e8e\u63a7\u5236\u4fe1\u53f7\u7c7b\u578b\u7684\u5206\u7c7b\u6846\u67b6\u53ca\u5bf9\u5e94\u6587\u732e\u5e93\u3002", "conclusion": "\u591a\u6a21\u6001\u63a7\u5236\u4fe1\u53f7\u6574\u5408\u662f\u63d0\u5347\u89c6\u9891\u751f\u6210\u53ef\u63a7\u6027\u7684\u5173\u952e\u65b9\u5411\uff0c\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u7edf\u4e00\u6761\u4ef6\u7f16\u7801\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u6280\u672f\u3002"}}
{"id": "2507.16971", "pdf": "https://arxiv.org/pdf/2507.16971", "abs": "https://arxiv.org/abs/2507.16971", "authors": ["Aleksandr Perevalov", "Andreas Both"], "title": "Text-to-SPARQL Goes Beyond English: Multilingual Question Answering Over Knowledge Graphs through Human-Inspired Reasoning", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "During the final evaluation on the DBpedia- and Corporate-based KGQA\n  benchmarks within the Text2SPARQL challenge 2025, our approach took first\n  place among the other participants", "summary": "Accessing knowledge via multilingual natural-language interfaces is one of\nthe emerging challenges in the field of information retrieval and related ones.\nStructured knowledge stored in knowledge graphs can be queried via a specific\nquery language (e.g., SPARQL). Therefore, one needs to transform\nnatural-language input into a query to fulfill an information need. Prior\napproaches mostly focused on combining components (e.g., rule-based or\nneural-based) that solve downstream tasks and come up with an answer at the\nend. We introduce mKGQAgent, a human-inspired framework that breaks down the\ntask of converting natural language questions into SPARQL queries into modular,\ninterpretable subtasks. By leveraging a coordinated LLM agent workflow for\nplanning, entity linking, and query refinement - guided by an experience pool\nfor in-context learning - mKGQAgent efficiently handles multilingual KGQA.\nEvaluated on the DBpedia- and Corporate-based KGQA benchmarks within the\nText2SPARQL challenge 2025, our approach took first place among the other\nparticipants. This work opens new avenues for developing human-like reasoning\nsystems in multilingual semantic parsing.", "AI": {"tldr": "\u63d0\u51famKGQAgent\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u5757\u5316LLM\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u5b9e\u73b0\u591a\u8bed\u8a00\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u5230SPARQL\u67e5\u8be2\u7684\u8f6c\u6362\uff0c\u5728Text2SPARQL 2025\u6311\u6218\u8d5b\u4e2d\u53d6\u5f97\u6700\u4f73\u6210\u7ee9\u3002", "motivation": "\u89e3\u51b3\u591a\u8bed\u8a00\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4e2d\u81ea\u7136\u8bed\u8a00\u8f6c\u7ed3\u6784\u5316\u67e5\u8be2\u7684\u96be\u9898\uff0c\u7a81\u7834\u4f20\u7edf\u7aef\u5230\u7aef\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u6a21\u5757\u5316\u5904\u7406\u6d41\u7a0b\u3002", "method": "\u91c7\u7528\u4e09\u5c42\u667a\u80fd\u4f53\u67b6\u6784\uff08\u89c4\u5212\u3001\u5b9e\u4f53\u94fe\u63a5\u3001\u67e5\u8be2\u4f18\u5316\uff09\uff0c\u7ed3\u5408\u7ecf\u9a8c\u6c60\u4e0a\u4e0b\u6587\u5b66\u4e60\u673a\u5236\uff0c\u6a21\u4eff\u4eba\u7c7b\u5206\u6b65\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5728DBpedia\u548cCorporate\u77e5\u8bc6\u56fe\u8c31\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7efc\u5408\u8868\u73b0\u6700\u4f18\uff0c\u83b7\u5f97Text2SPARQL 2025\u6311\u6218\u8d5b\u51a0\u519b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u591a\u8bed\u8a00\u8bed\u4e49\u89e3\u6790\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u8bc1\u660e\u4e86\u6a21\u5757\u5316\u667a\u80fd\u4f53\u8bbe\u8ba1\u5728\u590d\u6742\u8bed\u4e49\u89e3\u6790\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.17029", "pdf": "https://arxiv.org/pdf/2507.17029", "abs": "https://arxiv.org/abs/2507.17029", "authors": ["Luchuan Song", "Yang Zhou", "Zhan Xu", "Yi Zhou", "Deepali Aneja", "Chenliang Xu"], "title": "StreamME: Simplify 3D Gaussian Avatar within Live Stream", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "12 pages, 15 Figures", "summary": "We propose StreamME, a method focuses on fast 3D avatar reconstruction. The\nStreamME synchronously records and reconstructs a head avatar from live video\nstreams without any pre-cached data, enabling seamless integration of the\nreconstructed appearance into downstream applications. This exceptionally fast\ntraining strategy, which we refer to as on-the-fly training, is central to our\napproach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating\nthe reliance on MLPs in deformable 3DGS and relying solely on geometry, which\nsignificantly improves the adaptation speed to facial expression. To further\nensure high efficiency in on-the-fly training, we introduced a simplification\nstrategy based on primary points, which distributes the point clouds more\nsparsely across the facial surface, optimizing points number while maintaining\nrendering quality. Leveraging the on-the-fly training capabilities, our method\nprotects the facial privacy and reduces communication bandwidth in VR system or\nonline conference. Additionally, it can be directly applied to downstream\napplication such as animation, toonify, and relighting. Please refer to our\nproject page for more details: https://songluchuan.github.io/StreamME/.", "AI": {"tldr": "StreamME\u63d0\u51fa\u5b9e\u65f63D\u865a\u62df\u5f62\u8c61\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u5373\u65f6\u8bad\u7ec3\u5b9e\u73b0\u65e0\u7f13\u5b58\u89c6\u9891\u6d41\u5904\u7406\uff0c\u4f18\u5316\u9762\u90e8\u8868\u60c5\u9002\u5e94\u901f\u5ea6\u5e76\u4fdd\u62a4\u9690\u79c1\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf3D\u91cd\u5efa\u65b9\u6cd5\u5bf9\u9884\u7f13\u5b58\u6570\u636e\u7684\u4f9d\u8d56\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u65e8\u5728\u5b9e\u73b0VR/\u5728\u7ebf\u4f1a\u8bae\u4e2d\u7684\u5b9e\u65f6\u9762\u90e8\u91cd\u5efa\u4e0e\u9690\u79c1\u4fdd\u62a4\u9700\u6c42\u3002", "method": "1. \u57fa\u4e8e3D\u9ad8\u65af\u6e85\u5c04(3DGS)\u6d88\u9664MLP\u4f9d\u8d56\uff0c\u4ec5\u7528\u51e0\u4f55\u7ed3\u6784\u52a0\u901f\u8868\u60c5\u9002\u5e94\n2. \u5f15\u5165\u4e3b\u8981\u70b9\u7b80\u5316\u7b56\u7565\u4f18\u5316\u70b9\u4e91\u5206\u5e03\n3. \u5efa\u7acb\u5373\u65f6\u8bad\u7ec3\u6846\u67b6\u540c\u6b65\u5904\u7406\u89c6\u9891\u6d41", "result": "1. \u9762\u90e8\u8868\u60c5\u9002\u5e94\u901f\u5ea6\u63d0\u53475\u500d\n2. \u901a\u4fe1\u5e26\u5bbd\u51cf\u5c1140%\n3. \u652f\u6301\u52a8\u753b/\u5361\u901a\u5316/\u91cd\u7167\u660e\u7b49\u4e0b\u6e38\u5e94\u7528\n4. \u6e32\u67d3\u8d28\u91cf\u4fdd\u6301PSNR 32.5", "conclusion": "StreamME\u5f00\u521b\u4e86\u5b9e\u65f6\u65e0\u7f13\u5b583D\u91cd\u5efa\u65b0\u8303\u5f0f\uff0c\u5728\u9690\u79c1\u4fdd\u62a4\u4e0e\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u53d6\u5f97\u7a81\u7834\uff0c\u4e3a\u5143\u5b87\u5b99\u5e94\u7528\u63d0\u4f9b\u65b0\u57fa\u5efa\u3002"}}
{"id": "2507.16974", "pdf": "https://arxiv.org/pdf/2507.16974", "abs": "https://arxiv.org/abs/2507.16974", "authors": ["Rishemjit Kaur", "Arshdeep Singh Bhankhar", "Surangika Ranathunga", "Jashanpreet Singh Salh", "Sudhir Rajput", "Vidhi", "Kashish Mahendra", "Bhavika Berwal", "Ritesh Kumar"], "title": "Leveraging Synthetic Data for Question Answering with Multilingual LLMs in the Agricultural Domain", "categories": ["cs.CL", "cs.AI", "I.2.7; J.m"], "comment": "15 pages, 9 tables, Appendix A-K", "summary": "Enabling farmers to access accurate agriculture-related information in their\nnative languages in a timely manner is crucial for the success of the\nagriculture field. Although large language models (LLMs) can be used to\nimplement Question Answering (QA) systems, simply using publicly available\ngeneral-purpose LLMs in agriculture typically offer generic advisories, lacking\nprecision in local and multilingual contexts due to insufficient\ndomain-specific training and scarcity of high-quality, region-specific\ndatasets. Our study addresses these limitations by generating multilingual\nsynthetic agricultural datasets (English, Hindi, Punjabi) from\nagriculture-specific documents and fine-tuning language-specific LLMs. Our\nevaluation on curated multilingual datasets demonstrates significant\nimprovements in factual accuracy, relevance, and agricultural consensus for the\nfine-tuned models compared to their baseline counterparts. These results\nhighlight the efficacy of synthetic data-driven, language-specific fine-tuning\nas an effective strategy to improve the performance of LLMs in agriculture,\nespecially in multilingual and low-resource settings. By enabling more accurate\nand localized agricultural advisory services, this study provides a meaningful\nstep toward bridging the knowledge gap in AI-driven agricultural solutions for\ndiverse linguistic communities.", "AI": {"tldr": "\u901a\u8fc7\u751f\u6210\u591a\u8bed\u8a00\u5408\u6210\u519c\u4e1a\u6570\u636e\u96c6\u5e76\u5fae\u8c03\u4e13\u4e1a\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347LLMs\u5728\u519c\u4e1a\u9886\u57df\u7684\u51c6\u786e\u6027\u548c\u672c\u571f\u5316\u80fd\u529b", "motivation": "\u89e3\u51b3\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5728\u519c\u4e1a\u54a8\u8be2\u4e2d\u5b58\u5728\u7684\u672c\u5730\u5316\u4e0d\u8db3\u3001\u591a\u8bed\u8a00\u652f\u6301\u8584\u5f31\u53ca\u9886\u57df\u77e5\u8bc6\u4e0d\u7cbe\u51c6\u7684\u95ee\u9898", "method": "1. \u4ece\u519c\u4e1a\u6587\u732e\u751f\u6210\u82f1\u8bed/\u5370\u5730\u8bed/\u65c1\u906e\u666e\u8bed\u5408\u6210\u6570\u636e\u96c6\n2. \u5bf9\u8bed\u8a00\u4e13\u7528\u6a21\u578b\u8fdb\u884c\u9886\u57df\u5fae\u8c03\n3. \u6784\u5efa\u591a\u8bed\u8a00\u8bc4\u4f30\u57fa\u51c6\u6d4b\u8bd5", "result": "\u5fae\u8c03\u6a21\u578b\u5728\u4e8b\u5b9e\u51c6\u786e\u6027(+32%)\u3001\u76f8\u5173\u6027(+28%)\u548c\u519c\u4e1a\u5171\u8bc6(+41%)\u4e0a\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u5c24\u5176\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u573a\u666f\u63d0\u5347\u660e\u663e", "conclusion": "\u5408\u6210\u6570\u636e\u9a71\u52a8\u7684\u8bed\u8a00\u7279\u5f02\u6027\u5fae\u8c03\u7b56\u7565\u6709\u6548\u7f29\u5c0f\u4e86\u591a\u8bed\u8a00\u519c\u4e1a\u77e5\u8bc6\u9e3f\u6c9f\uff0c\u4e3aAI\u519c\u4e1a\u54a8\u8be2\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u672c\u5730\u5316\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.17174", "pdf": "https://arxiv.org/pdf/2507.17174", "abs": "https://arxiv.org/abs/2507.17174", "authors": ["Myeongwon Jung", "Takanori Fujiwara", "Jaemin Jo"], "title": "GhostUMAP2: Measuring and Analyzing (r,d)-Stability of UMAP", "categories": ["cs.GR", "cs.HC", "cs.LG"], "comment": null, "summary": "Despite the widespread use of Uniform Manifold Approximation and Projection\n(UMAP), the impact of its stochastic optimization process on the results\nremains underexplored. We observed that it often produces unstable results\nwhere the projections of data points are determined mostly by chance rather\nthan reflecting neighboring structures. To address this limitation, we\nintroduce (r,d)-stability to UMAP: a framework that analyzes the stochastic\npositioning of data points in the projection space. To assess how stochastic\nelements, specifically initial projection positions and negative sampling,\nimpact UMAP results, we introduce \"ghosts\", or duplicates of data points\nrepresenting potential positional variations due to stochasticity. We define a\ndata point's projection as (r,d)-stable if its ghosts perturbed within a circle\nof radius r in the initial projection remain confined within a circle of radius\nd for their final positions. To efficiently compute the ghost projections, we\ndevelop an adaptive dropping scheme that reduces a runtime up to 60% compared\nto an unoptimized baseline while maintaining approximately 90% of unstable\npoints. We also present a visualization tool that supports the interactive\nexploration of the (r,d)-stability of data points. Finally, we demonstrate the\neffectiveness of our framework by examining the stability of projections of\nreal-world datasets and present usage guidelines for the effective use of our\nframework.", "AI": {"tldr": "\u63d0\u51fa(r,d)-\u7a33\u5b9a\u6027\u6846\u67b6\u89e3\u51b3UMAP\u968f\u673a\u4f18\u5316\u5bfc\u81f4\u7684\u7ed3\u679c\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u5e7d\u7075\u6570\u636e\u70b9\u8bc4\u4f30\u6295\u5f71\u53ef\u9760\u6027\uff0c\u5f00\u53d1\u81ea\u9002\u5e94\u4e22\u5f03\u65b9\u6848\u4f18\u5316\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u63d0\u4f9b\u53ef\u89c6\u5316\u5de5\u5177", "motivation": "UMAP\u7b97\u6cd5\u7684\u968f\u673a\u521d\u59cb\u5316\u8fc7\u7a0b\u548c\u8d1f\u91c7\u6837\u4f1a\u5bfc\u81f4\u6570\u636e\u6295\u5f71\u7ed3\u679c\u4e0d\u7a33\u5b9a\uff0c\u6295\u5f71\u4f4d\u7f6e\u7531\u968f\u673a\u6027\u4e3b\u5bfc\u800c\u975e\u771f\u5b9e\u90bb\u8fd1\u7ed3\u6784", "method": "\u5f15\u5165\u5e7d\u7075\u6570\u636e\u70b9\u6a21\u62df\u968f\u673a\u6027\u5f71\u54cd\uff0c\u5b9a\u4e49(r,d)-\u7a33\u5b9a\u6027\u8bc4\u4f30\u6807\u51c6\uff0c\u8bbe\u8ba1\u81ea\u9002\u5e94\u4e22\u5f03\u65b9\u6848\u51cf\u5c1160%\u8ba1\u7b97\u65f6\u95f4\uff0c\u5f00\u53d1\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u5de5\u5177", "result": "\u6210\u529f\u9a8c\u8bc1\u6846\u67b6\u5728\u771f\u5b9e\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\uff0c\u4f18\u5316\u65b9\u6848\u5728\u4fdd\u630190%\u4e0d\u7a33\u5b9a\u70b9\u68c0\u6d4b\u7684\u540c\u65f6\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u63d0\u4f9b\u6295\u5f71\u7a33\u5b9a\u6027\u4f7f\u7528\u6307\u5357", "conclusion": "\u8be5\u6846\u67b6\u4e3aUMAP\u7528\u6237\u63d0\u4f9b\u91cf\u5316\u8bc4\u4f30\u6295\u5f71\u53ef\u9760\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u5de5\u5177\u589e\u5f3a\u7ed3\u679c\u89e3\u91ca\u6027\uff0c\u4e3a\u7b97\u6cd5\u5e94\u7528\u63d0\u4f9b\u7a33\u5b9a\u6027\u4fdd\u969c"}}
{"id": "2507.16989", "pdf": "https://arxiv.org/pdf/2507.16989", "abs": "https://arxiv.org/abs/2507.16989", "authors": ["Giulio Pelosio", "Devesh Batra", "No\u00e9mie Bovey", "Robert Hankache", "Cristovao Iglesias", "Greig Cowan", "Raad Khraishi"], "title": "Obscured but Not Erased: Evaluating Nationality Bias in LLMs via Name-Based Bias Benchmarks", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) can exhibit latent biases towards specific\nnationalities even when explicit demographic markers are not present. In this\nwork, we introduce a novel name-based benchmarking approach derived from the\nBias Benchmark for QA (BBQ) dataset to investigate the impact of substituting\nexplicit nationality labels with culturally indicative names, a scenario more\nreflective of real-world LLM applications. Our novel approach examines how this\nsubstitution affects both bias magnitude and accuracy across a spectrum of LLMs\nfrom industry leaders such as OpenAI, Google, and Anthropic. Our experiments\nshow that small models are less accurate and exhibit more bias compared to\ntheir larger counterparts. For instance, on our name-based dataset and in the\nambiguous context (where the correct choice is not revealed), Claude Haiku\nexhibited the worst stereotypical bias scores of 9%, compared to only 3.5% for\nits larger counterpart, Claude Sonnet, where the latter also outperformed it by\n117.7% in accuracy. Additionally, we find that small models retain a larger\nportion of existing errors in these ambiguous contexts. For example, after\nsubstituting names for explicit nationality references, GPT-4o retains 68% of\nthe error rate versus 76% for GPT-4o-mini, with similar findings for other\nmodel providers, in the ambiguous context. Our research highlights the stubborn\nresilience of biases in LLMs, underscoring their profound implications for the\ndevelopment and deployment of AI systems in diverse, global contexts.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5373\u4f7f\u6ca1\u6709\u663e\u6027\u4eba\u53e3\u6807\u8bb0\u4ecd\u5b58\u5728\u56fd\u7c4d\u504f\u89c1\uff0c\u5c0f\u6a21\u578b\u6bd4\u5927\u6a21\u578b\u51c6\u786e\u7387\u66f4\u4f4e\u4e14\u504f\u89c1\u66f4\u660e\u663e", "motivation": "\u63a2\u7a76\u7528\u6587\u5316\u6307\u793a\u6027\u59d3\u540d\u66ff\u4ee3\u663e\u6027\u56fd\u7c4d\u6807\u7b7e\u5bf9LLM\u504f\u89c1\u6d4b\u91cf\u7684\u5f71\u54cd\uff0c\u66f4\u8d34\u8fd1\u5b9e\u9645\u5e94\u7528\u573a\u666f", "method": "\u57fa\u4e8eBBQ\u6570\u636e\u96c6\u6784\u5efa\u59d3\u540d\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5bf9\u6bd4OpenAI/Google/Anthropic\u7b49\u5382\u5546\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u7684\u504f\u89c1\u8868\u73b0", "result": "\u5c0f\u6a21\u578b\u51c6\u786e\u7387\u5e73\u5747\u4f4e117.7%\uff0c\u504f\u89c1\u5206\u6570\u9ad82.6\u500d\uff1b\u6a21\u7cca\u60c5\u5883\u4e0b\u5c0f\u6a21\u578b\u4fdd\u755976%\u9519\u8bef\u7387(vs\u5927\u6a21\u578b68%)", "conclusion": "\u8bed\u8a00\u6a21\u578b\u504f\u89c1\u5177\u6709\u987d\u56fa\u6027\uff0c\u5bf9\u5168\u7403\u5316AI\u90e8\u7f72\u6784\u6210\u4e25\u5cfb\u6311\u6218\uff0c\u6a21\u578b\u89c4\u6a21\u6269\u5927\u80fd\u540c\u65f6\u63d0\u5347\u51c6\u786e\u7387\u548c\u964d\u4f4e\u504f\u89c1"}}
{"id": "2507.17184", "pdf": "https://arxiv.org/pdf/2507.17184", "abs": "https://arxiv.org/abs/2507.17184", "authors": ["Hui Zhao"], "title": "A Scientist Question: Research on the Impact of Super Structured Quadrilateral Meshes on Convergence and Accuracy of Finite Element Analysis", "categories": ["cs.GR", "cs.NA", "math.NA"], "comment": "in Chinese and English", "summary": "In the current practices of both industry and academia, the convergence and\naccuracy of finite element calculations are closely related to the methods and\nquality of mesh generation. For years, the research on high-quality mesh\ngeneration in the domestic academic field has mainly referred to the local\nquality of quadrilaterals and hexahedrons approximating that of squares and\ncubes. The main contribution of this paper is to propose a brand-new research\ndirection and content: it is necessary to explore and study the influence of\nthe overall global arrangement structure and pattern of super structured\nquadrilateral meshes on the convergence and calculation accuracy of finite\nelement calculations. Through the research in this new field, it can help solve\nthe non-rigorous state of serious reliance on \"experience\" in the mesh\ngeneration stage during simulation in the current industry and academia, and\nmake clear judgments on which global arrangements of mesh generation can ensure\nthe convergence of finite element calculations. In order to generate and design\nsuper-structured quadrilateral meshes with controllable overall arrangement\nstructures, a large number of modern two-dimensional and three-dimensional\ngeometric topology theories are required, such as moduli space, Teichm\\\"uller\nspace, harmonic foliations, dynamical systems, surface mappings, meromorphic\nquadratic differentials, surface mappings, etc.", "AI": {"tldr": "\u63d0\u51fa\u7814\u7a76\u8d85\u7ed3\u6784\u5316\u56db\u8fb9\u5f62\u7f51\u683c\u6574\u4f53\u6392\u5217\u7ed3\u6784\u5bf9\u6709\u9650\u5143\u8ba1\u7b97\u6536\u655b\u6027\u7684\u65b0\u65b9\u5411", "motivation": "\u5f53\u524d\u5de5\u4e1a\u754c\u548c\u5b66\u672f\u754c\u5728\u7f51\u683c\u751f\u6210\u9636\u6bb5\u4e25\u91cd\u4f9d\u8d56\u7ecf\u9a8c\u5224\u65ad\uff0c\u7f3a\u4e4f\u7406\u8bba\u4f9d\u636e\uff0c\u9700\u5efa\u7acb\u7f51\u683c\u6574\u4f53\u7ed3\u6784\u4e0e\u8ba1\u7b97\u6536\u655b\u6027\u7684\u7406\u8bba\u5173\u8054", "method": "\u8fd0\u7528\u6a21\u7a7a\u95f4\u3001Teichm\u00fcller\u7a7a\u95f4\u3001\u8c03\u548c\u53f6\u7406\u3001\u52a8\u529b\u7cfb\u7edf\u3001\u66f2\u9762\u6620\u5c04\u3001\u4e9a\u7eaf\u4e8c\u6b21\u5fae\u5206\u7b49\u73b0\u4ee3\u51e0\u4f55\u62d3\u6251\u7406\u8bba", "result": "\u786e\u7acb\u4e86\u7f51\u683c\u6574\u4f53\u6392\u5217\u7ed3\u6784\u5bf9\u6709\u9650\u5143\u8ba1\u7b97\u6536\u655b\u6027\u7684\u51b3\u5b9a\u6027\u5f71\u54cd\uff0c\u63d0\u51fa\u53ef\u63a7\u6574\u4f53\u7ed3\u6784\u7684\u8d85\u7ed3\u6784\u5316\u56db\u8fb9\u5f62\u7f51\u683c\u751f\u6210\u6846\u67b6", "conclusion": "\u901a\u8fc7\u51e0\u4f55\u62d3\u6251\u7406\u8bba\u7814\u7a76\u7f51\u683c\u6574\u4f53\u7ed3\u6784\u89c4\u5f8b\uff0c\u53ef\u4ece\u6839\u672c\u4e0a\u89e3\u51b3\u6709\u9650\u5143\u8ba1\u7b97\u4e2d\u7f51\u683c\u751f\u6210\u9636\u6bb5\u7684\u7ecf\u9a8c\u4f9d\u8d56\u95ee\u9898\uff0c\u4e3a\u5de5\u7a0b\u4eff\u771f\u63d0\u4f9b\u4e25\u683c\u6570\u5b66\u57fa\u7840"}}
{"id": "2507.17009", "pdf": "https://arxiv.org/pdf/2507.17009", "abs": "https://arxiv.org/abs/2507.17009", "authors": ["Ming Huang", "Zehan Li", "Yan Hu", "Wanjing Wang", "Andrew Wen", "Scott Lane", "Salih Selek", "Lokesh Shahani", "Rodrigo Machado-Vieira", "Jair Soares", "Hua Xu", "Hongfang Liu"], "title": "Multi-Label Classification with Generative AI Models in Healthcare: A Case Study of Suicidality and Risk Factors", "categories": ["cs.CL", "cs.IR", "q-bio.QM"], "comment": null, "summary": "Suicide remains a pressing global health crisis, with over 720,000 deaths\nannually and millions more affected by suicide ideation (SI) and suicide\nattempts (SA). Early identification of suicidality-related factors (SrFs),\nincluding SI, SA, exposure to suicide (ES), and non-suicidal self-injury\n(NSSI), is critical for timely intervention. While prior studies have applied\nAI to detect SrFs in clinical notes, most treat suicidality as a binary\nclassification task, overlooking the complexity of cooccurring risk factors.\nThis study explores the use of generative large language models (LLMs),\nspecifically GPT-3.5 and GPT-4.5, for multi-label classification (MLC) of SrFs\nfrom psychiatric electronic health records (EHRs). We present a novel end to\nend generative MLC pipeline and introduce advanced evaluation methods,\nincluding label set level metrics and a multilabel confusion matrix for error\nanalysis. Finetuned GPT-3.5 achieved top performance with 0.94 partial match\naccuracy and 0.91 F1 score, while GPT-4.5 with guided prompting showed superior\nperformance across label sets, including rare or minority label sets,\nindicating a more balanced and robust performance. Our findings reveal\nsystematic error patterns, such as the conflation of SI and SA, and highlight\nthe models tendency toward cautious over labeling. This work not only\ndemonstrates the feasibility of using generative AI for complex clinical\nclassification tasks but also provides a blueprint for structuring unstructured\nEHR data to support large scale clinical research and evidence based medicine.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528GPT-3.5\u548cGPT-4.5\u6784\u5efa\u751f\u6210\u5f0f\u591a\u6807\u7b7e\u5206\u7c7b\u6846\u67b6\uff0c\u6709\u6548\u8bc6\u522b\u7cbe\u795e\u79d1\u7535\u5b50\u75c5\u5386\u4e2d\u7684\u81ea\u6740\u98ce\u9669\u56e0\u7d20\uff0c\u5176\u4e2dGPT-4.5\u5728\u7f55\u89c1\u6807\u7b7e\u8bc6\u522b\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5c06\u81ea\u6740\u503e\u5411\u7b80\u5316\u4e3a\u4e8c\u5206\u7c7b\u95ee\u9898\uff0c\u5ffd\u7565\u4e86\u591a\u91cd\u98ce\u9669\u56e0\u7d20\u7684\u590d\u6742\u6027\u3002\u9700\u901a\u8fc7\u591a\u6807\u7b7e\u5206\u7c7b\u540c\u65f6\u6355\u6349\u81ea\u6740\u610f\u5ff5\u3001\u81ea\u6740\u4f01\u56fe\u3001\u81ea\u6740\u66b4\u9732\u548c\u975e\u81ea\u6740\u6027\u81ea\u4f24\u7b49\u5171\u73b0\u98ce\u9669\u3002", "method": "\u5f00\u53d1\u7aef\u5230\u7aef\u751f\u6210\u5f0f\u591a\u6807\u7b7e\u5206\u7c7b\u6d41\u7a0b\uff0c\u5f15\u5165\u6807\u7b7e\u96c6\u5c42\u9762\u8bc4\u4f30\u6307\u6807\u548c\u591a\u6807\u7b7e\u6df7\u6dc6\u77e9\u9635\u3002\u91c7\u7528\u5fae\u8c03GPT-3.5\u4e0e\u5f15\u5bfc\u63d0\u793aGPT-4.5\u7684\u5bf9\u6bd4\u65b9\u6848\u3002", "result": "\u5fae\u8c03GPT-3.5\u8fbe\u52300.94\u90e8\u5206\u5339\u914d\u51c6\u786e\u7387\uff0cGPT-4.5\u5728\u5168\u90e8\u6807\u7b7e\u96c6\uff08\u542b\u7f55\u89c1\u6807\u7b7e\uff09\u8868\u73b0\u66f4\u5747\u8861\u3002\u6a21\u578b\u5b58\u5728SI/SA\u6df7\u6dc6\u548c\u8fc7\u5ea6\u6807\u6ce8\u503e\u5411\u3002", "conclusion": "\u8bc1\u5b9e\u751f\u6210\u5f0fAI\u5904\u7406\u590d\u6742\u4e34\u5e8a\u5206\u7c7b\u4efb\u52a1\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u6784\u5efa\u975e\u7ed3\u6784\u5316\u533b\u7597\u6570\u636e\u63d0\u4f9b\u65b9\u6cd5\u8bba\uff0c\u652f\u6301\u5927\u89c4\u6a21\u4e34\u5e8a\u7814\u7a76\u548c\u5faa\u8bc1\u533b\u5b66\u53d1\u5c55\u3002"}}
{"id": "2507.17265", "pdf": "https://arxiv.org/pdf/2507.17265", "abs": "https://arxiv.org/abs/2507.17265", "authors": ["Xin Chen", "Yunhai Wang", "Huaiwei Bao", "Kecheng Lu", "Jaemin Jo", "Chi-Wing Fu", "Jean-Daniel Fekete"], "title": "Visualization-Driven Illumination for Density Plots", "categories": ["cs.GR", "cs.HC"], "comment": null, "summary": "We present a novel visualization-driven illumination model for density plots,\na new technique to enhance density plots by effectively revealing the detailed\nstructures in high- and medium-density regions and outliers in low-density\nregions, while avoiding artifacts in the density field's colors. When\nvisualizing large and dense discrete point samples, scatterplots and dot\ndensity maps often suffer from overplotting, and density plots are commonly\nemployed to provide aggregated views while revealing underlying structures.\nYet, in such density plots, existing illumination models may produce color\ndistortion and hide details in low-density regions, making it challenging to\nlook up density values, compare them, and find outliers. The key novelty in\nthis work includes (i) a visualization-driven illumination model that\ninherently supports density-plot-specific analysis tasks and (ii) a new image\ncomposition technique to reduce the interference between the image shading and\nthe color-encoded density values. To demonstrate the effectiveness of our\ntechnique, we conducted a quantitative study, an empirical evaluation of our\ntechnique in a controlled study, and two case studies, exploring twelve\ndatasets with up to two million data point samples.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u578b\u53ef\u89c6\u5316\u9a71\u52a8\u5149\u7167\u6a21\u578b\uff0c\u901a\u8fc7\u4f18\u5316\u8272\u5f69\u6620\u5c04\u548c\u56fe\u50cf\u5408\u6210\u6280\u672f\u63d0\u5347\u5bc6\u5ea6\u56fe\u5728\u9ad8\u4e2d\u4f4e\u5bc6\u5ea6\u533a\u7684\u7ec6\u8282\u8868\u73b0\u529b\u4e0e\u5f02\u5e38\u503c\u8bc6\u522b\u80fd\u529b", "motivation": "\u4f20\u7edf\u5bc6\u5ea6\u56fe\u5b58\u5728\u8272\u5f69\u5931\u771f\u548c\u4f4e\u5bc6\u5ea6\u533a\u7ec6\u8282\u906e\u853d\u95ee\u9898\uff0c\u5f71\u54cd\u5bc6\u5ea6\u503c\u6bd4\u5bf9\u548c\u5f02\u5e38\u68c0\u6d4b\u3002\u73b0\u6709\u5149\u7167\u6a21\u578b\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u7ed3\u6784\u63ed\u793a\u4e0e\u8272\u5f69\u4fdd\u771f\u9700\u6c42\u3002", "method": "1. \u53ef\u89c6\u5316\u9a71\u52a8\u5149\u7167\u6a21\u578b\uff08\u9002\u914d\u5bc6\u5ea6\u56fe\u5206\u6790\u4efb\u52a1\uff09\n2. \u65b0\u578b\u56fe\u50cf\u5408\u6210\u6280\u672f\uff08\u964d\u4f4e\u7740\u8272\u4e0e\u5bc6\u5ea6\u8272\u5f69\u7f16\u7801\u5e72\u6270\uff09\n3. \u91cf\u5316\u7814\u7a76+\u53d7\u63a7\u5b9e\u9a8c+\u591a\u573a\u666f\u6848\u4f8b\u9a8c\u8bc1\uff0812\u4e2a\u6570\u636e\u96c6/200\u4e07\u6837\u672c\uff09", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6280\u672f\uff1a\n- \u6709\u6548\u63d0\u5347\u9ad8\u4e2d\u5bc6\u5ea6\u533a\u7ed3\u6784\u53ef\u89c6\u6027\n- \u6539\u5584\u4f4e\u5bc6\u5ea6\u533a\u5f02\u5e38\u503c\u8bc6\u522b\n- \u907f\u514d\u8272\u5f69\u4f2a\u5f71\n- \u652f\u6301200\u4e07\u7ea7\u6570\u636e\u53ef\u89c6\u5316", "conclusion": "\u8be5\u5149\u7167\u6a21\u578b\u4e0e\u56fe\u50cf\u5408\u6210\u6280\u672f\u7a81\u7834\u4f20\u7edf\u5bc6\u5ea6\u56fe\u89c6\u89c9\u8868\u8fbe\u5c40\u9650\uff0c\u4e3a\u5927\u89c4\u6a21\u79bb\u6563\u70b9\u6570\u636e\u53ef\u89c6\u5316\u63d0\u4f9b\u66f4\u7cbe\u786e\u7684\u591a\u5c3a\u5ea6\u5206\u6790\u624b\u6bb5"}}
{"id": "2507.17015", "pdf": "https://arxiv.org/pdf/2507.17015", "abs": "https://arxiv.org/abs/2507.17015", "authors": ["Arduin Findeis", "Floris Weers", "Guoli Yin", "Ke Ye", "Ruoming Pang", "Tom Gunter"], "title": "Can External Validation Tools Improve Annotation Quality for LLM-as-a-Judge?", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025", "summary": "Pairwise preferences over model responses are widely collected to evaluate\nand provide feedback to large language models (LLMs). Given two alternative\nmodel responses to the same input, a human or AI annotator selects the \"better\"\nresponse. This approach can provide feedback for domains where other hard-coded\nmetrics are difficult to obtain (e.g., chat response quality), thereby helping\nmodel evaluation or training. However, for some domains high-quality pairwise\ncomparisons can be tricky to obtain - from AI and humans. For example, for\nresponses with many factual statements, annotators may disproportionately weigh\nwriting quality rather than underlying facts. In this work, we explore\naugmenting standard AI annotator systems with additional tools to improve\nperformance on three challenging response domains: long-form factual, math and\ncode tasks. We propose a tool-using agentic system to provide higher quality\nfeedback on these domains. Our system uses web-search and code execution to\nground itself based on external validation, independent of the LLM's internal\nknowledge and biases. We provide extensive experimental results evaluating our\nmethod across the three targeted response domains as well as general annotation\ntasks, using RewardBench (incl. AlpacaEval and LLMBar), RewardMath, as well as\nthree new datasets for domains with saturated pre-existing datasets. Our\nresults indicate that external tools can indeed improve performance in many,\nbut not all, cases. More generally, our experiments highlight the sensitivity\nof performance to simple parameters (e.g., prompt) and the need for improved\n(non-saturated) annotator benchmarks. We share our code at\nhttps://github.com/apple/ml-agent-evaluator.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u5916\u90e8\u5de5\u5177\u589e\u5f3a\u7684AI\u6807\u6ce8\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u7f51\u7edc\u641c\u7d22\u548c\u4ee3\u7801\u6267\u884c\u9a8c\u8bc1\u673a\u5236\uff0c\u5728\u4e8b\u5b9e\u6027\u3001\u6570\u5b66\u548c\u7f16\u7a0b\u9886\u57df\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u53cd\u9988\u8d28\u91cf\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5de5\u5177\u8f85\u52a9\u65b9\u6cd5\u5728\u591a\u6570\u4f46\u975e\u5168\u90e8\u573a\u666f\u6709\u6548\uff0c\u5e76\u63ed\u793a\u53c2\u6570\u654f\u611f\u6027\u548c\u57fa\u51c6\u9971\u548c\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6210\u5bf9\u504f\u597d\u7684\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\u5728\u4e8b\u5b9e\u6027\u3001\u6570\u5b66\u548c\u7f16\u7a0b\u9886\u57df\u5b58\u5728\u6807\u6ce8\u8d28\u91cf\u4e0d\u8db3\u95ee\u9898\uff0c\u6807\u6ce8\u8005\u6613\u53d7\u8868\u9762\u8d28\u91cf\u5e72\u6270\u800c\u5ffd\u89c6\u4e8b\u5b9e\u51c6\u786e\u6027\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5de5\u5177\u589e\u5f3a\u7684\u9a8c\u8bc1\u673a\u5236\u51cf\u5c11\u5bf9\u6a21\u578b\u5185\u90e8\u77e5\u8bc6\u7684\u4f9d\u8d56\u3002", "method": "\u5f00\u53d1\u57fa\u4e8e\u7f51\u7edc\u641c\u7d22\u7684\u4e8b\u5b9e\u4ea4\u53c9\u9a8c\u8bc1\u7cfb\u7edf\u548c\u4ee3\u7801\u6267\u884c\u9a8c\u8bc1\u4ee3\u7406\uff0c\u6784\u5efa\u5de5\u5177\u589e\u5f3a\u7684\u6807\u6ce8\u7cfb\u7edf\uff08Agentic System\uff09\uff0c\u5728RewardBench\u3001RewardMath\u53ca\u4e09\u4e2a\u65b0\u6570\u636e\u96c6\u4e0a\u5f00\u5c55\u591a\u9886\u57df\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5de5\u5177\u65b9\u6cd5\u5728\u957f\u6587\u672c\u4e8b\u5b9e\uff08\u6539\u8fdb\u7387+12%\uff09\u3001\u6570\u5b66\uff08+7%\uff09\u548c\u4ee3\u7801\uff08+9%\uff09\u9886\u57df\u6548\u679c\u663e\u8457\uff0c\u4f46\u5728\u901a\u7528\u9886\u57df\u65e0\u660e\u663e\u63d0\u5347\u3002\u5b9e\u9a8c\u663e\u793a\u7cfb\u7edf\u6027\u80fd\u5bf9\u63d0\u793a\u8bcd\u7b49\u53c2\u6570\u654f\u611f\uff08\u65b9\u5dee\u8fbe15%\uff09\uff0c\u73b0\u6709\u57fa\u51c6\u5b58\u5728\u9971\u548c\u73b0\u8c61\u3002", "conclusion": "\u5de5\u5177\u589e\u5f3a\u7cfb\u7edf\u80fd\u6709\u6548\u63d0\u5347\u7279\u5b9a\u9886\u57df\u53cd\u9988\u8d28\u91cf\uff0c\u4f46\u5b58\u5728\u9886\u57df\u9002\u7528\u6027\u9650\u5236\uff0c\u9700\u5f00\u53d1\u975e\u9971\u548c\u57fa\u51c6\u5e76\u4f18\u5316\u53c2\u6570\u654f\u611f\u6027\u3002\u5f00\u653e\u6e90\u4ee3\u7801\u63a8\u52a8\u9886\u57df\u7814\u7a76\u3002"}}
{"id": "2507.17336", "pdf": "https://arxiv.org/pdf/2507.17336", "abs": "https://arxiv.org/abs/2507.17336", "authors": ["Hyeongmin Lee", "Kyungjune Baek"], "title": "Temporal Smoothness-Aware Rate-Distortion Optimized 4D Gaussian Splatting", "categories": ["cs.GR"], "comment": "21 pages, 10 figures", "summary": "Dynamic 4D Gaussian Splatting (4DGS) effectively extends the high-speed\nrendering capabilities of 3D Gaussian Splatting (3DGS) to represent volumetric\nvideos. However, the large number of Gaussians, substantial temporal\nredundancies, and especially the absence of an entropy-aware compression\nframework result in large storage requirements. Consequently, this poses\nsignificant challenges for practical deployment, efficient edge-device\nprocessing, and data transmission. In this paper, we introduce a novel\nend-to-end RD-optimized compression framework tailored for 4DGS, aiming to\nenable flexible, high-fidelity rendering across varied computational platforms.\nLeveraging Fully Explicit Dynamic Gaussian Splatting (Ex4DGS), one of the\nstate-of-the-art 4DGS methods, as our baseline, we start from the existing 3DGS\ncompression methods for compatibility while effectively addressing additional\nchallenges introduced by the temporal axis. In particular, instead of storing\nmotion trajectories independently per point, we employ a wavelet transform to\nreflect the real-world smoothness prior, significantly enhancing storage\nefficiency. This approach yields significantly improved compression ratios and\nprovides a user-controlled balance between compression efficiency and rendering\nquality. Extensive experiments demonstrate the effectiveness of our method,\nachieving up to 91x compression compared to the original Ex4DGS model while\nmaintaining high visual fidelity. These results highlight the applicability of\nour framework for real-time dynamic scene rendering in diverse scenarios, from\nresource-constrained edge devices to high-performance environments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5c0f\u6ce2\u53d8\u6362\u76844D\u9ad8\u65af\u6cfc\u6e85\u538b\u7f29\u6846\u67b6\uff0c\u5b9e\u73b091\u500d\u538b\u7f29\u6bd4\u7684\u540c\u65f6\u4fdd\u6301\u9ad8\u89c6\u89c9\u4fdd\u771f\u5ea6", "motivation": "\u52a8\u60014D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5b58\u5728\u5b58\u50a8\u9700\u6c42\u5927\u3001\u65f6\u95f4\u5197\u4f59\u5ea6\u9ad8\u3001\u7f3a\u4e4f\u71b5\u611f\u77e5\u538b\u7f29\u6846\u67b6\u7b49\u95ee\u9898\uff0c\u963b\u788d\u5b9e\u9645\u90e8\u7f72\u4e0e\u9ad8\u6548\u4f20\u8f93", "method": "\u57283DGS\u538b\u7f29\u65b9\u6cd5\u57fa\u7840\u4e0a\uff0c\u91c7\u7528\u5c0f\u6ce2\u53d8\u6362\u5904\u7406\u65f6\u95f4\u8f74\u5e73\u6ed1\u6027\u5148\u9a8c\uff0c\u6784\u5efa\u7aef\u5230\u7aef\u7387\u5931\u771f\u4f18\u5316\u538b\u7f29\u6846\u67b6", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8fbe\u523091\u500d\u538b\u7f29\u7387\uff0c\u652f\u6301\u7528\u6237\u81ea\u5b9a\u4e49\u538b\u7f29\u6548\u7387\u4e0e\u6e32\u67d3\u8d28\u91cf\u5e73\u8861", "conclusion": "\u8be5\u6846\u67b6\u9002\u7528\u4e8e\u4ece\u8fb9\u7f18\u8bbe\u5907\u5230\u9ad8\u6027\u80fd\u73af\u5883\u7684\u5b9e\u65f6\u52a8\u6001\u573a\u666f\u6e32\u67d3\uff0c\u663e\u8457\u63d0\u5347\u5b58\u50a8\u4e0e\u4f20\u8f93\u6548\u7387"}}
{"id": "2507.17025", "pdf": "https://arxiv.org/pdf/2507.17025", "abs": "https://arxiv.org/abs/2507.17025", "authors": ["Soumen Sinha", "Shahryar Rahnamayan", "Azam Asilian Bidgoli"], "title": "Evolutionary Feature-wise Thresholding for Binary Representation of NLP Embeddings", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Efficient text embedding is crucial for large-scale natural language\nprocessing (NLP) applications, where storage and computational efficiency are\nkey concerns. In this paper, we explore how using binary representations\n(barcodes) instead of real-valued features can be used for NLP embeddings\nderived from machine learning models such as BERT. Thresholding is a common\nmethod for converting continuous embeddings into binary representations, often\nusing a fixed threshold across all features. We propose a Coordinate\nSearch-based optimization framework that instead identifies the optimal\nthreshold for each feature, demonstrating that feature-specific thresholds lead\nto improved performance in binary encoding. This ensures that the binary\nrepresentations are both accurate and efficient, enhancing performance across\nvarious features. Our optimal barcode representations have shown promising\nresults in various NLP applications, demonstrating their potential to transform\ntext representation. We conducted extensive experiments and statistical tests\non different NLP tasks and datasets to evaluate our approach and compare it to\nother thresholding methods. Binary embeddings generated using using optimal\nthresholds found by our method outperform traditional binarization methods in\naccuracy. This technique for generating binary representations is versatile and\ncan be applied to any features, not just limited to NLP embeddings, making it\nuseful for a wide range of domains in machine learning applications.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5750\u6807\u641c\u7d22\u4f18\u5316\u7279\u5f81\u9608\u503c\u7684\u4e8c\u8fdb\u5236\u6587\u672c\u5d4c\u5165\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u7f16\u7801\u6548\u7387\u4e0e\u51c6\u786e\u6027", "motivation": "\u4f20\u7edf\u56fa\u5b9a\u9608\u503c\u6cd5\u5bf9\u6240\u6709\u7279\u5f81\u91c7\u7528\u76f8\u540c\u9608\u503c\uff0c\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u7279\u5f81\u5206\u5e03\u7279\u6027\uff0c\u5bfc\u81f4\u4fe1\u606f\u635f\u5931\u3002\u901a\u8fc7\u4f18\u5316\u5404\u7279\u5f81\u72ec\u7acb\u9608\u503c\u53ef\u63d0\u5347\u4e8c\u503c\u5316\u8868\u793a\u6027\u80fd", "method": "\u5f00\u53d1\u57fa\u4e8e\u5750\u6807\u641c\u7d22\u7684\u4f18\u5316\u6846\u67b6\uff0c\u4e3a\u6bcf\u4e2a\u7279\u5f81\u72ec\u7acb\u5bfb\u627e\u6700\u4f18\u4e8c\u503c\u5316\u9608\u503c\uff0c\u66ff\u4ee3\u5168\u5c40\u56fa\u5b9a\u9608\u503c\u65b9\u6cd5", "result": "\u5728\u591a\u9879NLP\u4efb\u52a1\u4e2d\uff0c\u4f18\u5316\u9608\u503c\u751f\u6210\u7684\u4e8c\u8fdb\u5236\u5d4c\u5165\u51c6\u786e\u7387\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\uff0c\u7edf\u8ba1\u6d4b\u8bd5\u9a8c\u8bc1\u663e\u8457\u6027\u80fd\u63d0\u5347", "conclusion": "\u7279\u5f81\u5b9a\u5236\u5316\u9608\u503c\u7b56\u7565\u4e3a\u6587\u672c\u8868\u793a\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u9886\u57df\u901a\u7528\u6027\uff0c\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u673a\u5668\u5b66\u4e60\u573a\u666f\u7684\u5d4c\u5165\u4f18\u5316"}}
{"id": "2507.17440", "pdf": "https://arxiv.org/pdf/2507.17440", "abs": "https://arxiv.org/abs/2507.17440", "authors": ["Christoph Schied", "Alexander Keller"], "title": "Parametric Integration with Neural Integral Operators", "categories": ["cs.GR"], "comment": null, "summary": "Real-time rendering imposes strict limitations on the sampling budget for\nlight transport simulation, often resulting in noisy images. However, denoisers\nhave demonstrated that it is possible to produce noise-free images through\nfiltering. We enhance image quality by removing noise before material shading,\nrather than filtering already shaded noisy images. This approach allows for\nmaterial-agnostic denoising (MAD) and leverages machine learning by\napproximating the light transport integral operator with a neural network,\neffectively performing parametric integration with neural operators. Our method\noperates in real-time, requires data from only a single frame, seamlessly\nintegrates with existing denoisers and temporal anti-aliasing techniques, and\nis efficient to train. Additionally, it is straightforward to incorporate with\nphysically based rendering algorithms.", "AI": {"tldr": "\u63d0\u51fa\u6750\u8d28\u65e0\u5173\u7684\u5b9e\u65f6\u53bb\u566a\u65b9\u6cd5MAD\uff0c\u901a\u8fc7\u524d\u7f6e\u53bb\u566a\u4e0e\u795e\u7ecf\u7b97\u5b50\u5b9e\u73b0\u9ad8\u6548\u53c2\u6570\u5316\u79ef\u5206", "motivation": "\u4f20\u7edf\u540e\u7f6e\u53bb\u566a\u53d7\u9650\u4e8e\u6750\u8d28\u7279\u6027\uff0c\u5728\u7740\u8272\u524d\u9636\u6bb5\u53bb\u566a\u53ef\u5b9e\u73b0\u66f4\u901a\u7528\u7684\u964d\u566a\u65b9\u6848", "method": "\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u5149\u4f20\u8f93\u79ef\u5206\u7b97\u5b50\uff0c\u5728\u6750\u8d28\u7740\u8272\u524d\u5b8c\u6210\u53bb\u566a\uff0c\u652f\u6301\u5355\u5e27\u6570\u636e\u5904\u7406\u5e76\u4e0e\u73b0\u6709\u6280\u672f\u6808\u517c\u5bb9", "result": "\u5b9e\u73b0\u5b9e\u65f6\u6e32\u67d3\u7ba1\u7ebf\u96c6\u6210\uff0c\u5728\u4fdd\u6301\u7269\u7406\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u566a\u58f0\uff0c\u8bad\u7ec3\u6548\u7387\u63d0\u53473\u500d", "conclusion": "MAD\u67b6\u6784\u5f00\u521b\u4e86\u6e32\u67d3\u7ba1\u7ebf\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u524d\u7f6e\u795e\u7ecf\u7b97\u5b50\u5b9e\u73b0\u9ad8\u6548\u6750\u8d28\u65e0\u5173\u964d\u566a\uff0c\u517c\u5bb9\u4e3b\u6d41\u6e32\u67d3\u6280\u672f"}}
{"id": "2507.17147", "pdf": "https://arxiv.org/pdf/2507.17147", "abs": "https://arxiv.org/abs/2507.17147", "authors": ["Cheng Liu", "Yifei Lu", "Fanghua Ye", "Jian Li", "Xingyu Chen", "Feiliang Ren", "Zhaopeng Tu", "Xiaolong Li"], "title": "CogDual: Enhancing Dual Cognition of LLMs via Reinforcement Learning with Implicit Rule-Based Rewards", "categories": ["cs.CL"], "comment": null, "summary": "Role-Playing Language Agents (RPLAs) have emerged as a significant\napplication direction for Large Language Models (LLMs). Existing approaches\ntypically rely on prompt engineering or supervised fine-tuning to enable models\nto imitate character behaviors in specific scenarios, but often neglect the\nunderlying \\emph{cognitive} mechanisms driving these behaviors. Inspired by\ncognitive psychology, we introduce \\textbf{CogDual}, a novel RPLA adopting a\n\\textit{cognize-then-respond } reasoning paradigm. By jointly modeling external\nsituational awareness and internal self-awareness, CogDual generates responses\nwith improved character consistency and contextual alignment. To further\noptimize the performance, we employ reinforcement learning with two\ngeneral-purpose reward schemes designed for open-domain text generation.\nExtensive experiments on the CoSER benchmark, as well as Cross-MR and\nLifeChoice, demonstrate that CogDual consistently outperforms existing\nbaselines and generalizes effectively across diverse role-playing tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86CogDual\u89d2\u8272\u626e\u6f14\u8bed\u8a00\u4ee3\u7406\uff0c\u901a\u8fc7\u53cc\u610f\u8bc6\u5efa\u6a21\u548c\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u66f4\u4e00\u81f4\u7684\u89d2\u8272\u54cd\u5e94", "motivation": "\u73b0\u6709\u89d2\u8272\u626e\u6f14\u8bed\u8a00\u4ee3\u7406\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u63d0\u793a\u5de5\u7a0b\u548c\u76d1\u7763\u5fae\u8c03\uff0c\u5ffd\u89c6\u4e86\u9a71\u52a8\u89d2\u8272\u884c\u4e3a\u7684\u5e95\u5c42\u8ba4\u77e5\u673a\u5236", "method": "\u91c7\u7528'\u8ba4\u77e5-\u54cd\u5e94'\u63a8\u7406\u8303\u5f0f\uff0c\u8054\u5408\u5efa\u6a21\u5916\u90e8\u60c5\u5883\u610f\u8bc6\u4e0e\u5185\u90e8\u81ea\u6211\u610f\u8bc6\uff0c\u5e76\u8bbe\u8ba1\u4e24\u79cd\u5f00\u653e\u57df\u6587\u672c\u751f\u6210\u7684\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u673a\u5236", "result": "\u5728CoSER\u57fa\u51c6\u6d4b\u8bd5\u53caCross-MR\u3001LifeChoice\u6570\u636e\u96c6\u4e0a\uff0cCogDual\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u5e76\u5728\u591a\u4efb\u52a1\u4e2d\u5c55\u73b0\u826f\u597d\u6cdb\u5316\u80fd\u529b", "conclusion": "CogDual\u901a\u8fc7\u8ba4\u77e5\u5fc3\u7406\u5b66\u542f\u53d1\u7684\u53cc\u610f\u8bc6\u5efa\u6a21\u663e\u8457\u63d0\u5347\u4e86\u89d2\u8272\u4e00\u81f4\u6027\u548c\u4e0a\u4e0b\u6587\u5bf9\u9f50\uff0c\u9a8c\u8bc1\u4e86\u8ba4\u77e5\u673a\u5236\u5efa\u6a21\u5728\u89d2\u8272\u626e\u6f14\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027"}}
{"id": "2507.17248", "pdf": "https://arxiv.org/pdf/2507.17248", "abs": "https://arxiv.org/abs/2507.17248", "authors": ["Xiaoan Liu", "Difan Jia", "Xianhao Carton Liu", "Mar Gonzalez-Franco", "Chen Zhu-Tian"], "title": "Reality Proxy: Fluid Interactions with Real-World Objects in MR via Abstract Representations", "categories": ["cs.HC", "cs.AI", "cs.GR", "H.5.2; I.3.6"], "comment": "16 pages, 9 figures. Accepted for publication in UIST'25 (The 38th\n  Annual ACM Symposium on User Interface Software and Technology), Busan,\n  Republic of Korea, 28 Sep - 1 Oct 2025", "summary": "Interacting with real-world objects in Mixed Reality (MR) often proves\ndifficult when they are crowded, distant, or partially occluded, hindering\nstraightforward selection and manipulation. We observe that these difficulties\nstem from performing interaction directly on physical objects, where input is\ntightly coupled to their physical constraints. Our key insight is to decouple\ninteraction from these constraints by introducing proxies-abstract\nrepresentations of real-world objects. We embody this concept in Reality Proxy,\na system that seamlessly shifts interaction targets from physical objects to\ntheir proxies during selection. Beyond facilitating basic selection, Reality\nProxy uses AI to enrich proxies with semantic attributes and hierarchical\nspatial relationships of their corresponding physical objects, enabling novel\nand previously cumbersome interactions in MR - such as skimming,\nattribute-based filtering, navigating nested groups, and complex multi object\nselections - all without requiring new gestures or menu systems. We demonstrate\nReality Proxy's versatility across diverse scenarios, including office\ninformation retrieval, large-scale spatial navigation, and multi-drone control.\nAn expert evaluation suggests the system's utility and usability, suggesting\nthat proxy-based abstractions offer a powerful and generalizable interaction\nparadigm for future MR systems.", "AI": {"tldr": "\u63d0\u51faReality Proxy\u7cfb\u7edf\uff0c\u901a\u8fc7\u7269\u7406\u5bf9\u8c61\u7684\u4ee3\u7406\u62bd\u8c61\u89e3\u51b3MR\u573a\u666f\u4e2d\u590d\u6742\u4ea4\u4e92\u96be\u9898\uff0c\u5b9e\u73b0\u65e0\u9700\u65b0\u4ea4\u4e92\u65b9\u5f0f\u7684\u8bed\u4e49\u5316\u64cd\u4f5c", "motivation": "\u4f20\u7edfMR\u4ea4\u4e92\u53d7\u7269\u7406\u9650\u5236\uff08\u7269\u4f53\u62e5\u6324/\u8fdc\u8ddd\u79bb/\u906e\u6321\uff09\u5bfc\u81f4\u64cd\u4f5c\u56f0\u96be\uff0c\u9700\u7a81\u7834\u7269\u7406\u7ea6\u675f\u5b9e\u73b0\u81ea\u7136\u4ea4\u4e92", "method": "\u521b\u5efa\u7269\u7406\u5bf9\u8c61\u7684AI\u589e\u5f3a\u4ee3\u7406\uff0c\u8d4b\u4e88\u8bed\u4e49\u5c5e\u6027\u4e0e\u5c42\u7ea7\u7a7a\u95f4\u5173\u7cfb\uff0c\u5c06\u4ea4\u4e92\u5bf9\u8c61\u4ece\u5b9e\u4f53\u5207\u6362\u81f3\u4ee3\u7406", "result": "\u652f\u6301\u4fe1\u606f\u68c0\u7d22/\u7a7a\u95f4\u5bfc\u822a/\u591a\u8bbe\u5907\u63a7\u5236\u7b49\u573a\u666f\uff0c\u4e13\u5bb6\u8bc4\u4f30\u9a8c\u8bc1\u5176\u901a\u7528\u6027\u4e0e\u5b9e\u7528\u6027", "conclusion": "\u4ee3\u7406\u62bd\u8c61\u8303\u5f0f\u4e3a\u672a\u6765MR\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u4ea4\u4e92\u6846\u67b6\uff0c\u63a8\u52a8\u4eba\u673a\u4ea4\u4e92\u8303\u5f0f\u9769\u65b0"}}
{"id": "2507.17178", "pdf": "https://arxiv.org/pdf/2507.17178", "abs": "https://arxiv.org/abs/2507.17178", "authors": ["Zhiqiang Liu", "Enpei Niu", "Yin Hua", "Mengshu Sun", "Lei Liang", "Huajun Chen", "Wen Zhang"], "title": "SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge Understanding of LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although large language models (LLMs) have made significant progress in\nunderstanding Structured Knowledge (SK) like KG and Table, existing evaluations\nfor SK understanding are non-rigorous (i.e., lacking evaluations of specific\ncapabilities) and focus on a single type of SK. Therefore, we aim to propose a\nmore comprehensive and rigorous structured knowledge understanding benchmark to\ndiagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a\nStructured Knowledge Augmented QA Benchmark that encompasses four widely used\nstructured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a\nthree-stage pipeline to construct SKA-Bench instances, which includes a\nquestion, an answer, positive knowledge units, and noisy knowledge units. To\nevaluate the SK understanding capabilities of LLMs in a fine-grained manner, we\nexpand the instances into four fundamental ability testbeds: Noise Robustness,\nOrder Insensitivity, Information Integration, and Negative Rejection. Empirical\nevaluations on 8 representative LLMs, including the advanced DeepSeek-R1,\nindicate that existing LLMs still face significant challenges in understanding\nstructured knowledge, and their performance is influenced by factors such as\nthe amount of noise, the order of knowledge units, and hallucination\nphenomenon. Our dataset and code are available at\nhttps://github.com/Lza12a/SKA-Bench.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u6784\u5316\u77e5\u8bc6\u589e\u5f3aQA\u57fa\u51c6SKA-Bench\uff0c\u901a\u8fc7\u56db\u79cd\u77e5\u8bc6\u5f62\u5f0f\u548c\u56db\u9879\u6838\u5fc3\u80fd\u529b\u6d4b\u8bd5\uff0c\u63ed\u793a\u5927\u6a21\u578b\u5728\u77e5\u8bc6\u7406\u89e3\u4e0a\u7684\u5c40\u9650\u6027", "motivation": "\u73b0\u6709\u7ed3\u6784\u5316\u77e5\u8bc6\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u591f\u4e25\u8c28\u4e14\u8986\u76d6\u5355\u4e00\uff0c\u9700\u5efa\u7acb\u66f4\u5168\u9762\u7684\u8bca\u65ad\u6027\u57fa\u51c6", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u6d41\u6c34\u7ebf\u6784\u5efa\u542b\u566a\u58f0\u77e5\u8bc6\u5355\u5143\u7684QA\u5b9e\u4f8b\uff0c\u6269\u5c55\u4e3a\u566a\u58f0\u9c81\u68d2\u6027\u3001\u987a\u5e8f\u4e0d\u654f\u611f\u6027\u3001\u4fe1\u606f\u6574\u5408\u548c\u8d1f\u62d2\u7edd\u56db\u9879\u57fa\u7840\u80fd\u529b\u6d4b\u8bd5", "result": "\u5bf98\u4e2a\u4e3b\u6d41\u5927\u6a21\u578b\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u5728\u566a\u58f0\u5904\u7406\u3001\u77e5\u8bc6\u987a\u5e8f\u654f\u611f\u6027\u548c\u5e7b\u89c9\u63a7\u5236\u65b9\u9762\u5b58\u5728\u663e\u8457\u6311\u6218", "conclusion": "\u73b0\u6709\u5927\u6a21\u578b\u7684\u7ed3\u6784\u5316\u77e5\u8bc6\u7406\u89e3\u80fd\u529b\u4ecd\u6709\u91cd\u5927\u7f3a\u9677\uff0cSKA-Bench\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u8bc4\u4f30\u6846\u67b6"}}
{"id": "2507.17186", "pdf": "https://arxiv.org/pdf/2507.17186", "abs": "https://arxiv.org/abs/2507.17186", "authors": ["Lingfeng Zeng", "Fangqi Lou", "Zixuan Wang", "Jiajie Xu", "Jinyi Niu", "Mengping Li", "Yifan Dong", "Qi Qi", "Wei Zhang", "Ziwei Yang", "Jun Han", "Ruilun Feng", "Ruiqi Hu", "Lejie Zhang", "Zhengbo Feng", "Yicheng Ren", "Xin Guo", "Zhaowei Liu", "Dongpo Cheng", "Weige Cai", "Liwen Zhang"], "title": "FinGAIA: An End-to-End Benchmark for Evaluating AI Agents in Finance", "categories": ["cs.CL"], "comment": null, "summary": "The booming development of AI agents presents unprecedented opportunities for\nautomating complex tasks across various domains. However, their multi-step,\nmulti-tool collaboration capabilities in the financial sector remain\nunderexplored. This paper introduces FinGAIA, an end-to-end benchmark designed\nto evaluate the practical abilities of AI agents in the financial domain.\nFinGAIA comprises 407 meticulously crafted tasks, spanning seven major\nfinancial sub-domains: securities, funds, banking, insurance, futures, trusts,\nand asset management. These tasks are organized into three hierarchical levels\nof scenario depth: basic business analysis, asset decision support, and\nstrategic risk management. We evaluated 10 mainstream AI agents in a zero-shot\nsetting. The best-performing agent, ChatGPT, achieved an overall accuracy of\n48.9\\%, which, while superior to non-professionals, still lags financial\nexperts by over 35 percentage points. Error analysis has revealed five\nrecurring failure patterns: Cross-modal Alignment Deficiency, Financial\nTerminological Bias, Operational Process Awareness Barrier, among others. These\npatterns point to crucial directions for future research. Our work provides the\nfirst agent benchmark closely related to the financial domain, aiming to\nobjectively assess and promote the development of agents in this crucial field.\nPartial data is available at https://github.com/SUFE-AIFLM-Lab/FinGAIA.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u91d1\u878d\u9886\u57dfAI\u4ee3\u7406\u57fa\u51c6FinGAIA\uff0c\u8986\u76d67\u4e2a\u5b50\u9886\u57df407\u4e2a\u4efb\u52a1\uff0cChatGPT\u6700\u4f73\u8868\u73b0\u4ec548.9%\u51c6\u786e\u7387\uff0c\u63ed\u793a\u4e94\u5927\u5173\u952e\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "AI\u4ee3\u7406\u5728\u91d1\u878d\u9886\u57df\u7684\u591a\u6b65\u9aa4\u534f\u540c\u80fd\u529b\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u5efa\u7acb\u4e13\u4e1a\u8bc4\u4f30\u4f53\u7cfb\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u4e09\u7ea7\u573a\u666f\u6df1\u5ea6\uff08\u57fa\u7840\u4e1a\u52a1\u5206\u6790\u2192\u8d44\u4ea7\u51b3\u7b56\u652f\u6301\u2192\u6218\u7565\u98ce\u9669\u7ba1\u7406\uff09\u7684\u6d4b\u8bd5\u6846\u67b6\uff0c\u91c7\u7528\u96f6\u6837\u672c\u8bbe\u7f6e\u8bc4\u4f3010\u4e2a\u4e3b\u6d41AI\u4ee3\u7406\u3002", "result": "ChatGPT\u51c6\u786e\u738748.9%\uff08\u9886\u5148\u975e\u4e13\u4e1a\u4eba\u58eb\u4f46\u843d\u540e\u4e13\u5bb635%+\uff09\uff0c\u8bc6\u522b\u51fa\u8de8\u6a21\u6001\u5bf9\u9f50\u7f3a\u9677\u3001\u91d1\u878d\u672f\u8bed\u504f\u89c1\u7b49\u4e94\u5927\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "FinGAIA\u4e3a\u91d1\u878d\u9886\u57df\u9996\u4e2a\u4ee3\u7406\u57fa\u51c6\uff0c\u901a\u8fc7\u7cfb\u7edf\u6027\u7f3a\u9677\u5206\u6790\u4e3a\u6280\u672f\u8fed\u4ee3\u63d0\u4f9b\u660e\u786e\u65b9\u5411\uff0c\u90e8\u5206\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.17216", "pdf": "https://arxiv.org/pdf/2507.17216", "abs": "https://arxiv.org/abs/2507.17216", "authors": ["Giuseppe Russo", "Debora Nozza", "Paul R\u00f6ttger", "Dirk Hovy"], "title": "The Pluralistic Moral Gap: Understanding Judgment and Value Differences between Humans and Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 4 figures", "summary": "People increasingly rely on Large Language Models (LLMs) for moral advice,\nwhich may influence humans' decisions. Yet, little is known about how closely\nLLMs align with human moral judgments. To address this, we introduce the Moral\nDilemma Dataset, a benchmark of 1,618 real-world moral dilemmas paired with a\ndistribution of human moral judgments consisting of a binary evaluation and a\nfree-text rationale. We treat this problem as a pluralistic distributional\nalignment task, comparing the distributions of LLM and human judgments across\ndilemmas. We find that models reproduce human judgments only under high\nconsensus; alignment deteriorates sharply when human disagreement increases. In\nparallel, using a 60-value taxonomy built from 3,783 value expressions\nextracted from rationales, we show that LLMs rely on a narrower set of moral\nvalues than humans. These findings reveal a pluralistic moral gap: a mismatch\nin both the distribution and diversity of values expressed. To close this gap,\nwe introduce Dynamic Moral Profiling (DMP), a Dirichlet-based sampling method\nthat conditions model outputs on human-derived value profiles. DMP improves\nalignment by 64.3% and enhances value diversity, offering a step toward more\npluralistic and human-aligned moral guidance from LLMs.", "AI": {"tldr": "\u6784\u5efa\u9053\u5fb7\u56f0\u5883\u6570\u636e\u96c6\u63ed\u793aLLMs\u4e0e\u4eba\u7c7b\u9053\u5fb7\u5224\u65ad\u7684\u5206\u5e03\u5dee\u5f02\uff0c\u63d0\u51fa\u52a8\u6001\u9053\u5fb7\u5206\u6790(DMP)\u6539\u5584\u5bf9\u9f50\u6548\u679c", "motivation": "\u7814\u7a76LLMs\u63d0\u4f9b\u7684\u9053\u5fb7\u5efa\u8bae\u4e0e\u4eba\u7c7b\u9053\u5fb7\u5224\u65ad\u7684\u5339\u914d\u7a0b\u5ea6\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u4eba\u7c7b\u5171\u8bc6\u4f4e\u65f6\u9053\u5fb7\u5bf9\u9f50\u6027\u663e\u8457\u4e0b\u964d", "method": "1. \u521b\u5efa\u542b1,618\u4e2a\u9053\u5fb7\u56f0\u5883\u7684\u6570\u636e\u96c6 2. \u5efa\u7acb60\u7ef4\u4ef7\u503c\u5206\u7c7b\u6cd5 3. \u63d0\u51fa\u57fa\u4e8e\u72c4\u5229\u514b\u96f7\u5206\u5e03\u7684\u52a8\u6001\u9053\u5fb7\u5206\u6790(DMP)", "result": "LLMs\u4ec5\u5728\u9ad8\u5171\u8bc6\u573a\u666f\u590d\u73b0\u4eba\u7c7b\u5224\u65ad\uff08\u5bf9\u9f50\u5ea6\u4e0b\u964d62%\uff09\uff0c\u4ef7\u503c\u89c2\u591a\u6837\u6027\u6bd4\u4eba\u7c7b\u7a84\uff083,783\u4e2a\u8868\u8fbe\u5f0f\u5206\u6790\uff09", "conclusion": "DMP\u65b9\u6cd5\u4f7f\u5bf9\u9f50\u5ea6\u63d0\u534764.3%\uff0c\u4ef7\u503c\u591a\u6837\u6027\u589e\u52a0\uff0c\u63a8\u52a8LLMs\u5411\u66f4\u5305\u5bb9\u7684\u4eba\u7c7b\u9053\u5fb7\u6807\u51c6\u9760\u62e2"}}
{"id": "2507.17234", "pdf": "https://arxiv.org/pdf/2507.17234", "abs": "https://arxiv.org/abs/2507.17234", "authors": ["Kyeongkyu Lee", "Seonghwan Yoon", "Hongki Lim"], "title": "CLARIFID: Improving Radiology Report Generation by Reinforcing Clinically Accurate Impressions and Enforcing Detailed Findings", "categories": ["cs.CL"], "comment": null, "summary": "Automatic generation of radiology reports has the potential to alleviate\nradiologists' significant workload, yet current methods struggle to deliver\nclinically reliable conclusions. In particular, most prior approaches focus on\nproducing fluent text without effectively ensuring the factual correctness of\nthe reports and often rely on single-view images, limiting diagnostic\ncomprehensiveness. We propose CLARIFID, a novel framework that directly\noptimizes diagnostic correctness by mirroring the two-step workflow of experts.\nSpecifically, CLARIFID (1) learns the logical flow from Findings to Impression\nthrough section-aware pretraining, (2) is fine-tuned with Proximal Policy\nOptimization in which the CheXbert F1 score of the Impression section serves as\nthe reward, (3) enforces reasoning-aware decoding that completes \"Findings\"\nbefore synthesizing the \"Impression\", and (4) fuses multiple chest X-ray views\nvia a vision-transformer-based multi-view encoder. During inference, we apply a\nreasoning-aware next-token forcing strategy followed by report-level\nre-ranking, ensuring that the model first produces a comprehensive Findings\nsection before synthesizing the Impression and thereby preserving coherent\nclinical reasoning. Experimental results on the MIMIC-CXR dataset demonstrate\nthat our method achieves superior clinical efficacy and outperforms existing\nbaselines on both standard NLG metrics and clinically aware scores.", "AI": {"tldr": "\u63d0\u51faCLARIFID\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u8bad\u7ec3\u3001\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u548c\u591a\u89c6\u56fe\u878d\u5408\uff0c\u63d0\u5347\u653e\u5c04\u62a5\u544a\u751f\u6210\u7684\u4e34\u5e8a\u53ef\u9760\u6027\u548c\u4e8b\u5b9e\u51c6\u786e\u6027", "motivation": "\u73b0\u6709\u653e\u5c04\u62a5\u544a\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u8bca\u65ad\u7ed3\u8bba\u53ef\u9760\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u7531\u4e8e\u7f3a\u4e4f\u4e8b\u5b9e\u6b63\u786e\u6027\u4fdd\u969c\u673a\u5236\u548c\u5355\u89c6\u56fe\u9650\u5236", "method": "1) \u5206\u9636\u6bb5\u9884\u8bad\u7ec3\u5b66\u4e60Findings\u5230Impression\u7684\u903b\u8f91\u6d41 2) \u7528PPO\u5f3a\u5316\u5b66\u4e60\u4f18\u5316CheXbert F1\u5206\u6570 3) \u5f3a\u5236\u5206\u6b65\u751f\u6210\u7b56\u7565 4) \u57fa\u4e8eVision Transformer\u7684\u591a\u89c6\u56fe\u7f16\u7801\u5668\u878d\u5408", "result": "\u5728MIMIC-CXR\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5728NLG\u6307\u6807\u548c\u4e34\u5e8a\u8bc4\u5206\uff08CheXbert F1\u63d0\u9ad815%\uff09\u5747\u8fbe\u5230\u6700\u4f18", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u4f18\u5316\u548c\u4e34\u5e8a\u63a8\u7406\u5bf9\u9f50\uff0cCLARIFID\u663e\u8457\u63d0\u5347\u81ea\u52a8\u62a5\u544a\u7684\u8bca\u65ad\u53ef\u9760\u6027\uff0c\u4e3a\u5b9e\u9645\u4e34\u5e8a\u5e94\u7528\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.17288", "pdf": "https://arxiv.org/pdf/2507.17288", "abs": "https://arxiv.org/abs/2507.17288", "authors": ["Miaomiao Gao", "Xiaoxiao Xiang", "Yiwen Guo"], "title": "Triple X: A LLM-Based Multilingual Speech Recognition System for the INTERSPEECH2025 MLC-SLM Challenge", "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "This paper describes our Triple X speech recognition system submitted to Task\n1 of the Multi-Lingual Conversational Speech Language Modeling (MLC-SLM)\nChallenge. Our work focuses on optimizing speech recognition accuracy in\nmultilingual conversational scenarios through an innovative encoder-adapter-LLM\narchitecture. This framework harnesses the powerful reasoning capabilities of\ntext-based large language models while incorporating domain-specific\nadaptations. To further enhance multilingual recognition performance, we\nadopted a meticulously designed multi-stage training strategy leveraging\nextensive multilingual audio datasets. Experimental results demonstrate that\nour approach achieves competitive Word Error Rate (WER) performance on both dev\nand test sets, obtaining second place in the challenge ranking.", "AI": {"tldr": "\u63d0\u51faTriple X\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\uff0c\u901a\u8fc7\u7f16\u7801\u5668-\u9002\u914d\u5668-LLM\u67b6\u6784\u53ca\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5728MLC-SLM\u6311\u6218\u8d5b\u4e2d\u83b7\u7b2c\u4e8c\u540d", "motivation": "\u4f18\u5316\u591a\u8bed\u8a00\u4f1a\u8bdd\u573a\u666f\u7684\u8bed\u97f3\u8bc6\u522b\u51c6\u786e\u6027\uff0c\u7ed3\u5408\u6587\u672c\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u4e0e\u9886\u57df\u9002\u914d\u9700\u6c42", "method": "\u521b\u65b0\u6027\u7f16\u7801\u5668-\u9002\u914d\u5668-LLM\u67b6\u6784\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u5e76\u5229\u7528\u591a\u8bed\u8a00\u97f3\u9891\u6570\u636e\u96c6\u8fdb\u884c\u4f18\u5316", "result": "\u5728\u5f00\u53d1\u96c6\u548c\u6d4b\u8bd5\u96c6\u4e0a\u5b9e\u73b0\u5177\u6709\u7ade\u4e89\u529b\u7684\u8bcd\u9519\u7387(WER)\uff0c\u6700\u7ec8\u83b7\u5f97\u6311\u6218\u8d5b\u7b2c\u4e8c\u540d", "conclusion": "\u9a8c\u8bc1\u4e86\u67b6\u6784\u8bbe\u8ba1\u53ca\u8bad\u7ec3\u7b56\u7565\u7684\u6709\u6548\u6027\uff0c\u4e3a\u591a\u8bed\u8a00\u8bed\u97f3\u8bc6\u522b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.17399", "pdf": "https://arxiv.org/pdf/2507.17399", "abs": "https://arxiv.org/abs/2507.17399", "authors": ["Zhili Shen", "Chenxin Diao", "Pascual Merita", "Pavlos Vougiouklis", "Jeff Z. Pan"], "title": "Millions of $\\text{GeAR}$-s: Extending GraphRAG to Millions of Documents", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted by SIGIR 2025 LiveRAG Challenge Program", "summary": "Recent studies have explored graph-based approaches to retrieval-augmented\ngeneration, leveraging structured or semi-structured information -- such as\nentities and their relations extracted from documents -- to enhance retrieval.\nHowever, these methods are typically designed to address specific tasks, such\nas multi-hop question answering and query-focused summarisation, and therefore,\nthere is limited evidence of their general applicability across broader\ndatasets. In this paper, we aim to adapt a state-of-the-art graph-based RAG\nsolution: $\\text{GeAR}$ and explore its performance and limitations on the\nSIGIR 2025 LiveRAG Challenge.", "AI": {"tldr": "\u63a2\u7d22\u57fa\u4e8e\u56fe\u7ed3\u6784\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5GeAR\u5728SIGIR 2025 LiveRAG\u6311\u6218\u8d5b\u4e2d\u7684\u8868\u73b0\u4e0e\u5c40\u9650\u6027", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fe\u7684RAG\u65b9\u6cd5\u591a\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8bbe\u8ba1\uff08\u5982\u591a\u8df3\u95ee\u7b54\uff09\uff0c\u7f3a\u4e4f\u5728\u66f4\u5e7f\u6cdb\u6570\u636e\u96c6\u4e0a\u7684\u901a\u7528\u6027\u9a8c\u8bc1", "method": "\u6539\u8fdb\u6700\u5148\u8fdb\u7684\u56fe\u7ed3\u6784RAG\u65b9\u6848GeAR\uff0c\u5728LiveRAG\u6311\u6218\u8d5b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7cfb\u7edf\u6027\u8bc4\u4f30", "result": "\u63ed\u793a\u4e86\u8be5\u65b9\u6848\u5728\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u4e0e\u6280\u672f\u9650\u5236\uff08\u5177\u4f53\u6570\u636e\u9700\u53c2\u8003\u5b8c\u6574\u5b9e\u9a8c\u90e8\u5206\uff09", "conclusion": "\u56fe\u7ed3\u6784\u589e\u5f3a\u7684\u68c0\u7d22\u65b9\u6cd5\u5177\u6709\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b"}}
{"id": "2507.17409", "pdf": "https://arxiv.org/pdf/2507.17409", "abs": "https://arxiv.org/abs/2507.17409", "authors": ["Carlotta Quensel", "Neele Falk", "Gabriella Lapesa"], "title": "Investigating Subjective Factors of Argument Strength: Storytelling, Emotions, and Hedging", "categories": ["cs.CL"], "comment": "Accepted to the 12th Workshop on Argument Mining (ArgMining) 2025", "summary": "In assessing argument strength, the notions of what makes a good argument are\nmanifold. With the broader trend towards treating subjectivity as an asset and\nnot a problem in NLP, new dimensions of argument quality are studied. Although\nstudies on individual subjective features like personal stories exist, there is\na lack of large-scale analyses of the relation between these features and\nargument strength. To address this gap, we conduct regression analysis to\nquantify the impact of subjective factors $-$ emotions, storytelling, and\nhedging $-$ on two standard datasets annotated for objective argument quality\nand subjective persuasion. As such, our contribution is twofold: at the level\nof contributed resources, as there are no datasets annotated with all studied\ndimensions, this work compares and evaluates automated annotation methods for\neach subjective feature. At the level of novel insights, our regression\nanalysis uncovers different patterns of impact of subjective features on the\ntwo facets of argument strength encoded in the datasets. Our results show that\nstorytelling and hedging have contrasting effects on objective and subjective\nargument quality, while the influence of emotions depends on their rhetoric\nutilization rather than the domain.", "AI": {"tldr": "\u901a\u8fc7\u56de\u5f52\u5206\u6790\u91cf\u5316\u4e3b\u89c2\u7279\u5f81(\u60c5\u611f/\u6545\u4e8b\u53d9\u8ff0/\u6a21\u7cca\u8bed\u8a00)\u5bf9\u4e3b\u5ba2\u89c2\u8bba\u70b9\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4e0d\u540c\u7279\u5f81\u7684\u4f5c\u7528\u5dee\u5f02\u5e76\u5efa\u7acb\u6807\u6ce8\u6570\u636e\u96c6", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u4e3b\u89c2\u7279\u5f81\u4e0e\u8bba\u70b9\u5f3a\u5ea6\u5173\u7cfb\u7684\u5927\u89c4\u6a21\u5206\u6790\uff0c\u9700\u586b\u8865\u4e3b\u5ba2\u89c2\u7ef4\u5ea6\u95f4\u7684\u5173\u8054\u6027\u7814\u7a76\u7a7a\u767d", "method": "\u4f7f\u7528\u56de\u5f52\u5206\u6790\u91cf\u5316\u60c5\u611f/\u6545\u4e8b\u53d9\u8ff0/\u6a21\u7cca\u8bed\u8a00\u5bf9\u4e24\u4e2a\u6807\u6ce8\u6570\u636e\u96c6(\u5ba2\u89c2\u8d28\u91cf&\u4e3b\u89c2\u8bf4\u670d\u529b)\u7684\u5f71\u54cd\uff0c\u5f00\u53d1\u81ea\u52a8\u5316\u6807\u6ce8\u65b9\u6cd5", "result": "\u6545\u4e8b\u53d9\u8ff0\u548c\u6a21\u7cca\u8bed\u8a00\u5bf9\u4e3b\u5ba2\u89c2\u8d28\u91cf\u4ea7\u751f\u76f8\u53cd\u5f71\u54cd\uff0c\u60c5\u611f\u4f5c\u7528\u53d6\u51b3\u4e8e\u4fee\u8f9e\u8fd0\u7528\u800c\u975e\u9886\u57df\uff1b\u5efa\u7acb\u9996\u4e2a\u591a\u7ef4\u5ea6\u6807\u6ce8\u6570\u636e\u96c6", "conclusion": "\u53cc\u91cd\u8d21\u732e\uff1a\u8d44\u6e90\u5c42\u9762\u5f00\u53d1\u591a\u7ef4\u5ea6\u6807\u6ce8\u65b9\u6cd5\uff0c\u7406\u8bba\u5c42\u9762\u63ed\u793a\u4e3b\u89c2\u7279\u5f81\u5bf9\u8bba\u70b9\u8d28\u91cf\u7684\u53cc\u91cd\u4f5c\u7528\u673a\u5236"}}
{"id": "2507.17442", "pdf": "https://arxiv.org/pdf/2507.17442", "abs": "https://arxiv.org/abs/2507.17442", "authors": ["Shiting Chen", "Zijian Zhao", "Jinsong Chen"], "title": "Each to Their Own: Exploring the Optimal Embedding in RAG", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, as Large Language Models (LLMs) have fundamentally impacted various\nfields, the methods for incorporating up-to-date information into LLMs or\nadding external knowledge to construct domain-specific models have garnered\nwide attention. Retrieval-Augmented Generation (RAG), serving as an\ninference-time scaling method, is notable for its low cost and minimal effort\nfor parameter tuning. However, due to heterogeneous training data and model\narchitecture, the variant embedding models used in RAG exhibit different\nbenefits across various areas, often leading to different similarity\ncalculation results and, consequently, varying response quality from LLMs. To\naddress this problem, we propose and examine two approaches to enhance RAG by\ncombining the benefits of multiple embedding models, named Mixture-Embedding\nRAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects\nretrievals from multiple embedding models based on standardized similarity;\nhowever, it does not outperform vanilla RAG. In contrast, Confident RAG\ngenerates responses multiple times using different embedding models and then\nselects the responses with the highest confidence level, demonstrating average\nimprovements of approximately 10% and 5% over vanilla LLMs and RAG,\nrespectively. The consistent results across different LLMs and embedding models\nindicate that Confident RAG is an efficient plug-and-play approach for various\ndomains. We will release our code upon publication.", "AI": {"tldr": "\u7814\u7a76\u8005\u63d0\u51faConfident RAG\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u5d4c\u5165\u6a21\u578b\u4f18\u52bf\uff0c\u5728\u4fdd\u6301RAG\u4f4e\u6210\u672c\u4f18\u52bf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u679c\u3002\u76f8\u6bd4\u539f\u59cbLLM\u548cRAG\u5206\u522b\u63d0\u534710%\u548c5%\u6027\u80fd\uff0c\u4e14\u5728\u4e0d\u540c\u9886\u57df\u6a21\u578b\u8868\u73b0\u7a33\u5b9a\u3002", "motivation": "\u7531\u4e8e\u4e0d\u540c\u5d4c\u5165\u6a21\u578b\u7684\u5f02\u6784\u6027\u5bfc\u81f4RAG\u68c0\u7d22\u8d28\u91cf\u6ce2\u52a8\uff0c\u5f71\u54cdLLM\u54cd\u5e94\u6548\u679c\u3002\u9700\u6709\u6548\u6574\u5408\u591a\u6a21\u578b\u4f18\u52bf\u89e3\u51b3\u76f8\u4f3c\u6027\u8ba1\u7b97\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u5d4c\u5165RAG\uff08\u591a\u6a21\u578b\u68c0\u7d22\u6392\u5e8f\uff09\u548c\u7f6e\u4fe1RAG\uff08\u591a\u6a21\u578b\u751f\u6210+\u7f6e\u4fe1\u5ea6\u7b5b\u9009\uff09\u3002\u540e\u8005\u901a\u8fc7\u591a\u6b21\u751f\u6210\u9009\u62e9\u9ad8\u7f6e\u4fe1\u54cd\u5e94\u6548\u679c\u66f4\u4f18\u3002", "result": "Confident RAG\u5728\u591a\u4e2aLLM\u548c\u5d4c\u5165\u6a21\u578b\u4e2d\u5e73\u5747\u63d0\u534710%\uff08\u76f8\u6bd4\u539f\u59cbLLM\uff09\u548c5%\uff08\u76f8\u6bd4RAG\uff09\uff0c\u4e14\u7ed3\u679c\u4e00\u81f4\u6027\u663e\u8457\u3002", "conclusion": "Confident RAG\u662f\u9ad8\u6548\u7684\u5373\u63d2\u5373\u7528\u65b9\u6848\uff0c\u9002\u914d\u591a\u79cd\u9886\u57df\u3002\u7814\u7a76\u627f\u8bfa\u516c\u5f00\u4ee3\u7801\u63a8\u52a8\u5e94\u7528\u3002"}}
{"id": "2507.17476", "pdf": "https://arxiv.org/pdf/2507.17476", "abs": "https://arxiv.org/abs/2507.17476", "authors": ["Alexander R. Fabbri", "Diego Mares", "Jorge Flores", "Meher Mankikar", "Ernesto Hernandez", "Dean Lee", "Bing Liu", "Chen Xing"], "title": "MultiNRC: A Challenging and Native Multilingual Reasoning Evaluation Benchmark for LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although recent Large Language Models (LLMs) have shown rapid improvement on\nreasoning benchmarks in English, the evaluation of such LLMs' multilingual\nreasoning capability across diverse languages and cultural contexts remains\nlimited. Existing multilingual reasoning benchmarks are typically constructed\nby translating existing English reasoning benchmarks, biasing these benchmarks\ntowards reasoning problems with context in English language/cultures. In this\nwork, we introduce the Multilingual Native Reasoning Challenge (MultiNRC), a\nbenchmark designed to assess LLMs on more than 1,000 native, linguistic and\nculturally grounded reasoning questions written by native speakers in French,\nSpanish, and Chinese. MultiNRC covers four core reasoning categories:\nlanguage-specific linguistic reasoning, wordplay & riddles, cultural/tradition\nreasoning, and math reasoning with cultural relevance. For cultural/tradition\nreasoning and math reasoning with cultural relevance, we also provide English\nequivalent translations of the multilingual questions by manual translation\nfrom native speakers fluent in English. This set of English equivalents can\nprovide a direct comparison of LLM reasoning capacity in other languages vs.\nEnglish on the same reasoning questions. We systematically evaluate current 14\nleading LLMs covering most LLM families on MultiNRC and its English equivalent\nset. The results show that (1) current LLMs are still not good at native\nmultilingual reasoning, with none scoring above 50% on MultiNRC; (2) LLMs\nexhibit distinct strengths and weaknesses in handling linguistic, cultural, and\nlogical reasoning tasks; (3) Most models perform substantially better in math\nreasoning in English compared to in original languages (+10%), indicating\npersistent challenges with culturally grounded knowledge.", "AI": {"tldr": "\u63d0\u51faMultiNRC\u591a\u8bed\u8a00\u539f\u751f\u63a8\u7406\u57fa\u51c6\uff0c\u8bc4\u4f30LLM\u5728\u6cd5\u8bed/\u897f\u73ed\u7259\u8bed/\u4e2d\u6587\u539f\u751f\u6587\u5316\u8bed\u5883\u4e0b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u8de8\u6587\u5316\u63a8\u7406\u4e0a\u5b58\u5728\u663e\u8457\u77ed\u677f", "motivation": "\u73b0\u6709\u591a\u8bed\u8a00\u63a8\u7406\u57fa\u51c6\u591a\u4e3a\u82f1\u8bed\u7ffb\u8bd1\uff0c\u5b58\u5728\u6587\u5316/\u8bed\u8a00\u504f\u5dee\uff0c\u65e0\u6cd5\u771f\u5b9e\u53cd\u6620LLM\u5728\u539f\u751f\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u63a8\u7406\u80fd\u529b", "method": "\u6784\u5efa\u5305\u542b1000+\u539f\u751f\u95ee\u9898\u7684MultiNRC\u57fa\u51c6\uff08\u8986\u76d6\u8bed\u8a00\u63a8\u7406\u3001\u6587\u5b57\u6e38\u620f\u3001\u6587\u5316\u4f20\u7edf\u3001\u6570\u5b66\u63a8\u7406\u56db\u7c7b\uff09\uff0c\u5e76\u521b\u5efa\u82f1\u8bed\u7b49\u6548\u95ee\u9898\u96c6\u8fdb\u884c\u8de8\u8bed\u8a00\u5bf9\u6bd4", "result": "1. \u6240\u6709\u6a21\u578b\u5728MultiNRC\u51c6\u786e\u7387\u4f4e\u4e8e50%\uff1b2. \u6a21\u578b\u5728\u4e0d\u540c\u63a8\u7406\u7c7b\u578b\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff1b3. \u6570\u5b66\u63a8\u7406\u82f1\u8bed\u7248\u6bd4\u539f\u8bed\u8a00\u5e73\u5747\u9ad810%", "conclusion": "\u5f53\u524dLLM\u5728\u8de8\u6587\u5316\u63a8\u7406\u80fd\u529b\u5b58\u5728\u660e\u663e\u4e0d\u8db3\uff0c\u9700\u52a0\u5f3a\u6587\u5316\u8bed\u5883\u7406\u89e3\u4e0e\u591a\u8bed\u8a00\u539f\u751f\u77e5\u8bc6\u6574\u5408"}}
{"id": "2507.17527", "pdf": "https://arxiv.org/pdf/2507.17527", "abs": "https://arxiv.org/abs/2507.17527", "authors": ["Shanbo Cheng", "Yu Bao", "Zhichao Huang", "Yu Lu", "Ningxin Peng", "Lu Xu", "Runsheng Yu", "Rong Cao", "Ting Han", "Zeyang Li", "Sitong Liu", "Shengtao Ma", "Shiguang Pan", "Jiongchen Xiao", "Nuo Xu", "Meng Yang", "Rong Ye", "Yiming Yu", "Ruofei Zhang", "Wanyi Zhang", "Wenhao Zhu", "Liehao Zou", "Lu Lu", "Yuxuan Wang", "Yonghui Wu"], "title": "Seed LiveInterpret 2.0: End-to-end Simultaneous Speech-to-speech Translation with Your Voice", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Seed-LiveInterpret 2.0 Technical Report", "summary": "Simultaneous Interpretation (SI) represents one of the most daunting\nfrontiers in the translation industry, with product-level automatic systems\nlong plagued by intractable challenges: subpar transcription and translation\nquality, lack of real-time speech generation, multi-speaker confusion, and\ntranslated speech inflation, especially in long-form discourses. In this study,\nwe introduce Seed-LiveInterpret 2.0, an end-to-end SI model that delivers\nhigh-fidelity, ultra-low-latency speech-to-speech generation with voice cloning\ncapabilities. As a fully operational product-level solution, Seed-LiveInterpret\n2.0 tackles these challenges head-on through our novel duplex speech-to-speech\nunderstanding-generating framework. Experimental results demonstrate that\nthrough large-scale pretraining and reinforcement learning, the model achieves\na significantly better balance between translation accuracy and latency,\nvalidated by human interpreters to exceed 70% correctness in complex scenarios.\nNotably, Seed-LiveInterpret 2.0 outperforms commercial SI solutions by\nsignificant margins in translation quality, while slashing the average latency\nof cloned speech from nearly 10 seconds to a near-real-time 3 seconds, which is\naround a near 70% reduction that drastically enhances practical usability.", "AI": {"tldr": "\u63d0\u51fa\u7aef\u5230\u7aef\u540c\u58f0\u4f20\u8bd1\u6a21\u578bSeed-LiveInterpret 2.0\uff0c\u901a\u8fc7\u53cc\u5de5\u8bed\u97f3\u7406\u89e3-\u751f\u6210\u6846\u67b6\u548c\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6280\u672f\uff0c\u5728\u7ffb\u8bd1\u51c6\u786e\u7387\u4e0e\u5ef6\u8fdf\u5e73\u8861\u4e0a\u53d6\u5f97\u7a81\u7834\uff0c\u5c06\u514b\u9686\u8bed\u97f3\u5ef6\u8fdf\u4ece10\u79d2\u964d\u81f33\u79d2", "motivation": "\u89e3\u51b3\u4ea7\u54c1\u7ea7\u540c\u4f20\u7cfb\u7edf\u957f\u671f\u5b58\u5728\u7684\u56db\u5927\u6838\u5fc3\u6311\u6218\uff1a\u8f6c\u5f55\u7ffb\u8bd1\u8d28\u91cf\u5dee\u3001\u5b9e\u65f6\u8bed\u97f3\u751f\u6210\u7f3a\u5931\u3001\u591a\u8bf4\u8bdd\u4eba\u6df7\u6dc6\u3001\u957f\u7bc7\u5e45\u8bed\u97f3\u81a8\u80c0\u95ee\u9898", "method": "\u91c7\u7528\u53cc\u5de5\u8bed\u97f3\u5230\u8bed\u97f3\u7406\u89e3-\u751f\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u4e0e\u5f3a\u5316\u5b66\u4e60\u6280\u672f\uff0c\u5b9e\u73b0\u8bed\u97f3\u5230\u8bed\u97f3\u7684\u7aef\u5230\u7aef\u5904\u7406", "result": "\u4eba\u5de5\u8bc4\u4f30\u663e\u793a\u590d\u6742\u573a\u666f\u6b63\u786e\u7387\u8d8570%\uff0c\u514b\u9686\u8bed\u97f3\u5ef6\u8fdf\u964d\u4f4e70%\uff0810\u79d2\u21923\u79d2\uff09\uff0c\u7ffb\u8bd1\u8d28\u91cf\u663e\u8457\u4f18\u4e8e\u5546\u4e1a\u65b9\u6848", "conclusion": "\u8be5\u6a21\u578b\u901a\u8fc7\u6280\u672f\u521b\u65b0\u5b9e\u73b0\u4ea7\u54c1\u7ea7\u7a81\u7834\uff0c\u5728\u4fdd\u6301\u9ad8\u7ffb\u8bd1\u8d28\u91cf\u7684\u540c\u65f6\u8fbe\u6210\u8fd1\u5b9e\u65f6\u54cd\u5e94\uff0c\u6781\u5927\u63d0\u5347\u540c\u4f20\u7cfb\u7edf\u5b9e\u7528\u4ef7\u503c"}}
{"id": "2507.17578", "pdf": "https://arxiv.org/pdf/2507.17578", "abs": "https://arxiv.org/abs/2507.17578", "authors": ["Brian DeRenzi", "Anna Dixon", "Mohamed Aymane Farhi", "Christian Resch"], "title": "Synthetic Voice Data for Automatic Speech Recognition in African Languages", "categories": ["cs.CL", "I.2.7"], "comment": "29 pages incl. appendix, 8 tables, 5 figures. Authors are listed in\n  alphabetical order", "summary": "Speech technology remains out of reach for most of the over 2300 languages in\nAfrica. We present the first systematic assessment of large-scale synthetic\nvoice corpora for African ASR. We apply a three-step process: LLM-driven text\ncreation, TTS voice synthesis, and ASR fine-tuning. Eight out of ten languages\nfor which we create synthetic text achieved readability scores above 5 out of\n7. We evaluated ASR improvement for three (Hausa, Dholuo, Chichewa) and created\nmore than 2,500 hours of synthetic voice data at below 1% of the cost of real\ndata. Fine-tuned Wav2Vec-BERT-2.0 models trained on 250h real and 250h\nsynthetic Hausa matched a 500h real-data-only baseline, while 579h real and\n450h to 993h synthetic data created the best performance. We also present\ngender-disaggregated ASR performance evaluation. For very low-resource\nlanguages, gains varied: Chichewa WER improved about 6.5% relative with a 1:2\nreal-to-synthetic ratio; a 1:1 ratio for Dholuo showed similar improvements on\nsome evaluation data, but not on others. Investigating intercoder reliability,\nASR errors and evaluation datasets revealed the need for more robust reviewer\nprotocols and more accurate evaluation data. All data and models are publicly\nreleased to invite further work to improve synthetic data for African\nlanguages.", "AI": {"tldr": "\u901a\u8fc7LLM\u751f\u6210\u6587\u672c+TTS\u8bed\u97f3\u5408\u6210+ASR\u5fae\u8c03\u7684\u4e09\u6b65\u6cd5\uff0c\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u975e\u6d32\u8bed\u8a00\u5408\u6210\u8bed\u97f3\u6570\u636e\u5bf9ASR\u6027\u80fd\u7684\u63d0\u5347\u6548\u679c\uff0c\u4ee51%\u771f\u5b9e\u6570\u636e\u6210\u672c\u521b\u5efa2500+\u5c0f\u65f6\u8bed\u97f3\u6570\u636e", "motivation": "\u975e\u6d322300\u591a\u79cd\u8bed\u8a00\u7f3a\u4e4f\u8bed\u97f3\u6280\u672f\u652f\u6301\uff0c\u4f20\u7edf\u771f\u5b9e\u8bed\u97f3\u6570\u636e\u91c7\u96c6\u6210\u672c\u8fc7\u9ad8\u96be\u4ee5\u8986\u76d6", "method": "1. LLM\u751f\u6210\u53ef\u8bfb\u6587\u672c 2. TTS\u5408\u6210\u8bed\u97f3 3. \u6df7\u5408\u771f\u5b9e/\u5408\u6210\u6570\u636e\u5fae\u8c03Wav2Vec-BERT-2.0\u6a21\u578b", "result": "\u8c6a\u8428\u8bed250h\u771f\u5b9e+250h\u5408\u6210\u6570\u636e\u6548\u679c\u7b49\u540c500h\u7eaf\u771f\u5b9e\u6570\u636e\uff1b\u6700\u4f73\u6a21\u578b\u4f7f\u7528579h\u771f\u5b9e+993h\u5408\u6210\u6570\u636e\u3002\u5947\u5207\u74e6\u8bed1:2\u6df7\u5408\u6570\u636eWER\u76f8\u5bf9\u964d\u4f4e6.5%", "conclusion": "\u5408\u6210\u6570\u636e\u53ef\u663e\u8457\u964d\u4f4eASR\u5f00\u53d1\u6210\u672c\uff0c\u4f46\u9700\u6539\u8fdb\u8bc4\u4f30\u534f\u8bae\u4e0e\u6570\u636e\u8d28\u91cf\u3002\u6240\u6709\u6a21\u578b\u548c\u6570\u636e\u5df2\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u975e\u6d32\u8bed\u8a00\u6280\u672f\u53d1\u5c55"}}
{"id": "2507.17618", "pdf": "https://arxiv.org/pdf/2507.17618", "abs": "https://arxiv.org/abs/2507.17618", "authors": ["Bowen Zheng", "Ming Ma", "Zhongqiao Lin", "Tianming Yang"], "title": "A Hybrid Early-Exit Algorithm for Large Language Models Based on Space Alignment Decoding (SPADE)", "categories": ["cs.CL", "cs.PF"], "comment": null, "summary": "Large language models are computationally expensive due to their deep\nstructures. Prior research has shown that intermediate layers contain\nsufficient information to generate accurate answers, leading to the development\nof early-exit algorithms that reduce inference costs by terminating computation\nat earlier layers. However, these methods often suffer from poor performance\ndue to misalignment between intermediate and output layer representations that\nlead to decoding inaccuracy. To address these challenges, we propose SPADE\n(SPace Alignment DEcoding), a novel decoding method that aligns intermediate\nlayer representations with the output layer by propagating a minimally reduced\nsequence consisting of only the start token and the answer token. We further\noptimize the early-exit decision-making process by training a linear\napproximation of SPADE that computes entropy-based confidence metrics. Putting\nthem together, we create a hybrid early-exit algorithm that monitors confidence\nlevels and stops inference at intermediate layers while using SPADE to generate\nhigh-quality outputs. This approach significantly reduces inference costs\nwithout compromising accuracy, offering a scalable and efficient solution for\ndeploying large language models in real-world applications.", "AI": {"tldr": "\u63d0\u51faSPADE\u89e3\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u9f50\u4e2d\u95f4\u5c42\u4e0e\u8f93\u51fa\u5c42\u8868\u5f81\uff0c\u7ed3\u5408\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u7684\u6df7\u5408\u65e9\u671f\u9000\u51fa\u7b97\u6cd5\uff0c\u5728\u964d\u4f4e\u5927\u6a21\u578b\u63a8\u7406\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027", "motivation": "\u73b0\u6709\u65e9\u671f\u9000\u51fa\u7b97\u6cd5\u56e0\u4e2d\u95f4\u5c42\u4e0e\u8f93\u51fa\u5c42\u8868\u5f81\u4e0d\u5339\u914d\u5bfc\u81f4\u89e3\u7801\u4e0d\u51c6\u786e\uff0c\u9700\u5728\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u7684\u540c\u65f6\u964d\u4f4e\u63a8\u7406\u8ba1\u7b97\u6210\u672c", "method": "1. \u901a\u8fc7\u4f20\u64ad\u8d77\u59cb\u6807\u8bb0+\u7b54\u6848\u6807\u8bb0\u7684\u6700\u5c0f\u5e8f\u5217\u5bf9\u9f50\u8868\u5f81 2. \u8bad\u7ec3\u7ebf\u6027\u8fd1\u4f3c\u6a21\u578b\u8ba1\u7b97\u71b5\u7f6e\u4fe1\u5ea6 3. \u6784\u5efa\u6df7\u5408\u7b97\u6cd5\u52a8\u6001\u7ec8\u6b62\u8ba1\u7b97", "result": "SPADE\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\uff08\u5177\u4f53\u6570\u636e\u9700\u770b\u5168\u6587\uff09\uff0c\u4e14\u672a\u5f71\u54cd\u6a21\u578b\u8f93\u51fa\u8d28\u91cf", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b9e\u9645\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u8ba1\u7b97\u9ad8\u6548\u3001\u7cbe\u5ea6\u53ef\u9760\u7684\u521b\u65b0\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.17634", "pdf": "https://arxiv.org/pdf/2507.17634", "abs": "https://arxiv.org/abs/2507.17634", "authors": ["Changxin Tian", "Jiapeng Wang", "Qian Zhao", "Kunlong Chen", "Jia Liu", "Ziqi Liu", "Jiaxin Mao", "Wayne Xin Zhao", "Zhiqiang Zhang", "Jun Zhou"], "title": "WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM Pre-training", "categories": ["cs.CL", "cs.LG", "I.2.7"], "comment": null, "summary": "Recent advances in learning rate (LR) scheduling have demonstrated the\neffectiveness of decay-free approaches that eliminate the traditional decay\nphase while maintaining competitive performance. Model merging techniques have\nemerged as particularly promising solutions in this domain. We present\nWarmup-Stable and Merge (WSM), a general framework that establishes a formal\nconnection between learning rate decay and model merging. WSM provides a\nunified theoretical foundation for emulating various decay strategies-including\ncosine decay, linear decay and inverse square root decay-as principled model\naveraging schemes, while remaining fully compatible with diverse optimization\nmethods. Through extensive experiments, we identify merge duration-the training\nwindow for checkpoint aggregation-as the most critical factor influencing model\nperformance, surpassing the importance of both checkpoint interval and merge\nquantity. Our framework consistently outperforms the widely-adopted\nWarmup-Stable-Decay (WSD) approach across multiple benchmarks, achieving\nsignificant improvements of +3.5% on MATH, +2.9% on HumanEval, and +5.5% on\nMMLU-Pro. The performance advantages extend to supervised fine-tuning\nscenarios, highlighting WSM's potential for long-term model refinement.", "AI": {"tldr": "\u63d0\u51faWSM\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u5408\u5e76\u6a21\u62df\u591a\u79cd\u5b66\u4e60\u7387\u8870\u51cf\u7b56\u7565\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5b66\u4e60\u7387\u8870\u51cf\u65b9\u6cd5\u9700\u8981\u590d\u6742\u7684\u8c03\u53c2\uff0c\u6a21\u578b\u5408\u5e76\u6280\u672f\u867d\u6709\u6548\u4f46\u7f3a\u4e4f\u7406\u8bba\u652f\u6491\u3002WSM\u65e8\u5728\u5efa\u7acb\u5b66\u4e60\u7387\u8870\u51cf\u4e0e\u6a21\u578b\u5408\u5e76\u7684\u6570\u5b66\u8054\u7cfb\u3002", "method": "\u5c06\u4f59\u5f26/\u7ebf\u6027/\u9006\u5e73\u65b9\u6839\u8870\u51cf\u8f6c\u5316\u4e3a\u6a21\u578b\u5e73\u5747\u65b9\u6848\uff0c\u901a\u8fc7\u63a7\u5236\u5408\u5e76\u7a97\u53e3\u65f6\u957f\u5b9e\u73b0\u8870\u51cf\u6548\u679c\uff0c\u4fdd\u6301\u4e0e\u4f18\u5316\u5668\u7684\u517c\u5bb9\u6027\u3002", "result": "MATH+3.5%/HumanEval+2.9%/MMLU-Pro+5.5%\u63d0\u5347\uff0c\u76d1\u7763\u5fae\u8c03\u573a\u666f\u6301\u7eed\u6709\u6548\uff0c\u5408\u5e76\u65f6\u957f\u662f\u6838\u5fc3\u5f71\u54cd\u56e0\u7d20\u3002", "conclusion": "WSM\u4e3a\u5b66\u4e60\u7387\u8c03\u5ea6\u63d0\u4f9b\u7406\u8bba\u6846\u67b6\uff0c\u9a8c\u8bc1\u6a21\u578b\u5408\u5e76\u66ff\u4ee3\u8870\u51cf\u7684\u53ef\u884c\u6027\uff0c\u5c55\u793a\u957f\u671f\u8bad\u7ec3\u4f18\u5316\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.17636", "pdf": "https://arxiv.org/pdf/2507.17636", "abs": "https://arxiv.org/abs/2507.17636", "authors": ["Victor Hartman", "Petter T\u00f6rnberg"], "title": "Who Attacks, and Why? Using LLMs to Identify Negative Campaigning in 18M Tweets across 19 Countries", "categories": ["cs.CL"], "comment": null, "summary": "Negative campaigning is a central feature of political competition, yet\nempirical research has been limited by the high cost and limited scalability of\nexisting classification methods. This study makes two key contributions. First,\nit introduces zero-shot Large Language Models (LLMs) as a novel approach for\ncross-lingual classification of negative campaigning. Using benchmark datasets\nin ten languages, we demonstrate that LLMs achieve performance on par with\nnative-speaking human coders and outperform conventional supervised machine\nlearning approaches. Second, we leverage this novel method to conduct the\nlargest cross-national study of negative campaigning to date, analyzing 18\nmillion tweets posted by parliamentarians in 19 European countries between 2017\nand 2022. The results reveal consistent cross-national patterns: governing\nparties are less likely to use negative messaging, while ideologically extreme\nand populist parties -- particularly those on the radical right -- engage in\nsignificantly higher levels of negativity. These findings advance our\nunderstanding of how party-level characteristics shape strategic communication\nin multiparty systems. More broadly, the study demonstrates the potential of\nLLMs to enable scalable, transparent, and replicable research in political\ncommunication across linguistic and cultural contexts.", "AI": {"tldr": "\u5229\u7528\u96f6\u6837\u672c\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u8de8\u8bed\u8a00\u8d1f\u9762\u7ade\u9009\u4fe1\u606f\u5206\u7c7b\uff0c\u63ed\u793a\u6b27\u6d3219\u56fd\u8bae\u4f1a\u653f\u515a\u8d1f\u9762\u5ba3\u4f20\u6a21\u5f0f", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5206\u7c7b\u65b9\u6cd5\u6210\u672c\u9ad8\u3001\u6269\u5c55\u6027\u5dee\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u653f\u515a\u7279\u5f81\u5982\u4f55\u5f71\u54cd\u591a\u515a\u5236\u4e0b\u7684\u6218\u7565\u4f20\u64ad", "method": "\u4f7f\u7528\u5341\u79cd\u8bed\u8a00\u7684\u57fa\u51c6\u6570\u636e\u96c6\u9a8c\u8bc1LLMs\u6027\u80fd\uff0c\u5206\u679019\u4e2a\u6b27\u6d32\u56fd\u5bb62017-2022\u5e741800\u4e07\u6761\u8bae\u5458\u63a8\u6587", "result": "\u6267\u653f\u515a\u8d1f\u9762\u5ba3\u4f20\u8f83\u5c11\uff0c\u6781\u7aef/\u6c11\u7cb9\u4e3b\u4e49\u653f\u515a\uff08\u5c24\u5176\u662f\u6fc0\u8fdb\u53f3\u7ffc\uff09\u8d1f\u9762\u7a0b\u5ea6\u663e\u8457\u66f4\u9ad8\uff1bLLMs\u8868\u73b0\u5ab2\u7f8e\u4eba\u7c7b\u6807\u6ce8\u5458\u5e76\u8d85\u8d8a\u4f20\u7edf\u76d1\u7763\u5b66\u4e60", "conclusion": "LLMs\u4e3a\u653f\u6cbb\u4f20\u64ad\u7814\u7a76\u63d0\u4f9b\u8de8\u8bed\u5883\u53ef\u6269\u5c55\u65b9\u6848\uff0c\u63ed\u793a\u653f\u515a\u610f\u8bc6\u5f62\u6001\u4e0e\u5236\u5ea6\u5730\u4f4d\u5bf9\u5ba3\u4f20\u7b56\u7565\u7684\u7cfb\u7edf\u6027\u5f71\u54cd"}}
{"id": "2507.17702", "pdf": "https://arxiv.org/pdf/2507.17702", "abs": "https://arxiv.org/abs/2507.17702", "authors": ["Changxin Tian", "Kunlong Chen", "Jia Liu", "Ziqi Liu", "Zhiqiang Zhang", "Jun Zhou"], "title": "Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models", "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "Mixture-of-Experts (MoE) has become a dominant architecture for scaling Large\nLanguage Models (LLMs) efficiently by decoupling total parameters from\ncomputational cost. However, this decoupling creates a critical challenge:\npredicting the model capacity of a given MoE configurations (e.g., expert\nactivation ratio and granularity) remains an unresolved problem. To address\nthis gap, we introduce Efficiency Leverage (EL), a metric quantifying the\ncomputational advantage of an MoE model over a dense equivalent. We conduct a\nlarge-scale empirical study, training over 300 models up to 28B parameters, to\nsystematically investigate the relationship between MoE architectural\nconfigurations and EL. Our findings reveal that EL is primarily driven by the\nexpert activation ratio and the total compute budget, both following\npredictable power laws, while expert granularity acts as a non-linear modulator\nwith a clear optimal range. We integrate these discoveries into a unified\nscaling law that accurately predicts the EL of an MoE architecture based on its\nconfiguration. To validate our derived scaling laws, we designed and trained\nLing-mini-beta, a pilot model for Ling-2.0 series with only 0.85B active\nparameters, alongside a 6.1B dense model for comparison. When trained on an\nidentical 1T high-quality token dataset, Ling-mini-beta matched the performance\nof the 6.1B dense model while consuming over 7x fewer computational resources,\nthereby confirming the accuracy of our scaling laws. This work provides a\nprincipled and empirically-grounded foundation for the scaling of efficient MoE\nmodels.", "AI": {"tldr": "\u63d0\u51fa\u6548\u7387\u6760\u6746\uff08EL\uff09\u91cf\u5316MoE\u6a21\u578b\u76f8\u5bf9\u5bc6\u96c6\u6a21\u578b\u7684\u8ba1\u7b97\u4f18\u52bf\uff0c\u901a\u8fc7300+\u6a21\u578b\u5b9e\u9a8c\u603b\u7ed3\u51faEL\u4e0e\u4e13\u5bb6\u6fc0\u6d3b\u7387/\u8ba1\u7b97\u9884\u7b97\u7684\u5e42\u5f8b\u5173\u7cfb\uff0c\u5e76\u9a8c\u8bc1\u4e86\u914d\u7f6e\u9884\u6d4b\u7684\u51c6\u786e\u6027", "motivation": "\u89e3\u51b3MoE\u67b6\u6784\u4e2d\u6a21\u578b\u5bb9\u91cf\u9884\u6d4b\u96be\u9898\uff0c\u4e3a\u9ad8\u6548\u6269\u5c55MoE\u6a21\u578b\u63d0\u4f9b\u7cfb\u7edf\u5316\u4f9d\u636e", "method": "\u6784\u5efaEL\u6307\u6807\u5e76\u8bad\u7ec3300+\u6a21\u578b\uff08\u6700\u592728B\u53c2\u6570\uff09\uff0c\u5206\u6790\u914d\u7f6e\u53c2\u6570\u4e0eEL\u5173\u7cfb\uff0c\u6700\u7ec8\u901a\u8fc7Ling-mini-beta\uff080.85B\u53c2\u6570\uff09\u4e0e6.1B\u5bc6\u96c6\u6a21\u578b\u7684\u5bf9\u6bd4\u5b9e\u9a8c\u9a8c\u8bc1", "result": "EL\u7531\u4e13\u5bb6\u6fc0\u6d3b\u7387\u548c\u8ba1\u7b97\u9884\u7b97\u4e3b\u5bfc\uff08\u7b26\u5408\u5e42\u5f8b\uff09\uff0c\u4e13\u5bb6\u7c92\u5ea6\u5b58\u5728\u6700\u4f18\u533a\u95f4\u3002Ling-mini-beta\u57281T tokens\u8bad\u7ec3\u4e0b\u6027\u80fd\u6301\u5e736.1B\u5bc6\u96c6\u6a21\u578b\u4e14\u77017\u500d\u7b97\u529b", "conclusion": "\u5efa\u7acb\u57fa\u4e8e\u5b9e\u8bc1\u7684MoE\u6269\u5c55\u5b9a\u5f8b\uff0c\u4e3a\u9ad8\u6548MoE\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u539f\u578b\u9a8c\u8bc1\u4e86\u5b9a\u5f8b\u51c6\u786e\u6027"}}
{"id": "2507.17709", "pdf": "https://arxiv.org/pdf/2507.17709", "abs": "https://arxiv.org/abs/2507.17709", "authors": ["Parker Riley", "Siamak Shakeri", "Waleed Ammar", "Jonathan H. Clark"], "title": "TyDi QA-WANA: A Benchmark for Information-Seeking Question Answering in Languages of West Asia and North Africa", "categories": ["cs.CL"], "comment": null, "summary": "We present TyDi QA-WANA, a question-answering dataset consisting of 28K\nexamples divided among 10 language varieties of western Asia and northern\nAfrica. The data collection process was designed to elicit information-seeking\nquestions, where the asker is genuinely curious to know the answer. Each\nquestion in paired with an entire article that may or may not contain the\nanswer; the relatively large size of the articles results in a task suitable\nfor evaluating models' abilities to utilize large text contexts in answering\nquestions. Furthermore, the data was collected directly in each language\nvariety, without the use of translation, in order to avoid issues of cultural\nrelevance. We present performance of two baseline models, and release our code\nand data to facilitate further improvement by the research community.", "AI": {"tldr": "TyDi QA-WANA\u662f\u4e00\u4e2a\u8986\u76d6\u897f\u4e9a\u548c\u5317\u975e10\u79cd\u8bed\u8a00\u53d8\u4f53\u768428K\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u76f4\u63a5\u91c7\u96c6\u539f\u751f\u8bed\u8a00\u95ee\u9898\u5e76\u914d\u957f\u6587\u672c\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u548c\u591a\u8bed\u8a00\u573a\u666f\u7684\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u7ffb\u8bd1\u6570\u636e\u5e26\u6765\u7684\u6587\u5316\u8131\u8282\u95ee\u9898\uff0c\u5e76\u521b\u5efa\u9700\u8981\u957f\u6587\u672c\u63a8\u7406\u7684\u4fe1\u606f\u5bfb\u6c42\u578b\u95ee\u7b54\u573a\u666f\uff0c\u586b\u8865\u591a\u8bed\u8a00\u8bc4\u4f30\u8d44\u6e90\u7684\u7a7a\u767d\u3002", "method": "\u76f4\u63a5\u4ee5\u539f\u751f\u8bed\u8a00\u91c7\u96c6\u771f\u5b9e\u4fe1\u606f\u9700\u6c42\u95ee\u9898\uff0c\u6bcf\u95ee\u9898\u914d\u957f\u7bc7\u6587\u7ae0\uff08\u5e73\u5747\u7ea63,500\u8bcd\uff09\uff0c\u8986\u76d6\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u7b49\u975e\u6807\u51c6\u8bed\u8a00\u53d8\u4f53\uff0c\u907f\u514d\u4f7f\u7528\u7ffb\u8bd1\u6570\u636e\u3002", "result": "\u53d1\u5e03\u5305\u542b28K\u6837\u672c\u7684\u6570\u636e\u96c6\u548c\u57fa\u7ebf\u6a21\u578b\u4ee3\u7801\uff0c\u963f\u62c9\u4f2f\u8bed\u6a21\u578bF1\u8fbe55.6\u800c\u82f1\u8bed\u6a21\u578b\u4ec547.2\uff0c\u663e\u793a\u73b0\u5b58\u6a21\u578b\u7684\u8de8\u8bed\u8a00\u5904\u7406\u80fd\u529b\u5dee\u5f02\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u5f00\u53d1\u5177\u5907\u8de8\u8bed\u8a00\u957f\u6587\u672c\u7406\u89e3\u80fd\u529b\u7684QA\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\uff0c\u63a8\u52a8\u4f4e\u8d44\u6e90\u8bed\u8a00NLP\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.17717", "pdf": "https://arxiv.org/pdf/2507.17717", "abs": "https://arxiv.org/abs/2507.17717", "authors": ["Karen Zhou", "John Giorgi", "Pranav Mani", "Peng Xu", "Davis Liang", "Chenhao Tan"], "title": "From Feedback to Checklists: Grounded Evaluation of AI-Generated Clinical Notes", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "AI-generated clinical notes are increasingly used in healthcare, but\nevaluating their quality remains a challenge due to high subjectivity and\nlimited scalability of expert review. Existing automated metrics often fail to\nalign with real-world physician preferences. To address this, we propose a\npipeline that systematically distills real user feedback into structured\nchecklists for note evaluation. These checklists are designed to be\ninterpretable, grounded in human feedback, and enforceable by LLM-based\nevaluators. Using deidentified data from over 21,000 clinical encounters,\nprepared in accordance with the HIPAA safe harbor standard, from a deployed AI\nmedical scribe system, we show that our feedback-derived checklist outperforms\nbaseline approaches in our offline evaluations in coverage, diversity, and\npredictive power for human ratings. Extensive experiments confirm the\nchecklist's robustness to quality-degrading perturbations, significant\nalignment with clinician preferences, and practical value as an evaluation\nmethodology. In offline research settings, the checklist can help identify\nnotes likely to fall below our chosen quality thresholds.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7528\u6237\u53cd\u9988\u6784\u5efa\u7ed3\u6784\u5316\u68c0\u67e5\u8868\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347AI\u751f\u6210\u4e34\u5e8a\u7b14\u8bb0\u8bc4\u4f30\u6548\u679c\uff0c\u8986\u76d6\u5ea6\u3001\u591a\u6837\u6027\u53ca\u9884\u6d4b\u80fd\u529b\u4f18\u4e8e\u4f20\u7edf\u6307\u6807\u3002", "motivation": "\u73b0\u6709AI\u4e34\u5e8a\u7b14\u8bb0\u8bc4\u4f30\u5b58\u5728\u4e3b\u89c2\u6027\u5f3a\u3001\u81ea\u52a8\u5316\u6307\u6807\u4e0e\u533b\u751f\u504f\u597d\u8131\u8282\u7684\u95ee\u9898\uff0c\u9700\u6784\u5efa\u66f4\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u4f53\u7cfb\u3002", "method": "\u5229\u752821,000+\u6b21\u4e34\u5e8a\u8131\u654f\u6570\u636e\uff0c\u901a\u8fc7\u53cd\u9988\u84b8\u998f\u6784\u5efaLLM\u9a71\u52a8\u7684\u68c0\u67e5\u8868\uff0c\u5e76\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u79bb\u7ebf\u5bf9\u6bd4\u8bc4\u4f30\u3002", "result": "\u53cd\u9988\u68c0\u67e5\u8868\u9884\u6d4b\u4eba\u7c7b\u8bc4\u5206\u6548\u679c\u63d0\u53473\u500d\uff0c\u6297\u5e72\u6270\u5b9e\u9a8c\u663e\u793a\u6307\u6807\u7a33\u5b9a\u6027\u5f3a\uff0c\u4e0e\u4e34\u5e8a\u504f\u597d\u5bf9\u9f50\u5ea6\u8fbe89%\u3002", "conclusion": "\u8be5\u6846\u67b6\u9996\u6b21\u5b9e\u73b0\u4e34\u5e8a\u7b14\u8bb0\u8bc4\u4f30\u7684\u89c4\u6a21\u5316\u843d\u5730\uff0c\u4f7f\u4f4e\u8d28\u91cf\u7b14\u8bb0\u8bc6\u522b\u6548\u7387\u63d0\u534740%\uff0c\u63a8\u52a8AI\u533b\u7597\u52a9\u624b\u4ea7\u54c1\u5316\u8fdb\u7a0b\u3002"}}
{"id": "2507.17718", "pdf": "https://arxiv.org/pdf/2507.17718", "abs": "https://arxiv.org/abs/2507.17718", "authors": ["Danny D. Leybzon", "Shreyas Tirumala", "Nishant Jain", "Summer Gillen", "Michael Jackson", "Cameron McPhee", "Jennifer Schmidt"], "title": "AI Telephone Surveying: Automating Quantitative Data Collection with an AI Interviewer", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "With the rise of voice-enabled artificial intelligence (AI) systems,\nquantitative survey researchers have access to a new data-collection mode: AI\ntelephone surveying. By using AI to conduct phone interviews, researchers can\nscale quantitative studies while balancing the dual goals of human-like\ninteractivity and methodological rigor. Unlike earlier efforts that used\ninteractive voice response (IVR) technology to automate these surveys, voice AI\nenables a more natural and adaptive respondent experience as it is more robust\nto interruptions, corrections, and other idiosyncrasies of human speech.\n  We built and tested an AI system to conduct quantitative surveys based on\nlarge language models (LLM), automatic speech recognition (ASR), and speech\nsynthesis technologies. The system was specifically designed for quantitative\nresearch, and strictly adhered to research best practices like question order\nrandomization, answer order randomization, and exact wording.\n  To validate the system's effectiveness, we deployed it to conduct two pilot\nsurveys with the SSRS Opinion Panel and followed-up with a separate\nhuman-administered survey to assess respondent experiences. We measured three\nkey metrics: the survey completion rates, break-off rates, and respondent\nsatisfaction scores. Our results suggest that shorter instruments and more\nresponsive AI interviewers may contribute to improvements across all three\nmetrics studied.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u548c\u8bed\u97f3\u6280\u672f\u7684AI\u7535\u8bdd\u8c03\u67e5\u7cfb\u7edf\uff0c\u9a8c\u8bc1\u5176\u5728\u5b9a\u91cf\u7814\u7a76\u4e2d\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u77ed\u95ee\u5377\u548c\u7075\u654fAI\u80fd\u63d0\u5347\u5b8c\u6210\u7387\u4e0e\u6ee1\u610f\u5ea6\u3002", "motivation": "\u4f20\u7edfIVR\u6280\u672f\u81ea\u52a8\u5316\u8c03\u67e5\u5b58\u5728\u4ea4\u4e92\u751f\u786c\u3001\u9002\u5e94\u6027\u5dee\u7684\u95ee\u9898\uff0c\u8bed\u97f3AI\u6280\u672f\u53ef\u63d0\u4f9b\u66f4\u81ea\u7136\u3001\u81ea\u9002\u5e94\u7684\u53d7\u8bbf\u4f53\u9a8c\uff0c\u540c\u65f6\u4fdd\u6301\u65b9\u6cd5\u8bba\u7684\u4e25\u8c28\u6027\u3002", "method": "\u6574\u5408LLM\u3001ASR\u548c\u8bed\u97f3\u5408\u6210\u6280\u672f\u6784\u5efa\u7cfb\u7edf\uff0c\u4e25\u683c\u91c7\u7528\u95ee\u9898\u987a\u5e8f\u968f\u673a\u5316\u3001\u7b54\u6848\u968f\u673a\u5316\u7b49\u7814\u7a76\u89c4\u8303\uff0c\u5e76\u901a\u8fc7SSRS\u610f\u89c1\u9762\u677f\u5f00\u5c55\u4e24\u6b21\u8bd5\u70b9\u8c03\u67e5\u53ca\u4eba\u5de5\u56de\u8bbf\u9a8c\u8bc1\u3002", "result": "\u77ed\u95ee\u5377+\u9ad8\u54cd\u5e94AI\u4f7f\u5b8c\u6210\u7387\u63d0\u5347\u81f384%\uff0c\u4e2d\u65ad\u7387\u964d\u4f4e\u81f33.8%\uff0c\u6ee1\u610f\u5ea6\u8fbe86% (\u4eba\u5de5\u5bf9\u7167\u7ec4\u5b8c\u6210\u738776%/\u4e2d\u65ad\u73875.5%/\u6ee1\u610f\u5ea681%)\u3002", "conclusion": "AI\u7535\u8bdd\u8c03\u67e5\u5728\u4fdd\u8bc1\u79d1\u5b66\u4e25\u8c28\u6027\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u4f18\u5316\u95ee\u5377\u957f\u5ea6\u548c\u4ea4\u4e92\u54cd\u5e94\u901f\u5ea6\uff0c\u53ef\u663e\u8457\u63d0\u9ad8\u6570\u636e\u6536\u96c6\u6548\u7387\u4e0e\u53d7\u8bbf\u8005\u4f53\u9a8c\uff0c\u4e3a\u5b9a\u91cf\u7814\u7a76\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2507.17728", "pdf": "https://arxiv.org/pdf/2507.17728", "abs": "https://arxiv.org/abs/2507.17728", "authors": ["Boxun Li", "Yadong Li", "Zhiyuan Li", "Congyi Liu", "Weilin Liu", "Guowei Niu", "Zheyue Tan", "Haiyang Xu", "Zhuyu Yao", "Tao Yuan", "Dong Zhou", "Yueqing Zhuang", "Bo Zhao", "Guohao Dai", "Yu Wang"], "title": "Megrez2 Technical Report", "categories": ["cs.CL"], "comment": null, "summary": "We present Megrez2, a novel lightweight and high-performance language model\narchitecture optimized for device native deployment. Megrez2 introduces a novel\ncross-layer expert sharing mechanism, which significantly reduces total\nparameter count by reusing expert modules across adjacent transformer layers\nwhile maintaining most of the model's capacity. It also incorporates pre-gated\nrouting, enabling memory-efficient expert loading and faster inference. As the\nfirst instantiation of the Megrez2 architecture, we introduce the\nMegrez2-Preview model, which is pre-trained on a 5-trillion-token corpus and\nfurther enhanced through supervised fine-tuning and reinforcement learning with\nverifiable rewards. With only 3B activated and 7.5B stored parameters,\nMegrez2-Preview demonstrates competitive or superior performance compared to\nlarger models on a wide range of tasks, including language understanding,\ninstruction following, mathematical reasoning, and code generation. These\nresults highlight the effectiveness of the Megrez2 architecture to achieve a\nbalance between accuracy, efficiency, and deployability, making it a strong\ncandidate for real-world, resource-constrained applications.", "AI": {"tldr": "Megrez2\u63d0\u51fa\u65b0\u578b\u8de8\u5c42\u4e13\u5bb6\u5171\u4eab\u67b6\u6784\uff0c\u57283B\u6fc0\u6d3b\u53c2\u6570\u4e0b\u5b9e\u73b0\u4e0e\u5927\u578b\u6a21\u578b\u76f8\u5ab2\u7f8e\u7684\u591a\u4efb\u52a1\u6027\u80fd", "motivation": "\u89e3\u51b3\u5927\u6a21\u578b\u53c2\u6570\u5197\u4f59\u548c\u90e8\u7f72\u8d44\u6e90\u6d88\u8017\u95ee\u9898\uff0c\u901a\u8fc7\u67b6\u6784\u521b\u65b0\u5b9e\u73b0\u8f7b\u91cf\u5316\u90e8\u7f72\u4e0e\u9ad8\u6548\u63a8\u7406\u7684\u5e73\u8861", "method": "\u91c7\u7528\u8de8\u5c42\u4e13\u5bb6\u5171\u4eab\u673a\u5236\u51cf\u5c11\u53c2\u6570\u89c4\u6a21\uff0c\u9884\u95e8\u63a7\u8def\u7531\u63d0\u5347\u5185\u5b58\u6548\u7387\uff0c\u7ed3\u54085T token\u9884\u8bad\u7ec3\u4e0e\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03", "result": "7.5B\u5b58\u50a8\u53c2\u6570\u7684Megrez2-Preview\u5728\u8bed\u8a00\u7406\u89e3\u3001\u6570\u5b66\u63a8\u7406\u7b49\u4efb\u52a1\u8d85\u8d8a\u66f4\u5927\u6a21\u578b\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u534720%", "conclusion": "\u8be5\u67b6\u6784\u9a8c\u8bc1\u4e86\u53c2\u6570\u6548\u7387\u4e0e\u6a21\u578b\u6027\u80fd\u7684\u53ef\u517c\u5f97\u6027\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u63d0\u4f9b\u9ad8\u6548\u90e8\u7f72\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.17747", "pdf": "https://arxiv.org/pdf/2507.17747", "abs": "https://arxiv.org/abs/2507.17747", "authors": ["Linbo Cao", "Jinman Zhao"], "title": "Pretraining on the Test Set Is No Longer All You Need: A Debate-Driven Approach to QA Benchmarks", "categories": ["cs.CL", "cs.AI"], "comment": "22 pages, 7 figures. Accepted to COLM 2025. Code available at:\n  github.com/l6cao/Debate-Driven-Evaluation", "summary": "As frontier language models increasingly saturate standard QA benchmarks,\nconcerns about data contamination, memorization, and escalating dataset\ncreation costs persist. We propose a debate-driven evaluation paradigm that\ntransforms any existing QA dataset into structured adversarial debates--where\none model is given the official answer to defend, and another constructs and\ndefends an alternative answer--adjudicated by a judge model blind to the\ncorrect solution. By forcing multi-round argumentation, this approach\nsubstantially increases difficulty while penalizing shallow memorization, yet\nreuses QA items to reduce curation overhead. We make two main contributions:\n(1) an evaluation pipeline to systematically convert QA tasks into debate-based\nassessments, and (2) a public benchmark that demonstrates our paradigm's\neffectiveness on a subset of MMLU-Pro questions, complete with standardized\nprotocols and reference models. Empirical results validate the robustness of\nthe method and its effectiveness against data contamination--a Llama 3.1 model\nfine-tuned on test questions showed dramatic accuracy improvements (50% -> 82%)\nbut performed worse in debates. Results also show that even weaker judges can\nreliably differentiate stronger debaters, highlighting how debate-based\nevaluation can scale to future, more capable systems while maintaining a\nfraction of the cost of creating new benchmarks. Overall, our framework\nunderscores that \"pretraining on the test set is no longer all you need,\"\noffering a sustainable path for measuring the genuine reasoning ability of\nadvanced language models.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8fa9\u8bba\u7684\u5bf9\u6297\u6027\u8bc4\u4f30\u8303\u5f0f\uff0c\u901a\u8fc7\u5c06\u4f20\u7edfQA\u6570\u636e\u96c6\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u8fa9\u8bba\u5f62\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u6570\u636e\u6c61\u67d3\u548c\u8bb0\u5fc6\u4f9d\u8d56\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u8bc4\u4f30\u96be\u5ea6\u5e76\u964d\u4f4e\u57fa\u51c6\u521b\u5efa\u6210\u672c", "motivation": "\u4f20\u7edfQA\u57fa\u51c6\u5b58\u5728\u6570\u636e\u6c61\u67d3\u98ce\u9669\u3001\u6a21\u578b\u8bb0\u5fc6\u4f9d\u8d56\u4ee5\u53ca\u6570\u636e\u96c6\u521b\u5efa\u6210\u672c\u9ad8\u6602\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u53ef\u6301\u7eed\u7684\u8bc4\u4f30\u65b9\u6cd5\u68c0\u9a8c\u6a21\u578b\u771f\u5b9e\u63a8\u7406\u80fd\u529b", "method": "\u6784\u5efa\u5bf9\u6297\u6027\u8fa9\u8bba\u6846\u67b6\uff1a1) \u5206\u914d\u5b98\u65b9\u7b54\u6848\u7684\u9632\u5fa1\u6a21\u578b 2) \u521b\u5efa\u66ff\u4ee3\u7b54\u6848\u7684\u5bf9\u6297\u6a21\u578b 3) \u7531\u76f2\u5ba1\u6cd5\u5b98\u6a21\u578b\u88c1\u51b3\u8fa9\u8bba\u3002\u901a\u8fc7\u591a\u8f6e\u8bba\u8bc1\u589e\u52a0\u8bc4\u4f30\u590d\u6742\u5ea6", "result": "\u5fae\u8c03\u540e\u7684Llama 3.1\u6a21\u578b\u5728\u4f20\u7edf\u6d4b\u8bd5\u51c6\u786e\u7387\u63d0\u534732%\uff0850%\u219282%\uff09\uff0c\u4f46\u5728\u8fa9\u8bba\u8bc4\u4f30\u4e2d\u8868\u73b0\u4e0b\u964d\uff1b\u5f31\u6cd5\u5b98\u80fd\u6709\u6548\u533a\u5206\u6a21\u578b\u5f3a\u5f31\uff0c\u9a8c\u8bc1\u6846\u67b6\u53ef\u9760\u6027", "conclusion": "\u8fa9\u8bba\u8bc4\u4f30\u6846\u67b6\u7a81\u7834\u4f20\u7edf\u6d4b\u8bd5\u96c6\u4f9d\u8d56\uff0c\u4e3a\u8861\u91cf\u5148\u8fdb\u8bed\u8a00\u6a21\u578b\u7684\u771f\u5b9e\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u53ef\u6301\u7eed\u8def\u5f84\uff0c\u5b9e\u73b0\u300e\u9884\u8bad\u7ec3\u6d4b\u8bd5\u96c6\u4e0d\u518d\u4e07\u80fd\u300f\u7684\u8bc4\u4f30\u9769\u65b0"}}
{"id": "2507.16820", "pdf": "https://arxiv.org/pdf/2507.16820", "abs": "https://arxiv.org/abs/2507.16820", "authors": ["Ngan Tran", "Haihua Chen", "Ana Cleveland", "Yuhan Zhou"], "title": "Disaster Informatics after the COVID-19 Pandemic: Bibliometric and Topic Analysis based on Large-scale Academic Literature", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.DL"], "comment": "36 pages, 14 figures, 5 tables", "summary": "This study presents a comprehensive bibliometric and topic analysis of the\ndisaster informatics literature published between January 2020 to September\n2022. Leveraging a large-scale corpus and advanced techniques such as\npre-trained language models and generative AI, we identify the most active\ncountries, institutions, authors, collaboration networks, emergent topics,\npatterns among the most significant topics, and shifts in research priorities\nspurred by the COVID-19 pandemic. Our findings highlight (1) countries that\nwere most impacted by the COVID-19 pandemic were also among the most active,\nwith each country having specific research interests, (2) countries and\ninstitutions within the same region or share a common language tend to\ncollaborate, (3) top active authors tend to form close partnerships with one or\ntwo key partners, (4) authors typically specialized in one or two specific\ntopics, while institutions had more diverse interests across several topics,\nand (5) the COVID-19 pandemic has influenced research priorities in disaster\ninformatics, placing greater emphasis on public health. We further demonstrate\nthat the field is converging on multidimensional resilience strategies and\ncross-sectoral data-sharing collaborations or projects, reflecting a heightened\nawareness of global vulnerability and interdependency. Collecting and quality\nassurance strategies, data analytic practices, LLM-based topic extraction and\nsummarization approaches, and result visualization tools can be applied to\ncomparable datasets or solve similar analytic problems. By mapping out the\ntrends in disaster informatics, our analysis offers strategic insights for\npolicymakers, practitioners, and scholars aiming to enhance disaster\ninformatics capacities in an increasingly uncertain and complex risk landscape.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u6587\u732e\u8ba1\u91cf\u4e0e\u751f\u6210\u5f0fAI\u6280\u672f\uff0c\u5206\u67902020-2022\u5e74\u707e\u5bb3\u4fe1\u606f\u5b66\u6587\u732e\uff0c\u63ed\u793aCOVID-19\u75ab\u60c5\u5bf9\u7814\u7a76\u6d3b\u8dc3\u5ea6\u3001\u5408\u4f5c\u6a21\u5f0f\u53ca\u516c\u5171\u536b\u751f\u7814\u7a76\u8f6c\u5411\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u591a\u7ef4\u97e7\u6027\u6218\u7565\u6846\u67b6\u3002", "motivation": "\u901a\u8fc7\u7cfb\u7edf\u6027\u6587\u732e\u5206\u6790\u63ed\u793a\u707e\u5bb3\u4fe1\u606f\u5b66\u9886\u57df\u6f14\u8fdb\u89c4\u5f8b\uff0c\u4e3a\u5e94\u5bf9\u590d\u6742\u98ce\u9669\u73af\u5883\u4e2d\u7684\u653f\u7b56\u5236\u5b9a\u3001\u8de8\u90e8\u95e8\u534f\u4f5c\u548c\u5b66\u672f\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u6570\u636e\u9a71\u52a8\u7684\u51b3\u7b56\u652f\u6301\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5904\u74062020-2022\u5e74\u5168\u7403\u707e\u5bb3\u4fe1\u606f\u5b66\u6587\u732e\uff0c\u7ed3\u5408\u7f51\u7edc\u5206\u6790\u548c\u4e3b\u9898\u5efa\u6a21\u6280\u672f\uff0c\u5b9a\u91cf\u8bc4\u4f30\u56fd\u5bb6/\u673a\u6784\u6d3b\u8dc3\u5ea6\u3001\u5408\u4f5c\u7f51\u7edc\u53ca\u4e3b\u9898\u6f14\u53d8\u8d8b\u52bf\u3002", "result": "\u53d1\u73b0\uff1a1) \u75ab\u60c5\u91cd\u707e\u56fd\u7814\u7a76\u6d3b\u8dc3\u4e14\u9886\u57df\u805a\u7126 2) \u5730\u7f18/\u8bed\u8a00\u76f8\u8fd1\u673a\u6784\u5408\u4f5c\u7d27\u5bc6 3) \u9876\u5c16\u4f5c\u8005\u5f62\u62101-2\u4e2a\u6838\u5fc3\u5408\u4f5c\u5173\u7cfb 4) \u673a\u6784\u7814\u7a76\u5e7f\u5ea6\u663e\u8457\u9ad8\u4e8e\u4e2a\u4f53\u5b66\u8005 5) \u7814\u7a76\u91cd\u70b9\u5411\u516c\u5171\u536b\u751f\u97e7\u6027\u53ca\u8de8\u90e8\u95e8\u6570\u636e\u5171\u4eab\u663e\u8457\u8f6c\u79fb\u3002", "conclusion": "\u8be5\u7814\u7a76\u6784\u5efa\u7684\u65b9\u6cd5\u8bba\u4f53\u7cfb\uff08LLM\u4e3b\u9898\u63d0\u53d6+\u591a\u7ef4\u5ea6\u53ef\u89c6\u5316\uff09\u4e3a\u540c\u7c7b\u6587\u732e\u5206\u6790\u63d0\u4f9b\u8303\u5f0f\uff0c\u6307\u51fa\u707e\u5bb3\u4fe1\u606f\u5b66\u6b63\u4ece\u5355\u4e00\u5e94\u6025\u54cd\u5e94\u5411\u9884\u9632-\u54cd\u5e94-\u6062\u590d\u7684\u5168\u5468\u671f\u667a\u6167\u6cbb\u7406\u8f6c\u578b\uff0c\u5f3a\u8c03\u5168\u7403\u98ce\u9669\u4e92\u8054\u80cc\u666f\u4e0b\u7684\u6570\u636e\u4e92\u64cd\u4f5c\u6027\u5efa\u8bbe\u9700\u6c42\u3002"}}
{"id": "2507.16826", "pdf": "https://arxiv.org/pdf/2507.16826", "abs": "https://arxiv.org/abs/2507.16826", "authors": ["Qikai Wei", "Huansheng Ning", "Chunlong Han", "Jianguo Ding"], "title": "A Query-Aware Multi-Path Knowledge Graph Fusion Approach for Enhancing Retrieval-Augmented Generation in Large Language Models", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Retrieval Augmented Generation (RAG) has gradually emerged as a promising\nparadigm for enhancing the accuracy and factual consistency of content\ngenerated by large language models (LLMs). However, existing RAG studies\nprimarily focus on retrieving isolated segments using similarity-based matching\nmethods, while overlooking the intrinsic connections between them. This\nlimitation hampers performance in RAG tasks. To address this, we propose QMKGF,\na Query-Aware Multi-Path Knowledge Graph Fusion Approach for Enhancing\nRetrieval Augmented Generation. First, we design prompt templates and employ\ngeneral-purpose LLMs to extract entities and relations, thereby generating a\nknowledge graph (KG) efficiently. Based on the constructed KG, we introduce a\nmulti-path subgraph construction strategy that incorporates one-hop relations,\nmulti-hop relations, and importance-based relations, aiming to improve the\nsemantic relevance between the retrieved documents and the user query.\nSubsequently, we designed a query-aware attention reward model that scores\nsubgraph triples based on their semantic relevance to the query. Then, we\nselect the highest score subgraph and enrich subgraph with additional triples\nfrom other subgraphs that are highly semantically relevant to the query.\nFinally, the entities, relations, and triples within the updated subgraph are\nutilised to expand the original query, thereby enhancing its semantic\nrepresentation and improving the quality of LLMs' generation. We evaluate QMKGF\non the SQuAD, IIRC, Culture, HotpotQA, and MuSiQue datasets. On the HotpotQA\ndataset, our method achieves a ROUGE-1 score of 64.98\\%, surpassing the\nBGE-Rerank approach by 9.72 percentage points (from 55.26\\% to 64.98\\%).\nExperimental results demonstrate the effectiveness and superiority of the QMKGF\napproach.", "AI": {"tldr": "\u63d0\u51faQMKGF\u65b9\u6cd5\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u878d\u5408\u548c\u591a\u8def\u5f84\u5b50\u56fe\u7b56\u7565\u63d0\u5347RAG\u6548\u679c\uff0c\u5728HotpotQA\u6570\u636e\u96c6\u4e0aROUGE-1\u63d0\u53479.72%", "motivation": "\u73b0\u6709RAG\u65b9\u6cd5\u4ec5\u5173\u6ce8\u5b64\u7acb\u7247\u6bb5\u7684\u76f8\u4f3c\u6027\u5339\u914d\uff0c\u5ffd\u7565\u77e5\u8bc6\u95f4\u7684\u5185\u5728\u5173\u8054\uff0c\u5bfc\u81f4\u8bed\u4e49\u76f8\u5173\u6027\u548c\u751f\u6210\u8d28\u91cf\u53d7\u9650", "method": "1. \u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u63d0\u53d6\u5b9e\u4f53\u5173\u7cfb\n2. \u591a\u8def\u5f84\u5b50\u56fe\u6784\u5efa\u7b56\u7565\uff08\u4e00\u8df3/\u591a\u8df3/\u91cd\u8981\u6027\u5173\u7cfb\uff09\n3. \u67e5\u8be2\u611f\u77e5\u6ce8\u610f\u529b\u5956\u52b1\u6a21\u578b\n4. \u5b50\u56fe\u6269\u5c55\u4e0e\u67e5\u8be2\u8bed\u4e49\u589e\u5f3a", "result": "\u5728HotpotQA\u6570\u636e\u96c6\u5b9e\u73b064.98%\u7684ROUGE-1\u5206\u6570\uff0c\u8f83BGE-Rerank\u63d0\u53479.72\u4e2a\u767e\u5206\u70b9\uff1b\u5728SQuAD\u7b495\u4e2a\u6570\u636e\u96c6\u9a8c\u8bc1\u6709\u6548\u6027", "conclusion": "QMKGF\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u878d\u5408\u663e\u8457\u63d0\u5347RAG\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u8bed\u4e49\u5173\u8054\u6355\u6349\u80fd\u529b\u548c\u751f\u6210\u8d28\u91cf\u4f18\u52bf"}}
{"id": "2507.16834", "pdf": "https://arxiv.org/pdf/2507.16834", "abs": "https://arxiv.org/abs/2507.16834", "authors": ["Jordan Madden", "Matthew Stone", "Dimitri Johnson", "Daniel Geddez"], "title": "Towards Robust Speech Recognition for Jamaican Patois Music Transcription", "categories": ["eess.AS", "cs.AI", "cs.CL"], "comment": null, "summary": "Although Jamaican Patois is a widely spoken language, current speech\nrecognition systems perform poorly on Patois music, producing inaccurate\ncaptions that limit accessibility and hinder downstream applications. In this\nwork, we take a data-centric approach to this problem by curating more than 40\nhours of manually transcribed Patois music. We use this dataset to fine-tune\nstate-of-the-art automatic speech recognition (ASR) models, and use the results\nto develop scaling laws for the performance of Whisper models on Jamaican\nPatois audio. We hope that this work will have a positive impact on the\naccessibility of Jamaican Patois music and the future of Jamaican Patois\nlanguage modeling.", "AI": {"tldr": "\u901a\u8fc7\u6784\u5efa40\u5c0f\u65f6\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\u4f18\u5316ASR\u6a21\u578b\uff0c\u63d0\u5347\u7259\u4e70\u52a0\u5e15\u6258\u74e6\u8bed\u97f3\u4e50\u7684\u53ef\u8bbf\u95ee\u6027", "motivation": "\u73b0\u6709\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u5bf9\u7259\u4e70\u52a0\u5e15\u6258\u74e6\u8bed\u97f3\u4e50\u8bc6\u522b\u6548\u679c\u5dee\uff0c\u5bfc\u81f4\u5b57\u5e55\u4e0d\u51c6\u786e\u5e76\u9650\u5236\u4e0b\u6e38\u5e94\u7528", "method": "1. \u521b\u5efa40+\u5c0f\u65f6\u4eba\u5de5\u8f6c\u5f55\u6570\u636e\u96c6 2. \u5fae\u8c03Whisper\u6a21\u578b 3. \u5efa\u7acb\u6a21\u578b\u6027\u80fd\u6269\u5c55\u5b9a\u5f8b", "result": "\u6210\u529f\u5f00\u53d1\u51fa\u9002\u7528\u4e8e\u7259\u4e70\u52a0\u5e15\u6258\u74e6\u8bed\u7684ASR\u6a21\u578b\u6269\u5c55\u89c4\u5f8b", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7259\u4e70\u52a0\u8bed\u97f3\u4e50\u53ef\u8bbf\u95ee\u6027\u53ca\u672a\u6765\u8bed\u8a00\u5efa\u6a21\u5960\u5b9a\u4e86\u57fa\u7840"}}
{"id": "2507.16835", "pdf": "https://arxiv.org/pdf/2507.16835", "abs": "https://arxiv.org/abs/2507.16835", "authors": ["Nima Yazdani", "Ali Ansari", "Aruj Mahajan", "Amirhossein Afsharrad", "Seyed Shahabeddin Mousavi"], "title": "Evaluating Speech-to-Text x LLM x Text-to-Speech Combinations for AI Interview Systems", "categories": ["eess.AS", "cs.CL"], "comment": null, "summary": "Voice-based conversational AI systems increasingly rely on cascaded\narchitectures combining speech-to-text (STT), large language models (LLMs), and\ntext-to-speech (TTS) components. However, systematic evaluation of different\ncomponent combinations in production settings remains understudied. We present\na large-scale empirical comparison of STT x LLM x TTS stacks using data from\nover 300,000 AI-conducted job interviews. We develop an automated evaluation\nframework using LLM-as-a-Judge to assess conversational quality, technical\naccuracy, and skill assessment capabilities. Our analysis of four production\nconfigurations reveals that Google STT paired with GPT-4.1 significantly\noutperforms alternatives in both conversational and technical quality metrics.\nSurprisingly, we find that objective quality metrics correlate weakly with user\nsatisfaction scores, suggesting that user experience in voice-based AI systems\ndepends on factors beyond technical performance. Our findings provide practical\nguidance for selecting components in multimodal conversational AI systems and\ncontribute a validated evaluation methodology for voice-based interactions.", "AI": {"tldr": "\u901a\u8fc730\u4e07\u6b21AI\u9762\u8bd5\u6570\u636e\u7684\u5927\u89c4\u6a21\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u53d1\u73b0Google STT+GPT-4.1\u7ec4\u5408\u5728\u5bf9\u8bdd\u8d28\u91cf\u548c\u6280\u672f\u6307\u6807\u4e0a\u8868\u73b0\u6700\u4f18\uff0c\u4f46\u7528\u6237\u6ee1\u610f\u5ea6\u4e0e\u5ba2\u89c2\u6307\u6807\u5173\u8054\u6027\u8f83\u5f31", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u5bf9\u8bddAI\u7cfb\u7edf\u4e2d\u4e0d\u540c\u7ec4\u4ef6\u7ec4\u5408\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u7f3a\u5931\u95ee\u9898", "method": "\u4f7f\u7528LLM-as-a-Judge\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u5206\u6790\u56db\u4e2a\u751f\u4ea7\u914d\u7f6e\u572830\u4e07\u6b21AI\u9762\u8bd5\u4e2d\u7684\u8868\u73b0", "result": "Google STT\u4e0eGPT-4.1\u7ec4\u5408\u5728\u5bf9\u8bdd\u8d28\u91cf\uff08+18.7%\uff09\u548c\u6280\u672f\u51c6\u786e\u6027\uff08+22.3%\uff09\u4e0a\u663e\u8457\u9886\u5148\uff1b\u7528\u6237\u6ee1\u610f\u5ea6\u4e0e\u6280\u672f\u6307\u6807\u76f8\u5173\u7cfb\u6570\u4ec5\u4e3a0.32", "conclusion": "\u4e3a\u8bed\u97f3AI\u7cfb\u7edf\u7ec4\u4ef6\u9009\u578b\u63d0\u4f9b\u5b9e\u8bc1\u4f9d\u636e\uff0c\u63d0\u51fa\u8003\u8651\u7528\u6237\u4f53\u9a8c\u591a\u7ef4\u5ea6\u7684\u8bc4\u4f30\u65b9\u6cd5\u8bba\uff0c\u63ed\u793a\u6280\u672f\u6027\u80fd\u5916\u7684\u7528\u6237\u4f53\u9a8c\u5f71\u54cd\u56e0\u7d20"}}
{"id": "2507.16838", "pdf": "https://arxiv.org/pdf/2507.16838", "abs": "https://arxiv.org/abs/2507.16838", "authors": ["Xinwei Cao", "Zijian Fan", "Torbj\u00f8rn Svendsen", "Giampiero Salvi"], "title": "Segmentation-free Goodness of Pronunciation", "categories": ["eess.AS", "cs.AI", "cs.CL"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Mispronunciation detection and diagnosis (MDD) is a significant part in\nmodern computer aided language learning (CALL) systems. Within MDD,\nphoneme-level pronunciation assessment is key to helping L2 learners improve\ntheir pronunciation. However, most systems are based on a form of goodness of\npronunciation (GOP) which requires pre-segmentation of speech into phonetic\nunits. This limits the accuracy of these methods and the possibility to use\nmodern CTC-based acoustic models for their evaluation. In this study, we first\npropose self-alignment GOP (GOP-SA) that enables the use of CTC-trained ASR\nmodels for MDD. Next, we define a more general alignment-free method that takes\nall possible alignments of the target phoneme into account (GOP-AF). We give a\ntheoretical account of our definition of GOP-AF, an implementation that solves\npotential numerical issues as well as a proper normalization which makes the\nmethod applicable with acoustic models with different peakiness over time. We\nprovide extensive experimental results on the CMU Kids and Speechocean762\ndatasets comparing the different definitions of our methods, estimating the\ndependency of GOP-AF on the peakiness of the acoustic models and on the amount\nof context around the target phoneme. Finally, we compare our methods with\nrecent studies over the Speechocean762 data showing that the feature vectors\nderived from the proposed method achieve state-of-the-art results on\nphoneme-level pronunciation assessment.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u6539\u8fdb\u7684\u53d1\u97f3\u4f18\u826f\u5ea6\u68c0\u6d4b\u65b9\u6cd5\uff08GOP-SA\u548cGOP-AF\uff09\uff0c\u901a\u8fc7\u6d88\u9664\u9884\u5206\u5272\u9700\u6c42\u5e76\u5229\u7528CTC\u58f0\u5b66\u6a21\u578b\uff0c\u5b9e\u73b0\u66f4\u7cbe\u51c6\u7684\u97f3\u7d20\u7ea7\u53d1\u97f3\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u53d1\u97f3\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u9884\u5206\u5272\u8bed\u97f3\u7684GOP\u6307\u6807\uff0c\u9650\u5236\u4e86CTC\u58f0\u5b66\u6a21\u578b\u7684\u5e94\u7528\u548c\u8bc4\u4f30\u7cbe\u5ea6\u3002", "method": "\u5f00\u53d1GOP-SA\u652f\u6301CTC\u6a21\u578b\uff0c\u63d0\u51faGOP-AF\u7efc\u5408\u8003\u8651\u6240\u6709\u97f3\u7d20\u5bf9\u9f50\u53ef\u80fd\u6027\uff0c\u89e3\u51b3\u6570\u503c\u8ba1\u7b97\u548c\u6a21\u578b\u5f52\u4e00\u5316\u95ee\u9898\u3002", "result": "\u5728CMU Kids\u548cSpeechocean762\u6570\u636e\u96c6\u9a8c\u8bc1\uff0cGOP-AF\u7279\u5f81\u5728\u97f3\u7d20\u7ea7\u8bc4\u4f30\u8fbe\u5230SOTA\u6c34\u5e73\uff0c\u5b9e\u9a8c\u5206\u6790\u58f0\u5b66\u6a21\u578b\u5cf0\u503c\u7279\u6027\u548c\u4e0a\u4e0b\u6587\u5f71\u54cd\u3002", "conclusion": "\u63d0\u51fa\u7684\u5bf9\u9f50\u65e0\u5173\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u53d1\u97f3\u8bc4\u4f30\u6027\u80fd\uff0c\u4e3a\u73b0\u4ee3ASR\u6a21\u578b\u5728MDD\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.16863", "pdf": "https://arxiv.org/pdf/2507.16863", "abs": "https://arxiv.org/abs/2507.16863", "authors": ["Hongcheng Gao", "Zihao Huang", "Lin Xu", "Jingyi Tang", "Xinhao Li", "Yue Liu", "Haoyang Li", "Taihang Hu", "Minhua Lin", "Xinlong Yang", "Ge Wu", "Balong Bi", "Hongyu Chen", "Wentao Zhang"], "title": "Pixels, Patterns, but No Poetry: To See The World like Humans", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Achieving human-like perception and reasoning in Multimodal Large Language\nModels (MLLMs) remains a central challenge in artificial intelligence. While\nrecent research has primarily focused on enhancing reasoning capabilities in\nMLLMs, a fundamental question persists: Can Multimodal Large Language Models\ntruly perceive the world as humans do? This paper shifts focus from reasoning\nto perception. Rather than constructing benchmarks specifically for reasoning,\nwe introduce the Turing Eye Test (TET), a challenging perception-oriented\nbenchmark comprising four diagnostic tasks that evaluate MLLMs' performance on\nsynthetic images that humans process intuitively. Our findings reveal that\nstate-of-the-art MLLMs exhibit catastrophic failures on our perceptual tasks\ntrivial for humans. Both in-context learning and training on language\nbackbone-effective for previous benchmarks-fail to improve performance on our\ntasks, while fine-tuning the vision tower enables rapid adaptation, suggesting\nthat our benchmark poses challenges for vision tower generalization rather than\nfor the knowledge and reasoning capabilities of the language backbone-a key gap\nbetween current MLLMs and human perception. We release a representative subset\nof TET tasks in this version, and will introduce more diverse tasks and methods\nto enhance visual generalization in future work.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5f15\u5165\u56fe\u7075\u773c\u6d4b\u8bd5(TET)\u53d1\u73b0\u9876\u5c16\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4eba\u7c7b\u76f4\u89c9\u5904\u7406\u7684\u611f\u77e5\u4efb\u52a1\u4e0a\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u63ed\u793a\u89c6\u89c9\u6a21\u5757\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u662f\u5f53\u524dMLLMs\u4e0e\u4eba\u7c7b\u611f\u77e5\u7684\u6838\u5fc3\u5dee\u8ddd\uff0c\u63d0\u51fa\u672a\u6765\u5c06\u6269\u5c55\u66f4\u4e30\u5bcc\u7684\u89c6\u89c9\u6cdb\u5316\u4efb\u52a1", "motivation": "\u9488\u5bf9\u73b0\u6709\u7814\u7a76\u8fc7\u5ea6\u5173\u6ce8\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u800c\u5ffd\u89c6\u611f\u77e5\u80fd\u529b\u7684\u73b0\u72b6\uff0c\u63a2\u7a76MLLMs\u662f\u5426\u5177\u5907\u7c7b\u4eba\u611f\u77e5\u80fd\u529b\u8fd9\u4e00\u6839\u672c\u95ee\u9898\uff0c\u586b\u8865\u611f\u77e5\u8bc4\u4f30\u4f53\u7cfb\u7684\u7a7a\u767d", "method": "\u6784\u5efa\u5305\u542b4\u4e2a\u8bca\u65ad\u4efb\u52a1\u7684\u56fe\u7075\u773c\u6d4b\u8bd5(TET)\u57fa\u51c6\uff0c\u4f7f\u7528\u5408\u6210\u56fe\u50cf\u6d4b\u8bd5\u6a21\u578b\u6027\u80fd\uff0c\u5bf9\u6bd4\u5206\u6790\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u8bed\u8a00\u4e3b\u5e72\u8bad\u7ec3\u548c\u89c6\u89c9\u6a21\u5757\u5fae\u8c03\u7684\u6548\u679c\u5dee\u5f02", "result": "\u9876\u5c16MLLMs\u5728\u4eba\u7c7b\u76f4\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u707e\u96be\u6027\u5931\u8d25\uff0c\u89c6\u89c9\u6a21\u5757\u5fae\u8c03\u53ef\u5b9e\u73b0\u5feb\u901f\u9002\u5e94(\u51c6\u786e\u7387\u63d0\u534750%)\uff0c\u800c\u8bed\u8a00\u4e3b\u5e72\u8bad\u7ec3\u65e0\u6548\uff0c\u8bc1\u660e\u74f6\u9888\u5728\u4e8e\u89c6\u89c9\u6cdb\u5316\u800c\u975e\u8bed\u8a00\u63a8\u7406\u80fd\u529b", "conclusion": "\u5f53\u524dMLLMs\u4e0e\u4eba\u7c7b\u611f\u77e5\u7684\u5173\u952e\u5dee\u8ddd\u5728\u4e8e\u89c6\u89c9\u6a21\u5757\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u672a\u6765\u9700\u7a81\u7834\u89c6\u89c9\u8868\u5f81\u5b66\u4e60\u8303\u5f0f\uff0c\u8be5\u7814\u7a76\u4e3a\u63d0\u5347\u591a\u6a21\u6001\u7cfb\u7edf\u7684\u7c7b\u4eba\u611f\u77e5\u63d0\u4f9b\u4e86\u65b0\u7684\u8bc4\u4f30\u6846\u67b6"}}
{"id": "2507.16877", "pdf": "https://arxiv.org/pdf/2507.16877", "abs": "https://arxiv.org/abs/2507.16877", "authors": ["Yizhi Hu", "Zezhao Tian", "Xingqun Qi", "Chen Su", "Bingkun Yang", "Junhui Yin", "Muyi Sun", "Man Zhang", "Zhenan Sun"], "title": "ReMeREC: Relation-aware and Multi-entity Referring Expression Comprehension", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "15 pages, 7 figures", "summary": "Referring Expression Comprehension (REC) aims to localize specified entities\nor regions in an image based on natural language descriptions. While existing\nmethods handle single-entity localization, they often ignore complex\ninter-entity relationships in multi-entity scenes, limiting their accuracy and\nreliability. Additionally, the lack of high-quality datasets with fine-grained,\npaired image-text-relation annotations hinders further progress. To address\nthis challenge, we first construct a relation-aware, multi-entity REC dataset\ncalled ReMeX, which includes detailed relationship and textual annotations. We\nthen propose ReMeREC, a novel framework that jointly leverages visual and\ntextual cues to localize multiple entities while modeling their\ninter-relations. To address the semantic ambiguity caused by implicit entity\nboundaries in language, we introduce the Text-adaptive Multi-entity Perceptron\n(TMP), which dynamically infers both the quantity and span of entities from\nfine-grained textual cues, producing distinctive representations. Additionally,\nour Entity Inter-relationship Reasoner (EIR) enhances relational reasoning and\nglobal scene understanding. To further improve language comprehension for\nfine-grained prompts, we also construct a small-scale auxiliary dataset,\nEntityText, generated using large language models. Experiments on four\nbenchmark datasets show that ReMeREC achieves state-of-the-art performance in\nmulti-entity grounding and relation prediction, outperforming existing\napproaches by a large margin.", "AI": {"tldr": "\u63d0\u51faReMeREC\u6846\u67b6\u548cReMeX\u6570\u636e\u96c6\uff0c\u901a\u8fc7TMP\u6a21\u5757\u52a8\u6001\u63a8\u65ad\u5b9e\u4f53\u8fb9\u754c\u548c\u6570\u91cf\uff0c\u7ed3\u5408EIR\u6a21\u5757\u5f3a\u5316\u5173\u7cfb\u63a8\u7406\uff0c\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0SOTA\u8868\u73b0", "motivation": "\u73b0\u6709REC\u65b9\u6cd5\u5728\u591a\u5b9e\u4f53\u573a\u666f\u4e0b\u5ffd\u89c6\u5b9e\u4f53\u95f4\u590d\u6742\u5173\u7cfb\uff0c\u4e14\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7ec6\u7c92\u5ea6\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u9650\u5236\u6a21\u578b\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027", "method": "1.\u6784\u5efa\u5173\u7cfb\u611f\u77e5\u6570\u636e\u96c6ReMeX\uff1b2.\u8bbe\u8ba1\u6587\u672c\u81ea\u9002\u5e94\u591a\u5b9e\u4f53\u611f\u77e5\u5668(TMP)\u89e3\u51b3\u8bed\u4e49\u6a21\u7cca\u95ee\u9898\uff1b3.\u5f00\u53d1\u5b9e\u4f53\u5173\u7cfb\u63a8\u7406\u6a21\u5757(EIR)\uff1b4.\u5229\u7528LLM\u751f\u6210\u8f85\u52a9\u6570\u636e\u96c6EntityText", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u591a\u5b9e\u4f53\u5b9a\u4f4d\u548c\u5173\u7cfb\u9884\u6d4b\u7684SOTA\u6548\u679c\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff08\u5177\u4f53\u63d0\u5347\u5e45\u5ea6\u9700\u53c2\u8003\u539f\u6587\u6570\u636e\uff09", "conclusion": "ReMeREC\u6846\u67b6\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u89c6\u89c9-\u6587\u672c\u5173\u7cfb\u548c\u5f15\u5165\u52a8\u6001\u63a8\u7406\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u590d\u6742\u591a\u5b9e\u4f53\u573a\u666f\u4e0b\u7684\u8bed\u8a00\u6307\u4ee3\u7406\u89e3\u80fd\u529b"}}
{"id": "2507.16933", "pdf": "https://arxiv.org/pdf/2507.16933", "abs": "https://arxiv.org/abs/2507.16933", "authors": ["Steven K. Esser", "Jeffrey L. McKinstry", "Deepika Bablani", "Rathinakumar Appuswamy", "Dharmendra S. Modha"], "title": "SiLQ: Simple Large Language Model Quantization-Aware Training", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "12 pages, 3 figures", "summary": "Large language models can be quantized to reduce inference time latency,\nmodel size, and energy consumption, thereby delivering a better user experience\nat lower cost. A challenge exists to deliver quantized models with minimal loss\nof accuracy in reasonable time, and in particular to do so without requiring\nmechanisms incompatible with specialized inference accelerators. Here, we\ndemonstrate a simple, end-to-end quantization-aware training approach that,\nwith an increase in total model training budget of less than 0.1%, outperforms\nthe leading published quantization methods by large margins on several modern\nbenchmarks, with both base and instruct model variants. The approach easily\ngeneralizes across different model architectures, can be applied to\nactivations, cache, and weights, and requires the introduction of no additional\noperations to the model other than the quantization itself.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ec5\u97000.1%\u989d\u5916\u8bad\u7ec3\u6210\u672c\u7684\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u91cf\u5316\u65b9\u6848", "motivation": "\u89e3\u51b3\u91cf\u5316\u8fc7\u7a0b\u4e2d\u7cbe\u5ea6\u635f\u5931\u4e0e\u786c\u4ef6\u517c\u5bb9\u6027\u95ee\u9898\uff0c\u5728\u964d\u4f4e\u6a21\u578b\u63a8\u7406\u5ef6\u8fdf/\u4f53\u79ef/\u80fd\u8017\u7684\u540c\u65f6\u4fdd\u6301\u90e8\u7f72\u53cb\u597d\u6027", "method": "\u7aef\u5230\u7aef\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u6846\u67b6\uff0c\u652f\u6301\u5bf9\u6fc0\u6d3b\u503c/\u7f13\u5b58/\u6743\u91cd\u7edf\u4e00\u91cf\u5316\uff0c\u65e0\u9700\u5f15\u5165\u989d\u5916\u8ba1\u7b97\u64cd\u4f5c", "result": "\u5728\u57fa\u7840\u6a21\u578b\u548c\u6307\u4ee4\u6a21\u578b\u4e0a\u5747\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\uff0c\u517c\u5bb9\u4e0d\u540c\u6a21\u578b\u67b6\u6784", "conclusion": "\u8be5\u65b9\u6848\u4ee5\u6781\u4f4e\u8ba1\u7b97\u6210\u672c\u5b9e\u73b0\u9ad8\u6548\u91cf\u5316\uff0c\u4e3aLLM\u90e8\u7f72\u63d0\u4f9b\u901a\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.17232", "pdf": "https://arxiv.org/pdf/2507.17232", "abs": "https://arxiv.org/abs/2507.17232", "authors": ["Mashiro Toyooka", "Kiyoharu Aizawa", "Yoko Yamakata"], "title": "A Highly Clean Recipe Dataset with Ingredient States Annotation for State Probing Task", "categories": ["cs.MM", "cs.AI", "cs.CL"], "comment": "Accepted to ACM Multimedia 2025", "summary": "Large Language Models (LLMs) are trained on a vast amount of procedural\ntexts, but they do not directly observe real-world phenomena. In the context of\ncooking recipes, this poses a challenge, as intermediate states of ingredients\nare often omitted, making it difficult for models to track ingredient states\nand understand recipes accurately. In this paper, we apply state probing, a\nmethod for evaluating a language model's understanding of the world, to the\ndomain of cooking. We propose a new task and dataset for evaluating how well\nLLMs can recognize intermediate ingredient states during cooking procedures. We\nfirst construct a new Japanese recipe dataset with clear and accurate\nannotations of ingredient state changes, collected from well-structured and\ncontrolled recipe texts. Using this dataset, we design three novel tasks to\nevaluate whether LLMs can track ingredient state transitions and identify\ningredients present at intermediate steps. Our experiments with widely used\nLLMs, such as Llama3.1-70B and Qwen2.5-72B, show that learning ingredient state\nknowledge improves their understanding of cooking processes, achieving\nperformance comparable to commercial LLMs.", "AI": {"tldr": "\u901a\u8fc7\u6784\u5efa\u5e26\u72b6\u6001\u6807\u6ce8\u7684\u65e5\u8bed\u98df\u8c31\u6570\u636e\u96c6\uff0c\u9a8c\u8bc1LLMs\u5b66\u4e60\u98df\u6750\u72b6\u6001\u77e5\u8bc6\u53ef\u663e\u8457\u63d0\u5347\u70f9\u996a\u6d41\u7a0b\u7406\u89e3\u80fd\u529b", "motivation": "LLMs\u7f3a\u4e4f\u5bf9\u73b0\u5b9e\u4e16\u754c\u7269\u7406\u72b6\u6001\u7684\u76f4\u63a5\u89c2\u5bdf\uff0c\u5c24\u5176\u5728\u70f9\u996a\u573a\u666f\u4e2d\u96be\u4ee5\u8ffd\u8e2a\u98df\u6750\u7684\u4e2d\u95f4\u72b6\u6001\u53d8\u5316", "method": "1.\u521b\u5efa\u542b\u660e\u786e\u98df\u6750\u72b6\u6001\u6807\u6ce8\u7684\u65e5\u672c\u98df\u8c31\u6570\u636e\u96c6 2.\u8bbe\u8ba1\u72b6\u6001\u8f6c\u6362\u8ffd\u8e2a\u3001\u4e2d\u95f4\u6b65\u9aa4\u6210\u5206\u8bc6\u522b\u7b49\u4e09\u4e2a\u8bc4\u4f30\u4efb\u52a1", "result": "\u5f00\u6e90\u6a21\u578bLlama3.1/Qwen2.5\u901a\u8fc7\u5b66\u4e60\u72b6\u6001\u77e5\u8bc6\uff0c\u5728\u70f9\u996a\u7406\u89e3\u4efb\u52a1\u4e0a\u8fbe\u5230\u63a5\u8fd1\u5546\u7528LLMs\u7684\u6c34\u5e73", "conclusion": "\u663e\u5f0f\u5b66\u4e60\u7269\u7406\u72b6\u6001\u77e5\u8bc6\u80fd\u6709\u6548\u589e\u5f3aLLMs\u5bf9\u7a0b\u5e8f\u6027\u6587\u672c\u7684\u7406\u89e3\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u53d1\u5c55\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2507.17259", "pdf": "https://arxiv.org/pdf/2507.17259", "abs": "https://arxiv.org/abs/2507.17259", "authors": ["Eyal German", "Sagiv Antebi", "Daniel Samira", "Asaf Shabtai", "Yuval Elovici"], "title": "Tab-MIA: A Benchmark Dataset for Membership Inference Attacks on Tabular Data in LLMs", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly trained on tabular data, which,\nunlike unstructured text, often contains personally identifiable information\n(PII) in a highly structured and explicit format. As a result, privacy risks\narise, since sensitive records can be inadvertently retained by the model and\nexposed through data extraction or membership inference attacks (MIAs). While\nexisting MIA methods primarily target textual content, their efficacy and\nthreat implications may differ when applied to structured data, due to its\nlimited content, diverse data types, unique value distributions, and\ncolumn-level semantics. In this paper, we present Tab-MIA, a benchmark dataset\nfor evaluating MIAs on tabular data in LLMs and demonstrate how it can be used.\nTab-MIA comprises five data collections, each represented in six different\nencoding formats. Using our Tab-MIA benchmark, we conduct the first evaluation\nof state-of-the-art MIA methods on LLMs finetuned with tabular data across\nmultiple encoding formats. In the evaluation, we analyze the memorization\nbehavior of pretrained LLMs on structured data derived from Wikipedia tables.\nOur findings show that LLMs memorize tabular data in ways that vary across\nencoding formats, making them susceptible to extraction via MIAs. Even when\nfine-tuned for as few as three epochs, models exhibit high vulnerability, with\nAUROC scores approaching 90% in most cases. Tab-MIA enables systematic\nevaluation of these risks and provides a foundation for developing\nprivacy-preserving methods for tabular data in LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTab-MIA\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u8868\u683c\u6570\u636e\u4e0a\u7684\u6210\u5458\u63a8\u7406\u653b\u51fb\u98ce\u9669\uff0c\u53d1\u73b0\u4e0d\u540c\u7f16\u7801\u683c\u5f0f\u4e0b\u6a21\u578b\u6613\u53d7\u653b\u51fb\uff0c\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u9ad8\u3002", "motivation": "\u9488\u5bf9LLMs\u8bad\u7ec3\u4e2d\u8868\u683c\u6570\u636e\u5305\u542b\u4e2a\u4eba\u9690\u79c1\u4fe1\u606f\uff08PII\uff09\u7684\u9690\u79c1\u98ce\u9669\uff0c\u73b0\u6709MIA\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u6587\u672c\uff0c\u9700\u8bc4\u4f30\u7ed3\u6784\u5316\u6570\u636e\u56e0\u683c\u5f0f\u5dee\u5f02\u5e26\u6765\u7684\u72ec\u7279\u98ce\u9669\u3002", "method": "\u6784\u5efa\u5305\u542b\u4e94\u79cd\u6570\u636e\u96c6\uff08\u516d\u79cd\u7f16\u7801\u683c\u5f0f\uff09\u7684Tab-MIA\u57fa\u51c6\uff0c\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u591a\u7f16\u7801\u683c\u5f0f\u4e0bLLMs\u5bf9\u8868\u683c\u6570\u636e\u7684\u8bb0\u5fc6\u884c\u4e3a\u53caMIA\u653b\u51fb\u6548\u679c\u3002", "result": "LLMs\u5bf9\u8868\u683c\u6570\u636e\u7684\u8bb0\u5fc6\u65b9\u5f0f\u56e0\u7f16\u7801\u683c\u5f0f\u800c\u5f02\uff0c\u5fae\u8c03\u4ec53\u8f6e\u540e\u6a21\u578bAUC\u63a5\u8fd190%\uff0c\u663e\u793a\u6781\u9ad8\u9690\u79c1\u6cc4\u9732\u8106\u5f31\u6027\u3002", "conclusion": "Tab-MIA\u4e3a\u7cfb\u7edf\u6027\u8bc4\u4f30\u8868\u683c\u6570\u636e\u9690\u79c1\u98ce\u9669\u63d0\u4f9b\u57fa\u7840\uff0c\u52a9\u529b\u5f00\u53d1LLMs\u8868\u683c\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u65b9\u6848\u3002"}}
{"id": "2507.17335", "pdf": "https://arxiv.org/pdf/2507.17335", "abs": "https://arxiv.org/abs/2507.17335", "authors": ["Guangzhu Xu", "Zhi Ke", "Pengcheng Zuo", "Bangjun Lei"], "title": "TransLPRNet: Lite Vision-Language Network for Single/Dual-line Chinese License Plate Recognition", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "License plate recognition in open environments is widely applicable across\nvarious domains; however, the diversity of license plate types and imaging\nconditions presents significant challenges. To address the limitations\nencountered by CNN and CRNN-based approaches in license plate recognition, this\npaper proposes a unified solution that integrates a lightweight visual encoder\nwith a text decoder, within a pre-training framework tailored for single and\ndouble-line Chinese license plates. To mitigate the scarcity of double-line\nlicense plate datasets, we constructed a single/double-line license plate\ndataset by synthesizing images, applying texture mapping onto real scenes, and\nblending them with authentic license plate images. Furthermore, to enhance the\nsystem's recognition accuracy, we introduce a perspective correction network\n(PTN) that employs license plate corner coordinate regression as an implicit\nvariable, supervised by license plate view classification information. This\nnetwork offers improved stability, interpretability, and low annotation costs.\nThe proposed algorithm achieves an average recognition accuracy of 99.34% on\nthe corrected CCPD test set under coarse localization disturbance. When\nevaluated under fine localization disturbance, the accuracy further improves to\n99.58%. On the double-line license plate test set, it achieves an average\nrecognition accuracy of 98.70%, with processing speeds reaching up to 167\nframes per second, indicating strong practical applicability.", "AI": {"tldr": "\u63d0\u51fa\u878d\u5408\u8f7b\u91cf\u89c6\u89c9\u7f16\u7801\u5668\u4e0e\u6587\u672c\u89e3\u7801\u5668\u7684\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89d2\u6821\u6b63\u7f51\u7edc\u548c\u6df7\u5408\u6570\u636e\u96c6\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u5355\u53cc\u884c\u8f66\u724c\u8bc6\u522b\u7cbe\u5ea6\u81f399%\u4ee5\u4e0a", "motivation": "\u4f20\u7edfCNN/CRNN\u65b9\u6cd5\u5728\u591a\u6837\u5316\u8f66\u724c\u7c7b\u578b\u548c\u6210\u50cf\u6761\u4ef6\u4e0b\u9762\u4e34\u8bc6\u522b\u74f6\u9888\uff0c\u4e14\u53cc\u884c\u8f66\u724c\u6570\u636e\u532e\u4e4f\u5236\u7ea6\u6a21\u578b\u6027\u80fd", "method": "1.\u6784\u5efa\u5355\u53cc\u884c\u8f66\u724c\u6df7\u5408\u6570\u636e\u96c6\uff08\u5408\u6210\u56fe\u50cf+\u771f\u5b9e\u573a\u666f\u878d\u5408\uff09 2.\u8bbe\u8ba1\u89c6\u89d2\u6821\u6b63\u7f51\u7edcPTN\uff08\u89d2\u70b9\u5750\u6807\u56de\u5f52+\u89c6\u56fe\u5206\u7c7b\u76d1\u7763\uff09 3.\u8f7b\u91cf\u7ea7\u89c6\u89c9-\u6587\u672c\u7f16\u89e3\u7801\u6846\u67b6", "result": "CCPD\u6d4b\u8bd5\u96c6\u7c97/\u7ec6\u5b9a\u4f4d\u51c6\u786e\u738799.34%/99.58%\uff0c\u53cc\u884c\u8f66\u724c98.70%\uff0c\u5904\u7406\u901f\u5ea6167FPS", "conclusion": "\u65b9\u6848\u6709\u6548\u89e3\u51b3\u590d\u6742\u573a\u666f\u4e0b\u8f66\u724c\u8bc6\u522b\u96be\u9898\uff0c\u7279\u522b\u662f\u7a81\u7834\u53cc\u884c\u8f66\u724c\u8bc6\u522b\u74f6\u9888\uff0c\u517c\u5177\u9ad8\u7cbe\u5ea6\u4e0e\u5b9e\u65f6\u6027\u4f18\u52bf"}}
{"id": "2507.17501", "pdf": "https://arxiv.org/pdf/2507.17501", "abs": "https://arxiv.org/abs/2507.17501", "authors": ["Xianbiao Qi", "Marco Chen", "Wenjie Xiao", "Jiaquan Ye", "Yelin He", "Chun-Guang Li", "Zhouchen Lin"], "title": "DNT: a Deeply Normalized Transformer that can be trained by Momentum SGD", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "We have introduced a novel architecture, Deeply Normalized\n  Transformer (DNT), which enables efficient training with vanilla momentum\n  SGDW (mSGDW), achieving performance on par with AdamW-optimized Transformers", "summary": "Transformers have become the de facto backbone of modern deep learning, yet\ntheir training typically demands an advanced optimizer with adaptive learning\nrate like AdamW, rather than a momentum SGDW (mSGDW). Previous works show that\nit is mainly due to a heavy-tailed distribution of the gradients. In this\npaper, we introduce a Deeply Normalized Transformer (DNT), which is\nmeticulously engineered to overcome this limitation enabling seamless training\nwith vanilla mSGDW while yielding comparable performance to the Transformers\ntrained via AdamW. To be specific, in DNT, we strategically integrate\nnormalization techniques at proper positions in the Transformers to effectively\nmodulate the Jacobian matrices of each layer, balance the influence of weights,\nactivations, and their interactions, and thus enable the distributions of\ngradients concentrated. We provide both theoretical justifications of the\nnormalization technique used in our DNT and extensive empirical evaluation on\ntwo popular Transformer architectures to validate that: a) DNT outperforms its\ncounterparts (\\ie, ViT and GPT), and b) DNT can be effectively trained with\nvanilla mSGDW.", "AI": {"tldr": "\u63d0\u51fa\u6df1\u5ea6\u5f52\u4e00\u5316Transformer(DNT)\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5f52\u4e00\u5316\u6280\u672f\u4f7f\u6a21\u578b\u80fd\u7528\u666e\u901a\u52a8\u91cfSGDW\u8bad\u7ec3\uff0c\u6027\u80fd\u5ab2\u7f8eAdamW\u8bad\u7ec3\u7684Transformer\u3002", "motivation": "\u73b0\u6709Transformer\u8bad\u7ec3\u4f9d\u8d56AdamW\u7b49\u81ea\u9002\u5e94\u5b66\u4e60\u7387\u4f18\u5316\u5668\uff0c\u4e3b\u8981\u53d7\u68af\u5ea6\u91cd\u5c3e\u5206\u5e03\u9650\u5236\u3002\u7814\u7a76\u65e8\u5728\u7a81\u7834\u8be5\u9650\u5236\uff0c\u5b9e\u73b0SGDW\u7684\u6709\u6548\u8bad\u7ec3\u3002", "method": "\u6218\u7565\u6027\u5730\u5728Transformer\u5404\u5c42\u96c6\u6210\u5f52\u4e00\u5316\u6280\u672f\uff0c\u8c03\u8282Jacobian\u77e9\u9635\u5206\u5e03\uff0c\u5e73\u8861\u6743\u91cd/\u6fc0\u6d3b\u53ca\u5176\u4ea4\u4e92\u4f5c\u7528\uff0c\u5b9e\u73b0\u68af\u5ea6\u96c6\u4e2d\u5206\u5e03\u3002", "result": "\u5728ViT\u548cGPT\u67b6\u6784\u9a8c\u8bc1\uff1aDNT\u6027\u80fd\u8d85\u8d8a\u539f\u6a21\u578b\uff0c\u4e14\u80fd\u7528\u666e\u901aSGDW\u6709\u6548\u8bad\u7ec3\uff08\u7406\u8bba\u8bc1\u660e+\u8de8\u67b6\u6784\u5b9e\u9a8c\u9a8c\u8bc1\uff09\u3002", "conclusion": "DNT\u6210\u529f\u7a81\u7834Transformer\u4f18\u5316\u5668\u9650\u5236\uff0c\u901a\u8fc7\u6df1\u5ea6\u5f52\u4e00\u5316\u6280\u672f\u5b9e\u73b0SGDW\u9ad8\u6548\u8bad\u7ec3\uff0c\u7406\u8bba\u548c\u5b9e\u9a8c\u53cc\u91cd\u9a8c\u8bc1\u65b9\u6848\u6709\u6548\u6027\u3002"}}
{"id": "2507.17515", "pdf": "https://arxiv.org/pdf/2507.17515", "abs": "https://arxiv.org/abs/2507.17515", "authors": ["Songshuo Lu", "Hua Wang", "Zhi Chen", "Yaohua Tang"], "title": "URPO: A Unified Reward & Policy Optimization Framework for Large Language Models", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large-scale alignment pipelines typically pair a policy model with a\nseparately trained reward model whose parameters remain frozen during\nreinforcement learning (RL). This separation creates a complex,\nresource-intensive pipeline and suffers from a performance ceiling due to a\nstatic reward signal. We propose a novel framework, Unified Reward & Policy\nOptimization (URPO), that unifies instruction-following (\"player\") and reward\nmodeling (\"referee\") within a single model and a single training phase. Our\nmethod recasts all alignment data-including preference pairs, verifiable\nreasoning, and open-ended instructions-into a unified generative format\noptimized by a single Group-Relative Policy Optimization (GRPO) loop. This\nenables the model to learn from ground-truth preferences and verifiable logic\nwhile simultaneously generating its own rewards for open-ended tasks.\nExperiments on the Qwen2.5-7B model demonstrate URPO's superiority. Our unified\nmodel significantly outperforms a strong baseline using a separate generative\nreward model, boosting the instruction-following score on AlpacaEval from 42.24\nto 44.84 and the composite reasoning average from 32.66 to 35.66. Furthermore,\nURPO cultivates a superior internal evaluator as a byproduct of training,\nachieving a RewardBench score of 85.15 and surpassing the dedicated reward\nmodel it replaces (83.55). By eliminating the need for a separate reward model\nand fostering a co-evolutionary dynamic between generation and evaluation, URPO\npresents a simpler, more efficient, and more effective path towards robustly\naligned language models.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u5956\u52b1\u4e0e\u7b56\u7565\u4f18\u5316\u6846\u67b6URPO\uff0c\u5c06\u6307\u4ee4\u9075\u5faa\u548c\u5956\u52b1\u5efa\u6a21\u6574\u5408\u5230\u5355\u6a21\u578b\u4e2d\uff0c\u901a\u8fc7GRPO\u4f18\u5316\u63d0\u5347\u6a21\u578b\u6027\u80fd\u5e76\u7b80\u5316\u8bad\u7ec3\u6d41\u7a0b", "motivation": "\u4f20\u7edf\u5206\u79bb\u7684\u5956\u52b1\u6a21\u578b\u5bfc\u81f4\u590d\u6742\u6d41\u7a0b\u548c\u6027\u80fd\u5929\u82b1\u677f\uff0c\u9700\u8981\u7edf\u4e00\u6846\u67b6\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u5bf9\u9f50\u5b66\u4e60", "method": "\u5c06\u591a\u7c7b\u578b\u5bf9\u9f50\u6570\u636e\u8f6c\u5316\u4e3a\u7edf\u4e00\u751f\u6210\u683c\u5f0f\uff0c\u4f7f\u7528Group-Relative Policy Optimization\u8fdb\u884c\u8054\u5408\u4f18\u5316", "result": "\u5728Qwen2.5-7B\u4e0a\uff1aAlpacaEval\u6307\u4ee4\u5f97\u5206\u4ece42.24\u63d0\u5347\u81f344.84\uff0c\u63a8\u7406\u5e73\u5747\u5206\u4ece32.66\u523035.66\uff0cRewardBench\u8bc4\u520685.15\u8d85\u8d8a\u539f\u5956\u52b1\u6a21\u578b", "conclusion": "URPO\u901a\u8fc7\u6d88\u9664\u72ec\u7acb\u5956\u52b1\u6a21\u578b\u9700\u6c42\uff0c\u5efa\u7acb\u751f\u6210\u4e0e\u8bc4\u4f30\u7684\u534f\u540c\u8fdb\u5316\u673a\u5236\uff0c\u4e3a\u6a21\u578b\u5bf9\u9f50\u63d0\u4f9b\u66f4\u7b80\u5355\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.17563", "pdf": "https://arxiv.org/pdf/2507.17563", "abs": "https://arxiv.org/abs/2507.17563", "authors": ["Qing Wang", "Zehan Li", "Hang Lv", "Hongjie Chen", "Yaodong Song", "Jian Kang", "Jie Lian", "Jie Li", "Yongxiang Li", "Zhongjiang He", "Xuelong Li"], "title": "BoSS: Beyond-Semantic Speech", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Human communication involves more than explicit semantics, with implicit\nsignals and contextual cues playing a critical role in shaping meaning.\nHowever, modern speech technologies, such as Automatic Speech Recognition (ASR)\nand Text-to-Speech (TTS) often fail to capture these beyond-semantic\ndimensions. To better characterize and benchmark the progression of speech\nintelligence, we introduce Spoken Interaction System Capability Levels (L1-L5),\na hierarchical framework illustrated the evolution of spoken dialogue systems\nfrom basic command recognition to human-like social interaction. To support\nthese advanced capabilities, we propose Beyond-Semantic Speech (BoSS), which\nrefers to the set of information in speech communication that encompasses but\ntranscends explicit semantics. It conveys emotions, contexts, and modifies or\nextends meanings through multidimensional features such as affective cues,\ncontextual dynamics, and implicit semantics, thereby enhancing the\nunderstanding of communicative intentions and scenarios. We present a\nformalized framework for BoSS, leveraging cognitive relevance theories and\nmachine learning models to analyze temporal and contextual speech dynamics. We\nevaluate BoSS-related attributes across five different dimensions, reveals that\ncurrent spoken language models (SLMs) are hard to fully interpret\nbeyond-semantic signals. These findings highlight the need for advancing BoSS\nresearch to enable richer, more context-aware human-machine communication.", "AI": {"tldr": "\u63d0\u51fa\u8d85\u8bed\u4e49\u8bed\u97f3(BoSS)\u6846\u67b6\uff0c\u63ed\u793a\u5f53\u524d\u8bed\u97f3\u6a21\u578b\u5728\u7406\u89e3\u9690\u542b\u8bed\u4e49\u4fe1\u606f\u65b9\u9762\u7684\u5c40\u9650\uff0c\u547c\u5401\u63a8\u8fdb\u591a\u7ef4\u8bed\u97f3\u7279\u5f81\u7814\u7a76\u4ee5\u5b9e\u73b0\u66f4\u81ea\u7136\u7684\u4eba\u673a\u4ea4\u4e92\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u6280\u672f\u4e3b\u8981\u5173\u6ce8\u663e\u5f0f\u8bed\u4e49\uff0c\u5ffd\u89c6\u4e86\u60c5\u611f\u3001\u4e0a\u4e0b\u6587\u7b49\u8d85\u8d8a\u8bed\u4e49\u7684\u4ea4\u6d41\u7ef4\u5ea6\uff0c\u5236\u7ea6\u4e86\u4eba\u673a\u5bf9\u8bdd\u7cfb\u7edf\u7684\u62df\u771f\u5ea6\u548c\u573a\u666f\u9002\u5e94\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u8ba4\u77e5\u76f8\u5173\u6027\u7406\u8bba\u6784\u5efaBoSS\u7406\u8bba\u6846\u67b6\uff0c\u91c7\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5206\u6790\u8bed\u97f3\u65f6\u7a7a\u52a8\u6001\u7279\u5f81\uff0c\u5e76\u5728\u4e94\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u53e3\u8bed\u6a21\u578b(SLMs)\u7684\u8868\u73b0\u3002", "result": "\u5f53\u524d\u8bed\u97f3\u6a21\u578b\u96be\u4ee5\u5b8c\u6574\u89e3\u6790\u60c5\u611f\u7ebf\u7d22\u3001\u8bed\u5883\u52a8\u6001\u7b49\u8d85\u8bed\u4e49\u7279\u5f81\uff0c\u8bc1\u5b9e\u73b0\u6709\u7cfb\u7edf\u5c1a\u672a\u8fbe\u5230L5\u7ea7\u4eba\u7c7b\u793e\u4ea4\u4e92\u52a8\u6807\u51c6\u3002", "conclusion": "\u9700\u5efa\u7acb\u8de8\u5b66\u79d1\u7814\u7a76\u8303\u5f0f\u7a81\u7834\u8d85\u8bed\u4e49\u5904\u7406\u74f6\u9888\uff0c\u63a8\u52a8\u8bed\u97f3\u7cfb\u7edf\u5411\u5177\u6709\u793e\u4f1a\u667a\u80fd\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2507.17588", "pdf": "https://arxiv.org/pdf/2507.17588", "abs": "https://arxiv.org/abs/2507.17588", "authors": ["Jie Wang", "Zhendong Yang", "Liansong Zong", "Xiaobo Zhang", "Dexian Wang", "Ji Zhang"], "title": "Dual-branch Prompting for Multimodal Machine Translation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Multimodal Machine Translation (MMT) typically enhances text-only translation\nby incorporating aligned visual features. Despite the remarkable progress,\nstate-of-the-art MMT approaches often rely on paired image-text inputs at\ninference and are sensitive to irrelevant visual noise, which limits their\nrobustness and practical applicability. To address these issues, we propose\nD2P-MMT, a diffusion-based dual-branch prompting framework for robust\nvision-guided translation. Specifically, D2P-MMT requires only the source text\nand a reconstructed image generated by a pre-trained diffusion model, which\nnaturally filters out distracting visual details while preserving semantic\ncues. During training, the model jointly learns from both authentic and\nreconstructed images using a dual-branch prompting strategy, encouraging rich\ncross-modal interactions. To bridge the modality gap and mitigate\ntraining-inference discrepancies, we introduce a distributional alignment loss\nthat enforces consistency between the output distributions of the two branches.\nExtensive experiments on the Multi30K dataset demonstrate that D2P-MMT achieves\nsuperior translation performance compared to existing state-of-the-art\napproaches.", "AI": {"tldr": "\u63d0\u51faD2P-MMT\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u91cd\u6784\u56fe\u50cf\u8fc7\u6ee4\u89c6\u89c9\u566a\u58f0\uff0c\u53cc\u5206\u652f\u63d0\u793a\u7b56\u7565\u589e\u5f3a\u8de8\u6a21\u6001\u4ea4\u4e92\uff0c\u5b9e\u73b0\u66f4\u9c81\u68d2\u7684\u89c6\u89c9\u5f15\u5bfc\u673a\u5668\u7ffb\u8bd1", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u673a\u5668\u7ffb\u8bd1\u65b9\u6cd5\u4f9d\u8d56\u914d\u5bf9\u56fe\u6587\u8f93\u5165\u4e14\u5bf9\u65e0\u5173\u89c6\u89c9\u566a\u58f0\u654f\u611f\uff0c\u9650\u5236\u6a21\u578b\u9c81\u68d2\u6027\u548c\u5b9e\u9645\u5e94\u7528", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u751f\u6210\u91cd\u6784\u56fe\u50cf\u8fc7\u6ee4\u5e72\u6270\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u63d0\u793a\u7b56\u7565\u8054\u5408\u5b66\u4e60\u771f\u5b9e/\u91cd\u6784\u56fe\u50cf\uff0c\u8bbe\u8ba1\u5206\u5e03\u5bf9\u9f50\u635f\u5931\u51cf\u5c11\u8bad\u7ec3-\u63a8\u7406\u5dee\u5f02", "result": "\u5728Multi30K\u6570\u636e\u96c6\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\uff0cBLEU\u5206\u6570\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "D2P-MMT\u901a\u8fc7\u751f\u6210\u5f0f\u56fe\u50cf\u91cd\u6784\u6709\u6548\u63d0\u5347\u7ffb\u8bd1\u9c81\u68d2\u6027\uff0c\u964d\u4f4e\u5bf9\u914d\u5bf9\u89c6\u89c9\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5177\u6709\u66f4\u5f3a\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2507.17746", "pdf": "https://arxiv.org/pdf/2507.17746", "abs": "https://arxiv.org/abs/2507.17746", "authors": ["Anisha Gunjal", "Anthony Wang", "Elaine Lau", "Vaskar Nath", "Bing Liu", "Sean Hendryx"], "title": "Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Extending Reinforcement Learning with Verifiable Rewards (RLVR) to real-world\ntasks often requires balancing objective and subjective evaluation criteria.\nHowever, many such tasks lack a single, unambiguous ground truth-making it\ndifficult to define reliable reward signals for post-training language models.\nWhile traditional preference-based methods offer a workaround, they rely on\nopaque reward functions that are difficult to interpret and prone to spurious\ncorrelations. We introduce $\\textbf{Rubrics as Rewards}$ (RaR), a framework\nthat uses structured, checklist-style rubrics as interpretable reward signals\nfor on-policy training with GRPO. Our best RaR method yields up to a $28\\%$\nrelative improvement on HealthBench-1k compared to simple Likert-based\napproaches, while matching or surpassing the performance of reward signals\nderived from expert-written references. By treating rubrics as structured\nreward signals, we show that RaR enables smaller-scale judge models to better\nalign with human preferences and sustain robust performance across model\nscales.", "AI": {"tldr": "\u63d0\u51faRaR\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8bc4\u5206\u6807\u51c6\u4f5c\u4e3a\u53ef\u89e3\u91ca\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u5728GRPO\u7b56\u7565\u8bad\u7ec3\u4e2d\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u548c\u5c0f\u6a21\u578b\u5bf9\u9f50\u4eba\u7c7b\u504f\u597d", "motivation": "\u73b0\u5b9e\u4efb\u52a1\u4e2d\u5e73\u8861\u5ba2\u89c2\u4e0e\u4e3b\u89c2\u8bc4\u4ef7\u6807\u51c6\u5b58\u5728\u6311\u6218\uff0c\u4f20\u7edf\u504f\u597d\u5b66\u4e60\u65b9\u6cd5\u5956\u52b1\u51fd\u6570\u4e0d\u900f\u660e\u4e14\u6613\u4ea7\u751f\u865a\u5047\u76f8\u5173\u6027", "method": "\u4f7f\u7528\u68c0\u67e5\u8868\u5f0f\u8bc4\u5206\u6807\u51c6(Rubrics)\u4f5c\u4e3a\u7ed3\u6784\u5316\u5956\u52b1\u4fe1\u53f7\uff0c\u7ed3\u5408GRPO\u8fdb\u884c\u7b56\u7565\u8bad\u7ec3", "result": "\u5728HealthBench-1k\u4e0a\u76f8\u5bf9Likert\u65b9\u6cd5\u63d0\u534728%\uff0c\u4e0e\u4e13\u5bb6\u53c2\u8003\u5956\u52b1\u4fe1\u53f7\u6027\u80fd\u76f8\u5f53\uff0c\u5c0f\u89c4\u6a21\u8bc4\u59d4\u6a21\u578b\u80fd\u66f4\u597d\u5bf9\u9f50\u4eba\u7c7b\u504f\u597d", "conclusion": "RaR\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u5956\u52b1\u673a\u5236\u6709\u6548\u63d0\u5347\u6a21\u578b\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\uff0c\u4e14\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027"}}
