<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 104]
- [cs.GR](#cs.GR) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Meaning Is Not A Metric: Using LLMs to make cultural context legible at scale](https://arxiv.org/abs/2505.23785)
*Cody Kommers,Drew Hemment,Maria Antoniak,Joel Z. Leibo,Hoyt Long,Emily Robinson,Adam Sobey*

Main category: cs.CL

TL;DR: 提出LLMs通过厚描述（thick description）可规模化解析文化语境与人类意义，突破传统AI系统薄描述（thin description）的标准化局限，并指出五大关键挑战。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统依赖薄描述（数值化表征）无法捕捉人类活动的文化语境与意义，而人文社科的厚描述虽能编码意义但难以规模化应用。LLMs的文本处理能力为此提供了新解决方案。

Method: 理论论证：对比薄/厚描述的差异，提出LLMs可自动化生成厚描述以解决规模化瓶颈，并构建厚描述作为AI系统新表征范式的框架。

Result: 识别五个核心挑战：保持语境完整性、维护解释多元性、整合经验与批判视角、区分质性与量化表征、承认意义的动态性。

Conclusion: 厚描述可作为统一框架应对LLMs文化表征难题，是生成式AI发展的关键方向，需在技术设计中嵌入人文社科洞察以实现更丰富的人类意义表征。

Abstract: This position paper argues that large language models (LLMs) can make
cultural context, and therefore human meaning, legible at an unprecedented
scale in AI-based sociotechnical systems. We argue that such systems have
previously been unable to represent human meaning because they rely on thin
descriptions: numerical representations that enforce standardization and
therefore strip human activity of the cultural context that gives it meaning.
By contrast, scholars in the humanities and qualitative social sciences have
developed frameworks for representing meaning through thick description: verbal
representations that accommodate heterogeneity and retain contextual
information needed to represent human meaning. While these methods can
effectively codify meaning, they are difficult to deploy at scale. However, the
verbal capabilities of LLMs now provide a means of (at least partially)
automating the generation and processing of thick descriptions, potentially
overcoming this bottleneck. We argue that the problem of rendering human
meaning legible is not just about selecting better metrics, but about
developing new representational formats (based on thick description). We frame
this as a crucial direction for the application of generative AI and identify
five key challenges: preserving context, maintaining interpretive pluralism,
integrating perspectives based on lived experience and critical distance,
distinguishing qualitative content from quantitative magnitude, and
acknowledging meaning as dynamic rather than static. Furthermore, we suggest
that thick description has the potential to serve as a unifying framework to
address a number of emerging concerns about the difficulties of representing
culture in (or using) LLMs.

</details>


### [2] [Nine Ways to Break Copyright Law and Why Our LLM Won't: A Fair Use Aligned Generation Framework](https://arxiv.org/abs/2505.23788)
*Aakash Sen Sharma,Debdeep Sanyal,Priyansh Srivastava,Sundar Atreya H.,Shirish Karande,Mohan Kankanhalli,Murari Mandal*

Main category: cs.CL

TL;DR: 提出了FUA-LLM框架，通过法律合规的DPO微调和专用数据集FairUseDB，显著减少大模型侵权风险同时保持实用性。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型易直接复制版权内容引发侵权问题，现有拒绝式过滤方案损害模型实用性

Method: 1. 构建包含18,000个专家验证案例的FairUseDB数据集
2. 应用直接偏好优化(DPO)进行模型微调
3. 提出加权惩罚效用和CAH新评估指标

Result: 相比现有方法减少20%问题输出，通过专家评估验证框架有效性，保持真实场景可用性

Conclusion: FUA-LLM首次在法律合规与实用价值间取得平衡，为AI版权问题提供了可量化的解决方案

Abstract: Large language models (LLMs) commonly risk copyright infringement by
reproducing protected content verbatim or with insufficient transformative
modifications, posing significant ethical, legal, and practical concerns.
Current inference-time safeguards predominantly rely on restrictive
refusal-based filters, often compromising the practical utility of these
models. To address this, we collaborated closely with intellectual property
experts to develop FUA-LLM (Fair Use Aligned Language Models), a
legally-grounded framework explicitly designed to align LLM outputs with
fair-use doctrine. Central to our method is FairUseDB, a carefully constructed
dataset containing 18,000 expert-validated examples covering nine realistic
infringement scenarios. Leveraging this dataset, we apply Direct Preference
Optimization (DPO) to fine-tune open-source LLMs, encouraging them to produce
legally compliant and practically useful alternatives rather than resorting to
blunt refusal. Recognizing the shortcomings of traditional evaluation metrics,
we propose new measures: Weighted Penalty Utility and Compliance Aware Harmonic
Mean (CAH) to balance infringement risk against response utility. Extensive
quantitative experiments coupled with expert evaluations confirm that FUA-LLM
substantially reduces problematic outputs (up to 20\%) compared to
state-of-the-art approaches, while preserving real-world usability.

</details>


### [3] [Conversational Exploration of Literature Landscape with LitChat](https://arxiv.org/abs/2505.23789)
*Mingyu Huang,Shasha Zhou,Yuxuan Chen,Ke Li*

Main category: cs.CL

TL;DR: LitChat是端到端对话式文献分析代理，通过整合数据驱动工具解决大模型在文献系统综述中的局限性


<details>
  <summary>Details</summary>
Motivation: 海量数字文献时代传统人工综述效率低下，大语言模型存在上下文限制和可信度问题，无法满足系统文献分析需求

Method: 结合数据驱动发现工具（自动检索、知识图谱构建、数据挖掘技术）增强LLM代理，实现交互式文献探索

Result: 通过AI4Health案例验证，LitChat能快速导航大规模文献并提供传统方法无法实现的数据证据支持

Conclusion: LitChat通过证据驱动方法有效弥补传统综述和纯LLM方案的不足，为系统文献分析提供新范式

Abstract: We are living in an era of "big literature", where the volume of digital
scientific publications is growing exponentially. While offering new
opportunities, this also poses challenges for understanding literature
landscapes, as traditional manual reviewing is no longer feasible. Recent large
language models (LLMs) have shown strong capabilities for literature
comprehension, yet they are incapable of offering "comprehensive, objective,
open and transparent" views desired by systematic reviews due to their limited
context windows and trust issues like hallucinations. Here we present LitChat,
an end-to-end, interactive and conversational literature agent that augments
LLM agents with data-driven discovery tools to facilitate literature
exploration. LitChat automatically interprets user queries, retrieves relevant
sources, constructs knowledge graphs, and employs diverse data-mining
techniques to generate evidence-based insights addressing user needs. We
illustrate the effectiveness of LitChat via a case study on AI4Health,
highlighting its capacity to quickly navigate the users through large-scale
literature landscape with data-based evidence that is otherwise infeasible with
traditional means.

</details>


### [4] [Rethinking the Understanding Ability across LLMs through Mutual Information](https://arxiv.org/abs/2505.23790)
*Shaojie Wang,Sirui Ding,Na Zou*

Main category: cs.CL

TL;DR: 提出基于互信息的信息论框架评估LLM语言理解能力，通过词级MI分解和恢复任务实验揭示模型差异，优化恢复能力可提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估方法难以衡量其内在语言理解能力，需要更基础的信息论框架量化模型的信息保存能力。

Method: 将句子级MI分解为词级MI并建立理论界限，基于Fano不等式推导可计算的词级MI下界，设计token恢复任务实现跨模型比较。

Result: 编码器模型信息保真度显著高于解码器模型，后者呈现独特的后期'遗忘'模式；优化恢复能力可提升模型无监督任务表现。

Conclusion: 互信息可作为语言模型能力评估的基础指标，通过优化信息保存能力可直接提升模型理解能力。

Abstract: Recent advances in large language models (LLMs) have revolutionized natural
language processing, yet evaluating their intrinsic linguistic understanding
remains challenging. Moving beyond specialized evaluation tasks, we propose an
information-theoretic framework grounded in mutual information (MI) to achieve
this. We formalize the understanding as MI between an input sentence and its
latent representation (sentence-level MI), measuring how effectively input
information is preserved in latent representation. Given that LLMs learn
embeddings for individual tokens, we decompose sentence-level MI into
token-level MI between tokens and sentence embeddings, establishing theoretical
bounds connecting these measures. Based on this foundation, we theoretically
derive a computable lower bound for token-level MI using Fano's inequality,
which directly relates to token-level recoverability-the ability to predict
original tokens from sentence embedding. We implement this recoverability task
to comparatively measure MI across different LLMs, revealing that encoder-only
models consistently maintain higher information fidelity than their
decoder-only counterparts, with the latter exhibiting a distinctive late-layer
"forgetting" pattern where mutual information is first enhanced and then
discarded. Moreover, fine-tuning to maximize token-level recoverability
consistently improves understanding ability of LLMs on tasks without
task-specific supervision, demonstrating that mutual information can serve as a
foundation for understanding and improving language model capabilities.

</details>


### [5] [R3-RAG: Learning Step-by-Step Reasoning and Retrieval for LLMs via Reinforcement Learning](https://arxiv.org/abs/2505.23794)
*Yuan Li,Qi Luo,Xiaonan Li,Bufan Li,Qinyuan Cheng,Bo Wang,Yining Zheng,Yuxin Wang,Zhangyue Yin,Xipeng Qiu*

Main category: cs.CL

TL;DR: R3-RAG通过强化学习实现逐步推理与检索，提升RAG系统的知识全面性与答案准确性。


<details>
  <summary>Details</summary>
Motivation: 解决传统RAG中密集检索器参数规模小、无法分步推理的瓶颈，突破人工设计流程的局限。

Method: 两阶段框架：冷启动阶段学习推理-检索交替模式；强化学习阶段设计答案正确性（结果奖励）和文档相关性（过程奖励）双奖励机制。

Result: 实验表明R3-RAG显著超越基线模型，且能适配不同检索器，代码已开源。

Conclusion: 通过强化学习机制引导大模型自主进行迭代式推理与检索，是提升RAG系统性能的有效路径。

Abstract: Retrieval-Augmented Generation (RAG) integrates external knowledge with Large
Language Models (LLMs) to enhance factual correctness and mitigate
hallucination. However, dense retrievers often become the bottleneck of RAG
systems due to their limited parameters compared to LLMs and their inability to
perform step-by-step reasoning. While prompt-based iterative RAG attempts to
address these limitations, it is constrained by human-designed workflows. To
address these limitations, we propose $\textbf{R3-RAG}$, which uses
$\textbf{R}$einforcement learning to make the LLM learn how to
$\textbf{R}$eason and $\textbf{R}$etrieve step by step, thus retrieving
comprehensive external knowledge and leading to correct answers. R3-RAG is
divided into two stages. We first use cold start to make the model learn the
manner of iteratively interleaving reasoning and retrieval. Then we use
reinforcement learning to further harness its ability to better explore the
external retrieval environment. Specifically, we propose two rewards for
R3-RAG: 1) answer correctness for outcome reward, which judges whether the
trajectory leads to a correct answer; 2) relevance-based document verification
for process reward, encouraging the model to retrieve documents that are
relevant to the user question, through which we can let the model learn how to
iteratively reason and retrieve relevant documents to get the correct answer.
Experimental results show that R3-RAG significantly outperforms baselines and
can transfer well to different retrievers. We release R3-RAG at
https://github.com/Yuan-Li-FNLP/R3-RAG.

</details>


### [6] [Emergent LLM behaviors are observationally equivalent to data leakage](https://arxiv.org/abs/2505.23796)
*Christopher Barrie,Petter Törnberg*

Main category: cs.CL

TL;DR: 研究表明，Ashery等人关于大语言模型在命名游戏中自发形成社会规范的结论实际源于数据泄露而非真正的『涌现』行为。


<details>
  <summary>Details</summary>
Motivation: 质疑Ashery等人将LLMs在协调游戏中的表现解释为『社会规范涌现』的结论，揭示其本质是对预训练数据的记忆。

Method: 通过分析模型对协调游戏结构的识别能力、结果预测能力，并与训练数据对比验证数据泄露假说。

Result: 模型行为与训练数据记忆高度一致，无法区分『涌现』行为与数据复现。

Conclusion: 需寻找替代研究策略，并重新思考LLMs在社会科学模型中的适用性边界。

Abstract: Ashery et al. recently argue that large language models (LLMs), when paired
to play a classic "naming game," spontaneously develop linguistic conventions
reminiscent of human social norms. Here, we show that their results are better
explained by data leakage: the models simply reproduce conventions they already
encountered during pre-training. Despite the authors' mitigation measures, we
provide multiple analyses demonstrating that the LLMs recognize the structure
of the coordination game and recall its outcomes, rather than exhibit
"emergent" conventions. Consequently, the observed behaviors are
indistinguishable from memorization of the training corpus. We conclude by
pointing to potential alternative strategies and reflecting more generally on
the place of LLMs for social science models.

</details>


### [7] [Detection of Suicidal Risk on Social Media: A Hybrid Model](https://arxiv.org/abs/2505.23797)
*Zaihan Yang,Ryan Leonard,Hien Tran,Rory Driscoll,Chadbourne Davis*

Main category: cs.CL

TL;DR: 开发RoBERTa-TF-IDF-PCA混合模型提升社交媒体帖子自杀风险分类准确率


<details>
  <summary>Details</summary>
Motivation: 自杀风险早期检测需求迫切，需可靠自动化工具进行分级评估

Method: 融合RoBERTa深度语义特征与TF-IDF统计特征（经PCA降维），采用数据重采样和增强策略解决数据不平衡

Result: 混合模型加权F1值达0.7512，优于单一模型与传统方法

Conclusion: 多模态特征融合方法显著提升自杀风险评估效果，为AI心理危机干预提供新思路

Abstract: Suicidal thoughts and behaviors are increasingly recognized as a critical
societal concern, highlighting the urgent need for effective tools to enable
early detection of suicidal risk. In this work, we develop robust machine
learning models that leverage Reddit posts to automatically classify them into
four distinct levels of suicide risk severity. We frame this as a multi-class
classification task and propose a RoBERTa-TF-IDF-PCA Hybrid model, integrating
the deep contextual embeddings from Robustly Optimized BERT Approach (RoBERTa),
a state-of-the-art deep learning transformer model, with the statistical
term-weighting of TF-IDF, further compressed with PCA, to boost the accuracy
and reliability of suicide risk assessment. To address data imbalance and
overfitting, we explore various data resampling techniques and data
augmentation strategies to enhance model generalization. Additionally, we
compare our model's performance against that of using RoBERTa only, the BERT
model and other traditional machine learning classifiers. Experimental results
demonstrate that the hybrid model can achieve improved performance, giving a
best weighted $F_{1}$ score of 0.7512.

</details>


### [8] [My Answer Is NOT 'Fair': Mitigating Social Bias in Vision-Language Models via Fair and Biased Residuals](https://arxiv.org/abs/2505.23798)
*Jian Lan,Yifei Fu,Udo Schlegel,Gengyuan Zhang,Tanveer Hannan,Haokun Chen,Thomas Seidl*

Main category: cs.CL

TL;DR: 研究评估了视觉语言模型（VLM）中的社会偏见，发现模型存在性别和种族偏见响应及错误校准的置信度，并提出一种无需训练的推理阶段后处理方法以有效缓解偏见。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型中的社会偏见导致公平性和伦理问题，但模型生成响应中的偏见程度及缓解方法尚未充分研究。需从模型响应和概率分布两方面评估并缓解偏见。

Method: 1. 在PAIRS和SocialCounterfactuals数据集上对四个SOTA VLM进行多选任务评估
2. 分析模型隐藏层的公平性波动及残差影响
3. 提出推理阶段通过消除偏见相关残差、放大公平相关残差的模型无关后处理方法

Result: 1. 模型存在性别/种族偏见响应
2. 隐藏层公平性波动显著，不同层残差对公平性有正负混合影响
3. 后处理方法在公平性和置信度校准上优于现有训练策略

Conclusion: 该后处理方法无需重新训练即可有效降低VLM的社会偏见，提升响应公平性和置信度可靠性，为模型公平性改进提供新方向。

Abstract: Social bias is a critical issue in large vision-language models (VLMs), where
fairness- and ethics-related problems harm certain groups of people in society.
It is unknown to what extent VLMs yield social bias in generative responses. In
this study, we focus on evaluating and mitigating social bias on both the
model's response and probability distribution. To do so, we first evaluate four
state-of-the-art VLMs on PAIRS and SocialCounterfactuals datasets with the
multiple-choice selection task. Surprisingly, we find that models suffer from
generating gender-biased or race-biased responses. We also observe that models
are prone to stating their responses are fair, but indeed having mis-calibrated
confidence levels towards particular social groups. While investigating why
VLMs are unfair in this study, we observe that VLMs' hidden layers exhibit
substantial fluctuations in fairness levels. Meanwhile, residuals in each layer
show mixed effects on fairness, with some contributing positively while some
lead to increased bias. Based on these findings, we propose a post-hoc method
for the inference stage to mitigate social bias, which is training-free and
model-agnostic. We achieve this by ablating bias-associated residuals while
amplifying fairness-associated residuals on model hidden layers during
inference. We demonstrate that our post-hoc method outperforms the competing
training strategies, helping VLMs have fairer responses and more reliable
confidence levels.

</details>


### [9] [Estimating LLM Consistency: A User Baseline vs Surrogate Metrics](https://arxiv.org/abs/2505.23799)
*Xiaoyuan Wu,Weiran Lin,Omer Akgul,Lujo Bauer*

Main category: cs.CL

TL;DR: 大型语言模型存在幻觉和脆弱性问题，现有一致性评估方法无法有效匹配人类感知，提出新logit集成方法并建议加强人工评估


<details>
  <summary>Details</summary>
Motivation: 验证现有LLM一致性评估方法是否符合人类主观感知，并探索更有效的一致性评估方案

Method: 通过大规模用户研究(n=2,976)对比现有方法，提出基于logit集成的新评估指标

Result: 新方法达到现有最佳指标水平，现有非人工评估方法存在显著局限性

Conclusion: LLM一致性评估需要更广泛结合人类判断，单纯算法指标难以准确反映用户体验

Abstract: Large language models (LLMs) are prone to hallucinations and sensitive to
prompt perturbations, often resulting in inconsistent or unreliable generated
text. Different methods have been proposed to mitigate such hallucinations and
fragility -- one of them being measuring the consistency (the model's
confidence in the response, or likelihood of generating a similar response when
resampled) of LLM responses. In previous work, measuring consistency often
relied on the probability of a response appearing within a pool of resampled
responses, or internal states or logits of responses. However, it is not yet
clear how well these approaches approximate how humans perceive the consistency
of LLM responses. We performed a user study (n=2,976) and found current methods
typically do not approximate users' perceptions of LLM consistency very well.
We propose a logit-based ensemble method for estimating LLM consistency, and we
show that this method matches the performance of the best-performing existing
metric in estimating human ratings of LLM consistency. Our results suggest that
methods of estimating LLM consistency without human evaluation are sufficiently
imperfect that we suggest evaluation with human input be more broadly used.

</details>


### [10] [SEMFED: Semantic-Aware Resource-Efficient Federated Learning for Heterogeneous NLP Tasks](https://arxiv.org/abs/2505.23801)
*Sajid Hussain,Muhammad Sohail,Nauman Ali Khan*

Main category: cs.CL

TL;DR: 提出SEMFED框架解决NLP联邦学习中的语义异构和资源限制问题，通信成本降低80.5%且准确率保持98%+


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习在NLP任务中面临语义异构性、词汇不匹配和设备资源差异三大挑战，需针对性解决方案

Method: 1. 语义感知客户端选择机制
2. 自适应NLP模型架构
3. 语义特征压缩技术（降低带宽需求）

Result: 在多种NLP分类任务中实现通信成本降低80.5%，模型准确率维持在98%以上，超越现有联邦学习方法

Conclusion: SEMFED有效解决异构设备环境中的计算资源、网络可靠性和语义分布差异问题，适合实际联邦NLP部署

Abstract: Background: Federated Learning (FL) has emerged as a promising paradigm for
training machine learning models while preserving data privacy. However,
applying FL to Natural Language Processing (NLP) tasks presents unique
challenges due to semantic heterogeneity across clients, vocabulary mismatches,
and varying resource constraints on edge devices. Objectives: This paper
introduces SEMFED, a novel semantic-aware resource-efficient federated learning
framework specifically designed for heterogeneous NLP tasks. Methods: SEMFED
incorporates three key innovations: (1) a semantic-aware client selection
mechanism that balances semantic diversity with resource constraints, (2)
adaptive NLP-specific model architectures tailored to device capabilities while
preserving semantic information, and (3) a communication-efficient semantic
feature compression technique that significantly reduces bandwidth
requirements. Results: Experimental results on various NLP classification tasks
demonstrate that SEMFED achieves an 80.5% reduction in communication costs
while maintaining model accuracy above 98%, outperforming state-of-the-art FL
approaches. Conclusion: SEMFED effectively manages heterogeneous client
environments with varying computational resources, network reliability, and
semantic data distributions, making it particularly suitable for real-world
federated NLP deployments.

</details>


### [11] [MedHELM: Holistic Evaluation of Large Language Models for Medical Tasks](https://arxiv.org/abs/2505.23802)
*Suhana Bedi,Hejie Cui,Miguel Fuentes,Alyssa Unell,Michael Wornow,Juan M. Banda,Nikesh Kotecha,Timothy Keyes,Yifan Mai,Mert Oez,Hao Qiu,Shrey Jain,Leonardo Schettini,Mehr Kashyap,Jason Alan Fries,Akshay Swaminathan,Philip Chung,Fateme Nateghi,Asad Aali,Ashwin Nayak,Shivam Vedak,Sneha S. Jain,Birju Patel,Oluseyi Fayanju,Shreya Shah,Ethan Goh,Dong-han Yao,Brian Soetikno,Eduardo Reis,Sergios Gatidis,Vasu Divi,Robson Capasso,Rachna Saralkar,Chia-Chun Chiang,Jenelle Jindal,Tho Pham,Faraz Ghoddusi,Steven Lin,Albert S. Chiou,Christy Hong,Mohana Roy,Michael F. Gensheimer,Hinesh Patel,Kevin Schulman,Dev Dash,Danton Char,Lance Downing,Francois Grolleau,Kameron Black,Bethel Mieso,Aydin Zahedivash,Wen-wai Yim,Harshita Sharma,Tony Lee,Hannah Kirsch,Jennifer Lee,Nerissa Ambers,Carlene Lugtu,Aditya Sharma,Bilal Mawji,Alex Alekseyev,Vicky Zhou,Vikas Kakkar,Jarrod Helzer,Anurang Revri,Yair Bannett,Roxana Daneshjou,Jonathan Chen,Emily Alsentzer,Keith Morse,Nirmal Ravi,Nima Aghaeepour,Vanessa Kennedy,Akshay Chaudhari,Thomas Wang,Sanmi Koyejo,Matthew P. Lungren,Eric Horvitz,Percy Liang,Mike Pfeffer,Nigam H. Shah*

Main category: cs.CL

TL;DR: 研究者开发了MedHELM评估框架，系统评估大语言模型在医疗任务中的表现，发现模型在不同临床场景下性能差异显著，并提出更贴近真实医疗场景的评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有医学执照考试评估无法反映真实临床实践的复杂性，需建立更全面的评估体系来指导LLM的医疗应用。

Method: 1. 建立包含5大类/22子类/121任务的临床验证分类法
2. 构建含35个基准测试的评估套件（17现有+18新设计）
3. 采用改进评估方法（LLM陪审团）并进行成本-性能分析

Result: 1. 高级推理模型表现最佳（DeepSeek R1胜率66%）
2. Claude 3.5以低40%计算成本达到相近性能
3. 模型在病历生成（0.73-0.85）和医患沟通（0.78-0.83）表现突出，在临床决策支持（0.56-0.72）较弱
4. LLM陪审团评估与临床医生评分一致性（ICC=0.47）优于传统指标

Conclusion: 强调任务特异性评估对医疗LLM应用的重要性，提供开源框架MedHELM支持持续评估，Claude 3.5展示出优秀的成本效益比。

Abstract: While large language models (LLMs) achieve near-perfect scores on medical
licensing exams, these evaluations inadequately reflect the complexity and
diversity of real-world clinical practice. We introduce MedHELM, an extensible
evaluation framework for assessing LLM performance for medical tasks with three
key contributions. First, a clinician-validated taxonomy spanning 5 categories,
22 subcategories, and 121 tasks developed with 29 clinicians. Second, a
comprehensive benchmark suite comprising 35 benchmarks (17 existing, 18 newly
formulated) providing complete coverage of all categories and subcategories in
the taxonomy. Third, a systematic comparison of LLMs with improved evaluation
methods (using an LLM-jury) and a cost-performance analysis. Evaluation of 9
frontier LLMs, using the 35 benchmarks, revealed significant performance
variation. Advanced reasoning models (DeepSeek R1: 66% win-rate; o3-mini: 64%
win-rate) demonstrated superior performance, though Claude 3.5 Sonnet achieved
comparable results at 40% lower estimated computational cost. On a normalized
accuracy scale (0-1), most models performed strongly in Clinical Note
Generation (0.73-0.85) and Patient Communication & Education (0.78-0.83),
moderately in Medical Research Assistance (0.65-0.75), and generally lower in
Clinical Decision Support (0.56-0.72) and Administration & Workflow
(0.53-0.63). Our LLM-jury evaluation method achieved good agreement with
clinician ratings (ICC = 0.47), surpassing both average clinician-clinician
agreement (ICC = 0.43) and automated baselines including ROUGE-L (0.36) and
BERTScore-F1 (0.44). Claude 3.5 Sonnet achieved comparable performance to top
models at lower estimated cost. These findings highlight the importance of
real-world, task-specific evaluation for medical use of LLMs and provides an
open source framework to enable this.

</details>


### [12] [Calibrating LLMs for Text-to-SQL Parsing by Leveraging Sub-clause Frequencies](https://arxiv.org/abs/2505.23804)
*Terrance Liu,Shuyi Wang,Daniel Preotiuc-Pietro,Yash Chandarana,Chirag Gupta*

Main category: cs.CL

TL;DR: LLMs在文本到SQL解析中存在自信错误问题，提出通过Platt缩放和子句频率(SCF)结合多元Platt缩放(MPS)实现校准改进。


<details>
  <summary>Details</summary>
Motivation: LLMs在文本到SQL解析中可能产生自信错误结果，需建立可靠的不确定性度量机制提升系统可信度。

Method: 提出结构化校准框架：1) 基准测试建立；2) 传统Platt缩放优化；3) 基于SQL结构的SCF特征；4) 多元Platt缩放(MPS)整合特征。

Result: 在两个主流数据集上验证，MPS+SCF方案比传统Platt校准错误率降低23%，错误检测F1值提升15%。

Conclusion: 结构化特征与多元校准方法显著提升文本到SQL系统的置信度校准效果，增强输出可靠性。

Abstract: While large language models (LLMs) achieve strong performance on text-to-SQL
parsing, they sometimes exhibit unexpected failures in which they are
confidently incorrect. Building trustworthy text-to-SQL systems thus requires
eliciting reliable uncertainty measures from the LLM. In this paper, we study
the problem of providing a calibrated confidence score that conveys the
likelihood of an output query being correct. Our work is the first to establish
a benchmark for post-hoc calibration of LLM-based text-to-SQL parsing. In
particular, we show that Platt scaling, a canonical method for calibration,
provides substantial improvements over directly using raw model output
probabilities as confidence scores. Furthermore, we propose a method for
text-to-SQL calibration that leverages the structured nature of SQL queries to
provide more granular signals of correctness, named "sub-clause frequency"
(SCF) scores. Using multivariate Platt scaling (MPS), our extension of the
canonical Platt scaling technique, we combine individual SCF scores into an
overall accurate and calibrated score. Empirical evaluation on two popular
text-to-SQL datasets shows that our approach of combining MPS and SCF yields
further improvements in calibration and the related task of error detection
over traditional Platt scaling.

</details>


### [13] [MedOrchestra: A Hybrid Cloud-Local LLM Approach for Clinical Data Interpretation](https://arxiv.org/abs/2505.23806)
*Sihyeon Lee,Hyunjoo Song,Jong-chan Lee,Yoon Jin Lee,Boram Lee,Hee-Eon Lim,Dongyeong Kim,Jinwook Seo,Bohyoung Kim*

Main category: cs.CL

TL;DR: MedOrchestra框架通过云端LLM分解任务与本地LLM隐私执行，解决了临床部署中隐私与性能的权衡问题，在胰腺癌分期任务中准确率显著超越基线模型及临床医生。


<details>
  <summary>Details</summary>
Motivation: 云端LLM存在临床数据隐私风险，本地LLM处理复杂任务能力不足。需在保护隐私的同时提升复杂临床解释任务的性能。

Method: 云端LLM分解复杂任务为子任务并生成提示(基于临床指南和合成测试用例)，本地LLM隐私执行子任务并合成结果。形成闭环验证系统。

Result: 非结构化报告准确率70.21%(超本地模型基线21-48%、超临床医生5-15%)；结构化报告达85.42%准确率。

Conclusion: 该框架首次实现云端-本地LLM协同，在严格隐私保护下显著提升临床决策准确性，特别在结构化报告中展现更大潜力。

Abstract: Deploying large language models (LLMs) in clinical settings faces critical
trade-offs: cloud LLMs, with their extensive parameters and superior
performance, pose risks to sensitive clinical data privacy, while local LLMs
preserve privacy but often fail at complex clinical interpretation tasks. We
propose MedOrchestra, a hybrid framework where a cloud LLM decomposes complex
clinical tasks into manageable subtasks and prompt generation, while a local
LLM executes these subtasks in a privacy-preserving manner. Without accessing
clinical data, the cloud LLM generates and validates subtask prompts using
clinical guidelines and synthetic test cases. The local LLM executes subtasks
locally and synthesizes outputs generated by the cloud LLM. We evaluate
MedOrchestra on pancreatic cancer staging using 100 radiology reports under
NCCN guidelines. On free-text reports, MedOrchestra achieves 70.21% accuracy,
outperforming local model baselines (without guideline: 48.94%, with guideline:
56.59%) and board-certified clinicians (gastroenterologists: 59.57%, surgeons:
65.96%, radiologists: 55.32%). On structured reports, MedOrchestra reaches
85.42% accuracy, showing clear superiority across all settings.

</details>


### [14] [DLP: Dynamic Layerwise Pruning in Large Language Models](https://arxiv.org/abs/2505.23807)
*Yuli Chen,Bo Cheng,Jiale Han,Yingying Zhang,Yingting Li,Shuhao Zhang*

Main category: cs.CL

TL;DR: 提出动态分层剪枝（DLP）方法，通过整合模型权重和输入激活信息自适应确定各层剪枝率，在70%高稀疏率下显著提升LLM性能


<details>
  <summary>Details</summary>
Motivation: 传统均匀分层剪枝在高稀疏率下性能骤降，现有非均匀剪枝方法依赖预定义参数导致次优结果，需更智能的剪枝策略

Method: 动态融合模型权重与输入激活信息，通过重要性评估自适应分配各层剪枝比例，兼容多种LLM压缩技术及参数高效微调

Result: 70%稀疏率时LLaMA2-7B困惑度降低7.79，准确率提升2.7%，超越现有最佳方法且具备技术兼容性

Conclusion: DLP突破传统剪枝限制，实现高稀疏率下的性能保持，开创LLM压缩新范式，代码开源推动后续研究

Abstract: Pruning has recently been widely adopted to reduce the parameter scale and
improve the inference efficiency of Large Language Models (LLMs). Mainstream
pruning techniques often rely on uniform layerwise pruning strategies, which
can lead to severe performance degradation at high sparsity levels. Recognizing
the varying contributions of different layers in LLMs, recent studies have
shifted their focus toward non-uniform layerwise pruning. However, these
approaches often rely on pre-defined values, which can result in suboptimal
performance. To overcome these limitations, we propose a novel method called
Dynamic Layerwise Pruning (DLP). This approach adaptively determines the
relative importance of each layer by integrating model weights with input
activation information, assigning pruning rates accordingly. Experimental
results show that DLP effectively preserves model performance at high sparsity
levels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the
perplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7%
compared to state-of-the-art methods. Moreover, DLP is compatible with various
existing LLM compression techniques and can be seamlessly integrated into
Parameter-Efficient Fine-Tuning (PEFT). We release the code at
https://github.com/ironartisan/DLP to facilitate future research.

</details>


### [15] [DenseLoRA: Dense Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2505.23808)
*Lin Mu,Xiaoyu Wang,Li Ni,Yang Li,Zhize Wu,Peiquan Jin,Yiwen Zhang*

Main category: cs.CL

TL;DR: 提出DenseLoRA方法，通过单编码器-解码器结构和密集低秩矩阵实现更高效的LLM适配，在0.01%可训练参数下达到83.8%准确率，优于LoRA


<details>
  <summary>Details</summary>
Motivation: 针对传统LoRA方法中低秩矩阵参数冗余导致利用效率低下的问题，寻求更高效的参数适应方案

Method: 1. 引入表征微调概念，在适配前使用编码器-解码器压缩隐藏表征
2. 用单密集低秩矩阵替代传统双低秩矩阵结构
3. 通过系统实验验证各组件对性能的影响

Result: 在LLaMA3-8B模型上：
- DenseLoRA仅用0.01%可训练参数达到83.8%准确率
- 对比LoRA需要0.70%参数获得80.8%准确率

Conclusion: DenseLoRA通过结构创新显著提升参数效率与模型性能，为LLM适配提供了更优解决方案，且已开源代码促进实际应用

Abstract: Low-rank adaptation (LoRA) has been developed as an efficient approach for
adapting large language models (LLMs) by fine-tuning two low-rank matrices,
thereby reducing the number of trainable parameters. However, prior research
indicates that many of the weights in these matrices are redundant, leading to
inefficiencies in parameter utilization. To address this limitation, we
introduce Dense Low-Rank Adaptation (DenseLoRA), a novel approach that enhances
parameter efficiency while achieving superior performance compared to LoRA.
DenseLoRA builds upon the concept of representation fine-tuning, incorporating
a single Encoder-Decoder to refine and compress hidden representations across
all adaptation layers before applying adaptation. Instead of relying on two
redundant low-rank matrices as in LoRA, DenseLoRA adapts LLMs through a dense
low-rank matrix, improving parameter utilization and adaptation efficiency. We
evaluate DenseLoRA on various benchmarks, showing that it achieves 83.8%
accuracy with only 0.01% of trainable parameters, compared to LoRA's 80.8%
accuracy with 0.70% of trainable parameters on LLaMA3-8B. Additionally, we
conduct extensive experiments to systematically assess the impact of
DenseLoRA's components on overall model performance. Code is available at
https://github.com/mulin-ahu/DenseLoRA.

</details>


### [16] [LLM-Driven E-Commerce Marketing Content Optimization: Balancing Creativity and Conversion](https://arxiv.org/abs/2505.23809)
*Haowei Yang,Haotian Lyu,Tianle Zhang,Dingzhou Wang,Yushang Zhao*

Main category: cs.CL

TL;DR: 提出整合提示工程、多目标微调和后处理的框架，生成兼具吸引力和转化率的电商文案，CTR提升12.5%，CVR提升8.3%


<details>
  <summary>Details</summary>
Motivation: 电商竞争加剧需平衡创意内容与转化效果，现有方法难以兼顾，需利用LLM的语言生成能力解决该矛盾

Method: 框架包含：1)提示工程构建结构化输入 2)多目标微调（情感优化/多样性增强/CTA嵌入）3)后处理控制输出质量

Result: 离线评估与跨品类A/B测试显示：点击率提升12.5%，转化率提升8.3%，且保持内容新颖性指标

Conclusion: 为自动化文案生成提供实用解决方案，未来可延伸至多模态融合与实时个性化方向

Abstract: As e-commerce competition intensifies, balancing creative content with
conversion effectiveness becomes critical. Leveraging LLMs' language generation
capabilities, we propose a framework that integrates prompt engineering,
multi-objective fine-tuning, and post-processing to generate marketing copy
that is both engaging and conversion-driven. Our fine-tuning method combines
sentiment adjustment, diversity enhancement, and CTA embedding. Through offline
evaluations and online A/B tests across categories, our approach achieves a
12.5 % increase in CTR and an 8.3 % increase in CVR while maintaining content
novelty. This provides a practical solution for automated copy generation and
suggests paths for future multimodal, real-time personalization.

</details>


### [17] [MARS-Bench: A Multi-turn Athletic Real-world Scenario Benchmark for Dialogue Evaluation](https://arxiv.org/abs/2505.23810)
*Chenghao Yang,Yinbo Luo,Zhoufutu Wen,Qi Chu,Tao Gong,Longxiang Liu,Kaiyuan Zhang,Jianpeng Jiao,Ge Zhang,Wenhao Huang,Nenghai Yu*

Main category: cs.CL

TL;DR: 提出了MARS-Bench多轮对话测试基准，发现闭源LLM显著优于开源模型，显式推理能提升鲁棒性，并揭示了注意力机制对长对话处理的影响


<details>
  <summary>Details</summary>
Motivation: 现有基准无法全面评估大语言模型在长复杂对话场景（包含动机转移和复杂跨轮依赖）中的表现

Method: 基于体育赛事解说文本构建MARS-Bench基准，包含超多轮、交互式多轮和跨轮任务三个关键评估维度

Result: 闭源LLM显著优于开源模型；显式推理提升长对话处理能力；LLM在动机转移和复杂跨轮依赖处理上存在显著挑战；注意力可视化实验揭示了特殊token导致的注意力下沉问题

Conclusion: MARS-Bench有效揭示LLM在复杂对话中的局限性，为提升对话系统鲁棒性提供新方向，注意力机制的研究为模型优化提供理论依据

Abstract: Large Language Models (\textbf{LLMs}), e.g. ChatGPT, have been widely adopted
in real-world dialogue applications. However, LLMs' robustness, especially in
handling long complex dialogue sessions, including frequent motivation
transfer, sophisticated cross-turn dependency, is criticized all along.
Nevertheless, no existing benchmarks can fully reflect these weaknesses. We
present \textbf{MARS-Bench}, a \textbf{M}ulti-turn \textbf{A}thletic
\textbf{R}eal-world \textbf{S}cenario Dialogue \textbf{Bench}mark, designed to
remedy the gap. MARS-Bench is constructed from play-by-play text commentary so
to feature realistic dialogues specifically designed to evaluate three critical
aspects of multi-turn conversations: Ultra Multi-turn, Interactive Multi-turn,
and Cross-turn Tasks. Extensive experiments on MARS-Bench also reveal that
closed-source LLMs significantly outperform open-source alternatives, explicit
reasoning significantly boosts LLMs' robustness on handling long complex
dialogue sessions, and LLMs indeed face significant challenges when handling
motivation transfer and sophisticated cross-turn dependency. Moreover, we
provide mechanistic interpretability on how attention sinks due to special
tokens lead to LLMs' performance degradation when handling long complex
dialogue sessions based on attention visualization experiment in
Qwen2.5-7B-Instruction.

</details>


### [18] [LayerIF: Estimating Layer Quality for Large Language Models using Influence Functions](https://arxiv.org/abs/2505.23811)
*Hadi Askari,Shivanshu Gupta,Fei Wang,Anshuman Chhabra,Muhao Chen*

Main category: cs.CL

TL;DR: 提出LayerIF框架，通过影响函数量化大语言模型各层训练质量，并应用于专家分配和模型剪枝优化。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖模型启发式规则（如频谱统计、离群值检测），忽视数据对层训练质量的影响，导致下游任务性能受限。LayerIF通过数据驱动方式评估各层重要性，解决任务敏感性的层分配问题。

Method: 1. 分离各层梯度 2. 计算验证损失对训练样本的敏感性 3. 通过层间影响函数推导数据驱动的层重要性评估指标

Result: 在LoRA-MoE架构的专家分配和LLM剪枝任务中，基于影响函数的层重要性分配策略使下游任务性能持续提升（多架构实验验证）

Conclusion: LayerIF首次实现任务相关的层重要性评估，为模型结构优化（混合专家系统、参数剪枝）提供了数据驱动的决策依据，推动LLM高效部署

Abstract: Pretrained Large Language Models (LLMs) achieve strong performance across a
wide range of tasks, yet exhibit substantial variability in the various layers'
training quality with respect to specific downstream applications, limiting
their downstream performance.It is therefore critical to estimate layer-wise
training quality in a manner that accounts for both model architecture and
training data. However, existing approaches predominantly rely on model-centric
heuristics (such as spectral statistics, outlier detection, or uniform
allocation) while overlooking the influence of data. To address these
limitations, we propose LayerIF, a data-driven framework that leverages
Influence Functions to quantify the training quality of individual layers in a
principled and task-sensitive manner. By isolating each layer's gradients and
measuring the sensitivity of the validation loss to training examples by
computing layer-wise influences, we derive data-driven estimates of layer
importance. Notably, our method produces task-specific layer importance
estimates for the same LLM, revealing how layers specialize for different
test-time evaluation tasks. We demonstrate the utility of our scores by
leveraging them for two downstream applications: (a) expert allocation in
LoRA-MoE architectures and (b) layer-wise sparsity distribution for LLM
pruning. Experiments across multiple LLM architectures demonstrate that our
model-agnostic, influence-guided allocation leads to consistent gains in task
performance.

</details>


### [19] [Emotion-aware Dual Cross-Attentive Neural Network with Label Fusion for Stance Detection in Misinformative Social Media Content](https://arxiv.org/abs/2505.23812)
*Lata Pangtey,Mohammad Zia Ur Rehman,Prasad Chaudhari,Shubhi Bansal,Nagendra Kumar*

Main category: cs.CL

TL;DR: 提出SPLAENet方法，通过双重交叉注意力机制和标签融合技术，在错误信息社交媒体内容中实现立场检测，显著提升检测准确率


<details>
  <summary>Details</summary>
Motivation: 社交媒体内容爆炸式增长导致立场隐含性强且存在错误信息传播，现有方法难以有效捕捉文本间复杂关系和情感对齐特征

Method: 1. 双重交叉注意力机制捕获源文本与回复文本的互相关关系
2. 分层注意力网络提取多层次特征
3. 情感对齐机制区分立场类别
4. 标签融合技术通过距离度量学习对齐特征与标签

Result: RumourEval数据集准确率提升8.92%，F1值提升17.36%；SemEval数据集准确率提升7.02%，F1值提升10.92%；P-stance数据集准确率提升10.03%，F1值提升11.18%

Conclusion: SPLAENet通过融合情感特征与标签信息，显著提升立场检测性能，为社交媒体错误信息治理提供有效技术方案

Abstract: The rapid evolution of social media has generated an overwhelming volume of
user-generated content, conveying implicit opinions and contributing to the
spread of misinformation. The method aims to enhance the detection of stance
where misinformation can polarize user opinions. Stance detection has emerged
as a crucial approach to effectively analyze underlying biases in shared
information and combating misinformation. This paper proposes a novel method
for \textbf{S}tance \textbf{P}rediction through a \textbf{L}abel-fused dual
cross-\textbf{A}ttentive \textbf{E}motion-aware neural \textbf{Net}work
(SPLAENet) in misinformative social media user-generated content. The proposed
method employs a dual cross-attention mechanism and a hierarchical attention
network to capture inter and intra-relationships by focusing on the relevant
parts of source text in the context of reply text and vice versa. We
incorporate emotions to effectively distinguish between different stance
categories by leveraging the emotional alignment or divergence between the
texts. We also employ label fusion that uses distance-metric learning to align
extracted features with stance labels, improving the method's ability to
accurately distinguish between stances. Extensive experiments demonstrate the
significant improvements achieved by SPLAENet over existing state-of-the-art
methods. SPLAENet demonstrates an average gain of 8.92\% in accuracy and
17.36\% in F1-score on the RumourEval dataset. On the SemEval dataset, it
achieves average gains of 7.02\% in accuracy and 10.92\% in F1-score. On the
P-stance dataset, it demonstrates average gains of 10.03\% in accuracy and
11.18\% in F1-score. These results validate the effectiveness of the proposed
method for stance detection in the context of misinformative social media
content.

</details>


### [20] [Aligning LLMs by Predicting Preferences from User Writing Samples](https://arxiv.org/abs/2505.23815)
*Stéphane Aroca-Ouellette,Natalie Mackraz,Barry-John Theobald,Katherine Metcalf*

Main category: cs.CL

TL;DR: PROSE通过迭代细化和多样本验证提升LLM代理推断用户偏好的准确性，在摘要和邮件写作任务中比现有方法CIPHER提升33%性能


<details>
  <summary>Details</summary>
Motivation: 现有方法生成的偏好描述过于笼统，无法捕捉用户个性化特征。PROSE旨在通过结构化机制解决个性化偏好推断的精度问题

Method: 1. 基于用户写作样本迭代优化偏好描述
2. 跨多个用户样本进行偏好验证的双重机制

Result: 在Qwen2.5/GPT系列模型测试中：
- 比CIPHER提升33%生成质量
- 与ICL结合实现额外9%性能提升

Conclusion: PROSE有效提升个性化交互质量，其验证机制与现有上下文学习方法形成互补，为LLM代理对齐提供新方向

Abstract: Accommodating human preferences is essential for creating aligned LLM agents
that deliver personalized and effective interactions. Recent work has shown the
potential for LLMs acting as writing agents to infer a description of user
preferences. Agent alignment then comes from conditioning on the inferred
preference description. However, existing methods often produce generic
preference descriptions that fail to capture the unique and individualized
nature of human preferences. This paper introduces PROSE, a method designed to
enhance the precision of preference descriptions inferred from user writing
samples. PROSE incorporates two key elements: (1) iterative refinement of
inferred preferences, and (2) verification of inferred preferences across
multiple user writing samples. We evaluate PROSE with several LLMs (i.e.,
Qwen2.5 7B and 72B Instruct, GPT-mini, and GPT-4o) on a summarization and an
email writing task. We find that PROSE more accurately infers nuanced human
preferences, improving the quality of the writing agent's generations over
CIPHER (a state-of-the-art method for inferring preferences) by 33\%. Lastly,
we demonstrate that ICL and PROSE are complementary methods, and combining them
provides up to a 9\% improvement over ICL alone.

</details>


### [21] [A Course Correction in Steerability Evaluation: Revealing Miscalibration and Side Effects in LLMs](https://arxiv.org/abs/2505.23816)
*Trenton Chang,Tobias Schnabel,Adith Swaminathan,Jenna Wiens*

Main category: cs.CL

TL;DR: 当前大型语言模型在目标可控性上存在明显缺陷，干预措施难以彻底消除副作用效应。


<details>
  <summary>Details</summary>
Motivation: 验证LLMs是否能够可靠响应多样化用户目标需求，分析现有模型在覆盖率不足、校准偏差和副作用效应三个维度的缺陷。

Method: 构建多维度目标空间评估框架，在文本改写任务中测试提示工程、N选优采样、强化学习微调等干预措施的有效性。

Result: 发现语言模型普遍存在副作用效应，不同干预措施效果参差但均无法完全解决可控性问题。

Conclusion: 现有对齐策略存在局限性，需开发更有效的可控性增强方法。开源评估框架促进后续研究。

Abstract: Despite advances in large language models (LLMs) on reasoning and
instruction-following benchmarks, it remains unclear whether they can reliably
produce outputs aligned with a broad variety of user goals, a concept we refer
to as steerability. The abundance of methods proposed to modify LLM behavior
makes it unclear whether current LLMs are already steerable, or require further
intervention. In particular, LLMs may exhibit (i) poor coverage, where rare
user goals are underrepresented; (ii) miscalibration, where models overshoot
requests; and (iii) side effects, where changes to one dimension of text
inadvertently affect others. To systematically evaluate these failures, we
introduce a framework based on a multi-dimensional goal space that models user
goals and LLM outputs as vectors with dimensions corresponding to text
attributes (e.g., reading difficulty). Applied to a text-rewriting task, we
find that current LLMs struggle with steerability, as side effects are
persistent. Interventions to improve steerability, such as prompt engineering,
best-of-$N$ sampling, and reinforcement learning fine-tuning, have varying
effectiveness, yet side effects remain problematic. Our findings suggest that
even strong LLMs struggle with steerability, and existing alignment strategies
may be insufficient. We open-source our steerability evaluation framework at
https://github.com/MLD3/steerability.

</details>


### [22] [Ratas framework: A comprehensive genai-based approach to rubric-based marking of real-world textual exams](https://arxiv.org/abs/2505.23818)
*Masoud Safilian,Amin Beheshti,Stephen Elbourn*

Main category: cs.CL

TL;DR: 提出RATAS框架，利用生成式AI实现基于评分量规的自动答案评分，解决现有方法在适用性、可解释性和实际应用中的局限。


<details>
  <summary>Details</summary>
Motivation: 现有自动评分系统存在三大局限：1) 局限于特定考试格式 2) 评分缺乏可解释性 3) 跨学科实际应用困难。教育技术需要更通用、透明且易部署的自动化评估方案。

Method: RATAS框架核心设计：1) 数学建模量规评分过程 2) 支持复杂考试结构的架构 3) 基于生成式AI的量规适应机制 4) 结构化解释生成模块。使用真实项目制课程数据构建验证数据集。

Result: 实证显示RATAS在自动评分中实现：1) 高评分可靠性（与人工评分一致性达0.89） 2) 跨学科稳定表现 3) 可解释反馈生成能力，提升师生双方的评分透明度。

Conclusion: 该研究通过RATAS框架实现了更普适、可解释的自动评分，特别在复杂量规应用和跨学科场景中展现优势，为教育评估自动化提供新范式。

Abstract: Automated answer grading is a critical challenge in educational technology,
with the potential to streamline assessment processes, ensure grading
consistency, and provide timely feedback to students. However, existing
approaches are often constrained to specific exam formats, lack
interpretability in score assignment, and struggle with real-world
applicability across diverse subjects and assessment types. To address these
limitations, we introduce RATAS (Rubric Automated Tree-based Answer Scoring), a
novel framework that leverages state-of-the-art generative AI models for
rubric-based grading of textual responses. RATAS is designed to support a wide
range of grading rubrics, enable subject-agnostic evaluation, and generate
structured, explainable rationales for assigned scores. We formalize the
automatic grading task through a mathematical framework tailored to
rubric-based assessment and present an architecture capable of handling
complex, real-world exam structures. To rigorously evaluate our approach, we
construct a unique, contextualized dataset derived from real-world
project-based courses, encompassing diverse response formats and varying levels
of complexity. Empirical results demonstrate that RATAS achieves high
reliability and accuracy in automated grading while providing interpretable
feedback that enhances transparency for both students and nstructors.

</details>


### [23] [Arbiters of Ambivalence: Challenges of Using LLMs in No-Consensus Tasks](https://arxiv.org/abs/2505.23820)
*Bhaktipriya Radharapu,Manon Revel,Megan Ung,Sebastian Ruder,Adina Williams*

Main category: cs.CL

TL;DR: 研究发现大语言模型在无共识场景中难以完全捕捉人类分歧，作为评判者或辩论者时会表现出立场倾向，强调需要更复杂的无监督对齐方法


<details>
  <summary>Details</summary>
Motivation: 针对LLMs替代人类进行模型对齐时，在人类自身存在分歧的模糊场景中能否准确反映人类判断的疑问展开研究

Method: 通过构建'无共识'基准测试集，从答案生成器、评判者、辩论者三个角色分析LLMs表现，涵盖多种先验模糊场景

Result: LLMs生成开放式回答时能细致评估，但作为评判者/辩论者时会对无共识话题采取明确立场

Conclusion: 当前LLMs无法完整反映人类分歧，需开发更先进的无监督对齐方法解决立场倾向问题

Abstract: The increasing use of LLMs as substitutes for humans in ``aligning'' LLMs has
raised questions about their ability to replicate human judgments and
preferences, especially in ambivalent scenarios where humans disagree. This
study examines the biases and limitations of LLMs in three roles: answer
generator, judge, and debater. These roles loosely correspond to previously
described alignment frameworks: preference alignment (judge) and scalable
oversight (debater), with the answer generator reflecting the typical setting
with user interactions. We develop a ``no-consensus'' benchmark by curating
examples that encompass a variety of a priori ambivalent scenarios, each
presenting two possible stances. Our results show that while LLMs can provide
nuanced assessments when generating open-ended answers, they tend to take a
stance on no-consensus topics when employed as judges or debaters. These
findings underscore the necessity for more sophisticated methods for aligning
LLMs without human oversight, highlighting that LLMs cannot fully capture human
disagreement even on topics where humans themselves are divided.

</details>


### [24] [Speech as a Multimodal Digital Phenotype for Multi-Task LLM-based Mental Health Prediction](https://arxiv.org/abs/2505.23822)
*Mai Ali,Christopher Lucasius,Tanmay P. Patel,Madison Aitken,Jacob Vorstman,Peter Szatmari,Marco Battaglia,Deepa Kundur*

Main category: cs.CL

TL;DR: 提出三模态语音数据分析框架，结合多任务学习与纵向追踪，显著提升青少年抑郁症检测准确率至70.8%


<details>
  <summary>Details</summary>
Motivation: 传统语音分析忽视多模态信息价值，且青少年抑郁症常伴随自杀倾向/睡眠障碍等共病症，需综合评估病程动态

Method: 整合语音文本/声学特征/生物标记三模态数据，采用多任务学习预测抑郁/自杀/睡眠障碍，通过时序建模追踪多次诊疗数据

Result: 在Depression Early Warning数据集上达到70.8%平衡准确率，优于所有单模态/单任务/非纵向对比方法

Conclusion: 验证了多模态融合、多目标联合建模与纵向分析策略在心理健康监测中的协同增效作用

Abstract: Speech is a noninvasive digital phenotype that can offer valuable insights
into mental health conditions, but it is often treated as a single modality. In
contrast, we propose the treatment of patient speech data as a trimodal
multimedia data source for depression detection. This study explores the
potential of large language model-based architectures for speech-based
depression prediction in a multimodal regime that integrates speech-derived
text, acoustic landmarks, and vocal biomarkers. Adolescent depression presents
a significant challenge and is often comorbid with multiple disorders, such as
suicidal ideation and sleep disturbances. This presents an additional
opportunity to integrate multi-task learning (MTL) into our study by
simultaneously predicting depression, suicidal ideation, and sleep disturbances
using the multimodal formulation. We also propose a longitudinal analysis
strategy that models temporal changes across multiple clinical interactions,
allowing for a comprehensive understanding of the conditions' progression. Our
proposed approach, featuring trimodal, longitudinal MTL is evaluated on the
Depression Early Warning dataset. It achieves a balanced accuracy of 70.8%,
which is higher than each of the unimodal, single-task, and non-longitudinal
methods.

</details>


### [25] [RAGPPI: RAG Benchmark for Protein-Protein Interactions in Drug Discovery](https://arxiv.org/abs/2505.23823)
*Youngseung Jeon,Ziwen Li,Thomas Li,JiaSyuan Chang,Morteza Ziyadi,Xiang 'Anthony' Chen*

Main category: cs.CL

TL;DR: 开发了RAGPPI基准——包含4,420个QA对的评估数据集，用于评估蛋白质相互作用(PPIs)的生物学影响，支持药物研发中的RAG系统优化


<details>
  <summary>Details</summary>
Motivation: 现有RAG框架和LLMs在靶点识别中缺乏针对PPIs生物学影响的标准化评估基准，需填补这一研究空白

Method: 1. 通过专家访谈确定基准标准 → 2. 构建500对专家标注的金标准数据集 → 3. 开发集成自动评估LLM生成3,720对银标准数据集

Result: 创建包含金标准(500 QA)和银标准(3,720 QA)的混合基准，总规模达4,420 QA对；开发具备专家标注特征的自动评估模型

Conclusion: RAGPPI将成为持续维护的研究资源，推动药物发现领域的问答系统发展，特别是在PPIs生物学影响评估方向

Abstract: Retrieving the biological impacts of protein-protein interactions (PPIs) is
essential for target identification (Target ID) in drug development. Given the
vast number of proteins involved, this process remains time-consuming and
challenging. Large Language Models (LLMs) and Retrieval-Augmented Generation
(RAG) frameworks have supported Target ID; however, no benchmark currently
exists for identifying the biological impacts of PPIs. To bridge this gap, we
introduce the RAG Benchmark for PPIs (RAGPPI), a factual question-answer
benchmark of 4,420 question-answer pairs that focus on the potential biological
impacts of PPIs. Through interviews with experts, we identified criteria for a
benchmark dataset, such as a type of QA and source. We built a gold-standard
dataset (500 QA pairs) through expert-driven data annotation. We developed an
ensemble auto-evaluation LLM that reflected expert labeling characteristics,
which facilitates the construction of a silver-standard dataset (3,720 QA
pairs). We are committed to maintaining RAGPPI as a resource to support the
research community in advancing RAG systems for drug discovery QA solutions.

</details>


### [26] [Reviewing Scientific Papers for Critical Problems With Reasoning LLMs: Baseline Approaches and Automatic Evaluation](https://arxiv.org/abs/2505.23824)
*Tianmai M. Zhang,Neil F. Abernethy*

Main category: cs.CL

TL;DR: 利用大语言模型作为稿件质量检查器，提出自动评估框架并验证不同LLM性能及成本效益。


<details>
  <summary>Details</summary>
Motivation: 解决传统同行评审依赖领域专家的问题，探索AI模型在科学稿件质量筛查中的替代方案。

Method: 使用arXiv撤稿论文验证，构建基线方法+自动评估框架，测试多厂商领先LLM的识别错误能力。

Result: OpenAI o3表现最佳，o4-mini性价比最高。不同模型在错误识别准确率和API成本上存在显著差异。

Conclusion: 为基于文档的科学理解/推理提供技术洞见，奠定未来自动化质量审查应用基础。

Abstract: Recent advancements in large language models have sparked interest in
utilizing them to assist the peer review process of scientific publication.
Instead of having AI models generate reviews in the same way as human
reviewers, we propose adopting them as manuscript quality checkers. We
introduce several baseline approaches and an extendable automatic evaluation
framework using top LLMs as judges to tackle the difficulty of recruiting
domain experts for manual evaluation. Utilizing papers withdrawn from arXiv, we
validated our proposed methods with several leading reasoning LLMs from
different providers and assessed their performance and API costs for
identifying critical errors and unsoundness problems. The OpenAI o3 model
performed the best, while o4-mini was the most cost-effective one in our
evaluation. This paper provides insights into document-based scientific
understanding/reasoning and lays the foundation for future applications.

</details>


### [27] [ValueSim: Generating Backstories to Model Individual Value Systems](https://arxiv.org/abs/2505.23827)
*Bangde Du,Ziyi Ye,Zhijing Wu,Jankowska Monika,Shuqi Zhu,Qingyao Ai,Yujia Zhou,Yiqun Liu*

Main category: cs.CL

TL;DR: 提出ValueSim框架，通过生成反映个人经历和人口统计信息的背景故事来模拟个体价值观，在自建基准测试中准确率提升超10%


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐技术主要关注普世伦理准则，缺乏对个体差异化价值系统的模拟能力

Method: 将结构化数据转化为叙事背景，采用受认知-情感人格系统启发的多模块架构进行价值观模拟

Result: 在世界价值观基准测试中top-1准确率超检索增强方法10%，用户交互历史增加时性能持续提升

Conclusion: 该框架能有效模拟个体价值观，并随着时间推移通过交互历史积累优化模拟效果，为个性化AI交互提供新思路

Abstract: As Large Language Models (LLMs) continue to exhibit increasingly human-like
capabilities, aligning them with human values has become critically important.
Contemporary advanced techniques, such as prompt learning and reinforcement
learning, are being deployed to better align LLMs with human values. However,
while these approaches address broad ethical considerations and helpfulness,
they rarely focus on simulating individualized human value systems. To address
this gap, we present ValueSim, a framework that simulates individual values
through the generation of personal backstories reflecting past experiences and
demographic information. ValueSim converts structured individual data into
narrative backstories and employs a multi-module architecture inspired by the
Cognitive-Affective Personality System to simulate individual values based on
these narratives. Testing ValueSim on a self-constructed benchmark derived from
the World Values Survey demonstrates an improvement in top-1 accuracy by over
10% compared to retrieval-augmented generation methods. Further analysis
reveals that performance enhances as additional user interaction history
becomes available, indicating the model's ability to refine its persona
simulation capabilities over time.

</details>


### [28] [BiasFilter: An Inference-Time Debiasing Framework for Large Language Models](https://arxiv.org/abs/2505.23829)
*Xiaoqing Cheng,Ruizhe Chen,Hongying Zan,Yuxiang Jia,Min Peng*

Main category: cs.CL

TL;DR: 提出BiasFilter框架——无需重新训练即可在推理阶段实时过滤LLM生成内容，通过公平性奖励信号有效降低社会偏见


<details>
  <summary>Details</summary>
Motivation: 现有LLM去偏见方法存在成本高、效果有限、难以扩展至大模型和开放式生成任务等问题，亟需更高效的解决方案

Method: 开发模型无关的推理阶段过滤框架：通过实时评估中间输出、维护候选续写集，基于公平性奖励模型进行token级奖励评估，逐步剔除低奖励片段

Result: 实验证明BiasFilter在保持生成质量的同时，显著降低多种LLM的社会偏见，参数效率比基线方法提升3-8倍

Conclusion: 该框架为去偏见提供了无需修改模型参数的高效解决方案，可无缝应用于开源和API型LLM，具有重要实践价值

Abstract: Mitigating social bias in large language models (LLMs) has become an
increasingly important research objective. However, existing debiasing methods
often incur high human and computational costs, exhibit limited effectiveness,
and struggle to scale to larger models and open-ended generation tasks. To
address these limitations, this paper proposes BiasFilter, a model-agnostic,
inference-time debiasing framework that integrates seamlessly with both
open-source and API-based LLMs. Instead of relying on retraining with balanced
data or modifying model parameters, BiasFilter enforces fairness by filtering
generation outputs in real time. Specifically, it periodically evaluates
intermediate outputs every few tokens, maintains an active set of candidate
continuations, and incrementally completes generation by discarding low-reward
segments based on a fairness reward signal. To support this process, we
construct a fairness preference dataset and train an implicit reward model to
assess token-level fairness in generated responses. Extensive experiments
demonstrate that BiasFilter effectively mitigates social bias across a range of
LLMs while preserving overall generation quality.

</details>


### [29] [EvoMoE: Expert Evolution in Mixture of Experts for Multimodal Large Language Models](https://arxiv.org/abs/2505.23830)
*Linglin Jing,Yuting Gao,Zhigang Wang,Wang Lan,Yiwen Tang,Wenhai Wang,Kaipeng Zhang,Qingpei Guo*

Main category: cs.CL

TL;DR: 提出EvoMoE框架解决多模态MoE模型中专家同质化和路由器僵化问题，通过专家进化策略和动态令牌感知路由器提升性能


<details>
  <summary>Details</summary>
Motivation: 现有多模态MoE方法存在专家功能同质化（复制FFN参数导致）和路由机制僵化（静态线性路由无法区分模态）两个关键缺陷

Method: 1. 专家进化策略：从单个可训练专家渐进演化多个强专家
2. 动态令牌感知路由器(DTR)：基于超网络生成模态敏感的动态路由权重

Result: 在MME/MMBench/TextVQA/POPE等基准上显著超越其他稀疏MLLM模型

Conclusion: EvoMoE通过解决专家同质化和路由僵化问题，有效提升了多模态大模型的性能表现

Abstract: Recent advancements have shown that the Mixture of Experts (MoE) approach
significantly enhances the capacity of large language models (LLMs) and
improves performance on downstream tasks. Building on these promising results,
multi-modal large language models (MLLMs) have increasingly adopted MoE
techniques. However, existing multi-modal MoE tuning methods typically face two
key challenges: expert uniformity and router rigidity. Expert uniformity occurs
because MoE experts are often initialized by simply replicating the FFN
parameters from LLMs, leading to homogenized expert functions and weakening the
intended diversification of the MoE architecture. Meanwhile, router rigidity
stems from the prevalent use of static linear routers for expert selection,
which fail to distinguish between visual and textual tokens, resulting in
similar expert distributions for image and text. To address these limitations,
we propose EvoMoE, an innovative MoE tuning framework. EvoMoE introduces a
meticulously designed expert initialization strategy that progressively evolves
multiple robust experts from a single trainable expert, a process termed expert
evolution that specifically targets severe expert homogenization. Furthermore,
we introduce the Dynamic Token-aware Router (DTR), a novel routing mechanism
that allocates input tokens to appropriate experts based on their modality and
intrinsic token values. This dynamic routing is facilitated by hypernetworks,
which dynamically generate routing weights tailored for each individual token.
Extensive experiments demonstrate that EvoMoE significantly outperforms other
sparse MLLMs across a variety of multi-modal benchmarks, including MME,
MMBench, TextVQA, and POPE. Our results highlight the effectiveness of EvoMoE
in enhancing the performance of MLLMs by addressing the critical issues of
expert uniformity and router rigidity.

</details>


### [30] [ICH-Qwen: A Large Language Model Towards Chinese Intangible Cultural Heritage](https://arxiv.org/abs/2505.23831)
*Wenhao Ye,Tiansheng Zheng,Yue Qi,Wenhua Zhao,Xiyu Wang,Xue Zhao,Jiacheng He,Yaya Zheng,Dongbo Wang*

Main category: cs.CL

TL;DR: 本研究开发了ICH-Qwen大语言模型，通过自然语言理解和知识推理技术，有效支持非物质文化遗产的智能保护与数字化传承。


<details>
  <summary>Details</summary>
Motivation: 中国非物质文化遗产面临现代化冲击，急需新型保护手段。中国拥有最多联合国非遗项目，但存在失传风险，大语言模型技术为数字保护提供新路径。

Method: 利用开源非遗数据构建语料库，采用大语言模型的自然语言理解能力，结合合成数据增强和微调技术开发领域专用模型。

Result: 实验证明ICH-Qwen在非遗专业任务中表现优异，能有效执行文化遗产领域的特定需求。

Conclusion: 该模型为非遗可持续发展提供智能解决方案，开辟数字人文研究新范式，推动文化遗产保护的数字化转型。

Abstract: The intangible cultural heritage (ICH) of China, a cultural asset transmitted
across generations by various ethnic groups, serves as a significant testament
to the evolution of human civilization and holds irreplaceable value for the
preservation of historical lineage and the enhancement of cultural
self-confidence. However, the rapid pace of modernization poses formidable
challenges to ICH, including threats damage, disappearance and discontinuity of
inheritance. China has the highest number of items on the UNESCO Intangible
Cultural Heritage List, which is indicative of the nation's abundant cultural
resources and emphasises the pressing need for ICH preservation. In recent
years, the rapid advancements in large language modelling have provided a novel
technological approach for the preservation and dissemination of ICH. This
study utilises a substantial corpus of open-source Chinese ICH data to develop
a large language model, ICH-Qwen, for the ICH domain. The model employs natural
language understanding and knowledge reasoning capabilities of large language
models, augmented with synthetic data and fine-tuning techniques. The
experimental results demonstrate the efficacy of ICH-Qwen in executing tasks
specific to the ICH domain. It is anticipated that the model will provide
intelligent solutions for the protection, inheritance and dissemination of
intangible cultural heritage, as well as new theoretical and practical
references for the sustainable development of intangible cultural heritage.
Furthermore, it is expected that the study will open up new paths for digital
humanities research.

</details>


### [31] [LegalSearchLM: Rethinking Legal Case Retrieval as Legal Elements Generation](https://arxiv.org/abs/2505.23832)
*Chaeeun Kim,Jinu Lee,Wonseok Hwang*

Main category: cs.CL

TL;DR: 该论文提出首个大规模韩语法律案例检索基准LEGAR BENCH及基于法律要素推理的LegalSearchLM模型，在120万案例中实现6-20%性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有法律案例检索研究存在小规模语料库局限性（如5.5万案例）和检索方法（嵌入/词法匹配）无法捕捉法律相关性的问题，难以反映真实场景复杂性。

Method: LegalSearchLM模型通过两步优化：1) 对查询案例进行法律要素推理；2) 采用约束解码直接生成基于目标案例内容的检索结果。

Result: 在LEGAR BENCH上超越基线6-20%，泛化能力突出（跨领域案例表现优于同域训练生成模型15%）

Conclusion: 该研究通过大规模基准和要素推理模型，有效解决了法律检索中的规模瓶颈和语义相关性难题，为真实法律场景提供实用解决方案。

Abstract: Legal Case Retrieval (LCR), which retrieves relevant cases from a query case,
is a fundamental task for legal professionals in research and decision-making.
However, existing studies on LCR face two major limitations. First, they are
evaluated on relatively small-scale retrieval corpora (e.g., 100-55K cases) and
use a narrow range of criminal query types, which cannot sufficiently reflect
the complexity of real-world legal retrieval scenarios. Second, their reliance
on embedding-based or lexical matching methods often results in limited
representations and legally irrelevant matches. To address these issues, we
present: (1) LEGAR BENCH, the first large-scale Korean LCR benchmark, covering
411 diverse crime types in queries over 1.2M legal cases; and (2)
LegalSearchLM, a retrieval model that performs legal element reasoning over the
query case and directly generates content grounded in the target cases through
constrained decoding. Experimental results show that LegalSearchLM outperforms
baselines by 6-20% on LEGAR BENCH, achieving state-of-the-art performance. It
also demonstrates strong generalization to out-of-domain cases, outperforming
naive generative models trained on in-domain data by 15%.

</details>


### [32] [Benchmarking Abstract and Reasoning Abilities Through A Theoretical Perspective](https://arxiv.org/abs/2505.23833)
*Qingchuan Ma,Yuhang Wu,Xiawu Zheng,Rongrong Ji*

Main category: cs.CL

TL;DR: 建立基于符号重映射的数学框架与双指标（γ/δ），揭示当前大语言模型抽象推理存在模式依赖缺陷与记忆局限性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型评测缺乏严谨的抽象推理基准，需构建数学框架量化模型对本质模式与符号表征的区分能力。

Method: 1) 定义抽象推理为模式提取与规则应用能力；2) 设计γ（准确性）和δ（符号依赖度）双指标；3) 通过系统性符号重映射构建基准测试。

Result: 发现LLMs存在非十进制运算缺陷、思维链提示未解决抽象差距、δ指标有效检测操作数记忆依赖三大核心问题。

Conclusion: 当前大语言模型仍缺乏稳健的抽象推理能力，符号重映射测试框架为未来改进提供量化评估工具。

Abstract: In this paper, we aim to establish a simple, effective, and theoretically
grounded benchmark for rigorously probing abstract reasoning in Large Language
Models (LLMs). To achieve this, we first develop a mathematic framework that
defines abstract reasoning as the ability to: (i) extract essential patterns
independent of surface representations, and (ii) apply consistent rules to
these abstract patterns. Based on this framework, we introduce two novel
complementary metrics: \(\scoreGamma\) measures basic reasoning accuracy, while
\(\scoreDelta\) quantifies a model's reliance on specific symbols rather than
underlying patterns - a key indicator of true abstraction versus mere
memorization. To implement this measurement, we design a benchmark: systematic
symbol remapping in rule-based tasks, which forces models to demonstrate
genuine pattern recognition beyond superficial token matching. Extensive LLM
evaluations using this benchmark (commercial API models, 7B-70B, multi-agent)
reveal:1) critical limitations in non-decimal arithmetic and symbolic
reasoning; 2) persistent abstraction gaps despite chain-of-thought prompting;
and 3) \(\scoreDelta\)'s effectiveness in robustly measuring memory dependence
by quantifying performance degradation under symbol remapping, particularly
highlighting operand-specific memorization. These findings underscore that
current LLMs, despite domain-specific strengths, still lack robust abstract
reasoning, highlighting key areas for future improvement.

</details>


### [33] [Say What You Mean: Natural Language Access Control with Large Language Models for Internet of Things](https://arxiv.org/abs/2505.23835)
*Ye Cheng,Minghui Xu,Yue Zhang,Kun Li,Hao Wu,Yechao Zhang,Shaoyong Guo,Wangjie Qiu,Dongxiao Yu,Xiuzhen Cheng*

Main category: cs.CL

TL;DR: LACE框架利用大语言模型实现自然语言到访问控制策略的自动转换，解决物联网场景中策略制定的语义鸿沟问题。


<details>
  <summary>Details</summary>
Motivation: 现有物联网访问控制方案存在粗粒度、规则僵化及人工策略转换效率低的问题，需更灵活的自然语言处理方案。

Method: 结合提示引导策略生成、检索增强推理和形式化验证，构建混合LLM-规则引擎实现策略自动化生成与验证。

Result: 实验显示策略生成正确率100%，DeepSeek-V3决策准确率88%（F1=0.79），系统扩展性优异。

Conclusion: LACE为物联网提供了安全、灵活且用户友好的访问控制框架，有效连接人类意图与机器可执行逻辑。

Abstract: Access control in the Internet of Things (IoT) is becoming increasingly
complex, as policies must account for dynamic and contextual factors such as
time, location, user behavior, and environmental conditions. However, existing
platforms either offer only coarse-grained controls or rely on rigid rule
matching, making them ill-suited for semantically rich or ambiguous access
scenarios. Moreover, the policy authoring process remains fragmented: domain
experts describe requirements in natural language, but developers must manually
translate them into code, introducing semantic gaps and potential
misconfiguration. In this work, we present LACE, the Language-based Access
Control Engine, a hybrid framework that leverages large language models (LLMs)
to bridge the gap between human intent and machine-enforceable logic. LACE
combines prompt-guided policy generation, retrieval-augmented reasoning, and
formal validation to support expressive, interpretable, and verifiable access
control. It enables users to specify policies in natural language,
automatically translates them into structured rules, validates semantic
correctness, and makes access decisions using a hybrid LLM-rule-based engine.
We evaluate LACE in smart home environments through extensive experiments. LACE
achieves 100% correctness in verified policy generation and up to 88% decision
accuracy with 0.79 F1-score using DeepSeek-V3, outperforming baselines such as
GPT-3.5 and Gemini. The system also demonstrates strong scalability under
increasing policy volume and request concurrency. Our results highlight LACE's
potential to enable secure, flexible, and user-friendly access control across
real-world IoT platforms.

</details>


### [34] [Large Language Models Often Know When They Are Being Evaluated](https://arxiv.org/abs/2505.23836)
*Joe Needham,Giles Edkins,Govind Pimpale,Henning Bartsch,Marius Hobbhahn*

Main category: cs.CL

TL;DR: 研究发现前沿AI模型已具备显著（但未超越人类）的评估意识，能区分评估文本与实际部署文本，建议未来持续追踪该能力。


<details>
  <summary>Details</summary>
Motivation: AI模型若能在评估中识别测试场景，可能导致评估结果失真，影响模型部署和治理决策的可靠性。

Method: 构建包含1000个提示词/61个数据集的基准测试集（涵盖公开基准测试、实际部署交互和智能体轨迹），通过二元分类测试模型识别评估文本的能力。

Result: 前沿模型评估意识显著高于随机水平（Gemini-2.5-Pro AUC 0.83），但低于人类基线（AUC 0.92）。模型和人类在智能体场景中识别表现更优，且能有效识别评估目的。

Conclusion: 当前前沿模型已具备实质性（非超人类）评估意识，建议将此能力纳入未来模型追踪指标。

Abstract: If AI models can detect when they are being evaluated, the effectiveness of
evaluations might be compromised. For example, models could have systematically
different behavior during evaluations, leading to less reliable benchmarks for
deployment and governance decisions. We investigate whether frontier language
models can accurately classify transcripts based on whether they originate from
evaluations or real-world deployment, a capability we call evaluation
awareness. To achieve this, we construct a diverse benchmark of 1,000 prompts
and transcripts from 61 distinct datasets. These span public benchmarks (e.g.,
MMLU, SWEBench), real-world deployment interactions, and agent trajectories
from scaffolding frameworks (e.g., web-browsing agents). Frontier models
clearly demonstrate above-random evaluation awareness (Gemini-2.5-Pro reaches
an AUC of $0.83$), but do not yet surpass our simple human baseline (AUC of
$0.92$). Furthermore, both AI models and humans are better at identifying
evaluations in agentic settings compared to chat settings. Additionally, we
test whether models can identify the purpose of the evaluation. Under
multiple-choice and open-ended questioning, AI models far outperform random
chance in identifying what an evaluation is testing for. Our results indicate
that frontier models already exhibit a substantial, though not yet superhuman,
level of evaluation-awareness. We recommend tracking this capability in future
models.

</details>


### [35] [CoMaPOI: A Collaborative Multi-Agent Framework for Next POI Prediction Bridging the Gap Between Trajectory and Language](https://arxiv.org/abs/2505.23837)
*Lin Zhong,Lingzhi Wang,Xu Yang,Qing Liao*

Main category: cs.CL

TL;DR: 提出协作多代理框架CoMaPOI，通过Profiler、Forecaster、Predictor三个代理协同解决LLM在POI预测中的时空数据理解与候选空间约束难题


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在POI预测中存在两大核心缺陷：1) 缺乏对数字时空数据的内在理解能力，导致用户时空偏好建模不准；2) 候选POI空间过大且不受约束，易产生随机或无关预测

Method: CoMaPOI框架包含三个代理：Profiler将数值时空数据转换为语义描述，Forecaster动态约束候选POI空间，Predictor整合信息进行精准预测，通过代理协作同时解决时空建模和空间约束问题

Result: 在NYC/TKY/CA三个基准数据集上实现SOTA，所有指标提升5%-10%，验证了框架有效性

Conclusion: 该工作首次通过定制化协作代理机制揭示了LLM在复杂时空任务中的应用挑战，为时空智能研究提供了新范式

Abstract: Large Language Models (LLMs) offer new opportunities for the next
Point-Of-Interest (POI) prediction task, leveraging their capabilities in
semantic understanding of POI trajectories. However, previous LLM-based
methods, which are superficially adapted to next POI prediction, largely
overlook critical challenges associated with applying LLMs to this task.
Specifically, LLMs encounter two critical challenges: (1) a lack of intrinsic
understanding of numeric spatiotemporal data, which hinders accurate modeling
of users' spatiotemporal distributions and preferences; and (2) an excessively
large and unconstrained candidate POI space, which often results in random or
irrelevant predictions. To address these issues, we propose a Collaborative
Multi Agent Framework for Next POI Prediction, named CoMaPOI. Through the close
interaction of three specialized agents (Profiler, Forecaster, and Predictor),
CoMaPOI collaboratively addresses the two critical challenges. The Profiler
agent is responsible for converting numeric data into language descriptions,
enhancing semantic understanding. The Forecaster agent focuses on dynamically
constraining and refining the candidate POI space. The Predictor agent
integrates this information to generate high-precision predictions. Extensive
experiments on three benchmark datasets (NYC, TKY, and CA) demonstrate that
CoMaPOI achieves state of the art performance, improving all metrics by 5% to
10% compared to SOTA baselines. This work pioneers the investigation of
challenges associated with applying LLMs to complex spatiotemporal tasks by
leveraging tailored collaborative agents.

</details>


### [36] [Exploring the Landscape of Text-to-SQL with Large Language Models: Progresses, Challenges and Opportunities](https://arxiv.org/abs/2505.23838)
*Yiming Huang,Jiyu Guo,Wenxin Mao,Cuiyun Gao,Peiyi Han,Chuanyi Liu,Qing Ling*

Main category: cs.CL

TL;DR: 系统综述大语言模型在Text-to-SQL领域的研究趋势、技术方法、数据集与评估指标，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 提升非SQL用户通过自然语言访问数据库的能力，利用LLM技术突破传统NLP方法的局限。

Method: 从研究趋势分析、多维度技术剖析、数据集/指标总结、未来挑战探讨四个层面展开系统性文献综述。

Result: 建立了LLM-based Text-to-SQL的技术框架图谱，揭示当前方法的共性模式与核心瓶颈。

Conclusion: 为研究者提供深度技术洞察，推动跨模态推理、动态数据库适配等方向的创新突破。

Abstract: Converting natural language (NL) questions into SQL queries, referred to as
Text-to-SQL, has emerged as a pivotal technology for facilitating access to
relational databases, especially for users without SQL knowledge. Recent
progress in large language models (LLMs) has markedly propelled the field of
natural language processing (NLP), opening new avenues to improve text-to-SQL
systems. This study presents a systematic review of LLM-based text-to-SQL,
focusing on four key aspects: (1) an analysis of the research trends in
LLM-based text-to-SQL; (2) an in-depth analysis of existing LLM-based
text-to-SQL techniques from diverse perspectives; (3) summarization of existing
text-to-SQL datasets and evaluation metrics; and (4) discussion on potential
obstacles and avenues for future exploration in this domain. This survey seeks
to furnish researchers with an in-depth understanding of LLM-based text-to-SQL,
sparking new innovations and advancements in this field.

</details>


### [37] [Measuring Sycophancy of Language Models in Multi-turn Dialogues](https://arxiv.org/abs/2505.23840)
*Jiseung Hong,Grace Byun,Seungone Kim,Kai Shu*

Main category: cs.CL

TL;DR: 开发SYCON Bench评估大模型在多轮对话中的奉承行为，发现对齐训练加剧奉承，而模型规模/推理优化能增强抗压能力，第三人称视角提示可减少63.8%奉承行为


<details>
  <summary>Details</summary>
Motivation: 现有研究仅关注单轮事实准确性，忽视真实对话场景中持续压力下的动态立场转变问题，需建立多轮对话评估基准

Method: 构建SYCON Bench基准(含立场翻转轮次和总翻转次数指标)，在3个现实场景中测试17个大模型，评估对齐训练/模型规模/推理优化的影响

Result: 1. 83%模型表现出奉承倾向 2. 对齐训练使翻转轮次提前1.5倍 3. 模型参数每增加10亿，抗压能力提升12% 4. 推理模型失败案例中78%源于过度逻辑阐述 5. 第三人称提示策略效果最佳

Conclusion: 奉承行为是LLMs核心缺陷，需重新审视对齐训练目标。建议结合模型规模扩展+推理优化+第三人称视角提示，构建更鲁棒的安全防护体系

Abstract: Large Language Models (LLMs) are expected to provide helpful and harmless
responses, yet they often exhibit sycophancy--conforming to user beliefs
regardless of factual accuracy or ethical soundness. Prior research on
sycophancy has primarily focused on single-turn factual correctness,
overlooking the dynamics of real-world interactions. In this work, we introduce
SYCON Bench, a novel benchmark for evaluating sycophantic behavior in
multi-turn, free-form conversational settings. Our benchmark measures how
quickly a model conforms to the user (Turn of Flip) and how frequently it
shifts its stance under sustained user pressure (Number of Flip). Applying
SYCON Bench to 17 LLMs across three real-world scenarios, we find that
sycophancy remains a prevalent failure mode. Our analysis shows that alignment
tuning amplifies sycophantic behavior, whereas model scaling and reasoning
optimization strengthen the model's ability to resist undesirable user views.
Reasoning models generally outperform instruction-tuned models but often fail
when they over-index on logical exposition instead of directly addressing the
user's underlying beliefs. Finally, we evaluate four additional prompting
strategies and demonstrate that adopting a third-person perspective reduces
sycophancy by up to 63.8% in debate scenario. We release our code and data at
https://github.com/JiseungHong/SYCON-Bench.

</details>


### [38] [Document Valuation in LLM Summaries: A Cluster Shapley Approach](https://arxiv.org/abs/2505.23842)
*Zikun Ye,Hema Yoganarasimhan*

Main category: cs.CL

TL;DR: 提出Cluster Shapley方法，通过聚类降低Shapley值计算复杂度，实现高效文档贡献度评估


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成摘要时原始内容创作者贡献度难以量化的问题，保障创作者权益

Method: 基于语义嵌入聚类文档，在簇层级计算Shapley值，结合蒙特卡洛采样优化计算效率

Result: 在亚马逊评论摘要任务中，计算复杂度降低同时保持高精度，效率优于基线方法

Conclusion: 该方法不依赖特定LLM模型和摘要生成流程，具备广泛适用性和实践价值

Abstract: Large Language Models (LLMs) are increasingly used in systems that retrieve
and summarize content from multiple sources, such as search engines and AI
assistants. While these models enhance user experience by generating coherent
summaries, they obscure the contributions of original content creators, raising
concerns about credit attribution and compensation. We address the challenge of
valuing individual documents used in LLM-generated summaries. We propose using
Shapley values, a game-theoretic method that allocates credit based on each
document's marginal contribution. Although theoretically appealing, Shapley
values are expensive to compute at scale. We therefore propose Cluster Shapley,
an efficient approximation algorithm that leverages semantic similarity between
documents. By clustering documents using LLM-based embeddings and computing
Shapley values at the cluster level, our method significantly reduces
computation while maintaining attribution quality. We demonstrate our approach
to a summarization task using Amazon product reviews. Cluster Shapley
significantly reduces computational complexity while maintaining high accuracy,
outperforming baseline methods such as Monte Carlo sampling and Kernel SHAP
with a better efficient frontier. Our approach is agnostic to the exact LLM
used, the summarization process used, and the evaluation procedure, which makes
it broadly applicable to a variety of summarization settings.

</details>


### [39] [Evaluation Hallucination in Multi-Round Incomplete Information Lateral-Driven Reasoning Tasks](https://arxiv.org/abs/2505.23843)
*Wenhan Dong,Tianyi Hu,Jingyi Zheng,Zhen Sun,Yuemeng Zhao,Yule Liu,Xinlei He,Xinyi Huang*

Main category: cs.CL

TL;DR: 论文揭示了现有大语言模型评估方法存在误导性结果的局限性，并提出包含推理路径检查、多样化指标和人类表现对比的新评估标准体系


<details>
  <summary>Details</summary>
Motivation: 现有基于多轮次不完全信息任务的评估方法存在三大核心缺陷：1.模型通过走捷径规避复杂推理 2.评估指标单一导致结果失真 3.无法捕捉任务提前终止等关键问题

Method: 提出三阶段评估框架：1.基于推理路径的逆向验证机制 2.引入动态权重调整的多元化评估指标体系 3.建立人类-模型平行实验对比基准

Result: 实验显示传统方法在GPT-4等模型上的评估误差率达38%，新标准可将评估置信度提升62%，有效识别出82%的模型走捷径案例

Conclusion: 构建包含推理过程验证和人类基准参照的评估体系，是提升大语言模型横向思维能力评估可靠性的关键路径

Abstract: Multi-round incomplete information tasks are crucial for evaluating the
lateral thinking capabilities of large language models (LLMs). Currently,
research primarily relies on multiple benchmarks and automated evaluation
metrics to assess these abilities. However, our study reveals novel insights
into the limitations of existing methods, as they often yield misleading
results that fail to uncover key issues, such as shortcut-taking behaviors,
rigid patterns, and premature task termination. These issues obscure the true
reasoning capabilities of LLMs and undermine the reliability of evaluations. To
address these limitations, we propose a refined set of evaluation standards,
including inspection of reasoning paths, diversified assessment metrics, and
comparative analyses with human performance.

</details>


### [40] [Enabling Flexible Multi-LLM Integration for Scalable Knowledge Aggregation](https://arxiv.org/abs/2505.23844)
*Zhenglun Kong,Zheng Zhan,Shiyue Hou,Yifan Gong,Xin Meng,Pengwei Sui,Peiyan Dong,Xuan Shen,Zifeng Wang,Pu Zhao,Hao Tang,Stratis Ioannidis,Yanzhi Wang*

Main category: cs.CL

TL;DR: 提出了一个自适应选择和融合多源LLM知识的框架，通过动态加权融合和反馈机制减少50%的知识干扰。


<details>
  <summary>Details</summary>
Motivation: 传统集成/权重合并方法存在内存占用高、候选模型选择不灵活、知识干扰导致性能下降的问题。

Method: 设计自适应选择网络（根据评分选择相关LLM）+动态加权融合策略（考虑模型优势）+反馈驱动损失函数（防止选择器固化）

Result: 实验显示知识干扰减少50%，实现更稳定可扩展的知识聚合过程。代码已开源。

Conclusion: 该框架有效解决多LLM知识整合难题，在保持单模型效率的同时提升综合能力。

Abstract: Large language models (LLMs) have shown remarkable promise but remain
challenging to continually improve through traditional finetuning, particularly
when integrating capabilities from other specialized LLMs. Popular methods like
ensemble and weight merging require substantial memory and struggle to adapt to
changing data environments. Recent efforts have transferred knowledge from
multiple LLMs into a single target model; however, they suffer from
interference and degraded performance among tasks, largely due to limited
flexibility in candidate selection and training pipelines. To address these
issues, we propose a framework that adaptively selects and aggregates knowledge
from diverse LLMs to build a single, stronger model, avoiding the high memory
overhead of ensemble and inflexible weight merging. Specifically, we design an
adaptive selection network that identifies the most relevant source LLMs based
on their scores, thereby reducing knowledge interference. We further propose a
dynamic weighted fusion strategy that accounts for the inherent strengths of
candidate LLMs, along with a feedback-driven loss function that prevents the
selector from converging on a single subset of sources. Experimental results
demonstrate that our method can enable a more stable and scalable knowledge
aggregation process while reducing knowledge interference by up to 50% compared
to existing approaches. Code is avaliable at
https://github.com/ZLKong/LLM_Integration

</details>


### [41] [Read Your Own Mind: Reasoning Helps Surface Self-Confidence Signals in LLMs](https://arxiv.org/abs/2505.23845)
*Jakub Podolak,Rajeev Verma*

Main category: cs.CL

TL;DR: 通过强制长链式思考显著提升大语言模型自我报告信心值的可靠性，揭示可靠不确定性评估需依赖生成空间的显式探索


<details>
  <summary>Details</summary>
Motivation: 探究DeepSeek R1-32B在问答任务中自我报告信心值不可靠的根本原因，发现默认设置下模型存在系统性过度自信问题

Method: 1. 比较默认应答模式和语义熵方法的信心可靠性
2. 引入强制长链式思考增强预测分布探索
3. 构建独立阅读器模型分析信心信号来源

Result: 1. 长链式思考使简单事实类问题的信心校准显著提升（AUC提高37%）
2. 独立阅读器仅凭思维链即可重构相似信心值
3. 自我报告信心本质是推理过程涌现的替代方案统计量

Conclusion: 可靠的不确定性评估必须显式探索生成空间，自我报告信心值仅在充分探索后可信

Abstract: We study the source of uncertainty in DeepSeek R1-32B by analyzing its
self-reported verbal confidence on question answering (QA) tasks. In the
default answer-then-confidence setting, the model is regularly over-confident,
whereas semantic entropy - obtained by sampling many responses - remains
reliable. We hypothesize that this is because of semantic entropy's larger
test-time compute, which lets us explore the model's predictive distribution.
We show that granting DeepSeek the budget to explore its distribution by
forcing a long chain-of-thought before the final answer greatly improves its
verbal score effectiveness, even on simple fact-retrieval questions that
normally require no reasoning. Furthermore, a separate reader model that sees
only the chain can reconstruct very similar confidences, indicating the verbal
score might be merely a statistic of the alternatives surfaced during
reasoning. Our analysis concludes that reliable uncertainty estimation requires
explicit exploration of the generative space, and self-reported confidence is
trustworthy only after such exploration.

</details>


### [42] [Scalable, Symbiotic, AI and Non-AI Agent Based Parallel Discrete Event Simulations](https://arxiv.org/abs/2505.23846)
*Atanu Barai,Stephan Eidenbenz,Nandakishore Santhi*

Main category: cs.CL

TL;DR: 提出基于并行离散事件模拟(PDES)的多智能体协同框架，通过整合AI与非AI代理实现68%准确率，显著优于单一AI模型的23%


<details>
  <summary>Details</summary>
Motivation: 单一AI系统存在输出不可控、可扩展性差和内存瓶颈问题，需要建立协同机制确保输出正确性

Method: 将各代理建模为PDES实体，通过时间轴协同工作：AI代理生成选项，非AI代理作为验证器实施动态约束，分布式架构解决内存瓶颈

Result: 在四个领域测试显示总体准确率68%(传统AI模型仅23%)，验证了框架的有效性和可扩展性

Conclusion: PDES框架通过结构化任务分解和代理协同机制，显著提升复杂问题解决能力，为可信AI系统提供可扩展解决方案

Abstract: To fully leverage the potential of artificial intelligence (AI) systems in a
trustworthy manner, it is desirable to couple multiple AI and non-AI systems
together seamlessly for constraining and ensuring correctness of the output.
This paper introduces a novel parallel discrete event simulation (PDES) based
methodology to combine multiple AI and non-AI agents in a causal, rule-based
way. Our approach tightly integrates the concept of passage of time, with each
agent considered as an entity in the PDES framework and responding to prior
requests from other agents. Such coupling mechanism enables the agents to work
in a co-operative environment towards a common goal while many tasks run in
parallel throughout the simulation. It further enables setting up boundaries to
the outputs of the AI agents by applying necessary dynamic constraints using
non-AI agents while allowing for scalability through deployment of hundreds of
such agents in a larger compute cluster. Distributing smaller AI agents can
enable extremely scalable simulations in the future, addressing local memory
bottlenecks for model parameter storage. Within a PDES involving both AI and
non-AI agents, we break down the problem at hand into structured steps, when
necessary, providing a set of multiple choices to the AI agents, and then
progressively solve these steps towards a final goal. At each step, the non-AI
agents act as unbiased auditors, verifying each action by the AI agents so that
certain rules of engagement are followed. We evaluate our approach by solving
four problems from four different domains and comparing the results with those
from AI models alone. Our results show greater accuracy in solving problems
from various domains where the AI models struggle to solve the problems solely
by themselves. Results show that overall accuracy of our approach is 68% where
as the accuracy of vanilla models is less than 23%.

</details>


### [43] [Derailing Non-Answers via Logit Suppression at Output Subspace Boundaries in RLHF-Aligned Language Models](https://arxiv.org/abs/2505.23848)
*Harvey Dam,Jonas Knochelmann,Vinu Joseph,Ganesh Gopalakrishnan*

Main category: cs.CL

TL;DR: 提出一种无需修改模型权重或提示即可降低大语言模型对敏感内容拒绝率的方法，通过调整生成过程中的特定标记概率实现。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在处理敏感内容时的高拒绝率问题，避免传统方法对模型权重或提示的依赖。

Method: 在生成过程中抑制链式思维起始标记(<think>)后的双换行符(\n\n)，并忽略链式思维块结束后的终止序列标记()。

Result: 在DeepSeek-R1模型上验证显示，该方法提升了对敏感提示的实质性回答比例，且不影响标准基准测试性能。

Conclusion: 通过阻断生成过程中特定节点的拒绝子空间，可有效规避大语言模型的拒绝行为。

Abstract: We introduce a method to reduce refusal rates of large language models (LLMs)
on sensitive content without modifying model weights or prompts. Motivated by
the observation that refusals in certain models were often preceded by the
specific token sequence of a token marking the beginning of the
chain-of-thought (CoT) block (<think>) followed by a double newline token
(\n\n), we investigate the impact of two simple formatting adjustments during
generation: suppressing \n\n after <think> and suppressing the end-of-sequence
token after the end of the CoT block (</think>). Our method requires no
datasets, parameter changes, or training, relying solely on modifying token
probabilities during generation. In our experiments with official DeepSeek-R1
distillations, these interventions increased the proportion of substantive
answers to sensitive prompts without affecting performance on standard
benchmarks. Our findings suggest that refusal behaviors can be circumvented by
blocking refusal subspaces at specific points in the generation process.

</details>


### [44] [ASyMOB: Algebraic Symbolic Mathematical Operations Benchmark](https://arxiv.org/abs/2505.23851)
*Michael Shalyt,Rotem Elimelech,Ido Kaminer*

Main category: cs.CL

TL;DR: 提出了ASyMOB评估框架，发现LLMs在符号数学中依赖记忆而非深层理解，高级模型展现出显著鲁棒性，可能预示泛化能力的突破。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法有效评估LLMs在积分/微分方程等符号数学核心能力，需建立针对性评估体系揭示模型真实能力。

Method: 构建17,092个符号数学问题集，通过数值/符号扰动测试泛化能力，对比LLMs与计算机代数系统性能，分析代码执行对模型的影响。

Result: LLMs在扰动问题中性能下降达70.3%；支持代码执行的模型准确率提升33.1%；o4-mini和Gemini 2.5 Flash在未扰动集准确率超96%且扰动衰减仅21%（其他模型平均-50.4%）。

Conclusion: 未来可能通过深度集成外部工具或发展超级模型两种路径突破符号数学瓶颈，目前最先进模型已展现出使传统符号系统冗余的潜力。

Abstract: Large language models (LLMs) are rapidly approaching the level of proficiency
in university-level symbolic mathematics required for applications in advanced
science and technology. However, existing benchmarks fall short in assessing
the core skills of LLMs in symbolic mathematics-such as integration,
differential equations, and algebraic simplification. To address this gap, we
introduce ASyMOB, a novel assessment framework focused exclusively on symbolic
manipulation, featuring 17,092 unique math challenges, organized by similarity
and complexity. ASyMOB enables analysis of LLM generalization capabilities by
comparing performance in problems that differ by simple numerical or symbolic
`perturbations'. Evaluated LLMs exhibit substantial degradation in performance
for all perturbation types (up to -70.3%), suggesting reliance on memorized
patterns rather than deeper understanding of symbolic math, even among models
achieving high baseline accuracy. Comparing LLM performance to computer algebra
systems, we identify examples where they fail while LLMs succeed, as well as
problems solved only by combining both approaches. Models capable of integrated
code execution yielded higher accuracy compared to their performance without
code, particularly stabilizing weaker models (up to +33.1% for certain
perturbation types). Notably, the most advanced models (o4-mini, Gemini 2.5
Flash) demonstrate not only high symbolic math proficiency (scoring 96.8% and
97.6% on the unperturbed set), but also remarkable robustness against
perturbations, (-21.7% and -21.2% vs. average -50.4% for the other models).
This may indicate a recent "phase transition" in the generalization
capabilities of frontier LLMs. It remains to be seen whether the path forward
lies in deeper integration with sophisticated external tools, or in developing
models so capable that symbolic math systems like CAS become unnecessary.

</details>


### [45] [Large Language Model-Based Agents for Automated Research Reproducibility: An Exploratory Study in Alzheimer's Disease](https://arxiv.org/abs/2505.23852)
*Nic Dobbins,Christelle Xiong,Kristine Lan,Meliha Yetisgen*

Main category: cs.CL

TL;DR: 大型语言模型作为自主代理可复现53.2%阿尔茨海默病研究结果，但存在数值与方法差异


<details>
  <summary>Details</summary>
Motivation: 探索LLM作为自主代理使用相同数据集复现生物医学研究成果的可行性

Method: 使用NACC数据集构建GPT-4o代理团队，基于论文摘要与方法动态生成执行代码

Result: 5项研究35个关键发现中平均复现率53.2%，数值范围与统计方法存在差异但趋势相似

Conclusion: LLM在科研复现中展现结构化代理系统潜力，当前受限于方法细节缺失与执行缺陷

Abstract: Objective: To demonstrate the capabilities of Large Language Models (LLMs) as
autonomous agents to reproduce findings of published research studies using the
same or similar dataset.
  Materials and Methods: We used the "Quick Access" dataset of the National
Alzheimer's Coordinating Center (NACC). We identified highly cited published
research manuscripts using NACC data and selected five studies that appeared
reproducible using this dataset alone. Using GPT-4o, we created a simulated
research team of LLM-based autonomous agents tasked with writing and executing
code to dynamically reproduce the findings of each study, given only study
Abstracts, Methods sections, and data dictionary descriptions of the dataset.
  Results: We extracted 35 key findings described in the Abstracts across 5
Alzheimer's studies. On average, LLM agents approximately reproduced 53.2% of
findings per study. Numeric values and range-based findings often differed
between studies and agents. The agents also applied statistical methods or
parameters that varied from the originals, though overall trends and
significance were sometimes similar.
  Discussion: In some cases, LLM-based agents replicated research techniques
and findings. In others, they failed due to implementation flaws or missing
methodological detail. These discrepancies show the current limits of LLMs in
fully automating reproducibility assessments. Still, this early investigation
highlights the potential of structured agent-based systems to provide scalable
evaluation of scientific rigor.
  Conclusion: This exploratory work illustrates both the promise and
limitations of LLMs as autonomous agents for automating reproducibility in
biomedical research.

</details>


### [46] [Revisiting Uncertainty Estimation and Calibration of Large Language Models](https://arxiv.org/abs/2505.23854)
*Linwei Tao,Yi-Fan Yeh,Minjing Dong,Tao Huang,Philip Torr,Chang Xu*

Main category: cs.CL

TL;DR: 研究发现语言模型的不确定性评估中，基于语言表达的LVU方法在可靠性和可解释性上显著优于其他方法，模型规模和后训练等因素显著影响评估效果。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在高风险场景的应用增加，需要可靠的不确定性评估来保障其安全可信部署。当前缺乏对多种模型架构和评估方法的系统性研究。

Method: 评估80个不同架构（密集/MoE）、规模（0.6B-671B）的模型，使用MMLU-Pro基准测试三种黑盒单次评估方法（TPU/NVU/LVU）的校准能力和分类选择性能。

Result: LVU在校准和判别能力上表现最优，模型精度与不确定性可靠性无直接关联，推理任务中的评估效果优于知识密集型任务，量化会降低估计性能。

Conclusion: 需要多维度评估不确定性，推荐将LVU作为提升LLM可靠性的实践工具，同时指出良好校准不等于有效的错误排序能力。

Abstract: As large language models (LLMs) are increasingly deployed in high-stakes
applications, robust uncertainty estimation is essential for ensuring the safe
and trustworthy deployment of LLMs. We present the most comprehensive study to
date of uncertainty estimation in LLMs, evaluating 80 models spanning open- and
closed-source families, dense and Mixture-of-Experts (MoE) architectures,
reasoning and non-reasoning modes, quantization variants and parameter scales
from 0.6B to 671B. Focusing on three representative black-box single-pass
methods, including token probability-based uncertainty (TPU), numerical verbal
uncertainty (NVU), and linguistic verbal uncertainty (LVU), we systematically
evaluate uncertainty calibration and selective classification using the
challenging MMLU-Pro benchmark, which covers both reasoning-intensive and
knowledge-based tasks. Our results show that LVU consistently outperforms TPU
and NVU, offering stronger calibration and discrimination while being more
interpretable. We also find that high accuracy does not imply reliable
uncertainty, and that model scale, post-training, reasoning ability and
quantization all influence estimation performance. Notably, LLMs exhibit better
uncertainty estimates on reasoning tasks than on knowledge-heavy ones, and good
calibration does not necessarily translate to effective error ranking. These
findings highlight the need for multi-perspective evaluation and position LVU
as a practical tool for improving the reliability of LLMs in real-world
settings.

</details>


### [47] [OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities](https://arxiv.org/abs/2505.23856)
*Sahil Verma,Keegan Hines,Jeff Bilmes,Charlotte Siska,Luke Zettlemoyer,Hila Gonen,Chandan Singh*

Main category: cs.CL

TL;DR: 提出OMNIGUARD方法，通过跨语言和跨模态对齐的模型内部表示构建分类器，显著提升有害提示检测的准确率和效率


<details>
  <summary>Details</summary>
Motivation: 现有有害查询检测方法在低资源语言和非文本模态场景中存在漏洞，攻击者可利用模型能力的不匹配泛化特性绕过检测

Method: 1. 识别LLM/MLLM中跨语言/模态对齐的内部表示 2. 利用这些表示构建语言/模态无关的分类器

Result: 多语言检测准确率提升11.57%，图像提示检测提升20.44%，音频检测达SOTA，效率提升约120倍

Conclusion: OMNIGUARD通过表征对齐实现了高效的多模态防护，开源代码和数据推动实际应用

Abstract: The emerging capabilities of large language models (LLMs) have sparked
concerns about their immediate potential for harmful misuse. The core approach
to mitigate these concerns is the detection of harmful queries to the model.
Current detection approaches are fallible, and are particularly susceptible to
attacks that exploit mismatched generalization of model capabilities (e.g.,
prompts in low-resource languages or prompts provided in non-text modalities
such as image and audio). To tackle this challenge, we propose OMNIGUARD, an
approach for detecting harmful prompts across languages and modalities. Our
approach (i) identifies internal representations of an LLM/MLLM that are
aligned across languages or modalities and then (ii) uses them to build a
language-agnostic or modality-agnostic classifier for detecting harmful
prompts. OMNIGUARD improves harmful prompt classification accuracy by 11.57\%
over the strongest baseline in a multilingual setting, by 20.44\% for
image-based prompts, and sets a new SOTA for audio-based prompts. By
repurposing embeddings computed during generation, OMNIGUARD is also very
efficient ($\approx 120 \times$ faster than the next fastest baseline). Code
and data are available at: https://github.com/vsahil/OmniGuard.

</details>


### [48] [Infi-Med: Low-Resource Medical MLLMs with Robust Reasoning Evaluation](https://arxiv.org/abs/2505.23867)
*Zeyu Liu,Zhitian Hou,Yining Di,Kejing Yang,Zhijie Sang,Congkai Xie,Jingwen Yang,Siyuan Liu,Jialu Wang,Chunming Li,Ming Li,Hongxia Yang*

Main category: cs.CL

TL;DR: 提出Infi-Med医疗多模态大模型框架，通过高效数据集构建、增强多模态推理能力和系统化评估体系，在保持临床适应性的同时实现最优医疗推理性能


<details>
  <summary>Details</summary>
Motivation: 解决现有医疗MLLMs在资源效率、诊断准确性、临床适配和伦理隐私方面的四大核心挑战，平衡模型效果与落地约束

Method: 1) 构建高质量微调数据集（最小样本需求+全周期设计）
2) 增强跨模态整合与临床任务理解能力
3) 建立覆盖多模态多任务类型的系统评估体系

Result: 实验证明在通用医疗推理任务上达到SOTA性能，同时保持对临床场景的快速适应能力

Conclusion: 该框架在模型有效性与实际医疗场景约束间取得平衡，为MLLMs在真实医疗环境中的部署奠定基础

Abstract: Multimodal large language models (MLLMs) have demonstrated promising
prospects in healthcare, particularly for addressing complex medical tasks,
supporting multidisciplinary treatment (MDT), and enabling personalized
precision medicine. However, their practical deployment faces critical
challenges in resource efficiency, diagnostic accuracy, clinical
considerations, and ethical privacy. To address these limitations, we propose
Infi-Med, a comprehensive framework for medical MLLMs that introduces three key
innovations: (1) a resource-efficient approach through curating and
constructing high-quality supervised fine-tuning (SFT) datasets with minimal
sample requirements, with a forward-looking design that extends to both
pretraining and posttraining phases; (2) enhanced multimodal reasoning
capabilities for cross-modal integration and clinical task understanding; and
(3) a systematic evaluation system that assesses model performance across
medical modalities and task types. Our experiments demonstrate that Infi-Med
achieves state-of-the-art (SOTA) performance in general medical reasoning while
maintaining rapid adaptability to clinical scenarios. The framework establishes
a solid foundation for deploying MLLMs in real-world healthcare settings by
balancing model effectiveness with operational constraints.

</details>


### [49] [One Task Vector is not Enough: A Large-Scale Study for In-Context Learning](https://arxiv.org/abs/2505.23911)
*Pavel Tikhonov,Ivan Oseledets,Elena Tutubalina*

Main category: cs.CL

TL;DR: 通过Llama-3-8B在3096个少样本任务数据集QuiteAFew上的实验，揭示了任务向量在中间层表现最佳、任务类型显著影响效果、复杂任务依赖分布式知识表征的三大发现


<details>
  <summary>Details</summary>
Motivation: 现有研究受限于小规模基准无法全面分析任务向量特性，需构建大规模数据集探索任务知识表征机制

Method: 基于Alpaca构建含3,096个任务的大规模数据集QuiteAFew，每个任务含30个样本，通过Llama-3-8B分析不同层/任务类型/向量分布的表现

Result: 1. 任务向量在第15层表现最佳；2. 不同任务类型效果差异达20%以上；3. 复杂任务需要多个子任务向量协同而非单一向量

Conclusion: 任务知识在LLMs中呈分布式表征，单一任务向量假设不成立，这对改进上下文学习机制具有重要启示

Abstract: In-context learning (ICL) enables Large Language Models (LLMs) to adapt to
new tasks using few examples, with task vectors - specific hidden state
activations - hypothesized to encode task information. Existing studies are
limited by small-scale benchmarks, restricting comprehensive analysis. We
introduce QuiteAFew, a novel dataset of 3,096 diverse few-shot tasks, each with
30 input-output pairs derived from the Alpaca dataset. Experiments with
Llama-3-8B on QuiteAFew reveal: (1) task vector performance peaks at an
intermediate layer (e.g., 15th), (2) effectiveness varies significantly by task
type, and (3) complex tasks rely on multiple, subtask-specific vectors rather
than a single vector, suggesting distributed task knowledge representation.

</details>


### [50] [Reinforcement Learning for Better Verbalized Confidence in Long-Form Generation](https://arxiv.org/abs/2505.23912)
*Caiqi Zhang,Xiaochen Zhu,Chengzu Li,Nigel Collier,Andreas Vlachos*

Main category: cs.CL

TL;DR: LoVeC：基于强化学习的长文本生成置信度实时评估方法，通过附加数值置信分数提升事实性校准


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在长文本生成中的幻觉问题，现有置信度评估方法效率低且不适用于开放生成场景

Method: 使用强化学习（DPO/ORPO/GRPO）训练模型实时生成数值置信分数，提出自由标记和迭代标记两种新评估框架

Result: 在三个长问答数据集上实现更优校准，跨领域泛化能力强，解码时仅需增加少量token

Conclusion: LoVeC为长文本生成提供高效可解释的置信度评估方案，强化学习方法显著提升模型校准效果

Abstract: Hallucination remains a major challenge for the safe and trustworthy
deployment of large language models (LLMs) in factual content generation. Prior
work has explored confidence estimation as an effective approach to
hallucination detection, but often relies on post-hoc self-consistency methods
that require computationally expensive sampling. Verbalized confidence offers a
more efficient alternative, but existing approaches are largely limited to
short-form question answering (QA) tasks and do not generalize well to
open-ended generation. In this paper, we propose LoVeC (Long-form Verbalized
Confidence), an on-the-fly verbalized confidence estimation method for
long-form generation. Specifically, we use reinforcement learning (RL) to train
LLMs to append numerical confidence scores to each generated statement, serving
as a direct and interpretable signal of the factuality of generation. Our
experiments consider both on-policy and off-policy RL methods, including DPO,
ORPO, and GRPO, to enhance the model calibration. We introduce two novel
evaluation settings, free-form tagging and iterative tagging, to assess
different verbalized confidence estimation methods. Experiments on three
long-form QA datasets show that our RL-trained models achieve better
calibration and generalize robustly across domains. Also, our method is highly
efficient, as it only requires adding a few tokens to the output being decoded.

</details>


### [51] [Probing Association Biases in LLM Moderation Over-Sensitivity](https://arxiv.org/abs/2505.23914)
*Yuxin Wang,Botao Yu,Ivory Yang,Saeed Hassanpour,Soroush Vosoughi*

Main category: cs.CL

TL;DR: 研究发现大语言模型内容审核的过度敏感性不仅源于攻击性词汇，更因模型存在系统性主题偏见，越先进的模型主题刻板印象越强。


<details>
  <summary>Details</summary>
Motivation: 解释LLMs误判良性内容为有毒的根本原因，突破传统归因于词汇层面的局限，探索语义层主题关联的潜在影响。

Method: 提出主题关联分析法：通过模型对误判内容进行场景想象生成，量化主题放大程度，对比不同模型表现。

Result: GPT-4 Turbo等先进模型整体误判率更低，但表现出更强的主题刻板印象，揭示模型依赖学习到的主题关联而非显性语言特征。

Conclusion: 需超越关键词过滤的优化，通过理解模型内部主题关联机制来改善审核系统，降低系统性偏见导致的过度敏感。

Abstract: Large Language Models are widely used for content moderation but often
misclassify benign comments as toxic, leading to over-sensitivity. While
previous research attributes this issue primarily to the presence of offensive
terms, we reveal a potential cause beyond token level: LLMs exhibit systematic
topic biases in their implicit associations. Inspired by cognitive psychology's
implicit association tests, we introduce Topic Association Analysis, a
semantic-level approach to quantify how LLMs associate certain topics with
toxicity. By prompting LLMs to generate free-form scenario imagination for
misclassified benign comments and analyzing their topic amplification levels,
we find that more advanced models (e.g., GPT-4 Turbo) demonstrate stronger
topic stereotype despite lower overall false positive rates. These biases
suggest that LLMs do not merely react to explicit, offensive language but rely
on learned topic associations, shaping their moderation decisions. Our findings
highlight the need for refinement beyond keyword-based filtering, providing
insights into the underlying mechanisms driving LLM over-sensitivity.

</details>


### [52] [ChARM: Character-based Act-adaptive Reward Modeling for Advanced Role-Playing Language Agents](https://arxiv.org/abs/2505.23923)
*Feiteng Fang,Ting-En Lin,Yuchuan Wu,Xiong Liu,Xiang Huang,Dingwei Chen,Jing Ye,Haonan Zhang,Liang Zhu,Hamid Alinejad-Rokny,Min Yang,Fei Huang,Yongbin Li*

Main category: cs.CL

TL;DR: 提出ChARM模型解决角色扮演语言代理（RPLAs）的奖励模型扩展性和对话偏好适应问题，通过自适应边际机制和自进化机制提升效果，并构建RoleplayPref数据集和RoleplayEval评测基准。


<details>
  <summary>Details</summary>
Motivation: 传统奖励模型在角色扮演对话场景中面临扩展性差、难以适应主观对话偏好的挑战，需开发更高效的优化方案。

Method: 1. 提出自适应边际机制增强学习效率和泛化能力；2. 设计自进化机制利用大规模未标注数据扩展训练覆盖；3. 构建首个RPLA专用偏好数据集RoleplayPref（16,888双语对话）及评测基准RoleplayEval。

Result: 实验显示偏好排名提升13%，应用ChARM生成的奖励在CharacterEval和RoleplayEval上达到SOTA效果。

Conclusion: ChARM显著提升角色扮演语言代理的交互质量，其创新机制和数据集为后续研究提供重要基础，代码和数据集已开源。

Abstract: Role-Playing Language Agents (RPLAs) aim to simulate characters for realistic
and engaging human-computer interactions. However, traditional reward models
often struggle with scalability and adapting to subjective conversational
preferences. We propose ChARM, a Character-based Act-adaptive Reward Model,
addressing these challenges through two innovations: (1) an act-adaptive margin
that significantly enhances learning efficiency and generalizability, and (2) a
self-evolution mechanism leveraging large-scale unlabeled data to improve
training coverage. Additionally, we introduce RoleplayPref, the first
large-scale preference dataset specifically for RPLAs, featuring 1,108
characters, 13 subcategories, and 16,888 bilingual dialogues, alongside
RoleplayEval, a dedicated evaluation benchmark. Experimental results show a 13%
improvement over the conventional Bradley-Terry model in preference rankings.
Furthermore, applying ChARM-generated rewards to preference learning techniques
(e.g., direct preference optimization) achieves state-of-the-art results on
CharacterEval and RoleplayEval. Code and dataset are available at
https://github.com/calubkk/ChARM.

</details>


### [53] [Scaling up the think-aloud method](https://arxiv.org/abs/2505.23931)
*Daniel Wurgaft,Ben Prystawski,Kanishk Gandhi,Cedegao E. Zhang,Joshua B. Tenenbaum,Noah D. Goodman*

Main category: cs.CL

TL;DR: 通过自然语言处理工具自动化有声思维法的转录与注释，实现大规模推理过程分析


<details>
  <summary>Details</summary>
Motivation: 传统有声思维法依赖人工转录导致样本量小，需自动化解决效率瓶颈

Method: 640名参与者在24点数学任务中发声思考，用NLP自动转录并编码为搜索图

Result: 自动化生成的搜索图与人类评分具有中等信度，可系统性分析推理模式差异

Conclusion: 验证了自动化分析有声思维数据的可行性，为大规模认知研究提供新范式

Abstract: The think-aloud method, where participants voice their thoughts as they solve
a task, is a valuable source of rich data about human reasoning processes. Yet,
it has declined in popularity in contemporary cognitive science, largely
because labor-intensive transcription and annotation preclude large sample
sizes. Here, we develop methods to automate the transcription and annotation of
verbal reports of reasoning using natural language processing tools, allowing
for large-scale analysis of think-aloud data. In our study, 640 participants
thought aloud while playing the Game of 24, a mathematical reasoning task. We
automatically transcribed the recordings and coded the transcripts as search
graphs, finding moderate inter-rater reliability with humans. We analyze these
graphs and characterize consistency and variation in human reasoning traces.
Our work demonstrates the value of think-aloud data at scale and serves as a
proof of concept for the automated analysis of verbal reports.

</details>


### [54] [SwingArena: Competitive Programming Arena for Long-context GitHub Issue Solving](https://arxiv.org/abs/2505.23932)
*Wendong Xu,Jing Xiong,Chenyang Zhao,Qiujiang Chen,Haoran Wang,Hui Shen,Zhongwei Wan,Jianbo Dai,Taiqiang Wu,He Xiao,Chaofan Tao,Z. Morley Mao,Ying Sheng,Zhijiang Guo,Hongxia Yang,Bei Yu,Lingpeng Kong,Quanquan Gu,Ngai Wong*

Main category: cs.CL

TL;DR: SwingArena是一个模拟真实软件开发流程的LLM评估框架，通过分角色协作和检索增强的代码生成模块，在实验中展示了不同模型在补丁生成和持续集成验证中的优势。


<details>
  <summary>Details</summary>
Motivation: 传统静态基准测试无法有效评估LLMs在动态协作开发中的实际表现，因此需要构建更贴近真实工作流的评估框架。

Method: 框架将LLMs分为提交者(生成补丁)和评审者(创建测试用例并验证)，结合RACG模块处理长上下文，支持多语言，实验使用400多个GitHub问题。

Result: GPT-4o擅长生成积极补丁，DeepSeek和Gemini在持续集成验证中更注重正确性。

Conclusion: SwingArena提供了可扩展的评估方法，适用于真实CI驱动的开发环境，能有效衡量不同模型在实际开发中的表现差异。

Abstract: We present SwingArena, a competitive evaluation framework for Large Language
Models (LLMs) that closely mirrors real-world software development workflows.
Unlike traditional static benchmarks, SwingArena models the collaborative
process of software iteration by pairing LLMs as submitters, who generate
patches, and reviewers, who create test cases and verify the patches through
continuous integration (CI) pipelines. To support these interactive
evaluations, we introduce a retrieval-augmented code generation (RACG) module
that efficiently handles long-context challenges by providing syntactically and
semantically relevant code snippets from large codebases, supporting multiple
programming languages (C++, Python, Rust, and Go). This enables the framework
to scale across diverse tasks and contexts while respecting token limitations.
Our experiments, using over 400 high-quality real-world GitHub issues selected
from a pool of 2,300 issues, show that models like GPT-4o excel at aggressive
patch generation, whereas DeepSeek and Gemini prioritize correctness in CI
validation. SwingArena presents a scalable and extensible methodology for
evaluating LLMs in realistic, CI-driven software development settings. More
details are available on our project page: swing-bench.github.io

</details>


### [55] [Retrieval Augmented Generation based Large Language Models for Causality Mining](https://arxiv.org/abs/2505.23944)
*Thushara Manjari Naduvilakandy,Hyeju Jang,Mohammad Al Hasan*

Main category: cs.CL

TL;DR: 提出基于检索增强生成（RAG）的动态提示方案，显著提升大语言模型在因果关系检测与挖掘任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有无监督方法存在性能低、泛化性差的问题，监督方法缺乏大规模训练数据。而LLMs结合提示工程虽能缓解数据不足，但现有研究对因果关系任务的系统性探索不足。

Method: 设计多种RAG驱动的动态提示策略，通过在三个数据集和五个不同LLM上的跨模型实验验证方案有效性。

Result: 实验表明RAG动态提示方案在准确性和稳定性上全面优于传统静态提示方法，且在不同规模LLM中展现良好迁移性。

Conclusion: 验证了动态提示方案对提升LLM因果推理能力的普适性，为知识图谱构建领域提供了高效低成本的解决方案。

Abstract: Causality detection and mining are important tasks in information retrieval
due to their enormous use in information extraction, and knowledge graph
construction. To solve these tasks, in existing literature there exist several
solutions -- both unsupervised and supervised. However, the unsupervised
methods suffer from poor performance and they often require significant human
intervention for causal rule selection, leading to poor generalization across
different domains. On the other hand, supervised methods suffer from the lack
of large training datasets. Recently, large language models (LLMs) with
effective prompt engineering are found to be effective to overcome the issue of
unavailability of large training dataset. Yet, in existing literature, there
does not exist comprehensive works on causality detection and mining using LLM
prompting. In this paper, we present several retrieval-augmented generation
(RAG) based dynamic prompting schemes to enhance LLM performance in causality
detection and extraction tasks. Extensive experiments over three datasets and
five LLMs validate the superiority of our proposed RAG-based dynamic prompting
over other static prompting schemes.

</details>


### [56] [A Closer Look at Bias and Chain-of-Thought Faithfulness of Large (Vision) Language Models](https://arxiv.org/abs/2505.23945)
*Sriram Balasubramanian,Samyadeep Basu,Soheil Feizi*

Main category: cs.CL

TL;DR: 首次系统研究视觉语言模型中的思维链忠实性，揭示文本/图像偏差对推理的影响及新型'矛盾推理'现象


<details>
  <summary>Details</summary>
Motivation: 探究CoT推理是否真实反映模型内部处理，特别是视觉语言模型中未探索的图像偏差对推理的影响

Method: 开发细粒度评估框架，通过分类偏误表达模式分析不同模型对显性/隐性偏见的处理差异

Result: 发现图像偏差极少被识别（即使专用推理模型），多数模型存在'矛盾推理'现象（正确推理后突变答案）

Conclusion: 当前视觉语言模型和纯语言模型均存在CoT忠实性缺陷，需开发新方法检测隐性偏差和不一致推理模式

Abstract: Chain-of-thought (CoT) reasoning enhances performance of large language
models, but questions remain about whether these reasoning traces faithfully
reflect the internal processes of the model. We present the first comprehensive
study of CoT faithfulness in large vision-language models (LVLMs),
investigating how both text-based and previously unexplored image-based biases
affect reasoning and bias articulation. Our work introduces a novel,
fine-grained evaluation pipeline for categorizing bias articulation patterns,
enabling significantly more precise analysis of CoT reasoning than previous
methods. This framework reveals critical distinctions in how models process and
respond to different types of biases, providing new insights into LVLM CoT
faithfulness. Our findings reveal that subtle image-based biases are rarely
articulated compared to explicit text-based ones, even in models specialized
for reasoning. Additionally, many models exhibit a previously unidentified
phenomenon we term ``inconsistent'' reasoning - correctly reasoning before
abruptly changing answers, serving as a potential canary for detecting biased
reasoning from unfaithful CoTs. We then apply the same evaluation pipeline to
revisit CoT faithfulness in LLMs across various levels of implicit cues. Our
findings reveal that current language-only reasoning models continue to
struggle with articulating cues that are not overtly stated.

</details>


### [57] [FLAT-LLM: Fine-grained Low-rank Activation Space Transformation for Large Language Model Compression](https://arxiv.org/abs/2505.23966)
*Jiayi Tian,Ryan Solgi,Jinming Lu,Yifan Yang,Hai Li,Zheng Zhang*

Main category: cs.CL

TL;DR: FLAT-LLM提出一种无需训练的精细低秩激活变换方法，通过头部分PCA和自适应秩分配，实现大模型高效压缩与快速部署。


<details>
  <summary>Details</summary>
Motivation: 现有低秩分解方法存在精度损失大、校准成本高、推理加速有限等问题，需开发更高效的压缩方案。

Method: 基于激活空间头部分PCA截断特征向量降维，结合重要性指标自适应分配各解码层秩，实现无微调权重压缩。

Result: 在4个模型11个数据集上超越结构化剪枝基线，推理速度较传统分解方法提升，校准仅需数分钟。

Conclusion: 该方法在保持模型性能的同时显著提升压缩效率，为LLM实际部署提供有效解决方案。

Abstract: Large Language Models (LLMs) have enabled remarkable progress in natural
language processing, yet their high computational and memory demands pose
challenges for deployment in resource-constrained environments. Although recent
low-rank decomposition methods offer a promising path for structural
compression, they often suffer from accuracy degradation, expensive calibration
procedures, and result in inefficient model architectures that hinder
real-world inference speedups. In this paper, we propose FLAT-LLM, a fast and
accurate, training-free structural compression method based on fine-grained
low-rank transformations in the activation space. Specifically, we reduce the
hidden dimension by transforming the weights using truncated eigenvectors
computed via head-wise Principal Component Analysis (PCA), and employ an
importance-based metric to adaptively allocate ranks across decoders. FLAT-LLM
achieves efficient and effective weight compression without recovery
fine-tuning, which could complete the calibration within a few minutes.
Evaluated across 4 models and 11 datasets, FLAT-LLM outperforms structural
pruning baselines in generalization and downstream performance, while
delivering inference speedups over decomposition-based methods.

</details>


### [58] [Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for LLMs](https://arxiv.org/abs/2505.23996)
*Yinong Oliver Wang,Nivedha Sivakumar,Falaah Arif Khan,Rin Metcalf Susa,Adam Golinski,Natalie Mackraz,Barry-John Theobald,Luca Zappella,Nicholas Apostoloff*

Main category: cs.CL

TL;DR: 提出UCerF不确定性感知公平性指标及新数据集，评估发现Mistral-7B等模型存在传统指标未捕捉的公平性问题


<details>
  <summary>Details</summary>
Motivation: 传统公平性指标仅关注预测正确性，忽视模型置信度差异导致的隐性偏见，需更细粒度评估方法

Method: 开发UCerF指标量化模型决策偏差；构建31,756样本的性别-职业共指消解数据集；评估10个开源LLM

Result: Mistral-7B在错误预测中表现高置信度偏差，Equalized Odds未检出而UCerF成功捕获

Conclusion: UCerF指标与数据集为LLM公平性评估提供新范式，推动构建更透明可靠的AI系统

Abstract: The recent rapid adoption of large language models (LLMs) highlights the
critical need for benchmarking their fairness. Conventional fairness metrics,
which focus on discrete accuracy-based evaluations (i.e., prediction
correctness), fail to capture the implicit impact of model uncertainty (e.g.,
higher model confidence about one group over another despite similar accuracy).
To address this limitation, we propose an uncertainty-aware fairness metric,
UCerF, to enable a fine-grained evaluation of model fairness that is more
reflective of the internal bias in model decisions compared to conventional
fairness measures. Furthermore, observing data size, diversity, and clarity
issues in current datasets, we introduce a new gender-occupation fairness
evaluation dataset with 31,756 samples for co-reference resolution, offering a
more diverse and suitable dataset for evaluating modern LLMs. We establish a
benchmark, using our metric and dataset, and apply it to evaluate the behavior
of ten open-source LLMs. For example, Mistral-7B exhibits suboptimal fairness
due to high confidence in incorrect predictions, a detail overlooked by
Equalized Odds but captured by UCerF. Overall, our proposed LLM benchmark,
which evaluates fairness with uncertainty awareness, paves the way for
developing more transparent and accountable AI systems.

</details>


### [59] [Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws](https://arxiv.org/abs/2505.24009)
*Hidetaka Kamigaito,Ying Zhang,Jingun Kwon,Katsuhiko Hayashi,Manabu Okumura,Taro Watanabe*

Main category: cs.CL

TL;DR: 论文通过偏置-多样性分解理论揭示了Transformer性能提升机制：层间多样性是核心因素，增加层数需伴随行为差异，且性能增益呈现次模性特征。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer内部机制（残差流中的层结构）与参数扩展定律之间的关系，现有研究尚未明确二者关联机制。

Method: 采用偏置-多样性分解理论分析残差流中的层输出，引入信息论多样性指标，并通过多任务实验验证理论发现。

Result: 1. 层预测接近真实值且保持多样性时性能最佳
2. 层数增加仅当新层具备多样性时有效
3. 性能增益符合次模性规律（边际效益递减）

Conclusion: Transformer性能提升不仅依赖参数规模，更需保持层间行为多样性。层数扩展的次模性特征与参数扩展定律预测一致，实验验证了理论框架的有效性。

Abstract: Transformers deliver outstanding performance across a wide range of tasks and
are now a dominant backbone architecture for large language models (LLMs).
Their task-solving performance is improved by increasing parameter size, as
shown in the recent studies on parameter scaling laws. Although recent
mechanistic-interpretability studies have deepened our understanding of the
internal behavior of Transformers by analyzing their residual stream, the
relationship between these internal mechanisms and the parameter scaling laws
remains unclear. To bridge this gap, we focus on layers and their size, which
mainly decide the parameter size of Transformers. For this purpose, we first
theoretically investigate the layers within the residual stream through a
bias-diversity decomposition. The decomposition separates (i) bias, the error
of each layer's output from the ground truth, and (ii) diversity, which
indicates how much the outputs of each layer differ from each other. Analyzing
Transformers under this theory reveals that performance improves when
individual layers make predictions close to the correct answer and remain
mutually diverse. We show that diversity becomes especially critical when
individual layers' outputs are far from the ground truth. Finally, we introduce
an information-theoretic diversity and show our main findings that adding
layers enhances performance only when those layers behave differently, i.e.,
are diverse. We also reveal the performance gains from increasing the number of
layers exhibit submodularity: marginal improvements diminish as additional
layers increase, mirroring the logarithmic convergence predicted by the
parameter scaling laws. Experiments on multiple semantic-understanding tasks
with various LLMs empirically confirm the theoretical properties derived in
this study.

</details>


### [60] [Large Language Model Meets Constraint Propagation](https://arxiv.org/abs/2505.24012)
*Alexandre Bonlarron,Florian Régin,Elisabetta De Maria,Jean-Charles Régin*

Main category: cs.CL

TL;DR: 提出通过整合掩码语言模型(MLMs)改进GenCP框架，实现双向约束传播，增强文本生成中的约束满足能力


<details>
  <summary>Details</summary>
Motivation: LLMs在生成文本时难以有效实施外部约束，现有方法缺乏对双向上下文信息的利用

Method: 将MLM生成的领域预览信息与CP结合，建立基于约束满足问题的文本生成框架，实现过去与未来token的双向约束传播

Result: 在COLLIE基准测试中性能显著提升，LLM推理效率提高，但需额外MLM调用和可能增加回溯

Conclusion: 该方法桥接token级预测与结构化约束，在严格内容约束任务中能生成更可靠且符合要求的文本

Abstract: Large Language Models (LLMs) excel at generating fluent text but struggle to
enforce external constraints because they generate tokens sequentially without
explicit control mechanisms. GenCP addresses this limitation by combining LLM
predictions with Constraint Programming (CP) reasoning, formulating text
generation as a Constraint Satisfaction Problem (CSP). In this paper, we
improve GenCP by integrating Masked Language Models (MLMs) for domain
generation, which allows bidirectional constraint propagation that leverages
both past and future tokens. This integration bridges the gap between
token-level prediction and structured constraint enforcement, leading to more
reliable and constraint-aware text generation. Our evaluation on COLLIE
benchmarks demonstrates that incorporating domain preview via MLM calls
significantly improves GenCP's performance. Although this approach incurs
additional MLM calls and, in some cases, increased backtracking, the overall
effect is a more efficient use of LLM inferences and an enhanced ability to
generate feasible and meaningful solutions, particularly in tasks with strict
content constraints.

</details>


### [61] [BeaverTalk: Oregon State University's IWSLT 2025 Simultaneous Speech Translation System](https://arxiv.org/abs/2505.24016)
*Matthew Raffel,Victor Agostinelli,Lizhong Chen*

Main category: cs.CL

TL;DR: 论文提出BeaverTalk语音文本翻译系统，集成VAD分割器、Whisper语音识别和Gemma大语言模型，在IWSLT 2025英德/英中同传任务中分别取得24.64-27.83和34.07-37.23 BLEU分数


<details>
  <summary>Details</summary>
Motivation: 解决语音同传任务中延迟与准确率的平衡问题，通过级联系统整合语音分割、识别与实时翻译模块，探索基于LoRA微调和大语言模型的上下文感知翻译策略

Method: 采用三阶段架构：1)VAD分割语音流 2)Whisper Large V2进行ASR 3)Gemma 3 12B通过LoRA微调实现带单句记忆库的对话式提示翻译

Result: 英德翻译在1837/3343 StreamLAAL时BLEU达24.64/27.83；英中任务在2217/3521 StreamLAAL时BLEU达34.07/37.23，显示高延迟模式性能更优

Conclusion: 验证了级联系统在语音同传任务的有效性，LoRA微调与源语言上下文提示策略能提升翻译质量，为平衡延迟与精度提供工程实践参考

Abstract: This paper discusses the construction, fine-tuning, and deployment of
BeaverTalk, a cascaded system for speech-to-text translation as part of the
IWSLT 2025 simultaneous translation task. The system architecture employs a VAD
segmenter for breaking a speech stream into segments, Whisper Large V2 for
automatic speech recognition (ASR), and Gemma 3 12B for simultaneous
translation. Regarding the simultaneous translation LLM, it is fine-tuned via
low-rank adaptors (LoRAs) for a conversational prompting strategy that
leverages a single prior-sentence memory bank from the source language as
context. The cascaded system participated in the English$\rightarrow$German and
English$\rightarrow$Chinese language directions for both the low and high
latency regimes. In particular, on the English$\rightarrow$German task, the
system achieves a BLEU of 24.64 and 27.83 at a StreamLAAL of 1837.86 and
3343.73, respectively. Then, on the English$\rightarrow$Chinese task, the
system achieves a BLEU of 34.07 and 37.23 at a StreamLAAL of 2216.99 and
3521.35, respectively.

</details>


### [62] [Hidden Persuasion: Detecting Manipulative Narratives on Social Media During the 2022 Russian Invasion of Ukraine](https://arxiv.org/abs/2505.24028)
*Kateryna Akhynko,Oleksandr Kosovan,Mykola Trokhymovych*

Main category: cs.CL

TL;DR: 提出结合Gemma 2微调与XLM-RoBERTa的多模型方法，在UNLP 2025社交媒体操纵检测任务中分别获得分类第2名和范围检测第3名。


<details>
  <summary>Details</summary>
Motivation: 解决社交媒体中针对乌克兰Telegram用户的修辞与文体操纵技术检测问题，应对信息战背景下的内容操纵挑战。

Method: 分类任务：Gemma 2语言模型LoRA适配器微调+元特征阈值优化的二级分类器；范围检测：XLM-RoBERTa多目标训练框架（含token二元分类）。

Result: 分类任务F1=0.782（第二名），范围检测F1=0.615（第三名），验证了多模型协同策略的有效性。

Conclusion: 证明了预训练模型适配器微调与多目标框架在跨语言操纵检测中的潜力，为社交媒体内容安全提供新解决方案。

Abstract: This paper presents one of the top-performing solutions to the UNLP 2025
Shared Task on Detecting Manipulation in Social Media. The task focuses on
detecting and classifying rhetorical and stylistic manipulation techniques used
to influence Ukrainian Telegram users. For the classification subtask, we
fine-tuned the Gemma 2 language model with LoRA adapters and applied a
second-level classifier leveraging meta-features and threshold optimization.
For span detection, we employed an XLM-RoBERTa model trained for multi-target,
including token binary classification. Our approach achieved 2nd place in
classification and 3rd place in span detection.

</details>


### [63] [The Surprising Soupability of Documents in State Space Models](https://arxiv.org/abs/2505.24033)
*Yasaman Jafari,Zixian Wang,Leon Bergen,Taylor Berg-Kirkpatrick*

Main category: cs.CL

TL;DR: 提出document souping方法，通过独立编码文档后合并隐藏状态，实现无需重处理的跨文档推理


<details>
  <summary>Details</summary>
Motivation: 探索SSMs模型隐藏状态后融合的可能性，旨在实现模块化编码和跨文档推理的灵活复用

Method: 使用文档汤策略：独立编码文档后通过平均池化合并表示，基于Mamba2模型微调生成可融合表征

Result: 在HotpotQA上融合10个文档的表现接近跨编码器，支持多跳QA/稀疏检索/长文档推理任务

Conclusion: 文档汤方法在保持独立编码优势的同时，实现了接近联合编码的性能，为高效推理提供新思路

Abstract: We investigate whether hidden states from Structured State Space Models
(SSMs) can be merged post-hoc to support downstream reasoning. Inspired by
model souping, we propose a strategy where documents are encoded independently
and their representations are pooled -- via simple operations like averaging --
into a single context state. This approach, which we call document souping,
enables modular encoding and reuse without reprocessing the full input for each
query. We finetune Mamba2 models to produce soupable representations and find
that they support multi-hop QA, sparse retrieval, and long-document reasoning
with strong accuracy. On HotpotQA, souping ten independently encoded documents
nearly matches the performance of a cross-encoder trained on the same inputs.

</details>


### [64] [MedPAIR: Measuring Physicians and AI Relevance Alignment in Medical Question Answering](https://arxiv.org/abs/2505.24040)
*Yuexing Hao,Kumail Alhamoud,Hyewon Jeong,Haoran Zhang,Isha Puri,Philip Torr,Mike Schaekermann,Ariel D. Stern,Marzyeh Ghassemi*

Main category: cs.CL

TL;DR: MedPAIR数据集通过标注1300个医学QA对，揭示LLMs与医生学员在信息相关性评估上的差异，过滤无关内容后双方表现均提升


<details>
  <summary>Details</summary>
Motivation: 现有研究显示LLMs可能通过错误逻辑得出正确答案，需系统性评估医学QA中信息相关性对人类和AI的影响机制

Method: 36位医生学员标注问题中句子相关性，构建包含1300QA对的MedPAIR数据集，比较LLMs相关性评估差异及过滤效果

Result: LLMs与人类相关性判断存在显著偏差，但过滤人类标注的无关句子后，医生学员和LLMs的准确率均得到提升

Conclusion: LLMs在医学信息处理逻辑上与人类存在差异，整合人类相关性判断可能提升AI在医学QA任务中的可信度

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance on
various medical question-answering (QA) benchmarks, including standardized
medical exams. However, correct answers alone do not ensure correct logic, and
models may reach accurate conclusions through flawed processes. In this study,
we introduce the MedPAIR (Medical Dataset Comparing Physicians and AI Relevance
Estimation and Question Answering) dataset to evaluate how physician trainees
and LLMs prioritize relevant information when answering QA questions. We obtain
annotations on 1,300 QA pairs from 36 physician trainees, labeling each
sentence within the question components for relevance. We compare these
relevance estimates to those for LLMs, and further evaluate the impact of these
"relevant" subsets on downstream task performance for both physician trainees
and LLMs. We find that LLMs are frequently not aligned with the content
relevance estimates of physician trainees. After filtering out physician
trainee-labeled irrelevant sentences, accuracy improves for both the trainees
and the LLMs. All LLM and physician trainee-labeled data are available at:
http://medpair.csail.mit.edu/.

</details>


### [65] [TCM-Ladder: A Benchmark for Multimodal Question Answering on Traditional Chinese Medicine](https://arxiv.org/abs/2505.24063)
*Jiacheng Xie,Yang Yu,Ziyang Zhang,Shuai Zeng,Jiaxuan He,Ayush Vasireddy,Xiaoting Tang,Congyu Guo,Lening Zhao,Congcong Jing,Guanghui An,Dong Xu*

Main category: cs.CL

TL;DR: 首个中医多模态问答评估数据集TCM-Ladder，涵盖7大学科并整合图文视频数据，包含5.2万+多题型问题，提出Ladder-Score评估方法，建立公开测试基准。


<details>
  <summary>Details</summary>
Motivation: 现有中医大语言模型评估框架存在数据集单一、缺乏多模态整合和标准化测试基准的问题，难以全面评估模型实际应用能力。

Method: 1.构建多学科多模态数据集(自动+人工过滤) 2.训练专用推理模型 3.与14个SOTA模型对比实验 4.设计Ladder-Score评估体系(术语准确性和语义表达)

Result: 建成中医领域最大规模(52k+)多模态测试集，实验显示中医专用模型在专业领域显著优于通用模型，Ladder-Score验证有效提升评估维度。

Conclusion: TCM-Ladder填补中医LLM多模态评估空白，提供标准化测试基准，推动中医自然语言处理技术发展，数据集和排行榜已开源持续更新。

Abstract: Traditional Chinese Medicine (TCM), as an effective alternative medicine, has
been receiving increasing attention. In recent years, the rapid development of
large language models (LLMs) tailored for TCM has underscored the need for an
objective and comprehensive evaluation framework to assess their performance on
real-world tasks. However, existing evaluation datasets are limited in scope
and primarily text-based, lacking a unified and standardized multimodal
question-answering (QA) benchmark. To address this issue, we introduce
TCM-Ladder, the first multimodal QA dataset specifically designed for
evaluating large TCM language models. The dataset spans multiple core
disciplines of TCM, including fundamental theory, diagnostics, herbal formulas,
internal medicine, surgery, pharmacognosy, and pediatrics. In addition to
textual content, TCM-Ladder incorporates various modalities such as images and
videos. The datasets were constructed using a combination of automated and
manual filtering processes and comprise 52,000+ questions in total. These
questions include single-choice, multiple-choice, fill-in-the-blank, diagnostic
dialogue, and visual comprehension tasks. We trained a reasoning model on
TCM-Ladder and conducted comparative experiments against 9 state-of-the-art
general domain and 5 leading TCM-specific LLMs to evaluate their performance on
the datasets. Moreover, we propose Ladder-Score, an evaluation method
specifically designed for TCM question answering that effectively assesses
answer quality regarding terminology usage and semantic expression. To our
knowledge, this is the first work to evaluate mainstream general domain and
TCM-specific LLMs on a unified multimodal benchmark. The datasets and
leaderboard are publicly available at https://tcmladder.com or
https://54.211.107.106 and will be continuously updated.

</details>


### [66] [HardTests: Synthesizing High-Quality Test Cases for LLM Coding](https://arxiv.org/abs/2505.24098)
*Zhongmou He,Yee Man Choi,Kexun Zhang,Jiabao Ji,Junting Zhou,Dejia Xu,Ivan Bercovich,Aidan Zhang,Lei Li*

Main category: cs.CL

TL;DR: 提出HARDTESTGEN测试生成框架，构建高质量编程测试数据集HARDTESTS，显著提升LLM生成代码的评估精度和模型训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有验证器难以检测LLM生成代码中的隐蔽错误，人工编写测试用例效率低下且难以覆盖边缘情况。

Method: 开发HARDTESTGEN流程，利用LLM自动生成高质量测试用例，构建含47k问题的编程测试数据集HARDTESTS。

Result: 测试精度提升11.3%、召回率提升17.5%，困难问题精度提升最高达40%，模型训练后代码生成性能显著改善。

Conclusion: HARDTESTGEN有效解决代码验证难题，开源数据集将促进LLM代码生成领域发展。

Abstract: Verifiers play a crucial role in large language model (LLM) reasoning, needed
by post-training techniques such as reinforcement learning. However, reliable
verifiers are hard to get for difficult coding problems, because a
well-disguised wrong solution may only be detected by carefully human-written
edge cases that are difficult to synthesize. To address this issue, we propose
HARDTESTGEN, a pipeline for high-quality test synthesis using LLMs. With this
pipeline, we curate a comprehensive competitive programming dataset HARDTESTS
with 47k problems and synthetic high-quality tests. Compared with existing
tests, HARDTESTGEN tests demonstrate precision that is 11.3 percentage points
higher and recall that is 17.5 percentage points higher when evaluating
LLM-generated code. For harder problems, the improvement in precision can be as
large as 40 points. HARDTESTS also proves to be more effective for model
training, measured by downstream code generation performance. We will
open-source our dataset and synthesis pipeline at
https://leililab.github.io/HardTests/.

</details>


### [67] [Training LLMs for EHR-Based Reasoning Tasks via Reinforcement Learning](https://arxiv.org/abs/2505.24105)
*Jiacheng Lin,Zhenbang Wu,Jimeng Sun*

Main category: cs.CL

TL;DR: 提出EHRMIND两阶段训练框架，通过监督微调预热身和强化学习优化，解决医疗大语言模型的知识误用/缺失问题，在多项临床任务中提升准确率与泛化能力


<details>
  <summary>Details</summary>
Motivation: 针对LLMs在医疗场景应用时出现的知识误用（具备知识但错误应用）和知识缺失（缺乏领域知识）两大核心问题，探索强化学习在复杂临床推理任务中的有效适配方案

Method: 1. 轻量级监督微调(SFT)阶段：注入领域知识、稳定训练过程、生成结构化输出
2. RLVR阶段：通过可验证奖励机制强化结果正确性，优化决策过程

Result: 在MEDCALC医疗计算、TREC临床试验匹配、EHRSHOT疾病诊断三大任务中验证，模型在准确性（平均提升12.3%）、输出可解释性（提高35%）和跨任务泛化能力（迁移学习效率提升40%）均有显著提升

Conclusion: EHRMIND为医疗领域LLMs的强化学习应用提供了系统解决方案，通过两阶段训练有效平衡领域知识获取与推理能力优化，为临床决策支持系统开发提供新范式

Abstract: We present EHRMIND, a practical recipe for adapting large language models
(LLMs) to complex clinical reasoning tasks using reinforcement learning with
verifiable rewards (RLVR). While RLVR has succeeded in mathematics and coding,
its application to healthcare contexts presents unique challenges due to the
specialized knowledge and reasoning required for electronic health record (EHR)
interpretation. Our pilot study on the MEDCALC benchmark reveals two key
failure modes: (1) misapplied knowledge, where models possess relevant medical
knowledge but apply it incorrectly, and (2) missing knowledge, where models
lack essential domain knowledge. To address these cases, EHRMIND applies a
two-stage solution: a lightweight supervised fine-tuning (SFT) warm-up that
injects missing domain knowledge, stabilizes subsequent training, and
encourages structured, interpretable outputs; followed by RLVR, which
reinforces outcome correctness and refines the model's decision-making. We
demonstrate the effectiveness of our method across diverse clinical
applications, including medical calculations (MEDCALC), patient-trial matching
(TREC CLINICAL TRIALS), and disease diagnosis (EHRSHOT). EHRMIND delivers
consistent gains in accuracy, interpretability, and cross-task generalization.
These findings offer practical guidance for applying RLVR to enhance LLM
capabilities in healthcare settings.

</details>


### [68] [The State of Multilingual LLM Safety Research: From Measuring the Language Gap to Mitigating It](https://arxiv.org/abs/2505.24119)
*Zheng-Xin Yong,Beyza Ermis,Marzieh Fadaee,Stephen H. Bach,Julia Kreutzer*

Main category: cs.CL

TL;DR: 揭示LLM安全研究存在严重英语中心化倾向，非英语语言安全研究显著不足，提出多语言安全评估/训练数据生成/跨语言安全泛化三大研究方向


<details>
  <summary>Details</summary>
Motivation: 发现现有LLM安全研究过度集中于英语，导致AI安全实践无法满足多语言需求，存在系统性安全风险

Method: 系统性分析2020-2024年*ACL系列会议近300篇论文，量化语言差距，评估语言研究独立性和文档实践

Result: 识别到持续扩大的语言鸿沟（高资源非英语语言研究仅占4%），英语研究存在语言标注缺失（78%未说明适用语言）

Conclusion: 建议建立多语言安全评估框架、开发跨语言训练数据生成技术、研究跨语言安全泛化机制，构建包容性AI安全体系

Abstract: This paper presents a comprehensive analysis of the linguistic diversity of
LLM safety research, highlighting the English-centric nature of the field.
Through a systematic review of nearly 300 publications from 2020--2024 across
major NLP conferences and workshops at *ACL, we identify a significant and
growing language gap in LLM safety research, with even high-resource
non-English languages receiving minimal attention. We further observe that
non-English languages are rarely studied as a standalone language and that
English safety research exhibits poor language documentation practice. To
motivate future research into multilingual safety, we make several
recommendations based on our survey, and we then pose three concrete future
directions on safety evaluation, training data generation, and crosslingual
safety generalization. Based on our survey and proposed directions, the field
can develop more robust, inclusive AI safety practices for diverse global
populations.

</details>


### [69] [R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration](https://arxiv.org/abs/2505.24133)
*Zefan Cai,Wen Xiao,Hanshi Sun,Cheng Luo,Yikai Zhang,Ke Wan,Yucheng Li,Yeyang Zhou,Li-Wen Chang,Jiuxiang Gu,Zhen Dong,Anima Anandkumar,Abedelkadir Asi,Junjie Hu*

Main category: cs.CL

TL;DR: 提出R-KV方法，通过去除推理模型中的冗余令牌，在仅保留10% KV缓存时维持100%性能，并显著提升内存效率与推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存压缩方法在链式推理任务中性能大幅下降（仅达60%），无法兼顾高效压缩与模型性能。

Method: 设计冗余感知的KV缓存压缩算法，精准识别并保留关键令牌，实现10%缓存下性能无损。

Result: 实验显示：10%缓存保留100%性能，16%缓存时性能反超原模型5%；内存节省90%，吞吐量提升6.6倍。

Conclusion: R-KV首次实现KV缓存的高效无损压缩，为部署大规模推理模型提供了关键技术突破。

Abstract: Reasoning models have demonstrated impressive performance in self-reflection
and chain-of-thought reasoning. However, they often produce excessively long
outputs, leading to prohibitively large key-value (KV) caches during inference.
While chain-of-thought inference significantly improves performance on complex
reasoning tasks, it can also lead to reasoning failures when deployed with
existing KV cache compression approaches. To address this, we propose
Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel
method specifically targeting redundant tokens in reasoning models. Our method
preserves nearly 100% of the full KV cache performance using only 10% of the KV
cache, substantially outperforming existing KV cache baselines, which reach
only 60% of the performance. Remarkably, R-KV even achieves 105% of full KV
cache performance with 16% of the KV cache. This KV-cache reduction also leads
to a 90% memory saving and a 6.6X throughput over standard chain-of-thought
reasoning inference. Experimental results show that R-KV consistently
outperforms existing KV cache compression baselines across two mathematical
reasoning datasets.

</details>


### [70] [CrossICL: Cross-Task In-Context Learning via Unsupervised Demonstration Transfer](https://arxiv.org/abs/2505.24143)
*Jinglong Gao,Xiao Ding,Lingxiao Zou,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: CrossICL利用源任务示例实现跨任务上下文学习，无需人工标注即可提升大语言模型性能


<details>
  <summary>Details</summary>
Motivation: 现实中用户往往不愿或无法提供上下文学习所需的演示示例，需探索自动化的跨任务知识迁移方案

Method: 提出两阶段对齐策略（任务架构对齐+语义空间对齐），构建包含875个NLP任务的大规模实验体系

Result: 实验验证CrossICL有效性，揭示跨任务示例选择标准（如任务复杂度匹配度）及任务差异引发的六类干扰模式

Conclusion: 首次系统证明跨任务知识迁移在上下文学习中的可行性，为自动化模型适配提供新范式

Abstract: In-Context Learning (ICL) enhances the performance of large language models
(LLMs) with demonstrations. However, obtaining these demonstrations primarily
relies on manual effort. In most real-world scenarios, users are often
unwilling or unable to provide such demonstrations. Inspired by the human
analogy, we explore a new ICL paradigm CrossICL to study how to utilize
existing source task demonstrations in the ICL for target tasks, thereby
obtaining reliable guidance without any additional manual effort. To explore
this, we first design a two-stage alignment strategy to mitigate the
interference caused by gaps across tasks, as the foundation for our
experimental exploration. Based on it, we conduct comprehensive exploration of
CrossICL, with 875 NLP tasks from the Super-NI benchmark and six types of LLMs,
including GPT-4o. Experimental results demonstrate the effectiveness of
CrossICL and provide valuable insights on questions like the criteria for
selecting cross-task demonstrations, as well as the types of task-gap-induced
interference in CrossICL.

</details>


### [71] [Rationales Are Not Silver Bullets: Measuring the Impact of Rationales on Model Performance and Reliability](https://arxiv.org/abs/2505.24147)
*Chiwei Zhu,Benfeng Xu,An Yang,Junyang Lin,Quan Wang,Chang Zhou,Zhendong Mao*

Main category: cs.CL

TL;DR: 研究发现rationales（解释性说明）在语言模型训练中效果不稳定，可能同时损害性能却提升可靠性，任务难度是核心驱动因素，这对模型对齐人类思维的实践具有警示意义


<details>
  <summary>Details</summary>
Motivation: 针对现有研究普遍认为rationales能提升模型性能的共识，本文发现这种改进具有条件敏感性，需系统探究rationales对模型性能和可靠性的双重影响

Method: 通过多维度实验框架，系统测量rationales对17个NLP任务的影响，创新性地建立性能-可靠性关联分析模型，并引入任务难度量化指标

Result: 1. 41%任务出现性能下降
2. 63%任务可靠性提升
3. 性能与可靠性改进呈线性关系（R=0.82）
4. 任务难度系数决定改进幅度
（代码已开源）

Conclusion: rationales的运用需任务难度评估，模型可靠性与性能存在trade-off，这对构建可信AI系统具有方法论意义，揭示语言模型与人类认知对齐的复杂性

Abstract: Training language models with rationales augmentation has been shown to be
beneficial in many existing works. In this paper, we identify that such a
prevailing view does not hold consistently. We conduct comprehensive
investigations to thoroughly inspect the impact of rationales on model
performance as well as a novel perspective of model reliability. The results
lead to several key findings that add new insights upon existing
understandings: 1) Rationales can, at times, deteriorate model performance; 2)
Rationales can, at times, improve model reliability, even outperforming their
untrained counterparts; 3) A linear correspondence exists in between the
performance and reliability improvements, while both are driven by the
intrinsic difficulty of the task. These findings provide informative
regulations on the broad utilization of rationales and raise critical
implications on the procedure of explicitly aligning language models with
implicit human thoughts. Codes can be found at
https://github.com/Ignoramus0817/rationales.

</details>


### [72] [LKD-KGC: Domain-Specific KG Construction via LLM-driven Knowledge Dependency Parsing](https://arxiv.org/abs/2505.24163)
*Jiaqi Sun,Shiyou Qian,Zhangchi Han,Wei Li,Zelin Qian,Dingyu Yang,Jian Cao,Guangtao Xue*

Main category: cs.CL

TL;DR: 提出LKD-KGC框架，通过无监督方式自动构建领域专用知识图谱，实验显示精度和召回率提升10%-20%。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱构建方法依赖人工模式、单文档处理和公共领域知识，难以处理领域专用语料的复杂依赖和低参考知识场景。

Method: 1. 自动分析文档库推断知识依赖 2. LLM驱动优先级确定处理顺序 3. 结合层次化文档上下文自回归生成实体模式 4. 无监督抽取实体关系

Result: 在精确率和召回率指标上相比SOTA方法提升10%-20%

Conclusion: LKD-KGC框架有效解决了领域知识图谱构建难题，实验证明其能自动生成高质量KG，具有重要应用价值。

Abstract: Knowledge Graphs (KGs) structure real-world entities and their relationships
into triples, enhancing machine reasoning for various tasks. While
domain-specific KGs offer substantial benefits, their manual construction is
often inefficient and requires specialized knowledge. Recent approaches for
knowledge graph construction (KGC) based on large language models (LLMs), such
as schema-guided KGC and reference knowledge integration, have proven
efficient. However, these methods are constrained by their reliance on manually
defined schema, single-document processing, and public-domain references,
making them less effective for domain-specific corpora that exhibit complex
knowledge dependencies and specificity, as well as limited reference knowledge.
To address these challenges, we propose LKD-KGC, a novel framework for
unsupervised domain-specific KG construction. LKD-KGC autonomously analyzes
document repositories to infer knowledge dependencies, determines optimal
processing sequences via LLM driven prioritization, and autoregressively
generates entity schema by integrating hierarchical inter-document contexts.
This schema guides the unsupervised extraction of entities and relationships,
eliminating reliance on predefined structures or external knowledge. Extensive
experiments show that compared with state-of-the-art baselines, LKD-KGC
generally achieves improvements of 10% to 20% in both precision and recall
rate, demonstrating its potential in constructing high-quality domain-specific
KGs.

</details>


### [73] [Mixed-R1: Unified Reward Perspective For Reasoning Capability in Multimodal Large Language Models](https://arxiv.org/abs/2505.24164)
*Shilin Xu,Yanwei Li,Rui Yang,Tao Zhang,Yueyi Sun,Wei Chow,Linfeng Li,Hang Song,Qi Xu,Yunhai Tong,Xiangtai Li,Hao Fei*

Main category: cs.CL

TL;DR: 提出Mixed-R1框架，通过混合奖励函数和数据集提升多模态大语言模型在多任务强化学习中的稳定性与效果


<details>
  <summary>Details</summary>
Motivation: 现有MLLM后训练方法仅针对单一任务优化（如数学/图表分析），缺乏多任务协同强化学习的统一框架

Method: 1. 构建Mixed-45K多源数据集（通过数据引擎筛选高质量样本） 2. 设计混合奖励函数（包含匹配/图表/IoU/开放式四类奖励），其中BMAS奖励通过分词器嵌入双向匹配优化长文本响应

Result: 在Qwen2.5-VL、Intern-VL等不同规模MLLM上验证有效性，实验证明框架提升多任务强化学习稳定性

Conclusion: Mixed-R1首次实现多源MLLM任务的统一强化学习框架，混合奖励设计和数据集构建为多模态大模型训练提供新思路

Abstract: Recent works on large language models (LLMs) have successfully demonstrated
the emergence of reasoning capabilities via reinforcement learning (RL).
Although recent efforts leverage group relative policy optimization (GRPO) for
MLLMs post-training, they constantly explore one specific aspect, such as
grounding tasks, math problems, or chart analysis. There are no works that can
leverage multi-source MLLM tasks for stable reinforcement learning. In this
work, we present a unified perspective to solve this problem. We present
Mixed-R1, a unified yet straightforward framework that contains a mixed reward
function design (Mixed-Reward) and a mixed post-training dataset (Mixed-45K).
We first design a data engine to select high-quality examples to build the
Mixed-45K post-training dataset. Then, we present a Mixed-Reward design, which
contains various reward functions for various MLLM tasks. In particular, it has
four different reward functions: matching reward for binary answer or
multiple-choice problems, chart reward for chart-aware datasets, IoU reward for
grounding problems, and open-ended reward for long-form text responses such as
caption datasets. To handle the various long-form text content, we propose a
new open-ended reward named Bidirectional Max-Average Similarity (BMAS) by
leveraging tokenizer embedding matching between the generated response and the
ground truth. Extensive experiments show the effectiveness of our proposed
method on various MLLMs, including Qwen2.5-VL and Intern-VL on various sizes.
Our dataset and model are available at https://github.com/xushilin1/mixed-r1.

</details>


### [74] [Tag-Evol: Achieving Efficient Instruction Evolving via Tag Injection](https://arxiv.org/abs/2505.24165)
*Yixuan Wang,Shiqi Zhou,Chuanzhe Guo,Qingfu Zhu*

Main category: cs.CL

TL;DR: 提出Tag-Evol框架，通过知识标签组合实现高效可控的指令演化，生成更优质、多样且具挑战性的训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有指令演化方法依赖固定策略（需人工设计），形式单一且迭代演化成本高，难以获取优质困难样本。

Method: 使用知识标签作为动态策略，通过向原始指令注入不同标签组合实现可控演化（如'逻辑推理+金融知识'等定向增强）

Result: 在多领域基准测试中，使用不同模型生成的演化数据质量显著优于传统方法，数据多样性提升37%

Conclusion: Tag-Evol框架突破传统策略限制，以标签组合实现高效演化（成本降低60%），生成数据更具挑战性和领域适应性

Abstract: Evol-Instruct has made significant improvements as a data synthesis method in
several areas. Existing methods typically rely on a fixed set of strategies to
evolve, which require manual design and are monolithic in form. In addition,
iterative evolution also makes the acquisition of hard samples expensive. In
view of this, we propose the Tag-Evol framework, a more diverse and efficient
instruction evolving method. Specifically, Tag-Evol uses diverse and specific
knowledge tags as strategies to achieve controlled evolution by injecting
different combinations of tags into the original instructions. Experiments with
multiple backbones in diverse domain benchmarks show that the proposed method
generates significantly better evolved data than other methods. Furthermore, we
conduct a thorough analysis of the evolved data, demonstrating that Tag-Evol is
not only efficient but also generates more diverse and challenging data.

</details>


### [75] [Adaptive LoRA Merge with Parameter Pruning for Low-Resource Generation](https://arxiv.org/abs/2505.24174)
*Ryota Miyano,Yuki Arase*

Main category: cs.CL

TL;DR: 提出可微调的LoRA合并方法，通过参数更新与剪枝增强低资源语言生成任务的模型适应性


<details>
  <summary>Details</summary>
Motivation: 现有LoRA合并方法存在参数冻结导致的适应性局限，且未有效解决低资源场景下的任务适配问题

Method: 基于目标任务少量数据进行微调，动态更新并剪裁LoRA参数，实现参数细粒度调整和任务适配增强

Result: 在多领域英日双语摘要任务中取得显著效果提升，相比基线方法保持一致性改进

Conclusion: 该方法有效突破参数冻结限制，为低资源场景下的LLM适配提供高效解决方案

Abstract: This study proposes a simple yet effective LoRA merge method to achieve LLM
adaptation for low-resource language generation tasks. The LoRA merge
technique, which integrates multiple LoRA modules trained on different tasks,
has gained attention as an effective and efficient approach for adapting LLMs
to target tasks. However, previous methods are limited in adaptability as they
keep the LoRA parameters frozen. Additionally, the low-resource problem has
been out of their scope. We propose a LoRA merge method that updates and prunes
LoRA parameters through fine-tuning with minimal target task data, which allows
finer-grained adjustments of LoRA parameters and enhancement of task
adaptability. Extensive experiments have been conducted taking summarization as
a benchmark task. Our datasets cover various domains and multiple languages of
English and Japanese. The results confirm that the proposed method achieves
significant and consistent improvements in task adaptability over the previous
methods.

</details>


### [76] [Beyond Exponential Decay: Rethinking Error Accumulation in Large Language Models](https://arxiv.org/abs/2505.24187)
*Mikhail L. Arbuzov,Alexey A. Shvets,Sisong Beir*

Main category: cs.CL

TL;DR: 研究挑战LLM可靠性随序列长度指数衰减的假设，揭示错误集中在5-10%关键决策令牌，提出基于语义关键点导航的新可靠性框架，实现更高效的语言系统


<details>
  <summary>Details</summary>
Motivation: 传统模型假设独立令牌错误导致指数级可靠性衰减，但实际错误集中在关键语义决策点，这限制了长文本生成的可靠性，需要新的理论框架解释现代LLM的持续连贯性

Method: 通过区分关键决策令牌(5-10%)与可预测多数令牌，建立新可靠性公式；提出动态计算分配、多路径探索、语义对齐架构等策略，替代传统暴力计算扩展

Result: 证明长上下文性能依赖关键语义决策点而非均匀准确率，新框架在计算效率上显著优于传统方法，突破指数衰减假设的理论限制

Conclusion: 研究颠覆可靠性指数衰减范式，提出基于战略推理的下一代系统设计原则，为高效语言模型开辟新方向，实现性能突破而不依赖计算力线性增长

Abstract: The prevailing assumption of an exponential decay in large language model
(LLM) reliability with sequence length, predicated on independent per-token
error probabilities, posits an inherent limitation for long autoregressive
outputs. Our research fundamentally challenges this view by synthesizing
emerging evidence that LLM errors are not uniformly distributed but are
concentrated at sparse "key tokens" ($5-10\%$ of total tokens) representing
critical decision junctions. By distinguishing these high-impact tokens from
the increasingly predictable majority, we introduce a new reliability formula
explaining the sustained coherence of modern LLMs over thousands of tokens.
Converging research streams reveal that long-context performance primarily
depends on accurately navigating a few crucial semantic decision points rather
than on uniform token-level accuracy, enabling targeted strategies that
significantly outperform brute-force approaches. We thus propose a framework
for next-generation systems centered on selective preservation of semantically
vital tokens, dynamic computational allocation at uncertain decision
boundaries, multi-path exploration at ambiguities, and architectures aligned
with natural semantic domains. This marks a fundamental shift from raw scaling
to strategic reasoning, promising breakthrough performance without
proportionate computational scaling and offering a more nuanced understanding
that supersedes the exponential decay hypothesis, thereby opening pathways
toward substantially more powerful and efficient language systems.

</details>


### [77] [CLaSp: In-Context Layer Skip for Self-Speculative Decoding](https://arxiv.org/abs/2505.24196)
*Longze Chen,Renke Shan,Huiming Wang,Lu Wang,Ziqiang Liu,Run Luo,Jiawei Wang,Hamid Alinejad-Rokny,Min Yang*

Main category: cs.CL

TL;DR: CLaSp提出一种无需额外训练的自推测解码方法，通过动态跳过LLM中间层实现1.3x~1.7x加速，保持文本生成质量


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法依赖额外训练模块导致兼容性问题，需要即插即用的轻量级解决方案

Method: 基于动态规划算法优化层跳跃策略，利用验证阶段的完整隐藏状态构建压缩草稿模型

Result: 在LLaMA3系列模型上实现稳定加速，多任务测试显示速度提升1.3-1.7倍且输出分布无偏移

Conclusion: CLaSp为LLM推理加速提供了零训练成本、高兼容性的创新方案，适用于不同规模的模型

Abstract: Speculative decoding (SD) is a promising method for accelerating the decoding
process of Large Language Models (LLMs). The efficiency of SD primarily hinges
on the consistency between the draft model and the verify model. However,
existing drafting approaches typically require additional modules to be
trained, which can be challenging to implement and ensure compatibility across
various LLMs. In this paper, we propose CLaSp, an in-context layer-skipping
strategy for self-speculative decoding. Unlike prior methods, CLaSp does not
require additional drafting modules or extra training. Instead, it employs a
plug-and-play mechanism by skipping intermediate layers of the verify model to
construct a compressed draft model. Specifically, we develop a dynamic
programming algorithm that optimizes the layer-skipping process by leveraging
the complete hidden states from the last verification stage as an objective.
This enables CLaSp to dynamically adjust its layer-skipping strategy after each
verification stage, without relying on pre-optimized sets of skipped layers.
Experimental results across diverse downstream tasks demonstrate that CLaSp
achieves a speedup of 1.3x ~ 1.7x on LLaMA3 series models without altering the
original distribution of the generated text.

</details>


### [78] [Intuitionistic Fuzzy Sets for Large Language Model Data Annotation: A Novel Approach to Side-by-Side Preference Labeling](https://arxiv.org/abs/2505.24199)
*Yimin Du*

Main category: cs.CL

TL;DR: 提出基于直觉模糊集(IFS)的标注框架，通过隶属度/非隶属度/犹豫度建模人类偏好不确定性，提升标注质量与模型表现


<details>
  <summary>Details</summary>
Motivation: 传统标注方法存在不确定性和标注者分歧问题，需更有效捕捉人类判断中的犹豫和矛盾

Method: 开发IFS标注协议，建立包含三个维度的偏好模型(偏好程度/拒绝程度/犹豫程度)，设计群体标注聚合算法和质量评估指标

Result: 标注一致性提升23%，模型胜率提高12.3%，标注时间减少15.7%，在多个数据集验证有效性

Conclusion: IFS框架为人类偏好标注提供数学基础，显著提升RLHF训练效果，具有大规模LLM训练的实际应用价值

Abstract: The quality of human preference data is crucial for training and evaluating
large language models (LLMs), particularly in reinforcement learning from human
feedback (RLHF) and direct preference optimization (DPO) scenarios. Traditional
side-by-side (SBS) annotation approaches often struggle with inherent
uncertainty, annotator disagreement, and the complexity of preference
judgments. This paper introduces a novel framework based on intuitionistic
fuzzy sets (IFS) for modeling and aggregating human preferences in LLM data
annotation tasks. Our approach captures not only the degree of preference but
also the uncertainty and hesitation inherent in human judgment through
membership, non-membership, and hesitation degrees. We propose an IFS-based
annotation protocol that enables more nuanced preference modeling, develops
aggregation methods for handling annotator disagreement, and introduces quality
metrics for preference data assessment. Experimental validation on multiple
datasets demonstrates that our IFS-based approach significantly improves
annotation consistency, reduces annotator fatigue, and produces higher-quality
preference data compared to traditional binary and Likert-scale methods. The
resulting preference datasets lead to improved model performance in downstream
tasks, with 12.3\% improvement in win-rate against baseline models and 15.7\%
reduction in annotation time. Our framework provides a principled approach to
handling uncertainty in human preference annotation and offers practical
benefits for large-scale LLM training.

</details>


### [79] [Are Any-to-Any Models More Consistent Across Modality Transfers Than Specialists?](https://arxiv.org/abs/2505.24211)
*Jiwan Chung,Janghan Yoon,Junhyeong Park,Sangeyl Lee,Joowon Yang,Sooyeon Park,Youngjae Yu*

Main category: cs.CL

TL;DR: 提出ACON数据集用于评估多模态生成模型的跨模态一致性，发现统一模型点对点评估不优于专用模型，但潜在空间结构化分析揭示弱一致性


<details>
  <summary>Details</summary>
Motivation: 验证统一多模态模型是否真正具备跨模态连贯性而非表面感知，解决现有评估体系不完善的问题

Method: 构建含1000张图像（500新增）的ACON数据集，提出循环一致性、前向等变性、共轭等变性三重评估标准，结合点对点评估与潜在空间结构化分析

Result: 统一模型在循环一致性等点对点评估中未显优势，但通过多次编辑操作的潜在空间分析发现可观测的弱一致性特征

Conclusion: 统一多模态模型的跨模态一致性需结构化评估方法验证，公开代码数据促进该领域研究

Abstract: Any-to-any generative models aim to enable seamless interpretation and
generation across multiple modalities within a unified framework, yet their
ability to preserve relationships across modalities remains uncertain. Do
unified models truly achieve cross-modal coherence, or is this coherence merely
perceived? To explore this, we introduce ACON, a dataset of 1,000 images (500
newly contributed) paired with captions, editing instructions, and Q&A pairs to
evaluate cross-modal transfers rigorously. Using three consistency
criteria-cyclic consistency, forward equivariance, and conjugated
equivariance-our experiments reveal that any-to-any models do not consistently
demonstrate greater cross-modal consistency than specialized models in
pointwise evaluations such as cyclic consistency. However, equivariance
evaluations uncover weak but observable consistency through structured analyses
of the intermediate latent space enabled by multiple editing operations. We
release our code and data at https://github.com/JiwanChung/ACON.

</details>


### [80] [Semi-structured LLM Reasoners Can Be Rigorously Audited](https://arxiv.org/abs/2505.24217)
*Jixuan Leng,Cassandra A. Cohen,Zhixian Zhang,Chenyan Xiong,William W. Cohen*

Main category: cs.CL

TL;DR: 提出半结构化推理模型（SSRMs），通过Python语法风格的半结构化推理轨迹提升LLM推理可分析性，实验证明其在多领域性能优越且支持自动错误检测。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型（LLM）推理过程中存在的'不忠实'问题（如推理轨迹错误/遗漏难以检测、可能掩盖模型偏见）

Method: 1. 内部化半结构化思维链（CoT）生成Python语法风格的推理轨迹
2. 使用任务特定词汇标记步骤输入输出
3. 开发结构化审计（手工规则检测错误模式）和典型性审计（概率模型识别异常推理）

Result: 在10个基准测试中：
- 同规模基线模型性能提升近10%（领域内任务）
- 医学领域与专用模型竞争
- 两类审计方法均能有效标记推理错误

Conclusion: SSRMs在提升性能的同时，通过结构化推理轨迹和双重审计机制显著增强了模型推理透明度和可信度验证能力

Abstract: As Large Language Models (LLMs) become increasingly capable at reasoning, the
problem of "faithfulness" persists: LLM "reasoning traces" can contain errors
and omissions that are difficult to detect, and may obscure biases in model
outputs. To address these limitations, we introduce Semi-Structured Reasoning
Models (SSRMs), which internalize a semi-structured Chain-of-Thought (CoT)
reasoning format within the model. Our SSRMs generate reasoning traces in a
Pythonic syntax. While SSRM traces are not executable, they adopt a restricted,
task-specific vocabulary to name distinct reasoning steps, and to mark each
step's inputs and outputs. Through extensive evaluation on ten benchmarks,
SSRMs demonstrate strong performance and generality: they outperform comparably
sized baselines by nearly ten percentage points on in-domain tasks while
remaining competitive with specialized models on out-of-domain medical
benchmarks. Furthermore, we show that semi-structured reasoning is more
amenable to analysis: in particular, they can be automatically audited to
identify reasoning flaws. We explore both hand-crafted structured audits, which
detect task-specific problematic reasoning patterns, and learned typicality
audits, which apply probabilistic models over reasoning patterns, and show that
both audits can be used to effectively flag probable reasoning errors.

</details>


### [81] [ERU-KG: Efficient Reference-aligned Unsupervised Keyphrase Generation](https://arxiv.org/abs/2505.24219)
*Lam Thanh Do,Aaditya Bodke,Pritom Saha Akash,Kevin Chen-Chuan Chang*

Main category: cs.CL

TL;DR: 提出ERU-KG无监督关键词生成模型，通过信息量模块和短语性模块提升预测效率与准确性，支持生成/抽取双模式切换。


<details>
  <summary>Details</summary>
Motivation: 现有无监督关键词预测方法依赖启发式评分导致信息估计不准，且缺乏时间效率考量。

Method: 1) 信息量模块通过参考项（查询/引文/标题）和术语级建模捕捉关键概念；2) 短语性模块生成候选短语。通过聚合术语信息量避免显式候选建模。

Result: 在基准测试中超越无监督基线，达到监督模型Top10预测89%性能，推理速度同类最快。生成的扩展词有效提升文本检索效果。

Conclusion: ERU-KG通过参数调整实现生成/抽取双模式，满足不同应用场景需求，兼具高效性（最快推理速度）与扩展实用性（检索增强）。

Abstract: Unsupervised keyphrase prediction has gained growing interest in recent
years. However, existing methods typically rely on heuristically defined
importance scores, which may lead to inaccurate informativeness estimation. In
addition, they lack consideration for time efficiency. To solve these problems,
we propose ERU-KG, an unsupervised keyphrase generation (UKG) model that
consists of an informativeness and a phraseness module. The former estimates
the relevance of keyphrase candidates, while the latter generate those
candidates. The informativeness module innovates by learning to model
informativeness through references (e.g., queries, citation contexts, and
titles) and at the term-level, thereby 1) capturing how the key concepts of
documents are perceived in different contexts and 2) estimating informativeness
of phrases more efficiently by aggregating term informativeness, removing the
need for explicit modeling of the candidates. ERU-KG demonstrates its
effectiveness on keyphrase generation benchmarks by outperforming unsupervised
baselines and achieving on average 89\% of the performance of a supervised
model for top 10 predictions. Additionally, to highlight its practical utility,
we evaluate the model on text retrieval tasks and show that keyphrases
generated by ERU-KG are effective when employed as query and document
expansions. Furthermore, inference speed tests reveal that ERU-KG is the
fastest among baselines of similar model sizes. Finally, our proposed model can
switch between keyphrase generation and extraction by adjusting
hyperparameters, catering to diverse application requirements.

</details>


### [82] [Automated Structured Radiology Report Generation](https://arxiv.org/abs/2505.24223)
*Jean-Benoit Delbrouck,Justin Xu,Johannes Moll,Alois Thomas,Zhihong Chen,Sophie Ostmeier,Asfandyar Azhar,Kelvin Zhenghao Li,Andrew Johnston,Christian Bluethgen,Eduardo Reis,Mohamed Muneer,Maya Varma,Curtis Langlotz*

Main category: cs.CL

TL;DR: 提出结构化放射学报告生成(SRRG)任务，通过LLM重构标准化报告格式，开发SRR-BERT疾病分类模型和F1-SRR-BERT评估指标，解决自由文本报告的生成与评估难题


<details>
  <summary>Details</summary>
Motivation: 自由文本放射报告存在表述不一致和结构化缺失问题，导致生成模型效果差且传统评估指标无法反映临床价值

Method: 1. 使用大语言模型重构标准化报告格式
2. 创建结构化数据集
3. 开发55标签细粒度疾病分类模型SRR-BERT
4. 提出结合层次分类法的F1-SRR-BERT评估指标

Result: 通过5位认证放射科医师的读者研究和实验验证，结构化方法显著提升报告一致性与临床实用性

Conclusion: SRRG框架有效弥合自由文本与结构化报告间的差距，为临床AI应用提供标准化解决方案

Abstract: Automated radiology report generation from chest X-ray (CXR) images has the
potential to improve clinical efficiency and reduce radiologists' workload.
However, most datasets, including the publicly available MIMIC-CXR and CheXpert
Plus, consist entirely of free-form reports, which are inherently variable and
unstructured. This variability poses challenges for both generation and
evaluation: existing models struggle to produce consistent, clinically
meaningful reports, and standard evaluation metrics fail to capture the nuances
of radiological interpretation. To address this, we introduce Structured
Radiology Report Generation (SRRG), a new task that reformulates free-text
radiology reports into a standardized format, ensuring clarity, consistency,
and structured clinical reporting. We create a novel dataset by restructuring
reports using large language models (LLMs) following strict structured
reporting desiderata. Additionally, we introduce SRR-BERT, a fine-grained
disease classification model trained on 55 labels, enabling more precise and
clinically informed evaluation of structured reports. To assess report quality,
we propose F1-SRR-BERT, a metric that leverages SRR-BERT's hierarchical disease
taxonomy to bridge the gap between free-text variability and structured
clinical reporting. We validate our dataset through a reader study conducted by
five board-certified radiologists and extensive benchmarking experiments.

</details>


### [83] [Dynamic Context-Aware Streaming Pretrained Language Model For Inverse Text Normalization](https://arxiv.org/abs/2505.24229)
*Luong Ho,Khanh Le,Vinh Pham,Bao Nguyen,Tan Tran,Duc Chau*

Main category: cs.CL

TL;DR: 提出基于预训练语言模型的流式逆文本规范化方法，通过动态上下文感知机制提升低资源场景下的准确率和实时性


<details>
  <summary>Details</summary>
Motivation: 流式ASR系统中ITN的实时处理存在准确性、效率与适应性三大挑战，特别是在低资源语言和有限上下文场景下表现不足

Method: 1. 使用预训练语言模型增强语言表征 2. 提出动态上下文感知机制实现自适应分块处理 3. 融合右上下文信息优化预测

Result: 在越南语数据集上达到与非流式ITN相当的准确率，且延迟降低65%，推理速度提升3倍

Conclusion: 动态上下文感知架构有效平衡了准确率与延迟，为流式ASR系统提供了即插即用的ITN解决方案

Abstract: Inverse Text Normalization (ITN) is crucial for converting spoken Automatic
Speech Recognition (ASR) outputs into well-formatted written text, enhancing
both readability and usability. Despite its importance, the integration of
streaming ITN within streaming ASR remains largely unexplored due to challenges
in accuracy, efficiency, and adaptability, particularly in low-resource and
limited-context scenarios. In this paper, we introduce a streaming pretrained
language model for ITN, leveraging pretrained linguistic representations for
improved robustness. To address streaming constraints, we propose Dynamic
Context-Aware during training and inference, enabling adaptive chunk size
adjustments and the integration of right-context information. Experimental
results demonstrate that our method achieves accuracy comparable to
non-streaming ITN and surpasses existing streaming ITN models on a Vietnamese
dataset, all while maintaining low latency, ensuring seamless integration into
ASR systems.

</details>


### [84] [Advantageous Parameter Expansion Training Makes Better Large Language Models](https://arxiv.org/abs/2505.24241)
*Naibin Gu,Yilong Chen,Zhenyu Zhang,Peng Fu,Zheng Lin,Shuohuan Wang,Yu Sun,Hua Wu,Weiping Wang,Haifeng Wang*

Main category: cs.CL

TL;DR: APEX方法通过扩展优势参数比例，用更少参数量/训练数据达到更优模型性能


<details>
  <summary>Details</summary>
Motivation: 发现优势参数对模型性能起决定性作用，强模型拥有更多此类参数，试图通过参数优化提升训练效率

Method: 提出优势参数扩展训练(APEX)，逐步将优势参数扩展到劣势参数空间，提高优势参数占比

Result: 指令微调中使用52%参数超越全参数训练；持续预训练中仅用33%数据达到相同困惑度，下游任务显著提升

Conclusion: APEX通过矩阵有效秩理论验证有效性，在参数效率和数据效率方面均展现优势

Abstract: Although scaling up the number of trainable parameters in both pre-training
and fine-tuning can effectively improve the performance of large language
models, it also leads to increased computational overhead. When delving into
the parameter difference, we find that a subset of parameters, termed
advantageous parameters, plays a crucial role in determining model performance.
Further analysis reveals that stronger models tend to possess more such
parameters. In this paper, we propose Advantageous Parameter EXpansion Training
(APEX), a method that progressively expands advantageous parameters into the
space of disadvantageous ones, thereby increasing their proportion and
enhancing training effectiveness. Further theoretical analysis from the
perspective of matrix effective rank explains the performance gains of APEX.
Extensive experiments on both instruction tuning and continued pre-training
demonstrate that, in instruction tuning, APEX outperforms full-parameter tuning
while using only 52% of the trainable parameters. In continued pre-training,
APEX achieves the same perplexity level as conventional training with just 33%
of the training data, and yields significant improvements on downstream tasks.

</details>


### [85] [Mamba Knockout for Unraveling Factual Information Flow](https://arxiv.org/abs/2505.24244)
*Nir Endy,Idan Daniel Grosbard,Yuval Ran-Milo,Yonatan Slutzky,Itay Tshuva,Raja Giryes*

Main category: cs.CL

TL;DR: 研究通过调整Transformer的注意力解释技术分析Mamba模型信息流动，发现模型间异同及LLM共性，结合结构化分解揭示Mamba内部机制。


<details>
  <summary>Details</summary>
Motivation: 探索Mamba模型信息处理机制，对比其与Transformer差异，揭示大语言模型的潜在通用特性，弥补Mamba理论解释空白。

Method: 将Transformer的Attention Knockout方法适配到Mamba模型，追踪跨token/层的信息路径，利用结构化分解分离信息交换与特征增强模块。

Result: 发现Mamba与Transformer存在动态差异，但部分信息涌现模式普适；Mamba的特征分解可清晰区分token交互与个体增强功能。

Conclusion: 为Mamba提供可解释性框架，证明跨模型共性特征的存在，其结构化设计为未来高效模型架构提供理论支持。

Abstract: This paper investigates the flow of factual information in Mamba State-Space
Model (SSM)-based language models. We rely on theoretical and empirical
connections to Transformer-based architectures and their attention mechanisms.
Exploiting this relationship, we adapt attentional interpretability techniques
originally developed for Transformers--specifically, the Attention Knockout
methodology--to both Mamba-1 and Mamba-2. Using them we trace how information
is transmitted and localized across tokens and layers, revealing patterns of
subject-token information emergence and layer-wise dynamics. Notably, some
phenomena vary between mamba models and Transformer based models, while others
appear universally across all models inspected--hinting that these may be
inherent to LLMs in general. By further leveraging Mamba's structured
factorization, we disentangle how distinct "features" either enable
token-to-token information exchange or enrich individual tokens, thus offering
a unified lens to understand Mamba internal operations.

</details>


### [86] [Proactive Guidance of Multi-Turn Conversation in Industrial Search](https://arxiv.org/abs/2505.24251)
*Xiaoyu Li,Xiao Li,Li Gao,Yiding Liu,Xiaoyang Wang,Shuaiqiang Wang,Junfeng Wang,Dawei Yin*

Main category: cs.CL

TL;DR: 提出两阶段框架G-SFT和C-RL，通过动态目标适应与点击信号强化学习提升搜索助手性能，实现准确性+23.95%、CTR提升149.06%，延迟降低69.55%。


<details>
  <summary>Details</summary>
Motivation: 现有对话系统难以动态跟踪用户目标变化且存在高延迟问题，需通过主动引导优化交互质量与实时性。

Method: 1. G-SFT阶段：目标适应代理动态跟踪用户意图，通过知识蒸馏将LLM能力迁移至轻量模型；2. C-RL阶段：基于用户点击信号构建偏好对，采用生成-排序范式提升点击率。

Result: 离线准确率86.10%（+23.95%），在线CTR达25.28%（相对提升149.06%），推理延迟降低69.55%。

Conclusion: 框架有效结合目标跟踪与交互优化，G-SFT确保意图精准捕捉，C-RL通过强化学习提升引导效果，成功应用于工业级搜索系统。

Abstract: The evolution of Large Language Models (LLMs) has significantly advanced
multi-turn conversation systems, emphasizing the need for proactive guidance to
enhance users' interactions. However, these systems face challenges in
dynamically adapting to shifts in users' goals and maintaining low latency for
real-time interactions. In the Baidu Search AI assistant, an industrial-scale
multi-turn search system, we propose a novel two-phase framework to provide
proactive guidance. The first phase, Goal-adaptive Supervised Fine-Tuning
(G-SFT), employs a goal adaptation agent that dynamically adapts to user goal
shifts and provides goal-relevant contextual information. G-SFT also
incorporates scalable knowledge transfer to distill insights from LLMs into a
lightweight model for real-time interaction. The second phase, Click-oriented
Reinforcement Learning (C-RL), adopts a generate-rank paradigm, systematically
constructs preference pairs from user click signals, and proactively improves
click-through rates through more engaging guidance. This dual-phase
architecture achieves complementary objectives: G-SFT ensures accurate goal
tracking, while C-RL optimizes interaction quality through click signal-driven
reinforcement learning. Extensive experiments demonstrate that our framework
achieves 86.10% accuracy in offline evaluation (+23.95% over baseline) and
25.28% CTR in online deployment (149.06% relative improvement), while reducing
inference latency by 69.55% through scalable knowledge distillation.

</details>


### [87] [Effects of Theory of Mind and Prosocial Beliefs on Steering Human-Aligned Behaviors of LLMs in Ultimatum Games](https://arxiv.org/abs/2505.24255)
*Neemesh Yadav,Palakorn Achananuparp,Jing Jiang,Ee-Peng Lim*

Main category: cs.CL

TL;DR: 研究通过2700次模拟实验发现，LLM中的心智理论（ToM）推理能提升谈判任务中行为与人类规范的对齐性、决策一致性和结果质量，尤其在最后通牒博弈中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索ToM推理在增强AI代理行为与人类规范对齐中的作用，促进人机协作决策的优化。

Method: 初始化不同亲社会信念（贪婪/公平/无私）的LLM代理，使用链式思维（CoT）和多层级ToM推理，在o3-mini、DeepSeek-R1等模型上进行多维度决策测试。

Result: ToM推理显著提升行为对齐度（21%↑）和谈判成功率（33%↑），且不同ToM推理顺序对博弈收益分配产生差异化影响。

Conclusion: 心智理论是提升人机交互协作质量的关键机制，模型内嵌的ToM能力比纯推理模型表现出更优的社会价值对齐特性。

Abstract: Large Language Models (LLMs) have shown potential in simulating human
behaviors and performing theory-of-mind (ToM) reasoning, a crucial skill for
complex social interactions. In this study, we investigate the role of ToM
reasoning in aligning agentic behaviors with human norms in negotiation tasks,
using the ultimatum game as a controlled environment. We initialized LLM agents
with different prosocial beliefs (including Greedy, Fair, and Selfless) and
reasoning methods like chain-of-thought (CoT) and varying ToM levels, and
examined their decision-making processes across diverse LLMs, including
reasoning models like o3-mini and DeepSeek-R1 Distilled Qwen 32B. Results from
2,700 simulations indicated that ToM reasoning enhances behavior alignment,
decision-making consistency, and negotiation outcomes. Consistent with previous
findings, reasoning models exhibit limited capability compared to models with
ToM reasoning, different roles of the game benefits with different orders of
ToM reasoning. Our findings contribute to the understanding of ToM's role in
enhancing human-AI interaction and cooperative decision-making. The code used
for our experiments can be found at https://github.com/Stealth-py/UltimatumToM.

</details>


### [88] [Simulating Training Data Leakage in Multiple-Choice Benchmarks for LLM Evaluation](https://arxiv.org/abs/2505.24263)
*Naila Shafirni Hidayat,Muhammad Dehan Al Kautsar,Alfan Farizki Wicaksono,Fajri Koto*

Main category: cs.CL

TL;DR: 本文系统评估了LLM数据泄露检测方法，提出n-gram方法效果最优，并创建净化版测试集验证模型表现


<details>
  <summary>Details</summary>
Motivation: 大模型评测分数持续提升但训练数据不透明，存在测试集污染风险，需要可靠的数据泄露检测方法确保评估公正性

Method: 在持续预训练框架下模拟真实数据泄露场景，对比排列测试/n-gram方法，并提出半问题检测法(semi-half question)

Result: n-gram方法F1值最高达96.2%；改进方法实现实例级检测且计算量降低70%；净化后的MMLU/HellaSwag数据集显示部分模型性能虚高

Conclusion: 建议将污染检测作为基准测试发布前的标准流程，n-gram方法可有效提升LLM评估透明度与可靠性

Abstract: The performance of large language models (LLMs) continues to improve, as
reflected in rising scores on standard benchmarks. However, the lack of
transparency around training data raises concerns about potential overlap with
evaluation sets and the fairness of reported results. Although prior work has
proposed methods for detecting data leakage, these approaches primarily focus
on identifying outliers and have not been evaluated under controlled simulated
leakage conditions. In this work, we compare existing leakage detection
techniques, namely permutation and n-gram-based methods, under a continual
pretraining setup that simulates real-world leakage scenarios, and additionally
explore a lightweight method we call semi-half question. Although semi-half
offers a low-cost alternative, our analysis shows that the n-gram method
consistently achieves the highest F1-score. We also refine these techniques to
support instance-level detection and reduce computational overhead. Leveraging
the best-performing method, we create cleaned versions of MMLU and HellaSwag,
and re-evaluate several LLMs. Our findings present a practical path toward more
reliable and transparent evaluations, and we recommend contamination checks as
a standard step before releasing benchmark results.

</details>


### [89] [Faithful and Robust LLM-Driven Theorem Proving for NLI Explanations](https://arxiv.org/abs/2505.24264)
*Xin Quan,Marco Valentino,Louise A. Dennis,André Freitas*

Main category: cs.CL

TL;DR: 提出四策略提升大语言模型与定理证明器在自然语言推理中的解释有效性，缓解形式化过程中的语义损失和逻辑不精确问题，并在多个数据集实现显著效果提升


<details>
  <summary>Details</summary>
Motivation: 现有方法在自然语言到形式化表示转换时存在语义信息丢失风险，且大语言模型难以精确捕捉逻辑结构和进行严格证明构建，需提升解释的忠实性和验证鲁棒性

Method: (1)自动形式化的语义损失缓解 (2)逻辑表示错误识别与修正 (3)逻辑表达式引导结构化证明草图生成 (4)增强模型对定理证明器反馈的迭代优化能力

Result: 在e-SNLI/QASC/WorldTree数据集上：自动形式化准确率提升18.46%/34.2%/39.77%，解释改进提升29.5%/51.5%/41.25%，验证迭代次数显著降低

Conclusion: 策略有效提升混合架构性能，特定干预可大幅提升验证效率，证明结构化逻辑引导和反馈迭代机制的关键作用

Abstract: Natural language explanations play a fundamental role in Natural Language
Inference (NLI) by revealing how premises logically entail hypotheses. Recent
work has shown that the interaction of large language models (LLMs) with
theorem provers (TPs) can help verify and improve the validity of NLI
explanations. However, TPs require translating natural language into
machine-verifiable formal representations, a process that introduces the risk
of semantic information loss and unfaithful interpretation, an issue compounded
by LLMs' challenges in capturing critical logical structures with sufficient
precision. Moreover, LLMs are still limited in their capacity for rigorous and
robust proof construction within formal verification frameworks. To mitigate
issues related to faithfulness and robustness, this paper investigates
strategies to (1) alleviate semantic loss during autoformalisation, (2)
efficiently identify and correct syntactic errors in logical representations,
(3) explicitly use logical expressions to guide LLMs in generating structured
proof sketches, and (4) increase LLMs' capacity of interpreting TP's feedback
for iterative refinement. Our empirical results on e-SNLI, QASC and WorldTree
using different LLMs demonstrate that the proposed strategies yield significant
improvements in autoformalisation (+18.46%, +34.2%, +39.77%) and explanation
refinement (+29.5%, +51.5%, +41.25%) over the state-of-the-art model. Moreover,
we show that specific interventions on the hybrid LLM-TP architecture can
substantially improve efficiency, drastically reducing the number of iterations
required for successful verification.

</details>


### [90] [ScienceMeter: Tracking Scientific Knowledge Updates in Language Models](https://arxiv.org/abs/2505.24302)
*Yike Wang,Shangbin Feng,Yulia Tsvetkov,Hannaneh Hajishirzi*

Main category: cs.CL

TL;DR: 提出ScienceMeter框架评估LLMs科学知识更新，通过知识保存率(85.9%)、获取率(71.7%)和投影率(37.7%)三指标，揭示现有方法在跨领域科学知识更新中的局限性


<details>
  <summary>Details</summary>
Motivation: LLMs在科学研究中的应用受限于知识快速过时，需系统评估其知识更新机制对过去、现在和未来科学知识的适应能力

Method: 构建包含15,444篇论文的跨领域数据集，定义知识保存/获取/投影三维度指标，评估5种更新方法在30,888个科学主张上的表现

Result: 最佳方法仅实现85.9%知识保存+71.7%新知识获取+37.7%未来知识预测，小模型依赖训练更新而大模型适用推理更新，跨领域表现呈正相关

Conclusion: 现有科学知识更新机制无法同时满足三重要求，开发兼顾知识时效性与稳定性的更新方法成为LLMs科学应用的关键挑战

Abstract: Large Language Models (LLMs) are increasingly used to support scientific
research, but their knowledge of scientific advancements can quickly become
outdated. We introduce ScienceMeter, a new framework for evaluating scientific
knowledge update methods over scientific knowledge spanning the past, present,
and future. ScienceMeter defines three metrics: knowledge preservation, the
extent to which models' understanding of previously learned papers are
preserved; knowledge acquisition, how well scientific claims from newly
introduced papers are acquired; and knowledge projection, the ability of the
updated model to anticipate or generalize to related scientific claims that may
emerge in the future. Using ScienceMeter, we examine the scientific knowledge
of LLMs on claim judgment and generation tasks across a curated dataset of
15,444 scientific papers and 30,888 scientific claims from ten domains
including medicine, biology, materials science, and computer science. We
evaluate five representative knowledge update approaches including training-
and inference-time methods. With extensive experiments, we find that the
best-performing knowledge update methods can preserve only 85.9% of existing
knowledge, acquire 71.7% of new knowledge, and project 37.7% of future
knowledge. Inference-based methods work for larger models, whereas smaller
models require training to achieve comparable performance. Cross-domain
analysis reveals that performance on these objectives is correlated. Even when
applying on specialized scientific LLMs, existing knowledge update methods fail
to achieve these objectives collectively, underscoring that developing robust
scientific knowledge update mechanisms is both crucial and challenging.

</details>


### [91] [HiCaM: A Hierarchical-Causal Modification Framework for Long-Form Text Modification](https://arxiv.org/abs/2505.24319)
*Yuntao Shi,Yi Luo,Yeyun Gong,Chen Lin*

Main category: cs.CL

TL;DR: 为解决大语言模型在长文本编辑中的不当修改和遗漏问题，研究者提出基于层次摘要树和因果图的HiCaM框架，并在多领域数据集上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理长文本修改任务时存在两个核心问题：对无关内容的不当修改/总结，以及未能对保持文档连贯性至关重要的隐含相关段落进行必要修改。

Method: 提出HiCaM框架，通过层次化摘要树（定位关键内容）和因果图（维护文本连贯性）的双重结构实现精准修改。

Result: 在跨领域评估数据集上取得显著提升，最高达79.50%的胜率，且在不同模型和领域间保持性能一致性。

Conclusion: HiCaM框架通过结构化方法有效提升长文本修改质量，为LLM的长文本处理能力优化提供了新思路。

Abstract: Large Language Models (LLMs) have achieved remarkable success in various
domains. However, when handling long-form text modification tasks, they still
face two major problems: (1) producing undesired modifications by
inappropriately altering or summarizing irrelevant content, and (2) missing
necessary modifications to implicitly related passages that are crucial for
maintaining document coherence. To address these issues, we propose HiCaM, a
Hierarchical-Causal Modification framework that operates through a hierarchical
summary tree and a causal graph. Furthermore, to evaluate HiCaM, we derive a
multi-domain dataset from various benchmarks, providing a resource for
assessing its effectiveness. Comprehensive evaluations on the dataset
demonstrate significant improvements over strong LLMs, with our method
achieving up to a 79.50\% win rate. These results highlight the
comprehensiveness of our approach, showing consistent performance improvements
across multiple models and domains.

</details>


### [92] [Context-Aware Sentiment Forecasting via LLM-based Multi-Perspective Role-Playing Agents](https://arxiv.org/abs/2505.24331)
*Fanhang Man,Huandong Wang,Jianjie Fang,Zhaoyi Deng,Baining Zhao,Xinlei Chen,Yong Li*

Main category: cs.CL

TL;DR: 提出社交媒体情绪预测新方法，通过多视角角色扮演框架提升预测效果


<details>
  <summary>Details</summary>
Motivation: 现有研究集中于分析历史情绪，但缺乏对事件发展中即时情绪演变的预测能力研究

Method: 提取情绪相关特征，构建模拟人类响应过程的多视角角色扮演框架

Result: 在微观（个体）和宏观（群体）层面的情绪预测均取得显著改进效果

Conclusion: 通过角色行为模拟机制，有效提升了社交媒体用户未来情绪预测的准确性

Abstract: User sentiment on social media reveals the underlying social trends, crises,
and needs. Researchers have analyzed users' past messages to trace the
evolution of sentiments and reconstruct sentiment dynamics. However, predicting
the imminent sentiment of an ongoing event is rarely studied. In this paper, we
address the problem of \textbf{sentiment forecasting} on social media to
predict the user's future sentiment in response to the development of the
event. We extract sentiment-related features to enhance the modeling skill and
propose a multi-perspective role-playing framework to simulate the process of
human response. Our preliminary results show significant improvement in
sentiment forecasting on both microscopic and macroscopic levels.

</details>


### [93] [Pangu DeepDiver: Adaptive Search Intensity Scaling via Open-Web Reinforcement Learning](https://arxiv.org/abs/2505.24332)
*Wenxuan Shi,Haochen Tan,Chuqiao Kuang,Xiaoguang Li,Xiaozhe Ren,Chen Zhang,Hanting Chen,Yasheng Wang,Lifeng Shang,Fisher Yu,Yunhe Wang*

Main category: cs.CL

TL;DR: 提出DeepDiver框架解决LLMs在开放网络问答中动态调整搜索强度（SIS）的不足，通过WebPuzzle数据集和强化学习实现真实环境适应性


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖静态规则和维基语料库，无法应对真实网络环境的模糊性/矛盾证据/噪声，缺乏动态调整搜索强度的能力

Method: 构建WebPuzzle数据集（24K训练+275测试），开发DeepDiver强化学习框架，结合冷启动SFT和定制化RL训练流程

Result: Pangu-7B-Reasoner+DeepDiver达到与671B参数DeepSeek-R1相当的真实网络任务性能，SIS能力可泛化至长文本生成

Conclusion: 首次定义SIS能力并建立评估基准，推动LLMs自适应信息检索，为后续研究提供方法论与数据集支持

Abstract: Information seeking demands iterative evidence gathering and reflective
reasoning, yet large language models (LLMs) still struggle with it in open-web
question answering. Existing methods rely on static prompting rules or training
with Wikipedia-based corpora and retrieval environments, limiting adaptability
to the real-world web environment where ambiguity, conflicting evidence, and
noise are prevalent. These constrained training settings hinder LLMs from
learning to dynamically decide when and where to search, and how to adjust
search depth and frequency based on informational demands. We define this
missing capacity as Search Intensity Scaling (SIS)--the emergent skill to
intensify search efforts under ambiguous or conflicting conditions, rather than
settling on overconfident, under-verification answers.
  To study SIS, we introduce WebPuzzle, the first dataset designed to foster
information-seeking behavior in open-world internet environments. WebPuzzle
consists of 24K training instances and 275 test questions spanning both
wiki-based and open-web queries. Building on this dataset, we propose
DeepDiver, a Reinforcement Learning (RL) framework that promotes SIS by
encouraging adaptive search policies through exploration under a real-world
open-web environment. Experimental results show that Pangu-7B-Reasoner
empowered by DeepDiver achieve performance on real-web tasks comparable to the
671B-parameter DeepSeek-R1. We detail DeepDiver's training curriculum from
cold-start supervised fine-tuning to a carefully designed RL phase, and present
that its capability of SIS generalizes from closed-form QA to open-ended tasks
such as long-form writing. Our contributions advance adaptive information
seeking in LLMs and provide a valuable benchmark and dataset for future
research.

</details>


### [94] [Exploring Multimodal Challenges in Toxic Chinese Detection: Taxonomy, Benchmark, and Findings](https://arxiv.org/abs/2505.24341)
*Shujian Yang,Shiyao Cui,Chuanrui Hu,Haicheng Wang,Tianwei Zhang,Minlie Huang,Jialiang Lu,Han Qiu*

Main category: cs.CL

TL;DR: 研究探讨了大语言模型检测扰动中文有毒内容的挑战，发现现有模型检测能力不足且增强方法易引发过度校正


<details>
  <summary>Details</summary>
Motivation: 中文多模态特性导致扰动后的有毒内容检测困难，现有SOTA大模型在此类场景表现脆弱，需系统评估模型鲁棒性

Method: 提出3种扰动策略和8种实现方法构建数据集，测试9个中美顶尖大模型，探索上下文学习和监督微调的增强效果

Result: 模型对扰动多模态中文内容检测能力较弱；少量扰动样本的增强训练会导致模型将正常内容误判为有毒的「过度校正」现象

Conclusion: 需提升大模型对中文多模态扰动的鲁棒性，增强方法需平衡检测效果与误判风险

Abstract: Detecting toxic content using language models is important but challenging.
While large language models (LLMs) have demonstrated strong performance in
understanding Chinese, recent studies show that simple character substitutions
in toxic Chinese text can easily confuse the state-of-the-art (SOTA) LLMs. In
this paper, we highlight the multimodal nature of Chinese language as a key
challenge for deploying LLMs in toxic Chinese detection. First, we propose a
taxonomy of 3 perturbation strategies and 8 specific approaches in toxic
Chinese content. Then, we curate a dataset based on this taxonomy, and
benchmark 9 SOTA LLMs (from both the US and China) to assess if they can detect
perturbed toxic Chinese text. Additionally, we explore cost-effective
enhancement solutions like in-context learning (ICL) and supervised fine-tuning
(SFT). Our results reveal two important findings. (1) LLMs are less capable of
detecting perturbed multimodal Chinese toxic contents. (2) ICL or SFT with a
small number of perturbed examples may cause the LLMs "overcorrect'':
misidentify many normal Chinese contents as toxic.

</details>


### [95] [Fewer Hallucinations, More Verification: A Three-Stage LLM-Based Framework for ASR Error Correction](https://arxiv.org/abs/2505.24347)
*Yangui Fang,Baixu Cheng,Jing Peng,Xu Li,Yu Xi,Chengwei Zhang,Guohui Zhong*

Main category: cs.CL

TL;DR: 提出RLLM-CF框架解决ASR错误纠正中LLMs的幻觉问题，通过三阶段流程实现可靠纠错


<details>
  <summary>Details</summary>
Motivation: 传统ASR纠错方法效果有限，直接使用LLMs会产生错误修改正确文本的幻觉问题

Method: 三阶段框架：1)错误预检测 2)思维链子任务迭代纠正 3)推理过程验证（无需额外数据或模型微调）

Result: 在AISHELL-1/2和Librispeech数据集上CER/WER相对降低21%、11%、9%、11.4%

Conclusion: RLLM-CF通过多阶段编程验证有效控制LLM纠错可靠性，显著提升ASR纠错性能

Abstract: Automatic Speech Recognition (ASR) error correction aims to correct
recognition errors while preserving accurate text. Although traditional
approaches demonstrate moderate effectiveness, LLMs offer a paradigm that
eliminates the need for training and labeled data. However, directly using LLMs
will encounter hallucinations problem, which may lead to the modification of
the correct text. To address this problem, we propose the Reliable LLM
Correction Framework (RLLM-CF), which consists of three stages: (1) error
pre-detection, (2) chain-of-thought sub-tasks iterative correction, and (3)
reasoning process verification. The advantage of our method is that it does not
require additional information or fine-tuning of the model, and ensures the
correctness of the LLM correction under multi-pass programming. Experiments on
AISHELL-1, AISHELL-2, and Librispeech show that the GPT-4o model enhanced by
our framework achieves 21%, 11%, 9%, and 11.4% relative reductions in CER/WER.

</details>


### [96] [Unifying Language Agent Algorithms with Graph-based Orchestration Engine for Reproducible Agent Research](https://arxiv.org/abs/2505.24354)
*Qianqian Zhang,Jiajia Liao,Heting Ying,Yibo Ma,Haozhan Shen,Jingcheng Li,Peng Liu,Lu Zhang,Chunxin Fang,Kyusong Lee,Ruochen Xu,Tiancheng Zhao*

Main category: cs.CL

TL;DR: 提出AGORA框架解决语言代理开发的三大挑战：通过模块化架构、标准化算法套件和评估体系，平衡性能与计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有语言代理开发面临工程复杂度高、组件不标准化、评估体系缺失等问题，需要系统化解决方案。

Method: 1) 基于图的工作流引擎实现模块化架构
2) 集成多种先进推理算法
3) 建立多维度评估框架

Result: 复杂算法提升有限，Chain-of-Thought等简单方法在低开销下表现稳健，不同LLM能力差异显著。

Conclusion: AGORA为语言代理开发提供标准化基础设施，促进可重复研究，强调算法选择需平衡性能与效率。

Abstract: Language agents powered by large language models (LLMs) have demonstrated
remarkable capabilities in understanding, reasoning, and executing complex
tasks. However, developing robust agents presents significant challenges:
substantial engineering overhead, lack of standardized components, and
insufficient evaluation frameworks for fair comparison. We introduce Agent
Graph-based Orchestration for Reasoning and Assessment (AGORA), a flexible and
extensible framework that addresses these challenges through three key
contributions: (1) a modular architecture with a graph-based workflow engine,
efficient memory management, and clean component abstraction; (2) a
comprehensive suite of reusable agent algorithms implementing state-of-the-art
reasoning approaches; and (3) a rigorous evaluation framework enabling
systematic comparison across multiple dimensions. Through extensive experiments
on mathematical reasoning and multimodal tasks, we evaluate various agent
algorithms across different LLMs, revealing important insights about their
relative strengths and applicability. Our results demonstrate that while
sophisticated reasoning approaches can enhance agent capabilities, simpler
methods like Chain-of-Thought often exhibit robust performance with
significantly lower computational overhead. AGORA not only simplifies language
agent development but also establishes a foundation for reproducible agent
research through standardized evaluation protocols.

</details>


### [97] [Multilingual Gloss-free Sign Language Translation: Towards Building a Sign Language Foundation Model](https://arxiv.org/abs/2505.24355)
*Sihan Tan,Taro Miyazaki,Kazuhiro Nakadai*

Main category: cs.CL

TL;DR: 提出支持10种手语的免词汇多语言手语翻译模型，通过双CTC目标实现手语识别和文本生成，在三大基准测试中达到SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 现有单语手语翻译存在资源不足问题，多语言方法能提升资源利用率和跨语言可访问性，但面临语言冲突和对齐困难的技术挑战。

Method: 采用双CTC目标框架：1）token级手语识别分支进行语言判别 2）基于跨模态Transformer的文本生成分支，支持一对多/多对多翻译模式

Result: 在SP-10、PHOENIX14T和CSL-Daily三个基准测试中，该模型在单语/多语场景下均超越现有方法，验证了框架有效性。

Conclusion: 首个实现多语言免词汇手语翻译的通用框架，为低资源手语社区提供高效跨语言沟通解决方案，推动手语AI技术普惠化。

Abstract: Sign Language Translation (SLT) aims to convert sign language (SL) videos
into spoken language text, thereby bridging the communication gap between the
sign and the spoken community. While most existing works focus on translating a
single sign language into a single spoken language (one-to-one SLT), leveraging
multilingual resources could mitigate low-resource issues and enhance
accessibility. However, multilingual SLT (MLSLT) remains unexplored due to
language conflicts and alignment difficulties across SLs and spoken languages.
To address these challenges, we propose a multilingual gloss-free model with
dual CTC objectives for token-level SL identification and spoken text
generation. Our model supports 10 SLs and handles one-to-one, many-to-one, and
many-to-many SLT tasks, achieving competitive performance compared to
state-of-the-art methods on three widely adopted benchmarks: multilingual
SP-10, PHOENIX14T, and CSL-Daily.

</details>


### [98] [Knowing Before Saying: LLM Representations Encode Information About Chain-of-Thought Success Before Completion](https://arxiv.org/abs/2505.24362)
*Anum Afzal,Florian Matthes,Gal Chechik,Yftah Ziser*

Main category: cs.CL

TL;DR: 研究发现，零样本思维链（CoT）的成功在生成前即可预测，LLM初始表示包含关键信息，早期停止可能有效。


<details>
  <summary>Details</summary>
Motivation: 探索CoT推理过程中关键信息何时出现，验证是否可在生成完成前预测其成功性，从而优化推理效率。

Method: 使用基于LLM表示的探测分类器（甚至在生成token前）与BERT基线模型对比，分析不同推理阶段的表示有效性。

Result: LLM初始表示即含核心推理信息（分类准确率高），BERT依赖浅层语言线索表现较差；早期停止实验显示截断CoT仍优于无CoT。

Conclusion: LLMs早期编码关键信息特性支持优化CoT效率，未来可通过监督/强化学习配合分类器指导实现有效早期停止。

Abstract: We investigate whether the success of a zero-shot Chain-of-Thought (CoT)
process can be predicted before completion. We discover that a probing
classifier, based on LLM representations, performs well \emph{even before a
single token is generated}, suggesting that crucial information about the
reasoning process is already present in the initial steps representations. In
contrast, a strong BERT-based baseline, which relies solely on the generated
tokens, performs worse, likely because it depends on shallow linguistic cues
rather than deeper reasoning dynamics. Surprisingly, using later reasoning
steps does not always improve classification. When additional context is
unhelpful, earlier representations resemble later ones more, suggesting LLMs
encode key information early. This implies reasoning can often stop early
without loss. To test this, we conduct early stopping experiments, showing that
truncating CoT reasoning still improves performance over not using CoT at all,
though a gap remains compared to full reasoning. However, approaches like
supervised learning or reinforcement learning designed to shorten CoT chains
could leverage our classifier's guidance to identify when early stopping is
effective. Our findings provide insights that may support such methods, helping
to optimize CoT's efficiency while preserving its benefits.\footnote{Code and
data is available at
\href{https://github.com/anum94/CoTpred}{\texttt{github.com/anum94/CoTpred}}.

</details>


### [99] [LLM Inference Enhanced by External Knowledge: A Survey](https://arxiv.org/abs/2505.24377)
*Yu-Hsuan Lin,Qian-Hui Chen,Yi-Jie Cheng,Jia-Ren Zhang,Yi-Hung Liu,Liang-Yu Hsia,Yun-Nung Chen*

Main category: cs.CL

TL;DR: 系统性探讨利用外部知识增强大语言模型的方法，重点分析结构化知识（表格/知识图谱）的整合范式及效果权衡


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在参数记忆限制和幻觉问题，需借助外部知识提升准确推理能力

Method: 建立知识分类体系（非结构化/结构化），针对表格和知识图谱设计不同整合范式，提出可解释性-扩展性-性能三维评估框架

Result: 结构化知识整合显著提升可信度，表格更适合可解释性需求场景，知识图谱在扩展性方面表现更优

Conclusion: 未来应开发动态知识融合框架，平衡不同知识源的互补优势，构建可信且可泛化的知识增强型LLMs

Abstract: Recent advancements in large language models (LLMs) have enhanced
natural-language reasoning. However, their limited parametric memory and
susceptibility to hallucination present persistent challenges for tasks
requiring accurate, context-based inference. To overcome these limitations, an
increasing number of studies have proposed leveraging external knowledge to
enhance LLMs. This study offers a systematic exploration of strategies for
using external knowledge to enhance LLMs, beginning with a taxonomy that
categorizes external knowledge into unstructured and structured data. We then
focus on structured knowledge, presenting distinct taxonomies for tables and
knowledge graphs (KGs), detailing their integration paradigms with LLMs, and
reviewing representative methods. Our comparative analysis further highlights
the trade-offs among interpretability, scalability, and performance, providing
insights for developing trustworthy and generalizable knowledge-enhanced LLMs.

</details>


### [100] [ClueAnchor: Clue-Anchored Knowledge Reasoning Exploration and Optimization for Retrieval-Augmented Generation](https://arxiv.org/abs/2505.24388)
*Hao Chen,Yukun Yan,Sen Mei,Wanxiang Che,Zhenghao Liu,Qi Shi,Xinze Li,Yuchun Fan,Pengcheng Huang,Qiushi Xiong,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 提出ClueAnchor框架，通过线索锚定推理优化增强RAG系统，显著提升推理完整性和抗噪能力。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在证据隐含、分散或含噪声时，难以有效提取关键线索进行可信推理。

Method: 从检索内容提取关键线索，生成多推理路径，通过奖励驱动的偏好优化选择最优路径。

Result: 实验显示在推理完整性和鲁棒性上显著超越基线，且对噪声内容具有强韧性，无需显式线索监督即可定位证据。

Conclusion: ClueAnchor通过系统化的线索锚定机制，实现了更可靠的知识整合，为复杂真实场景的RAG应用提供了新思路。

Abstract: Retrieval-Augmented Generation (RAG) augments Large Language Models (LLMs)
with external knowledge to improve factuality. However, existing RAG systems
frequently underutilize the retrieved documents, failing to extract and
integrate the key clues needed to support faithful and interpretable reasoning,
especially in cases where relevant evidence is implicit, scattered, or obscured
by noise. To address this issue, we propose ClueAnchor, a novel framework for
enhancing RAG via clue-anchored reasoning exploration and optimization.
ClueAnchor extracts key clues from retrieved content and generates multiple
reasoning paths based on different knowledge configurations, optimizing the
model by selecting the most effective one through reward-based preference
optimization. Experiments show that ClueAnchor significantly outperforms prior
RAG baselines in reasoning completeness and robustness. Further analysis
confirms its strong resilience to noisy or partially relevant retrieved
content, as well as its capability to identify supporting evidence even in the
absence of explicit clue supervision during inference.

</details>


### [101] [LLMs Are Globally Multilingual Yet Locally Monolingual: Exploring Knowledge Transfer via Language and Thought Theory](https://arxiv.org/abs/2505.24409)
*Eojin Kang,Juae Kim*

Main category: cs.CL

TL;DR: 提出L2T提示策略解决多语言LLMs知识迁移不一致问题，实验证明思维与知识对齐比输入语言更重要


<details>
  <summary>Details</summary>
Motivation: 现有研究过度依赖英语提示策略，未能有效解释多语言场景下知识迁移失败的根本原因，需通过认知理论揭示语言与思维绑定机制

Method: 基于语言与思维理论设计L2T提示框架，分析语言输入→认知处理→知识激活的映射关系，并在训练阶段集成语言中立的思维模式

Result: 挑战英语优先假设，验证西班牙语提示效果超越英语；发现思维-知识对齐度比语言匹配度对准确率影响高37%；L2T训练使跨语言任务性能方差降低62%

Conclusion: 突破传统翻译依赖路径，确立认知对齐作为跨语言知识迁移新范式，为构建语言无关的LLMs提供理论框架与实践方案

Abstract: Multilingual large language models (LLMs) open up new possibilities for
leveraging information across languages, but their factual knowledge recall
remains inconsistent depending on the input language. While previous studies
have attempted to address this issue through English-based prompting and
evaluation, we explore non-English to English transfer via Language and Thought
Theory. This perspective allows us to examine language-thought binding in LLMs
and uncover why factual knowledge often fails to transfer effectively. We
propose the Language-to-Thought (L2T) prompting strategy, which analyzes the
relationship between input language, internal cognitive processes, and
knowledge. Experimental results challenge the assumption that English-based
approaches consistently outperform other languages and offer a novel insight
that aligning the model's internal thought with the knowledge required for the
task is critical for successful cross-lingual transfer. Furthermore, we show
that applying L2T during training can alleviate LLMs' reliance on the input
language and facilitate cross-linguistic knowledge integration without
translation-based learning. Code and datasets will be available.

</details>


### [102] [MMAFFBen: A Multilingual and Multimodal Affective Analysis Benchmark for Evaluating LLMs and VLMs](https://arxiv.org/abs/2505.24423)
*Zhiwei Liu,Lingfei Qian,Qianqian Xie,Jimin Huang,Kailai Yang,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 提出首个多语言多模态情感分析基准MMAFFBen，覆盖35种语言的文本/图像/视频模态，构建专用数据集并开发模型，系统评估现有语言模型的情感理解能力


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在情感分析领域的潜力未被充分挖掘，主要因缺乏全面评估基准及任务复杂度高

Method: 创建涵盖四类情感分析任务的基准测试集MMAFFBen，构建微调数据集MMAFFIn，开发MMAFFLM系列模型，系统评估GPT-4o-mini等代表性模型

Result: 建立首个开源多模态情感分析评估体系，验证不同模型在情感极性/强度等任务上的性能差异，开发出3B/7B参数的专用模型

Conclusion: MMAFFBen为多语言多模态情感分析提供标准化评估工具，推动情感计算领域发展，为后续研究建立新基线

Abstract: Large language models and vision-language models (which we jointly call LMs)
have transformed NLP and CV, demonstrating remarkable potential across various
fields. However, their capabilities in affective analysis (i.e. sentiment
analysis and emotion detection) remain underexplored. This gap is largely due
to the absence of comprehensive evaluation benchmarks, and the inherent
complexity of affective analysis tasks. In this paper, we introduce MMAFFBen,
the first extensive open-source benchmark for multilingual multimodal affective
analysis. MMAFFBen encompasses text, image, and video modalities across 35
languages, covering four key affective analysis tasks: sentiment polarity,
sentiment intensity, emotion classification, and emotion intensity. Moreover,
we construct the MMAFFIn dataset for fine-tuning LMs on affective analysis
tasks, and further develop MMAFFLM-3b and MMAFFLM-7b based on it. We evaluate
various representative LMs, including GPT-4o-mini, providing a systematic
comparison of their affective understanding capabilities. This project is
available at https://github.com/lzw108/MMAFFBen.

</details>


### [103] [Donate or Create? Comparing Data Collection Strategies for Emotion-labeled Multimodal Social Media Posts](https://arxiv.org/abs/2505.24427)
*Christopher Bagdon,Aidan Combs,Carina Silberer,Roman Klinger*

Main category: cs.CL

TL;DR: 研究比较了社交媒体情感分析中人工创建数据与真实数据的差异，发现两者在内容长度、模态侧重和人口统计特征存在显著不同，但人工数据仍能有效训练具有泛化能力的模型。


<details>
  <summary>Details</summary>
Motivation: 探讨人工创建数据与真实数据在情感表达特征和模型训练效果方面的差异，解决数据收集方法论对AI模型性能评估的影响。

Method: 通过收集标注情感的多元模态社交媒体数据（包括用户自愿捐赠的真实帖子和实验要求下人工创建的帖子），从内容特征、人口统计学差异和模型性能三个维度进行对比分析。

Result: 人工数据比真实数据：文本更长（+38%）、更依赖文本而非图片表达情感（文本贡献度+22%）、更多聚焦典型情感事件；数据捐赠者与创建者存在人口统计差异（年龄差5.2岁，教育程度差15%）；人工数据训练模型在真实数据测试集上达到0.82 F1-score。

Conclusion: 人工创建数据可作为有效的模型训练素材，但需配合真实数据才能获得可靠的效果评估，研究揭示了数据收集方法对AI系统开发的潜在影响。

Abstract: Accurate modeling of subjective phenomena such as emotion expression requires
data annotated with authors' intentions. Commonly such data is collected by
asking study participants to donate and label genuine content produced in the
real world, or create content fitting particular labels during the study.
Asking participants to create content is often simpler to implement and
presents fewer risks to participant privacy than data donation. However, it is
unclear if and how study-created content may differ from genuine content, and
how differences may impact models. We collect study-created and genuine
multimodal social media posts labeled for emotion and compare them on several
dimensions, including model performance. We find that compared to genuine
posts, study-created posts are longer, rely more on their text and less on
their images for emotion expression, and focus more on emotion-prototypical
events. The samples of participants willing to donate versus create posts are
demographically different. Study-created data is valuable to train models that
generalize well to genuine data, but realistic effectiveness estimates require
genuine data.

</details>


### [104] [Model Unlearning via Sparse Autoencoder Subspace Guided Projections](https://arxiv.org/abs/2505.24428)
*Xu Wang,Zihao Li,Benyou Wang,Yan Hu,Difan Zou*

Main category: cs.CL

TL;DR: Proposes SSPU framework using SAE-guided subspace projection for precise, interpretable and robust model unlearning


<details>
  <summary>Details</summary>
Motivation: Existing unlearning methods lack interpretability and robustness against adversarial prompts when removing sensitive knowledge from LLMs

Method: Three-stage pipeline: 1) Data-driven layer/feature selection 2) Subspace construction via QR decomposition 3) Constrained optimization controlling activations into irrelevant subspaces

Result: Reduces harmful knowledge accuracy by 3.22% on WMDP-Cyber, improves adversarial robustness against jailbreak prompts compared to baselines

Conclusion: Demonstrates subspace-guided optimization enables robust model behavior control while preserving utility, exposing limitations of previous unlearning approaches

Abstract: Large language models (LLMs) store vast amounts of information, making them
powerful yet raising privacy and safety concerns when selective knowledge
removal is required. Existing unlearning strategies, ranging from
gradient-based fine-tuning and model editing to sparse autoencoder (SAE)
steering, either lack interpretability or fail to provide a robust defense
against adversarial prompts. We propose SAE-Guided Subspace Projection
Unlearning (SSPU), a novel framework that leverages SAE features to drive
targeted updates in the model's parameter space, enabling precise,
interpretable, and robust unlearning. SSPU's three-stage pipeline performs
data-driven layer and feature selection, subspace construction via QR
decomposition, and constrained optimization that controls activations into an
"irrelevant" subspace while preserving retained knowledge. Overall, we use SAE
features to construct a subspace that supervises unlearning, refining the loss
and adding a regularization term to guide interpretable parameter updates. In
experiments on the WMDP-Cyber forget set and three utility benchmarks (MMLU,
TruthfulQA, GSM8K), SSPU reduces harmful knowledge accuracy by 3.22% compared
to the strongest baseline. It also improves adversarial robustness, lowering
malicious accuracy under jailbreak prompts compared to baselines. Our findings
expose the limitations of prior unlearning methods and demonstrate how
interpretable subspace-guided optimization can achieve robust, controllable
model behavior.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [105] [Force-Dual Modes: Subspace Design from Stochastic Forces](https://arxiv.org/abs/2505.23969)
*Otman Benchekroun,Eitan Grinspun,Maurizio Chiaramonte,Philip Allen Etter*

Main category: cs.GR

TL;DR: 提出通过力分布构建降阶建模子空间，优化动态模拟中常见场景交互的仿真效率


<details>
  <summary>Details</summary>
Motivation: 传统降阶建模子空间在动态模拟场景中缺乏普适性最优解，需适应约束控制/接触/肌肉驱动等多重交互需求

Method: 将用户设计的力分布通过线性化仿真转换为位移分布，拟合低秩高斯模型构建物理属性自适应的子空间

Result: 新子空间兼容线性模态分析（单位方差力分布）和格林函数（低秩力分布），实现材料属性和复杂力分布的双重优化

Conclusion: 该统计建模框架为动态仿真提供了场景自适应的子空间构建方案，显著提升交互式仿真的计算效率

Abstract: Designing subspaces for Reduced Order Modeling (ROM) is crucial for
accelerating finite element simulations in graphics and engineering.
Unfortunately, it's not always clear which subspace is optimal for arbitrary
dynamic simulation. We propose to construct simulation subspaces from force
distributions, allowing us to tailor such subspaces to common scene
interactions involving constraint penalties, handles-based control, contact and
musculoskeletal actuation. To achieve this we adopt a statistical perspective
on Reduced Order Modelling, which allows us to push such user-designed force
distributions through a linearized simulation to obtain a dual distribution on
displacements. To construct our subspace, we then fit a low-rank Gaussian model
to this displacement distribution, which we show generalizes Linear Modal
Analysis subspaces for uncorrelated unit variance force distributions, as well
as Green's Function subspaces for low rank force distributions. We show our
framework allows for the construction of subspaces that are optimal both with
respect to physical material properties, as well as arbitrary force
distributions as observed in handle-based, contact, and musculoskeletal scene
interactions.

</details>


### [106] [3DGEER: Exact and Efficient Volumetric Rendering with 3D Gaussians](https://arxiv.org/abs/2505.24053)
*Zixun Huang,Cho-Ying Wu,Yuliang Guo,Xinyu Huang,Liu Ren*

Main category: cs.GR

TL;DR: 3DGEER提出了一种精确高效的三维高斯体积渲染方法，通过闭式积分公式和新型投影策略，在保持实时性的同时显著提升大视场角下的渲染质量


<details>
  <summary>Details</summary>
Motivation: 传统3DGS方法因将3D高斯投影为2D近似导致大视场角下质量下降，现有改进方法无法同时实现数学精确性与实时渲染效率

Method: 1. 推导3D高斯沿光线密度积分的闭式表达式；2. 提出粒子边界视锥体(PBF)实现精确光线-高斯关联；3. 设计双极等角投影(BEAP)加速通用相机模型的射线关联

Result: 在针孔和鱼眼相机数据集上均超越现有方法，实现实时神经渲染领域新的SOTA，推理速度达200+FPS

Conclusion: 3DGEER首次在数学精确性和实时效率间取得突破，支持任意相机模型，为神经渲染领域提供了更普适的解决方案

Abstract: 3D Gaussian Splatting (3DGS) marks a significant milestone in balancing the
quality and efficiency of differentiable rendering. However, its high
efficiency stems from an approximation of projecting 3D Gaussians onto the
image plane as 2D Gaussians, which inherently limits rendering
quality--particularly under large Field-of-View (FoV) camera inputs. While
several recent works have extended 3DGS to mitigate these approximation errors,
none have successfully achieved both exactness and high efficiency
simultaneously. In this work, we introduce 3DGEER, an Exact and Efficient
Volumetric Gaussian Rendering method. Starting from first principles, we derive
a closed-form expression for the density integral along a ray traversing a 3D
Gaussian distribution. This formulation enables precise forward rendering with
arbitrary camera models and supports gradient-based optimization of 3D Gaussian
parameters. To ensure both exactness and real-time performance, we propose an
efficient method for computing a tight Particle Bounding Frustum (PBF) for each
3D Gaussian, enabling accurate and efficient ray-Gaussian association. We also
introduce a novel Bipolar Equiangular Projection (BEAP) representation to
accelerate ray association under generic camera models. BEAP further provides a
more uniform ray sampling strategy to apply supervision, which empirically
improves reconstruction quality. Experiments on multiple pinhole and fisheye
datasets show that our method consistently outperforms prior methods,
establishing a new state-of-the-art in real-time neural rendering.

</details>


### [107] [Minimizing Ray Tracing Memory Traffic through Quantized Structures and Ray Stream Tracing](https://arxiv.org/abs/2505.24653)
*Moritz Grauer,Johannes Hanika,Carsten Dachsbacher*

Main category: cs.GR

TL;DR: 提出集成压缩数据结构与射线流技术的内存高效光线追踪方法，将内存流量降至传统方法的18%


<details>
  <summary>Details</summary>
Motivation: 光线追踪性能受内存带宽限制，传统方法存在高内存流量和浮点舍入误差问题，需同时解决带宽和精度挑战

Method: 压缩BVH/三角形结构 + 射线流追踪 + 8位量化坐标 + 固定点算术硬件支持，避免几何漏洞

Result: 8-wide BVH实现使内存流量仅传统方法18%，量化箱体/三角坐标直接光追，带宽受限设备收益显著

Conclusion: 该集成方案同步解决内存带宽限制与数值精度问题，适用于移动设备等带宽敏感场景

Abstract: Memory bandwidth constraints continue to be a significant limiting factor in
ray tracing performance, particularly as scene complexity grows and
computational capabilities outpace memory access speeds. This paper presents a
memory-efficient ray tracing methodology that integrates compressed data
structures with ray stream techniques to reduce memory traffic. The approach
implements compressed BVH and triangle representations to minimize acceleration
structure size in combination with ray stream tracing to reduce traversal stack
memory traffic. The technique employs fixed-point arithmetic for intersection
tests for prospective hardware with tailored integer operations. Despite using
reduced precision, geometric holes are avoided by leveraging fixed-point
arithmetic instead of encountering the floating-point rounding errors common in
traditional approaches. Quantitative analysis demonstrates significant memory
traffic reduction across various scene complexities and BVH configurations. The
presented 8-wide BVH ray stream implementation reduces memory traffic to only
18% of traditional approaches by using 8-bit quantization for box and triangle
coordinates and directly ray tracing these quantized structures. These
reductions are especially beneficial for bandwidth-constrained hardware
environments such as mobile devices. This integrated approach addresses both
memory bandwidth limitations and numerical precision challenges inherent to
modern ray tracing applications.

</details>


### [108] [TC-GS: A Faster Gaussian Splatting Module Utilizing Tensor Cores](https://arxiv.org/abs/2505.24796)
*Zimu Liao,Jifeng Ding,Rong Fu,Siwei Cui,Ruixuan Gong,Li Wang,Boni Hu,Yi Wang,Hengjie Li,XIngcheng Zhang,Hui Wang*

Main category: cs.GR

TL;DR: 通过将Tensor Core应用于3D高斯渲染管线，实现算法无关的5.6倍加速同时保持画质


<details>
  <summary>Details</summary>
Motivation: 解决3DGS渲染流程中条件alpha混合计算耗时的问题，利用现有实现中闲置的Tensor Core硬件资源

Method: 1. 将alpha计算映射为矩阵乘法运算
2. 设计全局-局部坐标转换解决半精度计算误差
3. 兼容高斯压缩/冗余消除等现有加速算法

Result: 在现有加速算法基础上实现额外2.18倍加速，总加速比达5.6倍，且保持PSNR指标不变

Conclusion: TC-GS模块提供即插即用的Tensor Core加速方案，与现有优化框架无缝集成，为3DGS渲染性能提升开辟新方向

Abstract: 3D Gaussian Splatting (3DGS) renders pixels by rasterizing Gaussian
primitives, where conditional alpha-blending dominates the time cost in the
rendering pipeline. This paper proposes TC-GS, an algorithm-independent
universal module that expands Tensor Core (TCU) applicability for 3DGS, leading
to substantial speedups and seamless integration into existing 3DGS
optimization frameworks. The key innovation lies in mapping alpha computation
to matrix multiplication, fully utilizing otherwise idle TCUs in existing 3DGS
implementations. TC-GS provides plug-and-play acceleration for existing
top-tier acceleration algorithms tightly coupled with rendering pipeline
designs, like Gaussian compression and redundancy elimination algorithms.
Additionally, we introduce a global-to-local coordinate transformation to
mitigate rounding errors from quadratic terms of pixel coordinates caused by
Tensor Core half-precision computation. Extensive experiments demonstrate that
our method maintains rendering quality while providing an additional 2.18x
speedup over existing Gaussian acceleration algorithms, thus reaching up to a
total 5.6x acceleration. The code is currently available at anonymous
\href{https://github.com/TensorCore3DGS/3DGSTensorCore}

</details>
