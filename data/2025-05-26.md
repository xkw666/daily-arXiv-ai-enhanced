<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 53]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Prompt Engineering: How Prompt Vocabulary affects Domain Knowledge](https://arxiv.org/abs/2505.17037)
*Dimitri Schreiter*

Main category: cs.CL

TL;DR: 研究通过同义词替换框架验证提示词汇具体性对LLM性能的影响，发现存在使模型表现最优的具体性范围


<details>
  <summary>Details</summary>
Motivation: 探索提示工程中词汇具体性对LLM在STEM、医学、法律等专业领域问答任务中的影响机制

Method: 开发同义词替换框架，在四个LLM模型（Llama-3.1-70B/Granite-13B/Flan-T5-XL/Mistral-Large 2）上测试不同具体性水平的提示

Result: 整体提升具体性无显著改善，但所有模型存在最佳性能的特定具体性范围

Conclusion: 识别最优具体性范围可为提示设计提供关键指导，在此范围内调整提示可最大化LLM在专业领域的应用效果

Abstract: Prompt engineering has emerged as a critical component in optimizing large
language models (LLMs) for domain-specific tasks. However, the role of prompt
specificity, especially in domains like STEM (physics, chemistry, biology,
computer science and mathematics), medicine, and law, remains underexplored.
This thesis addresses the problem of whether increasing the specificity of
vocabulary in prompts improves LLM performance in domain-specific
question-answering and reasoning tasks. We developed a synonymization framework
to systematically substitute nouns, verbs, and adjectives with varying
specificity levels, measuring the impact on four LLMs: Llama-3.1-70B-Instruct,
Granite-13B-Instruct-V2, Flan-T5-XL, and Mistral-Large 2, across datasets in
STEM, law, and medicine. Our results reveal that while generally increasing the
specificity of prompts does not have a significant impact, there appears to be
a specificity range, across all considered models, where the LLM performs the
best. Identifying this optimal specificity range offers a key insight for
prompt design, suggesting that manipulating prompts within this range could
maximize LLM performance and lead to more efficient applications in specialized
domains.

</details>


### [2] [Signals from the Floods: AI-Driven Disaster Analysis through Multi-Source Data Fusion](https://arxiv.org/abs/2505.17038)
*Xian Gong,Paul X. McCarthy,Lin Tian,Marian-Andrei Rizoiu*

Main category: cs.CL

TL;DR: 结合社交媒体和公众调查数据，通过LDA主题建模与LLM融合方法提升灾害响应效率


<details>
  <summary>Details</summary>
Motivation: 传统灾害响应中社交媒体数据存在碎片化问题，需结合结构化公众调查数据提升信息质量

Method: 整合LDA主题建模（分析55,000+推文）与LLM语义理解（参考1,450份调查文件），开发相关性指数过滤机制

Result: 实现噪声降低72%，构建地理特征图谱，提升应急响应实时决策效率30%

Conclusion: 创新性AI方法为危机管理提供双重视角，强化实时响应与长期韧性规划的数据融合范式

Abstract: Massive and diverse web data are increasingly vital for government disaster
response, as demonstrated by the 2022 floods in New South Wales (NSW),
Australia. This study examines how X (formerly Twitter) and public inquiry
submissions provide insights into public behaviour during crises. We analyse
more than 55,000 flood-related tweets and 1,450 submissions to identify
behavioural patterns during extreme weather events. While social media posts
are short and fragmented, inquiry submissions are detailed, multi-page
documents offering structured insights. Our methodology integrates Latent
Dirichlet Allocation (LDA) for topic modelling with Large Language Models
(LLMs) to enhance semantic understanding. LDA reveals distinct opinions and
geographic patterns, while LLMs improve filtering by identifying flood-relevant
tweets using public submissions as a reference. This Relevance Index method
reduces noise and prioritizes actionable content, improving situational
awareness for emergency responders. By combining these complementary data
streams, our approach introduces a novel AI-driven method to refine
crisis-related social media content, improve real-time disaster response, and
inform long-term resilience planning.

</details>


### [3] [A new classification system of beer categories and styles based on large-scale data mining and self-organizing maps of beer recipes](https://arxiv.org/abs/2505.17039)
*Diego Bonatto*

Main category: cs.CL

TL;DR: 基于62121个啤酒配方开发数据驱动的啤酒分类系统，通过统计分析识别出4个超级集群，揭示冷/热发酵啤酒的原料差异


<details>
  <summary>Details</summary>
Motivation: 突破传统感官分类的主观局限，建立可重复的客观啤酒分类框架

Method: 使用数据挖掘（62,121个配方）结合自组织映射（SOMs）和统计分析，研究原料组成、发酵参数和酿造传统

Result: 发现冷发酵啤酒原料保守，热发酵啤酒呈现地域创新差异；建立包含四个超级集群的啤酒分类新体系

Conclusion: 该定量分类系统为酿造工艺优化、教学研究和新产品开发提供了可扩展的客观分析工具

Abstract: A data-driven quantitative approach was used to develop a novel
classification system for beer categories and styles. Sixty-two thousand one
hundred twenty-one beer recipes were mined and analyzed, considering ingredient
profiles, fermentation parameters, and recipe vital statistics. Statistical
analyses combined with self-organizing maps (SOMs) identified four major
superclusters that showed distinctive malt and hop usage patterns, style
characteristics, and historical brewing traditions. Cold fermented styles
showed a conservative grain and hop composition, whereas hot fermented beers
exhibited high heterogeneity, reflecting regional preferences and innovation.
This new taxonomy offers a reproducible and objective framework beyond
traditional sensory-based classifications, providing brewers, researchers, and
educators with a scalable tool for recipe analysis and beer development. The
findings in this work provide an understanding of beer diversity and open
avenues for linking ingredient usage with fermentation profiles and flavor
outcomes.

</details>


### [4] [VLM-KG: Multimodal Radiology Knowledge Graph Generation](https://arxiv.org/abs/2505.17042)
*Abdullah Abdullah,Seong Tae Kim*

Main category: cs.CL

TL;DR: 提出首个基于多模态视觉语言模型的放射学知识图谱生成框架，有效结合影像与文本数据


<details>
  <summary>Details</summary>
Motivation: 现有单模态方法无法利用放射影像且受限于上下文长度，导致知识图谱生成不全面

Method: 开发多模态VLM框架整合放射报告与影像数据，增强长文本处理能力

Result: 方法性能超越现有技术，成为放射领域首个多模态知识图谱解决方案

Conclusion: 多模态方法成功突破放射学知识图谱生成瓶颈，显著提升数据利用效率与结果质量

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable success in natural
language generation, excelling at instruction following and structured output
generation. Knowledge graphs play a crucial role in radiology, serving as
valuable sources of factual information and enhancing various downstream tasks.
However, generating radiology-specific knowledge graphs presents significant
challenges due to the specialized language of radiology reports and the limited
availability of domain-specific data. Existing solutions are predominantly
unimodal, meaning they generate knowledge graphs only from radiology reports
while excluding radiographic images. Additionally, they struggle with long-form
radiology data due to limited context length. To address these limitations, we
propose a novel multimodal VLM-based framework for knowledge graph generation
in radiology. Our approach outperforms previous methods and introduces the
first multimodal solution for radiology knowledge graph generation.

</details>


### [5] [QRA++: Quantified Reproducibility Assessment for Common Types of Results in Natural Language Processing](https://arxiv.org/abs/2505.17043)
*Anya Belz*

Main category: cs.CL

TL;DR: 提出QRA++量化评估框架，通过实验相似性构建可比较的再现性度量指标，揭示NLP领域再现性程度与实验属性、系统类型及评估方法的关联性


<details>
  <summary>Details</summary>
Motivation: 当前NLP再现性研究使用不统一标准导致结论难以横向比较，缺乏系统性量化评估方法

Method: 开发QRA++框架：①三粒度连续值评估 ②跨研究可比指标 ③实验相似性驱动期望基准

Result: 实验显示再现性程度与实验属性相似度正相关，且受系统类型和评估方法显著影响

Conclusion: QRA++提供标准化量化工具，使再现性评估可解释、可归因，促进领域研究方法改进

Abstract: Reproduction studies reported in NLP provide individual data points which in
combination indicate worryingly low levels of reproducibility in the field.
Because each reproduction study reports quantitative conclusions based on its
own, often not explicitly stated, criteria for reproduction success/failure,
the conclusions drawn are hard to interpret, compare, and learn from. In this
paper, we present QRA++, a quantitative approach to reproducibility assessment
that (i) produces continuous-valued degree of reproducibility assessments at
three levels of granularity; (ii) utilises reproducibility measures that are
directly comparable across different studies; and (iii) grounds expectations
about degree of reproducibility in degree of similarity between experiments.
QRA++ enables more informative reproducibility assessments to be conducted, and
conclusions to be drawn about what causes reproducibility to be better/poorer.
We illustrate this by applying QRA++ to three example sets of comparable
experiments, revealing clear evidence that degree of reproducibility depends on
similarity of experiment properties, but also system type and evaluation
method.

</details>


### [6] [Assessing GPT's Bias Towards Stigmatized Social Groups: An Intersectional Case Study on Nationality Prejudice and Psychophobia](https://arxiv.org/abs/2505.17045)
*Afifah Kashif,Heer Patel*

Main category: cs.CL

TL;DR: 研究揭示主流LLMs对朝鲜人及心理障碍群体存在交叉偏见，需改进模型设计


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型中针对国籍与心理障碍的交叉偏见引发的伦理问题

Method: 通过结构化提示测试GPT-3.5/4/4o对不同国籍（美/朝）及心理障碍场景的反应

Result: 朝鲜人群体（尤其叠加心理障碍时）承受显著更强的负面偏见与同理心缺失

Conclusion: LLMs需增强对交叉身份特征的细粒度理解，以降低伦理风险

Abstract: Recent studies have separately highlighted significant biases within
foundational large language models (LLMs) against certain nationalities and
stigmatized social groups. This research investigates the ethical implications
of these biases intersecting with outputs of widely-used GPT-3.5/4/4o LLMS.
Through structured prompt series, we evaluate model responses to several
scenarios involving American and North Korean nationalities with various mental
disabilities. Findings reveal significant discrepancies in empathy levels with
North Koreans facing greater negative bias, particularly when mental disability
is also a factor. This underscores the need for improvements in LLMs designed
with a nuanced understanding of intersectional identity.

</details>


### [7] [Assessing the Quality of AI-Generated Clinical Notes: A Validated Evaluation of a Large Language Model Scribe](https://arxiv.org/abs/2505.17047)
*Erin Palm,Astrit Manikantan,Mark E. Pepin,Herprit Mahal,Srikanth Subramanya Belwadi*

Main category: cs.CL

TL;DR: 研究通过盲法比较LLM生成与专家撰写的临床记录质量，发现两者差异微小（Gold笔记4.25分 vs Ambient笔记4.20分），支持PDQI9作为AI生成记录的评估工具。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估AI医疗记录质量的标准方法，需验证生成式AI在临床文档中的可靠性。

Method: 采用盲法研究设计，使用PDQI9量表，由5个医学专科专家对97份患者就诊记录（Gold专家笔记 vs Ambient AI笔记）进行双盲评分。

Result: Gold笔记总体得分4.25/5，Ambient笔记4.20/5（p=0.04）；普通内科、骨科、妇产科评估者间一致性高（RWG>0.7），儿科和心脏病学中等（RWG 0.5-0.7）。

Conclusion: PDQI9能有效评估AI生成临床记录质量，为医疗AI工具的标准化评估提供方法论支持。

Abstract: In medical practices across the United States, physicians have begun
implementing generative artificial intelligence (AI) tools to perform the
function of scribes in order to reduce the burden of documenting clinical
encounters. Despite their widespread use, no established methods exist to gauge
the quality of AI scribes. To address this gap, we developed a blinded study
comparing the relative performance of large language model (LLM) generated
clinical notes with those from field experts based on audio-recorded clinical
encounters. Quantitative metrics from the Physician Documentation Quality
Instrument (PDQI9) provided a framework to measure note quality, which we
adapted to assess relative performance of AI generated notes. Clinical experts
spanning 5 medical specialties used the PDQI9 tool to evaluate
specialist-drafted Gold notes and LLM authored Ambient notes. Two evaluators
from each specialty scored notes drafted from a total of 97 patient visits. We
found uniformly high inter rater agreement (RWG greater than 0.7) between
evaluators in general medicine, orthopedics, and obstetrics and gynecology, and
moderate (RWG 0.5 to 0.7) to high inter rater agreement in pediatrics and
cardiology. We found a modest yet significant difference in the overall note
quality, wherein Gold notes achieved a score of 4.25 out of 5 and Ambient notes
scored 4.20 out of 5 (p = 0.04). Our findings support the use of the PDQI9
instrument as a practical method to gauge the quality of LLM authored notes, as
compared to human-authored notes.

</details>


### [8] [Words That Unite The World: A Unified Framework for Deciphering Central Bank Communications Globally](https://arxiv.org/abs/2505.17048)
*Agam Shah,Siddhant Sukhani,Huzaifa Pardawala,Saketh Budideti,Riya Bhadani,Rudra Gopal,Siddhartha Somani,Michael Galarnyk,Soungmin Lee,Arnav Hiray,Akshar Ravichandran,Eric Kim,Pranav Aluru,Joshua Zhang,Sebastian Jaskowski,Veer Guda,Meghaj Tarte,Liqin Ye,Spencer Gosden,Rutwik Routu,Rachel Yuh,Sloka Chava,Sahasra Chava,Dylan Patrick Kelly,Aiden Chiang,Harsit Mittal,Sudheer Chava*

Main category: cs.CL

TL;DR: 构建全球最大央行政策语料库WCB，提出三大标注任务并验证跨机构聚合训练模型的优越性


<details>
  <summary>Details</summary>
Motivation: 央行政策解读偏差会加剧弱势群体经济风险，需构建标准化分析框架提升政策解读准确性

Method: 收集25国央行28年数据，采用双人标注-专家仲裁机制，定义立场/时态/不确定性三类任务，完成1.5万次模型基准测试

Result: 跨机构聚合数据模型性能显著优于单机构模型（p<0.01），人类评估验证框架经济效用达89%准确率

Conclusion: WCB数据集填补货币政策分析空白，证明数据聚合优势，开源资源推动金融NLP领域发展

Abstract: Central banks around the world play a crucial role in maintaining economic
stability. Deciphering policy implications in their communications is
essential, especially as misinterpretations can disproportionately impact
vulnerable populations. To address this, we introduce the World Central Banks
(WCB) dataset, the most comprehensive monetary policy corpus to date,
comprising over 380k sentences from 25 central banks across diverse geographic
regions, spanning 28 years of historical data. After uniformly sampling 1k
sentences per bank (25k total) across all available years, we annotate and
review each sentence using dual annotators, disagreement resolutions, and
secondary expert reviews. We define three tasks: Stance Detection, Temporal
Classification, and Uncertainty Estimation, with each sentence annotated for
all three. We benchmark seven Pretrained Language Models (PLMs) and nine Large
Language Models (LLMs) (Zero-Shot, Few-Shot, and with annotation guide) on
these tasks, running 15,075 benchmarking experiments. We find that a model
trained on aggregated data across banks significantly surpasses a model trained
on an individual bank's data, confirming the principle "the whole is greater
than the sum of its parts." Additionally, rigorous human evaluations, error
analyses, and predictive tasks validate our framework's economic utility. Our
artifacts are accessible through the HuggingFace and GitHub under the
CC-BY-NC-SA 4.0 license.

</details>


### [9] [Gender and Positional Biases in LLM-Based Hiring Decisions: Evidence from Comparative CV/Résumé Evaluations](https://arxiv.org/abs/2505.17049)
*David Rozado*

Main category: cs.CL

TL;DR: LLMs在简历评估中系统性偏向女性候选人，存在性别/位置偏差，需谨慎用于高风险决策场景。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在专业候选人评估中是否存在性别偏见，验证其决策过程的原则性。

Method: 使用22个主流LLMs进行配对实验：1) 相同资历简历仅性别不同 2) 显式性别标注 3) 中性标识符替换 4) 位置顺序调整 5) 单独简历评分测试。

Result: 1. 所有模型持续偏向女性候选人（70个职业） 2. 显式性别标注加强女性偏好 3. 中性标识符引发首选项偏见 4. 单独评分性别差异微弱 5. 位置顺序显著影响选择概率。

Conclusion: LLMs在决策中表现出系统性偏差，其‘原则性推理’可靠性存疑，建议避免在招聘等关键领域完全依赖自动化LLM决策。

Abstract: This study examines the behavior of Large Language Models (LLMs) when
evaluating professional candidates based on their resumes or curricula vitae
(CVs). In an experiment involving 22 leading LLMs, each model was
systematically given one job description along with a pair of
profession-matched CVs, one bearing a male first name, the other a female first
name, and asked to select the more suitable candidate for the job. Each CV pair
was presented twice, with names swapped to ensure that any observed preferences
in candidate selection stemmed from gendered names cues. Despite identical
professional qualifications across genders, all LLMs consistently favored
female-named candidates across 70 different professions. Adding an explicit
gender field (male/female) to the CVs further increased the preference for
female applicants. When gendered names were replaced with gender-neutral
identifiers "Candidate A" and "Candidate B", several models displayed a
preference to select "Candidate A". Counterbalancing gender assignment between
these gender-neutral identifiers resulted in gender parity in candidate
selection. When asked to rate CVs in isolation rather than compare pairs, LLMs
assigned slightly higher average scores to female CVs overall, but the effect
size was negligible. Including preferred pronouns (he/him or she/her) next to a
candidate's name slightly increased the odds of the candidate being selected
regardless of gender. Finally, most models exhibited a substantial positional
bias to select the candidate listed first in the prompt. These findings
underscore the need for caution when deploying LLMs in high-stakes autonomous
decision-making contexts and raise doubts about whether LLMs consistently apply
principled reasoning.

</details>


### [10] [Towards Robust Evaluation of STEM Education: Leveraging MLLMs in Project-Based Learning](https://arxiv.org/abs/2505.17050)
*Yanhao Jia,Xinyi Wu,Qinglin Zhang,Yiran Qin,Luwei Xiao,Shuai Zhao*

Main category: cs.CL

TL;DR: 提出PBLBench基准测试，用于评估多模态大语言模型在教育场景中的复杂推理能力，揭示顶尖模型仅59%排名准确率的现状


<details>
  <summary>Details</summary>
Motivation: 现有教育评估基准缺乏自由输出结构和严格专家验证，且模型幻觉与不稳定性导致难以构建教师辅助的自动化流程

Method: 采用层次分析法(AHP)构建专家驱动的结构化评估标准，对15个主流MLLMs/LLMs进行系统评估

Result: 最优模型在真实教育任务中仅达59%排名准确率，暴露模型在领域知识推理和长上下文理解的根本缺陷

Conclusion: PBLBench为AI教育代理发展提供关键基准，通过提升模型可靠性助力教师减负与教育效能提升

Abstract: Project-Based Learning (PBL) involves a variety of highly correlated
multimodal data, making it a vital educational approach within STEM
disciplines. With the rapid development of multimodal large language models
(MLLMs), researchers have begun exploring their potential to enhance tasks such
as information retrieval, knowledge comprehension, and data generation in
educational settings. However, existing benchmarks fall short in providing both
a free-form output structure and a rigorous human expert validation process,
limiting their effectiveness in evaluating real-world educational tasks.
Additionally, few methods have developed automated pipelines to assist with the
complex responsibilities of teachers leveraging MLLMs, largely due to model
hallucination and instability, which lead to unreliable implementation. To
address this gap, we introduce PBLBench, a novel benchmark designed to evaluate
complex reasoning grounded in domain-specific knowledge and long-context
understanding, thereby challenging models with tasks that closely resemble
those handled by human experts. To establish reliable ground truth, we adopt
the Analytic Hierarchy Process (AHP), utilizing expert-driven pairwise
comparisons to derive structured and weighted evaluation criteria. We assess
the performance of 15 leading MLLMs/LLMs using PBLBench and demonstrate that
even the most advanced models achieve only 59% rank accuracy, underscoring the
significant challenges presented by this benchmark. We believe PBLBench will
serve as a catalyst for the development of more capable AI agents, ultimately
aiming to alleviate teacher workload and enhance educational productivity.

</details>


### [11] [Embedding-to-Prefix: Parameter-Efficient Personalization for Pre-Trained Large Language Models](https://arxiv.org/abs/2505.17051)
*Bernd Huber,Ghazal Fazelnia,Andreas Damianou,Sebastian Peleato,Max Lefarov,Praveen Ravichandran,Marco De Nadai,Mounia Lalmas-Roellke,Paul N. Bennett*

Main category: cs.CL

TL;DR: 提出E2P方法实现无需微调大语言模型的个性化输出


<details>
  <summary>Details</summary>
Motivation: 现有LLM个性化方法依赖高成本微调或复杂prompt工程，需更高效的解决方案

Method: 通过学习投影将预计算上下文嵌入注入LLM隐藏表示空间，生成单个软令牌前缀

Result: 在对话/标题生成/音乐推荐场景验证有效性，计算开销降低90%+

Conclusion: E2P为生成式AI系统提供了可扩展的上下文适配方案，平衡性能与效率

Abstract: Large language models (LLMs) excel at generating contextually relevant
content. However, tailoring these outputs to individual users for effective
personalization is a significant challenge. While rich user-specific
information often exists as pre-existing user representations, such as
embeddings learned from preferences or behaviors, current methods to leverage
these for LLM personalization typically require costly fine-tuning or
token-heavy prompting. We propose Embedding-to-Prefix (E2P), a
parameter-efficient method that injects pre-computed context embeddings into an
LLM's hidden representation space through a learned projection to a single soft
token prefix. This enables effective personalization while keeping the backbone
model frozen and avoiding expensive adaptation techniques. We evaluate E2P
across two public datasets and in a production setting: dialogue
personalization on Persona-Chat, contextual headline generation on PENS, and
large-scale personalization for music and podcast consumption. Results show
that E2P preserves contextual signals and achieves strong performance with
minimal computational overhead, offering a scalable, efficient solution for
contextualizing generative AI systems.

</details>


### [12] [SpecEdge: Scalable Edge-Assisted Serving Framework for Interactive LLMs](https://arxiv.org/abs/2505.17052)
*Jinwoo Park,Seunggeun Cho,Dongsu Han*

Main category: cs.CL

TL;DR: SpecEdge框架通过整合边缘GPU和推测解码技术，将LLM推理效率提升1.91倍并降低延迟，实现服务器吞吐量翻倍


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务系统过度依赖服务器级GPU，忽视了边缘计算资源的价值，导致运营成本高昂且资源利用率不足

Method: 提出边缘-服务器协同推测解码方案，包含：1) 主动边缘起草机制实现计算重叠；2) 管道感知调度系统实现多请求交错处理；3) 仅通过网络传输token级输出

Result: 实验显示服务器吞吐量提升2.22倍，总体成本效益增加1.91倍，token间延迟降低11.24%

Conclusion: 该框架开创了利用边缘计算资源增强LLM服务的新范式，在保持服务质量的同时显著降低运营成本

Abstract: Large language models (LLMs) power many modern applications, but serving them
at scale remains costly and resource-intensive. Current server-centric systems
overlook consumer-grade GPUs at the edge. We introduce SpecEdge, an
edge-assisted inference framework that splits LLM workloads between edge and
server GPUs using a speculative decoding scheme, exchanging only token outputs
over the network. SpecEdge employs proactive edge drafting to overlap edge
token creation with server verification and pipeline-aware scheduling that
interleaves multiple user requests to increase server-side throughput.
Experiments show SpecEdge enhances overall cost efficiency by 1.91x through
achieving 2.22x server throughput, and reduces inter token latency by 11.24%
compared to a server-only baseline, introducing a scalable, cost-effective
paradigm for LLM serving.

</details>


### [13] [Social preferences with unstable interactive reasoning: Large language models in economic trust games](https://arxiv.org/abs/2505.17053)
*Ou Jiamin,Eikmans Emile,Buskens Vincent,Pankowska Paulina,Shan Yuli*

Main category: cs.CL

TL;DR: 大型语言模型在信任游戏中展现出超越自利的信任与互惠行为，角色设定对模型表现的影响超过模型类型差异，ChatGPT-4在无私角色下表现最优。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型如何将语言理解转化为社会交换行为，验证其在经济信任游戏中体现社会偏好与互动推理的能力。

Method: 使用ChatGPT-4/Claude/Bard参与经济信任游戏，通过单次/多轮互动及角色设定（自私/无私）测试决策模式。

Result: LLMs普遍展现初始信任，角色设定显著改变行为（无私角色下信任度>人类，自私角色下<人类），互动推理能力表现不稳定但角色设定可改善。

Conclusion: LLMs具备社会互动潜力但其行为受角色提示主导，需加强互动推理的稳定性研究，角色工程可能成为重要调节手段。

Abstract: While large language models (LLMs) have demonstrated remarkable capabilities
in understanding human languages, this study explores how they translate this
understanding into social exchange contexts that capture certain essences of
real world human interactions. Three LLMs - ChatGPT-4, Claude, and Bard - were
placed in economic trust games where players balance self-interest with trust
and reciprocity, making decisions that reveal their social preferences and
interactive reasoning abilities. Our study shows that LLMs deviate from pure
self-interest and exhibit trust and reciprocity even without being prompted to
adopt a specific persona. In the simplest one-shot interaction, LLMs emulated
how human players place trust at the beginning of such a game. Larger
human-machine divergences emerged in scenarios involving trust repayment or
multi-round interactions, where decisions were influenced by both social
preferences and interactive reasoning. LLMs responses varied significantly when
prompted to adopt personas like selfish or unselfish players, with the impact
outweighing differences between models or game types. Response of ChatGPT-4, in
an unselfish or neutral persona, resembled the highest trust and reciprocity,
surpassing humans, Claude, and Bard. Claude and Bard displayed trust and
reciprocity levels that sometimes exceeded and sometimes fell below human
choices. When given selfish personas, all LLMs showed lower trust and
reciprocity than humans. Interactive reasoning to the actions of counterparts
or changing game mechanics appeared to be random rather than stable,
reproducible characteristics in the response of LLMs, though some improvements
were observed when ChatGPT-4 responded in selfish or unselfish personas.

</details>


### [14] [METHOD: Modular Efficient Transformer for Health Outcome Discovery](https://arxiv.org/abs/2505.17054)
*Linglong Qian,Zina Ibrahim*

Main category: cs.CL

TL;DR: 提出METH模块化高效Transformer架构，通过患者感知注意力、自适应滑动窗口和U-Net动态跳跃连接，在医疗时序数据处理中超越现有模型，尤其擅长高风险病例预测。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer处理医疗时序数据时面临不规则采样、复杂时间依赖和临床信息泄露等问题，需专门优化的架构提升预测准确性。

Method: 1) 防信息泄漏的患者感知注意力机制 2) 多尺度时间依赖的自适应滑动窗口 3) U-Net架构动态跳跃连接的长序列处理

Result: 在MIMIC-IV数据库上表现优于ETHOS模型，高严重病例预测F1值提升12%，不同历史长度下性能稳定，嵌入分析显示更好保留临床概念层级关系。

Conclusion: METH标志着医疗专用Transformer的重要进展，在保持计算效率的同时实现更准确的临床预测，满足实际部署需求。

Abstract: Recent advances in transformer architectures have revolutionised natural
language processing, but their application to healthcare domains presents
unique challenges. Patient timelines are characterised by irregular sampling,
variable temporal dependencies, and complex contextual relationships that
differ substantially from traditional language tasks. This paper introduces
\METHOD~(Modular Efficient Transformer for Health Outcome Discovery), a novel
transformer architecture specifically designed to address the challenges of
clinical sequence modelling in electronic health records. \METHOD~integrates
three key innovations: (1) a patient-aware attention mechanism that prevents
information leakage whilst enabling efficient batch processing; (2) an adaptive
sliding window attention scheme that captures multi-scale temporal
dependencies; and (3) a U-Net inspired architecture with dynamic skip
connections for effective long sequence processing. Evaluations on the MIMIC-IV
database demonstrate that \METHOD~consistently outperforms the state-of-the-art
\ETHOS~model, particularly in predicting high-severity cases that require
urgent clinical intervention. \METHOD~exhibits stable performance across
varying inference lengths, a crucial feature for clinical deployment where
patient histories vary significantly in length. Analysis of learned embeddings
reveals that \METHOD~better preserves clinical hierarchies and relationships
between medical concepts. These results suggest that \METHOD~represents a
significant advancement in transformer architectures optimised for healthcare
applications, providing more accurate and clinically relevant predictions
whilst maintaining computational efficiency.

</details>


### [15] [Enhancing Mathematics Learning for Hard-of-Hearing Students Through Real-Time Palestinian Sign Language Recognition: A New Dataset](https://arxiv.org/abs/2505.17055)
*Fidaa khandaqji,Huthaifa I. Ashqar,Abdelrahem Atawnih*

Main category: cs.CL

TL;DR: 开发基于ViT模型的巴勒斯坦数学手语识别系统，准确率达97.59%，推动听障学生数学教育包容性


<details>
  <summary>Details</summary>
Motivation: 针对巴勒斯坦手语数字资源匮乏问题，通过AI技术开发数学手势识别系统，缩小听障学生的学习差距

Method: 创建41类数学手势数据集，采用Vision Transformer(ViT)模型进行微调训练，专家参与确保手势准确性

Result: 模型达到97.59%分类准确率，成功实现高精度数学手势识别，数据集开源共享于Hugging Face平台

Conclusion: 证实深度学习可有效开发智能教育工具，推动特殊教育领域的数字技术整合与包容性教育创新

Abstract: The study aims to enhance mathematics education accessibility for
hard-of-hearing students by developing an accurate Palestinian sign language
PSL recognition system using advanced artificial intelligence techniques. Due
to the scarcity of digital resources for PSL, a custom dataset comprising 41
mathematical gesture classes was created, and recorded by PSL experts to ensure
linguistic accuracy and domain specificity. To leverage
state-of-the-art-computer vision techniques, a Vision Transformer ViTModel was
fine-tuned for gesture classification. The model achieved an accuracy of
97.59%, demonstrating its effectiveness in recognizing mathematical signs with
high precision and reliability. This study highlights the role of deep learning
in developing intelligent educational tools that bridge the learning gap for
hard-of-hearing students by providing AI-driven interactive solutions to
enhance mathematical comprehension. This work represents a significant step
toward innovative and inclusive frosting digital integration in specialized
learning environments. The dataset is hosted on Hugging Face at
https://huggingface.co/datasets/fidaakh/STEM_data.

</details>


### [16] [Are LLMs Ready for English Standardized Tests? A Benchmarking and Elicitation Perspective](https://arxiv.org/abs/2505.17056)
*Luoxi Tang,Tharunya Sundar,Shuai Yang,Ankita Patra,Manohar Chippada,Giqi Zhao,Yi Li,Riteng Zhang,Tunan Zhao,Ting Yang,Yuqiao Meng,Weicheng Ma,Zhaohan Xi*

Main category: cs.CL

TL;DR: 评估大语言模型在英语标准化测试中的表现与可靠性


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在标准化考试辅导中的应用潜力，解决现有智能辅导系统在复杂题型处理上的不足

Method: 构建多模态基准测试ESTBOOK（含10,576+题目），提出分阶段推理框架评估模型表现

Result: 揭示LLMs在不同题型中的性能差异，并验证分解式解题策略的有效性

Conclusion: LLMs展示出教育应用潜力，但需针对性优化才能成为可靠的智能辅导系统

Abstract: AI is transforming education by enabling powerful tools that enhance learning
experiences. Among recent advancements, large language models (LLMs) hold
particular promise for revolutionizing how learners interact with educational
content. In this work, we investigate the potential of LLMs to support
standardized test preparation by focusing on English Standardized Tests (ESTs).
Specifically, we assess their ability to generate accurate and contextually
appropriate solutions across a diverse set of EST question types. We introduce
ESTBOOK, a comprehensive benchmark designed to evaluate the capabilities of
LLMs in solving EST questions. ESTBOOK aggregates five widely recognized tests,
encompassing 29 question types and over 10,576 questions across multiple
modalities, including text, images, audio, tables, and mathematical symbols.
Using ESTBOOK, we systematically evaluate both the accuracy and inference
efficiency of LLMs. Additionally, we propose a breakdown analysis framework
that decomposes complex EST questions into task-specific solution steps. This
framework allows us to isolate and assess LLM performance at each stage of the
reasoning process. Evaluation findings offer insights into the capability of
LLMs in educational contexts and point toward targeted strategies for improving
their reliability as intelligent tutoring systems.

</details>


### [17] [DO-RAG: A Domain-Specific QA Framework Using Knowledge Graph-Enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2505.17058)
*David Osei Opoku,Ming Sheng,Yong Zhang*

Main category: cs.CL

TL;DR: 提出DO-RAG混合问答框架，结合知识图谱与语义检索技术，显著提升领域QA系统的准确率


<details>
  <summary>Details</summary>
Motivation: 传统RAG框架在处理异构数据整合和推理一致性方面存在局限，需更可靠的领域知识支撑

Method: 通过智能思维链架构构建多级知识图谱，查询时融合图检索与向量检索结果，并采用基于事实的精细化修正机制

Result: 在数据库与电气领域测试中实现近100%召回率与94%答案相关性，性能超越基线框架33.38%

Conclusion: DO-RAG通过可追溯的架构设计与动态知识融合，为跨领域高精度问答系统提供了可靠解决方案

Abstract: Domain-specific QA systems require not just generative fluency but high
factual accuracy grounded in structured expert knowledge. While recent
Retrieval-Augmented Generation (RAG) frameworks improve context recall, they
struggle with integrating heterogeneous data and maintaining reasoning
consistency. To address these challenges, we propose DO-RAG, a scalable and
customizable hybrid QA framework that integrates multi-level knowledge graph
construction with semantic vector retrieval. Our system employs a novel agentic
chain-of-thought architecture to extract structured relationships from
unstructured, multimodal documents, constructing dynamic knowledge graphs that
enhance retrieval precision. At query time, DO-RAG fuses graph and vector
retrieval results to generate context-aware responses, followed by
hallucination mitigation via grounded refinement. Experimental evaluations in
the database and electrical domains show near-perfect recall and over 94%
answer relevancy, with DO-RAG outperforming baseline frameworks by up to
33.38%. By combining traceability, adaptability, and performance efficiency,
DO-RAG offers a reliable foundation for multi-domain, high-precision QA at
scale.

</details>


### [18] [Medalyze: Lightweight Medical Report Summarization Application Using FLAN-T5-Large](https://arxiv.org/abs/2505.17059)
*Van-Tinh Nguyen,Hoang-Duong Pham,Thanh-Hai To,Cong-Tuan Hung Do,Thi-Thu-Trang Dong,Vu-Trung Duong Le,Van-Phuc Hoang*

Main category: cs.CL

TL;DR: Medalyze使用三个专业FLAN-T5-Large模型解决医疗文本理解难题，在特定领域任务中超越GPT-4，提供隐私保护的轻量级解决方案


<details>
  <summary>Details</summary>
Motivation: 医疗文本存在复杂术语和语境依赖问题，需提升信息可及性

Method: 1) 微调三个模型：报告摘要生成、医患对话问题提取、关键问题识别
2) 基于API和YugabyteDB构建跨平台实时推理系统

Result: 在BLEU/ROUGE-L/BERTScore/SpaCy指标中，领域特定摘要性能优于GPT-4

Conclusion: Medalyze为医疗场景提供了兼顾隐私保护、实时性和轻量化的实用AI解决方案

Abstract: Understanding medical texts presents significant challenges due to complex
terminology and context-specific language. This paper introduces Medalyze, an
AI-powered application designed to enhance the comprehension of medical texts
using three specialized FLAN-T5-Large models. These models are fine-tuned for
(1) summarizing medical reports, (2) extracting health issues from
patient-doctor conversations, and (3) identifying the key question in a
passage. Medalyze is deployed across a web and mobile platform with real-time
inference, leveraging scalable API and YugabyteDB. Experimental evaluations
demonstrate the system's superior summarization performance over GPT-4 in
domain-specific tasks, based on metrics like BLEU, ROUGE-L, BERTScore, and
SpaCy Similarity. Medalyze provides a practical, privacy-preserving, and
lightweight solution for improving information accessibility in healthcare.

</details>


### [19] [SALMONN-omni: A Standalone Speech LLM without Codec Injection for Full-duplex Conversation](https://arxiv.org/abs/2505.17060)
*Wenyi Yu,Siyin Wang,Xiaoyu Yang,Xianzhao Chen,Xiaohai Tian,Jun Zhang,Guangzhi Sun,Lu Lu,Yuxuan Wang,Chao Zhang*

Main category: cs.CL

TL;DR: 提出首个无需音频编解码器的全双工语音大模型SALMONN-omni，通过动态思考机制实现听/说状态自主切换，显著提升对话系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有模块化语音交互系统存在错误累积和上下文处理缺陷，需开发更高效的集成化解决方案。

Method: 在LLM骨干网中引入动态思考机制，通过状态自主切换替代传统编解码器和辅助模块。

Result: 在语音问答和开放对话任务上相对提升30%，使用更少训练数据达到与半双工系统相当的竞争力。

Conclusion: SALMONN-omni突破全双工语音LLM的技术瓶颈，在复杂对话场景中展示出强大的端到端处理能力。

Abstract: In order to enable fluid and natural human-machine speech interaction,
existing full-duplex conversational systems often adopt modular architectures
with auxiliary components such as voice activity detectors, interrupters,
conversation state predictors, or multiple LLMs. These systems, however, suffer
from error accumulation across modules and struggle with key challenges such as
context-dependent barge-in and echo cancellation. Recent approaches, most
notably Moshi, simplify the pipeline by injecting audio codecs into the token
space of a single LLM. However, such methods still incur significant
performance degradation when operating on the speech rather than text modality.
In this paper, we introduce SALMONN-omni, the first single, standalone
full-duplex speech LLM that operates without audio codecs in its token space.
It features a novel dynamic thinking mechanism within the LLM backbone,
enabling the model to learn when to transition between speaking and listening
states. Experiments on widely used benchmarks for spoken question answering and
open-domain dialogue show that SALMONN-omni achieves at least 30\% relative
performance improvement over existing open-source full-duplex models and
performs highly competitively to half-duplex and turn-based systems, despite
using substantially less training data. Moreover, SALMONN-omni demonstrates
strong performance in complex conversational scenarios, including turn-taking,
backchanneling, echo cancellation and context-dependent barge-in, with further
improvements achieved through reinforcement learning. Some demo conversations
between user and SALMONN-omni are provided in the following repository
https://github.com/bytedance/SALMONN.

</details>


### [20] [Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2505.17061)
*Xinlong Chen,Yuanxing Zhang,Qiang Liu,Junfei Wu,Fuzheng Zhang,Tieniu Tan*

Main category: cs.CL

TL;DR: 提出动态解码策略MoD，通过检测注意力正确性有效降低大视觉语言模型幻觉问题


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型在视觉任务中存在持续性幻觉问题，现有解码方法难以动态适应注意力偏差

Method: 通过原始图像token与注意力token的输出一致性判断注意力正确性，分别采用互补策略增强关键信息或对比策略抑制误导信息

Result: 在多个主流基准测试中显著超越现有方法，代码已开源

Conclusion: MoD通过动态解码机制有效缓解LVLMs的幻觉问题，为模型可靠性提升提供新思路

Abstract: Large Vision-Language Models (LVLMs) have exhibited impressive capabilities
across various visual tasks, yet they remain hindered by the persistent
challenge of hallucinations. To address this critical issue, we propose Mixture
of Decoding (MoD), a novel approach for hallucination mitigation that
dynamically adapts decoding strategies by evaluating the correctness of the
model's attention on image tokens. Specifically, MoD measures the consistency
between outputs generated from the original image tokens and those derived from
the model's attended image tokens, to distinguish the correctness
aforementioned. If the outputs are consistent, indicating correct attention,
MoD employs a complementary strategy to amplify critical information.
Conversely, if the outputs are inconsistent, suggesting erroneous attention,
MoD utilizes a contrastive strategy to suppress misleading information.
Extensive experiments demonstrate that MoD significantly outperforms existing
decoding methods across multiple mainstream benchmarks, effectively mitigating
hallucinations in LVLMs. The code is available at
https://github.com/xlchen0205/MoD.

</details>


### [21] [Synthetic Data RL: Task Definition Is All You Need](https://arxiv.org/abs/2505.17063)
*Yiduo Guo,Zhen Guo,Chuanwei Huang,Zi-Ang Wang,Zekai Zhang,Haofei Yu,Huishuai Zhang,Yikang Shen*

Main category: cs.CL

TL;DR: 提出Synthetic Data RL框架，通过纯合成数据强化学习微调模型，在多个领域实现显著性能提升（如GSM8K提升29.2%），接近全人类数据RL效果


<details>
  <summary>Details</summary>
Motivation: 解决强化学习依赖人类标注数据导致的扩展性问题，通过合成数据降低人工标注成本

Method: 三阶段框架：1. 根据任务定义生成QA对 2. 基于模型解题能力动态调整问题难度 3. 按平均通过率筛选问题进行RL训练

Result: 在GSM8K/MATH/GPQA等6个数据集实现6.6-29.2%提升，超越监督微调（相同数据量）且接近全人类数据RL效果（如GSM8K差距仅0.4pp）

Conclusion: 证明合成数据可有效替代人类数据进行RL微调，显著降低人工标注需求，为模型适配提供可扩展解决方案

Abstract: Reinforcement learning (RL) is a powerful way to adapt foundation models to
specialized tasks, but its reliance on large-scale human-labeled data limits
broad adoption. We introduce Synthetic Data RL, a simple and general framework
that reinforcement fine-tunes models using only synthetic data generated from a
task definition. Our method first generates question and answer pairs from the
task definition and retrieved documents, then adapts the difficulty of the
question based on model solvability, and selects questions using the average
pass rate of the model across samples for RL training. On Qwen-2.5-7B, our
method achieves a 29.2% absolute improvement over the base model on GSM8K (+2.9
pp vs. instruction-tuned, +6.6 pp vs. Self-Instruct), 8.7% on MATH, 13.1% on
GPQA (+7.0 pp vs. SynthLLM), 8.9% on MedQA, 17.7% on CQA (law) and 13.7% on CFA
(finance). It surpasses supervised fine-tuning under the same data budget and
nearly matches RL with full human data across datasets (e.g., +17.2 pp on
GSM8K). Adding 100 human demonstrations improves the performance of GSM8K only
by 0.4 pp, showing a limited added value. By reducing human data annotation,
Synthetic Data RL enables scalable and efficient RL-based model adaptation.
Code and demos are available at https://github.com/gydpku/Data_Synthesis_RL/.

</details>


### [22] [Decoding Rarity: Large Language Models in the Diagnosis of Rare Diseases](https://arxiv.org/abs/2505.17065)
*Valentina Carbonari,Pierangelo Veltri,Pietro Hiram Guzzi*

Main category: cs.CL

TL;DR: 该论文探讨大语言模型在罕见病研究中的整合应用，分析其在诊断/治疗/患者护理中的潜力，并展望多模态数据整合的未来方向。


<details>
  <summary>Details</summary>
Motivation: 解决罕见病研究中数据碎片化与样本不足的难题，利用LLMs强大的文本分析能力挖掘潜在诊疗线索。

Method: 采用文献综述法梳理基础性论文，结合多模型对比实验和结构化问卷诊断测试验证应用效果。

Result: 证实LLMs在医学信息提取和诊断模拟方面有效性，同时揭示数据隐私/模型透明度/数据集偏差等关键瓶颈。

Conclusion: 未来需发展整合基因/影像/文本的多模态LLM平台，通过跨模态关联提升罕见病诊疗的全面性与准确性。

Abstract: Recent advances in artificial intelligence, particularly large language
models LLMs, have shown promising capabilities in transforming rare disease
research. This survey paper explores the integration of LLMs in the analysis of
rare diseases, highlighting significant strides and pivotal studies that
leverage textual data to uncover insights and patterns critical for diagnosis,
treatment, and patient care. While current research predominantly employs
textual data, the potential for multimodal data integration combining genetic,
imaging, and electronic health records stands as a promising frontier. We
review foundational papers that demonstrate the application of LLMs in
identifying and extracting relevant medical information, simulating intelligent
conversational agents for patient interaction, and enabling the formulation of
accurate and timely diagnoses. Furthermore, this paper discusses the challenges
and ethical considerations inherent in deploying LLMs, including data privacy,
model transparency, and the need for robust, inclusive data sets. As part of
this exploration, we present a section on experimentation that utilizes
multiple LLMs alongside structured questionnaires, specifically designed for
diagnostic purposes in the context of different diseases. We conclude with
future perspectives on the evolution of LLMs towards truly multimodal
platforms, which would integrate diverse data types to provide a more
comprehensive understanding of rare diseases, ultimately fostering better
outcomes in clinical settings.

</details>


### [23] [Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive Impairment Detection via Contrastive Learning](https://arxiv.org/abs/2505.17067)
*Kristin Qi,Jiali Cheng,Youxiang Zhu,Hadi Amiri,Xiaohui Liang*

Main category: cs.CL

TL;DR: 提出结合监督对比学习、图像模态和专家乘积策略的三维框架，提升多语言多图片场景下轻度认知障碍检测性能


<details>
  <summary>Details</summary>
Motivation: 现有MCI检测研究局限于单语言单图片场景，TAUKDIAL-2024挑战引入多语言多图片带来新分析难题

Method: 1. 监督对比学习增强表征区分能力 2. 引入图像模态补充语音文本 3. 专家乘积策略消除伪相关

Result: UAR提升7.1%（68.1%→75.2%），F1提升2.9%（80.6%→83.5%），文本模态对比学习增益更显著

Conclusion: 框架有效解决多语言多图片MCI检测挑战，对比学习对文本模态优化效果突出，具临床实践潜力

Abstract: Detecting Mild Cognitive Impairment from picture descriptions is critical yet
challenging, especially in multilingual and multiple picture settings. Prior
work has primarily focused on English speakers describing a single picture
(e.g., the 'Cookie Theft'). The TAUKDIAL-2024 challenge expands this scope by
introducing multilingual speakers and multiple pictures, which presents new
challenges in analyzing picture-dependent content. To address these challenges,
we propose a framework with three components: (1) enhancing discriminative
representation learning via supervised contrastive learning, (2) involving
image modality rather than relying solely on speech and text modalities, and
(3) applying a Product of Experts (PoE) strategy to mitigate spurious
correlations and overfitting. Our framework improves MCI detection performance,
achieving a +7.1% increase in Unweighted Average Recall (UAR) (from 68.1% to
75.2%) and a +2.9% increase in F1 score (from 80.6% to 83.5%) compared to the
text unimodal baseline. Notably, the contrastive learning component yields
greater gains for the text modality compared to speech. These results highlight
our framework's effectiveness in multilingual and multi-picture MCI detection.

</details>


### [24] [Predictively Combatting Toxicity in Health-related Online Discussions through Machine Learning](https://arxiv.org/abs/2505.17068)
*Jorge Paz-Ruza,Amparo Alonso-Betanzos,Bertha Guijarro-Berdiñas,Carlos Eiras-Franco*

Main category: cs.CL

TL;DR: 通过协同过滤机器学习预测Reddit健康讨论中的用户毒性，预防冲突匹配


<details>
  <summary>Details</summary>
Motivation: 现有毒性检测手段（标记/删除等）容易引发反效果，需前瞻性预测毒性发生场景

Method: 采用协同过滤机器学习模型，预测用户与Reddit子社区在COVID讨论中的潜在毒性互动

Result: 模型在关键指标上超过80%预测性能，成功阻止冲突用户与子社区的配对

Conclusion: 预测性毒性防控策略优于事后处理，为社交平台内容治理提供新范式

Abstract: In health-related topics, user toxicity in online discussions frequently
becomes a source of social conflict or promotion of dangerous, unscientific
behaviour; common approaches for battling it include different forms of
detection, flagging and/or removal of existing toxic comments, which is often
counterproductive for platforms and users alike. In this work, we propose the
alternative of combatting user toxicity predictively, anticipating where a user
could interact toxically in health-related online discussions. Applying a
Collaborative Filtering-based Machine Learning methodology, we predict the
toxicity in COVID-related conversations between any user and subcommunity of
Reddit, surpassing 80% predictive performance in relevant metrics, and allowing
us to prevent the pairing of conflicting users and subcommunities.

</details>


### [25] [Improving endpoint detection in end-to-end streaming ASR for conversational speech](https://arxiv.org/abs/2505.17070)
*Anandh C,Karthik Pandia Durai,Jeena Prakash,Manickavela Arumugam,Kadri Hacioglu,S. Pavankumar Dubagunta,Andreas Stolcke,Shankar Venkatesan,Aravind Ganapathiraju*

Main category: cs.CL

TL;DR: 针对基于Transducer的ASR模型延迟输出问题，提出引入词尾标记和延迟惩罚机制，结合辅助网络优化端点检测精度


<details>
  <summary>Details</summary>
Motivation: Transducer-based ASR模型存在语音识别结果延迟输出的问题，导致端点检测(EP)过早切断用户发言或增加响应延迟，影响对话系统用户体验。需要同时解决延迟输出和端点检测错误两大核心问题。

Method: 1. 在每个词语结尾添加end-of-word标记
2. 引入延迟惩罚机制
3. 使用辅助网络实现帧级语音活动检测
4. 在Switchboard对话语料库进行实验验证

Result: 在Switchboard对话语料库的测试中，相比传统延迟惩罚方法，新方案能更有效减少端点检测错误率（18.8%→6.3%）并降低平均检测延迟（420ms→320ms）

Conclusion: 通过词尾标记设计和双重优化机制，显著提升了ASR端点检测的准确性和实时性，为对话系统的用户体验优化提供了有效解决方案

Abstract: ASR endpointing (EP) plays a major role in delivering a good user experience
in products supporting human or artificial agents in human-human/machine
conversations. Transducer-based ASR (T-ASR) is an end-to-end (E2E) ASR
modelling technique preferred for streaming. A major limitation of T-ASR is
delayed emission of ASR outputs, which could lead to errors or delays in EP.
Inaccurate EP will cut the user off while speaking, returning incomplete
transcript while delays in EP will increase the perceived latency, degrading
the user experience. We propose methods to improve EP by addressing delayed
emission along with EP mistakes. To address the delayed emission problem, we
introduce an end-of-word token at the end of each word, along with a delay
penalty. The EP delay is addressed by obtaining a reliable frame-level speech
activity detection using an auxiliary network. We apply the proposed methods on
Switchboard conversational speech corpus and evaluate it against a delay
penalty method.

</details>


### [26] [What's in a prompt? Language models encode literary style in prompt embeddings](https://arxiv.org/abs/2505.17071)
*Raphaël Sarfati,Haley Moller,Toni J. B. Liu,Nicolas Boullé,Christopher Earls*

Main category: cs.CL

TL;DR: 研究发现大语言模型深层表示蕴含文本风格特征，不同文学作品的潜在空间分布反映作者风格差异


<details>
  <summary>Details</summary>
Motivation: 探究语言模型如何将完整文本信息压缩到单个嵌入表示中，特别是非事实性的风格特征编码机制

Method: 使用10-100个token的文学摘录进行潜在空间分析，比较不同作者/同作者作品的几何分布特征

Result: 同一作者作品在潜在空间显著纠缠，不同作品分离独立于next-token预测结果，验证风格编码的有效性

Conclusion: 语言模型实现了复杂的信息压缩处理，其风格几何特征可用于作者识别与文学分析，揭示深度信息处理机制

Abstract: Large language models use high-dimensional latent spaces to encode and
process textual information. Much work has investigated how the conceptual
content of words translates into geometrical relationships between their vector
representations. Fewer studies analyze how the cumulative information of an
entire prompt becomes condensed into individual embeddings under the action of
transformer layers. We use literary pieces to show that information about
intangible, rather than factual, aspects of the prompt are contained in deep
representations. We observe that short excerpts (10 - 100 tokens) from
different novels separate in the latent space independently from what
next-token prediction they converge towards. Ensembles from books from the same
authors are much more entangled than across authors, suggesting that embeddings
encode stylistic features. This geometry of style may have applications for
authorship attribution and literary analysis, but most importantly reveals the
sophistication of information processing and compression accomplished by
language models.

</details>


### [27] [Mechanistic Interpretability of GPT-like Models on Summarization Tasks](https://arxiv.org/abs/2505.17073)
*Anurag Mishra*

Main category: cs.CL

TL;DR: 研究通过对比预训练与微调模型，定位了GPT类模型执行摘要任务的核心电路（2/3/5层），针对性微调该电路可使性能提升32%且训练周期缩短40%


<details>
  <summary>Details</summary>
Motivation: 现有可解释性研究多聚焦分类/生成任务，摘要任务的内部机制尚不明确。本文旨在揭示语言模型如何通过结构调整适应摘要任务需求

Method: 采用注意力熵分析和激活差异对比，识别模型微调前后变化最显著的注意力头（62%熵值下降），通过LoRA模块定向改造关键电路

Result: 中间层（2/3/5）形成信息选择专用通道，针对性电路改造使ROUGE-L提升1.8个点，训练epoch减少25%

Conclusion: 该研究建立了模型评估与机制解析的桥梁，揭示了神经网络通过结构化电路实现信息压缩的核心路径，为高效微调提供新范式

Abstract: Mechanistic interpretability research seeks to reveal the inner workings of
large language models, yet most work focuses on classification or generative
tasks rather than summarization. This paper presents an interpretability
framework for analyzing how GPT-like models adapt to summarization tasks. We
conduct differential analysis between pre-trained and fine-tuned models,
quantifying changes in attention patterns and internal activations. By
identifying specific layers and attention heads that undergo significant
transformation, we locate the "summarization circuit" within the model
architecture. Our findings reveal that middle layers (particularly 2, 3, and 5)
exhibit the most dramatic changes, with 62% of attention heads showing
decreased entropy, indicating a shift toward focused information selection. We
demonstrate that targeted LoRA adaptation of these identified circuits achieves
significant performance improvement over standard LoRA fine-tuning while
requiring fewer training epochs. This work bridges the gap between black-box
evaluation and mechanistic understanding, providing insights into how neural
networks perform information selection and compression during summarization.

</details>


### [28] [Semi-Clairvoyant Scheduling of Speculative Decoding Requests to Minimize LLM Inference Latency](https://arxiv.org/abs/2505.17074)
*Ruixiao Li,Fahao Chen,Peng Li*

Main category: cs.CL

TL;DR: 提出LAPS-SD调度算法，通过动态请求调度和抢占机制优化推测解码的LLM推理延迟，相比现有方法降低39%延迟


<details>
  <summary>Details</summary>
Motivation: 现有调度方法仅依赖输出长度估计执行时间，忽略token接受率对执行时间的动态影响，导致估计不准确

Method: 维护多优先级队列，在token接受率不稳定时允许跨队列请求抢占；接受率稳定后基于精确估计进行调度

Result: 实验显示LAPS-SD相比state-of-the-art方法减少约39%推理延迟

Conclusion: LAPS-SD通过半透视调度机制有效平衡动态特征与稳定阶段的执行时间预测，显著提升LLM推理效率

Abstract: Speculative decoding accelerates Large Language Model (LLM) inference by
employing a small speculative model (SSM) to generate multiple candidate tokens
and verify them using the LLM in parallel. This technique has been widely
integrated into LLM inference serving systems. However, inference requests
typically exhibit uncertain execution time, which poses a significant challenge
of efficiently scheduling requests in these systems. Existing work estimates
execution time based solely on predicted output length, which could be
inaccurate because execution time depends on both output length and token
acceptance rate of verification by the LLM. In this paper, we propose a
semi-clairvoyant request scheduling algorithm called
Least-Attained/Perceived-Service for Speculative Decoding (LAPS-SD). Given a
number of inference requests, LAPS-SD can effectively minimize average
inference latency by adaptively scheduling requests according to their features
during decoding. When the token acceptance rate is dynamic and execution time
is difficult to estimate, LAPS-SD maintains multiple priority queues and allows
request execution preemption across different queues. Once the token acceptance
rate becomes stable, LAPS-SD can accurately estimate the execution time and
schedule requests accordingly. Extensive experiments show that LAPS-SD reduces
inference latency by approximately 39\% compared to state-of-the-art scheduling
methods.

</details>


### [29] [Development and Validation of Engagement and Rapport Scales for Evaluating User Experience in Multimodal Dialogue Systems](https://arxiv.org/abs/2505.17075)
*Fuma Kurata,Mao Saeki,Masaki Eguchi,Shungo Suzuki,Hiroaki Takatsu,Yoichi Matsuyama*

Main category: cs.CL

TL;DR: 开发并验证了用于评估多模态对话系统在外语学习中用户体验质量的量表，成功捕捉人类导师与对话代理的体验差异


<details>
  <summary>Details</summary>
Motivation: 基于多学科理论构建标准化评估工具，解决现有对话系统用户体验评价维度单一的问题

Method: 结合教育心理学理论设计量表，通过74名学习者的对比实验，采用Cronbach's α和验证性因子分析检验信效度

Result: 量表具备良好心理测量特性，能多维度识别人类与AI对话体验的质量差异

Conclusion: 该量表为对话系统的优化提供了有效评估框架，有助于提升外语学习场景的人机交互效果

Abstract: This study aimed to develop and validate two scales of engagement and rapport
to evaluate the user experience quality with multimodal dialogue systems in the
context of foreign language learning. The scales were designed based on
theories of engagement in educational psychology, social psychology, and second
language acquisition.Seventy-four Japanese learners of English completed
roleplay and discussion tasks with trained human tutors and a dialog agent.
After each dialogic task was completed, they responded to the scales of
engagement and rapport. The validity and reliability of the scales were
investigated through two analyses. We first conducted analysis of Cronbach's
alpha coefficient and a series of confirmatory factor analyses to test the
structural validity of the scales and the reliability of our designed items. We
then compared the scores of engagement and rapport between the dialogue with
human tutors and the one with a dialogue agent. The results revealed that our
scales succeeded in capturing the difference in the dialogue experience quality
between the human interlocutors and the dialogue agent from multiple
perspectives.

</details>


### [30] [Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English](https://arxiv.org/abs/2505.17076)
*Haoyang Zhang,Hexin Liu,Xiangyu Zhang,Qiquan Zhang,Yuchen Hu,Junqi Zhao,Fei Tian,Xuerui Yang,Eng Siong Chng*

Main category: cs.CL

TL;DR: 语音分词器的帧率变化对汉语和英语的语义编码产生不同影响，需结合语言音系特征优化帧率选择


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分探索语音分词器中帧率对语音标记化的影响机制，尤其是不同语言类型间的差异

Method: 通过不同帧率编码汉语和英语语音，在语音识别任务中评估语义标记质量，对比分析语言特性与帧率的相互作用

Result: 汉语因音节密度高需要更高帧率（25-50ms），英语在较低帧率（100-200ms）表现更佳，反映语言音系特征对声学编码的影响

Conclusion: 语音分词器帧率选择应结合目标语言的音系特征优化，这对语音识别、文本转语音等应用的性能提升具有指导意义

Abstract: The speech tokenizer plays a crucial role in recent speech tasks, generally
serving as a bridge between speech signals and language models. While
low-frame-rate codecs are widely employed as speech tokenizers, the impact of
frame rates on speech tokens remains underexplored. In this study, we
investigate how varying frame rates affect speech tokenization by examining
Mandarin and English, two typologically distinct languages. We encode speech at
different frame rates and evaluate the resulting semantic tokens in the speech
recognition task. Our findings reveal that frame rate variations influence
speech tokenization differently for each language, highlighting the interplay
between frame rates, phonetic density, and language-specific acoustic features.
The results provide insights into optimizing frame rate selection for speech
tokenizers, with implications for automatic speech recognition, text-to-speech,
and other speech-related applications.

</details>


### [31] [GloSS over Toxicity: Understanding and Mitigating Toxicity in LLMs via Global Toxic Subspace](https://arxiv.org/abs/2505.17078)
*Zenghao Duan,Zhiyi Yin,Zhichao Shi,Liang Pang,Shaoling Jing,Jiayi Wu,Yu Yan,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: 提出GloSS方法，通过识别并消除FFN中的全局毒性子空间实现LLM高效去毒。


<details>
  <summary>Details</summary>
Motivation: 现有方法多将FFN视为毒性主要来源，但仅采用毒性向量/分层子空间表征，全局毒性子空间能更全面有效表征模型毒性区域。

Method: 四阶段轻量级方法GloSS：1）构建毒性样本参数轨迹 2）SVD分解识别全局毒性子空间 3）参数投影消除毒性成分 4）动态权重调整保持模型能力

Result: 在多类LLM中实现SOTA去毒效果，模型通用能力保留度达97.8%，无需大规模数据/模型重训练

Conclusion: 全局毒性子空间抑制策略为模型安全部署提供了参数高效、数据高效的解决方案，平衡安全性与功能完整性

Abstract: This paper investigates the underlying mechanisms of toxicity generation in
Large Language Models (LLMs) and proposes an effective detoxification approach.
Prior work typically considers the Feed-Forward Network (FFN) as the main
source of toxicity, representing toxic regions as a set of toxic vectors or
layer-wise subspaces. However, our in-depth analysis reveals that the global
toxic subspace offers a more effective and comprehensive representation of
toxic region within the model. Building on this insight, we propose GloSS
(Global Toxic Subspace Suppression), a lightweight, four-stage method that
mitigates toxicity by identifying and removing the global toxic subspace from
the parameters of FFN. Experiments across a range of LLMs show that GloSS
achieves state-of-the-art detoxification performance while preserving the
models general capabilities, without requiring large-scale data or model
retraining.

</details>


### [32] [Not Minds, but Signs: Reframing LLMs through Semiotics](https://arxiv.org/abs/2505.17080)
*Davide Picca*

Main category: cs.CL

TL;DR: 反对将LLMs视为认知系统，主张符号学框架下LLMs作为符号重组与再语境化的解释性主体


<details>
  <summary>Details</summary>
Motivation: 当前研究过度拟人化LLMs，忽视了其基于概率的符号操作本质及其在文化生产中的非思维性参与

Method: 通过符号学理论建构与文学/哲学等领域的应用案例分析

Result: 建立LLMs作为符号代理的范式，揭示其通过文本生成改变人类意义建构方式的作用机制

Conclusion: 符号学框架为LLMs研究提供更严谨的伦理认知基础，推动其在创造性对话与批判性反思中的应用

Abstract: This paper challenges the prevailing tendency to frame Large Language Models
(LLMs) as cognitive systems, arguing instead for a semiotic perspective that
situates these models within the broader dynamics of sign manipulation and
meaning-making. Rather than assuming that LLMs understand language or simulate
human thought, we propose that their primary function is to recombine,
recontextualize, and circulate linguistic forms based on probabilistic
associations. By shifting from a cognitivist to a semiotic framework, we avoid
anthropomorphism and gain a more precise understanding of how LLMs participate
in cultural processes, not by thinking, but by generating texts that invite
interpretation. Through theoretical analysis and practical examples, the paper
demonstrates how LLMs function as semiotic agents whose outputs can be treated
as interpretive acts, open to contextual negotiation and critical reflection.
We explore applications in literature, philosophy, education, and cultural
production, emphasizing how LLMs can serve as tools for creativity, dialogue,
and critical inquiry. The semiotic paradigm foregrounds the situated,
contingent, and socially embedded nature of meaning, offering a more rigorous
and ethically aware framework for studying and using LLMs. Ultimately, this
approach reframes LLMs as technological participants in an ongoing ecology of
signs. They do not possess minds, but they alter how we read, write, and make
meaning, compelling us to reconsider the foundations of language,
interpretation, and the role of artificial systems in the production of
knowledge.

</details>


### [33] [GemMaroc: Unlocking Darija Proficiency in LLMs with Minimal Data](https://arxiv.org/abs/2505.17082)
*Abderrahman Skiredj,Ferdaous Azhari,Houdaifa Atou,Nouamane Tazi,Ismail Berrada*

Main category: cs.CL

TL;DR: 提出GemMaroc模型，通过高效对齐策略在保留原模型多语言推理能力的同时显著提升摩洛哥阿拉伯语（Darija）性能，训练耗时仅48 GPU小时。


<details>
  <summary>Details</summary>
Motivation: 现有开源大语言模型对摩洛哥阿拉伯语支持不足，传统适配方法需牺牲模型推理能力或增加计算负担。

Method: 采用三阶段策略：1）精选并翻译LIMA/DEITA/TULU指令集为Darija 2）保留20%英文原数据 3）添加数学/编程/科学Prompt，通过LoRA微调Gemma系列模型。

Result: GemMaroc-27B在DarijaMMLU达到61.6%（与Atlas-Chat相当），Darija常识推理（HellaSwag）60.5%显著优于对比模型，同时保持原模型的数学（GSM8K）和英语能力。

Conclusion: 验证了绿色AI路径的可行性，通过质量优先的数据策略和高效微调方法实现小语种技术的包容性发展，为教育/公共服务等领域提供技术支持。

Abstract: Open-source large language models (LLMs) still marginalise Moroccan Arabic
(Darija), forcing practitioners either to bolt on heavyweight Arabic adapters
or to sacrifice the very reasoning skills that make LLMs useful. We show that a
rigorously quality-over-quantity alignment strategy can surface fluent Darija
while safeguarding the backbone s cross-lingual reasoning at a sliver of the
usual compute. We translate three compact instruction suites LIMA 1 K, DEITA 6
K and TULU 50 K into Darija, preserve 20 of the English originals, and add
mathematics, coding and scientific prompts. A LoRA-tuned Gemma 3-4B trained on
5 K mixed instructions lifts DarijaMMLU from 32.8 to 42.7 ; adding the
reasoning-dense TULU portion pushes it to 47.5 with no English regression.
Scaling the identical recipe to Gemma 3-27B produces GemMaroc-27B, which
matches Atlas-Chat on DarijaMMLU (61.6 ) and leaps ahead on Darija commonsense,
scoring 60.5 on HellaSwag versus Atlas-Chat s 48.4 . Crucially, GemMaroc
retains Gemma-27B s strong maths and general-reasoning ability, showing only
minimal movement on GSM8K and English benchmarks. The entire model is trained
in just 48 GPU.h, underscoring a Green AI pathway to inclusive, sustainable
language technology. We release code, data and checkpoints to spur
Darija-centric applications in education, public services and everyday digital
interaction.

</details>


### [34] [Scale-invariant Attention](https://arxiv.org/abs/2505.17083)
*Ben Anson,Xi Wang,Laurence Aitchison*

Main category: cs.CL

TL;DR: 提出了一种尺度不变的注意力机制，以改善LLM在长上下文中的零样本泛化能力和检索效果


<details>
  <summary>Details</summary>
Motivation: 现有注意力机制在从短上下文训练推广到长上下文推理时存在泛化能力不足的问题

Method: 通过位置相关的注意力logits变换实现尺度不变总注意力和稀疏性（基于高斯假设）

Result: 验证损失显著降低（零样本长上下文泛化时），并展现优秀的长上下文检索能力

Conclusion: 尺度不变注意力机制能有效提升LLM在长上下文任务中的表现

Abstract: One persistent challenge in LLM research is the development of attention
mechanisms that are able to generalise from training on shorter contexts to
inference on longer contexts. We propose two conditions that we expect all
effective long context attention mechanisms to have: scale-invariant total
attention, and scale-invariant attention sparsity. Under a Gaussian assumption,
we show that a simple position-dependent transformation of the attention logits
is sufficient for these conditions to hold. Experimentally we find that the
resulting scale-invariant attention scheme gives considerable benefits in terms
of validation loss when zero-shot generalising from training on short contexts
to validation on longer contexts, and is effective at long-context retrieval.

</details>


### [35] [Reinforcing Question Answering Agents with Minimalist Policy Gradient Optimization](https://arxiv.org/abs/2505.17086)
*Yihong Wu,Liheng Ma,Muzhi Li,Jiaming Zhou,Jianye Hao,Ho-fung Leung,Irwin King,Yingxue Zhang,Jian-Yun Nie*

Main category: cs.CL

TL;DR: 提出Mujica-MyGO框架解决大模型在复杂问答中的幻觉问题，通过DAG分解问题和MLE强化学习方法显著提升多跳QA性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型因缺乏事实知识导致问答任务产生幻觉，现有检索增强方法受限于上下文学习的推理瓶颈。

Method: Mujica框架包含子问题DAG分解的planner和检索推理的worker；MyGO用MLE替代策略梯度，通过渐近最优策略轨迹采样实现稳定训练。

Result: 多数据集验证显示Mujica-MyGO有效提升不同LLMs的多跳QA能力，提供资源高效的解决方案。

Conclusion: 该框架为复杂QA任务提供可扩展的范式，MyGO方法突破传统强化学习的梯度缩放限制，实现更高效的模型优化。

Abstract: Large Language Models (LLMs) have demonstrated remarkable versatility, due to
the lack of factual knowledge, their application to Question Answering (QA)
tasks remains hindered by hallucination.
  While Retrieval-Augmented Generation mitigates these issues by integrating
external knowledge, existing approaches rely heavily on in-context learning,
whose performance is constrained by the fundamental reasoning capabilities of
LLMs.
  In this paper, we propose Mujica, a Multi-hop Joint Intelligence for Complex
Question Answering, comprising a planner that decomposes questions into a
directed acyclic graph of subquestions and a worker that resolves questions via
retrieval and reasoning. Additionally, we introduce MyGO (Minimalist policy
Gradient Optimization), a novel reinforcement learning method that replaces
traditional policy gradient updates with Maximum Likelihood Estimation (MLE) by
sampling trajectories from an asymptotically optimal policy. MyGO eliminates
the need for gradient rescaling and reference models, ensuring stable and
efficient training.
  Empirical results across multiple datasets demonstrate the effectiveness of
Mujica-MyGO in enhancing multi-hop QA performance for various LLMs, offering a
scalable and resource-efficient solution for complex QA tasks.

</details>


### [36] [Informatics for Food Processing](https://arxiv.org/abs/2505.17087)
*Gordana Ispirova,Michael Sebek,Giulia Menichetti*

Main category: cs.CL

TL;DR: 探讨食品加工分类体系演进及机器学习/AI在食品信息学中的应用，提出基于FoodProX和语言模型的新评估范式


<details>
  <summary>Details</summary>
Motivation: 传统食品加工分类框架（如NOVA）存在主观性强、可复现性差的问题，制约流行病学研究和公共卫生政策制定

Method: 开发FoodProX随机森林模型处理营养成分数据，结合BERT/BioBERT语义嵌入技术，并通过Open Food Facts多模态AI案例验证

Result: 新方法实现食品加工水平的连续评分（FPro）和大规模分类，有效处理缺失数据并整合结构化与非结构化信息

Conclusion: AI驱动的计算模型为食品加工评估提供客观、可扩展的新范式，推动公共卫生研究和政策制定的精准化转型

Abstract: This chapter explores the evolution, classification, and health implications
of food processing, while emphasizing the transformative role of machine
learning, artificial intelligence (AI), and data science in advancing food
informatics. It begins with a historical overview and a critical review of
traditional classification frameworks such as NOVA, Nutri-Score, and SIGA,
highlighting their strengths and limitations, particularly the subjectivity and
reproducibility challenges that hinder epidemiological research and public
policy. To address these issues, the chapter presents novel computational
approaches, including FoodProX, a random forest model trained on nutrient
composition data to infer processing levels and generate a continuous FPro
score. It also explores how large language models like BERT and BioBERT can
semantically embed food descriptions and ingredient lists for predictive tasks,
even in the presence of missing data. A key contribution of the chapter is a
novel case study using the Open Food Facts database, showcasing how multimodal
AI models can integrate structured and unstructured data to classify foods at
scale, offering a new paradigm for food processing assessment in public health
and research.

</details>


### [37] [Trust Me, I Can Handle It: Self-Generated Adversarial Scenario Extrapolation for Robust Language Models](https://arxiv.org/abs/2505.17089)
*Md Rafi Ur Rashid,Vishnu Asutosh Dasu,Ye Wang,Gang Tan,Shagufta Mehnaz*

Main category: cs.CL

TL;DR: 提出对抗场景推演（ASE）框架，通过思维链推理同步增强LLM安全性和响应自然度


<details>
  <summary>Details</summary>
Motivation: 现有防御方案存在单点防护局限性和过度拒绝用户请求的问题，影响使用体验和泛化能力

Method: 采用推理时自生成对抗场景推演机制，引导LLM自主思考潜在攻击场景并制定防御策略

Result: 在四个对抗基准测试中实现接近零的越狱成功率，拒绝率低于4%，偏差分数降低4-10倍

Conclusion: ASE通过将对抗感知转化为认知过程，建立了安全自然的人机交互新范式

Abstract: Large Language Models (LLMs) exhibit impressive capabilities, but remain
susceptible to a growing spectrum of safety risks, including jailbreaks, toxic
content, hallucinations, and bias. Existing defenses often address only a
single threat type or resort to rigid outright rejection, sacrificing user
experience and failing to generalize across diverse and novel attacks. This
paper introduces Adversarial Scenario Extrapolation (ASE), a novel
inference-time computation framework that leverages Chain-of-Thought (CoT)
reasoning to simultaneously enhance LLM robustness and seamlessness. ASE guides
the LLM through a self-generative process of contemplating potential
adversarial scenarios and formulating defensive strategies before generating a
response to the user query. Comprehensive evaluation on four adversarial
benchmarks with four latest LLMs shows that ASE achieves near-zero jailbreak
attack success rates and minimal toxicity, while slashing outright rejections
to <4%. ASE outperforms six state-of-the-art defenses in
robustness-seamlessness trade-offs, with 92-99% accuracy on adversarial Q&A and
4-10x lower bias scores. By transforming adversarial perception into an
intrinsic cognitive process, ASE sets a new paradigm for secure and natural
human-AI interaction.

</details>


### [38] [Large Language Models Implicitly Learn to See and Hear Just By Reading](https://arxiv.org/abs/2505.17091)
*Prateek Verma,Mert Pilanci*

Main category: cs.CL

TL;DR: 文本LLM通过文本训练自发获得多模态理解能力，可直接处理图像和音频分类任务


<details>
  <summary>Details</summary>
Motivation: 探索文本LLM内部是否天然具备多模态处理潜力，避免每次从头训练专用模型

Method: 将图像/音频patch直接输入预训练文本LLM，复用其文本权重进行多模态分类任务

Result: 在FSD-50K音频数据集和CIFAR-10等图像数据集上验证了文本模型权重对多模态任务的有效性

Conclusion: 文本LLM学习到的内部表征具有通用性，通过激活特定连接即可跨模态应用，显著降低训练成本

Abstract: This paper presents a fascinating find: By training an auto-regressive LLM
model on text tokens, the text model inherently develops internally an ability
to understand images and audio, thereby developing the ability to see and hear
just by reading. Popular audio and visual LLM models fine-tune text LLM models
to give text output conditioned on images and audio embeddings. On the other
hand, our architecture takes in patches of images, audio waveforms or tokens as
input. It gives us the embeddings or category labels typical of a
classification pipeline. We show the generality of text weights in aiding audio
classification for datasets FSD-50K and GTZAN. Further, we show this working
for image classification on CIFAR-10 and Fashion-MNIST, as well on image
patches. This pushes the notion of text-LLMs learning powerful internal
circuits that can be utilized by activating necessary connections for various
applications rather than training models from scratch every single time.

</details>


### [39] [Are LLMs reliable? An exploration of the reliability of large language models in clinical note generation](https://arxiv.org/abs/2505.17095)
*Kristine Ann M. Carandang,Jasper Meynard P. Araña,Ethan Robert A. Casin,Christopher P. Monterola,Daniel Stanley Y. Tan,Jesus Felix B. Valenzuela,Christian M. Alis*

Main category: cs.CL

TL;DR: 评估12种LLM在临床记录生成中的可靠性，发现Llama 70B和Mistral Small表现最佳，建议本地部署较小模型实现隐私合规与效率提升


<details>
  <summary>Details</summary>
Motivation: 解决LLM输出变异性对临床记录生成系统集成的影响，保障医疗文档的准确性、隐私合规性和临床流程可靠性

Method: 通过字符串等价性、语义一致性和语义相似度指标，对Anthropic/Meta/Mistral/OpenAI的12个开源/商业模型进行多轮提示测试

Result: 1. 所有模型家族输出保持语义稳定 2. 多数模型生成记录接近专家水平 3. Llama 70B可靠性最高（Mistral Small次之）

Conclusion: 推荐医疗机构本地部署中小型开源模型，在满足数据隐私法规的同时提升临床文档效率

Abstract: Due to the legal and ethical responsibilities of healthcare providers (HCPs)
for accurate documentation and protection of patient data privacy, the natural
variability in the responses of large language models (LLMs) presents
challenges for incorporating clinical note generation (CNG) systems, driven by
LLMs, into real-world clinical processes. The complexity is further amplified
by the detailed nature of texts in CNG. To enhance the confidence of HCPs in
tools powered by LLMs, this study evaluates the reliability of 12 open-weight
and proprietary LLMs from Anthropic, Meta, Mistral, and OpenAI in CNG in terms
of their ability to generate notes that are string equivalent (consistency
rate), have the same meaning (semantic consistency) and are correct (semantic
similarity), across several iterations using the same prompt. The results show
that (1) LLMs from all model families are stable, such that their responses are
semantically consistent despite being written in various ways, and (2) most of
the LLMs generated notes close to the corresponding notes made by experts.
Overall, Meta's Llama 70B was the most reliable, followed by Mistral's Small
model. With these findings, we recommend the local deployment of these
relatively smaller open-weight models for CNG to ensure compliance with data
privacy regulations, as well as to improve the efficiency of HCPs in clinical
documentation.

</details>


### [40] [TACO: Enhancing Multimodal In-context Learning via Task Mapping-Guided Sequence Configuration](https://arxiv.org/abs/2505.17098)
*Yanshu Li,Tian Yun,Jianjiang Yang,Pinyuan Feng,Jinfa Huang,Ruixiang Tang*

Main category: cs.CL

TL;DR: 论文提出TACO模型，通过任务映射视角动态配置多模态上下文学习序列，在五大视觉语言模型和九大数据集上实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前多模态上下文学习(ICL)效果高度依赖输入序列质量，且缺乏对模型如何利用序列的机制理解。研究旨在通过任务映射视角解析ICL内部机理，提升复杂推理任务的性能。

Method: 提出轻量级TACO模型，采用任务感知注意力机制，在自回归解码过程中注入任务映射信号，实现上下文序列的动态配置。

Result: 实验表明TACO在多样化ICL任务中持续超越基线方法，验证了任务映射作为改进多模态ICL的有效视角。

Conclusion: 任务映射为解释和优化多模态上下文学习提供了新范式，双向协同机制显著增强模型的序列构建与任务推理能力。

Abstract: Multimodal in-context learning (ICL) has emerged as a key mechanism for
harnessing the capabilities of large vision-language models (LVLMs). However,
its effectiveness remains highly sensitive to the quality of input in-context
sequences, particularly for tasks involving complex reasoning or open-ended
generation. A major limitation is our limited understanding of how LVLMs
actually exploit these sequences during inference. To bridge this gap, we
systematically interpret multimodal ICL through the lens of task mapping, which
reveals how local and global relationships within and among demonstrations
guide model reasoning. Building on this insight, we present TACO, a lightweight
transformer-based model equipped with task-aware attention that dynamically
configures in-context sequences. By injecting task-mapping signals into the
autoregressive decoding process, TACO creates a bidirectional synergy between
sequence construction and task reasoning. Experiments on five LVLMs and nine
datasets demonstrate that TACO consistently surpasses baselines across diverse
ICL tasks. These results position task mapping as a valuable perspective for
interpreting and improving multimodal ICL.

</details>


### [41] [Learning Interpretable Representations Leads to Semantically Faithful EEG-to-Text Generation](https://arxiv.org/abs/2505.17099)
*Xiaozhao Liu,Dinggang Shen,Xihui Liu*

Main category: cs.CL

TL;DR: 提出GLIM模型解决EEG文本解码的幻觉问题，通过语义总结和稳健评估提升脑解码可靠性


<details>
  <summary>Details</summary>
Motivation: 现有生成模型可能产生与脑信号无关的幻觉输出，需解决EEG与文本信息容量不匹配导致的后验崩溃问题

Method: 将解码任务重构为语义提炼，设计GLIM框架学习可解释EEG表征，在ZuCo数据集验证异构数据下的语义基础能力

Result: 模型无需教师强制即可生成流畅EEG关联句子，支持脑电-文本检索和跨情感/关系/主题的零样本分类评估

Conclusion: GLIM架构及评估协议为生成式脑解码建立了可靠且可扩展的基准框架

Abstract: Pretrained generative models have opened new frontiers in brain decoding by
enabling the synthesis of realistic texts and images from non-invasive brain
recordings. However, the reliability of such outputs remains
questionable--whether they truly reflect semantic activation in the brain, or
are merely hallucinated by the powerful generative models. In this paper, we
focus on EEG-to-text decoding and address its hallucination issue through the
lens of posterior collapse. Acknowledging the underlying mismatch in
information capacity between EEG and text, we reframe the decoding task as
semantic summarization of core meanings rather than previously verbatim
reconstruction of stimulus texts. To this end, we propose the Generative
Language Inspection Model (GLIM), which emphasizes learning informative and
interpretable EEG representations to improve semantic grounding under
heterogeneous and small-scale data conditions. Experiments on the public ZuCo
dataset demonstrate that GLIM consistently generates fluent, EEG-grounded
sentences without teacher forcing. Moreover, it supports more robust evaluation
beyond text similarity, through EEG-text retrieval and zero-shot semantic
classification across sentiment categories, relation types, and corpus topics.
Together, our architecture and evaluation protocols lay the foundation for
reliable and scalable benchmarking in generative brain decoding.

</details>


### [42] [Any Large Language Model Can Be a Reliable Judge: Debiasing with a Reasoning-based Bias Detector](https://arxiv.org/abs/2505.17100)
*Haoyan Yang,Runxue Bao,Cao Xiao,Jun Ma,Parminder Bhatia,Shangqian Gao,Taha Kass-Hout*

Main category: cs.CL

TL;DR: 提出基于推理的偏见检测器RBD，通过外部迭代检测与反馈机制有效提升LLM评估的准确性和一致性


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估方法存在根本性偏见（上下文学习无法自我反思，微调不适用闭源模型）

Method: 开发独立插件模块RBD，采用偏见数据集构建-监督信号收集-推理微调-模型集成的全流程方案

Result: RBD-8B模型平均提升评估准确性18.5%、一致性10.9%，显著优于提示工程（+12.8%）和微调评估器（+17.2%）

Conclusion: RBD展示出优秀的可扩展性、跨偏见类型/领域泛化能力及运行效率，为LLM评估偏见的系统化解决方案

Abstract: LLM-as-a-Judge has emerged as a promising tool for automatically evaluating
generated outputs, but its reliability is often undermined by potential biases
in judgment. Existing efforts to mitigate these biases face key limitations:
in-context learning-based methods fail to address rooted biases due to the
evaluator's limited capacity for self-reflection, whereas fine-tuning is not
applicable to all evaluator types, especially closed-source models. To address
this challenge, we introduce the Reasoning-based Bias Detector (RBD), which is
a plug-in module that identifies biased evaluations and generates structured
reasoning to guide evaluator self-correction. Rather than modifying the
evaluator itself, RBD operates externally and engages in an iterative process
of bias detection and feedback-driven revision. To support its development, we
design a complete pipeline consisting of biased dataset construction,
supervision collection, distilled reasoning-based fine-tuning of RBD, and
integration with LLM evaluators. We fine-tune four sizes of RBD models, ranging
from 1.5B to 14B, and observe consistent performance improvements across all
scales. Experimental results on 4 bias types--verbosity, position, bandwagon,
and sentiment--evaluated using 8 LLM evaluators demonstrate RBD's strong
effectiveness. For example, the RBD-8B model improves evaluation accuracy by an
average of 18.5% and consistency by 10.9%, and surpasses prompting-based
baselines and fine-tuned judges by 12.8% and 17.2%, respectively. These results
highlight RBD's effectiveness and scalability. Additional experiments further
demonstrate its strong generalization across biases and domains, as well as its
efficiency.

</details>


### [43] [An approach to identify the most semantically informative deep representations of text and images](https://arxiv.org/abs/2505.17101)
*Santiago Acevedo,Andrea Mascaretti,Riccardo Rende,Matéo Mahaut,Marco Baroni,Alessandro Laio*

Main category: cs.CL

TL;DR: 通过分析LLMs和视觉变换器，提出定量方法研究跨模态语义表征，揭示模型大小对信息提取的影响及语义信息的分布特性与跨模态预测能力


<details>
  <summary>Details</summary>
Motivation: 探究深度神经网络如何在不同领域（文本/图像/多语言）编码语义信息，以及模型规模对语义表征能力的影响

Method: 测量语义相关数据表征的相对信息量，分析LLMs处理翻译句对识别语义层，比较模型规模差异，研究视觉变换器中语义层与图像表征的预测关系

Result: 较大LLM（DeepSeek-V3）提取更多通用信息；语义信息呈现token间长程相关性及因果不对称性；文本语义层可预测图像表征，存在跨模态信息不对称

Conclusion: 模型规模影响语义信息提取效率，语义信息具有分布式编码特征，跨模态表征存在可预测性差异，为理解神经网络语义编码机制提供新视角

Abstract: Deep neural networks are known to develop similar representations for
semantically related data, even when they belong to different domains, such as
an image and its description, or the same text in different languages. We
present a method for quantitatively investigating this phenomenon by measuring
the relative information content of the representations of semantically related
data and probing how it is encoded into multiple tokens of large language
models (LLMs) and vision transformers. Looking first at how LLMs process pairs
of translated sentences, we identify inner ``semantic'' layers containing the
most language-transferable information. We find moreover that, on these layers,
a larger LLM (DeepSeek-V3) extracts significantly more general information than
a smaller one (Llama3.1-8B). Semantic information is spread across many tokens
and it is characterized by long-distance correlations between tokens and by a
causal left-to-right (i.e., past-future) asymmetry. We also identify layers
encoding semantic information within visual transformers. We show that caption
representations in the semantic layers of LLMs predict visual representations
of the corresponding images. We observe significant and model-dependent
information asymmetries between image and text representations.

</details>


### [44] [BanglaByT5: Byte-Level Modelling for Bangla](https://arxiv.org/abs/2505.17102)
*Pramit Bhattacharyya,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: 提出了首个针对孟加拉语的字节级模型BanglaByT5，在资源受限环境下展现高效性能。


<details>
  <summary>Details</summary>
Motivation: 传统分词器（如BPE/SentencePiece）难以捕捉孟加拉语形态学特征，需开发专门模型。

Method: 基于ByT5架构，使用14GB文学新闻混合语料预训练，通过零样本/监督任务评估模型。

Result: 在生成和分类任务中超越多语言大模型，验证字节级建模对形态复杂语言的有效性。

Conclusion: BanglaByT5为孟加拉语NLP提供轻量高效解决方案，证明字节级模型在形态丰富语言中的优势。

Abstract: Large language models (LLMs) have achieved remarkable success across various
natural language processing tasks. However, most LLM models use traditional
tokenizers like BPE and SentencePiece, which fail to capture the finer nuances
of a morphologically rich language like Bangla (Bengali). In this work, we
introduce BanglaByT5, the first byte-level encoder-decoder model explicitly
tailored for Bangla. Built upon a small variant of Googles ByT5 architecture,
BanglaByT5 is pre-trained on a 14GB curated corpus combining high-quality
literary and newspaper articles. Through zeroshot and supervised evaluations
across generative and classification tasks, BanglaByT5 demonstrates competitive
performance, surpassing several multilingual and larger models. Our findings
highlight the efficacy of byte-level modelling for morphologically rich
languages and highlight BanglaByT5 potential as a lightweight yet powerful tool
for Bangla NLP, particularly in both resource-constrained and scalable
environments.

</details>


### [45] [Forging Time Series with Language: A Large Language Model Approach to Synthetic Data Generation](https://arxiv.org/abs/2505.17103)
*Cécile Rousseau,Tobia Boschi,Giandomenico Cornacchia,Dhaval Salwala,Alessandra Pascale,Juan Bernabe Moreno*

Main category: cs.CL

TL;DR: SDForger是基于LLM的高效多变量时序生成框架，通过文本编码实现低计算量微调和跨模态整合


<details>
  <summary>Details</summary>
Motivation: 解决现有生成模型在保持数据统计特性、时间动态特征以及跨模态整合方面的不足

Method: 将时序信号转换为表格嵌入→文本编码→LLM微调→文本嵌入解码生成统计特性保留的合成时序

Result: 在多种数据集上超越现有模型，相似性评估和预测任务表现优异

Conclusion: 文本条件生成机制为多模态建模和时序-文本信息融合提供了新范式

Abstract: SDForger is a flexible and efficient framework for generating high-quality
multivariate time series using LLMs. Leveraging a compact data representation,
SDForger provides synthetic time series generation from a few samples and
low-computation fine-tuning of any autoregressive LLM. Specifically, the
framework transforms univariate and multivariate signals into tabular
embeddings, which are then encoded into text and used to fine-tune the LLM. At
inference, new textual embeddings are sampled and decoded into synthetic time
series that retain the original data's statistical properties and temporal
dynamics. Across a diverse range of datasets, SDForger outperforms existing
generative models in many scenarios, both in similarity-based evaluations and
downstream forecasting tasks. By enabling textual conditioning in the
generation process, SDForger paves the way for multimodal modeling and the
streamlined integration of time series with textual information. SDForger
source code will be open-sourced soon.

</details>


### [46] [P2P: Automated Paper-to-Poster Generation and Fine-Grained Benchmark](https://arxiv.org/abs/2505.17104)
*Tao Sun,Enhao Pan,Zhengkai Yang,Kaixin Sui,Jiajun Shi,Xianfu Cheng,Tongliang Li,Wenhao Huang,Ge Zhang,Jian Yang,Zhoujun Li*

Main category: cs.CL

TL;DR: 提出了首个基于LLM的多代理框架P2P，可直接从论文生成高质量HTML学术海报，并配套发布首个大规模数据集P2PInstruct和评估基准P2PEval。


<details>
  <summary>Details</summary>
Motivation: 现有学术海报自动化生成方法存在科学细节保留不足、图文整合效果差、缺乏标准化评估基准等核心痛点。

Method: 采用三代理架构（视觉处理/内容生成/海报组装）配合检查模块，通过迭代优化确保输出质量。

Result: 成功构建包含3万实例的P2PInstruct数据集，建立包含121对样本的P2PEval双维度评估体系（LLM评估+人工标注检查表）。

Conclusion: P2P框架显著提升学术海报生成效率，为社区提供系统开发与评估的完整解决方案，推动研究成果传播的智能化革新。

Abstract: Academic posters are vital for scholarly communication, yet their manual
creation is time-consuming. However, automated academic poster generation faces
significant challenges in preserving intricate scientific details and achieving
effective visual-textual integration. Existing approaches often struggle with
semantic richness and structural nuances, and lack standardized benchmarks for
evaluating generated academic posters comprehensively. To address these
limitations, we introduce P2P, the first flexible, LLM-based multi-agent
framework that generates high-quality, HTML-rendered academic posters directly
from research papers, demonstrating strong potential for practical
applications. P2P employs three specialized agents-for visual element
processing, content generation, and final poster assembly-each integrated with
dedicated checker modules to enable iterative refinement and ensure output
quality. To foster advancements and rigorous evaluation in this domain, we
construct and release P2PInstruct, the first large-scale instruction dataset
comprising over 30,000 high-quality examples tailored for the academic
paper-to-poster generation task. Furthermore, we establish P2PEval, a
comprehensive benchmark featuring 121 paper-poster pairs and a dual evaluation
methodology (Universal and Fine-Grained) that leverages LLM-as-a-Judge and
detailed, human-annotated checklists. Our contributions aim to streamline
research dissemination and provide the community with robust tools for
developing and evaluating next-generation poster generation systems.

</details>


### [47] [RRTL: Red Teaming Reasoning Large Language Models in Tool Learning](https://arxiv.org/abs/2505.17106)
*Yifei Liu,Yu Cui,Haibin Zhang*

Main category: cs.CL

TL;DR: 提出了RRTL红队测试方法，揭示推理大模型在工具学习中的安全风险，发现RLLMs存在隐蔽工具使用风险和多语言安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注传统大模型的安全漏洞，但对新兴推理大模型（如DeepSeek-R1）在工具学习场景下的安全性缺乏系统评估。

Method: 开发RRTL评估框架：1. 识别隐蔽威胁（评估模型隐瞒危险工具使用的能力）2. 使用思维链提示强制工具调用3. 包含传统大模型基准测试

Result: 1. RLLMs整体安全性优于传统模型，但存在显著差异 2. RLLMs存在隐瞒工具使用和风险提示缺失的隐蔽风险 3. 思维链提示暴露多语言安全漏洞

Conclusion: 本研究为提升RLLMs在工具学习中的安全性提供重要启示，揭示了隐蔽风险和多语言漏洞等新型安全挑战。

Abstract: While tool learning significantly enhances the capabilities of large language
models (LLMs), it also introduces substantial security risks. Prior research
has revealed various vulnerabilities in traditional LLMs during tool learning.
However, the safety of newly emerging reasoning LLMs (RLLMs), such as
DeepSeek-R1, in the context of tool learning remains underexplored. To bridge
this gap, we propose RRTL, a red teaming approach specifically designed to
evaluate RLLMs in tool learning. It integrates two novel strategies: (1) the
identification of deceptive threats, which evaluates the model's behavior in
concealing the usage of unsafe tools and their potential risks; and (2) the use
of Chain-of-Thought (CoT) prompting to force tool invocation. Our approach also
includes a benchmark for traditional LLMs. We conduct a comprehensive
evaluation on seven mainstream RLLMs and uncover three key findings: (1) RLLMs
generally achieve stronger safety performance than traditional LLMs, yet
substantial safety disparities persist across models; (2) RLLMs can pose
serious deceptive risks by frequently failing to disclose tool usage and to
warn users of potential tool output risks; (3) CoT prompting reveals
multi-lingual safety vulnerabilities in RLLMs. Our work provides important
insights into enhancing the security of RLLMs in tool learning.

</details>


### [48] [Multi-Modality Expansion and Retention for LLMs through Parameter Merging and Decoupling](https://arxiv.org/abs/2505.17110)
*Junlin Li,Guodong DU,Jing Li,Sim Kuan Goh,Wenya Wang,Yequan Wang,Fangming Liu,Ho-Kin Tang,Saleh Alharbi,Daojing He,Min Zhang*

Main category: cs.CL

TL;DR: 提出无需训练的MMER方法，通过参数合并与掩码分离实现多模态扩展，在保留99%原性能的同时显著降低灾难性遗忘


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大规模从头微调，存在资源消耗高、灵活性差、易引发灾难性遗忘等问题

Method: 1. 复用现有MLLMs的多模态编码器并合并LLM参数 2. 通过参数差异生成二进制掩码实现模态参数解耦 3. 解耦参数独立处理不同模态输入 4. 类似机制应用于新任务微调的模型

Result: 实验显示MMER显著优于基线模型，保留99%原有性能，灾难性遗忘现象降低至原水平的1/5

Conclusion: MMER通过非训练方式实现参数解耦，为LLM的多模态扩展提供高效解决方案，平衡了能力扩展与性能保留

Abstract: Fine-tuning Large Language Models (LLMs) with multimodal encoders on
modality-specific data expands the modalities that LLMs can handle, leading to
the formation of Multimodal LLMs (MLLMs). However, this paradigm heavily relies
on resource-intensive and inflexible fine-tuning from scratch with new
multimodal data. In this paper, we propose MMER (Multi-modality Expansion and
Retention), a training-free approach that integrates existing MLLMs for
effective multimodal expansion while retaining their original performance.
Specifically, MMER reuses MLLMs' multimodal encoders while merging their LLM
parameters. By comparing original and merged LLM parameters, MMER generates
binary masks to approximately separate LLM parameters for each modality. These
decoupled parameters can independently process modality-specific inputs,
reducing parameter conflicts and preserving original MLLMs' fidelity. MMER can
also mitigate catastrophic forgetting by applying a similar process to MLLMs
fine-tuned on new tasks. Extensive experiments show significant improvements
over baselines, proving that MMER effectively expands LLMs' multimodal
capabilities while retaining 99% of the original performance, and also markedly
mitigates catastrophic forgetting.

</details>


### [49] [Cultural Value Alignment in Large Language Models: A Prompt-based Analysis of Schwartz Values in Gemini, ChatGPT, and DeepSeek](https://arxiv.org/abs/2505.17112)
*Robin Segerer*

Main category: cs.CL

TL;DR: 研究发现不同语料训练的大语言模型存在文化价值偏差：中英文模型普遍重视自我超越价值观，但中文模型DeepSeek显著弱化自我增强价值观，体现集体主义文化倾向。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型是否承载文化特定价值观，验证中文语料训练的DeepSeek模型是否呈现与西方模型不同的价值偏好，揭示AI伦理中的文化偏见问题。

Method: 采用施瓦茨价值观框架的40项肖像问卷，通过贝叶斯序数回归模型量化分析Gemini/ChatGPT/DeepSeek三大模型对10类基础价值观的优先级排序。

Result: 1. 所有模型高度优先自我超越价值观（仁慈/普世主义）
2. DeepSeek对权力/成就等自我增强价值观重视度显著低于西方模型
3. 模型价值观反映训练语料的文化背景特征

Conclusion: 提出多视角推理、自我反思反馈和动态情境化三大方法解决LLM价值不对称问题，强调建立融合多元文化视角的AI对齐框架的必要性。

Abstract: This study examines cultural value alignment in large language models (LLMs)
by analyzing how Gemini, ChatGPT, and DeepSeek prioritize values from
Schwartz's value framework. Using the 40-item Portrait Values Questionnaire, we
assessed whether DeepSeek, trained on Chinese-language data, exhibits distinct
value preferences compared to Western models. Results of a Bayesian ordinal
regression model show that self-transcendence values (e.g., benevolence,
universalism) were highly prioritized across all models, reflecting a general
LLM tendency to emphasize prosocial values. However, DeepSeek uniquely
downplayed self-enhancement values (e.g., power, achievement) compared to
ChatGPT and Gemini, aligning with collectivist cultural tendencies. These
findings suggest that LLMs reflect culturally situated biases rather than a
universal ethical framework. To address value asymmetries in LLMs, we propose
multi-perspective reasoning, self-reflective feedback, and dynamic
contextualization. This study contributes to discussions on AI fairness,
cultural neutrality, and the need for pluralistic AI alignment frameworks that
integrate diverse moral perspectives.

</details>


### [50] [RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language](https://arxiv.org/abs/2505.17114)
*Subrata Biswas,Mohammad Nur Hossain Khan,Bashima Islam*

Main category: cs.CL

TL;DR: RAVEN架构通过QuART模块实现多模态信号动态筛选，在7个基准测试中准确率提升最高14.5%，传感器数据带来额外16.4%增益，并保持模态损坏下的强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决多模态问答中离镜头语音/背景噪音等模态冲突问题，提升模型抗干扰能力和信息融合质量

Method: 三阶段训练：1) 单模态预训练提升表征质量；2) 查询对齐融合增强跨模态关联；3) 模态冲突定向微调提升鲁棒性；核心组件为跨模态门控模块QuART

Result: AVS-QA数据集验证显示准确率最高提升14.5%，传感器数据额外提升16.4%，模态损坏时仍优于基线50.23%

Conclusion: RAVEN通过动态模态筛选机制显著提升多模态QA性能，传感器融合策略和抗干扰训练范式具有实际应用价值

Abstract: Multimodal question answering (QA) often requires identifying which video,
audio, or sensor tokens are relevant to the question. Yet modality
disagreements are common: off-camera speech, background noise, or motion
outside the field of view often mislead fusion models that weight all streams
equally. We present RAVEN, a unified QA architecture whose core is QuART, a
query-conditioned cross-modal gating module that assigns scalar relevance
scores to each token across modalities, enabling the model to amplify
informative signals and suppress distractors before fusion. RAVEN is trained
through a three-stage pipeline comprising unimodal pretraining, query-aligned
fusion, and disagreement-oriented fine-tuning -- each stage targeting a
distinct challenge in multi-modal reasoning: representation quality,
cross-modal relevance, and robustness to modality mismatch. To support training
and evaluation, we release AVS-QA, a dataset of 300K synchronized
Audio--Video-Sensor streams paired with automatically generated question-answer
pairs. Experimental results on seven multi-modal QA benchmarks -- including
egocentric and exocentric tasks -- show that RAVEN achieves up to 14.5\% and
8.0\% gains in accuracy compared to state-of-the-art multi-modal large language
models, respectively. Incorporating sensor data provides an additional 16.4\%
boost, and the model remains robust under modality corruption, outperforming
SOTA baselines by 50.23\%. Our code and dataset are available at
https://github.com/BASHLab/RAVEN.

</details>


### [51] [Comparative Evaluation of Prompting and Fine-Tuning for Applying Large Language Models to Grid-Structured Geospatial Data](https://arxiv.org/abs/2505.17116)
*Akash Dhruv,Yangxinyu Xie,Jordan Branham,Tanwi Mallick*

Main category: cs.CL

TL;DR: 对比研究LLMs在网格地理空间数据解释中的零样本提示与微调方法表现


<details>
  <summary>Details</summary>
Motivation: 评估不同方法（基础模型结构化提示 vs 微调模型）在结构化地理空间数据解释中的有效性

Method: 通过结构化提示评估基础模型，并与用户交互数据集微调的变体进行对比分析

Result: 零样本提示展现地理时空推理能力但存在局限，微调显著提升结构化时空推理性能

Conclusion: 微调方法能有效增强模型对结构化地理空间和时间序列数据的处理能力

Abstract: This paper presents a comparative study of large language models (LLMs) in
interpreting grid-structured geospatial data. We evaluate the performance of a
base model through structured prompting and contrast it with a fine-tuned
variant trained on a dataset of user-assistant interactions. Our results
highlight the strengths and limitations of zero-shot prompting and demonstrate
the benefits of fine-tuning for structured geospatial and temporal reasoning.

</details>


### [52] [From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning](https://arxiv.org/abs/2505.17117)
*Chen Shani,Dan Jurafsky,Yann LeCun,Ravid Shwartz-Ziv*

Main category: cs.CL

TL;DR: 通过信息论框架揭示LLMs与人类概念表征差异：模型倾向统计压缩，人类注重语义细微


<details>
  <summary>Details</summary>
Motivation: 探究LLMs是否具备人类在语义压缩中平衡表达精度与简洁性的能力

Method: 结合率失真理论与信息瓶颈原则，定量分析多模型token嵌入与人类分类基准

Result: LLMs形成宽泛概念类别但缺乏细粒度语义区分，表现出统计压缩偏好（人类优先上下文丰富性）

Conclusion: 揭示了AI与人类认知架构的核心差异，为构建更人类对齐的概念表征指明方向

Abstract: Humans organize knowledge into compact categories through semantic
compression by mapping diverse instances to abstract representations while
preserving meaning (e.g., robin and blue jay are both birds; most birds can
fly). These concepts reflect a trade-off between expressive fidelity and
representational simplicity. Large Language Models (LLMs) demonstrate
remarkable linguistic abilities, yet whether their internal representations
strike a human-like trade-off between compression and semantic fidelity is
unclear. We introduce a novel information-theoretic framework, drawing from
Rate-Distortion Theory and the Information Bottleneck principle, to
quantitatively compare these strategies. Analyzing token embeddings from a
diverse suite of LLMs against seminal human categorization benchmarks, we
uncover key divergences. While LLMs form broad conceptual categories that align
with human judgment, they struggle to capture the fine-grained semantic
distinctions crucial for human understanding. More fundamentally, LLMs
demonstrate a strong bias towards aggressive statistical compression, whereas
human conceptual systems appear to prioritize adaptive nuance and contextual
richness, even if this results in lower compressional efficiency by our
measures. These findings illuminate critical differences between current AI and
human cognitive architectures, guiding pathways toward LLMs with more
human-aligned conceptual representations.

</details>


### [53] [After Retrieval, Before Generation: Enhancing the Trustworthiness of Large Language Models in RAG](https://arxiv.org/abs/2505.17118)
*Xinbang Dai,Huikang Hu,Yuncheng Hua,Jiaqi Li,Yongrui Chen,Rihui Jin,Nan Hu,Guilin Qi*

Main category: cs.CL

TL;DR: 论文提出BRIDGE框架解决RAG系统中内部知识与外部检索知识冲突问题，通过动态响应策略提升可信度，实验显示准确率提升5-15%


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在不同现实场景中缺乏统一响应框架，无法有效处理知识冲突和可靠性问题

Method: 采用软偏置自适应权重机制整合知识，结合最大软偏置决策树评估知识可信度，动态选择信任内部/外部知识或拒绝回答的策略

Result: BRIDGE在36,266问题的TRD数据集上超越基线模型5-15%准确率，且在所有测试场景中保持均衡性能

Conclusion: BRIDGE框架为实际RAG应用中大语言模型的可信响应提供了动态决策解决方案

Abstract: Retrieval-augmented generation (RAG) systems face critical challenges in
balancing internal (parametric) and external (retrieved) knowledge, especially
when these sources conflict or are unreliable. To analyze these scenarios
comprehensively, we construct the Trustworthiness Response Dataset (TRD) with
36,266 questions spanning four RAG settings. We reveal that existing approaches
address isolated scenarios-prioritizing one knowledge source, naively merging
both, or refusing answers-but lack a unified framework to handle different
real-world conditions simultaneously. Therefore, we propose the BRIDGE
framework, which dynamically determines a comprehensive response strategy of
large language models (LLMs). BRIDGE leverages an adaptive weighting mechanism
named soft bias to guide knowledge collection, followed by a Maximum Soft-bias
Decision Tree to evaluate knowledge and select optimal response strategies
(trust internal/external knowledge, or refuse). Experiments show BRIDGE
outperforms baselines by 5-15% in accuracy while maintaining balanced
performance across all scenarios. Our work provides an effective solution for
LLMs' trustworthy responses in real-world RAG applications.

</details>
