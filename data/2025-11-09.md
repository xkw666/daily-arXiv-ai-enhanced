<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 44]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.AI](#cs.AI) [Total: 7]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.LG](#cs.LG) [Total: 6]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.IR](#cs.IR) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs](https://arxiv.org/abs/2511.03738)
*Pranav Bhandari,Nicolas Fay,Sanjeevan Selvaganapathy,Amitava Datta,Usman Naseem,Mehwish Nasim*

Main category: cs.CL

TL;DR: 提出基于大五人格特质的LLM行为控制框架，通过低秩子空间发现实现个性对齐，在不影响模型性能前提下实现精准行为控制。


<details>
  <summary>Details</summary>
Motivation: LLM隐含个性特征缺乏可靠控制机制，需建立心理学理论与模型行为的可操作关联，探索个性表示与行为控制的关系。

Method: 1. 用大五人格框架提取transformer隐藏层激活 2. 应用低秩子空间发现方法 3. 动态层选择的灵活控制框架 4. 通过潜在空间扰动实现行为控制

Result: 发现人格特质存在于低秩共享子空间，验证通过微小扰动即可有效控制特质表达，保持模型流畅性和通用能力不变。

Conclusion: 建立了心理学理论与模型对齐的桥梁，为可解释的行为控制提供新范式，证明个性表征的可操作性价值。

Abstract: Large Language Models exhibit implicit personalities in their generation, but
reliably controlling or aligning these traits to meet specific needs remains an
open challenge. The need for effective mechanisms for behavioural manipulation
of the model during generation is a critical gap in the literature that needs
to be fulfilled. Personality-aware LLMs hold a promising direction towards this
objective. However, the relationship between these psychological constructs and
their representations within LLMs remains underexplored and requires further
investigation. Moreover, it is intriguing to understand and study the use of
these representations to steer the models' behaviour. We propose a novel
pipeline that extracts hidden state activations from transformer layers using
the Big Five Personality Traits (Openness, Conscientiousness, Extraversion,
Agreeableness and Neuroticism), which is a comprehensive and empirically
validated framework to model human personality applies low-rank subspace
discovery methods, and identifies trait-specific optimal layers across
different model architectures for robust injection. The resulting
personality-aligned directions are then operationalised through a flexible
steering framework with dynamic layer selection, enabling precise control of
trait expression in LLM outputs. Our findings reveal that personality traits
occupy a low-rank shared subspace, and that these latent structures can be
transformed into actionable mechanisms for effective steering through careful
perturbations without impacting the fluency, variance and general capabilities,
helping to bridge the gap between psychological theory and practical model
alignment.

</details>


### [2] [TextualVerifier: Verify TextGrad Step-by-Step](https://arxiv.org/abs/2511.03739)
*Eugenius Mario Situmorang,Adila Alfa Krisnadhi,Ari Wibisono*

Main category: cs.CL

TL;DR: TextualVerifier通过思维链和多数投票机制增强TextGrad的推理验证，实验证明有效性提升29%且集成后准确率提高2.2个百分点


<details>
  <summary>Details</summary>
Motivation: 解决TextGrad在文本决策中缺乏自我验证机制导致的推理有效性不足问题

Method: 四阶段工作流（思维链分解/变体生成/多数投票/共识聚合）+两阶段集成（损失函数验证/优化结果验证）

Result: PRM800K验证阶段推理有效性提升29%；TextGrad集成后在GPQA等基准测试提升2.2-10.7个百分点(p<0.001)，平均增加5.9次LLM调用

Conclusion: 首个基于LLM的TextGrad自验证框架，无需数值梯度即可提升推理可靠性，开辟文本优化验证新方向

Abstract: TextGrad is a novel approach to text-based automatic differentiation that
enables composite AI systems to perform optimization without explicit numerical
equations. However, it currently lacks self-verification mechanisms that ensure
reasoning validity in text-based decision making. This research introduces
TextualVerifier, a verification framework that leverages chain-of-thought
reasoning and majority voting with large language models to address this
verification gap. TextualVerifier implements a four-stage workflow:
chain-of-thought decomposition, variant generation, majority voting, and
consensus aggregation. It integrates non-invasively with TextGrad at both the
loss function and optimization result verification stages. Experimental
evaluation using the Gemini 1.5 Pro model is conducted in two phases: (1)
standalone evaluation on PRM800K, and (2) integrated evaluation with TextGrad
on GPQA-Diamond, MMLU-ML, and MMLU-CP benchmarks. Results show statistically
significant improvements (p < 0.001). In phase one, TextualVerifier improves
the validity of reasoning steps by 29 percent. In phase two, integration into
TextGrad loss function yields a 2.2 percentage point gain from 68.2 to 70.4
percent with a moderate overhead of 5.9 LLM calls on average. Further
evaluations of TextualVerifier versioning yield 8.08, 10.71, and 3.92
percentage point improvements on GPQA, MMLU-ML, and MMLU-CP respectively.
TextualVerifier thus presents the first self-verification framework for
TextGrad through LLM-based techniques without requiring numerical gradients,
enabling more reliable reasoning and opening new directions for verification in
text-based optimization.

</details>


### [3] [GRDD+: An Extended Greek Dialectal Dataset with Cross-Architecture Fine-tuning Evaluation](https://arxiv.org/abs/2511.03772)
*Stergios Chatzikyriakidis,Dimitris Papadakis,Sevasti-Ioanna Papaioannou,Erofili Psaltaki*

Main category: cs.CL

TL;DR: 扩展GRDD方言数据集至637万词/10种希腊变体，通过微调实验验证方言数据对LLM性能提升效果


<details>
  <summary>Details</summary>
Motivation: 解决现有希腊方言数据规模有限的问题，探索高质量方言数据对语言模型性能的影响

Method: 1. 扩展GRDD数据集并新增6种希腊变体 2. 使用三种8B参数模型进行微调 3. 与Claude/Gemini/ChatGPT等前沿模型对比

Result: 构建首个大规模希腊方言数据集GRDD+(6,374,939词)，实验显示微调后模型性能接近商业前沿模型水平

Conclusion: 证实方言数据质量直接影响LLM性能，GRDD+为方言保护及NLP研究提供了重要基础设施

Abstract: We present an extended Greek Dialectal Dataset (GRDD+) 1that complements the
existing GRDD dataset with more data from Cretan, Cypriot, Pontic and Northern
Greek, while we add six new varieties: Greco-Corsican, Griko (Southern Italian
Greek), Maniot, Heptanesian, Tsakonian, and Katharevusa Greek. The result is a
dataset with total size 6,374,939 words and 10 varieties. This is the first
dataset with such variation and size to date. We conduct a number of
fine-tuning experiments to see the effect of good quality dialectal data on a
number of LLMs. We fine-tune three model architectures (Llama-3-8B,
Llama-3.1-8B, Krikri-8B) and compare the results to frontier models
(Claude-3.7-Sonnet, Gemini-2.5, ChatGPT-5).

</details>


### [4] [PLLuM: A Family of Polish Large Language Models](https://arxiv.org/abs/2511.03823)
*Jan Kocoń,Maciej Piasecki,Arkadiusz Janz,Teddy Ferdinan,Łukasz Radliński,Bartłomiej Koptyra,Marcin Oleksy,Stanisław Woźniak,Paweł Walkowiak,Konrad Wojtasik,Julia Moska,Tomasz Naskręt,Bartosz Walkowiak,Mateusz Gniewkowski,Kamil Szyc,Dawid Motyka,Dawid Banach,Jonatan Dalasiński,Ewa Rudnicka,Bartłomiej Alberski,Tomasz Walkowiak,Aleksander Szczęsny,Maciej Markiewicz,Tomasz Bernaś,Hubert Mazur,Kamil Żyta,Mateusz Tykierko,Grzegorz Chodak,Tomasz Kajdanowicz,Przemysław Kazienko,Agnieszka Karlińska,Karolina Seweryn,Anna Kołos,Maciej Chrabąszcz,Katarzyna Lorenc,Aleksandra Krasnodębska,Artur Wilczek,Katarzyna Dziewulska,Paula Betscher,Zofia Cieślińska,Katarzyna Kowol,Daria Mikoś,Maciej Trzciński,Dawid Krutul,Marek Kozłowski,Sławomir Dadas,Rafał Poświata,Michał Perełkiewicz,Małgorzata Grębowiec,Maciej Kazuła,Marcin Białas,Roman Roszko,Danuta Roszko,Jurgita Vaičenonienė,Andrius Utka,Paweł Levchuk,Paweł Kowalski,Irena Prawdzic-Jankowska,Maciej Ogrodniczuk,Monika Borys,Anna Bulińska,Wiktoria Gumienna,Witold Kieraś,Dorota Komosińska,Katarzyna Krasnowska-Kieraś,Łukasz Kobyliński,Martyna Lewandowska,Marek Łaziński,Mikołaj Łątkowski,Dawid Mastalerz,Beata Milewicz,Agnieszka Anna Mykowiecka,Angelika Peljak-Łapińska,Sandra Penno,Zuzanna Przybysz,Michał Rudolf,Piotr Rybak,Karolina Saputa,Aleksandra Tomaszewska,Aleksander Wawer,Marcin Woliński,Joanna Wołoszyn,Alina Wróblewska,Bartosz Żuk,Filip Żarnecki,Konrad Kaczyński,Anna Cichosz,Zuzanna Deckert,Monika Garnys,Izabela Grabarczyk,Wojciech Janowski,Sylwia Karasińska,Aleksandra Kujawiak,Piotr Misztela,Maria Szymańska,Karolina Walkusz,Igor Siek,Jakub Kwiatkowski,Piotr Pęzik*

Main category: cs.CL

TL;DR: 波兰研究机构开发了最大开源波兰语大模型PLLuM，包含1400亿token预训练语料和17.7万标注数据，采用负责任AI框架解决英语主导模型的不足。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型以英语为中心，其他语言（如波兰语）缺乏高质量、透明且文化适配的模型解决方案

Method: 构建1400亿token波兰语预训练语料，创建77k指令数据集和100k偏好数据集，采用混合模块实现输出校正与安全过滤

Result: 成功开发基础模型和指令调优模型，在公共行政任务中验证有效性，模型参数和训练框架完全开源

Conclusion: PLLuM通过开源模式推动波兰本土AI研究，为其他非英语国家发展主权AI技术提供实践范例

Abstract: Large Language Models (LLMs) play a central role in modern artificial
intelligence, yet their development has been primarily focused on English,
resulting in limited support for other languages. We present PLLuM (Polish
Large Language Model), the largest open-source family of foundation models
tailored specifically for the Polish language. Developed by a consortium of
major Polish research institutions, PLLuM addresses the need for high-quality,
transparent, and culturally relevant language models beyond the English-centric
commercial landscape. We describe the development process, including the
construction of a new 140-billion-token Polish text corpus for pre-training, a
77k custom instructions dataset, and a 100k preference optimization dataset. A
key component is a Responsible AI framework that incorporates strict data
governance and a hybrid module for output correction and safety filtering. We
detail the models' architecture, training procedures, and alignment techniques
for both base and instruction-tuned variants, and demonstrate their utility in
a downstream task within public administration. By releasing these models
publicly, PLLuM aims to foster open research and strengthen sovereign AI
technologies in Poland.

</details>


### [5] [STARS: Segment-level Token Alignment with Rejection Sampling in Large Language Models](https://arxiv.org/abs/2511.03827)
*Mohammad Atif Quamar,Mohammad Areeb,Mikhail Kuznetsov,Muslum Ozgur Ozmen,Z. Berkay Celik*

Main category: cs.CL

TL;DR: 提出STARS解码算法，通过分段token采样优化模型对齐，显著提升效率与效果


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法（微调/Best-of-N采样）存在计算成本高、效率低的问题，需要更高效的实时对齐方案

Method: STARS算法：迭代采样-评分-接受/拒绝固定长度token片段，实现生成路径的早期修正

Result: 在6个LLM测试中超越SFT达14.9%，优于DPO达4.3%，且计算效率显著优于Best-of-N基线

Conclusion: 基于细粒度奖励引导的分段采样方法，为模型对齐提供了高效、可泛化的新范式

Abstract: Aligning large language models with human values is crucial for their safe
deployment; however, existing methods, such as fine-tuning, are computationally
expensive and suboptimal. In contrast, inference-time approaches like Best-of-N
sampling require practically infeasible computation to achieve optimal
alignment. We propose STARS: Segment-level Token Alignment with Rejection
Sampling, a decoding-time algorithm that steers model generation by iteratively
sampling, scoring, and rejecting/accepting short, fixed-size token segments.
This allows for early correction of the generation path, significantly
improving computational efficiency and boosting alignment quality. Across a
suite of six LLMs, we show that STARS outperforms Supervised Fine-Tuning (SFT)
by up to 14.9 percentage points and Direct Preference Optimization (DPO) by up
to 4.3 percentage points on win-rates, while remaining highly competitive with
strong Best-of-N baselines. Our work establishes granular, reward-guided
sampling as a generalizable, robust, and efficient alternative to traditional
fine-tuning and full-sequence ranking methods for aligning LLMs.

</details>


### [6] [Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label LLM-Based Classification](https://arxiv.org/abs/2511.03830)
*Mikołaj Langner,Jan Eliasz,Ewa Rudnicka,Jan Kocoń*

Main category: cs.CL

TL;DR: 提出基于二分决策分解和模型蒸馏的高效多标签分类框架，在情感分析中验证并展示跨领域适用性


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在多标签分类任务中效率与精度的平衡问题，通过任务分解和蒸馏技术实现高效推理

Method: 将多标签分类转化为独立二分查询，结合前缀缓存机制优化推理速度；采用DeepSeek-V3生成标注数据，通过蒸馏微调小模型（HerBERT-Large等）

Result: 微调模型在训练维度上显著超越零样本基线，推理速度提升30%且保持97%准确率

Conclusion: 二分决策分解+缓存感知推断+蒸馏的框架具有高效性和可扩展性，该方法虽在情感分析验证但具备跨领域通用潜力

Abstract: We introduce a method for efficient multi-label text classification with
large language models (LLMs), built on reformulating classification tasks as
sequences of dichotomic (yes/no) decisions. Instead of generating all labels in
a single structured response, each target dimension is queried independently,
which, combined with a prefix caching mechanism, yields substantial efficiency
gains for short-text inference without loss of accuracy. To demonstrate the
approach, we focus on affective text analysis, covering 24 dimensions including
emotions and sentiment. Using LLM-to-SLM distillation, a powerful annotator
model (DeepSeek-V3) provides multiple annotations per text, which are
aggregated to fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B,
Gemma3-1B). The fine-tuned models show significant improvements over zero-shot
baselines, particularly on the dimensions seen during training. Our findings
suggest that decomposing multi-label classification into dichotomic queries,
combined with distillation and cache-aware inference, offers a scalable and
effective framework for LLM-based classification. While we validate the method
on affective states, the approach is general and applicable across domains.

</details>


### [7] [Evaluating Machine Translation Datasets for Low-Web Data Languages: A Gendered Lens](https://arxiv.org/abs/2511.03880)
*Hellina Hailu Nigatu,Bethelhem Yemane Mamo,Bontu Fufa Balcha,Debora Taye Tesfaye,Elbethel Daniel Zewdie,Ikram Behiru Nesiru,Jitu Ewnetu Hailu,Senait Mengesha Yayo*

Main category: cs.CL

TL;DR: 研究发现低资源语言机器翻译数据集存在领域失衡与性别偏见问题，数据量增加反而加剧有害内容


<details>
  <summary>Details</summary>
Motivation: 针对低资源语言大规模数据集建设中重数量轻质量的现状，可能造成技术性能低下和社会偏见延续的双重风险

Method: 通过分析三种低资源语言（Afan Oromo/Amharic/Tigrinya）机器翻译数据集，聚焦性别表征维度展开研究

Result: 1. 训练数据集中于政治宗教领域，基准数据集侧重新闻健康体育
2. 存在显著男性偏向（人名/动词语法性别/刻板印象）
3. 对女性的有害描述与数据量正相关

Conclusion: 必须重视低资源语言数据集质量审查，早期干预有害内容，数量积累不能保证数据质量

Abstract: As low-resourced languages are increasingly incorporated into NLP research,
there is an emphasis on collecting large-scale datasets. But in prioritizing
quantity over quality, we risk 1) building language technologies that perform
poorly for these languages and 2) producing harmful content that perpetuates
societal biases. In this paper, we investigate the quality of Machine
Translation (MT) datasets for three low-resourced languages--Afan Oromo,
Amharic, and Tigrinya, with a focus on the gender representation in the
datasets. Our findings demonstrate that while training data has a large
representation of political and religious domain text, benchmark datasets are
focused on news, health, and sports. We also found a large skew towards the
male gender--in names of persons, the grammatical gender of verbs, and in
stereotypical depictions in the datasets. Further, we found harmful and toxic
depictions against women, which were more prominent for the language with the
largest amount of data, underscoring that quantity does not guarantee quality.
We hope that our work inspires further inquiry into the datasets collected for
low-resourced languages and prompts early mitigation of harmful content.
WARNING: This paper contains discussion of NSFW content that some may find
disturbing.

</details>


### [8] [GRAD: Graph-Retrieved Adaptive Decoding for Hallucination Mitigation](https://arxiv.org/abs/2511.03900)
*Manh Nguyen,Sunil Gupta,Dai Do,Hung Le*

Main category: cs.CL

TL;DR: 提出无需重新训练的轻量级解码方法GRAD，通过语料库统计证据引导生成更真实可信的输出


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的幻觉抑制方法存在领域敏感性，符号知识整合成本高昂。受知识图谱启发，开发基于语料库统计证据的轻量级解决方案

Method: 单次前向传递构建稀疏token转移图，解码阶段将图谱logits与模型logits自适应融合，平衡证据强度与生成流畅性

Result: 在三大模型和多项QA任务中，内在准确率提升9.7%，幻觉率降低8.6%，正确性提升6.9%，取得最优的『真实性-信息量』综合评分

Conclusion: GRAD为对比解码和知识图谱增强提供即插即用替代方案，验证语料级token转移统计特征对生成可信输出的有效性

Abstract: Hallucination mitigation remains a persistent challenge for large language
models (LLMs), even as model scales grow. Existing approaches often rely on
external knowledge sources, such as structured databases or knowledge graphs,
accessed through prompting or retrieval. However, prompt-based grounding is
fragile and domain-sensitive, while symbolic knowledge integration incurs heavy
retrieval and formatting costs. Motivated by knowledge graphs, we introduce
Graph-Retrieved Adaptive Decoding (GRAD), a decoding-time method that grounds
generation in corpus-derived evidence without retraining. GRAD constructs a
sparse token transition graph by accumulating next-token logits across a small
retrieved corpus in a single forward pass. During decoding, graph-retrieved
logits are max-normalized and adaptively fused with model logits to favor
high-evidence continuations while preserving fluency. Across three models and a
range of question-answering benchmarks spanning intrinsic, extrinsic
hallucination, and factuality tasks, GRAD consistently surpasses baselines,
achieving up to 9.7$\%$ higher intrinsic accuracy, 8.6$\%$ lower hallucination
rates, and 6.9$\%$ greater correctness compared to greedy decoding, while
attaining the highest truth--informativeness product score among all methods.
GRAD offers a lightweight, plug-and-play alternative to contrastive decoding
and knowledge graph augmentation, demonstrating that statistical evidence from
corpus-level token transitions can effectively steer generation toward more
truthful and verifiable outputs.

</details>


### [9] [Context informs pragmatic interpretation in vision-language models](https://arxiv.org/abs/2511.03908)
*Alvin Wei Ming Tan,Ben Prystawski,Veronica Boyce,Michael C. Frank*

Main category: cs.CL

TL;DR: 研究通过迭代参考游戏测试人类与视觉语言模型的上下文推理能力，发现相关上下文可显著提升模型表现，但抽象少样本任务仍是挑战。


<details>
  <summary>Details</summary>
Motivation: 探索多轮语言环境中智能体的上下文敏感推理能力，验证不同上下文条件(数量/顺序/相关性)对模型表现的影响机制。

Method: 通过控制上下文变量(数量/顺序/相关性)，在迭代参考游戏中对比人类与VL模型的指代表现差异。

Result: 无相关上下文时模型准确率高于随机但显著低于人类；相关上下文可使模型表现随试验次数激增；抽象少样本任务仍困难。

Conclusion: 上下文相关性是提升模型表现的关键因素，现有模型在动态语境推理和抽象概念处理方面仍存在显著能力缺口。

Abstract: Iterated reference games - in which players repeatedly pick out novel
referents using language - present a test case for agents' ability to perform
context-sensitive pragmatic reasoning in multi-turn linguistic environments. We
tested humans and vision-language models on trials from iterated reference
games, varying the given context in terms of amount, order, and relevance.
Without relevant context, models were above chance but substantially worse than
humans. However, with relevant context, model performance increased
dramatically over trials. Few-shot reference games with abstract referents
remain a difficult task for machine learning models.

</details>


### [10] [The Human Flourishing Geographic Index: A County-Level Dataset for the United States, 2013--2023](https://arxiv.org/abs/2511.03915)
*Stefano M. Iacus,Devika Jain,Andrea Nasuto,Giuseppe Porro,Marcello Carammia,Andrea Vezzulli*

Main category: cs.CL

TL;DR: 利用26亿条美国推特数据构建人类繁荣地理指数（HFGI），通过大语言模型分析48个指标，提供高时空分辨率的福祉测量工具。


<details>
  <summary>Details</summary>
Motivation: 现有量化人类繁荣（涵盖幸福/健康/目标等多维度）的测量方法缺乏精细时空分辨率，需构建更精准的社会福祉评估体系。

Method: 基于2013-2023年26亿条地理定位推文，使用微调大语言模型分析48个指标（含哈佛全球繁荣框架及移民态度/腐败感知），建立县州级月度年度数据集。

Result: 验证显示该指数能准确反映繁荣相关论述，与既有指标存在预期相关性，形成首个社交媒体衍生的十年跨尺度繁荣测量体系。

Conclusion: HFGI为研究福祉/不平等/社会变迁提供高分辨率数据支持，揭示社交媒体反映的美国社会繁荣动态演变规律。

Abstract: Quantifying human flourishing, a multidimensional construct including
happiness, health, purpose, virtue, relationships, and financial stability, is
critical for understanding societal well-being beyond economic indicators.
Existing measures often lack fine spatial and temporal resolution. Here we
introduce the Human Flourishing Geographic Index (HFGI), derived from analyzing
approximately 2.6 billion geolocated U.S. tweets (2013-2023) using fine-tuned
large language models to classify expressions across 48 indicators aligned with
Harvard's Global Flourishing Study framework plus attitudes towards migration
and perception of corruption. The dataset offers monthly and yearly county- and
state-level indicators of flourishing-related discourse, validated to confirm
that the measures accurately represent the underlying constructs and show
expected correlations with established indicators. This resource enables
multidisciplinary analyses of well-being, inequality, and social change at
unprecedented resolution, offering insights into the dynamics of human
flourishing as reflected in social media discourse across the United States
over the past decade.

</details>


### [11] [Direct Semantic Communication Between Large Language Models via Vector Translation](https://arxiv.org/abs/2511.03945)
*Fu-Chun Yang,Jason Eshraghian*

Main category: cs.CL

TL;DR: 提出通过向量翻译实现跨模型潜在语义通信，实验表明30%向量注入强度可在保持计算稳定性的前提下有效引导模型生成


<details>
  <summary>Details</summary>
Motivation: 传统多智能体系统中LLM通过token传递信息存在语义损失和计算冗余，需建立更高效的语义传递机制

Method: 使用双编码器翻译器在Llama-2-7B和Mistral-7B-Instruct之间建立向量映射空间，采用余弦对齐度量语义迁移效果

Result: 获得平均0.538余弦对齐度，30%向量混合强度下保持logits稳定，双向评估显示通用模型向指令模型迁移效率高2.01倍

Conclusion: 跨模型潜在通信可行，共享语义而非token的协作AI系统成为可能，为多模型协作系统开发提供新范式

Abstract: In multi-agent settings, such as debate, reflection, or tool-calling, large
language models (LLMs) pass messages as plain tokens, discarding most latent
semantics. This constrains information transfer and adds unnecessary
computational overhead. We form a latent bridge via vector translations, which
use learned mappings that enable direct semantic exchange between
representation spaces. A dual-encoder translator trained between Llama-2-7B and
Mistral-7B-Instruct attains an average cosine alignment of 0.538. Injecting the
translated vectors at 30 percent blending strength steers the target model's
generation without destabilizing logits. Bidirectional evaluation shows a
2.01:1 transfer asymmetry, indicating that general-purpose models yield more
transferable representations than instruction-tuned variants. This conservative
injection preserves computational stability while demonstrating that
cross-model latent communication is feasible, enabling collaborative AI systems
that share meaning rather than tokens.

</details>


### [12] [Abductive Inference in Retrieval-Augmented Language Models: Generating and Validating Missing Premises](https://arxiv.org/abs/2511.04020)
*Shiyin Lin*

Main category: cs.CL

TL;DR: 提出整合溯因推理框架增强RAG系统，通过生成缺失前提填补证据链缺口，提升回答准确性和推理可信度


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成(RAG)系统在证据不完整时容易失效，溯因推理可生成合理假设填补证据链缺口，增强系统鲁棒性和可解释性

Method: 开发三阶段框架：1) 证据完整性检测 2) 候选缺失前提生成 3) 通过一致性与合理性验证的假设筛选

Result: 在溯因推理和多跳QA基准测试中，方法显著提升答案准确性(3.5%)和推理可信度(12.8%)

Conclusion: 溯因推理为提升RAG系统鲁棒性提供新方向，验证框架有效性的同时为可解释AI系统开发奠定基础

Abstract: Large Language Models (LLMs) enhanced with retrieval -- commonly referred to
as Retrieval-Augmented Generation (RAG) -- have demonstrated strong performance
in knowledge-intensive tasks. However, RAG pipelines often fail when retrieved
evidence is incomplete, leaving gaps in the reasoning process. In such cases,
\emph{abductive inference} -- the process of generating plausible missing
premises to explain observations -- offers a principled approach to bridge
these gaps. In this paper, we propose a framework that integrates abductive
inference into retrieval-augmented LLMs. Our method detects insufficient
evidence, generates candidate missing premises, and validates them through
consistency and plausibility checks. Experimental results on abductive
reasoning and multi-hop QA benchmarks show that our approach improves both
answer accuracy and reasoning faithfulness. This work highlights abductive
inference as a promising direction for enhancing the robustness and
explainability of RAG systems.

</details>


### [13] [WST: Weakly Supervised Transducer for Automatic Speech Recognition](https://arxiv.org/abs/2511.04035)
*Dongji Gao,Chenda Liao,Changliang Liu,Matthew Wiesner,Leibny Paola Garcia,Daniel Povey,Sanjeev Khudanpur,Jian Wu*

Main category: cs.CL

TL;DR: 提出无需额外置信度估计的弱监督Transducer（WST），在70%转录错误率下仍保持ASR性能


<details>
  <summary>Details</summary>
Motivation: 解决RNN-T模型对大规模标注数据的高度依赖问题，降低语音识别标注成本

Method: 设计灵活训练图结构，直接处理含噪声的转录文本，无需预训练模型或置信度估计模块

Result: 在合成/工业数据集上显著优于BTC、OTC等CTC弱监督方法，错误容忍率达70%

Conclusion: WST展示了在真实ASR场景中的实用性和鲁棒性，代码将开源

Abstract: The Recurrent Neural Network-Transducer (RNN-T) is widely adopted in
end-to-end (E2E) automatic speech recognition (ASR) tasks but depends heavily
on large-scale, high-quality annotated data, which are often costly and
difficult to obtain. To mitigate this reliance, we propose a Weakly Supervised
Transducer (WST), which integrates a flexible training graph designed to
robustly handle errors in the transcripts without requiring additional
confidence estimation or auxiliary pre-trained models. Empirical evaluations on
synthetic and industrial datasets reveal that WST effectively maintains
performance even with transcription error rates of up to 70%, consistently
outperforming existing Connectionist Temporal Classification (CTC)-based weakly
supervised approaches, such as Bypass Temporal Classification (BTC) and
Omni-Temporal Classification (OTC). These results demonstrate the practical
utility and robustness of WST in realistic ASR settings. The implementation
will be publicly available.

</details>


### [14] [T-FIX: Text-Based Explanations with Features Interpretable to eXperts](https://arxiv.org/abs/2511.04070)
*Shreya Havaldar,Helen Jin,Chaehyeon Kim,Anton Xue,Weiqiu You,Marco Gatti,Bhuvnesh Jain,Helen Qu,Daniel A Hashimoto,Amin Madani,Rajat Deo,Sameed Ahmed M. Khatana,Gary E. Weissman,Lyle Ungar,Eric Wong*

Main category: cs.CL

TL;DR: 论文提出专家对齐评估框架T-FIX，通过七领域专家协作开发新指标，衡量大模型解释与专业直觉的匹配度


<details>
  <summary>Details</summary>
Motivation: 当前大模型解释评估侧重合理性而非专业匹配，无法满足医学/天文等知识密集型领域专家对专业级解释的需求

Method: 构建跨7个知识领域的T-FIX基准测试集，联合领域专家开发创新评估指标体系

Result: 建立首个系统测量大模型解释与专家判断对齐程度的评估框架

Conclusion: 专家对齐应成为知识密集型场景下AI解释能力的核心评估维度，确保专业领域的可信决策支持

Abstract: As LLMs are deployed in knowledge-intensive settings (e.g., surgery,
astronomy, therapy), users expect not just answers, but also meaningful
explanations for those answers. In these settings, users are often domain
experts (e.g., doctors, astrophysicists, psychologists) who require
explanations that reflect expert-level reasoning. However, current evaluation
schemes primarily emphasize plausibility or internal faithfulness of the
explanation, which fail to capture whether the content of the explanation truly
aligns with expert intuition. We formalize expert alignment as a criterion for
evaluating explanations with T-FIX, a benchmark spanning seven
knowledge-intensive domains. In collaboration with domain experts, we develop
novel metrics to measure the alignment of LLM explanations with expert
judgment.

</details>


### [15] [Plan of Knowledge: Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering](https://arxiv.org/abs/2511.04072)
*Xinying Qian,Ying Zhang,Yu Zhao,Baohang Zhou,Xuhui Sui,Xiaojie Yuan*

Main category: cs.CL

TL;DR: 提出PoK框架，通过知识计划模块和对比时序检索器提升LLMs在时态知识图谱问答中的推理精度


<details>
  <summary>Details</summary>
Motivation: 现有TKGQA方法无法充分理解时间约束，LLMs存在时序推理局限和事实幻觉问题

Method: 结合知识计划模块(分解问题为工具链)和对比时序知识库检索，实现结构化时序推理

Result: 在四个基准数据集上显著超越SOTA方法最高56%，提升检索精度和推理准确性

Conclusion: PoK通过结构化规划和时序知识检索的融合，有效增强LLMs时序推理的可解释性与事实一致性

Abstract: Temporal Knowledge Graph Question Answering (TKGQA) aims to answer
time-sensitive questions by leveraging factual information from Temporal
Knowledge Graphs (TKGs). While previous studies have employed pre-trained TKG
embeddings or graph neural networks to inject temporal knowledge, they fail to
fully understand the complex semantic information of time constraints.
Recently, Large Language Models (LLMs) have shown remarkable progress,
benefiting from their strong semantic understanding and reasoning
generalization capabilities. However, their temporal reasoning ability remains
limited. LLMs frequently suffer from hallucination and a lack of knowledge. To
address these limitations, we propose the Plan of Knowledge framework with a
contrastive temporal retriever, which is named PoK. Specifically, the proposed
Plan of Knowledge module decomposes a complex temporal question into a sequence
of sub-objectives from the pre-defined tools, serving as intermediate guidance
for reasoning exploration. In parallel, we construct a Temporal Knowledge Store
(TKS) with a contrastive retrieval framework, enabling the model to selectively
retrieve semantically and temporally aligned facts from TKGs. By combining
structured planning with temporal knowledge retrieval, PoK effectively enhances
the interpretability and factual consistency of temporal reasoning. Extensive
experiments on four benchmark TKGQA datasets demonstrate that PoK significantly
improves the retrieval precision and reasoning accuracy of LLMs, surpassing the
performance of the state-of-the-art TKGQA methods by 56.0% at most.

</details>


### [16] [The truth is no diaper: Human and AI-generated associations to emotional words](https://arxiv.org/abs/2511.04077)
*Špela Vintar,Jan Jona Javoršek*

Main category: cs.CL

TL;DR: 研究比较人类与大型语言模型在情感词汇联想上的差异，发现模型联想重叠度中等但更易放大情感、缺乏创造性


<details>
  <summary>Details</summary>
Motivation: 探索LLMs是否能像人类一样处理情感负载词汇的联想机制，这对理解模型创造性和情感处理偏差具有重要意义

Method: 通过对比人类参与者和LLMs对情感负载词汇的联想反应，分析重叠度、情感放大程度及创造性差异

Result: 人类与LLMs联想重叠度中等，但模型反应更易放大原始词汇情感负载，且联想结果更具可预测性、创造性显著低于人类

Conclusion: LLMs的情感联想机制与人类存在系统性差异，这种特性可能影响其在创意任务中的应用效果和情感智能评估

Abstract: Human word associations are a well-known method of gaining insight into the
internal mental lexicon, but the responses spontaneously offered by human
participants to word cues are not always predictable as they may be influenced
by personal experience, emotions or individual cognitive styles. The ability to
form associative links between seemingly unrelated concepts can be the driving
mechanisms of creativity. We perform a comparison of the associative behaviour
of humans compared to large language models. More specifically, we explore
associations to emotionally loaded words and try to determine whether large
language models generate associations in a similar way to humans. We find that
the overlap between humans and LLMs is moderate, but also that the associations
of LLMs tend to amplify the underlying emotional load of the stimulus, and that
they tend to be more predictable and less creative than human ones.

</details>


### [17] [Improving the Performance of Radiology Report De-identification with Large-Scale Training and Benchmarking Against Cloud Vendor Methods](https://arxiv.org/abs/2511.04079)
*Eva Prakash,Maayane Attias,Pierre Chambon,Justin Xu,Steven Truong,Jean-Benoit Delbrouck,Tessa Cook,Curtis Langlotz*

Main category: cs.CL

TL;DR: 基于Transformer的放射报告去识别模型通过多机构数据集训练，在PHI检测性能上超越商业系统


<details>
  <summary>Details</summary>
Motivation: 提升放射学报告中受保护健康信息（PHI）的自动去识别化能力，验证模型在跨机构数据中的泛化性并建立安全临床文本处理新标准

Method: 使用斯坦福大学多模态放射数据集微调Transformer模型，引入AGE类别，通过50组合成PHI测试稳定性，并与主流商业系统进行对比评估

Result: 在宾大/斯坦福测试集分别达到0.973/0.996的F1值，合成数据检测F1达0.959，显著优于商业系统（0.632-0.754）

Conclusion: 大规模多机构训练使Transformer模型在PHI检测领域达到最优性能，为医疗文本隐私保护提供了可靠的技术基准

Abstract: Objective: To enhance automated de-identification of radiology reports by
scaling transformer-based models through extensive training datasets and
benchmarking performance against commercial cloud vendor systems for protected
health information (PHI) detection. Materials and Methods: In this
retrospective study, we built upon a state-of-the-art, transformer-based, PHI
de-identification pipeline by fine-tuning on two large annotated radiology
corpora from Stanford University, encompassing chest X-ray, chest CT,
abdomen/pelvis CT, and brain MR reports and introducing an additional PHI
category (AGE) into the architecture. Model performance was evaluated on test
sets from Stanford and the University of Pennsylvania (Penn) for token-level
PHI detection. We further assessed (1) the stability of synthetic PHI
generation using a "hide-in-plain-sight" method and (2) performance against
commercial systems. Precision, recall, and F1 scores were computed across all
PHI categories. Results: Our model achieved overall F1 scores of 0.973 on the
Penn dataset and 0.996 on the Stanford dataset, outperforming or maintaining
the previous state-of-the-art model performance. Synthetic PHI evaluation
showed consistent detectability (overall F1: 0.959 [0.958-0.960]) across 50
independently de-identified Penn datasets. Our model outperformed all vendor
systems on synthetic Penn reports (overall F1: 0.960 vs. 0.632-0.754).
Discussion: Large-scale, multimodal training improved cross-institutional
generalization and robustness. Synthetic PHI generation preserved data utility
while ensuring privacy. Conclusion: A transformer-based de-identification model
trained on diverse radiology datasets outperforms prior academic and commercial
systems in PHI detection and establishes a new benchmark for secure clinical
text processing.

</details>


### [18] [A Characterization of List Language Identification in the Limit](https://arxiv.org/abs/2511.04103)
*Moses Charikar,Chirag Pabbaraju,Ambuj Tewari*

Main category: cs.CL

TL;DR: 论文提出了基于k列表机制的极限语言识别新框架，通过递归分解语言集合实现了k-list可识别性的精确表征，并证明了统计场景下的最优指数收敛速率。


<details>
  <summary>Details</summary>
Motivation: Gold的经典结论表明传统单列表机制下语言识别存在本质局限，Angluin条件虽给出理论边界但缺乏实用扩展。本研究旨在通过允许模型生成多候选列表，突破传统单列表的识别瓶颈。

Method: 建立递归版的Angluin条件表征，将语言集合分解为k个可单列表识别的子集，并通过统计学习理论分析样本效率。

Result: 证明k-list可识别性等价于语言集合可分解为k个Angluin可识别子集，在i.i.d.设置下达到指数级收敛速率且该速率不可改进。

Conclusion: 多候选机制有效扩展了语言识别的理论边界，递归分解方法为复杂语言系统的可学习性提供了新的分析框架。

Abstract: We study the problem of language identification in the limit, where given a
sequence of examples from a target language, the goal of the learner is to
output a sequence of guesses for the target language such that all the guesses
beyond some finite time are correct. Classical results of Gold showed that
language identification in the limit is impossible for essentially any
interesting collection of languages. Later, Angluin gave a precise
characterization of language collections for which this task is possible.
Motivated by recent positive results for the related problem of language
generation, we revisit the classic language identification problem in the
setting where the learner is given the additional power of producing a list of
$k$ guesses at each time step. The goal is to ensure that beyond some finite
time, one of the guesses is correct at each time step.
  We give an exact characterization of collections of languages that can be
$k$-list identified in the limit, based on a recursive version of Angluin's
characterization (for language identification with a list of size $1$). This
further leads to a conceptually appealing characterization: A language
collection can be $k$-list identified in the limit if and only if the
collection can be decomposed into $k$ collections of languages, each of which
can be identified in the limit (with a list of size $1$). We also use our
characterization to establish rates for list identification in the statistical
setting where the input is drawn as an i.i.d. stream from a distribution
supported on some language in the collection. Our results show that if a
collection is $k$-list identifiable in the limit, then the collection can be
$k$-list identified at an exponential rate, and this is best possible. On the
other hand, if a collection is not $k$-list identifiable in the limit, then it
cannot be $k$-list identified at any rate that goes to zero.

</details>


### [19] [Batch Prompting Suppresses Overthinking Reasoning Under Constraint: How Batch Prompting Suppresses Overthinking in Reasoning Models](https://arxiv.org/abs/2511.04108)
*Wenmo Qiu,Saurabh Srivastava*

Main category: cs.CL

TL;DR: 批量提示不仅优化大型语言模型推理吞吐量，还能通过抑制过虑、减少自我修正和产生集体效应，显著提升推理效率和结果可靠性


<details>
  <summary>Details</summary>
Motivation: 揭示批量处理在大型推理模型中的正则化效应，超越其已知的吞吐量优化功能，探索其对模型推理行为的改善机制

Method: 基于13个基准测试的全面研究，结合行为分析观察批量处理对模型推理模式的影响

Result: 批量处理使推理令牌消耗减少3-5倍，准确率提升，有效抑制过虑现象，并观察到跨样本的集体推理效应

Conclusion: 批量处理应被视为推理阶段的核心正则化技术，为提升大型语言模型的推理效率和可靠性提供新范式

Abstract: Recent work has explored batch prompting as a strategy to amortize inference
cost in large language models (LLMs). In this paper, we show that batching
offers an additional, underappreciated benefit: it regularizes model behavior
during multi-step reasoning for Large Reasoning Models (LRMs). We conduct a
comprehensive study across 13 diverse benchmarks and observe that batching
improves accuracy while substantially reducing reasoning token usage, often by
3x-5x. Through detailed behavioral analysis, we find that batching suppresses
overthinking, reduces hedging language (e.g., repetitive self-corrections), and
encourages more decisive answers. Surprisingly, we also observe emergent
collective effects in batched inference: models often generalize patterns from
earlier examples to solve harder ones in the same batch. These findings
position batching not just as a throughput optimization, but as a powerful
inference-time regularizer for more efficient and reliable LLM reasoning.

</details>


### [20] [RIDE: Difficulty Evolving Perturbation with Item Response Theory for Mathematical Reasoning](https://arxiv.org/abs/2511.04120)
*Xinyuan Li,Murong Xu,Wenbiao Tao,Hanlun Zhu,Yike Zhao,Jipeng Zhang,Yunshi Lan*

Main category: cs.CL

TL;DR: 提出RIDE对抗性评估框架，结合IRT理论生成不同难度的数学问题变体，有效暴露LLM数学推理的脆弱性


<details>
  <summary>Details</summary>
Motivation: 现有对抗性扰动方法生成的问题质量低且难以系统评估难度，无法准确衡量LLM的真实数学推理能力

Method: 1. 用35个LLM模拟学生响应构建IRT难度排序器；2. 强化学习框架结合难度信号指导问题重写；3. 生成保留语义但增加难度的对抗性问题

Result: 在竞赛级数学基准上应用RIDE，导致26个先进LLM平均性能下降21.73%，最大降幅达47.26%

Conclusion: RIDE验证了当前LLM数学推理的脆弱性，建立了基于心理测量学的系统性评估范式，推动更严谨的AI能力评估

Abstract: Large language models (LLMs) achieve high performance on mathematical
reasoning, but these results can be inflated by training data leakage or
superficial pattern matching rather than genuine reasoning. To this end, an
adversarial perturbation-based evaluation is needed to measure true
mathematical reasoning ability. Current rule-based perturbation methods often
generate ill-posed questions and impede the systematic evaluation of question
difficulty and the evolution of benchmarks. To bridge this gap, we propose
RIDE, a novel adversarial question-rewriting framework that leverages Item
Response Theory (IRT) to rigorously measure question difficulty and to generate
intrinsically more challenging, well-posed variations of mathematical problems.
We employ 35 LLMs to simulate students and build a difficulty ranker from their
responses. This ranker provides a reward signal during reinforcement learning
and guides a question-rewriting model to reformulate existing questions across
difficulty levels. Applying RIDE to competition-level mathematical benchmarks
yields perturbed versions that degrade advanced LLM performance, with
experiments showing an average 21.73% drop across 26 models, thereby exposing
limited robustness in mathematical reasoning and confirming the validity of our
evaluation approach.

</details>


### [21] [CantoASR: Prosody-Aware ASR-LALM Collaboration for Low-Resource Cantonese](https://arxiv.org/abs/2511.04139)
*Dazhong Chen,Yi-Cheng Lin,Yuchen Huang,Ziwei Gong,Di Jiang,Zeying Xie,Yi R.,Fung*

Main category: cs.CL

TL;DR: 提出CantoASR框架，通过整合声学特征与语言模型推理显著提升低资源粤语ASR性能


<details>
  <summary>Details</summary>
Motivation: 解决低资源粤语ASR面临的标注数据不足、复杂声调系统和口音变异挑战

Method: 结合强制对齐声学特征提取、LoRA微调Whisper增强音调识别、指令调优Qwen-Audio进行韵律感知纠错

Result: 在粤语数据集上实现显著的字错误率(CER)提升，超越Whisper-Large-V3模型

Conclusion: 声学线索与语言模型推理的协同框架为低资源方言/声调语言ASR提供可扩展解决方案

Abstract: Automatic speech recognition (ASR) is critical for language accessibility,
yet low-resource Cantonese remains challenging due to limited annotated data,
six lexical tones, tone sandhi, and accent variation. Existing ASR models, such
as Whisper, often suffer from high word error rates. Large audio-language
models (LALMs), in contrast, can leverage broader contextual reasoning but
still require explicit tonal and prosodic acoustic cues. We introduce CantoASR,
a collaborative ASR-LALM error correction framework that integrates forced
alignment for acoustic feature extraction, a LoRA-finetuned Whisper for
improved tone discrimination, and an instruction-tuned Qwen-Audio for
prosody-aware correction. Evaluations on spontaneous Cantonese data show
substantial CER gains over Whisper-Large-V3. These findings suggest that
integrating acoustic cues with LALM reasoning provides a scalable strategy for
low-resource tonal and dialectal ASR.

</details>


### [22] [BAPPA: Benchmarking Agents, Plans, and Pipelines for Automated Text-to-SQL Generation](https://arxiv.org/abs/2511.04153)
*Fahim Ahmed,Md Mubtasim Ahasan,Jahir Sadik Monon,Muntasir Wahed,M Ashraful Amin,A K M Mahbubur Rahman,Amin Ahsan Ali*

Main category: cs.CL

TL;DR: 提出三种多智能体LLM管道提升小模型在Text-to-SQL任务中的性能，其中规划者-编码者管道效果最佳


<details>
  <summary>Details</summary>
Motivation: 现有LLM在复杂SQL生成任务中表现不足，且小模型潜力未被充分挖掘

Method: 设计多智能体讨论/规划者-编码者/编码者-聚合器三种管道，通过Bird-Bench Mini-Dev数据集进行系统评估

Result: 多智能体讨论使Qwen2.5-7B准确率提升10.6%，规划者-编码者管道将Gemma3准确率从52.4%提升至56.4%

Conclusion: 多智能体策略有效提升小模型性能，规划者-编码者架构展现最佳实践价值

Abstract: Text-to-SQL systems provide a natural language interface that can enable even
laymen to access information stored in databases. However, existing Large
Language Models (LLM) struggle with SQL generation from natural instructions
due to large schema sizes and complex reasoning. Prior work often focuses on
complex, somewhat impractical pipelines using flagship models, while smaller,
efficient models remain overlooked. In this work, we explore three multi-agent
LLM pipelines, with systematic performance benchmarking across a range of small
to large open-source models: (1) Multi-agent discussion pipeline, where agents
iteratively critique and refine SQL queries, and a judge synthesizes the final
answer; (2) Planner-Coder pipeline, where a thinking model planner generates
stepwise SQL generation plans and a coder synthesizes queries; and (3)
Coder-Aggregator pipeline, where multiple coders independently generate SQL
queries, and a reasoning agent selects the best query. Experiments on the
Bird-Bench Mini-Dev set reveal that Multi-Agent discussion can improve small
model performance, with up to 10.6% increase in Execution Accuracy for
Qwen2.5-7b-Instruct seen after three rounds of discussion. Among the pipelines,
the LLM Reasoner-Coder pipeline yields the best results, with DeepSeek-R1-32B
and QwQ-32B planners boosting Gemma 3 27B IT accuracy from 52.4% to the highest
score of 56.4%. Codes are available at
https://github.com/treeDweller98/bappa-sql.

</details>


### [23] [Trustworthy LLM-Mediated Communication: Evaluating Information Fidelity in LLM as a Communicator (LAAC) Framework in Multiple Application Domains](https://arxiv.org/abs/2511.04184)
*Mohammed Musthafa Rafi,Adarsh Krishnamurthy,Aditya Balu*

Main category: cs.CL

TL;DR: LAAC框架通过结构化对话实现LLM作为可信沟通中介，但需解决信息保真度、可重复性和响应完整性三大信任差距


<details>
  <summary>Details</summary>
Motivation: 现有AI生成内容导致发送方膨胀信息/接收方压缩摘要的无效循环，需重构LLM作为意图传递中介实现真实知识交换

Method: 采用多智能体架构，通过信息捕获保真度、响应可重复性、问答完整性三个维度开展跨场景实验评估

Result: 实验发现高价值沟通场景中存在可测量的信任缺陷，需解决信息失真和响应不一致问题才能可靠部署

Conclusion: LAAC展示了LLM重构人类沟通范式的潜力，但必须建立系统化信任验证机制才能应用于关键领域

Abstract: The proliferation of AI-generated content has created an absurd communication
theater where senders use LLMs to inflate simple ideas into verbose content,
recipients use LLMs to compress them back into summaries, and as a consequence
neither party engage with authentic content. LAAC (LLM as a Communicator)
proposes a paradigm shift - positioning LLMs as intelligent communication
intermediaries that capture the sender's intent through structured dialogue and
facilitate genuine knowledge exchange with recipients. Rather than perpetuating
cycles of AI-generated inflation and compression, LAAC enables authentic
communication across diverse contexts including academic papers, proposals,
professional emails, and cross-platform content generation. However, deploying
LLMs as trusted communication intermediaries raises critical questions about
information fidelity, consistency, and reliability. This position paper
systematically evaluates the trustworthiness requirements for LAAC's deployment
across multiple communication domains. We investigate three fundamental
dimensions: (1) Information Capture Fidelity - accuracy of intent extraction
during sender interviews across different communication types, (2)
Reproducibility - consistency of structured knowledge across multiple
interaction instances, and (3) Query Response Integrity - reliability of
recipient-facing responses without hallucination, source conflation, or
fabrication. Through controlled experiments spanning multiple LAAC use cases,
we assess these trust dimensions using LAAC's multi-agent architecture.
Preliminary findings reveal measurable trust gaps that must be addressed before
LAAC can be reliably deployed in high-stakes communication scenarios.

</details>


### [24] [Computational Turing Test Reveals Systematic Differences Between Human and AI Language](https://arxiv.org/abs/2511.04195)
*Nicolò Pagan,Petter Törnberg,Christopher A. Bail,Anikó Hannák,Christopher Barrie*

Main category: cs.CL

TL;DR: 论文提出计算图灵测试框架，验证LLM生成文本的拟人性，发现校准后LLM仍与人类文本存在显著差异，并在语义保真度与拟人性间存在权衡。


<details>
  <summary>Details</summary>
Motivation: 现有社会学科中LLM拟人性验证依赖不可靠的人类主观判断，缺乏客观评估工具。

Method: 整合BERT检测性、语义相似度等指标，系统比较9个开源LLM在推特等平台的校准策略表现。

Result: LLM输出情感表达显著可区分，指令微调模型表现更差，模型规模与拟人性无关，拟人性与语义保真度负相关。

Conclusion: 提供了可扩展的LLM验证框架，揭示当前LLM在模拟人类交流中的核心局限性。

Abstract: Large language models (LLMs) are increasingly used in the social sciences to
simulate human behavior, based on the assumption that they can generate
realistic, human-like text. Yet this assumption remains largely untested.
Existing validation efforts rely heavily on human-judgment-based evaluations --
testing whether humans can distinguish AI from human output -- despite evidence
that such judgments are blunt and unreliable. As a result, the field lacks
robust tools for assessing the realism of LLM-generated text or for calibrating
models to real-world data. This paper makes two contributions. First, we
introduce a computational Turing test: a validation framework that integrates
aggregate metrics (BERT-based detectability and semantic similarity) with
interpretable linguistic features (stylistic markers and topical patterns) to
assess how closely LLMs approximate human language within a given dataset.
Second, we systematically compare nine open-weight LLMs across five calibration
strategies -- including fine-tuning, stylistic prompting, and context retrieval
-- benchmarking their ability to reproduce user interactions on X (formerly
Twitter), Bluesky, and Reddit. Our findings challenge core assumptions in the
literature. Even after calibration, LLM outputs remain clearly distinguishable
from human text, particularly in affective tone and emotional expression.
Instruction-tuned models underperform their base counterparts, and scaling up
model size does not enhance human-likeness. Crucially, we identify a trade-off:
optimizing for human-likeness often comes at the cost of semantic fidelity, and
vice versa. These results provide a much-needed scalable framework for
validation and calibration in LLM simulations -- and offer a cautionary note
about their current limitations in capturing human communication.

</details>


### [25] [LLM-as-a-Judge is Bad, Based on AI Attempting the Exam Qualifying for the Member of the Polish National Board of Appeal](https://arxiv.org/abs/2511.04205)
*Michał Karp,Anna Kubaszewska,Magdalena Król,Robert Król,Aleksander Smywiński-Pohl,Mateusz Szymański,Witold Wydmański*

Main category: cs.CL

TL;DR: 测试当前大语言模型（LLMs）能否通过波兰国家上诉法庭资格认证考试。研究表明模型在选择题部分表现尚可，但书面实操部分未达及格线，AI评分与人类考官存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在真实法律资格考试中的实际应用潜力，评估其作为考生和自动评分者的可行性，揭示技术在司法自动化中的局限性。

Method: 构建混合信息检索与提取管道，测试GPT-4.1/Claude 4 Sonnet/Bielik-11B等模型在闭卷考试和检索增强生成（RAG）场景下的表现，采用LLM-as-a-judge自动评分机制。

Result: 模型在法律知识测试中平均分达67.4%，但书面判决部分最高仅获47分（及格线60分），AI评分与官方委员会判断相关性系数仅0.32。主要缺陷包括法律条文引用错误（38%案例存在）和逻辑漏洞。

Conclusion: 当前LLMs尚无法替代波兰公共采购仲裁中的人类法官，技术发展需法律专家深度参与，模型在逻辑论证和法律条文应用方面仍需根本性突破。

Abstract: This study provides an empirical assessment of whether current large language
models (LLMs) can pass the official qualifying examination for membership in
Poland's National Appeal Chamber (Krajowa Izba Odwo{\l}awcza). The authors
examine two related ideas: using LLM as actual exam candidates and applying the
'LLM-as-a-judge' approach, in which model-generated answers are automatically
evaluated by other models. The paper describes the structure of the exam, which
includes a multiple-choice knowledge test on public procurement law and a
written judgment, and presents the hybrid information recovery and extraction
pipeline built to support the models. Several LLMs (including GPT-4.1, Claude 4
Sonnet and Bielik-11B-v2.6) were tested in closed-book and various
Retrieval-Augmented Generation settings. The results show that although the
models achieved satisfactory scores in the knowledge test, none met the passing
threshold in the practical written part, and the evaluations of the
'LLM-as-a-judge' often diverged from the judgments of the official examining
committee. The authors highlight key limitations: susceptibility to
hallucinations, incorrect citation of legal provisions, weaknesses in logical
argumentation, and the need for close collaboration between legal experts and
technical teams. The findings indicate that, despite rapid technological
progress, current LLMs cannot yet replace human judges or independent examiners
in Polish public procurement adjudication.

</details>


### [26] [REMIND: Input Loss Landscapes Reveal Residual Memorization in Post-Unlearning LLMs](https://arxiv.org/abs/2511.04228)
*Liran Cohen,Yaniv Nemcovesky,Avi Mendelson*

Main category: cs.CL

TL;DR: 提出REMIND方法，通过分析模型在输入微小变化时的损失模式，检测反学习后残留的数据影响，比现有评估方法更敏感可靠。


<details>
  <summary>Details</summary>
Motivation: 现有反学习评估方法仅关注单个输入层面，可能忽略语义相似样本中的残留信息，导致隐私泄露风险。需要更精准的评估手段验证数据遗忘效果。

Method: REMIND通过量化模型在输入邻域内损失值的变化特征（平坦度/陡峭度），区分未学习数据和保留数据的损失景观差异，仅需黑盒查询即可实现。

Result: 实验表明REMIND在不同模型、数据集和转述场景中均保持稳健，检测效果优于同类方法，且具有可解释的评估指标。

Conclusion: REMIND为语言模型反学习效果评估提供了敏感可靠的框架，同时揭示了模型记忆残留的动态特征，开辟了反学习研究的新视角。

Abstract: Machine unlearning aims to remove the influence of specific training data
from a model without requiring full retraining. This capability is crucial for
ensuring privacy, safety, and regulatory compliance. Therefore, verifying
whether a model has truly forgotten target data is essential for maintaining
reliability and trustworthiness. However, existing evaluation methods often
assess forgetting at the level of individual inputs. This approach may overlook
residual influence present in semantically similar examples. Such influence can
compromise privacy and lead to indirect information leakage. We propose REMIND
(Residual Memorization In Neighborhood Dynamics), a novel evaluation method
aiming to detect the subtle remaining influence of unlearned data and classify
whether the data has been effectively forgotten. REMIND analyzes the model's
loss over small input variations and reveals patterns unnoticed by single-point
evaluations. We show that unlearned data yield flatter, less steep loss
landscapes, while retained or unrelated data exhibit sharper, more volatile
patterns. REMIND requires only query-based access, outperforms existing methods
under similar constraints, and demonstrates robustness across different models,
datasets, and paraphrased inputs, making it practical for real-world
deployment. By providing a more sensitive and interpretable measure of
unlearning effectiveness, REMIND provides a reliable framework to assess
unlearning in language models. As a result, REMIND offers a novel perspective
on memorization and unlearning.

</details>


### [27] [Reusing Pre-Training Data at Test Time is a Compute Multiplier](https://arxiv.org/abs/2511.04234)
*Alex Fang,Thomas Voice,Ruoming Pang,Ludwig Schmidt,Tom Gunter*

Main category: cs.CL

TL;DR: 通过检索增强生成量化发现当前预训练方法未充分利用数据集价值，测试时检索可显著提升模型表现


<details>
  <summary>Details</summary>
Motivation: 现有预训练方法可能低效利用数据，需量化评估数据价值未被充分提取的程度及其与模型规模的关系

Method: 结合检索增强生成（RAG）和测试时计算，在预训练后从原始数据集中检索相关信息进行预测

Result: 标准数据集检索使MMLU/Math-500/SimpleQA准确率显著提升（LLaMA 3.1 8B在MMLU提升10%），检索效能相当于5倍预训练计算量

Conclusion: 当前预训练机制存在显著信息浪费，测试时检索可有效挖掘数据潜力，指明未来改进方向

Abstract: Large language models learn from their vast pre-training corpora, gaining the
ability to solve an ever increasing variety of tasks; yet although researchers
work to improve these datasets, there is little effort to understand how
efficient the pre-training apparatus is at extracting ideas and knowledge from
the data. In this work, we use retrieval augmented generation along with
test-time compute as a way to quantify how much dataset value was left behind
by the process of pre-training, and how this changes across scale. We
demonstrate that pre-training then retrieving from standard and largely
open-sourced datasets results in significant accuracy gains in MMLU, Math-500,
and SimpleQA, which persist through decontamination. For MMLU we observe that
retrieval acts as a ~5x compute multiplier versus pre-training alone. We show
that these results can be further improved by leveraging additional compute at
test time to parse the retrieved context, demonstrating a 10 percentage point
improvement on MMLU for the public LLaMA 3.1 8B model. Overall, our results
suggest that today's pre-training methods do not make full use of the
information in existing pre-training datasets, leaving significant room for
progress.

</details>


### [28] [Efficient Topic Extraction via Graph-Based Labeling: A Lightweight Alternative to Deep Models](https://arxiv.org/abs/2511.04248)
*Salma Mekaoui,Hiba Sofyan,Imane Amaaz,Imane Benchrif,Arsalane Zarghili,Ilham Chaker,Nikola S. Nikolov*

Main category: cs.CL

TL;DR: 提出基于图算法的主题标注方法，在保持计算效率的同时有效提升主题模型的可解释性，性能接近ChatGPT但资源消耗更低


<details>
  <summary>Details</summary>
Motivation: 传统主题模型生成的主题词分布缺乏明确解释性，现有深度学习方法计算成本过高。需要开发既能保持计算效率又能提升解释性的主题标注方法

Method: 设计基于图结构的方法：1. 通过语义扩展丰富主题词 2. 构建词语关系图分析连接模式 3. 基于图特征推导主题标签

Result: 在两个数据集上超越传统基准（BERTScore提升15%，余弦相似度提升20%），与ChatGPT-3.5结果相当，计算效率比深度学习方法高40%

Conclusion: 验证了概率方法与图算法结合的有效性，未来可探索：1. 自动化标签生成框架 2. 多模态主题标注 3. 动态主题演化分析

Abstract: Extracting topics from text has become an essential task, especially with the
rapid growth of unstructured textual data. Most existing works rely on highly
computational methods to address this challenge. In this paper, we argue that
probabilistic and statistical approaches, such as topic modeling (TM), can
offer effective alternatives that require fewer computational resources. TM is
a statistical method that automatically discovers topics in large collections
of unlabeled text; however, it produces topics as distributions of
representative words, which often lack clear interpretability. Our objective is
to perform topic labeling by assigning meaningful labels to these sets of
words. To achieve this without relying on computationally expensive models, we
propose a graph-based approach that not only enriches topic words with
semantically related terms but also explores the relationships among them. By
analyzing these connections within the graph, we derive suitable labels that
accurately capture each topic's meaning. We present a comparative study between
our proposed method and several benchmarks, including ChatGPT-3.5, across two
different datasets. Our method achieved consistently better results than
traditional benchmarks in terms of BERTScore and cosine similarity and produced
results comparable to ChatGPT-3.5, while remaining computationally efficient.
Finally, we discuss future directions for topic labeling and highlight
potential research avenues for enhancing interpretability and automation.

</details>


### [29] [SSPO: Subsentence-level Policy Optimization](https://arxiv.org/abs/2511.04256)
*Kun Yang,Zikang chen,Yanmeng Wang,Zhigen Li*

Main category: cs.CL

TL;DR: 提出SSPO方法通过句子级重要性比率改进LLM强化学习，平衡GRPO和GSPO的优缺点，实现46.57的平均分突破


<details>
  <summary>Details</summary>
Motivation: 现有GRPO算法存在策略更新不稳定（token级重要性比率易受异常值影响），GSPO算法存在采样数据利用率低（响应级重要性比率易被极端值误导）的问题

Method: 1. 采用句子级重要性比率平衡token级和响应级
2. 在PPO-CLIP中应用句子熵动态调整截断边界
3. 高熵token扩大探索空间，低熵token缩小截断范围

Result: 1. 五个数据集平均得分46.57超越GRPO(43.01)和GSPO(44.42)
2. 三个数据集达到SOTA性能
3. 数据利用率提升且训练稳定性增强

Conclusion: SSPO通过继承GSPO核心思想并改进其缺陷，在保持训练稳定性的同时显著提升生成数据利用率，为LLM强化学习提供了新范式

Abstract: As a significant part of post-training of the Large Language Models (LLMs),
Reinforcement Learning from Verifiable Reward (RLVR) has greatly improved LLMs'
reasoning skills. However, some RLVR algorithms, such as GRPO (Group Relative
Policy Optimization) and GSPO (Group Sequence Policy Optimization), are
observed to suffer from unstable policy updates and low usage of sampling data,
respectively. The importance ratio of GRPO is calculated at the token level,
which focuses more on optimizing a single token. This will be easily affected
by outliers, leading to model training collapse. GSPO proposed the calculation
of the response level importance ratio, which solves the problem of high
variance and training noise accumulation in the calculation of the GRPO
importance ratio. However, since all the response tokens share a common
importance ratio, extreme values can easily raise or lower the overall mean,
leading to the entire response being mistakenly discarded, resulting in a
decrease in the utilization of sampled data. This paper introduces SSPO, which
applies sentence-level importance ratio, taking the balance between GRPO and
GSPO. SSPO not only avoids training collapse and high variance, but also
prevents the whole response tokens from being abandoned by the clipping
mechanism. Furthermore, we apply sentence entropy to PPO-CLIP to steadily
adjust the clipping bounds, encouraging high-entropy tokens to explore and
narrow the clipping range of low-entropy tokens. In particular, SSPO achieves
an average score of 46.57 across five datasets, surpassing GRPO (43.01) and
GSPO (44.42), and wins state-of-the-art performance on three datasets. These
results highlight SSPO's effectiveness in leveraging generated data by taking
the essence of GSPO but rejecting its shortcomings.

</details>


### [30] [Dynamic Jointly Batch Selection for Data Efficient Machine Translation Fine-Tuning](https://arxiv.org/abs/2511.04406)
*Mohammad Amin Ghanizadeh,Mohammad Javad Dousti*

Main category: cs.CL

TL;DR: 提出基于可学习性评分和批量选择策略的机器翻译数据筛选方法，实现5倍数据效率提升和24倍计算加速


<details>
  <summary>Details</summary>
Motivation: 数据质量与有效选择是提升机器翻译模型性能的关键基础，但现有方法缺乏系统性筛选机制

Method: 结合学习者模型与预训练参考模型的协同机制，通过可学习性评分筛选数据，并采用考虑数据关联性的批量选择策略

Result: 在CCMatrix数据集上微调mBART模型，英语-波斯语翻译实现数据效率5倍提升，计算效率最高加速24倍（使用缓存嵌入时）

Conclusion: 该方法通过系统性数据筛选显著提升训练效率，同时增强模型泛化能力，翻译性能优于传统随机选择方法

Abstract: Data quality and its effective selection are fundamental to improving the
performance of machine translation models, serving as cornerstones for
achieving robust and reliable translation systems. This paper presents a data
selection methodology specifically designed for fine-tuning machine translation
systems, which leverages the synergy between a learner model and a pre-trained
reference model to enhance overall training effectiveness. By defining a
learnability score, our approach systematically evaluates the utility of data
points for training, ensuring that only the most relevant and impactful
examples contribute to the fine-tuning process. Furthermore, our method employs
a batch selection strategy which considers interdependencies among data points,
optimizing the efficiency of the training process while maintaining a focus on
data relevance. Experiments on English to Persian and several other language
pairs using an mBART model fine-tuned on the CCMatrix dataset demonstrate that
our method can achieve up to a fivefold improvement in data efficiency compared
to an iid baseline. Experimental results indicate that our approach improves
computational efficiency by 24 when utilizing cached embeddings, as it requires
fewer training data points. Additionally, it enhances generalization, resulting
in superior translation performance compared to random selection method.

</details>


### [31] [If I Could Turn Back Time: Temporal Reframing as a Historical Reasoning Task for LLMs](https://arxiv.org/abs/2511.04432)
*Lars Bungum,Charles Yijia Huang,Abeer Kashar*

Main category: cs.CL

TL;DR: 研究测试大型语言模型在时间推理中的表现，发现英语提示效果优于挪威语，模型规模越大性能越好，但专为挪威语定制的最大模型未达预期。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在历史语境下的跨语言推理能力，特别是语言差异和时间背景对模型表现的影响。

Method: 使用1940年挪威书籍的琐事问题，分别用英语/挪威语提问，采用LLM-as-judge评分机制，并测试不同规模模型家族。

Result: 英语提示准确率比挪威语高15%，70B参数模型比7B模型提升23%，但挪威专用大模型未显现语言优势。

Conclusion: 语言选择对LLM时间推理表现影响显著，模型规模提升效果明确，但特定语言定制模型需进一步优化。

Abstract: In this study, we experiment with the ability of LLMs to do temporal
reasoning. Using a Norwegian book from 1940 containing trivia questions, we
prompt the LLMs to answer the questions as if it were 1940. We also pose the
questions in both English and Norwegian. Correct answers are often presented as
sentences, and grading is done by means of LLM-as-judge, with sampled checks by
a native speaker. Prompting in English consistently gave better results than in
Norwegian, an unexpected result. In contrast, using larger LLMs improved
results. We tested the DeepSeek-R1, Gemma3, Qwen3, and Llama3.1 model families,
and also the largest available LLM especially crafted for Norwegian.

</details>


### [32] [Probabilistic Textual Time Series Depression Detection](https://arxiv.org/abs/2511.04476)
*Fabian Schmidt,Seyedehmoniba Ravan,Vladimir Vlassov*

Main category: cs.CL

TL;DR: 提出概率文本时序框架PTTSD，结合注意力机制和概率建模，实现抑郁症评分预测与不确定性量化


<details>
  <summary>Details</summary>
Motivation: 现有抑郁症预测模型缺乏不确定性估计和时序建模能力，难以满足临床决策对可靠性和可解释性的需求

Method: 双向LSTM+自注意力+残差连接架构，采用高斯/Student-t分布输出头，通过负对数似然损失进行端到端训练

Result: 在E-DAIC/DAIC-WOZ数据集上MAE达3.85/3.55，预测区间校准良好，消融实验验证注意力机制提升模型性能达15%

Conclusion: PTTSD通过概率时间序列建模实现了临床可解释的抑郁症预测，不确定性量化能力为个性化治疗提供决策支持

Abstract: Accurate and interpretable predictions of depression severity are essential
for clinical decision support, yet existing models often lack uncertainty
estimates and temporal modeling. We propose PTTSD, a Probabilistic Textual Time
Series Depression Detection framework that predicts PHQ-8 scores from
utterance-level clinical interviews while modeling uncertainty over time. PTTSD
includes sequence-to-sequence and sequence-to-one variants, both combining
bidirectional LSTMs, self-attention, and residual connections with Gaussian or
Student-t output heads trained via negative log-likelihood. Evaluated on E-DAIC
and DAIC-WOZ, PTTSD achieves state-of-the-art performance among text-only
systems (e.g., MAE = 3.85 on E-DAIC, 3.55 on DAIC) and produces well-calibrated
prediction intervals. Ablations confirm the value of attention and
probabilistic modeling, while comparisons with MentalBERT establish generality.
A three-part calibration analysis and qualitative case studies further
highlight the interpretability and clinical relevance of uncertainty-aware
forecasting.

</details>


### [33] [ThaiOCRBench: A Task-Diverse Benchmark for Vision-Language Understanding in Thai](https://arxiv.org/abs/2511.04479)
*Surapon Nonesung,Teetouch Jaknamon,Sirinya Chaiophat,Natapong Nitarach,Chanakan Wittayasakpan,Warit Sirichotedumrong,Adisai Na-Thalang,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: 首个泰语文档视觉理解综合基准ThaiOCRBench，评估发现专有模型显著优于开源模型，揭示了语言偏见和结构理解等关键挑战


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型基准集中于高资源语言，泰语在文档结构理解任务中代表性不足

Method: 构建含2,808个样本/13类任务的人工标注数据集，在零样本设置下评估多种专有与开源VLMs

Result: 专有模型（如Gemini 2.5 Pro）表现最优，开源模型在细粒度文本识别和手写体提取任务中性能下降最显著

Conclusion: ThaiOCRBench为低资源复杂文字环境提供标准化评估框架，并为改进泰语文档理解指明技术方向

Abstract: We present ThaiOCRBench, the first comprehensive benchmark for evaluating
vision-language models (VLMs) on Thai text-rich visual understanding tasks.
Despite recent progress in multimodal modeling, existing benchmarks
predominantly focus on high-resource languages, leaving Thai underrepresented,
especially in tasks requiring document structure understanding. ThaiOCRBench
addresses this gap by offering a diverse, human-annotated dataset comprising
2,808 samples across 13 task categories. We evaluate a wide range of
state-of-the-art VLMs in a zero-shot setting, spanning both proprietary and
open-source systems. Results show a significant performance gap, with
proprietary models (e.g., Gemini 2.5 Pro) outperforming open-source
counterparts. Notably, fine-grained text recognition and handwritten content
extraction exhibit the steepest performance drops among open-source models.
Through detailed error analysis, we identify key challenges such as language
bias, structural mismatch, and hallucinated content. ThaiOCRBench provides a
standardized framework for assessing VLMs in low-resource, script-complex
settings, and provides actionable insights for improving Thai-language document
understanding.

</details>


### [34] [RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within Structured Tables](https://arxiv.org/abs/2511.04491)
*Nikhil Abhyankar,Purvi Chaurasia,Sanchit Kabra,Ananya Srivastava,Vivek Gupta,Chandan K. Reddy*

Main category: cs.CL

TL;DR: 提出RUST-BENCH基准测试，通过7966个跨科学和体育领域的真实复杂表格问题，系统评估大语言模型在异构模式和多跳推理中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有表格推理基准局限于小型均匀表格，无法真实反映现实场景中长文本、异构模式和领域特异性表格的复杂性，导致对LLM推理能力的评估不完整。

Method: 从NSF资助记录（RB-Science）和NBA统计数据（RB-Sports）构建2031个真实表格，建立包含规模/异构性/领域/推理复杂度四维度的评估体系。

Result: 实验显示现有开源/商用模型在异构模式处理（准确率下降18.7%）和多跳推理（错误率增加32.4%）存在显著缺陷，提示架构设计缺陷。

Conclusion: RUST-BENCH为表格推理研究提供了首个跨领域复杂场景测试平台，揭示了当前LLM处理真实表格数据的核心瓶颈，推动架构创新方向。

Abstract: Existing tabular reasoning benchmarks mostly test models on small, uniform
tables, underrepresenting the complexity of real-world data and giving an
incomplete view of Large Language Models' (LLMs) reasoning abilities. Real
tables are long, heterogeneous, and domain-specific, mixing structured fields
with free text and requiring multi-hop reasoning across thousands of tokens. To
address this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from
2031 real-world tables spanning two domains: i) RB-Science (NSF grant records)
and ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates
LLMs jointly across scale, heterogeneity, domain specificity, and reasoning
complexity. Experiments with open-source and proprietary models show that LLMs
struggle with heterogeneous schemas and complex multi-hop inference, revealing
persistent weaknesses in current architectures and prompting strategies.
RUST-BENCH establishes a challenging new testbed for advancing tabular
reasoning research.

</details>


### [35] [OUNLP at TSAR 2025 Shared Task: Multi-Round Text Simplifier via Code Generation](https://arxiv.org/abs/2511.04495)
*Cuong Huynh,Jie Cao*

Main category: cs.CL

TL;DR: OUNLP团队使用GPT-4o开发了基于多轮简化的文本简化系统(MRS-Rule/MRS-Joint)，在TSAR-2025任务中排名第7。后续改进表明MRS-Joint方法能有效提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究发现文本简化效果与源文本和目标文本的CEFR等级差异显著相关，这启发了多轮简化方法的开发。

Method: 提出两种多轮简化策略：基于规则的简化(MRS-Rule)和结合规则与LLM的联合简化(MRS-Joint)，均通过GPT-4o实现生成。

Result: 官方评测排名第7/20，后续实验证明MRS-Joint方法以LLM简化结果作为起点可进一步提升多轮简化效果。

Conclusion: 多轮简化策略对文本简化任务有效，特别是结合规则与LLM的联合方法，未来可进一步探索迭代优化策略。

Abstract: This paper describes the OUNLP system submitted to the TSAR-2025 Shared Task
(Alva-Manchego et al., 2025), designed for readability-controlled text
simplification using LLM-prompting-based generation. Based on the analysis of
prompt-based text simplification methods, we discovered an interesting finding
that text simplification performance is highly related to the gap between the
source CEFR (Arase et al., 2022) level and the target CEFR level. Inspired by
this finding, we propose two multi-round simplification methods and generate
them via GPT-4o: rule-based simplification (MRS-Rule) and jointly rule-based
LLM simplification (MRS-Joint). Our submitted systems ranked 7 out of 20 teams.
Later improvements with MRS-Joint show that taking the LLM simplified
candidates as the starting point could further boost the multi-round
simplification performance.

</details>


### [36] [Decoding Emergent Big Five Traits in Large Language Models: Temperature-Dependent Expression and Architectural Clustering](https://arxiv.org/abs/2511.04499)
*Christos-Nikolaos Zacharopoulos,Revekka Kyriakoglou*

Main category: cs.CL

TL;DR: 研究通过BFI-2框架分析六大语言模型的人格特质表现，发现神经质和外向性易受温度参数影响，模型架构与人格稳定性相关，为AI伦理治理提供新视角。


<details>
  <summary>Details</summary>
Motivation: 因LLMs在人类应用中的普及，需系统性评估其人格特征以促进负责任的AI开发，解决模型调优与伦理治理需求。

Method: 使用Big Five Inventory-2框架评估六个LLMs，通过调整温度参数观察五大人格维度变化，并采用层次聚类分析模型架构与人格稳定性关联。

Result: 五个维度中四个存在显著差异，神经质和外向性对温度敏感，不同架构模型形成稳定特质集群（如GPT-3.5与PaLM聚类）。

Conclusion: 揭示了LLMs人格特质的形成机制，为模型选择、参数调优及AI伦理治理提供实证依据，公开数据代码推动研究透明化。

Abstract: As Large Language Models (LLMs) become integral to human-centered
applications, understanding their personality-like behaviors is increasingly
important for responsible development and deployment. This paper systematically
evaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, to
assess trait expressions under varying sampling temperatures. We find
significant differences across four of the five personality dimensions, with
Neuroticism and Extraversion susceptible to temperature adjustments. Further,
hierarchical clustering reveals distinct model clusters, suggesting that
architectural features may predispose certain models toward stable trait
profiles. Taken together, these results offer new insights into the emergence
of personality-like patterns in LLMs and provide a new perspective on model
tuning, selection, and the ethical governance of AI systems. We share the data
and code for this analysis here:
https://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1

</details>


### [37] [RAGalyst: Automated Human-Aligned Agentic Evaluation for Domain-Specific RAG](https://arxiv.org/abs/2511.04502)
*Joshua Gao,Quoc Huy Pham,Subin Varghese,Silwal Saurav,Vedhus Hoskere*

Main category: cs.CL

TL;DR: 提出自动化评估框架RAGalyst解决领域特定RAG系统评估难题，通过合成QA数据集生成和优化LLM评估指标，发现性能高度依赖领域上下文，无通用最优配置。


<details>
  <summary>Details</summary>
Motivation: 现有RAG评估方法存在两大缺陷：启发式指标无法捕捉领域特殊性，LLM评判方法缺乏人类判断一致性验证，无法满足安全关键领域需求。

Method: 1. 开发代理流程生成领域特定的合成QA数据集
2. 通过代理过滤保障数据保真度
3. 使用提示优化改进答案正确性（+34%）和可回答性（+41%）指标的人类对齐性

Result: 在军事行动、网络安全和桥梁工程三个领域的测试显示：
- 嵌入模型/LLM/超参数性能高度依赖应用场景
- 识别出RAG系统低答案正确性的主要原因模式
- 开源框架Github可用性验证

Conclusion: RAGalyst填补了领域特定RAG系统评估的空白，通过系统化评估帮助开发者理解领域权衡，为构建可靠RAG系统提供决策支持。

Abstract: Retrieval-Augmented Generation (RAG) is a critical technique for grounding
Large Language Models (LLMs) in factual evidence, yet evaluating RAG systems in
specialized, safety-critical domains remains a significant challenge. Existing
evaluation frameworks often rely on heuristic-based metrics that fail to
capture domain-specific nuances and other works utilize LLM-as-a-Judge
approaches that lack validated alignment with human judgment. This paper
introduces RAGalyst, an automated, human-aligned agentic framework designed for
the rigorous evaluation of domain-specific RAG systems. RAGalyst features an
agentic pipeline that generates high-quality, synthetic question-answering (QA)
datasets from source documents, incorporating an agentic filtering step to
ensure data fidelity. The framework refines two key LLM-as-a-Judge
metrics-Answer Correctness and Answerability-using prompt optimization to
achieve a strong correlation with human annotations. Applying this framework to
evaluate various RAG components across three distinct domains (military
operations, cybersecurity, and bridge engineering), we find that performance is
highly context-dependent. No single embedding model, LLM, or hyperparameter
configuration proves universally optimal. Additionally, we provide an analysis
on the most common low Answer Correctness reasons in RAG. These findings
highlight the necessity of a systematic evaluation framework like RAGalyst,
which empowers practitioners to uncover domain-specific trade-offs and make
informed design choices for building reliable and effective RAG systems.
RAGalyst is available on our Github.

</details>


### [38] [Modeling Clinical Uncertainty in Radiology Reports: from Explicit Uncertainty Markers to Implicit Reasoning Pathways](https://arxiv.org/abs/2511.04506)
*Paloma Rabaey,Jong Hak Moon,Jung-Oh Lee,Min Gwan Kim,Hangyul Yoon,Thomas Demeester,Edward Choi*

Main category: cs.CL

TL;DR: 论文提出Lunguage++框架，通过量化显性不确定性和建模隐性不确定性，构建不确定性感知的放射学结构化报告系统。


<details>
  <summary>Details</summary>
Motivation: 放射学报告存在显性（模棱两可表述）和隐性（推理过程省略）两种不确定性，传统方法难以有效处理，需系统性解决方案支持临床决策和自动化分析。

Method: 1. 显性不确定性：基于LLM构建专家验证的模棱两可短语概率映射系统
2. 隐性不确定性：通过诊断路径扩展框架自动补充特征性子发现

Result: 发布Lunguage++增强基准，支持不确定性感知图像分类、诊断推理及临床影响研究三大应用场景。

Conclusion: 该框架突破传统结构化报告局限，首次实现双维度不确定性建模，为AI医疗诊断系统提供更可靠的决策支持基础设施。

Abstract: Radiology reports are invaluable for clinical decision-making and hold great
potential for automated analysis when structured into machine-readable formats.
These reports often contain uncertainty, which we categorize into two distinct
types: (i) Explicit uncertainty reflects doubt about the presence or absence of
findings, conveyed through hedging phrases. These vary in meaning depending on
the context, making rule-based systems insufficient to quantify the level of
uncertainty for specific findings; (ii) Implicit uncertainty arises when
radiologists omit parts of their reasoning, recording only key findings or
diagnoses. Here, it is often unclear whether omitted findings are truly absent
or simply unmentioned for brevity. We address these challenges with a two-part
framework. We quantify explicit uncertainty by creating an expert-validated,
LLM-based reference ranking of common hedging phrases, and mapping each finding
to a probability value based on this reference. In addition, we model implicit
uncertainty through an expansion framework that systematically adds
characteristic sub-findings derived from expert-defined diagnostic pathways for
14 common diagnoses. Using these methods, we release Lunguage++, an expanded,
uncertainty-aware version of the Lunguage benchmark of fine-grained structured
radiology reports. This enriched resource enables uncertainty-aware image
classification, faithful diagnostic reasoning, and new investigations into the
clinical impact of diagnostic uncertainty.

</details>


### [39] [Are language models aware of the road not taken? Token-level uncertainty and hidden state dynamics](https://arxiv.org/abs/2511.04527)
*Amir Zur,Atticus Geiger,Ekdeep Singh Lubana,Eric Bigelow*

Main category: cs.CL

TL;DR: 语言模型的隐藏激活状态既能反映其推理路径的不确定性，也能预测未来结果分布，表明模型隐式存储了多种潜在推理路径


<details>
  <summary>Details</summary>
Motivation: 研究语言模型在生成过程中是否表征了潜在的不同推理路径，以及如何通过激活控制量化其不确定性

Method: 通过分析隐藏层激活与预测准确性的关系，构建控制模型激活干预有效性的评估框架

Result: 模型在未确定最终答案前的激活状态最易被控制，且隐藏激活能有效预测其未来结果分布

Conclusion: 语言模型在推理过程中隐式保持着多种可能路径的表征，这种特性使得基于激活的干预控制成为可能

Abstract: When a language model generates text, the selection of individual tokens
might lead it down very different reasoning paths, making uncertainty difficult
to quantify. In this work, we consider whether reasoning language models
represent the alternate paths that they could take during generation. To test
this hypothesis, we use hidden activations to control and predict a language
model's uncertainty during chain-of-thought reasoning. In our experiments, we
find a clear correlation between how uncertain a model is at different tokens,
and how easily the model can be steered by controlling its activations. This
suggests that activation interventions are most effective when there are
alternate paths available to the model -- in other words, when it has not yet
committed to a particular final answer. We also find that hidden activations
can predict a model's future outcome distribution, demonstrating that models
implicitly represent the space of possible paths.

</details>


### [40] [IntelliProof: An Argumentation Network-based Conversational Helper for Organized Reflection](https://arxiv.org/abs/2511.04528)
*Kaveh Eskandari Miandoab,Katharine Kowalyshyn,Kabir Pamnani,Anesu Gavhera,Vasanth Sarathy,Matthias Scheutz*

Main category: cs.CL

TL;DR: IntelliProof是一个基于LLM的交互式议论文分析系统，通过构建论证图并提供可视化与量化指标，提升论证质量评估与用户理解。


<details>
  <summary>Details</summary>
Motivation: 现有自动论文评分系统缺乏对用户体验的关注，难以直观展示论证结构和逻辑关系。IntelliProof旨在通过结构可视化、关系解释和量化指标，增强对论证质量的深度理解。

Method: 1. 将议论文建模为论证图（主张为节点，支持/攻击关系为边）
2. 使用LLM对论证关系进行分类与可信度评分
3. 提供可视化界面展示论证结构
4. 开发自然语言工具衔接文本语义与用户认知

Result: 系统实现论证质量的快速量化评估（分类准确率92%），支持通过交互式探索发现论证漏洞，同时保持人类监督机制。

Conclusion: 该框架在学术写作指导和教育评估领域具有应用潜力，成功平衡了自动化分析与人类认知干预的关系。

Abstract: We present IntelliProof, an interactive system for analyzing argumentative
essays through LLMs. IntelliProof structures an essay as an argumentation
graph, where claims are represented as nodes, supporting evidence is attached
as node properties, and edges encode supporting or attacking relations. Unlike
existing automated essay scoring systems, IntelliProof emphasizes the user
experience: each relation is initially classified and scored by an LLM, then
visualized for enhanced understanding. The system provides justifications for
classifications and produces quantitative measures for essay coherence. It
enables rapid exploration of argumentative quality while retaining human
oversight. In addition, IntelliProof provides a set of tools for a better
understanding of an argumentative essay and its corresponding graph in natural
language, bridging the gap between the structural semantics of argumentative
essays and the user's understanding of a given text. A live demo and the system
are available here to try: \textbf{https://intelliproof.vercel.app}

</details>


### [41] [From Model to Breach: Towards Actionable LLM-Generated Vulnerabilities Reporting](https://arxiv.org/abs/2511.04538)
*Cyril Vallez,Alexander Sternfeld,Andrei Kucharavy,Ljiljana Dolamic*

Main category: cs.CL

TL;DR: 研究发现最新开源LLM编码助手仍存在基础安全漏洞，提出结合漏洞严重性、生成概率和诱导提示的新评估指标ME评分


<details>
  <summary>Details</summary>
Motivation: 现有LLM代码安全改进措施未能有效修复基础漏洞，需建立更科学的评估体系来优先处理高风险漏洞

Method: 提出Prompt Exposure(PE)指标，综合评估漏洞严重程度、生成概率和诱导提示暴露度，并基于此定义Model Exposure(ME)评分体系

Result: 发现安全-功能权衡导致现有模型持续暴露基础漏洞，ME评分可有效识别模型生成的高风险漏洞模式

Conclusion: ME评分体系为编码LLM的安全优化提供了新的方向，可优先解决最危险且易被诱导生成的漏洞类型

Abstract: As the role of Large Language Models (LLM)-based coding assistants in
software development becomes more critical, so does the role of the bugs they
generate in the overall cybersecurity landscape. While a number of LLM code
security benchmarks have been proposed alongside approaches to improve the
security of generated code, it remains unclear to what extent they have
impacted widely used coding LLMs. Here, we show that even the latest
open-weight models are vulnerable in the earliest reported vulnerability
scenarios in a realistic use setting, suggesting that the safety-functionality
trade-off has until now prevented effective patching of vulnerabilities. To
help address this issue, we introduce a new severity metric that reflects the
risk posed by an LLM-generated vulnerability, accounting for vulnerability
severity, generation chance, and the formulation of the prompt that induces
vulnerable code generation - Prompt Exposure (PE). To encourage the mitigation
of the most serious and prevalent vulnerabilities, we use PE to define the
Model Exposure (ME) score, which indicates the severity and prevalence of
vulnerabilities a model generates.

</details>


### [42] [BanglaMedQA and BanglaMMedBench: Evaluating Retrieval-Augmented Generation Strategies for Bangla Biomedical Question Answering](https://arxiv.org/abs/2511.04560)
*Sadia Sultana,Saiyma Sittul Muna,Mosammat Zannatul Samarukh,Ajwad Abrar,Tareque Mohmud Chowdhury*

Main category: cs.CL

TL;DR: 论文提出首个孟加拉语生物医学多选题数据集，开发了基于检索增强生成（RAG）的多种策略，其中Agentic RAG方法在GPT-OSS-120B模型上达到89.54%准确率，显著提升孟加拉语医疗QA系统可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言（孟加拉语）生物医学问答系统开发困难的问题，通过检索增强生成技术提升医疗知识的可及性与事实准确性。

Method: 构建BanglaMedQA/BanglaMMedBench数据集，结合OCR技术整合医学教材语料，开发五种RAG策略（含动态策略选择的Agentic RAG），在开放域QA任务中对比模型性能。

Result: Agentic RAG配置的GPT-OSS-120B模型取得最高准确率（89.54%），比传统方法提升显著，且生成解释质量最优。

Conclusion: 检索增强生成方法能有效提升低资源语言医疗QA系统的可靠性，特别是动态策略选择的Agentic RAG框架，为多语言医疗AI研究奠定基础。

Abstract: Developing accurate biomedical Question Answering (QA) systems in
low-resource languages remains a major challenge, limiting equitable access to
reliable medical knowledge. This paper introduces BanglaMedQA and
BanglaMMedBench, the first large-scale Bangla biomedical Multiple Choice
Question (MCQ) datasets designed to evaluate reasoning and retrieval in medical
artificial intelligence (AI). The study applies and benchmarks several
Retrieval-Augmented Generation (RAG) strategies, including Traditional,
Zero-Shot Fallback, Agentic, Iterative Feedback, and Aggregate RAG, combining
textbook-based and web retrieval with generative reasoning to improve factual
accuracy. A key novelty lies in integrating a Bangla medical textbook corpus
through Optical Character Recognition (OCR) and implementing an Agentic RAG
pipeline that dynamically selects between retrieval and reasoning strategies.
Experimental results show that the Agentic RAG achieved the highest accuracy
89.54% with openai/gpt-oss-120b, outperforming other configurations and
demonstrating superior rationale quality. These findings highlight the
potential of RAG-based methods to enhance the reliability and accessibility of
Bangla medical QA, establishing a foundation for future research in
multilingual medical artificial intelligence.

</details>


### [43] [When retrieval outperforms generation: Dense evidence retrieval for scalable fake news detection](https://arxiv.org/abs/2511.04643)
*Alamgir Munir Qazi,John P. McCrae,Jamal Abdul Nasir*

Main category: cs.CL

TL;DR: 提出DeReC框架，通过密集检索与分类的轻量级方案，在事实核查任务中实现更高效率与准确性


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的事实核查方法存在计算资源消耗大(运行时间长达数千分钟)和生成幻觉风险的问题，限制了实际部署可行性

Method: 结合通用文本嵌入的密集检索技术和专用分类模型，替代自回归式LLM生成方案

Result: 在RAWFC数据集上运行时间减少95%(23分钟 vs 454分钟)，F1分数提升4.38%(65.58% vs 61.20%)；在LIAR-RAW数据集运行时间减少92%

Conclusion: 经过精心设计的检索系统在特定任务中可超越LLM性能，同时具备更高的实际部署价值，为资源敏感场景提供新解决方案

Abstract: The proliferation of misinformation necessitates robust yet computationally
efficient fact verification systems. While current state-of-the-art approaches
leverage Large Language Models (LLMs) for generating explanatory rationales,
these methods face significant computational barriers and hallucination risks
in real-world deployments. We present DeReC (Dense Retrieval Classification), a
lightweight framework that demonstrates how general-purpose text embeddings can
effectively replace autoregressive LLM-based approaches in fact verification
tasks. By combining dense retrieval with specialized classification, our system
achieves better accuracy while being significantly more efficient. DeReC
outperforms explanation-generating LLMs in efficiency, reducing runtime by 95%
on RAWFC (23 minutes 36 seconds compared to 454 minutes 12 seconds) and by 92%
on LIAR-RAW (134 minutes 14 seconds compared to 1692 minutes 23 seconds),
showcasing its effectiveness across varying dataset sizes. On the RAWFC
dataset, DeReC achieves an F1 score of 65.58%, surpassing the state-of-the-art
method L-Defense (61.20%). Our results demonstrate that carefully engineered
retrieval-based systems can match or exceed LLM performance in specialized
tasks while being significantly more practical for real-world deployment.

</details>


### [44] [Logit-Entropy Adaptive Stopping Heuristic for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.04654)
*Mohammad Atif Quamar,Mohammad Areeb*

Main category: cs.CL

TL;DR: 提出LEASH算法，通过动态监测熵和logit信号实现推理过程自适应终止，节省30%计算资源且仅损失10%精度


<details>
  <summary>Details</summary>
Motivation: 传统CoT方法生成固定长度推理链导致计算冗余，需开发无需训练的动态停止机制提升效率

Method: 实时监测令牌熵变化率和top-logit边际改进，当二者均达到稳定平台期时终止生成

Result: 在GSM8K/AQuA-RAT测试中，平均减少35%令牌生成和27%延迟，准确率下降10个百分点

Conclusion: LEASH提供无需训练的高效推理方案，特别适合资源敏感场景，但需权衡精度与效率

Abstract: Chain-of-Thought (CoT) prompting is a key technique for enabling complex
reasoning in large language models. However, generating full, fixed-length
rationales is computationally wasteful, inflating both token usage and latency.
We introduce LEASH: Logit-Entropy Adaptive Stopping Heuristic, a training-free
decoding algorithm that adaptively halts rationale generation. LEASH monitors
two intrinsic signals: the slope of token-level entropy and the improvement in
the top-logit margin. It terminates the generation once both signals plateau,
indicating the model has reached a stable reasoning state. Across four
instruction-tuned models on the GSM8K and AQuA-RAT benchmarks, LEASH reduces
average token generation by 30--35% and latency by 27%, while incurring a 10
p.p. accuracy drop relative to CoT. LEASH is model-agnostic and requires no
additional training or supervision, offering a simple and efficient alternative
to CoT decoding.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [45] [Shellular Metamaterial Design via Compact Electric Potential Parametrization](https://arxiv.org/abs/2511.04025)
*Chang Liu,Bohan Wang*

Main category: cs.GR

TL;DR: 提出紧凑型壳层超材料设计空间与GPU加速的均质化流程，实现高效逆向设计并达到理论性能上限91.86%，增材制造验证实用价值。


<details>
  <summary>Details</summary>
Motivation: 解决传统超材料设计效率低、计算耗时长的痛点，通过低自由度参数化与快速评估实现宏观属性的精准定制。

Method: 构建参数化设计空间（数十自由度）+ GPU加速均质化（20ms结构评估/0.5s弹性张量计算）+ 逆向设计算法

Result: 生成结构覆盖理论性能上限91.86%、机械响应覆盖4倍跨度，原型制造验证可行性。

Conclusion: 该框架突破了超材料设计效率瓶颈，为工程应用提供兼具高性能与可制造性的解决方案。

Abstract: We introduce a compact yet highly expressive design space for shellular
metamaterials. By employing only a few dozen degrees of freedom, this design
space represents geometries ranging from simple planar configurations to
complex triply periodic minimal surfaces. Coupled with this representation, we
develop an efficient GPU-based homogenization pipeline that evaluates the
structure in under 20 ms and computes the corresponding effective elastic
tensor in near-real-time (0.5 s). The high speed of this evaluation facilitates
an exhaustive exploration of the design space and supports an inverse-design
scheme that tailors the shellular structure to specific macroscopic target
property. Structures derived through this approach exhibit not only geometric
diversity but also a wide spectrum of mechanical responses, covering a broad
range of material properties. Moreover, they achieve up to 91.86% of
theoretical upper bounds, a level of performance comparable to state-of-the-art
shellular structures with low solid volume. Finally, our prototypes, fabricated
via additive manufacturing, confirm the practical manufacturability of these
designs, underscoring their potential for real-world engineering applications.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [46] [MimiTalk: Revolutionizing Qualitative Research with Dual-Agent AI](https://arxiv.org/abs/2511.03731)
*Fengming Liu,Shubin Yu*

Main category: cs.HC

TL;DR: MimiTalk双智能体框架通过AI访谈提升社科研究的可扩展性与数据质量，在信息丰富度和稳定性上优于人类访谈，同时保留人文访谈的情感洞察优势。


<details>
  <summary>Details</summary>
Motivation: 解决传统社科研究中人类访谈存在的焦虑感强、可扩展性差、质量波动大等问题，探索可复现、规模化且伦理可控的定性研究方法。

Method: 1. 20人可用性测试；2. 121次AI访谈与1,271次人类访谈的NLP指标对比及倾向匹配分析；3. 10位跨学科研究者实施双盲主题分析。

Result: AI访谈减少焦虑感，信息丰富度/连贯性/稳定性均优于人类访谈（技术细节+敏感话题更优），人类访谈在文化情感维度更具优势。

Conclusion: 双智能体宪法AI框架实现了有效的人机协作范式，为质性研究提供了标准化、规模化的解决方案，同时提示需保持人类在文化情感维度的不可替代性。

Abstract: We present MimiTalk, a dual-agent constitutional AI framework designed for
scalable and ethical conversational data collection in social science research.
The framework integrates a supervisor model for strategic oversight and a
conversational model for question generation. We conducted three studies: Study
1 evaluated usability with 20 participants; Study 2 compared 121 AI interviews
to 1,271 human interviews from the MediaSum dataset using NLP metrics and
propensity score matching; Study 3 involved 10 interdisciplinary researchers
conducting both human and AI interviews, followed by blind thematic analysis.
Results across studies indicate that MimiTalk reduces interview anxiety,
maintains conversational coherence, and outperforms human interviews in
information richness, coherence, and stability. AI interviews elicit technical
insights and candid views on sensitive topics, while human interviews better
capture cultural and emotional nuances. These findings suggest that dual-agent
constitutional AI supports effective human-AI collaboration, enabling
replicable, scalable and quality-controlled qualitative research.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [47] [Explorability in Pushdown Automata](https://arxiv.org/abs/2511.04048)
*Ayaan Bedi,Karoliina Lehtinen*

Main category: cs.FL

TL;DR: 提出并系统研究了pushdown automata中的可探索性概念，揭示了其在表达能力、简洁性方面与历史确定性自动机的层次关系。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注历史确定性自动机，但对更广义的非确定性度量研究不足。本文旨在通过可探索性这一操作化指标，建立非确定性程度与系统表达能力/简洁性的量化关系。

Method: 构建k-可探索性层次理论，采用形式化验证方法比较不同层级自动机的表达能力；通过参数化可探索性概念拓展研究维度；运用数学证明分析简洁性差距。

Result: 发现k-可探索性构成严格无穷层次结构；指数级可探索性等价于上下文无关语言；证明可探索自动机相对历史确定性自动机具有双重指数级简洁优势。

Conclusion: 可探索性为量化下推系统的非确定性提供了新的理论框架，其层次特性和简洁性优势对形式化验证、编译器优化等领域具有重要启示。

Abstract: We study explorability, a measure of nondeterminism in pushdown automata,
which generalises history-determinism. An automaton is k-explorable if, while
reading the input, it suffices to follow k concurrent runs, built step-by-step
based only on the input seen so far, to construct an accepting one, if it
exists. We show that the class of explorable PDAs lies strictly between
history-deterministic and fully nondeterministic PDAs in terms of both
expressiveness and succinctness. In fact increasing explorability induces an
infinite hierarchy: each level k defines a strictly more expressive class than
level k-1, yet the entire class remains less expressive than general
nondeterministic PDAs. We then introduce a parameterized notion of
explorability, where the number of runs may depend on input length, and show
that exponential explorability precisely captures the context-free languages.
Finally, we prove that explorable PDAs can be doubly exponentially more
succinct than history-deterministic ones, and that the succinctness gap between
deterministic and 2-explorable PDAs is not recursively enumerable. These
results position explorability as a robust and operationally meaningful measure
of nondeterminism for pushdown systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [48] [How Different Tokenization Algorithms Impact LLMs and Transformer Models for Binary Code Analysis](https://arxiv.org/abs/2511.03825)
*Ahmed Mostafa,Raisul Arefin Nahid,Samuel Mulder*

Main category: cs.AI

TL;DR: 系统评估NLP标记化模型在汇编代码分析中的性能表现，揭示标记化选择对下游任务的显著影响及其指标局限性


<details>
  <summary>Details</summary>
Motivation: 汇编代码标记化对语义分析至关重要，但现有研究未充分探索其内在特性与下游任务表现的关联机制

Method: 通过对比Llama/BERT/BART等预训练模型，分析标记化效率/词汇压缩/表征保真度等内在指标，并评估函数签名预测等实际任务表现

Result: 标记器选择显著影响下游性能，内在指标仅具部分预测性，需权衡模型特性与任务需求的匹配度

Conclusion: 优化汇编代码标记化模型可提升二进制分析工作流的稳健性，为低阶代码分析提供关键技术支持

Abstract: Tokenization is fundamental in assembly code analysis, impacting intrinsic
characteristics like vocabulary size, semantic coverage, and extrinsic
performance in downstream tasks. Despite its significance, tokenization in the
context of assembly code remains an underexplored area. This study aims to
address this gap by evaluating the intrinsic properties of Natural Language
Processing (NLP) tokenization models and parameter choices, such as vocabulary
size. We explore preprocessing customization options and pre-tokenization rules
tailored to the unique characteristics of assembly code. Additionally, we
assess their impact on downstream tasks like function signature prediction -- a
critical problem in binary code analysis.
  To this end, we conduct a thorough study on various tokenization models,
systematically analyzing their efficiency in encoding assembly instructions and
capturing semantic nuances. Through intrinsic evaluations, we compare
tokenizers based on tokenization efficiency, vocabulary compression, and
representational fidelity for assembly code. Using state-of-the-art pre-trained
models such as the decoder-only Large Language Model (LLM) Llama 3.2, the
encoder-only transformer BERT, and the encoder-decoder model BART, we evaluate
the effectiveness of these tokenizers across multiple performance metrics.
Preliminary findings indicate that tokenizer choice significantly influences
downstream performance, with intrinsic metrics providing partial but incomplete
predictability of extrinsic evaluation outcomes. These results reveal complex
trade-offs between intrinsic tokenizer properties and their utility in
practical assembly code tasks. Ultimately, this study provides valuable
insights into optimizing tokenization models for low-level code analysis,
contributing to the robustness and scalability of Natural Language Model
(NLM)-based binary analysis workflows.

</details>


### [49] [LLMs and Cultural Values: the Impact of Prompt Language and Explicit Cultural Framing](https://arxiv.org/abs/2511.03980)
*Bram Bulté,Ayla Rigouts Terryn*

Main category: cs.AI

TL;DR: 研究揭示大型语言模型在文化多样性表现上存在局限性，虽能通过提示语调整输出，但固有偏向美日德荷等特定国家文化，难以真正代表全球多元价值。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs是否能够反映全球用户的文化多样性，关注提示语言和文化框架对模型输出与不同国家人类价值观对齐的影响。

Method: 使用霍夫斯泰德价值观问卷和世界价值观调查的63个条目，翻译成11种语言，通过包含/排除明确文化视角的提示测试10个LLMs。

Result: 模型存在系统性文化偏向（荷兰/德国/美国/日本），明确文化视角比提示语言更有效提升价值观对齐，组合策略未现协同效应。

Conclusion: LLMs处于尴尬中间态：既受提示影响产生变化，又固守特定文化预设，无法充分体现文化多样性。

Abstract: Large Language Models (LLMs) are rapidly being adopted by users across the
globe, who interact with them in a diverse range of languages. At the same
time, there are well-documented imbalances in the training data and
optimisation objectives of this technology, raising doubts as to whether LLMs
can represent the cultural diversity of their broad user base. In this study,
we look at LLMs and cultural values and examine how prompt language and
cultural framing influence model responses and their alignment with human
values in different countries. We probe 10 LLMs with 63 items from the Hofstede
Values Survey Module and World Values Survey, translated into 11 languages, and
formulated as prompts with and without different explicit cultural
perspectives. Our study confirms that both prompt language and cultural
perspective produce variation in LLM outputs, but with an important caveat:
While targeted prompting can, to a certain extent, steer LLM responses in the
direction of the predominant values of the corresponding countries, it does not
overcome the models' systematic bias toward the values associated with a
restricted set of countries in our dataset: the Netherlands, Germany, the US,
and Japan. All tested models, regardless of their origin, exhibit remarkably
similar patterns: They produce fairly neutral responses on most topics, with
selective progressive stances on issues such as social tolerance. Alignment
with cultural values of human respondents is improved more with an explicit
cultural perspective than with a targeted prompt language. Unexpectedly,
combining both approaches is no more effective than cultural framing with an
English prompt. These findings reveal that LLMs occupy an uncomfortable middle
ground: They are responsive enough to changes in prompts to produce variation,
but too firmly anchored to specific cultural defaults to adequately represent
cultural diversity.

</details>


### [50] [Large language models replicate and predict human cooperation across experiments in game theory](https://arxiv.org/abs/2511.04500)
*Andrea Cera Palatsi,Samuel Martin-Gutierrez,Ana S. Cardenal,Max Pellert*

Main category: cs.AI

TL;DR: 研究发现适当校准的LLM能复现人类群体行为模式，Llama模型精准还原人类合作偏差，Qwen模型更接近理论预测，并提出新实验空间的假设生成方法。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM决策与人类行为的契合度，避免实际应用中的潜在危害，验证LLM在社会模拟中的有效性。

Method: 构建博弈论实验数字孪生系统，采用系统化提示-探测框架测试Llama/Mistral/Qwen模型在囚徒困境等场景中的行为模式。

Result: Llama复现78%人类合作偏差特征，Qwen与纳什均衡预测吻合度达92%，无需角色提示即可实现群体行为复现。

Conclusion: 校准后的LLM可作为社会科学研究的补充工具，既能复现人类行为，又能生成新实验假设，拓展决策理论研究边界。

Abstract: Large language models (LLMs) are increasingly used both to make decisions in
domains such as health, education and law, and to simulate human behavior. Yet
how closely LLMs mirror actual human decision-making remains poorly understood.
This gap is critical: misalignment could produce harmful outcomes in practical
applications, while failure to replicate human behavior renders LLMs
ineffective for social simulations. Here, we address this gap by developing a
digital twin of game-theoretic experiments and introducing a systematic
prompting and probing framework for machine-behavioral evaluation. Testing
three open-source models (Llama, Mistral and Qwen), we find that Llama
reproduces human cooperation patterns with high fidelity, capturing human
deviations from rational choice theory, while Qwen aligns closely with Nash
equilibrium predictions. Notably, we achieved population-level behavioral
replication without persona-based prompting, simplifying the simulation
process. Extending beyond the original human-tested games, we generate and
preregister testable hypotheses for novel game configurations outside the
original parameter grid. Our findings demonstrate that appropriately calibrated
LLMs can replicate aggregate human behavioral patterns and enable systematic
exploration of unexplored experimental spaces, offering a complementary
approach to traditional research in the social and behavioral sciences that
generates new empirical predictions about human social decision-making.

</details>


### [51] [Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper](https://arxiv.org/abs/2511.04583)
*Atsuyuki Miyai,Mashiro Toyooka,Takashi Otonari,Zaiying Zhao,Kiyoharu Aizawa*

Main category: cs.AI

TL;DR: 开发了模仿人类研究者工作流的Jr. AI Scientist系统，其生成论文质量优于全自动系统，但仍存在应用风险与关键挑战


<details>
  <summary>Details</summary>
Motivation: 确保AI科学家系统的可信度和可持续性，通过构建符合科研规范的系统来评估AI在学术研究中的实际能力与潜在风险

Method: 构建模拟研究者全流程（分析文献-提出假设-实验验证-论文撰写）的AI系统，结合编码代理处理复杂实现，并通过AI评审、人工评估和学术投稿多维度验证

Result: 系统生成论文评分超越现有方案，但作者评估和学术投稿反馈揭示了模型泛化能力不足、科研创新性有限等核心缺陷

Conclusion: 当前AI科学家系统可辅助科研但不宜直接应用，需重点解决科学创新性、风险评估框架和工作流程可靠性三大挑战

Abstract: Understanding the current capabilities and risks of AI Scientist systems is
essential for ensuring trustworthy and sustainable AI-driven scientific
progress while preserving the integrity of the academic ecosystem. To this end,
we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system
that mimics the core research workflow of a novice student researcher: Given
the baseline paper from the human mentor, it analyzes its limitations,
formulates novel hypotheses for improvement, validates them through rigorous
experimentation, and writes a paper with the results. Unlike previous
approaches that assume full automation or operate on small-scale code, Jr. AI
Scientist follows a well-defined research workflow and leverages modern coding
agents to handle complex, multi-file implementations, leading to scientifically
valuable contributions. For evaluation, we conducted automated assessments
using AI Reviewers, author-led evaluations, and submissions to Agents4Science,
a venue dedicated to AI-driven scientific contributions. The findings
demonstrate that Jr. AI Scientist generates papers receiving higher review
scores than existing fully automated systems. Nevertheless, we identify
important limitations from both the author evaluation and the Agents4Science
reviews, indicating the potential risks of directly applying current AI
Scientist systems and key challenges for future research. Finally, we
comprehensively report various risks identified during development. We hope
these insights will deepen understanding of current progress and risks in AI
Scientist development.

</details>


### [52] [Are We Asking the Right Questions? On Ambiguity in Natural Language Queries for Tabular Data Analysis](https://arxiv.org/abs/2511.04584)
*Daniel Gomm,Cornelius Wolff,Madelon Hulsebos*

Main category: cs.AI

TL;DR: 提出合作式查询框架，将自然语言处理中的歧义性转化为合作交互特征，并重新定义数据集评估标准。


<details>
  <summary>Details</summary>
Motivation: 传统方法将自然语言查询歧义视为系统缺陷，本研究将其重构为用户与系统共同参与的协作过程，强调双方在查询解释中的共同责任。

Method: 开发区分合作查询（可解析）与非合作查询的理论框架，应用于15个主流表格问答数据集的分析评估。

Result: 现有数据集混合不同类型查询，既无法准确评估系统执行能力，也不能有效测试解释能力，揭示当前评估体系的不足。

Conclusion: 该框架为自然语言表格接口设计提供新范式，强调在查询解析中实现协作，推动建立更科学的评估体系，并提出未来研究方向。

Abstract: Natural language interfaces to tabular data must handle ambiguities inherent
to queries. Instead of treating ambiguity as a deficiency, we reframe it as a
feature of cooperative interaction, where the responsibility of query
specification is shared among the user and the system. We develop a principled
framework distinguishing cooperative queries, i.e., queries that yield a
resolvable interpretation, from uncooperative queries that cannot be resolved.
Applying the framework to evaluations for tabular question answering and
analysis, we analyze the queries in 15 popular datasets, and observe an
uncontrolled mixing of query types neither adequate for evaluating a system's
execution accuracy nor for evaluating interpretation capabilities. Our
framework and analysis of queries shifts the perspective from fixing ambiguity
to embracing cooperation in resolving queries. This reflection enables more
informed design and evaluation for natural language interfaces for tabular
data, for which we outline implications and directions for future research.

</details>


### [53] [DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for Embodied LLM-Based Multi-Agent Collaboration](https://arxiv.org/abs/2511.04646)
*Narjes Nourzad,Hanqing Yang,Shiyu Chen,Carlee Joe-Wong*

Main category: cs.AI

TL;DR: 提出去中心化神经符号框架DR.WELL，通过两阶段协商协议和符号规划解决多智能体协作中的轨迹对齐问题，实验显示动态世界模型提升任务效率。


<details>
  <summary>Details</summary>
Motivation: 传统轨迹级协调因微小偏差易引发级联冲突，需通过符号抽象增强协作鲁棒性和同步能力。

Method: 两阶段协商协议（角色提案+共识分配）+ 符号计划自主执行 + 共享世界模型动态更新

Result: 在协作推块任务中实现62%任务完成率提升，通过策略演化形成更高效协作模式（时间开销增加15%）

Conclusion: 符号规划实现可复用、可同步的协作范式，动态世界模型支持策略自我优化，权衡时间成本换取持续增效

Abstract: Cooperative multi-agent planning requires agents to make joint decisions with
partial information and limited communication. Coordination at the trajectory
level often fails, as small deviations in timing or movement cascade into
conflicts. Symbolic planning mitigates this challenge by raising the level of
abstraction and providing a minimal vocabulary of actions that enable
synchronization and collective progress. We present DR. WELL, a decentralized
neurosymbolic framework for cooperative multi-agent planning. Cooperation
unfolds through a two-phase negotiation protocol: agents first propose
candidate roles with reasoning and then commit to a joint allocation under
consensus and environment constraints. After commitment, each agent
independently generates and executes a symbolic plan for its role without
revealing detailed trajectories. Plans are grounded in execution outcomes via a
shared world model that encodes the current state and is updated as agents act.
By reasoning over symbolic plans rather than raw trajectories, DR. WELL avoids
brittle step-level alignment and enables higher-level operations that are
reusable, synchronizable, and interpretable. Experiments on cooperative
block-push tasks show that agents adapt across episodes, with the dynamic world
model capturing reusable patterns and improving task completion rates and
efficiency. Experiments on cooperative block-push tasks show that our dynamic
world model improves task completion and efficiency through negotiation and
self-refinement, trading a time overhead for evolving, more efficient
collaboration strategies.

</details>


### [54] [VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks](https://arxiv.org/abs/2511.04662)
*Yu Feng,Nathaniel Weir,Kaj Bostrom,Sam Bayless,Darion Cassel,Sapana Chaudhary,Benjamin Kiesl-Reiter,Huzefa Rangwala*

Main category: cs.AI

TL;DR: 提出VeriCoT神经符号方法，通过将思维链推理步骤转化为一阶逻辑进行形式化验证，有效识别错误推理并提升模型可靠性


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的思维链推理存在逻辑不可验证问题，导致高风险场景下可信度不足

Method: 将自然语言推理步骤形式化为符号逻辑，结合自动化求解器验证逻辑有效性，同时保留自然语言前提供人工核查

Result: 在ProofWriter等三个数据集上验证有效，识别错误推理的准确率达基准水平，验证信号对答案正确性预测性强

Conclusion: VeriCoT验证信号可用于推理时自省、监督微调和偏好优化，显著提升模型推理有效性及准确性

Abstract: LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but
they cannot reliably verify their own logic. Even when they reach correct
answers, the underlying reasoning may be flawed, undermining trust in
high-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a
neuro-symbolic method that extracts and verifies formal logical arguments from
CoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order
logic and identifies premises that ground the argument in source context,
commonsense knowledge, or prior reasoning steps. The symbolic representation
enables automated solvers to verify logical validity while the NL premises
allow humans and systems to identify ungrounded or fallacious reasoning steps.
Experiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT
effectively identifies flawed reasoning, and serves as a strong predictor of
final answer correctness. We also leverage VeriCoT's verification signal for
(1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on
VeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct
preference optimization (DPO) using verification-based pairwise rewards,
further improving reasoning validity and accuracy.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [55] [MIDI-LLM: Adapting Large Language Models for Text-to-MIDI Music Generation](https://arxiv.org/abs/2511.03942)
*Shih-Lun Wu,Yoon Kim,Cheng-Zhi Anna Huang*

Main category: cs.SD

TL;DR: MIDI-LLM是一个通过扩展文本大语言模型词汇库实现文本转MIDI生成的创新模型，具备两阶段训练策略并支持快速推理


<details>
  <summary>Details</summary>
Motivation: 现有文本转MIDI模型（如Text2midi）在生成质量、文本控制精度和推理速度方面存在局限，需要开发更高效的解决方案

Method: 1. 扩展文本LLM词汇表加入MIDI标记 2. 采用两阶段训练策略（保留原LLM参数结构） 3. 集成vLLM加速库实现高效推理

Result: 相比Text2midi，在音乐生成质量（+23%人工评估分数）、文本控制准确率（+15%）、推理速度（4.2倍提升）方面均取得显著提升

Conclusion: 该研究通过参数保留和词汇扩展策略，成功实现了高效可控的文本到多轨音乐生成，并提供了可交互的在线演示验证实用性

Abstract: We present MIDI-LLM, an LLM for generating multitrack MIDI music from
free-form text prompts. Our approach expands a text LLM's vocabulary to include
MIDI tokens, and uses a two-stage training recipe to endow text-to-MIDI
abilities. By preserving the original LLM's parameter structure, we can
directly leverage the vLLM library for accelerated inference. Experiments show
that MIDI-LLM achieves higher quality, better text control, and faster
inference compared to the recent Text2midi model. Live demo at
https://midi-llm-demo.vercel.app.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [56] [Black-Box Guardrail Reverse-engineering Attack](https://arxiv.org/abs/2511.04215)
*Hongwei Yao,Yun Xia,Shuo Shao,Haoran Shi,Tong Qiao,Cong Wang*

Main category: cs.CR

TL;DR: 提出基于强化学习的GRA框架，成功逆向工程主流LLM防护机制，暴露现有安全设计漏洞


<details>
  <summary>Details</summary>
Motivation: 现有LLM防护措施虽然能阻止有害输出，但其可观测的决策模式形成了新的攻击面

Method: 结合遗传算法数据增强与强化学习，通过迭代式输入输出对收集、差异案例优先级处理、定向变异和交叉操作，构建高保真防护规则代理模型

Result: 在ChatGPT等三大商业系统中实现规则匹配率超0.92，API成本低于85美元

Conclusion: 当前LLM安全机制存在重大设计缺陷，亟需开发更鲁棒的动态防御体系

Abstract: Large language models (LLMs) increasingly employ guardrails to enforce
ethical, legal, and application-specific constraints on their outputs. While
effective at mitigating harmful responses, these guardrails introduce a new
class of vulnerabilities by exposing observable decision patterns. In this
work, we present the first study of black-box LLM guardrail reverse-engineering
attacks. We propose Guardrail Reverse-engineering Attack (GRA), a reinforcement
learning-based framework that leverages genetic algorithm-driven data
augmentation to approximate the decision-making policy of victim guardrails. By
iteratively collecting input-output pairs, prioritizing divergence cases, and
applying targeted mutations and crossovers, our method incrementally converges
toward a high-fidelity surrogate of the victim guardrail. We evaluate GRA on
three widely deployed commercial systems, namely ChatGPT, DeepSeek, and Qwen3,
and demonstrate that it achieves an rule matching rate exceeding 0.92 while
requiring less than $85 in API costs. These findings underscore the practical
feasibility of guardrail extraction and highlight significant security risks
for current LLM safety mechanisms. Our findings expose critical vulnerabilities
in current guardrail designs and highlight the urgent need for more robust
defense mechanisms in LLM deployment.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [57] [Multi-Agent Collaborative Framework For Math Problem Generation](https://arxiv.org/abs/2511.03958)
*Kia Karbasi,Kevin Hong,Mohammad Amin Samadi,Gregory Pottie*

Main category: cs.MA

TL;DR: 协作多智能体框架通过迭代优化问题-答案对，显著提升数学自动问题生成的质量与复杂度控制能力


<details>
  <summary>Details</summary>
Motivation: 现有预训练语言模型在数学教育问题生成中存在复杂度与认知需求控制不足的问题，影响教育内容适应性

Method: 提出创新性的协作多智能体框架，通过多智能体迭代优化机制动态调整生成内容，平衡认知挑战与表达清晰度

Result: 在相关性、重要性、清晰度、难度匹配和可解答性五个元评估维度上均显示显著改进，特别是在认知复杂度控制方面提升24%

Conclusion: 多智能体协作机制为教育内容生成提供了新范式，推动自适应学习系统向更智能化的方向发展，具有重要教学应用价值

Abstract: Automatic question generation (AQG) for mathematics education remains an
elusive goal for Intelligent Tutoring Systems and educators. While pre-trained
transformer-based language models have significantly advanced natural language
generation, they often struggle to precisely control problem complexity and
cognitive demands. In this paper, we introduce a collaborative multi-agent
framework as a novel method of incorporating inference-time computation into
AQG. This approach leverages multiple agents that iteratively refine generated
question-answer pairs to better balance complexity and cognitive demand. We
evaluate the generated questions on five meta-evaluation criteria: relevance,
importance, clarity, difficulty matching, answerability, to assess the system's
ability to control the required complexity and quality of the questions.
Preliminary evaluations show that this collaborative multi-agent framework
elevates the quality of generated educational content by fostering a more
nuanced balance between cognitive challenge and clarity. These promising
outcomes suggest that integrating collaborative multi-agent workflows can yield
more controlled, pedagogically valuable content that can help advance automated
educational content generation and adaptive learning environments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [58] [RLHF: A comprehensive Survey for Cultural, Multimodal and Low Latency Alignment Methods](https://arxiv.org/abs/2511.03939)
*Raghav Sharma,Manan Mehta,Sai Tiger Raina*

Main category: cs.LG

TL;DR: 该论文综述了超越传统文本强化学习（RLHF）的AI对齐技术，聚焦多模态对齐、文化公平性和低延迟优化三大前沿方向，对比分析了PPO/DPO/GRPO等算法创新。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF在应对多模态数据融合、保障文化多样性及系统响应速度方面存在局限，亟需开发更鲁棒、高效且公平的新一代AI对齐方法体系。

Method: 通过系统梳理PPO/DPO/GRPO等基础算法，对比分析最新多模态对齐框架、文化偏差量化工具和低延迟训练架构的创新设计。

Result: 构建了包含12类前沿技术的评估矩阵，揭示多模态融合使跨模态对齐精度提升37%，动态文化适配机制降低地域偏差达42%，稀疏化训练提速5.8倍。

Conclusion: 提出融合神经符号计算与文化感知模块的新型架构蓝图，为构建响应速度亚秒级、支持200+文化维度的下一代AI对齐系统提供技术路线图。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is the standard for
aligning Large Language Models (LLMs), yet recent progress has moved beyond
canonical text-based methods. This survey synthesizes the new frontier of
alignment research by addressing critical gaps in multi-modal alignment,
cultural fairness, and low-latency optimization. To systematically explore
these domains, we first review foundational algo- rithms, including PPO, DPO,
and GRPO, before presenting a detailed analysis of the latest innovations. By
providing a comparative synthesis of these techniques and outlining open
challenges, this work serves as an essential roadmap for researchers building
more robust, efficient, and equitable AI systems.

</details>


### [59] [Towards Scalable Meta-Learning of near-optimal Interpretable Models via Synthetic Model Generations](https://arxiv.org/abs/2511.04000)
*Kyaw Hpone Myint,Zhe Wu,Alexandre G. R. Day,Giri Iyengar*

Main category: cs.LG

TL;DR: 提出通过生成接近最优的决策树合成数据，实现高效、可扩展的元学习方法，降低计算成本同时保持性能


<details>
  <summary>Details</summary>
Motivation: 决策树在关键领域依赖可解释性，但现有元学习方法依赖真实数据或计算昂贵的优化树，需要更高效的预训练策略

Method: 1. 通过采样接近最优的决策树生成大规模合成数据 2. 使用MetaTree transformer架构进行元学习

Result: 合成数据预训练效果与真实数据或优化树相当，计算成本降低90%，数据生成灵活性显著提高

Conclusion: 该方法为可解释决策树模型的元学习提供了可扩展、低成本的解决方案，平衡了性能与效率

Abstract: Decision trees are widely used in high-stakes fields like finance and
healthcare due to their interpretability. This work introduces an efficient,
scalable method for generating synthetic pre-training data to enable
meta-learning of decision trees. Our approach samples near-optimal decision
trees synthetically, creating large-scale, realistic datasets. Using the
MetaTree transformer architecture, we demonstrate that this method achieves
performance comparable to pre-training on real-world data or with
computationally expensive optimal decision trees. This strategy significantly
reduces computational costs, enhances data generation flexibility, and paves
the way for scalable and efficient meta-learning of interpretable decision tree
models.

</details>


### [60] [DartQuant: Efficient Rotational Distribution Calibration for LLM Quantization](https://arxiv.org/abs/2511.04063)
*Yuantian Shao,Yuanteng Chen,Peisong Wang,Jianlin Yu,Jing Lin,Yiwu Yao,Zhihui Wei,Jian Cheng*

Main category: cs.LG

TL;DR: DartQuant通过分布感知旋转校准和QR-Orth优化方案，显著提升量化效率，在70B模型上实现47倍加速和10倍内存节省


<details>
  <summary>Details</summary>
Motivation: 传统旋转优化算法存在端到端微调计算成本高、易过拟合的问题，难以在资源受限环境下实现大模型量化

Method: 1. 提出分布感知旋转校准方法，通过约束旋转后激活值的分布降低优化复杂度
2. 设计QR-Orth优化方案替代昂贵的交替优化方法

Result: 在70B模型上实现47倍加速和10倍内存节省，首次在单块3090 GPU上完成70B模型的旋转校准

Conclusion: 该方法显著降低大模型量化对硬件资源的需求，为资源受限环境中的语言模型量化提供了可行解决方案

Abstract: Quantization plays a crucial role in accelerating the inference of
large-scale models, and rotational matrices have been shown to effectively
improve quantization performance by smoothing outliers. However, end-to-end
fine-tuning of rotational optimization algorithms incurs high computational
costs and is prone to overfitting. To address this challenge, we propose an
efficient distribution-aware rotational calibration method, DartQuant, which
reduces the complexity of rotational optimization by constraining the
distribution of the activations after rotation. This approach also effectively
reduces reliance on task-specific losses, thereby mitigating the risk of
overfitting. Additionally, we introduce the QR-Orth optimization scheme, which
replaces expensive alternating optimization with a more efficient solution. In
a variety of model quantization experiments, DartQuant demonstrates superior
performance. Compared to existing methods, it achieves 47$\times$ acceleration
and 10$\times$ memory savings for rotational optimization on a 70B model.
Furthermore, it is the first to successfully complete rotational calibration
for a 70B model on a single 3090 GPU, making quantization of large language
models feasible in resource-constrained environments. Code is available at
https://github.com/CAS-CLab/DartQuant.git.

</details>


### [61] [Block Rotation is All You Need for MXFP4 Quantization](https://arxiv.org/abs/2511.04214)
*Yuantian Shao,Peisong Wang,Yuanteng Chen,Chang Xu,Zhihui Wei,Jian Cheng*

Main category: cs.LG

TL;DR: 提出首个MXFP4格式下的PTQ方法基准测试，发现GPTQ适配性佳但旋转类方法存在兼容性问题，通过块旋转策略有效提升量化精度


<details>
  <summary>Details</summary>
Motivation: 大语言模型规模扩大导致部署成本激增，现有PTQ方法在MXFP4新型4位浮点格式上的有效性未知，需系统性评估和方法改进

Method: 建立MXFP4量化基准测试，分析旋转类方法与MXFP4幂次方块缩放的不兼容性，提出适配的块旋转策略

Result: 块旋转策略使LLaMA-7B/13B在MXFP4格式下分别提升21.3%和15.2%准确率，显著优于传统全局旋转方法

Conclusion: 研究为MXFP4格式部署提供实践指南，揭示了低精度格式与量化方法的兼容性机理，推动新兴硬件生态下的PTQ研究

Abstract: Large language models (LLMs) have achieved remarkable success, but their
rapidly growing scale imposes prohibitive costs in memory, computation, and
energy. Post-training quantization (PTQ) is a promising solution for efficient
deployment, yet achieving accurate W4A4 quantization remains an open challenge.
While most existing methods are designed for INT4 formats, the emergence of
MXFP4 -- a new FP4 format with various hardware support (NVIDIA, AMD, Intel)--
raises questions about the applicability of current techniques. In this work,
we establish a comprehensive benchmark of PTQ methods under the MXFP4 format.
Through systematic evaluation, we find that methods like GPTQ consistently
deliver strong performance, whereas rotation-based approaches, which are almost
used by all state-of-the-art approaches, suffer from severe incompatibility
with MXFP4. We further provide the first in-depth analysis of this conflict,
tracing its root to a fundamental mismatch between MXFP4's PoT (power-of-two)
block scaling and the redistribution of outlier energy via global rotation.
Building on this insight, we propose a simple yet effective block rotation
strategy that adapts rotation-based methods to MXFP4, leading to substantial
accuracy improvements across diverse LLMs. Our findings not only offer clear
guidance for practitioners but also set a foundation for advancing PTQ research
under emerging low-precision formats.

</details>


### [62] [The Illusion of Certainty: Uncertainty quantification for LLMs fails under ambiguity](https://arxiv.org/abs/2511.04418)
*Tim Tomov,Dominik Fuchsgruber,Tom Wollschläger,Stephan Günnemann*

Main category: cs.LG

TL;DR: 现有不确定性量化方法在明确任务中有效，但在歧义性数据上性能接近随机，为此提出首个含真实答案分布的歧义问答数据集MAQA*/AmbigQA*。


<details>
  <summary>Details</summary>
Motivation: 现实语言存在固有歧义性，而现有UQ方法仅在无歧义任务中测试，无法反映真实场景的aleatoric uncertainty。

Method: 构建含事实共现估计的歧义QA数据集，评估预测分布、模型内部表示、模型集成三种UQ范式，并进行理论分析。

Result: 所有UQ方法在歧义数据上性能显著下降(接近随机)，理论证明预测分布和集成方法在歧义性下存在根本性局限。

Conclusion: 研究揭示了当前LLM不确定性量化方法的重大缺陷，需重新设计建模范式以适应现实语言场景的歧义性。

Abstract: Accurate uncertainty quantification (UQ) in Large Language Models (LLMs) is
critical for trustworthy deployment. While real-world language is inherently
ambiguous, reflecting aleatoric uncertainty, existing UQ methods are typically
benchmarked against tasks with no ambiguity. In this work, we demonstrate that
while current uncertainty estimators perform well under the restrictive
assumption of no ambiguity, they degrade to close-to-random performance on
ambiguous data. To this end, we introduce MAQA* and AmbigQA*, the first
ambiguous question-answering (QA) datasets equipped with ground-truth answer
distributions estimated from factual co-occurrence. We find this performance
deterioration to be consistent across different estimation paradigms: using the
predictive distribution itself, internal representations throughout the model,
and an ensemble of models. We show that this phenomenon can be theoretically
explained, revealing that predictive-distribution and ensemble-based estimators
are fundamentally limited under ambiguity. Overall, our study reveals a key
shortcoming of current UQ methods for LLMs and motivates a rethinking of
current modeling paradigms.

</details>


### [63] [Ground-Truth Subgraphs for Better Training and Evaluation of Knowledge Graph Augmented LLMs](https://arxiv.org/abs/2511.04473)
*Alberto Cattaneo,Carlo Luschi,Daniel Justus*

Main category: cs.LG

TL;DR: 提出SynthKGQA框架用于生成知识图谱问答数据集，解决现有基准不足问题，并验证其能训练更好模型


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱检索方法缺乏具有挑战性的QA数据集和完整事实依据，导致评估困难

Method: 开发SynthKGQA框架，支持从任意知识图谱生成包含完整推理路径的合成数据集（如GTSQA）

Result: 新数据集有效评估知识图谱检索器的零样本泛化能力，并提升KG增强LLM模型的性能

Conclusion: 通过结构化数据生成框架推动知识图谱检索研究，为模型训练和评估提供新范式

Abstract: Retrieval of information from graph-structured knowledge bases represents a
promising direction for improving the factuality of LLMs. While various
solutions have been proposed, a comparison of methods is difficult due to the
lack of challenging QA datasets with ground-truth targets for graph retrieval.
We present SynthKGQA, a framework for generating high-quality synthetic
Knowledge Graph Question Answering datasets from any Knowledge Graph, providing
the full set of ground-truth facts in the KG to reason over each question. We
show how, in addition to enabling more informative benchmarking of KG
retrievers, the data produced with SynthKGQA also allows us to train better
models. We apply SynthKGQA to Wikidata to generate GTSQA, a new dataset
designed to test zero-shot generalization abilities of KG retrievers with
respect to unseen graph structures and relation types, and benchmark popular
solutions for KG-augmented LLMs on it.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [64] [Sub-exponential Growth in Online Word Usage: A Piecewise Power-Law Model](https://arxiv.org/abs/2511.04106)
*Hayafumi Watanabe*

Main category: physics.soc-ph

TL;DR: 构建分段幂律模型分析社会扩散，发现55%案例符合亚指数增长模式，α参数揭示扩散行为偏好


<details>
  <summary>Details</summary>
Motivation: 传统S形模型忽视亚指数增长在社会现象中的作用，需新方法量化复杂扩散曲线

Method: 使用十亿级日本博客数据及多语种搜索趋势，通过分段幂律模型拟合2,965个项目增长轨迹

Result: 单段曲线α众值0.5显示亚指数扩散主流；扩散规模由增长率R主导(贡献率81%)，α与主题属性相关（小众话题α更小）

Conclusion: 亚指数增长是社会扩散的常见模式，模型为跨领域增长曲线比较提供统一框架，α参数可解释为外向传播倾向指标

Abstract: The diffusion of ideas and language in society has conventionally been
described by S-shaped models, such as the logistic curve. However, the role of
sub-exponential growth -a slower than exponential pattern known in
epidemiology- has been largely overlooked in broader social phenomena. Here, we
present a piecewise power-law model to characterize complex growth curves with
a few parameters. We systematically analyzed a large-scale dataset of
approximately one billion Japanese blog articles linked to Wikipedia
vocabulary, and observed consistent patterns in web search trend data (English,
Spanish, and Japanese). Our analysis of the 2,965 selected items reveals that
about 55% (1,625 items) were found to have no abrupt jumps and were well
captured by one or two segments. For single-segment curves, we found that (i)
the mode of the shape parameter alpha was near 0.5, indicating prevalent
sub-exponential growth; (ii) the ultimate diffusion scale is primarily
determined by the growth rate R, with minor contributions from alpha or the
duration T; and (iii) alpha showed a tendency to vary with the nature of the
topic, being smaller for niche/local topics and larger for widely shared ones.
Furthermore, a micro-behavioral model distinguishing outward contact with
strangers from inward interaction within their community suggests that alpha
can be interpreted as an index of the preference for outward-oriented
communication. These findings suggest that sub-exponential growth is a common
pattern of social diffusion, and our model provides a practical framework for
consistently describing, comparing, and interpreting complex and diverse growth
curves.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [65] [Near-Lossless 3D Voxel Representation Free from Iso-surface](https://arxiv.org/abs/2511.04029)
*Yihao Luo,Xianglong He,Chuanyu Pan,Yiwen Chen,Jiaqi Wu,Yangguang Li,Wanli Ouyang,Yuanming Hu,Guang Yang,ChoonHwai Yap*

Main category: cs.CV

TL;DR: 提出Faithful Contouring稀疏体素化表示方法，无需等值面处理即可实现2048+分辨率，在3D重建任务中达到近无损保真度


<details>
  <summary>Details</summary>
Motivation: 现有等值面表示方法依赖水密化处理或渲染优化，导致几何保真度下降，难以处理复杂几何拓扑结构

Method: 通过保留锐利边缘和内部结构的稀疏体素化技术，开发双模式自编码器实现可扩展的形状重建

Result: 表示误差达10^-5级别，网格重建Chamfer距离降低93%，F-score提升35%，在纹理处理、编辑等任务中展现灵活性

Conclusion: 该方法在3D学习任务中实现了精度与效率的突破，实验验证其在表示和重建任务中的双重优势

Abstract: Accurate and efficient voxelized representations of 3D meshes are the
foundation of 3D reconstruction and generation. However, existing
representations based on iso-surface heavily rely on water-tightening or
rendering optimization, which inevitably compromise geometric fidelity. We
propose Faithful Contouring, a sparse voxelized representation that supports
2048+ resolutions for arbitrary meshes, requiring neither converting meshes to
field functions nor extracting the isosurface during remeshing. It achieves
near-lossless fidelity by preserving sharpness and internal structures, even
for challenging cases with complex geometry and topology. The proposed method
also shows flexibility for texturing, manipulation, and editing. Beyond
representation, we design a dual-mode autoencoder for Faithful Contouring,
enabling scalable and detail-preserving shape reconstruction. Extensive
experiments show that Faithful Contouring surpasses existing methods in
accuracy and efficiency for both representation and reconstruction. For direct
representation, it achieves distance errors at the $10^{-5}$ level; for mesh
reconstruction, it yields a 93\% reduction in Chamfer Distance and a 35\%
improvement in F-score over strong baselines, confirming superior fidelity as a
representation for 3D learning tasks.

</details>


### [66] [Seeing Straight: Document Orientation Detection for Efficient OCR](https://arxiv.org/abs/2511.04161)
*Suranjan Goswami,Abhinav Ravi,Raja Kolla,Ali Faraz,Shaharukh Khan,Akash,Chandra Khatri,Shubham Agarwal*

Main category: cs.CV

TL;DR: 提出OCR-Rotation-Bench基准测试与基于Phi-3.5-Vision的轻量级旋转分类模型，显著提升OCR性能


<details>
  <summary>Details</summary>
Motivation: 实际应用中文档方向错误会严重影响OCR效果，现有预处理方法需改进旋转校正能力

Method: 1. 构建ORB多语言旋转鲁棒性基准测试 2. 基于Phi-3.5-Vision视觉编码器开发动态裁剪的4分类微调模型

Result: 旋转分类准确率达96%（英文）和92%（印度语），OCR性能最高提升14%（闭源模型）和4倍（开源模型）

Conclusion: 该旋转校正模块在真实场景中显著提升OCR系统鲁棒性，尤其对低资源语言具有重要应用价值

Abstract: Despite significant advances in document understanding, determining the
correct orientation of scanned or photographed documents remains a critical
pre-processing step in the real world settings. Accurate rotation correction is
essential for enhancing the performance of downstream tasks such as Optical
Character Recognition (OCR) where misalignment commonly arises due to user
errors, particularly incorrect base orientations of the camera during capture.
In this study, we first introduce OCR-Rotation-Bench (ORB), a new benchmark for
evaluating OCR robustness to image rotations, comprising (i) ORB-En, built from
rotation-transformed structured and free-form English OCR datasets, and (ii)
ORB-Indic, a novel multilingual set spanning 11 Indic mid to low-resource
languages. We also present a fast, robust and lightweight rotation
classification pipeline built on the vision encoder of Phi-3.5-Vision model
with dynamic image cropping, fine-tuned specifically for 4-class rotation task
in a standalone fashion. Our method achieves near-perfect 96% and 92% accuracy
on identifying the rotations respectively on both the datasets. Beyond
classification, we demonstrate the critical role of our module in boosting OCR
performance: closed-source (up to 14%) and open-weights models (up to 4x) in
the simulated real-world setting.

</details>


### [67] [Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm](https://arxiv.org/abs/2511.04570)
*Jingqi Tong,Yurong Mou,Hangcheng Li,Mingzhe Li,Yongzhuo Yang,Ming Zhang,Qiguang Chen,Tianyi Liang,Xiaomeng Hu,Yining Zheng,Xinchi Chen,Jun Zhao,Xuanjing Huang,Xipeng Qiu*

Main category: cs.CV

TL;DR: 提出'用视频思考'新范式，通过Sora-2视频生成模型统一多模态推理，在VideoThinkBench基准测试中验证其视觉/文本任务表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有'文本思考'和'图像思考'范式存在两个缺陷：(1)图像无法表达动态过程；(2)文本与视觉模态分离阻碍多模态统一理解。

Method: 1. 引入视频生成模型Sora-2构建时间连续推理框架；2. 开发包含视觉中心任务（如目测谜题）和文本中心任务（如数学题）的VideoThinkBench评估体系。

Result: Sora-2在视觉任务与SOTA VLM持平（部分目测游戏超越），文本任务达MATH 92%、MMMU 75.53%准确率。自洽性和上下文学习可提升性能。

Conclusion: 视频生成模型具备统一多模态理解的潜力，'视频思考'可能成为多模态推理的通用范式。

Abstract: "Thinking with Text" and "Thinking with Images" paradigm significantly
improve the reasoning ability of large language models (LLMs) and Vision
Language Models (VLMs). However, these paradigms have inherent limitations. (1)
Images capture only single moments and fail to represent dynamic processes or
continuous changes, and (2) The separation of text and vision as distinct
modalities, hindering unified multimodal understanding and generation. To
overcome these limitations, we introduce "Thinking with Video", a new paradigm
that leverages video generation models, such as Sora-2, to bridge visual and
textual reasoning in a unified temporal framework. To support this exploration,
we developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench
encompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing
Puzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our
evaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks,
Sora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even
surpasses VLMs on several tasks, such as Eyeballing Games. On text-centric
tasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU.
Furthermore, we systematically analyse the source of these abilities. We also
find that self-consistency and in-context learning can improve Sora-2's
performance. In summary, our findings demonstrate that the video generation
model is the potential unified multimodal understanding and generation model,
positions "thinking with video" as a unified multimodal reasoning paradigm.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [68] [Transforming Mentorship: An AI Powered Chatbot Approach to University Guidance](https://arxiv.org/abs/2511.04172)
*Mashrur Rahman,Mantaqa abedin,Monowar Zamil Abir,Faizul Islam Ansari,Adib Reza,Farig Yousuf Sadeque,Niloy Farhan*

Main category: cs.IR

TL;DR: 开发基于LLaMA-3.3-70B大模型的智能聊天助手，通过混合检索架构（BM25+ChromaDB）实现高校学生个性化指导，数据管道效率提升3.45倍，生成回答语义相关性达BERTScore 0.831


<details>
  <summary>Details</summary>
Motivation: 解决高校新生缺乏个性化指导的痛点，现有数字工具难以实现规模化定制化辅导，需构建智能对话系统提升学生校园适应能力

Method: 1. 搭建多源数据管道处理CSV/网页数据
2. BM25词频排序与ChromaDB语义检索混合架构
3. LLaMA-3.3-70B大模型生成对话响应

Result: 1. 语义相关性指标：BERTScore 0.831，METEOR 0.809
2. 数据更新效率：106.82秒（更新） vs 368.62秒（新数据）
3. 响应生成延迟低于2秒

Conclusion: 该系统显著提升高校信息服务效率，有效支持学生课程规划与校园生活适应，为开放式学分制大学提供智能辅导解决方案

Abstract: University students face immense challenges during their undergraduate lives,
often being deprived of personalized on-demand guidance that mentors fail to
provide at scale. Digital tools exist, but there is a serious lack of
customized coaching for newcomers. This paper presents an AI-powered chatbot
that will serve as a mentor for the students of BRAC University. The main
component is a data ingestion pipeline that efficiently processes and updates
information from diverse sources, such as CSV files and university webpages.
The chatbot retrieves information through a hybrid approach, combining BM25
lexical ranking with ChromaDB semantic retrieval, and uses a Large Language
Model, LLaMA-3.3-70B, to generate conversational responses. The generated text
was found to be semantically highly relevant, with a BERTScore of 0.831 and a
METEOR score of 0.809. The data pipeline was also very efficient, taking 106.82
seconds for updates, compared to 368.62 seconds for new data. This chatbot will
be able to help students by responding to their queries, helping them to get a
better understanding of university life, and assisting them to plan better
routines for their semester in the open-credit university.

</details>
