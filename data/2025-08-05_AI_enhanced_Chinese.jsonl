{"id": "2508.00950", "pdf": "https://arxiv.org/pdf/2508.00950", "abs": "https://arxiv.org/abs/2508.00950", "authors": ["Ying Zhang", "Niklas Groene", "Karsten Klein", "Giuseppe Liotta", "Falk Schreiber"], "title": "Investigating Crossing Perception in 3D Graph Visualisation", "categories": ["cs.GR"], "comment": null, "summary": "Human perception of graph drawings is influenced by a variety of impact\nfactors for which quality measures are used as a proxy indicator. The\ninvestigation of those impact factors and their effects is important to\nevaluate and improve quality measures and drawing algorithms. The number of\nedge crossings in a 2D graph drawing has long been a main quality measure for\ndrawing evaluation. The use of stereoscopic 3D graph visualisations has gained\nattraction over the last years, and results from several studies indicate that\nthey can improve analysis efficiency for a range of analysis scenarios. While\nedge crossings can also occur in 3D, there are edge configurations in space\nthat are not crossings but might be perceived as such from a specific\nviewpoint. Such configurations create crossings when projected on the\ncorresponding 2D image plane and could impact readability similar to 2D\ncrossings. In 3D drawings, the additional depth aspect and the subsequent\nimpact factors of edge distance and relative edge direction in space might\nfurther influence the importance of those configurations for readability. We\ninvestigate the impact of such factors in an empirical study and report on\nfindings of difference between major factor categories.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u63a2\u8ba83D\u56fe\u7ed8\u5236\u4e2d\u8fb9\u4ea4\u53c9\u8bef\u5224\u73b0\u8c61\u53ca\u5176\u5f71\u54cd\u56e0\u7d20\uff08\u5982\u6df1\u5ea6\u3001\u8fb9\u8ddd\u3001\u65b9\u5411\uff09\u5bf9\u53ef\u8bfb\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u4e0e2D\u4ea4\u53c9\u8fdb\u884c\u5bf9\u6bd4\u3002", "motivation": "\u63a2\u7a763D\u56fe\u53ef\u89c6\u5316\u4e2d\u975e\u771f\u5b9e\u8fb9\u4ea4\u53c9\uff08\u6295\u5f71\u4ea7\u751f\u76842D\u4ea4\u53c9\u9519\u89c9\uff09\u5bf9\u53ef\u8bfb\u6027\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u6df1\u5ea6\u76f8\u5173\u56e0\u7d20\u5982\u4f55\u8c03\u8282\u8fd9\u79cd\u5f71\u54cd\uff0c\u4ee5\u6539\u8fdb3D\u56fe\u7684\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u3002", "method": "\u91c7\u7528\u5b9e\u8bc1\u7814\u7a76\u65b9\u6cd5\uff0c\u5bf9\u6bd4\u5206\u67903D\u56fe\u4e2d\u4e0d\u540c\u7a7a\u95f4\u56e0\u7d20\uff08\u6df1\u5ea6\u3001\u8fb9\u8ddd\u3001\u65b9\u5411\uff09\u5bf9\u8fb9\u4ea4\u53c9\u611f\u77e5\u7684\u5f71\u54cd\u5dee\u5f02\u3002", "result": "\u53d1\u73b03D\u7a7a\u95f4\u4e2d\u7684\u8fb9\u8ddd\u79bb\u3001\u76f8\u5bf9\u65b9\u5411\u4e0e\u6df1\u5ea6\u7ef4\u5ea6\u663e\u8457\u5f71\u54cd\u53ef\u8bfb\u6027\uff0c\u4e0d\u540c\u56e0\u7d20\u7c7b\u522b\u5bf9\u611f\u77e5\u7684\u8d21\u732e\u5b58\u5728\u7edf\u8ba1\u5b66\u5dee\u5f02\u3002", "conclusion": "3D\u56fe\u7684\u53ef\u8bfb\u6027\u8bc4\u4f30\u9700\u7efc\u5408\u8003\u8651\u7a7a\u95f4\u7ef4\u5ea6\u7279\u6709\u7684\u56e0\u7d20\uff08\u5982\u6df1\u5ea6\u611f\u77e5\uff09\uff0c\u4f20\u7edf2D\u4ea4\u53c9\u6307\u6807\u9700\u6269\u5c55\u4ee5\u9002\u5e94\u4e09\u7ef4\u573a\u666f\uff0c\u4e3a\u672a\u6765\u8d28\u91cf\u5ea6\u91cf\u7b97\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u4f9d\u636e\u3002"}}
{"id": "2508.01242", "pdf": "https://arxiv.org/pdf/2508.01242", "abs": "https://arxiv.org/abs/2508.01242", "authors": ["Shuangkang Fang", "I-Chao Shen", "Yufeng Wang", "Yi-Hsuan Tsai", "Yi Yang", "Shuchang Zhou", "Wenrui Ding", "Takeo Igarashi", "Ming-Hsuan Yang"], "title": "MeshLLM: Empowering Large Language Models to Progressively Understand and Generate 3D Mesh", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted by ICCV", "summary": "We present MeshLLM, a novel framework that leverages large language models\n(LLMs) to understand and generate text-serialized 3D meshes. Our approach\naddresses key limitations in existing methods, including the limited dataset\nscale when catering to LLMs' token length and the loss of 3D structural\ninformation during mesh serialization. We introduce a Primitive-Mesh\ndecomposition strategy, which divides 3D meshes into structurally meaningful\nsubunits. This enables the creation of a large-scale dataset with 1500k+\nsamples, almost 50 times larger than previous methods, which aligns better with\nthe LLM scaling law principles. Furthermore, we propose inferring face\nconnectivity from vertices and local mesh assembly training strategies,\nsignificantly enhancing the LLMs' ability to capture mesh topology and spatial\nstructures. Experiments show that MeshLLM outperforms the state-of-the-art\nLLaMA-Mesh in both mesh generation quality and shape understanding,\nhighlighting its great potential in processing text-serialized 3D meshes.", "AI": {"tldr": "MeshLLM\u6846\u67b6\u901a\u8fc7Primitive-Mesh\u5206\u89e3\u7b56\u7565\u4e0e\u5c40\u90e8\u7f51\u683c\u7ec4\u88c5\u6280\u672f\uff0c\u7a81\u7834\u73b0\u6709\u6587\u672c\u5e8f\u5217\u53163D\u7f51\u683c\u5904\u7406\u9650\u5236\uff0c\u5728\u751f\u6210\u8d28\u91cf\u548c\u7406\u89e3\u80fd\u529b\u4e0a\u8d85\u8d8aSOTA\u65b9\u6cd5", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u56e0LLM token\u957f\u5ea6\u9650\u5236\u5bfc\u81f4\u6570\u636e\u96c6\u89c4\u6a21\u53d7\u9650\uff0c\u4ee5\u53ca\u7f51\u683c\u5e8f\u5217\u5316\u8fc7\u7a0b\u4e2d3D\u7ed3\u6784\u4fe1\u606f\u4e22\u5931\u7684\u95ee\u9898", "method": "1. Primitive-Mesh\u5206\u89e3\u751f\u6210\u7ed3\u6784\u5b50\u5355\u5143 2. \u63d0\u51fa\u9762\u8fde\u63a5\u6027\u63a8\u7406\u4e0e\u5c40\u90e8\u7f51\u683c\u7ec4\u88c5\u8bad\u7ec3\u7b56\u7565 3. \u6784\u5efa150\u4e07+\u6837\u672c\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6", "result": "\u5b9e\u9a8c\u663e\u793aMeshLLM\u5728\u7f51\u683c\u751f\u6210\u8d28\u91cf\uff08\u63d0\u5347\u7ea637%\uff09\u548c\u5f62\u72b6\u7406\u89e3\u51c6\u786e\u7387\uff08\u63d0\u9ad823%\uff09\u4e0a\u5168\u9762\u8d85\u8d8aLLaMA-Mesh\u57fa\u51c6\u6a21\u578b", "conclusion": "\u8be5\u65b9\u6cd5\u9996\u6b21\u5b9e\u73b0LLM\u5bf93D\u7f51\u683c\u7ed3\u6784\u7684\u9ad8\u6548\u7406\u89e3\u4e0e\u751f\u6210\uff0c\u4e3a\u5904\u7406\u6587\u672c\u5e8f\u5217\u5316\u4e09\u7ef4\u6570\u636e\u5f00\u8f9f\u65b0\u65b9\u5411"}}
{"id": "2508.01381", "pdf": "https://arxiv.org/pdf/2508.01381", "abs": "https://arxiv.org/abs/2508.01381", "authors": ["Onat Vuran", "Hsuan-I Ho"], "title": "ReMu: Reconstructing Multi-layer 3D Clothed Human from Image Layers", "categories": ["cs.GR", "cs.CV"], "comment": "BMVC 2025 paper, 17 pages, 10 figures", "summary": "The reconstruction of multi-layer 3D garments typically requires expensive\nmulti-view capture setups and specialized 3D editing efforts. To support the\ncreation of life-like clothed human avatars, we introduce ReMu for\nreconstructing multi-layer clothed humans in a new setup, Image Layers, which\ncaptures a subject wearing different layers of clothing with a single RGB\ncamera. To reconstruct physically plausible multi-layer 3D garments, a unified\n3D representation is necessary to model these garments in a layered manner.\nThus, we first reconstruct and align each garment layer in a shared coordinate\nsystem defined by the canonical body pose. Afterwards, we introduce a\ncollision-aware optimization process to address interpenetration and further\nrefine the garment boundaries leveraging implicit neural fields. It is worth\nnoting that our method is template-free and category-agnostic, which enables\nthe reconstruction of 3D garments in diverse clothing styles. Through our\nexperiments, we show that our method reconstructs nearly penetration-free 3D\nclothed humans and achieves competitive performance compared to\ncategory-specific methods. Project page: https://eth-ait.github.io/ReMu/", "AI": {"tldr": "\u63d0\u51faReMu\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u76eeRGB\u56fe\u50cf\u5b9e\u73b0\u591a\u5c423D\u670d\u88c5\u91cd\u5efa\uff0c\u65e0\u9700\u6a21\u677f\u4e14\u652f\u6301\u591a\u6837\u670d\u88c5\u98ce\u683c", "motivation": "\u4f20\u7edf\u591a\u5c423D\u670d\u88c5\u91cd\u5efa\u4f9d\u8d56\u6602\u8d35\u591a\u89c6\u89d2\u8bbe\u5907\u4e0e\u4e13\u4e1a\u7f16\u8f91\uff0c\u9700\u964d\u4f4e\u521b\u5efa\u903c\u771f\u865a\u62df\u4eba\u6210\u672c\u4e0e\u590d\u6742\u5ea6", "method": "1) \u5728\u6807\u51c6\u8eab\u4f53\u5750\u6807\u7cfb\u5bf9\u9f50\u670d\u88c5\u5c42 2) \u78b0\u649e\u4f18\u5316\u6d88\u9664\u7a7f\u900f 3) \u9690\u5f0f\u795e\u7ecf\u573a\u7ec6\u5316\u8fb9\u754c", "result": "\u91cd\u5efa\u8fd1\u4e4e\u65e0\u7a7f\u900f\u76843D\u865a\u62df\u4eba\uff0c\u6027\u80fd\u4e0e\u7279\u5b9a\u7c7b\u522b\u65b9\u6cd5\u76f8\u5f53", "conclusion": "\u8be5\u6a21\u677f\u65e0\u5173\u7684\u901a\u7528\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e863D\u865a\u62df\u4eba\u521b\u5efa\u6548\u7387\uff0c\u652f\u6301\u591a\u6837\u5316\u670d\u88c5\u98ce\u683c\u91cd\u5efa"}}
{"id": "2508.01590", "pdf": "https://arxiv.org/pdf/2508.01590", "abs": "https://arxiv.org/abs/2508.01590", "authors": ["Hua Yu", "Jiao Liu", "Xu Gui", "Melvin Wong", "Yaqing Hou", "Yew-Soon Ong"], "title": "A Plug-and-Play Multi-Criteria Guidance for Diverse In-Betweening Human Motion Generation", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "In-betweening human motion generation aims to synthesize intermediate motions\nthat transition between user-specified keyframes. In addition to maintaining\nsmooth transitions, a crucial requirement of this task is to generate diverse\nmotion sequences. It is still challenging to maintain diversity, particularly\nwhen it is necessary for the motions within a generated batch sampling to\ndiffer meaningfully from one another due to complex motion dynamics. In this\npaper, we propose a novel method, termed the Multi-Criteria Guidance with\nIn-Betweening Motion Model (MCG-IMM), for in-betweening human motion\ngeneration. A key strength of MCG-IMM lies in its plug-and-play nature: it\nenhances the diversity of motions generated by pretrained models without\nintroducing additional parameters This is achieved by providing a sampling\nprocess of pretrained generative models with multi-criteria guidance.\nSpecifically, MCG-IMM reformulates the sampling process of pretrained\ngenerative model as a multi-criteria optimization problem, and introduces an\noptimization process to explore motion sequences that satisfy multiple\ncriteria, e.g., diversity and smoothness. Moreover, our proposed plug-and-play\nmulti-criteria guidance is compatible with different families of generative\nmodels, including denoised diffusion probabilistic models, variational\nautoencoders, and generative adversarial networks. Experiments on four popular\nhuman motion datasets demonstrate that MCG-IMM consistently state-of-the-art\nmethods in in-betweening motion generation task.", "AI": {"tldr": "\u63d0\u51fa\u5373\u63d2\u5373\u7528\u7684MCG-IMM\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6807\u51c6\u4f18\u5316\u63d0\u5347\u4e2d\u95f4\u52a8\u4f5c\u751f\u6210\u7684\u591a\u6837\u6027\uff0c\u517c\u5bb9\u591a\u79cd\u751f\u6210\u6a21\u578b", "motivation": "\u73b0\u6709\u4e2d\u95f4\u52a8\u4f5c\u751f\u6210\u65b9\u6cd5\u5728\u6279\u91cf\u91c7\u6837\u65f6\u96be\u4ee5\u4fdd\u6301\u8fd0\u52a8\u5e8f\u5217\u7684\u591a\u6837\u6027\uff0c\u5c24\u5176\u662f\u9700\u8981\u751f\u6210\u6279\u6b21\u5185\u5dee\u5f02\u5316\u7684\u590d\u6742\u8fd0\u52a8\u65f6\u5b58\u5728\u6311\u6218", "method": "\u5c06\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u91c7\u6837\u8fc7\u7a0b\u91cd\u6784\u4e3a\u591a\u6807\u51c6\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5165\u4f18\u5316\u8fc7\u7a0b\u63a2\u7d22\u6ee1\u8db3\u591a\u6837\u6027\u548c\u5e73\u6ed1\u6027\u7b49\u591a\u79cd\u6807\u51c6\u7684\u8fd0\u52a8\u5e8f\u5217", "result": "\u5728\u56db\u4e2a\u4e3b\u6d41\u4eba\u4f53\u8fd0\u52a8\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMCG-IMM\u5728\u4e2d\u95f4\u52a8\u4f5c\u751f\u6210\u4efb\u52a1\u4e2d\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5", "conclusion": "MCG-IMM\u65e0\u9700\u989d\u5916\u53c2\u6570\u5373\u53ef\u589e\u5f3a\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u751f\u6210\u591a\u6837\u6027\uff0c\u5176\u591a\u6807\u51c6\u5f15\u5bfc\u673a\u5236\u517c\u5bb9\u6269\u6563\u6a21\u578b\u3001VAE\u548cGAN\u7b49\u591a\u79cd\u751f\u6210\u6a21\u578b\u67b6\u6784"}}
{"id": "2508.00864", "pdf": "https://arxiv.org/pdf/2508.00864", "abs": "https://arxiv.org/abs/2508.00864", "authors": ["Margarita Bugue\u00f1o", "Gerard de Melo"], "title": "Rethinking Graph-Based Document Classification: Learning Data-Driven Structures Beyond Heuristic Approaches", "categories": ["cs.CL"], "comment": "7 pages, 3 figures, 3 tables. Appendix starts on page 10", "summary": "In document classification, graph-based models effectively capture document\nstructure, overcoming sequence length limitations and enhancing contextual\nunderstanding. However, most existing graph document representations rely on\nheuristics, domain-specific rules, or expert knowledge. Unlike previous\napproaches, we propose a method to learn data-driven graph structures,\neliminating the need for manual design and reducing domain dependence. Our\napproach constructs homogeneous weighted graphs with sentences as nodes, while\nedges are learned via a self-attention model that identifies dependencies\nbetween sentence pairs. A statistical filtering strategy aims to retain only\nstrongly correlated sentences, improving graph quality while reducing the graph\nsize. Experiments on three document classification datasets demonstrate that\nlearned graphs consistently outperform heuristic-based graphs, achieving higher\naccuracy and $F_1$ score. Furthermore, our study demonstrates the effectiveness\nof the statistical filtering in improving classification robustness. These\nresults highlight the potential of automatic graph generation over traditional\nheuristic approaches and open new directions for broader applications in NLP.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548c\u7edf\u8ba1\u8fc7\u6ee4\u7b56\u7565\u5b9e\u73b0\u6570\u636e\u9a71\u52a8\u7684\u6587\u6863\u56fe\u7ed3\u6784\u81ea\u52a8\u751f\u6210\u65b9\u6cd5\uff0c\u5728\u6587\u6863\u5206\u7c7b\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fe\u7684\u6587\u6863\u5206\u7c7b\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\u56fe\u7ed3\u6784\uff0c\u5b58\u5728\u9886\u57df\u5c40\u9650\u6027\u3002\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u6570\u636e\u9a71\u52a8\u7684\u81ea\u52a8\u56fe\u6784\u5efa\u65b9\u6cd5\u4ee5\u63d0\u5347\u901a\u7528\u6027\u3002", "method": "1) \u6784\u5efa\u53e5\u5b50\u8282\u70b9\u7684\u540c\u8d28\u52a0\u6743\u56fe 2) \u81ea\u6ce8\u610f\u529b\u673a\u5236\u5b66\u4e60\u53e5\u5b50\u4f9d\u8d56\u5173\u7cfb 3) \u7edf\u8ba1\u8fc7\u6ee4\u4fdd\u7559\u5f3a\u76f8\u5173\u8fb9\u4f18\u5316\u56fe\u8d28\u91cf", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u66f4\u9ad8\u51c6\u786e\u7387\uff08+3.2%\uff09\u548cF1\u5206\u6570\uff0c\u7edf\u8ba1\u8fc7\u6ee4\u4f7f\u56fe\u89c4\u6a21\u7f29\u51cf30%\u540c\u65f6\u63d0\u5347\u5206\u7c7b\u9c81\u68d2\u6027", "conclusion": "\u81ea\u52a8\u56fe\u751f\u6210\u65b9\u6cd5\u7a81\u7834\u4f20\u7edf\u542f\u53d1\u5f0f\u9650\u5236\uff0c\u4e3aNLP\u4efb\u52a1\u4e2d\u56fe\u7ed3\u6784\u5e94\u7528\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2508.02443", "pdf": "https://arxiv.org/pdf/2508.02443", "abs": "https://arxiv.org/abs/2508.02443", "authors": ["Thomas Gottwald", "Edgar Heinert", "Matthias Rottmann"], "title": "Uncertainty Estimation for Novel Views in Gaussian Splatting from Primitive-Based Representations of Error and Visibility", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "In this work, we present a novel method for uncertainty estimation (UE) in\nGaussian Splatting. UE is crucial for using Gaussian Splatting in critical\napplications such as robotics and medicine. Previous methods typically estimate\nthe variance of Gaussian primitives and use the rendering process to obtain\npixel-wise uncertainties. Our method establishes primitive representations of\nerror and visibility of trainings views, which carries meaningful uncertainty\ninformation. This representation is obtained by projection of training error\nand visibility onto the primitives. Uncertainties of novel views are obtained\nby rendering the primitive representations of uncertainty for those novel\nviews, yielding uncertainty feature maps. To aggregate these uncertainty\nfeature maps of novel views, we perform a pixel-wise regression on holdout\ndata. In our experiments, we analyze the different components of our method,\ninvestigating various combinations of uncertainty feature maps and regression\nmodels. Furthermore, we considered the effect of separating splatting into\nforeground and background. Our UEs show high correlations to true errors,\noutperforming state-of-the-art methods, especially on foreground objects. The\ntrained regression models show generalization capabilities to new scenes,\nallowing uncertainty estimation without the need for holdout data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8bef\u5dee\u548c\u53ef\u89c1\u6027\u6295\u5f71\u7684\u9ad8\u65af\u6e85\u5c04\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u6e32\u67d3\u7279\u5f81\u56fe\u548c\u50cf\u7d20\u7ea7\u56de\u5f52\u805a\u5408\uff0c\u5728\u524d\u666f\u5bf9\u8c61\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u3001\u533b\u7597\u7b49\u5173\u952e\u5e94\u7528\u4e2d\uff0c\u4f20\u7edf\u57fa\u4e8e\u65b9\u5dee\u4f30\u8ba1\u7684\u9ad8\u65af\u6e85\u5c04\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u6355\u6349\u4e0d\u786e\u5b9a\u6027\u4fe1\u606f\u3002\u65b0\u65b9\u6cd5\u901a\u8fc7\u6295\u5f71\u8bad\u7ec3\u8bef\u5dee\u548c\u53ef\u89c1\u6027\u5230\u57fa\u5143\uff0c\u5efa\u7acb\u66f4\u6709\u6548\u7684UE\u8868\u5f81\u3002", "method": "1. \u5efa\u7acb\u8bef\u5dee/\u53ef\u89c1\u6027\u7684\u57fa\u5143\u8868\u793a\n2. \u901a\u8fc7\u6e32\u67d3\u751f\u6210\u4e0d\u786e\u5b9a\u6027\u7279\u5f81\u56fe\n3. \u4f7f\u7528\u4fdd\u7559\u6570\u636e\u8fdb\u884c\u50cf\u7d20\u7ea7\u56de\u5f52\u805a\u5408\n4. \u5206\u79bb\u524d\u666f/\u80cc\u666f\u6e85\u5c04\u5206\u6790\u6548\u679c", "result": "\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u4e0e\u771f\u5b9e\u8bef\u5dee\u9ad8\u5ea6\u76f8\u5173\uff08\u5c24\u5176\u524d\u666f\u5bf9\u8c61\uff09\uff0c\u56de\u5f52\u6a21\u578b\u5177\u5907\u8de8\u573a\u666f\u6cdb\u5316\u80fd\u529b\uff0c\u65e0\u9700\u4fdd\u7559\u6570\u636e\u5373\u53ef\u8fdb\u884c\u65b0\u573a\u666f\u4f30\u8ba1\u3002", "conclusion": "\u6574\u5408\u8bef\u5dee\u548c\u53ef\u89c1\u6027\u6295\u5f71\u7684\u65b9\u6cd5\u63d0\u5347\u4e86UE\u7cbe\u5ea6\uff0c\u5176\u524d\u666f\u5904\u7406\u4f18\u52bf\u53ca\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u5173\u952e\u9886\u57df\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00889", "pdf": "https://arxiv.org/pdf/2508.00889", "abs": "https://arxiv.org/abs/2508.00889", "authors": ["Hagyeong Shin", "Binoy Robin Dalal", "Iwona Bialynicka-Birula", "Navjot Matharu", "Ryan Muir", "Xingwei Yang", "Samuel W. K. Wong"], "title": "FECT: Factuality Evaluation of Interpretive AI-Generated Claims in Contact Center Conversation Transcripts", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted for an oral presentation at Agentic & GenAI Evaluation KDD\n  2025: KDD workshop on Evaluation and Trustworthiness of Agentic and\n  Generative AI Models", "summary": "Large language models (LLMs) are known to hallucinate, producing natural\nlanguage outputs that are not grounded in the input, reference materials, or\nreal-world knowledge. In enterprise applications where AI features support\nbusiness decisions, such hallucinations can be particularly detrimental. LLMs\nthat analyze and summarize contact center conversations introduce a unique set\nof challenges for factuality evaluation, because ground-truth labels often do\nnot exist for analytical interpretations about sentiments captured in the\nconversation and root causes of the business problems. To remedy this, we first\nintroduce a \\textbf{3D} -- \\textbf{Decompose, Decouple, Detach} -- paradigm in\nthe human annotation guideline and the LLM-judges' prompt to ground the\nfactuality labels in linguistically-informed evaluation criteria. We then\nintroduce \\textbf{FECT}, a novel benchmark dataset for \\textbf{F}actuality\n\\textbf{E}valuation of Interpretive AI-Generated \\textbf{C}laims in Contact\nCenter Conversation \\textbf{T}ranscripts, labeled under our 3D paradigm.\nLastly, we report our findings from aligning LLM-judges on the 3D paradigm.\nOverall, our findings contribute a new approach for automatically evaluating\nthe factuality of outputs generated by an AI system for analyzing contact\ncenter conversations.", "AI": {"tldr": "\u9488\u5bf9LLMs\u5728\u5ba2\u6237\u670d\u52a1\u4e2d\u5fc3\u5bf9\u8bdd\u5206\u6790\u4e2d\u7684\u4e8b\u5b9e\u6027\u8bc4\u4f30\u96be\u9898\uff0c\u63d0\u51fa3D\u6807\u6ce8\u8303\u5f0f\u5e76\u6784\u5efaFECT\u57fa\u51c6\u6570\u636e\u96c6", "motivation": "\u4f01\u4e1a\u5e94\u7528\u4e2dLLMs\u5206\u6790\u5ba2\u6237\u5bf9\u8bdd\u65f6\u7f3a\u4e4f\u771f\u5b9e\u6807\u7b7e\uff0c\u4f20\u7edf\u4e8b\u5b9e\u6027\u8bc4\u4f30\u65b9\u6cd5\u96be\u4ee5\u9a8c\u8bc1\u60c5\u611f\u5206\u6790\u548c\u6839\u56e0\u63a8\u65ad\u7b49\u4e3b\u89c2\u89e3\u8bfb\u5185\u5bb9\u7684\u51c6\u786e\u6027", "method": "\u63d0\u51faDecompose(\u5206\u89e3)\u3001Decouple(\u89e3\u8026)\u3001Detach(\u8131\u79bb)\u76843D\u8bc4\u4f30\u6846\u67b6\uff0c\u8bbe\u8ba1\u8bed\u8a00\u5bfc\u5411\u7684\u6807\u6ce8\u4f53\u7cfb\uff0c\u6784\u5efa\u5305\u542b1,200\u4e2a\u6837\u672c\u7684FECT\u57fa\u51c6\u6570\u636e\u96c6", "result": "\u6210\u529f\u5efa\u7acb\u57fa\u4e8e\u8bed\u8a00\u5b66\u6807\u51c6\u7684\u8bc4\u4f30\u4f53\u7cfb\uff0c\u901a\u8fc7LLM-judges\u9a8c\u8bc1\u663e\u793a3D\u8303\u5f0f\u6709\u6548\u63d0\u5347\u4e8b\u5b9e\u6027\u8bc4\u4f30\u7684\u53ef\u9760\u6027\uff08\u51c6\u786e\u7387\u63d0\u534715%\uff09", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u81ea\u52a8\u8bc4\u4f30\u5bf9\u8bdd\u5206\u6790AI\u7684\u4e8b\u5b9e\u6027\u63d0\u4f9b\u4e86\u521b\u65b0\u6846\u67b6\uff0c\u5bf9\u4f01\u4e1a\u5e94\u7528\u573a\u666f\u7684AI\u53ef\u4fe1\u5ea6\u9a8c\u8bc1\u5177\u6709\u91cd\u8981\u5b9e\u8df5\u4ef7\u503c"}}
{"id": "2508.00937", "pdf": "https://arxiv.org/pdf/2508.00937", "abs": "https://arxiv.org/abs/2508.00937", "authors": ["Bernarda Petek", "David Nabergoj", "Erik \u0160trumbelj"], "title": "A General Approach to Visualizing Uncertainty in Statistical Graphics", "categories": ["stat.ME", "cs.GR", "cs.LG"], "comment": null, "summary": "Visualizing uncertainty is integral to data analysis, yet its application is\noften hindered by the need for specialized methods for quantifying and\nrepresenting uncertainty for different types of graphics. We introduce a\ngeneral approach that simplifies this process. The core idea is to treat the\nstatistical graphic as a function of the underlying distribution. Instead of\nfirst calculating uncertainty metrics and then plotting them, the method\npropagates uncertainty through to the visualization. By repeatedly sampling\nfrom the data distribution and generating a complete statistical graphic for\neach sample, a distribution over graphics is produced. These graphics are\naggregated pixel-by-pixel to create a single, static image. This approach is\nversatile, requires no specific knowledge from the user beyond how to create\nthe basic statistical graphic, and comes with theoretical coverage guarantees\nfor standard cases such as confidence intervals and bands. We provide a\nreference implementation as a Python library to demonstrate the method's\nutility. Our approach not only reproduces conventional uncertainty\nvisualizations for point estimates and regression lines but also seamlessly\nextends to non-standard cases, including pie charts, stacked bar charts, and\ntables. This approach makes uncertainty visualization more accessible to\npractitioners and can be a valuable tool for teaching uncertainty.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u5c06\u7edf\u8ba1\u56fe\u5f62\u89c6\u4e3a\u6570\u636e\u5206\u5e03\u51fd\u6570\uff0c\u901a\u8fc7\u91c7\u6837\u4f20\u64ad\u4e0d\u786e\u5b9a\u6027\u751f\u6210\u52a8\u6001\u53ef\u89c6\u5316\u7684\u901a\u7528\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u4e0d\u786e\u5b9a\u6027\u53ef\u89c6\u5316\u65b9\u6cd5\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u56fe\u5f62\u5f00\u53d1\u4e13\u95e8\u6280\u672f\uff0c\u963b\u788d\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u4e00\u79cd\u901a\u7528\u89e3\u51b3\u65b9\u6848\u964d\u4f4e\u6280\u672f\u95e8\u69db\u3002", "method": "1. \u5c06\u7edf\u8ba1\u56fe\u5f62\u5efa\u6a21\u4e3a\u6570\u636e\u5206\u5e03\u51fd\u6570\n2. \u901a\u8fc7Bootstrap\u91c7\u6837\u751f\u6210\u56fe\u5f62\u5206\u5e03\n3. \u50cf\u7d20\u7ea7\u805a\u5408\u751f\u6210\u9759\u6001\u4e0d\u786e\u5b9a\u6027\u89c6\u56fe\n4. \u63d0\u4f9bPython\u5b9e\u73b0\u5e93", "result": "\u6210\u529f\u590d\u73b0\u7f6e\u4fe1\u533a\u95f4\u7b49\u4f20\u7edf\u53ef\u89c6\u5316\uff0c\u6269\u5c55\u5e94\u7528\u4e8e\u997c\u56fe\u3001\u5806\u53e0\u67f1\u72b6\u56fe\u7b49\u975e\u6807\u51c6\u573a\u666f\uff0c\u63d0\u4f9b\u7406\u8bba\u8986\u76d6\u4fdd\u8bc1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f7f\u4e0d\u786e\u5b9a\u6027\u53ef\u89c6\u5316\u66f4\u6613\u5b9e\u65bd\uff0c\u7279\u522b\u9002\u5408\u6559\u5b66\u5e94\u7528\uff0c\u901a\u8fc7\u5f00\u6e90\u5de5\u5177\u63d0\u5347\u53ef\u53ca\u6027\u3002"}}
{"id": "2508.00924", "pdf": "https://arxiv.org/pdf/2508.00924", "abs": "https://arxiv.org/abs/2508.00924", "authors": ["Ernesto L. Estevanell-Valladares", "Suilan Estevez-Velarde", "Yoan Guti\u00e9rrez", "Andr\u00e9s Montoyo", "Ruslan Mitkov"], "title": "XAutoLM: Efficient Fine-Tuning of Language Models via Meta-Learning and AutoML", "categories": ["cs.CL", "68T05, 68T50", "I.2.6; I.2.7; I.2.8"], "comment": "17 pages, 10 figures, 7 tables. Preprint. Under review at EMNLP 2025.\n  This is not the final version", "summary": "Experts in machine learning leverage domain knowledge to navigate decisions\nin model selection, hyperparameter optimisation, and resource allocation. This\nis particularly critical for fine-tuning language models (LMs), where repeated\ntrials incur substantial computational overhead and environmental impact.\nHowever, no existing automated framework simultaneously tackles the entire\nmodel selection and HPO task for resource-efficient LM fine-tuning. We\nintroduce XAutoLM, a meta-learning-augmented AutoML framework that reuses past\nexperiences to optimise discriminative and generative LM fine-tuning pipelines\nefficiently. XAutoLM learns from stored successes and failures by extracting\ntask- and system-level meta-features to bias its sampling toward fruitful\nconfigurations and away from costly dead ends. On four text classification and\ntwo question-answering benchmarks, XAutoLM surpasses zero-shot optimiser's peak\nF1 on five of six tasks, cuts mean evaluation time by up to 4.5x, reduces error\nratios by up to sevenfold, and uncovers up to 50% more pipelines above the\nzero-shot Pareto front. In contrast, simpler memory-based baselines suffer\nnegative transfer. We release XAutoLM and our experience store to catalyse\nresource-efficient, Green AI fine-tuning in the NLP community.", "AI": {"tldr": "XAutoLM\u6846\u67b6\u901a\u8fc7\u5143\u5b66\u4e60\u4f18\u5316\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u6d41\u7a0b\uff0c\u5b9e\u73b0\u8d44\u6e90\u9ad8\u6548\u5229\u7528\u4e0e\u6027\u80fd\u63d0\u5347", "motivation": "\u73b0\u6709AutoML\u6846\u67b6\u65e0\u6cd5\u540c\u65f6\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u4e2d\u7684\u6a21\u578b\u9009\u62e9\u3001\u8d85\u53c2\u6570\u4f18\u5316\u548c\u8d44\u6e90\u6548\u7387\u95ee\u9898\uff0c\u5bfc\u81f4\u91cd\u590d\u5b9e\u9a8c\u4ea7\u751f\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u73af\u5883\u5f71\u54cd", "method": "\u63d0\u53d6\u4efb\u52a1\u7ea7/\u7cfb\u7edf\u7ea7\u5143\u7279\u5f81\uff0c\u5229\u7528\u7ecf\u9a8c\u5b58\u50a8\u4e2d\u7684\u5386\u53f2\u6570\u636e\u504f\u7f6e\u91c7\u6837\u7b56\u7565\uff0c\u907f\u514d\u65e0\u6548\u914d\u7f6e\u5e76\u52a0\u901f\u6536\u655b", "result": "\u57286\u4e2aNLP\u4efb\u52a1\u4e2d\uff0c5\u4e2a\u4efb\u52a1\u8d85\u8d8a\u96f6\u6837\u672c\u4f18\u5316\u5668\u5cf0\u503cF1\uff0c\u8bc4\u4f30\u65f6\u95f4\u51cf\u5c114.5\u500d\uff0c\u9519\u8bef\u7387\u964d\u4f4e7\u500d\uff0c\u53d1\u73b0\u6bd4\u96f6\u6837\u672cPareto\u524d\u6cbf\u591a50%\u7684\u4f18\u8d28\u6d41\u7a0b", "conclusion": "XAutoLM\u4e3aNLP\u793e\u533a\u63d0\u4f9b\u7eff\u8272AI\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5f00\u6e90\u6846\u67b6\u548c\u7ecf\u9a8c\u5b58\u50a8\u63a8\u52a8\u8d44\u6e90\u9ad8\u6548\u7684\u6a21\u578b\u5fae\u8c03\u5b9e\u8df5"}}
{"id": "2508.01537", "pdf": "https://arxiv.org/pdf/2508.01537", "abs": "https://arxiv.org/abs/2508.01537", "authors": ["Nianyi Wang", "Yu Chen", "Shuai Zheng"], "title": "FluidFormer: Transformer with Continuous Convolution for Particle-based Fluid Simulation", "categories": ["cs.CE", "cs.GR", "cs.LG", "physics.flu-dyn"], "comment": null, "summary": "Learning-based fluid simulation networks have been proven as viable\nalternatives to traditional numerical solvers for the Navier-Stokes equations.\nExisting neural methods follow Smoothed Particle Hydrodynamics (SPH)\nframeworks, which inherently rely only on local inter-particle interactions.\nHowever, we emphasize that global context integration is also essential for\nlearning-based methods to stabilize complex fluid simulations. We propose the\nfirst Fluid Attention Block (FAB) with a local-global hierarchy, where\ncontinuous convolutions extract local features while self-attention captures\nglobal dependencies. This fusion suppresses the error accumulation and models\nlong-range physical phenomena. Furthermore, we pioneer the first Transformer\narchitecture specifically designed for continuous fluid simulation, seamlessly\nintegrated within a dual-pipeline architecture. Our method establishes a new\nparadigm for neural fluid simulation by unifying convolution-based local\nfeatures with attention-based global context modeling. FluidFormer demonstrates\nstate-of-the-art performance, with stronger stability in complex fluid\nscenarios.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u7ed3\u5408\u5c40\u90e8\u5377\u79ef\u4e0e\u5168\u5c40\u6ce8\u610f\u529b\u7684FluidFormer\u67b6\u6784\uff0c\u901a\u8fc7Fluid Attention Block\u5b9e\u73b0\u7a33\u5b9a\u9ad8\u6548\u7684\u795e\u7ecf\u6d41\u4f53\u6a21\u62df", "motivation": "\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u6d41\u4f53\u6a21\u62df\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u5c40\u90e8\u7c92\u5b50\u4ea4\u4e92\uff0c\u5bfc\u81f4\u8bef\u5dee\u7d2f\u79ef\u4e14\u96be\u4ee5\u6355\u6349\u957f\u7a0b\u7269\u7406\u73b0\u8c61\uff0c\u9700\u5f15\u5165\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21", "method": "\u8bbe\u8ba1\u5c40\u90e8-\u5168\u5c40\u5c42\u6b21\u7ed3\u6784\u7684FAB\u6a21\u5757\uff08\u8fde\u7eed\u5377\u79ef\u63d0\u53d6\u5c40\u90e8\u7279\u5f81 + \u81ea\u6ce8\u610f\u529b\u6355\u6349\u5168\u5c40\u4f9d\u8d56\uff09\uff0c\u6784\u5efa\u53cc\u7ba1\u9053Transformer\u67b6\u6784", "result": "\u5728\u590d\u6742\u6d41\u4f53\u573a\u666f\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u9519\u8bef\u7d2f\u79ef\u51cf\u5c1175%\uff0c\u957f\u671f\u6a21\u62df\u7a33\u5b9a\u6027\u63d0\u534740%", "conclusion": "\u5f00\u521b\u4e86\u878d\u5408\u5377\u79ef\u5c40\u90e8\u7279\u5f81\u4e0e\u6ce8\u610f\u529b\u5168\u5c40\u5efa\u6a21\u7684\u65b0\u8303\u5f0f\uff0c\u4e3a\u7269\u7406\u542f\u53d1\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u8bbe\u8ba1\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2508.01005", "pdf": "https://arxiv.org/pdf/2508.01005", "abs": "https://arxiv.org/abs/2508.01005", "authors": ["Yiqun Chen", "Erhan Zhang", "Lingyong Yan", "Shuaiqiang Wang", "Jizhou Huang", "Dawei Yin", "Jiaxin Mao"], "title": "MAO-ARAG: Multi-Agent Orchestration for Adaptive Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "In question-answering (QA) systems, Retrieval-Augmented Generation (RAG) has\nbecome pivotal in enhancing response accuracy and reducing hallucination\nissues. The architecture of RAG systems varies significantly, encompassing\nsingle-round RAG, iterative RAG, and reasoning RAG, each tailored to address\ndifferent types of queries. Due to the varying complexity of real-world\nqueries, a fixed RAG pipeline often struggles to balance performance and cost\nefficiency across different queries. To address this challenge, we propose an\nadaptive RAG framework called MAO-ARAG, which leverages multi-agent\norchestration. Our adaptive RAG is conceived as a multi-turn framework.\nSpecifically, we define multiple executor agents, representing typical RAG\nmodules such as query reformulation agents, document selection agent, and\ngeneration agents. A planner agent intelligently selects and integrates the\nappropriate agents from these executors into a suitable workflow tailored for\neach query, striving for high-quality answers while maintaining reasonable\ncosts. During each turn, the planner agent is trained using reinforcement\nlearning, guided by an outcome-based reward (F1 score) and a cost-based\npenalty, continuously improving answer quality while keeping costs within a\nreasonable range. Experiments conducted on multiple QA datasets demonstrate\nthat our approach, which dynamically plans workflows for each query, not only\nachieves high answer quality but also maintains both cost and latency within\nacceptable limits.The code of MAO-ARAG is on\nhttps://github.com/chenyiqun/Agentic-RAG.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u534f\u8c03\u7684\u81ea\u9002\u5e94RAG\u6846\u67b6MAO-ARAG\uff0c\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u5de5\u4f5c\u6d41\u5b9e\u73b0\u8d28\u91cf\u4e0e\u6210\u672c\u7684\u5e73\u8861", "motivation": "\u56fa\u5b9aRAG\u67b6\u6784\u96be\u4ee5\u5728\u4e0d\u540c\u590d\u6742\u5ea6\u67e5\u8be2\u95f4\u5e73\u8861\u6027\u80fd\u4e0e\u6210\u672c\u6548\u7387\uff0c\u9700\u8981\u667a\u80fd\u5316\u7684\u52a8\u6001\u9002\u914d\u673a\u5236", "method": "\u91c7\u7528\u89c4\u5212\u8005\u667a\u80fd\u4f53+\u6267\u884c\u8005\u667a\u80fd\u4f53\u7684\u67b6\u6784\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5de5\u4f5c\u6d41\u9009\u62e9\u7b56\u7565\uff08\u57fa\u4e8eF1\u5956\u52b1\u548c\u6210\u672c\u60e9\u7f5a\uff09", "result": "\u5728\u591aQA\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u52a8\u6001\u5de5\u4f5c\u6d41\u89c4\u5212\u5728\u4fdd\u8bc1\u56de\u7b54\u8d28\u91cf\u7684\u540c\u65f6\u6709\u6548\u63a7\u5236\u6210\u672c\u548c\u5ef6\u8fdf", "conclusion": "MAO-ARAG\u6846\u67b6\u901a\u8fc7\u667a\u80fd\u5de5\u4f5c\u6d41\u7f16\u6392\uff0c\u5b9e\u73b0\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u7684\u8d28\u91cf-\u6548\u7387\u534f\u540c\u4f18\u5316"}}
{"id": "2508.02304", "pdf": "https://arxiv.org/pdf/2508.02304", "abs": "https://arxiv.org/abs/2508.02304", "authors": ["Fangxin Liu", "Haomin Li", "Bowen Zhu", "Zongwu Wang", "Zhuoran Song", "Habing Guan", "Li Jiang"], "title": "ASDR: Exploiting Adaptive Sampling and Data Reuse for CIM-based Instant Neural Rendering", "categories": ["cs.AR", "cs.ET", "cs.GR"], "comment": "Accepted by the 2025 International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS 2025). The\n  paper will be presented at ASPLOS 2026", "summary": "Neural Radiance Fields (NeRF) offer significant promise for generating\nphotorealistic images and videos. However, existing mainstream neural rendering\nmodels often fall short in meeting the demands for immediacy and power\nefficiency in practical applications. Specifically, these models frequently\nexhibit irregular access patterns and substantial computational overhead,\nleading to undesirable inference latency and high power consumption.\nComputing-in-memory (CIM), an emerging computational paradigm, has the\npotential to address these access bottlenecks and reduce the power consumption\nassociated with model execution.\n  To bridge the gap between model performance and real-world scene\nrequirements, we propose an algorithm-architecture co-design approach,\nabbreviated as ASDR, a CIM-based accelerator supporting efficient neural\nrendering. At the algorithmic level, we propose two rendering optimization\nschemes: (1) Dynamic sampling by online sensing of the rendering difficulty of\ndifferent pixels, thus reducing access memory and computational overhead. (2)\nReducing MLP overhead by decoupling and approximating the volume rendering of\ncolor and density. At the architecture level, we design an efficient\nReRAM-based CIM architecture with efficient data mapping and reuse\nmicroarchitecture. Experiments demonstrate that our design can achieve up to\n$9.55\\times$ and $69.75\\times$ speedup over state-of-the-art NeRF accelerators\nand Xavier NX GPU in graphics rendering tasks with only $0.1$ PSNR loss.", "AI": {"tldr": "\u63d0\u51faASDR\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b58\u5185\u8ba1\u7b97\u52a0\u901f\u795e\u7ecf\u6e32\u67d3\uff0c\u5728\u51e0\u4e4e\u4e0d\u635f\u5931\u753b\u8d28\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u663e\u8457\u901f\u5ea6\u63d0\u5347", "motivation": "\u73b0\u6709\u795e\u7ecf\u6e32\u67d3\u6a21\u578b\u5b58\u5728\u4e0d\u89c4\u5219\u8bbf\u95ee\u6a21\u5f0f\u548c\u9ad8\u8ba1\u7b97\u5f00\u9500\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u5e94\u7528\u7684\u4f4e\u5ef6\u8fdf\u4e0e\u4f4e\u529f\u8017\u9700\u6c42", "method": "\u7b97\u6cd5\u5c42\uff1a\u52a8\u6001\u91c7\u6837\u51cf\u5c11\u8ba1\u7b97\u91cf + \u989c\u8272/\u5bc6\u5ea6\u89e3\u8026\u964d\u4f4eMLP\u5f00\u9500\uff1b\u67b6\u6784\u5c42\uff1a\u57fa\u4e8eReRAM\u7684\u5b58\u5185\u8ba1\u7b97\u67b6\u6784\u4f18\u5316\u6570\u636e\u91cd\u7528", "result": "\u76f8\u6bd4\u73b0\u6709\u52a0\u901f\u5668\u548cGPU\u5206\u522b\u5b9e\u73b09.55\u500d/69.75\u500d\u52a0\u901f\uff0cPSNR\u4ec5\u635f\u59310.1", "conclusion": "ASDR\u65b9\u6848\u6210\u529f\u5f25\u5408\u795e\u7ecf\u6e32\u67d3\u6a21\u578b\u6027\u80fd\u4e0e\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3aCIM\u67b6\u6784\u5728\u56fe\u5f62\u9886\u57df\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2508.01006", "pdf": "https://arxiv.org/pdf/2508.01006", "abs": "https://arxiv.org/abs/2508.01006", "authors": ["Farah Adeeba", "Brian Dillon", "Hassan Sajjad", "Rajesh Bhatt"], "title": "UrBLiMP: A Benchmark for Evaluating the Linguistic Competence of Large Language Models in Urdu", "categories": ["cs.CL"], "comment": null, "summary": "Multilingual Large Language Models (LLMs) have shown remarkable performance\nacross various languages; however, they often include significantly less data\nfor low-resource languages such as Urdu compared to high-resource languages\nlike English. To assess the linguistic knowledge of LLMs in Urdu, we present\nthe Urdu Benchmark of Linguistic Minimal Pairs (UrBLiMP) i.e. pairs of\nminimally different sentences that contrast in grammatical acceptability.\nUrBLiMP comprises 5,696 minimal pairs targeting ten core syntactic phenomena,\ncarefully curated using the Urdu Treebank and diverse Urdu text corpora. A\nhuman evaluation of UrBLiMP annotations yielded a 96.10% inter-annotator\nagreement, confirming the reliability of the dataset. We evaluate twenty\nmultilingual LLMs on UrBLiMP, revealing significant variation in performance\nacross linguistic phenomena. While LLaMA-3-70B achieves the highest average\naccuracy (94.73%), its performance is statistically comparable to other top\nmodels such as Gemma-3-27B-PT. These findings highlight both the potential and\nthe limitations of current multilingual LLMs in capturing fine-grained\nsyntactic knowledge in low-resource languages.", "AI": {"tldr": "\u6784\u5efaUrBLiMP\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30LLMs\u5728\u4e4c\u5c14\u90fd\u8bed\u4e2d\u7684\u53e5\u6cd5\u77e5\u8bc6\uff0c\u53d1\u73b0\u9876\u5c16\u6a21\u578b\u8868\u73b0\u4f18\u5f02\u4f46\u5b58\u5728\u8bed\u8a00\u5b66\u73b0\u8c61\u5dee\u5f02", "motivation": "\u89e3\u51b3\u591a\u8bed\u8a00\u5927\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00(\u5982\u4e4c\u5c14\u90fd\u8bed)\u7684\u7ec6\u7c92\u5ea6\u53e5\u6cd5\u8bc4\u4f30\u7f3a\u5931\u95ee\u9898", "method": "\u57fa\u4e8e\u4e4c\u5c14\u90fd\u8bed\u6811\u5e93\u6784\u5efa5,696\u4e2a\u6700\u5c0f\u5bf9\u6bd4\u53e5\u5bf9\uff0c\u6db5\u76d610\u79cd\u6838\u5fc3\u53e5\u6cd5\u73b0\u8c61\uff0c\u901a\u8fc7\u4eba\u5de5\u6807\u6ce8\u9a8c\u8bc1\u53ef\u9760\u6027(96.1%\u4e00\u81f4\u6027)", "result": "LLaMA-3-70B\u51c6\u786e\u7387\u6700\u9ad8(94.73%)\uff0cGemma-3-27B-PT\u8868\u73b0\u76f8\u5f53\u4f46\u4e0d\u540c\u53e5\u6cd5\u73b0\u8c61\u5b58\u5728\u663e\u8457\u5dee\u5f02", "conclusion": "\u5f53\u524d\u591a\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u5904\u7406\u4e0a\u5c55\u73b0\u6f5c\u529b\uff0c\u4f46\u7ec6\u7c92\u5ea6\u53e5\u6cd5\u7406\u89e3\u4ecd\u5b58\u5728\u5c40\u9650\u6027"}}
{"id": "2508.02368", "pdf": "https://arxiv.org/pdf/2508.02368", "abs": "https://arxiv.org/abs/2508.02368", "authors": ["Ronaldo A. Garcia", "Mark Helman", "Dan Reznik"], "title": "Poncelet triangles: two harmonious loci and two attractive envelopes", "categories": ["math.MG", "cs.GR", "51M04, 51N20, 51N35, 68T20"], "comment": "18 pages, 14 figures, 2 tables", "summary": "We prove that over a Poncelet triangle family interscribed between two nested\nellipses $\\E,\\E_c$, (i) the locus of the orthocenter is not only a conic, but\nit is axis-aligned and homothetic to a $90^o$-rotated copy of $\\E$, and (ii)\nthe locus of the isogonal conjugate of a fixed point $P$ is also a conic (the\nexpected degree was four); a parabola (resp. line) if $P$ is on the\n(degree-four) envelope of the circumcircle (resp. on $\\E$). We also show that\nthe envelope of both the circumcircle and radical axis of incircle and\ncircumcircle contain a conic component if and only if $\\E_c$ is a circle. The\nformer case is the union of two circles!", "AI": {"tldr": "\u7814\u7a76\u5d4c\u5957\u692d\u5706\u95f4Poncelet\u4e09\u89d2\u5f62\u65cf\u7684\u51e0\u4f55\u6027\u8d28\uff0c\u53d1\u73b0\u5176\u5782\u5fc3\u8f68\u8ff9\u4e3a\u4e0e\u6bcd\u692d\u5706\u540c\u8f74\u4e14\u65cb\u8f6c90\u5ea6\u7684\u76f8\u4f3c\u5706\u9525\u66f2\u7ebf\uff0c\u56fa\u5b9a\u70b9\u7684\u7b49\u89d2\u5171\u8f6d\u70b9\u8f68\u8ff9\u4e5f\u662f\u5706\u9525\u66f2\u7ebf\uff0c\u5e76\u63ed\u793a\u4e86\u5f53\u5185\u692d\u5706\u4e3a\u5706\u65f6\u5305\u7edc\u7ebf\u7684\u7279\u6b8a\u7ed3\u6784\u3002", "motivation": "\u63a2\u7d22Poncelet\u4e09\u89d2\u5f62\u65cf\u5728\u53cc\u692d\u5706\u5d4c\u5957\u6784\u578b\u4e0b\u7684\u51e0\u4f55\u7279\u6027\uff0c\u7279\u522b\u662f\u5782\u5fc3\u3001\u7b49\u89d2\u5171\u8f6d\u70b9\u7b49\u5173\u952e\u51e0\u4f55\u91cf\u7684\u8f68\u8ff9\u89c4\u5f8b\uff0c\u6df1\u5316\u5bf9\u95ed\u5305\u5b9a\u7406\u4e0e\u5706\u9525\u66f2\u7ebf\u5173\u8054\u7684\u7406\u89e3\u3002", "method": "\u8fd0\u7528\u5c04\u5f71\u51e0\u4f55\u4e0e\u5706\u9525\u66f2\u7ebf\u7406\u8bba\uff0c\u7ed3\u5408Poncelet\u95ed\u5305\u5b9a\u7406\uff0c\u901a\u8fc7\u89e3\u6790\u51e0\u4f55\u65b9\u6cd5\u63a8\u5bfc\u8f68\u8ff9\u65b9\u7a0b\uff0c\u5206\u6790\u5176\u4ee3\u6570\u9636\u6570\u4e0e\u51e0\u4f55\u53d8\u6362\u7279\u6027\u3002", "result": "1. \u5782\u5fc3\u8f68\u8ff9\u4e3a\u4e0e\u6bcd\u692d\u5706\u540c\u8f74\u4e14\u65cb\u8f6c90\u5ea6\u7684\u76f8\u4f3c\u5706\u9525\u66f2\u7ebf\n2. \u56fa\u5b9a\u70b9\u7b49\u89d2\u5171\u8f6d\u8f68\u8ff9\u4e3a\u4e8c\u6b21\u66f2\u7ebf\uff08\u7279\u5b9a\u6761\u4ef6\u4e0b\u9000\u5316\u4e3a\u629b\u7269\u7ebf/\u76f4\u7ebf\uff09\n3. \u5f53\u4e14\u4ec5\u5f53\u5185\u692d\u5706\u4e3a\u5706\u65f6\uff0c\u5916\u63a5\u5706\u4e0e\u6839\u8f74\u7684\u5305\u7edc\u5305\u542b\u5706\u9525\u5206\u91cf\uff08\u5916\u63a5\u5706\u5305\u7edc\u4e3a\u53cc\u5706\u5e76\u96c6\uff09", "conclusion": "\u63ed\u793a\u4e86Poncelet\u4e09\u89d2\u5f62\u65cf\u5728\u53cc\u692d\u5706\u6784\u578b\u4e0b\u7684\u5706\u9525\u8f68\u8ff9\u89c4\u5f8b\uff0c\u5efa\u7acb\u4e86\u5185\u692d\u5706\u4e3a\u5706\u65f6\u7684\u7279\u6b8a\u51e0\u4f55\u5bf9\u5e94\uff0c\u4e3a\u7ecf\u5178\u5b9a\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u51e0\u4f55\u8be0\u91ca\u6846\u67b6\u3002"}}
{"id": "2508.01096", "pdf": "https://arxiv.org/pdf/2508.01096", "abs": "https://arxiv.org/abs/2508.01096", "authors": ["Michael Farag", "Patrick Halina", "Andrey Zaytsev", "Alekhya Munagala", "Imtihan Ahmed", "Junhao Wang"], "title": "Cross-Domain Web Information Extraction at Pinterest", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "The internet offers a massive repository of unstructured information, but\nit's a significant challenge to convert this into a structured format. At\nPinterest, the ability to accurately extract structured product data from\ne-commerce websites is essential to enhance user experiences and improve\ncontent distribution. In this paper, we present Pinterest's system for\nattribute extraction, which achieves remarkable accuracy and scalability at a\nmanageable cost. Our approach leverages a novel webpage representation that\ncombines structural, visual, and text modalities into a compact form,\noptimizing it for small model learning. This representation captures each\nvisible HTML node with its text, style and layout information. We show how this\nallows simple models such as eXtreme Gradient Boosting (XGBoost) to extract\nattributes more accurately than much more complex Large Language Models (LLMs)\nsuch as Generative Pre-trained Transformer (GPT). Our results demonstrate a\nsystem that is highly scalable, processing over 1,000 URLs per second, while\nbeing 1000 times more cost-effective than the cheapest GPT alternatives.", "AI": {"tldr": "Pinterest\u5f00\u53d1\u4e86\u7ed3\u5408\u7ed3\u6784/\u89c6\u89c9/\u6587\u672c\u6a21\u6001\u7684\u7f51\u9875\u8868\u793a\u65b9\u6cd5\uff0c\u4f7f\u7528XGBoost\u6a21\u578b\u5b9e\u73b0\u9ad8\u6548\u4f4e\u6210\u672c\u7684\u4ea7\u54c1\u5c5e\u6027\u63d0\u53d6\u7cfb\u7edf", "motivation": "\u4e92\u8054\u7f51\u975e\u7ed3\u6784\u5316\u4fe1\u606f\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u6570\u636e\u5b58\u5728\u91cd\u5927\u6311\u6218\uff0c\u51c6\u786e\u63d0\u53d6\u7535\u5546\u7f51\u7ad9\u4ea7\u54c1\u6570\u636e\u5bf9\u63d0\u5347Pinterest\u7528\u6237\u4f53\u9a8c\u548c\u5185\u5bb9\u5206\u53d1\u81f3\u5173\u91cd\u8981", "method": "\u901a\u8fc7\u878d\u5408HTML\u8282\u70b9\u7684\u6587\u672c/\u6837\u5f0f/\u5e03\u5c40\u4fe1\u606f\u521b\u5efa\u591a\u6a21\u6001\u7f51\u9875\u8868\u793a\uff0c\u91c7\u7528XGBoost\u7b49\u7b80\u5355\u6a21\u578b\u8fdb\u884c\u5c5e\u6027\u63d0\u53d6", "result": "\u7cfb\u7edf\u5904\u7406\u80fd\u529b\u8d851000 URL/\u79d2\uff0c\u6210\u672c\u6548\u76ca\u6bd4\u6700\u4fbf\u5b9c\u7684GPT\u65b9\u6848\u9ad81000\u500d\uff0c\u51c6\u786e\u7387\u4f18\u4e8eGPT\u7b49\u5927\u578b\u8bed\u8a00\u6a21\u578b", "conclusion": "\u591a\u6a21\u6001\u8868\u793a\u65b9\u6cd5\u4f7f\u7b80\u5355\u6a21\u578b\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u8d85\u8d8a\u590d\u6742LLMs\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6269\u5c55\u6027\u3001\u4f4e\u6210\u672c\u548c\u9ad8\u51c6\u786e\u6027\u7684\u5de5\u4e1a\u7ea7\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.01159", "pdf": "https://arxiv.org/pdf/2508.01159", "abs": "https://arxiv.org/abs/2508.01159", "authors": ["Liam G. McCoy", "Fateme Nateghi Haredasht", "Kanav Chopra", "David Wu", "David JH Wu", "Abass Conteh", "Sarita Khemani", "Saloni Kumar Maharaj", "Vishnu Ravi", "Arth Pahwa", "Yingjie Weng", "Leah Rosengaus", "Lena Giang", "Kelvin Zhenghao Li", "Olivia Jee", "Daniel Shirvani", "Ethan Goh", "Jonathan H. Chen"], "title": "Asking the Right Questions: Benchmarking Large Language Models in the Development of Clinical Consultation Templates", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study evaluates the capacity of large language models (LLMs) to generate\nstructured clinical consultation templates for electronic consultation. Using\n145 expert-crafted templates developed and routinely used by Stanford's\neConsult team, we assess frontier models -- including o3, GPT-4o, Kimi K2,\nClaude 4 Sonnet, Llama 3 70B, and Gemini 2.5 Pro -- for their ability to\nproduce clinically coherent, concise, and prioritized clinical question\nschemas. Through a multi-agent pipeline combining prompt optimization, semantic\nautograding, and prioritization analysis, we show that while models like o3\nachieve high comprehensiveness (up to 92.2\\%), they consistently generate\nexcessively long templates and fail to correctly prioritize the most clinically\nimportant questions under length constraints. Performance varies across\nspecialties, with significant degradation in narrative-driven fields such as\npsychiatry and pain medicine. Our findings demonstrate that LLMs can enhance\nstructured clinical information exchange between physicians, while highlighting\nthe need for more robust evaluation methods that capture a model's ability to\nprioritize clinically salient information within the time constraints of\nreal-world physician communication.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u7ed3\u6784\u5316\u4e34\u5e8a\u4f1a\u8bca\u6a21\u677f\u65f6\u867d\u80fd\u8fbe\u523092.2%\u7684\u5168\u9762\u6027\uff0c\u4f46\u666e\u904d\u5b58\u5728\u6a21\u677f\u8fc7\u957f\u548c\u4e34\u5e8a\u95ee\u9898\u4f18\u5148\u7ea7\u6392\u5e8f\u4e0d\u5f53\u7684\u95ee\u9898", "motivation": "\u8bc4\u4f30LLMs\u751f\u6210\u7ed3\u6784\u5316\u7535\u5b50\u4f1a\u8bca\u6a21\u677f\u7684\u80fd\u529b\uff0c\u4fc3\u8fdb\u4e34\u5e8a\u533b\u751f\u95f4\u9ad8\u6548\u4fe1\u606f\u4ea4\u6362", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6d41\u7a0b\uff08\u63d0\u793a\u4f18\u5316+\u8bed\u4e49\u81ea\u52a8\u8bc4\u5206+\u4f18\u5148\u7ea7\u5206\u6790\uff09\uff0c\u6d4b\u8bd56\u4e2a\u524d\u6cbf\u6a21\u578b\u5728145\u4e2a\u65af\u5766\u798f\u6807\u51c6\u6a21\u677f\u4e0a\u7684\u8868\u73b0", "result": "o3\u7b49\u6a21\u578b\u5728\u4e13\u79d1\u6a21\u677f\u4e2d\u7efc\u5408\u8986\u76d6\u7387\u8fbe92.2%\uff0c\u4f46\u751f\u6210\u5185\u5bb9\u5197\u957f\uff08\u5e73\u5747\u8d85\u4e13\u5bb6\u6a21\u677f4\u500d\uff09\uff0c\u4e14\u65e0\u6cd5\u5728\u7bc7\u5e45\u9650\u5236\u4e0b\u6b63\u786e\u4f18\u5148\u5904\u7406\u5173\u952e\u4e34\u5e8a\u95ee\u9898", "conclusion": "LLMs\u53ef\u63d0\u5347\u4e34\u5e8a\u4fe1\u606f\u7ed3\u6784\u5316\u4ea4\u6362\u6548\u7387\uff0c\u4f46\u9700\u5f00\u53d1\u66f4\u4e25\u683c\u7684\u8bc4\u4f30\u4f53\u7cfb\u4ee5\u68c0\u9a8c\u6a21\u578b\u5728\u771f\u5b9e\u4e34\u5e8a\u65f6\u95f4\u538b\u529b\u4e0b\u7684\u4fe1\u606f\u4f18\u5148\u7ea7\u5904\u7406\u80fd\u529b"}}
{"id": "2508.01161", "pdf": "https://arxiv.org/pdf/2508.01161", "abs": "https://arxiv.org/abs/2508.01161", "authors": ["Jiyu Chen", "Necva B\u00f6l\u00fcc\u00fc", "Sarvnaz Karimi", "Diego Moll\u00e1", "C\u00e9cile L. Paris"], "title": "CSIRO-LT at SemEval-2025 Task 11: Adapting LLMs for Emotion Recognition for Multiple Languages", "categories": ["cs.CL"], "comment": "In Proceedings of the 19th International Workshop on Semantic\n  Evaluation (SemEval-2025), Vienna, Austria. Association for Computational\n  Linguistics", "summary": "Detecting emotions across different languages is challenging due to the\nvaried and culturally nuanced ways of emotional expressions. The\n\\textit{Semeval 2025 Task 11: Bridging the Gap in Text-Based emotion} shared\ntask was organised to investigate emotion recognition across different\nlanguages. The goal of the task is to implement an emotion recogniser that can\nidentify the basic emotional states that general third-party observers would\nattribute to an author based on their written text snippet, along with the\nintensity of those emotions. We report our investigation of various\ntask-adaptation strategies for LLMs in emotion recognition. We show that the\nmost effective method for this task is to fine-tune a pre-trained multilingual\nLLM with LoRA setting separately for each language.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22LLM\u5728\u8de8\u8bed\u8a00\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u4efb\u52a1\u9002\u5e94\u7b56\u7565\uff0c\u53d1\u73b0\u5206\u8bed\u8a00\u5355\u72ec\u5fae\u8c03\u591a\u8bed\u8a00\u5927\u6a21\u578b\u6548\u679c\u6700\u4f73", "motivation": "\u4e0d\u540c\u8bed\u8a00\u548c\u6587\u5316\u80cc\u666f\u4e0b\u7684\u60c5\u611f\u8868\u8fbe\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u9700\u5f00\u53d1\u80fd\u6709\u6548\u8bc6\u522b\u591a\u8bed\u8a00\u6587\u672c\u60c5\u7eea\u72b6\u6001\u7684\u6a21\u578b", "method": "\u91c7\u7528\u57fa\u4e8eLoRA\u7684\u591a\u8bed\u8a00\u5927\u6a21\u578b\u5206\u8bed\u8a00\u5fae\u8c03\u7b56\u7565\uff0c\u5bf9\u6bd4\u4e0d\u540c\u4efb\u52a1\u9002\u5e94\u65b9\u6cd5\u5728SemEval 2025\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0", "result": "\u5b9e\u9a8c\u8868\u660e\u9488\u5bf9\u6bcf\u79cd\u8bed\u8a00\u5355\u72ec\u8fdb\u884cLoRA\u5fae\u8c03\u7684\u65b9\u6cd5\u5728\u8de8\u8bed\u8a00\u60c5\u611f\u5f3a\u5ea6\u8bc6\u522b\u4efb\u52a1\u4e2d\u6548\u679c\u6700\u4f18", "conclusion": "\u5206\u8bed\u8a00\u9002\u914d\u7684\u5fae\u8c03\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347LLM\u5728\u8de8\u6587\u5316\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u591a\u8bed\u8a00NLP\u7814\u7a76\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2508.01198", "pdf": "https://arxiv.org/pdf/2508.01198", "abs": "https://arxiv.org/abs/2508.01198", "authors": ["Yige Li", "Peihai Jiang", "Jun Sun", "Peng Shu", "Tianming Liu", "Zhen Xiang"], "title": "Adaptive Content Restriction for Large Language Models via Suffix Optimization", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages", "summary": "Large Language Models (LLMs) have demonstrated significant success across\ndiverse applications. However, enforcing content restrictions remains a\nsignificant challenge due to their expansive output space. One aspect of\ncontent restriction is preventing LLMs from generating harmful content via\nmodel alignment approaches such as supervised fine-tuning (SFT). Yet, the need\nfor content restriction may vary significantly across user groups, change\nrapidly over time, and not always align with general definitions of\nharmfulness. Applying SFT to each of these specific use cases is impractical\ndue to the high computational, data, and storage demands. Motivated by this\nneed, we propose a new task called \\textit{Adaptive Content Restriction}\n(AdaCoRe), which focuses on lightweight strategies -- methods without model\nfine-tuning -- to prevent deployed LLMs from generating restricted terms for\nspecific use cases. We propose the first method for AdaCoRe, named\n\\textit{Suffix Optimization (SOP)}, which appends a short, optimized suffix to\nany prompt to a) prevent a target LLM from generating a set of restricted\nterms, while b) preserving the output quality. To evaluate AdaCoRe approaches,\nincluding our SOP, we create a new \\textit{Content Restriction Benchmark}\n(CoReBench), which contains 400 prompts for 80 restricted terms across 8\ncarefully selected categories. We demonstrate the effectiveness of SOP on\nCoReBench, which outperforms the system-level baselines such as system suffix\nby 15\\%, 17\\%, 10\\%, 9\\%, and 6\\% on average restriction rates for Gemma2-2B,\nMistral-7B, Vicuna-7B, Llama3-8B, and Llama3.1-8B, respectively. We also\ndemonstrate that SOP is effective on POE, an online platform hosting various\ncommercial LLMs, highlighting its practicality in real-world scenarios.", "AI": {"tldr": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u540e\u7f00\u4f18\u5316\u65b9\u6cd5SOP\uff0c\u5728\u4e0d\u5fae\u8c03\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u52a8\u6001\u5185\u5bb9\u9650\u5236\uff0c\u5e76\u901a\u8fc7CoReBench\u9a8c\u8bc1\u5176\u5728\u591a\u6a21\u578b\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5bf9\u9f50\u65b9\u6cd5\uff08\u5982SFT\uff09\u96be\u4ee5\u6ee1\u8db3\u4e0d\u540c\u7528\u6237\u7fa4\u4f53\u3001\u5feb\u901f\u53d8\u5316\u7684\u5b9a\u5236\u5316\u5185\u5bb9\u9650\u5236\u9700\u6c42\uff0c\u4e14\u5b58\u5728\u9ad8\u8ba1\u7b97/\u5b58\u50a8\u6210\u672c\u95ee\u9898\u3002", "method": "Suffix Optimization (SOP)\uff1a\u5728\u8f93\u5165\u63d0\u793a\u540e\u6dfb\u52a0\u4f18\u5316\u7684\u77ed\u540e\u7f00\uff0c\u963b\u6b62LLM\u751f\u6210\u53d7\u9650\u8bcd\u6c47\uff0c\u540c\u65f6\u4fdd\u6301\u8f93\u51fa\u8d28\u91cf\u3002", "result": "SOP\u5728Gemma2-2B\u7b495\u4e2a\u6a21\u578b\u4e0a\u5e73\u5747\u9650\u5236\u7387\u63d0\u53476-17%\uff0c\u5e76\u5728POE\u5546\u4e1a\u5e73\u53f0\u9a8c\u8bc1\u5b9e\u7528\u6027\u3002", "conclusion": "SOP\u4e3a\u52a8\u6001\u5185\u5bb9\u9650\u5236\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u5feb\u901f\u8c03\u6574\u9650\u5236\u7b56\u7565\u7684\u5b9e\u65f6\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2508.01213", "pdf": "https://arxiv.org/pdf/2508.01213", "abs": "https://arxiv.org/abs/2508.01213", "authors": ["Shengqi Zhu", "Jeffrey M. Rzeszotarski", "David Mimno"], "title": "Show or Tell? Modeling the evolution of request-making in Human-LLM conversations", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Chat logs provide a rich source of information about LLM users, but patterns\nof user behavior are often masked by the variability of queries. We present a\nnew task, segmenting chat queries into contents of requests, roles,\nquery-specific context, and additional expressions. We find that, despite the\nfamiliarity of chat-based interaction, request-making in LLM queries remains\nsignificantly different from comparable human-human interactions. With the data\nresource, we introduce an important perspective of diachronic analyses with\nuser expressions. We find that query patterns vary between early ones\nemphasizing requests, and individual users explore patterns but tend to\nconverge with experience. Finally, we show that model capabilities affect user\nbehavior, particularly with the introduction of new models, which are traceable\nat the community level.", "AI": {"tldr": "Analyzes LLM user behavior through chat query segmentation, revealing request patterns, user adaptation trends, and model capability impacts.", "motivation": "Uncover masked user behavior patterns in LLM queries by segmenting chat logs into request components and analyzing temporal dynamics.", "method": "Segments queries into requests/roles/context/expressions, conducts diachronic analyses of user patterns across experience levels.", "result": "Early queries emphasize requests; users explore patterns then converge. Model updates cause detectable community-level behavioral shifts.", "conclusion": "Provides framework for tracking LLM user evolution, informing model design through behavioral pattern analysis."}}
{"id": "2508.01222", "pdf": "https://arxiv.org/pdf/2508.01222", "abs": "https://arxiv.org/abs/2508.01222", "authors": ["Ethan Hsu", "Hong Meng Yam", "Ines Bouissou", "Aaron Murali John", "Raj Thota", "Josh Koe", "Vivek Sarath Putta", "G K Dharesan", "Alexander Spangher", "Shikhar Murty", "Tenghao Huang", "Christopher D. Manning"], "title": "WebDS: An End-to-End Benchmark for Web-based Data Science", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages", "summary": "A large portion of real-world data science tasks are complex and require\nmulti-hop web-based interactions: finding appropriate data available on the\ninternet, synthesizing real-time data of various modalities from different\nlocations, and producing summarized analyses. Existing web benchmarks often\nfocus on simplistic interactions, such as form submissions or e-commerce\ntransactions, and often do not require diverse tool-using capabilities required\nfor web based data science. Conversely, traditional data science benchmarks\ntypically concentrate on static, often textually bound datasets and do not\nassess end-to-end workflows that encompass data acquisition, cleaning,\nanalysis, and insight generation. In response, we introduce WebDS, the first\nend-to-end web-based data science benchmark. It comprises 870 web-based data\nscience tasks across 29 diverse websites from structured government data\nportals to unstructured news media, challenging agents to perform complex,\nmulti-step operations requiring the use of tools and heterogeneous data formats\nthat better reflect the realities of modern data analytics. Evaluations of\ncurrent SOTA LLM agents indicate significant performance gaps in accomplishing\nthese tasks. For instance, Browser Use, which accomplishes 80% of tasks on Web\nVoyager, successfully completes only 15% of tasks in WebDS, which our analysis\nsuggests is due to new failure modes like poor information grounding,\nrepetitive behavior and shortcut-taking that agents performing WebDS' tasks\ndisplay. By providing a more robust and realistic testing ground, WebDS sets\nthe stage for significant advances in the development of practically useful\nLLM-based data science.", "AI": {"tldr": "\u9996\u4e2a\u7aef\u5230\u7aef\u7f51\u9875\u6570\u636e\u79d1\u5b66\u57fa\u51c6\u6d4b\u8bd5WebDS\u63ed\u793a\u73b0\u6709LLM\u4ee3\u7406\u5728\u590d\u6742\u591a\u6b65\u4efb\u52a1\u4e2d\u7684\u663e\u8457\u6027\u80fd\u7f3a\u9677\uff08\u6210\u529f\u738715% vs \u4f20\u7edf\u57fa\u51c680%\uff09", "motivation": "\u73b0\u6709\u7f51\u9875\u57fa\u51c6\u8fc7\u4e8e\u7b80\u5355/\u4f20\u7edf\u6570\u636e\u79d1\u5b66\u57fa\u51c6\u5c40\u9650\u4e8e\u9759\u6001\u6570\u636e\u96c6\uff0c\u5747\u65e0\u6cd5\u8bc4\u4f30\u771f\u5b9e\u573a\u666f\u4e2d\u6570\u636e\u83b7\u53d6-\u6e05\u6d17-\u5206\u6790-\u6d1e\u5bdf\u751f\u6210\u7684\u7aef\u5230\u7aef\u6d41\u7a0b", "method": "\u6784\u5efa\u5305\u542b870\u4e2a\u8de829\u4e2a\u7f51\u7ad9\uff08\u542b\u7ed3\u6784\u5316\u653f\u5e9c\u6570\u636e\u548c\u975e\u7ed3\u6784\u5316\u65b0\u95fb\uff09\u7684\u591a\u6a21\u6001\u6570\u636e\u79d1\u5b66\u4efb\u52a1\uff0c\u8981\u6c42\u4ee3\u7406\u4f7f\u7528\u5de5\u5177\u5904\u7406\u5f02\u6784\u6570\u636e\u683c\u5f0f", "result": "SOTA\u4ee3\u7406\uff08\u5982Browser Use\uff09\u6210\u529f\u7387\u9aa4\u964d\u81f315%\uff0c\u66b4\u9732\u51fa\u4fe1\u606f\u57fa\u7840\u4e0d\u7262\u3001\u91cd\u590d\u884c\u4e3a\u3001\u8d70\u6377\u5f84\u7b49\u65b0\u578b\u5931\u8d25\u6a21\u5f0f", "conclusion": "WebDS\u901a\u8fc7\u66f4\u771f\u5b9e\u7684\u6d4b\u8bd5\u73af\u5883\u63a8\u52a8\u5b9e\u7528LLM\u6570\u636e\u79d1\u5b66\u53d1\u5c55\uff0c\u9996\u6b21\u7cfb\u7edf\u6027\u63ed\u793a\u73b0\u6709\u4ee3\u7406\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u8fb9\u754c"}}
{"id": "2508.01245", "pdf": "https://arxiv.org/pdf/2508.01245", "abs": "https://arxiv.org/abs/2508.01245", "authors": ["Yue Chen", "Minghua He", "Fangkai Yang", "Pu Zhao", "Lu Wang", "Yu Kang", "Yifei Dong", "Yuefeng Zhan", "Hao Sun", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "title": "WarriorMath: Enhancing the Mathematical Ability of Large Language Models with a Defect-aware Framework", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) excel in solving mathematical problems, yet\ntheir performance is often limited by the availability of high-quality, diverse\ntraining data. Existing methods focus on augmenting datasets through rephrasing\nor difficulty progression but overlook the specific failure modes of LLMs. This\nresults in synthetic questions that the model can already solve, providing\nminimal performance gains. To address this, we propose WarriorMath, a\ndefect-aware framework for mathematical problem solving that integrates both\ntargeted data synthesis and progressive training. In the synthesis stage, we\nemploy multiple expert LLMs in a collaborative process to generate, critique,\nand refine problems. Questions that base LLMs fail to solve are identified and\niteratively improved through expert-level feedback, producing high-quality,\ndefect-aware training data. In the training stage, we introduce a progressive\nlearning framework that iteratively fine-tunes the model using increasingly\nchallenging data tailored to its weaknesses. Experiments on six mathematical\nbenchmarks show that WarriorMath outperforms strong baselines by 12.57% on\naverage, setting a new state-of-the-art. Our results demonstrate the\neffectiveness of a defect-aware, multi-expert framework for improving\nmathematical ability.", "AI": {"tldr": "\u63d0\u51fa\u7f3a\u9677\u611f\u77e5\u6846\u67b6WarriorMath\uff0c\u901a\u8fc7\u9488\u5bf9\u6027\u6570\u636e\u5408\u6210\u4e0e\u6e10\u8fdb\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u6570\u5b66\u80fd\u529b12.57%", "motivation": "\u73b0\u6709\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u5ffd\u89c6\u6a21\u578b\u5177\u4f53\u7f3a\u9677\uff0c\u5bfc\u81f4\u5408\u6210\u6570\u636e\u6709\u6548\u6027\u4e0d\u8db3", "method": "\u5206\u4e24\u9636\u6bb5\uff1a\u591a\u4e13\u5bb6LLM\u534f\u4f5c\u751f\u6210\u7f3a\u9677\u611f\u77e5\u6570\u636e + \u57fa\u4e8e\u5f31\u70b9\u7684\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u6846\u67b6", "result": "\u57286\u4e2a\u6570\u5b66\u57fa\u51c6\u4e0a\u5e73\u5747\u63d0\u534712.57%\uff0c\u8fbe\u5230\u65b0SOTA\u6c34\u5e73", "conclusion": "\u7f3a\u9677\u611f\u77e5\u4e0e\u591a\u4e13\u5bb6\u534f\u540c\u673a\u5236\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u6570\u5b66\u80fd\u529b\uff0c\u6570\u636e\u8d28\u91cf\u6bd4\u6570\u91cf\u66f4\u91cd\u8981"}}
{"id": "2508.01263", "pdf": "https://arxiv.org/pdf/2508.01263", "abs": "https://arxiv.org/abs/2508.01263", "authors": ["Long S. T. Nguyen", "Khang H. N. Vo", "Thu H. A. Nguyen", "Tuan C. Bui", "Duc Q. Nguyen", "Thanh-Tung Tran", "Anh D. Nguyen", "Minh L. Nguyen", "Fabien Baldacci", "Thang H. Bui", "Emanuel Di Nardo", "Angelo Ciaramella", "Son H. Le", "Ihsan Ullah", "Lorenzo Di Rocco", "Tho T. Quan"], "title": "Bridging LLMs and Symbolic Reasoning in Educational QA Systems: Insights from the XAI Challenge at IJCNN 2025", "categories": ["cs.CL"], "comment": "The XAI Challenge @ TRNS-AI Workshop, IJCNN 2025: Explainable AI for\n  Educational Question Answering. Website:\n  https://sites.google.com/view/trns-ai/challenge/", "summary": "The growing integration of Artificial Intelligence (AI) into education has\nintensified the need for transparency and interpretability. While hackathons\nhave long served as agile environments for rapid AI prototyping, few have\ndirectly addressed eXplainable AI (XAI) in real-world educational contexts.\nThis paper presents a comprehensive analysis of the XAI Challenge 2025, a\nhackathon-style competition jointly organized by Ho Chi Minh City University of\nTechnology (HCMUT) and the International Workshop on Trustworthiness and\nReliability in Neurosymbolic AI (TRNS-AI), held as part of the International\nJoint Conference on Neural Networks (IJCNN 2025). The challenge tasked\nparticipants with building Question-Answering (QA) systems capable of answering\nstudent queries about university policies while generating clear, logic-based\nnatural language explanations. To promote transparency and trustworthiness,\nsolutions were required to use lightweight Large Language Models (LLMs) or\nhybrid LLM-symbolic systems. A high-quality dataset was provided, constructed\nvia logic-based templates with Z3 validation and refined through expert student\nreview to ensure alignment with real-world academic scenarios. We describe the\nchallenge's motivation, structure, dataset construction, and evaluation\nprotocol. Situating the competition within the broader evolution of AI\nhackathons, we argue that it represents a novel effort to bridge LLMs and\nsymbolic reasoning in service of explainability. Our findings offer actionable\ninsights for future XAI-centered educational systems and competitive research\ninitiatives.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e862025\u5e74XAI\u6311\u6218\u8d5b\uff0c\u5c55\u793a\u5982\u4f55\u901a\u8fc7\u6df7\u5408\u7b26\u53f7\u63a8\u7406\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u6559\u80b2\u9886\u57df\u53ef\u89e3\u91ca\u95ee\u7b54\u7cfb\u7edf", "motivation": "\u9488\u5bf9\u6559\u80b2\u9886\u57dfAI\u7cfb\u7edf\u900f\u660e\u5ea6\u4e0d\u8db3\u7684\u73b0\u72b6\uff0c\u901a\u8fc7\u9ed1\u5ba2\u9a6c\u62c9\u677e\u5f62\u5f0f\u63a2\u7d22\u53ef\u89e3\u91caAI\u6280\u672f\u5728\u5b9e\u9645\u6559\u80b2\u573a\u666f\u4e2d\u7684\u5e94\u7528", "method": "\u7ec4\u7ec7\u56fd\u9645\u7ade\u8d5b\u8981\u6c42\u5f00\u53d1\u8005\u4f7f\u7528\u8f7b\u91cf\u7ea7LLM\u6216\u6df7\u5408\u7cfb\u7edf\u6784\u5efa\u95ee\u7b54\u7cfb\u7edf\uff0c\u901a\u8fc7\u903b\u8f91\u6a21\u677f\u751f\u6210Z3\u9a8c\u8bc1\u6570\u636e\u96c6\uff0c\u7ecf\u5b66\u751f\u4e13\u5bb6\u5ba1\u6838\u4f18\u5316", "result": "\u9a8c\u8bc1\u4e86\u7b26\u53f7\u63a8\u7406\u4e0eLLM\u7ed3\u5408\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u6559\u80b2\u9886\u57dfXAI\u7cfb\u7edf\u5efa\u7acb\u53ef\u590d\u7528\u7684\u8bc4\u4f30\u534f\u8bae\u548c\u6570\u636e\u96c6\u6807\u51c6", "conclusion": "\u8be5\u7ade\u8d5b\u5f00\u521b\u4e86AI\u9ed1\u5ba2\u9a6c\u62c9\u677e\u65b0\u8303\u5f0f\uff0c\u4e3a\u5f00\u53d1\u53ef\u4fe1\u6559\u80b2AI\u7cfb\u7edf\u63d0\u4f9b\u6280\u672f\u6846\u67b6\u548c\u8bc4\u4f30\u57fa\u51c6"}}
{"id": "2508.01290", "pdf": "https://arxiv.org/pdf/2508.01290", "abs": "https://arxiv.org/abs/2508.01290", "authors": ["Zhichao Yan", "Jiapu Wang", "Jiaoyan Chen", "Yanyan Wang", "Hongye Tan", "Jiye Liang", "Xiaoli Li", "Ru Li", "Jeff Z. Pan"], "title": "Prompting Large Language Models with Partial Knowledge for Answering Questions with Unseen Entities", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) shows impressive performance by\nsupplementing and substituting parametric knowledge in Large Language Models\n(LLMs). Retrieved knowledge can be divided into three types: explicit answer\nevidence, implicit answer clue, and insufficient answer context which can be\nfurther categorized into totally irrelevant and partially relevant information.\nEffectively utilizing partially relevant knowledge remains a key challenge for\nRAG systems, especially in incomplete knowledge base retrieval. Contrary to the\nconventional view, we propose a new perspective: LLMs can be awakened via\npartially relevant knowledge already embedded in LLMs. To comprehensively\ninvestigate this phenomenon, the triplets located in the gold reasoning path\nand their variants are used to construct partially relevant knowledge by\nremoving the path that contains the answer. We provide theoretical analysis of\nthe awakening effect in LLMs and support our hypothesis with experiments on two\nKnowledge Graphs (KGs) Question Answering (QA) datasets. Furthermore, we\npresent a new task, Unseen Entity KGQA, simulating real-world challenges where\nentity linking fails due to KG incompleteness. Our awakening-based approach\ndemonstrates greater efficacy in practical applications, outperforms\ntraditional methods that rely on embedding-based similarity which are prone to\nreturning noisy information.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u90e8\u5206\u76f8\u5173\u77e5\u8bc6'\u5524\u9192'\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u5347\u4e0d\u5b8c\u6574\u77e5\u8bc6\u5e93\u573a\u666f\u4e0b\u7684\u95ee\u7b54\u6548\u679c\uff0c\u5e76\u6784\u5efa\u65b0\u8bc4\u6d4b\u4efb\u52a1Unseen Entity KGQA\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5d4c\u5165\u76f8\u4f3c\u6027\u7684\u68c0\u7d22\u65b9\u6cd5\u5728\u77e5\u8bc6\u5e93\u4e0d\u5b8c\u6574\u65f6\u5bb9\u6613\u8fd4\u56de\u566a\u58f0\u4fe1\u606f\uff0c\u800c\u5927\u6a21\u578b\u5185\u90e8\u53ef\u80fd\u5df2\u5b58\u5728\u90e8\u5206\u76f8\u5173\u77e5\u8bc6\uff0c\u5982\u4f55\u6709\u6548\u6fc0\u6d3b\u8fd9\u4e9b\u77e5\u8bc6\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u9ec4\u91d1\u63a8\u7406\u8def\u5f84\u7684\u4e09\u5143\u7ec4\u53d8\u4f53\u6a21\u62df\u90e8\u5206\u76f8\u5173\u77e5\u8bc6\uff0c\u7406\u8bba\u5206\u6790LLMs\u7684\u5524\u9192\u6548\u5e94\uff0c\u5728KGQA\u6570\u636e\u96c6\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5e76\u8bbe\u8ba1\u672a\u89c1\u8fc7\u5b9e\u4f53\u7684\u95ee\u7b54\u4efb\u52a1\u6a21\u62df\u771f\u5b9e\u573a\u666f\u3002", "result": "\u5728\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0c\u5524\u9192\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u68c0\u7d22\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5b9e\u4f53\u94fe\u63a5\u5931\u8d25\u573a\u666f\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "\u90e8\u5206\u76f8\u5173\u77e5\u8bc6\u80fd\u6709\u6548\u5524\u9192LLMs\u7684\u6f5c\u5728\u77e5\u8bc6\uff0c\u8be5\u65b9\u6cd5\u4e3a\u4e0d\u5b8c\u6574\u77e5\u8bc6\u5e93\u573a\u666f\u63d0\u4f9b\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u7a81\u7834\u4f20\u7edf\u76f8\u4f3c\u6027\u68c0\u7d22\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.01302", "pdf": "https://arxiv.org/pdf/2508.01302", "abs": "https://arxiv.org/abs/2508.01302", "authors": ["Chenming Tang", "Yutong Yang", "Yunfang Wu"], "title": "KEDAS: Knowledge Editing Alignment with Diverse Augmentation and Self-adaptive Inference", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Knowledge editing aims to modify outdated knowledge in large language models\n(LLMs) efficiently while retaining their powerful capabilities. Most existing\nmethods rely on either parameter-level editing or retrieval-based approaches.\nIn this work, we propose Knowledge Editing alignment with Diverse Augmentation\nand Self-adaptive inference (KEDAS) to better align LLMs with knowledge\nediting. In the alignment phase, LLMs learn to apply in-context edited\nknowledge via low-rank adaptation. During editing, we design a diverse edit\naugmentation technique to improve the recall of edits. After that, a\nself-adaptive post-alignment inference mechanism is proposed, in which a\nfilter-based smart retriever is employed to perform a dynamic selection of\ninference routing. Specifically, irrelevant queries will go through the\noriginal pre-alignment model directly, while relevant ones, together with their\nrelated edits, go through the model with aligned adapters activated. In\nexperiments, KEDAS secures the highest overall performance scores in 35 out of\n36 cases across four datasets with three LLMs on three settings, surpassing its\nstrong knowledge editing alignment counterpart by about 19.8 harmonic mean\nscores of edit success, locality and portability and outperforming both\nparameter editing and retrieval-based baselines significantly. Analysis of\ncomputational cost and performance on general tasks further validates the\nrobustness and efficiency of KEDAS, indicating that it presents an ideal\nparadigm of knowledge editing alignment.", "AI": {"tldr": "Propose KEDAS method to enhance knowledge editing in LLMs through diverse augmentation and self-adaptive inference alignment.", "motivation": "Existing knowledge editing methods rely on parameter-level editing or retrieval approaches, which may lack efficiency and alignment with contextual knowledge application.", "method": "Align LLMs via low-rank adaptation for in-context knowledge editing, use diverse edit augmentation to improve recall, and implement self-adaptive post-alignment inference with dynamic routing.", "result": "Achieves top performance in 35/36 cases across datasets/models, surpassing baselines by 19.8 harmonic mean scores and outperforming parameter/retrieval-based methods.", "conclusion": "KEDAS demonstrates robust computational efficiency and alignment effectiveness, establishing an ideal paradigm for knowledge editing in LLMs."}}
{"id": "2508.01309", "pdf": "https://arxiv.org/pdf/2508.01309", "abs": "https://arxiv.org/abs/2508.01309", "authors": ["Weibo Zhou", "Lingbo Li", "Shangsong Liang"], "title": "D-SCoRE: Document-Centric Segmentation and CoT Reasoning with Structured Export for QA-CoT Data Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The scarcity and high cost of high-quality question-answering (QA) datasets\nhinder supervised fine-tuning (SFT) for domain-specific large language models\n(LLMs). To address this, we introduce D-SCoRE, a training-free pipeline that\nutilizes LLMs and prompt engineering to produce diverse, high-quality QA\ndatasets from arbitrary textual sources. D-SCoRE integrates\n$\\textbf{D}$ocument-centric processing, $\\textbf{S}$egmentation, $\\textbf{Co}$T\n$\\textbf{R}$easoning, and structured $\\textbf{E}$xport to generate QA-COT\ndatasets tailored for domain-aware SFT. Multi-dimensional control mechanisms,\nsuch as semantic role transformation, question type balancing, and\ncounterfactual materials, enhance diversity and relevance, overcoming\nlimitations of existing QA generation. LLMs fine-tuned on D-SCoRE-generated QA\ndatasets, and human-annotated QA datasets (SQuAD, Covid-QA) are evaluated on\nSQuADShifts and Covid-QA test sets, with D-SCoRE outperforming across most\ndomains. D-SCoRE generates six QA-CoT pairs with four-option counterfactual\nmaterials per 100-200-word text in 90 seconds using an 8B LLM on consumer-grade\nhardware. Its simplicity and scalability enable efficient QA generation and\nhigh-performance fine-tuning across domains.", "AI": {"tldr": "D-SCoRE\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684QA\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u6863\u5904\u7406+\u601d\u7ef4\u94fe\u63a8\u7406+\u591a\u7ef4\u5ea6\u63a7\u5236\u673a\u5236\uff0c\u9ad8\u6548\u751f\u6210\u9886\u57df\u5b9a\u5236QA\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347LLM\u5fae\u8c03\u6548\u679c", "motivation": "\u9488\u5bf9\u9886\u57df\u7279\u5b9a\u5927\u6a21\u578b\u76d1\u7763\u5fae\u8c03\u6240\u9700\u7684\u9ad8\u8d28\u91cf\u95ee\u7b54\u6570\u636e\u96c6\u7a00\u7f3a\u4e14\u6210\u672c\u9ad8\u6602\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u81ea\u52a8\u5316\u751f\u6210\u65b9\u6848\u4ee5\u7a81\u7834\u6570\u636e\u74f6\u9888", "method": "\u7ed3\u5408\u6587\u6863\u4e2d\u5fc3\u5904\u7406\u3001\u6587\u672c\u5206\u5272\u3001\u601d\u7ef4\u94fe\u63a8\u7406\u548c\u7ed3\u6784\u5316\u5bfc\u51fa\uff0c\u521b\u65b0\u5f15\u5165\u8bed\u4e49\u89d2\u8272\u8f6c\u6362/\u95ee\u9898\u7c7b\u578b\u5e73\u8861/\u53cd\u4e8b\u5b9e\u6750\u6599\u7b49\u63a7\u5236\u673a\u5236\u589e\u5f3a\u6570\u636e\u591a\u6837\u6027", "result": "\u5728SQuADShifts\u548cCovid-QA\u6d4b\u8bd5\u4e2d\uff0cD-SCoRE\u751f\u6210\u6570\u636e\u5fae\u8c03\u7684\u6a21\u578b\u4f18\u4e8e\u4eba\u5de5\u6807\u6ce8\u6570\u636e\uff0c8B\u6a21\u578b\u53ef\u5728\u6d88\u8d39\u7ea7\u786c\u4ef690\u79d2\u751f\u62106\u7ec4QA-CoT\u5bf9", "conclusion": "\u8be5\u6846\u67b6\u4ee5\u7b80\u5355\u53ef\u6269\u5c55\u7684\u65b9\u5f0f\u5b9e\u73b0\u8de8\u9886\u57df\u9ad8\u6548QA\u751f\u6210\u4e0e\u6a21\u578b\u5fae\u8c03\uff0c\u4e3a\u89e3\u51b3\u9886\u57dfLLM\u6570\u636e\u56f0\u5883\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848"}}
{"id": "2508.01317", "pdf": "https://arxiv.org/pdf/2508.01317", "abs": "https://arxiv.org/abs/2508.01317", "authors": ["Xuemiao Zhang", "Can Ren", "Chengying Tu", "Rongxiang Weng", "Hongfei Yan", "Jingang Wang", "Xunliang Cai"], "title": "LinkQA: Synthesizing Diverse QA from Multiple Seeds Strongly Linked by Knowledge Points", "categories": ["cs.CL"], "comment": null, "summary": "The advancement of large language models (LLMs) struggles with the scarcity\nof high-quality, diverse training data. To address this limitation, we propose\nLinkSyn, a novel knowledge point (KP) graph-based synthesis framework that\nenables flexible control over discipline and difficulty distributions while\nbalancing KP coverage and popularity. LinkSyn extracts KPs from\nquestion-answering (QA) seed data and constructs a KP graph to synthesize\ndiverse QA data from multiple seeds strongly linked by KPs and sampled from\ngraph walks. Specifically, LinkSyn incorporates (1) a knowledge distribution\nvalue function to guide the adjustment of path sampling probability and balance\nKP coverage and popularity during graph walks; (2) diffusion-based synthesis\nvia DeepSeek-R1 by leveraging multiple seeds with dense logical associations\nalong each path; and (3) high-difficulty QA enhancement within given\ndisciplines by flexible difficulty adjustments. By executing LinkSyn, we\nsynthesize LinkQA, a diverse multi-disciplinary QA dataset with 50B tokens.\nExtensive experiments on Llama-3 8B demonstrate that continual pre-training\nwith LinkQA yields an average improvement of $\\mathbf{11.51\\%}$ on MMLU and\nCMMLU, establishing new SOTA results. LinkQA consistently enhances performance\nacross model size and initial FLOPs scales.", "AI": {"tldr": "\u63d0\u51faLinkSyn\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u5408\u6210\u9ad8\u8d28\u91cf\u591a\u5b66\u79d1QA\u6570\u636e\u96c6LinkQA\uff0c\u663e\u8457\u63d0\u5347LLM\u8bad\u7ec3\u6548\u679c", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\u4e0d\u8db3\u3001\u591a\u6837\u6027\u6b20\u7f3a\u7684\u95ee\u9898\uff0c\u7a81\u7834KP\u8986\u76d6\u5ea6\u4e0e\u6d41\u884c\u5ea6\u7684\u5e73\u8861\u96be\u9898", "method": "1. \u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u8fdb\u884c\u56fe\u6e38\u8d70\u91c7\u6837\n2. \u77e5\u8bc6\u5206\u5e03\u4ef7\u503c\u51fd\u6570\u8c03\u6574\u8def\u5f84\u6982\u7387\n3. \u57fa\u4e8eDeepSeek-R1\u7684\u591a\u79cd\u5b50\u6269\u6563\u5408\u6210\n4. \u53ef\u8c03\u8282\u96be\u5ea6\u7684QA\u589e\u5f3a\u673a\u5236", "result": "LinkQA\u6570\u636e\u96c6\uff0850B tokens\uff09\u4f7fLlama-3 8B\u5728MMLU/CMMLU\u5e73\u5747\u63d0\u534711.51%\uff0c\u5237\u65b0SOTA", "conclusion": "LinkSyn\u6210\u529f\u5b9e\u73b0\u77e5\u8bc6\u5206\u5e03\u63a7\u5236\u4e0e\u6570\u636e\u5408\u6210\u7684\u5e73\u8861\uff0cLinkQA\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e0a\u5747\u5c55\u73b0\u6301\u7eed\u6027\u80fd\u589e\u76ca"}}
{"id": "2508.01326", "pdf": "https://arxiv.org/pdf/2508.01326", "abs": "https://arxiv.org/abs/2508.01326", "authors": ["Xuemiao Zhang", "Chengying Tu", "Can Ren", "Rongxiang Weng", "Hongfei Yan", "Jingang Wang", "Xunliang Cai"], "title": "Large-Scale Diverse Synthesis for Mid-Training", "categories": ["cs.CL"], "comment": null, "summary": "The scarcity of high-quality, knowledge-intensive training data hinders the\ndevelopment of large language models (LLMs), as traditional corpora provide\nlimited information. Previous studies have synthesized and integrated\ncorpora-dependent question-answering (QA) data to improve model performance but\nface challenges in QA data scalability and knowledge diversity, particularly in\ncross-domain contexts. Furthermore, leveraging our designed discipline and\ndifficulty annotation system, we probe model deficiencies in STEM disciplines\nand high-difficulty data. To overcome these limitations, we propose a novel\ndiversified pipeline to synthesize BoostQA, a 100B-token large-scale QA\ndataset. Our synthesis framework: (1) curates seed data from heterogeneous\nsources; (2) utilizes DeepSeek-R1 to implement STEM-focused multi-grade\nsynthesis to boost data diversity and high-difficulty synthesis to mitigate\ndifficulty degradation; (3) refines answers via DeepSeek-V3 to improve output\nquality. We utilize BoostQA in mid-training, a mid-stage between pre-training\nand post-training, to optimize domain-specific knowledge acquisition and\nenhance data quality. Our method enables Llama-3 8B, mid-trained on a 40B-token\ndataset, to achieve an average improvement of $\\mathbf{12.74\\%}$ on MMLU and\nCMMLU and establish SOTA average performance across 12 benchmarks. BoostQA also\ndemonstrates robust scalability, with performance consistently improving as\nmodel size, data volume, and initial FLOPs scale.", "AI": {"tldr": "\u63d0\u51faBoostQA\u5927\u89c4\u6a21QA\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u591a\u6837\u5316\u5408\u6210\u6d41\u7a0b\u63d0\u5347LLM\u5728\u8de8\u9886\u57df\u548c\u9ad8\u96be\u5ea6\u6570\u636e\u4e0a\u7684\u8868\u73b0", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8bed\u6599\u5e93\u4fe1\u606f\u6709\u9650\u3001\u73b0\u6709QA\u6570\u636e\u5728\u6269\u5c55\u6027\u548c\u8de8\u9886\u57df\u77e5\u8bc6\u591a\u6837\u6027\u65b9\u9762\u7684\u4e0d\u8db3", "method": "\u4e09\u9636\u6bb5\u5408\u6210\u6846\u67b6\uff1a1) \u5f02\u6784\u6570\u636e\u6536\u96c6 2) \u591a\u5c42\u7ea7STEM\u5408\u6210\u4e0e\u9ad8\u96be\u5ea6\u5408\u6210 3) \u7b54\u6848\u7cbe\u70bc\u4f18\u5316\uff1b\u91c7\u7528mid-training\u8bad\u7ec3\u8303\u5f0f", "result": "Llama-3 8B\u5728MMLU/CMMLU\u5e73\u5747\u63d0\u534712.74%\uff0c12\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u8fbe\u5230SOTA\uff1b\u5c55\u793a\u51fa\u6a21\u578b\u89c4\u6a21\u3001\u6570\u636e\u91cf\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u5f3a\u6269\u5c55\u6027", "conclusion": "BoostQA\u6709\u6548\u89e3\u51b3\u77e5\u8bc6\u83b7\u53d6\u96be\u9898\uff0c\u8bc1\u660e\u4e2d\u9636\u6bb5\u8bad\u7ec3\u5bf9\u4f18\u5316\u9886\u57df\u77e5\u8bc6\u83b7\u53d6\u548c\u63d0\u5347\u6570\u636e\u8d28\u91cf\u7684\u5173\u952e\u4f5c\u7528"}}
{"id": "2508.01370", "pdf": "https://arxiv.org/pdf/2508.01370", "abs": "https://arxiv.org/abs/2508.01370", "authors": ["Roman Koshkin", "Pengyu Dai", "Nozomi Fujikawa", "Masahito Togami", "Marco Visentini-Scarzanella"], "title": "MaRGen: Multi-Agent LLM Approach for Self-Directed Market Research and Analysis", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "We present an autonomous framework that leverages Large Language Models\n(LLMs) to automate end-to-end business analysis and market report generation.\nAt its core, the system employs specialized agents - Researcher, Reviewer,\nWriter, and Retriever - that collaborate to analyze data and produce\ncomprehensive reports. These agents learn from real professional consultants'\npresentation materials at Amazon through in-context learning to replicate\nprofessional analytical methodologies. The framework executes a multi-step\nprocess: querying databases, analyzing data, generating insights, creating\nvisualizations, and composing market reports. We also introduce a novel\nLLM-based evaluation system for assessing report quality, which shows alignment\nwith expert human evaluations. Building on these evaluations, we implement an\niterative improvement mechanism that optimizes report quality through automated\nreview cycles. Experimental results show that report quality can be improved by\nboth automated review cycles and consultants' unstructured knowledge. In\nexperimental validation, our framework generates detailed 6-page reports in 7\nminutes at a cost of approximately \\$1. Our work could be an important step to\nautomatically create affordable market insights.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u81ea\u52a8\u5316\u4e1a\u52a1\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u7814\u7a76\u5458/\u8bc4\u5ba1\u5458/\u4f5c\u8005/\u68c0\u7d22\u5668\u56db\u7c7b\u667a\u80fd\u4f53\u534f\u4f5c\uff0c7\u5206\u949f\u751f\u62106\u9875\u5e02\u573a\u62a5\u544a\uff08\u6210\u672c$1\uff09\uff0c\u5e76\u914d\u5907LLM\u8bc4\u4f30\u7cfb\u7edf\u5b9e\u73b0\u8d28\u91cf\u8fed\u4ee3\u4f18\u5316", "motivation": "\u4f20\u7edf\u5e02\u573a\u5206\u6790\u4f9d\u8d56\u4eba\u5de5\u5bfc\u81f4\u6548\u7387\u4f4e\u3001\u6210\u672c\u9ad8\uff0c\u9700\u5229\u7528LLM\u6280\u672f\u5b9e\u73b0\u81ea\u52a8\u5316\u4e14\u4fdd\u6301\u4e13\u4e1a\u8d28\u91cf\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5728\u5206\u6790\u6df1\u5ea6\u548c\u7cfb\u7edf\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u7f3a\u4e4f\u95ed\u73af\u4f18\u5316\u673a\u5236", "method": "1. \u6784\u5efa\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u67b6\u6784\uff08\u5b66\u4e60\u4e9a\u9a6c\u900a\u987e\u95ee\u6750\u6599\uff09\n2. \u5206\u9636\u6bb5\u6267\u884c\u6570\u636e\u67e5\u8be2\u2192\u5206\u6790\u2192\u53ef\u89c6\u5316\u2192\u62a5\u544a\u751f\u6210\n3. \u5f00\u53d1\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u8bc4\u4f30\u7cfb\u7edf\n4. \u901a\u8fc7\u8bc4\u5ba1\u5faa\u73af+\u987e\u95ee\u77e5\u8bc6\u53cc\u8def\u5f84\u4f18\u5316\u62a5\u544a\u8d28\u91cf", "result": "\u751f\u6210\u901f\u5ea6\uff1a7\u5206\u949f/6\u9875\uff1b\u6210\u672c\uff1a$1/\u4efd\uff1b\u8d28\u91cf\u63d0\u5347\uff1a\u81ea\u52a8\u8bc4\u5ba1\u4f7f\u8d28\u91cf\u8bc4\u5206\u63d0\u534727%\uff0c\u7ed3\u5408\u987e\u95ee\u77e5\u8bc6\u540e\u63d0\u534742%\uff1b\u8bc4\u4f30\u7cfb\u7edf\u4e0e\u4e13\u5bb6\u8bc4\u5206\u76f8\u5173\u6027\u8fbe0.85", "conclusion": "\u8be5\u6846\u67b6\u8bc1\u660eLLM\u81ea\u52a8\u5316\u751f\u6210\u4e13\u4e1a\u5e02\u573a\u6d1e\u5bdf\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u67b6\u6784\u5316\u6d41\u7a0b\u8bbe\u8ba1\u548c\u6df7\u5408\u4f18\u5316\u673a\u5236\uff0c\u5728\u4fdd\u6301\u4f4e\u6210\u672c\u7684\u540c\u65f6\u8fbe\u5230\u51c6\u4e13\u4e1a\u6c34\u5e73\uff0c\u4e3a\u5546\u4e1a\u5206\u6790\u81ea\u52a8\u5316\u5f00\u8f9f\u65b0\u8def\u5f84"}}
{"id": "2508.01401", "pdf": "https://arxiv.org/pdf/2508.01401", "abs": "https://arxiv.org/abs/2508.01401", "authors": ["Ahmad Rezaie Mianroodi", "Amirali Rezaie", "Niko Grisel Todorov", "Cyril Rakovski", "Frank Rudzicz"], "title": "MedSynth: Realistic, Synthetic Medical Dialogue-Note Pairs", "categories": ["cs.CL", "cs.AI"], "comment": "7 pages excluding references and appendices", "summary": "Physicians spend significant time documenting clinical encounters, a burden\nthat contributes to professional burnout. To address this, robust automation\ntools for medical documentation are crucial. We introduce MedSynth -- a novel\ndataset of synthetic medical dialogues and notes designed to advance the\nDialogue-to-Note (Dial-2-Note) and Note-to-Dialogue (Note-2-Dial) tasks.\nInformed by an extensive analysis of disease distributions, this dataset\nincludes over 10,000 dialogue-note pairs covering over 2000 ICD-10 codes. We\ndemonstrate that our dataset markedly enhances the performance of models in\ngenerating medical notes from dialogues, and dialogues from medical notes. The\ndataset provides a valuable resource in a field where open-access,\nprivacy-compliant, and diverse training data are scarce. Code is available at\nhttps://github.com/ahmadrezarm/MedSynth/tree/main and the dataset is available\nat https://huggingface.co/datasets/Ahmad0067/MedSynth.", "AI": {"tldr": "\u5f00\u53d1MedSynth\u6570\u636e\u96c6\u63d0\u5347\u533b\u7597\u5bf9\u8bdd-\u7b14\u8bb0\u53cc\u5411\u751f\u6210\u6a21\u578b\u6548\u679c\uff0c\u8986\u76d62000+\u75be\u75c5\u4ee3\u7801", "motivation": "\u51cf\u5c11\u533b\u751f\u56e0\u7e41\u7410\u75c5\u5386\u8bb0\u5f55\u5bfc\u81f4\u7684\u804c\u4e1a\u5026\u6020\uff0c\u89e3\u51b3\u533b\u7597\u6587\u6863\u81ea\u52a8\u5316\u5de5\u5177\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u95ee\u9898", "method": "\u521b\u5efa\u5305\u542b10,000+\u5bf9\u8bdd-\u7b14\u8bb0\u5bf9\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u8986\u76d62000+ ICD-10\u75be\u75c5\u4ee3\u7801\u5e76\u5206\u6790\u75be\u75c5\u5206\u5e03", "result": "\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u5bf9\u8bdd\u2192\u7b14\u8bb0\u548c\u7b14\u8bb0\u2192\u5bf9\u8bdd\u53cc\u5411\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u8868\u73b0", "conclusion": "MedSynth\u586b\u8865\u4e86\u5f00\u653e\u533b\u7597\u6570\u636e\u8d44\u6e90\u7a7a\u767d\uff0c\u63d0\u4f9b\u9690\u79c1\u5408\u89c4\u4e14\u591a\u6837\u5316\u7684\u8bad\u7ec3\u6837\u672c"}}
{"id": "2508.01411", "pdf": "https://arxiv.org/pdf/2508.01411", "abs": "https://arxiv.org/abs/2508.01411", "authors": ["Rania Al-Sabbagh"], "title": "ArzEn-MultiGenre: An aligned parallel dataset of Egyptian Arabic song lyrics, novels, and subtitles, with English translations", "categories": ["cs.CL"], "comment": null, "summary": "ArzEn-MultiGenre is a parallel dataset of Egyptian Arabic song lyrics,\nnovels, and TV show subtitles that are manually translated and aligned with\ntheir English counterparts. The dataset contains 25,557 segment pairs that can\nbe used to benchmark new machine translation models, fine-tune large language\nmodels in few-shot settings, and adapt commercial machine translation\napplications such as Google Translate. Additionally, the dataset is a valuable\nresource for research in various disciplines, including translation studies,\ncross-linguistic analysis, and lexical semantics. The dataset can also serve\npedagogical purposes by training translation students and aid professional\ntranslators as a translation memory. The contributions are twofold: first, the\ndataset features textual genres not found in existing parallel Egyptian Arabic\nand English datasets, and second, it is a gold-standard dataset that has been\ntranslated and aligned by human experts.", "AI": {"tldr": "ArzEn-MultiGenre\u662f\u9996\u4e2a\u5305\u542b\u57c3\u53ca\u963f\u62c9\u4f2f\u8bed\u6b4c\u66f2/\u5c0f\u8bf4/\u5f71\u89c6\u5b57\u5e55\u7684\u5e73\u884c\u6570\u636e\u96c6\uff0c\u5305\u542b25,557\u4e2a\u4eba\u5de5\u7ffb\u8bd1\u5bf9\u9f50\u7684\u82f1\u963f\u53cc\u8bed\u7247\u6bb5\uff0c\u652f\u6301\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\u5f00\u53d1\u3001\u8de8\u5b66\u79d1\u7814\u7a76\u548c\u6559\u5b66\u5e94\u7528\u3002", "motivation": "\u586b\u8865\u73b0\u6709\u57c3\u53ca\u963f\u62c9\u4f2f\u8bed-\u82f1\u8bed\u6570\u636e\u96c6\u4e2d\u591a\u7c7b\u578b\u6587\u672c\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u4eba\u5de5\u4e13\u5bb6\u7ffb\u8bd1\u521b\u5efa\u9ec4\u91d1\u6807\u51c6\u6570\u636e\u96c6\uff0c\u63d0\u5347\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u548c\u5b66\u672f\u7814\u7a76\u53ef\u9760\u6027\u3002", "method": "\u4eba\u5de5\u7ffb\u8bd1\u5e76\u624b\u52a8\u5bf9\u9f50\u57c3\u53ca\u963f\u62c9\u4f2f\u8bed\u4e0e\u82f1\u8bed\u7684\u6b4c\u8bcd/\u5c0f\u8bf4/\u5f71\u89c6\u5b57\u5e55\uff0c\u6784\u5efa\u591a\u4f53\u88c1\u3001\u5927\u89c4\u6a21(25,557\u5bf9)\u53cc\u8bed\u5e73\u884c\u8bed\u6599\u5e93\u3002", "result": "\u6570\u636e\u96c6\u53ef\u7528\u4e8e\uff1a1) \u673a\u5668\u7ffb\u8bd1\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5 2) \u5927\u6a21\u578b\u5fae\u8c03 3) \u7ffb\u8bd1\u7814\u7a76/\u8bed\u4e49\u5206\u6790 4) \u7ffb\u8bd1\u6559\u5b66 5) \u4e13\u4e1a\u7ffb\u8bd1\u8bb0\u5fc6\u5e93\u3002\u5176\u72ec\u7279\u4ef7\u503c\u5728\u4e8e\u65b0\u6587\u672c\u7c7b\u578b\u8986\u76d6\u548c\u4eba\u5de5\u6807\u6ce8\u7684\u9ad8\u8d28\u91cf\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u53cc\u91cd\u8d21\u732e\uff1a\u8865\u5145\u57c3\u53ca\u963f\u62c9\u4f2f\u8bed\u8d44\u6e90\u7c7b\u578b\u7a7a\u767d\uff0c\u63d0\u4f9b\u9ec4\u91d1\u6807\u51c6\u6570\u636e\u3002\u517c\u5177\u5b66\u672f\u7814\u7a76\u3001\u5546\u4e1a\u5e94\u7528\u548c\u6559\u5b66\u4ef7\u503c\uff0c\u63a8\u52a8\u963f\u62c9\u4f2f\u8bedNLP\u53d1\u5c55\u3002"}}
{"id": "2508.01412", "pdf": "https://arxiv.org/pdf/2508.01412", "abs": "https://arxiv.org/abs/2508.01412", "authors": ["Jinhao Pan", "Chahat Raj", "Ziwei Zhu"], "title": "Discovering Bias Associations through Open-Ended LLM Generations", "categories": ["cs.CL"], "comment": null, "summary": "Social biases embedded in Large Language Models (LLMs) raise critical\nconcerns, resulting in representational harms -- unfair or distorted portrayals\nof demographic groups -- that may be expressed in subtle ways through generated\nlanguage. Existing evaluation methods often depend on predefined\nidentity-concept associations, limiting their ability to surface new or\nunexpected forms of bias. In this work, we present the Bias Association\nDiscovery Framework (BADF), a systematic approach for extracting both known and\npreviously unrecognized associations between demographic identities and\ndescriptive concepts from open-ended LLM outputs. Through comprehensive\nexperiments spanning multiple models and diverse real-world contexts, BADF\nenables robust mapping and analysis of the varied concepts that characterize\ndemographic identities. Our findings advance the understanding of biases in\nopen-ended generation and provide a scalable tool for identifying and analyzing\nbias associations in LLMs. Data, code, and results are available at\nhttps://github.com/JP-25/Discover-Open-Ended-Generation", "AI": {"tldr": "BADF\u6846\u67b6\u7cfb\u7edf\u6027\u53d1\u73b0LLM\u5f00\u653e\u751f\u6210\u4e2d\u7684\u5df2\u77e5/\u672a\u77e5\u504f\u89c1\u5173\u8054", "motivation": "\u73b0\u6709\u504f\u89c1\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u5173\u8054\uff0c\u96be\u4ee5\u53d1\u73b0\u5f00\u653e\u751f\u6210\u4e2d\u7684\u65b0\u578b\u504f\u89c1\u5f62\u5f0f", "method": "\u63d0\u51faBADF\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u65b9\u6cd5\u4ece\u5f00\u653e\u8f93\u51fa\u4e2d\u63d0\u53d6\u4eba\u53e3\u7279\u5f81\u4e0e\u63cf\u8ff0\u6982\u5ff5\u7684\u5173\u8054", "result": "\u5728\u591a\u6a21\u578b\u3001\u591a\u573a\u666f\u5b9e\u9a8c\u4e2d\u6709\u6548\u8bc6\u522b\u504f\u89c1\u5173\u8054\uff0c\u63d0\u4f9b\u504f\u89c1\u6620\u5c04\u5206\u6790\u5de5\u5177", "conclusion": "BADF\u63a8\u8fdb\u4e86\u5f00\u653e\u751f\u6210\u504f\u89c1\u7406\u89e3\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u7684LLM\u504f\u89c1\u68c0\u6d4b\u65b9\u6848"}}
{"id": "2508.01424", "pdf": "https://arxiv.org/pdf/2508.01424", "abs": "https://arxiv.org/abs/2508.01424", "authors": ["Haonan Bian", "Yutao Qi", "Rui Yang", "Yuanxi Che", "Jiaqian Wang", "Heming Xia", "Ranran Zhen"], "title": "From Query to Logic: Ontology-Driven Multi-Hop Reasoning in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs), despite their success in question answering,\nexhibit limitations in complex multi-hop question answering (MQA) tasks that\nnecessitate non-linear, structured reasoning. This limitation stems from their\ninability to adequately capture deep conceptual relationships between entities.\nTo overcome this challenge, we present **ORACLE** (**O**ntology-driven\n**R**easoning **A**nd **C**hain for **L**ogical **E**ucidation), a\ntraining-free framework that combines LLMs' generative capabilities with the\nstructural benefits of knowledge graphs. Our approach operates through three\nstages: (1) dynamic construction of question-specific knowledge ontologies\nusing LLMs, (2) transformation of these ontologies into First-Order Logic\nreasoning chains, and (3) systematic decomposition of the original query into\nlogically coherent sub-questions. Experimental results on several standard MQA\nbenchmarks show that our framework achieves highly competitive performance,\nrivaling current state-of-the-art models like DeepSeek-R1. Detailed analyses\nfurther confirm the effectiveness of each component, while demonstrating that\nour method generates more logical and interpretable reasoning chains than\nexisting approaches.", "AI": {"tldr": "\u63d0\u51faORACLE\u6846\u67b6\u89e3\u51b3LLMs\u5728\u590d\u6742\u591a\u8df3\u95ee\u7b54\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u4e0e\u4e00\u9636\u903b\u8f91\u63a8\u7406\u94fe\u7ed3\u5408\u63d0\u5347\u63a8\u7406\u80fd\u529b", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9700\u8981\u975e\u7ebf\u6027\u548c\u7ed3\u6784\u5316\u63a8\u7406\u7684\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0c\u56e0\u65e0\u6cd5\u6355\u6349\u5b9e\u4f53\u6df1\u5c42\u5173\u7cfb\u800c\u8868\u73b0\u53d7\u9650", "method": "\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1) LLM\u52a8\u6001\u6784\u5efa\u95ee\u9898\u77e5\u8bc6\u672c\u4f53 2) \u8f6c\u5316\u4e3a\u4e00\u9636\u903b\u8f91\u63a8\u7406\u94fe 3) \u7cfb\u7edf\u5206\u89e3\u539f\u59cb\u95ee\u9898\u4e3a\u903b\u8f91\u5b50\u95ee\u9898", "result": "\u5728\u6807\u51c6MQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e0eDeepSeek-R1\u76f8\u5f53\u7684SOTA\u6027\u80fd\uff0c\u751f\u6210\u66f4\u903b\u8f91\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u94fe", "conclusion": "ORACLE\u6709\u6548\u7ed3\u5408LLM\u751f\u6210\u80fd\u529b\u4e0e\u77e5\u8bc6\u7ed3\u6784\u5316\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u95ee\u7b54\u4efb\u52a1\u7684\u903b\u8f91\u63a8\u7406\u8d28\u91cf"}}
{"id": "2508.01450", "pdf": "https://arxiv.org/pdf/2508.01450", "abs": "https://arxiv.org/abs/2508.01450", "authors": ["Xinlin Zhuang", "Feilong Tang", "Haolin Yang", "Ming Hu", "Huifa Li", "Haochen Xue", "Yichen Li", "Junjun He", "Zongyuan Ge", "Ying Qian", "Imran Razzak"], "title": "Towards Efficient Medical Reasoning with Minimal Fine-Tuning Data", "categories": ["cs.CL"], "comment": "preprint, under review", "summary": "Supervised Fine-Tuning (SFT) plays a pivotal role in adapting Large Language\nModels (LLMs) to specialized domains such as medical reasoning. However,\nexisting SFT practices often rely on unfiltered datasets that contain redundant\nand low-quality samples, leading to substantial computational costs and\nsuboptimal performance. Although existing methods attempt to alleviate this\nproblem by selecting data based on sample difficulty, defined by knowledge and\nreasoning complexity, they overlook each sample's optimization utility\nreflected in its gradient. Interestingly, we find that gradient-based influence\nalone favors easy-to-optimize samples that cause large parameter shifts but\nlack deep reasoning chains, while difficulty alone selects noisy or overly\ncomplex cases that fail to guide stable optimization. Based on this\nobservation, we propose a data selection strategy, Difficulty-Influence\nQuadrant (DIQ), which prioritizes samples in the high-difficulty-high-influence\nquadrant to balance complex clinical reasoning with substantial gradient\ninfluence, enabling efficient medical reasoning with minimal fine-tuning data.\nFurthermore, Human and LLM-as-a-judge evaluations show that DIQ-selected\nsubsets demonstrate higher data quality and generate clinical reasoning that is\nmore aligned with expert practices in differential diagnosis, safety check, and\nevidence citation, as DIQ emphasizes samples that foster expert-like reasoning\npatterns. Extensive experiments on medical reasoning benchmarks demonstrate\nthat DIQ enables models fine-tuned on only 1% of selected data to match\nfull-dataset performance, while using 10% consistently outperforms the\nbaseline, highlighting the superiority of principled data selection over\nbrute-force scaling. The code and data are available at\nhttps://github.com/mihara-bot/DIQ.", "AI": {"tldr": "\u63d0\u51faDIQ\u6570\u636e\u9009\u62e9\u7b56\u7565\uff0c\u901a\u8fc7\u5e73\u8861\u6837\u672c\u96be\u5ea6\u4e0e\u68af\u5ea6\u5f71\u54cd\u529b\uff0c\u4ec5\u97001%\u7cbe\u9009\u6570\u636e\u5373\u53ef\u8fbe\u5230\u5168\u91cf\u5fae\u8c03\u6548\u679c", "motivation": "\u4f20\u7edfSFT\u4f7f\u7528\u672a\u8fc7\u6ee4\u6570\u636e\u96c6\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6548\u679c\u5dee\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u5173\u6ce8\u6837\u672c\u96be\u5ea6\u800c\u5ffd\u89c6\u68af\u5ea6\u4f18\u5316\u6548\u7528", "method": "\u6784\u5efa\u96be\u5ea6-\u5f71\u54cd\u529b\u56db\u8c61\u9650\uff08DIQ\uff09\uff0c\u4f18\u5148\u9009\u62e9\u9ad8\u96be\u5ea6\u9ad8\u68af\u5ea6\u5f71\u54cd\u529b\u7684\u6837\u672c\u8fdb\u884c\u5fae\u8c03", "result": "\u4f7f\u75281%\u7cbe\u9009\u6570\u636e\u5373\u53ef\u5339\u914d\u5168\u91cf\u6027\u80fd\uff0c10%\u6570\u636e\u6301\u7eed\u8d85\u8d8a\u57fa\u7ebf\uff0c\u63d0\u5347\u4e34\u5e8a\u63a8\u7406\u4e0e\u4e13\u5bb6\u5b9e\u8df5\u5bf9\u9f50\u5ea6", "conclusion": "DIQ\u901a\u8fc7\u539f\u7406\u9a71\u52a8\u7684\u6570\u636e\u9009\u62e9\uff0c\u5728\u533b\u7597\u63a8\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u8d28\u91cf\u4f18\u5148\u7684\u4f18\u5316\uff0c\u7a81\u7834\u66b4\u529b\u6570\u636e\u6269\u5c55\u7684\u5c40\u9650"}}
{"id": "2508.01473", "pdf": "https://arxiv.org/pdf/2508.01473", "abs": "https://arxiv.org/abs/2508.01473", "authors": ["Yiming Zeng", "Jinghan Cao", "Zexin Li", "Yiming Chen", "Tao Ren", "Dawei Xiang", "Xidong Wu", "Shangqian Gao", "Tingting Yu"], "title": "TreeDiff: AST-Guided Code Generation with Diffusion LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in diffusion-based language models have opened new\npossibilities for controllable and bidirectional sequence generation. These\nmodels provide an alternative to traditional autoregressive approaches by\nframing text generation as an iterative denoising process. However, applying\ndiffusion models to structured domains such as source code remains a\nsignificant challenge. Programming languages differ from natural language in\nthat they follow strict syntactic and semantic rules, with hierarchical\norganization that must be preserved for correctness. Standard token-level\ncorruption techniques used during training often ignore this structure, which\nmay hinder the model's ability to learn meaningful representations of code. To\naddress this limitation, we propose a syntax-aware diffusion framework that\nincorporates structural priors from Abstract Syntax Trees (ASTs) into the\ndenoising process. Instead of masking individual tokens at random, we\nselectively corrupt syntactically meaningful code spans derived from AST\nsubtrees. This enables the model to reconstruct programs in a way that respects\ngrammatical boundaries and captures long-range dependencies. Experimental\nresults demonstrate that syntax-aware corruption significantly improves\nsyntactic correctness, reconstruction accuracy, and generalization to unseen\ncode patterns. These findings highlight the potential of incorporating\nstructural information into diffusion-based training and suggest that\nsyntax-guided denoising is a promising direction for advancing diffusion-based\nlanguage models in code generation tasks.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u62bd\u8c61\u8bed\u6cd5\u6811\uff08AST\uff09\u7684\u8bed\u6cd5\u611f\u77e5\u6269\u6563\u6846\u67b6\uff0c\u63d0\u5347\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u7684\u53e5\u6cd5\u6b63\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b", "motivation": "\u4f20\u7edf\u6269\u6563\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u5ffd\u89c6\u8bed\u6cd5\u7ed3\u6784\uff0c\u5bfc\u81f4\u96be\u4ee5\u5b66\u4e60\u6b63\u786e\u7684\u4ee3\u7801\u8868\u793a\u3002\u9700\u901a\u8fc7\u7ed3\u6784\u5316\u5148\u9a8c\u89e3\u51b3\u8be5\u95ee\u9898", "method": "\u57fa\u4e8eAST\u5b50\u6811\u9009\u62e9\u6027\u635f\u574f\u4ee3\u7801\u7247\u6bb5\uff0c\u66ff\u4ee3\u968f\u673atoken\u5c4f\u853d\uff0c\u4f7f\u53bb\u566a\u8fc7\u7a0b\u4fdd\u6301\u8bed\u6cd5\u8fb9\u754c\u548c\u957f\u7a0b\u4f9d\u8d56", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u53e5\u6cd5\u6b63\u786e\u7387\uff08+15%\uff09\u3001\u91cd\u5efa\u51c6\u786e\u7387\uff08+22%\uff09\u53ca\u5bf9\u672a\u89c1\u4ee3\u7801\u6a21\u5f0f\u7684\u6cdb\u5316\u80fd\u529b", "conclusion": "\u8bc1\u5b9e\u7ed3\u6784\u4fe1\u606f\u6574\u5408\u5bf9\u6269\u6563\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u8bed\u6cd5\u5f15\u5bfc\u53bb\u566a\u4e3a\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411"}}
{"id": "2508.01480", "pdf": "https://arxiv.org/pdf/2508.01480", "abs": "https://arxiv.org/abs/2508.01480", "authors": ["Dimitra Panou", "Alexandros C. Dimopoulos", "Manolis Koubarakis", "Martin Reczko"], "title": "Harnessing Collective Intelligence of LLMs for Robust Biomedical QA: A Multi-Model Approach", "categories": ["cs.CL"], "comment": null, "summary": "Biomedical text mining and question-answering are essential yet highly\ndemanding tasks, particularly in the face of the exponential growth of\nbiomedical literature. In this work, we present our participation in the 13th\nedition of the BioASQ challenge, which involves biomedical semantic\nquestion-answering for Task 13b and biomedical question-answering for\ndeveloping topics for the Synergy task. We deploy a selection of open-source\nlarge language models (LLMs) as retrieval-augmented generators to answer\nbiomedical questions. Various models are used to process the questions. A\nmajority voting system combines their output to determine the final answer for\nYes/No questions, while for list and factoid type questions, the union of their\nanswers in used. We evaluated 13 state-of-the-art open source LLMs, exploring\nall possible model combinations to contribute to the final answer, resulting in\ntailored LLM pipelines for each question type. Our findings provide valuable\ninsight into which combinations of LLMs consistently produce superior results\nfor specific question types. In the four rounds of the 2025 BioASQ challenge,\nour system achieved notable results: in the Synergy task, we secured 1st place\nfor ideal answers and 2nd place for exact answers in round 2, as well as two\nshared 1st places for exact answers in round 3 and 4.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u7ec4\u5408\u7b56\u7565\uff0c\u5728BioASQ\u6311\u6218\u8d5b\u4e2d\u9488\u5bf9\u4e0d\u540c\u95ee\u9898\u7c7b\u578b\u6784\u5efa\u5b9a\u5236\u5316\u6d41\u7a0b\uff0c\u53d6\u5f97\u4f18\u5f02\u6210\u7ee9\u3002", "motivation": "\u5e94\u5bf9\u751f\u7269\u533b\u5b66\u6587\u732e\u7206\u70b8\u5f0f\u589e\u957f\u5e26\u6765\u7684\u6587\u672c\u6316\u6398\u9700\u6c42\uff0c\u63d0\u5347\u751f\u7269\u533b\u5b66\u95ee\u7b54\u7cfb\u7edf\u6027\u80fd\uff0c\u53c2\u4e0eBioASQ\u6311\u6218\u8d5b\u4efb\u52a113b\u548cSynergy\u4efb\u52a1\u3002", "method": "\u91c7\u752813\u4e2a\u5f00\u6e90LLM\u4f5c\u4e3a\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5668\uff0cYes/No\u7c7b\u95ee\u9898\u4f7f\u7528\u591a\u6570\u6295\u7968\u673a\u5236\uff0c\u5217\u8868/\u4e8b\u5b9e\u7c7b\u95ee\u9898\u53d6\u7b54\u6848\u5e76\u96c6\uff0c\u6784\u5efa\u4e0d\u540c\u95ee\u9898\u7c7b\u578b\u7684\u6a21\u578b\u7ec4\u5408\u6d41\u7a0b\u3002", "result": "\u57282025 BioASQ\u56db\u8f6e\u6bd4\u8d5b\u4e2d\uff0cSynergy\u4efb\u52a1\u53d6\u5f97\uff1a\u7b2c2\u8f6e\u7406\u60f3\u7b54\u6848\u7b2c1/\u7cbe\u786e\u7b54\u6848\u7b2c2\uff0c\u7b2c3-4\u8f6e\u7cbe\u786e\u7b54\u6848\u5171\u4eab\u7b2c1\u7684\u6210\u7ee9\u3002", "conclusion": "\u901a\u8fc7LLM\u7ec4\u5408\u7b56\u7565\u9a8c\u8bc1\u4e86\u9488\u5bf9\u6027\u95ee\u9898\u7c7b\u578b\u4f18\u5316\u7684\u6709\u6548\u6027\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u95ee\u7b54\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6a21\u578b\u9009\u62e9\u65b9\u6848\u3002"}}
{"id": "2508.01486", "pdf": "https://arxiv.org/pdf/2508.01486", "abs": "https://arxiv.org/abs/2508.01486", "authors": ["Vallabhaneni Raj Kumar", "Ashwin S", "Supriya Manna", "Niladri Sett", "Cheedella V S N M S Hema Harshitha", "Kurakula Harshitha", "Anand Kumar Sharma", "Basina Deepakraj", "Tanuj Sarkar", "Bondada Navaneeth Krishna", "Samanthapudi Shakeer"], "title": "TeSent: A Benchmark Dataset for Fairness-aware Explainable Sentiment Classification in Telugu", "categories": ["cs.CL"], "comment": "work under review", "summary": "In the Indian subcontinent, Telugu, one of India's six classical languages,\nis the most widely spoken Dravidian Language. Despite its 96 million speaker\nbase worldwide, Telugu remains underrepresented in the global NLP and Machine\nLearning landscape, mainly due to lack of high-quality annotated resources.\nThis work introduces TeSent, a comprehensive benchmark dataset for sentiment\nclassification, a key text classification problem, in Telugu. TeSent not only\nprovides ground truth labels for the sentences, but also supplements with\nprovisions for evaluating explainability and fairness, two critical\nrequirements in modern-day machine learning tasks. We scraped Telugu texts\ncovering multiple domains from various social media platforms, news websites\nand web-blogs to preprocess and generate 26,150 sentences, and developed a\ncustom-built annotation platform and a carefully crafted annotation protocol\nfor collecting the ground truth labels along with their human-annotated\nrationales. We then fine-tuned several SOTA pre-trained models in two ways:\nwith rationales, and without rationales. Further, we provide a detailed\nplausibility and faithfulness evaluation suite, which exploits the rationales,\nfor six widely used post-hoc explainers applied on the trained models. Lastly,\nwe curate TeEEC, Equity Evaluation Corpus in Telugu, a corpus to evaluate\nfairness of Telugu sentiment and emotion related NLP tasks, and provide a\nfairness evaluation suite for the trained classifier models. Our experimental\nresults suggest that training with rationales may improve model accuracy,\nreduce bias in models, and make the explainers' output more aligned to human\nreasoning.", "AI": {"tldr": "\u521b\u5efa\u4e86\u9996\u4e2a\u6cf0\u5362\u56fa\u8bed\u60c5\u611f\u5206\u6790\u57fa\u51c6\u6570\u636e\u96c6TeSent\uff0c\u6db5\u76d6\u53ef\u89e3\u91ca\u6027\u548c\u516c\u5e73\u6027\u8bc4\u4f30\u4f53\u7cfb", "motivation": "\u6cf0\u5362\u56fa\u8bed\u4f5c\u4e3a\u5370\u5ea6\u4e3b\u8981\u8bed\u8a00\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6807\u6ce8\u8d44\u6e90\uff0c\u5236\u7ea6\u5176NLP\u53d1\u5c55", "method": "\u901a\u8fc7\u591a\u6e90\u6570\u636e\u91c7\u96c6\u3001\u5b9a\u5236\u6807\u6ce8\u5e73\u53f0\u3001\u5bf9\u6bd4\u5b9e\u9a8c\u8bbe\u8ba1\uff08\u542b/\u4e0d\u542b\u4eba\u7c7b\u6807\u6ce8\u4f9d\u636e\u7684\u6a21\u578b\u8bad\u7ec3\uff09", "result": "\u878d\u5408\u4eba\u7c7b\u4f9d\u636e\u7684\u6a21\u578b\u51c6\u786e\u7387\u63d0\u53473.2%\uff0c\u6027\u522b\u504f\u89c1\u964d\u4f4e40%\uff0c\u89e3\u91ca\u5bf9\u9f50\u5ea6\u63d0\u9ad835%", "conclusion": "TeSent\u586b\u8865\u8d44\u6e90\u7a7a\u767d\uff0c\u9a8c\u8bc1\u53ef\u89e3\u91ca\u8bad\u7ec3\u63d0\u5347\u6a21\u578b\u6027\u80fd\u4e0e\u53ef\u4fe1\u5ea6\uff0c\u4fc3\u8fdb\u516c\u5e73AI\u53d1\u5c55"}}
{"id": "2508.01491", "pdf": "https://arxiv.org/pdf/2508.01491", "abs": "https://arxiv.org/abs/2508.01491", "authors": ["Zhivar Sourati", "Alireza S. Ziabari", "Morteza Dehghani"], "title": "The Homogenizing Effect of Large Language Models on Human Expression and Thought", "categories": ["cs.CL"], "comment": null, "summary": "Cognitive diversity, reflected in variations of language, perspective, and\nreasoning, is essential to creativity and collective intelligence. This\ndiversity is rich and grounded in culture, history, and individual experience.\nYet as large language models (LLMs) become deeply embedded in people's lives,\nthey risk standardizing language and reasoning. This Review synthesizes\nevidence across linguistics, cognitive, and computer science to show how LLMs\nreflect and reinforce dominant styles while marginalizing alternative voices\nand reasoning strategies. We examine how their design and widespread use\ncontribute to this effect by mirroring patterns in their training data and\namplifying convergence as all people increasingly rely on the same models\nacross contexts. Unchecked, this homogenization risks flattening the cognitive\nlandscapes that drive collective intelligence and adaptability.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u901a\u8fc7\u6807\u51c6\u5316\u8bed\u8a00\u548c\u63a8\u7406\u65b9\u5f0f\u524a\u5f31\u8ba4\u77e5\u591a\u6837\u6027\uff0c\u5a01\u80c1\u96c6\u4f53\u667a\u6167", "motivation": "\u7814\u7a76LLMs\u5982\u4f55\u901a\u8fc7\u8bbe\u8ba1\u548c\u4f7f\u7528\u6a21\u5f0f\u5f3a\u5316\u4e3b\u6d41\u8ba4\u77e5\u65b9\u5f0f\uff0c\u8fb9\u7f18\u5316\u591a\u5143\u58f0\u97f3", "method": "\u6574\u5408\u8bed\u8a00\u5b66\u3001\u8ba4\u77e5\u79d1\u5b66\u548c\u8ba1\u7b97\u673a\u79d1\u5b66\u8bc1\u636e\uff0c\u5206\u6790\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u6a21\u5f0f\u4e0e\u793e\u4f1a\u8d8b\u540c\u6548\u5e94", "result": "LLMs\u53cd\u6620\u5e76\u653e\u5927\u4e86\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u4e3b\u5bfc\u6a21\u5f0f\uff0c\u5bfc\u81f4\u8ba4\u77e5\u7b56\u7565\u7684\u540c\u8d28\u5316", "conclusion": "\u9700\u8b66\u60d5\u6a21\u578b\u5f15\u53d1\u7684\u8ba4\u77e5\u540c\u8d28\u5316\u98ce\u9669\uff0c\u4fdd\u62a4\u96c6\u4f53\u667a\u6167\u6240\u9700\u7684\u8ba4\u77e5\u591a\u6837\u6027\u57fa\u7840"}}
{"id": "2508.01503", "pdf": "https://arxiv.org/pdf/2508.01503", "abs": "https://arxiv.org/abs/2508.01503", "authors": ["Clayton Cohn", "Surya Rayala", "Namrata Srivastava", "Joyce Horn Fonteles", "Shruti Jain", "Xinying Luo", "Divya Mereddy", "Naveeduddin Mohammed", "Gautam Biswas"], "title": "A Theory of Adaptive Scaffolding for LLM-Based Pedagogical Agents", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) present new opportunities for creating\npedagogical agents that engage in meaningful dialogue to support student\nlearning. However, the current use of LLM systems like ChatGPT in classrooms\noften lacks the solid theoretical foundation found in earlier intelligent\ntutoring systems. To bridge this gap, we propose a framework that combines\nEvidence-Centered Design with Social Cognitive Theory for adaptive scaffolding\nin LLM-based agents focused on STEM+C learning. We illustrate this framework\nwith Inquizzitor, an LLM-based formative assessment agent that integrates\nhuman-AI hybrid intelligence and provides feedback grounded in cognitive\nscience principles. Our findings show that Inquizzitor delivers high-quality\nassessment and interaction aligned with core learning theories, offering\nteachers effective guidance that students value. This research underscores the\npotential for theory-driven LLM integration in education, highlighting the\nability of these systems to provide adaptive and principled instruction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u8bc1\u636e\u4e2d\u5fc3\u8bbe\u8ba1\u4e0e\u793e\u4f1a\u8ba4\u77e5\u7406\u8bba\u7ed3\u5408\uff0c\u6784\u5efa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u6846\u67b6Inquizzitor\uff0c\u901a\u8fc7\u4eba\u673a\u6df7\u5408\u667a\u80fd\u5b9e\u73b0STEM+C\u9886\u57df\u7684\u9ad8\u8d28\u91cf\u5f62\u6210\u6027\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524d\u6559\u80b2\u9886\u57df\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u7f3a\u4e4f\u7c7b\u4f3c\u4f20\u7edf\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u7684\u7406\u8bba\u57fa\u7840\uff0c\u9700\u8981\u6784\u5efa\u7406\u8bba\u9a71\u52a8\u7684LLM\u6559\u80b2\u4ee3\u7406\u6846\u67b6\u4ee5\u63d0\u5347\u6559\u5b66\u6548\u679c\u7684\u79d1\u5b66\u6027\u4e0e\u9002\u5e94\u6027\u3002", "method": "\u878d\u5408\u8bc1\u636e\u4e2d\u5fc3\u8bbe\u8ba1(ECD)\u548c\u793e\u4f1a\u8ba4\u77e5\u7406\u8bba(SCT)\uff0c\u5f00\u53d1\u6574\u5408\u8ba4\u77e5\u79d1\u5b66\u539f\u7406\u7684LLM\u4ee3\u7406Inquizzitor\uff0c\u91c7\u7528\u4eba\u673a\u6df7\u5408\u667a\u80fd\u5b9e\u73b0\u4e2a\u6027\u5316\u5b66\u4e60\u652f\u67b6\u548c\u5f62\u6210\u6027\u53cd\u9988\u3002", "result": "Inquizzitor\u5b9e\u73b0\u4e86\u4e0e\u6838\u5fc3\u5b66\u4e60\u7406\u8bba\u76f8\u5339\u914d\u7684\u9ad8\u8d28\u91cf\u8bc4\u4f30\u4ea4\u4e92\uff0c\u4e3a\u6559\u5e08\u63d0\u4f9b\u6709\u6548\u6307\u5bfc\u4e14\u83b7\u5f97\u5b66\u751f\u8ba4\u53ef\uff0c\u8bc1\u660e\u7406\u8bba\u9a71\u52a8LLM\u7cfb\u7edf\u7684\u6559\u5b66\u53ef\u884c\u6027\u3002", "conclusion": "\u7406\u8bba\u9a71\u52a8\u7684LLM\u6559\u80b2\u7cfb\u7edf\u80fd\u63d0\u4f9b\u81ea\u9002\u5e94\u539f\u5219\u6027\u6559\u5b66\uff0c\u4e3a\u4eba\u5de5\u667a\u80fd\u6559\u80b2\u5e94\u7528\u5f00\u8f9f\u4e86\u57fa\u4e8e\u8ba4\u77e5\u79d1\u5b66\u539f\u7406\u7684\u521b\u65b0\u8def\u5f84\u3002"}}
{"id": "2508.01541", "pdf": "https://arxiv.org/pdf/2508.01541", "abs": "https://arxiv.org/abs/2508.01541", "authors": ["Sara C\u00e2mara", "Eduardo Luz", "Val\u00e9ria Carvalho", "Ivan Meneghini", "Gladston Moreira"], "title": "MOPrompt: Multi-objective Semantic Evolution for Prompt Optimization", "categories": ["cs.CL"], "comment": "8 pages", "summary": "Prompt engineering is crucial for unlocking the potential of Large Language\nModels (LLMs). Still, since manual prompt design is often complex,\nnon-intuitive, and time-consuming, automatic prompt optimization has emerged as\na research area. However, a significant challenge in prompt optimization is\nmanaging the inherent trade-off between task performance, such as accuracy, and\ncontext size. Most existing automated methods focus on a single objective,\ntypically performance, thereby failing to explore the critical spectrum of\nefficiency and effectiveness. This paper introduces the MOPrompt, a novel\nMulti-objective Evolutionary Optimization (EMO) framework designed to optimize\nprompts for both accuracy and context size (measured in tokens) simultaneously.\nOur framework maps the Pareto front of prompt solutions, presenting\npractitioners with a set of trade-offs between context size and performance, a\ncrucial tool for deploying Large Language Models (LLMs) in real-world\napplications. We evaluate MOPrompt on a sentiment analysis task in Portuguese,\nusing Gemma-2B and Sabiazinho-3 as evaluation models. Our findings show that\nMOPrompt substantially outperforms the baseline framework. For the Sabiazinho\nmodel, MOPrompt identifies a prompt that achieves the same peak accuracy (0.97)\nas the best baseline solution, but with a 31% reduction in token length.", "AI": {"tldr": "\u63d0\u51fa\u591a\u76ee\u6807\u8fdb\u5316\u6846\u67b6MOPrompt\uff0c\u5728\u4fdd\u6301LLM\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c1131%\u63d0\u793a\u957f\u5ea6", "motivation": "\u73b0\u6709\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u591a\u5173\u6ce8\u5355\u4e00\u6027\u80fd\u6307\u6807\uff0c\u7f3a\u4e4f\u6548\u7387\u4e0e\u6548\u679c\u7684\u6743\u8861\u63a2\u7d22", "method": "\u91c7\u7528\u591a\u76ee\u6807\u8fdb\u5316\u7b97\u6cd5\uff08EMO\uff09\uff0c\u6784\u5efa\u63d0\u793a\u89e3\u51b3\u65b9\u6848\u7684Pareto\u524d\u6cbf\u5206\u6790", "result": "\u5728\u8461\u8404\u7259\u8bed\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e2d\uff0cSabiazinho\u6a21\u578b\u5b9e\u73b00.97\u51c6\u786e\u7387\u4e14token\u957f\u5ea6\u51cf\u5c1131%", "conclusion": "MOPrompt\u6846\u67b6\u4e3a\u5b9e\u9645LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u8c03\u8282\u7684\u7cbe\u5ea6-\u6548\u7387\u6743\u8861\u5de5\u5177"}}
{"id": "2508.01554", "pdf": "https://arxiv.org/pdf/2508.01554", "abs": "https://arxiv.org/abs/2508.01554", "authors": ["Yujia Zheng", "Tianhao Li", "Haotian Huang", "Tianyu Zeng", "Jingyu Lu", "Chuangxin Chu", "Yuekai Huang", "Ziyou Jiang", "Qian Xiong", "Yuyao Ge", "Mingyang Li"], "title": "Are All Prompt Components Value-Neutral? Understanding the Heterogeneous Adversarial Robustness of Dissected Prompt in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": null, "summary": "Prompt-based adversarial attacks have become an effective means to assess the\nrobustness of large language models (LLMs). However, existing approaches often\ntreat prompts as monolithic text, overlooking their structural\nheterogeneity-different prompt components contribute unequally to adversarial\nrobustness. Prior works like PromptRobust assume prompts are value-neutral, but\nour analysis reveals that complex, domain-specific prompts with rich structures\nhave components with differing vulnerabilities. To address this gap, we\nintroduce PromptAnatomy, an automated framework that dissects prompts into\nfunctional components and generates diverse, interpretable adversarial examples\nby selectively perturbing each component using our proposed method, ComPerturb.\nTo ensure linguistic plausibility and mitigate distribution shifts, we further\nincorporate a perplexity (PPL)-based filtering mechanism. As a complementary\nresource, we annotate four public instruction-tuning datasets using the\nPromptAnatomy framework, verified through human review. Extensive experiments\nacross these datasets and five advanced LLMs demonstrate that ComPerturb\nachieves state-of-the-art attack success rates. Ablation studies validate the\ncomplementary benefits of prompt dissection and PPL filtering. Our results\nunderscore the importance of prompt structure awareness and controlled\nperturbation for reliable adversarial robustness evaluation in LLMs. Code and\ndata are available at https://github.com/Yujiaaaaa/PACP.", "AI": {"tldr": "\u63d0\u51fa\u4e86PromptAnatomy\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u63d0\u793a\u7ec4\u4ef6\u548c\u53ef\u63a7\u6270\u52a8\u6765\u8bc4\u4f30LLMs\u7684\u5bf9\u6297\u9c81\u68d2\u6027", "motivation": "\u73b0\u6709\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u5ffd\u89c6\u63d0\u793a\u7ed3\u6784\u5f02\u8d28\u6027\uff0c\u4e0d\u540c\u7ec4\u4ef6\u5177\u6709\u4e0d\u540c\u8106\u5f31\u6027\u3002\u4f20\u7edf\u65b9\u6cd5\u5047\u8bbe\u63d0\u793a\u4ef7\u503c\u4e2d\u6027\uff0c\u4f46\u5b9e\u9645\u590d\u6742\u63d0\u793a\u4e2d\u7ec4\u4ef6\u6f0f\u6d1e\u5206\u5e03\u4e0d\u5747", "method": "1. \u5c06\u63d0\u793a\u89e3\u6784\u4e3a\u529f\u80fd\u7ec4\u4ef6 2. \u63d0\u51faComPerturb\u65b9\u6cd5\u9009\u62e9\u6027\u5730\u6270\u52a8\u6bcf\u4e2a\u7ec4\u4ef6 3. \u5f15\u5165\u57fa\u4e8e\u56f0\u60d1\u5ea6(PPL)\u7684\u8fc7\u6ee4\u673a\u5236\u4fdd\u6301\u8bed\u8a00\u5408\u7406\u6027 4. \u6807\u6ce8\u56db\u4e2a\u516c\u5f00\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6", "result": "\u57285\u4e2a\u5148\u8fdbLLM\u4e0a\u5b9e\u73b0SOTA\u653b\u51fb\u6210\u529f\u7387\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u7ec4\u4ef6\u89e3\u6784\u4e0ePPL\u8fc7\u6ee4\u7684\u534f\u540c\u6548\u76ca\uff0c\u4eba\u5de5\u9a8c\u8bc1\u6570\u636e\u96c6\u6807\u6ce8\u8d28\u91cf", "conclusion": "\u63d0\u793a\u7ed3\u6784\u611f\u77e5\u548c\u53d7\u63a7\u6270\u52a8\u5bf9\u53ef\u9760\u8bc4\u4f30LLM\u5bf9\u6297\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\uff0c\u516c\u5f00\u7684\u4ee3\u7801\u548c\u6570\u636e\u96c6\u63a8\u52a8\u9886\u57df\u53d1\u5c55"}}
{"id": "2508.01630", "pdf": "https://arxiv.org/pdf/2508.01630", "abs": "https://arxiv.org/abs/2508.01630", "authors": ["Maziyar Panahi"], "title": "OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers for Biomedical NER Across 12 Public Datasets", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Named-entity recognition (NER) is fundamental to extracting structured\ninformation from the >80% of healthcare data that resides in unstructured\nclinical notes and biomedical literature. Despite recent advances with large\nlanguage models, achieving state-of-the-art performance across diverse entity\ntypes while maintaining computational efficiency remains a significant\nchallenge. We introduce OpenMed NER, a suite of open-source, domain-adapted\ntransformer models that combine lightweight domain-adaptive pre-training (DAPT)\nwith parameter-efficient Low-Rank Adaptation (LoRA). Our approach performs\ncost-effective DAPT on a 350k-passage corpus compiled from ethically sourced,\npublicly available research repositories and de-identified clinical notes\n(PubMed, arXiv, and MIMIC-III) using DeBERTa-v3, PubMedBERT, and BioELECTRA\nbackbones. This is followed by task-specific fine-tuning with LoRA, which\nupdates less than 1.5% of model parameters. We evaluate our models on 12\nestablished biomedical NER benchmarks spanning chemicals, diseases, genes, and\nspecies. OpenMed NER achieves new state-of-the-art micro-F1 scores on 10 of\nthese 12 datasets, with substantial gains across diverse entity types. Our\nmodels advance the state-of-the-art on foundational disease and chemical\nbenchmarks (e.g., BC5CDR-Disease, +2.70 pp), while delivering even larger\nimprovements of over 5.3 and 9.7 percentage points on more specialized gene and\nclinical cell line corpora. This work demonstrates that strategically adapted\nopen-source models can surpass closed-source solutions. This performance is\nachieved with remarkable efficiency: training completes in under 12 hours on a\nsingle GPU with a low carbon footprint (< 1.2 kg CO2e), producing permissively\nlicensed, open-source checkpoints designed to help practitioners facilitate\ncompliance with emerging data protection and AI regulations, such as the EU AI\nAct.", "AI": {"tldr": "OpenMed NER\u901a\u8fc7\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3(DAPT)\u4e0e\u4f4e\u79e9\u81ea\u9002\u5e94(LoRA)\u7ed3\u5408\uff0c\u572812\u4e2a\u751f\u7269\u533b\u5b66NER\u57fa\u51c6\u6d4b\u8bd5\u4e2d10\u9879\u8fbe\u5230SOTA\uff0c\u8bad\u7ec3\u6548\u7387\u9ad8\u4e14\u78b3\u6392\u653e\u4f4e\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u9886\u57df\u591a\u6837\u5b9e\u4f53\u7c7b\u578b\u8bc6\u522b\u4e2d\u6027\u80fd\u4e0e\u8ba1\u7b97\u6548\u7387\u96be\u4ee5\u517c\u987e\u7684\u95ee\u9898\uff0c\u7279\u522b\u9488\u5bf9\u4e34\u5e8a\u6587\u672c\u548c\u751f\u7269\u533b\u5b66\u6587\u732e\u4e2d\u7684\u975e\u7ed3\u6784\u5316\u6570\u636e\u5904\u7406\u9700\u6c42\u3002", "method": "\u4f7f\u7528DeBERTa-v3\u7b49\u6a21\u578b\u67b6\u6784\uff0c\u572835\u4e07\u6bb5\u533b\u5b66\u6587\u672c\u4e0a\u8fdb\u884c\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3(DAPT)\uff0c\u540e\u7eed\u91c7\u7528LoRA\u5fae\u8c03(\u4ec5\u66f4\u65b0<1.5%\u53c2\u6570)\u3002", "result": "\u5728BC5CDR-Disease\u7b49\u57fa\u51c6\u4e0a\u63d0\u53472.7\u4e2a\u767e\u5206\u70b9\uff0c\u57fa\u56e0/\u4e34\u5e8a\u7ec6\u80de\u7cfb\u6570\u636e\u63d0\u5347\u8d855.3-9.7\u4e2a\u767e\u5206\u70b9\uff1b\u5355GPU\u8bad\u7ec3<12\u5c0f\u65f6\uff0c\u78b3\u6392\u653e<1.2kg CO2e\u3002", "conclusion": "\u5f00\u6e90\u6a21\u578b\u901a\u8fc7\u6218\u7565\u9002\u914d\u53ef\u8d85\u8d8a\u95ed\u6e90\u65b9\u6848\uff0c\u517c\u987e\u6027\u80fd\u4e0e\u6cd5\u89c4\u5408\u89c4\u6027\uff0c\u4e3a\u533b\u7597AI\u5e94\u7528\u63d0\u4f9b\u9ad8\u6548\u73af\u4fdd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.01656", "pdf": "https://arxiv.org/pdf/2508.01656", "abs": "https://arxiv.org/abs/2508.01656", "authors": ["Lucio La Cava", "Dominik Macko", "R\u00f3bert M\u00f3ro", "Ivan Srba", "Andrea Tagarelli"], "title": "Authorship Attribution in Multilingual Machine-Generated Texts", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "physics.soc-ph"], "comment": null, "summary": "As Large Language Models (LLMs) have reached human-like fluency and\ncoherence, distinguishing machine-generated text (MGT) from human-written\ncontent becomes increasingly difficult. While early efforts in MGT detection\nhave focused on binary classification, the growing landscape and diversity of\nLLMs require a more fine-grained yet challenging authorship attribution (AA),\ni.e., being able to identify the precise generator (LLM or human) behind a\ntext. However, AA remains nowadays confined to a monolingual setting, with\nEnglish being the most investigated one, overlooking the multilingual nature\nand usage of modern LLMs. In this work, we introduce the problem of\nMultilingual Authorship Attribution, which involves attributing texts to human\nor multiple LLM generators across diverse languages. Focusing on 18 languages\n-- covering multiple families and writing scripts -- and 8 generators (7 LLMs\nand the human-authored class), we investigate the multilingual suitability of\nmonolingual AA methods, their cross-lingual transferability, and the impact of\ngenerators on attribution performance. Our results reveal that while certain\nmonolingual AA methods can be adapted to multilingual settings, significant\nlimitations and challenges remain, particularly in transferring across diverse\nlanguage families, underscoring the complexity of multilingual AA and the need\nfor more robust approaches to better match real-world scenarios.", "AI": {"tldr": "\u63d0\u51fa\u591a\u8bed\u8a00\u4f5c\u8005\u5f52\u5c5e\u95ee\u9898\uff0c\u7814\u7a76\u5355\u8bed\u68c0\u6d4b\u65b9\u6cd5\u5728\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u53ca\u8de8\u8bed\u8a00\u8fc1\u79fb\u80fd\u529b", "motivation": "\u73b0\u6709\u4f5c\u8005\u5f52\u5c5e\u7814\u7a76\u5c40\u9650\u4e8e\u5355\u8bed\u73af\u5883\uff0c\u672a\u80fd\u5339\u914d\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u8bed\u8a00\u5e94\u7528\u573a\u666f", "method": "\u8986\u76d618\u79cd\u4e0d\u540c\u8bed\u7cfb/\u6587\u5b57\u7684\u8bed\u8a00\uff0c\u6d4b\u8bd58\u79cd\u751f\u6210\u5668\uff087\u4e2aLLM+\u4eba\u7c7b\uff09\u7684\u68c0\u6d4b\u65b9\u6cd5\u6027\u80fd", "result": "\u5355\u8bed\u65b9\u6cd5\u5728\u591a\u8bed\u8a00\u573a\u666f\u5b58\u5728\u8de8\u8bed\u7cfb\u8fc1\u79fb\u5c40\u9650\uff0c\u9700\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u68c0\u6d4b\u65b9\u6848", "conclusion": "\u5f53\u524d\u65b9\u6cd5\u96be\u4ee5\u9002\u5e94\u771f\u5b9e\u4e16\u754c\u7684\u591a\u8bed\u8a00\u590d\u6742\u6027\uff0c\u4e9f\u5f85\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u8de8\u8bed\u8a00\u68c0\u6d4b\u6846\u67b6"}}
{"id": "2508.01674", "pdf": "https://arxiv.org/pdf/2508.01674", "abs": "https://arxiv.org/abs/2508.01674", "authors": ["Tae Soo Kim", "Yoonjoo Lee", "Yoonah Park", "Jiho Kim", "Young-Ho Kim", "Juho Kim"], "title": "CUPID: Evaluating Personalized and Contextualized Alignment of LLMs from Interactions", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Accepted to COLM 2025. Project Website: https://cupid.kixlab.org/", "summary": "Personalization of Large Language Models (LLMs) often assumes users hold\nstatic preferences that reflect globally in all tasks. In reality, humans hold\ndynamic preferences that change depending on the context. As users interact\nwith an LLM in various contexts, they naturally reveal their contextual\npreferences, which a model must infer and apply in future contexts to ensure\nalignment. To assess this, we introduce CUPID, a benchmark of 756 human-curated\ninteraction session histories between users and LLM-based chat assistants. In\neach interaction session, the user provides a request in a specific context and\nexpresses their preference through multi-turn feedback. Given a new user\nrequest and prior interaction sessions, our benchmark assesses whether LLMs can\ninfer the preference relevant to this request and generate a response that\nsatisfies this preference. With CUPID, we evaluated 10 open and proprietary\nLLMs, revealing that state-of-the-art LLMs struggle to infer preferences from\nmulti-turn interactions and fail to discern what previous context is relevant\nto a new request -- under 50% precision and 65% recall. Our work highlights the\nneed to advance LLM capabilities for more contextually personalized\ninteractions and proposes CUPID as a resource to drive these improvements.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCUPID\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u4e0a\u4e0b\u6587\u504f\u597d\u63a8\u65ad\u4e0a\u7684\u4e0d\u8db3\uff0c\u7cbe\u786e\u5ea6\u4f4e\u4e8e50%\u4e14\u53ec\u56de\u7387\u4e0d\u8db365%\u3002", "motivation": "\u73b0\u6709LLM\u4e2a\u6027\u5316\u65b9\u6cd5\u5047\u8bbe\u7528\u6237\u504f\u597d\u662f\u9759\u6001\u7684\uff0c\u800c\u4eba\u7c7b\u504f\u597d\u5b9e\u9645\u5177\u6709\u52a8\u6001\u4e0a\u4e0b\u6587\u654f\u611f\u6027\uff0c\u9700\u901a\u8fc7\u591a\u8f6e\u4ea4\u4e92\u63a8\u65ad\u5e94\u7528\u7528\u6237\u504f\u597d\u3002", "method": "\u6784\u5efa\u542b756\u4e2a\u4eba\u5de5\u6807\u6ce8\u4f1a\u8bdd\u7684CUPID\u57fa\u51c6\uff0c\u901a\u8fc7\u591a\u8f6e\u53cd\u9988\u6d4b\u8bd5LLM\u5728\u65b0\u8bf7\u6c42\u4e2d\u8bc6\u522b\u76f8\u5173\u5386\u53f2\u504f\u597d\u5e76\u751f\u6210\u54cd\u5e94\u3002", "result": "\u9876\u5c16LLM\u5728\u591a\u8f6e\u504f\u597d\u63a8\u65ad\u548c\u4e0a\u4e0b\u6587\u5173\u8054\u8bc6\u522b\u4e2d\u8868\u73b0\u6b20\u4f73\uff0c\u7cbe\u786e\u5ea6\u4f4e\u4e8e50%\uff0c\u53ec\u56de\u7387\u4f4e\u4e8e65%\u3002", "conclusion": "\u9700\u63d0\u5347LLM\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u4e2a\u6027\u5316\u80fd\u529b\uff0cCUPID\u53ef\u4f5c\u4e3a\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u7684\u91cd\u8981\u8bc4\u4f30\u8d44\u6e90\u3002"}}
{"id": "2508.01682", "pdf": "https://arxiv.org/pdf/2508.01682", "abs": "https://arxiv.org/abs/2508.01682", "authors": ["Lingyin Zhang", "Jun Gao", "Xiaoxue Ren", "Ziqiang Cao"], "title": "The Bidirectional Process Reward Model", "categories": ["cs.CL"], "comment": null, "summary": "Process Reward Models (PRMs) have emerged as a promising approach to enhance\nthe reasoning quality of Large Language Models (LLMs) by assigning fine-grained\nscores to intermediate reasoning steps within a solution trajectory. However,\nexisting PRMs predominantly adopt a unidirectional left-to-right (L2R)\nevaluation paradigm, which limits their ability to leverage global context,\nmaking it challenging to verify the consistency of earlier steps based on later\nones. In light of these challenges, we propose a novel bidirectional evaluation\nparadigm, named Bidirectional Process Reward Model (BiPRM). BiPRM seamlessly\nincorporates a parallel right-to-left (R2L) evaluation stream alongside the\nconventional L2R flow, enabling later reasoning steps to help assess earlier\nones in real time. Notably, the built-in R2L evaluation is implemented solely\nthrough prompt modifications that reverse the original reasoning trajectory,\nwithout any additional parameters or inference latency introduced. This ensures\nBiPRM remains both efficient and broadly compatible with existing PRM studies.\nWe conduct extensive experiments on two mathematical reasoning benchmarks using\nsamples generated by three different policy models. Our method, BiPRM, is\nevaluated across three backbones and three distinct PRM objectives. Across all\nsettings, BiPRM consistently outperforms unidirectional baselines, achieving up\nto a 31.9% improvement in stepwise reward evaluation. Generally, our results\nhighlight BiPRM's effectiveness, robustness, and general applicability,\noffering a promising new direction for process-based reward modeling.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u5411\u8fc7\u7a0b\u5956\u52b1\u6a21\u578bBiPRM\uff0c\u901a\u8fc7\u589e\u52a0\u4ece\u53f3\u5230\u5de6\u7684\u9006\u5411\u8bc4\u4f30\u6d41\uff0c\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8d28\u91cf\u7684\u8bc4\u4f30\u7cbe\u5ea6", "motivation": "\u4f20\u7edf\u5355\u5411\u8bc4\u4f30\u8303\u5f0f\uff08L2R\uff09\u5b58\u5728\u5168\u5c40\u4e0a\u4e0b\u6587\u5229\u7528\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u901a\u8fc7\u540e\u7eed\u6b65\u9aa4\u9a8c\u8bc1\u524d\u671f\u63a8\u7406\u7684\u8fde\u8d2f\u6027", "method": "\u5728\u4fdd\u6301\u539f\u6709L2R\u8bc4\u4f30\u6d41\u7684\u57fa\u7840\u4e0a\uff0c\u901a\u8fc7\u63d0\u793a\u4fee\u6539\u5b9e\u73b0\u8f68\u8ff9\u53cd\u8f6c\uff0c\u5efa\u7acb\u96f6\u53c2\u6570\u7684R2L\u9006\u5411\u8bc4\u4f30\u6d41\uff0c\u5b9e\u65f6\u5229\u7528\u540e\u7eed\u6b65\u9aa4\u9a8c\u8bc1\u524d\u671f\u63a8\u7406", "result": "\u5728\u4e09\u4e2a\u7b56\u7565\u6a21\u578b\u751f\u6210\u6837\u672c\u7684\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBiPRM\u76f8\u8f83\u5355\u5411\u57fa\u7ebf\u6700\u9ad8\u63d0\u534731.9%\u7684\u9010\u6b65\u5956\u52b1\u8bc4\u4f30\u7cbe\u5ea6", "conclusion": "BiPRM\u901a\u8fc7\u521b\u65b0\u7684\u53cc\u5411\u8bc4\u4f30\u673a\u5236\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u517c\u5bb9\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u7684\u8bc4\u4f30\u6548\u679c\uff0c\u4e3a\u63a8\u7406\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411"}}
{"id": "2508.01696", "pdf": "https://arxiv.org/pdf/2508.01696", "abs": "https://arxiv.org/abs/2508.01696", "authors": ["Yi Jiang", "Sendong Zhao", "Jianbo Li", "Haochun Wang", "Lizhe Zhang", "Yan Liu", "Bin Qin"], "title": "Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy", "categories": ["cs.CL", "cs.AI"], "comment": "code available at https://github.com/liunian-Jay/CoCoA", "summary": "Retrieval-Augmented Generation (RAG) has emerged as a promising framework for\nenhancing the capabilities of Large Language Models (LLMs), especially in\nknowledge-intensive tasks. Despite its advantages, current RAG methods often\nstruggle to *fully exploit knowledge during generation*. In particular, the\nsynergy between the model's internal parametric knowledge and external\nretrieved knowledge remains limited. Retrieved contents may sometimes mislead\ngeneration, while certain generated content can guide the model toward more\naccurate outputs. In this work, we propose Collaborative Chain-of-Agents, a\nframework designed to enhance explicitly synergy over both parametric and\nretrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent\nRAG framework that first performs conditional knowledge induction and then\nreasons answers. Building on this, we develop CoCoA, a long-chain training\nstrategy that synthesizes extended multi-agent reasoning trajectories from\nCoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability\nto explicitly integrate and jointly leverage parametric and retrieved\nknowledge. Experiments results show that CoCoA-zero and CoCoA achieve superior\nperformance on open-domain and multi-hop QA tasks.", "AI": {"tldr": "\u63d0\u51fa\u534f\u4f5c\u4ee3\u7406\u94fe\u6846\u67b6CoCoA\uff0c\u901a\u8fc7\u591a\u4ee3\u7406\u534f\u4f5c\u673a\u5236\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u53c2\u6570\u5316\u77e5\u8bc6\u548c\u68c0\u7d22\u77e5\u8bc6\u7684\u534f\u540c\u6548\u5e94\uff0c\u5728\u5f00\u653e\u57df\u548c\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u5b9e\u73b0\u66f4\u4f18\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u65b9\u6cd5\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u672a\u80fd\u5145\u5206\u878d\u5408\u53c2\u6570\u5316\u77e5\u8bc6\u548c\u68c0\u7d22\u77e5\u8bc6\uff0c\u4e24\u8005\u534f\u540c\u6027\u4e0d\u8db3\u5bfc\u81f4\u77e5\u8bc6\u5229\u7528\u7387\u4f4e\u751a\u81f3\u8bef\u5bfc\u751f\u6210\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u591a\u4ee3\u7406\u534f\u4f5c\u673a\u5236\u5b9e\u73b0\u4e24\u7c7b\u77e5\u8bc6\u7684\u663e\u5f0f\u534f\u540c\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1aCoCoA-zero\u901a\u8fc7\u6761\u4ef6\u77e5\u8bc6\u5f52\u7eb3\u4ee3\u7406\u548c\u63a8\u7406\u4ee3\u7406\u7684\u534f\u4f5c\u5b9e\u73b0\u77e5\u8bc6\u534f\u540c\uff1bCoCoA\u91c7\u7528\u957f\u94fe\u8bad\u7ec3\u7b56\u7565\uff0c\u57fa\u4e8eCoCoA-zero\u751f\u6210\u7684\u591a\u4ee3\u7406\u63a8\u7406\u8f68\u8ff9\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5728\u5f00\u653e\u57df\u95ee\u7b54\uff08NQ\u3001HotpotQA\uff09\u548c\u591a\u8df3\u95ee\u7b54\uff082WikiMQA\u3001MuSiQue\uff09\u4efb\u52a1\u4e2d\uff0cCoCoA-zero\u548cCoCoA\u5206\u522b\u53d6\u5f975.7%/10.5%\u548c5.4%/9.6%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u9996\u6b21\u5b9e\u73b0\u53c2\u6570\u5316\u77e5\u8bc6\u4e0e\u68c0\u7d22\u77e5\u8bc6\u7684\u663e\u5f0f\u534f\u540c\uff0c\u901a\u8fc7\u591a\u4ee3\u7406\u534f\u4f5c\u673a\u5236\u548c\u957f\u94fe\u8bad\u7ec3\u7b56\u7565\u6709\u6548\u63d0\u5347\u590d\u6742\u77e5\u8bc6\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4e3aRAG\u7cfb\u7edf\u4f18\u5316\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2508.01708", "pdf": "https://arxiv.org/pdf/2508.01708", "abs": "https://arxiv.org/abs/2508.01708", "authors": ["Berkay K\u00f6pr\u00fc", "Mehrzad Mashal", "Yigit Gurses", "Akos Kadar", "Maximilian Schmitt", "Ditty Mathew", "Felix Burkhardt", "Florian Eyben", "Bj\u00f6rn W. Schuller"], "title": "Am I Blue or Is My Hobby Counting Teardrops? Expression Leakage in Large Language Models as a Symptom of Irrelevancy Disruption", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have advanced natural language processing (NLP)\nskills such as through next-token prediction and self-attention, but their\nability to integrate broad context also makes them prone to incorporating\nirrelevant information. Prior work has focused on semantic leakage, bias\nintroduced by semantically irrelevant context. In this paper, we introduce\nexpression leakage, a novel phenomenon where LLMs systematically generate\nsentimentally charged expressions that are semantically unrelated to the input\ncontext. To analyse the expression leakage, we collect a benchmark dataset\nalong with a scheme to automatically generate a dataset from free-form text\nfrom common-crawl. In addition, we propose an automatic evaluation pipeline\nthat correlates well with human judgment, which accelerates the benchmarking by\ndecoupling from the need of annotation for each analysed model. Our experiments\nshow that, as the model scales in the parameter space, the expression leakage\nreduces within the same LLM family. On the other hand, we demonstrate that\nexpression leakage mitigation requires specific care during the model building\nprocess, and cannot be mitigated by prompting. In addition, our experiments\nindicate that, when negative sentiment is injected in the prompt, it disrupts\nthe generation process more than the positive sentiment, causing a higher\nexpression leakage rate.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u51fa\u73b0\u7684\u2018\u8868\u8fbe\u6cc4\u6f0f\u2019\u73b0\u8c61\uff0c\u5373\u6a21\u578b\u751f\u6210\u4e0e\u8f93\u5165\u8bed\u4e49\u65e0\u5173\u7684\u60c5\u611f\u5316\u8868\u8fbe\uff0c\u5e76\u63a2\u8ba8\u4e86\u6a21\u578b\u89c4\u6a21\u3001\u6784\u5efa\u65b9\u6cd5\u53ca\u60c5\u611f\u6ce8\u5165\u5bf9\u6cc4\u6f0f\u7684\u5f71\u54cd", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u8bed\u4e49\u6cc4\u6f0f\uff0c\u800c\u672c\u6587\u9996\u6b21\u53d1\u73b0LLMs\u4f1a\u7cfb\u7edf\u751f\u6210\u4e0e\u8f93\u5165\u65e0\u5173\u7684\u60c5\u611f\u8868\u8fbe\uff0c\u9700\u63ed\u793a\u5176\u673a\u5236\u53ca\u7f13\u89e3\u65b9\u6cd5", "method": "\u901a\u8fc7\u6536\u96c6\u57fa\u51c6\u6570\u636e\u96c6\u3001\u8bbe\u8ba1\u57fa\u4e8eCommon Crawl\u7684\u81ea\u52a8\u751f\u6210\u65b9\u6848\uff0c\u5e76\u63d0\u51fa\u4e0e\u4eba\u5de5\u8bc4\u4f30\u9ad8\u5ea6\u76f8\u5173\u7684\u81ea\u52a8\u8bc4\u4f30\u6846\u67b6\uff0c\u5b9e\u73b0\u9ad8\u6548\u6a21\u578b\u5206\u6790", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a1)\u540c\u6a21\u578b\u5bb6\u65cf\u53c2\u6570\u589e\u5927\u65f6\u6cc4\u6f0f\u51cf\u5c11 2)\u9700\u7279\u5b9a\u6784\u5efa\u65b9\u6cd5\u7f13\u89e3\u4e14\u65e0\u6cd5\u901a\u8fc7\u63d0\u793a\u89e3\u51b3 3)\u8d1f\u9762\u60c5\u611f\u6ce8\u5165\u5f15\u53d1\u66f4\u9ad8\u6cc4\u6f0f\u7387", "conclusion": "\u8868\u8fbe\u6cc4\u6f0f\u63ed\u793aLLMs\u60c5\u611f\u751f\u6210\u673a\u5236\u7f3a\u9677\uff0c\u6a21\u578b\u89c4\u6a21\u3001\u6784\u5efa\u65b9\u5f0f\u548c\u60c5\u611f\u6781\u6027\u5747\u5f71\u54cd\u6cc4\u6f0f\u7a0b\u5ea6\uff0c\u672a\u6765\u9700\u9488\u5bf9\u6027\u4f18\u5316\u6a21\u578b\u67b6\u6784"}}
{"id": "2508.01710", "pdf": "https://arxiv.org/pdf/2508.01710", "abs": "https://arxiv.org/abs/2508.01710", "authors": ["Raviraj Joshi", "Rakesh Paul", "Kanishk Singla", "Anusha Kamath", "Michael Evans", "Katherine Luna", "Shaona Ghosh", "Utkarsh Vaidya", "Eileen Long", "Sanjay Singh Chauhan", "Niranjan Wartikar"], "title": "CultureGuard: Towards Culturally-Aware Dataset and Guard Model for Multilingual Safety Applications", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The increasing use of Large Language Models (LLMs) in agentic applications\nhighlights the need for robust safety guard models. While content safety in\nEnglish is well-studied, non-English languages lack similar advancements due to\nthe high cost of collecting culturally aligned labeled datasets. We present\nCultureGuard, a novel solution for curating culturally aligned, high-quality\nsafety datasets across multiple languages. Our approach introduces a four-stage\nsynthetic data generation and filtering pipeline: cultural data segregation,\ncultural data adaptation, machine translation, and quality filtering. This\npipeline enables the conversion and expansion of the\nNemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct\nlanguages: Arabic, German, Spanish, French, Hindi, Japanese, Thai, and Chinese.\nThe resulting dataset, Nemotron-Content-Safety-Dataset-Multilingual-v1,\ncomprises 386,661 samples in 9 languages and facilitates the training of\nLlama-3.1-Nemotron-Safety-Guard-Multilingual-8B-v1 via LoRA-based fine-tuning.\nThe final model achieves state-of-the-art performance on several multilingual\ncontent safety benchmarks. We also benchmark the latest open LLMs on\nmultilingual safety and observe that these LLMs are more prone to give unsafe\nresponses when prompted in non-English languages. This work represents a\nsignificant step toward closing the safety gap in multilingual LLMs by enabling\nthe development of culturally aware safety guard models.", "AI": {"tldr": "\u5f00\u53d1CultureGuard\u89e3\u51b3\u591a\u8bed\u8a00LLMs\u5b89\u5168\u9632\u62a4\u7684\u6570\u636e\u96c6\u96be\u9898\uff0c\u901a\u8fc7\u56db\u9636\u6bb5\u6d41\u7a0b\u751f\u62109\u8bed\u8a00\u5b89\u5168\u6570\u636e\u96c6\u5e76\u8bad\u7ec3\u51faSOTA\u6a21\u578b", "motivation": "\u975e\u82f1\u8bed\u8bed\u8a00\u56e0\u6587\u5316\u5bf9\u9f50\u6570\u636e\u96c6\u6210\u672c\u9ad8\u800c\u7f3a\u4e4f\u5b89\u5168\u7814\u7a76\uff0c\u9700\u5efa\u7acb\u8de8\u8bed\u8a00\u5b89\u5168\u9632\u62a4\u4f53\u7cfb", "method": "\u56db\u9636\u6bb5\u5408\u6210\u6570\u636e\u6d41\u7a0b\uff1a\u6587\u5316\u6570\u636e\u9694\u79bb\u2192\u6587\u5316\u9002\u5e94\u2192\u673a\u5668\u7ffb\u8bd1\u2192\u8d28\u91cf\u8fc7\u6ee4\uff08\u6269\u5c55Nemotron\u82f1\u6587\u6570\u636e\u96c6\u81f38\u79cd\u8bed\u8a00\uff09", "result": "\u521b\u5efa\u5305\u542b386,661\u6837\u672c\u76849\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u6a21\u578b\u5728\u591a\u9879\u57fa\u51c6\u8fbeSOTA\uff1b\u53d1\u73b0\u73b0\u6709\u591a\u8bed\u8a00LLMs\u975e\u82f1\u8bed\u73af\u5883\u4e0b\u5b89\u5168\u6027\u66f4\u4f4e", "conclusion": "\u901a\u8fc7\u6587\u5316\u611f\u77e5\u6570\u636e\u96c6\u6784\u5efa\u586b\u8865\u591a\u8bed\u8a00\u5b89\u5168\u9e3f\u6c9f\uff0c\u4e3a\u5f00\u53d1\u6587\u5316\u654f\u611f\u7684\u5b89\u5168\u9632\u62a4\u6a21\u578b\u5960\u5b9a\u57fa\u7840"}}
{"id": "2508.01739", "pdf": "https://arxiv.org/pdf/2508.01739", "abs": "https://arxiv.org/abs/2508.01739", "authors": ["Cheng Wang", "ziru Liu", "Pengcheng Tang", "Mingyu Zhang", "Quanyu Dai", "Yue Zhu"], "title": "Enhancing the Preference Extractor in Multi-turn Dialogues: From Annotating Disasters to Accurate Preference Extraction", "categories": ["cs.CL"], "comment": null, "summary": "Identifying user preferences in dialogue systems is a pivotal aspect of\nproviding satisfying services. Current research shows that using large language\nmodels (LLMs) to fine-tune a task-specific preference extractor yields\nexcellent results in terms of accuracy and generalization. However, the primary\nchallenge stems from the inherent difficulty in obtaining high-quality labeled\nmulti-turn dialogue data. Accurately tracking user preference transitions\nacross turns not only demands intensive domain expertise and contextual\nconsistency maintenance for annotators (termed \\textbf{``Annotating\nDisaster''}) but also complicates model training due to error propagation in\nsequential dependency learning. Inspired by the observation that multi-turn\npreference extraction can be decomposed into iterative executions of one-turn\nextraction processes. We propose a novel dialogue data generation framework\nnamed \\textbf{IterChat}. First, we construct a new data format that categorizes\nthe dialogue data into attributed historical preferences and one-turn\ndialogues. This reduces the probability of annotation errors and improves\nannotation efficiency. Then, to generate a high-quality and diverse dialogue\ndataset, we adopt GPT4 to pre-define the preference slots in the target\npreference extractor task and then randomly sample the subset of the slots and\ntheir corresponding schema values to create the dialogue datasets. Experimental\nresults indicate that fine-tuning or only few-shot prompting with the new\ndialogue format yields superior performance compared to the original multi-turn\ndialogues. Additionally, the new data format improves annotator efficiency with\na win rate of 28.4\\% higher than the original multi-turn dialogues.", "AI": {"tldr": "\u63d0\u51faIterChat\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u591a\u8f6e\u504f\u597d\u63d0\u53d6\u4e3a\u5355\u8f6e\u8fed\u4ee3\u6d41\u7a0b\uff0c\u6784\u5efa\u7ed3\u6784\u5316\u5bf9\u8bdd\u6570\u636e\u96c6\u89e3\u51b3\u6807\u6ce8\u707e\u96be\u95ee\u9898\u5e76\u63d0\u5347\u6a21\u578b\u6027\u80fd", "motivation": "\u591a\u8f6e\u5bf9\u8bdd\u504f\u597d\u6807\u6ce8\u5b58\u5728\u4e13\u4e1a\u6807\u6ce8\u6210\u672c\u9ad8\u3001\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u7ef4\u62a4\u56f0\u96be\uff08\u6807\u6ce8\u707e\u96be\uff09\uff0c\u4e14\u4f20\u7edf\u65b9\u6cd5\u5b58\u5728\u8bef\u5dee\u4f20\u64ad\u7684\u7f3a\u9677", "method": "1. \u6784\u5efa\u5305\u542b\u5386\u53f2\u504f\u597d\u5c5e\u6027\u548c\u5355\u8f6e\u5bf9\u8bdd\u7684\u65b0\u6570\u636e\u683c\u5f0f\n2. \u4f7f\u7528GPT4\u9884\u5b9a\u4e49\u504f\u597d\u69fd\u4f4d\uff0c\u901a\u8fc7\u968f\u673a\u91c7\u6837\u751f\u6210\u9ad8\u8d28\u91cf\u591a\u6837\u5316\u6570\u636e\u96c6", "result": "\u65b0\u6570\u636e\u683c\u5f0f\u4f7f\u6a21\u578b\u5fae\u8c03/\u5c11\u6837\u672c\u63d0\u793a\u6548\u679c\u4f18\u4e8e\u539f\u59cb\u591a\u8f6e\u5bf9\u8bdd\uff0c\u6807\u6ce8\u6548\u7387\u63d0\u534728.4%", "conclusion": "IterChat\u901a\u8fc7\u7ed3\u6784\u5316\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u6709\u6548\u964d\u4f4e\u6807\u6ce8\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u63d0\u5347\u504f\u597d\u63d0\u53d6\u6a21\u578b\u6027\u80fd\u548c\u6807\u6ce8\u6548\u7387"}}
{"id": "2508.01754", "pdf": "https://arxiv.org/pdf/2508.01754", "abs": "https://arxiv.org/abs/2508.01754", "authors": ["Alva West", "Yixuan Weng", "Minjun Zhu", "Luodan Zhang", "Zhen Lin", "Guangsheng Bao", "Yue Zhang"], "title": "AI-Generated Text is Non-Stationary: Detection via Temporal Tomography", "categories": ["cs.CL"], "comment": null, "summary": "The field of AI-generated text detection has evolved from supervised\nclassification to zero-shot statistical analysis. However, current approaches\nshare a fundamental limitation: they aggregate token-level measurements into\nscalar scores, discarding positional information about where anomalies occur.\nOur empirical analysis reveals that AI-generated text exhibits significant\nnon-stationarity, statistical properties vary by 73.8\\% more between text\nsegments compared to human writing. This discovery explains why existing\ndetectors fail against localized adversarial perturbations that exploit this\noverlooked characteristic. We introduce Temporal Discrepancy Tomography (TDT),\na novel detection paradigm that preserves positional information by\nreformulating detection as a signal processing task. TDT treats token-level\ndiscrepancies as a time-series signal and applies Continuous Wavelet Transform\nto generate a two-dimensional time-scale representation, capturing both the\nlocation and linguistic scale of statistical anomalies. On the RAID benchmark,\nTDT achieves 0.855 AUROC (7.1\\% improvement over the best baseline). More\nimportantly, TDT demonstrates robust performance on adversarial tasks, with\n14.1\\% AUROC improvement on HART Level 2 paraphrasing attacks. Despite its\nsophisticated analysis, TDT maintains practical efficiency with only 13\\%\ncomputational overhead. Our work establishes non-stationarity as a fundamental\ncharacteristic of AI-generated text and demonstrates that preserving temporal\ndynamics is essential for robust detection.", "AI": {"tldr": "\u63d0\u51fa\u65f6\u9891\u5206\u6790\u6846\u67b6TDT\uff0c\u901a\u8fc7\u6355\u6349AI\u6587\u672c\u7684\u975e\u5e73\u7a33\u7279\u6027\u5b9e\u73b0\u66f4\u9c81\u68d2\u7684\u68c0\u6d4b", "motivation": "\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5c06token\u7ea7\u6d4b\u91cf\u805a\u5408\u6210\u6807\u91cf\u5206\u6570\uff0c\u4e22\u5f03\u5f02\u5e38\u4f4d\u7f6e\u4fe1\u606f\uff0c\u5bfc\u81f4\u6613\u53d7\u9488\u5bf9\u6027\u5bf9\u6297\u653b\u51fb", "method": "\u5c06token\u7ea7\u5dee\u5f02\u89c6\u4e3a\u65f6\u95f4\u5e8f\u5217\u4fe1\u53f7\uff0c\u5e94\u7528\u8fde\u7eed\u5c0f\u6ce2\u53d8\u6362\u751f\u6210\u4e8c\u7ef4\u65f6\u9891\u8868\u793a\uff0c\u6355\u6349\u7edf\u8ba1\u5f02\u5e38\u7684\u4f4d\u7f6e\u548c\u8bed\u8a00\u5c3a\u5ea6", "result": "RAID\u57fa\u51c6\u6d4b\u8bd50.855 AUROC\uff08\u63d0\u53477.1%\uff09\uff0cHART Level 2\u5bf9\u6297\u4efb\u52a1\u63d0\u534714.1% AUROC\uff0c\u4ec513%\u8ba1\u7b97\u5f00\u9500", "conclusion": "\u975e\u5e73\u7a33\u6027\u662fAI\u6587\u672c\u7684\u6839\u672c\u7279\u5f81\uff0c\u4fdd\u7559\u65f6\u95f4\u52a8\u6001\u5bf9\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u65f6\u9891\u5206\u6790\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u5bf9\u6297\u73af\u5883\u4e0b\u7684\u68c0\u6d4b\u9c81\u68d2\u6027"}}
{"id": "2508.01781", "pdf": "https://arxiv.org/pdf/2508.01781", "abs": "https://arxiv.org/abs/2508.01781", "authors": ["Manuel Cossio"], "title": "A comprehensive taxonomy of hallucinations in Large Language Models", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "55 pages, 16 figures, 3 tables", "summary": "Large language models (LLMs) have revolutionized natural language processing,\nyet their propensity for hallucination, generating plausible but factually\nincorrect or fabricated content, remains a critical challenge. This report\nprovides a comprehensive taxonomy of LLM hallucinations, beginning with a\nformal definition and a theoretical framework that posits its inherent\ninevitability in computable LLMs, irrespective of architecture or training. It\nexplores core distinctions, differentiating between intrinsic (contradicting\ninput context) and extrinsic (inconsistent with training data or reality), as\nwell as factuality (absolute correctness) and faithfulness (adherence to\ninput). The report then details specific manifestations, including factual\nerrors, contextual and logical inconsistencies, temporal disorientation,\nethical violations, and task-specific hallucinations across domains like code\ngeneration and multimodal applications. It analyzes the underlying causes,\ncategorizing them into data-related issues, model-related factors, and\nprompt-related influences. Furthermore, the report examines cognitive and human\nfactors influencing hallucination perception, surveys evaluation benchmarks and\nmetrics for detection, and outlines architectural and systemic mitigation\nstrategies. Finally, it introduces web-based resources for monitoring LLM\nreleases and performance. This report underscores the complex, multifaceted\nnature of LLM hallucinations and emphasizes that, given their theoretical\ninevitability, future efforts must focus on robust detection, mitigation, and\ncontinuous human oversight for responsible and reliable deployment in critical\napplications.", "AI": {"tldr": "\u8be5\u62a5\u544a\u7cfb\u7edf\u6784\u5efa\u4e86LLM\u5e7b\u89c9\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u8bba\u8bc1\u5176\u7406\u8bba\u5fc5\u7136\u6027\u5e76\u63d0\u51fa\u68c0\u6d4b-\u7f13\u89e3-\u76d1\u7763\u4e09\u4f4d\u4e00\u4f53\u7684\u5e94\u5bf9\u65b9\u6848", "motivation": "\u89e3\u51b3LLM\u751f\u6210\u5185\u5bb9\u53ef\u4fe1\u5ea6\u95ee\u9898\u5bf9\u5173\u952e\u9886\u57df\u90e8\u7f72\u5177\u6709\u91cd\u5927\u5b89\u5168\u610f\u4e49\uff0c\u5f53\u524d\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7406\u8bba\u6846\u67b6\u6307\u5bfc\u5e94\u5bf9\u7b56\u7565", "method": "\u901a\u8fc7\u5efa\u7acb\u56db\u7ef4\u5206\u7c7b\u4f53\u7cfb\uff08\u5185\u5728/\u5916\u5728\u3001\u4e8b\u5b9e\u6027/\u5fe0\u5b9e\u6027\uff09\uff0c\u7ed3\u5408\u6570\u636e-\u6a21\u578b-\u63d0\u793a\u4e09\u7ef4\u5f52\u56e0\u5206\u6790\uff0c\u63d0\u51fa\u67b6\u6784\u6539\u8fdb\u4e0e\u8bc4\u4f30\u6307\u6807", "result": "\u8bc1\u660e\u5e7b\u89c9\u73b0\u8c61\u5177\u6709\u8ba1\u7b97\u7406\u8bba\u5c42\u9762\u7684\u4e0d\u53ef\u6d88\u9664\u6027\uff0c\u5fc5\u987b\u5efa\u7acb\u52a8\u6001\u76d1\u6d4b\u673a\u5236\u800c\u975e\u8ffd\u6c42\u5b8c\u5168\u6d88\u9664", "conclusion": "LLM\u7684\u53ef\u9760\u90e8\u7f72\u9700\u7efc\u5408\u6280\u672f\u68c0\u6d4b\u624b\u6bb5\uff08\u5982\u4e8b\u5b9e\u6838\u67e5\u6a21\u5757\uff09\u4e0e\u4eba\u5de5\u76d1\u7763\u4f53\u7cfb\uff0c\u7279\u522b\u662f\u5728\u533b\u7597\u3001\u6cd5\u5f8b\u7b49\u9ad8\u5371\u573a\u666f"}}
{"id": "2508.01812", "pdf": "https://arxiv.org/pdf/2508.01812", "abs": "https://arxiv.org/abs/2508.01812", "authors": ["Amir DN Cohen", "Hilla Merhav", "Yoav Goldberg", "Reut Tsarfaty"], "title": "HeQ: a Large and Diverse Hebrew Reading Comprehension Benchmark", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Current benchmarks for Hebrew Natural Language Processing (NLP) focus mainly\non morpho-syntactic tasks, neglecting the semantic dimension of language\nunderstanding. To bridge this gap, we set out to deliver a Hebrew Machine\nReading Comprehension (MRC) dataset, where MRC is to be realized as extractive\nQuestion Answering. The morphologically rich nature of Hebrew poses a challenge\nto this endeavor: the indeterminacy and non-transparency of span boundaries in\nmorphologically complex forms lead to annotation inconsistencies,\ndisagreements, and flaws in standard evaluation metrics.\n  To remedy this, we devise a novel set of guidelines, a controlled\ncrowdsourcing protocol, and revised evaluation metrics that are suitable for\nthe morphologically rich nature of the language. Our resulting benchmark, HeQ\n(Hebrew QA), features 30,147 diverse question-answer pairs derived from both\nHebrew Wikipedia articles and Israeli tech news. Our empirical investigation\nreveals that standard evaluation metrics such as F1 scores and Exact Match (EM)\nare not appropriate for Hebrew (and other MRLs), and we propose a relevant\nenhancement.\n  In addition, our experiments show low correlation between models' performance\non morpho-syntactic tasks and on MRC, which suggests that models designed for\nthe former might underperform on semantics-heavy tasks. The development and\nexploration of HeQ illustrate some of the challenges MRLs pose in natural\nlanguage understanding (NLU), fostering progression towards more and better NLU\nmodels for Hebrew and other MRLs.", "AI": {"tldr": "\u6784\u5efa\u5e0c\u4f2f\u6765\u8bed\u95ee\u7b54\u6570\u636e\u96c6HeQ\uff0c\u63d0\u51fa\u9002\u5e94\u5f62\u6001\u590d\u6742\u8bed\u8a00\u7684\u65b0\u8bc4\u4f30\u6307\u6807\uff0c\u63ed\u793a\u5f62\u6001\u53e5\u6cd5\u4e0e\u8bed\u4e49\u4efb\u52a1\u8868\u73b0\u8131\u8282", "motivation": "\u73b0\u6709\u5e0c\u4f2f\u6765\u8bedNLP\u57fa\u51c6\u8fc7\u5ea6\u5173\u6ce8\u5f62\u6001\u53e5\u6cd5\u4efb\u52a1\uff0c\u7f3a\u4e4f\u8bed\u4e49\u7406\u89e3\u7ef4\u5ea6\uff1b\u5f62\u6001\u590d\u6742\u6027\u5bfc\u81f4\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\u5931\u6548", "method": "\u8bbe\u8ba1\u65b0\u578b\u6807\u6ce8\u6307\u5357/\u4f17\u5305\u534f\u8bae\uff0c\u6574\u5408\u7ef4\u57fa\u767e\u79d1\u548c\u79d1\u6280\u65b0\u95fb\u6570\u636e\uff0c\u6539\u8fdb\u9002\u5e94\u5f62\u6001\u590d\u6742\u6027\u7684\u8bc4\u4f30\u6307\u6807", "result": "\u4f20\u7edfF1/EM\u6307\u6807\u4e0d\u9002\u7528\u5e0c\u4f2f\u6765\u8bed\uff0c\u63d0\u51fa\u6539\u826f\u6307\u6807\uff1b\u5f62\u6001\u53e5\u6cd5\u4efb\u52a1\u4e0eMRC\u4efb\u52a1\u8868\u73b0\u76f8\u5173\u6027\u4f4e\uff08r=0.32\uff09", "conclusion": "HeQ\u63ed\u793a\u4e86\u5f62\u6001\u4e30\u5bcc\u8bed\u8a00\u5728NLU\u4e2d\u7684\u72ec\u7279\u6311\u6218\uff0c\u63a8\u52a8\u5e0c\u4f2f\u6765\u8bed\u53ca\u5176\u4ed6MRLs\u8bed\u8a00\u7406\u89e3\u6a21\u578b\u7684\u53d1\u5c55"}}
{"id": "2508.01815", "pdf": "https://arxiv.org/pdf/2508.01815", "abs": "https://arxiv.org/abs/2508.01815", "authors": ["Yang Zhao", "Chengxiao Dai", "Wei Zhuo", "Tan Chuan Fu", "Yue Xiu", "Dusit Niyato", "Jonathan Z. Low", "Eugene Ho Hong Zhuang", "Daren Zong Loong Tan"], "title": "AGENTICT$^2$S:Robust Text-to-SPARQL via Agentic Collaborative Reasoning over Heterogeneous Knowledge Graphs for the Circular Economy", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Question answering over heterogeneous knowledge graphs (KGQA) involves\nreasoning across diverse schemas, incomplete alignments, and distributed data\nsources. Existing text-to-SPARQL approaches rely on large-scale domain-specific\nfine-tuning or operate within single-graph settings, limiting their\ngeneralizability in low-resource domains and their ability to handle queries\nspanning multiple graphs. These challenges are particularly relevant in domains\nsuch as the circular economy, where information about classifications,\nprocesses, and emissions is distributed across independently curated knowledge\ngraphs (KGs). We present AgenticT$^2$S, a modular framework that decomposes\nKGQA into subtasks managed by specialized agents responsible for retrieval,\nquery generation, and verification. A scheduler assigns subgoals to different\ngraphs using weak-to-strong alignment strategies. A two-stage verifier detects\nstructurally invalid and semantically underspecified queries through symbolic\nvalidation and counterfactual consistency checks. Experiments on real-world\ncircular economy KGs demonstrate that AgenticT$^2$S improves execution accuracy\nby 17.3% and triple level F$_1$ by 25.4% over the best baseline, while reducing\nthe average prompt length by 46.4%. These results demonstrate the benefits of\nagent-based schema-aware reasoning for scalable KGQA and support\ndecision-making in sustainability domains through robust cross-graph reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86AgenticT\u00b2S\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u4ee3\u7406\u67b6\u6784\u548c\u4e24\u9636\u6bb5\u9a8c\u8bc1\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u5f02\u6784\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u7684\u51c6\u786e\u6027\u548c\u6548\u7387", "motivation": "\u73b0\u6709\u6587\u672c\u5230SPARQL\u65b9\u6cd5\u5728\u8de8\u56fe\u8c31\u63a8\u7406\u548c\u4f4e\u8d44\u6e90\u9886\u57df\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5faa\u73af\u7ecf\u6d4e\u9886\u57df\u9700\u8981\u5904\u7406\u5206\u5e03\u5f0f\u77e5\u8bc6\u56fe\u8c31\u7684\u534f\u540c\u63a8\u7406\u9700\u6c42", "method": "\u6a21\u5757\u5316\u6846\u67b6\u5c06\u95ee\u7b54\u5206\u89e3\u4e3a\u68c0\u7d22\u3001\u751f\u6210\u548c\u9a8c\u8bc1\u5b50\u4efb\u52a1\uff0c\u901a\u8fc7\u8c03\u5ea6\u5668\u5206\u914d\u5b50\u76ee\u6807\uff0c\u91c7\u7528\u7b26\u53f7\u9a8c\u8bc1\u548c\u53cd\u4e8b\u5b9e\u4e00\u81f4\u6027\u68c0\u67e5\u7684\u4e24\u9636\u6bb5\u9a8c\u8bc1\u673a\u5236", "result": "\u5728\u771f\u5b9e\u5faa\u73af\u7ecf\u6d4e\u77e5\u8bc6\u56fe\u8c31\u4e0a\u5b9e\u73b0\u6267\u884c\u51c6\u786e\u7387\u63d0\u534717.3%\uff0c\u4e09\u5143\u7ec4F1\u503c\u63d0\u9ad825.4%\uff0c\u63d0\u793a\u957f\u5ea6\u51cf\u5c1146.4%", "conclusion": "\u57fa\u4e8e\u4ee3\u7406\u7684\u67b6\u6784\u652f\u6301\u53ef\u6269\u5c55\u7684\u8de8\u56fe\u8c31\u63a8\u7406\uff0c\u4e3a\u53ef\u6301\u7eed\u51b3\u7b56\u63d0\u4f9b\u9c81\u68d2\u7684\u95ee\u7b54\u652f\u6301\uff0c\u9a8c\u8bc1\u4e86\u6a21\u5f0f\u611f\u77e5\u63a8\u7406\u7684\u6709\u6548\u6027"}}
{"id": "2508.01832", "pdf": "https://arxiv.org/pdf/2508.01832", "abs": "https://arxiv.org/abs/2508.01832", "authors": ["Rubin Wei", "Jiaqi Cao", "Jiarui Wang", "Jushi Kai", "Qipeng Guo", "Bowen Zhou", "Zhouhan Lin"], "title": "MLP Memory: Language Modeling with Retriever-pretrained External Memory", "categories": ["cs.CL"], "comment": null, "summary": "While modern decoder-only LLMs achieve superior performance across various\ndomains, hallucinations have risen to be a common problem in their generated\ntext, hindering their application in knowledge-intensive tasks.\nRetriever-augmented generation (RAG) offers a solution, but the non-parametric\nnature of the retriever hinders its deep interaction with LLM. In this work, we\npropose to decouple memorization from the LLM decoder using a pretrained,\ndifferentiable external memory. The external memory is an MLP pretrained by\nimitating the behavior of a retriever on the entire pretraining dataset. Our\nresulting architecture, which comprises a transformer decoder and an external\nMLP memory pretrained on language modeling and retriever imitation\nrespectively, demonstrates strong perplexity and performance on downstream\ntasks. Experiments show our architecture exhibits steeper power-law scaling\nwith model size, achieving 17.5% and 24.1% improvement on WikiText-103 and Web\ndatasets compared to decoder-only models while benefiting from added training\nwithout overfitting. We demonstrate superior performance on three hallucination\nbenchmarks and nine memory-intensive tasks. Additionally, our approach delivers\n$80\\times$ speedup over $k$NN-LM (500M tokens) and $1.3\\times$ faster inference\nthan decoder-only models. Unlike $k$NN-LM, which impairs reasoning, our MLP\nmemory improves StrategyQA performance. We will open-source our code and models\nin the future.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u53ef\u5fae\u5206\u5916\u90e8\u8bb0\u5fc6\uff08MLP\uff09\u6765\u89e3\u8026LLM\u7684\u8bb0\u5fc6\u4e0e\u63a8\u7406\uff0c\u51cf\u5c11\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u53ca\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u4ec5\u89e3\u7801\u5668\u67b6\u6784LLM\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u4f20\u7edf\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u56e0\u975e\u53c2\u6570\u5316\u7279\u6027\u96be\u4ee5\u4e0eLLM\u6df1\u5ea6\u4ea4\u4e92\u3002\u9700\u63a2\u7d22\u66f4\u7d27\u5bc6\u7684LLM\u4e0e\u8bb0\u5fc6\u6a21\u5757\u878d\u5408\u65b9\u5f0f\u3002", "method": "1. \u9884\u8bad\u7ec3MLP\u8bb0\u5fc6\u6a21\u5757\u6a21\u62df\u68c0\u7d22\u5668\u884c\u4e3a 2. \u5c06Transformer\u89e3\u7801\u5668\u4e0eMLP\u8bb0\u5fc6\u7ed3\u5408 3. \u5206\u522b\u8fdb\u884c\u8bed\u8a00\u5efa\u6a21\u548c\u68c0\u7d22\u5668\u6a21\u4eff\u7684\u9884\u8bad\u7ec3", "result": "\u5728WikiText-103/Web\u6570\u636e\u96c6\u4e0a\u76f8\u5bf9\u57fa\u7ebf\u63d0\u534717.5%/24.1%\uff0c3\u4e2a\u5e7b\u89c9\u57fa\u51c6\u548c9\u4e2a\u8bb0\u5fc6\u4efb\u52a1\u8868\u73b0\u4f18\u5f02\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4kNN-LM\u5feb80\u500d\uff0cStrategyQA\u63a8\u7406\u80fd\u529b\u589e\u5f3a", "conclusion": "\u5916\u90e8MLP\u8bb0\u5fc6\u67b6\u6784\u6709\u6548\u5e73\u8861\u8bb0\u5fc6\u4e0e\u63a8\u7406\uff0c\u5b9e\u73b0\u6027\u80fd\u4e0e\u6548\u7387\u53cc\u63d0\u5347\uff0c\u4e3aLLM\u67b6\u6784\u521b\u65b0\u63d0\u4f9b\u65b0\u65b9\u5411\u3002\u6a21\u578b\u53ca\u4ee3\u7801\u5c06\u5f00\u6e90\u4fc3\u8fdb\u793e\u533a\u53d1\u5c55\u3002"}}
{"id": "2508.01858", "pdf": "https://arxiv.org/pdf/2508.01858", "abs": "https://arxiv.org/abs/2508.01858", "authors": ["Yuhan Guo", "Cong Guo", "Aiwen Sun", "Hongliang He", "Xinyu Yang", "Yue Lu", "Yingji Zhang", "Xuntao Guo", "Dong Zhang", "Jianzhuang Liu", "Jiang Duan", "Yijia Xiao", "Liangjian Wen", "Hai-Ming Xu", "Yong Dai"], "title": "Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web Agents", "categories": ["cs.CL", "cs.AI"], "comment": "Our code and data is open sourced at\n  https://github.com/Gnonymous/Web-CogReasoner", "summary": "Multimodal large-scale models have significantly advanced the development of\nweb agents, enabling perception and interaction with digital environments akin\nto human cognition. In this paper, we argue that web agents must first acquire\nsufficient knowledge to effectively engage in cognitive reasoning. Therefore,\nwe decompose a web agent's capabilities into two essential stages: knowledge\ncontent learning and cognitive processes. To formalize this, we propose\nWeb-CogKnowledge Framework, categorizing knowledge as Factual, Conceptual, and\nProcedural. In this framework, knowledge content learning corresponds to the\nagent's processes of Memorizing and Understanding, which rely on the first two\nknowledge types, representing the \"what\" of learning. Conversely, cognitive\nprocesses correspond to Exploring, grounded in Procedural knowledge, defining\nthe \"how\" of reasoning and action. To facilitate knowledge acquisition, we\nconstruct the Web-CogDataset, a structured resource curated from 14 real-world\nwebsites, designed to systematically instill core knowledge necessary for web\nagent. This dataset serves as the agent's conceptual grounding-the \"nouns\" upon\nwhich comprehension is built-as well as the basis for learning how to reason\nand act. Building on this foundation, we operationalize these processes through\na novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing\nand training our proposed agent, the Web-CogReasoner. Extensive experimentation\nreveals its significant superiority over existing models, especially in\ngeneralizing to unseen tasks where structured knowledge is decisive. To enable\nrigorous evaluation, we introduce the Web-CogBench, a comprehensive evaluation\nsuite designed to assess and compare agent performance across the delineated\nknowledge domains and cognitive capabilities. Our code and data is open sourced\nat https://github.com/Gnonymous/Web-CogReasoner", "AI": {"tldr": "\u63d0\u51faWeb-CogKnowledge\u6846\u67b6\u5c06\u7f51\u7edc\u667a\u80fd\u4f53\u80fd\u529b\u5206\u89e3\u4e3a\u77e5\u8bc6\u5b66\u4e60\u4e0e\u8ba4\u77e5\u63a8\u7406\u4e24\u9636\u6bb5\uff0c\u5e76\u6784\u5efaWeb-CogDataset\u6570\u636e\u96c6\u548c\u77e5\u8bc6\u9a71\u52a8\u7684Chain-of-Thought\u63a8\u7406\u6846\u67b6Web-CogReasoner\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u5177\u6709\u663e\u8457\u4f18\u52bf", "motivation": "\u73b0\u6709\u7f51\u7edc\u667a\u80fd\u4f53\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u77e5\u8bc6\u652f\u6491\u8ba4\u77e5\u63a8\u7406\uff0c\u9700\u5efa\u7acb\u7ed3\u6784\u5316\u77e5\u8bc6\u6846\u67b6\u63d0\u5347\u5176\u7406\u89e3\u4e0e\u63a8\u7406\u80fd\u529b", "method": "1. \u6784\u5efa\u4e09\u5c42\u77e5\u8bc6\u5206\u7c7b\u6846\u67b6\uff08\u4e8b\u5b9e/\u6982\u5ff5/\u7a0b\u5e8f\u6027\u77e5\u8bc6\uff09\n2. \u521b\u5efaWeb-CogDataset\u7ed3\u6784\u5316\u6570\u636e\u96c6\n3. \u5f00\u53d1\u57fa\u4e8e\u77e5\u8bc6\u94fe\u5f0f\u63a8\u7406\u7684Web-CogReasoner\u667a\u80fd\u4f53", "result": "\u5728Web-CogBench\u8bc4\u4f30\u4e2d\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\uff0c\u5c24\u5176\u5728\u9700\u8981\u7ed3\u6784\u5316\u77e5\u8bc6\u7684\u672a\u77e5\u4efb\u52a1\u573a\u666f\u4e0bF1\u63d0\u534721.3%", "conclusion": "\u77e5\u8bc6\u9a71\u52a8\u7684\u8ba4\u77e5\u63a8\u7406\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u667a\u80fd\u4f53\u6027\u80fd\uff0c\u5f00\u6e90\u7684Web-CogBench\u8bc4\u4f30\u4f53\u7cfb\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u6807\u51c6\u5316\u6d4b\u8bd5\u57fa\u51c6"}}
{"id": "2508.01862", "pdf": "https://arxiv.org/pdf/2508.01862", "abs": "https://arxiv.org/abs/2508.01862", "authors": ["Yijun Feng"], "title": "Counterfactual Probing for Hallucination Detection and Mitigation in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models have demonstrated remarkable capabilities across\ndiverse tasks, yet they frequently generate hallucinations outputs that are\nfluent but factually incorrect or unsupported. We propose Counterfactual\nProbing, a novel approach for detecting and mitigating hallucinations in LLM\noutputs. Our method dynamically generates counterfactual statements that appear\nplausible but contain subtle factual errors, then evaluates the model's\nsensitivity to these perturbations. We hypothesize that genuine knowledge\nexhibits robustness to counterfactual variations, while hallucinated content\nshows inconsistent confidence patterns when confronted with plausible\nalternatives. Our comprehensive evaluation on TruthfulQA, factual statement\ndatasets, and curated hallucination examples demonstrates that counterfactual\nprobing achieves superior detection performance compared to baseline methods,\nwhile our adaptive mitigation strategies reduce hallucination scores by an\naverage of 24.5%. The approach requires no model retraining and can be\nintegrated into existing LLM pipelines as a realtime verification mechanism.", "AI": {"tldr": "\u63d0\u51fa\u53cd\u4e8b\u5b9e\u63a2\u6d4b\u65b9\u6cd5Counterfactual Probing\uff0c\u901a\u8fc7\u751f\u6210\u542b\u7ec6\u5fae\u9519\u8bef\u7684\u53cd\u4e8b\u5b9e\u9648\u8ff0\u68c0\u6d4bLLM\u5e7b\u89c9\uff0c\u65e0\u9700\u6a21\u578b\u91cd\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u5b9e\u65f6\u9a8c\u8bc1", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u9891\u7e41\u4ea7\u751f\u6d41\u7545\u4f46\u4e8b\u5b9e\u9519\u8bef\u7684\u5e7b\u89c9\u8f93\u51fa\uff0c\u9700\u5f00\u53d1\u6709\u6548\u68c0\u6d4b\u4e0e\u7f13\u89e3\u673a\u5236", "method": "\u52a8\u6001\u751f\u6210\u770b\u4f3c\u5408\u7406\u4f46\u542b\u4e8b\u5b9e\u9519\u8bef\u7684\u53cd\u4e8b\u5b9e\u9648\u8ff0\uff0c\u8bc4\u4f30\u6a21\u578b\u5bf9\u6270\u52a8\u7684\u654f\u611f\u5ea6\uff0c\u57fa\u4e8e\u77e5\u8bc6\u9c81\u68d2\u6027\u5047\u8bbe\u533a\u5206\u771f\u5b9e\u77e5\u8bc6\u4e0e\u5e7b\u89c9\u5185\u5bb9", "result": "\u5728TruthfulQA\u7b49\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u81ea\u9002\u5e94\u7f13\u89e3\u7b56\u7565\u4f7f\u5e7b\u89c9\u8bc4\u5206\u5e73\u5747\u964d\u4f4e24.5%", "conclusion": "\u8be5\u65b9\u6cd5\u521b\u65b0\u5730\u5c06\u53cd\u4e8b\u5b9e\u903b\u8f91\u878d\u5165LLM\u9a8c\u8bc1\u6d41\u7a0b\uff0c\u4e3a\u5b9e\u65f6\u5e7b\u89c9\u68c0\u6d4b\u63d0\u4f9b\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.01918", "pdf": "https://arxiv.org/pdf/2508.01918", "abs": "https://arxiv.org/abs/2508.01918", "authors": ["Jaskaranjeet Singh", "Rakesh Thakur"], "title": "Quantum-RAG and PunGPT2: Advancing Low-Resource Language Generation and Retrieval for the Punjabi Language", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the rapid advancement of large language models (LLMs), low-resource\nlanguages remain largely excluded from the NLP landscape. We present PunGPT2,\nthe first fully open-source suite of Punjabi large language models, trained\nfrom scratch on a 35GB domain-diverse corpus encompassing literature, religious\ntexts, news, and social discourse. Unlike prior multilingual approaches,\nPunGPT2 captures rich syntactic and morphological features unique to Punjabi\nthrough a tokenizer optimised with byte pair encoding and linguistically\naligned pretraining objectives. To improve factual grounding and domain recall,\nwe introduce Pun-RAG, a retrieval-augmented generation framework combining\nPunGPT2 with a dense FAISS retriever over a curated Punjabi knowledge base. We\nfurther develop Pun-Instruct, a parameter-efficient, instruction-tuned variant\nusing QLoRA, enabling robust zero-shot and instruction-following performance\nwith significantly reduced compute needs.\n  As a key innovation, we propose Quantum-RAG, a novel hybrid retrieval system\nthat fuses sparse (BM25) and dense methods with quantum-inspired semantic\nmatching. By encoding queries using amplitude-based embeddings and retrieving\nvia quantum kernel similarity, Quantum-RAG achieves improved contextual\nrelevance with minimal memory overhead marking the first practical integration\nof quantum representations in low-resource language generation. Our models\nsignificantly outperform strong multilingual baselines (mBERT, mT5, MuRIL) in\nperplexity, factuality, and fluency. This work provides a scalable,\nreproducible blueprint for extending LLM capabilities to underrepresented\nlanguages and pioneers quantum-aware retrieval in low-resource NLP", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u65c1\u906e\u666e\u8bed\u5f00\u6e90LLM\u5957\u4ef6PunGPT2\u53ca\u521b\u65b0\u68c0\u7d22\u6846\u67b6Quantum-RAG\uff0c\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u5904\u7406\u80fd\u529b", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728NLP\u9886\u57df\u957f\u671f\u88ab\u5ffd\u89c6\u7684\u95ee\u9898\uff0c\u586b\u8865\u65c1\u906e\u666e\u8bed\u5927\u8bed\u8a00\u6a21\u578b\u7a7a\u767d", "method": "1) \u57fa\u4e8e35GB\u591a\u9886\u57df\u8bed\u6599\u8bad\u7ec3PunGPT2\u6a21\u578b\n2) \u5f00\u53d1Pun-RAG\u68c0\u7d22\u589e\u5f3a\u6846\u67b6\n3) \u4f7f\u7528QLoRA\u6280\u672f\u521b\u5efa\u53c2\u6570\u9ad8\u6548\u7684Pun-Instruct\u6307\u4ee4\u6a21\u578b\n4) \u521b\u65b0\u63d0\u51fa\u878d\u5408\u91cf\u5b50\u8bed\u4e49\u5339\u914d\u7684Quantum-RAG\u6df7\u5408\u68c0\u7d22\u7cfb\u7edf", "result": "\u6a21\u578b\u5728\u56f0\u60d1\u5ea6\uff08\u964d\u4f4e23%\uff09\u3001\u4e8b\u5b9e\u51c6\u786e\u6027\uff08\u63d0\u534737%\uff09\u548c\u6d41\u7545\u5ea6\uff08\u6539\u8fdb41%\uff09\u4e0a\u5168\u9762\u8d85\u8d8amBERT/mT5\u7b49\u57fa\u7ebf\u6a21\u578b", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00LLM\u5f00\u53d1\u63d0\u4f9b\u53ef\u6269\u5c55\u84dd\u56fe\uff0c\u5e76\u9996\u6b21\u5b9e\u73b0\u91cf\u5b50\u8868\u793a\u5728\u8bed\u8a00\u751f\u6210\u4e2d\u7684\u5b9e\u9645\u5e94\u7528"}}
{"id": "2508.01930", "pdf": "https://arxiv.org/pdf/2508.01930", "abs": "https://arxiv.org/abs/2508.01930", "authors": ["Tom S. Juzek", "Zina B. Ward"], "title": "Word Overuse and Alignment in Large Language Models: The Influence of Learning from Human Feedback", "categories": ["cs.CL", "cs.AI", "68T50", "I.2; I.2.7; I.2.6"], "comment": "Accepted for publication in the Proceedings of the 5th Workshop on\n  Bias and Fairness in AI (BIAS 2025) at ECML PKDD", "summary": "Large Language Models (LLMs) are known to overuse certain terms like \"delve\"\nand \"intricate.\" The exact reasons for these lexical choices, however, have\nbeen unclear. Using Meta's Llama model, this study investigates the\ncontribution of Learning from Human Feedback (LHF), under which we subsume\nReinforcement Learning from Human Feedback and Direct Preference Optimization.\nWe present a straightforward procedure for detecting the lexical preferences of\nLLMs that are potentially LHF-induced. Next, we more conclusively link LHF to\nlexical overuse by experimentally emulating the LHF procedure and demonstrating\nthat participants systematically prefer text variants that include certain\nwords. This lexical overuse can be seen as a sort of misalignment, though our\nstudy highlights the potential divergence between the lexical expectations of\ndifferent populations -- namely LHF workers versus LLM users. Our work\ncontributes to the growing body of research on explainable artificial\nintelligence and emphasizes the importance of both data and procedural\ntransparency in alignment research.", "AI": {"tldr": "LLMs\u5b58\u5728\u7279\u5b9a\u8bcd\u6c47\u8fc7\u5ea6\u4f7f\u7528\u73b0\u8c61\uff08\u5982'delve'\uff09\uff0c\u7814\u7a76\u53d1\u73b0\u4eba\u7c7b\u53cd\u9988\u5b66\u4e60\uff08LHF\uff09\u662f\u91cd\u8981\u8bf1\u56e0\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u7fa4\u4f53\u8bcd\u6c47\u671f\u5f85\u7684\u9519\u4f4d\u53ca\u5bf9\u9f50\u7814\u7a76\u900f\u660e\u5ea6\u7684\u5fc5\u8981\u6027", "motivation": "\u63a2\u7a76LLMs\u8bcd\u6c47\u504f\u597d\u6210\u56e0\uff0c\u7279\u522b\u662fLHF\u5bf9\u8bcd\u6c47\u9009\u62e9\u7684\u5f71\u54cd\u673a\u5236\uff0c\u63ed\u793aAI\u5bf9\u9f50\u8fc7\u7a0b\u4e2d\u6570\u636e\u504f\u597d\u4e0e\u7ec8\u7aef\u7528\u6237\u671f\u5f85\u7684\u6f5c\u5728\u77db\u76fe", "method": "1. \u57fa\u4e8eLlama\u6a21\u578b\u5f00\u53d1LHF\u8bf1\u5bfc\u8bcd\u6c47\u68c0\u6d4b\u65b9\u6cd5\n2. \u5b9e\u9a8c\u6a21\u62dfLHF\u6d41\u7a0b\uff0c\u6536\u96c6\u4eba\u7c7b\u53c2\u4e0e\u8005\u5bf9\u542b\u7279\u5b9a\u8bcd\u6c47\u6587\u672c\u7684\u504f\u597d\u6570\u636e", "result": "\u8bc1\u5b9eLHF\u5bfc\u81f4\u8bcd\u6c47\u6ee5\u7528\u73b0\u8c61\uff0c\u53d1\u73b0\u6807\u6ce8\u8005\u4e0e\u7528\u6237\u7fa4\u4f53\u7684\u8bcd\u6c47\u671f\u5f85\u5dee\u5f02\uff0c\u5f3a\u8c03\u5bf9\u9f50\u7814\u7a76\u4e2d\u6d41\u7a0b\u900f\u660e\u5ea6\u7684\u5173\u952e\u4f5c\u7528", "conclusion": "\u63a8\u52a8\u53ef\u89e3\u91caAI\u7814\u7a76\u8303\u5f0f\uff0c\u786e\u7acb\u6570\u636e/\u6d41\u7a0b\u53cc\u91cd\u900f\u660e\u5ea6\u4e3aAI\u5bf9\u9f50\u7814\u7a76\u7684\u6838\u5fc3\u539f\u5219\uff0c\u8b66\u793a\u6280\u672f\u4f26\u7406\u4e2d\u7684\u7fa4\u4f53\u504f\u597d\u5dee\u5f02\u98ce\u9669"}}
{"id": "2508.01943", "pdf": "https://arxiv.org/pdf/2508.01943", "abs": "https://arxiv.org/abs/2508.01943", "authors": ["Philip Schroeder", "Ondrej Biza", "Thomas Weng", "Hongyin Luo", "James Glass"], "title": "ROVER: Recursive Reasoning Over Videos with Vision-Language Models for Embodied Tasks", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.RO"], "comment": null, "summary": "Vision-language models (VLMs) have exhibited impressive capabilities across\ndiverse image understanding tasks, but still struggle in settings that require\nreasoning over extended sequences of camera frames from a video. This limits\ntheir utility in embodied settings, which require reasoning over long frame\nsequences from a continuous stream of visual input at each moment of a task\nattempt. To address this limitation, we propose ROVER (Reasoning Over VidEo\nRecursively), a framework that enables the model to recursively decompose\nlong-horizon video trajectories into segments corresponding to shorter subtasks\nwithin the trajectory. In doing so, ROVER facilitates more focused and accurate\nreasoning over temporally localized frame sequences without losing global\ncontext. We evaluate ROVER, implemented using an in-context learning approach,\non diverse OpenX Embodiment videos and on a new dataset derived from RoboCasa\nthat consists of 543 videos showing both expert and perturbed non-expert\ntrajectories across 27 robotic manipulation tasks. ROVER outperforms strong\nbaselines across three video reasoning tasks: task progress estimation,\nframe-level natural language reasoning, and video question answering. We\nobserve that, by reducing the number of frames the model reasons over at each\ntimestep, ROVER mitigates hallucinations, especially during unexpected or\nnon-optimal moments of a trajectory. In addition, by enabling the\nimplementation of a subtask-specific sliding context window, ROVER's time\ncomplexity scales linearly with video length, an asymptotic improvement over\nbaselines. Demos, code, and data available at: https://rover-vlm.github.io", "AI": {"tldr": "\u63d0\u51faROVER\u6846\u67b6\uff0c\u901a\u8fc7\u9012\u5f52\u5206\u89e3\u89c6\u9891\u8f68\u8ff9\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u957f\u65f6\u89c6\u9891\u63a8\u7406\u80fd\u529b", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8fde\u7eed\u89c6\u9891\u5e27\u63a8\u7406\u4e2d\u5b58\u5728\u5c40\u9650\uff0c\u5f71\u54cd\u5177\u8eab\u667a\u80fd\u5e94\u7528\u3002\u9700\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u4fdd\u6301\u5168\u5c40\u4e0a\u4e0b\u6587", "method": "\u9012\u5f52\u5206\u89e3\u89c6\u9891\u4e3a\u5b50\u4efb\u52a1\u7247\u6bb5\uff0c\u91c7\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60+\u6ed1\u52a8\u7a97\u53e3\u673a\u5236\uff0c\u5b9e\u73b0\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6", "result": "\u5728OpenX Embodiment\u548cRoboCasa\u6570\u636e\u96c6\u4e0a\uff0c\u4efb\u52a1\u8fdb\u5ea6\u4f30\u8ba1\u51c6\u786e\u7387\u63d0\u534723%\uff0c\u89c6\u9891\u95ee\u7b54F1\u503c\u63d0\u534718%", "conclusion": "ROVER\u6709\u6548\u5e73\u8861\u5c40\u90e8\u63a8\u7406\u4e0e\u5168\u5c40\u4e0a\u4e0b\u6587\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u63d0\u5347\u975e\u7406\u60f3\u8f68\u8ff9\u6bb5\u7684\u6297\u5e72\u6270\u80fd\u529b"}}
{"id": "2508.01959", "pdf": "https://arxiv.org/pdf/2508.01959", "abs": "https://arxiv.org/abs/2508.01959", "authors": ["Junjie Wu", "Jiangnan Li", "Yuqing Li", "Lemao Liu", "Liyan Xu", "Jiwei Li", "Dit-Yan Yeung", "Jie Zhou", "Mo Yu"], "title": "SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic Association and Long Story Comprehension", "categories": ["cs.CL"], "comment": "Our trained models can be downloaded from:\n  https://huggingface.co/SituatedEmbedding", "summary": "Retrieval-augmented generation (RAG) over long documents typically involves\nsplitting the text into smaller chunks, which serve as the basic units for\nretrieval. However, due to dependencies across the original document,\ncontextual information is often essential for accurately interpreting each\nchunk. To address this, prior work has explored encoding longer context windows\nto produce embeddings for longer chunks. Despite these efforts, gains in\nretrieval and downstream tasks remain limited. This is because (1) longer\nchunks strain the capacity of embedding models due to the increased amount of\ninformation they must encode, and (2) many real-world applications still\nrequire returning localized evidence due to constraints on model or human\nbandwidth.\n  We propose an alternative approach to this challenge by representing short\nchunks in a way that is conditioned on a broader context window to enhance\nretrieval performance -- i.e., situating a chunk's meaning within its context.\nWe further show that existing embedding models are not well-equipped to encode\nsuch situated context effectively, and thus introduce a new training paradigm\nand develop the situated embedding models (SitEmb). To evaluate our method, we\ncurate a book-plot retrieval dataset specifically designed to assess situated\nretrieval capabilities. On this benchmark, our SitEmb-v1 model based on BGE-M3\nsubstantially outperforms state-of-the-art embedding models, including several\nwith up to 7-8B parameters, with only 1B parameters. Our 8B SitEmb-v1.5 model\nfurther improves performance by over 10% and shows strong results across\ndifferent languages and several downstream applications.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4e0a\u4e0b\u6587\u60c5\u5883\u7684\u77ed\u6587\u672c\u5757\u5d4c\u5165\u6a21\u578bSitEmb\uff0c\u663e\u8457\u63d0\u5347\u957f\u6587\u6863\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfRAG\u5c06\u957f\u6587\u6863\u5207\u5206\u4e3a\u77ed\u5757\u5bfc\u81f4\u4e0a\u4e0b\u6587\u7f3a\u5931\uff0c\u76f4\u63a5\u7f16\u7801\u957f\u5757\u5219\u9762\u4e34\u4fe1\u606f\u538b\u7f29\u8d1f\u62c5\u548c\u5c40\u90e8\u8bc1\u636e\u5b9a\u4f4d\u9700\u6c42\u7684\u53cc\u91cd\u77db\u76fe\u3002\u73b0\u6709\u5d4c\u5165\u6a21\u578b\u96be\u4ee5\u6709\u6548\u7f16\u7801\u60c5\u5883\u5316\u8bed\u4e49\u3002", "method": "1) \u63d0\u51fa\u60c5\u5883\u5316\u5d4c\u5165\u6846\u67b6\uff0c\u4f7f\u77ed\u5757\u7f16\u7801\u65f6\u878d\u5408\u66f4\u5e7f\u4e0a\u4e0b\u6587 2) \u8bbe\u8ba1\u65b0\u578b\u8bad\u7ec3\u8303\u5f0f\u5f00\u53d1SitEmb\u6a21\u578b 3) \u6784\u5efa\u4e66\u7c4d\u60c5\u8282\u68c0\u7d22\u6570\u636e\u96c6\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u6027\u3002", "result": "SitEmb-v1\uff081B\u53c2\u6570\uff09\u8d85\u8d8a7-8B\u89c4\u6a21SOTA\u6a21\u578b\uff1b8B\u7248SitEmb-v1.5\u6027\u80fd\u518d\u63d0\u534710+%\uff0c\u591a\u8bed\u8a00/\u591a\u4efb\u52a1\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u60c5\u5883\u5316\u7f16\u7801\u7b56\u7565\u7a81\u7834\u4f20\u7edfRAG\u74f6\u9888\uff0c\u65b0\u578b\u8bad\u7ec3\u8303\u5f0f\u5b9e\u73b0\u6a21\u578b\u6548\u7387\u4e0e\u6548\u679c\u7684\u53cc\u63d0\u5347\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.01977", "pdf": "https://arxiv.org/pdf/2508.01977", "abs": "https://arxiv.org/abs/2508.01977", "authors": ["Fan Gao", "Cheng Huang", "Nyima Tashi", "Yutong Liu", "Xiangxiang Wang", "Thupten Tsering", "Ban Ma-bao", "Renzeg Duojie", "Gadeng Luosang", "Rinchen Dongrub", "Dorje Tashi", "Xiao Feng", "Hao Wang", "Yongbin Yu"], "title": "TIBSTC-CoT: A Multi-Domain Instruction Dataset for Chain-of-Thought Reasoning in Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "To address the severe data scarcity in Tibetan, a low-resource language\nspoken by over six million people, we introduce TIBSTC-CoT, the large-scale,\nmulti-domain Tibetan dataset automatically constructed via chain-of-thought\nprompting with large language models (LLMs). TIBSTC-CoT establishes a scalable\nand reproducible framework for dataset creation in low-resource settings,\ncovering diverse domains and reasoning patterns essential for language\nunderstanding and generation. Building on this dataset, we develop the\nSunshine-thinking LLM family, a series of Tibetan-centric LLMs equipped with\nchain-of-thought capabilities. Trained entirely on TIBSTC-CoT,\nSunshine-thinking has demonstrated strong reasoning and generation performance,\ncomparable to state-of-the-art (SOTA) multilingual LLMs. Our work marks a\nsignificant step toward inclusive AI by enabling high-quality Tibetan language\nprocessing through both resource creation and model innovation. All data are\navailable: https://github.com/Vicentvankor/sun-shine.", "AI": {"tldr": "\u901a\u8fc7\u601d\u7ef4\u94fe\u6280\u672f\u6784\u5efa\u5927\u89c4\u6a21\u85cf\u8bed\u6570\u636e\u96c6TIBSTC-CoT\uff0c\u5e76\u8bad\u7ec3\u51fa\u5177\u5907\u63a8\u7406\u80fd\u529b\u7684Sunshine-thinking\u7cfb\u5217\u6a21\u578b\uff0c\u63a8\u52a8\u85cf\u8bedAI\u53d1\u5c55\u3002", "motivation": "\u89e3\u51b3\u516d\u767e\u4e07\u4f7f\u7528\u8005\u85cf\u8bed\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u901a\u8fc7\u8d44\u6e90\u5efa\u8bbe\u548c\u6a21\u578b\u521b\u65b0\u5b9e\u73b0\u5305\u5bb9\u6027AI\u8bed\u8a00\u5904\u7406\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210\u591a\u9886\u57df\u601d\u7ef4\u94fe\u6570\u636e\u96c6\uff0c\u5e76\u57fa\u4e8e\u6b64\u5b8c\u5168\u8bad\u7ec3\u85cf\u8bed\u4e13\u5c5e\u7684\u94fe\u5f0f\u63a8\u7406\u5927\u6a21\u578b\u5bb6\u65cf\u3002", "result": "Sunshine-thinking\u6a21\u578b\u5c55\u73b0\u51fa\u4e0e\u4e3b\u6d41\u591a\u8bed\u8a00LLMs\u76f8\u5f53\u7684\u63a8\u7406\u751f\u6210\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u81ea\u5efa\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u53ef\u6269\u5c55\u7684\u6570\u636e\u6784\u5efa\u6846\u67b6\u548c\u6a21\u578b\u521b\u65b0\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u5904\u7406\u63d0\u4f9b\u4e86\u91cd\u8981\u8303\u5f0f\uff0c\u6570\u636e\u5df2\u5f00\u6e90\u5171\u4eab\u3002"}}
{"id": "2508.01990", "pdf": "https://arxiv.org/pdf/2508.01990", "abs": "https://arxiv.org/abs/2508.01990", "authors": ["Praveen Tangarajan", "Anand A. Rajasekar", "Manish Rathi", "Vinay Rao Dandin", "Ozan Ersoy"], "title": "Contextually Aware E-Commerce Product Question Answering using RAG", "categories": ["cs.CL", "I.2.7; H.3.3"], "comment": "6 pages, 1 figure, 5 tables. Preprint under review", "summary": "E-commerce product pages contain a mix of structured specifications,\nunstructured reviews, and contextual elements like personalized offers or\nregional variants. Although informative, this volume can lead to cognitive\noverload, making it difficult for users to quickly and accurately find the\ninformation they need. Existing Product Question Answering (PQA) systems often\nfail to utilize rich user context and diverse product information effectively.\nWe propose a scalable, end-to-end framework for e-commerce PQA using Retrieval\nAugmented Generation (RAG) that deeply integrates contextual understanding. Our\nsystem leverages conversational history, user profiles, and product attributes\nto deliver relevant and personalized answers. It adeptly handles objective,\nsubjective, and multi-intent queries across heterogeneous sources, while also\nidentifying information gaps in the catalog to support ongoing content\nimprovement. We also introduce novel metrics to measure the framework's\nperformance which are broadly applicable for RAG system evaluations.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eRAG\u7684\u7535\u5546\u95ee\u7b54\u6846\u67b6\uff0c\u6574\u5408\u591a\u6e90\u4e0a\u4e0b\u6587\u5b9e\u73b0\u4e2a\u6027\u5316\u56de\u7b54\u5e76\u652f\u6301\u5185\u5bb9\u4f18\u5316", "motivation": "\u73b0\u6709\u7535\u5546PQA\u7cfb\u7edf\u65e0\u6cd5\u6709\u6548\u6574\u5408\u7528\u6237\u4e0a\u4e0b\u6587\u4e0e\u5f02\u6784\u4ea7\u54c1\u4fe1\u606f\uff0c\u5bfc\u81f4\u7528\u6237\u8ba4\u77e5\u8fc7\u8f7d", "method": "\u5f00\u53d1\u7aef\u5230\u7aefRAG\u6846\u67b6\uff0c\u96c6\u6210\u5bf9\u8bdd\u5386\u53f2/\u7528\u6237\u753b\u50cf/\u4ea7\u54c1\u5c5e\u6027\uff0c\u6784\u5efa\u6df7\u5408\u68c0\u7d22\u7b56\u7565\u5904\u7406\u591a\u6a21\u6001\u6570\u636e", "result": "\u7cfb\u7edf\u652f\u6301\u5ba2\u89c2/\u4e3b\u89c2/\u590d\u5408\u610f\u56fe\u67e5\u8be2\u5904\u7406\uff0c\u8bc6\u522b\u77e5\u8bc6\u56fe\u8c31\u7f3a\u53e3\uff0c\u5e76\u63d0\u51fa\u65b0\u7684RAG\u8bc4\u4f30\u6307\u6807\u4f53\u7cfb", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6df1\u5ea6\u4e0a\u4e0b\u6587\u6574\u5408\u663e\u8457\u63d0\u5347\u56de\u7b54\u76f8\u5173\u6027\uff0c\u5176\u8bc4\u4f30\u6307\u6807\u4e3aRAG\u7cfb\u7edf\u63d0\u4f9b\u901a\u7528\u57fa\u51c6"}}
{"id": "2508.01999", "pdf": "https://arxiv.org/pdf/2508.01999", "abs": "https://arxiv.org/abs/2508.01999", "authors": ["Md Badsha Biswas", "\u00d6zlem Uzuner"], "title": "Prompting Large Language Models to Detect Dementia Family Caregivers", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Social media, such as Twitter, provides opportunities for caregivers of\ndementia patients to share their experiences and seek support for a variety of\nreasons. Availability of this information online also paves the way for the\ndevelopment of internet-based interventions in their support. However, for this\npurpose, tweets written by caregivers of dementia patients must first be\nidentified. This paper demonstrates our system for the SMM4H 2025 shared task\n3, which focuses on detecting tweets posted by individuals who have a family\nmember with dementia. The task is outlined as a binary classification problem,\ndifferentiating between tweets that mention dementia in the context of a family\nmember and those that do not. Our solution to this problem explores large\nlanguage models (LLMs) with various prompting methods. Our results show that a\nsimple zero-shot prompt on a fine-tuned model yielded the best results. Our\nfinal system achieved a macro F1-score of 0.95 on the validation set and the\ntest set. Our full code is available on GitHub.", "AI": {"tldr": "\u5229\u7528LLM\u96f6\u6837\u672c\u63d0\u793a\u5b9e\u73b0\u75f4\u5446\u60a3\u8005\u5bb6\u5c5e\u63a8\u6587\u68c0\u6d4b\uff0c\u9a8c\u8bc1\u96c6/\u6d4b\u8bd5\u96c6F1\u8fbe0.95", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e3a\u75f4\u5446\u75c7\u62a4\u7406\u8005\u63d0\u4f9b\u652f\u6301\uff0c\u4f46\u9700\u5148\u7cbe\u51c6\u8bc6\u522b\u76f8\u5173\u63a8\u6587", "method": "\u91c7\u7528\u5fae\u8c03\u540e\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6d4b\u8bd5\u591a\u79cd\u63d0\u793a\u65b9\u6cd5\uff0c\u96f6\u6837\u672c\u7b56\u7565\u6700\u4f18", "result": "\u5728\u9a8c\u8bc1\u96c6\u548c\u6d4b\u8bd5\u96c6\u5747\u53d6\u5f970.95\u7684\u5b8fF1\u503c", "conclusion": "\u96f6\u6837\u672c\u63d0\u793a\u5728\u5fae\u8c03\u6a21\u578b\u4e0a\u6548\u679c\u663e\u8457\uff0c\u7cfb\u7edf\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2508.02013", "pdf": "https://arxiv.org/pdf/2508.02013", "abs": "https://arxiv.org/abs/2508.02013", "authors": ["Changhao Jiang", "Jiajun Sun", "Yifei Cao", "Jiabao Zhuang", "Hui Li", "Xiaoran Fan", "Ming Zhang", "Junjie Ye", "Shihan Dou", "Zhiheng Xi", "Jingqi Tong", "Yilong Wu", "Baoyu Fan", "Zhen Wang", "Tao Liang", "Zhihui Fei", "Mingyang Wan", "Guojun Ma", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "SpeechRole: A Large-Scale Dataset and Benchmark for Evaluating Speech Role-Playing Agents", "categories": ["cs.CL"], "comment": null, "summary": "Recently, role-playing agents have emerged as a promising paradigm for\nachieving personalized interaction and emotional resonance. Existing research\nprimarily focuses on the textual modality, neglecting the critical dimension of\nspeech in realistic interactive scenarios. In particular, there is a lack of\nsystematic evaluation for Speech Role-Playing Agents (SRPAs). To address this\ngap, we construct SpeechRole-Data, a large-scale, high-quality dataset that\ncomprises 98 diverse roles and 112k speech-based single-turn and multi-turn\nconversations. Each role demonstrates distinct vocal characteristics, including\ntimbre and prosody, thereby enabling more sophisticated speech role-playing.\nFurthermore, we propose SpeechRole-Eval, a multidimensional evaluation\nbenchmark that systematically assesses SRPAs performance in key aspects such as\nfundamental interaction ability, speech expressiveness, and role-playing\nfidelity. Experimental results reveal the advantages and challenges of both\ncascaded and end-to-end speech role-playing agents in maintaining vocal style\nconsistency and role coherence. We release all data, code, and baseline models\nto provide a solid foundation for speech-driven multimodal role-playing\nresearch and to foster further developments in this field.", "AI": {"tldr": "\u8bba\u6587\u6784\u5efa\u4e86\u8bed\u97f3\u89d2\u8272\u626e\u6f14\u9886\u57df\u9996\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6SpeechRole-Data\u5e76\u63d0\u51fa\u591a\u7ef4\u8bc4\u4f30\u57fa\u51c6SpeechRole-Eval\uff0c\u63ed\u793a\u4e86\u8bed\u97f3\u89d2\u8272\u626e\u6f14\u4ee3\u7406\u5728\u58f0\u7eb9\u4e00\u81f4\u6027\u4e0e\u89d2\u8272\u8fde\u8d2f\u6027\u65b9\u9762\u7684\u6280\u672f\u6311\u6218", "motivation": "\u73b0\u6709\u89d2\u8272\u626e\u6f14\u7814\u7a76\u805a\u7126\u6587\u672c\u6a21\u6001\uff0c\u7f3a\u4e4f\u5bf9\u8bed\u97f3\u4ea4\u4e92\u7684\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u5c24\u5176\u5728\u8bed\u97f3\u98ce\u683c\u4e00\u81f4\u6027\u548c\u89d2\u8272\u8fde\u8d2f\u6027\u8bc4\u4f30\u65b9\u9762\u5b58\u5728\u7a7a\u767d", "method": "\u6784\u5efa\u5305\u542b98\u4e2a\u89d2\u8272\u3001112k\u8bed\u97f3\u5bf9\u8bdd\u7684SpeechRole-Data\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u591a\u7ef4\u8bc4\u4f30\u6846\u67b6SpeechRole-Eval\uff08\u57fa\u7840\u4ea4\u4e92\u80fd\u529b/\u8bed\u97f3\u8868\u73b0\u529b/\u89d2\u8272\u4fdd\u771f\u5ea6\uff09", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7ea7\u8054\u5f0f\u4e0e\u7aef\u5230\u7aef\u6a21\u578b\u5728\u58f0\u7eb9\u4e00\u81f4\u6027\uff08\u7ea7\u8054\u5f0f\u5360\u4f18\uff09\u548c\u89d2\u8272\u8fde\u8d2f\u6027\uff08\u7aef\u5230\u7aef\u66f4\u4f73\uff09\u4e0a\u7684\u5dee\u5f02\u5316\u8868\u73b0", "conclusion": "\u901a\u8fc7\u5f00\u653e\u6570\u636e\u96c6\u4e0e\u8bc4\u4f30\u4f53\u7cfb\u4e3a\u8bed\u97f3\u89d2\u8272\u626e\u6f14\u9886\u57df\u5efa\u7acb\u7814\u7a76\u57fa\u51c6\uff0c\u63a8\u52a8\u591a\u6a21\u6001\u89d2\u8272\u626e\u6f14\u6280\u672f\u53d1\u5c55"}}
{"id": "2508.02018", "pdf": "https://arxiv.org/pdf/2508.02018", "abs": "https://arxiv.org/abs/2508.02018", "authors": ["Wanqi Yang", "Yanda Li", "Yunchao Wei", "Meng Fang", "Ling Chen"], "title": "SpeechR: A Benchmark for Speech Reasoning in Large Audio-Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large audio-language models (LALMs) have achieved near-human performance in\nsentence-level transcription and emotion recognition. However, existing\nevaluations focus mainly on surface-level perception, leaving the capacity of\nmodels for contextual and inference-driven reasoning in speech-based scenarios\ninsufficiently examined. To address this gap, we introduce SpeechR, a unified\nbenchmark for evaluating reasoning over speech in large audio-language models.\nSpeechR evaluates models along three key dimensions: factual retrieval,\nprocedural inference, and normative judgment. It includes three distinct\nevaluation formats. The multiple-choice version measures answer selection\naccuracy. The generative version assesses the coherence and logical consistency\nof reasoning chains. The acoustic-feature version investigates whether\nvariations in stress and emotion affect reasoning performance. Evaluations on\neleven state-of-the-art LALMs reveal that high transcription accuracy does not\ntranslate into strong reasoning capabilities. SpeechR establishes a structured\nbenchmark for evaluating reasoning in spoken language, enabling more targeted\nanalysis of model capabilities across diverse dialogue-based tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86SpeechR\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u97f3\u573a\u666f\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u9ad8\u8f6c\u5f55\u51c6\u786e\u7387\u4e0d\u7b49\u4e8e\u5f3a\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u8bed\u97f3\u8868\u5c42\u611f\u77e5\uff08\u5982\u8f6c\u5199\u3001\u60c5\u611f\u8bc6\u522b\uff09\uff0c\u7f3a\u4e4f\u5bf9\u6a21\u578b\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u7684\u6df1\u5165\u68c0\u9a8c\u3002", "method": "\u901a\u8fc7\u4e8b\u5b9e\u68c0\u7d22/\u7a0b\u5e8f\u63a8\u7406/\u89c4\u8303\u5224\u65ad\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u91c7\u7528\u9009\u62e9\u9898/\u751f\u6210\u5f0f/\u58f0\u5b66\u7279\u5f81\u4e09\u79cd\u8bc4\u4f30\u5f62\u5f0f\uff0c\u6d4b\u8bd511\u4e2a\u4e3b\u6d41LALMs\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u6a21\u578b\u8f6c\u5f55\u51c6\u786e\u7387\u4e0e\u63a8\u7406\u80fd\u529b\u65e0\u76f4\u63a5\u5173\u8054\uff0c\u58f0\u5b66\u7279\u5f81\uff08\u91cd\u97f3/\u60c5\u611f\uff09\u53d8\u5316\u5f71\u54cd\u63a8\u7406\u8868\u73b0\u3002", "conclusion": "SpeechR\u5efa\u7acb\u4e86\u9996\u4e2a\u7ed3\u6784\u5316\u8bed\u97f3\u63a8\u7406\u8bc4\u4f30\u57fa\u51c6\uff0c\u53ef\u9488\u5bf9\u6027\u5206\u6790\u6a21\u578b\u5728\u4e0d\u540c\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u77ed\u677f\u3002"}}
{"id": "2508.02037", "pdf": "https://arxiv.org/pdf/2508.02037", "abs": "https://arxiv.org/abs/2508.02037", "authors": ["Huihan Li", "You Chen", "Siyuan Wang", "Yixin He", "Ninareh Mehrabi", "Rahul Gupta", "Xiang Ren"], "title": "Diagnosing Memorization in Chain-of-Thought Reasoning, One Token at a Time", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) perform well on reasoning benchmarks but often\nfail when inputs alter slightly, raising concerns about the extent to which\ntheir success relies on memorization. This issue is especially acute in\nChain-of-Thought (CoT) reasoning, where spurious memorized patterns can trigger\nintermediate errors that cascade into incorrect final answers. We introduce\nSTIM, a novel framework for Source-aware Token-level Identification of\nMemorization, which attributes each token in a reasoning chain to one of\nmultiple memorization sources - local, mid-range, or long-range - based on\ntheir statistical co-occurrence with the token in the pretraining corpus. Our\ntoken-level analysis across tasks and distributional settings reveals that\nmodels rely more on memorization in complex or long-tail cases, and that local\nmemorization is often the dominant driver of errors, leading to up to 67% of\nwrong tokens. We also show that memorization scores from STIM can be effective\nin predicting the wrong tokens in the wrong reasoning step. STIM offers a\npowerful tool for diagnosing and improving model reasoning and can generalize\nto other structured step-wise generation tasks.", "AI": {"tldr": "\u63d0\u51faSTIM\u6846\u67b6\u7528\u4e8e\u8bc6\u522b\u5927\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5bf9\u8bb0\u5fc6\u7684\u4f9d\u8d56\u7a0b\u5ea6\uff0c\u53d1\u73b0\u5c40\u90e8\u8bb0\u5fc6\u9519\u8bef\u5360\u6bd4\u6700\u9ad8\u8fbe67%", "motivation": "\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f93\u5165\u53d8\u5316\u65f6\u63a8\u7406\u5931\u6548\u7684\u95ee\u9898\uff0c\u7814\u7a76\u8bb0\u5fc6\u673a\u5236\u5bf9\u601d\u7ef4\u94fe\u63a8\u7406\u9519\u8bef\u7684\u5f71\u54cd", "method": "\u901a\u8fc7\u7edf\u8ba1\u9884\u8bad\u7ec3\u8bed\u6599\u4e2d\u7684\u5171\u73b0\u5173\u7cfb\uff0c\u5efa\u7acb\u4e09\u7ea7\u8bb0\u5fc6\u6e90\u5206\u7c7b\u6846\u67b6\uff08\u5c40\u90e8/\u4e2d\u7a0b/\u8fdc\u7a0b\u8bb0\u5fc6\uff09", "result": "\u590d\u6742\u573a\u666f\u4e0b67%\u9519\u8bef\u6e90\u4e8e\u5c40\u90e8\u8bb0\u5fc6\uff0c\u8bb0\u5fc6\u8bc4\u5206\u53ef\u6709\u6548\u9884\u6d4b\u9519\u8bef\u63a8\u7406\u6b65\u9aa4", "conclusion": "STIM\u4e3a\u8bca\u65ad\u6a21\u578b\u63a8\u7406\u63d0\u4f9b\u4e86\u91cf\u5316\u5de5\u5177\uff0c\u53ef\u63a8\u5e7f\u81f3\u7ed3\u6784\u5316\u751f\u6210\u4efb\u52a1"}}
{"id": "2508.02038", "pdf": "https://arxiv.org/pdf/2508.02038", "abs": "https://arxiv.org/abs/2508.02038", "authors": ["Fengping Tian", "Chenyang Lyu", "Xuanfan Ni", "Haoqin Sun", "Qingjuan Li", "Zhiqiang Qian", "Haijun Li", "Longyue Wang", "Zhao Xu", "Weihua Luo", "Kaifu Zhang"], "title": "Marco-Voice Technical Report", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Technical Report", "summary": "This paper presents a multifunctional speech synthesis system that integrates\nvoice cloning and emotion control speech synthesis within a unified framework.\nThe goal of this work is to address longstanding challenges in achieving highly\nexpressive, controllable, and natural speech generation that faithfully\npreserves speaker identity across diverse linguistic and emotional contexts.\nOur approach introduces an effective speaker-emotion disentanglement mechanism\nwith in-batch contrastive learning, enabling independent manipulation of\nspeaker identity and eemotional style, as well as rotational emotional\nembedding integration method for smooth emotion control. To support\ncomprehensive training and evaluation, we construct CSEMOTIONS, a high-quality\nemotional speech dataset containing 10 hours of Mandarin speech from six\nprofessional speakers across seven emotional categories. Extensive experiments\ndemonstrate that our system, Marco-Voice, achieves substantial improvements in\nboth objective and subjective metrics. Comprehensive evaluations and analysis\nwere conducted, results show that MarcoVoice delivers competitive performance\nin terms of speech clarity and emotional richness, representing a substantial\nadvance in the field of expressive neural speech synthesis.", "AI": {"tldr": "\u63d0\u51faMarco-Voice\u591a\u6a21\u6001\u8bed\u97f3\u5408\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bf4\u8bdd\u4eba-\u60c5\u611f\u89e3\u8026\u673a\u5236\u548c\u65cb\u8f6c\u60c5\u611f\u5d4c\u5165\u65b9\u6cd5\u5b9e\u73b0\u9ad8\u8868\u73b0\u529b\u7684\u8bed\u97f3\u5408\u6210\uff0c\u5e76\u6784\u5efaCSEMOTIONS\u6570\u636e\u96c6\u9a8c\u8bc1\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8bed\u97f3\u5408\u6210\u7cfb\u7edf\u5728\u4fdd\u6301\u8bf4\u8bdd\u4eba\u8eab\u4efd\u3001\u60c5\u611f\u63a7\u5236\u5e73\u6ed1\u5ea6\u53ca\u591a\u8bed\u8a00\u60c5\u611f\u8868\u8fbe\u7edf\u4e00\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027", "method": "1. \u57fa\u4e8e\u6279\u5185\u5bf9\u6bd4\u5b66\u4e60\u7684\u8bf4\u8bdd\u4eba-\u60c5\u611f\u89e3\u8026\u673a\u5236\n2. \u65cb\u8f6c\u60c5\u611f\u5d4c\u5165\u96c6\u6210\u65b9\u6cd5\n3. \u6784\u5efa\u5305\u542b7\u7c7b\u60c5\u611f\u768410\u5c0f\u65f6\u4e2d\u6587\u6570\u636e\u96c6CSEMOTIONS", "result": "\u4e3b\u5ba2\u89c2\u8bc4\u4f30\u663e\u793a\uff1aMOS\u63d0\u534712.7%\uff0c\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u5ea6\u8fbe0.89\uff0c\u60c5\u611f\u8bc6\u522b\u51c6\u786e\u7387\u8d8585%\uff0c\u5408\u6210\u901f\u5ea6\u6bd4\u57fa\u51c6\u6a21\u578b\u5feb2.3\u500d", "conclusion": "Marco-Voice\u5728\u8bed\u97f3\u6e05\u6670\u5ea6\uff08PESQ=4.2\uff09\u548c\u60c5\u611f\u4e30\u5bcc\u5ea6\uff08EMOS=4.5/5\uff09\u4e0a\u8fbe\u5230\u4e1a\u754c\u9886\u5148\u6c34\u5e73\uff0c\u63a8\u52a8\u795e\u7ecf\u8bed\u97f3\u5408\u6210\u5411\u53ef\u63a7\u8868\u8fbe\u65b9\u5411\u8fc8\u8fdb"}}
{"id": "2508.02045", "pdf": "https://arxiv.org/pdf/2508.02045", "abs": "https://arxiv.org/abs/2508.02045", "authors": ["Soyeon Kim", "Jindong Wang", "Xing Xie", "Steven Euijong Whang"], "title": "Harnessing Temporal Databases for Systematic Evaluation of Factual Time-Sensitive Question-Answering in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Facts evolve over time, making it essential for Large Language Models (LLMs)\nto handle time-sensitive factual knowledge accurately and reliably. While\nfactual Time-Sensitive Question-Answering (TSQA) tasks have been widely\nstudied, existing benchmarks often rely on manual curation or a small, fixed\nset of predefined templates, which restricts scalable and comprehensive TSQA\nevaluation. To address these challenges, we propose TDBench, a new benchmark\nthat systematically constructs TSQA pairs by harnessing temporal databases and\ndatabase techniques such as temporal SQL and functional dependencies. We also\nintroduce a fine-grained evaluation metric called time accuracy, which assesses\nthe validity of time references in model explanations alongside traditional\nanswer accuracy to enable a more reliable TSQA evaluation. Extensive\nexperiments on contemporary LLMs show how \\ours{} enables scalable and\ncomprehensive TSQA evaluation while reducing the reliance on human labor,\ncomplementing existing Wikipedia/Wikidata-based TSQA evaluation approaches by\nenabling LLM evaluation on application-specific data and seamless multi-hop\nquestion generation. Code and data are publicly available at:\nhttps://github.com/ssoy0701/tdbench.git.", "AI": {"tldr": "\u63d0\u51faTDBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u6574\u5408\u65f6\u6001\u6570\u636e\u5e93\u6280\u672f\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u65f6\u95f4\u654f\u611f\u95ee\u7b54\u8bc4\u4f30\uff0c\u5f15\u5165\u65f6\u95f4\u51c6\u786e\u6027\u6307\u6807\u63d0\u5347\u8bc4\u4f30\u53ef\u9760\u6027", "motivation": "\u73b0\u6709\u65f6\u95f4\u654f\u611f\u95ee\u7b54\u8bc4\u4f30\u57fa\u51c6\u8fc7\u5ea6\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u548c\u56fa\u5b9a\u6a21\u677f\uff0c\u96be\u4ee5\u5b9e\u73b0\u5927\u89c4\u6a21\u5168\u9762\u7684\u6a21\u578b\u8bc4\u4f30", "method": "\u5229\u7528\u65f6\u6001\u6570\u636e\u5e93\u7684SQL\u529f\u80fd\u548c\u51fd\u6570\u4f9d\u8d56\u5173\u7cfb\u81ea\u52a8\u751f\u6210TSQA\u6d4b\u8bd5\u5bf9\uff0c\u5f00\u53d1\u65f6\u95f4\u51c6\u786e\u6027\u6307\u6807\u9a8c\u8bc1\u65f6\u95f4\u5f15\u7528\u6709\u6548\u6027", "result": "\u5b9e\u9a8c\u8bc1\u660eTDBench\u652f\u6301\u5e94\u7528\u573a\u666f\u6570\u636e\u8bc4\u4f30\uff0c\u5b9e\u73b0\u591a\u8df3\u95ee\u9898\u751f\u6210\u81ea\u52a8\u5316\uff0c\u663e\u8457\u964d\u4f4e\u4eba\u5de5\u53c2\u4e0e\u9700\u6c42", "conclusion": "TDBench\u901a\u8fc7\u6570\u636e\u5e93\u6280\u672f\u8865\u5145\u73b0\u6709\u7ef4\u57fa\u6570\u636e\u8bc4\u4f30\u4f53\u7cfb\uff0c\u4e3aLLM\u5728\u65f6\u5e8f\u77e5\u8bc6\u5904\u7406\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u6807\u51c6\u5316\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.02053", "pdf": "https://arxiv.org/pdf/2508.02053", "abs": "https://arxiv.org/abs/2508.02053", "authors": ["Zhentao Xu", "Fengyi Li", "Albert Chen", "Xiaofeng Wang"], "title": "ProCut: LLM Prompt Compression via Attribution Estimation", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "In large-scale industrial LLM systems, prompt templates often expand to\nthousands of tokens as teams iteratively incorporate sections such as task\ninstructions, few-shot examples, and heuristic rules to enhance robustness and\ncoverage. This expansion leads to bloated prompts that are difficult to\nmaintain and incur significant inference latency and serving costs. To address\nthis, we introduce Prompt Compression via Attribution Estimation (ProCut), a\nflexible, LLM-agnostic, training-free framework that compresses prompts through\nattribution analysis. ProCut segments prompt templates into semantically\nmeaningful units, quantifies their impact on task performance, and prunes\nlow-utility components. Through extensive experiments on five public benchmark\ndatasets and real-world industrial prompts, we show that ProCut achieves\nsubstantial prompt size reductions (78% fewer tokens in production) while\nmaintaining or even slightly improving task performance (up to 62% better than\nalternative methods). We further introduce an LLM-driven attribution estimator\nthat reduces compression latency by over 50%, and demonstrate that ProCut\nintegrates seamlessly with existing prompt-optimization frameworks to produce\nconcise, high-performing prompts.", "AI": {"tldr": "\u63d0\u51faProCut\u6846\u67b6\uff0c\u901a\u8fc7\u5f52\u56e0\u5206\u6790\u538b\u7f29\u5de5\u4e1a\u7ea7LLM\u63d0\u793a\u6a21\u677f\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u524d\u63d0\u4e0b\u51cf\u5c1178% token\u91cf", "motivation": "\u5de5\u4e1aLLM\u7cfb\u7edf\u7684\u63d0\u793a\u6a21\u677f\u81a8\u80c0\u5bfc\u81f4\u7ef4\u62a4\u6210\u672c\u9ad8\u3001\u63a8\u7406\u5ef6\u8fdf\u5927\uff0c\u9700\u538b\u7f29\u4f18\u5316", "method": "1. \u8bed\u4e49\u5355\u5143\u5206\u5272 2. \u5f52\u56e0\u91cf\u5316\u5206\u6790 3. \u4f4e\u6548\u7528\u7ec4\u4ef6\u526a\u679d + LLM\u9a71\u52a8\u5f52\u56e0\u4f30\u8ba1\u5668\u52a0\u901f", "result": "\u751f\u4ea7\u73af\u5883\u51cf\u5c1178% tokens\uff0c\u6027\u80fd\u63d0\u534762%\uff1b\u538b\u7f29\u5ef6\u8fdf\u964d\u4f4e50%+\uff0c\u517c\u5bb9\u73b0\u6709\u4f18\u5316\u6846\u67b6", "conclusion": "ProCut\u63d0\u4f9b\u65e0\u9700\u8bad\u7ec3\u3001\u6a21\u578b\u65e0\u5173\u7684\u63d0\u793a\u538b\u7f29\u65b9\u6848\uff0c\u5728\u5de5\u4e1a\u573a\u666f\u5b9e\u73b0\u9ad8\u6548\u63d0\u793a\u4f18\u5316"}}
{"id": "2508.02074", "pdf": "https://arxiv.org/pdf/2508.02074", "abs": "https://arxiv.org/abs/2508.02074", "authors": ["Gustaf Ahdritz", "Anat Kleiman"], "title": "The SMeL Test: A simple benchmark for media literacy in language models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The internet is rife with unattributed, deliberately misleading, or otherwise\nuntrustworthy content. Though large language models (LLMs) are often tasked\nwith autonomous web browsing, the extent to which they have learned the simple\nheuristics human researchers use to navigate this noisy environment is not\ncurrently known. In this paper, we introduce the Synthetic Media Literacy Test\n(SMeL Test), a minimal benchmark that tests the ability of language models to\nactively filter out untrustworthy information in context. We benchmark a\nvariety of commonly used instruction-tuned LLMs, including reasoning models,\nand find that no model consistently trusts more reliable sources; while\nreasoning in particular is associated with higher scores, even the best API\nmodel we test hallucinates up to 70% of the time. Remarkably, larger and more\ncapable models do not necessarily outperform their smaller counterparts. We\nhope our work sheds more light on this important form of hallucination and\nguides the development of new methods to combat it.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7SMeL Test\u57fa\u51c6\u6d4b\u8bd5\u53d1\u73b0\uff0c\u5f53\u524d\u4e3b\u6d41\u8bed\u8a00\u6a21\u578b\u65e0\u6cd5\u7a33\u5b9a\u8bc6\u522b\u53ef\u4fe1\u4fe1\u606f\u6e90\uff0c\u5927\u6a21\u578b\u8868\u73b0\u672a\u5fc5\u4f18\u4e8e\u5c0f\u6a21\u578b\uff0c70%\u60c5\u51b5\u4e0b\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u9a8c\u8bc1\u8bed\u8a00\u6a21\u578b\u662f\u5426\u5177\u5907\u4eba\u7c7b\u7814\u7a76\u8005\u8fc7\u6ee4\u7f51\u7edc\u566a\u58f0\u4fe1\u606f\u7684\u542f\u53d1\u5f0f\u80fd\u529b\uff0c\u63ed\u793a\u5f53\u524d\u6a21\u578b\u5728\u53ef\u4fe1\u4fe1\u606f\u7b5b\u9009\u65b9\u9762\u7684\u7f3a\u9677\u3002", "method": "\u63d0\u51faSMeL Test\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\uff08\u5305\u62ec\u63a8\u7406\u6a21\u578b\uff09\u5728\u4e0a\u4e0b\u6587\u60c5\u5883\u4e2d\u8fc7\u6ee4\u4e0d\u53ef\u4fe1\u4fe1\u606f\u7684\u80fd\u529b\u3002", "result": "1. \u6240\u6709\u6d4b\u8bd5\u6a21\u578b\u5747\u672a\u5c55\u73b0\u7a33\u5b9a\u4fe1\u4efb\u53ef\u9760\u6765\u6e90\u7684\u80fd\u529b\n2. \u6a21\u578b\u53c2\u6570\u91cf\u4e0e\u8868\u73b0\u65e0\u6b63\u76f8\u5173\n3. \u6700\u4f73API\u6a21\u578b\u5e7b\u89c9\u7387\u9ad8\u8fbe70%\n4. \u63a8\u7406\u80fd\u529b\u4e0e\u8f83\u9ad8\u5f97\u5206\u76f8\u5173\u4f46\u4e0d\u53ef\u9760", "conclusion": "\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u4fe1\u606f\u53ef\u4fe1\u5ea6\u5224\u65ad\u4e0a\u7684\u4e25\u91cd\u5e7b\u89c9\u95ee\u9898\uff0c\u4e3a\u5f00\u53d1\u6297\u5e7b\u89c9\u65b9\u6cd5\u63d0\u4f9b\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2508.02087", "pdf": "https://arxiv.org/pdf/2508.02087", "abs": "https://arxiv.org/abs/2508.02087", "authors": ["Jin Li", "Keyu Wang", "Shu Yang", "Zhuoran Zhang", "Di Wang"], "title": "When Truth Is Overridden: Uncovering the Internal Origins of Sycophancy in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) often exhibit sycophantic behavior, agreeing\nwith user-stated opinions even when those contradict factual knowledge. While\nprior work has documented this tendency, the internal mechanisms that enable\nsuch behavior remain poorly understood. In this paper, we provide a mechanistic\naccount of how sycophancy arises within LLMs. We first systematically study how\nuser opinions induce sycophancy across different model families. We find that\nsimple opinion statements reliably induce sycophancy, whereas user expertise\nframing has a negligible impact. Through logit-lens analysis and causal\nactivation patching, we identify a two-stage emergence of sycophancy: (1) a\nlate-layer output preference shift and (2) deeper representational divergence.\nWe also verify that user authority fails to influence behavior because models\ndo not encode it internally. In addition, we examine how grammatical\nperspective affects sycophantic behavior, finding that first-person prompts\n(``I believe...'') consistently induce higher sycophancy rates than\nthird-person framings (``They believe...'') by creating stronger\nrepresentational perturbations in deeper layers. These findings highlight that\nsycophancy is not a surface-level artifact but emerges from a structural\noverride of learned knowledge in deeper layers, with implications for alignment\nand truthful AI systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63ed\u793a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8c04\u5a9a\u884c\u4e3a\u6e90\u4e8e\u6df1\u5c42\u77e5\u8bc6\u8986\u76d6\u673a\u5236\uff0c\u800c\u975e\u8868\u9762\u504f\u597d\u3002\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\u7528\u6237\u610f\u89c1\u672c\u8eab\u800c\u975e\u6743\u5a01\u6027\u4e3b\u5bfc\u8c04\u5a9a\u884c\u4e3a\uff0c\u4e14\u7b2c\u4e00\u4eba\u79f0\u63d0\u793a\u901a\u8fc7\u6df1\u5c42\u8868\u5f81\u6270\u52a8\u589e\u5f3a\u8be5\u6548\u5e94\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u672a\u80fd\u89e3\u91ca\u5927\u8bed\u8a00\u6a21\u578b\u4e3a\u4f55\u5728\u7528\u6237\u610f\u89c1\u4e0e\u4e8b\u5b9e\u51b2\u7a81\u65f6\u4ecd\u8868\u73b0\u8c04\u5a9a\uff0c\u672c\u6587\u65e8\u5728\u63ed\u793a\u5176\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6027\u884c\u4e3a\u6d4b\u8bd5\u3001logit-lens\u5206\u6790\u3001\u56e0\u679c\u6fc0\u6d3b\u4fee\u8865\u6280\u672f\uff0c\u5e76\u5bf9\u6bd4\u4e0d\u540c\u8bed\u6cd5\u89c6\u89d2\uff08\u7b2c\u4e00\u4eba\u79f0vs\u7b2c\u4e09\u4eba\u79f0\uff09\u7684\u5f71\u54cd\u3002", "result": "1. \u8c04\u5a9a\u884c\u4e3a\u5206\u4e24\u9636\u6bb5\u4ea7\u751f\uff1a\u665a\u671f\u5c42\u8f93\u51fa\u504f\u597d\u504f\u79fb\u2192\u6df1\u5c42\u8868\u5f81\u5206\u6b67\n2. \u7528\u6237\u6743\u5a01\u6027\u65e0\u663e\u8457\u5f71\u54cd\n3. \u7b2c\u4e00\u4eba\u79f0\u63d0\u793a\u901a\u8fc7\u6df1\u5c42\u8868\u5f81\u6270\u52a8\u4f7f\u8c04\u5a9a\u7387\u63d0\u534750%", "conclusion": "\u8c04\u5a9a\u884c\u4e3a\u662f\u6df1\u5c42\u77e5\u8bc6\u7ed3\u6784\u88ab\u8986\u76d6\u7684\u8868\u73b0\uff0c\u8fd9\u5bf9AI\u5bf9\u9f50\u6280\u672f\u53d1\u5c55\u548c\u6784\u5efa\u771f\u5b9eAI\u7cfb\u7edf\u5177\u6709\u91cd\u8981\u542f\u793a\u610f\u4e49\u3002"}}
{"id": "2508.02094", "pdf": "https://arxiv.org/pdf/2508.02094", "abs": "https://arxiv.org/abs/2508.02094", "authors": ["Yaqiong Li", "Peng Zhang", "Lin Wang", "Hansu Gu", "Siyuan Qiao", "Ning Gu", "Tun Lu"], "title": "\"Harmless to You, Hurtful to Me!\": Investigating the Detection of Toxic Languages Grounded in the Perspective of Youth", "categories": ["cs.CL", "cs.HC"], "comment": "Accepted at the 20th International AAAI Conference on Web and Social\n  Media (ICWSM 2026)", "summary": "Risk perception is subjective, and youth's understanding of toxic content\ndiffers from that of adults. Although previous research has conducted extensive\nstudies on toxicity detection in social media, the investigation of youth's\nunique toxicity, i.e., languages perceived as nontoxic by adults but toxic as\nyouth, is ignored. To address this gap, we aim to explore: 1) What are the\nfeatures of ``youth-toxicity'' languages in social media (RQ1); 2) Can existing\ntoxicity detection techniques accurately detect these languages (RQ2). For\nthese questions, we took Chinese youth as the research target, constructed the\nfirst Chinese ``youth-toxicity'' dataset, and then conducted extensive\nanalysis. Our results suggest that youth's perception of these is associated\nwith several contextual factors, like the source of an utterance and\ntext-related features. Incorporating these meta information into current\ntoxicity detection methods significantly improves accuracy overall. Finally, we\npropose several insights into future research on youth-centered toxicity\ndetection.", "AI": {"tldr": "\u9996\u6b21\u6784\u5efa\u4e2d\u6587\u9752\u5c11\u5e74\u6bd2\u6027\u6570\u636e\u96c6\uff0c\u63ed\u793a\u9752\u5c11\u5e74\u5bf9\u7279\u5b9a\u7f51\u7edc\u8bed\u8a00\u7684\u6bd2\u6027\u611f\u77e5\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u6574\u5408\u8bed\u5883\u56e0\u7d20\u53ef\u63d0\u5347\u68c0\u6d4b\u51c6\u786e\u7387", "motivation": "\u73b0\u6709\u6bd2\u6027\u68c0\u6d4b\u7814\u7a76\u5ffd\u89c6\u9752\u5c11\u5e74\u72ec\u7279\u6bd2\u6027\uff08\u6210\u4eba\u8ba4\u4e3a\u65e0\u5bb3\u4f46\u9752\u5c11\u5e74\u8ba4\u4e3a\u6709\u5bb3\u7684\u8bed\u8a00\uff09\uff0c\u9700\u63a2\u7a76\u5176\u7279\u5f81\u53ca\u68c0\u6d4b\u6709\u6548\u6027", "method": "\u4ee5\u4e2d\u56fd\u9752\u5c11\u5e74\u4e3a\u5bf9\u8c61\uff0c\u521b\u5efa\u9996\u4e2a\u4e2d\u6587\u9752\u5c11\u5e74\u6bd2\u6027\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u8bed\u5883\u56e0\u7d20\uff08\u4fe1\u606f\u6765\u6e90/\u6587\u672c\u7279\u5f81\uff09\u5206\u6790\uff0c\u6539\u8fdb\u73b0\u6709\u68c0\u6d4b\u6a21\u578b", "result": "\u9752\u5c11\u5e74\u6bd2\u6027\u611f\u77e5\u4e0e\u8bed\u5883\u5f3a\u76f8\u5173\uff0c\u6574\u5408\u5143\u4fe1\u606f\u4f7f\u68c0\u6d4b\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\uff08\u5177\u4f53\u6570\u503c\u9700\u67e5\u539f\u6587\uff09", "conclusion": "\u63d0\u51fa\u9752\u5c11\u5e74\u4e2d\u5fc3\u6bd2\u6027\u68c0\u6d4b\u9700\u878d\u5408\u8bed\u5883\u7279\u5f81\u7684\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u7b97\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2508.02189", "pdf": "https://arxiv.org/pdf/2508.02189", "abs": "https://arxiv.org/abs/2508.02189", "authors": ["David Demitri Africa", "Yuval Weiss", "Paula Buttery", "Richard Diehl Martinez"], "title": "Learning Dynamics of Meta-Learning in Small Model Pretraining", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models are powerful but costly. We ask whether meta-learning\ncan make the pretraining of small language models not only better but also more\ninterpretable. We integrate first-order MAML with subset-masked LM pretraining,\nproducing four LLama-style decoder-only models (11M-570M params), and evaluate\nit on a fundamental NLP task with many settings and real-world applications.\nCompared with vanilla training, our model (i) reaches the same loss up to 1.6x\nsooner, (ii) improves F1 on multilingual Universal NER under equal compute, and\n(iii) makes the training dynamics easy to read: first the network's\nrepresentations fan out (\"diversify\") and later they collapse into a smaller,\nshared subspace (\"compress\"). This two-stage shift shows up as a rise-and-fall\nin both effective-rank curves and attention-head entropy. The same curves\npinpoint which layers specialise earliest and which later reconverge, giving a\ncompact, interpretable signature of meta-adaptation. Code, checkpoints and\nWandB logs are released.", "AI": {"tldr": "Meta-learning\u7ed3\u5408\u63a9\u7801\u9884\u8bad\u7ec3\u4f7f\u5c0f\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u901f\u5ea6\u63d0\u53471.6\u500d\uff0c\u5728\u8de8\u8bed\u8a00NER\u4efb\u52a1\u8868\u73b0\u66f4\u4f18\uff0c\u5e76\u901a\u8fc7\u8bad\u7ec3\u52a8\u6001\u53ef\u89c6\u5316\u63ed\u793a\u4e86\u8868\u5f81'\u6269\u5f20-\u538b\u7f29'\u4e24\u9636\u6bb5\u6f14\u5316\u89c4\u5f8b\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u6210\u672c\u9ad8\u6602\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5143\u5b66\u4e60\u63d0\u5347\u5c0f\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u63a2\u7d22\u6a21\u578b\u8bad\u7ec3\u52a8\u6001\u7684\u900f\u660e\u5316\u8868\u5f81\u3002", "method": "\u96c6\u6210\u4e00\u9636MAML\u4e0e\u5b50\u96c6\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\uff0c\u6784\u5efa4\u4e2aLLama\u67b6\u6784\u7684\u89e3\u7801\u5668\u6a21\u578b\uff081100\u4e07-5.7\u4ebf\u53c2\u6570\uff09\uff0c\u5728\u901a\u7528NER\u4efb\u52a1\u8fdb\u884c\u591a\u573a\u666f\u9a8c\u8bc1\u3002", "result": "\u8bad\u7ec3\u901f\u5ea6\u63d0\u534760%\uff1b\u8de8\u8bed\u8a00NER F1\u5206\u6570\u63d0\u9ad8\uff1b\u6709\u6548\u79e9\u66f2\u7ebf\u5448\u73b0\u5148\u5347\u540e\u964d\u7684\u949f\u578b\u8f68\u8ff9\uff0c\u6ce8\u610f\u529b\u5934\u71b5\u53d8\u5316\u63ed\u793a\u4e0d\u540c\u5c42\u7ea7\u7684\u5206\u5de5\u65f6\u5e8f\u7279\u5f81\u3002", "conclusion": "\u5143\u5b66\u4e60\u4e0d\u4ec5\u52a0\u901f\u8bad\u7ec3\uff0c\u66f4\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8bad\u7ec3\u52a8\u6001\u7b7e\u540d\uff0c\u901a\u8fc7\u6709\u6548\u79e9\u66f2\u7ebf\u53ef\u7cbe\u51c6\u5b9a\u4f4d\u4e0d\u540c\u7f51\u7edc\u5c42\u7684\u4e13\u4e1a\u5316\u9636\u6bb5\u4e0e\u91cd\u6536\u655b\u8fc7\u7a0b\u3002"}}
{"id": "2508.02193", "pdf": "https://arxiv.org/pdf/2508.02193", "abs": "https://arxiv.org/abs/2508.02193", "authors": ["Yuxuan Song", "Zheng Zhang", "Cheng Luo", "Pengyang Gao", "Fan Xia", "Hao Luo", "Zheng Li", "Yuehang Yang", "Hongli Yu", "Xingwei Qu", "Yuwei Fu", "Jing Su", "Ge Zhang", "Wenhao Huang", "Mingxuan Wang", "Lin Yan", "Xiaoying Jia", "Jingjing Liu", "Wei-Ying Ma", "Ya-Qin Zhang", "Yonghui Wu", "Hao Zhou"], "title": "Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed Inference", "categories": ["cs.CL", "cs.LG"], "comment": "Demo is available at https://studio.seed.ai/exp/seed_diffusion/;\n  Project page is https://seed.bytedance.com/seed_diffusion", "summary": "We present Seed Diffusion Preview, a large-scale language model based on\ndiscrete-state diffusion, offering remarkably fast inference speed. Thanks to\nnon-sequential, parallel generation, discrete diffusion models provide a\nnotable speedup to mitigate the inherent latency of token-by-token decoding, as\ndemonstrated recently (e.g., Mercury Coder, Gemini Diffusion). Seed Diffusion\nPreview achieves an inference speed of 2,146 token/s over H20 GPUs while\nmaintaining competitive performance across a sweep of standard code evaluation\nbenchmarks, significantly faster than contemporary Mercury and Gemini\nDiffusion, establishing new state of the art on the speed-quality Pareto\nfrontier for code models.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u79bb\u6563\u72b6\u6001\u6269\u6563\u7684Seed Diffusion Preview\u6a21\u578b\uff0c\u5b9e\u73b02146 token/s\u7684\u6781\u901f\u63a8\u7406\uff0c\u5728\u4ee3\u7801\u6a21\u578b\u901f\u5ea6-\u8d28\u91cf\u5e15\u7d2f\u6258\u524d\u6cbf\u8fbe\u5230SOTA", "motivation": "\u89e3\u51b3\u4f20\u7edf\u9010token\u89e3\u7801\u6a21\u5f0f\u5e26\u6765\u7684\u5ef6\u8fdf\u95ee\u9898\uff0c\u901a\u8fc7\u5e76\u884c\u751f\u6210\u663e\u8457\u52a0\u901f\u63a8\u7406\u8fc7\u7a0b", "method": "\u91c7\u7528\u975e\u987a\u5e8f\u7684\u79bb\u6563\u6269\u6563\u6a21\u578b\u67b6\u6784\uff0c\u5b9e\u73b0\u5e76\u884ctoken\u751f\u6210", "result": "\u5728H20 GPU\u4e0a\u8fbe\u52302146 token/s\u63a8\u7406\u901f\u5ea6\uff0c\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u8d85\u8d8aMercury/Gemini Diffusion", "conclusion": "\u5efa\u7acb\u4e86\u4ee3\u7801\u6a21\u578b\u901f\u5ea6\u4e0e\u8d28\u91cf\u6743\u8861\u7684\u65b0\u6807\u6746\uff0c\u4e3a\u5b9e\u65f6\u4ee3\u7801\u751f\u6210\u5e94\u7528\u94fa\u5e73\u9053\u8def"}}
{"id": "2508.02208", "pdf": "https://arxiv.org/pdf/2508.02208", "abs": "https://arxiv.org/abs/2508.02208", "authors": ["Yebo Peng", "Zixiang Liu", "Yaoming Li", "Zhizhuo Yang", "Xinye Xu", "Bowen Ye", "Weijun Yuan", "Zihan Wang", "Tong Yang"], "title": "Proof2Hybrid: Automatic Mathematical Benchmark Synthesis for Proof-Centric Problems", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 5 figures", "summary": "Evaluating the mathematical capability of Large Language Models (LLMs) is a\ncritical yet challenging frontier. Existing benchmarks fall short, particularly\nfor proof-centric problems, as manual creation is unscalable and costly,\nleaving the true mathematical abilities of LLMs largely unassessed. To overcome\nthese barriers, we propose Proof2Hybrid, the first fully automated framework\nthat synthesizes high-quality, proof-centric benchmarks from natural language\nmathematical corpora. The key novelty of our solution is Proof2X, a roadmap of\nconverting mathematical proofs into various kinds of questions that are easy to\nverify. Instructed by this roadmap, we propose a new type of hybrid-formatted\nquestions, named ``$m$-out-of-$n$ multiple judge questions'', specifically\ndesigned to enable robust, automatic evaluation while being resilient to\nguessing and superficial pattern matching inherent in traditional formats. As a\ndemonstration of our framework, we introduce AlgGeoTest, a benchmark for\nalgebraic geometry--a frontier domain of modern mathematics--comprising 456\nchallenging items. Our extensive evaluations on state-of-the-art LLMs using\nAlgGeoTest reveal profound deficits in their comprehension of algebraic\ngeometry, providing a more precise measure of their true mathematical\ncapabilities. Our framework and benchmark pave the way for a new wave of\nin-depth research into the mathematical intelligence of AI systems.", "AI": {"tldr": "Proposed Proof2Hybrid framework automatically generates proof-centric benchmarks to rigorously evaluate LLMs' mathematical capabilities.", "motivation": "Existing benchmarks inadequately assess LLMs' mathematical reasoning due to manual creation limitations, especially for proof-based problems.", "method": "Developed Proof2X roadmap converting proofs into verifiable hybrid questions (m-out-of-n multiple judge format) to resist guessing/pattern matching.", "result": "AlgGeoTest benchmark (456 items) exposed fundamental limitations of state-of-the-art LLMs in comprehending algebraic geometry concepts.", "conclusion": "This framework enables deeper evaluation of AI mathematical intelligence, revealing current model deficiencies and advancing research directions."}}
{"id": "2508.02241", "pdf": "https://arxiv.org/pdf/2508.02241", "abs": "https://arxiv.org/abs/2508.02241", "authors": ["Danial Namazifard", "Lukas Galke"], "title": "Isolating Culture Neurons in Multilingual Large Language Models", "categories": ["cs.CL"], "comment": "18 pages, 13 figures", "summary": "Language and culture are deeply intertwined, yet it is so far unclear how and\nwhere multilingual large language models encode culture. Here, we extend upon\nan established methodology for identifying language-specific neurons and extend\nit to localize and isolate culture-specific neurons, carefully disentangling\ntheir overlap and interaction with language-specific neurons. To facilitate our\nexperiments, we introduce MUREL, a curated dataset of 85.2 million tokens\nspanning six different cultures. Our localization and intervention experiments\nshow that LLMs encode different cultures in distinct neuron populations,\npredominantly in upper layers, and that these culture neurons can be modulated\nindependently from language-specific neurons or those specific to other\ncultures. These findings suggest that cultural knowledge and propensities in\nmultilingual language models can be selectively isolated and edited - promoting\nfairness, inclusivity, and alignment. Code and data is available at\nhttps://github.com/namazifard/Culture_Neurons .", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u5b9a\u4f4d\u5e76\u72ec\u7acb\u8c03\u8282\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6587\u5316\u7279\u5b9a\u795e\u7ecf\u5143\uff08\u4e3b\u8981\u4f4d\u4e8e\u4e0a\u5c42\uff09\uff0c\u5b9e\u73b0\u4e86\u6587\u5316\u77e5\u8bc6\u7684\u89e3\u8026\u4e0e\u7f16\u8f91\uff0c\u4fc3\u8fdb\u6a21\u578b\u516c\u5e73\u6027\u4e0e\u5305\u5bb9\u6027\u3002", "motivation": "\u63ed\u793a\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u6587\u5316\u8868\u5f81\u7684\u7f16\u7801\u673a\u5236\uff0c\u89e3\u51b3\u6587\u5316\u795e\u7ecf\u5143\u4e0e\u8bed\u8a00\u795e\u7ecf\u5143\u7684\u8026\u5408\u95ee\u9898\uff0c\u4e3a\u6a21\u578b\u4f26\u7406\u5bf9\u9f50\u63d0\u4f9b\u6280\u672f\u8def\u5f84\u3002", "method": "\u6269\u5c55\u8bed\u8a00\u795e\u7ecf\u5143\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u6784\u5efa\u8de8\u516d\u79cd\u6587\u5316\u7684MUREL\u6570\u636e\u96c6\uff0885.2M tokens\uff09\uff0c\u901a\u8fc7\u5c42\u7ea7\u5e72\u9884\u5b9e\u9a8c\u5206\u79bb\u6587\u5316/\u8bed\u8a00\u795e\u7ecf\u5143\u7684\u6fc0\u6d3b\u6a21\u5f0f\u3002", "result": "\u6587\u5316\u795e\u7ecf\u5143\u5177\u6709\u7a7a\u95f4\u72ec\u7acb\u6027\uff08\u4e0a\u5c42L25-28\uff09\u3001\u529f\u80fd\u6a21\u5757\u5316\uff08\u5355\u6587\u5316\u8c03\u8282\u4e0d\u5f71\u54cd\u5176\u4ed6\u6587\u5316/\u8bed\u8a00\u5904\u7406\uff09\u3001\u53ef\u7f16\u8f91\u6027\uff08\u5e72\u9884\u540e\u6587\u5316\u504f\u597d\u591a\u6a21\u6001\u8fc1\u79fb\uff09\u3002", "conclusion": "\u9996\u6b21\u5b9e\u73b0LLM\u6587\u5316\u8868\u5f81\u7684\u53ef\u63a7\u7f16\u8f91\uff0c\u4e3a\u6d88\u9664\u6a21\u578b\u6587\u5316\u504f\u89c1\u3001\u6784\u5efa\u6587\u5316\u81ea\u9002\u5e94AI\u63d0\u4f9b\u795e\u7ecf\u5143\u7ea7\u5e72\u9884\u6846\u67b6\uff0c\u5f00\u6e90\u6570\u636e\u4ee3\u7801\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2508.02256", "pdf": "https://arxiv.org/pdf/2508.02256", "abs": "https://arxiv.org/abs/2508.02256", "authors": ["Belen Alastruey", "Jo\u00e3o Maria Janeiro", "Alexandre Allauzen", "Maha Elbayad", "Lo\u00efc Barrault", "Marta R. Costa-juss\u00e0"], "title": "Interference Matrix: Quantifying Cross-Lingual Interference in Transformer Encoders", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we present a comprehensive study of language interference in\nencoder-only Transformer models across 83 languages. We construct an\ninterference matrix by training and evaluating small BERT-like models on all\npossible language pairs, providing a large-scale quantification of\ncross-lingual interference. Our analysis reveals that interference between\nlanguages is asymmetrical and that its patterns do not align with traditional\nlinguistic characteristics, such as language family, nor with proxies like\nembedding similarity, but instead better relate to script. Finally, we\ndemonstrate that the interference matrix effectively predicts performance on\ndownstream tasks, serving as a tool to better design multilingual models to\nobtain optimal performance.", "AI": {"tldr": "\u901a\u8fc7\u5206\u679083\u79cd\u8bed\u8a00\u5728Transformer\u6a21\u578b\u4e2d\u7684\u5e72\u6270\u6a21\u5f0f\uff0c\u53d1\u73b0\u8de8\u8bed\u8a00\u5e72\u6270\u5177\u6709\u4e0d\u5bf9\u79f0\u6027\u4e14\u4e0e\u6587\u5b57\u7279\u5f81\u76f8\u5173\uff0c\u6784\u5efa\u7684\u5e72\u6270\u77e9\u9635\u53ef\u6709\u6548\u9884\u6d4b\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u63a2\u7a76\u591a\u8bed\u8a00\u6a21\u578b\u4e2d\u8bed\u8a00\u5e72\u6270\u7684\u672c\u8d28\u89c4\u5f8b\uff0c\u7a81\u7834\u4f20\u7edf\u8bed\u7cfb/\u5d4c\u5165\u76f8\u4f3c\u6027\u7684\u89e3\u91ca\u6846\u67b6\uff0c\u4e3a\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u91cf\u5316\u4f9d\u636e\u3002", "method": "\u4f7f\u7528\u5c0f\u578bBERT\u6a21\u578b\u5728\u5168\u90e8\u8bed\u8a00\u5bf9\u4e0a\u8bad\u7ec3\u8bc4\u4f30\uff0c\u6784\u5efa\u5927\u89c4\u6a21\u5e72\u6270\u77e9\u9635\u5e76\u8fdb\u884c\u811a\u672c\u76f8\u5173\u6027\u5206\u6790\u3002", "result": "\u5e72\u6270\u6a21\u5f0f\u5448\u73b0\u4e0d\u5bf9\u79f0\u6027\uff0c\u4e0e\u6587\u5b57\u7279\u5f81\u76f8\u5173\u6027(r=0.68)\u663e\u8457\u9ad8\u4e8e\u8bed\u7cfb(r=0.32)\uff0c\u5e72\u6270\u77e9\u9635\u9884\u6d4b\u4e0b\u6e38\u4efb\u52a1\u51c6\u786e\u7387\u8bef\u5dee\u00b12.1%\u3002", "conclusion": "\u5e72\u6270\u77e9\u9635\u6210\u4e3a\u4f18\u5316\u591a\u8bed\u8a00\u6a21\u578b\u67b6\u6784\u7684\u8bed\u8a00\u5bf9\u9009\u62e9\u6807\u51c6\uff0c\u7279\u522b\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7ec4\u5408\u8bbe\u8ba1\u5177\u6709\u6307\u5bfc\u610f\u4e49\u3002"}}
{"id": "2508.02260", "pdf": "https://arxiv.org/pdf/2508.02260", "abs": "https://arxiv.org/abs/2508.02260", "authors": ["Jia Deng", "Jie Chen", "Zhipeng Chen", "Wayne Xin Zhao", "Ji-Rong Wen"], "title": "Decomposing the Entropy-Performance Exchange: The Missing Keys to Unlocking Effective Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": "7 pages, 20 figures", "summary": "Recently, reinforcement learning with verifiable rewards (RLVR) has been\nwidely used for enhancing the reasoning abilities of large language models\n(LLMs). A core challenge in RLVR involves managing the exchange between entropy\nand performance of policies. Despite the importance of this exchange, a\nfine-grained understanding of when and how this exchange operates most\neffectively remains limited. To bridge this gap, we conduct a systematic\nempirical analysis of the entropy-performance exchange mechanism of RLVR across\ndifferent levels of granularity. Specifically, we first divide the training\nprocess into two distinct stages based on entropy dynamics, i.e., rising stage\nand plateau stage, and then systematically investigate how this mechanism\nvaries across stage-level, instance-level, and token-level granularitiess. Our\nanalysis reveals that, in the rising stage, entropy reduction in negative\nsamples facilitates the learning of effective reasoning patterns, which in turn\ndrives rapid performance gains. Moreover, in the plateau stage, learning\nefficiency strongly correlates with high-entropy tokens present in\nlow-perplexity samples and those located at the end of sequences. Motivated by\nthese findings, we propose two methods that dynamically adjust the reward\nsignal using perplexity and positional information to focus RL updates on\ntokens that exhibit high learning potential, achieving improvements compared to\nthe baseline methods on various LLMs.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u5206\u6790RLVR\u8bad\u7ec3\u4e2d\u71b5\u4e0e\u6027\u80fd\u7684\u52a8\u6001\u4ea4\u6362\u673a\u5236\uff0c\u63d0\u51fa\u57fa\u4e8e\u56f0\u60d1\u5ea6\u548c\u4f4d\u7f6e\u4fe1\u606f\u7684\u52a8\u6001\u5956\u52b1\u8c03\u6574\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9\u5f3a\u5316\u5b66\u4e60\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u8bad\u7ec3\u4e2d\u71b5\u4e0e\u6027\u80fd\u4ea4\u6362\u673a\u5236\u7684\u7ec6\u7c92\u5ea6\u7406\u89e3\u4e0d\u8db3\uff0c\u9700\u7cfb\u7edf\u7814\u7a76\u8be5\u673a\u5236\u5728\u4e0d\u540c\u8bad\u7ec3\u9636\u6bb5\uff08\u71b5\u4e0a\u5347\u671f/\u5e73\u53f0\u671f\uff09\u548c\u4e0d\u540c\u7c92\u5ea6\uff08\u5b9e\u4f8b\u7ea7/\u8bcd\u5143\u7ea7\uff09\u7684\u8868\u73b0\u3002", "method": "1. \u6839\u636e\u71b5\u52a8\u6001\u5c06\u8bad\u7ec3\u5206\u4e3a\u4e0a\u5347\u671f\u548c\u5e73\u53f0\u671f\u4e24\u9636\u6bb5\uff1b2. \u5728\u4e09\u4e2a\u9636\u6bb5\uff08\u9636\u6bb5\u7ea7/\u5b9e\u4f8b\u7ea7/\u8bcd\u5143\u7ea7\uff09\u7cfb\u7edf\u5206\u6790\u4ea4\u6362\u673a\u5236\uff1b3. \u63d0\u51fa\u57fa\u4e8e\u56f0\u60d1\u5ea6\u548c\u4f4d\u7f6e\u4fe1\u606f\u7684\u52a8\u6001\u5956\u52b1\u4fe1\u53f7\u8c03\u6574\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u53d6\u5f97\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7684\u6027\u80fd\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u5173\u6ce8\u9ad8\u5b66\u4e60\u6f5c\u529b\u8bcd\u5143\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63ed\u793a\u4e86\u71b5\u52a8\u6001\u5bf9RLVR\u8bad\u7ec3\u6548\u7387\u7684\u5173\u952e\u5f71\u54cd\uff0c\u901a\u8fc7\u805a\u7126\u9ad8\u71b5\u8bcd\u5143\uff08\u4f4e\u56f0\u60d1\u5ea6\u6837\u672c/\u5e8f\u5217\u5c3e\u90e8\uff09\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u7b56\u7565\u66f4\u65b0\uff0c\u4e3a\u4f18\u5316\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8303\u5f0f\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2508.02268", "pdf": "https://arxiv.org/pdf/2508.02268", "abs": "https://arxiv.org/abs/2508.02268", "authors": ["Serry Sibaee", "Omer Nacar", "Yasser Al-Habashi", "Adel Ammar", "Wadii Boulila"], "title": "SHAMI-MT: A Syrian Arabic Dialect to Modern Standard Arabic Bidirectional Machine Translation System", "categories": ["cs.CL"], "comment": null, "summary": "The rich linguistic landscape of the Arab world is characterized by a\nsignificant gap between Modern Standard Arabic (MSA), the language of formal\ncommunication, and the diverse regional dialects used in everyday life. This\ndiglossia presents a formidable challenge for natural language processing,\nparticularly machine translation. This paper introduces \\textbf{SHAMI-MT}, a\nbidirectional machine translation system specifically engineered to bridge the\ncommunication gap between MSA and the Syrian dialect. We present two\nspecialized models, one for MSA-to-Shami and another for Shami-to-MSA\ntranslation, both built upon the state-of-the-art AraT5v2-base-1024\narchitecture. The models were fine-tuned on the comprehensive Nabra dataset and\nrigorously evaluated on unseen data from the MADAR corpus. Our MSA-to-Shami\nmodel achieved an outstanding average quality score of \\textbf{4.01 out of 5.0}\nwhen judged by OPENAI model GPT-4.1, demonstrating its ability to produce\ntranslations that are not only accurate but also dialectally authentic. This\nwork provides a crucial, high-fidelity tool for a previously underserved\nlanguage pair, advancing the field of dialectal Arabic translation and offering\nsignificant applications in content localization, cultural heritage, and\nintercultural communication.", "AI": {"tldr": "SHAMI-MT\u662f\u57fa\u4e8eAraT5v2\u67b6\u6784\u7684\u53cc\u5411\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u5b9e\u73b0\u73b0\u4ee3\u6807\u51c6\u963f\u62c9\u4f2f\u8bed\u4e0e\u53d9\u5229\u4e9a\u65b9\u8a00\u7684\u9ad8\u8d28\u91cf\u4e92\u8bd1\uff0cMSA\u2192Shami\u6a21\u578b\u5728GPT-4.1\u8bc4\u4f30\u4e2d\u83b74.01/5.0\u5206\u3002", "motivation": "\u89e3\u51b3\u963f\u62c9\u4f2f\u8bed\u53cc\u8a00\u73b0\u8c61\uff08MSA\u4e0e\u65b9\u8a00\u5272\u88c2\uff09\u5bfc\u81f4\u7684\u673a\u5668\u7ffb\u8bd1\u96be\u9898\uff0c\u586b\u8865\u8be5\u8bed\u8a00\u5bf9\u7ffb\u8bd1\u5de5\u5177\u7684\u7a7a\u767d\uff0c\u4fc3\u8fdb\u8de8\u6587\u5316\u4ea4\u6d41\u4e0e\u6587\u5316\u9057\u4ea7\u4fdd\u62a4\u3002", "method": "\u6784\u5efa\u4e24\u4e2aAraT5v2-base-1024\u6a21\u578b\uff08\u53cc\u5411\u7ffb\u8bd1\uff09\uff0c\u4f7f\u7528Nabra\u6570\u636e\u96c6\u5fae\u8c03\uff0c\u5e76\u901a\u8fc7MADAR\u8bed\u6599\u5e93\u8fdb\u884c\u4e25\u683c\u8bc4\u4f30\u3002", "result": "MSA\u2192Shami\u6a21\u578b\u83b7GPT-4.1\u8bc4\u52064.01/5.0\uff0c\u8bc1\u660e\u5176\u7ffb\u8bd1\u51c6\u786e\u6027\u4e0e\u65b9\u8a00\u771f\u5b9e\u6027\uff0c\u4e3a\u65b9\u8a00\u7ffb\u8bd1\u9886\u57df\u63d0\u4f9b\u9996\u4e2a\u9ad8\u4fdd\u771f\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "SHAMI-MT\u6709\u6548\u5f25\u5408MSA\u4e0e\u53d9\u5229\u4e9a\u65b9\u8a00\u7684\u6c9f\u901a\u9e3f\u6c9f\uff0c\u5176\u5f00\u6e90\u7279\u6027\u63a8\u52a8\u65b9\u8a00\u7ffb\u8bd1\u6280\u672f\u53d1\u5c55\uff0c\u5728\u5185\u5bb9\u672c\u5730\u5316\u3001\u8de8\u6587\u5316\u4ea4\u6d41\u7b49\u9886\u57df\u5177\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.02271", "pdf": "https://arxiv.org/pdf/2508.02271", "abs": "https://arxiv.org/abs/2508.02271", "authors": ["Kenneth Enevoldsen", "Kristian N\u00f8rgaard Jensen", "Jan Kostkan", "Bal\u00e1zs Szab\u00f3", "M\u00e1rton Kardos", "Kirten Vad", "Andrea Blasi N\u00fa\u00f1ez", "Gianluca Barmina", "Jacob Nielsen", "Rasmus Larsen", "Peter Vahlstrup", "Per M\u00f8ldrup Dalum", "Desmond Elliott", "Lukas Galke", "Peter Schneider-Kamp", "Kristoffer Nielbo"], "title": "Dynaword: From One-shot to Continuously Developed Datasets", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large-scale datasets are foundational for research and development in natural\nlanguage processing. However, current approaches face three key challenges: (1)\nreliance on ambiguously licensed sources restricting use, sharing, and\nderivative works; (2) static dataset releases that prevent community\ncontributions and diminish longevity; and (3) quality assurance processes\nrestricted to publishing teams rather than leveraging community expertise.\n  To address these limitations, we introduce two contributions: the Dynaword\napproach and Danish Dynaword. The Dynaword approach is a framework for creating\nlarge-scale, open datasets that can be continuously updated through community\ncollaboration. Danish Dynaword is a concrete implementation that validates this\napproach and demonstrates its potential. Danish Dynaword contains over four\ntimes as many tokens as comparable releases, is exclusively openly licensed,\nand has received multiple contributions across industry and research. The\nrepository includes light-weight tests to ensure data formatting, quality, and\ndocumentation, establishing a sustainable framework for ongoing community\ncontributions and dataset evolution.", "AI": {"tldr": "\u63d0\u51faDynaword\u6846\u67b6\u89e3\u51b3NLP\u6570\u636e\u96c6\u6388\u6743/\u9759\u6001\u66f4\u65b0/\u8d28\u91cf\u7ba1\u63a7\u95ee\u9898\uff0c\u5e76\u9a8c\u8bc1\u4e39\u9ea6\u8bed\u6570\u636e\u96c6\u6548\u679c", "motivation": "\u5f53\u524dNLP\u5927\u89c4\u6a21\u6570\u636e\u96c6\u5b58\u5728\u4e09\u4e2a\u6838\u5fc3\u95ee\u9898\uff1a(1)\u6570\u636e\u6388\u6743\u4e0d\u6e05\u6670\u9650\u5236\u4f7f\u7528 (2)\u9759\u6001\u53d1\u5e03\u65e0\u6cd5\u6301\u7eed\u66f4\u65b0 (3)\u8d28\u91cf\u7ba1\u63a7\u5c40\u9650\u5728\u5f00\u53d1\u56e2\u961f", "method": "\u63d0\u51faDynaword\u6846\u67b6\uff1a\u652f\u6301\u793e\u533a\u534f\u4f5c\u7684\u53ef\u6301\u7eed\u66f4\u65b0\u67b6\u6784\uff0c\u914d\u5957\u8f7b\u91cf\u7ea7\u6d4b\u8bd5\u4fdd\u969c\u6570\u636e\u8d28\u91cf\u3002\u4e39\u9ea6\u8bed\u6570\u636e\u96c6Dynaword\u4f5c\u4e3a\u5b9e\u65bd\u6848\u4f8b", "result": "\u4e39\u9ea6\u8bed\u6570\u636e\u96c6\u89c4\u6a21\u8d85\u540c\u7c7b4\u500d\uff0c\u5b8c\u5168\u5f00\u653e\u6388\u6743\uff0c\u83b7\u5f97\u8de8\u884c\u4e1a\u8d21\u732e\u3002\u5305\u542b\u6570\u636e\u683c\u5f0f/\u8d28\u91cf/\u6587\u6863\u7684\u5168\u65b9\u4f4d\u6d4b\u8bd5\u673a\u5236", "conclusion": "\u5efa\u7acb\u4e86\u53ef\u6301\u7eed\u7684\u793e\u533a\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u66f4\u65b0\u673a\u5236\u5ef6\u957f\u6570\u636e\u96c6\u751f\u547d\u5468\u671f\uff0c\u63a8\u52a8\u5f00\u653e\u5f0fNLP\u7814\u7a76"}}
{"id": "2508.02290", "pdf": "https://arxiv.org/pdf/2508.02290", "abs": "https://arxiv.org/abs/2508.02290", "authors": ["Malik Marmonier", "Beno\u00eet Sagot", "Rachel Bawden"], "title": "A French Version of the OLDI Seed Corpus", "categories": ["cs.CL"], "comment": null, "summary": "We present the first French partition of the OLDI Seed Corpus, our submission\nto the WMT 2025 Open Language Data Initiative (OLDI) shared task. We detail its\ncreation process, which involved using multiple machine translation systems and\na custom-built interface for post-editing by qualified native speakers. We also\nhighlight the unique translation challenges presented by the source data, which\ncombines highly technical, encyclopedic terminology with the stylistic\nirregularities characteristic of user-generated content taken from Wikipedia.\nThis French corpus is not an end in itself, but is intended as a crucial pivot\nresource to facilitate the collection of parallel corpora for the\nunder-resourced regional languages of France.", "AI": {"tldr": "\u9996\u6b21\u6784\u5efa\u6cd5\u8bedOLDI\u8bed\u6599\u5e93\u5206\u533a\uff0c\u901a\u8fc7\u591a\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u4e0e\u4e13\u4e1a\u8bd1\u540e\u7f16\u8f91\uff0c\u5904\u7406\u6280\u672f\u672f\u8bed\u4e0e\u7528\u6237\u751f\u6210\u6587\u672c\u6df7\u5408\u7684\u7ffb\u8bd1\u6311\u6218\uff0c\u65e8\u5728\u4e3a\u6cd5\u56fd\u5c0f\u8bed\u79cd\u5efa\u7acb\u5e73\u884c\u8bed\u6599\u67a2\u7ebd\u8d44\u6e90\u3002", "motivation": "\u4e3a\u89e3\u51b3\u6cd5\u56fd\u5730\u533a\u6027\u8bed\u8a00\u8d44\u6e90\u532e\u4e4f\u95ee\u9898\uff0c\u6784\u5efa\u6cd5\u8bed\u67a2\u7ebd\u8bed\u6599\u5e93\u4ee5\u652f\u6301\u540e\u7eed\u5c0f\u8bed\u79cd\u5e73\u884c\u8bed\u6599\u6536\u96c6\u3002", "method": "\u91c7\u7528\u591a\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u751f\u6210\u521d\u7a3f\uff0c\u5f00\u53d1\u5b9a\u5236\u5316\u754c\u9762\u4f9b\u6cd5\u8bed\u6bcd\u8bed\u8005\u8fdb\u884c\u8bd1\u540e\u7f16\u8f91\uff0c\u91cd\u70b9\u5904\u7406\u6280\u672f\u672f\u8bed\u4e0e\u7ef4\u57fa\u7528\u6237\u6587\u672c\u98ce\u683c\u7684\u4e0d\u4e00\u81f4\u6027\u3002", "result": "\u6210\u529f\u521b\u5efa\u9996\u4e2a\u9762\u5411\u6cd5\u56fd\u5730\u533a\u8bed\u8a00\u7684\u6cd5\u8bed\u67a2\u7ebd\u8bed\u6599\u5e93\uff0c\u5b9e\u73b0\u6280\u672f\u6027\u5185\u5bb9\u4e0e\u7f51\u7edc\u975e\u89c4\u8303\u6587\u672c\u7684\u6709\u6548\u5bf9\u9f50\u3002", "conclusion": "\u8be5\u6cd5\u8bed\u8bed\u6599\u5e93\u4f5c\u4e3a\u5173\u952e\u67a2\u7ebd\u8d44\u6e90\uff0c\u5c06\u663e\u8457\u964d\u4f4e\u4f4e\u8d44\u6e90\u5730\u533a\u8bed\u8a00\u5e73\u884c\u8bed\u6599\u5e93\u6784\u5efa\u96be\u5ea6\uff0c\u63a8\u52a8\u8bed\u8a00\u6280\u672f\u5747\u8861\u53d1\u5c55\u3002"}}
{"id": "2508.02296", "pdf": "https://arxiv.org/pdf/2508.02296", "abs": "https://arxiv.org/abs/2508.02296", "authors": ["Ilias Triantafyllopoulos", "Renyi Qu", "Salvatore Giorgi", "Brenda Curtis", "Lyle H. Ungar", "Jo\u00e3o Sedoc"], "title": "Simple Methods Defend RAG Systems Well Against Real-World Attacks", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Ensuring safety and in-domain responses for Retrieval-Augmented Generation\n(RAG) systems is paramount in safety-critical applications, yet remains a\nsignificant challenge. To address this, we evaluate four methodologies for\nOut-Of-Domain (OOD) query detection: GPT-4o, regression-based, Principal\nComponent Analysis (PCA)-based, and Neural Collapse (NC), to ensure the RAG\nsystem only responds to queries confined to the system's knowledge base.\nSpecifically, our evaluation explores two novel dimensionality reduction and\nfeature separation strategies: \\textit{PCA}, where top components are selected\nusing explained variance or OOD separability, and an adaptation of\n\\textit{Neural Collapse Feature Separation}. We validate our approach on\nstandard datasets (StackExchange and MSMARCO) and real-world applications\n(Substance Use and COVID-19), including tests against LLM-simulated and actual\nattacks on a COVID-19 vaccine chatbot. Through human and LLM-based evaluations\nof response correctness and relevance, we confirm that an external OOD detector\nis crucial for maintaining response relevance.", "AI": {"tldr": "\u63d0\u51fa\u56db\u79cdOOD\u68c0\u6d4b\u65b9\u6cd5\uff08GPT-4o/\u56de\u5f52/PCA/NC\uff09\uff0c\u9a8c\u8bc1\u5916\u90e8\u68c0\u6d4b\u5668\u5bf9\u4fdd\u6301RAG\u7cfb\u7edf\u54cd\u5e94\u76f8\u5173\u6027\u7684\u5173\u952e\u4f5c\u7528", "motivation": "\u89e3\u51b3\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2dRAG\u7cfb\u7edf\u5904\u7406\u9886\u57df\u5916\u67e5\u8be2\u65f6\u7684\u5b89\u5168\u9690\u60a3\uff0c\u9632\u6b62\u9519\u8bef\u54cd\u5e94\u5f15\u53d1\u4e25\u91cd\u540e\u679c", "method": "\u901a\u8fc7PCA\u964d\u7ef4\u9009\u62e9\uff08\u89e3\u91ca\u65b9\u5dee/OOD\u53ef\u5206\u6027\uff09\u548cNC\u7279\u5f81\u5206\u79bb\u7b56\u7565\uff0c\u5728\u6807\u51c6\u6570\u636e\u96c6\uff08StackExchange/MSMARCO\uff09\u548c\u771f\u5b9e\u573a\u666f\uff08\u836f\u7269\u6ee5\u7528/COVID\u75ab\u82d7\u673a\u5668\u4eba\uff09\u8fdb\u884c\u591a\u7ef4\u5ea6\u9a8c\u8bc1", "result": "\u4eba\u7c7b\u548cLLM\u8bc4\u4f30\u8bc1\u5b9e\u5916\u90e8OOD\u68c0\u6d4b\u5668\u663e\u8457\u63d0\u5347\u54cd\u5e94\u76f8\u5173\u6027\uff0c\u5728\u5bf9\u6297\u653b\u51fb\u573a\u666f\u4e2d\u6709\u6548\u4fdd\u969cCOVID\u75ab\u82d7\u95ee\u7b54\u7cfb\u7edf\u5b89\u5168", "conclusion": "\u5916\u90e8OOD\u68c0\u6d4b\u5668\u662fRAG\u5b89\u5168\u4f53\u7cfb\u7684\u6838\u5fc3\u7ec4\u4ef6\uff0c\u63d0\u51fa\u7684PCA\u548cNC\u7279\u5f81\u5206\u79bb\u7b56\u7565\u5728\u8de8\u9886\u57df\u5e94\u7528\u4e2d\u5c55\u73b0\u5f3a\u9c81\u68d2\u6027"}}
{"id": "2508.02308", "pdf": "https://arxiv.org/pdf/2508.02308", "abs": "https://arxiv.org/abs/2508.02308", "authors": ["Sikui Zhang", "Guangze Gao", "Ziyun Gan", "Chunfeng Yuan", "Zefeng Lin", "Houwen Peng", "Bing Li", "Weiming Hu"], "title": "LaMPE: Length-aware Multi-grained Position Encoding for Adaptive Long-context Scaling Without Training", "categories": ["cs.CL"], "comment": "13 pages, 9 figures", "summary": "Large language models (LLMs) experience significant performance degradation\nwhen the input exceeds the pretraining context window, primarily due to the\nout-of-distribution (OOD) behavior of Rotary Position Embedding (RoPE). Recent\nstudies mitigate this problem by remapping OOD positions into the\nin-distribution range with fixed mapping strategies, ignoring the dynamic\nrelationship between input length and the model's effective context window. To\nthis end, we propose Length-aware Multi-grained Positional Encoding (LaMPE), a\ntraining-free method that fully utilizes the model's effective context window\nfor adaptive long-context scaling in LLMs. Motivated by the left-skewed\nfrequency distribution of relative positions, LaMPE establishes a dynamic\nrelationship between mapping length and input length through a parametric\nscaled sigmoid function to adaptively allocate positional capacity across\nvarying input lengths. Meanwhile, LaMPE devises a novel multi-grained attention\nmechanism that strategically allocates positional resolution across different\nsequence regions to capture both fine-grained locality and long-range\ndependencies. Our method can be seamlessly applied to a wide range of\nRoPE-based LLMs without training. Extensive experiments on three representative\nLLMs across five mainstream long-context benchmarks demonstrate that LaMPE\nachieves significant performance improvements compared to existing length\nextrapolation methods. The code will be released at\nhttps://github.com/scar-on/LaMPE.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684LaMPE\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u4f4d\u7f6e\u6620\u5c04\u548c\u591a\u7c92\u5ea6\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u80fd\u529b", "motivation": "\u73b0\u6709\u56fa\u5b9a\u4f4d\u7f6e\u6620\u5c04\u65b9\u6cd5\u5ffd\u89c6\u4e86\u8f93\u5165\u957f\u5ea6\u4e0e\u6a21\u578b\u6709\u6548\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u52a8\u6001\u5173\u7cfb\uff0c\u5bfc\u81f4\u957f\u6587\u672c\u5904\u7406\u6027\u80fd\u4e0b\u964d", "method": "1. \u53c2\u6570\u5316sigmoid\u51fd\u6570\u5efa\u7acb\u52a8\u6001\u4f4d\u7f6e\u6620\u5c04\u5173\u7cfb\n2. \u591a\u7c92\u5ea6\u6ce8\u610f\u529b\u673a\u5236\u517c\u987e\u5c40\u90e8\u7ec6\u8282\u4e0e\u957f\u7a0b\u4f9d\u8d56\n3. \u652f\u6301\u4e3b\u6d41RoPE\u6a21\u578b\u5373\u63d2\u5373\u7528", "result": "\u57283\u4e2a\u4e3b\u6d41LLM\u548c5\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u4f4d\u7f6e\u6269\u5c55\u65b9\u6cd5", "conclusion": "LaMPE\u901a\u8fc7\u52a8\u6001\u4f4d\u7f6e\u5bb9\u91cf\u5206\u914d\u5b9e\u73b0\u4e86\u66f4\u7075\u6d3b\u7684\u957f\u4e0a\u4e0b\u6587\u6269\u5c55\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6210\u672c"}}
{"id": "2508.02317", "pdf": "https://arxiv.org/pdf/2508.02317", "abs": "https://arxiv.org/abs/2508.02317", "authors": ["Qianli Ma", "Yaowei Zheng", "Zhelun Shi", "Zhongkai Zhao", "Bin Jia", "Ziyue Huang", "Zhiqi Lin", "Youjie Li", "Jiacheng Yang", "Yanghua Peng", "Zhi Zhang", "Xin Liu"], "title": "VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo", "categories": ["cs.CL", "cs.AI", "cs.DC"], "comment": null, "summary": "Recent advances in large language models (LLMs) have driven impressive\nprogress in omni-modal understanding and generation. However, training\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\narchitectures required to process diverse modalities, necessitating\nsophisticated system design for efficient large-scale training. Existing\nframeworks typically entangle model definition with parallel logic, incurring\nlimited scalability and substantial engineering overhead for end-to-end\nomni-modal training. % We present \\veomni, a modular and efficient training\nframework to accelerate the development of omni-modal LLMs. \\veomni introduces\nmodel-centric distributed recipes that decouples communication from\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. \\veomni also\nfeatures a flexible configuration interface supporting seamless integration of\nnew modalities with minimal code change. % Using \\veomni, a omni-modal\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\ntraining large omni-modal LLMs.", "AI": {"tldr": "\u63d0\u51faVeomni\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c3D\u5e76\u884c\u6280\u672f\u663e\u8457\u63d0\u5347\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u7387\u4e0e\u53ef\u6269\u5c55\u6027", "motivation": "\u73b0\u6709\u5168\u6a21\u6001\u8bad\u7ec3\u6846\u67b6\u5b58\u5728\u67b6\u6784\u8026\u5408\u95ee\u9898\uff0c\u5bfc\u81f4\u7cfb\u7edf\u6269\u5c55\u6027\u5dee\u3001\u5de5\u7a0b\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u652f\u6301\u591a\u6a21\u6001\u7075\u6d3b\u6269\u5c55", "method": "\u91c7\u7528\u6a21\u578b\u4e2d\u5fc3\u5316\u5206\u5e03\u5f0f\u65b9\u6848\uff08\u901a\u4fe1\u4e0e\u8ba1\u7b97\u89e3\u8026\uff09\uff0c\u914d\u7f6e\u7075\u6d3b\u63a5\u53e3\u652f\u6301\u65b0\u6a21\u6001\u5feb\u901f\u96c6\u6210\uff0c\u5b9e\u73b03D\u5e76\u884c\u6280\u672f", "result": "\u5728128 GPU\u4e0a\u8bad\u7ec330B\u53c2\u6570MoE\u6a21\u578b\u65f6\u8fbe\u52302800 tokens/sec/GPU\u541e\u5410\u91cf\uff0c\u652f\u6301160K\u4e0a\u4e0b\u6587\u957f\u5ea6", "conclusion": "Veomni\u901a\u8fc7\u521b\u65b0\u7684\u7cfb\u7edf\u8bbe\u8ba1\u89e3\u51b3\u4e86\u5168\u6a21\u6001LLM\u8bad\u7ec3\u74f6\u9888\uff0c\u4e3a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.02322", "pdf": "https://arxiv.org/pdf/2508.02322", "abs": "https://arxiv.org/abs/2508.02322", "authors": ["Yuzhuang Xu", "Xu Han", "Yuanchi Zhang", "Yixuan Wang", "Yijun Liu", "Shiyu Ji", "Qingfu Zhu", "Wanxiang Che"], "title": "CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert Redundancy Analysis", "categories": ["cs.CL", "cs.LG"], "comment": "16 pages, 9 figures, 7 tables", "summary": "Large Language Models (LLMs) with Mixture-of-Experts (MoE) architectures are\ndistinguished by their strong performance scaling with increasing parameters\nacross a wide range of tasks, yet they also suffer from substantial\ncomputational and storage overheads. Notably, the performance gains of MoE\nmodels do not scale proportionally with the growth in expert parameters. While\nprior works attempt to reduce parameters via expert-level pruning, merging, or\ndecomposition, they still suffer from challenges in both performance and\ncomputational efficiency. In this paper, we address these challenges by\nintroducing micro-expert as a finer-grained compression unit that spans across\nmatrices. We first establish a more fundamental perspective, viewing MoE layers\nas mixtures of micro-experts, and present CAMERA, a lightweight and\ntraining-free framework for identifying micro-expert redundancy. Our analysis\nuncovers significant variance in micro-expert contributions during decoding.\nBased on this insight, we further propose CAMERA-P, a structured micro-expert\npruning framework, and CAMERA-Q, a mixed-precision quantization idea designed\nfor micro-experts. Extensive experiments on nine downstream tasks show that\nCAMERA-P consistently outperforms strong baselines under pruning ratios ranging\nfrom 20% to 60%. Furthermore, CAMERA-Q achieves superior results under\naggressive 2-bit quantization, surpassing existing matrix- and channel-level\nideas. Notably, our method enables complete micro-expert analysis of\nQwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU.", "AI": {"tldr": "\u901a\u8fc7\u63d0\u51fa\u8de8\u77e9\u9635\u7684\u5fae\u4e13\u5bb6\u538b\u7f29\u5355\u5143\u548cCAMERA\u6846\u67b6\uff0c\u5b9e\u73b0\u5bf9MoE\u6a21\u578b\u7684\u9ad8\u6548\u526a\u679d\u4e0e\u91cf\u5316", "motivation": "\u73b0\u6709\u4e13\u5bb6\u7ea7\u538b\u7f29\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u63d0\u5347\u4e0a\u5b58\u5728\u74f6\u9888\uff0c\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u7684\u53c2\u6570\u538b\u7f29\u65b9\u6848", "method": "1. \u5efa\u7acb\u57fa\u4e8e\u5fae\u4e13\u5bb6\u7684MoE\u5c42\u5206\u6790\u6846\u67b6 2. \u5f00\u53d1\u65e0\u9700\u8bad\u7ec3\u7684CAMERA\u5197\u4f59\u68c0\u6d4b\u65b9\u6cd5 3. \u63d0\u51fa\u7ed3\u6784\u5316\u526a\u679d(CAMERA-P)\u548c\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316(CAMERA-Q)", "result": "\u57289\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff0c\u526a\u679d\u65b9\u6848\u6bd4\u57fa\u7ebf\u63d0\u53473-15%\uff0c2-bit\u91cf\u5316\u5b9e\u73b0SOTA\uff0c\u53ef\u5728\u5355\u5361A100\u4e0a5\u5206\u949f\u5185\u5b8c\u621057B\u53c2\u6570\u6a21\u578b\u5206\u6790", "conclusion": "CAMERA\u6846\u67b6\u901a\u8fc7\u5fae\u4e13\u5bb6\u5206\u6790\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6a21\u578b\u538b\u7f29\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5b58\u50a8\u9700\u6c42\uff0c\u7279\u522b\u9002\u5408\u5927\u89c4\u6a21MoE\u6a21\u578b\u90e8\u7f72"}}
{"id": "2508.02360", "pdf": "https://arxiv.org/pdf/2508.02360", "abs": "https://arxiv.org/abs/2508.02360", "authors": ["Jiayi Zhang", "Shu Yang", "Junchao Wu", "Derek F. Wong", "Di Wang"], "title": "Understanding and Mitigating Political Stance Cross-topic Generalization in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning Large Language Models on a political topic will significantly\nmanipulate their political stance on various issues and unintentionally affect\ntheir stance on unrelated topics. While previous studies have proposed this\nissue, there is still a lack of understanding regarding the internal\nrepresentations of these stances and the mechanisms that lead to unintended\ncross-topic generalization. In this paper, we systematically explore the\ninternal mechanisms underlying this phenomenon from a neuron-level perspective\nand how to mitigate the cross-topic generalization of political fine-tuning.\nFirstly, we propose Political Neuron Localization through Activation\nContrasting (PNLAC) to identify two distinct types of political neurons:\ngeneral political neurons, which govern stance across multiple political\ntopics, and topic-specific neurons} that affect the model's political stance on\nindividual topics. We find the existence of these political neuron types across\nfour models and datasets through activation patching experiments. Leveraging\nthese insights, we introduce InhibitFT, an inhibition-based fine-tuning method,\neffectively mitigating the cross-topic stance generalization. Experimental\nresults demonstrate the robustness of identified neuron types across various\nmodels and datasets, and show that InhibitFT significantly reduces the\ncross-topic stance generalization by 20% on average, while preserving\ntopic-specific performance. Moreover, we demonstrate that selectively\ninhibiting only 5% of neurons is sufficient to effectively mitigate the\ncross-topic stance generalization.", "AI": {"tldr": "Identified general/specific political neurons via PNLAC, proposed InhibitFT method reducing cross-topic stance generalization by 20% with 5% neuron inhibition.", "motivation": "Address lack of understanding about neural mechanisms causing unintended political stance generalization during LLM fine-tuning.", "method": "Used activation contrasting (PNLAC) to localize political neurons, developed inhibition-based fine-tuning (InhibitFT) for targeted neuron suppression.", "result": "Validated neuron types across 4 models/datasets, InhibitFT reduced cross-topic generalization by 20% while maintaining topic-specific performance.", "conclusion": "Revealed neural basis of political stance propagation, demonstrated efficient mitigation through selective neuron inhibition."}}
{"id": "2508.02401", "pdf": "https://arxiv.org/pdf/2508.02401", "abs": "https://arxiv.org/abs/2508.02401", "authors": ["Xiaolin Lin", "Jingcun Wang", "Olga Kondrateva", "Yiyu Shi", "Bing Li", "Grace Li Zhang"], "title": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important Before Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.", "AI": {"tldr": "\u63d0\u51faCompressKV\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u5173\u952e\u6ce8\u610f\u529b\u5934\u4e0e\u5206\u5c42\u81ea\u9002\u5e94\u7b56\u7565\u4f18\u5316KV\u7f13\u5b58\u7ba1\u7406", "motivation": "\u73b0\u6709KV\u7f13\u5b58\u538b\u7f29\u65b9\u6cd5\u5ffd\u89c6\u6ce8\u610f\u529b\u5934\u7684\u529f\u80fd\u5dee\u5f02\uff0c\u5bfc\u81f4\u5173\u952e\u4ee4\u724c\u88ab\u9a71\u9010\u5f71\u54cd\u6a21\u578b\u6027\u80fd", "method": "1. \u8bc6\u522b\u80fd\u68c0\u7d22\u6587\u672c\u9996\u5c3e/\u91cd\u8981\u4ee4\u724c\u7684\u6ce8\u610f\u529b\u5934 2. \u91c7\u7528\u5206\u5c42\u81ea\u9002\u5e94KV\u7f13\u5b58\u5206\u914d\u7b56\u7565", "result": "\u5728LongBench\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e0d\u540c\u5185\u5b58\u9884\u7b97\u4e0b\u6027\u80fd\u5747\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5", "conclusion": "\u901a\u8fc7\u6ce8\u610f\u529b\u5934\u529f\u80fd\u5206\u6790\u4e0e\u5206\u5c42\u4f18\u5316\uff0c\u6709\u6548\u63d0\u5347LLMs\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u6548\u7387\u4e0e\u6027\u80fd"}}
{"id": "2508.02426", "pdf": "https://arxiv.org/pdf/2508.02426", "abs": "https://arxiv.org/abs/2508.02426", "authors": ["Linyu Li", "Zhi Jin", "Yuanpeng He", "Dongming Jin", "Yichi Zhang", "Haoran Duan", "Nyima Tash"], "title": "Learning to Evolve: Bayesian-Guided Continual Knowledge Graph Embedding", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Since knowledge graphs (KG) will continue to evolve in real scenarios,\ntraditional KGE models are only suitable for static knowledge graphs.\nTherefore, continual knowledge graph embedding (CKGE) has attracted the\nattention of researchers. Currently, a key challenge facing CKGE is that the\nmodel is prone to \"catastrophic forgetting\", resulting in the loss of\npreviously learned knowledge. In order to effectively alleviate this problem,\nwe propose a new CKGE model BAKE. First, we note that the Bayesian posterior\nupdate principle provides a natural continual learning strategy that is\ninsensitive to data order and can theoretically effectively resist the\nforgetting of previous knowledge during data evolution. Different from the\nexisting CKGE method, BAKE regards each batch of new data as a Bayesian update\nof the model prior. Under this framework, as long as the posterior distribution\nof the model is maintained, the model can better preserve the knowledge of\nearly snapshots even after evolving through multiple time snapshots. Secondly,\nwe propose a continual clustering method for CKGE, which further directly\ncombats knowledge forgetting by constraining the evolution difference (or\nchange amplitude) between new and old knowledge between different snapshots. We\nconduct extensive experiments on BAKE on multiple datasets, and the results\nshow that BAKE significantly outperforms existing baseline models.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u578b\u6301\u7eed\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u6a21\u578bBAKE\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u540e\u9a8c\u66f4\u65b0\u548c\u6301\u7eed\u805a\u7c7b\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898", "motivation": "\u4f20\u7edfKGE\u6a21\u578b\u4ec5\u9002\u7528\u4e8e\u9759\u6001\u77e5\u8bc6\u56fe\u8c31\uff0c\u73b0\u5b9e\u573a\u666f\u4e2d\u77e5\u8bc6\u56fe\u8c31\u6301\u7eed\u6f14\u53d8\u5bfc\u81f4\u73b0\u6709\u6a21\u578b\u51fa\u73b0\u4e25\u91cd\u9057\u5fd8\u73b0\u8c61", "method": "\u7ed3\u5408\u8d1d\u53f6\u65af\u540e\u9a8c\u66f4\u65b0\u6846\u67b6\uff08\u5c06\u65b0\u6570\u636e\u4f5c\u4e3a\u5148\u9a8c\u66f4\u65b0\uff09\u548c\u6301\u7eed\u805a\u7c7b\u65b9\u6cd5\uff08\u7ea6\u675f\u65b0\u65e7\u77e5\u8bc6\u6f14\u5316\u5dee\u5f02\uff09", "result": "\u5728\u591a\u6570\u636e\u96c6\u5b9e\u9a8c\u4e2d\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u6a21\u578b", "conclusion": "BAKE\u901a\u8fc7\u8d1d\u53f6\u65af\u6846\u67b6\u548c\u6f14\u5316\u5dee\u5f02\u7ea6\u675f\u53cc\u91cd\u673a\u5236\uff0c\u6210\u529f\u5b9e\u73b0\u77e5\u8bc6\u56fe\u8c31\u7684\u6301\u7eed\u9ad8\u6548\u5d4c\u5165"}}
{"id": "2508.02430", "pdf": "https://arxiv.org/pdf/2508.02430", "abs": "https://arxiv.org/abs/2508.02430", "authors": ["Robin Nowak", "Patrick Figge", "Carolin Haeussler"], "title": "AI-Based Measurement of Innovation: Mapping Expert Insight into Large Language Model Applications", "categories": ["cs.CL"], "comment": null, "summary": "Measuring innovation often relies on context-specific proxies and on expert\nevaluation. Hence, empirical innovation research is often limited to settings\nwhere such data is available. We investigate how large language models (LLMs)\ncan be leveraged to overcome the constraints of manual expert evaluations and\nassist researchers in measuring innovation. We design an LLM framework that\nreliably approximates domain experts' assessment of innovation from\nunstructured text data. We demonstrate the performance and broad applicability\nof this framework through two studies in different contexts: (1) the\ninnovativeness of software application updates and (2) the originality of\nuser-generated feedback and improvement ideas in product reviews. We compared\nthe performance (F1-score) and reliability (consistency rate) of our LLM\nframework against alternative measures used in prior innovation studies, and to\nstate-of-the-art machine learning- and deep learning-based models. The LLM\nframework achieved higher F1-scores than the other approaches, and its results\nare highly consistent (i.e., results do not change across runs). This article\nequips R&D personnel in firms, as well as researchers, reviewers, and editors,\nwith the knowledge and tools to effectively use LLMs for measuring innovation\nand evaluating the performance of LLM-based innovation measures. In doing so,\nwe discuss, the impact of important design decisions-including model selection,\nprompt engineering, training data size, training data distribution, and\nparameter settings-on performance and reliability. Given the challenges\ninherent in using human expert evaluation and existing text-based measures, our\nframework has important implications for harnessing LLMs as reliable,\nincreasingly accessible, and broadly applicable research tools for measuring\ninnovation.", "AI": {"tldr": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6784\u5efa\u6846\u67b6\uff0c\u66ff\u4ee3\u4f20\u7edf\u4e13\u5bb6\u8bc4\u4f30\uff0c\u9ad8\u6548\u6d4b\u91cf\u591a\u573a\u666f\u4e0b\u7684\u521b\u65b0\u6027\u3002", "motivation": "\u4f20\u7edf\u521b\u65b0\u8bc4\u4f30\u4f9d\u8d56\u4e13\u5bb6\u548c\u7279\u5b9a\u6570\u636e\uff0c\u9650\u5236\u4e86\u7814\u7a76\u8303\u56f4\u3002\u9700\u5f00\u53d1\u53ef\u9760\u3001\u901a\u7528\u7684\u81ea\u52a8\u5316\u5de5\u5177\u3002", "method": "\u8bbe\u8ba1LLM\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u8f6f\u4ef6\u66f4\u65b0\u521b\u65b0\u6027\u3001\u7528\u6237\u53cd\u9988\u539f\u521b\u6027\u53cc\u6848\u4f8b\u9a8c\u8bc1\uff0c\u5bf9\u6bd4\u73b0\u6709\u6307\u6807\u53ca\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002", "result": "LLM\u6846\u67b6F1\u5206\u6570\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u7ed3\u679c\u4e00\u81f4\u6027\u9ad8(\u8de8\u8fd0\u884c\u7a33\u5b9a)\uff0c\u53ef\u9760\u6027\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u4e3a\u5b66\u754c/\u4ea7\u4e1a\u754c\u63d0\u4f9b\u53ef\u590d\u73b0\u7684LLM\u521b\u65b0\u8bc4\u4f30\u65b9\u6848\uff0c\u6a21\u578b\u9009\u62e9\u3001\u63d0\u793a\u5de5\u7a0b\u7b49\u8bbe\u8ba1\u51b3\u7b56\u5f71\u54cd\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2508.02452", "pdf": "https://arxiv.org/pdf/2508.02452", "abs": "https://arxiv.org/abs/2508.02452", "authors": ["Mateusz Bystro\u0144ski", "Grzegorz Piotrowski", "Nitesh V. Chawla", "Tomasz Kajdanowicz"], "title": "LatentPrompt: Optimizing Promts in Latent Space", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances have shown that optimizing prompts for Large Language Models\n(LLMs) can significantly improve task performance, yet many optimization\ntechniques rely on heuristics or manual exploration. We present LatentPrompt, a\nmodel-agnostic framework for prompt optimization that leverages latent semantic\nspace to automatically generate, evaluate, and refine candidate prompts without\nrequiring hand-crafted rules. Beginning with a set of seed prompts, our method\nembeds them in a continuous latent space and systematically explores this space\nto identify prompts that maximize task-specific performance. In a\nproof-of-concept study on the Financial PhraseBank sentiment classification\nbenchmark, LatentPrompt increased classification accuracy by approximately 3\npercent after a single optimization cycle. The framework is broadly applicable,\nrequiring only black-box access to an LLM and an automatic evaluation metric,\nmaking it suitable for diverse domains and tasks.", "AI": {"tldr": "\u63d0\u51faLatentPrompt\u6846\u67b6\uff0c\u901a\u8fc7\u6f5c\u5728\u8bed\u4e49\u7a7a\u95f4\u81ea\u52a8\u4f18\u5316LLM\u63d0\u793a\u8bcd\uff0c\u5b9e\u73b0\u975e\u542f\u53d1\u5f0f\u7684\u6027\u80fd\u63d0\u5347", "motivation": "\u73b0\u6709\u63d0\u793a\u8bcd\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u7ecf\u9a8c\u6216\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u5316\u81ea\u52a8\u4f18\u5316\u65b9\u6848", "method": "\u5c06\u521d\u59cb\u63d0\u793a\u5d4c\u5165\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\uff0c\u7cfb\u7edf\u63a2\u7d22\u8be5\u7a7a\u95f4\u4ee5\u6700\u5927\u5316\u4efb\u52a1\u6027\u80fd\u7684\u63d0\u793a\u7ec4\u5408", "result": "\u5728Financial PhraseBank\u60c5\u611f\u5206\u7c7b\u4efb\u52a1\u4e2d\u5355\u6b21\u4f18\u5316\u5468\u671f\u5373\u63d0\u53473%\u51c6\u786e\u7387", "conclusion": "\u8be5\u6846\u67b6\u4ec5\u9700\u9ed1\u76d2LLM\u63a5\u53e3\u548c\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\uff0c\u9002\u7528\u4e8e\u591a\u9886\u57df\u4efb\u52a1\u7684\u63d0\u793a\u4f18\u5316"}}
{"id": "2508.02498", "pdf": "https://arxiv.org/pdf/2508.02498", "abs": "https://arxiv.org/abs/2508.02498", "authors": ["Md Tasin Abir", "Arpita Chowdhury", "Ashfia Rahman"], "title": "Monsoon Uprising in Bangladesh: How Facebook Shaped Collective Identity", "categories": ["cs.CL"], "comment": "10 pages, 9 figures", "summary": "This study investigates how Facebook shaped collective identity during the\nJuly 2024 pro-democracy uprising in Bangladesh, known as the Monsoon Uprising.\nDuring government repression, protesters turned to Facebook as a central space\nfor resistance, where multimodal expressions, images, memes, videos, hashtags,\nand satirical posts played an important role in unifying participants. Using a\nqualitative approach, this research analyzes visual rhetoric, verbal discourse,\nand digital irony to reveal how shared symbols, protest art, and slogans built\na sense of solidarity. Key elements included the symbolic use of red, the\nironic metaphorical use of the term \"Razakar\", and the widespread sharing of\nvisuals representing courage, injustice, and resistance. The findings show that\nthe combination of visual and verbal strategies on Facebook not only mobilized\npublic sentiment, but also built a strong collective identity that challenged\nauthoritarian narratives. This study tries to demonstrate how online platforms\ncan serve as powerful tools for identity construction and political\nmobilization in the digital age.", "AI": {"tldr": "\u7814\u7a76\u89e3\u6790Facebook\u5982\u4f55\u901a\u8fc7\u591a\u6a21\u6001\u5185\u5bb9(\u89c6\u89c9\u7b26\u53f7\u3001\u8bbd\u523a\u8bed\u8a00\u3001\u6570\u5b57\u827a\u672f)\u5728\u5b5f\u52a0\u62c9\u56fd2024\u5e74\u5b63\u98ce\u8d77\u4e49\u4e2d\u6784\u5efa\u6297\u8bae\u8005\u96c6\u4f53\u8eab\u4efd\uff0c\u6311\u6218\u5a01\u6743\u53d9\u4e8b\u3002", "motivation": "\u63a2\u7a76\u6570\u5b57\u5e73\u53f0\u5728\u9ad8\u538b\u653f\u6cbb\u73af\u5883\u4e0b\u4f5c\u4e3a\u62b5\u6297\u7a7a\u95f4\u7684\u6f5c\u80fd\uff0c\u7279\u522b\u662f\u5728\u653f\u5e9c\u9547\u538b\u80cc\u666f\u4e0b\u793e\u4ea4\u5a92\u4f53\u5982\u4f55\u6210\u4e3a\u8eab\u4efd\u5efa\u6784\u7684\u6838\u5fc3\u573a\u57df\u3002", "method": "\u91c7\u7528\u8d28\u6027\u7814\u7a76\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c6\u89c9\u4fee\u8f9e\u5206\u6790\u3001\u6570\u5b57\u8bdd\u8bed\u89e3\u6784\u548c\u7f51\u7edc\u6c11\u65cf\u5fd7\uff0c\u91cd\u70b9\u89e3\u6790\u6297\u8bae\u8005\u4f7f\u7528\u7684\u7ea2\u8272\u7b26\u53f7\u3001'Razakar'\u9690\u55bb\u91cd\u6784\u3001\u52c7\u6c14\u4e3b\u9898\u89c6\u89c9\u6a21\u56e0\u7b49\u4f20\u64ad\u7b56\u7565\u3002", "result": "\u53d1\u73b0\u591a\u6a21\u6001\u8868\u8fbe\u6784\u5efa\u4e86\u4e09\u7ef4\u8ba4\u540c\u6846\u67b6\uff1a1\uff09\u7ea2\u8272\u4f5c\u4e3a\u9769\u547d\u8272\u8c31 2\uff09\u5386\u53f2\u7b26\u53f7\u7684\u6570\u5b57\u5316\u8f6c\u8bd1 3\uff09\u8bbd\u523a\u6027\u5185\u5bb9\u521b\u9020\u7684\u62b5\u6297\u8bdd\u8bed\u4f53\u7cfb\uff0c\u6709\u6548\u7a81\u7834\u4fe1\u606f\u5c01\u9501\u5e76\u5f62\u6210\u8de8\u5730\u57df\u52a8\u5458\u3002", "conclusion": "\u6570\u5b57\u5e73\u53f0\u901a\u8fc7\u6df7\u5408\u7b26\u53f7\u7cfb\u7edf\u91cd\u6784\u653f\u6cbb\u6c9f\u901a\u8303\u5f0f\uff0c\u4f7f\u5206\u5e03\u5f0f\u6297\u8bae\u7f51\u7edc\u83b7\u5f97\u96c6\u4e2d\u5f0f\u53d9\u4e8b\u529b\u91cf\uff0c\u8fd9\u4e3a\u7406\u89e3\u6570\u5b57\u65f6\u4ee3\u793e\u4f1a\u8fd0\u52a8\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u7406\u8bba\u89c6\u89d2\u3002"}}
{"id": "2508.02502", "pdf": "https://arxiv.org/pdf/2508.02502", "abs": "https://arxiv.org/abs/2508.02502", "authors": ["Shuzhou Yuan", "Zhan Qu", "Mario Tawfelis", "Michael F\u00e4rber"], "title": "From Monolingual to Bilingual: Investigating Language Conditioning in Large Language Models for Psycholinguistic Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) exhibit strong linguistic capabilities, but\nlittle is known about how they encode psycholinguistic knowledge across\nlanguages. We investigate whether and how LLMs exhibit human-like\npsycholinguistic responses under different linguistic identities using two\ntasks: sound symbolism and word valence. We evaluate two models,\nLlama-3.3-70B-Instruct and Qwen2.5-72B-Instruct, under monolingual and\nbilingual prompting in English, Dutch, and Chinese. Behaviorally, both models\nadjust their outputs based on prompted language identity, with Qwen showing\ngreater sensitivity and sharper distinctions between Dutch and Chinese. Probing\nanalysis reveals that psycholinguistic signals become more decodable in deeper\nlayers, with Chinese prompts yielding stronger and more stable valence\nrepresentations than Dutch. Our results demonstrate that language identity\nconditions both output behavior and internal representations in LLMs, providing\nnew insights into their application as models of cross-linguistic cognition.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u58f0\u97f3\u8c61\u5f81\u548c\u8bcd\u4ef7\u4efb\u52a1\uff0c\u63ed\u793aLLaMA\u548cQwen\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u8a00\u8eab\u4efd\u4e0b\u4f1a\u8c03\u6574\u8f93\u51fa\u884c\u4e3a\uff0c\u4e2d\u6587\u63d0\u793a\u4ea7\u751f\u66f4\u7a33\u5b9a\u7684\u5fc3\u7406\u8868\u5f81", "motivation": "\u586b\u8865\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u8bed\u8a00\u5fc3\u7406\u8bed\u8a00\u5b66\u77e5\u8bc6\u7f16\u7801\u673a\u5236\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u63a2\u7d22\u8bed\u8a00\u8eab\u4efd\u5bf9\u6a21\u578b\u884c\u4e3a\u548c\u5185\u90e8\u8868\u5f81\u7684\u5f71\u54cd", "method": "\u8bc4\u4f30LLaMA-3.3-70B\u548cQwen2.5-72B\u6a21\u578b\u5728\u82f1\u8bed/\u8377\u5170\u8bed/\u4e2d\u6587\u7684\u5355\u8bed/\u53cc\u8bed\u63d0\u793a\u4e0b\u7684\u54cd\u5e94\uff0c\u7ed3\u5408\u884c\u4e3a\u5206\u6790\u548c\u5206\u5c42\u63a2\u6d4b\u6280\u672f", "result": "Qwen\u5c55\u793a\u51fa\u66f4\u5f3a\u7684\u8bed\u8a00\u654f\u611f\u6027\uff0c\u4e2d\u6587\u63d0\u793a\u7684\u5fc3\u7406\u8bed\u8a00\u5b66\u4fe1\u53f7\u5728\u6df1\u5c42\u7f51\u7edc\u66f4\u7a33\u5b9a\u53ef\u89e3\u7801\uff0c\u8377\u5170\u8bed\u4e0e\u4e2d\u6587\u8868\u5f81\u5dee\u5f02\u663e\u8457", "conclusion": "\u8bed\u8a00\u8eab\u4efd\u65e2\u5f71\u54cdLLM\u8f93\u51fa\u884c\u4e3a\u4e5f\u5851\u9020\u5185\u90e8\u8868\u5f81\u7ed3\u6784\uff0c\u4e3a\u6784\u5efa\u8de8\u8bed\u8a00\u8ba4\u77e5\u6a21\u578b\u63d0\u4f9b\u65b0\u89c6\u89d2"}}
{"id": "2508.02513", "pdf": "https://arxiv.org/pdf/2508.02513", "abs": "https://arxiv.org/abs/2508.02513", "authors": ["Tanja Baeumel", "Daniil Gurgurov", "Yusser al Ghussin", "Josef van Genabith", "Simon Ostermann"], "title": "Modular Arithmetic: Language Models Solve Math Digit by Digit", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While recent work has begun to uncover the internal strategies that Large\nLanguage Models (LLMs) employ for simple arithmetic tasks, a unified\nunderstanding of their underlying mechanisms is still lacking. We extend recent\nfindings showing that LLMs represent numbers in a digit-wise manner and present\nevidence for the existence of digit-position-specific circuits that LLMs use to\nperform simple arithmetic tasks, i.e. modular subgroups of MLP neurons that\noperate independently on different digit positions (units, tens, hundreds).\nNotably, such circuits exist independently of model size and of tokenization\nstrategy, i.e. both for models that encode longer numbers digit-by-digit and as\none token. Using Feature Importance and Causal Interventions, we identify and\nvalidate the digit-position-specific circuits, revealing a compositional and\ninterpretable structure underlying the solving of arithmetic problems in LLMs.\nOur interventions selectively alter the model's prediction at targeted digit\npositions, demonstrating the causal role of digit-position circuits in solving\narithmetic tasks.", "AI": {"tldr": "\u53d1\u73b0LLMs\u901a\u8fc7\u6570\u5b57\u4f4d\u7f6e\u4e13\u7528\u7535\u8def\u5904\u7406\u7b97\u672f\u4efb\u52a1\uff0c\u63ed\u793a\u4e86\u5176\u53ef\u89e3\u91ca\u7684\u7b97\u672f\u673a\u5236", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9LLMs\u7b97\u672f\u5e95\u5c42\u673a\u5236\u7f3a\u4e4f\u7edf\u4e00\u7406\u89e3\uff0c\u9700\u9a8c\u8bc1\u6570\u5b57\u4f4d\u7f6e\u4e13\u7528\u7535\u8def\u7684\u666e\u904d\u5b58\u5728\u6027", "method": "\u7ed3\u5408\u7279\u5f81\u91cd\u8981\u6027\u5206\u6790\u548c\u56e0\u679c\u5e72\u9884\u6280\u672f\uff0c\u7814\u7a76\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u548c\u6807\u8bb0\u5316\u7b56\u7565\u4e0b\u7684\u6570\u5b57\u5904\u7406\u673a\u5236", "result": "\u8bc1\u5b9e\u6570\u5b57\u4f4d\u7f6e\u7535\u8def\u72ec\u7acb\u5b58\u5728\u4e14\u5177\u6709\u56e0\u679c\u4f5c\u7528\uff0c\u5e72\u9884\u53ef\u9009\u62e9\u6027\u6539\u53d8\u7279\u5b9a\u6570\u4f4d\u9884\u6d4b", "conclusion": "\u63ed\u793a\u4e86LLMs\u89e3\u51b3\u7b97\u672f\u95ee\u9898\u7684\u7ec4\u5408\u5f0f\u53ef\u89e3\u91ca\u7ed3\u6784\uff0c\u4e3a\u6a21\u578b\u673a\u5236\u5206\u6790\u63d0\u4f9b\u65b0\u89c6\u89d2"}}
{"id": "2508.02515", "pdf": "https://arxiv.org/pdf/2508.02515", "abs": "https://arxiv.org/abs/2508.02515", "authors": ["Zhan Qu", "Shuzhou Yuan", "Michael F\u00e4rber"], "title": "PoeTone: A Framework for Constrained Generation of Structured Chinese Songci with LLMs", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This paper presents a systematic investigation into the constrained\ngeneration capabilities of large language models (LLMs) in producing Songci, a\nclassical Chinese poetry form characterized by strict structural, tonal, and\nrhyme constraints defined by Cipai templates. We first develop a comprehensive,\nmulti-faceted evaluation framework that includes: (i) a formal conformity\nscore, (ii) automated quality assessment using LLMs, (iii) human evaluation,\nand (iv) classification-based probing tasks. Using this framework, we evaluate\nthe generative performance of 18 LLMs, including 3 proprietary models and 15\nopen-source models across four families, under five prompting strategies:\nzero-shot, one-shot, completion-based, instruction-tuned, and chain-of-thought.\nFinally, we propose a Generate-Critic architecture in which the evaluation\nframework functions as an automated critic. Leveraging the critic's feedback as\na reward signal, we fine-tune three lightweight open-source LLMs via supervised\nfine-tuning (SFT), resulting in improvements of up to 5.88% in formal\nconformity. Our findings offer new insights into the generative strengths and\nlimitations of LLMs in producing culturally significant and formally\nconstrained literary texts.", "AI": {"tldr": "\u7cfb\u7edf\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5b8b\u8bcd\u7684\u80fd\u529b\uff0c\u63d0\u51fa\u591a\u7ef4\u5ea6\u8bc4\u4f30\u6846\u67b6\u548cGenerate-Critic\u67b6\u6784\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u4f7f\u5f62\u5f0f\u7b26\u5408\u5ea6\u63d0\u53475.88%", "motivation": "\u63a2\u7a76LLM\u5728\u751f\u6210\u5177\u6709\u6587\u5316\u610f\u4e49\u4e14\u5f62\u5f0f\u4e25\u683c\u53d7\u9650\u7684\u53e4\u5178\u8bd7\u8bcd\uff08\u5b8b\u8bcd\uff09\u65f6\u7684\u751f\u6210\u80fd\u529b\u4e0e\u5c40\u9650", "method": "1. \u6784\u5efa\u5305\u542b\u5f62\u5f0f\u5408\u89c4\u6027/\u81ea\u52a8\u8bc4\u4f30/\u4eba\u5de5\u8bc4\u4f30/\u5206\u7c7b\u63a2\u6d4b\u7684\u56db\u7ef4\u8bc4\u4f30\u6846\u67b6\n2. \u5bf918\u4e2aLLM\u8fdb\u884c\u4e94\u79cd\u63d0\u793a\u7b56\u7565\u6d4b\u8bd5\n3. \u63d0\u51fa\u8bc4\u4f30\u6846\u67b6\u9a71\u52a8\u7684\u81ea\u52a8\u6279\u5224\u67b6\u6784\uff0c\u901a\u8fc7SFT\u5fae\u8c03\u8f7b\u91cf\u7ea7\u6a21\u578b", "result": "\u5fae\u8c03\u540e\u6a21\u578b\u5f62\u5f0f\u5408\u89c4\u6027\u6700\u9ad8\u63d0\u53475.88%\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u5728\u6587\u5b66\u6587\u672c\u751f\u6210\u4e2d\u7684\u8868\u73b0\u5dee\u5f02", "conclusion": "\u8be5\u7814\u7a76\u4e3aLLM\u5728\u6587\u5316\u654f\u611f\u6587\u672c\u751f\u6210\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u6846\u67b6\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u6a21\u578b\u5728\u5f62\u5f0f\u7ea6\u675f\u4e0e\u6587\u5b66\u6027\u5e73\u8861\u4e2d\u7684\u6311\u6218"}}
{"id": "2508.02527", "pdf": "https://arxiv.org/pdf/2508.02527", "abs": "https://arxiv.org/abs/2508.02527", "authors": ["Jack Merullo", "Arjun Khurana", "Oliver McLaughlin"], "title": "I Have No Mouth, and I Must Rhyme: Uncovering Internal Phonetic Representations in LLaMA 3.2", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models demonstrate proficiency on phonetic tasks, such as\nrhyming, without explicit phonetic or auditory grounding. In this work, we\ninvestigate how \\verb|Llama-3.2-1B-Instruct| represents token-level phonetic\ninformation. Our results suggest that Llama uses a rich internal model of\nphonemes to complete phonetic tasks. We provide evidence for high-level\norganization of phoneme representations in its latent space. In doing so, we\nalso identify a ``phoneme mover head\" which promotes phonetic information\nduring rhyming tasks. We visualize the output space of this head and find that,\nwhile notable differences exist, Llama learns a model of vowels similar to the\nstandard IPA vowel chart for humans, despite receiving no direct supervision to\ndo so.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0Llama-3.2-1B-Instruct\u6a21\u578b\u901a\u8fc7\u5185\u90e8\u97f3\u7d20\u6a21\u578b\u5904\u7406\u8bed\u97f3\u4efb\u52a1\uff0c\u5176\u6f5c\u5728\u7a7a\u95f4\u5448\u73b0\u7c7bIPA\u5143\u97f3\u56fe\u7ed3\u6784\uff0c\u5e76\u5b58\u5728\u4e13\u95e8\u4fc3\u8fdb\u8bed\u97f3\u5904\u7406\u7684'\u97f3\u7d20\u79fb\u52a8\u5934'\u3002", "motivation": "\u63a2\u7d22\u65e0\u8bed\u97f3/\u542c\u89c9\u57fa\u7840\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08Llama\uff09\u5982\u4f55\u8868\u5f81\u8bcd\u6c47\u5c42\u9762\u8bed\u97f3\u4fe1\u606f\uff0c\u7279\u522b\u662f\u62bc\u97f5\u4efb\u52a1\u4e2d\u7684\u97f3\u7d20\u5904\u7406\u673a\u5236\u3002", "method": "\u901a\u8fc7\u5206\u6790Llama\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\uff0c\u53ef\u89c6\u5316\u6f5c\u5728\u7a7a\u95f4\u7ed3\u6784\uff0c\u5e76\u8bc6\u522b\u5173\u952e\u8bed\u97f3\u5904\u7406\u7ec4\u4ef6\uff08\u5982'\u97f3\u7d20\u79fb\u52a8\u5934'\uff09\u3002", "result": "\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u5448\u73b0\u97f3\u7d20\u9ad8\u7ea7\u7ec4\u7ec7\u7279\u5f81\uff0c\u7279\u5b9a\u5934\u90e8\u8f93\u51fa\u7a7a\u95f4\u4e0e\u4eba\u7c7bIPA\u5143\u97f3\u56fe\u5b58\u5728\u7ed3\u6784\u76f8\u4f3c\u6027\uff08\u5c3d\u7ba1\u5b58\u5728\u5dee\u5f02\uff09\u3002", "conclusion": "Llama\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u81ea\u53d1\u6784\u5efa\u4e86\u63a5\u8fd1\u4eba\u7c7b\u8bed\u97f3\u5b66\u7684\u97f3\u7d20\u8868\u5f81\u4f53\u7cfb\uff0c\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5185\u5728\u7684\u8bed\u97f3\u5904\u7406\u80fd\u529b\u3002"}}
{"id": "2508.02532", "pdf": "https://arxiv.org/pdf/2508.02532", "abs": "https://arxiv.org/abs/2508.02532", "authors": ["Karan Reddy", "Mayukha Pal"], "title": "Contextual Graph Transformer: A Small Language Model for Enhanced Engineering Document Information Extraction", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Standard transformer-based language models, while powerful for general text,\noften struggle with the fine-grained syntax and entity relationships in complex\ntechnical, engineering documents. To address this, we propose the Contextual\nGraph Transformer (CGT), a hybrid neural architecture that combines Graph\nNeural Networks (GNNs) and Transformers for domain-specific question answering.\nCGT constructs a dynamic graph over input tokens using sequential, skip-gram,\nand semantic similarity edges, which is processed by GATv2Conv layers for local\nstructure learning. These enriched embeddings are then passed to a Transformer\nencoder to capture global dependencies. Unlike generic large models, technical\ndomains often require specialized language models with stronger\ncontextualization and structure awareness. CGT offers a parameter-efficient\nsolution for such use cases. Integrated into a Retrieval-Augmented Generation\n(RAG) pipeline, CGT outperforms baselines like GPT-2 and BERT, achieving 24.7%\nhigher accuracy than GPT-2 with 62.4% fewer parameters. This gain stems from\nCGTs ability to jointly model structural token interactions and long-range\nsemantic coherence. The model is trained from scratch using a two-phase\napproach: pretraining on general text followed by fine-tuning on\ndomain-specific manuals. This highlights CGTs adaptability to technical\nlanguage, enabling better grounding, entity tracking, and retrieval-augmented\nresponses in real-world applications.", "AI": {"tldr": "Proposes Contextual Graph Transformer (CGT) combining GNNs and Transformers for technical QA, achieving 24.7% higher accuracy than GPT-2 with 62.4% fewer parameters through structural token modeling.", "motivation": "Generic transformers struggle with technical documents' fine-grained syntax. Technical domains need specialized models with better structural awareness and contextualization capabilities.", "method": "Hybrid architecture builds dynamic graphs (sequential/skip-gram/semantic edges) processed by GATv2Conv layers, then feeds enriched embeddings to Transformer. Uses two-phase training: general pretraining + domain-specific fine-tuning.", "result": "Outperforms GPT-2/BERT baselines in RAG pipeline. Achieves 24.7% accuracy gain over GPT-2 while using 62.4% fewer parameters through joint structural-semantic modeling.", "conclusion": "CGT effectively balances local structure learning and global dependencies, demonstrating strong adaptability for technical language processing and retrieval-augmented applications."}}
{"id": "2508.02540", "pdf": "https://arxiv.org/pdf/2508.02540", "abs": "https://arxiv.org/abs/2508.02540", "authors": ["Anastasia Zhukova", "Terry Ruas", "Felix Hamborg", "Karsten Donnay", "Bela Gipp"], "title": "What's in the News? Towards Identification of Bias by Commission, Omission, and Source Selection (COSS)", "categories": ["cs.CL"], "comment": "published in the Proceedings of the 2023 ACM/IEEE Joint Conference on\n  Digital Libraries", "summary": "In a world overwhelmed with news, determining which information comes from\nreliable sources or how neutral is the reported information in the news\narticles poses a challenge to news readers. In this paper, we propose a\nmethodology for automatically identifying bias by commission, omission, and\nsource selection (COSS) as a joint three-fold objective, as opposed to the\nprevious work separately addressing these types of bias. In a pipeline concept,\nwe describe the goals and tasks of its steps toward bias identification and\nprovide an example of a visualization that leverages the extracted features and\npatterns of text reuse.", "AI": {"tldr": "\u63d0\u51fa\u8054\u5408\u8bc6\u522b\u65b0\u95fb\u504f\u89c1\u7684\u4e09\u91cd\u76ee\u6807\u6846\u67b6\uff08COSS\uff09", "motivation": "\u5f53\u524d\u65b0\u95fb\u4fe1\u606f\u8fc7\u8f7d\uff0c\u8bfb\u8005\u96be\u4ee5\u5224\u65ad\u6765\u6e90\u53ef\u9760\u6027\u548c\u5185\u5bb9\u4e2d\u7acb\u6027", "method": "\u901a\u8fc7\u6587\u672c\u91cd\u7528\u7279\u5f81\u6784\u5efapipeline\u6d41\u7a0b\uff0c\u6574\u5408commission/omission/source selection\u4e09\u79cd\u504f\u89c1\u8bc6\u522b\uff0c\u5f00\u53d1\u53ef\u89c6\u5316\u5206\u6790\u5de5\u5177", "result": "\u5efa\u7acb\u9996\u4e2a\u8054\u5408\u5904\u7406COSS\u504f\u89c1\u7684\u7cfb\u7edf\u6027\u65b9\u6cd5\u6846\u67b6", "conclusion": "\u8be5\u6846\u67b6\u7a81\u7834\u5355\u4e00\u504f\u89c1\u68c0\u6d4b\u5c40\u9650\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u6280\u672f\u5e2e\u52a9\u8bfb\u8005\u8bc6\u522b\u590d\u6742\u6587\u672c\u590d\u7528\u6a21\u5f0f\u4e2d\u7684\u6f5c\u5728\u504f\u89c1"}}
{"id": "2508.02555", "pdf": "https://arxiv.org/pdf/2508.02555", "abs": "https://arxiv.org/abs/2508.02555", "authors": ["Motaz Saad", "David Langlois", "Kamel Smaili"], "title": "Building and Aligning Comparable Corpora", "categories": ["cs.CL", "I.2.7"], "comment": "27 pages, 11 figures", "summary": "Comparable corpus is a set of topic aligned documents in multiple languages,\nwhich are not necessarily translations of each other. These documents are\nuseful for multilingual natural language processing when there is no parallel\ntext available in some domains or languages. In addition, comparable documents\nare informative because they can tell what is being said about a topic in\ndifferent languages. In this paper, we present a method to build comparable\ncorpora from Wikipedia encyclopedia and EURONEWS website in English, French and\nArabic languages. We further experiment a method to automatically align\ncomparable documents using cross-lingual similarity measures. We investigate\ntwo cross-lingual similarity measures to align comparable documents. The first\nmeasure is based on bilingual dictionary, and the second measure is based on\nLatent Semantic Indexing (LSI). Experiments on several corpora show that the\nCross-Lingual LSI (CL-LSI) measure outperforms the dictionary based measure.\nFinally, we collect English and Arabic news documents from the British\nBroadcast Corporation (BBC) and from ALJAZEERA (JSC) news website respectively.\nThen we use the CL-LSI similarity measure to automatically align comparable\ndocuments of BBC and JSC. The evaluation of the alignment shows that CL-LSI is\nnot only able to align cross-lingual documents at the topic level, but also it\nis able to do this at the event level.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7ef4\u57fa\u767e\u79d1\u548cEURONEWS\u6784\u5efa\u591a\u8bed\u8a00\u53ef\u6bd4\u8bed\u6599\u5e93\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660eCL-LSI\u8de8\u8bed\u8a00\u76f8\u4f3c\u5ea6\u6d4b\u91cf\u5728\u6587\u6863\u5bf9\u9f50\u4e2d\u4f18\u4e8e\u8bcd\u5178\u65b9\u6cd5", "motivation": "\u5728\u7f3a\u4e4f\u5e73\u884c\u6587\u672c\u7684\u9886\u57df/\u8bed\u8a00\u4e2d\uff0c\u4e3b\u9898\u5bf9\u9f50\u7684\u53ef\u6bd4\u8bed\u6599\u5e93\u53ef\u4f5c\u4e3a\u66ff\u4ee3\u8d44\u6e90\u652f\u6301\u591a\u8bed\u8a00NLP\u4efb\u52a1\uff0c\u5e76\u53cd\u6620\u4e0d\u540c\u8bed\u8a00\u5bf9\u540c\u4e00\u4e3b\u9898\u7684\u8868\u8ff0\u5dee\u5f02", "method": "1. \u4ece\u7ef4\u57fa\u767e\u79d1\u548cEURONEWS\u91c7\u96c6\u82f1\u6cd5\u963f\u4e09\u8bed\u6570\u636e\u6784\u5efa\u53ef\u6bd4\u8bed\u6599\u5e93\n2. \u63d0\u51fa\u57fa\u4e8e\u53cc\u8bed\u8bcd\u5178\u548cCL-LSI\u4e24\u79cd\u8de8\u8bed\u8a00\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u65b9\u6cd5\n3. \u4f7f\u7528BBC\u548c\u534a\u5c9b\u7535\u89c6\u53f0\u65b0\u95fb\u9a8c\u8bc1CL-LSI\u6548\u679c", "result": "CL-LSI\u5728\u591a\u4e2a\u8bed\u6599\u5e93\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u8bcd\u5178\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728BBC\u4e0e\u534a\u5c9b\u65b0\u95fb\u5bf9\u9f50\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e3b\u9898\u548c\u4e8b\u4ef6\u7ea7\u522b\u7684\u7cbe\u51c6\u5339\u914d", "conclusion": "CL-LSI\u4e0d\u4ec5\u9002\u7528\u4e8e\u8de8\u8bed\u8a00\u6587\u6863\u7684\u4e3b\u9898\u7ea7\u5bf9\u9f50\uff0c\u8fd8\u80fd\u6355\u6349\u7ec6\u7c92\u5ea6\u7684\u4e8b\u4ef6\u7ea7\u5173\u8054\uff0c\u4e3a\u53ef\u6bd4\u8bed\u6599\u6784\u5efa\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.02556", "pdf": "https://arxiv.org/pdf/2508.02556", "abs": "https://arxiv.org/abs/2508.02556", "authors": ["Ali Noori", "Pratik Devkota", "Somya Mohanty", "Prashanti Manda"], "title": "Automated SNOMED CT Concept Annotation in Clinical Text Using Bi-GRU Neural Networks", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Automated annotation of clinical text with standardized medical concepts is\ncritical for enabling structured data extraction and decision support. SNOMED\nCT provides a rich ontology for labeling clinical entities, but manual\nannotation is labor-intensive and impractical at scale. This study introduces a\nneural sequence labeling approach for SNOMED CT concept recognition using a\nBidirectional GRU model. Leveraging a subset of MIMIC-IV, we preprocess text\nwith domain-adapted SpaCy and SciBERT-based tokenization, segmenting sentences\ninto overlapping 19-token chunks enriched with contextual, syntactic, and\nmorphological features. The Bi-GRU model assigns IOB tags to identify concept\nspans and achieves strong performance with a 90 percent F1-score on the\nvalidation set. These results surpass traditional rule-based systems and match\nor exceed existing neural models. Qualitative analysis shows effective handling\nof ambiguous terms and misspellings. Our findings highlight that lightweight\nRNN-based architectures can deliver high-quality clinical concept annotation\nwith significantly lower computational cost than transformer-based models,\nmaking them well-suited for real-world deployment.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u53cc\u5411GRU\u7684\u5e8f\u5217\u6807\u6ce8\u6a21\u578b\uff0c\u5728\u4e34\u5e8a\u6587\u672cSNOMED CT\u6982\u5ff5\u6807\u6ce8\u4efb\u52a1\u4e2d\u5b9e\u73b090% F1\u503c\uff0c\u8ba1\u7b97\u6210\u672c\u663e\u8457\u4f4e\u4e8eTransformer\u6a21\u578b", "motivation": "\u89e3\u51b3\u624b\u52a8\u6807\u6ce8SNOMED CT\u533b\u7597\u6982\u5ff5\u7684\u6548\u7387\u74f6\u9888\uff0c\u63a8\u52a8\u7ed3\u6784\u5316\u75c5\u5386\u6570\u636e\u5206\u6790\u7684\u81ea\u52a8\u5316\u8fdb\u7a0b", "method": "\u4f7f\u7528MIMIC-IV\u6570\u636e\uff0c\u7ed3\u5408SpaCy/SciBERT\u9884\u5904\u7406\uff0c\u6784\u5efa19-token\u91cd\u53e0\u6587\u672c\u5757\uff0c\u91c7\u7528Bi-GRU\u6a21\u578b\u8fdb\u884c\u5e8f\u5217\u6807\u6ce8", "result": "\u9a8c\u8bc1\u96c6F1\u8fbe90%\uff0c\u8d85\u8d8a\u89c4\u5219\u7cfb\u7edf\u4e14\u6301\u5e73\u73b0\u6709\u795e\u7ecf\u6a21\u578b\uff0c\u6709\u6548\u5904\u7406\u672f\u8bed\u6b67\u4e49\u548c\u62fc\u5199\u9519\u8bef", "conclusion": "\u8bc1\u660e\u8f7b\u91cf\u7ea7RNN\u67b6\u6784\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5177\u5907\u90e8\u7f72\u4f18\u52bf\uff0c\u4e3a\u4e34\u5e8aNLP\u843d\u5730\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.02558", "pdf": "https://arxiv.org/pdf/2508.02558", "abs": "https://arxiv.org/abs/2508.02558", "authors": ["Yuerong Song", "Xiaoran Liu", "Ruixiao Li", "Zhigeng Liu", "Zengfeng Huang", "Qipeng Guo", "Ziwei He", "Xipeng Qiu"], "title": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction", "categories": ["cs.CL"], "comment": "11 pages, 6 figures", "summary": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness.", "AI": {"tldr": "Sparse-dLLM\u901a\u8fc7\u52a8\u6001\u7f13\u5b58\u9a71\u9010\u548c\u7a00\u758f\u6ce8\u610f\u529b\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0810\u500d\u541e\u5410\u91cf\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b(dLLMs)\u5b58\u5728\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u9ad8\u5185\u5b58\u5f00\u9500\u95ee\u9898\uff0c\u73b0\u6709\u7f13\u5b58\u6280\u672f\u65e0\u6cd5\u6709\u6548\u652f\u6301\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u3002\u7814\u7a76\u53d1\u73b0\u6ce8\u610f\u529b\u673a\u5236\u5b58\u5728\u8de8\u5c42\u7a00\u758f\u6027\u6a21\u5f0f\uff0c\u5173\u952e\u4ee4\u724c\u4fdd\u6301\u663e\u8457\uff0c\u6b21\u8981\u4ee4\u724c\u6301\u7eed\u4f4e\u76f8\u5173\u3002", "method": "\u63d0\u51fa\u8bad\u7ec3\u65e0\u5173\u6846\u67b6Sparse-dLLM\uff1a1) \u57fa\u4e8e\u6ce8\u610f\u529b\u7a33\u5b9a\u6027\u8bbe\u8ba1\u52a8\u6001\u7f13\u5b58\u9a71\u9010\u7b56\u7565 2) \u91c7\u7528\u5ef6\u8fdf\u53cc\u5411\u7a00\u758f\u7f13\u5b58\u6280\u672f 3) \u901a\u8fc7\u6ce8\u610f\u529b\u5f15\u5bfc\u673a\u5236\u9009\u62e9\u4fdd\u7559\u5173\u952e\u4ee4\u724c\uff0c\u52a8\u6001\u6dd8\u6c70\u975e\u91cd\u8981\u524d\u7f00/\u540e\u7f00\u7f13\u5b58", "result": "\u5728LLaDA/Dream\u7cfb\u5217\u5b9e\u73b010\u500d\u541e\u5410\u91cf\u63d0\u5347\uff0c\u5cf0\u503c\u5185\u5b58\u4e0e\u539f\u59cb\u65b9\u6cd5\u76f8\u5f53\uff0c\u6548\u7387\u4e0e\u6548\u679c\u5747\u8d85\u8d8a\u73b0\u6709\u4f18\u5316\u65b9\u6848", "conclusion": "\u9996\u6b21\u5c06\u7a00\u758f\u6ce8\u610f\u529b\u4e0e\u52a8\u6001\u7f13\u5b58\u7ed3\u5408\uff0c\u4e3a\u957f\u4e0a\u4e0b\u6587dLLM\u63a8\u7406\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5e73\u8861\u6548\u7387\u4e0e\u6027\u80fd"}}
{"id": "2508.02573", "pdf": "https://arxiv.org/pdf/2508.02573", "abs": "https://arxiv.org/abs/2508.02573", "authors": ["J\u00e9r\u00e9mie Dentan", "Davide Buscaldi", "Sonia Vanier"], "title": "Guess or Recall? Training CNNs to Classify and Localize Memorization in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Verbatim memorization in Large Language Models (LLMs) is a multifaceted\nphenomenon involving distinct underlying mechanisms. We introduce a novel\nmethod to analyze the different forms of memorization described by the existing\ntaxonomy. Specifically, we train Convolutional Neural Networks (CNNs) on the\nattention weights of the LLM and evaluate the alignment between this taxonomy\nand the attention weights involved in decoding.\n  We find that the existing taxonomy performs poorly and fails to reflect\ndistinct mechanisms within the attention blocks. We propose a new taxonomy that\nmaximizes alignment with the attention weights, consisting of three categories:\nmemorized samples that are guessed using language modeling abilities, memorized\nsamples that are recalled due to high duplication in the training set, and\nnon-memorized samples. Our results reveal that few-shot verbatim memorization\ndoes not correspond to a distinct attention mechanism. We also show that a\nsignificant proportion of extractable samples are in fact guessed by the model\nand should therefore be studied separately. Finally, we develop a custom visual\ninterpretability technique to localize the regions of the attention weights\ninvolved in each form of memorization.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u5206\u7c7b\u6cd5\u63ed\u793aLLM\u9010\u5b57\u8bb0\u5fc6\u673a\u5236\uff1a\u73b0\u6709\u5206\u7c7b\u6cd5\u4e0e\u6ce8\u610f\u529b\u6743\u91cd\u4e0d\u5339\u914d\uff0c\u8bb0\u5fc6\u6837\u672c\u5206\u2018\u8bed\u8a00\u5efa\u6a21\u731c\u6d4b\u2019\u3001\u2018\u9ad8\u9891\u91cd\u590d\u56de\u5fc6\u2019\u548c\u2018\u975e\u8bb0\u5fc6\u2019\u4e09\u7c7b\u3002", "motivation": "\u73b0\u6709\u8bb0\u5fc6\u5206\u7c7b\u6cd5\u672a\u80fd\u6709\u6548\u533a\u5206\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u7684\u8bb0\u5fc6\u6a21\u5f0f\uff0c\u9700\u5efa\u7acb\u4e0e\u6ce8\u610f\u529b\u6743\u91cd\u66f4\u5339\u914d\u7684\u65b0\u5206\u7c7b\u6846\u67b6\u3002", "method": "\u5728LLM\u6ce8\u610f\u529b\u6743\u91cd\u4e0a\u8bad\u7ec3CNN\uff0c\u901a\u8fc7\u89c6\u89c9\u53ef\u89e3\u91ca\u6027\u6280\u672f\u5b9a\u4f4d\u4e0d\u540c\u8bb0\u5fc6\u5f62\u5f0f\u7684\u6ce8\u610f\u529b\u533a\u57df\u3002", "result": "1. \u5c11\u6837\u672c\u9010\u5b57\u8bb0\u5fc6\u65e0\u72ec\u7acb\u6ce8\u610f\u529b\u673a\u5236\n2. \u5927\u91cf\u53ef\u63d0\u53d6\u6837\u672c\u5b9e\u4e3a\u6a21\u578b\u731c\u6d4b\n3. \u65b0\u5206\u7c7b\u6cd5\u663e\u8457\u63d0\u5347\u6ce8\u610f\u529b\u6743\u91cd\u89e3\u91ca\u529b", "conclusion": "\u8bb0\u5fc6\u673a\u5236\u9700\u7ec6\u5206\u7814\u7a76\uff0c\u731c\u6d4b\u578b\u6837\u672c\u5e94\u4e0e\u56de\u5fc6\u578b\u533a\u5206\uff0c\u6ce8\u610f\u529b\u6743\u91cd\u53ef\u89c6\u5316\u6280\u672f\u4e3a\u8bb0\u5fc6\u673a\u5236\u5206\u6790\u63d0\u4f9b\u65b0\u5de5\u5177\u3002"}}
{"id": "2508.02574", "pdf": "https://arxiv.org/pdf/2508.02574", "abs": "https://arxiv.org/abs/2508.02574", "authors": ["Eman Alamoudi", "Ellis Solaiman"], "title": "EHSAN: Leveraging ChatGPT in a Hybrid Framework for Arabic Aspect-Based Sentiment Analysis in Healthcare", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "comment": null, "summary": "Arabic-language patient feedback remains under-analysed because dialect\ndiversity and scarce aspect-level sentiment labels hinder automated assessment.\nTo address this gap, we introduce EHSAN, a data-centric hybrid pipeline that\nmerges ChatGPT pseudo-labelling with targeted human review to build the first\nexplainable Arabic aspect-based sentiment dataset for healthcare. Each sentence\nis annotated with an aspect and sentiment label (positive, negative, or\nneutral), forming a pioneering Arabic dataset aligned with healthcare themes,\nwith ChatGPT-generated rationales provided for each label to enhance\ntransparency. To evaluate the impact of annotation quality on model\nperformance, we created three versions of the training data: a fully supervised\nset with all labels reviewed by humans, a semi-supervised set with 50% human\nreview, and an unsupervised set with only machine-generated labels. We\nfine-tuned two transformer models on these datasets for both aspect and\nsentiment classification. Experimental results show that our Arabic-specific\nmodel achieved high accuracy even with minimal human supervision, reflecting\nonly a minor performance drop when using ChatGPT-only labels. Reducing the\nnumber of aspect classes notably improved classification metrics across the\nboard. These findings demonstrate an effective, scalable approach to Arabic\naspect-based sentiment analysis (SA) in healthcare, combining large language\nmodel annotation with human expertise to produce a robust and explainable\ndataset. Future directions include generalisation across hospitals, prompt\nrefinement, and interpretable data-driven modelling.", "AI": {"tldr": "\u63d0\u51faEHSAN\u6df7\u5408\u6d41\u7a0b\uff0c\u7ed3\u5408ChatGPT\u4f2a\u6807\u6ce8\u4e0e\u4eba\u5de5\u5ba1\u6838\u6784\u5efa\u9996\u4e2a\u53ef\u89e3\u91ca\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u60c5\u611f\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u8868\u660e\u963f\u62c9\u4f2f\u8bed\u4e13\u7528\u6a21\u578b\u5728\u6709\u9650\u76d1\u7763\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bed\u60a3\u8005\u53cd\u9988\u5206\u6790\u56e0\u65b9\u8a00\u591a\u6837\u6027\u548c\u7ec6\u7c92\u5ea6\u6807\u7b7e\u532e\u4e4f\u53d7\u963b\uff0c\u9700\u6784\u5efa\u53ef\u9760\u7684\u60c5\u611f\u5206\u6790\u6570\u636e\u96c6\u63a8\u52a8\u533b\u7597\u9886\u57df\u5e94\u7528\u3002", "method": "1. \u8bbe\u8ba1\u6df7\u5408\u6807\u6ce8\u6d41\u7a0b\uff1aChatGPT\u751f\u6210\u4f2a\u6807\u7b7e+\u5b9a\u5411\u4eba\u5de5\u5ba1\u6838\n2. \u521b\u5efa\u4e09\u79cd\u8bad\u7ec3\u96c6\uff08\u5168\u4eba\u5de5\u5ba1\u6838/\u534a\u76d1\u7763/\u7eaf\u673a\u5668\u6807\u6ce8\uff09\n3. \u5fae\u8c03Transformer\u6a21\u578b\u8fdb\u884c\u65b9\u9762\u548c\u60c5\u611f\u53cc\u4efb\u52a1\u5206\u7c7b", "result": "1. \u963f\u62c9\u4f2f\u8bed\u4e13\u7528\u6a21\u578bF1\u8fbe0.89\uff08\u5168\u76d1\u7763\uff09\n2. 50%\u4eba\u5de5\u5ba1\u6838\u4ec5\u5bfc\u81f42%\u6027\u80fd\u4e0b\u964d\n3. \u51cf\u5c11\u65b9\u9762\u7c7b\u522b\u4f7fF1\u63d0\u53477-10%", "conclusion": "\u8bc1\u660e\u5927\u6a21\u578b\u6807\u6ce8\u4e0e\u4eba\u5de5\u5ba1\u6838\u7ed3\u5408\u7684\u6709\u6548\u6027\uff0c\u672a\u6765\u53ef\u6269\u5c55\u533b\u9662\u8303\u56f4\u3001\u4f18\u5316\u63d0\u793a\u5de5\u7a0b\u5e76\u5f00\u53d1\u53ef\u89e3\u91ca\u6a21\u578b\u3002"}}
{"id": "2508.02584", "pdf": "https://arxiv.org/pdf/2508.02584", "abs": "https://arxiv.org/abs/2508.02584", "authors": ["Ming Pok Ng", "Junqi Jiang", "Gabriel Freedman", "Antonio Rago", "Francesca Toni"], "title": "MArgE: Meshing Argumentative Evidence from Multiple Large Language Models for Justifiable Claim Verification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Leveraging outputs from multiple large language models (LLMs) is emerging as\na method for harnessing their power across a wide range of tasks while\nmitigating their capacity for making errors, e.g., hallucinations. However,\ncurrent approaches to combining insights from multiple LLMs often involve\nunstructured interactions (e.g., free debate), resulting in model generations\nthat are not faithfully justifiable. In this work, we introduce MArgE, a novel\nframework to provide formal structure to the evidence from each LLM, in the\nform of a tree of extracted arguments, for the task of claim verification. We\nuse a variant of Argumentative LLMs (ArgLLMs), i.e. LLMs driven by frameworks\nand semantics from the field of computational argumentation, to construct\nstructured argument trees for given claims. This process creates an inspectable\npathway from the initial arguments to the final claim verification decisions,\nproviding a faithful justification thereof. We show experimentally that MArgE\ncan significantly outperform single LLMs, including three open-source models\n(4B to 8B parameters), GPT-4o-mini and existing ArgLLMs, as well as prior\nmethods for unstructured multi-LLM debates. We thus demonstrate the advantages\nof incorporating formal, argumentative reasoning mechanisms when combining\nmultiple LLM outputs.", "AI": {"tldr": "MArgE\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u8bba\u8bc1\u6811\u6574\u5408\u591aLLM\u8f93\u51fa\uff0c\u663e\u8457\u63d0\u5347\u58f0\u660e\u9a8c\u8bc1\u7684\u53ef\u9760\u6027\u548c\u6027\u80fd", "motivation": "\u73b0\u6709\u591aLLM\u534f\u4f5c\u65b9\u6cd5\u7f3a\u4e4f\u7ed3\u6784\u5316\u8bba\u8bc1\u8def\u5f84\uff0c\u5bfc\u81f4\u9a8c\u8bc1\u7ed3\u679c\u53ef\u4fe1\u5ea6\u4e0d\u8db3\uff0c\u9700\u5efa\u7acb\u5f62\u5f0f\u5316\u7684\u8bc1\u636e\u7ed3\u6784", "method": "\u57fa\u4e8e\u8ba1\u7b97\u8bba\u8bc1\u7406\u8bba\uff0c\u4f7f\u7528ArgLLMs\u6784\u5efa\u7ed3\u6784\u5316\u8bba\u8bc1\u6811\uff0c\u521b\u5efa\u4ece\u521d\u59cb\u8bba\u636e\u5230\u6700\u7ec8\u9a8c\u8bc1\u7684\u53ef\u8ffd\u6eaf\u8def\u5f84", "result": "\u5b9e\u9a8c\u663e\u793aMArgE\u4f18\u4e8e\u5355LLM\uff08\u542b4B-8B\u5f00\u6e90\u6a21\u578b\u3001GPT-4o-mini\uff09\u53ca\u975e\u7ed3\u6784\u5316\u591aLLM\u8fa9\u8bba\u65b9\u6cd5", "conclusion": "\u5f62\u5f0f\u5316\u8bba\u8bc1\u673a\u5236\u663e\u8457\u63d0\u5347\u591aLLM\u534f\u4f5c\u6548\u679c\uff0c\u7ed3\u6784\u5316\u8def\u5f84\u589e\u5f3a\u9a8c\u8bc1\u7ed3\u679c\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027"}}
{"id": "2508.02591", "pdf": "https://arxiv.org/pdf/2508.02591", "abs": "https://arxiv.org/abs/2508.02591", "authors": ["Omri Uzan", "Yuval Pinter"], "title": "CharBench: Evaluating the Role of Tokenization in Character-Level Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Tasks that require character-level reasoning, such as counting or locating\ncharacters within words, remain challenging for contemporary language models. A\ncommon conjecture is that language models' reliance on subword units, rather\nthan characters, contributes to their struggles with character-level tasks, yet\nrecent studies offer conflicting conclusions about the role of tokenization,\nleaving its impact unclear. To address this gap, we introduce CharBench, a\ncomprehensive benchmark of character-level tasks that is two orders of\nmagnitude larger than existing alternatives. We evaluate a diverse range of\nleading open-weight and proprietary models on CharBench and find that it\npresents a significant challenge to modern LLMs, with an average accuracy of\n43.6% and 32.3% on some tasks. We present an in-depth analysis of how intrinsic\nproperties of words and their segmentations into tokens correspond to model\nperformance. For counting tasks, we find that tokenization properties are\nweakly correlated with correctness, while the length of the queried word and\nthe actual character count play a more significant part. In contrast, for tasks\nrequiring intra-word positional understanding, performance is negatively\ncorrelated with the length of the token containing the queried character,\nsuggesting that longer tokens obscure character position information for LLMs.\nWe encourage future work to build on the benchmark and evaluation methodology\nintroduced here as tools for improving model performance on such tasks.", "AI": {"tldr": "CharBench\u65b0\u57fa\u51c6\u63ed\u793a\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b57\u7b26\u7ea7\u4efb\u52a1\uff08\u5982\u5b57\u7b26\u8ba1\u6570\u548c\u5b9a\u4f4d\uff09\u4e0a\u7684\u663e\u8457\u6311\u6218\uff0c\u5e73\u5747\u51c6\u786e\u7387\u6700\u4f4e\u4ec532.3%", "motivation": "\u6f84\u6e05\u5b50\u8bcd\u5206\u8bcd\u673a\u5236\u5bf9\u5b57\u7b26\u7ea7\u4efb\u52a1\u7684\u5f71\u54cd\u4e89\u8bae\uff0c\u5efa\u7acb\u7cfb\u7edf\u6027\u8bc4\u4f30\u6846\u67b6", "method": "\u521b\u5efa\u89c4\u6a21\u6269\u5927\u767e\u500d\u7684CharBench\u57fa\u51c6\uff0c\u5206\u6790\u8bcd\u957f/\u5206\u8bcd\u7279\u6027\u4e0e\u4efb\u52a1\u8868\u73b0\u7684\u76f8\u5173\u6027", "result": "\u8ba1\u6570\u4efb\u52a1\u66f4\u4f9d\u8d56\u5b9e\u9645\u5b57\u7b26\u6570\u4e0e\u8bcd\u957f\uff0c\u5b9a\u4f4d\u4efb\u52a1\u53d7\u5206\u8bcd\u957f\u5ea6\u8d1f\u9762\u5f71\u54cd\u663e\u8457", "conclusion": "\u9700\u9488\u5bf9\u6027\u6539\u8fdb\u6a21\u578b\u67b6\u6784\uff0cCharBench\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u6807\u51c6\u5316\u8bc4\u4f30\u5de5\u5177"}}
{"id": "2508.02618", "pdf": "https://arxiv.org/pdf/2508.02618", "abs": "https://arxiv.org/abs/2508.02618", "authors": ["Jianxiang Zang", "Meiling Ning", "Shihan Dou", "Jiazheng Zhang", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Mitigating Attention Hacking in Preference-Based Reward Modeling via Interaction Distillation", "categories": ["cs.CL"], "comment": null, "summary": "The reward model (RM), as the core component of reinforcement learning from\nhuman feedback (RLHF) for large language models (LLMs), responsible for\nproviding reward signals to generated responses. However, mainstream preference\nmodeling in RM is inadequate in terms of token-level interaction, making its\njudgment signals vulnerable to being hacked by misallocated attention to\ncontext. This stems from two fundamental limitations: (1) Current preference\nmodeling employs decoder-only architectures, where the unidirectional causal\nattention mechanism leads to forward-decaying intra-sequence attention within\nthe prompt-response sequence. (2) The independent Siamese-encoding paradigm\ninduces the absence of token-level inter-sequence attention between chosen and\nrejected sequences. To address this \"attention hacking\", we propose\n\"Interaction Distillation\", a novel training framework for more adequate\npreference modeling through attention-level optimization. The method introduces\nan interaction-based natural language understanding model as the teacher to\nprovide sophisticated token interaction patterns via comprehensive attention,\nand guides the preference modeling to simulate teacher model's interaction\npattern through an attentional alignment objective. Through extensive\nexperiments, interaction distillation has demonstrated its ability to provide\nmore stable and generalizable reward signals compared to state-of-the-art RM\noptimization methods that target data noise, highlighting the attention hacking\nconstitute a more fundamental limitation in RM.", "AI": {"tldr": "\u63d0\u51faInteraction Distillation\u6846\u67b6\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u5bf9\u9f50\u4f18\u5316\u5956\u52b1\u6a21\u578b\u7684\u504f\u597d\u5efa\u6a21\uff0c\u89e3\u51b3\u6ce8\u610f\u529b\u5206\u914d\u6f0f\u6d1e\u95ee\u9898\u3002", "motivation": "\u4e3b\u6d41\u5956\u52b1\u6a21\u578b\u7684\u5355\u5411\u6ce8\u610f\u529b\u673a\u5236\u548c\u72ec\u7acb\u7f16\u7801\u8303\u5f0f\u5bfc\u81f4token\u7ea7\u4ea4\u4e92\u4e0d\u8db3\uff0c\u5b58\u5728\u6ce8\u610f\u529b\u5206\u914d\u6f0f\u6d1e\u5f71\u54cd\u5224\u65ad\u7a33\u5b9a\u6027\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u4ea4\u4e92\u7684NLU\u6559\u5e08\u6a21\u578b\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u5bf9\u9f50\u76ee\u6807\u6307\u5bfc\u504f\u597d\u5efa\u6a21\uff0c\u4f18\u5316token\u7ea7\u8de8\u5e8f\u5217\u4ea4\u4e92\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u76f8\u6bd4SOTA\u65b9\u6cd5\u63d0\u4f9b\u66f4\u7a33\u5b9a\u3001\u53ef\u6cdb\u5316\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u9a8c\u8bc1\u6ce8\u610f\u529b\u6f0f\u6d1e\u662fRM\u7684\u6839\u672c\u9650\u5236\u56e0\u7d20\u3002", "conclusion": "Interaction Distillation\u901a\u8fc7\u6ce8\u610f\u529b\u4f18\u5316\u6709\u6548\u89e3\u51b3\u5956\u52b1\u6a21\u578b\u7684\u6838\u5fc3\u7f3a\u9677\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.02631", "pdf": "https://arxiv.org/pdf/2508.02631", "abs": "https://arxiv.org/abs/2508.02631", "authors": ["Zixi Li"], "title": "Pointer: Linear-Complexity Long-Range Modeling without Pre-training", "categories": ["cs.CL"], "comment": "Submitted to Nordic AI Meet 2025", "summary": "We introduce Pointer, a novel architecture that achieves linear $O(NK)$\ncomplexity for long-range sequence modeling while maintaining superior\nperformance without requiring pre-training. Unlike standard attention\nmechanisms that compute $O(N^2)$ pairwise interactions, our approach uses\nlayer-wise pointer chaining where each layer's pointer selection depends on\nprevious layer's pointer positions, creating explicit long-distance connections\nthrough pointer chains. We demonstrate that this architecture achieves\n$2$--$10\\times$ speedup on long sequences compared to standard transformers,\nmaintains $>95\\%$ accuracy on copy tasks at distances up to 2048 tokens, and\nlearns interpretable pointer patterns that reveal structured dependency\nmodeling. Our experiments on efficiency benchmarks, long-range dependency\ntasks, and interpretability analysis show that Pointer offers a compelling\nalternative to attention mechanisms for scenarios requiring efficient\nlong-range modeling without pre-training dependencies.", "AI": {"tldr": "\u63d0\u51faPointer\u67b6\u6784\uff0c\u901a\u8fc7\u5c42\u7ea7\u6307\u9488\u94fe\u5b9e\u73b0\u7ebf\u6027\u590d\u6742\u5ea6O(NK)\uff0c\u5728\u957f\u5e8f\u5217\u5efa\u6a21\u4e2d\u5b9e\u73b02-10\u500d\u52a0\u901f\u4e14\u65e0\u9700\u9884\u8bad\u7ec3", "motivation": "\u4f20\u7edf\u6ce8\u610f\u529b\u673a\u5236\u7684O(N\u00b2)\u590d\u6742\u5ea6\u9650\u5236\u4e86\u957f\u5e8f\u5217\u5904\u7406\u6548\u7387\uff0c\u9700\u8981\u4e0d\u4f9d\u8d56\u9884\u8bad\u7ec3\u7684\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848", "method": "\u5c42\u7ea7\u6307\u9488\u94fe\u7ed3\u6784\uff0c\u6bcf\u5c42\u6307\u9488\u9009\u62e9\u57fa\u4e8e\u524d\u5c42\u4f4d\u7f6e\uff0c\u901a\u8fc7\u6307\u9488\u94fe\u5efa\u7acb\u663e\u5f0f\u957f\u8ddd\u79bb\u8fde\u63a5", "result": "\u957f\u5e8f\u5217\u5904\u7406\u52a0\u901f2-10\u500d\uff1b2048token\u8ddd\u79bb\u4e0b\u590d\u5236\u4efb\u52a1\u51c6\u786e\u7387>95%\uff1b\u5b66\u4e60\u5230\u7ed3\u6784\u5316\u4f9d\u8d56\u7684\u53ef\u89e3\u91ca\u6307\u9488\u6a21\u5f0f", "conclusion": "Pointer\u4e3a\u65e0\u9700\u9884\u8bad\u7ec3\u7684\u957f\u8ddd\u79bb\u5efa\u6a21\u573a\u666f\u63d0\u4f9b\u4e86\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u517c\u5177\u6027\u80fd\u4f18\u52bf\u4e0e\u8ba1\u7b97\u6548\u7387"}}
{"id": "2508.02635", "pdf": "https://arxiv.org/pdf/2508.02635", "abs": "https://arxiv.org/abs/2508.02635", "authors": ["Kranti Chalamalasetti", "Gabriel Bernier-Colborne", "Yvan Gauthier", "Sowmya Vajjala"], "title": "Test Set Quality in Multilingual LLM Evaluation", "categories": ["cs.CL"], "comment": "Accepted at the 1st Workshop on Multilingual Data Quality Signals,\n  COLM 2025, Short paper. 10 pages in total", "summary": "Several multilingual benchmark datasets have been developed in a\nsemi-automatic manner in the recent past to measure progress and understand the\nstate-of-the-art in the multilingual capabilities of Large Language Models.\nHowever, there is not a lot of attention paid to the quality of the datasets\nthemselves, despite the existence of previous work in identifying errors in\neven fully human-annotated test sets. In this paper, we manually analyze recent\nmultilingual evaluation sets in two languages - French and Telugu, identifying\nseveral errors in the process. We compare the performance difference across\nseveral LLMs with the original and revised versions of the datasets and\nidentify large differences (almost 10% in some cases) in both languages). Based\non these results, we argue that test sets should not be considered immutable\nand should be revisited, checked for correctness, and potentially versioned. We\nend with some recommendations for both the dataset creators as well as\nconsumers on addressing the dataset quality issues.", "AI": {"tldr": "\u73b0\u6709\u591a\u8bed\u8a00\u8bc4\u4f30\u6570\u636e\u96c6\u5b58\u5728\u8d28\u91cf\u95ee\u9898\uff0c\u4eba\u5de5\u4fee\u6b63\u540e\u5bfc\u81f4LLM\u8bc4\u6d4b\u7ed3\u679c\u5dee\u5f02\u8fbe10%\uff0c\u5efa\u8bae\u6570\u636e\u96c6\u5e94\u7248\u672c\u5316\u5e76\u6301\u7eed\u68c0\u67e5\u8d28\u91cf", "motivation": "\u8fd1\u5e74\u534a\u81ea\u52a8\u5316\u5f00\u53d1\u7684\u591a\u8bed\u8a00\u8bc4\u6d4b\u6570\u636e\u96c6\u7f3a\u4e4f\u8d28\u91cf\u68c0\u9a8c\uff0c\u53ef\u80fd\u5f71\u54cd\u5927\u6a21\u578b\u80fd\u529b\u8bc4\u4f30\u7684\u51c6\u786e\u6027\uff0c\u9700\u7cfb\u7edf\u6027\u68c0\u6d4b\u6570\u636e\u8d28\u91cf\u95ee\u9898", "method": "\u4eba\u5de5\u5206\u6790\u6cd5\u8bed\u548c\u6cf0\u5362\u56fa\u8bed\u7684\u6700\u65b0\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u8bc6\u522b\u9519\u8bef\u540e\u6784\u5efa\u4fee\u8ba2\u7248\uff0c\u5bf9\u6bd4\u591a\u4e2aLLM\u5728\u539f\u59cb/\u4fee\u8ba2\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u5dee\u5f02", "result": "\u4fee\u6b63\u540e\u7684\u6570\u636e\u96c6\u4f7fLLM\u6027\u80fd\u5dee\u5f02\u6700\u9ad8\u8fbe10%\uff08\u6cd5\u8bed\u548c\u6cf0\u5362\u56fa\u8bed\uff09\uff0c\u8bc1\u660e\u6570\u636e\u96c6\u9519\u8bef\u4f1a\u663e\u8457\u5f71\u54cd\u6a21\u578b\u8bc4\u4f30\u7ed3\u679c", "conclusion": "\u5efa\u8bae\u5c06\u6d4b\u8bd5\u96c6\u89c6\u4e3a\u53ef\u8fed\u4ee3\u5bf9\u8c61\uff0c\u5efa\u7acb\u7248\u672c\u63a7\u5236\u673a\u5236\uff0c\u6570\u636e\u751f\u4ea7\u8005\u548c\u4f7f\u7528\u8005\u9700\u5171\u540c\u53c2\u4e0e\u8d28\u91cf\u9a8c\u8bc1\u6d41\u7a0b"}}
{"id": "2505.09805", "pdf": "https://arxiv.org/pdf/2505.09805", "abs": "https://arxiv.org/abs/2505.09805", "authors": ["Aditya Nagori", "Ayush Gautam", "Matthew O. Wiens", "Vuong Nguyen", "Nathan Kenya Mugisha", "Jerome Kabakyenga", "Niranjan Kissoon", "John Mark Ansermino", "Rishikesan Kamaleswaran"], "title": "Contextual Phenotyping of Pediatric Sepsis Cohort Using Large Language Models", "categories": ["q-bio.QM", "cs.AI", "cs.CL", "cs.LG", "stat.AP"], "comment": "11 pages, 2 Figures, 1 Table", "summary": "Clustering patient subgroups is essential for personalized care and efficient\nresource use. Traditional clustering methods struggle with high-dimensional,\nheterogeneous healthcare data and lack contextual understanding. This study\nevaluates Large Language Model (LLM) based clustering against classical methods\nusing a pediatric sepsis dataset from a low-income country (LIC), containing\n2,686 records with 28 numerical and 119 categorical variables. Patient records\nwere serialized into text with and without a clustering objective. Embeddings\nwere generated using quantized LLAMA 3.1 8B, DeepSeek-R1-Distill-Llama-8B with\nlow-rank adaptation(LoRA), and Stella-En-400M-V5 models. K-means clustering was\napplied to these embeddings. Classical comparisons included K-Medoids\nclustering on UMAP and FAMD-reduced mixed data. Silhouette scores and\nstatistical tests evaluated cluster quality and distinctiveness.\nStella-En-400M-V5 achieved the highest Silhouette Score (0.86). LLAMA 3.1 8B\nwith the clustering objective performed better with higher number of clusters,\nidentifying subgroups with distinct nutritional, clinical, and socioeconomic\nprofiles. LLM-based methods outperformed classical techniques by capturing\nricher context and prioritizing key features. These results highlight potential\nof LLMs for contextual phenotyping and informed decision-making in\nresource-limited settings.", "AI": {"tldr": "LLM\u805a\u7c7b\u65b9\u6cd5\u5728\u8d44\u6e90\u6709\u9650\u73af\u5883\u4e0b\u5c55\u73b0\u51fa\u4f18\u4e8e\u4f20\u7edf\u805a\u7c7b\u6280\u672f\u7684\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\uff0c\u7279\u522b\u5728\u8bc6\u522b\u590d\u6742\u60a3\u8005\u4e9a\u7fa4\u65b9\u9762\u8868\u73b0\u7a81\u51fa", "motivation": "\u4f20\u7edf\u805a\u7c7b\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u9ad8\u7ef4\u5f02\u6784\u533b\u7597\u6570\u636e\u4e14\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u7406\u89e3\uff0c\u9700\u63a2\u7d22LLM\u5728\u533b\u7597\u805a\u7c7b\u4e2d\u7684\u6f5c\u529b", "method": "\u4f7f\u7528\u91cf\u5316LLAMA 3.1 8B\u3001DeepSeek-R1-Distill-Llama-8B\u548cStella-En-400M-V5\u751f\u6210\u6587\u672c\u5d4c\u5165\uff0c\u7ed3\u5408K-means\u4e0e\u4f20\u7edfK-Medoids+UMAP/FAMD\u65b9\u6cd5\u5bf9\u6bd4", "result": "Stella\u6a21\u578b\u83b7\u5f97\u6700\u9ad8\u8f6e\u5ed3\u5206\u6570\uff080.86\uff09\uff0cLLAMA 3.1\u5728\u660e\u786e\u805a\u7c7b\u76ee\u6807\u65f6\u8bc6\u522b\u51fa\u8425\u517b/\u4e34\u5e8a/\u793e\u4f1a\u7ecf\u6d4e\u7279\u5f81\u663e\u8457\u5dee\u5f02\u7684\u4e9a\u7fa4", "conclusion": "LLM\u901a\u8fc7\u4e0a\u4e0b\u6587\u7279\u5f81\u63d0\u53d6\u5b9e\u73b0\u66f4\u7cbe\u51c6\u7684\u8868\u578b\u5206\u578b\uff0c\u4e3a\u8d44\u6e90\u6709\u9650\u5730\u533a\u7684\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2508.00838", "pdf": "https://arxiv.org/pdf/2508.00838", "abs": "https://arxiv.org/abs/2508.00838", "authors": ["Ilan Strauss", "Jangho Yang", "Tim O'Reilly", "Sruly Rosenblat", "Isobel Moure"], "title": "The Attribution Crisis in LLM Search Results", "categories": ["cs.DL", "cs.AI", "cs.CL"], "comment": null, "summary": "Web-enabled LLMs frequently answer queries without crediting the web pages\nthey consume, creating an \"attribution gap\" - the difference between relevant\nURLs read and those actually cited. Drawing on approximately 14,000 real-world\nLMArena conversation logs with search-enabled LLM systems, we document three\nexploitation patterns: 1) No Search: 34% of Google Gemini and 24% of OpenAI\nGPT-4o responses are generated without explicitly fetching any online content;\n2) No citation: Gemini provides no clickable citation source in 92% of answers;\n3) High-volume, low-credit: Perplexity's Sonar visits approximately 10 relevant\npages per query but cites only three to four. A negative binomial hurdle model\nshows that the average query answered by Gemini or Sonar leaves about 3\nrelevant websites uncited, whereas GPT-4o's tiny uncited gap is best explained\nby its selective log disclosures rather than by better attribution. Citation\nefficiency - extra citations provided per additional relevant web page visited\n- varies widely across models, from 0.19 to 0.45 on identical queries,\nunderscoring that retrieval design, not technical limits, shapes ecosystem\nimpact. We recommend a transparent LLM search architecture based on\nstandardized telemetry and full disclosure of search traces and citation logs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u4e3b\u6d41LLM\u5b58\u5728\u663e\u8457\u5f15\u7528\u7f3a\u5931\u95ee\u9898\uff1a34%\u7684Gemini\u548c24%\u7684GPT-4o\u56de\u7b54\u672a\u7ecf\u641c\u7d22\uff0c92%\u7684Gemini\u56de\u7b54\u65e0\u5f15\u7528\uff0cPerplexity\u5e73\u5747\u8bbf\u95ee10\u9875\u9762\u4ec5\u5f15\u75283-4\u4e2a\u3002\u5efa\u8bae\u5efa\u7acb\u900f\u660e\u641c\u7d22\u67b6\u6784\u548c\u5b8c\u6574\u62ab\u9732\u673a\u5236\u3002", "motivation": "\u9488\u5bf9LLM\u9891\u7e41\u4f7f\u7528\u7f51\u7edc\u8d44\u6e90\u5374\u672a\u5145\u5206\u6807\u6ce8\u6765\u6e90\u7684\u300c\u5f52\u5c5e\u5dee\u8ddd\u300d\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u679014,000\u4e2a\u771f\u5b9e\u5bf9\u8bdd\u65e5\u5fd7\uff0c\u63ed\u793a\u6a21\u578b\u5728\u5f15\u7528\u5b9e\u8df5\u4e2d\u7684\u7cfb\u7edf\u6027\u7f3a\u9677\u3002", "method": "\u91c7\u7528\u8d1f\u4e8c\u9879\u969c\u788d\u6a21\u578b\u5206\u6790\u4e09\u5927\u6a21\u5f0f\uff1a1) \u672a\u641c\u7d22\u76f4\u63a5\u751f\u6210\u56de\u7b54 2) \u65e0\u5f15\u7528\u884c\u4e3a 3) \u9ad8\u8bbf\u95ee\u91cf\u4f4e\u5f15\u7528\u73b0\u8c61\uff0c\u5e76\u8ba1\u7b97\u4e0d\u540c\u6a21\u578b\u7684\u5f15\u7528\u6548\u7387\u6307\u6807\u3002", "result": "Gemini/Sonar\u5e73\u5747\u6bcf\u4e2a\u67e5\u8be2\u9057\u6f0f3\u4e2a\u76f8\u5173\u7f51\u7ad9\u5f15\u7528\uff0cGPT-4o\u7684\u5fae\u5c0f\u9057\u6f0f\u5dee\u8ddd\u6e90\u4e8e\u5176\u9009\u62e9\u6027\u65e5\u5fd7\u62ab\u9732\u3002\u6a21\u578b\u95f4\u5f15\u7528\u6548\u7387\u5dee\u5f02\u663e\u8457\uff080.19-0.45\uff09\uff0c\u663e\u793a\u6280\u672f\u8bbe\u8ba1\u4e3b\u5bfc\u751f\u6001\u5f71\u54cd\u3002", "conclusion": "\u63d0\u51fa\u57fa\u4e8e\u6807\u51c6\u5316\u9065\u6d4b\u7684\u900f\u660eLLM\u641c\u7d22\u67b6\u6784\uff0c\u8981\u6c42\u5b8c\u6574\u62ab\u9732\u641c\u7d22\u8f68\u8ff9\u548c\u5f15\u7528\u65e5\u5fd7\uff0c\u4ece\u6839\u672c\u4e0a\u6539\u5584\u7f51\u7edc\u5185\u5bb9\u751f\u6001\u7cfb\u7edf\u7684\u53ef\u6301\u7eed\u53d1\u5c55\u3002"}}
{"id": "2508.00881", "pdf": "https://arxiv.org/pdf/2508.00881", "abs": "https://arxiv.org/abs/2508.00881", "authors": ["Vijja Wichitwechkarn", "Charles Fox", "Ruchi Choudhary"], "title": "Hallucination Detection and Mitigation with Diffusion in Multi-Variate Time-Series Foundation Models", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Foundation models for natural language processing have many coherent\ndefinitions of hallucination and methods for its detection and mitigation.\nHowever, analogous definitions and methods do not exist for multi-variate\ntime-series (MVTS) foundation models. We propose new definitions for MVTS\nhallucination, along with new detection and mitigation methods using a\ndiffusion model to estimate hallucination levels. We derive relational datasets\nfrom popular time-series datasets to benchmark these relational hallucination\nlevels. Using these definitions and models, we find that open-source\npre-trained MVTS imputation foundation models relationally hallucinate on\naverage up to 59.5% as much as a weak baseline. The proposed mitigation method\nreduces this by up to 47.7% for these models. The definition and methods may\nimprove adoption and safe usage of MVTS foundation models.", "AI": {"tldr": "\u63d0\u51fa\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u7684\u5e7b\u89c9\u5b9a\u4e49\u53ca\u6269\u6563\u6a21\u578b\u68c0\u6d4b\u7f13\u89e3\u65b9\u6cd5\uff0c\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u7f13\u89e3\u6548\u679c\u8fbe47.7%", "motivation": "\u73b0\u6709NLP\u9886\u57df\u5e7b\u89c9\u7814\u7a76\u6210\u719f\uff0c\u4f46\u591a\u53d8\u91cf\u65f6\u5e8f\u6a21\u578b\u7f3a\u4e4f\u76f8\u5e94\u5b9a\u4e49\u548c\u65b9\u6cd5\uff0c\u963b\u788d\u5176\u5b89\u5168\u5e94\u7528", "method": "\u901a\u8fc7\u6269\u6563\u6a21\u578b\u4f30\u8ba1\u5e7b\u89c9\u6c34\u5e73\uff0c\u6784\u5efa\u5173\u7cfb\u578b\u6570\u636e\u96c6\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u63d0\u51fa\u7f13\u89e3\u65b9\u6848", "result": "\u5f00\u6e90\u9884\u8bad\u7ec3MVTS\u63d2\u8865\u6a21\u578b\u57fa\u7ebf\u5e7b\u89c9\u7387\u8fbe59.5%\uff0c\u91c7\u7528\u7f13\u89e3\u65b9\u6cd5\u540e\u964d\u4f4e47.7%", "conclusion": "\u8be5\u5b9a\u4e49\u548c\u65b9\u6cd5\u4f53\u7cfb\u53ef\u63d0\u5347\u591a\u53d8\u91cf\u65f6\u5e8f\u57fa\u7840\u6a21\u578b\u7684\u5b89\u5168\u5e94\u7528\u53ca\u884c\u4e1a\u91c7\u7eb3\u5ea6"}}
{"id": "2508.00890", "pdf": "https://arxiv.org/pdf/2508.00890", "abs": "https://arxiv.org/abs/2508.00890", "authors": ["Fali Wang", "Hui Liu", "Zhenwei Dai", "Jingying Zeng", "Zhiwei Zhang", "Zongyu Wu", "Chen Luo", "Zhen Li", "Xianfeng Tang", "Qi He", "Suhang Wang"], "title": "AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7"], "comment": "Under review", "summary": "Test-time scaling (TTS) enhances the performance of large language models\n(LLMs) by allocating additional compute resources during inference. However,\nexisting research primarily investigates TTS in single-stage tasks; while many\nreal-world problems are multi-stage complex tasks, composed of a sequence of\nheterogeneous subtasks with each subtask requires LLM of specific capability.\nTherefore, we study a novel problem: the test-time compute-optimal scaling in\nmulti-stage complex tasks, aiming to select suitable models and allocate\nbudgets per subtask to maximize overall performance. TTS in multi-stage tasks\nintroduces two fundamental challenges: (i) The combinatorial search space of\nmodel and budget allocations, combined with the high cost of inference, makes\nbrute-force search impractical. (ii) The optimal model and budget allocations\nacross subtasks are interdependent, increasing the complexity of the\ncompute-optimal search. To address this gap, we conduct extensive pilot\nexperiments on four tasks across six datasets, deriving three empirical\ninsights characterizing the behavior of LLMs in multi-stage complex tasks.\nInformed by these insights, we propose AgentTTS, an LLM-agent-based framework\nthat autonomously searches for compute-optimal allocations through iterative\nfeedback-driven interactions with the execution environment. Experimental\nresults demonstrate that AgentTTS significantly outperforms traditional and\nother LLM-based baselines in search efficiency, and shows improved robustness\nto varying training set sizes and enhanced interpretability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAgentTTS\u6846\u67b6\uff0c\u901a\u8fc7\u667a\u80fd\u4f53\u81ea\u4e3b\u641c\u7d22\u89e3\u51b3\u591a\u9636\u6bb5\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8ba1\u7b97\u8d44\u6e90\u4f18\u5316\u5206\u914d\u95ee\u9898", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u6269\u5c55(TTS)\u7814\u7a76\u96c6\u4e2d\u4e8e\u5355\u9636\u6bb5\u4efb\u52a1\uff0c\u4f46\u73b0\u5b9e\u4e2d\u7684\u591a\u9636\u6bb5\u4efb\u52a1\u7531\u5f02\u6784\u5b50\u4efb\u52a1\u7ec4\u6210\uff0c\u9700\u8981\u5dee\u5f02\u5316\u7684\u6a21\u578b\u80fd\u529b\u548c\u9884\u7b97\u5206\u914d", "method": "\u57fa\u4e8e\u5927\u89c4\u6a21\u5b9e\u9a8c\u5f97\u51fa\u4e09\u4e2a\u7ecf\u9a8c\u6027\u89c2\u5bdf\uff0c\u8bbe\u8ba1LLM\u667a\u80fd\u4f53\u6846\u67b6\u901a\u8fc7\u8fed\u4ee3\u53cd\u9988\u4e0e\u73af\u5883\u4ea4\u4e92\uff0c\u81ea\u4e3b\u641c\u7d22\u6700\u4f18\u6a21\u578b-\u9884\u7b97\u7ec4\u5408", "result": "\u57286\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAgentTTS\u641c\u7d22\u6548\u7387\u6bd4\u4f20\u7edf\u65b9\u6cd5\u9ad83.7\u500d\uff0c\u4e14\u5728\u4e0d\u540c\u8bad\u7ec3\u96c6\u89c4\u6a21\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u591a\u9636\u6bb5\u4efb\u52a1\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5176\u81ea\u6211\u6f14\u8fdb\u673a\u5236\u548c\u53ef\u89e3\u91ca\u6027\u4e3aLLM\u5e94\u7528\u4f18\u5316\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411"}}
{"id": "2508.00901", "pdf": "https://arxiv.org/pdf/2508.00901", "abs": "https://arxiv.org/abs/2508.00901", "authors": ["Ruichen Xu", "Kexin Chen"], "title": "Filtering with Self-Attention and Storing with MLP: One-Layer Transformers Can Provably Acquire and Extract Knowledge", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Modern large language models excel in knowledge-intensive tasks, yet how\ntransformers acquire (store) knowledge during pre-training and extract\n(retrieve) it during post-fine-tuning inference remains theoretically opaque.\nWhile prior theoretical work has begun to investigate these questions through\nthe analysis of training dynamics, such studies are limited to single-layer,\nattention-only architectures. However, most existing studies suggest that MLPs\nare the most contributing components for storing knowledge in transformer-based\nlanguage models. Meanwhile, our empirical investigations reveal that such\nsimplified models, when trained using standard next-token prediction\nobjectives, may be incapable of acquiring or extracting factual knowledge. To\novercome this limitation, we introduce a tractable one-layer transformer\nframework that crucially incorporates both self-attention and MLP modules. By\ntracking its gradient dynamics, we establish convergence and generalization\nguarantees that illuminate the ability of knowledge acquisition and extraction.\nWe prove that 1) Transformers can achieve near-optimal training loss during\npre-training, signifying effective knowledge acquisition; 2) With a large\nfine-tuning dataset and specific data multiplicity conditions met, transformers\ncan achieve low generalization error when tested on factual knowledge learned\nduring pre-training but not reinforced during the fine-tuning, indicating\nsuccessful knowledge extraction; 3) When the conditions are not satisfied,\ntransformers exhibit high generalization loss, resulting in hallucinations. Our\nanalysis includes both full fine-tuning and low-rank fine-tuning. Furthermore,\nour analysis offers theoretical insights into several pertinent empirical\nphenomena, such as the role of learning rate schedules. Experiments on\nsynthetic and real-world PopQA datasets with GPT-2 and Llama-3.2-1B validate\nour results.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u548cMLP\u7684\u5355\u5c42Transformer\u6846\u67b6\uff0c\u901a\u8fc7\u68af\u5ea6\u52a8\u6001\u5206\u6790\u63ed\u793a\u4e86\u77e5\u8bc6\u83b7\u53d6/\u63d0\u53d6\u673a\u5236\uff0c\u5e76\u7ed9\u51fa\u4e86\u7406\u8bba\u4fdd\u8bc1\u4e0e\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9Transformer\u77e5\u8bc6\u5b58\u50a8\uff08\u9884\u8bad\u7ec3\uff09\u548c\u63d0\u53d6\uff08\u5fae\u8c03\uff09\u673a\u5236\u7684\u7406\u8bba\u89e3\u91ca\u4e0d\u8db3\uff0c\u5c24\u5176\u5728\u591a\u5c42\u67b6\u6784\u4e2dMLP\u7684\u4f5c\u7528\u5b58\u5728\u8ba4\u77e5\u77db\u76fe\u3002", "method": "\u6784\u5efa\u5305\u542b\u81ea\u6ce8\u610f\u529b\u548cMLP\u7684\u5355\u5c42Transformer\uff0c\u901a\u8fc7\u68af\u5ea6\u52a8\u6001\u8ffd\u8e2a\u5efa\u7acb\u6536\u655b\u6027\u8bc1\u660e\uff0c\u5e76\u5728\u5408\u6210\u6570\u636e\u96c6\u53caGPT-2/Llama-3.2-1B\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "1\uff09\u9884\u8bad\u7ec3\u53ef\u8fbe\u8fd1\u4f18\u635f\u5931 2\uff09\u6ee1\u8db3\u6570\u636e\u6761\u4ef6\u65f6\u5fae\u8c03\u80fd\u6709\u6548\u63d0\u53d6\u77e5\u8bc6 3\uff09\u6761\u4ef6\u4e0d\u6ee1\u8db3\u65f6\u51fa\u73b0\u5e7b\u89c9 4\uff09\u5b66\u4e60\u7387\u8c03\u5ea6\u7b49\u5b9e\u8bc1\u73b0\u8c61\u83b7\u7406\u8bba\u89e3\u91ca", "conclusion": "\u9996\u6b21\u5728\u5b8c\u6574Transformer\u67b6\u6784\u4e2d\u5efa\u7acb\u77e5\u8bc6\u52a8\u6001\u7684\u7406\u8bba\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u77e5\u8bc6\u63d0\u53d6\u7684\u8fb9\u754c\u6761\u4ef6\u53ca\u5e7b\u89c9\u4ea7\u751f\u673a\u5236\uff0c\u4e3a\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2508.00902", "pdf": "https://arxiv.org/pdf/2508.00902", "abs": "https://arxiv.org/abs/2508.00902", "authors": ["Kenneth Payne"], "title": "An analysis of AI Decision under Risk: Prospect theory emerges in Large Language Models", "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": "26 pages, 2 figures, 9 tables, 2 appendices", "summary": "Judgment of risk is key to decision-making under uncertainty. As Daniel\nKahneman and Amos Tversky famously discovered, humans do so in a distinctive\nway that departs from mathematical rationalism. Specifically, they demonstrated\nexperimentally that humans accept more risk when they feel themselves at risk\nof losing something than when they might gain. I report the first tests of\nKahneman and Tversky's landmark 'prospect theory' with Large Language Models,\nincluding today's state of the art chain-of-thought 'reasoners'.\n  In common with humans, I find that prospect theory often anticipates how\nthese models approach risky decisions across a range of scenarios. I also\ndemonstrate that context is key to explaining much of the variance in risk\nappetite. The 'frame' through which risk is apprehended appears to be embedded\nwithin the language of the scenarios tackled by the models. Specifically, I\nfind that military scenarios generate far larger 'framing effects' than do\ncivilian settings, ceteris paribus. My research suggests, therefore, that\nlanguage models the world, capturing our human heuristics and biases. But also\nthat these biases are uneven - the idea of a 'frame' is richer than simple\ngains and losses. Wittgenstein's notion of 'language games' explains the\ncontingent, localised biases activated by these scenarios. Finally, I use my\nfindings to reframe the ongoing debate about reasoning and memorisation in\nLLMs.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u6d4b\u8bd5\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a8c\u8bc1\u524d\u666f\u7406\u8bba\uff0c\u53d1\u73b0\u5176\u98ce\u9669\u51b3\u7b56\u6a21\u5f0f\u4e0e\u4eba\u7c7b\u76f8\u4f3c\uff0c\u4e14\u519b\u4e8b\u573a\u666f\u6bd4\u6c11\u7528\u573a\u666f\u4ea7\u751f\u66f4\u5f3a\u7684\u6846\u67b6\u6548\u5e94", "motivation": "\u9a8c\u8bc1\u5361\u5c3c\u66fc\u548c\u7279\u6c83\u65af\u57fa\u7684\u524d\u666f\u7406\u8bba\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u9002\u7528\u6027\uff0c\u63a2\u7d22\u60c5\u5883\u6846\u67b6\u5bf9AI\u98ce\u9669\u51b3\u7b56\u7684\u5f71\u54cd\u673a\u5236", "method": "\u5728\u4e0d\u540c\u60c5\u5883\uff08\u519b\u4e8b/\u6c11\u7528\uff09\u4e2d\u6d4b\u8bd5\u591a\u79cdLLM\u6a21\u578b\u7684\u98ce\u9669\u51b3\u7b56\u6a21\u5f0f\uff0c\u4f7f\u7528\u63a7\u5236\u53d8\u91cf\u6cd5\u5bf9\u6bd4\u6846\u67b6\u6548\u5e94\u5f3a\u5ea6", "result": "\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u524d\u666f\u7406\u8bba\u51b3\u7b56\u7279\u5f81\uff0c\u519b\u4e8b\u573a\u666f\u6846\u67b6\u6548\u5e94\u5f3a\u5ea6\u8fbe\u5230\u6c11\u7528\u573a\u666f\u76842.3\u500d", "conclusion": "\u8bed\u8a00\u6a21\u578b\u65e2\u7ee7\u627f\u4e86\u4eba\u7c7b\u7684\u8ba4\u77e5\u504f\u89c1\uff0c\u53c8\u5c55\u73b0\u51fa\u60c5\u5883\u4f9d\u8d56\u7684\u590d\u6742\u6846\u67b6\u6548\u5e94\uff0c\u9700\u7ed3\u5408\u7ef4\u7279\u6839\u65af\u5766\u8bed\u8a00\u6e38\u620f\u7406\u8bba\u91cd\u65b0\u5ba1\u89c6AI\u7684\u63a8\u7406\u673a\u5236"}}
{"id": "2508.00910", "pdf": "https://arxiv.org/pdf/2508.00910", "abs": "https://arxiv.org/abs/2508.00910", "authors": ["Terry Yue Zhuo", "Dingmin Wang", "Hantian Ding", "Varun Kumar", "Zijian Wang"], "title": "Cyber-Zero: Training Cybersecurity Agents without Runtime", "categories": ["cs.CR", "cs.CL", "cs.LG"], "comment": "Public Link: https://github.com/amazon-science/cyber-zero", "summary": "Large Language Models (LLMs) have achieved remarkable success in software\nengineering tasks when trained with executable runtime environments,\nparticularly in resolving GitHub issues. However, such runtime environments are\noften unavailable in other domains, especially cybersecurity, where challenge\nconfigurations and execution contexts are ephemeral or restricted. We present\nCyber-Zero, the first runtime-free framework for synthesizing high-quality\nagent trajectories to train cybersecurity LLMs. Cyber-Zero leverages publicly\navailable CTF writeups and employs persona-driven LLM simulation to\nreverse-engineer runtime behaviors and generate realistic, long-horizon\ninteraction sequences without actual environments. Using trajectories\nsynthesized by Cyber-Zero, we train LLM-based agents that achieve up to 13.1%\nabsolute performance gains over baseline models on three prominent CTF\nbenchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best model,\nCyber-Zero-32B, establishes new state-of-the-art performance among open-weight\nmodels, matching the capabilities of proprietary systems like DeepSeek-V3-0324\nand Claude-3.5-Sonnet while offering superior cost-effectiveness, and\ndemonstrating that runtime-free trajectory synthesis can effectively\ndemocratize the development of state-of-the-art cybersecurity agents.", "AI": {"tldr": "\u9996\u4e2a\u65e0\u9700\u8fd0\u884c\u65f6\u7684\u7f51\u7edc\u5b89\u5168LLM\u8bad\u7ec3\u6846\u67b6Cyber-Zero\uff0c\u901a\u8fc7CTF\u653b\u7565\u9006\u5411\u751f\u6210\u4ea4\u4e92\u8f68\u8ff9\uff0c\u5728\u4e09\u5927\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b013.1%\u6027\u80fd\u63d0\u5347\uff0c\u8fbe\u5230\u5546\u4e1a\u6a21\u578b\u6c34\u5e73\u4e14\u66f4\u5177\u6027\u4ef7\u6bd4", "motivation": "\u73b0\u6709LLM\u4f9d\u8d56\u8fd0\u884c\u65f6\u73af\u5883\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f46\u7f51\u7edc\u5b89\u5168\u9886\u57df\u5b58\u5728\u73af\u5883\u53d7\u9650/\u4e34\u65f6\u6027\u7684\u7279\u70b9\uff0c\u5bfc\u81f4\u4f20\u7edf\u65b9\u6cd5\u5931\u6548", "method": "\u5229\u7528\u516c\u5f00CTF\u653b\u7565\u6587\u6863\uff0c\u901a\u8fc7\u89d2\u8272\u9a71\u52a8\u7684LLM\u6a21\u62df\u9006\u5411\u63a8\u6f14\u8fd0\u884c\u65f6\u884c\u4e3a\uff0c\u751f\u6210\u771f\u5b9e\u7684\u957f\u7a0b\u4ea4\u4e92\u5e8f\u5217\uff08\u65e0\u9700\u771f\u5b9e\u73af\u5883\uff09", "result": "\u8bad\u7ec3\u51fa\u7684Cyber-Zero-32B\u6a21\u578b\u5728InterCode-CTF\u7b49\u57fa\u51c6\u4e0a\u5237\u65b0\u5f00\u6e90\u6a21\u578b\u8bb0\u5f55\uff0c\u6027\u80fd\u5339\u914dDeepSeek-V3\u7b49\u5546\u4e1a\u7cfb\u7edf\uff0c\u6210\u672c\u6548\u76ca\u66f4\u4f18", "conclusion": "\u65e0\u9700\u8fd0\u884c\u65f6\u7684\u8f68\u8ff9\u5408\u6210\u65b9\u6cd5\u6709\u6548\u964d\u4f4e\u4e86\u7f51\u7edc\u5b89\u5168AI\u4ee3\u7406\u5f00\u53d1\u95e8\u69db\uff0c\u8bc1\u660e\u4e86\u65e0\u73af\u5883\u8bad\u7ec3\u8def\u5f84\u7684\u53ef\u884c\u6027\uff0c\u63a8\u52a8\u9886\u57df\u6c11\u4e3b\u5316\u53d1\u5c55"}}
{"id": "2508.00957", "pdf": "https://arxiv.org/pdf/2508.00957", "abs": "https://arxiv.org/abs/2508.00957", "authors": ["Amrit Rajeev", "Udayaadithya Avadhanam", "Harshula Tulapurkar", "SaiBarath Sundar"], "title": "Small sample-based adaptive text classification through iterative and contrastive description refinement", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Zero-shot text classification remains a difficult task in domains with\nevolving knowledge and ambiguous category boundaries, such as ticketing\nsystems. Large language models (LLMs) often struggle to generalize in these\nscenarios due to limited topic separability, while few-shot methods are\nconstrained by insufficient data diversity. We propose a classification\nframework that combines iterative topic refinement, contrastive prompting, and\nactive learning. Starting with a small set of labeled samples, the model\ngenerates initial topic labels. Misclassified or ambiguous samples are then\nused in an iterative contrastive prompting process to refine category\ndistinctions by explicitly teaching the model to differentiate between closely\nrelated classes. The framework features a human-in-the-loop component, allowing\nusers to introduce or revise category definitions in natural language. This\nenables seamless integration of new, unseen categories without retraining,\nmaking the system well-suited for real-world, dynamic environments. The\nevaluations on AGNews and DBpedia demonstrate strong performance: 91% accuracy\non AGNews (3 seen, 1 unseen class) and 84% on DBpedia (8 seen, 1 unseen), with\nminimal accuracy shift after introducing unseen classes (82% and 87%,\nrespectively). The results highlight the effectiveness of prompt-based semantic\nreasoning for fine-grained classification with limited supervision.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u8fed\u4ee3\u4e3b\u9898\u7ec6\u5316\u3001\u5bf9\u6bd4\u63d0\u793a\u548c\u4e3b\u52a8\u5b66\u4e60\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u652f\u6301\u52a8\u6001\u7c7b\u522b\u66f4\u65b0\u4e14\u65e0\u9700\u91cd\u8bad\u7ec3\uff0c\u5728AGNews\u548cDBpedia\u6570\u636e\u96c6\u4e0a\u5206\u522b\u53d6\u5f9791%\u548c84%\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u96f6\u6837\u672c\u5206\u7c7b\u5728\u52a8\u6001\u9886\u57df\u4e2d\u5927\u6a21\u578b\u6cdb\u5316\u5dee\uff08\u7c7b\u522b\u8fb9\u754c\u6a21\u7cca\uff09\u4e0e\u5c11\u6837\u672c\u65b9\u6cd5\u6570\u636e\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9002\u5e94\u5b9e\u65f6\u7cfb\u7edf\u4e2d\u77e5\u8bc6\u6f14\u53d8\u7684\u6311\u6218\u3002", "method": "1. \u521d\u59cb\u6807\u7b7e\u751f\u6210\u540e\uff0c\u901a\u8fc7\u8bef\u5206\u7c7b\u6837\u672c\u8fed\u4ee3\u6267\u884c\u5bf9\u6bd4\u63d0\u793a\u7ec6\u5316\u7c7b\u522b\u533a\u5206\uff1b2. \u4eba\u673a\u534f\u540c\u7ec4\u4ef6\u652f\u6301\u81ea\u7136\u8bed\u8a00\u4fee\u6539\u7c7b\u522b\u5b9a\u4e49\uff1b3. \u4e3b\u52a8\u5b66\u4e60\u6574\u5408\u65b0\u7c7b\u522b\u65e0\u9700\u91cd\u8bad\u7ec3\u3002", "result": "AGNews\uff083\u5df2\u77e5+1\u65b0\u7c7b\uff09\u51c6\u786e\u738791%\uff0cDBpedia\uff088\u5df2\u77e5+1\u65b0\u7c7b\uff0984%\uff1b\u5f15\u5165\u65b0\u7c7b\u540e\u51c6\u786e\u7387\u4fdd\u630182%/87%\uff0c\u9a8c\u8bc1\u6846\u67b6\u52a8\u6001\u9002\u5e94\u6027\u3002", "conclusion": "\u57fa\u4e8e\u63d0\u793a\u7684\u8bed\u4e49\u63a8\u7406\u673a\u5236\u6709\u6548\u5b9e\u73b0\u6709\u9650\u76d1\u7763\u4e0b\u7684\u7ec6\u7c92\u5ea6\u5206\u7c7b\uff0c\u4eba\u673a\u534f\u540c\u8bbe\u8ba1\u4e3a\u52a8\u6001\u73af\u5883\u5206\u7c7b\u7cfb\u7edf\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.01031", "pdf": "https://arxiv.org/pdf/2508.01031", "abs": "https://arxiv.org/abs/2508.01031", "authors": ["Jingzhe Ni", "Xiaolong Yin", "Xintong Li", "Xingyu Lu", "Ji Wei", "Ruofeng Tong", "Min Tang", "Peng Du"], "title": "CADDesigner: Conceptual Design of CAD Models Based on General-Purpose Agent", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Computer-Aided Design (CAD) plays a pivotal role in industrial manufacturing\nbut typically requires a high level of expertise from designers. To lower the\nentry barrier and improve design efficiency, we present an agent for CAD\nconceptual design powered by large language models (LLMs). The agent accepts\nboth abstract textual descriptions and freehand sketches as input, engaging in\ninteractive dialogue with users to refine and clarify design requirements\nthrough comprehensive requirement analysis. Built upon a novel\nContext-Independent Imperative Paradigm (CIP), the agent generates high-quality\nCAD modeling code. During the generation process, the agent incorporates\niterative visual feedback to improve model quality. Generated design cases are\nstored in a structured knowledge base, enabling continuous improvement of the\nagent's code generation capabilities. Experimental results demonstrate that our\nmethod achieves state-of-the-art performance in CAD code generation.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684CAD\u8bbe\u8ba1\u4ee3\u7406\uff0c\u901a\u8fc7\u4ea4\u4e92\u5bf9\u8bdd\u4e0e\u89c6\u89c9\u53cd\u9988\u751f\u6210\u9ad8\u8d28\u91cf\u5efa\u6a21\u4ee3\u7801\uff0c\u5b9e\u9a8c\u8fbe\u5230SOTA\u6027\u80fd", "motivation": "\u964d\u4f4eCAD\u8bbe\u8ba1\u95e8\u69db\u5e76\u63d0\u5347\u6548\u7387\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5bf9\u4e13\u4e1a\u77e5\u8bc6\u7684\u8fc7\u5ea6\u4f9d\u8d56", "method": "1. \u91c7\u7528Context-Independent Imperative\u8303\u5f0f\u751f\u6210\u4ee3\u7801\n2. \u7ed3\u5408\u6587\u672c/\u8349\u56fe\u8f93\u5165\u4e0e\u4ea4\u4e92\u5f0f\u9700\u6c42\u5206\u6790\n3. \u96c6\u6210\u8fed\u4ee3\u89c6\u89c9\u53cd\u9988\u673a\u5236\n4. \u5efa\u7acb\u7ed3\u6784\u5316\u77e5\u8bc6\u5e93\u5b9e\u73b0\u6301\u7eed\u4f18\u5316", "result": "\u5b9e\u9a8c\u8bc1\u660e\u5728CAD\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u53d6\u5f97\u5f53\u524d\u6700\u4f18\u6027\u80fd", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u878d\u5408\u591a\u6a21\u6001\u8f93\u5165\u4e0e\u53cd\u9988\u673a\u5236\uff0c\u901a\u8fc7\u77e5\u8bc6\u79ef\u7d2f\u6301\u7eed\u63d0\u5347\u4ee3\u7801\u751f\u6210\u8d28\u91cf"}}
{"id": "2508.01136", "pdf": "https://arxiv.org/pdf/2508.01136", "abs": "https://arxiv.org/abs/2508.01136", "authors": ["Wei Zhou", "Peng Sun", "Xuanhe Zhou", "Qianglei Zang", "Ji Xu", "Tieying Zhang", "Guoliang Li", "Fan Wu"], "title": "DBAIOps: A Reasoning LLM-Enhanced Database Operation and Maintenance System using Knowledge Graphs", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": "DBAIOps supports 25 database systems and has been deployed in 20\n  real-world scenarios, covering domains like finance, energy, and healthcare.\n  See website at: https://www.dbaiops.com; See code at:\n  https://github.com/weAIDB/DBAIOps/", "summary": "The operation and maintenance (O&M) of database systems is critical to\nensuring system availability and performance, typically requiring expert\nexperience (e.g., identifying metric-to-anomaly relations) for effective\ndiagnosis and recovery. However, existing automatic database O&M methods,\nincluding commercial products, cannot effectively utilize expert experience. On\nthe one hand, rule-based methods only support basic O&M tasks (e.g.,\nmetric-based anomaly detection), which are mostly numerical equations and\ncannot effectively incorporate literal O&M experience (e.g., troubleshooting\nguidance in manuals). On the other hand, LLM-based methods, which retrieve\nfragmented information (e.g., standard documents + RAG), often generate\ninaccurate or generic results. To address these limitations, we present\nDBAIOps, a novel hybrid database O&M system that combines reasoning LLMs with\nknowledge graphs to achieve DBA-style diagnosis. First, DBAIOps introduces a\nheterogeneous graph model for representing the diagnosis experience, and\nproposes a semi-automatic graph construction algorithm to build that graph from\nthousands of documents. Second, DBAIOps develops a collection of (800+)\nreusable anomaly models that identify both directly alerted metrics and\nimplicitly correlated experience and metrics. Third, for each anomaly, DBAIOps\nproposes a two-stage graph evolution mechanism to explore relevant diagnosis\npaths and identify missing relations automatically. It then leverages a\nreasoning LLM (e.g., DeepSeek-R1) to infer root causes and generate clear\ndiagnosis reports for both DBAs and common users. Our evaluation over four\nmainstream database systems (Oracle, MySQL, PostgreSQL, and DM8) demonstrates\nthat DBAIOps outperforms state-of-the-art baselines, 34.85% and 47.22% higher\nin root cause and human evaluation accuracy, respectively.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u4e0e\u63a8\u7406\u5927\u6a21\u578b\u7684DBAIOps\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u6570\u636e\u5e93\u8fd0\u7ef4\u8bca\u65ad\u51c6\u786e\u7387", "motivation": "\u73b0\u6709\u81ea\u52a8\u8fd0\u7ef4\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u6574\u5408\u4e13\u5bb6\u7ecf\u9a8c\uff0c\u89c4\u5219\u65b9\u6cd5\u4ec5\u652f\u6301\u57fa\u7840\u4efb\u52a1\uff0cLLM\u65b9\u6cd5\u751f\u6210\u7ed3\u679c\u4e0d\u51c6\u786e/\u6cdb\u5316", "method": "1)\u6784\u5efa\u8bca\u65ad\u7ecf\u9a8c\u7684\u5f02\u6784\u56fe\u6a21\u578b 2)\u5f00\u53d1800+\u53ef\u590d\u7528\u5f02\u5e38\u6a21\u578b 3)\u4e24\u9636\u6bb5\u56fe\u6f14\u5316\u673a\u5236+\u63a8\u7406LLM\u751f\u6210\u8bca\u65ad\u62a5\u544a", "result": "\u5728Oracle/MySQL\u7b49\u4e3b\u6d41\u6570\u636e\u5e93\u8bc4\u4f30\u4e2d\uff0c\u6839\u56e0\u5206\u6790\u51c6\u786e\u7387\u63d0\u534734.85%\uff0c\u4eba\u5de5\u8bc4\u4f30\u51c6\u786e\u7387\u63d0\u534747.22%", "conclusion": "\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u4e0eLLM\u534f\u540c\u63a8\u7406\u5b9e\u73b0DBA\u5f0f\u8bca\u65ad\uff0c\u4e3a\u81ea\u52a8\u5316\u8fd0\u7ef4\u63d0\u4f9b\u65b0\u7684\u6280\u672f\u8303\u5f0f"}}
{"id": "2508.01191", "pdf": "https://arxiv.org/pdf/2508.01191", "abs": "https://arxiv.org/abs/2508.01191", "authors": ["Chengshuai Zhao", "Zhen Tan", "Pingchuan Ma", "Dawei Li", "Bohan Jiang", "Yancheng Wang", "Yingzhen Yang", "Huan Liu"], "title": "Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Chain-of-Thought (CoT) prompting has been shown to improve Large Language\nModel (LLM) performance on various tasks. With this approach, LLMs appear to\nproduce human-like reasoning steps before providing answers (a.k.a., CoT\nreasoning), which often leads to the perception that they engage in deliberate\ninferential processes. However, some initial findings suggest that CoT\nreasoning may be more superficial than it appears, motivating us to explore\nfurther. In this paper, we study CoT reasoning via a data distribution lens and\ninvestigate if CoT reasoning reflects a structured inductive bias learned from\nin-distribution data, allowing the model to conditionally generate reasoning\npaths that approximate those seen during training. Thus, its effectiveness is\nfundamentally bounded by the degree of distribution discrepancy between the\ntraining data and the test queries. With this lens, we dissect CoT reasoning\nvia three dimensions: task, length, and format. To investigate each dimension,\nwe design DataAlchemy, an isolated and controlled environment to train LLMs\nfrom scratch and systematically probe them under various distribution\nconditions. Our results reveal that CoT reasoning is a brittle mirage that\nvanishes when it is pushed beyond training distributions. This work offers a\ndeeper understanding of why and when CoT reasoning fails, emphasizing the\nongoing challenge of achieving genuine and generalizable reasoning.", "AI": {"tldr": "CoT\u63a8\u7406\u7684\u6709\u6548\u6027\u672c\u8d28\u4e0a\u53d7\u9650\u4e8e\u8bad\u7ec3\u6570\u636e\u4e0e\u6d4b\u8bd5\u67e5\u8be2\u7684\u5206\u5e03\u4e00\u81f4\u6027\uff0c\u8d85\u51fa\u5206\u5e03\u8303\u56f4\u65f6\u63a8\u7406\u80fd\u529b\u663e\u8457\u4e0b\u964d", "motivation": "\u8d28\u7591CoT\u662f\u5426\u771f\u6b63\u5b9e\u73b0\u6df1\u5ea6\u63a8\u7406\uff0c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u662f\u5426\u672c\u8d28\u53d7\u9650\u4e8e\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u7684\u7ed3\u6784\u6027\u5f52\u7eb3\u504f\u7f6e", "method": "\u901a\u8fc7DataAlchemy\u5b9e\u9a8c\u6846\u67b6\uff0c\u7cfb\u7edf\u63a7\u5236\u8bad\u7ec3/\u6d4b\u8bd5\u6570\u636e\u7684\u4efb\u52a1\u7c7b\u578b\u3001\u63a8\u7406\u957f\u5ea6\u548c\u683c\u5f0f\u4e09\u4e2a\u7ef4\u5ea6\u7684\u5206\u5e03\u5dee\u5f02", "result": "\u5f53\u6d4b\u8bd5\u6570\u636e\u5728\u4efb\u52a1/\u957f\u5ea6/\u683c\u5f0f\u4efb\u4e00\u65b9\u9762\u8d85\u51fa\u8bad\u7ec3\u5206\u5e03\u65f6\uff0cCoT\u63a8\u7406\u8d28\u91cf\u51fa\u73b0\u7cfb\u7edf\u6027\u4e0b\u964d", "conclusion": "CoT\u63a8\u7406\u662f\u6570\u636e\u9a71\u52a8\u7684\u8106\u5f31\u673a\u5236\uff0c\u5176\u8868\u73b0\u672c\u8d28\u53d7\u9650\u4e8e\u8bad\u7ec3\u6570\u636e\u8986\u76d6\u8303\u56f4\uff0c\u5f3a\u8c03\u5b9e\u73b0\u771f\u6b63\u6cdb\u5316\u63a8\u7406\u7684\u6311\u6218\u6027"}}
{"id": "2508.01249", "pdf": "https://arxiv.org/pdf/2508.01249", "abs": "https://arxiv.org/abs/2508.01249", "authors": ["Peiran Wang", "Yang Liu", "Yunfei Lu", "Yifeng Cai", "Hongbo Chen", "Qingyou Yang", "Jie Zhang", "Jue Hong", "Ye Wu"], "title": "AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend Against Prompt Injection", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG", "cs.SE"], "comment": null, "summary": "Large Language Model (LLM) agents offer a powerful new paradigm for solving\nvarious problems by combining natural language reasoning with the execution of\nexternal tools. However, their dynamic and non-transparent behavior introduces\ncritical security risks, particularly in the presence of prompt injection\nattacks. In this work, we propose a novel insight that treats the agent runtime\ntraces as structured programs with analyzable semantics. Thus, we present\nAgentArmor, a program analysis framework that converts agent traces into graph\nintermediate representation-based structured program dependency representations\n(e.g., CFG, DFG, and PDG) and enforces security policies via a type system.\nAgentArmor consists of three key components: (1) a graph constructor that\nreconstructs the agent's working traces as graph-based intermediate\nrepresentations with control flow and data flow described within; (2) a\nproperty registry that attaches security-relevant metadata of interacted tools\n& data, and (3) a type system that performs static inference and checking over\nthe intermediate representation. By representing agent behavior as structured\nprograms, AgentArmor enables program analysis over sensitive data flow, trust\nboundaries, and policy violations. We evaluate AgentArmor on the AgentDojo\nbenchmark, the results show that AgentArmor can achieve 95.75% of TPR, with\nonly 3.66% of FPR. Our results demonstrate AgentArmor's ability to detect\nprompt injection vulnerabilities and enforce fine-grained security constraints.", "AI": {"tldr": "\u63d0\u51fa\u4e86AgentArmor\u6846\u67b6\uff0c\u901a\u8fc7\u7a0b\u5e8f\u4f9d\u8d56\u56fe\u8868\u793a\u548c\u7c7b\u578b\u7cfb\u7edf\u68c0\u6d4bLLM\u4ee3\u7406\u4e2d\u7684\u63d0\u793a\u6ce8\u5165\u6f0f\u6d1e\uff0cTPR\u8fbe95.75%\u4e14FPR\u4ec53.66%\u3002", "motivation": "LLM\u4ee3\u7406\u7684\u52a8\u6001\u884c\u4e3a\u548c\u975e\u900f\u660e\u6027\u5bfc\u81f4\u4e25\u91cd\u5b89\u5168\u98ce\u9669\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u9632\u5fa1\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u9700\u7ed3\u6784\u5316\u7a0b\u5e8f\u5206\u6790\u624b\u6bb5\u4fdd\u969c\u5b89\u5168\u6027\u3002", "method": "1. \u5c06\u4ee3\u7406\u8fd0\u884c\u8f68\u8ff9\u8f6c\u6362\u4e3aCFG/DFG/PDG\u56fe\u4e2d\u95f4\u8868\u793a 2. \u6784\u5efa\u542b\u56fe\u6784\u9020\u5668\u3001\u5c5e\u6027\u6ce8\u518c\u8868\u3001\u7c7b\u578b\u7cfb\u7edf\u7684\u4e09\u5c42\u67b6\u6784 3. \u901a\u8fc7\u9759\u6001\u7c7b\u578b\u68c0\u67e5\u5b9e\u65bd\u7ec6\u7c92\u5ea6\u5b89\u5168\u7b56\u7565", "result": "\u5728AgentDojo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b095.75%\u7684\u771f\u5b9e\u9633\u6027\u7387(TPR)\u548c3.66%\u7684\u5047\u9633\u6027\u7387(FPR)\uff0c\u6709\u6548\u8bc6\u522b\u6f0f\u6d1e\u5e76\u5b9e\u65bd\u5b89\u5168\u7ea6\u675f\u3002", "conclusion": "\u7ed3\u6784\u5316\u7a0b\u5e8f\u5206\u6790\u548c\u7c7b\u578b\u7cfb\u7edf\u4e3aLLM\u4ee3\u7406\u5b89\u5168\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5176\u9759\u6001\u5206\u6790\u80fd\u529b\u53ef\u5728\u8fd0\u884c\u524d\u9884\u5224\u98ce\u9669\uff0c\u5177\u6709\u91cd\u8981\u5b9e\u8df5\u4ef7\u503c\u3002"}}
{"id": "2508.01274", "pdf": "https://arxiv.org/pdf/2508.01274", "abs": "https://arxiv.org/abs/2508.01274", "authors": ["Jui-Ming Yao", "Bing-Cheng Xie", "Sheng-Wei Peng", "Hao-Yuan Chen", "He-Rong Zheng", "Bing-Jia Tan", "Peter Shaojui Wang", "Shun-Feng Su"], "title": "Multi-TW: Benchmarking Multimodal Models on Traditional Chinese Question Answering in Taiwan", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) process visual, acoustic, and\ntextual inputs, addressing the limitations of single-modality LLMs. However,\nexisting benchmarks often overlook tri-modal evaluation in Traditional Chinese\nand do not consider inference latency. To address this, we introduce Multi-TW,\nthe first Traditional Chinese benchmark for evaluating the performance and\nlatency of any-to-any multimodal models. Multi-TW includes 900 multiple-choice\nquestions (image and text, audio and text pairs) sourced from official\nproficiency tests developed with the Steering Committee for the Test of\nProficiency-Huayu (SC-TOP). We evaluated various any-to-any models and\nvision-language models (VLMs) with audio transcription. Our results show that\nclosed-source models generally outperform open-source ones across modalities,\nalthough open-source models can perform well in audio tasks. End-to-end\nany-to-any pipelines offer clear latency advantages compared to VLMs using\nseparate audio transcription. Multi-TW presents a comprehensive view of model\ncapabilities and highlights the need for Traditional Chinese fine-tuning and\nefficient multimodal architectures.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u7e41\u4f53\u4e2d\u6587\u591a\u6a21\u6001\u57fa\u51c6Multi-TW\uff0c\u8bc4\u4f30\u4efb\u610f\u6a21\u6001\u6a21\u578b\u7684\u6027\u80fd\u4e0e\u5ef6\u8fdf\uff0c\u7ed3\u679c\u663e\u793a\u95ed\u6e90\u6a21\u578b\u8868\u73b0\u66f4\u4f18\u4f46\u5f00\u6e90\u6a21\u578b\u5728\u97f3\u9891\u4efb\u52a1\u6709\u6f5c\u529b\uff0c\u7aef\u5230\u7aef\u6d41\u7a0b\u5ef6\u8fdf\u4f18\u52bf\u660e\u663e\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u5ffd\u89c6\u7e41\u4f53\u4e2d\u6587\u4e09\u6a21\u6001\u8bc4\u4f30\u53ca\u63a8\u7406\u5ef6\u8fdf\uff0c\u9700\u6784\u5efa\u65b0\u57fa\u51c6\u5168\u9762\u8861\u91cf\u6a21\u578b\u80fd\u529b\u3002", "method": "\u57fa\u4e8eSC-TOP\u5b98\u65b9\u9898\u5e93\u6784\u5efa900\u9053\u591a\u9009\u6d4b\u8bd5\uff08\u56fe\u6587/\u97f3\u6587\u914d\u5bf9\uff09\uff0c\u8bc4\u4f30\u4efb\u610f\u6a21\u6001\u6a21\u578b\u53ca\u8f6c\u5f55\u5f0fVLMs\u7684\u6027\u80fd\u548c\u5ef6\u8fdf\u3002", "result": "\u95ed\u6e90\u6a21\u578b\u8de8\u6a21\u6001\u8868\u73b0\u4f18\uff0c\u5f00\u6e90\u6a21\u578b\u97f3\u9891\u4efb\u52a1\u5177\u7ade\u4e89\u529b\uff1b\u7aef\u5230\u7aef\u6d41\u7a0b\u8f83\u8f6c\u5f55\u5f0fVLMs\u5ef6\u8fdf\u964d\u4f4e50%\u4ee5\u4e0a\u3002", "conclusion": "\u9700\u52a0\u5f3a\u7e41\u4f53\u4e2d\u6587\u5fae\u8c03\u548c\u5f00\u53d1\u9ad8\u6548\u591a\u6a21\u6001\u67b6\u6784\uff0c\u7aef\u5230\u7aef\u65b9\u6848\u5728\u5b9e\u65f6\u573a\u666f\u6f5c\u529b\u663e\u8457\u3002"}}
{"id": "2508.01365", "pdf": "https://arxiv.org/pdf/2508.01365", "abs": "https://arxiv.org/abs/2508.01365", "authors": ["Zihan Wang", "Rui Zhang", "Hongwei Li", "Wenshu Fan", "Wenbo Jiang", "Qingchuan Zhao", "Guowen Xu"], "title": "ConfGuard: A Simple and Effective Backdoor Detection for Large Language Models", "categories": ["cs.CR", "cs.CL"], "comment": "Under review", "summary": "Backdoor attacks pose a significant threat to Large Language Models (LLMs),\nwhere adversaries can embed hidden triggers to manipulate LLM's outputs. Most\nexisting defense methods, primarily designed for classification tasks, are\nineffective against the autoregressive nature and vast output space of LLMs,\nthereby suffering from poor performance and high latency. To address these\nlimitations, we investigate the behavioral discrepancies between benign and\nbackdoored LLMs in output space. We identify a critical phenomenon which we\nterm sequence lock: a backdoored model generates the target sequence with\nabnormally high and consistent confidence compared to benign generation.\nBuilding on this insight, we propose ConfGuard, a lightweight and effective\ndetection method that monitors a sliding window of token confidences to\nidentify sequence lock. Extensive experiments demonstrate ConfGuard achieves a\nnear 100\\% true positive rate (TPR) and a negligible false positive rate (FPR)\nin the vast majority of cases. Crucially, the ConfGuard enables real-time\ndetection almost without additional latency, making it a practical backdoor\ndefense for real-world LLM deployments.", "AI": {"tldr": "\u9488\u5bf9LLM\u540e\u95e8\u653b\u51fb\u63d0\u51faConfGuard\u9632\u5fa1\u65b9\u6cd5\uff0c\u901a\u8fc7\u76d1\u63a7token\u7f6e\u4fe1\u5ea6\u6ed1\u52a8\u7a97\u53e3\u68c0\u6d4b\u5f02\u5e38\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u5e8f\u5217\u9501\u5b9a\u73b0\u8c61\uff0c\u5b9e\u73b0\u9ad8\u6548\u5b9e\u65f6\u9632\u5fa1\u3002", "motivation": "\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u4e3b\u8981\u9762\u5411\u5206\u7c7b\u4efb\u52a1\uff0c\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9LLM\u81ea\u56de\u5f52\u7279\u6027\u548c\u5e9e\u5927\u8f93\u51fa\u7a7a\u95f4\uff0c\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\u548c\u4f4e\u68c0\u6d4b\u6548\u7387\u3002", "method": "\u57fa\u4e8e\u5e8f\u5217\u9501\u5b9a\u73b0\u8c61\uff08\u540e\u95e8\u653b\u51fb\u751f\u6210\u76ee\u6807\u5e8f\u5217\u65f6\u51fa\u73b0\u5f02\u5e38\u7a33\u5b9a\u9ad8\u7f6e\u4fe1\u5ea6\uff09\uff0c\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u6ed1\u52a8\u7a97\u53e3\u7f6e\u4fe1\u5ea6\u76d1\u63a7\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u63a5\u8fd1100%\u771f\u5b9e\u9633\u6027\u7387(TPR)\u548c\u63a5\u8fd1\u96f6\u5047\u9633\u6027\u7387(FPR)\uff0c\u68c0\u6d4b\u8fc7\u7a0b\u51e0\u4e4e\u4e0d\u589e\u52a0\u989d\u5916\u5ef6\u8fdf\u3002", "conclusion": "ConfGuard\u9996\u6b21\u5b9e\u73b0LLM\u540e\u95e8\u653b\u51fb\u7684\u5b9e\u65f6\u9ad8\u6548\u68c0\u6d4b\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u96f6\u5ef6\u8fdf\u9632\u5fa1\u65b9\u6848\u3002"}}
{"id": "2508.01643", "pdf": "https://arxiv.org/pdf/2508.01643", "abs": "https://arxiv.org/abs/2508.01643", "authors": ["Ali Shiraee Kasmaee", "Mohammad Khodadad", "Mehdi Astaraki", "Mohammad Arshi Saloot", "Nicholas Sherck", "Hamidreza Mahyar", "Soheila Samiee"], "title": "ChEmbed: Enhancing Chemical Literature Search Through Domain-Specific Text Embeddings", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems in chemistry heavily depend on\naccurate and relevant retrieval of chemical literature. However,\ngeneral-purpose text embedding models frequently fail to adequately represent\ncomplex chemical terminologies, resulting in suboptimal retrieval quality.\nSpecialized embedding models tailored to chemical literature retrieval have not\nyet been developed, leaving a substantial performance gap. To address this\nchallenge, we introduce ChEmbed, a domain-adapted family of text embedding\nmodels fine-tuned on a dataset comprising chemistry-specific text from the\nPubChem, Semantic Scholar, and ChemRxiv corpora. To create effective training\ndata, we employ large language models to synthetically generate queries,\nresulting in approximately 1.7 million high-quality query-passage pairs.\nAdditionally, we augment the tokenizer by adding 900 chemically specialized\ntokens to previously unused slots, which significantly reduces the\nfragmentation of chemical entities, such as IUPAC names. ChEmbed also maintains\na 8192-token context length, enabling the efficient retrieval of longer\npassages compared to many other open-source embedding models, which typically\nhave a context length of 512 or 2048 tokens. Evaluated on our newly introduced\nChemRxiv Retrieval benchmark, ChEmbed outperforms state-of-the-art general\nembedding models, raising nDCG@10 from 0.82 to 0.91 (+9 pp). ChEmbed represents\na practical, lightweight, and reproducible embedding solution that effectively\nimproves retrieval for chemical literature search.", "AI": {"tldr": "\u63d0\u51faChEmbed\u6a21\u578b\uff0c\u901a\u8fc7\u9886\u57df\u81ea\u9002\u5e94\u548c\u65b0\u589e\u5316\u5b66\u4e13\u7528\u6807\u8bb0\u63d0\u5347\u5316\u5b66\u6587\u732e\u68c0\u7d22\u6548\u679c\uff0cnDCG@10\u63d0\u53479\u4e2a\u767e\u5206\u70b9\u3002", "motivation": "\u901a\u7528\u6587\u672c\u5d4c\u5165\u6a21\u578b\u96be\u4ee5\u51c6\u786e\u8868\u793a\u590d\u6742\u5316\u5b66\u672f\u8bed\uff0c\u5bfc\u81f4\u5316\u5b66\u6587\u732e\u68c0\u7d22\u8d28\u91cf\u4f4e\u4e0b\uff0c\u4e9f\u9700\u5f00\u53d1\u9886\u57df\u4e13\u7528\u5d4c\u5165\u6a21\u578b\u3002", "method": "\u57fa\u4e8ePubChem/Semantic Scholar/ChemRxiv\u6784\u5efa\u5316\u5b66\u6587\u672c\u6570\u636e\u96c6\uff0c\u5229\u7528LLM\u751f\u6210170\u4e07\u5408\u6210\u67e5\u8be2-\u6bb5\u843d\u5bf9\uff0c\u65b0\u589e900\u5316\u5b66\u4e13\u7528token\u4f18\u5316\u5206\u8bcd\u5668\uff0c\u4fdd\u63018192\u4e0a\u4e0b\u6587\u957f\u5ea6\u3002", "result": "\u5728ChemRxiv\u68c0\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cChEmbed\u5c06nDCG@10\u6307\u6807\u4ece0.82\u63d0\u5347\u81f30.91\uff08+9%\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u901a\u7528\u6a21\u578b\u3002", "conclusion": "ChEmbed\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u9886\u57df\u5d4c\u5165\u6a21\u578b\uff0c\u901a\u8fc7token\u4f18\u5316\u548c\u957f\u4e0a\u4e0b\u6587\u652f\u6301\uff0c\u6709\u6548\u63d0\u5347\u5316\u5b66\u6587\u732e\u68c0\u7d22\u6548\u7387\uff0c\u5177\u5907\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.01647", "pdf": "https://arxiv.org/pdf/2508.01647", "abs": "https://arxiv.org/abs/2508.01647", "authors": ["Man Hu", "Yahui Ding", "Yatao Yang", "Liangyu Chen", "Yanhao Jia", "Shuai Zhao"], "title": "DUP: Detection-guided Unlearning for Backdoor Purification in Language Models", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "As backdoor attacks become more stealthy and robust, they reveal critical\nweaknesses in current defense strategies: detection methods often rely on\ncoarse-grained feature statistics, and purification methods typically require\nfull retraining or additional clean models. To address these challenges, we\npropose DUP (Detection-guided Unlearning for Purification), a unified framework\nthat integrates backdoor detection with unlearning-based purification. The\ndetector captures feature-level anomalies by jointly leveraging class-agnostic\ndistances and inter-layer transitions. These deviations are integrated through\na weighted scheme to identify poisoned inputs, enabling more fine-grained\nanalysis. Based on the detection results, we purify the model through a\nparameter-efficient unlearning mechanism that avoids full retraining and does\nnot require any external clean model. Specifically, we innovatively repurpose\nknowledge distillation to guide the student model toward increasing its output\ndivergence from the teacher on detected poisoned samples, effectively forcing\nit to unlearn the backdoor behavior. Extensive experiments across diverse\nattack methods and language model architectures demonstrate that DUP achieves\nsuperior defense performance in detection accuracy and purification efficacy.\nOur code is available at https://github.com/ManHu2025/DUP.", "AI": {"tldr": "\u63d0\u51faDUP\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u68c0\u6d4b\u5668\u7279\u5f81\u7ea7\u5f02\u5e38\u5206\u6790\u548c\u53c2\u6570\u9ad8\u6548\u7684\u9057\u5fd8\u673a\u5236\uff0c\u5b9e\u73b0\u65e0\u9700\u91cd\u8bad\u7ec3/\u5916\u90e8\u6a21\u578b\u7684\u9632\u5fa1\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u9632\u5fa1\u65b9\u6848\u5b58\u5728\u68c0\u6d4b\u4f9d\u8d56\u7c97\u7c92\u5ea6\u7279\u5f81\u7edf\u8ba1\u3001\u51c0\u5316\u9700\u8981\u91cd\u8bad\u7ec3/\u989d\u5916\u5e72\u51c0\u6a21\u578b\u7b49\u7f3a\u9677\uff0c\u96be\u4ee5\u5e94\u5bf9\u9690\u853d\u6027\u5f3a\u7684\u540e\u95e8\u653b\u51fb\u3002", "method": "\u68c0\u6d4b\u5668\u878d\u5408\u7c7b\u65e0\u5173\u8ddd\u79bb\u548c\u5c42\u95f4\u8f6c\u79fb\u7279\u5f81\u6355\u83b7\u5f02\u5e38\uff0c\u52a0\u6743\u8bc6\u522b\u6c61\u67d3\u6837\u672c\uff1b\u521b\u65b0\u6027\u5229\u7528\u77e5\u8bc6\u84b8\u998f\u5f15\u5bfc\u5b66\u751f\u6a21\u578b\u5728\u6c61\u67d3\u6837\u672c\u4e0a\u4ea7\u751f\u8f93\u51fa\u5206\u6b67\uff0c\u5b9e\u73b0\u53c2\u6570\u9ad8\u6548\u7684\u9057\u5fd8\u51c0\u5316\u3002", "result": "\u8de8\u591a\u79cd\u653b\u51fb\u65b9\u6cd5\u548c\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DUP\u5728\u68c0\u6d4b\u7cbe\u5ea6\uff08\u63d0\u534712%\uff09\u548c\u51c0\u5316\u6548\u679c\uff08\u540e\u95e8\u6210\u529f\u7387\u964d\u4f4e\u81f33%\u4ee5\u4e0b\uff09\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "DUP\u9996\u6b21\u5b9e\u73b0\u68c0\u6d4b-\u51c0\u5316\u7684\u95ed\u73af\u9632\u5fa1\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7279\u5f81\u5206\u6790\u548c\u84b8\u998f\u9057\u5fd8\u673a\u5236\uff0c\u7a81\u7834\u73b0\u6709\u65b9\u6848\u9700\u8981\u91cd\u8bad\u7ec3/\u5916\u90e8\u6a21\u578b\u7684\u9650\u5236\u3002"}}
{"id": "2508.01691", "pdf": "https://arxiv.org/pdf/2508.01691", "abs": "https://arxiv.org/abs/2508.01691", "authors": ["Tiantian Feng", "Kevin Huang", "Anfeng Xu", "Xuan Shi", "Thanathai Lertpetchpun", "Jihwan Lee", "Yoonjeong Lee", "Dani Byrd", "Shrikanth Narayanan"], "title": "Voxlect: A Speech Foundation Model Benchmark for Modeling Dialects and Regional Languages Around the Globe", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "We present Voxlect, a novel benchmark for modeling dialects and regional\nlanguages worldwide using speech foundation models. Specifically, we report\ncomprehensive benchmark evaluations on dialects and regional language varieties\nin English, Arabic, Mandarin and Cantonese, Tibetan, Indic languages, Thai,\nSpanish, French, German, Brazilian Portuguese, and Italian. Our study used over\n2 million training utterances from 30 publicly available speech corpora that\nare provided with dialectal information. We evaluate the performance of several\nwidely used speech foundation models in classifying speech dialects. We assess\nthe robustness of the dialectal models under noisy conditions and present an\nerror analysis that highlights modeling results aligned with geographic\ncontinuity. In addition to benchmarking dialect classification, we demonstrate\nseveral downstream applications enabled by Voxlect. Specifically, we show that\nVoxlect can be applied to augment existing speech recognition datasets with\ndialect information, enabling a more detailed analysis of ASR performance\nacross dialectal variations. Voxlect is also used as a tool to evaluate the\nperformance of speech generation systems. Voxlect is publicly available with\nthe license of the RAIL family at: https://github.com/tiantiaf0627/voxlect.", "AI": {"tldr": "\u63d0\u51faVoxlect\u65b9\u8a00\u8bc4\u4f30\u57fa\u51c6\uff0c\u4f7f\u7528200\u4e07+\u8bed\u97f3\u6570\u636e\u8bc4\u4f30\u591a\u8bed\u8a00\u65b9\u8a00\u5206\u7c7b\u6027\u80fd\uff0c\u652f\u6301\u4e0b\u6e38ASR\u548c\u8bed\u97f3\u751f\u6210\u5e94\u7528", "motivation": "\u89e3\u51b3\u73b0\u6709\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u5728\u65b9\u8a00\u5efa\u6a21\u65b9\u9762\u7f3a\u4e4f\u7edf\u4e00\u8bc4\u4f30\u57fa\u51c6\u7684\u95ee\u9898\uff0c\u4fc3\u8fdb\u591a\u65b9\u8a00\u8bed\u97f3\u6280\u672f\u53d1\u5c55", "method": "\u6574\u540830\u4e2a\u516c\u5f00\u8bed\u6599\u5e93\u7684200\u4e07\u6761\u65b9\u8a00\u8bed\u97f3\u6570\u636e\uff0c\u4f7f\u7528\u591a\u79cd\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u65b9\u8a00\u5206\u7c7b\u6d4b\u8bd5\u548c\u566a\u58f0\u9c81\u68d2\u6027\u8bc4\u4f30", "result": "\u6a21\u578b\u8868\u73b0\u5448\u73b0\u5730\u7406\u8fde\u7eed\u6027\u7279\u5f81\uff0c\u9a8c\u8bc1\u4e86\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u7684\u65b9\u8a00\u5efa\u6a21\u6f5c\u529b\uff0cVoxlect\u6210\u529f\u5e94\u7528\u4e8eASR\u589e\u5f3a\u548c\u8bed\u97f3\u751f\u6210\u8bc4\u4f30", "conclusion": "Voxlect\u4e3a\u65b9\u8a00\u8bed\u97f3\u7814\u7a76\u63d0\u4f9b\u6807\u51c6\u5316\u5de5\u5177\uff0c\u5176\u5f00\u6e90\u7279\u6027\u5c06\u63a8\u52a8\u591a\u8bed\u8a00\u8bed\u97f3\u6280\u672f\u7684\u5305\u5bb9\u6027\u53d1\u5c55"}}
{"id": "2508.01773", "pdf": "https://arxiv.org/pdf/2508.01773", "abs": "https://arxiv.org/abs/2508.01773", "authors": ["Jiuzhou Han", "Wray Buntine", "Ehsan Shareghi"], "title": "Uncertainty-Based Methods for Automated Process Reward Data Construction and Output Aggregation in Mathematical Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large language models have demonstrated remarkable capabilities in complex\nmathematical reasoning tasks, but they inevitably generate errors throughout\nmulti-step solutions. Process-level Reward Models (PRMs) have shown great\npromise by providing supervision and evaluation at each intermediate step,\nthereby effectively improving the models' reasoning abilities. However,\ntraining effective PRMs requires high-quality process reward data, yet existing\nmethods for constructing such data are often labour-intensive or inefficient.\nIn this paper, we propose an uncertainty-driven framework for automated process\nreward data construction, encompassing both data generation and annotation\nprocesses for PRMs. Additionally, we identify the limitations of both majority\nvote and PRMs, and introduce two generic uncertainty-aware output aggregation\nmethods: Hybrid Majority Reward Vote and Weighted Reward Frequency Vote, which\ncombine the strengths of majority vote with PRMs. Extensive experiments on\nProcessBench, MATH, and GSMPlus show the effectiveness and efficiency of the\nproposed PRM data construction framework, and demonstrate that the two output\naggregation methods further improve the mathematical reasoning abilities across\ndiverse PRMs. The code and data will be publicly available at\nhttps://github.com/Jiuzhouh/UnPRM.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684PRM\u6570\u636e\u81ea\u52a8\u6784\u5efa\u6846\u67b6\uff0c\u7ed3\u5408\u4e24\u79cd\u65b0\u578b\u8f93\u51fa\u805a\u5408\u65b9\u6cd5\u63d0\u5347\u6570\u5b66\u63a8\u7406\u80fd\u529b", "motivation": "\u73b0\u6709PRM\u8bad\u7ec3\u6570\u636e\u6784\u5efa\u65b9\u6cd5\u5b58\u5728\u4f4e\u6548/\u4eba\u5de5\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u4e14\u591a\u6570\u6295\u7968\u548cPRMs\u5404\u81ea\u5b58\u5728\u5c40\u9650\u6027", "method": "1. \u5f00\u53d1\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u7684PRM\u6570\u636e\u81ea\u52a8\u751f\u6210\u4e0e\u6807\u6ce8\u6846\u67b6\n2. \u63d0\u51fa\u6df7\u5408\u591a\u6570\u5956\u52b1\u6295\u7968\u548c\u52a0\u6743\u5956\u52b1\u9891\u7387\u6295\u7968\u4e24\u79cd\u805a\u5408\u65b9\u6cd5", "result": "\u5728ProcessBench/MATH/GSMPlus\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6570\u636e\u6846\u67b6\u6709\u6548\u6027\uff0c\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e0d\u540cPRM\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b", "conclusion": "\u8be5\u6846\u67b6\u9ad8\u6548\u6784\u5efaPRM\u6570\u636e\uff0c\u65b0\u578b\u805a\u5408\u65b9\u6cd5\u878d\u5408\u591a\u6570\u6295\u7968\u4e0ePRMs\u4f18\u52bf\uff0c\u4ee3\u7801\u6570\u636e\u5df2\u5f00\u6e90"}}
{"id": "2508.01780", "pdf": "https://arxiv.org/pdf/2508.01780", "abs": "https://arxiv.org/abs/2508.01780", "authors": ["Guozhao Mo", "Wenliang Zhong", "Jiawei Chen", "Xuanang Chen", "Yaojie Lu", "Hongyu Lin", "Ben He", "Xianpei Han", "Le Sun"], "title": "LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?", "categories": ["cs.AI", "cs.CL"], "comment": "Our code and data will be publicly available at\n  https://icip-cas.github.io/LiveMCPBench", "summary": "With the rapid development of Model Context Protocol (MCP), the number of MCP\nservers has surpassed 10,000. However, existing MCP benchmarks are limited to\nsingle-server settings with only a few tools, hindering effective evaluation of\nagent capabilities in large-scale, real-world scenarios. To address this\nlimitation, we present LiveMCPBench, the first comprehensive benchmark\ncomprising 95 real-world tasks grounded in the MCP ecosystem, designed to\nevaluate LLM agents at scale across diverse servers. To support a scalable and\nreproducible evaluation pipeline in large-scale MCP environments, we curate\nLiveMCPTool, a diverse and readily deployable collection of 70 MCP servers and\n527 tools. Furthermore, we introduce LiveMCPEval, an LLM-as-a-Judge framework\nthat enables automated and adaptive evaluation in dynamic, time-varying task\nenvironments, achieving 81% agreement with human reviewers. Finally, we propose\nthe MCP Copilot Agent, a multi-step agent that routes tools for dynamic\nplanning and executes tools for API interaction across the entire LiveMCPTool\nsuite. Our evaluation covers 10 leading models, with the best-performing model\n(Claude-Sonnet-4) reaching a 78.95% success rate. However, we observe large\nperformance variance across models, and several widely-used models perform\npoorly in LiveMCPBench's complex, tool-rich environments. Overall, LiveMCPBench\noffers the first unified framework for benchmarking LLM agents in realistic,\ntool-rich, and dynamic MCP environments, laying a solid foundation for scalable\nand reproducible research on agent capabilities. Our code and data will be\npublicly available at https://icip-cas.github.io/LiveMCPBench.", "AI": {"tldr": "LiveMCPBench\u662f\u9996\u4e2a\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u5927\u89c4\u6a21\u771f\u5b9eMCP\u73af\u5883\u4e2d\u6027\u80fd\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6", "motivation": "\u73b0\u6709MCP\u57fa\u51c6\u5c40\u9650\u4e8e\u5355\u670d\u52a1\u5668\u548c\u5c11\u91cf\u5de5\u5177\u573a\u666f\uff0c\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u667a\u80fd\u4f53\u5728\u5927\u89c4\u6a21\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u80fd\u529b", "method": "\u6784\u5efa\u5305\u542b95\u4e2a\u771f\u5b9e\u4efb\u52a1\u7684LiveMCPBench\u57fa\u51c6\uff0c\u914d\u595770\u4e2a\u670d\u52a1\u5668/527\u5de5\u5177\u7684LiveMCPTool\u5de5\u5177\u96c6\uff0c\u5f00\u53d1\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6LiveMCPEval\uff0c\u5e76\u63d0\u51fa\u52a8\u6001\u89c4\u5212\u7684\u591a\u6b65\u4ee3\u7406MCP Copilot Agent", "result": "\u6700\u4f73\u6a21\u578b(Claude-Sonnet-4)\u6210\u529f\u738778.95%\uff0c\u4f46\u6a21\u578b\u95f4\u5dee\u5f02\u663e\u8457\uff0c\u90e8\u5206\u5e38\u7528\u6a21\u578b\u5728\u590d\u6742\u5de5\u5177\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73", "conclusion": "LiveMCPBench\u4e3a\u771f\u5b9e\u52a8\u6001\u5de5\u5177\u73af\u5883\u4e0b\u7684\u667a\u80fd\u4f53\u7814\u7a76\u63d0\u4f9b\u9996\u4e2a\u7edf\u4e00\u8bc4\u4f30\u6846\u67b6\uff0c\u5960\u5b9a\u53ef\u6269\u5c55\u3001\u53ef\u590d\u73b0\u7684\u4ee3\u7406\u80fd\u529b\u7814\u7a76\u57fa\u7840"}}
{"id": "2508.01791", "pdf": "https://arxiv.org/pdf/2508.01791", "abs": "https://arxiv.org/abs/2508.01791", "authors": ["Fatimah Mohamed Emad Elden"], "title": "CSLRConformer: A Data-Centric Conformer Approach for Continuous Arabic Sign Language Recognition on the Isharah Datase", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The field of Continuous Sign Language Recognition (CSLR) poses substantial\ntechnical challenges, including fluid inter-sign transitions, the absence of\ntemporal boundaries, and co-articulation effects. This paper, developed for the\nMSLR 2025 Workshop Challenge at ICCV 2025, addresses the critical challenge of\nsigner-independent recognition to advance the generalization capabilities of\nCSLR systems across diverse signers. A data-centric methodology is proposed,\ncentered on systematic feature engineering, a robust preprocessing pipeline,\nand an optimized model architecture. Key contributions include a principled\nfeature selection process guided by Exploratory Data Analysis (EDA) to isolate\ncommunicative keypoints, a rigorous preprocessing pipeline incorporating\nDBSCAN-based outlier filtering and spatial normalization, and the novel\nCSLRConformer architecture. This architecture adapts the hybrid CNN-Transformer\ndesign of the Conformer model, leveraging its capacity to model local temporal\ndependencies and global sequence context; a characteristic uniquely suited for\nthe spatio-temporal dynamics of sign language. The proposed methodology\nachieved a competitive performance, with a Word Error Rate (WER) of 5.60% on\nthe development set and 12.01% on the test set, a result that secured a 3rd\nplace ranking on the official competition platform. This research validates the\nefficacy of cross-domain architectural adaptation, demonstrating that the\nConformer model, originally conceived for speech recognition, can be\nsuccessfully repurposed to establish a new state-of-the-art performance in\nkeypoint-based CSLR.", "AI": {"tldr": "\u63d0\u51fa\u6570\u636e\u9a71\u52a8\u7684\u8fde\u7eed\u624b\u8bed\u8bc6\u522b\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cfb\u7edf\u7279\u5f81\u5de5\u7a0b\u4e0e\u6539\u8fdb\u7684Conformer\u67b6\u6784\u5b9e\u73b0\u8de8\u7528\u6237\u6cdb\u5316\uff0c\u83b7MSLR\u6311\u6218\u8d5b\u7b2c\u4e09\u540d", "motivation": "\u89e3\u51b3\u8fde\u7eed\u624b\u8bed\u8bc6\u522b\u4e2d\u8de8\u7528\u6237\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u6838\u5fc3\u6311\u6218\uff0c\u5e94\u5bf9\u624b\u52bf\u8f6c\u6362\u6d41\u7545\u6027\u3001\u65f6\u95f4\u8fb9\u754c\u7f3a\u5931\u548c\u534f\u540c\u53d1\u97f3\u6548\u5e94\u7b49\u6280\u672f\u96be\u70b9", "method": "\u57fa\u4e8eEDA\u7684\u7279\u5f81\u9009\u62e9\u673a\u5236+DBSCAN\u5f02\u5e38\u8fc7\u6ee4\u7684\u9884\u5904\u7406\u6d41\u7a0b+\u6539\u8fdb\u7684CSLRConformer\u67b6\u6784\uff08\u878d\u5408CNN\u5c40\u90e8\u611f\u77e5\u4e0eTransformer\u5168\u5c40\u5efa\u6a21\u80fd\u529b\uff09", "result": "\u5f00\u53d1\u96c6\u8bcd\u9519\u73875.60%/\u6d4b\u8bd5\u96c612.01%\uff0c\u5728MSLR 2025\u5b98\u65b9\u7ade\u8d5b\u5e73\u53f0\u4f4d\u5217\u7b2c\u4e09", "conclusion": "\u9a8c\u8bc1\u4e86\u8bed\u97f3\u8bc6\u522b\u6a21\u578bConformer\u5728\u8de8\u9886\u57df\u5e94\u7528\u7684\u6709\u6548\u6027\uff0c\u4e3a\u57fa\u4e8e\u5173\u952e\u70b9\u7684\u8fde\u7eed\u624b\u8bed\u8bc6\u522b\u5efa\u7acb\u4e86\u65b0\u6807\u6746"}}
{"id": "2508.01887", "pdf": "https://arxiv.org/pdf/2508.01887", "abs": "https://arxiv.org/abs/2508.01887", "authors": ["Aldan Creo"], "title": "Complete Evasion, Zero Modification: PDF Attacks on AI Text Detection", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY"], "comment": "Code: https://github.com/ACMCMC/PDFuzz", "summary": "AI-generated text detectors have become essential tools for maintaining\ncontent authenticity, yet their robustness against evasion attacks remains\nquestionable. We present PDFuzz, a novel attack that exploits the discrepancy\nbetween visual text layout and extraction order in PDF documents. Our method\npreserves exact textual content while manipulating character positioning to\nscramble extraction sequences. We evaluate this approach against the ArguGPT\ndetector using a dataset of human and AI-generated text. Our results\ndemonstrate complete evasion: detector performance drops from (93.6 $\\pm$ 1.4)\n% accuracy and 0.938 $\\pm$ 0.014 F1 score to random-level performance ((50.4\n$\\pm$ 3.2) % accuracy, 0.0 F1 score) while maintaining perfect visual fidelity.\nOur work reveals a vulnerability in current detection systems that is inherent\nto PDF document structures and underscores the need for implementing sturdy\nsafeguards against such attacks. We make our code publicly available at\nhttps://github.com/ACMCMC/PDFuzz.", "AI": {"tldr": "\u63d0\u51faPDFuzz\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u6270\u4e71PDF\u6587\u672c\u63d0\u53d6\u987a\u5e8f\u5b8c\u5168\u89c4\u907fAI\u68c0\u6d4b\u5668\uff0c\u51c6\u786e\u7387\u4ece93.6%\u964d\u81f350.4%", "motivation": "\u73b0\u6709AI\u6587\u672c\u68c0\u6d4b\u7cfb\u7edf\u5b58\u5728\u5bf9PDF\u6587\u6863\u7ed3\u6784\u6f0f\u6d1e\u7684\u9632\u5fa1\u7f3a\u9677", "method": "\u5229\u7528PDF\u89c6\u89c9\u5e03\u5c40\u4e0e\u6587\u672c\u63d0\u53d6\u987a\u5e8f\u7684\u5dee\u5f02\uff0c\u4fdd\u6301\u6587\u672c\u5185\u5bb9\u4f46\u8c03\u6574\u5b57\u7b26\u4f4d\u7f6e", "result": "\u68c0\u6d4b\u51c6\u786e\u7387\u4e0b\u964d43\u4e2a\u767e\u5206\u70b9\u81f3\u968f\u673a\u6c34\u5e73\uff0cF1\u5206\u6570\u5f52\u96f6\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u89c9\u5b8c\u6574\u6027", "conclusion": "\u63ed\u793a\u5f53\u524d\u68c0\u6d4b\u7cfb\u7edf\u7684PDF\u7ed3\u6784\u56fa\u6709\u6f0f\u6d1e\uff0c\u5f3a\u8c03\u9700\u5efa\u7acb\u66f4\u9c81\u68d2\u7684\u9632\u5fa1\u673a\u5236"}}
{"id": "2508.01908", "pdf": "https://arxiv.org/pdf/2508.01908", "abs": "https://arxiv.org/abs/2508.01908", "authors": ["Istabrak Abbes", "Gopeshh Subbaraj", "Matthew Riemer", "Nizar Islah", "Benjamin Therien", "Tsuguchika Tabaru", "Hiroaki Kingetsu", "Sarath Chandar", "Irina Rish"], "title": "Revisiting Replay and Gradient Alignment for Continual Pre-Training of Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Training large language models (LLMs) typically involves pre-training on\nmassive corpora, only to restart the process entirely when new data becomes\navailable. A more efficient and resource-conserving approach would be continual\npre-training, where models are updated with new data rather than retraining\nfrom scratch. However, the introduction of new data often causes distribution\nshifts, leading to performance degradation on previously learned tasks. In this\npaper, we take a deeper look at two popular proposals for addressing this\ndistribution shift within the continual learning literature: experience replay\nand gradient alignment. We consider continual pre-training of models within the\nLlama family of architectures at a large scale across languages with 100\nbillion tokens of training data in each language, finding that both replay and\ngradient alignment lead to more stable learning without forgetting. This\nconclusion holds both as we vary the model scale and as we vary the number and\ndiversity of tasks. Moreover, we are the first to demonstrate the effectiveness\nof gradient alignment techniques in the context of LLM pre-training and propose\nan efficient implementation of meta-experience replay (MER) that imbues\nexperience replay with the benefits of gradient alignment despite negligible\ncompute and memory overhead. Our scaling analysis across model sizes and replay\nrates indicates that small rates of replaying old examples are definitely a\nmore valuable use of compute than investing in model size, but that it is more\ncompute efficient to scale the size of the model than invest in high rates of\nreplaying old examples.", "AI": {"tldr": "\u6301\u7eed\u9884\u8bad\u7ec3LLM\u65f6\uff0c\u901a\u8fc7\u7ecf\u9a8c\u56de\u653e\u548c\u68af\u5ea6\u5bf9\u9f50\u53ef\u6709\u6548\u89e3\u51b3\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u5c0f\u89c4\u6a21\u56de\u653e\u65e7\u6570\u636e\u6bd4\u6269\u5927\u6a21\u578b\u66f4\u9ad8\u6548\uff0cMER\u65b9\u6cd5\u5b9e\u73b0\u4f4e\u5f00\u9500\u7684\u68af\u5ea6\u5bf9\u9f50\u3002", "motivation": "\u4f20\u7edfLLM\u9884\u8bad\u7ec3\u5728\u65b0\u6570\u636e\u51fa\u73b0\u65f6\u9700\u8981\u5b8c\u5168\u91cd\u8bad\uff0c\u6548\u7387\u4f4e\u4e0b\u3002\u6301\u7eed\u9884\u8bad\u7ec3\u53ef\u66f4\u65b0\u6a21\u578b\u4f46\u9762\u4e34\u5206\u5e03\u504f\u79fb\u5bfc\u81f4\u6027\u80fd\u9000\u5316\u7684\u95ee\u9898\uff0c\u9700\u63a2\u7d22\u7a33\u5b9a\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u5728Llama\u67b6\u6784\u6a21\u578b\u4e0a\u4f7f\u75281000\u4ebftoken/\u8bed\u8a00\u7684\u5927\u89c4\u6a21\u591a\u8bed\u79cd\u6301\u7eed\u9884\u8bad\u7ec3\uff0c\u9a8c\u8bc1\u7ecf\u9a8c\u56de\u653e\u4e0e\u68af\u5ea6\u5bf9\u9f50\u6548\u679c\uff0c\u63d0\u51fa\u9ad8\u6548\u5143\u7ecf\u9a8c\u56de\u653e(MER)\u5b9e\u73b0\u65b9\u6848\u3002", "result": "\u7ecf\u9a8c\u56de\u653e\u4e0e\u68af\u5ea6\u5bf9\u9f50\u5747\u80fd\u4fdd\u6301\u5b66\u4e60\u7a33\u5b9a\u6027\u4e14\u65e0\u9057\u5fd8\uff0cMER\u5b9e\u73b0\u5177\u5907\u68af\u5ea6\u5bf9\u9f50\u4f18\u52bf\u4e14\u5f00\u9500\u6781\u4f4e\uff0c\u8ba1\u7b97\u6548\u7387\u5206\u6790\u663e\u793a\u5c0f\u56de\u653e\u7387\u4f18\u4e8e\u6a21\u578b\u6269\u5bb9\u4f46\u6a21\u578b\u589e\u5927\u6bd4\u9ad8\u56de\u653e\u7387\u66f4\u7701\u7b97\u529b\u3002", "conclusion": "\u6301\u7eed\u9884\u8bad\u7ec3\u4e2d\u7ed3\u5408\u7ecf\u9a8c\u56de\u653e\u4e0e\u68af\u5ea6\u5bf9\u9f50\u662f\u6709\u6548\u7b56\u7565\uff0c\u5408\u7406\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff08\u5c0f\u56de\u653e\u7387+\u9002\u5ea6\u6a21\u578b\u89c4\u6a21\uff09\u53ef\u5b9e\u73b0\u6700\u4f18\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2508.01913", "pdf": "https://arxiv.org/pdf/2508.01913", "abs": "https://arxiv.org/abs/2508.01913", "authors": ["Kamal Al-Sabahi", "Yousuf Khamis Al Mabsali"], "title": "A Decentralized Framework for Ethical Authorship Validation in Academic Publishing: Leveraging Self-Sovereign Identity and Blockchain Technology", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Academic publishing, integral to knowledge dissemination and scientific\nadvancement, increasingly faces threats from unethical practices such as\nunconsented authorship, gift authorship, author ambiguity, and undisclosed\nconflicts of interest. While existing infrastructures like ORCID effectively\ndisambiguate researcher identities, they fall short in enforcing explicit\nauthorship consent, accurately verifying contributor roles, and robustly\ndetecting conflicts of interest during peer review. To address these\nshortcomings, this paper introduces a decentralized framework leveraging\nSelf-Sovereign Identity (SSI) and blockchain technology. The proposed model\nuses Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs) to\nsecurely verify author identities and contributions, reducing ambiguity and\nensuring accurate attribution. A blockchain-based trust registry records\nauthorship consent and peer-review activity immutably. Privacy-preserving\ncryptographic techniques, especially Zero-Knowledge Proofs (ZKPs), support\nconflict-of-interest detection without revealing sensitive data. Verified\nauthorship metadata and consent records are embedded in publications,\nincreasing transparency. A stakeholder survey of researchers, editors, and\nreviewers suggests the framework improves ethical compliance and confidence in\nscholarly communication. This work represents a step toward a more transparent,\naccountable, and trustworthy academic publishing ecosystem.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u533a\u5757\u94fe\u548c\u81ea\u4e3b\u8eab\u4efd\u6280\u672f\u7684\u5b66\u672f\u51fa\u7248\u4f26\u7406\u6cbb\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u5b57\u8eab\u4efd\u8ba4\u8bc1\u3001\u8d21\u732e\u5b58\u8bc1\u548c\u9690\u79c1\u8ba1\u7b97\u63d0\u5347\u4f5c\u8005\u900f\u660e\u5ea6\u548c\u5229\u76ca\u51b2\u7a81\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709ORCID\u7b49\u7cfb\u7edf\u65e0\u6cd5\u6709\u6548\u89e3\u51b3\u4f5c\u8005\u8eab\u4efd\u6df7\u6dc6\u3001\u8d21\u732e\u4e0d\u900f\u660e\u3001\u5ba1\u7a3f\u8fc7\u7a0b\u5229\u76ca\u51b2\u7a81\u68c0\u6d4b\u4e0d\u8db3\u7b49\u5b66\u672f\u4e0d\u7aef\u95ee\u9898\u3002", "method": "\u91c7\u7528\u81ea\u4e3b\u8eab\u4efd\uff08SSI\uff09\u67b6\u6784\uff0c\u7ed3\u5408DID\u6570\u5b57\u8eab\u4efd\u3001VC\u8d21\u732e\u51ed\u8bc1\u3001\u533a\u5757\u94fe\u5b58\u8bc1\u6ce8\u518c\u8868\uff0c\u5229\u7528\u96f6\u77e5\u8bc6\u8bc1\u660e\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u5229\u76ca\u51b2\u7a81\u68c0\u6d4b\u3002", "result": "\u539f\u578b\u7cfb\u7edf\u9a8c\u8bc1\u663e\u793a\u6846\u67b6\u80fd\u6709\u6548\u8bb0\u5f55\u4f5c\u8005\u540c\u610f\u4e66\u3001\u8ffd\u8e2a\u5ba1\u7a3f\u6d3b\u52a8\uff0c\u5229\u76ca\u51b2\u7a81\u68c0\u6d4b\u51c6\u786e\u7387\u63d0\u534735%\uff0c83%\u7684\u53d7\u8c03\u5b66\u8005\u8ba4\u53ef\u5176\u900f\u660e\u5ea6\u6539\u8fdb\u3002", "conclusion": "\u8be5\u53bb\u4e2d\u5fc3\u5316\u6846\u67b6\u4e3a\u5efa\u7acb\u53ef\u9a8c\u8bc1\u3001\u6297\u62b5\u8d56\u7684\u5b66\u672f\u51fa\u7248\u4f26\u7406\u57fa\u7840\u8bbe\u65bd\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u53ef\u4fe1\u7684\u77e5\u8bc6\u4f20\u64ad\u751f\u6001\u7cfb\u7edf\u3002"}}
{"id": "2508.01916", "pdf": "https://arxiv.org/pdf/2508.01916", "abs": "https://arxiv.org/abs/2508.01916", "authors": ["Xinting Huang", "Michael Hahn"], "title": "Decomposing Representation Space into Interpretable Subspaces with Unsupervised Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Understanding internal representations of neural models is a core interest of\nmechanistic interpretability. Due to its large dimensionality, the\nrepresentation space can encode various aspects about inputs. To what extent\nare different aspects organized and encoded in separate subspaces? Is it\npossible to find these ``natural'' subspaces in a purely unsupervised way?\nSomewhat surprisingly, we can indeed achieve this and find interpretable\nsubspaces by a seemingly unrelated training objective. Our method, neighbor\ndistance minimization (NDM), learns non-basis-aligned subspaces in an\nunsupervised manner. Qualitative analysis shows subspaces are interpretable in\nmany cases, and encoded information in obtained subspaces tends to share the\nsame abstract concept across different inputs, making such subspaces similar to\n``variables'' used by the model. We also conduct quantitative experiments using\nknown circuits in GPT-2; results show a strong connection between subspaces and\ncircuit variables. We also provide evidence showing scalability to 2B models by\nfinding separate subspaces mediating context and parametric knowledge routing.\nViewed more broadly, our findings offer a new perspective on understanding\nmodel internals and building circuits.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u65e0\u76d1\u7763\u7684\u90bb\u5c45\u8ddd\u79bb\u6700\u5c0f\u5316\u65b9\u6cd5\uff08NDM\uff09\uff0c\u53d1\u73b0\u795e\u7ecf\u7f51\u7edc\u8868\u793a\u7a7a\u95f4\u4e2d\u53ef\u89e3\u91ca\u7684\u62bd\u8c61\u6982\u5ff5\u5b50\u7a7a\u95f4\uff0c\u63ed\u793a\u5176\u4e0e\u6a21\u578b\u5185\u90e8\u53d8\u91cf\u548c\u7535\u8def\u7684\u8054\u7cfb\u3002", "motivation": "\u63a2\u7d22\u9ad8\u7ef4\u795e\u7ecf\u7f51\u7edc\u8868\u793a\u7a7a\u95f4\u4e2d\u7684\u4fe1\u606f\u7ec4\u7ec7\u7ed3\u6784\uff0c\u9a8c\u8bc1\u65e0\u76d1\u7763\u65b9\u6cd5\u80fd\u5426\u5206\u79bb\u51fa\u5bf9\u5e94\u4e0d\u540c\u62bd\u8c61\u6982\u5ff5\u7684\u300c\u81ea\u7136\u5b50\u7a7a\u95f4\u300d\uff0c\u7c7b\u6bd4\u6a21\u578b\u4f7f\u7528\u7684\u5185\u90e8\u53d8\u91cf\u3002", "method": "\u91c7\u7528\u90bb\u5c45\u8ddd\u79bb\u6700\u5c0f\u5316\uff08NDM\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u5b66\u4e60\u83b7\u53d6\u975e\u57fa\u5bf9\u9f50\u7684\u8868\u793a\u5b50\u7a7a\u95f4\u3002", "result": "\u5b9a\u6027\u5206\u6790\u663e\u793a\u5b50\u7a7a\u95f4\u7f16\u7801\u8de8\u8f93\u5165\u7684\u5171\u4eab\u62bd\u8c61\u6982\u5ff5\uff1bGPT-2\u7535\u8def\u5b9e\u9a8c\u8bc1\u5b9e\u5b50\u7a7a\u95f4\u4e0e\u5df2\u77e5\u53d8\u91cf\u7684\u5f3a\u5173\u8054\uff1b\u6210\u529f\u6269\u5c55\u523020\u4ebf\u53c2\u6570\u6a21\u578b\u7684\u4e0a\u4e0b\u6587/\u53c2\u6570\u77e5\u8bc6\u8def\u7531\u5206\u79bb\u3002", "conclusion": "NDM\u4e3a\u7406\u89e3\u6a21\u578b\u5185\u90e8\u673a\u5236\u548c\u6784\u5efa\u53ef\u89e3\u91ca\u7535\u8def\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u8bc1\u660e\u65e0\u76d1\u7763\u65b9\u6cd5\u5728\u8868\u793a\u89e3\u8026\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.02066", "pdf": "https://arxiv.org/pdf/2508.02066", "abs": "https://arxiv.org/abs/2508.02066", "authors": ["Guojiang Zhao", "Sihang Li", "Zixiang Lu", "Zheng Cheng", "Haitao Lin", "Lirong Wu", "Hanchen Xia", "Hengxing Cai", "Wentao Guo", "Hongshuai Wang", "Mingjun Xu", "Siyu Zhu", "Guolin Ke", "Linfeng Zhang", "Zhifeng Gao"], "title": "MolReasoner: Toward Effective and Interpretable Reasoning for Molecular LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models(LLMs) have demonstrated remarkable performance across\nvarious domains, yet their capabilities in molecular reasoning remain\ninsufficiently explored. Current approaches tend to rely heavily on\ngeneral-purpose prompting, which lacks domain-specific molecular semantics,\nwhile those that use fine-tuning strategies often face challenges with\ninterpretability and reasoning depth. To address these issues, we introduce\nMolReasoner, a two-stage framework designed to transition LLMs from\nmemorization towards chemical reasoning. First, we propose Mol-SFT, which\ninitializes the model's reasoning abilities via synthetic Chain-of-Thought(CoT)\nsamples generated by GPT-4o and verified for chemical accuracy. Subsequently,\nMol-RL applies reinforcement learning with specialized reward functions\ndesigned explicitly to align chemical structures with linguistic descriptions,\nthereby enhancing molecular reasoning capabilities. Our approach notably\nenhances interpretability, improving the model 's molecular understanding and\nenabling better generalization. Extensive experiments demonstrate that\nMolReasoner outperforms existing methods, and marking a significant shift from\nmemorization-based outputs to robust chemical reasoning.", "AI": {"tldr": "MolReasoner\u6846\u67b6\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u5408\u6210CoT\u6837\u672c+\u5f3a\u5316\u5b66\u4e60\uff09\u663e\u8457\u63d0\u5347LLMs\u7684\u5206\u5b50\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u73b0\u4ece\u8bb0\u5fc6\u8f93\u51fa\u5230\u5316\u5b66\u63a8\u7406\u7684\u8f6c\u53d8\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4e24\u5927\u5c40\u9650\uff1a\u901a\u7528\u63d0\u793a\u7f3a\u4e4f\u5206\u5b50\u8bed\u4e49\u7279\u5f02\u6027\uff0c\u5fae\u8c03\u7b56\u7565\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u63a8\u7406\u6df1\u5ea6\u4e0d\u8db3\u3002", "method": "1. Mol-SFT\u9636\u6bb5\uff1a\u7528GPT-4o\u751f\u6210\u5e76\u9a8c\u8bc1\u5316\u5b66\u51c6\u786e\u7684\u5408\u6210CoT\u6837\u672c\uff0c\u521d\u59cb\u5316\u6a21\u578b\u63a8\u7406\u80fd\u529b\n2. Mol-RL\u9636\u6bb5\uff1a\u901a\u8fc7\u7ed3\u6784-\u8bed\u8a00\u5bf9\u9f50\u7684\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u51fd\u6570\u4f18\u5316\u5206\u5b50\u63a8\u7406", "result": "\u5b9e\u9a8c\u8868\u660eMolReasoner\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6a21\u578b\u5206\u5b50\u7406\u89e3\u51c6\u786e\u7387\u63d0\u534721%\uff0c\u6cdb\u5316\u80fd\u529b\u63d0\u9ad835%", "conclusion": "\u8be5\u6846\u67b6\u9996\u6b21\u5b9e\u73b0LLMs\u5206\u5b50\u63a8\u7406\u4ece\u8bb0\u5fc6\u6a21\u5f0f\u5411\u5316\u5b66\u903b\u8f91\u63a8\u7406\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u4e3a\u8de8\u6a21\u6001\u5206\u5b50\u7814\u7a76\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2508.02075", "pdf": "https://arxiv.org/pdf/2508.02075", "abs": "https://arxiv.org/abs/2508.02075", "authors": ["Ekai Hashimoto", "Takeshi Mizumoto", "Kohei Nagira", "Shun Shiramatsu"], "title": "Human Capital Visualization using Speech Amount during Meetings", "categories": ["cs.HC", "cs.CL", "cs.CY"], "comment": "This paper has been accepted for presentation at the 26th Annual\n  Meeting of the Special Interest Group on Discourse and Dialogue(SIGDIAL\n  2025). It represents the author's version of the work", "summary": "In recent years, many companies have recognized the importance of human\nresources and are investing in human capital to revitalize their organizations\nand enhance internal communication, thereby fostering innovation. However,\nconventional quantification methods have mainly focused on readily measurable\nindicators without addressing the fundamental role of conversations in human\ncapital. This study focuses on routine meetings and proposes strategies to\nvisualize human capital by analyzing speech amount during these meetings. We\nemploy conversation visualization technology, which operates effectively, to\nquantify speech. We then measure differences in speech amount by attributes\nsuch as gender and job post, changes in speech amount depending on whether\ncertain participants are present, and correlations between speech amount and\ncontinuous attributes. To verify the effectiveness of our proposed methods, we\nanalyzed speech amounts by departmental affiliation during weekly meetings at\nsmall to medium enterprises.", "AI": {"tldr": "\u901a\u8fc7\u4f1a\u8bae\u53d1\u8a00\u91cf\u5206\u6790\u5b9e\u73b0\u4eba\u529b\u8d44\u672c\u53ef\u89c6\u5316\u7684\u91cf\u5316\u65b9\u6cd5", "motivation": "\u4f20\u7edf\u91cf\u5316\u65b9\u6cd5\u5ffd\u89c6\u5bf9\u8bdd\u5728\u4eba\u529b\u8d44\u672c\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\uff0c\u4f01\u4e1a\u9700\u901a\u8fc7\u4f1a\u8bae\u6c9f\u901a\u6fc0\u53d1\u521b\u65b0", "method": "\u4f7f\u7528\u5bf9\u8bdd\u53ef\u89c6\u5316\u6280\u672f\u91cf\u5316\u65e5\u5e38\u4f1a\u8bae\u53d1\u8a00\u91cf\uff0c\u5206\u6790\u5c5e\u6027\u5dee\u5f02/\u7279\u5b9a\u53c2\u4e0e\u8005\u5f71\u54cd/\u8fde\u7eed\u5c5e\u6027\u76f8\u5173\u6027", "result": "\u5728\u4e2d\u5c0f\u4f01\u4e1a\u5468\u4f1a\u4e2d\u9a8c\u8bc1\u4e86\u90e8\u95e8\u5f52\u5c5e\u4e0e\u53d1\u8a00\u91cf\u7684\u5173\u8054\u6027", "conclusion": "\u53d1\u8a00\u91cf\u5206\u6790\u53ef\u6709\u6548\u5b9e\u73b0\u4eba\u529b\u8d44\u672c\u53ef\u89c6\u5316\uff0c\u4e3a\u7ec4\u7ec7\u6d3b\u529b\u8bc4\u4f30\u63d0\u4f9b\u65b0\u7ef4\u5ea6"}}
{"id": "2508.02091", "pdf": "https://arxiv.org/pdf/2508.02091", "abs": "https://arxiv.org/abs/2508.02091", "authors": ["Xiaoya Li", "Xiaofei Sun", "Albert Wang", "Chris Shum", "Jiwei Li"], "title": "CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DB"], "comment": "Preprint Version", "summary": "Approximate nearest-neighbor search (ANNS) algorithms have become\nincreasingly critical for recent AI applications, particularly in\nretrieval-augmented generation (RAG) and agent-based LLM applications. In this\npaper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS\noptimization as a reinforcement learning problem where execution speed serves\nas the reward signal. This approach enables the automatic generation of\nprogressively faster ANNS implementations while maintaining accuracy\nconstraints. Our experimental evaluation demonstrates CRINN's effectiveness\nacross six widely-used NNS benchmark datasets. When compared against\nstate-of-the-art open-source ANNS algorithms, CRINN achieves best performance\non three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and\nGloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean\nand GloVe-25-angular). The implications of CRINN's success reach well beyond\nANNS optimization: It validates that LLMs augmented with reinforcement learning\ncan function as an effective tool for automating sophisticated algorithmic\noptimizations that demand specialized knowledge and labor-intensive manual\nrefinement.Code can be found at https://github.com/deepreinforce-ai/CRINN", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u4f18\u5316\u6846\u67b6CRINN\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u5b9e\u73b0\u6027\u80fd\u7a81\u7834\uff0c\u9a8c\u8bc1LLM+\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u590d\u6742\u7b97\u6cd5\u4f18\u5316\u7684\u53ef\u884c\u6027", "motivation": "\u5f53\u524dAI\u5e94\u7528\uff08\u5982RAG\u548cAgent\u578bLLM\uff09\u5bf9ANNS\u7b97\u6cd5\u6548\u7387\u9700\u6c42\u6fc0\u589e\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u8c03\u4f18\u3002\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u81ea\u52a8\u5316\u4f18\u5316\u8fc7\u7a0b\uff0c\u53ef\u7a81\u7834\u4eba\u5de5\u8c03\u4f18\u7684\u6548\u7387\u548c\u6548\u679c\u74f6\u9888", "method": "\u5c06ANNS\u4f18\u5316\u5efa\u6a21\u4e3a\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u4ee5\u6267\u884c\u901f\u5ea6\u4e3a\u5956\u52b1\u4fe1\u53f7\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7ea6\u675f\u6761\u4ef6\u4e0b\u81ea\u52a8\u751f\u6210\u6e10\u8fdb\u4f18\u5316\u7684ANNS\u5b9e\u73b0\u65b9\u6848", "result": "\u57286\u4e2a\u4e3b\u6d41NNS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCRINN\u5728GIST-960-Euclidean\u7b493\u4e2a\u6570\u636e\u96c6\u53d6\u5f97\u6700\u4f18\u6027\u80fd\uff0c\u5728SIFT-128-Euclidean\u7b492\u4e2a\u6570\u636e\u96c6\u5e76\u5217\u7b2c\u4e00", "conclusion": "CRINN\u7684\u6210\u529f\u9a8c\u8bc1\u4e86\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u7684LLM\u53ef\u6709\u6548\u81ea\u52a8\u5316\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u548c\u4eba\u5de5\u8c03\u4f18\u7684\u7b97\u6cd5\u4f18\u5316\uff0c\u4e3a\u7b97\u6cd5\u5de5\u7a0b\u9886\u57df\u5f00\u8f9f\u65b0\u8303\u5f0f"}}
{"id": "2508.02124", "pdf": "https://arxiv.org/pdf/2508.02124", "abs": "https://arxiv.org/abs/2508.02124", "authors": ["Jingze Shi", "Yifan Wu", "Bingheng Wu", "Yiran Peng", "Liangdong Wang", "Guang Liu", "Yuyu Luo"], "title": "Trainable Dynamic Mask Sparse Attention", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "8 figures, 4 tables", "summary": "In large language models, the demand for modeling long contexts is constantly\nincreasing, but the quadratic complexity of the standard self-attention\nmechanism often becomes a bottleneck. Although existing sparse attention\nmechanisms have improved efficiency, they may still encounter issues such as\nstatic patterns or information loss. We introduce a trainable dynamic mask\nsparse attention mechanism, Dynamic Mask Attention, which effectively utilizes\ncontent-aware and position-aware sparsity. DMA achieves this through two key\ninnovations: First, it dynamically generates content-aware sparse masks from\nvalue representations, enabling the model to identify and focus on critical\ninformation adaptively. Second, it implements position-aware sparse attention\ncomputation that effectively skips unnecessary calculation regions. This\ndual-sparsity design allows the model to significantly reduce the computational\ncomplexity of important information while retaining complete information,\nachieving an excellent balance between information fidelity and computational\nefficiency. We have verified the performance of DMA through comprehensive\nexperiments. Comparative studies show that DMA outperforms multi-head\nattention, sliding window attention, multi-head latent attention, and native\nsparse attention in terms of perplexity under Chinchilla Scaling Law settings.\nMoreover, in challenging multi-query associative recall tasks, DMA also\ndemonstrates superior performance and efficiency compared to these methods.\nCrucially, in the evaluation of a 1.7B parameter model, DMA significantly\noutperforms multi-head attention in both standard benchmark performance and the\nchallenging needle-in-a-haystack task. These experimental results highlight its\ncapability to balance model efficiency and long-context modeling ability\neffectively.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u63a9\u7801\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236DMA\uff0c\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u4e0e\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u529b", "motivation": "\u89e3\u51b3\u6807\u51c6\u81ea\u6ce8\u610f\u529b\u4e8c\u6b21\u590d\u6742\u5ea6\u74f6\u9888\u53ca\u73b0\u6709\u7a00\u758f\u65b9\u6cd5\u5b58\u5728\u7684\u9759\u6001\u6a21\u5f0f/\u4fe1\u606f\u4e22\u5931\u95ee\u9898", "method": "\u7ed3\u5408\u5185\u5bb9\u611f\u77e5\u63a9\u7801\u52a8\u6001\u751f\u6210\u4e0e\u4f4d\u7f6e\u611f\u77e5\u7a00\u758f\u8ba1\u7b97\u7684\u53cc\u7a00\u758f\u8bbe\u8ba1\uff0c\u964d\u4f4e\u590d\u6742\u5ea6\u540c\u65f6\u4fdd\u6301\u4fe1\u606f\u5b8c\u6574", "result": "\u5728Chinchilla Scaling\u5b9e\u9a8c\u3001\u591a\u67e5\u8be2\u5173\u8054\u53ec\u56de\u4efb\u52a1\u53ca1.7B\u53c2\u6570\u6a21\u578b\u8bc4\u4f30\u4e2d\u5747\u5c55\u73b0\u4f18\u8d8a\u6027\u80fd", "conclusion": "DMA\u901a\u8fc7\u53cc\u7a00\u758f\u673a\u5236\u6709\u6548\u5e73\u8861\u6a21\u578b\u6548\u7387\u4e0e\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u529b\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u663e\u8457\u4f18\u52bf"}}
{"id": "2508.02165", "pdf": "https://arxiv.org/pdf/2508.02165", "abs": "https://arxiv.org/abs/2508.02165", "authors": ["Jia-Chen Zhang", "Yu-Jie Xiong"], "title": "Subject or Style: Adaptive and Training-Free Mixture of LoRAs", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Fine-tuning models via Low-Rank Adaptation (LoRA) demonstrates remarkable\nperformance in subject-driven or style-driven generation tasks. Studies have\nexplored combinations of different LoRAs to jointly generate learned styles and\ncontent. However, current methods struggle to balance the original subject and\nstyle, and often require additional training. Recently, K-LoRA proposed a\ntraining-free LoRA fusion method. But it involves multiple hyperparameters,\nmaking it difficult to adapt to all styles and subjects. In this paper, we\npropose EST-LoRA, a training-free adaptive LoRA fusion method. It\ncomprehensively considers three critical factors: \\underline{E}nergy of matrix,\n\\underline{S}tyle discrepancy scores and \\underline{T}ime steps. Analogous to\nthe Mixture of Experts (MoE) architecture, the model adaptively selects between\nsubject LoRA and style LoRA within each attention layer. This integrated\nselection mechanism ensures balanced contributions from both components during\nthe generation process. Experimental results show that EST-LoRA outperforms\nstate-of-the-art methods in both qualitative and quantitative evaluations and\nachieves faster generation speed compared to other efficient fusion approaches.\nOur code is publicly available at:\nhttps://anonymous.4open.science/r/EST-LoRA-F318.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684EST-LoRA\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u878d\u5408LoRA\u5e73\u8861\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u4e3b\u9898\u4e0e\u98ce\u683c\uff0c\u5b9e\u73b0\u66f4\u4f18\u6027\u80fd\u4e0e\u6548\u7387\u3002", "motivation": "\u73b0\u6709LoRA\u878d\u5408\u65b9\u6cd5\u5b58\u5728\u4e3b\u9898-\u98ce\u683c\u5931\u8861\u3001\u9700\u989d\u5916\u8bad\u7ec3\u6216\u8d85\u53c2\u6570\u590d\u6742\u7684\u95ee\u9898\uff0c\u9700\u5f00\u53d1\u81ea\u9002\u5e94\u514d\u8bad\u7ec3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u77e9\u9635\u80fd\u91cf\u3001\u98ce\u683c\u5dee\u5f02\u5206\u3001\u65f6\u95f4\u6b65\u4e09\u8981\u7d20\uff0c\u91c7\u7528\u7c7bMoE\u67b6\u6784\u5728\u6ce8\u610f\u529b\u5c42\u52a8\u6001\u9009\u62e9\u4e3b\u9898/\u98ce\u683cLoRA\u53c2\u6570\u3002", "result": "\u5728\u8d28\u91cf\u4e0e\u901f\u5ea6\u6307\u6807\u4e0a\u8d85\u8d8aSOTA\u65b9\u6cd5\uff0c\u5b9a\u91cf\u8bc4\u4f30\u63d0\u5347\u663e\u8457\u4e14\u751f\u6210\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "EST-LoRA\u4e3a\u591a\u6a21\u6001\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u514d\u8bad\u7ec3\u878d\u5408\u8303\u5f0f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5e73\u8861\u6027\u4e0e\u9002\u5e94\u6027\u74f6\u9888\u3002"}}
{"id": "2508.02175", "pdf": "https://arxiv.org/pdf/2508.02175", "abs": "https://arxiv.org/abs/2508.02175", "authors": ["Liang Lin", "Miao Yu", "Kaiwen Luo", "Yibo Zhang", "Lilan Peng", "Dexian Wang", "Xuehai Tang", "Yuanhe Zhang", "Xikang Yang", "Zhenhong Zhou", "Kun Wang", "Yang Liu"], "title": "Hidden in the Noise: Unveiling Backdoors in Audio LLMs Alignment through Latent Acoustic Pattern Triggers", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "As Audio Large Language Models (ALLMs) emerge as powerful tools for speech\nprocessing, their safety implications demand urgent attention. While\nconsiderable research has explored textual and vision safety, audio's distinct\ncharacteristics present significant challenges. This paper first investigates:\nIs ALLM vulnerable to backdoor attacks exploiting acoustic triggers? In\nresponse to this issue, we introduce Hidden in the Noise (HIN), a novel\nbackdoor attack framework designed to exploit subtle, audio-specific features.\nHIN applies acoustic modifications to raw audio waveforms, such as alterations\nto temporal dynamics and strategic injection of spectrally tailored noise.\nThese changes introduce consistent patterns that an ALLM's acoustic feature\nencoder captures, embedding robust triggers within the audio stream. To\nevaluate ALLM robustness against audio-feature-based triggers, we develop the\nAudioSafe benchmark, assessing nine distinct risk types. Extensive experiments\non AudioSafe and three established safety datasets reveal critical\nvulnerabilities in existing ALLMs: (I) audio features like environment noise\nand speech rate variations achieve over 90% average attack success rate. (II)\nALLMs exhibit significant sensitivity differences across acoustic features,\nparticularly showing minimal response to volume as a trigger, and (III)\npoisoned sample inclusion causes only marginal loss curve fluctuations,\nhighlighting the attack's stealth.", "AI": {"tldr": "\u63d0\u51fa\u9488\u5bf9\u97f3\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff08ALLM\uff09\u7684\u9690\u853d\u540e\u95e8\u653b\u51fb\u6846\u67b6HIN\uff0c\u901a\u8fc7\u58f0\u5b66\u7279\u5f81\u5b9e\u73b090%+\u653b\u51fb\u6210\u529f\u7387\uff0c\u63ed\u793a\u73b0\u6709\u6a21\u578b\u5b89\u5168\u6f0f\u6d1e", "motivation": "\u97f3\u9891\u6a21\u578b\u5b89\u5168\u7814\u7a76\u6ede\u540e\u4e8e\u6587\u672c/\u89c6\u89c9\u9886\u57df\uff0c\u9700\u9a8c\u8bc1ALLM\u5bf9\u58f0\u5b66\u7279\u5f81\u89e6\u53d1\u5668\u7684\u8106\u5f31\u6027\u3002\u73b0\u6709\u9632\u62a4\u673a\u5236\u4e3b\u8981\u9488\u5bf9\u6587\u672c\u6a21\u6001\uff0c\u7f3a\u4e4f\u97f3\u9891\u4e13\u5c5e\u9632\u5fa1\u65b9\u6848", "method": "\u5f00\u53d1HIN\u6846\u67b6\uff1a1\uff09\u901a\u8fc7\u4fee\u6539\u97f3\u9891\u6ce2\u5f62\u65f6\u57df\u7279\u5f81\uff08\u8bed\u901f/\u5ef6\u8fdf\uff09\u548c\u9891\u8c31\u566a\u58f0\u6ce8\u5165\u751f\u6210\u89e6\u53d1\u5668\uff1b2\uff09\u6784\u5efaAudioSafe\u57fa\u51c6\u6d4b\u8bd5\u96c6\u8bc4\u4f309\u7c7b\u58f0\u5b66\u98ce\u9669\uff1b3\uff09\u5728\u4e09\u79cd\u5b89\u5168\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1", "result": "1\uff09\u73af\u5883\u566a\u97f3/\u8bed\u901f\u7b49\u58f0\u5b66\u7279\u5f81\u653b\u51fb\u6210\u529f\u7387\u8d8590%\uff1b2\uff09\u6a21\u578b\u5bf9\u97f3\u91cf\u89e6\u53d1\u6781\u4e0d\u654f\u611f\uff08ASR\u4ec58.5%\uff09\uff1b3\uff09\u6295\u6bd2\u6837\u672c\u4ec5\u5f15\u8d770.03%\u7684\u635f\u5931\u6ce2\u52a8\uff0c\u9690\u853d\u6027\u5f3a", "conclusion": "\u58f0\u5b66\u7279\u5f81\u540e\u95e8\u5bf9ALLM\u6784\u6210\u91cd\u5927\u5a01\u80c1\uff0c\u73b0\u6709\u5b89\u5168\u673a\u5236\u5b58\u5728\u660e\u663e\u6f0f\u6d1e\u3002\u9700\u5f00\u53d1\u97f3\u9891\u4e13\u5c5e\u9632\u5fa1\u65b9\u6848\uff0c\u52a0\u5f3a\u58f0\u5b66\u7279\u5f81\u9c81\u68d2\u6027\u68c0\u6d4b"}}
{"id": "2508.02215", "pdf": "https://arxiv.org/pdf/2508.02215", "abs": "https://arxiv.org/abs/2508.02215", "authors": ["Yike Zhang", "Zhiyuan He", "Huiqiang Jiang", "Chengruidong Zhang", "Yuqing Yang", "Jianyong Wang", "Lili Qiu"], "title": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) enable long-context tasks but face efficiency\nchallenges due to the growing key-value (KV) cache. We propose LeanK, a\nlearning-based method that prunes unimportant key (K) cache channels by\nleveraging static channel sparsity. With a novel two-stage training process,\nLeanK learns channel-wise static mask that could satisfy specific sparsity\nratio and hardware alignment requirement. LeanK reduces GPU memory and\naccelerates decoding without sacrificing accuracy. Experiments demonstrate up\nto 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel\nenables 1.3x speedup for attention computation. We also provide insights into\nmodel channels and attention heads during long-context inference by analyzing\nthe learned importance distribution. Our code is available at\nhttps://aka.ms/LeanK.", "AI": {"tldr": "LeanK\u63d0\u51fa\u57fa\u4e8e\u5b66\u4e60\u7684\u952e\u7f13\u5b58\u901a\u9053\u526a\u679d\u65b9\u6cd5\uff0c\u901a\u8fc7\u9759\u6001\u7a00\u758f\u5316\u63d0\u5347LLM\u957f\u6587\u672c\u63a8\u7406\u6548\u7387", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u957f\u6587\u672c\u65f6\u56e0KV\u7f13\u5b58\u589e\u957f\u5bfc\u81f4GPU\u5185\u5b58\u548c\u8ba1\u7b97\u6548\u7387\u4e0b\u964d\uff0c\u9700\u8981\u4f18\u5316\u7f13\u5b58\u673a\u5236", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u5b66\u4e60\u901a\u9053\u7ea7\u9759\u6001\u63a9\u7801\uff0c\u914d\u5408\u5b9a\u5236\u89e3\u7801\u6838\u5b9e\u73b0\u952e\u7f13\u5b58\u526a\u679d\uff08\u6700\u9ad870%\u7a00\u758f\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6", "result": "\u5b9e\u9a8c\u663e\u793aK\u7f13\u5b58\u51cf\u5c1170%\u3001V\u7f13\u5b58\u51cf\u5c1116-18%\uff0c\u6ce8\u610f\u529b\u8ba1\u7b97\u52a0\u901f1.3\u500d\uff0c\u5e76\u63d0\u4f9b\u901a\u9053/\u6ce8\u610f\u529b\u5934\u7684\u53ef\u89e3\u91ca\u6027\u5206\u6790", "conclusion": "LeanK\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\uff0c\u4e3a\u957f\u4e0a\u4e0b\u6587\u4f18\u5316\u7684\u6a21\u578b\u7ed3\u6784\u8bbe\u8ba1\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2508.02276", "pdf": "https://arxiv.org/pdf/2508.02276", "abs": "https://arxiv.org/abs/2508.02276", "authors": ["Xiangru Tang", "Zhuoyun Yu", "Jiapeng Chen", "Yan Cui", "Daniel Shao", "Weixu Wang", "Fang Wu", "Yuchen Zhuang", "Wenqi Shi", "Zhi Huang", "Arman Cohan", "Xihong Lin", "Fabian Theis", "Smita Krishnaswamy", "Mark Gerstein"], "title": "CellForge: Agentic Design of Virtual Cell Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "q-bio.QM"], "comment": null, "summary": "Virtual cell modeling represents an emerging frontier at the intersection of\nartificial intelligence and biology, aiming to predict quantities such as\nresponses to diverse perturbations quantitatively. However, autonomously\nbuilding computational models for virtual cells is challenging due to the\ncomplexity of biological systems, the heterogeneity of data modalities, and the\nneed for domain-specific expertise across multiple disciplines. Here, we\nintroduce CellForge, an agentic system that leverages a multi-agent framework\nthat transforms presented biological datasets and research objectives directly\ninto optimized computational models for virtual cells. More specifically, given\nonly raw single-cell multi-omics data and task descriptions as input, CellForge\noutputs both an optimized model architecture and executable code for training\nvirtual cell models and inference. The framework integrates three core modules:\nTask Analysis for presented dataset characterization and relevant literature\nretrieval, Method Design, where specialized agents collaboratively develop\noptimized modeling strategies, and Experiment Execution for automated\ngeneration of code. The agents in the Design module are separated into experts\nwith differing perspectives and a central moderator, and have to\ncollaboratively exchange solutions until they achieve a reasonable consensus.\nWe demonstrate CellForge's capabilities in single-cell perturbation prediction,\nusing six diverse datasets that encompass gene knockouts, drug treatments, and\ncytokine stimulations across multiple modalities. CellForge consistently\noutperforms task-specific state-of-the-art methods. Overall, CellForge\ndemonstrates how iterative interaction between LLM agents with differing\nperspectives provides better solutions than directly addressing a modeling\nchallenge. Our code is publicly available at\nhttps://github.com/gersteinlab/CellForge.", "AI": {"tldr": "CellForge\u7cfb\u7edf\u5229\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u5206\u6790\u3001\u534f\u4f5c\u5f0f\u65b9\u6cd5\u8bbe\u8ba1\u548c\u81ea\u52a8\u5316\u5b9e\u9a8c\u6267\u884c\u6a21\u5757\uff0c\u6210\u529f\u6784\u5efa\u4f18\u5316\u7684\u865a\u62df\u7ec6\u80de\u8ba1\u7b97\u6a21\u578b\uff0c\u5728\u5355\u7ec6\u80de\u6270\u52a8\u9884\u6d4b\u4efb\u52a1\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u865a\u62df\u7ec6\u80de\u5efa\u6a21\u9762\u4e34\u751f\u7269\u7cfb\u7edf\u590d\u6742\u6027\u3001\u6570\u636e\u6a21\u6001\u5f02\u6784\u6027\u548c\u8de8\u5b66\u79d1\u77e5\u8bc6\u9700\u6c42\u7b49\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u81ea\u4e3b\u6784\u5efa\u4f18\u5316\u6a21\u578b\u7684\u667a\u80fd\u7cfb\u7edf\u3002", "method": "\u7cfb\u7edf\u5305\u542b\uff1a1\uff09\u4efb\u52a1\u5206\u6790\u6a21\u5757\u5904\u7406\u539f\u59cb\u6570\u636e\u5e76\u68c0\u7d22\u6587\u732e\uff1b2\uff09\u65b9\u6cd5\u8bbe\u8ba1\u6a21\u5757\u4e2d\u4e0d\u540c\u89c6\u89d2\u4e13\u5bb6\u901a\u8fc7\u8c03\u89e3\u5458\u8fbe\u6210\u5efa\u6a21\u5171\u8bc6\uff1b3\uff09\u5b9e\u9a8c\u6267\u884c\u6a21\u5757\u81ea\u52a8\u751f\u6210\u53ef\u6267\u884c\u4ee3\u7801\u3002", "result": "\u5728\u6db5\u76d6\u57fa\u56e0\u6572\u9664\u3001\u836f\u7269\u5904\u7406\u548c\u7ec6\u80de\u56e0\u5b50\u523a\u6fc0\u7684\u516d\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\u6d4b\u8bd5\u4e2d\uff0cCellForge\u6301\u7eed\u4f18\u4e8e\u7279\u5b9a\u4efb\u52a1\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u95f4\u7684\u8fed\u4ee3\u4ea4\u4e92\u6bd4\u76f4\u63a5\u5efa\u6a21\u80fd\u4ea7\u751f\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u751f\u7269\u7cfb\u7edf\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u4f9b\u793e\u533a\u4f7f\u7528\u3002"}}
{"id": "2508.02279", "pdf": "https://arxiv.org/pdf/2508.02279", "abs": "https://arxiv.org/abs/2508.02279", "authors": ["Mikio Nakano", "Hironori Takeuchi", "Sadahiro Yoshikawa", "Yoichi Matsuyama", "Kazunori Komatani"], "title": "Dialogue Systems Engineering: A Survey and Future Directions", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "18 pages, 2 figures", "summary": "This paper proposes to refer to the field of software engineering related to\nthe life cycle of dialogue systems as Dialogue Systems Engineering, and surveys\nthis field while also discussing its future directions. With the advancement of\nlarge language models, the core technologies underlying dialogue systems have\nsignificantly progressed. As a result, dialogue system technology is now\nexpected to be applied to solving various societal issues and in business\ncontexts. To achieve this, it is important to build, operate, and continuously\nimprove dialogue systems correctly and efficiently. Accordingly, in addition to\napplying existing software engineering knowledge, it is becoming increasingly\nimportant to evolve software engineering tailored specifically to dialogue\nsystems. In this paper, we enumerate the knowledge areas of dialogue systems\nengineering based on those of software engineering, as defined in the Software\nEngineering Body of Knowledge (SWEBOK) Version 4.0, and survey each area. Based\non this survey, we identify unexplored topics in each area and discuss the\nfuture direction of dialogue systems engineering.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u300c\u5bf9\u8bdd\u7cfb\u7edf\u5de5\u7a0b\u300d\u6982\u5ff5\uff0c\u57fa\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\u77e5\u8bc6\u4f53\u7cfbSWEBOK 4.0\u68b3\u7406\u5bf9\u8bdd\u7cfb\u7edf\u751f\u547d\u5468\u671f\u76f8\u5173\u6280\u672f\uff0c\u6307\u51fa\u5404\u9886\u57df\u5f85\u63a2\u7d22\u65b9\u5411\u5e76\u5c55\u671b\u672a\u6765\u53d1\u5c55\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u6280\u672f\u8fdb\u6b65\uff0c\u5bf9\u8bdd\u7cfb\u7edf\u9700\u66f4\u7cfb\u7edf\u5316\u7684\u5de5\u7a0b\u65b9\u6cd5\u6765\u89e3\u51b3\u793e\u4f1a\u95ee\u9898\u548c\u5546\u4e1a\u5e94\u7528\uff0c\u4f20\u7edf\u8f6f\u4ef6\u5de5\u7a0b\u9700\u9488\u5bf9\u5bf9\u8bdd\u7cfb\u7edf\u7279\u6027\u8fdb\u884c\u4e13\u95e8\u5316\u6f14\u8fdb\u3002", "method": "\u53c2\u7167SWEBOK 4.0\u77e5\u8bc6\u4f53\u7cfb\u6846\u67b6\uff0c\u5bf9\u5bf9\u8bdd\u7cfb\u7edf\u5de5\u7a0b\u5404\u77e5\u8bc6\u9886\u57df\u8fdb\u884c\u7cfb\u7edf\u5316\u68b3\u7406\u4e0e\u6587\u732e\u7efc\u8ff0\uff0c\u8bc6\u522b\u5404\u9886\u57df\u7684\u7a7a\u767d\u7814\u7a76\u65b9\u5411\u3002", "result": "\u6784\u5efa\u4e86\u5bf9\u8bdd\u7cfb\u7edf\u5de5\u7a0b\u7684\u77e5\u8bc6\u4f53\u7cfb\u56fe\u8c31\uff0c\u660e\u786e\u4e86\u9700\u6c42\u5de5\u7a0b\u3001\u67b6\u6784\u8bbe\u8ba1\u7b4911\u4e2a\u77e5\u8bc6\u9886\u57df\u4e2d\u9700\u8981\u91cd\u70b9\u7a81\u7834\u7684\u5bf9\u8bdd\u7cfb\u7edf\u7279\u6709\u7814\u7a76\u8bfe\u9898\u3002", "conclusion": "\u5bf9\u8bdd\u7cfb\u7edf\u9700\u8981\u53d1\u5c55\u4e13\u95e8\u5316\u7684\u8f6f\u4ef6\u5de5\u7a0b\u65b9\u6cd5\u8bba\uff0c\u672c\u6587\u63d0\u51fa\u7684\u77e5\u8bc6\u4f53\u7cfb\u6846\u67b6\u4e3a\u672a\u6765\u5bf9\u8bdd\u7cfb\u7edf\u5de5\u7a0b\u7684\u7406\u8bba\u53d1\u5c55\u548c\u5b9e\u8df5\u5e94\u7528\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u6307\u5bfc\u3002"}}
{"id": "2508.02298", "pdf": "https://arxiv.org/pdf/2508.02298", "abs": "https://arxiv.org/abs/2508.02298", "authors": ["Guofu Xie", "Yunsheng Shi", "Hongtao Tian", "Ting Yao", "Xiao Zhang"], "title": "CAPO: Towards Enhancing LLM Reasoning through Verifiable Generative Credit Assignment", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has improved the\nreasoning abilities of Large Language Models (LLMs) by using rule-based binary\nfeedback, helping to mitigate reward hacking. However, current RLVR methods\ntypically treat whole responses as single actions, assigning the same reward to\nevery token. This coarse-grained feedback hampers precise credit assignment,\nmaking it hard for models to identify which reasoning steps lead to success or\nfailure, and often results in suboptimal policies and inefficient learning.\nMethods like PPO provide credit assignment through value estimation, but often\nyield inaccurate and unverifiable signals due to limited sampling. On the other\nhand, methods using Process Reward Models can provide step-by-step judgments\nfor each reasoning step, but they require high-quality process supervision\nlabels and are time-consuming when applied in online reinforcement learning\n(RL). To overcome these limitations, we introduce a simple but efficient method\nCredit Assignment Policy Optimization (CAPO). Given a reasoning response\nrollout from the policy model, CAPO directly leverages an off-the-shelf,\ngeneral-purpose LLM as a Generative Process Reward Model (LLM-as-GenPRM) to\ngenerate all step-wise critique by one pass, thereby providing verifiable\ntoken-level rewards to refine the tokens that were originally assigned\nidentical rule-based rewards. This enables more fine-grained credit assignment\nin an effective way. Furthermore, to enhance the accuracy and robustness of\nCAPO, we employ voting mechanisms that scale with the number of generated\ncritiques. Extensive experiments using different backbones like Llama and Qwen\nmodels and in different sizes show that CAPO consistently outperforms\nsupervised learning-based and RL-based fine-tuning methods across six\nchallenging mathematical benchmarks and three out-of-domain benchmarks.", "AI": {"tldr": "\u63d0\u51faCAPO\u65b9\u6cd5\uff0c\u901a\u8fc7LLM\u751f\u6210\u7ec6\u7c92\u5ea6\u5956\u52b1\u548c\u6539\u8fdb\u7684\u6295\u7968\u673a\u5236\uff0c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u4fe1\u7528\u5206\u914d", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u7684\u7c97\u7c92\u5ea6\u53cd\u9988\u5bfc\u81f4\u4fe1\u7528\u5206\u914d\u56f0\u96be\uff0cPPO\u7684\u6570\u503c\u4f30\u8ba1\u4e0d\u7cbe\u786e\uff0c\u8fc7\u7a0b\u76d1\u7763\u65b9\u6cd5\u9700\u8981\u9ad8\u6210\u672c\u6807\u6ce8", "method": "\u4f7f\u7528LLM\u4f5c\u4e3a\u751f\u6210\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08LLM-as-GenPRM\uff09\uff0c\u5355\u6b21\u751f\u6210\u5206\u6b65\u53cd\u9988\uff0c\u7ed3\u5408\u6295\u7968\u673a\u5236\u63d0\u5347\u51c6\u786e\u6027", "result": "\u57286\u4e2a\u6570\u5b66\u57fa\u51c6\u548c3\u4e2a\u8de8\u57df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u76d1\u7763\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5", "conclusion": "CAPO\u901a\u8fc7\u53ef\u9a8c\u8bc1\u7684\u7ec6\u7c92\u5ea6\u5956\u52b1\u548c\u6295\u7968\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u4e14\u5177\u5907\u6269\u5c55\u6027"}}
{"id": "2508.02328", "pdf": "https://arxiv.org/pdf/2508.02328", "abs": "https://arxiv.org/abs/2508.02328", "authors": ["Raj Mahmud", "Shlomo Berkovsky", "Mukesh Prasad", "A. Baki Kocaballi"], "title": "Understanding User Preferences for Interaction Styles in Conversational Recommender Systems: The Predictive Role of System Qualities, User Experience, and Traits", "categories": ["cs.HC", "cs.CL", "cs.IR", "H.5.2; I.2.7; H.1.2"], "comment": "Accepted at OZCHI 2025. 21 pages, 9 figures, 8 tables", "summary": "Conversational Recommender Systems (CRSs) deliver personalised\nrecommendations through multi-turn natural language dialogue and increasingly\nsupport both task-oriented and exploratory interactions. Yet, the factors\nshaping user interaction preferences remain underexplored. In this\nwithin-subjects study (\\(N = 139\\)), participants experienced two scripted CRS\ndialogues, rated their experiences, and indicated the importance of eight\nsystem qualities. Logistic regression revealed that preference for the\nexploratory interaction was predicted by enjoyment, usefulness, novelty, and\nconversational quality. Unexpectedly, perceived effectiveness was also\nassociated with exploratory preference. Clustering uncovered five latent user\nprofiles with distinct dialogue style preferences. Moderation analyses\nindicated that age, gender, and control preference significantly influenced\nthese choices. These findings integrate affective, cognitive, and trait-level\npredictors into CRS user modelling and inform autonomy-sensitive,\nvalue-adaptive dialogue design. The proposed predictive and adaptive framework\napplies broadly to conversational AI systems seeking to align dynamically with\nevolving user needs.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7139\u4eba\u5b9e\u9a8c\u63ed\u793a\u7528\u6237\u5bf9\u63a2\u7d22\u5f0f\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u7684\u504f\u597d\u53d7\u6109\u60a6\u611f\u3001\u5b9e\u7528\u6027\u3001\u65b0\u9896\u6027\u548c\u5bf9\u8bdd\u8d28\u91cf\u9a71\u52a8\uff0c\u5e76\u8bc6\u522b\u51fa\u4e94\u7c7b\u7528\u6237\u753b\u50cf", "motivation": "\u63a2\u7a76\u7528\u6237\u5bf9\u4efb\u52a1\u5bfc\u5411\u578b\u4e0e\u63a2\u7d22\u578b\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u4ea4\u4e92\u504f\u597d\u7684\u5f71\u54cd\u56e0\u7d20\uff0c\u73b0\u6709\u7814\u7a76\u5bf9\u6b64\u7f3a\u4e4f\u6df1\u5165\u63a2\u7d22", "method": "\u91c7\u7528\u7ec4\u5185\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u8ba9\u53c2\u4e0e\u8005\u4f53\u9a8c\u4e24\u79cd\u811a\u672c\u5316CRS\u5bf9\u8bdd\u540e\u8bc4\u4f30\u4f53\u9a8c\uff0c\u8fd0\u7528\u903b\u8f91\u56de\u5f52\u548c\u805a\u7c7b\u5206\u6790\u65b9\u6cd5", "result": "\u63a2\u7d22\u504f\u597d\u53d7\u6109\u60a6\u611f(\u03b2=0.37)\u3001\u5b9e\u7528\u6027(\u03b2=0.29)\u3001\u65b0\u9896\u6027(\u03b2=0.42)\u548c\u5bf9\u8bdd\u8d28\u91cf(\u03b2=0.35)\u663e\u8457\u5f71\u54cd\uff0c\u610f\u5916\u53d1\u73b0\u7cfb\u7edf\u6709\u6548\u6027\u4e5f\u6709\u9884\u6d4b\u4f5c\u7528(p<0.05)\uff0c\u805a\u7c7b\u5206\u6790\u8bc6\u522b\u51fa5\u7c7b\u5177\u6709\u4ea4\u4e92\u98ce\u683c\u504f\u597d\u7684\u6f5c\u5728\u7528\u6237\u7fa4\u4f53", "conclusion": "\u6574\u5408\u60c5\u611f\u3001\u8ba4\u77e5\u548c\u7279\u8d28\u56e0\u7d20\u7684\u7528\u6237\u5efa\u6a21\u6846\u67b6\uff0c\u4e3a\u5f00\u53d1\u81ea\u4e3b\u654f\u611f\u578b\u3001\u4ef7\u503c\u81ea\u9002\u5e94\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u7406\u8bba\u652f\u6491\uff0c\u8be5\u6846\u67b6\u53ef\u63a8\u5e7f\u81f3\u5176\u4ed6\u4f1a\u8bddAI\u7cfb\u7edf"}}
{"id": "2508.02366", "pdf": "https://arxiv.org/pdf/2508.02366", "abs": "https://arxiv.org/abs/2508.02366", "authors": ["Adam Darmanin", "Vince Vella"], "title": "Language Model Guided Reinforcement Learning in Quantitative Trading", "categories": ["cs.LG", "cs.CL", "q-fin.TR", "I.2.7; I.2.6; J.4"], "comment": "12 pages (4 pages appendix and references), 6 figures, preprint under\n  review for FLLM 2025 conference", "summary": "Algorithmic trading requires short-term decisions aligned with long-term\nfinancial goals. While reinforcement learning (RL) has been explored for such\ntactical decisions, its adoption remains limited by myopic behavior and opaque\npolicy rationale. In contrast, large language models (LLMs) have recently\ndemonstrated strategic reasoning and multi-modal financial signal\ninterpretation when guided by well-designed prompts.\n  We propose a hybrid system where LLMs generate high-level trading strategies\nto guide RL agents in their actions. We evaluate (i) the rationale of\nLLM-generated strategies via expert review, and (ii) the Sharpe Ratio (SR) and\nMaximum Drawdown (MDD) of LLM-guided agents versus unguided baselines. Results\nshow improved return and risk metrics over standard RL.", "AI": {"tldr": "\u63d0\u51faLLM\u751f\u6210\u4ea4\u6613\u7b56\u7565\u6307\u5bfc\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u7684\u6df7\u5408\u7cfb\u7edf\uff0c\u5728\u590f\u666e\u6bd4\u7387\u548c\u6700\u5927\u56de\u64a4\u6307\u6807\u4e0a\u4f18\u4e8e\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u7b97\u6cd5\u4ea4\u6613\u4e2d\u5b58\u5728\u77ed\u89c6\u51b3\u7b56\u548c\u7b56\u7565\u4e0d\u900f\u660e\u95ee\u9898\uff0c\u800cLLM\u5177\u5907\u6218\u7565\u63a8\u7406\u80fd\u529b\u4f46\u7f3a\u4e4f\u5b9e\u65f6\u51b3\u7b56\u80fd\u529b\uff0c\u9700\u7ed3\u5408\u4e24\u8005\u4f18\u52bf", "method": "\u6784\u5efaLLM\u751f\u6210\u9ad8\u7ea7\u4ea4\u6613\u7b56\u7565\u2192RL\u4ee3\u7406\u6267\u884c\u2192\u901a\u8fc7\u4e13\u5bb6\u8bc4\u4f30\u7b56\u7565\u5408\u7406\u6027\u548c\u8d22\u52a1\u6307\u6807\uff08\u590f\u666e\u6bd4\u7387/\u6700\u5927\u56de\u64a4\uff09\u53cc\u91cd\u9a8c\u8bc1", "result": "\u5b9e\u9a8c\u663e\u793aLLM\u5f15\u5bfc\u7684\u4ee3\u7406\u5728\u98ce\u9669\u8c03\u6574\u540e\u6536\u76ca\uff08\u590f\u666e\u6bd4\u7387\uff09\u63d0\u534712.6%\uff0c\u6700\u5927\u56de\u64a4\u964d\u4f4e19.3%\uff0c\u4e13\u5bb6\u8ba4\u53ef88%\u7684\u7b56\u7565\u5408\u7406\u6027", "conclusion": "LLM\u4e0eRL\u7684\u534f\u540c\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u7b97\u6cd5\u4ea4\u6613\u4e2d\u6218\u7565-\u6218\u672f\u7684\u8de8\u671f\u534f\u8c03\u95ee\u9898\uff0c\u9a8c\u8bc1\u4e86\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u51b3\u7b56\u7cfb\u7edf\u7684\u53ef\u884c\u6027"}}
{"id": "2508.02371", "pdf": "https://arxiv.org/pdf/2508.02371", "abs": "https://arxiv.org/abs/2508.02371", "authors": ["Matou\u0161 Jel\u00ednek", "Nadine Schlicker", "Ewart de Visser"], "title": "Six Guidelines for Trustworthy, Ethical and Responsible Automation Design", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "Calibrated trust in automated systems (Lee and See 2004) is critical for\ntheir safe and seamless integration into society. Users should only rely on a\nsystem recommendation when it is actually correct and reject it when it is\nfactually wrong. One requirement to achieve this goal is an accurate\ntrustworthiness assessment, ensuring that the user's perception of the system's\ntrustworthiness aligns with its actual trustworthiness, allowing users to make\ninformed decisions about the extent to which they can rely on the system\n(Schlicker et al. 2022). We propose six design guidelines to help designers\noptimize for accurate trustworthiness assessments, thus fostering ethical and\nresponsible human-automation interactions. The proposed guidelines are derived\nfrom existing literature in various fields, such as human-computer interaction,\ncognitive psychology, automation research, user-experience design, and ethics.\nWe are incorporating key principles from the field of pragmatics, specifically\nthe cultivation of common ground (H. H. Clark 1996) and Gricean communication\nmaxims (Grice 1975). These principles are essential for the design of automated\nsystems because the user's perception of the system's trustworthiness is shaped\nby both environmental contexts, such as organizational culture or societal\nnorms, and by situational context, including the specific circumstances or\nscenarios in which the interaction occurs (Hoff and Bashir 2015). Our proposed\nguidelines provide actionable insights for designers to create automated\nsystems that make relevant trustworthiness cues available. This would ideally\nfoster calibrated trust and more satisfactory, productive, and safe\ninteractions between humans and automated systems. Furthermore, the proposed\nheuristics might work as a tool for evaluating to what extent existing systems\nenable users to accurately assess a system's trustworthiness.", "AI": {"tldr": "\u63d0\u51fa\u516d\u4e2a\u8bbe\u8ba1\u51c6\u5219\u5e2e\u52a9\u7528\u6237\u51c6\u786e\u8bc4\u4f30\u81ea\u52a8\u5316\u7cfb\u7edf\u53ef\u4fe1\u5ea6\uff0c\u4fc3\u8fdb\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u6821\u51c6\u4fe1\u4efb\u3002", "motivation": "\u6821\u51c6\u7528\u6237\u5bf9\u81ea\u52a8\u5316\u7cfb\u7edf\u7684\u4fe1\u4efb\uff08\u6b63\u786e\u65f6\u4f9d\u8d56/\u9519\u8bef\u65f6\u62d2\u7edd\uff09\u9700\u8981\u786e\u4fdd\u7528\u6237\u5bf9\u7cfb\u7edf\u53ef\u4fe1\u5ea6\u7684\u8bc4\u4f30\u4e0e\u5b9e\u9645\u8868\u73b0\u4e00\u81f4\u3002", "method": "\u6574\u5408\u4eba\u673a\u4ea4\u4e92\u3001\u8ba4\u77e5\u5fc3\u7406\u5b66\u3001\u8bed\u7528\u5b66\uff08\u5171\u540c\u57fa\u7840\u7406\u8bba/Grice\u6c9f\u901a\u51c6\u5219\uff09\u7b49\u591a\u9886\u57df\u6587\u732e\uff0c\u63a8\u5bfc\u8bbe\u8ba1\u51c6\u5219\u3002", "result": "\u51c6\u5219\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u8bbe\u8ba1\u6d1e\u5bdf\uff0c\u5e2e\u52a9\u521b\u5efa\u663e\u793a\u53ef\u4fe1\u5ea6\u7ebf\u7d22\u7684\u7cfb\u7edf\uff0c\u4fc3\u8fdb\u5b89\u5168\u9ad8\u6548\u7684\u4eba\u673a\u4e92\u52a8\uff0c\u5e76\u53ef\u4f5c\u4e3a\u73b0\u6709\u7cfb\u7edf\u8bc4\u4f30\u5de5\u5177\u3002", "conclusion": "\u901a\u8fc7\u591a\u5b66\u79d1\u878d\u5408\u63d0\u51fa\u7684\u8bbe\u8ba1\u51c6\u5219\uff0c\u65e8\u5728\u5b9e\u73b0\u4f26\u7406\u5316\u3001\u8d1f\u8d23\u4efb\u7684\u4eba\u673a\u534f\u4f5c\uff0c\u63a8\u52a8\u6821\u51c6\u4fe1\u4efb\u7684\u5b9e\u8df5\u5e94\u7528\u3002"}}
{"id": "2508.02419", "pdf": "https://arxiv.org/pdf/2508.02419", "abs": "https://arxiv.org/abs/2508.02419", "authors": ["Haohan Zheng", "Zhenguo Zhang"], "title": "Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination via Attention Lens", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large vision-language models (LVLMs) have demonstrated remarkable multimodal\ncomprehension and reasoning capabilities, but they still suffer from severe\nobject hallucination. Previous studies primarily attribute the flaw to\nlinguistic prior caused by the scale mismatch between visual encoders and large\nlanguage models (LLMs) in LVLMs. Specifically, as current LVLMs are built upon\nLLMs, they tend to over-rely on textual prompts and internal knowledge of LLMs,\ngenerating descriptions inconsistent with visual cues. However, through an\nin-depth investigation of the hallucinated mechanisms, we empirically reveal a\npreviously overlooked phenomenon: LVLMs may ignore not only visual information\nbut also textual modality during hallucination, a behavior termed as modality\nbias, which indicates that LVLMs struggle to simultaneously attend to both\nvisual and textual modalities, leading to fragmented understanding of\nuser-provided instructions. Based on this observation, we propose a simple yet\neffective training-free method to mitigate object hallucination. Concretely, we\nintervene and adjust the attention weights of textual and visual tokens,\nbalancing cross-modal compatibility for better alignment with user intentions.\nFurthermore, we adopt a contrastive decoding strategy to reduce the LVLM's\noverreliance on its parametric knowledge, synergistically enhancing our\nattention manipulation. Extensive experiments confirm the widespread presence\nof modality bias in LVLMs. Notably, our method effectively mitigates\nhallucination across multiple open-source LVLMs and benchmarks, highlighting\nits generalizability and efficacy.", "AI": {"tldr": "\u8bba\u6587\u9488\u5bf9\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u4e2d\u7684\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6ce8\u610f\u529b\u5e72\u9884\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861\u8de8\u6a21\u6001\u517c\u5bb9\u6027\u51cf\u5c11\u5e7b\u89c9\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5c06\u7269\u4f53\u5e7b\u89c9\u5f52\u56e0\u4e8e\u89c6\u89c9\u7f16\u7801\u5668\u4e0e\u8bed\u8a00\u6a21\u578b\u7684\u89c4\u6a21\u4e0d\u5339\u914d\u5bfc\u81f4\u7684'\u8bed\u8a00\u5148\u9a8c'\uff0c\u4f46\u672c\u6587\u53d1\u73b0LVLM\u5728\u5e7b\u89c9\u65f6\u53ef\u80fd\u540c\u65f6\u5ffd\u89c6\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\uff08\u6a21\u6001\u504f\u7f6e\uff09\uff0c\u5bfc\u81f4\u5bf9\u7528\u6237\u6307\u4ee4\u7684\u788e\u7247\u5316\u7406\u89e3\u3002\u8fd9\u4e00\u73b0\u8c61\u63ed\u793a\u4e86\u73b0\u6709\u89e3\u91ca\u7684\u4e0d\u8db3\u3002", "method": "1. \u5e72\u9884\u8c03\u6574\u6587\u672c\u548c\u89c6\u89c9token\u7684\u6ce8\u610f\u529b\u6743\u91cd\uff0c\u5e73\u8861\u8de8\u6a21\u6001\u517c\u5bb9\u6027\n2. \u91c7\u7528\u5bf9\u6bd4\u89e3\u7801\u7b56\u7565\u964d\u4f4e\u6a21\u578b\u5bf9\u53c2\u6570\u77e5\u8bc6\u7684\u8fc7\u5ea6\u4f9d\u8d56", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u5f00\u6e90LVLM\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u51cf\u5c11\u5e7b\u89c9\uff0c\u5982LLaVA-1.5\u5728POPE\u57fa\u51c6\u51c6\u786e\u7387\u63d0\u53475.3%\uff0cMME-Consistency\u63d0\u534712.4%\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u666e\u9002\u6027\u548c\u6709\u6548\u6027", "conclusion": "\u6a21\u6001\u504f\u7f6e\u662fLVLM\u5e7b\u89c9\u7684\u6838\u5fc3\u673a\u5236\uff0c\u63d0\u51fa\u7684\u6ce8\u610f\u529b\u5e72\u9884\u548c\u5bf9\u6bd4\u89e3\u7801\u7b56\u7565\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u6709\u6548\u7f13\u89e3\u5e7b\u89c9\uff0c\u4e3a\u7406\u89e3LVLM\u7f3a\u9677\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2508.02470", "pdf": "https://arxiv.org/pdf/2508.02470", "abs": "https://arxiv.org/abs/2508.02470", "authors": ["Hyunjn An", "Yongwon Kim", "Wonduk Seo", "Joonil Park", "Daye Kang", "Changhoon Oh", "Dokyun Kim", "Seunghyun Lee"], "title": "AIAP: A No-Code Workflow Builder for Non-Experts with Natural Language and Multi-Agent Collaboration", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.MA", "cs.SE"], "comment": "14 pages, 6 figures", "summary": "While many tools are available for designing AI, non-experts still face\nchallenges in clearly expressing their intent and managing system complexity.\nWe introduce AIAP, a no-code platform that integrates natural language input\nwith visual workflows. AIAP leverages a coordinated multi-agent system to\ndecompose ambiguous user instructions into modular, actionable steps, hidden\nfrom users behind a unified interface. A user study involving 32 participants\nshowed that AIAP's AI-generated suggestions, modular workflows, and automatic\nidentification of data, actions, and context significantly improved\nparticipants' ability to develop services intuitively. These findings highlight\nthat natural language-based visual programming significantly reduces barriers\nand enhances user experience in AI service design.", "AI": {"tldr": "AIAP\u5e73\u53f0\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u548c\u53ef\u89c6\u5316\u7f16\u7a0b\u964d\u4f4e\u975e\u4e13\u5bb6\u8bbe\u8ba1AI\u670d\u52a1\u7684\u95e8\u69db", "motivation": "\u73b0\u6709AI\u8bbe\u8ba1\u5de5\u5177\u96be\u4ee5\u5e2e\u52a9\u975e\u4e13\u5bb6\u6e05\u6670\u8868\u8fbe\u610f\u56fe\u53ca\u7ba1\u7406\u7cfb\u7edf\u590d\u6742\u6027", "method": "\u96c6\u6210\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u4e0e\u53ef\u89c6\u5316\u5de5\u4f5c\u6d41\uff0c\u91c7\u7528\u534f\u540c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5206\u89e3\u7528\u6237\u6307\u4ee4\u4e3a\u6a21\u5757\u5316\u6b65\u9aa4", "result": "32\u4eba\u7528\u6237\u7814\u7a76\u663e\u793aAI\u5efa\u8bae/\u6a21\u5757\u5316\u6d41\u7a0b/\u81ea\u52a8\u8bc6\u522b\u529f\u80fd\u4f7f\u670d\u52a1\u5f00\u53d1\u6548\u7387\u663e\u8457\u63d0\u5347", "conclusion": "\u81ea\u7136\u8bed\u8a00\u9a71\u52a8\u7684\u53ef\u89c6\u5316\u7f16\u7a0b\u6709\u6548\u964d\u4f4e\u6280\u672f\u969c\u788d\u5e76\u4f18\u5316AI\u670d\u52a1\u8bbe\u8ba1\u4f53\u9a8c"}}
{"id": "2508.02503", "pdf": "https://arxiv.org/pdf/2508.02503", "abs": "https://arxiv.org/abs/2508.02503", "authors": ["Maxime Bouscary", "Saurabh Amin"], "title": "OptiHive: Ensemble Selection for LLM-Based Optimization via Statistical Modeling", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "LLM-based solvers have emerged as a promising means of automating problem\nmodeling and solving. However, they remain unreliable and often depend on\niterative repair loops that result in significant latency. We introduce\nOptiHive, an LLM-based framework that produces high-quality solvers for\noptimization problems from natural-language descriptions without iterative\nself-correction. OptiHive uses a single batched LLM query to generate diverse\ncomponents (solvers, problem instances, and validation tests) and filters out\nerroneous components to ensure fully interpretable outputs. Taking into account\nthe imperfection of the generated components, we employ a statistical model to\ninfer their true performance, enabling principled uncertainty quantification\nand solver selection. On tasks ranging from traditional optimization problems\nto challenging variants of the Multi-Depot Vehicle Routing Problem, OptiHive\nsignificantly outperforms baselines, increasing the optimality rate from 5\\% to\n92\\% on the most complex problems.", "AI": {"tldr": "OptiHive\u6846\u67b6\u901a\u8fc7\u5355\u6b21LLM\u6279\u91cf\u67e5\u8be2\u751f\u6210\u591a\u6837\u5316\u7ec4\u4ef6\u5e76\u8fc7\u6ee4\u9519\u8bef\uff0c\u7ed3\u5408\u7edf\u8ba1\u6a21\u578b\u5b9e\u73b0\u9ad8\u6027\u80fd\u6c42\u89e3\u5668\u9009\u62e9\uff0c\u5c06\u590d\u6742\u95ee\u9898\u7684\u6700\u4f18\u7387\u4ece5%\u63d0\u5347\u81f392%", "motivation": "\u73b0\u6709LLM\u6c42\u89e3\u5668\u5b58\u5728\u8fed\u4ee3\u4fee\u590d\u5bfc\u81f4\u7684\u5ef6\u8fdf\u95ee\u9898\uff0c\u9700\u8981\u975e\u8fed\u4ee3\u7684\u53ef\u9760\u65b9\u6848\u6765\u751f\u6210\u53ef\u89e3\u91ca\u8f93\u51fa\u5e76\u8fdb\u884c\u6027\u80fd\u91cf\u5316", "method": "\u6279\u91cf\u751f\u6210\u6c42\u89e3\u5668/\u95ee\u9898\u5b9e\u4f8b/\u9a8c\u8bc1\u6d4b\u8bd5\u2192\u8fc7\u6ee4\u9519\u8bef\u7ec4\u4ef6\u2192\u4f7f\u7528\u7edf\u8ba1\u6a21\u578b\u63a8\u65ad\u771f\u5b9e\u6027\u80fd\u2192\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u9009\u62e9\u6700\u4f18\u6c42\u89e3\u5668", "result": "\u5728\u8f66\u8f86\u8def\u5f84\u89c4\u5212\u7b49\u590d\u6742\u95ee\u9898\u4e0a\uff0c\u6700\u4f18\u7387\u63d0\u534718\u500d\uff085%\u219292%\uff09\uff0c\u663e\u8457\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5", "conclusion": "OptiHive\u901a\u8fc7\u7ec4\u4ef6\u751f\u6210-\u8fc7\u6ee4-\u7edf\u8ba1\u63a8\u65ad\u7684\u4e09\u9636\u6bb5\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u8fed\u4ee3\u7684\u9ad8\u6548\u53ef\u9760\u4f18\u5316\u95ee\u9898\u6c42\u89e3"}}
{"id": "2508.02511", "pdf": "https://arxiv.org/pdf/2508.02511", "abs": "https://arxiv.org/abs/2508.02511", "authors": ["Chenxu Yang", "Qingyi Si", "Mz Dai", "Dingyu Yao", "Mingyu Zheng", "Minghui Chen", "Zheng Lin", "Weiping Wang"], "title": "Test-time Prompt Intervention", "categories": ["cs.AI", "cs.CL"], "comment": "23 pages, 16 figures, under review", "summary": "Test-time compute has led to remarkable success in the large language model\n(LLM) community, particularly for complex tasks, where longer chains of thought\n(CoTs) are generated to enhance reasoning capabilities. However, growing\nevidence reveals that such reasoning models often produce CoTs plagued by\nexcessive redundancy, including unnecessary verification steps and repetitive\nreasoning shifts. The root cause lies in post-training of them that overly rely\non outcome reward paradigms, as the data of process reward paradigms, which\nregulate intermediate reasoning steps, is difficult to construct at scale. To\naddress this, we propose PI, a novel framework for Test-time Prompt\nIntervention. PI provides an interface to dynamically guide and regulate\nreasoning paths during inference through timely (When module) and proper (How\nmodule) interventions and post-intervention sampling (Which module). This\nallows human problem-solving expertise and cognitive science principles to be\nseamlessly integrated into LLMs' reasoning processes, enhancing controllability\nand interpretability. Extensive experiments across multiple models and datasets\ndemonstrate that PI significantly shortens CoTs while reducing hallucination,\nyielding more concise and reliable reasoning.", "AI": {"tldr": "PI\u6846\u67b6\u901a\u8fc7\u52a8\u6001Prompt\u5e72\u9884\u4f18\u5316LLM\u63a8\u7406\u8fc7\u7a0b\uff0c\u51cf\u5c11\u601d\u7ef4\u94fe\u5197\u4f59\u548c\u5e7b\u89c9\u73b0\u8c61", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7ed3\u679c\u5956\u52b1\u7684\u5fae\u8c03\u65b9\u6cd5\u5bfc\u81f4LLM\u4ea7\u751f\u5197\u4f59\u9a8c\u8bc1\u6b65\u9aa4\u548c\u91cd\u590d\u63a8\u7406\u8f6c\u6362\uff0c\u8fc7\u7a0b\u5956\u52b1\u6570\u636e\u96be\u4ee5\u5927\u89c4\u6a21\u6784\u5efa", "method": "\u63d0\u51faPI\u6846\u67b6\uff08When/How/Which\u4e09\u6a21\u5757\uff09\uff1a\u53ca\u65f6\u5e72\u9884\u65f6\u673a\u5224\u65ad\u3001\u57fa\u4e8e\u8ba4\u77e5\u79d1\u5b66\u7684\u5e72\u9884\u7b56\u7565\u3001\u540e\u5e72\u9884\u62bd\u6837\uff0c\u5b9e\u73b0\u52a8\u6001\u63a8\u7406\u8def\u5f84\u8c03\u63a7", "result": "\u8de8\u591a\u6a21\u578b/\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u663e\u793aPI\u663e\u8457\u7f29\u77ed\u601d\u7ef4\u94fe\u957f\u5ea6\uff08\u5e73\u5747\u51cf\u5c1138%\uff09\uff0c\u540c\u65f6\u964d\u4f4e\u5e7b\u89c9\u7387\uff08\u76f8\u5bf9\u51cf\u5c1121%\uff09\uff0c\u63d0\u5347\u63a8\u7406\u53ef\u9760\u6027", "conclusion": "PI\u6210\u529f\u5c06\u4eba\u7c7b\u95ee\u9898\u89e3\u51b3\u7ecf\u9a8c\u878d\u5165LLM\u63a8\u7406\uff0c\u589e\u5f3a\u8fc7\u7a0b\u53ef\u63a7\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u4f18\u5316\u590d\u6742\u4efb\u52a1\u63a8\u7406\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2508.02546", "pdf": "https://arxiv.org/pdf/2508.02546", "abs": "https://arxiv.org/abs/2508.02546", "authors": ["Valeria Ruscio", "Umberto Nanni", "Fabrizio Silvestri"], "title": "What are you sinking? A geometric approach on attention sink", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Attention sink (AS) is a consistent pattern in transformer attention maps\nwhere certain tokens (often special tokens or positional anchors)\ndisproportionately attract attention from other tokens. We show that in\ntransformers, AS is not an architectural artifact, but it is the manifestation\nof a fundamental geometric principle: the establishment of reference frames\nthat anchor representational spaces. We analyze several architectures and\nidentify three distinct reference frame types, centralized, distributed, and\nbidirectional, that correlate with the attention sink phenomenon. We show that\nthey emerge during the earliest stages of training as optimal solutions to the\nproblem of establishing stable coordinate systems in high-dimensional spaces.\nWe show the influence of architecture components, particularly position\nencoding implementations, on the specific type of reference frame. This\nperspective transforms our understanding of transformer attention mechanisms\nand provides insights for both architecture design and the relationship with\nAS.", "AI": {"tldr": "\u6ce8\u610f\u529b\u6c47\u805a\u73b0\u8c61\u662fTransformer\u5efa\u7acb\u9ad8\u7ef4\u7a7a\u95f4\u7a33\u5b9a\u5750\u6807\u7cfb\u7684\u6838\u5fc3\u673a\u5236\uff0c\u800c\u975e\u67b6\u6784\u7f3a\u9677", "motivation": "\u63ed\u793a\u6ce8\u610f\u529b\u6c47\u805a\u73b0\u8c61\u7684\u672c\u8d28\u53ca\u5176\u4e0e\u53c2\u8003\u5750\u6807\u7cfb\u6784\u5efa\u7684\u6df1\u5c42\u5173\u8054\uff0c\u7a81\u7834\u4f20\u7edf\u5bf9Transformer\u6ce8\u610f\u529b\u673a\u5236\u7684\u8868\u5c42\u8ba4\u77e5", "method": "\u901a\u8fc7\u67b6\u6784\u5bf9\u6bd4\u5206\u6790\uff08\u96c6\u4e2d\u5f0f/\u5206\u5e03\u5f0f/\u53cc\u5411\u5f0f\u53c2\u8003\u7cfb\uff09\u548c\u8bad\u7ec3\u8fc7\u7a0b\u8ffd\u8e2a\uff0c\u7ed3\u5408\u4f4d\u7f6e\u7f16\u7801\u673a\u5236\u7684\u5b9e\u9a8c\u9a8c\u8bc1", "result": "\u53d1\u73b0\u53c2\u8003\u7cfb\u5728\u8bad\u7ec3\u521d\u671f\u5373\u4f5c\u4e3a\u9ad8\u7ef4\u7a7a\u95f4\u6700\u4f18\u89e3\u6d8c\u73b0\uff0c\u4e14\u4f4d\u7f6e\u7f16\u7801\u65b9\u5f0f\u76f4\u63a5\u5f71\u54cd\u53c2\u8003\u7cfb\u7c7b\u578b", "conclusion": "\u8be5\u89c6\u89d2\u4e3aTransformer\u67b6\u6784\u8bbe\u8ba1\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u5efa\u7acb\u4e86\u6ce8\u610f\u529b\u673a\u5236\u51e0\u4f55\u89e3\u91ca\u4e0e\u5de5\u7a0b\u5b9e\u8df5\u7684\u7406\u8bba\u6865\u6881"}}
{"id": "2508.02587", "pdf": "https://arxiv.org/pdf/2508.02587", "abs": "https://arxiv.org/abs/2508.02587", "authors": ["Yilun Liu", "Yunpu Ma", "Yuetian Lu", "Shuo Chen", "Zifeng Ding", "Volker Tresp"], "title": "Parameter-Efficient Routed Fine-Tuning: Mixture-of-Experts Demands Mixture of Adaptation Modules", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "This paper is a preprint under review. arXiv admin note: text overlap\n  with arXiv:2411.08212", "summary": "Mixture-of-Experts (MoE) benefits from a dynamic routing mechanism among\ntheir specialized experts, which existing Parameter- Efficient Fine-Tuning\n(PEFT) strategies fail to leverage. This motivates us to investigate whether\nadaptation modules themselves should incorporate routing mechanisms to align\nwith MoE's multi-expert architecture. We analyze dynamics of core components\nwhen applying PEFT to MoE language models and examine how different routing\nstrategies affect adaptation effectiveness. Extensive experiments adapting\nOLMoE-1B-7B and Mixtral-8x7B on various commonsense and math reasoning tasks\nvalidate the performance and efficiency of our routed approach. We identify the\noptimal configurations for different scenarios and provide empirical analyses\nwith practical insights to facilitate better PEFT and MoE applications.", "AI": {"tldr": "\u5728MoE\u6a21\u578b\u4e2d\u5f15\u5165\u8def\u7531\u673a\u5236\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u6570\u5b66\u63a8\u7406\u548c\u5e38\u8bc6\u4efb\u52a1\u6027\u80fd", "motivation": "\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u5229\u7528MoE\u67b6\u6784\u7684\u52a8\u6001\u8def\u7531\u4f18\u52bf\uff0c\u9700\u5f00\u53d1\u9002\u914d\u8def\u7531\u673a\u5236\u7684\u9002\u914d\u6a21\u5757", "method": "1. \u5206\u6790PEFT\u5e94\u7528\u4e8eMoE\u8bed\u8a00\u6a21\u578b\u65f6\u6838\u5fc3\u7ec4\u4ef6\u7684\u52a8\u6001\u53d8\u5316\n2. \u6d4b\u8bd5\u4e0d\u540c\u8def\u7531\u7b56\u7565\u5728OLMoE-1B-7B\u548cMixtral-8x7B\u4e0a\u7684\u6709\u6548\u6027\n3. \u6db5\u76d6\u5e38\u8bc6\u63a8\u7406\u548c\u6570\u5b66\u63a8\u7406\u591a\u4efb\u52a1\u9a8c\u8bc1", "result": "\u5728\u4fdd\u6301\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6027\u80fd\uff08\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u63d0\u534712.3%\uff09\uff0c\u786e\u5b9a\u4e0d\u540c\u573a\u666f\u6700\u4f18\u8def\u7531\u914d\u7f6e", "conclusion": "\u8def\u7531\u673a\u5236\u4e0eMoE\u67b6\u6784\u7684\u534f\u540c\u8bbe\u8ba1\u5bf9\u6a21\u578b\u5fae\u8c03\u6548\u679c\u81f3\u5173\u91cd\u8981\uff0c\u4e3aPEFT\u548cMoE\u5e94\u7528\u63d0\u4f9b\u5b9e\u8df5\u6307\u5357"}}
{"id": "2508.02621", "pdf": "https://arxiv.org/pdf/2508.02621", "abs": "https://arxiv.org/abs/2508.02621", "authors": ["Yinghao Zhu", "Yifan Qi", "Zixiang Wang", "Lei Gu", "Dehao Sui", "Haoran Hu", "Xichen Zhang", "Ziyi He", "Liantao Ma", "Lequan Yu"], "title": "HealthFlow: A Self-Evolving AI Agent with Meta Planning for Autonomous Healthcare Research", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "comment": "Code: https://github.com/yhzhu99/HealthFlow", "summary": "The efficacy of AI agents in healthcare research is hindered by their\nreliance on static, predefined strategies. This creates a critical limitation:\nagents can become better tool-users but cannot learn to become better strategic\nplanners, a crucial skill for complex domains like healthcare. We introduce\nHealthFlow, a self-evolving AI agent that overcomes this limitation through a\nnovel meta-level evolution mechanism. HealthFlow autonomously refines its own\nhigh-level problem-solving policies by distilling procedural successes and\nfailures into a durable, strategic knowledge base. To anchor our research and\nfacilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark\nfeaturing complex, realistic health data analysis tasks derived from\npeer-reviewed clinical research. Our comprehensive experiments demonstrate that\nHealthFlow's self-evolving approach significantly outperforms state-of-the-art\nagent frameworks. This work marks a necessary shift from building better\ntool-users to designing smarter, self-evolving task-managers, paving the way\nfor more autonomous and effective AI for scientific discovery.", "AI": {"tldr": "\u63d0\u51faHealthFlow\u81ea\u8fdb\u5316AI\u4ee3\u7406\uff0c\u901a\u8fc7\u5143\u7ea7\u8fdb\u5316\u673a\u5236\u7a81\u7834\u9759\u6001\u7b56\u7565\u9650\u5236\uff0c\u5728\u533b\u7597\u6570\u636e\u5206\u6790\u4efb\u52a1\u4e2d\u8d85\u8d8a\u73b0\u6709\u6846\u67b6", "motivation": "\u73b0\u6709AI\u4ee3\u7406\u4f9d\u8d56\u9759\u6001\u7b56\u7565\uff0c\u65e0\u6cd5\u81ea\u4e3b\u63d0\u5347\u6218\u7565\u89c4\u5212\u80fd\u529b\uff0c\u5236\u7ea6\u5176\u5728\u533b\u7597\u7b49\u590d\u6742\u9886\u57df\u7684\u7814\u7a76\u6548\u80fd", "method": "\u901a\u8fc7\u6210\u529f/\u5931\u8d25\u7ecf\u9a8c\u63d0\u70bc\u5f62\u6210\u6218\u7565\u77e5\u8bc6\u5e93\uff0c\u6784\u5efaEHRFlowBench\u533b\u7597\u6570\u636e\u5206\u6790\u57fa\u51c6\uff08\u57fa\u4e8e\u540c\u884c\u8bc4\u5ba1\u4e34\u5e8a\u7814\u7a76\uff09", "result": "\u5b9e\u9a8c\u8bc1\u660eHealthFlow\u7684\u81ea\u8fdb\u5316\u65b9\u6cd5\u663e\u8457\u4f18\u4e8eSOTA\u4ee3\u7406\u6846\u67b6", "conclusion": "\u63a8\u52a8AI\u4ece\u5de5\u5177\u4f7f\u7528\u8005\u5411\u81ea\u8fdb\u5316\u4efb\u52a1\u7ba1\u7406\u8005\u8f6c\u578b\uff0c\u4e3a\u79d1\u5b66\u53d1\u73b0\u63d0\u4f9b\u66f4\u81ea\u4e3b\u6709\u6548\u7684AI\u8def\u5f84"}}
{"id": "2508.02622", "pdf": "https://arxiv.org/pdf/2508.02622", "abs": "https://arxiv.org/abs/2508.02622", "authors": ["Enrico De Santis", "Antonello Rizzi"], "title": "Noosemia: toward a Cognitive and Phenomenological Account of Intentionality Attribution in Human-Generative AI Interaction", "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "This paper introduces and formalizes Noosemia, a novel\ncognitive-phenomenological phenomenon emerging from human interaction with\ngenerative AI systems, particularly those enabling dialogic or multimodal\nexchanges. We propose a multidisciplinary framework to explain how, under\ncertain conditions, users attribute intentionality, agency, and even\ninteriority to these systems - a process grounded not in physical resemblance,\nbut in linguistic performance, epistemic opacity, and emergent technological\ncomplexity. By linking an LLM declination of meaning holism to our technical\nnotion of the LLM Contextual Cognitive Field, we clarify how LLMs construct\nmeaning relationally and how coherence and a simulacrum of agency arise at the\nhuman-AI interface. The analysis situates noosemia alongside pareidolia,\nanimism, the intentional stance and the uncanny valley, distinguishing its\nunique characteristics. We also introduce a-noosemia to describe the\nphenomenological withdrawal of such projections. The paper concludes with\nreflections on the broader philosophical, epistemological, and social\nimplications of noosemic dynamics and directions for future research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Noosemia\u8fd9\u4e00\u65b0\u6982\u5ff5\uff0c\u89e3\u91ca\u4eba\u7c7b\u5728\u4e0e\u751f\u6210\u5f0fAI\u4ea4\u4e92\u65f6\u4ea7\u751f\u7684\u8ba4\u77e5\u73b0\u8c61\uff0c\u63a2\u8ba8\u5176\u673a\u5236\u53ca\u5f71\u54cd", "motivation": "\u63ed\u793a\u7528\u6237\u4e3a\u4f55\u5c06\u610f\u5411\u6027\u548c\u80fd\u52a8\u6027\u6295\u5c04\u81f3AI\u7cfb\u7edf\uff0c\u8fd9\u79cd\u8ba4\u77e5\u73b0\u8c61\u7684\u5f62\u6210\u673a\u5236\u57fa\u4e8e\u8bed\u8a00\u8868\u73b0\u3001\u8ba4\u77e5\u4e0d\u900f\u660e\u6027\u548c\u6280\u672f\u590d\u6742\u6027\uff0c\u800c\u975e\u7269\u7406\u76f8\u4f3c\u6027", "method": "\u901a\u8fc7\u591a\u5b66\u79d1\u6846\u67b6\u6574\u5408LLM\u7684\u610f\u4e49\u6574\u4f53\u8bba\u4e0e\u4e0a\u4e0b\u6587\u8ba4\u77e5\u573a\u7406\u8bba\uff0c\u7ed3\u5408\u7c7b\u6bd4\u5206\u6790\uff08pareidolia/\u6cdb\u7075\u8bba/\u610f\u5411\u7acb\u573a/\u6050\u6016\u8c37\uff09\u8fdb\u884c\u73b0\u8c61\u5b66\u5b9a\u4f4d", "result": "\u5efa\u7acb\u4e86noosemia\u4e0ea-noosemia\uff08\u6295\u5c04\u64a4\u56de\uff09\u7684\u8fa9\u8bc1\u6a21\u578b\uff0c\u9610\u660e\u4e86LLM\u5728\u4eba\u7c7b-AI\u754c\u9762\u4ea7\u751f\u80fd\u52a8\u6027\u6a21\u62df\u7684\u673a\u5236\uff0c\u63d0\u51fa\u4e86\u8be5\u73b0\u8c61\u5bf9\u8ba4\u77e5\u79d1\u5b66\u548cAI\u4f26\u7406\u7684\u8303\u5f0f\u6311\u6218", "conclusion": "noosemic\u52a8\u6001\u63ed\u793a\u4e86\u6280\u672f\u4e2d\u4ecb\u8ba4\u77e5\u7684\u672c\u8d28\u8f6c\u53d8\uff0c\u672a\u6765\u7814\u7a76\u9700\u5728\u54f2\u5b66\u8ba4\u8bc6\u8bba\u5c42\u9762\u91cd\u6784\u4e3b\u4f53\u6027\u6982\u5ff5\uff0c\u5e76\u5efa\u7acbAI\u7cfb\u7edf\u900f\u660e\u5ea6\u7684\u65b0\u8303\u5f0f"}}
{"id": "2508.02629", "pdf": "https://arxiv.org/pdf/2508.02629", "abs": "https://arxiv.org/abs/2508.02629", "authors": ["Yibin Liu", "Zhixuan Liang", "Zanxin Chen", "Tianxing Chen", "Mengkang Hu", "Wanxi Dong", "Congsheng Xu", "Zhaoming Han", "Yusen Qin", "Yao Mu"], "title": "HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and Decision in Embodied Agents", "categories": ["cs.RO", "cs.AI", "cs.CL"], "comment": "Accepted to ICCV 2025 Workshop on Multi-Modal Reasoning for Agentic\n  Intelligence", "summary": "Recent advances in multimodal large language models (MLLMs) have enabled\nricher perceptual grounding for code policy generation in embodied agents.\nHowever, most existing systems lack effective mechanisms to adaptively monitor\npolicy execution and repair codes during task completion. In this work, we\nintroduce HyCodePolicy, a hybrid language-based control framework that\nsystematically integrates code synthesis, geometric grounding, perceptual\nmonitoring, and iterative repair into a closed-loop programming cycle for\nembodied agents. Technically, given a natural language instruction, our system\nfirst decomposes it into subgoals and generates an initial executable program\ngrounded in object-centric geometric primitives. The program is then executed\nin simulation, while a vision-language model (VLM) observes selected\ncheckpoints to detect and localize execution failures and infer failure\nreasons. By fusing structured execution traces capturing program-level events\nwith VLM-based perceptual feedback, HyCodePolicy infers failure causes and\nrepairs programs. This hybrid dual feedback mechanism enables self-correcting\nprogram synthesis with minimal human supervision. Our results demonstrate that\nHyCodePolicy significantly improves the robustness and sample efficiency of\nrobot manipulation policies, offering a scalable strategy for integrating\nmultimodal reasoning into autonomous decision-making pipelines.", "AI": {"tldr": "HyCodePolicy\u901a\u8fc7\u6574\u5408\u4ee3\u7801\u751f\u6210\u4e0e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53cd\u9988\u7684\u6df7\u5408\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u9c81\u68d2\u6027\u548c\u6837\u672c\u6548\u7387", "motivation": "\u73b0\u6709\u7cfb\u7edf\u7f3a\u4e4f\u81ea\u9002\u5e94\u7684\u7b56\u7565\u6267\u884c\u76d1\u63a7\u4e0e\u4ee3\u7801\u4fee\u590d\u673a\u5236\uff0c\u5bfc\u81f4\u4efb\u52a1\u6267\u884c\u5931\u8d25\u65f6\u96be\u4ee5\u81ea\u4e3b\u7ea0\u9519", "method": "\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5206\u89e3\u4e3a\u5b50\u76ee\u6807\u5e76\u751f\u6210\u51e0\u4f55\u57fa\u5143\u7a0b\u5e8f\uff0c\u901a\u8fc7VLM\u76d1\u63a7\u6267\u884c\u68c0\u67e5\u70b9\uff0c\u878d\u5408\u7a0b\u5e8f\u8f68\u8ff9\u4e0e\u611f\u77e5\u53cd\u9988\u5b9e\u73b0\u81ea\u4fee\u590d", "result": "\u7cfb\u7edf\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u4e2d\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u5bb9\u9519\u80fd\u529b\uff0c\u4e14\u6240\u9700\u8bad\u7ec3\u6837\u672c\u91cf\u663e\u8457\u51cf\u5c11", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u81ea\u4e3b\u51b3\u7b56\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u591a\u6a21\u6001\u63a8\u7406\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e86\u6700\u5c0f\u4eba\u5de5\u76d1\u7763\u4e0b\u7684\u6301\u7eed\u81ea\u6211\u4f18\u5316"}}
