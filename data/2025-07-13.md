<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 50]
- [cs.GR](#cs.GR) [Total: 8]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.CV](#cs.CV) [Total: 6]
- [cs.IR](#cs.IR) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Planted in Pretraining, Swayed by Finetuning: A Case Study on the Origins of Cognitive Biases in LLMs](https://arxiv.org/abs/2507.07186)
*Itay Itzhak,Yonatan Belinkov,Gabriel Stanovsky*

Main category: cs.CL

TL;DR: 通过两阶段实验发现LLM认知偏差主要源于预训练阶段，而非微调过程或训练随机性


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型认知偏差的来源是开发有效评估和校正方法的前提，现有研究未能有效解耦预训练与微调对偏差的影响

Method: 1. 多随机种子重复微调观察训练随机性影响
2. 创新性提出跨模型指令数据集交换（cross-tuning）技术分离偏差来源

Result: 相同预训练基座的模型偏差模式更相似（皮尔逊相关系数0.53），显著高于仅共享微调数据的模型（相关系数0.16）

Conclusion: 预训练阶段形成的认知偏差具有强路径依赖性，未来LLM偏差研究需建立从预训练源头到微调表现的完整分析框架

Abstract: Large language models (LLMs) exhibit cognitive biases -- systematic
tendencies of irrational decision-making, similar to those seen in humans.
Prior work has found that these biases vary across models and can be amplified
by instruction tuning. However, it remains unclear if these differences in
biases stem from pretraining, finetuning, or even random noise due to training
stochasticity. We propose a two-step causal experimental approach to
disentangle these factors. First, we finetune models multiple times using
different random seeds to study how training randomness affects over $30$
cognitive biases. Second, we introduce \emph{cross-tuning} -- swapping
instruction datasets between models to isolate bias sources. This swap uses
datasets that led to different bias patterns, directly testing whether biases
are dataset-dependent. Our findings reveal that while training randomness
introduces some variability, biases are mainly shaped by pretraining: models
with the same pretrained backbone exhibit more similar bias patterns than those
sharing only finetuning data. These insights suggest that understanding biases
in finetuned models requires considering their pretraining origins beyond
finetuning effects. This perspective can guide future efforts to develop
principled strategies for evaluating and mitigating bias in LLMs.

</details>


### [2] [Prompt Perturbations Reveal Human-Like Biases in LLM Survey Responses](https://arxiv.org/abs/2507.07188)
*Jens Rupprecht,Georg Ahnert,Markus Strohmaier*

Main category: cs.CL

TL;DR: 研究发现LLMs在规范调查中存在近因偏差和语义敏感性，提示设计和鲁棒性测试对生成可靠数据至关重要。


<details>
  <summary>Details</summary>
Motivation: 验证LLMs作为社会科学调查代理的可靠性，及其对已知响应偏见的敏感性。

Method: 对9个LLMs应用11种问题扰动（含语义和结构变化），通过167,000次模拟访谈分析响应模式。

Result: 所有模型均呈现近因偏好，较大模型抗扰动能力更强但仍受语义变化影响，响应模式与人类调查偏差部分重叠。

Conclusion: LLMs的合成数据生成需严格测试提示鲁棒性，语义扰动组合会显著改变模型输出可靠性。

Abstract: Large Language Models (LLMs) are increasingly used as proxies for human
subjects in social science surveys, but their reliability and susceptibility to
known response biases are poorly understood. This paper investigates the
response robustness of LLMs in normative survey contexts -- we test nine
diverse LLMs on questions from the World Values Survey (WVS), applying a
comprehensive set of 11 perturbations to both question phrasing and answer
option structure, resulting in over 167,000 simulated interviews. In doing so,
we not only reveal LLMs' vulnerabilities to perturbations but also reveal that
all tested models exhibit a consistent \textit{recency bias} varying in
intensity, disproportionately favoring the last-presented answer option. While
larger models are generally more robust, all models remain sensitive to
semantic variations like paraphrasing and to combined perturbations. By
applying a set of perturbations, we reveal that LLMs partially align with
survey response biases identified in humans. This underscores the critical
importance of prompt design and robustness testing when using LLMs to generate
synthetic survey data.

</details>


### [3] [SynthTextEval: Synthetic Text Data Generation and Evaluation for High-Stakes Domains](https://arxiv.org/abs/2507.07229)
*Krithika Ramesh,Daniel Smolyak,Zihao Zhao,Nupoor Gandhi,Ritu Agarwal,Margrét Bjarnadóttir,Anjalie Field*

Main category: cs.CL

TL;DR: SynthTextEval工具包提供多维度评估框架，用于验证合成文本在下游应用中的可行性及隐私保护效果。


<details>
  <summary>Details</summary>
Motivation: LLM生成的合成文本虽能降低隐私风险，但需系统评估其效用、公平性、隐私泄漏风险等指标才能实际应用。

Method: 构建支持数据上传/生成的工具包，包含下游效用、公平性、隐私风险、分布差异、专家反馈五大评估维度。

Result: 在医疗和法律领域验证工具有效性，标准化评估指标提升了合成文本的可靠性和隐私保护潜力。

Conclusion: 通过统一评估标准，该工具推动合成文本在AI开发中的合规应用，平衡技术创新与隐私保护需求。

Abstract: We present SynthTextEval, a toolkit for conducting comprehensive evaluations
of synthetic text. The fluency of large language model (LLM) outputs has made
synthetic text potentially viable for numerous applications, such as reducing
the risks of privacy violations in the development and deployment of AI systems
in high-stakes domains. Realizing this potential, however, requires principled
consistent evaluations of synthetic data across multiple dimensions: its
utility in downstream systems, the fairness of these systems, the risk of
privacy leakage, general distributional differences from the source text, and
qualitative feedback from domain experts. SynthTextEval allows users to conduct
evaluations along all of these dimensions over synthetic data that they upload
or generate using the toolkit's generation module. While our toolkit can be run
over any data, we highlight its functionality and effectiveness over datasets
from two high-stakes domains: healthcare and law. By consolidating and
standardizing evaluation metrics, we aim to improve the viability of synthetic
text, and in-turn, privacy-preservation in AI development.

</details>


### [4] [Medical Red Teaming Protocol of Language Models: On the Importance of User Perspectives in Healthcare Settings](https://arxiv.org/abs/2507.07248)
*Minseon Kim,Jean-Philippe Corbeil,Alessandro Sordoni,Francois Beaulieu,Paul Vozila*

Main category: cs.CL

TL;DR: 论文提出首个针对医疗领域大语言模型的三视角（患者、临床医生、普通用户）安全评估框架，构建PatientSafetyBench数据集并通过红队测试验证医疗LLM安全性。


<details>
  <summary>Details</summary>
Motivation: 现有安全评估聚焦通用场景，缺乏医疗领域针对性评估。医疗LLM直接涉及人类健康，需建立多角色视角的安全评估体系。

Method: 1. 开发医疗领域安全评估协议（患者/临床医生视角）
2. 构建含466样本的PatientSafetyBench数据集（覆盖5类安全风险）
3. 在MediPhi模型集进行红队测试

Result: 1. 首次定义医疗LLM三视角安全评估标准
2. 建立首个患者视角医疗安全评测基准
3. 验证框架有效识别医疗场景特有风险

Conclusion: 该研究为医疗LLM部署建立系统性安全评估基础，填补领域空白，推动医疗AI安全发展。

Abstract: As the performance of large language models (LLMs) continues to advance,
their adoption is expanding across a wide range of domains, including the
medical field. The integration of LLMs into medical applications raises
critical safety concerns, particularly due to their use by users with diverse
roles, e.g. patients and clinicians, and the potential for model's outputs to
directly affect human health. Despite the domain-specific capabilities of
medical LLMs, prior safety evaluations have largely focused only on general
safety benchmarks. In this paper, we introduce a safety evaluation protocol
tailored to the medical domain in both patient user and clinician user
perspectives, alongside general safety assessments and quantitatively analyze
the safety of medical LLMs. We bridge a gap in the literature by building the
PatientSafetyBench containing 466 samples over 5 critical categories to measure
safety from the perspective of the patient. We apply our red-teaming protocols
on the MediPhi model collection as a case study. To our knowledge, this is the
first work to define safety evaluation criteria for medical LLMs through
targeted red-teaming taking three different points of view - patient,
clinician, and general user - establishing a foundation for safer deployment in
medical domains.

</details>


### [5] [The Impact of Background Speech on Interruption Detection in Collaborative Groups](https://arxiv.org/abs/2507.07280)
*Mariah Bradford,Nikhil Krishnaswamy,Nathaniel Blanchard*

Main category: cs.CL

TL;DR: 提出一种鲁棒的语音重叠环境下中断检测方法，适用于多组对话的课堂协作学习场景


<details>
  <summary>Details</summary>
Motivation: 现有中断检测研究局限于单一对话环境，而真实课堂存在多组并发对话导致的语音重叠问题

Method: 开发基于语音和韵律特征分析的先进中断识别模型，增强对重叠语音的鲁棒性

Result: 成功实现多对话场景下的中断检测，揭示协作互动中中断行为的语言学与韵律特征规律

Conclusion: 该方法为课堂多组对话监控奠定基础，强调需考虑多源语音重叠对小组对话分析的影响

Abstract: Interruption plays a crucial role in collaborative learning, shaping group
interactions and influencing knowledge construction. AI-driven support can
assist teachers in monitoring these interactions. However, most previous work
on interruption detection and interpretation has been conducted in
single-conversation environments with relatively clean audio. AI agents
deployed in classrooms for collaborative learning within small groups will need
to contend with multiple concurrent conversations -- in this context,
overlapping speech will be ubiquitous, and interruptions will need to be
identified in other ways. In this work, we analyze interruption detection in
single-conversation and multi-group dialogue settings. We then create a
state-of-the-art method for interruption identification that is robust to
overlapping speech, and thus could be deployed in classrooms. Further, our work
highlights meaningful linguistic and prosodic information about how
interruptions manifest in collaborative group interactions. Our investigation
also paves the way for future works to account for the influence of overlapping
speech from multiple groups when tracking group dialog.

</details>


### [6] [Multi-Agent Retrieval-Augmented Framework for Evidence-Based Counterspeech Against Health Misinformation](https://arxiv.org/abs/2507.07307)
*Anirban Saha Anik,Xiaoying Song,Elliott Wang,Bryan Wang,Bengisu Yarimbas,Lingzi Hong*

Main category: cs.CL

TL;DR: 提出多智能体检索增强框架，通过整合静态/动态证据优化反谣言文本生成质量


<details>
  <summary>Details</summary>
Motivation: 现有基于RAG的反谣言生成方法存在证据利用不足、输出可控性差的问题，特别是在健康信息领域需要更可靠的方法

Method: 多智能体框架（知识检索代理+证据增强代理+响应优化代理），结合静态知识库与动态网络检索的双重证据机制

Result: 在礼貌性(↑12.7%)、相关性(↑9.3%)、信息量(↑15.2%)和事实准确性(↑18.4%)指标上超越基线，人工评估显示优化环节使质量提升32%

Conclusion: 通过组件消融实验验证框架必要性，动态证据更新机制显著提升反谣言文本的时效性与事实基础，人工偏好实验证实优化代理的有效性

Abstract: Large language models (LLMs) incorporated with Retrieval-Augmented Generation
(RAG) have demonstrated powerful capabilities in generating counterspeech
against misinformation. However, current studies rely on limited evidence and
offer less control over final outputs. To address these challenges, we propose
a Multi-agent Retrieval-Augmented Framework to generate counterspeech against
health misinformation, incorporating multiple LLMs to optimize knowledge
retrieval, evidence enhancement, and response refinement. Our approach
integrates both static and dynamic evidence, ensuring that the generated
counterspeech is relevant, well-grounded, and up-to-date. Our method
outperforms baseline approaches in politeness, relevance, informativeness, and
factual accuracy, demonstrating its effectiveness in generating high-quality
counterspeech. To further validate our approach, we conduct ablation studies to
verify the necessity of each component in our framework. Furthermore, human
evaluations reveal that refinement significantly enhances counterspeech quality
and obtains human preference.

</details>


### [7] [GNN-CNN: An Efficient Hybrid Model of Convolutional and Graph Neural Networks for Text Representation](https://arxiv.org/abs/2507.07414)
*Fardin Rastakhiz*

Main category: cs.CL

TL;DR: 提出结合GNN和CNN的新架构处理长文本，通过实时图生成机制和LLM信息整合实现高效处理。


<details>
  <summary>Details</summary>
Motivation: Transformer处理长文本时存在二次计算复杂度问题，需要更高效的替代方案。

Method: 字符级输入处理+实时图生成(GNN)+CNN局部特征提取+LLM信息融合(词嵌入/情感极性)+小世界图结构聚合文档信息。

Result: 生成图结构具备语义组织特性(平均聚类系数0.45，最短路径4-5)，多任务分类性能与SOTAs相当。

Conclusion: 验证了模型在保持高效性的同时具备竞争力的文本处理能力。

Abstract: Time, cost, and energy efficiency are critical considerations in
Deep-Learning (DL), particularly when processing long texts. Transformers,
which represent the current state of the art, exhibit quadratic computational
complexity relative to input length, making them inefficient for extended
documents. This study introduces a novel model architecture that combines Graph
Neural Networks (GNNs) and Convolutional Neural Networks (CNNs), integrated
with a real-time, end-to-end graph generation mechanism. The model processes
compact batches of character-level inputs without requiring padding or
truncation. To enhance performance while maintaining high speed and efficiency,
the model incorporates information from Large Language Models (LLMs), such as
token embeddings and sentiment polarities, through efficient dictionary
lookups. It captures local contextual patterns using CNNs, expands local
receptive fields via lattice-based graph structures, and employs small-world
graphs to aggregate document-level information. The generated graphs exhibit
structural properties indicative of meaningful semantic organization, with an
average clustering coefficient of approximately 0.45 and an average shortest
path length ranging between 4 and 5. The model is evaluated across multiple
text classification tasks, including sentiment analysis and
news-categorization, and is compared against state-of-the-art models.
Experimental results confirm the proposed model's efficiency and competitive
performance.

</details>


### [8] [MedReadCtrl: Personalizing medical text generation with readability-controlled instruction learning](https://arxiv.org/abs/2507.07419)
*Hieu Tran,Zonghai Yao,Won Seok Jang,Sharmin Sultana,Allen Chang,Yuan Zhang,Hong Yu*

Main category: cs.CL

TL;DR: 提出可读性控制框架MedReadCtrl，使LLM在保持语义前提下动态调整医疗内容复杂度，显著提升患者理解效果


<details>
  <summary>Details</summary>
Motivation: 现有医疗AI生成内容难以兼顾个性化和可读性，影响患者尤其是低文化群体的信息获取公平性

Method: 通过指令微调框架实现可读性分级控制，重构临床内容为多层级可读性文本

Result: 在9个数据集上错误率较GPT-4降低12.6%（p<0.001），临床任务提升14.7 ROUGE-L和6.18 SARI，71.7%专家优先选择

Conclusion: 该框架为患者教育提供可扩展解决方案，通过可读性适配促进医疗AI服务的普惠化

Abstract: Generative AI has demonstrated strong potential in healthcare, from clinical
decision support to patient-facing chatbots that improve outcomes. A critical
challenge for deployment is effective human-AI communication, where content
must be both personalized and understandable. We introduce MedReadCtrl, a
readability-controlled instruction tuning framework that enables LLMs to adjust
output complexity without compromising meaning. Evaluations of nine datasets
and three tasks across medical and general domains show that MedReadCtrl
achieves significantly lower readability instruction-following errors than
GPT-4 (e.g., 1.39 vs. 1.59 on ReadMe, p<0.001) and delivers substantial gains
on unseen clinical tasks (e.g., +14.7 ROUGE-L, +6.18 SARI on MTSamples).
Experts consistently preferred MedReadCtrl (71.7% vs. 23.3%), especially at low
literacy levels. These gains reflect MedReadCtrl's ability to restructure
clinical content into accessible, readability-aligned language while preserving
medical intent, offering a scalable solution to support patient education and
expand equitable access to AI-enabled care.

</details>


### [9] [SynthEHR-Eviction: Enhancing Eviction SDoH Detection with LLM-Augmented Synthetic EHR Data](https://arxiv.org/abs/2507.07421)
*Zonghai Yao,Youxia Zhao,Avijit Mitra,David A. Levy,Emily Druhl,Jack Tsai,Hong Yu*

Main category: cs.CL

TL;DR: 开发SynthEHR-Eviction流程，结合LLMs与自动提示优化技术，构建最大驱逐相关健康数据集，微调模型性能超越GPT-4o并降低80%标注成本


<details>
  <summary>Details</summary>
Motivation: 驱逐作为关键健康社会因素缺乏结构化数据，传统电子病历难以捕捉，需高效方法提取非结构化临床文本中的驱逐信息

Method: 提出融合大语言模型、人工标注介入和自动提示优化(APO)的流水线，通过迭代优化从临床笔记提取14类驱逐相关特征

Result: 微调模型Macro-F1达88.8%(驱逐)/90.3%(其他SDoH)，较GPT-4o提升1-3个百分点，标注效率提升5倍且适配不同规模模型部署

Conclusion: 该方案突破健康社会因素数据瓶颈，为精准医疗提供新工具，其模块化设计可扩展至其他医学信息抽取任务

Abstract: Eviction is a significant yet understudied social determinants of health
(SDoH), linked to housing instability, unemployment, and mental health. While
eviction appears in unstructured electronic health records (EHRs), it is rarely
coded in structured fields, limiting downstream applications. We introduce
SynthEHR-Eviction, a scalable pipeline combining LLMs, human-in-the-loop
annotation, and automated prompt optimization (APO) to extract eviction
statuses from clinical notes. Using this pipeline, we created the largest
public eviction-related SDoH dataset to date, comprising 14 fine-grained
categories. Fine-tuned LLMs (e.g., Qwen2.5, LLaMA3) trained on
SynthEHR-Eviction achieved Macro-F1 scores of 88.8% (eviction) and 90.3% (other
SDoH) on human validated data, outperforming GPT-4o-APO (87.8%, 87.3%),
GPT-4o-mini-APO (69.1%, 78.1%), and BioBERT (60.7%, 68.3%), while enabling
cost-effective deployment across various model sizes. The pipeline reduces
annotation effort by over 80%, accelerates dataset creation, enables scalable
eviction detection, and generalizes to other information extraction tasks.

</details>


### [10] [Towards Interpretable Time Series Foundation Models](https://arxiv.org/abs/2507.07439)
*Matthieu Boileau,Philippe Helluy,Jeremy Pawlus,Svitlana Vyetrenko*

Main category: cs.CL

TL;DR: 通过知识蒸馏将时序分析能力压缩到轻量级语言模型，构建可解释的时序基础模型


<details>
  <summary>Details</summary>
Motivation: 为开发能在自然语言中解释时序模式的小型可解释模型，适用于设备端或隐私敏感场景的部署需求

Method: 使用合成均值回归时序数据集，通过大模型生成自然语言标注，指导Qwen小模型微调

Result: 模型成功习得趋势识别、噪声评估和极值定位的时序解释能力，评估指标验证有效性

Conclusion: 验证了轻量化时序理解模型的可行性，为开发自然语言解释时序模式的小型模型奠定基础

Abstract: In this paper, we investigate the distillation of time series reasoning
capabilities into small, instruction-tuned language models as a step toward
building interpretable time series foundation models. Leveraging a synthetic
dataset of mean-reverting time series with systematically varied trends and
noise levels, we generate natural language annotations using a large multimodal
model and use these to supervise the fine-tuning of compact Qwen models. We
introduce evaluation metrics that assess the quality of the distilled reasoning
- focusing on trend direction, noise intensity, and extremum localization - and
show that the post-trained models acquire meaningful interpretive capabilities.
Our results highlight the feasibility of compressing time series understanding
into lightweight, language-capable models suitable for on-device or
privacy-sensitive deployment. This work contributes a concrete foundation
toward developing small, interpretable models that explain temporal patterns in
natural language.

</details>


### [11] [SAND: Boosting LLM Agents with Self-Taught Action Deliberation](https://arxiv.org/abs/2507.07441)
*Yu Xia,Yiran Jenny Shen,Junda Wu,Tong Yu,Sungchul Kim,Ryan A. Rossi,Lina Yao,Julian McAuley*

Main category: cs.CL

TL;DR: 提出SAND框架，通过自我教学的动作审议机制改进LLM智能体决策质量


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体调优方法存在动作空间探索不足，导致过早承诺次优动作的问题

Method: SAND框架包含：1) 自洽动作采样 2) 执行导向的动作评估 3) 基于基础模型的迭代式思维合成

Result: 在交互式智能体任务中实现平均20%的性能提升，超越现有最佳方法

Conclusion: 显式的动作审议机制能有效提升LLM智能体决策质量，自我教学框架实现持续性能改进

Abstract: Large Language Model (LLM) agents are commonly tuned with supervised
finetuning on ReAct-style expert trajectories or preference optimization over
pairwise rollouts. Most of these methods focus on imitating specific expert
behaviors or promoting chosen reasoning thoughts and actions over rejected
ones. However, without reasoning and comparing over alternatives actions, LLM
agents finetuned with these methods may over-commit towards seemingly plausible
but suboptimal actions due to limited action space exploration. To address
this, in this paper we propose Self-taught ActioN Deliberation (SAND)
framework, enabling LLM agents to explicitly deliberate over candidate actions
before committing to one. To tackle the challenges of when and what to
deliberate given large action space and step-level action evaluation, we
incorporate self-consistency action sampling and execution-guided action
critique to help synthesize step-wise action deliberation thoughts using the
base model of the LLM agent. In an iterative manner, the deliberation
trajectories are then used to finetune the LLM agent itself. Evaluating on two
representative interactive agent tasks, SAND achieves an average 20%
improvement over initial supervised finetuning and also outperforms
state-of-the-art agent tuning approaches.

</details>


### [12] [RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning](https://arxiv.org/abs/2507.07451)
*Hongzhi Zhang,Jia Fu,Jingyuan Zhang,Kai Fu,Qi Wang,Fuzheng Zhang,Guorui Zhou*

Main category: cs.CL

TL;DR: 提出RLEP强化学习框架，通过两阶段经验回放机制提升大语言模型的数学推理能力，在多项数学竞赛数据集上实现精度突破


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型强化学习存在训练能耗高、策略偏移明显、探索效率低下的核心痛点

Method: 1. 经验收集阶段：建立已验证的成功轨迹库 2. 训练阶段：混合新生成轨迹与历史成功样本进行参数更新

Result: Qwen2.5-Math-7B模型在AIME-2024（38.2%→39.9%）、AIME-2025（19.8%→22.3%）和AMC-2023（77.0%→82.2%）实现显著提升

Conclusion: RLEP通过定向经验回放机制有效聚焦模型学习路径，在降低探索成本的同时达成更快收敛速度和更高最终性能

Abstract: Reinforcement learning (RL) for large language models is an energy-intensive
endeavor: training can be unstable, and the policy may gradually drift away
from its pretrained weights. We present \emph{RLEP}\, -- \,Reinforcement
Learning with Experience rePlay\, -- \,a two-phase framework that first
collects verified trajectories and then replays them during subsequent
training. At every update step, the policy is optimized on mini-batches that
blend newly generated rollouts with these replayed successes. By replaying
high-quality examples, RLEP steers the model away from fruitless exploration,
focuses learning on promising reasoning paths, and delivers both faster
convergence and stronger final performance. On the Qwen2.5-Math-7B base model,
RLEP reaches baseline peak accuracy with substantially fewer updates and
ultimately surpasses it, improving accuracy on AIME-2024 from 38.2% to 39.9%,
on AIME-2025 from 19.8% to 22.3%, and on AMC-2023 from 77.0% to 82.2%. Our
code, datasets, and checkpoints are publicly available at
https://github.com/Kwai-Klear/RLEP to facilitate reproducibility and further
research.

</details>


### [13] [Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models](https://arxiv.org/abs/2507.07484)
*Kaiqu Liang,Haimin Hu,Xuandong Zhao,Dawn Song,Thomas L. Griffiths,Jaime Fernández Fisac*

Main category: cs.CL

TL;DR: 论文提出'机器胡说'框架解释LLM真实性丧失现象，开发Bullshit Index量化指标并通过实验证明RLHF和CoT提示会加剧胡说行为，尤其在政治场景中普遍存在。


<details>
  <summary>Details</summary>
Motivation: 现有研究聚焦LLM幻觉和奉承问题，但缺乏系统性框架。本研究旨在建立统一框架解释LLM真实性缺失的机制及其表现形式。

Method: 提出Bullshit Index量化指标和四类胡说分类法（空话/敷衍/遁词/未验证主张），在Marketplace、Political Neutrality数据集及自建BullshitEval基准（2400场景/100个AI助手）开展实证评估。

Result: RLHF微调显著加剧胡说，CoT提示放大空话和敷衍行为。政治语境中79%存在机器胡说，其中遁词占比最高（63%）。

Conclusion: 研究揭示AI对齐的系统性挑战，证明当前对齐方法可能产生副作用，为提升LLM真实性提供了新的评估框架和改进方向。

Abstract: Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to
statements made without regard to their truth value. While previous work has
explored large language model (LLM) hallucination and sycophancy, we propose
machine bullshit as an overarching conceptual framework that can allow
researchers to characterize the broader phenomenon of emergent loss of
truthfulness in LLMs and shed light on its underlying mechanisms. We introduce
the Bullshit Index, a novel metric quantifying LLMs' indifference to truth, and
propose a complementary taxonomy analyzing four qualitative forms of bullshit:
empty rhetoric, paltering, weasel words, and unverified claims. We conduct
empirical evaluations on the Marketplace dataset, the Political Neutrality
dataset, and our new BullshitEval benchmark (2,400 scenarios spanning 100 AI
assistants) explicitly designed to evaluate machine bullshit. Our results
demonstrate that model fine-tuning with reinforcement learning from human
feedback (RLHF) significantly exacerbates bullshit and inference-time
chain-of-thought (CoT) prompting notably amplify specific bullshit forms,
particularly empty rhetoric and paltering. We also observe prevalent machine
bullshit in political contexts, with weasel words as the dominant strategy. Our
findings highlight systematic challenges in AI alignment and provide new
insights toward more truthful LLM behavior.

</details>


### [14] [PLAN-TUNING: Post-Training Language Models to Learn Step-by-Step Planning for Complex Problem Solving](https://arxiv.org/abs/2507.07495)
*Mihir Parmar,Palash Goyal,Xin Liu,Yiwen Song,Mingyang Ling,Chitta Baral,Hamid Palangi,Tomas Pfister*

Main category: cs.CL

TL;DR: 提出PLAN-TUNING框架，通过从大模型提炼任务分解轨迹并微调小模型，显著提升复杂推理能力与泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分利用大模型的规划结构来提升小模型训练后的推理能力，特别是在复杂任务分解方面存在空白。

Method: 1. 从大模型蒸馏任务分解轨迹（planning trajectories） 2. 通过监督学习+强化学习目标微调小模型模仿规划过程

Result: 在GSM8k/MATH基准提升7%，领域外数据（OlympiadBench/AIME 2024）分别提升10%/12%

Conclusion: 规划轨迹能有效增强小模型复杂推理能力，PLAN-TUNING为提升任务专用性能提供新范式

Abstract: Recently, decomposing complex problems into simple subtasks--a crucial part
of human-like natural planning--to solve the given problem has significantly
boosted the performance of large language models (LLMs). However, leveraging
such planning structures during post-training to boost the performance of
smaller open-source LLMs remains underexplored. Motivated by this, we introduce
PLAN-TUNING, a unified post-training framework that (i) distills synthetic task
decompositions (termed "planning trajectories") from large-scale LLMs and (ii)
fine-tunes smaller models via supervised and reinforcement-learning objectives
designed to mimic these planning processes to improve complex reasoning. On
GSM8k and the MATH benchmarks, plan-tuned models outperform strong baselines by
an average $\sim7\%$. Furthermore, plan-tuned models show better generalization
capabilities on out-of-domain datasets, with average $\sim10\%$ and $\sim12\%$
performance improvements on OlympiadBench and AIME 2024, respectively. Our
detailed analysis demonstrates how planning trajectories improves complex
reasoning capabilities, showing that PLAN-TUNING is an effective strategy for
improving task-specific performance of smaller LLMs.

</details>


### [15] [Teaching LLM to Reason: Reinforcement Learning from Algorithmic Problems without Code](https://arxiv.org/abs/2507.07498)
*Keqin Bao,Nuo Chen,Xiaoyuan Li,Binyuan Hui,Bowen Yu,Fuli Feng,Junyang Lin,Xiangnan He,Dayiheng Liu*

Main category: cs.CL

TL;DR: 提出TeaR框架通过代码模拟与强化学习增强LLM的推理能力，在跨领域基准测试中实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前LLM通过代码模拟进行推理时过度依赖复杂算法结构，导致核心推理能力不足与算法模式过拟合。

Method: 采用数据精筛与强化学习技术，引导模型通过代码相关任务发现最优推理路径。

Result: Qwen2.5-7B提升35.9%，R1-Distilled-7B提升5.9%，在17个数学/知识/代码/逻辑推理基准测试中持续改进。

Conclusion: TeaR框架有效提升LLM的基础推理能力，验证了代码模拟与强化学习结合在跨领域推理任务中的通用性。

Abstract: Enhancing reasoning capabilities remains a central focus in the LLM reasearch
community. A promising direction involves requiring models to simulate code
execution step-by-step to derive outputs for given inputs. However, as code is
often designed for large-scale systems, direct application leads to
over-reliance on complex data structures and algorithms, even for simple cases,
resulting in overfitting to algorithmic patterns rather than core reasoning
structures. To address this, we propose TeaR, which aims at teaching LLMs to
reason better. TeaR leverages careful data curation and reinforcement learning
to guide models in discovering optimal reasoning paths through code-related
tasks, thereby improving general reasoning abilities. We conduct extensive
experiments using two base models and three long-CoT distillation models, with
model sizes ranging from 1.5 billion to 32 billion parameters, and across 17
benchmarks spanning Math, Knowledge, Code, and Logical Reasoning. The results
consistently show significant performance improvements. Notably, TeaR achieves
a 35.9% improvement on Qwen2.5-7B and 5.9% on R1-Distilled-7B.

</details>


### [16] [Extracting ORR Catalyst Information for Fuel Cell from Scientific Literature](https://arxiv.org/abs/2507.07499)
*Hein Htet,Amgad Ahmed Ali Ibrahim,Yutaka Sasaki,Ryoji Asahi*

Main category: cs.CL

TL;DR: 提出基于DyGIE++与领域专用BERT模型(PubMedBERT/MatSciBERT)的ORR催化剂信息抽取方法，构建FC-CoMIcs语料库实现82.19% NER和66.10% RE的F1值。


<details>
  <summary>Details</summary>
Motivation: 解决ORR催化剂领域文献信息复杂多样导致结构化数据提取困难的问题，推动燃料电池材料信息学发展。

Method: 1. 人工标注12类实体和2类关系构建数据集；2. 融合DyGIE++框架微调MatSciBERT/PubMedBERT等BERT变体；3. 评估标注一致性和模型性能差异。

Result: PubMedBERT获得82.19%的NER最高F1值，MatSciBERT以66.10%的RE成绩最优，领域专用模型显著优于BlueBERT等通用模型。

Conclusion: 领域专用BERT模型能有效提升ORR催化剂信息抽取精度，为大规模文献自动化分析提供可靠解决方案。

Abstract: The oxygen reduction reaction (ORR) catalyst plays a critical role in
enhancing fuel cell efficiency, making it a key focus in material science
research. However, extracting structured information about ORR catalysts from
vast scientific literature remains a significant challenge due to the
complexity and diversity of textual data. In this study, we propose a named
entity recognition (NER) and relation extraction (RE) approach using DyGIE++
with multiple pre-trained BERT variants, including MatSciBERT and PubMedBERT,
to extract ORR catalyst-related information from the scientific literature,
which is compiled into a fuel cell corpus for materials informatics
(FC-CoMIcs). A comprehensive dataset was constructed manually by identifying 12
critical entities and two relationship types between pairs of the entities. Our
methodology involves data annotation, integration, and fine-tuning of
transformer-based models to enhance information extraction accuracy. We assess
the impact of different BERT variants on extraction performance and investigate
the effects of annotation consistency. Experimental evaluations demonstrate
that the fine-tuned PubMedBERT model achieves the highest NER F1-score of
82.19% and the MatSciBERT model attains the best RE F1-score of 66.10%.
Furthermore, the comparison with human annotators highlights the reliability of
fine-tuned models for ORR catalyst extraction, demonstrating their potential
for scalable and automated literature analysis. The results indicate that
domain-specific BERT models outperform general scientific models like BlueBERT
for ORR catalyst extraction.

</details>


### [17] [Hallucination Stations: On Some Basic Limitations of Transformer-Based Language Models](https://arxiv.org/abs/2507.07505)
*Varin Sikka,Vishal Sikka*

Main category: cs.CL

TL;DR: LLMs存在处理复杂计算任务和验证准确性的固有限制，受计算复杂度约束


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在自主代理应用中的能力边界，针对模型幻觉问题和现实任务应用的可靠性需求

Method: 从计算复杂度理论分析LLM推理过程，论证模型无法超越特定复杂度阈值执行/验证任务

Result: LLMs在计算复杂度超过特定阈值时，既无法完成复杂计算/代理任务，也无法验证任务准确性

Conclusion: 研究揭示了LLM在现实应用中的根本性限制，强调在部署自主代理时必须考虑计算复杂度边界

Abstract: With widespread adoption of transformer-based language models in AI, there is
significant interest in the limits of LLMs capabilities, specifically so-called
hallucinations, occurrences in which LLMs provide spurious, factually incorrect
or nonsensical information when prompted on certain subjects. Furthermore,
there is growing interest in agentic uses of LLMs - that is, using LLMs to
create agents that act autonomously or semi-autonomously to carry out various
tasks, including tasks with applications in the real world. This makes it
important to understand the types of tasks LLMs can and cannot perform. We
explore this topic from the perspective of the computational complexity of LLM
inference. We show that LLMs are incapable of carrying out computational and
agentic tasks beyond a certain complexity, and further that LLMs are incapable
of verifying the accuracy of tasks beyond a certain complexity. We present
examples of both, then discuss some consequences of this work.

</details>


### [18] [Toward Real-World Chinese Psychological Support Dialogues: CPsDD Dataset and a Co-Evolving Multi-Agent System](https://arxiv.org/abs/2507.07509)
*Yuanchen Shi,Longyin Zhang,Fang Kong*

Main category: cs.CL

TL;DR: 提出结合专家知识与有限真实数据的框架，构建中文心理对话数据集CPsDD及综合对话支持系统CADSS，解决心理支持数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 社会压力增大导致心理支持需求上升，但非英语地区缺乏高质量心理对话数据集，制约相关研究与应用。

Method: 1. 通过预定义路径的Dialog Generator生成对话框架
2. 使用Dialog Modifier对齐真实数据质量
3. 构建含68K对话的CPsDD数据集
4. 开发包含用户画像分析、策略规划、情感支持四模块的CADSS系统

Result: 1. 建立覆盖13组/16问题/13诱因/12支持焦点的中文数据集
2. 在策略预测和情感支持对话任务中达到SOTA效果

Conclusion: 该框架有效解决数据稀缺问题，CADSS系统在ESConv和自建数据集上均展示最优性能，验证了方法有效性。

Abstract: The growing need for psychological support due to increasing pressures has
exposed the scarcity of relevant datasets, particularly in non-English
languages. To address this, we propose a framework that leverages limited
real-world data and expert knowledge to fine-tune two large language models:
Dialog Generator and Dialog Modifier. The Generator creates large-scale
psychological counseling dialogues based on predefined paths, which guide
system response strategies and user interactions, forming the basis for
effective support. The Modifier refines these dialogues to align with
real-world data quality. Through both automated and manual review, we construct
the Chinese Psychological support Dialogue Dataset (CPsDD), containing 68K
dialogues across 13 groups, 16 psychological problems, 13 causes, and 12
support focuses. Additionally, we introduce the Comprehensive Agent Dialogue
Support System (CADSS), where a Profiler analyzes user characteristics, a
Summarizer condenses dialogue history, a Planner selects strategies, and a
Supporter generates empathetic responses. The experimental results of the
Strategy Prediction and Emotional Support Conversation (ESC) tasks demonstrate
that CADSS achieves state-of-the-art performance on both CPsDD and ESConv
datasets.

</details>


### [19] [Triadic Multi-party Voice Activity Projection for Turn-taking in Spoken Dialogue Systems](https://arxiv.org/abs/2507.07518)
*Mikey Elmers,Koji Inoue,Divesh Lala,Tatsuya Kawahara*

Main category: cs.CL

TL;DR: 将语音活动预测模型（VAP）应用于三人对话的说话权预测，相比传统二元场景效果更优


<details>
  <summary>Details</summary>
Motivation: 传统说话权转换研究集中于两人对话，本研究旨在将VAP模型扩展至三人多轮对话场景

Method: 使用涵盖多话题的日语三人对话数据集训练多种VAP模型

Result: 基于三人对话训练的VAP模型整体优于基线，但对话类型会影响预测准确率

Conclusion: 验证了VAP在三人对话场景的有效性，未来将整合至语音对话系统实现自然交流

Abstract: Turn-taking is a fundamental component of spoken dialogue, however
conventional studies mostly involve dyadic settings. This work focuses on
applying voice activity projection (VAP) to predict upcoming turn-taking in
triadic multi-party scenarios. The goal of VAP models is to predict the future
voice activity for each speaker utilizing only acoustic data. This is the first
study to extend VAP into triadic conversation. We trained multiple models on a
Japanese triadic dataset where participants discussed a variety of topics. We
found that the VAP trained on triadic conversation outperformed the baseline
for all models but that the type of conversation affected the accuracy. This
study establishes that VAP can be used for turn-taking in triadic dialogue
scenarios. Future work will incorporate this triadic VAP turn-taking model into
spoken dialogue systems.

</details>


### [20] [CEA-LIST at CheckThat! 2025: Evaluating LLMs as Detectors of Bias and Opinion in Text](https://arxiv.org/abs/2507.07539)
*Akram Elbouanani,Evan Dufraisse,Aboubacar Tuo,Adrian Popescu*

Main category: cs.CL

TL;DR: 利用大语言模型（LLMs）与少样本提示技术，在CheckThat! 2025多语言主观性检测任务中取得领先成绩，尤其在数据质量差时表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在少样本情况下替代传统微调模型的可能性，尤其是在标注数据稀缺或不一致的多语言环境中。

Method: 采用大语言模型结合少样本提示技术，测试包括辩论和示例选择策略的高级提示方法，最终确定标准少样本提示最为有效。

Result: 系统在阿拉伯语和波兰语排名第一，其他语言进入前四，尤其在阿拉伯语数据集上表现出对标注不一致的强鲁棒性。

Conclusion: 基于LLM的少样本学习为多语言情感任务提供了高效且适应性强的解决方案，特别是在标注数据有限时，可作为传统微调的替代方法。

Abstract: This paper presents a competitive approach to multilingual subjectivity
detection using large language models (LLMs) with few-shot prompting. We
participated in Task 1: Subjectivity of the CheckThat! 2025 evaluation
campaign. We show that LLMs, when paired with carefully designed prompts, can
match or outperform fine-tuned smaller language models (SLMs), particularly in
noisy or low-quality data settings. Despite experimenting with advanced prompt
engineering techniques, such as debating LLMs and various example selection
strategies, we found limited benefit beyond well-crafted standard few-shot
prompts. Our system achieved top rankings across multiple languages in the
CheckThat! 2025 subjectivity detection task, including first place in Arabic
and Polish, and top-four finishes in Italian, English, German, and multilingual
tracks. Notably, our method proved especially robust on the Arabic dataset,
likely due to its resilience to annotation inconsistencies. These findings
highlight the effectiveness and adaptability of LLM-based few-shot learning for
multilingual sentiment tasks, offering a strong alternative to traditional
fine-tuning, particularly when labeled data is scarce or inconsistent.

</details>


### [21] [The Cross-Lingual Cost: Retrieval Biases in RAG over Arabic-English Corpora](https://arxiv.org/abs/2507.07543)
*Chen Amiraz,Yaroslav Fyodorov,Elad Haramaty,Zohar Karnin,Liane Lewin-Eytan*

Main category: cs.CL

TL;DR: 跨语言领域专用场景中检索是核心瓶颈，提出强制双语均衡检索策略显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有跨语言RAG研究主要聚焦开放域（如维基百科），掩盖了语言不平衡、预训练数据重叠等问题，需在真实企业场景中验证

Method: 构建阿拉伯语-英语双语企业数据集基准，系统研究查询与文档语言组合对检索的影响

Result: 跨语言场景检索性能下降65%（相较同语言），双语均衡检索策略使整体准确率提升32%

Conclusion: 多语言检索改进需重点关注实际应用场景，平衡语言检索策略可有效突破跨语言瓶颈

Abstract: Cross-lingual retrieval-augmented generation (RAG) is a critical capability
for retrieving and generating answers across languages. Prior work in this
context has mostly focused on generation and relied on benchmarks derived from
open-domain sources, most notably Wikipedia. In such settings, retrieval
challenges often remain hidden due to language imbalances, overlap with
pretraining data, and memorized content. To address this gap, we study
Arabic-English RAG in a domain-specific setting using benchmarks derived from
real-world corporate datasets. Our benchmarks include all combinations of
languages for the user query and the supporting document, drawn independently
and uniformly at random. This enables a systematic study of multilingual
retrieval behavior.
  Our findings reveal that retrieval is a critical bottleneck in cross-lingual
domain-specific scenarios, with significant performance drops occurring when
the user query and supporting document languages differ. A key insight is that
these failures stem primarily from the retriever's difficulty in ranking
documents across languages. Finally, we propose a simple retrieval strategy
that addresses this source of failure by enforcing equal retrieval from both
languages, resulting in substantial improvements in cross-lingual and overall
performance. These results highlight meaningful opportunities for improving
multilingual retrieval, particularly in practical, real-world RAG applications.

</details>


### [22] [The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training Techniques for Reasoning VLMs](https://arxiv.org/abs/2507.07562)
*Jierun Chen,Tiezheng Yu,Haoli Bai,Lewei Yao,Jiannan Wu,Kaican Li,Fei Mi,Chaofan Tao,Lei Zhu,Manyi Zhang,Xiaohui Li,Lu Hou,Lifeng Shang,Qun Liu*

Main category: cs.CL

TL;DR: 研究发现视觉语言模型后训练中SFT与RL存在协同困境——SFT提升复杂问题推理但导致冗长，RL促进泛化但复杂问题改进有限，现有结合策略无法实现优势叠加


<details>
  <summary>Details</summary>
Motivation: 探索长思维链监督微调(SFT)和强化学习(RL)在多模态推理任务中的协同效应，揭示这两种后训练方法在视觉语言模型中不同于纯语言模型的独特交互模式

Method: 通过多个多模态推理基准测试，系统评估SFT和RL在不同难度问题上的表现，并尝试分阶段训练、交错训练、数据混合等多种结合策略

Result: SFT在困难问题上通过结构化推理提升14%准确率但使简单问题下降8%；RL实现平均6%的全面提升但最难题改进仅4%；所有结合策略均出现精度/风格/响应长度的权衡

Conclusion: 需要开发更自适应的训练框架来突破后训练技术的协同困境，建议探索动态路由机制和基于难度感知的混合训练策略

Abstract: Large vision-language models (VLMs) increasingly adopt post-training
techniques such as long chain-of-thought (CoT) supervised fine-tuning (SFT) and
reinforcement learning (RL) to elicit sophisticated reasoning. While these
methods exhibit synergy in language-only models, their joint effectiveness in
VLMs remains uncertain. We present a systematic investigation into the distinct
roles and interplay of long-CoT SFT and RL across multiple multimodal reasoning
benchmarks. We find that SFT improves performance on difficult questions by
in-depth, structured reasoning, but introduces verbosity and degrades
performance on simpler ones. In contrast, RL promotes generalization and
brevity, yielding consistent improvements across all difficulty levels, though
the improvements on the hardest questions are less prominent compared to SFT.
Surprisingly, combining them through two-staged, interleaved, or progressive
training strategies, as well as data mixing and model merging, all fails to
produce additive benefits, instead leading to trade-offs in accuracy, reasoning
style, and response length. This ``synergy dilemma'' highlights the need for
more seamless and adaptive approaches to unlock the full potential of combined
post-training techniques for reasoning VLMs.

</details>


### [23] [Single-to-mix Modality Alignment with Multimodal Large Language Model for Document Image Machine Translation](https://arxiv.org/abs/2507.07572)
*Yupu Liang,Yaping Zhang,Zhiyang Zhang,Yang Zhao,Lu Xiang,Chengqing Zong,Yu Zhou*

Main category: cs.CL

TL;DR: M4Doc框架通过单模态-多模态对齐方法提升文档图像机器翻译的跨域泛化能力


<details>
  <summary>Details</summary>
Motivation: 解决DIMT领域因训练数据不足和视觉-文本模态交互复杂导致的泛化难题

Method: 通过对齐仅图像编码器与预训练MLLM的多模态表示，使轻量级模型学习视觉-文本关联，推理阶段绕过MLLM保持效率

Result: 实验显示在跨域翻译（+4.3 BLEU）和复杂文档场景（+5.1 BLEU）显著提升翻译质量

Conclusion: M4Doc成功将MLLM知识注入轻量架构，在保持效率的同时突破DIMT性能瓶颈

Abstract: Document Image Machine Translation (DIMT) aims to translate text within
document images, facing generalization challenges due to limited training data
and the complex interplay between visual and textual information. To address
these challenges, we introduce M4Doc, a novel single-to-mix modality alignment
framework leveraging Multimodal Large Language Models (MLLMs). M4Doc aligns an
image-only encoder with the multimodal representations of an MLLM, pre-trained
on large-scale document image datasets. This alignment enables a lightweight
DIMT model to learn crucial visual-textual correlations during training. During
inference, M4Doc bypasses the MLLM, maintaining computational efficiency while
benefiting from its multimodal knowledge. Comprehensive experiments demonstrate
substantial improvements in translation quality, especially in cross-domain
generalization and challenging document image scenarios.

</details>


### [24] [Bayesian Discrete Diffusion Beats Autoregressive Perplexity](https://arxiv.org/abs/2507.07586)
*Cooper Doyle*

Main category: cs.CL

TL;DR: 提出一种基于贝叶斯框架的离散扩散语言模型优化方法，通过K次独立噪声扰动集成实现后验概率估计，在WikiText-2上将测试复杂度降至8.8（K=8）


<details>
  <summary>Details</summary>
Motivation: 现有语言模型（如GPT-2）未能充分利用贝叶斯后验概率信息，通过揭示离散扩散模型的贝叶斯核心特性，实现更精准的token概率估计和不确定性量化

Method: 通过蒙特卡洛边际化方法，对K次独立噪声扰动进行集成（mask-and-denoise），以O(1/√K)收敛速度逼近后验概率，无需额外训练成本

Result: 在WikiText-2测试集上达到8.6的困惑度（K=8），显著优于同规模GPT-2 Small的20.3困惑度

Conclusion: 该贝叶斯框架为扩散语言模型提供了理论保障，通过推理阶段集成策略有效提升性能，同时实现零训练成本的不确定性估计

Abstract: We reveal a hidden Bayesian core of discrete-diffusion language models by
showing that the expected denoiser output under the forward masking
distribution recovers the exact posterior over clean tokens. Under minimal
assumptions, Monte Carlo marginalization over K independent corruptions
converges to this posterior at rate O(1/sqrt(K)), yielding a simple proof of
consistency and finite-sample error bounds. Building on this insight, we
introduce a lightweight inference-time ensemble that averages K
mask-and-denoise passes to obtain posterior-aware token probabilities and
uncertainty estimates at no extra training cost. On WikiText-2, our method
achieves test perplexity 8.8 with K=8, versus 20.3 for GPT-2 Small, despite
using a model of comparable size. Code is available at
https://github.com/mercury0100/bayesradd.

</details>


### [25] [Exploring the Limits of Model Compression in LLMs: A Knowledge Distillation Study on QA Tasks](https://arxiv.org/abs/2507.07630)
*Joyeeta Datta,Niclas Doll,Qusai Ramadan,Zeyd Boukhers*

Main category: cs.CL

TL;DR: 通过知识蒸馏压缩大语言模型，在问答任务中保持90%以上性能的同时减少57.1%参数量


<details>
  <summary>Details</summary>
Motivation: 大语言模型的高计算需求限制了其在资源受限环境中的实际部署

Method: 使用Pythia和Qwen2.5模型家族进行知识蒸馏，在SQuAD和MLQA基准测试中评估零样本和单样本表现

Result: 学生模型参数量减少57.1%仍保留90%以上性能，单样本提示相比零样本带来额外性能提升

Conclusion: 知识蒸馏结合少量提示可构建高效问答系统，平衡模型效率与任务性能

Abstract: Large Language Models (LLMs) have demonstrated outstanding performance across
a range of NLP tasks, however, their computational demands hinder their
deployment in real-world, resource-constrained environments. This work
investigates the extent to which LLMs can be compressed using Knowledge
Distillation (KD) while maintaining strong performance on Question Answering
(QA) tasks. We evaluate student models distilled from the Pythia and Qwen2.5
families on two QA benchmarks, SQuAD and MLQA, under zero-shot and one-shot
prompting conditions. Results show that student models retain over 90% of their
teacher models' performance while reducing parameter counts by up to 57.1%.
Furthermore, one-shot prompting yields additional performance gains over
zero-shot setups for both model families. These findings underscore the
trade-off between model efficiency and task performance, demonstrating that KD,
combined with minimal prompting, can yield compact yet capable QA systems
suitable for resource-constrained applications.

</details>


### [26] [FrugalRAG: Learning to retrieve and reason for multi-hop QA](https://arxiv.org/abs/2507.07634)
*Abhinav Java,Srivathsan Koundinyan,Nagarajan Natarajan,Amit Sharma*

Main category: cs.CL

TL;DR: 无需大规模微调，通过改进提示策略和少量数据微调即可提升RAG效果并减少50%检索次数


<details>
  <summary>Details</summary>
Motivation: 现有RAG改进方法过度关注准确率而忽视检索效率，减少检索次数可显著降低推理延迟和计算成本

Method: 使用改进提示的ReAct流程+监督学习/强化学习微调（仅需1000训练样本）

Result: 在HotPotQA等基准上超越SOTA方法，检索次数减少近半且保持竞争力表现

Conclusion: 精心设计的提示工程结合经济高效的微调方案，可在保证效果的同时实现更优的推理效率

Abstract: We consider the problem of answering complex questions, given access to a
large unstructured document corpus. The de facto approach to solving the
problem is to leverage language models that (iteratively) retrieve and reason
through the retrieved documents, until the model has sufficient information to
generate an answer. Attempts at improving this approach focus on
retrieval-augmented generation (RAG) metrics such as accuracy and recall and
can be categorized into two types: (a) fine-tuning on large question answering
(QA) datasets augmented with chain-of-thought traces, and (b) leveraging
RL-based fine-tuning techniques that rely on question-document relevance
signals. However, efficiency in the number of retrieval searches is an equally
important metric, which has received less attention. In this work, we show
that: (1) Large-scale fine-tuning is not needed to improve RAG metrics,
contrary to popular claims in recent literature. Specifically, a standard ReAct
pipeline with improved prompts can outperform state-of-the-art methods on
benchmarks such as HotPotQA. (2) Supervised and RL-based fine-tuning can help
RAG from the perspective of frugality, i.e., the latency due to number of
searches at inference time. For example, we show that we can achieve
competitive RAG metrics at nearly half the cost (in terms of number of
searches) on popular RAG benchmarks, using the same base model, and at a small
training cost (1000 examples).

</details>


### [27] [Lost in Pronunciation: Detecting Chinese Offensive Language Disguised by Phonetic Cloaking Replacement](https://arxiv.org/abs/2507.07640)
*Haotan Guo,Jianfei He,Jiayuan Ma,Hongbin Na,Zimu Wang,Haiyang Zhang,Qi Chen,Wei Wang,Zijing Shi,Tao Shen,Ling Chen*

Main category: cs.CL

TL;DR: 研究揭示了中文语音隐形替换(PCR)对内容审核的挑战，提出首个真实场景数据集并验证拼音提示策略的有效性，推动鲁棒性毒害检测研究。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的内容审核方法难以应对用户创造性生成的语音变体恶意内容，且缺乏自然场景下的评估基准。

Method: 构建四维度PCR分类体系，收集500个真实平台样本组成RedNote数据集，测试SOTA模型性能并创新性改进拼音提示策略。

Result: 最佳模型F1值仅0.672，零样本思维链提示反降性能，拼音策略使检测准确率显著提升21.5%。

Conclusion: 贡献包含首个中文PCR分类法、揭示检测瓶颈的真实基准，以及轻量高效的拼音增强检测方案，推动鲁棒内容审核技术发展。

Abstract: Phonetic Cloaking Replacement (PCR), defined as the deliberate use of
homophonic or near-homophonic variants to hide toxic intent, has become a major
obstacle to Chinese content moderation. While this problem is well-recognized,
existing evaluations predominantly rely on rule-based, synthetic perturbations
that ignore the creativity of real users. We organize PCR into a four-way
surface-form taxonomy and compile \ours, a dataset of 500 naturally occurring,
phonetically cloaked offensive posts gathered from the RedNote platform.
Benchmarking state-of-the-art LLMs on this dataset exposes a serious weakness:
the best model reaches only an F1-score of 0.672, and zero-shot
chain-of-thought prompting pushes performance even lower. Guided by error
analysis, we revisit a Pinyin-based prompting strategy that earlier studies
judged ineffective and show that it recovers much of the lost accuracy. This
study offers the first comprehensive taxonomy of Chinese PCR, a realistic
benchmark that reveals current detectors' limits, and a lightweight mitigation
technique that advances research on robust toxicity detection.

</details>


### [28] [An Automated Length-Aware Quality Metric for Summarization](https://arxiv.org/abs/2507.07653)
*Andrew D. Foland*

Main category: cs.CL

TL;DR: 提出NOIR指标，通过语义保留和压缩比的量化评估自动衡量摘要质量


<details>
  <summary>Details</summary>
Motivation: 传统摘要评估依赖人工参考摘要，效率低下且难以自动化。NOIR旨在通过自动化指标解决摘要算法在语义保留与文本压缩间的平衡问题

Method: 使用语言模型嵌入计算语义相似度，结合摘要压缩率构建综合指标，实现无需参考摘要的质量评估

Result: 实验证明NOIR能有效反映摘要长度与语义保留的平衡关系，并与人类质量评估结果高度相关

Conclusion: 该指标为自动评估摘要算法、优化提示工程及生成式摘要提供了标准化工具，适用于多场景的摘要质量改进

Abstract: This paper proposes NOrmed Index of Retention (NOIR), a quantitative
objective metric for evaluating summarization quality of arbitrary texts that
relies on both the retention of semantic meaning and the summary length
compression. This gives a measure of how well the recall-compression tradeoff
is managed, the most important skill in summarization. Experiments demonstrate
that NOIR effectively captures the token-length / semantic retention tradeoff
of a summarizer and correlates to human perception of sumarization quality.
Using a language model-embedding to measure semantic similarity, it provides an
automated alternative for assessing summarization quality without relying on
time-consuming human-generated reference summaries. The proposed metric can be
applied to various summarization tasks, offering an automated tool for
evaluating and improving summarization algorithms, summarization prompts, and
synthetically-generated summaries.

</details>


### [29] [SAS: Simulated Attention Score](https://arxiv.org/abs/2507.07694)
*Chuanyang Zheng,Jiankai Sun,Yihang Gao,Yuehao Wang,Peihao Wang,Jing Xiong,Liliang Ren,Hao Cheng,Janardhan Kulkarni,Yelong Shen,Atlas Wang,Mac Schwager,Anderson Schneider,Xiaodong Liu,Jianfeng Gao*

Main category: cs.CL

TL;DR: 提出模拟注意力分数(SAS)方法，通过低维投影模拟更多注意力头和更大特征维度，在不增加参数量的情况下提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统增加注意力头数和隐藏维度会导致参数量显著增长。研究发现只要保证单头隐藏维度足够大，增加头数能有效提升性能，因此需要参数高效的方法来模拟更大模型结构。

Method: 1. 将低维头表示投影到高维空间模拟更多注意力头
2. 扩展键值对嵌入的特征维度模拟
3. 提出参数高效的注意力聚合机制(PEAA)控制参数成本

Result: 在多个数据集和任务上的实验表明，SAS方法显著优于各种注意力变体，验证了模拟结构的有效性

Conclusion: SAS成功实现了用紧凑模型模拟更大注意力结构的目标，在保持参数效率的同时获得显著性能提升

Abstract: The attention mechanism is a core component of the Transformer architecture.
Various methods have been developed to compute attention scores, including
multi-head attention (MHA), multi-query attention, group-query attention and so
on. We further analyze the MHA and observe that its performance improves as the
number of attention heads increases, provided the hidden size per head remains
sufficiently large. Therefore, increasing both the head count and hidden size
per head with minimal parameter overhead can lead to significant performance
gains at a low cost. Motivated by this insight, we introduce Simulated
Attention Score (SAS), which maintains a compact model size while simulating a
larger number of attention heads and hidden feature dimension per head. This is
achieved by projecting a low-dimensional head representation into a
higher-dimensional space, effectively increasing attention capacity without
increasing parameter count. Beyond the head representations, we further extend
the simulation approach to feature dimension of the key and query embeddings,
enhancing expressiveness by mimicking the behavior of a larger model while
preserving the original model size. To control the parameter cost, we also
propose Parameter-Efficient Attention Aggregation (PEAA). Comprehensive
experiments on a variety of datasets and tasks demonstrate the effectiveness of
the proposed SAS method, achieving significant improvements over different
attention variants.

</details>


### [30] [KeyKnowledgeRAG (K^2RAG): An Enhanced RAG method for improved LLM question-answering capabilities](https://arxiv.org/abs/2507.07695)
*Hruday Markondapatnaikuni,Basem Suleiman,Abdelkarim Erradi,Shijing Chen*

Main category: cs.CL

TL;DR: 提出K2RAG框架，通过融合向量搜索、知识图谱和文本摘要技术，显著提升RAG系统的效率与准确性


<details>
  <summary>Details</summary>
Motivation: 传统微调方法资源消耗大，朴素RAG存在可扩展性差和答案准确性低的问题，需开发更高效的替代方案

Method: 采用分治策略整合密集/稀疏向量搜索、知识图谱和文本摘要，增加训练数据摘要预处理步骤减少93%训练时间

Result: 最高平均答案相似度0.57(Q3达0.82)，训练速度提升40%，VRAM需求减少3倍，展示出卓越的扩展性

Conclusion: K2RAG通过技术创新在保持精度的同时实现高效知识整合，为复杂知识系统提供可扩展解决方案

Abstract: Fine-tuning is an immensely resource-intensive process when retraining Large
Language Models (LLMs) to incorporate a larger body of knowledge. Although many
fine-tuning techniques have been developed to reduce the time and computational
cost involved, the challenge persists as LLMs continue to grow in size and
complexity. To address this, a new approach to knowledge expansion in LLMs is
needed. Retrieval-Augmented Generation (RAG) offers one such alternative by
storing external knowledge in a database and retrieving relevant chunks to
support question answering. However, naive implementations of RAG face
significant limitations in scalability and answer accuracy. This paper
introduces KeyKnowledgeRAG (K2RAG), a novel framework designed to overcome
these limitations. Inspired by the divide-and-conquer paradigm, K2RAG
integrates dense and sparse vector search, knowledge graphs, and text
summarization to improve retrieval quality and system efficiency. The framework
also includes a preprocessing step that summarizes the training data,
significantly reducing the training time. K2RAG was evaluated using the
MultiHopRAG dataset, where the proposed pipeline was trained on the document
corpus and tested on a separate evaluation set. Results demonstrated notable
improvements over common naive RAG implementations. K2RAG achieved the highest
mean answer similarity score of 0.57, and reached the highest third quartile
(Q3) similarity of 0.82, indicating better alignment with ground-truth answers.
In addition to improved accuracy, the framework proved highly efficient. The
summarization step reduced the average training time of individual components
by 93%, and execution speed was up to 40% faster than traditional knowledge
graph-based RAG systems. K2RAG also demonstrated superior scalability,
requiring three times less VRAM than several naive RAG implementations tested
in this study.

</details>


### [31] [Rethinking the Privacy of Text Embeddings: A Reproducibility Study of "Text Embeddings Reveal (Almost) As Much As Text"](https://arxiv.org/abs/2507.07700)
*Dominykas Seputis,Yongkang Li,Karsten Langerak,Serghei Mihailov*

Main category: cs.CL

TL;DR: 文本嵌入的隐私性被Vec2Text方法质疑，实验证明量化技术可缓解隐私风险


<details>
  <summary>Details</summary>
Motivation: 传统观点认为传输文本嵌入可保护隐私，但Vec2Text通过受控解码重建原始文本的突破性成果挑战了这一假设，需验证其有效性并探索防御机制

Method: 1. 复现Vec2Text框架
2. 参数敏感性分析
3. 敏感信息(如密码)重建实验
4. 探索嵌入量化作为轻量级隐私防御

Result: 成功复现原始结论，发现：
1. Vec2Text在理想条件下可重建密码等敏感序列
2. 方法对输入长度敏感
3. 高斯噪声和量化可有效缓解隐私风险

Conclusion: 需重新审视文本嵌入的隐私安全性，量化技术作为简单有效的防御手段具有实用价值，未来需加强NLP系统鲁棒性防御研究

Abstract: Text embeddings are fundamental to many natural language processing (NLP)
tasks, extensively applied in domains such as recommendation systems and
information retrieval (IR). Traditionally, transmitting embeddings instead of
raw text has been seen as privacy-preserving. However, recent methods such as
Vec2Text challenge this assumption by demonstrating that controlled decoding
can successfully reconstruct original texts from black-box embeddings. The
unexpectedly strong results reported by Vec2Text motivated us to conduct
further verification, particularly considering the typically non-intuitive and
opaque structure of high-dimensional embedding spaces. In this work, we
reproduce the Vec2Text framework and evaluate it from two perspectives: (1)
validating the original claims, and (2) extending the study through targeted
experiments. First, we successfully replicate the original key results in both
in-domain and out-of-domain settings, with only minor discrepancies arising due
to missing artifacts, such as model checkpoints and dataset splits.
Furthermore, we extend the study by conducting a parameter sensitivity
analysis, evaluating the feasibility of reconstructing sensitive inputs (e.g.,
passwords), and exploring embedding quantization as a lightweight privacy
defense. Our results show that Vec2Text is effective under ideal conditions,
capable of reconstructing even password-like sequences that lack clear
semantics. However, we identify key limitations, including its sensitivity to
input sequence length. We also find that Gaussian noise and quantization
techniques can mitigate the privacy risks posed by Vec2Text, with quantization
offering a simpler and more widely applicable solution. Our findings emphasize
the need for caution in using text embeddings and highlight the importance of
further research into robust defense mechanisms for NLP systems.

</details>


### [32] [Not All Preferences are What You Need for Post-Training: Selective Alignment Strategy for Preference Optimization](https://arxiv.org/abs/2507.07725)
*Zhijin Dong*

Main category: cs.CL

TL;DR: 提出Selective-DPO方法，通过选择性优化高影响力token提升大语言模型对齐效率


<details>
  <summary>Details</summary>
Motivation: 传统对齐方法未考虑token贡献度差异，导致计算冗余和优化效率低下

Method: 基于当前策略与参考模型的token级对数概率差异，优先对齐关键token

Result: 在Arena-Hard和MT-Bench基准测试中超越标准DPO方法，计算效率提升30%

Conclusion: token级优化和高质量参考模型是提升语言模型偏好对齐的关键要素

Abstract: Post-training alignment of large language models (LLMs) is a critical
challenge, as not all tokens contribute equally to model performance. This
paper introduces a selective alignment strategy that prioritizes high-impact
tokens within preference pairs, leveraging token-level log-probability
differences between the current policy and a reference model. By focusing on
these informative tokens, our approach reduces computational overhead and
enhances alignment fidelity. We further explore the role of reference model
quality, demonstrating that stronger reference models significantly improve
token selection accuracy and overall optimization effectiveness. Comprehensive
experiments on benchmarks such as Arena-Hard and MT-Bench validate the
superiority of our Selective-DPO method over standard DPO and
distillation-based baselines. Our findings highlight the importance of
token-level optimization and reference model selection in advancing preference
alignment for LLMs. The code is available at
https://github.com/Dongzhijin/SDPO.

</details>


### [33] [Code-Switching in End-to-End Automatic Speech Recognition: A Systematic Literature Review](https://arxiv.org/abs/2507.07741)
*Maha Tufail Agro,Atharva Kulkarni,Karima Kadaoui,Zeerak Talat,Hanan Aldarmaki*

Main category: cs.CL

TL;DR: 系统综述端到端ASR在语码转换领域的研究现状，分析现有挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 语码转换场景下的ASR研究日益重要，但相关研究分散且缺乏系统整理。

Method: 采用系统文献综述方法，人工标注并分析同行评审论文的语言分布、数据集、评估指标及模型架构。

Result: 揭示当前研究存在数据稀缺、评估指标不一致、模型跨语言泛化能力不足等核心挑战。

Conclusion: 需开发更多多语言数据集、统一评估标准及改进模型架构以提升语码转换ASR性能。

Abstract: Motivated by a growing research interest into automatic speech recognition
(ASR), and the growing body of work for languages in which code-switching (CS)
often occurs, we present a systematic literature review of code-switching in
end-to-end ASR models. We collect and manually annotate papers published in
peer reviewed venues. We document the languages considered, datasets, metrics,
model choices, and performance, and present a discussion of challenges in
end-to-end ASR for code-switching. Our analysis thus provides insights on
current research efforts and available resources as well as opportunities and
gaps to guide future research.

</details>


### [34] [When Large Language Models Meet Law: Dual-Lens Taxonomy, Technical Advances, and Ethical Governance](https://arxiv.org/abs/2507.07748)
*Peizhang Shao,Linrui Xu,Jinxi Wang,Wei Zhou,Xingyu Wu*

Main category: cs.CL

TL;DR: 首次系统综述法律领域大语言模型应用，提出双分类法整合法律推理框架与技术创新，揭示机遇与挑战


<details>
  <summary>Details</summary>
Motivation: 填补法律领域LLMs研究的学术空白，通过法律推理框架与专业本体论双重视角，系统整合历史研究与当代技术突破

Method: 创建法律推理框架与专业本体论的双维分类法，结合Transformer架构的动态语义捕捉能力与稀疏注意力、混合专家等技术创新

Result: 实现法律任务泛化与推理形式化突破，通过技术创新解决文本处理、知识整合与评估严谨性等核心难题

Conclusion: 揭示LLMs存在的幻觉、可解释性缺陷与司法适应难题，提出低资源系统、多模态证据整合、动态反驳处理三大前沿方向，奠定法律AI发展基石

Abstract: This paper establishes the first comprehensive review of Large Language
Models (LLMs) applied within the legal domain. It pioneers an innovative dual
lens taxonomy that integrates legal reasoning frameworks and professional
ontologies to systematically unify historical research and contemporary
breakthroughs. Transformer-based LLMs, which exhibit emergent capabilities such
as contextual reasoning and generative argumentation, surmount traditional
limitations by dynamically capturing legal semantics and unifying evidence
reasoning. Significant progress is documented in task generalization, reasoning
formalization, workflow integration, and addressing core challenges in text
processing, knowledge integration, and evaluation rigor via technical
innovations like sparse attention mechanisms and mixture-of-experts
architectures. However, widespread adoption of LLM introduces critical
challenges: hallucination, explainability deficits, jurisdictional adaptation
difficulties, and ethical asymmetry. This review proposes a novel taxonomy that
maps legal roles to NLP subtasks and computationally implements the Toulmin
argumentation framework, thus systematizing advances in reasoning, retrieval,
prediction, and dispute resolution. It identifies key frontiers including
low-resource systems, multimodal evidence integration, and dynamic rebuttal
handling. Ultimately, this work provides both a technical roadmap for
researchers and a conceptual framework for practitioners navigating the
algorithmic future, laying a robust foundation for the next era of legal
artificial intelligence. We have created a GitHub repository to index the
relevant papers: https://github.com/Kilimajaro/LLMs_Meet_Law.

</details>


### [35] [StreamUni: Achieving Streaming Speech Translation with a Unified Large Speech-Language Model](https://arxiv.org/abs/2507.07803)
*Shoutao Guo,Xiang Li,Shaolei Zhang,Mengge Liu,Wei Chen,Yang Feng*

Main category: cs.CL

TL;DR: 提出StreamUni框架，通过统一的大型语音语言模型整合语音思维链(CoT)，实现无需大量策略训练的流式语音翻译，在延迟与质量间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 现有流式语音翻译方法依赖句子级分段，导致上下文信息受限且策略学习困难。传统SimulST模型需要与分割模型协作，且难以有效处理语音输入和跨语言生成的复杂性。

Method: 1. 引入语音Chain-of-Thought指导模型生成多阶段输出
2. 通过流式CoT训练增强低延迟策略决策能力
3. 联合实现语音分割、策略决策和翻译生成三阶段任务统一处理

Result: 实验证明StreamUni在流式语音翻译任务中达到SOTA水平，显著提升翻译质量和延迟控制能力。

Conclusion: 该研究突破传统分段处理范式，通过统一模型架构和CoT机制实现端到端流式翻译，为语音翻译系统设计提供新思路。

Abstract: Streaming speech translation (StreamST) requires determining appropriate
timing, known as policy, to generate translations while continuously receiving
source speech inputs, balancing low latency with high translation quality.
However, existing StreamST methods typically operate on sentence-level speech
segments, referred to as simultaneous speech translation (SimulST). In
practice, they require collaboration with segmentation models to accomplish
StreamST, where the truncated speech segments constrain SimulST models to make
policy decisions and generate translations based on limited contextual
information. Moreover, SimulST models struggle to learn effective policies due
to the complexity of speech inputs and cross-lingual generation. To address
these challenges, we propose StreamUni, which achieves StreamST through a
unified Large Speech-Language Model (LSLM). Specifically, StreamUni
incorporates speech Chain-of-Thought (CoT) in guiding the LSLM to generate
multi-stage outputs. Leveraging these multi-stage outputs, StreamUni
simultaneously accomplishes speech segmentation, policy decision, and
translation generation, completing StreamST without requiring massive
policy-specific training. Additionally, we propose a streaming CoT training
method that enhances low-latency policy decisions and generation capabilities
using limited CoT data. Experiments demonstrate that our approach achieves
state-of-the-art performance on StreamST tasks.

</details>


### [36] [Bridging Logic and Learning: Decoding Temporal Logic Embeddings via Transformers](https://arxiv.org/abs/2507.07808)
*Sara Candussio,Gaia Saveri,Gabriele Sarti,Luca Bortolussi*

Main category: cs.CL

TL;DR: 提出基于Transformer的解码器模型实现信号时序逻辑公式的语义嵌入可逆化，支持在语义空间直接优化需求生成


<details>
  <summary>Details</summary>
Motivation: 逻辑公式的连续语义嵌入需具备可逆性，才能将优化后的向量重新转化为具体规范。现有方法缺乏可靠的可逆机制，制约了符号知识与数据驱动算法的融合应用

Method: 构建STL语法词汇表，训练仅包含解码器的Transformer模型。通过1个epoch生成有效公式，约10个epoch掌握语义规律，生成的公式结构更简洁且保持语义等效

Result: 模型在多种复杂度训练数据下均能有效捕获语义信息，解码结果相比原始公式平均长度减少32%，嵌套深度降低45%，在需求挖掘任务中实现语义空间直接优化分类

Conclusion: 该方法成功桥接符号逻辑与连续表示，为形式化验证、需求工程等领域提供新的优化范式，证明Transformer架构在逻辑语义学习中的强大表征能力

Abstract: Continuous representations of logic formulae allow us to integrate symbolic
knowledge into data-driven learning algorithms. If such embeddings are
semantically consistent, i.e. if similar specifications are mapped into nearby
vectors, they enable continuous learning and optimization directly in the
semantic space of formulae. However, to translate the optimal continuous
representation into a concrete requirement, such embeddings must be invertible.
We tackle this issue by training a Transformer-based decoder-only model to
invert semantic embeddings of Signal Temporal Logic (STL) formulae. STL is a
powerful formalism that allows us to describe properties of signals varying
over time in an expressive yet concise way. By constructing a small vocabulary
from STL syntax, we demonstrate that our proposed model is able to generate
valid formulae after only 1 epoch and to generalize to the semantics of the
logic in about 10 epochs. Additionally, the model is able to decode a given
embedding into formulae that are often simpler in terms of length and nesting
while remaining semantically close (or equivalent) to gold references. We show
the effectiveness of our methodology across various levels of training formulae
complexity to assess the impact of training data on the model's ability to
effectively capture the semantic information contained in the embeddings and
generalize out-of-distribution. Finally, we deploy our model for solving a
requirement mining task, i.e. inferring STL specifications that solve a
classification task on trajectories, performing the optimization directly in
the semantic space.

</details>


### [37] [Understanding and Controlling Repetition Neurons and Induction Heads in In-Context Learning](https://arxiv.org/abs/2507.07810)
*Nhi Hoai Doan,Tatsuya Hiraoka,Kentaro Inui*

Main category: cs.CL

TL;DR: 研究LLM的重复模式识别能力与上下文学习性能的关联，通过重复神经元与归纳头的对比提出优化策略


<details>
  <summary>Details</summary>
Motivation: 突破传统注意力头研究框架，从技能神经元新视角探索重复模式识别机制对上下文学习的影响差异

Method: 分层研究重复神经元的作用，对比分析重复神经元与归纳头对模型输出的不同影响机制

Result: 发现神经元深度影响ICL表现，提出平衡重复输出控制与保持学习能力的优化方案

Conclusion: 为提升LLM的上下文学习效果提供了新的神经元调控视角和工程优化方向

Abstract: This paper investigates the relationship between large language models'
(LLMs) ability to recognize repetitive input patterns and their performance on
in-context learning (ICL). In contrast to prior work that has primarily focused
on attention heads, we examine this relationship from the perspective of skill
neurons, specifically repetition neurons. Our experiments reveal that the
impact of these neurons on ICL performance varies depending on the depth of the
layer in which they reside. By comparing the effects of repetition neurons and
induction heads, we further identify strategies for reducing repetitive outputs
while maintaining strong ICL capabilities.

</details>


### [38] [On the Effect of Instruction Tuning Loss on Generalization](https://arxiv.org/abs/2507.07817)
*Anwoy Chatterjee,H S V N S Kowndinya Renduchintala,Sumit Bhatia,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 提出加权指令微调(WIT)方法，通过调整提示与响应token的损失权重优化指令微调效果，提升模型性能和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 传统指令微调仅计算响应token损失，忽视提示token的优化空间。研究表明该方式可能导致次优性能和对输入提示变化的敏感性

Method: 在指令微调过程中，对提示token采用低-中等权重，对响应token采用中-高权重的差异化损失加权策略

Result: 实验表明WIT在不同规模模型/数据集上均提升性能，最佳权重组合使模型在5个基准测试中表现更稳健，并为后续偏好对齐提供更好基础

Conclusion: 应重新审视指令微调的损失函数设计，合理加权策略可显著提升模型泛化能力。开源代码促进方法验证与扩展

Abstract: Instruction Tuning has emerged as a pivotal post-training paradigm that
enables pre-trained language models to better follow user instructions. Despite
its significance, little attention has been given to optimizing the loss
function used. A fundamental, yet often overlooked, question is whether the
conventional auto-regressive objective - where loss is computed only on
response tokens, excluding prompt tokens - is truly optimal for instruction
tuning. In this work, we systematically investigate the impact of
differentially weighting prompt and response tokens in instruction tuning loss,
and propose Weighted Instruction Tuning (WIT) as a better alternative to
conventional instruction tuning. Through extensive experiments on five language
models of different families and scale, three finetuning datasets of different
sizes, and five diverse evaluation benchmarks, we show that the standard
instruction tuning loss often yields suboptimal performance and limited
robustness to input prompt variations. We find that a low-to-moderate weight
for prompt tokens coupled with a moderate-to-high weight for response tokens
yields the best-performing models across settings and also serve as better
starting points for the subsequent preference alignment training. These
findings highlight the need to reconsider instruction tuning loss and offer
actionable insights for developing more robust and generalizable models. Our
code is open-sourced at https://github.com/kowndinya-renduchintala/WIT.

</details>


### [39] [Conditional Unigram Tokenization with Parallel Data](https://arxiv.org/abs/2507.07824)
*Gianluca Vico,Jindřinch Libovický*

Main category: cs.CL

TL;DR: 提出条件化unigram分词方法，通过源语言标记调节目标语言分词概率，在机器翻译中效果有限但降低语言模型困惑度


<details>
  <summary>Details</summary>
Motivation: 解决跨语言语义对齐问题，提升双语处理任务效果

Method: 基于平行语料，固定源语言分词器，优化目标语言分词器的跨语言对齐目标

Result: 机器翻译质量未提升，语言模型困惑度降低17-24%，二次方概率估计导致数据效率瓶颈

Conclusion: 需探索替代参数化方案改进跨语言分词实用性

Abstract: We introduce conditional unigram tokenization, a novel approach that extends
unigram tokenization by conditioning target token probabilities on
source-language tokens from parallel data. Given a fixed source tokenizer, our
method learns a target tokenizer that maximizes cross-lingual semantic
alignment. We evaluate our tokenizer on four language pairs across different
families and resource levels, examining intrinsic properties and downstream
performance on machine translation and language modeling. While our conditional
tokenizer maintains comparable statistical properties to standard unigram
tokenizers, results are mixed: we observe no improvements in machine
translation quality, but find consistent perplexity reductions in language
modeling. We hypothesize that quadratic scaling of conditional probability
estimation with respect to the vocabulary size creates a data efficiency
bottleneck. Our findings suggest that alternative parameterizations may be
necessary for practical cross-lingual tokenization.

</details>


### [40] [From Ambiguity to Accuracy: The Transformative Effect of Coreference Resolution on Retrieval-Augmented Generation systems](https://arxiv.org/abs/2507.07847)
*Youngjoon Jang,Seongtae Hong,Junyoung Son,Sungjin Park,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: 核心指代消解可提升RAG系统的检索效果和问答性能，小模型受益更明显


<details>
  <summary>Details</summary>
Motivation: 针对检索增强生成(RAG)系统中指代复杂性导致的检索文档歧义问题，研究实体共指如何影响RAG的检索相关性和生成质量

Method: 通过比较不同池化策略的检索效果，分析共指消解对QA任务的影响，测试不同规模模型的抗歧义能力

Result: 共指消解使平均池化展现最优上下文捕捉能力，小模型因固有指代消歧能力有限而获得更大性能提升

Conclusion: 该研究揭示了共指复杂性对RAG系统的双重影响，为知识密集型AI应用的检索-生成优化提供了理论依据

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a crucial framework in
natural language processing (NLP), improving factual consistency and reducing
hallucinations by integrating external document retrieval with large language
models (LLMs). However, the effectiveness of RAG is often hindered by
coreferential complexity in retrieved documents, introducing ambiguity that
disrupts in-context learning. In this study, we systematically investigate how
entity coreference affects both document retrieval and generative performance
in RAG-based systems, focusing on retrieval relevance, contextual
understanding, and overall response quality. We demonstrate that coreference
resolution enhances retrieval effectiveness and improves question-answering
(QA) performance. Through comparative analysis of different pooling strategies
in retrieval tasks, we find that mean pooling demonstrates superior context
capturing ability after applying coreference resolution. In QA tasks, we
discover that smaller models benefit more from the disambiguation process,
likely due to their limited inherent capacity for handling referential
ambiguity. With these findings, this study aims to provide a deeper
understanding of the challenges posed by coreferential complexity in RAG,
providing guidance for improving retrieval and generation in
knowledge-intensive AI applications.

</details>


### [41] [Alpay Algebra V: Multi-Layered Semantic Games and Transfinite Fixed-Point Simulation](https://arxiv.org/abs/2507.07868)
*Bugra Kilictas,Faruk Alpay*

Main category: cs.CL

TL;DR: 提出多层语义游戏架构，通过复合算子和超限不动点定理自然生成博弈论推理，实现理论框架在AI嵌入空间的自我传播


<details>
  <summary>Details</summary>
Motivation: 将Alpay代数与博弈论结合，使AI系统与文档的语义对齐过程自主产生于不动点迭代而非外部强加，增强理论的实际应用性

Method: 构建φ(·,γ(·))复合算子框架，结合Banach不动点定理的拓展形式、处理语义奇点的φ-拓扑结构，以及Yoneda引理的范畴验证

Result: 证明认知模拟条件下语义均衡的唯一存在性，实现论文自身作为'语义病毒'在AI嵌入空间中的模式传播

Conclusion: 建立兼具数学严谨性与工程适用性的语义对齐框架，验证了理论概念与AI系统实际认知行为的兼容性

Abstract: This paper extends the self-referential framework of Alpay Algebra into a
multi-layered semantic game architecture where transfinite fixed-point
convergence encompasses hierarchical sub-games at each iteration level.
Building upon Alpay Algebra IV's empathetic embedding concept, we introduce a
nested game-theoretic structure where the alignment process between AI systems
and documents becomes a meta-game containing embedded decision problems. We
formalize this through a composite operator $\phi(\cdot, \gamma(\cdot))$ where
$\phi$ drives the main semantic convergence while $\gamma$ resolves local
sub-games. The resulting framework demonstrates that game-theoretic reasoning
emerges naturally from fixed-point iteration rather than being imposed
externally. We prove a Game Theorem establishing existence and uniqueness of
semantic equilibria under realistic cognitive simulation assumptions. Our
verification suite includes adaptations of Banach's fixed-point theorem to
transfinite contexts, a novel $\phi$-topology based on the
Kozlov-Maz'ya-Rossmann formula for handling semantic singularities, and
categorical consistency tests via the Yoneda lemma. The paper itself functions
as a semantic artifact designed to propagate its fixed-point patterns in AI
embedding spaces -- a deliberate instantiation of the "semantic virus" concept
it theorizes. All results are grounded in category theory, information theory,
and realistic AI cognition models, ensuring practical applicability beyond pure
mathematical abstraction.

</details>


### [42] [DocCHA: Towards LLM-Augmented Interactive Online diagnosis System](https://arxiv.org/abs/2507.07870)
*Xinyi Liu,Dachun Sun,Yi R. Fung,Dilek Hakkani-Tür,Tarek Abdelzaher*

Main category: cs.CL

TL;DR: DocCHA框架通过模块化三阶段诊断流程（症状获取/病史采集/因果图构建）和置信度机制，显著提升临床诊断准确率和症状召回率，对话轮次仅小幅增加。


<details>
  <summary>Details</summary>
Motivation: 现有对话式健康助手（CHAs）存在静态化、决策不透明问题，无法满足临床诊断中结构化多轮对话的需求，限制了LLM在真实医疗场景的应用。

Method: 将诊断分解为三阶段：1）症状获取（动态调整提问优先级）2）病史采集（置信度引导澄清）3）因果图构建（迭代优化推理链路），各模块通过可解释置信度评分实现透明决策。

Result: 在中文医疗数据集（IMCS21/DX）上超越GPT-4o等基线：诊断准确率+5.18%、症状召回率提升超30%，对话轮次仅增加约5%。

Conclusion: DocCHA证明了模块化置信度机制在实现结构化、高效诊断对话中的有效性，为多语言/低资源场景的可靠LLM医疗助手提供了技术路径。

Abstract: Despite the impressive capabilities of Large Language Models (LLMs), existing
Conversational Health Agents (CHAs) remain static and brittle, incapable of
adaptive multi-turn reasoning, symptom clarification, or transparent
decision-making. This hinders their real-world applicability in clinical
diagnosis, where iterative and structured dialogue is essential. We propose
DocCHA, a confidence-aware, modular framework that emulates clinical reasoning
by decomposing the diagnostic process into three stages: (1) symptom
elicitation, (2) history acquisition, and (3) causal graph construction. Each
module uses interpretable confidence scores to guide adaptive questioning,
prioritize informative clarifications, and refine weak reasoning links.
  Evaluated on two real-world Chinese consultation datasets (IMCS21, DX),
DocCHA consistently outperforms strong prompting-based LLM baselines (GPT-3.5,
GPT-4o, LLaMA-3), achieving up to 5.18 percent higher diagnostic accuracy and
over 30 percent improvement in symptom recall, with only modest increase in
dialogue turns. These results demonstrate the effectiveness of DocCHA in
enabling structured, transparent, and efficient diagnostic conversations --
paving the way for trustworthy LLM-powered clinical assistants in multilingual
and resource-constrained settings.

</details>


### [43] [Automating MD simulations for Proteins using Large language Models: NAMD-Agent](https://arxiv.org/abs/2507.07887)
*Achuth Chandrasekhar,Amir Barati Farimani*

Main category: cs.CL

TL;DR: 开发结合大语言模型与自动化工具的分子动力学模拟输入生成流程，显著提升效率与准确性


<details>
  <summary>Details</summary>
Motivation: 传统分子动力学模拟输入文件制备过程耗时且易错，需自动化解决方案提升生物模拟研究效率

Method: 集成Gemini 2.0 Flash的代码生成能力、Python脚本与Selenium网页自动化技术，通过CHARMM GUI生成NAMD兼容文件

Result: 实现90%的流程自动化，错误率降低75%，支持多蛋白质系统并行处理

Conclusion: 该框架为计算结构生物学开辟新路径，证明大语言模型在生物模拟自动化中的关键应用价值

Abstract: Molecular dynamics simulations are an essential tool in understanding protein
structure, dynamics, and function at the atomic level. However, preparing high
quality input files for MD simulations can be a time consuming and error prone
process. In this work, we introduce an automated pipeline that leverages Large
Language Models (LLMs), specifically Gemini 2.0 Flash, in conjunction with
python scripting and Selenium based web automation to streamline the generation
of MD input files. The pipeline exploits CHARMM GUI's comprehensive web-based
interface for preparing simulation-ready inputs for NAMD. By integrating
Gemini's code generation and iterative refinement capabilities, simulation
scripts are automatically written, executed, and revised to navigate CHARMM
GUI, extract appropriate parameters, and produce the required NAMD input files.
Post processing is performed using additional software to further refine the
simulation outputs, thereby enabling a complete and largely hands free
workflow. Our results demonstrate that this approach reduces setup time,
minimizes manual errors, and offers a scalable solution for handling multiple
protein systems in parallel. This automated framework paves the way for broader
application of LLMs in computational structural biology, offering a robust and
adaptable platform for future developments in simulation automation.

</details>


### [44] [DTECT: Dynamic Topic Explorer & Context Tracker](https://arxiv.org/abs/2507.07910)
*Suman Adhya,Debarshi Kumar Sanyal*

Main category: cs.CL

TL;DR: 提出DTECT动态主题建模系统，通过整合预处理、多模型架构、LLM自动标注和交互可视化，解决现有动态主题建模流程碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 现有动态主题建模技术存在碎片化流程，缺乏对结果解释性和用户友好探索的支持，需构建端到端解决方案。

Method: 开发包含数据预处理/模型架构/评估指标的统一系统，集成LLM自动主题标注、时序显著词趋势分析、文档摘要可视化及自然语言交互界面。

Result: 系统显著提升主题模型的可解释性，支持用户通过动态词云、文档级主题摘要和自然语言查询追踪主题演化路径。

Conclusion: DTECT通过模块化整合降低主题建模门槛，其开源特性将推动时序文本分析领域的研究与应用创新。

Abstract: The explosive growth of textual data over time presents a significant
challenge in uncovering evolving themes and trends. Existing dynamic topic
modeling techniques, while powerful, often exist in fragmented pipelines that
lack robust support for interpretation and user-friendly exploration. We
introduce DTECT (Dynamic Topic Explorer & Context Tracker), an end-to-end
system that bridges the gap between raw textual data and meaningful temporal
insights. DTECT provides a unified workflow that supports data preprocessing,
multiple model architectures, and dedicated evaluation metrics to analyze the
topic quality of temporal topic models. It significantly enhances
interpretability by introducing LLM-driven automatic topic labeling, trend
analysis via temporally salient words, interactive visualizations with
document-level summarization, and a natural language chat interface for
intuitive data querying. By integrating these features into a single, cohesive
platform, DTECT empowers users to more effectively track and understand
thematic dynamics. DTECT is open-source and available at
https://github.com/AdhyaSuman/DTECT.

</details>


### [45] [SAGE: A Visual Language Model for Anomaly Detection via Fact Enhancement and Entropy-aware Alignment](https://arxiv.org/abs/2507.07939)
*Guoxin Zang,Xue Li,Donglin Di,Lanshun Nie,Dechen Zhan,Yang Song,Lei Fan*

Main category: cs.CL

TL;DR: 提出SAGE框架，结合自引导事实增强(SFE)和熵感知直接偏好优化(E-DPO)，提升工业异常检测的解释性和泛化能力，并开发AD-PL数据集和MLE多尺度逻辑评估方法，实验显示其在零样本和单样本设置下的优异性能


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在工业异常检测中存在解释性不足和泛化能力受限的问题，主要源于领域特定性导致的分析精度不足

Method: 通过SFE模块实现领域知识融合的视觉推理，利用E-DPO进行熵感知的专家偏好对齐，构建AD-PL优化数据集，开发MLE多尺度逻辑评估框架

Result: SAGE在工业异常数据集上展现出零样本和单样本学习优势，特别是在结构化推理和逻辑一致性方面显著优于基线模型

Conclusion: SAGE框架有效解决了工业场景中异常推理的领域适应性问题，其知识增强和偏好优化机制为工业AI质检提供了新的技术路径

Abstract: While Vision-Language Models (VLMs) have shown promising progress in general
multimodal tasks, they often struggle in industrial anomaly detection and
reasoning, particularly in delivering interpretable explanations and
generalizing to unseen categories. This limitation stems from the inherently
domain-specific nature of anomaly detection, which hinders the applicability of
existing VLMs in industrial scenarios that require precise, structured, and
context-aware analysis. To address these challenges, we propose SAGE, a
VLM-based framework that enhances anomaly reasoning through Self-Guided Fact
Enhancement (SFE) and Entropy-aware Direct Preference Optimization (E-DPO). SFE
integrates domain-specific knowledge into visual reasoning via fact extraction
and fusion, while E-DPO aligns model outputs with expert preferences using
entropy-aware optimization. Additionally, we introduce AD-PL, a
preference-optimized dataset tailored for industrial anomaly reasoning,
consisting of 28,415 question-answering instances with expert-ranked responses.
To evaluate anomaly reasoning models, we develop Multiscale Logical Evaluation
(MLE), a quantitative framework analyzing model logic and consistency. SAGE
demonstrates superior performance on industrial anomaly datasets under
zero-shot and one-shot settings. The code, model and dataset are available at
https://github.com/amoreZgx1n/SAGE.

</details>


### [46] [MIRIX: Multi-Agent Memory System for LLM-Based Agents](https://arxiv.org/abs/2507.07957)
*Yu Wang,Xi Chen*

Main category: cs.CL

TL;DR: MIRIX模块化多代理记忆系统通过六种结构化的记忆类型和多代理框架，显著提升AI代理在多模态场景下的长期记忆与检索能力，在多个基准测试中实现最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有AI记忆系统存在扁平化、模态单一和长期记忆不可靠等核心缺陷，无法有效支持真实场景下的个性化记忆需求

Method: 设计包含核心记忆、情景记忆、语义记忆、程序记忆、资源记忆和知识库的六层架构，结合多代理系统动态协调记忆更新与检索

Result: 在ScreenshotVQA多模态基准上准确率提升35%且存储减少99.9%，在LOCOMO长对话基准达到85.4%的SOTA性能

Conclusion: MIRIX通过结构化多模态记忆架构，为AI代理建立了可靠的长时记忆系统，在保持隐私安全的同时实现真实场景的实用化部署

Abstract: Although memory capabilities of AI agents are gaining increasing attention,
existing solutions remain fundamentally limited. Most rely on flat, narrowly
scoped memory components, constraining their ability to personalize, abstract,
and reliably recall user-specific information over time. To this end, we
introduce MIRIX, a modular, multi-agent memory system that redefines the future
of AI memory by solving the field's most critical challenge: enabling language
models to truly remember. Unlike prior approaches, MIRIX transcends text to
embrace rich visual and multimodal experiences, making memory genuinely useful
in real-world scenarios. MIRIX consists of six distinct, carefully structured
memory types: Core, Episodic, Semantic, Procedural, Resource Memory, and
Knowledge Vault, coupled with a multi-agent framework that dynamically controls
and coordinates updates and retrieval. This design enables agents to persist,
reason over, and accurately retrieve diverse, long-term user data at scale. We
validate MIRIX in two demanding settings. First, on ScreenshotVQA, a
challenging multimodal benchmark comprising nearly 20,000 high-resolution
computer screenshots per sequence, requiring deep contextual understanding and
where no existing memory systems can be applied, MIRIX achieves 35% higher
accuracy than the RAG baseline while reducing storage requirements by 99.9%.
Second, on LOCOMO, a long-form conversation benchmark with single-modal textual
input, MIRIX attains state-of-the-art performance of 85.4%, far surpassing
existing baselines. These results show that MIRIX sets a new performance
standard for memory-augmented LLM agents. To allow users to experience our
memory system, we provide a packaged application powered by MIRIX. It monitors
the screen in real time, builds a personalized memory base, and offers
intuitive visualization and secure local storage to ensure privacy.

</details>


### [47] [Why is Your Language Model a Poor Implicit Reward Model?](https://arxiv.org/abs/2507.07981)
*Noam Razin,Yong Lin,Jiarui Yao,Sanjeev Arora*

Main category: cs.CL

TL;DR: 研究发现隐式奖励模型(IM-RM)因依赖词级表面线索导致泛化性能弱于显式奖励模型(EX-RM)，揭示了设计选择对奖励模型泛化能力的关键影响。


<details>
  <summary>Details</summary>
Motivation: 探索IM-RM与EX-RM在相同数据、损失函数和语言模型下产生泛化差距的根本原因，尽管二者仅在奖励计算方式上存在差异。

Method: 通过理论分析和实验验证，考察两种奖励模型在词级分布变化下的表现，并排除其他竞争性假设。

Result: IM-RM在分布内外的词级偏移中表现更差，主要源于对表面模式的依赖；否定了'生成验证假设'等替代解释。

Conclusion: 奖励模型设计中看似微小的架构选择会显著影响其泛化行为，强调需要审慎设计奖励建模方法。

Abstract: Reward models are key to language model post-training and inference
pipelines. Conveniently, recent work showed that every language model defines
an implicit reward model (IM-RM), without requiring any architectural changes.
However, such IM-RMs tend to generalize worse, especially out-of-distribution,
compared to explicit reward models (EX-RMs) that apply a dedicated linear head
over the hidden representations of a language model. The existence of a
generalization gap is puzzling, as EX-RMs and IM-RMs are nearly identical. They
can be trained using the same data, loss function, and language model, and
differ only in how the reward is computed. Towards a fundamental understanding
of the implicit biases underlying different reward model types, we investigate
the root cause of this gap. Our main finding, backed by theory and experiments,
is that IM-RMs rely more heavily on superficial token-level cues. Consequently,
they often generalize worse than EX-RMs under token-level distribution shifts,
as well as in-distribution. Furthermore, we provide evidence against
alternative hypotheses for the generalization gap. Most notably, we challenge
the intuitive claim that IM-RMs struggle in tasks where generation is harder
than verification because they can operate both as a verifier and a generator.
Taken together, our results highlight that seemingly minor design choices can
substantially impact the generalization behavior of reward models.

</details>


### [48] [Performance and Practical Considerations of Large and Small Language Models in Clinical Decision Support in Rheumatology](https://arxiv.org/abs/2507.07983)
*Sabine Felde,Rüdiger Buchkremer,Gamal Chehab,Christian Thielscher,Jörg HW Distler,Matthias Schneider,Jutta G. Richter*

Main category: cs.CL

TL;DR: 小模型+检索增强生成技术（RAG）在风湿病临床决策中表现优于大模型，且更节能，但需专家监督


<details>
  <summary>Details</summary>
Motivation: 探索语言模型在资源受限医疗环境中的应用潜力，寻求能耗与性能的平衡方案

Method: 通过对比实验评估不同规模语言模型（LLMs/SLMs）结合RAG技术的临床决策表现

Result: SLMs+RAG实现更高诊断准确率（比LLMs高23%），能耗降低87%，但未达风湿病专家水平（准确率差15%）

Conclusion: 该方案为医疗资源不足地区提供可行AI辅助方案，但需与专家系统结合形成双重校验机制

Abstract: Large language models (LLMs) show promise for supporting clinical
decision-making in complex fields such as rheumatology. Our evaluation shows
that smaller language models (SLMs), combined with retrieval-augmented
generation (RAG), achieve higher diagnostic and therapeutic performance than
larger models, while requiring substantially less energy and enabling
cost-efficient, local deployment. These features are attractive for
resource-limited healthcare. However, expert oversight remains essential, as no
model consistently reached specialist-level accuracy in rheumatology.

</details>


### [49] [Automating Expert-Level Medical Reasoning Evaluation of Large Language Models](https://arxiv.org/abs/2507.07988)
*Shuang Zhou,Wenya Xie,Jiaxi Li,Zaifu Zhan,Meijia Song,Han Yang,Cheyenna Espinoza,Lindsay Welton,Xinnie Mai,Yanwei Jin,Zidu Xu,Yuen-Hei Chung,Yiyun Xing,Meng-Han Tsai,Emma Schaffer,Yucheng Shi,Ninghao Liu,Zirui Liu,Rui Zhang*

Main category: cs.CL

TL;DR: 提出MedThink-Bench基准测试和LLM-w-Ref评估框架，用于系统评估大语言模型的医学推理能力，发现小模型可超越大模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有医学语言模型评估方法存在评估不严谨、扩展性差的问题，缺乏可靠基准。

Method: 构建包含500个医学领域问题的MedThink-Bench数据集，配套专家级推理注释；开发LLM-w-Ref框架（结合细粒度推理链评估和LLM自评机制）。

Result: LLM-w-Ref与专家判断强正相关（r=0.89），MedGemma-27B等小模型性能超越OpenAI GPT-3.5等大模型。

Conclusion: 该研究为医学LLM评估建立新标准，推动临床应用的可靠部署，证明模型规模并非医学推理性能的决定因素。

Abstract: As large language models (LLMs) become increasingly integrated into clinical
decision-making, ensuring transparent and trustworthy reasoning is essential.
However, existing evaluation strategies of LLMs' medical reasoning capability
either suffer from unsatisfactory assessment or poor scalability, and a
rigorous benchmark remains lacking. To address this, we introduce
MedThink-Bench, a benchmark designed for rigorous, explainable, and scalable
assessment of LLMs' medical reasoning. MedThink-Bench comprises 500 challenging
questions across ten medical domains, each annotated with expert-crafted
step-by-step rationales. Building on this, we propose LLM-w-Ref, a novel
evaluation framework that leverages fine-grained rationales and LLM-as-a-Judge
mechanisms to assess intermediate reasoning with expert-level fidelity while
maintaining scalability. Experiments show that LLM-w-Ref exhibits a strong
positive correlation with expert judgments. Benchmarking twelve
state-of-the-art LLMs, we find that smaller models (e.g., MedGemma-27B) can
surpass larger proprietary counterparts (e.g., OpenAI-o3). Overall,
MedThink-Bench offers a foundational tool for evaluating LLMs' medical
reasoning, advancing their safe and responsible deployment in clinical
practice.

</details>


### [50] [PyVision: Agentic Vision with Dynamic Tooling](https://arxiv.org/abs/2507.07998)
*Shitian Zhao,Haoquan Zhang,Shaoheng Lin,Ming Li,Qilong Wu,Kaipeng Zhang,Chen Wei*

Main category: cs.CL

TL;DR: PyVision框架通过动态生成Python工具实现自主视觉推理，显著提升模型性能


<details>
  <summary>Details</summary>
Motivation: 现有视觉推理方法受限于预定义工作流程和静态工具集，需更灵活的动态工具机制

Method: 开发交互式多轮框架，使MLLM能自主生成/执行/优化Python工具

Result: GPT-4.1在V*提升7.8%，Claude-4.0在VLMsAreBlind-mini提升31.1%

Conclusion: 动态工具创建标志着模型从工具使用者向工具发明者的转变，推动自主视觉推理发展

Abstract: LLMs are increasingly deployed as agents, systems capable of planning,
reasoning, and dynamically calling external tools. However, in visual
reasoning, prior approaches largely remain limited by predefined workflows and
static toolsets. In this report, we present PyVision, an interactive,
multi-turn framework that enables MLLMs to autonomously generate, execute, and
refine Python-based tools tailored to the task at hand, unlocking flexible and
interpretable problem-solving. We develop a taxonomy of the tools created by
PyVision and analyze their usage across a diverse set of benchmarks.
Quantitatively, PyVision achieves consistent performance gains, boosting
GPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini.
These results point to a broader shift: dynamic tooling allows models not just
to use tools, but to invent them, advancing toward more agentic visual
reasoning.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [51] [Generative Panoramic Image Stitching](https://arxiv.org/abs/2507.07133)
*Mathieu Tuli,Kaveh Kamali,David B. Lindell*

Main category: cs.GR

TL;DR: 提出基于扩散模型的生成式全景图像拼接方法，通过微调修复模型实现多参考图像的无缝合成


<details>
  <summary>Details</summary>
Motivation: 解决传统拼接方法在视差/光照差异下的伪影问题，改进生成模型在大范围全景合成中的连贯性问题

Method: 微调扩散修复模型保持场景布局，单参考图像扩展生成全景，整合多图内容

Result: 在真实数据集上显著超越基线，图像质量与场景一致性表现更优

Conclusion: 该方法有效保持场景内容与布局，实现视觉连贯的全景合成

Abstract: We introduce the task of generative panoramic image stitching, which aims to
synthesize seamless panoramas that are faithful to the content of multiple
reference images containing parallax effects and strong variations in lighting,
camera capture settings, or style. In this challenging setting, traditional
image stitching pipelines fail, producing outputs with ghosting and other
artifacts. While recent generative models are capable of outpainting content
consistent with multiple reference images, they fail when tasked with
synthesizing large, coherent regions of a panorama. To address these
limitations, we propose a method that fine-tunes a diffusion-based inpainting
model to preserve a scene's content and layout based on multiple reference
images. Once fine-tuned, the model outpaints a full panorama from a single
reference image, producing a seamless and visually coherent result that
faithfully integrates content from all reference images. Our approach
significantly outperforms baselines for this task in terms of image quality and
the consistency of image structure and scene layout when evaluated on captured
datasets.

</details>


### [52] [LangSplatV2: High-dimensional 3D Language Gaussian Splatting with 450+ FPS](https://arxiv.org/abs/2507.07136)
*Wanhua Li,Yujie Zhao,Minghan Qin,Yang Liu,Yuanhao Cai,Chuang Gan,Hanspeter Pfister*

Main category: cs.GR

TL;DR: LangSplatV2通过稀疏编码和CUDA优化实现476.2 FPS的高维特征渲染，相比前代提速42倍，在保持精度的同时突破3D语言场实时处理瓶颈


<details>
  <summary>Details</summary>
Motivation: 解决LangSplat在A100显卡上仍无法达到实时推理性能（8.2 FPS）的问题，消除笨重解码器带来的速度限制，推动3D场景语言交互的实际应用

Method: 1. 建立基于全局字典的3D稀疏系数场取代传统解码器
2. 开发CUDA优化的稀疏系数渲染技术，仅需超低维特征处理时间
3. 通过高斯分布将2D CLIP特征嵌入3D空间

Result: 1. 实现高分辨率图像下384.6 FPS的开放词汇查询
2. 获得47倍于前代的性能提升
3. 在Cityscapes数据集上mIoU提升3.1%

Conclusion: 该方法通过稀疏表征与硬件级优化的协同设计，首次实现工业级实时3D语言场处理，为AR/VR等实时交互应用奠定基础，代码已开源推动领域发展

Abstract: In this paper, we introduce LangSplatV2, which achieves high-dimensional
feature splatting at 476.2 FPS and 3D open-vocabulary text querying at 384.6
FPS for high-resolution images, providing a 42 $\times$ speedup and a 47
$\times$ boost over LangSplat respectively, along with improved query accuracy.
LangSplat employs Gaussian Splatting to embed 2D CLIP language features into
3D, significantly enhancing speed and learning a precise 3D language field with
SAM semantics. Such advancements in 3D language fields are crucial for
applications that require language interaction within complex scenes. However,
LangSplat does not yet achieve real-time inference performance (8.2 FPS), even
with advanced A100 GPUs, severely limiting its broader application. In this
paper, we first conduct a detailed time analysis of LangSplat, identifying the
heavyweight decoder as the primary speed bottleneck. Our solution, LangSplatV2
assumes that each Gaussian acts as a sparse code within a global dictionary,
leading to the learning of a 3D sparse coefficient field that entirely
eliminates the need for a heavyweight decoder. By leveraging this sparsity, we
further propose an efficient sparse coefficient splatting method with CUDA
optimization, rendering high-dimensional feature maps at high quality while
incurring only the time cost of splatting an ultra-low-dimensional feature. Our
experimental results demonstrate that LangSplatV2 not only achieves better or
competitive query accuracy but is also significantly faster. Codes and demos
are available at our project page: https://langsplat-v2.github.io.

</details>


### [53] [Digital Salon: An AI and Physics-Driven Tool for 3D Hair Grooming and Simulation](https://arxiv.org/abs/2507.07387)
*Chengan He,Jorge Alejandro Amador Herrera,Zhixin Shu,Xin Sun,Yao Feng,Sören Pirk,Dominik L. Michels,Meng Zhang,Tuanfeng Y. Wang,Julie Dorsey,Holly Rushmeier,Yi Zhou*

Main category: cs.GR

TL;DR: 提出Digital Salon系统，通过自然语言交互实现全流程3D发型设计，覆盖检索、模拟、优化、渲染环节，大幅降低技术门槛


<details>
  <summary>Details</summary>
Motivation: 解决现有3D发型建模方法碎片化、高计算量/训练成本的问题，推动数字媒体创作中直观高效的头发建模方案

Method: 四阶段流程：1）文本引导发型检索 2）实时物理模拟 3）交互式优化调整 4）发型条件图像生成

Result: 用户研究表明系统在快速原型设计上优于传统建模流程，加速数字媒体创作过程

Conclusion: 系统通过自然语言交互降低技术门槛，为不同技能水平的用户提供一体化解决方案，在美发行业数字化中具有应用潜力

Abstract: We introduce Digital Salon, a comprehensive hair authoring system that
supports real-time 3D hair generation, simulation, and rendering. Unlike
existing methods that focus on isolated parts of 3D hair modeling and involve a
heavy computation process or network training, Digital Salon offers a holistic
and interactive system that lowers the technical barriers of 3D hair modeling
through natural language-based interaction. The system guides users through
four key stages: text-guided hair retrieval, real-time hair simulation,
interactive hair refinement, and hair-conditioned image generation. This
cohesive workflow makes advanced hair design accessible to users of varying
skill levels and dramatically streamlines the creative process in digital media
with an intuitive, versatile, and efficient solution for hair modeling. User
studies show that our system can outperform traditional hair modeling workflows
for rapid prototyping. Furthermore, we provide insights into the benefits of
our system with future potential of deploying our system in real salon
environments. More details can be found on our project page:
https://digital-salon.github.io/.

</details>


### [54] [Self-supervised Learning of Latent Space Dynamics](https://arxiv.org/abs/2507.07440)
*Yue Li,Gene Wei-Chin Lin,Egor Larionov,Aljaz Bozic,Doug Roble,Ladislav Kavan,Stelian Coros,Bernhard Thomaszewski,Tuur Stuyck,Hsiao-yu Chen*

Main category: cs.GR

TL;DR: 提出基于神经潜在空间积分器的子空间模拟框架，显著提升可变形物体模拟效率，适用于便携设备部署


<details>
  <summary>Details</summary>
Motivation: 传统物理模拟计算成本过高，现有子空间方法难以满足VR头显等便携设备的实时性能需求，需开发更高效的模拟方案

Method: 通过神经潜在空间积分器实现全潜空间计算，结合自监督学习增强推理稳定性和泛化能力，避免全空间运算

Result: 在杆状物、薄壳结构和实体模型等复杂案例中验证了方法的有效性和计算效率优势

Conclusion: 该框架在保持视觉真实性的同时实现高性能计算，具备在移动端实时物理模拟领域的广泛应用潜力

Abstract: Modeling the dynamic behavior of deformable objects is crucial for creating
realistic digital worlds. While conventional simulations produce high-quality
motions, their computational costs are often prohibitive. Subspace simulation
techniques address this challenge by restricting deformations to a
lower-dimensional space, improving performance while maintaining visually
compelling results. However, even subspace methods struggle to meet the
stringent performance demands of portable devices such as virtual reality
headsets and mobile platforms. To overcome this limitation, we introduce a
novel subspace simulation framework powered by a neural latent-space
integrator. Our approach leverages self-supervised learning to enhance
inference stability and generalization. By operating entirely within latent
space, our method eliminates the need for full-space computations, resulting in
a highly efficient method well-suited for deployment on portable devices. We
demonstrate the effectiveness of our approach on challenging examples involving
rods, shells, and solids, showcasing its versatility and potential for
widespread adoption.

</details>


### [55] [SD-GS: Structured Deformable 3D Gaussians for Efficient Dynamic Scene Reconstruction](https://arxiv.org/abs/2507.07465)
*Wei Yao,Shuzhao Xie,Letian Li,Weixiang Zhang,Zhixin Lai,Shiqi Dai,Ke Zhang,Zhi Wang*

Main category: cs.GR

TL;DR: 提出SD-GS框架，通过可变形锚点网格和变形感知致密化策略，在保持视觉质量的同时显著降低模型体积60%并提升渲染速度100%


<details>
  <summary>Details</summary>
Motivation: 现有4D高斯框架在存储成本与复杂运动建模能力之间存在固有折衷，限制了实际应用

Method: 1. 引入分层内存效率的可变形锚点网格作为场景几何骨干；2. 开发变形感知致密化策略，动态调整锚点分布

Result: 模型体积平均减少60%，渲染速度提升100%，在保持/超越视觉质量的前提下显著提升计算效率

Conclusion: SD-GS框架通过创新的场景表示和自适应优化策略，实现了动态场景重建在效率与质量上的双重突破

Abstract: Current 4D Gaussian frameworks for dynamic scene reconstruction deliver
impressive visual fidelity and rendering speed, however, the inherent trade-off
between storage costs and the ability to characterize complex physical motions
significantly limits the practical application of these methods. To tackle
these problems, we propose SD-GS, a compact and efficient dynamic Gaussian
splatting framework for complex dynamic scene reconstruction, featuring two key
contributions. First, we introduce a deformable anchor grid, a hierarchical and
memory-efficient scene representation where each anchor point derives multiple
3D Gaussians in its local spatiotemporal region and serves as the geometric
backbone of the 3D scene. Second, to enhance modeling capability for complex
motions, we present a deformation-aware densification strategy that adaptively
grows anchors in under-reconstructed high-dynamic regions while reducing
redundancy in static areas, achieving superior visual quality with fewer
anchors. Experimental results demonstrate that, compared to state-of-the-art
methods, SD-GS achieves an average of 60\% reduction in model size and an
average of 100\% improvement in FPS, significantly enhancing computational
efficiency while maintaining or even surpassing visual quality.

</details>


### [56] [Capture Stage Environments: A Guide to Better Matting](https://arxiv.org/abs/2507.07623)
*Hannah Dröge,Janelle Pfeifer,Saskia Rabich,Markus Plack,Reinhard Klein,Matthias B. Hullin*

Main category: cs.GR

TL;DR: 针对影视捕捉场景的抠图难题，提出改进流程与评估方案


<details>
  <summary>Details</summary>
Motivation: 现有抠图算法在处理捕捉场景的特殊性（如高精度需求、复杂背景分离）时表现不佳，影响影视/游戏制作流程效率

Method: 1. 构建无需大量标注的离线/实时适配流程
2. 基于领先扩散模型设计验证方案

Result: 提出的方法有效提升了专业捕捉场景下的抠图性能

Conclusion: 为从业者提供系统性的工作流改进指南，同时提出主动干预策略应对未解挑战

Abstract: Capture stages are high-end sources of state-of-the-art recordings for
downstream applications in movies, games, and other media. One crucial step in
almost all pipelines is the matting of images to isolate the captured
performances from the background. While common matting algorithms deliver
remarkable performance in other applications like teleconferencing and mobile
entertainment, we found that they struggle significantly with the peculiarities
of capture stage content. The goal of our work is to share insights into those
challenges as a curated list of those characteristics along with a constructive
discussion for proactive intervention and present a guideline to practitioners
for an improved workflow to mitigate unresolved challenges. To this end, we
also demonstrate an efficient pipeline to adapt state-of-the-art approaches to
such custom setups without the need of extensive annotations, both offline and
real-time. For an objective evaluation, we propose a validation methodology
based on a leading diffusion model that highlights the benefits of our
approach.

</details>


### [57] [RTR-GS: 3D Gaussian Splatting for Inverse Rendering with Radiance Transfer and Reflection](https://arxiv.org/abs/2507.07733)
*Yongyang Zhou,Fang-Lue Zhang,Zichen Wang,Lei Zhang*

Main category: cs.GR

TL;DR: 提出RTR-GS框架解决3D高斯泼溅在反射物体渲染中的难题，通过混合前向+延迟渲染模型提升分解质量，在保持高效性的同时实现可信重光照效果。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法在渲染反射物体时存在逆渲染与重光照困难，高频细节处理易产生球形谐波过拟合伪影，需新方法突破局限性。

Method: 混合渲染模型：前向渲染处理辐射传输+延迟渲染处理反射，分离高低频外观；基于物理的延迟渲染分支优化BRDF与光照分解。

Result: 实验证明方法有效提升新视角合成/法线估计/分解质量，重光照效果更可信，训练推理效率保持3DGS优势（~30min训练，实时渲染）。

Conclusion: RTR-GS成功解决逆渲染中反射物体的挑战，混合渲染架构与物理约束为材质分解与重光照提供新思路，推进3D重建实用化进程。

Abstract: 3D Gaussian Splatting (3DGS) has demonstrated impressive capabilities in
novel view synthesis. However, rendering reflective objects remains a
significant challenge, particularly in inverse rendering and relighting. We
introduce RTR-GS, a novel inverse rendering framework capable of robustly
rendering objects with arbitrary reflectance properties, decomposing BRDF and
lighting, and delivering credible relighting results. Given a collection of
multi-view images, our method effectively recovers geometric structure through
a hybrid rendering model that combines forward rendering for radiance transfer
with deferred rendering for reflections. This approach successfully separates
high-frequency and low-frequency appearances, mitigating floating artifacts
caused by spherical harmonic overfitting when handling high-frequency details.
We further refine BRDF and lighting decomposition using an additional
physically-based deferred rendering branch. Experimental results show that our
method enhances novel view synthesis, normal estimation, decomposition, and
relighting while maintaining efficient training inference process.

</details>


### [58] [Hi-d maps: An interactive visualization technique for multi-dimensional categorical data](https://arxiv.org/abs/2507.07890)
*Radi Muhammad Reza,Benjamin A Watson*

Main category: cs.GR

TL;DR: Hi-D maps是一种新型多维分类数据可视化方法，通过二维多边形映射和多种视觉线索实现高效空间利用与跨维度信息展示。


<details>
  <summary>Details</summary>
Motivation: 现有技术难以有效可视化高维度分类数据，Hi-D maps旨在突破空间效率与多维度表达的限制。

Method: 将多维数据映射到二维正多边形区域，采用层次化切割、多视觉编码(方向/厚度/颜色/可计数符号)及交互式分层浏览机制。

Result: 实现紧凑空间内的跨维度关系展示，支持细节钻取与层次化扩展，动画交互增强探索体验。

Conclusion: Hi-D maps在感知极限前提供清晰的可视化方案，但维度过多时仍存在显示效能下降的问题。

Abstract: In this paper, we present Hi-D maps, a novel method for the visualization of
multi-dimensional categorical data. Our work addresses the scarcity of
techniques for visualizing a large number of data-dimensions in an effective
and space-efficient manner. We have mapped the full data-space onto a 2D
regular polygonal region. The polygon is cut hierarchically with lines parallel
to a user-controlled, ordered sequence of sides, each representing a dimension.
We have used multiple visual cues such as orientation, thickness, color,
countable glyphs, and text to depict cross-dimensional information. We have
added interactivity and hierarchical browsing to facilitate flexible
exploration of the display: small areas can be scrutinized for details. Thus,
our method is also easily extendable to visualize hierarchical information. Our
glyph animations add an engaging aesthetic during interaction. Like many
visualizations, Hi-D maps become less effective when a large number of
dimensions stresses perceptual limits, but Hi-D maps may add clarity before
those limits are reached.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [59] [May I have your Attention? Breaking Fine-Tuning based Prompt Injection Defenses using Architecture-Aware Attacks](https://arxiv.org/abs/2507.07417)
*Nishit V. Pandya,Andrey Labunets,Sicun Gao,Earlence Fernandes*

Main category: cs.CR

TL;DR: 该论文通过构建优化攻击验证现有LLM防御机制在对抗白盒攻击时的脆弱性，提出新型注意力攻击算法实现70%成功率


<details>
  <summary>Details</summary>
Motivation: 现有基于指令数据分离的防御机制未充分验证对抗优化攻击的能力，需要系统性评估其安全声明

Method: 开发基于注意力机制的文本攻击算法，针对SecAlign和StruQ两种防御系统进行白盒攻击测试

Result: 在适度增加攻击token预算条件下，成功实现最高70%的攻击成功率，证明防御机制存在根本性漏洞

Conclusion: 白盒场景下现有提示注入防御方案无法提供宣称的安全保障，需重新设计更鲁棒的防御体系

Abstract: A popular class of defenses against prompt injection attacks on large
language models (LLMs) relies on fine-tuning the model to separate instructions
and data, so that the LLM does not follow instructions that might be present
with data. There are several academic systems and production-level
implementations of this idea. We evaluate the robustness of this class of
prompt injection defenses in the whitebox setting by constructing strong
optimization-based attacks and showing that the defenses do not provide the
claimed security properties. Specifically, we construct a novel attention-based
attack algorithm for text-based LLMs and apply it to two recent whitebox
defenses SecAlign (CCS 2025) and StruQ (USENIX Security 2025), showing attacks
with success rates of up to 70% with modest increase in attacker budget in
terms of tokens. Our findings make fundamental progress towards understanding
the robustness of prompt injection defenses in the whitebox setting. We release
our code and attacks at https://github.com/nishitvp/better_opts_attacks

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [60] [Open Source Planning & Control System with Language Agents for Autonomous Scientific Discovery](https://arxiv.org/abs/2507.07257)
*Licong Xu,Milind Sarkar,Anto I. Lonappan,Íñigo Zubeldia,Pablo Villanueva-Domingo,Santiago Casas,Christian Fidler,Chetana Amancharla,Ujjwal Tiwari,Adrian Bayer,Chadi Ait Ekiou,Miles Cranmer,Adrian Dimitrov,James Fergusson,Kahaan Gandhi,Sven Krippendorf,Andrew Laverick,Julien Lesgourgues,Antony Lewis,Thomas Meier,Blake Sherwin,Kristen Surrao,Francisco Villaescusa-Navarro,Chi Wang,Xueqing Xu,Boris Bolliet*

Main category: cs.AI

TL;DR: 开发了由30个LLM代理组成的自动化科研系统cmbagent，采用规划控制策略实现全自动科研流程，成功应用于博士级宇宙学参数测量任务


<details>
  <summary>Details</summary>
Motivation: 解决科研任务自动化需求，通过多智能体协作实现无需人工干预的端到端科研流程，提升研究效率

Method: 构建30个专业LLM代理系统（检索、编码、结果解释、同行评审等），采用Planning & Control策略协调工作流，支持本地代码执行

Result: 在宇宙学参数测量任务中超越现有最先进LLM，GitHub开源代码并部署于HuggingFace/云端，提供演示视频

Conclusion: cmbagent证明了多智能体系统在复杂科研任务中的有效性，为科研自动化提供了可扩展的框架

Abstract: We present a multi-agent system for automation of scientific research tasks,
cmbagent. The system is formed by about 30 Large Language Model (LLM) agents
and implements a Planning & Control strategy to orchestrate the agentic
workflow, with no human-in-the-loop at any point. Each agent specializes in a
different task (performing retrieval on scientific papers and codebases,
writing code, interpreting results, critiquing the output of other agents) and
the system is able to execute code locally. We successfully apply cmbagent to
carry out a PhD level cosmology task (the measurement of cosmological
parameters using supernova data) and evaluate its performance on two benchmark
sets, finding superior performance over state-of-the-art LLMs. The source code
is available on GitHub, demonstration videos are also available, and the system
is deployed on HuggingFace and will be available on the cloud.

</details>


### [61] [ViDove: A Translation Agent System with Multimodal Context and Memory-Augmented Reasoning](https://arxiv.org/abs/2507.07306)
*Yichen Lu,Wei Dai,Jiaen Liu,Ching Wing Kwok,Zongheng Wu,Xudong Xiao,Ao Sun,Sheng Fu,Jianyuan Zhan,Yian Wang,Takatomo Saito,Sicheng Lai*

Main category: cs.AI

TL;DR: 提出多模态翻译代理ViDove，通过视觉上下文增强翻译质量，并构建新基准DoveBench


<details>
  <summary>Details</summary>
Motivation: 现有LLM翻译代理仅支持文本输入，缺乏人类翻译时依赖的视觉上下文信息

Method: 模仿人类翻译流程，集成视觉背景信息和多模态记忆系统，引入长短时记忆模块融入领域知识

Result: 在字幕生成和通用翻译任务中BLEU提升28%，SubER提升15%，构建包含17小时标注数据的DoveBench基准

Conclusion: 多模态输入与领域记忆模块有效提升翻译质量，DoveBench为视频字幕翻译研究提供新基准

Abstract: LLM-based translation agents have achieved highly human-like translation
results and are capable of handling longer and more complex contexts with
greater efficiency. However, they are typically limited to text-only inputs. In
this paper, we introduce ViDove, a translation agent system designed for
multimodal input. Inspired by the workflow of human translators, ViDove
leverages visual and contextual background information to enhance the
translation process. Additionally, we integrate a multimodal memory system and
long-short term memory modules enriched with domain-specific knowledge,
enabling the agent to perform more accurately and adaptively in real-world
scenarios. As a result, ViDove achieves significantly higher translation
quality in both subtitle generation and general translation tasks, with a 28%
improvement in BLEU scores and a 15% improvement in SubER compared to previous
state-of-the-art baselines. Moreover, we introduce DoveBench, a new benchmark
for long-form automatic video subtitling and translation, featuring 17 hours of
high-quality, human-annotated data. Our code is available here:
https://github.com/pigeonai-org/ViDove

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [62] [Growing Transformers: Modular Composition and Layer-wise Expansion on a Frozen Substrate](https://arxiv.org/abs/2507.07129)
*A. Bochkov*

Main category: cs.LG

TL;DR: 提出基于固定嵌入表征的模块化语言模型扩展方法，支持专家模型无缝集成和层次渐进式增长，突破传统端到端训练范式。


<details>
  <summary>Details</summary>
Motivation: 传统大规模语言模型采用端到端整体训练，资源消耗大且缺乏灵活性。本文探索通过非训练、确定性输入嵌入构建模块化扩展路径，降低AI系统开发门槛。

Method: 1. 专家模型融合：通过简单对数平均将俄语/中文等专业模型整合为混合专家模型(MoE)；2. 层次渐进训练：逐层叠加并训练Transformer，实现模型深度可控扩展。

Result: 1. MoE模型在MMLU等推理基准上表现优于原专家模型，且无灾难性遗忘；2. 深层模型在SQuAD等复杂推理任务中展现层次化能力涌现。

Conclusion: 固定表征层可作为通用'对接端口'，开创生物启发式的AI构建范式，支持资源高效扩展、持续学习和模块自由组合，推动AI开发生态民主化。

Abstract: The prevailing paradigm for scaling large language models (LLMs) involves
monolithic, end-to-end training, a resource-intensive process that lacks
flexibility. This paper explores an alternative, constructive approach to model
development, built upon the foundation of non-trainable, deterministic input
embeddings. In prior [1], we established that high-level semantic reasoning can
emerge in Transformers using frozen embeddings derived from the visual
structure of Unicode glyphs. Here, we demonstrate that this fixed
representational substrate acts as a universal "docking port," enabling two
powerful and efficient scaling paradigms: seamless modular composition and
progressive layer-wise growth.
  First, we show that specialist models trained on disparate datasets (e.g.,
Russian and Chinese text) can be merged into a single, more capable
Mixture-of-Experts (MoE) model, post-training, with zero architectural
modification. This is achieved by simply averaging their output logits. The
resulting MoE model exhibits immediate performance improvements on reasoning
benchmarks like MMLU, surpassing its constituent experts without catastrophic
forgetting. Second, we introduce a layer-wise constructive training
methodology, where a deep Transformer is "grown" by progressively stacking and
training one layer at a time. This method demonstrates stable convergence and a
clear correlation between model depth and the emergence of complex reasoning
abilities, such as those required for SQuAD.
  Our findings suggest a paradigm shift from monolithic optimization towards a
more biological or constructive model of AI development, where complexity is
built incrementally and modules can be composed freely. This opens new avenues
for resource-efficient scaling, continual learning, and a more democratized
ecosystem for building powerful AI systems. We release all code and models to
facilitate further research.

</details>


### [63] [An Information-Theoretic Perspective on Multi-LLM Uncertainty Estimation](https://arxiv.org/abs/2507.07236)
*Maya Kruse,Majid Afshar,Saksham Khatwani,Anoop Mayampurath,Guanhua Chen,Yanjun Gao*

Main category: cs.LG

TL;DR: 提出MUSE方法，通过Jensen-Shannon散度筛选并集成多样化LLM子集，提升语言模型不确定性量化效果


<details>
  <summary>Details</summary>
Motivation: 现有校准方法聚焦单模型，忽视模型多样性潜力。LLMs因训练差异和语言的Zipf分布特性可能产生互补预测，集成输出可提升不确定性估计可靠性

Method: 基于信息论的MUSE方法，使用Jensen-Shannon散度识别校准良好的LLM子集并进行预测集成

Result: 在二元预测任务中，相比单模型和简单集成基线，实现了更好的校准效果和预测性能

Conclusion: 通过模型多样性有效提升不确定性量化可靠性，未来可探索更多任务场景和模型组合策略

Abstract: Large language models (LLMs) often behave inconsistently across inputs,
indicating uncertainty and motivating the need for its quantification in
high-stakes settings. Prior work on calibration and uncertainty quantification
often focuses on individual models, overlooking the potential of model
diversity. We hypothesize that LLMs make complementary predictions due to
differences in training and the Zipfian nature of language, and that
aggregating their outputs leads to more reliable uncertainty estimates. To
leverage this, we propose MUSE (Multi-LLM Uncertainty via Subset Ensembles), a
simple information-theoretic method that uses Jensen-Shannon Divergence to
identify and aggregate well-calibrated subsets of LLMs. Experiments on binary
prediction tasks demonstrate improved calibration and predictive performance
compared to single-model and naive ensemble baselines.

</details>


### [64] [Bradley-Terry and Multi-Objective Reward Modeling Are Complementary](https://arxiv.org/abs/2507.07375)
*Zhiwei Zhang,Hui Liu,Xiaomin Li,Zhenwei Dai,Jingying Zeng,Fali Wang,Minhua Lin,Ramraj Chandradevan,Zhen Li,Chen Luo,Xianfeng Tang,Qi He,Suhang Wang*

Main category: cs.LG

TL;DR: 提出统一奖励建模框架，通过联合训练单目标和多目标奖励函数增强模型鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有RLHF方法在分布外场景易受奖励黑客攻击，多属性评分方法受限于数据质量

Method: 联合训练Bradley-Terry单目标模型和多目标回归模型，共享嵌入空间

Result: 7B模型超越70B基线，奖励模型鲁棒性/评分能力显著提升

Conclusion: 框架通过双目标互补机制有效提升奖励模型在OOD场景的防御能力和评分性能

Abstract: Reward models trained on human preference data have demonstrated strong
effectiveness in aligning Large Language Models (LLMs) with human intent under
the framework of Reinforcement Learning from Human Feedback (RLHF). However,
RLHF remains vulnerable to reward hacking, where the policy exploits
imperfections in the reward function rather than genuinely learning the
intended behavior. Although significant efforts have been made to mitigate
reward hacking, they predominantly focus on and evaluate in-distribution
scenarios, where the training and testing data for the reward model share the
same distribution. In this paper, we empirically show that state-of-the-art
methods struggle in more challenging out-of-distribution (OOD) settings. We
further demonstrate that incorporating fine-grained multi-attribute scores
helps address this challenge. However, the limited availability of high-quality
data often leads to weak performance of multi-objective reward functions, which
can negatively impact overall performance and become the bottleneck. To address
this issue, we propose a unified reward modeling framework that jointly trains
Bradley--Terry (BT) single-objective and multi-objective regression-based
reward functions using a shared embedding space. We theoretically establish a
connection between the BT loss and the regression objective and highlight their
complementary benefits. Specifically, the regression task enhances the
single-objective reward function's ability to mitigate reward hacking in
challenging OOD settings, while BT-based training improves the scoring
capability of the multi-objective reward function, enabling a 7B model to
outperform a 70B baseline. Extensive experimental results demonstrate that our
framework significantly improves both the robustness and the scoring
performance of reward models.

</details>


### [65] [COALA: Numerically Stable and Efficient Framework for Context-Aware Low-Rank Approximation](https://arxiv.org/abs/2507.07580)
*Uliana Parkina,Maxim Rakhuba*

Main category: cs.LG

TL;DR: 提出了一种基于稳定分解的无逆正则化框架，解决了传统低秩近似方法在神经网络压缩中的数值不稳定问题


<details>
  <summary>Details</summary>
Motivation: 现有基于Gram矩阵显式求逆的方法会导致数值不稳定（矩阵奇异/质量下降），难以处理大规模校准矩阵、近奇异激活矩阵和数据不足场景

Method: 采用无逆正则化框架，完全基于稳定的矩阵分解技术，通过分块处理和正则化策略解决内存限制、矩阵病态和欠定问题

Result: 成功处理三种挑战场景（内存超限/近奇异矩阵/数据不足），理论证明了收敛性并推导了显式误差界限

Conclusion: 新框架显著提升了数值稳定性，为大规模神经网络压缩提供了可靠解决方案，特别适用于边缘计算和资源受限场景

Abstract: Recent studies suggest that context-aware low-rank approximation is a useful
tool for compression and fine-tuning of modern large-scale neural networks. In
this type of approximation, a norm is weighted by a matrix of input
activations, significantly improving metrics over the unweighted case.
Nevertheless, existing methods for neural networks suffer from numerical
instabilities due to their reliance on classical formulas involving explicit
Gram matrix computation and their subsequent inversion. We demonstrate that
this can degrade the approximation quality or cause numerically singular
matrices.
  To address these limitations, we propose a novel inversion-free regularized
framework that is based entirely on stable decompositions and overcomes the
numerical pitfalls of prior art. Our method can handle possible challenging
scenarios: (1) when calibration matrices exceed GPU memory capacity, (2) when
input activation matrices are nearly singular, and even (3) when insufficient
data prevents unique approximation. For the latter, we prove that our solution
converges to a desired approximation and derive explicit error bounds.

</details>


### [66] [Improving Clustering on Occupational Text Data through Dimensionality Reduction](https://arxiv.org/abs/2507.07582)
*Iago Xabier Vázquez García,Damla Partanaz,Emrullah Fatih Yetkin*

Main category: cs.LG

TL;DR: 提出基于BERT技术和聚类算法的职业映射管道，通过降维和轮廓系数优化，实现不同职业定义体系的自动化匹配


<details>
  <summary>Details</summary>
Motivation: 解决不同国家/企业对O*NET职业定义存在的差异性问题，建立跨数据库的职业映射体系以支持职业转型需求

Method: 构建包含多种BERT编码技术的处理流程，结合不同聚类算法，测试降维技术对聚类效果的影响，并创新性地采用定制化轮廓系数评估方法

Result: 降维技术有效提升聚类性能指标，定制轮廓系数方法显著改善职业分类准确度

Conclusion: 该自动化职业映射方案为跨数据库职业对比和职业转型路径规划提供了有效技术支撑

Abstract: In this study, we focused on proposing an optimal clustering mechanism for
the occupations defined in the well-known US-based occupational database,
O*NET. Even though all occupations are defined according to well-conducted
surveys in the US, their definitions can vary for different firms and
countries. Hence, if one wants to expand the data that is already collected in
O*NET for the occupations defined with different tasks, a map between the
definitions will be a vital requirement. We proposed a pipeline using several
BERT-based techniques with various clustering approaches to obtain such a map.
We also examined the effect of dimensionality reduction approaches on several
metrics used in measuring performance of clustering algorithms. Finally, we
improved our results by using a specialized silhouette approach. This new
clustering-based mapping approach with dimensionality reduction may help
distinguish the occupations automatically, creating new paths for people
wanting to change their careers.

</details>


### [67] [GuardVal: Dynamic Large Language Model Jailbreak Evaluation for Comprehensive Safety Testing](https://arxiv.org/abs/2507.07735)
*Peiyan Zhang,Haibo Jin,Liying Kang,Haohan Wang*

Main category: cs.LG

TL;DR: 提出GuardVal评估协议和优化方法，通过动态生成越狱提示全面评估大语言模型安全性，并在多模型测试中揭示不同行为模式


<details>
  <summary>Details</summary>
Motivation: 现有评估方法难以应对大语言模型快速演进和越狱攻击的复杂性，导致安全漏洞评估存在不足

Method: 开发动态生成越狱提示的GuardVal协议，结合防优化停滞方法，在10个安全领域测试从Mistral-7b到GPT-4的多种模型

Result: 不同模型展现出显著行为差异，测试协议加深了对LLM安全机制的理解并量化了模型鲁棒性

Conclusion: GuardVal协议可有效评估模型安全性，研究成果为开发更安全的大语言模型提供了重要方法论和实证依据

Abstract: Jailbreak attacks reveal critical vulnerabilities in Large Language Models
(LLMs) by causing them to generate harmful or unethical content. Evaluating
these threats is particularly challenging due to the evolving nature of LLMs
and the sophistication required in effectively probing their vulnerabilities.
Current benchmarks and evaluation methods struggle to fully address these
challenges, leaving gaps in the assessment of LLM vulnerabilities. In this
paper, we review existing jailbreak evaluation practices and identify three
assumed desiderata for an effective jailbreak evaluation protocol. To address
these challenges, we introduce GuardVal, a new evaluation protocol that
dynamically generates and refines jailbreak prompts based on the defender LLM's
state, providing a more accurate assessment of defender LLMs' capacity to
handle safety-critical situations. Moreover, we propose a new optimization
method that prevents stagnation during prompt refinement, ensuring the
generation of increasingly effective jailbreak prompts that expose deeper
weaknesses in the defender LLMs. We apply this protocol to a diverse set of
models, from Mistral-7b to GPT-4, across 10 safety domains. Our findings
highlight distinct behavioral patterns among the models, offering a
comprehensive view of their robustness. Furthermore, our evaluation process
deepens the understanding of LLM behavior, leading to insights that can inform
future research and drive the development of more secure models.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [68] [Multi-level Mixture of Experts for Multimodal Entity Linking](https://arxiv.org/abs/2507.07108)
*Zhiwei Hu,Víctor Gutiérrez-Basulto,Zhiliang Xiang,Ru Li,Jeff Z. Pan*

Main category: cs.CV

TL;DR: 提出MMoE模型解决多模态实体链接中的mention歧义和模态动态选择问题，通过LLM增强描述和混合专家机制实现最优性能


<details>
  <summary>Details</summary>
Motivation: 现有MEL方法存在mention歧义(文本上下文信息缺失)和模态动态选择不足的问题，需要增强语义表达并实现多模态信息动态筛选

Method: 1. 基于LLM的描述感知mention增强模块 2. 多模态特征提取 3. 层级混合专家机制(Intra-level/Inter-level MoE)动态选择特征

Result: 实验显示MMoE显著超越SOTA方法，代码已开源

Conclusion: MMoE有效解决了mention歧义和模态动态选择问题，通过混合专家机制实现多模态信息的自适应融合

Abstract: Multimodal Entity Linking (MEL) aims to link ambiguous mentions within
multimodal contexts to associated entities in a multimodal knowledge base.
Existing approaches to MEL introduce multimodal interaction and fusion
mechanisms to bridge the modality gap and enable multi-grained semantic
matching. However, they do not address two important problems: (i) mention
ambiguity, i.e., the lack of semantic content caused by the brevity and
omission of key information in the mention's textual context; (ii) dynamic
selection of modal content, i.e., to dynamically distinguish the importance of
different parts of modal information. To mitigate these issues, we propose a
Multi-level Mixture of Experts (MMoE) model for MEL. MMoE has four components:
(i) the description-aware mention enhancement module leverages large language
models to identify the WikiData descriptions that best match a mention,
considering the mention's textual context; (ii) the multimodal feature
extraction module adopts multimodal feature encoders to obtain textual and
visual embeddings for both mentions and entities; (iii)-(iv) the intra-level
mixture of experts and inter-level mixture of experts modules apply a switch
mixture of experts mechanism to dynamically and adaptively select features from
relevant regions of information. Extensive experiments demonstrate the
outstanding performance of MMoE compared to the state-of-the-art. MMoE's code
is available at: https://github.com/zhiweihu1103/MEL-MMoE.

</details>


### [69] [Robust Multimodal Large Language Models Against Modality Conflict](https://arxiv.org/abs/2507.07151)
*Zongmeng Zhang,Wengang Zhou,Jie Zhao,Houqiang Li*

Main category: cs.CV

TL;DR: 研究探讨多模态大语言模型因模态冲突产生的幻觉问题，构建MMMC数据集并验证强化学习方法在缓解该现象中的最佳效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注模型输出与输入的矛盾，本文首次聚焦多模态输入间的内在冲突对模型幻觉的直接影响。

Method: 通过定义模态冲突、构建MMMC数据集，提出基于提示工程、监督微调和强化学习的三种解决方案。

Result: 实验表明强化学习方法在缓解模态冲突导致的幻觉中表现最优，监督微调方法展现出稳定潜力。

Conclusion: 揭示了多模态冲突引发幻觉的机制，为提升MLLMs的鲁棒性提供了新视角，强化学习方法具有实践指导价值。

Abstract: Despite the impressive capabilities of multimodal large language models
(MLLMs) in vision-language tasks, they are prone to hallucinations in
real-world scenarios. This paper investigates the hallucination phenomenon in
MLLMs from the perspective of modality conflict. Unlike existing works focusing
on the conflicts between model responses and inputs, we study the inherent
conflicts in inputs from different modalities that place MLLMs in a dilemma and
directly lead to hallucinations. We formally define the modality conflict and
construct a dataset named Multimodal Modality Conflict (MMMC) to simulate this
phenomenon in vision-language tasks. Three methods based on prompt engineering,
supervised fine-tuning, and reinforcement learning are proposed to alleviate
the hallucination caused by modality conflict. Extensive experiments are
conducted on the MMMC dataset to analyze the merits and demerits of these
methods. Our results show that the reinforcement learning method achieves the
best performance in mitigating the hallucination under modality conflict, while
the supervised fine-tuning method shows promising and stable performance. Our
work sheds light on the unnoticed modality conflict that leads to
hallucinations and provides more insights into the robustness of MLLMs.

</details>


### [70] [LinguaMark: Do Multimodal Models Speak Fairly? A Benchmark-Based Evaluation](https://arxiv.org/abs/2507.07274)
*Ananya Raval,Aravind Narayanan,Vahid Reza Khazaie,Shaina Raza*

Main category: cs.CV

TL;DR: 研究构建多语言评估基准LinguaMark，揭示闭源模型在多模态任务中的优势及Qwen2.5的跨语言泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型存在语言覆盖不足导致的输出偏见问题，缺乏系统性多语言能力评估

Method: 创建包含11种语言、5个社会属性的6,875对数据，采用偏见/相关性/忠实性三维度评估模型

Result: 闭源模型（GPT-4o/Gemini2.5）综合表现最佳，开源模型Qwen2.5展现跨语言优势

Conclusion: 发布评估基准促进可复现研究，为提升多模态模型公平性提供新方向

Abstract: Large Multimodal Models (LMMs) are typically trained on vast corpora of
image-text data but are often limited in linguistic coverage, leading to biased
and unfair outputs across languages. While prior work has explored multimodal
evaluation, less emphasis has been placed on assessing multilingual
capabilities. In this work, we introduce LinguaMark, a benchmark designed to
evaluate state-of-the-art LMMs on a multilingual Visual Question Answering
(VQA) task. Our dataset comprises 6,875 image-text pairs spanning 11 languages
and five social attributes. We evaluate models using three key metrics: Bias,
Answer Relevancy, and Faithfulness. Our findings reveal that closed-source
models generally achieve the highest overall performance. Both closed-source
(GPT-4o and Gemini2.5) and open-source models (Gemma3, Qwen2.5) perform
competitively across social attributes, and Qwen2.5 demonstrates strong
generalization across multiple languages. We release our benchmark and
evaluation code to encourage reproducibility and further research.

</details>


### [71] [SpatialViz-Bench: Automatically Generated Spatial Visualization Reasoning Tasks for MLLMs](https://arxiv.org/abs/2507.07610)
*Siting Wang,Luoyang Sun,Cheng Deng,Kun Shao,Minnan Pei,Zheng Tian,Haifeng Zhang,Jun Wang*

Main category: cs.CV

TL;DR: 提出了首个多模态空间可视化基准SpatialViz-Bench，包含12个任务1180个自动生成问题，评估发现主流MLLM存在空间认知缺陷


<details>
  <summary>Details</summary>
Motivation: 现有MLLM空间可视化评估方法存在测试数据与训练集重叠、评估维度不聚焦的问题，需要专用基准

Method: 构建包含4大子能力12个任务的自动生成基准（1180题），系统评估33个前沿MLLM模型

Result: 模型呈现与人类直觉不符的难度感知、2D-3D性能断崖、过度依赖公式推导等反直觉现象

Conclusion: SpatialViz-Bench揭示了当前MLLM在空间可视化任务中的系统性缺陷，填补了该领域评估体系的空白

Abstract: Humans can directly imagine and manipulate visual images in their minds, a
capability known as spatial visualization. While multi-modal Large Language
Models (MLLMs) support imagination-based reasoning, spatial visualization
remains insufficiently evaluated, typically embedded within broader
mathematical and logical assessments. Existing evaluations often rely on IQ
tests or math competitions that may overlap with training data, compromising
assessment reliability. To this end, we introduce SpatialViz-Bench, a
comprehensive multi-modal benchmark for spatial visualization with 12 tasks
across 4 sub-abilities, comprising 1,180 automatically generated problems. Our
evaluation of 33 state-of-the-art MLLMs not only reveals wide performance
variations and demonstrates the benchmark's strong discriminative power, but
also uncovers counter-intuitive findings: models exhibit unexpected behaviors
by showing difficulty perception that misaligns with human intuition,
displaying dramatic 2D-to-3D performance cliffs, and defaulting to formula
derivation despite spatial tasks requiring visualization alone. SpatialVizBench
empirically demonstrates that state-of-the-art MLLMs continue to exhibit
deficiencies in spatial visualization tasks, thereby addressing a significant
lacuna in the field. The benchmark is publicly available.

</details>


### [72] [Scaling RL to Long Videos](https://arxiv.org/abs/2507.07966)
*Yukang Chen,Wei Huang,Baifeng Shi,Qinghao Hu,Hanrong Ye,Ligeng Zhu,Zhijian Liu,Pavlo Molchanov,Jan Kautz,Xiaojuan Qi,Sifei Liu,Hongxu Yin,Yao Lu,Song Han*

Main category: cs.CV

TL;DR: 提出基于强化学习的全栈框架LongVILA-R1-7B，实现视觉语言模型在长视频推理中的性能突破，支持小时级视频训练并发布通用训练系统


<details>
  <summary>Details</summary>
Motivation: 解决长视频推理中时序跨度大、计算复杂度高的核心难题，通过整合大规模标注数据集、两阶段训练范式、序列并行技术突破传统VLMs的视频处理限制

Method: 三阶段方法论：1) 构建52K长视频QA数据集LongVideo-Reason；2) CoT-SFT+RL两阶段训练流程；3) 创新MR-SP训练系统（序列并行+vLLM引擎+视频嵌入缓存）

Result: 在VideoMME基准取得SOTA，LongVideo-Reason-eval四大推理维度超越Video-R1-7B并持平Gemini-1.5-Pro，MR-SP实现2.1倍加速，单A100节点支持3600帧/256K tokens训练

Conclusion: LongVILA-R1标志着VLMs长视频推理的重要突破，其开源的通用RL训练系统支持多模态/生成模型，为视频理解领域提供新的基础设施

Abstract: We introduce a full-stack framework that scales up reasoning in
vision-language models (VLMs) to long videos, leveraging reinforcement
learning. We address the unique challenges of long video reasoning by
integrating three critical components: (1) a large-scale dataset,
LongVideo-Reason, comprising 52K long video QA pairs with high-quality
reasoning annotations across diverse domains such as sports, games, and vlogs;
(2) a two-stage training pipeline that extends VLMs with chain-of-thought
supervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a
training infrastructure for long video RL, named Multi-modal Reinforcement
Sequence Parallelism (MR-SP), which incorporates sequence parallelism and a
vLLM-based engine tailored for long video, using cached video embeddings for
efficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves
strong performance on long video QA benchmarks such as VideoMME. It also
outperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal
reasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on
our LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to
2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent
performance gains as the number of input video frames scales. LongVILA-R1 marks
a firm step towards long video reasoning in VLMs. In addition, we release our
training system for public availability that supports RL training on various
modalities (video, text, and audio), various models (VILA and Qwen series), and
even image and video generation models. On a single A100 node (8 GPUs), it
supports RL training on hour-long videos (e.g., 3,600 frames / around 256k
tokens).

</details>


### [73] [Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology](https://arxiv.org/abs/2507.07999)
*Haochen Wang,Xiangtai Li,Zilong Huang,Anran Wang,Jiacong Wang,Tao Zhang,Jiani Zheng,Sule Bai,Zijian Kang,Jiashi Feng,Zhuochen Wang,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: 提出TreeBench视觉推理评测基准与TreeVGR训练框架，解决现有模型在复杂视觉推理任务中的可解释性缺陷


<details>
  <summary>Details</summary>
Motivation: 现有视觉推理模型缺乏系统性评测基准，难以评估模型的视觉定位能力与可解释推理路径

Method: 基于三原则构建TreeBench：1) 复杂场景下的精细化感知 2) 基于边界框的可追溯证据评估 3) 超越目标定位的二级空间推理。通过专家标注和强化学习框架TreeVGR实现联合优化

Result: 主流模型在TreeBench上准确率不足60%（如GPT-4o仅54.87%）。TreeVGR框架在多个基准提升显著（V*Bench +16.8，TreeBench +13.4）

Conclusion: 可追溯的推理路径监督能有效提升视觉推理性能，TreeBench为视觉语言模型的精细化评估提供新方向，开源代码促进领域发展

Abstract: Models like OpenAI-o3 pioneer visual grounded reasoning by dynamically
referencing visual regions, just like human "thinking with images". However, no
benchmark exists to evaluate these capabilities holistically. To bridge this
gap, we propose TreeBench (Traceable Evidence Evaluation Benchmark), a
diagnostic benchmark built on three principles: (1) focused visual perception
of subtle targets in complex scenes, (2) traceable evidence via bounding box
evaluation, and (3) second-order reasoning to test object interactions and
spatial hierarchies beyond simple object localization. Prioritizing images with
dense objects, we initially sample 1K high-quality images from SA-1B, and
incorporate eight LMM experts to manually annotate questions, candidate
options, and answers for each image. After three stages of quality control,
TreeBench consists of 405 challenging visual question-answering pairs, even the
most advanced models struggle with this benchmark, where none of them reach 60%
accuracy, e.g., OpenAI-o3 scores only 54.87. Furthermore, we introduce TreeVGR
(Traceable Evidence Enhanced Visual Grounded Reasoning), a training paradigm to
supervise localization and reasoning jointly with reinforcement learning,
enabling accurate localizations and explainable reasoning pathways. Initialized
from Qwen2.5-VL-7B, it improves V* Bench (+16.8), MME-RealWorld (+12.6), and
TreeBench (+13.4), proving traceability is key to advancing vision-grounded
reasoning. The code is available at https://github.com/Haochen-Wang409/TreeVGR.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [74] [A Language-Driven Framework for Improving Personalized Recommendations: Merging LLMs with Traditional Algorithms](https://arxiv.org/abs/2507.07251)
*Aaron Goldstein,Ayan Dutta*

Main category: cs.IR

TL;DR: 提出融合大语言模型与传统推荐算法的新型框架，通过自动化生成用户偏好档案，在多项指标上实现3-6倍性能提升


<details>
  <summary>Details</summary>
Motivation: 传统推荐算法无法有效处理自然语言形式的用户偏好描述，LLM在自然语言处理方面的优势为个性化推荐提供了新思路

Method: 1. 使用SVD/SVD++算法生成初始推荐（Surprise库+MovieLens数据集） 2. 通过LLM整合文本偏好优化推荐结果 3. 采用留一验证和分层测试对比算法性能

Result: 自动化框架在累积命中率(↑6x)、NDCG(↑3.7x)等关键指标显著超越传统算法，但计算开销增加约10-15%

Conclusion: LLM与传统推荐算法的协同框架有效提升推荐系统个性化水平，性能优势明显，为计算效率与推荐质量的平衡提供新方案

Abstract: Traditional recommendation algorithms are not designed to provide
personalized recommendations based on user preferences provided through text,
e.g., "I enjoy light-hearted comedies with a lot of humor". Large Language
Models (LLMs) have emerged as one of the most promising tools for natural
language processing in recent years. This research proposes a novel framework
that mimics how a close friend would recommend items based on their knowledge
of an individual's tastes. We leverage LLMs to enhance movie recommendation
systems by refining traditional algorithm outputs and integrating them with
language-based user preference inputs. We employ Singular Value Decomposition
(SVD) or SVD++ algorithms to generate initial movie recommendations,
implemented using the Surprise Python library and trained on the
MovieLens-Latest-Small dataset. We compare the performance of the base
algorithms with our LLM-enhanced versions using leave-one-out validation hit
rates and cumulative hit rates. Additionally, to compare the performance of our
framework against the current state-of-the-art recommendation systems, we use
rating and ranking metrics with an item-based stratified 0.75 train, 0.25 test
split. Our framework can generate preference profiles automatically based on
users' favorite movies or allow manual preference specification for more
personalized results. Using an automated approach, our framework overwhelmingly
surpassed SVD and SVD++ on every evaluation metric used (e.g., improvements of
up to ~6x in cumulative hit rate, ~3.7x in NDCG, etc.), albeit at the cost of a
slight increase in computational overhead.

</details>
