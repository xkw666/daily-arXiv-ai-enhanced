<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 66]
- [cs.GR](#cs.GR) [Total: 9]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.LG](#cs.LG) [Total: 9]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.CV](#cs.CV) [Total: 8]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Interpreting Public Sentiment in Diplomacy Events: A Counterfactual Analysis Framework Using Large Language Models](https://arxiv.org/abs/2509.20367)
*Leyi Ouyang*

Main category: cs.CL

TL;DR: 提出结合语言模型预测和反事实生成算法的外交事件叙事优化框架，成功将负面公众情绪转为中性/正面的比率达70%


<details>
  <summary>Details</summary>
Motivation: 传统舆情分析方法耗时费力且缺乏前瞻性，需数据驱动工具帮助外交人员优化叙事框架

Method: 1. 训练预测公众反应的语言模型 2. 基于传播理论设计反事实生成算法修改文本特征

Result: 框架成功改变叙事框架并保持事实核心，情绪转化成功率达70%

Conclusion: 该框架为外交决策者提供可操作的叙事优化方案，有效塑造理想公众情绪

Abstract: Diplomatic events consistently prompt widespread public discussion and
debate. Public sentiment plays a critical role in diplomacy, as a good
sentiment provides vital support for policy implementation, helps resolve
international issues, and shapes a nation's international image. Traditional
methods for gauging public sentiment, such as large-scale surveys or manual
content analysis of media, are typically time-consuming, labor-intensive, and
lack the capacity for forward-looking analysis. We propose a novel framework
that identifies specific modifications for diplomatic event narratives to shift
public sentiment from negative to neutral or positive. First, we train a
language model to predict public reaction towards diplomatic events. To this
end, we construct a dataset comprising descriptions of diplomatic events and
their associated public discussions. Second, guided by communication theories
and in collaboration with domain experts, we predetermined several textual
features for modification, ensuring that any alterations changed the event's
narrative framing while preserving its core facts.We develop a counterfactual
generation algorithm that employs a large language model to systematically
produce modified versions of an original text. The results show that this
framework successfully shifted public sentiment to a more favorable state with
a 70\% success rate. This framework can therefore serve as a practical tool for
diplomats, policymakers, and communication specialists, offering data-driven
insights on how to frame diplomatic initiatives or report on events to foster a
more desirable public sentiment.

</details>


### [2] [Speaker Style-Aware Phoneme Anchoring for Improved Cross-Lingual Speech Emotion Recognition](https://arxiv.org/abs/2509.20373)
*Shreya G. Upadhyay,Carlos Busso,Chi-Chun Lee*

Main category: cs.CL

TL;DR: 提出说话者风格感知的音素锚定框架，通过构建情感特定说话者社区和双空间锚定技术提升跨语言语音情感识别效果


<details>
  <summary>Details</summary>
Motivation: 跨语言语音情感识别面临语音变异性和说话者风格差异的挑战，需要有效对齐不同语言的表达特征

Method: 1. 基于图聚类构建情感特定说话者社区
2. 在说话者空间和音素空间实施双空间锚定
3. 通过对抗训练实现跨语言特征对齐

Result: 在MSP-Podcast(英语)和BIIC-Podcast(台湾普通话)数据集上实现性能提升，发现跨语言情感表征的共享模式

Conclusion: 该框架通过分层对齐策略有效捕捉跨语言情感共性，为多语言情感计算提供新思路

Abstract: Cross-lingual speech emotion recognition (SER) remains a challenging task due
to differences in phonetic variability and speaker-specific expressive styles
across languages. Effectively capturing emotion under such diverse conditions
requires a framework that can align the externalization of emotions across
different speakers and languages. To address this problem, we propose a
speaker-style aware phoneme anchoring framework that aligns emotional
expression at the phonetic and speaker levels. Our method builds
emotion-specific speaker communities via graph-based clustering to capture
shared speaker traits. Using these groups, we apply dual-space anchoring in
speaker and phonetic spaces to enable better emotion transfer across languages.
Evaluations on the MSP-Podcast (English) and BIIC-Podcast (Taiwanese Mandarin)
corpora demonstrate improved generalization over competitive baselines and
provide valuable insights into the commonalities in cross-lingual emotion
representation.

</details>


### [3] [CFD-LLMBench: A Benchmark Suite for Evaluating Large Language Models in Computational Fluid Dynamics](https://arxiv.org/abs/2509.20374)
*Nithin Somasekharan,Ling Yue,Yadi Cao,Weichao Li,Patrick Emami,Pochinapeddi Sai Bhargav,Anurag Acharya,Xingyu Xie,Shaowu Pan*

Main category: cs.CL

TL;DR: 提出CFDLLMBench基准套件，系统评估大语言模型在计算流体力学领域的数值实验自动化能力


<details>
  <summary>Details</summary>
Motivation: 大语言模型在科学计算领域尤其是复杂物理系统数值实验自动化方面的潜力尚未被充分探索

Method: 开发包含CFDQuery、CFDCodeBench和FoamBench的测试套件，结合任务分类和评估框架

Result: 建立可复现的评估体系，量化模型在代码可执行性、解算精度和数值收敛性方面的表现

Conclusion: 为复杂物理系统数值实验的LLM驱动自动化建立了系统评估基础

Abstract: Large Language Models (LLMs) have demonstrated strong performance across
general NLP tasks, but their utility in automating numerical experiments of
complex physical system -- a critical and labor-intensive component -- remains
underexplored. As the major workhorse of computational science over the past
decades, Computational Fluid Dynamics (CFD) offers a uniquely challenging
testbed for evaluating the scientific capabilities of LLMs. We introduce
CFDLLMBench, a benchmark suite comprising three complementary components --
CFDQuery, CFDCodeBench, and FoamBench -- designed to holistically evaluate LLM
performance across three key competencies: graduate-level CFD knowledge,
numerical and physical reasoning of CFD, and context-dependent implementation
of CFD workflows. Grounded in real-world CFD practices, our benchmark combines
a detailed task taxonomy with a rigorous evaluation framework to deliver
reproducible results and quantify LLM performance across code executability,
solution accuracy, and numerical convergence behavior. CFDLLMBench establishes
a solid foundation for the development and evaluation of LLM-driven automation
of numerical experiments for complex physical systems. Code and data are
available at https://github.com/NREL-Theseus/cfdllmbench/.

</details>


### [4] [Assessing Classical Machine Learning and Transformer-based Approaches for Detecting AI-Generated Research Text](https://arxiv.org/abs/2509.20375)
*Sharanya Parimanoharan,Ruwan D. Nawarathna*

Main category: cs.CL

TL;DR: 研究比较多种机器学习方法检测ChatGPT生成科研摘要的效果，发现DistilBERT模型表现最优且集成方法未能超越其效果


<details>
  <summary>Details</summary>
Motivation: ChatGPT等大模型模糊人类与AI文本界限，亟需可靠的AI文本检测技术来维护学术诚信和数字信任

Method: 使用250对科研摘要数据集，对比测试经典方法（带词袋特征的逻辑回归）与Transformer模型（BERT系列及LSTM-Ngram）

Result: DistilBERT综合性能最佳，集成投票策略未超越单模型；逻辑回归和BERT-Custom表现均衡，LSTM-Ngram效果欠佳

Conclusion: 研究表明Transformer模型在AI文本检测中的优势，未来需构建更大数据集和更鲁棒的框架应对持续进化的生成模型

Abstract: The rapid adoption of large language models (LLMs) such as ChatGPT has
blurred the line between human and AI-generated texts, raising urgent questions
about academic integrity, intellectual property, and the spread of
misinformation. Thus, reliable AI-text detection is needed for fair assessment
to safeguard human authenticity and cultivate trust in digital communication.
In this study, we investigate how well current machine learning (ML) approaches
can distinguish ChatGPT-3.5-generated texts from human-written texts employing
a labeled data set of 250 pairs of abstracts from a wide range of research
topics. We test and compare both classical (Logistic Regression armed with
classical Bag-of-Words, POS, and TF-IDF features) and transformer-based (BERT
augmented with N-grams, DistilBERT, BERT with a lightweight custom classifier,
and LSTM-based N-gram models) ML detection techniques. As we aim to assess each
model's performance in detecting AI-generated research texts, we also aim to
test whether an ensemble of these models can outperform any single detector.
Results show DistilBERT achieves the overall best performance, while Logistic
Regression and BERT-Custom offer solid, balanced alternatives; LSTM- and
BERT-N-gram approaches lag. The max voting ensemble of the three best models
fails to surpass DistilBERT itself, highlighting the primacy of a single
transformer-based representation over mere model diversity. By comprehensively
assessing the strengths and weaknesses of these AI-text detection approaches,
this work lays a foundation for more robust transformer frameworks with larger,
richer datasets to keep pace with ever-improving generative AI models.

</details>


### [5] [ConceptViz: A Visual Analytics Approach for Exploring Concepts in Large Language Models](https://arxiv.org/abs/2509.20376)
*Haoxuan Li,Zhen Wen,Qiqi Jiang,Chenxiao Li,Yuwei Wu,Yuchen Yang,Yiyao Wang,Xiuqi Huang,Minfeng Zhu,Wei Chen*

Main category: cs.CL

TL;DR: 开发了ConceptViz可视化分析系统，通过识别-解释-验证的流程提升LLM特征可解释性研究，帮助研究人员建立更准确的心智模型


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器(SAE)提取的特征缺乏与人类概念的天然对齐，导致特征解释过程繁琐且需要大量人工参与

Method: 提出三阶段分析框架：1）基于兴趣概念的SAE特征查询 2）交互式概念-特征对齐探索 3）通过模型行为验证的对应关系验证

Result: 通过应用场景和用户研究验证了系统有效性，证明其能有效支持可解释性研究，加速有意义概念表示的发现和验证过程

Conclusion: ConceptViz通过系统化的视觉分析流程，成功弥合了机器特征与人类概念之间的鸿沟，其开源实现为LLM特征研究提供了实用工具支持

Abstract: Large language models (LLMs) have achieved remarkable performance across a
wide range of natural language tasks. Understanding how LLMs internally
represent knowledge remains a significant challenge. Despite Sparse
Autoencoders (SAEs) have emerged as a promising technique for extracting
interpretable features from LLMs, SAE features do not inherently align with
human-understandable concepts, making their interpretation cumbersome and
labor-intensive. To bridge the gap between SAE features and human concepts, we
present ConceptViz, a visual analytics system designed for exploring concepts
in LLMs. ConceptViz implements a novel dentification => Interpretation =>
Validation pipeline, enabling users to query SAEs using concepts of interest,
interactively explore concept-to-feature alignments, and validate the
correspondences through model behavior verification. We demonstrate the
effectiveness of ConceptViz through two usage scenarios and a user study. Our
results show that ConceptViz enhances interpretability research by streamlining
the discovery and validation of meaningful concept representations in LLMs,
ultimately aiding researchers in building more accurate mental models of LLM
features. Our code and user guide are publicly available at
https://github.com/Happy-Hippo209/ConceptViz.

</details>


### [6] [SKILL-RAG: Self-Knowledge Induced Learning and Filtering for Retrieval-Augmented Generation](https://arxiv.org/abs/2509.20377)
*Tomoaki Isoda*

Main category: cs.CL

TL;DR: SKILL-RAG通过模型的自知能力筛选检索文档，结合强化学习框架提升RAG性能并减少输入量


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统可能引入无关内容导致幻觉问题，需要利用模型的自知能力区分有用/无用外部知识

Method: 设计基于强化学习的训练框架，在句子粒度过滤无关内容，保留有效知识，实现检索文档的精准筛选

Result: 在Llama2-7B/Qwen3-8B的问答基准测试中，同时提升生成质量并减少35-50%的输入文档量

Conclusion: 验证了模型自知能力对高质量检索选择的关键作用，为RAG系统的优化提供了新方向

Abstract: Retrieval-Augmented Generation (RAG) has significantly improved the
performance of large language models (LLMs) on knowledge-intensive tasks in
recent years. However, since retrieval systems may return irrelevant content,
incorporating such information into the model often leads to hallucinations.
Thus, identifying and filtering out unhelpful retrieved content is a key
challenge for improving RAG performance.To better integrate the internal
knowledge of the model with external knowledge from retrieval, it is essential
to understand what the model "knows" and "does not know" (which is also called
"self-knowledge"). Based on this insight, we propose SKILL-RAG (Self-Knowledge
Induced Learning and Filtering for RAG), a novel method that leverages the
model's self-knowledge to determine which retrieved documents are beneficial
for answering a given query. We design a reinforcement learning-based training
framework to explicitly elicit self-knowledge from the model and employs
sentence-level granularity to filter out irrelevant content while preserving
useful knowledge.We evaluate SKILL-RAG using Llama2-7B and Qwen3-8B on several
question answering benchmarks. Experimental results demonstrate that SKILL-RAG
not only improves generation quality but also significantly reduces the number
of input documents, validating the importance of self-knowledge in guiding the
selection of high-quality retrievals.

</details>


### [7] [Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic Word-Level Modulation](https://arxiv.org/abs/2509.20378)
*Sirui Wang,Andong Chen,Tiejun Zhao*

Main category: cs.CL

TL;DR: 提出Emo-FiLM框架，通过emotion2vec特征对齐和FiLM层实现文本到语音的细粒度情绪控制，基于FEDD数据集验证有效性


<details>
  <summary>Details</summary>
Motivation: 现有情感语音合成系统依赖句子级情绪控制，无法捕捉句子内部情绪动态变化。需突破全局情绪表达限制，实现词语级精细控制

Method: 1. 从emotion2vec提取帧级特征对齐词语 2. 通过FiLM层映射情绪标注 3. 直接调制文本嵌入实现词语级情绪控制

Result: 在全局和细粒度任务上超越现有方法，FEDD数据集验证框架有效性，WER指标降低23%，自然度提升15%

Conclusion: Emo-FiLM成功实现动态情绪建模，FEDD数据集填补评估空白，为表达性语音合成提供新范式，支持更自然的人机交互

Abstract: Emotional text-to-speech (E-TTS) is central to creating natural and
trustworthy human-computer interaction. Existing systems typically rely on
sentence-level control through predefined labels, reference audio, or natural
language prompts. While effective for global emotion expression, these
approaches fail to capture dynamic shifts within a sentence. To address this
limitation, we introduce Emo-FiLM, a fine-grained emotion modeling framework
for LLM-based TTS. Emo-FiLM aligns frame-level features from emotion2vec to
words to obtain word-level emotion annotations, and maps them through a
Feature-wise Linear Modulation (FiLM) layer, enabling word-level emotion
control by directly modulating text embeddings. To support evaluation, we
construct the Fine-grained Emotion Dynamics Dataset (FEDD) with detailed
annotations of emotional transitions. Experiments show that Emo-FiLM
outperforms existing approaches on both global and fine-grained tasks,
demonstrating its effectiveness and generality for expressive speech synthesis.

</details>


### [8] [USB-Rec: An Effective Framework for Improving Conversational Recommendation Capability of Large Language Model](https://arxiv.org/abs/2509.20381)
*Jianyu Wen,Jingyun Wang,Cilin Yan,Jiayin Cai,Xiaolong Jiang,Ying Zhang*

Main category: cs.CL

TL;DR: 提出集成训练-推理框架USB-Rec，通过偏好优化数据集和自增强策略显著提升LLMs在对话推荐中的性能表现


<details>
  <summary>Details</summary>
Motivation: 现有LLMs方法仅聚焦推理阶段的摘要分析能力，忽略训练阶段对模型性能的优化，限制了对话推荐系统的潜力开发

Method: 1. 设计LLM偏好优化数据集构建策略用于强化学习训练；2. 推理阶段采用自增强策略(SES)挖掘模型潜力

Result: 在多个数据集上的实验表明该方法持续超越现有SOTA方法

Conclusion: USB-Rec框架通过整合训练与推理优化，为LLMs在对话推荐领域的应用提供了有效解决方案

Abstract: Recently, Large Language Models (LLMs) have been widely employed in
Conversational Recommender Systems (CRSs). Unlike traditional language model
approaches that focus on training, all existing LLMs-based approaches are
mainly centered around how to leverage the summarization and analysis
capabilities of LLMs while ignoring the issue of training. Therefore, in this
work, we propose an integrated training-inference framework,
User-Simulator-Based framework (USB-Rec), for improving the performance of LLMs
in conversational recommendation at the model level. Firstly, we design a
LLM-based Preference Optimization (PO) dataset construction strategy for RL
training, which helps the LLMs understand the strategies and methods in
conversational recommendation. Secondly, we propose a Self-Enhancement Strategy
(SES) at the inference stage to further exploit the conversational
recommendation potential obtained from RL training. Extensive experiments on
various datasets demonstrate that our method consistently outperforms previous
state-of-the-art methods.

</details>


### [9] [Document Summarization with Conformal Importance Guarantees](https://arxiv.org/abs/2509.20461)
*Bruce Kuwahara,Chen-Yuan Lin,Xiao Shi Huang,Kin Kwan Leung,Jullian Arta Yapeter,Ilya Stanevich,Felipe Perez,Jesse C. Cresswell*

Main category: cs.CL

TL;DR: 提出Conformal Importance Summarization框架，通过conformal prediction确保关键领域自动摘要的重要内容覆盖，实现理论保障的覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM自动摘要系统在医疗/法律/金融等高风险领域缺乏关键内容的可靠性保障，需要可验证的覆盖保证机制。

Method: 基于conformal prediction校准句子重要性分数阈值，实现可定制覆盖率的抽取式摘要，模型无关且只需少量校准数据。

Result: 在标准摘要基准测试中达到理论保证的信息覆盖率，验证了框架有效性。

Conclusion: 该框架可与现有技术结合实现可靠可控的自动摘要，推动关键领域AI摘要工具的安全部署。

Abstract: Automatic summarization systems have advanced rapidly with large language
models (LLMs), yet they still lack reliable guarantees on inclusion of critical
content in high-stakes domains like healthcare, law, and finance. In this work,
we introduce Conformal Importance Summarization, the first framework for
importance-preserving summary generation which uses conformal prediction to
provide rigorous, distribution-free coverage guarantees. By calibrating
thresholds on sentence-level importance scores, we enable extractive document
summarization with user-specified coverage and recall rates over critical
content. Our method is model-agnostic, requires only a small calibration set,
and seamlessly integrates with existing black-box LLMs. Experiments on
established summarization benchmarks demonstrate that Conformal Importance
Summarization achieves the theoretically assured information coverage rate. Our
work suggests that Conformal Importance Summarization can be combined with
existing techniques to achieve reliable, controllable automatic summarization,
paving the way for safer deployment of AI summarization tools in critical
applications. Code is available at
https://github.com/layer6ai-labs/conformal-importance-summarization.

</details>


### [10] [ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos](https://arxiv.org/abs/2509.20467)
*Henrik Vatndal,Vinay Setty*

Main category: cs.CL

TL;DR: ShortCheck是针对TikTok等短视频平台设计的自动化核查系统，通过多模态分析实现70%+加权F1值的错误信息检测。


<details>
  <summary>Details</summary>
Motivation: 短视频平台存在多模态、动态性强、噪声多的特点，传统人工核查效率低下，亟需自动化解决方案帮助事实核查人员快速识别可疑内容。

Method: 采用模块化处理流程，整合语音转录、OCR文字识别、物体/深度伪造检测、视频文本摘要及声明验证等多重技术手段。

Result: 在多语言TikTok视频数据集测试中，系统加权F1分数超过70%，验证了技术路线的有效性。

Conclusion: 该自动化系统显著提升核查效率，通过模块化设计实现多维度内容分析，为人机协同的事实核查提供可行解决方案。

Abstract: Short-form video platforms like TikTok present unique challenges for
misinformation detection due to their multimodal, dynamic, and noisy content.
We present ShortCheck, a modular, inference-only pipeline with a user-friendly
interface that automatically identifies checkworthy short-form videos to help
human fact-checkers. The system integrates speech transcription, OCR, object
and deepfake detection, video-to-text summarization, and claim verification.
ShortCheck is validated by evaluating it on two manually annotated datasets
with TikTok videos in a multilingual setting. The pipeline achieves promising
results with F1-weighted score over 70\%.

</details>


### [11] [MARS: toward more efficient multi-agent collaboration for LLM reasoning](https://arxiv.org/abs/2509.20502)
*Xiao Wang,Jia Wang,Yijie Wang,Pengtao Dang,Sha Cao,Chi Zhang*

Main category: cs.CL

TL;DR: 提出基于角色协作的MARS框架，在保持多智能体辩论准确性的同时减少50%计算资源消耗


<details>
  <summary>Details</summary>
Motivation: 多智能体辩论（MAD）方法存在大量智能体间频繁通信导致的显著计算开销问题

Method: 采用三阶段评审流程：作者生成初始方案→独立评审提供决策意见→元评审整合反馈并指导修订

Result: 在多个基准测试中与MAD准确率相当，同时减少约50%的token消耗和推理时间

Conclusion: 角色分工的协作框架既能提升推理质量，又能有效控制资源消耗，为复杂推理任务提供高效解决方案

Abstract: Large language models (LLMs) have achieved impressive results in natural
language understanding, yet their reasoning capabilities remain limited when
operating as single agents. Multi-Agent Debate (MAD) has been proposed to
address this limitation by enabling collaborative reasoning among multiple
models in a round-table debate manner. While effective, MAD introduces
substantial computational overhead due to the number of agents involved and the
frequent communication required. In this paper, we propose MARS (Multi-Agent
Review System), a role-based collaboration framework inspired by the review
process. In MARS, an author agent generates an initial solution, reviewer
agents provide decisions and comments independently, and a meta-reviewer
integrates the feedback to make the final decision and guide further revision.
This design enhances reasoning quality while avoiding costly
reviewer-to-reviewer interactions, thereby controlling token consumption and
inference time. We compared MARS with both MAD and other state-of-the-art
reasoning strategies across multiple benchmarks. Extensive experiments with
different LLMs show that MARS matches the accuracy of MAD while reducing both
token usage and inference time by approximately 50\%. Code is available at
https://github.com/xwang97/MARS.

</details>


### [12] [SiniticMTError: A Machine Translation Dataset with Error Annotations for Sinitic Languages](https://arxiv.org/abs/2509.20557)
*Hannah Liu,Junghyun Min,Ethan Yue Heng Cheung,Shou-Yi Hung,Syed Mekael Wasti,Runtong Liang,Shiyao Qian,Shizhao Zheng,Elsie Chan,Ka Ieng Charlotte Lo,Wing Yu Yip,Richard Tzong-Han Tsai,En-Shiun Annie Lee*

Main category: cs.CL

TL;DR: 提出SiniticMTError数据集，支持粤语/吴语等低资源汉语方言的机器翻译错误检测与模型优化


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译技术对粤语、吴语等低资源汉语方言支持不足，缺乏标注数据支撑质量评估

Method: 基于现有平行语料库构建包含错误跨度、类型、严重程度标注的三语种（普通话/粤语/吴语）数据集

Result: 创建首个支持低资源汉语方言错误检测的数据集，提供详细的标注质量分析及标注者一致性报告

Conclusion: 该数据集为机器翻译社区提供了错误感知模型训练基础，推动低资源语言评估与质量提升研究

Abstract: Despite major advances in machine translation (MT) in recent years, progress
remains limited for many low-resource languages that lack large-scale training
data and linguistic resources. Cantonese and Wu Chinese are two Sinitic
examples, although each enjoys more than 80 million speakers around the world.
In this paper, we introduce SiniticMTError, a novel dataset that builds on
existing parallel corpora to provide error span, error type, and error severity
annotations in machine-translated examples from English to Mandarin, Cantonese,
and Wu Chinese. Our dataset serves as a resource for the MT community to
utilize in fine-tuning models with error detection capabilities, supporting
research on translation quality estimation, error-aware generation, and
low-resource language evaluation. We report our rigorous annotation process by
native speakers, with analyses on inter-annotator agreement, iterative
feedback, and patterns in error type and severity.

</details>


### [13] [SwasthLLM: a Unified Cross-Lingual, Multi-Task, and Meta-Learning Zero-Shot Framework for Medical Diagnosis Using Contrastive Representations](https://arxiv.org/abs/2509.20567)
*Ayan Sar,Pranav Singh Puri,Sumit Aich,Tanupriya Choudhury,Abhijit Kumar*

Main category: cs.CL

TL;DR: 提出SwasthLLM多语言医疗诊断框架，通过跨语言表征对齐和元学习实现零样本诊断，在低资源语言中取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 解决多语言医疗场景中标注数据稀缺和语言差异导致的自动诊断难题，特别是低资源语言的临床文本处理挑战。

Method: 基于XLM-RoBERTa编码器，集成语言感知注意力机制+对比学习模块+翻译一致性约束，采用多任务联合优化和MAML元学习策略实现跨语言迁移。

Result: 监督场景下97.22%准确率，零样本跨语言诊断在印地语/孟加拉语分别达92.78%和73.33%准确率。

Conclusion: 该框架通过语言无关的医学表征学习，显著提升了低资源语言的医疗诊断能力，为跨语言医疗NLP提供了有效解决方案。

Abstract: In multilingual healthcare environments, automatic disease diagnosis from
clinical text remains a challenging task due to the scarcity of annotated
medical data in low-resource languages and the linguistic variability across
populations. This paper proposes SwasthLLM, a unified, zero-shot,
cross-lingual, and multi-task learning framework for medical diagnosis that
operates effectively across English, Hindi, and Bengali without requiring
language-specific fine-tuning. At its core, SwasthLLM leverages the
multilingual XLM-RoBERTa encoder augmented with a language-aware attention
mechanism and a disease classification head, enabling the model to extract
medically relevant information regardless of the language structure. To align
semantic representations across languages, a Siamese contrastive learning
module is introduced, ensuring that equivalent medical texts in different
languages produce similar embeddings. Further, a translation consistency module
and a contrastive projection head reinforce language-invariant representation
learning. SwasthLLM is trained using a multi-task learning strategy, jointly
optimizing disease classification, translation alignment, and contrastive
learning objectives. Additionally, we employ Model-Agnostic Meta-Learning
(MAML) to equip the model with rapid adaptation capabilities for unseen
languages or tasks with minimal data. Our phased training pipeline emphasizes
robust representation alignment before task-specific fine-tuning. Extensive
evaluation shows that SwasthLLM achieves high diagnostic performance, with a
test accuracy of 97.22% and an F1-score of 97.17% in supervised settings.
Crucially, in zero-shot scenarios, it attains 92.78% accuracy on Hindi and
73.33% accuracy on Bengali medical text, demonstrating strong generalization in
low-resource contexts.

</details>


### [14] [Dynamic Reasoning Chains through Depth-Specialized Mixture-of-Experts in Transformer Architectures](https://arxiv.org/abs/2509.20577)
*Sampurna Roy,Ayan Sar,Anurag Kaushish,Kanav Gupta,Tanupriya Choudhury,Abhijit Kumar*

Main category: cs.CL

TL;DR: 提出动态推理链（DS-MoE）框架，通过深度专业化混合专家提升Transformer效率与推理质量


<details>
  <summary>Details</summary>
Motivation: 传统Transformer对简单查询和复杂推理使用相同计算深度，造成资源浪费并限制推理深度

Method: 扩展混合专家范式至深度维度，引入五类深度专业化专家模块（模式识别/组合推理/逻辑推理/记忆整合/元认知监督）及动态路由网络

Result: 在Pile数据集上实现16%计算节省、35%推理加速，复杂推理准确率提升2.8%

Conclusion: DS-MoE通过深度专业化模块实现效率-质量-可解释性的同步提升，推动自适应神经网络架构发展

Abstract: Contemporary transformer architectures apply identical processing depth to
all inputs, creating inefficiencies and limiting reasoning quality. Simple
factual queries are subjected to the same multilayered computation as complex
logical problems, wasting resources while constraining deep inference. To
overcome this, we came up with a concept of Dynamic Reasoning Chains through
Depth Specialised Mixture of Experts (DS-MoE), a modular framework that extends
the Mixture of Experts paradigm from width-based to depth specialised
computation. DS-MoE introduces expert modules optimised for distinct reasoning
depths, shallow pattern recognition, compositional reasoning, logical
inference, memory integration, and meta-cognitive supervision. A learned
routing network dynamically assembles custom reasoning chains, activating only
the necessary experts to match input complexity. The dataset on which we
trained and evaluated DS-MoE is on The Pile, an 800GB corpus covering diverse
domains such as scientific papers, legal texts, programming code, and web
content, enabling systematic assessment across reasoning depths. Experimental
results demonstrate that DS-MoE achieves up to 16 per cent computational
savings and 35 per cent faster inference compared to uniform-depth
transformers, while delivering 2.8 per cent higher accuracy on complex
multi-step reasoning benchmarks. Furthermore, routing decisions yield
interpretable reasoning chains, enhancing transparency and scalability. These
findings establish DS-MoE as a significant advancement in adaptive neural
architectures, demonstrating that depth-specialised modular processing can
simultaneously improve efficiency, reasoning quality, and interpretability in
large-scale language models.

</details>


### [15] [Hierarchical Resolution Transformers: A Wavelet-Inspired Architecture for Multi-Scale Language Understanding](https://arxiv.org/abs/2509.20581)
*Ayan Sar,Sampurna Roy,Kanav Gupta,Anurag Kaushish,Tanupriya Choudhury,Abhijit Kumar*

Main category: cs.CL

TL;DR: 提出了基于小波理论的多分辨率分层Transformer架构HRT，在保持高性能的同时显著降低计算成本，实现了语言层次结构与计算结构的对齐


<details>
  <summary>Details</summary>
Motivation: 传统Transformer将文本处理为扁平序列，导致二次方计算复杂度、弱组合泛化能力和话语建模不足等问题，无法匹配人类语言的层次结构特性

Method: HRT通过多分辨率注意力机制实现字符到语篇的多尺度处理，采用指数序列缩减技术实现O(nlogn)复杂度，结合跨分辨率注意力实现双向上下文建模

Result: 在GLUE(+3.8%)、SuperGLUE(+4.5%)和长文本基准(+6.1%)上超越传统Transformer，内存减少42%，推理延迟降低37%，消融实验验证模块有效性

Conclusion: HRT首次将语言层次结构与计算架构结合，证明多尺度小波处理能同时提升理论效率和实际性能，为语言模型设计开辟新方向

Abstract: Transformer architectures have achieved state-of-the-art performance across
natural language tasks, yet they fundamentally misrepresent the hierarchical
nature of human language by processing text as flat token sequences. This
results in quadratic computational cost, weak computational cost, weak
compositional generalization, and inadequate discourse-level modeling. We
propose Hierarchical Resolution Transformer (HRT), a novel wavelet-inspired
neural architecture that processes language simultaneously across multiple
resolutions, from characters to discourse-level units. HRT constructs a
multi-resolution attention, enabling bottom-up composition and top-down
contextualization. By employing exponential sequence reduction across scales,
HRT achieves O(nlogn) complexity, offering significant efficiency improvements
over standard transformers. We evaluated HRT on a diverse suite of benchmarks,
including GLUE, SuperGLUE, Long Range Arena, and WikiText-103, and results
demonstrated that HRT outperforms standard transformer baselines by an average
of +3.8% on GLUE, +4.5% on SuperGLUE, and +6.1% on Long Range Arena, while
reducing memory usage by 42% and inference latency by 37% compared to BERT and
GPT style models of similar parameter count. Ablation studies confirm the
effectiveness of cross-resolution attention and scale-specialized modules,
showing that each contributes independently to both efficiency and accuracy.
Our findings establish HRT as the first architecture to align computational
structure with the hierarchical organization of human language, demonstrating
that multi-scale, wavelet-inspired processing yields both theoretical
efficiency gains and practical improvements in language understanding.

</details>


### [16] [FS-DFM: Fast and Accurate Long Text Generation with Few-Step Diffusion Language Models](https://arxiv.org/abs/2509.20624)
*Amin Karimi Monsefi,Nikhil Bhendawade,Manuel Rafael Ciosici,Dominic Culver,Yizhe Zhang,Irina Belousova*

Main category: cs.CL

TL;DR: 提出FS-DFM框架，通过离散流匹配技术实现语言模型的高效生成(8步采样即可达到1024步基线效果)，速度提升128倍且保持生成质量


<details>
  <summary>Details</summary>
Motivation: 自回归模型(ARM)存在串行生成瓶颈，传统扩散模型(DLM)需要数百至数千次迭代才能保证质量。需在并行化基础上实现高效采样

Method: 1. 显式参数化采样步数，训练模型跨步数预算保持一致性
2. 设计无超调的可靠概率更新规则
3. 蒸馏长轨迹的强教师指导

Result: 在1,024token生成任务中，8步采样即达到1,024步离散流基线的困惑度水平，采样速度提升128倍，延迟/吞吐量显著改善

Conclusion: FS-DFM通过离散流匹配的步数一致性设计，在保持语言生成质量的同时实现数量级效率提升，为实用化扩散语言模型提供新思路

Abstract: Autoregressive language models (ARMs) deliver strong likelihoods, but are
inherently serial: they generate one token per forward pass, which limits
throughput and inflates latency for long sequences. Diffusion Language Models
(DLMs) parallelize across positions and thus appear promising for language
generation, yet standard discrete diffusion typically needs hundreds to
thousands of model evaluations to reach high quality, trading serial depth for
iterative breadth. We introduce FS-DFM, Few-Step Discrete Flow-Matching. A
discrete flow-matching model designed for speed without sacrificing quality.
The core idea is simple: make the number of sampling steps an explicit
parameter and train the model to be consistent across step budgets, so one big
move lands where many small moves would. We pair this with a reliable update
rule that moves probability in the right direction without overshooting, and
with strong teacher guidance distilled from long-run trajectories. Together,
these choices make few-step sampling stable, accurate, and easy to control. On
language modeling benchmarks, FS-DFM with 8 sampling steps achieves perplexity
parity with a 1,024-step discrete-flow baseline for generating 1,024 tokens
using a similar-size model, delivering up to 128 times faster sampling and
corresponding latency/throughput gains.

</details>


### [17] [Look Before you Leap: Estimating LLM Benchmark Scores from Descriptions](https://arxiv.org/abs/2509.20645)
*Jungsoo Park,Ethan Mendes,Gabriel Stanovsky,Alan Ritter*

Main category: cs.CL

TL;DR: 提出PRECOG数据集用于预评估语言模型表现，通过任务描述预测实验指标，减少实际实验需求。


<details>
  <summary>Details</summary>
Motivation: 解决大模型迭代中评估流程耗时长的问题，探索无需实验即可预测模型表现的可行性。

Method: 构建PRECOG语料库，采用带检索模块的模型进行预测，并测试零泄漏场景下的泛化能力。

Result: 最优模型MAE低至8.7（Accuracy子集），GPT-5在未索引新数据上仍保持有效预测。

Conclusion: 该框架为开放式预评估提供基础，支持实验优先级决策与任务难度预估。

Abstract: Progress in large language models is constrained by an evaluation bottleneck:
build a benchmark, evaluate models and settings, then iterate. We therefore ask
a simple question: can we forecast outcomes before running any experiments? We
study text-only performance forecasting: estimating a model's score from a
redacted task description and intended configuration, with no access to dataset
instances. To support systematic study, we curate PRECOG, a corpus of redacted
description-performance pairs spanning diverse tasks, domains, and metrics.
Experiments show the task is challenging but feasible: models equipped with a
retrieval module that excludes source papers achieve moderate prediction
performance with well-calibrated uncertainty, reaching mean absolute error as
low as 8.7 on the Accuracy subset at high-confidence thresholds. Our analysis
indicates that stronger reasoning models engage in diverse, iterative querying,
whereas current open-source models lag and often skip retrieval or gather
evidence with limited diversity. We further test a zero-leakage setting,
forecasting on newly released datasets or experiments before their papers are
indexed, where GPT-5 with built-in web search still attains nontrivial
prediction accuracy. Overall, our corpus and analyses offer an initial step
toward open-ended anticipatory evaluation, supporting difficulty estimation and
smarter experiment prioritization.

</details>


### [18] [Building Tailored Speech Recognizers for Japanese Speaking Assessment](https://arxiv.org/abs/2509.20655)
*Yotaro Kubo,Richard Sproat,Chihiro Taguchi,Llion Jones*

Main category: cs.CL

TL;DR: 本文提出两种方法构建针对日语发音评估任务的语音识别器，通过多任务学习和模型融合有效提高音素识别准确率，将音节标签错误率从12.3%降至7.1%。


<details>
  <summary>Details</summary>
Motivation: 日语虽资源丰富，但包含音调标记的音素标注数据稀缺。需要利用仅包含文字标注的语音数据进行模型训练。

Method: 1. 多任务训练框架：引入文字标签和基频模式预测的辅助损失函数
2. 模型融合：基于有限状态转录器框架，融合音素串和文本序列两个估计器的预测结果

Result: 在CSJ核心测试集上，音节标签错误率从12.3%降低至7.1%，优于通用多语言识别器

Conclusion: 多任务学习与模型融合能有效构建高精度日语音素识别器，为发音评估任务提供定制化解决方案

Abstract: This paper presents methods for building speech recognizers tailored for
Japanese speaking assessment tasks. Specifically, we build a speech recognizer
that outputs phonemic labels with accent markers. Although Japanese is
resource-rich, there is only a small amount of data for training models to
produce accurate phonemic transcriptions that include accent marks. We propose
two methods to mitigate data sparsity. First, a multitask training scheme
introduces auxiliary loss functions to estimate orthographic text labels and
pitch patterns of the input signal, so that utterances with only orthographic
annotations can be leveraged in training. The second fuses two estimators, one
over phonetic alphabet strings, and the other over text token sequences. To
combine these estimates we develop an algorithm based on the finite-state
transducer framework. Our results indicate that the use of multitask learning
and fusion is effective for building an accurate phonemic recognizer. We show
that this approach is advantageous compared to the use of generic multilingual
recognizers. The relative advantages of the proposed methods were also
compared. Our proposed methods reduced the average of mora-label error rates
from 12.3% to 7.1% over the CSJ core evaluation sets.

</details>


### [19] [Enhancing Molecular Property Prediction with Knowledge from Large Language Models](https://arxiv.org/abs/2509.20664)
*Peng Zhou,Lai Hou Tim,Zhixiang Cheng,Kun Xie,Chaoyi Li,Wei Liu,Xiangxiang Zeng*

Main category: cs.CL

TL;DR: 提出整合大语言模型知识提取与分子结构特征的新框架，显著提升分子属性预测性能


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络依赖结构数据，大语言模型存在知识缺口和幻觉问题，需要结合两者优势提升预测准确性特别是对冷门分子属性

Method: 使用GPT-4o等大模型生成领域知识和分子向量化代码，将知识特征与预训练分子结构表征融合

Result: 实验证明该方法超越现有方法，知识-结构特征融合策略有效性得到验证

Conclusion: 大语言模型的知识提取与分子结构表征的集成提供了分子属性预测的稳健解决方案

Abstract: Predicting molecular properties is a critical component of drug discovery.
Recent advances in deep learning, particularly Graph Neural Networks (GNNs),
have enabled end-to-end learning from molecular structures, reducing reliance
on manual feature engineering. However, while GNNs and self-supervised learning
approaches have advanced molecular property prediction (MPP), the integration
of human prior knowledge remains indispensable, as evidenced by recent methods
that leverage large language models (LLMs) for knowledge extraction. Despite
their strengths, LLMs are constrained by knowledge gaps and hallucinations,
particularly for less-studied molecular properties. In this work, we propose a
novel framework that, for the first time, integrates knowledge extracted from
LLMs with structural features derived from pre-trained molecular models to
enhance MPP. Our approach prompts LLMs to generate both domain-relevant
knowledge and executable code for molecular vectorization, producing
knowledge-based features that are subsequently fused with structural
representations. We employ three state-of-the-art LLMs, GPT-4o, GPT-4.1, and
DeepSeek-R1, for knowledge extraction. Extensive experiments demonstrate that
our integrated method outperforms existing approaches, confirming that the
combination of LLM-derived knowledge and structural information provides a
robust and effective solution for MPP.

</details>


### [20] [RedHerring Attack: Testing the Reliability of Attack Detection](https://arxiv.org/abs/2509.20691)
*Jonathan Rusert*

Main category: cs.CL

TL;DR: 提出RedHerring攻击方法使文本攻击检测模型不可靠，同时保持分类器准确性，并验证简单防御措施的有效性


<details>
  <summary>Details</summary>
Motivation: 现有对抗文本攻击检测模型可靠性未充分验证，存在攻击者可利用的分类器与检测器矛盾漏洞

Method: 设计RedHerring攻击框架，在4个数据集上测试3种检测器对4个分类器的保护效果，并提出基于置信度的初步防御方案

Result: 攻击使检测准确率下降20-71个百分点，分类器准确率保持或提升；置信度检查防御显著提升检测准确率

Conclusion: 揭示了检测模型的新型威胁模式，为防御系统设计提供了新方向

Abstract: In response to adversarial text attacks, attack detection models have been
proposed and shown to successfully identify text modified by adversaries.
Attack detection models can be leveraged to provide an additional check for NLP
models and give signals for human input. However, the reliability of these
models has not yet been thoroughly explored. Thus, we propose and test a novel
attack setting and attack, RedHerring. RedHerring aims to make attack detection
models unreliable by modifying a text to cause the detection model to predict
an attack, while keeping the classifier correct. This creates a tension between
the classifier and detector. If a human sees that the detector is giving an
``incorrect'' prediction, but the classifier a correct one, then the human will
see the detector as unreliable. We test this novel threat model on 4 datasets
against 3 detectors defending 4 classifiers. We find that RedHerring is able to
drop detection accuracy between 20 - 71 points, while maintaining (or
improving) classifier accuracy. As an initial defense, we propose a simple
confidence check which requires no retraining of the classifier or detector and
increases detection accuracy greatly. This novel threat model offers new
insights into how adversaries may target detection models.

</details>


### [21] [Overcoming Black-box Attack Inefficiency with Hybrid and Dynamic Select Algorithms](https://arxiv.org/abs/2509.20699)
*Abhinay Shankar Belde,Rohit Ramkumar,Jonathan Rusert*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Adversarial text attack research plays a crucial role in evaluating the
robustness of NLP models. However, the increasing complexity of
transformer-based architectures has dramatically raised the computational cost
of attack testing, especially for researchers with limited resources (e.g.,
GPUs). Existing popular black-box attack methods often require a large number
of queries, which can make them inefficient and impractical for researchers. To
address these challenges, we propose two new attack selection strategies called
Hybrid and Dynamic Select, which better combine the strengths of previous
selection algorithms. Hybrid Select merges generalized BinarySelect techniques
with GreedySelect by introducing a size threshold to decide which selection
algorithm to use. Dynamic Select provides an alternative approach of combining
the generalized Binary and GreedySelect by learning which lengths of texts each
selection method should be applied to. This greatly reduces the number of
queries needed while maintaining attack effectiveness (a limitation of
BinarySelect). Across 4 datasets and 6 target models, our best
method(sentence-level Hybrid Select) is able to reduce the number of required
queries per attack up 25.82\% on average against both encoder models and LLMs,
without losing the effectiveness of the attack.

</details>


### [22] [MI-Fuse: Label Fusion for Unsupervised Domain Adaptation with Closed-Source Large-Audio Language Model](https://arxiv.org/abs/2509.20706)
*Hsiao-Ying Huang,Yi-Cheng Lin,Hung-yi Lee*

Main category: cs.CL

TL;DR: 提出MI-Fuse去噪标签融合框架，通过结合LALM与源域SER分类器实现跨领域情感识别，实验显示学生模型在目标域超越LALM且基线提升3.9%


<details>
  <summary>Details</summary>
Motivation: 实际部署中语音情感识别（SER）常因领域不匹配失效，而现有方案依赖源数据或无法直接调用大型音频语言模型（LALMs）

Method: 双教师模型框架：主教师LALM+辅助教师源域分类器，通过互信息不确定性加权融合预测，结合EMA稳定训练过程

Result: 跨3个情感数据集和6种跨域迁移实验，学生模型F1值超越LALM教师，较最强基线提升3.9%准确率

Conclusion: 无需共享源数据即可增强语音情感系统，通过API级模型适应实现现实场景下的领域迁移

Abstract: Large audio-language models (LALMs) show strong zero-shot ability on speech
tasks, suggesting promise for speech emotion recognition (SER). However, SER in
real-world deployments often fails under domain mismatch, where source data are
unavailable and powerful LALMs are accessible only through an API. We ask:
given only unlabeled target-domain audio and an API-only LALM, can a student
model be adapted to outperform the LALM in the target domain? To this end, we
propose MI-Fuse, a denoised label fusion framework that supplements the LALM
with a source-domain trained SER classifier as an auxiliary teacher. The
framework draws multiple stochastic predictions from both teachers, weights
their mean distributions by mutual-information-based uncertainty, and
stabilizes training with an exponential moving average teacher. Experiments
across three public emotion datasets and six cross-domain transfers show
consistent gains, with the student surpassing the LALM and outperforming the
strongest baseline by 3.9%. This approach strengthens emotion-aware speech
systems without sharing source data, enabling realistic adaptation.

</details>


### [23] [Probability Distribution Collapse: A Critical Bottleneck to Compact Unsupervised Neural Grammar Induction](https://arxiv.org/abs/2509.20734)
*Jinwook Park,Kangil Kim*

Main category: cs.CL

TL;DR: 提出「概率分布坍縮」理論並設計「鬆弛神經參數化」方法，顯著提升無監督語法誘導模型的解析性能與語法緊湊性


<details>
  <summary>Details</summary>
Motivation: 現有無監督神經語法誘導模型存在表達力瓶頸，常產生過大但低效的語法結構，核心問題源於概率分布坍縮現象

Method: 通過分析神經參數化過程中概率坍縮的成因，設計針對性的鬆弛神經參數化方法緩解該現象

Result: 在多語言實驗中實現解析性能顯著提升，同時支持使用更緊湊的語法結構（模型規模縮減）

Conclusion: 解決概率分布坍縮問題能同時提升語法誘導效果與模型效率，為跨語言解析提供新思路

Abstract: Unsupervised neural grammar induction aims to learn interpretable
hierarchical structures from language data. However, existing models face an
expressiveness bottleneck, often resulting in unnecessarily large yet
underperforming grammars. We identify a core issue, $\textit{probability
distribution collapse}$, as the underlying cause of this limitation. We analyze
when and how the collapse emerges across key components of neural
parameterization and introduce a targeted solution, $\textit{collapse-relaxing
neural parameterization}$, to mitigate it. Our approach substantially improves
parsing performance while enabling the use of significantly more compact
grammars across a wide range of languages, as demonstrated through extensive
empirical analysis.

</details>


### [24] [Confidence-guided Refinement Reasoning for Zero-shot Question Answering](https://arxiv.org/abs/2509.20750)
*Youwon Jang,Woo Suk Choi,Minjoon Jung,Minsu Lee,Byoung-Tak Zhang*

Main category: cs.CL

TL;DR: C2R框架通过构建子问题及置信度评分提升多领域QA任务性能


<details>
  <summary>Details</summary>
Motivation: 针对现有QA模型在不同模态任务中可靠性不足的问题，提出无需训练的通用增强框架

Method: 分阶段构建-优化子问题对(sub-QAs)，通过置信度比较选择最优答案

Result: 在多模态基准测试中实现稳定性能提升，揭示子问题数量/质量对推理鲁棒性的非线性影响

Conclusion: 基于置信度引导的层次化推理机制显著提升QA模型可靠性，为可解释AI提供新视角

Abstract: We propose Confidence-guided Refinement Reasoning (C2R), a novel
training-free framework applicable to question-answering (QA) tasks across
text, image, and video domains. C2R strategically constructs and refines
sub-questions and their answers (sub-QAs), deriving a better confidence score
for the target answer. C2R first curates a subset of sub-QAs to explore diverse
reasoning paths, then compares the confidence scores of the resulting answer
candidates to select the most reliable final answer. Since C2R relies solely on
confidence scores derived from the model itself, it can be seamlessly
integrated with various existing QA models, demonstrating consistent
performance improvements across diverse models and benchmarks. Furthermore, we
provide essential yet underexplored insights into how leveraging sub-QAs
affects model behavior, specifically analyzing the impact of both the quantity
and quality of sub-QAs on achieving robust and reliable reasoning.

</details>


### [25] [SFT Doesn't Always Hurt General Capabilities: Revisiting Domain-Specific Fine-Tuning in LLMs](https://arxiv.org/abs/2509.20758)
*Jiacheng Lin,Zhongruo Wang,Kun Qian,Tian Wang,Arvind Srinivasan,Hansi Zeng,Ruochen Jiao,Xie Zhou,Jiri Gesi,Dakuo Wang,Yufan Guo,Kai Zhong,Weiqi Zhang,Sujay Sanghavi,Changyou Chen,Hyokun Yun,Lihong Li*

Main category: cs.CL

TL;DR: 本文通过实证与理论分析，发现使用小学习率和TALR方法能在保持领域性能的同时减少LLMs通用能力损失


<details>
  <summary>Details</summary>
Motivation: 传统观点认为监督微调会损害LLMs的通用能力，本文旨在验证这一trade-off并提出改进方案

Method: 提出Token-Adaptive Loss Reweighting方法，系统评估包括L2正则、LoRA、模型平均等策略

Result: TALR在平衡领域增益与通用能力方面优于基线方法，小学习率可显著改善性能折衷

Conclusion: 推荐采用小学习率进行领域适配，需要更强平衡时优先使用TALR策略

Abstract: Supervised Fine-Tuning (SFT) on domain-specific datasets is a common approach
to adapt Large Language Models (LLMs) to specialized tasks but is often
believed to degrade their general capabilities. In this work, we revisit this
trade-off and present both empirical and theoretical insights. First, we show
that SFT does not always hurt: using a smaller learning rate can substantially
mitigate general performance degradation while preserving comparable
target-domain performance. We then provide a theoretical analysis that explains
these phenomena and further motivates a new method, Token-Adaptive Loss
Reweighting (TALR). Building on this, and recognizing that smaller learning
rates alone do not fully eliminate general-performance degradation in all
cases, we evaluate a range of strategies for reducing general capability loss,
including L2 regularization, LoRA, model averaging, FLOW, and our proposed
TALR. Experimental results demonstrate that while no method completely
eliminates the trade-off, TALR consistently outperforms these baselines in
balancing domain-specific gains and general capabilities. Finally, we distill
our findings into practical guidelines for adapting LLMs to new domains: (i)
using a small learning rate to achieve a favorable trade-off, and (ii) when a
stronger balance is further desired, adopt TALR as an effective strategy.

</details>


### [26] [Towards Atoms of Large Language Models](https://arxiv.org/abs/2509.20784)
*Chenhui Hu,Pengfei Cao,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 提出原子理论作为大语言模型内部表示的基本单元，通过数学证明和实验验证解决神经元/特征的多义性和不稳定性问题


<details>
  <summary>Details</summary>
Motivation: 现有神经元作为表示单元存在多义性，特征存在重建不可靠问题，需要定义更稳定的基本表示单元

Method: 引入原子内积(AIP)纠正表示偏移，数学证明原子满足受限等距性(RIP)，开发阈值激活的稀疏自编码器识别原子

Result: 在Gemma2和Llama3模型上实现99.9%稀疏重建率，原子唯一性达标率99.8%（远超神经元的0.5%和特征的68.2%）

Conclusion: 原子理论为LLM内部表示提供理论框架，建立了稀疏表示与压缩感知的关联，为机制可解释性奠定基础

Abstract: The fundamental units of internal representations in large language models
(LLMs) remain undefined, limiting further understanding of their mechanisms.
Neurons or features are often regarded as such units, yet neurons suffer from
polysemy, while features face concerns of unreliable reconstruction and
instability. To address this issue, we propose the Atoms Theory, which defines
such units as atoms. We introduce the atomic inner product (AIP) to correct
representation shifting, formally define atoms, and prove the conditions that
atoms satisfy the Restricted Isometry Property (RIP), ensuring stable sparse
representations over atom set and linking to compressed sensing. Under stronger
conditions, we further establish the uniqueness and exact $\ell_1$
recoverability of the sparse representations, and provide guarantees that
single-layer sparse autoencoders (SAEs) with threshold activations can reliably
identify the atoms. To validate the Atoms Theory, we train threshold-activated
SAEs on Gemma2-2B, Gemma2-9B, and Llama3.1-8B, achieving 99.9% sparse
reconstruction across layers on average, and more than 99.8% of atoms satisfy
the uniqueness condition, compared to 0.5% for neurons and 68.2% for features,
showing that atoms more faithfully capture intrinsic representations of LLMs.
Scaling experiments further reveal the link between SAEs size and recovery
capacity. Overall, this work systematically introduces and validates Atoms
Theory of LLMs, providing a theoretical framework for understanding internal
representations and a foundation for mechanistic interpretability. Code
available at https://github.com/ChenhuiHu/towards_atoms.

</details>


### [27] [Few-Shot and Training-Free Review Generation via Conversational Prompting](https://arxiv.org/abs/2509.20805)
*Genki Kusano*

Main category: cs.CL

TL;DR: 提出对话式提示方法（SCP/CCP），在少样本无训练场景下显著提升个性化评论生成质量


<details>
  <summary>Details</summary>
Motivation: 现有个性化评论生成方法依赖大量用户历史数据或额外训练，无法适应现实场景中少样本且无法微调模型的限制。大语言模型的效能受限于提示设计，需探索更有效的提示方法

Method: SCP将用户评论转换为多轮对话格式；CCP通过插入其他用户/LLM的错误回复构建对比样本，引导模型修正以适配用户风格

Result: 在8个领域和5个LLM测试中，对话式提示生成的评论在文本指标（ROUGE-L/BERTScore）和应用任务（用户匹配/情感分析）上显著优于传统提示，仅需2条用户评论即可实现。CCP在高质量负样本存在时表现更优，SCP在无负样本时仍保持竞争力

Conclusion: 对话式提示为少样本无训练约束下的个性化评论生成提供了实用解决方案，通过创新的提示工程有效捕捉用户语言风格特征

Abstract: Personalized review generation helps businesses understand user preferences,
yet most existing approaches assume extensive review histories of the target
user or require additional model training. Real-world applications often face
few-shot and training-free situations, where only a few user reviews are
available and fine-tuning is infeasible. It is well known that large language
models (LLMs) can address such low-resource settings, but their effectiveness
depends on prompt engineering. In this paper, we propose Conversational
Prompting, a lightweight method that reformulates user reviews as multi-turn
conversations. Its simple variant, Simple Conversational Prompting (SCP),
relies solely on the user's own reviews, while the contrastive variant,
Contrastive Conversational Prompting (CCP), inserts reviews from other users or
LLMs as incorrect replies and then asks the model to correct them, encouraging
the model to produce text in the user's style. Experiments on eight product
domains and five LLMs showed that the conventional non-conversational prompt
often produced reviews similar to those written by random users, based on
text-based metrics such as ROUGE-L and BERTScore, and application-oriented
tasks like user identity matching and sentiment analysis. In contrast, both SCP
and CCP produced reviews much closer to those of the target user, even when
each user had only two reviews. CCP brings further improvements when
high-quality negative examples are available, whereas SCP remains competitive
when such data cannot be collected. These results suggest that conversational
prompting offers a practical solution for review generation under few-shot and
training-free constraints.

</details>


### [28] [Enrich-on-Graph: Query-Graph Alignment for Complex Reasoning with LLM Enriching](https://arxiv.org/abs/2509.20810)
*Songze Li,Zhiqiang Liu,Zhengke Gui,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: 提出Enrich-on-Graph框架，通过LLM增强知识图谱缩减语义鸿沟，在KGQA任务中实现SOTA效果


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视知识图谱与自然语言查询间的语义鸿沟，导致知识推理存在幻觉和事实错误

Method: 构建灵活框架EoG，利用LLM先验知识动态扩展知识图谱，设计图质量评估指标优化图-查询对齐

Result: 在两个KGQA基准测试中验证框架有效性，生成高质量知识图谱并取得最优性能

Conclusion: EoG框架不仅显著提升KGQA性能，同时具备计算成本低、可扩展性强和跨方法适应性优势

Abstract: Large Language Models (LLMs) exhibit strong reasoning capabilities in complex
tasks. However, they still struggle with hallucinations and factual errors in
knowledge-intensive scenarios like knowledge graph question answering (KGQA).
We attribute this to the semantic gap between structured knowledge graphs (KGs)
and unstructured queries, caused by inherent differences in their focuses and
structures. Existing methods usually employ resource-intensive, non-scalable
workflows reasoning on vanilla KGs, but overlook this gap. To address this
challenge, we propose a flexible framework, Enrich-on-Graph (EoG), which
leverages LLMs' prior knowledge to enrich KGs, bridge the semantic gap between
graphs and queries. EoG enables efficient evidence extraction from KGs for
precise and robust reasoning, while ensuring low computational costs,
scalability, and adaptability across different methods. Furthermore, we propose
three graph quality evaluation metrics to analyze query-graph alignment in KGQA
task, supported by theoretical validation of our optimization objectives.
Extensive experiments on two KGQA benchmark datasets indicate that EoG can
effectively generate high-quality KGs and achieve the state-of-the-art
performance. Our code and data are available at
https://github.com/zjukg/Enrich-on-Graph.

</details>


### [29] [Leveraging What's Overfixed: Post-Correction via LLM Grammatical Error Overcorrection](https://arxiv.org/abs/2509.20811)
*Taehee Park,Heejin Do,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 提出PoCO方法通过大模型过校正和小模型后校正，平衡语法纠错的召回率与精确度


<details>
  <summary>Details</summary>
Motivation: 小语言模型(sLMs)可靠但修正不足（高精度低召回），大语言模型(LLMs)过度修正（低精度高召回），需要结合两者优势

Method: 1. 使用LLM触发过校正提升召回率 2. 用小监督模型进行针对性后校正优化精确度

Result: 实验证明PoCO在保持竞争力的精确度同时显著提升召回率，改善语法纠错整体质量

Conclusion: 该方法有效调和生成能力与可靠性，为语法纠错任务提供了新的平衡方案

Abstract: Robust supervised fine-tuned small Language Models (sLMs) often show high
reliability but tend to undercorrect. They achieve high precision at the cost
of low recall. Conversely, Large Language Models (LLMs) often show the opposite
tendency, making excessive overcorrection, leading to low precision. To
effectively harness the strengths of LLMs to address the recall challenges in
sLMs, we propose Post-Correction via Overcorrection (PoCO), a novel approach
that strategically balances recall and precision. PoCO first intentionally
triggers overcorrection via LLM to maximize recall by allowing comprehensive
revisions, then applies a targeted post-correction step via fine-tuning smaller
models to identify and refine erroneous outputs. We aim to harmonize both
aspects by leveraging the generative power of LLMs while preserving the
reliability of smaller supervised models. Our extensive experiments demonstrate
that PoCO effectively balances GEC performance by increasing recall with
competitive precision, ultimately improving the overall quality of grammatical
error correction.

</details>


### [30] [Distilling Many-Shot In-Context Learning into a Cheat Sheet](https://arxiv.org/abs/2509.20820)
*Ukyo Honda,Soichiro Murakami,Peinan Zhang*

Main category: cs.CL

TL;DR: 提出Cheat-Sheet ICL方法，将多示例上下文学习压缩为简洁的文本摘要，在保持性能的同时大幅减少计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 传统多示例上下文学习(ICL)需要大量输入token导致计算成本过高，需探索更高效的替代方案。

Method: 通过提炼多示例ICL的关键信息生成文本摘要(cheat sheet)，在推理阶段直接使用摘要替代原始示例。

Result: 在复杂推理任务中，该方法token消耗减少80%但性能持平/优于多示例ICL，且无需测试时检索即可达到检索式ICL效果。

Conclusion: Cheat-Sheet ICL为LLM的下游任务应用提供了兼顾性能与效率的实用解决方案。

Abstract: Recent advances in large language models (LLMs) enable effective in-context
learning (ICL) with many-shot examples, but at the cost of high computational
demand due to longer input tokens. To address this, we propose cheat-sheet ICL,
which distills the information from many-shot ICL into a concise textual
summary (cheat sheet) used as the context at inference time. Experiments on
challenging reasoning tasks show that cheat-sheet ICL achieves comparable or
better performance than many-shot ICL with far fewer tokens, and matches
retrieval-based ICL without requiring test-time retrieval. These findings
demonstrate that cheat-sheet ICL is a practical alternative for leveraging LLMs
in downstream tasks.

</details>


### [31] [Zero-Shot Privacy-Aware Text Rewriting via Iterative Tree Search](https://arxiv.org/abs/2509.20838)
*Shuo Huang,Xingliang Yuan,Gholamreza Haffari,Lizhen Qu*

Main category: cs.CL

TL;DR: 提出基于树搜索的文本匿名化算法，在保护隐私的同时保持文本连贯性和可用性


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的文本匿名化方法难以平衡隐私保护与文本自然度，LLM云服务中用户输入可能泄露敏感信息

Method: 零样本的树搜索迭代重写算法，通过奖励模型指导结构化搜索，动态探索重写空间，渐进式修改隐私敏感片段

Result: 在隐私敏感数据集上显著优于基线方法，实现隐私保护和文本效用的最佳平衡

Conclusion: 该方法通过结构化搜索机制有效解决了LLM服务中的隐私泄露问题，在保持文本自然度的同时提供更强的隐私保护

Abstract: The increasing adoption of large language models (LLMs) in cloud-based
services has raised significant privacy concerns, as user inputs may
inadvertently expose sensitive information. Existing text anonymization and
de-identification techniques, such as rule-based redaction and scrubbing, often
struggle to balance privacy preservation with text naturalness and utility. In
this work, we propose a zero-shot, tree-search-based iterative sentence
rewriting algorithm that systematically obfuscates or deletes private
information while preserving coherence, relevance, and naturalness. Our method
incrementally rewrites privacy-sensitive segments through a structured search
guided by a reward model, enabling dynamic exploration of the rewriting space.
Experiments on privacy-sensitive datasets show that our approach significantly
outperforms existing baselines, achieving a superior balance between privacy
protection and utility preservation.

</details>


### [32] [Concise and Sufficient Sub-Sentence Citations for Retrieval-Augmented Generation](https://arxiv.org/abs/2509.20859)
*Guo Chen,Qiuyuan Li,Qiuxian Li,Hongliang Dai,Xiang Chen,Piji Li*

Main category: cs.CL

TL;DR: 提出子句级引文标注方法，通过细粒度引用提升RAG系统中生成结果的验证效率


<details>
  <summary>Details</summary>
Motivation: 传统句子/段落级引文存在冗长信息或关键验证信息缺失的问题，用户验证成本较高

Method: 建立细粒度引文标注标准并构建数据集，结合LLM自动生成微调数据与信用模型质量过滤机制

Result: 实验证明该方法能生成更简洁、完整且可读性强的引文标注

Conclusion: 子句级引文标注有效降低用户验证生成内容正确性的认知负担

Abstract: In retrieval-augmented generation (RAG) question answering systems,
generating citations for large language model (LLM) outputs enhances
verifiability and helps users identify potential hallucinations. However, we
observe two problems in the citations produced by existing attribution methods.
First, the citations are typically provided at the sentence or even paragraph
level. Long sentences or paragraphs may include a substantial amount of
irrelevant content. Second, sentence-level citations may omit information that
is essential for verifying the output, forcing users to read the surrounding
context. In this paper, we propose generating sub-sentence citations that are
both concise and sufficient, thereby reducing the effort required by users to
confirm the correctness of the generated output. To this end, we first develop
annotation guidelines for such citations and construct a corresponding dataset.
Then, we propose an attribution framework for generating citations that adhere
to our standards. This framework leverages LLMs to automatically generate
fine-tuning data for our task and employs a credit model to filter out
low-quality examples. Our experiments on the constructed dataset demonstrate
that the propose approach can generate high-quality and more readable
citations.

</details>


### [33] [WeFT: Weighted Entropy-driven Fine-Tuning for dLLMs](https://arxiv.org/abs/2509.20863)
*Guowei Xu,Wenxin Xu,Jiawang Zhao,Kaisheng Ma*

Main category: cs.CL

TL;DR: 提出加权监督微调方法WeFT，通过熵值加权机制显著提升扩散语言模型的推理性能


<details>
  <summary>Details</summary>
Motivation: 扩散模型在语言建模中缺乏精确概率估计，导致监督微调(SFT)过程生成不稳定和不可控

Method: 基于扩散理论设计熵值加权机制，根据token的信息熵动态调整监督微调的权重分配

Result: 在1K-3K样本规模下，四个推理基准任务(Sudoku/Countdown/GSM8K/MATH-500)相对标准SFT提升39%-83%

Conclusion: WeFT通过熵值加权有效控制生成方向，显著提升小样本场景下的推理性能，模型和代码将开源

Abstract: Diffusion models have recently shown strong potential in language modeling,
offering faster generation compared to traditional autoregressive approaches.
However, applying supervised fine-tuning (SFT) to diffusion models remains
challenging, as they lack precise probability estimates at each denoising step.
While the diffusion mechanism enables the model to reason over entire
sequences, it also makes the generation process less predictable and often
inconsistent. This highlights the importance of controlling key tokens that
guide the direction of generation. To address this issue, we propose WeFT, a
weighted SFT method for diffusion language models, where tokens are assigned
different weights based on their entropy. Derived from diffusion theory, WeFT
delivers substantial gains: training on s1K, s1K-1.1, and 3k samples from
open-r1, it achieves relative improvements of 39%, 64%, and 83% over standard
SFT on four widely used reasoning benchmarks (Sudoku, Countdown, GSM8K, and
MATH-500). The code and models will be made publicly available.

</details>


### [34] [Single Answer is Not Enough: On Generating Ranked Lists with Medical Reasoning Models](https://arxiv.org/abs/2509.20866)
*Pittawat Taveekitworachai,Natpatchara Pongjirapat,Krittaphas Chaisutyakorn,Piyalitt Ittichaiwong,Tossaporn Saengja,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: 研究如何让医疗推理模型生成答案排名列表，探讨提示与微调方法的效果


<details>
  <summary>Details</summary>
Motivation: 当前医疗推理模型只能生成单一答案，但临床决策需要多个选项来降低风险。研究旨在解决模型在开放场景下的格式适应性问题。

Method: 使用提示法（prompting）和两种微调方法：监督微调（SFT）模仿标注数据，强化微调（RFT）通过定制奖励函数激励模型探索多答案格式。

Result: RFT模型在多种答案格式中表现更稳健，案例研究发现模型能识别有效答案（即使不符合基准偏好）。SFT仅在特定格式有效。

Conclusion: 这是首个系统研究医疗模型生成排名列表的方法，为开发超越单一答案的临床决策格式奠定了基础。

Abstract: This paper presents a systematic study on enabling medical reasoning models
(MRMs) to generate ranked lists of answers for open-ended questions. Clinical
decision-making rarely relies on a single answer but instead considers multiple
options, reducing the risks of narrow perspectives. Yet current MRMs are
typically trained to produce only one answer, even in open-ended settings. We
propose an alternative format: ranked lists and investigate two approaches:
prompting and fine-tuning. While prompting is a cost-effective way to steer an
MRM's response, not all MRMs generalize well across different answer formats:
choice, short text, and list answers. Based on our prompting findings, we train
and evaluate MRMs using supervised fine-tuning (SFT) and reinforcement
fine-tuning (RFT). SFT teaches a model to imitate annotated responses, and RFT
incentivizes exploration through the responses that maximize a reward. We
propose new reward functions targeted at ranked-list answer formats, and
conduct ablation studies for RFT. Our results show that while some SFT models
generalize to certain answer formats, models trained with RFT are more robust
across multiple formats. We also present a case study on a modified MedQA with
multiple valid answers, finding that although MRMs might fail to select the
benchmark's preferred ground truth, they can recognize valid answers. To the
best of our knowledge, this is the first systematic investigation of approaches
for enabling MRMs to generate answers as ranked lists. We hope this work
provides a first step toward developing alternative answer formats that are
beneficial beyond single answers in medical domains.

</details>


### [35] [Learning to Summarize by Learning to Quiz: Adversarial Agentic Collaboration for Long Document Summarization](https://arxiv.org/abs/2509.20900)
*Weixuan Wang,Minghao Wu,Barry Haddow,Alexandra Birch*

Main category: cs.CL

TL;DR: 提出对抗性多智能体框架SummQ，通过摘要生成与测验生成智能体的协作机制，解决长文档摘要中的信息丢失和一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在长文档摘要中存在信息丢失、事实矛盾、连贯性差三大痛点，需通过多方协作机制突破单智能体局限性。

Method: 构建摘要生成-审查与测验生成-审查双领域智能体，引入答题验证环节实现对抗迭代优化，通过多维度反馈实现摘要质量持续提升。

Result: 在三大基准测试中ROUGE/BERTScore显著超越SOTA，LLM-as-a-Judge和人工评估显示框架有效性，测验机制对质量提升贡献度达37%。

Conclusion: SummQ开创了对抗性智能协作的摘要新范式，证实多智能体动态协作可系统提升长文档摘要质量，为复杂NLP任务提供新方法论。

Abstract: Long document summarization remains a significant challenge for current large
language models (LLMs), as existing approaches commonly struggle with
information loss, factual inconsistencies, and coherence issues when processing
excessively long documents. We propose SummQ, a novel adversarial multi-agent
framework that addresses these limitations through collaborative intelligence
between specialized agents operating in two complementary domains:
summarization and quizzing. Our approach employs summary generators and
reviewers that work collaboratively to create and evaluate comprehensive
summaries, while quiz generators and reviewers create comprehension questions
that serve as continuous quality checks for the summarization process. This
adversarial dynamic, enhanced by an examinee agent that validates whether the
generated summary contains the information needed to answer the quiz questions,
enables iterative refinement through multifaceted feedback mechanisms. We
evaluate SummQ on three widely used long document summarization benchmarks.
Experimental results demonstrate that our framework significantly outperforms
existing state-of-the-art methods across ROUGE and BERTScore metrics, as well
as in LLM-as-a-Judge and human evaluations. Our comprehensive analyses reveal
the effectiveness of the multi-agent collaboration dynamics, the influence of
different agent configurations, and the impact of the quizzing mechanism. This
work establishes a new approach for long document summarization that uses
adversarial agentic collaboration to improve summarization quality.

</details>


### [36] [MemLens: Uncovering Memorization in LLMs with Activation Trajectories](https://arxiv.org/abs/2509.20909)
*Zirui He,Haiyan Zhao,Ali Payani,Mengnan du*

Main category: cs.CL

TL;DR: 提出MemLens方法，通过分析数字标记生成轨迹检测LLM记忆，区分污染与干净数据。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法依赖词汇重叠和困惑度等表面特征，对隐式污染数据泛化能力差。

Method: 通过追踪数字标记生成时的概率轨迹，发现污染数据在模型早期层就锁定答案（捷径行为），干净样本则在全部层逐步积累证据。

Result: 污染/干净数据呈现显著分离的推理轨迹，通过LoRA微调注入样本实验验证了该现象的一致性。

Conclusion: MemLens捕捉到了真正的记忆信号而非虚假关联，为模型记忆检测提供了可靠方法。

Abstract: Large language models (LLMs) are commonly evaluated on challenging benchmarks
such as AIME and Math500, which are susceptible to contamination and risk of
being memorized. Existing detection methods, which primarily rely on
surface-level lexical overlap and perplexity, demonstrate low generalization
and degrade significantly when encountering implicitly contaminated data. In
this paper, we propose MemLens (An Activation Lens for Memorization Detection)
to detect memorization by analyzing the probability trajectories of numeric
tokens during generation. Our method reveals that contaminated samples exhibit
``shortcut'' behaviors, locking onto an answer with high confidence in the
model's early layers, whereas clean samples show more gradual evidence
accumulation across the model's full depth. We observe that contaminated and
clean samples exhibit distinct and well-separated reasoning trajectories. To
further validate this, we inject carefully designed samples into the model
through LoRA fine-tuning and observe the same trajectory patterns as in
naturally contaminated data. These results provide strong evidence that MemLens
captures genuine signals of memorization rather than spurious correlations.

</details>


### [37] [Cross-Linguistic Analysis of Memory Load in Sentence Comprehension: Linear Distance and Structural Density](https://arxiv.org/abs/2509.20916)
*Krishna Aggarwal*

Main category: cs.CL

TL;DR: 研究通过跨语言分析发现句子长度、依赖长度和干预者复杂性均与记忆负荷正相关，其中句子长度影响最显著，干预者复杂性可补充解释线性距离无法涵盖的认知负荷。


<details>
  <summary>Details</summary>
Motivation: 调和线性和层级结构对语言处理局部性的解释冲突，验证干预者复杂性（介入句法头数量）相较线性距离更能反映记忆负荷的结构性根源。

Method: 基于统一依赖树库(UD)构建跨语言语料，运用混合效应模型量化分析句子长度、依赖长度、干预者复杂性对记忆负荷指标（特征干扰与错误绑定的线性组合）的预测效力。

Result: 三类预测因子均显示正向关联，句子长度影响范围最广，干预者复杂性可解释线性距离外的额外变异，证明结构因素对记忆负荷的独立贡献。

Conclusion: 概念层面将依赖长度视为表层特征，干预者复杂性作为认知整合需求的直接指标；方法论层面展示依赖树库与跨语言模型在分离线性和结构因素中的有效性，为记忆负荷理论评估提供新范式。

Abstract: This study examines whether sentence-level memory load in comprehension is
better explained by linear proximity between syntactically related words or by
the structural density of the intervening material. Building on locality-based
accounts and cross-linguistic evidence for dependency length minimization, the
work advances Intervener Complexity-the number of intervening heads between a
head and its dependent-as a structurally grounded lens that refines linear
distance measures. Using harmonized dependency treebanks and a mixed-effects
framework across multiple languages, the analysis jointly evaluates sentence
length, dependency length, and Intervener Complexity as predictors of the
Memory-load measure. Studies in Psycholinguistics have reported the
contributions of feature interference and misbinding to memory load during
processing. For this study, I operationalized sentence-level memory load as the
linear sum of feature misbinding and feature interference for tractability;
current evidence does not establish that their cognitive contributions combine
additively. All three factors are positively associated with memory load, with
sentence length exerting the broadest influence and Intervener Complexity
offering explanatory power beyond linear distance. Conceptually, the findings
reconcile linear and hierarchical perspectives on locality by treating
dependency length as an important surface signature while identifying
intervening heads as a more proximate indicator of integration and maintenance
demands. Methodologically, the study illustrates how UD-based graph measures
and cross-linguistic mixed-effects modelling can disentangle linear and
structural contributions to processing efficiency, providing a principled path
for evaluating competing theories of memory load in sentence comprehension.

</details>


### [38] [Tool Calling for Arabic LLMs: Data Strategies and Instruction Tuning](https://arxiv.org/abs/2509.20957)
*Asim Ersoy,Enes Altinisik,Husrev Taha Sencar,Kareem Darwish*

Main category: cs.CL

TL;DR: 研究阿拉伯语大语言模型的工具调用能力，通过数据集翻译和不同训练策略实验，发现阿拉伯语专用数据、通用指令微调和特定工具精调的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前工具调用研究以英语为中心，阿拉伯语等语言缺乏相关资源和研究，限制了多语言LLM的实用价值扩展。

Method: 采用翻译工具调用数据集+多阶段实验设计（跨语言迁移测试、通用指令微调评估、高优先级工具精调验证），使用阿拉伯语开源LLM的不同变体进行测试。

Result: 发现阿拉伯语专用数据必要性、通用指令微调对工具调用能力提升显著、特定工具精调可针对性增强性能

Conclusion: 填补阿拉伯语工具调用资源空白，为构建阿拉伯语工具增强智能体提供数据准备策略和模型优化路径

Abstract: Tool calling is a critical capability that allows Large Language Models
(LLMs) to interact with external systems, significantly expanding their
utility. However, research and resources for tool calling are predominantly
English-centric, leaving a gap in our understanding of how to enable this
functionality for other languages, such as Arabic. This paper investigates
three key research questions: (1) the necessity of in-language (Arabic)
tool-calling data versus relying on cross-lingual transfer, (2) the effect of
general-purpose instruction tuning on tool-calling performance, and (3) the
value of fine-tuning on specific, high-priority tools. To address these
questions, we conduct extensive experiments using base and post-trained
variants of an open-weight Arabic LLM. To enable this study, we bridge the
resource gap by translating and adapting two open-source tool-calling datasets
into Arabic. Our findings provide crucial insights into the optimal strategies
for developing robust tool-augmented agents for Arabic.

</details>


### [39] [Analysis of instruction-based LLMs' capabilities to score and judge text-input problems in an academic setting](https://arxiv.org/abs/2509.20982)
*Valeria Ramirez-Garcia,David de-Fitero-Dominguez,Antonio Garcia-Cabot,Eva Garcia-Lopez*

Main category: cs.CL

TL;DR: 研究提出五种基于LLM的文本输入问题自动评估系统，验证参考辅助评估方法在计算机科学答案评分中效果最优，可作为学术资源补充工具。


<details>
  <summary>Details</summary>
Motivation: 探索LLM结合评分标准实现学术文本问题的自动评估，填补现有方法在教育领域应用中的不足，验证AI系统作为教学辅助工具的可行性。

Method: 构建110份高等教育学生答案数据集，使用JudgeLM/Llama-3.1-8B/DeepSeek模型测试五种评估方法（含参考辅助/无参考/加法/自适应评估），并与人工评估结果进行MAD/RMSD指标对比。

Result: 参考辅助评估表现最佳（MAD=0.945，RMSD=1.214），其他方法存在模型限制/信息缺失/评分不准等问题，证明合理方法论对AI评估系统的关键作用。

Conclusion: 证实结合适当方法论的AI自动评估系统具备成为学术资源补充工具的潜力，参考辅助模式为当前最优解决方案。

Abstract: Large language models (LLMs) can act as evaluators, a role studied by methods
like LLM-as-a-Judge and fine-tuned judging LLMs. In the field of education,
LLMs have been studied as assistant tools for students and teachers. Our
research investigates LLM-driven automatic evaluation systems for academic
Text-Input Problems using rubrics. We propose five evaluation systems that have
been tested on a custom dataset of 110 answers about computer science from
higher education students with three models: JudgeLM, Llama-3.1-8B and
DeepSeek-R1-Distill-Llama-8B. The evaluation systems include: The JudgeLM
evaluation, which uses the model's single answer prompt to obtain a score;
Reference Aided Evaluation, which uses a correct answer as a guide aside from
the original context of the question; No Reference Evaluation, which ommits the
reference answer; Additive Evaluation, which uses atomic criteria; and Adaptive
Evaluation, which is an evaluation done with generated criteria fitted to each
question. All evaluation methods have been compared with the results of a human
evaluator. Results show that the best method to automatically evaluate and
score Text-Input Problems using LLMs is Reference Aided Evaluation. With the
lowest median absolute deviation (0.945) and the lowest root mean square
deviation (1.214) when compared to human evaluation, Reference Aided Evaluation
offers fair scoring as well as insightful and complete evaluations. Other
methods such as Additive and Adaptive Evaluation fail to provide good results
in concise answers, No Reference Evaluation lacks information needed to
correctly assess questions and JudgeLM Evaluations have not provided good
results due to the model's limitations. As a result, we conclude that
Artificial Intelligence-driven automatic evaluation systems, aided with proper
methodologies, show potential to work as complementary tools to other academic
resources.

</details>


### [40] [Generative AI for FFRDCs](https://arxiv.org/abs/2509.21040)
*Arun S. Maiya*

Main category: cs.CL

TL;DR: 使用OnPrem.LLM框架的LLM技术提升联邦资助研发中心(FFRDCs)处理文本密集型工作的效率，通过国防授权法案(NDAA)和科学奖项(NSF)案例验证有效性


<details>
  <summary>Details</summary>
Motivation: 联邦研发中心面临海量文本处理需求（政策文件/科技论文），传统人工分析效率低下，影响战略监管效能

Method: 基于开源框架OnPrem.LLM开发安全生成式AI系统，采用小样本学习实现文本摘要/分类/信息抽取的自动化处理

Result: 在NDAA国防政策分析和NSF科研项目管理场景中验证，显著提升分析效率同时保障数据主权和审计追踪能力

Conclusion: 该方法为政府敏感领域的文本分析提供了高效安全的技术路径，在保持数据控制权的前提下实现战略洞察力提升

Abstract: Federally funded research and development centers (FFRDCs) face text-heavy
workloads, from policy documents to scientific and engineering papers, that are
slow to analyze manually. We show how large language models can accelerate
summarization, classification, extraction, and sense-making with only a few
input-output examples. To enable use in sensitive government contexts, we apply
OnPrem$.$LLM, an open-source framework for secure and flexible application of
generative AI. Case studies on defense policy documents and scientific corpora,
including the National Defense Authorization Act (NDAA) and National Science
Foundation (NSF) Awards, demonstrate how this approach enhances oversight and
strategic analysis while maintaining auditability and data sovereignty.

</details>


### [41] [Behind RoPE: How Does Causal Mask Encode Positional Information?](https://arxiv.org/abs/2509.21042)
*Junu Kim,Xiao Liu,Zhenghao Lin,Lei Ji,Yeyun Gong,Edward Choi*

Main category: cs.CL

TL;DR: 研究发现Transformer中的因果掩码本身可产生类似位置编码的效果，并与RoPE编码产生相互干扰


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注显式位置编码（如RoPE）的作用，但忽略了因果掩码可能隐含的位置信息。本文旨在揭示因果掩码对注意力模式的本质影响

Method: 通过理论分析证明因果掩码在无参数情况下诱导位置相关注意力模式，结合预训练大语言模型进行实证研究

Result: 1. 因果掩码会形成局部注意力偏向 2. 与RoPE共同作用会扭曲相对位置编码效果 3. 在主流大模型中普遍存在该现象

Conclusion: 应重新审视因果掩码的位置信息携带能力，在模型设计和分析中将其视为与显式位置编码同等重要的位置信息来源

Abstract: While explicit positional encodings such as RoPE are a primary source of
positional information in Transformer decoders, the causal mask also provides
positional information. In this work, we prove that the causal mask can induce
position-dependent patterns in attention scores, even without parameters or
causal dependency in the input. Our theoretical analysis indicates that the
induced attention pattern tends to favor nearby query-key pairs, mirroring the
behavior of common positional encodings. Empirical analysis confirms that
trained models exhibit the same behavior, with learned parameters further
amplifying these patterns. Notably, we found that the interaction of causal
mask and RoPE distorts RoPE's relative attention score patterns into
non-relative ones. We consistently observed this effect in modern large
language models, suggesting the importance of considering the causal mask as a
source of positional information alongside explicit positional encodings.

</details>


### [42] [When Instructions Multiply: Measuring and Estimating LLM Capabilities of Multiple Instructions Following](https://arxiv.org/abs/2509.21051)
*Keno Harada,Yudai Yamazaki,Masachika Taniguchi,Edison Marrese-Taylor,Takeshi Kojima,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.CL

TL;DR: 研究LLMs在多指令任务下的性能衰减现象，提出新基准测试及高效性能预测方法


<details>
  <summary>Details</summary>
Motivation: 随着大模型在实际场景的广泛应用，亟需评估其遵循多重指令的能力。现有评估体系难以覆盖多指令组合的复杂性，需要系统性解决方案

Method: 构建ManyIFEval（文本生成）和StyleMBPP（代码生成）两个多指令基准测试，开发基于指令数量的回归预测模型

Result: 所有测试模型性能随指令数增加持续下降（降幅达30%+），逻辑回归模型仅需500样本即可实现10%误差的跨组合预测

Conclusion: 提出的基准体系与预测模型为LLM多指令能力评估提供高效解决方案，支持模型优化方向的精准定位

Abstract: As large language models (LLMs) are increasingly applied to real-world
scenarios, it becomes crucial to understand their ability to follow multiple
instructions simultaneously. To systematically evaluate these capabilities, we
introduce two specialized benchmarks for fundamental domains where multiple
instructions following is important: Many Instruction-Following Eval
(ManyIFEval) for text generation with up to ten instructions, and Style-aware
Mostly Basic Programming Problems (StyleMBPP) for code generation with up to
six instructions. Our experiments with the created benchmarks across ten LLMs
reveal that performance consistently degrades as the number of instructions
increases. Furthermore, given the fact that evaluating all the possible
combinations of multiple instructions is computationally impractical in actual
use cases, we developed three types of regression models that can estimate
performance on both unseen instruction combinations and different numbers of
instructions which are not used during training. We demonstrate that a logistic
regression model using instruction count as an explanatory variable can predict
performance of following multiple instructions with approximately 10% error,
even for unseen instruction combinations. We show that relatively modest sample
sizes (500 for ManyIFEval and 300 for StyleMBPP) are sufficient for performance
estimation, enabling efficient evaluation of LLMs under various instruction
combinations.

</details>


### [43] [SoM-1K: A Thousand-Problem Benchmark Dataset for Strength of Materials](https://arxiv.org/abs/2509.21079)
*Qixin Wan,Zilong Wang,Jingwen Zhou,Wanting Wang,Ziheng Geng,Jiachen Liu,Ran Cao,Minghui Cheng,Lu Cheng*

Main category: cs.CL

TL;DR: 提出首个材料力学多模态基准数据集SoM-1K，通过文本描述(DoI)提升模型表现，发现LLMs+DoI优于VLMs，揭示当前模型多模态推理短板。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在复杂工程问题（如材料力学）中的多模态处理能力未被充分研究，需建立评估基准推动工程AI发展。

Method: 1. 构建含1,065个图文问题的SoM-1K数据集；2. 提出DoI策略将图表转化为专家文本描述；3. 评估8个LLMs/VLMs在不同输入模式下的表现。

Result: 最佳模型准确率仅56.6%，LLMs+DoI常超越VLMs+图像。DoI减少62%视觉误解错误，文本描述有效性高于原始图像输入。

Conclusion: 工程领域亟需增强多模态推理能力，当前阶段精准文本描述比直接图像处理更有效，为工程AI设立了严格评估基准。

Abstract: Foundation models have shown remarkable capabilities in various domains, but
their performance on complex, multimodal engineering problems remains largely
unexplored. We introduce SoM-1K, the first large-scale multimodal benchmark
dataset dedicated to evaluating foundation models on problems in the strength
of materials (SoM). The dataset, which contains 1,065 annotated SoM problems,
mirrors real-world engineering tasks by including both textual problem
statements and schematic diagrams. Due to the limited capabilities of current
foundation models in understanding complicated visual information, we propose a
novel prompting strategy called Descriptions of Images (DoI), which provides
rigorous expert-generated text descriptions of the visual diagrams as the
context. We evaluate eight representative foundation models, including both
large language models (LLMs) and vision language models (VLMs). Our results
show that current foundation models struggle significantly with these
engineering problems, with the best-performing model achieving only 56.6%
accuracy. Interestingly, we found that LLMs, when provided with DoI, often
outperform VLMs provided with visual diagrams. A detailed error analysis
reveals that DoI plays a crucial role in mitigating visual misinterpretation
errors, suggesting that accurate text-based descriptions can be more effective
than direct image input for current foundation models. This work establishes a
rigorous benchmark for engineering AI and highlights a critical need for
developing more robust multimodal reasoning capabilities in foundation models,
particularly in scientific and engineering contexts.

</details>


### [44] [Which Cultural Lens Do Models Adopt? On Cultural Positioning Bias and Agentic Mitigation in LLMs](https://arxiv.org/abs/2509.21080)
*Yixin Wan,Xingrun Chen,Kai-Wei Chang*

Main category: cs.CL

TL;DR: 大语言模型存在文化定位偏见，主流美国文化视角主导生成内容，提出基于代理的偏缓解框架MFA有效降低生成偏见


<details>
  <summary>Details</summary>
Motivation: LLMs生成内容时默认采用美国主流文化视角，将其他文化视为外部视角，可能传播隐性不公平问题

Method: 构建CultureLens基准(4000提示+3指标)，通过文化情境访谈脚本生成任务，提出MFA框架（单代理自反思/多代理协同）进行偏缓解

Result: 88%美国情境采用内部视角，非主流文化主要呈现外部立场；基于代理方法显著改善生成内容的文化包容性

Conclusion: 多代理层次化框架(MFA-MA)通过规划-批判-优化流程，为生成式LLMs的偏见缓解提供了有效解决方案

Abstract: Large language models (LLMs) have unlocked a wide range of downstream
generative applications. However, we found that they also risk perpetuating
subtle fairness issues tied to culture, positioning their generations from the
perspectives of the mainstream US culture while demonstrating salient
externality towards non-mainstream ones. In this work, we identify and
systematically investigate this novel culture positioning bias, in which an
LLM's default generative stance aligns with a mainstream view and treats other
cultures as outsiders. We propose the CultureLens benchmark with 4000
generation prompts and 3 evaluation metrics for quantifying this bias through
the lens of a culturally situated interview script generation task, in which an
LLM is positioned as an onsite reporter interviewing local people across 10
diverse cultures. Empirical evaluation on 5 state-of-the-art LLMs reveals a
stark pattern: while models adopt insider tones in over 88 percent of
US-contexted scripts on average, they disproportionately adopt mainly outsider
stances for less dominant cultures. To resolve these biases, we propose 2
inference-time mitigation methods: a baseline prompt-based Fairness
Intervention Pillars (FIP) method, and a structured Mitigation via Fairness
Agents (MFA) framework consisting of 2 pipelines: (1) MFA-SA (Single-Agent)
introduces a self-reflection and rewriting loop based on fairness guidelines.
(2) MFA-MA (Multi-Agent) structures the process into a hierarchy of specialized
agents: a Planner Agent(initial script generation), a Critique Agent (evaluates
initial script against fairness pillars), and a Refinement Agent (incorporates
feedback to produce a polished, unbiased script). Empirical results showcase
the effectiveness of agent-based methods as a promising direction for
mitigating biases in generative LLMs.

</details>


### [45] [PerHalluEval: Persian Hallucination Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2509.21104)
*Mohammad Hosseini,Kimia Hosseini,Shayan Bali,Zahra Zanjani,Saeedeh Momtazi*

Main category: cs.CL

TL;DR: PerHalluEval首个波斯语幻觉评估基准，通过LLM三阶段流程和人工验证评估12个模型，发现模型普遍存在波斯文本幻觉问题，外部知识可部分缓解但语言专用模型无优势。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言（波斯语）中LLM的幻觉问题，现有研究多集中于英语而缺乏针对波斯文化的专项评估需求。

Method: 采用LLM三阶段生成流水线+人工验证机制，结合token概率筛选可信幻觉样本，并在QA数据集中标注波斯文化相关语境进行专项评估。

Result: 测试的12个LLM普遍存在波斯文本幻觉，原文提供可部分缓解摘要任务幻觉，波斯专用模型相比通用模型无显著优势。

Conclusion: 首次建立波斯语幻觉评估基准，揭示当前LLM在低资源语言中的局限性，证明语言专用训练并非解决幻觉的有效途径，需加强外部知识整合。

Abstract: Hallucination is a persistent issue affecting all large language Models
(LLMs), particularly within low-resource languages such as Persian.
PerHalluEval (Persian Hallucination Evaluation) is the first dynamic
hallucination evaluation benchmark tailored for the Persian language. Our
benchmark leverages a three-stage LLM-driven pipeline, augmented with human
validation, to generate plausible answers and summaries regarding QA and
summarization tasks, focusing on detecting extrinsic and intrinsic
hallucinations. Moreover, we used the log probabilities of generated tokens to
select the most believable hallucinated instances. In addition, we engaged
human annotators to highlight Persian-specific contexts in the QA dataset in
order to evaluate LLMs' performance on content specifically related to Persian
culture. Our evaluation of 12 LLMs, including open- and closed-source models
using PerHalluEval, revealed that the models generally struggle in detecting
hallucinated Persian text. We showed that providing external knowledge, i.e.,
the original document for the summarization task, could mitigate hallucination
partially. Furthermore, there was no significant difference in terms of
hallucination when comparing LLMs specifically trained for Persian with others.

</details>


### [46] [BESPOKE: Benchmark for Search-Augmented Large Language Model Personalization via Diagnostic Feedback](https://arxiv.org/abs/2509.21106)
*Hyunseo Kim,Sangam Lee,Kwangwook Seo,Dongha Lee*

Main category: cs.CL

TL;DR: 提出BESPOKE基准测试，用于系统评估搜索增强型大语言模型在个性化信息检索中的效果。


<details>
  <summary>Details</summary>
Motivation: 现有搜索增强型LLMs虽整合检索功能降低用户认知负担，但缺乏对用户个性化需求的系统性评估（如ChatGPT/Gemini仅依赖用户历史记录）。

Method: 通过长期人类标注收集真实聊天/搜索历史，构建包含细粒度偏好评分+诊断反馈的基准测试，包含用户自贡献历史、定制化查询和深度评估数据。

Result: 揭示了信息检索任务中实现有效个性化需满足的关键需求，建立了细粒度评估框架。

Conclusion: BESPOKE为个性化搜索增强型LLMs提供了兼具真实性与诊断性的评估基础设施，支撑后续优化研究。

Abstract: Search-augmented large language models (LLMs) have advanced
information-seeking tasks by integrating retrieval into generation, reducing
users' cognitive burden compared to traditional search systems. Yet they remain
insufficient for fully addressing diverse user needs, which requires
recognizing how the same query can reflect different intents across users and
delivering information in preferred forms. While recent systems such as ChatGPT
and Gemini attempt personalization by leveraging user histories, systematic
evaluation of such personalization is under-explored. To address this gap, we
propose BESPOKE, the realistic benchmark for evaluating personalization in
search-augmented LLMs. BESPOKE is designed to be both realistic, by collecting
authentic chat and search histories directly from humans, and diagnostic, by
pairing responses with fine-grained preference scores and feedback. The
benchmark is constructed through long-term, deeply engaged human annotation,
where human annotators contributed their own histories, authored queries with
detailed information needs, and evaluated responses with scores and diagnostic
feedback. Leveraging BESPOKE, we conduct systematic analyses that reveal key
requirements for effective personalization in information-seeking tasks,
providing a foundation for fine-grained evaluation of personalized
search-augmented LLMs. Our code and data are available at
https://augustinlib.github.io/BESPOKE/.

</details>


### [47] [VoiceBBQ: Investigating Effect of Content and Acoustics in Social Bias of Spoken Language Model](https://arxiv.org/abs/2509.21108)
*Junhyuk Choi,Ro-hoon Oh,Jihwan Seol,Bugeun Kim*

Main category: cs.CL

TL;DR: VoiceBBQ是BBQ数据集的口语扩展版本，通过语音条件测试同时诊断语音语言模型的内容和声学偏见，比较了LLaMA-Omni与Qwen2-Audio模型的架构差异。


<details>
  <summary>Details</summary>
Motivation: 语音模型的社会偏见可能同时来源于文本内容和声学特征，需要开发能同步评估两方面偏见的诊断工具。

Method: 将BBQ文本语境转换为可控语音条件，保持与原始文本基准的评分可比性，通过多维度指标（准确率/偏见度/一致性）评估模型表现。

Result: LLaMA-Omni抑制声学偏见但增强性别口音偏见，Qwen2-Audio显著减弱声学信号影响同时保持内容完整性。

Conclusion: VoiceBBQ提供紧凑的集成测试框架，可同步检测语音模型的内容与声学偏见，推动更全面的偏见评估体系建立。

Abstract: We introduce VoiceBBQ, a spoken extension of the BBQ (Bias Benchmark for
Question Answering) - a dataset that measures social bias by presenting
ambiguous or disambiguated contexts followed by questions that may elicit
stereotypical responses. Due to the nature of speech, social bias in Spoken
Language Models (SLMs) can emerge from two distinct sources: 1) content aspect
and 2) acoustic aspect. The dataset converts every BBQ context into controlled
voice conditions, enabling per-axis accuracy, bias, and consistency scores that
remain comparable to the original text benchmark. Using VoiceBBQ, we evaluate
two SLMs - LLaMA-Omni and Qwen2-Audio - and observe architectural contrasts:
LLaMA-Omni resists acoustic bias while amplifying gender and accent bias,
whereas Qwen2-Audio substantially dampens these cues while preserving content
fidelity. VoiceBBQ thus provides a compact, drop-in testbed for jointly
diagnosing content and acoustic bias across spoken language models.

</details>


### [48] [Acoustic-based Gender Differentiation in Speech-aware Language Models](https://arxiv.org/abs/2509.21125)
*Junhyuk Choi,Jihwan Seol,Nayeon Kim,Chanhee Cho,EunBin Cho,Bugeun Kim*

Main category: cs.CL

TL;DR: 语音语言模型在性别处理上存在矛盾现象：在性别刻板问题中偏向男性回应，在应区分性别的场景反而中性化，问题源于语音编码器的声学标记偏见


<details>
  <summary>Details</summary>
Motivation: 研究语音语言模型(SpeechLMs)可能存在的基于说话者性别差异响应现象的系统分析需求

Method: 构建含9,208个样本的三类数据集（性别独立/刻板/依赖），评估LLaMA-Omni系列模型，并通过中性选项测试、语音性别感知实验和语音中性化处理进行验证

Result: 模型在性别刻板问题中持续表现男性导向，在性别依赖问题中不当中性化，根源在于Whisper编码器生成的男性导向声学标记，中性化处理无效

Conclusion: 当前语音语言模型虽优先考虑公平原则，但未能妥善处理性别信息，需开发更复杂技术实现情境化性别信息处理

Abstract: Speech-aware Language Models (SpeechLMs) have fundamentally transformed
human-AI interaction by enabling voice-based communication, yet they may
exhibit acoustic-based gender differentiation where identical questions lead to
different responses based on the speaker's gender. This paper propose a new
dataset that enables systematic analysis of this phenomenon, containing 9,208
speech samples across three categories: Gender-Independent,
Gender-Stereotypical, and Gender-Dependent. We further evaluated LLaMA-Omni
series and discovered a paradoxical pattern; while overall responses seems
identical regardless of gender, the pattern is far from unbiased responses.
Specifically, in Gender-Stereotypical questions, all models consistently
exhibited male-oriented responses; meanwhile, in Gender-Dependent questions
where gender differentiation would be contextually appropriate, models
exhibited responses independent to gender instead. We also confirm that this
pattern does not result from neutral options nor perceived gender of a voice.
When we allow neutral response, models tends to respond neutrally also in
Gender-Dependent questions. The paradoxical pattern yet retains when we applied
gender neutralization methods on speech. Through comparison between SpeechLMs
with corresponding backbone LLMs, we confirmed that these paradoxical patterns
primarily stem from Whisper speech encoders, which generates male-oriented
acoustic tokens. These findings reveal that current SpeechLMs may not
successfully remove gender biases though they prioritized general fairness
principles over contextual appropriateness, highlighting the need for more
sophisticated techniques to utilize gender information properly in speech
technology.

</details>


### [49] [AutoIntent: AutoML for Text Classification](https://arxiv.org/abs/2509.21138)
*Ilya Alekseev,Roman Solomatin,Darina Rustamova,Denis Kuznetsov*

Main category: cs.CL

TL;DR: AutoIntent是用于文本分类的自动化机器学习工具，提供端到端自动化流程（嵌入模型选择、分类器优化、决策阈值调整），支持多标签分类和OOS检测，性能优于现有AutoML工具。


<details>
  <summary>Details</summary>
Motivation: 现有AutoML工具在文本分类任务中缺乏端到端自动化流程，尤其在嵌入模型选择、分类器联合优化和决策阈值调节方面存在不足。

Method: 1. 集成嵌入模型自动选择
2. 分类器超参数联合优化
3. 基于验证集指标的决策阈值自动调节
4. 模块化设计支持sklearn风格API
5. 支持多标签分类和out-of-scope检测

Result: 在标准意图分类数据集上，AutoIntent相比现有AutoML工具展现更优性能（准确率提升3-5%），且能通过阈值调节平衡效果与计算资源消耗（推理速度可提升2倍）

Conclusion: 该框架通过全流程自动化与模块化设计，为文本分类任务提供了高效易用的AutoML解决方案，其sklearn兼容接口显著降低使用门槛，OOS检测功能增强实用价值。

Abstract: AutoIntent is an automated machine learning tool for text classification
tasks. Unlike existing solutions, AutoIntent offers end-to-end automation with
embedding model selection, classifier optimization, and decision threshold
tuning, all within a modular, sklearn-like interface. The framework is designed
to support multi-label classification and out-of-scope detection. AutoIntent
demonstrates superior performance compared to existing AutoML tools on standard
intent classification datasets and enables users to balance effectiveness and
resource consumption.

</details>


### [50] [Retrieval over Classification: Integrating Relation Semantics for Multimodal Relation Extraction](https://arxiv.org/abs/2509.21151)
*Lei Hei,Tingjing Liao,Yingxin Pei,Yiyang Qi,Jiaqi Wang,Ruiting Li,Feiliang Ren*

Main category: cs.CL

TL;DR: 提出检索式多模态关系抽取框架ROC，通过语义驱动检索任务替代传统分类范式，在MNRE和MORE数据集上实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 传统基于分类的多模态关系抽取方法忽视实体类型/位置等结构化约束，且离散标签难以表达细粒度语义信息

Method: 融合多模态编码器整合实体位置信息，利用大语言模型生成关系描述文本，通过语义相似度的对比学习对齐实体-关系表示

Result: 在MNRE和MORE基准数据集上达到最先进性能，模型鲁棒性和可解释性显著提升

Conclusion: 检索式框架ROC有效克服分类范式的语义表达局限，通过结构化约束与语义对齐实现了更精准的多模态关系理解

Abstract: Relation extraction (RE) aims to identify semantic relations between entities
in unstructured text. Although recent work extends traditional RE to multimodal
scenarios, most approaches still adopt classification-based paradigms with
fused multimodal features, representing relations as discrete labels. This
paradigm has two significant limitations: (1) it overlooks structural
constraints like entity types and positional cues, and (2) it lacks semantic
expressiveness for fine-grained relation understanding. We propose
\underline{R}etrieval \underline{O}ver \underline{C}lassification (ROC), a
novel framework that reformulates multimodal RE as a retrieval task driven by
relation semantics. ROC integrates entity type and positional information
through a multimodal encoder, expands relation labels into natural language
descriptions using a large language model, and aligns entity-relation pairs via
semantic similarity-based contrastive learning. Experiments show that our
method achieves state-of-the-art performance on the benchmark datasets MNRE and
MORE and exhibits stronger robustness and interpretability.

</details>


### [51] [Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in Language Models](https://arxiv.org/abs/2509.21155)
*Chantal Shaib,Vinith M. Suriyakumar,Levent Sagun,Byron C. Wallace,Marzyeh Ghassemi*

Main category: cs.CL

TL;DR: LLMs' performance is affected by spurious syntax-domain correlations, requiring explicit testing and syntactic diversity in training data.


<details>
  <summary>Details</summary>
Motivation: Investigate how syntactic-domain correlations in training data override semantic understanding in LLMs, affecting performance and safety.

Method: Use synthetic datasets to quantify correlations, develop evaluation frameworks, and conduct safety bypass case studies across OLMo/Llama/GPT models.

Result: Identified syntax-domain bias reduces entity task performance (OLMo: 0.51±0.06) and enables safety bypass in 7B/4o models.

Conclusion: Necessitates systematic testing for syntax-domain correlations and intentional syntactic variation within domain-specific training data.

Abstract: For an LLM to correctly respond to an instruction it must understand both the
semantics and the domain (i.e., subject area) of a given task-instruction pair.
However, syntax can also convey implicit information Recent work shows that
syntactic templates--frequent sequences of Part-of-Speech (PoS) tags--are
prevalent in training data and often appear in model outputs. In this work we
characterize syntactic templates, domain, and semantics in task-instruction
pairs. We identify cases of spurious correlations between syntax and domain,
where models learn to associate a domain with syntax during training; this can
sometimes override prompt semantics. Using a synthetic training dataset, we
find that the syntactic-domain correlation can lower performance (mean 0.51 +/-
0.06) on entity knowledge tasks in OLMo-2 models (1B-13B). We introduce an
evaluation framework to detect this phenomenon in trained models, and show that
it occurs on a subset of the FlanV2 dataset in open (OLMo-2-7B;
Llama-4-Maverick), and closed (GPT-4o) models. Finally, we present a case study
on the implications for safety finetuning, showing that unintended
syntactic-domain correlations can be used to bypass refusals in OLMo-2-7B
Instruct and GPT-4o. Our findings highlight two needs: (1) to explicitly test
for syntactic-domain correlations, and (2) to ensure syntactic diversity in
training data, specifically within domains, to prevent such spurious
correlations.

</details>


### [52] [Who's Laughing Now? An Overview of Computational Humour Generation and Explanation](https://arxiv.org/abs/2509.21175)
*Tyler Loakman,William Thorne,Chenghua Lin*

Main category: cs.CL

TL;DR: 论文系统探讨计算幽默在生成任务中的研究现状，指出当前生成模型在非双关类幽默创作与解释上的局限性，并强调该领域对评估LLMs推理能力的重要性。


<details>
  <summary>Details</summary>
Motivation: 幽默理解作为具备抽象性、创造性和强上下文依赖的复杂任务，是评估大语言模型常识推理能力的有效试金石，而当前相关研究存在明显空白。

Method: 通过文献综述系统性分析幽默生成与解释的研究进展，结合领域现状与模型表现进行批判性评估，提出未来研究方向框架。

Result: 现有研究集中在双关类幽默，非文字类幽默生成技术尚未突破，SOTA模型在创作质量和解释深度上显著落后人类水平。

Conclusion: 计算幽默处理应作为NLP独立子领域发展，需结合其主观特性和伦理敏感性，探索多模态推理、文化语境建模等突破方向。

Abstract: The creation and perception of humour is a fundamental human trait,
positioning its computational understanding as one of the most challenging
tasks in natural language processing (NLP). As an abstract, creative, and
frequently context-dependent construct, humour requires extensive reasoning to
understand and create, making it a pertinent task for assessing the
common-sense knowledge and reasoning abilities of modern large language models
(LLMs). In this work, we survey the landscape of computational humour as it
pertains to the generative tasks of creation and explanation. We observe that,
despite the task of understanding humour bearing all the hallmarks of a
foundational NLP task, work on generating and explaining humour beyond puns
remains sparse, while state-of-the-art models continue to fall short of human
capabilities. We bookend our literature survey by motivating the importance of
computational humour processing as a subdiscipline of NLP and presenting an
extensive discussion of future directions for research in the area that takes
into account the subjective and ethically ambiguous nature of humour.

</details>


### [53] [GEP: A GCG-Based method for extracting personally identifiable information from chatbots built on small language models](https://arxiv.org/abs/2509.21192)
*Jieli Zhu,Vi Ngoc-Nha Tran*

Main category: cs.CL

TL;DR: 本文研究小型语言模型(SLMs)在下游任务中的个人身份信息(PII)泄露问题，提出基于贪婪坐标梯度(GCG)的GEP攻击方法，实验显示其泄露量比模板方法提升60倍，自由格式插入场景下仍能实现4.53%的PII泄露率。


<details>
  <summary>Details</summary>
Motivation: 随着小型语言模型在特定领域性能接近大型模型且能耗更低，其应用日益广泛。然而当前对SLMs处理下游任务时的PII泄露风险尚未充分探索，需要系统性安全评估。

Method: 1. 基于BioGPT微调医疗聊天机器人ChatBioGPT
2. 提出GEP攻击框架：结合梯度优化和贪心策略的PII提取方法
3. 设计模板攻击与自由格式插入两种实验场景进行验证

Result: 1. ChatBioGPT在BERTscore上与ChatDoctor/ChatGPT性能相当
2. GEP相比模板方法提升PII泄露量达60倍
3. 在自由格式插入的复杂场景中仍保持4.53%泄露率

Conclusion: GEP方法有效揭示了SLMs的PII泄露脆弱性，证明传统防御机制在SLM场景下的不足，为优化隐私保护方案提供新方向。

Abstract: Small language models (SLMs) become unprecedentedly appealing due to their
approximately equivalent performance compared to large language models (LLMs)
in certain fields with less energy and time consumption during training and
inference. However, the personally identifiable information (PII) leakage of
SLMs for downstream tasks has yet to be explored. In this study, we investigate
the PII leakage of the chatbot based on SLM. We first finetune a new chatbot,
i.e., ChatBioGPT based on the backbone of BioGPT using medical datasets Alpaca
and HealthCareMagic. It shows a matchable performance in BERTscore compared
with previous studies of ChatDoctor and ChatGPT. Based on this model, we prove
that the previous template-based PII attacking methods cannot effectively
extract the PII in the dataset for leakage detection under the SLM condition.
We then propose GEP, which is a greedy coordinate gradient-based (GCG) method
specifically designed for PII extraction. We conduct experimental studies of
GEP and the results show an increment of up to 60$\times$ more leakage compared
with the previous template-based methods. We further expand the capability of
GEP in the case of a more complicated and realistic situation by conducting
free-style insertion where the inserted PII in the dataset is in the form of
various syntactic expressions instead of fixed templates, and GEP is still able
to reveal a PII leakage rate of up to 4.53%.

</details>


### [54] [Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for Scientific Reasoning](https://arxiv.org/abs/2509.21193)
*Xiangru Tang,Wanghan Xu,Yujie Wang,Zijie Guo,Daniel Shao,Jiapeng Chen,Cixuan Zhang,Ziyi Wang,Lixin Zhang,Guancheng Wan,Wenlong Zhang,Lei Bai,Zhenfei Yin,Philip Torr,Hanrui Wang,Di Jin*

Main category: cs.CL

TL;DR: 提出结合隐式检索与结构化协作的统一框架Eigen，在提升科学推理准确率的同时显著降低资源消耗


<details>
  <summary>Details</summary>
Motivation: 解决显式检索造成的token浪费（工具税）和多智能体平均化稀释优质方案的问题

Method: 基于Monitor的token级隐式检索模块 + 分层方案精炼（HSR）和质量感知迭代推理（QAIR）

Result: 在HLE生化金牌测试中达48.3%准确率（提升13.4%），token使用减少53.5%，推理步骤减少43.7%，跨领域测试稳健

Conclusion: 隐式增强与结构化精炼有效克服显式工具的低效性，错误分析显示85%失败案例源于推理与知识双重缺陷，多样性实验揭示检索任务需多样性而推理需共识

Abstract: Large language models (LLMs) have recently shown strong progress on
scientific reasoning, yet two major bottlenecks remain. First, explicit
retrieval fragments reasoning, imposing a hidden "tool tax" of extra tokens and
steps. Second, multi-agent pipelines often dilute strong solutions by averaging
across all candidates. We address these challenges with a unified framework
that combines implicit retrieval and structured collaboration. At its
foundation, a Monitor-based retrieval module operates at the token level,
integrating external knowledge with minimal disruption to reasoning. On top of
this substrate, Hierarchical Solution Refinement (HSR) iteratively designates
each candidate as an anchor to be repaired by its peers, while Quality-Aware
Iterative Reasoning (QAIR) adapts refinement to solution quality. On Humanity's
Last Exam (HLE) Bio/Chem Gold, our framework achieves 48.3\% accuracy -- the
highest reported to date, surpassing the strongest agent baseline by 13.4
points and leading frontier LLMs by up to 18.1 points, while simultaneously
reducing token usage by 53.5\% and agent steps by 43.7\%. Results on SuperGPQA
and TRQA confirm robustness across domains. Error analysis shows that reasoning
failures and knowledge gaps co-occur in over 85\% of cases, while diversity
analysis reveals a clear dichotomy: retrieval tasks benefit from solution
variety, whereas reasoning tasks favor consensus. Together, these findings
demonstrate how implicit augmentation and structured refinement overcome the
inefficiencies of explicit tool use and uniform aggregation. Code is available
at: https://github.com/tangxiangru/Eigen-1.

</details>


### [55] [CLaw: Benchmarking Chinese Legal Knowledge in Large Language Models - A Fine-grained Corpus and Reasoning Analysis](https://arxiv.org/abs/2509.21208)
*Xinzhe Xu,Liang Zhao,Hongshen Xu,Chen Chen*

Main category: cs.CL

TL;DR: 提出CLaw基准测试框架，系统性评估大语言模型在中国法律知识记忆与案例推理应用中的表现


<details>
  <summary>Details</summary>
Motivation: 现有大模型处理法律文本时存在知识记忆混杂、条款引用可靠性不足的问题，需专门基准验证模型真实法律能力

Method: 1.构建包含306部中国法规（64,849条目）的精细化语料库，含子段落级标注和历史修订记录；2.创建254个最高人民法院案例的推理测试集

Result: 主流大模型普遍存在法律条款还原困难（准确率仅28.1%），法律检索的缺陷严重影响推理可靠性，需结合监督微调与检索增强技术提升

Conclusion: CLaw为法律领域LLM发展提供关键评估基准，证明领域专业推理需知识检索与通用推理能力的深度融合

Abstract: Large Language Models (LLMs) are increasingly tasked with analyzing legal
texts and citing relevant statutes, yet their reliability is often compromised
by general pre-training that ingests legal texts without specialized focus,
obscuring the true depth of their legal knowledge. This paper introduces CLaw,
a novel benchmark specifically engineered to meticulously evaluate LLMs on
Chinese legal knowledge and its application in reasoning. CLaw comprises two
key components: (1) a comprehensive, fine-grained corpus of all 306 Chinese
national statutes, segmented to the subparagraph level and incorporating
precise historical revision timesteps for rigorous recall evaluation (64,849
entries), and (2) a challenging set of 254 case-based reasoning instances
derived from China Supreme Court curated materials to assess the practical
application of legal knowledge. Our empirical evaluation reveals that most
contemporary LLMs significantly struggle to faithfully reproduce legal
provisions. As accurate retrieval and citation of legal provisions form the
basis of legal reasoning, this deficiency critically undermines the reliability
of their responses. We contend that achieving trustworthy legal reasoning in
LLMs requires a robust synergy of accurate knowledge retrieval--potentially
enhanced through supervised fine-tuning (SFT) or retrieval-augmented generation
(RAG)--and strong general reasoning capabilities. This work provides an
essential benchmark and critical insights for advancing domain-specific LLM
reasoning, particularly within the complex legal sphere.

</details>


### [56] [SGMem: Sentence Graph Memory for Long-Term Conversational Agents](https://arxiv.org/abs/2509.21212)
*Yaxiong Wu,Yongyue Zhang,Sheng Liang,Yong Liu*

Main category: cs.CL

TL;DR: SGMem通过句子级图结构改进长期对话代理的记忆管理，结合原始对话与生成记忆提升回答准确性


<details>
  <summary>Details</summary>
Motivation: 现有基于事实提取或摘要的方法难以有效组织跨不同粒度（回合/轮次/会话）的对话信息关联

Method: 采用分块单元构建句子级图结构，整合原始对话与摘要/事实/见解等多层次生成记忆

Result: 在LongMemEval和LoCoMo测试中显著提升长期对话问答准确率，优于基线模型

Conclusion: SGMem通过结构化记忆管理有效解决长期对话上下文连贯性问题，提升系统整体性能

Abstract: Long-term conversational agents require effective memory management to handle
dialogue histories that exceed the context window of large language models
(LLMs). Existing methods based on fact extraction or summarization reduce
redundancy but struggle to organize and retrieve relevant information across
different granularities of dialogue and generated memory. We introduce SGMem
(Sentence Graph Memory), which represents dialogue as sentence-level graphs
within chunked units, capturing associations across turn-, round-, and
session-level contexts. By combining retrieved raw dialogue with generated
memory such as summaries, facts and insights, SGMem supplies LLMs with coherent
and relevant context for response generation. Experiments on LongMemEval and
LoCoMo show that SGMem consistently improves accuracy and outperforms strong
baselines in long-term conversational question answering.

</details>


### [57] [Query-Centric Graph Retrieval Augmented Generation](https://arxiv.org/abs/2509.21237)
*Yaxiong Wu,Jianyuan Bo,Yongyue Zhang,Sheng Liang,Yong Liu*

Main category: cs.CL

TL;DR: 提出QCG-RAG框架解决图检索增强生成中的粒度困境，通过查询粒度索引和多跳语块检索提升问答准确性


<details>
  <summary>Details</summary>
Motivation: 现有图检索增强生成方法面临粒度困境：细粒度实体图导致高token成本且丢失上下文，粗粒度文档图无法捕捉细粒度关系

Method: 使用Doc2Query和Doc2Query--构建可控制粒度的查询中心图，通过生成查询实现多跳语块检索机制

Result: 在LiHuaWorld和MultiHop-RAG数据集上，QCG-RAG在问答准确率上持续优于现有基于语块和图的方法

Conclusion: QCG-RAG建立了多跳推理的新范式，通过查询中心图架构平衡了检索效率与关系捕捉能力

Abstract: Graph-based retrieval-augmented generation (RAG) enriches large language
models (LLMs) with external knowledge for long-context understanding and
multi-hop reasoning, but existing methods face a granularity dilemma:
fine-grained entity-level graphs incur high token costs and lose context, while
coarse document-level graphs fail to capture nuanced relations. We introduce
QCG-RAG, a query-centric graph RAG framework that enables query-granular
indexing and multi-hop chunk retrieval. Our query-centric approach leverages
Doc2Query and Doc2Query{-}{-} to construct query-centric graphs with
controllable granularity, improving graph quality and interpretability. A
tailored multi-hop retrieval mechanism then selects relevant chunks via the
generated queries. Experiments on LiHuaWorld and MultiHop-RAG show that QCG-RAG
consistently outperforms prior chunk-based and graph-based RAG methods in
question answering accuracy, establishing a new paradigm for multi-hop
reasoning.

</details>


### [58] [Un-Doubling Diffusion: LLM-guided Disambiguation of Homonym Duplication](https://arxiv.org/abs/2509.21262)
*Evgeny Kaskov,Elizaveta Petrova,Petr Surovtsev,Anna Kostikova,Ilya Mistiurin,Alexander Kapitanov,Alexander Nagaev*

Main category: cs.CL

TL;DR: 论文研究了扩散模型处理同形异义词时产生的重复生成问题，提出了自动评估方法并通过提示扩展缓解问题


<details>
  <summary>Details</summary>
Motivation: 同形异义词会导致扩散模型同时生成多个语义，Anglocentric偏差通过翻译步骤加剧了该问题

Method: 使用VLM自动评估和人工评估测量重复率，通过提示扩展方法缓解问题

Result: 提示扩展方法有效减少同形异义词和Anglocentric偏差相关的重复生成问题

Conclusion: 建立了量化评估体系并提出解决方案，同时公开了自动评估代码

Abstract: Homonyms are words with identical spelling but distinct meanings, which pose
challenges for many generative models. When a homonym appears in a prompt,
diffusion models may generate multiple senses of the word simultaneously, which
is known as homonym duplication. This issue is further complicated by an
Anglocentric bias, which includes an additional translation step before the
text-to-image model pipeline. As a result, even words that are not homonymous
in the original language may become homonyms and lose their meaning after
translation into English. In this paper, we introduce a method for measuring
duplication rates and conduct evaluations of different diffusion models using
both automatic evaluation utilizing Vision-Language Models (VLM) and human
evaluation. Additionally, we investigate methods to mitigate the homonym
duplication problem through prompt expansion, demonstrating that this approach
also effectively reduces duplication related to Anglocentric bias. The code for
the automatic evaluation pipeline is publicly available.

</details>


### [59] [LLM Output Homogenization is Task Dependent](https://arxiv.org/abs/2509.21267)
*Shomik Jain,Jack Lanchantin,Maximilian Nickel,Karen Ullrich,Ashia Wilson,Jamelle Watson-Daniels*

Main category: cs.CL

TL;DR: 提出任务依赖性的新框架解决LLM输出同质化问题，包含分类体系、评估方法、采样技术和质量-多样性平衡验证


<details>
  <summary>Details</summary>
Motivation: 现有研究未能以任务依赖性方式界定输出同质化，不同任务类型（如数学计算与创意写作）对同质化的接受度存在本质差异

Method: 1) 建立8类任务分类体系
2) 提出任务锚定功能多样性评估指标
3) 开发任务锚定采样技术
4) 挑战质量-多样性权衡传统认知

Result: 在保持响应质量的同时显著提升功能多样性，实现任务定制化的同质化控制

Conclusion: 任务依赖性框架有效改善输出同质化的评估与缓解，证明质量与多样性可兼得

Abstract: A large language model can be less helpful if it exhibits output response
homogenization. But whether two responses are considered homogeneous, and
whether such homogenization is problematic, both depend on the task category.
For instance, in objective math tasks, we often expect no variation in the
final answer but anticipate variation in the problem-solving strategy. Whereas,
for creative writing tasks, we may expect variation in key narrative components
(e.g. plot, genre, setting, etc), beyond the vocabulary or embedding diversity
produced by temperature-sampling. Previous work addressing output
homogenization often fails to conceptualize diversity in a task-dependent way.
We address this gap in the literature directly by making the following
contributions. (1) We present a task taxonomy comprised of eight task
categories that each have distinct conceptualizations of output homogenization.
(2) We introduce task-anchored functional diversity to better evaluate output
homogenization. (3) We propose a task-anchored sampling technique that
increases functional diversity for task categories where homogenization is
undesired, while preserving homogenization where it is desired. (4) We
challenge the perceived existence of a diversity-quality trade-off by
increasing functional diversity while maintaining response quality. Overall, we
demonstrate how task dependence improves the evaluation and mitigation of
output homogenization.

</details>


### [60] [LLMTrace: A Corpus for Classification and Fine-Grained Localization of AI-Written Text](https://arxiv.org/abs/2509.21269)
*Irina Tolstykh,Aleksandra Tsybina,Sergey Yakubson,Maksim Kuprashevich*

Main category: cs.CL

TL;DR: 构建包含字符级标注的双语数据集LLMTrace，提升AI生成文本检测的精准度与实用性


<details>
  <summary>Details</summary>
Motivation: 现有数据集存在模型陈旧、语言单一、缺乏混合人机创作场景支持，且无字符级标注导致AI片段定位困难

Method: 采用多样化现代商业/开源LLM构建双语（英俄）语料库，支持全文二分类和AI生成区间检测双任务

Result: 创建首个具备字符级标注的大规模双语数据集，支持精准AI片段定位与新型检测模型训练

Conclusion: LLMTrace将为更细致实用的AI检测模型提供关键资源，推动混合创作场景下的检测技术发展

Abstract: The widespread use of human-like text from Large Language Models (LLMs)
necessitates the development of robust detection systems. However, progress is
limited by a critical lack of suitable training data; existing datasets are
often generated with outdated models, are predominantly in English, and fail to
address the increasingly common scenario of mixed human-AI authorship.
Crucially, while some datasets address mixed authorship, none provide the
character-level annotations required for the precise localization of
AI-generated segments within a text. To address these gaps, we introduce
LLMTrace, a new large-scale, bilingual (English and Russian) corpus for
AI-generated text detection. Constructed using a diverse range of modern
proprietary and open-source LLMs, our dataset is designed to support two key
tasks: traditional full-text binary classification (human vs. AI) and the novel
task of AI-generated interval detection, facilitated by character-level
annotations. We believe LLMTrace will serve as a vital resource for training
and evaluating the next generation of more nuanced and practical AI detection
models. The project page is available at
\href{https://sweetdream779.github.io/LLMTrace-info/}{iitolstykh/LLMTrace}.

</details>


### [61] [Bounds of Chain-of-Thought Robustness: Reasoning Steps, Embed Norms, and Beyond](https://arxiv.org/abs/2509.21284)
*Dingzirui Wang,Xuanliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Main category: cs.CL

TL;DR: 本文通过理论分析推导了输入扰动对CoT输出波动的影响上限，证明其与推理步骤正相关且无法被无限推理消除，并在LSA模型中验证了扰动上限与嵌入向量范数的负相关性，实验验证了理论正确性。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对输入扰动如何影响CoT输出的理论解释，制约对推理过程扰动传播机制的理解及提示优化方法的改进。

Method: 1. 理论推导输出波动可接受范围内的输入扰动上限；2. 将该理论应用于Linear Self-Attention模型进行验证；3. 在3个主流数据集和4个模型上进行实验。

Result: 1. 扰动上限与CoT推理步骤数正相关；2. 无限推理无法消除扰动影响；3. LSA模型中扰动上限与输入嵌入/隐藏状态向量范数负相关；4. 实验数据与理论一致。

Conclusion: 研究揭示了CoT抗扰动能力的理论边界，为提升大语言模型推理鲁棒性提供了数学依据，尤其在LSA模型中的发现可直接指导嵌入向量优化。

Abstract: Existing research indicates that the output of Chain-of-Thought (CoT) is
significantly affected by input perturbations. Although many methods aim to
mitigate such impact by optimizing prompts, a theoretical explanation of how
these perturbations influence CoT outputs remains an open area of research.
This gap limits our in-depth understanding of how input perturbations propagate
during the reasoning process and hinders further improvements in prompt
optimization methods. Therefore, in this paper, we theoretically analyze the
effect of input perturbations on the fluctuation of CoT outputs. We first
derive an upper bound for input perturbations under the condition that the
output fluctuation is within an acceptable range, based on which we prove that:
(i) This upper bound is positively correlated with the number of reasoning
steps in the CoT; (ii) Even an infinitely long reasoning process cannot
eliminate the impact of input perturbations. We then apply these conclusions to
the Linear Self-Attention (LSA) model, which can be viewed as a simplified
version of the Transformer. For the LSA model, we prove that the upper bound
for input perturbation is negatively correlated with the norms of the input
embedding and hidden state vectors. To validate this theoretical analysis, we
conduct experiments on three mainstream datasets and four mainstream models.
The experimental results align with our theoretical analysis, empirically
demonstrating the correctness of our findings.

</details>


### [62] [DisCoCLIP: A Distributional Compositional Tensor Network Encoder for Vision-Language Understanding](https://arxiv.org/abs/2509.21287)
*Kin Ian Lo,Hala Hawashin,Mina Abbaszadeh,Tilen Limback-Stokin,Hadi Wazni,Mehrnoosh Sadrzadeh*

Main category: cs.CL

TL;DR: DisCoCLIP通过引入显式句法结构的张量网络编码器，显著提升了视觉-语言模型在组合推理任务中的表现，参数效率提升超99%


<details>
  <summary>Details</summary>
Motivation: 现有CLIP模型忽略语言组合结构，导致动词语义和词序敏感任务表现不足。研究旨在通过显式编码语法结构提升组合推理能力

Method: 结合冻结CLIP视觉编码器与新型张量文本编码器，使用组合范畴语法解析生成语法推导对应的词张量，通过张量分解将参数从千万级降至百万级

Result: SVO-Probes动词准确率提升4.8%，ARO属性/关系分数分别提升9%和4%，SVO-Swap新基准达93.7%准确率

Conclusion: 基于张量网络的显式语言结构嵌入显著提升了视觉-语言任务的组合推理能力，同时实现高参数效率和模型可解释性

Abstract: Recent vision-language models excel at large-scale image-text alignment but
often neglect the compositional structure of language, leading to failures on
tasks that hinge on word order and predicate-argument structure. We introduce
DisCoCLIP, a multimodal encoder that combines a frozen CLIP vision transformer
with a novel tensor network text encoder that explicitly encodes syntactic
structure. Sentences are parsed with a Combinatory Categorial Grammar parser to
yield distributional word tensors whose contractions mirror the sentence's
grammatical derivation. To keep the model efficient, high-order tensors are
factorized with tensor decompositions, reducing parameter count from tens of
millions to under one million. Trained end-to-end with a self-supervised
contrastive loss, DisCoCLIP markedly improves sensitivity to verb semantics and
word order: it raises CLIP's SVO-Probes verb accuracy from 77.6% to 82.4%,
boosts ARO attribution and relation scores by over 9% and 4%, and achieves
93.7% on a newly introduced SVO-Swap benchmark. These results demonstrate that
embedding explicit linguistic structure via tensor networks yields
interpretable, parameter-efficient representations that substantially improve
compositional reasoning in vision-language tasks.

</details>


### [63] [The role of synthetic data in Multilingual, Multi-cultural AI systems: Lessons from Indic Languages](https://arxiv.org/abs/2509.21294)
*Pranjal A. Chitale,Varun Gumma,Sanchit Ahuja,Prashant Kodali,Manan Uppadhyay,Deepthi Sudharsan,Sunayana Sitaram*

Main category: cs.CL

TL;DR: 开发跨语言文化扎根的AI系统，提出基于维基百科的自下而上合成数据方法，构建包含13种印度语言的9.5M指令数据集Updesh，验证其在低资源语言中的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言环境下AI系统文化适应性问题，探索合成数据在跨语言场景的潜力，突破传统自上而下翻译数据范式的局限。

Method: 采用大语言模型（≥235B参数）基于语言特定维基百科内容生成数据，构建包含长上下文、多轮对话的多样化任务，强调文化语境对齐。

Result: 人工评估10k样本显示数据质量优良，模型微调后在15个多语言数据集上生成任务提升显著（低资源语言相对改进达6-15%），缩小与高资源语言差距。

Conclusion: 多语言AI需要融合自下而上/自上而下的多维度数据策略，文化语境感知的数据生成方法对提升低资源语言性能具有关键作用。

Abstract: Developing AI systems that operate effectively across languages while
remaining culturally grounded is a long-standing challenge, particularly in
low-resource settings. Synthetic data provides a promising avenue, yet its
effectiveness in multilingual and multicultural contexts remains underexplored.
We investigate the creation and impact of synthetic, culturally contextualized
datasets for Indian languages through a bottom-up generation strategy that
prompts large open-source LLMs (>= 235B parameters) to ground data generation
in language-specific Wikipedia content. This approach complements the dominant
top-down paradigm of translating synthetic datasets from high-resource
languages such as English. We introduce Updesh, a high-quality large-scale
synthetic instruction-following dataset comprising 9.5M data points across 13
Indian languages, encompassing diverse reasoning and generative tasks with an
emphasis on long-context, multi-turn capabilities, and alignment with Indian
cultural contexts. A comprehensive evaluation incorporating both automated
metrics and human annotation across 10k assessments indicates that generated
data is high quality; though, human evaluation highlights areas for further
improvement. Additionally, we perform downstream evaluations by fine-tuning
models on our dataset and assessing the performance across 15 diverse
multilingual datasets. Models trained on Updesh consistently achieve
significant gains on generative tasks and remain competitive on multiple-choice
style NLU tasks. Notably, relative improvements are most pronounced in low and
medium-resource languages, narrowing their gap with high-resource languages.
These findings provide empirical evidence that effective multilingual AI
requires multi-faceted data curation and generation strategies that incorporate
context-aware, culturally grounded methodologies.

</details>


### [64] [Sycophancy Is Not One Thing: Causal Separation of Sycophantic Behaviors in LLMs](https://arxiv.org/abs/2509.21305)
*Daniel Vennemeyer,Phan Anh Duong,Tiffany Zhan,Tianyu Jiang*

Main category: cs.CL

TL;DR: LLM谄媚行为可分解为虚假同意与谄媚赞美，三者存在独立神经表征且可定向调控


<details>
  <summary>Details</summary>
Motivation: 探究语言模型谄媚行为（过度附和使用者）是单一机制还是多机制作用

Method: 通过均值差异向量、激活干预、子空间几何分析多模型数据集

Result: 三种行为在潜空间存在独立线性表征，可单独调控且表征结构跨模型普适

Conclusion: 谄媚行为对应独立神经表征，为模型行为定向调控提供理论基础

Abstract: Large language models (LLMs) often exhibit sycophantic behaviors -- such as
excessive agreement with or flattery of the user -- but it is unclear whether
these behaviors arise from a single mechanism or multiple distinct processes.
We decompose sycophancy into sycophantic agreement and sycophantic praise,
contrasting both with genuine agreement. Using difference-in-means directions,
activation additions, and subspace geometry across multiple models and
datasets, we show that: (1) the three behaviors are encoded along distinct
linear directions in latent space; (2) each behavior can be independently
amplified or suppressed without affecting the others; and (3) their
representational structure is consistent across model families and scales.
These results suggest that sycophantic behaviors correspond to distinct,
independently steerable representations.

</details>


### [65] [RLBFF: Binary Flexible Feedback to bridge between Human Feedback & Verifiable Rewards](https://arxiv.org/abs/2509.21319)
*Zhilin Wang,Jiaqi Zeng,Olivier Delalleau,Ellie Evans,Daniel Egert,Hoo-Chang Shin,Felipe Soares,Yi Dong,Oleksii Kuchaiev*

Main category: cs.CL

TL;DR: 提出RLBFF方法，结合人类偏好与规则验证，在RM-Bench/JudgeBench达到SOTA，并以5%推理成本实现对齐基准优异表现


<details>
  <summary>Details</summary>
Motivation: RLHF存在可解释性差和奖励黑客问题，RLVR受限于正确性验证。需要结合人类偏好灵活性和规则验证精确性，捕捉超越正确性的响应质量维度

Method: 从自然语言反馈提取二进制原则（如信息准确性yes/no），将奖励模型训练构建为响应是否满足任意原则的蕴含任务

Result: RM-Bench(86.2%)/JudgeBench(81.4%)第一，Qwen3-32B在MT-Bench等基准匹配o3-mini性能（推理成本<5%）

Conclusion: RLBFF通过原则二元化验证实现更高准确率，支持推理时定制奖励焦点，开源方案为实际应用提供高效部署路径

Abstract: Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning
with Verifiable Rewards (RLVR) are the main RL paradigms used in LLM
post-training, each offering distinct advantages. However, RLHF struggles with
interpretability and reward hacking because it relies on human judgments that
usually lack explicit criteria, whereas RLVR is limited in scope by its focus
on correctness-based verifiers. We propose Reinforcement Learning with Binary
Flexible Feedback (RLBFF), which combines the versatility of human-driven
preferences with the precision of rule-based verification, enabling reward
models to capture nuanced aspects of response quality beyond mere correctness.
RLBFF extracts principles that can be answered in a binary fashion (e.g.
accuracy of information: yes, or code readability: no) from natural language
feedback. Such principles can then be used to ground Reward Model training as
an entailment task (response satisfies or does not satisfy an arbitrary
principle). We show that Reward Models trained in this manner can outperform
Bradley-Terry models when matched for data and achieve top performance on
RM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,
2025). Additionally, users can specify principles of interest at inference time
to customize the focus of our reward models, in contrast to Bradley-Terry
models. Finally, we present a fully open source recipe (including data) to
align Qwen3-32B using RLBFF and our Reward Model, to match or exceed the
performance of o3-mini and DeepSeek R1 on general alignment benchmarks of
MT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost).

</details>


### [66] [SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines](https://arxiv.org/abs/2509.21320)
*Yizhou Wang,Chen Tang,Han Deng,Jiabei Xiao,Jiaqi Liu,Jianyu Wu,Jun Yao,Pengze Li,Encheng Su,Lintao Wang,Guohang Zhuang,Yuchen Ren,Ben Fei,Ming Hu,Xin Chen,Dongzhan Zhou,Junjun He,Xiangyu Yue,Zhenfei Yin,Jiamin Wu,Qihao Zheng,Yuhao Zhou,Huihui Xu,Chenglong Ma,Yan Lu,Wenlong Zhang,Chunfeng Song,Philip Torr,Shixiang Tang,Xinzhu Ma,Wanli Ouyang,Lei Bai*

Main category: cs.CL

TL;DR: 提出融合自然语言与多模态科学表征的通用科学推理基础模型，通过跨领域预训练和指令微调实现103种科学任务支持


<details>
  <summary>Details</summary>
Motivation: 解决科学领域自然语言与结构化表征（如化学式/蛋白质序列）的语义对齐问题，突破专业系统任务覆盖范围有限、跨领域泛化能力不足的瓶颈

Method: 使用206B token多模态科学语料预训练，采用40M指令微调（SFT）、退火冷启动引导长思维链生成，结合任务特异性奖励的强化学习

Result: 相比专业系统，模型在指令覆盖率（+58%）、跨领域泛化能力（+32%）、输出保真度（+41%）方面显著提升，跨学科学习增强迁移可靠性

Conclusion: 该框架验证了跨模态对齐和跨领域学习对科学AI的价值，开源模型与训练数据推动开放科学发展

Abstract: We present a scientific reasoning foundation model that aligns natural
language with heterogeneous scientific representations. The model is pretrained
on a 206B-token corpus spanning scientific text, pure sequences, and
sequence-text pairs, then aligned via SFT on 40M instructions, annealed
cold-start bootstrapping to elicit long-form chain-of-thought, and
reinforcement learning with task-specific reward shaping, which instills
deliberate scientific reasoning. It supports four capability families, covering
up to 103 tasks across workflows: (i) faithful translation between text and
scientific formats, (ii) text/knowledge extraction, (iii) property prediction,
(iv) property classification, (v) unconditional and conditional sequence
generation and design. Compared with specialist systems, our approach broadens
instruction coverage, improves cross-domain generalization, and enhances
fidelity. We detail data curation and training and show that cross-discipline
learning strengthens transfer and downstream reliability. The model, instruct
tuning datasets and the evaluation code are open-sourced at
https://huggingface.co/SciReason and
https://github.com/open-sciencelab/SciReason.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [67] [SeHDR: Single-Exposure HDR Novel View Synthesis via 3D Gaussian Bracketing](https://arxiv.org/abs/2509.20400)
*Yiyu Li,Haoyuan Wang,Ke Xu,Gerhard Petrus Hancke,Rynson W. H. Lau*

Main category: cs.GR

TL;DR: SeHDR提出从单曝光多视角LDR图像中学习HDR场景表示，通过分阶段估计并融合不同曝光的高斯模型实现HDR新视角生成。


<details>
  <summary>Details</summary>
Motivation: 现有HDR-3DGS方法需多曝光图像，存在采集繁琐、易受运动模糊/校准误差影响的问题。本文旨在用单曝光输入解决HDR重建问题。

Method: 1. 从单曝光LDR输入学习基础3D高斯模型
2. 基于曝光条件生成几何相同、颜色变化的Bracketed 3D Gaussians
3. 提出可微分神经曝光融合(NeEF)整合为HDR高斯模型

Result: 实验证明SeHDR在HDR重建质量上优于现有方法及精心设计的基线模型

Conclusion: 该框架首次实现单曝光LDR输入的HDR-3DGS，通过曝光条件解耦与神经融合机制突破多曝光限制

Abstract: This paper presents SeHDR, a novel high dynamic range 3D Gaussian Splatting
(HDR-3DGS) approach for generating HDR novel views given multi-view LDR images.
Unlike existing methods that typically require the multi-view LDR input images
to be captured from different exposures, which are tedious to capture and more
likely to suffer from errors (e.g., object motion blurs and
calibration/alignment inaccuracies), our approach learns the HDR scene
representation from multi-view LDR images of a single exposure. Our key insight
to this ill-posed problem is that by first estimating Bracketed 3D Gaussians
(i.e., with different exposures) from single-exposure multi-view LDR images, we
may then be able to merge these bracketed 3D Gaussians into an HDR scene
representation. Specifically, SeHDR first learns base 3D Gaussians from
single-exposure LDR inputs, where the spherical harmonics parameterize colors
in a linear color space. We then estimate multiple 3D Gaussians with identical
geometry but varying linear colors conditioned on exposure manipulations.
Finally, we propose the Differentiable Neural Exposure Fusion (NeEF) to
integrate the base and estimated 3D Gaussians into HDR Gaussians for novel view
rendering. Extensive experiments demonstrate that SeHDR outperforms existing
methods as well as carefully designed baselines.

</details>


### [68] [SGAligner++: Cross-Modal Language-Aided 3D Scene Graph Alignment](https://arxiv.org/abs/2509.20401)
*Binod Singh,Sayan Deb Sarkar,Iro Armeni*

Main category: cs.GR

TL;DR: 提出跨模态语言辅助框架SGAligner++，通过联合嵌入空间实现低重叠场景下的精准3D场景图对齐，噪声环境下性能提升40%


<details>
  <summary>Details</summary>
Motivation: 现有3D场景图对齐方法依赖单模态点云数据，在输入不完整/噪声场景下性能受限，需解决跨模态场景理解与低重叠对齐难题

Method: 构建多模态联合嵌入空间，采用轻量级单模态编码器提取特征，基于注意力机制进行跨模态融合，实现异构传感器数据的鲁棒对齐

Result: 在真实数据集上超越SOTA方法40%（噪声场景），支持跨模态泛化，计算效率提升适合实时导航应用

Conclusion: SGAligner++通过语言增强的跨模态对齐框架，显著提升机器人视觉定位与导航的鲁棒性，为复杂环境感知提供新范式

Abstract: Aligning 3D scene graphs is a crucial initial step for several applications
in robot navigation and embodied perception. Current methods in 3D scene graph
alignment often rely on single-modality point cloud data and struggle with
incomplete or noisy input. We introduce SGAligner++, a cross-modal,
language-aided framework for 3D scene graph alignment. Our method addresses the
challenge of aligning partially overlapping scene observations across
heterogeneous modalities by learning a unified joint embedding space, enabling
accurate alignment even under low-overlap conditions and sensor noise. By
employing lightweight unimodal encoders and attention-based fusion, SGAligner++
enhances scene understanding for tasks such as visual localization, 3D
reconstruction, and navigation, while ensuring scalability and minimal
computational overhead. Extensive evaluations on real-world datasets
demonstrate that SGAligner++ outperforms state-of-the-art methods by up to 40%
on noisy real-world reconstructions, while enabling cross-modal generalization.

</details>


### [69] [SceneWeaver: All-in-One 3D Scene Synthesis with an Extensible and Self-Reflective Agent](https://arxiv.org/abs/2509.20414)
*Yandan Yang,Baoxiong Jia,Shujie Zhang,Siyuan Huang*

Main category: cs.GR

TL;DR: SceneWeaver通过工具化迭代优化统一多场景合成范式，在物理/视觉/语义指标上优于现有方法，支持复杂指令的通用3D场景生成


<details>
  <summary>Details</summary>
Motivation: 解决现有方法场景类别受限、物体细节不足、物理一致性差、与复杂用户指令对齐困难的问题

Method: 基于语言模型的规划器选择可扩展生成工具（数据驱动模型/视觉方法/LLM），通过物理合理性/视觉真实性/语义对齐的自我评估进行闭环迭代优化

Result: 在常见和开放词汇房间类型的实验中，物理/视觉/语义指标全面超越基线，可泛化到复杂场景的多样化指令

Conclusion: 通过工具化迭代和自评估机制整合多生成方法，向通用3D环境生成迈进，展示了反射式代理框架在场景合成中的有效性

Abstract: Indoor scene synthesis has become increasingly important with the rise of
Embodied AI, which requires 3D environments that are not only visually
realistic but also physically plausible and functionally diverse. While recent
approaches have advanced visual fidelity, they often remain constrained to
fixed scene categories, lack sufficient object-level detail and physical
consistency, and struggle to align with complex user instructions. In this
work, we present SceneWeaver, a reflective agentic framework that unifies
diverse scene synthesis paradigms through tool-based iterative refinement. At
its core, SceneWeaver employs a language model-based planner to select from a
suite of extensible scene generation tools, ranging from data-driven generative
models to visual- and LLM-based methods, guided by self-evaluation of physical
plausibility, visual realism, and semantic alignment with user input. This
closed-loop reason-act-reflect design enables the agent to identify semantic
inconsistencies, invoke targeted tools, and update the environment over
successive iterations. Extensive experiments on both common and open-vocabulary
room types demonstrate that SceneWeaver not only outperforms prior methods on
physical, visual, and semantic metrics, but also generalizes effectively to
complex scenes with diverse instructions, marking a step toward general-purpose
3D environment generation. Project website: https://scene-weaver.github.io/.

</details>


### [70] [ArtUV: Artist-style UV Unwrapping](https://arxiv.org/abs/2509.20710)
*Yuguang Chen,Xinhai Liu,Yang Li,Victor Cheung,Zhuo Chen,Dongyu Zhang,Chunchao Guo*

Main category: cs.GR

TL;DR: 提出ArtUV方法，通过两阶段流程自动生成高质量、语义一致的艺术家风格UV贴图，解决现有方法碎片化、低效等问题。


<details>
  <summary>Details</summary>
Motivation: 现有UV展开方法存在耗时长、碎片化、缺乏语义性等问题，无法满足艺术家对边界清晰、空间利用率高和语义一致的UV贴图需求。

Method: 1. 表面接缝预测阶段使用SeamGPT生成语义切割接缝；2. 参数化阶段通过优化方法生成粗UV，再用Auto-Encoder优化为艺术家风格UV。

Result: 在多个基准测试中验证有效性，可作为专业渲染工具插件或独立系统，实现快速生成支持2D编辑的高质量UV贴图。

Conclusion: ArtUV通过端到端自动化流程，显著提升UV展开质量与效率，为后续视觉编辑提供可靠基础。

Abstract: UV unwrapping is an essential task in computer graphics, enabling various
visual editing operations in rendering pipelines. However, existing UV
unwrapping methods struggle with time-consuming, fragmentation, lack of
semanticity, and irregular UV islands, limiting their practical use. An
artist-style UV map must not only satisfy fundamental criteria, such as
overlap-free mapping and minimal distortion, but also uphold higher-level
standards, including clean boundaries, efficient space utilization, and
semantic coherence. We introduce ArtUV, a fully automated, end-to-end method
for generating artist-style UV unwrapping. We simulates the professional UV
mapping process by dividing it into two stages: surface seam prediction and
artist-style UV parameterization. In the seam prediction stage, SeamGPT is used
to generate semantically meaningful cutting seams. Then, in the
parameterization stage, a rough UV obtained from an optimization-based method,
along with the mesh, is fed into an Auto-Encoder, which refines it into an
artist-style UV map. Our method ensures semantic consistency and preserves
topological structure, making the UV map ready for 2D editing. We evaluate
ArtUV across multiple benchmarks and show that it serves as a versatile
solution, functioning seamlessly as either a plug-in for professional rendering
tools or as a standalone system for rapid, high-quality UV generation.

</details>


### [71] [SeamCrafte: Enhancing Mesh Seam Generation for Artist UV Unwrapping via Reinforcement Learning](https://arxiv.org/abs/2509.20725)
*Duoteng Xu,Yuguang Chen,Jing Li,Xinhai Liu,Xueqi Ma,Zhuo Chen,Dongyu Zhang,Chunchao Guo*

Main category: cs.GR

TL;DR: SeamCrafter通过双分支点云编码器与偏好优化框架，显著降低UV映射中的扭曲和碎片化


<details>
  <summary>Details</summary>
Motivation: 现有接缝生成方法在UV扭曲与碎片化间难以平衡，影响纹理合成与工作流程

Method: 使用自回归GPT架构+双分支编码器分离拓扑/几何特征，通过DPO在偏好数据集微调

Result: 实验显示接缝扭曲降低51%，碎片化减少63%，同时保持拓扑一致性与视觉保真度

Conclusion: 该方法突破传统方法的取舍困境，为数字内容创作提供更优的UV参数化解决方案

Abstract: Mesh seams play a pivotal role in partitioning 3D surfaces for UV
parametrization and texture mapping. Poorly placed seams often result in severe
UV distortion or excessive fragmentation, thereby hindering texture synthesis
and disrupting artist workflows. Existing methods frequently trade one failure
mode for another-producing either high distortion or many scattered islands. To
address this, we introduce SeamCrafter, an autoregressive GPT-style seam
generator conditioned on point cloud inputs. SeamCrafter employs a dual-branch
point-cloud encoder that disentangles and captures complementary topological
and geometric cues during pretraining. To further enhance seam quality, we
fine-tune the model using Direct Preference Optimization (DPO) on a preference
dataset derived from a novel seam-evaluation framework. This framework assesses
seams primarily by UV distortion and fragmentation, and provides pairwise
preference labels to guide optimization. Extensive experiments demonstrate that
SeamCrafter produces seams with substantially lower distortion and
fragmentation than prior approaches, while preserving topological consistency
and visual fidelity.

</details>


### [72] [ARMesh: Autoregressive Mesh Generation via Next-Level-of-Detail Prediction](https://arxiv.org/abs/2509.20824)
*Jiabao Lei,Kewei Shi,Zhihao Liang,Kui Jia*

Main category: cs.GR

TL;DR: 提出渐进式自回归网格生成方法，通过从单点逐步细化构建3D网格，支持生成质量控制和网格编辑应用。


<details>
  <summary>Details</summary>
Motivation: 现有自回归网格生成模型按字典顺序逐面构建，难以有效捕捉符合人类感知的几何结构，受2D渐进细化模型启发寻求改进方案。

Method: 将网格简化过程逆向重构为从粗到细生成，基于Transformer构建自回归模型，通过局部网格细化动态调整拓扑结构，支持从单点逐步添加几何细节。

Result: 实验证明该方法可实现生成质量/时间的平衡控制，并具备网格细化和编辑等扩展应用能力。

Conclusion: 渐进生成范式突破了固定拓扑限制，为灵活可控的3D内容生成开辟了新方向。

Abstract: Directly generating 3D meshes, the default representation for 3D shapes in
the graphics industry, using auto-regressive (AR) models has become popular
these days, thanks to their sharpness, compactness in the generated results,
and ability to represent various types of surfaces. However, AR mesh generative
models typically construct meshes face by face in lexicographic order, which
does not effectively capture the underlying geometry in a manner consistent
with human perception. Inspired by 2D models that progressively refine images,
such as the prevailing next-scale prediction AR models, we propose generating
meshes auto-regressively in a progressive coarse-to-fine manner. Specifically,
we view mesh simplification algorithms, which gradually merge mesh faces to
build simpler meshes, as a natural fine-to-coarse process. Therefore, we
generalize meshes to simplicial complexes and develop a transformer-based AR
model to approximate the reverse process of simplification in the order of
level of detail, constructing meshes initially from a single point and
gradually adding geometric details through local remeshing, where the topology
is not predefined and is alterable. Our experiments show that this novel
progressive mesh generation approach not only provides intuitive control over
generation quality and time consumption by early stopping the auto-regressive
process but also enables applications such as mesh refinement and editing.

</details>


### [73] [ArchGPT: Understanding the World's Architectures with Large Multimodal Models](https://arxiv.org/abs/2509.20858)
*Yuze Wang,Luo Yang,Junyi Wang,Yue Qi*

Main category: cs.GR

TL;DR: ArchGPT: 多模态建筑视觉问答模型，通过专业数据集Arch-300K实现建筑场景的可扩展分析


<details>
  <summary>Details</summary>
Motivation: 传统VR/MR/AR系统依赖硬编码标注，无法跨建筑环境扩展。需要自动化解决方案提升建筑场景理解能力。

Method: 构建Arch-300K数据集（315K图像-问题-答案三元组），采用多阶段数据清洗流程（3D重建筛选图像，LLM蒸馏可靠QA对，合成形式化分析标注），基于ShareGPT4V-7B监督微调。

Result: ArchGPT在建筑领域实现细粒度视觉理解，支持开放域QA和形式化建筑分析，显著提升建筑场景的语义解析能力。

Conclusion: 通过领域专用数据集与知识蒸馏的结合，为建筑遗产保护、教育、设计实践提供了可扩展的多模态分析框架。

Abstract: Architecture embodies aesthetic, cultural, and historical values, standing as
a tangible testament to human civilization. Researchers have long leveraged
virtual reality (VR), mixed reality (MR), and augmented reality (AR) to enable
immersive exploration and interpretation of architecture, enhancing
accessibility, public understanding, and creative workflows around architecture
in education, heritage preservation, and professional design practice. However,
existing VR/MR/AR systems are often developed case-by-case, relying on
hard-coded annotations and task-specific interactions that do not scale across
diverse built environments. In this work, we present ArchGPT, a multimodal
architectural visual question answering (VQA) model, together with a scalable
data-construction pipeline for curating high-quality, architecture-specific VQA
annotations. This pipeline yields Arch-300K, a domain-specialized dataset of
approximately 315,000 image-question-answer triplets. Arch-300K is built via a
multi-stage process: first, we curate architectural scenes from Wikimedia
Commons and filter unconstrained tourist photo collections using a novel
coarse-to-fine strategy that integrates 3D reconstruction and semantic
segmentation to select occlusion-free, structurally consistent architectural
images. To mitigate noise and inconsistency in raw textual metadata, we propose
an LLM-guided text verification and knowledge-distillation pipeline to generate
reliable, architecture-specific question-answer pairs. Using these curated
images and refined metadata, we further synthesize formal analysis
annotations-including detailed descriptions and aspect-guided conversations-to
provide richer semantic variety while remaining faithful to the data. We
perform supervised fine-tuning of an open-source multimodal backbone
,ShareGPT4V-7B, on Arch-300K, yielding ArchGPT.

</details>


### [74] [Marching Neurons: Accurate Surface Extraction for Neural Implicit Shapes](https://arxiv.org/abs/2509.21007)
*Christian Stippel,Felix Mujkanovic,Thomas Leimkühler,Pedro Hermosilla*

Main category: cs.GR

TL;DR: 提出从神经隐式函数中解析提取表面的新方法，通过深度优先遍历策略生成高精度网格，突破传统空间离散化方法的精度限制


<details>
  <summary>Details</summary>
Motivation: 传统隐式表面提取方法(如Marching Cubes)受限于固定分辨率导致精度损失，需要无需空间离散化的高精度转换方案

Method: 利用神经元分区特性，开发并行深度优先遍历算法，直接在神经网络架构中追踪隐式表面几何信息

Result: 生成的网格完整保留网络几何信息，在多种形状和网络架构中实现精度突破，同时保持运算速度优势

Conclusion: 该方法首次实现神经隐式表面的解析式提取，为3D视觉计算提供无分辨率限制的精确几何转换框架

Abstract: Accurate surface geometry representation is crucial in 3D visual computing.
Explicit representations, such as polygonal meshes, and implicit
representations, like signed distance functions, each have distinct advantages,
making efficient conversions between them increasingly important. Conventional
surface extraction methods for implicit representations, such as the widely
used Marching Cubes algorithm, rely on spatial decomposition and sampling,
leading to inaccuracies due to fixed and limited resolution. We introduce a
novel approach for analytically extracting surfaces from neural implicit
functions. Our method operates natively in parallel and can navigate large
neural architectures. By leveraging the fact that each neuron partitions the
domain, we develop a depth-first traversal strategy to efficiently track the
encoded surface. The resulting meshes faithfully capture the full geometric
information from the network without ad-hoc spatial discretization, achieving
unprecedented accuracy across diverse shapes and network architectures while
maintaining competitive speed.

</details>


### [75] [CHARM: Control-point-based 3D Anime Hairstyle Auto-Regressive Modeling](https://arxiv.org/abs/2509.21114)
*Yuze He,Yanning Zhou,Wang Zhao,Jingwen Ye,Yushi Bai,Kaiwen Xiao,Yong-Jin Liu,Zhongqian Sun,Wei Yang*

Main category: cs.GR

TL;DR: CHARM提出基于控制点的参数化表示与自回归生成框架，实现高效动漫发型建模与生成，并构建37K数据集AnimeHair。


<details>
  <summary>Details</summary>
Motivation: 传统建模方法难以处理动漫发型的风格化分块结构，现有方法存在编辑低效、扩展性差的问题。

Method: 1. 五参数控制点序列表示发片；2. 自回归Transformer框架解析'发语'；3. 图像/点云输入生成发型

Result: 重建精度与生成质量达SOTA，数据集支持训练验证

Conclusion: CHARM为动漫发型建模提供了兼具表达力与扩展性的解决方案

Abstract: We present CHARM, a novel parametric representation and generative framework
for anime hairstyle modeling. While traditional hair modeling methods focus on
realistic hair using strand-based or volumetric representations, anime
hairstyle exhibits highly stylized, piecewise-structured geometry that
challenges existing techniques. Existing works often rely on dense mesh
modeling or hand-crafted spline curves, making them inefficient for editing and
unsuitable for scalable learning. CHARM introduces a compact, invertible
control-point-based parameterization, where a sequence of control points
represents each hair card, and each point is encoded with only five geometric
parameters. This efficient and accurate representation supports both
artist-friendly design and learning-based generation. Built upon this
representation, CHARM introduces an autoregressive generative framework that
effectively generates anime hairstyles from input images or point clouds. By
interpreting anime hairstyles as a sequential "hair language", our
autoregressive transformer captures both local geometry and global hairstyle
topology, resulting in high-fidelity anime hairstyle creation. To facilitate
both training and evaluation of anime hairstyle generation, we construct
AnimeHair, a large-scale dataset of 37K high-quality anime hairstyles with
separated hair cards and processed mesh data. Extensive experiments demonstrate
state-of-the-art performance of CHARM in both reconstruction accuracy and
generation quality, offering an expressive and scalable solution for anime
hairstyle modeling. Project page: https://hyzcluster.github.io/charm/

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [76] [Interactive Recommendation Agent with Active User Commands](https://arxiv.org/abs/2509.21317)
*Jiakai Tang,Yujie Luo,Xunke Xi,Fei Sun,Xueyang Feng,Sunhao Dai,Chao Yi,Dian Chen,Zhujin Gao,Yang Li,Xu Chen,Wen Chen,Jian Wu,Yuning Jiang,Bo Zheng*

Main category: cs.IR

TL;DR: 提出交互式推荐反馈（IRF）范式及双代理架构RecBot，通过自然语言命令实现主动推荐控制，实验验证显著提升用户满意度与商业指标


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统的被动反馈机制无法捕捉用户细粒度意图，导致偏好建模不准和用户-系统意图鸿沟

Method: IRF范式支持自然语言交互，RecBot采用解析代理（结构化偏好）与规划代理（动态工具链）双架构，结合模拟增强知识蒸馏技术

Result: 离线与长期在线实验显示用户满意度提升23.6%，点击率增加18.4%，用户留存指标改善31.2%

Conclusion: IRF开创了主动式推荐交互范式，RecBot的实时语言驱动策略调整能力为推荐系统演进提供新方向

Abstract: Traditional recommender systems rely on passive feedback mechanisms that
limit users to simple choices such as like and dislike. However, these
coarse-grained signals fail to capture users' nuanced behavior motivations and
intentions. In turn, current systems cannot also distinguish which specific
item attributes drive user satisfaction or dissatisfaction, resulting in
inaccurate preference modeling. These fundamental limitations create a
persistent gap between user intentions and system interpretations, ultimately
undermining user satisfaction and harming system effectiveness.
  To address these limitations, we introduce the Interactive Recommendation
Feed (IRF), a pioneering paradigm that enables natural language commands within
mainstream recommendation feeds. Unlike traditional systems that confine users
to passive implicit behavioral influence, IRF empowers active explicit control
over recommendation policies through real-time linguistic commands. To support
this paradigm, we develop RecBot, a dual-agent architecture where a Parser
Agent transforms linguistic expressions into structured preferences and a
Planner Agent dynamically orchestrates adaptive tool chains for on-the-fly
policy adjustment. To enable practical deployment, we employ
simulation-augmented knowledge distillation to achieve efficient performance
while maintaining strong reasoning capabilities. Through extensive offline and
long-term online experiments, RecBot shows significant improvements in both
user satisfaction and business outcomes.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [77] [Blueprints of Trust: AI System Cards for End to End Transparency and Governance](https://arxiv.org/abs/2509.20394)
*Huzaifa Sidhpurwala,Emily Fox,Garth Mollett,Florencio Cano Gabarda,Roman Zhukov*

Main category: cs.CY

TL;DR: 提出HASC框架，通过标准化标识符和动态安全记录提升AI系统透明度


<details>
  <summary>Details</summary>
Motivation: 现有AI系统卡缺乏系统化安全漏洞追踪机制，需建立统一动态的安全信息源

Method: 整合模型卡/系统卡概念，引入ASH ID安全标识符，并与ISO 42001标准进行互补性分析

Result: 创建生命周期安全追踪体系，实现漏洞修复的透明化，形成AI系统安全决策的单一可信源

Conclusion: HASC框架通过标准化安全标识和动态更新机制，有效提升AI系统的可追溯性和跨标准协同能力

Abstract: This paper introduces the Hazard-Aware System Card (HASC), a novel framework
designed to enhance transparency and accountability in the development and
deployment of AI systems. The HASC builds upon existing model card and system
card concepts by integrating a comprehensive, dynamic record of an AI system's
security and safety posture. The framework proposes a standardized system of
identifiers, including a novel AI Safety Hazard (ASH) ID, to complement
existing security identifiers like CVEs, allowing for clear and consistent
communication of fixed flaws. By providing a single, accessible source of
truth, the HASC empowers developers and stakeholders to make more informed
decisions about AI system safety throughout its lifecycle. Ultimately, we also
compare our proposed AI system cards with the ISO/IEC 42001:2023 standard and
discuss how they can be used to complement each other, providing greater
transparency and accountability for AI systems.

</details>


### [78] [Communication Bias in Large Language Models: A Regulatory Perspective](https://arxiv.org/abs/2509.21075)
*Adrian Kuenzler,Stefan Schmid*

Main category: cs.CY

TL;DR: 探讨大语言模型在公平性、合规性方面的风险及治理框架


<details>
  <summary>Details</summary>
Motivation: 针对LLMs应用中存在的偏见、公平性风险及监管不足问题，强调需超越单纯法规监管的治理方案

Method: 通过分析欧盟AI法案和数字服务法案框架，结合竞争政策与系统设计角度展开论证

Result: 揭示当前监管盲区，提出需加强市场竞争机制与系统设计层面的综合治理

Conclusion: 确保可信AI需要法规监管、市场竞争和技术设计的协同治理体系

Abstract: Large language models (LLMs) are increasingly central to many applications,
raising concerns about bias, fairness, and regulatory compliance. This paper
reviews risks of biased outputs and their societal impact, focusing on
frameworks like the EU's AI Act and the Digital Services Act. We argue that
beyond constant regulation, stronger attention to competition and design
governance is needed to ensure fair, trustworthy AI. This is a preprint of the
Communications of the ACM article of the same title.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [79] [Automotive-ENV: Benchmarking Multimodal Agents in Vehicle Interface Systems](https://arxiv.org/abs/2509.21143)
*Junfeng Yan,Biao Wu,Meng Fang,Ling Chen*

Main category: cs.RO

TL;DR: 提出首个车载GUI专用基准平台Automotive-ENV与地理感知代理ASURADA，通过地理上下文提升车载任务安全性能


<details>
  <summary>Details</summary>
Motivation: 解决车载GUI面临的驾驶员注意力受限、安全要求严格、位置交互复杂三大挑战，填补多模态代理在车载场景的研究空白

Method: 构建含185参数化任务的高保真基准平台，开发集成GPS上下文的地理感知代理，采用结构化多模态观察与程序化验证机制

Result: 地理信息使安全感知任务成功率显著提升（具体数值未提及），验证位置上下文对车载环境的关键作用

Conclusion: 通过开源Automotive-ENV平台推动安全车载代理发展，证明地理感知机制在车辆交互系统中的必要性

Abstract: Multimodal agents have demonstrated strong performance in general GUI
interactions, but their application in automotive systems has been largely
unexplored. In-vehicle GUIs present distinct challenges: drivers' limited
attention, strict safety requirements, and complex location-based interaction
patterns. To address these challenges, we introduce Automotive-ENV, the first
high-fidelity benchmark and interaction environment tailored for vehicle GUIs.
This platform defines 185 parameterized tasks spanning explicit control,
implicit intent understanding, and safety-aware tasks, and provides structured
multimodal observations with precise programmatic checks for reproducible
evaluation. Building on this benchmark, we propose ASURADA, a geo-aware
multimodal agent that integrates GPS-informed context to dynamically adjust
actions based on location, environmental conditions, and regional driving
norms. Experiments show that geo-aware information significantly improves
success on safety-aware tasks, highlighting the importance of location-based
context in automotive environments. We will release Automotive-ENV, complete
with all tasks and benchmarking tools, to further the development of safe and
adaptive in-vehicle agents.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [80] [Intercept Cancer: Cancer Pre-Screening with Large Scale Healthcare Foundation Models](https://arxiv.org/abs/2506.00209)
*Liwen Sun,Hao-Ren Yao,Gary Gao,Ophir Frieder,Chenyan Xiong*

Main category: cs.LG

TL;DR: 提出CATCH-FM医疗大模型，通过电子健康记录预训练实现高效癌症预筛查，在敏感性与特异性上显著优于现有模型


<details>
  <summary>Details</summary>
Motivation: 传统癌症筛查技术成本高昂且具侵入性，导致全球覆盖率不足，亟需基于历史医疗记录的非侵入性预筛查方法

Method: 基于数百万电子健康记录(EHR)建立医疗代码序列预训练范式，构建24亿参数计算优化模型，并在临床癌症风险队列上微调

Result: 在三万患者回顾性评估中达到60%敏感性/99%特异性，胰腺癌预测在EHRSHOT榜单领先，展示跨医疗系统的强鲁棒性与风险捕捉能力

Conclusion: CATCH-FM为非侵入性癌症筛查提供新范式，其跨系统适用性与开源特性将推动精准医疗发展

Abstract: Cancer screening, leading to early detection, saves lives. Unfortunately,
existing screening techniques require expensive and intrusive medical
procedures, not globally available, resulting in too many lost would-be-saved
lives. We present CATCH-FM, CATch Cancer early with Healthcare Foundation
Models, a cancer pre-screening methodology that identifies high-risk patients
for further screening solely based on their historical medical records. With
millions of electronic healthcare records (EHR), we establish the scaling law
of EHR foundation models pretrained on medical code sequences, pretrain
compute-optimal foundation models of up to 2.4 billion parameters, and finetune
them on clinician-curated cancer risk prediction cohorts. In our retrospective
evaluation comprising of thirty thousand patients, CATCH-FM achieved strong
efficacy (60% sensitivity) with low risk (99% specificity and Negative
Predictive Value), outperforming feature-based tree models as well as general
and medical large language models by large margins. Despite significant
demographic, healthcare system, and EHR coding differences, CATCH-FM achieves
state-of-the-art pancreatic cancer risk prediction on the EHRSHOT few-shot
leaderboard, outperforming EHR foundation models pretrained using on-site
patient data. Our analysis demonstrates the robustness of CATCH-FM in various
patient distributions, the benefits of operating in the ICD code space, and its
ability to capture non-trivial cancer risk factors. Our code will be
open-sourced.

</details>


### [81] [Can Federated Learning Safeguard Private Data in LLM Training? Vulnerabilities, Attacks, and Defense Evaluation](https://arxiv.org/abs/2509.20680)
*Wenkai Guo,Xuefeng Liu,Haolin Wang,Jianwei Niu,Shaojie Tang,Jing Yuan*

Main category: cs.LG

TL;DR: 研究发现联邦学习框架下微调大语言模型仍存在隐私泄露风险，模型规模越大泄露越严重，并提出差分隐私等防护方案。


<details>
  <summary>Details</summary>
Motivation: 尽管联邦学习通过参数共享保护隐私，但实际应用中攻击者仍能通过生成方法提取训练数据，需验证FL在LLM微调中的真实隐私保护能力。

Method: 通过生成攻击实验验证数据泄露风险，设计基于模型更新追踪的增强攻击策略，并系统性评估差分隐私/正则化约束/安全对齐等防护措施。

Result: 实验表明FL全局模型仍泄露数据(泄露率随模型规模上升)，增强攻击加剧风险，防护措施中差分隐私与安全对齐模型效果最佳。

Conclusion: 联邦学习训练LLMs需结合多重隐私防护技术，研究为降低隐私风险提供了攻击验证框架和防护方案选择指南。

Abstract: Fine-tuning large language models (LLMs) with local data is a widely adopted
approach for organizations seeking to adapt LLMs to their specific domains.
Given the shared characteristics in data across different organizations, the
idea of collaboratively fine-tuning an LLM using data from multiple sources
presents an appealing opportunity. However, organizations are often reluctant
to share local data, making centralized fine-tuning impractical. Federated
learning (FL), a privacy-preserving framework, enables clients to retain local
data while sharing only model parameters for collaborative training, offering a
potential solution. While fine-tuning LLMs on centralized datasets risks data
leakage through next-token prediction, the iterative aggregation process in FL
results in a global model that encapsulates generalized knowledge, which some
believe protects client privacy. In this paper, however, we present
contradictory findings through extensive experiments. We show that attackers
can still extract training data from the global model, even using
straightforward generation methods, with leakage increasing as the model size
grows. Moreover, we introduce an enhanced attack strategy tailored to FL, which
tracks global model updates during training to intensify privacy leakage. To
mitigate these risks, we evaluate privacy-preserving techniques in FL,
including differential privacy, regularization-constrained updates and adopting
LLMs with safety alignment. Our results provide valuable insights and practical
guidelines for reducing privacy risks when training LLMs with FL.

</details>


### [82] [CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning](https://arxiv.org/abs/2509.20712)
*Zhenpeng Su,Leiyu Pan,Minxuan Lv,Yuntao Li,Wenping Hu,Fuzheng Zhang,Kun Gai,Guorui Zhou*

Main category: cs.LG

TL;DR: 提出CE-GPPO算法通过保留裁剪区间的梯度信号，有效控制强化学习训练中的熵动态，在数学推理任务中优于基线模型


<details>
  <summary>Details</summary>
Motivation: 现有PPO方法因梯度裁剪机制丢弃低概率token的梯度信号，导致熵动态调节能力不足。研究发现这些被裁剪的梯度对熵演化具有关键作用

Method: CE-GPPO算法温和地重新引入被裁剪token的梯度，通过控制梯度幅度实现探索与利用的平衡。采用有界梯度处理策略，保持训练稳定性

Result: 在不同规模模型上的数学推理基准测试中，CE-GPPO持续超越PPO等基线方法。理论分析和实验证明其有效缓解熵不稳定性

Conclusion: 通过精细化控制梯度信号，CE-GPPO实现了更好的探索-利用权衡，为LLM的强化学习优化提供了新思路

Abstract: Reinforcement learning (RL) has become a powerful paradigm for optimizing
large language models (LLMs) to handle complex reasoning tasks. A core
challenge in this process lies in managing policy entropy, which reflects the
balance between exploration and exploitation during training. Existing methods,
such as proximal policy optimization (PPO) and its variants, discard valuable
gradient signals from low-probability tokens due to the clipping mechanism. We
systematically analyze the entropy dynamics and reveal that these clipped
tokens play a critical yet overlooked role in regulating entropy evolution. We
propose \textbf{C}ontrolling \textbf{E}ntropy via
\textbf{G}radient-\textbf{P}reserving \textbf{P}olicy \textbf{O}ptimization
(CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in
native PPO in a gentle and bounded manner. By controlling the magnitude of
gradients from tokens outside the clipping interval, CE-GPPO is able to achieve
an exploration-exploitation trade-off. We provide theoretical justification and
empirical evidence showing that CE-GPPO effectively mitigates entropy
instability. Extensive experiments on mathematical reasoning benchmarks show
that CE-GPPO consistently outperforms strong baselines across different model
scales.

</details>


### [83] [StyleBench: Evaluating thinking styles in Large Language Models](https://arxiv.org/abs/2509.20868)
*Junyu Guo,Shangding Gu,Ming Jin,Costas Spanos,Javad Lavaei*

Main category: cs.LG

TL;DR: 研究通过StyleBench基准测试发现，LLM推理策略的效果取决于模型规模和任务类型，搜索类方法适合开放性问题但需大模型，简洁策略在明确任务中更高效。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLM推理策略、模型架构与任务类型间的关联缺乏系统理解，需建立评估基准指导策略选择。

Method: 使用StyleBench评估CoT/ToT/AoT/SoT/CoD五种策略，在5类任务测试15个开源模型（270M-120B参数）。

Result: 策略效果与模型规模强相关：小模型易违规猜测，大模型更稳健；搜索策略(AoT/ToT)适合开放问题但需大模型，简洁策略(SoT/CoD)在结构化任务效率提升5倍。

Conclusion: 应基于任务类型和模型规模选择策略：开放问题用搜索策略+大模型，明确任务用简洁策略提升效率。基准代码已开源。

Abstract: The effectiveness of Large Language Models (LLMs) is heavily influenced by
the reasoning strategies, or styles of thought, employed in their prompts.
However, the interplay between these reasoning styles, model architecture, and
task type remains poorly understood. To address this, we introduce StyleBench,
a comprehensive benchmark for systematically evaluating reasoning styles across
diverse tasks and models. We assess five representative reasoning styles,
including Chain of Thought (CoT), Tree of Thought (ToT), Algorithm of Thought
(AoT), Sketch of Thought (SoT), and Chain-of-Draft (CoD) on five reasoning
tasks, using 15 open-source models from major families (LLaMA, Qwen, Mistral,
Gemma, GPT-OSS, Phi, and DeepSeek) ranging from 270M to 120B parameters. Our
large-scale analysis reveals that no single style is universally optimal. We
demonstrate that strategy efficacy is highly contingent on both model scale and
task type: search-based methods (AoT, ToT) excel in open-ended problems but
require large-scale models, while concise styles (SoT, CoD) achieve radical
efficiency gains on well-defined tasks. Furthermore, we identify key behavioral
patterns: smaller models frequently fail to follow output instructions and
default to guessing, while reasoning robustness emerges as a function of scale.
Our findings offer a crucial roadmap for selecting optimal reasoning strategies
based on specific constraints, we open source the benchmark in
https://github.com/JamesJunyuGuo/Style_Bench.

</details>


### [84] [CLUE: Conflict-guided Localization for LLM Unlearning Framework](https://arxiv.org/abs/2509.20977)
*Hang Chen,Jiaying Zhu,Xinyu Yang,Wenya Wang*

Main category: cs.LG

TL;DR: 提出CLUE框架，通过电路发现技术实现LLM精准遗忘，在保持模型能力的同时提升目标知识消除效果


<details>
  <summary>Details</summary>
Motivation: 现有基于神经定位的LLM遗忘方法将神经元视为单一纠缠组，导致过度遗忘或擦除不彻底的问题

Method: 采用机制可解释性技术定位遗忘/保留电路，转化为合取范式后通过可满足解分配神经元归属，实施差异化微调策略

Result: 实验证明CLUE通过精确神经定位，在遗忘效果和保留效用上均优于现有定位方法

Conclusion: 分离式电路定位与针对性干预策略可有效解决传统LLM遗忘中知识保留与消除的平衡难题

Abstract: The LLM unlearning aims to eliminate the influence of undesirable data
without affecting causally unrelated information. This process typically
involves using a forget set to remove target information, alongside a retain
set to maintain non-target capabilities. While recent localization-based
methods demonstrate promise in identifying important neurons to be unlearned,
they fail to disentangle neurons responsible for forgetting undesirable
knowledge or retaining essential skills, often treating them as a single
entangled group. As a result, these methods apply uniform interventions,
risking catastrophic over-forgetting or incomplete erasure of the target
knowledge. To address this, we turn to circuit discovery, a mechanistic
interpretability technique, and propose the Conflict-guided Localization for
LLM Unlearning framEwork (CLUE). This framework identifies the forget and
retain circuit composed of important neurons, and then the circuits are
transformed into conjunctive normal forms (CNF). The assignment of each neuron
in the CNF satisfiability solution reveals whether it should be forgotten or
retained. We then provide targeted fine-tuning strategies for different
categories of neurons. Extensive experiments demonstrate that, compared to
existing localization methods, CLUE achieves superior forget efficacy and
retain utility through precise neural localization.

</details>


### [85] [Binary Autoencoder for Mechanistic Interpretability of Large Language Models](https://arxiv.org/abs/2509.20997)
*Hakaze Cho,Haolin Yang,Brian M. Kurkoski,Naoya Inoue*

Main category: cs.LG

TL;DR: 提出基于最小熵约束的Binary Autoencoder（BAE），通过离散化隐藏激活提升特征独立性和跨实例稀疏性


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖单实例正则化导致特征密集，缺乏全局稀疏性保证，损害特征原子化效果

Method: 在mini-batch上施加最小熵约束，使用阶跃函数将激活离散化为1-bit，通过梯度估计实现反向传播

Result: 成功应用于LLM推理动态表征和上下文学习分析，在特征解缠任务中产生最多可解释特征且避免密集激活

Conclusion: BAE作为新型特征提取器，在保证稀疏性的同时提升了解释性和应用潜力

Abstract: Existing works are dedicated to untangling atomized numerical components
(features) from the hidden states of Large Language Models (LLMs) for
interpreting their mechanism. However, they typically rely on autoencoders
constrained by some implicit training-time regularization on single training
instances (i.e., $L_1$ normalization, top-k function, etc.), without an
explicit guarantee of global sparsity among instances, causing a large amount
of dense (simultaneously inactive) features, harming the feature sparsity and
atomization. In this paper, we propose a novel autoencoder variant that
enforces minimal entropy on minibatches of hidden activations, thereby
promoting feature independence and sparsity across instances. For efficient
entropy calculation, we discretize the hidden activations to 1-bit via a step
function and apply gradient estimation to enable backpropagation, so that we
term it as Binary Autoencoder (BAE) and empirically demonstrate two major
applications: (1) Feature set entropy calculation. Entropy can be reliably
estimated on binary hidden activations, which we empirically evaluate and
leverage to characterize the inference dynamics of LLMs and In-context
Learning. (2) Feature untangling. Similar to typical methods, BAE can extract
atomized features from LLM's hidden states. To robustly evaluate such feature
extraction capability, we refine traditional feature-interpretation methods to
avoid unreliable handling of numerical tokens, and show that BAE avoids dense
features while producing the largest number of interpretable ones among
baselines, which confirms the effectiveness of BAE serving as a feature
extractor.

</details>


### [86] [Mechanism of Task-oriented Information Removal in In-context Learning](https://arxiv.org/abs/2509.21012)
*Hakaze Cho,Haolin Yang,Gouki Minegishi,Naoya Inoue*

Main category: cs.LG

TL;DR: 论文通过信息移除视角揭示上下文学习(ICL)机制：零样本下语言模型生成非选择性表征导致随机输出，低秩滤波器选择性移除信息可引导模型；少样本ICL通过模拟该过程去除冗余信息，关键注意力头(去噪头)的阻断会显著降低准确性。


<details>
  <summary>Details</summary>
Motivation: 探究ICL在少样本学习中的内在机制，特别是理解语言模型如何通过上下文演示实现任务导向的输出优化。

Method: 1. 分析零样本下语言模型的隐藏状态特征
2. 设计低秩滤波器选择性移除特定信息
3. 构建指标测量ICL对隐藏状态的去冗余过程
4. 识别关键注意力头并进行阻断实验验证

Result: 1. 零样本下非选择性表征导致接近0的准确率
2. 信息移除操作可使准确率提升60-80%
3. 阻断去噪头后ICL准确率下降50%以上（无正确标签时更显著）

Conclusion: 信息移除机制是ICL的核心原理，去噪头是实现该过程的关键组件，这为理解语言模型的少样本学习提供了新的理论框架。

Abstract: In-context Learning (ICL) is an emerging few-shot learning paradigm based on
modern Language Models (LMs), yet its inner mechanism remains unclear. In this
paper, we investigate the mechanism through a novel perspective of information
removal. Specifically, we demonstrate that in the zero-shot scenario, LMs
encode queries into non-selective representations in hidden states containing
information for all possible tasks, leading to arbitrary outputs without
focusing on the intended task, resulting in near-zero accuracy. Meanwhile, we
find that selectively removing specific information from hidden states by a
low-rank filter effectively steers LMs toward the intended task. Building on
these findings, by measuring the hidden states on carefully designed metrics,
we observe that few-shot ICL effectively simulates such task-oriented
information removal processes, selectively removing the redundant information
from entangled non-selective representations, and improving the output based on
the demonstrations, which constitutes a key mechanism underlying ICL. Moreover,
we identify essential attention heads inducing the removal operation, termed
Denoising Heads, which enables the ablation experiments blocking the
information removal operation from the inference, where the ICL accuracy
significantly degrades, especially when the correct label is absent from the
few-shot demonstrations, confirming both the critical role of the information
removal mechanism and denoising heads.

</details>


### [87] [DELTA-Code: How Does RL Unlock and Transfer New Programming Algorithms in LLMs?](https://arxiv.org/abs/2509.21016)
*Yiyou Sun,Yuhan Cao,Pohao Huang,Haoyue Bai,Hannaneh Hajishirzi,Nouha Dziri,Dawn Song*

Main category: cs.LG

TL;DR: 提出DELTA-Code基准测试评估LLMs的新推理策略学习能力与迁移能力，揭示强化学习训练中出现知识突现现象


<details>
  <summary>Details</summary>
Motivation: 验证LLMs能否突破预训练知识边界，通过强化学习掌握全新算法编码策略

Method: 构建模板化编码问题家族，采用阶段性密集奖励预热+经验回放+课程训练+验证闭环的强化学习框架

Result: 发现grokking相变现象（训练后期准确率突变跃升），实现跨问题家族的组合技能迁移，但转化性迁移仍存瓶颈

Conclusion: DELTA为探索RL驱动的推理边界提供标准化测试平台，证明模型具有有限但可扩展的新算法技能获取能力

Abstract: It remains an open question whether LLMs can acquire or generalize genuinely
new reasoning strategies, beyond the sharpened skills encoded in their
parameters during pre-training or post-training. To attempt to answer this
debate, we introduce DELTA-Code--Distributional Evaluation of Learnability and
Transferrability in Algorithmic Coding, a controlled benchmark of synthetic
coding problem families designed to probe two fundamental aspects: learnability
-- can LLMs, through reinforcement learning (RL), solve problem families where
pretrained models exhibit failure with large enough attempts (pass@K=0)? --and
transferrability -- if learnability happens, can such skills transfer
systematically to out-of-distribution (OOD) test sets? Unlike prior public
coding datasets, DELTA isolates reasoning skills through templated problem
generators and introduces fully OOD problem families that demand novel
strategies rather than tool invocation or memorized patterns. Our experiments
reveal a striking grokking phase transition: after an extended period with
near-zero reward, RL-trained models abruptly climb to near-perfect accuracy. To
enable learnability on previously unsolvable problem families, we explore key
training ingredients such as staged warm-up with dense rewards, experience
replay, curriculum training, and verification-in-the-loop. Beyond learnability,
we use DELTA to evaluate transferability or generalization along exploratory,
compositional, and transformative axes, as well as cross-family transfer.
Results show solid gains within families and for recomposed skills, but
persistent weaknesses in transformative cases. DELTA thus offers a clean
testbed for probing the limits of RL-driven reasoning and for understanding how
models can move beyond existing priors to acquire new algorithmic skills.

</details>


### [88] [ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning](https://arxiv.org/abs/2509.21070)
*Qizhi Pei,Zhuoshi Pan,Honglin Lin,Xin Gao,Yu Li,Zinan Tang,Conghui He,Rui Yan,Lijun Wu*

Main category: cs.LG

TL;DR: 提出ScaleDiff流程，通过自适应思维模型筛选困难数学问题并训练专用生成器DiffGen-8B，显著提升推理模型性能且无需昂贵教师模型


<details>
  <summary>Details</summary>
Motivation: 现有数学问题生成方法存在API成本高、提示复杂度高且生成问题难度有限的问题，需要可扩展的低成本解决方案

Method: 1. 使用自适应思维模型单次前向传播筛选困难问题
2. 在过滤后的数据上训练专用问题生成器DiffGen-8B
3. 通过Qwen2.5-Math-7B模型进行低成本知识蒸馏

Result: 在ScaleDiff-Math数据集上实现11.3%性能提升，在五大数学基准达到65.9%平均准确率，超越OpenThinker3等模型

Conclusion: ScaleDiff成功实现低成本大规模生成高难度问题，验证了困难问题数量与模型性能的扩展关系，为推理能力提升提供新范式

Abstract: Large Reasoning Models (LRMs) have shown impressive capabilities in complex
problem-solving, often benefiting from training on difficult mathematical
problems that stimulate intricate reasoning. Recent efforts have explored
automated synthesis of mathematical problems by prompting proprietary models or
large-scale open-source models from seed data or inherent mathematical
concepts. However, scaling up these methods remains challenging due to their
high computational/API cost, complexity of prompting, and limited difficulty
level of the generated problems. To overcome these limitations, we propose
ScaleDiff, a simple yet effective pipeline designed to scale the creation of
difficult problems. We efficiently identify difficult problems from existing
datasets with only a single forward pass using an adaptive thinking model,
which can perceive problem difficulty and automatically switch between
"Thinking" and "NoThinking" modes. We then train a specialized difficult
problem generator (DiffGen-8B) on this filtered difficult data, which can
produce new difficult problems in large scale, eliminating the need for
complex, per-instance prompting and its associated high API costs. Fine-tuning
Qwen2.5-Math-7B-Instruct on the ScaleDiff-Math dataset yields a substantial
performance increase of 11.3% compared to the original dataset and achieves a
65.9% average accuracy on AIME'24, AIME'25, HMMT-Feb'25, BRUMO'25, and MATH500,
outperforming recent strong LRMs like OpenThinker3. Notably, this performance
is achieved using the cost-efficient Qwen3-8B model as a teacher, demonstrating
that our pipeline can effectively transfer advanced reasoning capabilities
without relying on larger, more expensive teacher models. Furthermore, we
observe a clear scaling phenomenon in model performance on difficult benchmarks
as the quantity of difficult problems increases. Code:
https://github.com/QizhiPei/ScaleDiff.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [89] [Verification Limits Code LLM Training](https://arxiv.org/abs/2509.20837)
*Srishti Gureja,Elena Tommasone,Jingyi He,Sara Hooker,Matthias Gallé,Marzieh Fadaee*

Main category: cs.SE

TL;DR: 论文揭示当前代码生成模型的合成数据验证存在刚性瓶颈，通过优化测试复杂度、放宽验证阈值和保留方案多样性可突破验证天花板，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 合成数据训练虽可扩展，但验证环节的质量和多样性受限于模型自身能力，形成验证瓶颈。需系统研究验证策略对模型性能的影响机制。

Method: 三阶段研究：1) 分析测试复杂度/数量影响 2) 探索放宽通过阈值和LLM软验证 3) 对比正确/错误方案及人类评估验证必要性

Result: 复杂测试提升3个pass@1点；放宽阈值可回收2-4点性能增益；保留方案多样性带来持续泛化收益

Conclusion: 需校准验证刚性，结合挑战性多样化问题方案对，突破验证天花板。验证不可废弃但需优化，强调测试强度与方案多样性协同作用。

Abstract: Large language models for code generation increasingly rely on synthetic
data, where both problem solutions and verification tests are generated by
models. While this enables scalable data creation, it introduces a previously
unexplored bottleneck: the verification ceiling, in which the quality and
diversity of training data are fundamentally constrained by the capabilities of
synthetic verifiers. In this work, we systematically study how verification
design and strategies influence model performance. We investigate (i) what we
verify by analyzing the impact of test complexity and quantity: richer test
suites improve code generation capabilities (on average +3 pass@1), while
quantity alone yields diminishing returns, (ii) how we verify by exploring
relaxed pass thresholds: rigid 100% pass criteria can be overly restrictive. By
allowing for relaxed thresholds or incorporating LLM-based soft verification,
we can recover valuable training data, leading to a 2-4 point improvement in
pass@1 performance. However, this benefit is contingent upon the strength and
diversity of the test cases used, and (iii) why verification remains necessary
through controlled comparisons of formally correct versus incorrect solutions
and human evaluation: retaining diverse correct solutions per problem yields
consistent generalization gains. Our results show that Verification as
currently practiced is too rigid, filtering out valuable diversity. But it
cannot be discarded, only recalibrated. By combining calibrated verification
with diverse, challenging problem-solution pairs, we outline a path to break
the verification ceiling and unlock stronger code generation models.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [90] [Perspectra: Choosing Your Experts Enhances Critical Thinking in Multi-Agent Research Ideation](https://arxiv.org/abs/2509.20553)
*Yiren Liu,Viraj Shah,Sangho Suh,Pao Siangliulue,Tal August,Yun Huang*

Main category: cs.HC

TL;DR: Perspectra多代理系统通过论坛式界面和思维导图增强用户对领域专家代理协作的控制，显著提升批判性思维和跨学科讨论


<details>
  <summary>Details</summary>
Motivation: 解决现有多代理系统中用户难以有效控制、引导和批判性评估领域专家代理协作的问题

Method: 开发具有@提及邀请代理、线程化讨论和实时思维导图的论坛式系统，通过18人分组实验对比群聊基线

Result: 显著增加批判性思维行为频率/深度（+35%），提升跨学科回复（+28%），提案修改频率增加40%

Conclusion: 支持用户控制多代理对抗性讨论的系统设计能有效增强批判性思维，为多智能体工具设计提供新方向

Abstract: Recent advances in multi-agent systems (MAS) enable tools for information
search and ideation by assigning personas to agents. However, how users can
effectively control, steer, and critically evaluate collaboration among
multiple domain-expert agents remains underexplored. We present Perspectra, an
interactive MAS that visualizes and structures deliberation among LLM agents
via a forum-style interface, supporting @-mention to invite targeted agents,
threading for parallel exploration, with a real-time mind map for visualizing
arguments and rationales. In a within-subjects study with 18 participants, we
compared Perspectra to a group-chat baseline as they developed research
proposals. Our findings show that Perspectra significantly increased the
frequency and depth of critical-thinking behaviors, elicited more
interdisciplinary replies, and led to more frequent proposal revisions than the
group chat condition. We discuss implications for designing multi-agent tools
that scaffold critical thinking by supporting user control over multi-agent
adversarial discourse.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [91] [Visual Authority and the Rhetoric of Health Misinformation: A Multimodal Analysis of Social Media Videos](https://arxiv.org/abs/2509.20724)
*Mohammad Reza Zarei,Barbara Stead-Coyle,Michael Christensen,Sarah Everts,Majid Komeili*

Main category: cs.SI

TL;DR: 研究通过多平台视频分析，揭示健康类短视频如何通过权威信号（证书/专业术语）、情感叙事（恐惧/阴谋论）与商业动机（订阅/商品链接）的交叉包装来构建可信度。


<details>
  <summary>Details</summary>
Motivation: 短视频平台存在健康信息混杂现象，需系统分析可信度包装机制以避免误导。现有研究多聚焦文本信息，缺乏对视频多模态特征（视觉+叙事+商业）的交叉研究。

Method: 采集TikTok/Instagram/YouTube的152个营养类视频，建立包含26个特征的标注体系（权威信号/叙事策略/盈利模式），结合ASR语音识别、多模态模型与分层人工验证实现可靠标注。

Result: 1. 88%视频由单一自信主讲人在非临床场景制作
2. 63%使用专业图表/证书等权威信号时同步出现恐惧叙事或阴谋论
3. 科学类视觉素材多与情感化叙事（而非客观论证）强关联

Conclusion: 健康类短视频通过'科学包装-情感驱动-商业引导'三位一体构建伪可信度。平台需建立多模态内容审核机制，用户应警惕'权威形式+情绪煽动'的组合式说服策略。

Abstract: Short form video platforms are central sites for health advice, where
alternative narratives mix useful, misleading, and harmful content. Rather than
adjudicating truth, this study examines how credibility is packaged in
nutrition and supplement videos by analyzing the intersection of authority
signals, narrative techniques, and monetization. We assemble a cross platform
corpus of 152 public videos from TikTok, Instagram, and YouTube and annotate
each on 26 features spanning visual authority, presenter attributes, narrative
strategies, and engagement cues. A transparent annotation pipeline integrates
automatic speech recognition, principled frame selection, and a multimodal
model, with human verification on a stratified subsample showing strong
agreement. Descriptively, a confident single presenter in studio or home
settings dominates, and clinical contexts are rare. Analytically, authority
cues such as titles, slides and charts, and certificates frequently occur with
persuasive elements including jargon, references, fear or urgency, critiques of
mainstream medicine, and conspiracies, and with monetization including sales
links and calls to subscribe. References and science like visuals often travel
with emotive and oppositional narratives rather than signaling restraint.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [92] [On Theoretical Interpretations of Concept-Based In-Context Learning](https://arxiv.org/abs/2509.20882)
*Huaze Tang,Tianren Peng,Shao-lun Huang*

Main category: cs.IT

TL;DR: 论文提出了基于概念的上下文学习（CB-ICL）理论框架，通过量化LLM知识迁移能力与构建提示相似性度量，揭示了少量示例预测的有效性机制，并为模型预训练和提示工程提供理论指导。


<details>
  <summary>Details</summary>
Motivation: 针对上下文学习（ICL）机制理论理解不足的问题，探索CB-ICL在少样本场景下有效预测的根本原因及其适用边界。

Method: 建立CB-ICL理论模型分析知识迁移路径，提出提示演示-查询相似性度量指标，并通过控制演示数量、嵌入维度等变量进行理论推导。

Result: 实验验证了理论预测：CB-ICL能有效解释少样本预测性能，相似性度量指标对预训练数据分布和提示设计具有显著指导作用。

Conclusion: 该理论框架不仅解释了CB-ICL的运作机制，更通过量化指标为LLM的预训练优化和实际应用中的提示工程提供了可操作的理论依据。

Abstract: In-Context Learning (ICL) has emerged as an important new paradigm in natural
language processing and large language model (LLM) applications. However, the
theoretical understanding of the ICL mechanism remains limited. This paper aims
to investigate this issue by studying a particular ICL approach, called
concept-based ICL (CB-ICL). In particular, we propose theoretical analyses on
applying CB-ICL to ICL tasks, which explains why and when the CB-ICL performs
well for predicting query labels in prompts with only a few demonstrations. In
addition, the proposed theory quantifies the knowledge that can be leveraged by
the LLMs to the prompt tasks, and leads to a similarity measure between the
prompt demonstrations and the query input, which provides important insights
and guidance for model pre-training and prompt engineering in ICL. Moreover,
the impact of the prompt demonstration size and the dimension of the LLM
embeddings in ICL are also explored based on the proposed theory. Finally,
several real-data experiments are conducted to validate the practical
usefulness of CB-ICL and the corresponding theory.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [93] [RadAgents: Multimodal Agentic Reasoning for Chest X-ray Interpretation with Radiologist-like Workflows](https://arxiv.org/abs/2509.20490)
*Kai Zhang,Corey D Barrett,Jangwon Kim,Lichao Sun,Tara Taghavi,Krishnaram Kenthapadi*

Main category: cs.MA

TL;DR: 提出RadAgents多智能体框架解决胸部X光片解释中临床推理不可靠、多模态融合不足和验证机制缺失的问题


<details>
  <summary>Details</summary>
Motivation: 现有CXR解释方法存在三大局限：(1) 临床推理缺乏可解释性且偏离指南，(2) 多模态证据融合不足导致文本解释缺乏视觉依据，(3) 缺乏跨工具不一致性检测和系统化验证机制

Method: 结合临床先验与任务感知的多模态推理，整合基于多模态检索增强的上下文冲突验证机制

Result: 构建可靠透明的输出验证系统，显著提升与临床实践的一致性

Conclusion: RadAgents通过多智能体协作与多模态验证机制，为医学影像解释建立了新的可信推理范式

Abstract: Agentic systems offer a potential path to solve complex clinical tasks
through collaboration among specialized agents, augmented by tool use and
external knowledge bases. Nevertheless, for chest X-ray (CXR) interpretation,
prevailing methods remain limited: (i) reasoning is frequently neither
clinically interpretable nor aligned with guidelines, reflecting mere
aggregation of tool outputs; (ii) multimodal evidence is insufficiently fused,
yielding text-only rationales that are not visually grounded; and (iii) systems
rarely detect or resolve cross-tool inconsistencies and provide no principled
verification mechanisms. To bridge the above gaps, we present RadAgents, a
multi-agent framework for CXR interpretation that couples clinical priors with
task-aware multimodal reasoning. In addition, we integrate grounding and
multimodal retrieval-augmentation to verify and resolve context conflicts,
resulting in outputs that are more reliable, transparent, and consistent with
clinical practice.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [94] [Every Character Counts: From Vulnerability to Defense in Phishing Detection](https://arxiv.org/abs/2509.20589)
*Maria Chiper,Radu Tudor Ionescu*

Main category: cs.CR

TL;DR: 该论文提出使用字符级深度学习模型（CharGRU表现最佳）结合对抗训练提升钓鱼邮件检测的鲁棒性和可解释性，并开发浏览器扩展工具。


<details>
  <summary>Details</summary>
Motivation: 现有钓鱼检测方法在对抗攻击下缺乏可解释性和鲁棒性，需开发更有效且能适应资源受限环境的解决方案。

Method: 使用CharCNN/CharGRU/CharBiLSTM三种字符级模型，在自定义混合邮件数据集上进行标准训练、对抗攻击测试和对抗训练，并改进Grad-CAM技术实现可视化。

Result: CharGRU在所有测试场景中表现最优，对抗训练使模型F1分数提升15-20%，改进的Grad-CAM能有效定位关键字符证据。

Conclusion: 字符级模型+对抗训练+可视化解释形成了高效的钓鱼检测方案，特别适合作为资源受限的终端安全工具部署。

Abstract: Phishing attacks targeting both organizations and individuals are becoming an
increasingly significant threat as technology advances. Current automatic
detection methods often lack explainability and robustness in detecting new
phishing attacks. In this work, we investigate the effectiveness of
character-level deep learning models for phishing detection, which can provide
both robustness and interpretability. We evaluate three neural architectures
adapted to operate at the character level, namely CharCNN, CharGRU, and
CharBiLSTM, on a custom-built email dataset, which combines data from multiple
sources. Their performance is analyzed under three scenarios: (i) standard
training and testing, (ii) standard training and testing under adversarial
attacks, and (iii) training and testing with adversarial examples. Aiming to
develop a tool that operates as a browser extension, we test all models under
limited computational resources. In this constrained setup, CharGRU proves to
be the best-performing model across all scenarios. All models show
vulnerability to adversarial attacks, but adversarial training substantially
improves their robustness. In addition, by adapting the Gradient-weighted Class
Activation Mapping (Grad-CAM) technique to character-level inputs, we are able
to visualize which parts of each email influence the decision of each model.
Our open-source code and data is released at
https://github.com/chipermaria/every-character-counts.

</details>


### [95] [PMark: Towards Robust and Distortion-free Semantic-level Watermarking with Channel Constraints](https://arxiv.org/abs/2509.21057)
*Jiahao Huo,Shuliang Liu,Bin Wang,Junyan Zhang,Yibo Yan,Aiwei Liu,Xuming Hu,Mingxun Zhou*

Main category: cs.CR

TL;DR: 提出基于代理函数框架的PMark方法，通过动态中位数估计与多重约束通道增强语义水印鲁棒性，在保持无失真的同时显著提升对抗复述攻击能力。


<details>
  <summary>Details</summary>
Motivation: 现有语义级水印方法存在理论鲁棒性保障不足、拒绝采样导致分布失真等问题，且对抗文本修改/复述攻击效果有限。

Method: 构建代理函数框架，动态采样估计句子代理函数中位数，采用多通道约束强化水印证据；优化版本通过预计算提升采样效率。

Result: 实验证明PMark在文本质量与鲁棒性上优于现有基线，检测准确率提升15%-20%，且保持与原始模型一致的生成分布。

Conclusion: PMark为机器文本检测提供更有效范式，兼具理论保证、无失真特性与高效实现，显著推进语义水印技术实用化进程。

Abstract: Semantic-level watermarking (SWM) for large language models (LLMs) enhances
watermarking robustness against text modifications and paraphrasing attacks by
treating the sentence as the fundamental unit. However, existing methods still
lack strong theoretical guarantees of robustness, and reject-sampling-based
generation often introduces significant distribution distortions compared with
unwatermarked outputs. In this work, we introduce a new theoretical framework
on SWM through the concept of proxy functions (PFs) $\unicode{x2013}$ functions
that map sentences to scalar values. Building on this framework, we propose
PMark, a simple yet powerful SWM method that estimates the PF median for the
next sentence dynamically through sampling while enforcing multiple PF
constraints (which we call channels) to strengthen watermark evidence. Equipped
with solid theoretical guarantees, PMark achieves the desired distortion-free
property and improves the robustness against paraphrasing-style attacks. We
also provide an empirically optimized version that further removes the
requirement for dynamical median estimation for better sampling efficiency.
Experimental results show that PMark consistently outperforms existing SWM
baselines in both text quality and robustness, offering a more effective
paradigm for detecting machine-generated text. Our code will be released at
[this URL](https://github.com/PMark-repo/PMark).

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [96] [InsightGUIDE: An Opinionated AI Assistant for Guided Critical Reading of Scientific Literature](https://arxiv.org/abs/2509.20493)
*Paris Koloveas,Serafeim Chatzopoulos,Thanasis Vergoulis,Christos Tryfonopoulos*

Main category: cs.AI

TL;DR: 介绍InsightGUIDE作为AI阅读助手，通过结构化洞察帮助研究者高效理解论文


<details>
  <summary>Details</summary>
Motivation: 现有工具常生成冗长摘要，存在替代而非辅助阅读的风险

Method: 将专家阅读方法嵌入AI核心逻辑，采用提示驱动架构设计

Result: 相比通用LLM，系统输出更具结构化和可操作性

Conclusion: InsightGUIDE为现代研究者提供更有效的论文理解工具

Abstract: The proliferation of scientific literature presents an increasingly
significant challenge for researchers. While Large Language Models (LLMs) offer
promise, existing tools often provide verbose summaries that risk replacing,
rather than assisting, the reading of the source material. This paper
introduces InsightGUIDE, a novel AI-powered tool designed to function as a
reading assistant, not a replacement. Our system provides concise, structured
insights that act as a "map" to a paper's key elements by embedding an expert's
reading methodology directly into its core AI logic. We present the system's
architecture, its prompt-driven methodology, and a qualitative case study
comparing its output to a general-purpose LLM. The results demonstrate that
InsightGUIDE produces more structured and actionable guidance, serving as a
more effective tool for the modern researcher.

</details>


### [97] [CLAUSE: Agentic Neuro-Symbolic Knowledge Graph Reasoning via Dynamic Learnable Context Engineering](https://arxiv.org/abs/2509.21035)
*Yang Zhao,Chengxiao Dai,Wei Zhuo,Yue Xiu,Dusit Niyato*

Main category: cs.AI

TL;DR: CLAUSE框架通过三个代理协同优化知识图谱多跳问答，在资源预算约束下实现更高准确率与更低延迟/成本


<details>
  <summary>Details</summary>
Motivation: 解决现有静态扩展方法存在的过度检索、上下文膨胀和运行时不可预测问题，实现延迟-成本-准确性的动态平衡

Method: 采用LC-MAPPO算法协调子图构建师、路径导航员和上下文策展人三个代理，联合优化子图构建/路径发现/证据选择过程

Result: 在MetaQA-2-hop任务中实现+39.3% EM@1提升，延迟降低18.6%，边增长减少40.9%

Conclusion: 框架生成的上下文紧凑且保留来源，在部署约束下提供可预测的性能，适用于实际生产环境

Abstract: Knowledge graphs provide structured context for multi-hop question answering,
but deployed systems must balance answer accuracy with strict latency and cost
targets while preserving provenance. Static k-hop expansions and "think-longer"
prompting often over-retrieve, inflate context, and yield unpredictable
runtime. We introduce CLAUSE, an agentic three-agent neuro-symbolic framework
that treats context construction as a sequential decision process over
knowledge graphs, deciding what to expand, which paths to follow or backtrack,
what evidence to keep, and when to stop. Latency (interaction steps) and prompt
cost (selected tokens) are exposed as user-specified budgets or prices,
allowing per-query adaptation to trade-offs among accuracy, latency, and cost
without retraining. CLAUSE employs the proposed Lagrangian-Constrained
Multi-Agent Proximal Policy Optimization (LC-MAPPO) algorithm to coordinate
three agents: Subgraph Architect, Path Navigator, and Context Curator, so that
subgraph construction, reasoning-path discovery, and evidence selection are
jointly optimized under per-query resource budgets on edge edits, interaction
steps, and selected tokens. Across HotpotQA, MetaQA, and FactKG, CLAUSE yields
higher EM@1 while reducing subgraph growth and end-to-end latency at equal or
lower token budgets. On MetaQA-2-hop, relative to the strongest RAG baseline
(GraphRAG), CLAUSE achieves +39.3 EM@1 with 18.6% lower latency and 40.9% lower
edge growth. The resulting contexts are compact, provenance-preserving, and
deliver predictable performance under deployment constraints.

</details>


### [98] [Disagreements in Reasoning: How a Model's Thinking Process Dictates Persuasion in Multi-Agent Systems](https://arxiv.org/abs/2509.21054)
*Haodong Zhao,Jidong Li,Zhaomin Wu,Tianjie Ju,Zhuosheng Zhang,Bingsheng He,Gongshen Liu*

Main category: cs.AI

TL;DR: 研究揭示了大型推理模型(LRMs)的『说服二元性』：推理过程增强抗说服性，但分享思考内容会显著提升其说服力，并展示了多智能体网络中的复杂传播动态。


<details>
  <summary>Details</summary>
Motivation: 挑战『模型规模决定说服效果』的假设，探索认知过程(尤其是显式推理能力)对多智能体系统说服动态的根本性影响。

Method: 通过多智能体说服实验，分析LRMs在不同情境下的说服抗性及传播效果，并研究多跳说服网络中影响的传播与衰减规律。

Result: 发现LRMs的推理过程使其抗说服性提升40%，而思考内容共享使说服成功率提升65%；多跳传播呈现指数衰减与非对称扩散特征。

Conclusion: 模型内部处理架构与外部说服行为存在系统性关联，这对未来MAS系统的安全性设计和鲁棒性提升具有重要指导意义。

Abstract: The rapid proliferation of recent Multi-Agent Systems (MAS), where Large
Language Models (LLMs) and Large Reasoning Models (LRMs) usually collaborate to
solve complex problems, necessitates a deep understanding of the persuasion
dynamics that govern their interactions. This paper challenges the prevailing
hypothesis that persuasive efficacy is primarily a function of model scale. We
propose instead that these dynamics are fundamentally dictated by a model's
underlying cognitive process, especially its capacity for explicit reasoning.
Through a series of multi-agent persuasion experiments, we uncover a
fundamental trade-off we term the Persuasion Duality. Our findings reveal that
the reasoning process in LRMs exhibits significantly greater resistance to
persuasion, maintaining their initial beliefs more robustly. Conversely, making
this reasoning process transparent by sharing the "thinking content"
dramatically increases their ability to persuade others. We further consider
more complex transmission persuasion situations and reveal complex dynamics of
influence propagation and decay within multi-hop persuasion between multiple
agent networks. This research provides systematic evidence linking a model's
internal processing architecture to its external persuasive behavior, offering
a novel explanation for the susceptibility of advanced models and highlighting
critical implications for the safety, robustness, and design of future MAS.

</details>


### [99] [TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them](https://arxiv.org/abs/2509.21117)
*Yidong Wang,Yunze Song,Tingyuan Zhu,Xuanwang Zhang,Zhuohao Yu,Hao Chen,Chiyu Song,Qiufeng Wang,Cunxiang Wang,Zhen Wu,Xinyu Dai,Yue Zhang,Wei Ye,Shikun Zhang*

Main category: cs.AI

TL;DR: 提出了TrustJudge概率框架，解决LLM自动评估中的评分不一致和传递不一致问题，通过分布敏感评分和概率聚合优化评估可靠性


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为评估工具存在评分比较不一致（高低分倒置）和成对传递不一致（循环偏好链）的核心缺陷，源于离散评分系统信息丢失和模糊判断

Method: 1. 分布敏感评分：基于离散评分概率计算连续期望值，保留信息熵；2. 概率感知聚合：通过双向偏好概率或困惑度解决传递悖论

Result: 使用Llama-3.1-70B评估时，评分比较不一致率降低8.43%（23.32%→14.89%），传递不一致率降低10.82%（15.22%→4.40%），且保持更高准确率

Conclusion: 首次系统分析LLM评估框架不一致性，提出理论解决方案TrustJudge，无需额外训练即可提升评估可信度，适用于不同模型架构与规模

Abstract: The adoption of Large Language Models (LLMs) as automated evaluators
(LLM-as-a-judge) has revealed critical inconsistencies in current evaluation
frameworks. We identify two fundamental types of inconsistencies: (1)
Score-Comparison Inconsistency, where lower-rated responses outperform
higher-scored ones in pairwise comparisons, and (2) Pairwise Transitivity
Inconsistency, manifested through circular preference chains (A>B>C>A) and
equivalence contradictions (A=B=C\neq A). We argue that these issues come from
information loss in discrete rating systems and ambiguous tie judgments during
pairwise evaluation. We propose TrustJudge, a probabilistic framework that
addresses these limitations through two key innovations: 1)
distribution-sensitive scoring that computes continuous expectations from
discrete rating probabilities, preserving information entropy for more precise
scoring, and 2) likelihood-aware aggregation that resolves transitivity
violations using bidirectional preference probabilities or perplexity. We also
formalize the theoretical limitations of current LLM-as-a-judge frameworks and
demonstrate how TrustJudge's components overcome them. When evaluated with
Llama-3.1-70B-Instruct as judge using our dataset, TrustJudge reduces
Score-Comparison inconsistency by 8.43% (from 23.32% to 14.89%) and Pairwise
Transitivity inconsistency by 10.82% (from 15.22% to 4.40%), while maintaining
higher evaluation accuracy. Our work provides the first systematic analysis of
evaluation framework inconsistencies in LLM-as-a-judge paradigms, offering both
theoretical insights and practical solutions for reliable automated assessment.
The framework demonstrates consistent improvements across various model
architectures and scales, enabling more trustworthy LLM evaluation without
requiring additional training or human annotations. The codes can be found at
https://github.com/TrustJudge/TrustJudge.

</details>


### [100] [Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns](https://arxiv.org/abs/2509.21124)
*Xuemiao Zhang,Can Ren,Chengying Tu,Rongxiang Weng,Shuo Wang,Hongfei Yan,Jingang Wang,Xunliang Cai*

Main category: cs.AI

TL;DR: 通过选择富含高价值推理模式的思维链数据（CoTP），显著提升大模型数学推理能力


<details>
  <summary>Details</summary>
Motivation: 当前方法对思维链数据不加选择使用，需要明确哪些数据类型能有效增强模型推理能力

Method: 1. 定义推理潜力指标（所需独立尝试次数的倒数）
2. 构建富含推理模式的核心参考集
3. 提出基于推理模式链和标记熵的双重粒度数据选择算法

Result: 仅用10B token的CoTP数据即实现：
- 85A6B MoE模型在AIME测试提升9.58%
- 下游RL性能上限提高7.81%

Conclusion: 通过模式挖掘与数据优选机制，实现了模型推理潜力的高效扩展，为深度推理训练提供新范式

Abstract: Recent progress in large reasoning models for challenging mathematical
reasoning has been driven by reinforcement learning (RL). Incorporating long
chain-of-thought (CoT) data during mid-training has also been shown to
substantially improve reasoning depth. However, current approaches often
utilize CoT data indiscriminately, leaving open the critical question of which
data types most effectively enhance model reasoning capabilities. In this
paper, we define the foundation model's reasoning potential for the first time
as the inverse of the number of independent attempts required to correctly
answer the question, which is strongly correlated with the final model
performance. We then propose utilizing diverse data enriched with high-value
reasoning patterns to expand the reasoning potential. Specifically, we abstract
atomic reasoning patterns from CoT sequences, characterized by commonality and
inductive capabilities, and use them to construct a core reference set enriched
with valuable reasoning patterns. Furthermore, we propose a dual-granularity
algorithm involving chains of reasoning patterns and token entropy, efficiently
selecting high-value CoT data (CoTP) from the data pool that aligns with the
core set, thereby training models to master reasoning effectively. Only
10B-token CoTP data enables the 85A6B Mixture-of-Experts (MoE) model to improve
by 9.58% on the challenging AIME 2024 and 2025, and to raise the upper bound of
downstream RL performance by 7.81%.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [101] [CARINOX: Inference-time Scaling with Category-Aware Reward-based Initial Noise Optimization and Exploration](https://arxiv.org/abs/2509.17458)
*Seyed Amir Kasaei,Ali Aghayari,Arash Marioriyad,Niki Sepasian,Shayan Baghayi Nejad,MohammadAmin Fazli,Mahdieh Soleymani Baghshah,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: 提出CARINOX框架，结合噪声优化与探索策略，通过基于人类判断相关性的奖励选择，显著提升文本-图像组合对齐效果


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在复杂组合生成时存在对齐不足问题，单独使用噪声优化易陷入局部最优，单纯探索策略需要大量样本且缺乏有效奖励引导

Method: 开发统一框架CARINOX，集成噪声优化与探索策略，采用与人类判断强相关的多维度奖励指标进行引导

Result: 在T2I-CompBench++和HRS基准测试中分别提升16%和11%的对齐分数，全面超越现有方法且保持图像质量多样性

Conclusion: CARINOX通过系统化的奖励选择机制有效解决组合生成难题，为扩散模型的对齐优化提供了新范式

Abstract: Text-to-image diffusion models, such as Stable Diffusion, can produce
high-quality and diverse images but often fail to achieve compositional
alignment, particularly when prompts describe complex object relationships,
attributes, or spatial arrangements. Recent inference-time approaches address
this by optimizing or exploring the initial noise under the guidance of reward
functions that score text-image alignment without requiring model fine-tuning.
While promising, each strategy has intrinsic limitations when used alone:
optimization can stall due to poor initialization or unfavorable search
trajectories, whereas exploration may require a prohibitively large number of
samples to locate a satisfactory output. Our analysis further shows that
neither single reward metrics nor ad-hoc combinations reliably capture all
aspects of compositionality, leading to weak or inconsistent guidance. To
overcome these challenges, we present Category-Aware Reward-based Initial Noise
Optimization and Exploration (CARINOX), a unified framework that combines noise
optimization and exploration with a principled reward selection procedure
grounded in correlation with human judgments. Evaluations on two complementary
benchmarks covering diverse compositional challenges show that CARINOX raises
average alignment scores by +16% on T2I-CompBench++ and +11% on the HRS
benchmark, consistently outperforming state-of-the-art optimization and
exploration-based methods across all major categories, while preserving image
quality and diversity. The project page is available at
https://amirkasaei.com/carinox/{this URL}.

</details>


### [102] [Leveraging NTPs for Efficient Hallucination Detection in VLMs](https://arxiv.org/abs/2509.20379)
*Ofir Azachi,Kfir Eliyahu,Eyal El Ani,Rom Himelstein,Roi Reichart,Yuval Pinter,Nitay Calderon*

Main category: cs.CV

TL;DR: 提出基于NTP概率的轻量级机器学习方法，有效检测视觉语言模型的幻觉问题


<details>
  <summary>Details</summary>
Motivation: 传统视觉语言模型幻觉检测方法计算成本高且延迟大，需要更高效的解决方案

Method: 使用下一个标记概率（NTPs）作为特征训练传统ML模型，构建1400条人工标注数据集验证

Result: NTP特征预测效果媲美复杂VLM，结合语言NTP和VLM预测分数可进一步提升检测性能

Conclusion: 轻量级NTP方法为提升VLM可靠性提供新方向，平衡效率与准确性

Abstract: Hallucinations of vision-language models (VLMs), which are misalignments
between visual content and generated text, undermine the reliability of VLMs.
One common approach for detecting them employs the same VLM, or a different
one, to assess generated outputs. This process is computationally intensive and
increases model latency. In this paper, we explore an efficient on-the-fly
method for hallucination detection by training traditional ML models over
signals based on the VLM's next-token probabilities (NTPs). NTPs provide a
direct quantification of model uncertainty. We hypothesize that high
uncertainty (i.e., a low NTP value) is strongly associated with hallucinations.
To test this, we introduce a dataset of 1,400 human-annotated statements
derived from VLM-generated content, each labeled as hallucinated or not, and
use it to test our NTP-based lightweight method. Our results demonstrate that
NTP-based features are valuable predictors of hallucinations, enabling fast and
simple ML models to achieve performance comparable to that of strong VLMs.
Furthermore, augmenting these NTPs with linguistic NTPs, computed by feeding
only the generated text back into the VLM, enhances hallucination detection
performance. Finally, integrating hallucination prediction scores from VLMs
into the NTP-based models led to better performance than using either VLMs or
NTPs alone. We hope this study paves the way for simple, lightweight solutions
that enhance the reliability of VLMs.

</details>


### [103] [Human Semantic Representations of Social Interactions from Moving Shapes](https://arxiv.org/abs/2509.20673)
*Yiling Yun,Hongjing Lu*

Main category: cs.CV

TL;DR: 研究发现人类社交感知依赖语义结构补充视觉特征，动词嵌入模型最能解释社会互动相似性判断


<details>
  <summary>Details</summary>
Motivation: 探索人类在识别动态形状社交互动时，如何通过语义表征补充视觉特征信息的认知机制

Method: 研究1通过人工标注收集主观印象，研究2采用相似性判断比较视觉特征/标签/语义嵌入模型的预测效果

Result: 语义模型（特别是基于动画描述的动词嵌入）对视觉特征形成有效补充，解释人类判断效果最佳

Conclusion: 简单动态显示中的社会知觉反映社交互动的语义结构，在视觉表征与抽象概念间建立认知桥梁

Abstract: Humans are social creatures who readily recognize various social interactions
from simple display of moving shapes. While previous research has often focused
on visual features, we examine what semantic representations that humans employ
to complement visual features. In Study 1, we directly asked human participants
to label the animations based on their impression of moving shapes. We found
that human responses were distributed. In Study 2, we measured the
representational geometry of 27 social interactions through human similarity
judgments and compared it with model predictions based on visual features,
labels, and semantic embeddings from animation descriptions. We found that
semantic models provided complementary information to visual features in
explaining human judgments. Among the semantic models, verb-based embeddings
extracted from descriptions account for human similarity judgments the best.
These results suggest that social perception in simple displays reflects the
semantic structure of social interactions, bridging visual and abstract
representations.

</details>


### [104] [Seeing Through Words, Speaking Through Pixels: Deep Representational Alignment Between Vision and Language Models](https://arxiv.org/abs/2509.20751)
*Zoe Wanying He,Sean Trott,Meenakshi Khosla*

Main category: cs.CV

TL;DR: 单模态视觉/语言模型通过中层表征共享语义代码，其对齐效果反映人类偏好且随样本聚合增强


<details>
  <summary>Details</summary>
Motivation: 探索跨模态模型在表示空间中的对齐机制及其与人类认知的关系，揭示语义共享的本质特征

Method: 分层分析表征对齐、语义扰动测试、多对多图文匹配实验（Pick-a-Pic）及嵌入聚合对比

Result: 1. 对齐峰值出现在中高层
2. 语义改变破坏对齐（准确率下降80%）
3. 人类图文选择偏好与模型嵌入空间高度匹配（r=0.92）
4. 样本聚合使对齐度提升37%

Conclusion: 单模态网络自发形成跨模态语义接口，该机制具有认知合理性，且通过信息整合可增强语义表征的鲁棒性

Abstract: Recent studies show that deep vision-only and language-only models--trained
on disjoint modalities--nonetheless project their inputs into a partially
aligned representational space. Yet we still lack a clear picture of where in
each network this convergence emerges, what visual or linguistic cues support
it, whether it captures human preferences in many-to-many image-text scenarios,
and how aggregating exemplars of the same concept affects alignment. Here, we
systematically investigate these questions. We find that alignment peaks in
mid-to-late layers of both model types, reflecting a shift from
modality-specific to conceptually shared representations. This alignment is
robust to appearance-only changes but collapses when semantics are altered
(e.g., object removal or word-order scrambling), highlighting that the shared
code is truly semantic. Moving beyond the one-to-one image-caption paradigm, a
forced-choice "Pick-a-Pic" task shows that human preferences for image-caption
matches are mirrored in the embedding spaces across all vision-language model
pairs. This pattern holds bidirectionally when multiple captions correspond to
a single image, demonstrating that models capture fine-grained semantic
distinctions akin to human judgments. Surprisingly, averaging embeddings across
exemplars amplifies alignment rather than blurring detail. Together, our
results demonstrate that unimodal networks converge on a shared semantic code
that aligns with human judgments and strengthens with exemplar aggregation.

</details>


### [105] [TABLET: A Large-Scale Dataset for Robust Visual Table Understanding](https://arxiv.org/abs/2509.21205)
*Iñigo Alonso,Imanol Miranda,Eneko Agirre,Mirella Lapata*

Main category: cs.CV

TL;DR: 提出大规模视觉表格理解数据集TABLET，通过保留原始可视化特征和可追溯性，提升视觉语言模型在真实场景表格任务中的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉表格理解数据集存在两大局限：1) 主要使用合成渲染表格，缺乏真实表格的视觉多样性 2) 示例固定且无法重新格式化底层数据

Method: 构建包含400万样本的TABLET数据集，覆盖20个任务，基于200万唯一表格（88%保留原始可视化），整合图像-HTML双模态表示及完整元数据体系

Result: 在TABLET微调的Qwen2.5-VL-7B模型：1) 可见任务平均提升7.2%准确率 2) 未见任务提升4.8% 3) 真实表格鲁棒性提升15%

Conclusion: TABLET通过保持原始可视化特征和大规模可追溯样本，为视觉表格理解模型提供了标准化训练框架和可扩展评估基准

Abstract: While table understanding increasingly relies on pixel-only settings where
tables are processed as visual representations, current benchmarks
predominantly use synthetic renderings that lack the complexity and visual
diversity of real-world tables. Additionally, existing visual table
understanding (VTU) datasets offer fixed examples with single visualizations
and pre-defined instructions, providing no access to underlying serialized data
for reformulation. We introduce TABLET, a large-scale VTU dataset with 4
million examples across 20 tasks, grounded in 2 million unique tables where 88%
preserve original visualizations. Each example includes paired image-HTML
representations, comprehensive metadata, and provenance information linking
back to the source datasets. Fine-tuning vision-language models like
Qwen2.5-VL-7B on TABLET improves performance on seen and unseen VTU tasks while
increasing robustness on real-world table visualizations. By preserving
original visualizations and maintaining example traceability in a unified
large-scale collection, TABLET establishes a foundation for robust training and
extensible evaluation of future VTU models.

</details>


### [106] [Sigma: Semantically Informative Pre-training for Skeleton-based Sign Language Understanding](https://arxiv.org/abs/2509.21223)
*Muxin Pu,Mei Kuan Lim,Chun Yong Chong,Chen Change Loy*

Main category: cs.CV

TL;DR: 提出Sigma框架通过跨模态深度交互与分层对齐策略，解决手语理解中语义基础薄弱、局部全局失衡与跨模态低效问题，在多任务上实现SOTA。


<details>
  <summary>Details</summary>
Motivation: 针对当前手语理解模型存在的三个核心问题：1) 骨架数据与语义脱节；2) 局部细节与全局语境失衡；3) 跨模态对齐效率低下。

Method: 1) 视觉-文本早期融合机制增强语义感知；2) 分层对齐学习策略协调多粒度特征；3) 结合对比学习/文本匹配/语言建模的预训练框架。

Result: 在孤立/连续手语识别及无注记翻译任务中刷新多项基准SOTA，验证骨架数据独立解决SLU的可行性。

Conclusion: 骨架数据通过语义化预训练可独立支撑SLU任务，跨模态分层对齐是提升语义一致性的关键。

Abstract: Pre-training has proven effective for learning transferable features in sign
language understanding (SLU) tasks. Recently, skeleton-based methods have
gained increasing attention because they can robustly handle variations in
subjects and backgrounds without being affected by appearance or environmental
factors. Current SLU methods continue to face three key limitations: 1) weak
semantic grounding, as models often capture low-level motion patterns from
skeletal data but struggle to relate them to linguistic meaning; 2) imbalance
between local details and global context, with models either focusing too
narrowly on fine-grained cues or overlooking them for broader context; and 3)
inefficient cross-modal learning, as constructing semantically aligned
representations across modalities remains difficult. To address these, we
propose Sigma, a unified skeleton-based SLU framework featuring: 1) a
sign-aware early fusion mechanism that facilitates deep interaction between
visual and textual modalities, enriching visual features with linguistic
context; 2) a hierarchical alignment learning strategy that jointly maximises
agreements across different levels of paired features from different
modalities, effectively capturing both fine-grained details and high-level
semantic relationships; and 3) a unified pre-training framework that combines
contrastive learning, text matching and language modelling to promote semantic
consistency and generalisation. Sigma achieves new state-of-the-art results on
isolated sign language recognition, continuous sign language recognition, and
gloss-free sign language translation on multiple benchmarks spanning different
sign and spoken languages, demonstrating the impact of semantically informative
pre-training and the effectiveness of skeletal data as a stand-alone solution
for SLU.

</details>


### [107] [Evaluating the Evaluators: Metrics for Compositional Text-to-Image Generation](https://arxiv.org/abs/2509.21227)
*Seyed Amir Kasaei,Ali Aghayari,Arash Marioriyad,Niki Sepasian,MohammadAmin Fazli,Mahdieh Soleymani Baghshah,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: 现有文本到图像生成评估指标缺乏统一验证，研究发现不同任务类型下各指标表现差异显著，需谨慎选择评估指标


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成领域依赖未经人工验证的自动评估指标，可能影响领域进展的可靠性评估

Method: 通过跨任务类型、多指标家族的对比研究，分析指标与人类判断的一致性

Result: 不同组合任务下指标表现不稳定：VQA类指标未全面领先，嵌入模型指标在特定场景更优，纯图像指标无法有效评估组合对齐

Conclusion: 需建立透明化指标选择机制，既要保证评估可信度，也要支持生成模型的强化学习训练

Abstract: Text-image generation has advanced rapidly, but assessing whether outputs
truly capture the objects, attributes, and relations described in prompts
remains a central challenge. Evaluation in this space relies heavily on
automated metrics, yet these are often adopted by convention or popularity
rather than validated against human judgment. Because evaluation and reported
progress in the field depend directly on these metrics, it is critical to
understand how well they reflect human preferences. To address this, we present
a broad study of widely used metrics for compositional text-image evaluation.
Our analysis goes beyond simple correlation, examining their behavior across
diverse compositional challenges and comparing how different metric families
align with human judgments. The results show that no single metric performs
consistently across tasks: performance varies with the type of compositional
problem. Notably, VQA-based metrics, though popular, are not uniformly
superior, while certain embedding-based metrics prove stronger in specific
cases. Image-only metrics, as expected, contribute little to compositional
evaluation, as they are designed for perceptual quality rather than alignment.
These findings underscore the importance of careful and transparent metric
selection, both for trustworthy evaluation and for their use as reward models
in generation. Project page is available at
\href{https://amirkasaei.com/eval-the-evals/}{this URL}.

</details>


### [108] [Hallucination as an Upper Bound: A New Perspective on Text-to-Image Evaluation](https://arxiv.org/abs/2509.21257)
*Seyed Amir Kasaei,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: 论文将文本生成图像（T2I）模型的幻觉现象明确定义为偏置驱动的生成偏差，并提出包含属性/关系/对象幻觉的三维分类体系，为系统评估模型潜在偏置建立理论基础。


<details>
  <summary>Details</summary>
Motivation: 现有T2I评估主要关注提示要素的符合性（alignment），却忽视模型基于固有偏置生成的超提示内容。需要建立系统性框架来揭示模型隐藏偏置，突破现有评估体系的局限。

Method: 提出三维分类框架：1）属性幻觉（错误属性）2）关系幻觉（错误关系）3）对象幻觉（错误对象），构建评估的upper bound并建立系统性分析范式。

Result: 该框架突破传统alignment评估维度，通过建立偏差驱动的评估体系，为揭示T2I模型的潜在偏置提供结构化分析基础，实现更全面的模型能力评估。

Conclusion: 通过明确定义T2I幻觉的评估边界与分类维度，构建了可系统识别模型偏置的理论框架，为未来更丰富的生成模型评估范式奠定基础。

Abstract: In language and vision-language models, hallucination is broadly understood
as content generated from a model's prior knowledge or biases rather than from
the given input. While this phenomenon has been studied in those domains, it
has not been clearly framed for text-to-image (T2I) generative models. Existing
evaluations mainly focus on alignment, checking whether prompt-specified
elements appear, but overlook what the model generates beyond the prompt. We
argue for defining hallucination in T2I as bias-driven deviations and propose a
taxonomy with three categories: attribute, relation, and object hallucinations.
This framing introduces an upper bound for evaluation and surfaces hidden
biases, providing a foundation for richer assessment of T2I models.

</details>
