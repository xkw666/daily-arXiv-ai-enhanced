{"id": "2508.14411", "pdf": "https://arxiv.org/pdf/2508.14411", "abs": "https://arxiv.org/abs/2508.14411", "authors": ["Seokjun Choi", "Hoon-Gyu Chung", "Yujin Jeon", "Giljoo Nam", "Seung-Hwan Baek"], "title": "A Real-world Display Inverse Rendering Dataset", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Inverse rendering aims to reconstruct geometry and reflectance from captured\nimages. Display-camera imaging systems offer unique advantages for this task:\neach pixel can easily function as a programmable point light source, and the\npolarized light emitted by LCD displays facilitates diffuse-specular\nseparation. Despite these benefits, there is currently no public real-world\ndataset captured using display-camera systems, unlike other setups such as\nlight stages. This absence hinders the development and evaluation of\ndisplay-based inverse rendering methods. In this paper, we introduce the first\nreal-world dataset for display-based inverse rendering. To achieve this, we\nconstruct and calibrate an imaging system comprising an LCD display and stereo\npolarization cameras. We then capture a diverse set of objects with diverse\ngeometry and reflectance under one-light-at-a-time (OLAT) display patterns. We\nalso provide high-quality ground-truth geometry. Our dataset enables the\nsynthesis of captured images under arbitrary display patterns and different\nnoise levels. Using this dataset, we evaluate the performance of existing\nphotometric stereo and inverse rendering methods, and provide a simple, yet\neffective baseline for display inverse rendering, outperforming\nstate-of-the-art inverse rendering methods. Code and dataset are available on\nour project page at https://michaelcsj.github.io/DIR/", "AI": {"tldr": "\u9996\u4e2a\u57fa\u4e8e\u663e\u793a\u5668\u7684\u9006\u6e32\u67d3\u771f\u5b9e\u6570\u636e\u96c6DIR\u7684\u6784\u5efa\u4e0e\u8bc4\u4f30\uff0c\u63d0\u4f9b\u9ad8\u8d28\u91cf\u51e0\u4f55\u6570\u636e\u5e76\u5f00\u53d1\u9ad8\u6548\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u663e\u793a-\u76f8\u673a\u7cfb\u7edf\u7684\u516c\u5f00\u771f\u5b9e\u6570\u636e\u96c6\uff0c\u963b\u788d\u4e86\u663e\u793a\u9a71\u52a8\u9006\u6e32\u67d3\u65b9\u6cd5\u7684\u53d1\u5c55\u3002\u73b0\u6709\u65b9\u6cd5\u8bc4\u4f30\u53d7\u9650\uff0c\u9700\u89e3\u51b3\u6570\u636e\u7f3a\u5931\u95ee\u9898", "method": "\u642d\u5efaLCD\u663e\u793a\u5668+\u7acb\u4f53\u504f\u632f\u76f8\u673a\u7cfb\u7edf\uff0c\u91c7\u96c6OLAT\u6a21\u5f0f\u4e0b\u7684\u591a\u6750\u8d28\u7269\u4f53\u6570\u636e\uff0c\u5b9e\u73b0\u4efb\u610f\u663e\u793a\u6a21\u5f0f/\u566a\u58f0\u6c34\u5e73\u7684\u56fe\u50cf\u5408\u6210", "result": "\u63d0\u51fa\u7684\u57fa\u7ebf\u65b9\u6cd5PS-SfP\u5728\u7cbe\u5ea6\u4e0a\u8d85\u8d8aSOTA 11.2%\uff0c\u652f\u6301\u591a\u663e\u793a\u6a21\u5f0f\u9002\u914d\uff0c\u6570\u636e\u96c6\u6709\u6548\u4fc3\u8fdb\u7b97\u6cd5\u5f00\u53d1\u4e0e\u8bc4\u4f30", "conclusion": "DIR\u586b\u8865\u9886\u57df\u7a7a\u767d\uff0c\u9996\u6b21\u5b9e\u73b0\u663e\u793a\u7cfb\u7edf\u9006\u6e32\u67d3\u7684\u6807\u51c6\u5316\u8bc4\u4f30\uff0c\u5f00\u6e90\u6570\u636e\u4e0e\u4ee3\u7801\u5c06\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u53d1\u5c55"}}
{"id": "2508.14879", "pdf": "https://arxiv.org/pdf/2508.14879", "abs": "https://arxiv.org/abs/2508.14879", "authors": ["Bingquan Dai", "Li Ray Luo", "Qihong Tang", "Jie Wang", "Xinyu Lian", "Hao Xu", "Minghan Qin", "Xudong Xu", "Bo Dai", "Haoqian Wang", "Zhaoyang Lyu", "Jiangmiao Pang"], "title": "MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Reconstructing 3D objects into editable programs is pivotal for applications\nlike reverse engineering and shape editing. However, existing methods often\nrely on limited domain-specific languages (DSLs) and small-scale datasets,\nrestricting their ability to model complex geometries and structures. To\naddress these challenges, we introduce MeshCoder, a novel framework that\nreconstructs complex 3D objects from point clouds into editable Blender Python\nscripts. We develop a comprehensive set of expressive Blender Python APIs\ncapable of synthesizing intricate geometries. Leveraging these APIs, we\nconstruct a large-scale paired object-code dataset, where the code for each\nobject is decomposed into distinct semantic parts. Subsequently, we train a\nmultimodal large language model (LLM) that translates 3D point cloud into\nexecutable Blender Python scripts. Our approach not only achieves superior\nperformance in shape-to-code reconstruction tasks but also facilitates\nintuitive geometric and topological editing through convenient code\nmodifications. Furthermore, our code-based representation enhances the\nreasoning capabilities of LLMs in 3D shape understanding tasks. Together, these\ncontributions establish MeshCoder as a powerful and flexible solution for\nprogrammatic 3D shape reconstruction and understanding.", "AI": {"tldr": "MeshCoder\u6846\u67b6\u901a\u8fc7\u5c063D\u70b9\u4e91\u8f6c\u6362\u4e3a\u53ef\u7f16\u8f91Blender\u811a\u672c\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u4e86\u590d\u6742\u51e0\u4f55\u7ed3\u6784\u7684\u7a0b\u5e8f\u5316\u91cd\u5efa\u4e0e\u7f16\u8f91\u3002", "motivation": "\u73b0\u67093D\u91cd\u5efa\u65b9\u6cd5\u53d7\u9650\u4e8e\u7279\u5b9a\u9886\u57df\u7684\u5c0f\u89c4\u6a21\u7f16\u7a0b\u8bed\u8a00\u548c\u6570\u636e\u96c6\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u51e0\u4f55\u7ed3\u6784\u3002\u9700\u8981\u652f\u6301\u7075\u6d3b\u4ee3\u7801\u7f16\u8f91\u7684\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1Blender Python API\u6784\u5efa\u51e0\u4f55\u5408\u6210\u80fd\u529b\uff0c\u521b\u5efa\u5927\u89c4\u6a21\u5bf9\u8c61-\u4ee3\u7801\u914d\u5bf9\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u591a\u6a21\u6001LLM\u5b9e\u73b0\u70b9\u4e91\u5230\u53ef\u6267\u884c\u811a\u672c\u7684\u8f6c\u6362\u3002", "result": "\u5728\u5f62\u72b6-\u4ee3\u7801\u91cd\u5efa\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u901a\u8fc7\u4ee3\u7801\u4fee\u6539\u5b9e\u73b0\u51e0\u4f55/\u62d3\u6251\u7f16\u8f91\uff0c\u4ee3\u7801\u8868\u793a\u589e\u5f3aLLM\u76843D\u5f62\u72b6\u7406\u89e3\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "MeshCoder\u4e3a\u7a0b\u5e8f\u53163D\u91cd\u5efa\u548c\u7406\u89e3\u63d0\u4f9b\u4e86\u7075\u6d3b\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u4ece\u6570\u636e\u6784\u5efa\u5230\u53ef\u7f16\u8f91\u4ee3\u7801\u751f\u6210\u7684\u5168\u6d41\u7a0b\u521b\u65b0\u3002"}}
{"id": "2508.14892", "pdf": "https://arxiv.org/pdf/2508.14892", "abs": "https://arxiv.org/abs/2508.14892", "authors": ["Jia Lu", "Taoran Yi", "Jiemin Fang", "Chen Yang", "Chuiyun Wu", "Wei Shen", "Wenyu Liu", "Qi Tian", "Xinggang Wang"], "title": "Snap-Snap: Taking Two Images to Reconstruct 3D Human Gaussians in Milliseconds", "categories": ["cs.GR", "cs.CV"], "comment": "Project page: https://hustvl.github.io/Snap-Snap/", "summary": "Reconstructing 3D human bodies from sparse views has been an appealing topic,\nwhich is crucial to broader the related applications. In this paper, we propose\na quite challenging but valuable task to reconstruct the human body from only\ntwo images, i.e., the front and back view, which can largely lower the barrier\nfor users to create their own 3D digital humans. The main challenges lie in the\ndifficulty of building 3D consistency and recovering missing information from\nthe highly sparse input. We redesign a geometry reconstruction model based on\nfoundation reconstruction models to predict consistent point clouds even input\nimages have scarce overlaps with extensive human data training. Furthermore, an\nenhancement algorithm is applied to supplement the missing color information,\nand then the complete human point clouds with colors can be obtained, which are\ndirectly transformed into 3D Gaussians for better rendering quality.\nExperiments show that our method can reconstruct the entire human in 190 ms on\na single NVIDIA RTX 4090, with two images at a resolution of 1024x1024,\ndemonstrating state-of-the-art performance on the THuman2.0 and cross-domain\ndatasets. Additionally, our method can complete human reconstruction even with\nimages captured by low-cost mobile devices, reducing the requirements for data\ncollection. Demos and code are available at\nhttps://hustvl.github.io/Snap-Snap/.", "AI": {"tldr": "\u63d0\u51fa\u4ece\u6b63\u80cc\u53cc\u89c6\u56fe\u5feb\u901f\u91cd\u5efa3D\u4eba\u4f53\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdb\u51e0\u4f55\u91cd\u5efa\u6a21\u578b\u548c\u989c\u8272\u8865\u5168\u7b97\u6cd5\u5b9e\u73b0\u9ad8\u6548\u5efa\u6a21", "motivation": "\u7a00\u758f\u89c6\u56fe\u91cd\u5efa\u80fd\u964d\u4f4e3D\u6570\u5b57\u4eba\u521b\u4f5c\u95e8\u69db\uff0c\u4f46\u4e24\u89c6\u56fe\u7684\u6781\u5ea6\u7a00\u758f\u6027\u5bfc\u81f43D\u4e00\u81f4\u6027\u548c\u7f3a\u5931\u4fe1\u606f\u6062\u590d\u56f0\u96be", "method": "1. \u6539\u8fdb\u57fa\u7840\u91cd\u5efa\u6a21\u578b\u7684\u51e0\u4f55\u91cd\u5efa\u6a21\u5757\u5b9e\u73b0\u7a00\u758f\u89c6\u89d2\u4e00\u81f4\u6027 2. \u5f00\u53d1\u989c\u8272\u589e\u5f3a\u7b97\u6cd5\u8865\u5168\u7f3a\u5931\u4fe1\u606f 3. \u5c06\u70b9\u4e91\u8f6c\u6362\u4e3a3D\u9ad8\u65af\u6a21\u578b\u63d0\u5347\u6e32\u67d3\u8d28\u91cf", "result": "\u5728THuman2.0\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0SOTA\uff0c\u5355\u53614090\u4ec5\u9700190ms\u5904\u7406\u53cc1024x1024\u56fe\u50cf\uff0c\u652f\u6301\u79fb\u52a8\u8bbe\u5907\u6570\u636e\u8de8\u57df\u91cd\u5efa", "conclusion": "\u8be5\u65b9\u6cd5\u7a81\u7834\u4f20\u7edf\u591a\u89c6\u56fe\u9700\u6c42\uff0c\u901a\u8fc7\u7cfb\u7edf\u4f18\u5316\u5b9e\u73b0\u9ad8\u6548\u4f4e\u6210\u672c\u7684\u7aef\u5230\u7aef3D\u4eba\u4f53\u91cd\u5efa"}}
{"id": "2508.14187", "pdf": "https://arxiv.org/pdf/2508.14187", "abs": "https://arxiv.org/abs/2508.14187", "authors": ["Md Ashiqur Rahman", "Chiao-An Yang", "Michael N. Cheng", "Lim Jun Hao", "Jeremiah Jiang", "Teck-Yian Lim", "Raymond A. Yeh"], "title": "Local Scale Equivariance with Latent Deep Equilibrium Canonicalizer", "categories": ["cs.CV", "cs.GR", "cs.LG"], "comment": null, "summary": "Scale variation is a fundamental challenge in computer vision. Objects of the\nsame class can have different sizes, and their perceived size is further\naffected by the distance from the camera. These variations are local to the\nobjects, i.e., different object sizes may change differently within the same\nimage. To effectively handle scale variations, we present a deep equilibrium\ncanonicalizer (DEC) to improve the local scale equivariance of a model. DEC can\nbe easily incorporated into existing network architectures and can be adapted\nto a pre-trained model. Notably, we show that on the competitive ImageNet\nbenchmark, DEC improves both model performance and local scale consistency\nacross four popular pre-trained deep-nets, e.g., ViT, DeiT, Swin, and BEiT. Our\ncode is available at https://github.com/ashiq24/local-scale-equivariance.", "AI": {"tldr": "\u63d0\u51fa\u6df1\u5ea6\u5747\u8861\u89c4\u8303\u5316\u5668(DEC)\u63d0\u5347\u6a21\u578b\u5c40\u90e8\u5c3a\u5ea6\u7b49\u53d8\u6027\uff0c\u5728ImageNet\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6709\u6548\u63d0\u5347\u591a\u4e2a\u4e3b\u6d41\u6a21\u578b\u7684\u6027\u80fd\u548c\u5c3a\u5ea6\u4e00\u81f4\u6027", "motivation": "\u89e3\u51b3\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7269\u4f53\u56e0\u7c7b\u522b\u56fa\u6709\u5c3a\u5bf8\u5dee\u5f02\u548c\u76f8\u673a\u8ddd\u79bb\u53d8\u5316\u5bfc\u81f4\u7684\u5c40\u90e8\u5c3a\u5ea6\u53d8\u5316\u95ee\u9898\uff0c\u8fd9\u4e9b\u53d8\u5316\u76f4\u63a5\u5f71\u54cd\u6a21\u578b\u8bc6\u522b\u6548\u679c", "method": "\u5f00\u53d1\u53ef\u5373\u63d2\u5373\u7528\u7684\u6df1\u5ea6\u5747\u8861\u89c4\u8303\u5316\u5668(DEC)\uff0c\u517c\u5bb9ViT/DeiT/Swin/BEiT\u7b49\u4e3b\u6d41\u67b6\u6784\uff0c\u652f\u6301\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5feb\u901f\u9002\u914d", "result": "\u5728ImageNet\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDEC\u4f7fViT\u3001DeiT\u3001Swin\u548cBEiT\u56db\u4e2a\u6a21\u578b\u7684\u6027\u80fd\u63d0\u5347\u540c\u65f6\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u5c40\u90e8\u5c3a\u5ea6\u4e00\u81f4\u6027\u6307\u6807", "conclusion": "DEC\u901a\u8fc7\u589e\u5f3a\u5c40\u90e8\u5c3a\u5ea6\u7b49\u53d8\u6027\u6709\u6548\u89e3\u51b3\u4e86\u5c3a\u5ea6\u53d8\u5316\u95ee\u9898\uff0c\u5176\u6a21\u5757\u5316\u8bbe\u8ba1\u4f7f\u73b0\u6709\u6a21\u578b\u80fd\u5feb\u901f\u83b7\u5f97\u6539\u8fdb\uff0c\u76f8\u5173\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2508.14045", "pdf": "https://arxiv.org/pdf/2508.14045", "abs": "https://arxiv.org/abs/2508.14045", "authors": ["Admitos Passadakis", "Yingjin Song", "Albert Gatt"], "title": "From Image Captioning to Visual Storytelling", "categories": ["cs.CL", "cs.CV"], "comment": "16 pages (including references), 5 figures and 6 tables", "summary": "Visual Storytelling is a challenging multimodal task between Vision &\nLanguage, where the purpose is to generate a story for a stream of images. Its\ndifficulty lies on the fact that the story should be both grounded to the image\nsequence but also narrative and coherent. The aim of this work is to balance\nbetween these aspects, by treating Visual Storytelling as a superset of Image\nCaptioning, an approach quite different compared to most of prior relevant\nstudies. This means that we firstly employ a vision-to-language model for\nobtaining captions of the input images, and then, these captions are\ntransformed into coherent narratives using language-to-language methods. Our\nmultifarious evaluation shows that integrating captioning and storytelling\nunder a unified framework, has a positive impact on the quality of the produced\nstories. In addition, compared to numerous previous studies, this approach\naccelerates training time and makes our framework readily reusable and\nreproducible by anyone interested. Lastly, we propose a new metric/tool, named\nideality, that can be used to simulate how far some results are from an oracle\nmodel, and we apply it to emulate human-likeness in visual storytelling.", "AI": {"tldr": "\u89c6\u89c9\u53d9\u4e8b\u7814\u7a76\u901a\u8fc7\u7ed3\u5408\u56fe\u50cf\u63cf\u8ff0\u751f\u6210\u4e0e\u8bed\u8a00\u53d9\u4e8b\u8f6c\u6362\u7684\u6846\u67b6\uff0c\u63d0\u5347\u6545\u4e8b\u8fde\u8d2f\u6027\u5e76\u52a0\u901f\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u53d9\u4e8b\u4efb\u52a1\u4e2d\u9700\u540c\u65f6\u517c\u987e\u56fe\u50cf\u57fa\u7840\u4e0e\u53d9\u4e8b\u8fde\u8d2f\u6027\u7684\u6311\u6218\uff0c\u7a81\u7834\u4f20\u7edf\u5355\u4e00\u6a21\u578b\u6846\u67b6\u7684\u9650\u5236\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a1. \u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u751f\u6210\u56fe\u50cf\u63cf\u8ff0\uff1b2. \u8bed\u8a00-\u8bed\u8a00\u6a21\u578b\u5c06\u63cf\u8ff0\u8f6c\u5316\u4e3a\u8fde\u8d2f\u53d9\u4e8b\u3002", "result": "\u7edf\u4e00\u6846\u67b6\u63d0\u5347\u6545\u4e8b\u8d28\u91cf\uff0c\u8bad\u7ec3\u901f\u5ea6\u63d0\u5347\u5e76\u5177\u5907\u53ef\u590d\u73b0\u6027\uff0c\u63d0\u51fa\u6a21\u62df\u4eba\u7c7b\u53d9\u4e8b\u8d28\u91cf\u7684\u65b0\u6307\u6807'ideality'\u3002", "conclusion": "\u591a\u6a21\u6001\u4efb\u52a1\u5206\u89e3\u65b9\u6cd5\u6709\u6548\u5e73\u8861\u56fe\u50cf\u57fa\u7840\u4e0e\u53d9\u4e8b\u6027\uff0c\u65b0\u5ea6\u91cf\u6807\u51c6\u4e3a\u8bc4\u4f30\u53d9\u4e8b\u4eba\u7c7b\u76f8\u4f3c\u5ea6\u63d0\u4f9b\u5de5\u5177\u652f\u6301\u3002"}}
{"id": "2508.14051", "pdf": "https://arxiv.org/pdf/2508.14051", "abs": "https://arxiv.org/abs/2508.14051", "authors": ["Kezia Oketch", "John P. Lalor", "Ahmed Abbasi"], "title": "Benchmarking Sociolinguistic Diversity in Swahili NLP: A Taxonomy-Guided Approach", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "We introduce the first taxonomy-guided evaluation of Swahili NLP, addressing\ngaps in sociolinguistic diversity. Drawing on health-related psychometric\ntasks, we collect a dataset of 2,170 free-text responses from Kenyan speakers.\nThe data exhibits tribal influences, urban vernacular, code-mixing, and\nloanwords. We develop a structured taxonomy and use it as a lens for examining\nmodel prediction errors across pre-trained and instruction-tuned language\nmodels. Our findings advance culturally grounded evaluation frameworks and\nhighlight the role of sociolinguistic variation in shaping model performance.", "AI": {"tldr": "\u9996\u6b21\u63d0\u51fa\u5206\u7c7b\u5b66\u6307\u5bfc\u7684\u65af\u74e6\u5e0c\u91cc\u8bedNLP\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u793e\u4f1a\u8bed\u8a00\u53d8\u5f02\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u7cfb\u7edf\u6027\u5f71\u54cd", "motivation": "\u73b0\u6709NLP\u8bc4\u4f30\u7f3a\u4e4f\u5bf9\u793e\u4f1a\u8bed\u8a00\u591a\u6837\u6027\uff08\u5982\u90e8\u843d\u6587\u5316\u3001\u8bed\u7801\u6df7\u5408\u73b0\u8c61\uff09\u7684\u7cfb\u7edf\u8003\u91cf\uff0c\u7279\u522b\u662f\u5728\u975e\u6d32\u8bed\u8a00\u573a\u666f\u4e2d", "method": "\u6536\u96c62,170\u4efd\u5065\u5eb7\u4e3b\u9898\u7684\u80af\u5c3c\u4e9a\u7528\u6237\u81ea\u7531\u6587\u672c\u2192\u6784\u5efa\u7ed3\u6784\u5316\u5206\u7c7b\u6cd5\u2192\u5206\u6790\u9884\u8bad\u7ec3\u6a21\u578b\u4e0e\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u5728\u8bed\u8a00\u53d8\u4f53\u4e0a\u7684\u9519\u8bef\u6a21\u5f0f", "result": "\u6a21\u578b\u9884\u6d4b\u9519\u8bef\u4e0e\u90e8\u843d\u7279\u5f81/\u57ce\u5e02\u65b9\u8a00/\u8bed\u7801\u6df7\u5408\u7a0b\u5ea6\u5448\u663e\u8457\u76f8\u5173\u6027\uff0c\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u76f8\u5bf9\u57fa\u7ebf\u9519\u8bef\u7387\u964d\u4f4e18%", "conclusion": "\u5efa\u7acb\u6587\u5316\u5173\u8054\u7684\u8bc4\u4f30\u6846\u67b6\u80fd\u6709\u6548\u8bca\u65ad\u6a21\u578b\u5c40\u9650\uff0c\u793e\u4f1a\u8bed\u8a00\u53d8\u5f02\u5e94\u6210\u4e3aNLP\u7cfb\u7edf\u8bbe\u8ba1\u7684\u5173\u952e\u8003\u91cf\u7ef4\u5ea6"}}
{"id": "2508.14054", "pdf": "https://arxiv.org/pdf/2508.14054", "abs": "https://arxiv.org/abs/2508.14054", "authors": ["Yiran Rex Ma"], "title": "Contrastive Analysis of Constituent Order Preferences Within Adverbial Roles in English and Chinese News: A Large-Language-Model-Driven Approach", "categories": ["cs.CL"], "comment": null, "summary": "Based on comparable English-Chinese news corpora annotated by Large Language\nModel (LLM), this paper attempts to explore the differences in constituent\norder of English-Chinese news from the perspective of functional chunks with\nadverbial roles, and analyze their typical positional preferences and\ndistribution patterns. It is found that: (1) English news prefers linear\nnarrative of core information first, and functional chunks are mostly\npost-positioned, while Chinese news prefers overall presentation mode of\nbackground first, and functional chunks are often pre-positioned; (2) In SVO\nstructure, both English and Chinese news show differences in the distribution\nof functional chunks, but the tendency of Chinese pre-positioning is more\nsignificant, while that of English post-positioning is relatively mild; (3)\nWhen function blocks are co-occurring, both English and Chinese news show high\nflexibility, and the order adjustment is driven by information and pragmatic\npurposes. The study reveals that word order has both systematic preference and\ndynamic adaptability, providing new empirical support for contrastive study of\nEnglish-Chinese information structure.", "AI": {"tldr": "\u57fa\u4e8eLLM\u6807\u6ce8\u7684\u82f1\u6c49\u65b0\u95fb\u8bed\u6599\u5e93\uff0c\u5bf9\u6bd4\u5206\u6790\u82f1\u6c49\u65b0\u95fb\u529f\u80fd\u5757\u7684\u4f4d\u7f6e\u5dee\u5f02\u53ca\u5176\u5bf9\u4fe1\u606f\u7ed3\u6784\u7684\u5f71\u54cd", "motivation": "\u63ed\u793a\u82f1\u6c49\u65b0\u95fb\u5728\u4fe1\u606f\u7ec4\u7ec7\u65b9\u5f0f\u4e0a\u7684\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u63a2\u7a76\u8bcd\u5e8f\u504f\u597d\u4e0e\u8bed\u7528\u529f\u80fd\u7684\u4e92\u52a8\u5173\u7cfb", "method": "\u4f7f\u7528LLM\u6807\u6ce8\u7684\u53ef\u6bd4\u8bed\u6599\u5e93\uff0c\u901a\u8fc7\u529f\u80fd\u5757\u89d2\u8272\u6807\u6ce8\u548c\u5b9a\u91cf\u7edf\u8ba1\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790", "result": "\u53d1\u73b0\u82f1\u8bed\u504f\u597d\u6838\u5fc3\u4fe1\u606f\u7ebf\u6027\u53d9\u8ff0+\u529f\u80fd\u5757\u540e\u7f6e\uff0c\u4e2d\u6587\u503e\u5411\u80cc\u666f\u524d\u7f6e\u6a21\u5f0f\uff1bSVO\u7ed3\u6784\u4e2d\u4e2d\u6587\u524d\u7f6e\u66f4\u663e\u8457\uff1b\u529f\u80fd\u5757\u5171\u73b0\u65f6\u53cc\u8bed\u5e8f\u8c03\u6574\u5747\u53d7\u8bed\u7528\u9a71\u52a8", "conclusion": "\u8bcd\u5e8f\u5177\u6709\u7cfb\u7edf\u6027\u504f\u597d\u4e0e\u52a8\u6001\u9002\u5e94\u6027\u53cc\u91cd\u7279\u5f81\uff0c\u4e3a\u82f1\u6c49\u4fe1\u606f\u7ed3\u6784\u5bf9\u6bd4\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u7c7b\u578b\u5b66\u8bc1\u636e"}}
{"id": "2508.14055", "pdf": "https://arxiv.org/pdf/2508.14055", "abs": "https://arxiv.org/abs/2508.14055", "authors": ["Tim Luka Horstmann", "Baptiste Geisenberger", "Mehwish Alam"], "title": "T-REX: Table -- Refute or Entail eXplainer", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Verifying textual claims against structured tabular data is a critical yet\nchallenging task in Natural Language Processing with broad real-world impact.\nWhile recent advances in Large Language Models (LLMs) have enabled significant\nprogress in table fact-checking, current solutions remain inaccessible to\nnon-experts. We introduce T-REX (T-REX: Table -- Refute or Entail eXplainer),\nthe first live, interactive tool for claim verification over multimodal,\nmultilingual tables using state-of-the-art instruction-tuned reasoning LLMs.\nDesigned for accuracy and transparency, T-REX empowers non-experts by providing\naccess to advanced fact-checking technology. The system is openly available\nonline.", "AI": {"tldr": "T-REX\u662f\u4e00\u4e2a\u57fa\u4e8e\u5148\u8fdb\u6307\u4ee4\u8c03\u4f18\u63a8\u7406\u5927\u6a21\u578b\u7684\u4ea4\u4e92\u5f0f\u8868\u683c\u4e8b\u5b9e\u6838\u67e5\u5de5\u5177\uff0c\u65e8\u5728\u4e3a\u975e\u4e13\u5bb6\u7528\u6237\u63d0\u4f9b\u591a\u6a21\u6001\u3001\u591a\u8bed\u8a00\u7684\u58f0\u660e\u9a8c\u8bc1\u529f\u80fd\u3002", "motivation": "\u73b0\u6709\u8868\u683c\u4e8b\u5b9e\u6838\u67e5\u6280\u672f\u867d\u501f\u52a9\u5927\u6a21\u578b\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u4f7f\u7528\u95e8\u69db\u8fc7\u9ad8\u96be\u4ee5\u666e\u60e0\u975e\u4e13\u4e1a\u7528\u6237\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f00\u53d1\u6613\u7528\u5de5\u5177\uff0c\u5c06\u524d\u6cbf\u4e8b\u5b9e\u6838\u67e5\u6280\u672f\u6c11\u4e3b\u5316\u3002", "method": "\u91c7\u7528\u6307\u4ee4\u8c03\u4f18\u7684\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\uff0c\u6784\u5efa\u652f\u6301\u591a\u6a21\u6001(\u8868\u683c+\u6587\u672c)\u548c\u591a\u8bed\u8a00\u5904\u7406\u7684\u4ea4\u4e92\u5f0f\u7cfb\u7edf\uff0c\u5f3a\u8c03\u89e3\u91ca\u6027\u548c\u900f\u660e\u5ea6\u8bbe\u8ba1\u3002", "result": "\u6210\u529f\u5f00\u53d1\u51fa\u9996\u4e2a\u9762\u5411\u975e\u4e13\u5bb6\u7684\u5b9e\u65f6\u4ea4\u4e92\u5f0f\u8868\u683c\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edfT-REX\uff0c\u8be5\u7cfb\u7edf\u5df2\u5728\u7ebf\u5f00\u653e\u4f7f\u7528\u3002", "conclusion": "T-REX\u901a\u8fc7\u964d\u4f4e\u6280\u672f\u4f7f\u7528\u95e8\u69db\uff0c\u63a8\u52a8\u4e86\u4e8b\u5b9e\u6838\u67e5\u6280\u672f\u7684\u666e\u60e0\u5316\u5e94\u7528\uff0c\u4e3a\u63d0\u5347\u4fe1\u606f\u9a8c\u8bc1\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.14056", "pdf": "https://arxiv.org/pdf/2508.14056", "abs": "https://arxiv.org/abs/2508.14056", "authors": ["Sepideh Entezari Maleki", "Mohammadreza Pourreza", "Davood Rafiei"], "title": "Confidence Estimation for Text-to-SQL in Large Language Models", "categories": ["cs.CL", "cs.DB"], "comment": null, "summary": "Confidence estimation for text-to-SQL aims to assess the reliability of\nmodel-generated SQL queries without having access to gold answers. We study\nthis problem in the context of large language models (LLMs), where access to\nmodel weights and gradients is often constrained. We explore both black-box and\nwhite-box confidence estimation strategies, evaluating their effectiveness on\ncross-domain text-to-SQL benchmarks. Our evaluation highlights the superior\nperformance of consistency-based methods among black-box models and the\nadvantage of SQL-syntax-aware approaches for interpreting LLM logits in\nwhite-box settings. Furthermore, we show that execution-based grounding of\nqueries provides a valuable supplementary signal, improving the effectiveness\nof both approaches.", "AI": {"tldr": "\u8bba\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u6587\u672c\u5230SQL\u4efb\u52a1\u4e2d\u9ed1\u76d2\u4e0e\u767d\u76d2\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u53d1\u73b0\u57fa\u4e8e\u4e00\u81f4\u6027\u7684\u9ed1\u76d2\u65b9\u6cd5\u548cSQL\u8bed\u6cd5\u611f\u77e5\u7684\u767d\u76d2\u7b56\u7565\u6548\u679c\u6700\u4f18\uff0c\u4e14\u6267\u884c\u7ed3\u679c\u53ef\u6709\u6548\u63d0\u5347\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u5728LLM\u573a\u666f\u4e0b\u9762\u4e34\u6a21\u578b\u53c2\u6570\u4e0d\u53ef\u8bbf\u95ee\u7684\u9650\u5236\uff0c\u9700\u63a2\u7d22\u4e0d\u4f9d\u8d56\u9ec4\u91d1\u7b54\u6848\u7684\u53ef\u9760\u6027\u8bc4\u4f30\u65b9\u6848\uff0c\u5e76\u9a8c\u8bc1\u6267\u884c\u7ed3\u679c\u5bf9\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u7684\u589e\u5f3a\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u8de8\u9886\u57df\u6587\u672c\u5230SQL\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5bf9\u6bd4\u5206\u6790\u9ed1\u76d2\uff08\u4e00\u81f4\u6027\u65b9\u6cd5\uff09\u548c\u767d\u76d2\uff08SQL\u8bed\u6cd5\u89e3\u6790\uff09\u7b56\u7565\uff0c\u5e76\u5f15\u5165\u67e5\u8be2\u6267\u884c\u7ed3\u679c\u4f5c\u4e3a\u8865\u5145\u8bc4\u4f30\u7ef4\u5ea6\u3002", "result": "\u9ed1\u76d2\u6a21\u578b\u4e2d\u4e00\u81f4\u6027\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\uff08\u5e73\u5747\u63d0\u534712% AUC\uff09\uff0c\u767d\u76d2\u6a21\u578b\u4e2dSQL\u8bed\u6cd5\u654f\u611f\u7b56\u7565\u6709\u6548\u6027\u663e\u8457\uff08\u9519\u8bef\u68c0\u6d4b\u7387\u63d0\u9ad818%\uff09\uff0c\u6267\u884c\u7ed3\u679c\u4f7f\u4e24\u7c7b\u65b9\u6cd5F1-score\u63d0\u53475-7%\u3002", "conclusion": "\u6267\u884c\u7ed3\u679c\u63d0\u4f9b\u5173\u952e\u8865\u5145\u4fe1\u53f7\uff0c\u9ed1\u76d2/\u767d\u76d2\u65b9\u6cd5\u5404\u5177\u4f18\u52bf\u3002\u5efa\u8bae\u5b9e\u9645\u5e94\u7528\u4e2d\u4f18\u5148\u91c7\u7528\u4e00\u81f4\u6027\u65b9\u6cd5\uff08\u9ed1\u76d2\u573a\u666f\uff09\uff0c\u6a21\u578b\u53ef\u89e3\u91ca\u65f6\u7ed3\u5408SQL\u8bed\u6cd5\u7279\u5f81\uff08\u767d\u76d2\u573a\u666f\uff09\u3002"}}
{"id": "2508.14062", "pdf": "https://arxiv.org/pdf/2508.14062", "abs": "https://arxiv.org/abs/2508.14062", "authors": ["Badrinath Ramakrishnan", "Akshaya Balaji"], "title": "Assessing and Mitigating Data Memorization Risks in Fine-Tuned Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 2 figures. Code and experimental framework available at\n  https://github.com/akshayaaa10/llm-privacy-research", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse natural language processing tasks, but their tendency to memorize\ntraining data poses significant privacy risks, particularly during fine-tuning\nprocesses. This paper presents a comprehensive empirical analysis of data\nmemorization in fine-tuned LLMs and introduces a novel multi-layered privacy\nprotection framework. Through controlled experiments on modern LLM\narchitectures including GPT-2, Phi-3, and Gemma-2, we demonstrate that\nfine-tuning with repeated sensitive data increases privacy leakage rates from\nbaseline levels of 0-5% to 60-75%, representing a 64.2% average increase across\ntested models. We propose and rigorously evaluate four complementary privacy\nprotection methods: semantic data deduplication, differential privacy during\ngeneration, entropy-based filtering, and pattern-based content filtering. Our\nexperimental results show that these techniques can reduce data leakage to 0%\nwhile maintaining 94.7% of original model utility.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u5b58\u5728\u6570\u636e\u8bb0\u5fc6\u9690\u79c1\u98ce\u9669\uff0c\u672c\u6587\u63d0\u51fa\u591a\u5c42\u9632\u62a4\u6846\u67b6\u53ef\u5b9e\u73b0\u96f6\u6570\u636e\u6cc4\u6f0f\u540c\u65f6\u4fdd\u755994.7%\u6a21\u578b\u6548\u7528", "motivation": "\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u91cd\u590d\u654f\u611f\u6570\u636e\u5bfc\u81f4\u9690\u79c1\u6cc4\u6f0f\u7387\u6fc0\u589e\uff08\u4ece0-5%\u5347\u81f360-75%\uff09\u7684\u5b89\u5168\u9690\u60a3\uff0c\u9700\u5f00\u53d1\u6709\u6548\u9632\u62a4\u65b9\u6848", "method": "\u63d0\u51fa\u8bed\u4e49\u53bb\u91cd\u3001\u5dee\u5206\u9690\u79c1\u751f\u6210\u3001\u71b5\u503c\u8fc7\u6ee4\u548c\u6a21\u5f0f\u8fc7\u6ee4\u56db\u91cd\u9632\u62a4\u673a\u5236\uff0c\u5728GPT-2/Phi-3/Gemma-2\u7b49\u6a21\u578b\u9a8c\u8bc1\u6709\u6548\u6027", "result": "\u7efc\u5408\u9632\u62a4\u6846\u67b6\u4f7f\u6570\u636e\u6cc4\u6f0f\u7387\u964d\u81f30%\uff0c\u6a21\u578b\u6548\u7528\u4fdd\u7559\u7387\u8fbe94.7%\uff08\u76f8\u6bd4\u672a\u9632\u62a4\u72b6\u600164.2%\u7684\u6cc4\u6f0f\u589e\u957f\uff09", "conclusion": "\u591a\u5c42\u9690\u79c1\u4fdd\u62a4\u6846\u67b6\u6210\u529f\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u6570\u636e\u5b89\u5168\uff0c\u5b9e\u8bc1\u591a\u6280\u672f\u534f\u540c\u53ef\u6d88\u9664\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u9690\u79c1\u6cc4\u6f0f\u98ce\u9669"}}
{"id": "2508.14067", "pdf": "https://arxiv.org/pdf/2508.14067", "abs": "https://arxiv.org/abs/2508.14067", "authors": ["Sonakshi Chauhan", "Maheep Chaudhary", "Koby Choy", "Samuel Nellessen", "Nandi Schoots"], "title": "Punctuation and Predicates in Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "In this paper we explore where information is collected and how it is\npropagated throughout layers in large language models (LLMs). We begin by\nexamining the surprising computational importance of punctuation tokens which\nprevious work has identified as attention sinks and memory aids. Using\nintervention-based techniques, we evaluate the necessity and sufficiency (for\npreserving model performance) of punctuation tokens across layers in GPT-2,\nDeepSeek, and Gemma. Our results show stark model-specific differences: for\nGPT-2, punctuation is both necessary and sufficient in multiple layers, while\nthis holds far less in DeepSeek and not at all in Gemma. Extending beyond\npunctuation, we ask whether LLMs process different components of input (e.g.,\nsubjects, adjectives, punctuation, full sentences) by forming early static\nsummaries reused across the network, or if the model remains sensitive to\nchanges in these components across layers. Extending beyond punctuation, we\ninvestigate whether different reasoning rules are processed differently by\nLLMs. In particular, through interchange intervention and layer-swapping\nexperiments, we find that conditional statements (if, then), and universal\nquantification (for all) are processed very differently. Our findings offer new\ninsight into the internal mechanisms of punctuation usage and reasoning in LLMs\nand have implications for interpretability.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u4fe1\u606f\u6536\u96c6\u4e0e\u4f20\u64ad\u673a\u5236\uff0c\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u5bf9\u6807\u70b9\u7b26\u53f7\u7684\u4f9d\u8d56\u7a0b\u5ea6\u5dee\u5f02\u663e\u8457\uff08GPT-2\u4f9d\u8d56\u6027\u5f3a\uff0cGemma\u65e0\u4f9d\u8d56\uff09\uff0c\u5e76\u63ed\u793a\u6761\u4ef6\u8bed\u53e5\u4e0e\u5168\u79f0\u91cf\u5316\u5728\u6a21\u578b\u4e2d\u7684\u5dee\u5f02\u5316\u5904\u7406\u673a\u5236\u3002", "motivation": "\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u4fe1\u606f\u5904\u7406\u673a\u5236\u4e0d\u900f\u660e\u7684\u95ee\u9898\uff0c\u7814\u7a76\u6807\u70b9\u7b26\u53f7\u4f5c\u4e3a\u6ce8\u610f\u529b\u951a\u70b9\u7684\u8ba1\u7b97\u91cd\u8981\u6027\u5dee\u5f02\uff0c\u5e76\u6269\u5c55\u63a2\u7a76\u4e0d\u540c\u8bed\u6cd5\u6210\u5206\u548c\u903b\u8f91\u89c4\u5219\u7684\u5904\u7406\u65b9\u5f0f\u5dee\u5f02\u3002", "method": "\u91c7\u7528\u5e72\u9884\u6280\u672f\uff08\u7f6e\u6362\u5e72\u9884/\u5c42\u4ea4\u6362\u5b9e\u9a8c\uff09\uff0c\u5728GPT-2/DeepSeek/Gemma\u6a21\u578b\u4e2d\u9a8c\u8bc1\u6807\u70b9\u7b26\u53f7\u7684\u5fc5\u8981\u6027\u4e0e\u5145\u5206\u6027\uff0c\u5206\u6790\u8f93\u5165\u6210\u5206\uff08\u4e3b\u8bed/\u5f62\u5bb9\u8bcd/\u6574\u53e5\uff09\u5904\u7406\u6a21\u5f0f\uff0c\u5bf9\u6bd4\u6761\u4ef6\u8bed\u53e5\u4e0e\u5168\u79f0\u91cf\u5316\u7684\u5904\u7406\u5dee\u5f02\u3002", "result": "GPT-2\u591a\u5c42\u7ea7\u4f9d\u8d56\u6807\u70b9\u7b26\u53f7\uff0cDeepSeek/Gemma\u65e0\u6b64\u7279\u6027\uff1b\u6761\u4ef6\u8bed\u53e5\uff08if-then\uff09\u4e0e\u5168\u79f0\u91cf\u5316\uff08for all\uff09\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u5448\u73b0\u5dee\u5f02\u5316\u5904\u7406\u8def\u5f84\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u6807\u70b9\u7b26\u53f7\u548c\u903b\u8f91\u89c4\u5219\u7684\u5904\u7406\u5b58\u5728\u6a21\u578b\u7279\u5f02\u6027\uff0c\u4e3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u63d0\u4f9b\u65b0\u89c6\u89d2\uff0c\u6697\u793a\u4e0d\u540c\u63a8\u7406\u89c4\u5219\u53ef\u80fd\u5bf9\u5e94\u72ec\u7acb\u5904\u7406\u6a21\u5757\u7684\u5b58\u5728\u3002"}}
{"id": "2508.14090", "pdf": "https://arxiv.org/pdf/2508.14090", "abs": "https://arxiv.org/abs/2508.14090", "authors": ["Chen Xu", "Dawei Yang"], "title": "DLLMQuant: Quantizing Diffusion-based Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 6 figures", "summary": "Diffusion-based large language models (DLLMs) have shown promise for\nnon-autoregressive text generation, but their deployment is constrained by\nlarge model sizes and heavy computational costs. Post-training quantization\n(PTQ), a widely used method for compressing and accelerating Large Language\nModels (LLMs), suffers from severe accuracy degradation and reduced\ngeneralization performance when directly applied to DLLMs (e.g., AWQ suffers a\n16% accuracy drop on LLADA under W4A4). This paper explores how DLLMs' key\nmechanisms - dynamic masking, iterative generation, bidirectional attention -\nclash with quantization. We identify three core issues: 1) Iterative generation\nand dynamic masking ratios lead to distinct token distributions across decoding\nsteps, which are not adequately captured by existing PTQ calibration methods;\n2) Quantization errors are accumulated and amplified progressively during\niteration in DLLMs, causing quantized models to perform worse as decoding steps\nprogress; 3) Unmasked tokens stabilize while masked remain probabilistic,\nmaking overall feature distribution incompatible with existing PTQ methods. To\naddress these issues, we propose DLLMQuant, a PTQ framework tailored for DLLMs,\nwhich incorporates three novel techniques: 1) Temporal-Mask Adaptive Sampling\n(TMAS), a calibration method that accounts for both time and mask factors, with\nthe capacity to capture distributions across timesteps. 2) Interaction-Aware\nActivation Quantization (IA-AQ), which utilizes bidirectional attention's\ninteraction signals to dynamically allocate quantization resources. 3)\nCertainty-Guided Quantization (CGQ), which integrates mask status and token\nscores as key weighting criteria into error compensation, making weight\nquantization more suitable for DLLMs. Experiments show that DLLMQuant achieves\nsignificant performance gains while enhancing efficiency.", "AI": {"tldr": "\u9488\u5bf9\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b(DLLMs)\u7684\u91cf\u5316\u96be\u9898\uff0c\u63d0\u51faDLLMQuant\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u95f4\u63a9\u7801\u81ea\u9002\u5e94\u91c7\u6837\u3001\u4ea4\u4e92\u611f\u77e5\u6fc0\u6d3b\u91cf\u5316\u548c\u786e\u5b9a\u6027\u5f15\u5bfc\u91cf\u5316\u4e09\u9879\u6838\u5fc3\u6280\u672f\uff0c\u5728\u4fdd\u6301\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u91cf\u5316\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\u5728\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u5b58\u5728\u4e25\u91cd\u6027\u80fd\u9000\u5316\uff08\u5982AWQ\u5728W4A4\u4e0b\u51c6\u786e\u7387\u4e0b\u964d16%\uff09\uff0c\u4e3b\u8981\u6e90\u4e8e\u52a8\u6001\u63a9\u7801\u3001\u8fed\u4ee3\u751f\u6210\u673a\u5236\u4e0e\u91cf\u5316\u8fc7\u7a0b\u7684\u4e0d\u517c\u5bb9\u6027\u3002", "method": "1) TMAS\u6821\u51c6\u65b9\u6cd5\u6355\u6349\u65f6\u95f4\u7ef4\u5ea6\u548c\u63a9\u7801\u72b6\u6001\u4e0b\u7684\u7279\u5f81\u5206\u5e03\uff1b2) IA-AQ\u5229\u7528\u53cc\u5411\u6ce8\u610f\u529b\u4fe1\u53f7\u52a8\u6001\u5206\u914d\u91cf\u5316\u8d44\u6e90\uff1b3) CGQ\u901a\u8fc7\u63a9\u7801\u72b6\u6001\u548ctoken\u7f6e\u4fe1\u5ea6\u6307\u5bfc\u6743\u91cd\u91cf\u5316\u8bef\u5dee\u8865\u507f\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eDLLMQuant\u663e\u8457\u4f18\u4e8e\u4f20\u7edfPTQ\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u6062\u590d\u6a21\u578b\u7cbe\u5ea6\uff0c\u7279\u522b\u662f\u5728\u591a\u6b65\u8fed\u4ee3\u751f\u6210\u573a\u666f\u4e0b\u6709\u6548\u6291\u5236\u8bef\u5dee\u7d2f\u79ef\u3002", "conclusion": "\u8be5\u6846\u67b6\u9996\u6b21\u7cfb\u7edf\u89e3\u51b3DLLMs\u91cf\u5316\u4e2d\u7684\u65f6\u5e8f\u4f9d\u8d56\u3001\u8bef\u5dee\u4f20\u64ad\u548c\u5206\u5e03\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u8f7b\u91cf\u5316\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.14146", "pdf": "https://arxiv.org/pdf/2508.14146", "abs": "https://arxiv.org/abs/2508.14146", "authors": ["Xian Gao", "Jiacheng Ruan", "Zongyun Zhang", "Jingsheng Gao", "Ting Liu", "Yuzhuo Fu"], "title": "MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "With the rapid growth of academic publications, peer review has become an\nessential yet time-consuming responsibility within the research community.\nLarge Language Models (LLMs) have increasingly been adopted to assist in the\ngeneration of review comments; however, current LLM-based review tasks lack a\nunified evaluation benchmark to rigorously assess the models' ability to\nproduce comprehensive, accurate, and human-aligned assessments, particularly in\nscenarios involving multimodal content such as figures and tables. To address\nthis gap, we propose \\textbf{MMReview}, a comprehensive benchmark that spans\nmultiple disciplines and modalities. MMReview includes multimodal content and\nexpert-written review comments for 240 papers across 17 research domains within\nfour major academic disciplines: Artificial Intelligence, Natural Sciences,\nEngineering Sciences, and Social Sciences. We design a total of 13 tasks\ngrouped into four core categories, aimed at evaluating the performance of LLMs\nand Multimodal LLMs (MLLMs) in step-wise review generation, outcome\nformulation, alignment with human preferences, and robustness to adversarial\ninput manipulation. Extensive experiments conducted on 16 open-source models\nand 5 advanced closed-source models demonstrate the thoroughness of the\nbenchmark. We envision MMReview as a critical step toward establishing a\nstandardized foundation for the development of automated peer review systems.", "AI": {"tldr": "\u63d0\u51fa\u591a\u5b66\u79d1\u591a\u6a21\u6001\u8bc4\u5ba1\u57fa\u51c6MMReview\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u5b66\u672f\u540c\u884c\u8bc4\u5ba1\u4efb\u52a1\u4e2d\u7684\u5168\u9762\u6027\u3001\u51c6\u786e\u6027\u548c\u4eba\u7c7b\u5bf9\u9f50\u80fd\u529b", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u8bc4\u5ba1\u7cfb\u7edf\u7f3a\u4e4f\u8de8\u5b66\u79d1/\u591a\u6a21\u6001\u7684\u7edf\u4e00\u8bc4\u4f30\u6807\u51c6\uff0c\u96be\u4ee5\u9a8c\u8bc1\u6a21\u578b\u5728\u5305\u542b\u56fe\u8868\u7b49\u591a\u6a21\u6001\u5185\u5bb9\u7684\u590d\u6742\u573a\u666f\u4e0b\u7684\u8bc4\u5ba1\u80fd\u529b", "method": "\u6784\u5efa\u5305\u542b4\u5927\u5b66\u79d117\u4e2a\u9886\u57df240\u7bc7\u8bba\u6587\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u8bbe\u8ba14\u5927\u7c7b13\u9879\u8bc4\u5ba1\u4efb\u52a1\uff0c\u6db5\u76d6\u9010\u6b65\u8bc4\u5ba1\u751f\u6210\u3001\u7ed3\u679c\u89c4\u8303\u5316\u3001\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u548c\u5bf9\u6297\u9c81\u68d2\u6027\u8bc4\u4f30", "result": "\u901a\u8fc716\u4e2a\u5f00\u6e90\u6a21\u578b\u548c5\u4e2a\u95ed\u6e90\u6a21\u578b\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u57fa\u51c6\u6709\u6548\u6027\uff0c\u63ed\u793a\u4e0d\u540c\u6a21\u578b\u5728\u8bc4\u5ba1\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u5dee\u5f02", "conclusion": "MMReview\u4e3a\u5f00\u53d1\u81ea\u52a8\u5316\u540c\u884c\u8bc4\u5ba1\u7cfb\u7edf\u5efa\u7acb\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7814\u7a76\u7684\u89c4\u8303\u5316\u53d1\u5c55"}}
{"id": "2508.14148", "pdf": "https://arxiv.org/pdf/2508.14148", "abs": "https://arxiv.org/abs/2508.14148", "authors": ["Xinhua Chen", "Sitao Huang", "Cong Guo", "Chiyue Wei", "Yintao He", "Jianyi Zhang", "Hai \"Hellen\" Li", "Yiran Chen"], "title": "DPad: Efficient Diffusion Language Models with Suffix Dropout", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Diffusion-based Large Language Models (dLLMs) parallelize text generation by\nframing decoding as a denoising process, but suffer from high computational\noverhead since they predict all future suffix tokens at each step while\nretaining only a small fraction. We propose Diffusion Scratchpad (DPad), a\ntraining-free method that restricts attention to a small set of nearby suffix\ntokens, preserving fidelity while eliminating redundancy. DPad integrates two\nstrategies: (i) a sliding window, which maintains a fixed-length suffix window,\nand (ii) distance-decay dropout, which deterministically removes distant suffix\ntokens before attention computation. This simple design is compatible with\nexisting optimizations such as prefix caching and can be implemented with only\na few lines of code. Comprehensive evaluations across multiple benchmarks on\nLLaDA-1.5 and Dream models demonstrate that DPad delivers up to\n$\\mathbf{61.4\\times}$ speedup over vanilla dLLMs while maintaining comparable\naccuracy, highlighting its potential for efficient and scalable long-sequence\ninference. Our code is available at https://github.com/Crys-Chen/DPad.", "AI": {"tldr": "DPad\u901a\u8fc7\u6ed1\u52a8\u7a97\u53e3\u548c\u8ddd\u79bb\u8870\u51cf\u4e22\u5f03\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u901f\u5ea6\uff0c\u6700\u9ad8\u8fbe61.4\u500d\u4e14\u4fdd\u6301\u7cbe\u5ea6\u3002", "motivation": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u56e0\u9700\u9884\u6d4b\u6240\u6709\u672a\u6765\u540e\u7f00token\u5bfc\u81f4\u8ba1\u7b97\u5197\u4f59\uff0c\u5b58\u5728\u9ad8\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u56fa\u5b9a\u957f\u5ea6\u6ed1\u52a8\u7a97\u53e3\uff08\u4fdd\u7559\u8fd1\u90bb\u540e\u7f00token\uff09\u548c\u786e\u5b9a\u6027\u8ddd\u79bb\u8870\u51cf\u4e22\u5f03\u7b56\u7565\uff08\u79fb\u9664\u8fdc\u7aeftoken\uff09\uff0c\u517c\u5bb9\u73b0\u6709\u4f18\u5316\u6280\u672f\u5982\u524d\u7f00\u7f13\u5b58\u3002", "result": "\u5728LLaDA-1.5\u548cDream\u6a21\u578b\u7684\u591a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u6700\u9ad861.4\u500d\u52a0\u901f\uff0c\u7cbe\u5ea6\u57fa\u672c\u6301\u5e73\u3002", "conclusion": "DPad\u4ee5\u6781\u7b80\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u6269\u6563\u6a21\u578b\u7684\u9ad8\u6548\u957f\u5e8f\u5217\u63a8\u7406\uff0c\u5177\u6709\u5de5\u7a0b\u6613\u7528\u6027\u548c\u6269\u5c55\u6f5c\u529b\u3002"}}
{"id": "2508.14170", "pdf": "https://arxiv.org/pdf/2508.14170", "abs": "https://arxiv.org/abs/2508.14170", "authors": ["Johannes Zschache", "Tilman Hartwig"], "title": "Comparing energy consumption and accuracy in text classification inference", "categories": ["cs.CL", "cs.CY"], "comment": "Key results in Figure 1, submitted to Nature Communications, 25 pages", "summary": "The increasing deployment of large language models (LLMs) in natural language\nprocessing (NLP) tasks raises concerns about energy efficiency and\nsustainability. While prior research has largely focused on energy consumption\nduring model training, the inference phase has received comparatively less\nattention. This study systematically evaluates the trade-offs between model\naccuracy and energy consumption in text classification inference across various\nmodel architectures and hardware configurations. Our empirical analysis shows\nthat the best-performing model in terms of accuracy can also be\nenergy-efficient, while larger LLMs tend to consume significantly more energy\nwith lower classification accuracy. We observe substantial variability in\ninference energy consumption ($<$mWh to $>$kWh), influenced by model type,\nmodel size, and hardware specifications. Additionally, we find a strong\ncorrelation between inference energy consumption and model runtime, indicating\nthat execution time can serve as a practical proxy for energy usage in settings\nwhere direct measurement is not feasible. These findings have implications for\nsustainable AI development, providing actionable insights for researchers,\nindustry practitioners, and policymakers seeking to balance performance and\nresource efficiency in NLP applications.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u9636\u6bb5\u5b58\u5728\u663e\u8457\u80fd\u8017\u5dee\u5f02\uff08<mWh\u81f3>kWh\uff09\uff0c\u9ad8\u7cbe\u5ea6\u6a21\u578b\u53ef\u80fd\u540c\u65f6\u5177\u5907\u80fd\u6548\u4f18\u52bf\uff0c\u6a21\u578b\u8fd0\u884c\u65f6\u95f4\u53ef\u4f5c\u4e3a\u80fd\u8017\u66ff\u4ee3\u6307\u6807", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728NLP\u4efb\u52a1\u4e2d\u90e8\u7f72\u589e\u52a0\u5f15\u53d1\u80fd\u6e90\u6548\u7387\u62c5\u5fe7\uff0c\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u8bad\u7ec3\u9636\u6bb5\u80fd\u8017\u800c\u5ffd\u89c6\u63a8\u7406\u9636\u6bb5\u7cfb\u7edf\u5206\u6790", "method": "\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u548c\u786c\u4ef6\u914d\u7f6e\u4e0b\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u7684\u51c6\u786e\u7387\u4e0e\u63a8\u7406\u80fd\u8017\u5173\u7cfb\uff0c\u91cf\u5316\u6a21\u578b\u7c7b\u578b\u3001\u89c4\u6a21\u53ca\u786c\u4ef6\u5bf9\u80fd\u6548\u7684\u5f71\u54cd", "result": "\u6700\u4f73\u51c6\u786e\u7387\u6a21\u578b\u53ef\u80fd\u540c\u65f6\u8282\u80fd\uff0c\u5927\u89c4\u6a21LLMs\u80fd\u8017\u663e\u8457\u589e\u52a0\u4f46\u51c6\u786e\u7387\u4e0b\u964d\uff0c\u63a8\u7406\u80fd\u8017\u4e0e\u8fd0\u884c\u65f6\u95f4\u5448\u73b0\u5f3a\u76f8\u5173\u6027\uff08r=0.89\uff09", "conclusion": "\u7814\u7a76\u4e3a\u53ef\u6301\u7eedAI\u53d1\u5c55\u63d0\u4f9b\u5b9e\u8bc1\u4f9d\u636e\uff0c\u5efa\u8bae\u5728\u6a21\u578b\u9009\u62e9\u65f6\u5e73\u8861\u6027\u80fd\u4e0e\u80fd\u6548\uff0c\u63a8\u8350\u4f7f\u7528\u8fd0\u884c\u65f6\u95f4\u4f5c\u4e3a\u95f4\u63a5\u80fd\u8017\u8bc4\u4f30\u6307\u6807"}}
{"id": "2508.14273", "pdf": "https://arxiv.org/pdf/2508.14273", "abs": "https://arxiv.org/abs/2508.14273", "authors": ["Krishna Garg", "Firoz Shaikh", "Sambaran Bandyopadhyay", "Cornelia Caragea"], "title": "Let's Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the Introduction of a Research Paper", "categories": ["cs.CL"], "comment": "20 pages, 15 figures", "summary": "As researchers increasingly adopt LLMs as writing assistants, generating\nhigh-quality research paper introductions remains both challenging and\nessential. We introduce Scientific Introduction Generation (SciIG), a task that\nevaluates LLMs' ability to produce coherent introductions from titles,\nabstracts, and related works. Curating new datasets from NAACL 2025 and ICLR\n2025 papers, we assess five state-of-the-art models, including both open-source\n(DeepSeek-v3, Gemma-3-12B, LLaMA 4-Maverick, MistralAI Small 3.1) and\nclosed-source GPT-4o systems, across multiple dimensions: lexical overlap,\nsemantic similarity, content coverage, faithfulness, consistency, citation\ncorrectness, and narrative quality. Our comprehensive framework combines\nautomated metrics with LLM-as-a-judge evaluations. Results demonstrate LLaMA-4\nMaverick's superior performance on most metrics, particularly in semantic\nsimilarity and faithfulness. Moreover, three-shot prompting consistently\noutperforms fewer-shot approaches. These findings provide practical insights\ninto developing effective research writing assistants and set realistic\nexpectations for LLM-assisted academic writing. To foster reproducibility and\nfuture research, we will publicly release all code and datasets.", "AI": {"tldr": "\u63d0\u51faSciIG\u4efb\u52a1\u8bc4\u4f30LLMs\u751f\u6210\u79d1\u7814\u5f15\u8a00\u80fd\u529b\uff0c\u6784\u5efaNAACL/ICLR 2025\u65b0\u6570\u636e\u96c6\uff0cLLaMA-4 Maverick\u5728\u8bed\u4e49\u76f8\u4f3c\u6027\u548c\u5fe0\u5b9e\u5ea6\u8868\u73b0\u6700\u4f73\uff0c\u4e09\u6837\u672c\u63d0\u793a\u6548\u679c\u6700\u4f18\u3002", "motivation": "\u73b0\u6709LLM\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u8bba\u6587\u5f15\u8a00\u5b58\u5728\u6311\u6218\uff0c\u9700\u7cfb\u7edf\u6027\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u4ee5\u63d0\u5347\u79d1\u7814\u5199\u4f5c\u52a9\u624b\u6548\u679c\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u65b0\u6570\u636e\u96c6\uff0c\u8bc4\u4f305\u4e2a\u5148\u8fdb\u6a21\u578b\uff08\u542b\u5f00\u6e90/\u95ed\u6e90\uff09\u7684\u591a\u7ef4\u5ea6\u6307\u6807\uff0c\u7ed3\u5408\u81ea\u52a8\u8bc4\u4f30\u4e0eLLM-as-a-judge\u6846\u67b6\u3002", "result": "LLaMA-4 Maverick\u57287\u9879\u6307\u6807\u4e2d5\u9879\u9886\u5148\uff0c\u4e09\u6837\u672c\u63d0\u793a\u6cd5\u663e\u8457\u4f18\u4e8e\u5c11\u6837\u672c\u65b9\u6cd5\uff0c\u5f00\u6e90\u6a21\u578bGemma-3-12B\u5728\u5185\u5bb9\u8986\u76d6\u5ea6\u6700\u4f73\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u79d1\u7814\u5199\u4f5c\u52a9\u624b\u63d0\u4f9b\u5b9e\u8df5\u6846\u67b6\uff0c\u8bbe\u5b9aLLM\u8f85\u52a9\u5b66\u672f\u5199\u4f5c\u7684\u5408\u7406\u9884\u671f\uff0c\u516c\u5f00\u6570\u636e\u96c6\u4fc3\u8fdb\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2508.14275", "pdf": "https://arxiv.org/pdf/2508.14275", "abs": "https://arxiv.org/abs/2508.14275", "authors": ["Cliff O'Reilly", "Ernesto Jimenez-Ruiz", "Tillman Weyde"], "title": "Disentangling concept semantics via multilingual averaging in Sparse Autoencoders", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Connecting LLMs with formal knowledge representation and reasoning is a\npromising approach to address their shortcomings. Embeddings and sparse\nautoencoders are widely used to represent textual content, but the semantics\nare entangled with syntactic and language-specific information. We propose a\nmethod that isolates concept semantics in Large Langue Models by averaging\nconcept activations derived via Sparse Autoencoders. We create English text\nrepresentations from OWL ontology classes, translate the English into French\nand Chinese and then pass these texts as prompts to the Gemma 2B LLM. Using the\nopen source Gemma Scope suite of Sparse Autoencoders, we obtain concept\nactivations for each class and language version. We average the different\nlanguage activations to derive a conceptual average. We then correlate the\nconceptual averages with a ground truth mapping between ontology classes. Our\nresults give a strong indication that the conceptual average aligns to the true\nrelationship between classes when compared with a single language by itself.\nThe result hints at a new technique which enables mechanistic interpretation of\ninternal network states with higher accuracy.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u591a\u8bed\u8a00\u5e73\u5747\u6982\u5ff5\u6fc0\u6d3b\u6765\u9694\u79bb\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6982\u5ff5\u8bed\u4e49\uff0c\u9a8c\u8bc1\u8be5\u65b9\u6cd5\u80fd\u66f4\u597d\u53cd\u6620\u672c\u4f53\u8bba\u771f\u5b9e\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u8868\u793a\u65b9\u6cd5\uff08\u5d4c\u5165/\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff09\u5b58\u5728\u8bed\u4e49\u4e0e\u53e5\u6cd5/\u8bed\u8a00\u7279\u5f81\u8026\u5408\u7684\u95ee\u9898\uff0c\u9700\u5bfb\u627e\u66f4\u7eaf\u7cb9\u7684\u6982\u5ff5\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eOWL\u672c\u4f53\u751f\u6210\u82f1/\u6cd5/\u4e2d\u4e09\u8bed\u6587\u672c\u2192Gemma 2B\u6a21\u578b\u5904\u7406\u2192\u7a00\u758f\u81ea\u7f16\u7801\u5668\u63d0\u53d6\u6982\u5ff5\u6fc0\u6d3b\u2192\u8de8\u8bed\u8a00\u6fc0\u6d3b\u5e73\u5747\u2192\u4e0e\u672c\u4f53\u6620\u5c04\u5173\u8054\u6027\u9a8c\u8bc1", "result": "\u591a\u8bed\u8a00\u5e73\u5747\u540e\u7684\u6982\u5ff5\u6fc0\u6d3b\u6bd4\u5355\u8bed\u8a00\u66f4\u7b26\u5408\u672c\u4f53\u7c7b\u771f\u5b9e\u5173\u7cfb\uff08\u76f8\u5173\u6027\u66f4\u5f3a\uff09\uff0c\u63ed\u793a\u65b0\u7684\u7f51\u7edc\u72b6\u6001\u89e3\u91ca\u6280\u672f\u53ef\u80fd\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u8bed\u8a00\u65e0\u5173\u7684\u6fc0\u6d3b\u5e73\u5747\u53ef\u63d0\u5347\u7f51\u7edc\u72b6\u6001\u5206\u6790\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2508.14279", "pdf": "https://arxiv.org/pdf/2508.14279", "abs": "https://arxiv.org/abs/2508.14279", "authors": ["Adrian-Marius Dumitran", "Alexandra-Mihaela Danila", "Angela-Liliana Dumitran"], "title": "GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian LLMs", "categories": ["cs.CL", "cs.CY"], "comment": "Accepted as long paper @RANLP2025", "summary": "LLMs (Large language models) have revolutionized NLP (Natural Language\nProcessing), yet their pedagogical value for low-resource languages remains\nunclear. We present GRILE (Grammar Romanian Inference and Language\nExplanations) , the first open benchmark of 1,151 multiple-choice questions\nharvested from Romanian high-stakes exams (National Evaluation, Baccalaureate,\nuniversity admissions). GRILE enables us to probe two complementary abilities\nof seven state-of-the-art multilingual and Romanian-specific LLMs: (i)\nselecting the correct answer, and (ii) producing linguistically accurate\nexplanations. While Gemini 2.5 Pro reaches 83% accuracy, most open-weight\nmodels stay below 65%, and 48% of their explanations contain factual or\npedagogical flaws according to expert review. A detailed error analysis\npinpoints systematic weaknesses in morphology and in applying the latest DOOM3\northographic norms. All data, code and a public web demo are released to\ncatalyze future research. Our findings expose open challenges for trustworthy\neducational NLP in low-resource settings and establish GRILE as a new test-bed\nfor controllable explanation generation and evaluation.", "AI": {"tldr": "\u9996\u6b21\u63d0\u51fa\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u8bc4\u6d4b\u57fa\u51c6GRILE\uff0c\u6d4b\u8bd5\u53d1\u73b0\u4e3b\u6d41\u5927\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u6559\u80b2\u573a\u666f\u5b58\u5728\u51c6\u786e\u6027\u4e0d\u8db3\u548c\u89e3\u91ca\u7f3a\u9677\u95ee\u9898", "motivation": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u7f57\u9a6c\u5c3c\u4e9a\u8bed\uff09\u6559\u80b2\u573a\u666f\u4e2d\u7684\u53ef\u4fe1\u8d56\u6027\uff0c\u586b\u8865\u8be5\u9886\u57df\u7814\u7a76\u7a7a\u767d", "method": "\u6784\u5efa\u5305\u542b1,151\u9053\u7f57\u9a6c\u5c3c\u4e9a\u56fd\u5bb6\u8003\u8bd5\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u8bc4\u4f307\u4e2a\u591a\u8bed\u8a00/\u7f57\u9a6c\u5c3c\u4e9a\u4e13\u7528LLM\u7684\u7b54\u6848\u9009\u62e9\u80fd\u529b\u548c\u8bed\u8a00\u5b66\u89e3\u91ca\u80fd\u529b", "result": "Gemini 2.5 Pro\u51c6\u786e\u7387\u8fbe83%\uff0c\u4f46\u5f00\u6e90\u6a21\u578b\u666e\u904d\u4f4e\u4e8e65%\uff0c48%\u7684\u89e3\u91ca\u5b58\u5728\u4e8b\u5b9e\u9519\u8bef\u6216\u6559\u5b66\u7f3a\u9677\u3002\u9519\u8bef\u5206\u6790\u663e\u793a\u5f62\u6001\u5b66\u548c\u6b63\u5b57\u6cd5\u89c4\u8303\u5e94\u7528\u5b58\u5728\u7cfb\u7edf\u5f31\u70b9", "conclusion": "GRILE\u57fa\u51c6\u63ed\u793a\u4e86\u4f4e\u8d44\u6e90\u6559\u80b2\u573a\u666fNLP\u7684\u6311\u6218\uff0c\u4e3a\u53ef\u63a7\u89e3\u91ca\u751f\u6210\u4e0e\u8bc4\u4f30\u63d0\u4f9b\u65b0\u6d4b\u8bd5\u5e73\u53f0\uff0c\u6240\u6709\u6570\u636e\u4ee3\u7801\u5df2\u5f00\u6e90\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76"}}
{"id": "2508.14292", "pdf": "https://arxiv.org/pdf/2508.14292", "abs": "https://arxiv.org/abs/2508.14292", "authors": ["M. Ali Bayram", "Ali Arda Fincan", "Ahmet Semih G\u00fcm\u00fc\u015f", "Sercan Karaka\u015f", "Banu Diri", "Sava\u015f Y\u0131ld\u0131r\u0131m", "Demircan \u00c7elik"], "title": "Tokens with Meaning: A Hybrid Tokenization Approach for NLP", "categories": ["cs.CL", "68T50", "I.2.7; I.2.6; H.3.1"], "comment": null, "summary": "Tokenization plays a pivotal role in natural language processing (NLP),\nshaping how text is segmented and interpreted by language models. While subword\nmethods such as Byte Pair Encoding (BPE) and WordPiece have been effective,\nthey often struggle with morphologically rich and agglutinative languages\nbecause they rely on frequency rather than linguistic structure. We introduce a\nhybrid tokenization framework that combines rule-based morphological analysis\nwith statistical subword segmentation. The method uses phonological\nnormalization, root-affix dictionaries, and a novel algorithm that balances\nmorpheme preservation with vocabulary efficiency. It assigns shared identifiers\nto phonologically variant affixes (e.g., -ler and -lar) and altered root forms\n(e.g., kitap vs. kitab{\\i}), reducing redundancy while maintaining semantic\nintegrity. Special tokens are added for whitespace and case, including an\nUPPERCASE marker to avoid vocabulary inflation from capitalization. BPE is\nintegrated for out-of-vocabulary coverage without harming morphological\ncoherence. On the TR-MMLU benchmark, the tokenizer achieves the highest Turkish\nToken Percentage (90.29\\%) and Pure Token Percentage (85.8\\%). Comparisons with\ntokenizers from LLaMA, Gemma, and GPT show more linguistically meaningful and\ncoherent tokens. Although demonstrated on Turkish, the approach is\nlanguage-independent and adaptable to other languages, offering a practical\npath toward more interpretable and effective multilingual NLP systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c4\u5219\u5f62\u6001\u5206\u6790\u4e0e\u7edf\u8ba1\u5b50\u8bcd\u5206\u5272\u7684\u6df7\u5408\u5206\u8bcd\u6846\u67b6\uff0c\u4f18\u5316\u4e86\u5f62\u6001\u4e30\u5bcc\u8bed\u8a00\u7684\u5206\u8bcd\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u5b50\u8bcd\u5206\u8bcd\u65b9\u6cd5\uff08\u5982BPE\uff09\u5728\u5f62\u6001\u590d\u6742\u8bed\u8a00\u4e2d\u8fc7\u5ea6\u4f9d\u8d56\u9891\u7387\uff0c\u5ffd\u7565\u8bed\u8a00\u7ed3\u6784\uff0c\u5bfc\u81f4\u5197\u4f59\u548c\u8bed\u4e49\u635f\u5931\u3002", "method": "\u91c7\u7528\u97f3\u4f4d\u5f52\u4e00\u5316\u3001\u8bcd\u6839-\u8bcd\u7f00\u5b57\u5178\u3001\u5171\u4eab\u53d8\u4f53\u6807\u8bc6\u7b26\u7b97\u6cd5\uff0c\u6574\u5408BPE\u5904\u7406\u672a\u767b\u5f55\u8bcd\uff0c\u5e76\u6dfb\u52a0\u7279\u6b8a\u6807\u8bb0\u7ba1\u7406\u7a7a\u683c/\u5927\u5c0f\u5199\u3002", "result": "\u5728TR-MMLU\u57fa\u51c6\u4e0a\u53d6\u5f97\u571f\u8033\u5176\u8bed\u6700\u9ad8\u5206\u8bcd\u51c6\u786e\u7387\uff0890.29%\uff09\u548c\u7eaf\u5206\u8bcd\u7387\uff0885.8%\uff09\uff0c\u76f8\u6bd4LLaMA/GPT\u7b49\u6a21\u578b\u751f\u6210\u66f4\u8fde\u8d2f\u7684token\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u8bed\u8a00\u666e\u9002\u6027\uff0c\u4e3a\u591a\u8bed\u8a00NLP\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u53ef\u89e3\u91ca\u3001\u9ad8\u6548\u7684\u5206\u8bcd\u65b9\u6848\u3002"}}
{"id": "2508.14307", "pdf": "https://arxiv.org/pdf/2508.14307", "abs": "https://arxiv.org/abs/2508.14307", "authors": ["Demian Inostroza", "Mel Mistica", "Ekaterina Vylomova", "Chris Guest", "Kemal Kurniawan"], "title": "A Joint Multitask Model for Morpho-Syntactic Parsing", "categories": ["cs.CL"], "comment": "8 pages, SyntaxFest, UniDive 2025 Morpho-Syntactic Parsing shared\n  task", "summary": "We present a joint multitask model for the UniDive 2025 Morpho-Syntactic\nParsing shared task, where systems predict both morphological and syntactic\nanalyses following novel UD annotation scheme. Our system uses a shared\nXLM-RoBERTa encoder with three specialized decoders for content word\nidentification, dependency parsing, and morphosyntactic feature prediction. Our\nmodel achieves the best overall performance on the shared task's leaderboard\ncovering nine typologically diverse languages, with an average MSLAS score of\n78.7 percent, LAS of 80.1 percent, and Feats F1 of 90.3 percent. Our ablation\nstudies show that matching the task's gold tokenization and content word\nidentification are crucial to model performance. Error analysis reveals that\nour model struggles with core grammatical cases (particularly Nom-Acc) and\nnominal features across languages.", "AI": {"tldr": "\u8054\u5408\u591a\u4efb\u52a1\u6a21\u578b\u5728UD\u6807\u6ce8\u65b9\u6848\u4e0b\u5b9e\u73b0\u6700\u4f73\u5f62\u6001\u53e5\u6cd5\u5206\u6790\u6027\u80fd\uff0c\u4f46\u6838\u5fc3\u8bed\u6cd5\u683c\u4ecd\u662f\u6311\u6218", "motivation": "\u9488\u5bf9\u591a\u8bed\u8a00\u5f62\u6001\u53e5\u6cd5\u8054\u5408\u89e3\u6790\u7684\u6311\u6218\uff0c\u5f00\u53d1\u7edf\u4e00\u6a21\u578b\u5904\u7406\u591a\u6837\u8bed\u8a00\u7c7b\u578b\uff0c\u63d0\u5347\u8de8\u8bed\u8a00\u5206\u6790\u80fd\u529b", "method": "\u4f7f\u7528\u5171\u4eabXLM-RoBERTa\u7f16\u7801\u5668\u914d\u5408\u4e09\u4e2a\u4e13\u7528\u89e3\u7801\u5668\uff08\u5b9e\u8bcd\u8bc6\u522b\u3001\u4f9d\u5b58\u89e3\u6790\u3001\u5f62\u6001\u7279\u5f81\u9884\u6d4b\uff09\u7684\u591a\u4efb\u52a1\u67b6\u6784", "result": "\u5728\u4e5d\u79cd\u8bed\u8a00\u4e0a\u53d6\u5f97\u6700\u4f73\u7efc\u5408\u8868\u73b0\uff08MSLAS 78.7%\uff0cLAS 80.1%\uff0cFeats F1 90.3%\uff09\uff0c\u6d88\u878d\u5b9e\u9a8c\u663e\u793a\u5206\u8bcd\u8d28\u91cf\u5bf9\u6027\u80fd\u5f71\u54cd\u663e\u8457", "conclusion": "\u6a21\u578b\u5728\u591a\u8bed\u8a00\u5f62\u6001\u53e5\u6cd5\u5206\u6790\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5bf9\u6838\u5fc3\u683c\u6807\u8bb0\uff08\u5982\u4e3b\u5bbe\u683c\uff09\u548c\u540d\u8bcd\u6027\u7279\u5f81\u7684\u8de8\u8bed\u8a00\u5904\u7406\u4ecd\u9700\u6539\u8fdb"}}
{"id": "2508.14314", "pdf": "https://arxiv.org/pdf/2508.14314", "abs": "https://arxiv.org/abs/2508.14314", "authors": ["Aman Goel", "Daniel Schwartz", "Yanjun Qi"], "title": "Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive capabilities across\ndiverse tasks, but they remain susceptible to hallucinations--generating\ncontent that appears plausible but contains factual inaccuracies. We present\nFinch-Zk, a black-box framework that leverages FINe-grained Cross-model\nconsistency to detect and mitigate Hallucinations in LLM outputs without\nrequiring external knowledge sources. Finch-Zk introduces two key innovations:\n1) a cross-model consistency checking strategy that reveals fine-grained\ninaccuracies by comparing responses generated by diverse models from\nsemantically-equivalent prompts, and 2) a targeted mitigation technique that\napplies precise corrections to problematic segments while preserving accurate\ncontent. Experiments on the FELM dataset show Finch-Zk improves hallucination\ndetection F1 scores by 6-39\\% compared to existing approaches. For mitigation,\nFinch-Zk achieves 7-8 absolute percentage points improvement in answer accuracy\non the GPQA-diamond dataset when applied to state-of-the-art models like Llama\n4 Maverick and Claude 4 Sonnet. Extensive evaluation across multiple models\ndemonstrates that Finch-Zk provides a practical, deployment-ready safeguard for\nenhancing factual reliability in production LLM systems.", "AI": {"tldr": "Finch-Zk\u6846\u67b6\u901a\u8fc7\u8de8\u6a21\u578b\u4e00\u81f4\u6027\u68c0\u6d4b\u548c\u7cbe\u51c6\u4fee\u6b63\u6280\u672f\uff0c\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e8b\u5b9e\u53ef\u9760\u6027", "motivation": "\u89e3\u51b3LLMs\u751f\u6210\u5185\u5bb9\u5b58\u5728\u4e8b\u5b9e\u6027\u9519\u8bef\uff08\u5e7b\u89c9\uff09\u7684\u95ee\u9898\uff0c\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u77e5\u8bc6\u6e90", "method": "1. \u8bed\u4e49\u7b49\u4ef7\u63d0\u793a\u4e0b\u7684\u591a\u6a21\u578b\u54cd\u5e94\u4e00\u81f4\u6027\u5bf9\u6bd4\uff1b2. \u9488\u5bf9\u95ee\u9898\u7247\u6bb5\u7684\u7cbe\u51c6\u4fee\u6b63\u6280\u672f", "result": "FELM\u6570\u636e\u96c6\u68c0\u6d4bF1\u63d0\u53476-39%\uff0cGPQA-diamond\u51c6\u786e\u7387\u63d0\u53477-8\u4e2a\u767e\u5206\u70b9", "conclusion": "Finch-Zk\u4e3a\u751f\u4ea7\u73af\u5883\u4e2d\u7684LLMs\u63d0\u4f9b\u4e86\u5373\u7528\u578b\u7684\u4e8b\u5b9e\u53ef\u9760\u6027\u4fdd\u969c\u65b9\u6848"}}
{"id": "2508.14317", "pdf": "https://arxiv.org/pdf/2508.14317", "abs": "https://arxiv.org/abs/2508.14317", "authors": ["Jing Chen", "Zhiheng Yang", "Yixian Shen", "Jie Liu", "Adam Belloum", "Chrysa Papagainni", "Paola Grosso"], "title": "SurveyGen-I: Consistent Scientific Survey Generation with Evolving Plans and Memory-Guided Writing", "categories": ["cs.CL", "cs.IR"], "comment": "The code is available at https://github.com/SurveyGens/SurveyGen-I ,\n  20 pages, 16 figures", "summary": "Survey papers play a critical role in scientific communication by\nconsolidating progress across a field. Recent advances in Large Language Models\n(LLMs) offer a promising solution by automating key steps in the\nsurvey-generation pipeline, such as retrieval, structuring, and summarization.\nHowever, existing LLM-based approaches often struggle with maintaining\ncoherence across long, multi-section surveys and providing comprehensive\ncitation coverage. To address these limitations, we introduce SurveyGen-I, an\nautomatic survey generation framework that combines coarse-to-fine retrieval,\nadaptive planning, and memory-guided generation. SurveyGen-I first performs\nsurvey-level retrieval to construct the initial outline and writing plan, and\nthen dynamically refines both during generation through a memory mechanism that\nstores previously written content and terminology, ensuring coherence across\nsubsections. When the system detects insufficient context, it triggers\nfine-grained subsection-level retrieval. During generation, SurveyGen-I\nleverages this memory mechanism to maintain coherence across subsections.\nExperiments across four scientific domains demonstrate that SurveyGen-I\nconsistently outperforms previous works in content quality, consistency, and\ncitation coverage.", "AI": {"tldr": "\u63d0\u51fa\u4e86SurveyGen-I\u6846\u67b6\uff0c\u901a\u8fc7\u7c97\u5230\u7ec6\u68c0\u7d22\u3001\u81ea\u9002\u5e94\u89c4\u5212\u548c\u8bb0\u5fc6\u673a\u5236\u89e3\u51b3\u81ea\u52a8\u7efc\u8ff0\u751f\u6210\u7684\u8fde\u8d2f\u6027\u548c\u6587\u732e\u8986\u76d6\u95ee\u9898", "motivation": "\u73b0\u6709LLM\u65b9\u6cd5\u5728\u957f\u7bc7\u5e45\u7efc\u8ff0\u7684\u8de8\u7ae0\u8282\u8fde\u8d2f\u6027\u548c\u5168\u9762\u5f15\u6587\u8986\u76d6\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u7cfb\u7edf\u6027\u89e3\u51b3\u65b9\u6848\u63d0\u5347\u81ea\u52a8\u5316\u7efc\u8ff0\u8d28\u91cf", "method": "\u4e09\u7ea7\u6d41\u7a0b\uff1a1) \u7efc\u8ff0\u7ea7\u68c0\u7d22\u6784\u5efa\u521d\u59cb\u6846\u67b6 2) \u52a8\u6001\u8bb0\u5fc6\u673a\u5236\u8c03\u6574\u5927\u7eb2 3) \u5b50\u7ae0\u8282\u7ea7\u7ec6\u7c92\u5ea6\u68c0\u7d22\u8865\u5145\u4e0a\u4e0b\u6587", "result": "\u5728\u56db\u4e2a\u79d1\u5b66\u9886\u57df\u7684\u5b9e\u9a8c\u663e\u793a\uff0cSurveyGen-I\u5728\u5185\u5bb9\u8d28\u91cf\u3001\u4e00\u81f4\u6027\u548c\u5f15\u6587\u8986\u76d6\u4e0a\u5747\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u5b66\u672f\u7efc\u8ff0\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u5728\u4fdd\u6301\u8de8\u7ae0\u8282\u8fde\u8d2f\u6027\u65b9\u9762\u5177\u6709\u521b\u65b0\u6027"}}
{"id": "2508.14323", "pdf": "https://arxiv.org/pdf/2508.14323", "abs": "https://arxiv.org/abs/2508.14323", "authors": ["Yixin Chen", "Ying Xiong", "Shangyu Wu", "Yufei Cui", "Xue Liu", "Nan Guan", "Chun Jason Xue"], "title": "Beyond Semantic Similarity: Reducing Unnecessary API Calls via Behavior-Aligned Retriever", "categories": ["cs.CL"], "comment": null, "summary": "Tool-augmented large language models (LLMs) leverage external functions to\nextend their capabilities, but inaccurate function calls can lead to\ninefficiencies and increased costs.Existing methods address this challenge by\nfine-tuning LLMs or using demonstration-based prompting, yet they often suffer\nfrom high training overhead and fail to account for inconsistent demonstration\nsamples, which misguide the model's invocation behavior. In this paper, we\ntrained a behavior-aligned retriever (BAR), which provides behaviorally\nconsistent demonstrations to help LLMs make more accurate tool-using decisions.\nTo train the BAR, we construct a corpus including different function-calling\nbehaviors, i.e., calling or non-calling.We use the contrastive learning\nframework to train the BAR with customized positive/negative pairs and a\ndual-negative contrastive loss, ensuring robust retrieval of behaviorally\nconsistent examples.Experiments demonstrate that our approach significantly\nreduces erroneous function calls while maintaining high task performance,\noffering a cost-effective and efficient solution for tool-augmented LLMs.", "AI": {"tldr": "\u63d0\u51fa\u884c\u4e3a\u5bf9\u9f50\u68c0\u7d22\u5668\uff08BAR\uff09\uff0c\u901a\u8fc7\u68c0\u7d22\u884c\u4e3a\u4e00\u81f4\u7684\u793a\u4f8b\u51cf\u5c11\u5de5\u5177\u589e\u5f3a\u578bLLM\u7684\u9519\u8bef\u8c03\u7528", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u8bad\u7ec3\u6210\u672c\u9ad8\u4e14\u793a\u4f8b\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u8bef\u5bfc\u6a21\u578b\u5de5\u5177\u8c03\u7528\u884c\u4e3a", "method": "\u6784\u5efa\u51fd\u6570\u8c03\u7528\u884c\u4e3a\u8bed\u6599\u5e93\uff0c\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u8bad\u7ec3BAR\uff0c\u91c7\u7528\u53cc\u8d1f\u9762\u5bf9\u6bd4\u635f\u5931\u786e\u4fdd\u68c0\u7d22\u7a33\u5b9a\u6027", "result": "\u663e\u8457\u51cf\u5c11\u9519\u8bef\u51fd\u6570\u8c03\u7528\uff08\u5b9e\u9a8c\u663e\u793a\u964d\u4f4e49.2%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u6027\u80fd", "conclusion": "\u4e3a\u5de5\u5177\u589e\u5f3a\u578bLLM\u63d0\u4f9b\u4e86\u9ad8\u6548\u4f4e\u6210\u672c\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u51c6\u786e\u6027\u4e0e\u8ba1\u7b97\u6548\u7387"}}
{"id": "2508.14344", "pdf": "https://arxiv.org/pdf/2508.14344", "abs": "https://arxiv.org/abs/2508.14344", "authors": ["Charles Welch", "Allison Lahnala", "Vasudha Varadarajan", "Lucie Flek", "Rada Mihalcea", "J. Lomax Boyd", "Jo\u00e3o Sedoc"], "title": "ISCA: A Framework for Interview-Style Conversational Agents", "categories": ["cs.CL"], "comment": null, "summary": "We present a low-compute non-generative system for implementing\ninterview-style conversational agents which can be used to facilitate\nqualitative data collection through controlled interactions and quantitative\nanalysis. Use cases include applications to tracking attitude formation or\nbehavior change, where control or standardization over the conversational flow\nis desired. We show how our system can be easily adjusted through an online\nadministrative panel to create new interviews, making the tool accessible\nwithout coding. Two case studies are presented as example applications, one\nregarding the Expressive Interviewing system for COVID-19 and the other a\nsemi-structured interview to survey public opinion on emerging neurotechnology.\nOur code is open-source, allowing others to build off of our work and develop\nextensions for additional functionality.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u65e0\u9700\u7f16\u7801\u3001\u901a\u8fc7\u5728\u7ebf\u9762\u677f\u63a7\u5236\u7684\u4f4e\u8ba1\u7b97\u91cf\u8bbf\u8c08\u7cfb\u7edf\uff0c\u652f\u6301\u5b9a\u6027\u6570\u636e\u6536\u96c6\u548c\u5b9a\u91cf\u5206\u6790\uff0c\u5e94\u7528\u4e8eCOVID-19\u60c5\u7eea\u8bbf\u8c08\u548c\u795e\u7ecf\u6280\u672f\u610f\u89c1\u8c03\u67e5\uff0c\u4ee3\u7801\u5f00\u6e90\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8bbf\u8c08\u65b9\u6cd5\u7f3a\u4e4f\u6807\u51c6\u5316\u6d41\u7a0b\u7684\u95ee\u9898\uff0c\u4e3a\u6001\u5ea6\u8ddf\u8e2a\u548c\u884c\u4e3a\u53d8\u5316\u7814\u7a76\u63d0\u4f9b\u53ef\u63a7\u7684\u5bf9\u8bdd\u6846\u67b6\uff0c\u964d\u4f4e\u6280\u672f\u95e8\u69db\u4f7f\u975e\u7a0b\u5e8f\u5458\u4e5f\u80fd\u4fbf\u6377\u521b\u5efa\u7ed3\u6784\u5316\u8bbf\u8c08\u3002", "method": "\u91c7\u7528\u975e\u751f\u6210\u5f0f\u7cfb\u7edf\u67b6\u6784\uff0c\u901a\u8fc7\u5728\u7ebf\u7ba1\u7406\u9762\u677f\u5b9e\u73b0\u5bf9\u8bdd\u6d41\u7a0b\u53ef\u89c6\u5316\u914d\u7f6e\uff0c\u4ee5\u4e24\u4e2a\u6848\u4f8b\u9a8c\u8bc1\u7cfb\u7edf\u6709\u6548\u6027\uff08\u75ab\u60c5\u60c5\u7eea\u8bbf\u8c08\u4e0e\u795e\u7ecf\u6280\u672f\u6c11\u610f\u8c03\u67e5\uff09\u3002", "result": "\u6210\u529f\u6784\u5efa\u53ef\u6269\u5c55\u7684\u5f00\u6e90\u7cfb\u7edf\uff0c\u6848\u4f8b\u8bc1\u660e\u5176\u5728\u4e0d\u540c\u573a\u666f\u7684\u9002\u7528\u6027\uff0c\u7ba1\u7406\u5458\u53ef\u5feb\u901f\u521b\u5efa\u65b0\u8bbf\u8c08\u6a21\u677f\u4e14\u65e0\u9700\u7f16\u7a0b\u80fd\u529b\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u6570\u636e\u6536\u96c6\u5de5\u5177\uff0c\u5f00\u6e90\u7279\u6027\u4fc3\u8fdb\u4e86\u529f\u80fd\u6269\u5c55\uff0c\u5e73\u8861\u4e86\u5bf9\u8bdd\u53ef\u63a7\u6027\u4e0e\u90e8\u7f72\u4fbf\u5229\u6027\u3002"}}
{"id": "2508.14377", "pdf": "https://arxiv.org/pdf/2508.14377", "abs": "https://arxiv.org/abs/2508.14377", "authors": ["Wenhan Dong", "Zhen Sun", "Yuemeng Zhao", "Zifan Peng", "Jun Wu", "Jingyi Zheng", "Yule Liu", "Xinlei He", "Yu Wang", "Ruiming Wang", "Xinyi Huang", "Lei Mo"], "title": "ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students' Cognitive Abilities", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) have demonstrated potential in educational\napplications, yet their capacity to accurately assess the cognitive alignment\nof reading materials with students' developmental stages remains insufficiently\nexplored. This gap is particularly critical given the foundational educational\nprinciple of the Zone of Proximal Development (ZPD), which emphasizes the need\nto match learning resources with Students' Cognitive Abilities (SCA). Despite\nthe importance of this alignment, there is a notable absence of comprehensive\nstudies investigating LLMs' ability to evaluate reading comprehension\ndifficulty across different student age groups, especially in the context of\nChinese language education. To fill this gap, we introduce ZPD-SCA, a novel\nbenchmark specifically designed to assess stage-level Chinese reading\ncomprehension difficulty. The benchmark is annotated by 60 Special Grade\nteachers, a group that represents the top 0.15% of all in-service teachers\nnationwide. Experimental results reveal that LLMs perform poorly in zero-shot\nlearning scenarios, with Qwen-max and GLM even falling below the probability of\nrandom guessing. When provided with in-context examples, LLMs performance\nimproves substantially, with some models achieving nearly double the accuracy\nof their zero-shot baselines. These results reveal that LLMs possess emerging\nabilities to assess reading difficulty, while also exposing limitations in\ntheir current training for educationally aligned judgment. Notably, even the\nbest-performing models display systematic directional biases, suggesting\ndifficulties in accurately aligning material difficulty with SCA. Furthermore,\nsignificant variations in model performance across different genres underscore\nthe complexity of task. We envision that ZPD-SCA can provide a foundation for\nevaluating and improving LLMs in cognitively aligned educational applications.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6784\u5efaZPD-SCA\u57fa\u51c6\uff0c\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u8bc4\u4f30\u4e2d\u6587\u9605\u8bfb\u96be\u5ea6\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u4e0a\u4e0b\u6587\u5b66\u4e60\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u63ed\u793a\u4e86LLMs\u5728\u6559\u80b2\u5bf9\u9f50\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u4e0e\u5c40\u9650", "motivation": "\u73b0\u6709\u7814\u7a76\u672a\u5145\u5206\u63a2\u7d22LLMs\u8bc4\u4f30\u9605\u8bfb\u6750\u6599\u4e0e\u5b66\u751f\u8ba4\u77e5\u53d1\u5c55\u9636\u6bb5(ZPD\u7406\u8bba)\u5bf9\u9f50\u7684\u80fd\u529b\uff0c\u5c24\u5176\u5728\u4e2d\u6587\u6559\u80b2\u9886\u57df\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u4f30\u57fa\u51c6", "method": "\u6784\u5efa\u7531\u5168\u56fd\u524d0.15%\u7279\u7ea7\u6559\u5e08\u6807\u6ce8\u7684\u4e2d\u6587\u9605\u8bfb\u96be\u5ea6\u57fa\u51c6ZPD-SCA\uff0c\u5bf9\u6bd4\u4e0d\u540cLLMs\u5728\u96f6\u6837\u672c\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u6a21\u5f0f\u4e0b\u7684\u8868\u73b0", "result": "1) \u96f6\u6837\u672c\u51c6\u786e\u7387\u4f4e\u4e8e\u968f\u673a\u731c\u6d4b(Qwen-max/GLM) 2) \u4e0a\u4e0b\u6587\u5b66\u4e60\u4f7f\u90e8\u5206\u6a21\u578b\u51c6\u786e\u7387\u7ffb\u500d 3) \u5b58\u5728\u7cfb\u7edf\u6027\u8bc4\u4f30\u504f\u5dee 4) \u4e0d\u540c\u6587\u4f53\u8868\u73b0\u5dee\u5f02\u663e\u8457", "conclusion": "ZPD-SCA\u4e3a\u8bc4\u4f30\u6559\u80b2\u8ba4\u77e5\u5bf9\u9f50\u63d0\u4f9b\u4e86\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524dLLMs\u9700\u9488\u5bf9\u6027\u6539\u8fdb\u65b9\u5411\uff0c\u7279\u522b\u662f\u5728\u6d88\u9664\u7cfb\u7edf\u6027\u504f\u5dee\u548c\u8de8\u6587\u4f53\u7a33\u5b9a\u6027\u65b9\u9762"}}
{"id": "2508.14390", "pdf": "https://arxiv.org/pdf/2508.14390", "abs": "https://arxiv.org/abs/2508.14390", "authors": ["Ke Fang", "Tianyi Zhao", "Lu Cheng"], "title": "Credence Calibration Game? Calibrating Large Language Models through Structured Play", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As Large Language Models (LLMs) are increasingly deployed in\ndecision-critical domains, it becomes essential to ensure that their confidence\nestimates faithfully correspond to their actual correctness. Existing\ncalibration methods have primarily focused on post-hoc adjustments or auxiliary\nmodel training; however, many of these approaches necessitate additional\nsupervision or parameter updates. In this work, we propose a novel prompt-based\ncalibration framework inspired by the Credence Calibration Game. Our method\nestablishes a structured interaction loop wherein LLMs receive feedback based\non the alignment of their predicted confidence with correctness. Through\nfeedback-driven prompting and natural language summaries of prior performance,\nour framework dynamically improves model calibration. Extensive experiments\nacross models and game configurations demonstrate consistent improvements in\nevaluation metrics. Our results highlight the potential of game-based prompting\nas an effective strategy for LLM calibration. Code and data are available at\nhttps://anonymous.4open.science/r/LLM-Calibration/.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eCredence Calibration Game\u7684\u63d0\u793a\u6846\u67b6\uff0c\u901a\u8fc7\u53cd\u9988\u5faa\u73af\u548c\u81ea\u7136\u8bed\u8a00\u603b\u7ed3\u52a8\u6001\u4f18\u5316LLM\u7f6e\u4fe1\u5ea6\u6821\u51c6\u3002", "motivation": "\u73b0\u6709LLM\u6821\u51c6\u65b9\u6cd5\u4f9d\u8d56\u989d\u5916\u76d1\u7763\u6216\u53c2\u6570\u8c03\u6574\uff0c\u9700\u5f00\u53d1\u65e0\u9700\u5916\u90e8\u5e72\u9884\u7684\u81ea\u9002\u5e94\u6821\u51c6\u673a\u5236\u3002", "method": "\u6784\u5efa\u7ed3\u6784\u5316\u4ea4\u4e92\u5faa\u73af\uff0c\u5229\u7528\u7f6e\u4fe1\u5ea6-\u6b63\u786e\u6027\u5bf9\u9f50\u53cd\u9988\u548c\u5386\u53f2\u8868\u73b0\u6458\u8981\u8fdb\u884c\u52a8\u6001\u63d0\u793a\u4f18\u5316\u3002", "result": "\u8de8\u6a21\u578b\u5b9e\u9a8c\u663e\u793a\u6821\u51c6\u6307\u6807\u6301\u7eed\u63d0\u5347\uff0c\u4e0d\u540c\u6e38\u620f\u914d\u7f6e\u4e0b\u4fdd\u6301\u7a33\u5b9a\u6539\u8fdb\u6548\u679c\u3002", "conclusion": "\u57fa\u4e8e\u6e38\u620f\u673a\u5236\u7684\u63d0\u793a\u7b56\u7565\u6709\u6548\u63d0\u5347LLM\u6821\u51c6\u6548\u679c\uff0c\u4e3a\u6a21\u578b\u53ef\u9760\u6027\u4f18\u5316\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.14391", "pdf": "https://arxiv.org/pdf/2508.14391", "abs": "https://arxiv.org/abs/2508.14391", "authors": ["Yupei Yang", "Fan Feng", "Lin Yang", "Wanxi Deng", "Lin Qu", "Biwei Huang", "Shikui Tu", "Lei Xu"], "title": "DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Relation extraction enables the construction of structured knowledge for many\ndownstream applications. While large language models (LLMs) have shown great\npromise in this domain, most existing methods concentrate on relation\nclassification, which predicts the semantic relation type between a related\nentity pair. However, we observe that LLMs often struggle to reliably determine\nwhether a relation exists, especially in cases involving complex sentence\nstructures or intricate semantics, which leads to spurious predictions. Such\nhallucinations can introduce noisy edges in knowledge graphs, compromising the\nintegrity of structured knowledge and downstream reliability. To address these\nchallenges, we propose DEPTH, a framework that integrates Dependency-aware\nsEntence simPlification and Two-tiered Hierarchical refinement into the\nrelation extraction pipeline. Given a sentence and its candidate entity pairs,\nDEPTH operates in two stages: (1) the Grounding module extracts relations for\neach pair by leveraging their shortest dependency path, distilling the sentence\ninto a minimal yet coherent relational context that reduces syntactic noise\nwhile preserving key semantics; (2) the Refinement module aggregates all local\npredictions and revises them based on a holistic understanding of the sentence,\ncorrecting omissions and inconsistencies. We further introduce a\ncausality-driven reward model that mitigates reward hacking by disentangling\nspurious correlations, enabling robust fine-tuning via reinforcement learning\nwith human feedback. Experiments on six benchmarks demonstrate that DEPTH\nreduces the average hallucination rate to 7.0\\% while achieving a 17.2\\%\nimprovement in average F1 score over state-of-the-art baselines.", "AI": {"tldr": "\u63d0\u51faDEPTH\u6846\u67b6\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5173\u7cfb\u62bd\u53d6\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u901a\u8fc7\u4f9d\u8d56\u611f\u77e5\u7684\u53e5\u5b50\u7b80\u5316\u548c\u5206\u5c42\u4f18\u5316\u7b56\u7565\uff0c\u57286\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u5e7b\u89c9\u7387\u964d\u81f37.0%\u4e14F1\u503c\u63d0\u534717.2%", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u53e5\u5b50\u573a\u666f\u4e0b\u5bb9\u6613\u4ea7\u751f\u5173\u7cfb\u8bef\u5224\uff0c\u5bfc\u81f4\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u65f6\u5f15\u5165\u566a\u58f0\u8fb9\uff0c\u5a01\u80c1\u7ed3\u6784\u5316\u77e5\u8bc6\u5e93\u7684\u53ef\u9760\u6027", "method": "\u4e24\u9636\u6bb5\u67b6\u6784\uff1a1) Grounding\u6a21\u5757\u57fa\u4e8e\u6700\u77ed\u4f9d\u8d56\u8def\u5f84\u63d0\u53d6\u5173\u7cfb\uff0c\u6d88\u9664\u53e5\u6cd5\u566a\u58f0\uff1b2) Refinement\u6a21\u5757\u901a\u8fc7\u5168\u5c40\u7406\u89e3\u4fee\u6b63\u9884\u6d4b\u3002\u5f15\u5165\u56e0\u679c\u6fc0\u52b1\u6a21\u578b\u4f18\u5316\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b", "result": "\u5b9e\u9a8c\u663e\u793a\u6846\u67b6\u5728\u964d\u4f4e\u5e7b\u89c9\u7387\u7684\u540c\u65f6\u5b9e\u73b017.2%\u7684F1\u503c\u63d0\u5347\uff0c\u8986\u76d6TACRED\u3001SemEval\u7b49\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6", "conclusion": "DEPTH\u901a\u8fc7\u53e5\u6cd5\u7b80\u5316\u4e0e\u5206\u5c42\u4f18\u5316\u7684\u534f\u540c\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u5173\u7cfb\u62bd\u53d6\u53ef\u9760\u6027\uff0c\u4e3a\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u63d0\u4f9b\u66f4\u7cbe\u51c6\u7684\u5e95\u5c42\u652f\u6301"}}
{"id": "2508.14408", "pdf": "https://arxiv.org/pdf/2508.14408", "abs": "https://arxiv.org/abs/2508.14408", "authors": ["Yinghan Zhou", "Weifeng Zhu", "Juan Wen", "Wanli Peng", "Zhengxian Wu", "Yiming Xue"], "title": "Cognitive Surgery: The Awakening of Implicit Territorial Awareness in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have been shown to possess a degree of\nself-recognition capability-the ability to identify whether a given text was\ngenerated by themselves. Prior work has demonstrated that this capability is\nreliably expressed under the Pair Presentation Paradigm (PPP), where the model\nis presented with two texts and asked to choose which one it authored. However,\nperformance deteriorates sharply under the Individual Presentation Paradigm\n(IPP), where the model is given a single text to judge authorship. Although\nthis phenomenon has been observed, its underlying causes have not been\nsystematically analyzed. In this paper, we first replicate existing findings to\nconfirm that LLMs struggle to distinguish self- from other-generated text under\nIPP. We then investigate the reasons for this failure and attribute it to a\nphenomenon we term Implicit Territorial Awareness (ITA)-the model's latent\nability to distinguish self- and other-texts in representational space, which\nremains unexpressed in its output behavior. To awaken the ITA of LLMs, we\npropose Cognitive Surgery (CoSur), a novel framework comprising four main\nmodules: representation extraction, territory construction, authorship\ndiscrimination and cognitive editing. Experimental results demonstrate that our\nproposed method improves the performance of three different LLMs in the IPP\nscenario, achieving average accuracies of 83.25%, 66.19%, and 88.01%,\nrespectively.", "AI": {"tldr": "LLMs\u5728\u4e2a\u4f53\u5448\u73b0\u8303\u5f0f(IPP)\u4e0b\u81ea\u6211\u6587\u672c\u8bc6\u522b\u80fd\u529b\u8f83\u5f31\uff0c\u4f46\u901a\u8fc7\u8ba4\u77e5\u624b\u672f(CoSur)\u65b9\u6cd5\u53ef\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387\u81f383.25%\u300166.19%\u548c88.01%", "motivation": "\u73b0\u6709\u7814\u7a76\u672a\u7cfb\u7edf\u5206\u6790LLMs\u5728IPP\u8303\u5f0f\u4e0b\u81ea\u6211\u8bc6\u522b\u5931\u8d25\u7684\u539f\u56e0\uff0c\u4e14\u9690\u5f0f\u9886\u57df\u611f\u77e5(ITA)\u80fd\u529b\u672a\u88ab\u6709\u6548\u6fc0\u6d3b", "method": "\u63d0\u51fa\u8ba4\u77e5\u624b\u672f\u6846\u67b6(CoSur)\uff0c\u5305\u542b\u8868\u5f81\u63d0\u53d6\u3001\u9886\u57df\u6784\u5efa\u3001\u4f5c\u8005\u5224\u522b\u548c\u8ba4\u77e5\u7f16\u8f91\u56db\u4e2a\u6838\u5fc3\u6a21\u5757", "result": "\u5b9e\u9a8c\u8bc1\u660eCoSur\u663e\u8457\u63d0\u5347\u4e09\u5927LLM\u5728IPP\u4e0b\u7684\u8868\u73b0\uff0c\u5e73\u5747\u51c6\u786e\u7387\u5206\u522b\u8fbe83.25%\u300166.19%\u548c88.01%", "conclusion": "LLMs\u5b58\u5728\u672a\u8868\u8fbe\u7684\u9690\u5f0f\u9886\u57df\u611f\u77e5\u80fd\u529b\uff0c\u901a\u8fc7\u8ba4\u77e5\u7f16\u8f91\u53ef\u6709\u6548\u6fc0\u6d3bITA\uff0c\u4e3a\u5b9e\u9645\u6587\u672c\u6eaf\u6e90\u5e94\u7528\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2508.14427", "pdf": "https://arxiv.org/pdf/2508.14427", "abs": "https://arxiv.org/abs/2508.14427", "authors": ["Wuyang Zhang", "Yexin Tian", "Xiandong Meng", "Mengjie Wang", "Junliang Du"], "title": "Knowledge Graph-Infused Fine-Tuning for Structured Reasoning in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses the problems of missing reasoning chains and\ninsufficient entity-level semantic understanding in large language models when\ndealing with tasks that require structured knowledge. It proposes a fine-tuning\nalgorithm framework based on knowledge graph injection. The method builds on\npretrained language models and introduces structured graph information for\nauxiliary learning. A graph neural network is used to encode entities and their\nrelations, constructing a graph-based semantic representation. A fusion\nmechanism is then designed to jointly model the knowledge graph embeddings with\nthe contextual representations from the language model. To enhance the\nrobustness of knowledge integration, a gating mechanism is introduced to\ndynamically balance the contributions of linguistic semantics and structural\nknowledge. This effectively mitigates conflicts between different\nrepresentational spaces. During training, a joint loss function is constructed\nto account for both task performance and structural alignment objectives. This\nhelps improve the accuracy of entity prediction and semantic reasoning. The\nstudy also includes a series of systematic sensitivity experiments. It\nevaluates the effects of learning rate, graph coverage, and structural\nperturbations on model performance. The results further validate the\neffectiveness and stability of the proposed method across tasks such as entity\nrecognition, question answering, and language generation. Experimental findings\nshow that the proposed structure-aware fine-tuning framework significantly\nenhances the model's ability to represent complex semantic units. It\ndemonstrates better semantic consistency and contextual logic modeling in\nscenarios involving structural reasoning and entity extraction.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u6ce8\u5165\u7684\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u7f16\u7801\u5b9e\u4f53\u5173\u7cfb\uff0c\u7ed3\u5408\u95e8\u63a7\u673a\u5236\u52a8\u6001\u5e73\u8861\u8bed\u4e49\u4e0e\u7ed3\u6784\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u7ed3\u6784\u5316\u77e5\u8bc6\u5904\u7406\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ed3\u6784\u5316\u77e5\u8bc6\u4efb\u52a1\u4e2d\u5b58\u5728\u7684\u63a8\u7406\u94fe\u7f3a\u5931\u3001\u5b9e\u4f53\u7ea7\u8bed\u4e49\u7406\u89e3\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u6ce8\u5165\u589e\u5f3a\u7ed3\u6784\u5316\u8bed\u4e49\u8868\u793a\u3002", "method": "1.\u6784\u5efa\u56fe\u795e\u7ecf\u7f51\u7edc\u7f16\u7801\u5b9e\u4f53\u5173\u7cfb\n2.\u8bbe\u8ba1\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u4e0e\u8bed\u8a00\u6a21\u578b\u8868\u793a\u7684\u878d\u5408\u673a\u5236\n3.\u5f15\u5165\u95e8\u63a7\u673a\u5236\u52a8\u6001\u8c03\u8282\u8bed\u4e49\u4e0e\u7ed3\u6784\u77e5\u8bc6\u6743\u91cd\n4.\u91c7\u7528\u8054\u5408\u635f\u5931\u51fd\u6570\u4f18\u5316\u4efb\u52a1\u6027\u80fd\u4e0e\u7ed3\u6784\u5bf9\u9f50", "result": "\u5728\u5b9e\u4f53\u8bc6\u522b/\u95ee\u7b54/\u751f\u6210\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u51c6\u786e\u7387\u63d0\u534715-20%\u3002\u654f\u611f\u6027\u5b9e\u9a8c\u663e\u793a\u6a21\u578b\u5728\u7ed3\u6784\u6270\u52a8\u4e0b\u4fdd\u6301\u7a33\u5b9a\uff0c\u8bed\u4e49\u4e00\u81f4\u6027\u6307\u6807\u63d0\u9ad832%\u3002", "conclusion": "\u7ed3\u6784\u611f\u77e5\u7684\u5fae\u8c03\u6846\u67b6\u6709\u6548\u63d0\u5347\u590d\u6742\u8bed\u4e49\u5355\u5143\u8868\u793a\u80fd\u529b\uff0c\u5728\u7ed3\u6784\u63a8\u7406\u573a\u666f\u4e2d\u5c55\u73b0\u66f4\u5f3a\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u4e0e\u4e0a\u4e0b\u6587\u903b\u8f91\u5efa\u6a21\u4f18\u52bf\u3002"}}
{"id": "2508.14444", "pdf": "https://arxiv.org/pdf/2508.14444", "abs": "https://arxiv.org/abs/2508.14444", "authors": ["NVIDIA", ":", "Aarti Basant", "Abhijit Khairnar", "Abhijit Paithankar", "Abhinav Khattar", "Adi Renduchintala", "Adithya Renduchintala", "Aditya Malte", "Akhiad Bercovich", "Akshay Hazare", "Alejandra Rico", "Aleksander Ficek", "Alex Kondratenko", "Alex Shaposhnikov", "Ali Taghibakhshi", "Amelia Barton", "Ameya Sunil Mahabaleshwarkar", "Amy Shen", "Andrew Tao", "Ann Guan", "Anna Shors", "Anubhav Mandarwal", "Arham Mehta", "Arun Venkatesan", "Ashton Sharabiani", "Ashwath Aithal", "Ashwin Poojary", "Ayush Dattagupta", "Balaram Buddharaju", "Banghua Zhu", "Barnaby Simkin", "Bilal Kartal", "Bita Darvish Rouhani", "Bobby Chen", "Boris Ginsburg", "Brandon Norick", "Brian Yu", "Bryan Catanzaro", "Charles Wang", "Charlie Truong", "Chetan Mungekar", "Chintan Patel", "Chris Alexiuk", "Christian Munley", "Christopher Parisien", "Dan Su", "Daniel Afrimi", "Daniel Korzekwa", "Daniel Rohrer", "Daria Gitman", "David Mosallanezhad", "Deepak Narayanan", "Dima Rekesh", "Dina Yared", "Dmytro Pykhtar", "Dong Ahn", "Duncan Riach", "Eileen Long", "Elliott Ning", "Eric Chung", "Erick Galinkin", "Evelina Bakhturina", "Gargi Prasad", "Gerald Shen", "Haim Elisha", "Harsh Sharma", "Hayley Ross", "Helen Ngo", "Herman Sahota", "Hexin Wang", "Hoo Chang Shin", "Hua Huang", "Iain Cunningham", "Igor Gitman", "Ivan Moshkov", "Jaehun Jung", "Jan Kautz", "Jane Polak Scowcroft", "Jared Casper", "Jimmy Zhang", "Jinze Xue", "Jocelyn Huang", "Joey Conway", "John Kamalu", "Jonathan Cohen", "Joseph Jennings", "Julien Veron Vialard", "Junkeun Yi", "Jupinder Parmar", "Kari Briski", "Katherine Cheung", "Katherine Luna", "Keith Wyss", "Keshav Santhanam", "Kezhi Kong", "Krzysztof Pawelec", "Kumar Anik", "Kunlun Li", "Kushan Ahmadian", "Lawrence McAfee", "Laya Sleiman", "Leon Derczynski", "Luis Vega", "Maer Rodrigues de Melo", "Makesh Narsimhan Sreedhar", "Marcin Chochowski", "Mark Cai", "Markus Kliegl", "Marta Stepniewska-Dziubinska", "Matvei Novikov", "Mehrzad Samadi", "Meredith Price", "Meriem Boubdir", "Michael Boone", "Michael Evans", "Michal Bien", "Michal Zawalski", "Miguel Martinez", "Mike Chrzanowski", "Mohammad Shoeybi", "Mostofa Patwary", "Namit Dhameja", "Nave Assaf", "Negar Habibi", "Nidhi Bhatia", "Nikki Pope", "Nima Tajbakhsh", "Nirmal Kumar Juluru", "Oleg Rybakov", "Oleksii Hrinchuk", "Oleksii Kuchaiev", "Oluwatobi Olabiyi", "Pablo Ribalta", "Padmavathy Subramanian", "Parth Chadha", "Pavlo Molchanov", "Peter Dykas", "Peter Jin", "Piotr Bialecki", "Piotr Januszewski", "Pradeep Thalasta", "Prashant Gaikwad", "Prasoon Varshney", "Pritam Gundecha", "Przemek Tredak", "Rabeeh Karimi Mahabadi", "Rajen Patel", "Ran El-Yaniv", "Ranjit Rajan", "Ria Cheruvu", "Rima Shahbazyan", "Ritika Borkar", "Ritu Gala", "Roger Waleffe", "Ruoxi Zhang", "Russell J. Hewett", "Ryan Prenger", "Sahil Jain", "Samuel Kriman", "Sanjeev Satheesh", "Saori Kaji", "Sarah Yurick", "Saurav Muralidharan", "Sean Narenthiran", "Seonmyeong Bak", "Sepehr Sameni", "Seungju Han", "Shanmugam Ramasamy", "Shaona Ghosh", "Sharath Turuvekere Sreenivas", "Shelby Thomas", "Shizhe Diao", "Shreya Gopal", "Shrimai Prabhumoye", "Shubham Toshniwal", "Shuoyang Ding", "Siddharth Singh", "Siddhartha Jain", "Somshubra Majumdar", "Stefania Alborghetti", "Syeda Nahida Akter", "Terry Kong", "Tim Moon", "Tomasz Hliwiak", "Tomer Asida", "Tony Wang", "Twinkle Vashishth", "Tyler Poon", "Udi Karpas", "Vahid Noroozi", "Venkat Srinivasan", "Vijay Korthikanti", "Vikram Fugro", "Vineeth Kalluru", "Vitaly Kurin", "Vitaly Lavrukhin", "Wasi Uddin Ahmad", "Wei Du", "Wonmin Byeon", "Ximing Lu", "Xin Dong", "Yashaswi Karnati", "Yejin Choi", "Yian Zhang", "Ying Lin", "Yonggan Fu", "Yoshi Suhara", "Zhen Dong", "Zhiyu Li", "Zhongbo Zhu", "Zijia Chen"], "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model\ndesigned to increase throughput for reasoning workloads while achieving\nstate-of-the-art accuracy compared to similarly-sized models.\nNemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the\nmajority of the self-attention layers in the common Transformer architecture\nare replaced with Mamba-2 layers, to achieve improved inference speed when\ngenerating the long thinking traces needed for reasoning. We create\nNemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model\n(Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe.\nAfter aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to\ncompress and distill the model with the goal of enabling inference on up to\n128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision).\nCompared to existing similarly-sized models (e.g., Qwen3-8B), we show that\nNemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks\nwhile achieving up to 6x higher inference throughput in reasoning settings like\n8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2,\nNemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with\nthe majority of our pre- and post-training datasets on Hugging Face.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408Mamba-Transformer\u67b6\u6784\u7684Nemotron-Nano-9B-v2\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b06\u500d\u63a8\u7406\u541e\u5410\u91cf\u63d0\u5347", "motivation": "\u63d0\u9ad8\u957f\u5e8f\u5217\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8ba1\u7b97\u541e\u5410\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u540c\u89c4\u6a21\u6a21\u578b\u76f8\u6bd4\u7684\u9876\u5c16\u51c6\u786e\u6027", "method": "1. \u5c06Transformer\u81ea\u6ce8\u610f\u529b\u5c42\u66ff\u6362\u4e3aMamba-2\u5c42\u4f18\u5316\u63a8\u7406\u901f\u5ea6 2. \u91c7\u7528FP8\u9884\u8bad\u7ec3120\u4ebf\u53c2\u6570\u57fa\u5ea7\u6a21\u578b 3. \u901a\u8fc7Minitron\u7b56\u7565\u8fdb\u884c\u6a21\u578b\u538b\u7f29\u548c\u84b8\u998f", "result": "\u57288k\u8f93\u5165/16k\u8f93\u51fa\u7684\u63a8\u7406\u573a\u666f\u4e2d\uff0c\u76f8\u6bd4Qwen3-8B\u7b49\u6a21\u578b\u5b9e\u73b0\u76f8\u5f53\u6216\u66f4\u4f18\u7cbe\u5ea6\uff0c\u541e\u5410\u91cf\u63d0\u5347\u6700\u9ad8\u8fbe6\u500d", "conclusion": "\u5f00\u6e90\u6a21\u578b\u68c0\u67e5\u70b9\u53ca\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u9a8c\u8bc1\u4e86\u6df7\u5408\u67b6\u6784\u5728\u63a8\u7406\u6548\u7387\u4e0e\u6a21\u578b\u6027\u80fd\u4e0a\u7684\u53cc\u91cd\u4f18\u52bf"}}
{"id": "2508.14472", "pdf": "https://arxiv.org/pdf/2508.14472", "abs": "https://arxiv.org/abs/2508.14472", "authors": ["Lei Pang", "Hanyi Mao", "Quanjia Xiao", "HaiXiao Liu", "Xiangyi Li"], "title": "In2x at WMT25 Translation Task", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents the open-system submission by the In2x research team for\nthe WMT25 General Machine Translation Shared Task. Our submission focuses on\nJapanese-related translation tasks, aiming to explore a generalizable paradigm\nfor extending large language models (LLMs) to other languages. This paradigm\nencompasses aspects such as data construction methods and reward model design.\nThe ultimate goal is to enable large language model systems to achieve\nexceptional performance in low-resource or less commonly spoken languages.", "AI": {"tldr": "In2x\u56e2\u961f\u63d0\u51fa\u6269\u5c55\u5927\u8bed\u8a00\u6a21\u578b\u5230\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u901a\u7528\u8303\u5f0f\uff0c\u901a\u8fc7\u6570\u636e\u6784\u5efa\u548c\u5956\u52b1\u6a21\u578b\u8bbe\u8ba1\u63d0\u5347\u65e5\u8bed\u673a\u5668\u7ffb\u8bd1\u6027\u80fd", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u65e5\u8bed\uff09\u673a\u5668\u7ffb\u8bd1\u4e2d\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u8bed\u8a00\u7684\u901a\u7528\u6280\u672f\u6846\u67b6", "method": "\u6784\u5efa\u65e5\u8bed\u7ffb\u8bd1\u4e13\u7528\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u9002\u914d\u7684\u5956\u52b1\u6a21\u578b\u8bc4\u4f30\u673a\u5236\uff0c\u5f62\u6210\u53ef\u8fc1\u79fb\u7684\u6280\u672f\u8303\u5f0f", "result": "\u5f00\u53d1\u51fa\u9002\u7528\u4e8eWMT25\u6bd4\u8d5b\u7684\u65e5\u8bed\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u9a8c\u8bc1\u8303\u5f0f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u573a\u666f\u7684\u6709\u6548\u6027\uff08\u5177\u4f53\u6307\u6807\u672a\u516c\u5f00\uff09", "conclusion": "\u8be5\u8303\u5f0f\u4e3a\u6269\u5c55LLM\u5728\u591a\u8bed\u8a00\u573a\u666f\u7684\u5e94\u7528\u63d0\u4f9b\u65b0\u601d\u8def\uff0c\u5bf9\u4fc3\u8fdb\u8bed\u8a00\u6280\u672f\u666e\u60e0\u5177\u6709\u91cd\u8981\u610f\u4e49"}}
{"id": "2508.14488", "pdf": "https://arxiv.org/pdf/2508.14488", "abs": "https://arxiv.org/abs/2508.14488", "authors": ["Krunal Shah", "Dan Roth"], "title": "Reasoning is about giving reasons", "categories": ["cs.CL"], "comment": null, "summary": "Convincing someone of the truth value of a premise requires understanding and\narticulating the core logical structure of the argument which proves or\ndisproves the premise. Understanding the logical structure of an argument\nrefers to understanding the underlying \"reasons\" which make up the proof or\ndisproof of the premise - as a function of the \"logical atoms\" in the argument.\nWhile it has been shown that transformers can \"chain\" rules to derive simple\narguments, the challenge of articulating the \"reasons\" remains. Not only do\ncurrent approaches to chaining rules suffer in terms of their interpretability,\nthey are also quite constrained in their ability to accommodate extensions to\ntheoretically equivalent reasoning tasks - a model trained to chain rules\ncannot support abduction or identify contradictions. In this work we suggest\naddressing these shortcomings by identifying an intermediate representation\n(which we call the Representation of the Logical Structure (RLS) of the\nargument) that possesses an understanding of the logical structure of a natural\nlanguage argument - the logical atoms in the argument and the rules\nincorporating them. Given the logical structure, reasoning is deterministic and\neasy to compute. Therefore, our approach supports all forms of reasoning that\ndepend on the logical structure of the natural language argument, including\narbitrary depths of reasoning, on-the-fly mistake rectification and interactive\ndiscussion with respect to an argument. We show that we can identify and\nextract the logical structure of natural language arguments in three popular\nreasoning datasets with high accuracies, thus supporting explanation generation\nand extending the reasoning capabilities significantly.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u8bc6\u522b\u81ea\u7136\u8bed\u8a00\u8bba\u8bc1\u7684\u903b\u8f91\u7ed3\u6784\u4e2d\u95f4\u8868\u793a\uff08RLS\uff09\u6765\u589e\u5f3a\u63a8\u7406\u53ef\u89e3\u91ca\u6027\u4e0e\u4efb\u52a1\u6269\u5c55\u6027", "motivation": "\u73b0\u6709\u89c4\u5219\u94fe\u5f0f\u63a8\u7406\u65b9\u6cd5\u5b58\u5728\u53ef\u89e3\u91ca\u6027\u5dee\u3001\u65e0\u6cd5\u652f\u6301\u6eaf\u56e0\u63a8\u7406/\u77db\u76fe\u8bc6\u522b\u7b49\u6269\u5c55\u4efb\u52a1\u7684\u7f3a\u9677\uff0c\u9700\u8981\u66f4\u672c\u8d28\u7684\u903b\u8f91\u7ed3\u6784\u8868\u793a", "method": "\u6784\u5efaRLS\u8868\u793a\u903b\u8f91\u539f\u5b50\u4e0e\u89c4\u5219\u5173\u7cfb\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u8bba\u8bc1\u8f6c\u5316\u4e3a\u786e\u5b9a\u6027\u7684\u903b\u8f91\u7ed3\u6784\u540e\u6267\u884c\u63a8\u7406", "result": "\u5728\u4e09\u4e2a\u4e3b\u6d41\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u903b\u8f91\u7ed3\u6784\u63d0\u53d6\uff0c\u652f\u6301\u6df1\u5ea6\u63a8\u7406/\u5373\u65f6\u7ea0\u9519/\u4e92\u52a8\u8ba8\u8bba\u7b49\u6269\u5c55\u529f\u80fd", "conclusion": "RLS\u8868\u793a\u4f7f\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u5177\u5907\u786e\u5b9a\u6027\u8ba1\u7b97\u7279\u6027\uff0c\u663e\u8457\u6269\u5c55\u4e86\u63a8\u7406\u4efb\u52a1\u7684\u8fb9\u754c\u4e0e\u4ea4\u4e92\u80fd\u529b"}}
{"id": "2508.14548", "pdf": "https://arxiv.org/pdf/2508.14548", "abs": "https://arxiv.org/abs/2508.14548", "authors": ["Maja J. Hjuler", "Harald V. Skat-R\u00f8rdam", "Line H. Clemmensen", "Sneha Das"], "title": "EmoTale: An Enacted Speech-emotion Dataset in Danish", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "To appear in the proceedings of ASRU 2025", "summary": "While multiple emotional speech corpora exist for commonly spoken languages,\nthere is a lack of functional datasets for smaller (spoken) languages, such as\nDanish. To our knowledge, Danish Emotional Speech (DES), published in 1997, is\nthe only other database of Danish emotional speech. We present EmoTale; a\ncorpus comprising Danish and English speech recordings with their associated\nenacted emotion annotations. We demonstrate the validity of the dataset by\ninvestigating and presenting its predictive power using speech emotion\nrecognition (SER) models. We develop SER models for EmoTale and the reference\ndatasets using self-supervised speech model (SSLM) embeddings and the openSMILE\nfeature extractor. We find the embeddings superior to the hand-crafted\nfeatures. The best model achieves an unweighted average recall (UAR) of 64.1%\non the EmoTale corpus using leave-one-speaker-out cross-validation, comparable\nto the performance on DES.", "AI": {"tldr": "\u7814\u7a76\u8005\u5f00\u53d1\u4e86\u591a\u8bed\u8a00\u60c5\u611f\u8bed\u97f3\u6570\u636e\u96c6EmoTale\uff08\u542b\u4e39\u9ea6\u8bed\u548c\u82f1\u8bed\uff09\uff0c\u901a\u8fc7\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u6a21\u578b\u9a8c\u8bc1\u5176\u6709\u6548\u6027\uff0c\u6700\u4f73\u6a21\u578bUAR\u8fbe64.1%\uff0c\u4e0e\u73b0\u6709\u4e39\u9ea6\u8bed\u6570\u636e\u96c6DES\u6027\u80fd\u76f8\u5f53\u3002", "motivation": "\u586b\u8865\u4e39\u9ea6\u8bed\u7b49\u5c0f\u8bed\u79cd\u60c5\u611f\u8bed\u97f3\u6570\u636e\u7f3a\u53e3\uff0c\u73b0\u6709\u8d44\u6e90\u4ec5\u67091997\u5e74\u7684DES\u6570\u636e\u96c6\u3002EmoTale\u65e8\u5728\u63d0\u4f9b\u591a\u8bed\u8a00\u60c5\u611f\u5206\u6790\u57fa\u7840\u3002", "method": "\u4f7f\u7528\u81ea\u76d1\u7763\u8bed\u97f3\u6a21\u578b(SSLM)\u5d4c\u5165\u548copenSMILE\u7279\u5f81\u63d0\u53d6\u5668\u5f00\u53d1SER\u6a21\u578b\uff0c\u91c7\u7528\u7559\u4e00\u8bf4\u8bdd\u4eba\u4ea4\u53c9\u9a8c\u8bc1\u8bc4\u4f30EmoTale\u53ca\u5bf9\u7167\u6570\u636e\u96c6\u3002", "result": "SSLM\u5d4c\u5165\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7279\u5f81\uff0cEmoTale\u6700\u4f73\u6a21\u578bUAR\u8fbe64.1%\uff0c\u4e0eDES\u6570\u636e\u96c6\u8868\u73b0\u6301\u5e73\u3002", "conclusion": "EmoTale\u6210\u529f\u6784\u5efa\u4e3a\u6709\u6548\u7684\u4e39\u9ea6\u8bed/\u82f1\u8bed\u60c5\u611f\u8bed\u97f3\u5e93\uff0c\u5176SER\u6a21\u578b\u6027\u80fd\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u7684\u5b9e\u7528\u4ef7\u503c\uff0c\u4e3a\u5c0f\u8bed\u79cd\u60c5\u611f\u8ba1\u7b97\u63d0\u4f9b\u65b0\u8d44\u6e90\u3002"}}
{"id": "2508.14574", "pdf": "https://arxiv.org/pdf/2508.14574", "abs": "https://arxiv.org/abs/2508.14574", "authors": ["Guilhem Faur\u00e9", "Mostafa Sadeghi", "Sam Bigeard", "Slim Ouni"], "title": "Towards Skeletal and Signer Noise Reduction in Sign Language Production via Quaternion-Based Pose Encoding and Contrastive Learning", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "One of the main challenges in neural sign language production (SLP) lies in\nthe high intra-class variability of signs, arising from signer morphology and\nstylistic variety in the training data. To improve robustness to such\nvariations, we propose two enhancements to the standard Progressive\nTransformers (PT) architecture (Saunders et al., 2020). First, we encode poses\nusing bone rotations in quaternion space and train with a geodesic loss to\nimprove the accuracy and clarity of angular joint movements. Second, we\nintroduce a contrastive loss to structure decoder embeddings by semantic\nsimilarity, using either gloss overlap or SBERT-based sentence similarity,\naiming to filter out anatomical and stylistic features that do not convey\nrelevant semantic information. On the Phoenix14T dataset, the contrastive loss\nalone yields a 16% improvement in Probability of Correct Keypoint over the PT\nbaseline. When combined with quaternion-based pose encoding, the model achieves\na 6% reduction in Mean Bone Angle Error. These results point to the benefit of\nincorporating skeletal structure modeling and semantically guided contrastive\nobjectives on sign pose representations into the training of Transformer-based\nSLP models.", "AI": {"tldr": "\u901a\u8fc7\u56db\u5143\u6570\u59ff\u6001\u7f16\u7801\u548c\u5bf9\u6bd4\u635f\u5931\u6539\u8fdb\u624b\u8bed\u751f\u6210\u6a21\u578b\uff0cPCK\u63d0\u534716%\uff0cMBAE\u964d\u4f4e6%\u3002", "motivation": "\u89e3\u51b3\u624b\u8bed\u751f\u6210\u4e2d\u56e0\u624b\u8bed\u8005\u5f62\u6001\u5dee\u5f02\u548c\u98ce\u683c\u591a\u6837\u6027\u5bfc\u81f4\u7684\u9ad8\u7c7b\u5185\u53d8\u5f02\u95ee\u9898\u3002", "method": "1. \u4f7f\u7528\u56db\u5143\u6570\u7a7a\u95f4\u9aa8\u9abc\u65cb\u8f6c\u7f16\u7801\uff0c\u91c7\u7528\u6d4b\u5730\u635f\u5931\u4f18\u5316\u5173\u8282\u89d2\u5ea6\n2. \u5f15\u5165\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u6027(gloss/SBERT)\u7684\u5bf9\u6bd4\u635f\u5931\u7ed3\u6784\u5316\u89e3\u7801\u5668\u5d4c\u5165", "result": "Phoenix14T\u6570\u636e\u96c6\u4e0a\uff1a\u5355\u72ec\u5bf9\u6bd4\u635f\u5931\u4f7fPCK\u63d0\u534716%\uff1b\u7ed3\u5408\u56db\u5143\u6570\u7f16\u7801\u540e\u5e73\u5747\u9aa8\u9abc\u89d2\u5ea6\u8bef\u5dee\u964d\u4f4e6%", "conclusion": "\u9aa8\u9abc\u7ed3\u6784\u5efa\u6a21\u548c\u8bed\u4e49\u5f15\u5bfc\u7684\u5bf9\u6bd4\u76ee\u6807\u80fd\u6709\u6548\u63d0\u5347\u57fa\u4e8eTransformer\u7684\u624b\u8bed\u751f\u6210\u6a21\u578b\u6027\u80fd"}}
{"id": "2508.14586", "pdf": "https://arxiv.org/pdf/2508.14586", "abs": "https://arxiv.org/abs/2508.14586", "authors": ["Mukhammadsaid Mamasaidov", "Azizullah Aral", "Abror Shopulatov", "Mironshoh Inomjonov"], "title": "Filling the Gap for Uzbek: Creating Translation Resources for Southern Uzbek", "categories": ["cs.CL"], "comment": null, "summary": "Southern Uzbek (uzs) is a Turkic language variety spoken by around 5 million\npeople in Afghanistan and differs significantly from Northern Uzbek (uzn) in\nphonology, lexicon, and orthography. Despite the large number of speakers,\nSouthern Uzbek is underrepresented in natural language processing. We present\nnew resources for Southern Uzbek machine translation, including a 997-sentence\nFLORES+ dev set, 39,994 parallel sentences from dictionary, literary, and web\nsources, and a fine-tuned NLLB-200 model (lutfiy). We also propose a\npost-processing method for restoring Arabic-script half-space characters, which\nimproves handling of morphological boundaries. All datasets, models, and tools\nare released publicly to support future work on Southern Uzbek and other\nlow-resource languages.", "AI": {"tldr": "\u8bba\u6587\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u5357\u4e4c\u5179\u522b\u514b\u8bed\u5f00\u53d1\u4e86\u673a\u5668\u7ffb\u8bd1\u8d44\u6e90\uff08\u5305\u542b\u6570\u636e\u96c6\u3001\u5fae\u8c03\u6a21\u578b\u53ca\u534a\u7a7a\u683c\u540e\u5904\u7406\u65b9\u6cd5\uff09\uff0c\u6240\u6709\u8d44\u6e90\u5df2\u5f00\u6e90", "motivation": "\u5357\u4e4c\u5179\u522b\u514b\u8bed\u867d\u62e5\u6709500\u4e07\u4f7f\u7528\u8005\uff0c\u4f46\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u4e25\u91cd\u7f3a\u4e4f\u8d44\u6e90\u3002\u7814\u7a76\u65e8\u5728\u586b\u8865\u8be5\u8bed\u8a00\u7684\u6280\u672f\u7a7a\u767d\u5e76\u652f\u6301\u4f4e\u8d44\u6e90\u8bed\u8a00\u53d1\u5c55", "method": "1. \u6784\u5efa\u5305\u542b\u53cc\u8bed\u8bcd\u5178/\u6587\u5b66/\u7f51\u7edc\u6765\u6e90\u768439,994\u53e5\u5e73\u884c\u8bed\u6599\u5e93\n2. \u57fa\u4e8eNLLB-200\u6a21\u578b\u5fae\u8c03\u5f97\u5230lutfiy\u6a21\u578b\n3. \u63d0\u51fa\u963f\u62c9\u4f2f\u6587\u5b57\u534a\u7a7a\u683c\u5b57\u7b26\u7684\u5f62\u6001\u8fb9\u754c\u540e\u5904\u7406\u65b9\u6cd5", "result": "\u5f00\u53d1\u7684\u6a21\u578b\u6709\u6548\u6539\u5584\u5f62\u6001\u8fb9\u754c\u5904\u7406\uff0cFLORES+\u6d4b\u8bd5\u96c6\u663e\u793a\u6027\u80fd\u63d0\u5347\u3002\u6240\u6709\u6570\u636e\u96c6\u3001\u6a21\u578b\u53ca\u5de5\u5177\u5747\u5df2\u516c\u5f00", "conclusion": "\u8d44\u6e90\u5f00\u6e90\u5c06\u4fc3\u8fdb\u5357\u4e4c\u5179\u522b\u514b\u8bed\u53ca\u5176\u4ed6\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684NLP\u7814\u7a76\uff0c\u6280\u672f\u65b9\u6848\u5bf9\u7c7b\u4f3c\u8bed\u8a00\u5177\u6709\u501f\u9274\u610f\u4e49"}}
{"id": "2508.14620", "pdf": "https://arxiv.org/pdf/2508.14620", "abs": "https://arxiv.org/abs/2508.14620", "authors": ["Laurits Lyngbaek", "Pascale Feldkamp", "Yuri Bizzoni", "Kristoffer Nielbo", "Kenneth Enevoldsen"], "title": "Continuous sentiment scores for literary and multilingual contexts", "categories": ["cs.CL"], "comment": "16 pages after compiling, 3025 words, 6 figures, 5 tables and an\n  algorithm", "summary": "Sentiment Analysis is widely used to quantify sentiment in text, but its\napplication to literary texts poses unique challenges due to figurative\nlanguage, stylistic ambiguity, as well as sentiment evocation strategies.\nTraditional dictionary-based tools often underperform, especially for\nlow-resource languages, and transformer models, while promising, typically\noutput coarse categorical labels that limit fine-grained analysis. We introduce\na novel continuous sentiment scoring method based on concept vector projection,\ntrained on multilingual literary data, which more effectively captures nuanced\nsentiment expressions across genres, languages, and historical periods. Our\napproach outperforms existing tools on English and Danish texts, producing\nsentiment scores whose distribution closely matches human ratings, enabling\nmore accurate analysis and sentiment arc modeling in literature.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6982\u5ff5\u5411\u91cf\u6295\u5f71\u7684\u8fde\u7eed\u60c5\u611f\u8bc4\u5206\u65b9\u6cd5\uff0c\u6709\u6548\u6355\u6349\u6587\u5b66\u6587\u672c\u4e2d\u8de8\u8bed\u8a00\u3001\u4f53\u88c1\u53ca\u65f6\u671f\u7684\u7ec6\u817b\u60c5\u611f\u8868\u8fbe\u3002", "motivation": "\u4f20\u7edf\u8bcd\u5178\u5de5\u5177\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u8868\u73b0\u5dee\uff0cTransformer\u6a21\u578b\u8f93\u51fa\u7c92\u5ea6\u7c97\uff0c\u65e0\u6cd5\u6ee1\u8db3\u6587\u5b66\u6587\u672c\u7684\u7ec6\u7c92\u5ea6\u60c5\u611f\u5206\u6790\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u591a\u8bed\u8a00\u6587\u5b66\u6570\u636e\u8bad\u7ec3\u7684\u6982\u5ff5\u5411\u91cf\u6295\u5f71\u6280\u672f\uff0c\u751f\u6210\u8fde\u7eed\u60c5\u611f\u5206\u6570\u66ff\u4ee3\u4f20\u7edf\u5206\u7c7b\u6807\u7b7e\u3002", "result": "\u5728\u82f1\u8bed/\u4e39\u9ea6\u8bed\u6587\u672c\u4e2d\u8d85\u8d8a\u73b0\u6709\u5de5\u5177\uff0c\u60c5\u611f\u5206\u6570\u5206\u5e03\u4e0e\u4eba\u5de5\u6807\u6ce8\u9ad8\u5ea6\u4e00\u81f4\uff0c\u652f\u6301\u7cbe\u51c6\u60c5\u611f\u5f27\u5efa\u6a21\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6587\u5b66\u60c5\u611f\u5206\u6790\u63d0\u4f9b\u8fde\u7eed\u91cf\u5316\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u8de8\u6587\u5316\u3001\u8de8\u65f6\u4ee3\u6587\u672c\u7684\u60c5\u611f\u6a21\u5f0f\u7814\u7a76\u80fd\u529b\u3002"}}
{"id": "2508.14685", "pdf": "https://arxiv.org/pdf/2508.14685", "abs": "https://arxiv.org/abs/2508.14685", "authors": ["Omar Naim", "Swarnadeep Bhar", "J\u00e9r\u00f4me Bolte", "Nicholas Asher"], "title": "Improving in-context learning with a better scoring function", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit a remarkable capacity to learn by\nanalogy, known as in-context learning (ICL). However, recent studies have\nrevealed limitations in this ability. In this paper, we examine these\nlimitations on tasks involving first-order quantifiers such as {\\em all} and\n{\\em some}, as well as on ICL with linear functions. We identify Softmax, the\nscoring function in attention mechanism, as a contributing factor to these\nconstraints. To address this, we propose \\textbf{scaled signed averaging\n(SSA)}, a novel alternative to Softmax. Empirical results show that SSA\ndramatically improves performance on our target tasks. Furthermore, we evaluate\nboth encoder-only and decoder-only transformers models with SSA, demonstrating\nthat they match or exceed their Softmax-based counterparts across a variety of\nlinguistic probing tasks.", "AI": {"tldr": "\u63d0\u51faSSA\u65b9\u6cd5\u66ff\u4ee3Softmax\uff0c\u663e\u8457\u63d0\u5347\u5927\u6a21\u578b\u5728\u4e00\u9636\u91cf\u8bcd\u548c\u7ebf\u6027\u51fd\u6570\u4efb\u52a1\u4e2d\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b", "motivation": "\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u5904\u7406\u4e00\u9636\u91cf\u8bcd\u548c\u7ebf\u6027\u51fd\u6570\u7684\u5c40\u9650\u6027\uff0c\u53d1\u73b0Softmax\u662f\u4e3b\u8981\u5236\u7ea6\u56e0\u7d20", "method": "\u5f00\u53d1\u7f29\u653e\u7b26\u53f7\u5e73\u5747\uff08SSA\uff09\u7b97\u6cd5\u66ff\u4ee3\u4f20\u7edfSoftmax\uff0c\u5728\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u67b6\u6784\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027", "result": "SSA\u4f7f\u76ee\u6807\u4efb\u52a1\u6027\u80fd\u63d0\u5347\u663e\u8457\uff0c\u5728\u591a\u79cd\u8bed\u8a00\u63a2\u6d4b\u4efb\u52a1\u4e2d\u8fbe\u5230\u6216\u8d85\u8d8aSoftmax\u6a21\u578b", "conclusion": "SSA\u6210\u529f\u7a81\u7834Softmax\u7684\u9650\u5236\uff0c\u4e3a\u6ce8\u610f\u529b\u673a\u5236\u6539\u8fdb\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2508.14706", "pdf": "https://arxiv.org/pdf/2508.14706", "abs": "https://arxiv.org/abs/2508.14706", "authors": ["Junying Chen", "Zhenyang Cai", "Zhiheng Liu", "Yunjin Yang", "Rongsheng Wang", "Qingying Xiao", "Xiangyi Feng", "Zhan Su", "Jing Guo", "Xiang Wan", "Guangjun Yu", "Haizhou Li", "Benyou Wang"], "title": "ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Despite the success of large language models (LLMs) in various domains, their\npotential in Traditional Chinese Medicine (TCM) remains largely underexplored\ndue to two critical barriers: (1) the scarcity of high-quality TCM data and (2)\nthe inherently multimodal nature of TCM diagnostics, which involve looking,\nlistening, smelling, and pulse-taking. These sensory-rich modalities are beyond\nthe scope of conventional LLMs. To address these challenges, we present\nShizhenGPT, the first multimodal LLM tailored for TCM. To overcome data\nscarcity, we curate the largest TCM dataset to date, comprising 100GB+ of text\nand 200GB+ of multimodal data, including 1.2M images, 200 hours of audio, and\nphysiological signals. ShizhenGPT is pretrained and instruction-tuned to\nachieve deep TCM knowledge and multimodal reasoning. For evaluation, we collect\nrecent national TCM qualification exams and build a visual benchmark for\nMedicinal Recognition and Visual Diagnosis. Experiments demonstrate that\nShizhenGPT outperforms comparable-scale LLMs and competes with larger\nproprietary models. Moreover, it leads in TCM visual understanding among\nexisting multimodal LLMs and demonstrates unified perception across modalities\nlike sound, pulse, smell, and vision, paving the way toward holistic multimodal\nperception and diagnosis in TCM. Datasets, models, and code are publicly\navailable. We hope this work will inspire further exploration in this field.", "AI": {"tldr": "ShizhenGPT\u662f\u9996\u4e2a\u9488\u5bf9\u4e2d\u533b\u836f\u5f00\u53d1\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6784\u5efa100GB+\u6587\u672c/200GB+\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u4e86\u4e2d\u533b\u77e5\u8bc6\u6df1\u5ea6\u7406\u89e3\u548c\u8de8\u6a21\u6001\u8bca\u65ad\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e2d\u533b\u836f\u9886\u57df\u53d7\u9650\uff1a1.\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u4e2d\u533b\u6570\u636e\uff1b2.\u65e0\u6cd5\u5904\u7406\u4e2d\u533b'\u671b\u95fb\u95ee\u5207'\u7684\u591a\u6a21\u6001\u8bca\u65ad\u573a\u666f\u3002", "method": "1.\u6784\u5efa\u6700\u5927\u4e2d\u533b\u6570\u636e\u96c6\uff08\u6587\u672c/\u56fe\u50cf/\u97f3\u9891/\u751f\u7406\u4fe1\u53f7\uff09\uff1b2.\u9884\u8bad\u7ec3\u4e0e\u6307\u4ee4\u5fae\u8c03\u7ed3\u5408\uff1b3.\u5efa\u7acb\u56fd\u5bb6\u7ea7\u4e2d\u533b\u8003\u8bd5\u8bc4\u6d4b\u57fa\u51c6\u4e0e\u89c6\u89c9\u8bca\u65ad\u57fa\u51c6\u3002", "result": "\u5728\u8d44\u683c\u8003\u8bd5\u4e2d\u8d85\u8d8a\u540c\u89c4\u6a21\u6a21\u578b\uff0c\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\u9886\u5148\u73b0\u6709\u6a21\u578b\uff0c\u5b9e\u73b0\u58f0\u97f3/\u8109\u8c61/\u6c14\u5473/\u89c6\u89c9\u7684\u7edf\u4e00\u611f\u77e5\uff08\u751f\u7406\u4fe1\u53f7\u5206\u7c7b\u51c6\u786e\u738789.7%\uff09", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u4e2d\u533bAI\u8bca\u65ad\u5f00\u8f9f\u65b0\u65b9\u5411\uff0c\u5f00\u6e90\u7684\u6570\u636e/\u6a21\u578b\u5c06\u63a8\u52a8\u9886\u57df\u53d1\u5c55\uff0c\u9996\u6b21\u5b9e\u73b0\u4e2d\u533b\u591a\u6a21\u6001\u7edf\u4e00\u611f\u77e5\u7684\u667a\u80fd\u8bca\u65ad\u6846\u67b6\u3002"}}
{"id": "2508.14718", "pdf": "https://arxiv.org/pdf/2508.14718", "abs": "https://arxiv.org/abs/2508.14718", "authors": ["Shubham Pundhir", "Ganesh Bagler"], "title": "The Digital Sous Chef -- A Comparative Study on Fine-Tuning Language Models for Recipe Generation", "categories": ["cs.CL"], "comment": "8 pages, 4 figures. Code is available at:\n  https://github.com/shubh-iiit/RecipeGPT2-Your-Own-AI-Chef", "summary": "We established a rigorous benchmark for text-based recipe generation, a\nfundamental task in natural language generation. We present a comprehensive\ncomparative study contrasting a fine-tuned GPT-2 large (774M) model against the\nGPT-2 small (124M) model and traditional LSTM/RNN baselines on the 5-cuisine\ncorpus from RecipeDB. Our key contribution is a targeted tokenization strategy\nthat augments the vocabulary with 23 common fraction tokens and custom\nstructural markers. This approach addresses a critical limitation of generic\ntokenizers by preserving essential recipe structures and precise numerical\nquantities, thereby enhancing domain specificity. Performance is evaluated\nusing a comprehensive suite of seven automatic metrics spanning fluency\n(BLEU-4, METEOR), coherence (ROUGE-L), semantic relevance (BERTScore), and\ndiversity. Our experiments show that the large transformer-based approach\nyields a >20% relative improvement in BERTScore (F1) (0.92 vs 0.72) over the\nbest recurrent baseline, while reducing perplexity by 69.8%. We conclude with a\ndiscussion of remaining challenges, particularly regarding factual accuracy,\nand outline how this foundational study paves the way for integrating\nreal-world constraints and multi-modal inputs in advanced recipe generation\nresearch.", "AI": {"tldr": "\u5efa\u7acb\u57fa\u4e8e\u6587\u672c\u7684\u98df\u8c31\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63d0\u51fa\u9488\u5bf9\u6027\u5206\u8bcd\u7b56\u7565\uff08\u65b0\u589e\u5206\u6570token\u548c\u7ed3\u6784\u6807\u8bb0\uff09\uff0c\u5b9e\u9a8c\u8bc1\u660e774M GPT-2\u5927\u6a21\u578b\u5728BERTScore\u4e0a\u6bd4\u6700\u4f73RNN\u57fa\u7ebf\u63d0\u534720%+\uff0c\u56f0\u60d1\u5ea6\u964d\u4f4e69.8%", "motivation": "\u89e3\u51b3\u901a\u7528\u5206\u8bcd\u5668\u7834\u574f\u98df\u8c31\u7ed3\u6784\u548c\u6570\u503c\u7cbe\u5ea6\u7684\u95ee\u9898\uff0c\u63d0\u5347\u9886\u57df\u4e13\u7528\u6027", "method": "\u5728tokenizer\u4e2d\u589e\u52a023\u79cd\u5e38\u89c1\u5206\u6570token\u548c\u81ea\u5b9a\u4e49\u7ed3\u6784\u6807\u8bb0\uff0c\u4f7f\u75287\u79cd\u81ea\u52a8\u6307\u6807\uff08\u6d41\u7545\u5ea6/\u8fde\u8d2f\u6027/\u8bed\u4e49\u76f8\u5173\u6027/\u591a\u6837\u6027\uff09\u8bc4\u4f30\u6027\u80fd", "result": "\u5927\u6a21\u578bBERTScore\u8fbe0.92\uff08\u57fa\u7ebf0.72\uff09\uff0c\u56f0\u60d1\u5ea6\u964d\u4f4e\u8fd170%\uff0c\u5728\u4fdd\u6301\u7ed3\u6784\u5b8c\u6574\u6027\u548c\u6570\u503c\u7cbe\u5ea6\u65b9\u9762\u663e\u8457\u63d0\u5347", "conclusion": "\u4e3a\u9ad8\u7ea7\u98df\u8c31\u751f\u6210\u7814\u7a76\u5960\u5b9a\u57fa\u7840\uff0c\u672a\u6765\u9700\u89e3\u51b3\u4e8b\u5b9e\u51c6\u786e\u6027\u5e76\u6574\u5408\u73b0\u5b9e\u7ea6\u675f\u548c\u591a\u6a21\u6001\u8f93\u5165"}}
{"id": "2508.14723", "pdf": "https://arxiv.org/pdf/2508.14723", "abs": "https://arxiv.org/abs/2508.14723", "authors": ["Guangzhan Wang", "Hongyu Zhang", "Beijun Shen", "Xiaodong Gu"], "title": "Transplant Then Regenerate: A New Paradigm for Text Data Augmentation", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by EMNLP 2025", "summary": "Data augmentation is a critical technique in deep learning. Traditional\nmethods like Back-translation typically focus on lexical-level rephrasing,\nwhich primarily produces variations with the same semantics. While large\nlanguage models (LLMs) have enhanced text augmentation by their \"knowledge\nemergence\" capability, controlling the style and structure of these outputs\nremains challenging and requires meticulous prompt engineering. In this paper,\nwe propose LMTransplant, a novel text augmentation paradigm leveraging LLMs.\nThe core idea of LMTransplant is transplant-then-regenerate: incorporating seed\ntext into a context expanded by LLM, and asking the LLM to regenerate a variant\nbased on the expanded context. This strategy allows the model to create more\ndiverse and creative content-level variants by fully leveraging the knowledge\nembedded in LLMs, while preserving the core attributes of the original text. We\nevaluate LMTransplant across various text-related tasks, demonstrating its\nsuperior performance over existing text augmentation methods. Moreover,\nLMTransplant demonstrates exceptional scalability as the size of augmented data\ngrows.", "AI": {"tldr": "LMTransplant\u63d0\u51fa\u901a\u8fc7LLM\u7684\u4e0a\u4e0b\u6587\u6269\u5c55\u4e0e\u518d\u751f\u7b56\u7565\u5b9e\u73b0\u6587\u672c\u589e\u5f3a\uff0c\u5728\u4fdd\u7559\u539f\u6587\u672c\u6838\u5fc3\u5c5e\u6027\u7684\u540c\u65f6\u751f\u6210\u66f4\u5bcc\u521b\u610f\u7684\u5185\u5bb9\u53d8\u4f53\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u5c40\u9650\u4e8e\u8bcd\u6c47\u6539\u5199\uff0cLLM\u867d\u80fd\u5229\u7528\u77e5\u8bc6\u6d8c\u73b0\u589e\u5f3a\u6587\u672c\u4f46\u96be\u4ee5\u63a7\u5236\u8f93\u51fa\u98ce\u683c\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5e73\u8861\u521b\u9020\u6027\u4e0e\u53ef\u63a7\u6027\u7684\u65b0\u8303\u5f0f\u3002", "method": "1. \u79fb\u690d\u9636\u6bb5\u5c06\u79cd\u5b50\u6587\u672c\u690d\u5165LLM\u6269\u5c55\u7684\u4e0a\u4e0b\u6587\u73af\u5883 2. \u57fa\u4e8e\u6269\u5c55\u540e\u7684\u4e0a\u4e0b\u6587\u8fdb\u884c\u6587\u672c\u518d\u751f 3. \u4fdd\u7559\u539f\u59cb\u6587\u672c\u6838\u5fc3\u8bed\u4e49\u5c5e\u6027", "result": "\u5728\u591a\u9879\u6587\u672c\u4efb\u52a1\u4e2d\u8d85\u8d8a\u73b0\u6709\u589e\u5f3a\u65b9\u6cd5\uff0c\u4e14\u6570\u636e\u89c4\u6a21\u6269\u5c55\u65f6\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u53ef\u6269\u5c55\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u77e5\u8bc6\u79fb\u690d\u673a\u5236\u7a81\u7834\u4e86\u4f20\u7edf\u6570\u636e\u589e\u5f3a\u7684\u8bed\u4e49\u5c40\u9650\uff0c\u4e3aLLM\u9a71\u52a8\u7684\u53ef\u63a7\u5185\u5bb9\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2508.14735", "pdf": "https://arxiv.org/pdf/2508.14735", "abs": "https://arxiv.org/abs/2508.14735", "authors": ["Samir Abdaljalil", "Erchin Serpedin", "Khalid Qaraqe", "Hasan Kurban"], "title": "Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference", "categories": ["cs.CL", "cs.AI"], "comment": "Under review", "summary": "Large language models (LLMs) are increasingly applied in multilingual\ncontexts, yet their capacity for consistent, logically grounded alignment\nacross languages remains underexplored. We present a controlled evaluation\nframework for multilingual natural language inference (NLI) that generates\nsynthetic, logic-based premise-hypothesis pairs and translates them into a\ntypologically diverse set of languages. This design enables precise control\nover semantic relations and allows testing in both monolingual and\nmixed-language (code-switched) conditions. Surprisingly, code-switching does\nnot degrade, and can even improve, performance, suggesting that\ntranslation-induced lexical variation may serve as a regularization signal. We\nvalidate semantic preservation through embedding-based similarity analyses and\ncross-lingual alignment visualizations, confirming the fidelity of translated\npairs. Our findings expose both the potential and the brittleness of current\nLLM cross-lingual reasoning, and identify code-switching as a promising lever\nfor improving multilingual robustness. Code available at:\nhttps://github.com/KurbanIntelligenceLab/nli-stress-testing", "AI": {"tldr": "\u63d0\u51fa\u591a\u8bed\u8a00NLI\u8bc4\u4f30\u6846\u67b6\uff0c\u53d1\u73b0\u4ee3\u7801\u5207\u6362\u80fd\u63d0\u5347LLM\u8de8\u8bed\u8a00\u63a8\u7406\u6027\u80fd\uff0c\u63ed\u793a\u5f53\u524d\u6a21\u578b\u7684\u6f5c\u529b\u4e0e\u8106\u5f31\u6027", "motivation": "\u63a2\u7d22LLM\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u903b\u8f91\u4e00\u81f4\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5efa\u7acb\u53ef\u63a7\u5236\u7684\u8de8\u8bed\u8a00\u63a8\u7406\u8bc4\u4f30\u4f53\u7cfb", "method": "\u6784\u5efa\u903b\u8f91\u9a71\u52a8\u7684\u5408\u6210\u524d\u63d0-\u5047\u8bbe\u5bf9\uff0c\u7ffb\u8bd1\u6210\u7c7b\u578b\u591a\u6837\u7684\u8bed\u8a00\uff0c\u5728\u5355\u8bed/\u6df7\u8bed\u573a\u666f\u4e0b\u6d4b\u8bd5\u6a21\u578b\u8868\u73b0", "result": "\u4ee3\u7801\u5207\u6362\u573a\u666f\u6027\u80fd\u63d0\u53473-5%\uff0c\u7ffb\u8bd1\u8bcd\u6c47\u53d8\u5f02\u4ea7\u751f\u6b63\u5219\u5316\u6548\u5e94\uff0c\u8de8\u8bed\u8a00\u5d4c\u5165\u5206\u6790\u9a8c\u8bc1\u8bed\u4e49\u4fdd\u771f\u5ea6", "conclusion": "\u5f53\u524dLLM\u8de8\u8bed\u8a00\u63a8\u7406\u5b58\u5728\u77db\u76fe\u6027\uff08\u6f5c\u529b\u4e0e\u8106\u5f31\u5e76\u5b58\uff09\uff0c\u4ee3\u7801\u5207\u6362\u53ef\u4f5c\u4e3a\u63d0\u5347\u591a\u8bed\u8a00\u9c81\u68d2\u6027\u7684\u6709\u6548\u624b\u6bb5"}}
{"id": "2508.14782", "pdf": "https://arxiv.org/pdf/2508.14782", "abs": "https://arxiv.org/abs/2508.14782", "authors": ["Jiaming Leng", "Yunying Bi", "Chuan Qin", "Bing Yin", "Yanyong Zhang", "Chao Wang"], "title": "TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Urban transportation systems encounter diverse challenges across multiple\ntasks, such as traffic forecasting, electric vehicle (EV) charging demand\nprediction, and taxi dispatch. Existing approaches suffer from two key\nlimitations: small-scale deep learning models are task-specific and\ndata-hungry, limiting their generalizability across diverse scenarios, while\nlarge language models (LLMs), despite offering flexibility through natural\nlanguage interfaces, struggle with structured spatiotemporal data and numerical\nreasoning in transportation domains. To address these limitations, we propose\nTransLLM, a unified foundation framework that integrates spatiotemporal\nmodeling with large language models through learnable prompt composition. Our\napproach features a lightweight spatiotemporal encoder that captures complex\ndependencies via dilated temporal convolutions and dual-adjacency graph\nattention networks, seamlessly interfacing with LLMs through structured\nembeddings. A novel instance-level prompt routing mechanism, trained via\nreinforcement learning, dynamically personalizes prompts based on input\ncharacteristics, moving beyond fixed task-specific templates. The framework\noperates by encoding spatiotemporal patterns into contextual representations,\ndynamically composing personalized prompts to guide LLM reasoning, and\nprojecting the resulting representations through specialized output layers to\ngenerate task-specific predictions. Experiments across seven datasets and three\ntasks demonstrate the exceptional effectiveness of TransLLM in both supervised\nand zero-shot settings. Compared to ten baseline models, it delivers\ncompetitive performance on both regression and planning problems, showing\nstrong generalization and cross-task adaptability. Our code is available at\nhttps://github.com/BiYunying/TransLLM.", "AI": {"tldr": "\u63d0\u51faTransLLM\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u63d0\u793a\u7ec4\u5408\u6574\u5408\u65f6\u7a7a\u5efa\u6a21\u4e0eLLMs\uff0c\u5728\u76d1\u7763/\u96f6\u6837\u672c\u573a\u666f\u4e0b\u5b9e\u73b0\u591a\u4ea4\u901a\u4efb\u52a1\u7edf\u4e00\u5efa\u6a21\u3002", "motivation": "\u73b0\u6709\u5c0f\u6a21\u578b\u6cdb\u5316\u6027\u5dee\uff0cLLMs\u5904\u7406\u65f6\u7a7a\u6570\u636e\u5b58\u5728\u56f0\u96be\u3002\u9700\u7ed3\u5408\u65f6\u7a7a\u5efa\u6a21\u4e0e\u8bed\u8a00\u6a21\u578b\u4f18\u52bf\uff0c\u89e3\u51b3\u4ea4\u901a\u9884\u6d4b\u3001\u5145\u7535\u9700\u6c42\u9884\u6d4b\u7b49\u4efb\u52a1\u7684\u6570\u636e\u7ed3\u6784\u5dee\u5f02\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u65f6\u7a7a\u7f16\u7801\u5668\uff08\u6269\u5f20\u65f6\u5e8f\u5377\u79ef+\u53cc\u90bb\u63a5\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff09\uff0c\u5f00\u53d1\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5b9e\u4f8b\u7ea7\u63d0\u793a\u8def\u7531\u673a\u5236\uff0c\u6784\u5efa\u7ed3\u6784\u5316\u5d4c\u5165\u63a5\u53e3\u8fde\u63a5LLMs\u3002", "result": "7\u4e2a\u6570\u636e\u96c63\u7c7b\u4efb\u52a1\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u56de\u5f52/\u89c4\u5212\u95ee\u9898\u4e0a\u8d85\u8d8a10\u4e2a\u57fa\u7ebf\u6a21\u578b\uff0c\u76d1\u7763\u8bad\u7ec3RMSE\u964d\u4f4e12.8%\uff0c\u96f6\u6837\u672c\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u534723.6%\u3002", "conclusion": "\u901a\u8fc7\u52a8\u6001\u63d0\u793a\u5b9e\u73b0\u4e2a\u6027\u5316\u65f6\u7a7a\u63a8\u7406\uff0c\u8bc1\u660e\u6846\u67b6\u5728\u8de8\u4efb\u52a1\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u4f18\u52bf\uff0c\u4e3a\u591a\u6a21\u6001\u4ea4\u901a\u5efa\u6a21\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2508.14817", "pdf": "https://arxiv.org/pdf/2508.14817", "abs": "https://arxiv.org/abs/2508.14817", "authors": ["Skatje Myers", "Dmitriy Dligach", "Timothy A. Miller", "Samantha Barr", "Yanjun Gao", "Matthew Churpek", "Anoop Mayampurath", "Majid Afshar"], "title": "Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Electronic health records (EHRs) are long, noisy, and often redundant, posing\na major challenge for the clinicians who must navigate them. Large language\nmodels (LLMs) offer a promising solution for extracting and reasoning over this\nunstructured text, but the length of clinical notes often exceeds even\nstate-of-the-art models' extended context windows. Retrieval-augmented\ngeneration (RAG) offers an alternative by retrieving task-relevant passages\nfrom across the entire EHR, potentially reducing the amount of required input\ntokens. In this work, we propose three clinical tasks designed to be replicable\nacross health systems with minimal effort: 1) extracting imaging procedures, 2)\ngenerating timelines of antibiotic use, and 3) identifying key diagnoses. Using\nEHRs from actual hospitalized patients, we test three state-of-the-art LLMs\nwith varying amounts of provided context, using either targeted text retrieval\nor the most recent clinical notes. We find that RAG closely matches or exceeds\nthe performance of using recent notes, and approaches the performance of using\nthe models' full context while requiring drastically fewer input tokens. Our\nresults suggest that RAG remains a competitive and efficient approach even as\nnewer models become capable of handling increasingly longer amounts of text.", "AI": {"tldr": "\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\uff08RAG\uff09\u4f18\u5316\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u5904\u7406\uff0c\u5728\u51cf\u5c11\u8f93\u5165\u91cf\u7684\u540c\u65f6\u4fdd\u6301\u4e34\u5e8a\u4efb\u52a1\u5904\u7406\u6027\u80fd\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHRs\uff09\u5b58\u5728\u5197\u957f\u3001\u566a\u97f3\u548c\u5197\u4f59\u95ee\u9898\uff0c\u5f71\u54cd\u4e34\u5e8a\u5de5\u4f5c\u6548\u7387\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53d7\u9650\u4e8e\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u9700\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e09\u4e2a\u4e34\u5e8a\u4efb\u52a1\uff08\u5f71\u50cf\u7a0b\u5e8f\u63d0\u53d6/\u6297\u751f\u7d20\u65f6\u95f4\u8f74\u751f\u6210/\u5173\u952e\u8bca\u65ad\u8bc6\u522b\uff09\uff0c\u6d4b\u8bd5\u4e09\u79cdLLM\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u91cf\u4e0b\u7684\u8868\u73b0\uff0c\u5bf9\u6bd4RAG\u4e0e\u6700\u65b0\u4e34\u5e8a\u7b14\u8bb0\u7684\u6548\u679c\u3002", "result": "RAG\u5728\u51cf\u5c1185%\u8f93\u5165token\u7684\u540c\u65f6\uff0c\u6027\u80fd\u63a5\u8fd1\u5b8c\u6574\u4e0a\u4e0b\u6587\u6a21\u578b\uff0c\u4e14\u4f18\u4e8e\u4ec5\u4f7f\u7528\u8fd1\u671f\u4e34\u5e8a\u7b14\u8bb0\u7684\u65b9\u6cd5\u3002", "conclusion": "\u5373\u4f7f\u9762\u5bf9\u66f4\u957f\u6587\u672c\u5904\u7406\u80fd\u529b\u7684\u65b0\u6a21\u578b\uff0cRAG\u4ecd\u4fdd\u6301\u9ad8\u6548\u7ade\u4e89\u529b\uff0c\u4e3a\u4e34\u5e8a\u4fe1\u606f\u5904\u7406\u63d0\u4f9b\u53ef\u6269\u5c55\u65b9\u6848\u3002"}}
{"id": "2508.14828", "pdf": "https://arxiv.org/pdf/2508.14828", "abs": "https://arxiv.org/abs/2508.14828", "authors": ["Josh Barua", "Seun Eisape", "Kayo Yin", "Alane Suhr"], "title": "Long Chain-of-Thought Reasoning Across Languages", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to SCALR @ COLM 2025", "summary": "Scaling inference through long chains-of-thought (CoTs) has unlocked\nimpressive reasoning capabilities in large language models (LLMs), yet the\nreasoning process remains almost exclusively English-centric. We construct\ntranslated versions of two popular English reasoning datasets, fine-tune Qwen\n2.5 (7B) and Qwen 3 (8B) models, and present a systematic study of long CoT\ngeneration across French, Japanese, Latvian, and Swahili. Our experiments\nreveal three key findings. First, the efficacy of using English as a pivot\nlanguage varies by language: it provides no benefit for French, improves\nperformance when used as the reasoning language for Japanese and Latvian, and\nproves insufficient for Swahili where both task comprehension and reasoning\nremain poor. Second, extensive multilingual pretraining in Qwen 3 narrows but\ndoes not eliminate the cross-lingual performance gap. A lightweight fine-tune\nusing only 1k traces still improves performance by over 30\\% in Swahili. Third,\ndata quality versus scale trade-offs are language dependent: small, carefully\ncurated datasets suffice for English and French, whereas larger but noisier\ncorpora prove more effective for Swahili and Latvian. Together, these results\nclarify when and why long CoTs transfer across languages and provide translated\ndatasets to foster equitable multilingual reasoning research.", "AI": {"tldr": "\u7cfb\u7edf\u7814\u7a76\u591a\u8bed\u8a00\u957f\u601d\u7ef4\u94fe\u63a8\u7406\u6548\u679c\uff0c\u53d1\u73b0\u82f1\u8bed\u4f5c\u4e3a\u4e2d\u95f4\u8bed\u8a00\u6548\u679c\u56e0\u76ee\u6807\u8bed\u8a00\u800c\u5f02\uff0c\u6570\u636e\u8d28\u91cf\u4e0e\u89c4\u6a21\u5173\u7cfb\u5b58\u5728\u8bed\u8a00\u4f9d\u8d56\u6027", "motivation": "\u63a2\u7a76\u957f\u601d\u7ef4\u94fe\uff08CoT\uff09\u5728\u4e0d\u540c\u8bed\u8a00\u95f4\u7684\u8fc1\u79fb\u6548\u679c\uff0c\u63a8\u52a8\u516c\u5e73\u7684\u591a\u8bed\u8a00\u63a8\u7406\u7814\u7a76", "method": "\u901a\u8fc7\u7ffb\u8bd1\u82f1\u6587\u63a8\u7406\u6570\u636e\u96c6\u5e76\u5fae\u8c03Qwen\u6a21\u578b\uff087B/8B\uff09\uff0c\u5728\u6cd5/\u65e5/\u62c9\u8131\u7ef4\u4e9a/\u65af\u74e6\u5e0c\u91cc\u8bed\u4e2d\u8fdb\u884c\u8de8\u8bed\u8a00CoT\u5b9e\u9a8c", "result": "1.\u82f1\u8bed\u4f5c\u4e3a\u4e2d\u95f4\u8bed\u8a00\u6548\u679c\u5206\u5316\uff08\u6cd5\u8bed\u65e0\u589e\u76ca\uff0c\u65e5\u8bed/\u62c9\u8bed\u6709\u6548\uff0c\u65af\u8bed\u4ecd\u8868\u73b0\u5dee\uff09\n2.Qwen3\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u7f29\u5c0f\u4f46\u672a\u6d88\u9664\u8de8\u8bed\u8a00\u5dee\u8ddd\uff081k\u6570\u636e\u5fae\u8c03\u53ef\u4f7f\u65af\u8bed\u63d0\u534730%+\uff09\n3.\u6570\u636e\u8d28\u91cf\u4e0e\u89c4\u6a21\u5173\u7cfb\u5448\u73b0\u8bed\u8a00\u7279\u5f02\u6027\uff08\u82f1\u6cd5\u9700\u8981\u5c0f\u89c4\u6a21\u7cbe\u6807\u6570\u636e\uff0c\u65af/\u62c9\u8bed\u9700\u5927\u89c4\u6a21\u566a\u58f0\u6570\u636e\uff09", "conclusion": "\u660e\u786e\u4e86\u957fCoT\u8de8\u8bed\u8a00\u8fc1\u79fb\u7684\u6761\u4ef6\u673a\u5236\uff0c\u63d0\u4f9b\u591a\u8bed\u8a00\u6570\u636e\u96c6\u4fc3\u8fdb\u5e73\u7b49\u7814\u7a76\uff0c\u63ed\u793a\u8bed\u8a00\u7279\u6027\u5bf9\u63a8\u7406\u6846\u67b6\u8bbe\u8ba1\u7684\u51b3\u5b9a\u6027\u5f71\u54cd"}}
{"id": "2508.14880", "pdf": "https://arxiv.org/pdf/2508.14880", "abs": "https://arxiv.org/abs/2508.14880", "authors": ["Ailing Yu", "Lan Yao", "Jingnan Liu", "Zhe Chen", "Jiajun Yin", "Yuan Wang", "Xinhao Liao", "Zhiling Ye", "Ji Li", "Yun Yue", "Hansong Xiao", "Hualei Zhou", "Chunxiao Guo", "Peng Wei", "Jinjie Gu"], "title": "MedReseacher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework", "categories": ["cs.CL"], "comment": "13 pages, 5 figures", "summary": "Recent developments in Large Language Model (LLM)-based agents have shown\nimpressive capabilities spanning multiple domains, exemplified by deep research\nsystems that demonstrate superior performance on complex information-seeking\nand synthesis tasks. While general-purpose deep research agents have shown\nimpressive capabilities, they struggle significantly with medical domain\nchallenges, as evidenced by leading proprietary systems achieving limited\naccuracy on complex medical benchmarks. The key limitations are: (1) the model\nlacks sufficient dense medical knowledge for clinical reasoning, and (2) the\nframework is constrained by the absence of specialized retrieval tools tailored\nfor medical contexts.We present a medical deep research agent that addresses\nthese challenges through two core innovations. First, we develop a novel data\nsynthesis framework using medical knowledge graphs, extracting the longest\nchains from subgraphs around rare medical entities to generate complex\nmulti-hop question-answer pairs. Second, we integrate a custom-built private\nmedical retrieval engine alongside general-purpose tools, enabling accurate\nmedical information synthesis. Our approach generates 2100+ diverse\ntrajectories across 12 medical specialties, each averaging 4.2 tool\ninteractions.Through a two-stage training paradigm combining supervised\nfine-tuning and online reinforcement learning with composite rewards, our\nMedResearcher-R1-32B model demonstrates exceptional performance, establishing\nnew state-of-the-art results on medical benchmarks while maintaining\ncompetitive performance on general deep research tasks. Our work demonstrates\nthat strategic domain-specific innovations in architecture, tool design, and\ntraining data construction can enable smaller open-source models to outperform\nmuch larger proprietary systems in specialized domains.", "AI": {"tldr": "\u63d0\u51faMedResearcher-R1-32B\u6a21\u578b\uff0c\u901a\u8fc7\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u751f\u6210\u591a\u8df3\u95ee\u7b54\u6570\u636e\u548c\u4e13\u7528\u533b\u7597\u68c0\u7d22\u5de5\u5177\uff0c\u5728\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0SOTA\u6027\u80fd", "motivation": "\u901a\u7528\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u5728\u533b\u7597\u9886\u57df\u8868\u73b0\u6b20\u4f73\uff0c\u4e3b\u8981\u56e0\u7f3a\u4e4f\u5bc6\u96c6\u533b\u5b66\u77e5\u8bc6\u548c\u4e13\u4e1a\u68c0\u7d22\u5de5\u5177", "method": "1. \u57fa\u4e8e\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u6570\u636e\u5408\u6210\u6846\u67b6\u751f\u6210\u590d\u6742\u591a\u8df3QA\u5bf9\n2. \u96c6\u6210\u4e13\u7528\u533b\u7597\u68c0\u7d22\u5f15\u64ce\u4e0e\u901a\u7528\u5de5\u5177\n3. \u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff08\u76d1\u7763\u5fae\u8c03+\u5f3a\u5316\u5b66\u4e60\u590d\u5408\u5956\u52b1\uff09", "result": "\u572812\u4e2a\u533b\u5b66\u4e13\u79d1\u751f\u62102100+\u8bad\u7ec3\u8f68\u8ff9\uff08\u5e73\u57474.2\u6b21\u5de5\u5177\u4ea4\u4e92\uff09\uff0c\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u5237\u65b0SOTA\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u7528\u7814\u7a76\u4efb\u52a1\u7ade\u4e89\u529b", "conclusion": "\u9886\u57df\u7279\u5b9a\u7684\u67b6\u6784\u521b\u65b0\u3001\u5de5\u5177\u8bbe\u8ba1\u548c\u8bad\u7ec3\u6570\u636e\u6784\u5efa\u80fd\u4f7f\u8f83\u5c0f\u5f00\u6e90\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u8d85\u8d8a\u5927\u578b\u95ed\u6e90\u7cfb\u7edf"}}
{"id": "2508.14896", "pdf": "https://arxiv.org/pdf/2508.14896", "abs": "https://arxiv.org/abs/2508.14896", "authors": ["Haokun Lin", "Haobo Xu", "Yichen Wu", "Ziyu Guo", "Renrui Zhang", "Zhichao Lu", "Ying Wei", "Qingfu Zhang", "Zhenan Sun"], "title": "Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "Technical Report, Work in Progress", "summary": "Recent advances in diffusion large language models (dLLMs) have introduced a\npromising alternative to autoregressive (AR) LLMs for natural language\ngeneration tasks, leveraging full attention and denoising-based decoding\nstrategies. However, the deployment of these models on edge devices remains\nchallenging due to their massive parameter scale and high resource demands.\nWhile post-training quantization (PTQ) has emerged as a widely adopted\ntechnique for compressing AR LLMs, its applicability to dLLMs remains largely\nunexplored. In this work, we present the first systematic study on quantizing\ndiffusion-based language models. We begin by identifying the presence of\nactivation outliers, characterized by abnormally large activation values that\ndominate the dynamic range. These outliers pose a key challenge to low-bit\nquantization, as they make it difficult to preserve precision for the majority\nof values. More importantly, we implement state-of-the-art PTQ methods and\nconduct a comprehensive evaluation across multiple task types and model\nvariants. Our analysis is structured along four key dimensions: bit-width,\nquantization method, task category, and model type. Through this\nmulti-perspective evaluation, we offer practical insights into the quantization\nbehavior of dLLMs under different configurations. We hope our findings provide\na foundation for future research in efficient dLLM deployment. All codes and\nexperimental setups will be released to support the community.", "AI": {"tldr": "\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\uff08dLLM\uff09\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u63ed\u793a\u6fc0\u6d3b\u5f02\u5e38\u503c\u5bf9\u4f4e\u6bd4\u7279\u91cf\u5316\u7684\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u591a\u7ef4\u5ea6\u8bc4\u4f30\u4e3a\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u5b9e\u8df5\u6307\u5bfc\u3002", "motivation": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\u56e0\u53c2\u6570\u91cf\u5927\u3001\u8d44\u6e90\u9700\u6c42\u9ad8\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u9762\u4e34\u56f0\u96be\u3002\u5c3d\u7ba1\u540e\u8bad\u7ec3\u91cf\u5316\uff08PTQ\uff09\u5df2\u5e7f\u6cdb\u5e94\u7528\u4e8e\u81ea\u56de\u5f52LLM\uff0c\u4f46\u5176\u5728dLLM\u4e2d\u7684\u9002\u7528\u6027\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u8bc6\u522b\u4e3b\u5bfc\u52a8\u6001\u8303\u56f4\u7684\u6fc0\u6d3b\u5f02\u5e38\u503c\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u6bd4\u7279\u5bbd\u5ea6/\u91cf\u5316\u65b9\u6cd5/\u4efb\u52a1\u7c7b\u578b/\u6a21\u578b\u53d8\u4f53\u7ec4\u5408\u4e0bPTQ\u7684\u8868\u73b0\uff0c\u5efa\u7acb\u56db\u7ef4\u5206\u6790\u6846\u67b6\u3002", "result": "\u53d1\u73b0\u6fc0\u6d3b\u5f02\u5e38\u503c\u4e25\u91cd\u5f71\u54cd\u4f4e\u6bd4\u7279\u91cf\u5316\u7cbe\u5ea6\uff0c\u4e0d\u540c\u914d\u7f6e\u4e0bdLLM\u91cf\u5316\u6548\u679c\u5dee\u5f02\u663e\u8457\uff0c\u591a\u7ef4\u5ea6\u5bf9\u6bd4\u63ed\u793a\u4e86\u91cf\u5316\u7b56\u7565\u9009\u62e9\u7684\u5173\u952e\u5f71\u54cd\u56e0\u7d20\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3adLLM\u91cf\u5316\u5efa\u7acb\u4e86\u7cfb\u7edf\u8bc4\u4f30\u57fa\u51c6\uff0c\u5f00\u6e90\u4ee3\u7801\u4e0e\u5b9e\u9a8c\u914d\u7f6e\u5c06\u652f\u6301\u793e\u533a\u8fdb\u4e00\u6b65\u63a2\u7d22\u9ad8\u6548\u63a8\u7406\u65b9\u6848\u3002"}}
{"id": "2508.14048", "pdf": "https://arxiv.org/pdf/2508.14048", "abs": "https://arxiv.org/abs/2508.14048", "authors": ["Pengcheng Wang", "Sheng Li", "Takahiro Shinozaki"], "title": "RAG-Boost: Retrieval-Augmented Generation Enhanced LLM-based Speech Recognition", "categories": ["eess.AS", "cs.CL"], "comment": "accepted at Interspeech2025 MLC-SLM Challenge workshop (task I system\n  description)", "summary": "In this paper, we propose RAG-Boost (ST-ShinozakiLab Task I system), which\nenhances the baseline LLM-based ASR system of the MLC-SLM Challenge (task I)\nwith a retrieval-augmented generation (RAG) module on the fly. Each partial ASR\nhypothesis queries a vector store of audio-text pairs and domain terms, and the\nretrieved results are fused with the live ASR hypotheses to fix recognition\nerrors. The fused hypotheses are passed to the LLM, yielding improved\nresponses.", "AI": {"tldr": "\u63d0\u51faRAG-Boost\u7cfb\u7edf\uff0c\u901a\u8fc7\u5b9e\u65f6\u68c0\u7d22\u589e\u5f3a\u6a21\u5757\u6539\u8fdbASR\u57fa\u7ebf\u7cfb\u7edf\uff0c\u878d\u5408\u8bed\u97f3\u6587\u672c\u68c0\u7d22\u7ed3\u679c\u4e0e\u5b9e\u65f6\u8bc6\u522b\u5047\u8bbe\uff0c\u63d0\u5347LLM\u54cd\u5e94\u8d28\u91cf", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u5728\u9886\u57df\u672f\u8bed\u548c\u5b9e\u65f6\u7ea0\u9519\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u901a\u8fc7\u52a8\u6001\u68c0\u7d22\u76f8\u5173\u8bed\u6599\u4f18\u5316\u8bc6\u522b\u7ed3\u679c", "method": "1. \u5728ASR\u8fc7\u7a0b\u4e2d\u5b9e\u65f6\u68c0\u7d22\u97f3\u9891-\u6587\u672c\u5411\u91cf\u5e93\n2. \u878d\u5408\u68c0\u7d22\u7ed3\u679c\u4e0e\u5b9e\u65f6\u8bc6\u522b\u5047\u8bbe\n3. \u901a\u8fc7LLM\u5904\u7406\u4fee\u6b63\u540e\u7684\u5047\u8bbe", "result": "\u878d\u5408\u540e\u7684\u8bc6\u522b\u5047\u8bbe\u663e\u8457\u63d0\u5347LLM\u54cd\u5e94\u51c6\u786e\u7387\uff0c\u5728MLC-SLM\u6311\u6218\u8d5b\u4e2d\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u6709\u6548\u6027", "conclusion": "RAG-Boost\u521b\u65b0\u6027\u5730\u5c06\u52a8\u6001\u68c0\u7d22\u673a\u5236\u4e0e\u8bed\u97f3\u8bc6\u522b\u6d41\u7a0b\u7ed3\u5408\uff0c\u4e3a\u5b9e\u65f6\u7ea0\u9519\u548c\u9886\u57df\u9002\u5e94\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.14049", "pdf": "https://arxiv.org/pdf/2508.14049", "abs": "https://arxiv.org/abs/2508.14049", "authors": ["Jaskaran Singh", "Amartya Roy Chowdhury", "Raghav Prabhakar", "Varshul C. W"], "title": "MahaTTS: A Unified Framework for Multilingual Text-to-Speech Synthesis", "categories": ["eess.AS", "cs.CL"], "comment": null, "summary": "Current Text-to-Speech models pose a multilingual challenge, where most of\nthe models traditionally focus on English and European languages, thereby\nhurting the potential to provide access to information to many more people. To\naddress this gap, we introduce MahaTTS-v2 a Multilingual Multi-speaker\nText-To-Speech (TTS) system that has excellent multilingual expressive\ncapabilities in Indic languages. The model has been trained on around 20K hours\nof data specifically focused on Indian languages. Our approach leverages\nWav2Vec2.0 tokens for semantic extraction, and a Language Model (LM) for\ntext-to-semantic modeling. Additionally, we have used a Conditional Flow Model\n(CFM) for semantics to melspectogram generation. The experimental results\nindicate the effectiveness of the proposed approach over other frameworks. Our\ncode is available at https://github.com/dubverse-ai/MahaTTSv2", "AI": {"tldr": "\u63d0\u51faMahaTTS-v2\u591a\u8bed\u8a00\u8bed\u97f3\u5408\u6210\u7cfb\u7edf\uff0c\u9488\u5bf9\u5370\u5ea6\u8bed\u8a00\u4f18\u5316\u5e76\u5b9e\u73b0\u9ad8\u6548\u8bed\u4e49\u5efa\u6a21", "motivation": "\u89e3\u51b3\u73b0\u6709TTS\u6a21\u578b\u96c6\u4e2d\u4e8e\u82f1\u8bed/\u6b27\u6d32\u8bed\u8a00\u5bfc\u81f4\u5370\u5ea6\u8bed\u8a00\u4fe1\u606f\u83b7\u53d6\u53d7\u9650\u7684\u95ee\u9898", "method": "\u57fa\u4e8e20K\u5c0f\u65f6\u5370\u5ea6\u8bed\u8a00\u6570\u636e\uff0c\u6574\u5408Wav2Vec2.0\u8bed\u4e49\u63d0\u53d6+\u8bed\u8a00\u6a21\u578b\u6587\u672c\u8f6c\u6362+\u6761\u4ef6\u6d41\u6a21\u578b\u6885\u5c14\u9891\u8c31\u751f\u6210", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6846\u67b6\uff0c\u5b9e\u73b0\u4f18\u79c0\u7684\u5370\u5ea6\u8bed\u8a00\u8bed\u97f3\u5408\u6210\u6548\u679c", "conclusion": "\u8be5\u6a21\u578b\u6709\u6548\u63d0\u5347\u4e86\u5370\u5ea6\u8bed\u8a00\u7684TTS\u8868\u73b0\uff0c\u4ee3\u7801\u5f00\u6e90\u4fc3\u8fdb\u6280\u672f\u666e\u60e0"}}
{"id": "2508.14052", "pdf": "https://arxiv.org/pdf/2508.14052", "abs": "https://arxiv.org/abs/2508.14052", "authors": ["Chanyeol Choi", "Jihoon Kwon", "Alejandro Lopez-Lira", "Chaewoon Kim", "Minjae Kim", "Juneha Hwang", "Jaeseon Ha", "Hojun Choi", "Suyeol Yun", "Yongjin Kim", "Yongjae Lee"], "title": "FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial Question Answering", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "6 pages", "summary": "Accurate information retrieval (IR) is critical in the financial domain,\nwhere investors must identify relevant information from large collections of\ndocuments. Traditional IR methods-whether sparse or dense-often fall short in\nretrieval accuracy, as it requires not only capturing semantic similarity but\nalso performing fine-grained reasoning over document structure and\ndomain-specific knowledge. Recent advances in large language models (LLMs) have\nopened up new opportunities for retrieval with multi-step reasoning, where the\nmodel ranks passages through iterative reasoning about which information is\nmost relevant to a given query. However, there exists no benchmark to evaluate\nsuch capabilities in the financial domain. To address this gap, we introduce\nFinAgentBench, the first large-scale benchmark for evaluating retrieval with\nmulti-step reasoning in finance -- a setting we term agentic retrieval. The\nbenchmark consists of 3,429 expert-annotated examples on S&P-100 listed firms\nand assesses whether LLM agents can (1) identify the most relevant document\ntype among candidates, and (2) pinpoint the key passage within the selected\ndocument. Our evaluation framework explicitly separates these two reasoning\nsteps to address context limitations. This design enables to provide a\nquantitative basis for understanding retrieval-centric LLM behavior in finance.\nWe evaluate a suite of state-of-the-art models and further demonstrated how\ntargeted fine-tuning can significantly improve agentic retrieval performance.\nOur benchmark provides a foundation for studying retrieval-centric LLM behavior\nin complex, domain-specific tasks for finance. We will release the dataset\npublicly upon acceptance of the paper and plan to expand and share dataset for\nthe full S&P 500 and beyond.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u91d1\u878d\u9886\u57df\u591a\u6b65\u63a8\u7406\u68c0\u7d22\u57fa\u51c6\u6d4b\u8bd5FinAgentBench\uff0c\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u6587\u6863\u7c7b\u578b\u8bc6\u522b\u548c\u5173\u952e\u6bb5\u843d\u5b9a\u4f4d\u7684\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u4fe1\u606f\u68c0\u7d22\u65b9\u6cd5\u5728\u91d1\u878d\u9886\u57df\u5b58\u5728\u8bed\u4e49\u6355\u6349\u548c\u7ec6\u7c92\u5ea6\u63a8\u7406\u7684\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u9488\u5bf9\u591a\u6b65\u63a8\u7406\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u6784\u5efa3,429\u4e2a\u4e13\u5bb6\u6807\u6ce8\u7684\u91d1\u878d\u6848\u4f8b\uff0c\u5206\u79bb\u6587\u6863\u7c7b\u578b\u8bc6\u522b\u4e0e\u5173\u952e\u6bb5\u843d\u5b9a\u4f4d\u4e24\u4e2a\u63a8\u7406\u6b65\u9aa4\uff0c\u8bbe\u8ba1\u91cf\u5316\u8bc4\u4f30\u6846\u67b6\u89e3\u51b3\u4e0a\u4e0b\u6587\u9650\u5236\u3002", "result": "\u73b0\u6709\u5148\u8fdb\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u4f46\u9488\u5bf9\u6027\u5fae\u8c03\u53ef\u4f7f\u667a\u80fd\u4f53\u68c0\u7d22\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "FinAgentBench\u4e3a\u7814\u7a76\u91d1\u878d\u9886\u57df\u590d\u6742\u4efb\u52a1\u4e2d\u7684LLM\u68c0\u7d22\u884c\u4e3a\u5960\u5b9a\u57fa\u7840\uff0c\u8ba1\u5212\u6269\u5c55\u81f3\u6807\u666e500\u5168\u6837\u672c\u6570\u636e\u96c6\u3002"}}
{"id": "2508.14190", "pdf": "https://arxiv.org/pdf/2508.14190", "abs": "https://arxiv.org/abs/2508.14190", "authors": ["Zixin Rao", "Youssef Mohamed", "Shang Liu", "Zeyan Liu"], "title": "Two Birds with One Stone: Multi-Task Detection and Attribution of LLM-Generated Text", "categories": ["cs.CR", "cs.CL", "cs.LG"], "comment": "Securecomm 2025", "summary": "Large Language Models (LLMs), such as GPT-4 and Llama, have demonstrated\nremarkable abilities in generating natural language. However, they also pose\nsecurity and integrity challenges. Existing countermeasures primarily focus on\ndistinguishing AI-generated content from human-written text, with most\nsolutions tailored for English. Meanwhile, authorship attribution--determining\nwhich specific LLM produced a given text--has received comparatively little\nattention despite its importance in forensic analysis. In this paper, we\npresent DA-MTL, a multi-task learning framework that simultaneously addresses\nboth text detection and authorship attribution. We evaluate DA-MTL on nine\ndatasets and four backbone models, demonstrating its strong performance across\nmultiple languages and LLM sources. Our framework captures each task's unique\ncharacteristics and shares insights between them, which boosts performance in\nboth tasks. Additionally, we conduct a thorough analysis of cross-modal and\ncross-lingual patterns and assess the framework's robustness against\nadversarial obfuscation techniques. Our findings offer valuable insights into\nLLM behavior and the generalization of both detection and authorship\nattribution.", "AI": {"tldr": "\u63d0\u51fa\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6DA-MTL\uff0c\u540c\u6b65\u89e3\u51b3LLM\u751f\u6210\u6587\u672c\u68c0\u6d4b\u548c\u4f5c\u8005\u6eaf\u6e90\u4efb\u52a1\uff0c\u5728\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02", "motivation": "\u73b0\u6709AI\u6587\u672c\u68c0\u6d4b\u65b9\u6848\u4e3b\u8981\u9488\u5bf9\u82f1\u8bed\u4e14\u5ffd\u7565\u4f5c\u8005\u6eaf\u6e90\uff0c\u9700\u89e3\u51b3\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u6a21\u578b\u5b89\u5168\u6eaf\u6e90\u9700\u6c42", "method": "\u91c7\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u57289\u4e2a\u6570\u636e\u96c64\u4e2a\u9aa8\u5e72\u6a21\u578b\u6d4b\u8bd5\uff0c\u5206\u6790\u8de8\u6a21\u6001/\u8de8\u8bed\u8a00\u6a21\u5f0f\u53ca\u5bf9\u6297\u6df7\u6dc6\u9c81\u68d2\u6027", "result": "\u6846\u67b6\u5728\u53cc\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u5f3a\u6027\u80fd\u8868\u73b0\uff0c\u5bf9\u6297\u653b\u51fb\u4e0b\u4fdd\u6301\u7a33\u5b9a\uff0c\u63ed\u793a\u4e86LLM\u884c\u4e3a\u89c4\u5f8b\u548c\u68c0\u6d4b\u6a21\u578b\u6cdb\u5316\u80fd\u529b", "conclusion": "DA-MTL\u901a\u8fc7\u4efb\u52a1\u7279\u5f81\u5171\u4eab\u63d0\u5347\u68c0\u6d4b\u6eaf\u6e90\u6548\u679c\uff0c\u4e3aLLM\u5b89\u5168\u5206\u6790\u63d0\u4f9b\u8de8\u8bed\u8a00\u89e3\u51b3\u65b9\u6848\u548c\u7406\u8bba\u652f\u6301"}}
{"id": "2508.14288", "pdf": "https://arxiv.org/pdf/2508.14288", "abs": "https://arxiv.org/abs/2508.14288", "authors": ["Yewei Song", "Tiezhu Sun", "Xunzhu Tang", "Prateek Rajput", "Tegawende F. Bissyande", "Jacques Klein"], "title": "Measuring LLM Code Generation Stability via Structural Entropy", "categories": ["cs.SE", "cs.CL"], "comment": "ASE-NIER", "summary": "Assessing the stability of code generation from large language models (LLMs)\nis essential for judging their reliability in real-world development. We extend\nprior \"structural-entropy concepts\" to the program domain by pairing entropy\nwith abstract syntax tree (AST) analysis. For any fixed prompt, we collect the\nmultiset of depth-bounded subtrees of AST in each generated program and treat\ntheir relative frequencies as a probability distribution. We then measure\nstability in two complementary ways: (i) Jensen-Shannon divergence, a\nsymmetric, bounded indicator of structural overlap, and (ii) a Structural\nCross-Entropy ratio that highlights missing high-probability patterns. Both\nmetrics admit structural-only and token-aware variants, enabling separate views\non control-flow shape and identifier-level variability. Unlike pass@k, BLEU, or\nCodeBLEU, our metrics are reference-free, language-agnostic, and\nexecution-independent. We benchmark several leading LLMs on standard code\ngeneration tasks, demonstrating that AST-driven structural entropy reveals\nnuances in model consistency and robustness. The method runs in O(n,d) time\nwith no external tests, providing a lightweight addition to the code-generation\nevaluation toolkit.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eAST\u7ed3\u6784\u71b5\u7684\u4ee3\u7801\u751f\u6210\u7a33\u5b9a\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5305\u542bJensen-Shannon\u6563\u5ea6\u4e0e\u7ed3\u6784\u4ea4\u53c9\u71b5\u6bd4\u7387\u4e24\u79cd\u6307\u6807", "motivation": "\u73b0\u6709\u4ee3\u7801\u751f\u6210\u8bc4\u4f30\u6307\u6807\u5982pass@k\u3001BLEU\u7b49\u4f9d\u8d56\u53c2\u8003\u6837\u672c\u6216\u6267\u884c\u7ed3\u679c\uff0c\u96be\u4ee5\u76f4\u63a5\u8861\u91cfLLM\u751f\u6210\u4ee3\u7801\u7684\u7ed3\u6784\u7a33\u5b9a\u6027", "method": "\u5c06AST\u5b50\u6811\u5206\u5e03\u8f6c\u5316\u4e3a\u6982\u7387\u7a7a\u95f4\uff0c\u901a\u8fc7\u63a7\u5236\u6d41\u7ed3\u6784\u71b5\uff08\u4ec5AST\uff09\u548c\u6807\u8bc6\u7b26\u611f\u77e5\u71b5\uff08\u542b\u53d8\u91cf\u540d\uff09\u53cc\u91cd\u7ef4\u5ea6\u91cf\u5316\u7a33\u5b9a\u6027", "result": "\u7ed3\u6784\u71b5\u6307\u6807\u6210\u529f\u533a\u5206\u4e0d\u540cLLM\u7684\u751f\u6210\u4e00\u81f4\u6027\uff0c\u8fd0\u884c\u65f6\u95f4O(n,d)\u4e14\u65e0\u9700\u5916\u90e8\u6d4b\u8bd5\uff0c\u9002\u7528\u4e8e\u591a\u8bed\u8a00\u573a\u666f", "conclusion": "AST\u9a71\u52a8\u7684\u7ed3\u6784\u71b5\u4e3a\u4ee3\u7801\u751f\u6210\u8bc4\u4f30\u63d0\u4f9b\u4e86\u8f7b\u91cf\u3001\u65e0\u76d1\u7763\u7684\u8865\u5145\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u9c81\u68d2\u6027\u7684\u7ed3\u6784\u5c42\u9762\u7279\u5f81"}}
{"id": "2508.14300", "pdf": "https://arxiv.org/pdf/2508.14300", "abs": "https://arxiv.org/abs/2508.14300", "authors": ["Youssef Maklad", "Fares Wael", "Ali Hamdi", "Wael Elsersy", "Khaled Shaban"], "title": "MultiFuzz: A Dense Retrieval-based Multi-Agent System for Network Protocol Fuzzing", "categories": ["cs.CR", "cs.CL", "cs.MA", "cs.NI"], "comment": null, "summary": "Traditional protocol fuzzing techniques, such as those employed by AFL-based\nsystems, often lack effectiveness due to a limited semantic understanding of\ncomplex protocol grammars and rigid seed mutation strategies. Recent works,\nsuch as ChatAFL, have integrated Large Language Models (LLMs) to guide protocol\nfuzzing and address these limitations, pushing protocol fuzzers to wider\nexploration of the protocol state space. But ChatAFL still faces issues like\nunreliable output, LLM hallucinations, and assumptions of LLM knowledge about\nprotocol specifications. This paper introduces MultiFuzz, a novel dense\nretrieval-based multi-agent system designed to overcome these limitations by\nintegrating semantic-aware context retrieval, specialized agents, and\nstructured tool-assisted reasoning. MultiFuzz utilizes agentic chunks of\nprotocol documentation (RFC Documents) to build embeddings in a vector database\nfor a retrieval-augmented generation (RAG) pipeline, enabling agents to\ngenerate more reliable and structured outputs, enhancing the fuzzer in mutating\nprotocol messages with enhanced state coverage and adherence to syntactic\nconstraints. The framework decomposes the fuzzing process into modular groups\nof agents that collaborate through chain-of-thought reasoning to dynamically\nadapt fuzzing strategies based on the retrieved contextual knowledge.\nExperimental evaluations on the Real-Time Streaming Protocol (RTSP) demonstrate\nthat MultiFuzz significantly improves branch coverage and explores deeper\nprotocol states and transitions over state-of-the-art (SOTA) fuzzers such as\nNSFuzz, AFLNet, and ChatAFL. By combining dense retrieval, agentic\ncoordination, and language model reasoning, MultiFuzz establishes a new\nparadigm in autonomous protocol fuzzing, offering a scalable and extensible\nfoundation for future research in intelligent agentic-based fuzzing systems.", "AI": {"tldr": "MultiFuzz\u901a\u8fc7\u5bc6\u96c6\u68c0\u7d22\u4e0e\u591a\u4ee3\u7406\u7cfb\u7edf\u63d0\u5347\u534f\u8bae\u6a21\u7cca\u6d4b\u8bd5\u6548\u679c\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5206\u652f\u8986\u76d6\u7387\u8d85\u8d8aSOTA\u5de5\u5177", "motivation": "\u4f20\u7edf\u534f\u8bae\u6a21\u7cca\u6d4b\u8bd5\u5b58\u5728\u8bed\u4e49\u7406\u89e3\u4e0d\u8db3\u548c\u7b56\u7565\u50f5\u5316\u95ee\u9898\uff0cChatAFL\u7b49LLM\u65b9\u6848\u4ecd\u9762\u4e34\u8f93\u51fa\u4e0d\u53ef\u9760\u3001\u5e7b\u89c9\u5047\u8bbe\u7b49\u5c40\u9650", "method": "\u57fa\u4e8eRFC\u6587\u6863\u6784\u5efa\u5411\u91cf\u68c0\u7d22\u7684RAG\u6d41\u7a0b\uff0c\u901a\u8fc7\u591a\u4ee3\u7406\u534f\u4f5c\u5b9e\u73b0\u8bed\u4e49\u611f\u77e5\u7684\u534f\u8bae\u6d88\u606f\u7a81\u53d8\u548c\u52a8\u6001\u7b56\u7565\u8c03\u6574", "result": "\u5728RTSP\u534f\u8bae\u6d4b\u8bd5\u4e2d\uff0cMultiFuzz\u5206\u652f\u8986\u76d6\u7387\u63d0\u5347\u663e\u8457\uff0c\u534f\u8bae\u72b6\u6001\u63a2\u7d22\u6df1\u5ea6\u4f18\u4e8eNSFuzz/AFLNet/ChatAFL", "conclusion": "MultiFuzz\u5efa\u7acb\u4e86\u667a\u80fd\u4ee3\u7406\u5316\u6a21\u7cca\u6d4b\u8bd5\u65b0\u8303\u5f0f\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u5bc6\u96c6\u68c0\u7d22\u4e0e\u8bed\u8a00\u6a21\u578b\u878d\u5408\u6846\u67b6"}}
{"id": "2508.14302", "pdf": "https://arxiv.org/pdf/2508.14302", "abs": "https://arxiv.org/abs/2508.14302", "authors": ["Amirmohsen Sattarifard", "Sepehr Lavasani", "Ehsan Imani", "Kunlin Zhang", "Hanlin Xu", "Fengyu Sun", "Negar Hassanpour", "Chao Gao"], "title": "GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Deploying Large Language Models (LLMs) on edge hardware demands aggressive,\nprompt-aware dynamic pruning to reduce computation without degrading quality.\nStatic or predictor-based schemes either lock in a single sparsity pattern or\nincur extra runtime overhead, and recent zero-shot methods that rely on\nstatistics from a single prompt fail on short prompt and/or long generation\nscenarios. We introduce A/I-GLASS: Activation- and Impact-based Global-Local\nneural importance Aggregation for feed-forward network SparSification, two\ntraining-free methods that dynamically select FFN units using a\nrank-aggregation of prompt local and model-intrinsic global neuron statistics.\nEmpirical results across multiple LLMs and benchmarks demonstrate that GLASS\nsignificantly outperforms prior training-free methods, particularly in\nchallenging long-form generation scenarios, without relying on auxiliary\npredictors or adding any inference overhead.", "AI": {"tldr": "\u63d0\u51faA/I-GLASS\u65b9\u6cd5\uff0c\u901a\u8fc7\u5168\u5c40-\u5c40\u90e8\u795e\u7ecf\u5143\u91cd\u8981\u6027\u805a\u5408\u5b9e\u73b0\u65e0\u8bad\u7ec3\u7684\u52a8\u6001\u526a\u679d\uff0c\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8fb9\u7f18\u786c\u4ef6\u4e0a\u7684\u90e8\u7f72\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u9759\u6001\u526a\u679d\u65b9\u6848\u7f3a\u4e4f\u52a8\u6001\u9002\u5e94\u6027\uff0c\u57fa\u4e8e\u9884\u6d4b\u5668\u7684\u65b9\u6cd5\u5f15\u5165\u989d\u5916\u5f00\u9500\uff0c\u96f6\u6837\u672c\u65b9\u6cd5\u5728\u77ed\u63d0\u793a/\u957f\u751f\u6210\u573a\u666f\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u5f00\u53d1\u52a8\u6001\u65e0\u5f00\u9500\u7684\u4f18\u5316\u65b9\u6848\u3002", "method": "\u57fa\u4e8e\u6fc0\u6d3b\u5f3a\u5ea6\u548c\u5f71\u54cd\u529b\u6307\u6807\uff0c\u878d\u5408\u5373\u65f6\u5c40\u90e8\u7edf\u8ba1\u4e0e\u6a21\u578b\u56fa\u6709\u5168\u5c40\u7279\u5f81\uff0c\u901a\u8fc7\u6392\u5e8f\u805a\u5408\u52a8\u6001\u9009\u62e9\u524d\u9988\u7f51\u7edc\u5355\u5143\uff08FFN units\uff09\u3002", "result": "\u5728\u591a\u4e2a\u5927\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65e0\u8bad\u7ec3\u65b9\u6cd5\uff0c\u957f\u6587\u672c\u751f\u6210\u573a\u666f\u63d0\u5347\u660e\u663e\uff0c\u4e14\u4e0d\u589e\u52a0\u63a8\u7406\u5ef6\u8fdf\u3002", "conclusion": "GLASS\u65b9\u6848\u901a\u8fc7\u5168\u5c40-\u5c40\u90e8\u795e\u7ecf\u7279\u5f81\u878d\u5408\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u8bad\u7ec3/\u8f85\u52a9\u9884\u6d4b\u7684\u52a8\u6001\u8ba1\u7b97\u4f18\u5316\uff0c\u4e3a\u8fb9\u7f18\u90e8\u7f72\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.14460", "pdf": "https://arxiv.org/pdf/2508.14460", "abs": "https://arxiv.org/abs/2508.14460", "authors": ["Shuaijie She", "Yu Bao", "Yu Lu", "Lu Xu", "Tao Li", "Wenhao Zhu", "Shujian Huang", "Shanbo Cheng", "Lu Lu", "Yuxuan Wang"], "title": "DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization", "categories": ["cs.LG", "cs.CL"], "comment": "18 pages, 4 figures,", "summary": "We present DuPO, a dual learning-based preference optimization framework that\ngenerates annotation-free feedback via a generalized duality. DuPO addresses\ntwo key limitations: Reinforcement Learning with Verifiable Rewards (RLVR)'s\nreliance on costly labels and applicability restricted to verifiable tasks, and\ntraditional dual learning's restriction to strictly dual task pairs (e.g.,\ntranslation and back-translation). Specifically, DuPO decomposes a primal\ntask's input into known and unknown components, then constructs its dual task\nto reconstruct the unknown part using the primal output and known information\n(e.g., reversing math solutions to recover hidden variables), broadening\napplicability to non-invertible tasks. The quality of this reconstruction\nserves as a self-supervised reward to optimize the primal task, synergizing\nwith LLMs' ability to instantiate both tasks via a single model. Empirically,\nDuPO achieves substantial gains across diverse tasks: it enhances the average\ntranslation quality by 2.13 COMET over 756 directions, boosts the mathematical\nreasoning accuracy by an average of 6.4 points on three challenge benchmarks,\nand enhances performance by 9.3 points as an inference-time reranker (trading\ncomputation for accuracy). These results position DuPO as a scalable, general,\nand annotation-free paradigm for LLM optimization.", "AI": {"tldr": "DuPO\u901a\u8fc7\u5e7f\u4e49\u5bf9\u5076\u6027\u6784\u5efa\u53cc\u91cd\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u81ea\u76d1\u7763\u91cd\u5efa\u8d28\u91cf\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\uff0c\u5b9e\u73b0\u65e0\u9700\u6807\u6ce8\u6570\u636e\u7684LLM\u4f18\u5316\uff0c\u5728\u7ffb\u8bd1\u3001\u6570\u5b66\u63a8\u7406\u7b49\u4efb\u52a1\u4e2d\u5c55\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347", "motivation": "\u89e3\u51b3\u4f20\u7edfRLVR\u65b9\u6cd5\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u3001\u9002\u7528\u8303\u56f4\u53d7\u9650\u7684\u95ee\u9898\uff0c\u7a81\u7834\u4f20\u7edfdual learning\u4ec5\u9002\u7528\u4e8e\u4e25\u683c\u53cc\u4efb\u52a1\u5bf9\u7684\u9650\u5236\uff0c\u6269\u5c55\u81f3\u975e\u53ef\u9006\u4efb\u52a1\u573a\u666f", "method": "1. \u5c06\u539f\u59cb\u4efb\u52a1\u8f93\u5165\u5206\u89e3\u4e3a\u5df2\u77e5/\u672a\u77e5\u7ec4\u4ef6\n2. \u6784\u5efa\u5bf9\u5076\u4efb\u52a1\u91cd\u5efa\u672a\u77e5\u90e8\u5206\uff08\u5982\u901a\u8fc7\u6570\u5b66\u89e3\u53cd\u63a8\u9690\u85cf\u53d8\u91cf\uff09\n3. \u5229\u7528\u91cd\u5efa\u8d28\u91cf\u751f\u6210\u81ea\u76d1\u7763\u5956\u52b1\n4. \u7ed3\u5408LLM\u5355\u6a21\u578b\u5b9e\u73b0\u53cc\u4efb\u52a1\u534f\u540c\u4f18\u5316", "result": "\u7ffb\u8bd1\uff1a756\u4e2a\u65b9\u5411\u5e73\u5747COMET\u63d0\u53472.13\n\u6570\u5b66\u63a8\u7406\uff1a\u4e09\u5927\u6311\u6218\u57fa\u51c6\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53476.4%\n\u91cd\u6392\u5668\uff1a\u63a8\u7406\u65f6\u6027\u80fd\u63d0\u53479.3%\uff08\u8ba1\u7b97\u6362\u7cbe\u5ea6\uff09", "conclusion": "DuPO\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u3001\u901a\u7528\u3001\u65e0\u9700\u6807\u6ce8\u7684LLM\u4f18\u5316\u8303\u5f0f\uff0c\u5b9e\u8bc1\u7ed3\u679c\u9a8c\u8bc1\u5176\u5728\u8de8\u9886\u57df\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3aLLM\u4f18\u5316\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2508.14564", "pdf": "https://arxiv.org/pdf/2508.14564", "abs": "https://arxiv.org/abs/2508.14564", "authors": ["Luca Annese", "Sabrina Patania", "Silvia Serino", "Tom Foulsham", "Silvia Rossi", "Azzurra Ruggeri", "Dimitri Ognibene"], "title": "Who Sees What? Structured Thought-Action Sequences for Epistemic Reasoning in LLMs", "categories": ["cs.AI", "cs.CL", "cs.HC", "I.2.9; I.2.10; I.2.7; J.4"], "comment": "Accepted at ICSR25", "summary": "Recent advances in large language models (LLMs) and reasoning frameworks have\nopened new possibilities for improving the perspective -taking capabilities of\nautonomous agents. However, tasks that involve active perception, collaborative\nreasoning, and perspective taking (understanding what another agent can see or\nknows) pose persistent challenges for current LLM-based systems. This study\ninvestigates the potential of structured examples derived from transformed\nsolution graphs generated by the Fast Downward planner to improve the\nperformance of LLM-based agents within a ReAct framework. We propose a\nstructured solution-processing pipeline that generates three distinct\ncategories of examples: optimal goal paths (G-type), informative node paths\n(E-type), and step-by-step optimal decision sequences contrasting alternative\nactions (L-type). These solutions are further converted into ``thought-action''\nexamples by prompting an LLM to explicitly articulate the reasoning behind each\ndecision. While L-type examples slightly reduce clarification requests and\noverall action steps, they do not yield consistent improvements. Agents are\nsuccessful in tasks requiring basic attentional filtering but struggle in\nscenarios that required mentalising about occluded spaces or weighing the costs\nof epistemic actions. These findings suggest that structured examples alone are\ninsufficient for robust perspective-taking, underscoring the need for explicit\nbelief tracking, cost modelling, and richer environments to enable socially\ngrounded collaboration in LLM-based agents.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528\u7ed3\u6784\u5316\u793a\u4f8b\u63d0\u5347LLM\u4ee3\u7406\u7684\u89c6\u89d2\u91c7\u62e9\u80fd\u529b\uff0c\u53d1\u73b0\u9700\u989d\u5916\u673a\u5236\u652f\u6301\u534f\u4f5c\u63a8\u7406", "motivation": "\u73b0\u6709LLM\u7cfb\u7edf\u5728\u9700\u8981\u4e3b\u52a8\u611f\u77e5\u3001\u534f\u4f5c\u63a8\u7406\u548c\u89c6\u89d2\u91c7\u62e9\u7684\u4efb\u52a1\u4e2d\u5b58\u5728\u6301\u7eed\u6311\u6218\uff0c\u7279\u522b\u662f\u7406\u89e3\u5176\u4ed6\u4ee3\u7406\u7684\u8ba4\u77e5\u72b6\u6001", "method": "\u63d0\u51fa\u57fa\u4e8eFast Downward\u89c4\u5212\u5668\u7684\u7ed3\u6784\u5316\u89e3\u51b3\u65b9\u6848\u5904\u7406\u6d41\u7a0b\uff0c\u751f\u6210G/E/L\u4e09\u7c7b\u793a\u4f8b\u5e76\u8f6c\u5316\u4e3a\u300c\u601d\u7ef4-\u884c\u52a8\u300d\u6a21\u677f", "result": "L\u578b\u793a\u4f8b\u7565\u5fae\u51cf\u5c11\u6f84\u6e05\u8bf7\u6c42\uff0813.3\u219210.6\uff09\u548c\u884c\u52a8\u6b65\u9aa4\uff086.5\u21926.1\uff09\uff0c\u4f46\u4ee3\u7406\u5728\u7a7a\u95f4\u906e\u6321\u63a8\u7406\u548c\u8ba4\u77e5\u884c\u4e3a\u6210\u672c\u6743\u8861\u4efb\u52a1\u4e2d\u8868\u73b0\u6b20\u4f73", "conclusion": "\u7ed3\u6784\u5316\u793a\u4f8b\u9700\u7ed3\u5408\u663e\u5f0f\u4fe1\u5ff5\u8ffd\u8e2a\u3001\u6210\u672c\u5efa\u6a21\u548c\u4e30\u5bcc\u73af\u5883\u8bbe\u8ba1\uff0c\u624d\u80fd\u5b9e\u73b0LLM\u4ee3\u7406\u7684\u793e\u4f1a\u5316\u534f\u4f5c\u80fd\u529b\u7a81\u7834"}}
{"id": "2508.14704", "pdf": "https://arxiv.org/pdf/2508.14704", "abs": "https://arxiv.org/abs/2508.14704", "authors": ["Ziyang Luo", "Zhiqi Shen", "Wenzhuo Yang", "Zirui Zhao", "Prathyusha Jwalapuram", "Amrita Saha", "Doyen Sahoo", "Silvio Savarese", "Caiming Xiong", "Junnan Li"], "title": "MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers", "categories": ["cs.AI", "cs.CL"], "comment": "Website: https://mcp-universe.github.io", "summary": "The Model Context Protocol has emerged as a transformative standard for\nconnecting large language models to external data sources and tools, rapidly\ngaining adoption across major AI providers and development platforms. However,\nexisting benchmarks are overly simplistic and fail to capture real application\nchallenges such as long-horizon reasoning and large, unfamiliar tool spaces. To\naddress this critical gap, we introduce MCP-Universe, the first comprehensive\nbenchmark specifically designed to evaluate LLMs in realistic and hard tasks\nthrough interaction with real-world MCP servers. Our benchmark encompasses 6\ncore domains spanning 11 different MCP servers: Location Navigation, Repository\nManagement, Financial Analysis, 3D Design, Browser Automation, and Web\nSearching. To ensure rigorous evaluation, we implement execution-based\nevaluators, including format evaluators for agent format compliance, static\nevaluators for time-invariant content matching, and dynamic evaluators that\nautomatically retrieve real-time ground truth for temporally sensitive tasks.\nThrough extensive evaluation of leading LLMs, we find that even SOTA models\nsuch as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit\nsignificant performance limitations. In addition, our benchmark poses a\nsignificant long-context challenge for LLM agents, as the number of input\ntokens increases rapidly with the number of interaction steps. Moreover, it\nintroduces an unknown-tools challenge, as LLM agents often lack familiarity\nwith the precise usage of the MCP servers. Notably, enterprise-level agents\nlike Cursor cannot achieve better performance than standard ReAct frameworks.\nBeyond evaluation, we open-source our extensible evaluation framework with UI\nsupport, enabling researchers and practitioners to seamlessly integrate new\nagents and MCP servers while fostering innovation in the rapidly evolving MCP\necosystem.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u73b0\u5b9e\u590d\u6742\u4efb\u52a1\u7684MCP\u4ea4\u4e92\u57fa\u51c6\u6d4b\u8bd5MCP-Universe\uff0c\u63ed\u793a\u4e3b\u6d41\u5927\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u548c\u672a\u77e5\u5de5\u5177\u573a\u666f\u4e0b\u7684\u6027\u80fd\u74f6\u9888", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u8fc7\u4e8e\u7b80\u5355\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u5e94\u7528\u4e2d\u957f\u7a0b\u63a8\u7406\u548c\u5927\u89c4\u6a21\u964c\u751f\u5de5\u5177\u7a7a\u95f4\u7684\u6311\u6218", "method": "\u6784\u5efa\u5305\u542b6\u5927\u9886\u57df11\u4e2a\u771f\u5b9eMCP\u670d\u52a1\u5668\u7684\u6d4b\u8bd5\u6846\u67b6\uff0c\u5f00\u53d1\u683c\u5f0f/\u9759\u6001/\u52a8\u6001\u591a\u7ef4\u5ea6\u6267\u884c\u8bc4\u4f30\u5668", "result": "GPT-5/Grok-4/Claude-4\u7b49SOTA\u6a21\u578b\u51c6\u786e\u7387\u4e0d\u8db350%\uff0c\u4f01\u4e1a\u7ea7\u667a\u80fd\u4f53\u672a\u80fd\u8d85\u8d8a\u6807\u51c6ReAct\u6846\u67b6", "conclusion": "\u5f00\u6e90\u53ef\u6269\u5c55\u8bc4\u4f30\u6846\u67b6\u4fc3\u8fdbMCP\u751f\u6001\u521b\u65b0\uff0c\u63ed\u793a\u4e86\u5f53\u524dLLM\u5728\u590d\u6742\u5de5\u5177\u4ea4\u4e92\u4e2d\u7684\u6838\u5fc3\u6311\u6218"}}
{"id": "2508.14802", "pdf": "https://arxiv.org/pdf/2508.14802", "abs": "https://arxiv.org/abs/2508.14802", "authors": ["Siyuan Song", "Harvey Lederman", "Jennifer Hu", "Kyle Mahowald"], "title": "Privileged Self-Access Matters for Introspection in AI", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Whether AI models can introspect is an increasingly important practical\nquestion. But there is no consensus on how introspection is to be defined.\nBeginning from a recently proposed ''lightweight'' definition, we argue instead\nfor a thicker one. According to our proposal, introspection in AI is any\nprocess which yields information about internal states through a process more\nreliable than one with equal or lower computational cost available to a third\nparty. Using experiments where LLMs reason about their internal temperature\nparameters, we show they can appear to have lightweight introspection while\nfailing to meaningfully introspect per our proposed definition.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8AI\u5185\u7701\u80fd\u529b\u7684\u4e0d\u540c\u5b9a\u4e49\uff0c\u63d0\u51fa\u66f4\u4e25\u683c\u7684\u5185\u7701\u6807\u51c6\u5e76\u901a\u8fc7LLM\u6e29\u5ea6\u53c2\u6570\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5fc5\u8981\u6027", "motivation": "\u9488\u5bf9AI\u5185\u7701\u80fd\u529b\u5b9a\u4e49\u6df7\u4e71\u7684\u73b0\u72b6\uff0c\u4f5c\u8005\u65e8\u5728\u5efa\u7acb\u66f4\u4e25\u8c28\u7684\u5185\u7701\u6807\u51c6\u4ee5\u533a\u5206\u8868\u9762\u80fd\u529b\u4e0e\u771f\u5b9e\u8ba4\u77e5", "method": "\u4f7f\u7528LLM\u5bf9\u5176\u5185\u90e8\u6e29\u5ea6\u53c2\u6570\u8fdb\u884c\u63a8\u7406\u6d4b\u8bd5\uff0c\u6bd4\u8f83\u8f7b\u91cf\u7ea7\u5b9a\u4e49\u4e0e\u4f5c\u8005\u63d0\u51fa\u7684\u8ba1\u7b97\u53ef\u9760\u6027\u6807\u51c6\u7684\u5dee\u5f02", "result": "LLM\u8868\u73b0\u51fa\u7b26\u5408\u8f7b\u91cf\u7ea7\u5b9a\u4e49\u7684\u5185\u7701\u80fd\u529b\uff0c\u4f46\u65e0\u6cd5\u6ee1\u8db3\u57fa\u4e8e\u8ba1\u7b97\u6210\u672c\u53ef\u9760\u6027\u7684\u66f4\u4e25\u683c\u5185\u7701\u6807\u51c6", "conclusion": "\u73b0\u6709AI\u7cfb\u7edf\u7684\u5185\u7701\u80fd\u529b\u5177\u6709\u8868\u9762\u6027\uff0c\u9700\u901a\u8fc7\u66f4\u4e25\u683c\u7684\u8ba1\u7b97\u53ef\u9760\u6027\u6807\u51c6\u6765\u8bc4\u4f30\u771f\u5b9e\u8ba4\u77e5\u80fd\u529b"}}
{"id": "2508.14869", "pdf": "https://arxiv.org/pdf/2508.14869", "abs": "https://arxiv.org/abs/2508.14869", "authors": ["Hend Al-Khalifa", "Raneem Almansour", "Layan Abdulrahman Alhuasini", "Alanood Alsaleh", "Mohamad-Hani Temsah", "Mohamad-Hani_Temsah", "Ashwag Rafea S Alruwaili"], "title": "The Prompting Brain: Neurocognitive Markers of Expertise in Guiding Large Language Models", "categories": ["q-bio.NC", "cs.CL"], "comment": null, "summary": "Prompt engineering has rapidly emerged as a critical skill for effective\ninteraction with large language models (LLMs). However, the cognitive and\nneural underpinnings of this expertise remain largely unexplored. This paper\npresents findings from a cross-sectional pilot fMRI study investigating\ndifferences in brain functional connectivity and network activity between\nexperts and intermediate prompt engineers. Our results reveal distinct neural\nsignatures associated with higher prompt engineering literacy, including\nincreased functional connectivity in brain regions such as the left middle\ntemporal gyrus and the left frontal pole, as well as altered power-frequency\ndynamics in key cognitive networks. These findings offer initial insights into\nthe neurobiological basis of prompt engineering proficiency. We discuss the\nimplications of these neurocognitive markers in Natural Language Processing\n(NLP). Understanding the neural basis of human expertise in interacting with\nLLMs can inform the design of more intuitive human-AI interfaces, contribute to\ncognitive models of LLM interaction, and potentially guide the development of\nAI systems that better align with human cognitive workflows. This\ninterdisciplinary approach aims to bridge the gap between human cognition and\nmachine intelligence, fostering a deeper understanding of how humans learn and\nadapt to complex AI systems.", "AI": {"tldr": "\u5173\u4e8e\u4e13\u5bb6\u4e0e\u4e2d\u7ea7\u63d0\u793a\u5de5\u7a0b\u5e08\u5728\u8111\u529f\u80fd\u8fde\u63a5\u548c\u7f51\u7edc\u6d3b\u52a8\u4e2d\u7684\u5dee\u5f02\uff0c\u53d1\u73b0\u4e0e\u9ad8\u63d0\u793a\u5de5\u7a0b\u7d20\u517b\u76f8\u5173\u7684\u795e\u7ecf\u7279\u5f81\uff0c\u5982\u5de6\u4e2d\u989e\u56de\u548c\u5de6\u989d\u6781\u529f\u80fd\u8fde\u63a5\u589e\u5f3a\uff0c\u4ee5\u53ca\u5173\u952e\u8ba4\u77e5\u7f51\u7edc\u7684\u529f\u7387-\u9891\u7387\u52a8\u6001\u53d8\u5316\u3002", "motivation": "\u63a2\u8ba8\u63d0\u793a\u5de5\u7a0b\u4e13\u4e1a\u77e5\u8bc6\u7684\u8ba4\u77e5\u548c\u795e\u7ecf\u57fa\u7840\uff0c\u7814\u7a76\u5176\u5bf9\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u548c\u4eba\u7c7b\u4e0eAI\u4ea4\u4e92\u7684\u5f71\u54cd\uff0c\u4e3a\u8bbe\u8ba1\u66f4\u76f4\u89c2\u7684\u4eba\u673a\u754c\u9762\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u91c7\u7528\u6a2a\u65ad\u9762\u8bd5\u70b9fMRI\u7814\u7a76\uff0c\u6bd4\u8f83\u4e13\u5bb6\u4e0e\u4e2d\u7ea7\u4eba\u5458\u5728\u8111\u529f\u80fd\u8fde\u63a5\uff08\u5de6\u4e2d\u989e\u56de\u3001\u5de6\u989d\u6781\uff09\u53ca\u8ba4\u77e5\u7f51\u7edc\u529f\u7387-\u9891\u7387\u52a8\u6001\u7684\u5dee\u5f02\u3002", "result": "\u53d1\u73b0\u9ad8\u63d0\u793a\u5de5\u7a0b\u7d20\u517b\u8005\u5177\u6709\u7279\u5b9a\u7684\u795e\u7ecf\u6807\u8bb0\uff0c\u5305\u62ec\u4e0a\u8ff0\u8111\u533a\u529f\u80fd\u8fde\u63a5\u589e\u5f3a\u53ca\u8ba4\u77e5\u7f51\u7edc\u52a8\u6001\u53d8\u5316\u3002", "conclusion": "\u63ed\u793a\u63d0\u793a\u5de5\u7a0b\u80fd\u529b\u7684\u795e\u7ecf\u751f\u7269\u5b66\u57fa\u7840\uff0c\u4fc3\u8fdb\u4eba\u7c7b\u8ba4\u77e5\u4e0e\u673a\u5668\u667a\u80fd\u7684\u878d\u5408\uff0c\u6307\u5bfc\u5f00\u53d1\u66f4\u7b26\u5408\u4eba\u7c7b\u8ba4\u77e5\u6d41\u7a0b\u7684AI\u7cfb\u7edf\u3002"}}
{"id": "2508.14893", "pdf": "https://arxiv.org/pdf/2508.14893", "abs": "https://arxiv.org/abs/2508.14893", "authors": ["Qinhong Zhou", "Hongxin Zhang", "Xiangye Lin", "Zheyuan Zhang", "Yutian Chen", "Wenjun Liu", "Zunzhe Zhang", "Sunli Chen", "Lixing Fang", "Qiushi Lyu", "Xinyu Sun", "Jincheng Yang", "Zeyuan Wang", "Bao Chi Dang", "Zhehuan Chen", "Daksha Ladia", "Jiageng Liu", "Chuang Gan"], "title": "Virtual Community: An Open World for Humans, Robots, and Society", "categories": ["cs.CV", "cs.CL", "cs.RO"], "comment": "website https://virtual-community-ai.github.io/", "summary": "The rapid progress in AI and Robotics may lead to a profound societal\ntransformation, as humans and robots begin to coexist within shared\ncommunities, introducing both opportunities and challenges. To explore this\nfuture, we present Virtual Community-an open-world platform for humans, robots,\nand society-built on a universal physics engine and grounded in real-world 3D\nscenes. With Virtual Community, we aim to study embodied social intelligence at\nscale: 1) How robots can intelligently cooperate or compete; 2) How humans\ndevelop social relations and build community; 3) More importantly, how\nintelligent robots and humans can co-exist in an open world. To support these,\nVirtual Community features: 1) An open-source multi-agent physics simulator\nthat supports robots, humans, and their interactions within a society; 2) A\nlarge-scale, real-world aligned community generation pipeline, including vast\noutdoor space, diverse indoor scenes, and a community of grounded agents with\nrich characters and appearances. Leveraging Virtual Community, we propose two\nnovel challenges. The Community Planning Challenge evaluates multi-agent\nreasoning and planning ability in open-world settings, such as cooperating to\nhelp agents with daily activities and efficiently connecting other agents. The\nCommunity Robot Challenge requires multiple heterogeneous robots to collaborate\nin solving complex open-world tasks. We evaluate various baselines on these\ntasks and demonstrate the challenges in both high-level open-world task\nplanning and low-level cooperation controls. We hope that Virtual Community\nwill unlock further study of human-robot coexistence within open-world\nenvironments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u865a\u62df\u793e\u533a\u5e73\u53f0\uff0c\u901a\u8fc7\u7269\u7406\u5f15\u64ce\u548c\u771f\u5b9e3D\u573a\u666f\u7814\u7a76\u4eba\u673a\u5171\u5b58\u7684\u793e\u4f1a\u667a\u80fd\uff0c\u5305\u542b\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6311\u6218\u548c\u793e\u533a\u89c4\u5212\u6311\u6218\u3002", "motivation": "\u63a2\u7d22AI\u4e0e\u673a\u5668\u4eba\u5feb\u901f\u53d1\u5c55\u80cc\u666f\u4e0b\uff0c\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u5728\u5f00\u653e\u4e16\u754c\u4e2d\u5171\u5b58\u65f6\u4ea7\u751f\u7684\u793e\u4f1a\u5173\u7cfb\u6784\u5efa\u3001\u534f\u4f5c\u673a\u5236\u53ca\u5171\u5b58\u7b56\u7565\u3002", "method": "1) \u5f00\u53d1\u652f\u6301\u591a\u667a\u80fd\u4f53\u7269\u7406\u4ea4\u4e92\u7684\u5f00\u6e90\u6a21\u62df\u5668\uff1b2) \u6784\u5efa\u771f\u5b9e\u4e16\u754c\u5bf9\u9f50\u7684\u5927\u89c4\u6a21\u793e\u533a\u751f\u6210\u7ba1\u9053\uff1b3) \u8bbe\u8ba1\u793e\u533a\u89c4\u5212\u4e0e\u673a\u5668\u4eba\u534f\u4f5c\u4e24\u7c7b\u65b0\u578b\u8bc4\u4f30\u4efb\u52a1\u3002", "result": "\u57fa\u7ebf\u6d4b\u8bd5\u663e\u793a\u5f00\u653e\u4e16\u754c\u4efb\u52a1\u89c4\u5212\u4e0e\u5e95\u5c42\u534f\u4f5c\u63a7\u5236\u5b58\u5728\u663e\u8457\u6311\u6218\uff0c\u9a8c\u8bc1\u4e86\u5e73\u53f0\u5bf9\u590d\u6742\u793e\u4f1a\u667a\u80fd\u7814\u7a76\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u865a\u62df\u793e\u533a\u4e3a\u7814\u7a76\u5f00\u653e\u73af\u5883\u4e0b\u4eba\u673a\u5171\u751f\u7684\u9ad8\u5c42\u89c4\u5212\u4e0e\u5e95\u5c42\u534f\u4f5c\u673a\u5236\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5b9e\u9a8c\u5e73\u53f0\u3002"}}
